2018,Online Learning of Quantum States,Suppose we have many copies of an unknown n-qubit state $\rho$. We measure some copies of $\rho$ using a known two-outcome measurement E_1  then other copies using a measurement E_2  and so on. At each stage t  we generate a current hypothesis $\omega_t$ about the state $\rho$  using the outcomes of the previous measurements. We show that it is possible to do this in a way that guarantees that $|\trace(E_i \omega_t)  - \trace(E_i\rho)|$  the error in our prediction for the next measurement  is at least $eps$ at most $O(n / eps^2)  $\ times. Even in the non-realizable setting---where there could be arbitrary noise in the measurement outcomes---we show how to output hypothesis states that incur at most  $O(\sqrt {Tn})  $ excess loss over the best possible state on the first $T$ measurements. These results generalize a 2007 theorem by Aaronson on the PAC-learnability of quantum states  to the online and regret-minimization settings. We give three different ways to prove our results---using convex optimization  quantum postselection  and sequential fat-shattering dimension---which have different advantages in terms of parameters and portability.,Online Learning of Quantum States

Scott Aaronson
UT Austin ⇤

aaronson@cs.utexas.edu

Xinyi Chen

Google AI Princeton †
xinyic@google.com

Elad Hazan

Princeton University and Google AI Princeton

ehazan@cs.princeton.edu

Satyen Kale

Google AI  New York

satyenkale@google.com

Ashwin Nayak

University of Waterloo ‡

ashwin.nayak@uwaterloo.ca

Abstract

Suppose we have many copies of an unknown n-qubit state ⇢. We measure some
copies of ⇢ using a known two-outcome measurement E1  then other copies using
a measurement E2  and so on. At each stage t  we generate a current hypothesis !t
about the state ⇢  using the outcomes of the previous measurements. We show that
it is possible to do this in a way that guarantees that |Tr(Ei!t)  Tr(Ei⇢)|  the er-
ror in our prediction for the next measurement  is at least " at most On/"2 times.
Even in the “non-realizable” setting—where there could be arbitrary noise in the
measurement outcomes—we show how to output hypothesis states that incur at
most O(pT n ) excess loss over the best possible state on the ﬁrst T measurements.
These results generalize a 2007 theorem by Aaronson on the PAC-learnability of
quantum states  to the online and regret-minimization settings. We give three
different ways to prove our results—using convex optimization  quantum postse-
lection  and sequential fat-shattering dimension—which have different advantages
in terms of parameters and portability.

1

Introduction

State tomography is a fundamental task in quantum computing of great practical and theoretical
importance. In a typical scenario  we have access to an apparatus that is capable of producing many
copies of a quantum state  and we wish to obtain a description of the state via suitable measurements.
Such a description would allow us  for example  to check the accuracy with which the apparatus
constructs a speciﬁc target state.
How many single-copy measurements are needed to “learn” an unknown n-qubit quantum state ⇢?
Suppose we wish to reconstruct the full 2n ⇥ 2n density matrix  even approximately  to within "
in trace distance. If we make no assumptions about ⇢  then it is straightforward to show that the
number of measurements needed grows exponentially with n. In fact  even when we allow joint
measurement of multiple copies of the state  an exponential number of copies of ⇢ are required (see 
⇤Supported by a Vannevar Bush Faculty Fellowship from the US Department of Defense. Part of this work

was done while the author was supported by an NSF Alan T. Waterman Award.

†Part of this work was done when the author was a research assistant at Princeton University.
‡Research supported in part by NSERC Canada.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

e.g.  O’Donnell and Wright [2016]  Haah et al. [2017]). (A “joint measurement” of two or more
states on disjoint sequences of qubits is a single measurement of all the qubits together.)
Suppose  on the other hand  that there is some probability distribution D over possible yes/no
measurements  where we identify the measurements with 2n ⇥ 2n Hermitian matrices E with
eigenvalues in [0  1]. Further suppose we are only concerned about learning the state ⇢ well
enough to predict the outcomes of most measurements E drawn from D—where “predict” means
approximately calculating the probability  Tr(E⇢)  of a “yes” result. Then for how many (known)
sample measurements Ei  drawn independently from D  do we need to know the approximate value
of Tr(Ei⇢)  before we have enough data to achieve this?
Aaronson [2007] proved that the number of sample measurements needed  m  grows only linearly
with the number of qubits n. What makes this surprising is that it represents an exponential reduction
compared to full quantum state tomography. Furthermore  the prediction strategy is extremely simple.
Informally  we merely need to ﬁnd any “hypothesis state” ! that satisﬁes Tr(Ei!) ⇡ Tr(Ei⇢) for
all the sample measurements E1  . . .   Em. Then with high probability over the choice of sample
measurements  that hypothesis ! necessarily “generalizes”  in the sense that Tr(E!) ⇡ Tr(E⇢) for
most additional E’s drawn from D. The learning theorem led to followup work including a full
characterization of quantum advice (Aaronson and Drucker [2014]); efﬁcient learning for stabilizer
states (Rocchetto [2017]); the “shadow tomography” protocol (Aaronson [2018]); and recently  the
ﬁrst experimental demonstration of quantum state PAC-learning (Rocchetto et al. [2017]).
A major drawback of the learning theorem due to Aaronson is the assumption that the sample
measurements are drawn independently from D—and moreover  that the same distribution D governs
both the training samples  and the measurements on which the learner’s performance is later tested.
It has long been understood  in computational learning theory  that these assumptions are often
unrealistic: they fail to account for adversarial environments  or environments that change over
time. This is precisely the state of affairs in current experimental implementations of quantum
information processing. Not all measurements of quantum states may be available or feasible in a
speciﬁc implementation  which measurements are feasible is dictated by Nature  and as we develop
more control over the experimental set-up  more sophisticated measurements become available. The
task of learning a state prepared in the laboratory thus takes the form of a game  with the theorist
on one side  and the experimentalist and Nature on the other: the theorist is repeatedly challenged
to predict the behaviour of the state with respect to the next measurement that Nature allows the
experimentalist to realize  with the opportunity to reﬁne the hypothesis as more measurement data
become available.
It is thus desirable to design learning algorithms that work in the more stringent online learning model.
Here the learner is presented a sequence of input points  say x1  x2  . . .  one at a time. Crucially  there
is no assumption whatsoever about the xt’s: the sequence could be chosen adversarially  and even
adaptively  which means that the choice of xt might depend on the learner’s behavior on x1  . . .   xt1.
The learner is trying to learn some unknown function f (x)  about which it initially knows only that f
belongs to some hypothesis class H—or perhaps not even that; we also consider the scenario where
the learner simply tries to compete with the best predictor in H  which might or might not be a good
predictor. The learning proceeds as follows: for each t  the learner ﬁrst guesses a value yt for f (xt) 
and is then told the true value f (xt)  or perhaps only an approximation of this value. Our goal is
to design a learning algorithm with the following guarantee: regardless of the sequence of xt’s  the
learner’s guess  yt  will be far from the true value f (xt) at most k times (where k  of course  is as
small as possible). The xt’s on which the learner errs could be spaced arbitrarily; all we require is
that they be bounded in number.
This leads to the following question: can the learning theorem established by Aaronson [2007] be
generalized to the online learning setting? In other words: is it true that  given a sequence E1  E2  . . .
of yes/no measurements  where each Et is followed shortly afterward by an approximation of
Tr(Et⇢)  there is a way to anticipate the Tr(Et⇢) values by guesses yt 2 [0  1]  in such a way that
|yt  Tr(Et⇢)| >" at most  say  O(n) times (where "> 0 is some constant  and n again is the
number of qubits)? The purpose of this paper is to provide an afﬁrmative answer.
Throughout the paper  we consider only two-outcome measurements of an n qubit mixed state ⇢  and
we specify such a measurement by a 2n ⇥ 2n Hermitian matrix E with eigenvalues in [0  1]. We say
that E “accepts” ⇢ with probability Tr(E⇢) and “rejects” ⇢ with probability 1  Tr(E⇢). We prove
that:

2

"2 values of t.

Theorem 1. Let ⇢ be an n-qubit mixed state  and let E1  E2  . . . be a sequence of 2-outcome
measurements that are revealed to the learner one by one  each followed by a value bt 2 [0  1]
such that |Tr(Et⇢)  bt| "/3. Then there is an explicit strategy for outputting hypothesis states
!1 ! 2  . . . such that |Tr(Et!t)  Tr(Et⇢)| >" for at most O n

We also prove a theorem for the so-called regret minimization model (i.e.  the “non-realizable case”) 
where we make no assumption about the input data arising from an actual quantum state  and our
goal is simply to do not much worse than the best hypothesis state that could be found with perfect
foresight. In this model  the measurements E1  E2  . . . are presented to a learner one-by-one. In
iteration t  after seeing Et  the learner is challenged to output a hypothesis state !t  and then suffers
a “loss” equal to `t(Tr(Et!t)) where `t is a real function that is revealed to the learner. Important
examples of loss functions are L1 loss  when `t(z) := |z  bt|  and L2 loss  when `t(z) := (z  bt)2 
where bt 2 [0  1]. The number bt may be an approximation of Tr(Et⇢) for some ﬁxed but unknown
quantum state ⇢  but is allowed to be arbitrary in general. In particular  the pairs (Et  bt) may not
be consistent with any quantum state. Deﬁne the regret RT   after T iterations  to be the amount by
which the actual loss of the learner exceeds the loss of the best single hypothesis:

RT :=

`t(Tr(Et!t))  min

'

TXt=1

`t(Tr(Et')) .

TXt=1

The learner’s objective is to minimize regret. We show that:
Theorem 2. Let E1  E2  . . . be a sequence of two-outcome measurements on an n-qubit state
presented to the learner  and `1 ` 2  . . . be the corresponding loss functions revealed in successive
iterations in the regret minimization model. Suppose `t is convex and L-Lipschitz; in particular 
for every x 2 R  there is a sub-derivative `0t(x) such that |`0t(x)| L. Then there is an explicit
learning strategy that guarantees regret RT = O(LpT n ) for all T . This is so even assuming the
measurement Et and loss function `t are chosen adaptively  in response to the learner’s previous
behavior.
Speciﬁcally  the algorithm applies to L1 loss and L2 loss  and achieves regret O(pT n ) for both.
The online strategies we present enjoy several advantages over full state tomography  and even over
“state certiﬁcation”  in which we wish to test whether a given quantum state is close to a desired
state or far from it. Optimal algorithms for state tomography (O’Donnell and Wright [2016]  Haah
et al. [2017]) or certiﬁcation (B˘adescu et al. [2017]) require joint measurements of an exponential
number of copies of the quantum state  and assume the ability to perform noiseless  universal quantum
computation. On the other hand  the algorithms implicit in Theorems 1 and 2 involve only single-copy
measurements  allow for noisy measurements  and capture ground reality more closely. They produce
a hypothesis state that mimics the unknown state with respect to measurements that can be performed
in a given experimental set-up  and the accuracy of prediction improves as the set of available
measurements grows. For example  in the realizable case  i.e.  when the data arise from an actual

quantum state  the average L1 loss per iteration is O(pn/T ). This tends to zero  as the number of
measurements becomes large. Note that L1 loss may be as large as 1/2 per iteration in the worst
case  but this occurs at most O(pnT ) times. Finally  the algorithms have run time exponential in the
number of qubits in each iteration  but are entirely classical. Exponential run time is unavoidable  as
the measurements are presented explicitly as 2n ⇥ 2n matrices  where n is the number of qubits. If
we were required to output the hypothesis states  the length of the output—also exponential in the
number of qubits—would again entail exponential run time.
It is natural to wonder whether Theorems 1 and 2 leave any room for improvement. Theorem 1 is
asymptotically optimal in its mistake bound of O(n/"2); this follows from the property that n-qubit
quantum states  considered as a hypothesis class  have "-fat-shattering dimension ⇥(n/"2) (see  for
example  Aaronson [2007]). On the other hand  there is room to improve Theorem 2. The bounds
of which we are aware are ⌦(pT n ) for the L1 loss (see  e.g.  [Arora et al.  2012  Theorem 4.1]) in
the non-realizable case and ⌦(n) for the L2 loss in the realizable case  when the feedback consists
of the measurement outcomes. (The latter bound  as well as an ⌦(pT n ) bound for L1 loss in the
same setting  come from considering quantum mixed states that consist of n independent classical
coins  each of which could land heads with probability either 1/2 or 1/2 + ". The paramater " is set
topn/T .)

3

We mention an application of Theorem 1  that appears in simultaneous work. Aaronson [2018]
has given an algorithm for the so-called shadow tomography problem. Here we have an unknown
D-dimensional pure state ⇢  as well as known two-outcome measurements E1  . . .   Em. Our goal is
to approximate Tr(Ei⇢)  for every i  to within additive error ". We would like to do this by measuring
⇢⌦k  where k is as small as possible. Surprisingly  Aaronson [2018] showed that this can be achieved

both D and M. One component of his algorithm is essentially tantamount to online learning with

using Theorem 1 from this paper in a black-box manner  we can improve the sample complexity of

with k = eO((log M )4(log D)/"5)  that is  a number of copies of ⇢ that is only polylogarithmic in
eO(n/"3) mistakes—i.e.  the learning algorithm we present in Section 4 of this paper. However  by
shadow tomography toeO((log M )4(log D)/"4). Details appear in (Aaronson [2018]).

To maximize insight  in this paper we give three very different approaches to proving Theorems 1
and 2 (although we do not prove every statement with all three approaches). Our ﬁrst approach is to
adapt techniques from online convex optimization to the setting of density matrices  which in general
may be over a complex Hilbert space. This requires extending standard techniques to cope with
convexity and Taylor approximations  which are widely used for functions over the real domain  but
not over the complex domain. We also give an efﬁcient iterative algorithm to produce predictions.
This approach connects our problem to the modern mainstream of online learning algorithms  and
achieves the best parameters (as stated in Theorems 1 and 2).
Our second approach is via a postselection-based learning procedure  which starts with the maximally
mixed state as a hypothesis and then repeatedly reﬁnes it by simulating postselected measurements.
This approach builds on earlier work due to Aaronson [2005]  speciﬁcally the proof of BQP/qpoly ✓
PP/poly. The advantage is that it is almost entirely self-contained  requiring no “power tools” from
convex optimization or learning theory. On the other hand  the approach does not give optimal
parameters  and we do not know how to prove Theorem 2 with it.
Our third approach is via an upper-bound on the so-called sequential fat-shattering dimension of
quantum states  considered as a hypothesis class (see  e.g.  Rakhlin et al. [2015]). In the original
quantum PAC-learning theorem by Aaronson  the key step was to upper-bound the so-called "-fat-
shattering dimension of quantum states considered as a hypothesis class. Fat-shattering dimension is
a real-valued generalization of VC dimension. One can then appeal to known results to get a sample-
efﬁcient learning algorithm. For online learning  however  bounding the fat-shattering dimension
no longer sufﬁces; one instead needs to consider a possibly-larger quantity called sequential fat-
shattering dimension. However  by appealing to a lower bound due to Nayak [1999]  Ambainis et al.
[2002] for a variant of quantum random access codes  we are able to upper-bound the sequential
fat-shattering dimension of quantum states. Using known results—in particular  those due to Rakhlin
et al. [2015]—this implies the regret bound in Theorem 2  up to a multiplicative factor of log3/2 T .
The statement that the hypothesis class of n-qubit states has "-sequential fat-shattering dimension
O(n/"2) might be of independent interest: among other things  it implies that any online learning
algorithm that works given bounded sequential fat-shattering dimension  will work for online learning
of quantum states. We also give an alternative proof for the lower bound due to Nayak for quantum
random access codes  and extend it to codes that are decoded by what we call measurement decision
trees. We expect these also to be of independent interest.

1.1 Structure of the paper

We start by describing background and the technical learning setting as well as notations used
throughout (Section 2). In Section 3 we give the algorithms and main theorems derived using
convexity arguments and online convex optimization. In Section 4 we state the main theorem using
a postselection algorithm.
In Section 5 we give a sequential fat-shattering dimension bound for
quantum states and its implication for online learning of quantum states. Proofs of the theorems and
related claims are presented in the appendices.

2 Preliminaries and deﬁnitions
We deﬁne the trace norm of a matrix M as kMkTr := TrpM M†  where M† is the adjoint of M. We
denote the ith eigenvalue of a Hermitian matrix X by i(X)  its minimum eigenvalue by min(X) 
and its maximum eigenvalue by max(X). We sometimes use the notation X • Y to denote the trace

4

inner-product Tr(X†Y ) between two complex matrices of the same dimensions. By ‘log’ we denote
the natural logarithm  unless the base is explicitly mentioned.
An n-qubit quantum state ⇢ is an element of Cn  where Cn is the set of all trace-1 positive semi-
deﬁnite (PSD) complex matrices of dimension 2n:

Cn = {M 2 C2n⇥2n

  M = M†   M ⌫ 0   Tr(M ) = 1} .

Note that Cn is a convex set. A two-outcome measurement of an n-qubit state is deﬁned by a 2n ⇥ 2n
Hermitian matrix E with eigenvalues in [0  1]. The measurement E “accepts” ⇢ with probability
Tr(E⇢)  and “rejects” with probability 1  Tr(E⇢). For the algorithms we present in this article 
we assume that a two-outcome measurement is speciﬁed via a classical description of its deﬁning
matrix E. In the rest of the article  unless mentioned otherwise  a “measurement” refers to a “two-
outcome measurement”. We refer the reader to the book by Watrous [2018] for a more thorough
introduction to the relevant concepts from quantum information.

Online learning and regret.
In online learning of quantum states  we have a sequence of itera-
tions t = 1  2  3  . . . of the following form. First  the learner constructs a state !t 2 Cn; we say that
the learner “predicts” !t. It then suffers a “loss” `t(Tr(Et!t)) that depends on a measurement Et 
both of which are presented by an adversary. Commonly used loss functions are L2 loss (also called
“mean square error”)  given by

and L1 loss (also called “absolute loss”)  given by

`t(z) := (z  bt)2  

`t(z) := |z  bt|

 

where bt 2 [0  1]. The parameter bt may be an approximation of Tr(Et⇢) for some ﬁxed quantum
state ⇢ not known to the learner  obtained by measuring multiple copies of ⇢. However  in general 
the parameter is allowed to be arbitrary.
The learner then “observes” feedback from the measurement Et; the feedback is also provided by the
adversary. The simplest feedback is the realization of a binary random variable Yt such that

Yt =⇢ 1 with probability Tr(Et⇢)  
0 with probability 1  Tr(Et⇢) .

and

Another common feedback is a number bt as described above  especially in case that the learner
suffers L1 or L2 loss.
We would like to design a strategy for updating !t based on the loss  measurements  and feedback in
all the iterations so far  so that the learner’s total loss is minimized in the following sense. We would
like that over T iterations (for a number T known in advance)  the learner’s total loss is not much
more than that of the hypothetical strategy of outputting the same quantum state ' at every time step 
where ' minimizes the total loss with perfect hindsight. Formally this is captured by the notion of
regret RT   deﬁned as

RT :=

`t(Tr(Et!t))  min
'2Cn

TXt=1

`t(Tr(Et')) .

TXt=1

The sequence of measurements Et can be arbitrary  even adversarial  based on the learner’s previous
actions. Note that if the loss function is given by a ﬁxed state ⇢ (as in the case of mean square error) 
the minimum total loss would be 0. This is called the “realizable” case. However  in general  the loss
function presented by the adversary need not be consistent with any quantum state. This is called the
“non-realizable” case.
A special case of the online learning setting is called agnostic learning; here the measurements Et
are drawn from a ﬁxed and unknown distribution D. The setting is called “agnostic” because we still
do not assume that the losses correspond to any actual state ⇢ (i.e.  the setting may be non-realizable).

Online mistake bounds.
In some online learning scenarios the quantity of interest is not the mean
square error  or some other convex loss  but rather simply the total number of “mistakes” made. For
example  we may be interested in the number of iterations in which the predicted probability of

5

acceptance Tr(Et!t) is more than "-far from the actual value Tr(Et⇢)  where ⇢ is again a ﬁxed state
not known to the learner. More formally  let

`t(Tr(Et!t)) := | Tr(Et!t)  Tr(Et⇢)|

be the absolute loss function. Then the goal is to bound the number of iterations in which
`t(Tr(Et!t)) >"   regardless of the sequence of measurements Et presented by the adversary.
We assume that in this setting the adversary provides as feedback an approximation bt 2 [0  1] that
satisﬁes | Tr(Et⇢)  bt| "
3.
3 Online learning of quantum states

In this section  we use techniques from online convex optimization to minimize regret. The same
algorithms may be adapted to also minimize the number of mistakes made.

3.1 Regularized Follow-the-Leader

We ﬁrst follow the template of the Regularized Follow-the-Leader algorithm (RFTL; see  for example 
[Hazan  2015  Chapter 5]). The algorithm below makes use of von Neumann entropy  which relates
to the Matrix Exponentiated Gradient algorithm (Tsuda et al. [2005]).

Algorithm 1 RFTL for Quantum Tomography
1: Input: T   K := Cn  ⌘< 1
2: Set !1 := 2n .
3: for t = 1  . . .   T do
4:

2

Predict !t. Consider the convex and L-Lipschitz loss function `t : R ! R given by measure-
ment Et : `t(Tr(Et')). Let `0t(x) be a sub-derivative of `t with respect to x. Deﬁne

5:

Update decision according to the RFTL rule with von Neumann entropy:

rt := `0t(Tr(Et!t))Et .
tXs=1

Tr(rs') +

2nXi=1

'2K (⌘

!t+1 := arg min

i(') log i(')) .

(1)

6: end for

Remark 1: The mathematical program in Eq. (1) is convex  and thus can be solved in polynomial
time in the dimension  which is 2n.

Theorem 3. Setting ⌘ =q (log 2)n
  the regret of Algorithm 1 is bounded by 2Lp(2 log 2)T n .
Remark 2:
In the case where the feedback is an independent random variable Yt  where Yt = 0
with probability 1  Tr(Et⇢) and Yt = 1 with probability Tr(Et⇢) for a ﬁxed but unknown state ⇢ 
we deﬁne rt in Algorithm 1 as rt := 2(Tr(Et!t)  Yt)Et. Then E[rt] is the gradient of the L2
loss function where we receive precise feedback Tr(Et⇢) instead of Yt. It follows from the proof of
Theorem 3 that the expected L2 regret of Algorithm 1  namely

2T L2

E" TXt=1

(Tr(Et!t)  Tr(Et⇢))2#  

is bounded by O(pT n ).
The proof of Theorem 3 appears in Appendix B. The proof is along the lines of [Hazan  2015 
Theorem 5.2]  except that the loss function does not take a raw state as input  and our domain for
optimization is complex. Therefore  the mean value theorem does not hold  which means we need
to approximate the Bregman divergence instead of replacing it by a norm as in the original proof.
Another subtlety is that convexity needs to be carefully deﬁned with respect to the complex domain.

6

3.2 Matrix Multiplicative Weights

The Matrix Multiplicative Weights (MMW) algorithm [Arora and Kale  2016] provides an alternative
means of proving Theorem 2. The algorithm follows the template of Algorithm 1 with step 5 replaced
by the following update rule:

!t+1 :=

.

(2)

LPt
exp( ⌘
LPt
Tr(exp( ⌘

⌧ =1 r⌧ )
⌧ =1 r⌧ ))

In the notation of Arora and Kale [2016]  this algorithm is derived using the loss matrices Mt =
L `0t(Tr(Et!t))Et. Since kEtk  1 and |`0t(Tr(Et!t))| L  we have kMtk  1  as
Lrt = 1
1
requred in the analysis of the Matrix Multiplicative Weights algorithm. We have the following regret
bound for the algorithm (proved in Appendix C):
Theorem 4. Setting ⌘ = q (log 2)n
bounded by 2Lp(log 2)T n.

  the regret of the algorithm based on the update rule (2) is

4T

3.3 Proof of Theorem 1

Consider either the RFTL or MMW based online learning algorithm described in the previous
subsections  with the 1-Lipschitz convex absolute loss function `t(x) = |x  bt|. We run the
algorithm in a sub-sequence of the iterations  using only the measurements presented in those
iterations. The subsequence of iterations is determined as follows. Let !t denote the hypothesis
maintained by the algorithm in iteration t. We run the algorithm in iteration t if `t(Tr(Et!t)) > 2"
3 .
Note that whenever | Tr(Et!t)  Tr(Et⇢)| >"   we have `t(Tr(Et!t)) > 2"
3   so we update the
hypothesis according to the RFTL/MMW rule in that iteration.
As we explain next  the algorithm makes at most O( n
"2 ) updates regardless of the number of measure-
ments presented (i.e.  regardless of the number of iterations)  giving the required mistake bound. For
the true quantum state ⇢  we have `t(Tr(Et⇢)) < "
3 for all t. Thus if the algorithm makes T updates
3 T +O(pT n ).
(i.e.  we run the algorithm in T of the iterations)  the regret bound implies that 2"
Simplifying  we get the bound T = O( n

3 T  "

"2 )  as required.

4 Learning Using Postselection

In this section  we give a direct route to proving a slightly weaker version of Theorem 1: one that
does not need the tools of convex optimization  but only tools intrinsic to quantum information.
In the following  by a “register” we mean a designated sequence of qubits. Given a two-outcome
measurement E on n-qubits states  we deﬁne an operator M that “postselects” on acceptance
by E. (While a measurement results in a random outcome distributed according to the probability
of acceptance or rejection  postselection is a hypothetical operation that produces an outcome of
one’s choice with certainty.) Let U be any unitary operation on n + 1 qubits that maps states of

the form | i|0i to pE | i|0i + p  E | i|1i. Such a unitary operation always exists (see  e.g. 
[Watrous  2018  Theorem 2.42]). Denote the (n + 1)th qubit by register B. Let ⇧ := ⌦| 0ih0| be
the orthogonal projection onto states that equal |0i in register B. Then we deﬁne the operator M as
(3)

1

M(') :=

Tr(E')

TrBU1⇧U (' ⌦| 0ih0|) U1⇧U  

if Tr(E') 6= 0  and M(') := 0 otherwise. Here  TrB is the partial trace operator over qubit B [Wa-
trous  2018  Section 1.1]. This operator M has the effect of mapping the quantum state ' to the
(normalized) post-measurement state when we perform the measurement E and get outcome “yes”
(i.e.  the measurement “accepts”). We emphasize that we use a fresh ancilla qubit initialized to
state |0i as register B in every application of the operator M. We say that the postselection succeeds
with probability Tr(E').
We need a slight variant of a well-known result  which Aaronson called the “Quantum Union Bound”
(see  for example  Aaronson [2006  2016]  Wilde [2013]).

7

Theorem 5 (variant of Quantum Union Bound; Gao [2015]). Suppose we have a sequence of two-
outcome measurements E1  . . .   Ek  such that each Ei accepts a certain mixed state ' with probability
at least 1". Consider the corresponding operators M1 M2  . . .  Mk that postselect on acceptance
by the respective measurements E1  E2  . . .   Ek. Let e' denote the state (MkMk1 ···M1)(')
obtained by applying each of the k postselection operations in succession. Then the probability that
all the postselection operations succeed  i.e.  the k measurements all accept '  is at least 1  2pk".
Moreover  ke'  'kTr  4pk".

We may infer the above theorem by applying Theorem 1 from (Gao [2015]) to the state ' augmented
with k ancillary qubits B1  B2  . . .   Bk initialized to 0  and considering k orthogonal projection
operators U1
i ⇧iUi  where the unitary operator Ui and the projection operator ⇧i are as deﬁned for
the postselection operation Mi for Ei. The ith projection operator U1
i ⇧iUi acts on the registers
holding ' and the ith ancillary qubit Bi.
We prove the main result of this section using suitably deﬁned postselection operators in an online
learning algorithm (proof in Appendix D):
Theorem 6. Let ⇢ be an unknown n-qubit mixed state  let E1  E2  . . . be a sequence of two-outcome
measurements  and let "> 0. There exists a strategy for outputting hypothesis states !0 ! 1  . . . 
where !t depends only on E1  . . .   Et and real numbers b1  . . .   bt in [0  1]  such that as long as
|bt  Tr(Et⇢)| "/3 for every t  we have
for at most O n

" values of t. Here the Et’s and bt’s can otherwise be chosen adversarially.

|Tr(Et+1!t)  Tr(Et+1⇢)| >"

"3 log n

5 Learning Using Sequential Fat-Shattering Dimension

In this section  we prove regret bounds using the notion of sequential fat-shattering dimension. Let
S be a set of functions f : U ! [0  1]  and "> 0. Then  following Rakhlin et al. [2015]  let the
"-sequential fat-shattering dimension of S  or sfat"(S)  be the largest k for which we can construct a
complete binary tree T of depth k  such that

• each internal vertex v 2 T has associated with it a point xv 2 U and a real av 2 [0  1]  and
• for each leaf vertex v 2 T there exists an f 2 S that causes us to reach v if we traverse T
from the root such that at any internal node w we traverse the left subtree if f (xw)  aw  "
and the right subtree if f (xw)  aw +". If we view the leaf v as a k-bit string  the function f
is such that for all ancestors u of v  we have f (xu)  au  " if vi = 0  and f (xu)  au + "
if vi = 1  when u is at depth i  1 from the root.

An n-qubit state ⇢ induces a function f on the set of two-outcome measurements E deﬁned as f (E) :=
Tr(E⇢). With this correspondence in mind  we establish a bound on the sequential fat-shattering
dimension of the set of n-qubit quantum states. The bound is based on a generalization of “random
access coding” (Nayak [1999]  Ambainis et al. [2002]) called “serial encoding”. We derive the
following bound on the length of serial encoding. Let H(x) := x log2 x  (1  x) log2(1  x) be
the binary entropy function.
Corollary 7. Let k and n be positive integers. For each k-bit string y := y1 ··· yk  let ⇢y be an n-
qubit mixed state such that for each i 2{ 1  2  . . .   k}  there is a two-outcome measurement E0 that
depends only on i and the preﬁx v := y1y2 ··· yi1  and has the following properties

(iii) if yi = 0 then Tr(E0⇢y)  av  "  and
(iv) if yi = 1 then Tr(E0⇢y)  av + " 

where " 2 (0  1/2] and av 2 [0  1] is a “pivot point” associated with the preﬁx v. Then

n  ✓1  H✓ 1  "

2 ◆◆ k .

In particular  k = On/"2.

8

(The proof is presented in Appendix E).
Corollary 7 immediately implies the following theorem:
Theorem 8. Let U be the set of two-outcome measurements E on an n-qubit state  and let S be the
set of all functions f : U ! [0  1] that have the form f (E) := Tr(E⇢) for some ⇢. Then for all
"> 0  we have sfat"(S) = On/"2.

Theorem 8 strengthens an earlier result due to Aaronson [2007]  which proved the same upper
bound for the “ordinary” (non-sequential) fat-shattering dimension of quantum states considered as a
hypothesis class.
Now we may use existing results from the literature  which relate sequential fat-shattering dimension
to online learnability. In particular  in the non-realizable case  Rakhlin et al. [2015] recently showed
the following:
Theorem 9 (Rakhlin et al. [2015]). Let S be a set of functions f : U ! [0  1] and for every
integer t  1  let `t : [0  1] ! R be a convex  L-Lipschitz loss function. Suppose we are sequentially
presented elements x1  x2  . . . 2 U  with each xt followed by the loss function `t. Then there exists a
learning strategy that lets us output a sequence of hypotheses f1  f2  . . . 2 S  such that the regret is
upper-bounded as:
TXt=1

`t (ft(xt))  min
f2S

↵ (4↵ +

`t (f (xt)) + 2LT inf

12

pT Z 1

↵ ssfat(S) log✓ 2eT

 ◆d) .

TXt=1

This follows from Theorem 8 in (Rakhlin et al. [2015]) as in the proof of Proposition 9 in the same
article.
Combining Theorem 8 with Theorem 9 gives us the following:
Corollary 10. Suppose we are presented with a sequence of two-outcome measurements E1  E2  . . .
of an n-qubit state  with each Et followed by a loss function `t as in Theorem 9. Then there exists a
learning strategy that lets us output a sequence of hypothesis states !1 ! 2  . . . such that the regret
after the ﬁrst T iterations is upper-bounded as:

TXt=1

`t (Tr(Et!t))  min
!2Cn

TXt=1

`t (Tr(Et!)) + O⇣LpnT log3/2 T⌘ .

Note that the result due to Rakhlin et al. [2015] is non-explicit. In other words  by following this
approach  we do not derive any speciﬁc online learning algorithm for quantum states that has the
stated upper bound on regret; we only prove non-constructively that such an algorithm exists.
We expect that the approach in this section  based on sequential fat-shattering dimension  could also
be used to prove a mistake bound for the realizable case  but we leave that to future work.

6 Open Problems

We conclude with some questions arising from this work. The regret bound established in Theorem 2
for L1 loss is tight. Can we similarly achieve optimal regret for other loss functions of interest  for
example for L2-loss? It would also be interesting to obtain regret bounds in terms of the loss of the
best quantum state in hindsight  as opposed to T (the number of iterations)  using the techniques in
this article. Such a bound has been shown by [Tsuda et al.  2005  Lemma 3.2] for L2-loss using the
Matrix Exponentiated Gradient method.
In what cases can one do online learning of quantum states  not only with few samples  but also with
a polynomial amount of computation? What is the tight generalization of our results to measurements
with d outcomes? Is it the case  in online learning of quantum states  that any algorithm works  so
long as it produces hypothesis states that are approximately consistent with all the data seen so far?
Note that none of our three proof techniques seem to imply this general conclusion.

References
S. Aaronson. Limitations of quantum advice and one-way communication. Theory of Computing  1:

1–28  2005. Earlier version in CCC’2004. quant-ph/0402095.

9

S. Aaronson. QMA/qpoly is contained in PSPACE/poly: de-Merlinizing quantum protocols. In Proc.

Conference on Computational Complexity  pages 261–273  2006. quant-ph/0510230.

S. Aaronson. The learnability of quantum states. Proc. Roy. Soc. London  A463(2088):3089–3114 

2007. quant-ph/0608142.

S. Aaronson. The complexity of quantum states and transformations: From quantum money to
black holes  February 2016. Lecture Notes for the 28th McGill Invitational Workshop on Compu-
tational Complexity  Holetown  Barbados. With guest lectures by A. Bouland and L. Schaeffer.
www.scottaaronson.com/barbados-2016.pdf.

S. Aaronson. Shadow tomography of quantum states. In Proc. ACM STOC  STOC 2018  pages

325–338  New York  NY  USA  2018. ACM. ISBN 978-1-4503-5559-9. arXiv:1711.01053.

S. Aaronson and A. Drucker. A full characterization of quantum advice. SIAM J. Comput.  43(3):

1131–1183  2014. Earlier version in STOC’2010. arXiv:1004.0377.

A. Ambainis  A. Nayak  A. Ta-Shma  and U. V. Vazirani. Quantum dense coding and quantum ﬁnite
automata. J. of the ACM  49:496–511  2002. Combination of an earlier version in STOC’1999  pp.
376-383  arXiv:quant-ph/9804043 and (Nayak [1999]).

S. Arora and S. Kale. A combinatorial  primal-dual approach to semideﬁnite programs. J. ACM  63

(2):12:1–12:35  2016.

S. Arora  E. Hazan  and S. Kale. The multiplicative weights update method: a meta-algorithm and

applications. Theory of Computing  8(1):121–164  2012.

K. M. R. Audenaert and J. Eisert. Continuity bounds on the quantum relative entropy. Journal of

Mathematical Physics  46(10):102104  2005. arXiv:quant-ph/0503218.

C. B˘adescu  R. O’Donnell  and J. Wright. Quantum state certiﬁcation. Technical Report

arXiv:1708.06002 [quant-ph]  arXiv.org  2017.

Rajendra Bhatia. Matrix Analysis  volume 169 of Graduate Texts in Mathematics. Springer-Verlag 

New York  1997.

E. A. Carlen and E. H. Lieb. Remainder terms for some quantum entropy inequalities. Journal of

Mathematical Physics  55(4)  2014. arXiv:1402.3840.

J. Gao. Quantum union bounds for sequential projective measurements. Phys. Rev. A  92:052331 

Nov 2015.

J. Haah  A. W. Harrow  Z. Ji  X. Wu  and N. Yu. Sample-optimal tomography of quantum states.

IEEE Transactions on Information Theory  63(9):5628–5641  Sept 2017.

E. Hazan. Introduction to Online Convex Optimization  volume 2 of Foundations and Trends in

Optimization. 2015.

A. Nayak. Optimal lower bounds for quantum automata and random access codes. In Proc. IEEE

FOCS  pages 369–376  1999. quant-ph/9904093.

R. O’Donnell and J. Wright. Efﬁcient quantum tomography. In Proceedings of the Forty-eighth
Annual ACM Symposium on Theory of Computing  STOC ’16  pages 899–912  New York  NY 
USA  2016. ACM.

A. Rakhlin  K. Sridharan  and A. Tewari. Online learning via sequential complexities. The Journal of

Machine Learning Research  16(1):155–186  2015.

A. Rocchetto. Stabiliser states are efﬁciently PAC-learnable. arXiv:1705.00345  2017.
A. Rocchetto  S. Aaronson  S. Severini  G. Carvacho  D. Poderini  I. Agresti  M. Bentivegna  and

F. Sciarrino. Experimental learning of quantum states. arXiv:1712.00127  2017.

K. Tsuda  G. Rätsch  and M. K. Warmuth. Matrix exponentiated gradient updates for on-line learning

and Bregman projection. Journal of Machine Learning Research  6:995–1018  2005.

10

J. Watrous. Theory of Quantum Information. Cambridge University Press  May 2018.
M. Wilde. Sequential decoding of a general classical-quantum channel. Proc. Roy. Soc. London 

A469(2157):20130259  2013. arXiv:1303.0808.

11

,Scott Aaronson
Xinyi Chen
Elad Hazan
Satyen Kale
Ashwin Nayak
Ruoxi Sun
Scott Linderman
Ian Kinsella
Liam Paninski