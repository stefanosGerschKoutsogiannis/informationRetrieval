2012,Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs,Graphical model selection refers to the problem of estimating the unknown graph structure given observations at the nodes in the model. We consider a challenging instance of this problem when some of the nodes are latent or hidden.  We  characterize  conditions for tractable graph estimation and develop efficient methods with provable guarantees. We consider the class of Ising models Markov on  locally tree-like graphs  which are in the regime of correlation decay. We  propose an efficient method for graph estimation  and establish its structural consistency when the number of samples $n$ scales as $n = \Omega(\theta_{\min}^{-\delta \eta(\eta+1)-2}\log p)$  where $\theta_{\min}$ is the minimum edge potential  $\delta$ is the depth (i.e.  distance from a hidden node to the nearest  observed nodes)  and $\eta$ is a parameter which depends on the minimum and maximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides  flexibility to control  the number of latent variables and the cycle lengths in the output graph.  We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound  on sample requirements.,Latent Graphical Model Selection: Efﬁcient Methods

for Locally Tree-like Graphs

Animashree Anandkumar

UC Irvine

a.anandkumar@uci.edu

Ragupathyraj Valluvan

UC Irvine

rvalluva@uci.edu

Abstract

Graphical model selection refers to the problem of estimating the unknown graph
structure given observations at the nodes in the model. We consider a challenging
instance of this problem when some of the nodes are latent or hidden. We char-
acterize conditions for tractable graph estimation and develop efﬁcient methods
with provable guarantees. We consider the class of Ising models Markov on lo-
cally tree-like graphs  which are in the regime of correlation decay. We propose
an efﬁcient method for graph estimation  and establish its structural consistency
when the number of samples n scales as n = Ω(θ
log p)  where θmin
is the minimum edge potential  δ is the depth (i.e.  distance from a hidden node to
the nearest observed nodes)  and η is a parameter which depends on the minimum
and maximum node and edge potentials in the Ising model. The proposed method
is practical to implement and provides ﬂexibility to control the number of latent
variables and the cycle lengths in the output graph. We also present necessary
conditions for graph estimation by any method and show that our method nearly
matches the lower bound on sample requirements.

−δη(η+1)−2
min

Keywords: Graphical model selection  latent variables  quartet methods  locally tree-like graphs.

1

Introduction

It is widely recognized that the process of ﬁtting observed data to a statistical model needs to in-
corporate latent or hidden factors  which are not directly observed. Learning latent variable models
involves mainly two tasks: discovering structural relationships among the observed and hidden vari-
ables  and estimating the strength of such relationships. One of the simplest models is the latent
class model (LCM)  which incorporates a single hidden variable and the observed variables are
conditionally independent given the hidden variable. Latent tree models extend this model class
to incorporate many hidden variables in a hierarchical fashion. Latent trees have been effective in
modeling data in a variety of domains  such as phylogenetics [1]. Their computational tractability:
upon learning the latent tree model  enables the inference to be carried out efﬁciently through belief
propagation. There has been extensive work on learning latent trees  including some of the recent
works  e.g. [2–4]  demonstrate efﬁcient learning in high dimensions. However  despite the advan-
tages  the assumption of an underlying tree structure may be too restrictive. For instance  consider
the example of topic-word models  where topics (which are hidden) are discovered using informa-
tion about word co-occurrences. In this case  a latent tree model does not accurately represent the
hierarchy of topics and words  since there are many common words across different topics. Here  we
relax the latent tree assumption to incorporate cycles in the latent graphical model while retaining
many advantages of latent tree models  including tractable learning and inference. Relaxing the tree
constraint leads to many challenges: in general  learning these models is NP-hard  even when there
are no latent variables  and developing tractable methods for such models is itself an area of active
research  e.g. [5–7]. We consider structure estimation in latent graphical models Markov on locally

1

tree-like graphs. These extensions of latent tree models are relevant in many settings: for instance 
when there is a small overlap among different hierarchies of variables  the resulting graph has mostly
long cycles. There are many questions to be addressed: are there parameter regimes where these
models can be learnt consistently and efﬁciently? If so  are there practical learning algorithms?
Are learning guarantees for loopy models comparable to those for latent trees? How does learning
depend on various graph attributes such as node degrees  girth of the graph  and so on?

Our Approach: We consider learning Ising models with latent variables Markov on locally tree-
like graphs. We assume that the model parameters are in the regime of correlation decay. In this
regime  there are no long-range correlations  and the local statistics converge to a tree limit. Hence 
we can employ the available latent tree methods to learn “local” subgraphs consistently  as long as
they do not contain any cycles. However  merging these estimated local subgraphs (i.e.  latent trees)
remains a non-trivial challenge. It is not clear whether an efﬁcient approach is possible for matching
latent nodes during this process. We employ a different philosophy for building locally tree-like
graphs with latent variables. We decouple the process of introducing cycles and latent variables in
the output model. We initialize a loopy graph consisting of only the observed variables  and then
iteratively add latent variables to local neighborhoods of the graph. We establish correctness of our
method under a set of natural conditions. We establish that our method is structurally consistent
when the number of samples n scales as n = Ω(θ
log p)  where p is the number of
observed variables  θmin is the minimum edge potential  δ is the depth (i.e.  graph distance from a
hidden node to the nearest observed nodes)  and η is a parameter which depends on the minimum
and maximum node and edge potentials of the Ising model (η = 1 for homogeneous models).
The sample requirement for our method is comparable to the requirement for many popular latent
tree methods  e.g. [2–4]. Moreover  note that when there are no hidden variables (δ = 1)  the
sample complexity of our method is strengthened to n = Ω(θ−2
min log p)  which matches with the
sample complexity of existing algorithms for learning fully-observed Ising models [5–7]. Thus  we
present an efﬁcient method which bridges structure estimation in latent trees with estimation in fully
observed loopy graphical models. Finally  we present necessary conditions for graph estimation by
any method and show that our method nearly matches the lower bound. Our method has a number
of attractive features: it is amenable to parallelization making it efﬁcient on large datasets  provides
ﬂexibility to control the length of cycles and the number of latent variables in the output model  and
it can incorporate penalty scores such as the Bayesian information criterion (BIC) [8] to tradeoff
model complexity and ﬁdelity. Preliminary experiments on the newsgroup dataset suggests that
the method can discover intuitive relationships efﬁciently  and also compares well with the popular
latent Dirichlet allocation (LDA) [9] in terms of topic coherence and perplexity.

−δη(η+1)−2
min

Related Work: Learning latent trees has been studied extensively  mainly in the context of phy-
logenetics. Efﬁcient algorithms with provable guarantees are available (e.g. [2–4]). Our proposed
method for learning loopy models is inspired by the efﬁcient latent tree learning algorithm of [4].
Works on high-dimensional graphical model selection are more recent. They can be mainly classi-
ﬁed into two groups: non-convex local approaches [5  6  10] and those based on convex optimiza-
tion [7  11  12]. There is a general agreement that the success of these methods is related to the
presence of correlation decay in the model [13]. This work makes the connection explicit: it re-
lates the extent of correlation decay with the learning efﬁciency for latent models on large girth
graphs. An analogous study of the effect of correlation decay for learning fully observed models
is presented in [5]. This paper is the ﬁrst work to provide provable guarantees for learning discrete
graphical models on loopy graphs with latent variables (which can also be easily extended to Gaus-
sian models  see Remark following Theorem 1). The work in [12] considers learning latent Gaussian
graphical models using a convex relaxation method  by exploiting a sparse-low rank decomposition
of the Gaussian precision matrix. However  the method cannot be easily extended to discrete mod-
els. Moreover  the “incoherence” conditions required for the success of convex methods are hard to
interpret and verify in general. In contrast  our conditions for success are transparent and based on
the presence of correlation decay in the model.

2 System Model

Ising Models: A graphical model is a family of multivariate distributions Markov in accordance
to a ﬁxed undirected graph [14]. Each node in the graph i ∈ W is associated to a random variable Xi

2

(cid:32)(cid:88)

e∈E

(cid:88)

i∈V

(cid:33)

P (xW ) = exp

θi jxixj +

φixi − A(θ)

 

(2)

taking value in a set X . The set of edges E captures the set of conditional independence relations
among the random variables. We say that a set of random variables XW := {Xi  i ∈ W} with
probability mass function (pmf) P is Markov on the graph G if
P (xi|xN (i)) = P (xi|xW\i)

(1)
holds for all nodes i ∈ W   where N (i) are the neighbors of node i in graph G. The Hammersley-
Clifford theorem [14] states that under the positivity condition  given by P (xW ) > 0  for all
xW ∈ X |W|  a distribution P satisﬁes the Markov property according to a graph G iff. it factorizes
according to the cliques of G. A special case of graphical models is the class of Ising models  where
each node consists of a binary variable over {−1  +1} and there are only pairwise interactions in
the model. In this case  the joint distribution factorizes as

where θ := {θi j} and φ := {φi} are known as edge and the node potentials  and A(θ) is known
as the log-partition function  which serves to normalize the probability distribution. We consider
latent graphical models in which a subset of nodes is latent or hidden. Let H ⊂ W denote the
hidden nodes and V ⊂ W denote the observed nodes. Our goal is to discover the presence of hidden
variables XH and learn the unknown graph structure G(W )  given n i.i.d. samples from observed
variables XV . Let p := |V | denote the number of observed nodes and m := |W| denote the total
number of nodes.

Tractable Models for Learning:
In general  structure estimation of graphical models is NP-hard.
We now characterize a tractable class of models for which we can provide guarantees on graph
estimation.

Girth-Constrained Graph Families: We consider the family of graphs with a bound on the girth 
which is the length of the shortest cycle in the graph. Let GGirth(m; g) denote the ensemble of
graphs with girth at most g. There are many graph constructions which lead to a bound on girth.
For example  the bipartite Ramanujan graph [15] and the random Cayley graphs [16] have bounds
on the girth. Theoretical guarantees for our learning algorithm will depend on the girth of the graph.
However  our experiments reveal that our method is able to construct models with short cycles as
well.

Regime of Correlation Decay: This work establishes tractable learning when the graphical model
converges locally to a tree limit. A sufﬁcient condition for the existence of such limits is the regime
of correlation decay  which refers to the property that there are no long-range correlations in the
model [5]. In this regime  the marginal distribution at a node is asymptotically independent of the
conﬁguration of a growing boundary. For the class of Ising models in (2)  the regime of correlation
decay can be explicitly characterized  in terms of the maximum edge potential θmax of the model
and the maximum node degree ∆max. Deﬁne α := ∆max tanh θmax. When α < 1  the model is in
the regime of correlation decay  and we provide learning guarantees in this regime.

3 Method  Guarantees and Necessary Conditions

Background on Learning Latent Trees: Most latent tree learning methods are distance based 
meaning they are based on the presence of an additive tree metric between any two nodes in the
tree model. For Ising model (and more generally  any discrete model)  the “information” distance
between any two nodes i and j in a tree T is deﬁned as

d(i  j; T ) := − log | det(Pi j)| 

(3)
where Pi j denotes the joint probability distribution between nodes i and j. On a tree model T   it can
be established that {d(i  j)} is additive along any path in T . Learning latent trees can thus be refor-

mulated as learning tree structure T given end-to-end (estimated) distances d := {(cid:98)d(i  j) : i  j ∈ V }

between the observed nodes V . Various methods with performance guarantees have been proposed 
e.g. [2–4]. They are usually based on local tests such as quartet tests  involving groups of four nodes.

3

In [4]  the so-called CLGrouping method is proposed  which organically grows the tree structure by
adding latent nodes to local neighborhoods. In the initial step  the method constructs the minimum
spanning tree MST(V ; d) over the observed nodes V using distances d. The method then iteratively
visits local neighborhoods of MST(V ; d) and adds latent nodes by conducting local distance tests.
Since a tree structure is maintained in every iteration of the algorithm  we can parsimoniously add
hidden variables by selecting neighborhoods which locally maximize scores such as the Bayesian
information criterion (BIC) [8]. This method also allows for fast implementation by parallelization
of latent tree reconstruction in different neighborhoods  see [17] for details.

(4)

(cid:98)dn(i  j; G) := − log | det((cid:98)P n
i j)| 

Proposed Algorithm: We now propose a method for learning loopy latent graphical models. As
in the case of latent tree methods  our method is also based on estimated information distances

i j denotes the empirical probability distribution at nodes i and j computed using n i.i.d.

where (cid:98)P n
samples. The presence of correlation decay in the Ising model implies that (cid:98)dn(i  j; G) is approx-
ods. However  the challenge is in merging these local estimates together to get a global estimate (cid:98)G:

imately a tree metric when nodes i and j are “close” on graph G (compared to the girth g of the
graph). Thus  intuitively  local neighborhoods of G can be constructed through latent tree meth-

∀i  j ∈ V 

the presence of latent nodes in the local estimates makes merging challenging. Moreover  such a
merging-based method cannot easily incorporate global penalties for the number of latent variables
added in the output model  which is relevant to obtain parsimonious representations on real datasets.
We overcome the above challenges as follows: our proposed method decouples the process of adding
latent variables to local neighborhoods. Given a parameter r > 0  for every node i ∈ V   consider

cycles and latent nodes to the output model. It initializes a loopy graph (cid:98)G0 and then iteratively adds
the set of nodes Br(i;(cid:98)dn) := {j : (cid:98)dn(i  j) < r}. The initial graph estimate (cid:98)G0 is obtained by taking
The method then adds latent variables by considering only local neighborhoods in (cid:98)G0 and running a
latent tree reconstruction routine. By visiting all the neighborhoods  a graph estimate (cid:98)G is obtained.

(cid:98)G0 ← ∪i∈V MST(Br(i;(cid:98)dn);(cid:98)dn).

the union of local minimum spanning trees:

Implementation details about the algorithm are available in [17]. We subsequently establish that
correctness of the proposed method under a set of natural conditions. We require that the parameter
r  which determines the set Br(i; d) for each node i  needs to be chosen as a function of the depth δ
(i.e.  distance from a hidden node to its closest observed nodes) and girth g of the graph. In practice 
the parameter r provides ﬂexibility in tuning the length of cycles added to the graph estimate. When
r is large enough  we obtain a latent tree  while for small r  the graph estimate can contain many
short cycles (and potentially many components). In experiments  we evaluate the performance of
our method for different values of r. For more details  see Section 4.

(5)

3.1 Conditions for Recovery

We present a set of natural conditions on the graph structure and model parameters under which our
proposed method succeeds in structure estimation.

and let

(A1) Minimum Degree of Latent Nodes: We require that all latent nodes have degree at least
three  which is a natural assumption for identiﬁability of hidden variables. Otherwise  the
latent nodes can be marginalized to obtain an equivalent representation of the observed
statistics.
(A2) Bounded Potentials: The edge potentials θ := {θi j} of the Ising model are bounded 

θmin ≤ |θi j| ≤ θmax 

∀ (i  j) ∈ G.

(6)

Similarly assume bounded node potentials.

(A3) Correlation Decay: As described in Section 2  we assume correlation decay in the Ising

model. We require

α := ∆max tanh θmax < 1 

αg/2

θη(η+1)+2
min

= o(1) 

(7)

4

where ∆max is the maximum node degree  g is the girth and θmin  θmax are the minimum
and maximum (absolute) edge potentials in the model.
(A4) Distance Bounds: We now deﬁne certain quantities which depend on the edge potential
bounds. Given an Ising model P with edge potentials θ = {θi j} and node potentials
φ = {φi}  consider its attractive counterpart ¯P with edge potentials ¯θ := {|θi j|} and node
potentials ¯φ := {|φi|}. Let φ(cid:48)
max := maxi∈V atanh(¯E(Xi))  where ¯E is the expectation
with respect to the distribution ¯P . Let P (X1 2;{θ  φ1  φ2}) denote an Ising model on two
nodes {1  2} with edge potential θ and node potentials {φ1  φ2}. Our learning guarantees
depend on dmin and dmax deﬁned below.

dmin :=− log|det P (X1 2;{θmax  φ(cid:48)

max  φ(cid:48)

dmax :=− log|det P (X1 2;{θmin  0  0})| 

max})| 
dmax
dmin

η :=

.

(A5) Girth vs. Depth: The depth δ characterizes how close the latent nodes are to observed
nodes on graph G: for each hidden node h ∈ H  ﬁnd a set of four observed nodes which
form the shortest quartet with h as one of the middle nodes  and consider the largest graph
distance in that quartet. The depth δ is the worst-case distance over all hidden nodes. We
require the following tradeoff between the girth g and the depth δ:

− δη (η + 1) = ω(1) 

g
4

Further  the parameter r in our algorithm is chosen as

r > δ (η + 1) dmax +  

for some  > 0 

dmin − r = ω(1).

g
4

(8)

(9)

(A1) is a natural assumption on the minimum degree of the hidden nodes for identiﬁability. (A2)
assumes bounds on the edge potentials. It is natural that the sample requirement of any graph es-
timation algorithm depends on the “weakest” edge characterized by the minimum edge potential
θmin. Further  the maximum edge potential θmax characterizes the presence/absence of long range
correlations in the model  and is made exact in (A3). Intuitively  there is a tradeoff between the
maximum degree ∆max and the maximum edge potential θmax of the model. Moreover  (A3) pre-
scribes that the extent of correlation decay be strong enough (i.e.  a small α and a large enough girth
g) compared to the weakest edge in the model. Similar conditions have been imposed before for
graphical model selection in the regime of correlation decay when there are no hidden variables [5].
(A4) deﬁnes certain distance bounds. Intuitively  dmin and dmax are bounds on information dis-
tances given by the local tree approximation of the loopy model. Note that e−dmax = Ω(θmin) and
e−dmin = O(θmax). (A5) provides the tradeoff between the girth g and the depth δ. Intuitively  the
depth needs to be smaller than the girth to avoid encountering cycles during the process of graph re-
construction. Recall that the parameter r in our algorithm determines the neighborhood over which
local MSTs are built in the ﬁrst step. It is chosen such that it is roughly larger than the depth δ in
order for all the hidden nodes to be discovered. The upper bound on r ensures that the distortion
from an additive metric is not too large. The parameters for latent tree learning routines (such as
conﬁdence intervals for quartet tests) are chosen appropriately depending on dmin and dmax  see [17]
for details.

3.2 Guarantees

We now provide the main result of this paper that the proposed method correctly estimates the graph
structure of a loopy latent graphical model in high dimensions. Recall that δ is the depth (distance
from a hidden node to its closest observed nodes)  θmin is the minimum (absolute) edge potential
and η = dmax
dmin

is the ratio of distance bounds.

Theorem 1 (Structural Consistency and Sample Requirements) Under (A1)–(A5)  the proba-
bility that the proposed method is structurally consistent tends to one  when the number of samples
scales as

(cid:16)

n = Ω

−δη(η+1)−2
θ
min

log p

.

(10)

(cid:17)

5

Thus  for learning Ising models on locally tree-like graphs  the sample complexity is dependent both
on the minimum edge potential θmin and on the depth δ. Our method is efﬁcient in high dimensions
since the sample requirement is only logarithmic in the number of nodes p.
Dependence on Maximum Degree: For the correlation decay to hold (A3)  we require θmin ≤
θmax = Θ(1/∆max). This implies that the sample complexity is at least n = Ω(∆δη(η+1)+2
log p).

max

Comparison with Fully Observed Models:
In the special case when all the nodes are observed1
(δ = 1)  we strengthen the results for our method and establish that the sample complexity is
n = Ω(θ−2
min log p). This matches the best known sample complexity for learning fully observed
Ising models [5  6].

Comparison with Learning Latent Trees: Our method is an extension of latent tree methods
for learning locally tree-like graphs. The sample complexity of our method matches the sample
requirements for learning general latent tree models [2–4]. Thus  we establish that learning locally
tree-like graphs is akin to learning latent trees in the regime of correlation decay.

Extensions: We strengthen the above results to provide non-asymptotic sample complexity
bounds and also consider general discrete models  see [17] for details. The above results can also
be easily extended to Gaussian models using the notion of walk-summability in place of correlation
decay (see [18]) and the negative logarithm of the correlation coefﬁcient as the additive tree metric
(see [4]).

Dependence on Fraction of Observed Nodes:
In the special case when a fraction ρ of the nodes
are uniformly selected as observed nodes  we can provide probabilistic bounds on the depth δ in the
resulting latent model  see [17] for details. For η = 1 (homogeneous models) and regular graphs

∆min = ∆max = ∆  the sample complexity simpliﬁes to n = Ω(cid:0)∆2ρ−2(log p)3(cid:1) . Thus  we can

characterize an explicit dependence on the fraction of observed nodes ρ.

3.3 Necessary Conditions for Graph Estimation

We have so far provided sufﬁcient conditions for recovering locally tree-like graphs in latent Ising
models. We now provide necessary conditions on the number of samples required by any algorithm
n i.i.d. samples from the observed node set V and Gm is the set of all possible graphs on m nodes.
We ﬁrst deﬁne the notion of the graph edit distance.

to reconstruct the graph. Let (cid:98)Gn : (X |V |)n → Gm denote any deterministic graph estimator using
Deﬁnition 1 (Edit Distance) Let G (cid:98)G be two graphs2 with adjacency matrices AG  A(cid:98)G  and let
between G (cid:98)G is deﬁned as

V be the set of labeled vertices in both the graphs (with identical labels). Then the edit distance

dist((cid:98)G  G; V ) := min

π

||A(cid:98)G − π(AG)||1 

where π is any permutation on the unlabeled nodes while keeping the labeled nodes ﬁxed.

any permutation of AG over the unlabeled nodes. In our context  the labeled nodes correspond to
the observed nodes V while the unlabeled nodes correspond to latent nodes H. We now provide
necessary conditions for graph reconstruction up to certain edit distance.

In other words  the edit distance is the minimum number of entries that are different in A(cid:98)G and in
Theorem 2 (Necessary Condition for Graph Estimation) For any deterministic estimator (cid:98)Gm :

2ρmn (cid:55)→ Gm based on n i.i.d. samples  where ρ ∈ [0  1] is the fraction of observed nodes and m is
1In the trivial case  when all the nodes are observed and the graph is locally tree-like  our method reduces
to thresholding of information distances at each node  and building local MSTs. The threshold can be chosen
as r = dmax +   for some  > 0.

2We consider inexact graph matching where the unlabeled nodes can be unmatched. This is done by adding
required number of isolated unlabeled nodes in the other graph  and considering the modiﬁed adjacency matri-
ces [19].

6

(11)

2

the total number of nodes of an Ising model Markov on graph Gm ∈ GGirth(m; g  ∆min  ∆max) on
m nodes with girth g  minimum degree ∆min and maximum degree ∆max  for all  > 0  we have

P[dist((cid:98)Gm  Gm; V ) > m] ≥ 1 −

2nρmm(2+1)m3m

m0.5∆minm(m − g∆g

max)0.5∆minm  

under any sampling process used to choose the observed nodes.

Proof:

The proof is based on counting arguments. See [17] for details.

n = Ω(cid:0)∆minρ−1 log p(cid:1)

Lower Bound on Sample Requirements: The above result states that roughly

(12)
samples are required for structural consistency under any estimation method. Thus  when ρ =
Θ(1) (constant fraction of observed nodes)  polylogarithmic number of samples are necessary (n =
Ω(poly log p))  while when ρ = Θ(m−γ) for some γ > 0 (i.e.  a vanishing fraction of observed
nodes)  polynomial number of samples are necessary for reconstruction (n = Ω(poly(p)).

Comparison with Sample Complexity of Proposed Method: For Ising models  under uniform
sampling of observed nodes  we established that the sample complexity of the proposed method
scales as n = Ω(∆2ρ−2(log p)3) for regular graphs with degree ∆. Thus  we nearly match the
lower bound on sample complexity in (12).

4 Experiments

We employ latent graphical models for topic modeling. Each hidden variable in the model can
be thought of as representing a topic  and topics and words in a document are drawn jointly from
the graphical model. We conduct some preliminary experiments on 20 newsgroup dataset with
16 242 binary samples of 100 selected keywords. Each binary sample indicates the appearance
of the given words in each posting  these samples are divided in to two equal groups for learning
and testing purposes. We compare the performance with popular latent Dirichlet allocation (LDA)
model [9]. We evaluate performance in terms of perplexity and topic coherence. In addition  we also
study tradeoff between model complexity and data ﬁtting through the Bayesian information criterion
(BIC) [8].

Methods: We consider a regularized variant of the proposed method for latent graphical model
selection. Here  in every iteration  the decision to add hidden variables to a local neighborhood is
based on the improvement of the overall BIC score. This allows us to tradeoff model complexity
and data ﬁtting. Note that our proposed method only deals with structure estimation and we use
expectation maximization (EM) for parameter estimation. We compare the proposed method with
the LDA model3. This method is implemented in MATLAB. We used the modules for LBP  made
available with UGM4 package. The LDA models are learnt using the lda package5.

(cid:34)

n(cid:88)

k=1

− 1
np

(cid:35)

Performance Evaluation: We evaluate performance based on the test perplexity [20] given by

Perp-LL := exp

log P (xtest(k))

 

(13)

where n is the number of test samples and p is the number of observed variables (i.e.  words). Thus
the perplexity is monotonically decreasing in the test likelihood and a lower perplexity indicates a
better generalization performance. On lines of (13)  we also deﬁne

Perp-BIC := exp

BIC(xtest)

  BIC(xtest) :=

log P (xtest(k)) − 0.5(df) log n 

(cid:20)

− 1
np

(cid:21)

(14)
3Typically  LDA models the counts of different words in documents. Here  since we have binary data  we

consider a binary LDA model where the observed variables are binary.

4These codes are available at http://www.di.ens.fr/˜mschmidt/Software/UGM.html
5http://chasen.org/˜daiti-m/dist/lda/

n(cid:88)

k=1

7

Method
Proposed
Proposed
Proposed
Proposed

LDA
LDA
LDA
LDA

r
7
9
11
13
NA
NA
NA
NA

Hidden

32
24
26
24
10
20
30
40

Edges
183
129
125
123
NA
NA
NA
NA

PMI
0.4313
0.6037
0.4585
0.4289
0.2921
0.1919
0.1653
0.1470

Perp-LL
1.1498
1.1543
1.1555
1.1560
1.1480
1.1348
1.1421
1.1494

Perp-BIC
1.1518
1.1560
1.1571
1.1576
1.1544
1.1474
1.1612
1.1752

Table 1: Comparison of proposed method under different thresholds (r) with LDA under differ-
ent number of topics (i.e.  number of hidden variables) on 20 newsgroup data. For deﬁnition of
perplexity based on test likelihood and BIC scores  and PMI  see (13)  (14)  and (15).

where df is the degrees of freedom in the model. For a graphical model  we set df GM := m +
|E|  where m is the total number of variables (both observed and hidden) and |E| is the number
of edges in the model. For the LDA model  we set df LDA := (p(m − p) − 1)  where p is the
number of observed variables (i.e.  words) and m− p is the number of hidden variables (i.e.  topics).
This is because a LDA model is parameterized by a p × (m − p) topic probability matrix and a
(m − p)-length Dirichlet prior. Thus  the BIC perplexity in (14) is monotonically decreasing in
the BIC score  and a lower BIC perplexity indicates better tradeoff between model complexity and
data ﬁtting. However  the likelihood and BIC score in (13) and (14) are not tractable for exact
evaluation in general graphical models since they involve the partition function. We employ loopy
belief propagation (LBP) to evaluate them. Note that it is exact on a tree model and approximate
for loopy models.
In addition  we also evaluate topic coherence  frequently considered in topic
modeling. It is based on the average pointwise mutual information (PMI) score

PMI(Xi; Xj)  PMI(Xi; Xj) := log

P (Xi = 1  Xj = 1)

P (Xi = 1)P (Xj = 1)

  (15)

1

PMI :=

(cid:88)

(cid:88)
word pairs for each topic is(cid:0)10

45|H|

i j∈A(h)

h∈H

i<j

where the set A(h) represents the “top-10” words associated with topic h ∈ H. The number of such

(cid:1) = 45  and is used for normalization. In [21]  it is found that the

PMI scores are a good measure of human evaluated topic coherence when it is computed using an
external corpus. We compute PMI scores based on NYT articles bag-of-words dataset [22].

2

thresholds r ∈
Experimental Results: We learn the graph structures under different
{7  9  11  13}  which controls the length of cycles. At r = 13  we obtain a latent tree and for all
other values  we obtain loopy models. The the ﬁrst long cycle appears at r = 9. At r = 7  we ﬁnd a
combination of short and long cycles. We ﬁnd that models with cycles are more effective in discov-
ering intuitive relationships. For instance  in the latent tree (r = 13)  the link between “computer”
and “software” is missing due to the tree constraint  but is discovered when r ≤ 9. Moreover  we
see that common words across different topics tend to connect the local subgraphs  and thus loopy
models are better at discovering such relationships. The graph structures from the experiments are
available in [17]. In Table 1  we present results under our method and under LDA modeling. For the
LDA model  we vary the number of hidden variables (i.e.  topics) as {10  20  30  40}. In contrast 
our method is designed to optimize for the number of hidden variables  and does not need this input.
We note that our method is competitive in terms of both perplexity and topic coherence. We ﬁnd
that topic coherence (i.e.  PMI) for our method is optimal at r = 9  where the graph has a single
long cycle and a few short cycles.
The above experiments conﬁrm the effectiveness of our approach for discovering hidden topics  and
are in line with the theoretical guarantees established earlier in the paper. Our analysis reveals that
a large class of loopy graphical models with latent variables can be learnt efﬁciently.

Acknowledgement

This work is supported by NSF Award CCF-1219234  AFOSR Award FA9550-10-1-0310  ARO
Award W911NF-12-1-0404  the setup funds at UCI  and ONR award N00014-08-1-1015.

8

References
[1] R. Durbin  S. R. Eddy  A. Krogh  and G. Mitchison. Biological Sequence Analysis: Probabilistic Models

of Proteins and Nucleic Acids. Cambridge Univ. Press  1999.

[2] P. L. Erd¨os  L. A. Sz´ekely  M. A. Steel  and T. J. Warnow. A few logs sufﬁce to build (almost) all trees:

Part i. Random Structures and Algorithms  14:153–184  1999.

[3] E. Mossel. Distorted metrics on trees and phylogenetic forests. IEEE/ACM Transactions on Computa-

tional Biology and Bioinformatics  pages 108–116  2007.

[4] M.J. Choi  V.Y.F. Tan  A. Anandkumar  and A. Willsky. Learning latent tree graphical models. J. of

Machine Learning Research  12:1771–1812  May 2011.

[5] A. Anandkumar  V. Y. F. Tan  F. Huang  and A. S. Willsky. High-dimensional structure learning of Ising

models: local separation criterion. The Annals of Statistics  40(3):1346–1375  2012.

[6] A. Jalali  C. Johnson  and P. Ravikumar. On learning discrete graphical models using greedy methods. In

Proc. of NIPS  2011.

[7] P. Ravikumar  M.J. Wainwright  and J. Lafferty. High-dimensional Ising Model Selection Using l1-

Regularized Logistic Regression. Annals of Statistics  2008.

[8] G. Schwarz. Estimating the dimension of a model. Annals of Statistics  6(2):461–464  1978.
[9] D.M. Blei  A.Y. Ng  and M.I. Jordan. Latent dirichlet allocation. J. of Machine Learning Research 

3:993–1022  2003.

[10] G. Bresler  E. Mossel  and A. Sly. Reconstruction of Markov random ﬁelds from samples: some obser-
vations and algorithms. In Intl. workshop APPROX Approximation  Randomization and Combinatorial
Optimization  pages 343–356. Springer  2008.

[11] N. Meinshausen and P. B¨uhlmann. High dimensional graphs and variable selection with the lasso. Annals

of Statistics  34(3):1436–1462  2006.

[12] V. Chandrasekaran  P.A. Parrilo  and A.S. Willsky. Latent Variable Graphical Model Selection via Convex

Optimization. Arxiv preprint  2010.

[13] J. Bento and A. Montanari. Which Graphical Models are Difﬁcult to Learn? In Proc. of Neural Informa-

tion Processing Systems (NIPS)  Vancouver  Canada  Dec. 2009.

[14] M.J. Wainwright and M.I. Jordan. Graphical Models  Exponential Families  and Variational Inference.

Foundations and Trends in Machine Learning  1(1-2):1–305  2008.

[15] F.R.K. Chung. Spectral graph theory. Amer Mathematical Society  1997.
[16] A. Gamburd  S. Hoory  M. Shahshahani  A. Shalev  and B. Virag. On the girth of random cayley graphs.

Random Structures & Algorithms  35(1):100–117  2009.

[17] A. Anandkumar and R. Valluvan. Learning Loopy Graphical Models with Latent Variables: Efﬁcient
Methods and Guarantees. Under revision from Annals of Statistics. Available on ArXiv:1203.3887  Jan.
2012.

[18] A. Anandkumar  V. Y. F. Tan  F. Huang  and A. S. Willsky. High-Dimensional Gaussian Graphical Model
Selection: Walk-Summability and Local Separation Criterion. Accepted to J. Machine Learning Research 
ArXiv 1107.1270  June 2012.

[19] H. Bunke and G. Allermann. Inexact graph matching for structural pattern recognition. Pattern Recogni-

tion Letters  1(4):245–253  1983.

[20] D. Newman  E.V. Bonilla  and W. Buntine. Improving topic coherence with regularized topic models. In

Proc. of NIPS  2011.

[21] David Newman  Sarvnaz Karimi  and Lawrence Cavedon. External evaluation of topic models. In Pro-
ceedings of the 14th Australasian Computing Symposium(ACD2009)  page 8  Sydney  Australia  Decem-
ber 2009.

[22] A. Frank and A. Asuncion. UCI machine learning repository  2010.

9

,Richard Socher
Milind Ganjoo
Christopher Manning
Andrew Ng
Florian Stimberg
Andreas Ruttor
Manfred Opper