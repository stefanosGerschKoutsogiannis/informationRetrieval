2016,Finding significant combinations of features in the presence of categorical covariates,In high-dimensional settings  where the number of features p is typically much larger than the number of samples n  methods which can systematically examine arbitrary combinations of features  a huge 2^p-dimensional space  have recently begun to be explored. However  none of the current methods is able to assess the association between feature combinations and a target variable while conditioning on a categorical covariate  in order to correct for potential confounding effects.  We propose the Fast Automatic Conditional Search (FACS) algorithm  a significant discriminative itemset mining method which conditions on categorical covariates and only scales as O(k log k)  where k is the number of states of the categorical covariate. Based on the Cochran-Mantel-Haenszel Test  FACS demonstrates superior speed and statistical power on simulated and real-world datasets compared to the state of the art  opening the door to numerous applications in biomedicine.,Finding signiﬁcant combinations of features in the

presence of categorical covariates

Laetitia Papaxanthos∗  Felipe Llinares-López∗  Dean Bodenham  Karsten Borgwardt

Machine Learning and Computational Biology Lab

D-BSSE  ETH Zurich

*Equally contributing authors.

Abstract

In high-dimensional settings  where the number of features p is much larger than the
number of samples n  methods that systematically examine arbitrary combinations
of features have only recently begun to be explored. However  none of the current
methods is able to assess the association between feature combinations and a target
variable while conditioning on a categorical covariate. As a result  many false
discoveries might occur due to unaccounted confounding effects.
We propose the Fast Automatic Conditional Search (FACS) algorithm  a signiﬁcant
discriminative itemset mining method which conditions on categorical covariates
and only scales as O(k log k)  where k is the number of states of the categorical
covariate. Based on the Cochran-Mantel-Haenszel Test  FACS demonstrates supe-
rior speed and statistical power on simulated and real-world datasets compared to
the state of the art  opening the door to numerous applications in biomedicine.

Introduction

1
In the last 10 years  the amount of data available is growing at an unprecedented rate. However  in
many application domains  such as computational biology and healthcare  the amount of features is
growing much faster than typical sample sizes. Therefore  statistical inference in high-dimensional
spaces has become a tool of the utmost importance for practitioners in those ﬁelds. Despite the great
success of approaches based on sparsity-inducing regularizers [16  2]  the development of methods to
systematically explore arbitrary combinations of features and assess their statistical association with
a target of interest has been less studied. Exploring all combinations of p features is equivalent to
handling a 2p-dimensional space  thus combinatorial feature discovery exacerbates the challenges for
statistical inference in high-dimensional spaces even for moderate p.
Under the assumption that features and targets are binary random variables  recent work in the ﬁeld
of signiﬁcant discriminative itemset mining offers tools to solve the computational and statistical
challenges incurred by combinatorial feature discovery. However  all state-of-the-art approaches [15 
10  13  7  8] share a key limitation: no method exists to assess the conditional association between
feature combinations and the target. The ability to condition the associations on an observed covariate
is fundamental to correct for confounding effects. If unaccounted for  one may ﬁnd many false
positives that are actually associated with the covariate and not the class of interest [17]. For example 
in medical case/control association studies  it is common to search for combinations of genetic
variants that are associated with a disease of interest. In this setting  the class labels are the health
status of individuals  sick or healthy. The features represent binary genetic variants  encoded as 1
if the variant is altered and as 0 if not. Often  in high-order association studies  a subset of genetic
variants are combined to form a binary variable whose value is 1 if the subset only contains altered
genetic variants and is 0 otherwise. A subset of genetic variants is associated with the class label if the
frequencies of altered combinations in each class are statistically different. However  it is often the

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

case that the studied samples belong to several subpopulations  for example African-American  East
Asian or European Caucasian  which show differences in the prevalence of some altered combinations
of genetic variants because of systematic ancestry differences. When  additionally  the subpopulations
clusters are unevenly distributed across classes  it can result in false associations to the disease of
interest [12]. This is the reason why it is necessary to model ancestry differences between cases and
controls in the presence of population structure or to correct for covariates in more general settings.
Hence our goal in this article is to present the ﬁrst approach to signiﬁcant discriminative itemset
mining that allows one to correct for a confounding categorical covariate.
To reach this goal  we present the novel algorithm FACS  which enables signiﬁcant discrimina-
tive itemset mining with categorical covariates through the Cochran-Mantel-Haenszel test [9] in
O(k log k) time  where k is the number of states of the categorical covariate  compared to the standard
implementation which is exponential in k.
The rest of this article is organized as follows: In Section 2 we deﬁne the problem to be solved
and introduce the main theoretical concepts from related work that FACS is based on  namely the
Cochran-Mantel-Haenszel-test and Tarone’s testability criterion. In Section 3  we describe in detail
our contribution  the FACS algorithm and its efﬁcient implementation. Finally  Section 4 validates the
performance of our method on a set of simulated and biomedical datasets.
2 Problem statement and related work
In this section we introduce the necessary background  notation and terminology for the remainder
of this article. First  in Section 2.1 we rigorously deﬁne the problem we solve in this paper. Next 
in Sections 2.2 and 2.3 we describe two key elements on which our method is based: the Cochran-
Mantel-Haenszel (CMH) test and Tarone’s testability criterion.
2.1 Discovering signiﬁcant feature combinations in the presence of a categorical covariate
We consider a dataset of n observations D = {(ui  yi  ci)}n
i=1  where the ith observation consists of:
(I) a feature vector ui consisting of p binary features  ui j ∈ {0  1} for j = 1  . . .   p; (II) a binary class
label  yi ∈ {0  1}; and (III) a categorical covariate ci  which has k categories  i.e. ci ∈ {1  2  . . .   k}.
Given any subset of features S ⊆ {1  2  . . .   p}  we deﬁne its induced feature combination for the ith
j∈S ui j  such that zi S takes value 1 if and only if ui j = 1 for all features in
S. Now  we use ZS to denote the feature combination induced by S  of which zi S is the realization
for the ith observation. Similarly  we use Y to denote the label  and C to denote the covariate  of
which yi and ci are realizations  respectively  for i = 1  2  . . .   n. Below we use the standard notation
A ⊥⊥ B to denote “A is statistically independent of B”.
Typically  signiﬁcant discriminative itemset mining aims to ﬁnd all feature subsets S for which a
statistical association test rejects the null hypothesis  namely ZS ⊥⊥ Y   after a rigorous correction for
multiple hypothesis testing. However  for any feature subset such that ZS (cid:54)⊥⊥ Y but ZS ⊥⊥ Y | C  the
association between ZS and Y is exclusively mediated by the covariate C  which acts in this case as
a confounder creating spurious associations.
Our goal: In this work  the aim is to ﬁnd all feature subsets S for which a statistical association
test rejects the null hypothesis ZS ⊥⊥ Y | C  thus allowing to correct for a confounding categorical
covariate while keeping the computational efﬁciency  statistical power and the ability to correct for
multiple hypothesis testing of existing methods.
In the remainder of this section we will introduce two fundamental concepts our work relies upon.
The ﬁrst one is the Cochran-Mantel-Haenszel (CMH) test  which offers a principled way to test if a
feature combination ZS is conditionally dependent on the class labels Y given the covariate C  that
is  to test the null hypothesis ZS ⊥⊥ Y | C. The second concept is Tarone’s testability criterion  which
allows a correction for multiple hypothesis testing while retaining large statistical power  in scenarios
such as ours where billions or trillions of association tests must be performed.
Tarone’s testability criterion has only been successfully applied to unconditional association tests 
such as Fisher’s exact test [6] or Pearson’s χ2 test [11]. Thus  the state-of-the-art in signiﬁcant
discriminative itemset mining forces one to choose between: (a) using Bonferroni’s correction 
resulting in very low statistical power or an arbitrary limit in the cardinality of feature subsets (e.g.
[18])  or (b) using Tarone’s testability criterion  losing the ability to account for covariates and
resulting in potentially many confounded patterns being deemed signiﬁcant [15  13  7  8].

observation as zi S =(cid:81)

2

Our contribution: In this paper  we propose FACS  a novel algorithm that allows applying Tarone’s
testability criterion to the CMH test  allowing to correct for a categorical covariate in signiﬁcant
discriminative itemset mining for the ﬁrst time. FACS will be introduced in detail in Section 3.
2.2 Conditional association testing with the Cochran-Mantel-Haenszel (CMH) test
To test if ZS ⊥⊥ Y | C  the CMH test [9] arranges the n realisations of {(zi S   yi  ci)}n
distinct 2 × 2 contingency tables  one table for each possible value of the covariate c  as:

i=1 into k

Variables

y = 1
y = 0

Col totals

zS = 1
xS j − aS j

aS j

xS j

zS = 0
n1 j − aS j
nj − xS j

n2 j − xS j + aS j

Row totals

n1 j
n2 j
nj

where: (I) nj is the number of observations with c = j  n1 j of which have class label y = 1 and n2 j
of which have class label y = 0; (II) xS j is the number of observations with c = j and zi S = 1;
(III) aS is the number of observations with c = j  class label y = 1 and zi S = 1. Based on
{nj  n1 j  xS j  aS j}k

j=1  a p-value pS for feature combination ZS is computed as:



(cid:80)k

n1 j
nj

j=1

pS = 1 − Fχ2

1

(cid:16)(cid:80)k
(cid:16)
(cid:16)
j=1 aS j − xS j n1 j
1 − n1 j

(cid:17)

xS j

nj

nj

(cid:17)2

1 − xS j

nj



(cid:17)

(1)

1

(·) is the distribution function of a χ2 random variable with 1 degree of freedom. Finally 
where Fχ2
the feature combination ZS and its corresponding feature subset S will be deemed signiﬁcantly
associated if the p-value pS falls below a corrected signiﬁcance threshold δ  that is  if pS ≤ δ.
The CMH test can be understood as a form of meta-analysis applied to k disjoint datasets {Dj}k
j=1 
where Dj = {(ui  yi)| ci = j} contains only observations for which the covariate c takes value j.
For confounded feature combinations  the association might be large in the entire dataset D  but small
for conditional datasets Dj. Thus  the CMH test will not deem such feature combinations signiﬁcant.
2.3 The multiple testing problem in discriminative itemset mining
In our setup  one must perform 2p − 1 association tests  one for each possible subset of features. Even
for moderate p  this leads to an enormous number of tests  resulting in a large multiple hypothesis
testing problem. To produce statistically reliable results  the signiﬁcance threshold δ will be chosen to
guarantee that the Family-Wise Error Rate (FWER)  deﬁned as the probability of producing any false
positives  is upper-bounded by a signiﬁcance level α. FWER control is most commonly achieved with
Bonferroni’s correction [3  5]  which in our setup would imply using δ = α/(2p − 1) as signiﬁcance
threshold. However  Bonferroni’s correction tends to be overly conservative  resulting in very low
statistical power when the number of tests performed is large. In contrast  recent work in signiﬁcant
discriminative itemset mining [15  10  13  7] showed that  in this setting  Bonferroni’s correction can
be outperformed in terms of statistical power by Tarone’s testability criterion [14].
Tarone’s testability criterion is based on the observation that  for some discrete test statistics based on
contingency tables  a minimum attainable p-value can be computed as a function of the table margins.
Let Ψ(S) denote the minimum attainable p-value corresponding to the contingency table of feature
combination ZS. By deﬁnition  pS ≥ Ψ(S)  therefore Ψ(S) > δ implies that feature combination
ZS can never be deemed signiﬁcantly associated  and hence it cannot cause a false positive. In other
words  feature subsets S for which Ψ(S) > δ are irrelevant as far as the FWER is concerned. In
Tarone’s terminology  S is said to be untestable. Thus  deﬁning the set of testable feature subsets at
level δ as IT (δ) = {S| Ψ(S) ≤ δ}  Tarone’s testability criterion obtains the corrected signiﬁcance
threshold as δtar = max{δ : FWERtar(δ) ≤ α}  where FWERtar(δ) = δ|IT (δ)|. Note that this
amounts to applying a Bonferroni correction to feature subsets S in IT (δ) only. FWER control
follows from the fact that untestable feature subsets cannot affect the FWER. Since in practice
|IT (δ)| (cid:28) 2p − 1  Tarone’s testability criterion often outperforms Bonferroni’s correction in terms
of statistical power by a large margin.
The main practical limitation of Tarone’s testability criterion is its computational complexity. Naively
computing δtar would involve explicitly enumerating all 2p − 1 feature subsets and evaluating their
respective minimum attainable p-values  something unfeasible even for moderate p. Existing work in
signiﬁcant discriminative pattern mining solves that limitation by exploiting speciﬁc properties of

3

certain test statistics  such as Fisher’s Exact Test or Pearson’s χ2 test  that allow to apply branch-and-
bound algorithms to evaluate δtar. However  the properties those algorithms rely on do not apply to
conditional statistical association tests  such as the CMH test. In the next section  we present in detail
our novel approach to apply Tarone’s method to the CMH test.
3 Our contribution: The FACS algorithm
This section introduces the Fast Automatic Conditional Search (FACS) algorithm  the ﬁrst approach
that allows the application of Tarone’s testability criterion to the CMH test in a computationally
efﬁcient manner. Section 3.1 discusses the main challenges facing FACS and summarizes how
FACS improves the state of the art. Section 3.2 provides a high-level description of the algorithm.
Finally  Sections 3.3 and 3.4 detail the two key steps of FACS  which are also the main algorithmic
contributions of this work.
3.1 Overview and Contributions
The main objective of the FACS algorithm  described in Section 3.2 below  can be summarised as:
Objective: Given a dataset D = {(ui  yi  ci)}n

i=1  the goal of FACS is to:

1. Compute Tarone’s corrected signiﬁcance threshold δtar.
2. Retrieve all feature subsets S whose p-value pS is below δtar.

For both (1) and (2)  the test statistic of choice will be the CMH test  thus allowing to correct for a
confounding categorical covariate as described in Section 2.2.
The key contribution of our work is to bridge the gap between Tarone’s testability criterion and the
CMH test. Firstly  in Section 3.3  we show for the ﬁrst time that Tarone’s method can be applied to
the CMH test. More importantly  in Section 3.4 we introduce a novel branch-and-bound algorithm to
efﬁciently compute δtar without requiring the function Ψ computing Tarone’s minimum attainable
p-value to be monotonic. This allows us not only to apply Tarone’s testability criterion to the CMH
test  but to do so as efﬁciently as existing methods not able to handle confounding covariates do.
3.2 High-level description of FACS
As shown in the pseudocode in Algorithm 1  conceptually  FACS performs two main operations:
Algorithm 1 FACS
Input: Dataset D = {(ui  yi  ci)}n
Output: {S | pS ≤ δtar}
1: Initialize global variables δtar = 1
2: δtar IT (δtar) ← tarone_cmh(∅)
3: Return

Algorithm 2 tarone_cmh
Input: Current feature subset being processed S
1: if is_testable_cmh(S  δtar)

Append S to IT (δtar)
FWERtar(δtar) ← δtar|IT (δtar)|
while FWERtar(δtar) > α do

and IT (δtar) = ∅

target FWER α

then {see Sec-

tion 3.3}

Decrease δtar

IT (δtar) ←(cid:8)S ∈ IT (δtar) : is_testable(S  δtar)(cid:9)

FWERtar(δtar) ← δtar|IT (δtar)|

7:
8: if not is_prunable_cmh(S  δtar) then {see 3.4}
9:
10:

for S(cid:48) ∈ Children(S) do

tarone_cmh(S(cid:48))

i=1 

2:
3:
4:
5:
6:

{S ∈ IT (δtar)| pS ≤ δtar}

Firstly  Line 2 invokes the routine tarone_cmh  described in Algorithm 2. This routine uses our
novel branch-and-bound approach to efﬁciently compute Tarone’s corrected signiﬁcance threshold
δtar and the set of testable feature subsets IT (δtar).
Secondly  using the signiﬁcance threshold δtar obtained in the previous step  Line 3 evaluates the
conditional association of the feature combination ZS of each testable feature subset S ∈ IT (δtar)
with the class labels  given the categorical covariate  using the CMH test as shown in Section 2.2.
Note that  according to Tarone’s testability criterion  untestable feature subsets S (cid:54)∈ IT (δtar) cannot
be signiﬁcant and therefore do not need to be considered in this step. Since in practice |IT (δtar)| (cid:28)
2p − 1  the procedure tarone_cmh is the most critical part of FACS.
The routine tarone_cmh uses the enumeration scheme ﬁrst proposed in [10  13]. All 2p feature
subsets are arranged in an enumeration tree such that S(cid:48) ∈ Children(S) ⇒ S ⊂ S(cid:48). In other words 

4

the children of a feature subset S in the enumeration tree are obtained by adding an additional feature
to S. Before invoking tarone_cmh  in Line 1 of Algorithm 1 the signiﬁcance threshold δtar is
initialized to 1  the largest value it can take  and the set of testable feature combinations IT (δtar) is
initialized to the empty set. The enumeration procedure is started by calling tarone_cmh with the
empty feature subset S = ∅  which acts as the root of the enumeration tree1. All 2p − 1 non-empty
feature subsets will then be explored recursively by traversing the enumeration tree depth-ﬁrst.
Every time a feature subset S in the tree is visited  Line 1 of Algorithm 2 checks if it is testable  as
detailed in Section 3.3. If it is  S is appended to the set of testable feature subsets IT (δtar) in Line 2.
The FWER condition for Tarone’s testability criterion is checked in Lines 3 and 4. If it is found
to be violated  the signiﬁcance threshold δtar is decreased in Line 5 until the condition is satisﬁed
again  removing from IT (δtar) any feature subsets made untestable by decreasing δtar in Line 6 and
re-evaluating the FWER condition accordingly in Line 7. Before continuing the traversal of the tree
by exploring the children of the current feature subset S  Line 8 checks if our novel pruning criterion
applies  as described in Section 3.4. Only if it does not apply are all children of S visited recursively
in Lines 9 and 10. The testability and pruning conditions in Lines 1 and 8 become more stringent
as δtar decreases. Because of this  as δtar decreases along the enumeration procedure (see Line 5) 
increasingly larger parts of the search space are pruned. Thus  the algorithm terminates when  for the
current value of δtar and IT (δtar)  all feature subsets that cannot be pruned have been visited.
The two most challenging steps in FACS are the design of an appropriate testability criterion 
is_testable_cmh(S  δ)  and an efﬁcient pruning criterion  is_prunable_cmh(S  δ)  that circum-
vent the limitations of the current state of the art. These are now each described in detail.
3.3 A testability criterion for the CMH test
As mentioned in Section 2.3  Tarone’s testability criterion has only been applied to test statistics such
as Fisher’s exact test  Pearson’s χ2 test and the Mann-Whitney U Test  none of which allows for
incorporating covariates. However  the following proposition shows that the CMH test also has a
minimum attainable p-value Ψcmh(S):
Proposition 1 The CMH test has a minimum attainable p-value Ψcmh(S)  which can be computed
in O(k) time as a function of the margins {nj  n1 j  xS j}k
The proof of Proposition 1  provided in the Supp. Material  involves showing that Ψcmh(S) can be
computed from the k 2 × 2 contingency tables corresponding to ZS (see Section 2.2) by optimising
the p-value pS with respect to {aS j}k
j=1 ﬁxed.
3.4 A pruning criterion for the CMH test
State-of-the-art methods [15  8]  all of which are limited to unconditional association testing  exploit
the fact that the minimum attainable p-value function Ψ(S)  using either Fisher’s exact test or
Pearson’s χ2 test on a single contingency table  obeys a simple monotonicity property: S ⊆ S(cid:48) ⇒
Ψ(S) ≤ Ψ(S(cid:48)) provided that xS ≤ min(n1  n2). This leads to a remarkably simple pruning criterion:
if a feature subset S is non-testable  i.e. Ψ(S) > δ  and its support xS is smaller or equal to
min(n1  n2)  then all children S(cid:48) of S  which satisfy S ⊂ S(cid:48) by construction of the enumeration tree 
will also be non-testable and can be pruned from the search space. However  such a monotonicity
property does not hold for the CMH minimum attainable p-value function Ψcmh(S)  severely
complicating the development of an effective pruning criterion.
In Section 3.4.1 we show how to circumvent this limitation by introducing a novel pruning criterion
attainable p-value function Ψcmh(S) and prove that it leads to a valid pruning strategy. Finally  in

based on deﬁning a monotonic lower envelope (cid:101)Ψcmh(S) ≤ Ψcmh(S) of the original minimum
Section 3.4.2  we provide an efﬁcient algorithm to evaluate(cid:101)Ψcmh(S) in O(k log k) time  instead of a

j=1 while keeping the table margins {nj  n1 j  xS j}k

j=1 of the k 2 × 2 contingency tables.

naive implementation whose computational complexity would scale exponentially with k  the number
of categories for the covariate. Due to space constraints  all proofs are in the Supp. Material.

3.4.1 Deﬁnition and correctness of the pruning criterion

As mentioned above  existing unconditional signiﬁcant discriminative pattern mining meth-
ods only consider feature subsets S with support xS ≤ min(n1  n2) to be potentially prun-

1We deﬁne zi ∅ = 1 for all observations  so this artiﬁcial feature combination will never be signiﬁcant.

5

able. Analogously  we consider as potentially prunable the set of feature subsets IP P =
{S | xS j ≤ min(n1 j  n2 j)∀ j = 1  . . .   k}. Note that for k = 1  our deﬁnition reduces to that
of existing work. In itemset mining  a very large proportion of all feature subsets will have small
supports. Therefore  restricting the application of the pruning criterion to potentially prunable patterns
does not cause a loss of performance in practice. We can now state the deﬁnition of the lower envelope
for the CMH minimum attainable p-value:

set of potentially prunable patterns. Next  we show that unlike for the minimum attainable p-value
Lemma 1 Let S S(cid:48) ∈ IP P be two potentially prunable feature subsets such that S ⊆ S(cid:48). Then 

Deﬁnition 1 Let S ∈ IP P be a potentially prunable feature subset. The lower envelope(cid:101)Ψcmh(S) is
deﬁned as(cid:101)Ψcmh(S) = min{Ψcmh(S(cid:48)) | S(cid:48) ⊇ S}.
Note that  by construction (cid:101)Ψcmh(S) satisﬁes(cid:101)Ψcmh(S) ≤ Ψcmh(S) for all feature subsets S in the
function Ψcmh(S)  the monotonicity property holds for the lower envelope(cid:101)Ψcmh(S):
(cid:101)Ψcmh(S) ≤ (cid:101)Ψcmh(S(cid:48)) holds.
Theorem 1 Let S ∈ IP P be a potentially prunable feature subset such that (cid:101)Ψcmh(S) > δ. Then 
if and only if S ∈ IP P ⇔ xS j ≤ min(n1 j  n2 j)∀ j = 1  . . .   k and(cid:101)Ψcmh(S) > δtar.

Ψcmh(S(cid:48)) > δ for all S(cid:48) ⊇ S  i.e. all feature subsets containing S are non-testable at level δ and
can be pruned from the search space.
To summarize  the pruning criterion is_prunable_cmh in Line 8 of Algorithm 2 evaluates to true

Next  we state the main result of this section  which establishes our search space pruning criterion:

3.4.2 Evaluating the pruning criterion in O(k log k) time

In FACS  the pruning criterion stated above will be applied to all enumerated feature subsets. Hence 
it is mandatory to have an efﬁcient algorithm to compute the lower envelope for the CMH minimum

As shown in the proof of Proposition 1 in the Supp. Material  Ψcmh(S) depends on the pattern S
through its k-dimensional vector of supports xS = (xS 1  . . .   xS k). Also  the condition S(cid:48) ⊇ S
implies that xS(cid:48) j ≤ xS j ∀ j = 1  . . .   k. As a consequence  one can rewrite Deﬁnition 1 as
Ψcmh(xS(cid:48))  where the vector inequality xS(cid:48) ≤ xS holds component-wise. Thus 
j=1 xS j = O(mk) 
j=1. This scaling is clearly impractical  as even for moderate

attainable p-value(cid:101)Ψcmh(S) for any potentially prunable feature subset S ∈ IP P .
(cid:101)Ψcmh(S) = min
naively computing (cid:101)Ψ(S) would require optimizing Ψcmh over a set of size(cid:81)k
algorithm which evaluates(cid:101)Ψ(S) in only O(k log(k)) time. We will arrive at our ﬁnal result in two

where m is the geometric mean of {xS j}k
k it would result in an overhead large enough to outweigh the beneﬁts of pruning.
Because of this  in the remainder of this section we propose the last key part of FACS: an efﬁcient

xS(cid:48)≤xS

S(cid:48) j = 0 or x∗

Ψcmh(xS(cid:48)) satisﬁes x∗

steps  contained in Lemma 2 and Theorem 2.
Lemma 2 Let S ∈ IP P be a potentially prunable feature subset. The optimum x∗
optimization problem min
xS(cid:48)≤xS
In short  Lemma 2 shows that the optimum x∗

S(cid:48) of the discrete
S(cid:48) j = xS j for each j = 1  . . .   k.
mization problem deﬁning (cid:101)Ψ(S) is always a vertex of the discrete hypercube(cid:74)0  xS(cid:75). Thus  the
S(cid:48) = {Ψcmh(xS(cid:48))| xS(cid:48) ≤ xS} of the discrete opti-
computational complexity of evaluating (cid:101)Ψcmh(S) can be reduced from O(mk) to O(2k)  where
(cid:17)
for j = 1  . . .   k. Let πl and πr be permutations πl  πr : (cid:74)1  k(cid:75) (cid:55)→

m (cid:29) 2 for most patterns. Finally  building upon the result of Lemma 2  Theorem 2 below shows that
one can in fact ﬁnd the optimal vertex out of all O(2k) vertices in O(k log k) time.
Theorem 2 Let S ∈ IP P be a potentially testable feature subset and deﬁne βlS j = n2 j
and βrS j = n1 j
(cid:74)1  k(cid:75) such that βlS πl(1) ≤ . . . ≤ βlS πl(k) and βrS πr(1) ≤ . . . ≤ βrS πr(k)  respectively.
nj
Then  there exists an integer κ ∈(cid:74)1  k(cid:75) such that the optimum x∗

1 − xS j

1 − xS j

nj

nj

nj

(cid:16)

(cid:16)

(cid:17)

one of the two possible conditions: (I) x∗
j > κ or (II) x∗

S(cid:48) πr(j) = xS πr(j) for all j ≤ κ and x∗

S(cid:48) = arg min
xS(cid:48)≤xS
S(cid:48) πl(j) = xS πl(j) for all j ≤ κ and x∗
S(cid:48) πr(j) = 0 for all j > κ.

Ψcmh(xS(cid:48)) satisﬁes
S(cid:48) πl(j) = 0 for all

6

Figure 1: (a) Runtime as a function of the number of features  p. (b) Runtime as a function of the
number of categories of the covariate  k. (c) Precision as a function of the true signal strengh  ρtrue.
(d) False detection proportion as a function of the strength of the signal ρconf . n = 200 samples
were used in (a)  (b) and n = 500 in (c)  (d). Also  we set ρtrue = ρconf = ρ.
In summary  Theorem 2 above implies that the 2k candidates to be the optimum x∗
S(cid:48) according to
Lemma 2 can be narrowed down to only 2k vertices: k candidates satisfying the ﬁrst condition and k
the second condition. Moreover  evaluating Ψcmh for all k candidates satisfying the ﬁrst condition
(resp. the second condition) can be done in O(k) time rather than O(k2). This is due to the fact that
each of the k candidate vertices for each condition can be obtained by changing a single dimension
with respect to the previous one. Therefore  the operation dominating the computational complexity
is the sorting of the two k-vectors (βlS 1  . . .   βlS k) and (βrS 1  . . .   βrS k). As a consequence  the

runtime required to evaluate the lower envelope (cid:101)Ψcmh(S)  and thus our novel pruning criterion

is_prunable_cmh  scales as O(k log k) with the number of categories of the covariate.

4 Experiments
In Section 4.1 we describe a set of experiments on simulated datasets  evaluating the performance of
FACS in terms of runtime  precision and its ability to correct for confounding. Next  in Section 4.2 
we use our method in two applications in computational biology. Due to space constraints  only a
high-level summary of the experimental setup and results will be presented here. Additional details
can be found in the Supp. Material and code for FACS is available on GitHub2.

4.1 Runtime and power comparisons on simulated datasets

We compare FACS with four signiﬁcant discriminative itemset mining methods: LAMP-χ2  Bonf-CMH 
2k-FACS and mk-FACS. (1) LAMP-χ2 [15  10] is the state-of-the-art in signiﬁcant discriminative
itemset mining. It uses Tarone’s testability criterion but is based on Pearson’s χ2 test and thus cannot
account for covariates; (2) Bonf-CMH uses the CMH test  being able to correct for confounders  but
uses Bonferroni’s correction  resulting in a considerable loss of statistical power; (3) and (4) 2k-FACS
and mk-FACS are two suboptimal versions of FACS  which implement the pruning criterion using the
approach shown in Lemma 2  which scales as O(2k)  or via brute-force search  scaling as O(mk).
Runtime evaluations: Figure 1(a) shows that FACS scales as the state-of-the-art LAMP-χ2 when
increasing the number of features p  while the Bonferroni-based method Bonf-CMH scales consider-
ably worse. This indicates both that FACS is able to correct for covariates with virtually no runtime
overhead with respect to LAMP-χ2 and conﬁrms the efﬁcacy of Tarone’s testability criterion. Figure
1(b) shows that FACS can handle categorical covariates of high-cardinality k with almost no overhead 
in contrast to mk-FACS and 2k-FACS which are only applicable for low k. This demonstrates the
importance of our efﬁcient implementation of the pruning criterion.
Precision and false positive detection evaluations: We generated synthetic datasets with one truly
associated feature subset Strue and one confounded feature subset Sconf to evaluate precision and
ability to correct for confounders. Figure 1(c) shows that FACS has a similar precision as LAMP-χ2 
being slightly worse for weak signals and slightly better for stronger signals. Again  the performance
of the Bonferroni-based method Bonf-CMH is drastically worse. Most importantly  Figure 1(d)
indicates that unlike LAMP-χ2  FACS has the ability to greatly reduce the false positive detection by
conditioning on an appropriate categorical covariate.

2https://github.com/BorgwardtLab/FACS

7

102103104105106Number of features1.(a)10-210-11001011021031041051061071081091010Runtime (in seconds)One day100 daysOne yearFACS2k-CMHmk-CMHBonf-CMHLAMP-χ2051015202530Number of categories k1.(b)100101102103104105106107Runtime (in seconds)One day100 days0.00.20.40.60.81.0Strengthρ1.(c)0.00.20.40.60.81.0Precision0.00.20.40.60.81.0Strengthρ1.(d)0.00.20.40.60.81.0False positive detectionFACS-CMHLAMP-χ2Bonf-CMHTable 1: Total number of signiﬁcant combinations (hits) found by LAMP-χ2  FACS and BONF-CMH and
average genomic inﬂation factor λ. λ for BONF-CMH is similar to FACS since both use the CMH test.

Datasets

hits
LY 433
43

avrB

hits
19
1

BONF-CMH

hits
100 883
546

LAMP-χ2
λ
3.18
2.38

FACS
λ
1.17
1.21
4.2 Applications to computational biology
In this section  we look for signiﬁcant feature combinations in two widely investigated biological
applications: Genome-Wide Association Studies (GWAS)  using two A. thaliana datasets  and a study
of combinatorial regulation of gene expression in breast cancer cells.
A. thaliana GWAS: We apply FACS  LAMP-χ2 and Bonf-CMH to two datasets from the plant model
organism A. thaliana [1]  which contain 84 and 95 samples  respectively. The labels of each dataset
indicate the presence/absence of a plant defense-related phenotype: LY and avrB. In the two datasets 
each plant sample is represented by a sequence of approximately 214  000 genetic bases. The genetic
bases are encoded as binary features which indicate if the base at a speciﬁc locus is standard or altered.
To minimize the effect of the evolutionary correlations between nearby bases (< 10 kilo-bases) 
we downsampled each of the ﬁve chromosomes of each dataset  evenly by a factor of 20  using 20
different offsets. It resulted in complementary datasets containing between 1  423 and 2  661 features.
Our results for all methods are aggregated across all downsampled versions. In GWAS  one needs to
correct for the confounding effect of population structure to avoid many spurious associations. For
both datasets we condition on the ancestry  resulting in k = 5 and k = 3 categories for the covariate.
Table 1 shows the number of feature combinations (c.f. Section 2.1) reported as signiﬁcant by each
method  as well as the corresponding genomic inﬂation factor λ [4]  a popular criterion in statistical
genetics to quantify confounding. When compared to LAMP-χ2  we observe a severe reduction in the
number of feature combinations deemed signiﬁcant by FACS  as well as a sharp decrease in λ. This
strongly indicates that many feature combinations reported by LAMP-χ2 are affected by confounding.
The λ values of LAMP-χ2 show strong marginal associations between many feature combinations
and labels  inﬂating the corresponding Pearson χ2-test statistic values compared to the expected χ2
null distribution and resulting in many spurious associations. However  since most of those feature
combinations are independent of the labels given the covariates  the CMH test statistics values are
much closer to the χ2 distribution  leading to a lower λ and resulting in hits that are corrected for the
covariate. Moreover  the lack of power of BONF-CMH results in a very small number of hits.
Combinatorial regulation of gene expression in breast cancer cells: The breast cancer data set 
as used in [15]  includes 12  773 genes classiﬁed into up-regulated or not up-regulated. Each gene is
represented by 397 binary features which indicate the presence/absence of a sequence motif in the
neighborhood of this gene. We aim to ﬁnd combinations of motifs that are enriched in up-regulated
genes. Two sets of experiments were conducted  conditioning on 8 and 16 categories respectively. In
this case  the covariate groups together genes sharing similar sets of motifs. As previously  LAMP-χ2
reports 1  214 motif combinations as signiﬁcant  while FACS reports only 26 — a reduction of over
97%. Further studies shown in the Supp. Material strongly suggest that most motif combinations
found by LAMP-χ2 but not FACS are indeed due to confounding.
5 Conclusions
This article has presented FACS  the ﬁrst approach to signiﬁcant discriminative itemset mining that (i)
allows to condition on a categorical covariate  (ii) corrects for the inherent multiple testing problem
and (iii) retains high statistical power. Furthermore  we (iv) proved that the runtime of FACS scales
as O(k log k)  where k is the number of states of the categorical covariate. Regarding future work 
generalizing the state-of-the-art to handle continuous data is a key open problem in signiﬁcant
discriminative itemset mining. Solving it would greatly help make the framework applicable to new
domains. Another interesting improvement would be to combine FACS with the approach in [8]. In
their work  Tarone’s testability criterion is used along with permutation-testing to increase statistical
power by taking the redundancy between feature combinations into account. By using a similar
approach in combination with the CMH test  one could further increase statistical power while
retaining the ability to correct for a categorical covariate.
Acknowledgments: This work was funded in part by the SNSF Starting Grant ‘Signiﬁcant Pattern
Mining’ (KB) and the Marie Curie ITN MLPM2012  Grant No. 316861 (KB  FLL).

8

References
[1] S. Atwell  Y. S. Huang  B. J. Vilhjálmsson  G. Willems  M. Horton  Y. Li  D. Meng  A. Platt  A. M. Tarone 
T. T. Hu  et al. Genome-wide association study of 107 phenotypes in arabidopsis thaliana inbred lines.
Nature  465(7298):627–631  2010.

[2] C.-A. Azencott  D. Grimm  M. Sugiyama  Y. Kawahara  and K. M. Borgwardt. Efﬁcient network-guided

multi-locus association mapping with graph cuts. Bioinformatics  29(13):i171–i179  2013.

[3] C. E. Bonferroni. Teoria statistica delle classi e calcolo delle probabilità. Pubblicazioni del R Istituto

Superiore di Scienze Economiche e Commerciali di Firenze  8:3–62  1936.

[4] B. Devlin and K. Roeder. Genomic control for association studies. Biometrics  55(4):997–1004  1999.

[5] O. J. Dunn. Estimation of the medians for dependent variables. Ann. Math. Statist.  30(1):192–197  03

1959.

[6] R. A. Fisher. On the Interpretation of χ2 from Contingency Tables  and the Calculation of P. Journal of

the Royal Statistical Society  85(1):87–94  1922.

[7] F. Llinares-López  D. Grimm  D. A. Bodenham  U. Gieraths  M. Sugiyama  B. Rowan  and K. M. Borgwardt.
Genome-wide detection of intervals of genetic heterogeneity associated with complex traits. Bioinformatics 
31(12):240–249  2015.

[8] F. Llinares-López  M. Sugiyama  L. Papaxanthos  and K. M. Borgwardt. Fast and Memory-Efﬁcient
Signiﬁcant Pattern Mining via Permutation Testing. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining  Sydney  2015  pages 725–734. ACM  2015.

[9] N. Mantel and W. Haenszel. Statistical aspects of the analysis of data from retrospective studies of disease.

Journal of the National Cancer Institute  22(4):719  1959.

[10] S. Minato  T. Uno  K. Tsuda  A. Terada  and J. Sese. A fast method of statistical assessment for combinato-
rial hypotheses based on frequent itemset enumeration. In ECMLPKDD  volume 8725 of LNCS  pages
422–436  2014.

[11] K. Pearson. On the criterion that a given system of deviations from the probable in the case of a correlated
system of variables is such that it can reasonable be supposed to have arisen from random sampling.
Philosophical Magazine  50:157–175  1900.

[12] A. L. Price  N. J. Patterson  R. M. Plenge  M. E. Weinblatt  N. A. Shadick  and D. Reich. Principal
components analysis corrects for stratiﬁcation in genome-wide association studies. Nat Genet  38(8):904–
909  08 2006.

[13] M. Sugiyama  F. Llinares López  N. Kasenburg  and K. M. Borgwardt. Mining signiﬁcant subgraphs with

multiple testing correction. In SIAM Data Mining (SDM)  2015.

[14] R. E. Tarone. A modiﬁed bonferroni method for discrete data. Biometrics  46(2):515–522  1990.

[15] A. Terada  M. Okada-Hatakeyama  K. Tsuda  and J. Sese. Statistical signiﬁcance of combinatorial

regulations. Proceedings of the National Academy of Sciences  110(32):12996–13001  2013.

[16] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B (Methodological)  pages 267–288  1996.

[17] B. J. Vilhjálmsson and M. Nordborg. The nature of confounding in genome-wide association studies.

Nature Reviews Genetics  14(1):1–2  Jan. 2013.

[18] G. Webb. Discovering signiﬁcant rules. In Proceedings of the 12th ACM SIGKDD International Conference

on Knowledge Discovery and Data Mining  New York  2006  pages 434 – 443. ACM  2006.

9

,Dylan Festa
Guillaume Hennequin
Mate Lengyel
Laetitia Papaxanthos
Felipe Llinares-López
Dean Bodenham
Karsten Borgwardt
Wei-Ning Hsu
Yu Zhang
James Glass