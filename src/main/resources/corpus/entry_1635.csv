2009,Dual Averaging Method for Regularized Stochastic Learning and Online Optimization,We consider regularized stochastic learning and online optimization problems  where the objective function is the sum of two convex terms: one is the loss function of the learning task  and the other is a simple regularization term such as L1-norm for sparsity. We develop a new online algorithm  the regularized dual averaging method  that can explicitly exploit the regularization structure in an online setting. In particular  at each iteration  the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term  not just its subgradient. This method achieves the optimal convergence rate and often enjoys a low complexity per iteration similar as the standard stochastic gradient method. Computational experiments are presented for the special case of sparse online learning using L1-regularization.,Dual Averaging Method for Regularized Stochastic

Learning and Online Optimization

Microsoft Research  Redmond  WA 98052

Lin Xiao

lin.xiao@microsoft.com

Abstract

We consider regularized stochastic learning and online optimization problems 
where the objective function is the sum of two convex terms: one is the loss func-
tion of the learning task  and the other is a simple regularization term such as
ℓ1-norm for promoting sparsity. We develop a new online algorithm  the regular-
ized dual averaging (RDA) method  that can explicitly exploit the regularization
structure in an online setting. In particular  at each iteration  the learning variables
are adjusted by solving a simple optimization problem that involves the running
average of all past subgradients of the loss functions and the whole regulariza-
tion term  not just its subgradient. Computational experiments show that the RDA
method can be very effective for sparse online learning with ℓ1-regularization.

1

Introduction

In machine learning  online algorithms operate by repetitively drawing random examples  one at a
time  and adjusting the learning variables using simple calculations that are usually based on the
single example only. The low computational complexity (per iteration) of online algorithms is often
associated with their slow convergence and low accuracy in solving the underlying optimization
problems. As argued in [1  2]  the combined low complexity and low accuracy  together with other
tradeoffs in statistical learning theory  still make online algorithms a favorite choice for solving large-
scale learning problems. Nevertheless  traditional online algorithms  such as stochastic gradient
descent (SGD)  has limited capability of exploiting problem structure in solving regularized learning
problems. As a result  their low accuracy often makes it hard to obtain the desired regularization
effects  e.g.  sparsity under ℓ1-regularization. In this paper  we develop a new online algorithm  the
regularized dual averaging (RDA) method  that can explicitly exploit the regularization structure in
an online setting. We ﬁrst describe the two types of problems addressed by the RDA method.

1.1 Regularized stochastic learning

The regularized stochastic learning problems we consider are of the following form:

minimize



() ≜ E (  ) + Ψ()

(1)

where  ∈ R is the optimization variable (called weights in many learning problems)   = (  )
is an input-output pair drawn from an (unknown) underlying distribution   (  ) is the loss function
of using  and  to predict   and Ψ() is a regularization term. We assume  (  ) is convex in 
for each   and Ψ() is a closed convex function. Examples of the loss function  (  ) include:

∙ Least-squares:  ∈ R   ∈ R  and  (  (  )) = ( −  )2.
∙ Hinge loss:  ∈ R   ∈ {+1 −1}  and  (  (  )) = max{0  1 − ( )}.
∙ Logistic regression:  ∈ R  ∈{+1 −1}  and  (  (  )) = log1+ exp−( ).

1

Examples of the regularization term Ψ() include:

∙ ℓ1-regularization: Ψ() = ∥∥1 with  > 0. With ℓ1-regularization  we hope to get a

relatively sparse solution  i.e.  with many entries of  being zeroes.

∙ ℓ2-regularization: Ψ() = (/2)∥∥2
∙ Convex constraints: Ψ() is the indicator function of a closed convex set   i.e.  Ψ() = 0

2  for some  > 0.

if  ∈  and +∞ otherwise.

In this paper  we focus on online algorithms that process samples sequentially as they become avail-
able. Suppose at time   we have the most up-to-date weight . Whenever  is available  we can
evaluate the loss  (  )  and a subgradient  ∈ ∂ (  ) (here ∂ (  ) denotes the subdiffer-
ential of  with respect to ). Then we compute the new weight +1 based on these information.
For solving the problem (1)  the standard stochastic gradient descent (SGD) method takes the form

+1 =  −  ( + )  

(2)

where  is an appropriate stepsize  and  is a subgradient of Ψ at . The SGD method has been
very popular in the machine learning community due to its capability of scaling with large data sets
and good generalization performance observed in practice (e.g.  [3  4]).

Nevertheless  a main drawback of the SGD method is its lack of capability in exploiting problem
structure  especially for regularized learning problems. As a result  their low accuracy (compared
with interior-point method for batch optimization) often makes it hard to obtain the desired regu-
larization effect. An important example and motivation for this paper is ℓ1-regularized stochastic
learning  where Ψ() = ∥∥1. Even with relatively big   the SGD method (2) usually does not
generate sparse solutions because only in very rare cases two ﬂoat numbers add up to zero. Various
methods for rounding or truncating the solutions are proposed to generate sparse solutions (e.g.  [5]).

Inspired by recently developed ﬁrst-order methods for optimizing composite functions [6  7  8]  the
regularized dual averaging (RDA) method we develop exploits the full regularization structure at
each online iteration. In other words  at each iteration  the learning variables are adjusted by solving
a simple optimization problem that involves the whole regularization term  not just its subgradients.
For many practical learning problems  we actually are able to ﬁnd a closed-form solution for the
auxiliary optimization problem at each iteration. This means that the computational complexity per
iteration is ()  the same as the SGD method. Moreover  the RDA method converges to the optimal
solution of (1) with the optimal rate (1/√). If the the regularization function Ψ() is strongly
convex  we have the better rate (ln /) by setting appropriate parameters in the algorithm.

1.2 Regularized online optimization

In online optimization (e.g.  [9])  we make a sequence of decision   for  = 1  2  3  . . .. At each
time   a previously unknown cost function  is revealed  and we encounter a loss (). We
assume that the functions  are convex for all  ≥ 1. The goal of an online algorithm is to ensure
that the total cost up to each time   ࢣ
 =1 () 
the smallest total cost of any ﬁxed decision  from hindsight. The difference between these two
cost is called the regret of the online algorithm. Applications of online optimization include online
prediction of time series and sequential investment (e.g. [10]).

 =1 ()  is not much larger than minࢣ

In regularized online optimization  we add to each cost function a convex regularization func-
tion Ψ(). For any ﬁxed decision variable   consider the regret





() ≜

(3)

ࢣ =1 ( ) + Ψ( ) −

ࢣ =1 () + Ψ().

The RDA method we develop can also be used to solve the above regularized online optimization
problem  and it has an (√) regret bound. Again  if the regularization term Ψ() is strongly
convex  the regret bound is (ln ). However  the main advantage of the RDA method  compared
with other online algorithms  is its explicit regularization effect at each iteration.

2

Algorithm 1 Regularized dual averaging (RDA) method

input:

∙ a strongly convex function ℎ() with modulus 1 on domΨ  and 0 ∈ R  such that

0 = arg min



ℎ() ∈ Arg min



Ψ().

∙ a pre-determined nonnegative and nondecreasing sequence  for  ≥ 1.

initialize: 1 = 0  ¯0 = 0.
for  = 1  2  3  . . . do

1. Given the function   compute a subgradient ∈ ∂().
2. Update the average subgradient ¯:

¯ =

¯−1 +



 − 1



1


3. Compute the next iterate +1:

+1 = arg min

 ⟨¯  ⟩ + Ψ() +




ℎ()

end for

(4)

(5)

(6)

2 Regularized dual averaging method

ℎ( + (1 − )) ≤ ℎ() + (1 − )ℎ() −

In this section  we present the generic RDA method (Algorithm 1) for solving regularized stochastic
learning and online optimization problems  and give some concrete examples. To unify notation 
we write  (  ) as () for stochastic learning problems. The RDA method uses an auxiliary
strongly convex function ℎ(). A function ℎ is called strongly convex with respect to a norm ∥ ⋅ ∥ if
there exists a constant  > 0 such that
(7)
for all    ∈ domℎ. The constant  is called the convexity parameter  or the modulus of strong
convexity. In equation (4)  Arg min Ψ() denotes the convex set of minimizers of Ψ.
In Algorithm 1  step 1 is to compute a subgradient of  at   which is standard for all (sub)gradient-
based methods. Step 2 is the online version of computing average gradient ¯ (dual average). In
step 3  we assume that the functions Ψ and ℎ are simple  meaning that the minimization problem
in (6) can be solved with litter effort  especially if we are able to ﬁnd a closed-form solution for
+1. This assumption seems to be restrictive. But the following examples show that this indeed is
the case for many important learning problems in practice.

(1 − )∥ − ∥2 


2

 = √ 

If the regularization function Ψ() has convexity parameter  = 0 (i.e.  it is not strongly convex) 
we can choose a parameter  > 0 and use the sequence

 = 1  2  3  . . .

(8)
to obtain an (1/√) convergence rate for stochastic learning  or an (√) regret bound for online
optimization. The formal convergence theorems are given in Sections 3. Here are some examples:
∙ Nesterov’s dual averaging method. Let Ψ() be the indicator function of a close convex
set . This recovers the method of [11]: +1 = arg min∈⟨¯  ⟩ + (/√)ℎ().
∙ ℓ1-regularization: Ψ() = ∥∥1 for some  > 0. In this case  let 0 = 0 and

ℎ() =

1
2∥∥2

2 + ∥∥1 

where  ≥ 0 is a sparsity enhancing parameter. The solution to (6) can be found as

0

()

+1 =⎧⎨
⎩



√
 ¯()

−
=  + /√. Notice that the truncating threshold  is at least as large as .

  otherwise 

sign¯()

where RDA
This is the main difference of our method from related work  see Section 4.

 − RDA



 = 1  . . .    

(9)

¯()



if (cid:12)(cid:12)(cid:12)

 



(cid:12)(cid:12)(cid:12) ≤ RDA

3

If the regularization function Ψ() has convexity parameter  > 0  we can use any nonnegative 

nondecreasing sequence {}≥1 that is dominated by ln   to obtain an (ln /√) convergence
rate for stochastic learning  or an (ln ) regret bound for online optimization (see Section 3). For
simplicity  in the following examples  we use  = 0 for all  ≥ 1  and we do not need ℎ().
2 with    > 0. Then

2-regularization. Let Ψ() = ∥∥1 + (/2)∥∥2

∙ Mixed ℓ1/ℓ2

()

+1 =⎧⎨
⎩

0

−

1

 ¯()

 −  sign¯()



¯()

if (cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12) ≤  
  otherwise 

Of course  setting  = 0 gives the algorithm for pure ℓ2

2-regularization.

 = 1  . . .   .

∙ Kullback-Leibler (KL) divergence regularization: Ψ() = KL(∥)  where  lies in

the standard simplex   is a given probability distribution  and

KL(∥) ≜



ࢣ=1

() ln ()
() .

Note that KL(∥) is strongly convex with respect to ∥∥1 with modulus 1 (e.g.  [12]).
In this case 

()

() exp−
where +1 is a normalization parameter such thatࢣ

+1 =

+1

1

¯()

   
1

=1 ()

+1 = 1.

3 Regret bounds and convergence rates

We ﬁrst give bounds on the regret () deﬁned in (3)  when the RDA method is used for solving
regularized online optimization problem. To simplify notations  we deﬁne the following sequence:

Δ ≜ (0 − 1)ℎ(2) + 2 +

2
2

1

 + 

 

 = 1  2  3  . . .  

(10)

−1

ࢣ =0

where  and  are some given constants   is the convexity parameter of the regularization function
 =1 is the input sequence to the RDA method  which is nonnegative and nonde-
Ψ()  and {}
creasing. Notice that we just introduced an extra parameter 0. We require 0 > 0 to avoid blowup
of the ﬁrst term (when  = 0) in the summation in (10). This parameter does not appear in Algo-
rithm 1  instead  it is solely for the convenience of convergence analysis. In fact  whenever 1 > 0 
we can set 0 = 1  so that the term (0 − 1)ℎ(2) vanishes. We also note that 2 is determined
at the end of the step  = 1  so Δ1 is well deﬁned. Finally  for any given constant  > 0  we deﬁne

ℱ ≜ ∈ domΨ(cid:12)(cid:12) ℎ() ≤ 2 .

Theorem 1 Let the sequences {}
 =1 be generated by Algorithm 1. Assume there
is a constant  such that ∥∥∗ ≤  for all  ≥ 1  where ∥ ⋅ ∥∗ is the dual norm of ∥ ⋅ ∥. Then for
any  ≥ 1 and any  ∈ ℱ  we have
(11)

 =1 and {}

() ≤ Δ.

The proof of this theorem is given in the longer version of this paper [13]. Here we give some direct
consequences based on concrete choices of algorithmic parameters.
If the regularization function Ψ() has convexity parameter  = 0  then the sequence {}≥1
deﬁned in (8) together with 0 = 1 lead to
Δ = √2 +
 √.

2 1 +2√ − 2 ≤2 +

√ ≤ √2 +

2 1 +

2

2

2

1

−1

ࢣ =1

The best  that minimizes the above bound is ★ = /  which leads to

() ≤ 2√.

4

(12)

If the regularization function Ψ() is strongly convex  i.e.  with a convexity parameter  > 0  then
any nonnegative  nondecreasing sequence that is dominated by ln  will give an (ln ) regret bound.
We can simply choose ℎ() = (1/)Ψ() whenever needed. Here are several possibities:

∙ Positive constant sequences. For simplicity  let  =  for  ≥ 1 and 0 = 1. In this case 

Δ = 2 +

1
 ≤ 2 +

2
2

(1 + ln ).

∙ The logrithmic sequence. Let  = (1 + ln ) for  ≥ 1  and 0 = . In this case 

Δ = (1 + ln )2 +

2 (1 + ln ).
∙ The zero sequence  = 0 for  ≥ 1  with 0 = . Using ℎ() = (1/)Ψ()  we have

 + 1 + ln  ≤2 +

2 1 +

2

2

−1

1

Δ ≤ Ψ(2) +

1

 ≤

2
2



ࢣ =1

(6 + ln ) 

where we used Ψ(2) ≤ 22/  as proved in [13]. This bound does not depend on .

When Algorithm 1 is used to solve regularized stochastic learning problems  we have the following:
Theorem 2 Assume there exists an optimal solution ★ to the problem (1) that satisﬁes ℎ(★) ≤ 2
for some  > 0  and there is an  > 0 such that E∥∥2
∗ ≤ 2 for all  ∈ ∂ (  ) and  ∈ domΨ.
Then for any  ≥ 1  we have

2
2



ࢣ =1

ࢣ =1
2 1 +

2

E ( ¯) − (★) ≤

Δ


 

where

¯ =

 .

1




ࢣ =1

The proof of Theorem 2 is given in [13]. Further analysis for the cases  = 0 and  > 0 are the
same as before. We only need to divide every regret bound by  to obtain the convergence rate.

4 Related work

There have been several recent work that address online algorithms for regularized learning prob-
lems  especially with ℓ1-regularization; see  e.g.  [14  15  16  5  17].
In particular  a forward-
backward splitting method (FOBOS) is studied in [17] for solving the same problems we consider.
In an online setting  each iteration of the FOBOS method can be written as

+1 = arg min

  1

2 ∥ − ( − )∥2 + Ψ()  

(13)
where  is set to be (1/√) if Ψ() has convexity parameter  = 0  and (1/) if  > 0. The
RDA method and FOBOS use very different weights on the regularization term Ψ(): RDA in (6)
uses the original Ψ() without any scaling  while FOBOS scales Ψ() by a diminishing stepsize .
The difference is more clear in the special case of ℓ1-regularization  i.e.  when Ψ() = ∥∥1. For
this purpose  we consider the Truncated Gradient (TG) method proposed in [5]. The TG method
truncates the solutions obtained by the standard SGD method with an integer period  ≥ 1. More
speciﬁcally  each component of  is updated as
 − ()

  TG

()

(14)





   if mod(  ) = 0 

otherwise.

where TG

 =    the function mod(  ) means the remainder on division of  by   and



+1 = trnc()
()
 − ()
  ) =⎧⎨
⎩



trnc(  TG

0
 − TG




sign()



 

if ∣∣ ≤ TG
if TG
if ∣∣ > .

 < ∣∣ ≤  

When  = 1 and  = +∞  the TG method is the same as the FOBOS method (13). Now
used in (9): with  = (1/√)  we have
comparing the truncation thresholds TG
 = (1/√)RDA
. Therefore  the RDA method can generate much more sparse solutions.
TG
This is conﬁrmed by our computational experiments in Section 5.

and RDA







5

 = 0.01

 = 0.03

 = 0.1

 = 0.3

 = 1

 = 3

 = 10

SGD


TG


RDA


IPM
★

SGD
¯

TG
¯

RDA
¯

Figure 1: Sparsity patterns of the weight  and the average weight ¯ for classifying the digits 6
and 7 when varying the regularization parameter  from 0.01 to 10. The background gray represents
the value zero  bright spots represent positive values and dark spots represent negative values.

5 Computational experiments

We provide computational experiments for the ℓ1-RDA method using the MNIST dataset of hand-
written digits [18]. Each image from the dataset is represented by a 28 × 28 gray-scale pixel-map 
for a total of 784 features. Each of the 10 digits has roughly 6 000 training examples and 1 000
testing examples. No preprocessing of the data is employed.
We use ℓ1-regularized logistic regression to do binary classiﬁcation on each of the 45 pairs of dig-
its.
In the experiments  we compare the ℓ1-RDA method (9) with the SGD method (2) and the
TG/FOBOS method (14) with  = ∞. These three online algorithms have similar convergence rate
and the same order of computational complexity per iteration. We also compare them with the batch
optimization approach  using an efﬁcient interior-point method (IPM) developed by [19].

Each pair of digits have about 12 000 training examples and 2 000 testing examples. We use online
algorithms to go through the (randomly permuted) data only once  therefore the algorithms stop
at  = 12 000. We vary the regularization parameter  from 0.01 to 10. As a reference  the
maximum  for the batch optimization case [19] is mostly in the range of 30− 50 (beyond which the
optimal weights are all zeros). In the ℓ1-RDA method (9)  we use  = 5 000  and set  = 0 for basic
regularization  or  = 0.005 (effectively  = 25) for enhanced regularization effect. The tradeoffs
in choosing these parameters are further investigated in [13]. For the SGD and TG methods  we use a

constant stepsize  = (1/)Ý2/ . When  = /  which gives the best convergence bound (12)

6

Left:  = 1 for TG   = 0 for RDA

Right:  = 10 for TG   = 25 for RDA

 

SGD
TG
RDA

2000

4000

6000

8000

10000

12000

600

500

400

300

200

100

0
 
0

600

500

400

300

200

100

1
.
0
=

n
e
h
w
s
Z
N
N

0
1
=

n
e
h
w
s
Z
N
N

 

SGD
TG
RDA

2000

4000

6000

8000

10000

12000

600

500

400

300

200

100

0
 
0

600

500

400

300

200

100

0
0

2000

4000

6000

8000
Number of samples 

10000

12000

0
0

2000

4000

6000

8000
Number of samples 

10000

12000

Figure 2: Number of non-zeros (NNZs) in () for the three online algorithms (classifying 6 and 7).

for the RDA method  the corresponding  = (/)Ý2/ also gives the best convergence rate for

the SGD method (e.g.  [20]). In the TG method  the truncation period is set to  = 1 for basic
regularization  or  = 10 for enhanced regularization effect  as suggested in [5].
Figure 1 shows the sparsity patterns of the solutions  and ¯ for classifying the digits 6 and 7.
Both the TG and RDA methods were run with parameters for enhanced ℓ1-regularization:  = 10
for TG and  = 25 for RDA. The sparsity patterns obtained by the RDA method are most close to
the batch optimization results solved by IPM  especially for larger .
Figure 2 plots the number of non-zeros (NNZs) in () for different online algorithms. Only the
RDA method and TG with  = 1 give explicit zero weights at every step. In order to count the
NNZs in all other cases  we set a small threshold for rounding the weights to zero. Considering that
the magnitudes of the largest weights in Figure 1 are mostly on the order of 10−3  we set 10−5 as
the threshold and veriﬁed that rounding elements less than 10−5 to zero does not affect the testing
errors. Note that we do not truncate the weights for RDA and TG with  = 1 further  even if
some of their components are below 10−5. It can be seen that the RDA method maintains a much
more sparse () than the other two online algorithms. While the TG method generate more sparse
solutions than the SGD method when  is large  the NNZs in () oscillates with a very big range.
In contrast  the RDA method demonstrate a much more smooth variation in the NNZs.

Figure 3 illustrates the tradeoffs between sparsity and testing error rates for classifying 6 and 7.
Since the performance of the online algorithms vary when the training data are given in different
permutations  we run them on 100 randomly permuted sequences of the same training set  and
plot the means and standard deviations shown as error bars. For the SGD and TG methods  the
testing error rates of  vary a lot for different random sequences. In contrast  the RDA method
demonstrates very robust performance (small standard deviations) for    even though the theorems
only give performance bound for the averaged weight ¯ . Note that ¯ obtained by SGD and TG
have much smaller error rates than those of RDA and batch optimization  especially for larger .
The explanation is that these lower error rates are obtained with much more nonzero features.

Figure 4 shows summary of classiﬁcation results for all the 45 pairs of digits. For clarity of presenta-
tion  here we only plot results of the ℓ1-RDA method and batch optimization using IPM. (The NNZs
obtained by SGD and TG are mostly above the limit of the vertical axes  which is set at 200). We
see that  overall  the solutions obtained by the ℓ1-RDA method demonstrate very similar tradeoffs
between sparsity and testing error rates as rendered by the batch optimization solutions.

7

Last weight 

Average weight ¯

Last weight 

Average weight ¯

)

%

(

s
e
t
a
r

r
o
r
r
E

SGD
TG ( = 1)
RDA ( = 0)
IPM

10

1

 
0.01

0.1
600

0.1

1

10

s
Z
N
N

400

200

 

4

3

2

1

 
0.01

0
600

400

200

SGD
TG ( = 1)
RDA ( = 0)
IPM

0.1

1

10

 

10

1

 
0.01

0.1
600

400

200

SGD
TG ( = 10)
RDA ( = 25)
IPM

0.1

1

10

 

4

3

2

1

 
0.01

0
600

400

200

 

SGD
TG ( = 10)
RDA ( = 25)
IPM

0.1

1

10

0

0.01

0.1



1

10

0

0.01

0.1



1

10

0

0.01

0.1



1

10

0

0.01

0.1



1

10

Figure 3: Tradeoffs between testing error rates and NNZs in solutions (for classifying 6 and 7).

2

3

4

5

6

7

8

9

0.1

1

10

0.1

1

10

2

0

4

2

0

5

0

0.1

1

10

0.1

1

10

0.1

1

10

1

0.5

0

2

1

0

5

0

5

5

0

5

0.1

1

10

0.1

1

10

0.1

1

10

0

0.1

1

10

0.1

1

10

5

0

10

0.1

1

10

4

2

0

2

1

0

5

0

5

0.1

1

10

0.1

1

10

0.1

1

10

2

0

4

2

0

5

0

5

0.1

1

10

0.1

1

10

0.1

1

10

0

0.1

1

10

0

0.1

1

10

0

0.1

1

10

0

0.1

1

10

0

1

1

0.5

0

0.1

1

10

4

2

0

5

0

0

1

2

3

4

5

6

7

8

9

2

1

0

2

0

5

0.1

1

10

0.1

1

10

0.1

1

10

0.1

1

10

0.1

1

10

0

0.1

1

10

0.1

1

10

5

0

10

0.1

1

10

0.1

1

10

0

0.1

1

10

2

0

5

0

10

5

0

10

5

0

5

0

10

10

5

0

5

0.1

1

10

0.1

1

10

0

0.1

1

10

0.1

1

10

10

5

0

10

5

0

0.1

1

10

0.1

1

10

200
150
100
50
0

200
150
100
50
0

200
150
100
50
0

200
150
100
50
0

200
150
100
50
0

200
150
100
50
0

200
150
100
50
0

200
150
100
50
0

200
150
100
50
0

10

5

0

0.1

1

10

5

0

5

0

0.1

1

10

0.1

1

10

5

0

5

0

5

0.1

1

10

0.1

1

10

0

0.1

1

10

0

0.1

1

10

5

0

5

0

Figure 4: Binary classiﬁcation for all 45 pairs of digits. The images in the lower-left triangular area
show sparsity patterns of  with  = 1  obtained by the ℓ1-RDA with  = 25. The plots in
the upper-right triangular area show tradeoffs between sparsity and testing error rates  by varying 
from 0.1 to 10. The solid circles and solid squares show error rates and NNZs in    respectively 
using IPM for batch optimization. The hollow circles and hollow squares show error rates and
NNZs of    respectively  using the ℓ1-RDA method. The vertical bars centered at hollow circles
and squares show standard deviations by running on 100 random permutations of the training data.

8

References

[1] L. Bottou and O. Bousquet. The tradeoffs of large scale learning.

In J.C. Platt  D. Koller 
Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20 
pages 161–168. MIT Press  Cambridge  MA  2008.

[2] S. Shalev-Shwartz and N. Srebro. SVM optimization: Inverse dependence on training set size.

In Proceedings of the 25th International Conference on Machine Learning (ICML)  2008.

[3] L. Bottou and Y. LeCun. Large scale online learning. In S. Thrun  L. Saul  and B. Sch¨olkopf 
editors  Advances in Neural Information Processing Systems 16. MIT Press  Cambridge  MA 
2004.

[4] T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent
algorithms. In Proceedings of the 21st International Conference on Machine Learning (ICML) 
Banff  Alberta  Canada  2004.

[5] J. Langford  L. Li  and T. Zhang. Sparse online learning via truncated gradient. Journal of

Machine Learning Research  10:777–801  2009.

[6] Yu. Nesterov. Gradient methods for minimizing composiite objective function. CORE Dis-
cussion Paper 2007/76  Catholic University of Louvain  Center for Operations Research and
Econometrics  2007.

[7] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Sub-

mitted to SIAM Journal on Optimization  2008.

[8] A. Beck and M. Teboulle. A fast iterative shrinkage-threshold algorithm for linear inverse

problems. Technical report  Technion  2008. To appear in SIAM Journal on Image Sciences.

[9] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML)  pages 928–
936  Washington DC  2003.

[10] N. Cesa-Bianchi and G. Lugosi. Predictioin  Learning  and Games. Cambridge University

Press  2006.

[11] Yu. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Program-
ming  120(1):221–259  2009. Appeared early as CORE discussion paper 2005/67  Catholic
University of Louvain  Center for Operations Research and Econometrics.

[12] Yu. Nesterov. Smooth minimization of nonsmooth functions. Mathematical Programming 

103:127–152  2005.

[13] L. Xiao. Dual averaging method for regularized stochastic learning and online optimization.

Technical Report MSR-TR-2009-100  Microsoft Research  2009.

[14] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the ℓ1-
ball for learning in high dimensions. In Proceedings of the 25th International Conference on
Machine Learning (ICML)  pages 272–279  2008.

[15] P. Carbonetto  M. Schmidt  and N. De Freitas. An interior-point stochastic approximation
method and an 1-regularized delta rule. In D. Koller  D. Schuurmans  Y. Bengio  and L. Bot-
tou  editors  Advances in Neural Information Processing Systems 21  pages 233–240. MIT
Press  2009.

[16] S. Balakrishnan and D. Madigan. Algorithms for sparse linear classiﬁers in the massive data

setting. Journal of Machine Learning Research  9:313–337  2008.

[17] J. Duchi and Y. Singer. Efﬁcient learning using forward-backward splitting. In Proceedings of

Neural Information Processing Systems  December 2009.

[18] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE  86(11):2278–2324  1998. Dataset available at
http://yann.lecun.com/exdb/mnist.

[19] K. Koh  S.-J. Kim  and S. Boyd. An interior-point method for large-scale ℓ1-regularized logistic

regression. Journal of Machine Learning Research  8:1519–1555  2007.

[20] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

9

,Andrea Frome
Greg Corrado
Jon Shlens
Samy Bengio
Jeff Dean
Marc'Aurelio Ranzato
Tomas Mikolov
Uygar Sümbül
Douglas Roossien
Dawen Cai
Fei Chen
Nicholas Barry
John Cunningham
Edward Boyden
Liam Paninski
Ryuichi Kiryo
Gang Niu
Marthinus du Plessis
Masashi Sugiyama