2019,Neural Temporal-Difference Learning Converges to Global Optima,Temporal-difference learning (TD)  coupled with neural networks  is among the most fundamental building blocks of deep reinforcement learning. However  due to the nonlinearity in value function approximation  such a coupling leads to nonconvexity and even divergence in optimization. As a result  the global convergence of neural TD remains unclear. In this paper  we prove for the first time that neural TD converges at a sublinear rate to the global optimum of the mean-squared projected Bellman error for policy evaluation. In particular  we show how such global convergence is enabled by the overparametrization of neural networks  which also plays a vital role in the empirical success of neural TD. Beyond policy evaluation  we establish the global convergence of neural (soft) Q-learning  which is further connected to that of policy gradient algorithms.,Neural Temporal-Difference Learning

Converges to Global Optima

Qi Cai ⇤

Zhuoran Yang †

Jason D. Lee ‡

Zhaoran Wang ⇤

Abstract

Temporal-difference learning (TD)  coupled with neural networks  is among the
most fundamental building blocks of deep reinforcement learning. However  due
to the nonlinearity in value function approximation  such a coupling leads to non-
convexity and even divergence in optimization. As a result  the global convergence
of neural TD remains unclear. In this paper  we prove for the ﬁrst time that neural
TD converges at a sublinear rate to the global optimum of the mean-squared pro-
jected Bellman error for policy evaluation. In particular  we show how such global
convergence is enabled by the overparametrization of neural networks  which also
plays a vital role in the empirical success of neural TD.1

1

Introduction

Given a policy  temporal-different learning (TD) [49] aims to learn the corresponding (action-
)value function by following the semigradients of the mean-squared Bellman error in an online
manner. As the most-used policy evaluation algorithm  TD serves as the “critic” component of many
reinforcement learning algorithms  such as the actor-critic algorithm [31] and trust-region policy
optimization [47]. In particular  in deep reinforcement learning  TD is often applied to learn value
functions parametrized by neural networks [36  39  24]  which gives rise to neural TD. As policy
improvement relies crucially on policy evaluation  the optimization efﬁciency and statistical accuracy
of neural TD are critical to the performance of deep reinforcement learning. Towards theoretically
understanding deep reinforcement learning  the goal of this paper is to characterize the convergence
of neural TD.
Despite the broad applications of neural TD  its convergence remains rarely understood. Even
with linear value function approximation  the nonasymptotic convergence of TD remains open until
recently [6  33  14  48  45]  although its asymptotic convergence is well understood [28  55  9  32 
8]. Meanwhile  with nonlinear value function approximation  TD is known to diverge in general
[4  11  55]. To remedy this issue  [7] propose nonlinear (gradient) TD  which uses the tangent
vectors of nonlinear value functions in place of the feature vectors in linear TD. Unlike linear TD 
which converges to the global optimum of the mean-squared projected Bellman error (MSPBE) 
nonlinear TD is only guaranteed to converge to a local optimum asymptotically. As a result  the
statistical accuracy of the value function learned by nonlinear TD remains unclear. In contrast to such
conservative theory  neural TD  which straightforwardly combines TD with neural networks without
the explicit local linearization in nonlinear TD  often learns a desired value function that generalizes
well to unseen states in practice [18  2  26]. Hence  a gap separates theory from practice.

⇤Department of Industrial Engineering and Management Sciences  Northwestern University
†Department of Operations Research and Financial Engineering  Princeton University
‡Department of Electronic Engineering  Princeton University
1Beyond policy evaluation  we establish the global convergence of neural (soft) Q-learning  which is further
connected to that of policy gradient algorithms. See https://arxiv.org/abs/1905.10027 for the full
version.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

There exist three obstacles towards closing such a theory-practice gap: (i) MSPBE has an expectation
over the transition dynamics within the squared loss  which forbids the construction of unbiased
stochastic gradients [50]. As a result  even with linear value function approximation  TD largely
eludes the classical optimization framework  as it follows biased stochastic semigradients. (ii) When
the value function is parametrized by a neural network  MSPBE is nonconvex in the weights of the
neural network  which may introduce undesired stationary points such as local optima and saddle
points [30]. As a result  even an ideal algorithm that follows the population gradients of MSPBE may
get trapped. (iii) Due to the interplay between the bias in stochastic semigradients and the nonlinearity
in value function approximation  neural TD may even diverge [4  11  55]  instead of converging to
an undesired stationary point  as it lacks the explicit local linearization in nonlinear TD [7]. Such
divergence is also not captured by the classical optimization framework.

Contribution. Towards bridging theory and practice  we establish the ﬁrst nonasymptotic global rate
of convergence of neural TD. In detail  we prove that randomly initialized neural TD converges to the
global optimum of MSPBE at the rate of 1/T with population semigradients and at the rate of 1/pT
with stochastic semigradients. Here T is the number of iterations and the (action-)value function is
parametrized by a sufﬁciently wide two-layer neural network. Moreover  we prove that the projection
in MSPBE allows for a sufﬁciently rich class of functions  which has the same representation power
of a reproducing kernel Hilbert space associated with the random initialization. As a result  for a
broad class of reinforcement learning problems  neural TD attains zero MSPBE.
At the core of our analysis is the overparametrization of the two-layer neural network for value
function approximation [59  41  1  3]  which enables us to circumvent the three obstacles above. In
particular  overparametrization leads to an implicit local linearization that varies smoothly along the
solution path  which mirrors the explicit one in nonlinear TD [7]. Such an implicit local linearization
enables us to circumvent the third obstacle of possible divergence. Moreover  overparametrization
allows us to establish a notion of one-point monotonicity [25  19] for the semigradients followed by
neural TD  which ensures its evolution towards the global optimum of MSPBE along the solution
path. Such a notion of monotonicity enables us to circumvent the ﬁrst and second obstacles of bias
and nonconvexity. Broadly speaking  our theory backs the empirical success of overparametrized
neural networks in deep reinforcement learning. In particular  we show that instead of being a curse 
overparametrization is indeed a blessing for minimizing MSPBE in the presence of bias  nonconvexity 
and even divergence.

More Related Work. There is a large body of literature on the convergence of linear TD under
both asymptotic [28  55  9  32  8] and nonasymptotic [6  33  14  48] regimes. See [16] for a detailed
survey. In particular  our analysis is based on the recent breakthrough in the nonasymptotic analysis
of linear TD [6] and its extension to linear Q-learning [60]. An essential step of our analysis is
bridging the evolution of linear TD and neural TD through the implicit local linearization induced by
overparametrization.
To incorporate nonlinear value function approximation into TD  [7] propose the ﬁrst convergent
nonlinear TD based on explicit local linearization  which however only converges to a local optimum
of MSPBE. See [21  5] for a detailed survey. In contrast  we prove that  with the implicit local
linearization induced by overparametrization  neural TD  which is simpler to implement and more
widely used in deep reinforcement learning than nonlinear TD  provably converges to the global
optimum of MSPBE.
There exist various extensions of TD  including least-squares TD [12  10  34  22  56] and gradient
TD [51  52  7  37  17  57  54]. In detail  least-squares TD is based on batch update  which loses the
computational and statistical efﬁciency of the online update in TD. Meanwhile  gradient TD follows
unbiased stochastic gradients  but at the cost of introducing another optimization variable. Such a
reformulation leads to bilevel optimization  which is less stable in practice when combined with
neural networks [42]. As a result  both extensions of TD are less widely used in deep reinforcement
learning [18  2  26]. Moreover  when using neural networks for value function approximation  the
convergence to the global optimum of MSPBE remains unclear for both extensions of TD.
Our work is also related to the recent breakthrough in understanding overparametrized neural
networks  especially their generalization error [59  41  1  3]. See [20] for a detailed survey. In
particular  [15  1  3  13  29  35] characterize the implicit local linearization in the context of supervised
learning  where we train an overparametrized neural network by following the stochastic gradients

2

of the mean-squared error. In contrast  neural TD does not follow the stochastic gradients of any
objective function  hence leading to possible divergence  which makes the convergence analysis more
challenging.

2 Background

In Section 2.1  we brieﬂy review policy evaluation in reinforcement learning. In Section 2.2  we
introduce the corresponding optimization formulations.

2.1 Policy Evaluation
We consider a Markov decision process (S A P  r  )  in which an agent interacts with the environ-
ment to learn the optimal policy that maximizes the expected total reward. At the t-th time step  the
agent has a state st 2S and takes an action at 2A . Upon taking the action  the agent enters the
next state st+1 2S according to the transition probability P(·| st  at) and receives a random reward
rt = r(st  at) from the environment. The action that the agent takes at each state is decided by a
policy ⇡ : S!   where  is the set of all probability distributions over A. The performance of
policy ⇡ is measured by the expected total reward  J(⇡) = E[P1t=0 trt | at ⇠ ⇡(st)]  where < 1
is the discount factor.
Given policy ⇡  policy evaluation aims to learn the following two functions  the value function
V ⇡(s) = E[P1t=0 trt | s0 = s  at ⇠ ⇡(st)] and the action-value function (Q-function) Q⇡(s  a) =
E[P1t=0 trt | s0 = s  a0 = a  at ⇠ ⇡(st)]. Both functions form the basis for policy improvement.

Without loss of generality  we focus on learning the Q-function in this paper. We deﬁne the Bellman
evaluation operator 

T ⇡Q(s  a) = E[r(s  a) + Q(s0  a0)| s0 ⇠P (·| s  a)  a0 ⇠ ⇡(s0)] 

(2.1)

for which Q⇡ is the ﬁxed point  that is  the solution to the Bellman equation Q = T ⇡Q.
2.2 Optimization Formulation
Corresponding to (2.1)  we aim to learn Q⇡ by minimizing the mean-squared Bellman error (MSBE) 

(2.2)

min

✓

MSBE(✓) = E(s a)⇠µ⇥bQ✓(s  a) T ⇡bQ✓(s  a)2⇤ 

min

✓

where the Q-function is parametrized as bQ✓ with parameter ✓. Here µ is the stationary distribution

of (s  a) corresponding to policy ⇡. Due to Q-function approximation  we focus on minimizing the
following surrogate of MSBE  namely the projected mean-squared Bellman error (MSPBE) 

MSPBE(✓) = E(s a)⇠µ⇥bQ✓(s  a)  ⇧FT ⇡bQ✓(s  a)2⇤.
(2.3)
Here ⇧F is the projection onto a function class F. For example  for linear Q-function approximation
[49]  F takes the form {bQ✓0
: ✓0 2 ⇥}  where bQ✓0 is linear in ✓0 and ⇥ is the set of feasible
parameters. As another example  for nonlinear Q-function approximation [7]  F takes the form
{bQ✓ + r✓bQ>✓ (✓0  ✓) : ✓0 2 ⇥}  which consists of the local linearization of bQ✓0 at ✓.

Throughout this paper  we assume that we are able to sample tuples in the form of (s  a  r  s0  a0)
from the stationary distribution of policy ⇡ in an independent and identically distributed manner 
although our analysis can be extended to handle temporal dependence using the proof techniques of
[6]. With a slight abuse of notation  we use µ to denote the stationary distribution of (s  a  r  s0  a0)
corresponding to policy ⇡ and any of its marginal distributions.

3 Neural Temporal-Difference Learning

TD updates the parameter ✓ of the Q-function by taking the stochastic semigradient descent step
[49  53  50] 

✓0 ✓  ⌘ ·bQ✓(s  a)  r(s  a)  bQ✓(s0  a0) ·r ✓bQ✓(s  a) 

3

(3.1)

which corresponds to the MSBE in (2.2). Here (s  a  r  s0  a0) ⇠ µ and ⌘> 0 is the stepsize. In
a more general context  (3.1) is referred to as TD(0). In this paper  we focus on TD(0)  which is
abbreviated as TD  and leave the extension to TD() to future work.
In the sequel  we denote the state-action pair (s  a) 2S⇥A by a vector x 2X✓ Rd with d > 2. We
consider S to be continuous and A to be ﬁnite. Without loss of generality  we assume that kxk2 = 1
and |r(x)| is upper bounded by a constant r for any x 2X . We use a two-layer neural network

br(W >r x)

(3.2)

bQ(x; W ) =

1
pm

mXr=1

to parametrize the Q-function. Here  is the rectiﬁed linear unit (ReLU) activation function (y) =
max{0  y} and the parameter ✓ = (b1  . . .   bm  W1  . . .   Wm) are initialized as br ⇠ Unif({1  1})
and Wr ⇠ N (0  Id/d) for any r 2 [m] independently. During training  we only update W =
(W1  . . .   Wm) 2 Rmd  while keeping b = (b1  . . .   bm) 2 Rm ﬁxed as the random initialization.
To ensure global convergence  we incorporate an additional projection step with respect to W . See
Algorithm 1 for a detailed description.

t+2 · W + 1

t+2 · W (t + 1)

Initialization: SB = {W 2 Rmd : kW  W (0)k2  B} (B > 0)

Sample a tuple (s  a  r  s0  a0) from the stationary distribution µ of policy ⇡
Let x = (s  a)  x0 = (s0  a0)

Algorithm 1 Neural TD
1: Initialization: br ⇠ Unif({1  1})  Wr(0) ⇠ N (0  Id/d) (r 2 [m])  W = W (0) 
2: For t = 0 to T  2:
3:
4:
5:
6:
7:
8:
9: End For

Bellman residual calculation:  bQ(x; W (t))  r  bQ(x0; W (t))
TD update:fW (t + 1) W (t)  ⌘ ·r WbQ(x; W (t))
Projection: W (t + 1) argminW2SB kW fW (t + 1)k2
Averaging: W t+1
10: Output: bQout(·) bQ(· ; W )
E(s a r s0 a0)⇠µ⇥bQ✓(s  a)  r(s  a)  bQ✓(s0  a0) ·r ✓bQ✓(s  a)⇤
= E(s a)⇠µ⇥bQ✓(s  a)  E[r(s  a) + Q(s0  a0)| s0 ⇠P (·| s  a)  a0 ⇠ ⇡(s0)] ·r ✓bQ✓(s  a)⇤
= E(s a)⇠µ⇥bQ✓(s  a) T ⇡bQ✓(s  a)
}

To understand the intuition behind the global convergence of neural TD  note that for the TD update
in (3.1)  we have from (2.1) that

Here (i) is the Bellman residual at (s  a)  while (ii) is the gradient of the ﬁrst term in (i). Although the
TD update in (3.1) resembles the stochastic gradient descent step for minimizing a mean-squared
error  it is not an unbiased stochastic gradient of any objective function. However  we show that the
TD update yields a descent direction towards the global optimum of the MSPBE in (2.3). Moreover 
as the neural network becomes wider  the function class F that ⇧F projects onto in (2.3) becomes
richer. Correspondingly  the MSPBE reduces to the MSBE in (2.2) as the projection becomes closer
to identity  which implies the recovery of the desired Q-function Q⇡ such that Q⇡ = T ⇡Q⇡. See
Section 4 for a more rigorous characterization.

⇤.
·r ✓bQ✓(s  a)
}
|

{z

(ii)

(3.3)

|

(i)

{z

4 Main Results

In Section 4.1  we characterize the global optimality of the stationary point attained by Algorithm 1
in terms of minimizing the MSPBE in (2.3) and its other properties. In Section 4.2  we establish the
nonasymptotic global rates of convergence of neural TD to the global optimum of the MSPBE when
following the population semigradients in (3.3) and the stochastic semigradients in (3.1)  respectively.
We use the subscript Eµ[·] to denote the expectation over the randomness of the tuple (s  a  r  s  a0)
(or its concise form (x  r  x0)) conditional on all other randomness  e.g.  the random initialization

4

and the random current iterate. Meanwhile  we use the subscript Einit µ[·] when we are taking the
expectation over all randomness  including the random initialization.

4.1 Properties of Stationary Point
We consider the population version of the TD update in Line 6 of Algorithm 1 

fW (t + 1) W (t)  ⌘ · Eµ⇥x  r  x0; W (t) ·r WbQx; W (t)⇤ 

where µ is the stationary distribution and (x  r  x0; W (t)) = bQ(x; W (t)) r  bQ(x0; W (t)) is the

Bellman residual at (x  r  x0). The stationary point W † of (4.1) satisﬁes the following stationarity
condition 

(4.1)

Also  note that

Eµ[(x  r  x0; W †) ·r WbQ(x; W †)]>(W  W †)  0 
bQ(x; W ) =

mXr=1
and rWrbQ(x; W ) = br 1{W >r x > 0}x almost everywhere in Rmd. Meanwhile  recall that SB =
{W 2 Rmd : kW  W (0)k2  B}. We deﬁne the function class

for any W 2 SB.

(4.2)

br 1{W >r x > 0}W >r x

br(W >r x) =

mXr=1

1
pm

1
pm

F†B m =⇢ 1

pm

mXr=1

br 1{(W †r )>x > 0}W >r x : W 2 SB 

(4.3)

equivalent form

⌦bQ(· ; W †) T ⇡bQ(· ; W †)  f (·)  bQ(· ; W †)↵µ  0 

which consists of the local linearization of bQ(x; W ) at W = W †. Then (4.2) takes the following
which implies bQ(· ; W †) =⇧
F†B mT ⇡bQ(· ; W †) by the deﬁnition of the projection induced by h· ·iµ.
By (2.3)  bQ(· ; W †) is the global optimum of the MSPBE that corresponds to the projection onto
F†B m.
Intuitively  when using an overparametrized neural network with width m ! 1  the average
variation in each Wr diminishes to zero. Hence  roughly speaking  we have 1{Wr(t)>x > 0} =
1{Wr(0)>x > 0} with high probability for any t 2 [T ]. As a result  the function class F†B m deﬁned
in (4.3) approximates

for any f 2F †B m 

(4.4)

FB m =⇢ 1

pm

mXr=1

br 1{Wr(0)>x > 0}W >r x : W 2 SB.

(4.5)

In the sequel  we show that  to characterize the global convergence of Algorithm 1 with a sufﬁciently
large m  it sufﬁces to consider FB m in place of F†B m  which simpliﬁes the analysis  since the
distribution of W (0) is given. To this end  we deﬁne the approximate stationary point W ⇤ with
respect to the function class FB m deﬁned in (4.5).
Deﬁnition 4.1 (Approximate Stationary Point W ⇤). If W ⇤ = (W ⇤1   . . .   W ⇤m) 2 SB satisﬁes

where we deﬁne

Eµ[0(x  r  x0; W ⇤) ·r WbQ0(x; W ⇤)]>(W  W ⇤)  0 

for any W 2 SB 

(4.8)
then we say that W ⇤ is an approximate stationary point of the population update in (4.1). Here W ⇤
depends on the random initialization b = (b1  . . .   bm) and W (0) = (W1(0)  . . .   Wm(0)).

bQ0(x; W ) =
br 1{Wr(0)>x > 0}W >r x 
0(x  r  x0; W ) = bQ0(x; W )  r  bQ0(x0; W ) 

1
pm

mXr=1

(4.6)

(4.7)

5

The next lemma proves that such an approximate stationary point always exists  since it corresponds
to the ﬁxed point of the operator ⇧FB mT ⇡  which is a contraction in the `2-norm associated with
the stationary distribution µ.
Lemma 4.2 (Existence and Optimality of W ⇤). There exists an approximate stationary point W ⇤ for
any b 2 {1  1}m and W (0) 2 Rmd. Also  bQ0(· ; W ⇤) is the global optimum of the MSPBE that
corresponds to the projection onto FB m in (4.5).
Proof. See Appendix B.1 for a detailed proof.

4.2 Global Convergence
In this section  we establish the main results on the global convergence of neural TD in Algorithm 1.
We ﬁrst lay out the following regularity condition on the stationary distribution µ.
Assumption 4.3 (Regularity of Stationary Distribution µ). There exists a constant c0 > 0 such that
for any ⌧  0 and w ⇠ N (0  Id/d)  it holds almost surely that

(4.9)

Eµ⇥1{|w>x| ⌧} w⇤  c0 · ⌧/kwk2.

Assumption 4.3 regularizes the density of µ in terms of the marginal distribution of x. In particular  it
is straightforwardly implied when the density of µ in terms of state s is upper bounded.

Population Update: The next theorem establishes the nonasymptotic global rate of convergence of
neural TD when it follows population semigradients. Recall that the approximate stationary point W ⇤

and bQ0(· ; W ⇤) are deﬁned in Deﬁnition 4.1. Also  B is the radius of the set of feasible W   which is
deﬁned in Algorithm 1  T is the number of iterations   is the discount factor  and m is the width of
the neural network in (3.2).
Theorem 4.4 (Convergence of Population Update). We set ⌘ = (1)/8 in Algorithm 1 and replace
the TD update in Line 6 by the population update in (4.1). Under Assumption 4.3  the output bQout of
Algorithm 1 satisﬁes

+ O(B3m1/2 + B5/2m1/4) 

Einit µ⇥bQout(x)  bQ0(x; W ⇤)2⇤ 

16B2
(1  )2T

where the expectation is taken with respect to all randomness  including the random initialization and
the stationary distribution µ.

Proof. The key to the proof of Theorem 4.4 is the one-point monotonicity of the population semigra-

C.5 for a detailed proof.

dient g(t)  which is established through the local linearization bQ0(x; W ) of bQ(x; W ). See Appendix

Stochastic Update: To further prove the global convergence of neural TD when it follows stochastic
semigradients  we ﬁrst establish an upper bound of their variance  which affects the choice of the
stepsize ⌘. For notational simplicity  we deﬁne the stochastic and population semigradients as

g(t) = x  r  x0; W (t) ·r WbQx; W (t) 

Lemma 4.5 (Variance Bound). There exists 2
semigradient is upper bounded as Einit µ[kg(t)  g(t)k2
Proof. See Appendix B.2 for a detailed proof.

(4.10)
g = O(B2) such that the variance of the stochastic

g(t) = Eµ[g(t)].

2]  2

g for any t 2 [T ].

Based on Theorem 4.4 and Lemma 4.5  we establish the global convergence of neural TD in Algorithm
1.

Theorem 4.6 (Convergence of Stochastic Update). We set ⌘ = min{(1)/8  1/pT} in Algorithm
1. Under Assumption 4.3  the output bQout of Algorithm 1 satisﬁes

+ O(B3m1/2 + B5/2m1/4).

16(B2 + 2
g)

Einit µ⇥bQout(x)  bQ0(x; W ⇤)2⇤ 

(1  )2pT

6

Proof. See Appendix C.6 for a detailed proof.

As the width of the neural network m ! 1  Lemma 4.2 implies that bQ0(· ; W ⇤) is the global
optimum of the MSPBE in (2.3) with a richer function class FB 1 to project onto. In fact  the
function class FB 1  bQ(· ; W (0)) is a subset of an RKHS with H-norm upper bounded by B.
Here bQ(· ; W (0)) is deﬁned in (3.2). See Appendix A.2 for a more detailed discussion on the
representation power of FB 1. Therefore  if the desired Q-function Q⇡(·) falls into FB 1  it is the
global optimum of the MSPBE. In such a case  by Lemma 4.2 and Theorem 4.6  we approximately
obtain Q⇡(·) = bQ0(· ; W ⇤) through bQout(·).
More generally  the following proposition quantiﬁes the distance between bQ0(· ; W ⇤) and Q⇡(·) in
the case that Q⇡(·) does not fall into the function class FB m. In particular  it states that the `2-norm
distance kbQ0(· ; W ⇤)  Q⇡(·)kµ is upper bounded by the distance between Q⇡(·) and FB m.
Proposition 4.7 (Convergence of Stochastic Update to Q⇡). It holds that kbQ0(· ; W ⇤)  Q⇡(·)kµ 
(1  )1 ·k ⇧FB mQ⇡(·)  Q⇡(·)kµ  which by Theorem 4.6 implies
2Einit µ⇥⇧FB mQ⇡(x)  Q⇡(x)2⇤

32(B2 + 2
g)

Einit µ⇥bQout(x)  Q⇡(x)2⇤ 

+ O(B3m1/2 + B5/2m1/4).

(1  )2pT

+

(1  )2

Proof. See Appendix B.3 for a detailed proof.

Proposition 4.7 implies that if Q⇡(·) 2F B 1  then bQout(·) ! Q⇡(·) as T  m ! 1. In other words 

neural TD converges to the global optimum of the MSPBE in (2.3)  or equivalently  the MSBE in
(2.2)  both of which have objective value zero.

5 Proof Sketch

In the sequel  we sketch the proofs of Theorems 4.4 and 4.6 in Section 4.

5.1

Implicit Local Linearization via Overparametrization

Recall that as deﬁned in (4.7)  bQ0(x; W ) takes the form

bQ0(x; W ) =( x)>W 

where (x) =

1

pm ·1{W1(0)>x > 0}x  . . .   1{Wm(0)>x > 0}x 2 Rmd 

which is linear in the feature map (x). In other words  with respect to W   bQ0(x; W ) linearizes the
neural network bQ(x; W ) deﬁned in (3.2) locally at W (0). The following lemma characterizes the
difference between bQ(x; W (t))  which is along the solution path of neural TD in Algorithm 1  and
its local linearization bQ0(x; W (t)). In particular  we show that the error of such a local linearization
diminishes to zero as m ! 1. For notational simplicity  we use bQt(x) to denote bQ(x; W (t)) in
the sequel. Note that by (4.7) we have bQ0(x) = bQ(x; W (0)) = bQ0(x; W (0)). Recall that B is the
radius of the set of feasible W in (4.5).
Lemma 5.1 (Local Linearization of Q-Function). There exists a constant c1 > 0 such that for any
t 2 [T ]  it holds that

Einit µhbQt(x)  bQ0x; W (t)2i  4c1B3 · m1/2.

Proof. See Appendix C.1 for a detailed proof.

7

As a direct consequence of Lemma 5.1  the next lemma characterizes the effect of local linearization
on population semigradients. Recall that g(t) is deﬁned in (4.10). We denote by g0(t) the locally

linearized population semigradient  which is deﬁned by replacing bQt(x) in g(t) with its local
linearization bQ0(x; W (t)). In other words  by (4.10)  (4.7)  and (4.8)  we have
g(t) = Eµ⇥x  r  x0; W (t) ·r WbQx; W (t)⇤ 
g0(t) = Eµ⇥0x  r  x0; W (t) ·r WbQ0x; W (t)⇤.

(5.1)
(5.2)
Lemma 5.2 (Local Linearization of Semigradient). Let r be the upper bound of the reward r(x) for
any x 2X . There exists a constant c2 > 0 such that for any t 2 [T ]  it holds that
2⇤  (56c1B3 + 24c2B + 6c1Br2) · m1/2.

Einit⇥kg(t)  g0(t)k2

Proof. See Appendix C.2 for a detailed proof.

Lemmas 5.1 and 5.2 show that the error of local linearization diminishes as the degree of over-
parametrization increases along m. As a result  we do not require the explicit local linearization in
nonlinear TD [7]. Instead  we show that such an implicit local linearization sufﬁces to ensure the
global convergence of neural TD.

5.2 Proofs for Population Update

The characterization of the locally linearized Q-function in Lemma 5.1 and the locally linearized
population semigradients in Lemma 5.2 allows us to establish the following descent lemma  which
extends Lemma 3 of [6] for characterizing linear TD.
Lemma 5.3 (Population Descent Lemma). For {W (t)}t2[T ] in Algorithm 1 with the TD update in
Line 6 replaced by the population update in (4.1)  it holds that

kW (t + 1)  W ⇤k2

2  kW (t)  W ⇤k2

2 2⌘(1  )  8⌘2 · Eµh⇣bQ0x; W (t)  bQ0(x; W ⇤)⌘2i

2 + 2⌘B ·k g(t)  g0(t)k2

.

Error of Local Linearization

+ 2⌘2 ·k g(t)  g0(t)k2
|

{z

}

Proof. See Appendix C.3 for a detailed proof.

Lemma 5.3 shows that  with a sufﬁciently small stepsize ⌘  kW (t)  W ⇤k2 decays at each iteration
up to the error of local linearization  which is characterized by Lemma 5.2. By combining Lemmas
5.2 and 5.3 and further plugging them into a telescoping sum  we establish the convergence of bQout(·)
to the global optimum bQ0(· ; W ⇤) of the MSPBE. See Appendix C.5 for a detailed proof.

5.3 Proofs for Stochastic Update

Recall that the stochastic semigradient g(t) is deﬁned in (4.10). In parallel with Lemma 5.3  the
following lemma additionally characterizes the effect of the variance of g(t)  which is induced by
the randomness of the current tuple (x  r  x0). We use the subscript EW [·] to denote the expectation
over the randomness of the current iterate W (t) conditional on the random initialization b and W (0).
Correspondingly  EW µ[·] is over the randomness of both the current tuple (x  r  x0) and the current
iterate W (t) conditional on the random initialization.
Lemma 5.4 (Stochastic Descent Lemma). For {W (t)}t2[T ] in Algorithm 1  it holds that
EW µ⇥kW (t + 1)  W ⇤k2
2⇤
2⇤ 2⌘(1  )  8⌘2 · EW µh⇣bQ0x; W (t)  bQ0(x; W ⇤)⌘2i
 EW⇥kW (t)  W ⇤k2
2⇤
+ EW µ⇥⌘2 ·k g(t)  g(t)k2
|
}

2 + 2⌘B ·k g(t)  g0(t)k2⇤
+ EW⇥2⌘2 ·k g(t)  g0(t)k2
{z
|
}

Error of Local Linearization

Variance of Semigradient

{z

.

8

Proof. See Appendix C.4 for a detailed proof.

To ensure the global convergence of neural TD in the presence of the variance of g(t)  we rescale
the stepsize to be of order T 1/2. The rest proof of Theorem 4.6 mirrors that of Theorem 4.4. See
Appendix C.6 for a detailed proof.

6 Conclusions

In this paper we prove that neural TD converges at a sublinear rate to the global optimum of the
MSPBE for policy evaluation. In particular  we show how such global convergence is enabled by the
overparametrization of neural networks. Our results shed new light on the theoretical understanding
of RL with neural networks  which is widely employed in practice.

References
[1] Allen-Zhu  Z.  Li  Y. and Liang  Y. (2018). Learning and generalization in overparameterized

neural networks  going beyond two layers. arXiv preprint arXiv:1811.04918.

[2] Amiranashvili  A.  Dosovitskiy  A.  Koltun  V. and Brox  T. (2018). TD or not TD: Ana-
lyzing the role of temporal differencing in deep reinforcement learning. arXiv preprint
arXiv:1806.01175.

[3] Arora  S.  Du  S. S.  Hu  W.  Li  Z. and Wang  R. (2019). Fine-grained analysis of optimiza-
tion and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584.

[4] Baird  L. (1995). Residual algorithms: Reinforcement learning with function approximation.

In International Conference on Machine Learning.

[5] Bertsekas  D. P. (2019). Feature-based aggregation and deep reinforcement learning: A survey

and some new implementations. IEEE/CAA Journal of Automatica Sinica  6 1–31.

[6] Bhandari  J.  Russo  D. and Singal  R. (2018). A ﬁnite time analysis of temporal difference

learning with linear function approximation. arXiv preprint arXiv:1806.02450.

[7] Bhatnagar  S.  Precup  D.  Silver  D.  Sutton  R. S.  Maei  H. R. and Szepesv´ari  C. (2009).
Convergent temporal-difference learning with arbitrary smooth function approximation. In
Advances in Neural Information Processing Systems.

[8] Borkar  V. S. (2009). Stochastic Approximation: A Dynamical Systems Viewpoint  vol. 48.

Springer.

[9] Borkar  V. S. and Meyn  S. P. (2000). The ODE method for convergence of stochastic approxi-
mation and reinforcement learning. SIAM Journal on Control and Optimization  38 447–469.

[10] Boyan  J. A. (1999). Least-squares temporal difference learning. In International Conference

on Machine Learning.

[11] Boyan  J. A. and Moore  A. W. (1995). Generalization in reinforcement learning: Safely ap-

proximating the value function. In Advances in Neural Information Processing Systems.

[12] Bradtke  S. J. and Barto  A. G. (1996). Linear least-squares algorithms for temporal difference

learning. Machine Learning  22 33–57.

[13] Chizat  L. and Bach  F. (2018). A note on lazy training in supervised differentiable programming.

arXiv preprint arXiv:1812.07956.

[14] Dalal  G.  Sz¨or´enyi  B.  Thoppe  G. and Mannor  S. (2018). Finite sample analyses for TD(0)

with function approximation. In AAAI Conference on Artiﬁcial Intelligence.

9

[15] Daniely  A. (2017). SGD learns the conjugate kernel class of the network. In Advances in

Neural Information Processing Systems.

[16] Dann  C.  Neumann  G. and Peters  J. (2014). Policy evaluation with temporal differences: A

survey and comparison. Journal of Machine Learning Research  15 809–883.

[17] Du  S. S.  Chen  J.  Li  L.  Xiao  L. and Zhou  D. (2017). Stochastic variance reduction methods

for policy evaluation. In International Conference on Machine Learning.

[18] Duan  Y.  Chen  X.  Houthooft  R.  Schulman  J. and Abbeel  P. (2016). Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learn-
ing.

[19] Facchinei  F. and Pang  J.-S. (2007). Finite-Dimensional Variational Inequalities and Comple-

mentarity Problems. Springer Science & Business Media.

[20] Fan  J.  Ma  C. and Zhong  Y. (2019). A selective overview of deep learning. arXiv preprint

arXiv:1904.05526.

[21] Geist  M. and Pietquin  O. (2013). Algorithmic survey of parametric value function approxima-

tion. IEEE Transactions on Neural Networks and Learning Systems  24 845–867.

[22] Ghavamzadeh  M.  Lazaric  A.  Maillard  O. and Munos  R. (2010). LSTD with random pro-

jections. In Advances in Neural Information Processing Systems.

[23] Haarnoja  T.  Tang  H.  Abbeel  P. and Levine  S. (2017). Reinforcement learning with deep

energy-based policies. In International Conference on Machine Learning.

[24] Haarnoja  T.  Zhou  A.  Abbeel  P. and Levine  S. (2018).

maximum entropy deep reinforcement learning with a stochastic actor.
arXiv:1801.01290.

Soft actor-critic: Off-policy
arXiv preprint

[25] Harker  P. T. and Pang  J.-S. (1990). Finite-dimensional variational inequality and nonlinear
complementarity problems: a survey of theory  algorithms and applications. Mathematical
Programming  48 161–220.

[26] Henderson  P.  Islam  R.  Bachman  P.  Pineau  J.  Precup  D. and Meger  D. (2018). Deep

reinforcement learning that matters. In AAAI Conference on Artiﬁcial Intelligence.

[27] Hofmann  T.  Sch¨olkopf  B. and Smola  A. J. (2008). Kernel methods in machine learning.

Annals of Statistics 1171–1220.

[28] Jaakkola  T.  Jordan  M. I. and Singh  S. P. (1994). Convergence of stochastic iterative dynamic

programming algorithms. In Advances in Neural Information Processing Systems.

[29] Jacot  A.  Gabriel  F. and Hongler  C. (2018). Neural tangent kernel: Convergence and general-

ization in neural networks. In Advances in Neural Information Processing Systems.

[30] Jain  P. and Kar  P. (2017). Non-convex optimization for machine learning. Foundations and

Trends R in Machine Learning  10 142–336.

[31] Konda  V. R. and Tsitsiklis  J. N. (2000). Actor-critic algorithms.

Information Processing Systems.

In Advances in Neural

[32] Kushner  H. and Yin  G. G. (2003). Stochastic Approximation and Recursive Algorithms and

Applications. Springer Science & Business Media.

[33] Lakshminarayanan  C. and Szepesvari  C. (2018). Linear stochastic approximation: How far
does constant step-size and iterate averaging go? In International Conference on Artiﬁcial
Intelligence and Statistics.

[34] Lazaric  A.  Ghavamzadeh  M. and Munos  R. (2010). Finite-sample analysis of LSTD. In

International Conference on Machine Learning.

10

[35] Lee  J.  Xiao  L.  Schoenholz  S. S.  Bahri  Y.  Sohl-Dickstein  J. and Pennington  J. (2019).
Wide neural networks of any depth evolve as linear models under gradient descent. arXiv
preprint arXiv:1902.06720.

[36] Lillicrap  T. P.  Hunt  J. J.  Pritzel  A.  Heess  N.  Erez  T.  Tassa  Y.  Silver  D. and Wierstra  D.
(2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.

[37] Liu  B.  Liu  J.  Ghavamzadeh  M.  Mahadevan  S. and Petrik  M. (2015). Finite-sample analy-
sis of proximal gradient TD algorithms. In Conference on Uncertainty in Artiﬁcial Intelligence.

[38] Melo  F. S.  Meyn  S. P. and Ribeiro  M. I. (2008). An analysis of reinforcement learning with

function approximation. In International Conference on Machine Learning.

[39] Mnih  V.  Badia  A. P.  Mirza  M.  Graves  A.  Lillicrap  T.  Harley  T.  Silver  D. and
In In-

Kavukcuoglu  K. (2016). Asynchronous methods for deep reinforcement learning.
ternational Conference on Machine Learning.

[40] Neu  G.  Jonsson  A. and G´omez  V. (2017). A uniﬁed view of entropy-regularized markov

decision processes. arXiv preprint arXiv:1705.07798.

[41] Neyshabur  B.  Li  Z.  Bhojanapalli  S.  LeCun  Y. and Srebro  N. (2018). Towards understand-
ing the role of over-parametrization in generalization of neural networks. arXiv preprint
arXiv:1805.12076.

[42] Pfau  D. and Vinyals  O. (2016). Connecting generative adversarial networks and actor-critic

methods. arXiv preprint arXiv:1610.01945.

[43] Rahimi  A. and Recht  B. (2008). Random features for large-scale kernel machines. In Advances

in Neural Information Processing Systems.

[44] Rahimi  A. and Recht  B. (2008). Uniform approximation of functions with random bases. In

Annual Allerton Conference on Communication  Control  and Computing.

[45] Scherrer  B. (2010). Should one compute the temporal difference ﬁx point or minimize the
bellman residual? the uniﬁed oblique projection view. In International Conference on Machine
Learning.

[46] Schulman  J.  Chen  X. and Abbeel  P. (2017). Equivalence between policy gradients and soft

Q-learning. arXiv preprint arXiv:1704.06440.

[47] Schulman  J.  Levine  S.  Abbeel  P.  Jordan  M. and Moritz  P. (2015). Trust region policy

optimization. In International Conference on Machine Learning.

[48] Srikant  R. and Ying  L. (2019). Finite-time error bounds for linear stochastic approximation

and TD learning. arXiv preprint arXiv:1902.00923.

[49] Sutton  R. S. (1988). Learning to predict by the methods of temporal differences. Machine

Learning  3 9–44.

[50] Sutton  R. S. and Barto  A. G. (2018). Reinforcement Learning: An Introduction. MIT press.

[51] Sutton  R. S.  Maei  H. R.  Precup  D.  Bhatnagar  S.  Silver  D.  Szepesv´ari  C. and
Wiewiora  E. (2009). Fast gradient-descent methods for temporal-difference learning with
linear function approximation. In International Conference on Machine Learning.

[52] Sutton  R. S.  Maei  H. R. and Szepesv´ari  C. (2009). A convergent o(n) temporal-difference
algorithm for off-policy learning with linear function approximation. In Advances in Neural
Information Processing Systems.

[53] Szepesv´ari  C. (2010). Algorithms for reinforcement learning. Synthesis Lectures on Artiﬁcial

Intelligence and Machine Learning  4 1–103.

11

[54] Touati  A.  Bacon  P.-L.  Precup  D. and Vincent  P. (2017). Convergent tree-backup and retrace

with function approximation. arXiv preprint arXiv:1705.09322.

[55] Tsitsiklis  J. N. and Van Roy  B. (1997). Analysis of temporal-diffference learning with function

approximation. In Advances in Neural Information Processing Systems.

[56] Tu  S. and Recht  B. (2017). Least-squares temporal difference learning for the linear quadratic

regulator. arXiv preprint arXiv:1712.08642.

[57] Wang  Y.  Chen  W.  Liu  Y.  Ma  Z.-M. and Liu  T.-Y. (2017). Finite sample analysis of the
GTD policy evaluation algorithms in Markov setting. In Advances in Neural Information
Processing Systems.

[58] Williams  R. J. (1992). Simple statistical gradient-following algorithms for connectionist rein-

forcement learning. Machine Learning  8 229–256.

[59] Zhang  C.  Bengio  S.  Hardt  M.  Recht  B. and Vinyals  O. (2016). Understanding deep learn-

ing requires rethinking generalization. arXiv preprint arXiv:1611.03530.

[60] Zou  S.  Xu  T. and Liang  Y. (2019). Finite-sample analysis for SARSA and Q-learning with

linear function approximation. arXiv preprint arXiv:1902.02234.

12

,Qi Cai
Zhuoran Yang
Jason Lee
Zhaoran Wang