2016,Stochastic Gradient Methods for Distributionally Robust Optimization with f-divergences,We develop efficient solution methods for a robust empirical risk minimization problem designed to give calibrated confidence intervals on performance and provide optimal tradeoffs between bias and variance. Our methods apply to distributionally robust optimization problems proposed by Ben-Tal et al.  which put more weight on observations inducing high loss via a worst-case approach over a non-parametric uncertainty set on the underlying data distribution. Our algorithm solves the resulting minimax problems with nearly the same computational cost of stochastic gradient descent through the use of several carefully designed data structures. For a sample of size n  the per-iteration cost of our method scales as O(log n)  which allows us to give optimality certificates that distributionally robust optimization provides at little extra cost compared to empirical risk minimization and stochastic gradient methods.,Stochastic Gradient Methods for Distributionally

Robust Optimization with f-divergences

Hongseok Namkoong
Stanford University

hnamk@stanford.edu

Abstract

John C. Duchi

Stanford University

jduchi@stanford.edu

We develop efﬁcient solution methods for a robust empirical risk minimization
problem designed to give calibrated conﬁdence intervals on performance and
provide optimal tradeoffs between bias and variance. Our methods apply to dis-
tributionally robust optimization problems proposed by Ben-Tal et al.  which put
more weight on observations inducing high loss via a worst-case approach over a
non-parametric uncertainty set on the underlying data distribution. Our algorithm
solves the resulting minimax problems with nearly the same computational cost
of stochastic gradient descent through the use of several carefully designed data
structures. For a sample of size n  the per-iteration cost of our method scales as
O(log n)  which allows us to give optimality certiﬁcates that distributionally robust
optimization provides at little extra cost compared to empirical risk minimization
and stochastic gradient methods.

1

Introduction

In statistical learning or other data-based decision-making problems  it is desirable to give solutions
that come with guarantees on performance  at least to some speciﬁed conﬁdence level. For tasks
such as driving or medical diagnosis where safety and reliability are crucial  conﬁdence levels
have additional importance. Classical techniques in machine learning and statistics  including
regularization  stability  concentration inequalities  and generalization guarantees [6  25] provide
such guarantees  though often a more ﬁne-tuned certiﬁcate—one with calibrated conﬁdence—is
desirable. In this paper  we leverage techniques from the robust optimization literature [e.g. 2] 
building an uncertainty set around the empirical distribution of the data and studying worst case
performance in this uncertainty set. Recent work [15  13] shows how this approach can give (i)
calibrated statistical optimality certiﬁcates for stochastic optimization problems  (ii) performs a
natural type of regularization based on the variance of the objective and (iii) achieves fast rates of
convergence under more general conditions than empirical risk minimization by trading off bias
(approximation error) and variance (estimation error) optimally. In this paper  we propose efﬁcient
algorithms for such distributionally robust optimization problems.
We now provide our formal setting. Let X⇢ Rd be a compact convex set  and for a convex
function f : R+ ! R with f (1) = 0  deﬁne the f-divergence between distributions P and Q by
Df (P||Q) =R f ( dP
n} be an
uncertainty set around the uniform distribution /n  we develop methods for solving the robust
empirical risk minimization problem

dQ )dQ. Letting P⇢ n := {p 2 Rn : p> = 1  p  0  Df (p|| /n)  ⇢

minimize

x2X

sup
p2P⇢ n

pi`i(x).

nXi=1

(1)

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In problem (1)  the functions `i : X! R+ are convex and subdifferentiable  and we consider the
iid⇠ P0. We let `(x) = [`1(x) ··· `n(x)]> 2 Rn denote the
situation in which `i(x) = `(x; ⇠i) for ⇠i
vector of convex losses  so the robust objective (1) is supp2P⇢ n pT `(x).
A number of authors show how the robust formulation (1) provides guarantees. Duchi et al. [15]
show that the objective (1) is a convex approximation to regularizing the empirical risk by variance 

sup
p2P⇢ n

nXi=1

pi`i(x) =

1
n

nXi=1

`i(x) +r ⇢

n

VarP0(`(x; ⇠)) + oP0(n 1
2 )

(2)

uniformly in x 2X . Since the right hand side naturally trades off good loss performance (ap-
proximation error) and minimizing variance (estimation error) which is usually non-convex  the
robust formulation (1) provides a convex regularization for the standard empirical risk minimization
(ERM) problem. This trading between bias and variance leads to certiﬁcates on the optimal value
inf x2X EP0[`(x; ⇠)] so that under suitable conditions  we have

P✓ inf

x2X

EP0[`(x; ⇠)]  un◆ = P (W  p⇢) for W ⇠ N(0  1)

lim
n!1

(3)

where un = inf x2X supp2P⇢ n pT `(x) is the optimal robust objective. Duchi and Namkoong [13]
provide ﬁnite sample guarantees for the special case that f (t) = 1
2 (t  1)2  making the expansion (2)
more explicit and providing a number of consequences for estimation and optimization based on this
expansion (including fast rates for risk minimization). A special case of their results [13  §3.1] is as

of the class of functions F := {`(x;·) | x 2X}   assume that M  `(x; ⇠) for all x 2X  ⇠ 2 ⌅  and
for some ﬁxed > 0  deﬁne ⇢ = log 1
 + 10 VC(F) log VC(F). Then  with probability at least 1   
M⇢
n

follows. Letbxrob 2 argminx2X supp2P⇢ n pT `(x)  let VC(F) denote the VC-(subgraph)-dimension
EP0[`(bxrob; ⇠)]  un + O(1)

(4)
For large n  evaluating the objective (1) may be expensive; with ﬁxed p = /n  this has motivated an
extensive literature in stochastic and online optimization [27  23  19  16  18]. The problem (1) does
not admit quite such a straightforward approach. A ﬁrst idea  common in the robust optimization
literature [3]  is to obtain a problem that may be written as a sum of individual terms by taking the
dual of the inner supremum  yielding the convex problem

EP0[`(x; ⇠)] + 2s 2⇢VarbPn

x2X8<:

M⇢
n  inf

(`(x; ⇠))
n

9=;

+ O(1)

inf
x2X

sup
p2P⇢ n

p>`(x) =

inf

x2X  0 ⌘2R

1
n

f⇤✓ `i(x)  ⌘



◆ +

⇢
n

 + ⌘.

(5)

nXi=1

Here f⇤(s) = supt0{st  f (t)} is the Fenchel conjugate of the convex function f. While the
above dual reformulation is jointly convex in (x    ⌘)  canonical stochastic gradient descent (SGD)
procedures [23] generally fail because the variance of the objective (and its subgradients) explodes as
 ! 0. (This is not just a theoretical issue: in extensive simulations that we omit because they are a
bit boring  SGD and other heuristic approaches that impose shrinking bounds of the form t  ct > 0
at each iteration t all fail to optimize the objective (5).)
Instead  we view the robust ERM problem (1) as a game between the x (minimizing) player and p
(maximizing) player. Each player performs a variant of mirror descent (ascent)  and we show how
such an approach yields strong convergence guarantees  as well as good empirical performance. In
particular  we show (for many suitable divergences f) that if `i is L-Lipschitz and X has radius
bounded by R  then our procedure requires at most O( R2L2+⇢
) iterations to achieve an ✏-accurate
solution to problem (1)  which is comparable to the number of iterations required by SGD [23]. Our
solution strategy builds off of similar algorithms due to Nemirovski et al. [23  Sec. 3] and Ben-Tal et al.
[4]  and more directly procedures developed by Clarkson et al. [10] for solving two-player convex
games. Most directly relevant to our approach is that of Shalev-Shwartz and Wexler [26]  which
+ : pT = 1} and that there is some
solves problem (1) under the assumption that P⇢ n = {p 2 Rn
x with perfect loss performance  that is Pn
i=1 `i(x) = 0. We generalize these approaches to more
challenging f-divergence-constrained problems  and  for the 2 divergence with f (t) = 1
2 (t  1)2 

✏2

2

✏2

develop efﬁcient data structures that give a total run-time for solving problem (1) to ✏-accuracy
scaling as O((Cost(grad) + log n) R2L2+⇢
). Here Cost(grad) is the cost to compute the gradient of
a single term r`i(x) and perform a mirror descent step with x. Using SGD to solve the empirical
minimization problem to ✏-accuracy has run-time O(Cost(grad) R2L2
)  so we see that we can achieve
✏2
the guarantees (3)–(4) offered by the robust formulation (1) at little additional computational cost.
The remainder of the paper is organized as follows. We present our abstract algorithm in Section 2
and give guarantees on its performance in Section 3. In Section 4  we give efﬁcient computational
schemes for the case that f (t) = 1

2 (t  1)2  presenting experiments in Section 5.
2 A bandit mirror descent algorithm for the minimax problem
Under the conditions that ` is convex and X is compact  standard results [7] show that there exists a
saddle point (x?  p?) 2X⇥P ⇢ n for the robust problem (1) satisfying

supp>`(x?) | p 2P ⇢ n  p?>`(x?)  infp?>`(x) | x 2X .

We now describe a procedure for ﬁnding this saddle point by alternating a linear bandit-convex
optimization procedure [8] for p and a stochastic mirror descent procedure for x. Our approach builds
off of Nemirovski et al.’s [23] development of mirror descent for two-player stochastic games.
To describe our algorithm  we require a few standard tools. Let k·kx denote a norm on the space
X with dual norm kykx ⇤ = sup{hx  yi : kxk  1}  and let x be a differentiable strongly convex
function on X   meaning x(x+)  x(x)+r x(x)>+ 1
x for all . Let p a differentiable
strictly convex function on P⇢ n. For a differentiable convex function h  we deﬁne the Bregman
divergence Bh(x  y) = h(x)  h(y)  hrh(y)  x  yi 0. The Fenchel conjugate ⇤p of p is

2 kk2

{hs  pi  p(p)} .

 ⇤p (s) := sup

p {hs  pi  p(p)} and r ⇤p (s) = argmax

p

( ⇤p is differentiable because p is strongly convex [20  Chapter X].) We let gi(x) 2 @`i(x) be a
particular subgradient selection.
With this notation in place  we now give our algorithm  which alternates between gradient ascent
steps on p and subgradient descent steps on x. Roughly  we would like to alternate gradient ascent
steps for p  pt+1 pt + ↵p`(xt)  and descent steps xt+1 xt ↵xgi(xt) for x  where i is a random
index drawn according to pt. This procedure is inefﬁcient—requiring time of order nCost(grad) in
each iteration—so that we use stochastic estimates of the loss vector `(xt) developed in the linear
bandit literature [8] and variants of mirror descent to implement our algorithm.

Algorithm 1 Two-player Bandit Mirror Descent
1: Input: Stepsize ↵x ↵ p > 0  initialize: x1 2X   p1 = /n
2: for t = 1  2  . . .   T do
Sample It ⇠ pt  that is  set It = i with probability pt i
3:
Compute estimated loss for i 2 [n]:b`t i(x) = `i(x)
4:
Update p: wt+1 r ⇤p (r p(pt) + ↵pb`t(xt))  pt+1 argminp2P⇢ n B p(p  wt+1)
5:
Update x: yt+1 r ⇤x ( x(xt)  ↵xgIt(xt))  xt+1 argminx2X B x(x  yt+1)
6:
7: end for

1{It = i}

pi t

We specialize this general algorithm for speciﬁc choices of the divergence f and the functions x and
 p presently  ﬁrst brieﬂy discussing the algorithm. Note that in Step 5  the updates for p depend only

are efﬁciently computable  can yield substantial performance beneﬁts.

on a single index It 2{ 1  . . .   n} (the vectorb`(xt) is 1-sparse)  which  as long as the updates for p

3 Regret bounds

With our algorithm described  we now describe its convergence properties  specializing later to
speciﬁc families of f-divergences. We begin with the following result on pseudo-regret  which (with
minor modiﬁcations) is known [23  10  26]. We provide a proof for completeness in Appendix A.1.

3

+

}

1
↵x

{z

↵x
2

{z

TXt=1

t=1 xt and

B x(x?  x1) +

TXt=1
|

T1: ERM regret

T2: robust regret

E[kgIt(xt)k2
x ⇤]

t=1 pt. Then for the saddle point (x?  p?) we have

T PT
Lemma 1. Let the sequences xt and pt be generated by Algorithm 1. DeﬁnebxT := 1
T PT
bpT := 1
E[b`t(xt)>(p?  pt)]
TE[p?>`(bxT ) bp>T `(x?)] 
}
|
where the expectation is taken over the random draws It ⇠ pt. Moreover  E[b`t(xt)>(p  pt)] =
E[`(xt)>(p  pt)] for any vector p.
In the lemma  T1 is the standard regret when applying mirror descent to the ERM problem. In
Lp2/T yields
particular  if B x(x?  x1)  R2 and `i(x) is L-Lipschitz  then choosing ↵x = R
T1  RLpT . Because it is (relatively) easy to bound the term T1  the remainder of our arguments
focus on bounding the the second term T2  which is the regret that comes as a consequence of the
random sampling for the loss vectorb`t. This regret depends strongly on the distance-generating
function p. To the end of bounding T2  we use the following bound for the pseudo-regret of p  which
is standard [9  Chapter 11]  [8  Thm 5.3]. For completeness we outline the proof in Appendix A.2.
Lemma 2. For any p 2P ⇢ n  Algorithm 1 satisﬁes
TXt=1

B ⇤p ⇣r p(pt) + ↵pb`t(xt) r p(pt)⌘ .

TXt=1b`t(xt)>(p  pt) 

Lemma 2 shows that controlling the Bregman divergences B p and B ⇤p is sufﬁcient to bound T2 in
the basic regret bound of Lemma 1.
Now  we narrow our focus slightly to a specialized—but broad—family of divergences for which we
can give more explicit results. For k 2 R  the Cressie-Read divergence [12] of order k is

B p(p  p1)

+

1
↵p

(6)

↵p

tk  kt + k  1

 

@pi

fk(t) =

(7)
where fk(t) = 1 for t < 0  and for k 2{ 0  1} we deﬁne fk by its limits as k ! 0 or 1 (we have
f1(t) = t log t  t + 1 and f0(t) =  log t + t  1). Inspecting expression (6)  we might hope that
careful choices of p could yield regret bounds that grow slowly with T and have small dependence
on the sample size n. Indeed  this is the case  as we show in the sequel: for each divergence fk  we
may carefully choose p to achieve small regret. To prove our bounds  however  it is crucial that

k(k  1)

the importance sampling estimatorb`t has small variance  which in turn necessitates that pt i is not
too small. Generally  this means that in the update (Alg. 1  Line 5) to construct pt+1  we choose
 (p) to grow quickly as pi ! 0 (e.g. | @
 p(p)|! 1 )  but there is a tradeoff in that this may cause
large Bregman divergence terms (6). In the coming sections  we explore this tradeoff for various k 
providing regret bounds for each of the Cressie-Read divergences (7).
To control the B ⇤p terms in the bound (6)  we use the curvature of p (dually  smoothness of ⇤p)

to show that B ⇤p (u  v) ⇡P(ui  vi)2. For this approximation to hold  we shift our loss functions
based on the f-divergence. When k  2  we assume that `(x) 2 [0  1]n. If k < 2  we instead apply
Algorithm 1 with shifted losses `0(x) = `(x)    so that `0(x) 2 [1  0]n. We call the method with
`0 Algorithm 1’  noting thatb`t i(xt) = `i(xt)1
3.1 Power divergences when k 62 {0  1}
For our ﬁrst results  we prove a generic regret bound for Algorithm 1 when k 62 {0  1} by taking the
k(k1)Pn
distance-generating function p(p) = 1
i   which is differentiable and strictly convex on
+. Before proceeding further  we ﬁrst note that for p 2P ⇢ n and p1 = 1
Rn

1{It = i} in this case.

n   we have

i=1 pk

pt i

B p(p  p1) = p(p)  p(p1)  r p(p1)>(p  p1)

=

nk

k(k  1)

nXi=1(npi)k  knpi + k  1 = nkDf (p|| /n)  nk⇢

(8)

4

+

↵p
2

p1k

(9)

TXt=1

TXt=1

nk⇢
↵p

t i 35 .

E[`(xt)>(p  pt)] =

E24 Xi:pt i>0

E[b`t(xt)>(p  pt)] 

bounding the ﬁrst term in expression (6). From Lemma 2  it remains to bound the Bregman divergence
terms B ⇤p . Using smoothness of ⇤p in the positive orthant  we obtain the following bound.
Theorem 1. Assume that `(x) 2 [0  1]n. For any real-valued k  2 and any p 2P ⇢ n  Algorithm 1
satisﬁes
TXt=1
For k  2 with k 62 {0  1}  an identical bound holds for Algorithm 1’ with `0(x) = `(x)  .
See Appendix A.3 for the proof. We now use Theorem 1 to obtain concrete convergence guarantees for
Cressie-Read divergences with parameter k < 1  giving sublinear (in T ) regret bounds independent
of n. In the corollary  whose proof we provide in Appendix A.4  we let Ck ⇢ = (1k)(1k⇢)
  which
is positive for k < 0.
k ⇢ nkp2⇢/T Algorithm 1’ with `0(x) = `(x)  2
Corollary 1. For k 2 (1  0) and ↵p = C
[1  0]n acheives the regret bound
E[b`t(xt)>(p  pt)] q2C1k
TXt=1
TXt=1
E[b`t(xt)>(p  pt)] p2⇢T .

For k 2 (0  1) and ↵p = nkp2⇢/T   Algorithm 1’ with `0(x) = `(x)  2 [1  0]n acheives the

It is worth noting that despite the robustiﬁcation  the above regret is independent of n. In the special
case that k 2 (0  1)  Theorem 1 is the regret bound for the implicitly normalized forecaster of
Audibert and Bubeck [1] (cf. [8  Ch 5.4]).

E[`(xt)>(p  pt)] =

E[`(xt)>(p  pt)] =

regret bound

TXt=1

TXt=1

k ⇢ ⇢T .

k1

2

k

3.2 Regret bounds using the KL divergences (k = 1 and k = 0)
The choice f1(t) = t log t  t + 1 yields Df (P||Q) = Dkl (P||Q)  and in this case  we take
 p(p) =Pn
i=1 pi log pi  which means that Algorithm 1 performs entropic gradient ascent. To control
the divergence B ⇤p   we use the rescaled losses `0(x) = `(x)  (as we have k < 2). Then we have
the following bound  whose proof we provide in Appendix A.5.
Theorem 2. Algorithm 1’ with loss `0(x) = `(x)  yields

nT.

(10)

TXt=1

E[`(xt)>(p  pt)] =

TXt=1
nq 2⇢
T   we havePT

⇢
n↵p

E[b`t(xt)>(p  pt)] 
t=1 E[`(xt)>(p  pt)]  p2⇢T .

+

↵p
2

In particular  when ↵p = 1

Using k = 0  so that f0(t) =  log t + t  1  we obtain Df (P||Q) = Dkl (Q||P )  which results
in a robustiﬁcation technique identical to Owen’s original empirical likelihood [24]. We again use
the rescaled losses `0(x) = `(x)    but in this scenario we use the proximal function p(p) =
Pn
Theorem 3. Algorithm 1’ with loss `0(x) = `(x)  yields

i=1 log pi in Algorithm 1’. Then we have the following regret bound (see Appendix A.6).

TXt=1

TXt=1
E[b`t(xt)>(p  pt)] 
E[`(xt)>(p  pt)] =
t=1 E[`(xt)>(p  pt)]  p2⇢T .
T   we havePT

In particular  when ↵p =q 2⇢
In both of these cases  the expected pseudo-regret of our robust gradient procedure is independent of
n and grows as pT   which is essentially identical to that achieved by pure online gradient methods.

↵p
2

T.

⇢
↵p

+

5

3.3 Power divergences (k > 1)
Corollary 1 provides convergence guarantees for power divergences fk with k < 1  but says nothing
about the case that k > 1; the choice p(p) = 1
i allows the individual probabilities

i=1 pk

problem (1) by re-deﬁning our robust empirical distributions set  taking

pt i to be too small  which can cause excess variance ofb`. To remedy this  we regularize the robust

P⇢ n  :=np 2 Rn

+ | p 

f (npi)  ⇢o 

k(k1)Pn
nXi=1


n

 

where we no longer constrain the weights p to satisfy >p = 1. Nonetheless  it is still possible to
show that the guarantees (2) and (3) hold with P⇢ n  replacing P⇢ n. Indeed  we may give bounds for
the pseudo-regret of the regularized problem with P⇢ n   where we apply Algorithm 1 with a slightly
modiﬁed sampling strategy  drawing indices i according to the normalized distribution pt/Pn
i=1 pt i
and appropriately normalizing the loss estimate via

pt i

1{It = i} .

b`t i(xt) = nXi=1

pt i! `i(xt)
This vector is still unbiased for `(xt). Deﬁne the constant Ck := max{t : fk(t)  t}_ ⇢
C2 = 2 + p3). With our choice p(p) = 1
k(k1)Pn
result  whose proof we provide in Appendix A.7.
Theorem 4. For k 2 [2 1)  any p 2P ⇢ n   Algorithm 1 with ↵p = nkp⇢k1/ (4C3
TXt=1
E[b`t(xt)>(p  pt)]  2Ckp⇢Ck1kT

For k 2 (1  2)  assume that `(x) 2 [1  0]n. Then  Algorithm 1 gives identical bounds.
4 Efﬁcient updates when k = 2

E[`(xt)>(p  pt)] =

TXt=1

i=1 pk

n < 1 (so
i and for > 0  we obtain the following

kT ) yields

despite the sparsity ofb`(xt) (see Appendix B for concrete updates for each of our cases). In this

The previous section shows that Algorithm 1 with careful choice of p yields sublinear regret bounds.
The projection step pt+1 = argminp2P⇢ n  B p(p  wt+1)  however  can still take time linear in n
section  we show how to compute the bandit mirror descent update in Alg. 1  line 5  in time O(log n)
time for f2(t) = 1
i . Building off of Duchi et al. [14]  we use
carefully designed balanced binary search trees (BSTs) to this end.
The Lagrangian for the update pt+1 = argminp2P⇢ n  B p(p  wt+1) (suppressing t) is

i=1 p2

2Pn
2 (t  1)2 and p(p) = 1
n2 ⇢ 

L(p    ✓) = B p(p  w) 





n ◆

+. The KKT conditions imply (1+)p = w+ 

nXi=1

f2(npi)!  ✓>✓p 
n ◆+
p() =✓ 1
p2P⇢ n  L(p    ✓) = B p(p()  w)   ⇢ 

1
n 


n

inf





w +

(11)
1 + 
+ L(p    ✓). Substituting this into the Lagrangian  we obtain

1 + 

+

 

where p() = argminp2P⇢ n  inf ✓2Rn
the concave dual objective

n +✓  and strict complementarity

where   0 ✓ 2 Rn
yields

g() := sup

✓

fk(npi())! .

nXi=1

We can run a bisection search on the nondecreasing function g0() to ﬁnd  such that g0() = 0.
After algebraic manipulations  we have that

@
@

g() = g1() Xi2I()

w2

i + g2() Xi2I()

wi + g3()|I()| +

(1  )2

2n 

⇢
n2  

6

where I() := {1  i  n : wi  

n + ( 

g1() =

1

(1 + )2   g2() =

n  1)} and (see expression (18) in Appendix B.4)
n(1 + )2   g3() =

n2(1 + )2 

(1  )2

2

1

.

2n

✏ ) time  it sufﬁces to
To see that we can solve for ⇤ that acheives |g0(⇤)| ✏ in O(log n + log 1
evaluatePi2I() wq
i for q = 0  1  2 in time O(log n). To this end  we store the w’s in a balanced
search tree (e.g.  red-black tree) keyed on the weights up to a multiplicative and an additive constant.
A key ingredient in our implementation is that the BST stores in each node the sum of the appropriate
powers of values in the left and right subtree [14]. See Appendix C for detailed pseudocode for all
operations required in Algorithm 1: each subroutine (sampling It ⇠ pt  updating w  computing ⇤ 
and updating p(⇤)) require time O(log n) using standard BST operations.

5 Experiments

In this section  we present experimental results demonstrating the efﬁciency of our algorithm. We ﬁrst
compare our method with existing algorithms for solving the robust problem (1) on a synthetic dataset 
then investigating the robust formulation on real datasets to show how the calibrated conﬁdence
guarantees behave in practice  especially in comparison to the ERM. We experiment on natural high
dimensional datasets as well as those with many training examples.
Our implementation uses the efﬁcient updates outlined in Section 4. Throughout our experiments 
we use the best tuned step sizes for all methods. For the ﬁrst two experiments  we set ⇢ = 2
1 .9
so that the resulting robust objective (1) will be a calibrated 95% upper conﬁdence bound on the
optimal population risk. For our last experiment  the asymptotic regime (3) fails to hold due to the
high dimensional nature of the problem  so we choose ⇢ = 50 (somewhat arbitrarily  but other ⇢ give

similar behavior). We take X =x 2 Rd : kxk2  R for our experiments.
For the experiment with synthetic data  we compare our algorithm against two benchmark methods
for solving the robust problem (1). The ﬁrst is the interior point method for the dual reformulation (5)
using the Gurobi solver [17]. The second is using gradient descent  viewing the robust formulation (1)
as a minimization problem with the objective x 7! supp2P⇢ n  p>`(x). To efﬁciently compute the
gradient  we bisect over the dual form (5) with respect to   0  ⌘. We use the best step sizes for
both our proposed bandit-based algorithm and gradient descent.
iid⇠ N(0  I)
To generate the data  we choose a true classiﬁer x⇤ 2 Rd and sample the feature vectors ai
for i 2 [n]. We set the labels to be bi = sign(a>i x⇤) and ﬂip them with probability 10%. We use
the hinge loss `i(x) = 1  bia>i x+ with n = 2000  d = 500 and R = 10 in our experiment.
In Figure 1a  we plot the log optimality ratio (log of current objective value over optimal value)
with respect to the runtime for the three algorithms. While the interior point method (IPM) obtains
accurate solutions  it scales relatively poorly in n and d (the initial ﬂat region in the plot is due to
pre-computations for factorizing within the solver). Gradient descent performs quite well in this
moderate sized example although each iteration takes time ⌦(n).
We also perform experiments on two datasets with larger n: the Adult dataset [22] and the Reuters
RCV1 Corpus [21]. The Adult dataset has n = 32 561 training and 16 281 test examples with
123-dimensional features. We use binary logistic loss `i(x) = log(1 + exp(bia>i x)) to classify
whether the income level is greater than $5K. For the Reuters RCV1 Corpus  our task is to classify
whether a document belongs to the Corporate category. With d = 47 236 features  we randomly
split the 804 410 examples into 723 969 training (90% of data) and 80 441 (10% of data) test
examples. We use the hinge loss and solve the binary classiﬁcation problem for the document type.
To test the efﬁciency of our method in large scale settings  we plot the log ratio log Rn(x)
Rn(x?)  where
Rn(x) = supp2P⇢ n  p>`(x)  versus CPU time for our algorithm and gradient descent in Figure 1b.
As is somewhat typical of stochastic gradient-based methods  our bandit-based optimization algorithm
quickly obtains a solution with small optimality gap (about 2% relative error)  while the gradient
descent method eventually achieves better loss.
In Figures 2a–2d  we plot the loss value and the classiﬁcation error compared with applying pure
stochastic gradient descent to the standard empirical loss  plotting the conﬁdence bound for the robust

7

(a) Synthetic Data (n = 2000  d = 500)

(b) Reuters Corpus (n = 7.2 · 105  d ⇡ 5 · 104)

Figure 1: Comparison of Solvers

(a) Adult: Logistic Loss

(b) Adult: Classiﬁcation Error

(c) Reuters: Hinge Loss

(d) Reuters: Classiﬁcation Error

Figure 2: Comparison with ERM

method as well. As the theory suggests [15  13]  the robust objective provides upper conﬁdence
bounds on the true risk (approximated by the average loss on the test sample).

Acknowledgments
JCD and HN were partially supported by the SAIL-Toyota Center for AI Research and the National
Science Foundation award NSF-CAREER-1553086. HN was also partially supported Samsung
Fellowship.

8

References
[1] J.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. In

Journal of Machine Learning Research  pages 2635–2686  2010.

[2] A. Ben-Tal  L. E. Ghaoui  and A. Nemirovski. Robust Optimization. Princeton University Press 

2009.

[3] A. Ben-Tal  D. den Hertog  A. D. Waegenaere  B. Melenberg  and G. Rennen. Robust solutions
of optimization problems affected by uncertain probabilities. Management Science  59(2):
341–357  2013.

[4] A. Ben-Tal  E. Hazan  T. Koren  and S. Mannor. Oracle-based robust optimization via online

learning. Operations Research  63(3):628–638  2015.

[5] J. Borwein  A. J. Guirao  P. Hájek  and J. Vanderwerff. Uniformly convex functions on Banach

spaces. Proceedings of the American Mathematical Society  137(3):1081–1091  2009.

[6] S. Boucheron  O. Bousquet  and G. Lugosi. Theory of classiﬁcation: a survey of some recent

advances. ESAIM: Probability and Statistics  9:323–375  2005.

[7] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[8] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

[9] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

Press  2001.

[10] K. Clarkson  E. Hazan  and D. Woodruff. Sublinear optimization for machine learning. Journal

of the Association for Computing Machinery  59(5)  2012.

[11] T. H. Cormen  C. E. Leiserson  R. L. Rivest  and C. Stein. Introduction to Algorithms. MIT

[12] N. Cressie and T. R. Read. Multinomial goodness-of-ﬁt tests. Journal of the Royal Statistical

Society. Series B (Methodological)  pages 440–464  1984.

[13] J. C. Duchi and H. Namkoong. Statistics of robust optimization: A generalized empirical
likelihood approach. arXiv:1610.02581 [stat.ML]  2016. URL https://arxiv.org/abs/
1610.02581.

[14] J. C. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the
`1-ball for learning in high dimensions. In Proceedings of the 25th International Conference on
Machine Learning  2008.

[15] J. C. Duchi  P. W. Glynn  and H. Namkoong. Statistics of robust optimization: A generalized
empirical likelihood approach. arXiv:1610.03425 [stat.ML]  2016. URL https://arxiv.
org/abs/1610.03425.

[16] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization  I: a generic algorithmic framework. SIAM Journal on
Optimization  22(4):1469–1492  2012.

[17] I. Gurobi Optimization. Gurobi optimizer reference manual  2015. URL http://www.gurobi.

com.

ml.

[18] E. Hazan. The convex optimization approach to regret minimization. In Optimization for

Machine Learning  chapter 10. MIT Press  2012.

[19] E. Hazan and S. Kale. An optimal algorithm for stochastic strongly convex optimization. In
Proceedings of the Twenty Fourth Annual Conference on Computational Learning Theory  2011.
[20] J. Hiriart-Urruty and C. Lemaréchal. Convex Analysis and Minimization Algorithms I & II.

Springer  New York  1993.

[21] D. Lewis  Y. Yang  T. Rose  and F. Li. RCV1: A new benchmark collection for text categorization

research. Journal of Machine Learning Research  5:361–397  2004.

[22] M. Lichman. UCI machine learning repository  2013. URL http://archive.ics.uci.edu/

[23] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[24] A. B. Owen. Empirical likelihood. CRC press  2001.
[25] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to

Algorithms. Cambridge University Press  2014.

[26] S. Shalev-Shwartz and Y. Wexler. Minimizing the maximal loss: How and why? In Proceedings

of the 32nd International Conference on Machine Learning  2016.

[27] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In

Proceedings of the Twentieth International Conference on Machine Learning  2003.

9

,Hongseok Namkoong
John Duchi
Lars Mescheder
Sebastian Nowozin
Andreas Geiger