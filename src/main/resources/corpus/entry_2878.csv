2014,Learning Chordal Markov Networks by Dynamic Programming,We present an algorithm for finding a chordal Markov network that maximizes any given decomposable scoring function. The algorithm is based on a recursive characterization of clique trees  and it runs in O(4^n) time for n vertices. On an eight-vertex benchmark instance  our implementation turns out to be about ten million times faster than a recently proposed  constraint satisfaction based algorithm (Corander et al.  NIPS 2013). Within a few hours  it is able to solve instances up to 18 vertices  and beyond if we restrict the maximum clique size. We also study the performance of a recent integer linear programming algorithm (Bartlett and Cussens  UAI 2013). Our results suggest that  unless we bound the clique sizes  currently only the dynamic programming algorithm is guaranteed to solve instances with around 15 or more vertices.,Learning Chordal Markov Networks

by Dynamic Programming

Kustaa Kangas

Teppo Niinim¨aki Mikko Koivisto

Helsinki Institute for Information Technology HIIT

Department of Computer Science  University of Helsinki

{jwkangas tzniinim mkhkoivi}@cs.helsinki.fi

Abstract

We present an algorithm for ﬁnding a chordal Markov network that maximizes
any given decomposable scoring function. The algorithm is based on a recursive
characterization of clique trees  and it runs in O(4n) time for n vertices. On
an eight-vertex benchmark instance  our implementation turns out to be about
ten million times faster than a recently proposed  constraint satisfaction based
algorithm (Corander et al.  NIPS 2013). Within a few hours  it is able to solve
instances up to 18 vertices  and beyond if we restrict the maximum clique size.
We also study the performance of a recent integer linear programming algorithm
(Bartlett and Cussens  UAI 2013). Our results suggest that  unless we bound the
clique sizes  currently only the dynamic programming algorithm is guaranteed to
solve instances with around 15 or more vertices.

1

Introduction

Structure learning in Markov networks  also known as undirected graphical models or Markov
random ﬁelds  has attracted considerable interest in computational statistics  machine learning  and
artiﬁcial intelligence. Natural score-and-search formulations of the task have  however  proved to be
computationally very challenging. For example  Srebro [1] showed that ﬁnding a maximum-likelihood
chordal (or triangulated or decomposable) Markov network is NP-hard even for networks of treewidth
at most 2  in sharp contrast to the treewidth-1 case [2]. Consequently  various approximative
approaches and local search heuristics have been proposed [3  1  4  5  6  7  8  9  10  11].
Only very recently  Corander et al. [12] published the ﬁrst non-trivial algorithm that is guaranteed to
ﬁnd a globally optimal chordal Markov network. It is based on expressing the search space in terms of
logical constraints and employing the state-of-the-art solver technology equipped with optimization
capabilities. To this end  they adopt the usual clique tree  or junction tree  representation of chordal
graphs  and work with a particular characterization of clique trees  namely  that for any vertex of the
graph the cliques containing that vertex induce a connected subtree in the clique tree. The key idea
is to rephrase this property as what they call a balancing condition: for any vertex  the number of
cliques that contain it is one larger than the number of edges (the intersection of the adjacent cliques)
that contain it. They show that with appropriate  efﬁcient encodings of the constraints  an eight-vertex
instance can be solved to the optimum in a few days of computing  which could have been impossible
by a brute-force search. However  while the constraint satisfaction approach enables exploiting the
powerful technology  it is currently not clear  whether it scales to larger instances.
Here  we investigate an alternative approach to ﬁnd an optimal chordal Markov network. Like the
work of Corander at al. [12]  our algorithm stems from a particular characterization of clique trees of
chordal graphs. However  our characterization is quite different  being recursive in nature. It concords
the structure of common scoring functions and so yields a natural dynamic programming algorithm
that grows an optimal clique tree by selecting its cliques one by one. In its basic form  the algorithm

1

is very inefﬁcient. Fortunately  the ﬁne structure of the scoring function enables us to further factorize
the main dynamic programming step and so bring the time requirement down to O(4n) for instances
with n vertices. We also show that by setting the maximum clique size  equivalently the treewidth

(plus one)  to w ≤ n/4  the time requirement can be improved to O(cid:0)3n−w(cid:0)n

(cid:1)w(cid:1).

w

While our recursive characterization of clique trees and the resulting dynamic programming algorithm
are new  they are similar in spirit to a recent work by Korhonen and Parviainen [13]. Their algorithm
ﬁnds a bounded-treewidth Bayesian network structure that maximizes a decomposable score  running
in 3nnw+O(1) time  where w is the treewidth bound. For large w it thus is superexponentially slower
than our algorithm. The problems solved by the two algorithms are  of course  different: the class of
treewidth-w Bayesian networks properly extends the class of treewidth-w chordal Markov networks.
There is also more recent work for ﬁnding bounded-treewidth Bayesian networks by employing
constraint solvers: Berg et al. [14] solve the problem by casting into maximum satisﬁability  while
Parviainen et al. [15] cast into integer linear programming. For unbounded-treewidth Bayesian
networks  O(2nn2)-time algorithms based on dynamic programming are available [16  17  18].
However  none of these dynamic programming algorithms  nor their A* search based variant [19] 
enables adding the constraints of chordality or bounded width.
But the integer linear programming approach to ﬁnding optimal Bayesian networks  especially the
recent implementation by Bartlett and Cussens [20]  also enables adding the further constraints.1
We are not aware of any reasonable worst-case bounds for the algorithm’s time complexity  nor any
previous applications of the algorithm to the problem of learning chordal Markov networks. As a
second contribution of this paper  we report on an experimental study of the algorithm’s performance 
using both synthetic data and some frequently used machine learning benchmark datasets.
The remainder of this article begins by formulating the learning task as an optimization problem. Next
we present our recursive characterization of clique trees and a derivation of the dynamic programming
algorithm  with a rigorous complexity analysis. The experimental setting and results are reported in a
dedicated section. We end with a brief discussion.

2 The problem of learning chordal Markov networks

clique. Together  G and p form a Markov network if p(x1  . . .   xn) =(cid:81)

We adopt the hypergraph treatment of chordal Markov networks. For a gentler presentation and
proofs  see Lauritzen and Spiegelhalter [21  Sections 6 and 7]  Lauritzen [22]  and references therein.
Let p be a positive probability function over a product of n state spaces. Let G be an undirected
graph on the vertex set V = {1  . . .   n}  and call any maximal set of pairwise adjacent vertices of G a
C ψC(xC)  where C runs
through the cliques of G and each ψC is a mapping to positive reals. Here xC denotes (xv : v ∈ C).
The factors ψC take a particularly simple form when the graph G is chordal  that is  when every cycle
of G of length greater than three has a chord  which is an edge of G joining two nonconsecutive
vertices of the cycle. The chordality requirement can be expressed in terms of hypergraphs. Consider
ﬁrst an arbitrary hypergraph on V   identiﬁed with a collection C of subsets of V such that each
element of V belongs to some set in C. We call C reduced if no set in C is a proper subset of another
set in C  and acyclic if  in addition  the sets in C admit an ordering C1  . . .   Cm that has the running
intersection property: for each 2 ≤ j ≤ m  the intersection Sj = Cj ∩ (C1 ∪ ··· ∪ Cj−1) is a subset
of some Ci with i < j. We call the sets Sj the separators. The multiset of separators  denoted by
S  does not depend on the ordering and is thus unique for an acyclic hypergraph. Now  letting C be
the set of cliques of the chordal graph G  it is known that the hypergraph C is acyclic and that each
factor ψCj (xCj ) can be speciﬁed as the ratio p(xCj )/p(xSj ) of marginal probabilities (where we
deﬁne p(xS1) = 1). Also the converse holds: by connecting all pairs of vertices within each set of an
acyclic hypergraph we obtain a chordal graph.
Given multiple observations over the product state space  the data  we associate with each hyper-
S∈S p(S)  where the local score p(A) measures the
probability (density) of the data projected on A ⊆ V   possibly extended by some structure prior
or penalization term. The structure learning problem is to ﬁnd an acyclic hypergraph C on V that

graph C on V a score s(C) =(cid:81)

C∈C p(C)(cid:14)(cid:81)

1We thank an anonymous reviewer of an earlier version of this work for noticing this fact  which apparently

was not well known in the community  including the authors and reviewers of Corander’s et al. work [12].

2

maximizes the score s(C). This formulation covers a Bayesian approach  in which each p(A) is the
marginal likelihood for the data on A under a Dirichlet–multinomial model [23  7  12]  but also the
maximum-likelihood formulation  in which each p(A) is the empirical probability of the data on
A [23  1]. Motivated by these instantiations  we will assume that for any given A the value p(A) can
be efﬁciently computed  and we treat the values as the problem input.
Our approach to the problem exploits the fact [22  Prop. 2.27] that a reduced hypergraph C is acyclic
if and only if there is a junction tree T for C  that is  an undirected tree on the node set C that has the
junction property (JP): for any two nodes A and B in C and any C on the unique path in T between
A and B we have A ∩ B ⊆ C. Furthermore  by labeling each edge of T by the intersection of its
endpoints  the edge labels amount to the multiset of separators of the hypergraph C. Thus a junction
tree gives the separators explicitly  which motivates us to write s(T ) for the respective score s(C)
and solve the structure learning problem by ﬁnding a junction tree T over V that maximizes s(T ).
Here and henceforth  we say that a tree is over a set if the union of the tree’s nodes equals the set.
As our problem formulation does not explicitly refer to the underlying chordal graph and cliques  we
will speak of junction trees instead of equivalent but semantically more loaded clique trees. From
here on  a junction tree refers speciﬁcally to a junction tree whose node set is a reduced hypergraph.

3 Recursive characterization and dynamic programming

The score of a junction tree obeys a recursive factorization along subtrees (by rooting the tree at any
node)  given in Section 3.2 below. While this is the essential structural property of the score for our
dynamic programming algorithm  it does not readily yield the needed recurrence for the optimal
score. Indeed  we need a characterization of  not a ﬁxed junction tree  but the entire search space
of junction trees that concords the factorization of the score. We next give such a characterization
before we proceed to the derivation and analysis of the dynamic programming algorithm.

3.1 Recursive partition trees

We characterize the set of junction trees by expressing the ways in which they can partition V . The
idea is that when any tree of interest is rooted at some node  the subtrees amount to a partition of not
only the remaining nodes in the tree (which holds trivially) but also the remaining vertices (contained
in the nodes); and the subtrees also satisfy this property. See Figure 1 for an illustration.
If T is a tree over a set S  we write C(T ) for its node set and V (T ) for the union of its nodes  S. For
a family R of subsets of a set S  we say that R is a partition of S and denote R (cid:64) S if the members
of R are non-empty and pairwise disjoint  and their union is S.
Deﬁnition 1 (Recursive partition tree  RPT). Let T be a tree over a ﬁnite set V   rooted at C ∈
C(T ). Denote by C1  . . .   Ck the children of C  by Ti the subtree rooted at Ci  and let Ri = V (Ti)\C.
We say that T is a recursive partition tree (RPT) if it satisﬁes the following three conditions: (R1)
each Ti is a RPT over Ci ∪ Ri  (R2) {R1  . . .   Rk} (cid:64) V \ C  and (R3) C ∩ Ci is a proper subset of
both C and Ci. We denote by RPT(V  C) the set of all RPTs over V rooted at C.

We now present the following theorems to establish that  when edge directions are ignored  the
deﬁnitions of junction trees and recursive partition trees are equivalent.
Theorem 1. A junction tree T is a RPT when rooted at any C ∈ C(T ).
Theorem 2. A RPT is a junction tree (when considered undirected).

Our proofs of these results will use the following two observations:
Observation 3. A subtree of a junction tree is also a junction tree.
Observation 4. If T is a RPT  so is its every subtree rooted at any C ∈ C(T ).

Proof of Theorem 1. Let T be a junction tree over V and consider an arbitrary C ∈ C(T ). We show
by induction over the number of nodes that T is a RPT when rooted at C. Let Ci  Ti  and Ri be
deﬁned as in Deﬁnition 1 and consider the three RPT conditions. If C is the only node in T   the
conditions hold trivially. Assume they hold up to n − 1 nodes and consider the case |C(T )| = n. We
show that each condition holds.

3

Figure 1: An example of a chordal graph and a
corresponding recursive partition. The root node
C = {3  4  5} (dark grey) partitions the remaining
vertices into three disjoint sets R1 = {0  1  2} 
R2 = {6}  and R3 = {7  8  9} (light grey)  which
are connected to the root node by its child nodes
C1 = {1  2  3}  C2 = {4  5  6}  and C3 = {5  7}
respectively (medium grey).

i Ri =(cid:83)

reduced hypergraph Ci is non-empty and not contained in C. Second (cid:83)
(C ∪(cid:83)

i V (Ti)) \ C =(cid:83)C(T ) \ C = V \ C. Finally  to see that Ri are pairwise disjoint  assume to

(R1) By Observation 3 each Ti is a junction tree and thus  by the induction assumption  a RPT. It
remains to show that V (Ti) = Ci ∪ Ri. By deﬁnition both Ci ⊆ V (Ti) and Ri ⊆ V (Ti). Thus
Ci ∪ Ri ⊆ V (Ti). Assume then that x ∈ V (Ti)  i.e. x ∈ C(cid:48) for some C(cid:48) ∈ C(Ti). If x /∈ Ri 
then by deﬁnition x ∈ C. Since Ci is on the path between C and C(cid:48)  by JP x ∈ Ci. Therefore
V (Ti) ⊆ Ci ∪ Ri.
(R2) We show that the sets Ri partition V \ C. First  each Ri is non-empty since by deﬁnition of
i(V (Ti) \ C) =
the contrary that x ∈ Ri ∩ Rj for distinct Ri and Rj. This implies x ∈ A ∩ B for some A ∈ C(Ti)
and B ∈ C(Tj). Now  by JP x ∈ C  which contradicts the deﬁnition of Ri.
(R3) Follows by the deﬁnition of reduced hypergraph.
Proof of Theorem 2. Assume now that T is a RPT over V . We show that T is a junction tree. To see
that T has JP  consider arbitrary A  B ∈ C(T ). We show that A ∩ B is a subset of every C ∈ C(T )
on the path between A and B.
Consider ﬁrst the case that A is an ancestor of B and let B = C1  . . .   Cm = A be the path that
connects them. We show by induction over m that C1 ∩ Cm ⊆ Ci for every i = 1  . . .   m. The base
case m = 1 is trivial. Assume m > 1 and the claim holds up to m − 1. If i = m  the claim is trivial.
Let i < m. Denote by Tm−1 the subtree rooted at Cm−1 and let Rm−1 = V (Tm−1) \ Cm. Since
C1 ⊆ V (Tm−1) we have that C1 ∩ Cm = (C1 ∩ V (Tm−1)) ∩ Cm = C1 ∩ (Cm ∩ V (Tm−1)). By
Observation 4 Tm−1 is a RPT. Therefore  from (R1) it follows that V (Tm−1) = Cm−1 ∪ Rm−1 and
thus Cm ∩ V (Tm−1) = (Cm ∩ Cm−1) ∪ (Cm ∩ Rm−1) = Cm ∩ Cm−1. Plugging this above and
using the induction assumption we get C1 ∩ Cm = C1 ∩ (Cm ∩ Cm−1) ⊆ C1 ∩ Cm−1 ⊆ Ci.
Consider now the case that A and B have a least common ancestor C. By Observation 4  the subtree
rooted at C is a RPT. Thus  by (R1) and (R2) there are disjoint R and R(cid:48) such that A ⊆ C ∪ R and
B ⊆ C ∪ R(cid:48). Thus  A ∩ B ⊆ C  and consequently A ∩ B ⊆ A ∩ C. As we proved above  A ∩ C is
a subset of every node on the path between A and C  and therefore A ∩ B is also a subset of every
such node. Similarly  A ∩ B is a subset of every node on the path between B and C. Combining
these results  we have that A ∩ B is a subset of every node on the path between A and B.
Finally  to see that C(T ) is reduced  assume the opposite  that A ⊆ B for distinct A  B ∈ C(T ). Let
C be the node next to A on the path from A to B. By the initial assumption and JP A ⊆ A ∩ B ⊆ C.
As either A or C is a child of the other  this contradicts (R3) in the subtree rooted at the parent.

3.2 The main recurrence
We want to ﬁnd a junction tree T over V that maximizes the score s(T ). By Theorems 1 and 2 this
is equivalent to ﬁnding a RPT T that maximizes s(T ). Let T be a RPT rooted at C and denote by
C1  . . .   Ck the children of C and by Ti the subtree rooted at Ci. Then  the score factorizes as follows

s(T ) = p(C)

(1)
To see this  observe that each term of s(T ) is associated with a particular node or edge (separator) of
T . Thus the product of the s(Ti) consists of exactly the terms of s(T )  except for the ones associated
with the root C of T and the edges between C and each Ci.

p(C ∩ Ci)

i=1

k(cid:89)

s(Ti)

.

4

0123546789To make use of the above factorization  we introduce suitable constraints under which an optimal
tree can be constructed from subtrees that are  in turn  optimal with respect to analogous constraints
(cf. Bellman’s principle of optimality). Speciﬁcally  we deﬁne a function f that gives the score of an
optimal subtree over any subset of nodes as follows:
Deﬁnition 2. For S ⊂ V and ∅ (cid:54)= R ⊆ V \ S  let f (S  R) be the score of an optimal RPT over
S ∪ R rooted at a proper superset of S. That is

f (S  R) =

max

S ⊂ C ⊆ S ∪ R
T ∈RPT(S∪R C)

s(T ) .

Corollary 5. The score of an optimal RPT over V is given by f (∅  V ).
We now show that f admits the following recurrence  which shall be used as the basis of our dynamic
programming algorithm.
Lemma 6. Let S ⊂ V and ∅ (cid:54)= R ⊆ V \ S. Then

f (S  R) =

max

{R1  . . .   Rk} (cid:64) R \ C

S ⊂ C ⊆ S ∪ R
S1  . . .   Sk ⊂ C

p(C)

k(cid:89)

i=1

f (Si  Ri)

p(Si)

.

Proof. We ﬁrst show inductively that the recurrence is well deﬁned. Assume that the conditions
S ⊂ V and ∅ (cid:54)= R ⊆ V \ S hold. Observe that R is non-empty  every set has a partition  and C
is selected to be non-empty. Therefore  all three maximizations are over non-empty ranges and it
remains to show that the product over i = 1  . . .   k is well deﬁned. If |R| = 1  then R \ C = ∅ and
the product equals 1 by convention. Assume now that f (S  R) is deﬁned when |R| < m and consider
the case |R| = m. By construction Si ⊂ V   ∅ (cid:54)= Ri ⊆ V \ Si and |Ri| < |R| for every i = 1  . . .   k.
Thus  by the induction assumption each f (Si  Ri) is deﬁned and therefore the product is deﬁned.
We now show that the recurrence indeed holds. Let the root C in Deﬁnition 2 be ﬁxed and consider the
maximization over the trees T . By Deﬁnition 1  choosing a tree T ∈ RPT(S ∪ R  C) is equivalent
to choosing sets R1  . . .   Rk  sets C1  . . .   Ck  and trees T1  . . .  Tk such that (R0) Ri = V (Ti) \ C 
(R1) Ti is a RPT over Ci ∪ Ri rooted at Ci  (R2) {R1  . . .   Rk} (cid:64) (S ∪ R) \ C  and (R3) C ∩ Ci is
a proper subset of C and Ci.
Observe ﬁrst that (S ∪ R) \ C = R \ C and therefore (R2) is equivalent to choosing sets Ri such
that {R1  . . .   Rk} (cid:64) R \ C.
Denote by Si the intersection C ∩ Ci. We show that together (R0) and (R1) are equivalent to
saying that Ti is a RPT over Si ∪ Ri rooted at Ci. Assume ﬁrst that the conditions are true. By
(R1) it’s sufﬁcient to show that Ci ∪ Ri = Si ∪ Ri. From (R1) it follows that Ci ⊆ V (Ti)
and therefore Ci \ C ⊆ V (Ti) \ C  which by (R0) implies Ci \ C ⊆ Ri. This in turn implies
Ci ∪ Ri = (Ci ∩ C)∪ (Ci \ C)∪ Ri = Si ∪ Ri. Assume then that Ti is a RPT over Si ∪ Ri rooted at
Ci. Condition (R0) holds since V (Ti) \ C = (Si ∪ Ri) \ C = (Si \ C) ∪ (Ri \ C) = ∅ ∪ Ri = Ri.
Condition (R1) holds since Si ⊆ Ci ⊆ V (Ti) = Si ∪ Ri and thus Si ∪ Ri = Ci ∪ Ri.
Finally observe that (R3) is equivalent to ﬁrst choosing Si ⊂ C and then Ci ⊃ Si. By (R1) it must
also be that Ci ⊆ V (Ti) = Si ∪ Ri. Based on these observations  we can now write

s(T ) .

f (S  R) =

max

{R1  . . .   Rk} (cid:64) R \ C

S ⊂ C ⊆ S ∪ R
S1 ... Sk⊂C

∀i:Si⊂Ci⊆Ri∪Si

∀i:Ti is a RPT over Si ∪ Ri rooted at Ci

Next we factorize s(T ) using the factorization (1) of the score. In addition  once a root C  a partition
{R1  . . .   Rk}  and separators {S1  . . .   Sk} have been ﬁxed  then each pair (Ci Ti) can be chosen
independently for different i. Thus  the above maximization can be written as

 1

p(Si)

k(cid:89)

i=1

max

{R1  . . .   Rk} (cid:64) R \ C

S ⊂ C ⊆ S ∪ R
S1 ... Sk⊂C

p(C)

 .

·
Ti∈RPT(Si∪Ri Ci)

Si⊂Ci⊆Ri∪Si

max

s(Ti)

By applying Deﬁnition 2 to the inner maximization the claim follows.

5

3.3 Fast evaluation

p(C)

k(cid:89)
f (S  R)(cid:14)p(S) .

i=1

The direct evaluation of the recurrence in Lemma 6 would be very inefﬁcient  especially since it
involves maximization over all partitions of the vertex set. In order to evaluate it more efﬁciently  we
decompose it into multiple recurrences  each of which can take advantage of dynamic programming.
Observe ﬁrst that we can rewrite the recurrence as

f (S  R) =

max

S ⊂ C ⊆ S ∪ R

{R1  . . .   Rk} (cid:64) R \ C

h(C  Ri)  

(2)

where

(3)
We have simply moved the maximization over Si ⊂ C inside the product and written each factor
using a new function h. Due to how the sets C and Ri are selected  the arguments to h are always
non-empty and disjoint subsets of V . In a similar fashion  we can further rewrite recurrence 2 as

h(C  R) = max
S⊂C

p(C)g(C  R \ C)  

(4)

f (S  R) = max

S⊂C⊆S∪R

where we deﬁne

g(C  U ) =

max

{R1 ... Rk}(cid:64)U

k(cid:89)

h(C  Ri) .

i=1

Again  note that C and U are disjoint and C is non-empty. If U = ∅  then g(C  U ) = 1. Otherwise

g(C  U ) = max
∅(cid:54)=R⊆U

h(C  R)

max

{R2 ... Rk}(cid:64)U\R

h(C  Ri) = max
∅(cid:54)=R⊆U

h(C  R)g(C  U \ R) .

(5)

k(cid:89)

i=2

Thus  we have split the original recurrence into three simpler recurrences (4 5 3). We now obtain a
straightforward dynamic programming algorithm that evaluates f  g and h using these recurrences
with memoization  and then outputs the score f (∅  V ) of an optimal RPT.

3.4 Time and space requirements

We measure the time requirement by the number of basic operations  namely comparisons and
arithmetic operations  executed for pairs of real numbers. Likewise  we measure the space requirement
by the maximum number of real values stored at any point during the execution of the algorithm.
We consider both time and space in the more general setting where the width w ≤ n of the optimal
network is restricted by selecting every node (clique) C in recurrence (4) with the constraint |C| ≤ w.
We prove the following bounds by counting  for each of the three functions  the associated subset
triplets that meet the applicable disjointness  inclusion  and cardinality constraints:
Theorem 7. Let V be a set of size n and w ≤ n. Given the local scores of the subsets of V of size
at most w as input  a maximum-score junction tree over V of width at most w can be found using

(cid:0)n
(cid:1)3n−i basic operations and having a storage for 3(cid:80)w

(cid:1)2n−i real numbers.

6(cid:80)w

(cid:0)n

i=0

i

i=0

i

Proof. To bound the number of basic operations needed  we consider the evaluation of each the
functions f  g  and h using the recurrences (4 5 3). Consider ﬁrst f. Due to memoization  the
algorithm executes at most two basic operations (one comparison and one multiplication) per triplet
(S  R  C)  with S and R disjoint  S ⊂ C ⊆ S ∪ R  and |C| ≤ w. Subject to these constraints  a set C

of size i can be chosen in(cid:0)n
Thus  the number of basic operations needed is at most Nf = 2(cid:80)w
|C| ≤ w  and ∅ (cid:54)= R ⊆ U. A set C of size i can be chosen in(cid:0)n

(cid:1) ways  the set S ⊂ C in at most 2i ways  and the set R\ C in 2n−i ways.
(cid:0)n
(cid:1).
(cid:0)n
(cid:1) ways  and the remaining n − i

i
Similarly  for h the algorithm executes at most two basic operations per triplet (C  R  S)  with now C
and R disjoint  |C| ≤ w  and S ⊂ C. A calculation gives the same bound as for f. Finally consider g.
Now the algorithm executes at most two basic operations per triplet (C  U  R)  with C and U disjoint 
elements can be assigned into U and its subset R in 3n−i ways. Thus  the number of basic operations

(cid:1)2n−i2i = 2n+1(cid:80)w

i=0

i=0

i

i

i

6

w = 3

w = 4

w = 5

w = 6

w = ∞

(cid:0)n

Figure 2: The running time of Junctor and GOBNILP as a function of the number of vertices for
varying widths w  on sparse (top) and dense (bottom) synthetic instances with 100 (“small”)  1000
(“medium”)  and 10 000 (“large”) data samples. The dashed red line indicates the 4-hour timeout or
memout. For GOBNILP shown is the median of the running times on 15 random instances.

needed is at most Ng = 2(cid:80)w
(cid:1)3n−i is larger than(cid:0)n
(cid:0)n

(cid:1)3n−i. Finally  it is sufﬁcient to observe that there is a j such that
(cid:1)2n when i ≤ j  and smaller when i > j. Now because both terms sum up

i
to the same value 4n when i = 0  . . .   n  the bound Ng is always greater or equal to Nf .
We bound the storage requirement in a similar manner. For each function  the size of the ﬁrst argument
is at most w and the second argument is disjoint from the ﬁrst  yielding the claimed bound.
Remark 1. For w = n  the bounds for the number of basic operations and storage requirement in
Theorem 7 become 6 · 4n and 3 · 3n  respectively. When w ≤ n/4  the former bound can be replaced

(cid:1)3n−i−1 if and only if i ≤ (n − 3)/4.

i

i=0

i

(cid:1)3n−i ≤(cid:0) n

(cid:1)3n−w  since(cid:0)n

by 6w(cid:0)n
space by mapping a pair of sets (A  B) to(cid:80)n

i+1

w

i

Remark 2. Memoization requires indexing with pairs of disjoint sets. Representing sets as integers
allows efﬁcient lookups to a two-dimensional array  using O(4n) space. We can achieve O(3n)
a=1 3a−1Ia(A  B) where Ia(A  B) is 1 if a ∈ A  2 if
a ∈ B  and 0 otherwise. Each pair gets a unique index from 0 to 3n − 1 to a compact array. A na¨ıve
evaluation of the index adds an O(n) factor to the running time. This can be improved to constant
amortized time by updating the index incrementally while iterating over sets.

4 Experimental results

We have implemented the presented algorithm in a C++ program Junctor (Junction Trees Optimally
Recursively).2 In the experiments reported below  we compared the performance of Junctor and the
integer linear programming based solver GOBNILP by Bartlett and Cussens [20]. While GOBNILP
has been tailored for ﬁnding an optimal Bayesian network  it enables forbidding the so-called
v-structures in the network and  thereby  ﬁnding an optimal chordal Markov network  provided that
we use the BDeu score  as we have done  or some other special scoring function [23  24]. We note
that when forbidding v-structures  the standard score pruning rules [20  25] are no longer valid.
We ﬁrst investigated the performance on synthetic data generated from Bayesian networks of varying
size and density. We generated 15 datasets for each combination of the number of vertices n from 8 to
18  maximum indegree k = 4 (sparse) or k = 8 (dense)  and the number of samples m equaling 100 
1000  or 10 000  as follows: Along a random vertex ordering  we ﬁrst drew for each vertex the number
of its parents from the uniform distribution between 0 and k and then the actual parents uniformly
at random from its predecessors in the vertex ordering. Next  we assigned each vertex two possible
states and drew the parameters of the conditional distributions from the uniform distribution. Finally 
from the obtained joint distribution  we drew m independent samples. The input for Junctor and

2Junctor is publicly available at www.cs.helsinki.fi/u/jwkangas/junctor/.

7

810121416181s 60s 1h Junctor  anyGOBNILP  largeGOBNILP  mediumGOBNILP  small810121416181s 60s 1h 810121416181s 60s 1h 810121416181s 60s 1h 810121416181s 60s 1h 810121416181s 60s 1h 810121416181s 60s 1h 810121416181s 60s 1h 810121416181s 60s 1h 810121416181s 60s 1h Table 1: Benchmark instances with different numbers of attributes (n) and samples (m).

Dataset
Tic-tac-toe
Poker
Bridges
Flare
Zoo

Abbr.

X
P
B
F
Z

n
10
11
12
13
17

m
958
10000
108
1066
101

Abbr.

V
T
L

Dataset
Voting
Tumor
Lymph
Hypothyroid
Mushroom

n
17
18
19
22
22

m
435
339
148
3772
8124

w = 3

w = 4

w = 5

w = 6

w = ∞

Figure 3: The running time of Junctor against GOBNILP on the benchmark instances with at most
19 attributes  given in Table 1. The dashed red line indicates the 4-hour timeout or memout.

GOBNILP was produced using the BDeu score with equivalent sample size 1. For both programs  we
varied the maximum width parameter w from 3 to 6 and  in addition  examined the case of unbounded
width (w = ∞). Because the performance of Junctor only depends on n and w  we ran it only
once for each combination of the two. In contrast  the performance of GOBNILP is very sensitive to
various characteristics of the data  and therefore we ran it for all the combinations. All runs were
allowed 4 CPU hours and 32 GB of memory. The results (Figure 2) show that for large widths
Junctor scales better than GOBNILP (with respect to n)  and even for low widths Junctor is
superior to GOBNILP for smaller n. We found GOBNILP to exhibit moderate variance: 93% of all
running times (excluding timeouts) were within a factor of 5 of the respective medians shown in
Figure 2  while 73% were within a factor of 2. We observe that the running time of GOBNILP may
behave “discontinuously” (e.g.  small datasets around 15 vertices with width 4).
We also evaluated both programs on several benchmark instances taken from the UCI repository [26].
The datasets are summarized in Table 1. Figure 3 shows the results on the instances with at most 19
attributes  for which the runs were  again  allowed 4 CPU hours and 32 GB of memory. The results
are qualitatively in well agreement with the results obtained with synthetic data. For example  solving
the Bridges dataset on 12 attributes with width 5  takes less than one second by Junctor but around
7 minutes by GOBNILP. For the two 22-attribute datasets we allowed both programs one week of
CPU time and 128 GB of memory. Junctor was able to solve each within 33 hours for w = 3 and
within 74 hours for w = 4. GOBNILP was able to solve Hypothyroid up to w = 6 (in 24 hours  or
less for small widths)  but Mushroom only up to w = 3. For higher widths GOBNILP ran out of time.

5 Concluding remarks

We have investigated the structure learning problem in chordal Markov networks. We showed that the
commonly used scoring functions factorize in a way that enables a relatively efﬁcient dynamic pro-
gramming treatment. Our algorithm is the ﬁrst that is guaranteed to solve moderate-size instances to
the optimum within reasonable time. For example  whereas Corander et al. [12] report their algorithm
took more than 3 days on an eight-variable instance  our Junctor program solves any eight-variable
instance within 20 milliseconds. We also reported on the ﬁrst evaluation of GOBNILP [20] for solving
the problem  which highlighted the advantages of the dynamic programming approach.

Acknowledgments

This work was supported by the Academy of Finland  grant 276864. The authors thank Matti J¨arvisalo
for useful discussions on constraint programming approaches to learning Markov networks.

8

1s 60s 1h Junctor1s 60s 1h GOBNILPBFLPXTVZ1s 60s 1h Junctor1s 60s 1h GOBNILPBFLPXTVZ1s 60s 1h Junctor1s 60s 1h GOBNILPBFLPXTVZ1s 60s 1h Junctor1s 60s 1h GOBNILPBFLPXTVZ1s 60s 1h Junctor1s 60s 1h GOBNILPBFLPXTVZReferences
[1] N. Srebro. Maximum likelihood bounded tree-width Markov networks. Artiﬁcial Intelligence  143(1):123–

138  2003.

[2] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE

Transactions on Information Theory  14:462–467  1968.

[3] S. Della Pietra  V. J. Della Pietra  and J. D. Lafferty. Inducing features of random ﬁelds. IEEE Transactions

on Pattern Analysis and Machine Intelligence  19(4):380–393  1997.

[4] M. Narasimhan and J. A. Bilmes. PAC-learning bounded tree-width graphical models. In D. M. Chickering

and J. Y. Halpern  editors  UAI  pages 410–417. AUAI Press  2004.

[5] P. Abbeel  D. Koller  and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity.

Journal of Machine Learning Research  7:1743–1788  2006.

[6] A. Chechetka and C. Guestrin. Efﬁcient principled learning of thin junction trees. In J. C. Platt  D. Koller 

Y. Singer  and S. T. Roweis  editors  NIPS. Curran Associates  Inc.  2007.

[7] J. Corander  M. Ekdahl  and T. Koski. Parallell interacting MCMC for learning of topologies of graphical

models. Data Mining and Knowledge Discovery  17(3):431–456  2008.

[8] G. Elidan and S. Gould. Learning bounded treewidth Bayesian networks. Journal of Machine Learning

Research  9:2699–2731  2008.

[9] F. Bromberg  D. Margaritis  and V. Honavar. Efﬁcient Markov network structure discovery using indepen-

dence tests. Journal of Artiﬁcial Intelligence Research  35:449–484  2009.

[10] J. Davis and P. Domingos. Bottom-up learning of Markov network structure.

T. Joachims  editors  ICML  pages 271–278. Omnipress  2010.

In J. F¨urnkranz and

[11] J. Van Haaren and J. Davis. Markov network structure learning: A randomized feature generation approach.

In J. Hoffmann and B. Selman  editors  AAAI  pages 1148–1154. AAAI Press  2012.

[12] J. Corander  T. Janhunen  J. Rintanen  H. J. Nyman  and J. Pensar. Learning chordal Markov networks by
constraint satisfaction. In C. J. C. Burges  L. Bottou  Z. Ghahramani  and K. Q. Weinberger  editors  NIPS 
pages 1349–1357  2013.

[13] J. Korhonen and P. Parviainen. Exact learning of bounded tree-width Bayesian networks. In C. M. Carvalho
and P. Ravikumar  editors  AISTATS  volume 31 of JMLR Proceedings  pages 370–378. JMLR.org  2013.
[14] J. Berg  M. J¨arvisalo  and B. Malone. Learning optimal bounded treewidth Bayesian networks via maximum

satisﬁability. In S. Kaski and J. Corander  editors  AISTATS  pages 86–95. JMLR.org  2014.

[15] P. Parviainen  H. S. Farahani  and J. Lagergren. Learning bounded tree-width Bayesian networks using
integer linear programming. In S. Kaski and J. Corander  editors  AISTATS  pages 751–759. JMLR.org 
2014.

[16] S. Ott  S. Imoto  and S. Miyano. Finding optimal models for small gene networks. In R. B. Altman  A. K.

Dunker  L. Hunter  and T. E. Klein  editors  PSB  pages 557–567. World Scientiﬁc  2004.

[17] M. Koivisto and K. Sood. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine

Learning Research  pages 549–573  2004.

[18] T. Silander and P. Myllym¨aki. A simple approach for ﬁnding the globally optimal Bayesian network

structure. In R. Dechter and T. S. Richardson  editors  UAI  pages 445–452. AUAI Press  2006.

[19] C. Yuan and B. Malone. Learning optimal Bayesian networks: A shortest path perspective. Journal of

Artiﬁcial Intelligence Research  48:23–65  2013.

[20] M. Bartlett and J. Cussens. Advances in Bayesian network learning using integer programming. In UAI 

pages 182–191. AUAI Press  2013.

[21] S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical structures and
their application to expert systems. Journal of the Royal Statistical Society. Series B (Methodological) 
50(2):pp. 157–224  1988.

[22] S. L. Lauritzen. Graphical Models. Oxford University Press  1996.
[23] A. P. Dawid and S. L. Lauritzen. Hyper Markov laws in the statistical analysis of decomposable graphical

models. The Annals of Statistics  21(3):1272–1317  09 1993.

[24] D. Heckerman  D. Geiger  and D. M. Chickering. Learning Bayesian networks: The combination of

knowledge and statistical data. Machine Learning  20:197–243  1995.

[25] C. P. de Campos and Q. Ji. Efﬁcient structure learning of Bayesian networks using constraints. Journal of

Machine Learning Research  12:663–689  2011.

[26] K. Bache and M. Lichman. UCI machine learning repository  2013.

9

,Kustaa Kangas
Mikko Koivisto
Teppo Niinimäki