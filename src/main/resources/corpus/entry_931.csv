2019,Certainty Equivalence is Efficient for Linear Quadratic Control,We study the performance of the certainty equivalent controller on Linear Quadratic (LQ) control problems with unknown transition dynamics.  We show that for both the fully and partially observed settings  the sub-optimality gap between the cost incurred by playing the certainty equivalent controller on the true system and the cost incurred by using the optimal LQ controller enjoys a fast statistical rate  scaling as the square of the parameter error.  To the best of our knowledge  our result is the first sub-optimality guarantee in the partially observed Linear Quadratic Gaussian (LQG) setting.  Furthermore  in the fully observed Linear Quadratic Regulator (LQR)  our result improves upon recent work by Dean et al.  who present an algorithm achieving a sub-optimality gap linear in the parameter error.  A key part of our analysis relies on perturbation bounds for discrete Riccati equations.  We provide two new perturbation bounds  one that expands on an existing result from Konstantinov  and another based on a new elementary proof strategy.,Certainty Equivalence is Efﬁcient for Linear

Quadratic Control

Horia Mania

University of California  Berkeley

hmania@berkeley.edu

Stephen Tu

University of California  Berkeley

stephentu@berkeley.edu

Benjamin Recht

University of California  Berkeley

brecht@berkeley.edu

Abstract

We study the performance of the certainty equivalent controller on Linear Quadratic
(LQ) control problems with unknown transition dynamics. We show that for both
the fully and partially observed settings  the sub-optimality gap between the cost
incurred by playing the certainty equivalent controller on the true system and the
cost incurred by using the optimal LQ controller enjoys a fast statistical rate  scaling
as the square of the parameter error. To the best of our knowledge  our result is the
ﬁrst sub-optimality guarantee in the partially observed Linear Quadratic Gaussian
(LQG) setting. Furthermore  in the fully observed Linear Quadratic Regulator
(LQR)  our result improves upon recent work by Dean et al. [11]  who present an
algorithm achieving a sub-optimality gap linear in the parameter error. A key part
of our analysis relies on perturbation bounds for discrete Riccati equations. We
provide two new perturbation bounds  one that expands on an existing result from
Konstantinov et al. [25]  and another based on a new elementary proof strategy.

1

Introduction

One of the most straightforward methods for controlling a dynamical system with unknown transitions
is based on the certainty equivalence principle: a model of the system is ﬁt by observing its time
evolution  and a control policy is then designed by treating the ﬁtted model as the truth [6]. Despite
the simplicity of this method  it is challenging to guarantee its efﬁciency because small modeling
errors may propagate to large  undesirable behaviors on long time horizons. As a result  most work
on controlling systems with unknown dynamics has explicitly incorporated robustness against model
uncertainty [11  12  23  30  41  42].
In this work  we show that for the standard baseline of controlling an unknown linear dynamical
system with a quadratic objective function known as Linear Quadratic (LQ) control  certainty
equivalent control synthesis achieves better cost than prior methods that account for model uncertainty.
Our results hold for both the fully observed Linear Quadratic Regulator (LQR) and the partially
observed Linear Quadratic Gaussian (LQG) setting. For ofﬂine control  where one collects some
data and then designs a ﬁxed control policy to be run on an inﬁnite time horizon  we show that the
gap between the performance of the certainty equivalent controller and the optimal control policy
scales quadratically with the error in the model parameters for both LQR and LQG. To the best of
our knowledge  we provide the ﬁrst sub-optimality guarantee for LQG. Moreover  in the LQR setting
our work improves upon the recent result of Dean et al. [11]  who present an algorithm that achieves
a sub-optimality gap linear in the parameter error. In the case of online LQR control  where one

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

adaptively improves the control policy as new data comes in  our ofﬂine result implies that a simple 

polynomial time algorithm using "-greedy exploration sufﬁces for nearly optimal eO(pT ) regret.

2 Main Results for the Linear Quadratic Regulator

An instance of the linear quadratic regulator (LQR) is deﬁned by four matrices: two matrices
A? 2 Rn⇥n and B? 2 Rn⇥d that deﬁne the linear dynamics and two positive semideﬁnite matrices
Q 2 Rn⇥n and R 2 Rd⇥d that deﬁne the cost function. Given these matrices  the goal of LQR is to
solve the optimization problem

min

u0 u1 ...

T

lim
T!1

TXt=0

E" 1

x>t Qxt + u>t Rut# s.t. xt+1 = A?xt + B?ut + wt 
where xt  ut and wt denote the state  input (or action)  and noise at time t  respectively. The
expectation is over the initial state x0 ⇠N (0  In) and the i.i.d. noise wt ⇠N (0  2
wIn). When
the problem parameters (A?  B?  Q  R) are known the optimal policy is given by linear feedback 
ut = K?xt  where K? = (R + B>? P?B?)1B>? P?A? where P? is the (positive deﬁnite) solution
to the discrete Riccati equation

(1)

P? = A>? P?A?  A>? P?B?(R + B>? P?B?)1B>? P?A? + Q

(2)
and can be computed efﬁciently [4  see e.g.]. Problem (1) considers an average cost over an inﬁnite
horizon. The optimal controller for the ﬁnite horizon variant is also static and linear  but time-varying.
The LQR solution in this case can be computed efﬁciently via dynamic programming.
In this work we are interested in the control of a linear dynamical system with unknown transition

denote the Euclidean norm for vectors as well as the spectral (operator) norm for matrices.) Dean et al.

term C1(A?  B?  Q  R) that depends on the problem parameters. We show that the nominal controller

denote by J(A  B  K) the cost (1) obtained by using the actions ut = Kxt on the system (A  B) 

problem (1) while disregarding the modeling error  and use the resulting controller on the true system
(A?  B?). We interchangeably refer to the resulting policy as the certainty equivalent controller

parameters (A?  B?) based on estimates (bA bB). The cost matrices Q and R are assumed known.
We analyze the certainty equivalence approach: use the estimates (bA bB) to solve the optimization
or  following Dean et al. [11]  the nominal controller. We denote by bP the solution to the Riccati
equation (2) associated with the parameters (bA bB) and let bK be the corresponding controller. We
and we use bJ and J? to denote J(A?  B?  bK) and J(A?  B?  K?)  respectively.
Let "  0 such that kA?  bAk  " and kB? bBk  ". (Here and throughout this work we use k·k to
[11] introduced a robust controller that achieves bJ  J?  C1(A?  B?  Q  R)" for some complexity
ut = bKxt achieves bJ  J?  C2(A?  B?  Q  R)"2. Both results require " to be sufﬁciently small
(as a function of the problem parameters) and it is important to note that " must be much smaller for
the nominal controller to be guaranteed to stabilize the system than for the robust controller proposed
by Dean et al. [11]. However  our result shows that once the estimation error " is small enough 
the nominal controller performs better: the sub-optimality gap scales as O("2) versus O("). Both
the more stringent requirement on " and better performance of nominal control compared to robust
control  when the estimation error is sufﬁciently small  were observed empirically by Dean et al. [11].
Before we can formally state our result we need to introduce a few more concepts and assumptions.
It is common to assume that the cost matrices Q and R are positive deﬁnite. Under an additional
observability assumption  this condition can be relaxed to Q being positive semideﬁnite.
Assumption 1. The cost matrices Q and R are positive deﬁnite. Since scaling both Q and R does
not change the optimal controller K?  we can assume without loss of generality that (R)  1  where
(·) denotes the minimum singular value.
A square matrix M is stable if its spectral radius ⇢(M ) is (strictly) smaller than one. Recall that
the spectral radius is deﬁned as ⇢(M ) = max{|| :  is an eigenvalue of M}. A linear dynamical
system (A  B) in feedback with K is fully described by the closed loop matrix A + BK. More
precisely  in this case xt+1 = (A + BK)xt + wt. For a static linear controller ut = Kxt to achieve
ﬁnite LQR cost it is necessary and sufﬁcient that the closed loop matrix is stable.

2

In order to quantify the growth or decay of powers of a square matrix M  we deﬁne

⌧ (M  ⇢) := supkM kk⇢k : k  0 .

(3)
In other words  ⌧ (M  ⇢) is the smallest value such that kM kk  ⌧ (M  ⇢)⇢k for all k  0. We note
that ⌧ (M  ⇢) might be inﬁnite  depending on the value of ⇢  and it is always greater or equal than
one. If ⇢ is larger than ⇢(M )  we are guaranteed to have a ﬁnite ⌧ (M  ⇢) (this is a consequence of
Gelfand’s formula). In particular  if M is a stable matrix  we can choose ⇢< 1 such that ⌧ (M  ⇢) is
ﬁnite. Also  we note that ⌧ (M  ⇢) is a decreasing function of ⇢; if ⇢  kMk  we have ⌧ (M  ⇢) = 1.
At a high level  the quantity ⌧ (M  ⇢) measures the degree of transient response of the linear system
xt+1 = M xt + wt. In particular  when M is stable  ⌧ (M  ⇢) can be upper bounded by the H1-norm
of the system deﬁned by M  which is the `2 to `2 operator norm of the system and a fundamental
quantity in robust control [see 40  for more details].
Throughout this work we use the quantities ? := 1 + max{kA?k kB?k kP?k kK?k} and L? :=
A? + B?K?. We use ? as a uniform upper bound on the spectral norms of the relevant matrices for
the sake of algebraic simplicity. We are ready to state our meta theorem. The proofs for all the results
can be found in the full version of the paper [28].
Theorem 1. Suppose d  n. Let > 0 such that ⇢(L?)  < 1. Also  let "> 0 such that
kbA  A?k  " and kbB  B?k  " and assume kbP  P?k  f (") for some function f such that
f (")  ". Then  under Assumption 1 the certainty equivalent controller ut = bKxt achieves

⌧ (L?  )2

w d 9
?

(4)

as long as f (") is small enough so that the right hand side is smaller than 2
w.

bJ  J?  200 2

1  2 f (")2 

In Section 4 we present two upper bounds f (") on kbP  P?k: one based on a proof technique
proposed by Konstantinov et al. [25] and one based on our direct approach. Both of these upper
bounds satisfy f (") = O(") for " sufﬁciently small. For simplicity  in this section we only specialize
our meta-theorem (Theorem 1) using the perturbation result from our direct approach.
To state a specialization of Theorem 1 we need a few more concepts. A linear system (A  B) is
called controllable when the controllability matrix⇥B AB A2B . . . An1B⇤ has full row
rank. Controllability is a fundamental concept in control theory; it states that there exists a sequence
of inputs to the system (A  B) that moves it from any starting state to any ﬁnal state in at most n steps.
In this work we quantify how controllable a linear system is. We denote  for any integer `  1  the
matrix C` :=⇥B AB . . . A`1B⇤ and call the system (`  ⌫)-controllable if the n-th singular
value of C` is greater or equal than ⌫  i.e. (C`) =qminC`C>`   ⌫. Intuitively  the larger ⌫ is 

the less control effort is needed to move the system between two different states.
Assumption 2. We assume the unknown system (A?  B?) is (`  ⌫)-controllable  with ⌫> 0.
Assumption 2 was used in a different context by Cohen et al. [9]. For any controllable system and any
`  n there exists ⌫> 0 such that the system is (`  ⌫)-controllable. Therefore  (`  ⌫)-controllability
is really not much stronger of an assumption than controllability. As ` grows minimum singular value
(C`) also grows and therefore a larger ⌫ can be chosen so that the system is still (`  ⌫) controllable.
Note that controllability is not necessary for LQR to have a well-deﬁned solution: the weaker
requirement is that of stabilizability  in which there exists a feedback matrix K so that A? + B?K is

requires controllability  the result of Konstantinov et al. [25] only requires stabilizability. However 

stable. The result of Dean et al. [11] only requires stabilizability. While our upper bound on kbP P?k
our upper bound on kbP  P?k is sharper for some classes of systems (see Section 4). A direct plug
in of our perturbation result  presented in Section 4  into Theorem 1 yields the following guarantee.
Theorem 2. Suppose that d  n. Let ⇢ and  be two real values such that ⇢(A?)  ⇢ and
⇢(L?)  < 1. Also  let "> 0 such that kbA  A?k  " and kbB  B?k  " and deﬁne
 = max{1 "⌧ (A? ⇢ ) + ⇢}. Under Assumptions 1 and 2  the certainty equivalent controller
ut = bKxt satisﬁes the suboptimality gap
bJ  J? O (1) 2

⌫◆2
? ⌧ (A? ⇢ )64(`1) ⌧ (L?  )2
1  2
w. Here  O(1) denotes a universal constant.

min{(Q)2  (R)2}✓1 +

as long as the right hand side is smaller than 2

max{kQk2 kRk2}

w d` 5 15

1

"2   (5)

3

The exact form of Equation 5  such as the polynomial dependence on `  ?  etc  can be improved at
the expense of conciseness of the expression. In our proof we optimized for the latter. The factor

max{kQk2 kRk2}/ min(Q)2  (R)2 is the squared condition number of the cost function  a

natural quantity in the context of the optimization problem (1)  which can be seen as an inﬁnite
dimensional quadratic program with a linear constraint. The term ⌧ (L? )2
quantiﬁes the rate at which
12
the optimal controller drives the state towards zero. Generally speaking  the less stable the optimal
closed loop system is  the larger this term becomes.
An interesting trade-off arises between the factor `54(`1) (which arises from upper bounding
perturbations of powers of A? on a time interval of length `) and the factor ⌫ (the lower bound on
(C`))  which is increasing in `. Hence  the parameter ` should be seen as a free-parameter that can
be tuned to minimize the right hand side of (5). Now  we specialize Theorem 2 to a few cases.

Case: A? is contractive  i.e. kA?k < 1.
so that "  1  kA?k. Then  (5) simpliﬁes to:

In this case  we can choose ⇢ = kA?k and " small enough

Case: B? has rank n .

In this case  we can choose ` = 1. Then  (5) simpliﬁes to:

bJ  J? O (1) d 2
bJ  J? O (1) d 2

w 15

w `5 15
?

⌧ (L?  )2
1  2

? ⌧ (A? ⇢ )6 ⌧ (L?  )2
1  2

"2 .

1

max{kQk2 kRk2}

min{(Q)2  (R)2}✓1 +
⌫◆2
⌫◆2
min{(Q)2  (R)2}✓1 +

max{kQk2 kRk2}

1

"2 .

2.1 Comparison to Theorem 4.1 of Dean et al. [11].

Dean et al. [11] show that when their robust synthesis procedure is run with estimates (bA bB)
satisfying max{kbA A?k kbB  B?k}  "  [5(1 + kK?k) ?]1  the resulting controller satisﬁes:
(6)
Here  the quantity ? := supz2Tk(zIn  L?)1k is the H1-norm of the optimal closed loop system
L?. In order to compare Equation 6 to Equation 5  we upper bound the quantity ? in terms of
⌧ (L?  ) and . In particular  by a inﬁnite series expansion of the inverse (zIn  L?)1 we can show
 ?  ⌧ (L? )

bJ  J?  10(1 + kK?k) ?J?" + O("2) .

wn?. Therefore  Equation 6 gives us that:

1 . Also  we have J? = 2

w tr(P?)  2
w2
?

bJ  J? O (1)n2

⌧ (L?  )
1  

" + O("2) .

We see that the dependence on the parameters ? and ⌧ (L?  ) is signiﬁcantly milder compared
to Equation 5. Furthermore  this upper bound is valid for larger " than the upper bound given in
Theorem 2. Comparing these upper bound suggests that there is a price to pay for obtaining a fast rate 
and that in regimes of moderate uncertainty (moderate size of ")  being robust to model uncertainty is
important. This observation is supported by the empirical results of Dean et al. [11].
A similar trade-off between slow and fast rates arises in the setting of ﬁrst-order convex stochastic

optimization. The convergence rate O(1/pT ) of the stochastic gradient descent method can be
improved to O(1/T ) under a strong convexity assumption. However  the performance of stochastic
gradient descent  which can achieve a O(1/T ) rate  is sensitive to poorly estimated problem parame-
ters [29]. Similarly  in the case of LQR  the nominal controller achieves a fast rate  but it is much
more sensitive to estimation error than the robust controller of Dean et al. [11].

End-to-end guarantees. Theorem 2 can be combined with ﬁnite sample learning guarantees
(e.g. [11  15  33  34]) to obtain an end-to-end guarantee similar to Proposition 1.2 of Dean et al.
[11]. In general  estimating the transition parameters from N samples yields an estimation error

that scales as O(1/pN ). Therefore  Theorem 2 implies that bJ  J? O (1/N ) instead of the
bJ  J? O (1/pN ) rate from Proposition 1.2 of Dean et al. [11]. This is similar to the case of
linear regression  where O(1/pN ) estimation error for the parameters translates to a O(1/N ) fast

rate for prediction error. Furthermore  Simchowitz et al. [34] and Sarkar and Rakhlin [33] showed

4

that faster estimation rates are possible for some linear dynamical systems. Theorem 2 translates such
rates into control suboptimality guarantees in a transparent way.
Our result explains the behavior observed in Figure 4 of Dean et al. [11]. The authors propose
two procedures for synthesizing robust controllers for LQR with unknown transitions: one which

guarantees robustness of the performance gap bJ  J?  and one which only guarantees the stability of

the closed loop system. Dean et al. [11] observed that the latter performs better in the small estimation
error regime  which happens because the robustness constraint of the synthesis procedure becomes
inactive when the estimation error is small enough. Then  the second robust synthesis procedure
effectively outputs the certainty equivalent controller  which we now know to achieve a fast rate.

⌘ tId)  where ⇡ is the policy  updated in epochs  and 2

2.2 Nearly optimal eO(pT ) regret in the adaptive setting
The regret formulation of adaptive LQR was ﬁrst proposed by Abbasi-Yadkori and Szepesvári [1].
The task is to design an adaptive algorithm {ut}t0 to minimize regret  as deﬁned by Regret(T ) :=
PT
t=1 x>t Qxt + u>t Rut  T J?. Abbasi-Yadkori and Szepesvári [1] study the performance of
optimism in the face of uncertainty (OFU) and show that it has eO(pT ) regret  which is nearly
optimal for this problem formulation. However  the OFU algorithm requires repeated solutions to a
non-convex optimization problem for which no known efﬁcient algorithm exists.
To deal with the computational issues of OFU  Dean et al. [12] propose to analyze the behavior of "-
greedy exploration using the suboptimality gap results of Dean et al. [11]. In the context of continuous
control  "-greedy exploration refers to the application of the control law ut = ⇡(xt  xt1  ...  x0)+⌘t
with ⌘t ⇠N (0  2
⌘ t is the variance of the
⌘ t ⇠ t1/3 
exploration noise. Dean et al. [12] set the variance of the exploration noise as 2
and show that their method achieves eO(T 2/3) regret. They use epochs of size 2i and decompose
⌘ T⌘. Since the estimation error of the
the regret roughly as Regret(T ) = O⇣T (bJ  J?) + T 2
model parameters scales as O((⌘ T pT )1)  and since the suboptimality gap bJ  J? of the robust
controller is linear in the estimation error  we have Regret(T ) = O⇣ pT
⌘ T⌘. Then  setting
⌘ t ⇠ t1/3 balances these two terms and yields eO(T 2/3) regret. However  Theorem 2  which states
that the gap bJ  J? for the nominal controller depends quadratically on the estimation rate  implies
that online certainty equivalent control achieves Regret(T ) = O⇣ 1
⌘ T⌘. Here  the optimal
⌘ t ⇠ t1/2  yielding eO(pT ) regret. We note that the
observation that certainty equivalence coupled with "-greedy exploration achieves eO(pT ) regret was
certainty equivalent control yields an adaptive LQR algorithm with regret bounded as eO(pT ).

ﬁrst made by Faradonbeh et al. [16].
Corollary 1. (Informal) "-greedy exploration with exploration schedule 2

variance of the exploration noise scales as 2

⌘ t ⇠ t1/2 combined with

3 Main Results for the Linear Quadratic Gaussian Problem

+ T 2

2

⌘ T

+ T 2

2

⌘ T

Now we consider partially observable systems. In this case the system dynamics have the form:

xt+1 = A?xt + B?ut + wt   wt ⇠N (0  2

yt = C?xt + vt   vt ⇠N (0  2

vI) .

wI)  

In (7)  only the output process yt is observed. The LQG problem is deﬁned as1:

min

u0 u1 ...

lim
T!1

E" 1

T

TXt=0

y>t Qyt + u>t Rut# s.t. (7a)  (7b) .

(7a)
(7b)

(8)

1Note that many texts deﬁne the LQG cost in terms of xT

t Qxt instead of yT

t Qyt. We choose the latter

because we do not want the cost to be tied to a particular (unknown) state representation.

5

Here  the input ut is allowed to depend on the history2 Ht := (u0  ...  ut1  y0  ...  yt1). The opti-
mal solution to (8) is to set ut = K?bxt  with K? the optimal LQR solution to (A?  B?  CT
? QC?  R)
andbxt := E[xt|Ht]. The MSE estimatebxt can be solved efﬁciently via Kalman ﬁltering:

(9a)
(9b)
(9c)
There is an inherent ambiguity in the dynamics (7a)-(7b) which makes LQG more delicate than
LQR. In particular  for any invertible T   the LQG problem (8) with parameters (A?  B?  C?  Q  R) is
equivalent to the LQG problem with parameters (T A?T 1  T B?  C?T 1  Q  R) and appropriately

bxt+1 = A?bxt + B?ut + L?(yt  C?bxt)  
L? = A?⌃?CT
⌃? = A?⌃?AT

wI  A?⌃?CT

vI)1C?⌃?AT
? .

? (C?⌃?CT

? (C?⌃?CT

? + V )1  

? + 2

? + 2

such that there exists an unitary T such that:

rescaled noise processes. To deal with this ambiguity  we assume that we have estimates (bA bB bC bL)

(10)
Recent work [32  35  38] has shown how to obtain this style of estimates with guarantees from
input/output data. As in Section 2  we assume that the cost matrices (Q  R) are known. Then  we
study the performance of the certainty equivalence controller deﬁned by:

max{kbA  T A?T 1k kbB  T B?k kbC  C?T 1k kbL  T L?k}  ".
bxt+1 = bAbxt + bBut +bL(yt  bCbxt)   ut = bKbxt   bK = LQR(bA bB bCTQbC  R) .

(11)
Similarly to Theorem 1 for LQR  we state a meta theorem for LQG. Unlike Theorem 1  however 
we need a stronger type of Riccati perturbation guarantee which also allows for perturbation of the

Q matrix. Speciﬁcally  we suppose there exists 0 such that for any   0 and (bA bB bQ) with
max{kbA  Ak kbB  Bk kbQ  Qk}   the solutions P and bP of the Riccati equations with
parameters (A  B  Q  R) and (bA bB bQ  R) satisfy
(12)
for an increasing function f with f ()  . The constant 0 and function f are allowed
to depend on the parameters (A  B  Q  R).
In Section 4  we present a perturbation bound
(Proposition 1) that satisﬁes these properties. Similarly to Section 2  we deﬁne ? := 1 +
max{kA?k kB?k kC?k kK?k kL?k kP?k}. The following theorem is our main result for LQG.
Theorem 3. Suppose that (A?  B?) is stabilizable  (C?  A?) is observable  and that Assumption 1
holds. Let " be an upper bound on kbA  T A?T 1k  kbB  T B?k  kbC  C?T 1k  and kbL 
T L?k for some unitary transformation T . Suppose that assumption (12) holds with parameters
? QC?T 1  R) and that " is sufﬁciently small so that 3kC?k+kQk+"  0
(T A?T 1  T B?  T TCT
and "  1  where " := 73
(R) f (3kC?k2

kP  bPk  f ()  

+kQk+"). Let bK be deﬁned as in (11)  and deﬁne N? as

N? :=A? + B?K?
where the pair (K?  L?) is optimal for the LQG problem deﬁned by (A?  B?  C?  Q  R). Let > 0
be such that ⇢(N?) << 1. Then as long as " 
20?⌧ (N? )  the interconnection of (11) with (7)
using (bA bB bC  bK bL) is stable. Furthermore  the cost J(bA bB bC  bK bL) satisﬁes:

A?  L?C?  

? QC?) + tr(R))

⌧ 6(N?  )

w  2

B?K?

?"2 .

(13)

1

0

?

The proof of Theorem 3 appears in Appendix F. We note that such a  exists since ⇢(N?) < 1;
by the stability and observability assumptions in Theorem 3  we have that both A? + B?K? and
A?  L?C? are stable (c.f. Appendix E of Kailath et al. [24]). Theorem 3 is a meta-theorem showing
how perturbation bounds on the solution of Riccati equations translates into suboptimality bounds on
the performance of certainty equivalent control. Combining Theorem 3 with Proposition 1  we have
the following explicit result  an analogue of Theorem 2 for LQG. To simplify notation we denote by
dare(A  B  Q  R) the solution to the discrete algebraic Riccati equation deﬁned by the parameters A 
B  Q  and R.

2The one step delay in yt is a standard assumption in controls which slightly simpliﬁes the Kalman ﬁltering

expressions. Our results generalize to the setting where the history also contains the current observation yt.

6

J(bA bB bC  bK bL)  J? O (1) max{2

v}(tr(CT

(1  2)3 6

Theorem 4. Suppose that (A?  B?) is stabilizable  (C?  A?) is observable  and that Assumption 1

holds. Let " be an upper bound on kbA T A?T 1k  kbB  T B?k  kbC  C?T 1k  and kbL T L?k for
some unitary transformation T . Let P? = dare(A?  B?  CT
? QC?  R) and suppose that (P?)  1.
Let N? be as in (13) and ﬁx  such that ⇢(N?) << 1. As long as " satisﬁes "  (12)2
 
1
11
? kQk
we have the following sub-optimality bound:
J(bA bB bC  bK bL)  J? O (1) max{2
(1  2)5 "2 .
Several remarks are in order. First  the assumption that (P?)  1 is without loss of generality  since
we can always rescale Q and R without affecting the control solution. Next  we compare our results
here to a classic result from Doyle [13]  which states that there are no gain margins for LQG. We
remark that the notion of a gain margin is a robustness property that holds uniformly over a class
of perturbations of varying degree. Our results do not hold uniformly; we use quantities such as
⌧ (N?  ) and ? to quantify how much mismatch a given LQG instance can tolerate.

? QC?) + tr(R)) kQk2

(R)2 26

v}(tr(CT

⌧ 10(N?  )

w  2

⌧ 4(N? )

?

4 Riccati Perturbation Theory

As discussed in Sections 2 and 3  a key piece of our analysis is bounding the solutions to discrete
Riccati equations as we perturb the problem parameters. Speciﬁcally  we are interested in quantities

that it is not possible to ﬁnd universal values b  L. Consider the systems (A?  B?) = (1 " ) and

b  L such that kbP  P?k  L" if "< b   where " represents a bound on the perturbation. We note
(bA bB) = (1  0); the latter system is not stabilizable and hence bP does not even exist. Therefore  b

and L must depend on the system parameters.
While there is a long line of work analyzing perturbations of Riccati equations  we are not aware
of any result that offers explicit and easily interpretable b and L for a ﬁxed (A?  B?  Q  R); see
Konstantinov et al. [26] for an overview of this literature. In this section  we present two new
results for Riccati perturbation which offer interpretable bounds. The ﬁrst one expands upon the
operator-theoretic proof of Konstantinov et al. [25]; its proof can be found in Appendix B.1. In this
result we assume the cost matrix Q can also be perturbed  which is needed for our LQG guarantee. In

order to be consistent we denote the true cost by function by Q? and the estimated one by bQ.
Proposition 1. Let   ⇢(L?) and also let " such that kbA  A?k  kbB  B?k  and kbQ  Q?k are at
most ". Let k·k+ = k·k + 1. We assume that R  0  (A?  B?) is stabilizable  (Q1/2  A?) observable 
and (P?)  1.

⌧ (L?  )2

1  2 kA?k2

+kP?k2

+kB?k+kR1k+ 

as long as

kbP  P?k O (1) "
(1  2)2
⌧ (L?  )4 kA?k2

" O (1)

+ kP?k2

+ kB?k3

+ kR1k2

+ minkL?k2

+ .
+  kP?k1

We note that the assumption (P?)  1 can be made without loss of generality when the other
assumptions are satisﬁed. Since R  0 and (Q1/2  A) observable  the value function matrix P? is
guaranteed to be positive deﬁnite. Then  by rescalling Q and R we can ensure that (P?)  1.
We now present our direct approach  which uses Assumption 2 to give a bound which is sharper for
some systems (A?  B?) then the one provided by Proposition 1. Recall that any controllable system
is always (`  ⌫)-controllable for some ` and ⌫.

Proposition 2. Let ⇢  ⇢(A?) and also let "  0 such that kbA  A?k  " and kbB  B?k  ". Let
 := max{1 "⌧ (A? ⇢ ) + ⇢}. Under Assumptions 1 and 2 we have

5

2 ⌧ (A? ⇢ )32(`1)✓1 +

1

⌫◆ (1 + kB?k)2kP?k

max{kQk kRk}
min{(R)  (Q)}

 

as long as " is small enough so that the right hand side is smaller or equal than one.

kbP  P?k  32 " `

7

The proof of this result is deferred to Appendix B.2. We note that Proposition 2 can also be extended
to handle perturbations in the cost matrix Q. Proposition 2 requires an (`  ⌫)-controllable system
(A?  B?)  whereas Proposition 1 only requires a stabilizable system  which is a milder assumption.
However  Proposition 2 can offer a sharper guarantee. For example  consider the linear system

R are chosen to be the identity matrix I2. This system (A?  B?) is readily checked to be (1  )-
controllable. It is also straightforward to verify that as  tends to zero  Proposition 1 gives a bound of

with two dimensional states (n = 2) given by A? = 1.01 · I2 and B? = 1
0 . Both Q and
kbP  P?k = O("/4)  whereas Proposition 2 gives a sharper bound of kbP  P?k = O("/3).

0

5 Related Work

For the ofﬂine LQR batch setting  Fiechter [18] proved that the sub-optimality gap bJ  J? scales
as O(") for certainty equivalent control. A crucial assumption of his analysis is that the nominal
controller stabilizes the true unknown system. We give bounds on when this assumption is valid.
Recently  Dean et al. [11] proposed a robust controller synthesis procedure which takes model
uncertainty into account and whose suboptimality gap scales as O("). Tu and Recht [39] show
that the gap bJ  J? of certainty equivalent control scales asymptotically as O("2); we provide a
non-asymptotic analogue of this result. Fazel et al. [17] and Malik et al. [27] analyze a model-free
approach to policy optimization for LQR  in which the controller is directly optimized from sampled
rollouts. Malik et al. [27] showed that  after collecting N rollouts  a derivative free method achieves
a discounted cost gap that scales as O(1/pN ) or O(1/N )  depending on the oracle model used.

In the online LQR adaptive setting it is well understood that using the certainty equivalence principle
without adequate exploration can result in a lack of parameter convergence [see e.g. 5]. Abbasi-
Yadkori and Szepesvári [1] showed that optimism in the face of uncertainty (OFU)  when applied to

regret for the case when the state and inputs are both scalars. In a Bayesian setting Ouyang et al. [31]

their work does not provide any explicit dependencies on instance parameters. Finally  Cohen et al.

of the previous analysis. Ibrahimi et al. [22] showed that when the underlying system is sparse 
the dimension dependent constants in the regret bound can be improved. The main issue with
OFU for LQR is that there are no known computationally tractable ways of implementing it. In
order to deal with this  both Dean et al. [12] and Abbasi-Yadkori et al. [2] propose polynomial time

online LQR  yields eO(pT ) regret. Faradonbeh et al. [14] removed some un-necessary assumptions
algorithms for adaptive LQR based on "-greedy exploration which achieve eO(T 2/3) regret. Only
recently progress has been made on offering eO(pT ) regret guarantees for computationally tractable
algorithms. Abeille and Lazaric [3] show that Thompson sampling achieves eO(pT ) (frequentist)
showed that Thompson sampling achieves eO(pT ) expected regret. Faradonbeh et al. [16] argue that
certainty equivalence control with an epsilon-greedy-like scheme achieves eO(pT ) regret  though
[10] also give an efﬁcient algorithm based on semideﬁnite programming that achieves eO(pT ) regret.
Their main result requires the initial parameter error to scale as O(1/T 1/4). While they propose a
O(pT ) length warmup period to get around this  our analysis of "-greedy control does not require
oT (1) accuracy of the initial parameters. Moreover  there are specialized algorithms for solving
Riccati equations that are more efﬁcient than general semideﬁnite programming solvers.
The literature for LQG is less complete  with most of the focus on the estimation side. Hardt et al.
[19] show that gradient descent can be used to learn a model with good predictive performance  under
strong technical assumptions on the A matrix. A line of work [20  21] has focused on using spectral
ﬁltering techniques to learn a predictive model with low regret. Beyond predictive performance 
several works [32  35  38] show how to learn the system dynamics up to a similarity transform
from input/output data. Finally  we remark that Boczar et al. [8] give sub-optimality guarantees for
output-feedback of a single-input-single-output (SISO) linear system with no process noise.
A key part of our analysis involves bounding the perturbation of solutions to the discrete algebraic
Riccati equation. While there is a rich line of work studying perturbations of Riccati equations
[25  26  36  37]  the results in the literature are either asymptotic in nature or difﬁcult to use and
interpret. We clarify the operator-theoretic result of Konstantinov et al. [25] and provide an explicit
upper bound on the perturbation based on their proof strategy. Also  we take a new direct approach

8

and use an extended notion of controllability to give a constructive and simpler result. While the
result of Konstantinov et al. [25] applies more generally to systems that are stabilizable  we give
examples of linear systems for which our new perturbation result is tighter.
Finally  while we focus on a continuous control problem  we note that the performance of certainty
equivalence had been studied in the context of tabular MDPs  e.g. Azar et al. [7] derived matching
upper and lower bounds on the performance of value-iteration and policy-iteration that use estimated
transition probabilities.

6 Conclusion

Though a naïve Taylor expansion suggests that the fast rates we derive here must be achievable 
precisely computing such rates has been open since the 80s. All of the pieces we used here have existed
in the literature for some time  and perhaps it has just required a bit of time to align contemporary
rate-analyses in learning theory with earlier operator theoretic work in optimal control. There remain
many possible extensions to this work. The robust control approach of Dean et al. [11] applies to
many different objective functions besides quadratic costs  such as H1 and L1 control. It would be
interesting to know whether fast rates for control are possible for other objective functions. Finally 
determining the optimal minimax rate for both LQR and LQG would allow us to understand the
tradeoffs between nominal and robust control at a more ﬁne grained level.

Acknowledgements

We thank the anonymous reviewers for their valuable feedback. We also thank Elad Hazan and Martin
Wainwright  who both independently asked whether or not it was possible to show a fast rate for
LQR. As part of the RISE lab  HM is generally supported in part by NSF CISE Expeditions Award
CCF-1730628  DHS Award HSHQDC-16-3-00083  and gifts from Alibaba  Amazon Web Services 
Ant Financial  CapitalOne  Ericsson  GE  Google  Huawei  Intel  IBM  Microsoft  Scotiabank 
Splunk and VMware. ST is supported by a Google PhD fellowship. BR is generously supported in
part by ONR awards N00014-17-1-2191  N00014-17-1-2401  and N00014-18-1-2833  the DARPA
Assured Autonomy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs  a Siemens
Futuremakers Fellowship  and an Amazon AWS AI Research Award.

References
[1] Y. Abbasi-Yadkori and C. Szepesvári. Regret Bounds for the Adaptive Control of Linear

Quadratic Systems. In Conference on Learning Theory  2011.

[2] Y. Abbasi-Yadkori  N. Lazi´c  and C. Szepesvári. Model-Free Linear Quadratic Control via

Reduction to Expert Prediction. In AISTATS  2019.

[3] M. Abeille and A. Lazaric.

Improved Regret Bounds for Thompson Sampling in Linear

Quadratic Control Problems. In International Conference on Machine Learning  2018.

[4] B. D. O. Anderson and J. B. Moore. Optimal Control: Linear Quadratic Methods. 2007.

[5] K. J. Åström and B. Wittenmark. On Self Tuning Regulators. Automatica  9:185–199  1973.

[6] K. J. Åström and B. Wittenmark. Adaptive Control. 2013.

[7] M. G. Azar  R. Munos  and H. J. Kappen. Minimax pac bounds on the sample complexity of

reinforcement learning with a generative model. Machine learning  91(3):325–349  2013.

[8] R. Boczar  N. Matni  and B. Recht. Finite-Data Performance Guarantees for the Output-
Feedback Control of an Unknown System. In 57th IEEE Conference on Decision and Control 
2018.

[9] A. Cohen  A. Hassidim  T. Koren  N. Lazic  Y. Mansour  and K. Talwar. Online Linear Quadratic

Control. In International Conference on Machine Learning  2018.

9

[10] A. Cohen  T. Koren  and Y. Mansour. Learning Linear-Quadratic Regulators Efﬁciently with

only pT Regret. arXiv:1902.06223  2019.

[11] S. Dean  H. Mania  N. Matni  B. Recht  and S. Tu. On the Sample Complexity of the Linear

Quadratic Regulator. arXiv:1710.01688  2017.

[12] S. Dean  H. Mania  N. Matni  B. Recht  and S. Tu. Regret Bounds for Robust Adaptive Control

of the Linear Quadratic Regulator. In Neural Information Processing Systems  2018.

[13] J. C. Doyle. Guaranteed Margins for LQG Regulators. IEEE Transactions on Automatic Control 

23(4):756–757  1978.

[14] M. K. S. Faradonbeh  A. Tewari  and G. Michailidis. Optimism-Based Adaptive Regulation of

Linear-Quadratic Systems. arXiv:1711.07230  2017.

[15] M. K. S. Faradonbeh  A. Tewari  and G. Michailidis. Finite Time Identiﬁcation in Unstable

Linear Systems. Automatica  96:342–353  2018.

[16] M. K. S. Faradonbeh  A. Tewari  and G. Michailidis. Input Perturbations for Adaptive Regulation

and Learning. arXiv:1811.04258  2018.

[17] M. Fazel  R. Ge  S. M. Kakade  and M. Mesbahi. Global Convergence of Policy Gradient
Methods for the Linear Quadratic Regulator. In International Conference on Machine Learning 
2018.

[18] C.-N. Fiechter. PAC Adaptive Control of Linear Systems. In Conference on Learning Theory 

1997.

[19] M. Hardt  T. Ma  and B. Recht. Gradient Descent Learns Linear Dynamical Systems. Journal

of Machine Learning Research  19(29):1–44  2018.

[20] E. Hazan  K. Singh  and C. Zhang. Learning Linear Dynamical Systems via Spectral Filtering.

In Neural Information Processing Systems  2017.

[21] E. Hazan  H. Lee  K. Singh  C. Zhang  and Y. Zhang. Spectral Filtering for General Linear

Dynamical Systems. In Neural Information Processing Systems  2018.

[22] M. Ibrahimi  A. Javanmard  and B. V. Roy. Efﬁcient Reinforcement Learning for High Dimen-

sional Linear Quadratic Systems. In Neural Information Processing Systems  2012.

[23] G. N. Iyengar. Robust Dynamic Programming. Mathematics of Operations Research  30(2):

257–280  2005.

[24] T. Kailath  A. H. Sayed  and B. Hassibi. Linear Estimation. 2000.

[25] M. M. Konstantinov  P. H. Petkov  and N. D. Christov. Perturbation analysis of the discrete

riccati equation. Kybernetika  29(1):18–29  1993.

[26] M. M. Konstantinov  D.-W. Gu  V. Mehrmann  and P. H. Petkov. Perturbation theory for matrix

equations  volume 9. Gulf Professional Publishing  2003.

[27] D. Malik  A. Pananjady  K. Bhatia  K. Khamaru  P. L. Bartlett  and M. J. Wainwright. Derivative-
Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems. In AISTATS 
2019.

[28] H. Mania  S. Tu  and B. Recht. Certainty Equivalence is Efﬁcient for Linear Quadratic Control.

arXiv:1902.07826  2019.

[29] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust Stochastic Approximation Approach

to Stochastic Programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[30] A. Nilim and L. El Ghaoui. Robust Control of Markov Decision Processes with Uncertain

Transition Matrices. Operations Research  53(5):780–798  2005.

10

[31] Y. Ouyang  M. Gagrani  and R. Jain. Control of Unknown Linear Systems with Thompson

Sampling. In Allerton  2017.

[32] S. Oymak and N. Ozay. Non-asymptotic Identiﬁcation of LTI Systems from a Single Trajectory.

arXiv:1806.05722  2018.

[33] T. Sarkar and A. Rakhlin. Near optimal ﬁnite time identiﬁcation of arbitrary linear dynamical

systems. In International Conference on Machine Learning  2019.

[34] M. Simchowitz  H. Mania  S. Tu  M. I. Jordan  and B. Recht. Learning Without Mixing:
Towards A Sharp Analysis of Linear System Identiﬁcation. In Conference on Learning Theory 
2018.

[35] M. Simchowitz  R. Boczar  and B. Recht. Learning Linear Dynamical Systems with Semi-

Parametric Least Squares. In Conference on Learning Theory  2019.

[36] J.-g. Sun. Perturbation theory for algebraic riccati equations. SIAM Journal on Matrix Analysis

and Applications  19(1):39–65  1998.

[37] J.-g. Sun. Sensitivity analysis of the discrete-time algebraic riccati equation. Linear algebra

and its applications  275:595–615  1998.

[38] A. Tsiamis and G. J. Pappas. Finite Sample Analysis of Stochastic System Identiﬁcation.

arXiv:1903.09122  2019.

[39] S. Tu and B. Recht. The Gap Between Model-Based and Model-Free Methods on the Linear

Quadratic Regulator: An Asymptotic Viewpoint. In Conference on Learning Theory  2019.

[40] S. Tu  R. Boczar  A. Packard  and B. Recht. Non-Asymptotic Analysis of Robust Control from

Coarse-Grained Identiﬁcation. arXiv:1707.04791  2017.

[41] H. Xu and S. Mannor. Distributionally Robust Markov Decision Processes. Mathematics of

Operations Research  37(2):288–300  2012.

[42] K. Zhou  J. C. Doyle  and K. Glover. Robust and Optimal Control. 1995.

11

,Zhilin Yang
Jake Zhao
Bhuwan Dhingra
Kaiming He
William Cohen
Russ Salakhutdinov
Yann LeCun
Horia Mania
Stephen Tu
Benjamin Recht