2018,Reward learning from human preferences and demonstrations in Atari,To solve complex real-world problems with reinforcement learning  we cannot rely on manually specified reward functions. Instead  we need humans to communicate an objective to the agent directly. In this work  we combine two approaches to this problem: learning from expert demonstrations and learning from trajectory preferences. We use both to train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games. Additionally  we investigate the fit of the reward model  present some reward hacking problems  and study the effects of noise in the human labels.,Reward learning from human preferences and

demonstrations in Atari

Borja Ibarz
DeepMind

bibarz@google.com

Jan Leike
DeepMind

leike@google.com

Tobias Pohlen

DeepMind

pohlen@google.com

Geoffrey Irving

OpenAI

irving@openai.com

Dario Amodei

OpenAI

damodei@openai.com

Shane Legg
DeepMind

legg@google.com

Abstract

To solve complex real-world problems with reinforcement learning  we cannot rely
on manually speciﬁed reward functions. Instead  we can have humans communicate
an objective to the agent directly. In this work  we combine two approaches to
learning from human feedback: expert demonstrations and trajectory preferences.
We train a deep neural network to model the reward function and use its predicted
reward to train an DQN-based deep reinforcement learning agent on 9 Atari games.
Our approach beats the imitation learning baseline in 7 games and achieves strictly
superhuman performance on 2 games without using game rewards. Additionally 
we investigate the goodness of ﬁt of the reward model  present some reward hacking
problems  and study the effects of noise in the human labels.

1

Introduction

Reinforcement learning (RL) has recently been very successful in solving hard problems in domains
with well-speciﬁed reward functions (Mnih et al.  2015  2016; Silver et al.  2016). However  many
tasks of interest involve goals that are poorly deﬁned or hard to specify as a hard-coded reward. In
those cases we can rely on demonstrations from human experts (inverse reinforcement learning  Ng
and Russell  2000; Ziebart et al.  2008)  policy feedback (Knox and Stone  2009; Warnell et al.  2017) 
or trajectory preferences (Wilson et al.  2012; Christiano et al.  2017).
When learning from demonstrations  a policy model is trained to imitate a human demonstrator
on the task (Ho and Ermon  2016; Hester et al.  2018). If the policy model mimics the human
expert’s behavior well  it can achieve the performance of the human on the task. However  to provide
meaningful demonstrations  the human demonstrator has to have some familiarity with the task and
understand how to perform it. In this sense  imitation learning puts more burden on the human than
just providing feedback on behavior  which only requires the ability to judge outcomes. Moreover 
using this imitation learning approach it is impossible to signiﬁcantly exceed human performance.
To improve on imitation learning we can learn a reward function directly from human feedback  and
optimize it using reinforcement learning. In this work  we focus on reward learning from trajectory
preferences in the same way as Christiano et al. (2017). However  learning a reward function from
trajectory preferences expressed by a human suffers from two problems:

1. It is hard to obtain a good state space coverage with just random exploration guided by
preferences. If the state space distribution is bad  then the diversity of the trajectory that we
request preferences for is low and thus the human in the loop can’t convey much meaningful
information to the agent.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

2. Preferences are an inefﬁcient way of soliciting information from humans  providing only a

few hundred bits per hour per human.

Our approach addresses the problems in imitation learning and learning from trajectory preferences
by combining the two forms of feedback. First  we initialize the agent’s policy with imitation learning
from the expert demonstrations using the pretraining part of the DQfD algorithm (Hester et al.  2018).
Second  using trajectory preferences and expert demonstrations  we train a reward model that lets us
improve on the policy learned from imitation.
We evaluate our method on the Arcade Learning Environment (Bellemare et al.  2013) because
Atari games are RL problems difﬁcult enough to beneﬁt from nonlinear function approximation and
currently among the most diverse environments for RL. Moreover  Atari games provide well-speciﬁed
‘true’ reward functions  which allows us to objectively evaluate the performance of our method and to
do more rapid experimentation with ‘synthetic’ (simulated) human preferences based on the game
reward.
We show that demonstrations mitigate problem 1 by allowing a human that is familiar with the task to
guide exploration consistently. This allows us to learn to play exploration-heavy Atari games such as
Hero  Private Eye  and Montezuma’s Revenge. Moreover  in our experiments  using demonstrations
typically halves the amount of human time required to achieve the same level of performance;
demonstrations alleviate problem 2 by allowing the human to communicate more efﬁciently.

1.1 Related work
Learning from human feedback. There is a large body of work on reinforcement learning from
human ratings or rankings (Wirth et al.  2017): Knox and Stone (2009)  Pilarski et al. (2011)  Akrour
et al. (2012)  Wilson et al. (2012)  Wirth and Fürnkranz (2013)  Daniel et al. (2015)  El Asri et al.
(2016)  Wirth et al. (2016)  Mathewson and Pilarski (2017)  and others. Focusing speciﬁcally on deep
RL  Warnell et al. (2017) extend the TAMER framework to high-dimensional state spaces  using
feedback to train the policy directly (instead of the reward function). Lin et al. (2017) apply deep RL
from human feedback to 3D environments and improve the handling of low-quality or intermittent
feedback. Saunders et al. (2018) use human feedback as a blocker for unsafe actions rather than to
directly learn a policy. The direct predecessor of our work is Christiano et al. (2017)  with similar
tasks  rewards  policy architectures  and preference learning scheme.

Combining imitation learning and deep RL. Various work focuses on combining human demon-
strations with deep RL. Hester et al. (2018)  on whose method this work is based  use demonstrations
to pretrain a Q-function  followed by deep Q-learning with the demonstrations as an auxiliary margin
loss. Veˇcerík et al. (2017) apply the same technique to DDPG in robotics  and Zhang and Ma (2018)
pretrain actor-critic architectures with demonstrations. Nair et al. (2018) combine these methods with
hindsight experience replay (Andrychowicz et al.  2017). Zhu et al. (2018) combine imitation learning
and RL by summing an RL loss and a generative adversarial loss from imitating the demonstrator (Ho
and Ermon  2016). Finally  the ﬁrst published version of AlphaGo (Silver et al.  2016) pretrains from
human demonstrations. Our work differs from all these efforts in that it replaces the hand-coded RL
reward function with a learned reward function; this allows us to employ the imitation learning/RL
combination even in cases where we cannot specify a reward function.

Inverse reinforcement learning (IRL).
IRL (Ng and Russell  2000; Abbeel and Ng  2004; Ziebart
et al.  2008) use demonstrations to infer a reward function. Some versions of our method make use of
the demonstrations to train the reward function—speciﬁcally  our autolabel experiments label the
demonstrations as preferable to the agent policy. This is closely related to generative adversarial
imitation learning (Ho and Ermon  2016)  a form of IRL. Note  however  that in addition to training
the reward function from demonstrations we also train it from direct human feedback  which allows
us to surpass the performance of the demonstrator in 2 out of 9 games.

Reward-free learning. Reward-free learning attempts to avoid reward functions and instead use
measures of intrinsic motivation  typically based on information theory  as a training signal (Chentanez
et al.  2005; Schmidhuber  2006; Orseau et al.  2013). The intrinsic motivation measure may include
mutual information between actions and end states (Gregor et al.  2016)  state prediction error
or surprise (Pathak et al.  2017)  state visit counts (Storck et al.  1995; Bellemare et al.  2016) 

2

distinguishability to a decoder (Eysenbach et al.  2018)  or empowerment (Salge et al.  2014)  which
is also related to mutual information (Mohamed and Rezende  2015). The present work differs from
reward-free learning in that it attempts to learn complex reward functions through interaction with
humans  rather than replacing reward with a ﬁxed intrinsic objective.

2 Method

2.1 Setting

We consider an agent that is interacting sequentially with an environment over a number of time
steps (Sutton and Barto  2018): in time step t the agent receives an observation ot from the environ-
ment and takes an action at. We consider the episodic setting in which the agent continues to interact
until a terminal time step T is reached and the episode ends. Then a new episode starts. A trajectory
consists of the sequence (o1  a1)  . . . (oT   aT ) of observation-action pairs.
Typically in RL the agent also receives a reward rt 2 R at each time step. Importantly  in this work
we are not assuming that such reward is available directly. Instead  we assume that there is a human
in the loop who has an intention for the agent’s task  and communicates this intention to the agent
using two feedback channels:

1. Demonstrations: several trajectories of human behavior on the task.

2. Preferences: the human compares pairwise short trajectory segments of the agent’s behavior

and prefers those that are closer to the intended goal (Christiano et al.  2017).

In our setting  the demonstrations are available from the beginning of the experiment  while the
preferences are collected during the experiment while the agent is training.
The goal of the agent is to approximate as closely as possible the behavior intended by the human. It
achieves this by 1. imitating the behavior from the demonstrations  and 2. attempting to maximize a
reward function inferred from the preferences and demonstrations. This is explained in detail in the
following sections.

2.2 The training protocol

Our method for training the agent has the following components: an expert who provides demonstra-
tions; an annotator (possibly the same as the expert) who gives preference feedback; a reward model
that estimates a reward function from the annotator’s preferences and  possibly  the demonstrations;
and the policy  trained from the demonstrations and the reward provided by the reward model. The
reward model and the policy are trained jointly according to the following protocol:

Algorithm 1 Training protocol
1: The expert provides a set of demonstrations.
2: Pretrain the policy on the demonstrations using behavioral cloning using loss JE.
3: Run the policy in the environment and store these ‘initial trajectories.’
4: Sample pairs of clips (short trajectory segments) from the initial trajectories.
5: The annotator labels the pairs of clips  which get added to an annotation buffer.
6: (Optionally) automatically generate annotated pairs of clips from the demonstrations and add

them to the annotation buffer.

7: Train the reward model from the annotation buffer.
8: Pretrain of the policy on the demonstrations  with rewards from the reward model.
9: for M iterations do
10:
11:
12:
13:
14: end for

Train the policy in the environment for N steps with reward from the reward model.
Select pairs of clips from the resulting trajectories.
The annotator labels the pairs of clips  which get added to the annotation buffer.
Train the reward model for k batches from the annotation buffer.

3

Note that we pretrain the policy model twice before the main loop begins. The ﬁrst pretraining is
necessary to elicit preferences for the reward model. The policy is pretrained again because some
components of the DQfD loss function require reward labels on the demonstrations (see next section).

2.3 Training the policy

The algorithm we choose for reinforcement learning with expert demonstrations is deep Q-Learning
from demonstrations (DQfD; Hester et al.  2018)  which builds upon DQN (Mnih et al.  2015) and
some of its extensions (Schaul et al.  2015; Wang et al.  2016; Hasselt et al.  2016). The agent learns
an estimate of the action-value function (Sutton and Barto  2018) Q(o  a)  approximated by a deep
neural network with parameters ✓ that outputs a set of action-values Q(o ·; ✓) for a given input
observation o. This action-value function is learned from demonstrations and from agent experience 
both stored in a replay buffer (Mnih et al.  2015) in the form of transitions (ot  at  t+1  ot+1)  where
 is the reward discount factor (ﬁxed value at every step except 0 at end of an episode). Note that the
transition does not include the reward  which is computed from ot by the reward model ˆr.
During the pretraining phase  the replay buffer contains only the transitions from expert demon-
strations. During training  agent experience is added to the replay buffer. The buffer has a ﬁxed
maximum size  and once it is full the oldest transitions are removed in a ﬁrst-in ﬁrst-out manner.
Expert transitions are always kept in the buffer. Transitions are sampled for learning with probability
proportional to a priority  computed from their TD error at the moment they are added to and sampled
from the buffer (Schaul et al.  2015).
The training objective for the agent’s policy is the the cost function J(Q) = JP DDQn(Q) +
2JE(Q) + 3JL2(Q). The term JP DDQn is the prioritized (Schaul et al.  2015) dueling (Wang
et al.  2016) double (Hasselt et al.  2016) Q-loss (PDD)  combining 1- and 3-step returns (Hester et al. 
2018). This term attempts to ensure that the Q values satisfy the Bellman equation (Sutton and Barto 
2018). The term JE is a large-margin supervised loss  applied only to expert demonstrations. This
term tries to ensure that the value of the expert actions is above the value of the non-expert actions by
a given margin. Finally  the term JL2 is an L2-regularization term on the network parameters. The
hyperparameters 2 and 3 are scalar constants. The agent’s behavior is ✏-greedy with respect to the
action-value function Q(o ·; ✓).
2.4 Training the reward model

Our reward model is a convolutional neural network ˆr taking observation ot as input (we omit actions
in our experiments) and outputting an estimate of the corresponding reward rt+1 2 R. Since we do
not assume to have access to an environment reward  we resort to indirect training of this model via
preferences expressed by the annotator (Christiano et al.  2017). The annotator is given a pair of
clips  which are trajectory segments of 25 agent steps each (approximately 1.7 seconds long). The
annotator then indicates which clip is preferred  that the two clips are equally preferred  or that the
clips cannot be compared. In the latter case  the pair of clips is discarded. Otherwise the judgment is
recorded in an annotation buffer A as a triple (1  2  µ)  where 1  2 are the two episode segments
and µ is the judgment label (one of (0  1)  (1  0) or (0.5  0.5)).
To train the reward model ˆr on preferences  we interpret the reward model as a preference predictor
by assuming that the annotator’s probability of preferring a segment i depends exponentially on the
value of the reward summed over the length of the segment:

ˆr(o)! / exp Xo21

ˆr(o)! + exp Xo22

ˆr(o)!!

ˆP [1  2] = exp Xo21
loss(ˆr) =  X(1 2 µ)2A

We train ˆr to minimize the cross-entropy loss between these predictions and the actual judgment
labels:

µ(1) log ˆP [1  2] + µ(2) log ˆP [2  1]

This follows the Bradley-Terry model (Bradley and Terry  1952) for estimating score functions from
pairwise preferences. It can be interpreted as equating rewards with a preference ranking scale
analogous to the Elo ranking system developed for chess (Elo  1978).

4

Since the training set is relatively small (a few thousand pairs of clips) we incorporate a number
of modiﬁcations to prevent overﬁtting: adaptive regularization  Gaussian noise on the input  L2
regularization on the output (details in Appendix A). Finally  since the reward model is trained only
on comparisons  its scale is arbitrary  and we normalize it every 100 000 agent steps to be zero-mean
and have standard deviation 0.05 over the annotation buffer A. This value for the standard deviation
was chosen empirically; deep RL is very sensitive to the reward scale and this parameter is important
for the stability of training.

2.5 Selecting and annotating the video clips

The clips for annotation are chosen uniformly at random from the initial trajectories (line 3 in
Algorithm 1) and the trajectories generated during each iteration of the training protocol. Ideally we
would select clips based on uncertainty estimates from the reward model; however  the ensemble-
based uncertainty estimates used by Christiano et al. (2017) did not improve on uniform sampling
and slowed down the reward model updates. The annotated pairs are added to the annotation buffer 
which stores all the pairs that have been annotated so far. The number of pairs collected after each
protocol iteration decreases as the experiment progresses  according to a schedule (see details in
Appendix A).
In some experiments we attempt to leverage the expert demonstrations to enrich the set of initial
labels. In particular  each clip selected for annotation from the initial trajectories is paired with a clip
selected uniformly at random from the demonstrations and a labeled pair is automatically generated
in which the demonstration is preferred. Thus the initial batch of k pairs of clips produces 2k extra
annotated pairs without invoking the annotator  where k is the number of labels initially requested
from the annotator.
In the majority of our experiments the annotator is not a human. Instead we use a synthetic oracle
whose preferences over clips reﬂect the true reward of the underlying Atari game. This synthetic
feedback allows us to run a large number of simulations and investigate the quality of the learned
reward in some detail (see Section 3.2).

3 Experimental results

Our goal is to train an agent to play Atari games without access to the game’s reward function. There-
fore typical approaches  such as deep RL (Mnih et al.  2015  2016) and deep RL with demos (Hester
et al.  2018) cannot be applied here. We compare the following experimental setups (details are
provided in Appendix A):

1. Imitation learning (ﬁrst baseline). Learning purely from the demonstrations without rein-
forcement learning (Hester et al.  2018). In this setup  no preference feedback is provided to
the agent.

2. No demos (second baseline). Learning from preferences without expert demonstrations 

using the setup from Christiano et al. (2017) with PDD DQN instead of A3C.

3. Demos + preferences. Learning from both preferences and expert demonstrations.
4. Demos + preferences + autolabels. Learning from preferences and expert demonstrations 
with additional preferences automatically gathered by preferring demo clips to clips from
the initial trajectories (see Section 2.5).

We’ve selected 9 Atari games  6 of which (Beamrider  Breakout  Enduro  Pong  Q*bert  and Seaquest)
feature in Mnih et al. (2013) and Christiano et al. (2017). Compared to previous work we exclude
Space Invaders because we do not have demonstrations for it. The three additional games (Hero 
Montezuma’s Revenge  and Private Eye) were chosen for their exploration difﬁculty: without the
help of demonstrations  it is very hard to perform well in them (Hester et al.  2018).
In each experimental setup (except for imitation learning) we compare four feedback schedules. The
full schedule consists of 6800 labels (500 initial and 6300 spread along the training protocol). The
other three schedules reduce the total amount of feedback by a factor of 2  4 and 6 respectively (see
details in Appendix A).

5

Figure 1: Performance of our method on 9 Atari games after 50 million agent steps  for different
annotation schedules and training setups: no demos is the reward learning setup used by Christiano
et al. (2017)  trained with DQN; imitation is the baseline from DQfD without RL; demos + preferences
and demos + pr. + autolables use all demos and synthetic labels  with and without automatic labels
from demos; 20% demos + preferences is like demos + preferences but uses only 20% of the available
demos; demos + human preferences is the same setup as demos + preferences  but with a human
instead of the synthetic oracle. The vertical lines depict the standard deviation across three runs of
each experiment.

The majority of the experiments use the synthetic oracle for labeling. We also run experiments with
actual human annotators in the demos + preferences experimental setup  with the full schedule and
with the schedule reduced by a factor of 2. In our experiments the humans were contractors with no
experience in RL who were instructed as in Christiano et al. (2017) to only judge the outcome visible
in the segments. We label these experiments as human.
Figure 1 displays the mean episode returns in each game  setup and schedule  after 50 million agent
steps. We can compare the relative performance across four different experimental setups:
How much do preferences help (demos + preferences vs. imitation)? Our approach outperforms the
imitation learning baseline in all games except Private Eye. In 6 of the 9 games this holds in every
condition  even with the smallest amount of feedback. The bad performance of imitation learning in
most Atari tasks is a known problem (Hester et al.  2018) and in the absence of a reward function
preference feedback offers an excellent complement. Private Eye is a stark exception: imitation is
hard to beat even with access to reward (Hester et al.  2018)  and in our setting preference feedback is
seriously damaging  except when the demonstrations themselves are leveraged for labeling.
How much do demos help (demos + preferences vs. no demos)? Hero  Montezuma’s Revenge  Private
Eye and Q*bert beneﬁt greatly from demonstrations. Speciﬁcally  in Montezuma’s Revenge and
Private Eye there is no progress solely from preference feedback; without demonstrations Hero does
not beneﬁt from increased feedback; and in Q*bert demonstrations allow the agent to achieve better
performance with the shortest label schedule (1100 labels) than with the full no-demos schedule.
With just 20% of the demonstrations (typically a single episode) performance already improves
signiﬁcantly1. In the rest of the games the contribution of demonstrations is not signiﬁcant  except for
Enduro  where it is harmful  and possibly Seaquest. In Enduro this can be explained by the relatively
poor performance of the expert: this is the only game where the trained agents are superhuman in all
conditions. Note that our results for no demos are signiﬁcantly different from those in Christiano

1Experiments with 50% of the demonstrations (not shown) produced scores similar to the full demo

experiments—the beneﬁts of demonstration feedback seem to saturate quickly.

6

Figure 2: Aggregated performance over all games after 50 million agent steps for different schedules
and training setups. Performance is normalized for each game between 0 (return of a random policy)
and 1 (best return across all setups and schedules). The boxplots show the distribution over all 9
games  the bright notch representing the median  boxes reaching the 25 and 75 percentiles  and
whiskers the whole range. Their position along the x axis shows with the total number of annotation
labels used.

et al. (2017) because we use DQN (Mnih et al.  2015) instead of A3C (Mnih et al.  2016) to optimize
the policy (see Appendix F).
How does human feedback differ from the synthetic oracle (demos + preferences vs. human)? Only in
Beamrider is human feedback superior to synthetic feedback (probably because of implicit reward
shaping by the human). In most games performance is similar  but in Breakout  Montezuma’s Revenge
and Pong it is clearly inferior. This is due to attempts at reward shaping that produce misaligned
reward models (see Figure 3 and Appendix D) and  in the case of Montezuma’s Revenge  to the high
sensitivity of this game to errors in labeling (see Appendix E).
How much do automatic preference labels help (demos + preference vs. demos + preferences + auto
labels)? Preference labels generated automatically from demonstrations increase performance in
Private Eye  Hero  and Montezuma’s Revenge  where exploration is difﬁcult. On most games  there
are no signiﬁcant differences  except in Breakout where human demonstrations are low quality (they
do not ‘tunnel behind the wall’) and thus hurt performance.

3.1 Use of human time

Figure 2 summarizes the overall performance of each setup by human time invested. More than
half of the games achieve the best performance with full feedback and the help of demonstrations
for imitation and annotation  and  for each feedback schedule  the majority of games beneﬁt from
demonstrations  and from the use of demonstrations in annotation. With only 3400 labels even the
worst-performing game with demonstrations and automatic labels beats the median performance
without demonstrations and the full 6800 labels. If demonstrations are not available there are games
that never go beyond random-agent scores; demonstrations ensure a minimum of performance in any
game  as long as they are aided by some preference feedback. For further details refer to Appendix B.

3.2 Quality of reward model

In our experiments we are evaluating the agent on the Atari game score  which may or may not
align with the reward from the reward model that the agent is trained on. With synthetic labels the
learned reward should be a good surrogate of the true reward  and bad performance can stem from
two causes: (1) failure of the reward model to ﬁt the data  or (2) failure of the agent to maximize the
learned reward. With human labels there are two additional sources of error: (3) mislabeling and
(4) a misalignment between the true (Atari) reward function and the human’s reward function. In this
section we disentangle these possibilities.
Learning the reward model is a supervised learning task  and in Appendix C we argue that it succeeds
in ﬁtting the data well. Figure 3 compares the learned reward model with the true reward in three

7

Figure 3: True vs. learned reward accumulated in sequences of 25 (left) and 1000 (right) agent steps
in Enduro  Montezuma’s Revenge and Q*bert. Magenta and gray dots represent the model learned
from synthetic and human preferences  respectively. A fully aligned reward model would have all
points on a straight line. For this evaluation  the agent policy and reward model were ﬁxed after
successful full-schedule training (in the case of synthetic preference feedback we chose the most
successful seed; in the case of human preference feedback only one run was available).

games (see Appendix D for the other six games). Both synthetic (demos + pr. + autolabels in
Figure 1) and human preference models are presented for comparison. Perfect alignment between
true reward and modelled reward is achieved if they are equal up to an afﬁne-linear transformation; in
this case all points in the plot would be on a straight line. In most games the synthetically trained
reward model is reasonably well-aligned  so we can rule out cause (1).
In Enduro both human and synthetic preferences produce well-aligned reward models  especially
over long time horizons. Q*bert presents an interesting difference between human and synthetic
preferences: on short timescales  the human feedback does not capture ﬁne-grained reward distinc-
tions (e.g.  whether the agent covered one or two tiles) which are captured by the synthetic feedback.
However  on long timescales this does not matter much and both models align well. A similar pattern
occurs in Hero. Finally  in Montezuma’s Revenge human feedback fails while synthetic feedback
succeeds. This is partially due to a misalignment (because the human penalizes death while the Atari
score does not) and partially due to the sensitivity of the reward function to label noise. For more
details  see Appendix D.
The difference between synthetically and human-trained reward model captures causes (3) and (4).
To disentangle (3) and (4)  we also provide experiments with a mislabeling rate in Appendix E.

Reward hacking. To further evaluate the quality of the reward model  we run experiments with
frozen reward models obtained from successful runs. The result is shown in Figure 4  left. Although
a fully trained model should make learning the task easier  in no case is the ﬁxed-model performance
signiﬁcantly better than the online training performance  which suggests that joint training of agent
and reward is not intrinsically problematic. Moreover  in Hero  Montezuma  and Private Eye the
performance with a fully trained reward model is much worse than online reward model training. In
these cases the drop in performance happens when the agent learns to exploit undesired loopholes in
the reward function (Figure 4  right)  dramatically increasing the predicted reward with behaviors
that diminish the true score.2 These loopholes can be ﬁxed interactively when the model is trained
online with the agent  since exploitative behaviors that do not lead to good scores can be annotated as
soon as they feature signiﬁcantly in the agent’s policy  similar to adversarial training (Goodfellow
et al.  2014). With online training we also observed cases where performance temporarily drops  with
simultaneous increases in model reward  especially when labels are noisy (Appendix E).

2Videos at https://youtube.com/playlist?list=PLehfUY5AEKX-g-QNM7FsxRHgiTOCl-1hv

8

Figure 4: Failure modes when training from a frozen reward model (contrary to our method). Left:
performance at each game after 50 million agent steps. The darker colored bars show the results from
our training protocol (same as Figure 1) with the full label schedule. The reward model from the
best seed in these experiments is then frozen and used to train an agent from scratch  resulting in the
lighter colored bars. Right: average true return (blue) and average reward model return (red) during
training of three games (only one seed shown per game) from a frozen reward model. This showcases
how the agent learns to exploit the reward model: over time the perceived performance (according to
the reward model) increases  while the actual performance (according to the game score) plummets.
4 Discussion

Combining both preferences and demonstrations outperforms using either in isolation. Their combi-
nation is an effective way to provide guidance to an agent in the absence of explicit reward (Figure 1).
Even small amounts of preference feedback (about 1000 comparisons) let us outperform imitation
learning in 7 out of 9 games. Moreover  the addition of demonstrations to learning from preferences
typically results in substantial performance gains  especially in exploration-heavy games. We achieve
superhuman performance on Pong and Enduro  which is impossible even with perfect imitation.
Synthetic preference feedback proved more effective than feedback provided by humans. It could
be expected that human feedback has the advantage in the exploration-heavy games  where the
human can shape the reward to encourage promising exploration strategies. Analysis of the labels
shows that the human annotator prefers clips where the agent seems to be exploring in particular
directions. However  instead of encouraging exploration  this feedback produces ‘reward pits’ that
trap the agent into repetitive and fruitless behaviors. This effect is not novel; MacGlashan et al.
(2017) have previously argued that humans are bad at shaping reward. However  our results show
that demonstrations can provide consistent exploration guidance.
In addition to the experiments presented here  we were unsuccessful at achieving signiﬁcant per-
formance improvements from a variety of other ideas: distributional RL (Bellemare et al.  2017) 
quantile distributional RL (Dabney et al.  2017)  weight sharing between reward model and policy 
supplying the actions as input to the reward model  pretrained convolutional layers or semi-supervised
training of the reward model  phasing out of the large-margin supervised loss along training  and
other strategies of annotation from demos (see Appendix H).
In contrast to Christiano et al. (2017)  whose work we build upon here  we use the value-based
agent DQN/DQfD instead of the policy-gradient-based agent A3C. This shows that learning reward
functions is feasible across two very different RL algorithms with comparable success. Appendix F
compares the scores of the two agents.
Finally  Section 3.2 highlights a caveat of reward learning: sometimes the agent learns to exploit
unexpected sources of reward. This so-called reward hacking problem (Amodei et al.  2016; Everitt 
2018) is not unique to reward learning; hard-coded reward functions are also exploitable in this
way (Lehman et al.  2018). Importantly  we only found persistent reward hacking when the preference
feedback was frozen. This suggests that our method  keeping a human in the training loop who
provides online feedback to the agent  is effective in preventing reward hacking in Atari games.

9

Acknowledgements
We thank Serkan Cabi  Bilal Piot  Olivier Pietquin  Tom Everitt  and Miljan Martic for helpful
feedback and discussions. Moreover  we thank Elizabeth Barnes for proofreading the paper and
Ashwin Kakarla  Ethel Morgan  and Yannis Assael for helping us set up the human experiments. Last
but not least  we are grateful to the feedback annotators for their many hours of meticulous work.

References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In

International Conference on Machine Learning  pages 1–8  2004.

Riad Akrour  Marc Schoenauer  and Michèle Sebag. April: Active preference learning-based
reinforcement learning. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases  pages 116–131  2012.

Dario Amodei  Chris Olah  Jacob Steinhardt  Paul Christiano  John Schulman  and Dan Mané.

Concrete problems in AI safety. arXiv preprint arXiv:1606.06565  2016.

Marcin Andrychowicz  Filip Wolski  Alex Ray  Jonas Schneider  Rachel Fong  Peter Welinder  Bob
McGrew  Josh Tobin  OpenAI Pieter Abbeel  and Wojciech Zaremba. Hindsight experience replay.
In Advances in Neural Information Processing Systems  pages 5048–5058  2017.

Marc Bellemare  Sriram Srinivasan  Georg Ostrovski  Tom Schaul  David Saxton  and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems  pages 1471–1479  2016.

Marc G Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The Arcade Learning
Environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research 
47:253–279  2013.

Marc G Bellemare  Will Dabney  and Rémi Munos. A distributional perspective on reinforcement

learning. In International Conference on Machine Learning  pages 449–458  2017.

Ralph A Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method of

paired comparisons. Biometrika  39(3/4):324–345  1952.

Nuttapong Chentanez  Andrew G Barto  and Satinder P Singh. Intrinsically motivated reinforcement

learning. In Advances in Neural Information Processing Systems  pages 1281–1288  2005.

Paul F Christiano  Jan Leike  Tom Brown  Miljan Martic  Shane Legg  and Dario Amodei. Deep
reinforcement learning from human preferences. In Advances in Neural Information Processing
Systems  pages 4302–4310  2017.

Will Dabney  Mark Rowland  Marc G Bellemare  and Rémi Munos. Distributional reinforcement

learning with quantile regression. arXiv preprint arXiv:1710.10044  2017.

Christian Daniel  Oliver Kroemer  Malte Viering  Jan Metz  and Jan Peters. Active reward learning

with a novel acquisition function. Autonomous Robots  39(3):389–405  2015.

Layla El Asri  Bilal Piot  Matthieu Geist  Romain Laroche  and Olivier Pietquin. Score-based
inverse reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems  pages 457–465  2016.

Arpad Elo. The Rating of Chessplayers  Past and Present. Arco Pub.  1978.
Tom Everitt. Towards Safe Artiﬁcial General Intelligence. PhD thesis  Australian National University 

2018.

Benjamin Eysenbach  Abhishek Gupta  Julian Ibarz  and Sergey Levine. Diversity is all you need:

Learning skills without a reward function. arXiv preprint arXiv:1802.06070  2018.

Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572  2014.

10

Karol Gregor  Danilo Jimenez Rezende  and Daan Wierstra. Variational intrinsic control. arXiv

preprint arXiv:1611.07507  2016.

Hado van Hasselt  Arthur Guez  and David Silver. Deep reinforcement learning with double Q-

learning. In AAAI  pages 2094–2100  2016.

Todd Hester  Matej Veˇcerík  Olivier Pietquin  Marc Lanctot  Tom Schaul  Bilal Piot  Andrew
Sendonaris  Gabriel Dulac-Arnold  Ian Osband  John Agapiou  Joel Z Leibo  and Audrunas
Gruslys. Deep Q-learning from demonstrations. In AAAI  2018.

Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural

Information Processing Systems  pages 4565–4573  2016.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448–456  2015.

Max Jaderberg  Volodymyr Mnih  Wojciech Marian Czarnecki  Tom Schaul  Joel Z Leibo  David
Silver  and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In
International Conference on Learning Representations  2017.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

W Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The

TAMER framework. In International Conference on Knowledge Capture  pages 9–16  2009.

Joel Lehman  Jeff Clune  Dusan Misevic  Christoph Adami  Julie Beaulieu  Peter J Bentley  Samuel
Bernard  Guillaume Belson  David M Bryson  Nick Cheney  Antoine Cully  Stephane Doncieux 
Fred C Dyer  Kai Olav Ellefsen  Robert Feldt  Stephan Fischer  Stephanie Forrest  Antoine Frénoy 
Christian Gagné  Leni Le Goff  Laura M Grabowski  Babak Hodjat  Frank Hutter  Laurent Keller 
Carole Knibbe  Peter Krcah  Richard E Lenski  Hod Lipson  Robert MacCurdy  Carlos Maestre 
Risto Miikkulainen  Sara Mitri  David E Moriarty  Jean-Baptiste Mouret  Anh Nguyen  Charles
Ofria  Marc Parizeau  David Parsons  Robert T Pennock  William F Punch  Thomas S Ray  Marc
Schoenauer  Eric Shulte  Karl Sims  Kenneth O Stanley  François Taddei  Danesh Tarapore  Simon
Thibault  Westley Weimer  Richard Watson  and Jason Yosinski. The surprising creativity of digital
evolution: A collection of anecdotes from the evolutionary computation and artiﬁcial life research
communities. arXiv preprint arXiv:1803.03453  2018.

Zhiyu Lin  Brent Harrison  Aaron Keech  and Mark O Riedl. Explore  exploit or listen: Combining
human feedback and policy model to speed up deep reinforcement learning in 3D worlds. arXiv
preprint arXiv:1709.03969  2017.

James MacGlashan  Mark K Ho  Robert Loftin  Bei Peng  David Roberts  Matthew E Taylor  and
Michael L Littman. Interactive learning from policy-dependent human feedback. In International
Conference on Machine Learning  pages 2285–2294  2017.

Kory Mathewson and Patrick Pilarski. Actor-critic reinforcement learning with simultaneous human

control and feedback. arXiv preprint arXiv:1703.01274  2017.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan
Wierstra  and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602  2013.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G Bellemare 
Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  Stig Petersen  Charles
Beattie  Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan Wierstra  Shane
Legg  and Demis Hassabis. Human-level control through deep reinforcement learning. Nature 
518(7540):529  2015.

Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap  Tim
Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning  pages 1928–1937  2016.

11

Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically
motivated reinforcement learning. In Advances in Neural Information Processing Systems  pages
2125–2133  2015.

Ashvin Nair  Bob McGrew  Marcin Andrychowicz  Wojciech Zaremba  and Pieter Abbeel. Overcom-
ing exploration in reinforcement learning with demonstrations. In International Conference on
Robotics and Automation  pages 6292–6299  2018.

Andrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In International

Conference on Machine Learning  pages 663–670  2000.

Laurent Orseau  Tor Lattimore  and Marcus Hutter. Universal knowledge-seeking agents for stochastic

environments. In Algorithmic Learning Theory  pages 158–172  2013.

Deepak Pathak  Pulkit Agrawal  Alexei A Efros  and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning  pages 2778–2787 
2017.

Patrick M Pilarski  Michael R Dawson  Thomas Degris  Farbod Fahimi  Jason P Carey  and Richard
Sutton. Online human training of a myoelectric prosthesis controller via actor-critic reinforcement
learning. In International Conference on Rehabilitation Robotics  pages 1–7  2011.

Christoph Salge  Cornelius Glackin  and Daniel Polani. Empowerment—an introduction. In Guided

Self-Organization: Inception  pages 67–114. Springer  2014.

William Saunders  Girish Sastry  Andreas Stuhlmueller  and Owain Evans. Trial without error:
Towards safe reinforcement learning via human intervention. In International Conference on
Autonomous Agents and MultiAgent Systems  pages 2067–2069  2018.

Tom Schaul  John Quan  Ioannis Antonoglou  and David Silver. Prioritized experience replay. CoRR 

abs/1511.05952  2015.

Jürgen Schmidhuber. Developmental robotics  optimal artiﬁcial curiosity  creativity  music  and the

ﬁne arts. Connection Science  18(2):173–187  2006.

David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driessche 
Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  Sander Dieleman 
Dominik Grewe  John Nham  Nal Kalchbrenner  Ilya Sutskever  Timothy Lillicrap  Madeleine
Leach  Koray Kavukcuoglu  Thore Graepel  and Demis Hassabis. Mastering the game of Go with
deep neural networks and tree search. Nature  529(7587):484–489  2016.

Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning
Research  15(1):1929–1958  2014.

Jan Storck  Sepp Hochreiter  and Jürgen Schmidhuber. Reinforcement driven information acquisition
in non-deterministic environments. In International Conference on Artiﬁcial Neural Networks 
pages 159–164  1995.

Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT press  2nd edition 

2018.

Antti Tarvainen and Harri Valpola. Weight-averaged consistency targets improve semi-supervised

deep learning results. arXiv preprint arXiv:1703.01780  2017.

Matej Veˇcerík  Todd Hester  Jonathan Scholz  Fumin Wang  Olivier Pietquin  Bilal Piot  Nicolas Heess 
Thomas Rothörl  Thomas Lampe  and Martin Riedmiller. Leveraging demonstrations for deep
reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817 
2017.

Ziyu Wang  Tom Schaul  Matteo Hessel  Hado Hasselt  Marc Lanctot  and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International Conference on Machine
Learning  pages 1995–2003  2016.

12

Garrett Warnell  Nicholas Waytowich  Vernon Lawhern  and Peter Stone. Deep TAMER: Interactive

agent shaping in high-dimensional state spaces. arXiv preprint arXiv:1709.10163  2017.

Aaron Wilson  Alan Fern  and Prasad Tadepalli. A Bayesian approach for policy learning from
trajectory preference queries. In Advances in Neural Information Processing Systems  pages
1133–1141  2012.

Christian Wirth and Johannes Fürnkranz. Preference-based reinforcement learning: A preliminary
In ECML/PKDD Workshop on Reinforcement Learning from Generalized Feedback:

survey.
Beyond Numeric Rewards  2013.

Christian Wirth  J Fürnkranz  and Gerhard Neumann. Model-free preference-based reinforcement

learning. In AAAI  pages 2222–2228  2016.

Christian Wirth  Riad Akrour  Gerhard Neumann  and Johannes Fürnkranz. A survey of preference-
based reinforcement learning methods. The Journal of Machine Learning Research  18(1):4945–
4990  2017.

Xiaoqin Zhang and Huimin Ma. Pretraining deep actor-critic reinforcement learning algorithms with

expert demonstrations. arXiv preprint arXiv:1801.10459  2018.

Yuke Zhu  Ziyu Wang  Josh Merel  Andrei Rusu  Tom Erez  Serkan Cabi  Saran Tunyasuvunakool 
János Kramár  Raia Hadsell  Nando de Freitas  and Nicolas Heess. Reinforcement and imitation
learning for diverse visuomotor skills. arXiv preprint arXiv:1802.09564  2018.

Brian D Ziebart  Andrew L Maas  J Andrew Bagnell  and Anind K Dey. Maximum entropy inverse

reinforcement learning. In AAAI  pages 1433–1438  2008.

13

,Borja Ibarz
Jan Leike
Tobias Pohlen
Geoffrey Irving
Shane Legg
Dario Amodei
Hao(Jackson) Cui
Roni Khardon