2018,Optimization of Smooth Functions with Noisy Observations: Local Minimax Rates,We consider the problem of global optimization of an unknown non-convex smooth function with noisy zeroth-order feedback. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations. We show that for functions with fast growth around their global minima  carefully designed optimization algorithms can identify a near global minimizer with many fewer queries than worst-case global minimax theory predicts. For the special case of strongly convex and smooth functions  our implied convergence rates match the ones developed for zeroth-order convex optimization problems. On the other hand  we show that in the worst case no algorithm can converge faster than the minimax rate of estimating an unknown functions in linf-norm. Finally  we show that non-adaptive algorithms  although optimal in a global minimax sense  do not attain the optimal local minimax rate.,OptimizationofSmoothFunctionswithNoisyObservations:LocalMinimaxRatesYiningWang SivaramanBalakrishnan AartiSinghDepartmentofMachineLearningandStatisticsCarnegieMellonUniversity Pittsburgh PA 15213 USA{yiningwa aarti}@cs.cmu.edu siva@stat.cmu.eduAbstractWeconsidertheproblemofglobaloptimizationofanunknownnon-convexsmoothfunctionwithnoisyzeroth-orderfeedback.Weproposealocalminimaxframeworktostudythefundamentaldifﬁcultyofoptimizingsmoothfunctionswithadaptivefunctionevaluations.Weshowthatforfunctionswithfastgrowtharoundtheirglobalminima carefullydesignedoptimizationalgorithmscanidentifyanearglobalminimizerwithmanyfewerqueriesthanworst-caseglobalminimaxtheorypredicts.Forthespecialcaseofstronglyconvexandsmoothfunctions ourimpliedconvergenceratesmatchtheonesdevelopedforzeroth-orderconvexoptimizationproblems.Ontheotherhand weshowthatintheworstcasenoalgorithmcanconvergefasterthantheminimaxrateofestimatinganunknownfunctionsin‘8-norm.Finally weshowthatnon-adaptivealgorithms althoughoptimalinaglobalminimaxsense donotattaintheoptimallocalminimaxrate.1IntroductionGlobalfunctionoptimizationwithstochastic(zeroth-order)queryoraclesisanimportantprobleminoptimization machinelearningandstatistics.Tooptimizeanunknownboundedfunctionf:XÞÑRdeﬁnedonaknowncompactd-dimensionaldomainXĎRd thedataanalystmakesnactivequeriesx1 ... xnPXandobservesyt“fpxtq`wt wti.i.d.„Np0 1q 1t“1 ... n.(1)Thequeriesx1 ... xtareactiveinthesensethattheselectionofxtcandependonpreviousqueriesandtheirresponsesx1 y1 ... xt´1 yt´1.Afternqueries anestimatepxnPXisproducedthatapproximatelyminimizestheunknownfunctionf.Such“activequery”modelsarerelevantinabroadrangeof(noisy)globaloptimizationapplications forinstanceinhyper-parametertuningofmachinelearningalgorithms[40]andsequentialdesigninmaterialsynthesisexperimentswherethegoalistomaximizestrengthsoftheproducedmaterials[35 41].Sec.2.1givesarigorousformulationoftheactivequerymodelandcontrastsitwiththeclassicalpassivequerymodel.Theerrorofanestimatepxnismeasuredbythedifferenceoffppxnqandtheglobalminimumoff:Lppxn;fq:“fppxnq´f˚wheref˚:“infxPXfpxq.(2)ThroughoutthepaperwetakeXtobethed-dimensionalunitcuber0 1sd whileourresultscanbeeasilygeneralizedtoothercompactdomainssatisfyingminimalregularityconditions.Whenfbelongstoasmoothnessclass saytheHölderclasswithexponentα astraightforwardglobaloptimizationmethodistoﬁrstsamplenpointsuniformlyatrandomfromXandthenconstruct1Theexactdistributionofεtisnotimportant andourresultsholdforsub-Gaussiannoisetoo.32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018) Montréal Canada.nonparametricestimatespfnoffusingnonparametricregressionmethodssuchas(high-order)kernelsmoothingorlocalpolynomialregression[17 46].Classicalanalysisshowsthatthesup-normreconstructionerror}pfn´f}8“supxPX|pfnpxq´fpxq|canbeupperboundedbyrOPpn´α{p2α`dqq2.ThisglobalreconstructionguaranteethenimpliesanrOPpn´α{p2α`dqqupperboundonLppxn;fqbyconsideringpxnPXsuchthatpfnppxnq“infxPXpfnpxq(suchanpxnexistsbecauseXisclosedandbounded).Formally wehavethefollowingproposition(provedintheAppendix)thatconvertsaglobalreconstructionguaranteeintoanupperboundonoptimizationerror:Proposition1.Supposepfnppxnq“infxPXpfnpxq.ThenLppxn;fqď2}pfn´f}8.Typically fundamentallimitsontheoptimaloptimizationerrorareunderstoodthroughthelensofminimaxanalysiswheretheobjectofstudyisthe(global)minimaxrisk:infpxnsupfPFEfLppxn fq (3)whereFisacertainsmoothnessfunctionclasssuchastheHölderclass.Althoughoptimizationappearstobeeasierthanglobalreconstruction weshowthatthen´α{p2α`dqrateisnotimprovableintheglobalminimaxsenseinEq.(3)overHölderclasses.Suchasurprisingphenomenonwasalsonotedinpreviousworks[9 22 44]forrelatedproblems.Ontheotherhand extensiveempiricalevidencesuggeststhatnon-uniform/activeallocationsofquerypointscansigniﬁcantlyreduceopti-mizationerrorinpracticalglobaloptimizationofsmooth non-convexfunctions[40].Thisraisestheinterestingquestionofunderstanding fromatheoreticalperspective underwhatconditions/inwhatscenariosisglobaloptimizationofsmoothfunctionseasierthantheirreconstruction andthepowerofactive/feedback-drivenqueriesthatplayimportantrolesinglobaloptimization.Inthispaper weproposeatheoreticalframeworkthatpartiallyanswerstheabovequestions.Incontrasttoclassicalglobalminimaxanalysisofnonparametricestimationproblems weadoptalocalanalysiswhichcharacterizestheoptimalconvergencerateofoptimizationerrorwhentheunderlyingfunctionfiswithintheneighborhoodofa“reference”functionf0.(SeeSec.2.2forarigorousformulation.)OurmainresultsaretocharacterizethelocalconvergenceratesRnpf0qforawiderangeofreferencefunctionsf0PF.Ourcontributionscanbesummarizedasfollows:1.Wedesignaniterative(active)algorithmwhoseoptimizationerrorLppxn;fqconvergesatarateofRnpf0qdependingonthereferencefunctionf0.Whenthelevelsetsoff0satisfycertainregularityandpolynomialgrowthconditions thelocalrateRnpf0qcanbeupperboundedbyRnpf0q“rOpn´α{p2α`d´αβqq whereβPr0 d{αsisaparameterdependingonf0thatcharacterizesthevolumegrowthoflevelsetsoff0.(Seeassumption(A2) Proposition2andTheorem1fordetails).Theratematchestheglobalminimaxraten´α{p2α`dqforworst-casef0whereβ“0 buthasthepotentialofbeingmuchfasterwhenβą0.Weemphasizethatouralgorithmhasnoknowledgeoff0 αorβandachievesthisrateadaptively.2.Weprovelocalminimaxlowerboundsthatmatchthen´α{p2α`d´αβqupperbound uptologa-rithmicfactorsinn.Morespeciﬁcally weshowthateveniff0isknown no(active)algorithmcanestimatefincloseneighborhoodsoff0ataratefasterthann´α{p2α`d´αβq.Wefurthershowthat ifactivequeriesarenotavailableandx1 ... xnarei.i.d.uniformlysampledfromX then´α{p2α`dqglobalminimaxratealsoapplieslocallyregardlessofhowlargeβis.Thus thereisanexplicitgapbetweenlocalminimaxratesofactiveanduniformquerymodels.3.Inthespecialcasewhenfisconvex theglobaloptimizationproblemisusuallyreferredtoaszeroth-orderconvexoptimizationandthisproblemhasbeenwidelystudied[1 2 6 18 24 36].Ourresultsimplythat whenf0isstronglyconvexandsmooth thelocalminimaxrateRnpf0qisontheorderofrOpn´1{2q whichmatchestheconvergenceratesin[1].Additionally ournegativeresults(Theorem2)indicatethatthen´1{2ratecannotbeachievediff0ismerelyconvex whichseemstocontradictn´1{2resultsin[2 6]thatdonotrequirestrongconvexityoff.However itshouldbenotedthatmereconvexityoff0doesnotimplyconvexityoffinaneighborhoodoff0(e.g. }f´f0}8ďε).Ourresultsshowsigniﬁcantdifferencesintheintrinsicdifﬁcultyofzeroth-orderoptimizationofconvexandnear-convexfunctions.2IntherOp¨qorrOPp¨qnotationwedroppoly-logarithmicdependencyonn21.1RelatedWorkGlobaloptimization knownvariouslyasblack-boxoptimization Bayesianoptimizationandthecontinuous-armedbandit hasalonghistoryintheoptimizationresearchcommunity[25 26]andhasalsoreceivedasigniﬁcantamountofrecentinterestinstatisticsandmachinelearning[8 9 22 31 32 40].Manypreviousworks[8 28]havederivedratesfornon-convexsmoothpayoffsin“continuum-armed”banditproblems;however theydonotconsiderlocalratesspeciﬁctoobjectivefunctionswithcertaingrowthconditionsaroundtheoptima.Amongtheexistingworks [20 34]isprobablytheclosesttoourpaper whichstudiedasimilarproblemofestimatingthesetofalloptimaofasmoothfunctioninHausdorff’sdistance.ForHöldersmoothfunctionswithpolynomialgrowth [34]derivesann´1{p2α`d´αβqminimaxrateforαă1(laterimprovedtoαě1inhisthesis[33]) whichissimilartoourPropositions2and3.[20 34]alsodiscussedadaptivitytounknownsmoothnessparameters.Wehoweverremarkonseveraldifferencesbetweenourworkand[34].First in[20 34]onlyfunctionswithpolynomialgrowthareconsidered whileinourTheorems1and2functionalsεUnpf0qandεLnpf0qareproposedforgeneralreferencefunctionsf0satisfyingmildregularityconditions whichincludefunctionswithpolynomialgrowthasspecialcases.Inaddition [34]considerstheharderproblemofestimatingmaximasetsinHausdorffdistancethanproducingasingleapproximateoptimapxT.Asaresult sincetheconstructionofminimaxlowerboundin[34]isnolongervalidasanalgorithm withoutdistinguishingbetweentwofunctionswithdifferentoptimalsets canneverthelessproduceagoodapproximateoptimizeraslongasthetwofunctionsunderconsiderationhaveoverlappingoptimalsets.Newconstructionsandinformation-theoreticaltechniquesarethereforerequiredtoprovelowerboundsundertheweaker(one-point)approximateoptimizationframework.Finally weproveaminimaxlowerboundswhenonlyuniformquerypointsareavailableanddemonstrateasigniﬁcantgapbetweenalgorithmshavingaccesstouniformoradaptivelychosendatapoints.[31 32]imposeadditionalassumptionsonthelevelsetsoftheunderlyingfunctiontoobtainanimprovedconvergencerate.Thelevelsetassumptionsconsideredinthementionedreferencesareratherrestrictiveandessentiallyrequiretheunderlyingfunctiontobeuni-modal whileourassumptionsaremuchmoreﬂexibleandapplytomulti-modalfunctionsaswell.Inaddition [31 32]consideredanoiselesssettinginwhichexactfunctionevaluationsfpxtqcanbeobtained whileourpaperstudiesthenoisecorruptedmodelinEq.(1)forwhichvastlydifferentconvergenceratesarederived.Finally nomatchinglowerboundswereprovedin[31 32].[43]consideredzeroth-orderoptimizationofapproximatelyconvexfunctionsandderivednecessaryandsufﬁcientconditionsfortheconvergenceratestobepolynomialindomaindimensiond.The(stochastic)globaloptimizationproblemissimilartomodeestimationofeitherdensitiesorregressionfunctions whichhasarichliterature[13 27 39].Animportantdifferencebetweenstatisticalmodeestimationandglobaloptimizationisthewaysample/querypointsx1 ... xnPXaredistributed:inmodeestimationitiscustomarytoassumethesamplesareindependentlyandidenticallydistributed whileinglobaloptimizationsequentialdesignsofsamples/queriesareallowed.Furthermore toestimate/locatethemodeofanunknowndensityorregressionfunction suchamodehastobewell-deﬁned;ontheotherhand producinganestimatepxnwithsmallLppxn fqiseasierandresultsinweakerconditionsimposedontheunderlyingfunction.Methodology-wise ouriterativeprocedurealsoresemblesdisagreement-basedactivelearningmeth-ods[5 14 21].Theintermediatestepsofcandidatepointeliminationcanalsobeviewedassequencesoflevelsetestimationproblems[38 42 45]orclustertreeestimation[4 12]withactivequeries.Anotherlineofresearchhasfocusedonﬁrst-orderoptimizationofquasi-convexornon-convexfunctions[3 10 19 23 37 48] inwhichexactorunbiasedevaluationsoffunctiongradientsareavailableatquerypointsxPX.[48]consideredaCheeger’sconstantrestrictiononlevelsetswhichissimilartoourlevelsetregularityassumptions(A2andA2’).[15 16]studiedlocalminimaxratesofﬁrst-orderoptimizationofconvexfunctions.First-orderoptimizationdifferssigniﬁcantlyfromoursettingbecauseunbiasedgradientestimationisgenerallyimpossibleinthemodelofEq.(1).Furthermore mostworkson(ﬁrst-order)non-convexoptimizationfocusonconvergencetostationarypointsorlocalminima whileweconsiderconvergencetoglobalminima.3Figure1:InformalillustrationsofouralgorithmthatattainsTheorem1(detailsintheappendix).Solidbluecurvesdepicttheunderlyingfunctionftobeoptimized blackandredsoliddotsdenotethequerypointsandtheirresponsestpxt ytqu andblack/redverticallinesegmentscorrespondtouniformconﬁdenceintervalsonfunctionevaluationsconstructedusingcurrentbatchofdataobserved.Theleftﬁgureillustratestheﬁrstepochofouralgorithm wherequerypointsareuniformlysampledfromtheentiredomainX.Afterwards sub-optimallocationsbasedonconstructedconﬁdenceintervalsareremoved andashrinkt“candidateset”S1isobtained.Thealgorithmthenproceedstothesecondepoch illustratedintherightﬁgure wherequerypoints(inred)aresampledonlyfromtherestrictedcandidatesetandshorterconﬁdenceintervals(alsoinred)areconstructedandupdated.TheprocedureisrepeateduntilOplognqepochsarecompleted.2BackgroundandNotationWeﬁrstreviewstandardasymptoticnotationthatwillbeusedthroughoutthispaper.Fortwosequencestanu8n“1andtbnu8n“1 wewritean“OpbnqoranÀbniflimsupnÑ8|an|{|bn|ă8 orequivalentlybn“ΩpanqorbnÁan.Denotean“Θpbnqoran—bnifbothanÀbnandanÁbnhold.Wealsowritean“opbnqorequivalentlybn“ωpanqiflimnÑ8|an|{|bn|“0.FortwosequencesofrandomvariablestAnu8n“1andtBnu8n“1 denoteAn“OPpBnqifforeveryą0 thereexistsCą0suchthatlimsupnÑ8Prr|An|ąC|Bn|sď.Forrą0 1ďpď8andxPRd wedenoteBprpxq:“tzPRd:}z´x}pďruasthed-dimensional‘p-ballofradiusrcenteredatx wherethevector‘pnormisdeﬁnedas}x}p:“přdj“1|xj|pq1{pfor1ďpă8and}x}8:“max1ďjďd|xj|.ForanysubsetSĎRdwedenotebyBprpx;SqthesetBprpxqXS.2.1PassiveandActiveQueryModelsLetUbeaknownrandomquantitydeﬁnedonaprobabilityspaceU.Thefollowingdeﬁnitionscharacterizeallpassiveandactiveoptimizationalgorithms:Deﬁnition1(Thepassivequerymodel).Letx1 ... xnbei.i.d.pointsuniformlysampledonXandy1 ... ynbeobservationsfromthemodelEq.(1).ApassiveoptimizationalgorithmAwithnqueriesisparameterizedbyamappingφn:px1 y1 ... xn yn UqÞÑpxnthatmapsthei.i.d.observationstpxi yiquni“1toanestimatedoptimumpxnPX potentiallyrandomizedbyU.Deﬁnition2(Theactivequerymodel).Anactiveoptimizationalgorithmcanbeparameterizedbymappingspχ1 ... χn φnq wherefort“1 ... n χt:px1 y1 ... xt´1 yt´1 UqÞÑxtproducesaquerypointxtPXbasedonpreviousobservationstpxi tiqut´1i“1 andφn:px1 y1 ... xn yn UqÞÑpxnproducestheﬁnalestimate.Allmappingspχ1 ... χn φnqcanberandomizedbyU.2.2LocalMinimaxRatesWeusetheclassicallocalminimaxanalysis[47]tounderstandthefundamentalinformation-theoreticallimitsofnoisyglobaloptimizationofsmoothfunctions.Ontheupperboundside 4weseek(active)estimatorspxnsuchthatsupf0PΘsupfPΘ1 }f´f0}8ďεnpf0qPrfrLppxn;fqěC1¨Rnpf0qsď1{4 (4)whereC1ą0isapositiveconstant.Heref0PΘisreferredtoasthereferencefunction andfPΘ1isthetrueunderlyingfunctionwhichisassumedtobe“near”f0.TheminimaxconvergencerateofLppxn;fqisthencharacterizedlocallybyRnpf0qwhichdependsonthereferencefunctionf0.Theconstantof1{4ischosenarbitrarilyandanysmallconstantleadstosimilarconclusions.Toestablishnegativeresults(i.e. locallyminimaxlowerbounds) incontrasttotheupperboundformulation weassumethepotentialactiveoptimizationestimatorpxnhasperfectknowledgeaboutthereferencefunctionf0PΘ.WethenprovelocallyminimaxlowerboundsoftheforminfpxnsupfPΘ1 }f´f0}8ďεnpf0qPrfrLppxn;fqěC2¨Rnpf0qsě1{3 (5)whereC2ą0isanotherpositiveconstantandεnpf0q Rnpf0qaredesiredlocalconvergenceratesforfunctionsnearthereferencef0.Althoughinsomesenseclassical thelocalminimaxdeﬁnitionweproposewarrantsfurtherdiscussion.1.RolesofΘandΘ1:Thereferencefunctionf0andthetruefunctionsfareassumedtobelongtodifferentbutcloselyrelatedfunctionclassesΘandΘ1.Inparticular inourpaperΘĎΘ1 meaningthatlessrestrictiveassumptionsareimposedonthetrueunderlyingfunctionfcomparedtothoseimposedonthereferencefunctionf0onwhichRnandεnarebased.2.UpperBounds:Itisworthemphasizingthattheestimatorpxnhasnoknowledgeofthereferencefunctionf0.Fromtheperspectiveofupperbounds wecanconsiderthesimplertaskofproducingf0-dependentbounds(eliminatingthesecondsupremum)toinsteadstudythe(alreadyinteresting)quantity:supf0PΘPrf0rLppxn;f0qěC1Rnpf0qsď1{4.Asindicatedabovewemaintainthedouble-supremuminthedeﬁnitionbecausefewerassumptionsareimposeddirectlyonthetrueunderlyingfunctionf andfurtherbecauseitallowstomoredirectlycompareourupperandlowerbounds.3.LowerBoundsandthechoiceofthe“localizationradius”εnpf0q:Ourlowerboundsallowtheestimatorknowledgeofthereferencefunction(thismakesestablishingthelowerboundmorechallenging).Eq.(5)impliesthatnoestimatorpxncaneffectivelyoptimizeafunctionfclosetof0beyondtheconvergencerateofRnpf0q evenifperfectknowledgeofthereferencefunctionf0isavailableapriori.Theεnpf0qparameterthatdecidesthe“range”inwhichlocalminimaxratesapplyistakentobeonthesameorderastheactuallocalrateRnpf0qinthispaper.Thisis(uptoconstants)thesmallestradiusforwhichwecanhopetoobtainnon-triviallower-bounds:ifweconsideramuchsmallerradiusthanRnpf0qthenthetrivialestimatorwhichoutputstheminimizerofthereferencefunctionwouldachieveafasterratethanRnpf0q.Selectingthesmallestpossibleradiusmakesestablishingthelowerboundmostchallengingbutprovidesareﬁnedpictureofthecomplexityofzeroth-orderoptimization.3MainResultsWiththisbackgroundinplacewenowturnourattentiontoourmainresults.WebeginbycollectingourassumptionsaboutthetrueunderlyingfunctionandthereferencefunctioninSection3.1.WestateanddiscusstheconsequencesofourupperandlowerboundsinSections3.2and3.3respectively.WedefermosttechnicalproofstotheAppendixandturnourattentiontoouroptimizationalgorithminSectionA.3.1AssumptionsWeﬁrststateandmotivateassumptionsthatwillbeused.TheﬁrstassumptionstatesthatfislocallyHöldersmoothonitslevelsets.5(A1)Thereexistconstantsκ α Mą0suchthatfrestrictedonXf κ:“txPX:fpxqďf˚`κubelongstotheHölderclassΣαpMq meaningthatfisk-timesdifferentiableonXf κandfurthermoreforanyx x1PXf κ 3kÿj“0ÿα1`...`αd“j|fpα jqpxq|`ÿα1`...`αd“k|fpα kqpxq´fpα kqpx1q|}x´x1}α´k8ďM.(6)Herek“tαuisthelargestintegerlowerboundingαandfpα jqpxq:“Bjfpxq{Bxα11...Bxαdd.WeuseΣακpMqtodenotetheclassofallfunctionssatisfying(A1).Weremarkthat(A1)isweakerthanthestandardassumptionthatfonitsentiredomainXbelongstotheHölderclassΣαpMq.Thisisbecauseplaceswithfunctionvalueslargerthanf˚`κcanbeeasilydetectedandremovedbyapre-processingstep.Wegivefurtherdetailsofthepre-processingstepinSectionA.3.Ournextassumptionconcernthe“regularity”ofthelevelsetsofthe“reference”functionf0.DeﬁneLf0pq:“txPX:f0pxqďf˚0`uasthe-levelsetoff0 andµf0pq:“λpLf0pqqastheLebesguemeasureofLf0pq alsoknownasthedistributionfunction.DeﬁnealsoNpLf0pq δqasthesmallestnumberof‘2-ballsofradiusδthatcoverLf0pq.(A2)Thereexistconstantsc0ą0andC0ą0suchthatNpLf0pq δqďC0r1`µf0pqδ´dsforall δPp0 c0s.WeuseΘCtodenoteallfunctionsthatsatisfy(A2)withrespecttoparametersC“pc0 C0q.Atahigherlevel theregularitycondition(A2)assumesthatthelevelsetsaresufﬁciently“regular”suchthatcoveringthemwithsmall-radiusballsdoesnotrequiresigniﬁcantlylargertotalvolumes.Forexample consideraperfectlyregularcaseofLf0pqbeingthed-dimensional‘2ballofradiusr:Lf0pq“txPX:}x´x˚}2ďru.Clearly µf0pq—rd.Inaddition theδ-coveringnumberin‘2ofLf0pqisontheorderof1`pr{δqd—1`µf0pqδ´d whichsatisﬁesthescalingin(A2).When(A2)holds uniformconﬁdenceintervalsoffonitslevelsetsareeasytoconstructbecauselittlestatisticalefﬁciencyislostbyslightlyenlargingthelevelsetssothatcompleted-dimensionalcubesarecontainedintheenlargedlevelsets.Ontheotherhand whenregularityoflevelsetsfailstoholdsuchnonparametricestimationcanbeverydifﬁcultorevenimpossible.Asanextremeexample supposethelevelsetLf0pqconsistsofnstandaloneandwell-spacedpointsinX:theLebesguemeasureofLf0pqwouldbezero butatleastΩpnqqueriesarenecessarytoconstructuniformconﬁdenceintervalsonLf0pq.ItisclearthatsuchLf0pqviolates(A2) becauseNpLf0pq δqěnasδÑ0`butµf0pq“0.3.2UpperBoundThefollowingtheoremisourmainresultthatupperboundsthelocalminimaxrateofnoisyglobaloptimizationwithactivequeries.Theorem1.Foranyα M κ c0 C0ą0andf0PΣακpMqXΘC whereC“pc0 C0q deﬁneεUnpf0q:“sup!εą0:ε´p2`d{αqµf0pεqěn{logωn) (7)whereωą5`d{αisalargeconstant.SupposealsothatεUnpf0qÑ0asnÑ8.Thenforsufﬁcientlylargen thereexistsanestimatorpxnwithaccesstonactivequeriesx1 ... xnPX aconstantCRą0dependingonlyonα M κ c c0 C0andaconstantγą0dependingonlyonαanddsuchthatsupf0PΣακpMqXΘCsupfPΣακpMq }f´f0}8ďεUnpf0qPrf”Lppxn fqąCRlogγn¨pεUnpf0q`n´1{2qıď1{4.(8)3theparticular‘8normisusedforconvenienceonlyandcanbereplacedbyanyequivalentvectornorms.6Remark1.Unlikethe(local)smoothnessclassΣακpMq theadditionalfunctionclassΘCthatencapsulates(A2)isimposedonlyonthe“reference”functionf0butnotthetruefunctionftobeestimated.Thismakestheassumptionsconsiderablyweakerbecausethetruefunctionfmayviolate(A2)whileourresultsremainvalid.Remark2.Theestimatorpxndoesnotrequireknowledgeofparametersκ c0 C0orεUnpf0q andautomaticallyadaptstothem asshowninthenextsection.WhiletheknowledgeofsmoothnessparametersαandMseemstobenecessary weremarkthatitispossibletoadapttoαandMbyrunningOplog2nqparallelsessionsofpxnonOplognqgridsofαandMvalues andthenusingΩpn{log2nqsingle-pointqueriestodecideonthelocationwiththesmallestfunctionvalue.Suchanadaptivestrategywassuggestedin[20]toremoveanadditionalconditionin[34] whichalsoappliestooursettings.Remark3.Byrepeatingthealgorithmindependentlyforttimesandusingthe“multiplequery”strategyintheaboveremark thefailureprobabilityofourproposedalgorithmcanbereducedtoassmallas4´t anexponentiallydecayingprobabilitywithrespecttorepetitionst.Remark4.Whenthedistributionfunctionµf0pqdoesnotchangeabruptlywiththeexpressionofεUnpf0qcanbesigniﬁcantlysimpliﬁed.Inparticular ifforallPp0 c0sitholdsthatµf0p{lognqěµf0pq{rlognsOp1q (9)thenεUnpf0qcanbeupperboundedasεUnpf0qďrlognsOp1q¨sup!εą0:ε´p2`d{αqµf0pεqěn).(10)Itisalsonotedthatifµf0pqhasapolynomialbehaviorofµf0pq—βforsomeconstantβě0 thenEq.(9)issatisﬁedandsoisEq.(10).ThequantityεUnpf0q“inftεą0:ε´p2`d{αqµf0pεqěn{logωnuiscrucialindeterminingtheconvergencerateofoptimizationerrorofpxnlocallyaroundthereferencefunctionf0.WhilethedeﬁnitionofεUnpf0qismostlyimplicitandinvolvessolvinganinequalityconcerningthedistributionfunctionµf0p¨q weremarkthatitadmitsasimpleformwhenµf0hasapolynomialgrowthratesimilartoalocalTsybakovnoisecondition[29 46] asshownbythefollowingproposition:Proposition2.Supposeµf0pqÀβforsomeconstantβPr0 2`d{αq.ThenεUnpf0q“rOpn´α{p2α`d´αβqq.Inaddition ifβPr0 d{αsthenεUnpf0q`n´1{2ÀεUnpf0q“rOpn´α{p2α`d´αβqq.WeremarkthattheconditionβPr0 d{αswasalsoadoptedinthepreviouswork[34 Remark6]Also forLipschitzcontinuousfunctions(α“1)ourconditionsaresimilarto[20]andimpliesacorrespondingnear-optimalitydimensiond1consideredin[20].Proposition2canbeeasilyveriﬁedbysolvingthesystemε´p2`d{αqµf0pεqěn{logωnwiththeconditionµf0pqÀβ.Wethereforeomititsproof.Thefollowingtwoexamplesgivesomesimplereferencefunctionsf0thatsatisfytheµf0pqÀβconditioninProposition2withparticularvaluesofβ.Example1.Theconstantfunctionf0”0satisﬁes(A1) (A2)andtheconditioninProposition2withβ“0.Example2.f0PΣ2κpMqthatisstronglyconvex4satisﬁes(A1) (A2)andtheconditioninProposition2withβ“d{2.Example1issimpletoverify asthevolumeoflevelsetsoftheconstantfunctionf0”0exhibitsaphasetransitionat“0andą0 renderingβ“0theonlyparameteroptionforwhichµf0pqÀβ.Example2ismoreinvolved andholdsbecausethestrongconvexityoff0lowerboundsthegrowthrateoff0whenmovingawayfromitsminimum.WegivearigorousproofofExample2intheappendix.Wealsoremarkthatf0doesnotneedtobeexactlystronglyconvexforβ“d{2tohold andtheexampleisvalidfor e.g. piecewisestronglyconvexfunctionswithaconstantnumberofpiecestoo.TobestinterprettheresultsinTheorem1andProposition2 itisinstructivetocomparethe“local”raten´α{p2α`d´αβqwiththebaselineraten´α{p2α`dq whichcanbeattainedbyreconstructingf4Atwicedifferentiablefunctionf0isstronglyconvexifDσą0suchthat∇2f0pxqľσI @xPX.7insup-normandapplyingProposition1.Sinceβě0 thelocalconvergencerateestablishedinTheorem1isneverslower andtheimprovementcomparedtothebaselineraten´α{p2α`dqisdictatedbyβ whichgovernsthegrowthrateofvolumeoflevelsetsofthereferencefunctionf0.Inparticular forfunctionsthatgrowsfastwhenmovingawayfromitsminimum theparameterβislargeandthereforethelocalconvergenceratearoundf0couldbemuchfasterthann´α{p2α`dq.Theorem1alsoimpliesconcreteconvergenceratesforspecialfunctionsconsideredinExamples1and2.Fortheconstantreferencefunctionf0”0 Example1andTheorem1yieldthatRnpf0q—n´α{p2α`dq whichmatchesthebaselineraten´α{p2α`dqandsuggeststhatf0”0istheworst-casereferencefunction.Thisisintuitive becausef0”0hasthemostdrasticlevelsetchangeatÑ0`andthereforesmallperturbationsanywhereoff0resultinchangesoftheoptimallocations.Ontheotherhand iff0isstronglysmoothandconvexasinExample2 Theorem1suggeststhatRnpf0q—n´1{2 whichissigniﬁcantlybetterthanthen´2{p4`dqbaselinerate5andalsomatchesexistingworksonzeroth-orderoptimizationofconvexfunctions[1].Thefasterrateholdsintuitivelybecausestronglyconvexfunctionsgrowsfastwhenmovingawayfromtheminimum whichimpliessmalllevelsetchanges.Anactivequeryalgorithmcouldthenfocusmostofitsqueriesontothesmalllevelsetsoftheunderlyingfunction resultinginmoreaccuratelocalfunctionreconstructionsandfasteroptimizationerrorrate.OurproofofTheorem1isconstructive byupperboundingthelocalminimaxoptimizationerrorofanexplicitalgorithm.Atahigherlevel thealgorithmpartitionsthenactivequeriesevenlyintolognepochs andlevelsetsoffareestimatedattheendofeachepochbycomparing(uniform)conﬁdenceintervalsonadensegridonX.Itisthenprovedthatthevolumeoftheestimatedlevelsetscontractsgeometrically untilthetargetconvergencerateRnpf0qisattained.3.3LowerBoundsWeprovelocalminimaxlowerboundsthatmatchtheupperboundsinTheorem1uptologarithmicterms.AsweremarkedinSection2.2 inthelocalminimaxlowerboundformulationweassumethedataanalysthasfullknowledgeofthereferencefunctionf0 whichmakesthelowerboundsstrongerasmoreinformationisavailableapriori.Tofacilitatesuchastronglocalminimaxlowerbounds thefollowingadditionalconditionisimposedonthereferencefunctionf0ofwhichthedataanalysthasperfectinformation.(A2’)Thereexistconstantsc10 C10ą0suchthatMpLf0pq δqěC10µf0pqδ´dforall δPp0 c10s whereMpLf0pq δqisthemaximumnumberofdisjoint‘2ballsofradiusδthatcanbepackedintoLf0pq.WedenoteΘ1C1astheclassoffunctionsthatsatisfy(A2’)withrespecttoparametersC1“pc10 C10qą0.Intuitively (A2’)canberegardedasthe“reverse”versionof(A2) whichbasicallymeansthat(A2)is“tight”.Wearenowreadytostateourmainnegativeresult whichshows fromaninformation-theoreticalperspective thattheupperboundinTheorem1isnotimprovable.Theorem2.Supposeα c0 C0 c10 C10ą0andκ“8.DenoteC“pc0 C0qandC1“pc10 C10q.Foranyf0PΘCXΘ1C1 deﬁneεLnpf0q:“sup!εą0:ε´p2`d{αqµf0pεqěn).(11)ThenthereexistconstantMą0dependingonα d C C1suchthat foranyf0PΣακpM{2qXΘCXΘC1 infpxnsupfPΣακpMq }f´f0}8ď2εLnpf0qPrf“Lppxn;fqěεLnpf0q‰ě13.(12)Remark5.Foranyf0andnitalwaysholdsthatεLnpf0qďεUnpf0q.Remark6.Ifthedistributionfunctionµf0pqsatisﬁesEq.(9)inRemark4 thenεLnpf0qěεUnpf0q{rlognsOp1q.5Notethatf0beingstronglysmoothimpliesα“2inthelocalsmoothnessassumption.8Remark7.AstheupperboundinTheorem1mightdependsexponentiallyondomaindimensiond theremightalsobeanexponentialgapofdbetweentheupperandlowerboundsestablishedinTheorems1and2.Remark5showsthattheremightbeagapbetweenthelocallyminimaxupperandlowerboundsinTheorems1and2.Nevertheless Remark6showsthatunderthemildconditionofµf0pqdoesnotchangetooabruptlywith thegapbetweenεUnpf0qandεLnpf0qisonlyapoly-logarithmicterminn.Additionally thefollowingpropositionderivesexplicitexpressionofεLnpf0qforreferencefunctionswhosedistributionfunctionshaveapolynomialgrowth whichmatchestheProposition2uptolognfactors.Itsproofisagainstraightforward.Proposition3.Supposeµf0pqÁβforsomeβPr0 2`d{αq.ThenεLnpf0q“Ωpn´α{p2α`d´αβqq.Thefollowingpropositionadditionallyshowstheexistenceoff0PΣα8pMqXΘCXΘC1thatsatisﬁesµf0pq—βforanyvaluesofαą0andβPr0 d{αs.Itsproofisgivenintheappendix.Proposition4.Fixarbitraryα Mą0andβPr0 d{αs.Thereexistsf0PΣακpMqXΘCXΘC1forκ“8andconstantsC“pc0 C0q C1“pc10 C10qthatdependonlyonα β Manddsuchthatµf0pq—β.Theorem2andProposition3showthatthen´α{p2α`d´αβqupperboundonlocalminimaxcon-vergencerateestablishedinTheorem1isnotimprovableuptologarithmicfactorsofn.Suchinformation-theoreticallowerboundsontheconvergenceratesholdevenifthedataanalysthasperfectinformationoff0 thereferencefunctiononwhichthen´α{p2α`d´αβqlocalrateisbased.Ourresultsalsoimplyann´α{p2α`dqminimaxlowerboundoverallα-Höldersmoothfunctions showingthatwithoutadditionalassumptions noisyoptimizationofsmoothfunctionsisasdifﬁcultasreconstructingtheunknownfunctioninsup-norm.OurproofofTheorem2alsodiffersfromexistingminimaxlowerboundproofsforactivenonpara-metricmodels[11].TheclassicalapproachistoinvokeFano’sinequalityandtoupperboundtheKLdivergencebetweendifferentunderlyingfunctionsfandgusing}f´g}8 correspondingtothepointxPXthatleadstothelargestKLdivergence.Suchanapproach however doesnotproducetightlowerboundsforourproblem.Toovercomesuchdifﬁculties weborrowthelowerboundanalysisforbanditpureexplorationproblemsin[7].Inparticular ouranalysisconsidersthequerydistributionofanyactivequeryalgorithmA“pϕ1 ... ϕn φnqunderthereferencefunctionf0andboundstheperturbationinquerydistributionsbetweenf0andfusingLeCam’slemma.Afterwards anadversarialfunctionchoicefcanbemadebasedonthequerydistributionsoftheconsideredalgorithmA.Theorem2appliestoanyglobaloptimizationmethodthatmakesactivequeries correspondingtothequerymodelinDeﬁnition2.Thefollowingtheorem ontheotherhand showsthatforpassivealgorithms(Deﬁnition1)then´α{p2α`dqoptimizationrateisnotimprovableevenwithadditionallevelsetassumptionsimposedonf0.Thisdemonstratesanexplicitgapbetweenpassiveandadaptivequerymodelsinglobaloptimizationproblems.Theorem3.Supposeα c0 C0 c10 C10ą0andκ“8.DenoteC“pc0 C0qandC1“pc10 C10q.ThenthereexistconstantMą0dependingonα d C C1andNdependingonMsuchthat foranyf0PΣακpM{2qXΘCXΘC1satisfyingεLnpf0qďrεLn“:rlogn{nsα{p2α`dq infqxnsupfPΣακpMq }f´f0}8ď2rεLnPrf“Lppxn;fqěrεLn‰ě13forallněN.(13)Intuitively theapparentgapdemonstratedbyTheorems2and3betweentheactiveandpassivequerymodelsstemsfromtheobservationthat apassivealgorithmAonlyhasaccesstouniformlysampledquerypointsx1 ... xnandthereforecannotfocusonasmalllevelsetoffinordertoimprovequeryefﬁciency.Inaddition forfunctionsthatgrowfasterwhenmovingawayfromtheirminima(implyingalargervalueofβ) thegapbetweenpassiveandactivequerymodelsbecomesbiggerasactivequeriescanmoreeffectivelyexploittherestrictedlevelsetsofsuchfunctions.94ConclusionInthispaperweconsidertheproblemofnoisyzeroth-orderoptimizationofgeneralsmoothfunctions.Matchinglowerandupperboundsonthelocalminimaxconvergenceratesareestablished whicharesigniﬁcantlydifferentfromclassicalminimaxratesinnonparametricregressionproblems.Manyinterestingfuturedirectionsexistalongthislineofresearch includingexploitationofadditivestructuresintheunderlyingfunctionftocompletelyremovecurseofdimensionality functionswithspatiallyheterogeneoussmoothnessorlevelsetgrowthbehaviors andtodesignmorecomputationallyefﬁcientalgorithmsthatworkwellinpractice.AcknowledgementThisworkissupportedbyAFRLgrantFA8750-17-2-0212.Wethanktheanonymousreviewersformanyhelpfulsuggestionsthatimprovedthepresentationofthispaper.References[1]A.Agarwal O.Dekel andL.Xiao.Optimalalgorithmsforonlineconvexoptimizationwithmulti-pointbanditfeedback.InProceedingsoftheannualConferenceonLearningTheory(COLT) 2010.[2]A.Agarwal D.Foster D.Hsu S.Kakade andA.Rakhlin.Stochasticconvexoptimizationwithbanditfeedback.SIAMJournalonOptimization 23(1):213–240 2013.[3]N.Agarwal Z.Allen-Zhu B.Bullins E.Hazan andT.Ma.Findingapproximatelocalminimafasterthangradientdescent.InProceedingsoftheAnnualACMSIGACTSymposiumonTheoryofComputing(STOC) 2017.[4]S.Balakrishnan S.Narayanan A.Rinaldo A.Singh andL.Wasserman.Clustertreesonmanifolds.InProceedingsofAdvancesinNeuralInformationProcessingSystems(NIPS) 2013.[5]M.-F.Balcan A.Beygelzimer andJ.Langford.Agnosticactivelearning.JournalofComputerandSystemSciences 75(1):78–89 2009.[6]S.Bubeck R.Eldan andY.T.Lee.Kernel-basedmethodsforbanditconvexoptimization.InProceedingsoftheannualACMSIGACTSymposiumonTheoryofComputing(STOC) 2017.[7]S.Bubeck R.Munos andG.Stoltz.Pureexplorationinmulti-armedbanditsproblems.InProceedingsoftheInternationalconferenceonAlgorithmiclearningtheory(ALT) 2009.[8]S.Bubeck R.Munos G.Stoltz andC.Szepesvári.X-armedbandits.JournalofMachineLearningResearch 12(May):1655–1695 2011.[9]A.D.Bull.Convergenceratesofefﬁcientglobaloptimizationalgorithms.JournalofMachineLearningResearch 12(Oct):2879–2904 2011.[10]Y.Carmon O.Hinder J.C.Duchi andA.Sidford.“convexuntilprovenguilty":Dimension-freeaccelerationofgradientdescentonnon-convexfunctions.arXivpreprintarXiv:1705.02766 2017.[11]R.M.CastroandR.D.Nowak.Minimaxboundsforactivelearning.IEEETransactionsonInformationTheory 54(5):2339–2353 2008.[12]K.Chaudhuri S.Dasgupta S.Kpotufe andU.vonLuxburg.Consistentproceduresforclustertreeestimationandpruning.IEEETransactionsonInformationTheory 60(12):7900–7912 2014.[13]H.Chen.Lowerrateofconvergenceforlocatingamaximumofafunction.TheAnnalsofStatistics 16(3):1330–1334 1988.10[14]S.Dasgupta D.J.Hsu andC.Monteleoni.Ageneralagnosticactivelearningalgorithm.InProceedingsofAdvancesinneuralinformationprocessingsystems(NIPS) 2008.[15]J.DuchiandF.Ruan.Localasymptoticsforsomestochasticoptimizationproblems:Optimality constraintidentiﬁcation anddualaveraging.arXivpreprintarXiv:1612.05612 2016.[16]J.C.Duchi J.Lafferty andY.Zhu.Localminimaxcomplexityofstochasticconvexoptimization.InNIPS 2016.[17]J.FanandI.Gijbels.Localpolynomialmodellinganditsapplications.CRCPress 1996.[18]A.D.Flaxman A.T.Kalai andH.B.McHanan.Onlineconvexoptimizationinthebanditsetting:gradientdescentwithoutagradient.InProceedingsoftheACM-SIAMSymposiumonDiscreteAlgorithms(SODA) 2005.[19]R.Ge F.Huang C.Jin andY.Yuan.Escapingfromsaddlepoints-onlinestochasticgradientfortensordecomposition.InProceedingsoftheannualConferenceonLearningTheory(COLT) 2015.[20]J.-B.Grill M.Valko andR.Munos.Black-boxoptimizationofnoisyfunctionswithunknownsmoothness.InProceedingsofAdvancesinNeuralInformationProcessingSystems(NIPS) 2015.[21]S.Hanneke.Aboundonthelabelcomplexityofagnosticactivelearning.InProceedingsoftheInternationalConferenceonMachineLearning(ICML) 2007.[22]E.Hazan A.Klivans andY.Yuan.Hyperparameteroptimization:Aspectralapproach.arXivpreprintarXiv:1706.00764 2017.[23]E.Hazan K.Levy andS.Shalev-Shwartz.Beyondconvexity:Stochasticquasi-convexoptimization.InProceedingsofAdvancesinNeuralInformationProcessingSystems(NIPS) 2015.[24]K.G.Jamieson R.Nowak andB.Recht.Querycomplexityofderivative-freeoptimization.InProceedingsofAdvancesinNeuralInformationProcessingSystems(NIPS) 2012.[25]A.R.KanandG.T.Timmer.StochasticglobaloptimizationmethodspartI:Clusteringmethods.MathematicalProgramming 39(1):27–56 1987.[26]A.R.KanandG.T.Timmer.StochasticglobaloptimizationmethodspartII:Multilevelmethods.MathematicalProgramming 39(1):57–78 1987.[27]J.KieferandJ.Wolfowitz.Stochasticestimationofthemaximumofaregressionfunction.TheAnnalsofMathematicalStatistics 23(3):462–466 1952.[28]R.D.Kleinberg.Nearlytightboundsforthecontinuum-armedbanditproblem.InAdvancesinNeuralInformationProcessingSystems(NIPS) 2005.[29]A.P.KorostelevandA.B.Tsybakov.Minimaxtheoryofimagereconstruction volume82.SpringerScience&BusinessMedia 2012.[30]O.V.Lepski E.Mammen andV.G.Spokoiny.Optimalspatialadaptationtoinhomogeneoussmoothness:anapproachbasedonkernelestimateswithvariablebandwidthselectors.TheAnnalsofStatistics 25(3):929–947 1997.[31]C.Malherbe E.Contal andN.Vayatis.Arankingapproachtoglobaloptimization.InProceedingsoftheInternationalConferenceonMachineLearning(ICML) 2016.[32]C.MalherbeandN.Vayatis.Globaloptimizationoflipschitzfunctions.InProceedingsoftheInternationalConferenceonMachineLearning(ICML) 2017.[33]S.Minsker.Non-asymptoticboundsforpredictionproblemsanddensityestimation.PhDthesis GeorgiaInstituteofTechnology 2012.11[34]S.Minsker.Estimationofextremevaluesandassociatedlevelsetsofaregressionfunctionviaselectivesampling.InProceedingsofConferencesonLearningTheory(COLT) 2013.[35]N.Nakamura J.Seepaul J.B.Kadane andB.Reeja-Jayan.Designforlow-temperaturemicrowave-assistedcrystallizationofceramicthinﬁlms.AppliedStochasticModelsinBusinessandIndustry 2017.[36]A.NemirovskiandD.Yudin.Problemcomplexityandmethodefﬁciencyinoptimization.AWiley-IntersciencePublication 1983.[37]Y.NesterovandB.T.Polyak.Cubicregularizationofnewtonmethodanditsglobalperformance.MathematicalProgramming 108(1):177–205 2006.[38]W.Polonik.Measuringmassconcentrationsandestimatingdensitycontourclusters-anexcessmassapproach.TheAnnalsofStatistics 23(3):855–881 1995.[39]E.Purzen.Onestimationofaprobabilitydensityandmode.TheAnnalsofMathematicalStatistics 33(3):1065–1076 1962.[40]C.E.RasmussenandC.K.Williams.Gaussianprocessesformachinelearning volume1.MITpressCambridge 2006.[41]B.Reeja-Jayan K.L.Harrison K.Yang C.-L.Wang A.Yilmaz andA.Manthiram.Microwave-assistedlow-temperaturegrowthofthinﬁlmsinsolution.Scientiﬁcreports 2 2012.[42]P.RigolletandR.Vert.Optimalratesforplug-inestimatorsofdensitylevelsets.Bernoulli 15(4):1154–1178 2009.[43]A.RisteskiandY.Li.Algorithmsandmatchinglowerboundsforapproximately-convexoptimization.InProceedingsofAdvancesinNeuralInformationProcessingSystems(NIPS) 2016.[44]J.Scarlett I.Bogunovic andV.Cevher.Lowerboundsonregretfornoisygaussianprocessbanditoptimization.InProceedingsoftheannualConferenceonLearningTheory(COLT) 2017.[45]A.Singh C.Scott andR.Nowak.Adaptivehausdorffestimationofdensitylevelsets.TheAnnalsofStatistics 37(5B):2760–2782 2009.[46]A.B.Tsybakov.Introductiontononparametricestimation.SpringerSeriesinStatistics.Springer NewYork 2009.[47]A.W.VanderVaart.Asymptoticstatistics volume3.Cambridgeuniversitypress 1998.[48]Y.Zhang P.Liang andM.Charikar.Ahittingtimeanalysisofstochasticgradientlangevindynamics.InProceedingsoftheannualConferenceonLearningTheory(COLT) 2017.12,Yining Wang
Sivaraman Balakrishnan
Aarti Singh