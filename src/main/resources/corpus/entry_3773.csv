2019,No-Regret Learning in Unknown Games with Correlated Payoffs,We consider the problem of learning to play a repeated multi-agent game with an unknown reward function. Single player online learning algorithms attain strong regret bounds when provided with full information feedback  which unfortunately is unavailable in many real-world scenarios. Bandit feedback alone  i.e.  observing outcomes only for the selected action  yields substantially worse performance.  In this paper  we consider a natural model where  besides a noisy measurement of the obtained reward  the player can also observe the opponents' actions. This feedback model  together with a regularity assumption on the reward function  allows us to exploit the correlations among different game outcomes by means of Gaussian processes (GPs). We propose a novel confidence-bound based bandit algorithm GP-MW  which utilizes the GP model for the reward function and runs a multiplicative weight (MW) method. We obtain novel kernel-dependent regret bounds that are comparable to the known bounds in the full information setting  while substantially improving upon the existing bandit results. We experimentally demonstrate the effectiveness of  GP-MW in random matrix games  as well as real-world problems of traffic routing and movie recommendation. In our experiments  GP-MW consistently outperforms several baselines  while its performance is often comparable to methods that have access to full information feedback.,No-Regret Learning in Unknown Games with

Correlated Payoffs

Pier Giuseppe Sessa

ETH Zürich

sessap@ethz.ch

Ilija Bogunovic

ETH Zürich

ilijab@ethz.ch

Maryam Kamgarpour

ETH Zürich

maryamk@ethz.ch

Andreas Krause

ETH Zürich

krausea@ethz.ch

Abstract

We consider the problem of learning to play a repeated multi-agent game with an
unknown reward function. Single player online learning algorithms attain strong
regret bounds when provided with full information feedback  which unfortunately
is unavailable in many real-world scenarios. Bandit feedback alone  i.e.  observing
outcomes only for the selected action  yields substantially worse performance. In
this paper  we consider a natural model where  besides a noisy measurement of
the obtained reward  the player can also observe the opponents’ actions. This
feedback model  together with a regularity assumption on the reward function 
allows us to exploit the correlations among different game outcomes by means of
Gaussian processes (GPs). We propose a novel conﬁdence-bound based bandit
algorithm GP-MW  which utilizes the GP model for the reward function and runs
a multiplicative weight (MW) method. We obtain novel kernel-dependent regret
bounds that are comparable to the known bounds in the full information setting 
while substantially improving upon the existing bandit results. We experimentally
demonstrate the effectiveness of GP-MW in random matrix games  as well as real-
world problems of trafﬁc routing and movie recommendation. In our experiments 
GP-MW consistently outperforms several baselines  while its performance is often
comparable to methods that have access to full information feedback.

1

Introduction

Many real-world problems  such as trafﬁc routing [14]  market prediction [10]  and social network
dynamics [21]  involve multiple learning agents that interact and compete with each other. Such
problems can be described as repeated games  in which the goal of every agent is to maximize her
cumulative reward. In most cases  the underlying game is unknown to the agents  and the only way to
learn about it is by repeatedly playing and observing the corresponding game outcomes.
The performance of an agent in a repeated game is often measured in terms of regret. For example 
in trafﬁc routing  the regret of an agent quantiﬁes the reduction in travel time had the agent known
the routes chosen by the other agents. No-regret algorithms for playing unknown repeated games
exist  and their performance depends on the information available at every round. In the case of
full information feedback  the agent observes the obtained reward  as well as the rewards of other
non-played actions. While these algorithms attain strong regret guarantees  such full information
feedback is often unrealistic in real-world applications. In trafﬁc routing  for instance  agents only
observe the incurred travel times and cannot observe the travel times for the routes not chosen.
In this paper  we address this challenge by considering a more realistic feedback model  where at
every round of the game  the agent plays an action and observes the noisy reward outcome. In
addition to this bandit feedback  the agent also observes the actions played by other agents. Under
this feedback model and further regularity assumptions on the reward function  we present a novel

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

HEDGE [11]

EXP3 [3]

GP-MW [this paper]

Feedback
Regret

rewards for all actions

OpT log Ki

obtained reward

OpT Ki log Ki

obtained reward + opponents’ actions

OpT log Ki + TpT

Table 1: Finite action set regret bounds that depend on the available feedback observed by player i at
each time step. Time horizon is denoted with T   and Ki is the number of actions available to player i.
Kernel dependent quantity T (Eq. (3)) captures the degrees of freedom in the reward function.

no-regret algorithm for playing unknown repeated games. The proposed algorithm alleviates the need
for full information feedback while still achieving comparable regret guarantees.
Related Work. In the full information setting  multiplicative-weights (MW) algorithms [17] such
as HEDGE [11] attain optimal O(pT log Ki) regret  where Ki is the number of actions available to
agent i. In the case of convex action sets in Rdi  and convex and Lipschitz rewards  online convex
optimization algorithms attain optimal O(pT ) regret [25]. By only assuming Lipschitz rewards
and bounded action sets  O(pdiT log T ) regret follows from [18]  while in [13] the authors provide

efﬁcient gradient-based algorithms with ‘local’ regret guarantees. Full information feedback requires
perfect knowledge of the game and is unrealistic in many applications. Our proposed algorithm
overcomes this limitation while achieving comparable regret bounds.
In the more challenging bandit setting  existing algorithms have a substantially worse dependence on

the size of the action set. For ﬁnite actions  EXP3 [3] and its variants ensure optimal O(pT Ki log Ki)
O(poly(di)pT ) regret [6]  while in the case of Lipschitz rewards O(T

regret. In the case of convex action sets  and convex and Lipschitz rewards  bandit algorithms attain
di+1
di+2 log T ) regret can be
obtained [22]. In contrast  our algorithm works in the noisy bandit setting and requires the knowledge
of the actions played by other agents. This allows us to  under some regularity assumptions  obtain
substantially improved performance. In Table 1  we summarize the regret and feedback model of our
algorithm together with the existing no-regret algorithms.
The previously mentioned online algorithms reduce the unknown repeated game to a single agent
problem against an adversarial and adaptive environment that selects a different reward function at
every time step [7]. A fact not exploited by these algorithms is that in a repeated game  the rewards
obtained at different time steps are correlated through a static unknown reward function. In [24]
the authors use this fact to show that  if every agent uses a regularized no-regret algorithm  their
individual regret grows at a lower rate of O(T 1/4)  while the sum of their rewards grows only as O(1).
In contrast to [24]  we focus on the single-player viewpoint  and we do not make any assumption on
opponents strategies1. Instead  we show that by observing opponents’ actions  the agent can exploit
the structure of the reward function to reduce her individual regret.
Contributions. We propose a novel no-regret bandit algorithm GP-MW for playing unknown
repeated games. GP-MW combines the ideas of the multiplicative weights update method [17]  with
GP upper conﬁdence bounds  a powerful tool used in GP bandit algorithms (e.g.  [23  5]). When a
ﬁnite number Ki of actions is available to player i  we provide a novel high-probability regret bound

O(pT log Ki + TpT )  that depends on a kernel-dependent quantity T [23]. For common kernel
choices  this results in a sublinear regret bound  which grows only logarithmically in Ki. In the case
of inﬁnite action subsets of Rdi and Lipschitz rewards  via a discretization argument  we obtain a
high-probability regret bound of O(pdiT log(diT ) + TpT ). We experimentally demonstrate that

GP-MW outperforms existing bandit baselines in random matrix games and trafﬁc routing problems.
Moreover  we present an application of GP-MW to a novel robust Bayesian optimization setting in
which our algorithm performs favourably in comparison to other baselines.

2 Problem Formulation

We consider a repeated static game among N non-cooperative agents  or players. Each player i has
an action set Ai ✓ Rdi and a reward function ri : A = A1 ⇥···⇥A N ! [0  1]. We assume that
the reward function ri is unknown to player i. At every time t  players simultaneously choose actions
t )  which depends on the played action ai
at = (a1
t

t  ai

t   . . .   aN

t ) and player i obtains a reward ri(ai
1In fact  they are allowed to be adaptive and adversarial.

2

and the actions ai
t

:= (a1

t   . . .   ai1

t

to maximize the cumulative rewardPT

deﬁned as

  ai+1

  . . .   aN
t  ai

t
t=1 ri(ai

t ) of all the other players. The goal of player i is
t ). After T time steps  the regret of player i is

Ri(T ) = max
a2Ai

ri(a  ai

t ) 

TXt=1

TXt=1

ri(ai

t  ai

t )  

(1)

t=1.

t  ai

t  ai

t  ai

t }T

t) = ri(ai

t )  i.e.  rt(ai

t )]a2Ai 2 RKi. With bandit feedback  only the reward ri(ai

i.e.  the maximum gain the player could have achieved by playing the single best ﬁxed action in case
the sequence of opponents’ actions {ai
t=1 and the reward function were known in hindsight. An
algorithm is no-regret for player i if Ri(T )/T ! 0 as T ! 1 for any sequence {ai
t }T
First  we consider the case of a ﬁnite number of available actions Ki  i.e.  |Ai| = Ki. To achieve
no-regret  the player should play mixed strategies [7]  i.e.  probability distributions wi
t 2 [0  1]Ki
over Ai. With full-information feedback  at every time t player i observes the vector of rewards
rt = [ri(a  ai
t ) is observed by
the player. Existing full information and bandit algorithms [11  3]  reduce the repeated game to a
sequential decision making problem between player i and an adaptive environment that  at each time
t  selects a reward function rt : Ai ! [0  1]. In a repeated game  the reward that player i observes at
time t is a static ﬁxed function of (ai
t )  and in many practical settings
similar game outcomes lead to similar rewards (see  e.g.  the trafﬁc routing application in Section 4.2).
In contrast to existing approaches  we exploit such correlations by considering the feedback and
reward function models described below.
Feedback model. We consider a noisy bandit feedback model where  at every time t  player i
observes a noisy measurement of the reward ˜ri
t is i-sub-Gaussian  i.e. 
i /2) for all c 2 R  with independence over time. The presence of noise is
E[exp(c✏ i
t)]  exp(c22
typical in real-world applications  since perfect measurements are unrealistic  e.g.  measured travel
times in trafﬁc routing.
Besides the standard noisy bandit feedback  we assume player i also observes the played actions ai
t
of all the other players. In some applications  the reward function ri depends only indirectly on ai
t
through some aggregative function (ai
t ) represents
the total occupancy of the network’s edges  while in network games [15]  it represents the strategies
t ) instead of ai
of player i’s neighbours. In such cases  it is sufﬁcient for the player to observe (ai
.
t
Regularity assumption on rewards. In this work  we assume the unknown reward function ri :
A! [0  1] has a bounded norm in a reproducing kernel Hilbert space (RKHS) associated with a
positive semi-deﬁnite kernel function ki(· ·)  that satisﬁes ki(a  a0)  1 for all a  a0 2A . The
RKHS norm krikki =phri  riiki measures the smoothness of ri with respect to the kernel function
ki(· ·)  while the kernel encodes the similarity between two different outcomes of the game a  a0 2A .
Typical kernel choices are polynomial  Squared Exponential  and Matérn:

t ). For example  in trafﬁc routing [14]  (ai

t where ✏i

t = ri(ai

t  ai

t ) + ✏i

kpoly(a  a0) =✓b +

kM at´ern(a  a0) =

a>a0

 

kSE(a  a0) = exp✓
l ◆n
(⌫) sp2⌫
l !⌫
B⌫ sp2⌫
l !  

21⌫

s2

2l2◆  

where s = ka  a0k2  B⌫ is the modiﬁed Bessel function  and l  n  ⌫ > 0 are kernel hyperparameters
[20  Section 4]. This is a standard smoothness assumption used in kernelized bandits and Bayesian
optimization (e.g.  [23  9]). In our context it allows player i to use the observed history of play to
learn about ri and predict unseen game outcomes. Our results are not restricted to any speciﬁc kernel
function  and depending on the application at hand  various kernels can be used to model different
types of reward functions. Moreover  composite kernels (see e.g.  [16]) can be used to encode the
differences in the structural dependence of ri on ai and ai.
It is well known that Gaussian Process models can be used to learn functions with bounded RKHS
norm [23  9]. A GP is a probability distribution over functions f (a) ⇠GP (µ(a)  k(a  a0))  speciﬁed
by its mean and covariance functions µ(·) and k(· ·)  respectively. Given a history of measurements
j=1 with yj = f (aj) + ✏j and ✏j ⇠N (0  2)  the posterior distribution under
{yj}t

j=1 at points {aj}t

3

Algorithm 1 The GP-MW algorithm for player i
Input: Set of actions Ai  GP prior (µ0  0  ki)  parameters {t}t1 ⌘
1: Initialize: wi
(1  . . .   1) 2 RKi
2: for t = 1  2  . . .   T do
Sample action ai
t ⇠ wi
3:
t
Observe noisy reward ˜ri
4:

1 = 1
Ki

t and opponents’ actions ai
t
t  ai
t ) + ✏i
t
Compute optimistic reward estimates ˆrt 2 RKi :

˜ri
t = ri(ai

:

5:

6:

[ˆrt]a = min{1  U CBt(a  ai
t )}

for every a = 1  . . .   Ki

for every a = 1  . . .   Ki

(5)

(6)

Update mixed strategy:

[wi

t+1]a =

t]a exp(⌘ (1  [ˆrt]a))

t]k exp(⌘ (1  [ˆrt]k))

[wi
k=1[wi

PKi

Update µt  t according to (2)-(3) by appending (at  ˜ri

t) to the history of play.

7:
8: end for

a GP(0  k(a  a0)) prior is also Gaussian  with mean and variance functions:

µt(a) = kt(a)>(Kt + 2It)1yt
t (a) = k(a  a)  kt(a)>(Kt + 2It)1kt(a)  
2
where kt(a) = [k(aj  a)]t
j=1  yt = [y1  . . .   yt]>  and Kt = [k(aj  aj0)]j j0 is the kernel matrix.
At time t  an upper conﬁdence bound on f can be obtained as:

(2)
(3)

U CBt(a) := µt1(a) + tt1(a)  

(4)
where t is a parameter that controls the width of the conﬁdence bound and ensures U CBt(a)  f (a) 
for all a 2A and t  1  with high probability [23]. We make this statement precise in Theorem 1.
Due to the above regularity assumptions and feedback model  player i can use the history of play
t1)} to compute an upper conﬁdence bound U CBt(·) of the unknown reward
{(a1  ˜ri
function ri by using (4). In the next section  we present our algorithm that makes use of U CBt(·) to
simulate full information feedback.

1)  . . .   (at1  ˜ri

3 The GP-MW Algorithm

We now introduce GP-MW  a novel no-regret bandit algorithm  which can be used by a generic player
i (see Algorithm 1). GP-MW maintains a probability distribution (or mixed strategy) wi
t over Ai and
updates it at every time step using a multiplicative-weight (MW) subroutine (see (6)) that requires
full information feedback. Since such feedback is not available  GP-MW builds (in (5)) an optimistic
estimate of the true reward of every action via the upper conﬁdence bound U CBt of ri. Moreover 
since rewards are bounded in [0  1]  the algorithm makes use of min{1  U CBt(·)}. At every time
step t  GP-MW plays an action ai
t and
t  and uses the noisy reward observation ˜ri
actions ai
t played by other players to compute the updated upper conﬁdence bound U CBt+1(·).
In Theorem 1  we present a high-probability regret bound for GP-MW while all the proofs of this
section can be found in the supplementary material. The obtained bound depends on the maximum
information gain  a kernel-dependent quantity deﬁned as:

t sampled from wi

t := max
a1 ... at

1
2

log det(It + 2Kt) .

It quantiﬁes the maximal reduction in uncertainty about ri after observing outcomes {aj}t
j=1 and
the corresponding noisy rewards. The result of [23] shows that this quantity is sublinear in T   e.g. 
T = O((log T )d+1) in the case of kSE  and T = OT
where d is the total dimension of the outcomes a 2A   i.e.  d =PN

2⌫+d2+d log T in the case of kM at´ern 

i=1 di.

d2+d

4

Theorem 1. Fix  2 (0  1) and assume ✏i
t’s are i-sub-Gaussian with independence over time. For
any ri such that krikki  B  if player i plays actions from Ai  |Ai| = Ki  according to GP-MW
with t = B +p2(t1 + log(2/)) and ⌘ =p(8 log Ki)/T   then with probability at least 1   

Ri(T ) = O⇣pT log Ki +pT log(2/) + BpT T +pT T (T + log(2/))⌘ .

The proof of this theorem follows by the decomposition of the regret of GP-MW into the sum of two
terms. The ﬁrst term corresponds to the regret that player i incurs with respect to the sequence of
computed upper conﬁdence bounds. The second term is due to not knowing the true reward function
ri. The proof of Theorem 1 then proceeds by bounding the ﬁrst term using standard results from
adversarial online learning [7]  while the second term is upper bounded by using regret bounding
techniques from GP optimization [23  4].
Theorem 1 can be made more explicit by substituting bounds on T . For instance  in the case of the
squared exponential kernel  the regret bound becomes Ri(T ) = O⇣(log Ki)1/2 +(log T )d+1pT⌘.
In comparison to the standard multi-armed bandit regret bound O(pT Ki log Ki) (e.g.  [3])  this
regret bound does not depend on pKi  similarly to the ideal full information setting.

The case of continuous action sets
In this section  we consider the case when Ai is a (continuous) compact subset of Rdi. In this case 
further assumptions are required on ri and Ai to achieve sublinear regret. Hence  we assume a
bounded set Ai ⇢ Rdi and ri to be Lipschitz continuous in ai. Under the same assumptions  existing
regret bounds are O(pdiT log T ) and O(T
di+1
di+2 log T ) in the full information [18] and bandit setting
[22]  respectively. By using a discretization argument  we obtain a high probability regret bound for
GP-MW.
Corollary 1. Let  2 (0  1) and ✏i
t be i-sub-Gaussian with independence over time. Assume
krikk  B  Ai ⇢ [0  b]di  and ri is L-Lipschitz in its ﬁrst argument  and consider the discretization
[Ai]T with |[Ai]T| = (LbpdiT )di such that ka [a]Tk1 pdi/T /L for every a 2A i  where [a]T
is the closest point to a in [Ai]T . If player i plays actions from [Ai]T according to GP-MW with
t = B +p2(t1 + log(2/)) and ⌘ = p8di log(LbpdiT )/T   then with probability at least
1   
Ri(T ) = O✓qdiT log(LbpdiT ) +pT log(2/) + BpT T +pT T (T + log(2/))◆ .

By substituting bounds on T   our bound becomes Ri(T ) = O(T 1/2polylog(T)) in the case of the
SE kernel (for ﬁxed d). Such a bound has a strictly better dependence on T than the existing bandit
di+1
bound O(T
di+2 log T ) from [22]. Similarly to [22  18]  the algorithm resulting from Corollary 1
is not efﬁcient in high dimensional settings  as its computational complexity is exponential in di.

4 Experiments

In this section  we consider random matrix games and a trafﬁc routing model and compare GP-MW
with the existing algorithms for playing repeated games. Then  we show an application of GP-MW
to robust BO and compare it with existing baselines on a movie recommendation problem.

t   a2

t  a2

4.1 Repeated random matrix games
We consider a repeated matrix game between two players with actions A1 = A2 = {0  1  . . .   K  1}
and payoff matrices Ai 2 RK⇥K  i = 1  2. At every time step  each player i receives a payoff
t   where [Ai]i j indicates the (i  j)-th entry of matrix Ai. We select K = 30
ri(a1
and generate 10 random matrices with r1 = r2 ⇠ GP (0  k(· ·))  where k = kSE with l = 6. We set
the noise to ✏i
t ⇠N (0  1)  and use T = 200. For every game  we distinguish between two settings:
Against random opponent. In this setting  player-2 plays actions uniformly at random from A2 at
every round t  while player-1 plays according to a no-regret algorithm. In Figure 1a  we compare the

t ) = [Ai]a1

5

(a) Against random opponent

(b) GP-MW vs. EXP3.P.

Figure 1: GP-MW leads to smaller regret compared to EXP3.P. HEDGE is an idealized benchmark
which upper bounds the achievable performance. Shaded areas represent ± one standard deviation.

time-averaged regret of player-1 when playing according to HEDGE [11]  EXP3.P [3]  and GP-MW.
Our algorithm is run with the true function prior while HEDGE receives (unrealistic) noiseless full
information feedback (at every round t) and leads to the lowest regret. When only the noisy bandit
feedback is available  GP-MW signiﬁcantly outperforms EXP3.P.
GP-MW vs EXP3.P. Here  player-1 plays according to GP-MW while player-2 is an adaptive
adversary and plays using EXP3.P. In Figure 1b  we compare the regret of the two players averaged
over the game instances. GP-MW outperforms EXP3.P and ensures player-1 a smaller regret.

4.2 Repeated trafﬁc routing

We consider the Sioux-Falls road network [14  1]  a standard benchmark model in the transportation
literature. The network is a directed graph with 24 nodes and 76 edges (e 2 E). In this experiment 
we have N = 528 agents and every agent i seeks to send some number of units ui from a given
origin to a given destination node. To do so  agent i can choose among Ki = 5 possible routes
consisting of network edges E(i) ⇢ E. A route chosen by agent i corresponds to action ai 2 R|E(i)|
with [ai]e = ui in case e belongs to the route and [ai]e = 0 otherwise. The goal of each agent i is to
minimize the travel time weighted by the number of units ui. The travel time of an agent is unknown
and depends on the total occupancy of the traversed edges within the chosen route. Hence  the travel
time increases when more agents use the same edges. The number of units ui for every agent  as
well as travel time functions for each edge  are taken from [14  1]. A more detailed description of
our experimental setup is provided in Appendix C.
We consider a repeated game  where agents choose routes using either of the following algorithms:
• HEDGE. To run HEDGE  each agent has to observe the travel time incurred had she chosen any
different route. This requires knowing the exact travel time functions. Although these assumptions
are unrealistic  we use HEDGE as an idealized benchmark.

corresponds to the standard bandit feedback.

• EXP3.P. In the case of EXP3.P  agents only need to observe their incurred travel time. This
• GP-MW. Let (ai
t ) 2 R|E(i)| be the total occupancy (by other agents) of edges E(i) at time t.
To run GP-MW  agent i needs to observe a noisy measurement of the travel time as well as the
corresponding (ai
t ).

• Q-BRI (Q-learning Better Replies with Inertia algorithm [8]). This algorithm requires the same
feedback as GP-MW and is proven to asymptotically converge to a Nash equilibrium (as the
considered game is a potential game [19]). We use the same set of algorithm parameters as in [8].
For every agent i to run GP-MW  we use a composite kernel ki such that for every a1  a2 2A  
1 is a linear kernel
ki((ai
and ki
First  we consider a random subset of 100 agents that we refer to as learning agents. These agents
choose actions (routes) according to the aforementioned no-regret algorithms for T = 100 game

1  ai
1 + (ai
2 is a polynomial kernel of degree n 2{ 2  4  6}.

2 )) = ki

2 + (ai

2 ))   where ki

2) · ki

1 )  (ai

2  ai

1(ai

1  ai

2(ai

1 )  ai

6

Figure 2: GP-MW leads to a signiﬁcantly smaller average regret compared to EXP3.P and Q-BRI
and improves the overall congestion in the network. HEDGE represents an idealized full information
benchmark which upper bounds the achievable performance.

rounds. The remaining non-learning agents simply choose the shortest route  ignoring the presence
of the other agents. In Figure 2 (top plots)  we compare the average regret (expressed in hours) of
the learning agents when they use the different no-regret algorithms. We also show the associated
average congestion in the network (see (13) in Appendix C for a formal deﬁnition). When playing
according to GP-MW  agents incur signiﬁcantly smaller regret and the overall congestion is reduced
in comparison to EXP3.P and Q-BRI.
In our second experiment  we consider the same setup as before  but we vary the number of learning
agents. In Figure 2 (bottom plots)  we show the ﬁnal (when T = 100) average regret and congestion
as a function of the number of learning agents. We observe that GP-MW systematically leads to
a smaller regret and reduced congestion in comparison to EXP3.P and Q-BRI. Moreover  as the
number of learning agents increases  both HEDGE and GP-MW reduce the congestion in the network 
while this is not the case with EXP3.P or Q-BRI (due to a slower convergence).

4.3 GP-MW and robust Bayesian Optimization
In this section  we apply GP-MW to a novel robust Bayesian Optimization (BO) setting  similar to
the one considered in [4]. The goal is to optimize an unknown function f (under the same regularity
assumptions as in Section 2) from a sequence of queries and corresponding noisy observations. Very
often  the actual queried points may differ from the selected ones due to various input perturbations 
or the function may depend on external parameters that cannot be controlled (see [4] for examples).
This scenario can be modelled via a two player repeated game  where a player is competing against
an adversary. The unknown reward function is given by f : X⇥  ! R. At every round t of the
game  the player selects a point xt 2X   and the adversary chooses t 2 . The player then observes
the parameter t and a noisy estimate of the reward: f (xt  t) + ✏t. After T time steps  the player
incurs the regret

R(T ) = max
x2X

f (x  t) 

TXt=1

Note that both the regret deﬁnition and feedback model are the same as in Section 2.

f (xt  t).

TXt=1

7

(a) Users chosen at random.

(b) Users chosen by adaptive adversary.

Figure 3: GP-MW ensures no-regret against both randomly and adaptively chosen users  while
GP-UCB and STABLEOPT attain constant average regret.

In the standard (non-adversarial) Bayesian optimization setting  the GP-UCB algorithm [23] ensures
no-regret. On the other hand  the STABLEOPT algorithm [4] attains strong regret guarantees against
the worst-case adversary which perturbs the ﬁnal reported point xT . Here instead  we consider
the case where the adversary is adaptive at every time t  i.e.  it can adapt to past selected points
x1  . . .   xt1. We note that both GP-UCB and STABLEOPT fail to achieve no-regret in this setting 
as both algorithms are deterministic conditioned on the history of play. On the other hand  GP-MW
is a no-regret algorithm in this setting according to Theorem 1 (and Corollary 1).
Next  we demonstrate these observations experimentally in a movie recommendation problem.
Movie recommendation. We seek to recommend movies to users according to their preferences.
A priori it is unknown which user will see the recommendation at any time t. We assume that such a
user is chosen arbitrarily (possibly adversarially)  simultaneously to our recommendation.
We use the MovieLens-100K dataset [12] which provides a matrix of ratings for 1682 movies
rated by 943 users. We apply non-negative matrix factorization with p = 15 latent factors on the
incomplete rating matrix and obtain feature vectors mi  uj 2 Rp for movies and users  respectively.
Hence  m>i uj represents the rating of movie i by user j. At every round t  the player selects mt 2
{m1  . . .   m1682}  the adversary chooses (without observing mt) a user index it 2{ 1  . . .   943} 
and the player receives reward f (mt  it) = m>t uit. We model f via a GP with composite kernel
k((m  i)  (m0  i0)) = k1(m  m0) · k2(i  i0) where k1 is a linear kernel and k2 is a diagonal kernel.
We compare the performance of GP-MW against the ones of GP-UCB and STABLEOPT
when sequentially recommending movies.
In this experiment  we let GP-UCB select mt =
arg maxm maxi U CBt(m  i)  while STABLEOPT chooses mt = arg maxm mini U CBt(m  i)
at every round t. Both algorithms update their posteriors with measurements at (mt ˆit) with
ˆit = arg maxi U CBt(mt  i) in the case of GP-UCB and ˆit = arg mini LCBt(mt  i) for STA-
BLEOPT. Here  LCBt represents a lower conﬁdence bound on f (see [4] for details).
In Figure 3a  we show the average regret of the algorithms when the adversary chooses users uniformly
at random at every t. In our second experiment (Figure 3b)  we show their performance when the adver-
sary is adaptive and selects it according to the HEDGE algorithm. We observe that in both experiments
GP-MW is no-regret  while the average regrets of both GP-UCB and STABLEOPT do not vanish.

5 Conclusions

We have proposed GP-MW  a no-regret bandit algorithm for playing unknown repeated games. In
addition to the standard bandit feedback  the algorithm requires observing the actions of other players
after every round of the game. By exploiting the correlation among different game outcomes  it com-
putes upper conﬁdence bounds on the rewards and uses them to simulate unavailable full information
feedback. Our algorithm attains high probability regret bounds that can substantially improve upon
the existing bandit regret bounds. In our experiments  we have demonstrated the effectiveness of
GP-MW on synthetic games  and real-world problems of trafﬁc routing and movie recommendation.

8

Acknowledgments
This work was gratefully supported by Swiss National Science Foundation  under the grant SNSF
200021_172781  and by the European Union’s Horizon 2020 ERC grant 815943.

References
[1] Transportation network test problems. http://www.bgu.ac.il/ bargera/tntp/.

[2] Yasin Abbasi-Yadkori. Online Learning for Linearly Parametrized Control Problems. PhD

thesis  Edmonton  Alta.  Canada  2012.

[3] Peter Auer  Nicolò Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic

multiarmed bandit problem. SIAM J. Comput.  32(1):48–77  January 2003.

[4] Ilija Bogunovic  Jonathan Scarlett  Stefanie Jegelka  and Volkan Cevher. Adversarially robust
optimization with gaussian processes. In Neural Information Processing Systems (NeurIPS) 
2018.

[5] Ilija Bogunovic  Jonathan Scarlett  Andreas Krause  and Volkan Cevher. Truncated variance
reduction: A uniﬁed approach to bayesian optimization and level-set estimation. In Neural
Information Processing Systems (NeurIPS)  2016.

[6] Sébastien Bubeck  Yin Tat Lee  and Ronen Eldan. Kernel-based methods for bandit convex
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of

optimization.
Computing  STOC 2017  pages 72–85  2017.

[7] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge

University Press  New York  NY  USA  2006.

[8] Archie C. Chapman  David S. Leslie  Alex Rogers  and Nicholas R. Jennings. Convergent
learning algorithms for unknown reward games. SIAM J. Control and Optimization  51(4):3154–
3180  2013.

[9] Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International

Conference on Machine Learning (ICML)  2017.

[10] Itay P. Fainmesser. Community structure and market outcomes: A repeated games-in-networks

approach. American Economic Journal: Microeconomics  4(1):32–69  February 2012.

[11] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and
an application to boosting. Journal of Computer and System Sciences  55(1):119 – 139  1997.

[12] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM

Trans. Interact. Intell. Syst.  5(4):19:1–19:19  December 2015.

[13] Elad Hazan  Karan Singh  and Cyril Zhang. Efﬁcient regret minimization in non-convex games.

In International Conference on Machine Learning (ICML)  2017.

[14] Larry J. Leblanc. An algorithm for the discrete network design problem. Transportation Science 

9:183–199  08 1975.

[15] Matthew O. Jackson and Yves Zenou. Games on networks. In Handbook of Game Theory with

Economic Applications  volume 4  chapter 3  pages 95–163. Elsevier  2015.

[16] Andreas Krause and Cheng S. Ong. Contextual gaussian process bandit optimization. In Neural

Information Processing Systems (NeurIPS). 2011.

[17] N. Littlestone and M.K. Warmuth. The weighted majority algorithm. Information and Compu-

tation  108(2):212 – 261  1994.

[18] Odalric-Ambrym Maillard and Rémi Munos. Online learning in adversarial lipschitz envi-
ronments. In Machine Learning and Knowledge Discovery in Databases  pages 305–320 
2010.

9

[19] Dov Monderer and Lloyd S. Shapley. Potential games. Games and Economic Behavior 

14(1):124 – 143  1996.

[20] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine

Learning (Adaptive Computation and Machine Learning). The MIT Press  2005.

[21] Brian Skyrms and Robin Pemantle. A dynamic model of social network formation. Adaptive

Networks: Theory  Models and Applications  pages 231–251  2009.

[22] Aleksandrs Slivkins. Contextual bandits with similarity information. Journal of Machine

Learning Research  15:2533–2568  2014.

[23] Niranjan Srinivas  Andreas Krause  Sham Kakade  and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. In International Conference
on Machine Learning (ICML)  2010.

[24] Vasilis Syrgkanis  Alekh Agarwal  Haipeng Luo  and Robert E. Schapire. Fast convergence of

regularized learning in games. In Neural Information Processing Systems (NeurIPS)  2015.

[25] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

In International Conference on Machine Learning (ICML)  2003.

10

,Pier Giuseppe Sessa
Ilija Bogunovic
Maryam Kamgarpour
Andreas Krause