2012,Scalable nonconvex inexact proximal splitting,We study large-scale  nonsmooth  nonconconvex optimization problems. In particular  we focus on nonconvex problems with \emph{composite} objectives. This class of problems includes the extensively studied convex  composite objective problems as a special case. To tackle composite nonconvex problems  we introduce a powerful new framework based on asymptotically \emph{nonvanishing} errors  avoiding the common convenient assumption of eventually vanishing errors. Within our framework we derive both batch and incremental nonconvex proximal splitting algorithms. To our knowledge  our framework is first to develop and analyze incremental \emph{nonconvex} proximal-splitting algorithms  even if we disregard the ability to handle nonvanishing errors. We illustrate our theoretical framework by showing how it applies to difficult large-scale  nonsmooth  and nonconvex problems.,Scalable nonconvex inexact proximal splitting

Suvrit Sra

Max Planck Institute for Intelligent Systems

72076 T¨ubigen  Germany
suvrit@tuebingen.mpg.de

Abstract

We study a class of large-scale  nonsmooth  and nonconvex optimization prob-
lems. In particular  we focus on nonconvex problems with composite objectives.
This class includes the extensively studied class of convex composite objective
problems as a subclass. To solve composite nonconvex problems we introduce a
powerful new framework based on asymptotically nonvanishing errors  avoiding
the common stronger assumption of vanishing errors. Within our new framework
we derive both batch and incremental proximal splitting algorithms. To our knowl-
edge  our work is ﬁrst to develop and analyze incremental nonconvex proximal-
splitting algorithms  even if we were to disregard the ability to handle nonvanish-
ing errors. We illustrate one instance of our general framework by showing an
application to large-scale nonsmooth matrix factorization.

Introduction

1
This paper focuses on nonconvex composite objective problems having the form

minimize Φ(x) := f (x) + h(x)

x ∈ X  

(1)
where f : Rn → R is continuously differentiable  h : Rm → R ∪ {∞} is lower semi-continuous
(lsc) and convex (possibly nonsmooth)  and X is a compact convex set. We also make the common
assumption that ∇f is locally (in X ) Lipschitz continuous  i.e.  there is a constant L > 0 such that
(2)

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107)

for all x  y ∈ X .

Problem (1) is a natural but far-reaching generalization of composite objective convex problems 
which enjoy tremendous importance in machine learning; see e.g.  [2  3  11  34]. Although  convex
formulations are extremely useful  for many difﬁcult problems a nonconvex formulation is natu-
ral. Familiar examples include matrix factorization [20  23]  blind deconvolution [19]  dictionary
learning [18  23]  and neural networks [4  17].
The primary contribution of this paper is theoretical. Speciﬁcally  we present a new algorithmic
framework: Nonconvex Inexact Proximal Splitting (NIPS). Our framework solves (1) by “splitting”
the task into smooth (gradient) and nonsmooth (proximal) parts. Beyond splitting  the most notable
feature of NIPS is that it allows computational errors. This capability proves critical to obtaining
a scalable  incremental-gradient variant of NIPS  which  to our knowledge  is the ﬁrst incremental
proximal-splitting method for nonconvex problems.
NIPS further distinguishes itself in how it models computational errors. Notably  it does not require
the errors to vanish in the limit  which is a more realistic assumption as often one has limited to no
control over computational errors inherent to a complex system. In accord with the errors  NIPS also
does not require stepsizes (learning rates) to shrink to zero. In contrast  most incremental-gradient
methods [5] and stochastic gradient algorithms [16] do assume that the computational errors and
stepsizes decay to zero. We do not make these simplifying assumptions  which complicates the
convergence analysis a bit  but results in perhaps a more satisfying description.

1

Our analysis builds on the remarkable work of Solodov [29]  who studied the simpler setting of
differentiable nonconvex problems (which correspond with h ≡ 0 in (1)). NIPS is strictly more
general: unlike [29] it solves a non-differentiable problem by allowing a nonsmooth regularizer
h (cid:54)≡ 0  and this h is tackled by invoking proximal-splitting [8].
Proximal-splitting has proved to be exceptionally fruitful and effective [2  3  8  11]. It retains the
simplicity of gradient-projection while handling the nonsmooth regularizer h via its proximity op-
erator. This approach is especially attractive because for several important choices of h  efﬁcient
implementations of the associated proximity operators exist [2  22  23]. For convex problems  an
alternative to proximal splitting is the subgradient method; similarly  for nonconvex problems one
may use a generalized subgradient method [7  12]. However  as in the convex case  the use of sub-
gradients has drawbacks: it fails to exploit the composite structure  and even when using sparsity
promoting regularizers it does not generate intermediate sparse iterates [11].
Among batch nonconvex splitting methods  an early paper is [14]. More recently  in his pioneering
paper on convex composite minimization  Nesterov [26] also brieﬂy discussed nonconvex problems.
Both [14] and [26]  however  enforced monotonic descent in the objective value to ensure conver-
gence. Very recently  Attouch et al. [1] have introduced a generic method for nonconvex nonsmooth
problems based on Kurdyka-Łojasiewicz theory  but their entire framework too hinges on descent.
A method that uses nonmontone line-search to eliminate dependence on strict descent is [13].
In general  the insistence on strict descent and exact gradients makes many of the methods unsuitable
for incremental  stochastic  or online variants  all of which usually lead to a nonmonotone objective
values especially due to inexact gradients. Among nonmonotonic methods that apply to (1)  we are
aware of the generalized gradient-type algorithms of [31] and the stochastic generalized gradient
methods of [12]. Both methods  however  are analogous to the usual subgradient-based algorithms
that fail to exploit the composite objective structure  unlike proximal-splitting methods.
But proximal-splitting methods do not apply out-of-the-box to (1): nonconvexity raises signiﬁcant
obstructions  especially because nonmonotonic descent in the objective function values is allowed
and inexact gradient might be used. Overcoming these obstructions to achieve a scalable non-descent
based method that allows inexact gradients is what makes our NIPS framework novel.

2 The NIPS Framework
To simplify presentation  we replace h by the penalty function
g(x) := h(x) + δ(x|X ) 

(3)
where δ(·|X ) is the indicator function for X : δ(x|X ) = 0 for x ∈ X   and δ(x|X ) = ∞ for x (cid:54)∈ X .
With this notation  we may rewrite (1) as the unconstrained problem:

minx∈Rn Φ(x) := f (x) + g(x) 

(4)
and this particular formulation is our primary focus. We solve (4) via a proximal-splitting approach 
so let us begin by deﬁning our most important component.
Deﬁnition 1 (Proximity operator). Let g : Rn → R be an lsc  convex function. The proximity
operator for g  indexed by η > 0  is the nonlinear map [see e.g.  28; Def. 1.22]:

(5)

P g

η :

y (cid:55)→ argmin
x∈Rn

(cid:0)g(x) + 1

2η(cid:107)x − y(cid:107)2(cid:1).

The operator (5) was introduced by Moreau [24] (1962) as a generalization of orthogonal projec-
tions. It is also key to Rockafellar’s classic proximal point algorithm [27]  and it arises in a host of
proximal-splitting methods [2  3  8  11]  most notably in forward-backward splitting (FBS) [8].
FBS is particularly attractive because of its simplicity and algorithmic structure. It minimizes convex
composite objective functions by alternating between “forward” (gradient) steps and “backward”
(proximal) steps. Formally  suppose f in (4) is convex; for such f  FBS performs the iteration

(6)
where {ηk} is a suitable sequence of stepsizes. The usual convergence analysis of FBS is intimately
tied to convexity of f. Therefore  to tackle nonconvex f we must take a different approach. As

xk+1 = P g
ηk

k = 0  1  . . .  

(xk − ηk∇f (xk)) 

2

previously mentioned  such approaches were considered by Fukushima and Mine [14] and Nesterov
[26]  but both proved convergence by enforcing monotonic descent.
This insistence on descent severely impedes scalability. Thus  the key challenge is: how to retain
the algorithmic simplicity of FBS and allow nonconvex losses  without sacriﬁcing scalability?
We address this challenge by introducing the following inexact proximal-splitting iteration:

(xk − ηk∇f (xk) + ηke(xk)) 

xk+1 = P g
ηk

(7)
where e(xk) models the computational errors in computing the gradient ∇f (xk). We also assume
that for η > 0 smaller than some stepsize ¯η  the computational error is uniformly bounded  that is 
(8)

for some ﬁxed error level ¯ ≥ 0 

η(cid:107)e(x)(cid:107) ≤ ¯ 

and ∀x ∈ X .

k = 0  1  . . .  

(cid:88)

Condition (8) is weaker than the typical vanishing error requirements

η(cid:107)e(xk)(cid:107) < ∞ 

k

k→∞ η(cid:107)e(xk)(cid:107) = 0 

lim

which are stipulated by most analyses of methods with gradient errors [4  5]. Obviously  since errors
are nonvanishing  exact stationarity cannot be guaranteed. We will  however  show that the iterates
produced by (7) do progress towards reasonable inexact stationary points. We note in passing that
even if we assume the simpler case of vanishing errors  NIPS is still the ﬁrst nonconvex proximal-
splitting framework that does not insist on monotonicity  which complicates convergence analysis
but ultimately proves crucial to scalability.

Algorithm 1 Inexact Nonconvex Proximal Splitting (NIPS)
Input: Operator P g

η   and a sequence {ηk} satisfying
c ≤ lim inf k ηk 
Output: Approximate solution to (7)

lim supk ηk ≤ min{1  2/L − c}  

0 < c < 1/L.

(9)

k ← 0; Select arbitrary x0 ∈ X
while ¬ converged do

Compute approximate gradient (cid:101)∇f (xk) := ∇f (xk) − e(xk)

ηk (xk − ηk(cid:101)∇f (xk))

Update: xk+1 = P g
k ← k + 1

end while

2.1 Convergence analysis
We begin by characterizing inexact stationarity. A point x∗ is a stationary point for (4) if and only if
it satisﬁes the inclusion

(10)
where ∂Cφ denotes the Clarke subdifferential [7]. A brief exercise shows that this inclusion may be
equivalently recast as the ﬁxed-point equation (which augurs the idea of proximal-splitting)

0 ∈ ∂CΦ(x∗) := ∇f (x∗) + ∂g(x∗) 

x∗ = P g

η (x∗ − η∇f (x∗)) 

for η > 0.

(11)

This equation helps us deﬁne a measure of inexact stationarity: the proximal residual

(12)
Note that for an exact stationary point x∗ the residual norm (cid:107)ρ(x∗)(cid:107) = 0. Thus  we call a point x to
be -stationary if for a prescribed error level (x)  the corresponding residual norm satisﬁes

ρ(x) := x − P g

1 (x − ∇f (x)).

(cid:107)ρ(x)(cid:107) ≤ (x).

(13)
Assuming the error-level (x) (say if ¯ = lim supk (xk)) satisﬁes the bound (8)  we prove below

that the iterates(cid:8)xk(cid:9) generated by (7) satisfy an approximate stationarity condition of the form (13) 

by allowing the stepsize η to become correspondingly small (but strictly bounded away from zero).
We start by recalling two basic facts  stated without proof as they are standard knowledge.

3

Lemma 2 (Lipschitz-descent [see e.g.  25; Lemma 2.1.3]). Let f ∈ C 1

|f (x) − f (y) − (cid:104)∇f (y)  x − y(cid:105)| ≤ L

2 (cid:107)x − y(cid:107)2 

Lemma 3 (Nonexpansivity [see e.g.  9; Lemma 2.4]). The operator P g
∀ x  y ∈ Rn.

η (y)(cid:107) ≤ (cid:107)x − y(cid:107) 

η (x) − P g

(cid:107)P g

L(X ). Then 
∀ x  y ∈ X .
η is nonexpansive  that is 

(14)

(15)

Next we prove a crucial monotonicity property that actually subsumes similar results for projection
operators derived by Gafni and Bertsekas [15; Lem. 1]  and may therefore be of independent interest.
Lemma 4 (Prox-Monotonicity). Let y  z ∈ Rn  and η > 0. Deﬁne the functions

pg(η)

:= 1

η(cid:107)P g

η (y − ηz) − y(cid:107) 

and qg(η)

:= (cid:107)P g

η (y − ηz) − y(cid:107).

(16)

Then  pg(η) is a decreasing function of η  and qg(η) an increasing function of η.

Proof. Our proof exploits properties of Moreau-envelopes [28; pp. 19 52]  and we present it in the
language of proximity operators. Consider the “deﬂected” proximal objective

mg(x  η; y  z) := (cid:104)z  x − y(cid:105) + 1

2η(cid:107)x − y(cid:107)2 + g(x) 

for some y  z ∈ X .

(17)

Associate to objective mg the deﬂected Moreau-envelope

Eg(η) := inf

x∈X mg(x  η; y  z) 

(18)
η (y − ηz). Thus  Eg(η) is differentiable  and its
whose inﬁmum is attained at the unique point P g
derivative is given by E(cid:48)
2 p(η)2. Since Eg is convex in η 
E(cid:48)
g is increasing ([28; Thm. 2.26])  or equivalently p(η) is decreasing. Similarly  deﬁne ˆeg(γ) :=
Eg(1/γ); this function is concave in γ as it is a pointwise inﬁmum (indexed by x) of functions linear
in γ [see e.g.  §3.2.3 in 6]. Thus  its derivative ˆe(cid:48)
1/γ(x − γ−1y) − x(cid:107)2 = qg(1/γ)  is a
decreasing function of γ. Set η = 1/γ to conclude the argument about qg(η).

η (y − ηz) − y(cid:107)2 = − 1

g(η) = − 1

2η2(cid:107)P g

2(cid:107)P g

g(γ) = 1

We now proceed to bound the difference between objective function values from iteration k to k + 1 
by developing a bound of the form

(19)
Obviously  since we do not enforce strict descent  h(xk) may be negative too. However  we show
that for sufﬁciently large k the algorithm makes enough progress to ensure convergence.
Lemma 5. Let xk+1  xk  ηk  and X be as in (7)  and that ηk(cid:107)e(xk)(cid:107) ≤ (xk) holds. Then 

Φ(xk) − Φ(xk+1) ≥ h(xk).

Φ(xk) − Φ(xk+1) ≥ 2−Lηk

(cid:107)xk+1 − xk(cid:107)2 − 1

(xk)(cid:107)xk+1 − xk(cid:107).

2ηk

ηk

Proof. For the deﬂected Moreau envelope (17)  consider the directional derivative dmg with respect
to x in the direction w; at x = xk+1  this derivative satisﬁes the optimality condition

dmg(xk+1  η; y  z)(w) = (cid:104)z + η−1(xk+1 − y) + sk+1  w(cid:105) ≥ 0 

sk+1 ∈ ∂g(xk+1).

Set z = ∇f (xk) − e(xk)  y = xk  and w = xk − xk+1 in (21)  and rearrange to obtain
(cid:104)∇f (xk) − e(xk)  xk+1 − xk(cid:105) ≤ (cid:104)η−1(xk+1 − xk) + sk+1  xk − xk+1(cid:105).

(20)

(21)

(22)

(23)

From Lemma 2 it follows that

Φ(xk+1) ≤ f (xk) + (cid:104)∇f (xk)  xk+1 − xk(cid:105) + L

2 (cid:107)xk+1 − xk(cid:107)2 + g(xk+1) 

whereby upon adding and subtracting e(xk)  and then using (22) we further obtain

f (xk) + (cid:104)∇f (xk) − e(xk)  xk+1 − xk(cid:105) + L

≤ f (xk) + g(xk+1) + (cid:104)sk+1  xk − xk+1(cid:105) +(cid:0) L

2 (cid:107)xk+1 − xk(cid:107)2 + g(xk+1) + (cid:104)e(xk)  xk+1 − xk(cid:105)

(cid:1)(cid:107)xk+1 − xk(cid:107)2 + (cid:104)e(xk)  xk+1 − xk(cid:105)

2 − 1

ηk

(cid:107)xk+1 − xk(cid:107)2 + (cid:104)e(xk)  xk+1 − xk(cid:105)

≤ f (xk) + g(xk) − 2−Lηk
≤ Φ(xk) − 2−Lηk
≤ Φ(xk) − 2−Lηk

2ηk

2ηk

2ηk

(cid:107)xk+1 − xk(cid:107)2 + (cid:107)e(xk)(cid:107)(cid:107)xk+1 − xk(cid:107)
(xk)(cid:107)xk+1 − xk(cid:107).
(cid:107)xk+1 − xk(cid:107)2 + 1

ηk

The second inequality above follows from convexity of g  the third one from Cauchy-Schwarz  and
the last one by assumption on (xk). Now ﬂip signs and apply (23) to conclude the bound (20).

4

Next we further bound (20) by deriving two-sided bounds on (cid:107)xk+1 − xk(cid:107).
Lemma 6. Let xk+1  xk  and (xk) be as before; also let c and ηk satisfy (9). Then 

c(cid:107)ρ(xk)(cid:107) − (xk) ≤ (cid:107)xk+1 − xk(cid:107) ≤ (cid:107)ρ(xk)(cid:107) + (xk).

(24)

Proof. First observe that from Lemma 4 that for ηk > 0 it holds that

if 1 ≤ ηk then q(1) ≤ qg(ηk) 

and if ηk ≤ 1 then pg(1) ≤ pg(ηk) = 1

ηk

qg(ηk).

(25)

Using (25)  the triangle inequality  and Lemma 3  we have

min{1  ηk} qg(1) = min{1  ηk}(cid:107)ρ(xk)(cid:107) ≤ (cid:107)P g
≤ (cid:107)xk+1 − xk(cid:107) + (cid:107)xk+1 − P g
≤ (cid:107)xk+1 − xk(cid:107) + (cid:107)ηke(xk)(cid:107) ≤ (cid:107)xk+1 − xk(cid:107) + (xk).

(xk − ηk∇f (xk)) − xk(cid:107)
(xk − ηk∇f (xk))(cid:107)

ηk

ηk

From (9) it follows that for sufﬁciently large k we have (cid:107)xk+1 − xk(cid:107) ≥ c(cid:107)ρ(xk)(cid:107) − (xk). For the
upper bound note that

(cid:107)xk+1 − xk(cid:107) ≤ (cid:107)xk − P g

(xk − ηk∇f (xk))(cid:107) + (cid:107)P g

(xk − ηk∇f (xk)) − xk+1(cid:107)

ηk

ηk

≤ max{1  ηk}(cid:107)ρ(xk)(cid:107) + (cid:107)ηke(xk)(cid:107) ≤ (cid:107)ρ(xk)(cid:107) + (xk).

Lemma 5 and Lemma 6 help prove the following crucial corollary.
Corollary 7. Let xk  xk+1  ηk  and c be as above and k sufﬁciently large so that c and ηk satisfy (9).
Then  Φ(xk) − Φ(xk+1) ≥ h(xk) holds with h(xk) given by

2(2−2Lc)(cid:107)ρ(xk)(cid:107)2 −(cid:0) L2c2

2−cL + 1

c

(cid:1)(cid:107)ρ(xk)(cid:107)(xk) −(cid:0) 1

h(xk) := L2c3

(cid:1)(xk)2.

c − L2c
2(2−cL)

(26)

Proof. Plug in the bounds (24) into (20)  invoke (9)  and simplify—see [32] for details.

We now have all the ingredients to state the main convergence theorem.
Theorem 8 (Convergence). Let f ∈ C 1

Let(cid:8)xk(cid:9) ⊂ X be a sequence generated by (7)  and let condition (8) on each (cid:107)e(xk)(cid:107) hold. There
exists a limit point x∗ of the sequence(cid:8)xk(cid:9)  and a constant K > 0  such that (cid:107)ρ(x∗)(cid:107) ≤ K(x∗).
If(cid:8)Φ(xk)(cid:9) converges  then for every limit point x∗ of(cid:8)xk(cid:9) it holds that (cid:107)ρ(x∗)(cid:107) ≤ K(x∗).

L(X ) such that infX f > −∞ and let g be lsc  convex on X .

Proof. Lemma 5  6  and Corollary 7 have done all the hard work. Indeed  they allow us to reduce
our convergence proof to the case where the analysis of the differentiable case becomes applicable 
and an appeal to the analysis of [29; Thm. 2.1] grants us our claim.

Theorem 8 says that we can obtain an approximate stationary point for which the norm of the resid-
ual is bounded by a linear function of the error level. The statement of the theorem is written in a
conditional form  because nonvanishing errors e(x) prevent us from making a stronger statement.
In particular  once the iterates enter a region where the residual norm falls below the error thresh-

old  the behavior of(cid:8)xk(cid:9) may be arbitrary. This  however  is a small price to pay for having the

added ﬂexibility of nonvanishing errors. Under the stronger assumption of vanishing errors (and
diminishing stepsizes)  we can also obtain guarantees to exact stationary points.
3 Scaling up NIPS: incremental variant
We now apply NIPS to the large-scale setting  where we have composite objectives of the form

(27)
Φ(x) :=
(X ) function. For simplicity  we use L = maxt Lt in the sequel.
where each ft : Rn → R is a C 1
It is well-known that for such decomposable objectives it can be advantageous to replace the full

t ∇ft(x) by an incremental gradient ∇fσ(t)(x)  where σ(t) is some suitable index.

gradient(cid:80)

ft(x) + g(x) 

t=1

Lt

(cid:88)T

5

Nonconvex incremental methods for differentiable problems have been extensively analyzed  e.g. 
backpropagation algorithms [5  29]  which correspond to g(x) ≡ 0. However  when g(x) (cid:54)= 0  the
only incremental methods that we are aware of are stochastic generalized gradient methods of [12]
or the generalized gradient methods of [31]. As previously mentioned  both of these fail to exploit
the composite structure of the objective function  a disadvantage even in the convex case [11].
In stark contrast  we do exploit the composite structure of (27). Formally  we propose the following
incremental nonconvex proximal-splitting iteration:

xk+1 = M(cid:0)xk − ηk

(cid:88)T

∇ft(xk t)(cid:1) 

xk 1 = xk  xk t+1 = O(xk t − ηk∇ft(xk t)) 

t=1

k = 0  1  . . .  
t = 1  . . .   T − 1 

(28)

where O and M are appropriate operators  different choices of which lead to different algorithms.
For example  when X = Rn  g(x) ≡ 0  M = O = Id  and ηk → 0  then (28) reduces to the classic
incremental gradient method (IGM) [4]  and to the IGM of [30]  if lim ηk = ¯η > 0. If X a closed
convex set  g(x) ≡ 0  M is orthogonal projection onto X   O = Id  and ηk → 0  then iteration (28)
reduces to (projected) IGM [4  5].
We may consider four variants of (28) in Table 1; to our knowledge  all of these are new. Which
of the four variants one prefers depends on the complexity of the constraint set X and cost to apply
P g
η . The analysis of all four variants is similar  so we present details only for the most general case.
X
Rn
Rn

Penalty and constraints
penalized  unconstrained
penalized  unconstrained
penalized  constrained
penalized  constrained

Proximity operator calls
once every major (k) iteration
once every minor (k  t) iteration
once every major (k) iteration
once every minor (k  t) iteration

g
(cid:54)≡ 0
(cid:54)≡ 0

M O
P g
Id
η
η P g
P g
h(x) + δ(X|x) P g
η
Id
h(x) + δ(X|x) P g
η
η P g
η

Convex
Convex

Table 1: Different variants of incremental NIPS (28).

3.1 Convergence analysis
Speciﬁcally  we analyze convergence for the case M = O = P g
case treated by [30]. We begin by rewriting (28) in a form that matches the main iteration (7):

η by generalizing the differentiable

(cid:0)xk − ηk
(cid:0)xk − ηk
(cid:0)xk − ηk

(cid:88)T
(cid:88)T
(cid:88)

t

∇ft(xk t)(cid:1)
(cid:2)(cid:88)T
∇ft(xk) + ηke(xk)(cid:1).

∇ft(xk) + ηk

t=1

t=1

t=1

xk+1 = P g
η

= P g
η

= P g
η

ft(xk) − ft(xk t)(cid:3)(cid:1)

(29)

To show that iteration (29) is well-behaved and actually ﬁts the main NIPS iteration (7)  we must
ensure that the norm of the error term is bounded. We show this via a sequence of lemmas.
Lemma 9 (Bounded-increment). Let xk t+1 be computed by (28)  and let st ∈ ∂g(xk t). Then 

(cid:107)xk t+1 − xk t(cid:107) ≤ 2ηk(cid:107)∇ft(xk t) + st(cid:107).

(30)

Proof. From the deﬁnition of a proximity operator (5)  we have the inequality

1

2(cid:107)xk t+1 − xk t + ηk∇ft(xk t)(cid:107)2 + ηkg(xk t+1) ≤ 1
2(cid:107)xk t+1 − xk t(cid:107)2 ≤ ηk(cid:104)∇ft(xk t)  xk t − xk t+1(cid:105) + ηk(g(xk t) − g(xk t+1)).

2(cid:107)ηk∇ft(xk t)(cid:107)2 + ηkg(xk t) 

=⇒ 1

Since st ∈ ∂g(xk t)  we have g(xk t+1) ≥ g(xk t) + (cid:104)st  xk t+1 − xk t(cid:105). Therefore 
2(cid:107)xk t+1 − xk t(cid:107)2 ≤ ηk(cid:104)st  xk t − xk t+1(cid:105) + (cid:104)∇ft(xk t)  xk t − xk t+1(cid:105)

1

≤ ηk(cid:107)st + ∇ft(xk t)(cid:107)(cid:107)xk t − xk t+1(cid:107)
=⇒ (cid:107)xk t+1 − xk t(cid:107) ≤ 2ηk(cid:107)∇ft(xk t) + st(cid:107).

Lemma 9 proves helpful in bounding the overall error.

6

Lemma 10 (Bounded error). If for all xk ∈ X   (cid:107)∇ft(xk)(cid:107) ≤ M and (cid:107)∂g(xk)(cid:107) ≤ G  then there
exists a constant K1 > 0 such that (cid:107)e(xk)(cid:107) ≤ K1.

(cid:88)t−1

t := (cid:107)∇ft(xk t) − ∇ft(xk)(cid:107) 

Proof. To bound the error of using xk t instead of xk ﬁrst deﬁne the term
t = 1  . . .   T.
Then  an inductive argument (see [32] for details) shows that for 2 ≤ t ≤ T
(1 + 2ηkL)t−1−j(cid:107)∇fj(xk) + sj(cid:107).

Since (cid:107)e(xk)(cid:107) =(cid:80)T
(cid:88)T−1
(cid:88)T
(cid:88)T
(1 + 2ηkL)T−tβt ≤ (1 + 2ηkL)T−1(cid:88)T−1
≤(cid:88)T−1

t=1 t  and 1 = 0  (32) then leads to the bound

(1 + 2ηkL)t−1−jβj = 2ηkL

(cid:88)t−1

t ≤ 2ηkL

t ≤ 2ηkL

j=1

j=1

t=2

t=2

t=1

≤ C1(T − 1)(M + G) =: K1.

(31)

(32)

(cid:16)(cid:88)T−t−1

(1 + 2ηkL)j(cid:17)

j=0

βt
(cid:107)∇ft(x) + st(cid:107)

t=1

t=1

Thus  the error norm (cid:107)e(xk)(cid:107) is bounded from above by a constant  whereby it satisﬁes the require-
ment (8)  making the incremental NIPS method (28) a special case of the general NIPS framework.
This allows us to invoke the convergence result Theorem 8 for without further ado.
4
The main contribution of our paper is the new NIPS framework  and a speciﬁc application is not
one of the prime aims of this paper. We do  however  provide an illustrative application of NIPS to
a challenging nonconvex problem: sparsity regularized low-rank matrix factorization

Illustrative application

2(cid:107)Y − XA(cid:107)2

1

t=1

ψt(at) 

min
X A≥0

F + ψ0(X) +

(33)
where Y ∈ Rm×T   X ∈ Rm×K and A ∈ RK×T   with a1  . . .   aT as its columns. Problem (33)
generalizes the well-known nonnegative matrix factorization (NMF) problem of [20] by permitting
arbitrary Y (not necessarily nonnegative)  and adding regularizers on X and A. A related class of
problems was studied in [23]  but with a crucial difference: the formulation in [23] does not allow
nonsmooth regularizers on X. The class of problems studied in [23] is in fact a subset of those cov-
ered by NIPS. On a more theoretical note  [23] considered stochastic-gradient like methods whose
analysis requires computational errors and stepsizes to vanish  whereas our method is deterministic
and allows nonvanishing stepsizes and errors.
Following [23] we also rewrite (33) in a form more amenable to NIPS. We eliminate A and consider

(cid:88)T

minX φ(X) :=

ft(X) + g(X)  where

g(X) := ψ0(X) + δ(X|≥ 0) 

(34)

(cid:88)T

t=1

and where each ft(X) for 1 ≤ t ≤ T is deﬁned as

2(cid:107)yt − Xa(cid:107)2 + gt(a) 

1

ft(X) := mina

(35)
where gt(a) := ψt(a) + δ(a|≥ 0). For simplicity  assume that (35) attains its unique1 minimum 
say a∗  then ft(X) is differentiable and we have ∇X ft(X) = (Xa∗ − yt)(a∗)T . Thus  we can
instantiate (28)  and all we need is a subroutine for solving (35).2
We present empirical results on the following two variants of (34): (i) pure unpenalized NMF (ψt ≡
0 for 0 ≤ t ≤ T ) as a baseline; and (ii) sparsity penalized NMF where ψ0(X) ≡ λ(cid:107)X(cid:107)1 and
ψt(at) ≡ γ(cid:107)at(cid:107)1. Note that without the nonnegativity constraints  (34) is similar to sparse-PCA.
We use the following datasets and parameters:

(cid:74)i(cid:75) RAND: 4000 × 4000 dense random (uniform
[0  1]); rank-32 factorization; (λ  γ) = (10−5  10);(cid:74)ii(cid:75) CBCL: CBCL database [33]; 361 × 2429;
rank-49 factorization;(cid:74)iii(cid:75) YALE: Yale B Database [21]; 32256×2414 matrix; rank-32 factorization;
(cid:74)iv(cid:75) WEB: Web graph from google; sparse 714545 × 739454 (empty rows and columns removed)

matrix; ID: 2301 in the sparse matrix collection [10]); rank-4 factorization; (λ = γ = 10−6).

1Otherwise  at the expense of more notation  we can add a small strictly convex perturbation to ensure

uniqueness; this perturbation can be then absorbed into the overall computational error.

2In practice  it is better to use mini-batches  and we used the same sized mini-batches for all the algorithms.

7

Figure 1: Running times of NIPS (Matlab) versus SPAMS (C++) for NMF on RAND  CBCL  and YALE
datasets. Initial objective values and tiny runtimes have been suppressed for clarity of presentation.

On the NMF baseline (Fig. 1)  we compare NIPS against the well optimized state-of-the-art C++
toolbox SPAMS (version 2.3) [23]. We compare against SPAMS only on dense matrices  as its NMF
code seems to be optimized for this case. Obviously  the comparison is not fair: unlike SPAMS 
NIPS and its subroutines are all implemented in MATLAB  and they run equally easily on large
sparse matrices. Nevertheless  NIPS proves to be quite competitive: Fig. 1 shows that our MATLAB
implementation runs only slightly slower than SPAMS. We expect a well-tuned C++ implementation
of NIPS to run at least 4–10 times faster than the MATLAB version—the dashed line in the plots
visualizes what such a mere 3X-speedup to NIPS might mean.
Figure 2 shows numerical results comparing the stochastic generalized gradient (SGGD) algorithm
of [12] against NIPS  when started at the same point. As in well-known  SGGD requires careful
stepsize tuning; so we searched over a range of stepsizes  and have reported the best results. NIPS
too requires some stepsize tuning  but substantially lesser than SGGD. As predicted  the solutions
returned by NIPS have objective function values lower than SGGD  and have greater sparsity.

Figure 2: Sparse NMF: NIPS versus SGGD. The bar plots show the sparsity (higher is better) of the factors
X and A. Left plots for RAND dataset; right plots for WEB. As expected  SGGD yields slightly worse objective
function values and less sparse solutions than NIPS.

5 Discussion

We presented a new framework called NIPS  which solves a broad class of nonconvex composite
objective problems. NIPS permits nonvanishing computational errors  which can be practically use-
ful. We specialized NIPS to also obtain a scalable incremental version. Our numerical experiments
on large scale matrix factorization indicate that NIPS is competitive with state-of-the-art methods.
We conclude by mentioning that NIPS includes numerous other algorithms as special cases. For ex-
ample  batch and incremental convex FBS  convex and nonconvex gradient projection  the proximal-
point algorithm  among others. Theoretically  however  the most exciting open problem resulting
from this paper is: extend NIPS in a scalable way when even the nonsmooth part is nonconvex.
This case will require very different convergence analysis  and is left to the future.

References
[1] H. Attouch  J. Bolte  and B. F. Svaiter. Convergence of descent methods for semi-algebraic and tame prob-
lems: proximal algorithms  forward-backward splitting  and regularized Gauss-Seidel methods. Math.

8

010203040506070100Running time (seconds)Objective function value  NIPSSPAMS0510152025102.1102.2102.3Running time (seconds)Objective function value  NIPSSPAMS050100150200250300350400102.3102.4102.5102.6102.7102.8Running time (seconds)Objective function value  NIPSSPAMS01020304050607080101102103104Running time (seconds)Objective function value  NIPSSGGD00.10.20.30.40.50.60.70.80.9SGGD−ANIPS−ASGGD−XNIPS−XSparsityProgramming Series A  Aug. 2011. Online First.

[2] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Convex optimization with sparsity-inducing norms. In

S. Sra  S. Nowozin  and S. J. Wright  editors  Optimization for Machine Learning. MIT Press  2011.

[3] A. Beck and M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Prob-

lems. SIAM J. Imgaging Sciences  2(1):183–202  2009.

[4] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  second edition  1999.
[5] D. P. Bertsekas. Incremental Gradient  Subgradient  and Proximal Methods for Convex Optimization: A

Survey. Technical Report LIDS-P-2848  MIT  August 2010.

[6] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  March 2004.
[7] F. H. Clarke. Optimization and nonsmooth analysis. John Wiley & Sons  Inc.  1983.
[8] P. L. Combettes and J.-C. Pesquet. Proximal Splitting Methods in Signal Processing. arXiv:0912.3522v4 

May 2010.

[9] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale

Modeling and Simulation  4(4):1168–1200  2005.

[10] T. A. Davis and Y. Hu. The University of Florida Sparse Matrix Collection. ACM Transactions on

Mathematical Software  2011. To appear.

[11] J. Duchi and Y. Singer. Online and Batch Learning using Forward-Backward Splitting. J. Mach. Learning

Res. (JMLR)  Sep. 2009.

[12] Y. M. Ermoliev and V. I. Norkin. Stochastic generalized gradient method for nonconvex nonsmooth

stochastic optimization. Cybernetics and Systems Analysis  34:196–215  1998.

[13] M. A. T. Figueiredo  R. D. Nowak  and S. J. Wright. Gradient Projection for Sparse Reconstruction:
Application to Compressed Sensing and Other Inverse Problems. IEEE J. Selected Topics in Sig. Proc.  1
(4):586–597  2007.

[14] M. Fukushima and H. Mine. A generalized proximal point algorithm for certain non-convex minimization

problems. Int. J. Systems Science  12(8):989–1000  1981.

[15] E. M. Gafni and D. P. Bertsekas. Two-metric projection methods for constrained optimization. SIAM

Journal on Control and Optimization  22(6):936–964  1984.

[16] A. A. Gaivoronski. Convergence properties of backpropagation for neural nets via theory of stochastic

gradient methods. Part 1. Optimization methods and Software  4(2):117–134  1994.

[17] S. Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall PTR  1st edition  1994.
[18] K. Kreutz-Delgado  J. F. Murray  B. D. Rao  K. Engan  T.-W. Lee  and T. J. Sejnowski. Dictionary

learning algorithms for sparse representation. Neural Computation  15:349–396  2003.

[19] D. Kundur and D. Hatzinakos. Blind image deconvolution. IEEE Signal Processing Magazine  13(3) 

May 1996.

[20] D. D. Lee and H. S. Seung. Algorithms for Nonnegative Matrix Factorization. In NIPS  2000.
[21] K.C. Lee  J. Ho  and D. Kriegman. Acquiring linear subspaces for face recognition under variable lighting.

IEEE Trans. Pattern Anal. Mach. Intelligence  27(5):684–698  2005.

[22] J. Liu and J. Ye. Efﬁcient Euclidean projections in linear time. In ICML  Jun. 2009.
[23] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online Learning for Matrix Factorization and Sparse Coding.

JMLR  11:10–60  2010.

[24] J. J. Moreau. Fonctions convexes duales et points proximaux dans un espace hilbertien. C. R. Acad. Sci.

Paris Sr. A Math.  255:2897–2899  1962.

[25] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer  2004.
[26] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 2007/76 

Universit catholique de Louvain  September 2007.

[27] R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM J. Control and Optimiza-

tion  14  1976.

[28] R. T. Rockafellar and R. J.-B. Wets. Variational analysis. Springer  1998.
[29] M. V. Solodov. Convergence analysis of perturbed feasible descent methods. J. Optimization Theory and

Applications  93(2):337–353  1997.

[30] M. V. Solodov. Incremental gradient algorithms with stepsizes bounded away from zero. Computational

Optimization and Applications  11:23–35  1998.

[31] M. V. Solodov and S. K. Zavriev. Error stability properties of generalized gradient-type algorithms. J.

Optimization Theory and Applications  98(3):663–680  1998.

[32] S. Sra. Nonconvex proximal-splitting: Batch and incremental algorithms. Sep. 2012. arXiv:1109.0258v2.
[33] K.-K. Sung. Learning and Example Selection for Object and Pattern Recognition. PhD thesis  MIT  1996.
[34] L. Xiao. Dual averaging method for regularized stochastic learning and online optimization. In NIPS 

2009.

9

,Fabian Sinz
Anna Stockl
Jan Grewe
Jan Benda
Shuyang Gao
Greg Ver Steeg
Aram Galstyan