2019,Neural Trust Region/Proximal Policy Optimization Attains Globally Optimal Policy,Proximal policy optimization and trust region policy optimization (PPO and TRPO) with actor and critic parametrized by neural networks achieve significant empirical success in deep reinforcement learning. However  due to nonconvexity  the global convergence of PPO and TRPO remains less understood  which separates theory from practice. In this paper  we prove that a variant of PPO and TRPO equipped with overparametrized neural networks converges to the globally optimal policy at a sublinear rate. The key to our analysis is the global convergence of infinite-dimensional mirror descent under a notion of one-point monotonicity  where the gradient and iterate are instantiated by neural networks. In particular  the desirable representation power and optimization geometry induced by the overparametrization of such neural networks allow them to accurately approximate the infinite-dimensional gradient and iterate.,Neural Proximal/Trust Region Policy Optimization

Attains Globally Optimal Policy

Boyi Liu⇤†

Qi Cai⇤‡

Zhuoran Yang§

Zhaoran Wang¶

Abstract

Proximal policy optimization and trust region policy optimization (PPO and
TRPO) with actor and critic parametrized by neural networks achieve signiﬁcant
empirical success in deep reinforcement learning. However  due to nonconvexity 
the global convergence of PPO and TRPO remains less understood  which sepa-
rates theory from practice. In this paper  we prove that a variant of PPO and TRPO
equipped with overparametrized neural networks converges to the globally opti-
mal policy at a sublinear rate. The key to our analysis is the global convergence
of inﬁnite-dimensional mirror descent under a notion of one-point monotonicity 
where the gradient and iterate are instantiated by neural networks.
In particu-
lar  the desirable representation power and optimization geometry induced by the
overparametrization of such neural networks allow them to accurately approxi-
mate the inﬁnite-dimensional gradient and iterate.

1

Introduction

Policy optimization aims to ﬁnd the optimal policy that maximizes the expected total reward through
gradient-based updates. Coupled with neural networks  proximal policy optimization (PPO) [40]
and trust region policy optimization (TRPO) [39] are among the most important workhorses behind
the empirical success of deep reinforcement learning across applications such as games [34] and
robotics [13]. However  the global convergence of policy optimization  including PPO and TRPO 
remains less understood due to multiple sources of nonconvexity  including (i) the nonconvexity of
the expected total reward over the inﬁnite-dimensional policy space and (ii) the parametrization of
both policy (actor) and action-value function (critic) using neural networks  which leads to noncon-
vexity in optimizing their parameters. As a result  PPO and TRPO are only guaranteed to monoton-
ically improve the expected total reward over the inﬁnite-dimensional policy space [23  24  39  40] 
while the global optimality of the attained policy  the rate of convergence  as well as the impact
of parametrizing policy and action-value function all remain unclear. Such a gap between theory
and practice hinders us from better diagnosing the possible failure of deep reinforcement learning
[37  19  21] and applying it to critical domains such as healthcare [28] and autonomous driving [38]
in a more principled manner.
Closing such a theory-practice gap boils down to answering three key questions: (i) In the ideal case
that allows for inﬁnite-dimensional policy updates based on exact action-value functions  how do
PPO and TRPO converge to the optimal policy? (ii) When the action-value function is parametrized
by a neural network  how does temporal-difference learning (TD) [41] converge to an approximate
action-value function with sufﬁcient accuracy within each iteration of PPO and TRPO? (iii) When

⇤equal contribution
†Northwestern University; boyiliu2018@u.northwestern.edu
‡Northwestern University; qicai2022@u.northwestern.edu
§Princeton University; zy6@princeton.edu
¶Northwestern University; zhaoranwang@gmail.com

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the policy is parametrized by another neural network  based on the approximate action-value func-
tion attained by TD  how does stochastic gradient descent (SGD) converge to an improved policy
that accurately approximates its ideal version within each iteration of PPO and TRPO? However 
these questions largely elude the classical optimization framework  as questions (i)-(iii) involve non-
convexity  question (i) involves inﬁnite-dimensionality  and question (ii) involves bias in stochastic
(semi)gradients [44  42]. Moreover  the policy evaluation error arising from question (ii) compounds
with the policy improvement error arising from question (iii)  and they together propagate through
the iterations of PPO and TRPO  making the convergence analysis even more challenging.

Contribution. By answering questions (i)-(iii)  we establish the ﬁrst nonasymptotic global rate
of convergence of a variant of PPO (and TRPO) equipped with neural networks.
In detail 
we prove that  with policy and action-value function parametrized by randomly initialized and
overparametrized two-layer neural networks  PPO converges to the optimal policy at the rate of
O(1/pK)  where K is the number of iterations. For solving the subproblems of policy evaluation
and policy improvement within each iteration of PPO  we establish nonasymptotic upper bounds of
the numbers of TD and SGD iterations  respectively. In particular  we prove that  to attain an ✏ accu-
racy of policy evaluation and policy improvement  which appears in the constant of the O(1/pK)
rate of PPO  it sufﬁces to take O(1/✏2) TD and SGD iterations  respectively.
More speciﬁcally  to answer question (i)  we cast the inﬁnite-dimensional policy updates in the ideal
case as mirror descent iterations. To circumvent the lack of convexity  we prove that the expected
total reward satisﬁes a notation of one-point monotonicity [14]  which ensures that the ideal policy
sequence evolves towards the optimal policy. In particular  we show that  in the context of inﬁnite-
dimensional mirror descent  the exact action-value function plays the role of dual iterate  while the
ideal policy plays the role of primal iterate [31  32  36]. Such a primal-dual perspective allows us to
cast the policy evaluation error in question (ii) as the dual error and the policy improvement error in
question (iii) as the primal error. More speciﬁcally  the dual and primal errors arise from using neural
networks to approximate the exact action-value function and the ideal improved policy  respectively.
To characterize such errors in questions (ii) and (iii)  we unify the convergence analysis of TD for
minimizing the mean squared Bellman error (MSBE) [7] and SGD for minimizing the mean squared
error (MSE) [22  27  10  3  54  8  9  26  5]  both over neural networks. In particular  we show that
the desirable representation power and optimization geometry induced by the overparametrization of
neural networks enable the global convergence of both the MSBE and MSE  which correspond to the
dual and primal errors  at a sublinear rate to zero. By incorporating such errors into the analysis of
inﬁnite-dimensional mirror descent  we establish the global rate of convergence of PPO. As a side
product  the proof techniques developed here for handling nonconvexity  inﬁnite-dimensionality 
semigradient bias  and overparametrization may be of independent interest to the analysis of more
general deep reinforcement learning algorithms. In addition  it is worth mentioning that  when the
activation functions of neural networks are linear  our results cover the classical setting with linear
function approximation  which encompasses the classical tabular setting as a special case.

More Related Work. PPO [40] and TRPO [39] are proposed to improve the convergence of vanilla
policy gradient [49  43] in deep reinforcement learning. Related algorithms based on the idea of
KL-regularization include natural policy gradient and actor-critic [23  35]  entropy-regularized pol-
icy gradient and actor-critic [29]  primal-dual actor-critic [12  11]  soft Q-learning and actor-critic
[17  18]  and dynamic policy programming [6]. Despite its empirical success  policy optimization
generally lacks global convergence guarantees due to nonconvexity. One exception is the recent
analysis by [33]  which establishes the global convergence of TRPO to the optimal policy. However 
[33] require inﬁnite-dimensional policy updates based on exact action-value functions and do not
provide the nonasymptotic rate of convergence. In contrast  we allow for the parametrization of
both policy and action-value function using neural networks and provide the nonasymptotic rate of
PPO as well as the iteration complexity of solving the subproblems of policy improvement and pol-
icy evaluation. In particular  based on the primal-dual perspective of reinforcement learning [36]  we
develop a concise convergence proof of PPO as inﬁnite-dimensional mirror descent under one-point
monotonicity  which is of independent interest. In addition  we refer to the closely related concurrent
work [2] for the global convergence analysis of (natural) policy gradient for discrete state and action
spaces as well as continuous state space with linear function approximation. See also the concurrent
work [52]  which studies continuous state space with general function approximation  but only es-

2

tablishes the convergence to a locally optimal policy. In addition  in our companion paper [48]  we
establish the global convergence of neural (natural) policy gradient.

2 Background

In this section  we brieﬂy introduce the general setting of reinforcement learning as well as PPO
and TRPO.

Markov Decision Process. We consider the Markov decision process (S A P  r  )  where S
is a compact state space  A is a ﬁnite action space  P : S⇥S⇥A!
R is the transition kernel 
r : S⇥A ! R is the reward function  and  2 (0  1) is the discount factor. We track the performance
of a policy ⇡ : A⇥S! R using its action-value function (Q-function) Q⇡ : S⇥A! R  which is
deﬁned as

Correspondingly  the state-value function V ⇡ : S! R of a policy ⇡ is deﬁned as

Q⇡(s  a) = (1  ) · E 1Xt=0
V ⇡(s) = (1  ) · E 1Xt=0

t · r(st  at) s0 = s  a0 = a  at ⇠ ⇡(·| st)  st+1 ⇠P (·| st  at).
t · r(st  at) s0 = s  at ⇠ ⇡(·| st)  st+1 ⇠P (·| st  at).

(2.1)
The advantage function A⇡ : S⇥A! R of a policy ⇡ is deﬁned as A⇡(s  a) = Q⇡(s  a) 
V ⇡(s). We denote by ⌫⇡(s) and ⇡(s  a) = ⇡(a| s) · ⌫⇡(s) the stationary state distribution and
the stationary state-action distribution associated with a policy ⇡  respectively. Correspondingly  we
denote by E⇡ [· ] and E⌫⇡ [· ] the expectations E(s a)⇠⇡ [· ] = Ea⇠⇡(· | s) s⇠⌫⇡(·)[· ] and Es⇠⌫⇡ [· ] 
respectively. Meanwhile  we denote by h· ·i the inner product over A  e.g.  we have V ⇡(s) =
Ea⇠⇡(· | s)[Q⇡(s  a)] = hQ⇡(s ·) ⇡ (·| s)i.
PPO and TRPO. At the k-th iteration of PPO  the policy parameter ✓ is updated by

✓k+1 argmax

✓

bE ⇡✓(a| s)
⇡✓k (a| s) · Ak(s  a)  k · KL(⇡✓(·| s)k ⇡✓k (·| s)) 

(2.2)

where Ak is an estimator of A⇡✓k andbE[· ] is taken with respect to the empirical version of ⇡✓k

 
that is  the empirical stationary state-action distribution associated with the current policy ⇡✓k. In
practice  the penalty parameter k is adjusted by line search.
At the k-th iteration of TRPO  the policy parameter ✓ is updated by

bE ⇡✓(a| s)
⇡✓k (a| s) · Ak(s  a) 

✓k+1 argmax

subject to KL(⇡✓(·| s)k ⇡✓k (·| s))   

(2.3)
where  is the radius of the trust region. The PPO update in (2.2) can be viewed as a Lagrangian
relaxation of the TRPO update in (2.3) with Lagrangian multiplier k  which implies their updates
are equivalent if k is properly chosen. Without loss of generality  we focus on PPO hereafter.
It is worth mentioning that  compared with the original versions of PPO [40] and TRPO [39]  the
variants in (2.2) and (2.3) use KL(⇡✓(·| s)k ⇡✓k (·| s)) instead of KL(⇡✓k (·| s)k ⇡✓(·| s)). In Sec-
tions 3 and 4  we show that  as the original versions  such variants also allow us to approximately
obtain the improved policy ⇡✓k+1 using SGD  and moreover  enjoy global convergence.

✓

3 Neural PPO

We present more details of PPO with policy and action-value function parametrized by neural net-
works. For notational simplicity  we denote by ⌫k and k the stationary state distribution ⌫⇡✓k
and
  respectively. Also  we deﬁne an auxiliary distribution
the stationary state-action distribution ⇡✓k

ek over S⇥A asek = ⌫k⇡0.
Neural Network Parametrization. Without loss of generality  we assume that (s  a) 2 Rd for all
s 2S and a 2A . We parametrize a function u : S⇥A! R  e.g.  policy ⇡ or action-value function

3

Q⇡  by the following two-layer neural network  which is denoted by NN(↵; m) 

1
pm

mXi=1

bi

[↵(0)]i

u↵(s  a) =

for all i 2 [m].

bi · ([↵]>i (s  a)).

i.i.d.⇠N (0  Id/d) 

i.i.d.⇠ Unif({1  1}) 

(3.1)
Here m is the width of the neural network  bi 2 {1  1} (i 2 [m]) are the output weights  (·) is the
rectiﬁed linear unit (ReLU) activation  and ↵ = ([↵]>1   . . .   [↵]>m)> 2 Rmd with [↵]i 2 Rd (i 2 [m])
are the input weights. We consider the random initialization
(3.2)
We restrict the input weights ↵ to an `2-ball centered at the initialization ↵(0) by the projection
⇧B0(R↵)(↵0) = argmin↵2B0(R↵){k↵  ↵0k2}  where B0(R↵) = {↵ : k↵  ↵(0)k2  R↵}.
Throughout training  we only update ↵  while keeping bi (i 2 [m]) ﬁxed at the initialization. Hence 
we omit the dependency on bi (i 2 [m]) in NN(↵; m) and u↵(s  a).
Policy Improvement. We consider the population version of the objective function in (2.2) 

L(✓) = E⌫k⇥hQ!k (s ·) ⇡ ✓(·| s)i  k · KL(⇡✓(·| s)k ⇡✓k (·| s))⇤ 

(3.3)
where Q!k is an estimator of Q⇡✓k   that is  the exact action-value function of ⇡✓k. In the follow-
ing  we convert the subproblem max✓ L(✓) of policy improvement into a least-squares subprob-
lem. We consider the energy-based policy ⇡(a| s) / exp{⌧1f (s  a)}  which is abbreviated as
⇡ / exp{⌧1f}. Here f : S⇥A! R is the energy function and ⌧> 0 is the temperature
parameter. We have the following closed form of the ideal inﬁnite-dimensional policy update. See
also  e.g.  [1] for a Bayesian inference perspective.
Proposition 3.1. Let ⇡✓k / exp{⌧1
Q⇡✓k   the updateb⇡k+1 argmax⇡{E⌫k [hQ!k (s ·) ⇡ (·| s)i  k · KL(⇡(·| s)k ⇡✓k (·| s))]} gives

k f✓k} be an energy-based policy. Given an estimator Q!k of
(3.4)

k Q!k + ⌧1

k f✓k}.

Proof. See Appendix C for a detailed proof.

b⇡k+1 / exp{1

Here we note that the closed form of ideal inﬁnite-dimensional update in (3.4) holds state-wise. To

represent the ideal improved policyb⇡k+1 in Proposition 3.1 using the energy-based policy ⇡✓k+1 /
exp{⌧1

k+1f✓k+1}  we solve the subproblem of minimizing the MSE 

(3.5)
which is justiﬁed in Appendix B as a majorization of L(✓) deﬁned in (3.3). Here we use the neural
network parametrization f✓ = NN(✓; mf ) deﬁned in (3.1)  where ✓ denotes the input weights and

Eek⇥f✓(s  a)  ⌧k+1 · (1

k f✓k (s  a))2⇤ 

✓k+1 argmin
✓2B0(Rf )

k Q!k (s  a) + ⌧1

⇡✓k+1 approximates the ideal inﬁnite-dimensional policy update in (3.4) evenly well over all actions.
Also note that the subproblem in (3.5) allows for off-policy sampling of both states and actions [1].
To solve (3.5)  we use the SGD update

mf is the width. It is worth mentioning that in (3.5) we sample the actions according toek so that
✓(t + 1/2) ✓(t)  ⌘ ·f✓(t)(s  a)  ⌧k+1 · (1
k f✓k (s  a)) ·r ✓f✓(t)(s  a) 
where (s  a) ⇠ek and ✓(t + 1) ⇧B0(Rf )(✓(t + 1/2)). Here ⌘ is the stepsize. See Appendix A

Policy Evaluation. To obtain the estimator Q!k of Q⇡✓k in (3.3)  we solve the subproblem of
minimizing the MSBE 

for a detailed algorithm.

k Q!k (s  a) + ⌧1

(3.6)

!k argmin
!2B0(RQ)

Ek [(Q!(s  a)  [T ⇡✓k Q!](s  a))2].

(3.7)

Here the Bellman evaluation operator T ⇡ of a policy ⇡ is deﬁned as
We use the neural network parametrization Q! = NN(!; mQ) deﬁned in (3.1)  where ! denotes the
input weights and mQ is the width. To solve (3.7)  we use the TD update

[T ⇡Q](s  a) = E⇥(1  ) · r(s  a) +  · Q(s0  a0) s0 ⇠P (·| s  a)  a0 ⇠ ⇡(·| s0)⇤.

!(t + 1/2) !(t)  ⌘ ·Q!(t)(s  a)  (1  ) · r(s  a)   · Q!(t)(s0  a0) ·r !Q!(t)(s  a) 
(3.8)
where (s  a) ⇠ k  s0 ⇠P (·| s  a)  a0 ⇠ ⇡✓k (·| s0)  and !(t + 1) = ⇧B0(RQ)(!(t + 1/2)). Here ⌘
is the stepsize. See Appendix A for a detailed algorithm.

4

Neural PPO. By assembling the subproblems of policy improvement and policy evaluation  we
present neural PPO in Algorithm 1  which is characterized in Section 4.

Algorithm 1 Neural PPO
Require: MDP (S A P  r  )  penalty parameter   widths mf and mQ  number of SGD and TD
1: Initialize with uniform policy: ⌧0 1  f✓0 0  ⇡✓0 ⇡0 / exp{⌧1
2: for k = 0  . . .   K  1 do
3:
4:

iterations T   number of TRPO iterations K  and projection radii Rf  RQ
0 f✓0}
Set temperature parameter ⌧k+1 pK/(k + 1) and penalty parameter k pK
Sample {(st  at  a0
a0t ⇠ ⇡✓k (·| s0t)
Solve for Q!k = NN(!k; mQ) in (3.7) using the TD update in (3.8) (Algorithm 3)
Solve for f✓k+1 = NN(✓k+1; mf ) in (3.5) using the SGD update in (3.6) (Algorithm 2)
Update policy: ⇡✓k+1 / exp{⌧1

t ⇠ ⇡0(·| st)  s0t ⇠P (·| st  at) and

k+1f✓k+1}

t   s0t  a0t)}T

t=1 with (st  at) ⇠ k  a0

5:
6:
7:
8: end for

4 Main Results

In this section  we establish the global convergence of neural PPO in Algorithm 1 based on character-
izing the errors arising from solving the subproblems of policy improvement and policy evaluation
in (3.5) and (3.7)  respectively.
Our analysis relies on the following regularity condition on the boundedness of reward.
Assumption 4.1 (Bounded Reward). There exists a constant Rmax > 0 such that Rmax =
sup(s a)2S⇥A |r(s  a)|  which implies |V ⇡(s)| Rmax and |Q⇡(s  a)| Rmax for any policy
⇡.

To ensure the compatibility between the policy and the action-value function [25  43  23  35  46  47] 
we set mf = mQ and use the following random initialization. In Algorithm 1  we ﬁrst generate
according to (3.2) the random initialization ↵(0) = ✓(0) = !(0) and bi (i 2 [m])  and then use
it as the ﬁxed initialization of both SGD and TD in Lines 6 and 5 of Algorithm 1 for all k 2 [K] 
respectively.

4.1 Errors of Policy Improvement and Policy Evaluation
We deﬁne the following function class  which characterizes the representation power of the neural
network deﬁned in (3.1).
Deﬁnition 4.2. For any constant R > 0  we deﬁne the function class

FR m =⇢ 1

pm

mXi=1

bi · 1[↵(0)]>i (s  a) > 0 · [↵]>i (s  a) : k↵  ↵(0)k2  R 

where [↵(0)]i and bi (i 2 [m]) are the random initialization deﬁned in (3.2).
As m ! 1  FR m  NN(↵(0); m) approximates a subset of the reproducing kernel Hilbert space
(RKHS) induced by the kernel K(x  y) = Ez⇠N (0 Id/d)[1{z>x > 0  z>y > 0}x>y] [22  27  10 
3  54  8  9  26  5  7]. Such a subset is a ball with radius R in the corresponding H-norm  which is
known to be a rich function class [20]. Correspondingly  for a sufﬁciently large width m and radius
R  FR m is also a sufﬁciently rich function class.
Based on Deﬁnition 4.2  we lay out the following regularity condition on the action-value function
class.
Assumption 4.3 (Action-Value Function Class). It holds that Q⇡(s  a) 2F RQ mQ for any ⇡.
Assumption 4.3 states that FRQ mQ is closed under the Bellman evaluation operator T ⇡  as Q⇡ is the
ﬁxed-point solution of the Bellman equation T ⇡Q⇡ = Q⇡. Such a regularity condition is commonly
used in the literature [30  4  16  15  45  51]. In particular  [50] deﬁne a class of Markov decision
processes that satisfy such a regularity condition  which is sufﬁciently rich due to the representation
power of FRQ mQ.

5

In the sequel  we lay out another regularity condition on the stationary state-action distribution ⇡.
Assumption 4.4 (Regularity of Stationary Distribution). There exists a constant c > 0 such that for
any vector z 2 Rd and ⇣> 0  it holds almost surely that E⇡ [1{|z>(s  a)| ⇣}| z]  c · ⇣/kzk2
for any ⇡.

Assumption 4.4 states that the density of ⇡ is sufﬁciently regular. Such a regularity condition holds
as long as the stationary state distribution ⌫⇡ has upper bounded density.
We are now ready present bounds for errors induced by approximation via two-layer neural net-
works  with analysis generalizing those of [7  5] included in Appendix D. First  we characterize
the policy improvement error  which is induced by solving the subproblem in (3.5) using the SGD
update in (3.6)  in the following theorem. See Line 6 of Algorithm 1 and Algorithm 2 for a detailed
algorithm.
Theorem 4.5 (Policy Improvement Error). Suppose that Assumptions 4.1  4.3  and 4.4 hold. We
set T  64 and the stepsize to be ⌘ = T 1/2. Within the k-th iteration of Algorithm 1  the output
f✓ of Algorithm 2 satisﬁes

Einit ek⇥f✓(s  a)  ⌧k+1 · (1

f T 1/2 + R5/2

= O(R2

f m1/4

k Q!k (s  a) + ⌧1
).

f m1/2

+ R3

k f✓k (s  a))2⇤

f

f

Proof. See Appendix D for a detailed proof.

Similarly  we characterize the policy evaluation error  which is induced by solving the subproblem
in (3.7) using the TD update in (3.8)  in the following theorem. See Line 5 of Algorithm 1 and
Algorithm 3 for a detailed algorithm.
Theorem 4.6 (Policy Evaluation Error). Suppose that Assumptions 4.1  4.3  and 4.4 hold. We set
T  64/(1  )2 and the stepsize to be ⌘ = T 1/2. Within the k-th iteration of Algorithm 1  the
output Q! of Algorithm 3 satisﬁes

Einit k [(Q!(s  a)  Q⇡✓k (s  a))2] = O(R2

QT 1/2 + R5/2

Q m1/4

Q + R3

Qm1/2
Q ).

Proof. See Appendix D for a detailed proof.

As we show in Sections 4.3 and 5  Theorems 4.5 and 4.6 characterize the primal and dual errors of
the inﬁnite-dimensional mirror descent corresponding to neural PPO. In particular  such errors decay
to zero at the rate of 1/pT when the width mf = mQ is sufﬁciently large  where T is the number
of TD and SGD iterations in Algorithm 1. For notational simplicity  we omit the dependency on the
random initialization in the expectations hereafter.

⇡

4.2 Error Propagation
We denote by ⇡⇤ the optimal policy with ⌫⇤ being its stationary state distribution and ⇤ being its

based on Q!k  which is an estimator of the exact action-value function Q⇡✓k . Correspondingly  we
deﬁne the ideal improved policy based on Q⇡✓k as

stationary state-action distribution. Recall that  as deﬁned in (3.4) b⇡k+1 is the ideal improved policy
(4.1)
By the same proof of Proposition 3.1  we have ⇡k+1 / exp{1
k f✓k}  which is also an
energy-based policy.
We deﬁne the following quantities related to density ratios between policies or stationary distribu-
tions 

E⌫k⇥hQ⇡✓k (s ·) ⇡ (·  s)i  k · KL(⇡(·| s)k ⇡✓k (·| s))⇤ .

k Q⇡✓k + ⌧1

⇡k+1 = argmax

⇤k = Eek [|d⇤/dek  d(⇡✓k ⌫⇤)/dek|2]1/2  ⇤k = Ek [|d⇤/dk  d⌫⇤/d⌫k|2]1/2 
where d⇤/dek  d(⇡✓k ⌫⇤)/dek  d⇤/dk  and d⌫⇤/d⌫k are the Radon-Nikodym derivatives. A

closely related quantity known as the concentrability coefﬁcient is commonly used in the literature
[30  4  16  45  51]. In comparison  as our analysis is based on stationary distributions  our deﬁnitions
of ⇤k and ⇤k are simpler in that they do not require unrolling the state-action sequence. Then we
have the following lemma that quantiﬁes how the errors of policy improvement and policy evaluation
propagate into the inﬁnite-dimensional policy space.

(4.2)

6

Lemma 4.7 (Error Propagation). Suppose that the policy improvement error in Line 6 of Algorithm
1 satisﬁes

k Q!k (s  a)  ⌧1
and the policy evaluation error in Line 5 of Algorithm 1 satisﬁes

Eek⇥f✓k+1(s  a)  ⌧k+1 · (1
k f✓k (s  a))2⇤  ✏k+1 
E⌫⇤⇥⌦log(⇡✓k+1(·| s)/⇡k+1(·| s)) ⇡ ⇤(·| s)  ⇡✓k (·| s)↵⇤  "k 

Ek [(Q!k (s  a)  Q⇡✓k (s  a))2]  ✏0k.

For ⇡k+1 deﬁned in (4.1) and ⇡✓k+1 obtained in Line 7 of Algorithm 1  we have

k ✏0k · ⇤k.
Proof. See Appendix E for a detailed proof.

k+1✏k+1 · ⇤k+1 + 1

where "k = ⌧1

(4.3)

(4.4)

(4.5)

Lemma 4.7 quantiﬁes the difference between the ideal case  where we use the inﬁnite-dimensional
policy update based on the exact action-value function  and the realistic case  where we use the neu-
ral networks deﬁned in (3.1) to approximate the exact action-value function and the ideal improved
policy.
The following lemma characterizes the difference between f✓k+1 and f✓k.
Lemma 4.8 (Stepwise Energy Difference). Under the same conditions of Lemma 4.7  we have

E⌫⇤[k⌧1
k+1✏2

k+1f✓k+1(s ·)  ⌧1

1]  2"0k + 22
k M 
k+1 and M = 2E⌫⇤[maxa2A(Q!0(s  a))2] + 2R2
f .

k f✓k (s ·)k2

where "0k = |A| · ⌧2
Proof. See Appendix E for a detailed proof.

Intuitively  the bounded difference between f✓k+1 and f✓k+1 quantiﬁed in Lemma 4.8 is due to the
KL-regularization in (3.3)  which keeps the updated policy ⇡✓k+1 from being too far away from the
current policy ⇡✓k.
The differences characterized in Lemmas 4.7 and 4.8 play key roles in establishing the global con-
vergence of neural PPO.

4.3 Global Convergence of Neural PPO

We track the progress of neural PPO in Algorithm 1 using the expected total reward

k=1 attained by neural PPO in Algorithm 1  we have

L(⇡) = E⌫⇤[V ⇡(s)] = E⌫⇤[hQ⇡(s ·) ⇡ (·| s)i] 

(4.6)
where ⌫⇤ is the stationary state distribution of the optimal policy ⇡⇤. The following theorem char-
acterizes the global convergence of L(⇡✓k ) towards L(⇡⇤). Recall that Tf and TQ are the numbers
of SGD and TD iterations in Lines 6 and 5 of Algorithm 1  while ⇤k and ⇤k are deﬁned in (4.2).
Theorem 4.9 (Global Rate of Convergence of Neural PPO). Suppose that Assumptions 4.1  4.3 
and 4.4 hold. For the policy sequence {⇡✓k}K
0kKL(⇡⇤) L (⇡✓k ) 
k+1✏k+1 · ⇤k + 1
f T 1/2 + R5/2

Here "k = ⌧1
✏k+1 = O(R2
Also  we have M = 2E⌫⇤[maxa2A(Q!0(s  a))2] + 2R2
f .
Proof. See Section 5 for a detailed proof of Theorem 4.9. The key to our proof is the global conver-
gence of inﬁnite-dimensional mirror descent with errors under one-point monotonicity  where the
primal and dual errors are characterized by Theorems 4.5 and 4.6  respectively.

2 log |A| + M + 2PK1
(1  ) · pK
k+1  where
k+1✏2
QT 1/2 + R5/2
) ✏ 0k = O(R2

k ✏0k · ⇤k and "0k = |A| · ⌧2

k=0 ("k + "0k)

f m1/4

f m1/2

+ R3

Q m1/4

Q + R3

Qm1/2
Q ).

min

.

f

f

To understand Theorem 4.9  we consider the inﬁnite-dimensional policy update based on the exact
action-value function  that is  ✏k+1 = ✏0k = 0 for any k +1 2 [K]. In such an ideal case  by Theorem
4.9  neural PPO globally converges to the optimal policy ⇡⇤ at the rate of

min

0kKL(⇡⇤) L (⇡✓k ) 

7

2pM log |A|
(1  ) · pK

 

with the optimal choice of the penalty parameter k =pM K/ log |A|.
Note that Theorem 4.9 sheds light on the difﬁculty of choosing the optimal penalty coefﬁcient in
practice  which is observed by [40]. In particular  the optimal choice of  in k = pK is given by

pM

 

k=0 ("k + "0k)

 =

qlog |A| +PK1

k=0 ("k + "0k) may vary across different deep reinforcement learning problems. As

where M andPK1
a result  line search is often needed in practice.
To better understand Theorem 4.9  the following corollary quantiﬁes the minimum width mf and
mQ and the minimum number of SGD and TD iterations T that ensure the O(1/pK) rate of con-
vergence.
Corollary 4.10 (Iteration Complexity of Subproblems and Minimum Widths of Neural Networks).
4 + K4R10
f · |A|2) 
Suppose that Assumptions 4.1  4.3  and 4.4 hold. Let mf =⌦( K6R10
2) for any 0  k  K.

2 + K2R4

mQ =⌦K2R10
Q · ⇤k

We have

4  and T =⌦( K3R4
f · ⇤k
0kKL(⇡⇤) L (⇡✓k ) 

min

f · ⇤k
Q · ⇤k
2 log |A| + M + O(1)

f ·|A| + KR4
(1  ) · pK

.

Proof. See Appendix F for a detailed proof.

The difference between the requirements on the widths mf and mQ in Corollary 4.10 suggests that
the errors of policy improvement and policy evaluation play distinct roles in the global convergence
of neural PPO. In fact  Theorem 4.9 depends on the total error ⌧1
k ✏0k · ⇤k + |A| ·
⌧2
k+1 of the policy improvement error ✏k+1 is much larger than the
k+1✏2
weight 1
k+1 is a high-order term when ✏k+1 is
sufﬁciently small. In other words  the policy improvement error plays a more important role.

k of the policy evaluation error ✏0k  and |A| · ⌧2

k+1  where the weight ⌧1

k+1✏k+1 · ⇤k + 1

k+1✏2

5 Proof Sketch

In this section  we sketch the proof of Theorem 4.9. In detail  we cast neural PPO in Algorithm 1
as inﬁnite-dimensional mirror descent with primal and dual errors and exploit a notion of one-point
monotonicity to establish its global convergence.
We ﬁrst present the performance difference lemma of [24]. Recall that the expected total reward
L(⇡) is deﬁned in (4.6) and ⌫⇤ is the stationary state distribution of the optimal policy ⇡⇤.
Lemma 5.1 (Performance Difference). For L(⇡) deﬁned in (4.6)  we have

L(⇡) L (⇡⇤) = (1  )1 · E⌫⇤[hQ⇡(s ·) ⇡ (·| s)  ⇡⇤(·| s)i].

Proof. See Appendix G for a detailed proof.

for any ⇡.

E⌫⇤[hQ⇡(s ·) ⇡ (·| s)  ⇡⇤(·| s)i]  0 

Since the optimal policy ⇡⇤ maximizes the value function V ⇡(s) with respect to ⇡ for any s 2S  
we have L(⇡⇤) = E⌫⇤[V ⇡⇤(s)]  E⌫⇤[V ⇡(s)] = L(⇡) for any ⇡. As a result  we have
(5.1)
Under the variational inequality framework [14]  (5.1) corresponds to the monotonicity of the map-
ping Q⇡ evaluated at ⇡⇤ and any ⇡. Note that the classical notion of monotonicity requires the
evaluation at any pair ⇡0 and ⇡  while we restrict ⇡0 to ⇡⇤ in (5.1). Hence  we refer to (5.1) as one-
point monotonicity. In the context of nonconvex optimization  the mapping Q⇡ can be viewed as
the gradient of L(⇡) at ⇡  which lives in the dual space  while ⇡ lives in the primal space. Another
condition related to (5.1) in nonconvex optimization is known as dissipativity [53].
The following lemma establishes the one-step descent of the KL-divergence in the inﬁnite-
dimensional policy space  which follows from the analysis of mirror descent [31  32] as well as
the fact that given any ⌫k  the subproblem of policy improvement in (4.1) can be solved for each
s 2S individually.

8

Lemma 5.2 (One-Step Descent). For the ideal improved policy ⇡k+1 deﬁned in (4.1) and the current
policy ⇡✓k  we have that  for any s 2S  
KL(⇡⇤(·| s)k ⇡✓k+1(·| s))  KL(⇡⇤(·| s)k ⇡✓k (·| s))
⌦log(⇡✓k+1(·| s)/⇡k+1(·| s)) ⇡ ✓k (·| s)  ⇡⇤(·| s)↵  1
k+1f✓k+1(s ·)  ⌧1

·h Q⇡✓k (s ·) ⇡ ⇤(·| s)  ⇡✓k (·| s)i
k f✓k (s ·) ⇡ ✓k (·| s)  ⇡✓k+1(·| s)i.

1  h⌧1

k

 1/2 ·k ⇡✓k+1(·| s)  ⇡✓k (·| s)k2
Proof. See Appendix G for a detailed proof.

Based on Lemmas 5.1 and 5.2  we prove Theorem 4.9 by casting neural PPO as inﬁnite-dimensional
mirror descent with primal and dual errors  whose impact is characterized in Lemma 4.7. In partic-
ular  we employ the `1-`1 pair of primal-dual norms.
Proof of Theorem 4.9. Taking expectation with respect to s ⇠ ⌫⇤ and invoking Lemmas 4.7 and
5.2  we have
E⌫⇤[KL(⇡⇤(·| s)k ⇡✓k+1(·| s))]  E⌫⇤[KL(⇡⇤(·| s)k ⇡✓k (·| s))]

 "k  1

k

· E⌫⇤[hQ⇡✓k (s ·) ⇡ ⇤(·| s)  ⇡✓k (·| s)i]  1/2 · E⌫⇤[k⇡✓k+1(·| s)  ⇡✓k (·| s)k2
1]
k+1f✓k+1(s ·)  ⌧1

k f✓k (s ·) ⇡ ✓k (·| s)  ⇡✓k+1(·| s)i].

 E⌫⇤[h⌧1

By Lemma 5.1 and the H¨older’s inequality  we further have

E⌫⇤[KL(⇡⇤(·| s)k ⇡✓k+1(·| s))]  E⌫⇤[KL(⇡⇤(·| s)k ⇡✓k (·| s))]

k

 "k  (1  )1
+ E⌫⇤⇥k⌧1
 "k  (1  )1
 "k  (1  )1

k+1f✓k+1(s ·)  ⌧1

· (L(⇡⇤) L (⇡✓k ))  1/2 · E⌫⇤[k⇡✓k+1(·| s)  ⇡✓k (·| s)k2
1]
k f✓k (s ·)k1 ·k ⇡✓k (·| s)  ⇡✓k+1(·| s)k1⇤
k+1f✓k+1(s ·)  ⌧1

k f✓k (s ·)k2
1]
(5.2)
where in the second inequality we use 2xy  y2  x2 and in the last inequality we use Lemma 4.8.
Rearranging the terms in (5.2)  we have
(5.3)

· (L(⇡⇤) L (⇡✓k )) + 1/2 · E⌫⇤[k⌧1
· (L(⇡⇤) L (⇡✓k )) + ("0k + 2
k M ) 

k

k

(1  )1

k

· (L(⇡⇤) L (⇡✓k ))

 E⌫⇤[KL(⇡⇤(·| s)k ⇡✓k+1(·| s))]  E⌫⇤[KL(⇡⇤(·| s)k ⇡✓k (·| s))] + 2

k M + "k + "0k.

Telescoping (5.3) for k + 1 2 [K]  we obtain

K1Xk=0
(1  )1
 E⌫⇤[KL(⇡⇤(·| s)k ⇡✓K (·| s))]  E⌫⇤[KL(⇡⇤(·| s)k ⇡✓0(·| s))]

· (L(⇡✓k ) L (⇡⇤))

k

+ M

2
k +

k=0 1

("k + "0k).

K1Xk=0

K1Xk=0
k ·(L(⇡⇤)L(⇡✓k ))  (PK1
log |A| + MPK1
Setting the penalty parameter k = pK  we havePK1

Note that we have (i)PK1
k )·min0kK{L(⇡⇤)L(⇡✓k )} 
(ii) E⌫⇤[KL(⇡⇤(·| s)k ⇡✓0(·| s))]  log |A| due to the uniform initialization of policy  and that (iii)
the KL-divergence is nonnegative. Hence  we have
k +PK1
k = 1pK andPK1

k=0 1
which together with (5.4) concludes the proof of Theorem 4.9.

0kKL(⇡⇤) L (⇡✓k ) 

(1  )PK1

k=0 ("k + "0k)

k = 2 

k=0 1

k=0 2

k=0 2

k=0 1

(5.4)

min

k

.

Acknowledgement

The authors thank Jason D. Lee  Chi Jin  and Yu Bai for enlightening discussions throughout this
project.

9

References
[1] Abdolmaleki  A.  Springenberg  J. T.  Tassa  Y.  Munos  R.  Heess  N. and Riedmiller  M.

(2018). Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920.

[2] Agarwal  A.  Kakade  S. M.  Lee  J. D. and Mahajan  G. (2019). Optimality and approximation
with policy gradient methods in Markov decision processes. arXiv preprint arXiv:1908.00261.

[3] Allen-Zhu  Z.  Li  Y. and Liang  Y. (2018). Learning and generalization in overparameterized

neural networks  going beyond two layers. arXiv preprint arXiv:1811.04918.

[4] Antos  A.  Szepesv´ari  C. and Munos  R. (2008). Fitted Q-iteration in continuous action-space

mdps. In Advances in Neural Information Processing Systems.

[5] Arora  S.  Du  S. S.  Hu  W.  Li  Z. and Wang  R. (2019). Fine-grained analysis of optimiza-
arXiv preprint

tion and generalization for overparameterized two-layer neural networks.
arXiv:1901.08584.

[6] Azar  M. G.  G´omez  V. and Kappen  H. J. (2012). Dynamic policy programming. Journal of

Machine Learning Research  13 3207–3245.

[7] Cai  Q.  Yang  Z.  Lee  J. D. and Wang  Z. (2019). Neural temporal-difference learning con-

verges to global optima. arXiv preprint arXiv:1905.10027.

[8] Cao  Y. and Gu  Q. (2019). Generalization bounds of stochastic gradient descent for wide and

deep neural networks. arXiv preprint arXiv:1905.13210.

[9] Cao  Y. and Gu  Q. (2019). A generalization theory of gradient descent for learning over-

parameterized deep ReLU networks. arXiv preprint arXiv:1902.01384.

[10] Chizat  L. and Bach  F. (2018). A note on lazy training in supervised differentiable program-

ming. arXiv preprint arXiv:1812.07956.

[11] Cho  W. S. and Wang  M. (2017). Deep primal-dual reinforcement learning: Accelerating

actor-critic using Bellman duality. arXiv preprint arXiv:1712.02467.

[12] Dai  B.  Shaw  A.  Li  L.  Xiao  L.  He  N.  Liu  Z.  Chen  J. and Song  L. (2017). SBEED:
Convergent reinforcement learning with nonlinear function approximation. arXiv preprint
arXiv:1712.10285.

[13] Duan  Y.  Chen  X.  Houthooft  R.  Schulman  J. and Abbeel  P. (2016). Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learn-
ing.

[14] Facchinei  F. and Pang  J.-S. (2007). Finite-Dimensional Variational Inequalities and Comple-

mentarity Problems. Springer Science & Business Media.

[15] Farahmand  A.-m.  Ghavamzadeh  M.  Szepesv´ari  C. and Mannor  S. (2016). Regularized pol-
icy iteration with nonparametric function spaces. Journal of Machine Learning Research  17
4809–4874.

[16] Farahmand  A.-m.  Szepesv´ari  C. and Munos  R. (2010). Error propagation for approximate

policy and value iteration. In Advances in Neural Information Processing Systems.

[17] Haarnoja  T.  Tang  H.  Abbeel  P. and Levine  S. (2017). Reinforcement learning with deep

energy-based policies. In International Conference on Machine Learning.

[18] Haarnoja  T.  Zhou  A.  Abbeel  P. and Levine  S. (2018).

maximum entropy deep reinforcement learning with a stochastic actor.
arXiv:1801.01290.

Soft actor-critic: Off-policy
arXiv preprint

[19] Henderson  P.  Islam  R.  Bachman  P.  Pineau  J.  Precup  D. and Meger  D. (2018). Deep

reinforcement learning that matters. In AAAI Conference on Artiﬁcial Intelligence.

10

[20] Hofmann  T.  Sch¨olkopf  B. and Smola  A. J. (2008). Kernel methods in machine learning.

Annals of Statistics 1171–1220.

[21] Ilyas  A.  Engstrom  L.  Santurkar  S.  Tsipras  D.  Janoos  F.  Rudolph  L. and Madry  A.
(2018). Are deep policy gradient algorithms truly policy gradient algorithms? arXiv preprint
arXiv:1811.02553.

[22] Jacot  A.  Gabriel  F. and Hongler  C. (2018). Neural tangent kernel: Convergence and gener-

alization in neural networks. In Advances in Neural Information Processing Systems.

[23] Kakade  S. (2002). A natural policy gradient. In Advances in Neural Information Processing

Systems.

[24] Kakade  S. and Langford  J. (2002). Approximately optimal approximate reinforcement learn-

ing. In International Conference on Machine Learning.

[25] Konda  V. R. and Tsitsiklis  J. N. (2000). Actor-critic algorithms. In Advances in Neural In-

formation Processing Systems.

[26] Lee  J.  Xiao  L.  Schoenholz  S. S.  Bahri  Y.  Sohl-Dickstein  J. and Pennington  J. (2019).
Wide neural networks of any depth evolve as linear models under gradient descent. arXiv
preprint arXiv:1902.06720.

[27] Li  Y. and Liang  Y. (2018). Learning overparameterized neural networks via stochastic gradi-

ent descent on structured data. In Advances in Neural Information Processing Systems.

[28] Ling  Y.  Hasan  S. A.  Datla  V.  Qadir  A.  Lee  K.  Liu  J. and Farri  O. (2017). Diagnostic
inferencing via improving clinical concept extraction with deep reinforcement learning: A
preliminary study. In Machine Learning for Healthcare Conference.

[29] Mnih  V.  Badia  A. P.  Mirza  M.  Graves  A.  Lillicrap  T.  Harley  T.  Silver  D. and
Kavukcuoglu  K. (2016). Asynchronous methods for deep reinforcement learning. In Interna-
tional Conference on Machine Learning.

[30] Munos  R. and Szepesv´ari  C. (2008). Finite-time bounds for ﬁtted value iteration. Journal of

Machine Learning Research  9 815–857.

[31] Nemirovski  A. S. and Yudin  D. B. (1983). Problem Complexity and Method Efﬁciency in

Optimization. Springer.

[32] Nesterov  Y. (2013). Introductory Lectures on Convex Optimization: A Basic Course  vol. 87.

Springer Science & Business Media.

[33] Neu  G.  Jonsson  A. and G´omez  V. (2017). A uniﬁed view of entropy-regularized Markov

decision processes. arXiv preprint arXiv:1705.07798.

[34] OpenAI (2019). OpenAI Five. https://openai.com/five/.
[35] Peters  J. and Schaal  S. (2008). Natural actor-critic. Neurocomputing  71 1180–1190.
[36] Puterman  M. L. (2014). Markov Decision Processes: Discrete Stochastic Dynamic Program-

ming. John Wiley & Sons.

[37] Rajeswaran  A.  Lowrey  K.  Todorov  E. V. and Kakade  S. M. (2017). Towards generalization
and simplicity in continuous control. In Advances in Neural Information Processing Systems.

[38] Sallab  A. E.  Abdou  M.  Perot  E. and Yogamani  S. (2017). Deep reinforcement learning

framework for autonomous driving. Electronic Imaging  2017 70–76.

[39] Schulman  J.  Levine  S.  Abbeel  P.  Jordan  M. and Moritz  P. (2015). Trust region policy

optimization. In International Conference on Machine Learning.

[40] Schulman  J.  Wolski  F.  Dhariwal  P.  Radford  A. and Klimov  O. (2017). Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347.

11

[41] Sutton  R. S. (1988). Learning to predict by the methods of temporal differences. Machine

Learning  3 9–44.

[42] Sutton  R. S. and Barto  A. G. (2018). Reinforcement Learning: An Introduction. MIT press.

[43] Sutton  R. S.  McAllester  D. A.  Singh  S. P. and Mansour  Y. (2000). Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems.

[44] Szepesv´ari  C. (2010). Algorithms for reinforcement learning. Synthesis Lectures on Artiﬁcial

Intelligence and Machine Learning  4 1–103.

[45] Tosatto  S.  Pirotta  M.  D’Eramo  C. and Restelli  M. (2017). Boosted ﬁtted Q-iteration. In

International Conference on Machine Learning.

[46] Wagner  P. (2011). A reinterpretation of the policy oscillation phenomenon in approximate

policy iteration. In Advances in Neural Information Processing Systems.

[47] Wagner  P. (2013). Optimistic policy iteration and natural actor-critic: A unifying view and a

non-optimality result. In Advances in Neural Information Processing Systems.

[48] Wang  L.  Cai  Q.  Yang  Z. and Wang  Z. (2019). Neural policy gradient methods: Global

optimality and rates of convergence. arXiv preprint arXiv:1909.01150.

[49] Williams  R. J. (1992). Simple statistical gradient-following algorithms for connectionist rein-

forcement learning. Machine Learning  8 229–256.

[50] Yang  L. F. and Wang  M. (2019). Sample-optimal parametric Q-learning with linear transition

models. arXiv preprint arXiv:1902.04779.

[51] Yang  Z.  Xie  Y. and Wang  Z. (2019). A theoretical analysis of deep Q-learning. arXiv

preprint arXiv:1901.00137.

[52] Zhang  K.  Koppel  A.  Zhu  H. and Bas¸ar  T. (2019). Global convergence of policy gradient

methods to (almost) locally optimal policies. arXiv preprint arXiv:1906.08383.

[53] Zhou  M.  Liu  T.  Li  Y.  Lin  D.  Zhou  E. and Zhao  T. (2019). Toward understanding the
In International Conference on Machine

importance of noise in training neural networks.
Learning.

[54] Zou  D.  Cao  Y.  Zhou  D. and Gu  Q. (2018). Stochastic gradient descent optimizes over-

parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888.

12

,Boyi Liu
Qi Cai
Zhuoran Yang
Zhaoran Wang