2011,Sparse Recovery with Brownian Sensing,We consider the problem of recovering the parameter alpha in R^K of a sparse function f  i.e. the number of non-zero entries of alpha is small compared to the number K of features  given noisy evaluations of f at a set of well-chosen sampling points. We introduce an additional randomisation process  called Brownian sensing  based on the computation of stochastic integrals  which produces a Gaussian sensing matrix  for which good recovery properties are proven independently on the number of sampling points N  even when the features are arbitrarily non-orthogonal. Under the assumption that f is Hölder continuous with exponent at least 1/2  we provide an estimate a of the parameter such that ||\alpha - a||_2 = O(||eta||_2\sqrt{N})  where eta is the observation noise. The method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features. We report numerical experiments illustrating our method.,Sparse Recovery with Brownian Sensing

Alexandra Carpentier

INRIA Lille

Odalric-Ambrym Maillard

INRIA Lille

Alexandra.carpentier@inria.fr

odalricambrym.maillard@gmail.com

R´emi Munos
INRIA Lille

remi.munos@inria.fr

Abstract

We consider the problem of recovering the parameter α ∈ RK of a sparse function
f (i.e. the number of non-zero entries of α is small compared to the number K of
features) given noisy evaluations of f at a set of well-chosen sampling points. We
introduce an additional randomization process  called Brownian sensing  based on
the computation of stochastic integrals  which produces a Gaussian sensing ma-
trix  for which good recovery properties are proven  independently on the number
of sampling points N   even when the features are arbitrarily non-orthogonal. Un-
der the assumption that f is H¨older continuous with exponent at least 1/2  we pro-

vide an estimate �α of the parameter such that �α −�α�2 = O(�η�2/√N )  where

η is the observation noise. The method uses a set of sampling points uniformly
distributed along a one-dimensional curve selected according to the features. We
report numerical experiments illustrating our method.

1 Introduction

We consider the problem of sensing an unknown function f : X → R (where X ⊂ Rd)  where f
belongs to span of a large set of (known) features {ϕk}1≤k≤K of L2(X ):

f (x) =

K�k=1

αkϕk(x) 

where α ∈ RK is the unknown parameter  and is assumed to be S-sparse  i.e. �α�0
0}| ≤ S. Our goal is to recover α as accurately as possible.
In the setting considered here we are allowed to select the points {xn}1≤n≤N ∈ X where the
function f is evaluated  which results in the noisy observations

def= |{i : αk �=

yn = f (xn) + ηn 

(1)

i.e. 

def=

2

where ηn is an observation noise term. We assume that
�η�2
η2
n ≤ σ2. We write DN = ({xn  yn}1≤n≤N ) the set of observations and we are in-
terested in situations where N � K  i.e.  the number of observations is much smaller than the
number of features ϕk.

the noise is bounded 

N�n=1

The question we wish to address is: how well can we recover α based on a set of N noisy mea-
surements? Note that whenever the noise is non-zero  the recovery cannot be perfect  so we wish to

express the estimation error �α −�α�2 in terms of N   where �α is our estimate.

1

The proposed method. We address the problem of sparse recovery by combining the two ideas:

• Sparse recovery theorems (see Section 2) essentially say that in order to recover a vector
with a small number of measurements  one needs incoherence. The measurement basis 
corresponding to the pointwise evaluations f (xn)  should to be incoherent with the rep-
resentation basis  corresponding to the one on which the vector α is sparse. Interpreting
these basis in terms of linear operators  pointwise evaluation of f is equivalent to mea-
suring f using Dirac masses δxn (f ) def= f (xn). Since in general the representation basis
{ϕk}1≤k≤K is not incoherent with the measurement basis induced by Dirac operators  we
would like to consider another measurement basis  possibly randomized  in order that it
becomes incoherent with any representation basis.

• Since we are interested in reconstructing α  and since we assumed that f is linear in α 
we can apply any set of M linear operators {Tm}1≤m≤M to f = �k αkϕk  and consider
the problem transformed by the operators; the parameter α is thus also the solution to the
transformed problem Tm(f ) =�k αkTm(ϕk).

Thus  instead of considering the N×K sensing matrix Φ = (δxn (ϕk))k n  we consider a new M×K
sensing matrix A = (Tm(ϕk))k m  where the operators {Tm}1≤m≤M enforce incoherence between
bases. Provided that we can estimate Tm(f ) with the data set DN   we will be able to recover α. The
Brownian sensing approach followed here uses stochastic integral operators {Tm}1≤m≤M   which
makes the measurement basis incoherent with any representation basis  and generates a sensing
matrix A which is Gaussian (with i.i.d. rows).

The proposed algorithm (detailed in Section 3) recovers α by solving the system Aα ≈ �b by l1
minimization1  where�b ∈ RM is an estimate  based on the noisy observations yn  of the vector
b ∈ RM whose components are bm = Tmf .
Contribution: Our contribution is a sparse recovery result for arbitrary non-orthonormal functional
basis {ϕk}k≤K of a H¨older continuous function f . Theorem 4 states that our estimate �α satisﬁes
�α −�α�2 = O(�η�2/√N ) with high probability whatever N   under the assumption that the noise
• We show that when the sensing matrix A is Gaussian  i.e. when each row of the matrix is
drawn i.i.d. from a Gaussian distribution  orthonormality is not required for sparse recovery.
This result  stated in Proposition 1 (and used in Step 1 of the proof of Theorem 4)  is a
consequence of Theorem 3.1 of [10].

η is globally bounded  such as in [3  12]. This result is obtained by combining two contributions:

• The sensing matrix A is made Gaussian by choosing the operators Tm to be stochastic inte-
grals: Tmf def= 1√M �C
f dBm  where Bm are Brownian motions  and C is a 1-dimensional
curve of X appropriately chosen according to the functions {ϕk}k≤K (see the discussion
in Section 4). We call A the Brownian sensing matrix.

We have the property that the recovery property using the Brownian sensing matrix A only depends
on the number of Brownian motions M used in the stochastic integrals and not on the number of
sampled points N . Note that M can be chosen arbitrarily large as it is not linked with the limited
amount of data  but M affects the overall computational complexity of the method. The number of
sample N appears in the quality of estimation of b only  and this is where the assumption that f is
H¨older continuous comes into the picture.

Outline: In Section 2  we survey the large body of existing results about sparse recovery and relate
our contribution to this literature. In Section 3  we explain in detail the Brownian sensing recovery
method sketched above and state our main result in Theorem 4.

In Section 4  we ﬁrst discuss our result and compare it with existing work. Then we comment on
the choice and inﬂuence of the sampling domain C on the recovery performance.
Finally in Section 5  we report numerical experiments illustrating the recovery properties of the
Brownian sensing method  and the beneﬁt of the latter compared to a straightforward application of
compressed sensing when there is noise and very few sampling points.

1where the approximation sign ≈ refers to a minimization problem under a constraint coming from the

observation noise.

2

2 Relation to existing results

A standard approach in order to recover α is to consider the N × K matrix Φ = (ϕk(xn))k n  and
solve the system Φ�α ≈ y where y is the vector with components yn. Since N � K this is an ill-
posed problem. Under the sparsity assumption  a successful idea is ﬁrst to replace the initial problem
with the well-deﬁned problem of minimizing the �0 norm of α under the constraint that Φ�α ≈ y  and

then  since this problem is NP-hard  use convex relaxation of the �0 norm by replacing it with the �1
norm. We then need to ensure that the relaxation provides the same solution as the initial problem
making use of the �0 norm. The literature on this problem is huge (see [3  7  8  15  18  4  11] for
examples of papers that initiated this ﬁeld of research).

Generally  we can decompose the reconstruction problem into two distinct sub-problems. The ﬁrst
sub-problem (a) is to state conditions on the matrix Φ ensuring that the recovery is possible and
derive results for the estimation error under such conditions:

The ﬁrst important condition is the Restricted Isometry Property (RIP)  introduced in [5]  from
which we can derive the following recovery result stated in [6]:

√N

a�2

  deﬁned as δS =
�a�2 − 1|;�a�0 ≤ S}. Then if δ3S + δ4S < 2  for every S-sparse vector α ∈ RK   the

Theorem 1 (Cand´es & al  2006) Let δS be the restricted isometry constant of Φ√N
sup{| � Φ
solution �α to the �1-minimization problem min{�a�1; a satisﬁes �Φa − y�2

2 ≤ σ2} satisﬁes

CSσ2

 

where CS depends only on δ4S.

��α − α�2
2 ≤

N

Apart from the historical RIP  many other conditions emerged from works reporting the practical
difﬁculty to have the RIP satisﬁed  and thus weaker conditions ensuring reconstruction were derived.
See [17] for a precise survey of such conditions. A weaker condition for recovery is the compatibility
condition which leads to the following result from [16]:

Theorem 2 (Van de Geer & Buhlmann  2009) Assuming that the compatibility condition is satis-
ﬁed  i.e. for a set S of indices of cardinality S and a constant L 

C(L S) = min� S� Φ√N
α�2
�αS�2

  α satisﬁes �αS c�1 ≤ L�αS�1� > 0 
then for every S-sparse vector α ∈ RK  
the solution �α to the �1-minimization problem
min{�α�1; α satisﬁes �αS c�1 ≤ L�αS�1} satisﬁes for C a numerical constant:
C(L S)2

σ2 log(K)

N

C

2

1

.

The second sub-problem (b) of the global reconstruction problem is to provide the user with a
simple way to efﬁciently sample the space in order to build a matrix Φ such that the conditions
for recovery are fulﬁlled  at least with high probability. This can be difﬁcult in practice since it
involves understanding the geometry of high dimensional objects. For instance  to the best of our
knowledge  there is no result explaining how to sample the space so that the corresponding sensing
matrix Φ satisﬁes the nice recovery properties needed by the previous theorems  for a general family
of features {ϕk}k≤K .
However  it is proven in [12] that under some hypotheses on the functional basis  we are able to
recover the strong RIP property for the matrix Φ with high probability. This result  combined with a
recovery result  is stated as follows:

��α − α�2
2 ≤

Theorem 3 (Rauhut  2010) Assume that {ϕk}k≤K is an orthonormal basis of functions under a
measure ν  bounded by a constant Cϕ  and that we build DN by sampling f at random according
ϕS log(S)2 log(K) and
log(N ) ≥ c0C 2
to ν. Assume also that the noise is bounded �η�2 ≤ σ. If
N ≥ c1C 2
ϕS log(p−1)  then with probability at least 1 − p  for every S-sparse vector α ∈ RK   the
solution �α to the �1-minimization problem min{�a�1; a satisﬁes �Aa − y�2

2 ≤ σ2} satisﬁes

N

c2σ2
N

 

��α − α�2
2 ≤

3

where c0  c1 and c2 are some numerical constants.

In order to prove this theorem  the author of [12] showed that by sampling the points i.i.d. from ν 
then with with high probability the resulting matrix Φ is RIP. The strong point of this Theorem is that
we do not need to check conditions on the matrix Φ to guarantee that it is RIP  which is in practice
infeasible. But the weakness of the result is that the initial basis has to be orthonormal and bounded
under the given measure ν in order to get the RIP satisﬁed: the two conditions ensure incoherence
with Dirac observation basis. The speciﬁc case of an unbounded basis i.e.  Legendre Polynomial
basis  has been considered in [13]  but to the best of our knowledge  the problem of designing a
general sampling strategy such that the resulting sensing matrix possesses nice recovery properties
in the case of non-orthonormal basis remains unaddressed. Our contribution considers this case and
is described in the following section.

3 The “Brownian sensing” approach

A need for incoherence. When the representation and observation basis are not incoherent  the
sensing matrix Φ does not possess a nice recovery property. A natural idea is to change the observa-
tion basis by introducing a set of M linear operators {Tm}m≤M acting on the functions {ϕk}k≤K .
We have Tm(f ) =
αkTm(ϕk) for all 1 ≤ m ≤ M and our goal is to deﬁne the operators
{Tm}m≤M in order that the sensing matrix (Tm(ϕk))m k enjoys a nice recovery property  whatever
the representation basis {ϕk}k≤K .

K�k=1

The Brownian sensing operators. We now consider linear operators deﬁned by stochastic inte-
grals on a 1-dimensional curve C of X . First  we need to select a curve C ⊂ X of length l  such
that the covariance matrix VC  deﬁned by its elements (VC)i j = �C
ϕiϕj (for 1 ≤ i  j ≤ K)  is
invertible. We will discuss the existence of a such a curve later in Section 4. Then  we deﬁne the
linear operators {Tm}1≤m≤M as stochastic integrals over the curve C: Tm(g) def= 1√M �C
gdBm 
where {Bm}m≤M are M independent Brownian motions deﬁned on C.
Note that up to an appropriate speed-preserving parametrization g :
[0  l] → X of C  we can
work with the corresponding induced family {ψk}k≤K   where ψk = ϕk ◦ g  instead of the fam-
ily {ϕk}k≤K .
The sensing method. With the choice of the linear operators {Tm}m≤M deﬁned above  the param-
eter α ∈ RK now satisﬁes the following equation

Aα = b  

(2)

def= Tm(f ) = 1√M �C
f (x)dBm(x) and the so-
where b ∈ RM is deﬁned by its components bm
def= Tm(ϕk). Note that we
called Brownian sensing matrix A (of size M × K) has elements Am k
do not require sampling f in order to compute the elements of A. Thus  the samples only serve for
estimating b and for this purpose  we sample f at points {xn}1≤n≤N regularly chosen along the
curve C.
In general  for a curve C parametrized with speed-preserving parametrization g : [0  l] → X of C 
we have xn = g( n

N l) and the resulting estimate�b ∈ RM of b is deﬁned with components:

yn(Bm(xn+1) − Bm(xn)) .

(3)

1
√M

N−1�n=0

�bm =

Note that in the special case when X = C = [0  1]  we simply have xn = n
N .
The ﬁnal step of the proposed method is to apply standard recovery techniques (e.g.  l1 minimization
or Lasso) to compute �α for the system (2) where b is perturbed by the so-called sensing noise
ε def= b −�b (estimation error of the stochastic integrals).

4

3.1 Properties of the transformed objects

We now give two properties of the Brownian sensing matrix A and the sensing noise ε = b −�b .
Brownian sensing matrix. By deﬁnition of the stochastic integral operators {Tm}m≤M   the sensing
matrix A = (Tm(ϕk))m k is a centered Gaussian matrix  with

Cov(Am k  Am k� ) =

ϕk(x)ϕk� (x)dx .

1

M �C

Moreover by independence of the Brownian motions  each row Am · is i.i.d. from a centered Gaus-
sian distribution N (0  1
M VC)  where VC is the K × K covariance matrix of the basis  deﬁned by its
elements Vk k� =�C
ϕk(x)ϕk� (x)dx. Thanks to this nice structure  we can prove that A possesses a

property similar to RIP (in the sense of [10]) whenever M is large enough:

Proposition 1 For p > 0 and any integer t > 0  when M > C�
4 (t log(K/t) + log 1/p))  with C�
being a universal constant (deﬁned in [14  1])  then with probability at least 1 − p  for all t−sparse
vectors x ∈ RK  

1
2

νmin C�x�2 ≤ �Ax�2 ≤

νmax C�x�2 

3
2

where νmax C and νmin C are respectively the largest and smallest eigenvalues of V 1/2
Sensing noise. In order to state our main result  we need a bound on �ε�2
2. We consider the simplest
deterministic sensing design where we choose the sensing points to be uniformly distributed along
the curve C2.
Proposition 2 Assume that �η�2

2 ≤ σ2 and that f is (L  β)-H¨older  i.e.

C

.

∀(x  y) ∈ X 2 |f (x) − f (y)| ≤ L|x − y|β  

then for any p ∈ (0  1]  with probability at least 1 − p  we have the following bound on the sensing
noise ε = b −�b:

˜σ2(N  M  p)

 

�ε�2
2 ≤
N 2β−1 + σ2��1 + 2
˜σ2(N  M  p) def= 2� L2l2β

N

log(1/p)

M

+ 4� log(1/p)
M � .

where

Remark 1 The bound on the sensing noise �ε�2
2 contains two contributions: an approximation
error term which comes from the approximation of a stochastic integral with N points and that
scales with L2l2β/N 2β  and the observation noise term of order σ2/N . The observation noise term
(when σ2 > 0) dominates the approximation error term whenever β ≥ 1/2.

3.2 Main result.
In this section  we state our main recovery result for the Brownian sensing method  described in

Figure 1  using a uniform sampling method along a one-dimensional curve C ⊂ X ⊂ Rd. The proof

of the following theorem can be found in the supplementary material.

Theorem 4 (Main result) Assume that f is (L  β)-H¨older on X and that VC is invertible. Let us
write the condition number κC = νmax C/νmin C  where νmax C and νmin C are respectively the
largest and smallest eigenvalues of V 1/2
. For any p ∈ (0  1]  let
M ≥ 4c(4Sr log( K
4Sr ) + log 1/p) (where c is a universal constant deﬁned in [14  1]). Then  with
probability at least 1 − 3p  the solution �α obtained by the Brownian sensing approach described in

. Write r = �(3κC − 1)(

Figure 1  satisﬁes

4√2−1

)�2

C

1

where C is a numerical constant and ˜σ(N  M  p) is deﬁned in Proposition 2.

��α − α�2

2 ≤ C�

κ4
C

maxk�C

k� ˜σ2(N  M  p)

N

ϕ2

 

Note that a similar result (not reported in this conference paper) can be proven in the case of
i.i.d. sub-Gaussian noise  instead of a noise with bounded �2 norm considered here.

2Note that other deterministic  random  or low-discrepancy sequence could be used here.

5

Input: a curve C of length l such that VC is invertible. Parameters N and M .

• Select N uniform samples {xn}1≤n≤N along the curve C 
• Generate M Brownian motions {Bm}1≤m≤M along C.
• Compute the Brownian sensing matrix A ∈ RM×K

ϕk(x)dBm(x)).

(i.e. Am k = 1√M �C
(i.e.�bm = 1√M �N−1

• Compute the estimate�b ∈ RM
• Find �α  solution to

n=0 yn(Bm(xn+1) − Bm(xn))).

min

a � �a�1 such that �Aa −�b�2

2 ≤

˜σ2(N  M  p)

N

� .

Figure 1: The Brownian sensing approach using a uniform sampling along the curve C.

4 Discussion.

In this section we discuss the differences with previous results  especially with the work [12] recalled
in Theorem 3. We then comment on the choice of the curve C and illustrate examples of such curves
for different bases.

4.1 Comparison with known results

2 = O( L2l2β

N 2β + σ2

2 = O( σ2

The order of the bound. Concerning the scaling of the estimation error in terms of the number
of sensing points N   Theorem 3 of [12] (reminded in Section 2) states that when N is large enough
N ). In comparison 
N ) for any values of N . Thus  provided that the

(i.e.  N = Ω(S log(K)))  we can build an estimate �α such that ��α − α�2
our bound shows that ��α − α�2
function f has a H¨older exponent β ≥ 1/2  we obtain the same rate as in Theorem 3.
A weak assumption about the basis. Note that our recovery performance scales with the condi-
tion number κC of VC as well as the length l of the curve C. However  concerning the hypothesis
on the functions {ϕk}k≤K   we only assume that the covariance matrix VC is invertible on the curve
C  which enables to handle arbitrarily non-orthonormal bases. This means that the orthogonality
condition on the basis functions is not a crucial requirement to deduce sparse recovery properties.
To the best of our knowledge  this is an improvement over previously known results (such as the
work of [12]). Note however that if κC or l are too high  then the bound becomes loose. Also the
computational complexity of the Brownian sensing increases when κC is large  since it is necessary
to take a large M   i.e. to simulate more Brownian motions in that case.

A result that holds without any conditions on the number of sampling points. Theorem 4
requires a constraint on the number of Brownian motions M (i.e.  that M = Ω(S log K)) and not
on the number of sampling points N (as in [12]  see Theorem 3). This is interesting in practical
situations when we do not know the value of S  as we do not have to assume a lower-bound on N
to deduce the estimation error result. This is due to the fact that the Brownian sensing matrix A
only depends on the computation of the M stochastic integrals of the K functions ϕk  and does not
depend on the samples. The bound shows that we should take M as large as possible. However  M
impacts the numerical cost of the method. This implies in practice a trade-off between a large M
for a good estimation of α and a low M for low numerical cost.

4.2 The choice of the curve
Why sampling along a 1-dimensional curve C instead of sampling over the whole space X ? In
a bounded space X of dimension 1  both approaches are identical. But in dimension d > 1  following
the Brownian sensing approach while sampling over the whole space would require generating M
Brownian sheets (extension of Brownian motions to d > 1 dimensions) over X   and then building

6

1 (t1)...dBm

ϕk(t1  ...td)dBm

the M × K matrix A with elements Am k = �X
d (td). Assuming that
the covariance matrix VX is invertible  this Brownian sensing matrix is also Gaussian and enjoys
the same recovery properties as in the one-dimensional case. However  in this case  estimating
the stochastic integrals bm = �X
f dBm using sensing points along a (d-dimensional) grid would
provide an estimation error ε = b−�b that scales poorly with d since we integrate over a d dimensional
space. This explains our choice of selecting a 1-dimensional curve C instead of the whole space X
and sampling N points along the curve. This choice provides indeed a better estimation of b which
is deﬁned by a 1-dimensional stochastic integrals over C. Note that the only requirement for the
choice of the curve C is that the covariance matrix VC deﬁned along this curve should be invertible.
In addition  in some speciﬁc applications the sampling process can be very constrained by physical
systems and sampling uniformly in all the domain is typically costly. For example in some medical
experiments  e.g.  scanner or I.R.M.  it is only possible to sample along straight lines.

In the result of Theorem 4  the length l of
What the parameters of the curve tell us on a basis.
the curve C as well as the condition number κC = νmax C/νmin C are essential characteristics of the
efﬁciency of the method. It is important to note that those two variables are actually related. Indeed 
it may not be possible to ﬁnd a short curve C such that κC is small. For instance in the case where
the basis functions have compact support  if the curve C does not pass through the support of all
functions  VC will not be invertible. Any function whose support does not intersect with the curve
would indeed be an eigenvector of VC with a 0 eigenvalue. This indicates that the method will not
work well in the case of a very localized basis {ϕk}k≤K (e.g. wavelets with compact support)  since
the curve would have to cover the whole domain and thus l will be very large. On the other hand 
the situation may be much nicer when the basis is not localized  as in the case of a Fourier basis.
We show in the next subsection that in a d-dimensional Fourier basis  it is possible to ﬁnd a curve C
(actually a segment) such that the basis is orthonormal along the chosen line (i.e. κC = 1).

4.3 Examples of curves
For illustration  we exhibit three cases for which one can easily derive a curve C such that VC is
invertible. The method described in the previous section will work with the following examples.

In this case  we simply take C = X   and the sparse recovery is possible

X is a segment of R:
whenever the functions {ϕk}k≤K are linearly independent in L2.
Coordinate functions: Consider
the case when the basis are the coordinate functions
Then we can deﬁne the parametrization of the curve C by g(t) =
ϕk(t1  ...td) = tk.
α(t)(t  t2  . . .   td)  where α(t) is the solution to a differential equation such that �g�(t)�2 = 1 (which
implies that for any function h � h ◦ g = �C
h). The corresponding functions ψk(t) = α(t)tk are
linearly independent  since the only functions α(t) such that the {ψk}k≤K are not linearly indepen-
dent are functions that are 0 almost everywhere  which would contradict the deﬁnition of α(t). Thus
VC is invertible.
Fourier basis: Let us now consider the Fourier basis in Rd with frequency T :

ϕn1 ... nd (t1  ..  td) =�j

exp� −

2iπnjtj

T

� 

where nj ∈ {0  ...  T − 1} and tj ∈ [0  1]. Note that this basis is orthonormal under the uniform
distribution on [0  1]d. In this case we deﬁne g by g(t) = λ(t
T d−1 ) with λ =
� 1−T −2
1−T −2d (so that �g�(t)�2 = 1)  thus we deduce that:

T d−1   ...  t T d−1

T d−1   t T

1

Since nk ∈ {0  ...  T − 1}  the mapping that associates �j njT j−1 to (n1  . . .   nd) is a bijection
from {0  . . .   T − 1}d to {0  . . .   T d − 1}. Thus we can identify the family (ψn1 ... nd ) with the one
dimensional Fourier basis with frequency T d
λ   which means that the condition number ρ = 1 for
this curve. Therefore  for a d-dimensional function f   sparse in the Fourier basis  it is sufﬁcient to
sample along the curve induced by g to ensure that VC is invertible.

ψn1 ... nd (t) = exp� −

2iπtλ�j njT j−1

T d

�.

7

5 Numerical Experiments

In this section  we illustrate the method of Brownian sensing in dimension one. We consider
a non-orthonormal family {ϕk}k≤K of K = 100 functions of L2([0  2π]) deﬁned by ϕk(t) =
. In the experiments  we use a function f whose decomposition is 3-sparse and
which is (10  1)-H¨older  and we consider a bounded observation noise η  with different noise levels 

cos(tk)+cos(t(k+1))

√2π

where the noise level is deﬁned by σ2 =�N

n=1 η2
n.

Comparison of l1−minimization and Brownian Sensing

Comparison of l1−minimization and Brownian Sensing

Comparison of l1−minimization and Brownian Sensing

200

180

160

140

120

100

80

60

40

20

r
o
r
r
e

 
c
i
t
a
r
d
a
u
q
n
a
e
m

 

0
5

10

15

with noise variance 0

20

25

30

35

number of sampling points

r
o
r
r
e

 
c
i
t
a
r
d
a
u
q
n
a
e

 

M

220

200

180

160

140

120

100

80

60

40
5

10

15

40

45

50

with noise variance 0.5

20

25

30

35

Number of sampling points

220

200

180

160

140

120

r
o
r
r
e

 
c
i
t
a
r
d
a
u
q
n
a
e
m

 

40

45

50

100
5

10

15

with noise variance 1

20

25

30

number of sampling points

35

40

45

50

Figure 2: Mean squared estimation error using Brownian sensing (plain curve) and a direct l1-
minimization solving Φα ≈ y (dashed line)  for different noise level (σ2 = 0  σ2 = 0.5  σ2 = 1) 
plotted as a function of the number of sample points N .

In Figure 2  the plain curve represents the recovery performance  i.e.  mean squared error  of Brow-

nian sensing i.e.  minimizing �a�1 under constraint that �Aa −�b�2 ≤ 1.95�2(100/N + 2) using

M = 100 Brownian motions and a regular grid of N points  as a function of N 3. The dashed curve
represents the mean squared error of a regular l1 minimization of �a�1 under the constraint that
�Φa − y�2
2 ≤ σ2 (as described e.g. in [12])  where the N samples are drawn uniformly randomly
over the domain. The three different graphics correspond to different values of the noise level σ2
(from left to right 0  0.5 and 1). Note that the results are averaged over 5000 trials.

Figure 2 illustrates that  as expected  Brownian sensing outperforms the method described in [12]
for noisy measurements4. Note also that the method described in [12] recovers the sparse vector
when there is no noise  and that Brownian sensing in this case has a smoother dependency w.r.t. N .
Note that this improvement comes from the fact that we use the H¨older regularity of the function:
Compressed sensing may outperform Brownian sensing for arbitrarily non regular functions.

Conclusion

In this paper  we have introduced a so-called Brownian sensing approach  as a way to sample an un-
known function which has a sparse representation on a given non-orthonormal basis. Our approach
differs from previous attempts to apply compressed sensing in the fact that we build a “Brownian
sensing” matrix A based on a set of Brownian motions  which is independent of the function f . This
enables us to guarantee nice recovery properties of A. The function evaluations are used to estimate
the right hand side term b (stochastic integrals). In dimension d we proposed to sample the function
along a well-chosen curve  i.e. such that the corresponding covariance matrix is invertible. We pro-

vided competitive reconstruction error rates of order O(�η�2/√N ) when the observation noise η is

bounded and f is assumed to be H¨older continuous with exponent at least 1/2. We believe that the
H¨older assumption is not strictly required (the smoothness of f is assumed to derive nice estimations
of the stochastic integrals only)  and future works will consider weakening this assumption  possibly
by considering randomized sampling designs.

3We assume that we know a loose bound on the noise level  here σ2 ≤ 2  and we take p = 0.01.
4Note however that there is no theoretical guarantee that the method described in [12] works here since the

functions are not orthonormal.

8

Acknowledgements

This research was partially supported by the French Ministry of Higher Education and Research 
Nord- Pas-de-Calais Regional Council and FEDER through CPER 2007-2013  ANR projects
EXPLO-RA (ANR-08-COSI-004) and Lampada (ANR-09-EMER-007)  by the European Com-
munitys Seventh Framework Programme (FP7/2007-2013) under grant agreement 231495 (project
CompLACS)  and by Pascal-2.

References

[1] R. Baraniuk  M. Davenport  R. DeVore  and M. Wakin. A simple proof of the restricted isom-

etry property for random matrices. Constructive Approximation  28(3):253–263  2008.

[2] G. Bennett. Probability inequalities for the sum of independent random variables. Journal of

the American Statistical Association  57(297):33–45  1962.

[3] E. Cand`es and J. Romberg. Sparsity and incoherence in compressive sampling. Inverse Prob-

lems  23:969–985  2007.

[4] E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger than

n. Annals of Statistics  35(6):2313–2351  2007.

[5] E.J. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information. IEEE Transactions on information theory 
52(2):489–509  2006.

[6] E.J. Cand`es  J.K. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate

measurements. Communications on Pure and Applied Mathematics  59(8):1207  2006.

[7] D.L. Donoho. Compressed sensing. IEEE Transactions on Information Theory  52(4):1289–

1306  2006.

[8] D.L. Donoho and P.B. Stark. Uncertainty principles and signal recovery. SIAM Journal on

Applied Mathematics  49(3):906–931  1989.

[9] M. Fornasier and H. Rauhut. Compressive Sensing.
Mathematical Methods in Imaging. Springer  to appear.

In O. Scherzer  editor  Handbook of

[10] S. Foucart and M.J. Lai.

Sparsest solutions of underdetermined linear systems via lq-
minimization for 0 < q < p. Applied and Computational Harmonic Analysis  26(3):395–407 
2009.

[11] V. Koltchinskii. The Dantzig selector and sparsity oracle inequalities. Bernoulli  15(3):799–

828  2009.

[12] H. Rauhut. Compressive Sensing and Structured Random Matrices. Theoretical Foundations

and Numerical Methods for Sparse Recovery  9  2010.

[13] H. Rauhut and R. Ward. Sparse legendre expansions via l1 minimization. Arxiv preprint

arXiv:1003.0251  2010.

[14] M. Rudelson and R. Vershynin. On sparse reconstruction from Fourier and Gaussian measure-

ments. Communications on Pure and Applied Mathematics  61(8):1025–1045  2008.

[15] Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal

Statistical Society  Series B  58:267–288  1994.

[16] Sara A. van de Geer. The deterministic lasso. Seminar f¨ur Statistik  Eidgen¨ossische Technische

Hochschule (ETH) Z¨urich  2007.

[17] Sara A. van de Geer and Peter Buhlmann. On the conditions used to prove oracle results for

the lasso. Electronic Journal of Statistics  3:1360–1392  2009.

[18] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning

Research  7:2563  2006.

9

,Akshay Krishnamurthy
Alekh Agarwal
John Langford