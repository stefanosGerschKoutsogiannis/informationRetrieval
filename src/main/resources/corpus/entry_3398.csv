2016,Measuring the reliability of MCMC inference with bidirectional Monte Carlo,Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference  but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods  which can hide details and obscure inference failures. In this work  we extend the recently introduced bidirectional Monte Carlo technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data  we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We integrate our method into two probabilistic programming languages  WebPPL and Stan  and validate it on several models and datasets. As an example of how our method be used to guide the design of inference algorithms  we apply it to study the effectiveness of different model representations in WebPPL and Stan.,Measuring the reliability of MCMC inference with

bidirectional Monte Carlo

Roger B. Grosse

Siddharth Ancha

Department of Computer Science

Department of Computer Science

University of Toronto

University of Toronto

Daniel M. Roy

Department of Statistics
University of Toronto

Abstract

Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic
inference  but it is notoriously hard to measure the quality of approximate posterior
samples. This challenge is particularly salient in black box inference methods 
which can hide details and obscure inference failures. In this work  we extend
the recently introduced bidirectional Monte Carlo [GGA15] technique to evaluate
MCMC-based posterior inference algorithms. By running annealed importance
sampling (AIS) chains both from prior to posterior and vice versa on simulated data 
we upper bound in expectation the symmetrized KL divergence between the true
posterior distribution and the distribution of approximate samples. We integrate
our method into two probabilistic programming languages  WebPPL [GS] and Stan
[CGHL+ p]  and validate it on several models and datasets. As an example of how
our method be used to guide the design of inference algorithms  we apply it to
study the effectiveness of different model representations in WebPPL and Stan.

1

Introduction

Markov chain Monte Carlo (MCMC) is one of the most important classes of probabilistic inference
methods and underlies a variety of approaches to automatic inference [e.g. LTBS00; GMRB+08;
GS; CGHL+ p]. Despite its widespread use  it is still difﬁcult to rigorously validate the effectiveness
of an MCMC inference algorithm. There are various heuristics for diagnosing convergence  but
reliable quantitative measures are hard to ﬁnd. This creates difﬁculties both for end users of automatic
inference systems and for experienced researchers who develop models and algorithms.
In this paper  we extend the recently proposed bidirectional Monte Carlo (BDMC) [GGA15] method
to evaluate certain kinds of MCMC-based inference algorithms by bounding the symmetrized KL
divergence (Jeffreys divergence) between the distribution of approximate samples and the true
posterior distribution. Speciﬁcally  our method is applicable to algorithms which can be viewed as
importance sampling over an extended state space  such as annealed importance sampling (AIS;
[Nea01]) or sequential Monte Carlo (SMC; [MDJ06]). BDMC was proposed as a method for
accurately estimating the log marginal likelihood (log-ML) on simulated data by sandwiching the true
value between stochastic upper and lower bounds which converge in the limit of inﬁnite computation.
These log-likelihood values were used to benchmark marginal likelihood estimators. We show that it
can also be used to measure the accuracy of approximate posterior samples obtained from algorithms
like AIS or SMC. More precisely  we reﬁne the analysis of [GGA15] to derive an estimator which
upper bounds in expectation the Jeffreys divergence between the distribution of approximate samples
and the true posterior distribution. We show that this upper bound is quite accurate on some toy
distributions for which both the true Jeffreys divergence and the upper bound can be computed exactly.
We refer to our method of bounding the Jeffreys divergence by sandwiching the log-ML as Bounding
Divergences with REverse Annealing (BREAD).

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

While our method is only directly applicable to certain algorithms such as AIS or SMC  these
algorithms involve many of the same design choices as traditional MCMC methods  such as the
choice of model representation (e.g. whether to collapse out certain variables)  or the choice of
MCMC transition operators. Therefore  the ability to evaluate AIS-based inference should also yield
insights which inform the design of MCMC inference algorithms more broadly.
One additional hurdle must be overcome to use BREAD to evaluate posterior inference: the method
yields rigorous bounds only for simulated data because it requires an exact posterior sample. One
would like to be sure that the results on simulated data accurately reﬂect the accuracy of posterior
inference on the real-world data of interest. We present a protocol for using BREAD to diagnose
inference quality on real-world data. Speciﬁcally  we infer hyperparameters on the real data  simulate
data from those hyperparameters  measure inference quality on the simulated data  and validate the
consistency of the inference algorithm’s behavior between the real and simulated data. (This protocol
is somewhat similar in spirit to the parametric bootstrap [ET98].)
We integrate BREAD into the tool chains of two probabilistic programming languages: WebPPL
[GS] and Stan [CGHL+ p]. Both probabilistic programming systems can be used as automatic
inference software packages  where the user provides a program specifying a joint probabilistic model
over observed and unobserved quantities. In principle  probabilistic programming has the potential to
put the power of sophisticated probabilistic modeling and efﬁcient statistical inference into the hands
of non-experts  but realizing this vision is challenging because it is difﬁcult for a non-expert user
to judge the reliability of results produced by black-box inference. We believe BREAD provides a
rigorous  general  and automatic procedure for monitoring the quality of posterior inference  so that
the user of a probabilistic programming language can have conﬁdence in the accuracy of the results.
Our approach to evaluating probabilistic programming inference is closely related to independent
work [CTM16] that is also based on the ideas of BDMC. We discuss the relationships between both
methods in Section 4.
In summary  this work includes four main technical contributions. First  we show that BDMC yields
an estimator which upper bounds in expectation the Jeffreys divergence of approximate samples
from the true posterior. Second  we present a technique for exactly computing both the true Jeffreys
divergence and the upper bound on small examples  and show that the upper bound is often a
good match in practice. Third  we propose a protocol for using BDMC to evaluate the accuracy
of approximate inference on real-world datasets. Finally  we extend both WebPPL and Stan to
implement BREAD  and validate BREAD on a variety of probabilistic models in both frameworks.
As an example of how BREAD can be used to guide modeling and algorithmic decisions  we use
it to analyze the effectiveness of different representations of a matrix factorization model in both
WebPPL and Stan.

2 Background

2.1 Annealed Importance Sampling

Annealed importance sampling (AIS; [Nea01]) is a Monte Carlo algorithm commonly used to estimate
(ratios of) normalizing constants. More carefully  ﬁx a sequence of T distributions p1  . . .   pT   with
pt(x) = ft(x)/Zt. The ﬁnal distribution in the sequence  pT   is called the target distribution; the
ﬁrst distribution  p1  is called the initial distribution. It is required that one can obtain one or more
exact samples from p1.1 Given a sequence of reversible MCMC transition operators T1  . . .  TT  
where Tt leaves pt invariant  AIS produces a (nonnegative) unbiased estimate of ZT /Z1 as follows:
ﬁrst  we sample a random initial state x1 from p1 and set the initial weight w1 = 1. For every stage
t  2 we update the weight w and sample the state xt according to

wt wt1

ft(xt1)
ft1(xt1)

xt sample from Tt (x| xt1) .

(1)

Neal [Nea01] justiﬁed AIS by showing that it is a simple importance sampler over an extended state
space (see Appendix A for a derivation in our notation). From this analysis  it follows that the weight
wT is an unbiased estimate of the ratio ZT /Z1. Two trivial facts are worth highlighting: when Z1
1Traditionally  this has meant having access to an exact sampler. However  in this work  we sometimes have

access to a sample from p1  but not a sampler.

2

is known  Z1wT is an unbiased estimate of ZT   and when ZT is known  wT /ZT is an unbiased
estimate of 1/Z1. In practice  it is common to repeat the AIS procedure to produce K independent
estimates and combine these by simple averaging to reduce the variance of the overall estimate.
In most applications of AIS  the normalization constant ZT for the target distribution pT is the
focus of attention  and the initial distribution p1 is chosen to have a known normalization constant
Z1. Any sequence of intermediate distributions satisfying a mild domination criterion sufﬁces to
produce a valid estimate  but in typical applications  the intermediate distributions are simply deﬁned
to be geometric averages ft(x) = f1(x)1tfT (x)t  where the t are monotonically increasing
parameters with 1 = 0 and T = 1. (An alternative approach is to average moments [GMS13].)
In the setting of Bayesian posterior inference over parameters ✓ and latent variables z given some
ﬁxed observation y  we take f1(✓  z) = p(✓  z) to be the prior distribution (hence Z1 = 1)  and we
take fT (✓  z) = p(✓  z  y) = p(✓  z) p(y|✓  z). This can be viewed as the unnormalized posterior
distribution  whose normalizing constant ZT = p(y) is the marginal likelihood. Using geometric
averaging  the intermediate distributions are then
(2)
In addition to moment averaging  reasonable intermediate distributions can be produced in the
Bayesian inference setting by conditioning on a sequence of increasing subsets of data; this insight
relates AIS to the seemingly different class of sequential Monte Carlo (SMC) methods [MDJ06].

ft(✓  z) = p(✓  z) p(y|✓  z)t.

2.2 Stochastic lower bounds on the log partition function ratio
AIS produces a nonnegative unbiased estimate ˆR of the ratio R = ZT /Z1 of partition functions.
Unfortunately  because such ratios often vary across many orders of magnitude  it frequently happens
that ˆR underestimates R with overwhelming probability  while occasionally taking extremely large
values. Correspondingly  the variance may be extremely large  or even inﬁnite.
For these reasons  it is more meaningful to estimate log R. Unfortunately  the logarithm of a
nonnegative unbiased estimate (such as the AIS estimate) is  in general  a biased estimator of the log
estimand. More carefully  let ˆA be a nonnegative unbiased estimator for A = E[ ˆA]. Then  by Jensen’s
inequality  E[log ˆA]  log E[ ˆA] = log A  and so log ˆA is a lower bound on log A in expectation. The
estimator log ˆA satisﬁes another important property: by Markov’s inequality for nonnegative random
variables  Pr(log ˆA > log A + b) < eb  and so log ˆA is extremely unlikely to overestimate log A
by any appreciable number of nats. These observations motivate the following deﬁnition [BGS15]: a
stochastic lower bound on X is an estimator ˆX satisfying E[ ˆX]  X and Pr( ˆX > X + b) < eb.
Stochastic upper bounds are deﬁned analogously. The above analysis shows that log ˆA is a stochastic
lower bound on log A when ˆA is a nonnegative unbiased estimate of A  and  in particular  log ˆR is a
stochastic lower bound on log R. (It is possible to strengthen the tail bound by combining multiple
samples [GBD07].)

2.3 Reverse AIS and Bidirectional Monte Carlo
Upper and lower bounds are most useful in combination  as one can then sandwich the true value. As
described above  AIS produces a stochastic lower bound on the ratio R; many other algorithms do as
well. Upper bounds are more challenging to obtain. The key insight behind bidirectional Monte Carlo
(BDMC; [GGA15]) is that  provided one has an exact sample from the target distribution pT   one can
run AIS in reverse to produce a stochastic lower bound on log Rrev = log Z1/ZT   and therefore a
stochastic upper bound on log R =  log Rrev. (In fact  BDMC is a more general framework which
allows a variety of partition function estimators  but we focus on AIS for pedagogical purposes.)
More carefully  for t = 1  . . .   T   deﬁne ˜pt = pTt+1 and ˜Tt = TTt+1. Then ˜p1 corresponds
to our original target distribution pT and ˜pT corresponds to our original initial distribution p1. As
before  ˜Tt leaves ˜pt invariant. Consider the estimate produced by AIS on the sequence of distributions
˜p1  . . .   ˜pT and corresponding MCMC transition operators ˜T1  . . .   ˜TT . (In this case  the forward
chain of AIS corresponds to the reverse chain described in Section 2.1.) The resulting estimate ˆRrev
is a nonnegative unbiased estimator of Rrev. It follows that log ˆRrev is a stochastic lower bound
on log Rrev  and therefore log ˆR1
rev. BDMC is

rev is a stochastic upper bound on log R = log R1

3

simply the combination of this stochastic upper bound with the stochastic lower bound of Section 2.2.
Because AIS is a consistent estimator of the partition function ratio under the assumption of ergodicity
[Nea01]  the two bounds converge as T ! 1; therefore  given enough computation  BDMC can
sandwich log R to arbitrary precision.
Returning to the setting of Bayesian inference  given some ﬁxed observation y  we can apply BDMC
provided we have exact samples from both the prior distribution p(✓  z) and the posterior distribution
p(✓  z|y). In practice  the prior is typically easy to sample from  but it is typically infeasible to
generate exact posterior samples. However  in models where we can tractably sample from the joint
distribution p(✓  z  y)  we can generate exact posterior samples for simulated observations using the
elementary fact that

p(y) p(✓  z|y) = p(✓  z  y) = p(✓  z) p(y|✓  z).

(3)
In other words  if one ancestrally samples ✓  z  and y  this is equivalent to ﬁrst generating a dataset y
and then sampling (✓  z) exactly from the posterior. Therefore  for simulated data  one has access to a
single exact posterior sample; this is enough to obtain stochastic upper bounds on log R = log p(y).
2.4 WebPPL and Stan
We focus on two particular probabilistic programming packages. First  we consider WebPPL [GS]  a
lightweight probabilistic programming language built on Javascript  and intended largely to illustrate
some of the important ideas in probabilistic programming. Inference is based on Metropolis–Hastings
(M–H) updates to a program’s execution trace  i.e. a record of all stochastic decisions made by the
program. WebPPL has a small and clean implementation  and the entire implementation is described
in an online tutorial on probabilistic programming [GS].
Second  we consider Stan [CGHL+ p]  a highly engineered automatic inference system which is
widely used by statisticians and is intended to scale to large problems. Stan is based on the No U-Turn
Sampler (NUTS; [HG14])  a variant of Hamiltonian Monte Carlo (HMC; [Nea+11]) which chooses
trajectory lengths adaptively. HMC can be signiﬁcantly more efﬁcient than M–H over execution
traces because it uses gradient information to simultaneously update multiple parameters of a model 
but is less general because it requires a differentiable likelihood. (In particular  this disallows discrete
latent variables unless they are marginalized out analytically.)

3 Methods

There are at least two criteria we would desire from a sampling-based approximate inference algorithm
in order that its samples be representative of the true posterior distribution: we would like the
approximate distribution q(✓  z; y) to cover all the high-probability regions of the posterior p(✓  z|y) 
and we would like it to avoid placing probability mass in low-probability regions of the posterior. The
former criterion motivates measuring the KL divergence DKL(p(✓  z|y)k q(✓  z; y))  and the latter
criterion motivates measuring DKL(q(✓  z; y)k p(✓  z|y)). If we desire both simultaneously  this
motivates paying attention to the Jeffreys divergence  deﬁned as DJ(qkp) = DKL(qkp) + DKL(pkq).
In this section  we present Bounding Divergences with Reverse Annealing (BREAD)  a technique for
using BDMC to bound the Jeffreys divergence from the true posterior on simulated data  combined
with a protocol for using this technique to analyze sampler accuracy on real-world data.

3.1 Upper bounding the Jeffreys divergence in expectation
We now present our technique for bounding the Jeffreys divergence between the target distribution
and the distribution of approximate samples produced by AIS. In describing the algorithm  we revert
to the abstract state space formalism of Section 2.1  since the algorithm itself does not depend
on any structure speciﬁc to posterior inference (except for the ability to obtain an exact sample).
We ﬁrst repeat the derivation from [GGA15] of the bias of the stochastic lower bound log ˆR. Let
v = (x1  . . .   xT1) denote all of the variables sampled in AIS before the ﬁnal stage; the ﬁnal state
xT corresponds to the approximate sample produced by AIS. We can write the distributions over the
forward and reverse AIS chains as:

qf wd(v  xT ) = qf wd(v) qf wd(xT |v)
qrev(v  xT ) = pT (xT ) qrev(v|xT ).

4

(4)
(5)

The distribution of approximate samples qf wd(xT ) is obtained by marginalizing out v. Note that
sampling from qrev requires sampling exactly from pT   so strictly speaking  BREAD is limited
to those cases where one has at least one exact sample from pT — such as simulated data from a
probabilistic model (see Section 2.3).
The expectation of the estimate log ˆR of the log partition function ratio is given by:

E[log ˆR] = Eqf wd(v xT )log

fT (xT ) qrev(v|xT )

Z1 qf wd(v  xT ) 

(6)

= log ZT  log Z1  DKL(qf wd(xT ) qf wd(v|xT )k pT (xT ) qrev(v|xT ))
 log ZT  log Z1  DKL(qf wd(xT )k pT (xT )).

(7)
(8)
(Note that qf wd(v|xT ) is the conditional distribution of the forward chain  given that the ﬁnal state is
xT .) The inequality follows because marginalizing out variables cannot increase the KL divergence.
We now go beyond the analysis in [GGA15]  to bound the bias in the other direction. The expectation
of the reverse estimate ˆRrev is
E[log ˆRrev] = Eqrev(xT  v)log Z1 qf wd(v  xT )
fT (xT ) qrev(v|xT )

(9)

= log Z1  log ZT  DKL(pT (xT ) qrev(v|xT )k qf wd(xT ) qf wd(v|xT ))
 log Z1  log ZT  DKL(pT (xT )k qf wd(xT )).

(10)
(11)

rev can both be seen as estimators of log ZT
Z1

As discussed above  log ˆR and log ˆR1
  the former of
which is a stochastic lower bound  and the latter of which is a stochastic upper bound. Consider the
gap between these two bounds  ˆB   log ˆR1
rev  log ˆR. It follows from Eqs. (8) and (11) that  in
expectation  ˆB upper bounds the Jeffreys divergence
(12)

J   DJ(pT (xT )  qf wd(xT ))   DKL(pT (xT )k qf wd(xT )) + DKL(qf wd(xT )k pT (xT ))

between the target distribution pT and the distribution qf wd(pT ) of approximate samples.
Alternatively  if one happens to have some other lower bound L or upper bound U on log R  then one
can bound either of the one-sided KL divergences by running only one direction of AIS. Speciﬁcally 
from Eq. (8)  E[U  log ˆR]  DKL(qf wd(xT )k pT (xT ))  and from Eq. (11)  E[log ˆR1
rev  L] 
DKL(pT (xT )k qf wd(xT )).
How tight is the expectation B   E[ ˆB] as an upper bound on J ? We evaluated both B and J exactly
on some toy distributions and found them to be a fairly good match. Details are given in Appendix B.

3.2 Application to real-world data
So far  we have focused on the setting of simulated data  where it is possible to obtain an exact
posterior sample  and then to rigorously bound the Jeffreys divergence using BDMC. However  we
are more likely to be interested in evaluating the performance of inference on real-world data  so
we would like to simulate data which resembles a real-world dataset of interest. One particular
difﬁculty is that  in Bayesian analysis  hyperparameters are often assigned non-informative or weakly
informative priors  in order to avoid biasing the inference. This poses a challenge for BREAD  as
datasets generated from hyperparameters sampled from such priors (which are often very broad) can
be very dissimilar to real datasets  and hence conclusions from the simulated data may not generalize.
In order to generate simulated datasets which better match a real-world dataset of interest  we adopt
the following heuristic scheme: we ﬁrst perform approximate posterior inference on the real-world
dataset. Let ˆ⌘real denote the estimated hyperparameters. We then simulate parameters and data from
the forward model p(✓| ˆ⌘real)p(D| ˆ⌘real  ✓). The forward AIS chain is run on D in the usual way.
However  to initialize the reverse chain  we ﬁrst start with (ˆ⌘real  ✓)  and then run some number of
MCMC transitions which preserve p(⌘  ✓|D)  yielding an approximate posterior sample (⌘?  ✓?).
In general  (⌘?  ✓?) will not be an exact posterior sample  since ˆ⌘real was not sampled from p(⌘|D).
However  the true hyperparameters ˆ⌘real which generated D ought to be in a region of high posterior
mass unless the prior p(⌘) concentrates most of its mass away from ˆ⌘real. Therefore  we expect even
a small number of MCMC steps to produce a plausible posterior sample. This motivates our use of
(⌘?  ✓?) in place of an exact posterior sample. We validate this procedure in Section 5.1.2.

5

4 Related work

Much work has been devoted to the diagnosis of Markov chain convergence (e.g. [CC96; GR92;
BG98]). Diagnostics have been developed both for estimating the autocorrelation function of
statistics of interest (which determines the number of effective samples from an MCMC chain) and
for diagnosing whether Markov chains have reached equilibrium. In general  convergence diagnostics
cannot conﬁrm convergence; they can only identify particular forms of non-convergence. By contrast 
BREAD can rigorously demonstrate convergence in the simulated data setting.
There has also been much interest in automatically conﬁguring parameters of MCMC algorithms.
Since it is hard to reliably summarize the performance of an MCMC algorithm  such automatic
conﬁguration methods typically rely on method-speciﬁc analyses. For instance  Roberts and Rosenthal
[RR01] showed that the optimal acceptance rate of Metropolis–Hastings with an isotropic proposal
distribution is 0.234 under fairly general conditions. M–H algorithms are sometimes tuned to
achieve this acceptance rate  even in situations where the theoretical analysis doesn’t hold. Rigorous
convergence measures might enable more direct optimization of algorithmic hyperparameters.
Gorham and Mackey [GM15] presented a method for directly estimating the quality of a set of
approximate samples  independently of how those samples were obtained. This method has strong
guarantees under a strong convexity assumption. By contrast  BREAD makes no assumptions about
the distribution itself  so its mathematical guarantees (for simulated data) are applicable even to
multimodal or badly conditioned posteriors.
It has been observed that heating and cooling processes yield bounds on log-ratios of partition
functions by way of ﬁnite difference approximations to thermodynamic integration. Neal [Nea96]
used such an analysis to motivate tempered transitions  an MCMC algorithm based on heating and
cooling a distribution. His analysis cannot be directly applied to measuring convergence  as it assumed
equilibrium at each temperature. Jarzynski [Jar97] later gave a non-equilibrium analysis which is
equivalent to that underlying AIS [Nea01].
We have recently learned of independent work [CTM16] which also builds on BDMC to evaluate the
accuracy of posterior inference in a probabilistic programming language. In particular  Cusumano-
Towner and Mansinghka [CTM16] deﬁne an unbiased estimator for a quantity called the subjective
divergence. The estimator is equivalent to BDMC except that the reverse chain is initialized from an
arbitrary reference distribution  rather than the true posterior. In [CTM16]  the subjective divergence
is shown to upper bound the Jeffreys divergence when the true posterior is used; this is equivalent to
our analysis in Section 3.1. Much less is known about subjective divergence when the reference distri-
bution is not taken to be the true posterior. (Our approximate sampling scheme for hyperparameters
can be viewed as a kind of reference distribution.)

5 Experiments

In order to experiment with BREAD  we extended both WebPPL and Stan to run forward and reverse
AIS using the sequence of distributions deﬁned in Eq. (2). The MCMC transition kernels were the
standard ones provided by both platforms. Our ﬁrst set of experiments was intended to validate that
BREAD can be used to evaluate the accuracy of posterior inference in realistic settings. Next  we
used BREAD to explore the tradeoffs between two different representations of a matrix factorization
model in both WebPPL and Stan.

5.1 Validation

As described above  BREAD returns rigorous bounds on the Jeffreys divergence only when the data
are sampled from the model distribution. Here  we address three ways in which it could potentially
give misleading results. First  the upper bound B may overestimate the true Jeffreys divergence J .
Second  results on simulated data may not correspond to results on real-world data if the simulated
data are not representative of the real-world data. Finally  the ﬁtted hyperparameter procedure of
Section 3.2 may not yield a sample sufﬁciently representative of the true posterior p(✓  ⌘|D). The
ﬁrst issue  about the accuracy of the bound  is addressed in Appendix B.1.1; the bound appears to be
fairly close to the true Jeffreys divergence on some toy distributions. We address the other two issues
in this section. In particular  we attempted to validate that the behavior of the method on simulated

6

(a)

(b)

(c)

Figure 1: (a) Validation of the consistency of the behavior of forward AIS on real and simulated data for
the logistic regression model. Since the log-ML values need not match between the real and simulated data 
the y-axes for each curve are shifted based on the maximum log-ML lower bound obtained by forward AIS.
(b) Same as (a)  but for matrix factorization. The complete set of results on all datasets is given in Appendix D.
(c) Validation of the ﬁtted hyperparameter scheme on the logistic regression model (see Section 5.1.2 for details).
Reverse AIS curves are shown as the number of Gibbs steps used to initialize the hyperparameters is varied.

data is consistent with that on real data  and that the ﬁtted-hyperparameter samples can be used as a
proxy for samples from the posterior. All experiments in this section were performed using Stan.

5.1.1 Validating consistency of inference behavior between real and simulated data
To validate BREAD in a realistic setting  we considered ﬁve models based on examples from the Stan
manual [Sta]  and chose a publicly available real-world dataset for each model. These models include:
linear regression  logistic regression  matrix factorization  autoregressive time series modeling  and
mixture-of-Gaussians clustering. See Appendix C for model details and Stan source code.
In order to validate the use of simulated data as a proxy for real data in the context of BREAD 
we ﬁt hyperparameters to the real-world datasets and simulated data from those hyperparameters 
as described in Section 3.2. In Fig. 1 and Appendix D  we show the distributions of forward and
reverse AIS estimates on simulated data and forward AIS estimates on real-world data  based on 100
AIS chains for each condition.2 Because the distributions of AIS estimates included many outliers 
we visualize quartiles of the estimates rather than means.3 The real and simulated data need not
have the same marginal likelihood  so the AIS estimates for real and simulated data are shifted
vertically based on the largest forward AIS estimate obtained for each model. For all ﬁve models
under consideration  the forward AIS curves were nearly identical (up to a vertical shift)  and the
distributions of AIS estimates were very similar at each number of AIS steps. (An example where the
forward AIS curves failed to match up due to model misspeciﬁcation is given in Appendix D.) Since
the inference behavior appears to match closely between the real and simulated data  we conclude
that data simulated using ﬁtted hyperparameters can be a useful proxy for real data when evaluating
inference algorithms.

5.1.2 Validating the approximate posterior over hyperparameters
As described in Section 3.2  when we simulate data from ﬁtted hyperparameters  we use an ap-
proximate (rather than exact) posterior sample (⌘?  ✓?) to initialize the reverse chain. Because of
this  BREAD is not mathematically guaranteed to upper bound the Jeffreys divergence even on the
simulated data. In order to determine the effect of this approximation in practice  we repeated the
procedure of Section 5.1.1 for all ﬁve models  but varying S  the number of MCMC steps used to
obtain (⌘?  ✓?)  with S 2 {10  100  1000  10000}. The reverse AIS estimates are shown in Fig. 1
and Appendix D. (We do not show the forward AIS estimates because these are unaffected by S.) In
all ﬁve cases  the reverse AIS curves were statistically indistinguishable. This validates our use of
ﬁtted hyperparameters  as it suggests that the use of approximate samples of hyperparameters has
little impact on the reverse AIS upper bounds.

2The forward AIS chains are independent  while the reverse chains share an initial state.
3Normally  such outliers are not a problem for AIS  because one averages the weights wT   and this average is
insensitive to extremely small values. Unfortunately  the analysis of Section 3.1 does not justify such averaging 
so we report estimates corresponding to individual AIS chains.

7

(a)

(b)

Figure 2: Comparison of Jeffreys divergence bounds for matrix factorization in Stan and WebPPL  using the
collapsed and uncollapsed formulations. Given as a function of (a) number of MCMC steps  (b) running time.

5.2 Scientiﬁc ﬁndings produced by BREAD
Having validated various aspects of BREAD  we applied it to investigate the choice of model represen-
tation in Stan and WebPPL. During our investigation  we also uncovered a bug in WebPPL  indicating
the potential usefulness of BREAD as a means of testing the correctness of an implementation.

5.2.1 Comparing model representations
Many models can be written in more than one way  for example by introducing or collapsing latent
variables. Performance of probabilistic programming languages can be sensitive to such choices of
representation  and the representation which gives the best performance may vary from one language
to another. We consider the matrix factorization model described above  which we now specify in
more detail. We approximate an N ⇥ D matrix Y as a low rank matrix  the product of matrices
U and V with dimensions N ⇥ K and K ⇥ D respectively (where K < min(N  D)). We use a
spherical Gaussian observation model  and spherical Gaussian priors on U and V:

uik ⇠ N (0  2
u)

vkj ⇠ N (0  2
v)

yij | ui  vj ⇠ N (u>i vj  2)

v) and yi | V ⇠ N (0  uV>V + I).
We can also collapse U to obtain the model vkj ⇠ N (0  2
In general  collapsing variables can help MCMC samplers mix faster at the expense of greater
computational cost per update. The precise tradeoff can depend on the size of the model and dataset 
the choice of MCMC algorithm  and the underlying implementation  so it would be useful to have a
quantitative criterion to choose between them.
We ﬁxed the values of all hyperparameters to 1  and set N = 50  K = 5 and D = 25. We ran
BREAD on both platforms (Stan and WebPPL) and for both formulations (collapsed and uncollapsed)
(see Fig. 2). The simulated data and exact posterior sample were shared between all conditions in
order to make the results directly comparable.
As predicted  the collapsed sampler resulted in slower updates but faster convergence (in terms of
the number of steps). However  the per-iteration convergence beneﬁt of collapsing was much larger
in WebPPL than in Stan (perhaps because of the different underlying inference algorithm). Overall 
the tradeoff between efﬁciency and convergence speed appears to favour the uncollapsed version in
Stan  and the collapsed version in WebPPL (see Fig. 2(b)). (Note that this result holds only for our
particular choice of problem size; the tradeoff may change given different model or dataset sizes.)
Hence BREAD can provide valuable insights into the tricky question of which representations of
models to choose to achieve faster convergence.

5.2.2 Debugging
Mathematically  the forward and reverse AIS chains yield lower and upper bounds on log p(y) with
high probability; if this behavior is not observed  that indicates a bug. In our experimentation with
WebPPL  we observed a case where the reverse AIS chain yielded estimates signiﬁcantly lower than
those produced by the forward chain  inconsistent with the theoretical guarantee. This led us to
ﬁnd a subtle bug in how WebPPL sampled from a multivariate Gaussian distribution (which had the
effect that the exact posterior samples used to initialize the reverse chain were incorrect).4 These
days  while many new probabilistic programming languages are emerging and many are in active
development  such debugging capabilities provided by BREAD can potentially be very useful.

4Issue: https://github.com/probmods/webppl/issues/473

8

References

[BG98]

[BGS15]

[CC96]

[CGHL+ p]

[CTM16]

[ET98]

[GBD07]

[GGA15]

[GM15]

S. P. Brooks and A. Gelman. “General methods for monitoring convergence of
iterative simulations”. Journal of Computational and Graphical Statistics 7.4 (1998) 
pp. 434–455.
Y. Burda  R. B. Grosse  and R. Salakhutdinov. “Accurate and conservative estimates of
MRF log-likelihood using reverse annealing”. In: Artiﬁcial Intelligence and Statistics.
2015.
M. K. Cowles and B. P. Carlin. “Markov chain Monte Carlo convergence diagnostics:
a comparative review”. Journal of the American Statistical Association 91.434 (1996) 
pp. 883–904.
B. Carpenter  A. Gelman  M. Hoffman  D. Lee  B. Goodrich  M. Betancourt  M. A.
Brubaker  J. Guo  P. Li  and A. Riddell. “Stan: a probabilistic programming language”.
Journal of Statistical Software (in press).
M. F. Cusumano-Towner and V. K. Mansinghka. Quantifying the probable approxi-
mation error of probabilistic inference programs. arXiv:1606.00068. 2016.
B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall/CRC 
1998.
V. Gogate  B. Bidyuk  and R. Dechter. “Studies in lower bounding probability of
evidence using the Markov inequality”. In: Conference on Uncertainty in AI. 2007.
R. B. Grosse  Z. Ghahramani  and R. P. Adams. Sandwiching the marginal likelihood
with bidirectional Monte Carlo. arXiv:1511.02543. 2015.
J. Gorham and L. Mackey. “Measuring sample quality with Stein’s method”. In:
Neural Information Processing Systems. 2015.

[GR92]

[GS]

[HG14]

[Jar97]

[GMS13]

[LTBS00]

[GMRB+08] N. D. Goodman  V. K. Mansinghka  D. M. Roy  K. Bonawitz  and J. B. Tenenbaum.
“Church: a language for generative models”. In: Conference on Uncertainty in AI.
2008.
R. Grosse  C. J. Maddison  and R. Salakhutdinov. “Annealing between distributions
by averaging moments”. In: Neural Information Processing Systems. 2013.
A. Gelman and D. B. Rubin. “Inference from iterative simulation using multiple
sequences”. Statistical Science 7.4 (1992)  pp. 457–472.
N. D. Goodman and A. Stuhlmüller. The Design and Implementation of Probabilistic
Programming Languages. http://dippl.org.
M. D. Homan and A. Gelman. “The No-U-turn Sampler: Adaptively Setting Path
Lengths in Hamiltonian Monte Carlo”. J. Mach. Learn. Res. 15.1 (Jan. 2014) 
pp. 1593–1623. ISSN: 1532-4435.
C. Jarzynski. “Equilibrium free-energy differences from non-equilibrium measure-
ments: a master-equation approach”. Physical Review E 56 (1997)  pp. 5018–5035.
D. J. Lunn  A. Thomas  N. Best  and D. Spiegelhalter. “WinBUGS – a Bayesian mod-
elling framework: concepts  structure  and extensibility”. Statistics and Computing
10.4 (2000)  pp. 325–337.
P. del Moral  A. Doucet  and A. Jasra. “Sequential Monte Carlo samplers”. Journal of
the Royal Statistical Society: Series B (Statistical Methodology) 68.3 (2006)  pp. 411–
436.
R. M. Neal et al. “MCMC using Hamiltonian dynamics”. Handbook of Markov Chain
Monte Carlo 2 (2011)  pp. 113–162.
R. M. Neal. “Annealed importance sampling”. Statistics and Computing 11 (2001) 
pp. 125–139.
R. M. Neal. “Sampling from multimodal distributions using tempered transitions”.
Statistics and Computing 6.4 (1996)  pp. 353–366.
G. O. Roberts and J. S. Rosenthal. “Optimal scaling for various Metropolis–Hastings
algorithms”. Statistical Science 16.4 (2001)  pp. 351–367.
Stan Modeling Language Users Guide and Reference Manual. Stan Development
Team.

[Nea+11]

[Nea01]

[Nea96]

[RR01]

[MDJ06]

[Sta]

9

,Roger Grosse
Siddharth Ancha
Daniel Roy