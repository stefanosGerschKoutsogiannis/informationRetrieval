2011,Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation,Many machine learning and signal processing problems can be formulated as linearly constrained convex programs  which could be efficiently solved by the alternating direction method (ADM). However  usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue  we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence  we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example  we apply LADMAP to solve low-rank representation (LRR)  which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique  we are able to reduce the complexity $O(n^3)$  of the original ADM based method to $O(rn^2)$  where $r$ and $n$ are the rank and size of the representation matrix  respectively  hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms.,Linearized Alternating Direction Method with
Adaptive Penalty for Low-Rank Representation

Zhouchen Lin

Visual Computing Group
Microsoft Research Asia

Risheng Liu
Zhixun Su
School of Mathematical Sciences
Dalian University of Technology

Abstract

Many machine learning and signal processing problems can be formulated as lin-
early constrained convex programs  which could be efﬁciently solved by the alter-
nating direction method (ADM). However  usually the subproblems in ADM are
easily solvable only when the linear mappings in the constraints are identities. To
address this issue  we propose a linearized ADM (LADM) method by linearizing
the quadratic penalty term and adding a proximal term when solving the sub-
problems. For fast convergence  we also allow the penalty to change adaptively
according a novel update rule. We prove the global convergence of LADM with
adaptive penalty (LADMAP). As an example  we apply LADMAP to solve low-
rank representation (LRR)  which is an important subspace clustering technique
yet suffers from high computation cost. By combining LADMAP with a skinny
SVD representation technique  we are able to reduce the complexity O(n3) of
the original ADM based method to O(rn2)  where r and n are the rank and size
of the representation matrix  respectively  hence making LRR possible for large
scale applications. Numerical experiments verify that for LRR our LADMAP
based methods are much faster than state-of-the-art algorithms.

1 Introduction

Recently  compressive sensing [5] and sparse representation [19] have been hot research topics and
also have found abundant applications in signal processing and machine learning. Many of the
problems in these ﬁelds can be formulated as the following linearly constrained convex programs:
(1)

f (x) + g(y); s:t: A(x) + B(y) = c;

min
x;y

where x  y and c could be either vectors or matrices  f and g are convex functions (e.g.  the nuclear
norm ∥ · ∥∗ [2]  Frobenius norm ∥ · ∥  l2;1 norm ∥ · ∥2;1 [13]  and l1 norm ∥ · ∥1)  and A and B are
linear mappings.
Although the interior point method can be used to solve many convex programs  it may suffer from
unbearably high computation cost when handling large scale problems. For example  when using
CVX  an interior point based toolbox  to solve nuclear norm minimization (namely  f (X) = ∥X∥∗
in (1)) problems  such as matrix completion [4]  robust principal component analysis [18] and their
combination [3]  the complexity of each iteration is O(n6)  where n × n is the matrix size. To over-
come this issue  ﬁrst-order methods are often preferred. The accelerated proximal gradient (APG)
−2) convergence rate  where k is
algorithm [16] is a popular technique due to its guaranteed O(k
the iteration number. The alternating direction method (ADM) has also regained a lot of atten-
tion [11  15]. It updates the variables alternately by minimizing the augmented Lagrangian function
with respect to the variables in a Gauss-Seidel manner. While APG has to convert (1) into an approx-
imate unconstrained problem by adding the linear constraints to the objective function as a penalty 
hence only producing an approximate solution to (1)  ADM can solve (1) exactly. However  when

1

A or B is not the identity mapping  the subproblems in ADM may not have closed form solutions.
So solving them is cumbersome.
In this paper  we propose a linearized version of ADM (LADM) to overcome the difﬁculty in solving
subproblems. It is to replace the quadratic penalty term by linearizing the penalty term and adding
a proximal term. We also allow the penalty parameter to change adaptively and propose a novel
and simple rule to update it. Linearization makes the auxiliary variables unnecessary  hence saving
memory and waiving the expensive matrix inversions to update the auxiliary variables. Moreover 
without the extra constraints introduced by the auxiliary variables  the convergence is also faster.
Using a variable penalty parameter further speeds up the convergence. The global convergence of
LADM with adaptive penalty (LADMAP) is also proven.
As an example  we apply our LADMAP to solve the low-rank representation (LRR) problem [12]1:

∥Z∥∗ + (cid:22)∥E∥2;1; s:t: X = XZ + E;

min
Z;E

(2)

where X is the data matrix. LRR is an important robust subspace clustering technique and has
found wide applications in machine learning and computer vision  e.g.  motion segmentation  face
clustering  and temporal segmentation [12  14  6]. However  the existing LRR solver [12] is based on
ADM  which suffers from O(n3) computation complexity due to the matrix-matrix multiplications
and matrix inversions. Moreover  introducing auxiliary variables also slows down the convergence 
as there are more variables and constraints. Such a heavy computation load prevents LRR from large
scale applications. It is LRR that motivated us to develop LADMAP. We show that LADMAP can be
successfully applied to LRR  obtaining faster convergence speed than the original solver. By further
representing Z as its skinny SVD and utilizing an advanced functionality of the PROPACK [9]
package  the complexity of solving LRR by LADMAP becomes only O(rn2)  as there is no full
sized matrix-matrix multiplications  where r is the rank of the optimal Z. Numerical experiments
show the great speed advantage of our LADMAP based methods for solving LRR.
Our work is inspired by Yang et al. [20]. Nonetheless  the difference of our work from theirs is
distinct. First  they only proved the convergence of LADM for a speciﬁc problem  namely nuclear
norm regularization. Their proof utilized some special properties of the nuclear norm  while we
prove the convergence of LADM for general problems in (1). Second  they only proved in the case of
ﬁxed penalty  while we prove in the case of variable penalty. Although they mentioned the dynamic
updating rule proposed in [8]  their proof cannot be straightforwardly applied to the case of variable
penalty. Moreover  that rule is for ADM only. Third  the convergence speed of LADM heavily
depends on the choice of penalty. So it is difﬁcult to choose an optimal ﬁxed penalty that ﬁts for
different problems and problem sizes  while our novel updating rule for the penalty  although simple 
is effective for different problems and problem sizes. The linearization technique has also been used
in other optimization methods. For example  Yin [22] applied this technique to the Bregman iteration
for solving compressive sensing problems and proved that the linearized Bregman method converges
to an exact solution conditionally. In comparison  LADM (and LADMAP) always converges to an
exact solution.

2 Linearized Alternating Direction Method with Adaptive Penalty

2.1 The Alternating Direction Method

ADM is now very popular in solving large scale machine learning problems [1]. When solving (1)
by ADM  one operates on the following augmented Lagrangian function:

L(x; y; (cid:21)) = f (x) + g(y) + ⟨(cid:21);A(x) + B(y) − c⟩ +

(3)
where (cid:21) is the Lagrange multiplier  ⟨·;·⟩ is the inner product  and (cid:12) > 0 is the penalty parameter.
The usual augmented Lagrange multiplier method is to minimize L w.r.t. x and y simultaneously.
This is usually difﬁcult and does not exploit the fact that the objective function is separable. To
remedy this issue  ADM decomposes the minimization of L w.r.t. (x; y) into two subproblems that

∥A(x) + B(y) − c∥2;

(cid:12)
2

1Here we switch to bold capital letters in order to emphasize that the variables are matrices.

2

minimize w.r.t. x and y  respectively. More speciﬁcally  the iterations of ADM go as follows:

xk+1 = arg min

x

L(x; yk; (cid:21)k)

= arg min

x

f (x) +

(cid:12)
2

∥A(x) + B(yk) − c + (cid:21)k=(cid:12)∥2;

yk+1 = arg min

y

L(xk+1; y; (cid:21)k)

= arg min

g(y) +

y

(cid:12)
2

∥B(y) + A(xk+1) − c + (cid:21)k=(cid:12)∥2;

(cid:21)k+1 = (cid:21)k + (cid:12)[A(xk+1) + B(yk+1) − c]:

(6)
In many machine learning problems  as f and g are matrix or vector norms  the subproblems (4)
and (5) usually have closed form solutions when A and B are identities [2  12  21]. In this case 
ADM is appealing. However  in many problems A and B are not identities. For example  in matrix
completion A can be a selection matrix  and in LRR and 1D sparse representation A can be a general
matrix. In this case  there are no closed form solutions to (4) and (5). Then (4) and (5) have to be
solved iteratively. To overcome this difﬁculty  a common strategy is to introduce auxiliary variables
[12  1] u and v and reformulate problem (1) into an equivalent one:

(4)

(5)

(7)

f (x) + g(y); s:t: A(u) + B(v) = c; x = u; y = v;

min
x;y;u;v

and the corresponding ADM iterations analogous to (4)-(6) can be deduced. With more variables
and more constraints  more memory is required and the convergence of ADM also becomes slower.
Moreover  to update u and v  whose subproblems are least squares problems  expensive matrix
inversions are often necessary. Even worse  the convergence of ADM with more than two variables
is not guaranteed [7].
To avoid introducing auxiliary variables and still solve subproblems (4) and (5) efﬁciently  inspired
by Yang et al. [20]  we propose a linearization technique for (4) and (5). To further accelerate the
convergence of the algorithm  we also propose an adaptive rule for updating the penalty parameter.

2.2 Linearized ADM

By linearizing the quadratic term in (4) at xk and adding a proximal term  we have the following
approximation:

xk+1 = arg min

x

= arg min

f (x) +

x

2

f (x) + ⟨A∗
(cid:12)(cid:17)A

∥x − xk + A∗

((cid:21)k) + (cid:12)A∗

(A(xk) + B(yk) − c); x − xk⟩ + (cid:12)(cid:17)A
((cid:21)k + (cid:12)(A(xk) + B(yk) − c))=((cid:12)(cid:17)A)∥2;

2

∥x − xk∥2

(8)
where A∗ is the adjoint of A and (cid:17)A > 0 is a parameter whose proper value will be analyzed later.
The above approximation resembles that of APG [16]  but we do not use APG to solve (4) iteratively.
Similarly  subproblem (5) can be approximated by
∥y − yk + B∗

((cid:21)k + (cid:12)(A(xk+1) + B(yk) − c))=((cid:12)(cid:17)B)∥2:

yk+1 = arg min

g(y) +

(cid:12)(cid:17)B

(9)

y

2

The update of Lagrange multiplier still goes as (6)2.

2.3 Adaptive Penalty

In previous ADM and LADM approaches [15  21  20]  the penalty parameter (cid:12) is ﬁxed. Some schol-
ars have observed that ADM with a ﬁxed (cid:12) can converge very slowly and it is nontrivial to choose an
optimal ﬁxed (cid:12). So is LADM. Thus a dynamic (cid:12) is preferred in real applications. Although Tao et
al. [15] and Yang et al. [20] mentioned He et al.’s adaptive updating rule [8] in their papers  the rule
is for ADM only. We propose the following adaptive updating strategy for the penalty parameter (cid:12):
(10)
2As in [20]  we can also introduce a parameter (cid:13) and update (cid:21) as (cid:21)k+1 = (cid:21)k +(cid:13)(cid:12)[A(xk+1)+B(yk+1)(cid:0)c].
We choose not to do so in this paper in order not to make the exposition of LADMAP too complex. The readers
can refer to Supplementary Material for full details.

(cid:12)k+1 = min((cid:12)max; (cid:26)(cid:12)k);

3

{

where (cid:12)max is an upper bound of {(cid:12)k}. The value of (cid:26) is deﬁned as

√

√

(cid:17)A∥xk+1 − xk∥;

(cid:17)B∥yk+1 − yk∥)=∥c∥ < "2;

(cid:26)0;
1;

if (cid:12)k max(
otherwise;

(cid:26) =

(11)
where (cid:26)0 ≥ 1 is a constant. The condition to assign (cid:26) = (cid:26)0 comes from the analysis on the stopping
criteria (see Section 2.5). We recommend that (cid:12)0 = (cid:11)"2  where (cid:11) depends on the size of c. Our
updating rule is fundamentally different from He et al.’s for ADM [8]  which aims at balancing the
errors in the stopping criteria and involves several parameters.

2.4 Convergence of LADMAP

To prove the convergence of LADMAP  we ﬁrst have the following propositions.

Proposition 1
−(cid:12)k(cid:17)A(xk+1−xk)−A∗
(^(cid:21)k+1) ∈ @g(yk+1); (12)
where ~(cid:21)k+1 = (cid:21)k + (cid:12)k[A(xk) +B(yk)− c]  ^(cid:21)k+1 = (cid:21)k + (cid:12)k[A(xk+1) +B(yk)− c]  and @f and
@g are subgradients of f and g  respectively.

(~(cid:21)k+1) ∈ @f (xk+1); −(cid:12)k(cid:17)B(yk+1−yk)−B∗

∗

−2
k

∥(cid:21)k − (cid:21)

This can be easily proved by checking the optimality conditions of (8) and (9).
Proposition 2 Denote the operator norms of A and B as ∥A∥ and ∥B∥  respectively. If {(cid:12)k} is non-
decreasing and upper bounded  (cid:17)A > ∥A∥2  (cid:17)B > ∥B∥2  and (x
∗
∗
) is any Karush-Kuhn-
Tucker (KKT) point of problem (1) (see (13)-(14))  then: (1). {(cid:17)A∥xk − x
)∥2 +
∗∥2 − ∥A(xk − x
∗
∗∥2} is non-increasing. (2). ∥xk+1 − xk∥ → 0  ∥yk+1 − yk∥ → 0 
∗∥2 + (cid:12)
(cid:17)B∥yk − y
∥(cid:21)k+1 − (cid:21)k∥ → 0.
The proof can be found in Supplementary Material. Then we can prove the convergence of
LADMAP  as stated in the following theorem.
Theorem 3 If {(cid:12)k} is non-decreasing and upper bounded  (cid:17)A > ∥A∥2  and (cid:17)B > ∥B∥2  then the
sequence {(xk; yk; (cid:21)k)} generated by LADMAP converges to a KKT point of problem (1).
The proof can be found in Appendix A.

; y

; (cid:21)

2.5 Stopping Criteria

The KKT conditions of problem (1) are that there exists a triple (x

∗

∗

∗
; (cid:21)

; y

) such that

∗

) + B(y

A(x
) ∈ @f (x
∗
∗

);−B∗

∗

) − c = 0;

∗

) ∈ @g(y

∗

):

((cid:21)

−A∗

((cid:21)

(13)
(14)

(15)

The triple (x

∗

∗

∗

; (cid:21)

; y

) is called a KKT point. So the ﬁrst stopping criterion is the feasibility:

∥A(xk+1) + B(yk+1) − c∥=∥c∥ < "1:

As for the second KKT condition  we rewrite the second part of Proposition 1 as follows

−(cid:12)k[(cid:17)B(yk+1 − yk) + B∗

(A(xk+1 − xk))] − B∗

(16)
So for ~(cid:21)k+1 to satisfy the second KKT condition  both (cid:12)k(cid:17)A∥xk+1− xk∥ and (cid:12)k∥(cid:17)B(yk+1− yk) +
B∗

(A(xk+1 − xk))∥ should be small enough. This leads to the second stopping criterion:

(~(cid:21)k+1) ∈ @g(yk+1):

(cid:12)k max((cid:17)A∥xk+1 − xk∥=∥A∗

(c)∥; (cid:17)B∥yk+1 − yk∥=∥B∗

(c)∥) ≤ "
′
2:

By estimating ∥A∗
stopping criterion in use:

(c)∥ and ∥B∗
√

(cid:12)k max(

√

(cid:17)A∥c∥ and

(c)∥ by
(cid:17)A∥xk+1 − xk∥;

√

(17)
(cid:17)B∥c∥  respectively  we arrive at the second

√

(cid:17)B∥yk+1 − yk∥)=∥c∥ ≤ "2:

(18)

Finally  we summarize our LADMAP algorithm in Algorithm 1.

4

Algorithm 1 LADMAP for Problem (1)
Initialize: Set "1 > 0  "2 > 0  (cid:12)max ≫ (cid:12)0 > 0  (cid:17)A > ∥A∥2  (cid:17)B > ∥B∥2  x0  y0  (cid:21)0  and k ← 0.
while (15) or (18) is not satisﬁed do
Step 1: Update x by solving (8).
Step 2: Update y by solving (9).
Step 3: Update (cid:21) by (6).
Step 4: Update (cid:12) by (10) and (11).
Step 5: k ← k + 1.

end while

3 Applying LADMAP to LRR

In this section  we apply LADMAP to solve the LRR problem (2). We further introduce acceleration
tricks to reduce the computation complexity of each iteration.

3.1 Solving LRR by LADMAP

As the LRR problem (2) is a special case of problem (1)  PADM can be directly applied to it. The
two subproblems both have closed form solutions. In the subproblem for updating E  one may apply
k   to matrix Mk = −XZk + X− (cid:3)k=(cid:12)k.
−1
the l2;1-norm shrinkage operator [12]  with a threshold (cid:12)
In the subproblem for updating Z  one has to apply the singular value shrinkage operator [2]  with
X XT (XZk + Ek+1 − X + (cid:3)k=(cid:12)k)  where (cid:17)X >
−1
a threshold ((cid:12)k(cid:17)X )
max(X). If Nk is formed explicitly  the usual technique of partial SVD  using PROPACK [9] and
(cid:27)2
rank prediction3  can be utilized to compute the leading r singular values and associated vectors of
Nk efﬁciently  making the complexity of SVD computation O(rn2)  where r is the predicted rank
of Zk+1 and n is the column number of X. Note that as (cid:12)k is non-decreasing  the predicted rank is
almost non-decreasing  making the iterations computationally efﬁcient.

−1  to matrix Nk = Zk − (cid:17)

3.2 Acceleration Tricks for LRR

Up to now  LADMAP for LRR is still of complexity O(n3)  although partial SVD is already used.
This is because forming Mk and Nk requires full sized matrix-matrix multiplications  e.g.  XZk.
To break this complexity bound  we introduce a decomposition technique to further accelerate
LADMAP for LRR. By representing Zk as its skinny SVD: Zk = Uk(cid:6)kVT
k   some of the full sized
matrix-matrix multiplications are gone: they are replaced by successive reduced sized matrix-matrix
multiplications. For example  when updating E  XZk is computed as ((XUk)(cid:6)k)VT
k   reducing the
complexity to O(rn2). When computing the partial SVD of Nk  things are more complicated. If we
form Nk explicitly  we will face with computing XT (X + (cid:3)k=(cid:12)k)  which is neither low-rank nor
sparse4. Fortunately  in PROPACK the bi-diagonalizing process of Nk is done by the Lanczos pro-
cedure [9]  which only requires to compute matrix-vector multiplications Nkv and uT Nk  where u
and v are some vectors in the Lanczos procedure. So we may compute Nkv and uT Nk by multi-
plying the vectors u and v successively with the component matrices in Nk  rather than forming Nk
explicitly. So the computation complexity of partial SVD of Nk is still O(rn2). Consequently  with
our acceleration techniques  the complexity of our accelerated LADMAP (denoted as LADMAP(A)
for short) for LRR is O(rn2). LADMAP(A) is summarized in Algorithm 2.

3The current PROPACK can only output a given number of singular values and vectors. So one has to
predict the number of singular values that are greater than a threshold [11  20  16]. See step 3 of Algorithm 2.
Recently  we have modiﬁed PROPACK so that it can output the singular values that are greater than a threshold
and their corresponding singular vectors. See [10].

still O(rn2)  while XT Ek+1 could also be accelerated as Ek+1 is a column-sparse matrix.

4When forming Nk explicitly  XT XZk can be computed as ((XT (XUk))(cid:6)k)VT

k   whose complexity is

5

(cid:22)∥E∥2;1 + (cid:12)k

∥E + (XUk)(cid:6)kVT

max(X)  r = 5  and k ← 0.

Algorithm 2 Accelerated LADMAP for LRR (2)
Input: Observation matrix X and parameter (cid:22) > 0.
Initialize: Set E0  Z0 and (cid:3)0 to zero matrices  where Z0 is represented as (U0; (cid:6)0; V0) ←
(0; 0; 0). Set "1 > 0  "2 > 0  (cid:12)max ≫ (cid:12)0 > 0  (cid:17)X > (cid:27)2
while (15) or (18) is not satisﬁed do
Step 1: Update Ek+1 = arg min
E
subproblem can be solved by using Lemma 3.2 in [12].
Step 2: Update the skinny SVD (Uk+1; (cid:6)k+1; Vk+1) of Zk+1. First  compute the partial
SVD ~Ur ~(cid:6)r ~VT
r of the implicit matrix Nk  which is bi-diagonalized by the successive matrix-
′
vector multiplication technique described in Section 3.1. Second  Uk+1 = ~Ur(:; 1 : r
) 
) − ((cid:12)k(cid:17)X )
′ is the number of
(cid:6)k+1 = ~(cid:6)r(1 : r
singular values in (cid:6)r that are greater than ((cid:12)k(cid:17)X )
Step 3: Update the predicted rank r:
If r
Step 4: Update (cid:3)k+1 = (cid:3)k + (cid:12)k((XUk+1)(cid:6)k+1VT
Step 5: Update (cid:12)k+1 by (10)-(11).
Step 6: k ← k + 1.

−1I  Vk+1 = ~Vr(:; 1 : r

+ round(0:05n); n).

k+1 + Ek+1 − X).

< r  then r = min(r

+ 1; n); otherwise  r = min(r

− X + (cid:3)k=(cid:12)k∥2. This

′

; 1 : r

′

)  where r

2

k

−1.

′

′

′

′

end while

4 Experimental Results

In this section  we report numerical results on LADMAP  LADMAP(A) and other state-of-the-art
algorithms  including APG5  ADM6 and LADM  for LRR based data clustering problems. APG 
ADM  LADM and LADMAP all utilize the Matlab version of PROPACK [9]. For LADMAP(A) 
we provide two function handles to PROPACK which fulﬁls the successive matrix-vector multipli-
cations. All experiments are run and timed on a PC with an Intel Core i5 CPU at 2.67GHz and with
4GB of memory  running Windows 7 and Matlab version 7.10.
We test and compare these solvers on both synthetic multiple subspaces data and the real world
motion data (Hopkin155 motion segmentation database [17]). For APG  we set the parameters
−10  (cid:18) = 0:9 in its continuation technique and the Lipschitz constant (cid:28) =
(cid:12)0 = 0:01  (cid:12)min = 10
max(X). The parameters of ADM and LADM are the same as those in [12] and [20]  respectively.
(cid:27)2
In particular  for LADM the penalty is ﬁxed at (cid:12) = 2:5= min(m; n)  where m × n is the size of
−5  (cid:12)0 = min(m; n)"2  (cid:12)max = 1010  (cid:26)0 = 1:9 
X. For LADMAP  we set "1 = 10
max(X). As the code of ADM was downloaded  its stopping criteria  ∥XZk +
and (cid:17)X = 1:02(cid:27)2
Ek − X∥=∥X∥ ≤ "1 and max(∥Ek − Ek−1∥=∥X∥;∥Zk − Zk−1∥=∥X∥) ≤ "2  are used in all our
experiments7.

−4  "2 = 10

4.1 On Synthetic Data

i=1 are constructed  whose bases {Ui}s

The synthetic test data  parameterized as (s  p  d  ~r)  is created by the same procedure in [12]. s
independent subspaces {Si}s
i=1 are generated by Ui+1 =
TUi; 1 ≤ i ≤ s − 1  where T is a random rotation and U1 is a d × ~r random orthogonal matrix.
So each subspace has a rank of ~r and the data has an ambient dimension of d. Then p data points
are sampled from each subspace by Xi = UiQi; 1 ≤ i ≤ s  with Qi being an ~r × p i.i.d. zero
mean unit variance Gaussian matrix N (0; 1). 20% samples are randomly chosen to be corrupted by
adding Gaussian noise with zero mean and standard deviation 0:1∥x∥. We empirically ﬁnd that LRR
achieves the best clustering performance on this data set when (cid:22) = 0:1. So we test all algorithms
with (cid:22) = 0:1 in this experiment. To measure the relative errors in the solutions  we run LADMAP
2000 iterations with (cid:12)max = 103 to establish the ground truth solution (E0; Z0).
The computational comparison is summarized in Table 1. We can see that the iteration numbers and
the CPU times of both LADMAP and LADMAP(A) are much less than those of other methods  and

5Please see Supplementary Material for the detail of solving LRR by APG.
6We use the Matlab code provided online by the authors of [12].
7Note that the second criterion differs from that in (18). However  this does not harm the convergence of

LADMAP because (18) is always checked when updating (cid:12)k+1 (see (11)).

6

LADMAP(A) is further much faster than LADMAP. Moreover  the advantage of LADMAP(A) is
even greater when the ratio ~r=p  which is roughly the ratio of the rank of Z0 to the size of Z0  is
smaller  which testiﬁes to the complexity estimations on LADMAP and LADMAP(A) for LRR. It
is noteworthy that the iteration numbers of ADM and LADM seem to grow with the problem sizes 
while that of LADMAP is rather constant. Moreover  LADM is not faster than ADM. In particular 
on the last data we were unable to wait until LADM stopped. Finally  as APG converges to an
approximate solution to (2)  its relative errors are larger and its clustering accuracy is lower than
ADM and LADM based methods.

Table 1: Comparison among APG  ADM  LADM  LADMAP and LADMAP(A) on the synthetic
data. For each quadruple (s  p  d  ~r)  the LRR problem  with (cid:22) = 0:1  was solved for the same data
using different algorithms. We present typical running time (in ×103 seconds)  iteration number 
relative error (%) of output solution ( ^E; ^Z) and the clustering accuracy (%) of tested algorithms 
respectively.

Size (s  p  d  ~r)

(10  20 200  5)

(15  20 300  5)

(20  25  500  5)

(30  30  900  5)

LADMAP

LADMAP(A)

Method
APG
ADM
LADM

APG
ADM
LADM

APG
ADM
LADM

APG
ADM
LADM

LADMAP

LADMAP(A)

LADMAP

LADMAP(A)

LADMAP

LADMAP(A)

Time
0.0332
0.0529
0.0603
0.0145
0.0010
0.0869
0.1526
0.2943
0.0336
0.0015
1.8837
3.7139
8.1574
0.7762
0.0053
6.1252
11.7185

N.A.
2.3891
0.0058

Iter.
110
176
194
46
46
106
185
363
41
41
117
225
508
40
40
116
220
N.A.
44
44

∥ ^Z−Z0∥
∥Z0∥
2.2079
0.5491
0.5480
0.5480
0.5480
2.4824
0.6519
0.6518
0.6518
0.6518
2.8905
1.1191
0.6379
0.6379
0.6379
3.0667
0.6865
N.A.
0.6864
0.6864

∥ ^E−E0∥
∥E0∥
1.5096
0.5093
0.5024
0.5024
0.5024
1.0341
0.4078
0.4076
0.4076
0.4076
2.4017
1.0170
0.4268
0.4268
0.4268
0.9199
0.4866
N.A.
0.4294
0.4294

Acc.
81.5
90.0
90.0
90.0
90.0
80.0
83.7
86.7
86.7
86.7
72.4
80.0
80.0
84.6
84.6
69.4
76.0
N.A.
80.1
80.1

Table 2: Comparison among APG  ADM  LADM  LADMAP and LADMAP(A) on the Hopkins155
database. We present their average computing time (in seconds)  average number of iterations and
average classiﬁcation errors (%) on all 156 sequences.

Two Motion

Three Motion

Time
15.7836
53.3470
9.6701
3.6964
2.1348

Iter. CErr.
5.77
90
5.72
281
5.77
110
5.72
22
22
5.72

Time
46.4970
159.8644
22.1467
10.9438
6.1098

Iter. CErr.
16.52
90
16.52
284
16.52
64
16.52
22
22
16.52

Time
22.6277
77.0864
12.4520
5.3114
3.0202

All
Iter. CErr.
8.36
90
8.33
282
8.36
99
8.33
22
22
8.33

APG
ADM
LADM

LADMAP

LADMAP(A)

4.2 On Real World Data

We further test the performance of these algorithms on the Hopkins155 database [17]. This database
consists of 156 sequences  each of which has 39 to 550 data vectors drawn from two or three motions.
For computational efﬁciency  we preprocess the data by projecting it to be 5-dimensional using PCA.
As (cid:22) = 2:4 is the best parameter for this database [12]  we test all algorithms with (cid:22) = 2:4.
Table 2 shows the comparison among APG  ADM  LADM  LADMAP and LADMAP(A) on this
database. We can also see that LADMAP and LADMAP(A) are much faster than APG  ADM  and

7

LADM  and LADMAP(A) is also faster than LADMAP. However  in this experiment the advantage
of LADMAP(A) over LADMAP is not as dramatic as that in Table 1. This is because on this data (cid:22)
is chosen as 2:4  which cannot make the rank of the ground truth solution Z0 much smaller than the
size of Z0.

5 Conclusions

In this paper  we propose a linearized alternating direction method with adaptive penalty for solving
subproblems in ADM conveniently. With LADMAP  no auxiliary variables are required and the
convergence is also much faster. We further apply it to solve the LRR problem and combine it with
an acceleration trick so that the computation complexity is reduced from O(n3) to O(rn2)  which
is highly advantageous over the existing LRR solvers. Although we only present results on LRR 
LADMAP is actually a general method that can be applied to other convex programs.

Acknowledgments

The authors would like to thank Dr. Xiaoming Yuan for pointing us to [20]. This work is partially
supported by the grants of the NSFC-Guangdong Joint Fund (No. U0935004) and the NSFC Fund
(No. 60873181  61173103). R. Liu also thanks the support from CSC.

∞

∞

; (cid:21)

; y

; (cid:21)
∞

; y
∞

) is a KKT point of problem (1).

). We accomplish the proof in two steps.
∞

A Proof of Theorem 3
Proof By Proposition 2 (1)  {(xk; yk; (cid:21)k)} is bounded  hence has an accumulation point  say
(xkj ; ykj ; (cid:21)kj ) → (x
∞
1. We ﬁrst prove that (x
By Proposition 2 (2)  A(xk+1) + B(yk+1) − c = (cid:12)
k ((cid:21)k+1 − (cid:21)k) → 0: This shows that any
−1
accumulation point of {(xk; yk)} is a feasible solution.
By letting k = kj − 1 in Proposition 1 and the deﬁnition of subgradient  we have
) + ⟨xkj
− x
f (xkj ) + g(ykj ) ≤ f (x
;−(cid:12)kj−1(cid:17)A(xkj
∗
∗
∗
;−(cid:12)kj−1(cid:17)B(ykj
+⟨ykj
− ykj−1) − B∗
Let j → +∞  by observing Proposition 2 (2)  we have
) ≤ f (x
) + ⟨x
∞ − x
;−A∗
)⟩ + ⟨y
∗
∗
∗
∞
((cid:21)
∞⟩ − ⟨B(y
∞ − x
) − ⟨A(x
∗
∗
∗
); (cid:21)
= f (x
) − ⟨A(x
) + B(y
) − A(x
∗
∗
∗
∞
∞
= f (x
∗
∗
);
= f (x
∞
∞
where we have used the fact that both (x
that (x
Again  let k = kj − 1 in Proposition 1 and by the deﬁnition of subgradient  we have
(~(cid:21)kj )⟩; ∀x:

− xkj−1) − A∗
(^(cid:21)kj )⟩:
;−B∗
∞ − y
∗
((cid:21)
)⟩
∞ − y
∞
∗
); (cid:21)
∞⟩
) − B(y
∗
); (cid:21)

f (x) ≥ f (xkj ) + ⟨x − xkj ;−(cid:12)kj−1(cid:17)A(xkj

; y
) is an optimal solution to (1).

) are feasible solutions. So we conclude

) + g(y
) + g(y
) + g(y
) + g(y

) + g(y
− y
∗

∗
) and (x

(~(cid:21)kj )⟩

) + g(y

(19)

f (x

)⟩

; y

; y

∞

∞

∞

∞

((cid:21)

;−A∗
∞

) ∈ @g(y

) + ⟨x − x
((cid:21)

((cid:21)
). Therefore  (x

f (x) ≥ f (x
∞
). Similarly  −B∗

Fix x and let j → +∞  we see that
) ∈ @f (x
So −A∗
∞
point of problem (1).
2. We next prove that the whole sequence {(xk; yk; (cid:21)k)} converges to (x
∗
∗
∗
∞
By choosing (x
; (cid:21)
) = (x
)∥2 + (cid:17)B∥ykj
− y
∞∥2 + (cid:12)
∞
x
(cid:17)A∥xk − x
∞∥2 − ∥A(xk − x
∞
∞
∞
∞
; (cid:21)
; y
(x
∞
∞
As (x
{(xk; yk; (cid:21)k)} converges to a KKT point of problem (1).

−
∞∥2 → 0. By Proposition 2 (1)  we readily have
∞∥2 → 0. So (xk; yk; (cid:21)k) →
) can be an arbitrary accumulation point of {(xk; yk; (cid:21)k)}  we may conclude that

∞
) in Proposition 2  we have (cid:17)A∥xkj
∞
− (cid:21)

; y
−2
kj
)∥2 + (cid:17)B∥yk − y

∞
∞∥2 −∥A(xkj

∞

∞

∞

; (cid:21)

; y

) is a KKT

∞∥2 + (cid:12)

∥(cid:21)k − (cid:21)

∞
; (cid:21)
− x

∞
∥(cid:21)kj

).
∞

−2
k

; y

; y

; y

; (cid:21)

; (cid:21)

− xkj−1) − A∗
)⟩; ∀x:

∞

∞

∞

∞

).

∞

∗

8

References
[1] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statis-
tical learning via the alternating direction method of multipliers. In Michael Jordan  editor 
Foundations and Trends in Machine Learning  2010.

[2] J. Cai  E. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion.

preprint  2008.

[3] E. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? J. ACM  2011.
[4] E. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational Mathematics  2009.

[5] E. J. Cand`es and M. Wakin. An introduction to compressive sampling. IEEE Signal Processing

Magazine  2008.

[6] P. Favaro  R. Vidal  and A. Ravichandran. A closed form solution to robust subspace estimation

and clustering. In CVPR  2011.

[7] B. He  M. Tao  and X. Yuan. Alternating direction method with Gaussian back substitution for

separable convex programming. SIAM Journal on Optimization  accepted.

[8] B. He  H. Yang  and S. Wang. Alternating direction method with self-adaptive penalty param-
eters for monotone variational inequality. J. Optimization Theory and Applications  106:337–
356  2000.

[9] R. Larsen. Lanczos bidiagonalization with partial reorthogonalization. Department of Com-

puter Science  Aarhus University  Technical report  DAIMI PB-357  1998.

[10] Z. Lin. Some software packages for partial SVD computation. arXiv:1108.1548.
[11] Z. Lin  M. Chen  and Y. Ma. The augmented Lagrange multiplier method for exact re-
covery of corrupted low-rank matrices. UIUC Technical Report UILU-ENG-09-2215  2009 
arXiv:1009.5055.

[12] G. Liu  Z. Lin  and Y. Yu. Robust subspace segmentation by low-rank representation. In ICML 

2010.

[13] J. Liu  S. Ji  and J. Ye. Multi-task feature learning via efﬁcient l2;1 norm minimization. In UAI 

2009.

[14] Y. Ni  J. Sun  X. Yuan  S. Yan  and L. Cheong. Robust low-rank subspace segmentation with

semideﬁnite guarantees. In ICDM Workshop  2010.

[15] M. Tao and X.M. Yuan. Recovering low-rank and sparse components of matrices from incom-

plete and noisy observations. SIAM Journal on Optimization  21(1):57–81  2011.

[16] K. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized

least sequares problems. Paciﬁc J. Optimization  6:615–640  2010.

[17] R. Tron and R. Vidal. A benchmark for the comparison of 3D montion segmentation algo-

rithms. In CVPR  2007.

[18] J. Wright  A. Ganesh  S. Rao  and Y. Ma. Robust principal component analysis: Exact recovery

of corrupted low-rank matrices via convex optimization. In NIPS  2009.

[19] J. Wright  Y. Ma  J. Mairal  G. Sapirao  T. Huang  and S. Yan. Sparse representation for

computer vision and pattern recognition. Proceedings of the IEEE  2010.

[20] J. Yang and X. Yuan. Linearized augmented Lagrangian and alternating direction methods for

nuclear norm minimization. submitted  2011.

[21] J. Yang and Y. Zhang. Alternating direction algorithms for l1 problems in compressive sensing.

SIAM J. Scientiﬁc Computing  2010.

[22] W. Yin. Analysis and generalizations of the linearized Bregman method. SIAM Journal on

Imaging Sciences  2010.

9

,Nicolò Cesa-Bianchi
Claudio Gentile
Giovanni Zappella