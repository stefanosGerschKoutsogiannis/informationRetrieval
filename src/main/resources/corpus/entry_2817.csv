2010,Large-Scale Matrix Factorization with Missing Data under Additional Constraints,Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM)  non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semidefinite program (LRSDP) with the advantage that: $1)$ an efficient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems  and $2)$ additional constraints such as ortho-normality  required in orthographic SfM  can be directly incorporated in the new formulation. Our empirical evaluations suggest that  under the conditions of matrix completion theory  the proposed algorithm finds the optimal solution  and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the affine SfM problem  non-rigid SfM and photometric stereo problems.,Large-Scale Matrix Factorization with Missing Data

under Additional Constraints

Kaushik Mitra (cid:3)y

Department of Electrical and Computer Engineering and UMIACS

University of Maryland  College Park  MD 20742

kmitra@umiacs.umd.edu

Sameer Sheoreyy

Toyota Technological Institute  Chicago

ssameer@ttic.edu

Rama Chellappa

Department of Electrical and Computer Engineering and UMIACS

University of Maryland  College Park  MD 20742

rama@umaics.umd.edu

Abstract

Matrix factorization in the presence of missing data is at the core of many com-
puter vision problems such as structure from motion (SfM)  non-rigid SfM and
photometric stereo. We formulate the problem of matrix factorization with miss-
ing data as a low-rank semide(cid:2)nite program (LRSDP) with the advantage that:
1) an ef(cid:2)cient quasi-Newton implementation of the LRSDP enables us to solve
large-scale factorization problems  and 2) additional constraints such as ortho-
normality  required in orthographic SfM  can be directly incorporated in the new
formulation. Our empirical evaluations suggest that  under the conditions of ma-
trix completion theory  the proposed algorithm (cid:2)nds the optimal solution  and also
requires fewer observations compared to the current state-of-the-art algorithms.
We further demonstrate the effectiveness of the proposed algorithm in solving the
af(cid:2)ne SfM problem  non-rigid SfM and photometric stereo problems.

1 Introduction

Many computer vision problems such as SfM [26]  non-rigid SfM [3] and photometric stereo [11]
can be formulated as a matrix factorization problem. In all these problems  the measured data are
observations of the elements of an m (cid:2) n measurement matrix M of known rank r. The objective
is to factorize this measurement matrix M into factors A and B of dimensions m (cid:2) r and n (cid:2) r 
respectively such that the error jjM (cid:0) ABTjj is minimized. When all the elements of M are known 
and assuming that the elements are corrupted by Gaussian noise  the solution to this problem is given
by the singular value decomposition (SVD) of M. However  in most real applications many of the
elements of M will be missing and we need to solve a modi(cid:2)ed problem given by:

(1)
where (cid:12) is the Hadamard element-wise product  W is a weight matrix with zeroes at indices corre-
sponding to the missing elements of M  and jjAjj2
F are regularization terms which prevent

A;B jjW (cid:12) (M (cid:0) ABT )jj2
min

F   jjBjj2

(cid:3)Partially supported by an ARO MURI on oppurtunistic sensing under the grant W911NF-09-1-0383.
yKaushik Mitra and Sameer Sheorey contributed equally to this work.

F + (cid:21)1jjAjj2

F + (cid:21)2jjBjj2

F

1

data over(cid:2)tting. Matrix factorization with missing data is a dif(cid:2)cult non-convex problem with no
known globally convergent algorithm. The damped Newton algorithm [4]  a variant of Newton’s
method  is one of the most popular algorithms for solving this problem. However  this algorithm has
high computational complexity and memory requirements and so cannot be used for solving large
scale problems.
We formulate the matrix factorization with missing data problem as a LRSDP [6]  which is es-
sentially a rank constrained semide(cid:2)nite programming problem (SDP) and was proposed to solve
large SDP in an ef(cid:2)cient way. The advantages of formulating the matrix factorization problem as
a LRSDP problem are the following: 1) it inherits the ef(cid:2)ciency of the LRSDP algorithm. The
LRSDP algorithm is based on a quasi-Newton method which has lower computational complexity
and memory requirements than that of Newton’s method  and so is ideally suited for solving large
scale problems. 2) Many additional constraints  such as the ortho-normality constraints for the or-
thographic SfM  can be easily incorporated into the LRSDP-based factorization formulation; this is
possible because of the (cid:3)exible framework of the LRSDP (see section 2).
Prior Work Algorithms for matrix factorization in the presence of missing data can be broadly
divided into two main categories: initialization algorithms and iterative algorithms. Initialization
algorithms [26  13  10  18  25] generally minimize an algebraic or approximate cost of (1) and are
used for providing a good starting point for the iterative algorithms. Iterative algorithms are those
algorithms that directly minimize the cost function (1). Alternation algorithms [23  28  12  1  2  14] 
damped Newton algorithm [4] and our approach fall under this category. Alternation algorithms are
based on the fact that if one of the factors A or B is known  then there are closed form or numerical
solutions for the other factor. Though the alternation-based algorithms minimize the cost in each
iteration  they are essentially a coordinate descent approach and suffer from (cid:3)atlining  requiring
an excessive number of iterations before convergence [4]. To solve this problem  damped Newton
and hybrid algorithms between damped Newton and alternation were proposed in [4]. Although
these algorithms give very good results  they cannot be used for solving large-scale problems be-
cause of their high computational complexity and memory requirements. Other algorithms based on
Newton’s method have been proposed in [7  21]  which also cannot be used for solving large-scale
problems.
The matrix factorization with missing data problem is closely related to the matrix completion prob-
lem [9]. The goal of matrix completion is to (cid:2)nd a low-rank matrix which agrees with the observed
entries of the matrix M. Recently  many ef(cid:2)cient algorithms have been proposed for solving this
problem [8  17  19  16  15  20]. Some of them [16  15  20] are formulated as matrix factoriza-
tion problems. However  we note that these algorithms  by themselves  can not handle additional
constraints. Matrix factorization also arises while solving the collaborative (cid:2)ltering problem. Col-
laborative (cid:2)ltering is the task of predicting the interests of a user by collecting taste information
from many users  for example in a movie recommendation system. In [24]  collaborative (cid:2)ltering
is formulated as a matrix completion problem and solved using a semide(cid:2)nite program. Later a
fast version  using conjugate gradient  was proposed in [22]  but it also cannot handle additional
constraints.

2 Background: Low-rank semideﬁnite programming (LRSDP)

LRSDP was proposed in [6] to ef(cid:2)ciently solve a large scale SDP [27]. In the following paragraphs 
we brie(cid:3)y de(cid:2)ne the SDP and LRSDP problems  and discuss the ef(cid:2)cient algorithm used for solving
the LRSDP problem.
SDP is a sub(cid:2)eld of convex optimization concerned with the optimization of a linear objective
function over the intersection of the cone of positive semide(cid:2)nite matrices with an af(cid:2)ne space. The
standard-form SDP is given by:

min C (cid:15) X subject to Ai (cid:15) X = bi;

(2)
where C and Ai are n (cid:2) n real symmetric matrices  b is k-dimensional vector  and X is an n (cid:2) n
matrix variable  which is required to be symmetric and positive semide(cid:2)nite  as indicated by the
constraint X (cid:23) 0. The operator (cid:15) denotes the inner product in the space of n (cid:2) n symmetric
j=1 Aij Bij. The most common algorithms
matrices de(cid:2)ned as A (cid:15) B = trace(AT B) = Pn
for solving (2) are the interior point methods [27]. However  these are second-order methods  which

i = 1; : : : ; k X (cid:23) 0

i=1Pn

2

need to store and factorize a large (and often dense) matrix and hence are not suitable for solving
large scale problems.
In LRSDP a change of variables is introduced as X = RRT   where R is a real  n (cid:2) r matrix with
r (cid:20) n. This has the advantage that it removes the non-linear constraint X (cid:23) 0  which is the most
challenging aspect of solving (2). However  this comes with the cost that the problem may no longer
be a convex problem. The LRSDP formulation is given by:

i = 1; : : : ; k

(Nr) min C (cid:15) RRT subject to Ai (cid:15) RRT = bi;

(3)
Note that the LRSDP formulation depends on r; when r = n  (3) is equivalent to (2). But the
intention is to choose r as small as possible so as to reduce the number of variables  while the
problem remains equivalent to the original problem (2).
A non-linear optimization technique called the augmented Lagrangian method is used for solving
(3). The majority of the iterations in this algorithm involve the minimization of an augmented La-
grangian function with respect to the variable R which is done by a limited memory BFGS method.
BFGS  a quasi-Newton method  is much more ef(cid:2)cient than Newton’s method both in terms of com-
putations and memory requirement. The LRSDP algorithm further optimizes the computations and
storage requirements for sparse C and Ai matrices  which is true for problems of our interest. For
further details on the algorithm  see [6  5].

3 Matrix factorization using LRSDP (MF-LRSDP)

In this section  we formulate the matrix factorization with missing data as an LRSDP problem. We
do this in the following stages:
in section 3.1  we look at the noiseless case  that is  where the
measurement matrix M is not corrupted with noise  followed by the noisy measurement case in
section 3.2  and (cid:2)nally in section 3.3  we look at how additional constraints can be incorporated in
the LRSDP formulation.

3.1 Noiseless Case
When the observed elements of the m (cid:2) n dimensional measurement matrix M are not corrupted
with noise  a meaningful cost to minimize would be:

A;B jjAjj2
min

F + jjBjj2

F subject to (ABT )i;j = Mi;j for (i; j) 2 (cid:10)

(4)

where (cid:10) is the index set of the observed entries of M  and A  B are the desired factor matrices of
dimensions m (cid:2) r and n (cid:2) r respectively. To formulate this as a LRSDP problem  we introduce a
(m + n) (cid:2) r dimensional matrix R = (cid:18) A

B (cid:19). Then

We observe that the cost functionjjAjj2
as (RRT )i;j+m = Mi;j. Thus  (4) is equivalent to:

RRT = (cid:18) AAT ABT
F +jjBjj2

BAT BBT (cid:19)
F can be expressed as trace(RRT ) and the constraints

(5)

trace(RRT ) subject to (RRT )i;j+m = Mi;j for (i; j) 2 (cid:10)
This is already in the LRSDP form  since we can express the above equation as

min

R

min

R

C (cid:15) RRT subject to Al (cid:15) RRT = bl;

l = 1; : : : ;j(cid:10)j

(6)

(7)

where C is an (m + n) (cid:2) (m + n) identity matrix  and to simplify the notations we have introduced
the index l with (cid:10)(l) = (i; j)
l = 1; : : : ;j(cid:10)j. Al are sparse matrices with the non-zero entries at
indices (i; j + m) and (j + m; i) equal to 1=2 and bl = Mi;j. This completes the formulation of
the matrix factorization problem as an LRSDP problem for the noiseless case. Next we look at the
noisy case.

3

3.2 Noisy case

When the observed entries of M are corrupted with noise  an appropriate cost function to minimize
would be:

A;B jjW (cid:12) (M (cid:0) ABT )jj2
min

(8)
where (cid:12) is the Hadamard element-wise product and W is a weight matrix with zeros corresponding
to the missing entries and 1 to the observed entries in M. To formulate this as an LRSDP problem 
we introduce noise variables el; l = 1; 2; : : : ;j(cid:10)j which are de(cid:2)ned as el = (M (cid:0) (ABT ))l . Now 
(8) can be expressed as
(9)

F subject to (M (cid:0) ABT )l = el for l = 1; 2; : : : ;j(cid:10)j

F + (cid:21)jjBjj2

F + (cid:21)jjAjj2

A;B;e jjejj2
min

2 + (cid:21)jjAjj2

F + (cid:21)jjBjj2

F

Next  we aim to formulate this as a LRSDP problem. For this  we construct an augmented noise
vector E = [eT

1]T and de(cid:2)ne R to be

R = 0
@

(cid:18) A

B (cid:19) 0

0

E

1
A

(10)

R is a ‘block-diagonal’ matrix  where the blocks are of sizes (m + n) (cid:2) r and (j(cid:10)j + 1) (cid:2) 1
respectively. With this de(cid:2)nition  RRT is a block-diagonal matrix given by

RRT = 0
@

(cid:18) AAT ABT

BAT BBT (cid:19)

0

We can now express (8) in the following LRSDP form:

0

EET

1
A

min

R

C (cid:15) RRT subject to Al (cid:15) RRT = bl;

l = 1; : : : ;j(cid:10)j + 1

(11)

(12)

with

0

0

I(j(cid:10)j+1)(cid:2)(j(cid:10)j+1) (cid:19)

C = (cid:18) (cid:21)I(m+n)(cid:2)(m+n)

(13)
Note that the number of constraints j(cid:10)j + 1 in (12) is one more than the number of observations j(cid:10)j.
This is because the last constraint is used to set Ej(cid:10)j+1 = 1  which is done by choosing Aj(cid:10)j+1 to be
a sparse matrix with the non-zero entry at index (j(cid:10)j + l + m + n;j(cid:10)j + 1 + m + n) equal to 1 and
bj(cid:10)j+1 = 1. For the remaining values of l  the Al are sparse matrices with the non-zero entries at
indices (i; j +m)  (j +m; i)  (j(cid:10)j+1+m+n; l+m+n) and (l+m+n;j(cid:10)j+1+m+n) equal to 1=2
and bl = Ml. Note that (12) is a block-LRSDP problem (R has a block-diagonal structure)  which
is a simple extension of the original LRSDP problem [5]. This completes the LRSDP formulation
for the noisy case. Next  we look at incorporating additional constraints in this framework.

3.3 Enforcing Additional Constraints

Many additional constraints can be easily incorporated in the LRSDP formulation. We illustrate
this using the speci(cid:2)c example of orthographic SfM [26]. SfM is the problem of reconstructing the
scene structure (3-D point positions and camera parameters) from 2-D projections of the points in
the cameras. Suppose that m=2 cameras are looking at n 3-D points  then under the af(cid:2)ne camera
model  the 2-D imaged points can be arranged as an m (cid:2) n measurement matrix M with columns
corresponding to the n 3-D points and rows corresponding to the m=2 cameras (2 consecutive rows
per camera) [26]. Under this arrangement  M can be factorized as M = AB T   where A is a m (cid:2) 4
camera matrix and B is a n (cid:2) 4 structure matrix with the last column of B  an all-one vector.
Thus  M is a rank 4 matrix with a special structure for the last column of B. Further  under the
orthographic camera model  A has more structure (constraints): pair of ’rows’ that corresponds to
the same camera is ortho-normal. To state this constraints precisely  we decompose the A matrix as
A = [P t] where P is a m (cid:2) 3 sub-matrix consisting of the (cid:2)rst three columns and t is the last
column vector. We can now express the camera ortho-normality constraint through the P P T matrix 
whose diagonal elements should be 1 (normality constraint) and appropriate off-diagonal elements
should be 0 (orthogonality constraint). Since  the last column of B is the all one vector  we can write

4

B = [X 1]  where X is a n (cid:2) 3 matrix. Thus  ABT = P X + t1T and the observation error can
be expressed as el = (M (cid:0) P X)l (cid:0) ti for (cid:10)(l) = (i; j). A meaningful optimization problem to
solve here would be to minimize the observation error subject to the ortho-normality constraints:

e;P;X;t jjejj2
min

2

subject to el = (M (cid:0) P X)l (cid:0) ti;

l = 1; 2; : : : ;j(cid:10)j

(P P T )k;k = 1;

k = 1; 2; : : : ; m

(P P T )k;l = 0; if k and l are rows from same camera

(14)

To formulate this as an LRSDP problem  we introduce the augmented translation variable T =
[tT

1]T   and propose the following block-diagonal matrix R:
1
CA

R = 0
B@

X (cid:19) 0

T
0
0 E

(cid:18) P

0
0

0

(15)

With this de(cid:2)nition of R  we can express (14) as a LRSDP problem; following steps similar to the
previous sections  it is should be straight forward to (cid:2)gure out the appropriate C and Al matrices
required in this LRSDP formulation (3). This completes our illustration on the incorporation of
the ortho-normality constraints for the orthographic SfM case. This example should convince the
reader that many other application-speci(cid:2)c constraints can be directly incorporated into the LRSDP
formulation; this is because of the underlying SDP structure of the LRSDP.

4 Matrix Completion  Uniqueness and Convergence of MF-LRSDP

In this section  we state the main result of the matrix completion theory and discuss its implications
for the matrix factorization problem.

4.1 Matrix Completion Theory

min
X

Matrix completion theory considers the problem of recovering a low-rank matrix from a few samples
of its entries:

rank(X) subject to Xi;j = Mi;j for (i; j) 2 (cid:10)

(16)
More speci(cid:2)cally  it considers the following questions: 1) when does a partially observed matrix
have a unique low-rank solution? 2) How can this matrix be recovered? The answers to these
questions were provided in theorem 1:3 of [9] which states that if 1) the matrix M  that we want to
recover  has row and columns spaces incoherent with the standard basis and 2) we are given enough
entries ((cid:21) O(rd6=5 log d)  where d = max(m; n))  then there exists a unique low-rank solution to
(16). Further  the solution can be obtained by solving a convex relaxation of (16) given by:
(17)

X jjXjj(cid:3) subject to Xi;j = Mi;j for (i; j) 2 (cid:10)

min

where jjXjj(cid:3) is the nuclear norm of X  given by the sum of its singular values.
4.2 Relation with Matrix Factorization and its Implications

In matrix completion the objective is to (cid:2)nd a minimum rank matrix which agrees with the partial
observations (16)  whereas in matrix factorization we assume the rank r to be known  as in the
problems of SFM and photometric stereo  and we use the rank as a constraint. For example  in our
LRSDP formulation  we have imposed this rank constraint by (cid:2)xing the number of columns of the
factors A and B to r. However  though the matrix completion and factorization problems are de-
(cid:2)ned differently  they are closely related as revealed by their very similar Lagrangian formulations.
This fact has been used in solving the matrix completion problem via matrix factorization with an
appropriate rank [16  15  20]. We should also note that matrix completion theory helps us answer the
question raised in [4]: when is missing data matrix factorization unique (up to a gauge)? And from
the discussion in the previous section  it should be clear that the conditions of the matrix completion
theory are suf(cid:2)cient for guaranteeing us the required uniqueness. Further  in our experimental evalu-
ations (see next section)  we have found that the LRSDP formulation  though a non-convex problem
in general  converges to the global minimum solution under these conditions.

5

5 Experimental Evaluation

We evaluate the performance of the proposed LRSDP-based factorization algorithm (MF-LRSDP)
on both synthetic and real data and compare it against other algorithms such as alternation [4] 
damped Newton [4] and OptSpace [15]  which is one of state-of-the-art algorithms for matrix com-
pletion.

5.1 Evaluation with Synthetic Data

The important parameters in the matrix factorization with missing data problem are: the size of
the matrix M characterized by m and n  rank r  fraction of missing data and the variance (cid:27) 2 of
the observation noise. We evaluate the factorization algorithms by varying these parameters. We
consider two cases: data without noise and data with noise. For synthetic data without noise  we
generate n(cid:2) n matrices M of rank r by M = ABT   where A and B are n(cid:2) r random matrices with
each entry being sampled independently from a standard Gaussian distribution N (0; 1). Each entry
is then revealed randomly according to the missing data fraction. For synthetic data with noise  we
add independent Gaussian noise N (0; (cid:27)2) to the observed entries generated as above.
Exact Factorization: a ﬁrst comparison. We study the reconstruction rate of different algorithms
by varying the fraction of revealed entries per column (j(cid:10)j=n) for noiseless 500 (cid:2) 500 matrices of
rank 5. We declare a matrix to be reconstructed if jjM (cid:0) ^MjjF =jjMjjF (cid:20) 10(cid:0)4  where ^M = ^A ^B is
the reconstructed matrix and jj:jjF denotes the Frobenius norm. Reconstruction rate is de(cid:2)ned as the
fraction of trials for which the matrix was successfully reconstructed. In all the synthetic data exper-
iments  we performed 10 trials. Figure 1(a) shows the reconstruction rate by MF-LRSDP  alternation
and OptSpace. MF-LRSDP gives the best reconstruction results as it needs fewer observations for
matrix reconstruction than the other algorithms. It is followed by OptSpace and alternation  respec-
tively. MF-LRSDP also takes the least time  followed by OptSpace and alternation. For similar
comparison to other matrix completion algorithms such as ADMiRA [16]  SVT [8] and FPCA [17] 
the interested reader can look at [15]  where OptSpace was shown to be consistently better than
these algorithms. For the remaining experiments on synthetic data  we mostly compare MF-LRSDP
against OptSpace. Note that we have not included the damped Newton algorithm in this comparison
because it is very slow for matrices of this size.

MF−LRSDP
Alternation
OptSpace

1

0.8

0.6

0.4

0.2

e
t
a
r
 
n
o
i
t
c
u
r
t
s
n
o
c
e
R

 
0
0

l

e
a
c
s
g
o

 

l
 

n

i
 

s
d
n
o
c
e
s
n

 

i
 

e
m
T

i

10

20
|/n

|W

30

40

(a) Reconstruction rate

 

 

104

102

100

10−2
 
0

MF−LRSDP
Alternation
OptSpace

10

20
|/n

|W

(b) Timing results

30

40

Figure 1: (a) Reconstruction rate vs. fraction of revealed entries per column j(cid:10)j=n for 500 (cid:2) 500 matrices
of rank 5 by MF-LRSDP  alternation and OptSpace. The proposed algorithm MF-LRSDP gives the best recon-
struction results since it can reconstruct matrices with fewer observed entries. (b) Time taken for reconstruction
by different algorithms. MF-LRSDP takes the least time.
Exact Factorization: vary size. We study the reconstruction rate vs. fraction of revealed entries per
column j(cid:10)j=n for different sizes n of rank 5 square matrices by MF-LRSDP and OptSpace. Figure
2(a) shows that MF-LRSDP reconstructs matrices from fewer observed entries than OptSpace.
Exact Factorization: vary rank. We study the reconstruction rate vs. j(cid:10)j=n as we vary the rank r
of 500(cid:2) 500 matrices. Figure 2(b) again shows that MF-LRSDP gives better results than OptSpace.

6

MF−LRSDP
 n=400

 n=1000

 n=2000

1

0.8

0.6

0.4

0.2

e
t
a
r
 
n
o
i
t
c
u
r
t
s
n
o
c
e
R

 

OptSpace

n=400

n=1000

n=2000

 
0
5

10

15

20
|W

25

|/n

30

35

40

1

0.8

0.6

0.4

0.2

e
t
a
r
 
n
o
i
t
c
u
r
t
s
n
o
c
e
R

 

0
0

OptSpace
r=5

r=10

r=20

MF−LRSDP
r=5

r=10

r=20

100

 

150

E
S
M
R

2.5

2

1.5

1

0.5

 
0
0

50

|W

|/n

MF−LRSDP
Alternation
Damped Newton
OptSpace

1

4
Noise standard deviation s

2

3

 

5

(a) Reconstruction rate for different
sizes

(b) Reconstruction rate for differ-
ent ranks

(c) RMSE vs. noise std

Figure 2: (a) Reconstruction rate vs. fraction of revealed entries per column j(cid:10)j=n for rank 5 square matrices
of different sizes n by MF-LRSDP and OptSpace. MF-LRSDP reconstructs matrices from fewer observed
entries than OptSpace. (b) Reconstruction rate vs. j(cid:10)j=n for 500 (cid:2) 500 matrices of different ranks by MF-
LRSDP and OptSpace. Again MF-LRSDP needs fewer observations than OptSpace.
(c) RMSE vs. noise
standard deviation for rank 5  200 (cid:2) 200 matrices by MF-LRSDP  OptSpace  alternation and damped Newton.
All algorithms perform equally well.

Noisy Factorization: vary noise standard deviation.

For noisy data  we use the root mean
square error RMSE = 1=pmnjjM (cid:0) ^MjjF as a performance measure. We vary the standard
deviation (cid:27) of the additive noise for rank 5  200 (cid:2) 200 matrices and study the performance by
MF-LRSDP  OptSpace  alternation and damped Newton. Figure 2(c) shows that all the algorithms
perform equally well.
For timing comparisons  please refer to the supplementary material.

5.2 Evaluation with Real Data

We consider three problems: 1) af(cid:2)ne SfM 2) non-rigid SfM and 3) photometric stereo.
Afﬁne SfM. As discussed in section 3.3  for af(cid:2)ne SfM  the m(cid:2) n measurement matrix M is a rank
4 matrix with the last column of matrix B an all-one vector. M is generally an incomplete matrix
because not all the points are visible in all the cameras. We evaluate the performance of MF-LRSDP
on the ‘Dinosaur’ sequence used in [4  7]  for which M is a 72 (cid:2) 319 matrix with 72% missing
entries. We perform 25 trials and at each trial we provide the same random initializations to MF-
LRSDP  alternation and damped Newton (OptSpace has its only initialization technique). We use
the root mean square error over the observed entries  jjW (cid:12) (M (cid:0) ^M)jjF =pj(cid:10)j  as our performance
measure. Figure 3 shows the cumulative histogram over the RMS pixel error. MF-LRSDP gives
the best performance followed by damped Newton  alternation and OptSpace. We further tested the
algorithms on a ’longer Dinosaur’  the result of which is provided in the supplementary material.
Non-rigid SfM. In non-rigid SfM  non-rigid objects are expressed as a linear combination of b basis
shapes. In this case  the m (cid:2) n measurement matrix M can be expressed as M = AB T   where A
is an m (cid:2) 3b matrix and B is an n (cid:2) 3b matrix [3]. This makes M a rank 3b matrix. We test the
performance of the algorithms on the ’Giraffe’ sequence [4  7] for which M is a 240 (cid:2) 167 matrix
with 30% missing entries. We choose the rank as 6. Figure 3 shows the cumulative histogram of 25
trials from which we conclude that MF-LRSDP  alternation and damped Newton give good results.
Photometric Stereo. Photometric stereo is the problem of estimating the surface normals of an
object by imaging that object under different lighting conditions. Suppose we have n images of
the object under different lighting conditions with each image consisting of m pixels (m surface
normals) and we arrange them as an m (cid:2) n measurement matrix M. Then under Lambertian as-
sumptions  we can express M as M = ABT   where A is an m (cid:2) 3 matrix representing the surface
normals and re(cid:3)ectance and B is an n (cid:2) 3 matrix representing the light-source directions and in-
tensities [11]. Thus  M is a rank 3 matrix. Some of the image pixels are likely to be affected by
shadows and specularities and those pixels should not be included in the M matrix as they do not
obey the Lambertian assumption. This makes M  an incomplete matrix. We test the algorithms
on the ‘Face’ sequence [4  7] for which M is a 2944 (cid:2) 20 matrix with 42% missing entries. The
cumulative histogram in (cid:2)gure 3 shows that MF-LRSDP and damped Newton gives the best results
followed by alternation and OptSpace.

7

m
a
r
g
o
t
s
h
e
v

 

i

l

i
t
a
u
m
u
C

25

20

15

10

5

0

 

2

 

MF−LRSDP
Alternation
Damped Newton
OptSpace

4

6

RMS pixel error

8

(a) Dinosaur sequence

m
a
r
g
o
t
s
i
h
 
e
v
i
t
a
l
u
m
u
C

25

20

15

10

5

 
0
0

 

MF−LRSDP
Alternation
Damped Newton
OptSpace

25

20

15

10

5

m
a
r
g
o
t
s
i
h
 
e
v
i
t
a
l
u
m
u
C

 

MF−LRSDP
Alternation
damped Newton
OptSpace

0.2

0.4
RMS pixel error

0.6

0.8

1

(b) Giraffe sequence

 
0
0.035

0.04

0.045

RMS error

0.05

0.055

(c) Face sequence

Figure 3: Cumulative histogram (of 25 trials) for the Dinosaur  Giraffe and the Face sequence. For all of them 
MF-LRSDP consistently gives good results.

Additional constraints: Orthographic SfM. Orthographic SfM is a special case of af(cid:2)ne SfM 
where the camera matrix A satis(cid:2)es the additional constraint of ortho-normality  see section 3.3. We
show here that incorporating these constraints leads to a better solution. Figure 4 shows the input
point tracks  reconstructed point tracks without the constraints and reconstructed point tracks with
the constraints for the Dinosaur turntable sequence. Without the constraints many tracks fail to be
circular  whereas with the constraints all of them are circular (the dinosaur sequence is a turntable
sequence and the tracks are supposed to be circular). Thus  incorporating all the constraints of a
problem leads to better solution and MR-LRSDP provides a very (cid:3)exible framework for doing so.

(a) Input point tracks

(b) Reconstructed tracks without
constraints

(c) Reconstructed tracks with con-
straints

Figure 4: (a) Input (incomplete) point tracks of the Dinosaur turntable sequence  (b) reconstructed tracks
without orthonormality constraints and (c) reconstructed tracks with orthonormality contraints. Without the
constraints many tracks fail to be circular  whereas with the constraints all of them are circular (the dinosaur
sequence is a turntable sequence and the tracks are supposed to be circular).

6 Conclusion and Discussion

We have formulated the matrix factorization with missing data problem as a low-rank semide(cid:2)nite
programming problem MF-LRSDP. MF-LRSDP is an ef(cid:2)cient algorithm that can be used for solv-
ing large-scale factorization problems. It is also (cid:3)exible for handling many additional constraints
such as the ortho-normality constraints of orthographic SfM. Our empirical evaluations on synthetic
data show that it needs fewer observations for matrix factorization as compared to other algorithms
and it gives very good results on the real problems of SfM  non-rigid SfM and photometric stereo.
We note that though MF-LRSDP is a non-convex problem  it (cid:2)nds the global minimum under the
conditions of matrix completion theory. As a future work  it would be interesting to (cid:2)nd a theo-
retical justi(cid:2)cation for this. Also  it would be interesting to (cid:2)nd out how MF-LRSDP performs on
collaborative (cid:2)ltering problems.

References
[1] H. Aan(cid:230)s  R. Fisker  K. (cid:9)Astr¤om  and J. M. Carstensen. Robust factorization. IEEE TPAMI  2002.

8

[2] S. Brandt. Closed-form solutions for af(cid:2)ne reconstruction under missing data. In Stat. Methods for Video

Proc. (ECCV 02 Workshop)  2002.

[3] C. Bregler  A. Hertzmann  and H. Biermann. Recovering non-rigid 3d shape from image streams. In

CVPR  2000.

[4] A. M. Buchanan and A. W. Fitzgibbon. Damped newton algorithms for matrix factorization with missing

data. In CVPR  2005.

[5] S. Burer and C. Choi. Computational enhancements in low-rank semide(cid:2)nite programming. Optimization

Methods and Software  2006.

[6] S. Burer and R.D.C. Monteiro. A nonlinear programming algorithm for solving semide(cid:2)nite programs

via low-rank factorization. Mathematical Programming (series B  2001.

[7] Pei C. Optimization algorithms on subspaces: Revisiting missing data problem in low-rank matrix. IJCV 

2008.

[8] J. Cai  E. J. Cand(cid:30)es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Journal on Optimization  2010.

[9] E. J. Cand(cid:30)es and B. Recht. Exact matrix completion via convex optimization. Foundations on Computa-

tional Mathematics  2009.

[10] N. Guilbert  A.E. Bartoli  and A. Heyden. Af(cid:2)ne approximation for direct batch recovery of euclidian

structure and motion from sparse data. IJCV  2006.

[11] H. Hayakawa. Photometric stereo under a light source with arbitrary motion. JOSA  1994.
[12] D. Q. Huynh  R. Hartley  and A. Heyden. Outlier correction in image sequences for the af(cid:2)ne camera. In

ICCV  2003.

[13] D. W. Jacobs. Linear (cid:2)tting with missing data for structure-from-motion. CVIU  2001.
[14] Q. Ke and T. Kanade. Robust l1 norm factorization in the presence of outliers and missing data by

alternative convex programming. In CVPR  2005.

[15] R. H. Keshavan and S. Oh. A gradient descent algorithm on the grassman manifold for matrix completion.

CoRR  abs/0910.5260  2009.

[16] K. Lee and Y. Bresler. Admira: Atomic decomposition for minimum rank approximation. CoRR 

abs/0905.0044  2009.

[17] S. Ma  D. Goldfarb  and L. Chen. Fixed point and bregman iterative methods for matrix rank minimiza-

tion. Mathematical Programming  2009.

[18] D. Martinec and T. Pajdla. 3d reconstruction by (cid:2)tting low-rank matrices with missing data. In CVPR 

2005.

[19] R. Mazumder  T. Hastie  and R. Tibshirani. Spectral regularization algorithms for learning large incom-

plete matrices. http://www-stat.stanford.edu/ hastie/Papers/SVD JMLR.pdf  2009.

[20] R. Meka  P. Jain  and I. S. Dhillon. Guaranteed rank minimization via singular value projection. CoRR 

abs/0909.5457  2009.

[21] T. Okatani and K. Deguchi. On the wiberg algorithm for matrix factorization in the presence of missing

components. IJCV  2007.

[22] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.

In ICML  2005.

[23] H. Shum  K. Ikeuchi  and R. Reddy. Principal component analysis with missing data and its application

to polyhedral object modeling. IEEE TPAMI  1995.

[24] N. Srebro  J. D. M. Rennie  and T. Jaakkola. Maximum-margin matrix factorization. In NIPS  2004.
[25] J. P. Tardif  A. Bartoli  M. Trudeau  N. Guilbert  and S. Roy. Algorithms for batch matrix factorization

with application to structure-from-motion. In CVPR  2007.

[26] C. Tomasi and T. Kanade. Shape and motion from image streams under orthography: a factorization

method. IJCV  1992.

[27] L. Vandenberghe and S. Boyd. Semide(cid:2)nite programming. SIAM Rev.  1996.
[28] R. Vidal and R. Hartley. Motion segmentation with missing data using powerfactorization and gpca. In

In CVPR  2004.

9

,Han Liu
Lie Wang
Tuo Zhao