2019,Distribution-Independent PAC Learning of Halfspaces with Massart Noise,We study the problem of {\em distribution-independent} PAC learning of halfspaces in the presence of Massart noise. 
Specifically  we are given a set of labeled examples $(\bx  y)$ drawn 
from a distribution $\D$ on $\R^{d+1}$ such that the marginal distribution 
on the unlabeled points $\bx$ is arbitrary and the labels $y$ are generated by an unknown halfspace 
corrupted with Massart noise at noise rate $\eta<1/2$. The goal is to find 
a hypothesis $h$ that minimizes the misclassification error $\pr_{(\bx  y) \sim \D} \left[ h(\bx) \neq y \right]$. 

We give a $\poly\left(d  1/\eps\right)$ time algorithm for this problem with misclassification error $\eta+\eps$. 
We also provide evidence that improving on the error guarantee of our algorithm
might be computationally hard. Prior to our work  no efficient weak (distribution-independent) learner 
was known in this model  even for the class of disjunctions. The existence of such an algorithm 
for halfspaces (or even disjunctions) has been posed as an open question in various works  
starting with Sloan (1988)  Cohen (1997)  and was most recently highlighted in Avrim Blum's FOCS 2003 tutorial.,Distribution-Independent PAC Learning of

Halfspaces with Massart Noise

Ilias Diakonikolas

University of Wisconsin-Madison

ilias@cs.wisc.edu

Themis Gouleakis

Max Planck Institute for Informatics
tgouleak@mpi-inf.mpg.de

Christos Tzamos

University of Wisconsin-Madison

tzamos@wisc.edu

Abstract

We study the problem of distribution-independent PAC learning of halfspaces in
the presence of Massart noise. Speciﬁcally  we are given a set of labeled examples
(x  y) drawn from a distribution D on Rd+1 such that the marginal distribution on
the unlabeled points x is arbitrary and the labels y are generated by an unknown
halfspace corrupted with Massart noise at noise rate η < 1/2. The goal is to ﬁnd a
hypothesis h that minimizes the misclassiﬁcation error Pr(x y)∼D [h(x) (cid:54)= y].
We give a poly (d  1/) time algorithm for this problem with misclassiﬁcation
error η + . We also provide evidence that improving on the error guarantee of
our algorithm might be computationally hard. Prior to our work  no efﬁcient
weak (distribution-independent) learner was known in this model  even for the
class of disjunctions. The existence of such an algorithm for halfspaces (or even
disjunctions) has been posed as an open question in various works  starting with
Sloan (1988)  Cohen (1997)  and was most recently highlighted in Avrim Blum’s
FOCS 2003 tutorial.

Introduction

1
Halfspaces  or Linear Threshold Functions (henceforth LTFs)  are Boolean functions f : Rd → {±1}
of the form f (x) = sign((cid:104)w  x(cid:105) − θ)  where w ∈ Rd is the weight vector and θ ∈ R is the threshold.
(The function sign : R → {±1} is deﬁned as sign(u) = 1 if u ≥ 0 and sign(u) = −1 otherwise.)
The problem of learning an unknown halfspace is as old as the ﬁeld of machine learning — starting
with Rosenblatt’s Perceptron algorithm [Ros58] — and has arguably been the most inﬂuential problem
in the development of the ﬁeld. In the realizable setting  LTFs are known to be efﬁciently learnable
in Valiant’s distribution-independent PAC model [Val84] via Linear Programming [MT94]. In the
presence of corrupted data  the situation is more subtle and crucially depends on the underlying noise
model. In the agnostic model [Hau92  KSS94] – where an adversary is allowed to arbitrarily corrupt
an arbitrary η < 1/2 fraction of the labels – even weak learning is known to be computationally
intractable [GR06  FGKP06  Dan16]. On the other hand  in the presence of Random Classiﬁcation
Noise (RCN) [AL88] – where each label is ﬂipped independently with probability exactly η < 1/2 –
a polynomial time algorithm is known [BFKV96  BFKV97].
In this work  we focus on learning halfspaces with Massart noise [MN06]:
Deﬁnition 1.1 (Massart Noise Model). Let C be a class of Boolean functions over X = Rd  Dx be
an arbitrary distribution over X  and 0 ≤ η < 1/2. Let f be an unknown target function in C. A
noisy example oracle  EXMas(f Dx  η)  works as follows: Each time EXMas(f Dx  η) is invoked  it

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

returns a labeled example (x  y)  where x ∼ Dx  y = f (x) with probability 1− η(x) and y = −f (x)
with probability η(x)  for an unknown parameter η(x) ≤ η. Let D denote the joint distribution on
(x  y) generated by the above oracle. A learning algorithm is given i.i.d. samples from D and its goal
is to output a hypothesis h such that with high probability the error Pr(x y)∼D[h(x) (cid:54)= y] is small.
An equivalent formulation of the Massart model [Slo88  Slo92] is the following: With probability
1 − η  we have that y = f (x)  and with probability η the label y is controlled by an adversary. Hence 
the Massart model lies in between the RCN and the agnostic models. (Note that the RCN model
corresponds to the special case that η(x) = η for all x ∈ X.) It is well-known (see  e.g.  [MN06]) that
poly(d  1/) samples information-theoretically sufﬁce to compute a hypothesis with misclassiﬁcation
error OPT +   where OPT is the misclassiﬁcation error of the optimal halfspace. Also note that
OPT ≤ η by deﬁnition. The question is whether a polynomial time algorithm exists.

The existence of an efﬁcient distribution-independent learning algorithm for halfspaces (or even
disjunctions) in the Massart model has been posed as an open question in a number of works. In the
ﬁrst COLT conference [Slo88] (see also [Slo92])  Sloan deﬁned the malicious misclassiﬁcation noise
model (an equivalent formulation of Massart noise  described above) and asked whether there exists an
efﬁcient learning algorithm for disjunctions in this model. About a decade later  Cohen [Coh97] asked
the same question for the more general class of all LTFs. The question remained open — even for
weak learning of disjunctions! — and was highlighted in Avrim Blum’s FOCS 2003 tutorial [Blu03].
Speciﬁcally  prior to this work  even the following very basic special case remained open:

Given labeled examples from an unknown disjunction  corrupted with 1% Massart noise 

can we efﬁciently ﬁnd a hypothesis that achieves misclassiﬁcation error 49%?

The reader is referred to slides 39-40 of Avrim Blum’s FOCS’03 tutorial [Blu03]  where it is suggested
that the above problem might be easier than agnostically learning disjunctions. As a corollary of our
main result (Theorem 1.2)  we answer this question in the afﬁrmative. In particular  we obtain an
efﬁcient algorithm that achieves misclassiﬁcation error arbitrarily close to η for all LTFs.

1.1 Our Results

The main result of this paper is the following:
Theorem 1.2 (Main Result). There is an algorithm that for all 0 < η < 1/2  on input a set of i.i.d.
examples from a distribution D = EXMas(f Dx  η) on Rd+1  where f is an unknown halfspace on
Rd  it runs in poly(d  b  1/) time  where b is an upper bound on the bit complexity of the examples 
and outputs a hypothesis h that with high probability satisﬁes Pr(x y)∼D[h(x) (cid:54)= y] ≤ η + .
See Theorem 2.9 for a more detailed formal statement. For large-margin halfspaces  we obtain a
slightly better error guarantee; see Theorem 2.2 and Remark 2.6.

Discussion. We note that our algorithm is non-proper  i.e.  the hypothesis h itself is not a halfspace.
The polynomial dependence on b in the runtime cannot be removed  even in the noiseless case 
unless one obtains strongly-polynomial algorithms for linear programming. Finally  we note that the
misclassiﬁcation error of η translates to error 2η +  with respect to the target LTF.
Our algorithm gives error η +   instead of the information-theoretic optimum of OPT + . To
complement our positive result  we provide some evidence that improving on our (η + ) error
guarantee may be challenging. Roughly speaking  we show (see Theorems B.1 and B.2 in the
supplementary material) that natural approaches — involving convex surrogates and reﬁnements
thereof — inherently fail  even under margin assumptions. (See Section 1.2 for a discussion.)

Broader Context. This work is part of the broader agenda of designing robust estimators in the
distribution-independent setting with respect to natural noise models. A recent line of work [KLS09 
ABL17  DKK+16  LRV16  DKK+17  DKK+18  DKS18  KKM18  DKS19  DKK+19] has given
efﬁcient robust estimators for a range of learning tasks (both supervised and unsupervised) in the
presence of a small constant fraction of adversarial corruptions. A limitation of these results is
the assumption that the good data comes from a “tame” distribution  e.g.  Gaussian or isotropic
log-concave distribution. On the other hand  if no assumption is made on the good data and the noise
remains fully adversarial  these problems become computationally intractable [Ber06  GR06  Dan16].

2

This suggests the following general question: Are there realistic noise models that allow for efﬁcient
algorithms without imposing (strong) assumptions on the good data? Conceptually  the algorithmic
results of this paper could be viewed as an afﬁrmative answer to this question for the problem of
learning halfspaces.

1.2 Technical Overview

In this section  we provide an outline of our approach and a comparison to previous techniques. Since
the distribution on the unlabeled data is arbitrary  we can assume w.l.o.g. that the threshold θ = 0.

Massart Noise versus RCN. Random Classiﬁcation Noise (RCN) [AL88] is the special case of
Massart noise where each label is ﬂipped with probability exactly η < 1/2. At ﬁrst glance  it
might seem that Massart noise is easier to deal with computationally than RCN. After all  in the
Massart model we add at most as much noise as in the RCN model. It turns out that this intuition is
fundamentally ﬂawed. Roughly speaking  the ability of the Massart adversary to choose whether to
perturb a given label and  if so  with what probability (which is unknown to the learner)  makes the
design of efﬁcient algorithms in this model challenging. In particular  the well-known connection
between learning with RCN and the Statistical Query (SQ) model [Kea93  Kea98] no longer holds  i.e. 
the property of being an SQ algorithm does not automatically sufﬁce for noise-tolerant learning with
Massart noise. We note that this connection with the SQ model is leveraged in [BFKV96  BFKV97]
to obtain their polynomial time algorithm for learning halfspaces with RCN.

Large Margin Halfspaces. To illustrate our approach  we start by describing our learning algorithm
for γ-margin halfspaces on the unit ball. That is  we assume |(cid:104)w∗  x(cid:105)| ≥ γ for every x in the support 
where w∗ ∈ Rd with (cid:107)w∗(cid:107)2 = 1 deﬁnes the target halfspace hw∗ (x) = sign((cid:104)w∗  x(cid:105)). Our goal is
to design a poly(d  1/  1/γ) time learning algorithm in the presence of Massart noise.

In the RCN model  the large margin case is easy because the learning problem is essentially
convex. That is  there is a convex surrogate that allows us to formulate the problem as a convex
program. We can use SGD to ﬁnd a near-optimal solution to this convex program  which automatically
gives a strong proper learner. This simple fact does not appear explicitly in the literature  but follows
easily from standard tools.
[Byl94] showed that a variant of the Perceptron algorithm (which
can be viewed as gradient descent on a particular convex objective) learns γ-margin halfspaces in
poly(d  1/  1/γ) time. The algorithm in [Byl94] requires an additional anti-concentration condition
about the distribution  which is easy to remove. In Appendix C  we show that a “smoothed” version
of Bylander’s objective sufﬁces as a convex surrogate under only the margin assumption.

Roughly speaking  the reason that a convex surrogate works for RCN is that the expected effect
of the noise on each label is known a priori. Unfortunately  this is not the case for Massart noise. We
show (Theorem B.1 in Appendix B) that no convex surrogate can lead to a weak learner  even under

a margin assumption. That is  if (cid:98)w is the minimizer of G(w) = E(x y)∼D[φ(y(cid:104)w  x(cid:105))]  where φ can
be any convex function  then the hypothesis sign((cid:104)(cid:98)w  x(cid:105)) is not even a weak learner. So  in sharp

contrast with the RCN case  the problem is non-convex in this sense.

Our Massart learning algorithm for large margin halfspaces still uses a convex surrogate  but in a
qualitatively different way. Instead of attempting to solve the problem in one-shot  our algorithm
adaptively applies a sequence of convex optimization problems to obtain an accurate solution
in disjoint subsets of the space. Our iterative approach is motivated by a new structural lemma
(Lemma 2.5) establishing the following: Even though minimizing a convex proxy does not lead to
small misclassiﬁcation error over the entire space  there exists a region with non-trivial probability
mass where it does. Moreover  this region is efﬁciently identiﬁable by a simple thresholding rule.
Speciﬁcally  we show that there exists a threshold T > 0 (which can be found algorithmically) such

that the hypothesis sign((cid:104)(cid:98)w  x(cid:105)) has error bounded by η +  in the region RT = {x : |(cid:104)(cid:98)w  x(cid:105)| ≥ T}.
Here (cid:98)w is any near-optimal solution to an appropriate convex optimization problem  deﬁned via a

convex surrogate objective similar to the one used in [Byl94]. We note that Lemma 2.5 is the main
technical novelty of this paper and motivates our algorithm. Given Lemma 2.5  in any iteration i
we can ﬁnd the best threshold T (i) using samples  and obtain a learner with misclassiﬁcation error
η +  in the corresponding region. Since each region has non-trivial mass  iterating this scheme a
small number of times allows us to ﬁnd a non-proper hypothesis (a decision-list of halfspaces) with
misclassiﬁcation error at most η +  in the entire space.

3

The idea of iteratively optimizing a convex surrogate was used in [BFKV96] to learn halfspaces
with RCN without a margin. Despite this similarity  we note that the algorithm of [BFKV96] fails
to even obtain a weak learner in the Massart model. We point out two crucial technical differences:
First  the iterative approach in [BFKV96] was needed to achieve polynomial running time. As
mentioned already  a convex proxy is guaranteed to converge to the true solution with RCN  but the
convergence may be too slow (when the margin is tiny). In contrast  with Massart noise (even under a
margin condition) convex surrogates cannot even give weak learning in the entire domain. Second 
the algorithm of [BFKV96] used a ﬁxed threshold in each iteration  equal to the margin parameter
obtained after an appropriate pre-processing of the data (that is needed in order to ensure a weak
margin property). In contrast  in our setting  we need to ﬁnd an appropriate threshold T (i) in each
iteration i  according to the criterion speciﬁed by our Lemma 2.5.

General Case. Our algorithm for the general case (in the absence of a margin) is qualitatively similar
to our algorithm for the large margin case  but the details are more elaborate. We borrow an idea
from [BFKV96] that in some sense allows us to “reduce” the general case to the large margin case.
Speciﬁcally  [BFKV96] (see also [DV04a]) developed a pre-processing routine that slightly modiﬁes
the distribution on the unlabeled points and guarantees the following weak margin property: After pre-
processing  there exists an explicit margin parameter σ = Ω(1/poly(d  b))  such that any hyperplane
through the origin has at least a non-trivial mass of the distribution at distance at least σ from it.
Using this pre-processing step  we are able to adapt our algorithm from the previous subsection to
work without margin assumptions in poly(d  b  1/) time. While our analysis is similar in spirit
to the case of large margin  we note that the margin property obtained via the [BFKV96  DV04a]
preprocessing step is (necessarily) weaker  hence additional careful analysis is required.

Lower Bounds Against Natural Approaches. We have already explained our Theorem B.1  which
shows that using a convex surrogate over the entire space cannot not give a weak learner. Our
algorithm  however  can achieve error η +  by iteratively optimizing a speciﬁc convex surrogate
in disjoint subsets of the domain. A natural question is whether one can obtain qualitatively better
accuracy  e.g.  f (OPT)+  by using a different convex objective function in our iterative thresholding
approach. We show (Theorem B.2) that such an improvement is not possible: Using a different
convex proxy cannot lead to error better than (1− o(1))· η. It is a plausible conjecture that improving
on the error guarantee of our algorithm is computationally hard. We leave this as an intriguing open
problem for future work.

1.3 Prior and Related Work

Bylander [Byl94] gave a polynomial time algorithm to learn large margin halfspaces with RCN
(under an additional anti-concentration assumption). The work of Blum et al. [BFKV96  BFKV97]
gave the ﬁrst polynomial time algorithm for distribution-independent learning of halfspaces with
RCN without any margin assumptions. Soon thereafter  [Coh97] gave a polynomial-time proper
learning algorithm for the problem. Subsequently  Dunagan and Vempala [DV04b] gave a rescaled
perceptron algorithm for solving linear programs  which translates to a signiﬁcantly simpler and
faster proper learning algorithm.

The term “Massart noise” was coined after [MN06]. An equivalent version of the model was
previously studied by Rivest and Sloan [Slo88  Slo92  RS94  Slo96]  and a very similar asymmetric
random noise model goes back to Vapnik [Vap82]. Prior to this work  essentially no efﬁcient
algorithms with non-trivial error guarantees were known in the distribution-free Massart noise model.
It should be noted that polynomial time algorithms with error OPT +  are known [ABHU15  ZLC17 
YZ17] when the marginal distribution on the unlabeled data is uniform on the unit sphere. For the
case that the unlabeled data comes from an isotropic log-concave distribution  [ABHZ16] give a
d2poly(1/(1−2η))

/poly() sample and time algorithm.

1.4 Preliminaries
For n ∈ Z+  we denote [n] def= {1  . . .   n}. We will use small boldface characters for vectors and we
let ei denote the i-th vector of an orthonormal basis.

4

def= ((cid:80)d

For x ∈ Rd  and i ∈ [d]  xi denotes the i-th coordinate of x  and (cid:107)x(cid:107)2

i )1/2 denotes
the (cid:96)2-norm of x. We will use (cid:104)x  y(cid:105) for the inner product between x  y ∈ Rd. We will use E[X] for
the expectation of random variable X and Pr[E] for the probability of event E.

i=1 x2

An origin-centered halfspace is a Boolean-valued function hw : Rd → {±1} of the form
hw(x) = sign ((cid:104)w  x(cid:105))  where w ∈ Rd. (Note that we may assume w.l.o.g. that (cid:107)w(cid:107)2 = 1.) We
denote by Hd the class of all origin-centered halfspaces on Rd.
We consider a classiﬁcation problem where labeled examples (x  y) are drawn i.i.d. from a
distribution D. We denote by Dx the marginal of D on x  and for any x denote Dy(x) the distribution
of y conditional on x. Our goal is to ﬁnd a hypothesis classiﬁer h with low misclassiﬁcation error.
We will denote the misclassiﬁcation error of a hypothesis h with respect to D by errD
0−1(h) =
Pr(x y)∼D[h(x) (cid:54)= y]. Let OPT = minh∈Hd errD
0−1(h) denote the optimal misclassiﬁcation error
of any halfspace  and w∗ be the normal vector to a halfspace hw∗ that achieves this.

2 Algorithm for Learning Halfspaces with Massart Noise

In this section  we present the main result of this paper  which is an efﬁcient algorithm that achieves
η +  misclassiﬁcation error for distribution-independent learning of halfspaces with Massart noise η.
Our algorithm uses (stochastic) gradient descent on a convex proxy function L(w) for the
misclassiﬁcation error to identify a region with small misclassiﬁcation error. The loss function
penalizes the points which are misclassiﬁed by the threshold function hw  proportionally to the
distance from the corresponding hyperplane  while rewards the correctly classiﬁed points at a smaller
rate. Directly optimizing this convex objective does not lead to a separator with low error  but
guarantees that for a non-negligible fraction of the mass away from the separating hyperplane
the misclassiﬁcation error will be at most η + . Classifying points in this region according to
the hyperplane and recursively working on the remaining points  we obtain an improper learning
algorithm that achieves η +  error overall.

We now develop some necessary notation before proceeding with the description and analysis of

our algorithm.

Our algorithm considers the following convex proxy for the misclassiﬁcation error as a function

of the weight vector w:

L(w) = E

(x y)∼D[LeakyReluλ(−y(cid:104)w  x(cid:105))]  
(cid:26) (1 − λ)z

under the constraint (cid:107)w(cid:107)2 ≤ 1  where LeakyReluλ(z) =
leakage parameter  which we will set to be λ ≈ η.

λz

if z ≥ 0
if z < 0 and λ is the

We deﬁne the per-point misclassiﬁcation error and the error of the proxy function as err(w  x) =

Pry∼Dy(x)[w(x) (cid:54)= y] and (cid:96)(w  x) = Ey∼Dy(x)[LeakyReluλ(−y(cid:104)w  x(cid:105))] respectively.

Notice that errD

0−1(hw) = Ex∼Dx [err(w  x)] and L(w) = Ex∼Dx [(cid:96)(w  x)]. Moreover 

OPT = Ex∼Dx[err(w∗  x)] = Ex∼Dx [η(x)].
Relationship between proxy loss and misclassiﬁcation error We ﬁrst relate the proxy loss and
the misclassiﬁcation error.
Claim 2.1. For any w  x  we have that (cid:96)(w  x) = (err(w  x) − λ)|(cid:104)w  x(cid:105)|.

Proof. We consider two cases:
• Case sign((cid:104)w  x(cid:105)) = sign((cid:104)w∗  x(cid:105)):

(cid:96)(w  x) = η(x)(1 − λ)|(cid:104)w  x(cid:105)| − (1 − η(x))λ|(cid:104)w  x(cid:105)| = (η(x) − λ)|(cid:104)w  x(cid:105)|.

In this case  we have that err(w  x) = η(x)  while

• Case sign((cid:104)w  x(cid:105)) (cid:54)= sign((cid:104)w∗  x(cid:105)): In this case  we have that err(w  x) = 1 − η(x)  while

(cid:96)(w  x) = (1 − η(x))(1 − λ)|(cid:104)w  x(cid:105)| − η(x)λ|(cid:104)w  x(cid:105)| = (1 − η(x) − λ)|(cid:104)w  x(cid:105)|.

This completes the proof of Claim 2.1.

5

(cid:104) (cid:96)(w x)

(cid:105)

is equivalent to minimizing the misclassiﬁcation
Claim 2.1 shows that minimizing Ex∼Dx
error. Unfortunately  this objective is hard to minimize as it is non-convex  but one would hope that
minimizing L(w) instead may have a similar effect. As we show  this is not true because |(cid:104)w  x(cid:105)|
might vary signiﬁcantly across points  and in fact it is not possible to use a convex proxy that achieves
bounded misclassiﬁcation error directly.

|(cid:104)w x(cid:105)|

Our algorithm circumvents this difﬁculty by approaching the problem indirectly to ﬁnd a non-
proper classiﬁer. Speciﬁcally  our algorithm works in multiple rounds  where within each round
only points with high value of |(cid:104)w  x(cid:105)| are considered. The intuition is based on the fact that the
approximation of the convex proxy to the misclassiﬁcation error is more accurate for those points
that have comparable distance to the halfspace.
In Section 2.1  we handle the large margin case and in Section 2.2 we handle the general case.

2.1 Warm-up: Learning Large Margin Halfspaces

We consider the case that there is no probability mass within distance γ from the separating hyperplane
(cid:104)w∗  x(cid:105) = 0  (cid:107)w∗(cid:107)2 = 1. Formally  assume that for every x ∼ Dx  (cid:107)x(cid:107)2 ≤ 1 and that (cid:104)w∗  x(cid:105) ≥ γ.
The pseudo-code of our algorithm is given in Algorithm 1. Our algorithm returns a decision list
[(w(1)  T (1))  (w(2)  T (2)) ··· ] as output. To classify a point x given the decision list  the ﬁrst i is
identiﬁed such that |(cid:104)w(i)  x(cid:105)| ≥ T (i) and sign((cid:104)w(i)  x(cid:105)) is returned. If no such i exists  an arbitrary
prediction is returned.

Algorithm 1 Main Algorithm (with margin)
1: Set S(1) = Rd  λ = η +   m = ˜O( 1
γ24 ).
2: Set i ← 1.

3: Draw O(cid:0)(1/2) log(1/(γ))(cid:1) samples from Dx to form an empirical distribution ˜Dx.

(cid:2)x ∈ S(i)(cid:3) ≥  do

4: while Prx∼ ˜Dx
5:
6:
7:

L(i)(w(i)) ≤ minw:(cid:107)w(cid:107)2≤1 L(i)(w) + γ/2.

Set D(i) = D|S(i)  the distribution conditional on the unclassiﬁed points.
Let L(i)(w) = E(x y)∼D(i)[LeakyReluλ(−y(cid:104)w  x(cid:105))]
Run SGD on L(i)(w) for ˜O(1/(γ22)) iterations to get w(i) with (cid:107)w(i)(cid:107)2 = 1 such that
Draw m samples from D(i) to form an empirical distribution D(i)
m .
Find a threshold T (i) such that Pr(x y)∼D(i)

[hw(i)(x) (cid:54)= y(cid:12)(cid:12)|(cid:104)w(i)  x(cid:105)| ≥ T (i)]  is minimized.

[|(cid:104)w(i)  x(cid:105)| ≥ T (i)] ≥ γ and the empirical

misclassiﬁcation error  Pr(x y)∼D(i)

m

Update the unclassiﬁed region S(i+1) ← S(i) \ {x : |(cid:104)w(i)  x(cid:105)| ≥ T (i)} and set i ← i + 1.

m

10:
11: Return the classiﬁer [(w(1)  T (1))  (w(2)  T (2)) ··· ]

8:
9:

The main result of this section is the following:
Theorem 2.2. Let D be a distribution on Bd × {±1} such that Dx satisﬁes the γ-margin property
with respect to w∗ and y is generated by sign((cid:104)w∗  x(cid:105)) corrupted with Massart noise at rate η < 1/2.
Algorithm 1 uses ˜O(1/(γ35)) samples from D  runs in poly(d  1/  1/γ) time  and returns  with
probability 2/3  a classiﬁer h with misclassiﬁcation error errD
Our analysis focuses on a single iteration of Algorithm 1. We will show that a large fraction of
the points is classiﬁed at every iteration within error η + . To achieve this  we analyze the convex
objective L. We start by showing that the optimal classiﬁer w∗ obtains a signiﬁcantly negative
objective value.
Lemma 2.3. If λ ≥ η  then L(w∗) ≤ −γ(λ − OPT).

0−1(h) ≤ η + .

Proof. For any ﬁxed x  using Claim 2.1  we have that

(cid:96)(w∗  x) = (err(w∗  x) − λ)|(cid:104)w∗  x(cid:105)| = (η(x) − λ)|(cid:104)w∗  x(cid:105)| ≤ −γ(λ − η(x))  

since |(cid:104)w∗  x(cid:105)| ≥ γ and η(x) − λ ≤ 0. Taking expectation over x ∼ Dx  the statement follows.

6

Lemma 2.3 is the only place where the Massart noise assumption is used in our approach and
establishes that points with sufﬁciently negative value exist. As we will show  any weight vector
w with this property can be found with few samples and must accurately classify some region of
non-negligible mass away from it (Lemma 2.5).

We now argue that we can use stochastic gradient descent (SGD) to efﬁciently identify a point w
that achieves comparably small objective value to the guarantee of Lemma 2.3. We use the following
standard property of SGD:
Lemma 2.4 (see  e.g.  Theorem 3.4.11 in [Duc16]). Let L be any convex function. Consider the
(projected) SGD iteration that is initialized at w(0) = 0 and for every step computes

w(t+ 1

w:(cid:107)w(cid:107)2≤1

2 ) = w(t) − ρv(t)

where v(t) is a stochastic gradient such that for all steps E[v(t)|w(t)] ∈ ∂L(w(t)) and(cid:13)(cid:13)v(t)(cid:13)(cid:13)2 ≤ 1.

and w(t+1) = arg min

Assume that SGD is run for T iterations with step size ρ = 1√
t=1 w(t). Then  for
T
any   δ > 0  after T = Ω(log(1/δ)/2) iterations with probability with probability at least 1 − δ we
have that L( ¯w) ≤ minw:(cid:107)w(cid:107)2≤1 L(w) + .

that by running SGD on L(w) with projection to the unit (cid:96)2-ball for O(cid:0)log(1/δ)/(γ2(λ − OPT)2)(cid:1)

By Lemma 2.3  we know that minw:(cid:107)w(cid:107)2≤1 L(w) ≤ −γ(λ − OPT). By Lemma 2.4  it follows

steps  we ﬁnd a w such that L(w) ≤ −γ(λ − OPT)/2 with probability at least 1 − δ.

and let ¯w = 1
T

 

2 )(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)w − w(t+ 1
(cid:80)T

Note that we can assume without loss of generality that (cid:107)w(cid:107)2 = 1  as increasing the magnitude

of w only decreases the objective value.

We now consider the misclassiﬁcation error of the halfspace hw conditional on the points that are
further than some distance T from the separating hyperplane. We claim that there exists a threshold
T > 0 where the restriction has non-trivial mass and the conditional misclassiﬁcation error is small:
Lemma 2.5. Consider a vector w with L(w) < 0. There exists a threshold T ≥ 0 such that (i)
Pr(x y)∼D[|(cid:104)w  x(cid:105)| ≥ T ] ≥ |L(w)|
.

2λ   and (ii) Pr(x y)∼D[hw(x) (cid:54)= y(cid:12)(cid:12)|(cid:104)w  x(cid:105)| ≥ T ] ≤ λ − |L(w)|
Proof. We will show there is a T ≥ 0 such that Pr(x y)∼D[hw(x) (cid:54)= y(cid:12)(cid:12)|(cid:104)w  x(cid:105)| ≥ T ] ≤ λ − ζ 
where ζ def= |L(w)|/2  or equivalently  Ex∼Dx[(err(w  x) − λ + ζ)1|(cid:104)w x(cid:105)|≥T ] ≤ 0.
(cid:90) 1
For a T drawn uniformly at random in [0  1]  we have that:

[(err(w  x) − λ + ζ)1|(cid:104)w x(cid:105)|≥T ]dT = Ex∼Dx[(err(w  x) − λ)|(cid:104)w  x(cid:105)|] + ζEx∼Dx [|(cid:104)w  x(cid:105)|]

E

2

≤ Ex∼Dx[(cid:96)(w  x)] + ζ = L(w) + ζ = L(w)/2 < 0 .
Thus  there exists a ¯T such that Ex∼Dx[(err(w  x)− λ + ζ)1|(cid:104)w x(cid:105)|≥ ¯T ] ≤ 0. Consider the minimum
such ¯T . Then we have

x∼Dx

0

(cid:90) 1

¯T

By deﬁnition of ¯T   it must be the case that(cid:82) ¯T

Ex∼Dx [(err(w  x) − λ + ζ)1|(cid:104)w x(cid:105)|≥T ]dT ≥ −λ · Pr(x y)∼D[|(cid:104)w  x(cid:105)| ≥ ¯T ] .
(cid:90) 1

0 Ex∼Dx [(err(w  x) − λ + ζ)1|(cid:104)w x(cid:105)|≥T ]dT ≥ 0.

Ex∼Dx[(err(w  x) − λ + ζ)1|(cid:104)w x(cid:105)|≥T ]dT ≥ −λ · Pr(x y)∼D[|(cid:104)w  x(cid:105)| ≥ ¯T ]  

Therefore 

≥

L(w)

2

¯T

which implies that Pr(x y)∼D[|(cid:104)w  x(cid:105)| ≥ ¯T ] ≥ |L(w)|

2λ . This completes the proof of Lemma 2.5.

Even though minimizing the convex proxy L does not lead to low misclassiﬁcation error overall 
Lemma 2.5 shows that there exists a region of non-trivial mass where it does. This region is
identiﬁable by a simple threshold rule. We are now ready to prove Theorem 2.2.

7

Proof of Theorem 2.2. We consider the steps of Algorithm 1 in each iteration of the while loop. At
iteration i  we consider a distribution D(i) consisting only of points not handled in previous iterations.
We start by noting that with high probability the total number of iterations is ˜O(1/(γ)). This can
be seen as follows: The empirical probability mass under D(i)
m of the region {x : |(cid:104)w(i)  x(cid:105)| ≥ T (i)}
removed from S(i) to obtain S(i+1) is at least γ (Step 9). Since m = ˜O(1/(γ24))  the DKW
inequality [DKW56] implies that the true probability mass of this region is at least γ/2 with high
probability. By a union bound over i ≤ K = Θ(log(1/)/(γ))  it follows that with high probability
we have that PrDx[S(i+1)] ≤ (1 − γ/2)i for all i ∈ [K]. After K iterations  we will have that
PrDx[S(i+1)] ≤ /3. Step 3 guarantees that the mass of S(i) under ˜Dx is within an additive /3 of
its mass under Dx  for i ∈ [K]. This implies that the loop terminates after at most K iterations.

O(cid:0)log(1/δ)/(γ22)(cid:1) steps  we obtain a w(i) such that  with probability at least 1 − δ  it holds

By Lemma 2.3 and the fact that every D(i) has margin γ  it follows that the minimizer of the
loss L(i) has value less than −γ(λ − OPT(i)) ≤ −γ  as OPT(i) ≤ η and λ = η + . By the
guarantees of Lemma 2.4  running SGD in line 7 on L(i)(·) with projection to the unit (cid:96)2-ball for
L(i)(w(i)) ≤ −γ/2 and (cid:107)w(i)(cid:107)2 = 1. Here δ > 0 is a parameter that is selected so that the
following claim holds: With probability at least 9/10  for all iterations i of the while loop we have
that L(i)(w(i)) ≤ −γ/2. Since the total number of iterations is ˜O(1/(γ))  setting δ to ˜Ω(γ) and
applying a union bound over all iterations gives the previous claim. Therefore  the total number
of SGD steps per iteration is ˜O(1/(γ22)). For a given iteration of the while loop  running SGD

as Prx∼Dx

(cid:2)x ∈ S(i)(cid:3) ≥ 2/3.

requires ˜O(1/(γ22)) samples from D(i) which translate to at most ˜O(cid:0)1/(γ23)(cid:1) samples from D 
(b)) Pr(x y)∼D(i) [hw(x) (cid:54)= y(cid:12)(cid:12)|(cid:104)w  x(cid:105)| ≥ T ] ≤ η + . Line 9 of Algorithm 1 estimates the threshold

Lemma 2.5 implies that there exists T ≥ 0 such that: (a) Pr(x y)∼D(i) [|(cid:104)w  x(cid:105)| ≥ T ] ≥ γ  and

using samples. By the DKW inequality [DKW56]  we know that with m = ˜O(1/(γ24)) samples
we can estimate the CDF within error γ2 with probability 1 − poly(  γ). This sufﬁces to estimate
the probability mass of the region within additive γ2 and the misclassiﬁcation error within /3. This
is satisﬁed for all iterations with constant probability.

In summary  with high constant success probability  Algorithm 1 runs for ˜O(1/(γ)) iterations
and draws ˜O(1/(γ24)) samples per round for a total of ˜O(1/(γ35)) samples. As each iteration
runs in polynomial time  the total running time follows.

When the while loop terminates  we have that Prx∼Dx [x ∈ S(i)] ≤ 4/3  i.e.  we will have
accounted for at least a (1− 4/3)-fraction of the total probability mass. Since our algorithm achieves
misclassiﬁcation error at most η + 4/3 in all the regions we accounted for  its total misclassiﬁcation
error is at most η + 8/3. Rescaling  by a constant factor gives Theorem 2.2.
Remark 2.6. If the value of OPT is smaller than η − ξ for some value ξ > 0  Algorithm 1 gets
misclassiﬁcation error less than η − Ω(γ2ξ2) when run for  = O(γ2ξ2). This is because  in the ﬁrst
iteration  L(1)(w(1)) ≤ −γ(λ− OPT)/2 ≤ −γξ/2  which implies  by Lemma 2.5  that the obtained
error in S(1) is at most λ − γξ/4. The misclassiﬁcation error in the remaining regions is at most
λ +   and region S(1) has probability mass at least γξ/4. Thus  the total misclassiﬁcation error is at
most λ +  − γ2ξ2/16 = η − Ω(γ2ξ2)  when run for  = O(γ2ξ2).

2.2 The General Case
In the general case  we assume that Dx is an arbitrary distribution supported on b-bit integers. While
such a distribution might have exponentially small margin in the dimension d (or even 0)  we will
preprocess the distribution to ensure a margin condition by removing outliers.

We will require the following notion of an outlier:

Deﬁnition 2.7 ([DV04a]). We call a point x in the support of a distribution Dx a β-outlier  if there
exists a vector w ∈ Rd such that (cid:104)w  x(cid:105)2 ≤ β Ex∼Dx [(cid:104)w  x(cid:105)2].

We will use Theorem 3 of [DV04a]  which shows that any distribution supported on b-bit integers

can be efﬁciently preprocessed using samples so that no large outliers exist.

8

Lemma 2.8 (Rephrasing of Theorem 3 of [DV04a]). Using m = ˜O(d2b) samples from Dx  one
can identify with high probability an ellipsoid E such that Prx∼Dx [x ∈ E] ≥ 1
2 and Dx|E has no
Γ−1 = ˜O(db)-outliers.

Given this lemma  we can adapt Algorithm 1 for the large margin case to work in general.
The pseudo-code is given in Algorithm 2. It similarly returns a decision list [(w(1)  T (1)  E(1)) 
(w(2)  T (2)  E(2))  ··· ] as output.

Algorithm 2 Main Algorithm (general case)
1: Set S(1) = Rd  λ = η +   Γ−1 = ˜O(db)  m = ˜O( 1
2: Set i ← 1.

3: Draw O(cid:0)(1/2) log(1/(Γ))(cid:1) samples from Dx to form an empirical distribution ˜Dx.

Γ24 ).

(cid:2)x ∈ S(i)(cid:3) ≥  do

4: while Prx∼ ˜Dx
5:

6:

7:
8:

9:
10:

S(i)

Run the algorithm of Lemma 2.8 to remove Γ−1-outliers from the distribution DS(i) by
ﬁltering points outside the ellipsoid E(i).
[xxT ] and set D(i) = ΓΣ(i)−1/2 · D|S(i)∩E(i) be the distribution
D|S(i)∩E(i) brought in isotropic position and rescaled by Γ so that all vectors have (cid:96)2-norm at
most 1.

Let Σ(i) = E(x y)∼D(i)|

L(i)(w(i)) ≤ minw:(cid:107)w(cid:107)2≤1 L(i)(w) + Γ/2.

Let L(i)(w) = E(x y)∼D(i)[LeakyReluλ(−y(cid:104)w  x(cid:105))]
Run SGD on L(i)(w) for ˜O(1/(Γ22)) iterations  to get w(i) with (cid:107)w(i)(cid:107)2 = 1 such that
Draw m samples from D(i) to form an empirical distribution D(i)
m .
Find a threshold T (i) such that Pr(x y)∼D(i)

[|(cid:104)w(i)  x(cid:105)| ≥ T (i)] ≥ Γ and the empirical

[hw(x) (cid:54)= y(cid:12)(cid:12)|(cid:104)w(i)  x(cid:105)| ≥ T (i)]  is minimized.

misclassiﬁcation error  Pr(x y)∼D(i)

m

Revert the linear transformation by setting w(i) ← ΓΣ(i)−1/2 · w(i).
Update the unclassiﬁed region S(i+1) ← S(i) \ {x : x ∈ E(i) ∧ |(cid:104)w(i)  x(cid:105)| ≥ T (i)} and set

11:
12:
13: Return the classiﬁer [(w(1)  T (1)  E(1))  (w(2)  T (2)  E(2)) ··· ]

i ← i + 1.

m

Our main result is the following theorem:
Theorem 2.9. Let D be a distribution over (d + 1)-dimensional labeled examples with bit-complexity
b  generated by an unknown halfspace corrupted by Massart noise at rate η < 1/2. Algorithm 2 uses
˜O(d3b3/5) samples  runs in poly(d  1/  b) time  and returns  with probability 2/3  a classiﬁer h
with misclassiﬁcation error errD

0−1(h) ≤ η + .

3 Conclusions

The main contribution of this paper is the ﬁrst non-trivial learning algorithm for the class of halfspaces
(or even disjunctions) in the distribution-free PAC model with Massart noise. Our algorithm achieves
misclassiﬁcation error η +  in time poly(d  1/)  where η < 1/2 is an upper bound on the Massart
noise rate. The most obvious open problem is whether this error guarantee can be improved to
f (OPT) +  (for some function f : R → R such that limx→0 f (x) = 0) or  ideally  to OPT + . It
follows from our lower bound constructions that such an improvement would require new algorithmic
ideas. It is a plausible conjecture that obtaining better error guarantees is computationally intractable.
This is left as an interesting open problem for future work. Another open question is whether there is
an efﬁcient proper learner matching the error guarantees of our algorithm. We believe that this is
possible  building on the ideas in [DV04b]  but we did not pursue this direction. More broadly  what
other concept classes admit non-trivial algorithms in the Massart noise model? Can one establish
non-trivial reductions between the Massart noise model and the agnostic model? And are there
other natural semi-random input models that allow for efﬁcient PAC learning algorithms in the
distribution-free setting?

9

Acknowledgments

Part of this work was performed while Ilias Diakonikolas was at the Simons Institute for the Theory
of Computing during the program on Foundations of Data Science. Ilias Diakonikolas is supported by
Supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. This research
was performed while Themis Gouleakis was a postdoctoral researcher at USC.

References
[ABHU15] P. Awasthi  M. F. Balcan  N. Haghtalab  and R. Urner. Efﬁcient learning of linear
separators under bounded noise. In Proceedings of The 28th Conference on Learning
Theory  COLT 2015  pages 167–190  2015.

[ABHZ16] P. Awasthi  M. F. Balcan  N. Haghtalab  and H. Zhang. Learning and 1-bit compressed
sensing under asymmetric noise. In Proceedings of the 29th Conference on Learning
Theory  COLT 2016  pages 152–192  2016.

[ABL17] P. Awasthi  M. F. Balcan  and P. M. Long. The power of localization for efﬁciently

learning linear separators with noise. J. ACM  63(6):50:1–50:27  2017.

[AL88] D. Angluin and P. Laird. Learning from noisy examples. Mach. Learn.  2(4):343–370 

1988.

[Ber06] T. Bernholt. Robust estimators are hard to compute. Technical report  University of

Dortmund  Germany  2006.

[BFKV96] A. Blum  A. M. Frieze  R. Kannan  and S. Vempala. A polynomial-time algorithm for
learning noisy linear threshold functions. In 37th Annual Symposium on Foundations of
Computer Science  FOCS ’96  pages 330–338  1996.

[BFKV97] A. Blum  A. Frieze  R. Kannan  and S. Vempala. A polynomial time algorithm for

learning noisy linear threshold functions. Algorithmica  22(1/2):35–52  1997.

[Blu03] A. Blum. Machine learning: My favorite results  directions  and open problems. In 44th

Symposium on Foundations of Computer Science (FOCS 2003)  pages 11–14  2003.

[Byl94] T. Bylander. Learning linear threshold functions in the presence of classiﬁcation noise.
In Proceedings of the Seventh Annual ACM Conference on Computational Learning
Theory  COLT 1994  pages 340–347  1994.

[Coh97] E. Cohen. Learning noisy perceptrons by a perceptron in polynomial time. In Proceedings
of the Thirty-Eighth Symposium on Foundations of Computer Science  pages 514–521 
1997.

[Dan16] A. Daniely. Complexity theoretic limitations on learning halfspaces. In Proceedings
of the 48th Annual Symposium on Theory of Computing  STOC 2016  pages 105–117 
2016.

[DKK+16] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proceedings
of FOCS’16  pages 655–664  2016.

[DKK+17] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Being
robust (in high dimensions) can be practical. In Proceedings of the 34th International
Conference on Machine Learning  ICML 2017  pages 999–1008  2017.

[DKK+18] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Robustly
learning a gaussian: Getting optimal error  efﬁciently. In Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms  SODA 2018  pages 2683–2702 
2018.

[DKK+19] I. Diakonikolas  G. Kamath  D. Kane  J. Li  J. Steinhardt  and Alistair Stewart. Sever: A
robust meta-algorithm for stochastic optimization. In Proceedings of the 36th Interna-
tional Conference on Machine Learning  ICML 2019  pages 1596–1606  2019.

10

[DKS18] I. Diakonikolas  D. M. Kane  and A. Stewart. Learning geometric concepts with nasty
In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of

noise.
Computing  STOC 2018  pages 1061–1073  2018.

[DKS19] I. Diakonikolas  W. Kong  and A. Stewart. Efﬁcient algorithms and lower bounds for
robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms  SODA 2019  pages 2745–2754  2019.

[DKW56] A. Dvoretzky  J. Kiefer  and J. Wolfowitz. Asymptotic minimax character of the sample
distribution function and of the classical multinomial estimator. Ann. Mathematical
Statistics  27(3):642–669  1956.

[Duc16] J. C. Duchi. Introductory lectures on stochastic convex optimization. Park City Mathe-

matics Institute  Graduate Summer School Lectures  2016.

[DV04a] J. Dunagan and S. Vempala. Optimal outlier removal in high-dimensional spaces. J.

Computer & System Sciences  68(2):335–373  2004.

[DV04b] J. Dunagan and S. Vempala. A simple polynomial-time rescaling algorithm for solving
linear programs. In Proceedings of the 36th Annual ACM Symposium on Theory of
Computing  pages 315–320  2004.

[FGKP06] V. Feldman  P. Gopalan  S. Khot  and A. Ponnuswami. New results for learning noisy

parities and halfspaces. In Proc. FOCS  pages 563–576  2006.

[GR06] V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proc.
47th IEEE Symposium on Foundations of Computer Science (FOCS)  pages 543–552.
IEEE Computer Society  2006.

[Hau92] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and

other learning applications. Information and Computation  100:78–150  1992.

[Kea93] M. J. Kearns. Efﬁcient noise-tolerant learning from statistical queries. In Proceedings
of the Twenty-Fifth Annual ACM Symposium on Theory of Computing  pages 392–401 
1993.

[Kea98] M. J. Kearns. Efﬁcient noise-tolerant learning from statistical queries. Journal of the

ACM  45(6):983–1006  1998.

[KKM18] A. R. Klivans  P. K. Kothari  and R. Meka. Efﬁcient algorithms for outlier-robust
regression. In Conference On Learning Theory  COLT 2018  pages 1420–1430  2018.

[KLS09] A. Klivans  P. Long  and R. Servedio. Learning halfspaces with malicious noise. To
appear in Proc. 17th Internat. Colloq. on Algorithms  Languages and Programming
(ICALP)  2009.

[KSS94] M. Kearns  R. Schapire  and L. Sellie. Toward Efﬁcient Agnostic Learning. Machine

Learning  17(2/3):115–141  1994.

[LRV16] K. A. Lai  A. B. Rao  and S. Vempala. Agnostic estimation of mean and covariance. In

Proceedings of FOCS’16  2016.

[LS10] P. M. Long and R. A. Servedio. Random classiﬁcation noise defeats all convex potential

boosters. Machine Learning  78(3):287–304  2010.

[MN06] P. Massart and E. Nedelec. Risk bounds for statistical learning. Ann. Statist.  34(5):2326–

2366  10 2006.

[MT94] W. Maass and G. Turan. How fast can a threshold gate learn? In S. Hanson  G. Drastal 
and R. Rivest  editors  Computational Learning Theory and Natural Learning Systems 
pages 381–414. MIT Press  1994.

[Ros58] F. Rosenblatt. The Perceptron: a probabilistic model for information storage and

organization in the brain. Psychological Review  65:386–407  1958.

11

[RS94] R. Rivest and R. Sloan. A formal model of hierarchical concept learning. Information

and Computation  114(1):88–114  1994.

[Slo88] R. H. Sloan. Types of noise in data for concept learning. In Proceedings of the First
Annual Workshop on Computational Learning Theory  COLT ’88  pages 91–96  San
Francisco  CA  USA  1988. Morgan Kaufmann Publishers Inc.

[Slo92] R. H. Sloan. Corrigendum to types of noise in data for concept learning. In Proceedings
of the Fifth Annual ACM Conference on Computational Learning Theory  COLT 1992 
page 450  1992.

[Slo96] R. H. Sloan. Pac Learning  Noise  and Geometry  pages 21–41. Birkhäuser Boston 

Boston  MA  1996.

[Val84] L. G. Valiant. A theory of the learnable. In Proc. 16th Annual ACM Symposium on

Theory of Computing (STOC)  pages 436–445. ACM Press  1984.

[Vap82] V. Vapnik. Estimation of Dependences Based on Empirical Data: Springer Series in

Statistics. Springer-Verlag  Berlin  Heidelberg  1982.

[YZ17] S. Yan and C. Zhang. Revisiting perceptron: Efﬁcient and label-optimal learning
In Advances in Neural Information Processing Systems 30: Annual
of halfspaces.
Conference on Neural Information Processing Systems 2017  pages 1056–1066  2017.

[ZLC17] Y. Zhang  P. Liang  and M. Charikar. A hitting time analysis of stochastic gradient
langevin dynamics. In Proceedings of the 30th Conference on Learning Theory  COLT
2017  pages 1980–2022  2017.

12

,Ilias Diakonikolas
Themis Gouleakis
Christos Tzamos