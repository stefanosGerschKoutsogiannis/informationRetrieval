2014,On the Convergence Rate of Decomposable Submodular Function Minimization,Submodular functions describe a variety of discrete problems in machine learning  signal processing  and computer vision. However  minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use  parallelizable algorithm for minimizing submodular functions that decompose as the sum of simple" submodular functions. Empirically  this algorithm performs extremely well  but no theoretical analysis was given. In this paper  we show that the algorithm converges linearly  and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.",On the Convergence Rate of Decomposable

Submodular Function Minimization

Robert Nishihara  Stefanie Jegelka  Michael I. Jordan

Electrical Engineering and Computer Science

University of California

Berkeley  CA 94720

{rkn stefje jordan}@eecs.berkeley.edu

Abstract

Submodular functions describe a variety of discrete problems in machine learn-
ing  signal processing  and computer vision. However  minimizing submodular
functions poses a number of algorithmic challenges. Recent work introduced an
easy-to-use  parallelizable algorithm for minimizing submodular functions that
decompose as the sum of “simple” submodular functions. Empirically  this al-
gorithm performs extremely well  but no theoretical analysis was given. In this
paper  we show that the algorithm converges linearly  and we provide upper and
lower bounds on the rate of convergence. Our proof relies on the geometry of
submodular polyhedra and draws on results from spectral graph theory.

1

Introduction

A large body of recent work demonstrates that many discrete problems in machine learning can be
phrased as the optimization of a submodular set function [2]. A set function F : 2V ! R over a
ground set V of N elements is submodular if the inequality F (A) + F (B)  F (A[ B) + F (A\ B)
holds for all subsets A  B ✓ V . Problems like clustering [33]  structured sparse variable selection
[1]  MAP inference with higher-order potentials [28]  and corpus extraction problems [31] can be
reduced to the problem of submodular function minimization (SFM)  that is

F (A).

(P1)

min
A✓V

Although SFM is solvable in polynomial time  existing algorithms can be inefﬁcient on large-scale
problems. For this reason  the development of scalable  parallelizable algorithms has been an active
area of research [24  25  29  35]. Approaches to solving Problem (P1) are either based on combina-
torial optimization or on convex optimization via the Lov´asz extension.
Functions that occur in practice are usually not arbitrary and frequently possess additional ex-
ploitable structure. For example  a number of submodular functions admit specialized algorithms
that solve Problem (P1) very quickly. Examples include cut functions on certain kinds of graphs 
concave functions of the cardinality |A|  and functions counting joint ancestors in trees. We will use
the term simple to refer to functions F for which we have a fast subroutine for minimizing F + s 
where s 2 RN is any modular function. We treat these subroutines as black boxes. Many com-
monly occuring submodular functions (for example  graph cuts  hypergraph cuts  MAP inference
with higher-order potentials [16  28  37]  co-segmentation [22]  certain structured-sparsity inducing
functions [26]  covering functions [35]  and combinations thereof) can be expressed as a sum

(1)
of simple submodular functions. Recent work demonstrates that this structure offers important prac-
tical beneﬁts [25  29  35]. For instance  it admits iterative algorithms that minimize each Fr sepa-
rately and combine the results in a straightforward manner (for example  dual decomposition).

Fr(A)

r=1

F (A) =XR

1

In particular  it has been shown that the minimization of decomposable functions can be rephrased
as a best-approximation problem  the problem of ﬁnding the closest points in two convex sets [25].
This formulation brings together SFM and classical projection methods and yields empirically fast 
parallel  and easy-to-implement algorithms. In these cases  the performance of projection methods
depends heavily on the speciﬁc geometry of the problem at hand and is not well understood in
general. Indeed  while Jegelka et al. [25] show good empirical results  the analysis of this alternative
approach to SFM was left as an open problem.
Contributions. In this work  we study the geometry of the submodular best-approximation problem
and ground the prior empirical results in theoretical guarantees. We show that SFM via alternating
projections  or block coordinate descent  converges at a linear rate. We show that this rate holds
for the best-approximation problem  relaxations of SFM  and the original discrete problem. More
importantly  we prove upper and lower bounds on the worst-case rate of convergence. Our proof
relies on analyzing angles between the polyhedra associated with submodular functions and draws
on results from spectral graph theory. It offers insight into the geometry of submodular polyhedra
that may be beneﬁcial beyond the analysis of projection algorithms.
Submodular minimization. The ﬁrst polynomial-time algorithm for minimizing arbitrary submod-
ular functions was a consequence of the ellipsoid method [19]. Strongly and weakly polynomial-
time combinatorial algorithms followed [32]. The current fastest running times are O(N 5⌧1 + N 6)
[34] in general and O((N 4⌧1 + N 5) log Fmax) for integer-valued functions [23]  where Fmax =
maxA |F (A)| and ⌧1 is the time required to evaluate F . Some work has addressed decomposable
functions [25  29  35]. The running times in [29] apply to integer-valued functions and range from
O((N + R)2 log Fmax) for cuts to O((N + Q2R)(N + Q2R + QR⌧2) log Fmax)  where Q  N is
the maximal cardinality of the support of any Fr  and ⌧2 is the time required to minimize a simple
function. Stobbe and Krause [35] use a convex optimization approach based on Nesterov’s smooth-
ing technique. They achieve a (sublinear) convergence rate of O(1/k) for the discrete SFM problem.
Their results and our results do not rely on the function being integral.
Projection methods. Algorithms based on alternating projections between convex sets (and related
methods such as the Douglas–Rachford algorithm) have been studied extensively for solving convex
feasibility and best-approximation problems [4  5  7  11  12  20  21  36  38]. See Deutsch [10] for a
survey of applications. In the simple case of subspaces  the convergence of alternating projections
has been characterized in terms of the Friedrichs angle cF between the subspaces [5  6]. There are
often good ways to compute cF (see Lemma 6)  which allow us to obtain concrete linear rates of
convergence for subspaces. The general case of alternating projections between arbitrary convex
sets is less well understood. Bauschke and Borwein [3] give a general condition for the linear
convergence of alternating projections in terms of the value ⇤ (deﬁned in Section 3.1). However 
except in very limited cases  it is unclear how to compute or even bound ⇤. While it is known that
⇤ < 1 for polyhedra [5  Corollary 5.26]  the rate may be arbitrarily slow  and the challenge is
to bound the linear rate away from one. We are able to give a speciﬁc uniform linear rate for the
submodular polyhedra that arise in SFM.
Although both ⇤ and cF are useful quantities for understanding the convergence of projection
methods  they largely have been studied independently of one another.
In this work  we relate
these two quantities for polyhedra  thereby obtaining some of the generality of ⇤ along with the
computability of cF . To our knowledge  we are the ﬁrst to relate ⇤ and cF outside the case of
subspaces. We feel that this connection may be useful beyond the context of submodular polyhedra.

1.1 Background

Throughout this paper  we assume that F is a sum of simple submodular functions F1  . . .   FR and

that F (;) = 0. Points s 2 RN can be identiﬁed with (modular) set functions via s(A) =Pn2A sn.

The base polytope of F is deﬁned as the set of all modular functions that are dominated by F and
that sum to F (V ) 

B(F ) = {s 2 RN | s(A)  F (A) for all A ✓ V and s(V ) = F (V )}.

The Lov´asz extension f : RN ! R of F can be written as the support function of the base polytope 
that is f (x) = maxs2B(F ) s>x. Even though B(F ) may have exponentially many faces  the exten-
sion f can be evaluated in O(N log N ) time [15]. The discrete SFM problem (P1) can be relaxed to

2

the non-smooth convex optimization problem

min
x2[0 1]N

f (x) ⌘ min
x2[0 1]N

RXr=1

fr(x) 

(P2)

where fr is the Lov´asz extension of Fr. This relaxation is exact – rounding an optimal continuous
solution yields the indicator vector of an optimal discrete solution. The formulation in Problem (P2)
is amenable to dual decomposition [30] and smoothing techniques [35]  but suffers from the non-
smoothness of f [25]. Alternatively  we can formulate a proximal version of the problem

RXr=1

min
x2RN

f (x) + 1

2kxk2 ⌘ min
x2RN

(fr(x) + 1

2Rkxk2).

(P3)

By thresholding the optimal solution of Problem (P3) at zero  we recover the indicator vector of an
optimal discrete solution [17]  [2  Proposition 8.4].
Lemma 1. [25] The dual of the right-hand side of Problem (P3) is the best-approximation problem
(P4)

min ka  bk2 a 2A   b 2B  

r=1 ar = 0} and B = B(F1) ⇥···⇥ B(FR).

where A = {(a1  . . .   aR) 2 RN R |PR
Lemma 1 implies that we can minimize a decomposable submodular function by solving Prob-
lem (P4)  which means ﬁnding the closest points between the subspace A and the product B of base
polytopes. Projecting onto A is straightforward because A is a subspace. Projecting onto B amounts
to projecting onto each B(Fr) separately. The projection ⇧B(Fr)z of a point z onto B(Fr) may be
solved by minimizing Fr  z [25]. We can compute these projections easily because each Fr is
simple.
Throughout this paper  we use A and B to refer to the speciﬁc polyhedra deﬁned in Lemma 1
(which live in RN R) and P and Q to refer to general polyhedra (sometimes arbitrary convex sets) in
RD. Note that the polyhedron B depends on the submodular functions F1  . . .   FR  but we omit the
dependence to simplify our notation. Our bound will be uniform over all submodular functions.

2 Algorithm and Idea of Analysis

A popular class of algorithms for solving best-approximation problems are projection methods [5].
The most straightforward approach uses alternating projections (AP) or block coordinate descent.
Start with any point a0 2A   and inductively generate two sequences via bk =⇧ Bak and ak+1 =
⇧Abk. Given the nature of A and B  this algorithm is easy to implement and use in our setting  and
it solves Problem (P4) [25]. This is the algorithm that we will analyze.
The sequence (ak  bk) will eventually converge to an optimal pair (a⇤  b⇤). We say that AP converges
linearly with rate ↵< 1 if kaka⇤k  C1↵k and kbkb⇤k  C2↵k for all k and for some constants
C1 and C2. Smaller values of ↵ are better.
Analysis: Intuition. We will provide a detailed analysis of the convergence of AP for the polyhedra
A and B. To motivate our approach  we ﬁrst provide some intuition with the following much-
simpliﬁed setup. Let U and V be one-dimensional subspaces spanned by the unit vectors u and v
respectively. In this case  it is known that AP converges linearly with rate cos2 ✓  where ✓ 2 [0  ⇡
2 ]
is the angle such that cos ✓ = u>v. The smaller the angle  the slower the rate of convergence.
For subspaces U and V of higher dimension  the relevant generalization of the “angle” between the
subspaces is the Friedrichs angle [11  Deﬁnition 9.4]  whose cosine is given by

cF (U  V ) = supu>v | u 2 U \ (U \ V )?  v 2 V \ (U \ V )? kuk  1 kvk  1 .

(2)
In ﬁnite dimensions  cF (U  V ) < 1. In general  when U and V are subspaces of arbitrary dimension 
AP will converge linearly with rate cF (U  V )2 [11  Theorem 9.8]. If U and V are afﬁne spaces  AP
still converges linearly with rate cF (U  u  V  v)2  where u 2 U and v 2 V .
We are interested in rates for polyhedra P and Q  which we deﬁne as the intersection of ﬁnitely
many halfspaces. We generalize the preceding results by considering all pairs (Px  Qy) of

3

P

Q

E
v

H

P

Q0

E

Figure 1: The optimal sets E  H in Equation (4)  the vector v  and the shifted polyhedron Q0.

p2P

faces of P and Q and showing that the convergence rate of AP between P and Q is at worst
maxx y cF (a↵0(Px)  a↵0(Qy))2  where a↵(C) is the afﬁne hull of C and a↵0(C) = a↵(C)  c
for some c 2 C. The faces {Px}x2RD of P are deﬁned as the nonempty maximizers of linear
functions over P   that is
(3)

Px = arg max

x>p.

While we look at angles between pairs of faces  we remark that Deutsch and Hundal [13] consider a
different generalization of the “angle” between arbitrary convex sets.
Roadmap of the Analysis. Our analysis has two main parts. First  we relate the convergence rate
of AP between polyhedra P and Q to the angles between the faces of P and Q. To do so  we give a
general condition under which AP converges linearly (Theorem 2)  which we show depends on the
angles between the faces of P and Q (Corollary 5) in the polyhedral case. Second  we specialize
to the polyhedra A and B  and we equate the angles with eigenvalues of certain matrices and use
tools from spectral graph theory to bound the relevant eigenvalues in terms of the conductance of a
speciﬁc graph. This yields a worst-case bound of 1  1
In Theorem 14  we show a lower bound of 1  2⇡2
3 The Upper Bound
We ﬁrst derive an upper bound on the rate of convergence of AP between the polyhedra A and B.
The results in this section are proved in Appendix A.

N 2R on the worst-case convergence rate.

N 2R2 on the rate  stated in Theorem 12.

3.1 A Condition for Linear Convergence

We begin with a condition under which AP between two closed convex sets P and Q converges
linearly. This result is similar to that of Bauschke and Borwein [3  Corollary 3.14]  but the rate we
achieve is twice as fast and relies on slightly weaker assumptions.
We will need a few deﬁnitions from Bauschke and Borwein [3]. Let d(K1  K2) = inf{kk1  k2k :
k1 2 K1  k2 2 K2} be the distance between sets K1 and K2. Deﬁne the sets of “closest points” as
(4)
and let v =⇧ QP 0 (see Figure 1). Note that H = E + v  and when P \ Q 6= ; we have v = 0
and E = H = P \ Q. Therefore  we can think of the pair (E  H) as a generalization of the
intersection P \ Q to the setting where P and Q do not intersect. Pairs of points (e  e + v) 2 E ⇥ H
are solutions to the best-approximation problem between P and Q. In our analysis  we will mostly
study the translated version Q0 = Q  v of Q that intersects P at E.
For x 2 RD\E  the function  relates the distance to E with the distances to P and Q0 

H = {q 2 Q| d(q  P ) = d(Q  P )} 

E = {p 2 P | d(p  Q) = d(P  Q)}

If  is bounded  then whenever x is close to both P and Q0  it must also be close to their intersection.
If  for example  D  2 and P and Q are balls of radius one whose centers are separated by distance

(x) =

d(x  E)

max{d(x  P )  d(x  Q0)}

.

4

exactly two  then  is unbounded. The maximum ⇤ = supx2(P[Q0)\E (x) is useful for bounding
the convergence rate.
Theorem 2. Let P and Q be convex sets  and suppose that ⇤ < 1. Then AP between P and Q
converges linearly with rate 1  1

. Speciﬁcally 

2
⇤

kpk  p⇤k  2kp0  p⇤k(1  1

2
⇤

)k

and

kqk  q⇤k  2kq0  q⇤k(1  1

2
⇤

)k.

3.2 Relating ⇤ to the Angles Between Faces of the Polyhedra
In this section  we consider the case of polyhedra P and Q  and we bound ⇤ in terms of the angles
between pairs of their faces. In Lemma 3  we show that  is nondecreasing along the sequence of
points generated by AP between P and Q0. We treat points p for which (p) = 1 separately because
those are the points from which AP between P and Q0 converges in one step. This lemma enables us
to bound (p) by initializing AP at p and bounding  at some later point in the resulting sequence.
Lemma 3. For any p 2 P\E  either (p) = 1 or 1 < (p)  (⇧Q0p). Similarly  for any
q 2 Q0\E  either (q) = 1 or 1 < (q)  (⇧P q).
We can now bound  by angles between faces of P and Q.
Proposition 4. If P and Q are polyhedra and p 2 P\E  then there exist faces Px and Qy such that

1
(p)2  cF (a↵0(Px)  a↵0(Qy))2.

1 

The analogous statement holds when we replace p 2 P\E with q 2 Q0\E.
Note that a↵0(Qy) = a↵0(Q0y). Proposition 4 immediately gives us the following corollary.
Corollary 5. If P and Q are polyhedra  then

1 

1
2
⇤

 max
x y2RD

cF (a↵0(Px)  a↵0(Qy))2.

3.3 Angles Between Subspaces and Singular Values

Corollary 5 leaves us with the task of bounding the Friedrichs angle. To do so  we ﬁrst relate the
Friedrichs angle to the singular values of certain matrices in Lemma 6. We then specialize this to
base polyhedra of submodular functions. For convenience  we prove Lemma 6 in Appendix A.5 
though this result is implicit in the characterization of principal angles between subspaces given
in [27  Section 1]. Ideas connecting angles between subspaces and eigenvalues are also used by
Diaconis et al. [14].
Lemma 6. Let S and T be matrices with orthonormal rows and with equal numbers of columns.
If all of the singular values of ST > equal one  then cF (null(S)  null(T )) = 0. Otherwise 
cF (null(S)  null(T )) is equal to the largest singular value of ST > that is less than one.
Faces of relevant polyhedra. Let Ax and By be faces of the polyhedra A and B from Lemma 1.
Since A is a vector space  its only nonempty face is Ax = A. Hence  Ax = null(S)  where S is an
N ⇥ N R matrix of N ⇥ N identity matrices IN:
|

pR✓ IN ···
repeated R times ◆.
{z
}

The matrix for a↵0(By) requires a bit more elaboration. Since B is a Cartesian product  we have
By = B(F1)y1 ⇥···⇥ B(FR)yR  where y = (y1  . . .   yR) and B(Fr)yr is a face of B(Fr). To
proceed  we use the following characterization of faces of base polytopes [2  Proposition 4.7].
Proposition 7. Let F be a submodular function  and let B(F )x be a face of B(F ). Then there exists
a partition of V into disjoint sets A1  . . .   AM such that

S =

(5)

1

IN

a↵(B(F )x) =

M\m=1

{s 2 RN | s(A1 [···[ Am) = F (A1 [···[ Am)}.

5

The following corollary is immediate.
Corollary 8. Deﬁne F   B(F )x  and A1  . . .   AM as in Proposition 7. Then

a↵0(B(F )x) =

M\m=1

{s 2 RN | s(A1 [···[ Am) = 0}.

By Corollary 8  for each Fr  there exists a partition of V into disjoint sets Ar1  . . .   ArMr such that

a↵0(By) =

{(s1  . . .   sR) 2 RN R | sr(Ar1 [···[ Arm) = 0}.

(6)

R\r=1

Mr\m=1

In other words  we can write a↵0(By) as the nullspace of either of the matrices

1>A11...

1>A11[···[A1M1

...

T 0 =

0BBBBBBBBBBBB@

or T =

1CCCCCCCCCCCCA

1>AR1...

1>AR1[···[ARMR

0BBBBBBBBBBBBBBBB@

1>A11p|A11|
...
1>A1M1p|A1M1|

...

1>AR1p|AR1|
...

1>ARMR
p|ARMR|

 

1CCCCCCCCCCCCCCCCA

1

pR✓ 1A11p|A11|

where 1A is the indicator vector of A ✓ V . For T 0  this follows directly from Equation (6). T
can be obtained from T 0 via left multiplication by an invertible matrix  so T and T 0 have the same
nullspace. Lemma 6 then implies that cF (a↵0(Ax)  a↵0(By)) equals the largest singular value of

1A1M1p|A1M1|

ST > =

···

···

···
that is less than one. We rephrase this conclusion in the following remark.
Remark 9. The largest eigenvalue of (ST >)>(ST >) less than one equals cF (a↵0(Ax)  a↵0(By))2.
Let Mall = M1 +··· + MR. Then (ST >)>(ST >) is the Mall ⇥ Mall square matrix whose rows and
columns are indexed by (r  m) with 1  r  R and 1  m  Mr and whose entry corresponding
to row (r1  m1) and column (r2  m2) equals

1AR1p|AR1|

1ARMR

p|ARMR| ◆

1
R

1>Ar1m1

1Ar2m2

p|Ar1m1||Ar2m2|

3.4 Bounding the Relevant Eigenvalues

=

1
R

|Ar1m1 \ Ar2m2|

p|Ar1m1||Ar2m2|

.

It remains to bound the largest eigenvalue of (ST >)>(ST >) that is less than one. To do so  we view
the matrix in terms of the symmetric normalized Laplacian of a weighted graph. Let G be the graph
whose vertices are indexed by (r  m) with 1  r  R and 1  m  Mr. Let the edge between
vertices (r1  m1) and (r2  m2) have weight |Ar1m1 \ Ar2m2|. We may assume that G is connected
(the analysis in this case subsumes the analysis in the general case). The symmetric normalized
Laplacian L of this graph is closely related to our matrix of interest 

(ST >)>(ST >) = I  R1

R L.

(7)

Hence  the largest eigenvalue of (ST >)>(ST >) that is less than one can be determined from the
smallest nonzero eigenvalue 2(L) of L. We bound 2(L) via Cheeger’s inequality (stated in Ap-
pendix A.6) by bounding the Cheeger constant hG of G.
Lemma 10. For R  2  we have hG  2

N R and hence 2(L)  2

N 2R2 .

6

We prove Lemma 10 in Appendix A.7. Combining Remark 9  Equation (7)  and Lemma 10  we
obtain the following bound on the Friedrichs angle.
Proposition 11. Assuming that R  2  we have

cF (a↵0(Ax)  a↵0(By))2  1  R1

R

2

N 2R2  1  1

N 2R2 .

Together with Theorem 2 and Corollary 5  Proposition 11 implies the ﬁnal bound on the rate.
Theorem 12. The AP algorithm for Problem (P4) converges linearly with rate 1  1
N 2R2   i.e. 
N 2R2 )k.
kbk  b⇤k  2kb0  b⇤k(1  1

kak  a⇤k  2ka0  a⇤k(1  1

N 2R2 )k

and

4 A Lower Bound

To probe the tightness of Theorem 12  we construct a “bad” submodular function and decomposition
that lead to a slow rate. Appendix B gives the formal details. Our example is an augmented cut
function on a cycle: for each x  y 2 V   deﬁne Gxy to be the cut function of a single edge (x  y) 

Gxy =⇢1

0

if |A \{ x  y}| = 1
otherwise .

Take N to be even and R  2 and deﬁne the submodular function F lb = F lb

F lb
1 = G12 + G34 + ··· + G(N1)N

1 + ··· + F lb
F lb
2 = G23 + G45 + ··· + GN 1

R   where

r = 0 for all r  3. The optimal solution to the best-approximation problem is the all zeros

and F lb
vector.
Lemma 13. The cosine of the Friedrichs angle between A and a↵(Blb) is
R1  cos 2⇡
N .
⇤ = cF (A  a↵(Blb))2  and

Around the optimal solution 0  the polyhedra A and Blb behave like subspaces  and it is possible to
pick initializations a0 2A and b0 2B lb such that the Friedrichs angle exactly determines the rate
of convergence. That means 1  1/2

cF (A  a↵(Blb))2 = 1  1

kakk = (1  1

R (1  cos( 2⇡

N )))kka0k

and

kbkk = (1  1

R (1  cos( 2⇡

N )))kkb0k.

2 x2 leads to the following lower bound on the rate.

Bounding 1  cos(x)  1
Theorem 14. There exists a decomposed function F lb and initializations for which the convergence
rate of AP is at least 1  2⇡2
N 2R.
This theoretical bound can also be observed empirically (Figure 3 in Appendix B).

5 Convergence of the Primal Objective
We have shown that AP generates a sequence of points {ak}k0 and {bk}k0 in RN R such that
(ak  bk) ! (a⇤  b⇤) linearly  where (a⇤  b⇤) minimizes the objective in Problem (P4). In this section 
we show that this result also implies the linear convergence of the objective in Problem (P3) and of
the original discrete objective in Problem (P1). The proofs may be found in Appendix C.
Deﬁne the matrix = R1/2S  where S is the matrix deﬁned in Equation (5). Multiplication by 
maps a vector (w1  . . .   wR) to Pr wr  where wr 2 RN for each r. Set xk = bk and x⇤ = b⇤.
As shown in Jegelka et al. [25]  Problem (P3) is minimized by x⇤.
Proposition 15. We have f (xk) + 1

N 2R2 .
2kx⇤k2 linearly with rate 1  1
This linear rate of convergence translates into a linear rate for the original discrete problem.
Theorem 16. Choose A⇤ 2 arg minA✓V F (A). Let Ak be the suplevel set of xk with smallest
value of F . Then F (Ak) ! F (A⇤) linearly with rate 1  1

2kxkk2 ! f (x⇤) + 1

2N 2R2 .

7

6 Discussion

In this work  we analyze projection methods for parallel SFM and give upper and lower bounds on
the linear rate of convergence. This means that the number of iterations required for an accuracy of
✏ is logarithmic in 1/✏  not linear as in previous work [35]. Our rate is uniform over all submodular
functions. Moreover  our proof highlights how the number R of components and the facial structure
of B affect the convergence rate. These insights may serve as guidelines when working with projec-
tion algorithms and aid in the analysis of special cases. For example  reducing R is often possible.
Any collection of Fr that have disjoint support  such as the cut functions corresponding to the rows
or columns of a grid graph  can be grouped together without making the projection harder.
Our analysis also shows the effects of additional properties of F . For example  suppose that F
is separable  that is  F (V ) = F (S) + F (V \S) for some nonempty S ( V . Then the subsets
Arm ✓ V deﬁning the relevant faces of B satisfy either Arm ✓ S or Arm ✓ Sc [2]. This makes G
in Section 3.4 disconnected  and as a result  the N in Theorem 12 gets replaced by max{|S| |Sc|}
for an improved rate. This applies without the user needing to know S when running the algorithm.
A number of future directions suggest themselves. For example  Jegelka et al. [25] also considered
the related Douglas–Rachford (DR) algorithm. DR between subspaces converges linearly with rate
cF [6]  as opposed to c2
F for AP. We suspect that our approach may be modiﬁed to analyze DR
between polyhedra. Further questions include the extension to cyclic updates (instead of parallel
ones)  multiple polyhedra  and stochastic algorithms.

Acknowledgments. We would like to thank M˘ad˘alina Persu for suggesting the use of Cheeger’s
inequality. This research is supported in part by NSF CISE Expeditions Award CCF-1139158 
LBNL Award 7076018  and DARPA XData Award FA8750-12-2-0331  and gifts from Amazon
Web Services  Google  SAP  The Thomas and Stacey Siebel Foundation  Apple  C3Energy  Cisco 
Cloudera  EMC  Ericsson  Facebook  GameOnTalis  Guavus  HP  Huawei  Intel  Microsoft  NetApp 
Pivotal  Splunk  Virdata  VMware  WANdisco  and Yahoo!. This work is supported in part by the
Ofﬁce of Naval Research under grant number N00014-11-1-0688  the US ARL and the US ARO
under grant number W911NF-11-1-0391  and the NSF under grant number DGE-1106400.

References
[1] F. Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural Infor-

mation Processing Systems  2011.

[2] F. Bach. Learning with submodular functions: A convex optimization perspective. Foundations and

Trends in Machine Learning  6(2-3):145–373  2013.

[3] H. H. Bauschke and J. M. Borwein. On the convergence of von Neumann’s alternating projection algo-

rithm for two sets. Set-Valued Analysis  1(2):185–212  1993.

[4] H. H. Bauschke and J. M. Borwein. Dykstra’s alternating projection algorithm for two sets. Journal of

Approximation Theory  79(3):418–443  1994.

[5] H. H. Bauschke and J. M. Borwein. On projection algorithms for solving convex feasibility problems.

SIAM Review  38(3):367–426  1996.

[6] H. H. Bauschke  J. B. Cruz  T. T. Nghia  H. M. Phan  and X. Wang. The rate of linear convergence of the
Douglas–Rachford algorithm for subspaces is the cosine of the Friedrichs angle. Journal of Approximation
Theory  185:63–79  2014.

[7] A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods. SIAM Journal

on Optimization  23(4):2037–2060  2013.

[8] J. V. Burke and J. J. Mor´e. On the identiﬁcation of active constraints. SIAM Journal on Numerical

Analysis  25(5):1197–1211  1988.

[9] F. R. Chung. Spectral Graph Theory. American Mathematical Society  1997.
[10] F. Deutsch. The method of alternating orthogonal projections. In Approximation Theory  Spline Functions

and Applications  pages 105–121. Springer  1992.

[11] F. Deutsch. Best Approximation in Inner Product Spaces  volume 7. Springer  2001.
[12] F. Deutsch and H. Hundal. The rate of convergence of Dykstra’s cyclic projections algorithm: The poly-

hedral case. Numerical Functional Analysis and Optimization  15(5-6):537–565  1994.

8

[13] F. Deutsch and H. Hundal. The rate of convergence for the cyclic projections algorithm I: angles between

convex sets. Journal of Approximation Theory  142(1):36–55  2006.

[14] P. Diaconis  K. Khare  and L. Saloff-Coste. Stochastic alternating projections. Illinois Journal of Mathe-

matics  54(3):963–979  2010.

[15] J. Edmonds. Combinatorial Structures and Their Applications  chapter Submodular Functions  Matroids

and Certain Polyhedra  pages 69–87. Gordon and Breach  1970.

[16] A. Fix  T. Joachims  S. Park  and R. Zabih. Structured learning of sum-of-submodular higher order energy

functions. In Int. Conference on Computer Vision (ICCV)  2013.

[17] S. Fujishige and S. Isotani. A submodular function minimization algorithm based on the minimum-norm

base. Paciﬁc Journal of Optimization  7:3–17  2011.

[18] R. M. Gray. Toeplitz and circulant matrices: A review. Foundations and Trends in Communications and

Information Theory  2(3):155–239  2006.

[19] M. Gr¨otschel  L. Lov´asz  and A. Schrijver. The ellipsoid method and its consequences in combinatorial

optimization. Combinatorica  1(2):169–197  1981.

[20] L. Gubin  B. Polyak  and E. Raik. The method of projections for ﬁnding the common point of convex

sets. USSR Computational Mathematics and Mathematical Physics  7(6):1–24  1967.

[21] I. Halperin. The product of projection operators. Acta Sci. Math. (Szeged)  23:96–99  1962.
[22] D. Hochbaum and V. Singh. An efﬁcient algorithm for co-segmentation. In Int. Conference on Computer

Vision (ICCV)  2009.

[23] S. Iwata. A faster scaling algorithm for minimizing submodular functions. SIAM J. on Computing  32:

833–840  2003.

[24] S. Jegelka  H. Lin  and J. Bilmes. On fast approximate sumodular minimization. In Advances in Neural

Information Processing Systems  2011.

[25] S. Jegelka  F. Bach  and S. Sra. Reﬂection methods for user-friendly submodular optimization. In Ad-

vances in Neural Information Processing Systems  pages 1313–1321  2013.

[26] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for hierarchical sparse coding.

JMLR  page 22972334  2011.

[27] A. V. Knyazev and M. E. Argentati. Principal angles between subspaces in an A-based scalar product:
algorithms and perturbation estimates. SIAM Journal on Scientiﬁc Computing  23(6):2008–2040  2002.
[28] P. Kohli  L. Ladick´y  and P. Torr. Robust higher order potentials for enforcing label consistency. Int.

Journal of Computer Vision  82  2009.

[29] V. Kolmogorov. Minimizing a sum of submodular functions. Discrete Applied Mathematics  160(15):

2246–2258  2012.

[30] N. Komodakis  N. Paragios  and G. Tziritas. MRF energy minimization and beyond via dual decomposi-

tion. IEEE Trans. Pattern Analysis and Machine Intelligence  2011.

[31] H. Lin and J. Bilmes. Optimal selection of limited vocabulary speech corpora. In Proc. Interspeech  2011.
[32] S. McCormick. Handbook on Discrete Optimization  chapter Submodular Function Minimization  pages

321–391. Elsevier  2006.

[33] M. Narasimhan and J. Bilmes. Local search for balanced submodular clusterings. In IJCAI  pages 981–

986  2007.

[34] J. Orlin. A faster strongly polynomial time algorithm for submodular function minimization. Math.

Programming  118:237–251  2009.

[35] P. Stobbe and A. Krause. Efﬁcient minimization of decomposable submodular functions. In Advances in

Neural Information Processing Systems  2010.

[36] P. Tseng. Alternating projection-proximal methods for convex programming and variational inequalities.

SIAM Journal on Optimization  7(4):951–965  1997.

[37] S. Vicente  V. Kolmogorov  and C. Rother. Joint optimization of segmentation and appearance models. In

Int. Conference on Computer Vision (ICCV)  2009.

[38] J. Von Neumann. Functional Operators: The Geometry of Orthogonal Spaces. Princeton University

Press  1950.

9

,Jiashi Feng
Huan Xu
Shie Mannor
Shuicheng Yan
Robert Nishihara
Stefanie Jegelka
Michael Jordan