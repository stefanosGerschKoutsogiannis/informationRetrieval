2016,Minimizing Quadratic Functions in Constant Time,A sampling-based optimization method for quadratic functions is   proposed. Our method approximately solves the following   $n$-dimensional quadratic minimization problem in constant time    which is independent of $n$:   $z^*=\min_{\bv \in \bbR^n}\bracket{\bv}{A \bv} +   n\bracket{\bv}{\diag(\bd)\bv} + n\bracket{\bb}{\bv}$    where $A \in \bbR^{n \times n}$ is a matrix and $\bd \bb \in \bbR^n$   are vectors. Our theoretical analysis specifies the number of   samples $k(\delta  \epsilon)$ such that the approximated solution   $z$ satisfies $|z - z^*| = O(\epsilon n^2)$ with probability   $1-\delta$. The empirical performance (accuracy and runtime) is   positively confirmed by numerical experiments.,Minimizing Quadratic Functions in Constant Time

National Institute of Advanced Industrial Science and Technology

hayashi.kohei@gmail.com

Kohei Hayashi

National Institute of Informatics and Preferred Infrastructure  Inc.

yyoshida@nii.ac.jp

Yuichi Yoshida

Abstract

A sampling-based optimization method for quadratic functions is proposed.
Our method approximately solves the following n-dimensional quadratic min-
z∗ =
imization problem in constant
minv∈Rn(cid:104)v  Av(cid:105) + n(cid:104)v  diag(d)v(cid:105) + n(cid:104)b  v(cid:105)  where A ∈ Rn×n is a matrix and
d  b ∈ Rn are vectors. Our theoretical analysis speciﬁes the number of samples
k(δ  ) such that the approximated solution z satisﬁes |z − z∗| = O(n2) with
probability 1− δ. The empirical performance (accuracy and runtime) is positively
conﬁrmed by numerical experiments.

time  which is independent of n:

1

Introduction

A quadratic function is one of the most important function classes in machine learning  statistics 
and data mining. Many fundamental problems such as linear regression  k-means clustering  prin-
cipal component analysis  support vector machines  and kernel methods [14] can be formulated as a
minimization problem of a quadratic function.
In some applications  it is sufﬁcient to compute the minimum value of a quadratic function rather
than its solution. For example  Yamada et al. [21] proposed an efﬁcient method for estimating the
Pearson divergence  which provides useful information about data  such as the density ratio [18].
They formulated the estimation problem as the minimization of a squared loss and showed that the
Pearson divergence can be estimated from the minimum value. The least-squares mutual informa-
tion [19] is another example that can be computed in a similar manner.
Despite its importance  the minimization of a quadratic function has the issue of scalability. Let
n ∈ N be the number of variables (the “dimension” of the problem). In general  such a minimization
problem can be solved by quadratic programming (QP)  which requires poly(n) time. If the problem
is convex and there are no constraints  then the problem is reduced to solving a system of linear
equations  which requires O(n3) time. Both methods easily become infeasible  even for medium-
scale problems  say  n > 10000.
Although several techniques have been proposed to accelerate quadratic function minimization  they
require at least linear time in n. This is problematic when handling problems with an ultrahigh
dimension  for which even linear time is slow or prohibitive. For example  stochastic gradient
descent (SGD) is an optimization method that is widely used for large-scale problems. A nice
property of this method is that  if the objective function is strongly convex  it outputs a point that
is sufﬁciently close to an optimal solution after a constant number of iterations [5]. Nevertheless 
in each iteration  we need at least O(n) time to access the variables. Another technique is low-
rank approximation such as Nystr¨om’s method [20]. The underlying idea is the approximation
of the problem by using a low-rank matrix  and by doing so  we can drastically reduce the time

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

minimize

v∈Rn

complexity. However  we still need to compute the matrix–vector product of size n  which requires
O(n) time. Clarkson et al. [7] proposed sublinear-time algorithms for special cases of quadratic
function minimization. However  it is “sublinear” with respect to the number of pairwise interactions
of the variables  which is O(n2)  and their algorithms require O(n logc n) time for some c ≥ 1.
Our contributions: Let A ∈ Rn×n be a matrix and d  b ∈ Rn be vectors. Then  we consider the
following quadratic problem:

pn A d b(v)  where pn A d b(v) = (cid:104)v  Av(cid:105) + n(cid:104)v  diag(d)v(cid:105) + n(cid:104)b  v(cid:105).

(1)
Here  (cid:104)· ·(cid:105) denotes the inner product and diag(d) denotes the matrix whose diagonal entries are
speciﬁed by d. Note that a constant term can be included in (1); however  it is irrelevant when
optimizing (1)  and hence we ignore it.
Let z∗ ∈ R be the optimal value of (1) and let   δ ∈ (0  1) be parameters. Then  the main goal of
this paper is the computation of z with |z − z∗| = O(n2) with probability at least 1 − δ in constant
time  that is  independent of n. Here  we assume the real RAM model [6]  in which we can perform
basic algebraic operations on real numbers in one step. Moreover  we assume that we have query
accesses to A  b  and d  with which we can obtain an entry of them by specifying an index. We note
that z∗ is typically Θ(n2) because (cid:104)v  Av(cid:105) consists of Θ(n2) terms  and (cid:104)v  diag(d)v(cid:105) and (cid:104)b  v(cid:105)
consist of Θ(n) terms. Hence  we can regard the error of Θ(n2) as an error of Θ() for each term 
which is reasonably small in typical situations.
Let ·|S be an operator that extracts a submatrix (or subvector) speciﬁed by an index set S ⊂ N; then 
our algorithm is deﬁned as follows  where the parameter k := k(  δ) will be determined later.

Algorithm 1
Input: An integer n ∈ N  query accesses to the matrix A ∈ Rn×n and to the vectors d  b ∈ Rn 
1: S ← a sequence of k = k(  δ) indices independently and uniformly sampled from

and   δ > 0
{1  2  . . .   n}.

2: return n2

k2 minv∈Rn pk A|S  d|S  b|S (v).

In other words  we sample a constant number of indices from the set {1  2  . . .   n}  and then solve
the problem (1) restricted to these indices. Note that the number of queries and the time complexity
are O(k2) and poly(k)  respectively. In order to analyze the difference between the optimal values
of pn A d b and pk A|S  d|S  b|S   we want to measure the “distances” between A and A|S  d and d|S 
and b and b|S  and want to show them small. To this end  we exploit graph limit theory  initiated by
Lov´asz and Szegedy [11] (refer to [10] for a book)  in which we measure the distance between two
graphs on different number of vertices by considering continuous versions. Although the primary
interest of graph limit theory is graphs  we can extend the argument to analyze matrices and vectors.
Using synthetic and real settings  we demonstrate that our method is orders of magnitude faster than
standard polynomial-time algorithms and that the accuracy of our method is sufﬁciently high.

Related work: Several constant-time approximation algorithms are known for combinatorial op-
timization problems such as the max cut problem on dense graphs [8  13]  constraint satisfaction
problems [1  22]  and the vertex cover problem [15  16  25]. However  as far as we know  no such
algorithm is known for continuous optimization problems.
A related notion is property testing [9  17]  which aims to design constant-time algorithms that
distinguish inputs satisfying some predetermined property from inputs that are “far” from satisfying
it. Characterizations of constant-time testable properties are known for the properties of a dense
graph [2  3] and the afﬁne-invariant properties of a function on a ﬁnite ﬁeld [23  24].

2 Preliminaries
For an integer n  let [n] denote the set {1  2  . . .   n}. The notation a = b± c means that b− c ≤ a ≤
b + c. In this paper  we only consider functions and sets that are measurable.

2

Let S = (x1  . . .   xk) be a sequence of k indices in [n]. For a vector v ∈ Rn  we denote the
restriction of v to S by v|S ∈ Rk; that is  (v|S)i = vxi for every i ∈ [k]. For the matrix A ∈ Rn×n 
we denote the restriction of A to S by A|S ∈ Rk×k; that is  (A|S)ij = Axixj for every i  j ∈ [k].

2.1 Dikernels
Following [12]  we call a (measurable) function f : [0  1]2 → R a dikernel. A dikernel is a general-
ization of a graphon [11]  which is symmetric and whose range is bounded in [0  1]. We can regard a
dikernel as a matrix whose index is speciﬁed by a real value in [0  1]. We stress that the term dikernel
has nothing to do with kernel methods.

For two functions f  g : [0  1] → R  we deﬁne their inner product as (cid:104)f  g(cid:105) =(cid:82) 1
(cid:12)(cid:12)(cid:12)(cid:82)

0 f (x)g(x)dx. For a
dikernel W : [0  1]2 → R and a function f : [0  1] → R  we deﬁne a function W f : [0  1] → R as
(W f )(x) = (cid:104)W (x ·)  f(cid:105).
Let W : [0  1]2 → R be a dikernel. The Lp norm (cid:107)W(cid:107)p for p ≥ 1 and the cut norm (cid:107)W(cid:107)(cid:3) of W are
deﬁned as (cid:107)W(cid:107)p =
T W (x  y)dxdy
respectively  where the supremum is over all pairs of subsets. We note that these norms satisfy the
triangle inequalities and (cid:107)W(cid:107)(cid:3) ≤ (cid:107)W(cid:107)1.
Let λ be a Lebesgue measure. A map π : [0  1] → [0  1] is said to be measure-preserving  if
the pre-image π−1(X) is measurable for every measurable set X  and λ(π−1(X)) = λ(X). A
measure-preserving bijection is a measure-preserving map whose inverse map exists and is also
measurable (and then also measure-preserving). For a measure preserving bijection π : [0  1] →
[0  1] and a dikernel W : [0  1]2 → R  we deﬁne the dikernel π(W ) : [0  1]2 → R as π(W )(x  y) =
W (π(x)  π(y)).

(cid:82) 1
0 |W (x  y)|pdxdy

and (cid:107)W(cid:107)(cid:3) = supS T⊆[0 1]

(cid:17)1/p

(cid:16)(cid:82) 1

(cid:12)(cid:12)(cid:12) 

(cid:82)

S

0

[0  1

2.2 Matrices and Dikernels
Let W : [0  1]2 → R be a dikernel and S = (x1  . . .   xk) be a sequence of elements in [0  1]. Then 
we deﬁne the matrix W|S ∈ Rk×k so that (W|S)ij = W (xi  xj).

We can construct the dikernel (cid:98)A : [0  1]2 → R from the matrix A ∈ Rn×n as follows. Let I1 =
integer such that x ∈ Ii. Then  we deﬁne (cid:98)A(x  y) = Ain(x)in(y). The main motivation for creating a
n   . . .   1]. For x ∈ [0  1]  we deﬁne in(x) ∈ [n] as a unique
B of different sizes via the cut norm  that is  (cid:107)(cid:98)A − (cid:98)B(cid:107)(cid:3).
independently sampled from [n] exactly matches the distribution of (cid:98)A|S  where S is a sequence of

We note that the distribution of A|S  where S is a sequence of k indices that are uniformly and

dikernel from a matrix is that  by doing so  we can deﬁne the distance between two matrices A and

n ]  . . .   In = ( n−1

n ]  I2 = ( 1

n   2

k elements that are uniformly and independently sampled from [0  1].

3 Sampling Theorem and the Properties of the Cut Norm

In this section  we prove the following theorem  which states that  given a sequence of dikernels
W 1  . . .   W T : [0  1]2 → [−L  L]  we can obtain a good approximation to them by sampling a
sequence of a small number of elements in [0  1]. Formally  we prove the following:
Theorem 3.1. Let W 1  . . .   W T : [0  1]2 → [−L  L] be dikernels. Let S be a sequence of k
elements uniformly and independently sampled from [0  1]. Then  with a probability of at least
1 − exp(−Ω(kT / log2 k))  there exists a measure-preserving bijection π : [0  1] → [0  1] such that 
for any functions f  g : [0  1] → [−K  K] and t ∈ [T ]  we have

|(cid:104)f  W tg(cid:105) − (cid:104)f  π( (cid:91)W t|S)g(cid:105)| = O

(cid:16)

LK 2(cid:112)T / log2 k

(cid:17)

.

We start with the following lemma  which states that  if a dikernel W : [0  1]2 → R has a small cut
norm  then (cid:104)f  W f(cid:105) is negligible no matter what f is. Hence  we can focus on the cut norm when
proving Theorem 3.1.

3

Lemma 3.2. Let  ≥ 0 and W : [0  1]2 → R be a dikernel with (cid:107)W(cid:107)(cid:3) ≤ . Then  for any functions
f  g : [0  1] → [−K  K]  we have |(cid:104)f  W g(cid:105)| ≤ K 2.
Proof. For τ ∈ R and the function h : [0  1] → R  let Lτ (h) := {x ∈ [0  1] | h(x) = τ} be the level
set of h at τ. For f(cid:48) = f /K and g(cid:48) = g/K  we have

|(cid:104)f  W g(cid:105)| = K 2|(cid:104)f(cid:48)  W g(cid:48)(cid:105)| = K 2(cid:12)(cid:12)(cid:12)(cid:90) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:90)

(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1

|τ1||τ2|

≤ K 2

−1

−1

−1

(cid:90) 1
(cid:90)

−1

Lτ1 (f(cid:48))

Lτ2 (g(cid:48))
|τ1||τ2|dτ1dτ2 = K 2.

≤ K 2

−1

−1

(cid:90)

(cid:90)

Lτ1 (f(cid:48))

Lτ2 (g(cid:48))

W (x  y)dxdy

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dτ1dτ2

τ1τ2

W (x  y)dxdydτ1dτ2

(cid:12)(cid:12)(cid:12)

(cid:90)

To introduce the next technical tool  we need several deﬁnitions. We say that the partition Q is a
reﬁnement of the partition P = (V1  . . .   Vp) if Q is obtained by splitting each set Vi into one or more
parts. The partition P = (V1  . . .   Vp) of the interval [0  1] is called an equipartition if λ(Vi) = 1/p
for every i ∈ [p]. For the dikernel W : [0  1]2 → R and the equipartition P = (V1  . . .   Vp) of [0  1] 
we deﬁne WP : [0  1]2 → R as the function obtained by averaging each Vi × Vj for i  j ∈ [p]. More
formally  we deﬁne

WP (x  y) =

1

λ(Vi)λ(Vj)

Vi×Vj

W (x(cid:48)  y(cid:48))dx(cid:48)dy(cid:48) = p2

W (x(cid:48)  y(cid:48))dx(cid:48)dy(cid:48) 

where i and j are unique indices such that x ∈ Vi and y ∈ Vj  respectively.
The following lemma states that any function W : [0  1]2 → R can be well approximated by WP
for the equipartition P into a small number of parts.
Lemma 3.3 (Weak regularity lemma for functions on [0  1]2 [8]). Let P be an equipartition of [0  1]
into k sets. Then  for any dikernel W : [0  1]2 → R and  > 0  there exists a reﬁnement Q of P with
|Q| ≤ k2C/2 for some constant C > 0 such that

(cid:90)

Vi×Vj

Corollary 3.4. Let W 1  . . .   W T : [0  1]2 → R be dikernels. Then  for any  > 0  there exists an
equipartition P into |P| ≤ 2CT /2 parts for some constant C > 0 such that  for every t ∈ [T ] 

(cid:107)W − WQ(cid:107)(cid:3) ≤ (cid:107)W(cid:107)2.

(cid:107)W t − W tP(cid:107)(cid:3) ≤ (cid:107)W t(cid:107)2.

Proof. Let P 0 be a trivial partition  that is  a partition consisting of a single part [n]. Then  for each
t ∈ [T ]  we iteratively apply Lemma 3.3 with P t−1  W t  and   and we obtain the partition P t into
at most |P t−1|2C/2 parts such that (cid:107)W t − W tP t(cid:107)(cid:3) ≤ (cid:107)W t(cid:107)2. Since P t is a reﬁnement of P t−1 
we have (cid:107)W i − W iP t(cid:107)(cid:3) ≤ (cid:107)W i − W iP t−1(cid:107)(cid:3) for every i ∈ [t − 1]. Then  P T satisﬁes the desired
property with |P T| ≤ (2C/2
As long as S is sufﬁciently large  W and (cid:100)W|S are close in the cut norm:

Lemma 3.5 ((4.15) of [4]). Let W : [0  1]2 → [−L  L] be a dikernel and S be a sequence of k
elements uniformly and independently sampled from [0  1]. Then  we have

)T = 2CT /2.

≤ ES(cid:107)(cid:100)W|S(cid:107)(cid:3) − (cid:107)W(cid:107)(cid:3) <

− 2L
k

8L
k1/4

.

Finally  we need the following concentration inequality.
Lemma 3.6 (Azuma’s inequality). Let (Ω  A  P ) be a probability space  k be a positive integer  and
C > 0. Let z = (z1  . . .   zk)  where z1  . . .   zk are independent random variables  and zi takes
values in some measure space (Ωi  Ai). Let f : Ω1 × ··· × Ωk → R be a function. Suppose that
|f (x) − f (y)| ≤ C whenever x and y only differ in one coordinate. Then

(cid:104)|f (z) − Ez[f (z)]| > λC

(cid:105)

Pr

< 2e−λ2/2k.

4

Now we prove the counterpart of Theorem 3.1 for the cut norm.
[0  1]2 → [−L  L] be dikernels. Let S be a sequence of k
Lemma 3.7. Let W 1  . . .   W T :
elements uniformly and independently sampled from [0  1]. Then  with a probability of at least
1 − exp(−Ω(kT / log2 k))  there exists a measure-preserving bijection π : [0  1] → [0  1] such that 
for every t ∈ [T ]  we have

(cid:107)W t − π( (cid:91)W t|S)(cid:107)(cid:3) = O

(cid:16)

L(cid:112)T / log2 k

(cid:17)

.

Proof. First  we bound the expectations and then prove their concentrations. We apply Corollary 3.4
to W 1  . . .   W T and   and let P = (V1  . . .   Vp) be the obtained partition with p ≤ 2CT /2 parts
such that
(cid:107)W t − W tP(cid:107)(cid:3) ≤ L.
for every t ∈ [T ]. By Lemma 3.5  for every t ∈ [T ]  we have

ES(cid:107)(cid:92)

W tP|S − (cid:91)W t|S(cid:107)(cid:3) = ES(cid:107)(W tP − W t)|S(cid:92) (cid:107)(cid:3) ≤ L +

.
Then  for any measure-preserving bijection π : [0  1] → [0  1] and t ∈ [T ]  we have
ES(cid:107)W t − π( (cid:91)W t|S)(cid:107)(cid:3) ≤ (cid:107)W t − W tP(cid:107)(cid:3) + ES(cid:107)W tP − π(

(cid:92)
W tP|S) − π( (cid:91)W t|S)(cid:107)(cid:3)
(2)
Thus  we are left with the problem of sampling from P. Let S = {x1  . . .   xk} be a sequence of
independent random variables that are uniformly distributed in [0  1]  and let Zi be the number of
points xj that fall into the set Vi. It is easy to compute that

(cid:92)
W tP|S)(cid:107)(cid:3) + ES(cid:107)π(
(cid:92)
W tP|S)(cid:107)(cid:3).

+ ES(cid:107)W tP − π(

≤ 2L +

8L
k1/4

8L
k1/4

(cid:16) 1

(cid:17)

k
p

E[Zi] =

and Var[Zi] =
The partition P(cid:48) of [0  1] is constructed into the sets V (cid:48)
i ) = min(1/p  Zi/k). For each t ∈ [T ]  we construct the dikernel W
V (cid:48)
value of W

j is the same as the value of W tP on Vi × Vj. Then  W

− 1
p2
p such that λ(V (cid:48)
1   . . .   V (cid:48)

i × V (cid:48)

t on V (cid:48)

k <

k
p

p

.

t

the set Q =(cid:83)

j ). Then  there exists a bijection π such that π(

i ) = Zi/k and λ(Vi ∩
: [0  1] → R such that the
t agrees with W tP on
(cid:92)
W tP|S) = W

t

t(cid:107)1 ≤ 2L(1 − λ(Q))

(cid:16) 1

(cid:17)(cid:17)

min

 

Zi
k

p

 

p

min

= 2L

Zi
k

for each t ∈ [T ]. Then  for every t ∈ [T ]  we have

i j∈[p](Vi∩V (cid:48)
1 −(cid:16)(cid:88)
(cid:16)
(cid:107)W tP − π(
(cid:12)(cid:12)(cid:12) 1
= 2L
(cid:88)

i )×(Vj ∩V (cid:48)
(cid:16) 1
(cid:12)(cid:12)(cid:12) ≤ 2L
(cid:16)

i∈[p]
− Zi
k

(cid:17)(cid:17)2(cid:17) ≤ 4L
(cid:16)
1 −(cid:88)
(cid:92)
t(cid:107)(cid:3) ≤ (cid:107)W tP − W
W tP|S)(cid:107)(cid:3) = (cid:107)W tP − W
(cid:17)2(cid:17)1/2
(cid:16) 1
(cid:88)
(cid:16) 1
(cid:88)
The expectation of the right hand side is (4L2p/k2)(cid:80)
W tP|S)(cid:107)(cid:3) ≤ 2L(cid:112)p/k.
(cid:114) p

Schwartz inequality  E(cid:107)W tP − π(
Inserted this into (2)  we obtain

(cid:92)
W tP|S)(cid:107)2(cid:3) ≤ 4L2p

which we rewrite as

(cid:107)W tP − π(

− Zi
k

(cid:92)

i∈[p]

i∈[p]

i∈[p]

p

p

p

p

 

(cid:17)2

.

− Zi
k

Choosing  =(cid:112)CT /(log2 k1/4) =(cid:112)4CT /(log2 k)  we obtain the upper bound
(cid:115)

E(cid:107)W t − π( (cid:91)W t|S)(cid:107)(cid:3) ≤ 2L +
(cid:115)
E(cid:107)W t − π( (cid:91)W t|S)(cid:107)(cid:3) ≤ 2L

≤ 2L +

8L
k1/4

8L
k1/4

2L
k1/2

(cid:16)

+ 2L

= O

+

+

+

L

k

T

4CT
log2 k

8L
k1/4

2L
k1/4

log2 k

(cid:17)

.

2CT /2

.

i∈[p]
i∈[p] Var(Zi) < 4L2p/k. By the Cauchy-

5

Observing that (cid:107)W t − π( (cid:91)W t|S)(cid:107)(cid:3) changes by at most O(L/k) if one element in S changes  we

apply Azuma’s inequality with λ = k(cid:112)T / log2 k and the union bound to complete the proof.

The proof of Theorem 3.1 is immediately follows from Lemmas 3.2 and 3.7.

4 Analysis of Algorithm 1

In this section  we analyze Algorithm 1. Because we want to use dikernels for the analysis  we
introduce a continuous version of pn A d b (recall (1)). The real-valued function Pn A d b on the
functions f : [0  1] → R is deﬁned as

Pn A d b(f ) = (cid:104)f  (cid:98)Af(cid:105) + (cid:104)f 2  (cid:100)d1(cid:62)1(cid:105) + (cid:104)f (cid:100)b1(cid:62)1(cid:105) 

where f 2 : [0  1] → R is a function such that f 2(x) = f (x)2 for every x ∈ [0  1] and 1 : [0  1] → R
is the constant function that has a value of 1 everywhere. The following lemma states that the
minimizations of pn A d b and Pn A d b are equivalent:
Lemma 4.1. Let A ∈ Rn×n be a matrix and d  b ∈ Rn×n be vectors. Then  we have

min

v∈[−K K]n

pn A d b(v) = n2 ·

inf

f :[0 1]→[−K K]

Pn A d b(f ).

for any K > 0.
Proof. First  we show that n2 · inf f :[0 1]→[−K K] Pn A d b(f ) ≤ minv∈[−K K]n pn A d b(v). Given
a vector v ∈ [−K  K]n  we deﬁne f : [0  1] → [−K  K] as f (x) = vin(x). Then 
(cid:104)f  (cid:98)Af(cid:105) =
n2(cid:104)v  Av(cid:105) 
(cid:88)
(cid:104)f 2  (cid:100)d1(cid:62)1(cid:105) =
(cid:88)
(cid:104)f (cid:100)b1(cid:62)1(cid:105) =

(cid:90)
(cid:88)
(cid:90)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

1
n
(cid:104)v  b(cid:105).

Aijf (x)f (y)dxdy =

dif (x)2dxdy =

dif (x)2dx =

(cid:90)
(cid:90)
(cid:90)

(cid:90)
(cid:90)
(cid:90)

(cid:88)

bif (x)dxdy =

bif (x)dx =

Aijvivj =

(cid:104)v  diag(d)v(cid:105) 

i j∈[n]

i j∈[n]

i j∈[n]

div2

i =

bivi =

1
n2

i∈[n]

i∈[n]

Ii

Ij

Ii

Ij

1
n

1

Ii

1
n

i∈[n]

1
n

i j∈[n]

Ii

Ij

i∈[n]

Ii

Then  we have n2Pn A d b(f ) ≤ pn A d b(v).
Next  we show that minv∈[−K K]n pn A d b(v) ≤ n2 · inf f :[0 1]→[−K K] Pn A d b(f ). Let f :
[0  1] → [−K  K] be a measurable function. Then  for x ∈ [0  1]  we have
∂Pn A d b(f (x))

(cid:88)

(cid:88)

(cid:90)

(cid:90)

Aiin(x)f (y)dy +

Ain(x)jf (y)dy + 2din(x)f (x) + bin(x).

∂f (x)

=

i∈[n]

Ii

j∈[n]

Ij

Note that the form of this partial derivative only depends on in(x); hence  in the optimal solution
f∗ : [0  1] → [−K  K]  we can assume f∗(x) = f∗(y) if in(x) = in(y). In other words  f∗
is constant on each of the intervals I1  . . .   In. For such f∗  we deﬁne the vector v ∈ Rn as
vi = f∗(x)  where x ∈ [0  1] is any element in Ii. Then  we have

(cid:104)v  Av(cid:105) =

(cid:104)v  diag(d)v(cid:105) =

(cid:104)v  b(cid:105) =

Aijf∗(x)f∗(y)dxdy = n2(cid:104)f∗  (cid:98)Af∗(cid:105) 

(cid:90)

Aijvivj = n2 (cid:88)
(cid:90)
(cid:90)

(cid:88)
(cid:88)

(cid:90)
dif∗(x)2dx = n(cid:104)(f∗)2 (cid:100)d1T 1(cid:105) 
bif∗(x)dx = n(cid:104)f∗ (cid:100)b1T 1(cid:105).

i j∈[n]

i = n

i∈[n]

Ij

Ii

Ii

bivi = n

div2

i j∈[n]

(cid:88)
(cid:88)
(cid:88)

i∈[n]

i∈[n]

i∈[n]

Ii

Finally  we have pn A d b(v) ≤ n2Pn A d b(f∗).

Now we show that Algorithm 1 well-approximates the optimal value of (1) in the following sense:

6

Theorem 4.2. Let v∗ and z∗ be an optimal solution and the optimal value  respectively  of prob-
lem (1). By choosing k(  δ) = 2Θ(1/2) + Θ(log 1
δ )  with a probability of at least
1 − δ  a sequence S of k indices independently and uniformly sampled from [n] satisﬁes the fol-
lowing: Let ˜v∗ and ˜z∗ be an optimal solution and the optimal value  respectively  of the problem
minv∈Rk pk A|S  d|S  b|S (v). Then  we have

δ log log 1

(cid:12)(cid:12)(cid:12) n2
k2 ˜z∗ − z∗(cid:12)(cid:12)(cid:12) ≤ LK 2n2 

i |  maxi∈[n] |˜v∗

where K = max{maxi∈[n] |v∗

i |} and L = max{maxi j |Aij|  maxi |di|  maxi |bi|}.
δ ) and the dikernels (cid:98)A 
(cid:100)d1(cid:62)  and (cid:100)b1(cid:62). Then  with a probability of at least 1− δ  there exists a measure preserving bijection
b1(cid:62)|S))1(cid:105)|(cid:111) ≤ LK 2

Proof. We instantiate Theorem 3.1 with k = 2Θ(1/2) + Θ(log 1
π : [0  1] → [0  1] such that

(cid:110)|(cid:104)f  ((cid:98)A − π((cid:100)A|S))f(cid:105)| |(cid:104)f 2  ((cid:100)d1(cid:62) − π(

d1(cid:62)|S))1(cid:105)| |(cid:104)f  ((cid:100)b1(cid:62) − π(

δ log log 1

(cid:92)

(cid:92)

max
for any function f : [0  1] → [−K  K]. Then  we have

3

pk A|S  d|S  b|S (v) = min

pk A|S  d|S  b|S (v)

v∈[−K K]k
Pk A|S  d|S  b|S (f )

(cid:16)(cid:104)f  (π((cid:100)A|S) − (cid:98)A)f(cid:105) + (cid:104)f  (cid:98)Af(cid:105) + (cid:104)f 2  (π(
b1(cid:62)|S) − (cid:100)b1(cid:62))1(cid:105) + (cid:104)f (cid:100)b1(cid:62)1(cid:105)(cid:17)
(cid:104)f 2  (cid:100)d1(cid:62)1(cid:105) + (cid:104)f  (π(
(cid:16)(cid:104)f  (cid:98)Af(cid:105) + (cid:104)f 2  (cid:100)d1(cid:62)1(cid:105) + (cid:104)f (cid:100)b1(cid:62)1(cid:105) ± LK 2(cid:17)

(cid:92)

(cid:92)

(By Lemma 4.1)

d1(cid:62)|S) − (cid:100)d1(cid:62))1(cid:105)+

pn A d b(v) ± LK 2k2.

(By Lemma 4.1)

˜z∗ = min
v∈Rk
= k2 ·
= k2 ·

inf

f :[0 1]→[−K K]

inf

f :[0 1]→[−K K]

f :[0 1]→[−K K]

inf

k2

≤ k2 ·
n2 · min
n2 · min
v∈Rn

k2

=

=

v∈[−K K]n

pn A d b(v) ± LK 2k2 =

k2

n2 z∗ ± LK 2k2.

Rearranging the inequality  we obtain the desired result.

We can show that K is bounded when A is symmetric and full rank. To see this  we ﬁrst note
that we can assume A + ndiag(d) is positive-deﬁnite  as otherwise pn A d b is not bounded and
the problem is uninteresting. Then  for any set S ⊆ [n] of k indices  (A + ndiag(d))|S is again
positive-deﬁnite because it is a principal submatrix. Hence  we have v∗ = (A + ndiag(d))−1nb/2
and ˜v∗ = (A|S + ndiag(d|S))−1nb|S/2  which means that K is bounded.

5 Experiments

In this section  we demonstrate the effectiveness of our method by experiment.1 All experiments
were conducted on an Amazon EC2 c3.8xlarge instance. Error bars indicate the standard deviations
over ten trials with different random seeds.

Numerical simulation We investigated the actual relationships between n  k  and . To this end 
we prepared synthetic data as follows. We randomly generated inputs as Aij ∼ U[−1 1]  di ∼ U[0 1] 
and bi ∼ U[−1 1] for i  j ∈ [n]  where U[a b] denotes the uniform distribution with the support [a  b].
After that  we solved (1) by using Algorithm 1 and compared it with the exact solution obtained by
QP.2 The result (Figure 1) show the approximation errors were evenly controlled regardless of n 
which meets the error analysis (Theorem 4.2).

1The program codes are available at https://github.com/hayasick/CTOQ.
2We used GLPK (https://www.gnu.org/software/glpk/) for the QP solver.

7

Table 1: Pearson divergence: runtime (second).

e
s
o
p
o
r
P

k
d 20
40
80
160
m 20
40
80
160

¨o
r
t
s
y
N

n = 500
0.002
0.003
0.007
0.030
0.005
0.010
0.022
0.076

1000
0.002
0.003
0.007
0.030
0.012
0.022
0.049
0.116

2000
0.002
0.003
0.008
0.033
0.046
0.087
0.188
0.432

5000
0.002
0.003
0.008
0.035
0.274
0.513
0.942
1.972

Figure 1: Numerical simulation: abso-
lute approximation error scaled by n2.

e
s
o
p
o
r
P

k
d 20
40
80
160
m 20
40
80
160

¨o
r
t
s
y
N

Table 2: Pearson divergence: absolute approximation error.
n = 500
0.0027 ± 0.0028
0.0018 ± 0.0023
0.0007 ± 0.0008
0.0003 ± 0.0003
0.3685 ± 0.9142
0.3549 ± 0.6191
0.0184 ± 0.0192
0.0143 ± 0.0209

2000
0.0021 ± 0.0019
0.0012 ± 0.0011
0.0008 ± 0.0008
0.0003 ± 0.0003
3.1119 ± 6.1464
0.9838 ± 1.5422
0.2056 ± 0.2725
0.0585 ± 0.1112

1000
0.0012 ± 0.0012
0.0006 ± 0.0007
0.0004 ± 0.0003
0.0002 ± 0.0001
1.3006 ± 2.4504
0.4207 ± 0.7018
0.0398 ± 0.0472
0.0348 ± 0.0541

5000
0.0016 ± 0.0022
0.0011 ± 0.0020
0.0007 ± 0.0017
0.0002 ± 0.0003
0.6989 ± 0.9644
0.3744 ± 0.6655
0.5705 ± 0.7918
0.0254 ± 0.0285

(cid:80)n

j=1 φ(x(cid:48)

j  xl)φ(x(cid:48)

n(cid:48)) ∈ Rn(cid:48)

2 − minv∈Rn

n(cid:48) (cid:80)n(cid:48)

(cid:80)n
1  . . .   x(cid:48)
i=1 φ(xi  xl)φ(xi  xm) + 1−α

Application to kernel methods Next  we considered the kernel approximation of the Pearson
divergence [21]. The problem is deﬁned as follows. Suppose we have the two different data sets
x = (x1  . . .   xn) ∈ Rn and x(cid:48) = (x(cid:48)
where n  n(cid:48) ∈ N. Let H ∈ Rn×n
be a gram matrix such that Hl m = α
j  xm) 
where φ(· ·) is a kernel function and α ∈ (0  1) is a parameter. Also  let h ∈ Rn be a vector
n
such that hl = 1
i=1 φ(xi  xl). Then  an estimator of the α-relative Pearson divergence between
n
the distributions of x and x(cid:48) is obtained by − 1
2(cid:104)v  v(cid:105). Here 
λ > 0 is a regularization parameter. In this experiment  we used the Gaussian kernel φ(x  y) =
exp((x− y)2/2σ2) and set n(cid:48) = 200 and α = 0.5; σ2 and λ were chosen by 5-fold cross-validation
as suggested in [21]. We randomly generated the data sets as xi ∼ N (1  0.5) for i ∈ [n] and
j ∼ N (1.5  0.5) for j ∈ [n(cid:48)] where N (µ  σ2) denotes the Gaussian distribution with mean µ and
x(cid:48)
variance σ2.
We encoded this problem into (1) by setting A = 1
2n 1n  where 1n denotes
the n-dimensional vector whose elements are all one. After that  given k  we computed the second
step of Algorithm 1 with the pseudoinverse of A|S +kdiag(d|S). Absolute approximation errors and
runtimes were compared with Nystr¨om’s method whose approximated rank was set to k. In terms of
accuracy  our method clearly outperformed Nystr¨om’s method (Table 2). In addition  the runtimes
of our method were nearly constant  whereas the runtimes of Nystr¨om’s method grew linearly in k
(Table 1).

2 H  b = −h  and d = λ

2(cid:104)v  Hv(cid:105) − (cid:104)h  v(cid:105) + λ

1

6 Acknowledgments

We would like to thank Makoto Yamada for suggesting a motivating problem of our method. K. H. is
supported by MEXT KAKENHI 15K16055. Y. Y. is supported by MEXT Grant-in-Aid for Scientiﬁc
Research on Innovative Areas (No. 24106001)  JST  CREST  Foundations of Innovative Algorithms
for Big Data  and JST  ERATO  Kawarabayashi Large Graph Project.

8

●●●●●●●●●●●●●●●●●●●●0.000.050.100.000.050.100.000.050.100.000.050.10n=2005001000200010204080160k|z−z∗|n2References
[1] N. Alon  W. F. de la Vega  R. Kannan  and M. Karpinski. Random sampling and approximation of MAX-

CSP problems. In STOC  pages 232–239  2002.

[2] N. Alon  E. Fischer  I. Newman  and A. Shapira. A combinatorial characterization of the testable graph

properties: It’s all about regularity. SIAM Journal on Computing  39(1):143–167  2009.

[3] C. Borgs  J. Chayes  L. Lov´asz  V. T. S´os  B. Szegedy  and K. Vesztergombi. Graph limits and parameter

testing. In STOC  pages 261–270  2006.

[4] C. Borgs  J. T. Chayes  L. Lov´asz  V. T. S´os  and K. Vesztergombi. Convergent sequences of dense graphs
i: Subgraph frequencies  metric properties and testing. Advances in Mathematics  219(6):1801–1851 
2008.

[5] L. Bottou. Stochastic learning. In Advanced Lectures on Machine Learning  pages 146–168. 2004.
[6] V. Brattka and P. Hertling. Feasible real random access machines. Journal of Complexity  14(4):490–526 

[7] K. L. Clarkson  E. Hazan  and D. P. Woodruff. Sublinear optimization for machine learning. Journal of

[8] A. Frieze and R. Kannan. The regularity lemma and approximation schemes for dense problems.

In

1998.

the ACM  59(5):23:1–23:49  2012.

FOCS  pages 12–20  1996.

[9] O. Goldreich  S. Goldwasser  and D. Ron. Property testing and its connection to learning and approxima-

tion. Journal of the ACM  45(4):653–750  1998.

[10] L. Lov´asz. Large Networks and Graph Limits. American Mathematical Society  2012.
[11] L. Lov´asz and B. Szegedy. Limits of dense graph sequences. Journal of Combinatorial Theory  Series B 

[12] L. Lov´asz and K. Vesztergombi. Non-deterministic graph property testing. Combinatorics  Probability

96(6):933–957  2006.

and Computing  22(05):749–762  2013.

[13] C. Mathieu and W. Schudy. Yet another algorithm for dense max cut: go greedy. In SODA  pages 176–182 

[14] K. P. Murphy. Machine learning: a probabilistic perspective. The MIT Press  2012.
[15] H. N. Nguyen and K. Onak. Constant-time approximation algorithms via local improvements. In FOCS 

2008.

pages 327–336  2008.

[16] K. Onak  D. Ron  M. Rosen  and R. Rubinfeld. A near-optimal sublinear-time algorithm for approximat-

ing the minimum vertex cover size. In SODA  pages 1123–1131  2012.

[17] R. Rubinfeld and M. Sudan. Robust characterizations of polynomials with applications to program testing.

SIAM Journal on Computing  25(2):252–271  1996.

[18] M. Sugiyama  T. Suzuki  and T. Kanamori. Density Ratio Estimation in Machine Learning. Cambridge

[19] T. Suzuki and M. Sugiyama. Least-Squares Independent Component Analysis. Neural Computation 

University Press  2012.

23(1):284–301  2011.

[20] C. K. I. Williams and M. Seeger. Using the nystr¨om method to speed up kernel machines. In NIPS  2001.
[21] M. Yamada  T. Suzuki  T. Kanamori  H. Hachiya  and M. Sugiyama. Relative density-ratio estimation for

robust distribution comparison. In NIPS  2011.

[22] Y. Yoshida. Optimal constant-time approximation algorithms and (unconditional) inapproximability re-

sults for every bounded-degree CSP. In STOC  pages 665–674  2011.

[23] Y. Yoshida. A characterization of locally testable afﬁne-invariant properties via decomposition theorems.

In STOC  pages 154–163  2014.

[24] Y. Yoshida. Gowers norm  function limits  and parameter estimation. In SODA  pages 1391–1406  2016.
[25] Y. Yoshida  M. Yamamoto  and H. Ito. Improved constant-time approximation algorithms for maximum

matchings and other optimization problems. SIAM Journal on Computing  41(4):1074–1093  2012.

9

,Omer Levy
Yoav Goldberg
Kohei Hayashi
Yuichi Yoshida
Giacomo De Palma
Bobak Kiani
Seth Lloyd