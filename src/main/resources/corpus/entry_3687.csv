2014,Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms,Let $p$ be an unknown and arbitrary probability distribution over $[0  1)$. We consider the problem of \emph{density estimation}  in which a learning algorithm is given i.i.d. draws from $p$ and must (with high probability) output a hypothesis distribution that is close to $p$. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram  i.e.  a hypothesis distribution with a piecewise constant probability density function. In more detail  for any $k$ and $\eps$  we give an algorithm that makes $\tilde{O}(k/\eps^2)$ draws from $p$  runs in $\tilde{O}(k/\eps^2)$ time  and outputs a hypothesis distribution $h$ that is piecewise constant with $O(k \log^2(1/\eps))$ pieces. With high probability the hypothesis $h$ satisfies $\dtv(p h) \leq C \cdot \opt_k(p) + \eps$  where $\dtv$ denotes the total variation distance (statistical distance)  $C$ is a universal constant  and $\opt_k(p)$ is the smallest total variation distance between $p$ and any $k$-piecewise constant distribution. The sample size and running time of our algorithm are both optimal up to logarithmic factors. The ``approximation factor'' $C$ that is present in our result is inherent in the problem  as we prove that no algorithm with sample size bounded in terms of $k$ and $\eps$ can achieve $C < 2$ regardless of what kind of hypothesis distribution it uses.,Near–Optimal Density Estimation in Near–Linear

Time Using Variable–Width Histograms

Siu-On Chan

Microsoft Research

sochan@gmail.com

Ilias Diakonikolas

University of Edinburgh
ilias.d@ed.ac.uk

Rocco A. Servedio
Columbia University

rocco@cs.columbia.edu

Xiaorui Sun

Columbia University

xiaoruisun@cs.columbia.edu

Abstract

Let p be an unknown and arbitrary probability distribution over [0  1). We con-
sider the problem of density estimation  in which a learning algorithm is given
i.i.d. draws from p and must (with high probability) output a hypothesis distri-
bution that is close to p. The main contribution of this paper is a highly efﬁcient
density estimation algorithm for learning using a variable-width histogram  i.e.  a
hypothesis distribution with a piecewise constant probability density function.
In more detail  for any k and "  we give an algorithm that makes ˜O(k/"2) draws
from p  runs in ˜O(k/"2) time  and outputs a hypothesis distribution h that is piece-
wise constant with O(k log2(1/")) pieces. With high probability the hypothesis
h satisﬁes dTV(p  h)  C · optk(p) + "  where dTV denotes the total variation
distance (statistical distance)  C is a universal constant  and optk(p) is the small-
est total variation distance between p and any k-piecewise constant distribution.
The sample size and running time of our algorithm are optimal up to logarithmic
factors. The “approximation factor” C in our result is inherent in the problem 
as we prove that no algorithm with sample size bounded in terms of k and " can
achieve C < 2 regardless of what kind of hypothesis distribution it uses.

1

Introduction

Consider the following fundamental statistical task: Given independent draws from an unknown
probability distribution  what is the minimum sample size needed to obtain an accurate estimate of
the distribution? This is the question of density estimation  a classical problem in statistics with a
rich history and an extensive literature (see e.g.  [BBBB72  DG85  Sil86  Sco92  DL01]). While this
broad question has mostly been studied from an information–theoretic perspective  it is an inherently
algorithmic question as well  since the ultimate goal is to describe and understand algorithms that are
both computationally and information-theoretically efﬁcient. The need for computationally efﬁcient
learning algorithms is only becoming more acute with the recent ﬂood of data across the sciences;
the “gold standard” in this “big data” context is an algorithm with information-theoretically (near-)
optimal sample size and running time (near-) linear in its sample size.
In this paper we consider learning scenarios in which an algorithm is given an input data set which
is a sample of i.i.d. draws from an unknown probability distribution. It is natural to expect (and can
be easily formalized) that  if the underlying distribution of the data is inherently “complex”  it may
be hard to even approximately reconstruct the distribution. But what if the underlying distribution
is “simple” or “succinct” – can we then reconstruct the distribution to high accuracy in a computa-
tionally and sample-efﬁcient way? In this paper we answer this question in the afﬁrmative for the

1

problem of learning “noisy” histograms  arguably one of the most basic density estimation problems
in the literature.
To motivate our results  we begin by brieﬂy recalling the role of histograms in density estimation.
Histograms constitute “the oldest and most widely used method for density estimation” [Sil86]  ﬁrst
introduced by Karl Pearson in [Pea95]. Given a sample from a probability density function (pdf)
p  the method partitions the domain into a number of intervals (bins) B1  . . .   Bk  and outputs the
“empirical” pdf which is constant within each bin. A k-histogram is a piecewise constant distribution
over bins B1  . . .   Bk  where the probability mass of each interval Bj  j 2 [k]  equals the fraction of
observations in the interval. Thus  the goal of the “histogram method” is to approximate an unknown
pdf p by an appropriate k-histogram. It should be emphasized that the number k of bins to be used
and the “width” and location of each bin are unspeciﬁed; they are parameters of the estimation
problem and are typically selected in an ad hoc manner.
We study the following distribution learning question:

Suppose that there exists a k-histogram that provides an accurate approximation
to the unknown target distribution. Can we efﬁciently ﬁnd such an approximation?

In this paper  we provide a fairly complete afﬁrmative answer to this basic question. Given a bound
k on the number of intervals  we give an algorithm that uses a near-optimal sample size  runs in
near-linear time (in its sample size)  and approximates the target distribution nearly as accurately as
the best k-histogram.
To formally state our main result  we will need a few deﬁnitions. We work in a standard model of
learning an unknown probability distribution from samples  essentially that of [KMR+94]  which
is a natural analogue of Valiant’s well-known PAC model for learning Boolean functions [Val84] to
the unsupervised setting of learning an unknown probability distribution.1 A distribution learning
problem is deﬁned by a class C of distributions over a domain ⌦. The algorithm has access to
independent draws from an unknown pdf p  and its goal is to output a hypothesis distribution h
that is “close” to the target distribution p. We measure the closeness between distributions using
the statistical distance or total variation distance. In the “noiseless” setting  we are promised that
p 2 C and the goal is to construct a hypothesis h such that (with high probability) the total variation
distance dTV(h  p) between h and p is at most "  where " > 0 is the accuracy parameter.
The more challenging “noisy” or agnostic model captures the situation of having arbitrary (or even
adversarial) noise in the data. In this setting  we do not make any assumptions about the target den-
sity p and the goal is to ﬁnd a hypothesis h that is almost as accurate as the “best” approximation of p
by any distribution in C. Formally  given sample access to a (potentially arbitrary) target distribution
p and " > 0  the goal of an agnostic learning algorithm for C is to compute a hypothesis distribution
h such that dTV(h  p)  ↵ · optC(p) + "  where optC(p) := inf q2C dTV(q  p) – i.e.  optC(p) is
the statistical distance between p and the closest distribution to it in C – and ↵  1 is a constant
(that may depend on the class C). We will call such a learning algorithm an ↵-agnostic learning
algorithm for C; when ↵ > 1 we sometimes refer to this as a semi-agnostic learning algorithm.
A distribution f over a ﬁnite interval I ✓ R is called k-ﬂat if there exists a partition of I into k
intervals I1  . . .   Ik such that the pdf f is constant within each such interval. We henceforth (without
loss of generality for densities with bounded support) restrict ourselves to the case I = [0  1). Let
Ck be the class of all k-ﬂat distributions over [0  1). For a (potentially arbitrary) distribution p over
[0  1) we will denote by optk(p) := inf f2Ck dTV(f  p).
In this terminology  our learning problem is exactly the problem of agnostically learning the class
of k-ﬂat distributions. Our main positive result is a near-optimal algorithm for this problem  i.e. 
a semi-agnostic learning algorithm that has near-optimal sample size and near-linear running time.
More precisely  we prove the following:
Theorem 1 (Main). There is an algorithm A with the following property: Given k  1  " > 0 
and sample access to a target distribution p  algorithm A uses ˜O(k/"2) independent draws from
p  runs in time ˜O(k/"2)  and outputs a O(k log2(1/"))-ﬂat hypothesis distribution h that satisﬁes
dTV(h  p)  O(optk(p)) + " with probability at least 9/10.

1We remark that our model is essentially equivalent to the “minimax rate of convergence under the L1

distance” in statistics [DL01]  and our results carry over to this setting as well.

2

Using standard techniques  the conﬁdence probability can be boosted to 1    for any  > 0  with
a (necessary) overhead of O(log(1/)) in the sample size and the running time.
We emphasize that the difﬁculty of our result lies in the fact that the “optimal” piecewise constant
decomposition of the domain is both unknown and approximate (in the sense that optk(p) > 0);
and that our algorithm is both sample-optimal and runs in (near-) linear time. Even in the (signiﬁ-
cantly easier) case that the target p 2 Ck (i.e.  optk(p) = 0)  and the optimal partition is explicitly
given to the algorithm  it is known that a sample of size ⌦(k/"2) is information-theoretically nec-
essary. (This lower bound can  e.g.  be deduced from the standard fact that learning an unknown
discrete distribution over a k-element set to statistical distance " requires an ⌦(k/"2) size sample.)
Hence  our algorithm has provably optimal sample complexity (up to a logarithmic factor)  runs in
essentially sample linear time  and is ↵-agnostic for a universal constant ↵ > 1.
It should be noted that the sample size required for our problem is well-understood; it follows from
the VC theorem (Theorem 3) that O(k/"2) draws from p are information-theoretically sufﬁcient.
However  the theorem is non-constructive  and the “obvious” algorithm following from it has run-
ning time exponential in k and 1/". In recent work  Chan et al [CDSS14] presented an approach
employing an intricate combination of dynamic programming and linear programming which yields
a poly(k/") time algorithm for the above problem. However  the running time of the [CDSS14] al-
gorithm is ⌦(k3) even for constant values of "  making it impractical for applications. As discussed
below our algorithmic approach is signiﬁcantly different from that of
[CDSS14]  using neither
dynamic nor linear programming.
Applications. Nonparametric density estimation for shape restricted classes has been a subject
of study in statistics since the 1950’s (see [BBBB72] for an early book on the topic and [Gre56 
Bru58  Rao69  Weg70  HP76  Gro85  Bir87] for some of the early literature)  and has applications
to a range of areas including reliability theory (see [Reb05] and references therein). By using the
structural approximation results of Chan et al [CDSS13]  as an immediate corollary of Theorem 1
we obtain sample optimal and near-linear time estimators for various well-studied classes of shape
restricted densities including monotone  unimodal  and multimodal densities (with unknown mode
locations)  monotone hazard rate (MHR) distributions  and others (because of space constraints we
do not enumerate the exact descriptions of these classes or statements of these results here  but
instead refer the interested reader to [CDSS13]). Birg´e [Bir87] obtained a sample optimal and linear
time estimator for monotone densities  but prior to our work  no linear time and sample optimal
estimator was known for any of the other classes.
Our algorithm from Theorem 1 is ↵-agnostic for a constant ↵ > 1. It is natural to ask whether a
signiﬁcantly stronger accuracy guarantee is efﬁciently achievable; in particular  is there an agnostic
algorithm with similar running time and sample complexity and ↵ = 1? Perhaps surprisingly  we
provide a negative answer to this question. Even in the simplest nontrivial case that k = 2  and the
target distribution is deﬁned over a discrete domain [N ] = {1  . . .   N}  any ↵-agnostic algorithm
with ↵ < 2 requires large sample size:

Theorem 2 (Lower bound  Informal statement). Any 1.99-agnostic learning algorithm for 2-ﬂat
distributions over [N ] requires a sample of size ⌦(pN ).

See Theorem 7 in Section 4 for a precise statement. Note that there is an exact correspondence be-
tween distributions over the discrete domain [N ] and pdf’s over [0  1) which are piecewise constant
on each interval of the form [k/N  (k + 1)/N ) for k 2 {0  1  . . .   N  1}. Thus  Theorem 2 implies
that no ﬁnite sample algorithm can 1.99-agnostically learn even 2-ﬂat distributions over [0  1). (See
Corollary 4.1 in Section 4 for a detailed statement.)
Related work. A number of techniques for density estimation have been developed in the mathemat-
ical statistics literature  including kernels and variants thereof  nearest neighbor estimators  orthog-
onal series estimators  maximum likelihood estimators (MLE)  and others (see Chapter 2 of [Sil86]
for a survey of existing methods). The main focus of these methods has been on the statistical rate
of convergence  as opposed to the running time of the corresponding estimators. We remark that
the MLE does not exist for very simple classes of distributions (e.g.  unimodal distributions with
an unknown mode  see e.g  [Bir97]). We note that the notion of agnostic learning is related to the
literature on model selection and oracle inequalities [MP007]  however this work is of a different
ﬂavor and is not technically related to our results.

3

Histograms have also been studied extensively in various areas of computer science  including
databases and streaming [JKM+98  GKS06  CMN98  GGI+02] under various assumptions about
the input data and the precise objective. Recently  Indyk et al [ILR12] studied the problem of learn-
ing a k-ﬂat distribution over [N ] under the L2 norm and gave an efﬁcient algorithm with sample
complexity O(k2 log(N )/"4). Since the L1 distance is a stronger metric  Theorem 1 implies an
improved sample and time bound of ˜O(k/"2) for their setting.

2 Preliminaries

Throughout the paper we assume that the underlying distributions have Lebesgue measurable den-
sities. For a pdf p : [0  1) ! R+ and a Lebesgue measurable subset A ✓ [0  1)  i.e.  A 2 L([0  1)) 
we use p(A) to denoteRz2A p(z). The statistical distance or total variation distance between two
densities p  q : [0  1) ! R+ is dTV(p  q) := supA2L([0 1)) |p(A)  q(A)|. The statistical distance
2kp  qk1 where kp  qk1  the L1 distance between p and q 
satisﬁes the identity dTV(p  q) = 1
isR[0 1) |p(x)  q(x)|dx; for convenience in the rest of the paper we work with L1 distance. We
refer to a nonnegative function p over an interval (which need not necessarily integrate to one over
the interval) as a “sub-distribution.” Given a value  > 0  we say that a (sub-)distribution p over
[0  1) is -well-behaved if supx2[0 1) Prx⇠p[x]    i.e.  no individual real value is assigned more
than  probability under p. Any probability distribution with no atoms is -well-behaved for all
 > 0. Our results apply for general distributions over [0  1) which may have an atomic part as well
as a non-atomic part. Given m independent draws s1  . . .   sm from a distribution p over [0  1)  the

empirical distributionbpm over [0  1) is the discrete distribution supported on {s1  . . .   sm} deﬁned
as follows: for all z 2 [0  1)  Prx⇠bpm[x = z] = |{j 2 [m] | sj = z}|/m.
The VC inequality. Let p : [0  1) ! R be a Lebesgue measurable function. Given a family of
subsets A ✓ L([0  1)) over [0  1)  deﬁne kpkA = supA2A |p(A)|. The VC dimension of A is
the maximum size of a subset X ✓ [0  1) that is shattered by A (a set X is shattered by A if for
every Y ✓ X  some A 2 A satisﬁes A \ X = Y ). If there is a shattered subset of size s for all
s 2 +  then we say that the VC dimension of A is 1. The well-known Vapnik-Chervonenkis (VC)
inequality states the following:
Theorem 3 (VC inequality  [DL01  p.31]). Let p : I ! R+ be a probability density function over
I ✓ R andbpm be the empirical distribution obtained after drawing m points from p. Let A ✓ 2I be
a family of subsets with VC dimension d. Then E[kp bpmkA]  O(pd/m).

Partitioning into intervals of approximately equal mass. As a basic primitive  given access to
a sample drawn from a -well-behaved target distribution p over [0  1)  we will need to partition
[0  1) into ⇥(1/) intervals each of which has probability ⇥() under p. There is a simple algo-
rithm  based on order statistics  which does this and has the following performance guarantee (see
Appendix A.2 of [CDSS14]):
Lemma 2.1. Given  2 (0  1) and access to points drawn from a /64-well-behaved distribution
p over [0  1)  the procedure Approximately-Equal-Partition draws O((1/) log(1/))
points from p  runs in time ˜O(1/)  and with probability at least 99/100 outputs a partition of [0  1)
into ` = ⇥(1/) intervals such that p(Ij) 2 [/2  3] for all 1  j  `.

3 The algorithm and its analysis

In this section we prove our main algorithmic result  Theorem 1. Our approach has the following
high-level structure: In Section 3.1 we give an algorithm for agnostically learning a target distri-
bution p that is “nice” in two senses: (i) p is well-behaved (i.e.  it does not have any heavy atomic
elements)  and (ii) optk(p) is bounded from above by the error parameter ". In Section 3.2 we give a
general efﬁcient reduction showing how the second assumption can be removed  and in Section 3.3
we brieﬂy explain how the ﬁrst assumption can be removed  thus yielding Theorem 1.

4

3.1 The main algorithm

In this section we give our main algorithmic result  which handles well-behaved distributions p for
which optk(p) is not too large:
Theorem 4. There is an algorithm Learn-WB-small-opt-k-histogram that given as input
˜O(k/"2) i.i.d. draws from a target distribution p and a parameter " > 0  runs in time ˜O(k/"2)  and
has the following performance guarantee: If (i) p is "/ log(1/")
-well-behaved  and (ii) optk(p)  " 
then with probability at least 19/20  it outputs an O(k · log2(1/"))-ﬂat distribution h such that
dTV(p  h)  2 · optk(p) + 3".
We require some notation and terminology. Let r be a distribution over [0  1)  and let P be a set of
disjoint intervals that are contained in [0  1). We say that the P-ﬂattening of r  denoted (r)P  is the
sub-distribution deﬁned as

384k

r(v) =⇢ r(I)/|I|

0

if v 2 I  I 2 P
if v does not belong to any I 2 P

Observe that if P is a partition of [0  1)  then (since r is a distribution) (r)P is a distribution.
We say that two intervals I  I0 are consecutive if I = [a  b) and I0 = [b  c). Given two consecutive
intervals I  I0 contained in [0  1) and a sub-distribution r  we use ↵r(I  I0) to denote the L1 distance
between (r){I I0} and (r){I[I0}  i.e.  ↵r(I  I0) =RI[I0 |(r){I I0}(x)  (r){I[I0}(x)|dx. Note here
that {I [ I0} is a set that contains one element  the interval [a  c).
3.1.1 Intuition for the algorithm
We begin with a high-level intuitive explanation of the Learn-WB-small-opt-k-histogram
algorithm.
It starts in Step 1 by constructing a partition of [0  1) into z = ⇥(k/"0) intervals
I1  . . .   Iz (where "0 = ˜⇥(")) such that p has weight ⇥("0/k) on each subinterval. In Step 2 the
algorithm draws a sample of ˜O(k/"2) points from p and uses them to deﬁne an empirical distri-

bution bpm. This is the only step in which points are drawn from p. For the rest of this intuitive
explanation we pretend that the weightbp(I) that the empirical distributionbpm assigns to each inter-

val I is actually the same as the true weight p(I) (Lemma 3.1 below shows that this is not too far
from the truth).
Before continuing with our explanation of the algorithm  let us digress brieﬂy by imagining for a
moment that the target distribution p actually is a k-ﬂat distribution (i.e.  that optk(p) = 0). In this

given these it is not difﬁcult to construct a high-accuracy hypothesis).

case there are at most k “breakpoints”  and hence at most k intervals Ij for which ↵bpm(Ij  Ij+1) > 0 
so computing the ↵bpm(Ij  Ij+1) values would be an easy way to identify the true breakpoints (and
In reality  we may of course have optk(p) > 0; this means that if we try to use the ↵bpm(Ij  Ij+1)
criterion to identify “breakpoints” of the optimal k-ﬂat distribution that is closest to p (call this k-ﬂat
distribution q)  we may sometimes be “fooled” into thinking that q has a breakpoint in an interval
Ij where it does not (but rather the value ↵bpm(Ij  Ij+1) is large because of the difference between
q and p). However  recall that by assumption we have optk(p)  "; this bound can be used to
show that there cannot be too many intervals Ij for which a large value of ↵bpm(Ij  Ij+1) suggests
a “spurious breakpoint” (see the proof of Lemma 3.3). This is helpful  but in and of itself not
enough; since our partition I1  . . .   Iz divides [0  1) into k/"0 intervals  a naive approach based on
this would result in a (k/"0)-ﬂat hypothesis distribution  which in turn would necessitate a sample
complexity of ˜O(k/"03)  which is unacceptably high.
Instead  our algorithm performs a careful

process of iteratively merging consecutive intervals for which the ↵bpm(Ij  Ij+1) criterion indicates
that a merge will not adversely affect the ﬁnal accuracy by too much. As a result of this process
we end up with k · polylog(1/") intervals for the ﬁnal hypothesis  which enables us to output a
(k · polylog(1/"0))-ﬂat ﬁnal hypothesis using ˜O(k/"02) draws from p.
In more detail  this iterative merging is carried out by the main loop of the algorithm in Step 4.
Going into the t-th iteration of the loop  the algorithm has a partition Pt1 of [0  1) into disjoint
sub-intervals  and a set Ft1 ✓ Pt1 (i.e.  every interval belonging to Ft1 also belongs to Pt1).
Initially P0 contains all the intervals I1  . . .   Iz and F0 is empty. Intuitively  the intervals in Pt1 \

5

Ft1 are still being “processed”; such an interval may possibly be merged with a consecutive interval
from Pt1 \ Ft1 if doing so would only incur a small “cost” (see condition (iii) of Step 4(b) of the
algorithm).The intervals in Ft1 have been “frozen” and will not be altered or used subsequently in
the algorithm.

3.1.2 The algorithm

Algorithm Learn-WB-small-opt-k-histogram:
Input: parameters k  1  " > 0; access to i.i.d. draws from target distribution p over [0  1)
Output: If (i) p is "/ log(1/")
99/100 the output is a distribution q such that dTV(p  q)  2optk(p) + 3".

-well-behaved and (ii) optk(p)  "  then with probability at least

384k

1. Let "0 = "/ log(1/"). Run Algorithm Approximately-Equal-Partition on
input parameter "0
6k to partition [0  1) into z = ⇥(k/"0) intervals I1 = [i0  i1)  . . .  
Iz = [iz1  iz)  where i0 = 0 and iz = 1 
such that with probability at least
99/100  for each j 2 {1  . . .   z} we have p([ij1  ij)) 2 ["0/12k  "0/2k] (assuming p
is "0/(384k)-well-behaved).

2. Draw m = ˜O(k/"02) points from p and letbpm be the resulting empirical distribution.

3. Set P0 = {I1  I2  . . . Iz}  and F0 = ;.
4. Let s = log2

"0 . Repeat for t = 1  . . . until t = s:

1

(c) Initialize i to 1  and repeatedly execute one of the following four (mutually ex-

(a) Initialize Pt to ; and Ft to Ft1.
(b) Without loss of generality  assume Pt1 = {It1 1  . . .   It1 zt1} where inter-
val It1 i is to the left of It1 i+1 for all i. Scan left to right across the intervals
in Pt1 (i.e.  iterate over i = 1  . . .   zt1  1). If intervals It1 i  It1 i+1 are (i)
both not in Ft1  and (ii) ↵bpm(It1 i  It1 i+1) > "0/(2k)  then add both It1 i
and It1 i+1 into Ft.
clusive and exhaustive) cases until i > zt1:
[Case 1] i  zt1  1 and It1 i = [a  b)  It1 i+1 = [b  c) are consecutive
intervals both not in Ft. Add the merged interval It1 i [ It1 i+1 = [a  c) into
Pt. Set i i + 2.
[Case 2] i  zt1  1 and It1 i 2 Ft. Set i i + 1.
[Case 3] i  zt1  1  It1 i /2 Ft and It1 i+1 2 Ft. Add It1 i into Ft and
set i i + 2.
[Case 4] i = zt1. Add It1 zt1 into Ft if It1 zt1 is not in Ft and set i 
i + 1.

(d) Set Pt Pt [ Ft.

5. Output the |Ps|-ﬂat hypothesis distribution (bpm)Ps.

3.1.3 Analysis of the algorithm and proof of Theorem 4
It is straightforward to verify the claimed running time given Lemma 2.1  which bounds the running
time of Approximately-Equal-Partition.
Indeed  we note that Step 2  which simply
draws ˜O(k/"02) points and constructs the resulting empirical distribution  dominates the overall
running time. In the rest of this subsubsection we prove correctness.

high-accuracy estimate of the true probability of any union of consecutive intervals from I1  . . .   Iz.
The following lemma from [CDSS14] follows from the standard multiplicative Chernoff bound:
Lemma 3.1 (Lemma 12  [CDSS14]). With probability 99/100 over the sample drawn in Step 2  for

We ﬁrst observe that with high probability the empirical distributionbpm deﬁned in Step 2 gives a
every 0  a < b  z we have that |bpm([ia  ib))  p([ia  ib))| p"0(b  a) · "0/(10k).
for all 0  a < b  z. We use this to show that the ↵bpm(It1 i  It1 i+1) value that the algorithm

We henceforth assume that this 99/100-likely event indeed takes place  so the above inequality holds

6

uses in Step 4(b) is a good proxy for the actual value ↵p(It1 i  It1 i+1) (which of course is not
accessible to the algorithm):
Lemma 3.2. Fix 1  t  s. Then we have |↵bpm(It1 i  It1 i+1)  ↵p(It1 i  It1 i+1)| 

2"0/(5k).

Due to space constraints the proofs of all lemmas in this section are deferred to Appendix A.
For the rest of the analysis  let q denote a ﬁxed k-ﬂat distribution that is closest to p  so kp  qk1 =
optk(p). (We note that while optk(p) is deﬁned as inf q2C kp  qk1  standard closure arguments
can be used to show that the inﬁmum is actually achieved by some k-ﬂat distribution q.) Let Q be
the partition of [0  1) corresponding to the intervals on which q is piecewise constant. We say that a
breakpoint of Q is a value in [0  1] that is an endpoint of one of the (at most) k intervals in Q.
The following important lemma bounds the number of intervals in the ﬁnal partition Ps:
Lemma 3.3. Ps contains at most O(k log2(1/")) intervals.
The following deﬁnition will be useful:
Deﬁnition 5. Let P denote any partition of [0  1). We say that partition P is "0-good for (p  q) if for
every breakpoint v of Q  the interval I in P containing v satisﬁes p(I)  "0/(2k).
The above deﬁnition is justiﬁed by the following lemma:
Lemma 3.4. If P is "0-good for (p  q)  then kp  (p)Pk1  2optk(p) + "0.
We are now in a position to prove the following:
Lemma 3.5. There exists a partition R of [0  1) that is "0-good for (p  q) and satisﬁes

k(p)Ps  (p)Rk1  ".

We construct the claimed R based on Ps Ps1  . . .  P0 as follows: (i) If I is an interval in Ps not
containing a breakpoint of Q  then I is also in R; (ii) If I is an interval in Ps that does contain a
breakpoint of Q  then we further partition I into a set of intervals S in a recursive manner using
Ps1  . . .  P0 (see Appendix A.4). Finally  by putting everything together we can prove Theorem 4:
Proof of Theorem 4. By Lemma 3.4 applied to R  we have that kp  (p)Rk1  2optk(p) + "0. By
Lemma 3.5  we have that k(p)Ps(p)Rk1  "; thus the triangle inequality gives that kp(p)Psk1 
2optk(p) + 2". By Lemma 3.3 the partition Ps contains at most O(k log2(1/")) intervals  so both

of unions of up to ` intervals (which has VC dimension 2`). Consequently by the VC inequality

(p)Ps and (bpm)Ps are O(k log2(1/"))-ﬂat distributions. Thus  k(p)Ps  (bpm)Psk1 = k(p)Ps 
(bpm)PskA`  where ` = O(k log2(1/")) and A` is the family of all subsets of [0  1) that consist
(Theorem 3  for a suitable choice of m = ˜O(k/"02)  we have that E[k(p)Ps(bpm)Psk1]  4"0/100.
Markov’s inequality now gives that with probability at least 96/100  we have k(p)Ps  (bpm)Psk1 
Lemma 3.1)  we have that kp  (bpm)Psk1  2optk(p) + 3"  and the theorem is proved.

"0. Hence  with overall probability at least 19/20 (recall the 1/100 error probability incurred in

3.2 A general reduction to the case of small opt for semi-agnostic learning

In this section we show that under mild conditions  the general problem of agnostic distribution
is not too large
learning for a class C can be efﬁciently reduced to the special case when optC
compared with ". While the reduction is simple and generic  we have not previously encountered it
in the literature on density estimation  so we provide a proof in Appendix A.5. A precise statement
of the reduction follows:
Theorem 6. Let A be an algorithm with the following behavior: A is given as input i.i.d. points
drawn from p and a parameter " > 0. A uses m(") = ⌦(1/") draws from p  runs in time t(") =
⌦(1/")  and satisﬁes the following: if optC(p)  10"  then with probability at least 19/20 it outputs
a hypothesis distribution q such that (i) kp qk1  ↵· optC(p) + "  where ↵ is an absolute constant 
and (ii) given any r 2 [0  1)  the value q(r) of the pdf of q at r can be efﬁciently computed in T time
steps.

7

Then there is an algorithm A0 with the following performance guarantee: A0 is given as input i.i.d.
draws from p and a parameter " > 0.2 Algorithm A0 uses O(m("/10) + log log(1/")/"2) draws
from p  runs in time O(t("/10)) + T · ˜O(1/"2)  and outputs a hypothesis distribution q0 such that
with probability at least 39/40 we have kp  q0k1  10(↵ + 2) · optC(p) + ".
3.3 Dealing with distributions that are not well behaved

The assumption that the target distribution p is ˜⇥("/k)-well-behaved can be straightforwardly re-
moved by following the approach in Section 3.6 of [CDSS14]. That paper presents a simple linear-
time sampling-based procedure  using ˜O(k/") samples  that with high probability identiﬁes all the
“heavy” elements (atoms which cause p to not be well-behaved  if any such points exist).
Our overall algorithm ﬁrst runs this procedure to ﬁnd the set S of “heavy” elements  and then runs
the algorithm presented above (which succeeds for well-behaved distributions  i.e.  distributions
that have no “heavy” elements) using as its target distribution the conditional distribution of p over
[0  1) \ S (let us denote this conditional distribution by p0). A straightforward analysis given in
[CDSS14] shows that (i) optk(p)  optk(p0)  and moreover (ii) dTV(p  p0)  optk(p). Thus  by
the triangle inequality  any hypothesis h satisfying dTV(h  p0)  Coptk(p0) + " will also satisfy
dTV(h  p)  (C + 1)optk(p) + " as desired.
4 Lower bounds on agnostic learning

In this section we establish that ↵-agnostic learning with ↵ < 2 is information theoretically impos-
sible  thus establishing Theorem 2.
Fix any 0 < t < 1/2. We deﬁne a probability distribution Dt over a ﬁnite set of discrete distributions
over the domain [2N ] = {1  . . .   2N} as follows. (We assume without loss of generality below that
t is rational and that tN is an integer.) A draw of pS1 S2 t from Dt is obtained as follows.

pS1 S2 t(i) =

1. A set S1 ⇢ [N ] is chosen uniformly at random from all subsets of [N ] that contain precisely
tN elements. For i 2 [N ]  the distribution pS1 S2 t assigns probability weight as follows:
2(1  t)◆ if i 2 [N ] \ S1.
2. A set S2 ⇢ [N + 1  . . .   2N ] is chosen uniformly at random from all subsets of [N +
1  . . .   2N ] that contain precisely tN elements. For i 2 [N + 1  . . .   2N ]  the distribution
pS1 S2 t assigns probability weight as follows:
1

2N ✓1 +

if i 2 S1 

pS1 S2 t(i) =

1
4N

1

t

2N ✓1 

t

2(1  t)◆ if i 2 [N ] \ S1.

pS1 S2 t(i) =

3
4N

if i 2 S2 

Using a birthday paradox type argument  we show that no o(pN )-sample algorithm can successfully
distinguish between a distribution pS1 S2 t ⇠ Dt and the uniform distribution over [2N ]. We then
leverage this indistinguishability to show that any (2  )-semi-agnostic learning algorithm  even
for 2-ﬂat distributions  must use a sample of size ⌦(pN ) (see Appendix B for these proofs):
Theorem 7. Fix any  > 0 and any function f (·). There is no algorithm A with the following
property: given " > 0 and access to independent points drawn from an unknown distribution p over
[2N ]  algorithm A makes o(pN ) · f (") draws from p and with probability at least 51/100 outputs
a hypothesis distribution h over [2N ] satisfying kh  pk1  (2  )opt2(p) + ".
As described in the Introduction  via the obvious correspondence that maps distributions over [N ]
to distributions over [0  1)  we get the following:
Corollary 4.1. Fix any  > 0 and any function f (·). There is no algorithm A with the following
property: given " > 0 and access to independent draws from an unknown distribution p over [0  1) 
algorithm A makes f (") draws from p and with probability at least 51/100 outputs a hypothesis
distribution h over [0  1) satisfying kh  pk1  (2  )opt2(p) + ".

2 Note that now there is no guarantee that optC(p)  "; indeed  the point here is that optC(p) may be

arbitrary.

8

References
[AJOS14]

J. Acharya  A. Jafarpour  A. Orlitsky  and A.T. Suresh. Near-optimal-sample estimators for spher-
ical gaussian mixtures. Technical Report http://arxiv.org/abs/1402.4746  19 Feb 2014. A.5

[BBBB72] R.E. Barlow  D.J. Bartholomew  J.M. Bremner  and H.D. Brunk. Statistical Inference under Order

[Bir87]

[Bir97]

[Bru58]

Restrictions. Wiley  New York  1972. 1  1
L. Birg´e. Estimating a density under order restrictions: Nonasymptotic minimax risk. Annals of
Statistics  15(3):995–1012  1987. 1
L. Birg´e. Estimation of unimodal densities without smoothness assumptions. Annals of Statistics 
25(3):970–981  1997. 1
H. D. Brunk. On the estimation of parameters restricted by inequalities. Ann. Math. Statist. 
29(2):pp. 437–454  1958. 1

[CDSS13] S. Chan  I. Diakonikolas  R. Servedio  and X. Sun. Learning mixtures of structured distributions

over discrete domains. In SODA  pages 1380–1394  2013. 1

[DDS12]

[CMN98]

[CDSS14] S. Chan  I. Diakonikolas  R. Servedio  and X. Sun. Efﬁcient density estimation via piecewise
polynomial approximation. Technical Report http://arxiv.org/abs/1305.3207  conference version
in STOC  pages 604-613  2014. 1  2  3.1.3  3.1  3.3  A.2
S. Chaudhuri  R. Motwani  and V. Narasayya. Random sampling for histogram construction: How
much is enough? In SIGMOD Conference  pages 436–447  1998. 1
A. De  I. Diakonikolas  and R. Servedio. Inverse problems in approximate uniform generation.
Available at http://arxiv.org/pdf/1211.1722v1.pdf  2012. A.5
L. Devroye and L. Gy¨orﬁ. Nonparametric Density Estimation: The L1 View. John Wiley & Sons 
1985. 1
C. Daskalakis and G. Kamath. Faster and sample near-optimal algorithms for proper learning
mixtures of gaussians. In COLT  pages 1183–1213  2014. A.5
L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer Series in
Statistics  Springer  2001. 1  1  3  A.5

[DG85]

[DK14]

[DL01]

[GGI+02] A. Gilbert  S. Guha  P. Indyk  Y. Kotidis  S. Muthukrishnan  and M. Strauss. Fast  small-space

algorithms for approximate histogram maintenance. In STOC  pages 389–398  2002. 1
S. Guha  N. Koudas  and K. Shim. Approximation and streaming algorithms for histogram con-
struction problems. ACM Trans. Database Syst.  31(1):396–438  2006. 1
U. Grenander. On the theory of mortality measurement. Skand. Aktuarietidskr.  39:125–153  1956.
1
P. Groeneboom. Estimating a monotone density. In Proc. of the Berkeley Conference in Honor of
Jerzy Neyman and Jack Kiefer  pages 539–555  1985. 1
D. L. Hanson and G. Pledger. Consistency in concave regression. The Annals of Statistics  4(6):pp.
1038–1050  1976. 1
P. Indyk  R. Levi  and R. Rubinfeld. Approximating and Testing k-Histogram Distributions in
Sub-linear Time. In PODS  pages 15–22  2012. 1

[GKS06]

[Gre56]

[Gro85]

[HP76]

[ILR12]

[MP007]

[Pea95]

[Rao69]
[Reb05]

[Sco92]

[Sil86]
[Val84]

[Weg70]

[JKM+98] H. V. Jagadish  N. Koudas  S. Muthukrishnan  V. Poosala  K. Sevcik  and T. Suel. Optimal his-

tograms with quality guarantees. In VLDB  pages 275–286  1998. 1

[KMR+94] M. Kearns  Y. Mansour  D. Ron  R. Rubinfeld  R. Schapire  and L. Sellie. On the learnability of

discrete distributions. In Proc. 26th STOC  pages 273–282  1994. 1
Concentration inequalities and model selection. Lecture Notes in Mathematics  33  2003  Saint-
Flour  Cantal  2007. Massart  P. and Picard  J.  Springer. 1
K. Pearson. Contributions to the mathematical theory of evolution. ii. skew variation in homoge-
neous material. Philosophical Trans. of the Royal Society of London  186:343–414  1895. 1
B.L.S. Prakasa Rao. Estimation of a unimodal density. Sankhya Ser. A  31:23–36  1969. 1
L. Reboul. Estimation of a function under shape restrictions. Applications to reliability. Ann.
Statist.  33(3):1330–1356  2005. 1
D.W. Scott. Multivariate Density Estimation: Theory  Practice and Visualization. Wiley  New
York  1992. 1
B. W. Silverman. Density Estimation. Chapman and Hall  London  1986. 1  1
L. G. Valiant. A theory of the learnable. In Proc. 16th Annual ACM Symposium on Theory of
Computing (STOC)  pages 436–445. ACM Press  1984. 1
E.J. Wegman. Maximum likelihood estimation of a unimodal density. I. and II. Ann. Math. Statist. 
41:457–471  2169–2174  1970. 1

9

,Siu On Chan
Ilias Diakonikolas
Rocco Servedio
Xiaorui Sun
Liang Zhang
Guangming Zhu
Lin Mei
Peiyi Shen
Syed Afaq Ali Shah
Mohammed Bennamoun
Yuki Yoshida
Masato Okada