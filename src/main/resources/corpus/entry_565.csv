2019,Approximate Bayesian Inference for a Mechanistic Model of Vesicle Release at a Ribbon Synapse,The inherent noise of neural systems makes it difficult to construct models which accurately capture experimental measurements of their activity. While much research has been done on how to efficiently model neural activity with descriptive models such as linear-nonlinear-models (LN)  Bayesian inference for mechanistic models has received considerably less attention. One reason for this is that these models typically lead to intractable likelihoods and thus make parameter inference difficult.  Here  we develop an approximate Bayesian inference scheme for a fully stochastic  biophysically inspired model of glutamate release at the ribbon synapse  a highly specialized synapse found in different sensory systems. The model translates known structural features of the ribbon synapse into a set of stochastically coupled equations. We approximate the posterior distributions by updating a parametric prior distribution via Bayesian updating rules and show that model parameters can be efficiently estimated for synthetic and experimental data from in vivo two-photon experiments in the zebrafish retina. Also  we find that the model captures complex properties of the synaptic release such as the temporal precision and outperforms a standard GLM. Our framework provides a viable path forward for linking mechanistic models of neural activity to measured data.,Approximate Bayesian Inference for a Mechanistic

Model of Vesicle Release at a Ribbon Synapse

Cornelius Schröder∗

Institute for Ophthalmic Research

University of Tübingen

cornelius.schroeder@uni-tuebingen.de

Ben James∗

School of Life Sciences

University of Sussex
bmjame02@gmail.com

Leon Lagnado

School of Life Sciences

University of Sussex

l.lagnado@sussex.ac.uk

Philipp Berens

Institute for Ophthalmic Research

University of Tübingen

philipp.berens@uni-tuebingen.de

Abstract

The inherent noise of neural systems makes it difﬁcult to construct models which
accurately capture experimental measurements of their activity. While much
research has been done on how to efﬁciently model neural activity with descriptive
models such as linear-nonlinear-models (LN)  Bayesian inference for mechanistic
models has received considerably less attention. One reason for this is that these
models typically lead to intractable likelihoods and thus make parameter inference
difﬁcult. Here  we develop an approximate Bayesian inference scheme for a
fully stochastic  biophysically inspired model of glutamate release at the ribbon
synapse  a highly specialized synapse found in different sensory systems. The
model translates known structural features of the ribbon synapse into a set of
stochastically coupled equations. We approximate the posterior distributions by
updating a parametric prior distribution via Bayesian updating rules and show that
model parameters can be efﬁciently estimated for synthetic and experimental data
from in vivo two-photon experiments in the zebraﬁsh retina. Also  we ﬁnd that the
model captures complex properties of the synaptic release such as the temporal
precision and outperforms a standard GLM. Our framework provides a viable path
forward for linking mechanistic models of neural activity to measured data.

1

Introduction

The activity of sensory neurons is noisy — a central goal of systems neuroscience has therefore been
to devise probabilistic models that allow to model the stimulus-response relationship of such neurons
while capturing their variability [1]. Speciﬁcally  linear-nonlinear (LN) models and their generaliza-
tions have been used extensively to describe neural activity in the retina [2  3]. However  these type
of models cannot yield insights into the mechanistic foundations of the neural computations they aim
to describe  as they do not model their biophysical basis. On the other hand  mechanistic models on
the cellular or subcellular level have been rarely used to model stimulus-response relationships: they
require highly specialized experiments to estimate individual parameters [4  5]  making it difﬁcult to
employ them directly in a stimulus-response model; alternatively  they often result in an intractable
likelihood  making parameter inference challenging [6].

∗Equal contribution. Code available at https://github.com/berenslab/abc-ribbon

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Overview of the model. A. After a linear-non-linear processing stage  the signal is passed
to a biophysically inspired model of a ribbon synapse in which vesicles are released in discrete
events. B. Sketch of a bipolar cell with attached photoreceptors (left) and a high resolution electron
microscopy (EM) image of a ribbon synapse with its vesicle pools. The readily releasable pool is
highlighted in red  the reserve pool is shown in white (EM image adapted from [14]).

Here we make use of recent advances in approximate Bayesian computation (ABC) [6  7  8  9  10  11]
to ﬁt a fully stochastic  biophysically inspired model of vesicle release from the bipolar cell (BC)
axon terminal to functional two-photon imaging data from the zebraﬁsh retina (Fig. 1). It includes a
linear-nonlinear stage to model the stimulus dependency  and a set of stochastically coupled equations
modeling biophysical properties of the BC synapse. At this so-called “ribbon synapse”  a specialized
protein complex  the “ribbon”  acts as a conveyor belt that “tethers” and “loads” vesicles onto active
zones for future release [12  13]. It organizes vesicles into multiple pools: the “docked” (or readily
releasable) pool consists of a number of vesicles located directly above the plasma membrane  while
the “ribboned” pool consists of vesicles attached to the ribbon further from the cell membrane. The
docked vesicles are thus primed for immediate release and can be released simultaneously (so called
multivesicular release  MVR). The ribboned vesicles are held in reserve to reﬁll the docked pool as it
is depleted by exocytosis [14  15]. The transitions of vesicles between those pools can be modeled by
a set of coupled differential equations [16  4]  which we extend to a stochastic treatment. In addition
to photoreceptors and bipolar cells in the retina [17]  ribbon synapses are featured in many other
sensory systems  such as in auditory hair cells and the vestibular system [18].
Thus  our proposed Bayesian framework links stimulus-response modeling to a biophysically inspired 
mechanistic model of the ribbon synapse. This may contribute to a better understanding of sensory
computations across levels of description  with applications in a diverse range of sensory systems.

2 Previous work

Models of neural activity Variants and extensions of LN models have been widely used to model
the activity of retinal neurons [2  19  1  3]. In these descriptive models  the excitatory drive to
a cell is modeled as the convolution of a receptive ﬁeld kernel with the stimulus  followed by a
static nonlinearity. The result of this computation sets the rate of a stochastic spike generator  most
commonly using either a Binomial or Poisson distribution. These basic models have also been used to
approximate BC activity [20]  however they do not explicitly model the dynamics of vesicle release at
the ribbon synapse. Existing mechanistic models of synaptic release often require highly specialized
experiments to estimate parameters [21] or make only indirect inferences based on the spiking activity
of post-synaptic cells [22  23]. In addition  they have not been used to perform system identiﬁcation.
The linear-nonlinear kinetics (LNK) model [24] attempts to address this issue. After an initial LN
stage  the LNK model passes this information into a “kinetics block” consisting of a ﬁrst-order set
of kinetic equations implicitly representing the availability of vesicles. However  the LNK model
treats the states of the pools as rescaled Markov process and cannot easily account for discrete vesicle
release or MVR at the given noise level of single synapses.

2

Table 1: Variables  parameters and distributions of the model.

Variable Description

Parameter Movement
distribution

time stretch of the kernel
non-linearity
correlation of exocytosed vesicles
exocytosed vesicles
vesicles at dock
vesicles ribbon → dock
vesicles on ribbon
vesicles cytoplasm → ribbon

γ

k  h

ρ

pdt

pr

λc

dt
D

r

R

c

Distribution
N (µ  σ2)
N (µ  Σ)
N (µ  σ2)

Beta-Bin

res. Binomial N (µ  σ2)

res. Poisson

Γ

We address these issues by proposing a model that combines LN modeling for system identiﬁcation
with a probabilistic  biophysically inspired model for the ribbon synapse  with the capability to model
discrete  multi-vesicular release. In contrast to classical LN models  the parameters of this model are
readily interpretable as they directly refer to biological processes.

Approximate Bayesian Computation Many mechanistic models in computational neuroscience
only provide means to simulate data and do not yield an explicit likelihood function. Therefore 
their parameters cannot be inferred easily. In such simulator-based models  Bayesian inference can
be performed through techniques known as Approximate Bayesian Computation or likelihood-free
inference [8]. The general inference problem can be deﬁned as follows: given some experimental
data x0 and a mechanistic  simulator-based model p(x|θ) parametrized by θ  we want to approximate
the posterior distribution p(θ|x = x0). The simulator model allows us to generate samples xi given
any parameter θ  but the likelihood function cannot be evaluated. Often  xi is ﬁrst mapped to a
low dimensional space (so called “summary statistics”)  in which a loss function is computed. This
mapping deﬁnes the features the model is trying to capture [10].
There are two main approaches to solve the inference problem: (1) approximate the likelihood
p(x0|θ) and then sample (e.g. via MCMC) to get the posterior [8  10]. In this approach  guided
sampling is often used to generate new samples and either train a neural network or update other
parametric models for the likelihood [8  9]. One disadvantage of this approach is that there is a second
sampling step necessary to obtain the posterior  which can be as time consuming as the inference
of the likelihood. (2) approximate the posterior p(θ|x = x0). In principle  inference via rejection
sampling could be applied  but is often inefﬁcient. Thus  recently proposed methods use parametric
models (like a mixture of Gaussian) to approximate the posterior over several sampling rounds [6].
In our work  we use an ABC method of type (2) with parametric prior distributions and Bayesian
updating rules to approximate the posterior distribution p(θ|x = x0). We show that it efﬁciently
learns the parameters of the proposed release model.

3 Linear Non-Linear Release Model

Our model consists of two main parts (Fig. 1): a linear-nonlinear (LN) stage models the excitatory
drive to the BC and a release (R) stage  models the vesicle pools as dependent random variables (see
Appendix A for pseudocode). Therefore  we refer to the model as LNR-model.

3.1 Linear-Nonlinear stage

The ﬁrst stage of the LNR model is a standard LN model  in which a light stimulus l(t) is convolved
with a receptive ﬁeld wγ to yield the surrogate calcium concentration ca(t) in the synaptic terminal
which is then passed through a static nonlinearity:

ca(t) =

l(t − τ )wγ(τ )dτ .

(cid:90) T

τ =0

3

We assume wγ to be a biphasic kernel in order to model the signal processing performed in the
photoreceptor and the BC [16  25] (Figure 1A  B). A single parameter γ was used to stretch/compress
the kernel on the time axis to estimate the receptive ﬁeld (see Appendix C). An approach to allow for
more ﬂexibility (e.g. using basis function [2]) could in principle be used as well. However  this would
lead to a higher dimensional parameter space  making inference less efﬁcient. We used a sigmoidal
non-linearity to convert the calcium signal to the release probability:

pdt(t) = 1/(cid:0)1 + exp(−k(cat − h))(cid:1) 

(1)

where the parameters for the slope k and the half activation h are inferred from the data. We add a
small positive offset to the non-linearity and renormalized it to allow for spontaneous release.

3.2 Release stage

The second stage of the LNR model consists of a model for the synaptic dynamics based on the
structure of the BC ribbon: we deﬁne variables representing the number of vesicles present in each
pool of the ribbon and the number of vesicles moving between pools per timestep (see Table 1). We
use capital letters to deﬁne the number of vesicles in a speciﬁc pool  and lowercase letters to indicate
the moving vesicles. At each time step  vesicles are ﬁrst released from the dock  then new vesicles
are moved from the ribbon to the dock  and ﬁnally the ribbon is reﬁlled from the cytoplasm. For
simplicity  we assume that only the vesicle release probability is modulated by the excitatory drive;
the docking probabilities and rates of movement to the ribbon are constant over time.

Vesicle Release To model the correlated release of docked vesicles  we use a beta binomial distri-
bution. This is a binomial distribution for which the parameter p is itself a random variable  leading
to correlated events [26]. The release probability pdt is assumed to be the output of the LN stage
according to equation 1. To achieve a correlation ρ for the released vesicles and a release probability
of pdt the parameters for the beta binomial distribution are:

(cid:19)

(cid:18) 1

ρ

(cid:19)

(cid:18) 1

pdt

αt = pdt ·

− 1

and βt = αt ·

− 1

if pdt (cid:54)= 0.

 

Thus  in each time step  we ﬁrst draw the parameter ˜pt for the binomial distribution according to
a beta distribution: ˜pt ∼ Beta(αt  βt) and then sample the number of released vesicles dt from a
binomial distribution with parameters n = Dt−1 (the numbers of vesicles at the dock) and ˜pt:

(cid:40)
(cid:0)Dt−1

0

dt

(cid:1)˜p dt

p(dt|Dt−1) =

t (1 − ˜pt)(Dt−1−dt)

if pdt = 0 
otherwise.

Movement to the dock We assume that rt vesicles located at the ribbon move to the dock in each
time step. Because there is a maximum number of vesicles Dmax that can be docked  such that
rt + Dt−1 ≤ Dmax  we use a restricted binomial distribution to model stochastic vesicle docking:

(cid:1)prt


(cid:0)Rt−1
(cid:80)

0

r (1 − pr)(Rt−1−rt)

(cid:0)Rt−1

(cid:1)prt

rt

rt
rt≥Dmax−Dt

p(rt|Rt−1  Dt) =

r (1 − pr)(Rt−1−rt)

if rt < Dmax − Dt
if rt = Dmax − Dt
otherwise.

The ﬁrst case is the standard binomial distribution with appropriate parameters  the second case
models the assumption that moving more vesicles to the dock than its maximum capacity simply ﬁlls
the dock and assures that the probabilities over all possible events sum up to one.

Movement to the ribbon We assume a large number of vesicles available in the cytoplasm (which
is not explicitly modeled)  such that the number of vesicles ct moving from the cytoplasm to the
ribbon follows a Poisson distribution  again respecting the maximal ribbon capacity Rmax:

e−λ λct
(cid:80)

ct!

0

p(ct|Rt) =

ct≥Rmax−Rt

if ct < Rmax − Rt
if ct = Rmax − Rt
otherwise.

e−λ λct

ct!

4

Figure 2: Overview of the inference method. In each round samples are drawn from the (proposal)
prior (blue)  the model is evaluated and the response is mapped to its summary statistic. From this 
the loss per parameter θ is calculated  the best samples are accepted and used to update the (proposal)
prior via Bayesian updating rules  yielding a posterior (red)  which is the proposal prior for the next
round.

4 Bayesian Inference of Model Parameters

In the previous section  we constructed a fully stochastic model of vesicle release from BCs  including
an explicit mechanistic model of the ribbon synapse  reﬂecting the underlying biological structures.
The maximal capacity of the dock Dmax was set based on the measured data to the largest quantal
event observed in the functional recording (Dmax ≈ 7 − 8). Rmax  the maximal capacity of the
ribbon  was set to an estimate on the maximal number of vesicles at the ribbon in goldﬁsh rod
bipolar cells [27  28]  but decreased to reﬂect the smaller size of cone BCs in zebraﬁsh larva [29]
(Rmax ≈ 50).
Next  we developed an ABC framework for likelihood free inference to infer the remaining model
parameters (Table 1) from functional two-photon recordings. Our method uses parametric prior
distributions which are updated in multiple rounds via Bayesian updating rules to provide a unimodal
approximation to the posterior (Figure 2). Brieﬂy  in each round we ﬁrst draw a parameter vector
θ from the (proposal) prior and evaluate several runs of the model ˆdi for each sampled parameter
vector. Due to the stochasticity of the model  each evaluation returns a different trace  for which a
summary statistic is calculated. This summary statistic reduces the dimensionality of the simulated
trace to a low dimensional vector. Based on this the loss function L(θ) is calculated by comparing it
to the summary statistic of the observed data. The best parameters are used to calculate a posterior 
which is then used as a proposal prior in the next round (Fig. 2  pseudocode in Appendix E).

4.1 Prior distributions and inference

As priors  we used normal distributions for all parameters except for λc (Table 1)  where we used a
gamma distribution (the conjugate prior to the Poisson distribution). Some parameters were bounded
e.g. to the interval [0  1] and their distributions renormalized to effectively truncate the priors.
In each inference round  we used Bayesian updating rules to calculate the posterior distribution [30 
31] based on the j best parameters {θ}. For example  in round n + 1  we updated the hyperparameters
for the multivariate normal distribution of the NL parameters  k and h  as:

µn+1 =

κn

κn + j

µn +

j

¯θ

κn + j
κnj

κn + j

(¯θ − µn)(¯θ − µn)T  

where ¯θ is the mean over the best parameters and S =(cid:80)j

Λn+1 = Λn + S +

i=1(θi − ¯θ)(θi − ¯θ)T the (unnormalized)
covariance of these parameters. The mean is thus updated as a weighted average of the prior mean
and the mean of the best parameters  with weights speciﬁed by κ  which is updated as κn+1 = κn + j.
The posterior degrees of freedom νn+1  which is used to sample the covariance matrix Σ  is the prior
degrees of freedom plus the updating sample size: νn+1 = νn + j. With these updates we end up
in a two step sampling procedure: ﬁrst we draw the covariance Σ(n+1)i for each sample i of round
n + 1 from the inverse-Wishart distribution Inv-Wishart(Λ−1
n+1  νn+1)  and then we draw the samples
from the normal distribution N (µn+1  Σ(n+1)i).

5

Figure 3: Results for synthetic data. A. Simulated traces for the synthetic data and simulations
with the recovered  ﬁtted parameters in response to a binary light stimulus. B. The time course of
the mean and standard deviation of the different one dimensional marginal distributions over several
rounds. Notice the asymmetric distribution for λc. See Appendix Fig. 8 for the two dimensional
marginals. C. Relative count for the different event types  error bars indicating ± std. D. Discrepancy
of the data and ﬁtted traces. The discrepancy is deﬁned as the difference between the weighted
summary statistics of a single data trace and the remaining data (“leave-one-out-procedure”) and
accordingly the difference between the weighted summary statistics of a single ﬁtted trace and the
data. Error bars indicating ± std. E. The kernel of the linear stage. F. The non-linearity. Although its
parameter k is not matched perfectly in (B)  there is almost no difference between the ﬁtted and the
true non-linearities.

The parameters for the univariate normal distributions as well as for the Γ-distribution are similarly
updated in a Bayesian manner (see Appendix D). The number of drawn and accepted parameters
was constant (20 · 103 and 10) except for the ﬁrst round  where the number of drawn parameters was
doubled.

4.2 Summary statistics and loss function

As a summary statistic  on which the discrepancy between different traces is deﬁned  we used (1)
the histogram over the number of vesicles released in each event and (2) the Euclidean distance
between the simulated and measured response trace  convolved with a Gaussian kernel (width:
100 ms  inspired by [32]). The former proved especially useful in early rounds of inference. As
experiments typically consist of multiple repetitions of the same stimulus  we ﬁrst calculated the
summary statistics s(di) for the individual traces di  normalized each entry by the summary statistic
of the data traces and scaled it for its importance. This linear transformation is summarized in a
diagonal weight matrix W . We used the average euclidean distance of these weighted summary
statistics as the loss function L(θ) (see also Appendix E and F). For n data traces di and a batchsize
of m simulations ˆdj per parameter θ  this yields:

(cid:88)

i j

L(θ) =

1
nm

||W s(di) − W s( ˆdj)||2 .

The (weighted) summary statistics can also be used to calculate the variability of the data and compare
it to the summary statistics of the simulated data  giving us an estimate of the discrepancy between
the different traces (e.g. Fig. 3C  Fig. 5C).

4.3 Runtime and complexity

The runtime of the presented ABC method is dominated by the forward simulations of the model 
with a complexity O(n) if n is the number of drawn samples. This complexity is similar to SNPE-B

6

Figure 4: Two-photon imaging of in vivo zebraﬁsh BCs allows for counting glutamatergic vesi-
cles. A. Image of a zebraﬁsh BC expressing the Superfolder-iGluSnFR transgene. Dashed circles
indicate active zones where glutamate is released. B. Experimental glutamate release traces as ∆F/F
of one OFF BC in two trials and extracted events in response to a binary light stimulus. Notice the
high inter-trial variability.

[6]  which in addition requires training of a mixture density network  while we resort to analytic
updating formula. Although for expensive simulations  either strategy is often only a small fraction
of the total run time  our method should be advantageous if the simulation is fast and the posterior
unimodal. This direct estimation of the posterior stands in contrast to SNL [9] or BOLFI [8] where the
inference of the posterior involves a second sampling step via MCMC which can be slow. In addition 
BOLFI [8] uses a Gaussian process with complexity O(n3) in the vanilla version to approximate the
likelihood  which can be prohibitively slow.

5 Results

5.1 Model inference on synthetic data

Next  we tested whether we can successfully infer the parameters of the mechanistic model with the
procedure outlined above. For that  we chose a realistic parameter setting and used the model to
generate data. As the sample size per cell is severely limited in experimental data  we generated only
four traces of 140 seconds each (Fig. 3A). The light stimulus was the same binary noise stimulus that
we used for the experimental recordings.
The inference procedure proved very efﬁcient: most parameters converged quickly to their true
values with reasonable uncertainty (Fig. 3B). Only the slope parameter k of the non-linearity is
underestimated  likely because of the "non-linear" effect of k on the slope of the non-linearity and
the smaller prior mean. The method sets k to a value where a further increase woult not change
signiﬁcantly the output of the model (see also Fig. 3F). After inference  it is difﬁcult to differentiate
between the true and the ﬁtted traces and the histogram over the number of vesicles released in each
time step can be ﬁtted well (Fig. 3C). Indeed  simulations from the model where as similar to the
data as different data trials to one another (Fig. 3D). Our procedure identiﬁed the time scale of the
linear receptive ﬁeld as well as the non-linearity effectively (Fig. 3E and F). We validated the efﬁcacy
of our method also for other choices of parameters (not shown).

5.2 Model inference on BC recordings from zebraﬁsh retina

We acquired two-photon imaging measurements of the glutamate release from BC axon terminals
in the zebraﬁsh retina (n = 2 cells  see Fig. 4). Brieﬂy  linescans (1 x 120 pixels) were recorded at
1 kHz across the terminal of a BC expressing the glutamate reporter Superfolder-iGluSnFr  while
showing a 140 second light stimulus (discrete Gaussian or binary noise) with a frame rate of 10
Hz. For each recording  a region of interest (ROI) was deﬁned and the time series extracted as the
weighted average of the spatial proﬁle. Baseline drift was corrected  the traces were converted to
∆F/F and deconvolution was done with the kinetics of the reporter. Release events were identiﬁed
as local maxima above a user-deﬁned threshold in the imaging trace. The number of vesicles in each
release event was estimated using a mixture of Gaussian model. For more details see [33].
Figure 5A shows the LNR model ﬁtted to four recordings from one OFF BC (total duration of the
recordings: 560 sec). We ﬁnd that model parameters both for the LN-stage as well as the release stage
of the model can be inferred efﬁciently. Posteriors converged quickly (Fig. 5B and Appendix Fig.

7

Figure 5: Results for experimental data. A. Two experimental data traces and simulated traces with
corresponding ﬁtted parameters as well as two predictions from the GLM in response to a binary noise
stimulus. B. Some parameters are more restricted than others by the model. (See Appendix Section I
for all parameters  the two dimensional marginals  and the corresponding kernel and non-linearity) C.
Relative count for the different event types for the data and the models  inset on log-scale (mean±std).
D. Discrepancy as in Fig. 3C. E. Temporal jitter of different event types in response to a binary light
stimulus (mean±std  see Appendix Fig. 7 for the Gaussian noise stimulus). F. Cumulative release in
response to a “calcium step” (see Appendix Fig. 11 for a comparison to experimental data).

9). Interestingly  parameters such as the ribbon-to-dock-transition rate pr  which model not directly
observable properties of the system  also had larger uncertainty estimates. Similar to the synthetic
data  the histogram over the number of vesicles released in each event was matched well overall (Fig.
5C). In contrast to the synthetic data  the discrepancy among data traces was a bit smaller than the
discrepancy between the model ﬁt and the data traces (Fig. 5D). This is likely due to the fact that
some events were missed and the data contained more large amplitude events than predicted by the
model (Fig. 5A  C).
We ﬁnally tested whether the simple model captured two known properties of release events: the
temporal precision of events and the maximal release rates of the system. Interestingly  events with
many released vesicles were temporally more precise for both the data and the ﬁtted model (Fig. 5E 
F). As no summary statistic explicitly measured the temporal precision of different release event types
at this resolution  this can be seen as evidence that our model captures crucial aspects of processing
in BCs. Additionally  when comparing release rates with those recorded from electrically stimulated
cells  we ﬁnd the shape of cumulative vesicle release matches well with previously published results
(Fig. 5F and Appendix J). This indicates that the model also extrapolates well to new stimuli.

5.3 Comparison to a GLM

We compared the prediction performance of the LNR model to a generalized linear model (GLM)
[2]  a commonly used model in neural system identiﬁcation. Besides the stimulus term  it includes a
self-feedback term and assumes Poisson noise (for details see Appendix K). In contrast to the LNR
model  the GLM was not able to capture the MVR that is apparent in the data: The GLM did not
predict events with more than ﬁve vesicles at all  and already events with more than two vesicles
were rare (Fig. 5A C). This results in much larger discrepancies overall compared to the LNR model
(Fig. 5D). The weights of the linear part for the release history captured the suppression of additional
release after a release event partly  but could not model the full dynamics (Appendix Fig. 12C and
Fig. 5A). This shows that supplementing systems identiﬁcation models with biophysical components
can dramatically improve prediction accuracy  not only lead to more interpretable models.

8

6 Discussion

Here we developed a Bayesian inference framework for the LNR model  a probabilistic model of
vesicle release from BCs in the retina  which combines a systems identiﬁcation approach with a mech-
anistic  biophysically inspired component. In contrast to purely statistical models  the parameters of
the LNR model are readily interpretable in terms of properties of the ribbon synapase. Speciﬁcally  we
show that its parameters can be ﬁtted efﬁciently on synthetic data and two-photon imaging measure-
ments. The latter is remarkable  as mechanistic models often require highly specialized experiments
to determine individual parameters. In this proof-of-principle study  we show that the parameters of
the LNR model can be simply inferred from the functional measurements  opening possibilities for
inferring mechanistic models from large-scale imaging experiments e.g. for comparison across cell
types.
We found that the data overall was able to constrain the parameters very well  for both the LN-stage
and the release-stage of the LNR model. Parameters that referred to parts of the model that were not
directly observed in our measurements (such as the transition probability from the ribbon to the dock
pr) were ﬁtted with somewhat higher uncertainty  indicating that a larger range of parameter values
was compatible with the measurements. In addition  the LNR model captured MVR (the inferred
correlation between vesicles is ρ ≈ 0.35)  despite the inherent variability at the level of the single
synapse. In addition  the LNR model captured trends in temporal precision within MVR events as
well as release rates to non-phyisiological stimuli such as electrical stimulation - neither of which
were used during inference.
Our proposed framework for Bayesian inference in the LNR model is comparable to recent likelihood-
free inference methods (e.g. [6  11]). In contrast to those  we do not use a mixture density network
(MDN) to approximate the posterior distribution  but rather parametric distributions and analytic
Bayesian updating rules. In practice  MDNs can lead to unstable behavior for very small or large
weights and sometimes have non-optimal extrapolation properties (but see [34]). Due to its simplicity 
our method is less susceptible to such problems  but provides only a unimodal approximation of the
posterior p(θ|x = x0). For the LNR model  we rarely observe multimodality in the posterior  so our
methods yield a good and very fast approximation to the true posterior.
We combined a biophysically inspired mechanistic model with an efﬁcient likelihood free inference
method. This eases the development of more accurate and interpretable models  without the necessity
of having closed-form likelihoods. At the same time  we could show that our model is able to capture
the variablity inherent to the neural system we studied and that taking biophysical constraints into
account can even dramatically improve prediction accuracy of system identiﬁcation models compared
to standard systems identiﬁcation models. Taken together  the presented methods will allow for
further investigations into more complex systems  gaining mechanistic insight into how neurons cope
with noise.

Acknowledgments

We thank Soﬁe-Helene Seibel for help in the experiments and genetics for this project and for
the BC image in Fig. 4A  Christian Behrens for providing the PR/BC schema in Fig. 1B  Jan
Lause for his detailed feedback on the manuscript. The study was funded by the German Ministry
of Education and Research (BMBF  01GQ1601  01IS18052C and 01IS18039A) and the German
Research Foundation (BE5601/4-1  EXC 2064  project number 390727645). In addition  this project
has received funding from the European Union’s Horizon 2020 research and innovation programme
under the Marie Skłodowska-Curie grant agreement No 674901 and the Wellcome Trust (Investigator
Award 102905/Z/13/Z).

References
[1] Johnatan Aljadeff  Benjamin J. Lansdell  Adrienne L. Fairhall  and David Kleinfeld. Analysis of Neuronal

Spike Trains  Deconstructed. Neuron  91(2):221–259  2016.

[2] Jonathan W. Pillow  Jonathon Shlens  Liam Paninski  Alexander Sher  Alan M. Litke  E. J. Chichilnisky  and
Eero P. Simoncelli. Spatio-temporal correlations and visual signalling in a complete neuronal population.
Nature  454(7207):995–999  2008.

9

[3] Esteban Real  Hiroki Asari  Tim Gollisch  and Markus Meister. Neural Circuit Inference from Function to

Structure. Current Biology  pages 1–10  2017.

[4] Michael A Sikora  Jon Gottesman  and Robert F Miller. A computational model of the ribbon synapse.

Journal of neuroscience methods  145(1-2):47–61  2005.

[5] Tianruo Guo  David Tsai  Siwei Bai  John W Morley  Gregg J Suaning  Nigel H Lovell  and Socrates
Dokos. Understanding the retina: A review of computational models of the retina from the single cell to
the network level. Critical ReviewsTM in Biomedical Engineering  42(5)  2014.

[6] Jan-Matthis Lueckmann  Pedro J Goncalves  Giacomo Bassetto  Kaan Öcal  Marcel Nonnenmacher  and
Jakob H Macke. Flexible statistical inference for mechanistic models of neural dynamics. In Advances in
Neural Information Processing Systems  pages 1289–1299  2017.

[7] Jarno Lintusaari  Michael U Gutmann  Ritabrata Dutta  Samuel Kaski  and Jukka Corander. Fundamentals
and recent developments in approximate bayesian computation. Systematic biology  66(1):e66–e82  2017.

[8] Michael U Gutmann and Jukka Corander. Bayesian optimization for likelihood-free inference of simulator-

based statistical models. The Journal of Machine Learning Research  17(1):4256–4302  2016.

[9] George Papamakarios  David C Sterratt  and Iain Murray. Sequential neural likelihood: Fast likelihood-free

inference with autoregressive ﬂows. arXiv preprint arXiv:1805.07226  2018.

[10] Simon N Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature 

466(7310):1102  2010.

[11] George Papamakarios and Iain Murray. Fast ε-free inference of simulation models with bayesian conditional

density estimation. In Advances in Neural Information Processing Systems  pages 1028–1036  2016.

[12] Peter Sterling and Gary Matthews. Structure and function of ribbon synapses. Trends in neurosciences 

28(1):20–29  2005.

[13] Leon Lagnado and Frank Schmitz. Ribbon synapses and visual processing in the retina. Annual review of

vision science  1:235–262  2015.

[14] Matthew Holt  Anne Cooke  Andreas Neef  and Leon Lagnado. High Mobility of Vesicles Supports

Continuous Exocytosis at a Ribbon Synapse. Current Biology  2004.

[15] Joshua H. Singer  Luisa Lassova  Noga Vardi  and Jeffrey S. Diamond. Coordinated multivesicular release

at a mammalian ribbon synapse. Nature Neuroscience  2004.

[16] Tom Baden  Anton Nikolaev  Federico Esposti  Elena Dreosti  Benjamin Odermatt  and Leon Lagnado. A

Synaptic Mechanism for Temporal Filtering of Visual Signals. PLoS Biology  12(10)  2014.

[17] Tom Baden  Thomas Euler  Matti Weckström  and Leon Lagnado. Spikes and ribbon synapses in early

vision. Trends in neurosciences  36(8):480–488  2013.

[18] L. LoGiudice and G. Matthews. The Role of Ribbons at Sensory Synapses. The Neuroscientist  15(4):380–

391  2009.

[19] Liam Paninski  Jonathan Pillow  and Jeremy Lewi. Statistical models for neural encoding  decoding  and

optimal stimulus design. Progress in brain research  165:493–507  2007.

[20] Katrin Franke  Philipp Berens  Timm Schubert  Matthias Bethge  Thomas Euler  and Tom Baden. Inhibition

decorrelates visual feature representations in the inner retina. Nature  542(7642):439–444  2017.

[21] Michael A. Sikora  Jon Gottesman  and Robert F. Miller. A computational model of the ribbon synapse.

Journal of Neuroscience Methods  145(1-2):47–61  2005.

[22] M. Avissar  A. C. Furman  J. C. Saunders  and T. D. Parsons. Adaptation Reduces Spike-Count Reliability 

But Not Spike-Timing Precision  of Auditory Nerve Responses. Journal of Neuroscience  2007.

[23] A. J. Peterson  D. R. F. Irvine  and P. Heil. A Model of Synaptic Vesicle-Pool Depletion and Replenishment
Can Account for the Interspike Interval Distributions and Nonrenewal Properties of Spontaneous Spike
Trains of Auditory-Nerve Fibers. Journal of Neuroscience  2014.

[24] Yusuf Ozuysal and Stephen A. Baccus. Linking the Computational Structure of Variance Adaptation to

Biophysical Mechanisms. Neuron  73(5):1002–1015  2012.

10

[25] JL Schnapf  BJ Nunn  M Meister  and DA Baylor. Visual transduction in cones of the monkey macaca

fascicularis. The Journal of physiology  427(1):681–713  1990.

[26] Masato Hisakado  Kenji Kitsukawa  and Shintaro Mori. Correlated binomial models and correlation

structures. Journal of Physics A: Mathematical and General  39(50):15365–15378  2006.

[27] Gary Matthews and Paul Fuchs. The diverse roles of ribbon synapses in sensory neurotransmission. Nature

Reviews Neuroscience  11(12):812  2010.

[28] Henrique Von Gersdorff  Eilat Vardi  Gary Matthews  and Peter Sterling. Evidence that vesicles on the

synaptic ribbon of retinal bipolar neurons can be rapidly released. Neuron  1996.

[29] Henrique Von Gersdorff and Gary Mathews. Dynamics of synaptic vesicle fusion and membrane retrieval

in synaptic terminals. Nature  367(6465):735  1994.

[30] Andrew Gelman  Hal S Stern  John B Carlin  David B Dunson  Aki Vehtari  and Donald B Rubin. Bayesian

data analysis. Chapman and Hall/CRC  2013.

[31] Kevin P Murphy. Conjugate bayesian analysis of the gaussian distribution. def  1(2σ2):16  2007.

[32] S. Schreiber  J.M. Fellous  D. Whitmer  P. Tiesinga  and T.J. Sejnowski. A new correlation-based measure

of spike timing reliability. Neurocomputing  52-54:925–931  2003.

[33] Ben James  Léa Darnet  José Moya-Díaz  Soﬁe-Helene Seibel  and Leon Lagnado. An amplitude code

transmits information at a visual synapse. Nature Neuroscience  2019.

[34] David Greenberg  Marcel Nonnenmacher  and Jakob Macke. Automatic posterior transformation for
likelihood-free inference. In Proceedings of the 36th International Conference on Machine Learning 
volume 97 of Proceedings of Machine Learning Research  pages 2404–2414. PMLR  2019.

[35] Jonathan S. Marvin  Benjamin Scholl  Daniel E. Wilson  Kaspar Podgorski  Abbas Kazemipour  Jo-
hannes Alexander Müller  Susanne Schoch  Francisco José Urra Quiroz  Nelson Rebola  Huan Bao 
Justin P. Little  Ariana N. Tkachuk  Edward Cai  Adam W. Hantman  Samuel S.H. Wang  Victor J. DePiero 
Bart G. Borghuis  Edwin R. Chapman  Dirk Dietrich  David A. DiGregorio  David Fitzpatrick  and Loren L.
Looger. Stability  afﬁnity  and chromatic variants of the glutamate sensor iGluSnFR. Nature Methods 
2018.

[36] D. Zenisek  J. A. Steyer  and W. Almers. Transport  capture and exocytosis of single synaptic vesicles at

active zones. Nature  2000.

11

,Cornelius Schröder
Ben James
Leon Lagnado
Philipp Berens