2008,On the Reliability of Clustering Stability in the Large Sample Regime,Clustering stability is an increasingly popular family of methods for performing model selection in data clustering. The basic idea is that the chosen model should be stable under perturbation or resampling of the data. Despite being reasonably effective in practice  these methods are not well understood theoretically  and present some difficulties. In particular  when the data is assumed to be sampled from an underlying distribution  the solutions returned by the clustering algorithm will usually become more and more stable as the sample size increases. This raises a potentially serious practical difficulty with these methods  because it means there might be some hard-to-compute sample size  beyond which clustering stability estimators 'break down' and become unreliable in detecting the most stable model. Namely  all models will be relatively stable  with differences in their stability measures depending mostly on random and meaningless sampling artifacts. In this paper  we provide a set of general sufficient conditions  which ensure the reliability of clustering stability estimators in the large sample regime. In contrast to previous work  which concentrated on specific toy distributions or specific idealized clustering frameworks  here we make no such assumptions. We then exemplify how these conditions apply to several important families of clustering algorithms  such as maximum likelihood clustering  certain types of kernel clustering  and centroid-based clustering with any Bregman divergence. In addition  we explicitly derive the non-trivial asymptotic behavior of these estimators  for any framework satisfying our conditions. This can help us understand what is considered a 'stable' model by these estimators  at least for large enough samples.,On the Reliability of Clustering Stability in the Large

Sample Regime - Supplementary Material

Ohad Shamir† and Naftali Tishby†‡

† School of Computer Science and Engineering
‡ Interdisciplinary Center for Neural Computation

The Hebrew University
Jerusalem 91904  Israel

{ohadsh tishby}@cs.huji.ac.il

A Exact Formulation of the Sufﬁcient Conditions

In this section  we give a mathematically rigorous formulation of the sufﬁcient conditions discussed
in the main paper. For that we will need some additional notation.

it will be convenient

to deﬁne a scaled version of our distance measure

First of all 
dD(Ak(S1)  Ak(S2)) between clusterings. Formally  deﬁne the random variable
D (Ak(S1)  Ak(S2)) := √mdD(Ak(S1)  Ak(S2)) = √m Pr
fˆθ′ i(x)(cid:19)  
dm
where θ  θ′ ∈ Θ are the solutions returned by Ak(S1)  Ak(S2)  and S1  S2 are random samples  each
of size m  drawn i.i.d from the underlying distribution D. The scaling by the square root of the
sample size will allow us to analyze the non-trivial asymptotic behavior of these distance measures 
which without scaling simply converge to zero in probability as m → ∞.
For some ǫ > 0 and a set S ⊆ Rn  let Bǫ(S) be the ǫ-neighborhood of S  namely

x∼D(cid:18)argmax

fˆθ i(x) 6= argmax

i

i

In particular  dm
(Ak(S1)  Ak(S2)  Br/√m(∪i jFθ0 i j)) refers to the mass which switches clusters 
D
and is also inside an r/√m-neighborhood of the limit cluster boundaries (where the boundaries are
deﬁned with respect to fθ0(·)). Once again  when S1  S2 are random samples  we can think of it as
a random variable with respect to drawing and clustering S1  S2.
Conditions. The following conditions shall be assumed to hold:

1. Consistency Condition: ˆθ converges in probability (over drawing and clustering a sample
of size m  m → ∞) to some θ0 ∈ Θ. Furthermore  the association of clusters to indices
{1  . . .   k} is constant in some neighborhood of θ0.
2. Central Limit Condition: √m(ˆθ − θ0) converges in distribution to a multivariate zero

mean Gaussian random variable Z.

1

Bǫ(S) :=(cid:26)x ∈ X : inf

y∈S kx − yk2 ≤ ǫ(cid:27) .

In this paper  when we talk about neighborhoods in general  we will always assume they are uniform
(namely  contain an ǫ-neighborhood for some positive ǫ).
We will also need to deﬁne the following variant of dm
(Ak(S1)  Ak(S2))  where we restrict our-
D
selves to the mass in some subset of Rn. Formally  we deﬁne the restricted distance between two
clusterings  with respect to a set B ∈ Rn  as
D (Ak(S1)  Ak(S2)  B) := √m Pr

fˆθ i(x) 6= argmax

dm

(1)

i

x∼D(cid:0)argmax

i

fˆθ′ i(x) ∧ x ∈ B(cid:1).

3. Regularity Conditions:

(a) fθ(x) is Sufﬁciently Smooth: For any θ in some neighborhood of θ0  and any x in
some neighborhood of the cluster boundaries ∪i jFθ0 i j  fθ(x) is twice continuously
differentiable with respect to θ  with a non-zero ﬁrst derivative and uniformly bounded
second derivative for any x. Both fθ0 (x) and (∂/∂θ)fθ0 (x) are twice differentiable
with respect to any x ∈ X   with a uniformly bounded second derivative.
(b) Limit Cluster Boundaries are Reasonably Nice: For any two clusters i  j  Fθ0 i j is
either empty  or a compact  non-self-intersecting  orientable n− 1 dimensional hyper-
surface in Rn with ﬁnite positive volume  a boundary (edge)  and with a neighborhood
contained in X in which the underlying density function p(·) is continuous. Moreover 
the gradient ∇(fθ0 i(·) − fθ0 j(·)) has positive magnitude everywhere on Fθ0 i j.
(c) Intersections of Cluster Boundaries are Relatively Negligible: For any two distinct
non-empty cluster boundaries Fθ0 i j  Fθ0 i′ j′  we have that

1

1

1dx  

1dx

(d) Minimal Parametric Stability: It holds for some δ > 0 that

ǫ ZBǫ(Fθ0 i j∪Fθ0 i′  j′ )∩Bδ(Fθ0 i j )∩Bδ(Fθ0  i′ j′ )

ǫ ZBǫ(∂Fθ0 i j )
converge to 0 as ǫ  δ → 0 (in any manner)  where ∂Fθ0 i is the edge of Fθ0 i j.
Pr `dm
D (Ak(S1)  Ak(S2)  Br/√m (∪i j Fθ 0 i j))´ = O(r−3−δ) + o(1) 
where o(1) → 0 as m → ∞. Namely  the mass of D which switches between clusters
is with high probability inside thin strips around the limit cluster boundaries  and this
high probability increases at least polynomially as the width of the strips increase (see
below for a further discussion of this).

D (Ak(S1)  Ak(S2)) 6= dm

The regularity assumptions are relatively mild  and can usually be inferred based on the consistency
and central limit conditions  as well as the the speciﬁc clustering framework that we are considering.
For example  condition 3c and the assumptions on Fθ0 i j in condition 3b are fulﬁlled in a cluster-
ing framework where the clusters are separated by hyperplanes. As to condition 3d  suppose our
clustering framework is such that the cluster boundaries depend on ˆθ in a smooth manner. Then the
asymptotic normality of ˆθ  with variance O(1/m)  and the compactness of X   will generally imply
that the cluster boundaries obtained from clustering a sample are contained with high probability
inside strips of width O(1/√m) around the limit cluster boundaries. More speciﬁcally  the asymp-
totic probability of this happening for strips of width r/√m will be exponentially high in r  due
to the asymptotic normality of ˆθ. As a result  the mass which switches between clusters  when we
compare two independent clusterings  will be in those strips with probability exponentially high in
r. Therefore  condition 3d will hold by a large margin  since only polynomially high probability is
required there.

B Proofs - General Remarks

The proofs will use the additional notation and the sufﬁcient conditions  as presented in Sec. A.
Throughout the proofs  we will sometimes use the stochastic order notation Op(·) and op(·) (cf.
[8])  deﬁned as follows. Let {Xm} and {Ym} be sequences of random vectors  deﬁned on the same
probability space. We write Xm = Op(Ym) to mean that for each ǫ > 0 there exists a real number
M such that Pr(kXmk ≥ MkYmk) < ǫ if m is large enough. We write Xm = op(Ym) to mean that
Pr(kXmk ≥ ǫkYmk) → 0 for each ǫ > 0. Notice that {Ym} may also be non-random. For example 
Xm = op(1) means that Xm → 0 in probability. When we write for example Xm = Ym + op(1) 
we mean that Xm − Ym = op(1).
C Proof of Proposition 1

By condition 3a  fθ(x) has a ﬁrst order Taylor expansion with respect to any ˆθ close enough to θ0 
with a remainder term uniformly bounded for any x:

fˆθ(x) = fθ0(x) +(cid:18) ∂

∂θ

fθ0(x)(cid:19)⊤

(ˆθ − θ0) + o(kˆθ − θ0k).

(2)

2

By the asymptotic normality assumption  √mkˆθ − θ0k = Op(1)  hence kˆθ − θ0k = Op(1/√m).

Therefore  we get from Eq. (2) that

√m(cid:0)fˆθ(x) − fθ0(x)(cid:1) =(cid:18) ∂

∂θ

fθ0(x)(cid:19)⊤

(√m(ˆθ − θ0)) + op(1) 

(3)

where the remainder term op(1) does not depend on x. By regularity condition 3a and compactness
of X   (∂/∂θ)fθ0 (·) is a uniformly bounded vector-valued function from X to the Euclidean space
in which Θ resides. As a result  the mapping ˆθ 7→ ((∂/∂θ)fθ0 (·))⊤ˆθ is a mapping from Θ  with
the metric induced by the Euclidean space in which it resides  to the space of all uniformly bounded
Rk-valued functions on X . We can turn the latter space into a metric space by equipping it with
the obvious extension of the supremum norm (namely  for any two functions f (·)  g(·)  kf − gk :=
supx∈X kf (x)− g(x)k∞  where k·k∞ is the inﬁnity norm in Euclidean space). With this norm  the
mapping above is a continuous mapping between two metric spaces. We also know that √m(ˆθ−θ0)
converges in distribution to a multivariate Gaussian random variable Z. By the continuous mapping
theorem [8] and Eq. (3)  this implies that √m(fˆθ(·)−fθ0 (·)) converges in distribution to a Gaussian
process G(·)  where

G(·) :=(cid:18) ∂

∂θ

fθ0 (·)(cid:19)⊤

Z.

(4)

D Proof of Thm. 1

D.1 A High Level Description of the Proof

The full proof of Thm. 1 is rather long and technical  mostly due to the many technical subtleties
that need to be taken care of. Since these might obscure the main ideas  we present here separately
a general overview of the proof  without the ﬁner details.

Edm

Edm
D

Edm
D

(Ak(S1)  Ak(S2)). As a result  we will have to take a more indirect route.

m q  scaled by √m  boils down to trying to assess the
The purpose of the stability estimator ˆηk
”expected” value of the random variable dm
(Ak(S1)  Ak(S2)): we estimate q instantiations of
D
D (Ak(S1)  Ak(S2))  and take their average. Our goal is to show that this average  taking m → ∞ 
dm
is likely to be close to the value \instab(Ak D) as deﬁned in the theorem. The most straightforward
way to go about it is to prove that \instab(Ak D) actually equals limm→∞
(Ak(S1)  Ak(S2)) 
and then use some large deviation bound to prove that √m ˆηk
m q is indeed close to it with high
probability  if q is large enough. Unfortunately  computing limm→∞
D (Ak(S1)  Ak(S2)) is prob-
lematic. The reason is that the convergence tools at our disposal deals with convergence in dis-
tribution of random variables  but convergence in distribution does not necessarily imply conver-
gence of expectations.
In other words  we can try and analyze the asymptotic distribution of
(Ak(S1)  Ak(S2))  but the expected value of this asymptotic distribution is not necessarily the
dm
D
same as limm→∞
Here is the basic idea: instead of analyzing the asymptotic expectation of dm
(Ak(S1)  Ak(S2))  we
D
analyze the asymptotic expectation of a different random variable  dm
(Ak(S1)  Ak(S2)  B)  which
D
was formally deﬁned in Eq. (1). Informally  recall that dm
(Ak(S1)  Ak(S2)) is the mass of the un-
D
derlying distribution D which switches between clusters  when we draw and cluster two indepen-
(Ak(S1)  Ak(S2)  B) measures the subset of this mass  which
dent samples of size m. Then dm
D
lies inside some B ⊆ Rn. In particular  following the notation of Sec. A  we will pick B to be
(Ak(S1)  Ak(S2)  Br/√m(∪i jFθ0 i j)) for some r > 0. In words  this constitutes strips of width
dm
D
r/√m around the limit cluster boundaries. Writing the above expression for B as Br/√m  we have
that if r be large enough  then dm
(Ak(S1)  Ak(S2)) with
D
very high probability over drawing and clustering a pair of samples  for any large enough sample
size m. Basically  this is because the ﬂuctuations of the cluster boundaries  based on drawing and
clustering a random sample of size m  cannot be too large  and therefore the mass which switches
clusters is concentrated around the limit cluster boundaries  if m is large enough.
(Ak(S1)  Ak(S2)  Br/√m) is that it is bounded
The advantage of the ’surrogate’ random variable dm
D
for any ﬁnite r  unlike dm
(Ak(S1)  Ak(S2)). With bounded random variables  convergence in
D
distribution does imply convergence of expectations  and as a result we are able to calcu-
(Ak(S1)  Ak(S2)  Br/√m) explicitly. This will turn out to be very close to
late limm→∞

(Ak(S1)  Ak(S2)  Br/√m) is equal to dm
D

Edm
D

3

Edm
D

Edm
D

the proof

is divided into two parts:

m q will be close to limm→∞

(Ak(S1)  Ak(S2)  Br/√m) and dm
D

m q is an unbiased estimator of dm
D

\instab(Ak D) as it appears in the theorem (in fact  we can make it arbitrarily close to\instab(Ak D) by
making r large enough). Using the fact that dm
(Ak(S1)  Ak(S2))
D
are equal with very high probability  we show that conditioned on a highly probable event 
√m ˆηk
(Ak(S1)  Ak(S2)  Br/√m)  based on q instantiations  for
any sample size m. As a result  using large deviation bounds  we get that √m ˆηk
m q is close to
(Ak(S1)  Ak(S2)  Br/√m)  with a high probability which does not depend on m. Therefore  as
dm
D
m → ∞  √m ˆηk
(Ak(S1)  Ak(S2)  Br/√m) with high probability.

in Subsec. D.2  we calculate
(Ak(S1)  Ak(S2)  Br/√m) explicitly  while Subsec. D.3 executes the general plan out-

By picking r to scale appropriately with q  our theorem follows.
For convenience 
limm→∞
lined above to prove our theorem.
A few more words are in order about the calculation of limm→∞
(Ak(S1)  Ak(S2)  Br/√m)
in Subsec. D.2  since it is rather long and involved in itself. Our goal is to perform this calcu-
lation without going through an intermediate step of explicitly characterizing the distribution of
D (Ak(S1)  Ak(S2)  Br/√m). This is because the distribution might be highly dependent on the spe-
dm
ciﬁc clustering framework  and thus it is unsuitable for the level of generality which we aim at (in
other words  we do not wish to assume a speciﬁc clustering framework). The idea is as follows:
recall that dm
D (Ak(S1)  Ak(S2)  Br/√m) is the mass of the underlying distribution D  inside strips of
width r/√m around the limit cluster boundaries  which switches clusters when we draw and cluster
two independent samples of size m. For any x ∈ X   let Ax be the event that x switched clusters.
Then we can write dm
D

(Ak(S1)  Ak(S2)  Br/√m)  by Fubini’s theorem  as:

Edm
D

Edm

D (Ak(S1)  Ak(S2)  Br/√m) = √mEZBr/√m

1(Ax)p(x)dx =ZBr/√m

√m Pr(Ax)p(x)dx.
(5)

The heart of the proof is Lemma D.5  which considers what happens to the integral above inside a
single strip near one of the limit cluster boundaries Fθ0 i j. The main body of the proof then shows
how the result of Lemma D.5 can be combined to give the asymptotic value of Eq. (5) when we
take the integral over all of Br/√m. The bottom line is that we can simply sum the contributions
from each strip  because the intersection of these different strips is asymptotically negligible. All
the other lemmas in Subsec. D.2 develop technical results needed for our proof.

Finally  let us describe the proof of Lemma D.5 in a bit more detail. It starts with an expression
equivalent to the one in Eq. (5)  and transforms it to an expression composed of a constant value 
and a remainder term which converges to 0 as m → ∞. The development can be divided into a
number of steps. The ﬁrst step is rewriting everything using the asymptotic Gaussian distribution
of the cluster association function fˆθ(x) for each x  plus remainder terms (Eq. (13)). Since we are
integrating over x  special care is given to show that the convergence to the asymptotic distribution
is uniform for all x in the domain of integration. The second step is to rewrite the integral (which is
over a strip around the cluster boundary) as a double integral along the cluster boundary itself  and
along a normal segment at any point on the cluster boundary (Eq. (14)). Since the strips become
arbitrarily small as m → ∞  the third step consists of rewriting everything in terms of a Taylor
expansion around each point on the cluster boundary (Eq. (16)  Eq. (17) and Eq. (18)). The fourth
and ﬁnal step is a change of variables  and after a few more manipulations we get the required result.

D.2 Part 1: Auxiliary Result

As described in the previous subsection  we will need an auxiliary result (Proposition D.1 below) 
characterizing the asymptotic expected value of dm
D
Proposition D.1. Let r > 0.
limm→∞

(Ak(S1)  Ak(S2)  Br/√m(∪i jFθ0 i j)).

Assuming the set of conditions from Sec. A holds 

Edm

D (Ak(S1)  Ak(S2)  Br/√m(∪i jFθ0 i j)) is equal to
2(cid:18) 1
√π − h(r)(cid:19) X1≤i<j≤kZFθ0 i j

p(x)pVar(Gi(x) − Gj(x))
k∇(fθ0 i(x) − fθ0 j(x))k

dx 

where h(r) = O(exp(−r2)).

4

To prove this result  we will need several technical lemmas.
Lemma D.1. Let S be a hypersurface in Rn which fulﬁll the regularity conditions 3b and 3c for any
Fθ0 i j  and let g(·) be a continuous real function on X . Then for any ǫ > 0 
g(x + ynx)dydx + o(1) 

g(x)dx =

(6)

1

1

ǫ ZBǫ(S)

ǫ ZSZ ǫ

−ǫ

where nx is a unit normal vector to S at x  and o(1) → 0 as ǫ → 0.

Proof. Let B′ǫ(S) be a strip around S  composed of all points which are on some normal to S and
close enough to S:

B′ǫ(S) := {y ∈ Rn : ∃x ∈ S ∃y ∈ [−ǫ  ǫ]  y = x + ynx}.

Since S is orientable  then for small enough ǫ > 0  B′ǫ(S) is diffeomorphic to S × [−ǫ  ǫ].
particular  the map φ : S × [−ǫ  ǫ] 7→ B′ǫ(S)  deﬁned by

In

φ(x  y) = x + ynx

will be a diffeomorphism. Let Dφ(x  y) be the Jacobian of φ at the point (x  y) ∈ S × [−ǫ  ǫ]. Note
that Dφ(x  0) = 1 for every x ∈ S.
We now wish to claim that as ǫ → 0 
ǫ ZBǫ(S)

ǫ ZB′ǫ(S)

g(x)dx + o(1).

g(x)dx =

(7)

1

1

To see this  we begin by noting that B′ǫ(S) ⊆ Bǫ(S). Moreover  any point in Bǫ(S)\ B′ǫ(S) has the
property that its projection to the closest point in S is not a normal to S  and thus must be ǫ-close
to the edge of S. As a result of regularity condition 3c for S  and the fact that g(·) is continuous
and hence uniformly bounded in the volume of integration  we get that the integration of g(·) over
Bǫ \ B′ǫ is asymptotically negligible (as ǫ → 0)  and hence Eq. (7) is justiﬁed.
By the change of variables theorem from multivariate calculus  followed by Fubini’s theorem  and
using the fact that Dφ is continuous and equals 1 on S × {0} 

1

ǫ ZB′ǫ(S)

g(x)dx =

g(x + ynx)Dφ(x  y)dxdy

1

ǫ ZS×[−ǫ ǫ]
ǫ Z ǫ
−ǫ(cid:18)ZS
ǫ Z ǫ
−ǫ(cid:18)ZS

1

1

=

=

g(x + ynx)Dφ(x  y)dx(cid:19) dy
g(x + ynx)dx(cid:19) dy + o(1) 
where o(1) → 0 as ǫ → 0. Combining this with Eq. (7) yields the required result.
Lemma D.2. Let (gm : X 7→ R)∞m=1 be a sequence of integrable functions  such that gm(x) → 0
uniformly for all x as m → ∞. Then for any i  j ∈ {1  . . .   k}  i 6= j 
√mgm(x)p(x)dx → 0

ZBr/√m(Fθ0 i j )

as m → ∞

Proof. By the assumptions on (gm(·))∞m=1  there exists a sequence of positive constants (bm)∞m=1 
converging to 0  such that

ZBr/√m(Fθ0 i j )

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

√mgm(x)p(x)dx(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

5

≤ bmZBr/√m(Fθ0 i j )

√mp(x)dx.

For large enough m  p(x) is bounded and continuous in the volume of integration. Applying

Lemma D.1 with ǫ = r/√m  we have that as m → ∞ 

p(x)dx = bm√mZFθ0 i jZ r/√m

−r/√m

p(x + ynx)dydx + o(1)

bm√mZBr/√m(Fθ0 i j )
≤ bm√m

C
√m

+ o(1) = bmC + o(1)

for some constant C dependant on r and the upper bound on p(·). Since bm converge to 0  we have
that the expression in the lemma converges to 0 as well.
Lemma D.3. Let (Xm) and (Ym) be a sequence of real random variables  such that Xm  Ym are
deﬁned on the same probability space  and Xm − Ym converges to 0 in probability. Assume that Ym
converges in distribution to a continuous random variable Y . Then | Pr(Xm ≤ c) − Pr(Ym ≤ c)|
converges to 0 uniformly for all c ∈ R.
Proof. We will use the following standard fact (see for example section 7.2 of [4]): for any two real
random variables A  B  any c ∈ R and any ǫ > 0  it holds that

Pr(A ≤ c) ≤ Pr(B ≤ c + ǫ) + Pr(|A − B| > ǫ).

From this inequality  it follows that for any c ∈ R and any ǫ > 0 
| Pr(Xm ≤ c) − Pr(Ym ≤ c)| ≤(cid:16) Pr(Ym ≤ c + ǫ) − Pr(Ym ≤ c)(cid:17)

+(cid:16) Pr(Ym ≤ c) − Pr(Ym ≤ c − ǫ)(cid:17) + Pr(|Xm − Ym| ≥ ǫ).

(8)

We claim that the r.h.s of Eq. (8) converges to 0 uniformly for all c  from which the lemma follows.
To see this  we begin by noticing that Pr(|Xm − Ym| ≥ ǫ) converges to 0 for any ǫ by deﬁnition of
convergence in probability. Next  Pr(Ym ≤ c′) converges to Pr(Y ≤ c′) uniformly for all c′ ∈ R 
since Y is continuous (see section 1 of [6]). Moreover  since Y is a continuous random variable  we
have that its distribution function is uniformly continuous  hence Pr(Y ≤ c + ǫ) − Pr(Y ≤ c) and
Pr(Y ≤ c) − Pr(Y ≤ c − ǫ) converges to 0 as ǫ → 0  uniformly for all c. Therefore  by letting
m → ∞  and ǫ → 0 at an appropriate rate compared to m  we have that the l.h.s of Eq. (8) converges
to 0 uniformly for all c.
Lemma D.4. Pr((cid:10)a √m(fˆθ(x) − fθ0(x))(cid:11) < b) converges to Pr(ha  G(x)i < b) uniformly for
any x ∈ X   any a 6= 0 in some bounded subset of Rk  and any b ∈ R.
Proof. By Eq. (3) 

√m(cid:0)fˆθ(x) − fθ0(x)(cid:1) =(cid:18) ∂

∂θ

fθ0(x)(cid:19)⊤

(√m(ˆθ − θ0)) + op(1).

Where the remainder term does not depend on x. Thus  for any a in a bounded subset of Rk 

(cid:10)a √m(cid:0)fˆθ(x) − fθ0 (x)(cid:1)(cid:11) =*a(cid:18) ∂

∂θ

fθ0 (x)(cid:19)⊤

 √m(ˆθ − θ0)+ + op(1) 

(9)

Where the convergence in probability is uniform for all bounded a and x ∈ X .
We now need to use a result which tells us when is a convergence in distribution uniform. Using thm.
4.2 in [6]  we have that if a sequence of random vectors (Xm)∞m=1 in Euclidean space converge to a
random variable X in distribution  then Pr(hy  Xmi < b) converges to Pr(hy  Xi < b) uniformly
for any vector y and b ∈ R. We note that a stronger result (Thm. 6 in [2]) apparently allows us to
extend this to cases where Xm and X reside in some inﬁnite dimensional  separable Hilbert space
(for example  if Θ is a subset of an inﬁnite dimensional reproducing kernel Hilbert space in kernel
clustering). Therefore  recalling that √m(ˆθ − θ0) converges in distribution to a random normal

vector Z  we have that uniformly for all x  a  b 

6

Pr *a(cid:18) ∂

∂θ

fθ0(x)(cid:19)⊤

 √m(ˆθ − θ0)+ < b! = Pr *a(cid:18) ∂

fθ0 (x)(cid:19)⊤
= Pr (ha  G(x)i < b) + o(1)

∂θ

  Z+ < b! + o(1)

(10)

Here we think of a((∂/∂θ)fθ0(x))⊤ as the vector y to which we apply the theorem. By regularity

condition 3a  and assuming a 6= 0  we have that(cid:10)a((∂/∂θ)fθ0(x))⊤  Z(cid:11) is a continuous real ran-

dom variable for any x  unless Z = 0 in which case the lemma is trivial. Therefore  the conditions
of Lemma D.3 apply: the two sides of Eq. (9) give us two sequences of random variables which
converge in probability to each other  and by Eq. (10) we have convergence in distribution of one of
the sequences to a ﬁxed continuous random variable. Therefore  using Lemma D.3  we have that

Pr(cid:0)(cid:10)a √m(cid:0)fˆθ(x) − fθ0(x)(cid:1)(cid:11) < b(cid:1) = Pr *a(cid:18) ∂

∂θ

fθ0(x)(cid:19)⊤

where the convergence is uniform for any bounded a 6= 0  b and x ∈ X .
Combining Eq. (10) and Eq. (11) gives us the required result.

 √m(ˆθ − θ0)+ < b! + o(1) 

(11)

Lemma D.5. Fix some two clusters i  j. Assuming the expression below is integrable  we have that

2ZBr/√m(Fθ0 i j )
= 2(cid:18) 1

√π − h(r)(cid:19)ZFθ0 i j

√m Pr(fˆθ i(x) − fˆθ j(x) < 0) Pr(fˆθ i(x) − fˆθ j > 0)p(x)dx

p(x)pVar(Gi(x) − Gj(x))
k∇(fθ0 i(x) − fθ0 j(x))k

dx + o(1)

where o(1) → 0 as m → ∞ and h(r) = O(exp(−r2)).

Proof. Deﬁne a ∈ Rk as ai = 1  aj = −1  and 0 for any other entry. Applying Lemma D.4  with a
as above  we have that uniformly for all x in some small enough neighborhood around Fθ0 i j:

Pr(fˆθ i(x) − fˆθ j(x) < 0)
= Pr(cid:16)√m(fˆθ i(x) − fθ0 i(x)) − √m(fˆθ j(x) − fθ0 j(x)) < √m(fθ0 j(x) − fθ0 i(x))(cid:17)
= Pr(Gi(x) − Gj(x) < √m(fθ0 j(x) − fθ0 i(x))) + o(1).

where o(1) converges uniformly to 0 as m → ∞.
Since Gi(x)− Gj(x) has a zero mean normal distribution  we can rewrite the above (if Var(Gi(x)−
Gj(x)) > 0) as

<

Gi(x) − Gj(x)

Pr 
= Φ √m(fθ0 j(x) − fθ0 i(x))

pVar(Gi(x) − Gj(x))
pVar(Gi(x) − Gj(x)) ! + o(1) 

√m(fθ0 j(x) − fθ0 i(x))
pVar(Gi(x) − Gj(x)) ! + o(1)

(12)

where Φ(·) is the cumulative standard normal distribution function. Notice that by some abuse of
notation  the expression is also valid in the case where Var(Gi(x) − Gj(x)) = 0. In that case 
Gi(x) − Gj(x) is equal to 0 with probability 1  and thus Pr(Gi(x) − Gj(x) < √m(fθ0 j(x) −
fθ0 i(x))) is 1 if fθ0 j(x) − fθ0 i(x)) ≥ 0 and 0 if fθ0 j(x) − fθ0 i(x)) < 0. This is equal to
Eq. (12) if we are willing to assume that Φ(∞) = 1  Φ(0/0) = 1  Φ(−∞) = 0.

7

Therefore  we can rewrite the l.h.s of the equation in the lemma statement as

2ZBr/√m(Fθ0 i j )

√mΦ √m(fθ0 i(x) − fθ0 j(x))
pVar(Gi(x) − Gj(x)) !

 1 − Φ √m(fθ0 i(x) − fθ0 j(x))

pVar(Gi(x) − Gj(x)) !! + √mo(1)p(x)dx.

The integration of the remainder term can be rewritten as o(1) by Lemma D.2  and we get that the
expression can be rewritten as:

2ZBr/√m(Fθ0 i j )

√mΦ √m(fθ0 i(x) − fθ0 j(x))
pVar(Gi(x) − Gj(x)) !
 1 − Φ √m(fθ0 i(x) − fθ0 j(x))

pVar(Gi(x) − Gj(x)) !! p(x)dx + o(1).

(13)

One can verify that the expression inside the integral is a continuous function of x  by the regularity
conditions and the expression for G(·) as proven in Sec. C (namely Eq. (4)). We can therefore apply
Lemma D.1  and again take all the remainder terms outside of the integral by Lemma D.2  to get
that the above can be rewritten as

2ZFθ0 i jZ r/√m

−r/√m

√mΦ √m(fθ0 i(x + ynx) − fθ0 j(x + ynx))
pVar(Gi(x + ynx) − Gj(x + ynx)) !
 1 − Φ √m(fθ0 i(x + ynx) − fθ0 j(x + ynx))
pVar(Gi(x + ynx) − Gj(x + ynx)) !! p(x)dydx + o(1) 

where nx is a unit normal to Fθ0 i j at x.
Inspecting Eq. (14)  we see that y ranges over an arbitrarily small domain as m → ∞. This suggests
that we can rewrite the above using Taylor expansions  which is what we shall do next.
Let us assume for a minute that Var(Gi(x) − Gj(x)) > 0 for some point x ∈ Fθ0 i j. One can
verify that by the regularity conditions and the expression for G(·) in Eq. (4)  the expression

(14)

(15)

is twice differentiable  with a uniformly bounded second derivative. Therefore  we can rewrite the
expression in Eq. (15) as its ﬁrst-order Taylor expansion around each x ∈ Fθ0 i j  plus a remainder
term which is uniform for all x:

fθ0 i(·) − fθ0 j(·)
pVar(Gi(·) − Gj(·))

fθ0 i(x + ynx) − fθ0 j(x + ynx)
pVar(Gi(x + ynx) − Gj(x + ynx))

=

fθ0 i(x) − fθ0 j(x)
pVar(Gi(x) − Gj(x))

+ ∇ fθ0 i(x) − fθ0 j(x)

pVar(Gi(x) − Gj(x))! ynx + O(y2).

Since fθ0 i(x) − fθ0 j(x) = 0 for any x ∈ Fθ0 i j  the expression reduces after a simple calculation
to

Notice that ∇(fθ0 i(x) − fθ0 j(x)) (the gradient of fθ0 i(x) − fθ0 j(x)) has the same direction as
nx (the normal to the cluster boundary). Therefore  the expression above can be rewritten  up to a
sign  as

∇(fθ0 i(x) − fθ0 j(x))
pVar(Gi(x) − Gj(x))
pVar(Gi(x) − Gj(x))(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
y(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

∇(fθ0 i(x) − fθ0 j(x))

8

ynx + O(y2).

+ O(y2).

As a result  denoting s(x) := ∇(fθ0 i(x) − fθ0 j(x))/pVar(Gi(x) − Gj(x))  we have that
Φ √m(fθ0 i(x + ynx) − fθ0 j(x + ynx))
pVar(Gi(x + ynx) − Gj(x + ynx)) ! 1 − Φ √m(fθ0 i(x + ynx) − fθ0 j(x + ynx))
pVar(Gi(x + ynx) − Gj(x + ynx)) !!
= Φ(cid:16)√m(cid:0)ks(x)ky + O(y2)(cid:1)(cid:17) 1 − Φ(cid:16)√m(cid:0)ks(x)ky + O(y2)(cid:1)(cid:17)!
= Φ(cid:16)√m(cid:0)ks(x)ky(cid:1)(cid:17) 1 − Φ(cid:16)√m(cid:0)ks(x)ky(cid:1)(cid:17)! + O(√my2).

(17)

(16)

In the preceding development  we have assumed that Var(Gi(x) − Gj(x)) > 0. However  notice
that the expressions in Eq. (16) and Eq. (17)  without the remainder term  are both equal (to zero)
even if Var(Gi(x) − Gj(x)) = 0 (with our previous abuse of notation that Φ(−∞) = 0  Φ(∞) =
1). Moreover  since y takes values in [−r/√m  r/√m]  the remainder term O(√my2) is at most
O(√mr/m) = O(r/√m)  so it can be rewritten as o(1) which converges to 0 as m → ∞.

In conclusion  and again using Lemma D.2 to take the remainder terms outside of the integral  we
can rewrite Eq. (14) as

2ZFθ0 i jZ r/√m

√mΦ(cid:0)√mks(x)ky)(cid:1)(cid:0)1 − Φ(cid:0)√mks(x)ky)(cid:1)(cid:1) p(x)dydx + o(1).
We now perform a change of variables  letting zx = √mks(x)ky in the inner integral  and get

−r/√m

(18)

2ZFθ0 i jZ rks(x)k

−rks(x)k

1

ks(x)k

Φ (zx) (1 − Φ (zx)) p(x)dzxdx + o(1) 

which is equal by the mean value theorem to

p(x)
ks(x)k

dx! Z rks(x0)k

2 ZFθ0  i j
for some x0 ∈ Fθ0 i j.
By regularity condition 3b  it can be veriﬁed that ks(x)k is positive or inﬁnite for any x ∈ Fθ0 i j.
As a result  as r → ∞  we have that

Φ (zx0) (1 − Φ (zx0)) dzx0! + o(1)

−rks(x0)k

(19)

Z rks(x0)k

−rks(x0)k

Φ (zx0) (1 − Φ (zx0)) dzx0 −→Z ∞

−∞

Φ(zx0 )(1 − Φ(zx0 ))dzx0 =

1
√π

.

and the convergence to 1/√π is at a rate of O(exp(−r2)). Combining this with Eq. (19) gives us

the required result.

Proof of Proposition D.1. We can now turn to prove Proposition D.1 itself. For any x ∈ X   let Ax
be the event (over drawing and clustering a sample pair) that x switched clusters. For any Fθ0 i j
and sample size m  deﬁne F m
θ0 i j to be the subset of Fθ0 i j  which is at a distance of at least m−1/4
from any other cluster boundary (with respect to θ0). Formally 

F m

θ0 i j :=(cid:26)x ∈ Fθ0 i j : ∀ ({i′  j′} 6= {i  j}  Fθ0 i′ j′ 6= ∅)  

inf

y∈Fθ0 i′  j′ kx − yk ≥ m−1/4(cid:27) .

9

Letting S1  S2 be two independent samples of size m  we have by Fubini’s theorem that

Edm

D (Ak(S1)  Ak(S2)  Br/√m(∪i jFθ0 i j))
= √mES1 S2ZBr/√m(∪i j Fθ0 i j )
=ZBr/√m(∪i j F m

1(Ax)p(x)dx =ZBr/√m(∪i j Fθ0 i j )
√m Pr(Ax)p(x)dx +ZBr/√m(∪i j Fθ0 i j\F m

θ0 i j )

θ0 i j )

√m Pr(Ax)p(x)dx

√m Pr(Ax)p(x)dx.

As to the ﬁrst integral  notice that each point in F m
θ0 i′ j′ by a distance of at least 2m−1/4. Therefore  for large enough m  Br/√m(F m
F m
disjoint for each i  j  and we can rewrite the above as:

θ0 i j is separated from any point in any other
θ0 i j) are

θ0 i j )

√m Pr(Ax)p(x)dx.

√m Pr(Ax)p(x)dx +ZBr/√m(∪i j Fθ0 i j\F m

X1≤i<j≤kZBr/√m(F m
As to the second integral  notice that the integration is over points which are at a distance of at most
r/√m from some Fθ0 i j  and also at a distance of at most m−1/4 from some other Fθ0 i′ j′. By
regularity condition 3c  and the fact that m−1/4 → 0  it follows that this integral converges to 0 as
m → ∞  and we can rewrite the above as:
X1≤i<j≤kZBr/√m(F m

√m Pr(Ax)p(x)dx + o(1)

θ0 i j )

θ0 i j )

(20)

If there were only two clusters i  j  then

Pr(Ax) = 2 Pr(fˆθ i(x) − fˆθ j(x) < 0) Pr(fˆθ i(x) − fˆθ j > 0).

This is simply by deﬁnition of Ax: the probability that under one clustering  based on a random
sample  x is more associated with cluster i  and that under a second clustering  based on another
independent random sample  x is more associated with cluster j.
In general  we will have more than two clusters. However  notice that any point x in Br/√m(F m
θ0 i j)
(for some i  j) is much closer to Fθ0 i j than to any other cluster boundary. This is because its
distance to Fθ0 i j is on the order of 1/√m  while its distance to any other boundary is on the order
of m−1/4. Therefore  if x does switch clusters  then it is highly likely to switch between cluster i and
cluster j. Formally  by regularity condition 3d (which ensure that the cluster boundaries experience
at most O(1/√m) ﬂuctuations)  we have that uniformly for any x 

Pr(Ax) = 2 Pr(fˆθ i(x) − fˆθ j(x) < 0) Pr(fˆθ i(x) − fˆθ j > 0) + o(1) 

where o(1) converges to 0 as m → ∞.
Substituting this back to Eq. (20)  using Lemma D.2 to take the remainder term outside the integral 
and using the regularity condition 3c in the reverse direction to transform integrals over F m
θ0 i j
back into Fθ0 i j with asymptotically negligible remainder terms  we get that the quantity we are
interested in can be written as

X1≤i<j≤k

2ZBr/√m(Fθ0 i j )

√m Pr(fˆθ i(x) − fˆθ j(x) < 0) Pr(fˆθ i(x) − fˆθ j > 0)p(x)dx + o(1).

Now we can apply Lemma D.5 to each summand  and get the required result.

D.3 Part 2: Proof of Thm. 1

For notational convenience  we will denote

dm
D (r) := dm

D (Ak(S1)  Ak(S2)  Br/√m(∪i jFθ0 i j))

10

(Ak(S1

If \instab(Ak D) = 0  the proof of the
whenever the omitted terms are obvious from context.
In this special case  by deﬁnition of \instab(Ak D) in Thm. 1 and
theorem is straightforward.
Proposition D.1  we have that dm
(r) converges in probability to 0 for any r. By regularity con-
D
q Pq
dition 3d  for any ﬁxed q  1
i )) converges in probability to 0 (because
i=1 dm
(Ak(S1
D
i )  Br/√m(∪i jFθ0 i j)) with arbitrarily high probabil-
i )) = dm
dm
i )  Ak(S2
D
D
ity as r increases). Therefore  √m ˆηk
m q  which is a plug-in estimator of the expected value of
q Pq
i ))  converges in probability to 0 for any ﬁxed q as m → ∞  and the the-
orem follows for this special case. Therefore  we will assume from now on that \instab(Ak D) > 0.
We need the following variant of Hoeffding’s bound  adapted to conditional probabilities.

D (Ak(S1

i )  Ak(S2

i )  Ak(S2

i )  Ak(S2

i=1 dm

(Ak(S1

1

Lemma D.6. Fix some r > 0. Let X1  . . .   Xq be real  nonnegative  independent and identically
distributed random variables  such that Pr(X1 ∈ [0  r]) > 0. For any Xi  let Yi be a random
variable on the same probability space  such that Pr(Yi = Xi|Xi ∈ [0  r]) = 1. Then for any
ν > 0 

1
q

q

Xi=1

Pr (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Xi − E[Y1|X1 ∈ [0  r]](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≥ ν (cid:12)(cid:12)(cid:12) ∀i  Xi ∈ [0  r]! ≤ 2 exp(cid:18)−

2qν2

r2 (cid:19) .

Proof. Deﬁne an auxiliary set of random variables Z1  . . .   Zq  such that Pr(Zi ≤ a) = Pr(Xi ≤
a|Xi ∈ [0  r]) for any i  a. In words  Xi and Zi have the same distribution conditioned on the event
Xi ∈ [0  r]. Also  we have that Yi has the same distribution conditioned on Xi ∈ [0  r]. Therefore 
E[Y1|X1 ∈ [0  r]] = E[X1|X1 ∈ [0  r]]  and as a result E[Y1|X1 ∈ [0  r]] = E[Z1]. Therefore  the
probability in the lemma above can be written as

1
q

q

Xi=1

Pr (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Zi − E[Zi](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≥ ν!  

where Zi are bounded in [0  r] with probability 1. Applying the regular Hoeffding’s bound gives us
the required result.

(Ak(S1

i )  Ak(S2

We now turn to the proof of the theorem. Let Am
i } 
i   S2
i )). Namely  this is the event that for
dm
i )  Br/√m(∪i j Fθ0 i j )) = dm
D
D
all subsample pairs  the mass which switches clusters when we compare the two resulting clusterings
is always in an r/√m-neighborhood of the limit cluster boundaries.
Since p(·) is bounded  we have that dm
D
constants depending only on D and θ0. Using the law of total expectation  this implies that

r be the event that for all subsample pairs {S1
(Ak(S1

(r) is deterministically bounded by O(r)  with implicit

i )  Ak(S2

r ](cid:12)(cid:12)(cid:12)(cid:12)
D (r)|Am
D (r)|Am
r )(cid:19)(cid:18)E[dm

E[dm

D (r)] − E[dm
r )E[dm
Pr(Am

(cid:18)1 − Pr(Am
≤ (1 − Pr(Am

r ))O(r).

(cid:12)(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)(cid:12)

r ))E[dm

D (r)|¬Am

r ] − E[dm

D (r)|Am

r ] + (1 − Pr(Am
D (r)|¬Am

r ] − E[dm

D (r)|Am

r ](cid:12)(cid:12)(cid:12)(cid:12)

r ](cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

(21)

For any two events A  B  we have by the law of total probability that Pr(A) = Pr(B) Pr(A|B) +
Pr(Bc) Pr(A|Bc). From this it follows that Pr(A) ≤ Pr(B) + Pr(A|Bc). As a result  for any

11

ǫ > 0 

√m ˆηk

q

ǫ

2#! .

(22)

q

ǫ

1
q

1
q

≤

"(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

√m ˆηk

i )  Ak(S2

i )  Ak(S2

dm
D (Ak(S1

>

2!
D (Ak(S1
dm

i )) − \instab(Ak D)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

We will assume w.l.o.g that ǫ/2 < \instab(Ak D).

> ǫ(cid:17)
i )) − \instab(Ak D)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
> ǫi(cid:12)(cid:12)(cid:12)
Xi=1

m q − \instab(Ak D)(cid:12)(cid:12)(cid:12)
Xi=1
√m ˆηk
m q − \instab(Ak D)(cid:12)(cid:12)(cid:12)

Pr(cid:16)(cid:12)(cid:12)(cid:12)
≤ Pr (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
+ Pr h(cid:12)(cid:12)(cid:12)
Pr(cid:16)(cid:12)(cid:12)(cid:12)
tity ǫ′ for which ǫ′/2 < \instab(Ak D ).
We start by analyzing the conditional probability  forming the second summand in Eq. (22). Recall
i }q
i=1  uses an additional i.i.d sample S3
that ˆηk
i   S2
i ))/√mq ∈ [0  1]. This is achieved by
of size m to empirically estimate Pq dm
i )  Ak(S2
calculating the average percentage of instances in S3 which switches between clusterings. Thus 
m q is simply an empirical
conditioned on the event appearing in the second summand of Eq. (22)  ˆηk
average of m i.i.d random variables in [0  1]  whose expected value  denoted as v  is a strictly positive
number in the range of (\instab(Ak D) ± ǫ/2)/√m. Thus  the second summand of Eq. (22) refers to
an event where this empirical average is at a distance of at least ǫ/(2√m) from its expected value.
We can therefore apply a large deviation result to bound this probability. Since the expectation itself
is a (generally decreasing) function of the sample size m  we will need something a bit stronger than
the regular Hoeffding’s bound. Using a relative entropy version of Hoeffding’s bound [5]  we have
that the second summand in Eq. (22) is upper bounded by:

> ǫ(cid:17) in the equation above by replacing ǫ with some smaller quan-

m q  after clustering the q subsample pairs {S1

m q − \instab(Ak D)(cid:12)(cid:12)(cid:12)

Otherwise  we can upper bound

(Ak(S1

D

√m (cid:12)(cid:12)(cid:12)(cid:12)
exp(cid:18)−mDkl(cid:20) v + ǫ/2

(cid:12)(cid:12)(cid:12)(cid:12)

v

√m(cid:21)(cid:19) + exp(cid:18)−mDkl(cid:20)max(cid:26)0 

v

√m(cid:21)(cid:19)  

(23)

v − ǫ/2

√m (cid:27)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

where Dkl[p||q] := −p log(p/q)− (1− p) log((1− p)/(1− q)) for any q ∈ (0  1) and any p ∈ [0  1].
Using the fact that Dkl[p||q] ≥ (p− q)2/2 max{p  q}  we get that Eq. (23) can be upper bounded by
a quantity which converges to 0 as m → ∞. As a result  the second summand in Eq. (22) converges
to 0 as m → ∞.
As to the ﬁrst summand in Eq. (22)  using the triangle inequality and switching sides allows us to
upper bound it by:

1
q

q

Xi=1

Pr (cid:12)(cid:12)(cid:12)(cid:12)

dm
D (Ak(S1

i )  Ak(S2

i )) − E[dm

D (r)|Am

r ](cid:12)(cid:12)(cid:12)(cid:12)

≥

ǫ

2 −(cid:12)(cid:12)(cid:12)(cid:12)

E[dm

D (r)|Am

r ] − E[dm

D (r)](cid:12)(cid:12)(cid:12)(cid:12)

−(cid:12)(cid:12)(cid:12)(cid:12)

Edm

D (r) − \instab(Ak D)(cid:12)(cid:12)(cid:12)(cid:12)

By the deﬁnition of \instab(Ak D) as appearing in Thm. 1   and Proposition D.1 
D (r) − \instab(Ak D) = O(h(r)) = O(exp(−r2)).

Edm

lim
m→∞

(cid:19) (24)

(25)

Using Eq. (25) and Eq. (21)  we can upper bound Eq. (24) by

1
q

q

Xi=1

Pr (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

dm
D (Ak(S1

i )  Ak(S2

D (r)|Am
i )) − E[dm
ǫ
2 − (1 − Pr(Am

≥

r ](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

r ))O(r) − O(exp(−r2)) − o(1)(cid:17)  

(26)

12

where o(1) → 0 as m → ∞. Moreover  by using the law of total probability and Lemma D.6  we
have that for any ν > 0 

q

1
q

Pr (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Xi=1
≤ (1 − Pr(Am

≤ (1 − Pr(Am

r )) + 2 Pr(Am

r ](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

> ν(cid:12)(cid:12)(cid:12)

Am

r !

(27)

dm
D (Ak(S1

i )  Ak(S2

i )) − E[dm

q

1
q

> ν!
D (Ak(S1
dm

r ](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
D (r)|Am
Xi=1
r2 (cid:19) .

2qν2

r ) Pr (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
r ) exp(cid:18)−

r )) ∗ 1 + Pr(Am

i )  Ak(S2

i )) − E[dm

D (r)|Am

Lemma D.6 can be applied because dm
D
If m  r are such that

(Ak(S1

i )  Ak(S2

i )) = dm
D

(r) for any i  if Am

r occurs.

ǫ
2 − (1 − Pr(Am

r ))O(r) − O(exp(−r2)) − o(1) > 0 

(28)

we can substitute this expression instead of ν in Eq. (27)  and get that Eq. (26) is upper bounded by

(1 − Pr(Am

r )) + 2 Pr(Am

r ) exp −

2q(cid:0) ǫ

2 − (1 − Pr(Am

r ))O(r) − O(exp(−r2))) − o(1)(cid:1)2

r2

! .

(29)

Let

gm(r) :=

Pr

S1 S2∼Dm

(dm
D (r) 6= dm

D (Ak(S1)  Ak(S2)))

 

g(r) = lim
m→∞

gm(r)

By regularity condition 3d  g(r) = O(r−3−δ) for some δ > 0. Also  we have that Pr(Am
r ) =
r ) = (1 − g(r))q for any ﬁxed q. In consequence  as
(1 − gm(r))q  and therefore limm→∞ Pr(Am
m → ∞  Eq. (29) converges to
2 − (1 − (1 − g(r))q)O(r) − O(exp(−r2))(cid:1)2
! .
(1 − (1 − g(r)))q) + 2(1 − g(r))q exp −
2q(cid:0) ǫ

r2

(30)

Now we use the fact that r can be chosen arbitrarily. In particular  let r = q1/(2+δ/2)  where δ > 0
is the same quantity appearing in condition 3d. It follows that

1 − (1 − g(r))q ≤ qg(r) = O(q/r3+δ) = O(cid:16)q1− 3+δ
2+δ/2(cid:17)
(1 − (1 − g(r))q)O(r) = qg(r)O(r) = O(cid:16)q1− 2+δ
2+δ/2(cid:17) = O(q− δ
q/r2 = q1− 1
exp(−r2) = exp(−q

1+δ/4 ).

1+δ/4

1

4+δ )

It can be veriﬁed that the equations above imply the validness of Eq. (28) for large enough m and q
(and hence r). Substituting these equations into Eq. (30)  we get an upper bound

O(cid:16)q1− 3+δ

2+δ/2(cid:17) + exp(cid:18)−2q1− 1

1+δ/4 (cid:16) ǫ

2 − O(cid:16)q− δ

4+δ(cid:17) − O(cid:16)exp(−q

1

1+δ/4 )(cid:17)(cid:17)2(cid:19) .

Since δ > 0  it can be veriﬁed that the ﬁrst summand asymptotically dominates the second summand
(as q → ∞)  and can be bounded in turn by o(q−1/2).
Summarizing  we have that the ﬁrst summand in Eq. (22) converges to o(q−1/2) as m → ∞  and the
second summand in Eq. (22) converge to 0 as m → ∞  for any ﬁxed ǫ > 0  and thus Pr(|√m ˆηk
m q−
\instab(Ak D)| > ǫ) converges to o(q−1/2).

13

E Proof of Thm. 2 and Thm. 3

The tool we shall use for proving Thm. 2 and Thm. 3 is the following general central limit the-
orem for Z-estimators (Thm. 3.3.1 in [8]). We will ﬁrst quote the theorem and then explain the
terminology used.
Theorem E.1 (Van der Vaart). Let Ψm and Ψ be random maps and a ﬁxed map  respectively  from
a subset Θ of some Banach space into another Banach space such that as m → ∞ 

k√m(Ψm − Ψ)(ˆθ) − √m(Ψm − Ψ)(θ0)k

1 + √mkˆθ − θ0k

→ 0

(31)

θ0

Z.

in probability  and such that the sequence √m(Ψm − Ψ)(θ0) converges in distribution to a tight
random element Z. Let θ 7→ Ψ(θ) be Fr´echet-differentiable at θ0 with an invertible derivative
˙Ψθ0  which is assumed to be a continuous linear operator1. If Ψ(θ0) = 0 and Ψm(ˆθ)/√m → 0
in probability  and ˆθ converges in probability to θ0  then √m(ˆθ − θ0) converges in distribution to
− ˙Ψ−1
A Banach space is any complete normed vector space (possible inﬁnite dimensional). A tight ran-
dom element essentially means that an arbitrarily large portion of its distribution lies in compact
sets. This condition is trivial when Θ is a subset of Euclidean space. Fr´echet-differentiability of a
function f : U 7→ V at x ∈ U  where U  V are Banach spaces  means that there exists a bounded
linear operator A : U 7→ V such that

This is equivalent to regular differentiability in ﬁnite dimensional settings.

kf (x + h) − f (x) − A(h)kW

= 0.

lim
h→0

khkU

It is important to note that the theorem is stronger than what we actually need  since we only consider
ﬁnite dimensional Euclidean spaces  while the theorem deals with possibly inﬁnite dimensional
Banach spaces. In principle  it is possible to use this theorem to prove central limit theorems in
inﬁnite dimensional settings  for example in kernel clustering where the associated reproducing
kernel Hilbert space is inﬁnite dimensional. However  the required conditions become much less
trivial  and actually fail to hold in some cases (see below for further details).

We now turn to the proofs themselves. Since the proofs of Thm. 2 and Thm. 3 are almost identical 
we will prove them together  marking differences between them as needed. In order to allow uniform
notation in both cases  we shall assume that φ(·) is the identity mapping in Bregman divergence
clustering  and the feature map from X to H in kernel clustering.
With the assumptions that we made in the theorems  the only thing really left to show before applying
Thm. E.1 is that Eq. (31) holds. Notice that it is enough to show that
m − Ψi)(θ0)k

k√m(Ψi

m − Ψi)(ˆθ) − √m(Ψi
1 + √mkˆθ − θ0k

for any i ∈ {1  . . .   k}. We will prove this in a slightly more complicated way than necessary  which
also treats the case of kernel clustering where H is inﬁnite-dimensional. By Lemma 3.3.5 in [8] 
since X is bounded  it is sufﬁcient to show that for any i  there is some δ > 0 such that

→ 0

{ψi

ˆθ h(·) − ψi

θ0 h(·)}kˆθ−θ0k≤δ h∈X

is a Donsker class  where

ψi

θ h(x) =(cid:26)hθi − φ(x)  φ(h)i x ∈ Cθ i

otherwise.

0

Intuitively  a set of real functions {f (·)} from X (with any probability distribution D) to R is called
Donsker if it satisﬁes a uniform central limit theorem. Without getting too much into the details 

1A linear operator is automatically continuous in ﬁnite dimensional spaces  not necessarily in inﬁnite di-

mensional spaces.

14

this means that if we sample i.i.d m elements from D  then (f (x1) + . . . + f (xm))/√m converges
in distribution (as m → ∞) to a Gaussian random variable  and the convergence is uniform over all
f (·) in the set  in an appropriately deﬁned sense.
We use the fact that if F and G are Donsker classes  then so are F + G and F · G (see examples
2.10.7 and 2.10.8 in [8]). This allows us to reduce the problem to showing that the following three
function classes  from X to R  are Donsker:

{hθi  φ(h)i}kˆθ−θ0k≤δ h∈X

 

{hφ(·)  φ(h)i}h∈X

 

{1Cθ i(·)}kˆθ−θ0k≤δ.

(32)

Notice that the ﬁrst class is a set of bounded constant functions  while the third class is a set of
indicator functions for all possible clusters. One can now use several tools to show that each class
in Eq. (32) is Donsker. For example  consider a class of real functions on a bounded subset of some
Euclidean space. By Thm. 8.2.1 in [3] (and its preceding discussion)  the class is Donsker if any
function in the class is differentiable to a sufﬁciently high order. This ensures that the ﬁrst class in
Eq. (32) is Donsker  because it is composed of constant functions. As to the second class in Eq. (32) 
the same holds in the case of Bregman divergence clustering (where φ(·) is the identity function) 
because it is then just a set of linear functions. For ﬁnite dimensional kernel clustering  it is enough
to show that {h·  φ(h)i}h∈X is Donsker (namely  the same class of functions after performing the
transformation from X to φ(X )). This is again a set of linear functions in Hk  a subset of some
ﬁnite dimensional Euclidean space  and so it is Donsker. In inﬁnite dimensional kernel clustering 
our class of functions can be written as {k(·  h)}h∈X   where k(· ·) is the kernel function  so it is
Donsker if the kernel function is differentiable to a sufﬁciently high order.

The third class in Eq. (32) is more problematic. By Theorem 8.2.15 in [3] (and its preceding discus-
sion)  it sufﬁces that the boundary of each possible cluster is composed of a ﬁnite number of smooth
surfaces (differentiable to a high enough order) in some Euclidean space. In Bregman divergence
clustering  the clusters are separated by hyperplanes  which are linear functions (see appendix A in
[1])  and thus the class is Donsker. The same holds for ﬁnite dimensional kernel clustering. This
will still be true for inﬁnite dimensional kernel clustering  if we can guarantee that any cluster in
any solution close enough to θ0 in Θ will have smooth boundaries. Unfortunately  this does not hold
in some important cases. For example  universal kernels (such as the Gaussian kernel) are capable
of inducing cluster boundaries arbitrarily close in form to any continuous function  and thus our
line of attack will not work in such cases. In a sense  this is not too surprising  since these kernels
correspond to very ’rich’ hypothesis classes  and it is not clear if a precise characterization of their
stability properties  via central limit theorems  is at all possible.

Summarizing the above discussion  we have shown that for the settings assumed in our theorem  all
three classes in Eq. (32) are Donsker and hence Eq. (31) holds. We now return to deal with the other
ingredients required to apply Thm. E.1.

As to the asymptotic distribution of √m(Ψm − Ψ)(θ0)  since Ψ(θ0) = 0 by assumption  we have
that for any i ∈ {1  . . .   k} 

where x1  . . .   xm is the sample by which Ψm is deﬁned. The r.h.s of Eq. (33) is a sum of identically
distributed  independent random variables with zero mean  normalized by √m. As a result  by the
standard central limit theorem  √m(Ψi
m−Ψi)(θ0) converges in distribution to a zero mean Gaussian
random vector Y   with covariance matrix
Vi =ZCθ0 i

p(x)(φ(x) − θ0 i)(φ(x) − θ0 i)⊤dx.

Moreover  it is easily veriﬁed that Cov(∆i(θ0  x)  ∆i′ (θ0  x)) = 0 for any i 6= i′. Therefore 
√m(Ψm − Ψ)(θ0) converges in distribution to a zero mean Gaussian random vector  whose co-
variance matrix V is composed of k diagonal blocks (V1  . . .   Vk)  all other elements of V being
zero.

Thus  we can use Thm. E.1 to get that √m(ˆθ−θ0) converges in distribution to a zero mean Gaussian
random vector of the form − ˙Ψ−1
the form ˙Ψ−1
θ0

Y   which is a Gaussian random vector with a covariance matrix of

V ˙Ψ−1
θ0

θ0

.

15

√m(Ψi

m − Ψi)(θ0) =

1
√m

m

Xj=1

∆i(θ0  xj).

(33)

F Proof of Thm. 4

Since our algorithm returns a locally optimal solution with respect to the differentiable log-
likelihood function  we can frame it as a Z-estimator of the derivative of the log-likelihood function
with respect to the parameters  namely the score function

Ψm(ˆθ) =

1
m

∂
∂θ

log(q(xi|ˆθ)).

m

Xi=1

This is a random mapping based on the sample x1  . . .   xm.
Similarly  we can deﬁne Ψ(·) as the ’asymptotic’ score function with respect to the underlying
distribution D:

Ψ(ˆθ) =ZX

∂
∂θ

log(q(x|ˆθ))p(x)dx.

Under the assumptions we have made  the model ˆθ returned by the algorithm satisﬁes Ψm(ˆθ) = 0 
and ˆθ converges in probability to some θ0 for which Ψ(θ0) = 0. The asymptotic normality of
√m(ˆθ − θ0) is now an immediate consequence of central limit theorems for ’maximum likelihood’

Z-estimators  such as Thm. 5.21 in [7].

References
[1] A. Banerjee  S. Merugu  I. S. Dhillon  and J. Ghosh. Clustering with bregman divergences. Journal of

Machine Learning Research  6:1705–1749  2005.

[2] P. Billingsley and F. Topsøe. Uniformity in weak convergence. Probability Theory and Related Fields 

7:1–16  1967.

University Press  1999.

[3] R. Dudley. Uniform Central Limit Theorems. Cambridge Studies in Advanced Mathematics. Cambridge

[4] G. R. Grimmet and D. R. Stirzaker. Probability and Random Processes. Oxford University Press  2001.
[5] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American

[6] R. R. Rao. Relations betwen weak and uniform convergence of measures with applications. The Annals of

Statistical Association  58(301):13–30  Mar. 1963.

Mathematical Statistics  33(2):659–680  June 1962.

[7] A. W. V. D. Vaart. Asymptotic Statistics. Cambridge University Press  1998.
[8] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes : With Applications to

Statistics. Springer  1996.

16

,Ju Xu
Zhanxing Zhu