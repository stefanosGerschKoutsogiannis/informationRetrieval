2018,Limited Memory Kelley's Method Converges for Composite Convex and Submodular Objectives,The original simplicial method (OSM)  a variant of the classic Kelley’s cutting plane method  has been shown to converge to the minimizer of a composite convex and submodular objective  though no rate of convergence for this method was known. Moreover  OSM is required to solve subproblems in each iteration whose size grows linearly in the number of iterations.  We propose a limited memory version of Kelley’s method (L-KM) and of OSM that requires limited memory (at most n+ 1 constraints for an n-dimensional problem) independent of the iteration. We prove convergence for L-KM when the convex part of the objective g is strongly convex and show it converges linearly when g is also smooth. Our analysis relies on duality between minimization of the composite convex and submodular objective and minimization of a convex function over the submodular base polytope.  We introduce a limited memory version  L-FCFW  of the Fully-Corrective Frank-Wolfe (FCFW) method with approximate correction  to solve the dual problem. We show that L-FCFW and L-KM are dual algorithms that produce the same sequence of iterates; hence both converge linearly (when g is smooth and strongly convex) and with limited memory.  We propose L-KM to minimize composite convex and submodular objectives; however  our results on L-FCFW hold for general polytopes and may be of independent interest.,Limited memory Kelley’s Method Converges for
Composite Convex and Submodular Objectives

Song Zhou

Cornell University

sz557@cornell.edu

Swati Gupta

Georgia Institute of Technology

swatig@gatech.edu

Madeleine Udell
Cornell University

udell@cornell.edu

Abstract

The original simplicial method (OSM)  a variant of the classic Kelley’s cutting
plane method  has been shown to converge to the minimizer of a composite convex
and submodular objective  though no rate of convergence for this method was
known. Moreover  OSM is required to solve subproblems in each iteration whose
size grows linearly in the number of iterations. We propose a limited memory
version of Kelley’s method (L-KM) and of OSM that requires limited memory (at
most n + 1 constraints for an n-dimensional problem) independent of the iteration.
We prove convergence for L-KM when the convex part of the objective (g) is
strongly convex and show it converges linearly when g is also smooth. Our analysis
relies on duality between minimization of the composite objective and minimization
of a convex function over the corresponding submodular base polytope. We
introduce a limited memory version  L-FCFW  of the Fully-Corrective Frank-
Wolfe (FCFW) method with approximate correction  to solve the dual problem.
We show that L-FCFW and L-KM are dual algorithms that produce the same
sequence of iterates; hence both converge linearly (when g is smooth and strongly
convex) and with limited memory. We propose L-KM to minimize composite
convex and submodular objectives; however  our results on L-FCFW hold for
general polytopes and may be of independent interest.

1

Introduction

One of the earliest and fundamental methods to minimize non-smooth convex objectives is Kelley’s
method  which minimizes the maximum of lower bounds on the convex function given by the
supporting hyperplanes to the function at each previously queried point. An approximate solution
to the minimization problem is found by minimizing this piecewise linear approximation  and the
approximation is then strengthened by adding the supporting hyperplane at the current approximate
solution [11  6]. Many variants of Kelley’s method have been analyzed in the literature [16  12  8 
for e.g.]. Kelley’s method and its variants are a natural ﬁt for problem involving a piecewise linear
function  such as composite convex and submodular objectives. This paper deﬁnes a new limited
memory version of Kelley’s method adapted to composite convex and submodular objectives  and
establishes the ﬁrst convergence rate for such a method  solving the open problem proposed in [2  3].
Submodularity is a discrete analogue of convexity and has been used to model combinatorial con-
straints in a wide variety of machine learning applications  such as MAP inference  document
summarization  sensor placement  clustering  image segmentation [3  and references therein]. Sub-
modular set functions are deﬁned with respect to a ground set of elements V   which we may identify
with {1  . . .   n} where |V | = n. These functions capture the property of diminishing returns:
F : {0  1}n ! R is said to be submodular if F (A [{ e})  F (A)  F (B [{ e})  F (B) for
all A ✓ B ✓ V   e /2 B. Lovász gave a convex extension f : [0  1]n ! R of the submodular
set functions which takes the value of the set function on the vertices of the [0  1]n hypercube  i.e.
f (1S) = F (S)  where 1S is the indicator vector of the set S ✓ V [17]. (See Section 2 for details.)
32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In this work  we propose a variant of Kelley’s method  LIMITED MEMORY KELLEY’S METHOD
(L-KM)  to minimize the composite convex and submodular objective

minimize

g(x) + f (x) 

(P )
where g : Rn ! R is a closed strongly convex proper function and f : Rn ! R is the Lovász
extension (see Section 2 for details) of a given submodular function F : 2|E| ! R. Composite convex
and submodular objectives have been used extensively in sparse learning  where the support of the
model must satisfy certain combinatorial constraints. L-KM builds on the ORIGINAL SIMPLICIAL
METHOD (OSM)  proposed by Bach [3] to minimize such composite objectives. At the ith iteration 
OSM approximates the Lovász extension by a piecewise linear function f(i) whose epigraph is the
maximum of the supporting hyperplanes to the function at each previously queried point. It is natural
to approximate the submodular part of the objective by a piecewise linear function  since the Lovász
extension is piecewise linear (with possibly an exponential number of pieces). OSM terminates once
the algorithm reaches the optimal solution  in contrast to a subgradient method  which might continue
to oscillate. Contrast OSM with Kelley’s Method: Kelley’s Method approximates the full objective
function using a piecewise linear function  while OSM only approximates the Lovász extension f
and uses the exact form of g. In [3]  the authors show that OSM converges to the optimum; however 
no rate of convergence is given. Moreover  OSM maintains an approximation of the Lovász extension
by maintaining a set of linear constraints whose size grows linearly with the number of iterations.
Hence the subproblems are harder to solve with each iteration.
This paper introduces L-KM  a variant of OSM that uses no more than n + 1 linear constraints
in each approximation f(i) (and often  fewer) and provably converges when g is strongly convex.
When in addition g is smooth  our new analysis of L-KM shows that it converges linearly  and  as a
corollary  that OSM also converges linearly  which was previously unknown.
To establish this result  we introduce the algorithm L-FCFW to solve a problem dual to (P ):

maximize h(w)
subject to w 2 B(F ) 

(D)

where h : Rn ! R is a smooth concave function and B(F ) ⇢ Rn is the submodular base polytope
corresponding to a given submodular function F (deﬁned below). We show L-FCFW is a limited
memory version of the FULLY-CORRECTIVE FRANK-WOLFE (FCFW) method with approximate
correction [15]  and hence converges linearly to a solution of (D).
We show that L-KM and L-FCFW are dual algorithms in the sense that both algorithms produce
the same sequence of primal iterates and lower and upper bounds on the objective. This connection
immediately implies that L-KM converges linearly. Furthermore  when g is smooth as well as
strongly convex  we can recover the dual iterates of L-FCFW from the primal iterates of L-KM.
Related Work: The Original Simplicial Method was proposed by Bach (2013) [3]. As mentioned
earlier  it converges ﬁnitely but no known rate of convergence was known before the present work.
In 2015  Lacoste-Julien and Jaggi proved global linear convergence of variants of the Frank-Wolfe
algorithm  including the Fully Corrective Frank-Wolfe (FCFW) with approximate correction [15].
L-FCFW  proposed in this paper  can be shown to be a limited memory special case of the latter 
which proves linear convergence of both L-KM and OSM.
Many authors have studied convergence guarantees and reduced memory requirements for variants
of Kelley’s method [11  6]. These variants are computationally disadvantaged compared to OSM
unless these variants allow approximation of only part of the objective. Among the earliest work on
bounded storage in proximal level bundle methods is a paper by Kiwiel (1995) [12]. This method
projects iterates onto successive approximations of the level set of the objective; however  unlike our
method  it is sensitive to the choice of parameters (level sets) and oblivious to any simplicial structure:
iterates are often not extreme points of the epigraph of the function. Subsequent work on the proximal
setup uses trust regions  penalty functions  level sets  and other more complex algorithmic tools;
we refer the reader to [18] for a survey on bundle methods. For the dual problem  a paper by Von
Hohenbalken (1977) [24] shares some elements of our proof techniques. However  their results
only apply to differentiable objectives and do not bound the memory. Another restricted simplicial
decomposition method was proposed by Hearn et. al. (1987) [10]  which limits the constraint set
by user-deﬁned parameters (e.g.  r = 1 reduces to the Frank-Wolfe algorithm [9]): it can replace an
atom with minimal weight in the current convex combination with a prior iterate of the algorithm 

2

which may be strictly inside the feasible region. In contrast  L-FCFW obeys a known upper bound
(n + 1) on the number of vertices  and hence requires no parameter tuning.
Applications: Composite convex and submodular objectives have gained popularity over the last few
years in a large number of machine learning applications such as structured regularization or empirical
risk minimization [4]: minw2RnPi l(yi  w>xi) + ⌦(w)  where w are the model parameters and
⌦: Rn ! R is a regularizer. The Lovász extension can be used to obtain a convex relaxation of a
regularizer that penalizes the support of the solution w to achieve structured sparsity  which improves
model interpretable or encodes knowledge about the domain. For example  fused regularization
uses ⌦(w) =Pi |wi  wi+1|  which is the Lovász extension of the generalized cut function  and
group regularization uses ⌦(w) =Pg dgkwgk1  which is the Lovász extension of the coverage

submodular function. (See Appendix A  Table 1 for details on these and other submodular functions.)
Furthermore  minimizing a composite convex and submodular objective is dual to minimizing a
convex objective over a submodular polytope (under mild conditions). This duality is central to
the present work. First-order projection-based methods like online stochastic mirror descent and
its variants require computing a Bregman projection min{!(x) + r!(y)>(x  y) : x 2 P} to
minimize a strictly convex function ! : Rn ! R over the set P ✓ Rn. Computing this projection is
often difﬁcult  and prevents practical application of these methods  though this class of algorithms is
known to obtain near optimal convergence guarantees in various settings [20  1]. Using L-FCFW
to compute these projections can reduce the memory requirements in variants of online mirror
descent used for learning over spanning trees to reduce communication delays in networks  [13]) 
permutations to model scheduling delays [26]  and k-sets for principal component analysis [25] 
to give a few examples of submodular online learning problems. Other example applications of
convex minimization over submodular polytopes include computation of densest subgraphs [19] 
computation of a lower bound for the partition function of log-submodular distributions [7] and
distributed routing [14].
Summary of contributions: We discuss background and the problem formulations in Section 2.
Section 3 describes L-KM  our proposed limited memory version of OSM  and shows that L-KM
converges and solves a problem over Rn using subproblems with at most n + 1 constraints. We
introduce duality between our primal and dual problems in Section 4. Section 5 introduces a limited
memory (and hence faster) version of Fully-Corrective Frank-Wolfe  L-FCFW  and proves linear
convergence of L-FCFW. We establish the duality between L-KM and L-FCFW in Appendix E
and thereby show L-KM achieves linear convergence and L-FCFW solves subproblems over no
more than n + 1 vertices. We present preliminary experiments in Section 7 that highlight the reduced
memory usage of both L-KM and L-FCFW and show that their performance compares favorably
with OSM and FCFW respectively.

2 Background and Notation
Consider a ground set V of n elements on which the submodular function F : 2V ! R is deﬁned. The
function F is said to be submodular if F (A) + F (B)  F (A[ B) + F (A\ B) for all A  B ✓ V .
This is equivalent to the diminishing marginal returns characterization mentioned before. Without loss
of generality  we assume F (;) = 0. For x 2 Rn  A ✓ V   we deﬁne x(A) =Pk 2 A x(k) = 1>Ax 
where 1A 2 Rn is the indicator vector of A  and let both x(k) and xk denote the kth element of x.
Given a submodular set function F : V ! R  the submodular polyhedron and the base polytope
are deﬁned as P (F ) = {w 2 Rn : w(A)  F (A)  8 A ✓ V }  and B(F ) = {w 2 Rn : w(V ) =
F (V )  w 2 P (F )}  respectively. We use vert(B(F )) to denote the vertex set of B(F ). The Lovász
extension of F is the piecewise linear function [17]

f (x) = max

w 2 B(F )

w>x.

(1)

The Lovász extension can be computed using Edmonds’ greedy algorithm for maximizing linear
functions over the base polytope (in O(n log n + n) time  where  is the time required to compute
the submodular function value). This extension can be deﬁned for any set function  however it is
convex if and only if the set function is submodular [17]. We call a permutation ⇡ over [n] consistent1

1Therefore  the Lovász extension can also be written as f (x) = Pk x⇡k [F ({⇡1 ⇡ 2  . . .  ⇡ k}) 
F ({⇡1 ⇡ 2  . . .  ⇡ k1})] where ⇡ is a permutation consistent with x and F (;) = 0 by assumption.

3

with x 2 Rn if x⇡i  x⇡j whenever i  j. Each permutation ⇡ corresponds to an extreme point
x⇡k = F ({⇡1 ⇡ 2  . . .  ⇡ k})  F ({⇡1 ⇡ 2  . . .  ⇡ k1}) of the base polytope. For x 2 Rn  let V(x)
be the set of vertices B(F ) that correspond to permutations consistent with x. Note that

@f (x) = conv(V(x)) = argmax
w 2 B(F )

w>x 

(2)

where @f (x) is the subdifferential of f at x and conv(S) represents the convex hull of the set S.
We assume all convex functions in this paper are closed and proper [21]. Given a convex function
g : Rn ! R  its Fenchel conjugate g⇤ : Rn ! R is deﬁned as

g⇤(w) = max
x 2 Rn

w>x  g(x).

(3)

Note that when g is strongly convex  the right hand side of (3) always has an unique solution  so g⇤ is
deﬁned for all w 2 Rn. Fenchel conjugates are always convex  regardless of the convexity of the
original function. Since we assume g is closed  g⇤⇤ = g. Fenchel conjugates satisfy (@g)1 = @g⇤
in the following sense:

w 2 @g(x) () g(x) + g⇤(w) = w>x () x 2 @g⇤(w) 

(4)
where @g(x) is the subdifferential of g at x. When g is ↵-strongly convex and -smooth  g⇤ is
1/-strongly convex and 1/↵-smooth [21  Section 4.2]. (See Appendix A.2 for details.)
Proofs of all results that do not follow easily from the main text can be found in the appendix.

3 Limited Memory Kelley’s Method

We now present our novel limited memory adaptation L-KM of the Original Simplicial Method
(OSM). We ﬁrst brieﬂy review OSM as proposed by Bach [3  Section 7.7] and discuss problems
of OSM with respect to memory requirements and the rate of convergence. We then highlight the
changes in OSM  and verify that these changes will enable us to show a bound on the memory
requirements while maintaining ﬁnite convergence. Proofs omitted from this section can be found in
Appendix C.
Original Simplicial Method: To solve the primal problem (P )  it is natural to approximate the piece-
wise linear Lovász extension f with cutting planes derived from the function values and subgradients
of the function at previous iterations  which results in piecewise linear lower approximations of f.
This is the basic idea of OSM introduced by Bach in [3]. This approach contrasts with Kelley’s
Method  which approximates the entire objective function g + f. OSM adds a cutting plane to
the approximation of f at each iteration  so the number of the linear constraints in its subproblems
grows linearly with the number of iterations.2 Hence it becomes increasingly challenging to solve
the subproblem as the number of iterations grows up. Further  in spite of a ﬁnite convergence  as
mentioned in the introduction there was no known rate of convergence for OSM or its dual method
prior to this work.
Limited Memory Kelley’s Method: To address these two challenges — memory requirements and
unknown convergence rate — we introduce and analyze a novel limited memory version L-KM of
OSM which ensures that the number of cutting planes maintained by the algorithm at any iteration is
bounded by n + 1. This thrift bounds the size of the subproblems at any iteration  thereby making
L-KM cheaper to implement. We describe L-KM in detail in Algorithm 1.
L-KM and OSM differ only in the set of vertices V (i) considered at each step: L-KM keeps only
those vectors w 2V (i1) that maximize w>x(i)  whereas OSM keeps every vector in V (i1).
We state some properties of L-KM here with proofs in Appendix C. We will revisit many of these
properties later via the lens of duality.
The sets V (i) in L-KM are afﬁnely independent  which shows the size of the subproblems is bounded.
Theorem 1. For all i  0  vectors in V (i) are afﬁnely independent. Moreover  |V (i)| n + 1.

2 Concretely  we obtain OSM from Algorithm 1 by setting V (i) = V (i1) [{ v(i)} in step 6.

4

Algorithm 1 L-KM: The Limited Memory Kelley’s Method for (P )
Require: strongly convex function g : Rn ! R  submodular function F : 2n ! R  tolerance ✏  0
Ensure: ✏-suboptimal solution x] to (P )
1: initialize: choose x(0) 2 Rn  set ; ⇢V (0) ✓ vert(B(F )) afﬁnely independent
2: for i = 1  2  . . . do
3:

Convex subproblem. Deﬁne approximation f(i)(x) = max{w>x : w 2V (i1)} and solve

x(i) = argmin g(x) + f(i)(x).

4:

Submodular subproblem. Compute value and subgradient of f at x(i)

5:

6:

f (x(i)) = max
w2B(F )

w>x(i) 

v(i) 2 @f (x(i)) = argmax
w2B(F )

w>x(i).

Stopping condition. Break if duality gap p(i)  d(i)  ✏  where

p(i) = g(x(i)) + f (x(i)) 

d(i) = g(x(i)) + f(i)(x(i)).

Update memory. Identify active set A(i) and update memory V (i):

A(i) = {w 2V (i1) : w>x(i) = f(i)(x(i))} 

V (i) = A(i) [{ v(i)}.

7: return x(i)

Figure 1: An illustration of L-KM (a)-(d) (left to right): blue curve marked with ⇥ denotes the ith function
approximation g + f(i). In (d)  note that L-KM approximation g + f(4) is obtained by dropping the leftmost
constraint in g + f(3) (in (c))  unlike OSM.
L-KM may discard pieces of the lower approximation f(i) at each iteration. However  it does so
without any adverse affect on the solution to the current subproblem:
Lemma 1. The convex subproblem (in Step 3 of algorithm L-KM) has the same solution and optimal
value over the new active set A(i) as over the memory V (i1):

x(i) = argmin g(x) + max{w>x : w 2V (i1)} = argmin g(x) + max{w>x : w 2A (i)}.

Lemma 1 shows that L-KM remembers the important information about the solution  i.e. only the
tight subgradients  at each iteration. Note that at the ith iteration  the solution x(i) is unique by the
strong convexity of g  and thus we can improve the lower bound d(i) since new information (i.e. v(i))
is added:
Corollary 1. The sequence {d(i)} constructed by L-KM form strictly increasing lower bounds on
the value of (P ): d(1) < d(2) < ··· p? = minx 2 Rn f (x) + g(x).
Remark. It is easy to see that the sequence {p(i)} constructed by L-KM form upper bounds of p? 
hence by Corollary 1  {p(i)  d(i)} form valid optimality gaps for L-KM.
Corollary 2. L-KM does not stall: for any iterations i1 6= i2  we solve subproblems over a distinct
set of vertices V (i1) 6= V (i2).
We can strengthen Corollary 2 and show L-KM in fact converges to the exact solution in ﬁnite
iterations:
Theorem 2. L-KM (Algorithm 1) terminates after ﬁnitely many iterations. Moreover  for any given
✏  0  suppose L-KM terminates when i = i✏  then p? + ✏  p(i✏)  p? and p?  d(i✏)  p?  ✏.

5

In particular  when we choose ✏ = 0  we have p(i0) = p? = d(i✏)  and x(i✏) is the unique optimal
solution to (P ).

In this section  we have shown that L-KM solves a series of limited memory convex subproblems with
no more than n + 1 linear constraints  and produces strictly increasing lower bounds that converge to
the optimal value.

4 Duality
L-KM solves a series of subproblems parametrized by the sets V✓ vert(B(F )):

(PV)
Notice that when V = vert(B(F ))  we recover (P ). We now analyze these subproblems via duality.
The Lagrangian of this problem with dual variables v for v 2V is 

v 2V

minimize
g(x) + t
subject to t  v>x 

L(x  t  ) = g(x) + t +Xv2V

v(v>x  t).

The pair ((x  t)  ) are primal-dual optimal for this problem iff they satisfy the KKT conditions [5]:

• Optimality.

0 2 @xL(x  t  ) =) Xv2V

vv 2 @g(x) 

0 =

d

dtL(x  t  ) =) Xv2V

v = 1.

• Primal feasibility. t  v>x for each v 2V .
• Dual feasibility. v  0 for each v 2V .
• Complementary slackness. v(v>x  t) = 0 for each v 2V .

The requirement that  lie in the simplex emerges naturally from the optimality conditions  and

reduces the Lagrangian to L(x  ) = g(x) + (Pv2V vv)>x. One can introduce the variable
w =Pv2V vv 2 conv(V)  which is dual feasible so long as w 2 conv(V). We can rewrite the
Lagrangian in terms of x and w 2 conv(V) as L(x  w) = g(x) + w>x. Minimizing L(x  w) over x 
we obtain the dual problem
(DV)
Note (D) is the same as (DV) if V = vert(B(F )) and h(w) = g⇤(w)  the Fenchel conjugate of
g. Notice that g⇤ is smooth if g is strongly convex (Lemma 4 in Appendix A.2).
Theorem 3 (Strong Duality). The primal problem (PV) and the dual problem (DV) have the same
ﬁnite optimal value.

maximize g⇤(w)
subject to w 2 conv(V).

By analyzing the KKT conditions  we obtain the following result  which we will used later in the
design of our algorithms.
Lemma 2. Suppose (x  ) solve ((PV)  (DV)) and t = maxv2V v>x. By complementary slackness 

and in particular  {v : v > 0}✓{

v : v>x = t}.

v > 0 =) v>x = t 

Notice {v : v>x = t} is the active set of L-KM. We will see {v : v > 0} is the (minimal) active set
of the dual method L-FCFW. (If strict complementary slackness holds  these sets are the same.)
The ﬁrst KKT condition shows how to move between primal and dual optimal variables.
Theorem 4. If g : Rn ! R is strongly convex and w? solves (DV)  then

x? = (@g)1(w?) = rg⇤(w?)

(5)

solves (PV). If in addition g is smooth and x? solves (PV)  then w? = rg(x?) solves (DV).

6

Algorithm 2 L-FCFW: Limited Memory Fully Corrective Frank Wolfe for (D)
Require: smooth concave function h : Rn ! R  submodular function F : 2n ! R  tolerance ✏  0
Ensure: ✏-suboptimal solution w] to (D)
1: initialize: set ; ⇢V (0) ✓ vert(B(F ))
2: for i = 1  2  . . . do
3:

Convex subproblem. Solve

w(i) = argmax{h(w) : w 2 conv(V (i1))}.

For each v 2V (i)  deﬁne v  0 so that w(i) =Pv2V(i) vv andPv2V(i) v = 1.
Submodular subproblem. Compute gradient x(i) = rh(w(i)) and solve

v(i) = argmax{w>x(i) : w 2 B(F )}.

Stopping condition. Break if duality gap p(i)  d(i)  ✏  where

p(i) = (v(i))>x(i) 

d(i) = (w(i))>x(i).

Update memory. Identify a supserset of active vertices B(i) and update memory V (i):

B(i) ◆{ w 2V (i1) : w > 0}

V (i) = B(i) [{ v(i)}.

4:

5:

6:

7: return w(i)

Proof. Check the optimality conditions to prove the result. By deﬁnition  x? satisﬁes the ﬁrst
optimality condition. To check complementary slackness  we rewrite the condition as

v(v>x  t) = 0 8v 2V () Xv2V

vv!>

x = t () w>x = max
v2V

v>x.

Notice (w?)>(rg⇤(w?)) = maxv2V v>(rg⇤(w?)) by optimality of w?  since v  w? is a
feasible direction for any v 2V   proving x? = rg⇤(w?) solves (PV).
That the primal optimal variable yields a dual optimal variable via w? = rg(x?) follows from a
similar argument together with ideas from the proof of strong duality in Appendix D.

5 Solving the dual problem
Let’s return to the dual problem (D): maximize a smooth concave function h(w) = g⇤(w) over
the polytope B(F ) ✓ Rn. Linear optimization over this polytope is easy; hence a natural strategy
is to use the Frank-Wolfe method or one of its variants [15]. However  since the cost of each linear
minimization is not negligible  we will adopt a Frank-Wolfe variant that makes considerable progress
at each iteration by solving a subproblem of moderate complexity: LIMITED MEMORY FULLY
CORRECTIVE FRANK-WOLFE (L-FCFW  Algorithm 2)  which at every iteration exactly minimizes
the function g⇤(w) over the the convex hull of the current subset of vertices V (i). Here we
overload notation intentionally: when g is smooth and strongly convex  we will see that we can
choose the set of vertices V (i) in L-FCFW (Algorithm 2) so that the algorithm matches either L-KM
or OSM depending on the choice of B(i) (Line 6 of L-FCFW). For details of the duality between
L-KM and L-FCFW see Section 6.

Limited memory.
In L-FCFW  we may choose any active set B(i) ◆{ w 2V (i1) : w > 0}.
When B(i) = V (i1)  we call the algorithm (vanilla) FCFW. When B(i) is chosen to be small  we
call the algorithm LIMITED MEMORY FCFW (L-FCFW). Standard FCFW increases the size of the
active set at each iteration  whereas the most limited memory variant of L-FCFW uses only those
vertices needed to represent the iterate w(i).
Moreover  recall Carathéodory’s theorem (see e.g. [23]): for any set of vectors V  if x 2 conv(V) ✓
Rn  then there exists a subset A ✓V with |A| n + 1 such that x 2 conv(A). Hence we see we can
choose B(i) to contain at most n + 1 vertices at each iteration (hence n + 2 in V (i))  or even fewer if
the iterate lies on a low-dimensional face of B(F ). (The size of B(i) may depend on the solver used

7

for (DV); to reduce the size of B(i)  we can minimize a random linear objective over the optimal set
of (DV) as in [22].)
Linear convergence. Lacoste-Julien and Jaggi [15] show that FCFW converges linearly to an
✏-suboptimal solution when g is smooth and strongly convex so long as the active set B(i) and iterate
x(i) satisfy three conditions they call approximate correction(✏):

1. Better than FW. h(y(i))  min 2[0 1] h((1  )w(i1) + v(i1))).
2. Small away-step gap. max{(w(i)  v)>x(i) : v 2V (w(i))} ✏  where V(w(i)) = {v 2

V (i1) : v > 0}.

3. Representation. x(i) 2 conv(B(i)).

By construction  iterates of L-FCFW always satisfy these conditions with ✏ = 0:

1. Better than FW. For any  2 [0  1]  w = (1  )w(i1) + v(i1) is feasible.
2. Zero away-step gap. For each v 2V (i)  if w(i) = v  then clearly (w(i)  v)>(x(i)) = 0.
otherwise (if w(i) 6= v) v  w(i) is a feasible direction  and so by optimality of w(i)
(w(i)  v)>(x(i))  0.

3. Representation. We have w(i) 2 conv(B(i)) by construction of B(i).

Hence we have proved Theorem 5:
Theorem 5. Suppose g is ↵-smooth and -strongly convex. Let M be the diameter of B(F ) and
 be the pyramidal width3 of P   then the lower bounds d(i) in L-FCFW (Algorithm 2) converges
linearly at the rate of 1  ⇢  i.e. p?  d(i+1)  (1  ⇢)(p?  d(i))  where ⇢ = 
Primal-from-dual algorithm. Recall that dual iterates yield primal iterates via Theorem 4. Hence
the gradients x(i) = rg⇤(w(i)) computed by L-FCFW converge linearly to the solution x? of
(P ). However  it is difﬁcult to run L-FCFW directly to solve (D) given only access to g  since in that
case computing g⇤ and its gradient requires solving another optimization problem; moreover  we will
see below that L-KM computes the same iterates. See Appendix E for more discussion.

4↵ ( 

M )2.

6 L-KM (and OSM) converge linearly

L-KM (Algorithm 1) and L-FCFW (Algorithm 2) are dual algorithms in the following strong sense:
Theorem 6. Suppose g is ↵-smooth and -strongly convex. In L-FCFW (Algorithm 2)  suppose we
choose B(i) = A(i) = {v 2V (i1) : v>x(i) = w(i)>x(i)}. Then
1. The primal iterates x(i) of L-KM and L-FCFW match.
2. The sets V (i) used at each iteration of L-KM and L-FCFW match.
3. The upper and lower bounds p(i) and d(i) of L-KM and L-FCFW match.
Corollary 3. The active sets of L-FCFW can be chosen to satisfy |B(i)| n + 1.
Theorem 7. Suppose g is ↵-strongly convex and let M be the diameter of B(F )  the duality gap
p(i)  d(i) in L-KM (Algorithm 1) converges linearly: p(i)  d(i)  (p?  d(i)) + M 2/(2) when
(p?  d(i))  M 2/(2) and p(i)  d(i)  Mp2(p?  d(i))/ otherwise. Note that p?  d(i)
converges linearly by Theorem 5.
When g is smooth and strongly convex  OSM and vanilla FCFW are dual algorithms in the same
sense when we choose B(i) = V (i1). For details of the duality between OSM and L-FCFW see
Appendix G. Hence we have a similar convergence result for OSM:

3See Appendix H for deﬁnitions of the diameter and pyramidal width.

8

(a)

(b)

(c)

(d)

Figure 2: Dimension n = 10 in (a)  n = 100 in (b)  (c) and (d). The methods converged in (a)  (b)  (c) and (d).

(a)

(b)

(c)

Figure 3: L-FCFW and FCFW converged in all plots  FW with away steps has converged in (b)  (c) and (d).
Theorem 8. Suppose g is ↵-strongly convex and let M be the diameter of B(F )  the duality gap
p(i)  d(i) in OSM converges linearly: p(i)  d(i)  (p?  d(i)) + M 2/(2) when (p?  d(i)) 
M 2/(2) and p(i)  d(i)  Mp2(p?  d(i))/ otherwise.
Remark. Note that p?  d(i) converges linearly by Theorem 5  Theorem 7 and Theorem 7 imply
L-KM and OSM converge linearly when g is smooth and strongly convex.

Moreover  this connection generates a new way to prune the active set of L-KM even further using a
primal dual solver: we may use any active set B(i) ◆{ w 2V (i1) : w > 0}  where  2 R|V (i1)| is
a dual optimal solution to (PV). When strict complementary slackness fails  we can have B(i) ⇢A (i).
7 Experiments and Conclusion

2

We present in this section a computational study: we minimize non-separable composite functions
g + f where g(x) = x>(A + nIn)x + b>x for x 2 Rn  and f is the Lovász extension of the
submodular function F (A) = |A|(2n|A|+1)
for A ✓ [n]. To construct g(·)  entries of A 2 Mn and
b 2 Rn were randomly sampled from U [1  1]n⇥n  and U [0  n]n respectively. In is an n⇥ n identity
matrix. We remark that L-KM converges so quickly that the bound on the size of the active set is less
important  in practice  than the fact that the active set need not grow at every iteration.
Primal convergence: We ﬁrst solve a toy problem for n = 10 and show that the number of constraints
does not exceed n + 1. Note that the number of constraints might oscillate before it reaches n + 1
(Fig. 2(a)). We next compare the memory used in each iteration (Fig. 2(b))  the optimality gap per
iteration (Fig. 2(c))  and the running time (Fig. 2(d)) of L-KM and OSM by solving the problem
for n = 100 up to accuracy of 105 of the optimal value. Note that L-KM uses much less memory
compared to OSM  converges at almost the same rate in iterations  and its running time per iteration
improves as the iteration count increases.
Dual convergence: We compare the convergence of L-FCFW  FCFW and Frank-Wolfe with away
steps for the dual problem maxw 2 B(F ) (w  b)>(A + nIn)1(w  b) for n = 100 up to
relative accuracy of 105. L-FCFW maintains smaller sized subproblems (Figure (3)(a))  and it
converges faster than FCFW as the number of iteration increases (Figure (3)(d)). Their provable
duality gap converges linearly in the number of iterations. Moreover  as shown in Figures (3)(b) and
(c)  L-FCFW and FCFW return better approximate solutions than Frank-Wolfe with away steps
under the same optimality gap tolerance.
Conclusion This paper deﬁnes a new limited memory version of Kelley’s method adapted to compos-
ite convex and submodular objectives  and establishes the ﬁrst convergence rate for such a method 

9

solving the open problem proposed in [2  3]. We show bounds on the memory requirements and
convergence rate  and demonstrate compelling performance in practice.

Acknowledgments
This work was supported in part by DARPA Award FA8750-17-2-0101. A part of this work was
done while the ﬁrst author was at the Department of Mathematical Sciences  Tsinghua University and
while the second author was visiting the Simons Institute  UC Berkeley. The authors would also like
to thank Sebastian Pokutta for invaluable discussions on the Frank-Wolfe algorithm and its variants.

References
[1] J. Audibert  S. Bubeck  and G. Lugosi. Regret in online combinatorial optimization. Mathematics of

Operations Research  39(1):31–45  2013.

[2] Francis Bach. Duality between subgradient and conditional gradient methods. SIAM Journal on Optimiza-

tion  25(1):115–129  2015.

[3] Francis Bach et al. Learning with submodular functions: A convex optimization perspective. Foundations

and Trends R in Machine Learning  6(2-3):145–373  2013.

[4] Francis R Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural

Information Processing Systems  pages 118–126  2010.

[5] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press  2009.

[6] E Ward Cheney and Allen A Goldstein. Newton’s method for convex programming and Tchebycheff

approximation. Numerische Mathematik  1(1):253–268  1959.

[7] Josip Djolonga and Andreas Krause. From MAP to marginals: Variational inference in bayesian submodular

models. In Advances in Neural Information Processing Systems  pages 244–252  2014.

[8] Yoel Drori and Marc Teboulle. An optimal variant of Kelley’s cutting-plane method. Mathematical

Programming  160(1-2):321–351  2016.

[9] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research Logistics

Quarterly  3(1-2):95–110  1956.

[10] D. Hearn  S. Lawphongpanich  and J. Ventura. Restricted simplicial decomposition: Computation and

extensions. Mathematical Programming Study  31:99–118  1987.

[11] James E Kelley  Jr. The cutting-plane method for solving convex programs. Journal of the Society for

Industrial and Applied Mathematics  8(4):703–712  1960.

[12] Krzysztof C Kiwiel. Proximal level bundle methods for convex nondifferentiable optimization  saddle-point

problems and variational inequalities. Mathematical Programming  69(1-3):89–109  1995.

[13] W. M. Koolen  M. K. Warmuth  and J. Kivinen. Hedging structured concepts. COLT  2010.

[14] Walid Krichene  Syrine Krichene  and Alexandre Bayen. Convergence of mirror descent dynamics in the

routing game. In European Control Conference (ECC)  pages 569–574. IEEE  2015.

[15] Simon Lacoste-Julien and Martin Jaggi. On the global linear convergence of Frank-Wolfe optimization

variants. In Advances in Neural Information Processing Systems  pages 496–504  2015.

[16] Claude Lemaréchal  Arkadii Nemirovskii  and Yurii Nesterov. New variants of bundle methods. Mathe-

matical programming  69(1-3):111–147  1995.

[17] L. Lovász. Submodular functions and convexity. Mathematical Programming: The State of the Art  1983.

[18] Marko Mäkelä. Survey of bundle methods for nonsmooth optimization. Optimization methods and software 

17(1):1–29  2002.

[19] Kiyohito Nagano  Yoshinobu Kawahara  and Kazuyuki Aihara. Size-constrained submodular minimization
through minimum norm base. In Proceedings of the 28th International Conference on Machine Learning
(ICML)  pages 977–984  2011.

10

[20] A. S. Nemirovski and D. B. Yudin. Problem complexity and method efﬁciency in optimization. Wiley-

Interscience  New York  1983.

[21] Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Journal of Applied and

Computational Mathematics  15(1):3–43  2016.

[22] Madeleine Udell and Stephen Boyd. Bounding duality gap for separable problems with linear constraints.

Computational Optimization and Applications  64(2):355–378  2016.

[23] Roman Vershynin. High-dimensional probability: An introduction with applications in data science 

volume 47. Cambridge University Press  2018.

[24] Balder Von Hohenbalken. Simplicial decomposition in nonlinear programming algorithms. Mathematical

Programming  13(1):49–68  1977.

[25] M. K. Warmuth and D. Kuzmin. Randomized PCA algorithms with regret bounds that are logarithmic in

the dimension. In Advances in Neural Information Processing Systems  pages 1481–1488  2006.

[26] S. Yasutake  K. Hatano  S. Kijima  E. Takimoto  and M. Takeda. Online linear optimization over permuta-

tions. In Algorithms and Computation  pages 534–543. Springer  2011.

11

,Song Zhou
Swati Gupta
Madeleine Udell
Shangyu Chen
Wenya Wang
Sinno Jialin Pan