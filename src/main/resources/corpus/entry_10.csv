2010,(RF)^2 -- Random Forest Random Field,We combine random forest (RF) and conditional random field (CRF) into a new computational framework  called random forest random field (RF)^2. Inference of (RF)^2 uses the Swendsen-Wang cut algorithm  characterized by Metropolis-Hastings jumps. A jump from one state to another depends on the ratio of the proposal distributions  and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions  and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a non-parametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)^2. (RF)^2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions. In our empirical evaluation  we use only the visual information provided by image regions (e.g.  color  texture  spatial layout)  whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless  (RF)^2 outperforms the state of the art on benchmark datasets  in terms of accuracy and computation time.,(RF)2 — Random Forest Random Field

Nadia Payet and Sinisa Todorovic

School of Electrical Engineering and Computer Science

Oregon State University

payetn@onid.orst.edu  sinisa@eecs.oregonstate.edu

Abstract

We combine random forest (RF) and conditional random ﬁeld (CRF) into a new
computational framework  called random forest random ﬁeld (RF)2. Inference
of (RF)2 uses the Swendsen-Wang cut algorithm  characterized by Metropolis-
Hastings jumps. A jump from one state to another depends on the ratio of the
proposal distributions  and on the ratio of the posterior distributions of the two
states. Prior work typically resorts to a parametric estimation of these four dis-
tributions  and then computes their ratio. Our key idea is to instead directly es-
timate these ratios using RF. RF collects in leaf nodes of each decision tree the
class histograms of training examples. We use these class histograms for a non-
parametric estimation of the distribution ratios. We derive the theoretical error
bounds of a two-class (RF)2. (RF)2 is applied to a challenging task of multiclass
object recognition and segmentation over a random ﬁeld of input image regions.
In our empirical evaluation  we use only the visual information provided by image
regions (e.g.  color  texture  spatial layout)  whereas the competing methods addi-
tionally use higher-level cues about the horizon location and 3D layout of surfaces
in the scene. Nevertheless  (RF)2 outperforms the state of the art on benchmark
datasets  in terms of accuracy and computation time.

1 Introduction

This paper presents a new computational framework  called random forest random ﬁeld (RF)2 
which provides a principled way to jointly reason about multiple  statistically dependent random
variables and their attributes. We derive theoretical performance bounds of (RF)2  and demonstrate
its utility on a challenging task of conjoint object recognition and segmentation.

Identifying subimage ownership among occurrences of distinct object classes in an image is a fun-
damental  and one of the most actively pursued problem in computer vision  machine learning  and
artiﬁcial intelligence [1–11]. The goal is to assign the label of one of multiple semantic classes to
each image pixel. Our approach builds on the following common recognition strategies: (i) Labels
of neighboring image parts are likely to be correlated – one of the main principles of perceptual
organization; and (ii) Recognized objects dictate which other objects to expect in the scene  and
their scale and spatial conﬁguration – one of the main principles of context-driven recognition that
“binds” all object detections in a coherent scene interpretation. We formalize perceptual grouping
and context by a graphical model aimed at capturing statistical dependencies among random vari-
ables (i.e.  labels or attributes) associated with different pixel neighborhoods. Thus  we derive a
uniﬁed framework for combined object recognition and segmentation  as a graph-structured predic-
tion of all random variables in a single  consistent model of the scene.

The graphical model we use is Conditional Random Field (CRF) [12]—one of the most popular
models for structured inference over pixels [2  3]  patches [4  5]  or image regions [6–8]  for object
recognition and segmentation. CRF deﬁnes a posterior distribution of hidden random variables Y
(labels)  given observed image features X  in a factored form: p(Y |X; θ)= 1

Z(θ) Qc ψc(Yc  X; θ).

1

Each potential ψc is a function over a subset Yc⊆Y   conditioned on X  and parameterized by θ.
The potentials are often deﬁned as linear functions of parameters  ψc(Yc  X; θ)=θTΨc  where Ψc
is the output of some detectors over observables X [2–4]. This means that p(Y |X; θ) is modeled
as a log-linear function  which is not adequate when the detector outputs do not provide a linear
separability of the classes. Learning θ is hard  because computation of the partition function Z(θ)
is intractable for most graphs (except for chains and trees). Inference is typically posed as the joint

q(Y (t)→Y (t+1))

p(Y (t+1)|X)

MAP assignment that minimizes the energyPc ψc(Yc  X; θ)  which is also intractable for general

graphs. The intractability of CRF learning and inference often motivates prior work to resort to
approximate algorithms  e.g.  graph-cuts  and loopy belief propagation (LBP). The effect of these
approximations on the original semantics of CRF is poorly understood. For example  an approximate
inference stuck in a local maximum may not represent the intended consistent scene interpretation.
Motivation: Some of the aforementioned shortcomings can be addressed when CRF inference is
conducted using the Metropolis-Hastings (MH) algorithm. MH draws samples Y (t) from the CRF’s
posterior  p(Y |X)  and thus generates a Markov chain in which state Y (t+1) depends only on the
previous state Y (t). The jumps between the states are reversible  and governed by a proposal density
q(Y (t) → Y (t+1)). The proposal is accepted if the acceptance rate  α  drawn from U (0  1)  satisﬁes
α< min{1  q(Y (t+1)→Y (t))
p(Y (t)|X) }. MH provides strong theoretical guarantees of convergence
to the globally optimal state. As can be seen  the entire inference process is regulated by ratios of
the proposal and posterior distributions. Consequently  the bottleneck of every CRF learning and
inference — namely  computing the partition function Z — is eliminated in MH.
Our key idea is to directly estimate the ratios of the proposal and posterior distributions  instead
of computing each individual distribution for conducting MH jumps. Previous work on MH for
CRFs usually commits to linear forms of the potential functions  and spends computational re-
sources on estimating the four distributions: q(Y (t+1)→Y (t))  q(Y (t)→Y (t+1))  p(Y (t+1)|X)
and p(Y (t)|X).
q(Y (t)→Y (t+1)) and
p(Y (t+1)|X)
p(Y (t)|X)   in a non-parametric manner  since the acceptance rate of MH jumps depends only on
these ratios. To this end  we use the random forests (RF) [13]. Given a training set of labeled ex-
amples  RF grows many decision trees. We view the trees as a way of discriminatively structuring
evidence about the class distributions in the training set. In particular  each leaf of each tree in RF
stores a histogram of the number of training examples from each class that reached that leaf. When
a new example is encountered  it is “dropped” down each of the trees in the forest  until it reaches a
leaf in every tree. The class histograms stored in all these leaves can then be used as a robust esti-
mate of the ratio of that example’s posterior distributions. This is related to recent work on Hough
forests for object detection and localization [14]  where leaves collect information on locations and
sizes of bounding boxes of objects in training images. However  they use this evidence to predict
a spatial distribution of bounding boxes in a test image  whereas we use the evidence stored in tree
leaves to predict the distribution ratios. Evidence trees are also used in [15]  but only as a ﬁrst stage
of a stacked-classiﬁer architecture which replaces the standard majority voting of RF.

In contrast  our goal is to directly estimate the two ratios  q(Y (t+1)→Y (t))

RF is difﬁcult to analyze [13  16]. Regarding consistency of RF  it is known that their rate of con-
vergence to the optimal Bayes’ rule depends only on the number of informative variables. It is also
shown that RF that cuts down to pure leaves uses a weighted  layered  nearest neighbor rule [16].
We are not aware of any theoretical analysis of RF as an estimator of ratios of posterior distributions.
Contributions: We combine RF and CRF into a new  principled and elegant computational frame-
work (RF)2. Learning is efﬁciently conducted by RF which collects the class histograms of training
examples in leaf nodes of each decision tree. This evidence is then used for the non-parametric
estimation of the ratios of the proposal and posterior distributions  required by MH-based inference
of (RF)2. We derive the theoretical error bounds of estimating distribution ratios by a two-class RF 
which is then used to derive the theoretical performance bounds of a two-class (RF)2.
Paper Organization: Sections 2–4 specify the CRF model  its MH-based inference  and RF-based
learning. Sections 5–6 present our experimental evaluation  and theoretical analysis of (RF)2.

2

2 CRF Model

We formulate multiclass object recognition and segmentation as the MAP inference of a CRF  de-
ﬁned over a set of multiscale image regions. Regions are used as image features  because they are
dimensionally matched with 2D object occurrences in the image  and thus facilitate modeling of var-
ious perceptual-organization and contextual cues (e.g.  continuation  smoothness  containment  and
adjacency) that are often used in recognition [6–11]. Access to regions is provided by the state-of-
the-art  multiscale segmentation algorithm of [17]  which detects and closes object (and object-part)
boundaries using the domain knowledge. Since the right scale at which objects occur is unknown 
we use all regions from all scales.

The extracted regions are organized in a graph  G = (V  E)  with V and E are sets of nodes and
edges. The nodes i=1  . . .   N correspond to multiscale segments  and edges (i  j) ∈ E capture
their spatial relations. Each node i is characterized by a descriptor vector  xi  whose elements
describe photometric and geometric properties of the corresponding region (e.g.  color  shape  ﬁlter
responses). A pair of regions can have one of the following relationships: (1) ascendent/descendent 
(2) touching  and (3) far. Since the segmentation algorithm of [17] is strictly hierarchical  region
i is descendent of region j  if i is fully embedded as subregion within ancestor j. Two regions i
and j touch if they share a boundary part. Finally  if i and j are not in the hierarchical and touch
relationships then they are declared as far. Edges connect all node pairs E = V × V   |E| = N 2.
Each edge (i  j) is associated with a tag  eij  indicating the relationship type between i and j.
CRF is deﬁned as the graphical model over G. Let Y = {yi} denote all random variables associated
with the nodes  indicating the class label of the corresponding region  yi ∈ {0  1  . . .   K}  where
K denotes the total number of object classes  and label 0 is reserved for the background class. Let
pi = p(yi|xi) and pij = p(yi  yj|xi  xj   eij) denote the posterior distributions over nodes and pairs
of nodes. Then  we deﬁne CRF as
(1)
Multi-coloring of CRF is deﬁned as the joint MAP assignment Y ∗ = arg maxY p(Y |G). In the
following section  we explain how to conduct this inference.

p(Y |G) = Qi∈V p(yi|xi)Q(i j)∈E p(yi  yj|xi  xj   eij) = Qi∈V piQ(i j)∈E pij .

3 CRF Inference

For CRF inference  we use the Swendsen-Wang cut algorithm (SW-cut)  presented in [18]. SW-
cut iterates the Metropolis-Hastings (MH) reversible jumps through the following two steps. (1)
Graph clustering: SW-cut probabilistically samples connected components  CC’s  where each
CC represents a subset of nodes with the same color. This is done by probabilistically cutting
edges between all graph nodes that have the same color based on their posterior distributions
pij = p(yi  yj|xi  xj  eij). (2) Graph relabeling: SW-cut randomly selects one of the CC’s ob-
tained in step (1)  and randomly ﬂips the color of all nodes in that CC  and cuts their edges with the
rest of the graph nodes having that same color. In each iteration  SW-cut probabilistically decides
whether to accept the new coloring of the selected CC  or to keep the previous state. Unlike other
MCMC methods that consider one node at a time (e.g.  Gibbs sampler)  SW-cut operates on a num-
ber of nodes at once. Consequently  SW-cut converges faster and enables inference on relatively
large graphs. Below  we review steps (1) and (2) of SW-cut  for completeness.

In step (1)  edges of G are probabilistically sampled. This re-connects all nodes into new connected
components CC. If two nodes i and j have different labels  they cannot be in the same CC  so
their edge remains intact. If i and j have the same label  their edge is probabilistically sampled
according to posterior distribution pij. If in the latter case edge (i  j) is not sampled  we say that
it has been probabilistically “cut”. Step (1) results in a state A. In step (2)  we choose at random
a connected component CC from step (1)  and randomly reassign a new color to all nodes in that
CC. To separate the re-colored CC from the rest of the graph  we cut existing edges that connect
CC to the rest of the graph nodes with that same color. Step (2) results in a new state B. SW-cut
accepts state B if the acceptance rate is sufﬁciently large via a random thresholding. Let q(A → B)
be the proposal probability for moving from state A to B  and let q(B → A) denote the converse.
The acceptance rate  α(A→B)  of the move from A to B is deﬁned as
q(A → B)p(Y = A|G)(cid:19) .
q(B → A)p(Y = B|G)

α(A → B) = min(cid:18)1 

(2)

3

The computation complexity of each move is relatively low. The ratio q(B→A)
q(A→B) in (2) involves only
those edges that are “cut” around CC in states A and B – not all edges. Also  the ratio p(Y =B|G)
p(Y =A|G)
accounts only for the recolored nodes in CC – not the entire graph G  since all other probabilities
have not changed from state A to state B. Thus  from Eq. (1)  the ratios of the proposal and posterior
distributions characterizing states A and B can be speciﬁed as

q(B→A)
q(A→B)

= Q(i j)∈CutB
Q(i j)∈CutA

(1−pB
ij)
(1−pA
ij)

 

and

p(Y = B|G)
p(Y = A|G)

= Yi∈CC

pB
i
pA
i

· Yj∈N (i)

pB
ij
pA
ij

.

(3)

where CutA and CutB denote the sets of “cut” edges in states A and B  and N (i) is the set of
neighbors of node i  N (i) = {j : j ∈ V  (i  j) ∈ E}.
As shown in [18]  SW-cut is relatively insensitive to different initializations. In our experiments  we
initialize all nodes in the CRF with label 0. Next  we show how to compute the ratios in Eq. (3).

4 Learning

RF can be used for estimating the ratios of the proposal and posterior distributions  given by Eq. (3) 
since RF provides near Bayesian optimal decisions  as theoretically shown by Breiman [13]. In the
following  we describe how to build RF  and use it for computing the ratios in Eq. (3).

Our training data represent a set of M labeled regions. If region i falls within the bounding box of
an object in class y ∈ {1  2  . . .   K}  it receives label y. If i covers a number of bounding boxes
of different classes then i is added to the training set multiple times to account for all distinct class
labels it covers. Each region i is characterized by a d-dimensional descriptor vector  xi ∈ Rd 
which encodes the photometric and geometric properties of i. The training dataset {(xi  yi) : i =
1  . . .   M} is used to learn an ensemble of T decision trees representing RF.
In particular  each training sample is passed through every decision tree from the ensemble until it
reaches a leaf node. Each leaf l records a class histogram  Φl = {φl(y) : y = 1  . . .   K}  where
φl(y) counts the number of training examples belonging to class y that reached l. The total number
of training examples in l is then kΦlk. Also  for each pair of leaves (l  l′)  we record a two-class
histogram  Ψll′ = {ψll′ (y  y′  e) : y  y′ = 1  . . .   K; e = 1  2  3}  where ψll′ (y  y′  e) counts the
number of pairs of training examples belonging to classes y and y′ that reached leaves l and l′  and
also have the relationship type e – namely  ascendent/descendent  touching  or far relationship.
Given Φl and Ψll′  we in a position to estimate the ratios of the proposal and posterior distributions 
deﬁned in (3)  which control the Metropolis-Hastings jumps in the SW-cut. Suppose two regions 
represented by their descriptors xi and xj  are labeled as yA
in
state B of one iteration of the SW-cut. Also  after passing xi and xj through T decision trees of the
learned RF  suppose they reached leaves lt

j in state A  and yB

i and yB

i and yA

j

t=1 ψlt
i lt
t=1 ψlt
i lt

j

j   eij)

j in each tree t = 1  . . .   T . Then  we compute
i and lt
i   yB
(yB
p(Y = B|G)
p(Y = A|G)
i   yA
q(A→B)   it is necessary to compute each individ-
q(A→B) do not contain the same set of

for estimating

. (4)

(yA

 

j

pB
i
pA
i

i

t=1 φlt
t=1 φlt

= PT
PT

(yB
i )
(yA
i )

 

pB
ij
pA
ij

= PT
PT

i

j   eij)
To estimate the ratio of the proposal distributions  q(B→A)
ual probability pij  since the nominator and denominator of q(B→A)
“cut” edges  CutA 6= CutB  as speciﬁed in (3). Thus  we compute
for estimating

pij = PT
t=1 ψlt
i lt
PT
t=1 kΦ

j

(yi  yj  eij)
jk
ikkΦ

lt

lt

q(B→A)
q(A→B)

.

(5)

In the following  we ﬁrst present our empirical evaluation of (RF)2  and then derive the theoretical
performance bounds of a simple  two-class (RF)2.

5 Results

(RF)2 is evaluated on the task of object recognition and segmentation on two benchmark datasets.
First  the MSRC dataset consists of 591 images showing objects from 21 categories [3]. We use the

4

standard split of MSRC into training and test images [3]. Second  the Street-Scene dataset consists
of 3547 images of urban environments  and has manually annotated regions [6  19]. As in [6]  one
ﬁfth of the Street-Scene images are used for testing  and the rest  for training. Both datasets provide
labels of bounding boxes around object occurrences as ground truth.

Images are segmented using the multiscale segmentation algorithm of [17]  which uses the per-
ceptual signiﬁcance of a region boundary  Pb ∈ [0  100]  as an input parameter. We vary Pb =
30:10:150  and thus obtain a hierarchy of regions for each image. A region is characterized by a
descriptor vector consisting of the following properties: (i) 30-bin color histogram in the CIELAB
space; (ii) 250-dimensional histogram of ﬁlter responses of the MR8 ﬁlter bank  and the Laplacian of
Gaussian ﬁlters computed at each pixel  and mapped to 250 codewords whose dictionary is obtained
by K-means over all training images; (iii) 128-dimensional region boundary descriptor measuring
oriented contour energy along 8 orientations of each cell of a 4 × 4 grid overlaid over the region’s
bounding box; (iv) coordinates of the region’s centroid normalized to the image size. Regions ex-
tracted from training images are used for learning RF. A region that falls within a bounding box is
assigned the label of that box. If a region covers a number of bounding boxes of different classes 
it is added to the training set multiple times to account for each distinct label. We use the standard
random splits of training data to train 100 decision trees of RF  constructed in the top-down way.
The growth of each tree is constrained so its depth is less than 30  and its every leaf node contains at
least 20 training examples. To recognize and segment objects in a new test image  we ﬁrst extract a
hierarchy of regions from the image by the segmentation algorithm of [17]. Then  we build the fully
connected CRF graph from the extracted regions (Sec. 2)  and run the SW-cut inference (Sec. 4).

We examine the following three variants of (RF)2: (RF)2-1 — The spatial relationships of regions 
eij  are not accounted for when computing pij in Eq. (4) and Eq. (5); (RF)2-2 — The region rela-
tionships touching and far are considered  while the ascendent/descendent relationship is not cap-
tured; and (RF)2-3 — All three types of region layout and structural relationships are modeled. In
this paper  we consider (RF)2-3 as our default variant  and explicitly state when the other two are
used instead. Note that considering region layouts and structure changes only the class histograms
recorded by leaf nodes of the learned decision trees  but it does not increase complexity.

For quantitative evaluation  we compute the pixel-wise classiﬁcation accuracy averaged across all
test images  and object classes. This metric is suitable  because it does not favor object classes that
occur in images more frequently. Tab. 1 and Tab. 2 show our pixel-wise classiﬁcation accuracy
on MSRC and Street-Scene images. Table. 2 also compares the three variants of (RF)2 on MSRC
and Street-Scene images. The additional consideration of the region relationships touching and far
increases performance relative to that of (RF)2-1  as expected. Our performance is the best when all
three types of region relationships are modeled. The tables also present the pixel-wise classiﬁcation
accuracy of the state of the art CRF models [3 6 20 21]. Note that the methods of [6 21] additionally
use higher-level cues about the horizon location and 3D scene layout in their object recognition and
segmentation. As can be seen  (RF)2 outperforms the latest CRF models on both datasets.
Our segmentation results on example MSRC and Street-Scene images are shown in Fig. 5. Labels
of the ﬁnest-scale regions are depicted using distinct colors  since pixels get labels of the ﬁnest-scale
regions. As can be seen  (RF)2 correctly identiﬁes groups of regions that belong to the same class.
Since the depth of each decision tree in RF is less than 30  the complexity of dropping an instance
through one tree is O(1)  and through RF with T trees is O(T ). Our C-implementation of the RF-

d
o
h
t
e

M
[10]
[22]
[23]
[20]
[3]
Ours

e
n
a
l
p
o
r
e
A
88
82
83
100
60
100

e
l
c
y
c
i
B
91
72
79
98
75
99

d
r
i

B
34
24
30
11
19
42

t
a
o
B
49
18
27
63
7
69

y
d
o
B
54
66
67
55
62
68

g
n
i
d
l
i
u
B
30
49
69
73
62
74

k
o
o
B
93
93
80
78
92
95

r
a
C
82
74
70
88
63
88

t
a
C
56
75
68
11
54
77

r
i
a
h
C
74
51
45
80
15
80

w
o
C
68
97
78
74
58
99

g
o
D
54
35
52
43
19
61

r
e
w
o
l
F

90
74
47
72
63
93

e
c
a
F

77
87
84
72
74
91

s
s
a
r
G
71
88
96
96
97
99

d
a
o
R
31
78
78
76
86
78

p
e
e
h
S

64
97
80
90
50
99

n
g
i
S

82
36
61
92
35
93

y
k
S

84
78
95
50
83
96

e
e
r
T
69
79
87
76
86
90

r
e
t
a

W
58
54
67
61
53
68

Table 1: The average pixel-wise classiﬁcation accuracy on the MSRC dataset. (RF)2 yields the best
performance for all object classes except one.

5

Figure 1: Our object recognition and segmentation results on example images from the MSRC
dataset (top two rows)  and the Street-Scene dataset (bottom two rows). The ﬁgure depicts bound-
aries of the ﬁnest-scale regions found by the multiscale algorithm of [17]  and the color-coded labels
of these regions inferred by (RF)2. The results are good despite the presence of partial occlusion 
and changes in illumination and scale. (best viewed in color)

Method
(RF)2-1
(RF)2-2
(RF)2-3

[20]
[21]
[6]
[3]

MSRC

StreetScene
69.5%±13.7% 78.2%±0.5%
80.2%±14.4% 86.7%±0.5%
82.9%±15.8% 89.8%±0.6%

70.0%
76.4%
N/A
70.0%

N/A
83.0%
84.2%
N/A

Test time

45s
31s
31s
N/A
N/A
N/A
10-30s

1

0.8

0.6

0.4

0.2

0

 

 

P(ε)

0.15

0.2

0.25

0.3

γ

0.35

0.4

0.45

0.5

Table 2: The average pixel-wise classiﬁcation accuracy and
average computation times on the MSRC and Street-Scene
datasets of the three variants of our approach with those of
the state-of-the-art CRF-based methods.

Figure 2: The probability of classi-
ﬁcation error of (RF)2  P (ǫ)  given
by Eq. (6) and Theorem 1 as a
function of the margin  γ  of RF.

guided SW-cut inference of CRF takes 10s–30s on a 2.40GHz PC with 3.48GB RAM for MSRC
and Street-Scene images. Table 2 shows that our average running times are comparable to those of
the other CRF methods that use approximate inference [3  6  20  21].

6 Theoretical Analysis

We are interested in a theoretical explanation of the good performance of (RF)2 presented in the
previous section. In particular  we derive the theoretical performance bounds of a two-class (RF)2 
for simplicity. As explained in Sec. 3  we use the SW-cut for (RF)2 inference. The SW-cut iterates
the Metropolis-Hastings (MH) reversible jumps  and thus explores the state-space of solutions. An
MH jump between states A and B is controlled by the acceptance rate α(A→B) which depends on

6

the ratios of the proposal and posterior distributions  q(B→A)p(Y =B|G)
q(A→B)p(Y =A|G) . Below  we show that the
error made by the two-class RF in estimating these ratios is bounded. Our derivation of the error
bounds of RF is based on the theoretical analysis of evidence trees  presented in [15].

6.1 An Upper Error Bound of (RF)2

An error occurs along MH jumps when a balanced reversible jump is encountered  i.e.  when there
is no preference between jumping from state A to state B and reverse  q(B→A)
q(A→B) =1  and RF wrongly
predicts that the posterior distribution of state B is larger than that of A  p(Y =B|G)
p(Y =A|G)≥1. In this
case  α(A→B)=1  and the SW-cut will erroneously visit state B. We are interested in ﬁnding the
probability of this error  speciﬁed as

P (ǫ) = P (cid:18) p(Y = B|G)

p(Y = A|G) ≥ 1(cid:19) = P 
 Yi∈CC
i ∈ [0 ∞)  and Wij = pB

i /pA

pB
i
pA
i

· Yj∈N (i)

pB
ij
pA

ij ≥ 1
 .

(6)

1

1−n

n z

ij/pA

1−n

n w

n exp(−λ2w

1

n ). Also  the product W =Qn

From Eq. (6)  P (ǫ) can be computed using the probability density function of a product of ran-
ij ∈ [0 ∞)  within a speciﬁc con-
dom variables Zi = pB
nected component CC  where |CC|=n  i = 1  . . .   n  and j ∈ N (i). As we will prove in the
sequel  all random variables Zi have the same exponential distribution fZi(z)=λ1 exp(−λ1z).
Also  we will prove that all random variables Wij have the same exponential distribution
fWij (w)=λ2 exp(−λ2w). Then  it follows that the product Z=Qn
i=1 Zi=(Zi)n has the distribution
i=1Qj∈N (i) Wij =(Wij )nk≈(Wij )n has

fZ(z)= λ1
n exp(−λ1z
the distribution fW (w)= λ2
n )  where we approximate that the number of edges
within CC is the same as the number of nodes in CC  as a result of the probabilistic “cutting” of
graph edges by the SW-cut algorithm. Given fZ(z) and fW (w)  from Eq. (6)  we analytically de-
rive the probability that (RF)2 makes a wrong prediction  P (ǫ) = P (Z · W ≥ 1)  as stated in the
following theorem.
Theorem 1. The probability that (RF)2 makes a wrong prediction is P (ǫ)=P (Z·W ≥ 1)=λK1(λ) 
where Z∈[0 ∞) and W∈[0 ∞) are random variables characterized by the probability density func-
tions fZ(z)= λ1
n )  with parameters λ1 and
λ2  and where K1 is the modiﬁed Bessel function of the second kind  and λ = 2√λ1λ2.
Proof. Deﬁne H = Z · W . Then  fH(h)=R ∞
n K0(λh
1−R 1

z )dz = λ2
2n h

2n )  where
It follows that P (ǫ) = P (H≥1) =

As we will show in the following section  the parameter λ is directly proportional to a measure
of accuracy of RF predictions  referred to as probabilistic margin. Since K1(λ) is a decreasing
function  it follows that the probability that (RF)2 makes a wrong prediction is upper bounded  and
decreases as the probabilistic margin of RF increases.

K0 is the modiﬁed Bessel function of the second kind.

0 fH (h)dh = λK1(λ).(cid:3)

1

n ) and fW (w)= λ2

n w

1−n

n z

n exp(−λ1z

n exp(−λ2w

1
z fZ(z)fW ( h

1−n

1

1−n

1

0

6.2 A Mathematical Model of RF Performance

In this section  we derive that the RF estimates of the ratios of posteriors Zi and Wij have the
exponential distribution. We consider a binary classiﬁcation problem  for simplicity  where training
and test instances may have positive and negative labels. We assume that the two classes are balanced
P (y=+1) = P (y=−1) = 1/2. We deﬁne π to be a fraction of pairs of instances that have certain
relationship  corresponding to a particular spatial or structural relationship between pairs of regions 
deﬁned in Sec. 2. The learning algorithm that creates RF is not modeled. Instead  we assume that
the learned decision trees have the following properties. Each leaf node of a decision tree: (i) stores
a total of C training instances that reach the leaf; and (ii) has a probabilistic margin γ ∈ [0  1/2).
By margin  we mean that in every leaf reached by C training instances a fraction of 1/2 + γ of the
training instances will belong to one class (e.g.  positive)  and fraction 1/2 − γ of them will belong
to the other class (e.g.  negative). We say that a leaf is positive if a majority of the training instances
collected by the leaf is positive  or otherwise  we say that the leaf is negative. It is straightforward
to show that when a positive instance is dropped through one of the decision trees in RF  it will

7

reach a positive leaf with probability 1/2 + γ  and a negative leaf with probability 1/2 − γ [15].
Similarly holds for negative instances. A new test instance is classiﬁed by dropping it through T
decision trees  and taking a majority vote of the labels of all C · T training instances stored in the
leaves reached by the test instance. We refer to this classiﬁcation procedure as evidence voting [15] 
as opposed to decision voting over the leaf labels in the standard RF [13]. The following proposition
states that the probability that evidence voting misclassiﬁes an instance  P (ǫ1)  is upper bounded.
Proposition 1. The probability that RF with T trees  where every leaf stores C training instances 
incorrectly classiﬁes an instance is upper bounded  P (ǫ1)≤ exp(−8CT γ4).
Proof. Evidence voting for labeling an instance can be formalized as drawing a total of C·T inde-
pendent Bernoulli random variables  with the success rate p1  whose outcomes are {−1  +1}  where
+1 is received for correct  and −1 for incorrect labeling of the instance. Let S1 denote a sum of
these Bernoulli random variables. Thus  a positive instance is incorrectly labeled if S1≤0  and a neg-
ative instance is misclassiﬁed if S1>0. Since the two classes are balanced  by applying the standard
Chernoff bound  we obtain P (ǫ1)=P (S1≤0)≤ exp(cid:2)−2CT (p1−1/2)2(cid:3). The success rate p1 can be

derived as follows. When a positive (negative) instance is dropped through a decision tree  it will fall
in a positive (negative) leaf with probability 1/2 + γ  where it will be labeled as positive (negative)
with probability 1/2+γ; else  the positive (negative) instance will be routed to a negative (positive)
leaf with probability 1/2−γ  where it will be labeled as positive (negative) with probability 1/2−γ.
Consequently  the probability that an instance is correctly labeled  i.e.  the success rate of the asso-
ciated Bernoulli random variable  is p1=(1/2+γ)(1/2+γ)+(1/2−γ)(1/2−γ)=1/2 + 2γ2.(cid:3)
Evidence voting is also used for labeling pairs of instances. The probability that evidence voting
misclassiﬁes a pair of test instances  P (ǫ2)  is upper bounded  as stated in Proposition 2.
Proposition 2. Given RF as in Proposition 1  the probability that RF incorrectly labels a pair of
instances having a certain relationship is upper bounded  P (ǫ2) ≤ exp(−8C2T π4γ8).
Proof. Evidence voting for labeling a pair of instances can be formalized as drawing a total of
C2T independent Bernoulli random variables  with success rate p2  whose outcomes are {−1  +1} 
where +1 is received for correct  and −1 for incorrect labeling of the instance pair. Let S2 denote
a sum of these Bernoulli random variables. Then  P (ǫ2)=P (S2≤0)≤ exp(cid:2)−2C2T (p2−1/2)2(cid:3).
Similar to the proof of Proposition 1  by considering three possible cases of correct labeling of a
pair of instances when dropping the pair through a decision tree  the success rate p2 can be derived
as p2=π(1/2+γ2)(1/2+πγ2)+π(1/2−γ2)(1/2−πγ2)+(1−π)(1/2) = 1/2+2π2γ4  where π is a
fraction of pairs of instances that have the same type of relationship.(cid:3)

From Proposition 1  it follows that the probability that RF makes a wrong prediction about the pos-
terior ratio of an instance is upper bounded  P (Zi ≥ 1) = P (ǫ1) = exp(−8CT γ4)  ∀i ∈ CC. This
gives the probability density function fZi(z) = λ1 exp(−λ1z)  where λ1 = 8CT γ4. In addition 
From Proposition 2  it follows that the probability that RF makes a wrong prediction about the pos-
terior ratio of a pair of instances is upper bounded  P (Wij ≥ 1) = P (ǫ2) = exp(−8C2T π4γ8) 
∀i ∈ CC and j ∈ N (i). This gives the probability density function fWij (w) = λ2 exp(−λ2w) 
where λ2 = 8C2T π4γ8. By plugging these results in Theorem 1  we complete the derivation of the
upper error bound of (RF)2. From Theorem 1  P (ǫ) decreases when any of the following parameters
increases: C  T   γ  and π. Fig. 2 shows the inﬂuence of γ on P (ǫ)  when the other parameters are
ﬁxed to their typical values: C = 20  T = 100  and π = 0.1.

7 Conclusion

We have presented (RF)2 – a framework that uses the random forest (RF) for the MCMC-based
inference of a conditional random ﬁeld (CRF). Our key idea is to employ RF to directly compute
the ratios of the proposal and posterior distributions of states visited along the Metropolis-Hastings
reversible jumps  instead of estimating each individual distribution  and thus improve the conver-
gence rate and accuracy of the CRF inference. Such a non-parametric formulation of CRF and its
inference has been demonstrated to outperform  in terms of computation time and accuracy  existing
parametric CRF models on the task of multiclass object recognition and segmentation. We have also
derived the upper error bounds of the two-class RF and (RF)2  and showed that the classiﬁcation
error of (RF)2 decreases as any of the following RF parameters increases: the number of decision
trees  the number of training examples stored in every leaf node  and the probabilistic margin.

8

References

[1] L.-J. Li  R. Socher  and L. Fei-Fei  “Towards total scene understanding: Classiﬁcation  annotation and

segmentation in an automatic framework ” in CVPR  2009.

[2] X. He  R. S. Zemel  and M. A. Carreira-Perpinan  “Multiscale Conditional Random Fields for image

labeling ” in CVPR  2004  pp. 695–702.

[3] J. Shotton  J. Winn  C. Rother  and A. Criminisi  “Textonboost: Joint appearance  shape and context

modeling for multi-class object recognition and segmentation ” in ECCV  2006  pp. 1–15.

[4] J. Verbeek and B. Triggs  “Scene segmentation with CRFs learned from partially labeled images ” in

NIPS  2007.

[5] A. Torralba  K. P. Murphy  and W. T. Freeman  “Contextual models for object detection using boosted

random ﬁelds ” in NIPS  2004.

[6] S. Gould  T. Gao  and D. Koller  “Region-based segmentation and object detection ” in NIPS  2009.
[7] A. Rabinovich  A. Vedaldi  C. Galleguillos  E. Wiewiora  and S. Belongie  “Objects in context ” in ICCV 

2007.

[8] N. Payet and S. Todorovic  “From a set of shapes to object discovery ” in ECCV  2010.
[9] S. Todorovic and N. Ahuja  “Unsupervised category modeling  recognition  and segmentation in images ”

IEEE TPAMI  vol. 30  no. 12  pp. 1–17  2008.

[10] J. J. Lim  P. Arbelaez  C. Gu  and J. Malik  “Context by region ancestry ” in ICCV  2009.
[11] J. Sivic  B. C. Russell  A. Zisserman  W. T. Freeman  and A. A. Efros  “Unsupervised discovery of visual

object class hierarchies ” in CVPR  2008.

[12] J. Lafferty  A. McCallum  and F. Pereira  “Conditional random ﬁelds: Probabilistic models for segmenting

and labeling sequence data ” in ICML  2001  pp. 282–289.

[13] L. Breiman  “Random forests ” Mach. Learn.  vol. 45  no. 1  pp. 5–32  2001.
[14] J. Gall and V. Lempitsky  “Class-speciﬁc hough forests for object detection ” in CVPR  2009.
[15] G. Martinez-Munoz  N. Larios  E. Mortensen  W. Zhang  A. Yamamuro  R. Paasch  N. Payet  D. Lytle 
L. Shapiro  S. Todorovic  A. Moldenke  and T. Dietterich  “Dictionary-free categorization of very similar
objects via stacked evidence trees ” in CVPR  2009.

[16] Y. Lin and Y. Jeon  “Random forests and adaptive nearest neighbors ” Journal of the American Statistical

Association  pp. 101–474  2006.

[17] C. F. P. Arbelaez  M. Maire and J. Malik  “From contours to regions: An empirical evaluation ” in CVPR 

2009.

[18] A. Barbu and S.-C. Zhu  “Graph partition by Swendsen-Wang cuts ” in ICCV  2003  p. 320.
[19] S. Bileschi and L. Wolf  “A uniﬁed system for object detection  texture recognition  and context analysis

based on the standard model feature set ” in BMVC  2005.

[20] C. Galleguillos  B. McFee  S. Belongie  and G. R. G. Lanckriet  “Multi-class object localization by com-

bining local contextual interactions ” in CVPR  2010.

[21] S. Gould  R. Fulton  and D. Koller  “Decomposing a scene into geometric and semantically consistent

regions ” in ICCV  2009.

[22] J. Shotton  M. Johnson  and R. Cipolla  “Semantic texton forests for image categorization and segmenta-

tion ” in CVPR  2008.

[23] Z. Tu and X. Bai  “Auto-context and its application to high-level vision tasks and 3D brain image seg-

mentation ” IEEE TPAMI  vol. 99  2009.

9

,Oswin Krause
Dídac Rodríguez Arbonès
Christian Igel