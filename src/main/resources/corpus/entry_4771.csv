2019,Robust Principal Component Analysis with Adaptive Neighbors,Suppose certain data points are overly contaminated  then the existing principal component analysis (PCA) methods are frequently incapable of filtering out and eliminating the excessively polluted ones  which potentially lead to the functional degeneration of the corresponding models. To tackle the issue  we propose a general framework namely robust weight learning with adaptive neighbors (RWL-AN)  via which adaptive weight vector is automatically obtained with both robustness and sparse neighbors. More significantly  the degree of the sparsity is steerable such that only exact k well-fitting samples with least reconstruction errors are activated during the optimization  while the residual samples  i.e.  the extreme noised ones are eliminated for the global robustness. Additionally  the framework is further applied to PCA problem to demonstrate the superiority and effectiveness of the proposed RWL-AN model.,Robust Principal Component Analysis with Adaptive

Neighbors

Rui Zhang

Arizona State University

Tempe  AZ  U.S.A.

ruizhang8633@gmail.com

Hanghang Tong∗

University of Illinois at Urbana-Champaign

Urbana  Illinois  U.S.A.

htong@illinois.edu

Abstract

Suppose certain data points are overly contaminated  then the existing principal
component analysis (PCA) methods are frequently incapable of ﬁltering out and
eliminating the excessively polluted ones  which potentially lead to the functional
degeneration of the corresponding models. To tackle the issue  we propose a general
framework namely robust weight learning with adaptive neighbors (RWL-AN)  via
which adaptive weight vector is automatically obtained with both robustness and
sparse neighbors. More signiﬁcantly  the degree of the sparsity is steerable such
that only exact k well-ﬁtting samples with least reconstruction errors are activated
during the optimization  while the residual samples  i.e.  the extreme noised ones
are eliminated for the global robustness. Additionally  the framework is further
applied to PCA problem to demonstrate the superiority and effectiveness of the
proposed RWL-AN model.

1

Introduction

As for the high-quality data reconstruction  principal component analysis (PCA) [16  4  7] has been
widely investigated. To deal with high dimensional data  conventional PCA methods usually include
the data preprocessing  i.e.  vectorization of each data point. Nonetheless  the vectorization of the
data points could easily incur the curse of dimensionality. Therefore  two-dimensional reconstruction
has been brought to the study in the ﬁeld of image analysis. In sum  equipped with the PCA methods
[17  18  19]  the statistical properties of input data can be retained under the obtained subspace.
In reality  the presence of outliers in data largely reduces the performance of PCA approaches. The
existing reconstruction methods usually promote the robustness by exploiting the robust norms as
their loss functions [10]  e.g.  L1-norm and non-squared F -norm. More speciﬁcally  L1-norm based
approaches [5  14  9] are developed to alleviate the negative effects of local ill-dimensions. For
instance  Li et al. [5] proposed the L1-norm based 2DPCA (2DPCA-L1) by optimizing multiple
projection directions sequentially. The L1-norm based methods approximate the related optimization
problem and therefore often lead to a greedy strategy  which is potentially stuck with heuristic
solutions and large computational cost. Luo et al. [6  15] proposed a non-greedy algorithm for an
approximate solution to the L1-norm based maximization problem. Moreover  non-squared F -norm
based methods [10] are developed  where the sum of non-squared F -norm reconstruction errors is
minimized. Zhang et al. [20] optimized the robust non-squared F -norm based objective by virtue of
a dual problem  where the transitional weight is assigned to each term of the objective.
However  aforementioned robust approaches have lots of limitations. Firstly  all of them depend
on different types of loss functions  which are potentially sensitive to outliers. For instance  L1-
norm based methods are usually utilized to handle the occluded data with local outliers  while

∗Hanghang Tong is the Corresponding Author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

non-squared F -norm based approaches are effective to tackle the data with global noises. Secondly 
when certain samples are excessively polluted  weak robust methods might be incapable of preventing
the degeneration of the reconstruction. Zhang et al. [20] addressed this problem by learning a
sparse weight via a capped model [8  2]  where the threshold is pre-given to eliminate the terms with
larger reconstruction errors. In other words  the performance is sensitive to the choice of threshold.
Nonetheless  it is strenuous to search the optimal threshold with frequent inaccuracy. Accordingly 
the performance of the existing reconstruction methods is unsatisfactory.
In this paper  we propose a general framework named RWL-AN for learning an adaptive weight
vector with both robustness and sparse neighbors. RWL-AN can be further applied to a spectrum
of subspace learning approaches via the adaptive-weight strategy. Speciﬁcally speaking  RWL-AN
assigns a smaller weight to the term with larger reconstruction error automatically to reduce the
negative effect of local outliers. Besides the local robustness  the weight vector is sparse to prevent
the excessive noised terms from degrading the performance of the model. In other words  the degree
of the sparsity is steerable such that only speciﬁed k samples with least reconstruction errors are
effective to eliminate the extreme noised data points for the global robustness. By applying the
proposed RWL-AN framework to the PCA problem  the superiority and effectiveness of the proposed
method are demonstrated both theoretically and empirically.
Notations: In the paper  all the matrices are written in uppercase. For matrix M  the ij-th element
of M is denoted by mij . The trace of matrix M is denoted by T r(M). The (cid:96)2-norm of vector v is
denoted by (cid:107)v(cid:107)2. MT denotes the transpose operation of M. The Frobenius norm of matrix M is
denoted by (cid:107)M(cid:107)F . M⊥ denotes the orthogonal complement space of M.

2 Robust Principal Component Analysis Revisited
Given a dataset X = {x1  x2 ···   xN}  xi ∈ Rd represents the i-th sample. X ∈ Rd×N denotes
the associated matrix of the dataset X . To obtain the optimal mean automatically instead of directly
centering the data  Nie et al. [10] proposed a robust PCA model from the perspective of low-rank
approximation  i.e.  minimizing the reconstruction errors with optimal mean as

min

(1)
where variable m ∈ Rd serves as the optimal mean in Eq.
(1). Z = [z1  . . .   zN ] ∈ Rd×N
represents the low-rank approximation of X upon the orthogonal subspace W ∈ Rd×m. Via the
rank factorization of zi on the subspace W  we have zi = W(vi)T   where vi ∈ R1×m. Therefore 
problem (1) can be reformulated into

m rank(Z)=k

i=1

(cid:107)xi − m − zi(cid:107)2 

min

m vi WT W=I

(cid:107)xi − m − W(vi)T(cid:107)2 

(2)

whose third term within the (cid:96)2-norm is the low-rank reconstructed data. Accordingly  the solution of
vi could be achieved according to the Karush-Kuhn-Tucker (KKT) condition of problem (4) with

respect to (w.r.t.) vi as
(2) can be addressed by solving the following dual problem:

∂vi

= 0 ⇒ vi = (xi − m)T W.

i=1

∂

(cid:107)xi−m−W(vi)T (cid:107)2

. Therefore  problem

N(cid:80)

N(cid:88)

N(cid:88)

i=1

pi(cid:107)(cid:16)

I − WWT(cid:17)

N(cid:88)

i=1

min

m WT W=I

(xi − m)(cid:107)2
2.

(3)

1

2(cid:107)(I−WWT )(xi−m)(cid:107)2

where pi ←
serves as a transitional weight to be iteratively updated. In other
words  the smaller weight would be assigned to the term with larger outliers automatically and vice
versa for the robustness.
Motivated by problem (2)  Zhang et al. extends it to a 2D version to enhance the robustness of
2DPCA. Denote an image dataset A = {A1  A2 ···   AN}  where Ai ∈ Ru1×u2 represents the i-th
image matrix. Robust 2DPCA method is formulated as

min

M Bi UT

1 U1=I UT

2 U2=I

(cid:107)Ai − M − U1BiUT

2 (cid:107)F  

(4)

N(cid:80)

i=1

2

where U1 ∈ Ru1×d1 and U2 ∈ Ru2×d2 are left and right orthogonal subspaces for dimensionality
reduction  respectively. Bi ∈ Rd1×d2 denotes a low-dimenional representation of Ai. M ∈ Ru1×u2
serves as the optimal mean of input data. Since Bi is free from any constraint  problem (4) could be
rewritten as

(cid:107)Ai − M − U1UT

2 (Ai − M)U2UT

2 (cid:107)F .

(5)

N(cid:80)

i=1

min

M UT

1 U1=I UT

2 U2=I

3 Framework of Robust Weight Learning with Adaptive Neighbors

The robust PCA methods mentioned above frequently highlight the robustness and reduce the
impact of outliers by developing different metrics  which would possibly lead to various limitations.
Although sparsity could also be obtained via the capped model  the performance of the models are
often sensitive to the presetting threshold  which is difﬁcult to determine.
In this paper  a framework regarding adaptive weight learning is developed to apply to various
reconstruction approaches. The adaptive weight vector can be achieved by the proposed framework
with 1) robustness  i.e.  the term with larger reconstruction error is assigned with smaller weight to
prevent the outliers from dominating the model; 2) sparsity  i.e.  the images with excessive noises
are eliminated to prevent the ill samples from decreasing the performance. Accordingly  the proposed
framework for Robust Weight Learning with Adaptive Neighbors (RWL-AN) is formulated as

i=1

min

p≥0 pT 1=1

pig(xi) + γp2
i  

(6)
where g(xi) ∈ R+ denotes the reconstruction function under the i-th data point xi with trade-off
parameter γ. p = [p1  p2 ···   pN ]T is the weight vector  where pi is the weight assigned to the
i-th reconstruction term. The ﬁrst term in Eq. (6) indicates that a sample with large reconstruction
error should be assigned with a small weight  while the second term is the regularization to avoid
trivial solution and over-ﬁtting. It is worth mentioning that an efﬁcient technique is further applied to
solving problem (6)  such that the weight vector p has k adaptive neighbors (nonzero entries)  i.e. 
only k best well-ﬁtting samples are activated.
Particularly  the following speciﬁc derivation is provided to obtain the closed form solution to problem
(6). Denote g(xi) by gi  then problem (6) is equivalent to

N(cid:88)

pi +

.

(7)

Denote g = [g1  g2 ···   gN ]T   then problem (7) can be further rewritten as

(8)
where 0 = [0  0 ···   0]T ∈ RN and 1 = [1  1 ···   1]T ∈ RN . Due to the (cid:96)1-ball constraint p ≥ 0
and pT 1 = 1  the Lagrangian function is represented as

p≥0 pT 1=1

min

2

1
2

g
2γ

N(cid:88)

i=1

min

p≥0 pT 1=1

(cid:13)(cid:13)(cid:13)(cid:13)p +

1
2

g
2γ

gi
2γ

(cid:19)2
(cid:13)(cid:13)(cid:13)(cid:13)2

 

1
2

(cid:18)
(cid:13)(cid:13)(cid:13)(cid:13)p +
(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:18)

(cid:19)

pi =

λ − gi
2γ

 

+

3

L(p  λ  σ) =

(9)
where λ ∈ R and σ ∈ RN ≥ 0 are the Lagrangian multipliers. According to the KKT conditions 
the optimal solution to problem (8) satisﬁes

2

− λ(pT 1 − 1) − σT p 



∂L(p λ σ)

∂p

= 0 ⇒ pi + gi

2γ − λ − σi = 0
pi ≥ 0
σi ≥ 0
piσi = 0

.

From the KKT conditions in (10)  pi  (i = 1  2 ···   N ) can be summarized as

(10)

(11)

where the operator (•)+ = max(•  0). According to Eq. (11)  pi is non-negative and inversely
proportional to gi.
Furthermore  we attempt to determine λ and γ in Eq. (11). Without loss of generality  we assume
g1 ≤ g2 ≤ ··· ≤ gN and thus have p1 ≥ p2 ≥ ··· ≥ pN ≥ 0 based on the negative relationship
between pi and gi in Eq. (11). When only k neighbors of p are considered  we have

By combining Eq. (12) with the constraint pT 1 = 1  we have

(cid:19)

(cid:18)

k(cid:88)

i=1

λ − gi
2γ

= 1 ⇒ λ =

1
k

+

1

2γk

(12)

(13)

k(cid:88)

i=1

gi.

(cid:26)

pk > 0 ⇒ λ − gk
pk+1 = 0 ⇒ λ − gk+1

2γ > 0
2γ ≤ 0.

Based on the constraints in Eq. (12) and result in Eq. (13)  the following inequality w.r.t. γ can be

inferred  1

1

k > gk
k ≤ gk+1

2γ − 1
2γ − 1

2γk

2γk

k(cid:80)
k(cid:80)

i=1

i=1

gi

gi.

⇒ k
2

gk − 1
2

k(cid:88)

i=1

gi < γ ≤ k
2

gk+1 − 1
2

k(cid:88)

i=1

gi.

(14)

To achieve exact k nonzero weights  the upper bound γ = k

gj is selected. With λ and

2 gk+1 − 1

2

γ in Eqs. (13) and (14) respectively  pi in (11) can be eventually formulated as

j=1

k(cid:80)
k(cid:80)
2 gk+1 − 1

j=1

2

2k( k

k(cid:80)
k(cid:80)

j=1

gj)

2 gk+1 − 1

2

gj) +

gj − kgi



j=1

+

(cid:18)
(cid:32)

pi =

=

(cid:19)

 1

k

λ − gi
2γ

=

+

gk+1 − gi

kgk+1 −(cid:80)k

j=1 gj

+

k(cid:88)

j=1

gj − gi
2γ

+

(cid:33)

1

2γk

.

 2( k



=

+

(15)
From Eq. (15) regarding the weight pi  we could notice that 1) pi is non-negative and inversely
proportional to gi  which ensures the local robustness of reconstruction problem (6)  i.e.  the term
with larger reconstruction error is assigned with a smaller weight; 2) if i > k  then pi = 0  which
ensures the sparsity of p in problem (6)  such that only k terms with smallest reconstruction errors
are considered or activated; 3) k is a steerable integer parameter that directly manipulates the number
of activated samples  which indicates a global robustness to the outliers. According to Eq. (15) 
Algorithm 1 is developed by solving the proposed RWL-AN framework in (6).

Algorithm 1: Algorithm for solving RWL-AN in (6)
Input: a vector g = [g1  g2 ···   gN ]T that preserves the reconstruction errors under each sample;
Output: a weight vector p = [p1  p2 ···   pN ]T assigned to each term in the objective (6).
1 Sort g satisfying g1 ≤ g2 ··· ≤ gN ;

the integer parameter k (k ≤ N ) that controls the number of activated samples.

2 Calculate pi =

  (i = 1  2 ···   N );

 gk+1−gi
kgk+1− k(cid:80)

gj



j=1

+

4

4 Robust PCA under RWL-AN

Equipped with the RWL-AN framework in (6)  we propose the robust PCA model under the proposed
RWL-AN as

N(cid:80)

min

pi(cid:107)xi − m − W(vi)T(cid:107)2

2 + γp2
i

m vi p W

s.t. p ≥ 0  pT 1 = 1  WT W = I 

i=1

where W ∈ Rd×m is the orthogonal subspaces and vi ∈ R1×m denotes a low-dimensional repre-
sentation of xi. Similar as problem (2)  the optimal solution vi to problem (16) can be derived as
vi = (xi − m)T W. Speciﬁcally speaking  the term (cid:107)xi − m − W(vi)T(cid:107)2
2 exactly evaluates the
reconstruction error for the i-th data point and thus satisﬁes the deﬁnition of gi in the framework (6).
To solve problem (16)  we utilize an alternative optimization strategy  i.e.  coordinate-block descent
method [13].
Optimize W & m by ﬁxing p: When p is ﬁxed  problem (16) degenerates to

pi(cid:107)(cid:16)

I − WWT(cid:17)

N(cid:88)

i=1

min

m WT W=I

(xi − m)(cid:107)2
2 

where m serves as the mean variable.
Theorem 1. The optimal mean m∗ in problem (17) satisﬁes the form of

m∗ = Xp =

pixi.

N(cid:88)

i=1

(16)

(17)

(18)

(19)

(20)

Proof. By taking the derivative of Eq. (17) w.r.t. m and setting it to zero  we have

Note that(cid:0)m1T − X(cid:1) diag(p)1 = Wξ + W⊥η via the associated orthogonal decomposition  thus

(cid:16)

I − WWT(cid:17)(cid:0)m1T − X(cid:1) diag(p)1 = 0.

we have

Wξ − Wξ + W⊥η − 0 = 0 ⇒ η = 0.

Due to the constraint pT 1 = 1 and diag(p)1 = p  we could further obtain that

where ξ is an arbitrary vector. By substituting Eq. (19)  problem (17) can be rewritten as

m = Xp + Wξ 

pi(cid:107)(cid:16)

I − WWT(cid:17)

N(cid:88)

i=1

min

m WT W=I

(xi − Xp)(cid:107)2
2 

which is totally independent of ξ. Therefore  we could select ξ as the zero vector for the convenience 
such that the optimal mean m∗ is represented as Eq. (18).

According to Theorem 1  the optimal solution of m to problem (17) takes the form as derived in
(18). Therefore  problem (17) could be further reformulated into

T r(diag(p)(cid:0)X − Xdiag(p)11T(cid:1)T(cid:0)I − WWT(cid:1)(cid:0)X − Xdiag(p)11T(cid:1))
T r(WT X(cid:0)I − diag(p)11T(cid:1) diag(p)(cid:0)I − 11T diag(p)(cid:1) XT W)

min
⇒ max

WT W=I

(21)

WT W=I

= max

WT W=I

T r(WT XDXT W) 

where D = diag(p) − ppT . Hence  W are the k eigenvector matrix corresponding to the k largest
eigenvalues of XDXT [18].

Optimize p by ﬁxing W & m: Denote ri = (cid:107)(cid:16)
I − WWT(cid:17)
N(cid:88)

be rewritten as

(xi − m)(cid:107)2

2  then problem (16) could

(22)

min

p≥0 pT 1=1

piri + γp2
i .

i=1

5

Same as problem (6)  problem (22) can be solved with the closed form solution as represented
in Eq. (15)  where gi  (i = 1  2 ···   N ) is replaced by ri  (i = 1  2 ···   N ). k is an integer
parameter to determine the number of nonzero weights in p. Similarly  the i-th weight pi is inversely
proportional to the associated reconstruction error ri to promote the local robustness. In addition  as
for the i-th term satisfying i ≥ (k + 1)  the related weight vanishes  such that the excessive outliers 
which might potentially sabotage our model can be totally prevented. In other words  the sparsity
promotes the global robustness of the reconstruction problem (16). According to Eqs. (18)  (21) 
and (22)  an efﬁcient algorithm can be summarized in Algorithm 2 to solve problem (16). Since the
coordinate-block descent method is utilized with achieving the closed form solutions w.r.t. W  m 
and p  Algorithm 2 monotonically converges.

Algorithm 2: Algorithm for solving robust problem (16)
Input: an image matrix X = [x1  x2 ···   xN ]; the number of effective samples k.
Output: orthogonal subspace W ∈ Rd×m.
1 Initialize random p satisfying pT 1 = 1;
2 while not converge do
3
4

Update D ← diag(p) − ppT ;
Update W ← arg max

Update ri ← (cid:107)(cid:16)

I − WWT(cid:17)

T r(WT XDXT W);
(xi − Xp)(cid:107)2

2  (i = 1  2 ···   N );

WT W=I

5

6
7 end

Update {pi}N

i=1 by Algorithm 1 with inputting {ri}N

i=1;

5 Experiment

Diverse experiments are conducted to evaluate the performance of our method. Firstly  the experi-
mental settings are provided. Moreover  the experimental results on different tasks are recorded.

5.1 Experimental Settings

The proposed robust PCA with RWL-AN is compared to the reconstruction methods including
conventional PCA (denoted by PCA) [4]  robust PCA with optimal mean (denoted by RPCA-OM)
[10]  generalized low-rank approximations of matrices (denoted by GLRAM) [18]  robust 2DPCA
with optimal mean (denoted by R2DPCA) [20] and capped robust 2DPCA with optimal mean
(denoted by capped R2DPCA) [20]. The integer parameter k of our method is setted as [0.85N ] (N
is the total number of data points)  such that 85% samples are assigned with non-zero weights. As for
capped R2DPCA   is searched in the grid of {10  20 ···   50} and the best results are recorded.
Four benchmark face image datasets including AT&T [1]  UMIST [3]  FEI and FERET [12] are
utilized in the experiment. Table 1 reports the information for the benchmark datasets. In each
dataset  occlusions are placed with random size (over 25% area) on part of images (number of noised
samples = total number of samples × noise rate). Note that all the experiments are implemented by
MATLAB R2015b on Windows 7 PC with 3.20 GHz i5-3470 CPU and 16.0 GB main memory.

Table 1: The information of the benchmark datasets
Dataset
FERET
1400
80 × 80

AT&T
400
64 × 64

FEI
2600
32 × 32

No. of images
Size of images

UMIST
64 × 64

575

Class

40

20

200

200

All the methods are evaluated on two tasks regarding image reconstruction and clustering. As for
the reconstruction task  numerical results are recorded and compared. As for the clustering task  we
employ k-means as metric. Moreover  we run 50 times with random initialization in each experiment.

6

Table 2: Reconstruction error comparison. The best is bolded and runner-up is underlined.

noise rate

ours

GLRAM R2DPCA

capped
R2DPCA

PCA

RPCA-OM

AT&T

UMIST

FEI

FERET

raw
0.2
raw
0.2
raw
0.2
raw
0.2

3.56
6.65
4.31
8.50
1.35
2.19
14.70
23.26

21.87
59.45
23.45
59.18
21.26
24.81
44.89
99.61

13.44
26.91
16.16
28.13
4.29
7.45
30.90
51.32

13.44
21.88
16.16
24.11
4.29
6.52
30.90
33.20

1197.25
1291.10
674.67
720.49
533.63
505.60
1661.45
1603.78

5.27
19.19
6.43
26.66
1.99
10.30
19.21
67.75

(a) AT&T

(b) FEI

(c) FERET

(d) UMIST

Figure 1: Clustering accuracy of occluded images and their reconstructed images. The x-axis
represents the reduced dimensionality d1 of subspace U1 with the dimensionality d2 of U2 satisfying
d2 = d1  while W has the dimensionality d1 × d2.

5.2 Comparison of Reconstruction Error

i=1 pi(cid:107)xr

structed. The performance of the reconstructed methods are measured by(cid:80)N

Reconstruction problem is to seek the optimal subspace  upon which low-rank images are recon-
i − xo
i(cid:107)2
2   where
i represents the i-th reconstructed image and xo
i is the original image. For the fair comparison 
xr
weights are normalized. The reduced dimensionality for 2D method is d1 = 9  d2 = 10  such
that 1D methods perform with the reduced dimensionality m = 90. Table 2 records the results of
reconstruction error comparison. From Table 2  we could conclude that
1) As for the noised datasets  the proposed method achieves the best performance.
2) As for the raw datasets  RPCA-OM achieves the runner-up performance  while ours and R2DPCA
outperform GLRAM and PCA. The results also illustrate the superiority of the optimal-mean based
PCA methods.
3) By applying RWL-AN  the reconstruction performance of PCA is largely improved by outperform-
ing all the other competitors. Therefore  the effectiveness of the proposed framework RWL-AN is
veriﬁed.

(a) AT&T

(b) FERET

Figure 2: Reconstruction errors of our proposed method w.r.t the varying parameter k = N × krate
(N is the total number of samples).

7

Reduced dimension d1 of subspace (d1=d2)1520253035Clustering accuracy (%)3031323334353637383940oursGLRAMRPCA-OMR2DPCAcapped R2DPCABaselineReduced dimension d1 of subspace (d1=d2)1520253035Clustering accuracy (%)2830323436384042oursGLRAMRPCA-OMR2DPCAcapped R2DPCABaselineReduced dimension d1 of subspace (d1=d2)1520253035Clustering accuracy (%)26.52727.52828.52929.53030.5oursGLRAMRPCA-OMR2DPCAcapped R2DPCABaselineReduced dimension d1 of subspace (d1=d2)1520253035Clustering accuracy (%)383940414243444546474849oursGLRAMRPCA-OMR2DPCAcapped R2DPCABaseline0.50.60.70.80.9k_rate10152025Reconstruction error0.50.60.70.80.9k_rate202530354045Reconstruction errorTable 3: CPU Time comparison (seconds) when iteration number is ﬁxed as 50 for each algorithm.

Method
Ours

GLRAM
R2DPCA
RPCA-OM

AT&T
8.14
6.81
8.55
848.46

UMIST
11.40
8.78
12.55
925.73

FEI
16.76
14.71
17.49
266.58

FERET
49.03
45.22
49.69
2902.15

4) When severe occlusions are involved in the datasets  robust methods as our proposed method 
R2DPCA  capped R2DPCA  and RPCA-OM have better performance than the conventional methods
including GLRAM and PCA.
From Figure 1  it is noticed that the robust methods as our proposed method  R2DPCA  and capped
R2DPCA are superior to GLRAM. The capped R2DPCA overcomes R2DPCA a little  while the
proposed method has the outstanding performance under the most cases.
Table 3 reports the CPU time of the comparative algorithms except for capped R2DPCA  which is
time-consuming due to tuning an appropriate threshold. We can conclude that the optimal mean
based methods including ours  R2DPCA  RPCA-OM are slower than GLRAM due to the calculation
of optimal mean in each iteration. Besides that  the time consumption of ours and R2DPCA is similar.
In fact  the computation of our weight in Eq. (15) is more complicated than R2DPCA. Nonetheless 
due to the sparse weight in the proposed method  ours often runs faster.

5.3 Comparison of Clustering

(cid:80)N

In order to demonstrate the discriminative ability of the reconstructed algorithms  we further compare
the clustering results of the reconstructed images via k-means classiﬁer  where the clustering accuracy
[11] is computed by ACC = 1
i=1 δ(li  map(ci)). li denotes real label of the i-th instance  and
ci is the corresponding clustering index. map(·) denotes a function that maps each cluster index to
N
the best class label. δ(·) represents the δ-function  i.e.  value is 1 when two input parameters are
the same  and 0 otherwise. Figure 1 shows the clustering results under the reconstructed image of
different algorithms.
1) Since twenty percent of input images are occluded by noises for each dataset  the superior clustering
performance of the proposed method implies its stronger robustness to the outliers.

5.4 Sensitivity Analysis w.r.t. Parameter k

In this part  the corresponding experiments are conducted to investigate the sensitivity of our model
(16) regarding the parameter k . We utilize two benchmark datasets known as AT&T and FERET 
whose 20% samples are contaminated as previously described. We increase the degree of sparsity
by setting krate from 0.5 to 0.95  where the parameter k is calculated by N × krate. Moreover  the
related reconstruction errors of our proposed method are shown in Figure 2.
1) The curves in Figure 2 are steady when krate is less than 0.8. Afterwards  the curves increase
rapidly  since 20% polluted samples are included.
2) Our model is insensitive to parameter krate  when the krate ≤ 0.8  which is the pivotal point.
Therefore  we can either determine krate by tuning it or simply set it as a medium value such as 0.5.

6 Conclusion

In this paper  a general framework entitled RWL-AN is proposed  such that the adaptive weight
vector is achieved automatically with the local robustness. In particular  the weight vector is sparse
with adaptive neighbors  i.e.  the degree of the sparsity is steerable with only k activated samples of
least reconstruction errors. In other words  the sparsity is steerable to eliminate the excessive noised
samples for the global robustness. The framework is further applied to the PCA problem to achieve
both local and global robustness. Eventually  theoretical analysis and extensive experimental results
are presented to validate the superiority of the proposed method.

8

Acknowledgment

This work is supported by NSF (IIS-1651203  IIS-1715385)  and DHS (2017-ST-061-QA0001).

References
[1] Parameterisation

model

for

human

face

identiﬁcation.

http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html.  1994.

of

a

stochastic

[2] Hongchang Gao  Feiping Nie  Weidong Cai  and Heng Huang. Robust capped norm nonnegative matrix
factorization: Capped norm nmf. In ACM International on Conference on Information and Knowledge
Management  pages 871–880  2015.

[3] Daniel B Graham and Nigel M Allinson. Face recognition: From theory to applications. NATO ASI Series

F  Computer and Systems Sciences  (163):446–456  1998.

[4] I. T Jolliffe. Principal component analysis. 2002.

[5] Xuelong Li  Yanwei Pang  and Yuan Yuan. L1-norm-based 2dpca. IEEE Transactions on Systems Man &

Cybernetics Part B  40(4):1170–1175  2010.

[6] Minnan Luo  Feiping Nie  Xiaojun Chang  Yi Yang  Alexander Hauptmann  and Qinghua Zheng. Avoiding
optimal mean robust pca/2dpca with non-greedy l1-norm maximization. In International Joint Conference
on Artiﬁcial Intelligence  pages 1802–1808  2016.

[7] Feiping Nie  Heng Huang  Chris Ding  Dijun Luo  and Hua Wang. Robust principal component analysis
with non-greedy l1-norm maximization. In International Joint Conference on Artiﬁcial Intelligence  pages
1433–1438  2011.

[8] Feiping Nie  Zhouyuan Huo  and Heng Huang. Joint capped norms minimization for robust matrix recovery.

In Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence  pages 2557–2563  2017.

[9] Feiping Nie  Hua Wang  Cheng Deng  Xinbo Gao  Xuelong Li  and Heng Huang. New l1-norm relaxations
and optimizations for graph clustering. In Thirtieth AAAI Conference on Artiﬁcial Intelligence  pages
1962–1968  2016.

[10] Feiping Nie  Jianjun Yuan  and Heng Huang. Optimal mean robust principal component analysis. In

International Conference on Machine Learning  pages 1062–1070  2014.

[11] Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity.

IEEE Transactions on Acoustics Speech & Signal Processing  32(6):1258–1259  1998.

[12] I. Philips  H. Wechsler  J. Huang  and P. Rauss. The fere database and evaluation procedure for face

recognition algorithms. Image and Vision Computing  (16):295–306  1998.

[13] P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal

of Optimization Theory & Applications  109(3):475–494  2001.

[14] Haixian Wang and Jing Wang. 2dpca with l1-norm for simultaneously robust and sparse modelling. Neural

Networks the Ofﬁcial Journal of the International Neural Network Society  46(10):190  2013.

[15] Rong Wang  Feiping Nie  Xiaojun Yang  Feifei Gao  and Minli Yao. Robust 2dpca with non-greedy

l1-norm maximization for image analysis. IEEE Transactions on Cybernetics  45(5):1108–1112  2017.

[16] Svante Wold  Kim Esbensen  and Paul Geladi. Principal component analysis. Chemom.intell.lab  2(1):37–

52  1987.

[17] Jian Yang  David Zhang  Alejandro F. Frangi  and Jingyu Yang. Two-dimensional pca: A new approach to
appearance-based face representation and recognition. IEEE Trans Pattern Anal Mach Intell  26(1):131–
137  2004.

[18] Jieping Ye. Generalized low rank approximations of matrices. Machine Learning  61(1-3):167–191  2004.

[19] Daoqiang Zhang and Zhi Hua Zhou. (2d)2pca: Two-directional two-dimensional pca for efﬁcient face

representation and recognition. Neurocomputing  69(1):224–231  2005.

[20] Rui Zhang  Feiping Nie  and Xuelong Li. Auto-weighted two-dimensional principal component analysis
with robust outliers. In IEEE International Conference on Acoustics  Speech and Signal Processing  pages
6065–6069  2017.

9

,Kevin Duarte
Yogesh Rawat
Mubarak Shah
Rui Zhang
Hanghang Tong