2011,Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint,The goal of this paper is to investigate the advantages and disadvantages of learning in Banach spaces over Hilbert spaces. While many works have been carried out in generalizing Hilbert methods to Banach spaces  in this paper  we consider the simple problem of learning a Parzen window classifier in a reproducing kernel Banach space (RKBS)---which is closely related to the notion of embedding probability measures into an RKBS---in order to carefully understand its pros and cons over the Hilbert space classifier. We show that while this generalization yields richer distance measures on probabilities compared to its Hilbert space counterpart  it however suffers from serious computational drawback limiting its practical applicability  which therefore demonstrates the need for developing efficient learning algorithms in Banach spaces.,Learning in Hilbert vs. Banach Spaces: A Measure

Embedding Viewpoint

Bharath K. Sriperumbudur

Gatsby Unit

University College London

Kenji Fukumizu

The Institute of Statistical

Mathematics  Tokyo

Gert R. G. Lanckriet

Dept. of ECE
UC San Diego

bharath@gatsby.ucl.ac.uk

fukumizu@ism.ac.jp

gert@ece.ucsd.edu

Abstract

The goal of this paper is to investigate the advantages and disadvantages of learn-
ing in Banach spaces over Hilbert spaces. While many works have been carried
out in generalizing Hilbert methods to Banach spaces  in this paper  we consider
the simple problem of learning a Parzen window classiﬁer in a reproducing kernel
Banach space (RKBS)—which is closely related to the notion of embedding prob-
ability measures into an RKBS—in order to carefully understand its pros and cons
over the Hilbert space classiﬁer. We show that while this generalization yields
richer distance measures on probabilities compared to its Hilbert space counter-
part  it however suffers from serious computational drawback limiting its practi-
cal applicability  which therefore demonstrates the need for developing efﬁcient
learning algorithms in Banach spaces.

1 Introduction

Kernel methods have been popular in machine learning and pattern analysis for their superior per-
formance on a wide spectrum of learning tasks. They are broadly established as an easy way to
construct nonlinear algorithms from linear ones  by embedding data points into reproducing kernel
Hilbert spaces (RKHSs) [1  14  15]. Over the last few years  generalization of these techniques to
Banach spaces has gained interest. This is because any two Hilbert spaces over a common scalar
ﬁeld with the same dimension are isometrically isomorphic while Banach spaces provide more va-
riety in geometric structures and norms that are potentially useful for learning and approximation.

To sample the literature  classiﬁcation in Banach spaces  more generally in metric spaces were stud-
ied in [3  22  11  5]. Minimizing a loss function subject to a regularization condition on a norm
in a Banach space was studied by [3  13  24  21] and online learning in Banach spaces was con-
sidered in [17]. While all these works have focused on theoretical generalizations of Hilbert space
methods to Banach spaces  the practical viability and inherent computational issues associated with
the Banach space methods has so far not been highlighted. The goal of this paper is to study the
advantages/disadvantages of learning in Banach spaces in comparison to Hilbert space methods  in
particular  from the point of view of embedding probability measures into these spaces.

The concept of embedding probability measures into RKHS [4  6  9  16] provides a powerful and
straightforward method to deal with high-order statistics of random variables. An immediate appli-
cation of this notion is to problems of comparing distributions based on ﬁnite samples: examples
include tests of homogeneity [9]  independence [10]  and conditional independence [7]. Formally 
suppose we are given the set P(X ) of all Borel probability measures deﬁned on the topological
space X   and the RKHS (H  k) of functions on X with k as its reproducing kernel (r.k.). If k is
measurable and bounded  then we can embed P in H as

P 7→ZX

k(·  x) dP(x).

1

(1)

Given the embedding in (1)  the RKHS distance between the embeddings of P and Q deﬁnes a
pseudo-metric between P and Q as

γk(P  Q) :=(cid:13)(cid:13)(cid:13)(cid:13)
ZX
k(·  x) dP(x) andRX

k(·  x) dP(x) −ZX

k(·  x) dQ(x)(cid:13)(cid:13)(cid:13)(cid:13)H

on their embeddingsRX

It is clear that when the embedding in (1) is injective  then P and Q can be distinguished based
k(·  x) dQ(x). [18] related RKHS embeddings to the
problem of binary classiﬁcation by showing that γk(P  Q) is the negative of the optimal risk associ-
ated with the Parzen window classiﬁer in H. Extending this classiﬁer to Banach space and studying
the highlights/issues associated with this generalization will throw light on the same associated with
more complex Banach space learning algorithms. With this motivation  in this paper  we consider
the generalization of the notion of RKHS embedding of probability measures to Banach spaces—in
particular reproducing kernel Banach spaces (RKBSs) [24]—and then compare the properties of the
RKBS embedding to its RKHS counterpart.

.

(2)

To derive RKHS based learning algorithms  it is essential to appeal to the Riesz representation
theorem (as an RKHS is deﬁned by the continuity of evaluation functionals)  which establishes the
existence of a reproducing kernel. This theorem hinges on the fact that a notion of inner product can
be deﬁned on Hilbert spaces. In this paper  as in [24]  we deal with RKBSs that are uniformly Fr´echet
differentiable and uniformly convex (called as s.i.p. RKBS) as many Hilbert space arguments—most
importantly the Riesz representation theorem—can be carried over to such spaces through the notion
of semi-inner-product (s.i.p.) [12]  which is a more general structure than an inner product. Based
on Zhang et al. [24]  who recently developed RKBS counterparts of RKHS based algorithms like
regularization networks  support vector machines  kernel principal component analysis  etc.  we
provide a review of s.i.p. RKBS in Section 3. We present our main contributions in Sections 4 and
5. In Section 4  ﬁrst  we derive an RKBS embedding of P into B′ as

P 7→ZX

K(·  x) dP(x) 

(3)

where B is an s.i.p. RKBS with K as its reproducing kernel (r.k.) and B′ is the topological dual of
B. Note that (3) is similar to (1)  but more general than (1) as K in (3) need not have to be positive
deﬁnite (pd)  in fact  not even symmetric (see Section 3; also see Examples 2 and 3). Based on (3) 
we deﬁne

γK(P  Q) :=(cid:13)(cid:13)(cid:13)(cid:13)
ZX

K(·  x) dP(x) −ZX

K(·  x) dQ(x)(cid:13)(cid:13)(cid:13)(cid:13)B′

 

a pseudo-metric on P(X )  which we show to be the negative of the optimal risk associated with the
Parzen window classiﬁer in B′. Second  we characterize the injectivity of (3) in Section 4.1 wherein
we show that the characterizations obtained for the injectivity of (3) are similar to those obtained for
(1) and coincide with the latter when B is an RKHS. Third  in Section 4.2  we consider the empirical
estimation of γK(P  Q) based on ﬁnite random samples drawn i.i.d. from P and Q and study its
consistency and the rate of convergence. This is useful in applications like two-sample tests (also in
binary classiﬁcation as it relates to the consistency of the Parzen window classiﬁer) where different
P and Q are to be distinguished based on the ﬁnite samples drawn from them and it is important that
the estimator is consistent for the test to be meaningful. We show that the consistency and the rate
of convergence of the estimator depend on the Rademacher type of B′. This result coincides with
the one obtained for γk when B is an RKHS.
The above mentioned results  while similar to results obtained for RKHS embeddings  are signiﬁ-
cantly more general  as they apply RKBS spaces  which subsume RKHSs. We can therefore expect
to obtain “richer” metrics γK than when being restricted to RKHSs (see Examples 1–3). On the
other hand  one disadvantage of the RKBS framework is that γK(P  Q) cannot be computed in a
closed form unlike γk (see Section 4.3). Though this could seriously limit the practical impact of
the RKBS embeddings  in Section 5  we show that closed form expression for γK and its empirical
estimator can be obtained for some non-trivial Banach spaces (see Examples 1–3). However  the
critical drawback of the RKBS framework is that the computation of γK and its empirical estima-
tor is signiﬁcantly more involved and expensive than the RKHS framework  which means a simple
kernel algorithm like a Parzen window classiﬁer  when generalized to Banach spaces suffers from
a serious computational drawback  thereby limiting its practical impact. Given the advantages of
learning in Banach space over Hilbert space  this work  therefore demonstrates the need for the

2

development of efﬁcient algorithms in Banach spaces in order to make the problem of learning in
Banach spaces worthwhile compared to its Hilbert space counterpart. The proofs of the results in
Sections 4 and 5 are provided in the supplementary material.

2 Notation

We introduce some notation that is used throughout the paper. For a topological space X   C(X )
(resp. Cb(X )) denotes the space of all continuous (resp. bounded continuous) functions on X . For
a locally compact Hausdorff space X   f ∈ C(X ) is said to vanish at inﬁnity if for every ǫ > 0 the
set {x : |f (x)| ≥ ǫ} is compact. The class of all continuous f on X which vanish at inﬁnity is
denoted as C0(X ). For a Borel measure µ on X   Lp(X   µ) denotes the Banach space of p-power
(p ≥ 1) µ-integrable functions. For a function f deﬁned on Rd  ˆf and f∨ denote the Fourier
and inverse Fourier transforms of f . Since ˆf and f∨ on Rd can be deﬁned in L1  L2 or more
generally in distributional senses  they should be treated in the appropriate sense depending on the
context. In the L1 sense  the Fourier and inverse Fourier transforms of f ∈ L1(Rd) are deﬁned as:
ˆf (y) = (2π)−d/2RRd f (x) e−ihy xi dx and f∨(y) = (2π)−d/2RRd f (x) eihy xi dx  where i denotes
the imaginary unit √−1. φP :=RRd eih· xi dP(x) denotes the characteristic function of P.

3 Preliminaries: Reproducing Kernel Banach Spaces

In this section  we brieﬂy review the theory of RKBSs  which was recently studied by [24] in the
context of learning in Banach spaces. Let X be a prescribed input space.
Deﬁnition 1 (Reproducing kernel Banach space). An RKBS B on X is a reﬂexive Banach space of
functions on X such that its topological dual B′ is isometric to a Banach space of functions on X
and the point evaluations are continuous linear functionals on both B and B′.
Note that if B is a Hilbert space  then the above deﬁnition of RKBS coincides with that of an RKHS.
Let (· ·)B be a bilinear form on B× B′ wherein (f  g∗)B := g∗(f )  f ∈ B  g∗ ∈ B′. Theorem 2 in
[24] shows that if B is an RKBS on X   then there exists a unique function K : X × X → C called
the reproducing kernel (r.k.) of B  such that the following hold:

(a1) K(x ·) ∈ B  K(·  x) ∈ B′  x ∈ X  
(a2) f (x) = (f  K(·  x))B  f∗(x) = (K(x ·)  f∗)B  f ∈ B  f∗ ∈ B′  x ∈ X .

Note that K satisﬁes K(x  y) = (K(x ·)  K(·  y))B and therefore K(·  x) and K(x ·) are reproduc-
ing kernels for B and B′ respectively. When B is an RKHS  K is indeed the r.k. in the usual sense.
Though an RKBS has exactly one r.k.  different RKBSs may have the same r.k. (see Example 1) un-
like an RKHS  where no two RKHSs can have the same r.k (by the Moore-Aronszajn theorem [4]).
Due to the lack of inner product in B (unlike in an RKHS)  it can be shown that the r.k. for a general
RKBS can be any arbitrary function on X ×X for a ﬁnite set X [24]. In order to have a substitute for
inner products in the Banach space setting  [24] considered RKBS B that are uniformly Fr´echet dif-
ferentiable and uniformly convex (referred to as s.i.p. RKBS) as it allows Hilbert space arguments to
be carried over to B—most importantly  an analogue to the Riesz representation theorem holds (see
Theorem 3)—through the notion of semi-inner-product (s.i.p.) introduced by [12]. In the following 
we ﬁrst present results related to general s.i.p. spaces and then consider s.i.p. RKBS.
Deﬁnition 2 (S.i.p. space). A Banach space B is said to be uniformly Fr´echet differentiable if for
all f  g ∈ B  limt∈R t→0 kf +tgkB−kfkB
exists and the limit is approached uniformly for f  g in the
unit sphere of B. B is said to be uniformly convex if for all ǫ > 0  there exists a δ > 0 such that
kf + gkB ≤ 2 − δ for all f  g ∈ B with kfkB = kgkB = 1 and kf − gkB ≥ ǫ. B is called an
s.i.p. space if it is both uniformly Fr´echet differentiable and uniformly convex.
Note that uniform Fr´echet differentiability and uniform convexity are properties of the norm associ-
ated with B. [8  Theorem 3] has shown that if B is an s.i.p. space  then there exists a unique function
[· ·]B : B × B → C  called the semi-inner-product such that for all f  g  h ∈ B and λ ∈ C:

t

(a3) [f + g  h]B = [f  h]B + [g  h]B 
(a4) [λf  g]B = λ[f  g]B  [f  λg]B = λ[f  g]B 
(a5) [f  f ]B =: kfk2

B > 0 for f 6= 0 

3

t

kfkB

= Re([g f ]B)

(a6) (Cauchy-Schwartz) |[f  g]B|2 ≤ kfk2

B 
Bkgk2
and limt∈R t→0 kf +tgkB−kfkB
  f  g ∈ B  f 6= 0  where Re(α) and α represent the
real part and complex conjugate of a complex number α. Note that s.i.p. in general do not satisfy
conjugate symmetry  [f  g]B = [g  f ]B for all f  g ∈ B and therefore is not linear in the second
argument  unless B is a Hilbert space  in which case the s.i.p. coincides with the inner product.
Suppose B is an s.i.p. space. Then for each h ∈ B  f 7→ [f  h]B deﬁnes a continuous linear
functional on B  which can be identiﬁed with a unique element h∗ ∈ B′  called the dual function of
h. By this deﬁnition of h∗  we have h∗(f ) = (f  h∗)B = [f  h]B  f  h ∈ B. Using the structure of
s.i.p.  [8  Theorem 6] provided the following analogue in B to the Riesz representation theorem of
Hilbert spaces.
Theorem 3 ([8]). Suppose B is an s.i.p. space. Then

(a7) (Riesz representation theorem) For each g ∈ B′  there exists a unique h ∈ B such that
(a8) B′ is an s.i.p. space with respect to the s.i.p. deﬁned by [h∗  f∗]B′ := [f  h]B  f  h ∈ B

g = h∗  i.e.  g(f ) = [f  h]B  f ∈ B and kgkB′ = khkB.

and kh∗kB′ := [h∗  h∗]1/2
B′ .

For more details on s.i.p. spaces  we refer the reader to [8]. A concrete example of an s.i.p. space
is as follows  which will prove to be useful in Section 5. Let (X   A   µ) be a measure space and
B := Lp(X   µ) for some p ∈ (1  +∞). It is an s.i.p. space with dual B′ := Lq(X   µ) where
q = p
. Consequently  the semi-inner-
product on B is

p−1 . For each f ∈ B  its dual element in B′ is f∗ = f|f|p−2
kfkp−2
f g|g|p−2 dµ
kgkp−2
Lp(X  µ)

[f  g]B = g∗(f ) = RX

.

(4)

Having introduced s.i.p. spaces  we now discuss s.i.p. RKBS which was studied by [24]. Using the
Riesz representation for s.i.p. spaces (see (a7))  Theorem 9 in [24] shows that if B is an s.i.p. RKBS 
then there exists a unique r.k. K : X × X → C and a s.i.p. kernel G : X × X → C such that:

Lp (X  µ)

(a9) G(x ·) ∈ B for all x ∈ X   K(·  x) = (G(x ·))∗  x ∈ X  
(a10) f (x) = [f  G(x ·)]B  f∗(x) = [K(x ·)  f ]B for all f ∈ B  x ∈ X .

It is clear that G(x  y) = [G(x ·)  G(y ·)]B  x  y ∈ X . Since s.i.p. in general do not satisfy conju-
gate symmetry  G need not be Hermitian nor pd [24  Section 4.3]. The r.k. K and the s.i.p. kernel
G coincide when span{G(x ·) : x ∈ X} is dense in B  which is the case when B is an RKHS [24 
Theorems 2  10 and 11]. This means when B is an RKHS  then the conditions (a9) and (a10) reduce
to the well-known reproducing properties of an RKHS with the s.i.p. reducing to an inner product.

4 RKBS Embedding of Probability Measures

In this section  we present our main contributions of deriving and analyzing the RKBS embedding
of probability measures  which generalize the theory of RKHS embeddings. First  we would like to
remind the reader that the RKHS embedding in (1) can be derived by choosing F = {f : kfkH ≤ 1}
in

γF(P  Q) = sup

f∈F(cid:12)(cid:12)(cid:12)(cid:12)
ZX

f dP −ZX

f dQ(cid:12)(cid:12)(cid:12)(cid:12) .

See [19  20] for details. Similar to the RKHS case  in Theorem 4  we show that the RKBS embed-
dings can be obtained by choosing F = {f : kfkB ≤ 1} in γF(P  Q). Interestingly  though B does
not have an inner product  it can be seen that the structure of semi-inner-product is sufﬁcient enough
to generate an embedding similar to (1).
Theorem 4. Let B be an s.i.p. RKBS deﬁned on a measurable space X with G as the s.i.p. kernel
and K as the reproducing kernel with both G and K being measurable. Let F = {f : kfkB ≤ 1}
and G be bounded. Then

γK(P  Q) := γF(P  Q) =(cid:13)(cid:13)(cid:13)(cid:13)
ZX

K(·  x) dP(x) −ZX

K(·  x) dQ(x)(cid:13)(cid:13)(cid:13)(cid:13)B′

.

(5)

4

Based on Theorem 4 

it is clear that P can be seen as being embedded into B′ as P 7→
K(·  x) dP(x) and γK(P  Q) is the distance between the embeddings of P and Q. Therefore 
we arrive at an embedding which looks similar to (1) and coincides with (1) when B is an RKHS.

RX

Given these embeddings  two questions that need to be answered for these embeddings to be practi-
cally useful are: (⋆) When is the embedding injective? and (⋆⋆) Can γK(P  Q) in (5) be estimated
consistently and computed efﬁciently from ﬁnite random samples drawn i.i.d. from P and Q? The
signiﬁcance of (⋆) is that if (3) is injective  then such an embedding can be used to differentiate
between different P and Q  which can then be used in applications like two-sample tests to differen-
tiate between P and Q based on samples drawn i.i.d. from them if the answer to (⋆⋆) is afﬁrmative.
These questions are answered in the following sections.

Before that  we show how these questions are important in binary classiﬁcation. Following [18]  it
can be shown that γK is the negative of the optimal risk associated with a Parzen window classiﬁer
in B′  that separates the class-conditional distributions P and Q (refer to the supplementary material
for details). This means that if (3) is not injective  then the maximum risk is attained for P 6= Q  i.e. 
distinct distributions are not classiﬁable. Therefore  the injectivity of (3) is of primal importance in
applications. In addition  the question in (⋆⋆) is critical as well  as it relates to the consistency of the
Parzen window classiﬁer.

4.1 When is (3) injective?

The following result provides various characterizations for the injectivity of (3)  which are similar
(but more general) to those obtained for the injectivity of (1) and coincide with the latter when B is
an RKHS.
Theorem 5 (Injectivity of γK). Suppose B is an s.i.p. RKBS deﬁned on a topological space X with
K and G as its r.k. and s.i.p. kernel respectively. Then the following hold:
(a) Let X be a Polish space that is also locally compact Hausdorff. Suppose G is bounded and
K(x ·) ∈ C0(X ) for all x ∈ X . Then (3) is injective if B is dense in C0(X ).
(b) Suppose the conditions in (a) hold. Then (3) is injective if B is dense in Lp(X   µ) for any Borel
probability measure µ on X and some p ∈ [1 ∞).
Since it is not easy to check for the denseness of B in C0(X ) or Lp(X   µ)  in Theorem 6  we present
an easily checkable characterization for the injectivity of (3) when K is bounded continuous and
translation invariant on Rd. Note that Theorem 6 generalizes the characterization (see [19  20]) for
the injectivity of RKHS embedding (in (1)).
Theorem 6 (Injectivity of γK for translation invariant K). Let X = Rd. Suppose K(x  y) =
ψ(x − y)  where ψ : Rd → R is of the form ψ(x) = RRd eihx ωi dΛ(ω) and Λ is a ﬁnite complex-
valued Borel measure on Rd. Then (3) is injective if supp(Λ) = Rd. In addition if K is symmetric 
then the converse holds.
Remark 7. If ψ in Theorem 6 is a real-valued pd function  then by Bochner’s theorem  Λ has to be
real  nonnegative and symmetric  i.e.  Λ(dω) = Λ(−dω). Since ψ need not be a pd function for K
to be a real  symmetric r.k. of B  Λ need not be nonnegative. More generally  if ψ is a real-valued
function on Rd  then Λ is conjugate symmetric  i.e.  Λ(dω) = Λ(−dω). An example of a translation
invariant  real and symmetric (but not pd) r.k. that satisﬁes the conditions of Theorem 6 can be
obtained with ψ(x) = (4x6 + 9x4 − 18x2 + 15) exp(−x2). See Example 3 for more details.
4.2 Consistency Analysis

Consider a two-sample test  wherein given two sets of random samples  {Xj}m
j=1
drawn i.i.d. from distributions P and Q respectively  it is required to test whether P = Q or not.
Given a metric  γK on P(X )  the problem can equivalently be posed as testing for γK(P  Q) = 0 or
not  based on {Xj}m
j=1  in which case  γK(P  Q) is estimated based on these random
samples. For the test to be meaningful  it is important that this estimate of γK is consistent. [9]
showed that γK(Pm  Qn) is a consistent estimator of γK(P  Q) when B is an RKHS  where Pm :=
mPm
1
j=1 δYj and δx represents the Dirac measure at x ∈ X. Theorem 9
generalizes the consistency result in [9] by showing that γK(Pm  Qn) is a consistent estimator of

j=1 and {Yj}n

j=1 and {Yj}n

j=1 δXj   Qn := 1

nPn

5

j=1 ̺jfjkt

j=1 kfjkt

B)1/t  where {̺j}N

B)1/t ≤ C∗(PN

γK(P  Q) and the rate of convergence is O(m(1−t)/t + n(1−t)/t) if B′ is of type t  1 < t ≤ 2. Before
we present the result  we deﬁne the type of a Banach space  B [2  p. 303].
Deﬁnition 8 (Rademacher type of B). Let 1 ≤ t ≤ 2. A Banach space B is said to be of t-
Rademacher (or  more shortly  of type t) if there exists a constant C∗ such that for any N ≥ 1
j=1 ⊂ B: (EkPN
and any {fj}N
j=1 are
i.i.d. Rademacher (symmetric ±1-valued) random variables.
Clearly  every Banach space is of type 1. Since having type t′ for t′ > t implies having type t  let us
deﬁne t∗(B) := sup{t : B has type t}.
Theorem 9 (Consistency of γK(Pm  Qn)). Let B be an s.i.p. RKBS. Assume ν := sup{pG(x  x) :
i.i.d.∼ P
x ∈ X} < ∞. Fix δ ∈ (0  1). Then with probability 1−δ over the choice of samples {Xj}m
and {Yj}n
2(cid:17) 

|γK(Pm  Qn) − γK(P  Q)| ≤ 2C∗ν(cid:16)m

t (cid:17) +p18ν2 log(4/δ)(cid:16)m− 1

where t = t∗(B′) and C∗ is some universal constant.
It is clear from Theorem 9 that if t∗(B′) ∈ (1  2]  then γK(Pm  Qn) is a consistent estimator of
γK(P  Q). In addition  the best rate is obtained if t∗(B′) = 2  which is the case if B is an RKHS. In
Section 5  we will provide examples of s.i.p. RKBSs that satisfy t∗(B′) = 2.

i.i.d.∼ Q  we have

2 + n− 1

j=1

1−t

t + n

1−t

j=1

4.3 Computation of γK(P  Q)

We now consider the problem of computing γK(P  Q) and γK(Pm  Qn). Deﬁne λ∗P

:=

RX

K(·  x) dP(x). Consider
γ2
K(P  Q) = kλ∗P − λ∗Qk2

(a5)

B′

= [λ∗P − λ∗Q  λ∗P − λ∗Q]B′
K(·  x) dP(x)  λ∗P − λ∗QiB′ −hZX
[K(·  x)  λ∗P − λ∗Q]B′ dP(x) −ZX
K(·  y) d(P − Q)(y)iB′

= hZX
= ZX
= ZX hK(·  x) ZX

(∗)

(a3)

= [λ∗P  λ∗P − λ∗Q]B′ − [λ∗Q  λ∗P − λ∗Q]B′
K(·  x) dQ(x)  λ∗P − λ∗QiB′
[K(·  x)  λ∗P − λ∗Q]B′ dQ(x)

d(P − Q)(x) 

(6)

where (∗) is proved in the supplementary material. (6) is not reducible as the s.i.p. is not linear in
the second argument unless B is a Hilbert space. This means γK(P  Q) is not representable in terms
of the kernel function  K(x  y) unlike in the case of B being an RKHS  in which case the s.i.p. in
(6) reduces to an inner product providing

γ2

K(P  Q) =ZZX

K(x  y) d(P − Q)(x) d(P − Q)(y).

Since this issue holds for any P  Q ∈ P(X )  it also holds for Pm and Qn  which means γK(Pm  Qn)
cannot be computed in a closed form in terms of the kernel  K(x  y) unlike in the case of an RKHS
where γK(Pm  Qn) can be written as a simple V-statistic that depends only on K(x  y) computed
j=1. This is one of the main drawbacks of the RKBS approach where the
at {Xj}m
s.i.p. structure does not allow closed form representations in terms of the kernel K (also see [24]
where regularization algorithms derived in RKBS are not solvable unlike in an RKHS)  and therefore
could limit its practical viability. However  in the following section  we present non-trivial examples
of s.i.p. RKBSs for which γK(P  Q) and γK(Pm  Qn) can be obtained in closed forms.

j=1 and {Yj}n

5 Concrete Examples of RKBS Embeddings

In this section  we present examples of RKBSs and then derive the corresponding γK(P  Q) and
γK(Pm  Qn) in closed forms. To elaborate  we present three examples that cover the spectrum:
Example 1 deals with RKBS (in fact a family of RKBSs induced by the same r.k.) whose r.k. is pd 
Example 2 with RKBS whose r.k. is not symmetric and therefore not pd and Example 3 with RKBS
whose r.k. is symmetric but not pd. These examples show that the Banach space embeddings result
in richer metrics on P(X ) than those obtained through RKHS embeddings.

6

(7)

(8)

eihx ·i d(P − Q)(x)(cid:13)(cid:13)(cid:13)Lq(Rd µ)

= kφP − φQkLq(Rd µ) .

Example 1 (K is positive deﬁnite). Let µ be a ﬁnite nonnegative Borel measure on Rd. Then for
any 1 < p < ∞ with q = p
p−1

u(t)eihx ti dµ(t) : u ∈ Lp(Rd  µ)  x ∈ Rd(cid:27)  
is an RKBS with K(x  y) = G(x  y) = (µ(Rd))(p−2)/pRRd e−ihx−y ti dµ(t) as the r.k. and

Bpd

p (Rd) :=(cid:26)fu(x) =ZRd
γK(P  Q) =(cid:13)(cid:13)(cid:13)ZRd

First note that K is a translation invariant pd kernel on Rd as it is the Fourier transform of a
nonnegative ﬁnite Borel measure  µ  which follows from Bochner’s theorem. Therefore  though the
s.i.p. kernel and the r.k. of an RKBS need not be symmetric  the space in (7) is an interesting example
of an RKBS  which is induced by a pd kernel. In particular  it can be seen that many RKBSs (Bpd
p (Rd)
for any 1 < p < ∞) have the same r.k (ignoring the scaling factor which can be made one for any
p by choosing µ to be a probability measure). Second  note that Bpd
p is an RKHS when p = q = 2
and therefore (8) generalizes γk(P  Q) = kφP − φQkL2(Rd µ). By Theorem 6  it is clear that γK in
(8) is a metric on P(Rd) if and only if supp(µ) = Rd. Refer to the supplementary material for an
interpretation of Bpd
Example 2 (K is not symmetric). Let µ be a ﬁnite nonnegative Borel measure such that its moment-

p (Rd) as a generalization of Sobolev space [23  Chapter 10].

generating function  i.e.  Mµ(x) :=RRd ehx ti dµ(t) exists. Then for any 1 < p < ∞ with q = p

p−1

Bns

p (Rd) :=(cid:26)fu(x) =ZRd

u(t)ehx ti dµ(t) : u ∈ Lp(Rd  µ)  x ∈ Rd(cid:27)

2

3
2

3

but

not

ψ(x)

Bsnpd

(K is

deﬁnite).

symmetric

p (Rd) is an RKHS.
Let

(R) :=(cid:26)fu(x) =ZR

is an RKBS with K(x  y) = G(x  y) = (Mµ(qx))(p−2)/p Mµ(x(q − 1) + y) as the r.k. Suppose
P and Q are such that MP and MQ exist. Then γK(P  Q) = kRRd ehx ·i d(P − Q)(x)kLq(Rd µ) =
kMP − MQkLq(Rd µ)  which is the weighted Lq distance between the moment-generating functions
of P and Q. It is easy to see that if supp(µ) = Rd  then γK(P  Q) = 0 ⇒ MP = MQ a.e. ⇒ P =
Q  which means γK is a metric on P(Rd). Note that K is not symmetric (for q 6= 2) and therefore
is not pd. When p = q = 2  K(x  y) = Mµ(x + y) is pd and Bns
Example
positive
=
Ae−x2(cid:0)4x6 + 9x4 − 18x2 + 15(cid:1) with A := (1/243)(cid:0)4π2/25(cid:1)1/6. Then
u(t) dt : u ∈ L

(x − t)2e− 3(x−t)2
is an RKBS with r.k. K(x  y) = g(x  y) = ψ(x − y). Clearly  ψ and therefore K are not pd
34992√2(cid:0)x6 − 39x4 + 216x2 − 324(cid:1) is not nonnegative at
(though symmetric on R) as bψ(x) = −e− x2
every x ∈ R. Refer to the supplementary material for the derivation of K and bψ. In addition 
γK(P  Q) = kRR θ(· − x) d(P − Q)(x)kLq(R) = k(bθ (φP − φQ))∨kLq(R)  where θ(t) = t2e− 3
Since supp(bθ) = R  we have γK(P  Q) = 0 ⇒ (bθ (φP−φQ))∨ = 0 ⇒ bθ (φP−φQ) = 0 ⇒ φP = φQ

a.e.  which implies P = Q and therefore γK is a metric on P(R).
So far  we have presented different examples of RKBSs  wherein we have demonstrated the nature
of the r.k.  derived the Banach space embeddings in closed form and studied the conditions under
which it is injective. These examples also show that the RKBS embeddings result in richer distance
measures on probabilities compared to those obtained by the RKHS embeddings—an advantage
gained by moving from Hilbert to Banach spaces. Now  we consider the problem of computing
γK(Pm  Qn) in closed form and its consistency. In Section 4.3  we showed that γK(Pm  Qn) does
not have a nice closed form expression unlike in the case of B being an RKHS. However  in the
following  we show that for K in Examples 1–3  γK(Pm  Qn) has a closed form expression for
certain choices of q. Let us consider the estimation of γK(P  Q):

2 (R)  x ∈ R(cid:27)

2 t2.

3

4

γq

K(Pm  Qn) =(cid:13)(cid:13)(cid:13)(cid:13)
ZX
=ZX (cid:12)(cid:12)(cid:12)

q

b(x ·) d(Pm − Qn)(x)(cid:13)(cid:13)(cid:13)(cid:13)
nXj=1
mXj=1

b(Xj  t) −

1
n

1
m

Lq(X  µ)

b(Yj  t)(cid:12)(cid:12)(cid:12)

7

=ZX (cid:12)(cid:12)(cid:12)ZX

q

dµ(t) 

b(x  t) d(Pm − Qn)(x)(cid:12)(cid:12)(cid:12)

q

dµ(t)

(9)

where b(x  t) = eihx ti in Example 1  b(x  t) = ehx ti in Example 2 and b(x  t) = θ(x − t) with
q = 3 and µ being the Lebesgue measure in Example 3. Since the duals of RKBSs considered
in Examples 1–3 are of type min(q  2) for 1 ≤ q ≤ ∞ [2  p. 304]  by Theorem 9  γK(Pm  Qn)
) for q ∈
estimates γK(P  Q) consistently at a convergence rate of O(m
(1 ∞)  with the best rate of O(m−1/2 + n−1/2) attainable when q ∈ [2 ∞). This means for
q ∈ (2 ∞)  the same rate as attainable by the RKHS can be achieved. Now  the problem reduces
to computing γK(Pm  Qn). Note that (9) cannot be computed in a closed form for all q—see the
discussion in the supplementary material about approximating γK(Pm  Qn). However  when q = 2 
(9) can be computed very efﬁciently in closed form (in terms of K) as a V-statistic [9]  given by

min(q 2) + n

max(1−q −1)

max(1−q −1)

min(q 2)

n2

− 2

γq

(11)

mXj l=1

q

2−p

γ2
K(Pm  Qn) =

A(x1 ... xq)

}|

m2

sYj=1

+

nXj l=1

K(Xj  Xl)

K(Yj  Yl)

K(Xj  Yl)

mn

.

(10)

K(Pm  Qn) =ZX

{
b(x2j−1  t)b(x2j   t) dµ(t)

nXl=1
mXj=1
More generally  it can be shown that if q = 2s  s ∈ N  then (9) reduces to
qYj=1

z
···ZX
ZX
for which closed form computation is possible for appropriate choices of b and µ. Refer to
the supplementary material for the derivation of (11). For b and µ as in Example 1  we have
j=1 x2j(cid:17)  while for b and µ as in Example 2 
p K(cid:16)Ps
A(x1  . . .   xq) = (µ(Rd))
we have A(x1  . . .   xq) = Mµ(Pq
j=1 xj ). By appropriately choosing θ and µ in Example 3  we
can obtain a closed form expression for A(x1  . . .   xq)  which is proved in the supplementary mate-
rial. Note that choosing s = 1 in (11) results in (10). (11) shows that γq
K(Pm  Qn) can be computed
in a closed form in terms of A at a complexity of O(mq)  assuming m = n  which means the least
complexity is obtained for q = 2. The above discussion shows that for appropriate choices of q 
i.e.  q ∈ (2 ∞)  the RKBS embeddings in Examples 1–3 are useful in practice as γK(Pm  Qn) is
consistent and has a closed form expression. However  the drawback of the RKBS framework is that
the computation of γK(Pm  Qn) is more involved than its RKHS counterpart.

j=1 x2j−1 Ps

d(Pm − Qn)(xj )

6 Conclusion & Discussion

With a motivation to study the advantages/disadvantages of generalizing Hilbert space learning algo-
rithms to Banach spaces  in this paper  we generalized the notion of RKHS embedding of probability
measures to Banach spaces  in particular RKBS that are uniformly Fr´echet differentiable and uni-
formly convex—note that this is equivalent to generalizing a RKHS based Parzen window classiﬁer
to RKBS. While we showed that most of results in RKHS like injectivity of the embedding  con-
sistency of the Parzen window classiﬁer  etc.  nicely generalize to RKBS yielding richer distance
measures on probabilities  the generalized notion is less attractive in practice compared to its RKHS
counterpart because of the computational disadvantage associated with it. Since most of the existing
literature on generalizing kernel methods to Banach spaces deal with more complex algorithms than
a simple Parzen window classiﬁer that is considered in this paper  we believe that most of these
algorithms may have limited practical applicability  though they are theoretically appealing. This 
therefore raises an important open problem of developing computationally efﬁcient Banach space
based learning algorithms.

Acknowledgments

The authors thank the anonymous reviewers for their constructive comments that improved the pre-
sentation of the paper. Part of the work was done while B. K. S. was a Ph. D. student at UC
San Diego. B. K. S. and G. R. G. L. acknowledge support from the National Science Foundation
(grants DMS-MSPA 0625409 and IIS-1054960). K. F. was supported in part by JSPS KAKENHI
(B) 22300098.

References
[1] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc.  68:337–404  1950.

8

[2] B. Beauzamy. Introduction to Banach spaces and their Geometry. North-Holland  The Netherlands  1985.
[3] K. Bennett and E. Bredensteiner. Duality and geometry in svm classiﬁer. In Proc. 17th International

Conference on Machine Learning  pages 57–64  2000.

[4] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics.

Kluwer Academic Publishers  London  UK  2004.

[5] R. Der and D. Lee. Large-margin classiﬁcation in Banach spaces. In JMLR Workshop and Conference

Proceedings  volume 2  pages 91–98. AISTATS  2007.

[6] K. Fukumizu  F. R. Bach  and M. I. Jordan. Dimensionality reduction for supervised learning with repro-

ducing kernel Hilbert spaces. Journal of Machine Learning Research  5:73–99  2004.

[7] K. Fukumizu  A. Gretton  X. Sun  and B. Sch¨olkopf. Kernel measures of conditional dependence. In J.C.
Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems
20  pages 489–496  Cambridge  MA  2008. MIT Press.

[8] J. R. Giles. Classes of semi-inner-product spaces. Trans. Amer. Math. Soc.  129:436–446  1967.
[9] A. Gretton  K. M. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. Smola. A kernel method for the two sample
problem. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Information Processing
Systems 19  pages 513–520. MIT Press  2007.

[10] A. Gretton  K. Fukumizu  C. H. Teo  L. Song  B. Sch¨olkopf  and A. J. Smola. A kernel statistical test of
independence. In J. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information
Processing Systems 20  pages 585–592. MIT Press  2008.

[11] M. Hein  O. Bousquet  and B. Sch¨olkopf. Maximal margin classiﬁcation for metric spaces. J. Comput.

System Sci.  71:333–359  2005.

[12] G. Lumer. Semi-inner-product spaces. Trans. Amer. Math. Soc.  100:29–43  1961.
[13] C. A. Micchelli and M. Pontil. A function representation for learning in Banach spaces. In Conference

on Learning Theory  2004.

[14] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.
[15] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press 

UK  2004.

[16] A. J. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A Hilbert space embedding for distributions. In
Proc. 18th International Conference on Algorithmic Learning Theory  pages 13–31. Springer-Verlag 
Berlin  Germany  2007.

[17] K. Sridharan and A. Tewari. Convex games in Banach spaces. In Conference on Learning Theory  2010.
[18] B. K. Sriperumbudur  K. Fukumizu  A. Gretton  G. R. G. Lanckriet  and B. Sch¨olkopf. Kernel choice
and classiﬁability for RKHS embeddings of probability distributions.
In Y. Bengio  D. Schuurmans 
J. Lafferty  C. K. I. Williams  and A. Culotta  editors  Advances in Neural Information Processing Systems
22  pages 1750–1758. MIT Press  2009.

[19] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  G. R. G. Lanckriet  and B. Sch¨olkopf. Injective Hilbert
space embeddings of probability measures. In R. Servedio and T. Zhang  editors  Proc. of the 21st Annual
Conference on Learning Theory  pages 111–122  2008.

[20] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Sch¨olkopf  and G. R. G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research  11:1517–1561 
2010.

[21] H. Tong  D.-R. Chen  and F. Yang. Least square regression with ℓp-coefﬁcient regularization. Neural

Computation  22:3221–3235  2010.

[22] U. von Luxburg and O. Bousquet. Distance-based classiﬁcation with Lipschitz functions. Journal for

Machine Learning Research  5:669–695  2004.

[23] H. Wendland. Scattered Data Approximation. Cambridge University Press  Cambridge  UK  2005.
[24] H. Zhang  Y. Xu  and J. Zhang. Reproducing kernel Banach spaces for machine learning. Journal of

Machine Learning Research  10:2741–2775  2009.

9

,Haizi Yu
Tianxi Li
Lav Varshney
Fei Jiang
Guosheng Yin
Francesca Dominici