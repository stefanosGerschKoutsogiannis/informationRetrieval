2016,Optimal Binary Classifier Aggregation for General Losses,We address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting  to minimize prediction loss incurred on the unlabeled data. We find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses  extending a recent analysis of the problem for misclassification error. The result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization  but are minimax optimal without any relaxations. Their decision rules take a form familiar in decision theory -- applying sigmoid functions to a notion of ensemble margin -- without the assumptions typically made in margin-based learning.,Optimal Binary Classiﬁer Aggregation for General

Losses

Akshay Balsubramani

University of California  San Diego

abalsubr@ucsd.edu

Yoav Freund

University of California  San Diego

yfreund@ucsd.edu

Abstract

We address the problem of aggregating an ensemble of predictors with known loss
bounds in a semi-supervised binary classiﬁcation setting  to minimize prediction
loss incurred on the unlabeled data. We ﬁnd the minimax optimal predictions for
a very general class of loss functions including all convex and many non-convex
losses  extending a recent analysis of the problem for misclassiﬁcation error. The
result is a family of semi-supervised ensemble aggregation algorithms which are
as efﬁcient as linear learning by convex optimization  but are minimax optimal
without any relaxations. Their decision rules take a form familiar in decision
theory – applying sigmoid functions to a notion of ensemble margin – without the
assumptions typically made in margin-based learning.

1

Introduction

Consider a binary classiﬁcation problem  in which we are given an ensemble of individual classiﬁers
to aggregate into the most accurate predictor possible for data falling into two classes. Our predic-
tions are measured on a large test set of unlabeled data  on which we know the ensemble classiﬁers’
predictions but not the true test labels. Without using the unlabeled data  the prototypical super-
vised solution is empirical risk minimization (ERM): measure the errors of the ensemble classiﬁers
with labeled data  and then simply predict according to the best classiﬁer. But can we learn a better
predictor by using unlabeled data as well?
This problem is central to semi-supervised learning. The authors of this paper recently derived
the worst-case-optimal solution for it when performance is measured with classiﬁcation error ([1]).
However  this zero-one loss is inappropriate for other common binary classiﬁcation tasks  such as
estimating label probabilities  and handling false positives and false negatives differently. Such goals
motivate the use of different evaluation losses like log loss and cost-weighted misclassiﬁcation loss.
In this paper  we generalize the setup of [1] to these loss functions and a large class of others. Like
the earlier work  the choice of loss function completely speciﬁes the minimax optimal ensemble
aggregation algorithm in our setting  which is efﬁcient and scalable.
The algorithm learns weights over the ensemble classiﬁers by minimizing a convex function. The
optimal prediction on each example in the test set is a sigmoid-like function of a linear combination
of the ensemble predictions  using the learned weighting. Due to the minimax structure  this decision
rule depends solely upon the loss function and upon the structure of the ensemble predictions on data 
with no parameter or model choices.

1.1 Preliminaries
Our setting generalizes that of [1]  in which we are given an ensemble H = {h1  . . .   hp} and unla-
beled (test) examples x1  . . .   xn on which to predict. The ensemble’s predictions on the unlabeled

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

data are written as a matrix F:

F =

h1(x1) h1(x2)

...

...

hp(x1) hp(x2)



··· h1(xn)
...
··· hp(xn)

...

We use vector notation for the rows and columns of F: hi = (hi(x1) ···   hi(xn))(cid:62) and xj =
(h1(xj) ···   hp(xj))(cid:62). Each example j ∈ [n] has a binary label yj ∈ {−1  1}  but the test labels
are allowed to be randomized  represented by values in [−1  1] instead of just the two values {−1  1};
e.g. zi = 1
4. So the labels on the test data can be
represented by z = (z1; . . . ; zn) ∈ [−1  1]n  and are unknown to the predictor  which predicts
g = (g1; . . . ; gn) ∈ [−1  1]n.

2 indicates yi = +1 w.p. 3

4 and −1 w.p. 1

(1)

1.2 Loss Functions

2 (1 − gj) and (cid:96)−(gj) = 1

We incur loss on test example j according to its true label yj. If yj = 1  then the loss of predicting
gj ∈ [−1  1] on it is some function (cid:96)+(gj); and if yj = −1  then the loss is (cid:96)−(gj). To illustrate 
if the loss is the expected classiﬁcation error  then gj ∈ [−1  1] can be interpreted as a randomized
binary prediction in the same way as zj  so that (cid:96)+(gj) = 1
We call (cid:96)± the partial losses here  following earlier work (e.g. [16]). Since the true label can only
be ±1  the partial losses fully specify the decision-theoretic problem we face  and changing them is
tantamount to altering the prediction task.
What could such partial losses conceivably look like in general? Observe that they intuitively mea-
sure discrepancy to the true label ±1. Consequently  it is natural for e.g. (cid:96)+(g) to be decreasing 
as g increases toward the notional true label +1. This suggests that both partial losses (cid:96)+(·) and
(cid:96)−(·) would be monotonic  which we assume hereafter in this paper (throughout we use increasing
to mean “monotonically nondecreasing” and vice versa).
Assumption 1. Over the interval (−1  1)  (cid:96)+(·) is decreasing and (cid:96)−(·) is increasing  and both are
twice differentiable.

2 (1 + gj).

We view Assumption 1 as very mild  as motivated above. Notably  convexity or symmetry of the
partial losses are not required. In this paper  “general losses” refer to loss functions whose partial
losses satisfy Assumption 1  to contrast them with convex losses or other subclasses.
The expected loss incurred w.r.t. the randomized true labels zj is a linear combination of the partial
losses:

(cid:18) 1 + zj

(cid:19)

2

(cid:18) 1 − zj

(cid:19)

2

(cid:96)(zj  gj) :=

(cid:96)+(gj) +

(cid:96)−(gj)

(2)

Decision theory and learning theory have thoroughly investigated the nature of the loss (cid:96) and its
partial losses  particularly how to estimate the “conditional label probability” zj using (cid:96)(zj  gj). A
natural operation to do this is to minimize the loss over gj  and a loss (cid:96) such that arg min
(cid:96)(zj  g) =
g∈[−1 1]
zj (for all zj ∈ [−1  1]) is called a proper loss ([17  16]).

1.3 Minimax Formulation

As in [1]  we formulate the ensemble aggregation problem as a two-player zero-sum game between
a predictor and an adversary. In this game  the ﬁrst player is the predictor  playing predictions over
the test set g ∈ [−1  1]n. The adversary then sets the true labels z ∈ [−1  1]n.
The key idea is that any ensemble constituent i ∈ [p] known to have low loss on the test data gives
us information about the unknown z  as z is constrained to be “close” to the test predictions hi.
Each hypothesis in the ensemble represents such a constraint  and z is in the intersection of all these
constraint sets  which interact in ways that depend on the ensemble predictions F.
Accordingly  for now assume the predictor knows a vector of label correlations b such that

∀i ∈ [p] :

1
n

hi(xj)zj ≥ bi

(3)

n(cid:88)

j=1

2

n Fz ≥ b. When the ensemble is composed of binary classiﬁers which predict in [−1  1]  these
i.e. 1
p inequalities represent upper bounds on individual classiﬁer error rates. These can be estimated
from the training set w.h.p. when the training and test data are i.i.d. using uniform convergence 
exactly as in the prototypical supervised ERM procedure discussed in the introduction ([5]). So in
our game-theoretic formulation  the adversary plays under ensemble constraints deﬁned by b.
The predictor’s goal is to minimize the worst-case expected loss of g on the test data (w.r.t.
randomized labeling z)  using the loss function as deﬁned earlier in Equation (2):

the

n(cid:88)

j=1

(cid:96)(z  g) :=

1
n

(cid:96)(zj  gj)

This goal can be written as the following optimization problem  a two-player zero-sum game:

V := min

g∈[−1 1]n

max
z∈[−1 1]n 
n Fz≥b

1

(cid:96)(z  g)

= min

g∈[−1 1]n

1
n

max
z∈[−1 1]n 
n Fz≥b

1

n(cid:88)

(cid:20)(cid:18) 1 + zj

j=1

2

(cid:19)

(cid:96)+(gj) +

(cid:18) 1 − zj

(cid:19)

2

(cid:21)

(cid:96)−(gj)

(4)

(5)

In this paper  we solve the learning problem faced by the predictor  ﬁnding an optimal strategy
g∗ realizing the minimum in (4) for any given “general loss” (cid:96). This strategy guarantees the best
possible worst-case performance on the unlabeled dataset  with an upper bound of V on the loss.
Indeed  for all z0 and g0 obeying the constraints  Equation (4) implies the tight inequalities

min

g∈[−1 1]n

(cid:96)(z0  g)

(a)≤ V ≤ max
z∈[−1 1]n 
n Fz≥b

1

(cid:96)(z  g0)

(6)

and g∗ attains the equality in (a)  with a worst-case loss as good as any aggregated predictor.
In our formulation of the problem  the constraints on the adversary take a central role. As discussed
in previous work with this formulation ([1  2])  these constraints encode the information we have
about the true labels. Without them  the adversary would ﬁnd it optimal to trivially guarantee error
(arbitrarily close to) 1
2 by simply setting all labels uniformly at random (z = 0n). It is clear that
adding information through more constraints will never raise the error bound V . 1
Nothing has yet been assumed about (cid:96)(z  g) other than Assumption 1. Our main results will require
only this  holding for general losses. This brings us to this paper’s contributions:

1. We give the exact minimax g∗ ∈ [−1  1]n for general losses (Section 2.1). The optimal
prediction on each example j is a sigmoid function of a ﬁxed linear combination of the
ensemble’s p predictions on it  so g∗ is a non-convex function of the ensemble predictions.
By (6)  this incurs the lowest worst-case loss of any predictor constructed with the ensemble
information F and b.

2. We derive an efﬁcient algorithm for learning g∗  by solving a p-dimensional convex opti-
mization problem. This applies to a broad class of losses (cf. Lem. 2)  including any with
convex partial losses. Sec. 2 develops and discusses the results.

3. We extend the optimal g∗ and efﬁcient learning algorithm for it  as above  to a large variety
of more general ensembles and prediction scenarios (Sec. 3)  including constraints arising
from general loss bounds  and ensembles of “specialists” and heterogeneous features.

2 Results for Binary Classiﬁcation
Based on the loss  deﬁne the function Γ : [−1  1] (cid:55)→ R as Γ(g) := (cid:96)−(g) − (cid:96)+(g). (We also write
the vector Γ(g) componentwise with [Γ(g)]j = Γ(gj) for convenience  so that Γ(hi) ∈ Rn and
Γ(xj) ∈ Rp.) Observe that by Assumption 1  Γ(g) is increasing on its domain; so we can discuss
its inverse Γ−1(m)  which is typically sigmoid-shaped  as will be illustrated.
With these we will set up the solution to the game (4)  which relies on a convex function.

1However  it may pose difﬁculties in estimating b by applying uniform convergence over a larger H ([2]).

3

Figure 1: At left are plots of potential wells. At right are optimal prediction functions g  as a function of score.
Both are shown for various losses  as listed in Section 2.3.

Deﬁnition 1 (Potential Well). Deﬁne the potential well

−m + 2(cid:96)−(−1)

(cid:96)+(Γ−1(m)) + (cid:96)−(Γ−1(m))
m + 2(cid:96)+(1)

Ψ(m) :=

if m ≤ Γ(−1)
if m ∈ (Γ(−1)  Γ(1))
if m ≥ Γ(1)

Lemma 2. The potential well Ψ(m) is continuous and 1-Lipschitz. It is also convex under any of
the following conditions:

(A) The partial losses (cid:96)±(·) are convex over (−1  1).
(B) The loss function (cid:96)(· ·) is a proper loss.
+(x) for all x ∈ (−1  1).
(C) (cid:96)(cid:48)

+(x) ≥ (cid:96)(cid:48)(cid:48)

−(x)(cid:96)(cid:48)(cid:48)

−(x)(cid:96)(cid:48)

Condition (C) is also necessary for convexity of Ψ  under Assumption 1.

So the potential wells for different losses are shaped similarly  as seen in Figure 1. Lemma 2 tells
us that the potential well is easy to optimize under any of the given conditions. Note that these
conditions encompass convex surrogate losses commonly used in ERM  including all such “margin-
based” losses (convex univariate functions of zjgj)  introduced primarily for their favorable compu-
tational properties.
An easily optimized potential well beneﬁts us  because the learning problem basically consists of
optimizing it over the unlabeled data  as we will soon make explicit. The function that will actually
be optimized is in terms of the dual parameters  so we call it the slack function.
Deﬁnition 3 (Slack Function). Let σ ≥ 0p be a weight vector over H (not necessarily a distri-
bution). The vector of scores is F(cid:62)σ = (x(cid:62)
n σ)  whose elements’ magnitudes are the
margins. The prediction slack function is

1 σ  . . .   x(cid:62)

γ(σ  b) := γ(σ) := −b(cid:62)σ +

1
n

Ψ(x(cid:62)

j σ)

(7)

n(cid:88)

j=1

An optimal weight vector σ∗ is any minimizer of the slack function: σ∗ ∈ arg min
σ≥0p

[γ(σ)].

2.1 Solution of the Game

These are used to describe the minimax equilibrium of the game (4)  in our main result.
Theorem 4. The minimax value of the game (4) is

−b(cid:62)σ +

n(cid:88)

j=1

1
n



Ψ(x(cid:62)

j σ)

min

g∈[−1 1]n

max
z∈[−1 1]n 
n Fz≥b

1

(cid:96)(z  g) = V =

γ(σ∗) =

1
2

1
2

min
σ≥0p

4

The minimax optimal predictions are deﬁned as follows: for all j ∈ [n] 
j σ∗ ≤ Γ(−1)
j σ∗ ∈ (Γ(−1)  Γ(1))
j σ∗ ≥ Γ(1)

g∗
j := gj(σ∗) =

if x(cid:62)
if x(cid:62)
if x(cid:62)

Γ−1(x(cid:62)
1

−1

j σ∗)

(8)

g∗
j is always an increasing sigmoid  as shown in Figure 1.
We can also redo the proof of Theorem 4 when g ∈ [−1  1]n is not left as a free variable set in the
game  but instead is preset to g(σ) as in (8) for some (possibly suboptimal) weight vector σ.
Observation 5. For any weight vector σ0 ≥ 0p  the worst-case loss after playing g(σ0) is

(cid:96)(z  g(σ0)) ≤ 1
2

γ(σ0)

max
z∈[−1 1]n 
n Fz≥b

1

The proof is a simpliﬁed version of that of Theorem 4; there is no minimum over g to deal with 
and the minimum over σ ≥ 0p in Equation (13) is upper-bounded by using σ0. This result is an
expression of weak duality in our setting  and generalizes Observation 4 of [1].

2.2 Ensemble Aggregation Algorithm

Theorem 4 deﬁnes a prescription for aggregating the given ensemble predictions on the test set.
Learning: Minimize the slack function γ(σ)  ﬁnding the minimizer σ∗ that achieves V .
This is a convex optimization under broad conditions (Lemma 2)  and when the test examples are
i.i.d. the Ψ term is a sum of n i.i.d. functions. Therefore  it is readily amenable to standard ﬁrst-order
optimization methods which require only O(1) test examples at once. In practice  learning employs
such methods to approximately minimize γ  ﬁnding some σA such that γ(σA) ≤ γ(σ∗) +  for
some small . Standard convex optimization methods are guaranteed to do this for binary classiﬁer
ensembles  because the slack function is Lipschitz (Lemma 2) and (cid:107)b(cid:107)∞ ≤ 1.
Prediction: Predict g(σ∗) on any test example  as indicated in (8).
This decouples the prediction task over each test example separately  which requires O(p) time and
memory like p-dimensional linear prediction. After ﬁnding an -approximate minimizer σA in the
learning step as above  Observation 5 tells us that the prediction g(σA) has loss ≤ V + 
2.
In particular  note that there is no algorithmic dependence on n in either step in a statistical learning
setting. So though our formulation is transductive  it is no less tractable than a stochastic optimiza-
tion setting in which i.i.d. data arrive one at a time  and applies to this common situation.

2.3 Examples of Different Losses

To further illuminate Theorem 4  we detail a few special cases in which (cid:96)± are explicitly deﬁned.
These losses may be found throughout the literature (see e.g. [16]). The key functions Ψ and g∗ are
listed for these losses in Appendix A  and in many cases in Figure 1. The nonlinearities used for g∗
are sigmoids  arising solely from the intrinsic minimax structure of the classiﬁcation game.

in [1]  the work we generalize in this paper.

• 0-1 Loss: Here gj is taken to be a randomized binary prediction; this case was developed
• Log Loss  Square Loss
• Cost-Weighted Misclassiﬁcation (Quantile) Loss: This is deﬁned with a parameter c ∈
[0  1] representing the relative cost of false positives vs. false negatives  making the Bayes-
optimal classiﬁer the c-quantile of the conditional probability distribution ([19]).

typically

given

as
. Our formulation is equivalent when the

[0  1]

p  y

for

∈

• Exponential Loss  Logistic Loss
• Hellinger Loss:
is
1 − p − √

+(cid:0)√

(cid:16)(cid:0)√

p − √

y(cid:1)2

This

1 − y(cid:1)2(cid:17)

prediction and label are rescaled to [−1  1].

1
2

5

• “AdaBoost Loss”: If the goal of AdaBoost ([18]) is interpreted as class probability esti-
mation  the implied loss is proper and given in [6  16].
• Absolute Loss and Hinge Loss: The absolute loss can be deﬁned by (cid:96)abs∓ (gj) = 1 ± gj 
and the hinge loss also has (cid:96)abs∓ (gj) = 1 ± gj since the kink in the hinge loss only lies at
gj = ∓1. These partial losses are the same as for 0-1 loss up to scaling  and therefore all
our results for Ψ and g∗ are as well. So these losses are not shown in Appendix A.
• Sigmoid Loss: This is an example of a sigmoid-shaped margin loss  a nonconvex smooth
surrogate for 0-1 loss. Similar losses have arisen in a variety of binary classiﬁcation con-
texts  from robustness (e.g. [9]) to active learning ([10]) and structured prediction ([14]).

2.4 Related Work and Technical Discussion

There are two notable ways in which the result of Theorem 4 is particularly advantageous and gen-
eral. First  the fact that (cid:96)(z  g) can be non-convex in g  yet solvable by convex optimization  is a
major departure from previous work. Second  the solution has a convenient dependence on n (as
in [1])  simply averaging a function over the unlabeled data  which is not only mathematically con-
venient but also makes stochastic O(1)-space optimization practical. This is surprisingly powerful 
because the original minimax problem is jointly over the entire dataset  avoiding further indepen-
dence or decoupling assumptions.
Both these favorable properties stem from the structure of the binary classiﬁcation problem  as
we can describe by examining the optimization problem constructed within the proof of Thm. 4
(Appendix C.1). In it  the constraints which do not explicitly appear with Lagrange parameters are
all box  or L∞ norm  constraints. These decouple over the n test examples  so the problem can
be reduced to the one-dimensional optimization at the heart of Eq. (14)  which is solved ad hoc.
So we are able to obtain minimax results for these non-convex problems – the gi are “clipped”
sigmoid functions because of the bounding effect of the [−1  1] box constraints intrinsic to binary
classiﬁcation. We introduce Lagrange parameters σ only for the p remaining constraints in the
problem  which do not decouple as above  applying globally over the n test examples. However 
these constraints only depend on n as an average over examples (which is how they arise in dual
form in Equation (16) of the proof)  and the loss function itself is also an average (Equation (12)).
This makes the box constraint decoupling possible  and leads to the favorable dependence on n 
making an efﬁcient solution possible to a potentially ﬂagrantly non-convex problem.
To summarize  the technique of optimizing only “halfway into” the dual allows us to readily manip-
ulate the minimax problem exactly without using an approximation like weak duality  despite the
lack of convexity in g. This technique was used implicitly for a different purpose in the “drifting
game” analysis of boosting ([18]  Sec. 13.4.1). Existing boosting work is loosely related to our
approach in being a transductive game invoked to analyze ensemble aggregation  but it does not
consider unlabeled data and draws its power instead from being a repeated game ([18]).
The predecessor to this work ([1]) addresses a problem  0-1 loss minimization  that is known to be
NP-hard when solved directly ([11]). Using the unlabeled data is essential to surmounting this. It
gives the dual problem an independently interesting interpretation  so the learning problem is on the
always-convex Lagrange dual function and is therefore tractable.
This paper’s transductive formulation involves no surrogates or relaxations of the loss  in sharp con-
trast to most previous work. This allows us to bypass the consistency and agnostic-learning discus-
sions ([22  3]) common to ERM methods that use convex risk minimization. Convergence analyses
of those methods make heavy use of convexity of the losses and are generally done presupposing a
linear weighting over H ([21])  whereas here such structure emerges directly from Lagrange duality
and involves no convexity to derive the worst-case-optimal predictions.
The conditions in Assumption 1 are notably general. Differentiability of the partial losses is con-
venient  but not necessary  and only used because ﬁrst-order conditions are a convenient way to
establish convexity of the potential well in Lemma 2. It is never used elsewhere  including in the
minimax arguments used to prove Theorem 4. These manipulations are structured to be valid even if
(cid:96)± are non-monotonic; but in this case  g∗
j could turn out to be multi-valued and hence not a genuine
function of the example’s score.
We emphasize that our result on the minimax equilibrium (Theorem 4) holds for general losses
– the slack function may not be convex unless the further conditions of Lemma 2 are met  but

6

j σ) is always increasing in x(cid:62)

the interpretation of the optimum in terms of margins and sigmoid functions remains. All this
emerges from the inherent decision-theoretic structure of the problem (the proof of Appendix C.1). It
manifests in the fact that the function g(x(cid:62)
j σ for general losses  because
the function Γ is increasing. This monotonicity typically needs to be assumed in a generalized linear
model (GLM; [15]) and related settings. Γ appears loosely analogous to the link function used by
GLMs  with its inverse being used for prediction.
The optimal decision rules emerging from our framework are artiﬁcial neurons of the ensemble in-
puts. Helmbold et al. introduce the notion of a “matching loss” ([13]) for learning the parameters of
a (fully supervised) artiﬁcial neuron with an arbitrary increasing transfer function  effectively taking
the opposite tack of this paper in using a neuron’s transfer function to derive a loss to minimize in or-
der to learn the neuron’s weights by convex optimization. Our assumptions on the loss  particularly
condition (C) of Lemma 2  have arisen independently in earlier online learning work by some of the
same authors ([12]); this may suggest connections between our techniques. We also note that our
family of general losses was deﬁned independently by [19] in the ERM setting (dubbed “F-losses”)
– in which condition (C) of Lemma 2 also has signiﬁcance ([19]  Prop. 2) – but has seemingly
not been revisited thereafter. Further ﬂeshing out the above connections would be interesting future
work.

3 Extensions

We detail a number of generalizations to the basic prediction scenario of Sec. 2. These extensions
are not mutually exclusive  and apply in conjunction with each other  but we list them separately for
clarity. They illustrate the versatility of our minimax framework  particularly Sec. 3.4.

3.1 Weighted Test Sets and Covariate Shift

Though our results here deal with binary classiﬁcation of an unweighted test set  the formulation
deals with a weighted set with only a slight modiﬁcation of the slack function:
Theorem 6. For any vector r ≥ 0n 

−b(cid:62)σ +

n(cid:88)

1
n

1
2

min
σ≥0p

(cid:32)

rjΨ

x(cid:62)
j σ
rj

(cid:33)

n(cid:88)

j=1

1
n

min

rj(cid:96)(zj  gj) =

max
z∈[−1 1]n 
n Fz≥b

1

g∈[−1 1]n
r as the minimizer of the RHS above  the optimal predictions g∗ = g(σ∗

Writing σ∗
r )  as in Theorem 4.
Such weighted classiﬁcation can be parlayed into algorithms for general supervised learning prob-
lems via learning reductions ([4]). Allowing weights on the test set for the evaluation is tantamount
to accounting for known covariate shift in our setting; it would be interesting  though outside our
scope  to investigate scenarios with more uncertainty.

j=1

3.2 General Loss Constraints on the Ensemble

So far in the paper  we have considered the constraints on ensemble classiﬁers as derived from
their label correlations (i.e. 0-1 losses)  as in (3). However  this view can be extended signiﬁcantly
with the same analysis  because any general loss (cid:96)(z  g) is linear in z (Eq. (2))  which allows our
development to go through essentially intact.
In summary  a classiﬁer can be incorporated into our framework for aggregation if we have a gener-
alization loss bound on it  for any “general loss.” This permits an enormous variety of constraint sets 
as each classiﬁer considered can have constraints corresponding to any number of loss bounds on it 
even multiple loss bounds using different losses. For instance  h1 can yield a constraint correspond-
ing to a zero-one loss bound  h2 can yield one constraint corresponding to a square loss bound and
another corresponding to a zero-one loss bound  and so on. Appendix B details this idea  extending
Theorem 4 to general loss constraints.

3.3 Uniform Convergence Bounds for b

In our basic setup  b has been taken as a lower bound on ensemble classiﬁer label correlations.
But as mentioned earlier  the error in estimating b is in fact often quantiﬁed by two-sided uniform

7

convergence (L∞) bounds on b. Constraining z in this fashion amounts to L1 regularization of the
dual vector σ.
Proposition 7. For any  ≥ 0 

−b(cid:62)σ +

n(cid:88)

j=1

1
n



Ψ(x(cid:62)

j σ) + (cid:107)σ(cid:107)1

min

g∈[−1 1]n

max

z∈[−1 1]n 
(cid:107) 1
n Fz−b(cid:107)∞≤

(cid:96)(z  g) = min
σ∈Rp

As in Thm. 4  the optimal g∗ = g(σ∗

∞)  where σ∗

∞ is the minimizer of the right-hand side above.

Here we optimize over all vectors σ (not just nonnegative ones) in an L1-regularized problem  con-
venient in practice because we can cross-validate over the parameter . To our knowledge  this
particular analysis has been addressed in prior work only for the special case of the entropy loss on
the probability simplex  discussed further in [8].
Prop. 7 is a corollary of a more general result using differently scaled label correlation deviations
to regularizing the minimization over σ by its c-weighted L1 norm c(cid:62) |σ| (Thm. 11)  used to
penalize the ensemble nonuniformly ([7]). This situation is motivated by uniform convergence of
heterogeneous ensembles comprised of e.g. “specialist” predictors  for which a union bound ([5])

n Fz − b(cid:12)(cid:12) ≤ c for a general c ≥ 0n. This turns out to be equivalent

n Fz − b(cid:12)(cid:12) with varying coordinates. Such ensembles are discussed next.

within the ensemble  i.e. (cid:12)(cid:12) 1
results in(cid:12)(cid:12) 1

3.4 Heterogenous Ensembles of Specialist Classiﬁers and Features

All the results and algorithms in this paper apply in full generality to ensembles of “specialist”
classiﬁers that only predict on some subset of the test examples. This is done by merely calculating
the constraints over only these examples  and changing F and b accordingly ([2]).
To summarize this from [2]  suppose a classiﬁer i ∈ [p] decides to abstain on an example xj
(j ∈ [n]) with probability 1 − vi(x)  and otherwise predict hi(x). Our only assumption on
j=1 vi(xj) > 0  so classiﬁer i is not a useless

{vi(x1)  . . .   vi(xn)} is the reasonable one that(cid:80)n

specialist that abstains everywhere.
The information about z contributed by classiﬁer i is now not its overall correlation with z on the
entire test set  but rather the correlation with z restricted to the test examples on which it predicts.
n Sz  where the matrix S is formed by reweighting each row of F:
On the test set  this is written as 1

ρ1(x1)h1(x1) ρ1(x2)h1(x2)

ρ2(x1)h2(x1) ρ2(x2)h2(x2)

...

...

ρp(x1)hp(x1) ρp(x2)hp(x2)

  

··· ρ1(xn)h1(xn)
··· ρ2(xn)h2(xn)
...
··· ρp(xn)hp(xn)

...

S := n

(cid:80)n

vi(xj)
k=1 vi(xk)

ρi(xj) :=

(S = F when the entire ensemble consists of non-specialists  recovering our initial setup.) There-
n Sz ≥ bS  where bS gives the label correlations of each
fore  the ensemble constraints (3) become 1
classiﬁer restricted to the examples on which it predicts. Though this rescaling results in entries of
S having different ranges and magnitudes ≥ 1  our results and proofs remain entirely intact.
Indeed  despite the title  this paper applies far more generally than to an ensemble of binary classi-
ﬁers  because our proofs make no assumptions at all about the structure of F. Each predictor in the
ensemble can be thought of as a feature; it has so far been convenient to think of it as binary  fol-
lowing the perspective of binary classiﬁer aggregation  but it could as well be e.g. real-valued  and
the features can have very different scales (as in S above). An unlabeled example x is simply a vec-
tor of features  so arbitrarily abstaining specialists are equivalent to “missing features ” which this
framework handles seamlessly due to the given unlabeled data. Our development applies generally
to semi-supervised binary classiﬁcation.

Acknowledgements

AB is grateful to Chris “Ceej” Tosh for feedback that made the manuscript clearer. This work was
supported by the NSF (grant IIS-1162581).

8

,Akshay Balsubramani
Yoav Freund