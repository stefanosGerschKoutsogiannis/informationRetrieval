2013,A Gang of Bandits,Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications  such as online advertisement and  more generally  recommendation systems.  In many cases  however  these applications have a strong social component  whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance  we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. In this paper  we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically  we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to “share” signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments  carried out on synthetic and real-world datasets  show a marked increase in prediction performance obtained by exploiting the network structure.,A Gang of Bandits

Nicol`o Cesa-Bianchi

Universit`a degli Studi di Milano  Italy
nicolo.cesa-bianchi@unimi.it

Claudio Gentile

University of Insubria  Italy

claudio.gentile@uninsubria.it

Giovanni Zappella

Universit`a degli Studi di Milano  Italy
giovanni.zappella@unimi.it

Abstract

Multi-armed bandit problems formalize the exploration-exploitation trade-offs
arising in several industrially relevant applications  such as online advertisement
and  more generally  recommendation systems. In many cases  however  these
applications have a strong social component  whose integration in the bandit al-
gorithm could lead to a dramatic performance increase. For instance  content may
be served to a group of users by taking advantage of an underlying network of
social relationships among them. In this paper  we introduce novel algorithmic
approaches to the solution of such networked bandit problems. More speciﬁcally 
we design and analyze a global recommendation strategy which allocates a bandit
algorithm to each network node (user) and allows it to “share” signals (contexts
and payoffs) with the neghboring nodes. We then derive two more scalable vari-
ants of this strategy based on different ways of clustering the graph nodes. We
experimentally compare the algorithm and its variants to state-of-the-art methods
for contextual bandits that do not use the relational information. Our experiments 
carried out on synthetic and real-world datasets  show a consistent increase in
prediction performance obtained by exploiting the network structure.

1

Introduction

The ability of a website to present personalized content recommendations is playing an increasingly
crucial role in achieving user satisfaction. Because of the appearance of new content  and due to
the ever-changing nature of content popularity  modern approaches to content recommendation are
strongly adaptive  and attempt to match as closely as possible users’ interests by learning good map-
pings between available content and users. These mappings are based on “contexts”  that is sets of
features that  typically  are extracted from both contents and users. The need to focus on content
that raises the user interest and  simultaneously  the need of exploring new content in order to glob-
ally improve the user experience creates an exploration-exploitation dilemma  which is commonly
formalized as a multi-armed bandit problem. Indeed  contextual bandits have become a reference
model for the study of adaptive techniques in recommender systems (e.g  [5  7  15] ). In many cases 
however  the users targeted by a recommender system form a social network. The network struc-
ture provides an important additional source of information  revealing potential afﬁnities between
pairs of users. The exploitation of such afﬁnities could lead to a dramatic increase in the quality of
the recommendations. This is because the knowledge gathered about the interests of a given user
may be exploited to improve the recommendation to the user’s friends. In this work  an algorithmic
approach to networked contextual bandits is proposed which is provably able to leverage user simi-
larities represented as a graph. Our approach consists in running an instance of a contextual bandit
algorithm at each network node. These instances are allowed to interact during the learning process 

1

sharing contexts and user feedbacks. Under the modeling assumption that user similarities are prop-
erly reﬂected by the network structure  interactions allow to effectively speed up the learning process
that takes place at each node. This mechanism is implemented by running instances of a linear con-
textual bandit algorithm in a speciﬁc reproducing kernel Hilbert space (RKHS). The underlying
kernel  previously used for solving online multitask classiﬁcation problems (e.g.  [8])  is deﬁned in
terms of the Laplacian matrix of the graph. The Laplacian matrix provides the information we rely
upon to share user feedbacks from one node to the others  according to the network structure. Since
the Laplacian kernel is linear  the implementation in kernel space is straightforward. Moreover  the
existing performance guarantees for the speciﬁc bandit algorithm we use can be directly lifted to
the RKHS  and expressed in terms of spectral properties of the user network. Despite its crispness 
the principled approach described above has two drawbacks hindering its practical usage. First 
running a network of linear contextual bandit algorithms with a Laplacian-based feedback sharing
mechanism may cause signiﬁcant scaling problems  even on small to medium sized social networks.
Second  the social information provided by the network structure at hand need not be fully reliable
in accounting for user behavior similarities. Clearly enough  the more such algorithms hinge on
the network to improve learning rates  the more they are penalized if the network information is
noisy and/or misleading. After collecting empirical evidence on the sensitivity of networked bandit
methods to graph noise  we propose two simple modiﬁcations to our basic strategy  both aimed at
circumventing the above issues by clustering the graph nodes. The ﬁrst approach reduces graph
noise simply by deleting edges between pairs of clusters. By doing that  we end up running a scaled
down independent instance of our original strategy on each cluster. The second approach treats each
cluster as a single node of a much smaller cluster network. In both cases  we are able to empirically
improve prediction performance  and simultaneously achieve dramatic savings in running times.
We run experiments on two real-world datasets: one is extracted from the social bookmarking web
service Delicious  and the other one from the music streaming platform Last.fm.

2 Related work

The beneﬁt of using social relationships in order to improve the quality of recommendations is
a recognized fact in the literature of content recommender systems —see e.g.  [5  13  18] and the
survey [3]. Linear models for contextual bandits were introduced in [4]. Their application to person-
alized content recommendation was pioneered in [15]  where the LinUCB algorithm was introduced.
An analysis of LinUCB was provided in the subsequent work [9]. To the best of our knowledge 
this is the ﬁrst work that combines contextual bandits with the social graph information. However 
non-contextual stochastic bandits in social networks were studied in a recent independent work [20].
Other works  such as [2  19]  consider contextual bandits assuming metric or probabilistic dependen-
cies on the product space of contexts and actions. A different viewpoint  where each action reveals
information about other actions’ payoffs  is the one studied in [7  16]  though without the context
provided by feature vectors. A non-contextual model of bandit algorithms running on the nodes of
a graph was studied in [14]. In that work  only one node reveals its payoffs  and the statistical infor-
mation acquired by this node over time is spread across the entire network following the graphical
structure. The main result shows that the information ﬂow rate is sufﬁcient to control regret at each
node of the network. More recently  a new model of distributed non-contextual bandit algorithms
has been presented in [21]  where the number of communications among the nodes is limited  and
all the nodes in the network have the same best action.

3 Learning model

(cid:3)n

of its Laplacian matrix L = (cid:2)Li j

We assume the social relationships over users are encoded as a known undirected and connected
graph G = (V  E)  where V = {1  . . .   n} represents a set of n users  and the edges in E represent
the social links over pairs of users. Recall that a graph G can be equivalently deﬁned in terms
i j=1  where Li i is the degree of node i (i.e.  the number of
incoming/outgoing edges) and  for i (cid:54)= j  Li j equals −1 if (i  j) ∈ E  and 0 otherwise. Learning
proceeds in a sequential fashion: At each time step t = 1  2  . . .   the learner receives a user index
it ∈ V together with a set of context vectors Cit = {xt 1  xt 2  . . .   xt ct} ⊆ Rd. The learner then
selects some kt ∈ Cit to recommend to user it and observes some payoff at ∈ [−1  1]  a function

2

of it and ¯xt = xt kt. No assumptions whatsoever are made on the way index it and set Cit are
generated  in that they can arbitrarily depend on past choices made by the algorithm.1
A standard modeling assumption for bandit problems with contextual information (one that is also
adopted here) is to assume that rewards are generated by noisy versions of unknown linear func-
tions of the context vectors. That is  we assume each node i ∈ V hosts an unknown parame-
ter vector ui ∈ Rd  and that the reward value ai(x) associated with node i and context vector
x ∈ Rd is given by the random variable ai(x) = u(cid:62)
i x + i(x)  where i(x) is a conditionally
zero-mean and bounded variance noise term. Speciﬁcally  denoting by Et[· ] the conditional expec-
that for any ﬁxed i ∈ V and x ∈ Rd  the variable i(x) is conditionally sub-Gaussian with vari-
ance parameter σ2 > 0  namely  Et
This implies Et[i(x)] = 0 and Vt

tation E(cid:2)·(cid:12)(cid:12) (i1  Ci1   a1)  . . .   (it−1  Cit−1   at−1)(cid:3)  we take the general approach of [1]  and assume
(cid:2)exp(γ i(x))(cid:3) ≤ exp(cid:0)σ2 γ2/2(cid:1) for all γ ∈ R and all x  i.
(cid:2)i(x)(cid:3) ≤ σ2  where Vt[·] is a shorthand for the conditional
variance V(cid:2)·(cid:12)(cid:12) (i1  Ci1  a1)  . . .   (it−1  Cit−1   at−1)(cid:3). So we clearly have Et[ai(x)] = u(cid:62)
(cid:2)ai(x)(cid:3) ≤ σ2. Therefore  u(cid:62)

i x and
Vt
i x is the expected reward observed at node i for context vector x.
In the special case when the noise i(x) is a bounded random variable taking values in the range
[−1  1]  this implies Vt[ai(x)] ≤ 4.
The regret rt of the learner at time t is the amount by which the average reward of the best choice in
hindsight at node it exceeds the average reward of the algorithm’s choice  i.e. 

(cid:18)

(cid:19)

rt =

max
x∈Cit

u(cid:62)

it

x

− u(cid:62)

it

¯xt .

lative regret(cid:80)T

The goal of the algorithm is to bound with high probability (over the noise variables it) the cumu-
t=1 rt for the given sequence of nodes i1  . . .   iT and observed context vector sets
Ci1   . . .   CiT . We model the similarity among users in V by making the assumption that nearby
users hold similar underlying vectors ui  so that reward signals received at a given node it at time
t are also  to some extent  informative to learn the behavior of other users j connected to it within
G. We make this more precise by taking the perspective of known multitask learning settings (e.g. 
[8])  and assume that

is small compared to(cid:80)

(1)
i∈V (cid:107)ui(cid:107)2  where (cid:107)·(cid:107) denotes the standard Euclidean norm of vectors. That
is  although (1) may possibly contain a quadratic number of terms  the closeness of vectors lying
on adjacent nodes in G makes this sum comparatively smaller than the actual length of such vec-
tors. This will be our working assumption throughout  one that motivates the Laplacian-regularized
algorithm presented in Section 4  and empirically tested in Section 5.

(cid:107)ui − uj(cid:107)2

(cid:88)

(i j)∈E

4 Algorithm and regret analysis

Our bandit algorithm maintains at time t an estimate wi t for vector ui. Vectors wi t are updated
based on the reward signals as in a standard linear bandit algorithm (e.g.  [9]) operating on the
context vectors contained in Cit. Every node i of G hosts a linear bandit algorithm like the one
described in Figure 1. The algorithm in Figure 1 maintains at time t a prototype vector wt which
is the result of a standard linear least-squares approximation to the unknown parameter vector u
associated with the node under consideration. In particular  wt−1 is obtained by multiplying the
inverse correlation matrix Mt−1 and the bias vector bt−1. At each time t = 1  2  . . .   the algorithm
receives context vectors xt 1  . . .   xt ct contained in Ct  and must select one among them. The
linear bandit algorithm selects ¯xt = xt kt as the vector in Ct that maximizes an upper-conﬁdence-
corrected estimation of the expected reward achieved over context vectors xt k. The estimation
is based on the current wt−1  while the upper conﬁdence level CBt is suggested by the standard
analysis of linear bandit algorithms —see  e.g.  [1  9  10]. Once the actual reward at associated with
¯xt is observed  the algorithm uses ¯xt for updating Mt−1 to Mt via a rank-one adjustment  and bt−1
to bt via an additive update whose learning rate is precisely at. This algorithm can be seen as a
version of LinUCB [9]  a linear bandit algorithm derived from LinRel [4].

1 Formally  it and Cit can be arbitrary (measurable) functions of past rewards a1  . . .   at−1  indices

i1  . . .   it−1  and sets Ci1   . . .   Cit−1.

3

Init: b0 = 0 ∈ Rd and M0 = I ∈ Rd×d;
for t = 1  2  . . .   T do
Set wt−1 = M
Get context Ct = {xt 1  . . .   xt ct};
Set

−1
t−1 bt−1;

(cid:16)

kt = argmax
k=1 ... ct

(cid:62)
t−1xt k + CBt(xt k)

w

(cid:17)

where

CBt(xt k) =

(cid:113)

x(cid:62)
t kM

−1
t−1xt k

σ

(cid:115)

|Mt|
δ

ln



+ (cid:107)u(cid:107)

Set ¯xt = xt kt ;
Observe reward at ∈ [−1  1];
Update

• Mt = Mt−1 + ¯xt ¯x(cid:62)
t  
•

bt = bt−1 + at ¯xt .

end for

Figure 1: Pseudocode of the linear bandit algo-
rithm sitting at each node i of the given graph.

(xt ct )  and modi-

(xt k)  k = 1  . . .   ct;

  where

−1
t−1 bt−1;

(xt 1)  . . .   φit

Init: b0 = 0 ∈ Rdn and M0 = I ∈ Rdn×dn;
for t = 1  2  . . .   T do
Set wt−1 = M
Get it ∈ V   context Cit = {xt 1  . . .   xt ct};
ﬁed vectors(cid:101)φt 1  . . .  (cid:101)φt ct
Construct vectors φit
−1/2⊗ φit
(cid:16)
(cid:62)

t−1(cid:101)φt k + CBt((cid:101)φt k)
σ
t−1(cid:101)φt k
• Mt = Mt−1 +(cid:101)φt kt(cid:101)φ
bt = bt−1 + at(cid:101)φt k .

Observe reward at ∈ [−1  1] at node it;
Update
(cid:62)
t kt

(cid:101)φt k = A
(cid:113)(cid:101)φ

CBt((cid:101)φt k) =

Set kt = argmax
k=1 ... ct

|Mt|
δ

(cid:62)
t kM

(cid:115)

−1

•

w

ln

 

end for

where

(cid:17)

+ (cid:107)(cid:101)U(cid:107)

Figure 2: Pseudocode of the GOB.Lin algo-
rithm.

We now turn to describing our GOB.Lin (Gang Of Bandits  Linear version) algorithm. GOB.Lin lets
the algorithm in Figure 1 operate on each node i of G (we should then add subscript i throughout 
replacing wt by wi t  Mt by Mi t  and so forth). The updates Mi t−1 → Mi t and bi t−1 → bi t
are performed at node i through vector ¯xt both when i = it (i.e.  when node i is the one which the
context vectors in Cit refer to) and to a lesser extent when i (cid:54)= it (i.e.  when node i is not the one
which the vectors in Cit refer to). This is because  as we said  the payoff at received for node it is
somehow informative also for all other nodes i (cid:54)= it. In other words  because we are assuming the
underlying parameter vectors ui are close to each other  we should let the corresponding prototype
vectors wi t undergo similar updates  so as to also keep the wi t close to each other over time.
With this in mind  we now describe GOB.Lin in more detail. It is convenient to introduce ﬁrst some
extra matrix notation. Let A = In + L  where L is the Laplacian matrix associated with G  and
In is the n × n identity matrix. Set A⊗ = A ⊗ Id  the Kronecker product2 of matrices A and Id.
Moreover  the “compound” descriptor for the pairing (i  x) is given by the long (and sparse) vector
φi(x) ∈ Rdn deﬁned as

φi(x)(cid:62) =(cid:0) 0  . . .   0
(cid:124) (cid:123)(cid:122) (cid:125)

(i−1)d times

(cid:124) (cid:123)(cid:122) (cid:125)

  x(cid:62)  0  . . .   0
(n−i)d times

(cid:1) .

1   u(cid:62)

2   . . .   u(cid:62)

U = (u(cid:62)
in the two expressions for CBt can be replaced by suitable upper bounds.

With the above notation handy  a compact description of GOB.Lin is presented in Figure 2  where
we deliberately tried to mimic the pseudocode of Figure 1. Notice that in Figure 2 we overloaded
the notation for the conﬁdence bound CBt  which is now deﬁned in terms of the Laplacian L of G.

In particular  (cid:107)u(cid:107) in Figure 1 is replaced in Figure 2 by (cid:107)(cid:101)U(cid:107)  where (cid:101)U = A1/2⊗ U and we deﬁne
n )(cid:62) ∈ Rdn. Clearly enough  the potentially unknown quantities (cid:107)u(cid:107) and (cid:107)(cid:101)U(cid:107)
We now explain how the modiﬁed long vectors(cid:101)φt k = A
−1/2⊗ φit(xt k) act in the update of matrix
whose i-th block Di is the d × d matrix Di = Id +(cid:80)
Mt and vector bt. First  observe that if A⊗ were the identity matrix then  according to how the
dn-long vector whose i-th d-dimensional block contains(cid:80)
long vectors φit(xt k) are deﬁned  Mt would be a block-diagonal matrix Mt = diag(D1  . . .   Dn) 
t . Similarly  bt would be the
t : kt=i atxt. This would be equivalent
to running n independent linear bandit algorithms (Figure 1)  one per node of G. Now  because
A⊗ is not the identity  but contains graph G represented through its Laplacian matrix  the selected
vector xt kt ∈ Cit for node it gets spread via A
from the it-th block over all other blocks 
thereby making the contextual information contained in xt kt available to update the internal status
2 The Kronecker product between two matrices M ∈ Rm×n and N ∈ Rq×r is the block matrix M ⊗ N of

t : kt=i xtx(cid:62)

dimension mq × nr whose block on row i and column j is the q × r matrix Mi jN.

−1/2⊗

4

(cid:115)

(cid:18)

rt ≤ 2

T

2σ2 ln

|MT|
δ

T(cid:88)

t=1

(cid:19)

+ 2L(u1  . . .   un)

(1 + B2) ln|MT|

of all other nodes. Yet  the only reward signal observed at time t is the one available at node it. A
theoretical analysis of GOB.Lin relying on the learning model of Section 3 is sketched in Section 4.1.
GOB.Lin’s running time is mainly affected by the inversion of the dn× dn matrix Mt  which can be
performed in time of order (dn)2 per round by using well-known formulas for incremental matrix
inversions. The same quadratic dependence holds for memory requirements. In our experiments  we
observed that projecting the contexts on the principal components improved performance. Hence 
the quadratic dependence on the context vector dimension d is not really hurting us in practice. On
the other hand  the quadratic dependence on the number of nodes n may be a signiﬁcant limitation
to GOB.Lin’s practical deployment. In the next section  we show that simple graph compression
schemes (like node clustering) can conveniently be applied to both reduce edge noise and bring the
algorithm to reasonable scaling behaviors.

4.1 Regret Analysis

We now provide a regret analysis for GOB.Lin that relies on the high probability analysis contained
in [1] (Theorem 2 therein). The analysis can be seen as a combination of the multitask kernel
contained in  e.g.  [8  17  12] and a version of the linear bandit algorithm described and analyzed
in [1].
Theorem 1. Let the GOB.Lin algorithm of Figure 2 be run on graph G = (V  E)  V = {1  . . .   n} 
hosting at each node i ∈ V vector ui ∈ Rd. Deﬁne

L(u1  . . .   un) =

(cid:107)ui(cid:107)2 +

(cid:107)ui − uj(cid:107)2 .

(cid:88)

i∈V

(cid:88)

(i j)∈E

Let also the sequence of context vectors xt k be such that (cid:107)xt k(cid:107) ≤ B  for all k = 1  . . .   ct  and
t = 1  . . .   T . Then the cumulative regret satisﬁes

matrix)  the bound in the above theorem has an extra term(cid:80)
log determinant ln|MT| on the resulting matrix MT   due to the construction of (cid:101)φt k via A

with probability at least 1 − δ.
Compared to running n independent bandit algorithms (which corresponds to A⊗ being the identity
(i j)∈E (cid:107)ui − uj(cid:107)2  which we assume
small according to our working assumption. However  the bound has also a signiﬁcantly smaller
. In
particular  when the graph is very dense  the log determinant in GOB.Lin is a factor n smaller than
the corresponding term for the n independent bandit case (see  e.g. [8]  Section 4.2 therein). To make
things clear  consider two extreme situations. When G has no edges then TR(MT ) = TR(I) + T =
nd+T   hence ln|MT| ≤ dn ln(1+T /(dn)). On the other hand  When G is the complete graph then
TR(MT ) = TR(I) + 2t/(n + 1) = nd + 2T /(n + 1)  hence ln|MT| ≤ dn ln(1 + 2T /(dn(n + 1))).
The exact behavior of ln|Mt| (one that would ensure a signiﬁcant advantage in practice) depends
on the actual interplay between the data and the graph  so that the above linear dependence on dn is
really a coarse upper bound.

−1/2⊗

5 Experiments

t kM−1
x(cid:62)

In this section  we present an empirical comparison of GOB.Lin (and its variants) to linear ban-
dit algorithms which do not exploit the relational information provided by the graph. We run
our experiments by approximating the CBt function in Figure 1 with the simpliﬁed expression
t−1xt k log(t + 1)  and the CBt function in Figure 2 with the corresponding expression
α

(cid:113)
in which xt k is replaced by (cid:101)φt k. In both cases  the factor α is used as tunable parameter. Our

preliminary experiments show that this approximation does not affect the predictive performances
of the algorithms  while it speeds up computation signiﬁcantly. We tested our algorithm and its
competitors on a synthetic dataset and two freely available real-world datasets extracted from the
social bookmarking web service Delicious and from the music streaming service Last.fm. These
datasets are structured as follows.

5

4Cliques. This is an artiﬁcial dataset whose graph contains four cliques of 25 nodes each to which
we added graph noise. This noise consists in picking a random pair of nodes and deleting or creating
an edge between them. More precisely  we created a n× n symmetric noise matrix of random num-
bers in [0  1]  and we selected a threshold value such that the expected number of matrix elements
above this value is exactly some chosen noise rate parameter. Then we set to 1 all the entries whose
content is above the threshold  and to zero the remaining ones. Finally  we XORed the noise matrix
with the graph adjacency matrix  thus obtaining a noisy version of the original graph.
Last.fm. This is a social network containing 1 892 nodes and 12 717 edges. There are 17 632 items
(artists)  described by 11 946 tags. The dataset contains information about the listened artists  and
we used this information in order to create the payoffs: if a user listened to an artist at least once the
payoff is 1  otherwise the payoff is 0.
Delicious. This is a network with 1 861 nodes and 7 668 edges. There are 69 226 items (URLs)
described by 53 388 tags. The payoffs were created using the information about the bookmarked
URLs for each user: the payoff is 1 if the user bookmarked the URL  otherwise the payoff is 0.
Last.fm and Delicious were created by the Information Retrieval group at Universidad Autonoma
de Madrid for the HetRec 2011 Workshop [6] with the goal of investigating the usage of heteroge-
neous information in recommendation systems.3 These two networks are structurally different: on
Delicious  payoffs depend on users more strongly than on Last.fm. In other words  there are more
popular artists  whom everybody listens to  than popular websites  which everybody bookmarks —
see Figure 3. This makes a huge difference in practice  and the choice of these two datasets allow
us to make a more realistic comparison of recommendation techniques. Since we did not remove
any items from these datasets (neither the most frequent nor the least frequent)  these differences do
inﬂuence the behavior of all algorithms —see below.
Some statistics about Last.fm and Delicious are reported in Table 1. In Figure 3 we plotted the
distribution of the number of preferences per item in order to make clear and visible the differences
explained in the previous paragraphs.4

NODES
EDGES
AVG. DEGREE
ITEMS
NONZERO PAYOFFS
TAGS

LAST.FM DELICIOUS
1867
7668
8.21
69226
104799
53388

1892
12717
13.443
17632
92834
11946

Table 1: Main statistics for Last.fm and Delicious. ITEMS
counts the overall number of items  across all users  from
which Ct is selected. NONZERO PAYOFFS is the number
of pairs (user  item) for which we have a nonzero payoff.
TAGS is the number of distinct tags that were used to de-
scribe the items.

Figure 3: Plot of the number of prefer-
ences per item (users who bookmarked
the URL or listened to an artist). Both
axes have logarithmic scale.

We preprocessed datasets by breaking down the tags into smaller tags made up of single words. In
fact  many users tend to create tags like “webdesign tutorial css”. This tag has been splitted into
three smaller tags corresponding to the three words therein. More generally  we splitted all com-
pound tags containing underscores  hyphens and apexes. This makes sense because users create tags
independently  and we may have both “rock and roll” and “rock n roll”. Because of this splitting
operation  the number of unique tags decreased from 11 946 to 6 036 on Last.fm and from 53 388
to 9 949 on Delicious. On Delicious  we also removed all tags occurring less than ten times.5 The

3 Datasets and their full descriptions are available at www.grouplens.org/node/462.
4 In the context of recommender systems  these two datasets may be seen as representatives of two “markets”
whose products have signiﬁcantly different market shares (the well-known dichotomy of hit vs. niche products).
Niche product markets give rise to power laws in user preference statistics (as in the blue plot of Figure 3).

5 We did not repeat the same operation on Last.fm because this dataset was already extremely sparse.

6

 1 10 100 1000 1 10 100 1000 10000 100000NUM PREFERENCESITEM IDPreferences per itemDELICIOUSLASTFMTable 2: Normalized cumulated reward for different levels of graph noise (expected fraction of
perturbed edges) and payoff noise (largest absolute value of noise term ) on the 4Cliques dataset.
Graph noise increases from top to bottom  payoff noise increases from left to right. GOB.Lin is
clearly more robust to payoff noise than its competitors. On the other hand  GOB.Lin is sensitive to
high levels of graph noise. In the last row  graph noise is 41.7%  i.e.  the number of perturbed edges
is 500 out of 1200 edges of the original graph.

algorithms we tested do not use any prior information about which user provided a speciﬁc tag. We
used all tags associated with a single item to create a TF-IDF context vector that uniquely represents
that item  independent of which user the item is proposed to. In both datasets  we only retained
the ﬁrst 25 principal components of context vectors  so that xt k ∈ R25 for all t and k. We gen-
erated random context sets Cit of size 25 for Last.fm and Delicious  and of size 10 for 4Cliques.
In practical scenarios  these numbers would be varying over time  but we kept them ﬁxed so as
to simplify the experimental setting. In 4Cliques we assigned the same unit norm random vector
ui to every node in the same clique i of the original graph (before adding graph noise). Payoffs
were then generated according to the following stochastic model: ai(x) = u(cid:62)
i x +   where  (the
payoff noise) is uniformly distributed in a bounded interval centered around zero. For Delicious
and Last.fm  we created a set of context vectors for every round t as follows: we ﬁrst picked it
uniformly at random in {1  . . .   n}. Then  we generated context vectors xt 1  . . .   xt 25 in Cit by
picking 24 vectors at random from the dataset and one among those vectors with nonzero payoff
for user it. This is necessary in order to avoid a meaningless comparison: with high probability  a
purely random selection would result in payoffs equal to zero for all the context vectors in Cit. In
our experimental comparison  we tested GOB.Lin and its variants against two baselines: a baseline
LinUCB-IND that runs an independent instance of the algorithm in Figure 1 at each node (this is
equivalent to running GOB.Lin in Figure 2 with A⊗ = Idn) and a baseline LinUCB-SIN  which
runs a single instance of the algorithm in Figure 1 shared by all the nodes. LinUCB-IND turns to be

7

 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=0% payoff-noise=0GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=0% payoff-noise=0.25GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=0% payoff-noise=0.5GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=8.3% payoff-noise=0GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=8.3% payoff-noise=0.25GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=8.3% payoff-noise=0.5GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=20.8% payoff-noise=0GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=20.8% payoff-noise=0.25GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=20.8% payoff-noise=0.5GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=41.7% payoff-noise=0GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=41.7% payoff-noise=0.25GOB.LinLinUCB-INDLinUCB-SIN 0 500 1000 1500 2000 2500 3000 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIME4Cliques graph-noise=41.7% payoff-noise=0.5GOB.LinLinUCB-INDLinUCB-SINFigure 4: Cumulative reward for all the bandit algorithms introduced in this section.

a reasonable comparator when  as in the Delicious dataset  there are many moderately popular items.
On the other hand  LinUCB-SIN is a competitive baseline when  as in the Last.fm dataset  there are
few very popular items. The two scalable variants of GOB.Lin which we empirically analyzed are
based on node clustering 6 and are deﬁned as follows.
GOB.Lin.MACRO: GOB.Lin is run on a weighted graph whose nodes are the clusters of the origi-
nal graph. The edges are weighted by the number of inter-cluster edges in the original graph. When
all nodes are clustered together  then GOB.Lin.MACRO recovers the baseline LinUCB-SIN as a
special case. In order to strike a good trade-off between the speed of the algorithms and the loss of
information resulting from clustering  we tested three different cluster sizes: 50  100  and 200. Our
plots refer to the best performing choice.
GOB.Lin.BLOCK: GOB.Lin is run on a disconnected graph whose connected components are the
clusters. This makes A⊗ and Mt (Figure 2) block-diagonal matrices. When each node is clustered
individually  then GOB.Lin.BLOCK recovers the baseline LinUCB-IND as a special case. Similar
to GOB.Lin.MACRO  in order to trade-off running time and cluster sizes  we tested three different
cluster sizes (5  10  and 20)  and report only on the best performing choice.
As the running time of GOB.Lin scales quadratically with the number of nodes  the computational
savings provided by the clustering are also quadratic. Moreover  as we will see in the experiments 
the clustering acts as a regularizer  limiting the inﬂuence of noise. In all cases  the parameter α in

Figures 1 and 2 was selected based on the scale of instance vectors ¯xt and (cid:101)φt kt  respectively  and
rithm  as compared (“normalized”) to that of the random predictor  that is(cid:80)
tuned across appropriate ranges. Table 2 and Figure 4 show the cumulative reward for each algo-
t(at − ¯at)  where at is
the payoff obtained by the algorithm and ¯at is the payoff obtained by the random predictor  i.e.  the
average payoff over the context vectors available at time t. Table 2 (synthetic datasets) shows that
GOB.Lin and LinUCB-SIN are more robust to payoff noise than LinUCB-IND. Clearly  LinUCB-
SIN is also unaffected by graph noise  but it never outperforms GOB.Lin. When the payoff noise is
low and the graph noise grows GOB.Lin’s performance tends to degrade. Figure 4 reports the results
on the two real-world datasets. Notice that GOB.Lin and its variants always outperform the baselines
(not relying on graphical information) on both datasets. As expected  GOB.Lin.MACRO works best
on Last.fm  where many users gave positive payoffs to the same few items. Hence  macro nodes
apparently help GOB.Lin.MACRO to perform better than its corresponding baseline LinUCB-SIN.
In fact  GOB.Lin.MACRO also outperforms GOB.Lin  thus showing the regularization effect of us-
ing macro nodes. On Delicious  where we have many moderately popular items  GOB.Lin.BLOCK
tends to perform best  GOB.Lin being the runner-up. As expected  LinUCB-IND works better than
LinUCB-SIN  since the former is clearly more prone to personalize item recommendation than the
latter. Future work will consider experiments against different methods for sharing contextual and
feedback information in a set of users  such as the feature hashing technique of [22].
Acknowledgments
NCB and GZ gratefully acknowledge partial support by MIUR (project ARS TechnoMedia  PRIN
2010-2011  contract no. 2010N5K7EB-003). We thank the Laboratory for Web Algorithmics at
Dept. of Computer Science of University of Milan.

6 We used the freely available Graclus (see e.g. [11]) graph clustering tool with normalized cut  zero local

search steps  and no spectral clustering options.

8

 0 250 500 750 1000 1250 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIMELast.fmLinUCB-SINLinUCB-INDGOB.LinGOB.Lin.MACROGOB.Lin.BLOCK 0 25 50 75 100 125 150 0 2000 4000 6000 8000 10000CUMULATIVE REWARDTIMEDeliciousLinUCB-SINLinUCB-INDGOB.LinGOB.Lin.MACROGOB.Lin.BLOCKReferences
[1] Y. Abbasi-Yadkori  D. P´al  and C. Szepesv´ari. Improved algorithms for linear stochastic bandits. Advances

in Neural Information Processing Systems  2011.

[2] K. Amin  M. Kearns  and U. Syed. Graphical models for bandit problems. Proceedings of the Twenty-

Seventh Conference Uncertainty in Artiﬁcial Intelligence  2011.

[3] D. Asanov. Algorithms and methods in recommender systems. Berlin Institute of Technology  Berlin 

Germany  2011.

[4] P. Auer. Using conﬁdence bounds for exploration-exploitation trade-offs. Journal of Machine Learning

Research  3:397–422  2002.

[5] T. Bogers. Movie recommendation using random walks over the contextual graph. In CARS’10: Pro-

ceedings of the 2nd Workshop on Context-Aware Recommender Systems  2010.

[6] I. Cantador  P. Brusilovsky  and T. Kuﬂik. 2nd Workshop on Information Heterogeneity and Fusion in
Recommender Systems (HetRec 2011). In Proceedings of the 5th ACM Conference on Recommender
Systems  RecSys 2011. ACM  2011.

[7] S. Caron  B. Kveton  M. Lelarge  and S. Bhagat. Leveraging side observations in stochastic bandits. In

Proceedings of the 28th Conference on Uncertainty in Artiﬁcial Intelligence  pages 142–151  2012.

[8] G. Cavallanti  N. Cesa-Bianchi  and C. Gentile. Linear algorithms for online multitask classiﬁcation.

Journal of Machine Learning Research  11:2597–2630  2010.

[9] W. Chu  L. Li  L. Reyzin  and R. E. Schapire. Contextual bandits with linear payoff functions. In Pro-

ceedings of the International Conference on Artiﬁcial Intelligence and Statistics  pages 208–214  2011.

[10] K. Crammer and C. Gentile. Multiclass classiﬁcation with bandit feedback using adaptive regularization.

Machine Learning  90(3):347–383  2013.

[11] I. S. Dhillon  Y. Guan  and B. Kulis. Weighted graph cuts without eigenvectors a multilevel approach.

Pattern Analysis and Machine Intelligence  IEEE Transactions on  29(11):1944–1957  2007.

[12] T. Evgeniou and M. Pontil. Regularized multi–task learning. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and data mining  KDD ’04  pages 109–117  New York 
NY  USA  2004. ACM.

[13] I. Guy  N. Zwerdling  D. Carmel  I. Ronen  E. Uziel  S. Yogev  and S. Ofek-Koifman. Personalized
recommendation of social software items based on social relations. In Proceedings of the Third ACM
Conference on Recommender Sarxiv ystems  pages 53–60. ACM  2009.

[14] S. Kar  H. V. Poor  and S. Cui. Bandit problems in networks: Asymptotically efﬁcient distributed allo-
cation rules. In Decision and Control and European Control Conference (CDC-ECC)  2011 50th IEEE
Conference on  pages 1771–1778. IEEE  2011.

[15] L. Li  W. Chu  J. Langford  and R. E. Schapire. A contextual-bandit approach to personalized news
article recommendation. In Proceedings of the 19th International Conference on World Wide Web  pages
661–670. ACM  2010.

[16] S. Mannor and O. Shamir. From bandits to experts: On the value of side-observations. In Advances in

Neural Information Processing Systems  pages 684–692  2011.

[17] C. A. Micchelli and M. Pontil. Kernels for multi–task learning.

Processing Systems  pages 921–928  2004.

In Advances in Neural Information

[18] A. Said  E. W. De Luca  and S. Albayrak. How social relationships affect user similarities. In Proceedings

of the 2010 Workshop on Social Recommender Systems  pages 1–4  2010.

[19] A. Slivkins. Contextual bandits with similarity information. Journal of Machine Learning Research –

Proceedings Track  19:679–702  2011.

[20] B. Swapna  A. Eryilmaz  and N. B. Shroff. Multi-armed bandits in the presence of side observations in

social networks. In Proceedings of 52nd IEEE Conference on Decision and Control (CDC)  2013.

[21] B. Sz¨or´enyi  R. Busa-Fekete  I. Hegedus  R. Orm´andi  M. Jelasity  and B. K´egl. Gossip-based distributed
stochastic bandit algorithms. Proceedings of the 30th International Conference on Machine Learning 
2013.

[22] K. Weinberger  A. Dasgupta  J. Langford  A. Smola  and J. Attenberg. Feature hashing for large scale
In Proceedings of the 26th International Conference on Machine Learning  pages

multitask learning.
1113–1120. Omnipress  2009.

9

,Nicolò Cesa-Bianchi
Claudio Gentile
Giovanni Zappella