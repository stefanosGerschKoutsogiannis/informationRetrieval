2018,Uniform Convergence of Gradients for Non-Convex Learning and Optimization,We investigate 1) the rate at which refined properties of the empirical risk---in particular  gradients---converge to their population counterparts in standard non-convex learning tasks  and 2) the consequences of this convergence for optimization. Our analysis follows the tradition of norm-based capacity control. We propose vector-valued Rademacher complexities as a simple  composable  and user-friendly tool to derive dimension-free uniform convergence bounds for gradients in non-convex learning problems. As an application of our techniques  we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression  showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity  even when dimension is high or possibly infinite and multiple passes over the dataset are allowed.

Moving to non-smooth models we show----in contrast to the smooth case---that even for a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side  it is still possible to obtain dimension-independent rates under a new type of distributional assumption.,Uniform Convergence of Gradients for
Non-Convex Learning and Optimization

Dylan J. Foster
Cornell University

djfoster@cornell.edu

Ayush Sekhari
Cornell University

sekhari@cs.cornell.edu

Karthik Sridharan
Cornell University

sridharan@cs.cornell.edu

Abstract

We investigate 1) the rate at which reﬁned properties of the empirical risk—in
particular  gradients—converge to their population counterparts in standard non-
convex learning tasks  and 2) the consequences of this convergence for optimization.
Our analysis follows the tradition of norm-based capacity control. We propose
vector-valued Rademacher complexities as a simple  composable  and user-friendly
tool to derive dimension-free uniform convergence bounds for gradients in non-
convex learning problems. As an application of our techniques  we give a new
analysis of batch gradient descent methods for non-convex generalized linear
models and non-convex robust regression  showing how to use any algorithm that
ﬁnds approximate stationary points to obtain optimal sample complexity  even
when dimension is high or possibly inﬁnite and multiple passes over the dataset
are allowed.
Moving to non-smooth models we show—-in contrast to the smooth case—that
even for a single ReLU it is not possible to obtain dimension-independent conver-
gence rates for gradients in the worst case. On the positive side  it is still possible to
obtain dimension-independent rates under a new type of distributional assumption.

1

Introduction

The last decade has seen a string of empirical successes for gradient-based algorithms solving large
scale non-convex machine learning problems [24  16]. Inspired by these successes  the theory
community has begun to make progress on understanding when gradient-based methods succeed
for non-convex learning in certain settings of interest [17]. The goal of the present work is to
introduce learning-theoretic tools to—in a general sense—improve understanding of when and why
gradient-based methods succeed for non-convex learning problems.
In a standard formulation of the non-convex statistical learning problem  we aim to solve

minimize LD(w)∶= E(x y)∼D `(w ; x  y) 

where w∈W⊆ Rd is a parameter vector D is an unknown probability distribution over the instance
spaceX×Y  and the loss ` is a potentially non-convex function of w. The learner cannot observeD
directly and instead must ﬁnd a model̂w∈W that minimizes LD given only access to i.i.d. samples
(x1  y1)  . . .  (xn  yn)∼D. Their performance is quantiﬁed by the excess risk LD(̂w)− L￿  where
L￿= inf w∈W LD(w).
to minimize the empirical risk̂Ln(w)∶= 1
t=1 `(w ; xt  yt). If one succeeds at minimizinĝLn 
n∑n

classical statistical learning theory provides a comprehensive set of tools to bounds the excess risk
of the procedure. The caveat is that when ` is non-convex  global optimization of the empirical risk
may be far from easy. It is not typically viable to even verify whether one is at a global minimizer

Given only access to samples  a standard (“sample average approximation”) approach is to attempt

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

of successes showing that for non-convex problems arising in machine learning  iterative optimizers

might be challenging  there is an abundance of gradient methods that provably ﬁnd approximate

gradient-based optimization  the empirical risk may not inherit these properties due to stochasticity. In

of̂Ln. Moreover  even if the population risk LD has favorable properties that make it amenable to
the worst case  minimizing LD or̂Ln is simply intractable. However  recent years have seen a number
can succeed both in theory and in practice (see [17] for a survey). Notably  while minimizinĝLn
stationary points of the empirical risk  i.e.￿∇̂Ln(w)￿≤ " [33  10  38  3  26]. In view of this  the
high probability over samples  simultaneously over all w∈W ￿∇LD(w)￿≤￿∇̂Ln(w)￿+ "; Second 
to explore concrete non-convex problems where one can establish that the excess risk LD(̂w)− L￿ is

present work has two aims: First  to provide a general set of tools to prove uniform convergence
results for gradients  with the goal of bounding how many samples are required to ensure that with

small a consequence of this gradient uniform convergence. Together  these two directions yield direct
bounds on the convergence of non-convex gradient-based learning algorithms to low excess risk.
Our precise technical contributions are as follows:

• We bring vector-valued Rademacher complexities [30] and associated vector-valued contrac-
tion principles to bear on the analysis of uniform convergence for gradients. This approach
enables norm-based capacity control  meaning that the bounds are independent of dimension
whenever the predictor norm and data norm are appropriately controlled. We introduce a
“chain rule” for Rademacher complexity  which enables one to decompose the complexity
of gradients of compositions into complexities of their components  and makes deriving
dimension-independent complexity bounds for common non-convex classes quite simple.

• We establish variants of the Gradient Domination condition for the population risk in certain
non-convex learning settings. The condition bounds excess risk in terms of the magnitude
of gradients  and is satisﬁed in non-convex learning problems including generalized linear
models and robust regression. As a consequence of the gradient uniform convergence
bounds  we show how to use any algorithm that ﬁnds approximate stationary points for
smooth functions in a black-box fashion to obtain optimal sample complexity for these
models—both in high- and low-dimensional regimes. In particular  standard algorithms
including gradient descent [33]  SGD [10]  Non-convex SVRG [38  3]  and SCSG [26] enjoy
optimal sample complexity  even when allowed to take multiple passes over the dataset.

• We show that for non-smooth losses dimension-independent uniform convergence is not
possible in the worst case  but that this can be circumvented using a new type of margin
assumption.

Related Work This work is inspired by [31]  who gave dimension-dependent gradient and Hessian
convergence rates and optimization guarantees for the generalized linear model and robust regression
setups we study. We move beyond the dimension-dependent setting by providing norm-based capacity
control. Our bounds are independent of dimension whenever the predictor norm and data norm are
sufﬁciently controlled (they work in inﬁnite dimension in the `2 case)  but even when the norms are
large we recover the optimal dimension-dependent rates.
Optimizing the empirical risk under assumptions on the population risk has begun to attract signiﬁcant
attention (e.g. [12  18]). Without attempting a complete survey  we remark that these results typically

effect. We view these works as complementary to our norm-based analysis.

depend on dimension  e.g. [18] require poly(d) samples before their optimization guarantees take
Notation For a given norm￿⋅￿  the dual norm is denoted￿⋅￿￿.￿⋅￿p represents the standard `p norm
on Rd and￿⋅￿ denotes the spectral norm. 1 denotes the all-ones vector  with dimension made clear
from context. For a function f∶ Rd→ R ∇f(x)∈ Rd and∇2f(x)∈ Rd×d will denote the gradient
and the Hessian of f at x respectively. f is said to be L-Lipschitz with respect to a norm￿⋅￿ if
￿f(x)− f(y)￿≤ L￿x− y￿∀x  y. Similarly  f is said to be H-smooth w.r.t norm￿⋅￿ if its gradients are
H-Lipschitz with respect to￿⋅￿  i.e.￿∇f(x)−∇f(y)￿￿≤ H￿x− y￿ for some H.

2

2 Gradient Uniform Convergence: Why and How

2.1 Utility of Gradient Convergence: The Why
Before introducing our tools for establishing gradient uniform convergence  let us introduce a family
of losses for which this convergence has immediate consequences for the design of non-convex
statistical learning algorithms.

LD(w)− LD(w￿)≤ µ￿∇LD(w)￿↵ ∀w∈W 

Deﬁnition 1. The population risk LD satisﬁes the(↵  µ)-Gradient Domination condition with respect
to a norm￿⋅￿ if there are constants µ> 0  ↵∈[1  2] such that
where w￿∈ arg min w∈W LD(w) is any population risk minimizer.
The case ↵= 2 is often referred to as the Polyak-Łojasiewicz inequality [36  23]. The general GD

condition implies that all critical points are global  and is itself implied (under technical restrictions)
by many other well-known conditions including one-point convexity [28]  star convexity and ⌧-star
convexity [14]  and so-called “regularity conditions” [44]; for more see [23]. The GD condition is
satsiﬁed—sometimes locally rather than globally  and usually under distributional assumptions—
by the population risk in settings including neural networks with one hidden layer [28]  ResNets
with linear activations [13]  phase retrieval [44]  matrix factorization [29]  blind deconvolution [27] 
and—as we show here—generalized linear models and robust regression.
The GD condition states that to optimize the population risk it sufﬁces to ﬁnd a (population)
stationary point. What are the consequences of the statement for the learning problem  given that

(GD)

w∈W￿∇̂Ln(w)−∇LD(w)￿↵+ c￿￿

proposition shows  via gradient uniform convergence  that GD is immediately useful for non-convex
learning even when it is only satisﬁed at the population level.

the learner only has access to the empirical risk̂Ln which itself may not satisfy GD? The next
Proposition 1. Suppose that LD satisﬁes the(↵  µ)-GD condition. Then  for any  > 0  with
t=1  every algorithm̂walg satisﬁes
probability at least 1−  over the draw of the data{(xt  yt)}n
LD(̂walg)− L￿≤ 2 µ￿￿￿￿∇̂Ln(̂walg)￿↵+ E sup
2￿￿￿  
￿￿
where the constant c depends only on the range of￿∇`￿.
Note that ifW is a ﬁnite set  then standard concentration arguments for norms along with the union
bound imply that E supw∈W￿∇̂Ln(w)−∇LD(w)￿≤ O￿￿ log￿W￿n ￿. For smooth losses  ifW⊂ Rd
is contained in a bounded ball  then by simply discretizing the setW up to precision " (with O("−d)
elements)  one can easily obtain a bound of E supw∈W￿∇̂Ln(w)−∇LD(w)￿≤ O￿￿ d
n￿. This
following “norm-based capacity control” form: E supw∈W￿∇̂Ln(w)−∇LD(w)￿≤ O￿￿C(W)n ￿
whereC(W) is a norm-dependent  but dimension-independent measure of the size ofW. Given such
a bound  any algorithm that guarantees￿∇̂Ln(̂walg)￿≤ O￿ 1√n￿ for a(↵  µ)-GD loss will obtain
an overall excess risk bound of order O￿ µ
n↵￿2￿. For(1  µ1)-GD this translates to an overall O￿ µ1√n￿
n￿ rate. The ﬁrst rate becomes favorable when µ1≤√n⋅ µ2
rate  whereas(2  µ2)-GD implies a O￿ µ2
related only to the radius of the setW  while µ2 depends inversely on the smallest eigenvalue of the

approach recovers the dimension-dependent gradient convergence rates obtained in [31].
Our goal is to go beyond this type of analysis and provide dimension-free rates that apply even
when the dimension is larger than the number of examples  or possibly inﬁnite. Our bounds take the

population covariance and so is well-behaved only for low-dimensional problems unless one makes
strong distributional assumptions.
An important feature of our analysis is that we need to establish the GD condition only for the
population risk; for the examples we consider this is easy as long as we assume the model is well-
speciﬁed. Once this is done  our convergence results hold for any algorithm that works on the dataset

which typically happens for very high dimensional problems. For the examples we study  µ1 is

↵

log￿ 1
￿n

(1)

3

t=1 and ﬁnds an approximate ﬁrst-order stationary point with￿∇̂Ln(̂walg)￿≤ ". First-order

algorithms that ﬁnd approximate stationary points assuming only smoothness of the loss have enjoyed
a surge of recent interest [10  38  3  1]  so this is an appealing proposition.

{(xt  yt)}n

2.2 Vector Rademacher Complexities: The How
The starting point for our uniform convergence bounds for gradients is to apply the standard tool of
symmetrization—a vector-valued version  to be precise. To this end let us introduce a normed variant
of Rademacher complexity.

Deﬁnition 2 (Normed Rademacher Complexity). Given a vector valued class of functionF that
maps the spaceZ to a vector space equipped with norm￿⋅￿  we deﬁne the normed Rademacher
complexity forF on instances z1∶n via

(2)

R￿⋅￿(F ; z1∶n)∶= E✏ sup
f∈F￿ n￿t=1

✏tf(zt)￿.

(3)

(4)

1
2

E✏ sup

With this deﬁnition we are ready to provide a straightforward generalization of the standard real-
valued symmetrization lemma.

Rademacher complexity that allows to easily control gradients of composition of functions.

k=1￿∇Ft k(w)￿2≤ LF . Then 

✏t∇(Gt(Ft(w)))￿≤ LF E✏ sup
w∈W

E sup

w∈W￿∇̂Ln(w)−∇LD(w)￿≤ 4

n

n￿t=1￿✏t ∇Gt(Ft(w))￿+ LG E✏ sup

R￿⋅￿(∇`○W ; x1∶n  y1∶n)+ c￿￿

Rademacher random variables with ✏t denoting the tth column
The concrete learning settings we study—generalized linear models and robust regression—all
involve composing non-convex losses and non-linearities or transfer functions with a linear predictor.

Proposition 2. For any > 0  with probability at least 1−  over the data{(xt  yt)}n
t=1 
log￿ 1
￿n
￿￿  
where the constant c depends only on the range of￿∇`￿.
To bound the complexity of the gradient class∇`○W  we introduce a chain rule for the normed
Theorem 1 (Chain Rule for Rademacher Complexity). Let sequences of functions Gt∶ RK → R
and Ft∶ Rd→ RK be given. Suppose there are constants LG and LF such that for all 1≤ t≤ n 
￿∇Gt￿2≤ LG and￿∑K
w∈W￿ n￿t=1∇Ft(w)✏t￿ 
w∈W￿ n￿t=1
where∇Ft denotes the Jacobian of Ft  which lives in Rd×K  and ✏ ∈ {±1}K×n is a matrix of
That is  `(w; xt  yt) can be written as `(w; xt  yt)= Gt(Ft(w)) where Gt(a) is some L-Lipschitz
function that possibly depends on xt and yt and Ft(w)=￿w  xt￿. In this case  the chain rule for
derivatives gives us that∇`(w; xt  yt)= G′t(Ft(w))⋅∇Ft(w)= G′t(￿w  xt￿))xt. Using the chain
rule (with K= 1)  we conclude that
✏tG′t(￿w  xt￿))￿+ L⋅ E✏￿ n￿t=1
✏txt￿.
t=1 ✏txt￿. The ﬁrst
class of linear predictors and controlling the vector-valued random average E✏￿∑n
t=1 ✏txt￿= O(√n);
spaces  and more generally Banach spaces of Rademacher type 2  one has E✏￿∑n
Deﬁnition 3. For a function classG⊆￿Z→ RK￿  the vector-valued Rademacher complexity is

see Appendix A for details.
The key tool used to prove Theorem 1  which appears throughout the technical portions of this paper 
is the vector-valued Rademacher complexity due to [30].

term is handled using classical Rademacher complexity tools. As for the second term  it is a standard
result ([35]; see [19] for discussion in the context of learning theory) that for all smooth Banach

Thus  we have reduced the problem to controlling the Rademacher average for a real valued function

E✏ sup

w∈W￿ n￿t=1

✏t∇`(w ; xt  yt)￿≤ E✏￿ sup
w∈W

n￿t=1

(5)

→R(g ; z1∶n)∶= E✏ sup
g∈G

n￿t=1￿✏t  g(zt)￿.

4

The vector-valued Rademacher complexity arises through an elegant contraction trick due to Maurer.

Theorem 2 (Vector-valued contraction [30]). LetG ⊆ (Z → RK)  and let ht ∶ RK → R be a
sequence of functions for t∈[n]  each of which is L-Lipschitz with respect to `2. Then

(6)

E✏ sup

g∈G

n￿t=1

✏tht(g(zt))≤√2L⋅→R(G ; z1∶n).

We remark that while our applications require only gradient uniform convergence  we anticipate
that the tools of this section will ﬁnd use in settings where convergence of higher-order derivatives
is needed to ensure success of optimization routines. To this end  we have extended the chain rule
(Theorem 1) to handle Hessian convergence; see Appendix E.

3 Application: Smooth Models

In this section we instantiate the general gradient uniform convergence tools and the GD condition to
derive optimization consequences for two standard settings previously studied by [31]: generalized
linear models and robust regression.

Generalized Linear Model We ﬁrst consider the problem of learning a generalized linear model

with the square loss.1 Fix a norm￿⋅￿  takeX =￿x∈ Rd￿￿x￿≤ R￿ W =￿w∈ Rd￿￿w￿￿≤ B￿ 
andY = {0  1}. Choose a link function  ∶ R → [0  1] and deﬁne the loss to be `(w ; x  y) =
((￿w  x￿)− y)2. Standard choices for  include the logistic link function (s)=(1+ e−s)−1 and
the probit link function (s)= (s)  where  is the gaussian cumulative distribution function.
Assumption 1 (Generalized Linear Model Regularity). LetS=[−BR  BR].

To establish the GD property and provide uniform convergence bounds  we make the following
regularity assumptions on the loss.

(a)∃C≥ 1 s.t. max{′(s) ′′(s)}≤ C for all s∈S.
(b)∃c> 0 s.t. ′(s)≥ c for all s∈S.
(c) E[y￿ x]= (￿w￿  x￿) for some w￿∈W.

leads to three ﬁnal rates: a dimension-independent “slow rate” that holds for any smooth norm  a
dimension-dependent fast rate for the `2 norm  and a sparsity-dependent fast rate that holds under an
additional restricted eigenvalue assumption. This gives rise to a family of generic excess risk bounds.

Assumption (a) sufﬁces to bound the normed Rademacher complexity R￿⋅￿(∇`○W)  and combined
with (b) and (c) the assumption implies that LD satisﬁes three variants of GD condition  and this
To be precise  let us introduce some additional notation: ⌃= Ex[xx￿] is the data covariance matrix
and min(⌃) denotes the minimum non-zero eigenvalue. For sparsity dependent fast rates  deﬁne
C(S  ↵)∶=￿⌫∈ Rd￿￿⌫SC￿1≤ ↵￿⌫S￿1￿ and let min(⌃)= inf ⌫∈C(S(w￿) 1) ￿⌫ ⌃⌫￿
￿⌫ ⌫￿ be the restricted
eigenvalue.2 Lastly  recall that a norm￿⋅￿ is said to be -smooth if the function (x)= 1
2￿x￿2 has
-Lipschitz gradients with respect to￿⋅￿.
t=1 for any algorithm̂walg:
hold with probability at least 1−  over the draw of the data{(xt  yt)}n
● Norm-Based/High-Dimensional Setup. WhenX is the ball for -smooth norm￿⋅￿ andW is the
LD(̂walg)− L￿≤ µh⋅￿∇̂Ln(̂walg)￿+ Ch√n
● Low-Dimensional `2￿`2 Setup. WhenX andW are both `2 balls:
min(⌃)￿µl⋅￿∇̂Ln(̂walg)￿2+ Cl
n￿.

Theorem 3. For the generalized linear model setting  the following excess risk inequalities each

1[31] refer to the model as “binary classiﬁcation”  since  can model conditional probabilities of two classes.

LD(̂walg)− L￿≤

2Recall that S(w￿)⊆[d] is the set of non-zero entries of w￿  and for any vector w  wS∈ Rd refers to the

vector w with all entries in SC set to zero (as in [37]).

dual ball 

1

.

5

Model

Algorithm

Sample Complexity

Norm-based/Inﬁnite dim. Low-dim.

n/a

[31] Theorem 6 n/a

Generalized Linear Proposition 3

O("−2)
O("−2)
O("−2)

Table 1: Sample complexity comparison. Highlighted cells indicate optimal sample complexity.

[31] Theorem 4 n/a
GLMtron [21]
Robust Regression Proposition 3

O(d"−1)
O(d"−1)
O(d"−1)
O(d"−1)
● Sparse `∞￿`1 Setup. WhenX is the `∞ ball W is the `1 ball  and￿w￿￿1= B:3
The quantities Ch￿Cl￿Cs and µh￿µl￿µs are constants depending on(B  R  C  c   log(−1)) but
not explicitly on the dimension d (beyond logarithmic factors) or complexity of the classW (beyond

 min(⌃)￿µs⋅￿∇̂Ln(̂walg)￿2+ Cs
n￿.

LD(̂walg)− L￿≤ ￿w￿￿0

Precise statements for the problem dependent constants in Theorem 3 including dependence on the
norms R and B can be found in Appendix C.
We now formally introduce the robust regression setting and provide a similar guarantee.

B and R).

Similar to the generalized linear model setup  the robust regression setup satisﬁes three variants of

Theorem 4. For the robust regression setting  the following excess risk inequalities each hold with

with a canonical example being Tukey’s biweight loss.4 While optimization is clearly not possible
for arbitrary choices of ⇢  the following assumption is sufﬁcient to guarantee that the population risk

Robust Regression Fix a norm￿⋅￿ and takeX =￿x∈ Rd￿￿x￿≤ R￿ W=￿w∈ Rd￿￿w￿￿≤ B￿ 
andY=[−Y  Y] for some constant Y . We pick a potentially non-convex function ⇢∶ R→ R+ and
deﬁne the loss via `(w ; x  y)= ⇢(￿w  x￿− y). Non-convex choices for ⇢ arise in robust statistics 
LD satisﬁes the GD property.
Assumption 2 (Robust Regression Regularity). LetS=[−(BR+ Y) (BR+ Y)].

(a)∃C⇢≥ 1 s.t. max{⇢′(s) ⇢′′(s)}≤ C⇢ for all s∈S.
(b) ⇢′ is odd with ⇢′(s)> 0 for all s> 0 and h(s)∶= E⇣[⇢′(s+ ⇣)] has h′(0)> c⇢.
(c) There is w￿∈W such that y=￿w￿  x￿+ ⇣  and ⇣ is symmetric zero-mean given x.
t=1 for any algorithm̂walg:

the GD depending on assumptions on the norm￿⋅￿ and the data distribution.
probability at least 1−  over the draw of the data{(xt  yt)}n
● Norm-Based/High-Dimensional Setup. WhenX is the ball for -smooth norm￿⋅￿ andW is the
LD(̂walg)− L￿≤ µh⋅￿∇̂Ln(̂walg)￿+ Ch√n
● Low-Dimensional `2￿`2 Setup. WhenX andW are both `2 balls:
min(⌃)￿µl⋅￿∇̂Ln(̂walg)￿2+ Cl
n￿.
● Sparse `∞￿`1 Setup. WhenX is the `∞ ball W is the `1 ball  and￿w￿￿1= B:
 min(⌃)￿µs⋅￿∇̂Ln(̂walg)￿2+ Cs
n￿.
3The constraint￿w￿￿1= B simpliﬁes analysis of generic algorithms in the vein of constrained LASSO [41].
￿t￿≤ c.
4For a ﬁxed parameter c> 0 the biweight loss is deﬁned via ⇢(t)= c2
￿t￿≥ c.

LD(̂walg)− L￿≤
LD(̂walg)− L￿≤ ￿w￿￿0

6 ⋅￿ 1−(1−(t￿c)2)3 

dual ball 

1 

1

.

6

The constants Ch￿Cl￿Cs and µh￿µl￿µs depend on(B  R  C⇢  c⇢   log(−1))  but not explicitly on
the dimension d (beyond log factors) or complexity of the classW (beyond the range parameters B

and R).

Theorem 3 and Theorem 4 immediately imply that standard non-convex optimization algorithms for
ﬁnding stationary points can be converted to non-convex learning algorithms with optimal sample
complexity; this is summarized by the following theorem  focusing on the “high-dimensional” and
“low-dimensional” setups above in the case of the `2 norm for simplicity.

convex generalized linear model (under Assumption 1) and robust regression (under Assumption 2)
setting.

d I ￿⋅￿= `2. Consider the following meta-algorithm for the non-

Proposition 3. Suppose that ⌃￿ 1

dimension-independent5 constant.

" samples{(xt  yt)}n
"2∧ d
1. Gather n= 1
t=1.
2. Find a point̂walg∈W with￿∇̂Ln(̂walg)￿≤ O￿ 1√n￿  which is guaranteed to exist.

There are many non-convex optimization algorithms that provably ﬁnd an approximate stationary
point of the empirical risk  including gradient descent [33]  SGD [10]  and Non-convex SVRG [38  3].

a-priori. We can circumvent this difﬁculty and take advantage of these generic algorithms by instead
ﬁnding stationary points of the regularized empirical risk. We show that any algorithm that ﬁnds a

This meta-algorithm guarantees E LD(̂walg)− L￿ ≤ C⋅ "  where C is a problem-dependent but
Note  however  that these algorithms are not generically guaranteed to satisfy the constraint̂walg∈W
(unconstrained) stationary point of the regularized empirical risk indeed the obtains optimal O￿ 1
"2￿
n(w) = ̂Ln(w)+ 
setting. Let ̂L
2. For any  > 0 there is a setting of the regularization
2￿w￿2
n(̂walg)= 0 guarantees LD(̂walg)− L￿≤ ˜O￿￿ log(−1)n
parameter  such that any ̂walg with∇̂L
￿
with probability at least 1− .

sample complexity in the norm-based regime.
Theorem 9 (informal). Suppose we are in the generalized linear model setting or robust regression

See Appendix C.3 for the full theorem statement and proof.
Now is a good time to discuss connections to existing work in more detail.

regime is particularly interesting  and goes beyond recent analyses to non-convex statistical

have unavoidable dimension-dependence. This highlights the power of the norm-based
complexity analysis.

a) The sample complexity O￿ 1
"￿ for Proposition 3 is optimal up to dependence on Lip-
"2∧ d
schitz constants and the range parameters B and R [42]. The “high-dimensional” O￿ 1
"2￿
learning [31]  which use arguments involving pointwise covers of the spaceW and thus
b) In the low-dimensional O￿ d
"￿ sample complexity regime  Theorem 3 and Theorem 4 recovers
possible to guarantee c≥ e−BR  and so it may be more realistic to assume BR is constant.
"2￿ for the GLM setting. Our analysis
c) The GLMtron algorithm of [21] also obtains O￿ 1
stationary point ﬁnding algorithm will do. GLMtron has no guarantees in the O￿ d
"￿ regime 

the rates of [31] under the same assumptions—see Appendix C.3 for details. Notably  this is
the case even when the radius R is not constant. Note however that when B and R are large
the constants in Theorem 3 and Theorem 4 can be quite poor. For the logistic link it is only

whereas our meta-algorithm works in both high- and low-dimensional regimes. A signiﬁcant
beneﬁt of GLMtron  however  is that it does not require a lower bound on the derivative of
the link function . It is not clear if this assumption can be removed from our analysis.

shows that this sample complexity does not require specialized algorithms; any ﬁrst-order

d) As an alternative approach  stochastic optimization methods for ﬁnding ﬁrst-order stationary
points can be used to directly ﬁnd an approximate stationary point of the population risk

5Whenever B and R are constant.

7

regime it is possible to show that stochastic gradient descent (and for general smooth norms 

in Appendix C.3. This approach relies on returning a randomly selected iterate from the
sequence and only gives an in-expectation sample complexity guarantee  whereas Theorem 9
gives a high-probability guarantee.

￿LD(w)￿≤ "  so long as they draw a fresh sample at each step. In the high-dimensional
mirror descent) obtains O￿ 1
"2￿ sample complexity through this approach; this is sketched
Also  note that many stochastic optimization methods can exploit the(2  µ)-GD condition.
Suppose we are in the low-dimensional regime with ⌃￿ 1
optimization method that we are aware of is SNVRG [45]  which under the(2  O(d))-GD
condition will obtain " excess risk with O￿ d

"+ d3￿2
"1￿2￿ sample complexity.

d I. The fastest GD-based stochastic

This discussion is summarized in Table 3.

4 Non-Smooth Models

In the previous section we used gradient uniform convergence to derive immediate optimization
and generalization consequences by ﬁnding approximate stationary points of smooth non-convex
functions. In practice—notably in deep learning—it is common to optimize non-smooth non-convex
functions; deep neural networks with rectiﬁed linear units (ReLUs) are the canonical example [24  16].
In theory  it is trivial to construct non-smooth functions for which ﬁnding approximate stationary
points is intractable (see discussion in [2])  but it appears that in practice stochastic gradient descent
can indeed ﬁnd approximate stationary points of the empirical loss in standard neural network
architectures [43]. It is desirable to understand whether gradient generalization can occur in this
setting.
The ﬁrst result of this section is a lower bound showing that even for the simplest possible non-smooth
model—a single ReLU—it is impossible to achieve dimension-independent uniform convergence
results similar to those of the previous section. On the positive side  we show that it is possible to
obtain dimension-independent rates under an additional margin assumption.

Perceptron setup. Note that the loss is not smooth  and so the gradient is not well-deﬁned everywhere.
Thus  to make the problem well-deﬁned  we consider convergence for the following representative

uniform convergence for this setup must depend on dimension  even when the weight norm B and
data norm R are held constant.

The full setting is as follows: X ⊆￿x∈ Rd￿￿x￿2≤ 1￿ W ⊆￿w∈ Rd￿￿w￿2≤ 1￿ Y ={−1 +1} 
and `(w ; x  y)= (−￿w  x￿⋅ y)  where (s)= max{s  0}; this essentially matches the classical
from the subgradient:∇`(w ; x  y)∶=−y {y￿w  x￿≤ 0}⋅ x.6 Our ﬁrst theorem shows that gradient
Theorem 5. Under the problem setting deﬁned above  for all n∈ N there exist a sequence of instances
{(xt  yt)}n
a dimension-independent O(√n) upper bound on the Rademacher complexity. This is perhaps not

surprising since the gradients are discrete functions of w  and indeed VC-style arguments sufﬁce to
establish the lower bound.
In the classical statistical learning setting  the main route to overcoming dimension dependence—e.g. 
for linear classiﬁers—is to assume a margin  which allows one to move from a discrete class to
a real-valued class upon which a dimension-independent Rademacher complexity bound can be
applied [39]. Such arguments have recently been used to derive dimension-independent function
value uniform convergence bounds for deep ReLU networks as well [6  11]. However  this analysis
relies on one-sided control of the loss  so it is not clear whether it extends to the inherently directional
problem of gradient convergence. Our main contribution in this section is to introduce additional
machinery to prove dimension-free gradient convergence under a new type of margin assumption.

✏t∇`(w ; xt  yt)￿2= ⌦￿√dn∧ n￿.

This result contrasts the setting where  is smooth  where the techniques from Section 2 easily yield

t=1 such that

E✏ sup

w∈W￿ n￿t=1

6For general non-convex and non-smooth functions one can extend this approach by considering convergence

for a representative from the Clarke sub-differential [7  8].

8

Deﬁnition 4. Given a distribution P over the supportX and an increasing function ∶[0  1]→[0  1] 
any w∈W is said to satisfy the -soft-margin condition with respect to P if
￿w￿2￿x￿2 ≤ ￿￿≤ ().

∀∈[0  1]  Ex∼P￿ ￿ ￿￿w x￿￿

We call  a margin function. We deﬁne the set of all weights that satisfy the -soft-margin condition
with respect to a distribution P via:

(7)

W(  P)=￿w∈W ∶ ∀∈[0  1]  Ex∼P￿ ￿ ￿￿w x￿￿

￿w￿2￿x￿2 ≤ ￿￿≤ ()￿.

(8)

function  ﬁxed in advance.

Of particular interest isW( ̂Dn)  the set of all the weights that satisfy the -soft-margin condition
with respect to the empirical data distribution. That is  any w∈W( ̂Dn) predicts with at least
a  margin on all but a () fraction of the data. The following theorem provides a dimension-
independent uniform convergence bound for the gradients over the classW( ̂Dn) for any margin
Theorem 6. Let ∶[0  1]→[0  1] be a ﬁxed margin function. With probability at least 1−  over
the draw of the data{(xt  yt)}n
t=1 

>0￿￿￿￿￿￿￿￿￿
w∈W( ̂Dn)￿∇LD(w)−∇̂Ln(w)￿2≤ ˜O￿￿￿inf
￿(4)+ 1
) and log n factors.
where ˜O(⋅) hides log log( 1
As a concrete example  when () =  1
12)  thus circumventing the lower bound of Theorem 5 for large values
convergence bound of O(n− 1

2 Theorem 6 yields a dimension-independent uniform

￿￿￿￿ log￿ 1
￿n

4￿￿￿￿￿￿￿￿￿
￿￿￿  

1
 1
2 n 1

+

sup

of d.

5 Discussion

We showed that vector Rademacher complexities are a simple and effective tool for deriving
dimension-independent uniform convergence bounds and used these bounds in conjunction with the
(population) Gradient Domination property to derive optimal algorithms for non-convex statistical
learning in high and inﬁnite dimension. We hope that these tools will ﬁnd broader use for norm-based
capacity control in non-convex learning settings beyond those considered here. Of particular interest
are models where convergence of higher-order derivatives is needed to ensure success of optimization
routines. Appendix E contains an extension of Theorem 1 for Hessian uniform convergence  which
we anticipate will ﬁnd use in such settings.

dimension-independent norm-based capacity control. While there are many examples of models for

In Section 3 we analyzed generalized linear models and robust regression using both the(1  µ)-GD
property and the(2  µ)-GD property. In particular  the(1  µ)-GD property was critical to obtain
which the population risk satisﬁes(2  µ)-GD property (phase retrieval [40  44]  ResNets with linear
(1  µ)-GD property holds for these models. Establishing this property and consequently deriving

dimension-independent optimization guarantees is an exciting future direction.
Lastly  an important question is to analyze non-smooth problems beyond the simple ReLU example
considered in Section 4. See [9] for subsequent work in this direction.

activations [13]  matrix factorization [29]  blind deconvolution [27])  we do not know whether the

Acknowledgements K.S acknowledges support from the NSF under grants CDS&E-MSS 1521544
and NSF CAREER Award 1750575  and the support of an Alfred P. Sloan Fellowship. D.F. acknowl-
edges support from the NDSEG PhD fellowship and Facebook PhD fellowship.

9

References
[1] Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD. Advances in

Neural Information Processing Systems  2018.

[2] Zeyuan Allen-Zhu. How To Make the Gradients Small Stochastically. Advances in Neural

Information Processing Systems  2018.

[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning  pages 699–707  2016.

[4] Peter L Bartlett and John Shawe-Taylor. Generalization performance of support vector machines

and other pattern classiﬁers. In Advances in kernel methods  pages 43–54. MIT Press  1999.

[5] Peter L Bartlett  Olivier Bousquet  Shahar Mendelson  et al. Local rademacher complexities.

The Annals of Statistics  33(4):1497–1537  2005.

[6] Peter L Bartlett  Dylan J Foster  and Matus J Telgarsky. Spectrally-normalized margin bounds
for neural networks. In Advances in Neural Information Processing Systems  pages 6241–6250 
2017.

[7] Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization: theory

and examples. Springer Science & Business Media  2010.

[8] Frank H Clarke. Optimization and nonsmooth analysis  volume 5. Siam  1990.

[9] Damek Davis and Dmitriy Drusvyatskiy. Uniform graphical convergence of subgradients in

nonconvex optimization and learning. arXiv preprint arXiv:1810.07590  2018.

[10] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex

stochastic programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[11] Noah Golowich  Alexander Rakhlin  and Ohad Shamir. Size-independent sample complexity of

neural networks. Conference on Learning Theory  2018.

[12] Alon Gonen and Shai Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle

problems. Conference on Learning Theory  2017.

[13] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. International Conference on

Learning Representations  2017.

[14] Moritz Hardt  Tengyu Ma  and Benjamin Recht. Gradient descent learns linear dynamical

systems. Journal of Machine Learning Research  2018.

[15] Elad Hazan. Introduction to online convex optimization. Foundations and Trends® in Opti-

mization  2(3-4):157–325  2016.

[16] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[17] Prateek Jain and Purushottam Kar. Non-convex optimization for machine learning. Foundations

and Trends® in Machine Learning  10(3-4):142–336  2017.

[18] Chi Jin  Lydia T Liu  Rong Ge  and Michael I Jordan. Minimizing nonconvex population risk

from rough empirical risk. Advances in Neural Information Processing Systems  2018.

[19] Sham M. Kakade  Karthik Sridharan  and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds  margin bounds  and regularization. In Advances in Neural Information Processing
Systems 21  pages 793–800. MIT Press  2009.

[20] Sham M Kakade  Karthik Sridharan  and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds  margin bounds  and regularization. In Advances in neural information processing
systems  pages 793–800  2009.

10

[21] Sham M Kakade  Varun Kanade  Ohad Shamir  and Adam Kalai. Efﬁcient learning of general-
ized linear and single index models with isotonic regression. In Advances in Neural Information
Processing Systems  pages 927–935  2011.

[22] Sham M Kakade  Shai Shalev-Shwartz  and Ambuj Tewari. Regularization techniques for

learning with matrices. Journal of Machine Learning Research  13(Jun):1865–1890  2012.

[23] Hamed Karimi  Julie Nutini  and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases  pages 795–811. Springer  2016.

[24] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[25] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Springer-Verlag  New

York  1991.

[26] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Non-convex ﬁnite-sum optimization
via scsg methods. In Advances in Neural Information Processing Systems  pages 2345–2355 
2017.

[27] Xiaodong Li  Shuyang Ling  Thomas Strohmer  and Ke Wei. Rapid  robust  and reliable blind
deconvolution via nonconvex optimization. Applied and Computational Harmonic Analysis 
2018.

[28] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu

activation. In Advances in Neural Information Processing Systems  pages 597–607  2017.

[29] Huikang Liu  Weijie Wu  and Anthony Man-Cho So. Quadratic optimization with orthogonality
constraints: Explicit lojasiewicz exponent and linear convergence of line-search methods. In
Proceedings of The 33rd International Conference on Machine Learning  volume 48 of PMLR 
pages 1158–1167  2016.

[30] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International

Conference on Algorithmic Learning Theory  pages 3–17. Springer  2016.

[31] Song Mei  Yu Bai  and Andrea Montanari. The landscape of empirical risk for non-convex

losses. To appear in Annals of Statistics  2016.

[32] Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of machine learning.

2012.

[33] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87.

Springer Science & Business Media  2013.

[34] Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals

of Probability  22(4):1679–1706  1994.

[35] Gilles Pisier. Martingales with values in uniformly convex spaces. Israel Journal of Mathematics 

20:326–350  1975. ISSN 0021-2172.

[36] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychisli-

tel’noi Matematiki i Matematicheskoi Fiziki  3(4):643–653  1963.

[37] Garvesh Raskutti  Martin J Wainwright  and Bin Yu. Restricted eigenvalue properties for

correlated gaussian designs. Journal of Machine Learning Research  11:2241–2259  2010.

[38] Sashank J Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex Smola. Stochastic
variance reduction for nonconvex optimization. In International conference on machine learning 
pages 314–323  2016.

[39] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

11

[40] Ju Sun  Qing Qu  and John Wright. A geometric analysis of phase retrieval. In Information

Theory (ISIT)  2016 IEEE International Symposium on  pages 2379–2383. IEEE  2016.

[41] Robert Tibshirani  Martin Wainwright  and Trevor Hastie. Statistical learning with sparsity: the

lasso and generalizations. Chapman and Hall/CRC  2015.

[42] Alexandre B Tsybakov. Introduction to Nonparametric Estimation. Springer Science & Business

Media  2008.

[43] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. International Conference on Learning
Representations  2017.

[44] Huishuai Zhang  Yuejie Chi  and Yingbin Liang. Median-truncated nonconvex approach for

phase retrieval with outliers. International Conference on Machine Learning  2016.

[45] Dongruo Zhou  Pan Xu  and Quanquan Gu. Stochastic nested variance reduction for nonconvex

optimization. Advances in Neural Information Processing Systems  2018.

12

,Dylan Foster
Ayush Sekhari
Karthik Sridharan