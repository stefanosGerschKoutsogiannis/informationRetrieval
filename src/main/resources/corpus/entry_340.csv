2017,Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM,Traditional approaches to differential privacy assume a fixed privacy requirement ε for a computation  and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings  it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general “noise reduction” framework that can apply to a variety of private empirical risk minimization (ERM) algorithms  using them to “search” the space of privacy levels to find the empirically strongest one that meets the accuracy constraint  and incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data  which we term ex-post privacy  and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool  modifying it to allow for queries chosen depending on the database. Finally  we apply our approach to two common objective functions  regularized linear and logistic regression  and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger empirical baseline based on binary search.,Accuracy First: Selecting a Differential Privacy Level

for Accuracy-Constrained ERM

Katrina Ligett

Caltech and Hebrew University

Seth Neel

University of Pennsylvania

Aaron Roth

University of Pennsylvania

Bo Waggoner

University of Pennsylvania

Zhiwei Steven Wu
Microsoft Research

Abstract

Traditional approaches to differential privacy assume a ﬁxed privacy requirement
ε for a computation  and attempt to maximize the accuracy of the computation
subject to the privacy constraint. As differential privacy is increasingly deployed in
practical settings  it may often be that there is instead a ﬁxed accuracy requirement
for a given computation and the data analyst would like to maximize the privacy of
the computation subject to the accuracy constraint. This raises the question of how
to ﬁnd and run a maximally private empirical risk minimizer subject to a given
accuracy requirement. We propose a general “noise reduction” framework that
can apply to a variety of private empirical risk minimization (ERM) algorithms 
using them to “search” the space of privacy levels to ﬁnd the empirically strongest
one that meets the accuracy constraint  and incurring only logarithmic overhead
in the number of privacy levels searched. The privacy analysis of our algorithm
leads naturally to a version of differential privacy where the privacy parameters
are dependent on the data  which we term ex-post privacy  and which is related
to the recently introduced notion of privacy odometers. We also give an ex-post
privacy analysis of the classical AboveThreshold privacy tool  modifying it to allow
for queries chosen depending on the database. Finally  we apply our approach to
two common objective functions  regularized linear and logistic regression  and
empirically compare our noise reduction methods to (i) inverting the theoretical
utility guarantees of standard private ERM algorithms and (ii) a stronger  empirical
baseline based on binary search.1

1

Introduction and Related Work

Differential Privacy [7  8] enjoys over a decade of study as a theoretical construct  and a much more
recent set of large-scale practical deployments  including by Google [10] and Apple [11]. As the large
theoretical literature is put into practice  we start to see disconnects between assumptions implicit
in the theory and the practical necessities of applications. In this paper we focus our attention on
one such assumption in the domain of private empirical risk minimization (ERM): that the data
analyst ﬁrst chooses a privacy requirement  and then attempts to obtain the best accuracy guarantee
(or empirical performance) that she can  given the chosen privacy constraint. Existing theory is
tailored to this view: the data analyst can pick her privacy parameter ε via some exogenous process 
and either plug it into a “utility theorem” to upper bound her accuracy loss  or simply deploy her
algorithm and (privately) evaluate its performance. There is a rich and substantial literature on private
convex ERM that takes this approach  weaving tight connections between standard mechanisms in

1A full version of this paper appears on the arXiv preprint site: https://arxiv.org/abs/1705.10829.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

differential privacy and standard tools for empirical risk minimization. These methods for private
ERM include output and objective perturbation [5  14  18  4]  covariance perturbation [19]  the
exponential mechanism [16  2]  and stochastic gradient descent [2  21  12  6  20].
While these existing algorithms take a privacy-ﬁrst perspective  in practice  product requirements
may impose hard accuracy constraints  and privacy (while desirable) may not be the over-riding
concern. In such situations  things are reversed: the data analyst ﬁrst ﬁxes an accuracy requirement 
and then would like to ﬁnd the smallest privacy parameter consistent with the accuracy constraint.
Here  we ﬁnd a gap between theory and practice. The only theoretically sound method available is to
take a “utility theorem” for an existing private ERM algorithm and solve for the smallest value of
ε (the differential privacy parameter)—and other parameter values that need to be set—consistent
with her accuracy requirement  and then run the private ERM algorithm with the resulting ε. But
because utility theorems tend to be worst-case bounds  this approach will generally be extremely
conservative  leading to a much larger value of ε (and hence a much larger leakage of information)
than is necessary for the problem at hand. Alternately  the analyst could attempt an empirical search
for the smallest value of ε consistent with her accuracy goals. However  because this search is itself
a data-dependent computation  it incurs the overhead of additional privacy loss. Furthermore  it is
not a priori clear how to undertake such a search with nontrivial privacy guarantees for two reasons:
ﬁrst  the worst case could involve a very long search which reveals a large amount of information 
and second  the selected privacy parameter is now itself a data-dependent quantity  and so it is not
sensible to claim a “standard” guarantee of differential privacy for any ﬁnite value of ε ex-ante.
In this paper  we provide a principled variant of this second approach  which attempts to empirically
ﬁnd the smallest value of ε consistent with an accuracy requirement. We give a meta-method that
can be applied to several interesting classes of private learning algorithms and introduces very little
privacy overhead as a result of the privacy-parameter search. Conceptually  our meta-method initially
computes a very private hypothesis  and then gradually subtracts noise (making the computation less
and less private) until a sufﬁcient level of accuracy is achieved. One key technique that signiﬁcantly
reduces privacy loss over naive search is the use of correlated noise generated by the method of [15] 
which formalizes the conceptual idea of “subtracting” noise without incurring additional privacy
overhead. In order to select the most private of these queries that meets the accuracy requirement  we
introduce a natural modiﬁcation of the now-classic AboveThreshold algorithm [8]  which iteratively
checks a sequence of queries on a dataset and privately releases the index of the ﬁrst to approximately
exceed some ﬁxed threshold. Its privacy cost increases only logarithmically with the number of
queries. We provide an analysis of AboveThreshold that holds even if the queries themselves are
the result of differentially private computations  showing that if AboveThreshold terminates after t
queries  one only pays the privacy costs of AboveThreshold plus the privacy cost of revealing those
ﬁrst t private queries. When combined with the above-mentioned correlated noise technique of [15] 
this gives an algorithm whose privacy loss is equal to that of the ﬁnal hypothesis output – the previous
ones coming “for free” – plus the privacy loss of AboveThreshold. Because the privacy guarantees
achieved by this approach are not ﬁxed a priori  but rather are a function of the data  we introduce
and apply a new  corresponding privacy notion  which we term ex-post privacy  and which is closely
related to the recently introduced notion of “privacy odometers” [17].
In Section 4  we empirically evaluate our noise reduction meta-method  which applies to any ERM
technique which can be described as a post-processing of the Laplace mechanism. This includes both
direct applications of the Laplace mechanism  like output perturbation [5]; and more sophisticated
methods like covariance perturbation [19]  which perturbs the covariance matrix of the data and
then performs an optimization using the noisy data. Our experiments concentrate on (cid:96)2 regularized
least-squares regression and (cid:96)2 regularized logistic regression  and we apply our noise reduction
meta-method to both output perturbation and covariance perturbation. Our empirical results show
that the active  ex-post privacy approach massively outperforms inverting the theory curve  and also
improves on a baseline “ε-doubling” approach.

2 Privacy Background and Tools

2.1 Differential Privacy and Ex-Post Privacy
Let X denote the data domain. We call two datasets D  D(cid:48) ∈ X ∗ neighbors (written as D ∼ D(cid:48)) if
D can be derived from D(cid:48) by replacing a single data point with some other element of X .

2

Deﬁnition 2.1 (Differential Privacy [7]). Fix ε ≥ 0. A randomized algorithm A : X ∗ → O is
ε-differentially private if for every pair of neighboring data sets D ∼ D(cid:48) ∈ X ∗  and for every event
S ⊆ O:

Pr[A(D) ∈ S] ≤ exp(ε) Pr[A(D(cid:48)) ∈ S].

We call exp(ε) the privacy risk factor.

It is possible to design computations that do not satisfy the differential privacy deﬁnition  but whose
outputs are private to an extent that can be quantiﬁed after the computation halts. For example 
consider an experiment that repeatedly runs an ε(cid:48)-differentially private algorithm  until a stopping
condition deﬁned by the output of the algorithm itself is met. This experiment does not satisfy
ε-differential privacy for any ﬁxed value of ε  since there is no ﬁxed maximum number of rounds
for which the experiment will run (for a ﬁxed number of rounds  a simple composition theorem 
Theorem 2.5  shows that the ε-guarantees in a sequence of computations “add up.”) However  if ex-
post we see that the experiment has stopped after k rounds  the data can in some sense be assured an
“ex-post privacy loss” of only kε(cid:48). Rogers et al. [17] initiated the study of privacy odometers  which
formalize this idea. They study privacy composition when the data analyst can choose the privacy
parameters of subsequent computations as a function of the outcomes of previous computations.
We apply a related idea here  for a different purpose. Our goal is to design one-shot algorithms that
always achieve a target accuracy but that may have variable privacy levels depending on their input.
Deﬁnition 2.2. Given a randomized algorithm A : X ∗ → O  deﬁne the ex-post privacy loss2 of A
on outcome o to be

Loss(o) = max

D D(cid:48):D∼D(cid:48) log

Pr [A(D) = o]
Pr [A(D(cid:48)) = o]

.

We refer to exp (Loss(o)) as the ex-post privacy risk factor.
Deﬁnition 2.3 (Ex-Post Differential Privacy). Let E : O → (R≥0 ∪ {∞}) be a function on the
outcome space of algorithm A : X ∗ → O. Given an outcome o = A(D)  we say that A satisﬁes
E(o)-ex-post differential privacy if for all o ∈ O  Loss(o) ≤ E(o).
Note that if E(o) ≤ ε for all o  A is ε-differentially private. Ex-post differential privacy has the
same semantics as differential privacy  once the output of the mechanism is known: it bounds the
log-likelihood ratio of the dataset being D vs. D(cid:48)  which controls how an adversary with an arbitrary
prior on the two cases can update her posterior.

2.2 Differential Privacy Tools

Differentially private computations enjoy two nice properties:
Theorem 2.4 (Post Processing [7]). Let A : X ∗ → O be any ε-differentially private algorithm  and
let f : O → O(cid:48) be any function. Then the algorithm f ◦ A : X ∗ → O(cid:48) is also ε-differentially private.
Post-processing implies that  for example  every decision process based on the output of a differen-
tially private algorithm is also differentially private.
Theorem 2.5 (Composition [7]). Let A1 : X ∗ → O  A2 : X ∗ → O(cid:48) be algorithms that are
ε1- and ε2-differentially private  respectively. Then the algorithm A : X ∗ → O × O(cid:48) deﬁned as
A(x) = (A1(x)  A2(x)) is (ε1 + ε2)-differentially private.

The composition theorem holds even if the composition is adaptive—-see [9] for details.
The Laplace mechanism. The most basic subroutine we will use is the Laplace mechanism. The
Laplace Distribution centered at 0 with scale b is the distribution with probability density function
Lap (z|b) = 1
b . We say X ∼ Lap (b) when X has Laplace distribution with scale b. Let
f : X ∗ → Rd be an arbitrary d-dimensional function. The (cid:96)1 sensitivity of f is deﬁned to be
∆1(f ) = maxD∼D(cid:48) (cid:107)f (D) − f (D(cid:48))(cid:107)1. The Laplace mechanism with parameter ε simply adds noise
drawn independently from Lap

to each coordinate of f (x).

2b e− |z|

(cid:16) ∆1(f )

(cid:17)

ε

2If A’s output is from a continuous distribution rather than discrete  we abuse notation and write Pr[A(D) =

o] to mean the probability density at output o.

3

Theorem 2.6 ([7]). The Laplace mechanism is ε-differentially private.

Gradual private release. Koufogiannis et al. [15] study how to gradually release private data using
the Laplace mechanism with an increasing sequence of ε values  with a privacy cost scaling only with
the privacy of the marginal distribution on the least private release  rather than the sum of the privacy
costs of independent releases. For intuition  the algorithm can be pictured as a continuous random
walk starting at some private data v with the property that the marginal distribution at each point in
time is Laplace centered at v  with variance increasing over time. Releasing the value of the random
walk at a ﬁxed point in time gives a certain output distribution  for example  ˆv  with a certain privacy
guarantee ε. To produce ˆv(cid:48) whose ex-ante distribution has higher variance (is more private)  one can
simply “fast forward” the random walk from a starting point of ˆv to reach ˆv(cid:48); to produce a less private
ˆv(cid:48)  one can “rewind.” The total privacy cost is max{ε  ε(cid:48)} because  given the “least private” point
(say ˆv)  all “more private” points can be derived as post-processings given by taking a random walk
of a certain length starting at ˆv. Note that were the Laplace random variables used for each release
independent  the composition theorem would require summing the ε values of all releases.
In our private algorithms  we will use their noise reduction mechanism as a building block to generate
a list of private hypotheses θ1  . . .   θT with gradually increasing ε values. Importantly  releasing any
preﬁx (θ1  . . .   θt) only incurs the privacy loss in θt. More formally:
Algorithm 1 Noise Reduction [15]: NR(v  ∆ {εt})

(cid:46) drawn i.i.d. for each coordinate

Input: private vector v  sensitivity parameter ∆  list ε1 < ε2 < ··· < εT
Set ˆvT := v + Lap (∆/εT )
for t = T − 1  T − 2  . . .   1 do

(cid:16) εt

(cid:17)2

With probability
Else: set ˆvt := ˆvt+1 + Lap (∆/εt)

εt+1

: set ˆvt := ˆvt+1

(cid:46) drawn i.i.d. for each coordinate

Return ˆv1  . . .   ˆvT

Theorem 2.7 ([15]). Let f have (cid:96)1 sensitivity ∆ and let ˆv1  . . .   ˆvT be the output of Algorithm 1 on
v = f (D)  ∆  and the increasing list ε1  . . .   εT . Then for any t  the algorithm which outputs the
preﬁx (ˆv1  . . .   ˆvt) is εt-differentially private.

2.3 AboveThreshold with Private Queries

Our high-level approach to our eventual ERM problem will be as follows: Generate a sequence
of hypotheses θ1  . . .   θT   each with increasing accuracy and decreasing privacy; then test their
accuracy levels sequentially  outputting the ﬁrst one whose accuracy is “good enough.” The classical
AboveThreshold algorithm [8] takes in a dataset and a sequence of queries and privately outputs the
index of the ﬁrst query to exceed a given threshold (with some error due to noise). We would like to
use AboveThreshold to perform these accuracy checks  but there is an important obstacle: for us  the
“queries” themselves depend on the private data.3 A standard composition analysis would involve ﬁrst
privately publishing all the queries  then running AboveThreshold on these queries (which are now
public). Intuitively  though  it would be much better to generate and publish the queries one at a time 
until AboveThreshold halts  at which point one would not publish any more queries. The problem
with analyzing this approach is that  a-priori  we do not know when AboveThreshold will terminate;
to address this  we analyze the ex-post privacy guarantee of the algorithm.4
Let us say that an algorithm M (D) = (f1  . . .   fT ) is (ε1  . . .   εT )-preﬁx-private if for each t  the
function that runs M (D) and outputs just the preﬁx (f1  . . .   ft) is εt-differentially private.
Lemma 2.8. Let M : X ∗ → (X ∗ → O)T be a (ε1  . . .   εT )-preﬁx private algorithm that returns T
queries  and let each query output by M have (cid:96)1 sensitivity at most ∆. Then Algorithm 2 run on D 
εA  W   ∆  and M is E-ex-post differentially private for E((t ·)) = εA + εt for any t ∈ [T ].

3In fact  there are many applications beyond our own in which the sequence of queries input to AboveThresh-
old might be the result of some private prior computation on the data  and where we would like to release both the
stopping index of AboveThreshold and the “query object.” (In our case  the query objects will be parameterized
by learned hypotheses θ1  . . .   θT .)

4This result does not follow from a straightforward application of privacy odometers from [17]  because the

privacy analysis of algorithms like the noise reduction technique is not compositional.

4

Input: Dataset D  privacy loss ε  threshold W   (cid:96)1 sensitivity ∆  algorithm M

Algorithm 2 InteractiveAboveThreshold: IAT(D  ε  W  ∆  M )

Let ˆW = W + Lap(cid:0) 2∆
if ft(D) + Lap(cid:0) 4∆

Query ft ← M (D)t

(cid:1)
(cid:1) ≥ ˆW : then Output (t  ft); Halt.

for each query t = 1  . . .   T do

ε

ε

Output (T   ⊥).

The proof  which is a variant on the proof of privacy for AboveThreshold [8]  appears in the full
version  along with an accuracy theorem for IAT.

3 Noise-Reduction with Private ERM

In this section  we provide a general private ERM framework that allows us to approach the best
privacy guarantee achievable on the data given a target excess risk goal. Throughout the section 
we consider an input dataset D that consists of n row vectors X1  X2  . . .   Xn ∈ Rp and a column
y ∈ Rn. We will assume that each (cid:107)Xi(cid:107)1 ≤ 1 and |yi| ≤ 1. Let di = (Xi  yi) ∈ Rp+1 be the i-th
data record. Let (cid:96) be a loss function such that for any hypothesis θ and any data point (Xi  yi) the loss
is (cid:96)(θ  (Xi  yi)). Given an input dataset D and a regularization parameter λ  the goal is to minimize
the following regularized empirical loss function over some feasible set C:

n(cid:88)

i=1

L(θ  D) =

1
n

(cid:96)(θ  (Xi  yi)) +

(cid:107)θ(cid:107)2
2.

λ
2

Let θ∗ = argminθ∈C (cid:96)(θ  D). Given a target accuracy parameter α  we wish to privately compute a
θp that satisﬁes L(θp  D) ≤ L(θ∗  D) + α  while achieving the best ex-post privacy guarantee. For
simplicity  we will sometimes write L(θ) for L(θ  D).
One simple baseline approach is a “doubling method”: Start with a small ε value  run an ε-
differentially private algorithm to compute a hypothesis θ and use the Laplace mechanism to estimate
the excess risk of θ; if the excess risk is lower than the target  output θ; otherwise double the value of
ε and repeat the same process. (See the full version for details.) As a result  we pay for privacy loss
for every hypothesis we compute and every excess risk we estimate.
In comparison  our meta-method provides a more cost-effective way to select the privacy level. The
algorithm takes a more reﬁned set of privacy levels ε1 < . . . < εT as input and generates a sequence
of hypotheses θ1  . . .   θT such that the generation of each θt is εt-private. Then it releases the
hypotheses θt in order  halting as soon as a released hypothesis meets the accuracy goal. Importantly 
there are two key components that reduce the privacy loss in our method:

1. We use Algorithm 1  the “noise reduction” method of [15]  for generating the sequence of
hypotheses: we ﬁrst compute a very private and noisy θ1  and then obtain the subsequent
hypotheses by gradually “de-noising” θ1. As a result  any preﬁx (θ1  . . .   θk) incurs a
privacy loss of only εk (as opposed to (ε1 + . . . + εk) if the hypotheses were independent).

2. When evaluating the excess risk of each hypothesis  we use Algorithm 2  Interactive-
AboveThreshold  to determine if its excess risk exceeds the target threshold. This incurs
substantially less privacy loss than independently evaluating the excess risk of each hypothe-
sis using the Laplace mechanism (and hence allows us to search a ﬁner grid of values).

For the rest of this section  we will instantiate our method concretely for two ERM problems: ridge
regression and logistic regression. In particular  our noise-reduction method is based on two private
ERM algorithms: the recently introduced covariance perturbation technique [19] and the output
perturbation method [5].

5

3.1 Covariance Perturbation for Ridge Regression

In ridge regression  we consider the squared loss function: (cid:96)((Xi  yi)  θ) = 1
hence empirical loss over the data set is deﬁned as

2 (yi − (cid:104)θ  Xi(cid:105))2  and

(cid:107)y − Xθ(cid:107)2

1
2n

λ(cid:107)θ(cid:107)2

2

2 +

L(θ  D) =

where X denotes the (n × p) matrix with row vectors X1  . . .   Xn and y = (y1  . . .   yn). Since the

optimal solution for the unconstrained problem has (cid:96)2 norm no more than(cid:112)1/λ (see the full version
for a proof)  we will focus on optimizing θ over the constrained set C = {a ∈ Rp | (cid:107)a(cid:107)2 ≤(cid:112)1/λ} 

which will be useful for bounding the (cid:96)1 sensitivity of the empirical loss.
Before we formally introduce the covariance perturbation algorithm due to [19]  observe that the
optimal solution θ∗ can be computed as

2

 

θ∗ = argmin
θ∈C

L(θ  D) = argmin

(cid:124)

(θ

(X

(cid:124)

X)θ − 2(cid:104)X

(cid:124)

y  θ(cid:105))

λ(cid:107)θ(cid:107)2

2

.

+

θ∈C

(cid:124)

2n
In other words  θ∗ only depends on the private data through X
X. To compute a private
hypothesis  the covariance perturbation method simply adds Laplace noise to each entry of X
y and
X (the covariance matrix)  and solves the optimization based on the noisy matrix and vector. The
X
formal description of the algorithm and its guarantee are in Theorem 3.1. Our analysis differs from
the one in [19] in that their paper considers the “local privacy” setting  and also adds Gaussian noise
whereas we use Laplace. The proof is deferred to the full version.
Theorem 3.1. Fix any ε > 0. For any input data set D  consider the mechanism M that computes

y and X

(cid:124)

(cid:124)

(cid:124)

2

θp = argmin

θ∈C

1
2n

(cid:124)

(θ

(X

(cid:124)

X + B)θ − 2(cid:104)X

(cid:124)

y + b  θ(cid:105)) +

λ(cid:107)θ(cid:107)2

2

 

2

where B ∈ Rp×p and b ∈ Rp×1 are random Laplace matrices such that each entry of B and b is
drawn from Lap (4/ε). Then M satisﬁes ε-differential privacy and the output θp satisﬁes

√
[L(θp) − L(θ∗)] ≤ 4

E
B b

2(2(cid:112)p/λ + p/λ)

.

nε

(cid:124)

(cid:124)

In our algorithm COVNR  we will apply the noise reduction method  Algorithm 1  to produce a
sequence of noisy versions of the private data (X
y): (Z 1  z1)  . . .   (Z T   zT )  one for each
privacy level. Then for each (Z t  zt)  we will compute the private hypothesis by solving the noisy
version of the optimization problem in Equation (1). The full description of our algorithm COVNR is
in Algorithm 3  and satisﬁes the following guarantee:
Theorem 3.2. The instantiation of COVNR(D {ε1  . . .   εT}  α  γ) outputs a hypothesis θp that with
probability 1 − γ satisﬁes L(θp) − L(θ∗) ≤ α. Moreover  it is E-ex-post differentially private  where
the privacy loss function E : (([T ] ∪ {⊥}) × Rp) → (R≥0 ∪ {∞}) is deﬁned as E((k ·)) = ε0 + εk
for any k (cid:54)=⊥  E((⊥ ·)) = ∞  and

X  X

16((cid:112)1/λ + 1)2 log(2T /γ)

nα

is the privacy loss incurred by IAT.

ε0 =

3.2 Output Perturbation for Logistic Regression

In this setting 

Next  we show how to combine the output perturbation method with noise reduction for the
ridge regression problem.5
the input data consists of n labeled examples
(X1  y1)  . . .   (Xn  yn)  such that for each i  Xi ∈ Rp  (cid:107)Xi(cid:107)1 ≤ 1  and yi ∈ {−1  1}. The goal is to
train a linear classiﬁer given by a weight vector θ for the examples from the two classes. We consider
the logistic loss function: (cid:96)(θ  (Xi  yi)) = log(1 + exp(−yiθ
(cid:124)
log(1 + exp(−yiθ

Xi))  and the empirical loss is

n(cid:88)

L(θ  D) =

λ(cid:107)θ(cid:107)2

Xi)) +

(cid:124)

2

.

2

1
n

i=1

5We study the ridge regression problem for concreteness. Our method works for any ERM problem with

strongly convex loss functions.

6

Algorithm 3 Covariance Perturbation with Noise-Reduction: COVNR(D {ε1  . . .   εT}  α  γ)

Input: private data set D = (X  y)  accuracy parameter α  privacy levels ε1 < ε2 < . . . < εT  
and failure probability γ
Instantiate InteractiveAboveThreshold: A = IAT(D  ε0 −α/2  ∆ ·) with ε0 =

16∆(log(2T /γ))/α and ∆ = ((cid:112)1/λ + 1)2/(n)
Let C = {a ∈ Rp | (cid:107)a(cid:107)2 ≤(cid:112)1/λ} and θ∗ = argminθ∈C L(θ)

Compute noisy data:
(cid:124)
{Z t} = NR((X
for t = 1  . . .   T : do

X)  2 {ε1/2  . . .   εT /2}) 

{zt} = NR((X

(cid:124)

Y )  2 {ε1/2  . . .   εT /2})

(cid:0)θ

(cid:124)

Z tθ − 2(cid:104)zt  θ(cid:105)(cid:1) +

λ(cid:107)θ(cid:107)2

2

2

θt = argmin

θ∈C

1
2n

(1)

Let f t(D) = L(θ∗  D) − L(θt  D); Query A with query f t to check accuracy
if A returns (t  f t) then Output (t  θt)

(cid:46) Accurate hypothesis found.

Output: (⊥  θ∗)

The output perturbation method simply adds Laplace noise to perturb each coordinate of the optimal
solution θ∗. The following is the formal guarantee of output perturbation. Our analysis deviates
slightly from the one in [5] since we are adding Laplace noise (see the full version).
√
Theorem 3.3. Fix any ε > 0. Let r = 2
nλε . For any input dataset D  consider the mechanism that
ﬁrst computes θ∗ = argminθ∈Rp L(θ)  then outputs θp = θ∗ + b  where b is a random vector with its
entries drawn i.i.d. from Lap (r). Then M satisﬁes ε-differential privacy  and θp has excess risk

p

√
[L(θp) − L(θ∗)] ≤ 2

E
b

2p
nλε

+

4p2
n2λε2 .

Given the output perturbation method  we can simply apply the noise reduction method NR to the
optimal hypothesis θ∗ to generate a sequence of noisy hypotheses. We will again use Interactive-
AboveThreshold to check the excess risk of the hypotheses. The full algorithm OUTPUTNR follows
the same structure in Algorithm 3  and we defer the formal description to the full version.
Theorem 3.4. The instantiation of OUTPUTNR(D  ε0 {ε1  . . .   εT}  α  γ) is E-ex-post differentially
private and outputs a hypothesis θp that with probability 1 − γ satisﬁes L(θp) − L(θ∗) ≤ α  where
the privacy loss function E : (([T ] ∪ {⊥}) × Rp) → (R≥0 ∪ {∞}) is deﬁned as E((k ·)) = ε0 + εk
for any k (cid:54)=⊥  E((⊥ ·)) = ∞  and

ε0 ≤ 32 log(2T /γ)(cid:112)2 log 2/λ

nα

is the privacy loss incurred by IAT.

Proof sketch of Theorems 3.2 and 3.4. The accuracy guarantees for both algorithms follow from an
accuracy guarantee of the IAT algorithm (a variant on the standard AboveThreshold bound) and
the fact that we output θ∗ if IAT identiﬁes no accurate hypothesis. For the privacy guarantee  ﬁrst
note that any preﬁx of the noisy hypotheses θ1  . . .   θt satisﬁes εt-differential privacy because of
our instantiation of the Laplace mechanism (see the full version for the (cid:96)1 sensitivity analysis) and
noise-reduction method NR. Then the ex-post privacy guarantee directly follows Lemma 2.8.

4 Experiments

To evaluate the methods described above  we conducted empirical evaluations in two settings. We
used ridge regression to predict (log) popularity of posts on Twitter in the dataset of [1]  with p = 77
features and subsampled to n =100 000 data points. Logistic regression was applied to classifying

7

(a) Linear (ridge) regression 

vs theory approach.

(b) Regularized logistic regression 

vs theory approach.

(c) Linear (ridge) regression 

vs DOUBLINGMETHOD.

(d) Regularized logistic regression 

vs DOUBLINGMETHOD.

Figure 1: Ex-post privacy loss. (1a) and (1c)  left  represent ridge regression on the Twitter dataset 
where Noise Reduction and DOUBLINGMETHOD both use Covariance Perturbation. (1b) and (1d) 
right  represent logistic regression on the KDD-99 Cup dataset  where both Noise Reduction and
DOUBLINGMETHOD use Output Perturbation. The top plots compare Noise Reduction to the “theory
approach”: running the algorithm once using the value of ε that guarantees the desired expected
error via a utility theorem. The bottom compares to the DOUBLINGMETHOD baseline. Note the top
plots are generous to the theory approach: the theory curves promise only expected error  whereas
Noise Reduction promises a high probability guarantee. Each point is an average of 80 trials (Twitter
dataset) or 40 trials (KDD-99 dataset).

network events as innocent or malicious in the KDD-99 Cup dataset [13]  with 38 features and
subsampled to 100 000 points. Details of parameters and methods appear in the full version.6
In each case  we tested the algorithm’s average ex-post privacy loss for a range of input accuracy goals
α  ﬁxing a modest failure probability γ = 0.1 (and we observed that excess risks were concentrated
well below α/2  suggesting a pessimistic analysis). The results show our meta-method gives a
large improvement over the “theory” approach of simply inverting utility theorems for private ERM
algorithms. (In fact  the utility theorem for the popular private stochastic gradient descent algorithm
does not even give meaningful guarantees for the ranges of parameters tested; one would need an
order of magnitude more data points  and even then the privacy losses are enormous  perhaps due to
loose constants in the analysis.)
To gauge the more modest improvement over DOUBLINGMETHOD  note that the variation in the
privacy risk factor eε can still be very large; for instance  in the ridge regression setting of α = 0.05 

6 A full

Accuracy-First-Differential-Privacy.

implementation of our algorithms appears at:

https://github.com/steven7woo/

8

0.000.050.100.150.20Input α (excess error guarantee)05101520ex-post privacy loss Comparison to theory approachCovarPert theoryOutputPert theoryNoiseReduction0.000.050.100.150.20Input α (excess error guarantee)02468101214ex-post privacy loss Comparison to theory approachOutputPert theoryNoiseReduction0.000.050.100.150.20Input α (excess error guarantee)0246810ex-post privacy loss Comparison to DoublingDoublingNoiseReduction0.000.050.100.150.20Input α (excess error guarantee)0.00.51.01.52.02.53.03.5ex-post privacy loss Comparison to DoublingDoublingNoiseReductionNoise Reduction has eε ≈ 10.0 while DOUBLINGMETHOD has eε ≈ 495; at α = 0.075  the privacy
risk factors are 4.65 and 56.6 respectively.
Interestingly  for our meta-method  the contribution to privacy loss from “testing” hypotheses (the
InteractiveAboveThreshold technique) was signiﬁcantly larger than that from “generating” them
(NoiseReduction). One place where the InteractiveAboveThreshold analysis is loose is in using a
theoretical bound on the maximum norm of any hypothesis to compute the sensitivity of queries.
The actual norms of hypotheses tested was signiﬁcantly lower which  if taken as guidance to the
practitioner in advance  would drastically improve the privacy guarantee of both adaptive methods.

5 Future Directions

Throughout this paper  we focus on ε-differential privacy  instead of the weaker (ε  δ)-(approximate)
differential privacy. Part of the reason is that an analogue of Lemma 2.8 does not seem to hold for
(ε  δ)-differentially private queries without further assumptions  as the necessity to union-bound over
the δ “failure probability” that the privacy loss is bounded for each query can erase the ex-post gains.
We leave obtaining similar results for approximate differential privacy as an open problem. More
generally  we wish to extend our ex-post privacy framework to approximate differential privacy  or
to the stronger notion of concentrated differential privacy [3]. Such results will allow us to obtain
ex-post privacy guarantees for a much broader class of algorithms.

9

References
[1] The AMA Team at Laboratoire d’Informatique de Grenoble. Buzz prediction in online social

media  2017.

[2] Raef Bassily  Adam D. Smith  and Abhradeep Thakurta. Private empirical risk minimization 

revisited. CoRR  abs/1405.7085  2014.

[3] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simpliﬁcations  extensions 
and lower bounds. In Theory of Cryptography - 14th International Conference  TCC 2016-B 
Beijing  China  October 31 - November 3  2016  Proceedings  Part I  pages 635–658  2016.

[4] Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances
in Neural Information Processing Systems 21  Proceedings of the Twenty-Second Annual
Conference on Neural Information Processing Systems  Vancouver  British Columbia  Canada 
December 8-11  2008  pages 289–296  2008.

[5] Kamalika Chaudhuri  Claire Monteleoni  and Anand D. Sarwate. Differentially private empirical

risk minimization. Journal of Machine Learning Research  12:1069–1109  2011.

[6] John C. Duchi  Michael I. Jordan  and Martin J. Wainwright. Local privacy and statistical
minimax rates. In 51st Annual Allerton Conference on Communication  Control  and Computing 
Allerton 2013  Allerton Park & Retreat Center  Monticello  IL  USA  October 2-4  2013  page
1592  2013.

[7] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography Conference  pages 265–284.
Springer  2006.

[8] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Founda-

tions and Trends R(cid:13) in Theoretical Computer Science  9(3–4):211–407  2014.

[9] Cynthia Dwork  Guy N Rothblum  and Salil Vadhan. Boosting and differential privacy. In
Foundations of Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  pages 51–60.
IEEE  2010.

[10] Giulia Fanti  Vasyl Pihur  and Úlfar Erlingsson. Building a rappor with the unknown: Privacy-
preserving learning of associations and data dictionaries. Proceedings on Privacy Enhancing
Technologies (PoPETS)  issue 3  2016  2016.

[11] Andy Greenberg. Apple’s ’differential privacy’ is about collecting your data—but not your data.

Wired Magazine  2016.

[12] Prateek Jain  Pravesh Kothari  and Abhradeep Thakurta. Differentially private online learning.
In COLT 2012 - The 25th Annual Conference on Learning Theory  June 25-27  2012  Edinburgh 
Scotland  pages 24.1–24.34  2012.

[13] KDD’99. Kdd cup 1999 data  1999.

[14] Daniel Kifer  Adam D. Smith  and Abhradeep Thakurta. Private convex optimization for
empirical risk minimization with applications to high-dimensional regression. In COLT 2012
- The 25th Annual Conference on Learning Theory  June 25-27  2012  Edinburgh  Scotland 
pages 25.1–25.40  2012.

[15] Fragkiskos Koufogiannis  Shuo Han  and George J. Pappas. Gradual release of sensitive data

under differential privacy. Journal of Privacy and Conﬁdentiality  7  2017.

[16] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In Foundations
of Computer Science  2007. FOCS’07. 48th Annual IEEE Symposium on  pages 94–103. IEEE 
2007.

[17] Ryan M Rogers  Aaron Roth  Jonathan Ullman  and Salil Vadhan. Privacy odometers and
ﬁlters: Pay-as-you-go composition. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and
R. Garnett  editors  Advances in Neural Information Processing Systems 29  pages 1921–1929.
Curran Associates  Inc.  2016.

10

[18] Benjamin I. P. Rubinstein  Peter L. Bartlett  Ling Huang  and Nina Taft. Learning in a large
function space: Privacy-preserving mechanisms for SVM learning. CoRR  abs/0911.5708  2009.

[19] Adam Smith  Jalaj Upadhyay  and Abhradeep Thakurta. Is interaction necessary for distributed

private learning? IEEE Symposium on Security and Privacy  2017.

[20] Shuang Song  Kamalika Chaudhuri  and Anand D. Sarwate. Stochastic gradient descent with
differentially private updates. In IEEE Global Conference on Signal and Information Processing 
GlobalSIP 2013  Austin  TX  USA  December 3-5  2013  pages 245–248  2013.

[21] Oliver Williams and Frank McSherry. Probabilistic inference and differential privacy.

In
Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural
Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010 
Vancouver  British Columbia  Canada.  pages 2451–2459  2010.

11

,Peter Kairouz
Sewoong Oh
Pramod Viswanath
D. Sculley
Daniel Golovin
Eugene Davydov
Todd Phillips
Vinay Chaudhary
Michael Young
Jean-François Crespo
Dan Dennison
Katrina Ligett
Seth Neel
Aaron Roth
Steven Wu