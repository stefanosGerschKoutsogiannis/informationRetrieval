2019,Average Individual Fairness: Algorithms  Generalization and Experiments,We propose a new family of fairness definitions for classification problems that combine some of the best properties of both statistical and individual notions of fairness. We posit not only a distribution over individuals  but also a distribution over (or collection of) classification tasks. We then ask that standard statistics (such as error or false positive/negative rates) be (approximately) equalized across individuals  where the rate is defined as an expectation over the classification tasks. Because we are no longer averaging over coarse groups (such as race or gender)  this is a semantically meaningful individual-level constraint. Given a sample of individuals and problems  we design an oracle-efficient algorithm (i.e. one that is given access to any standard  fairness-free learning heuristic) for the fair empirical risk minimization task. We also show that given sufficiently many samples  the ERM solution generalizes in two directions: both to new individuals  and to new classification tasks  drawn from their corresponding distributions. Finally we implement our algorithm and empirically verify its effectiveness.,Average Individual Fairness:

Algorithms  Generalization and Experiments

Michael Kearns

University of Pennsylvania
mkearns@cis.upenn.edu

Aaron Roth

University of Pennsylvania
aaroth@cis.upenn.edu

Saeed Shariﬁ-Malvajerdi
University of Pennsylvania

saeedsh@wharton.upenn.edu

Abstract

We propose a new family of fairness deﬁnitions for classiﬁcation problems that
combine some of the best properties of both statistical and individual notions of
fairness. We posit not only a distribution over individuals  but also a distribution
over (or collection of) classiﬁcation tasks. We then ask that standard statistics
(such as error or false positive/negative rates) be (approximately) equalized across
individuals  where the rate is deﬁned as an expectation over the classiﬁcation tasks.
Because we are no longer averaging over coarse groups (such as race or gender) 
this is a semantically meaningful individual-level constraint. Given a sample of
individuals and problems  we design an oracle-efﬁcient algorithm (i.e. one that is
given access to any standard  fairness-free learning heuristic) for the fair empirical
risk minimization task. We also show that given sufﬁciently many samples  the
ERM solution generalizes in two directions: both to new individuals  and to new
classiﬁcation tasks  drawn from their corresponding distributions. Finally we
implement our algorithm and empirically verify its effectiveness.

1

Introduction

The community studying fairness in machine learning has yet to settle on deﬁnitions. At a high level 
existing deﬁnitional proposals can be divided into two groups: statistical fairness deﬁnitions and
individual fairness deﬁnitions. Statistical fairness deﬁnitions partition individuals into “protected
groups” (often based on race  gender  or some other binary protected attribute) and ask that some
statistic of a classiﬁer (error rate  false positive rate  positive classiﬁcation rate  etc.) be approximately
equalized across those groups. In contrast  individual deﬁnitions of fairness have no notion of
“protected groups”  and instead ask for constraints that bind on pairs of individuals. These constraints
can have the semantics that “similar individuals should be treated similarly” (Dwork et al. (2012))  or
that “less qualiﬁed individuals should not be preferentially favored over more qualiﬁed individuals”
(Joseph et al. (2016)). Both families of deﬁnitions have serious problems  which we will elaborate on.
But in summary  statistical deﬁnitions of fairness provide only very weak promises to individuals 
and so do not have very strong semantics. Existing proposals for individual fairness guarantees  on
the other hand  have very strong semantics  but have major obstacles to deployment  requiring strong
assumptions on either the data generating process or on society’s ability to instantiate an agreed-upon
fairness metric.
Statistical deﬁnitions of fairness are the most popular in the literature  in large part because they can
be easily checked and enforced on arbitrary data distributions. For example  a popular deﬁnition
(Hardt et al. (2016); Kleinberg et al. (2017); Chouldechova (2017)) asks that a classiﬁer’s false
positive rate should be equalized across the protected groups. This can sound attractive: in settings
in which a positive classiﬁcation leads to a bad outcome (e.g. incarceration)  it is the false positives
that are harmed by the errors of the classiﬁer  and asking that the false positive rate be equalized
across groups is asking that the harm caused by the algorithm should be proportionately spread
across protected populations. But the meaning of this guarantee to an individual is limited  because

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the word rate refers to an average over the population. To see why this limits the meaning of the
guarantee  consider the example given in Kearns et al. (2018): imagine a society that is equally split
between gender (Male  Female) and race (Blue  Green). Under the constraint that false positive
rates be equalized across both race and gender  a classiﬁer may incarcerate 100% of blue men and
green women  and 0% of green men and blue women. This equalizes the false positive rate across all
protected groups  but is cold comfort to any individual blue man and green woman. This effect isn’t
merely hypothetical — Kearns et al. (2018  2019) showed similar effects when using off-the-shelf
fairness constrained learning techniques on real datasets.
Individual deﬁnitions of fairness  on the other hand  can have strong individual level semantics. For
example  the constraint imposed by Joseph et al. (2016  2018) in online classiﬁcation problems
implies that the false positive rate must be equalized across all pairs of individuals who (truly) have
negative labels. Here the word rate has been redeﬁned to refer to an expectation over the randomness
of the classiﬁer  and there is no notion of protected groups. This kind of constraint provides a strong
individual level promise that one’s risk of being harmed by the errors of the classiﬁer are no higher
than they are for anyone else. Unfortunately  in order to non-trivially satisfy a constraint like this  it
is necessary to make strong realizability assumptions.

1.1 Our results

We propose an alternative deﬁnition of individual fairness that avoids the need to make assumptions
on the data generating process  while giving the learning algorithm more ﬂexibility to satisfy it in
non-trivial ways. We consider that in many applications each individual will be subject to decisions
made by many classiﬁcation tasks over a given period of time  not just one. For example  internet
users are shown a large number of targeted ads over the course of their usage of a platform  not
just one: the properties of the advertisers operating in the platform over a period of time are not
known up front  but have some statistical regularities. Public school admissions in cities like New
York are handled by a centralized match: students apply not just to one school  but to many  who
can each make their own admissions decisions (Abdulkadiro˘glu et al. (2005)). We model this by
imagining that not only is there an unknown distribution P over individuals  but there is an unknown
distribution Q over classiﬁcation problems (each of which is represented by an unknown mapping
from individual features to target labels). With this model in hand  we can now ask that the error rates
(or false positive or negative rates) be equalized across all individuals — where now rate is deﬁned as
the average over classiﬁcation tasks drawn from Q of the probability of a particular individual being
incorrectly classiﬁed.
We then derive a new oracle-efﬁcient algorithm for satisfying this guarantee in-sample  and prove
novel generalization guarantees showing that the guarantees of our algorithm hold also out of sample.
Oracle efﬁciency is an attractive framework in which to circumvent the worst-case hardness of even
unconstrained learning problems  and focus on the additional computational difﬁculty imposed by
fairness constraints. It assumes the existence of “oracles” that can solve weighted classiﬁcation
problems absent fairness constraints  and asks for efﬁcient reductions from the fairness constrained
learning problems to unconstrained problems. This has become a popular technique in the fair
machine learning literature (see e.g. Agarwal et al. (2018); Kearns et al. (2018)) — and one that
often leads to practical algorithms. The generalization guarantees we prove require the development
of new techniques because they refer to generalization in two orthogonal directions — over both
individuals and classiﬁcation problems. Our algorithm is run on a sample of n individuals sampled
from P and m problems sampled from Q. It is given access to an oracle (in practice  implemented
with a heuristic) for solving ordinary cost sensitive classiﬁcation problems over some hypothesis
space H. The algorithm runs in polynomial time (it performs only elementary calculations except for
calls to the learning oracle  and makes only a polynomial number of calls to the oracle) and returns
a mapping from problems to hypotheses that have the following properties  so long as n and m are
sufﬁciently large (polynomial in the VC-dimension of H and the desired error parameters): For any
α  with high probability over the draw of the n individuals from P and the m problems from Q

1. Accuracy: the error rate (computed in expectation over new individuals x ∼ P and new
problems f ∼ Q) is within O(α) of the optimal mapping from problems to classiﬁers in H 
subject to the constraint that for every pair of individuals x  x(cid:48) in the support of P  the error
rates (or false positive or negative rates) (computed in expectation over problems f ∼ Q) on
x and x(cid:48) differ by at most α.

2

individual
problem

space
X
F

element
x ∈ X
f ∈ F

P
Q

data set
X = {xi}n
F = {fj}m

i=1

j=1

Table 1: Summary of notations for individuals vs. problems
sample size

distribution

empirical dist.

(cid:98)P = U (X)
(cid:98)Q = U (F )

n

m

2. Fairness: with probability 1 − β over the draw of new individuals x  x(cid:48) ∼ P  the error rate
(or false positive or negatives rates) of the output mapping (computed in expectation over
problems f ∼ Q) on x will be within O(α) of that of x(cid:48).

The mapping from new classiﬁcation problems to hypotheses that we ﬁnd is derived from the dual
variables of the linear program representing our empirical risk minimization task  and we crucially
rely on the structure of this mapping to prove our generalization guarantees for new problems f ∼ Q.

1.2 Additional related work

The literature on fairness in machine learning has become much too large to comprehensively
summarize  but see Mitchell et al. (2018) for a recent survey. Here we focus on the most conceptually
related work  which has aimed to bridge the gap between the immediate applicability of statistical
deﬁnitions of fairness with the strong individual level semantics of individual notions of fairness.
One strand of this literature focuses on the “metric fairness” deﬁnition ﬁrst proposed by Dwork et al.
(2012)  and aims to ease the assumption that the learning algorithm has access to a task speciﬁc
fairness metric. Kim et al. (2018a) imagine access to an oracle which can provide unbiased estimates
to the metric distance between any pair of individuals  and show how to use this to satisfy a statistical
notion of fairness representing “average metric fairness” over pre-deﬁned groups. Gillen et al. (2018)
study a contextual bandit learning setting in which a human judge points out metric fairness violations
whenever they occur  and show that with this kind of feedback (under assumptions about consistency
with a family of metrics)  it is possible to quickly converge to the optimal fair policy. Yona and
Rothblum (2018) consider a PAC-based relaxation of metric fair learning  and show that empirical
metric-fairness generalizes to out-of-sample metric fairness. Another strand of this literature has
focused on mitigating the problems that arise when statistical notions of fairness are imposed over
coarsely deﬁned groups  by instead asking for statistical notions of fairness over exponentially many
or inﬁnitely many groups with a well deﬁned structure. This line includes Hébert-Johnson et al.
(2018) (focusing on calibration)  Kearns et al. (2018) (focusing on false positive and negative rates) 
and Kim et al. (2018b) (focusing on error rates).

2 Model and preliminaries
We model each individual in our framework by a vector of features x ∈ X   and we let each learning
problem 1 be represented by a binary function f ∈ F mapping X to {0  1}. We assume probability
measures P and Q over X and F  respectively. In the training phase there is a ﬁxed (across problems)
i=1 of n individuals sampled independently from P for which we have available labels
set X = {xi}n
j=1 drawn independently from Q 2. Therefore  a
corresponding to m tasks represented by F = {fj}m
training data set of n individuals X and m learning tasks F takes the form: S = {xi  (fj(xi))m
j=1}n
i=1.
We summarize the notations we use for individuals and problems in Table 1.
In general F will be unknown. We will aim to solve the (agnostic) learning problem over a hypothesis
class H  which need bear no relationship to F. We will allow for randomized classiﬁers  which we
model as learning over ∆(H)  the probability simplex over H. We assume throughout that H contains
the constant classiﬁers h0 and h1 where h0(x) = 0 and h1(x) = 1 for all x. Unlike usual learning
settings where the primary goal is to learn a single hypothesis p ∈ ∆(H)  our objective is to learn a
mapping ψ ∈ ∆(H)F that maps learning tasks f ∈ F represented as new labellings of the training
data to hypotheses p ∈ ∆(H). We will therefore have to formally deﬁne the error rates incurred
by a mapping ψ and use them to formalize a learning task subject to our proposed fairness notion.
For a mapping ψ  we write ψf to denote the classiﬁer corresponding to f under the mapping  i.e. 

1We will use the terms: problem  task  and labeling interchangeably.
2Throughout we will use subscript i to denote individuals and j to denote learning problems.

3

ψf = ψ (f ) ∈ ∆(H). Notice in the training phase  there are only m learning problems to be solved 
and therefore  the corresponding empirical problem reduces to learning m randomized classiﬁers.
In general  learning m speciﬁc classiﬁers for the training problems will not yield any generalizable
rule mapping new problems to classiﬁers — but the speciﬁc algorithm we propose for empirical risk
minimization will induce such a mapping  via a dual representation of the empirical risk minimizer.
Deﬁnition 2.1 (Individual and Overall Error Rates). For a mapping ψ ∈ ∆(H)F and distributions
ψ and err (ψ;P Q) = E

P and Q: E (x  ψ;Q) = Ef∼Q(cid:2)Ph∼ψf [h(x) (cid:54)= f (x)](cid:3) is the individual error rate of x incurred by

x∼P [E (x  ψ;Q)] is the overall error rate of ψ.

In the body of this paper  we will focus on a fairness constraint that asks that the individual error
rate should be approximately equalized across all individuals. In the supplement  we extend our
techniques to equalizing false positive and negative rates across individuals.
Deﬁnition 2.2 (Average Individual Fairness (AIF)). We say a mapping ψ ∈ ∆(H)F satisﬁes “(α  β)-
AIF” (reads (α  β)-approximate Average Individual Fairness) with respect to the distributions (P Q)
if there exists γ ≥ 0 such that: Px∼P (|E (x  ψ;Q) − γ| > α) ≤ β.
We brieﬂy ﬁx some notation: 1 [A] represents the indicator function of event A. For n ∈ N 
[n] = {1  2  . . .   n}. U (S) represents the uniform distribution over S. For a mapping ψ : A → B
and A(cid:48) ⊆ A  ψ|A(cid:48) represents ψ restricted to the domain A(cid:48). dH denotes the VC dimension of the
class H. CSC(H) denotes a cost sensitive classiﬁcation oracle for H:
Deﬁnition 2.3 (Cost Sensitive Classiﬁcation (CSC) in H). Let D = {xi  c1
i}n
i=1 denote a
i are the costs of classifying xi as positive (1) and
data set of n individuals xi where c1
negative (0) respectively. Given D  the cost sensitive classiﬁcation problem deﬁned over H is
takes D = {xi  c1
i}n
i=1 as input and outputs the solution to the optimization problem. We use
CSC(H; D) to denote the classiﬁer returned by CSC(H) on data set D. We say that an algorithm
is oracle efﬁcient if it runs in polynomial time given the ability to make unit-time calls to CSC(H).

the optimization problem: arg minh∈H(cid:80)n

i h(xi) + c0

i   c0

i (1 − h(xi))(cid:9). An oracle CSC(H)

i   c0

i and c0

(cid:8)c1

i=1

3 Learning subject to AIF

In this section we ﬁrst cast the learning problem subject to the AIF fairness constraints as the
constrained optimization problem (1) and then develop an oracle efﬁcient algorithm for solving its
corresponding empirical risk minimization (ERM) problem (in the spirit of Agarwal et al. (2018)).
In the coming sections we give a full analysis of the developed algorithm including its in-sample
accuracy/fairness guarantees and deﬁne the mapping it induces from new problems to hypotheses 
and ﬁnally establish out-of-sample bounds for this trained mapping.

Fair Learning Problem subject to (α  0)-AIF

min

ψ ∈ ∆(H)F   γ ∈ [0 1]
s.t. ∀x ∈ X :

err (ψ;P Q)
|E (x  ψ;Q) − γ| ≤ α

(1)

Deﬁnition 3.1 (OPT). Consider the optimization problem (1). Given distributions P and Q  and
fairness approximation parameter α  we denote the optimal solutions of (1) by ψ(cid:63) (α;P Q) and
γ(cid:63) (α;P Q)  and the value of the objective function at these optimal points by OPT (α;P Q).
We will use OPT as the benchmark with respect to which we evaluate the accuracy of our trained
mapping. It is worth noticing that the optimization problem (1) has a nonempty set of feasible
solutions for every α and all distributions P and Q because the following point is always feasible:
γ = 0.5 and ψf = 0.5h0 + 0.5h1 (i.e. random classiﬁcation) for all f ∈ F where h0 and h1 are
all-zero and all-one constant classiﬁers.

3.1 The empirical fair learning problem

We start to develop our algorithm by deﬁning the empirical version of (1) for a given training data
set of n individuals X = {xi}n
j=1. We will formulate the

i=1 and m learning problems F = {fj}m

4

empirical problem as ﬁnding a restricted mapping ψ|F by which we mean the domain of the mapping
is restricted to the training set F ⊆ F. We will later see how the dynamics of our proposed algorithm
allows us to extend the restricted mapping to a mapping from the entire space F. We slightly change
notation and represent a restricted mapping ψ|F explicitly by a vector p = (p1  . . .   pm) ∈ ∆(H)m
of randomized classiﬁers where pj ∈ ∆(H) corresponds to fj ∈ F . Using the empirical versions of
the individual and the overall error rates incurred by the mapping p (see Deﬁnition 2.1)  we cast the
empirical fair learning problem as the constrained optimization problem (2).

Empirical Fair Learning Problem

min

p ∈ ∆(H)m  γ ∈ [0 1]
s.t. ∀i ∈ {1  . . .   n}:

p;(cid:98)P  (cid:98)Q(cid:17)
(cid:16)
(cid:12)(cid:12)(cid:12)E(cid:16)
xi  p; (cid:98)Q(cid:17) − γ

err

(cid:12)(cid:12)(cid:12) ≤ 2α

(2)

r

(cid:16)

p  γ; (cid:98)Q(cid:17)

We use the dual perspective of constrained optimization to reduce the fair learning task (2) to a
two-player game between a “Learner” (primal player) and an “Auditor” (dual player). Towards

deriving the Lagrangian of (2)  we ﬁrst rewrite its constraints in r (p  γ; (cid:98)Q) ≤ 0 form where

E(cid:16)
xi  p; (cid:98)Q(cid:17) − γ − 2α
γ − E(cid:16)
xi  p; (cid:98)Q(cid:17) − 2α
(cid:3)
variables for r be represented by λ =(cid:2)λ+
i ∈ Λ  where Λ = {λ ∈ R2n
(2) is L (p  γ  λ) = err (p;(cid:98)P  (cid:98)Q) + λT r (p  γ; (cid:98)Q). We therefore consider solving:

represents the “fairness violations” of the pair (p  γ) in one single vector. Let the corresponding dual
+ |||λ||1 ≤ B}. Note
we place an upper bound B on the (cid:96)1-norm of λ in order to reason about the convergence of our
proposed algorithm. B will eventually factor into both the run-time and the approximation guarantees
of our solution. Using Equation (3) and the introduced dual variables  we have that the Lagrangian of

n

∈ R2n

i   λ

(3)

−
i

i=1

=

L (p  γ  λ)

L (p  γ  λ) = max
λ∈Λ

min

min

max
λ∈Λ

p ∈ ∆(H)m  γ ∈ [0 1]

p ∈ ∆(H)m  γ ∈ [0 1]

(4)
where strong duality holds because L is linear in its arguments and the domains of (p  γ) and λ are
convex and compact (Sion (1958)). From a game theoretic perspective  the solution to this minmax
problem can be seen as an equilibrium of a zero-sum game between two players. The primal player
(Learner) has strategy space ∆(H)m × [0  1] while the dual player (Auditor) has strategy space Λ 
and given a pair of chosen strategies (p  γ  λ)  the Lagrangian L (p  γ  λ) represents how much
the Learner has to pay to the Auditor — i.e. it deﬁnes the payoff function of a zero sum game in
which the Learner is the minimization player  and the Auditor is the maximization player. Using
no regret dynamics  an approximate equilibrium of this zero-sum game can be found in an iterative
framework. In each iteration  we let the dual player run the exponentiated gradient descent algorithm
and the primal player best respond. The best response problem of the Learner can be decoupled
into (m + 1) separate minimization problems and that in particular  the optimal classiﬁers p can
be viewed as the solutions to m weighted classiﬁcation problems in H where all m problems share
i ]i ∈ Rn over the training individuals. We write the best response of the
the weights w = [λ+
Learner in Subroutine 1 where we use the oracle CSC(H) (see Deﬁnition 2.3) to solve the weighted
classiﬁcation problems. See the supplementary ﬁle for the detailed derivation.

i − λ−

Subroutine 1: BEST– best response of the Learner in the AIF setting
Input: dual weights w = [λ+
for j = 1  . . .   m do

i=1 ∈ Rn  training examples S =(cid:8)xi  (fj(xi))m

i − λ−
i ]n

j=1

(cid:9)n

i=1

i ← (wi + 1/n)fj(xi) for i ∈ [n].
i   c0

i}n
i=1.

i ← (wi + 1/n)(1 − fj(xi)) and c0
c1
hj ← CSC (H; D) where D = {xi  c1
(cid:16)

h = (h1  h2  . . .   hm)   γ = 1 [(cid:80)n

end
Output:

(cid:17)

i=1 wi > 0]

5

3.2 Algorithm implementation and in-sample guarantees

an average over T classiﬁers where classiﬁer t is the solution to a CSC problem on X weighted

In Algorithm 2 (AIF-Learn)  with a slight deviation from what we described in the previous sub-
section  we implement the proposed algorithm. The deviation arises when the Auditor updates the
dual variables λ in each round  and is introduced in the service of arguing for generalization. To
counteract the inherent adaptivity of the algorithm (which makes the quantities estimated at each
round data dependent)  at each round t of the algorithm  we draw a fresh batch of m0 problems. From
another viewpoint – which is the way the algorithm is actually implemented – similar to usual batch
learning models we assume we have a training set F of m learning problems upfront. However  in
our proposed algorithm that runs for T iterations  we partition F into T equally-sized (m0) subsets
{Ft}T
t=1 uniformly at random and use only the batch Ft at round t to update λ. Without loss of
This is represented in Algorithm 2 by writing (cid:98)Qt = U (Ft) for the uniform distribution over the batch
generality and to avoid technical complications  we assume |Ft| = m0 = m/T is a natural number.
of problems Ft   and ht|Ft for the associated classiﬁers for Ft.
Notice AIF-Learn takes as input an approximation parameter ν ∈ [0  1] which will quantify how
close the output of the algorithm is to an equilibrium of the introduced game  and it will accordingly
vector wt ∈ Rn over the training individuals X and that each(cid:98)pj learned by our algorithm is in fact
propagate to the accuracy bounds. One important aspect of AIF-Learn is that it maintains a weight
by wt. As a consequence  we propose to extend the learned restricted mapping(cid:98)p to a mapping
(cid:98)ψ = (cid:98)ψ (X (cid:99)W ) that takes any problem f ∈ F as input (represented to (cid:98)ψ by the labels it induces on
the training data)  uses the individuals X along with the set of weights(cid:99)W to solve T CSC problems
in a similar fashion  and outputs the average of the learned classiﬁers denoted by (cid:98)ψf ∈ ∆(H). This
extension is consistent with(cid:98)p in the sense that (cid:98)ψ restricted to F will be exactly the(cid:98)p output by our
algorithm. The pseudocode for (cid:98)ψ (output by AIF-Learn) is written in detail in Mapping 3.
T   S ←(cid:8)xi  (fj(xi))j

α   T ← 16B2(1+2α)2 log(2n+1)
t=1 where |Ft| = m0.

Algorithm 2: AIF-Learn – learning subject to AIF
Input: fairness parameter α  approximation parameter ν  data X = {xi}n
B ← 1+2ν
Partition F : {Ft}T
θ1 ← 0
for t = 1  . . .   T do
1+(cid:80)
i t ← B
λ•
wt ← [λ+
i t − λ−
(ht  γt) ← BEST(wt; S)
θt+1 ← θt + η · r

for i ∈ [n] and • ∈ {+ −}

i=1 and F = {fj}m

  m0 ← m

  η ←

4(1+2α)2B

(cid:17)

(cid:9)

ν2

j=1

i

ν

(cid:80)T

end(cid:98)γ ← 1

T

t=1 γt  

Output: average plays

)

i t)

exp(θ•
i(cid:48)  •(cid:48) exp(θ•(cid:48)
i(cid:48)  t
i=1 ∈ Rn
(cid:16)
i t]n
ht|Ft  γt; (cid:98)Qt
(cid:80)T
(cid:98)p ← 1
(cid:17)
(cid:16)(cid:98)p (cid:98)γ  (cid:98)λ

T

(cid:80)T
(cid:98)λ ← 1
t=1 λt   (cid:99)W ← {wt}T
(cid:16)
  mapping (cid:98)ψ = (cid:98)ψ
X (cid:99)W

(see Mapping 3)

(cid:17)

T

t=1

t=1 ht  

We defer a complete in-sample analysis of Algorithm 2 to the supplementary ﬁle. At a high level  we
start by establishing the regret bound of the Auditor and choosing T and η such that her regret ≤ ν.

regret of the Auditor because she is using a batch of only m0 randomly selected problems to update
the fairness violation vector r. We therefore have to assume m0 is sufﬁciently large to control the

There will be an extra (cid:101)O((cid:112)1/m0) term originating from a high probability (Chernoff) bound in the
regret. Once the regret bound is established  we can show that the average played strategies ((cid:98)p (cid:98)γ  (cid:98)λ)
this guarantee and turn it into accuracy and fairness guarantees of the pair ((cid:98)p (cid:98)γ) with respect to the
empirical distributions ((cid:98)P  (cid:98)Q)  which results in Theorem 3.1.

output by Algorithm 2 forms a ν-approximate equilibrium of the game by which we mean: neither
player would gain more than ν if they deviated from these proposed strategies. Finally we can take

6

Mapping 3: (cid:98)ψ (X (cid:99)W ) – pseudocode

Input: f ∈ F (represented as {f (xi)}n
for t = 1  . . .   T do

i=1)

T

end

i   c0

i ← (wi t + 1/n)(1 − f (xi)) for i ∈ [n].
c1
i ← (wi t + 1/n)f (xi) for i ∈ [n].
c0
i}n
D ← {xi  c1
i=1.
hf wt ← CSC (H; D)
(cid:80)T
Output: (cid:98)ψf = 1
t=1 hf wt ∈ ∆(H)
Theorem 3.1 (In-sample Accuracy and Fairness). Suppose m0 ≥ O (log (nT /δ)/α2ν2). Let ((cid:98)p (cid:98)γ)
probability 1 − δ  err ((cid:98)p;(cid:98)P  (cid:98)Q) ≤ err (p;(cid:98)P  (cid:98)Q) + 2ν  and that(cid:98)p satisﬁes (3α  0)-AIF with respect
to the empirical distributions ((cid:98)P  (cid:98)Q). In other words  for all i ∈ [n]  |E(xi (cid:98)p; (cid:98)Q) −(cid:98)γ| ≤ 3α.

be the output of Algorithm 2 and let (p  γ) be any feasible pair of variables for (2). We have that with

Figure 1: Illustration of generalization directions.

3.3 Generalization theorems

simultaniously  given that both n and m are large enough. We will use OPT (see Deﬁnition 3.1) as a

will remain accurate and fair with respect to Q. We will eventually put these pieces together in

When it comes to out-of-sample performance in our framework  unlike in usual learning settings 
there are two distributions we need to reason about: the individual distribution P and the problem
distribution Q (see Figure 1 for a visual illustration of generalization directions in our framework).
almost every individual x ∼ P  where fairness is deﬁned with respect to the true problem distribution
Q. Given these two directions for generalization  we state our generalization guarantees in three
steps visualized by arrows in Figure 1. First  in Theorem 3.2  we ﬁx the empirical distribution of
underlying individual distribution P as long as n is sufﬁciently large. Second  in Theorem 3.3  we ﬁx

We need to argue that (cid:98)ψ induces a mapping that is accurate with respect to P and Q  and is fair for
the problems (cid:98)Q and show that the output (cid:98)ψ of Algorithm 2 is accurate and fair with respect to the
the empirical distribution of individuals (cid:98)P and consider generalization along the underlying problem
generating distribution Q. It will follow from the dynamics of the algorithm that the mapping (cid:98)ψ
Theorem 3.4 and argue that (cid:98)ψ is accurate and fair with respect to the underlying distributions (P Q)
benchmark for the accuracy of the mapping (cid:98)ψ. See the supplementary ﬁle for detailed proofs.
Theorem 3.2 (Generalization over P). Let 0 < δ < 1. Let (cid:98)ψ and(cid:98)γ be the outputs of Algorithm 2 and
suppose n ≥ (cid:101)O(cid:0)(m dH + log (1/ν2δ))/α2β2(cid:1). We have that with probability 1− 5δ  the mapping (cid:98)ψ
satisﬁes (5α  β)-AIF with respect to the distributions (P  (cid:98)Q)  i.e.  Px ∼P (|E(x (cid:98)ψ; (cid:98)Q)−(cid:98)γ| > 5α) ≤ β
and that err ((cid:98)ψ;P  (cid:98)Q) ≤ OPT (α;P  (cid:98)Q) + O (ν) + O (αβ) .
Theorem 3.3 (Generalization over Q). Let 0 < δ < 1. Let (cid:98)ψ and(cid:98)γ be the outputs of Algorithm 2 and
suppose m ≥ (cid:101)O(cid:0) log (n) log (n/δ) /ν4α4(cid:1). We have that with probability 1 − 6δ  the mapping (cid:98)ψ
x ∼(cid:98)P (|E(x (cid:98)ψ;Q)−(cid:98)γ| > 4α) = 0
satisﬁes (4α  0)-AIF with respect to the distributions ((cid:98)P Q)  i.e.  P
and that err ((cid:98)ψ;(cid:98)P Q) ≤ OPT (α;(cid:98)P Q) + O (ν).
Theorem 3.4 (Simultaneous Generalization over P and Q). Let 0 < δ < 1. Let (cid:98)ψ and (cid:98)γ
be the outputs of Algorithm 2 and suppose n ≥ (cid:101)O(cid:0)(m dH + log (1/ν2δ))/α2β2(cid:1) and m ≥
(cid:101)O(cid:0) log (n) log (n/δ) /ν4α4(cid:1). We have that with probability 1−12δ  the mapping(cid:98)ψ satisﬁes (6α  2β)-
AIF with respect to the distributions (P Q)  i.e.  Px ∼P (|E(x (cid:98)ψ;Q) −(cid:98)γ| > 6α) ≤ 2β and that
err ((cid:98)ψ;P Q) ≤ OPT (α;P Q) + O (ν) + O (αβ).

Note that the bounds on n and m in Theorem 3.4 are mutually dependent: n must be linear in m  but
m need only be logarithmic in n  and so both bounds can be simultaneously satisﬁed with sample
complexity that is only polynomial in the parameters of the problem.

7

4 Experimental evaluation

Figure 2: (a) Error-unfairness trajectory plots illustrating the convergence of algorithm AIF-Learn.
(b) In-sample error-unfairness tradeoffs and individual errors for AIF-Learn vs. the baseline model:
simple mixtures of the error-optimal model and random classiﬁcation. Gray dots are shifted upwards
slightly to avoid occlusions.

We have implemented the AIF-Learn algorithm and conclude with a brief experimental demonstra-
tion of its practical efﬁcacy using the Communities and Crime dataset3  which contains U.S. census
records with demographic information at the neighborhood level. To obtain a challenging instance of
our multi-problem framework  we treated each of the ﬁrst n = 200 neighborhoods as the “individuals”
in our sample  and binarized versions of the ﬁrst m = 50 variables as distinct prediction problems.
Another d = 20 of the variables were used as features for learning. For the base learning oracle
assumed by AIF-Learn  we used a linear threshold learning heuristic that has worked well in other
oracle-efﬁcient reductions (Kearns et al. (2018)).
Despite the absence of worst-case guarantees for the linear threshold heuristic  AIF-Learn seems
to empirically enjoy the strong convergence properties suggested by the theory. In Figure 2(a) we
show trajectory plots of the learned model’s error (x axis) versus its fairness violation (variation in
cross-problem individual error rates  y axis) over 1000 iterations of the algorithm for varying values
of the allowed fairness violation 2α (dashed horizontal lines). In each case we see the trajectory
eventually converge to a point which saturates the fairness constraint with the optimal error.
In Figure 2(b) we provide a more detailed view of the behavior and performance of AIF-Learn.
The x axis measures error rates  while the y axis measures the allowed fairness violation. For each
value of the allowed fairness violation 2α (which is the allowed gap between the smallest and largest
individual errors on input α)  there is a horizontal row of 200 blue dots showing the error rates for
each individual  and a single red dot representing the overall average of those individual error rates.
As expected  for large α (weak or no fairness constraint)  the overall error rate is lowest  but the
spread of individual error rates (unfairness) is greatest. As α is decreased  the spread of individual
error rates is greatly narrowed  at a cost of greater overall error.

A trivial way of achieving zero variability in individual error rates is to make all predictions randomly.
So as a baseline comparison for AIF-Learn  the gray dots in Figure 2(b) show the individual
error rates achieved by different mixtures of the unconstrained error-optimal model with random
classiﬁcations  with a black dot representing the overall average of these rates. When the weight on
random classiﬁcation is low (weak or no fairness  top row of gray dots)  the overall error is lowest and
the individual variation (unfairness) is highest. As we increase the weight on random classiﬁcation 
variation or unfairness decreases and the overall error gets worse. It is clear from the ﬁgure that
AIF-Learn is considerably outperforming this baseline  both in terms of the average errors (red vs.
black lines) and the individual errors (blue vs. gray dots).

3Described in detail and available for download at http://archive.ics.uci.edu/ml/datasets/

communities+and+crime

8

Figure 3: Pareto frontier of error and fairness violation rates on training and test data sets.

Finally we present out-of-sample performance of AIF-Learn in Figure 3. To be consistent with
in-sample results reported in Figure 2(b)  for each value of α  we trained a mapping on exactly the
same subset of the Communities and Crime data set (n = 200 individuals  m = 50 problems) that
we used before. Thus the red curve labelled “training” in Figure 3 is the same as the red curve
appearing in Figure 2(b). We used a completely fresh holdout consisting of n = 200 individuals and
m = 25 problems (binarized features from the dataset that weren’t previously used) to evaluate our
generalization performance over both individuals and problems  in terms of both accuracy and fairness
violation. Similar to the presentation of generalization theorems in Section 3.3  we demonstrate
experimental evaluation of generalization in three steps. The blue and green curves in Figure 3
represent generalization results over individuals (test data: test individuals and training problems) and
problems (test data: training individuals and test problems) respectively. The black curve represent
generalization across both individuals and problems where test individuals and test problems were
used to evaluate the performance of the trained models.
Two things stand out from Figure 3:

1. As predicted by the theory  our test curves track our training curves  but with higher error
and unfairness. In particular  the ordering of the models (each corresponds to one α) on
the Pareto frontier is the same in testing as in training  meaning that the training curve can
indeed be used to manage the trade-off out-of-sample as well.

2. The gap in error is substantially smaller than would be predicted by our theory: since our
training data set is so small  our theoretical guarantees are vacuous  but all points plotted in
our test Pareto curves are non-trivial in terms of both accuracy and fairness. Presumably the
gap in error would narrow on larger training data sets.

We have additional experimental results on a synthetic data set in the supplement.

References
Abdulkadiro˘glu  A.  Pathak  P. A.  and Roth  A. E. (2005). The new york city high school match.

American Economic Review  95(2):364–367.

Agarwal  A.  Beygelzimer  A.  Dudik  M.  Langford  J.  and Wallach  H. (2018). A reductions ap-
proach to fair classiﬁcation. In Dy  J. and Krause  A.  editors  Proceedings of the 35th International
Conference on Machine Learning  volume 80 of Proceedings of Machine Learning Research  pages
60–69  Stockholmsmässan  Stockholm Sweden. PMLR.

9

Chouldechova  A. (2017). Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. Big data  5(2):153–163.

Dwork  C.  Hardt  M.  Pitassi  T.  Reingold  O.  and Zemel  R. (2012). Fairness through awareness.
In Proceedings of the 3rd innovations in theoretical computer science conference  pages 214–226.
ACM.

Gillen  S.  Jung  C.  Kearns  M.  and Roth  A. (2018). Online learning with an unknown fairness

metric. In Advances in Neural Information Processing Systems  pages 2600–2609.

Hardt  M.  Price  E.  Srebro  N.  et al. (2016). Equality of opportunity in supervised learning. In

Advances in neural information processing systems  pages 3315–3323.

Hébert-Johnson  Ú.  Kim  M. P.  Reingold  O.  and Rothblum  G. N. (2018). Multicalibration: Calibra-
tion for the (computationally-identiﬁable) masses. In Dy  J. G. and Krause  A.  editors  Proceedings
of the 35th International Conference on Machine Learning  ICML 2018  Stockholmsmässan  Stock-
holm  Sweden  July 10-15  2018  volume 80 of Proceedings of Machine Learning Research  pages
1944–1953. PMLR.

Joseph  M.  Kearns  M.  Morgenstern  J.  Neel  S.  and Roth  A. (2018). Meritocratic fairness for
inﬁnite and contextual bandits. In Proceedings of the 2018 AAAI/ACM Conference on AI  Ethics 
and Society  pages 158–163. ACM.

Joseph  M.  Kearns  M.  Morgenstern  J. H.  and Roth  A. (2016). Fairness in learning: Classic and

contextual bandits. In Advances in Neural Information Processing Systems  pages 325–333.

Kearns  M.  Neel  S.  Roth  A.  and Wu  Z. S. (2018). Preventing fairness gerrymandering: Auditing
and learning for subgroup fairness. In International Conference on Machine Learning  pages
2569–2577.

Kearns  M.  Neel  S.  Roth  A.  and Wu  Z. S. (2019). An empirical study of rich subgroup fairness for
machine learning. In Proceedings of the Conference on Fairness  Accountability  and Transparency 
pages 100–109. ACM.

Kim  M.  Reingold  O.  and Rothblum  G. (2018a). Fairness through computationally-bounded

awareness. In Advances in Neural Information Processing Systems  pages 4842–4852.

Kim  M. P.  Ghorbani  A.  and Zou  J. Y. (2018b). Multiaccuracy: Black-box post-processing for

fairness in classiﬁcation. CoRR  abs/1805.12317.

Kleinberg  J.  Mullainathan  S.  and Raghavan  M. (2017). Inherent trade-offs in the fair determination
of risk scores. In 8th Innovations in Theoretical Computer Science Conference (ITCS 2017) 
volume 67  page 43. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik.

Mitchell  S.  Potash  E.  and Barocas  S. (2018). Prediction-based decisions and fairness: A catalogue

of choices  assumptions  and deﬁnitions. arXiv preprint arXiv:1811.07867.

Sion  M. (1958). On general minimax theorems. Paciﬁc J. Math.  8(1):171 – 176.

Yona  G. and Rothblum  G. N. (2018). Probably approximately metric-fair learning. In Dy  J. G.
and Krause  A.  editors  Proceedings of the 35th International Conference on Machine Learning 
ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  volume 80 of Proceedings
of Machine Learning Research  pages 5666–5674. PMLR.

10

,Saeed Sharifi-Malvajerdi
Michael Kearns
Aaron Roth