2016,Structured Matrix Recovery via the Generalized Dantzig Selector,In recent years  structured matrix recovery problems have gained considerable attention for its real world applications  such as recommender systems and computer vision. Much of the existing work has focused on matrices with low-rank structure  and limited progress has been made on matrices with other types of structure. In this paper we present non-asymptotic analysis for estimation of generally structured matrices via the generalized Dantzig selector based on sub-Gaussian measurements. We show that the estimation error can always be succinctly expressed in terms of a few geometric measures such as Gaussian widths of suitable sets associated with the structure of the underlying true matrix. Further  we derive general bounds on these geometric measures for structures characterized by unitarily invariant norms  a large family covering most matrix norms of practical interest. Examples are provided to illustrate the utility of our theoretical development.,Structured Matrix Recovery via the Generalized

Dantzig Selector

Sheng Chen
Arindam Banerjee
Dept. of Computer Science & Engineering

University of Minnesota  Twin Cities

{shengc banerjee}@cs.umn.edu

Abstract

In recent years  structured matrix recovery problems have gained considerable
attention for its real world applications  such as recommender systems and computer
vision. Much of the existing work has focused on matrices with low-rank structure 
and limited progress has been made on matrices with other types of structure. In
this paper we present non-asymptotic analysis for estimation of generally structured
matrices via the generalized Dantzig selector based on sub-Gaussian measurements.
We show that the estimation error can always be succinctly expressed in terms of a
few geometric measures such as Gaussian widths of suitable sets associated with
the structure of the underlying true matrix. Further  we derive general bounds on
these geometric measures for structures characterized by unitarily invariant norms 
a large family covering most matrix norms of practical interest. Examples are
provided to illustrate the utility of our theoretical development.

1

Introduction

Structured matrix recovery has found a wide spectrum of applications in real world  e.g.  recommender
systems [22]  face recognition [9]  etc. The recovery of an unknown structured matrix Θ∗ ∈ Rd×p
essentially needs to consider two aspects: the measurement model  i.e.  what kind of information
about the unknown matrix is revealed from each measurement  and the structure of the underlying
matrix  e.g.  sparse  low-rank  etc. In the context of structured matrix estimation and recovery  a
widely used measurement model is the linear measurement  i.e.  one has access to n observations
of the form yi = (cid:104)(cid:104)Θ∗  Xi(cid:105)(cid:105) + ωi for Θ∗  where (cid:104)(cid:104)· ·(cid:105)(cid:105) denotes the matrix inner product  i.e. 
(cid:104)(cid:104)A  B(cid:105)(cid:105) = Tr(AT B) for any A  B ∈ Rd×p  and ωi’s are additive noise. In the literature  various
types of measurement matrices Xi has been investigated  for example  Gaussian ensemble where Xi
consists of i.i.d. standard Gaussian entries [11]  rank-one projection model where Xi is randomly
generated with constraint rank(Xi) = 1 [7]. A special case of rank-one projection is the matrix
completion model [8]  in which Xi has a single entry equal to 1 with all the rest set to 0  i.e.  yi
takes the value of one entry from Θ∗ at each measurement. Other measurement models include
row-and-column afﬁne measurement [34]  exponential family matrix completion [21  20]  etc.
Previous work has shown that low-complexity structure of Θ∗  often captured by a small value of
some norm R(·)  can signiﬁcantly beneﬁt its recovery [11  26]. For instance  one of the popular
structures of Θ∗ is low-rank  which can be approximated by a small value of trace norm (i.e.  nuclear
norm) (cid:107) · (cid:107)tr. Under the low-rank assumption of Θ∗  recovery guarantees have been established for
different measurement matrices using convex programs  e.g.  trace-norm regularized least-square
estimator [10  27  26  21] 

n(cid:88)

i=1

min

Θ∈Rd×p

1
2

(yi − (cid:104)(cid:104)Xi  Θ(cid:105)(cid:105))2 + βn(cid:107)Θ∗(cid:107)tr  

(1)

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

(cid:32) n(cid:88)

i=1

(cid:33)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)op

and constrained trace-norm minimization estimators [10  27  11  7  20]  such as
≤ λn  

((cid:104)(cid:104)Xi  Θ(cid:105)(cid:105) − yi) Xi

(cid:107)Θ(cid:107)tr

s.t.

min

Θ∈Rd×p

(2)

where βn  λn are tuning parameters  and (cid:107) · (cid:107)op denotes the operator (spectral) norm. Among the
convex approaches  the exact recovery guarantee of a matrix-form basis-pursuit [14] estimator was
analyzed for the noiseless setting in [27]  under certain matrix-form restricted isometry property
(RIP). In the presence of noise  [10] also used matrix RIP to establish the recovery error bound for
both regularized and constraint estimators  i.e.  (1) and (2). In [7]  a variant of estimator (2) was
proposed and its recovery guarantee was built on a so-called restricted uniform boundedness (RUB)
condition  which is more suitable for the rank-one projection based measurement model. Despite
the fact that the low-rank structure has been well studied  only a few works extend to more general
structures. In [26]  the regularized estimator (1) was generalized by replacing the trace norm with a
decomposable norm R(·) for other structures. [11] extended the estimator in [27] with (cid:107)·(cid:107)tr replaced
by a norm from a broader class called atomic norm  but the consistency of the estimator is only
available when the noise vector is bounded.
In this work  we make two key contributions. First  we present a general framework for estimation of
structured matrices via the generalized Dantzig sector (GDS) [12  6] as follows

ˆΘ = argmin
Θ∈Rd×p

R(Θ) s.t. R∗

((cid:104)(cid:104)Xi  Θ(cid:105)(cid:105) − yi) Xi

≤ λn  

(3)

in which R(·) can be any norm and its dual norm is R∗(·). GDS has been studied in the context of
structured vectors [12]  so (3) can be viewed as a natural generalization to matrices. Note that the
estimator (2) is a special case of the formulation above  as operator norm is dual to trace norm. Our
deterministic analysis of the estimation error (cid:107) ˆΘ − Θ∗(cid:107)F relies on a condition based on a suitable
choice of λn and the restricted strong convexity (RSC) condition [26  3]. By assuming sub-Gaussian
Xi and ωi  we show that these conditions are satisﬁed with high probability  and the recovery error
can be expressed in terms of certain geometric measures of sets associated with Θ∗. Such a geometric
characterization is inspired by related advances in recent years [26  11  3]. One key ingredient in
such characterization is the Gaussian width [18]  which measures the size of sets in Rd×p. Related
advances can be found in [11  12  6]  but they all rely on the Gaussian measurements  to which
classical concentration results [18] are directly applicable. In contrast  our work allows general
sub-Gaussian measurement matrices and noise  by suitably using ideas from generic chaining [30]  a
powerful geometric approach to bounding stochastic processes. Our results can also be extended to
heavy tailed measurement and noise  following recent advances [28]. Recovery guarantees of the
GDS were analyzed for general norms in matrix completion setting [20]  but it is different from our
work since its measurement model is not sub-Gaussian as we consider.
Our second contribution is motivated by the fact that though certain existing analyses end up with
the geometric measures such as Gaussian widths  limited attention has been paid in bounding these
measures in terms of more easily understandable quantities especially for matrix norms. Here our key
novel contribution is deriving general bounds for those geometric measures for the class of unitarily
invariant norms  which are invariant under any unitary transformation  i.e.  for any matrix Θ ∈ Rd×p 
its norm value is equal to that of U ΘV if both U ∈ Rd×d and V ∈ Rp×p are unitary matrices. The
widely-used trace norm  spectral norm and Frobenius norm all belong to this class. A well-known
result is that any unitarily invariant matrix norm is equivalent to some vector norm applied on the
set of singular values [23] (see Lemma 1 for details)  and this equivalence allows us to build on the
techniques developed in [13] for vector norms to derive the bounds of the geometric measures for
unitarily invariant norms. Previously these general bounds were not available in the literature for the
matrix setting  and bounds were only in terms the geometric measures  which can be hard to interpret
or bound in terms of understandable quantities. We illustrate concrete versions of the general bounds
using the trace norm and the recently proposed spectral k-support norm [24].
The rest of the paper is organized as follows: we ﬁrst provide the deterministic analysis in Section 2.
In Section 3  we introduce some probability tools  which are used in the later analysis. In Section 4 
we present the probabilistic analysis for sub-Gaussian measurement matrices and noise  along with
the general bounds of the geometric measures for unitarily invariant norms. Section 5 is dedicated to
the examples for the application of general bounds  and we conclude in Section 6.

2

2 Deterministic Recovery Guarantees
To evaluate the performance of GDS (3)  we focus on the Frobenius-norm error  i.e.  (cid:107) ˆΘ − Θ∗(cid:107)F .
Throughout the paper  w.l.o.g. we assume that d ≤ p. For convenience  we denote the collection of
Xi’s by X = {Xi}n
i=1  and let ω = [ω1  ω2  . . .   ωn]T be the noise vector. In the following theorem 
we provide a deterministic bound for (cid:107) ˆΘ − Θ∗(cid:107)F under some standard assumptions on λn and X.
Theorem 1 Deﬁne the set ER(Θ∗) = cone{ ∆ ∈ Rd×p | R(∆ + Θ∗) ≤ R(Θ∗)} . Assume that

λn ≥ R∗

ωiXi

  and

(cid:104)(cid:104)Xi  ∆(cid:105)(cid:105)2/ (cid:107)∆(cid:107)2

F ≥ α > 0  ∀ ∆ ∈ ER(Θ∗) .

(cid:32) n(cid:88)

(cid:33)

n(cid:88)

(4)

(5)

i=1

i=1

Then the estimation (cid:107) ˆΘ − Θ∗(cid:107)F error satisﬁes

(cid:107) ˆΘ − Θ∗(cid:107)F ≤ 2ΨR(Θ∗)λn

α

 

where ΨR(·) is the restricted compatibility constant deﬁned as ΨR(Θ∗) = sup∆∈ER(Θ∗)
The proof is deferred to the supplement. The convex cone ER(Θ∗) plays a important role in character-
izing the error bound  and its geometry is determined by R(·) and Θ∗. The recovery bound assumes
no knowledge of the norm R(·) and true matrix Θ∗  thus allowing general structures. The second
condition in 4 is often referred to as restricted strong convexity [26]. In this work  we are particularly
interested in R(·) from the class of unitarily invariant matrix norm  which essentially satisﬁes the fol-
lowing property  R(Θ) = R(U ΘV ) for any Θ ∈ Rd×p and unitary matrices U ∈ Rd×d  V ∈ Rp×p.
A useful result for such norms is given in Lemma 1 (see [23  4] for details).

R(∆)
(cid:107)∆(cid:107)F

.

the singular values of a matrix Θ ∈ Rd×p are given by σ =
Lemma 1 Suppose that
[σ1  σ2  . . .   σd]T . A unitarily invariant norm R : Rd×p (cid:55)→ R can be characterized by some symmet-
ric gauge function1 f : Rd (cid:55)→ R as R(Θ) = f (σ)  and its dual norm is given by R∗(Θ) = f∗(σ).

As the sparsity of σ equals the rank of Θ  the class of unitarily invariant matrix norms is useful in
structured low-rank matrix recovery and includes many widely used norms  e.g.  trace norm with
f (·) = (cid:107)·(cid:107)1  Frobenius norm with f (·) = (cid:107)·(cid:107)2  Schatten p-norm with f (·) = (cid:107)·(cid:107)p  Ky Fan k-norm
when f (·) is the (cid:96)1 norm of the largest k elements in magnitude  etc.
Before proceeding with the analysis  we introduce some notations. For the rest of paper  we denote
by σ(Θ) ∈ Rd the vector of singular values (sorted in descending order) of matrix Θ ∈ Rd×p 
and may use the shorthand σ∗ for σ(Θ∗). For any θ ∈ Rd  we deﬁne the corresponding |θ|↓ by
arranging the absolute values of elements of θ in descending order. Given any matrix Θ ∈ Rd×p
and subspace M ⊆ Rd×p  we denote by ΘM the orthogonal projection of Θ onto M. Besides we
let colsp(Θ) (rowsp(Θ)) be the subspace spanned by columns (rows) of Θ. The notation Sdp−1
represents the unit sphere of Rd×p  i.e.  the set {Θ|(cid:107)Θ(cid:107)F = 1}. The unit ball of norm R(·) is denoted
by ΩR = {Θ | R(Θ) ≤ 1}. Throughout the paper  the symbols c  C  c0  C0  etc.  are reserved for
universal constants  which may be different at each occurrence.
In the rest of our analysis  we will frequently use the so-called ordered weighted (cid:96)1 (OWL) norm
for Rd [17]  which is deﬁned as (cid:107)θ(cid:107)w (cid:44) (cid:104)|θ|↓ |w|↓(cid:105)  where w ∈ Rd is a predeﬁned weight vector.
Noting that the OWL norm is a symmetric gauge  we deﬁne the spectral OWL norm for Θ as:
(cid:107)Θ(cid:107)w (cid:44) (cid:107)σ(Θ)(cid:107)w  i.e.  applying the OWL norm on σ(Θ).

3 Background and Preliminaries

The tools for our probabilistic analysis include the notion of Gaussian width [18]  sub-Gaussian
random matrices  and generic chaining [30]. Here we brieﬂy introduce the basic ideas and results for
each of them as needed for our analysis.

1Symmetric gauge function is a norm that is invariant under sign-changes and permutations of the elements.

3

3.1 Gaussian width and sub-Gaussian random matrices
The Gaussian width can be deﬁned for any subset A ⊆ Rd×p as follows [18  19] 

(cid:104)

(cid:104)(cid:104)G  Z(cid:105)(cid:105)(cid:105)

w(A) (cid:44) EG

 

sup
Z∈A

(6)
where G is a random matrix with i.i.d. standard Gaussian entries  i.e.  Gij ∼ N (0  1). The Gaussian
width essentially measures the size of the set A  and some of its properties can be found in [11  1].
≤ κ if |||(cid:104)(cid:104)X  Z(cid:105)(cid:105)|||ψ2
A random matrix X is sub-Gaussian with |||X|||ψ2
≤ κ for any Z ∈ Sdp−1 
where the ψ2 norm for sub-Gaussian random variable x is deﬁned as |||x|||ψ2
2 (E|x|q)
= supq≥1 q− 1
(see [31] for more details of ψ2 norm). One nice property of sub-Gaussian random variable is the
thin tail  i.e.  P(|x| > ) ≤ e · exp(−c2/(cid:107)x(cid:107)2
)  in which c is a constant.

1
q

ψ2

3.2 Generic chaining

Generic chaining is a powerful tool for bounding the supreme of stochastic processes [30]. Suppose
{Zt}t∈T is a centered stochastic process  where each Zt is a centered random variable. We assume
the index set T is endowed with some metric s(· ·).
In order to use generic chaining bound 
the critical condition for {Zt}t∈T to satisfy is that  for any u  v ∈ T   P (|Zu − Zv| ≥ ) ≤ c1 ·

exp(cid:0)−c22/s2(u  v)(cid:1)  where c1 and c2 are constants. Under this condition  we have
(cid:17) ≤ C2 exp(cid:0)−2(cid:1)  

|Zu − Zv| ≥ C1 (γ2(T   s) +  · diam (T   s))

Zt] ≤ c0γ2 (T   s)  

E[sup
t∈T

P(cid:16)

(7)

(8)

sup
u v∈T

where diam (T   s) is the diameter of set T w.r.t. the metric s(· ·). (7) is often referred to as generic
chaining bound (see Theorem 2.2.18 and 2.2.19 in [30])  and (8) is the Theorem 2.2.27 in [30]. The
functional γ2(T   s) essentially measures the geometric size of the set T under the metric s(· ·). To
avoid unnecessary complications  we omit the deﬁnition of γ2(T   s) here (see Chapter 2 of [30] for
an introduction if one is interested)  but provide two of its properties below 

γ2(T   s1) ≤ γ2(T   s2) if s1(u  v) ≤ s2(u  v) ∀ u  v ∈ T  

γ2(T   ηs) = η · γ2(T   s) for any η > 0 .

(9)
(10)

The important aspect of γ2-functional is the following majorizing measure theorem [29  30].

Then γ2(T   s) can be upper bounded by γ2(T   s) ≤ C0E [supt∈T Yt].

Theorem 2 Given any Gaussian process {Yt}t∈T   deﬁne s(u  v) =(cid:112)E|Yu − Yv|2 for u  v ∈ T .
Given Theorem 2  the metric s(U  V ) =(cid:112)E|(cid:104)(cid:104)G  U − V (cid:105)(cid:105)|2 = (cid:107)U − V (cid:107)F . Therefore we have

This theorem is essentially Theorem 2.4.1 in [30]. For our purpose  we simply focus on the Gaussian
process {Y∆ = (cid:104)(cid:104)G  ∆(cid:105)(cid:105)}∆∈A  in which A ⊆ Rd×p and G is a standard Gaussian random matrix.

γ2 (A (cid:107) · (cid:107)F ) ≤ C0E[ sup
∆∈A

(cid:104)(cid:104)G  ∆(cid:105)(cid:105)] = C0w(A)  

(11)

4 Error Bounds with Sub-Gaussian Measurement and Noise

Though the deterministic recovery bound (5) in Section 2 applies to any measurement X and noise
ω as long as the assumptions in (4) are satisﬁed  it is of practical interest to express the bound in
terms of the problem parameters  e.g.  d  p and n  for random X and ω sampled from some general
and widely used family of distributions. For this work  we assume that Xi’s in X are i.i.d. copies of
≤ κ for a constant κ  and the
a zero-mean random vector X  which is sub-Gaussian with |||X|||ψ2
noise ω contains i.i.d. centered random variables with (cid:107)ωi(cid:107)ψ2 ≤ τ for a constant τ. In this section 
we show that each quantity in (5) can be bounded using certain geometric measures associated with
the true matrix Θ∗. Further  we show that for unitarily invariant norms  the geometric measures can
themselves be bounded in terms of d  p  n  and structures associated with Θ∗.

4

4.1 Bounding restricted compatibility constant

Given the deﬁnition of restricted compatibility constant in Theorem 1  it involves no randomness and
purely depends on R(·) and the geometry of ER(Θ∗). Hence we directly work on its upper bound for
unitarily invariant norms. In general  characterizing the error cone ER(Θ∗) is difﬁcult  especially for
non-decomposable R(·). To address the issue  we ﬁrst deﬁne the seminorm below.
Deﬁnition 1 Given two orthogonal subspaces M1 M2 ⊆ Rd×p and two vectors w  z ∈ Rd  the
subspace spectral OWL seminorm for Rd×p is deﬁned as (cid:107)Θ(cid:107)w z (cid:44) (cid:107)ΘM1(cid:107)w + (cid:107)ΘM2(cid:107)z  where
ΘM1 and ΘM2 are the orthogonal projections of Θ onto M1 and M2  respectively.
Next we will construct such a seminorm based on a subgradient θ∗ of the symmetric gauge f
associated with R(·) at σ∗  which can be obtained by solving the so-called polar operator [32]

θ∗ ∈ argmax
x:f∗(x)≤1

(cid:104)x  σ∗(cid:105) .

(12)

max/θ∗

max (θ∗

2  . . .   θ∗

min (if θ∗

r   0  . . .   0]T ∈ Rd  z = [θ∗

Given that σ∗ is sorted  w.l.o.g. we may assume that θ∗ is nonnegative and sorted because (cid:104)σ∗  θ∗(cid:105) ≤
(cid:104)σ∗ |θ∗|↓(cid:105) and f∗(θ∗) = f∗(|θ∗|↓). Also  we denote by θ∗
min) the largest (smallest) element
min = 0  we deﬁne ρ = +∞). Throughout the paper  we
of the θ∗  and deﬁne ρ = θ∗
will frequently use these notations. As shown in the lemma below  a constructed seminorm based on
θ∗ will induce a set E(cid:48) that contains ER(Θ∗) and is considerably easier to work with.
Lemma 2 Assume that rank(Θ∗) = r and its compact SVD is given by Θ∗ = U ΣV T   where
U ∈ Rd×r  Σ ∈ Rr×r and V ∈ Rp×r.
Let θ∗ be any subgradient of f (σ∗)  w =
d  0  . . .   0]T ∈ Rd  U = colsp(U )
[θ∗
1  θ∗
r+2  . . .   θ∗
and V = rowsp(V T )  and deﬁne M1  M2 as M1 = {Θ | colsp(Θ) ⊆ U  rowsp(Θ) ⊆
V}  M2 = {Θ | colsp(Θ) ⊆ U⊥  rowsp(Θ) ⊆ V⊥}  where U⊥  V⊥ are orthogonal comple-
ments of U and V respectively. Then the speciﬁed subspace spectral OWL seminorm (cid:107) · (cid:107)w z satisﬁes
ER(Θ∗) ⊆ E(cid:48) (cid:44) cone{∆ | (cid:107)∆ + Θ∗(cid:107)w z ≤ (cid:107)Θ∗(cid:107)w z}
The proof is given in the supplementary. Base on the superset E(cid:48)  we are able to bound the restricted
compatibility constant for unitarily invariant norms by the following theorem.
Theorem 3 Assume there exist η1 and η2 such that the symmetric gauge f for R(·) satisﬁes f (δ) ≤
max{η1(cid:107)δ(cid:107)1  η2(cid:107)δ(cid:107)2} for any δ ∈ Rd. Then given a rank-r Θ∗  the restricted compatibility constant
ΨR(Θ∗) is upper bounded by

r+1  θ∗

ΨR(Θ∗) ≤ 2Φf (r) + max(cid:8)η2  η1(1 + ρ)

r(cid:9)  

√

(13)
min  and Φf (r) = sup(cid:107)δ(cid:107)0≤r f (δ)/(cid:107)δ(cid:107)2 is called sparse compatibility constant.

where ρ = θ∗

max/θ∗

Remark: The assumption for Theorem 3 might seem cumbersome at the ﬁrst glance  but the different
combinations of η1 and η2 give us more ﬂexibility. In fact  it trivially covers two cases  η2 = 0 along
with f (δ) ≤ η1(cid:107)δ(cid:107)1 for any δ  and the other way around  η1 = 0 along with f (δ) ≤ η2(cid:107)δ(cid:107)2.

4.2 Bounding restricted convexity α

The second condition in (4) is equivalent to(cid:80)n

the following theorem  we express the restricted convexity α in terms of Gaussian width.

i=1(cid:104)(cid:104)Xi  ∆(cid:105)(cid:105)2 ≥ α > 0  ∀ ∆ ∈ ER(Θ∗) ∩ Sdp−1. In

Theorem 4 Assume that Xi’s are i.i.d. copies of a centered isotropic sub-Gaussian random
matrix X with |||X|||ψ2
≤ κ  and let AR(Θ∗) = ER(Θ∗) ∩ Sdp−1. With probability at least
1 − exp(−ζw2(AR(Θ∗)))  the following inequality holds with absolute constant ζ and ξ 

inf
∆∈A

1
n

(cid:104)(cid:104)Xi  ∆(cid:105)(cid:105)2 ≥ 1 − ξκ2 · w(AR(Θ∗))

√

n

.

(14)

n(cid:88)

i=1

5

The proof is essentially an application of generic chaining [30] and the following theorem from [25].
Related line of works can be found in [15  16  5].

Theorem 5 (Theorem D in [25]) There exist absolute constants c1  c2  c3 for which the following
holds. Let (Ω  µ) be a probability space  H be a subset of the unit sphere of L2(µ)  i.e.  H ⊆ SL2 =
{h : |||h|||L2
≤ κ. Then  for any β > 0 and n ≥ 1 satisfying
c1κγ2(H |||·|||ψ2

n  with probability at least 1 − exp(−c2β2n/κ4)  we have

= 1}  and assume suph∈H |||h|||ψ2

) ≤ β

√

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

h2(Xi) − E(cid:2)h2(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ β .

n(cid:88)

i=1

sup
h∈H

(15)

Proof of Theorem 4:
probability space that X is deﬁned on  and construct

For simplicity  we use A as shorthand for AR(Θ∗). Let (Ω  µ) be the

H = {h(·) = (cid:104)(cid:104)·  ∆(cid:105)(cid:105) | ∆ ∈ A} .

≤ κ immediately implies that suph∈H |||h|||ψ2

|||X|||ψ2
≤ κ. As X is isotropic  i.e.  E[(cid:104)(cid:104)X  ∆(cid:105)(cid:105)2] = 1
for any ∆ ∈ A ⊆ Sdp−1  thus H ⊆ SL2 and E[h2] = 1 for any h ∈ H. Given h1 = (cid:104)(cid:104)·  ∆1(cid:105)(cid:105)  h2 =
(cid:104)(cid:104)·  ∆2(cid:105)(cid:105) ∈ H  where ∆1  ∆2 ∈ A  the metric induced by ψ2 norm satisﬁes |||h1 − h2|||ψ2
=
≤ κ(cid:107)∆1 − ∆2(cid:107)F . Using the properties of γ2-functional and the majorizing
|||(cid:104)(cid:104)X  ∆1 − ∆2(cid:105)(cid:105)|||ψ2
measure theorem in Section 3  we have
γ2(H |||·|||ψ2
) ≤ β

) ≤ κγ2(A (cid:107) · (cid:107)F ) ≤ κc4w(A)  
√
where c4 is an absolute constant. Hence  by choosing β = c1c4κ2w(A)/
condition c1κγ2(H |||·|||ψ2
least 1 − exp(−c2c2
4w2(A))  we have suph∈H
n(cid:88)
1c2

i=1 h2(Xi) − 1(cid:12)(cid:12) ≤ β  which implies
(cid:80)n

n  we can guarantee that
n holds for H. Applying Theorem 5 to this H  with probability at

(cid:12)(cid:12) 1

√

n

inf
∆∈A

1
n

i=1

(cid:104)(cid:104)Xi  ∆(cid:105)(cid:105)2 ≥ 1 − β .

1c2

4  ξ = c1c4  we complete the proof.

Letting ζ = c2c2
The bound (14) involves the Gaussian width of set AR(Θ∗)  i.e.  the error cone intersecting with unit
sphere. For unitarily invariant R  the theorem below provides a general way to bound w(AR(Θ∗)).
Theorem 6 Under the setting of Lemma 2  let ρ = θ∗
min and rank(Θ∗) = r. The Gaussian
width w(AR(Θ∗)) satisﬁes

max/θ∗

(cid:110)(cid:112)dp (cid:112)(2ρ2 + 1) (d + p − r) r

(cid:111)

w(AR(Θ∗)) ≤ min

.

(16)

The proof of Theorem 6 is included in the supplementary material  which relies on a few speciﬁc
properties of Gaussian random matrix [1  11].

In view of Theorem 1  we should choose the λn large enough to satisfy the condition in (4). Hence

i=1 ωiXi)  which holds with high probability.

i=1 are i.i.d. copies of a centered isotropic sub-Gaussian
≤ κ  and the noise ω consists of i.i.d. centered entries with
≤ τ. Let ΩR be the unit ball of R(·) and η = sup∆∈ΩR (cid:107)∆(cid:107)F . With probability at least

3η2(cid:1)  the following inequality holds

4.3 Bounding regularization parameter λn

we an upper bound for random quantity R∗ ((cid:80)n
1 − exp(−c1n) − c2 exp(cid:0)−w2(ΩR)/c2
(cid:32) n(cid:88)

Theorem 7 Assume that X = {Xi}n
random matrix X with |||X|||ψ2
|||ωi|||ψ2

(cid:33)

R∗

≤ c0κτ · √

ωiXi

i=1

6

nw(ΩR) .

(17)

i

ψ2

=

√

P((cid:107)ω(cid:107)2

2|||ωi|||ψ2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ψ1

≤ 4|||ωi|||2

[31]. By Bernstein’s inequality  we get

2τ  and(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ω2

≤
≤ 4τ 2  where we use the deﬁnition of ψ2 norm and its relation to ψ1 norm

Proof: For each entry in ω  we have(cid:112)E[ω2
i ](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ψ1
2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ω2
i ] ≤ √
2 − 2τ 2 ≥ ) ≤ P(cid:0)(cid:107)ω(cid:107)2
2] ≥ (cid:1) ≤ exp(cid:0)−c1 min(cid:0)2/16τ 4n  /4τ 2(cid:1)(cid:1) .
Taking  = 4τ 2n  we have P(cid:0)(cid:107)ω(cid:107)2 ≥ τ
6n(cid:1) ≤ exp (−c1n). Denote Yu =(cid:80)n
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ψ2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

For any u ∈ Sn−1  we get |||Yu|||ψ2

≤ cκ for any ∆ ∈ Sdp−1.

i=1 uiXi for u ∈ Rn.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

i|||(cid:104)(cid:104)Xi  ∆(cid:105)(cid:105)|||2
u2

|||(cid:104)(cid:104)Yu  ∆(cid:105)(cid:105)|||ψ2

2 − E[(cid:107)ω(cid:107)2

ui(cid:104)(cid:104)Xi  ∆(cid:105)(cid:105)

≤ cκ due to

i − E[ω2

≤ c

√

i=1

i=1

=

ψ2

For the rest of the proof  we may drop the subscript of Yu for convenience. We construct the stochastic
process {Z∆ = (cid:104)(cid:104)Y  ∆(cid:105)(cid:105)}∆∈ΩR  and note that any ZU and ZV from this process satisfy

P (|ZU − ZV | ≥ ) = P (|(cid:104)(cid:104)Y  U − V (cid:105)(cid:105)| ≥ ) ≤ e · exp(cid:0)−C2/κ2(cid:107)U − V (cid:107)2

(cid:1)  

F

for some universal constant C due to the sub-Gaussianity of Y . As ΩR is symmetric  it follows that

sup

U V ∈ΩR

|ZU − ZV | = 2 sup
∆∈ΩR

Z∆  

sup

U V ∈ΩR

(cid:107)U − V (cid:107)F = 2 sup
∆∈ΩR

(cid:107)∆(cid:107)F = 2η .

Let s(· ·) be the metric induced by norm κ(cid:107) · (cid:107)F and T = ΩR. Using deviation bound (8)  we have

Z∆ ≥ c4κ (γ2(ΩR (cid:107) · (cid:107)F ) +  · 2η)

(cid:19)

≤ c2 exp(cid:0)−2(cid:1)  

(cid:18)

P

2 sup
∆∈ΩR

where c2 and c4 are absolute constant. By (11)  there exist constants c3 and c5 such that
P (2R∗(Y ) ≥ c5κ (w(ΩR) + )) = P

Z∆ ≥ c5κ (w(ΩR) + )

≤ c2 exp(cid:0)−2/c2
3η2(cid:1) .

(cid:19)

(cid:18)

2 sup
∆∈ΩR

(cid:32)

Letting  = w(ΩR)  we have P (R∗(Yu) ≥ c5κw(ΩR)) ≤ c2 exp
√
Sn−1. Combining this with the bound for (cid:107)ω(cid:107)2 and letting c0 =

(cid:33)

(cid:32) n(cid:88)
P (R∗ (Yu) ≥ c5κw(ΩR)) + P(cid:16)(cid:107)ω(cid:107)2 ≥ τ

≥ c0κτ

nw(ΩR)

(cid:33)

≤ P

ωiXi

√

i=1

(cid:18) R∗ (Yω)

P

R∗

≤ sup
u∈Sn−1

which completes the proof.

(cid:16)− (w(ΩR)/c3η)2(cid:17)
+ P(cid:16)(cid:107)ω(cid:107)2 ≥ τ

6c5  by union bound  we have
√

(cid:19)

for any u ∈

(cid:17)

(cid:107)ω(cid:107)2
√

6n

≥ c5κw(ΩR)

(cid:17) ≤ c2 exp(cid:0)−w2(ΩR)/c2

6n

3η2(cid:1) + exp (−c1n)  

The theorem above shows that the lower bound of λn depends on the Gaussian width of the unit ball
of R(·). Next we give its general bound for the unitarily invariant matrix norm.
Theorem 8 Suppose that the symmetric gauge f associated with R(·) satisﬁes f (·) ≥ ν(cid:107) · (cid:107)1. Then
the Gaussian width w(ΩR) is upper bounded by

w(ΩR) ≤

p

.

(18)

√

√

d +
ν

5 Examples
Combining results in Section 4  we have that if the number of measurements n > O(w2(AR(Θ∗))) 
√
then the recovery error  with high probability  satisﬁes (cid:107) ˆΘ− Θ∗(cid:107)F ≤ O (ΨR(Θ∗)w(ΩR)/
n). Here
we give two examples based on the trace norm [10] and the recently proposed spectral k-support
norm [24] to illustrate how to bound the geometric measures and obtain the error bound.

7

√

5.1 Trace norm
Trace norm has been widely used in low-rank matrix recovery. The trace norm of Θ∗ is basically
the (cid:96)1 norm of σ∗  i.e.  f = (cid:107) · (cid:107)1. Now we turn to the three geometric measures. Assuming that
rank(Θ∗) = r (cid:28) d  one subgradient of (cid:107)σ∗(cid:107)1 is θ∗ = [1  1  . . .   1]T .
Restricted compatibility constant Ψtr(Θ∗): It is obvious that assumption in Theorem 3 will hold
for f by choosing η1 = 1 and η2 = 0  and we have ρ = 1. The sparse compatibility constant Φ(cid:96)1(r)
is

r because (cid:107)δ(cid:107)1 ≤ √
√
Gaussian width w(Atr(Θ∗)): As ρ = 1  Theorem 6 implies that w(Atr(Θ∗)) ≤(cid:112)3r(d + p − r).
r(cid:107)δ(cid:107)2 for any r-sparse δ. Using Theorem 3  we have Ψtr(Θ∗) ≤ 4
Gaussian width w(Ωtr): Using Theorem 8 with ν = 1  it is easy to see that w(Ωtr) ≤ √
Putting all the results together  we have (cid:107) ˆΘ − Θ∗(cid:107)F ≤ O((cid:112)rd/n +(cid:112)rp/n) holds with high
probability when n > O(r(d + p − r))  which matches the bound in [8].

d +

√

p.

r.

5.2 Spectral k-support norm

k

i

i

 

(cid:111)

√
max{(cid:107) · (cid:107)2 (cid:107) · (cid:107)1/

(cid:44) inf

(cid:107)θ(cid:107)sp

(cid:88)

(cid:12)(cid:12)(cid:12) (cid:107)ui(cid:107)0 ≤ k 

(cid:110)(cid:88)
The k-support norm proposed in [2] is deﬁned as
(cid:107)ui(cid:107)2
k = (cid:107)|θ|↓

(19)
and its dual norm is simply given by (cid:107)θ(cid:107)sp∗
1:k(cid:107)2. It is shown that k-support norm has similar
behavior as elastic-net regularizer [33]. Spectral k-support norm (denoted by (cid:107) · (cid:107)sk) of Θ∗ is deﬁned
by applying the k-support norm on σ∗  i.e.  f = (cid:107) · (cid:107)sp
k   which has demonstrated better performance
than trace norm in matrix completion task [24]. For simplicity  We assume that rank(Θ∗) = r = k
and (cid:107)σ∗(cid:107)2 = 1. One subgradient of (cid:107)σ∗(cid:107)sp
Restricted compatibility constant Ψsk(Θ∗): The following relation has been shown for k-support
norm in [2] 

k can be θ∗ = [σ∗

2  . . .   σ∗

r   . . .   σ∗

1  σ∗

r   σ∗

ui = θ

k} .

k} ≤ (cid:107) · (cid:107)sp

k ≤
Hence the assumption in Theorem 3 will hold for η1 =
√
The sparse compatibility constant Φsp
Using Theorem 3  we have Ψsk(Θ∗) ≤ 2

(20)
√
1/σ∗
2  and we have ρ = σ∗
r .
k and η2 =
k = (cid:107)δ(cid:107)2 for any k-sparse δ.
k (k) = 1 because (cid:107)δ(cid:107)sp
√
√
k (r) = Φsp
1/σ∗
2 (3 + σ∗

Gaussian width w(Ask(Θ∗)): Theorem 6 implies w(Ask(Θ∗)) ≤(cid:112)r(d + p − r) [2σ∗2
O((cid:112)rd/n +(cid:112)rp/n) when n > O(r(d + p − r)). The spectral k-support norm was ﬁrst introduced

r + 1].
Gaussian width w(Ωsk): The relation above for k-support norm shown in [2] also implies that
ν = 1/
Given the upper bounds for geometric measures  with high probability  we have (cid:107) ˆΘ − Θ∗(cid:107)F ≤

r. By Theorem 8  we get w(Ωsk) ≤ √

√
k = 1/

2 (1 + σ∗

1 /σ∗2

1/σ∗
r ).

√
r(

in [24]  in which no statistical results are provided. Although [20] investigated the statistical aspects
of spectral k-support norm in matrix completion setting  the analysis was quite different from our
setting. Hence this error bound is new in the literature.

r ) =

√
2 max{(cid:107) · (cid:107)2 (cid:107) · (cid:107)1/

(cid:113) 2

r ]T .

√

d +

p).

2 +

√

√

6 Conclusions

In this work  we present the recovery analysis for matrices with general structures  under the setting
of sub-Gaussian measurement and noise. Base on generic chaining and Gaussian width  the recovery
guarantees can be succinctly summarized in terms of some geometric measures. For the class
of unitarily invariant norms  we also provide novel general bounds of these measures  which can
signiﬁcantly facilitate the analysis in future.

Acknowledgements
The research was supported by NSF grants IIS-1563950  IIS-1447566  IIS-1447574  IIS-1422557 
CCF-1451986  CNS- 1314560  IIS-0953274  IIS-1029711  NASA grant NNX12AQ39A  and gifts
from Adobe  IBM  and Yahoo.

8

References
[1] D. Amelunxen  M. Lotz  M. B. McCoy  and J. A. Tropp. Living on the edge: Phase transitions in convex

programs with random data. Inform. Inference  3(3):224–294  2014.

[2] A. Argyriou  R. Foygel  and N. Srebro. Sparse prediction with the k-support norm. In NIPS  2012.
[3] A. Banerjee  S. Chen  F. Fazayeli  and V. Sivakumar. Estimation with norm regularization. In NIPS  2014.
[4] R. Bhatia. Matrix Analysis. Springer  1997.
[5] J. Bourgain  S. Dirksen  and J. Nelson. Toward a uniﬁed theory of sparse dimensionality reduction in

Euclidean space. Geometric and Functional Analysis  25(4):1009–1088  2015.

[6] T. T. Cai  T. Liang  and A. Rakhlin. Geometrizing Local Rates of Convergence for High-Dimensional

Linear Inverse Problems. arXiv:1404.4408  2014.

[7] T. T. Cai and A. Zhang. ROP: Matrix recovery via rank-one projections. The Annals of Statistics 

[8] E. Candès and B. Recht. Exact matrix completion via convex optimization. Communications of the ACM 

[9] E. J. Candès  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? Journal of the ACM 

43(1):102–138  2015.

55(6):111–119  2012.

58(3):11:1–11:37  2011.

[10] E. J. Candès and Y. Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of

noisy random measurements. IEEE Transactions on Information Theory  57(4):2342–2359  2011.

[11] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear inverse

problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[12] S. Chatterjee  S. Chen  and A. Banerjee. Generalized dantzig selector: Application to the k-support norm.

In Advances in Neural Information Processing Systems (NIPS)  2014.

[13] S. Chen and A. Banerjee. Structured estimation with atomic norms: General bounds and applications. In

NIPS  pages 2908–2916  2015.

43(1):129–159  2001.

[14] S. S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Rev. 

[15] S. Dirksen. Dimensionality reduction with subgaussian matrices: a uniﬁed theory. arXiv:1402.3973  2014.
[16] S. Dirksen. Tail bounds via generic chaining. Electron. J. Probab.  20  2015.
[17] M. A. T. Figueiredo and R. D. Nowak. Ordered weighted l1 regularized regression with strongly correlated

covariates: Theoretical aspects. In AISTATS  2016.

[18] Y. Gordon. Some inequalities for Gaussian processes and applications. Israel Journal of Mathematics 

50(4):265–289  1985.

[19] Y. Gordon. On Milman’s inequality and random subspaces which escape through a mesh in Rn. In
Geometric Aspects of Functional Analysis  volume 1317 of Lecture Notes in Mathematics  pages 84–106.
Springer  1988.

[20] S. Gunasekar  A. Banerjee  and J. Ghosh. Uniﬁed view of matrix completion under general structural

constraints. In NIPS  pages 1180–1188  2015.

[21] S. Gunasekar  P. Ravikumar  and J. Ghosh. Exponential family matrix completion under structural

constraints. In International Conference on Machine Learning (ICML)  2014.

[22] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems. Computer 

42(8):30–37  2009.

2(1-2):173–183  1995.

[23] A. S. Lewis. The Convex Analysis of Unitarily Invariant Matrix Functions. Journal of Convex Analysis 

[24] A. M. McDonald  M. Pontil  and D. Stamos. Spectral k-support norm regularization. In NIPS  2014.
[25] S. Mendelson  A. Pajor  and N. Tomczak-Jaegermann. Reconstruction and subGaussian operators in

asymptotic geometric analysis. Geometric and Functional Analysis  17:1248–1282  2007.

[26] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for the analysis of

regularized M-estimators. Statistical Science  27(4):538–557  2012.

[27] B. Recht  M. Fazel  and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via

nuclear norm minimization. SIAM Review  52(3):471–501  2010.

[28] V. Sivakumar  A. Banerjee  and P. Ravikumar. Beyond sub-gaussian measurements: High-dimensional

structured estimation with sub-exponential designs. In NIPS  pages 2206–2214  2015.

[29] M. Talagrand. A simple proof of the majorizing measure theorem. Geometric & Functional Analysis

GAFA  2(1):118–125  1992.

[30] M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer  2014.
[31] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and G. Kutyniok 

editors  Compressed Sensing  chapter 5  pages 210–268. Cambridge University Press  2012.

[32] X. Zhang  Y. Yu  and D. Schuurmans. Polar operators for structured sparse estimation. In NIPS  2013.
[33] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal

Statistical Society  Series B  67:301–320  2005.

[34] O. Zuk and A. Wagner. Low-rank matrix recovery from row-and-column afﬁne measurements.

In

International Conference on Machine Learning (ICML)  2015.

9

,Sheng Chen
Arindam Banerjee