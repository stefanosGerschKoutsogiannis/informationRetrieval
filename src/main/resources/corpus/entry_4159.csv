2014,Optimizing F-Measures by Cost-Sensitive Classification,We present a theoretical analysis of F-measures for binary  multiclass and multilabel classification. These performance measures are non-linear  but in many scenarios they are pseudo-linear functions of the per-class false negative/false positive rate. Based on this observation  we present a general reduction of F-measure maximization to cost-sensitive classification with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers  extending the existing theoretical results on F-measures  which are asymptotic in nature. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F-measure optimization tasks.,Optimizing F-Measures by Cost-Sensitive Classiﬁcation

Shameem A. Puthiya Parambath  Nicolas Usunier  Yves Grandvalet
Universit´e de Technologie de Compi`egne – CNRS  Heudiasyc UMR 7253

{sputhiya nusunier grandval}@utc.fr

Compi`egne  France

Abstract

We present a theoretical analysis of F -measures for binary  multiclass and mul-
tilabel classiﬁcation. These performance measures are non-linear  but in many
scenarios they are pseudo-linear functions of the per-class false negative/false
positive rate. Based on this observation  we present a general reduction of F -
measure maximization to cost-sensitive classiﬁcation with unknown costs. We
then propose an algorithm with provable guarantees to obtain an approximately
optimal classiﬁer for the F -measure by solving a series of cost-sensitive classi-
ﬁcation problems. The strength of our analysis is to be valid on any dataset and
any class of classiﬁers  extending the existing theoretical results on F -measures 
which are asymptotic in nature. We present numerical experiments to illustrate
the relative importance of cost asymmetry and thresholding when learning linear
classiﬁers on various F -measure optimization tasks.

1

Introduction

The F1-measure  deﬁned as the harmonic mean of the precision and recall of a binary decision
rule [20]  is a traditional way of assessing the performance of classiﬁers. As it favors high and bal-
anced values of precision and recall  this performance metric is usually preferred to (label-dependent
weighted) classiﬁcation accuracy when classes are highly imbalanced and when the cost of a false
positive relatively to a false negative is not naturally given for the problem at hand. The design of
methods to optimize F1-measure and its variants for multilabel classiﬁcation (the micro-  macro- 
per-instance-F1-measures  see [23] and Section 2)  and the theoretical analysis of the optimal clas-
siﬁers for such metrics have received considerable interest in the last 3-4 years [6  15  4  18  5  13] 
especially because rare classes appear naturally on most multilabel datasets with many labels.
The most usual way of optimizing F1-measure is to perform a two-step approach in which ﬁrst a
classiﬁer which output scores (e.g. a margin-based classiﬁer) is learnt  and then the decision thresh-
old is tuned a posteriori. Such an approach is theoretically grounded in binary classiﬁcation [15] and
for micro- or macro-F1-measures of multilabel classiﬁcation [13] in that a Bayes-optimal classiﬁer
for the corresponding F1-measure can be obtained by thresholding posterior probabilities of classes
(the threshold  however  depends on properties of the whole distribution and cannot be known in ad-
vance). Thus  such arguments are essentially asymptotic since the validity of the procedure is bound
to the ability to accurately estimate all the level sets of the posterior probabilities; in particular  the
proof does not hold if one wants to ﬁnd the optimal classiﬁer for the F1-measure over an arbitrary
set of classiﬁers (e.g. thresholded linear functions).
In this paper  we show that optimizing the F1-measure in binary classiﬁcation over any (possibly
restricted) class of functions and over any data distribution (population-level or on a ﬁnite sample)
can be reduced to solving an (inﬁnite) series of cost-sensitive classiﬁcation problems  but the cost
space can be discretized to obtain approximately optimal solutions. For binary classiﬁcation  as
well as for multilabel classiﬁcation (micro-F1-measure in general and the macro-F1-measure when
training independent classiﬁers per class)  the discretization can be made along a single real-valued

1

variable in [0  1] with approximation guarantees. Asymptotically  our result is  in essence  equivalent
to prior results since Bayes-optimal classiﬁers for cost-sensitive classiﬁcation are precisely given by
thresholding the posterior probabilities  and we recover the relationship between the optimal F1-
measure and the optimal threshold given by Lipton et al.
[13]. Our reduction to cost-sensitive
classiﬁcation  however  is strictly more general. Our analysis is based on the pseudo-linearity of
the F1-scores (the level sets  as function of the false negative rate and the false positive rate are
linear) and holds in any asymptotic or non-asymptotic regime  with any arbitrary set of classiﬁers
(without the requirement to output scores or accurate posterior probability estimates). Our formal
framework and the deﬁnition of pseudo-linearity is presented in the next section  and the reduction
to cost-sensitive classiﬁcation is presented in Section 2.
While our main contribution is the theoretical part  we also turn out to the practical suggestions of our
results. In particular  they suggest that  for binary classiﬁcation  learning cost-sensitive classiﬁers
may be more effective than thresholding probabilities. This is in-line with Musicant et al. [14] 
although their argument only applies to SVM and does not consider the F1-measure itself but a
continuous  non-convex approximation of it. Some experimental results are presented in Section 4 
before the conclusion of the paper.

2 Pseudo-Linearity and F -Measures

Our results are mainly motivated by the maximization of F -measures for binary and multilabel
classiﬁcation. They are based on a general property of these performance metrics  namely their
pseudo-linearity with respect to the false negative/false positive probabilities.
For binary classiﬁcation  the results we prove in Section 3 are that in order to optimize the F -
measure  it is sufﬁcient to solve a binary classiﬁcation problem with different costs allocated to false
positive and false negative errors (Proposition 4). However  these costs are not known a priori  so in
practice we need to learn several classiﬁers with different costs  and choose the best one (according
to the F -score) in a second step. Propositions 5 and 6 provide approximation guarantees on the
F -score we can obtain by following this principle depending on the granularity of the search in the
cost space.
Our results are not speciﬁc to the F1-measure in binary classiﬁcation  and they naturally extend to
other cases of F -measures with similar functional forms. For that reason  we present the results and
prove them directly for the general case  following the framework that we describe in this section.
We ﬁrst present the machine learning framework we consider  and then give the general deﬁnition of
pseudo-convexity. Then  we provide examples of F -measures for binary  multilabel and multiclass
classiﬁcation and we show how they ﬁt into this framework.

2.1 Notation and Deﬁnitions
We are given (i) a measurable space X ×Y  where X is the input space and Y is the (ﬁnite) prediction
set  (ii) a probability measure µ over X × Y  and (iii) a set of (measurable) classiﬁers H from the
input space X to Y. We distinguish here the prediction set Y from the label space L = {1  ...  L}: in
binary or single-label multi-class classiﬁcation  the prediction set Y is the label set L  but in multi-
label classiﬁcation  Y = 2L is the powerset of the set of possible labels. In that framework  we
assume that we have an i.i.d. sample drawn from an underlying data distribution P on X × Y. The
empirical distribution of this ﬁnite training (or test) sample will be denoted ˆP. Then  we may take
µ = P to get results at the population level (concerning expected errors)  or we may take µ = ˆP
to get results on a ﬁnite sample. Likewise  H can be a restricted set of functions such as linear
classiﬁers if X is a ﬁnite-dimensional vector space  or may be the set of all measurable classiﬁers
from X to Y to get results in terms of Bayes-optimal predictors. Finally  when needed  we will use
bold characters for vectors and normal font with subscript for indexing.
Throughout the paper  we need the notion of pseudo-linearity of a function  which itself is deﬁned
from the notion of pseudo-convexity (see e.g. [3  Deﬁnition 3.2.1]): a differentiable function F :
D ⊂ Rd → R  deﬁned on a convex open subset of Rd  is pseudo-convex if

∀e  e(cid:48) ∈ D   F (e) > F (e(cid:48)) ⇒ (cid:104)∇F (e)  e(cid:48) − e(cid:105) < 0  

where (cid:104).  .(cid:105) is the canonical dot product on Rd.

2

Moreover  F is pseudo-linear if both F and −F are pseudo-convex. The important property of
pseudo-linear functions is that their level sets are hyperplanes (intersected with the domain)  and that
sublevel and superlevel sets are half-spaces  all of these hyperplanes being deﬁned by the gradient.
In practice  working with gradients of non-linear functions may be cumbersome  so we will use the
following characterization  which is a rephrasing of [3  Theorem 3.3.9]:
Theorem 1 ([3]) A non-constant function F : D → R  deﬁned and differentiable on the open convex
set D ⊆ Rd  is pseudo-linear on D if and only if ∀e ∈ D   ∇F (e) (cid:54)= 0   and: ∃a : R → Rd and
∃b : R → R such that  for any t in the image of F :

F (e) ≥ t ⇔ (cid:104)a(t)  e(cid:105) + b(t) ≤ 0 and F (e) ≤ t ⇔ (cid:104)a(t)   e(cid:105) + b(t) ≥ 0 .

Pseudo-linearity is the main property of fractional-linear functions (ratios of linear functions). In-
deed  let us consider F : e ∈ Rd (cid:55)→ (α + (cid:104)β  e(cid:105))/(γ + (cid:104)δ  e(cid:105)) with α  γ ∈ R and β and δ in Rd. If
we restrict the domain of F to the set {e ∈ Rd|γ + (cid:104)δ  e(cid:105) > 0}  then  for all t in the image of F and
all e in its domain  we have: F (e) ≤ t ⇔ (cid:104)tδ − β  e(cid:105) + tγ − α ≥ 0   and the analogous equiva-
lence obtained by reversing the inequalities holds as well; the function thus satisﬁes the conditions
of Theorem 1. As we shall see  many F -scores can be written as fractional-linear functions.

2.2 Error Proﬁles and F -Measures

For all classiﬁcation tasks (binary  multiclass and multilabel)  the F -measures we consider are func-
tions of per-class recall and precision  which themselves are deﬁned in terms of the marginal prob-
abilities of classes and the per-class false negative/false positive probabilities. The marginal proba-
bilities of label k will be denoted by Pk  and the per-class false negative/false positive probabilities
of a classiﬁer h are denoted by FNk(h) and FPk(h). Their deﬁnitions are given below:

(binary/multiclass) Pk = µ({(x  y)|y = k})  FNk(h) = µ({(x  y)|y = k and h(x) (cid:54)= k})  
FPk(h) = µ({(x  y)|y (cid:54)= k and h(x) = k}) .
Pk = µ({(x  y)|y ∈ k})  FNk(h) = µ({(x  y)|k ∈ y and k (cid:54)∈ h(x)})  
FPk(h) = µ({(x  y)|y (cid:54)∈ k and k ∈ h(x)}) .

(multilabel)

These probabilities of a classiﬁer h are then summarized by the error proﬁle E(h):

E(h) =(cid:0)FN1(h)   FP1(h)   ...  FNL(h)   FPL(h)(cid:1) ∈ R2L  

so that e2k−1 is the false negative probability for class k and e2k is the false positive probability.

Binary Classiﬁcation In binary classiﬁcation  we have FN2 = FP1 and we write F -measures only
by reference to class 1. Then  for any β > 0 and any binary classiﬁer h  the Fβ-measure is

Fβ(h) =

(1 + β2)(P1 − FN1(h))

(1 + β2)P1 − FN1(h) + FP1(h)

.

The F1-measure  which is the most widely used  corresponds to the case β = 1. We can immediately
notice that Fβ is fractional-linear  hence pseudo-convex  with respect to FN1 and FP1. Thus  with
a slight (yet convenient) abuse of notation  we write the Fβ-measure for binary classiﬁcation as a
function of vectors in R4 = R2L which represent error proﬁles of classiﬁers:
(1 + β2)(P1 − e1)
(1 + β2)P1 − e1 + e2

∀e ∈ R4  Fβ(e) =

(binary)

.

Multilabel Classiﬁcation In multilabel classiﬁcation  there are several deﬁnitions of F -measures.
For those based on the error proﬁles  we ﬁrst have the macro-F -measures (denoted by M Fβ)  which
is the average over class labels of the Fβ-measures of each binary classiﬁcation problem associated
to the prediction of the presence/absence of a given class:

(multilabel–M acro)

M Fβ(e) =

1
L

(1 + β2)(P − e2k−1)
(1 + β2)P − e2k−1 + e2k

.

L(cid:88)

k=1

3

M Fβ is not a pseudo-linear function of an error proﬁle e. However  if the multi-label classiﬁcation
algorithm learns independent binary classiﬁers for each class (a method known as one-vs-rest or
binary relevance [23])  then each binary problem becomes independent and optimizing the macro-
F -score boils down to independently maximizing the Fβ-score for L binary classiﬁcation problems 
so that optimizing M Fβ is similar to optimizing Fβ in binary classiﬁcation.
There are also micro-F -measures for multilabel classiﬁcation. They correspond to Fβ-measures
for a new binary classiﬁcation problem over X × L  in which one maps a multilabel classiﬁer
h : X → Y (Y is here the power set of L) to the following binary classiﬁer ˜h : X × L → {0  1}: we
have ˜h(x  k) = 1 if k ∈ h(x)  and 0 otherwise. The micro-Fβ-measure  written as a function of an
error proﬁle e and denoted by mFβ(e)  is the Fβ-score of ˜h and can be written as:

(1 + β2)(cid:80)L
k=1 Pk +(cid:80)L

(1 + β2)(cid:80)L

k=1(Pk − e2k−1)

k=1(e2k − e2k−1)

.

(multilabel–micro)

mFβ(e) =

This function is also fractional-linear  and thus pseudo-linear as a function of e.
A third notion of Fβ-measure can be used in multilabel classiﬁcation  namely the per-instance Fβ
studied e.g. by [16  17  6  4  5]. The per-instance Fβ is deﬁned as the average  over instances x  of
the binary Fβ-measure for the problem of classifying labels given x. This corresponds to a speciﬁc
Fβ-maximization problem for each x and is not directly captured by our framework  because we
would need to solve different cost-sensitive classiﬁcation problems for each instance.

Multiclass Classiﬁcation The last example we take is from multiclass classiﬁcation. It differs
from multilabel classiﬁcation in that a single class must be predicted for each example. This restric-
tion imposes strong global constraints that make the task signiﬁcantly harder. As for the multillabel
case  there are many deﬁnitions of F -measures for multiclass classiﬁcation  and in fact several
deﬁnitions for the micro-F -measure itself. We will focus on the following one  which is used in in-
formation extraction (e.g. in the BioNLP challenge [12]). Given L class labels  we will assume that
label 1 corresponds to a “default” class  the prediction of which is considered as not important. In
information extraction  the “default” class corresponds to the (majority) case where no information
should be extracted. Then  a false negative is an example (x  y) such that y (cid:54)= 1 and h(x) (cid:54)= y  while
a false positive is an example (x  y) such that y = 1 and h(x) (cid:54)= y. This micro-F -measure  denoted
mcFβ can be written as:

(1 + β2)(1 − P1 −(cid:80)L
(1 + β2)(1 − P1) −(cid:80)L

k=2 e2k−1)
k=2 e2k−1 + e1

.

(multiclass–micro)

mcFβ(e) =

Once again  this kind of micro-Fβ-measure is pseudo-linear with respect to e.
Remark 2 (Training and generalization performance) Our results concern a ﬁxed distribution
µ  while the goal is to ﬁnd a classiﬁer with high generalization performance. With our notation  our
results apply to µ = P or µ = ˆP  and our implicit goal is to perform empirical risk minimization-
P
type learning  that is  to ﬁnd a classiﬁer with high value of F
β
counterpart F ˆP

(h)(cid:1) by maximizing its empirical

(the superscripts here make the underlying distribution explicit).

(cid:0)E

EˆP

(h)

(cid:16)

(cid:17)

P

β

Remark 3 (Expected Utility Maximization (EUM) vs Decision-Theoretic Approach (DTA))
Nan et al. [15] propose two possible deﬁnitions of the generalization performance in terms of
Fβ-scores.
In the ﬁrst framework  called EUM  the population-level Fβ-score is deﬁned as the
Fβ-score of the population-level error proﬁles. In contrast  the Decision-Theoretic approach deﬁnes
the population-level Fβ-score as the expected value of the Fβ-score over the distribution of test sets.
The EUM deﬁnition of generalization performance matches our framework using µ = P: in that
sense  we follow the EUM framework. Nonetheless  regardless of how we deﬁne the generalization
performance  our results can be used to maximize the empirical value of the Fβ-score.

3 Optimizing F -Measures by Reduction to Cost-Sensitive Classiﬁcation

The F -measures presented above are non-linear aggregations of false negative/positive probabilities
that cannot be written in the usual expected loss minimization framework; usual learning algorithms
are thus  intrinsically  not designed to optimize this kind of performance metrics.

4

In this section  we show in Proposition 4 that the optimal classiﬁer for a cost-sensitive classiﬁcation
problem with label dependent costs [7  24] is also an optimal classiﬁer for the pseudo-linear F -
measures (within a speciﬁc  yet arbitrary classiﬁer set H). In cost-sensitive classiﬁcation  each entry
of the error proﬁle is weighted by a non-negative cost  and the goal is to minimize the weighted
average error. Efﬁcient  consistent algorithms exist for such cost-sensitive problems [1  22  21].
Even though the costs corresponding to the optimal F -score are not known a priori  we show in
Proposition 5 that we can approximate the optimal classiﬁer with approximate costs. These costs 
explicitly expressed in terms of the optimal F -score  motivate a practical algorithm.

(cid:10)a(cid:0)F (cid:63)(cid:1)  e(cid:48)(cid:11) ⇔ F (e) = F (cid:63)

3.1 Reduction to Cost-Sensitive Classiﬁcation
In this section  F : D ⊂ Rd → R is a ﬁxed pseudo-linear function. We denote by a : R → Rd the
function mapping values of F to the corresponding hyperplane of Theorem 1. We assume that the
distribution µ is ﬁxed  as well as the (arbitrary) set of classiﬁer H. We denote by E (H) the closure
of the image of H under E  i.e. E (H) = cl({E(h)   h ∈ H}) (the closure ensures that E (H) is
compact and that minima/maxima are well-deﬁned)  and we assume E (H) ⊆ D. Finally  for the
sake of discussion with cost-sensitive classiﬁcation  we assume that a(t) ∈ Rd
+ for any e ∈ E (H) 
that is  lower values of errors entail higher values of F .

F (e(cid:48)). We have: e ∈ argmin
e(cid:48)∈E(H)

Proposition 4 Let F (cid:63) = max
e(cid:48)∈E(H)

Proof Let e(cid:63) ∈ argmaxe(cid:48)∈E(H) F (e(cid:48))  and let a(cid:63) = a(F (e(cid:63))) = a(cid:0)F (cid:63)(cid:1). We ﬁrst notice that

pseudo-linearity implies that the set of e ∈ D such that (cid:104)a(cid:63)  e(cid:105) = (cid:104)a(cid:63)  e(cid:63)(cid:105) corresponds to the
level set {e ∈ D|F (e) = F (e(cid:63)) = F (cid:63)}. Thus  we only need to show that e(cid:63) is a minimizer of
e(cid:48) (cid:55)→ (cid:104)a(cid:63)  e(cid:48)(cid:105) in E (H). To see this  we notice that pseudo-linearity implies
∀e(cid:48) ∈ D  F (e(cid:63)) ≥ F (e(cid:48)) ⇒ (cid:104)a(cid:63)  e(cid:63)(cid:105) ≤ (cid:104)a(cid:63)  e(cid:48)(cid:105)

from which we immediately get e(cid:63) ∈ argmine(cid:48)∈E(H) (cid:104)a(cid:63)  e(cid:48)(cid:105) since e(cid:63) maximizes F in E (H). (cid:3)

The proposition shows that a(cid:0)F (cid:63)(cid:1) are the costs that should be assigned to the error proﬁle in order
to ﬁnd the F -optimal classiﬁer in H. Hence maximizing F amounts to minimizing(cid:10)a(cid:0)F (cid:63)(cid:1)  E(h)(cid:11)
with respect to h  that is  amounts to solving a cost-sensitive classiﬁcation problem. The costs a(cid:0)F (cid:63)(cid:1)

are  however  not known a priori (because F (cid:63) is not known in general). The following result shows
that having only approximate costs is sufﬁcient to have an approximately F -optimal solution  which
gives us the main step towards a practical solution:
Proposition 5 Let ε0 ≥ 0 and ε1 ≥ 0  and assume that there exists Φ > 0 such that for all
e  e(cid:48) ∈ E (H) satisfying F (e(cid:48)) > F (e)  we have:

(1)
Then  let us take e(cid:63) ∈ argmaxe(cid:48)∈E(H) F (e(cid:48))  and denote a(cid:63) = a(F (e(cid:63))). Let furthermore g ∈ Rd
and h ∈ H satisfying the two following conditions:

F (e(cid:48)) − F (e) ≤ Φ(cid:104)a(F (e(cid:48)))   e − e(cid:48)(cid:105) .

+

(i) (cid:107) g − a(cid:63) (cid:107)2≤ ε0

(cid:104)g  e(cid:48)(cid:105) + ε1 .
F (E(h)) ≥ F (e(cid:63)) − Φ · (2ε0M + ε1)   where M = max
e(cid:48)∈E(H)

(ii) (cid:104)g  E(h)(cid:105) ≤ min
e(cid:48)∈E(H)

(cid:107) e(cid:48) (cid:107)2.

We have:

Proof Let e(cid:48) ∈ E (H). By writing (cid:104)g  e(cid:48)(cid:105) = (cid:104)g − a(cid:63)  e(cid:48)(cid:105) + (cid:104)a(cid:63)  e(cid:48)(cid:105) and applying Cauchy-Schwarz
inequality to (cid:104)g − a(cid:63)  e(cid:48)(cid:105) we get (cid:104)g  e(cid:48)(cid:105) ≤ (cid:104)a(cid:63)  e(cid:48)(cid:105) + ε0M using condition (i). Consequently

min

e(cid:48)∈E(H)

(cid:104)g  e(cid:48)(cid:105) ≤ min
e(cid:48)∈E(H)

(cid:104)a(cid:63)  e(cid:48)(cid:105) + ε0M = (cid:104)a(cid:63)  e(cid:63)(cid:105) + ε0M

(2)

Where the equality is given by Proposition 4. Now  let e = E(h)  assuming that classiﬁer h satisﬁes
condition (ii). Using (cid:104)a(cid:63)  e(cid:105) = (cid:104)a(cid:63) − g  e(cid:105) + (cid:104)g  e(cid:105) and Cauchy-Shwarz  we obtain:

(cid:104)a(cid:63)  e(cid:105) ≤ (cid:104)g  e(cid:105) + ε0M ≤ min
e(cid:48)∈E(H)

(cid:104)g  e(cid:48)(cid:105) + ε1 + ε0M ≤ (cid:104)a(cid:63)  e(cid:63)(cid:105) + ε1 + 2ε0M  

where the ﬁrst inequality comes from condition (ii) and the second inequality comes from (2). The
(cid:3)
ﬁnal result is obtained by plugging this inequality into (1).

5

Before discussing this result  we ﬁrst give explicit values of a and Φ for pseudo-linear F -measures:

Proposition 6 Fβ  mFβ and mcFβ deﬁned in Section 2 satisfy the conditions of Proposition 5 with:

(binary) Fβ :

Φ =

(multilabel–micro) mFβ : Φ =

1

β2P1

β2(cid:80)L

1
k=1 Pk

and ai(t) =

(multiclass–micro) mcFβ : Φ =

1

β2(1 − P1)

and ai(t) =

t
0

and a : t ∈ [0  1] (cid:55)→ (1 + β2− t  t  0  0) .

(cid:26)1 + β2 − t
1 + β2 − t

t

if i is odd
if i is even .
if i is odd and i (cid:54)= 1
if i = 1
otherwise

.

The proof is given in the longer version of the paper  and the values of Φ and a are valid for any set
of classiﬁers H. Note that the result on Fβ for binary classiﬁcation can be used for the macro-Fβ-
measure in multilabel classiﬁcation when training one binary classiﬁer per label. Also  the relative
costs (1+β2−t) for false negative and t for false positive imply that for the F1-measure  the optimal
classiﬁer is the solution of the cost-sensitive binary problem with costs (1 − F (cid:63)/2)  F (cid:63)/2. If we
take H as the set of all measurable functions  the Bayes-optimal classiﬁer for this cost is to predict
class 1 when µ(y = 1|x) ≥ F (cid:63)/2 (see e.g. [22]). Our propositions thus extends this known result
[13] to the non-asymptotic regime and to an arbitrary set of classiﬁers.

3.2 Practical Algorithm

Our results suggests that the optimization of pseudo-linear F -measures should wrap cost-sensitive
classiﬁcation algorithms  used in an inner loop  by an outer loop setting the appropriate costs.
In practice  since the function a : [0  1] → Rd  which assigns costs to probabilities of error  is
Lipschitz-continuous (with constant 2 on our examples)  it is sufﬁcient to discretize the interval
[0  1] to have a set of evenly spaced values {t1  ...  tC} (say  tj+1 − tj = ε0/2) to obtain an ε0-cover
{a(t1)  ...  a(tC)} of the possible costs. Using the approximate guarantee of Proposition 5  learning
a cost-sensitive classiﬁer for each a(ti) and selecting the one with optimal F -measure a posteriori
is sufﬁcient to obtain a M Φ(2ε0 + ε1)-optimal solution  where ε1 is the approximation guarantee of
the cost-sensitive classiﬁcation algorithm.
This meta-algorithm can be instantiated with any learning algorithm and different F -measures. In
our experiments of Section 4  we ﬁrst use it with cost-sensitive binary classiﬁcation algorithms: Sup-
port Vector Machines (SVMs) and logistic regression  both with asymmetric costs [2]  to optimize
the F1-measure in binary classiﬁcation and the macro-F1-score in multilabel classiﬁcation (training
one-vs-rest classiﬁers). Musicant et al. [14] also advocated for SVMs with asymmetric costs for
F1-measure optimization in binary classiﬁcation. However  their argument  speciﬁc to SVMs  is not
methodological but technical (relaxation of the maximization problem).

4 Experiments

The goal of this section is to give illustration of the algorithms suggested by the theory. First  our re-
sults suggest that cost-sensitive classiﬁcation algorithms may be preferable to the more usual proba-
bility thresholding method. We compare cost-sensitive classiﬁcation  as implemented by SVMs with
asymmetric costs  to thresholded logistic regression  with linear classiﬁers. Besides  the structured
SVM approach to F1-measure maximization SVMperf [11] provides another baseline. For complete-
ness  we also report results for thresholded SVMs  cost-sensitive logistic regression  and for the
thresholded versions of SVMperf and the cost-sensitive algorithms (a thresholded algorithm means
that the decision threshold is tuned a posteriori by maximizing the F1-score on the validation set).
Cost-sensitive SVMs and logistic regression (LR) differ in the loss they optimize (weighted hinge
loss for SVMs  weighted log-loss for LR)  and even though both losses are calibrated in the cost-
sensitive setting (that is  converging toward a Bayes-optimal classiﬁer as the number of examples and
the capacity of the class of function grow to inﬁnity) [22]  they behave differently on ﬁnite datasets
or with restricted classes of functions. We may also note that asymptotically  the Bayes-classiﬁer for

6

before thresholding

after thresholding

2
x

2
x

x1

x1

Figure 1: Decision boundaries for the galaxy dataset before and after thresholding the classiﬁer
scores of SVMperf (dotted  blue)  cost-sensitive SVM (dot-dashed  cyan)  logistic regression (solid 
red)  and cost-sensitive logistic regression (dashed  green). The horizontal black dotted line is an
optimal decision boundary.

a cost-sensitive binary classiﬁcation problem is a classiﬁer which thresholds the posterior probability
of being class 1. Thus  all methods but SVMperf are asymptotically equivalent  and our goal here is
to analyze their non-asymptotic behavior on a restricted class of functions.
Although our theoretical developments do not indicate any need to threshold the scores of classiﬁers 
the practical beneﬁts of a post-hoc adjustment of these scores can be important in terms of F1-
measure maximization. The reason is that the decision threshold given by cost-sensitive SVMs or
logistic regression might not be optimal in terms of the cost-sensitive 0/1-error  as already noted in
cost-sensitive learning scenarios [10  2]. This is illustrated in Figure 1  on the didactic “Galaxy”
distribution  consisting in four clusters of 2D-examples  indexed by z ∈ {1  2  3  4}  with prior
probability P(z = 1) = 0.01  P(z = 2) = 0.1  P(z = 3) = 0.001  and P(z = 4) = 0.889 
with respective class conditional probabilities P(y = 1|z = 1) = 0.9  P(y = 1|z = 2) = 0.09 
P(y = 1|z = 3) = 0.9  and P(y = 1|z = 4) = 0. We drew a very large sample (100 000 examples)
from the distribution  whose optimal F1-measure is 67.5%. Without tuning the decision threshold
of the classiﬁers  the best F1-measure among the classiﬁers is 55.3%  obtained by SVMperf  whereas
tuning thresholds enables to reach the optimal F1-measure for SVMperf and cost-sensitive SVM.
On the other hand  LR is severely affected by the non-linearity of the level sets of the posterior
probability distribution  and does not reach this limit (best F1-score of 48.9%). Note also that even
with this very large sample size  the SVM and LR classiﬁers are very different.
The datasets we use are Adult (binary classiﬁcation  32 561/16 281 train/test ex.  123 features) 
Letter (single label multiclass  26 classes  20 000 ex.  16 features)  and two text datasets: the
20 Newsgroups dataset News201 (single label multiclass  20 classes  15 935/3 993 train/test ex. 
62 061 features  scaled version) and Siam2 (multilabel  22 classes  21 519/7 077 train/test ex. 
30 438 features). All datasets except for News20 and Siam are obtained from the UCI repository3.
For each experiment  the training set was split at random  keeping 1/3 for the validation set used to
select all hyper-parameters  based on the maximization of the F1-measure on this set. For datasets
that do not come with a separate test set  the data was ﬁrst split to keep 1/4 for test. The algorithms
have from one to three hyper-parameters: (i) all algorithms are run with L2 regularization  with a
regularization parameter C ∈ {2−6  2−5  ...  26}; (ii) for the cost-sensitive algorithms  the cost for
false negatives is chosen in { 2−t
  t ∈ {0.1  0.2  ...  1.9}} of Proposition 6 4; (iii) for the thresholded
algorithms  the threshold is chosen among all the scores of the validation examples.

t

1http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/multiclass.

html#news20

2http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/multilabel.

html#siam-competition2007

3https://archive.ics.uci.edu/ml/datasets.html
4We take t greater than 1 in case the training asymmetry would be different from the true asymmetry [2].

7

−3−2−101201234−3−2−101201234Table 1: (macro-)F1-measures (in %). Options: T stands for thresholded  CS for cost-sensitive and
CS&T for cost-sensitive and thresholded.

–

67.3
52.5
59.5
49.4

CS
67.9
63.2
81.7
51.9

Baseline SVMperf SVMperf SVM SVM SVM LR LR
Options
CS
T
67.8 67.9
Adult
61.2 59.9
Letter
81.2 81.1
News20
53.9 53.8
Siam

CS&T
67.8
63.8
82.4
54.9

T
67.9
60.8
78.7
52.8

T

67.8
63.1
82.0
52.6

LR
CS&T
67.8
62.1
81.5
54.4

The library LibLinear [9] was used to implement SVMs5 and Logistic Regression (LR). A constant
feature with value 100 was added to each dataset to mimic an unregularized offset.
The results  averaged over ﬁve random splits  are reported in Table 1. As expected  the difference
between methods is less extreme than on the artiﬁcial “Galaxy” dataset. The Adult dataset is an
example where all methods perform nearly identically; the surrogate loss used in practice seems
unimportant. On the other datasets  we observe that thresholding has a rather large impact  and
especially for SVMperf; this is also true for the other classiﬁers: the unthresholded SVM and LR with
symmetric costs (unreported here) were not competitive as well. The cost-sensitive (thresholded)
SVM outperforms all other methods  as suggested by the theory. It is probably the method of choice
when predictive performance is a must.
On these datasets  thresholded LR behaves reasonably well considering its relatively low computa-
tional cost. Indeed  LR is much faster than SVM: in their thresholded cost-sensitive versions  the
timings for LR on News20 and Siam datasets are 6 400 and 8 100 seconds  versus 255 000 and
147 000 seconds for SVM respectively. Note that we did not try to optimize the running time in our
experiments. In particular  considerable time savings could be achieved by using warm-start.

5 Conclusion

We presented an analysis of F -measures  leveraging the property of pseudo-linearity of some of
them to obtain a strong non-asymptotic reduction to cost-sensitive classiﬁcation. The results hold
for any dataset and for any class of function. Our experiments on linear functions conﬁrm theory  by
demonstrating the practical interest of using cost-sensitive classiﬁcation algorithms rather than using
a simple probability thresholding. However  they also reveal that  for F -measure maximization 
thresholding the solutions provided by cost-sensitive algorithms further improves performances.
Algorithmically and empirically  we only explored the simplest case of our result (Fβ-measure in
binary classiﬁcation and macro-Fβ-measure in multilabel classiﬁcation)  but much more remains to
be done. First  the strategy we use for searching the optimal costs is a simple uniform discretization
procedure  and more efﬁcient exploration techniques could probably be developped. Second  al-
gorithms for the optimization of the micro-Fβ-measure in multilabel classiﬁcation received interest
recently as well [8  19]  but are for now limited to the selection of threshold after any kind of train-
ing. New methods for that measure may be designed from our reduction; we also believe that our
result can lead to progresses towards optimizing the micro-Fβ measure in multiclass classiﬁcation.

Acknowledgments

This work was carried out and funded in the framework of the Labex MS2T. It was supported by
the Picardy Region and the French Government  through the program “Investments for the future”
managed by the National Agency for Research (Reference ANR-11-IDEX-0004-02).

References
[1] N. Abe  B. Zadrozny  and J. Langford. An iterative method for multi-class cost-sensitive learning. In

W. Kim  R. Kohavi  J. Gehrke  and W. DuMouchel  editors  KDD  pages 3–11. ACM  2004.

[2] F. R. Bach  D. Heckerman  and E. Horvitz. Considering cost asymmetry in learning classiﬁers. J. Mach.

Learn. Res.  7:1713–1741  December 2006.

5The maximum number of iteration for SVMs was set to 50 000 instead of the default 1 000.

8

[3] A. Cambini and L. Martein. Generalized Convexity and Optimization  volume 616 of Lecture Notes in

Economics and Mathematical Systems. Springer  2009.

[4] W. Cheng  K. Dembczynski  E. H¨ullermeier  A. Jaroszewicz  and W. Waegeman. F-measure maximization
in topical classiﬁcation. In J. Yao  Y. Yang  R. Slowinski  S. Greco  H. Li  S. Mitra  and L. Polkowski 
editors  RSCTC  volume 7413 of Lecture Notes in Computer Science  pages 439–446. Springer  2012.

[5] K. Dembczynski  A. Jachnik  W. Kotlowski  W. Waegeman  and E. H¨ullermeier. Optimizing the F-
measure in multi-label classiﬁcation: Plug-in rule approach versus structured loss minimization.
In
S. Dasgupta and D. Mcallester  editors  Proceedings of the 30th International Conference on Machine
Learning (ICML-13)  volume 28  pages 1130–1138. JMLR Workshop and Conference Proceedings  May
2013.

[6] K. Dembczynski  W. Waegeman  W. Cheng  and E. H¨ullermeier. An exact algorithm for F-measure
maximization. In J. Shawe-Taylor  R. S. Zemel  P. L. Bartlett  F. C. N. Pereira  and K. Q. Weinberger 
editors  NIPS  pages 1404–1412  2011.

[7] C. Elkan. The foundations of cost-sensitive learning.

Intelligence  volume 17  pages 973–978  2001.

In International Joint Conference on Artiﬁcial

[8] R. E. Fan and C. J. Lin. A study on threshold selection for multi-label classiﬁcation. Technical report 

National Taiwan University  2007.

[9] R.-E. Fan  K.-W. Chang  C.-J. Hsieh  X.-R. Wang  and C.-J. Lin. Liblinear: A library for large linear

classiﬁcation. The Journal of Machine Learning Research  9:1871–1874  2008.

[10] Y. Grandvalet  J. Mari´ethoz  and S. Bengio. A probabilistic interpretation of SVMs with an application to

unbalanced classiﬁcation. In NIPS  2005.

[11] T. Joachims. A support vector method for multivariate performance measures. In Proceedings of the 22nd

International Conference on Machine Learning  pages 377–384. ACM Press  2005.

[12] J.-D. Kim  Y. Wang  and Y. Yasunori. The genia event extraction shared task  2013 edition - overview.
In Proceedings of the BioNLP Shared Task 2013 Workshop  pages 8–15  Soﬁa  Bulgaria  August 2013.
Association for Computational Linguistics.

[13] Z. C. Lipton  C. Elkan  and B. Naryanaswamy. Optimal thresholding of classiﬁers to maximize F1 mea-
sure. In T. Calders  F. Esposito  E. H¨ullermeier  and R. Meo  editors  Machine Learning and Knowledge
Discovery in Databases  volume 8725 of Lecture Notes in Computer Science  pages 225–239. Springer 
2014.

[14] D. R. Musicant  V. Kumar  and A. Ozgur. Optimizing F-measure with support vector machines.

Proceedings of the FLAIRS Conference  pages 356–360  2003.

In

[15] Y. Nan  K. M. A. Chai  W. S. Lee  and H. L. Chieu. Optimizing F-measures: A tale of two approaches.

In ICML. icml.cc / Omnipress  2012.

[16] J. Petterson and T. S. Caetano. Reverse multi-label learning. In NIPS  volume 1  pages 1912–1920  2010.
[17] J. Petterson and T. S. Caetano. Submodular multi-label learning. In NIPS  pages 1512–1520  2011.
[18] I. Pillai  G. Fumera  and F. Roli. F-measure optimisation in multi-label classiﬁers. In ICPR  pages 2424–

2427. IEEE  2012.

[19] I. Pillai  G. Fumera  and F. Roli. Threshold optimisation for multi-label classiﬁers. Pattern Recogn. 

46(7):2055–2065  July 2013.

[20] C. J. V. Rijsbergen. Information Retrieval. Butterworth-Heinemann  Newton  MA  USA  2nd edition 

1979.

[21] C. Scott. Calibrated asymmetric surrogate losses. Electronic Journal of Statistics  6:958–992  2012.
[22] I. Steinwart. How to compare different loss functions and their risks. Constructive Approximation 

26(2):225–287  2007.

[23] G. Tsoumakas and I. Katakis. Multi-label classiﬁcation: An overview. International Journal of Data

Warehousing and Mining (IJDWM)  3(3):1–13  2007.

[24] Z.-H. Zhou and X.-Y. Liu. On multi-class cost-sensitive learning. Computational Intelligence  26(3):232–

257  2010.

9

,Yacine Jernite
Yonatan Halpern
David Sontag
Shameem Puthiya Parambath
Nicolas Usunier
Yves Grandvalet
Dogyoon Song
Christina Lee
Yihua Li
Devavrat Shah