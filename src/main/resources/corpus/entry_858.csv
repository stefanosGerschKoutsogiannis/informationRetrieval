2018,BourGAN: Generative Networks with Metric Embeddings,This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens  we embed subsamples of the dataset from an arbitrary metric space into the L2 space  while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically  it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis  and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples  outperforming several recent GAN variants on a number of metrics and offering new features.,BourGAN: Generative Networks with Metric

Embeddings

Chang Xiao

Changxi Zheng

Peilin Zhong

Columbia University

{chang  peilin  cxz}@cs.columbia.edu

Abstract

This paper addresses the mode collapse for generative adversarial networks (GANs).
We view modes as a geometric structure of data distribution in a metric space.
Under this geometric lens  we embed subsamples of the dataset from an arbitrary
metric space into the `2 space  while preserving their pairwise distance distribution.
Not only does this metric embedding determine the dimensionality of the latent
space automatically  it also enables us to construct a mixture of Gaussians to draw
latent space random vectors. We use the Gaussian mixture model in tandem with a
simple augmentation of the objective function to train GANs. Every major step of
our method is supported by theoretical analysis  and our experiments on real and
synthetic data conﬁrm that the generator is able to produce samples spreading over
most of the modes while avoiding unwanted samples  outperforming several recent
GAN variants on a number of metrics and offering new features.

1

Introduction

In unsupervised learning  Generative Adversarial Networks (GANs) [1] is by far one of the most
widely used methods for training deep generative models. However  difﬁculties of optimizing GANs
have also been well observed [2  3  4  5  6  7  8]. One of the most prominent issues is mode collapse 
a phenomenon in which a GAN  after learning from a data distribution of multiple modes  generates
samples landed only in a subset of the modes. In other words  the generated samples lack the diversity
as shown in the real dataset  yielding a much lower entropy distribution.
We approach this challenge by questioning two fundamental properties of GANs. i) We question
the commonly used multivariate Gaussian that generates random vectors for the generator network.
We show that in the presence of separated modes  drawing random vectors from a single Gaussian
may lead to arbitrarily large gradients of the generator  and a better choice is by using a mixture of
Gaussians. ii) We consider the geometric interpretation of modes  and argue that the modes of a
data distribution should be viewed under a speciﬁc distance metric of data items – different metrics
may lead to different distributions of modes  and a proper metric can result in interpretable modes.
From this vantage point  we address the problem of mode collapse in a general metric space. To
our knowledge  despite the recent attempts of addressing mode collapse [3  9  10  6  11  12]  both
properties remain unexamined.

Technical contributions. We introduce BourGAN  an enhancement of GANs to avoid mode col-
lapse in any metric space. In stark contrast to all existing mode collapse solutions  BourGAN draws
random vectors from a Gaussian mixture in a low-dimensional latent space. The Gaussian mixture is
constructed to mirror the mode structure of the provided dataset under a given distance metric. We
derive the construction algorithm from metric embedding theory  namely the Bourgain Theorem [13].
Not only is using metric embeddings theoretically sound (as we will show)  it also brings signiﬁcant
advantages in practice. Metric embeddings enable us to retain the mode structure in the `2 latent space

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

X

Z

x1

x2

z1 z2
(a)

X

Z

(b)

X

Z

(c)

Figure 1: Multi-mode challenge. We train a generator G that maps a latent-space distribution Z to
the data distribution X with two modes. (a) Suppose Z is a Gaussian  and G can ﬁt both modes. If
we draw two i.i.d. samples z1  z2 from Z  then with at least a constant probability  G(z1) is close to
the center x1 of the ﬁrst mode  and G(z2) is close to another center x2. By the Mean Value Theorem 
there exists a z between z1 and z2 that has the absolute gradient value  |G0(z)| = | x2x1
z2z1 |  which can
be arbitrarily large  as |x2  x1| can be arbitrarily far. (b) Since G is Lipschitz continuous  using it to
map a Gaussian distribution to both modes unavoidably results in unwanted samples between the
modes (highlighted by the red dots). (c) Both challenges are resolved if we can construct a mixture
of Gaussian in latent space that captures the same modal structure as in the data distribution.

despite the metric used to measure modes in the dataset. In turn  the Gaussian mixture sampling in the
latent space eases the optimization of GANs  and unlike existing GANs that assume a user-speciﬁed
dimensionality of the latent space  our method automatically decides the dimensionality of the latent
space from the provided dataset.
To exploit the constructed Gaussian mixture for addressing mode collapse  we propose a simple
extension to the GAN objective that encourages the pairwise `2 distance of latent-space random
vectors to match the distance of the generated data samples in the metric space. That is  the
geometric structure of the Gaussian mixture is respected in the generated samples. Through a series
of (nontrivial) theoretical analyses  we show that if BourGAN is fully optimized  the logarithmic
pairwise distance distribution of its generated samples closely match the logarithmic pairwise distance
distribution of the real data items. In practice  this implies that mode collapse is averted.
We demonstrate the efﬁcacy of our method on both synthetic and real datasets. We show that our
method outperforms several recent GAN variants in terms of generated data diversity. In particular 
our method is robust to handle data distributions with multiple separated modes – challenging
situations where all existing GANs that we have experimented with produce unwanted samples (ones
that are not in any modes)  whereas our method is able to generate samples spreading over all modes
while avoiding unwanted samples.

2 Related Work

GANs and variants. The main goal of generative models in unsupervised learning is to produce
samples that follow an unknown distribution X   by learning from a set of unlabelled data items
i=1 drawn from X . In recent years  Generative Adversarial Networks (GANs) [1] have attracted
{xi}n
tremendous attention for training generative models. A GAN uses a neural network  called generator
G  to map a low-dimensional latent-space vector z 2 Rd  drawn from a standard distribution Z (e.g. 
a Gaussian or uniform distribution)  to generate data items in a space of interest such as natural images
and text. The generator G is trained in tandem with another neural network  called the discriminator
D  by solving a minmax optimization with the following objective.

Lgan(G  D) = Ex⇠X [log D(x)] + Ez⇠Z [log(1  D(G(z)))] .

(1)
This objective is minimized over G and maximized over D. Initially  GANs are demonstrated to
generate locally appreciable but globally incoherent images. Since then  they have been actively
improving. For example  DCGAN [8] proposes a class of empirically designed network architectures
that improve the naturalness of generated images. By extending the objective (1)  InfoGAN [14] is
able to learn interpretable representations in latent space  Conditional GAN [15] can produce more
realistic results by using additional supervised label. Several later variants have applied GANs to
a wide array of tasks [16  17] such as image-style transfer [18  19]  super-resolution [20]  image
manipulation [21]  video synthesis [22]  and 3D-shape synthesis [23]  to name a few.

Addressing difﬁculties. Despite tremendous success  GANs are generally hard to train. Prior
research has aimed to improve the stability of training GANs  mostly by altering its objective

2

function [24  4  25  26  27  28]. In a different vein  Salimans et al. [3] proposed a feature-matching
technique to stabilize the training process  and another line of work [5  6  29] uses an additional
network that maps generated samples back to latent vectors to provide feedback to the generator.
A notable problem of GANs is mode collapse  which is the focus of this work. For instance  when
trained on ten hand-written digits (using MNIST dataset) [30]  each digit represents a mode of data
distribution  but the generator often fails to produce a full set of the digits [25]. Several approaches
have been proposed to mitigate mode collapse  by modifying either the objective function [4  12]
or the network architectures [9  5  11  10  31]. While these methods are evaluated empirically 
theoretical understanding of why and to what extent these methods work is often lacking. More
recently  PacGAN [11] introduces a mathematical deﬁnition of mode collapse  which they used to
formally analyze their GAN variant. Very few previous works consider the construction of latent
space: VAE-GAN [29] constructs the latent space using variational autoencoder  and GLO [32] tries
to optimize both the generator network and latent-space representation using data samples. Yet  all
these methods still draw the latent random vectors from a multivariate Gaussian.

Differences from prior methods. Our approach differs from prior methods in several important
technical aspects. Instead of using a standard Gaussian to sample latent space  we propose to use
a Gaussian mixture model constructed using metric embeddings (e.g.  see [33  34  35] for metric
embeddings in both theoretical and machine learning fronts). Unlike all previous methods that require
the latent-space dimensionality to be speciﬁed a priori  our algorithm automatically determines
its dimensionality from the real dataset. Moreover  our method is able to incorporate any distance
metric  allowing the ﬂexibility of using proper metrics for learning interpretable modes. In addition
to empirical validation  the steps of our method are grounded by theoretical analysis.

3 Bourgain Generative Networks

We now introduce the algorithmic details of BourGAN  starting by describing the rationale behind the
proposed method. The theoretical understanding of our method will be presented in the next section.

Rationale and overview. We view modes in a dataset as a geometric structure embodied under a
speciﬁc distance metric. For example  in the widely tested MNIST dataset  only two modes emerge
under the pixel-wise `2 distance (Figure 2-left): images for the digit “1” are clustered in one mode 
while all other digits are landed in another mode. In contrast  under the classiﬁer distance metric
(deﬁned in Appendix F.3)  it appears that there exist 10 modes each corresponding to a different digit.
Consequently  the modes are interpretable (Figure 2-right). In this work  we aim to incorporate any
distance metric when addressing mode collapse  leaving the ﬂexibility of choosing a speciﬁc metric
to the user.
When there are multiple separated modes in a data distribution  mapping a Gaussian random variable
in latent space to the data distribution is fundamentally ill-posed. For example  as illustrated in
Figure 1-a and 1-b  this mapping imposes arbitrarily large gradients (at some latent space locations)
in the generator network  and large gradients render the generator unstable to train  as pointed out
by [37].
A natural choice is to use a mixture of Gaussians. As long as the Gaussian mixture is able to mirror
the mode structure of the given dataset  the problem of mapping it to the data distribution becomes
well-posed (Figure 1-c). To this end  our main idea is to use metric embeddings  one that map data
items under any metric to a low-dimensional `2 space with bounded pairwise distance distortion
(Section 3.3). After the embedding  we construct a Gaussian mixture in the `2 space  regardless of
the distance metric for the data items. In this process  the dimensionality of the latent space is also
automatically decided.
Our embedding algorithm  building upon the Bourgain Theorem  requires us to compute the pairwise
distances of data items  resulting in an O(n2) complexity  where n is the number of data items. When
n is large  we ﬁrst uniformly subsample m data items from the dataset to reduce the computational
cost of our metric embedding algorithm (Section 3.2). The subsampling step is theoretically sound:
we prove that when m is sufﬁciently large yet still much smaller than n  the geometric structure (i.e. 
the pairwise distance distribution) of data items is preserved in the subsamples.

3

012345678904812160.10.20.00.00.0060.0120.0010020004800.40.81.21.6Figure2:(Top)PairwisedistancedistributiononMNISTdatasetunderdifferentdistancemetrics.Left:`2distance Middle:EarthMover’sdistance(EMD)withaquadraticgroundmetric Right:classiﬁerdistance(deﬁnedinAppendixF.3).Under`2andEMDdistances fewseparatedmodesemerges andthepairwisedistancedistributionsresembleaGaussian.Undertheclassiﬁerdistance thepairwisedistancedistributionbecomesbimodal indicatingthatthereareseparatedmodes.(Bottom)t-SNEvisualization[36]ofdataitemsafterembeddedfromtheirmetricspaceto`2space.ColorindicateslabelsofMNISTimages(“1”-“9”).When`2distance(left)isused onlytwomodesareidentiﬁed:digit“1”andallothers butclassiﬁerdistance(right)cangroupdataitemsinto10individualmodes.Lastly whentrainingaBourGAN weencouragethegeometricstructureembodiedinthelatent-spaceGaussianmixturetobepreservedbythegeneratornetwork.Thereby themodestructureofthedatasetislearnedbythegenerator.ThisisrealizedbyaugmentingGAN’sobjectivetofosterthepreservationofthepairwisedistancedistributioninthetrainingprocess(Section3.4).3.1MetricsofDistanceandDistributionsBeforedelvingintoourmethod weintroduceafewtheoreticaltoolstoconcretizethegeometricstructureinadatadistribution pavingthewaytowardunderstandingouralgorithmicdetailsandsubsequenttheoreticalanalysis.Intherestofthispaper weborrowafewnotationalconventionsfromtheoreticalcomputerscience:weuse[n]todenotetheset{1 2 ··· n} R0todenotethesetofallnon-negativerealnumbers andlog(·)todenotelog2(·)forshort.Metricspace.Ametricspaceisdescribedbyapair(M d) whereMisasetandd:M⇥M!R0isadistancefunctionsuchthat8x y z2M wehavei)d(x y)=0 x=y ii)d(x y)=d(y x) andiii)d(x z)d(x y)+d(y z).IfMisaﬁniteset thenwecall(M d)aﬁnitemetricspace.Wasserstein-1distance.Wasserstein-1distance alsoknownastheEarth-Moverdistance isoneofthedistancemeasurestoquantifythesimilarityoftwodistributions deﬁnedasW(Pa Pb)=inf2⇧(Pa Pb)E(x y)⇠(|xy|) wherePaandPbaretwodistributionsonrealnumbers and⇧(Pa Pb)isthesetofalljointdistributions(x y)ontworealnumberswhosemarginaldistributionsarePaandPb respectively.Wasserstein-1distancehasbeenusedtoaugmentGAN’sobjectiveandimprovetrainingstability[4].Wewilluseittounderstandthetheoreticalguaranteesofourmethod.Logarithmicpairwisedistancedistribution(LPDD).Weproposetousethepairwisedistancedistributionofdataitemstoreﬂectthemodestructureinadataset(Figure2-top).Sincethepairwisedistanceismeasuredunderaspeciﬁcmetric itsdistributionalsodependsonthemetricchoice.Indeed ithasbeenusedin[9]toquantifyhowwellUnrolledGANaddressesmodecollapse.Concretely givenametricspace(M d) letXbeadistributionoverM and( ⇤)betworealvaluessatisfying0<2⇤.Considertwosamplesx yindependentlydrawnfromX andlet⌘bethelogarithmicdistancebetweenxandy(i.e. ⌘=log(d(x y))).Wecallthedistributionof⌘conditionedond(x y)2[ ⇤]the( ⇤)logarithmicpairwisedistancedistribution(LPDD)ofthe4distribution X . Throughout our theoretical analysis  LPDD of the distributions generated at various
steps of our method will be measured in Wasserstein-1 distance.
Remark. We choose to use logarithmic distance in order to reasonably compare two pairwise distance
distributions. The rationale is illustrated in Figure 6 in the appendix. Using logarithmic distance is
also beneﬁcial for training our GANs  which will become clear in Section 3.4. The (  ⇤) values in the
above deﬁnition are just for the sake of theoretical rigor  irrelevant from our practical implementation.
They are meant to avoid the theoretical situation where two samples are identical and then taking the
logarithm becomes no sense. In this section  the reader can skip these values and refer back when
reading our theoretical analysis (in Section 4 and the supplementary material).

3.2 Preprocessing: Subsample of Data Items
We now describe how to train BourGAN step by step. Provided with a multiset of data items
i=1 drawn independently from an unknown distribution X   we ﬁrst subsample m (m < n)
X = {xi}n
data items uniformly at random from X. This subsampling step is essential  especially when n is
large  for reducing the computational cost of metric embeddings as well as the number of dimensions
of the latent space (both described in Section 3.3). From now on  we use Y to denote the multiset of
data items subsampled from X (i.e.  Y ✓ X and |Y | = m). Elements in Y will be embedded in `2
space in the next step.
The subsampling strategy  while simple  is theoretically sound. Let P be the (  ⇤)-LPDD of the data
distribution X   and P0 be the LPDD of the uniform distribution on Y . We will show in Section 4 that
their Wasserstein-1 distance W (P P0) is tightly bounded if m is sufﬁciently large but much smaller
than n. In other words  the mode structure of the real data can be captured by considering only the
subsamples in Y . In practice  m is chosen automatically by a simple algorithm  which we describe in
Appendix F.1. In all our examples  we ﬁnd m = 4096 sufﬁcient.

3.3 Construction of Gaussian Mixture in Latent Space
Next  we construct a Gaussian mixture model for generating random vectors in latent space. First  we
embed data items from Y to an `2 space  one that the latent random vectors reside in. We want the
latent vector dimensionality to be small  while ensuring that the mode structure be well reﬂected in the
latent space. This requires the embedding to introduce minimal distortion on the pairwise distances of
data items. For this purpose  we propose an algorithm that leverages Bourgain’s embedding theorem.

Metric embeddings. Bourgain [13] introduced a method that can embeds any ﬁnite metric space
into a small `2 space with minimal distortion. The theorem is stated as follows:
Theorem 1 (Bourgain’s theorem). Consider a ﬁnite metric space (Y  d) with m = |Y |. There exists a
mapping g : Y ! Rk for some k = O(log2 m) such that 8y  y0 2 Y  d(y  y0)  kg(y)  g(y0)k2 
↵ · d(y  y0)  where ↵ is a constant satisfying ↵  O(log m).
The mapping g is constructed using a randomized algorithm also given by Bourgain [13]. Directly
applying Bourgain’s theorem results in a latent space of O(log2 m) dimensions. We can further
reduce the number of dimensions down to O(log m) through the following corollary.
Corollary 2 (Improved Bourgain embedding). Consider a ﬁnite metric space (Y  d) with m = |Y |.
There exist a mapping f : Y ! Rk for some k = O(log m) such that 8y  y0 2 Y  d(y  y0) 
kf (y)  f (y0)k2  ↵ · d(y  y0)  where ↵ is a constant satisfying ↵  O(log m).
Proved in Appendix B  this corollary is obtained by combining Theorem 1 with the Johnson-
Lindenstrauss (JL) lemma [38]. The mapping f is computed through a combination of the algorithms
for Bourgain’s theorem and the JL lemma. This algorithm of computing f is detailed in Appendix A.
Remark. Instead of using Bourgain embedding  one can ﬁnd a mapping f : Y ! Rk with bounded
distortion  namely  8y  y0 2 Y  d(y  y0)  kf (y)  f (y0)k2  ↵ · d(y  y0)  by solving a semideﬁnite
programming problem (e.g.  see [39  33]). This approach can ﬁnd an embedding with the least distor-
tion ↵. However  solving semideﬁnite programming problem is much more costly than computing
Bourgain embeddings. Even if the optimal distortion factor ↵ is found  it can still be as large as
O(log m) in the worst case [40]. Indeed  Bourgain embedding is optimal in the worst case.
Using the mapping f  we embed data items from Y (denoted as {yi}m
sions (k = O(log m)). Let F be the multiset of the resulting vectors in Rk (i.e.  F = {f (yi)}m

i=1) into the `2 space of k dimen-
i=1).

5

As we will formally state in Section 4  the Wasserstein-1 distance between the (  ⇤)LPDD of the
real data distribution X and the LPDD of the uniform distribution on F is tightly bounded. Simply
speaking  the mode structure in the real data is well captured by F in `2 space.

Latent-space Gaussian mixture. Now  we construct a distribution using F to draw random vectors
in latent space. A simple choice is the uniform distribution over F   but such a distribution is not
continuous over the latent space. Instead  we construct a mixture of Gaussians  each of which is
centered at a vector f (yi) in F . In particular  we generate a latent vector z 2 Rk in two steps: We ﬁrst
sample a vector µ 2 F uniformly at random  and then draw a vector z from the Gaussian distribution
N (µ  2)  where  is a smoothing parameter that controls the smoothness of the distribution of the
latent space. In practice  we choose  empirically ( = 0.1 for all our examples). We discuss our
choice of  in Appendix F.1.
i=1).
Remark. By this deﬁnition  the Gaussian mixture consists of m Gaussians (recall F = {f (yi)}m
But this does not mean that we construct m “modes” in the latent space. If two Gaussians are close
to each other in the latent space  they should be viewed as if they are from the same mode. It is
the overall distribution of the m Gaussians that reﬂects the distribution of modes. In this sense  the
number of modes in the latent space is implicitly deﬁned  and the m Gaussians are meant to enable
us to sample the modes in the latent space.

3.4 Training
The Gaussian mixture distribution Z in the latent space guarantees that the LPDD of Z is close
to (  ⇤)LPDD of the target distribution X (shown in Section 4). To exploit this property for
avoiding mode collapse  we encourage the generator network to match the pairwise distances of
generated samples with the pairwise `2 distances of latent vectors in Z. This is realized by a simple
augmentation of the GAN’s objective function  namely 

L(G  D) = Lgan + Ldist 

where Ldist(G) = Ezi zj⇠Zh(log(d(G(zi)  G(zj)))  log(kzi  zjk2))2i  

(2)
(3)

Lgan is the objective of the standard GAN in Eq. (1)  and  is a parameter to balance the two terms.
In Ldist  zi and zj are two i.i.d. samples from Z conditioned on zi 6= zj. Here the advantages of
using logarithmic distances are threefold: i) When there exists “outlier” modes that are far away
from others  logarithmic distance prevents those modes from being overweighted in the objective.
ii) Logarithm turns a uniform scale of the distance metric into a constant addend that has no effect
to the optimization. This is desired as the structure of modes is invariant under a uniform scale of
distance metric. iii) Logarithmic distances ease our theoretical analysis  which  as we will formalize
in Section 4  states that when Eq. (3) is optimized  the distribution of generated samples will closely
resemble the real distribution X . That is  mode collapse will be avoided.
In practice  when experimenting with real datasets  we ﬁnd that a simple pre-training step using the
correspondence between {yi}m
i=1 helps to improve the training stability. Although
not a focus of this paper  this step is described in Appendix C.

i=1 and {f (yi)}m

4 Theoretical Analysis

This section offers an theoretical analysis of our method presented in Section 3. We will state the main
theorems here while referring to the supplementary material for their rigorous proofs. Throughout 
we assume a property of the data distribution X : if two samples  a and b  are drawn independently
from X   then with a high probability (> 1/2) they are distinct (i.e.  Pra b⇠X (a 6= b)  1/2).
Range of pairwise distances. We ﬁrst formalize our deﬁnition of (  ⇤)LPDD in Section 3.1.
Recall that the multiset X = {xi}n
i=1 is our input dataset regarded as i.i.d. samples from X . We
would like to ﬁnd a range [  ⇤] such that the pairwise distances of samples from X is in this range
with a high probability (see Example-7 and -8 in Appendix D). Then  when considering the LPDD of
X   we account only for the pairwise distances in the range [  ⇤] so that the logarithmic pairwise
distance is well deﬁned. The values  and ⇤ are chosen by the following theorem  which we prove in
Appendix G.2.

6

Theorem3.Let=mini2[n1]:xi6=xi+1d(xi xi+1)and⇤=maxi2[n1]d(xi xi+1).8 2(0 1) ifnC/()forsomesufﬁcientlylargeconstantC>0 thenwithprobabilityatleast1 Pra b⇠X(d(a b)2[ ⇤]| ⇤)Pra b⇠X(a6=b).Simplyspeaking thistheoremstatesthatifwechooseand⇤asdescribedabove thenwehavePra b⇠X(d(a b)2[ ⇤]|a6=b)1O(1/n) meaningthatifnislarge thepairwisedistanceofanytwoi.i.d.samplesfromXisalmostcertainlyintherange[ ⇤].Therefore ( ⇤)LPDDisareasonablemeasureofthepairwisedistancedistributionofX.Inthispaper wealwaysusePtodenotethe( ⇤)LPDDoftherealdatadistributionX.Numberofsubsamples.Withthechoicesofand⇤ wehavethefollowingtheoremtoguaranteethesoundnessofoursubsamplingstepdescribedinSection3.2.Theorem4.LetY={yi}mi=1beamultisetofm=logO(1)(⇤/)·log(1/)i.i.d.samplesdrawnfromX andletP0betheLPDDoftheuniformdistributiononY.Forany2(0 1) withprobabilityatleast1 wehaveW(P P0)O(1).ProvedinAppendixG.3 thistheoremstatesthatweonlyneedm(ontheorderoflogO(1)(⇤/))subsamplestoformamultisetYthatwellcapturesthemodestructureintherealdata.2460.61.20FrequencyLog PairwiseDistance8Figure3:LPDDofuniformdistri-butionF(orange)andofsamplesfromGaussianmixture(blue).Discretelatentspace.Next welayatheoreticalfoundationforourmetricembeddingstepdescribedinSection3.3.RecallthatFisthemultisetofvectorsresultedfromembeddingdataitemsfromYtothe`2space(i.e. F={f(yi)}mi=1).AsprovedinAppendixG.4 wehave:Theorem5.LetFbetheuniformdistributiononthemultisetF.Thenwithprobabilityatleast0.99 wehaveW(P ˆP)O(logloglog(⇤/)) whereˆPistheLPDDofF.Herethetriple-logfunction(logloglog(⇤/))indicatesthattheWassersteindistanceboundcanbeverytight.AlthoughthistheoremstatesabouttheuniformdistributiononF notpreciselytheGaussianmixtureweconstructed itisaboutthecasewhenoftheGaussianmixtureapproacheszero.WealsoempiricallyveriﬁedtheconsistencyofLPDDfromGaussianmixturesamples(Figure3).GANobjective.Next wetheoreticallyjustifytheobjectivefunction(i.e. Eq.(3)inSection3.4).Let˜XbethedistributionofgeneratedsamplesG(z)forz⇠Zand˜Pbethe( ⇤)LPDDof˜X.Goodfellowetal.[1]showedthattheglobaloptimumoftheGANobjective(1)isreachedifandonlyif˜X=X.Then whenthisoptimumisachieved wemustalsohaveW(P ˜P)=0andW(˜P ˆP)O(logloglog(⇤/)).ThelatterisbecauseW(P ˆP)O(logloglog(⇤/))fromTheorem5.Asaresult theGAN’sminmaxproblem(1)isequivalenttotheconstrainedminmaxproblem minGmaxDLgan(G D) subjecttoW(˜P ˆP) whereisontheorderofO(logloglog(⇤/)).Apparently thisconstraintrenderstheminmaxproblemharder.Wethereforeconsidertheminmaxproblem minGmaxDLgan(G D) subjectedtoslightlystrengthenedconstraints 8z16=z22supp(Z) d(G(z1) G(z2))2[ ⇤] and(4)[log(d(G(z1) G(z2)))logkz1z2k2]22.(5)AsprovedinAppendixE iftheaboveconstraintsaresatisﬁed thenW(˜P ˆP)isautomaticallysatisﬁed.Inourtrainingprocess weassumethattheconstraint(4)isautomaticallysatisﬁed supportedbyTheorem3.Lastly insteadofusingEq.(5)asahardconstraint wetreatitasasoftconstraintshowingupintheobjectivefunction(3).Fromthisperspective thesecondterminourproposedobjective(2)canbeinterpretedasaLagrangemultiplieroftheconstraint.LPDDofthegeneratedsamples.Now ifthegeneratornetworkistrainedtosatisfythecon-straint(5) wehaveW(˜P ˆP)O(logloglog(⇤/)).NotethatthissatisfactiondoesnotimplythattheglobaloptimumoftheGANinEq.(1)hastobereached–suchaglobaloptimumishardtoachieveinpractice.Finally usingthetriangleinequalityoftheWasserstein-1distanceandTheorem5 wereachtheconclusionthatW(˜P P)W(˜P ˆP)+W(P ˆP)O(logloglog(⇤/)).(6)72D Ring2D Grid2D CircleGANUnrolled GANVEEGANPacGANBourGANTargetFigure4:Syntheticdatatests.Inallthreetests ourmethodclearlycapturesallthemodespresentedinthetargets whileproducingnounwantedsampleslocatedbetweentheregionsofmodes.ThismeansthattheLPDDofgeneratedsamplescloselyresemblesthatofthedatadistribution.Toputtheboundinaconcretecontext inExample9ofAppendixD weanalyzeatoycaseinathoughtexperimenttoshow ifthemodecollapseoccurs(evenpartially) howlargeW(˜P P)wouldbeincomparisontoourtheoreticalboundhere.5ExperimentsThissectionpresentstheempiricalevaluationsofourmethod.TherehasnotbeenaconsensusonhowtoevaluateGANsinthemachinelearningcommunity[41 42] andquantitativemeasureofmodecollapseisalsonotstraightforward.Wethereforeevaluateourmethodusingbothsyntheticandrealdatasets mostofwhichhavebeenusedbyrecentGANvariants.WereferthereadertoAppendixFfordetailedexperimentsetupsandcompleteresults whilehighlightingourmainﬁndingshere.Overview.Westartwithanoverviewofourexperiments.i)Onsyntheticdatasets wequantitativelycompareourmethodwithfourtypesofGANs includingtheoriginalGAN[1]andmorerecentVEEGAN[10] UnrolledGANs[9] andPacGAN[11] followingtheevaluationmetricsusedbythosemethods(AppendixF.2).ii)Wealsoexamineineachmodehowwellthedistributionofgeneratedsamplesmatchesthedatadistribution(AppendixF.2)–anewtestnotpresentedpreviously.iii)WecomparethetrainingconvergencerateofourmethodwithexistingGANs(AppendixF.2) examiningtowhatextenttheGaussianmixturesamplingisbeneﬁcial.iv)WechallengeourmethodwiththedifﬁcultstackedMNISTdataset(AppendixF.3) testinghowmanymodesitcancover.v)Mostnotably weexamineifthereare“falsepositive”samplesgeneratedbyourmethodandothers(Figure4).Thoseareunwantedsamplesnotlocatedinanymodes.Inallthesecomparisons weﬁndthatBourGANclearlyproduceshigher-qualitysamples.Inaddition weshowthatvi)ourmethodisabletoincorporatedifferentdistancemetrics onesthatleadtodifferentmodeinterpretations(AppendixF.3);andvii)ourpre-trainingstep(describedinAppendixC)furtheracceleratesthetrainingconvergenceinrealdatasets(AppendixF.2).Lastly viii)wepresentsomequalitativeresults(AppendixF.4).2DRing2DGrid2DCircle#modes(max8)W1lowquality#modes(max25)W1lowqualitycentercapturedW1lowqualityGAN1.038.600.06%17.71.61717.70%No32.590.14%Unrolled7.64.67812.03%14.92.23195.11%No0.3600.50%VEEGAN8.04.90413.23%24.40.83622.84%Yes0.46610.72%PacGAN7.81.4121.79%24.30.89820.54%Yes0.2631.38%BourGAN8.00.6870.12%25.00.2484.09%Yes0.0810.35%Table1:StatisticsofExperimentsonSyntheticDatasets8Figure 5: Qualitative results on CelebA dataset using DCGAN (Left) and BourGAN (Right) under
`2 metric. It appears that DCGAN generates some samples that are visually more implausible (e.g. 
red boxes) in comparison to BourGAN. Results are fairly sampled at random  not cherry-picked.

Quantitative evaluation. We compare BourGAN with other methods on three synthetic datasets:
eight 2D Gaussian distributions arranged in a ring (2D Ring)  twenty-ﬁve 2D Gaussian distributions
arranged in a grid (2D Grid)  and a circle surrounding a Gaussian placed in the center (2D Circle). The
ﬁrst two were used in previous methods [9  10  11]  and the last is proposed by us. The quantitative
performance of these methods are summarized in Table 1  where the column “# of modes” indicates
the average number of modes captured by these methods  and “low quality” indicates number of
samples that are more than 3⇥ standard deviations away from the mode centers. Both metrics are
used in previous methods [10  11]. For the 2D circle case  we also check if the central mode is
captured by the methods. Notice that all these metrics measure how many modes are captured  but
not how well the data distribution is captured. To understand this  we also compute the Wasserstein-1
distances between the distribution of generated samples and the data distribution (reported in Table 1).
It is evident that our method performs the best on all these metrics (see Appendix F.2 for more
details).

Avoiding unwanted samples. A notable advantage offered by our method is the ability to avoid
unwanted samples  ones that are located between the modes. We ﬁnd that all the four existing GANs
suffer from this problem (see Figure 4)  because they use Gaussian to draw latent vectors (recall
Figure 1). In contrast  our method generates no unwanted samples in all three test cases. We refer
the reader to Appendix F.3 for a detailed discussion of this feature and several other quantitative
comparisons.

Qualitative results. We further test our algorithm on real image datasets. Figure 5 illustrates
a qualitative comparison between DCGAN and our method  both using the same generator and
discriminator architectures and default hyperparameters. Appendix F.4 includes more experiments
and details.

6 Conclusion

This paper introduces BourGAN  a new GAN variant aiming to address mode collapse in generator
networks. In contrast to previous approaches  we draw latent space vectors using a Gaussian mixture 
which is constructed through metric embeddings. Supported by theoretical analysis and experiments 
our method enables a well-posed mapping between latent space and multi-modal data distributions.
In future  our embedding and Gaussian mixture sampling can also be readily combined with other
GAN variants and even other generative models to leverage their advantages.

Acknowledgements
We thank Daniel Hsu  Carl Vondrick and Henrique Maia for the helpful feedback. Chang Xiao
and Changxi Zheng are supported in part by the National Science Foundation (CAREER-1453101 
1717178 and 1816041) and generous donations from SoftBank and Adobe. Peilin Zhong is supported
in part by National Science Foundation (CCF-1703925  CCF-1421161  CCF-1714818  CCF-1617955
and CCF-1740833)  Simons Foundation (#491119 to Alexandr Andoni) and Google Research Award.

9

References
[1] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[2] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems  pages 271–279  2016.

[3] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems 
pages 2234–2242  2016.

[4] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein gan. arXiv preprint

arXiv:1701.07875  2017.

[5] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Olivier Mastropietro  Alex Lamb  Martin Ar-
jovsky  and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704 
2016.

[6] Jeff Donahue  Philipp Krähenbühl  and Trevor Darrell. Adversarial feature learning. arXiv

preprint arXiv:1605.09782  2016.

[7] Scott Reed  Zeynep Akata  Xinchen Yan  Lajanugen Logeswaran  Bernt Schiele  and Honglak
Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396  2016.

[8] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[9] Luke Metz  Ben Poole  David Pfau  and Jascha Sohl-Dickstein. Unrolled generative adversarial

networks. arXiv preprint arXiv:1611.02163  2016.

[10] Akash Srivastava  Lazar Valkoz  Chris Russell  Michael U Gutmann  and Charles Sutton.
Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in
Neural Information Processing Systems  pages 3310–3320  2017.

[11] Zinan Lin  Ashish Khetan  Giulia Fanti  and Sewoong Oh. Pacgan: The power of two samples

in generative adversarial networks. arXiv preprint arXiv:1712.04086  2017.

[12] Tong Che  Yanran Li  Athul Paul Jacob  Yoshua Bengio  and Wenjie Li. Mode regularized

generative adversarial networks. arXiv preprint arXiv:1612.02136  2016.

[13] Jean Bourgain. On lipschitz embedding of ﬁnite metric spaces in hilbert space. Israel Journal

of Mathematics  52(1-2):46–52  1985.

[14] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems  pages 2172–2180  2016.

[15] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint

arXiv:1411.1784  2014.

[16] Ashish Bora  Eric Price  and Alexandros G Dimakis. Ambientgan: Generative models from
lossy measurements. In International Conference on Learning Representations (ICLR)  2018.

[17] Ashish Bora  Ajil Jalal  Eric Price  and Alexandros G Dimakis. Compressed sensing using

generative models. arXiv preprint arXiv:1703.03208  2017.

[18] Phillip Isola  Jun-Yan Zhu  Tinghui Zhou  and Alexei A Efros. Image-to-image translation with

conditional adversarial networks. arXiv preprint  2017.

[19] Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A Efros. Unpaired image-to-image
translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593  2017.

10

[20] Christian Ledig  Lucas Theis  Ferenc Huszár  Jose Caballero  Andrew Cunningham  Alejandro
Acosta  Andrew Aitken  Alykhan Tejani  Johannes Totz  Zehan Wang  et al. Photo-realistic
single image super-resolution using a generative adversarial network. arXiv preprint  2016.

[21] Jun-Yan Zhu  Philipp Krähenbühl  Eli Shechtman  and Alexei A Efros. Generative visual
manipulation on the natural image manifold. In European Conference on Computer Vision 
pages 597–613. Springer  2016.

[22] Carl Vondrick  Hamed Pirsiavash  and Antonio Torralba. Generating videos with scene dynamics.

In Advances In Neural Information Processing Systems  pages 613–621  2016.

[23] Jiajun Wu  Chengkai Zhang  Tianfan Xue  Bill Freeman  and Josh Tenenbaum. Learning a
probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances
in Neural Information Processing Systems  pages 82–90  2016.

[24] Xudong Mao  Qing Li  Haoran Xie  Raymond YK Lau  Zhen Wang  and Stephen Paul Smolley.
Least squares generative adversarial networks. In 2017 IEEE International Conference on
Computer Vision (ICCV)  pages 2813–2821. IEEE  2017.

[25] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems 
pages 5769–5779  2017.

[26] Junbo Zhao  Michael Mathieu  and Yann LeCun. Energy-based generative adversarial network.

arXiv preprint arXiv:1609.03126  2016.

[27] Yunus Saatci and Andrew G Wilson. Bayesian gan.

processing systems  pages 3622–3631  2017.

In Advances in neural information

[28] Sanjeev Arora  Rong Ge  Yingyu Liang  Tengyu Ma  and Yi Zhang. Generalization and

equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573  2017.

[29] Anders Boesen Lindbo Larsen  Søren Kaae Sønderby  Hugo Larochelle  and Ole Winther.
Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300 
2015.

[30] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[31] Tero Karras  Timo Aila  Samuli Laine  and Jaakko Lehtinen. Progressive growing of gans for

improved quality  stability  and variation. arXiv preprint arXiv:1710.10196  2017.

[32] Piotr Bojanowski  Armand Joulin  David Lopez-Paz  and Arthur Szlam. Optimizing the latent

space of generative networks. arXiv preprint arXiv:1707.05776  2017.

[33] Jiˇrí Matoušek. Embedding ﬁnite metric spaces into normed spaces. In Lectures on Discrete

Geometry  pages 355–400. Springer  2002.

[34] Nicolas Courty  Rémi Flamary  and Mélanie Ducoffe. Learning wasserstein embeddings. arXiv

preprint arXiv:1710.07457  2017.

[35] Piotr Indyk and Jirı Matoušek. Low-distortion embeddings of ﬁnite metric spaces. Handbook

of discrete and computational geometry  37:46  2004.

[36] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine

learning research  9(Nov):2579–2605  2008.

[37] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In

Advances in Neural Information Processing Systems  pages 5591–5600  2017.

[38] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert

space. Contemporary mathematics  26(189-206):1  1984.

[39] Nathan Linial  Eran London  and Yuri Rabinovich. The geometry of graphs and some of its

algorithmic applications. Combinatorica  15(2):215–245  1995.

11

[40] Tom Leighton and Satish Rao. An approximate max-ﬂow min-cut theorem for uniform multi-
commodity ﬂow problems with applications to approximation algorithms. In Foundations of
Computer Science  1988.  29th Annual Symposium on  pages 422–431. IEEE  1988.

[41] Lucas Theis  Aäron van den Oord  and Matthias Bethge. A note on the evaluation of generative

models. arXiv preprint arXiv:1511.01844  2015.

[42] Ali Borji. Pros and cons of gan evaluation measures. arXiv preprint arXiv:1802.03446  2018.
[43] Chun-Liang Li  Wei-Cheng Chang  Yu Cheng  Yiming Yang  and Barnabás Póczos. Mmd
gan: Towards deeper understanding of moment matching network. In Advances in Neural
Information Processing Systems  pages 2200–2210  2017.

[44] Ilya O Tolstikhin  Sylvain Gelly  Olivier Bousquet  Carl-Johann Simon-Gabriel  and Bernhard
Schölkopf. Adagan: Boosting generative models. In Advances in Neural Information Processing
Systems  pages 5430–5439  2017.

[45] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[46] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine
Learning Research  15(1):1929–1958  2014.

[47] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. arXiv preprint arXiv:1502.03167  2015.

[48] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[49] Friedrich Pukelsheim. The three sigma rule. The American Statistician  48(2):88–91  1994.
[50] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[51] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-mnist: a novel image dataset for

benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747  2017.

12

,Matthias Hein
Simon Setzer
Leonardo Jost
Syama Sundar Rangapuram
Tim Salimans
Ian Goodfellow
Wojciech Zaremba
Vicki Cheung
Alec Radford
Xi Chen
Chang Xiao
Peilin Zhong
Changxi Zheng