2019,Can SGD Learn Recurrent Neural Networks with Provable Generalization?,Recurrent Neural Networks (RNNs) are among the most popular models in sequential data analysis. Yet  in the foundational PAC learning language  what concept class can it learn? Moreover  how can the same recurrent unit simultaneously learn functions from different input tokens to different output tokens  without affecting each other?
Existing generalization bounds for RNN scale exponentially with the input length  significantly limiting their practical implications.

In this paper  we show using the vanilla stochastic gradient descent (SGD)  RNN can actually learn some notable concept class \emph{efficiently}  meaning that both time and sample complexity scale \emph{polynomially} in the input length (or almost polynomially  depending on the concept).
This concept class at least includes functions where each output token is generated from inputs of earlier tokens using a smooth two-layer neural network.,Can SGD Learn Recurrent Neural Networks

with Provable Generalization?∗

Zeyuan Allen-Zhu
Microsoft Research AI

zeyuan@csail.mit.edu

Yuanzhi Li

Carnegie Mellon University
yuanzhil@andrew.cmu.edu

Abstract

Recurrent Neural Networks (RNNs) are among the most popular models in se-
quential data analysis. Yet  in the foundational PAC learning language  what con-
cept class can it learn? Moreover  how can the same recurrent unit simultaneously
learn functions from different input tokens to different output tokens  without af-
fecting each other? Existing generalization bounds for RNN scale exponentially
with the input length  signiﬁcantly limiting their practical implications.
In this paper  we show using the vanilla stochastic gradient descent (SGD)  RNN
can actually learn some notable concept class efﬁciently  meaning that both time
and sample complexity scale polynomially in the input length (or almost polyno-
mially  depending on the concept). This concept class at least includes functions
where each output token is generated from inputs of earlier tokens using a smooth
two-layer neural network.

Introduction

1
Recurrent neural networks (RNNs) is one of the most popular models in sequential data analy-
sis [25]. When processing an input sequence  RNNs repeatedly and sequentially apply the same
operation to each input token. The recurrent structure of RNNs allows it to capture the dependen-
cies among different tokens inside each sequence  which is empirically shown to be effective in
many applications such as natural language processing [28]  speech recognition [12] and so on.
The recurrent structure in RNNs shows great power in practice  however  it also imposes great
challenge in theory. Until now  RNNs remains to be one of the least theoretical understood models
in deep learning. Many fundamental open questions are still largely unsolved in RNNs  including

1. (Optimization). When can RNNs be trained efﬁciently?
2. (Generalization). When do the results learned by RNNs generalize to test data?

Question 1 is technically challenging due to the notorious question of vanishing/exploding gradients 
and the non-convexity of the training objective induced by non-linear activation functions.
Question 2 requires even deeper understanding of RNNs. For example  in natural language process-
ing  “Juventus beats Bacerlona” and “Bacerlona beats Juventus” have completely different mean-
ings. How can the same operation in RNN encode a different rule for “Juventus” at token 1 vs.
“Juventus” at token 3  instead of merely memorizing each training example?
There have been some recent progress towards obtaining more principled understandings of these
questions. On the optimization side  Hardt  Ma  and Recht [13] show that over-parameterization
can help in the training process of a linear dynamic system  which is a special case of RNNs with
linear activation functions. Allen-Zhu  Li  and Song [2] show that over-parameterization also helps
in training RNNs with ReLU activations. This latter result gives no generalization guarantee.

∗Full version and future updates can be found on https://arxiv.org/abs/1902.01028.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

On the generalization side  our understanding to RNN is even more limited. The VC-dimension
bounds [10] and [17] polynomially depend on the size of the network  and either only apply to
linear (or threshold) networks or to networks with one dimension input. However  a bound scaling
with the total number of parameters usually cannot be applied to modern neural networks  which are
heavily over-parameterized. Others [9  31] (or the earlier work [14]) establish sample complexity
bounds that exponentially grow in the input length. In particular  they depend on the operator norm
of the recurrent unit  that we denote by β. If β > 1  their bounds scale exponentially with input
length. Since most applications do not regularize β and allow β > 1 for a richer expressibility 2
their bounds are still insufﬁcient.
Indeed  bridging the gap between optimization (question 1) and generalization (question 2) can be
quite challenging in neural networks. The case of RNN is particularly so due to the (potentially)
exponential blowup in input length.

• Generalization (cid:57) Optimization. One could imagine adding a strong regularizer to ensure
β ≤ 1 for generalization purpose; however  it is unclear how an optimization algorithm such
as stochastic gradient descent (SGD) ﬁnds a network that both minimizes training loss and
maintains β ≤ 1. One could also use a very small network so the number of parameters is
limited; however  it is not clear how SGD ﬁnds a small network with small training loss.
• Optimization (cid:57) Generalization. One could try to train RNNs without any regularization;
however  it is then quite possible that the number of parameters need to be large and β > 1
after the training. This is so both in practice (since “memory implies larger spectral radius”
[24]) and in theory [2]. All known generalization bounds fail to apply in this regime.

In this paper  we give arguably the ﬁrst theoretical analysis of RNNs that captures optimization
and generalization simultaneously. Given any set of input sequences  as long as the outputs are
(approximately) realizable by some smooth function in a certain concept class  then after training
a vanilla RNN with ReLU activations  SGD provably ﬁnds a solution that has both small training
and generalization error. Our result allows β to be larger than 1 by a constant  but is still efﬁcient:
meaning that the iteration complexity of the SGD  the sample complexity  and the time complexity
scale only polynomially (or almost polynomially) with the length of the input.

2 Notations
We denote by (cid:107)·(cid:107)2 (or sometimes (cid:107)·(cid:107)) the Euclidean norm of vectors  and by (cid:107)·(cid:107)2 the spectral norm
of matrices. We denote by (cid:107)·(cid:107)∞ the inﬁnite norm of vectors  (cid:107)·(cid:107)0 the sparsity of vectors or diagonal
matrices  and (cid:107)·(cid:107)F the Frobenius norm of matrices. Given matrix W   we denote by Wk or wk the k-

th row vector of W . We denote the row (cid:96)p norm for W ∈ Rm×d as (cid:107)W(cid:107)2 p :=(cid:0)(cid:80)

(cid:1)1/p.

i∈[m] (cid:107)wi(cid:107)p

2

j )vi

(cid:81)i−1
j=1(I−(cid:98)vj(cid:98)v(cid:62)
(cid:107)(cid:81)i−1
j=1(I−(cid:98)vj(cid:98)v(cid:62)

By deﬁnition  (cid:107)W(cid:107)2 2 = (cid:107)W(cid:107)F . We use N (µ  σ) to denote Gaussian distribution with mean µ and
variance σ; or N (µ  Σ) to denote Gaussian vector with mean µ and covariance Σ. We use x = y± z
to denote that x ∈ [y − z  y + z]. We use 1event to denote the indicator function of whether event
is true. We denote by ek the k-th standard basis vector. We use σ(·) to denote the ReLU function
σ(x) = max{x  0} = 1x≥0 · x. Given univariate function f : R → R  we also use f to denote the
same function over vectors: f (x) = (f (x1)  . . .   f (xm)) if x ∈ Rm.
Given vectors v1  . . .   vn ∈ Rm  we deﬁne U = GS(v1  . . .   vn) as their Gram-Schmidt or-

thonormalization. Namely  U = [(cid:98)v1  . . .  (cid:98)vn] ∈ Rm×n where(cid:98)v1 = v1(cid:107)v1(cid:107) and for every i ≥ 2 
j )vi(cid:107). Note that in the occasion that(cid:81)i−1
(cid:98)vi =
(cid:98)vi be an arbitrary unit vector that is orthogonal to(cid:98)v1  . . .  (cid:98)vi−1.
order smooth function φ : R → R. Suppose φ(z) = (cid:80)∞

We say a function f : Rd → R is L-Lipscthiz continuous if |f (x) − f (y)| ≤ L(cid:107)x − y(cid:107)2; and say it
is is L-smooth if its gradient is L-Lipscthiz continuous  that is (cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ L(cid:107)x − y(cid:107)2.
Function complexity. The following notions from [1] measure the complexity of any inﬁnite-
i=0 cizi is its Taylor expansion. Given
2For instance  if W ∈ Rm×m is the recurrent weight matrix  and is followed with an ReLU activation
m )  the combined operator σ(W x) : Rm → Rm has operator
√
m )  then β becomes 1 but gradients will vanish
2

σ. Under standard random initialization N (0  2
norm
exponentially fast in L.

2 with high probability. If instead one uses N (0 

j=1(I −(cid:98)vj(cid:98)v(cid:62)

j )vi is the zero vector  we let

√

2

non-negative R 

(cid:16)
Cε(φ  R) :=(cid:80)∞
Cs(φ  R) := C∗(cid:80)∞
√
log(1/ε)
i=0(i + 1)1.75Ri|ci|

(C∗R)i +(cid:0)√

i=0

i

C∗R(cid:1)i(cid:17)|ci|

where C∗ is a sufﬁciently large constant (e.g.  104). It holds Cs(φ  R) ≤ Cε(φ  R) ≤ Cs(φ  O(R))×
poly(1/ε)  and for sin z  ez or low degree polynomials  they only differ by o(1/ε). [1]
Example 2.1. If φ(z) = zd for constant d then Cs(φ  R) ≤ O(Rd)  Cε(φ  R) ≤ O(Rdpolylog( 1
ε )).
For functions such as φ(z) = ez − 1  sin z  sigmoid(z) or tanh(z)  it sufﬁces to consider ε-
approximations of them so we can truncate their Taylor expansions to degree O(log(1/ε)). This
gives Cs(φ  R)  Cε(φ  R) ≤ (1/ε)O(log R).

3 Problem Formulation
The data are generated from an unknown distribution D over (x(cid:63)  y(cid:63)) ∈ (Rdx )(L−2) × Y (L−2).
(cid:96)(cid:107) = 1 and [x(cid:63)
2 without
Each input sequence x(cid:63) consists of x(cid:63)
L ∈ Y. The training dataset Z =
loss of generality.3 Each label sequence y(cid:63) consists of y(cid:63)
{((x(cid:63))(i)  (y(cid:63))(i))}i∈[N ] is given as N i.i.d. samples from D. When (x(cid:63)  y(cid:63)) is generated from D 
we call x(cid:63) the true input sequence and y(cid:63) the true label.
Deﬁnition 3.1. Without loss of generality (see Remark 3.4)  for each true input x(cid:63)  we transform it
into an actual input sequence x1  x2  . . .   xL ∈ Rdx+1 as follows.

L−1 ∈ Rdx with (cid:107)x(cid:63)
3  . . .   y(cid:63)

(cid:96) ]dx = 1

2  . . .   x(cid:63)

x1 = (0dx   1)

and x(cid:96) = (εxx(cid:63)

(cid:96)   0) for (cid:96) = 2  3  . . .   L − 1

and xL = (0dx   εx)

where εx ∈ (0  1) is a parameter to be chosen later. We then feed this actual sequence x into RNN.
Deﬁnition 3.2. We say the sequence x1  . . .   xL ∈ Rdx+1 is normalized if

(cid:107)x1(cid:107) = 1 and

(cid:107)x(cid:96)(cid:107) = εx

for all (cid:96) = 2  3  . . .   L.

3.1 Our Learner Network: Elman RNN
To present the simplest result  we focus on the classical Elman RNN with ReLU activation. Let
W ∈ Rm×m  A ∈ Rm×(dx+1)  and B ∈ Rd×m be the weight matrices.
Deﬁnition 3.3. Our Elman RNN can be described as follows. On input x1  . . .   xL ∈ Rdx+1 

h0 = 0 ∈ Rm
y(cid:96) = B · h(cid:96) ∈ Rd

g(cid:96) = W · h(cid:96)−1 + Ax(cid:96) ∈ Rm
h(cid:96) = σ(W · h(cid:96)−1 + Ax(cid:96)) ∈ Rm

m )  and the entries of B are i.i.d. generated from N (0  1
d ).

We say that W  A  B are at random initialization  if the entries of W and A are i.i.d. generated from
N (0  2
For simplicity  in this paper we only update W and let A and B be at their random initialization.
Thus  we write F(cid:96)(x(cid:63); W ) = y(cid:96) = Bh(cid:96) as the output of the (cid:96)-th layer.
L ∈ Y using some loss function
Our goal is to use y3  . . .   yL ∈ Rd to ﬁt the true label y(cid:63)
G : Rd × Y → R. In this paper we assume  for every y(cid:63) ∈ Y  G(0d  y(cid:63)) ∈ [−1  1] is bounded 
and G(·  y(cid:63)) is convex and 1-Lipschitz continuous in its ﬁrst variable. This includes for instance the
cross-entropy loss and (cid:96)2-regression loss (for y(cid:63) being bounded).4
Remark 3.4. Since we only update W   the label sequence y(cid:63)
L is off from the input sequence
L−1 by one. The last xL can be made zero  but we keep it normalized for notational
2  . . .   x(cid:63)
x(cid:63)
simplicity. The ﬁrst x1 gives a random seed fed into the RNN (one can equivalently put it into h0).
We have scaled down the input signals by εx  which can be equivalently thought as scaling down A.

3  . . .   y(cid:63)

3  . . .   y(cid:63)

(cid:96)(cid:107)2 ≤ 1 by padding(cid:112)1 − (cid:107)x(cid:63)

3This is without loss of generality  since 1

can always be ensured from (cid:107)x(cid:63)
assumption to simplify our notations: for instance  (x(cid:63)
concept class without bias.

(cid:96)(cid:107)2 = 1
(cid:96)(cid:107)2
2 to the second-last coordinate. We make this
2 allows us to focus only on networks in the
(cid:96) )dx = 1
4We use [−1  1] and 1-Lipschitzness for notation simplicity. In generally  our ﬁnal time and sample com-

2 can always be padded to the last coordinate  and (cid:107)x(cid:63)

plexity bounds only scale polynomially with the boundedness and Lipschitzness parameters.

3

3.2 Concept Class
Let {Φi→j r s : R → R}i j∈[L] r∈[p] s∈[d] be inﬁnite-order differentiable functions  and {w∗
i→j r s ∈
Rdx}i j∈[L] r∈[p] s∈[d] be unit vectors. Then  for every j = 3  4  . . .   L  we consider target functions
j : Rdx → Rd where F ∗
F ∗

(cid:1) can be written as

j 1  . . .   F ∗

j d

j =(cid:0)F ∗
j s(x(cid:63)) :=(cid:80)j−1

(cid:80)

(3.1)

r∈[p] Φi→j r s((cid:104)w∗
For proof simplicity  we assume Φi→j r s(0) = 0. We also use

F ∗

i=2

i→j r s  x(cid:63)

i (cid:105)) ∈ R .

Cε(Φ  R) = max
i j r s

{Cε(Φi→j r s  R)}

and Cs(Φ  R) = max
i j r s

{Cs(Φi→j r s  R)}

to denote the complexity of F ∗.
Agnostic PAC-learning language. Our concept class consists of all functions F ∗ in the form of
(3.1) with complexity bounded by threshold C and parameter p bounded by threshold p0. Let OPT
be the population risk achieved by the best target function in this concept class. Then  our goal is to
learn this concept class with population risk OPT + ε using sample and time complexity polynomial
in C  p0 and 1/ε. In the remainder of this paper  to simplify notations  we do not explicitly deﬁne
this concept class parameterized by C and p. Instead  we equivalently state our theorem with respect
to any (unknown) target function F ∗ with speciﬁc parameters C and p.
Example 3.5. Our concept class is general enough and contains functions where the output at each
token is generated from inputs of previous tokens using any two-layer neural network. Indeed  one
can verify that our general form (3.1) includes functions of the following:

j (x(cid:63)) =(cid:80)j−1
constant complexity). The target function can be(cid:80)
n = m  then(cid:80)

i φ(xi) = 0  otherwise it is non-zero.

i=2 A∗

F ∗

Example 3.6. Counting is an example task that falls into our concept class. Speciﬁcally  one can
deﬁne φ such that φ(a) = 1 and φ(b) = −1 (this can be achieved by a quadratic function with
i φ(xi). If the sequence is x = anbm such that

j−iφ(W ∗

j−ix(cid:63)

i ) .

4 Our Result: RNN Provably Learns the Concept Class
Suppose the distribution D is generated by some (unknown) target function F ∗ of the form (3.1) in
the concept class with population risk OPT  namely 

and suppose we are given training dataset Z consisting of N i.i.d. samples from D. We consider the
following stochastic training objective

E(x(cid:63) y(cid:63))∼D

(cid:1)(cid:105) ≤ OPT  
(cid:104)(cid:80)L
j=3 G(cid:0)F ∗
Obj(W (cid:48)) := E(x(cid:63) y(cid:63))∼Z(cid:2)Obj(x(cid:63)  y(cid:63); W (cid:48))(cid:3)
j=3 G(cid:0)λFj(x(cid:63); W + W (cid:48))  y(cid:63)
where Obj(x(cid:63)  y(cid:63); W (cid:48)) :=(cid:80)L

j (x(cid:63))  y(cid:63)
j

j

(cid:1)

Above  W ∈ Rm×m is random initialization  W (cid:48) ∈ Rm×m is the additional shift  and λ ∈ (0  1) is
a constant scaling factor on the network output.5 We consider the vanilla stochastic gradient descent
(SGD) algorithm with step size η. In each iteration t = 1  2  . . .   T   it updates

Wt = Wt−1 − η∇W (cid:48)Obj(x(cid:63)  y(cid:63); Wt−1)

for a random sample (x(cid:63)  y(cid:63)) from the training set Z.

Theorem 1. For every 0 < ε < (cid:101)O(cid:0)
and λ = (cid:101)Θ(cid:0) ε
N = |Z| ≥ poly(C  ε−1  log m)  then SGD with η = (cid:101)Θ(cid:0)
(cid:16) p2C 2poly(L  d)
T = (cid:101)Θ

(cid:1)  deﬁne complexity C = Cε(Φ 
(cid:1)  if the number of neurons m ≥ poly(C  ε−1) and the number of samples is
(cid:1) and
(cid:17)

poly(L d)·p·Cs(Φ O(

εL2d2m

L)

L2d

L))

√

1

1

(SGD)
√

ε2

5Equivalently  one can scale matrix B by factor λ. For notational simplicity  we split the matrix into W +W (cid:48)
but this does not change the algorithm since gradient with respect to W + W (cid:48) is the same with respect to W (cid:48).

4

satisﬁes that  with probability at least 1 − e−Ω(ρ2) over the random initialization

(cid:104) 1

T−1(cid:88)

T

t=0

E
sgd

E

(x(cid:63) y(cid:63))∼D

(cid:2)Obj(x(cid:63)  y(cid:63); W + Wt)(cid:3)(cid:105) ≤ OPT + ε .

Above  Esgd takes expectation with respect to the randomness of SGD. Since SGD takes only one
example per iteration  the sample complexity N is also bounded by T .

4.1 Our Contribution  Interpretation  and Discussion
Sample complexity. Our sample complexity only scales with log(m)  making the result applicable
to over-parameterized RNNs that have m (cid:29) N. Following Example 2.1  if φ(z) is constant degree
polynomial we have C = poly(L  log ε−1) so Theorem 1 says that RNN learns such concept class

with size m =

poly(L  d  p)

poly(ε)

and sample complexity min{N  T} =

p2poly(L  d  log m)

ε2

If φ(z) is a function with good Taylor truncation  such as ez − 1  sin z  sigmoid(z) or tanh(z)  then
C = LO(log(1/ε)) is almost polynomial.
Non-linear measurements. Our result shows that vanilla RNNs can efﬁciently learn a weighted
average of non-linear measurements of the input. As we argued in Example 3.5  this at least includes
functions where the output at each token is generated from inputs of previous tokens using any two-
layer neural networks. Average of non-linear measurements can be quite powerful  achieving the
state-of-the-art performance in some sequential applications such as sentence embedding [4] and
many others [23]  and acts as the base of attention mechanism in RNNs [5].
Adapt to tokens.
In the target function  Φi→j r s can be different at each token  meaning that they
can adapt to the position of the input tokens. We emphasize that the positions of the tokens (namely 
the values i  j) are not directly fed into the network  rather it is discovered through sequentially
reading the input. As one can see from our proofs  the ability of adapting to the tokens comes from
the inhomogeneity in hidden layers h(cid:96): even when x(cid:96) = x(cid:96)(cid:48) for different tokens (cid:96)(cid:48) (cid:54)= (cid:96)  there is still
big difference between h(cid:96) and h(cid:96)(cid:48). Albeit the same operator is applied to x(cid:96) and x(cid:96)(cid:48)  RNNs can still
use this crucial inhomogeneity to learn different functions at different tokens.
In our result  the function Φi→j r s only adapts with the positions of the input tokens  but in many
applications  we would like the function to adapt with the values of the past tokens x(cid:63)
i−1 as
well. We believe a study on other models (such as LSTM [15]) can potentially settle these questions.
Long term memory.
It is commonly believed that vanilla RNNs cannot capture long term depen-
dencies in the input. This does not contradict our result. Our complexity parameter Cε(Φ 
L) of
the learning process in Theorem 1 indeed suffers from L  the length of the input sequence. This is
due to the fact that vanilla RNN  the hidden neurons h(cid:96) will incorporate more and more noise as the
time horizon (cid:96) increases  making the new signal Ax(cid:96) less and less signiﬁcant.
Comparison to feed-forward networks. Recently there are many interesting results on analyzing
the learning process of feed-forward neural networks [7  8  11  16  18–20  26  27  29  30  32]. Most
of them either assume that the input is structured (e.g. Gaussian or separable) or only consider linear
networks. Allen-Zhu  Li  and Liang [1] show a result in the same ﬂavor as this paper but for two and
three-layer feedforward networks. Since RNNs apply the same unit repeatedly to each input token
in a sequence  our analysis is signiﬁcantly different from [1] and creates lots of difﬁculties in the
analysis.6

1  . . .   x(cid:63)

√

6More speciﬁcally  Allen-Zhu  Li  and Liang [1] study two (or three) layer feedforward networks  which
use one (or two) hidden weight matrix to learn one target function. Here in RNN  there is only one weight
matrix shared across the entire time horizon to learn L target functions at different input positions. In other
words  using “one weight” to learn “one target function” is known from prior work  but using “one weight”
to efﬁciently learn “L different target functions” is substantially more difﬁcult  especially when the position
information is not given as the input to the network. For example  our theorem implies that an RNN can
distinguish the sequences “AAAB” from “AABA”  since the order of A and B are different. This requires the
RNN to keep track  using one weight matrix  of the position information of the symbols in the sequence.

5

4.2 Conclusion
We show RNN can actually learn some notable concept class efﬁciently  using simple SGD method
with sample complexity polynomial or almost-polynomial in input length. This concept class at
least includes functions where each output token is generated from inputs of earlier tokens using a
smooth neural network. To the best of our knowledge  this is the ﬁrst proof that some non-trivial
concept class is efﬁciently learnable by RNN. Our sample complexity is almost independent of m 
making the result applicable to over-parameterized settings. On a separate note  our proof explains
why the same recurrent unit is capable of learning various functions from different input tokens to
different output tokens.
Section 6 through 9 continue to give sketch proofs. Our ﬁnal proofs reply on many other technical
properties of RNN that may be of independent interests: such as properties of RNN at random
initialization (which we include in Section B and C)  and properties of RNN stability (which we
include in Section D  E  F). Some of these properties are simple modiﬁcations from prior work  but
some are completely new and require new proof techniques (namely  Section C  D and E).

PROOF SKETCH

Our proof of Theorem 1 divides into four conceptual steps.

1. We obtain ﬁrst-order approximation of how much the outputs of the RNN change if we move
from W to W +W (cid:48). This change (up to small error) is a linear function in W (cid:48). (See Section 6).
(This step can be derived from prior work [2] without much difﬁculty.)
(cid:62) ∈ Rm×m so that this “linear function”  when

2. We construct some (unknown) matrix W

(cid:62)  approximately gives the target F ∗ in the concept class (see Section 5).

evaluated on W

3. We argue that the SGD method moves in a direction nearly as good as W

(This step is the most interesting part of this paper.)
(cid:62) and thus efﬁciently

decreases the training objective (see Section 7).

(This is a routine analysis of SGD in the non-convex setting given Steps 1&2.)
4. We use the ﬁrst-order linear approximation to derive a Rademacher complexity bound that
does not grow exponentially in L (see Section 8). By feeding the output of SGD into this
Rademacher complexity  we ﬁnish the proof of Theorem 1 (see Section 9).

(This is a one-paged proof given the Steps 1&2&3.)
Although our proofs are technical  to help the readers  we write 7 pages of sketch proofs for Steps
1 through 4. This can be found in Section 5 through 9. Due to space limitation  we only include
Section 5 in the main body. We introduce some notations for analysis purpose.
Deﬁnition 4.1. For each (cid:96) ∈ [L]  let D(cid:96) ∈ Rm×m be the diagonal matrix where

As a result  we can write h(cid:96) = D(cid:96)W h(cid:96)−1. For each 1 ≤ (cid:96) ≤ a ≤ L  we deﬁne

(D(cid:96))k k = 1(W·h(cid:96)−1+Ax(cid:96))k≥0 = 1(g(cid:96))k≥0 .

Back(cid:96)→a = BDaW ··· D(cid:96)+1W ∈ Rd×m.

with the understanding that Back(cid:96)→(cid:96) = B ∈ Rd×m.
Throughout the proofs  to simplify notations when specifying polynomial factors  we introduce

ρ := 100Ld log m and

 :=

We assume m ≥ poly() for some sufﬁciently large polynomial factor.

√

L) · log m

100Ldp · Cε(Φ 
ε

5 Existence of Good Network Through Backward
One of our main contributions is to show the existence of some “good linear network” to approximate
any target function. Let us explain what this means. Suppose W  A  B are at random initialization.
We consider a linear function over W

fj(cid:48) :=(cid:80)j(cid:48)

(cid:62) ∈ Rm×m:
i(cid:48)=2 Backi(cid:48)→j(cid:48) Di(cid:48)W

(cid:62)

hi(cid:48)−1 .

(5.1)

6

As we shall see later  in ﬁrst-order approximation  this linear function captures how much the output
of the RNN changes at token j(cid:48)  if one we move W to W + W (cid:48). The goal in this section is to
(cid:62) ∈ Rm×m satisfying that  for any true input x(cid:63) in the support of D  if we deﬁne
construct some W
the actual input x according to x(cid:63) (see Deﬁnition 3.1)  then with high probability 

(cid:80)
r∈[p] Φi→j(cid:48) r s(cid:48)((cid:104)w∗
(5.2)
(cid:62) can simultaneously
In our sketched proof below  it shall become clear how this same matrix W
represent functions Φi→j(cid:48) that come from different input tokens i. Since SGD can be shown to
descend in a direction “comparable” to W

(cid:62)  it converges to a matrix W with similar guarantees.

j(cid:48) s(cid:48)(x(cid:63)) =(cid:80)j(cid:48)−1

i (cid:105))
i→j(cid:48) r s(cid:48)  x(cid:63)

fj(cid:48) s(cid:48) ≈ F ∗

∀s(cid:48) ∈ [d]

i=2

Indicator to Function

5.1
In order to show (5.2)  we ﬁrst show a variant of the “indicator to function” lemma from [1].
Lemma 5.1 (indicator to function). For every smooth function Φ : R → R  every unit vector
w∗ ∈ Rdx with w∗
= 0  every constant σ ≥ 0.1  every constant γ > 1  every constant εe ∈

1

dx

Cs(Φ O(σ))

(cid:1)  there exists
(cid:0)0 
(a) (cid:12)(cid:12)Ea∼N (0 I) n∼N (0 σ2)
(b) (cid:12)(cid:12)Ea∼N (0 I) n∼N (0 σ2)

such that for every ﬁxed unit vectors x(cid:63) ∈ Rdx with x(cid:63)

C(cid:48) = Cεe (Φ  σ) and a function H : R → [−C(cid:48)  C(cid:48)] 

(cid:2)1(cid:104)a x(cid:63)(cid:105)+n≥0H (a)(cid:3) − Φ((cid:104)w∗  x(cid:63)(cid:105))(cid:12)(cid:12) ≤ εe
(cid:2)1(cid:104)a x(cid:63)(cid:105)+γn≥0H (a)(cid:3) − Φ(0)(cid:12)(cid:12) ≤ εe + O(cid:0) C(cid:48) log(γσ)

2  
= 1

dx

γσ

(cid:1)

(on target)

(off target)

Above  Lemma 5.1a says that we can use a bounded function 1(cid:104)a x(cid:63)(cid:105)+n≥0H (a) to ﬁt a target func-
tion Φ((cid:104)w∗  x(cid:63)(cid:105))  and Lemma 5.1b says that if the magnitude of n is large then this function is close
to being constant. For such reason  we can view n as “noise.” While the proof of 5.1a is from
prior work [1]  our new property 5.1b is completely new and it requires some technical challenge to
simultaneously guarantee 5.1a and 5.1b. The proof is in Appendix G.1

5.2 Fitting a Single Function
i (cid:105)). For this
We now try to apply Lemma 5.1 to approximate a single function Φi→j r s((cid:104)w∗
purpose  let us consider two (normalized) input sequences. The ﬁrst (null) sequence x(0) is given as

i→j r s  x(cid:63)

The second sequence x is generated from an input x(cid:63) in the support of D (recall Deﬁnition 3.1). Let

x(0)
1 = (0dx   1)

and x(0)

(cid:96) = (0dx   εx) for (cid:96) = 2  3  . . .   L

• h(cid:96)  D(cid:96)  Backi→j be deﬁned with respect to W  A  B and input sequence x  and
• h(0)
i→j be deﬁned with respect to W  A  B and input sequence x(0)

  Back(0)

  D(0)

(cid:96)

(cid:96)

(cid:96)

We remark that h(0)
has the good property that it does not depend x(cid:63) but somehow stays “close
enough” to the true h(cid:96) (see Appendix D for a full description).
Lemma 5.2 (ﬁt single function). For every 2 ≤ i < j ≤ L  r ∈ [p]  s ∈ [d] and every constant
√

(cid:1)  there exists C(cid:48) = Cεe(Φi→j r s 

L) so that  for every

√

1

εc = εeεx
4C(cid:48)

(cid:3)  such that  let

 

1

L))

ρ4C(cid:48)

Cs(Φi→j r s O(

εx ∈ (0 

and
  4(C(cid:48))2

εe ∈(cid:0)0 
(cid:1)
there exists a function Hi→j r s : R →(cid:2) − 4(C(cid:48))2
• (cid:101)wk (cid:101)ak ∼ N(cid:0)0  2I
(cid:12)(cid:12)(cid:12)(cid:12) E(cid:101)wk (cid:101)ak

with probability at least 1 − e−Ω(ρ2) over W and A 
(a) (on target)

(cid:20)
1|(cid:104)(cid:101)wk h(0)

(cid:1) be freshly new random vectors 

i−1(cid:105)|≤ εc√

(cid:21)
1(cid:104)(cid:101)wk hi−1(cid:105)+(cid:104)(cid:101)ak xi(cid:105)≥0Hi→j r s((cid:101)ak)

εeεx

εeεx

m

m

• x be a ﬁxed input sequence deﬁned by some x(cid:63) in the support of D (see Deﬁnition 3.1) 
• W  A be at random initialization 
• h(cid:96) be generated by W  A x and h(0)

(cid:96) be generated by W  A x(0)  and

− Φi→j r s((cid:104)w∗

i (cid:105))
i→j r s  x(cid:63)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ εe

7

(cid:20)
(b) (off target)  for every i(cid:48) (cid:54)= i
1|(cid:104)(cid:101)wk h(0)

(cid:12)(cid:12)(cid:12)(cid:12) E(cid:101)wk (cid:101)ak

i−1(cid:105)|≤ εc√

m

1(cid:104)(cid:101)wk hi(cid:48)−1(cid:105)+(cid:104)(cid:101)ak xi(cid:48)(cid:105)≥0Hi→j r s((cid:101)ak)

(cid:21)(cid:12)(cid:12)(cid:12)(cid:12) ≤ εe

Lemma 5.2 implies there is a quantity 1|(cid:104)(cid:101)wk h(0)
function and the random initialization (namely  (cid:101)wk (cid:101)ak) such that 
• when multiplying 1(cid:104)(cid:101)wk hi−1(cid:105)+(cid:104)(cid:101)ak xi(cid:105)≥0 gives the target Φi→j r s((cid:104)w∗
• when multiplying 1(cid:104)(cid:101)wk hi(cid:48)−1(cid:105)+(cid:104)(cid:101)ak xi(cid:48)(cid:105)≥0 gives near zero.

i−1(cid:105)|≤ εc√

m

i→j r s  x(cid:63)

i (cid:105)  but

Hi→j r s((cid:101)ak) that only depends on the target

The full proof is in Appendix G.2 but we sketch why Lemma 5.2 can be derived from Lemma 5.1.

x

i(cid:48)  0)(cid:11); but
m ) because (cid:107)hi(cid:48)−1(cid:107) ≈ 1 by random init. (see Lemma B.1a).
times larger than (cid:104)(cid:101)ak  xi(cid:48)(cid:105).
m because i = i(cid:48). Since h(0) can
m. Condition-

Sketch proof of Lemma 5.2. Let us focus on indicator 1(cid:104)(cid:101)wk hi(cid:48)−1(cid:105)+(cid:104)(cid:101)ak xi(cid:48)(cid:105)≥0:
m ) because (cid:104)(cid:101)ak  xi(cid:48)(cid:105) =(cid:10)((cid:101)ak  (εxx(cid:63)
• (cid:104)(cid:101)ak  xi(cid:48)(cid:105) is distributed like N (0  2ε2
• (cid:104)(cid:101)wk  hi(cid:48)−1(cid:105) is roughly N (0  2
Thus  if we treat (cid:104)(cid:101)wk  hi(cid:48)−1(cid:105) as the “noise n” in Lemma 5.1 it can be 1
To show Lemma 5.2a  we only need to focus on |(cid:104)(cid:101)wk  h(0)
be shown close to h (see Lemma D.1)  this is almost equivalent to |(cid:104)(cid:101)wk  hi(cid:48)−1(cid:105)| ≤ εc√
i(cid:48)−1(cid:105)| ≤ εc√
To show Lemma 5.2a  we can show when i(cid:48) (cid:54)= i  the indicator on |(cid:104)(cid:101)wk  hi−1(cid:105)| ≤ εc√
information about the true noise (cid:104)(cid:101)wk  hi(cid:48)−1(cid:105). This is so because hi−1 and hi(cid:48)−1 are somewhat
uncorrelated (details in Lemma B.1k). As a result  the “noise n” is still large and thus Lemma 5.1b
(cid:3)
applies with Φi→j r s(0) = 0.

ing on this happens  the “noise n” must be small so we can apply Lemma 5.1a.

m gives little

εx

5.3 Fitting the Target Function
We are now ready to design W

Deﬁnition 5.3. Suppose εe ∈(cid:0)0 

εc :=

L−1(cid:88)

εeεx
4C(cid:48)

L(cid:88)

  C :=

(cid:88)

i=2

j=i+1

r∈[p] s∈[d]

(cid:62)
k :=

w

(cid:62) ∈ Rm×m using Lemma 5.2.

√

L))

√

(cid:1)  C(cid:48) = Cεe(Φ 
(cid:13)(cid:13)(cid:13)e(cid:62)

1
m

  Ci→j s :=

(cid:1)  we choose

1

ρ4C(cid:48)
(cid:107)h(0)
i−1(cid:107)2 .

(cid:13)(cid:13)(cid:13)2

2

L)  εx ∈ (0 

s Back(0)
i→j

1
Cs(Φ O(

4(C(cid:48))2
εeεx

1

mCi→j s

e(cid:62)
s Back(0)
i→j

1|(cid:104)wk h(0)

i−1(cid:105)|≤ εc√

m

Hi→j r s(ak)h(0)
i−1

k

We construct W

(cid:62) ∈ Rm×m by deﬁning its k-th row vector as follows:

(cid:104)
(cid:13)(cid:13)(cid:13)e(cid:62)

(cid:105)
(cid:13)(cid:13)(cid:13)2

Above  functions Hi→j r s : R →(cid:2) − C  C(cid:3) come from Lemma 5.2.

where Ci→j s :=

s Back(0)
i→j

i−1(cid:107)2

(cid:107)h(0)

1
m

2

The following lemma that says fj(cid:48) s(cid:48) is close to the target function F ∗
j(cid:48) s(cid:48).
(cid:62) in Deﬁnition 5.3 satisﬁes the
Lemma 5.4 (existence through backward). The construction of W
following. For every normalized input sequence x generated from x(cid:63) in the support of D  we have
with probability at least 1 − e−Ω(ρ2) over W  A  B  it holds for every 3 ≤ j(cid:48) ≤ L and s(cid:48) ∈ [d]

fj(cid:48) s(cid:48) =

Φi→j(cid:48) r s(cid:48)((cid:104)w∗

i→j(cid:48) r s(cid:48)  x(cid:63)

pρ11 · O(εe + Cs(Φ  1)ε1/3

x + Cm−0.05)

.

(cid:17)

j(cid:48)−1(cid:88)

(cid:88)

i=2

r∈[p]

i (cid:105)) ±(cid:16)

8

Proof sketch of Lemma 5.4. Using deﬁnition of fj(cid:48) s(cid:48) in (5.1) and W
e(cid:62)
s Back(0)
i→j

(cid:2)e(cid:62)
s(cid:48) Backi(cid:48)→j(cid:48)(cid:3)

fj(cid:48) s(cid:48) =

(cid:88)

(cid:88)

(cid:88)

1

(cid:104)

(cid:105)

(cid:18)

(cid:62)  one can write down

mCi→j(cid:48) s
× 1|(cid:104)wk h(0)

i−1(cid:105)|≤ εc√

m

k

(cid:19)
i−1(cid:105)
1[gi(cid:48) ]k≥0Hi→j r s(ak)(cid:104)hi(cid:48)−1  h(0)

k

i(cid:48) j(cid:48) j

r∈[p] s∈[d]

k∈[m]

(5.3)

Now 

• The summands in (5.3) with i (cid:54)= i(cid:48) are negligible owing to Lemma 5.2b.
• The summands in (5.3) with i = i(cid:48) but j (cid:54)= j(cid:48) are negligible  after proving that Backi→j and
• The summands in (5.3) with s (cid:54)= s(cid:48) are negligible using the randomness of B.
• One can also prove Backi(cid:48)→j(cid:48) ≈ Back(0)

Backi→j(cid:48) are very uncorrelated (details in Lemma C.1).

i(cid:48)→j(cid:48) and hi(cid:48)−1 ≈ h(0)

i(cid:48)−1 (details in Lemma D.1).

(cid:19)

Together 

fj(cid:48) s(cid:48) ≈ j(cid:48)−1(cid:88)

i(cid:48)=2

(cid:88)

(cid:88)

r∈[p]

k∈[m]

(cid:18)

(cid:0)(cid:104)

e(cid:62)
s(cid:48) Back(0)

i(cid:48)→j(cid:48)

(cid:1)2

(cid:105)

k

1

mCi(cid:48)→j(cid:48) s(cid:48)

· 1|(cid:104)wk h(0)

i(cid:48)−1

1[gi(cid:48) ]k≥0Hi(cid:48)→j r s(cid:48)(ak)(cid:107)h(0)

i(cid:48)−1(cid:107)2

(cid:105)|≤ εc√

m

Applying Lemma 5.2a and using our choice of Ci(cid:48)→j(cid:48) s(cid:48)  this gives (in expectation)
j(cid:48) s(cid:48)(x(cid:63)) .

i (cid:105)) = F ∗

i→j(cid:48) r s(cid:48)  x(cid:63)

(cid:80)
r∈[p] Φi→j(cid:48) r s(cid:48)((cid:104)w∗

fj(cid:48) s(cid:48) ≈(cid:80)j(cid:48)−1

i=2

Proving concentration (with respect to k ∈ [m]) is a lot more challenging due to the sophisticated
fresh new samples (cid:101)wk (cid:101)ak for all k ∈ N and apply concentration only with respect to k ∈ N . Here 
correlations across different indices k. To achieve this  we replace some of the pairs wk  ak with
N is a random subset of [m] with cardinality m0.1. We show that the network stabilizes (details in
(cid:3)
(cid:1) (see Claim G.1). Crucially  this Frobenius norm scales
Section E) against such re-randomization. Full proof is in Section G.3.
Finally  one can show (cid:107)W
in m−1/2 so standard SGD analysis shall ensure that our sample complexity does not depend on m
(up to log factors).

(cid:62)(cid:107)F ≤ O(cid:0) pρ3C√

m

References
[1] Zeyuan Allen-Zhu  Yuanzhi Li  and Yingyu Liang. Learning and Generalization in Overpa-
rameterized Neural Networks  Going Beyond Two Layers. In NeurIPS  2019. Full version
available at http://arxiv.org/abs/1811.04918.

[2] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. On the convergence rate of training recurrent
neural networks. In NeurIPS  2019. Full version available at http://arxiv.org/abs/1810.
12065.

[3] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via
over-parameterization. In ICML  2019. Full version available at http://arxiv.org/abs/
1811.03962.

[4] Sanjeev Arora  Yingyu Liang  and Tengyu Ma. A simple but tough-to-beat baseline for sen-

tence embeddings. In ICLR  2017.

[5] Dzmitry Bahdanau  Kyunghyun Cho  and Yoshua Bengio. Neural machine translation by

jointly learning to align and translate. arXiv preprint arXiv:1409.0473  2014.

[6] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

[7] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer

neural network. arXiv preprint arXiv:1710.11241  2017.

9

[8] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with

gaussian inputs. arXiv preprint arXiv:1702.07966  2017.

[9] Minshuo Chen  Xingguo Li  and Tuo Zhao. On generalization bounds of a family of recurrent

neural networks  2019. URL https://openreview.net/forum?id=Skf-oo0qt7.

[10] Bhaskar Dasgupta and Eduardo D Sontag. Sample complexity for learning recurrent perceptron

mappings. In Advances in Neural Information Processing Systems  pages 204–210  1996.

[11] Rong Ge  Jason D Lee  and Tengyu Ma. Learning one-hidden-layer neural networks with

landscape design. arXiv preprint arXiv:1711.00501  2017.

[12] Alex Graves  Abdel-rahman Mohamed  and Geoffrey Hinton. Speech recognition with deep
In Acoustics  speech and signal processing (icassp)  2013 ieee

recurrent neural networks.
international conference on  pages 6645–6649. IEEE  2013.

[13] Moritz Hardt  Tengyu Ma  and Benjamin Recht. Gradient descent learns linear dynamical

systems. The Journal of Machine Learning Research  19(1):1025–1068  2018.

[14] David Haussler. Decision theoretic generalizations of the pac model for neural net and other

learning applications. Information and Computation  100(1):78–150  1992.

[15] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation  9

(8):1735–1780  1997.

[16] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Informa-

tion Processing Systems  pages 586–594  2016.

[17] Pascal Koiran and Eduardo D Sontag. Vapnik-chervonenkis dimension of recurrent neural

networks. Discrete Applied Mathematics  86(1):63–79  1998.

[18] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. In Advances in Neural Information Processing Systems
(NIPS)  2018.

[19] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu

activation. In Advances in Neural Information Processing Systems  pages 597–607  2017.

[20] Yuanzhi Li  Tengyu Ma  and Hongyang Zhang.

Algorithmic regularization in over-
parameterized matrix sensing and neural networks with quadratic activations. In COLT  2018.

[21] Percy Liang. CS229T/STAT231: Statistical Learning Theory (Winter 2016). https://web.

stanford.edu/class/cs229t/notes.pdf  April 2016. accessed January 2019.

[22] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International

Conference on Algorithmic Learning Theory  pages 3–17. Springer  2016.

[23] Jared Ostmeyer and Lindsay Cowell. Machine learning on sequential data using a recurrent

weighted average. Neurocomputing  2018.

[24] Razvan Pascanu  Tomas Mikolov  and Yoshua Bengio. On the difﬁculty of training recurrent
neural networks. In International Conference on Machine Learning  pages 1310–1318  2013.

[25] Hojjat Salehinejad  Julianne Baarbe  Sharan Sankar  Joseph Barfett  Errol Colak  and Shahrokh
Valaee. Recent advances in recurrent neural networks. arXiv preprint arXiv:1801.01078  2017.

[26] Mahdi Soltanolkotabi  Adel Javanmard  and Jason D Lee. Theoretical insights into the
arXiv preprint

optimization landscape of over-parameterized shallow neural networks.
arXiv:1707.04926  2017.

[27] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guar-

antees for multilayer neural networks. arXiv preprint arXiv:1605.08361  2016.

[28] Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural

networks. In Advances in neural information processing systems  pages 3104–3112  2014.

10

[29] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and
its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560 
2017.

[30] Bo Xie  Yingyu Liang  and Le Song. Diversity leads to generalization in neural networks.

arXiv preprint Arxiv:1611.03131  2016.

[31] Jiong Zhang  Qi Lei  and Inderjit S Dhillon. Stabilizing gradients for deep neural networks via

efﬁcient svd parameterization. arXiv preprint arXiv:1803.09327  2018.

[32] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery guar-

antees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175  2017.

11

,Zeyuan Allen-Zhu
Yuanzhi Li