2018,How Many Samples are Needed to Estimate a Convolutional Neural Network?,A widespread folklore for explaining the success of Convolutional Neural Networks (CNNs) is that CNNs use a more compact representation than the Fully-connected Neural Network (FNN) and thus require fewer training samples to accurately estimate their parameters. We initiate the study of rigorously characterizing the sample complexity of estimating CNNs. We show that for an $m$-dimensional convolutional filter with linear activation acting on a $d$-dimensional input  the sample complexity of achieving population prediction error of $\epsilon$ is $\widetilde{O(m/\epsilon^2)$  whereas the sample-complexity for its FNN counterpart is lower bounded by $\Omega(d/\epsilon^2)$ samples. Since  in typical settings $m \ll d$  this result demonstrates the advantage of using a CNN. We further consider the sample complexity of estimating a one-hidden-layer CNN with linear activation where both the $m$-dimensional convolutional filter and the $r$-dimensional output weights are unknown. For this model  we show that the sample complexity is $\widetilde{O}\left((m+r)/\epsilon^2\right)$ when the ratio between the stride size and the filter size is a constant. For both models  we also present lower bounds showing our sample complexities are tight up to logarithmic factors. Our main tools for deriving these results are a localized empirical process analysis and a new lemma characterizing the convolutional structure. We believe that these tools may inspire further developments in understanding CNNs.,How Many Samples are Needed to Estimate a

Convolutional Neural Network?

Simon S. Du˚

Carnegie Mellon University

Yining Wang*

Carnegie Mellon University

Xiyu Zhai

Massachusetts Institute of Technology

Sivaraman Balakrishnan
Carnegie Mellon University

Ruslan Salakhutdinov

Carnegie Mellon University

Aarti Singh

Carnegie Mellon University

Abstract

A widespread folklore for explaining the success of Convolutional Neural Net-
works (CNNs) is that CNNs use a more compact representation than the Fully-
connected Neural Network (FNN) and thus require fewer training samples to accu-
rately estimate their parameters. We initiate the study of rigorously characterizing
the sample complexity of estimating CNNs. We show that for an m-dimensional
convolutional ﬁlter with linear activation acting on a d-dimensional input  the sam-

ple complexity of achieving population prediction error of ✏ is rOpm{✏2q 2  whereas
the sample-complexity for its FNN counterpart is lower bounded by ⌦pd{✏2q sam-
ples. Since  in typical settings m ! d  this result demonstrates the advantage of
using a CNN. We further consider the sample complexity of estimating a one-
hidden-layer CNN with linear activation where both the m-dimensional convolu-
tional ﬁlter and the r-dimensional output weights are unknown. For this model 

we show that the sample complexity is rO`pm ` rq{✏2˘ when the ratio between

the stride size and the ﬁlter size is a constant. For both models  we also present
lower bounds showing our sample complexities are tight up to logarithmic factors.
Our main tools for deriving these results are a localized empirical process analysis
and a new lemma characterizing the convolutional structure. We believe that these
tools may inspire further developments in understanding CNNs.

1

Introduction

Convolutional Neural Networks (CNNs) have achieved remarkable impact in many machine learn-
ing applications  including computer vision (Krizhevsky et al.  2012)  natural language process-
ing (Yu et al.  2018) and reinforcement learning (Silver et al.  2016). The key building block of
these improvements is the use of convolutional (weight sharing) layers to replace traditional fully
connected layers  dating back to LeCun et al. (1995). A common folklore of explaining the suc-
cess of CNNs is that they are a more compact representation than Fully-connected Neural Networks
(FNNs) and thus require fewer samples to estimate. However  to our knowledge  there is no rigorous
characterization of the sample complexity of learning a CNN.
The main difﬁculty lies in the convolution structure. Consider the simplest CNN  a single convolu-
tional ﬁlter with linear activation followed by average pooling (see Figure 1a)  which represents a

˚Equal contribution.

2We use the standard big-O notation in this paper and use rOp¨q when we ignore poly-logarithmic factors.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

function F1 : Rd ﬁÑ R of the form:

F1px; wq “

r´1ÿ`“0

wJP`

sx 

(1)

where w P Rm is the ﬁlter of size m and a stride size of s  r « d{s is the total number of times ﬁlter
w is applied to an input vector x P Rd  and P`
sx :“ rx`s`1  x`s`2  . . .   x`s`ms is an m-dimensional
segment of the feature vector x. Noting that F1 is a linear function of x  we can also represent F1
by a one-layer fully connected neural network (linear predictor):

for some ✓ P Rd. Suppose we have n samples txi  yiun
use the least squares estimator:

F FNN
1

px  ✓q “ ✓Jx

(2)
i“1 where x is the input and y is the label and

p✓ :“ arg min

✓PRd

nÿi“1pyi ´ ✓Jxiq2.

`“0 P`

By a classical results analyzing the prediction error for linear regression (see for instance (Wasser-

where µ is the input distribution and ✓0 is the optimal linear predictor. The proof for FNN is fairly

gregated features and labels  respectively and then directly analyze this expression.
On the other hand  the network F1 can be viewed as a linear regression model with respect to w  by
sx P Rm. The classical analysis of
ordinary least squares in linear regression does not directly yield the optimal sample complexity in

man  2013))  under mild regularity conditions  we need n — d{✏2 to havebEx„µ|p✓Jx ´ ✓J0 x|2 § ✏ 
simple because we can writep✓ “`XJX˘´1 XJY (normal equation) where X and Y are the ag-
considering a “stacked” version of feature vectorsrxi “∞r´1
this case  because the distributional properties ofrxi as well as the spectral properties of the sample
covariance∞irxirxJi are difﬁcult to analyze due to the heavy correlation between coordinates of
rx corresponding to overlapping patches. We discuss further details of this aspect after our main

positive result in Theorem 1.
In this paper  we take a step towards understanding the statistical behavior of the CNN model de-
scribed above. We adopt tools from localized empirical process theory (van de Geer  2000) and
combine them with a structural property of convolutional ﬁlters (see Lemma 2) to give a complete
characterization of the statistical behavior of this simple CNN.
We ﬁrst consider the problem of learning a convolutional ﬁlter with average pooling as in Eq.(1)
using the least squares estimator. We show in the standard statistical learning setting  under fairly

where µ is the input distribution and w0 is the underlying true convolutional ﬁlter. Notably  to

bEx„µ|F1px pwq ´ F1px  w0q|2 “ rO´am{n¯  

natural conditions on the input distribution  pw satisﬁes
achieve an ✏ error  the CNN only needs rOpm{✏2q samples whereas the FNN needs ⌦pd{✏2q. Since
the ﬁlter size m ! d  this result clearly justiﬁes the folklore that the convolutional layer is a more
compact representation. Furthermore  we complement this upper bound with a minimax lower
bound which shows the error bound rOpam{nq is tight up to logarithmic factors.

Next  we consider a one-hidden-layer CNN (see Figure 1b):

a`wJP`

sx 

(3)

F2px; w  aq “

r´1ÿ`“0

where both the shared convolutional ﬁlter w P Rm and output weights a P Rr are unknown. This
architecture is previously considered in Du et al. (2017b). However the focus of that work is to un-
derstand the dynamics of gradient descent. Using similar tools as in analyzing a single convolutional

ﬁlter  we show that the least squares estimator achieves the error bound rOpapm ` rq{nq if the ratio

between the stride size and the ﬁlter size is a constant. Further  we present a minimax lower bound
showing that the obtain rate is tight up to logarithmic factors.
To our knowledge  these theoretical results are the ﬁrst sharp analyses of the statistical efﬁciency of
the CNN. These results suggest that if the input follows a (linear) CNN model  then it can be learned
more easily than treating it as a FNN since a CNN model reuses weights.

2

w

P 1

s x

w

P `+1

s

x

w

s

P `

s x

Input

x

1

1

1

+

F1(x; w)

w

P 1

s x

w

P `+1

s

x

w

s

P `

s x

Input

x

a1

a`

+
a`+1

F2(x; w  a)

(a) Prediction function formalized in Eq. (1). It
consists of a convolutional ﬁlter followed by av-
eraged pooling. The convolutional ﬁlter is un-
known.

(b) Prediction function formalized in Eq. (3) It
consists of a convolutional ﬁlter followed by a lin-
ear prediction layer. Both layers are unknown.

Figure 1: CNN architectures that we consider in this paper.

1.1 Comparison with Existing Work

Our work is closely related to the analysis of the generalization ability of neural networks (Arora
et al.  2018; Anthony & Bartlett  2009; Bartlett et al.  2017b a; Neyshabur et al.  2017; Konstantinos
et al.  2017). These generalization bounds are often of the form:

Lp✓q ´ Ltrp✓q§ D{?n

(4)
where ✓ represents the parameters of a neural network  Lp¨q and Ltrp¨q represent population and
empirical error under some additive loss  and D is the model capacity and is ﬁnite only if the
(spectral) norm of the weight matrix for each layer is bounded. Comparing with generalization
bounds based on model capacity  our result has two advantages:

1. If Lp¨q is taken to be the mean-squared3 error E|¨|2  Eq. (4) implies an rOp1{✏4q sample complex-
ity to achieve a standardized mean-square error ofaE| ¨ |2 § ✏  which is considerably larger
than the rOp1{✏2q sample complexity we established in this paper.
2. Since the complexity of a model class in regression problems typically depends on the magni-
tude of model parameters (e.g.  }w}2)  generalization error bounds like Eq. (4) are not scale-
independent and deteriorate if }w}2 is large. In contrast  our analysis has no dependency on the
scale of w and also places no constraints on }w}2.

On the other hand  we consider the special case where the neural network model is well-speciﬁed
and the label is generated according to a neural network with unbiased additive noise (see Eq. (5))
whereas the generalization bounds discussed in this section are typically model agnostic.

1.2 Other Related Work

Recently  researchers have been making progress in theoretically understanding various aspects of
neural networks  including hardness of learning (Goel et al.  2016; Song et al.  2017; Brutzkus &
Globerson  2017)  landscape of the loss function (Kawaguchi  2016; Choromanska et al.  2015;
Hardt & Ma  2016; Haeffele & Vidal  2015; Freeman & Bruna  2016; Safran & Shamir  2016; Zhou
& Feng  2017; Nguyen & Hein  2017b a; Ge et al.  2017b; Zhou & Feng  2017; Safran & Shamir 
2017; Du & Lee  2018)  dynamics of gradient descent (Tian  2017; Zhong et al.  2017b; Li & Yuan 
2017)  provable learning algorithms (Goel & Klivans  2017a b; Zhang et al.  2015)  etc.
Focusing on the convolutional neural network  most existing work has analyzed the convergence
rate of gradient descent or its variants (Du et al.  2017a b; Goel et al.  2018; Brutzkus & Globerson 
2017; Zhong et al.  2017a). Our paper differs from them in that we do not consider the computational
complexity but only the sample complexity and information theoretical limits of learning a CNN. It
is an open question when taking computational budget into account  what is the optimal estimator
for CNN.

3Because the standardized mean-square erroraE| ¨ |2 is not a sum of independent random variables  it is
difﬁcult  if not impossible  to apply generalization error bounds directly foraE| ¨ |2.

3

Convolutional structure has also been studied in the dictionary learning (Singh et al.  2018; Huang &
Anandkumar  2015) and blind de-convolution (Zhang et al.  2017) literature. These papers studied
the unsupervised setting where their goal is to recover structured signals from observations generated
according to convolution operations whereas our paper focuses on the supervised learning setting
with predictor having the convolution structure.

1.3 Organization
This paper is organized as follows. In Section 2  we formally setup the problem and assumptions. In
Section 3 we present our main theoretical results for learning a convolutional ﬁlter (see Eq. (1)). In
Section 4 we present our main theoretical results for learning a one-hidden-layer CNN (see Eq. (3)).
In Section 5  we use numerical experiments to verify our theoretical ﬁndings. We conclude and list
future directions in Section 6. Most technical proofs are deferred to the appendix.

2 Problem speciﬁcation and assumptions
Let txi  yiun
i“1 be a sample of n training data points  where xi P Rd denotes the d-dimensional
feature vector of the ith data point and yi P R is the corresponding real-valued response. We
consider a generic model of
(5)

yi “ Fpxi; w0q ` "i  where Er"i|xis “ 0.

In the model of Eq. (5)  F represents a certain network parameterized by a ﬁxed but unknown pa-
rameter w0 that takes a d-dimensional vector xi as input and outputs a single real-valued prediction
Fpxi; w0q. t"iun
i“1 represents stochastic noise inherent in the data  and is assumed to have mean
zero. The feature vectors of training data txiun
i“1 are sampled i.i.d. from an unknown distribution µ
supported on Rd.
Throughout this paper we make the following assumptions:

(A1) Sub-gaussian noise: there exists constant 2 †8 such that for any t P R  Eet"i § e2t2{2;
(A2) Sub-gaussian design: there exists constant ⌫2 †8 such that for any a P Rd  Eµx “ 0 and
(A3) Non-degeneracy: there exists constant  ° 0 such that minpEµxxJq• .

Eµ exptaJxu§ expt⌫2}a}2

2{2u;

We remark that the assumptions (A1) through (A3) are quite mild. In particular  we only impose
sub-Gaussianity conditions on the distributions of xi and "i  and do not assume they are gener-
ated/sampled from any exact distributions. The last non-degeneracy condition (A3) assumes that
there is a non-negligible probability mass along any direction of the input distributions. It is very
likely to be satisﬁed after simple pre-processing steps of input data  such as mean removal and
whitening of the sample covariance.

minimize the standardized population mean-square prediction error

We are interested in learning a parameter pwn using a training sample tpxi  yiqun
errµppwn  w0; Fq “bEx„µ |Fpx;pwnq ´ Fpx; w0q|2.

3 Convolutional ﬁlters with average pooling

i“1 of size n so as to

(6)

We ﬁrst consider a convolutional network with one convolutional layer  one convolutional ﬁlter 
an average pooling layer and linear activations. More speciﬁcally  for a single convolutional ﬁlter
w P Rm of size m and a stride of size s  the network can be written as

wJP`

sx 

(7)

where r « d{s is the total number of times ﬁlter w is applied to an input vector x  and P`
sx :“
rx`s`1  x`s`2  . . .   x`s`ms is an m-dimensional segment of the d-dimensional feature vector xi.
For simplicity  we assume that m is divisible s and let J “ m{s P N denote the number of strides
within a single ﬁlter of size m.

F1px; wq “

r´1ÿ`“0

4

3.1 The upper bound
Given training sample tpxi  yiqun

i“1  we consider the following least-squares estimator:

n

i“1.

(8)

(9)

1
n

nÿi“1pyi ´ F1pxi; wqq2 .

wPRm

`“0 P`

sxi is the stacked

i“1rxirxJi q´1∞n

i“1 yirxi  whererxi “∞r´1

In
addition  because the objective is a quadratic function in w  Eq. (8) is actually a convex optimization

version of input feature vector xi.
The following theorem upper bounds the expected population mean-square prediction error

pwn P arg min
Note the subscript n which emphasizes that pwn is trained using a sample of n data points.
problem and a global optimal solution pwn can be obtained efﬁciently. More speciﬁcally  pwn admits
the closed-form solution of pwn “ p∞n
errµppwn  w0; F1q of the least-square estimate pwn in Eq. (8).
Theorem 1. Fix an arbitrary  P p0  1{2q. Suppose (A1) through (A3) hold and ⌫alogpn{q•  
n Á ´2⌫2m logp⌫d log ´1q logpn´1q. Then there exists a universal constant C ° 0 such that
with probability 1 ´  over the random draws of x1  . . .   xn „ µ 
Eerrµppwn  w0; F1q§ Cc 2m logp´1⌫d logp´1qq

metric statistics problems  and also conﬁrms the “parameter count” intution that the estimation error
scales approximately with the number of parameters in a network (m in network F1).
We next brieﬂy explain the strategies we employ to prove Theorem 1. While it’s tempting to directly

Here the expectation is taken with respect to the randomness in t"iun
Theorem 1 shows that  with n “ r⌦pmq samples  the expected population mean-square error
errµppwn  w0; F1q scales as rOpa2m{nq. This matches the 1{?n statistical error for classical para-
use the closed-form expression pwn “ p∞n
i“1 yirxi to analyze pwn  such an approach
has two limitations. First  because we consider the population mean-square error errµppwn  w0; F1q 
such an approach would inevtiably require the analysis of spectral properties (e.g.  the least eigen-
value) of∞n
i“1rxirxJi   which is very challenging as heavy correlation occurs inrxi when ﬁlters are
overlapping (i.e.  s † m and J ° 1). It is likely that strong assumptions such as exact isotropic
Gaussianity of the feature vectors are needed to analyze the distributional propertiesrxi (Qu et al. 
2017). Also  such an approach relies on closed-forms of pwn and is difﬁcult to extend to other poten-
tial activations such as the ReLU activation. when no closed-form expressions of pwn exist.

To overcome the above difﬁculties  we adopt a localized empirical process approach introduced in
(van de Geer  2000) to upper bound the expected population mean-square prediction error. At the
core analysis is an upper bound on the covering number of a localized parameter set  with an inter-
esting argument that partitions a d-dimensional equivalent regressor for compactiﬁcation purposes
(see Lemmas 2 and 4 in the appendix for details). Our proof does not rely on the exact/closed-form

i“1rxirxJi q´1∞n

conditioned on x1  . . .   xn.

3.2 The lower bound

Section 6. The complete proof of Theorem 1 is placed in the appendix.

expression of pwn  and has the potential to be extended to other activation functions  as we discuss in
We prove the following information-theoretic lower bound on Eerrµppwn  w0q of any estimator pwn
calculated on a training sample of size n.
Theorem 2. Suppose x1  . . .   xn„Np0  Iq and "1  . . .  " n „ Np0  2q. Suppose also that m ´ s is
an even number. Then there exists a universal constant C1 ° 0 such that
Eerrµpwn  w0; F1q• C1c 2m
.

(10)

inf

n

sup
w0PRm

pwn

Remark 1. Theorem 2 is valid for any pair of (ﬁlter size  stride) combinations pm  sq  provided that
m is divisible by s and m ´ s is an even number. The latter requirement is a technical condtion in
our proof and is not critical  because one can double the size of m and s  and the lower bound in
Theorem 2 remains asymptotically on the same order.

5

Theorem 2 shows that any estimator pwn computed on a training set of size n must have a worst-case
error of at leasta2m{n. This suggests that our upper error bound in Theorem 1 is tight up to
logarithmic factors.
Our proof of Theorem 2 draws on tools from standard information-theoretical lower bounds such as
the Fano’s inequality (Yu  1997; Tsybakov  2009). The high-level idea is to construct a ﬁnite candi-
date set of parameters W Ñ Rm and upper bound the Kullback-Leibler (KL) divergence of induced
observable distributions and the population prediction mean-square error between parameters in the
candidate set W. The complete proof of Theorem 2 is placed in the appendix.
4 Convolutional ﬁlters with prediction layers

We consider a slightly more complicated convolutional network with two layers: the ﬁrst layer is a
single convolutional ﬁlter of size m  applied r times to a d-dimensional input vector with stride s;
the second layer is a linear regression prediction layer that produces a single real-valued output.
For such a two-layer network the parameter w can be speciﬁed as w “ pw  aq  where w P Rm is the
weights in the ﬁrst-layer convolutional ﬁlter and a P Rr is the weight in the second linear prediction
layer. The network F2px; wq “ F2px; w  aq can then be written as
sx.

a`wJP`

(11)

F2px; w  aq “

Note that in Eq. (11) the vector a P Rr is labeled as a “ pa0  a1  . . .   ar´1q for convenience that
matches with the labels of the operator P`
Compared to network F1 with average pooling  the new network F2 can be viewed as a weighted
pooling of convolutional ﬁlters  with weights a P Rr unknown and to be learnt. A graph illustration
of the network F2 is given in Figure 1b.

s for ` “ 0  . . .   r ´ 1.

r´1ÿ`“0

4.1 The upper bound
We again consider the least-squares estimator

(12)

pwn “ ppwn panq P arg min

wPRm aPRr

1
n

nÿi“1pyi ´ F2pxi; w  aqq2 .

i“1 of size n.

Again  we use subscript n to emphasize that both pwn and pan are computed on a training set

txi  yiun
Unlike the least squares problem in Eq. (8) for the F1 network  the optimization problem in Eq. (12)
has two optimization variables w  a and is therefore no longer convex. This means that popular
optimization algorithms like gradient descent do not necessarily converge to a global minima in

Eq. (12). Nevertheless  in this paper we choose to focus on the statistical properties of ppwn panq and

assume global minimality of Eq. (12) is achieved. On the other hand  because Eq. (12) resembles
the matrix sensing problem  it is possible that all local minima are global minima and saddle points
can be efﬁciently escaped (Ge et al.  2017a)  which we leave as future work.
The following theorem upper bounds the population mean-square prediction error of any global

minimizer pwn “ ppwn panq of Eq. (12).
Theorem 3. Fix arbitrary  P p0  1{2q and deﬁne J :“ m{s  where m is the ﬁlter size and
s is the stride. Suppose (A1) through (A3) hold and ⌫alogpn{q•   n Á ´2⌫2prJ `
mq logp⌫d log ´1q logpn´1q. Then there exists a universal constant C ° 0 such that with proba-
bility 1 ´  over the random draws of x1  . . .   xn „ µ 
Eerrµppwn  w0; F2q§ Cc 2prJ ` mq logp´1⌫d logp´1qq

Here the expectation is taken with respect to the randomness in t"iun
Theorem 3 is proved by a similar localized empirical process arguments as in the proof of Theorem
1. Due to space costraints we defer the complete proof of Theorem 3 to the appendix.

conditioned on x1  . . .   xn.

i“1.

(13)

n

6

10 0

10 -2

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -4

500

CNN
FNN

1000

1500

2000

Number of Training Data

10 0

10 -1

10 -2

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -3

500

CNN
FNN

1000

1500

2000

Number of Training Data

10 0

10 -1

10 -2

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -3

500

CNN
FNN

1000

1500

2000

Number of Training Data

(a) Filter size m “ 2.

(b) Filter size m “ 8.

(c) Filter size m “ 16.

Figure 2: Experiments on the problem of learning a convolutional ﬁlter with average pooling de-
scribed in Section 3 with stride size s “ 1.

Theorem 3 shows that errµppwn  w0; F2q can be upper bounded by rOpa2prJ ` mq{nq  provided
that at least n “r⌦prJ ` mq samples are available. Compared to the intuitive “parameter count” of
r`m (r parameters for a and m parameters for w)  our upper bound has an additional multiplicative
J “ m{s term  which is the number of strides within each m-dimensional ﬁlter. Therefore  our
upper bound only matches parameter counts when J is very small (e.g.  non-overlapping ﬁlters or
fast-moving ﬁlters where the stride s is at least a constant fraction of ﬁlter size m)  and becomes
large when the stride s is very small  leading to many convolutions being computed.
We conjecture that such an increase in error/sample complexity is due to an inefﬁciency in one
of our key technical lemmas. More speciﬁcally  in Lemma 7 in which we derive upper bounds
on covering number of localized parameter sets  we use the boundedness and low-dimensionality
of each segment of differences of equivalent parameters for compactiﬁcation purposes; such an
argument is not ideal  as it overlooks the correlation between different segments  connected by an
r-dimensional parameter a. A sharper covering number argument would potentially improve the
error analysis and achieve sample complexity scaling with r ` m.
4.2 The lower bound

We prove the following information-theoretical lower bound on Eerrµppwn  w0q of any estimator
pwn “ ppwn panq calculated on a training sample of size n.
Theorem 4. Suppose x1  . . .   xn„Np0  Iq and "1  . . .  " n „ Np0  2q. Then there exists a univer-
sal constant C1 ° 0 such that
inf

(14)

.

Eerrµppwn  w0; F2q• C1c 2pr ` mq

n

sup
w0

pwn

Theorem 4 shows that the error of any estimator pwn computed on a training sample of size n must
scale asa2pr ` mq{n  matching the parameter counts of r ` m for F2. It is proved by reducing

the regression problem under F2 to two separate ordinary linear regression problems and invoking
classical lower bounds for linear regression models (Wasserman  2013; Van der Vaart  1998). A
complete proof of Theorem 4 is given in the appendix.

5 Experiments

In this section we use simulations to verify our theoretical ﬁndings. For all experiments  we let
the ambient dimension d be 64 and the input distribution be Gaussian with mean 0 and identity
covariance. We use the population mean-square prediction error deﬁned in Eq. (6) as the evaluation
metric. In all plots  CNN represents using convolutional parameterization corresponding to Eq. (1)
or Eq. (3) and FNN represents using fully connected parametrization corresponding to Eq. (2).
In Figure 2 and Figure 3  we consider the problem of learning a convolutional ﬁlter with average
pooling which we analyzed in Section 3. We vary the number of samples  the dimension of ﬁlters
and the stride size. Here we compare parameterizing the prediction function as a d-dimensional
linear predictor and as a convolutional ﬁlter followed by average pooling. Experiments show CNN

7

10 0

10 -2

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -4

500

CNN
FNN

1000

1500

2000

Number of Training Data

10 0

10 -1

10 -2

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -3

500

CNN
FNN

1000

1500

2000

Number of Training Data

10 0

10 -1

10 -2

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -3

500

CNN
FNN

1000

1500

2000

Number of Training Data

(a) Filter size m “ 2.

(b) Filter size m “ 8.

(c) Filter size m “ 16.

Figure 3: Experiments on the problem of learning a convolutional ﬁlter with average pooling de-
scribed in Section 3 with stride size s “ m  i.e.  non-overlapping.

10 0

10 -1

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -2

500

CNN
FNN

1000

1500

2000

Number of Training Data

10 0

10 -1

10 -2

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -3

500

CNN
FNN

1000

1500

2000

Number of Training Data

10 0

10 -1

10 -2

r
o
r
r

 

E
g
n

i
t
s
e
T

10 -3

500

CNN
FNN

1000

1500

2000

Number of Training Data

(a) Stride size s “ 1.

(b) Stride size s “ m{2.

(c) Stride size s “ m  i.e.  non-
overlapping.

Figure 4: Experiment on the problem of one-hidden-layer convolutional neural network with a
shared ﬁlter and a prediction layer described in Section 4. The ﬁlter size m is chosen to be 8.

parameterization is consistently better than the FNN parameterization. Further  as number of training
samples increases  the prediction error goes down and as the dimension of ﬁlter increases  the error

goes up. These facts qualitatively justify our derived error bound rO` m

n˘. Lastly  in Figure 2 we
choose stride s “ 1 and in Figure 3 we choose stride size equals to the ﬁlter size s “ m  i.e. 
non-overlapping. Our experiment shows the stride does not affect the prediction error in this setting
which coincides our theoretical bound in which there is no stride size factor.
In Figure 4  we consider the one-hidden-layer CNN model analyzed in Section 4. Here we ﬁx the
ﬁlter size m “ 8 and vary the number of training samples and the stride size. When stride s “ 1 
convolutional parameterization has the same order parameters as the linear predictor parameteriza-
tion (r “ 57 so r ` m “ 65 « d “ 64) and Figure 4a shows they have similar performances.
In Figure 4b and Figure 4c we choose the stride to be m{2 “ 4 and m “ 8 (non-overlapping) 
respectively. Note these settings have less parameters (r ` m “ 23 for s “ 4 and r ` m “ 16 for
s “ 8) than the case when s “ 1 and so CNN gives better performance than FNN.
6 Conclusion and Future Directions

In this paper we give rigorous characterizations of the statistical efﬁciency of CNN with simple ar-
chitectures. Now we discuss how to extend our work to more complex models and main difﬁculties.

Non-linear Activation: Our paper only considered CNN with linear activation. A natural question
is what is the sample complexity of learning a CNN with non-linear activation like Recitifed Linear
Units (ReLU). We ﬁnd that even without convolution structure  this is a difﬁcult problem. For
linear activation function  we can show the empirical loss is a good approximation to the population
loss and we used this property to derive our upper bound. However  for ReLU activation  we can
ﬁnd a counter example for any ﬁnite n  which breaks our Lemma 3. We believe if there is a better
understanding of non-smooth activation which can replace our Lemma 3  we can extend our analysis
framework to derive sharp sample complexity bounds for CNN with non-linear activation function.

Multiple Filters: For both models we considered in this paper  there is only one shared ﬁlter. In
commonly used CNN architectures  there are multiple ﬁlters in each layer and multiple layers. Note

8

that if one considers a model of k ﬁlters with linear activation with k ° 1  one can always replace
this model by a single convolutional ﬁlter that equals to the summation of these k ﬁlters. Thus 
we can formally study the statistical behavior of wide and deep architectures only after we have
understood the non-linear activation function. Nevertheless  we believe our empirical process based
analysis is still applicable.

Acknowledgment

This research was partly funded by AFRL grant FA8750-17-2-0212 and DARPA D17AP00001.

References
Anthony  M.  & Bartlett  P. L. (2009). Neural network learning: Theoretical foundations. cambridge

university press.

Arora  S.  Ge  R.  Neyshabur  B.  & Zhang  Y. (2018). Stronger generalization bounds for deep nets

via a compression approach. arXiv preprint arXiv:1802.05296.

Bartlett  P. L.  Foster  D. J.  & Telgarsky  M. J. (2017a). Spectrally-normalized margin bounds for

neural networks. In Advances in Neural Information Processing Systems  (pp. 6241–6250).

Bartlett  P. L.  Harvey  N.  Liaw  C.  & Mehrabian  A. (2017b). Nearly-tight vcdimension and

pseudodimension bounds for piecewise linear neural networks. arxiv preprint. arXiv  1703.

Bickel  P. J.  Ritov  Y.  & Tsybakov  A. B. (2009). Simultaneous analysis of lasso and dantzig

selector. The Annals of Statistics  37(4)  1705–1732.

Brutzkus  A.  & Globerson  A. (2017). Globally optimal gradient descent for a Convnet with Gaus-

sian inputs. arXiv preprint arXiv:1702.07966.

Choromanska  A.  Henaff  M.  Mathieu  M.  Arous  G. B.  & LeCun  Y. (2015). The loss surfaces

of multilayer networks. In Artiﬁcial Intelligence and Statistics  (pp. 192–204).

Du  S. S.  & Lee  J. D. (2018). On the power of over-parametrization in neural networks with

quadratic activation. arXiv preprint arXiv:1803.01206.

Du  S. S.  Lee  J. D.  & Tian  Y. (2017a). When is a convolutional ﬁlter easy to learn? arXiv preprint

arXiv:1709.06129.

Du  S. S.  Lee  J. D.  Tian  Y.  Poczos  B.  & Singh  A. (2017b). Gradient descent learns one-hidden-

layer cnn: Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779.

Dudley  R. M. (1967). The sizes of compact subsets of hilbert space and continuity of gaussian

processes. Journal of Functional Analysis  1(3)  290–330.

Freeman  C. D.  & Bruna  J. (2016). Topology and geometry of half-rectiﬁed network optimization.

arXiv preprint arXiv:1611.01540.

Ge  R.  Jin  C.  & Zheng  Y. (2017a). No spurious local minima in nonconvex low rank problems:
A uniﬁed geometric analysis. In Proceedings of the 34th International Conference on Machine
Learning  (pp. 1233–1242).

Ge  R.  Lee  J. D.  & Ma  T. (2017b). Learning one-hidden-layer neural networks with landscape

design. arXiv preprint arXiv:1711.00501.

Goel  S.  Kanade  V.  Klivans  A.  & Thaler  J. (2016). Reliably learning the ReLU in polynomial

time. arXiv preprint arXiv:1611.10258.

Goel  S.  & Klivans  A. (2017a). Eigenvalue decay implies polynomial-time learnability for neural

networks. arXiv preprint arXiv:1708.03708.

Goel  S.  & Klivans  A. (2017b). Learning depth-three neural networks in polynomial time. arXiv

preprint arXiv:1709.06010.

9

Goel  S.  Klivans  A.  & Meka  R. (2018). Learning one convolutional layer with overlapping

patches. arXiv preprint arXiv:1802.02547.

Graham  R.  & Sloane  N. (1980). Lower bounds for constant weight codes. IEEE Transactions on

Information Theory  26(1)  37–43.

Haeffele  B. D.  & Vidal  R. (2015). Global optimality in tensor factorization  deep learning  and

beyond. arXiv preprint arXiv:1506.07540.

Hardt  M.  & Ma  T. (2016). Identity matters in deep learning. arXiv preprint arXiv:1611.04231.
Hoeffding  W. (1963). Probability inequalities for sums of bounded random variables. Journal of

the American Statistical Association  58(301)  13–30.

Huang  F.  & Anandkumar  A. (2015). Convolutional dictionary learning through tensor factoriza-

tion. In Feature Extraction: Modern Questions and Challenges  (pp. 116–129).

Kawaguchi  K. (2016). Deep learning without poor local minima. In Advances in Neural Information

Processing Systems  (pp. 586–594).

Konstantinos  P.  Davies  M.  & Vandergheynst  P. (2017). Pac-bayesian margin bounds for convo-

lutional neural networks-technical report. arXiv preprint arXiv:1801.00171.

Krizhevsky  A.  Sutskever  I.  & Hinton  G. E. (2012). Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems  (pp. 1097–1105).
LeCun  Y.  Bengio  Y.  et al. (1995). Convolutional networks for images  speech  and time series.

The handbook of brain theory and neural networks  3361(10)  1995.

Li  Y.  & Yuan  Y. (2017). Convergence analysis of two-layer neural networks with ReLU activation.

arXiv preprint arXiv:1705.09886.

Neyshabur  B.  Bhojanapalli  S.  McAllester  D.  & Srebro  N. (2017). A pac-bayesian approach to

spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564.

Nguyen  Q.  & Hein  M. (2017a). The loss surface and expressivity of deep convolutional neural

networks. arXiv preprint arXiv:1710.10928.

Nguyen  Q.  & Hein  M. (2017b). The loss surface of deep and wide neural networks. arXiv preprint

arXiv:1704.08045.

Qu  Q.  Zhang  Y.  Eldar  Y. C.  & Wright  J. (2017). Convolutional phase retrieval via gradient

descent. arXiv preprint arXiv:1712.00716.

Safran  I.  & Shamir  O. (2016). On the quality of the initial basin in overspeciﬁed neural networks.

In International Conference on Machine Learning  (pp. 774–782).

Safran  I.  & Shamir  O. (2017). Spurious local minima are common in two-layer relu neural net-

works. arXiv preprint arXiv:1712.08968.

Silver  D.  Huang  A.  Maddison  C. J.  Guez  A.  Sifre  L.  Van Den Driessche  G.  Schrittwieser 
J.  Antonoglou  I.  Panneershelvam  V.  Lanctot  M.  et al. (2016). Mastering the game of go with
deep neural networks and tree search. Nature  529(7587)  484–489.

Singh  S.  P´oczos  B.  & Ma  J. (2018). Minimax reconstruction risk of convolutional sparse dic-
tionary learning. In International Conference on Artiﬁcial Intelligence and Statistics  (pp. 1327–
1336).

Song  L.  Vempala  S.  Wilmes  J.  & Xie  B. (2017). On the complexity of learning neural networks.

In Advances in Neural Information Processing Systems  (pp. 5520–5528).

Tian  Y. (2017). An analytical formula of population gradient for two-layered ReLU network and

its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560.

Tsybakov  A. B. (2009). Introduction to nonparametric estimation. Springer Series in Statistics.

Springer  New York.

10

van de Geer  S. A. (2000). Empirical Processes in M-estimation  vol. 6. Cambridge university press.
Van der Vaart  A. W. (1998). Asymptotic statistics  vol. 3. Cambridge university press.
Vershynin  R. (2012). How close is the sample covariance matrix to the actual covariance matrix?

Journal of Theoretical Probability  25(3)  655–686.

Wang  Y.  & Singh  A. (2016). Noise-adaptive margin-based active learning and lower bounds under

tsybakov noise condition. In AAAI.

Wasserman  L. (2013). All of statistics: a concise course in statistical inference. Springer Science

& Business Media.

Yu  A. W.  Dohan  D.  Luong  M.-T.  Zhao  R.  Chen  K.  Norouzi  M.  & Le  Q. V. (2018). Qanet:
Combining local convolution with global self-attention for reading comprehension. arXiv preprint
arXiv:1804.09541.

Yu  B. (1997). Assouad  fano  and le cam. In Festschrift for Lucien Le Cam  (pp. 423–435). Springer.
Zhang  Y.  Lau  Y.  Kuo  H.-w.  Cheung  S.  Pasupathy  A.  & Wright  J. (2017). On the global ge-
ometry of sphere-constrained sparse blind deconvolution. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition  (pp. 4894–4902).

Zhang  Y.  Lee  J. D.  Wainwright  M. J.  & Jordan  M. I. (2015). Learning halfspaces and neural

networks with random initialization. arXiv preprint arXiv:1511.07948.

Zhong  K.  Song  Z.  & Dhillon  I. S. (2017a). Learning non-overlapping convolutional neural

networks with multiple kernels. arXiv preprint arXiv:1711.03440.

Zhong  K.  Song  Z.  Jain  P.  Bartlett  P. L.  & Dhillon  I. S. (2017b). Recovery guarantees for

one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175.

Zhou  P.  & Feng  J. (2017).

arXiv:1705.07038.

The landscape of deep learning algorithms.

arXiv preprint

11

,Simon Du
Yining Wang
Xiyu Zhai
Sivaraman Balakrishnan
Russ Salakhutdinov
Aarti Singh