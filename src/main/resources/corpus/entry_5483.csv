2016,The Multiple Quantile Graphical Model,We introduce the Multiple Quantile Graphical Model (MQGM)  which extends the neighborhood selection approach of Meinshausen and Buhlmann for learning sparse graphical models.  The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others.  Our approach models a set of conditional quantiles of one variable as a sparse function of all others  and hence offers a much richer  more expressive class of conditional distribution estimates.  We establish that  under suitable regularity conditions  the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows  even outside of the usual homoskedastic Gaussian data model. We develop an efficient algorithm for fitting the MQGM using the alternating direction method of multipliers.  We also describe a strategy for sampling from the joint distribution that underlies the MQGM estimate. Lastly  we present detailed experiments that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data.,The Multiple Quantile Graphical Model

Alnur Ali

Machine Learning Department
Carnegie Mellon University

alnurali@cmu.edu

J. Zico Kolter

Computer Science Department
Carnegie Mellon University

zkolter@cs.cmu.edu

Ryan J. Tibshirani

Department of Statistics

Carnegie Mellon University

ryantibs@cmu.edu

Abstract

We introduce the Multiple Quantile Graphical Model (MQGM)  which extends
the neighborhood selection approach of Meinshausen and Bühlmann for learning
sparse graphical models. The latter is deﬁned by the basic subproblem of model-
ing the conditional mean of one variable as a sparse function of all others. Our
approach models a set of conditional quantiles of one variable as a sparse function
of all others  and hence offers a much richer  more expressive class of conditional
distribution estimates. We establish that  under suitable regularity conditions  the
MQGM identiﬁes the exact conditional independencies with probability tending to
one as the problem size grows  even outside of the usual homoskedastic Gaussian
data model. We develop an efﬁcient algorithm for ﬁtting the MQGM using the
alternating direction method of multipliers. We also describe a strategy for sam-
pling from the joint distribution that underlies the MQGM estimate. Lastly  we
present detailed experiments that demonstrate the ﬂexibility and effectiveness of
the MQGM in modeling hetereoskedastic non-Gaussian data.

1

Introduction

We consider modeling the joint distribution Pr(y1  . . .   yd) of d random variables  given n indepen-
dent draws from this distribution y(1)  . . .   y(n) ∈ Rd  where possibly d (cid:29) n. Later  we generalize
this setup and consider modeling the conditional distribution Pr(y1  . . .   yd|x1  . . .   xp)  given n
independent pairs (x(1)  y(1))  . . .   (x(n)  y(n)) ∈ Rp+d. Our starting point is the neighborhood selec-
tion method [28]  which is typically considered in the context of multivariate Gaussian data  and seen
as a tool for covariance selection [8]: when Pr(y1  . . .   yd) is a multivariate Gaussian distribution 
it is a well-known fact that yj and yk are conditionally independent given the remaining variables
if and only if the coefﬁcent corresponding to yk is zero in the (linear) regression of yj on all other
variables (e.g.  [22]). Therefore  in neighborhood selection we compute  for each k = 1  . . .   d 
a lasso regression — in order to obtain a small set of conditional dependencies — of yk on the
remaining variables  i.e. 

(cid:19)2

θkjy(i)
j

+ λ(cid:107)θk(cid:107)1 

(1)

minimize

θk∈Rd

(cid:18)

y(i)

k −(cid:88)
n(cid:88)
Pr(y1  . . .   yd) ≈ d(cid:89)

j(cid:54)=k

i=1

for a tuning parameter λ > 0. This strategy can be seen as a pseudolikelihood approximation [4] 

Pr(yk|y¬k) 

(2)

where y¬k denotes all variables except yk. Under the multivariate Gaussian model for Pr(y1  . . .   yd) 
the conditional distributions Pr(yk|y¬k)  k = 1  . . .   d here are (univariate) Gaussians  and maximiz-
ing the pseudolikelihood in (2) is equivalent to separately maximizing the conditionals  as is precisely
done in (1) (with induced sparsity)  for k = 1  . . .   d.

k=1

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Following the pseudolikelihood-based approach traditionally means carrying out three steps: (i) we
write down a suitable family of joint distributions for Pr(y1  . . .   yd)  (ii) we derive the conditionals
Pr(yk|y¬k)  k = 1  . . .   d  and then (iii) we maximize each conditional likelihood by (freely) ﬁtting
the parameters. Neighborhood selection  and a number of related approaches that came after it (see
Section 2.1)  can be all thought of in this workﬂow. In many ways  step (ii) acts as the bottleneck
here  and to derive the conditionals  we are usually limited to a homoskedastic and parameteric family
for the joint distribution.
The approach we take in this paper differs somewhat substantially  as we begin by directly modeling
the conditionals in (2)  without any preconceived model for the joint distribution — in this sense  it
may be seen a type of dependency network [13] for continuous data. We also employ heteroskedastic 
nonparametric models for the conditional distributions  which allows us great ﬂexibility in learning
these conditional relationships. Our method  called the Multiple Quantile Graphical Model (MQGM) 
is a marriage of ideas in high-dimensional  nonparametric  multiple quantile regression with those in
the dependency network literature (the latter is typically focused on discrete  not continuous  data).
An outline for this paper is as follows. Section 2 reviews background material  and Section 3 develops
the MQGM estimator. Section 4 studies basic properties of the MQGM  and establishes a structure
recovery result under appropriate regularity conditions  even for heteroskedastic  non-Gaussian data.
Section 5 describes an efﬁcient ADMM algorithm for estimation  and Section 6 presents empirical
examples comparing the MQGM versus common alternatives. Section 7 concludes with a discussion.

2 Background

2.1 Neighborhood selection and related methods

Neighborhood selection has motivated a number of methods for learning sparse graphical models. The
literature here is vast; we do not claim to give a complete treatment  but just mention some relevant
approaches. Many pseudolikelihood approaches have been proposed  see e.g.  [35  33  12  24  17  1].
These works exploit the connection between estimating a sparse inverse covariance matrix and regres-
sion  and they vary in terms of the optimization algorithms they use and the theoretical guarantees
they offer. In a clearly related but distinct line of research  [45  2  11  36] proposed (cid:96)1-penalized
likelihood estimation in the Gaussian graphical model  a method now generally termed the graphical
lasso (GLasso). Following this  several recent papers have extended the GLasso in various ways. [10]
examined a modiﬁcation based on the multivariate Student t-distribution  for robust graphical model-
ing. [37  46  42] considered conditional distributions of the form Pr(y1  . . .   yd|x1  . . .   xp). [23]
proposed a model for mixed (both continuous and discrete) data types  generalizing both GLasso and
pairwise Markov random ﬁelds. [25  26] used copulas for learning non-Gaussian graphical models.
A strength of neighborhood-based (i.e.  pseudolikelihood-based) approaches lies in their simplicity;
because they essentially reduce to a collection of univariate probability models  they are in a sense
much easier to study outside of the typical homoskedastic  Gaussian data setting. [14  43  44] ele-
gantly studied the implications of using univariate exponential family models for the conditionals in
(2). Closely related to pseudoliklihood approaches are dependency networks [13]. Both frameworks
focus on the conditional distributions of one variable given all the rest; the difference lies in whether
or not the model for conditionals stems from ﬁrst specifying some family of joint distributions (pseu-
dolikelihood methods)  or not (dependency networks). Dependency networks have been thoroughly
studied for discrete data  e.g.  [13  29]. For continuous data  [40] proposed modeling the mean in a
Gaussian neighborhood regression as a nonparametric  additive function of the remaining variables 
yielding ﬂexible relationships — this is a type of dependency network for continuous data (though it
is not described by the authors in this way). Our method  the MQGM  also deals with continuous
data  and is the ﬁrst to our knowledge that allows for fully nonparametric conditional distributions  as
well as nonparametric contributions of the neighborhood variables  in each local model.

2.2 Quantile regression
In linear regression  we estimate the conditional mean of y|x1  . . .   xp from samples. Similarly  in α-
quantile regression [20]  we estimate the conditional α-quantile of y|x1  . . .   xp for a given α ∈ [0  1] 
formally Qy|x1 ... xp (α) = inf{t : Pr(y ≤ t|x1  . . .   xp) ≥ α}  by solving the convex optimization
j )  where ψα(z) = max{αz  (α − 1)z} is the α-
problem: minimizeθ

(cid:80)n
i=1 ψα(y(i) −(cid:80)p

j=1 θjx(i)

2

quantile loss (also called the “pinball” or “tilted absolute” loss). Quantile regression can be useful
when the conditional distribution in question is suspected to be heteroskedastic and/or non-Gaussian 
e.g.  heavy-tailed  or if we wish to understand properties of the distribution other than the mean 
e.g.  tail behavior. In multiple quantile regression  we solve several quantile regression problems
simultaneously  each corresponding to a different quantile level; these problems can be coupled
somehow to increase efﬁciency in estimation (see details in the next section). Again  the literature
on quantile regression is quite vast (especially that from econometrics)  and we only give a short
review here. A standard text is [18]. Nonparametric modeling of quantiles is a natural extension from
the (linear) quantile regression approach outlined above; in the univariate case (one conditioning
variable)  [21] suggested a method using smoothing splines  and [38] described an approach using
kernels. More recently  [19] studied the multivariate nonparametric case (more than one conditioning
variable)  using additive models. In the high-dimensional setting  where p is large  [3  16  9] studied
(cid:96)1-penalized quantile regression and derived estimation and recovery theory for non-(sub-)Gaussian
data. We extend results in [9] to prove structure recovery guarantees for the MQGM (in Section 4.3).

3 The multiple quantile graphical model

Many choices can be made with regards to the ﬁnal form of the MQGM  and to help in understanding
these options  we break down our presentation in parts. First ﬁx some ordered set A = {α1  . . .   αr}
of quantile levels  e.g.  A = {0.05  0.10  . . .   0.95}. For each variable yk  and each level α(cid:96)  we
model the conditional α(cid:96)-quantile given the other variables  using an additive expansion of the form:

Qyk|y¬k (α(cid:96)) = b∗

(cid:96)k +

f∗
(cid:96)kj(yj) 

(3)

d(cid:88)

j(cid:54)=k

(cid:19)

(cid:16)

(cid:88)

j(cid:54)=k

(cid:96)k ∈ R is an intercept term  and f∗

where b∗
(cid:96)kj  j = 1  . . .   d are smooth  but not parametric in form. In
its most general form  the MQGM estimator is deﬁned as a collection of optimization problems  over
k = 1  . . .   d and (cid:96) = 1  . . .   r:

n(cid:88)

i=1

ψα(cid:96)

(cid:18)

k − b(cid:96)k −(cid:88)

y(i)

j(cid:54)=k

minimize

b(cid:96)k  f(cid:96)kj∈F(cid:96)kj  

j=1 ... d

f(cid:96)kj(y(i)
j )

+

λ1P1(f(cid:96)kj) + λ2P2(f(cid:96)kj)

.

(4)

(cid:17)ω

Here λ1  λ2 ≥ 0 are tuning parameters  F(cid:96)kj  j = 1  . . .   d are univariate function spaces  ω > 0 is
a ﬁxed exponent  and P1  P2 are sparsity and smoothness penalty functions  respectively. We give
three examples below; many other variants are also possible.
m}  the span of m
Example 1: basis expansion model
basis functions  e.g.  radial basis functions (RBFs) with centers placed at appropriate locations across
the domain of variable j  for each j = 1  . . .   d. This means that each f(cid:96)kj ∈ F(cid:96)kj can be expressed
as f(cid:96)kj(x) = θT
m(x)).
Also consider an exponent ω = 1  and the sparsity and smoothness penalties

(cid:96)kjφj(x)  for a coefﬁcient vector θ(cid:96)kj ∈ Rm  where φj(x) = (φj

Consider taking F(cid:96)kj = span{φj

1(x)  . . .   φj

1  . . .   φj

P1(f(cid:96)kj) = (cid:107)θ(cid:96)kj(cid:107)2

and P2(f(cid:96)kj) = (cid:107)θ(cid:96)kj(cid:107)2
2 

ψα(cid:96)

+

j(cid:54)=k

(cid:17)

(cid:16)

(cid:16)

minimize

(cid:88)

b(cid:96)k  θ(cid:96)k=(θ(cid:96)k1 ... θ(cid:96)kd)

k   . . .   y(n)

Yk − b(cid:96)k1 − Φθ(cid:96)k

λ1(cid:107)θ(cid:96)kj(cid:107)2 + λ2(cid:107)θ(cid:96)kj(cid:107)2

Above  we have used the abbreviation ψα(cid:96) (z) =(cid:80)n

respectively  which are group lasso and ridge penalties  respectively. With these choices in place  the
MQGM problem in (4) can be rewritten in ﬁnite-dimensional form:

(cid:17)
(5)
i=1 ψα(cid:96)(zi) for a vector z = (z1  . . .   zn) ∈ Rn 
k ) ∈ Rn for the observations along variable k  1 = (1  . . .   1) ∈ Rn  and
and also Yk = (y(1)
j )T ∈ Rm.
Φ ∈ Rn×dm for the basis matrix  with blocks of columns to be understood as Φij = φ(y(i)
The basis expansion model is simple and tends to work well in practice  so we focus on it for most of
the paper. In principle  essentially all our results apply to the next two models we describe  as well.
n}  the span
Example 2: smoothing splines model
of m = n natural cubic splines with knots at y(1)
  for j = 1  . . .   d. As before  we can
(cid:96)kjgj(x) with coefﬁcients θ(cid:96)kj ∈ Rn  for f(cid:96)kj ∈ F(cid:96)kj. The work of [27]  on
then write f(cid:96)kj(x) = θT
high-dimensional additive smoothing splines  suggests a choice of exponent ω = 1/2  and penalties
P1(f(cid:96)kj) = (cid:107)Gjθ(cid:96)kj(cid:107)2

Now consider taking F(cid:96)kj = span{gj

and P2(f(cid:96)kj) = θT

  . . .   y(n)

j

1  . . .   gj

(cid:96)kjΩjθ(cid:96)kj 

.

2

2

j

3

i(cid:48)(y(i)

for sparsity and smoothness  respectively  where Gj ∈ Rn×n is a spline basis matrix with entries
Gj
ii(cid:48) = gj
j )  and Ωj is the smoothing spline penalty matrix containing integrated products of
pairs of twice differentiated basis functions. The MQGM problem in (4) can be translated into a
ﬁnite-dimensional form  very similar to what we have done in (5)  but we omit this for brevity.
Consider taking F(cid:96)kj = Hj  a univariate reproducing kernel Hilbert
Example 3: RKHS model
space (RKHS)  with kernel function κj(· ·). The representer theorem allows us to express each
i=1(θ(cid:96)kj)iκj(x  y(i)
j ) 
for a coefﬁcient vector θ(cid:96)kj ∈ Rn. The work of [34]  on high-dimensional additive RKHS modeling 
suggests a choice of exponent ω = 1  and sparsity and smoothness penalties

function f(cid:96)kj ∈ Hj in terms of the representers of evaluation  i.e.  f(cid:96)kj(x) =(cid:80)n

P1(f(cid:96)kj) = (cid:107)K jθ(cid:96)kj(cid:107)2

and P2(f(cid:96)kj) =

θT
(cid:96)kjK jθ(cid:96)kj 
respectively  where K j ∈ Rn×n is the kernel matrix with entries K j
ii(cid:48) = κj(y(i)
). Again  the
MQGM problem in (4) can be written in ﬁnite-dimensional form  now an SDP  omitted for brevity.
Structural constraints
Several structural constraints can be placed on top of the MQGM op-
timization problem in order to guide the estimated component functions to meet particular shape
requirements. An important example are non-crossing constraints (commonplace in nonparametric 
multiple quantile regression [18  38]): here  we optimize (4) jointly over (cid:96) = 1  . . .   r  subject to

  y(i(cid:48))

j

j

(cid:113)

b(cid:96)k +

f(cid:96)kj(y(i)

j ) ≤ b(cid:96)(cid:48)k +

f(cid:96)(cid:48)kj(y(i)

j ) 

for all α(cid:96) < α(cid:96)(cid:48)  and i = 1  . . .   n.

(6)

(cid:88)

j(cid:54)=k

(cid:88)

j(cid:54)=k

r(cid:88)

This ensures that the estimated quantiles obey the proper ordering  at the observations. For concrete-
ness  we consider the implications for the basis regression model  in Example 1 (similar statements
hold for the other two models). For each (cid:96) = 1  . . .   r  denote by F(cid:96)k(b(cid:96)k  θ(cid:96)k) the criterion in (5).
Introducing the non-crossing constraints requires coupling (5) over (cid:96) = 1  . . .   r  so that we now have
the following optimization problems  for each target variable k = 1  . . .   d:

k + ΦΘk)DT ≥ 0 

(cid:96)=1

Bk Θk

minimize

F(cid:96)k(b(cid:96)k  θ(cid:96)k)

subject to (1BT

(7)
where we denote Bk = (b1k  . . .   brk) ∈ Rr  Φ ∈ Rn×dm the basis matrix as before  Θk ∈ Rdm×r
given by column-stacking θ(cid:96)k ∈ Rdm  (cid:96) = 1  . . .   r  and D ∈ R(r−1)×r is the usual discrete
difference operator. (The inequality in (7) is to be interpreted componentwise.) Computationally 
coupling the subproblems across (cid:96) = 1  . . .   r clearly adds to the overall difﬁculty of the MQGM  but
statistically this coupling acts as a regularizer  by constraining the parameter space in a useful way 
thus increasing our efﬁciency in ﬁtting multiple quantile levels from the given data.
For a triplet (cid:96)  k  j  monotonicity constraints are also easy to add  i.e.  f(cid:96)kj(y(i)
) for all
j < y(i(cid:48))
y(i)
. Convexity constraints  where we require f(cid:96)kj to be convex over the observations  for a
particular (cid:96)  k  j  are also straightforward. Lastly  strong non-crossing constraints  where we enforce
(6) over all z ∈ Rd (not just over the observations) are also possible with positive basis functions.
Exogenous variables and conditional random ﬁelds
So far  we have considered modeling the
joint distribution Pr(y1  . . .   yd)  corresponding to learning a Markov random ﬁeld (MRF). It is not
hard to extend our framework to model the conditional distribution Pr(y1  . . .   yd|x1  . . .   xp) given
some exogenous variables x1  . . .   xp  corresponding to learning a conditional random ﬁeld (CRF).
(cid:96)k ∈ Rp in (5)  and the
To extend the basis regression model  we introduce the additional parameters θx
loss now becomes ψα(cid:96)(Yk − b(cid:96)k1T − Φθ(cid:96)k − Xθx
(cid:96)k)  where X ∈ Rn×q is ﬁlled with the exogenous
observations x(1)  . . .   x(n) ∈ Rq; the other models are changed similarly.

j ) ≤ f(cid:96)kj(y(i(cid:48))

j

j

4 Basic properties and theory

4.1 Quantiles and conditional independence
In the model (3)  when a particular variable yj has no contribution  i.e.  satisﬁed f∗
(cid:96)kj = 0 across all
quantile levels α(cid:96)  (cid:96) = 1  . . .   r  what does this imply about the conditional independence between yk
and yj  given the rest? Outside of the multivariate normal model (where the feature transformations
need only be linear)  nothing can be said in generality. But we argue that conditional independence can
be understood in a certain approximate sense (i.e.  in a projected approximation of the data generating
model). We begin with a simple lemma. Its proof is elementary  and given in the supplement.

4

Lemma 4.1. Let U  V  W be random variables  and suppose that all conditional quantiles of U|V  W
do not depend on V   i.e.  QU|V W (α) = QU|W (α) for all α ∈ [0  1]. Then U and V are conditionally
independent given W .

By the lemma  if we knew that QU|V W (α) = h(α  U  W ) for a function h  then it would follow that
U  V are conditionally independent given W (n.b.  the converse is true  as well). The MQGM problem
in (4)  with sparsity imposed on the coefﬁcients  essentially aims to achieve such a representation
for the conditional quantiles; of course we cannot use a fully nonparametric representation of the
conditional distribution yk|y¬k and instead we use an r-step approximation to the conditional cumu-
lative distribution function (CDF) of yk|y¬k (corresponding to estimating r conditional quantiles) 
and (say) in the basis regression model  limit the dependence on conditioning variables to be in terms
of an additive function of RBFs in yj  j (cid:54)= k. Thus  if at the solution in (5) we ﬁnd that ˆθ(cid:96)kj = 0 
(cid:96) = 1  . . .   r  we may interpret this to mean that yk and yj are conditionally independent given the
remaining variables  but according to the distribution deﬁned by the projection of yk|y¬k onto the
space of models considered in (5) (r-step conditional CDFs  which are additive expansions in yj 
j (cid:54)= k). This interpretation is no more tenuous (arguably  less so  as the model space here is much
larger) than that needed when applying standard neighborhood selection to non-Gaussian data.

4.2 Gibbs sampling and the “joint” distribution

When specifying a form for the conditional distributions in a pseudolikelihood approximation as in
(2)  it is natural to ask: what is the corresponding joint distribution? Unfortunately  for a general
collection of conditional distributions  there need not exist a compatible joint distribution  even
when all conditionals are continuous [41]. Still  pseudolikelihood approximations (a special case
of composite likelihood approximations)  possess solid theoretical backing  in that maximizing the
pseudolikelihood relates closely to minimizing a certain (expected composite) Kullback-Leibler
divergence  measured to the true conditionals [39]. Recently  [7  44] made nice progress in describing
speciﬁc conditions on conditional distributions that give rise to a valid joint distribution  though their
work was speciﬁc to exponential families. A practical answer to the question of this subsection is to
use Gibbs sampling  which attempts to draw samples consistent with the ﬁtted conditionals; this is
precisely the observation of [13]  who show that Gibbs sampling from discrete conditionals converges
to a unique stationary distribution  although this distribution may not actually be compatible with the
conditionals. The following result establishes the analogous claim for continuous conditionals; its
proof is in the supplement. We demonstrate the practical value of Gibbs sampling through various
examples in Section 6.
Lemma 4.2. Assume that the conditional distributions Pr(yk|y¬k)  k = 1  . . .   d take only positive
values on their domain. Then  for any given ordering of the variables  Gibbs sampling converges to a
unique stationary distribution that can be reached from any initial point. (This stationary distribution
depends on the ordering.)

4.3 Graph structure recovery

(cid:96)kjφj(x)∗ for coefﬁcients θ∗

When log d = O(n2/21)  and we assume somewhat standard regularity conditions (listed as A1–A4
in the supplement)  the MQGM estimate recovers the underlying conditional independencies with
high probability (interpreted in the projected model space  as explained in Section 4.1). Importantly 
we do not require a Gaussian  sub-Gaussian  or even parametric assumption on the data generating
process; instead  we assume i.i.d. draws y(1)  . . .   y(n) ∈ Rd  where the conditional distributions
yk|y¬k have quantiles speciﬁed by the model in (3) for k = 1  . . .   d  (cid:96) = 1  . . .   r  and further  each
f∗
(cid:96)kj(x) = θT
Let E∗ denote the corresponding edge set of conditional dependencies from these neighborhood
models  i.e.  {k  j} ∈ E∗ ⇐⇒ max(cid:96)=1 ... r max{(cid:107)θ∗
(cid:96)jk(cid:107)2} > 0. We deﬁne the estimated
(cid:96)kj(cid:107)2 |θ∗
the features have been scaled to satisfy (cid:107)Φj(cid:107) ≤ √
edge set ˆE in the analogous way  based on the solution in (5). Without a loss of generality  we assume
n for all j = 1  . . .   dm. The following is our
recovery result; its proof is provided in the supplement.
Theorem 4.3. Assume log d = O(n2/21)  and conditions A1–A4 in the supplement. Assume that
the tuning parameters λ1  λ2 satisfy λ1 (cid:16) (mn log(d2mr/δ) log3 n)1/2 and λ2 = o(n41/42/θ∗
max) 
(cid:96)kj(cid:107)2. Then for n sufﬁciently large  the MQGM estimate in (5) exactly
where θ∗
recovers the underlying conditional dependencies  i.e.  ˆE = E∗  with probability at least 1 − δ.

(cid:96)kj ∈ Rm  j = 1  . . .   d  as in the basis expansion model.

max = max(cid:96) k j (cid:107)θ∗

5

The theorem shows that the nonzero pattern in the MQGM estimate identiﬁes  with high probability 
the underlying conditional independencies. But to be clear  we emphasize that the MQGM estimate
is not an estimate of the inverse covariance matrix itself (this is also true of neighborhood regression 
SpaceJam of [40]  and many other methods for learning graphical models).

5 Computational approach

By design  the MQGM problem in (5) separates into d subproblems  across k = 1  . . .   d (it therefore
sufﬁces to consider only a single subproblem  so we omit notational dependence on k for auxil-
iary variables). While these subproblems are challenging for off-the-shelf solvers (even for only
moderately-sized graphs)  the key terms here all admit efﬁcient proximal operators [32]  which makes
operator splitting methods like the alternating direction method of multipliers [5] a natural choice.
As an illustration  we consider the non-crossing constraints in the basis regression model below.
Reparameterizing our problem  so that we may apply ADMM  yields:

(cid:80)d
j=1 (cid:107)W(cid:96)j(cid:107)2 + λ2

(cid:80)r
(cid:80)d
(8)
k + ΦΘk  W = Θk  Z = Yk1T − 1BT
j=1 ψα(cid:96) (A(cid:96)j)  and I+(·) is the indicator function of the space

F + I+(V DT )
k − ΦΘk 

2 (cid:107)W(cid:107)2

(cid:96)=1

minimize
Θk Bk V W Z
subject to

where for brevity ψA(A) =(cid:80)r

V = 1BT

ψA(Z) + λ1

(cid:96)=1

of elementwise nonnegative matrices. The augmented Lagrangian associated with (8) is:

Lρ(Θk  Bk  V  W  Z  UV   UW   UZ) = ψA(Z) + λ1

(cid:107)W(cid:107)2

F + I+(V DT )

d(cid:88)
r(cid:88)
F + (cid:107)Yk1T − 1BT
F + (cid:107)Θk − W + UW(cid:107)2

(cid:107)W(cid:96)j(cid:107)2 +

λ2
2

j=1

(cid:96)=1

(cid:16)(cid:107)1BT

(cid:17)

 

(cid:1)  

(cid:0)1BT
(cid:18)

+

ρ
2

k + ΦΘk − V + UV (cid:107)2

k − ΦΘk − Z + UZ(cid:107)2
F
(9)
where ρ > 0 is the augmented Lagrangian parameter  and UV   UW   UZ are dual variables correspond-
ing to the equality constraints on V  W  Z  respectively. Minimizing (9) over V yields:

V ← Piso

(cid:19)

(10)
where Piso(·) denotes the row-wise projection operator onto the isotonic cone (the space of compo-
nentwise nondecreasing vectors)  an O(nr) operation here [15]. Minimizing (9) over W(cid:96)j yields the
update:

k + ΦΘk + UV

λ1/ρ

W(cid:96)j ← (Θk)(cid:96)j + (UW )(cid:96)j

1 −

 

(11)
where (·)+ is the positive part operator. This can be seen by deriving the proximal operator of the
function f (x) = λ1(cid:107)x(cid:107)2 + (λ2/2)(cid:107)x(cid:107)2

2. Minimizing (9) over Z yields the update:

(cid:107)(Θk)(cid:96)j + (UW )(cid:96)j(cid:107)2

1 + λ2/ρ

Z ← prox(1/ρ)ψA (Yk1T − 1bT

(12)
where proxf (·) denotes the proximal operator of a function f. For the multiple quantile loss function
ψA  this is a kind of generalized soft-thresholding. The proof is given in the supplement.
Lemma 5.1. Let P+(·) and P−(·) be the elementwise positive and negative part operators  respec-
tively  and let a = (α1  . . .   αr). Then proxtψA (A) = P+(A − t1aT ) + P−(A − t1aT ).
Finally  differentiation in (9) with respect to Bk and Θk yields the simultaneous updates:

k − ΦΘk + UZ) 

+

(cid:21)

(cid:20) Θk

BT
k

(cid:20) ΦT Φ + 1

← 1
2

2 I ΦT 1
1T 1

1T Φ

(cid:21)−1(cid:18)

[I 0]T (W − UW ) +

[Φ 1]T (Yk1T − Z + UZ + V − UV )

(cid:19)

.

(13)

A complete description of our ADMM algorithm for solving the MQGM problem is in the supplement.
Gibbs sampling Having ﬁt the conditionals yk|y¬k  k = 1  . . . d  we may want to make predictions
or extract joint distributions over subsets of variables. As discussed in Section 4.2  there is no general
analytic form for these joint distributions  but the pseudolikelihood approximation underlying the
MQGM suggests a natural Gibbs sampler. A careful implementation that respects the additive model
in (3) yields a highly efﬁcient Gibbs sampler  especially for CRFs; the supplement gives details.

6

6 Empirical examples

6.1 Synthetic data

We consider synthetic examples  comparing the MQGM to neighborhood selection (MB)  the graphi-
cal lasso (GLasso)  SpaceJam [40]  the nonparanormal skeptic [26]  TIGER [24]  and neighborhood
selection using the absolute loss (Laplace).

Ring example As a simple but telling example  we drew n = 400 samples from a “ring” distribution
in d = 4 dimensions. Data were generated by drawing a random angle ν ∼ Uniform(0  1)  a random
radius R ∼ N (0  0.1)  and then computing the coordinates y1 = R cos ν  y2 = R sin ν and
y3  y4 ∼ N (0  1)  i.e.  y1 and y2 are the only dependent variables here. The MQGM was used with
m = 10 basis functions (RBFs)  and r = 20 quantile levels. The left panel of Figure 1 plots samples
(blue) of the coordinates y1  y2 as well as new samples from the MQGM (red) ﬁtted to these same
(blue) samples  obtained by using our Gibbs sampler; the samples from the MQGM appear to closely
match the samples from the underlying ring. The main panel of Figure 1 shows the conditional
dependencies recovered by the MQGM  SpaceJam  GLasso  and MB (plots for the other methods are
given in the supplement)  when run on the ring data. We visualize these dependencies by forming a
d × d matrix with the cell (j  k) set to black if j  k are conditionally dependent given the others  and
white otherwise. Across a range of tuning parameters for each method  the MQGM is the only one
that successfully recovers the underlying conditional dependencies  at some point along its solution
path. In the supplement  we present an evaluation of the conditional CDFs given by each method 
when run on the ring data; again  the MQGM performs best in this setting.
Larger examples To investigate performance at larger scales  we drew n ∈ {50  100  300} samples
from a multivariate normal and Student t-distribution (with 3 degrees of freedom)  both in d = 100
dimensions  both parameterized by a random  sparse  diagonally dominant d × d inverse covariance
matrix  following the procedure in [33  17  31  1]. Over the same set of sample sizes  with d = 100  we
also considered an autoregressive setup in which we drew samples of pairs of adjacent variables from
the ring distribution. In all three data settings (normal  t  and autoregressive)  we used m = 10 and
r = 20 for the MQGM. To summarize the performances  we considered a range of tuning parameters
for each method  computed corresponding false and true positive rates (in detecting conditional
dependencies)  and then computed the corresponding area under the curve (AUC)  following  e.g. 
[33  17  31  1]. Table 1 reports the median AUCs (across 50 trials) for all three of these examples; the
MQGM outperforms all other methods on the autoregressive example; on the normal and Student t
examples  it performs quite competitively.

Figure 1: Left: data from the ring distribution (blue) as well as new samples from the MQGM (red) ﬁtted to
the same (blue) data  obtained by using our Gibbs sampler. Right: conditional dependencies recovered by the
MQGM  MB  GLasso  and SpaceJam on the ring data; black means conditional dependence. The MQGM is the
only method that successfully recovers the underlying conditional dependencies along its solution path.

Table 1: AUC values for the MQGM  MB  GLasso  SpaceJam  the nonparanormal skeptic  TIGER  and
Laplace for the normal  t  and autoregressive data settings; higher is better  best in bold.

MQGM
MB
GLasso
SpaceJam
Nonpara.
TIGER
Laplace

n = 50
0.953
0.850
0.908
0.889
0.881
0.732
0.803

Normal
n = 100

0.976
0.959
0.964
0.968
0.962
0.921
0.931

n = 300

0.988
0.994
0.998
0.997
0.996
0.996
0.989

n = 50
0.928
0.844
0.691
0.893
0.862
0.420
0.800

Student t
n = 100

0.947
0.923
0.605
0.965
0.942
0.873
0.876

n = 300

0.981
0.988
0.965
0.993
0.998
0.989
0.991

n = 50
0.726
0.532
0.541
0.624
0.545
0.503
0.530

Autoregressive

n = 100

0.754
0.563
0.620
0.708
0.590
0.518
0.554

n = 300

0.955
0.725
0.711
0.854
0.612
0.718
0.758

7

−1.5−1.0−0.50.00.51.01.5y1−1.5−1.0−0.50.00.51.01.5y2truthMQGMMQGMTruth¸1=8.00000¸1=16.00000¸1=32.00000¸1=64.00000¸1=128.00000MBTruth¸1=0.12500¸1=0.25000¸1=0.50000¸1=1.00000¸1=2.00000GLassoTruth¸1=0.00781¸1=0.01562¸1=0.03125¸1=0.06250¸1=0.12500SpaceJamTruth¸1=0.06250¸1=0.12500¸1=0.25000¸1=0.50000¸1=1.00000Figure 2: Top panel and bottom row 
middle panel: conditional dependen-
cies recovered by the MQGM on the
ﬂu data; each of the ﬁrst ten cells corre-
sponds to a region of the U.S.  and black
means dependence. Bottom row  left
panel: wallclock time (in seconds) for
solving one subproblem using ADMM
versus SCS. Bottom row  right panel:
samples from the ﬁtted marginal distri-
bution of the weekly ﬂu incidence rates
at region 6; samples at larger quantiles
are shaded lighter  and the median is in
darker blue.

6.2 Modeling ﬂu epidemics

We study n = 937 weekly ﬂu incidence reports from September 28  1997 through August 30 
2015  across 10 regions in the United States (see the top panel of Figure 2)  obtained from [6]. We
considered d = 20 variables: the ﬁrst 10 encode the current week’s ﬂu incidence (precisely  the
percentage of doctor’s visits in which ﬂu-like symptoms are presented) in the 10 regions  and the last
10 encode the same but for the prior week. We set m = 5  r = 99  and also introduced exogenous
variables to encode the week numbers  so p = 1. Thus  learning the MQGM here corresponds
to learning the structure of a spatiotemporal graphical model  and reduces to solving 20 multiple
quantile regression subproblems  each of dimension (19 × 5 + 1) × 99 = 9504. All subproblems
took about 1 minute on a 6 core 3.3 Ghz Core i7 X980 processor.
The bottom left panel in Figure 2 plots the time (in seconds) taken for solving one subproblem using
ADMM versus SCS [30]  a cone solver that has been advocated as a reasonable choice for a class
of problems encapsulating (4); ADMM outperforms SCS by roughly two orders of magnitude. The
bottom middle panel of Figure 2 presents the conditional independencies recovered by the MQGM.
Nonzero entries in the upper left 10 × 10 submatrix correspond to dependencies between the yk
variables for k = 1  . . .   10; e.g.  the nonzero (0 2) entry suggests that region 1 and 3’s ﬂu reports are
dependent. The lower right 10 × 10 submatrix corresponds to the yk variables for k = 11  . . .   20 
and the nonzero banded entries suggest that at any region the previous week’s ﬂu incidence (naturally)
inﬂuences the next week’s. The top panel of Figure 2 visualizes these relationships by drawing an
edge between dependent regions; region 6 is highly connected  suggesting that it may be a bellwether
for other regions  roughly in keeping with the current understanding of ﬂu dynamics. To draw samples
from the ﬁtted distributions  we ran our Gibbs sampler over the year  generating 1000 total samples 
making 5 passes over all coordinates between each sample  and with a burn-in period of 100 iterations.
The bottom right panel of Figure 2 plots samples from the marginal distribution of the percentages
of ﬂu reports at region 6 (other regions are in the supplement) throughout the year  revealing the
heteroskedastic nature of the data.
For space reasons  our last example  on wind power data  is presented in the supplement.

7 Discussion

We proposed and studied the Multiple Quantile Graphical Model (MQGM). We established theoretical
and empirical backing to the claim that the MQGM is capable of compactly representing relationships
between heteroskedastic non-Gaussian variables. We also developed efﬁcient algorithms for both
estimation and sampling in the MQGM. All in all  we believe that our work represents a step forward
in the design of ﬂexible yet tractable graphical models.

Acknowledgements
AA was supported by DOE Computational Science Graduate Fellowship DE-
FG02-97ER25308. JZK was supported by an NSF Expeditions in Computation Award  CompSustNet 
CCF-1522054. RJT was supported by NSF Grants DMS-1309174 and DMS-1554123.

8

1234567891012345678910050100150200250Seconds101102103104Objective valueMQGMSCS16111616111630405081828Week0123456789% of flu-like symptomsRegion 6References
[1] Alnur Ali  Kshitij Khare  Sang-Yun Oh  and Bala Rajaratnam. Generalized pseudolikelihood methods for inverse covariance estimation.

Technical report  2016. Available at http://arxiv.org/pdf/1606.00033.pdf.

[2] Onureena Banerjee  Laurent El Ghaoui  and Alexandre d’Aspremont. Model selection through sparse maximum likelihood estimation

for multivariate Gaussian or binary data. Journal of Machine Learning Research  9:485–516  2008.

[3] Alexandre Belloni and Victor Chernozhukov. (cid:96)1-penalized quantile regression in high-dimensional sparse models. Annals of Statistics 

[4] Julian Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society: Series B  36(2):

39(1):82–130  2011.

192–236  1974.

[5] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed optimization and statistical learning via the

alternating direction method of multipliers. Foundations and Trends in Machine Learning  3(1):1–122  2011.

[6] Centers for Disease Control and Prevention (CDC). Inﬂuenza national and regional level graphs and data  August 2015. URL http:

//gis.cdc.gov/grasp/fluview/fluportaldashboard.html.

[7] Shizhe Chen  Daniela Witten  and Ali Shojaie. Selection and estimation for mixed graphical models. Biometrika  102(1):47–64  2015.
[8] Arthur Dempster. Covariance selection. Biometrics  28(1):157–175  1972.
[9] Jianqing Fan  Yingying Fan  and Emre Barut. Adaptive robust variable selection. Annals of Statistics  42(1):324–351  2014.
[10] Michael Finegold and Mathias Drton. Robust graphical modeling of gene networks using classical and alternative t-distributions. Annals

[11] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics  9

of Applied Statistics  5(2A):1057–1080  2011.

(3):432–441  2008.

[12] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Applications of the lasso and grouped lasso to the estimation of sparse graphical

models. Technical report  2010. Available at http://statweb.stanford.edu/~tibs/ftp/ggraph.pdf.

[13] David Heckerman  David Maxwell Chickering  David Meek  Robert Rounthwaite  and Carl Kadie. Dependency networks for inference 

collaborative ﬁltering  and data visualization. Journal of Machine Learning Research  1:49–75  2000.

[14] Holger Höﬂing and Robert Tibshirani. Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods. Journal of

Machine Learning Research  10:883–906  2009.

[15] Nicholas Johnson. A dynamic programming algorithm for the fused lasso and (cid:96)0-segmentation. Journal of Computational and Graphical

[16] Kengo Kato. Group lasso for high dimensional sparse quantile regression models. Technical report  2011. Available at http://arxiv.

Statistics  22(2):246–260  2013.

org/pdf/1103.1458.pdf.

[17] Kshitij Khare  Sang-Yun Oh  and Bala Rajaratnam. A convex pseudolikelihood framework for high dimensional partial correlation

estimation with convergence guarantees. Journal of the Royal Statistical Society: Series B  77(4):803–825  2014.

[18] Roger Koenker. Quantile Regression. Cambridge University Press  2005.
[19] Roger Koenker. Additive models for quantile regression: Model selection and conﬁdence bandaids. Brazilian Journal of Probability and

Statistics  25(3):239–262  2011.

[20] Roger Koenker and Gilbert Bassett. Regression quantiles. Econometrica  46(1):33–50  1978.
[21] Roger Koenker  Pin Ng  and Stephen Portnoy. Quantile smoothing splines. Biometrika  81(4):673–680  1994.
[22] Steffen Lauritzen. Graphical models. Oxford University Press  1996.
[23] Jason Lee and Trevor Hastie. Structure learning of mixed graphical models. In Proceedings of the 16th International Conference on

[24] Han Liu and Lie Wang. TIGER: A tuning-insensitive approach for optimally estimating Gaussian graphical models. Technical report 

Artiﬁcial Intelligence and Statistics  pages 388–396  2013.

2012. Available at http://arxiv.org/pdf/1209.2437.pdf.

Journal of Machine Learning Research  10:2295–2328  2009.

models. The Annals of Statistics  pages 2293–2326  2012.

[25] Han Liu  John Lafferty  and Larry Wasserman. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs.

[26] Han Liu  Fang Han  Ming Yuan  John Lafferty  and Larry Wasserman. High-dimensional semiparametric Gaussian copula graphical

[27] Lukas Meier  Sara van de Geer  and Peter Buhlmann. High-dimensional additive modeling. Annals of Statistics  37(6):3779–3821  2009.
[28] Nicolai Meinshausen and Peter Bühlmann. High-dimensional graphs and variable selection with the lasso. Annals of Statistics  34(3):

[29] Jennifer Neville and David Jensen. Dependency networks for relational data. In Proceedings of Fourth IEEE International Conference

1436–1462  2006.

on the Data Mining  pages 170–177. IEEE  2004.

[30] Brendan O’Donoghue  Eric Chu  Neal Parikh  and Stephen Boyd. Operator splitting for conic optimization via homogeneous self-dual

embedding. Technical report  2013. Available at https://stanford.edu/~boyd/papers/pdf/scs.pdf.

[31] Sang-Yun Oh  Onkar Dalal  Kshitij Khare  and Bala Rajaratnam. Optimization methods for sparse pseudolikelihood graphical model

selection. In Advances in Neural Information Processing Systems 27  pages 667–675  2014.

[32] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization  1(3):123–231  2013.
[33] Jie Peng  Pei Wang  Nengfeng Zhou  and Ji Zhu. Partial correlation estimation by joint sparse regression models. Journal of the American

Statistical Association  104(486):735–746  2009.

[34] Garvesh Raskutti  Martin Wainwright  and Bin Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex

programming. Journal of Machine Learning Research  13:389–427  2012.

[35] Guilherme Rocha  Peng Zhao  and Bin Yu. A path following algorithm for sparse pseudo-likelihood inverse covariance estimation

(SPLICE). Technical report  2008. Available at https://www.stat.berkeley.edu/~binyu/ps/rocha.pseudo.pdf.

[36] Adam Rothman  Peter Bickel  Elizaveta Levina  and Ji Zhu. Sparse permutation invariant covariance estimation. Electronic Journal of

Statistics  2:494–515  2008.

[38]

[37] Kyung-Ah Sohn and Seyoung Kim. Joint estimation of structured sparsity and output structure in multiple-output regression via inverse
covariance regularization. In Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics  pages 1081–1089 
2012.
Ichiro Takeuchi  Quoc Le  Timothy Sears  and Alexander Smola. Nonparametric quantile estimation. Journal of Machine Learning
Research  7:1231–1264  2006.

[39] Cristiano Varin and Paolo Vidoni. A note on composite likelihood inference and model selection. Biometrika  92(3):519–528  2005.
[40] Arend Voorman  Ali Shojaie  and Daniela Witten. Graph estimation with joint additive models. Biometrika  101(1):85–101  2014.
[41] Yuchung Wang and Edward Ip. Conditionally speciﬁed continuous distributions. Biometrika  95(3):735–746  2008.
[42] Matt Wytock and Zico Kolter. Sparse Gaussian conditional random ﬁelds: Algorithms  theory  and application to energy forecasting. In

Proceedings of the 30th International Conference on Machine Learning  pages 1265–1273  2013.

[43] Eunho Yang  Pradeep Ravikumar  Genevera Allen  and Zhandong Liu. Graphical models via generalized linear models. In Advances in

Neural Information Processing Systems 25  pages 1358–1366  2012.

[44] Eunho Yang  Pradeep Ravikumar  Genevera Allen  and Zhandong Liu. Graphical models via univariate exponential family distributions.

Journal of Machine Learning Research  16:3813–3847  2015.

[45] Ming Yuan and Yi Lin. Model selection and estimation in the Gaussian graphical model. Biometrika  94(1):19–35  2007.
[46] Xiao-Tong Yuan and Tong Zhang. Partial Gaussian graphical model estimation.

IEEE Transactions on Information Theory  60(3):

1673–1687  2014.

9

,Alnur Ali
J. Zico Kolter
Ryan Tibshirani