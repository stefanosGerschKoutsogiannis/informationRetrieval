2012,Perfect Dimensionality Recovery by Variational Bayesian PCA,The variational Bayesian (VB) approach is one of the best tractable approximations to the Bayesian estimation  and it was demonstrated to perform well in many applications. However  its good performance was not fully understood theoretically. For example  VB sometimes produces a sparse solution  which is regarded as a practical advantage of VB  but such sparsity is hardly observed in the rigorous Bayesian estimation. In this paper  we focus on probabilistic PCA and give more theoretical insight into the empirical success of VB. More specifically  for the situation where the noise variance is unknown  we derive a sufficient condition for perfect recovery of the true PCA dimensionality in the large-scale limit when the size of an observed matrix goes to infinity. In our analysis  we obtain bounds for a noise variance estimator and simple closed-form solutions for other parameters  which themselves are actually very useful for better implementation of VB-PCA.,Perfect Dimensionality Recovery

by Variational Bayesian PCA

Shinichi Nakajima
Nikon Corporation

Tokyo  140-8601  Japan

nakajima.s@nikon.co.jp

Ryota Tomioka

The University of Tokyo
Tokyo 113-8685  Japan

tomioka@mist.i.u-tokyo.ac.jp

Masashi Sugiyama

Tokyo Institute of Technology

Tokyo 152-8552  Japan

sugi@cs.titech.ac.jp

S. Derin Babacan

University of Illinois at Urbana-Champaign

Urbana  IL 61801  USA

dbabacan@illinois.edu

Abstract

The variational Bayesian (VB) approach is one of the best tractable approxima-
tions to the Bayesian estimation  and it was demonstrated to perform well in many
applications. However  its good performance was not fully understood theoreti-
cally. For example  VB sometimes produces a sparse solution  which is regarded
as a practical advantage of VB  but such sparsity is hardly observed in the rigorous
Bayesian estimation. In this paper  we focus on probabilistic PCA and give more
theoretical insight into the empirical success of VB. More speciﬁcally  for the sit-
uation where the noise variance is unknown  we derive a sufﬁcient condition for
perfect recovery of the true PCA dimensionality in the large-scale limit when the
size of an observed matrix goes to inﬁnity. In our analysis  we obtain bounds for
a noise variance estimator and simple closed-form solutions for other parameters 
which themselves are actually very useful for better implementation of VB-PCA.

1 Introduction

Variational Bayesian (VB) approximation [1] was proposed as a computationally efﬁcient alternative
to rigorous Bayesian estimation. The key idea is to force the posterior to be factorized  so that the
integration—a typical intractable operation in Bayesian methods—can be analytically performed
over each parameter with the other parameters ﬁxed. VB has been successfully applied to many
applications [4  7  20  11].
Typically  VB solves a non-convex optimization problem with an iterative algorithm [3]  which
makes theoretical analysis difﬁcult. An important exceptional case is the matrix factorization (MF)
model [11  6  19] with no missing entry in the observed matrix. Recently  the global analytic solution
of VBMF has been derived and theoretical properties such as the mechanism of sparsity induction
have been revealed [15  16]. These works also posed thought-provoking relations between VB
and rigorous Bayesian estimation: The VB posterior is actually quite different from the true Bayes
posterior (compare the left and the middle graphs in Fig. 1)  and VB induces sparsity in its solution
but such sparsity is hardly observed in rigorous Bayesian estimation (see the right graph in Fig. 1).1
These facts might deprive the justiﬁcation of VB based solely on the fact that it is one of the best
tractable approximations to the Bayesian estimation.

1Also in mixture models  inappropriate model pruning by VB approximation was discussed [12].

1

Bayes p osterior (V = 1)

MAP estimators:

0

.

2
.
0

3

(A  B ) ≈ (± 1  ± 1)

3

2

1

0

B

0.1

0 . 2
0.3

0.1

0.2
0.3

0.3
0.2
0.1

0
.
3

0
.
2

0
.
1

−2

−1

−1

−2

−3
−3

0
.
1

0
.
2

0
.
3

0.3
0 . 2

0.1

0.1
0.2
0.3

0.3
0.2

0.1

B

2
.
0

0

.

3

0
A

1

2

3

3

2

1

0

−1

−2

−3
−3

VB p osterior (V = 1)

VB estimator : (A  B ) = (0  0)

0.05

0.1

0

.

0
5

0.05

0.05

0.1

0.15

0

.

1

5

0.1

0.1

0.05

0.05

5

0 . 0

−2

−1

0
A

1

2

3

3

2

1

 

!U

F B

M AP

VB

EF B

EM AP

EVB

1

V

2

 

3

Figure 1: Dissimilarities between VB and the rigorous Bayesian estimation. (Left and Center) the
Bayes posterior and the VB posterior of a 1 × 1 MF model  V = BA + E  when V = 1 is observed
(E is a Gaussian noise). VB approximates the Bayes posterior having two modes by an origin-
centered Gaussian  which induces sparsity. (Right) Behavior of estimators of U = BA!  given the
observation V . The VB estimator (the magenta solid curve) is zero when V ≤ 1  which means exact
sparsity. On the other hand  FB (fully-Bayesian or rigorous Bayes; blue crosses) shows no sign of
sparsity. Further discussion will be provided in Section 5.2. All graphs are quoted from [15].

Since the probabilistic PCA [21  18  3] is an instance of MF  the global analytic solution derived
in [16] for MF can be utilized for analyzing the probabilistic PCA. Indeed  automatic dimensional-
ity selection of VB-PCA  which is an important practical advantage of VB-PCA  was theoretically
investigated in [17]. However  the noise variance  which is usually unknown in many realistic appli-
cations of PCA  was treated as a given constant in that analysis.2 In this paper  we consider a more
practical and challenging situation where the noise variance is unknown  and theoretically analyze
VB-PCA.
It was reported that noise variance estimation fails in some Bayesian approximation methods  if the
formal rank is set to be full [17]. With such methods  an additional model selection procedure is
required for dimensionality selection [14  5]. On the other hand  we theoretically show in this paper
that VB-PCA can estimate the noise variance accurately  and therefore automatic dimensionality se-
lection works well. More speciﬁcally  we establish a sufﬁcient condition that VB-PCA can perfectly
recover the true dimensionality in the large-scale limit when the size of an observed matrix goes to
inﬁnity. An interesting ﬁnding is that  although the objective function minimized for noise variance
estimation is multimodal in general  only a local search algorithm is required for perfect recovery.
Our results are based on the random matrix theory [2  5  13  22]  which elucidates the distribution
of singular values in the large-scale limit.
In the development of the above theoretical analysis  we obtain bounds for the noise variance esti-
mator and simple closed-form solutions for other parameters. We also show that they can be nicely
utilized for better implementation of VB-PCA.

2 Formulation

In this section  we introduce the variational Bayesian matrix factorization (VBMF).

2.1 Bayesian Matrix Factorization
Assume that we have an observation matrix V ∈ RL×M  which is the sum of a target matrix U ∈
RL×M and a noise matrix E∈ RL×M:

V = U + E.

In the matrix factorization model  the target matrix is assumed to be low rank  which can be ex-
pressed as the following factorizability:

2If the noise variance is known  we can actually show that dimensionality selection by VB-PCA is outper-

formed by a naive strategy (see Section 3.3). This means that VB-PCA is not very useful in this setting.

U = BA! 

2

where A ∈ RM×H and B ∈ RL×H. $ denotes the transpose of a matrix or vector. Thus  the rank
of U is upper-bounded by H ≤ min(L  M ).
In this paper  we consider the probabilistic matrix factorization (MF) model [19]  where the obser-
vation noise E and the priors of A and B are assumed to be Gaussian:
p(V |A  B) ∝ exp!− 1
Fro"  
2σ2’V − BA!’2
p(B) ∝ exp!− 1
2 tr!AC−1
A A!""  

(1)
(2)
Here  we denote by ’·’ Fro the Frobenius norm  and by tr(·) the trace of a matrix. We assume that
L ≤ M. If L > M  we may simply re-deﬁne the transpose V ! as V so that L ≤ M holds.3 Thus 
this does not impose any restriction. We assume that the prior covariance matrices CA and CB are
diagonal and positive deﬁnite  i.e. 
CA = diag(c2

p(A) ∝ exp!− 1

2 tr!BC−1

B B!"" .

CB = diag(c2

b1  . . .   c2

bH )

for cah  cbh > 0  h = 1  . . .   H. Without loss of generality  we assume that the diagonal entries of
the product CACB are arranged in non-increasing order  i.e.  cahcbh ≥ cah! cbh! for any pair h < h$.
Throughout the paper  we denote a column vector of a matrix by a bold lowercase letter  and a row
vector by a bold lowercase letter with a tilde  namely 

a1  . . .   c2

aH ) 

A = (a1  . . .   aH) = (#a1  . . .  #aM )! ∈ RM×H  B = (b1  . . .   bH) =$#b1  . . .  #bL%!

2.2 Variational Bayesian Approximation

∈ RL×H.

The Bayes posterior is given by

p(A  B|V ) = p(V |A B)p(A)p(B)

(3)
where p(Y ) = )p(V |A  B)*p(A)p(B). Here  )·*p denotes the expectation over the distribution p.
Since this expectation is intractable  we need to approximate the Bayes posterior.
Let r(A  B)  or r for short  be a trial distribution. The following functional with respect to r is called
the free energy:

p(V )

 

(4)

F (r) =&log

r(A B)

p(V |A B)p(A)p(B)’r(A B)

=&log r(A B)

p(A B|V )’r(A B) − log p(V ).

In the last equation  the ﬁrst term is the Kullback-Leibler (KL) divergence from the trial distribution
to the Bayes posterior  and the second term is a constant. Therefore  minimizing the free energy (4)
amounts to ﬁnding a distribution closest to the Bayes posterior in the sense of the KL divergence. A
general approach to Bayesian approximate inference is to ﬁnd the minimizer of the free energy (4)
with respect to r in some restricted function space.
In the VB approximation  the independence between the entangled parameter matrices A and B is
assumed:

r = r(A)r(B).

(5)
Under this constraint  an iterative algorithm for minimizing the free energy (4) was derived [3  11].

Let(r be such a minimizer  and we deﬁne the MF solution by the mean of the target matrix U:
In the context of PCA where V is a data matrix  the solution is given as the subspace spanned by (U.

The MF model has hyperparameters (CA  CB) in the priors (2). By manually choosing them  we
can control regularization and sparsity of the solution (e.g.  the PCA dimensions). A popular way
to set the hyperparameter in the Bayesian framework is again based on the minimization of the free
energy (4):

(U =)BA!*!r(A B) .

(6)

We refer to this method as an empirical VB (EVB) method. When the noise variance σ2 is unknown 
it can also be estimated as

((CA (CB) = argminCA CB (minr F (r; CA  CB|V )) .
(σ2 = argminσ2!minr CA CB F (r; CA  CB σ 2|V )" .

3When the number of samples is larger (smaller) than the data dimensionality in the PCA setting  the

observation matrix V should consist of the columns (rows)  each of which corresponds to each sample.

3

3 Simple Closed-Form Solutions of VBMF

Recently  the global analytic solution of VBMF has been derived [16]. However  it is given as a
solution of a quartic equation (Corollary 1 in [16])  and it is not easy to use for further analysis due
to its complicated expression. In this section  we derive much simpler forms  which will be used for
analyzing VB-PCA in the next section.

3.1 VB Solution

Our new analytic-form solution only involves linear and quadratic equations  which is summarized
in the following theorem (the proof is omitted due to the space limitation):

Theorem 1 Let

(7)
be the singular value decomposition (SVD) of V with its singular values {γh} arranged in non-
increasing order  and the associated right and left singular vectors {ωah  ωbh}. Then  the VB solu-
tion can be written as a truncated shrinkage SVD as follows:

h=1 γhωbhω!ah

V =+H

Here  the truncation threshold and the shrinkage estimator are  respectively  given by

if γh ≥ γVB
h  
otherwise.

(U VB =

h ωbhω!ah 

H h=1(γVB
h = σ.//0 (L+M )
h = γh21 − σ2

2γ2

γVB

˘γVB

where

h

0

h =-˘γVB
(γVB
+12 (L+M )

2 + σ2
c2
2c2

ah

2 + σ2
c2
2c2
bh

ah

bh32
bh33 .
h2M + L +4(M − L)2 + 4γ2

c2
ah

h
c2

− LM  

We can also derive a simple closed-form expression of the VB posterior (omitted).

3.2 EVB Solution

Combining Theorem 1 with the global EVB solution (Corollary 2 in [16])  we have the following
theorem (the proof is omitted):

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

Theorem 2 Let

α = L
M  

and let κ = κ(α) (> 1) be the zero-cross point of the following decreasing function:

x
Then  the EVB solution can be written as a truncated shrinkage SVD as follows:

Φ(x) = log(x+1)

− 1
2 .

where

Ξ (κ; α) = Φ (√ακ) + Φ$ κ√α%  
H h=1(γEVB
(γEVB
γEVB = σ4M + L + √LM$κ + 1
κ% 

h ωbhω!ah 

where

h

(U EVB =

Here  the truncation threshold and the shrinkage estimator are  respectively  given by

=5˘γEVB

0

h

if γh ≥ γEVB 
otherwise.

˘γEVB
h

= γh

2 61 − (M +L)σ2

γ2
h

+4$1 − (M +L)σ2

γ2
h

h 7 .
− 4LMσ4

γ4

%2

The EVB threshold (14) involves κ  which needs to be numerically computed. We can easily prepare
a table of the values for 0 <α ≤ 1 beforehand  like the cumulative Gaussian probability used in
statistical tests. Fig. 2 shows the EVB threshold (14) by a red solid curve labeled as ‘EVB’.

4

2

σ
M
√
/
γ

2.5
2
1.5
1
0.5
0
 
0

 

EVB
M PUL
VB F L

0.2

0.4

α

0.6

0.8

1

)
u
(
p

3

2

1

0
 
0

!u"p ( u )

 

α = 1
α = 0.1

u(0.1)

u(1)

1

2
u = γ

3
2/(σ
2 ∗M )

4

5

6

4

2

0
 
0

 

x

ψ0(x)
ψ(x)

6

8

2

4

x

Figure 2: Thresholds.

Figure 3: Marˇcenko-Pastur law.

Figure 4: ψ0(x) and ψ(x).

3.3 Large-Scale Limiting Behavior of EVB When Noise Variance Is Known

Here  we ﬁrst introduce a result from random matrix theory [13  22]  and then discuss the behavior
of EVB when the noise variance is known.
Assume that E∈ RL×M is a random matrix such that each element is independently drawn from a
distribution with mean zero and variance σ2∗ (not necessarily Gaussian). Let u1  u2  . . .   uL be the
eigenvalues of

Mσ 2∗EE!  and deﬁne the empirical distribution of the eigenvalues by

1

p(u) = 1

L (δ(u1) + δ(u2) + ··· + δ(uL))  

where δ(u) denotes the Dirac measure at u. Then the following proposition holds:

Proposition 1 (Marˇcenko-Pastur law) [13  22] In the large-scale limit when L and M go to in-
ﬁnity with its ratio α = L/M ﬁxed  the probability measure of the empirical distribution of the
eigenvalue u of

1

σ2∗M EE! converges almost surely to
√(u−u)(u−u)

p(u)du =

2παu

θ(u < u < u)du 

γMPUL = √Mσ 2∗u = (√L + √M )σ∗ 

(16)
where u = (1 − √α)2  u = (1 + √α)2  and θ(·) denotes an indicator function such that
θ(condition) = 1 if the condition is true and θ(condition) = 0 otherwise.
Fig. 3 shows the Marˇcenko-Pastur (MP) distributions for α = 0.1  1. The mean )u*p(u) = 1 (which
is constant for any 0 <α ≤ 1) and the upper-limit u = u(α) for α = 0.1  1 are indicated by
arrows. Note that the MP distribution appears for a single sample matrix; different from standard
“large-sample” theories  we do not need many sample matrices (this property is sometimes called
self-averaging). This single-sample property of the MP distribution is highly useful in our analysis
because we are working with a single observation matrix in the MF scenario.
Proposition 1 states that all singular values of the random matrix E are almost surely upper-bounded
by
(17)
which we call the Marˇcenko-Pastur upper-limit (MPUL). This property can be used for designing
estimators robust against noise [10  9]. Although EVB-PCA was proposed independently from the
random matrix theory [3]  its good performance can be proven with a related property to Proposi-
tion 1  as shown in Section 4.
When the noise variance is known  we can set the parameter to σ = σ∗ in Eq.(1). We depicted
MPUL (17) for this case in Fig. 2. We can see that MPUL lower-bounds the EVB threshold (14)
(actually this is true regardless of the value of κ > 0). This implies a nice behavior of EVB  i.e. 
EVB eliminates all noise components in the large-scale limit. However  a simple optimal strategy—
discarding the components with singular values smaller than γMPUL—outperforms EVB  because
signals lying between the gap [γMPUL γ EVB) are discarded by EVB. Therefore  EVB is not very
useful when σ2∗ is known. In Section 4  we investigate the behavior of EVB in a more practical and
challenging situation where σ2∗ is unknown and is also estimated from observation.
In Fig. 2  we also depicted the VB threshold (9) with almost ﬂat prior cah  cbh → ∞ (labeled
as ‘VBFL’) for comparison. Actually  this coincides with the mean of the MP distribution  i.e. 
h )2/(Mσ 2) = )u*p(u) = 1. This implies that VBFL retains a lot of noise com-
limcah  cah→∞(γVB
ponents  and does not perform well even when σ2∗ is known.

5

4 Analysis of EVB When Noise Variance Is Unknown

In this section  we derive bounds of the VB-based noise variance estimator  and obtain a sufﬁcient
condition for perfect dimensionality recovery in the large-scale limit.

4.1 Bounds of Noise Variance Estimator

The simple closed-form solution obtained in Section 3 is the global minimizer of the free energy
(4)  given σ2. Using the solution  we can explicitly describe the free energy as a function of σ2. We
obtain the following theorem (the proof is omitted):

Theorem 3 The noise variance estimator(σ2 EVB is the global minimizer of

h

Ω(σ−2) =+H

h=1 ψ$ γ2

where ψ (x) = ψ0 (x) + θ (x > x) ψ1 (x)  

h

Mσ 2%  
h=H+1 ψ0$ γ2
Mσ 2% ++L
x = 1 + α + √α!κ + κ−1"  
ψ1 (x) = log (√ακ(x) + 1) + α log$ κ(x)√α + 1% − √ακ(x) 
2√α2(x − (1 + α)) +8(x − (1 + α))2 − 4α3 .

κ is a constant deﬁned in Theorem 2  and κ(x) is a function of x (> x) deﬁned by

ψ0 (x) = x − log x 

κ(x) = 1

(18)

(19)
(20)

(21)

h/(σ2M )) are rescaled versions of the squared EVB threshold (14) and the
h/(σ2M )) =

Note that x and κ(γ2
EVB shrinkage estimator (15)  respectively  i.e.  x = (γEVB)2/(σ2M ) and κ(γ2
/(σ2√M L).
γh˘γEVB
The functions ψ0 (x) and ψ (x) are depicted in Fig. 4. We can prove the convexity of ψ0 (x) and
quasi-convexity of ψ (x)  which are useful properties in our theoretical analysis.

h

> 0 for h = 1  . . .   (H EVB and

h

h

= 0 for h = (H EVB + 1  . . .   H. Then we have the following theorem:

Let (H EVB be the rank estimated by VB  which satisﬁes(γEVB
(γEVB
Theorem 4 (H EVB is upper-bounded as
(H EVB ≤ H = min$9 L
1+α: − 1  H%  
and the noise variance estimator(σ2 EVB is bounded as follows:
M(L−H(1+α))3 <(σ2 EVB ≤ 1
H+1  "L
LM+L
h =

for h = 1  . . .   L 
for h = L + 1.

max2σ2

∞ for h = 0 

(Sketch of proof) First  we show that a global minimizer w.r.t. σ2 exists in (γ2

where σ2

h=1 γ2
h 

γ2
h
M x
0

h=H+1 γ2

h

(22)

(23)

(24)

(25)

L/M  γ2

1 /M )  and it

∂Ω

Θ(σ−2) ≡ 1
(M + L)σ2 <γ h!γh − ˘γEVB

h

is a stationary point. Given a hypothetic (H  the derivative of Ω w.r.t. σ−2 is written as

h=1 γh(γh−˘γEVB
h
LM

∂σ−2 = −σ2 + "#H

)+"L

h=#H+1

γ2
h

L

.

Eq.(15) implies the following bounds:

" < (√M + √L)2σ2

for γh >γ EVB 

(26)
which allows us to bound Θ by simple inequalities. Finding a condition prohibiting Θ to be zero
proves the theorem.
2
Theorem 4 states that EVB discards the (L−/L/(1 + α)0 + 1) ≥ 1 smallest components  regardless
of the observed values {γh}. For example  the half components are always discarded when the
matrix is square (i.e.  α = L/M = 1). The smallest singular value γL is always discarded  and
(σ2 EVB >γ 2

L/(M (L − (L − 1)(1 + α)) >γ 2

L/M always holds.

6

1

0.5

e
t
a
R

s
s
e
c
c
u
S

0
 
0

1

2

y

3

(a) α = 1

 

 

 

1

0.5

e
t
a
R

s
s
e
c
c
u
S

0
 
0

ξ = 0.0
ξ = 0.1
ξ = 0.2
ξ = 0.4
ξ = 0.6
ξ = 0.8
4

5

1

0.5

e
t
a
R

s
s
e
c
c
u
S

0
 
0

ξ = 0.0
ξ = 0.1
ξ = 0.2
ξ = 0.4
ξ = 0.6
ξ = 0.8
4

5

ξ = 0.0
ξ = 0.1
ξ = 0.2
ξ = 0.4
ξ = 0.6
ξ = 0.8
4

5

1

2

y

3

(c) α = 0.1

1

2

y

3

(b) α = 0.5

Figure 5: Success rate of dimensionality recovery in numerical simulation for M = 200. The
threshold for the guaranteed recovery (the second inequality in Eq.(28)) is depicted with a vertical
bar with the same color and line style.

4.2 Perfect Recovery Condition

Here  we derive a sufﬁcient condition for perfect recovery of the true PCA dimensionality in the
large-scale limit.
Assume that the observation matrix V is generated as

V = U∗ + E 

(27)
where U∗ is a true signal matrix with rank H∗ and the singular values {γ∗h}  and each element of
the noise matrix E is subject to a distribution with mean zero and variance σ2∗. We rely on a result
[2  5] on the eigenvalue distribution of the spiked covariance model [8]. The following theorem
guarantees the accuracy of VB-PCA:
Theorem 5 Assume H ≥ H∗ (i.e.  we set the rank of the MF model sufﬁciently large)  and denote
the relevant rank (dimensionality) ratio by

ξ = H∗
L .

ξ< 1

x and γ∗2

(28)

In the large-scale limit with ﬁnite α and H∗  EVB implemented with a local search algorithm for

the noise variance σ2 estimation almost surely recovers the true rank  i.e.  (H EVB = H∗  if ξ = 0 or

where x is deﬁned in Eq.(19).
(Sketch of proof) We ﬁrst show that  in the large-scale limit and when ξ = 0  Eq.(25) is equal to
h deﬁned in
zero if and only if σ2 = σ2∗. This means the perfect recovery in the no-signal case. σ2

1−xξ − α% · Mσ 2∗ 

H∗ >$ x−1

H∗+1 < (σ2 <σ 2

H∗. Using Eq.(26)  we can obtain a
2

Eq.(24) is actually the thresholding point of estimated(σ2 for the h-th component to be discarded.
Therefore  (H EVB = H∗ if and only if σ2
sufﬁcient condition that a local minimum exists only in this range  which proves the theorem.
Note that ξ → 0 in the large scale limit. However  we treated ξ as a positive value in Theorem 5 
hoping that the obtained result can approximately hold in a practical situation when L and M are
large but ﬁnite. The obtained result well explains the dependency on ξ in the numerical simulation
below.
Theorem 5 guarantees that  if the true rank H∗ is small enough compared with L and the smallest
signal γ∗H∗ is large enough compared with σ2∗  VB-PCA works perfectly. It is important to note
that  although the objective function (18) is non-convex and possibly multimodal in general  perfect
recovery does not require global search  but only a local search  of the objective function for noise
variance estimation.
Fig. 5 shows numerical results for M = 200 and α = 1  0.5  0.1. E was drawn from the Gaussian
distribution with variance σ2∗ = 1  and signal singular values were drawn from the uniform distri-
bution on [yMσ2∗  10M ] for different y (the horizontal axis of the graphs indicates y). The vertical
axis indicates the success rate of dimensionality recovery  i.e.  (H EVB = H∗  over 100 trials. If the

condition for ξ (the ﬁrst inequality in Eq.(28)) is violated  the corresponding line is depicted with
markers. Otherwise  the threshold of y for the guaranteed recovery (the second inequality in Eq.(28))
is indicated by a vertical bar with the same color and line style. We can see that the guarantee by
Theorem 5 approximately holds even in this small matrix size  although it is slightly conservative.

7

5 Discussion

Here  we discuss implementation of VB-PCA  and the origin of sparsity of VB.

5.1

Implementation

Implementation of VB-PCA (VB-MF) based on the result given in [16] involves a quartic equation.
This means that we need to use a highly complicated analytic-form solution  derived by  e.g.  Fer-
rari’s method  of a quartic equation  or rely on a numerical quartic solver  which is computationally
less efﬁcient. The theorems we gave in this paper can actually simplify the implementation greatly.
A table of κ deﬁned in Theorem 2 should be prepared beforehand. Given an observed matrix V  
we perform SVD and obtain the singular values. After that  in our new implementation  we ﬁrst
directly estimate the noise variance based on Theorem 3  using any 1-D local search algorithm—

a dimensionality reduction purpose  simply discarding all the components such that σ2
gives the result (here σ2

Theorem 4 helps restrict the search range. Then we obtain the noise variance estimator(σ2 EVB. For
h < (σ2 EVB
h is deﬁned by Eq.(24)). When the estimator (U EVB is needed  Theorem 2
gives the result for σ2 = (σ2 EVB. The VB posterior is also easily computed (its description is

omitted). In this way  we can perform VB-PCA  whose performance is guaranteed  very easily.

5.2 Origin of Exact Sparsity

Sparsity is regarded as a practical advantage of VB. Nevertheless  as discussed in Section 1  it is
not necessarily a property inherent in the rigorous Bayesian estimation. Actually  at least in MF 
the sparsity is induced by the independence assumption between A and B. Let us go back to Fig.1 
where the Bayes posterior for V = 1 is shown in the left graph. In this scalar factorization model  the

direction  and the mass in the second and the fourth quadrants toward the negative direction. Since
the Bayes posterior is skewed and more mass is put in the ﬁrst and the third quadrants  it is natural

probability mass in the ﬁrst and the third quadrants pulls the estimator (U = BA toward the positive
that the Bayesian estimator(γ = )BA*p(A B|V ) is positive. This is true even if V > 0 is very small.

On the other hand  the VB posterior (the middle graph) is prohibited to be skewed because of the
independent assumption  and thus it has to wait until V grows so that one of the modes has a enough
probability mass. This is the cause of sparsity in VBMF. The Bayes posterior (left graph) implies
that  if we restrict the posterior to be Gaussian  but allow to have correlation between A and B  exact
sparsity will not be observed.
It is observed that the Bayesian estimation gives a sparse solution when the hyper parameters
(CA  CB) are optimized. This estimator is also depicted as blue diamonds labeled as EFB (em-
pirical fully-Bayesian) in the right graph of Fig.1. Note that  in this case  the independence between
A and C−1/2
)  which are strongly correlated in the prior (2) and hence in
the Bayes posterior  is forced—the point estimation of CA (as well as CB) breaks the correlation
because approximating by the delta function induces the independence from all other parameters.
Further investigation on the relation between the independence constraint and the sparsity induction
is our future work.

(as well as B and C−1/2

A

B

6 Conclusion

In this paper  we considered the variational Bayesian PCA (VB-PCA) when the noise variance is
unknown. Analyzing the behavior of the noise variance estimator  we derived a sufﬁcient condition
for VB-PCA to perfectly recover the true dimensionality. Our result theoretically supports the use-
fulness of VB-PCA. In our theoretical analysis  we obtained bounds for a noise variance estimator
and simple closed-form solutions for other parameters  which were shown to be useful for better
implementation of VB-PCA.

Acknowledgments
SN  RT  and MS thank the support from MEXT Kakenhi 23120004  MEXT Kakenhi 22700138  and
the FIRST program  respectively. SDB was supported by a Beckman Postdoctoral Fellowship.

8

References
[1] H. Attias. Inferring parameters and structure of latent variable models by variational Bayes.
In Proceedings of the Fifteenth Conference Annual Conference on Uncertainty in Artiﬁcial
Intelligence (UAI-99)  pages 21–30  San Francisco  CA  1999. Morgan Kaufmann.

[2] J. Baik and J. W. Silverstein. Eigenvalues of large sample covariance matrices of spiked pop-

ulation models. Journal of Multivariate Analysis  97(6):1382–1408  2006.

[3] C. M. Bishop. Variational principal components. In Proc. of ICANN  volume 1  pages 514–509 

1999.

[4] Z. Ghahramani and M. J. Beal. Graphical models and variational methods. In Advanced Mean

Field Methods  pages 161–177. MIT Press  2001.

[5] D. C. Hoyle. Automatic PCA dimension selection for high dimensional data and small sample

sizes. Journal of Machine Learning Research  9:2733–2759  2008.

[6] A. Ilin and T. Raiko. Practical approaches to principal component analysis in the presence of

missing values. JMLR  11:1957–2000  2010.

[7] T. S. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods. Statis-

tics and Computing  10:25–37  2000.

[8] I. M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis.

Annals of Statistics  29:295–327  2001.

[9] N. El Karoui. Spectrum estimation for large dimensional covariance matrices using random

matrix theory. Annals of Statistics  36(6):2757–2790  2008.

[10] O. Ledoit and M. Wolf. A well-conditioned estimator for large-dimensional covariance matri-

ces. Journal of Multivariate Analysis  88(2):365–411  2004.

[11] Y. J. Lim and T. W. Teh. Variational Bayesian approach to movie rating prediction. In Pro-

ceedings of KDD Cup and Workshop  2007.

[12] D. J. C. Mackay. Local minima  symmetry-breaking  and model pruning in variational
free energy minimization. Available from http://www.inference.phy.cam.ac.uk/
mackay/minima.pdf. 2001.

[13] V. A. Marcenko and L. A. Pastur. Distribution of eigenvalues for some sets of random matrices.

Mathematics of the USSR-Sbornik  1(4):457–483  1967.

[14] T. P. Minka. Automatic choice of dimensionality for PCA. In Advances in NIPS  volume 13 

pages 598–604. MIT Press  2001.

[15] S. Nakajima and M. Sugiyama. Theoretical analysis of Bayesian matrix factorization. Journal

of Machine Learning Research  12:2579–2644  2011.

[16] S. Nakajima  M. Sugiyama  and S. D. Babacan. Global solution of fully-observed variational
Bayesian matrix factorization is column-wise independent. In Advances in Neural Information
Processing Systems 24  2011.

[17] S. Nakajima  M. Sugiyama  and S. D. Babacan. On Bayesian PCA: Automatic dimensionality
selection and analytic solution. In Proceedings of 28th International Conference on Machine
Learning (ICML2011)  Bellevue  WA  USA  Jun. 28–Jul.2 2011.

[18] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Compu-

tation  11:305–345  1999.

[19] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In J. C. Platt  D. Koller 
Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20 
pages 1257–1264  Cambridge  MA  2008. MIT Press.

[20] M. Sato  T. Yoshioka  S. Kajihara  K. Toyama  N. Goda  K. Doya  and M. Kawato. Hierarchical

Bayesian estimation for MEG inverse problem. Neuro Image  23:806–826  2004.

[21] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the

Royal Statistical Society  61:611–622  1999.

[22] K. W. Wachter. The strong limits of random matrix spectra for sample matrices of independent

elements. Annals of Probability  6:1–18  1978.

9

,Ferran Diego Andilla
Fred Hamprecht