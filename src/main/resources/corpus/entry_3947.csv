2019,On the Accuracy of Influence Functions for Measuring Group Effects,Influence functions estimate the effect of removing a training point on a model without the need to retrain. They are based on a first-order Taylor approximation that is guaranteed to be accurate for sufficiently small changes to the model  and so are commonly used to study the effect of individual points in large datasets. However  we often want to study the effects of large groups of training points  e.g.  to diagnose batch effects or apportion credit between different data sources. Removing such large groups can result in significant changes to the model. Are influence functions still accurate in this setting? In this paper  we find that across many different types of groups and for a range of real-world datasets  the predicted effect (using influence functions) of a group correlates surprisingly well with its actual effect  even if the absolute and relative errors are large. Our theoretical analysis shows that such strong correlation arises only under certain settings and need not hold in general  indicating that real-world datasets have particular properties that allow the influence approximation to be accurate.,On the Accuracy of Inﬂuence Functions

for Measuring Group Effects

Pang Wei Koh∗

Kai-Siang Ang∗
Department of Computer Science

Hubert H. K. Teo∗

{pangwei@cs  kaiang@  hteo@  pliang@cs}.stanford.edu

Stanford University

Percy Liang

Abstract

Inﬂuence functions estimate the effect of removing a training point on a model
without the need to retrain. They are based on a ﬁrst-order Taylor approximation
that is guaranteed to be accurate for sufﬁciently small changes to the model  and
so are commonly used to study the effect of individual points in large datasets.
However  we often want to study the effects of large groups of training points 
e.g.  to diagnose batch effects or apportion credit between different data sources.
Removing such large groups can result in signiﬁcant changes to the model. Are
inﬂuence functions still accurate in this setting? In this paper  we ﬁnd that across
many different types of groups and for a range of real-world datasets  the predicted
effect (using inﬂuence functions) of a group correlates surprisingly well with its
actual effect  even if the absolute and relative errors are large. Our theoretical anal-
ysis shows that such strong correlation arises only under certain settings and need
not hold in general  indicating that real-world datasets have particular properties
that allow the inﬂuence approximation to be accurate.

1

Introduction

Inﬂuence functions (Jaeckel  1972; Hampel  1974; Cook  1977) estimate the effect of removing an
individual training point on a model’s predictions without the computationally-prohibitive cost of
retraining the model. Tracing a model’s output back to its training data can be useful: inﬂuence
functions have been recently applied to explain predictions (Koh and Liang  2017)  produce conﬁdence
intervals (Schulam and Saria  2019)  investigate model bias (Brunet et al.  2018; Wang et al.  2019) 
improve human trust (Zhou et al.  2019)  and even craft data poisoning attacks (Koh et al.  2019).
Inﬂuence functions are based on ﬁrst-order Taylor approximations that are accurate for estimating
small perturbations to the model  which makes them suitable for predicting the effects of removing
individual training points on the model. However  we often want to study the effects of removing
groups of points  which represent large perturbations to the data. For example  we might wish
to analyze the effect of data collected from different experimental batches (Leek et al.  2010) or
demographic groups (Chen et al.  2018); apportion credit between crowdworkers  each of whom
generated part of the data (Arrieta-Ibarra et al.  2018); or  in a multi-party learning setting  ensure
that no individual user has too much inﬂuence on the joint model (Hayes and Ohrimenko  2018). Are
inﬂuence functions still accurate when predicting the effects of (removing) these larger groups?
In this paper  we ﬁrst show empirically that on real datasets and across a broad variety of groups of
data  the predicted and actual effects are strikingly correlated (Spearman ρ of 0.8 to 1.0)  such that
the groups with the largest actual effect also tend to have the largest predicted effect. Moreover  the
predicted effect tends to underestimate the actual effect  suggesting that it could be an approximate

∗Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

lower bound in practice. Using inﬂuence functions to predict the actual effect of removing large  co-
herent groups of data can therefore still be useful  even though the violation of the small-perturbation
assumption can result in high absolute and relative errors between the predicted and actual effects.
What explains these phenomena of correlation and underestimation? Prior theoretical work focused
on establishing the conditions under which this inﬂuence approximation is accurate  i.e.  the error
between the actual and predicted effects is small (Giordano et al.  2019b; Rad and Maleki  2018).
However  in our setting of removing large  coherent groups of data  this error can be quite large. As
a ﬁrst step towards understanding the behavior of the inﬂuence approximation in this regime  we
characterize the relationship between the predicted and actual effects of a group via the one-step
Newton approximation (Pregibon et al.  1981)  which we ﬁnd is a surprisingly accurate approximation
in practice. We show that correlation and underestimation arise under certain settings (e.g.  removing
multiple copies of a single training point)  but need not hold in general  which opens up the intriguing
question of why we observe those phenomena across a wide range of empirical settings.
Finally  we exploit the correlation of predicted and actual group effects in two example case studies: a
chemical-disease relationship (CDR) task  where the groups correspond to different labeling functions
(Hancock et al.  2018)  and a natural language inference (NLI) task (Williams et al.  2018)  where
the groups come from different crowdworkers. On the CDR task  we ﬁnd that the inﬂuence of
each labeling function correlates with its size (the number of examples it labels) but not its average
accuracy  which suggests that practitioners should focus on the coverage of the labeling functions they
construct. In contrast  on the NLI task  we ﬁnd that the inﬂuence of each crowdworker is uncorrelated
with the number of examples they contibute  which suggests that practitioners should focus on how
to elicit high-quality examples from crowdworkers over increasing quantity.

2 Background and problem setup
Consider learning a predictive model with parameters θ ∈ Θ that maps from an input space X to an
output space Y. We are given n training points {(x1  y1)  . . .   (xn  yn)} and a loss function (cid:96)(x  y  θ)
that is twice-differentiable and convex in θ. To train the model  we select the model parameters

(cid:35)

(cid:35)

(cid:34) n(cid:88)

i=1

(cid:34) n(cid:88)

i=1

ˆθ(1) = arg minθ∈Θ

(cid:96)(xi  yi; θ)

+

(cid:107)θ(cid:107)2

2

λ
2

(1)

that minimize the L2-regularized empirical risk  where λ > 0 controls regularization strength. The
all-ones vector 1 in ˆθ(1) denotes that the initial training points all have uniform sample weights.
Our goal is to measure the effects of different groups of training data on the model: if we removed a
subset of training points W   how much would the model ˆθ change? Concretely  we deﬁne a vector
w ∈ {0  1}n of sample weights with wi = I((xi  yi) ∈ W ) and consider the modiﬁed parameters

ˆθ(1 − w) = arg minθ∈Θ

(1 − wi)(cid:96)(xi  yi; θ)

+

(cid:107)θ(cid:107)2

2

λ
2

(2)

corresponding to retraining the model after excluding W . We refer to w as the subset (corresponding
to W ); the number of removed points as (cid:107)w(cid:107)1; and the fraction of removed points as α = (cid:107)w(cid:107)1/n.
The actual effect I∗

f : [0  1]n → R of the subset w is

where the evaluation function f : Θ → R measures a quantity of interest. Speciﬁcally  we study:

I∗
f (w) = f (ˆθ(1 − w)) − f (ˆθ(1)) 

(3)

• The change in test prediction  with f (θ) = θ(cid:62)xtest. Linear models (for regression or binary
classiﬁcation) make predictions that are functions of θ(cid:62)xtest  so this measures the effect
that removing a subset will have on the model’s prediction for some test point xtest.
• The change in test loss  with f (θ) = (cid:96)(xtest  ytest; θ)  which is similar to the test prediction.
i=1 wi(cid:96)(xi  yi; θ)  measures the increase in loss on
the removed points w. Its average over all subsets of size (cid:107)w(cid:107)1 is the estimated extra loss
that leave-(cid:107)w(cid:107)1-out cross-validation (CV) measures over the training loss.

• The change in self-loss  with f (θ) =(cid:80)n

2

Inﬂuence functions

2.1
The issue with computing the actual effect I∗
f (w) is that retraining the model to compute ˆθ(1 − w)
for each subset w can be prohibitively expensive. Inﬂuence functions provide a relatively efﬁcient
ﬁrst-order approximation to I∗

Consider the function qw : [0  1] → R with qw(t) = f(cid:0)ˆθ(1 − tw)(cid:1)  such that the actual effect I∗

f (w)
can be written as qw(1) − qw(0). We deﬁne the predicted effect of the subset w to be its inﬂuence
If (w) = q(cid:48)
w(0) ≈ qw(1)−qw(0); in this paper  we use the term predicted effect interchangeably with
inﬂuence. Intuitively  inﬂuence measures the effect of removing an inﬁnitesimal weight from each
point in w and then linearly extrapolates to removing all of w.2 By taking a Taylor approximation
(see  e.g.  Hampel et al. (1986) for details)  the inﬂuence can be computed as

f (w) that avoids retraining.

If (w) def= q(cid:48)

w(0) = ∇θf(cid:0)ˆθ(1)(cid:1)(cid:62)(cid:20) d
= ∇θf(cid:0)ˆθ(1)(cid:1)(cid:62)
i=1 wi∇θ(cid:96)(xi  yi; ˆθ(1))  H1 =(cid:80)n

dt
H−1
λ 1g1(w) 
i=1 ∇2

ˆθ(1 − tw)

(cid:21)

(cid:12)(cid:12)(cid:12)t=0

(4)

where g1(w) =(cid:80)n

θ(cid:96)(xi  yi; ˆθ(1))  and Hλ 1 = H1 + λI.
When measuring the change in test prediction or test loss  inﬂuence is additive: if w = w1 + w2  then
If (w) = If (w1) + If (w2)  i.e.  the inﬂuence of a subset is the sum of inﬂuences of its constituent
points  and we can efﬁciently compute the inﬂuence of any subset by pre-computing the inﬂuence of
each individual point (e.g.  by taking a single inverse Hessian-vector product  as in Koh and Liang
(2017)). However  when measuring the change in self-loss  inﬂuence is not additive and requires a
separate calculation for each subset removed.

2.2 Relation to prior work

Inﬂuence functions—introduced in the seminal work of Hampel (1974) and in Jaeckel (1972)  where
it was called the inﬁnitesimal jackknife—have a rich history in robust statistics. The use of inﬂuence
functions in the ML community is more recent  though growing; in Section 1  we provide references
for several recent applications of inﬂuence functions in ML.
Removing a single training point  especially when the total number of points n is large  represents a
small perturbation to the training distribution  so we expect the ﬁrst-order inﬂuence approximation
to be accurate. Indeed  prior work on the accuracy of inﬂuence has focused on this regime: e.g. 
Debruyne et al. (2008); Liu et al. (2014); Rad and Maleki (2018); Giordano et al. (2019b) give
evidence that the inﬂuence on self-loss can approximate LOOCV  and Koh and Liang (2017) similarly
examined the accuracy of estimating the change in test loss after removing single training points.
However  removing a constant fraction α of the training data represents a large perturbation to the
training distribution. To the best of our knowledge  this setting has not been empirically studied;
perhaps the closest work is Khanna et al. (2019)’s use of Bayesian quadrature to estimate a maximally
inﬂuential subset.
Instead  older references have alluded to the phenomena of correlation and
underestimation we observe: Pregibon et al. (1981) note that inﬂuence tends to be conservative  while
Hampel et al. (1986) say that “bold extrapolations” (i.e.  large perturbations) are often still useful.
On the theoretical front  Giordano et al. (2019b) established ﬁnite-sample error bounds that apply
to groups  e.g.  showing that the leave-k-out approximation is consistent as the fraction of removed
points α → 0. Our focus is instead on the relationship of the actual effect I∗
f (w) and predicted effect
(inﬂuence) If (w) in the regime where α is constant and the error |I∗

f (w) − If (w)| is large.

3 Empirical accuracy of inﬂuence functions on constructed groups

How well do inﬂuence functions estimate the effect of (removing) a group of training points? If
n is large and we remove a subset w uniformly at random  the new parameters ˆθ(1 − w) should
remain close to ˆθ(1) even when if fraction of removed points α is non-negligible  so the inﬂuence
f (w) − If (w)| should be small. However  we are usually interested in removing coherent 
error |I∗
non-random groups  e.g.  all points from a data source or share some feature. In such settings  the

2In the statistics literature  inﬂuence typically refers to the effect of adding weight  so the sign is ﬂipped.

3

λ/n Test acc.

Source

Dataset
Diabetes
Enron
Dogﬁsh
MNIST
CDR
MultiNLI

Classes
2
2
2
10
2
3

n
20  000
4  137
1  800
55  000
24  177
392  702

d
127
3  289
2  048
784
328
600

2.2 × 10−4
1.0 × 10−3
2.2 × 10−2
1.0 × 10−3
1.0 × 10−4
1.0 × 10−4

68.2% Strack et al. (2014)
96.1% Metsis et al. (2006)
98.5% Koh and Liang (2017)
92.1% LeCun et al. (1998)
67.4% Hancock et al. (2018)
50.4% Williams et al. (2018)

Table 1: Dataset characteristics and the test accuracies that logistic regression achieves (with regular-
ization λ selected by cross-validation). n is the training set size and d is the number of features.

parameters ˆθ(1 − w) and ˆθ(1) might differ substantially  and the error |I∗
f (w) − If (w)| could be
large. Put another way  there could be a cluster of points such that removing one of those points
would not change the model by much—so inﬂuence could be low—but removing all of them would.
Surprisingly (to us)  we found that even when removing large and coherent groups of points  the
inﬂuence If (w) behaved consistently relative to the actual effect I∗
f (w) on test predictions  test
losses  and self-loss  with two broad phenomena emerging:

1. Correlation: If (w) and I∗
2. Underestimation: If (w) and I∗

f (w) tend to have the same sign with |If (w)| < |I∗

f (w) rank subsets of points w similarly (e.g.  high Spearman ρ).
f (w)|.3
Here  we report results on 5 datasets chosen to span a range of applications  training set size n  and
number of features d (Table 1).4 In an attempt to make the inﬂuence approximation as inaccurate
as possible  we constructed a variety of subsets  from small (α = 0.25%) to large (α = 25%)  to be
coherent and have considerable inﬂuence on the model. On each dataset  we trained an L2-regularized
logistic regression model (or softmax for the multiclass tasks) and compared the inﬂuences and actual
effects of these subsets.

Group construction. Our aim is to construct coherent groups that when removed will substantially
change the model. To do so  we need to choose points that are similar in some way. Speciﬁcally  for
each dataset  we grouped points in 7 ways: 1) points that share feature values; 2) points that cluster
on their features or 3) on their gradients ∇θ(cid:96)(x  y  ˆθ(1))); 4) random points within the same class; 5)
random points from any class. We also grouped 6) points with large positive and 7) negative inﬂuence
on the test loss (cid:96)(xtest  ytest  ˆθ(1))  since intuitively  training points that all have high inﬂuence on
a test point should act together to change the model substantially. Overall  for each dataset  we
constructed 1 700 subsets ranging in size from 0.25% to 25% of the training points. See Appendix A
for more details.

Results. Figure 1 shows that the inﬂuences and actual effects of all of these subsets on test prediction
(Top)  test loss (Mid)  and self-loss (Bot) are highly correlated (Spearman ρ of 0.89 to 0.99 across all
plots)  even though the absolute and relative errors of the inﬂuence approximation can be quite large.
Moreover  the inﬂuence of a group tends to underestimate its actual effect in all settings except for
groups with negative inﬂuence on test loss (the left side of each plot in Figure 1-Mid). These trends
held across a wide range of regularizations λ  though correlation increased with λ (Appendix C.2).
In Section 5  we will use the CDR dataset (Hancock et al.  2018) and the MultiNLI (Williams et al. 
2018) dataset to show that correlation and underestimation also apply to groups of data that arise
naturally  and that inﬂuence functions can therefore be used to derive insights about real datasets and
applications. Before that  we ﬁrst attempt to develop some theoretical insight into the results above.

3 This holds with one exception: when measuring the change in test loss  f (θ) = (cid:96)(xtest  ytest; θ)  underes-

timation only holds when actual effect I∗

f (w) is positive (Figure 1-Mid).

4 The ﬁrst 4 datasets involve hospital readmission prediction  spam classiﬁcation  and object recognition  and
were used in Koh and Liang (2017) to study the inﬂuence of individual points. The ﬁfth dataset is a chemical-
disease relationship (CDR) dataset Hancock et al. (2018). In Section 5  we will also study the MultiNLI language
inference dataset (Williams et al.  2018)  which was omitted from the experiments here because its large size
makes repeated retraining to compute the actual effect too expensive. See Appendix B for dataset details.

4

Figure 1: Inﬂuences vs. actual effects of coherent groups of points ranging from 0.25% to 25%
in size. Each point corresponds to a group  and its color reﬂects how that group was constructed.
In Top and Mid  we show results for the test point with highest loss; other test points are similar
(Appendix C.1)  though with more curvature for test loss (Appendix C.3). The grey reference line
has slope 1  and the red borders represent points that are not plotted because they are outside the x- or
y-axis range. We omit the top row for MNIST  as θ(cid:62)xtest is not meaningful in the multi-class setting.

4 Theoretical analysis

The experimental results above show that there is consistent underestimation and high correlation
between the predicted effects  based on inﬂuence functions  and the actual effects of groups across a
variety of datasets  despite the inﬂuence approximation incurring large absolute and relative error. As
we discussed in Section 2.2  this is outside the regime of existing theory.
As an initial step towards understanding the high-error regime  we establish conditions under which
the actual effect I∗
f (w) lies approximately between If (w) and CmaxIf (w) for some Cmax > 0.
This cone constraint—so called because it implies that all points on the graph of inﬂuence vs. actual
effect lie within a cone—implies underestimation and  if Cmax is small  some degree of correlation.
We ﬁrst show that this constraint holds in restricted settings—when measuring self-loss  or when
removing multiple copies of the same point—and that Cmax varies inversely with the regularization
term λ  which is expected since stronger regularization reduces the change in the model. However 
the cone constraint is stronger than necessary because it bounds the degree of underestimation  and
we construct counterexamples to show that it need not hold in more general settings.
Our analysis centers on the one-step Newton approximation  which estimates the change in parameters

ˆθ(1 − w) − ˆθ(1) ≈ ∆θNt(w) def= (cid:0)Hλ 1(1 − w)(cid:1)−1

g1(w) 

where Hλ 1(1 − w) = ((cid:80)n

i=1(1 − wi)∇2

f (w) = f(cid:0)ˆθ(1) + ∆θNt(w)(cid:1) − f (ˆθ(1))) and the corresponding

θ(cid:96)(xi  yi; ˆθ(1))) + λI is the regularized empirical Hessian
at ˆθ(1) but reweighted after removing the subset w. This change in parameters gives the Newton
approximation of the effect I Nt
Newton error ErrNt-act(w) = I∗
Speciﬁcally  we decompose the error between the actual effect I∗
+ I Nt

f (w)  which measures its gap from the actual effect.

f (w) − I Nt
(cid:124)
f (w) − If (w) = I∗
I∗

f (w) and inﬂuence If (w) as
(cid:125)
f (w) − If (w).

(cid:123)(cid:122)

f (w)

(cid:124)

(cid:123)(cid:122)
f (w) − I Nt

ErrNt-act(w)

(cid:125)

ErrNt-inf (w)

(5)

5

50050DiabetesInfluence ontest prediction10010Enron505Dogfish505CDRMNISTShared feature valueFeature clusteringGradient clusteringRandom within classRandomLarge positive test infl.Large negative test infl.050Influence ontest loss05100.02.55.005100100246×103Influence onself-lossActual effect02004004 hidden02040012×10318 hidden024×1035 hiddenFigure 2: The Newton approximation accurately captures the actual effect for our datasets (though
there is more error on the Diabetes dataset)  with the same test point as in Figure 1-Top. We omit
MNIST and MultiNLI for computational reasons. See Figure C.4 for plots of test loss and self-loss.

O(cid:0)1/(σmin + λ)3(cid:1)  where λ is regularization strength and σmin is the smallest eigenvalue of the

In Section 4.1  we ﬁrst show that the Newton-actual error ErrNt-act(w) decays at a rate of

empirical Hessian H1. Empirically  this error is small on our datasets  so we focus on characterizing
the Newton-inﬂuence error ErrNt-inf (w) in Section 4.2. We use this characterization to study the
behavior of inﬂuence relative to the actual effect on self-loss (Section 4.3) and test prediction (Sec-
tion 4.4). For margin-based models  the test loss is a monotone function of the test prediction  so the
analysis is similar (Appendix D.3).

4.1 Bounding the error of the one-step Newton approximation
The Newton approximation is computationally expensive because it computes (Hλ 1(1 − w))−1 for
each w (instead of the ﬁxed H−1
λ 1 in the inﬂuence calculation). However  it provides more accurate
estimates (e.g.  Pregibon et al. (1981)  Rad and Maleki (2018))  and we show that its error can be
bounded as follows (all proofs in Appendix E):
Proposition 1. Let the Newton error be ErrNt-act(w) def= I∗
evaluation function f (θ) is Cf -Lipschitz and that the Hessian ∇2
1Cf CH C 2
(cid:96)

f (w) − I Nt
θ(cid:96)(x  y  θ) is CH-Lipschitz. Then

f (w). Assume that the

|ErrNt-act(w)| ≤ n(cid:107)w(cid:107)2

 

(σmin + λ)3

def= max1≤i≤n (cid:107)∇θ(cid:96)(xi  yi  ˆθ(1))(cid:107)2 to be the largest norm of a training point’s
where we deﬁne C(cid:96)
gradient at ˆθ(1)  and σmin to be the smallest eigenvalue of H1. ErrNt-act(w) only involves third-order
or higher derivatives of the loss  so it is 0 for quadratic losses.

Proposition 1 tells us that the Newton approximation is accurate when λ is large or the third derivative
of (cid:96)(x  y;·) (controlled by CH) is small. Empirically  the Newton error ErrNt-act(w) is strikingly
small in most of our settings (Figure 2)  even though the overall error of the inﬂuence approximation
I∗
f (w) − If (w) is still large. In the remainder of this section  we therefore focus on characterizing
the Newton-inﬂuence error ErrNt-inf (w)  under the assumption that the Newton approximation is
similar to the actual effect (within a factor of O(1/λ3)).

4.2 Characterizing the difference between the Newton approximation and inﬂuence
We next characterize the Newton-inﬂuence error ErrNt-inf (w) = I Nt
Proposition 2. Under the assumptions of Proposition 1 and the additional assumption that the third
derivative of f (θ) exists and is bounded in norm by Cf 3  the Newton-inﬂuence error ErrNt-inf (w) is
ErrNt-inf (w) = ∇θf (ˆθ(1))(cid:62)H

f (w) − If (w):

∆θNt(w)(cid:62)∇2

− 1
λ 1 g1(w) +

− 1
λ 1 D(w)H

2

2

1
2

(cid:124)

(cid:123)(cid:122)

θf (ˆθ(1))∆θNt(w) + Errf 3(w) 
Error from curvature of f (·)

(cid:125)

with D(w) def= (cid:0)I − H

− 1
λ 1 H1(w)H

2

− 1
2
λ 1

(cid:1)−1 − I and H1(w) def= (cid:80)n

matrix D(w) has eigenvalues between 0 and σmax

θ(cid:96)(xi  yi; ˆθ(1)). The error
λ   where σmax is the largest eigenvalue of H1. The

i=1 wi∇2

6

50050DiabetesNewton approx.on test predictionActual effect10010Enron505Dogfish505CDRShared feature valueFeature clusteringGradient clusteringRandom within classRandomLarge positive test infl.Large negative test infl.Figure 3: Inﬂuence If (w) vs. Newton ap-
proximation I Nt
f (w) on the test prediction on
two counterexamples detailed in Appendix D.1.
Left: We adversarially choose a set of w’s such
that If (w) and I Nt
f (w) can have different signs
and need not correlate. Right: When we only
remove copies of single points  underestima-
tion holds. However  we can control the scal-
ing factor d(w) between If (w) and I Nt
f (w) on
different groups  so correlation need not hold.

1Cf 3C 3

residual term Errf 3(w) captures the error due to third-order derivatives of f (·) and is bounded by
|Errf 3(w)| ≤ (cid:107)w(cid:107)3
We can interpret Proposition 2 as a formalization of Hampel et al. (1986)’s observation that inﬂuence
approximations are accurate when the model is robust and the curvature of the loss is low. In
general  the error decreases as λ increases and f (·) becomes less curved; in Figure C.2  we show that
increasing λ reduces error and increases correlation in our experiments.

(cid:96) /6(σmin + λ)3.

4.3 The relationship between inﬂuence and actual effect on self-loss

evaluation function f (·). We start with the self-loss f (θ) =(cid:80)n

Let us now apply Proposition 2 to analyze the behavior of inﬂuence under different choices of
i=1 wi(cid:96)(xi  yi; θ)  as its inﬂuences

and actual effects are always non-negative  and it is the cleanest to characterize:
Proposition 3. Under the assumptions of Proposition 2  the inﬂuence on the self-loss obeys

(cid:18)

(cid:19)

If (w) + Errf 3(w) ≤ I Nt

f (w) ≤

1 +

3σmax

2λ

+

σ2
max
2λ2

If (w) + Errf 3(w).

The constraint in Proposition 3 implies that up to O(1/λ3) terms  inﬂuence underestimates the
Newton approximation and therefore the actual effect. This explains the previously-unexplained
downward bias observed when using inﬂuence to approximate LOOCV (Debruyne et al.  2008;
Giordano et al.  2019b). Equivalently  all points on the graph of inﬂuences vs. actual effects lie within
the cone bounded by the lines with slope 1 and slope
λ+3σmax/2 lines  up to O(1/λ3) terms. As λ
grows  these lines will converge  and the error terms Errf 3(w) and ErrNt-act(w) will decay at a rate
of O(1/λ3)  forcing the inﬂuences and actual effects to be equal.
However  λ/σmax is quite small in our experiments in Section 3  so the actual correlation of inﬂuence
is better than predicted by this theory: in Figure 1-Bot  the sizes of the theoretically-permissible
cones can be quite large  but the points in the graphs nevertheless trace a tight curve through the cone.

λ

4.4 The relationship between inﬂuence and actual effect on a test point
We now turn to measuring the test prediction f (θ) = θ(cid:62)xtest. Here  we show that correlation and
underestimation need not hold  and that we cannot obtain a cone constraint similar to Proposition 3
− 1
− 1
except in a restricted setting. Deﬁne vtest = H
λ 1 g1(w). Proposition 2 gives:
λ 1 xtest and vw = H
Corollary 1. Suppose f (θ) = θ(cid:62)xtest. Then I Nt
f (w) = If (w) + vtest
(cid:62)D(w)vw  where D(w) =

(cid:1)−1 − I is the error matrix from Proposition 2.

(cid:0)I − H

− 1
λ 1 H1(w)H

− 1
2
λ 1

2

2

2

(cid:62)vw = 0 but the Newton approximation I Nt

Unfortunately  Corollary 1 implies that no cone constraint applies: in general  we can ﬁnd xtest such
that the inﬂuence If (w) = vtest
(cid:62)D(w)vw
is large. As a counterexample  Figure 3-Left shows that on synthetic data  If (w) and I Nt
f (w) can
even have opposite signs on some subsets w.
We can recover a cone constraint similar to Proposition 3 if we restrict our attention to the special
case where we use a margin-based model and remove (possibly multiple copies) of a single point:

f (w) = vtest

7

202Influence on test preditionNewton approximation505Proposition 4. Consider a binary classiﬁcation setting with y ∈ {−1  +1} and a margin-based
model with loss (cid:96)(x  y; θ) = φ(yθ(cid:62)x) for some φ : R → R+. Suppose f (θ) = θ(cid:62)xtest and that the
subset w comprises (cid:107)w(cid:107)1 identical copies of the training point (xw  yw). Then under the assumptions
of Proposition 1  the Newton approximation I Nt
f (w) is related to the inﬂuence If (w) according to

I Nt
f (w) =

If (w)
ˆθ(1)(cid:62)xw) · x(cid:62)

This implies the Newton approximation I Nt

1 − (cid:107)w(cid:107)1 · φ(cid:48)(cid:48)(yw

f (w) is bounded between If (w) and(cid:0)1 + σmax

wH−1

.

λ 1xw

(cid:1)If (w).

λ

Similar to Proposition 3  Proposition 4 shows that up to O(1/λ3) terms  the inﬂuence underestimates
the actual effect when removing copies of a single point. Moreover  all points on the graph of
inﬂuences vs. actual effects lie within the cone bounded by the lines with slope 1 and slope
λ/(λ + σmax)  up to O(1/λ3) terms. As λ/σmax grows  the cone shrinks  and correlation increases.
However  if λ/σmax is small (as in our experiments in Section 3)  the cone is wide  and the scaling
factor d(w) = 1/(1 − (cid:107)w(cid:107)1 · φ(cid:48)(cid:48)
λ 1xk) in Proposition 4 can be quite large for some subsets w
but not for others. In particular  d(w) is large when there are few remaining points in the direction
of the removed points. In Figure 3-Right  we exploit this fact to show that the inﬂuence If (w) and
Newton approximation I Nt
f (w) can exhibit low correlation (e.g.  low If (w) need not mean low
I Nt
f (w))  even in the simpliﬁed setting of removing copies of single points. We comment on the
analogue of d(w) in the general multiple-point setting in Appendix D.2  and on the inﬂuence on test
loss (instead of test prediction) in Appendix D.3.

k H−1

kx(cid:62)

5 Applications of inﬂuence functions on natural groups of data

The analysis in Section 4 shows that the cone constraint between predicted and actual group effects
need not always hold. Nonetheless  our experiments in Section 3 demonstrate that on real datasets 
the correlation is much stronger than the theory predicts. We now turn to using inﬂuence functions to
predict group effects in two case studies where groups arise naturally.

Chemical-disease relation (CDR). The CDR dataset tackles the following task: given text about
the relationship between a chemical and a disease  predict if the chemical causes the disease. It was
collected via data programming  where users provide labeling functions (LFs)—instead of labels—
that take in an unlabeled point and either abstain or output a heuristic label (Ratner et al.  2016).
Speciﬁcally  Hancock et al. (2018) collected natural language explanations of provided classiﬁcations;
parsed those explanations into LFs; and used those LFs to label a large pool of data (Appendix B.1).
We used inﬂuence functions to study two important properties of LFs: coverage  the fraction of
unlabeled points for which an LF outputs a non-abstaining label; and precision  the proportion of
correct labels output. We associated each LF with the group of points that it labeled  and computed
its inﬂuence; as expected  these correlated with actual effects on overall test loss (Spearman ρ = 1;
Figure C.5). LFs with higher coverage had more inﬂuence (Figure 4-Left; see also Figure C.6) 
but surprisingly  LFs with higher precision did not (Figure 4-Mid). The association with coverage
stems at least partially from class balance: each LF outputs either all positive or all negative labels 
so removing an LF with high coverage changes the class balance and consequently improves test
performance on one class at the expense of the other (Figure 4-Left). While these ﬁndings are not
causal claims  they suggest that the coverage of an LF  rather than its precision  might have a stronger
effect on its overall contribution to test performance.

MultiNLI. The MultiNLI dataset deals with natural language inference: determining if a pair of
sentences agree  contradict  or are neutral. Williams et al. (2018) presented crowdworkers with initial
sentences from ﬁve genres and asked them to generate follow-on sentences that were neutral or in
agreement/contradiction (Appendix B.2). We studied the effect that each crowdworker had on the
model’s test set performance by computing the inﬂuence of the examples they created on overall test
loss (Spearman ρ of 0.77 to 0.86 with actual effects across different genres; see Figure C.8).
Studying the inﬂuence of each crowdworker reveals that the number of examples a crowdworker
created was not predictive of inﬂuence on test performance: e.g.  the most proliﬁc crowdworker

8

Figure 4: In CDR  the inﬂuence of a label-
ing function (LF) on test performance is
predicted by its coverage (Left) but not its
precision (Mid). However  in MultiNLI 
the number of examples contributed by a
crowdworkers is not predictive of its inﬂu-
ence (Right). For CDR  LFs output either
all + or all − labels; we plot the inﬂuence
of each LF on the test points of the same
class.

contributed 35 000 examples but had negative inﬂuence  and we veriﬁed that removing all of those
examples and retraining the model indeed made overall test performance worse (Figure 4-Right).
Curiously  this effect was genre-speciﬁc: crowdworkers who improved performance on some gen-
res would lower performance on others (Figure C.10)  even though the number of examples they
contributed to a genre did not correlate with their inﬂuence on it (Figure C.11). We note that these
results are obtained on a baseline logistic regression model built on top of a continuous bag-of-words
representation. Identifying precisely what makes a crowdworker’s contributions useful  especially on
higher-performing models  could help us improve dataset collection and credit attribution as well as
better understand the biases due to annotator effects (Geva et al.  2019).

6 Discussion

In this paper  we showed empirically that the inﬂuences of groups of points are highly correlated
with  and consistently underestimate  their actual effects across a range of datasets  types of groups 
and sizes. These phenomena allows us to use inﬂuence functions to better understand the “different
stories that different parts of the data tell ” in the words of Hampel et al. (1986). We showed that we
can gain insight into the effects of a labeling function in data programming  or a crowdworker in a
crowdsourced dataset  by computing the inﬂuence of their corresponding group effects.
While these applications involved predeﬁned groups  inﬂuence functions could potentially also
discover coherent  semantically-relevant groups in the data. They can also be used to approximate
Shapley values  which are a different but related way of measuring the effect of data points; see  e.g. 
Jia et al. (2019) and Ghorbani and Zou (2019). Separately  inﬂuence functions can also estimate the
effects of adding training points. In this context  underestimation turns into overestimation  i.e.  the
inﬂuence of adding a group of training points tends to overestimate the actual effect of adding that
group. This raises the possibility of using inﬂuence functions to evaluate the vulnerability of a given
dataset and model to data poisoning attacks (Steinhardt et al.  2017).
Our theoretical analysis showed that while correlation and underestimation hold in some restricted
settings  they need not hold in general  realistic settings. This gap between theory and experiments
opens up important directions for future work: Why do we observe such striking correlation between
predicted and actual effects on real data? To what extent is this due to the speciﬁc model  datasets  or
subsets used? Do these trends hold for non-convex models like neural networks? Our work suggests
that there could be distributional assumptions that hold for real data and give rise to the broad
phenomena of correlation and underestimation. One promising lead is the surprising observation that
the Newton approximation is much more accurate than inﬂuence at predicting group effects  which
holds out the hope that we can understand group effects using just low-order terms (since the Newton
approximation only uses the ﬁrst and second derivatives of the loss) without needing to account for
the whole loss function through higher order terms (as in Giordano et al. (2019a)).

9

0.00.1LF coverage0250500Influence on test set lossCDR0.00.40.8LF precisionCDRPos. LFNeg. LF010000Examples contributed4202MultiNLIReproducibility

The code for replicating our experiments is available in the GitHub repository https:
//github.com/kohpangwei/group-influence-release. An executable version of this
paper is also available on CodaLab at https://worksheets.codalab.org/worksheets/
0xfed2ae0b9e5b44b7a1af8096365592a5.

Acknowledgments

We are grateful to Zhenghao Chen  Brad Efron  Jean Feng  Tatsunori Hashimoto  Robin Jia  Stephen
Mussmann  Aditi Raghunathan  Marco Túlio Ribeiro  Noah Simon  Jacob Steinhardt  and Jian Zhang
for helpful discussions and comments. We are further indebted to Ryan Giordano  Ruoxi Jia  and Will
Stephenson for discussion about prior work  and Samuel Bowman  Braden Hancock  Emma Pierson 
and Pranav Rajpurkar for their assistance with applications and datasets. This work was funded by an
Open Philanthropy Project Award. PWK was supported by the Facebook Fellowship Program.

References
I. Arrieta-Ibarra  L. Goff  D. Jiménez-Hernández  J. Lanier  and E. G. Weyl. Should we treat data
as labor? Moving beyond “free”. In American Economic Association Papers and Proceedings 
volume 108  pages 38–42  2018.

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.

M. Brunet  C. Alkalay-Houlihan  A. Anderson  and R. Zemel. Understanding the origins of bias in

word embeddings. arXiv preprint arXiv:1810.03611  2018.

I. Chen  F. D. Johansson  and D. Sontag. Why is my classiﬁer discriminatory? In Advances in Neural

Information Processing Systems (NeurIPS)  pages 3539–3550  2018.

R. D. Cook. Detection of inﬂuential observation in linear regression. Technometrics  19:15–18  1977.

M. Debruyne  M. Hubert  and J. A. Suykens. Model selection in kernel based regression using the

inﬂuence function. Journal of Machine Learning Research (JMLR)  9(0):2377–2400  2008.

M. Geva  Y. Goldberg  and J. Berant. Are we modeling the task or the annotator? an investigation
ofannotator bias in natural language understanding datasets. In Empirical Methods in Natural
Language Processing (EMNLP)  2019.

A. Ghorbani and J. Zou. Data shapley: Equitable valuation of data for machine learning. arXiv

preprint arXiv:1904.02868  2019.

R. Giordano  M. I. Jordan  and T. Broderick. A higher-order Swiss Army inﬁnitesimal jackknife.

arXiv preprint arXiv:1907.12116  2019a.

R. Giordano  W. Stephenson  R. Liu  M. Jordan  and T. Broderick. A Swiss Army inﬁnitesimal

jackknife. In Artiﬁcial Intelligence and Statistics (AISTATS)  pages 1139–1147  2019b.

F. R. Hampel. The inﬂuence curve and its role in robust estimation. Journal of the American

Statistical Association  69(346):383–393  1974.

F. R. Hampel  E. M. Ronchetti  P. J. Rousseeuw  and W. A. Stahel. Robust Statistics: The Approach

Based on Inﬂuence Functions. Wiley  1986.

B. Hancock  P. Varma  S. Wang  M. Bringmann  P. Liang  and C. Ré. Training classiﬁers with natural

language explanations. In Association for Computational Linguistics (ACL)  2018.

J. Hayes and O. Ohrimenko. Contamination attacks and mitigation in multi-party machine learning.

In Advances in Neural Information Processing Systems (NeurIPS)  pages 6604–6615  2018.

L. A. Jaeckel. The inﬁnitesimal jackknife. Unpublished memorandum  Bell Telephone Laboratories 

Murray Hill  NJ  1972.

10

R. Jia  D. Dao  B. Wang  F. A. Hubis  N. Hynes  N. M. Gurel  B. Li  C. Zhang  D. Song  and C. Spanos.
Towards efﬁcient data valuation based on the shapley value. arXiv preprint arXiv:1902.10275 
2019.

R. Khanna  B. Kim  J. Ghosh  and O. Koyejo. Interpreting black box predictions using Fisher kernels.

In Artiﬁcial Intelligence and Statistics (AISTATS)  pages 3382–3390  2019.

P. W. Koh and P. Liang. Understanding black-box predictions via inﬂuence functions. In International

Conference on Machine Learning (ICML)  2017.

P. W. Koh  J. Steinhardt  and P. Liang. Stronger data poisoning attacks break data sanitization

defenses. arXiv preprint arXiv:1811.00741  2019.

Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

J. T. Leek  R. B. Scharpf  H. C. Bravo  D. Simcha  B. Langmead  W. E. Johnson  D. Geman 
K. Baggerly  and R. A. Irizarry. Tackling the widespread and critical impact of batch effects in
high-throughput data. Nature Reviews Genetics  11(10)  2010.

Y. Liu  S. Jiang  and S. Liao. Efﬁcient approximation of cross-validation for kernel methods using
Bouligand inﬂuence function. In International Conference on Machine Learning (ICML)  pages
324–332  2014.

V. Metsis  I. Androutsopoulos  and G. Paliouras. Spam ﬁltering with naive Bayes – which naive

Bayes? In CEAS  volume 17  pages 28–69  2006.

D. Pregibon et al. Logistic regression diagnostics. Annals of Statistics  9(4):705–724  1981.

K. R. Rad and A. Maleki. A scalable estimate of the extra-sample prediction error via approximate

leave-one-out. arXiv preprint arXiv:1801.10243  2018.

A. J. Ratner  C. M. D. Sa  S. Wu  D. Selsam  and C. Ré. Data programming: Creating large training
sets  quickly. In Advances in Neural Information Processing Systems (NeurIPS)  pages 3567–3575 
2016.

P. Schulam and S. Saria. Can you trust this prediction? Auditing pointwise reliability after learning.

In Artiﬁcial Intelligence and Statistics (AISTATS)  pages 1022–1031  2019.

J. Steinhardt  P. W. Koh  and P. Liang. Certiﬁed defenses for data poisoning attacks. In Advances in

Neural Information Processing Systems (NeurIPS)  2017.

B. Strack  J. P. DeShazo  C. Gennings  J. L. Olmo  S. Ventura  K. J. Cios  and J. N. Clore. Impact of
HbA1c measurement on hospital readmission rates: Analysis of 70 000 clinical database patient
records. BioMed Research International  2014  2014.

H. Wang  B. Ustun  and F. P. Calmon. Repairing without retraining: Avoiding disparate impact with

counterfactual distributions. arXiv preprint arXiv:1901.10501  2019.

C. Wei  Y. Peng  R. Leaman  A. P. Davis  C. J. Mattingly  J. Li  T. C. Wiegers  and Z. Lu. Overview
of the BioCreative V chemical disease relation (cdr) task. In Proceedings of the Fifth BioCreative
Challenge Evaluation Workshop  pages 154–166  2015.

A. Williams  N. Nangia  and S. Bowman. A broad-coverage challenge corpus for sentence under-
standing through inference. In Association for Computational Linguistics (ACL)  pages 1112–1122 
2018.

J. Zhou  Z. Li  H. Hu  K. Yu  F. Chen  Z. Li  and Y. Wang. Effects of inﬂuence on user trust in
predictive decision making. In Conference on Human Factors in Computing Systems (CHI)  2019.

11

,Le Song
Santosh Vempala
John Wilmes
Bo Xie
Pang Wei Koh
Kai-Siang Ang
Hubert Teo
Percy Liang