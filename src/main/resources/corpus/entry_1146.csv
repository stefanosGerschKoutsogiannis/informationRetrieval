2019,Continuous-time Models for Stochastic Optimization Algorithms,We propose new continuous-time formulations for first-order stochastic optimization algorithms such as mini-batch gradient descent and variance-reduced methods. We exploit these continuous-time models  together with simple Lyapunov analysis as well as tools from stochastic calculus  in order to derive convergence bounds for various types of non-convex functions. Guided by such analysis  we show that the same Lyapunov arguments hold in discrete-time  leading to matching rates. In addition  we use these models and Ito calculus to infer novel insights on the dynamics of SGD  proving that a decreasing learning rate acts as time warping or  equivalently  as landscape stretching.,Continuous-time Models

for Stochastic Optimization Algorithms

Antonio Orvieto

Department of Computer Science

ETH Zurich  Switzerland ⇤

Aurelien Lucchi

Department of Computer Science

ETH Zurich  Switzerland

Abstract

We propose new continuous-time formulations for ﬁrst-order stochastic optimiza-
tion algorithms such as mini-batch gradient descent and variance-reduced methods.
We exploit these continuous-time models  together with simple Lyapunov analysis
as well as tools from stochastic calculus  in order to derive convergence bounds
for various types of non-convex functions. Guided by such analysis  we show that
the same Lyapunov arguments hold in discrete-time  leading to matching rates.
In addition  we use these models and Itô calculus to infer novel insights on the
dynamics of SGD  proving that a decreasing learning rate acts as time warping or 
equivalently  as landscape stretching.

Introduction

1
We consider the problem of ﬁnding the minimizer of a smooth non-convex function f : Rd ! R:
x⇤ := arg minx2Rd f (x). We are here speciﬁcally interested in a ﬁnite-sum setting which is
commonly encountered in machine learning and where f (·) can be written as a sum of individual
functions over datapoints. In such settings  the optimization method of choice is mini-batch Stochastic
Gradient Descent (MB-SGD) which simply iteratively computes stochastic gradients based on
averaging from sampled datapoints. The advantage of this approach is its cheap per-iteration
complexity which is independent of the size of the dataset. This is of course especially relevant
given the rapid growth in the size of the datasets commonly used in machine learning applications.
However  the steps of MB-SGD have a high variance  which can signiﬁcantly slow down the
speed of convergence [22  36]. In the case where f (·) is a strongly-convex function  SGD with a
decreasing learning rate achieves a sublinear rate of convergence in the number of iterations  while
its deterministic counterpart (i.e. full Gradient Descent  GD) exhibits a linear rate of convergence.
There are various ways to improve this rate. The ﬁrst obvious alternative is to systematically increase
the size of the mini-batch at each iteration: [20] showed that a controlled increase of the mini-batch
size yields faster rates of convergence. An alternative  that has become popular recently  is to use
variance reduction (VR) techniques such as SAG [56]  SVRG [32]  SAGA [16]  etc. The high-level
idea behind such algorithms is to re-use past gradients on top of MB-SGD in order to reduce the
variance of the stochastic gradients. This idea leads to faster rates: for general L-smooth objectives 

both SVRG and SAGA ﬁnd an ✏-approximate stationary point2 in OLn2/3/✏ stochastic gradient
computations [3  53]  compared to the O (Ln/✏) needed for GD [45] and the O1/✏2 needed for

MB-SGD [22]. As a consequence  most modern state-of-the-art optimizers designed for general
smooth objectives (Natasha [2]  SCSG [37]  Katyusha [1]  etc) are based on such methods. The
optimization algorithms discussed above are typically analyzed in their discrete form. One alternative
that has recently become popular in machine learning is to view these methods as continuous-time

⇤Correspondence to orvietoa@ethz.ch
2A point x where krf (x)k  ✏.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

processes. By doing so  one can take advantage of numerous tools from the ﬁeld of differential
equations and stochastic calculus. This has led to new insights about non-trivial phenomena in
non-convex optimization [40  31  60] and has allowed for more compact proofs of convergence for
gradient methods [57  42  34]. This perspective appears to be very fruitful  since it also has led to
the development of new discrete algorithms [68  9  64  65]. Finally  this connection goes beyond the
study of algorithms  and can be used for neural network architecture design [14  12].
This success is not surprising  given the impact of continuous-time models in various scientiﬁc
ﬁelds including  e.g.  mathematical ﬁnance  where these models are often used to get closed-form
solutions for derivative prices that are not available for discrete models (see e.g. the celebrated
Black-Scholes formula [10]  which is derived from Itô’s lemma [30]). Many other success stories
come from statistical physics [18]  biology [24] and engineering. Nonetheless  an important question 
which has encouraged numerous debates (see e.g. [62])  is about the reason behind the effectiveness
of continuous-time models. In optimization  this question is partially addressed for deterministic
accelerated methods by the works of [63  9  57] that provide a link between continuous and discrete
time. However  we found that this problem has received less attention in the context of stochastic
non-convex optimization and does not cover recent developments such as [32]. We therefore focus
on the latter setting for which we provide detailed comparisons and analysis of continuous- and
discrete-time methods. The paper is organized as follows:

1. In Sec. 2 we build new continuous-time models for SVRG and mini-batch SGD — which
include the effect of decaying learning rates and increasing batch-sizes. We show existence and
uniqueness of the solution to the corresponding stochastic differential equations.

2. In Sec. 3.1 we derive novel and interpretable non-asymptotic convergence rates for our models 
using the elegant machinery provided by stochastic calculus. We focus on various classes of
non-convex functions relevant for machine learning (see list in Sec. 3).

3. In Sec. 3.2 we complement each of our rates in continuous-time with equivalent results for
the algorithmic counterparts  using the same Lyapunov functions. This shows an algebraic
equivalence between continuous and discrete time and proves the effectiveness of our modeling
technique. To the best of our knowledge  most of these rates (in full generality) are novel 3.

4. In Sec. 4.1 we provide a new interpretation for the distribution induced by SGD with decreasing
stepsizes based on the Øksendal’s time change formula — which reveals an underlying time
warping phenomenon that can be used for designing Lyapunov functions.

5. In Sec. 4.2 we provide a dual interpretation of this last phenomenon as landscape stretching.

At a deeper level  our work proves that continuous-time models can adequately guide the analysis of
stochastic gradient methods and provide new thought-provoking perspectives on their dynamics.

NPN

i=1 be a collection of functions s.t. fi : Rd ! R for any i 2 [N ] and f (·) := 1

2 Uniﬁed models of stochastic gradient methods
i=1 fi(·).
Let {fi}N
In order to minimize f (·)  ﬁrst-order stochastic optimization algorithms rely on some noisy (but
usually unbiased) estimator G(·) of the gradient rf (·). In its full generality  Stochastic Gradient
Descent (SGD) builds a sequence of estimates of the solution x⇤ in a recursive way:
(SGD)
where (⌘k)k0 is a non-increasing deterministic sequence of positive numbers called the learning
rates sequence. Since G(xk  k) is stochastic  {xk}k0 is a stochastic process on some countable
probability space (⌦ F  P). Throughout this paper  we denote by {Fk}k0 the natural ﬁltration
induced by {xk}k0; by E the expectation over all the information F1 and by EFk the conditional
expectation given the information at step k. We consider the two following popular designs for G(·).
i) MB gradient estimator. The mini-batch gradient estimator at iteration k is GMB(xk  k) :=
bkPik2⌦k rfik (xk)  where bk := |⌦k| and the elements of ⌦k (the mini-batch) are sampled at each
iteration k independently  uniformly and with replacement from [N ]. Since ⌦k is random  GMB(x) is
a random variable with conditional (i.e. taking out randomness in xk) mean and covariance

xk+1 = xk  ⌘kG ({xi}0ik  k)  

1

EFk1 [GMB(xk  k)] = rf (xk) 

CovFk1 [GMB(xk  k)] =

⌃MB(xk)

bk

 

(1)

3We derive these rates in App. E and summarize them in Tb. 3.

2

NPN

i=1 (rf (x)  rfi(x)) (rf (x)  rfi(x))T is the one-sample covariance.
where ⌃MB(x) := 1
ii) VR gradient estimator. The basic idea of the original SVRG algorithm introduced in [32] is
to compute the full gradient at some chosen pivot point and combine it with stochastic gradients
computed at subsequent iterations. Combined with mini-batching [53]  this gradient estimator is:

GVR(xk  ˜xk  k) :=

rfik (xk)  rfik (˜xk) + rf (˜xk) 

1

bk Xik2⌦k

where ˜xk 2{ x0  x1  . . .   xk1} is the pivot used at iteration k. This estimator is unbiased  i.e.
EFk1 [GVR(xk  ˜xk  k)] = rf (xk). Its covariance is CovFk1 [GVR(xk  ˜xk  k)] = ⌃VR(xk ˜xk)
with
i=1 (rfi(x)  rfi(y) + rf (y)  rf (x)) (rfi(x)  rfi(y) + rf (y)  rf (x))T .
⌃VR(x  y) := 1

bk

2.1 Building the perturbed gradient ﬂow model
We take inspiration from [38] and [27] and build continuous-time models for SGD with either the
MB or the SVRG gradient estimators. The procedure has three steps.

NPN

(S1) We ﬁrst deﬁne the discretization stepsize h := ⌘0 — this variable is essential to provide a link
between continuous and discrete time. We assume it to be ﬁxed for the rest of this subsection.
Next  we deﬁne the adjustment-factors sequence ( k)k0 s.t. k = ⌘k/h (cf. Eq. 9 in [38]). In
this way — we decouple the two information contained in ⌘k: h controls the overall size of the
learning rate and k handles its variation4 during training.

(S2) Second  we write SGD as xk+1 = xk  ⌘k(rf (xk) + Vk)  where the error Vk has mean zero
be the principal square root5 of ⌃k  we can write SGD as

and covariance ⌃k. Next  let ⌃1/2

k

k Zk 

xk+1 = xk  ⌘krf (xk)  ⌘k⌃1/2

(PGD)
where Zk is a random variable with zero mean and unit covariance6. In order to build simple
continuous-time models  we assume that each Zk is Gaussian distributed: Zk ⇠N (0d  Id). To
highlight this assumption  we will refer to the last recursion as Perturbed Gradient Descent
(PGD) [15]. In Sec. 2.1 we motivate why this assumption  which is commonly used in the
literature [38]  is not restrictive for our purposes. By plugging in either ⌃k =⌃ MB(xk)/bk
or ⌃k =⌃ VR(xk  ˜xk)/bk  we get a discrete model for SGD with the MB or VR gradient
estimators.

(S3) Finally  we lift these PGD models to continuous time. The ﬁrst step is to rewrite them using k:
(MB-PGD)

where MB(x) :=⌃ 1/2(x)  VR(x  y) :=⌃ 1/2
VR (x  y) and ⇠k 2 [k] quantiﬁes the pivot staleness.
Readers familiar with stochastic analysis might recognize that MB-PGD and VR-PGD are the
steps of a numerical integrator (with stepsize h) of an SDE and of an SDDE  respectively. For
convenience of the reader  we give an hands-on introduction to these objects in App. B.

The resulting continuous-time models  which we analyse in this paper  are

dX(t) =  (t)rf (X(t)) dt + (t)ph/b(t) MB(X(t)) dB(t)
dX(t) =  (t)rf (X(t)) dt + (t)ph/b(t) VR(X(t)  X(t  ⇠(t))) dB(t)

where

(MB-PGF)

(VR-PGF)

4A popular choice (see e.g. [43]) is ⌘k = Ck↵  ↵ 2 [0  1]. Here  h = C and k = k↵ 2 [0  1].
5The unique positive semideﬁnite matrix such that ⌃k =⌃ 1/2
6Because ⌃1/2

k Zk has the same distribution as Vk  conditioned on xk.

k ⌃1/2
k .

3

xk+1 = xk  krf (xk)

adjusted gradient drift

xk+1 = xk  krf (xk)

adjusted gradient drift

|
|

{z
{z

}
}

{z

adjusted mini-batch volatility

phZk
h + kph/bk MB(xk)
|
}
h + kph/bk VR(xk  xk⇠k )
|
}

adjusted variance-reduced volatility

{z

phZk

(VR-PGD)

• ⇠ : R+ ! [0  T]  the staleness function  is s.t. ⇠(hk) = ⇠k for all k  0;
• (·) 2C 1(R+  [0  1])  the adjustment function  is s.t. (hk) = k for all k  0 and d (t)
dt  0;
• b(·) 2C 1(R+  R+)  the mini-batch size function is s.t. b(hk) = bk for all k  0 and b(t)  1;
• {B(t)}t0 is a ddimensional Brownian Motion on some ﬁltered probability space.
We conclude this subsection with some important remarks and clariﬁcations on the procedure above.

On the Gaussian assumption.
In (S2) we assumed that Zk is Gaussian distributed. If the mini-
batch size bk is large enough and the gradients are sampled from a distribution with ﬁnite variance 
then the assumption is sound: indeed  by the Berry–Esseen Theorem (see e.g. [17])  Zk approaches

N (0d  Id) in distribution with a rate O1/pbk. However  if bk is small or the underlying variance

is unbounded  the distribution of Zk has heavy tails [58]. Nonetheless  in the large-scale optimization
literature  the gradient variance is generally assumed to be bounded (see e.g. [22]  [11]) — hence  we
keep this assumption  which is practical and reasonable for many problems (likewise assumed in the
related literature [51  42  34  38  39]). Also  taking a different yet enlightening perspective  it is easy
to see that (see Sec. 4 of [11])  if one cares only about expected convergence guarantees — only the
ﬁrst and the second moments of the stochastic gradients have a quantitative effect on the rate.

Approximation guarantees. Recently  [28  39] showed that for a special case of MB-PGF ( k = 1 
and bk constant)  its solution {X(t)}0tT compares to SGD as follows: let K = bT /hc and consider
the iterates {xk}k2[K] of mini-batch SGD (i.e. without Gaussian assumption) with ﬁxed learning
rate h. Under mild assumptions on f (·)  there exists a constant C (independent of h) such that
kE[xk]  E[X(kh)]k  Ch for all k 2 [K]. Their proof argument relies on semi-group expansions
of the solution to the Kolmogorov backward equation  and can be adapted to provide a similar
result for our (more general) equations. However  this approach to motivate the continuous-time
formulation is very limited — as C depends exponentially on T (see also [57]). Nonetheless  under
strong-convexity  some uniform-in-time (a.k.a. shadowing) results were recently derived in [48  19].
In this paper  we take a different approach (similarly to [57] for deterministic methods) and provide
instead matching convergence rates in continuous and in discrete time using the same Lyapunov
function. We note that this is still a very strong indication of the effectiveness of our model to study
SGD  since it shows an algebraic equivalence between the continuous and the discrete case.

Comparison to the "ODE method". A powerful technique in stochastic approximation [36] is to
study SGD through the deterministic ODE ˙X = rf (X). A key result is that SGD  with decreasing
learning rate under the Robbins Monro [55] conditions  behaves like this ODE in the limit. Hence
the ODE can be used to characterize the asymptotic behaviour of SGD. In this work we instead take
inspiration from more recent literature [39] and build stochastic models which include the effect
of a decreasing learning rate into the drift and the volatility coefﬁcients through the adjustment
function (·). This allows  in contrast to the ODE method7  to provide non-asymptotic arguments
and convergence rates.

Local minima width. Our models conﬁrm  as noted in [31]  that the ratio of (initial) learning rate
h to batch size b(t) is a determinant factor of SGD dynamics. Compared to [31]  our model is more
general: indeed  we will see in Sec. 4.2 that the adjustment function also plays a fundamental role in
determining the width of the ﬁnal minima — since it acts like a "function stretcher".

2.2 Existence and uniqueness
Prior works that take an approach similar to ours [35  27  42]  assume the one-sample volatility
(·) to be Lipschitz continuous. This makes the proof of existence and uniqueness straightforward
(cf. a textbook like [41])  but we claim such assumption is not trivial in our setting where (·) is
data-dependent. Indeed  (·) is the result of a square root operation on the gradient covariance —
and the square root function is not Lipschitz around zero. App. C is dedicated to a rigorous proof of
existence and uniqueness  which is veriﬁed under the following condition:
(H)

Each fi(·) is C3  with bounded third derivative and L-smooth.

7This method is instead suitable to assess almost sure convergence and convergence in probability  which are

not considered in this paper for the sake of delivering convergence rates for population quantities.

4

f (·) is C1 and there exists µ > 0 s.t. krf (x)k2  2µ(f (x)  f (x?)) for all x 2 Rd.

This hypothesis is arguably not restrictive as it is usually satisﬁed by many loss functions encountered
in machine learning. As a result  under (H)  with probability 1 the realizations of the stochastic
processes {f (X(t))}t>0 and {X(t)}t>0 are continuous functions of time.
3 Matching convergence rates in continuous and discrete time
Even though in optimization  convex functions are central objects of study  many interesting objectives
found in machine learning are non-convex. However  most of the time  such functions still exhibit
some regularity. For instance  [25] showed that linear LSTMs induce weakly-quasi-convex objectives.
(HWQC) f (·) is C1 and exists ⌧> 0 and x? s.t. hrf (x)  x x?i  ⌧ (f (x) f (x?)) for all x 2 Rd.
Intuitively  (HWQC) requires the negative gradient to be always aligned with the direction of a global
minimum x?. Convex differentiable functions are weakly-quasi-convex (with ⌧ = 1)  but the WQC
class is richer and actually allows functions to be locally concave. Another important class of
problems (e.g.  under some assumptions  matrix completion [61]) satisfy the Polyak-Łojasiewicz
property  which is the weakest known sufﬁcient condition for GD to achieve linear convergence [50].
(HPŁ)
One can verify that if f (·) is strongly-convex  then it is PŁ. However  PŁ functions are not necessarily
convex. What’s more  a broad class of problems (dictionary learning [5]  phase retrieval [13] 
two-layer MLPs [39]) are related to a stronger condition: the restricted-secant-inequality [66].
(HRSI)
2kx  x⇤k2 for all x 2 Rd.
In [33] the authors prove strong-convexity ) (HRSI) ) (HPŁ) (with different constants).
3.1 Continuous-time analysis
First  we derive non-asymptotic rates for MB-PGF. For convenience  we deﬁne '(t) :=R t
⇤ := supx2Rd kMB(x)MB(x)Tks < 1  where k·k s denotes the spectral norm.
Theorem 1. Assume (H)  (H). Let t > 0 and ˜t 2 [0  t] be a random time point with distribution
'(t) for ˜t 2 [0  t] (and 0 otherwise). The solution to MB-PGF is s.t.
2 '(t) Z t

0 (s)ds 
which plays a fundamental role (see Sec. 4.1). As [42  34]  we introduce a bound on the volatility.
(H) 2

f (·) is C1 and there exists µ > 0 s.t. hrf (x)  x  x⇤i  µ

f (x0)  f (x?)

E⇥krf (X(˜t))k2⇤ 

h d L 2
⇤

 (s)2
b(s)

'(t)

ds.

 (˜t)

+

0

Proof. We use the energy function E(x  t) := f (x)  f (x?). Details in App. D.2.

⌅

Theorem 2. Assume (H)  (H)  (HWQC). Let ˜t be as in Thm. 1. The solution to MB-PGF is s.t.

 (s)2
b(s)

ds

(L⌧ ' (s) + 1)

(W1)

(W2)

 (s)2
b(s)

ds.

+

2 ⌧ '(t)

h d 2
⇤

E⇥f (X(˜t))  f (x?)⇤  kx0  x?k2
E [(f (X(t))  f (x?))]  kx0  x?k2

2 ⌧ '(t)Z t
2 ⌧ '(t)Z t
Proof. We use the energy functions E1 E2 s.t. E1(x) := 1
f (x?)) + 1

h d 2
⇤

2 ⌧ '(t)

+

0

0

2kx  x?k2 for (W1) and (W2)  respectively. Details in App. D.2.

2kx x?k2 and E2(x  t) := ⌧ '(t)(f (x))
⌅

Theorem 3. Assume (H)  (H)  (HPŁ). The solution to MB-PGF is s.t.

E[f (X(t))  f (x?)]  e2µ'(t)(f (x0)  f (x?)) +

h d L 2
⇤

2

Z t

0

 (s)2
b(s)

e2µ('(t)'(s))ds.

5

Table 1: Asymptotic rates for MB-PGF under (t) = O(ta) in the form O(t).  shown in the
table as a function of a. "⇠" indicates randomization of the result. Rates match with Tb. 1 in [43].
(⇠)  (H)  (H)

(⇠)  (H)  (H)  (HWQC)

(H)  (H)  (HWQC)

(H)  (H)  (HPŁ)

a

(0
  1/2)
(1/2   2/3)
(2/3  
1)

Cor. 3

a
a
a

Cor. 2
⇥
2a  1
1  a

Cor. 2

Cor. 1

a
1  a
1  a

a
1  a
1  a

⌅

Proof. We use the energy function E(x  t) := e2µ'(t)(f (x)  f (x?)). Details in App. D.2.
Decreasing mini-batch size. From Thm. 1  2  3  it is clear that  as it is well known [11  6]  a
simple way to converge to a local minimizer is to pick b(·) increasing as a function of time. However 
this corresponds to dramatically increasing the complexity in terms of gradient computations. In
continuous-time  we can account for this by introducing (t) =R t
0 b(s)ds  proportional to the number
of computed gradients at time t. The complexity in number of gradient computations can be derived
by substituting into the ﬁnal rate the new time variable 1(t) instead of t. As we will see in Thm. 5 
this concept extends to a more general setting and leads to valuable insights.
Asymptotic rates. Another way to guarantee convergence to a local minimizer is to decrease (·).
In App. D.3 we derive asymptotic rates for (t) = O(ta) and report the results in Tb. 1. The results
match exactly the corresponding know rates for SGD  stated under stronger assumptions in [43]. As
for increasing b(·)  decreasing (·) can also be seen as performing a time warp (see Thm. 5).
Ball of convergence. For (t) = 1  the sub-optimality gap derived in App. D.3.1 matches [11].
In contrast to GMB(·)  [3  4] have shown that signiﬁcant speed-ups are hard to obtain from parallel
gradient computations (i.e. for b(t) > 1) using GVR(·) 8. Also  our results for MB-PGF as well as
prior work [67  3  4  53] suggest that linear rates can only be obtained with (t) = 1. Hence  for
our analysis of VR-PGF  we focus on the case b(t) = (t) = 1. The following result  in the spirit
of [32  4]  relates to the so-called Option II of SVRG.

Theorem 4. Assume (H)  (HRSI) and choose ⇠(t) = t P1j=1 (t  jT) (sawtooth wave) 
where (·) is the Dirac delta. Let {X(t)}t0 be the solution to VR-PGF with additional jumps
at times (jT)j2N: we pick X(jT + T) uniformly in {X(s)}jTs<(j+1)T. Then 

E[kX(jT)  x?k2] =✓ 2hL2T + 1
T(µ  2hL2)◆j

kx0  x⇤k2.

Previous Literature (SDEs for MB-PGF).
[42] studied dual averaging using a similar SDE model
in the convex setting  under vanishing and persistent volatility. Part of their results are similar  yet
less general and not directly comparable. [51] studied a speciﬁc case of our equations  under constant
volatility (see also [52] and references therein). [34  65  64] studied extentions to [42] including
acceleration [45] and AC-SA [21]. To the best of our knowledge  there hasn’t been yet any analysis
of SVRG in continuous-time in the literature.

3.2 Discrete-time analysis and algebraic equivalence
We provide matching algorithmic counterparts (using the same Lyapunov function) for all our non-
asymptotic rates in App. D  along with Tb. 2 to summarize the results. We stress that the rates we
prove in discrete-time (i.e. for SGD with gradient estimators GMB or GVR) hold without Gaussian
noise assumption. This is a key result of this paper  which indicates that the tools of Itô calculus [30]
—which are able to provide more compact proofs [42  52] — yield calculations which are equivalent
to the ones used to analyze standard SGD. We invite the curious reader to go through the proofs in
the appendix to appreciate this correspondence as well as to inspect Tb. 3 in the appendix  which
provides a comparison of the discrere-time rates with Thms. 1  2  3 and 4.

8See e.g. Thm. 7 in [53] for a counterexample.

6

Cond.

Rate (Discrete-time  no Gaussian assumption)

(⇠) (H-) (H)

2 (f (x0)  f (x?))

(h'k+1)

+

h d L 2
⇤
(h'k+1)

 2
i
bi

h

kXi=0

 2
i
bi

h

(⇠) (H-) (H) (HWQC)

(H-) (H) (HWQC)

(H-) (H) (HPŁ)

(H-) (HRSI)

+

d h 2
⇤

+

h d 2
⇤

⌧ (h'k+1)

2 ⌧ (h'k+1)

kx0  x?k2
⌧ (h'k+1)

kXi=0
kXi=0
kx0  x?k2
2 ⌧ (h'k+1)
kYi=0
(1  µ h i)(f (x0)  f (x?)) +
✓ 1 + 2L2h2m
hm(µ  2L2h)◆j

Thm.

E.1

E.2

E.2

E.3

E.4

 2
i
bi

h

(1 + ⌧ 'i+1L)

 2
i
bi

h

kXi=0 Qk
Qi

h d L 2
⇤

2

`=0(1  µ h `)
j=0(1  µ h l)

kx0  x⇤k2 (under variance reduction)

Table 2: Summary of the rates we show in the appendix for SGD with mini-batch and VR  using a
Lyapunov argument inspired by the continuous-time analysis. (⇠) indicates randomized output. The
reader should compare the results with Thms. 1  2  3  4 (explicit comparison in the ﬁrst page of the
appendix). For the deﬁnition of the quantities in the rates  check App. E.

Now we ask the simple question: why is this the case? Using the concept of derivation from abstract
algebra  in App. A.2 we show that the discrete difference operator and the derivative operator enjoy
similar algebraic properties. Crucially  this is due to the smoothness of the underlying objective —
which implies a chain-rule9 for the difference operator. Hence  this equivalence is tightly linked
with optimization and might lead to numerous insights. We leave the exploration of this fascinating
direction to future work.

Literature comparison (algorithms). Even though partial10 results have been derived for the
function classes described above in [25  54]  an in-depth non-asymptotic analysis was still missing.
Rates in Tb. 3 (stated above in continuous-time as theorems) provide a generalization to the results
of [43] to the weaker function classes we considered (we never assume convexity). Regarding SVRG 
the rate we report uses a proof similar11 to [4  53] and is comparable to [32] (under convexity).

4

Insights provided by continuous-time models

Building on the tools we used so far  we provide novel insights on the dynamics of SGD. First  in
order to consider both MB-PGF and VR-PGF at the same time  we introduce a stochastic12 matrix
process {(t)}t0 adapted to the Brownian motion:

dX(t) =  (t)rf (X(t)) dt + (t)ph/b(t)(t) dB(t).

(PGF)
We show that annealing the learning rate through a decreasing (·) can be viewed as performing a
time dilation or  alternatively  as directly stretching the objective function. This view is inspired from
the use of Girsanov theorem [23] in ﬁnance: a deep result in stochastic analysis which is the formal
concept underlying the change of measure from real world to "risk-neutral" world.

9This is a key formula in the continuous-time analysis to compute the derivative of a Lyapunov function.
10The convergence under weak-quasi-convexity using a learning rate C/pk and a randomized output is
studied in [25] (Prop. 2.3 under Eq. 2.2 of their paper). On the same line  [33] studied the convergence for
PŁusing a learning rate C/pk and assuming bounded stochastic gradients. These results are strictly contained
in our rates.

11In particular  the lack of convexity causes the factor L2 in the linear rate.
12For MB-PGF  {(t)}t0 := {(X(t))}t0. For VR-PGF  {(t)}t0 := {(X(t)  X(t  ⇠(t)))}t0.

7

4.1 Time stretching through Øksendal’s formula
We notice that  in Thm. 1 2 3  the time variable t is always ﬁltered through the map '(·). Hence  '(·)
seems to act as a new time variable. We show this rigorously using Øksendal’s time change formula.
Theorem 5. Let {X(t)}t0 satisfy PGF and deﬁne ⌧ (·) = '1(·)  where '(t) =R t
0 (s)ds.
For all t  0  X (⌧ (t)) = Y (t) in distribution  where {Y (t)}t0 has the stochastic differential

dY (t) = rf (Y (t))dt +ph (⌧ (t))/b(⌧ (t))(⌧ (t)) dB(t).

Proof. We use the substitution formula for deterministic integrals combined with Øksendal’s formula
⌅
for time change in stochastic integrals — a key result in SDE theory. Details in App. F.

Example. We consider b(t) = 1  (s) = Id and (t) = 1/(t+
1) (popular annealing procedure [11]); we have '(t) = log(t + 1)
ph
and ⌧ (t) = et  1. dX(t) =  1
t+1 dB(t) is
t+1rf (X(t))dt 
s.t. the sped-up solution Y (t) = X(et  1) satisﬁes
dY (t) = rf (X(t))dt + phetdB(t).

(2)

In the example  Eq. (2) is the model for SGD with constant learning
rates but rapidly vanishing noise — which is arguably easier to
study compared to the original equation  that also includes time-
varying learning rates. Hence  this result draws a connection to
SGLD [52] and to prior work on SDE models [42]  which only
considered (t) = 1. But  most importantly — Thm. 5 allows
for more ﬂexibility in the analysis: to derive convergence rates13
one could work with either X(as we did in Sec. 2) or with Y (and
slow-down the rates afterwords).
We verify this result on a one dimensional quadratic  under the choice of parameters in our example 
using Euler-Maruyama simulation (i.e. PGD) with h = 103   = 5. In Fig. 1 we show the mean
and standard deviation relative to 20 realization of the Gaussian noise.
Note that in the case of variance reduction  the volatility is decreasing as a function of time [3]  even
with (t) = 1. Hence one gets a similar result without the change of variable.

Figure 1: Veriﬁcation of Thm. 5
on a 1d quadratic (100 samples):
empirically X(t) d= Y ('(t)).

4.2 Landspace stretching via solution feedback
Consider the (potentially non-convex) quadratic f (x) = hx  x?  H(x  x?)i. WLOG we
assume x? = 0d and that H is diagonal. For simplicity  consider again the case b(t) =
PGF reduces to a linear stochastic system:
1  (s) = Id and (t) = 1/(t + 1).
h
t + 1

dX(t) = 

HX(t)dt +

dB(t).

t + 1

1

t+1 HE[X(t)]dt.

By the variation-of-constants formula [41]  the expectation evolves
without bias: dE[X(t)] =  1
If we denote
by ui(t) the i-th coordinate of E[X(t)] we have d
dt ui(t) =
t+1 ui(t)  where i is the eigenvalue relative to the i-th direc-
 i
tion. Using separation of variables  we ﬁnd ui(t) = (t + 1)iui
0.
Moreover  we can invert space and time: t =ui

Feeding back this equation into the original differential — the
system becomes autonomous:

0/ui(t)1/i  1.

d
dt

ui(t) = i(ui

0) 1

i ui(t)1+ 1
i .

Figure 2: Landscape stretching
for an isotropic paraboloid.

13The design of the Lyapunov function might be easier if we change time variable. This is the case in our
setting  where '(t) comes directly into the Lyapunov functions and would be simply t for the transformed SDE.

8

From this simple derivation we get two important insights on the dynamics of PGF:

2. Inspecting the equivalent formulation d

0) 1

i ui(t)1+ 1

1. Comparing the solution ui(t) = (t + 1)iui

0 with the solution one would obtain with (t) = 1 
that is eitui
0 — we notice that the dynamics in the ﬁrst case is much slower: we get polynomial
convergence and divergence (when i  0) as opposed to exponential. This quantitatively shows
that decreasing the learning rate could slow down (from exponential to polynomial) the dynamics
of SGD around saddle points. However  note that  even though the speed is different  ui(·) and
vi(·) move along the same path14 by Thm 5.
i   we notice with surprise
0)rgi(ui(t))  where
— that this is a gradient system. Indeed the RHS can be written as C(i  ui
gi(x) = x2+ 1
is the equivalent landscape in the i-th direction. In particular  PGF on the
simple quadratic 1
2kxk2 with learning rate decreasing as 1/t behaves in expectation like PGF with
constant learning rate on a cubic. This shines new light on the fact that  as it is well known from
the literature [44]  by decreasing the learning rate we can only achieve sublinear convergence rates
on strongly convex stochastic problems. From our perspective  this happens simply because the
equivalent stretched landscape has vanishing curvature — hence  it is not strongly convex. We
illustrate this last example in Fig. 2 and note that the stretching effect is tangent to the expected
solution (in solid line).

dt ui(t) = i(ui

i

We believe the landscape stretching phenomenon we just outlined to be quite general and to also
hold asymptotically under strong convexity15: indeed it is well known that  by Taylor’s theorem 
in a neighborhood of the solution to a strongly convex problem the cost behaves as its quadratic
approximation. In dynamical systems  this linearization argument can be made precise and goes under
the name of Hartman-Grobman theorem (see e.g. [49]). Since the SDE we studied is memoryless
(no momentum)  at some point it will necessarily enter a neighborhood of the solution where the
dynamics is described by result in this section. We leave the veriﬁcation and formalization of the
argument we just outlined to future research.

5 Conclusion

We provided a detailed comparisons and analysis of continuous- and discrete-time methods in the
context of stochastic non-convex optimization. Notably  our analysis covers the variance-reduced
method introduced in [32]. The continuous-time perspective allowed us to deliver new insights about
how decreasing step-sizes lead to time and landscape stretching. There are many potential interesting
directions for future research such as extending our analysis to mirror-descent or accelerated gradient-
descent [35  60]  or to study state-of-the-art stochastic non-convex optimizers such as Natasha [2].
Finally  we believe it would be interesting to expand the work of [38  39] to better characterize the
convergence of MB-SGD and SVRG to the SDEs we studied here  perhaps with some asymptotic
arguments similar to the ones used in mean-ﬁeld theory [7  8].

References
[1] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. The

Journal of Machine Learning Research  18(1):8194–8244  2017.

[2] Zeyuan Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex
parameter. In Proceedings of the 34th International Conference on Machine Learning-Volume
70  pages 89–97. JMLR. org  2017.

[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning  pages 699–707  2016.

[4] Zeyuan Allen-Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-
convex objectives. In International conference on machine learning  pages 1080–1089  2016.
[5] Sanjeev Arora  Rong Ge  Tengyu Ma  and Ankur Moitra. Simple  efﬁcient  and neural algorithms

for sparse coding. In Proceedings of Machine Learning Research  2015.

14One is the time-changed version of the other (consider Thm. 5 with (t) = 0)  see also Fig. 1.
15Perhaps also in the neighborhood of any hyperbolic ﬁxed point  with implications about saddle point evasion.

9

[6] Lukas Balles  Javier Romero  and Philipp Hennig. Coupling adaptive batch sizes with learning

rates. arXiv preprint arXiv:1612.05086  2016.

[7] Michel Benaïm. Recursive algorithms  urn processes and chaining number of chain recurrent

sets. Ergodic Theory and Dynamical Systems  18(1):53–87  1998.

[8] Michel Benaim and Jean-Yves Le Boudec. A class of mean ﬁeld interaction models for computer

and communication systems. Performance evaluation  65(11-12):823–838  2008.

[9] Michael Betancourt  Michael I Jordan  and Ashia C Wilson. On symplectic optimization. arXiv

preprint arXiv:1802.03653  2018.

[10] Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. Journal of

political economy  81(3):637–654  1973.

[11] Léon Bottou  Frank E Curtis  and Jorge Nocedal. Optimization methods for large-scale machine

learning. SIAM Review  60(2):223–311  2018.

[12] Tian Qi Chen  Yulia Rubanova  Jesse Bettencourt  and David K Duvenaud. Neural ordinary
differential equations. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi 
and R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages 6572–
6583. Curran Associates  Inc.  2018.

[13] Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly
as easy as solving linear systems. In Advances in Neural Information Processing Systems  pages
739–747  2015.

[14] Marco Ciccone  Marco Gallieri  Jonathan Masci  Christian Osendorfer  and Faustino Gomez.
Nais-net: Stable deep networks from non-autonomous differential equations. arXiv preprint
arXiv:1804.07209  2018.

[15] Hadi Daneshmand  Jonas Kohler  Aurelien Lucchi  and Thomas Hofmann. Escaping saddles

with stochastic gradients. arXiv preprint arXiv:1803.05999  2018.

[16] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in neural
information processing systems  pages 1646–1654  2014.

[17] Rick Durrett. Probability: theory and examples. Cambridge university press  2010.
[18] Albert Einstein et al. On the motion of small particles suspended in liquids at rest required by

the molecular-kinetic theory of heat. Annalen der physik  17:549–560  1905.

[19] Yuanyuan Feng  Tingran Gao  Lei Li  Jian-Guo Liu  and Yulong Lu. Uniform-in-time weak
error analysis for stochastic gradient descent algorithms via diffusion approximation. arXiv
preprint arXiv:1902.00635  2019.

[20] Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data

ﬁtting. SIAM Journal on Scientiﬁc Computing  34(3):A1380–A1405  2012.

[21] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly
convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal
on Optimization  22(4):1469–1492  2012.

[22] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex

stochastic programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[23] Igor Vladimirovich Girsanov. On transforming a certain class of stochastic processes by
absolutely continuous substitution of measures. Theory of Probability & Its Applications 
5(3):285–301  1960.

[24] Narendra S Goel and Nira Richter-Dyn. Stochastic models in biology. Elsevier  2016.
[25] Moritz Hardt  Tengyu Ma  and Benjamin Recht. Gradient descent learns linear dynamical

systems. The Journal of Machine Learning Research  19(1):1025–1068  2018.

10

[26] Reza Harikandeh  Mohamed Osama Ahmed  Alim Virani  Mark Schmidt  Jakub Koneˇcn`y  and
Scott Sallinen. Stopwasting my gradients: Practical svrg. In Advances in Neural Information
Processing Systems  pages 2251–2259  2015.

[27] Li He  Qi Meng  Wei Chen  Zhi-Ming Ma  and Tie-Yan Liu. Differential equations for modeling

asynchronous algorithms. arXiv preprint arXiv:1805.02991  2018.

[28] Wenqing Hu  Chris Junchi Li  Lei Li  and Jian-Guo Liu. On the diffusion approximation of

nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562  2017.

[29] Nobuyuki Ikeda and Shinzo Watanabe. Stochastic differential equations and diffusion processes 

volume 24. Elsevier  2014.

[30] Kiyosi Itô. 109. stochastic integral. Proceedings of the Imperial Academy  20(8):519–524 

1944.

[31] Stanisław Jastrz˛ebski  Zachary Kenton  Devansh Arpit  Nicolas Ballas  Asja Fischer  Yoshua
arXiv preprint

Bengio  and Amos Storkey. Three factors inﬂuencing minima in sgd.
arXiv:1711.04623  2017.

[32] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[33] Hamed Karimi  Julie Nutini  and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases  pages 795–811. Springer  2016.

[34] Walid Krichene and Peter L Bartlett. Acceleration and averaging in stochastic descent dynamics.

In Advances in Neural Information Processing Systems  pages 6796–6806  2017.

[35] Walid Krichene  Alexandre Bayen  and Peter L Bartlett. Accelerated mirror descent in continu-
ous and discrete time. In Advances in neural information processing systems  pages 2845–2853 
2015.

[36] Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and

applications  volume 35. Springer Science & Business Media  2003.

[37] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Non-convex ﬁnite-sum optimization
via scsg methods. In Advances in Neural Information Processing Systems  pages 2348–2358 
2017.

[38] Qianxiao Li  Cheng Tai  and Weinan E. Stochastic modiﬁed equations and adaptive stochastic
gradient algorithms. In Proceedings of the 34th International Conference on Machine Learning 
volume 70 of Proceedings of Machine Learning Research  pages 2101–2110  2017.

[39] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu

activation. In Advances in Neural Information Processing Systems  pages 597–607  2017.

[40] Stephan Mandt  Matthew Hoffman  and David Blei. A variational analysis of stochastic gradient

algorithms. In International Conference on Machine Learning  pages 354–363  2016.

[41] Xuerong Mao. Stochastic differential equations and applications. Elsevier  2007.

[42] Panayotis Mertikopoulos and Mathias Staudigl. On the convergence of gradient-like ﬂows with

noisy gradient input. SIAM Journal on Optimization  28(1):163–197  2018.

[43] Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation
algorithms for machine learning. In Advances in Neural Information Processing Systems  pages
451–459  2011.

[44] Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method

efﬁciency in optimization. John Wiley and Sons  1983.

[45] Yurii Nesterov. Lectures on convex optimization. Springer  2018.

11

[46] Bernt Øksendal. When is a stochastic integral a time change of a diffusion?

theoretical probability  3(2):207–226  1990.

Journal of

[47] Bernt Øksendal. Stochastic differential equations. In Stochastic differential equations. Springer 

2003.

[48] Antonio Orvieto and Aurelien Lucchi. Shadowing properties of optimization algorithms. arXiv

preprint  2019.

[49] Lawrence Perko. Differential equations and dynamical systems  volume 7. Springer Science &

Business Media  2013.

[50] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychisli-

tel’noi Matematiki i Matematicheskoi Fiziki  3(4):643–653  1963.

[51] Maxim Raginsky and Jake Bouvrie. Continuous-time stochastic mirror descent on a network:
Variance reduction  consensus  convergence. In Decision and Control (CDC)  2012 IEEE 51st
Annual Conference on  pages 6793–6800. IEEE  2012.

[52] Maxim Raginsky  Alexander Rakhlin  and Matus Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849  2017.

[53] Sashank J Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex Smola. Stochastic
variance reduction for nonconvex optimization. In International conference on machine learning 
pages 314–323  2016.

[54] Sashank J Reddi  Suvrit Sra  Barnabás Póczos  and Alexander J Smola. Proximal stochastic
methods for nonsmooth nonconvex ﬁnite-sum optimization. In Advances in Neural Information
Processing Systems  pages 1145–1153  2016.

[55] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of

mathematical statistics  pages 400–407  1951.

[56] Nicolas L Roux  Mark Schmidt  and Francis R Bach. A stochastic gradient method with
an exponential convergence _rate for ﬁnite training sets. In Advances in neural information
processing systems  pages 2663–2671  2012.

[57] Bin Shi  Simon S Du  Weijie J Su  and Michael I Jordan. Acceleration via symplectic discretiza-

tion of high-resolution differential equations. arXiv preprint arXiv:1902.03694  2019.

[58] Umut Simsekli  Levent Sagun  and Mert Gurbuzbalaban. A tail-index analysis of stochastic

gradient noise in deep neural networks. arXiv preprint arXiv:1901.06053  2019.

[59] Daniel W Stroock and SR Srinivasa Varadhan. Multidimensional diffusion processes. Springer 

2007.

[60] Weijie Su  Stephen Boyd  and Emmanuel Candes. A differential equation for modeling nes-
terov’s accelerated gradient method: Theory and insights. In Advances in Neural Information
Processing Systems  pages 2510–2518  2014.

[61] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.

IEEE Transactions on Information Theory  62(11):6535–6579  2016.

[62] Eugene P Wigner. The unreasonable effectiveness of mathematics in the natural sciences. In

Mathematics and Science  pages 291–306. World Scientiﬁc  1990.

[63] Ashia C Wilson  Benjamin Recht  and Michael I Jordan. A lyapunov analysis of momentum

methods in optimization. arXiv preprint arXiv:1611.02635  2016.

[64] Pan Xu  Tianhao Wang  and Quanquan Gu. Accelerated stochastic mirror descent: From
continuous-time dynamics to discrete-time algorithms. In International Conference on Artiﬁcial
Intelligence and Statistics  pages 1087–1096  2018.

12

[65] Pan Xu  Tianhao Wang  and Quanquan Gu. Continuous and discrete-time accelerated stochastic
mirror descent for strongly convex functions. In International Conference on Machine Learning 
pages 5488–5497  2018.

[66] Hui Zhang. New analysis of linear convergence of gradient-type methods via unifying error

bound conditions. Mathematical Programming  pages 1–46  2016.

[67] Hui Zhang and Wotao Yin. Gradient methods for convex minimization: better rates under

weaker conditions. arXiv preprint arXiv:1303.4645  2013.

[68] Jingzhao Zhang  Aryan Mokhtari  Suvrit Sra  and Ali Jadbabaie. Direct runge-kutta discretiza-

tion achieves acceleration. arXiv preprint arXiv:1805.00521  2018.

13

,Antonio Orvieto
Aurelien Lucchi