2019,Meta Learning with Relational Information for Short Sequences,This paper proposes a new meta-learning method -- named HARMLESS (HAwkes Relational Meta Learning method for Short Sequences) for learning heterogeneous point process models from a collection of short event sequence data along with a relational network. Specifically  we propose a hierarchical Bayesian mixture Hawkes process model  which naturally incorporates the relational information among sequences into point process modeling. Compared with existing methods  our model can capture the underlying mixed-community patterns of the relational network  which simultaneously encourages knowledge sharing among sequences and facilitates adaptively learning for each individual sequence. We further propose an efficient stochastic variational meta-EM algorithm  which can scale to large problems. Numerical experiments on both synthetic and real data show that HARMLESS outperforms existing methods in terms of predicting the future events.,Meta Learning with Relational Information

for Short Sequences

Yujia Xie

College of Computing  Georgia Tech

Xie.Yujia000@gmail.com

Haoming Jiang

College of Engineering  Georgia Tech

jianghm@gatech.edu

Feng Liu

Florida Atlantic University

FLIU2016@fau.edu

Tuo Zhao

College of Engineering  Georgia Tech

tuo.zhao@isye.gatech.edu

Institute for Data and Decision Analytics  the Chinese University of Hong Kong  Shenzhen

Shenzhen Institute of Artiﬁcial Intelligence and Robotics for Society

Hongyuan Zha0

zhahy@cuhk.edu.cn

Abstract

This paper proposes a new meta-learning method – named HARMLESS (HAwkes
Relational Meta LEarning method for Short Sequences) for learning heterogeneous
point process models from short event sequence data along with a relational net-
work. Speciﬁcally  we propose a hierarchical Bayesian mixture Hawkes process
model  which naturally incorporates the relational information among sequences
into point process modeling. Compared with existing methods  our model can
capture the underlying mixed-community patterns of the relational network  which
simultaneously encourages knowledge sharing among sequences and facilitates
adaptive learning for each individual sequence. We further propose an efﬁcient
stochastic variational meta expectation maximization algorithm that can scale to
large problems. Numerical experiments on both synthetic and real data show that
HARMLESS outperforms existing methods in terms of predicting the future events.

Introduction

1
Event sequence data naturally arises in analyzing the temporal behavior of real world subjects
(Cleeremans and McClelland  1991). These sequences often contain rich information  which can
predict the future evolution of the subjects. For example  the timestamps of tweets of a twitter user
reﬂect his activeness and certain state of mind  and can be used to show when he will tweet next time
(Kobayashi and Lambiotte  2016). The job hopping history of a person usually suggests when he will
hop next time (Xu et al.  2017b). Unlike usual sequential data such as text data  event sequences are
always asynchronous and tend to be noisy (Ross et al.  1996). Therefore specialized algorithms are
needed to learn from such data.
In this paper  we are interested in short sequences  a type of sequence data that commonly appears
in many real-world applications. Such data is usually short for two possible reasons. One is that
the event sequences are short in nature  such as the job hopping history. Another is the observation
window is narrow. For example  we are interested in the criminal incidents of an area after a speciﬁc
regulation is published. Moreover  this kind of data usually appears as a collection of sequences 
such as the timestamps of many user’s tweets. Our goal is to extract information that can predict the
occurrence of future events from a large collection of such short sequences.

0Corresponding author. On leave from College of Computing  Georgia Institute of Technology

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Many existing literature considers medium-length or long sequences. They ﬁrst model a sequence as
a parametric point process  e.g.  Poisson process  Hawkes process or their neural variants  and apply
maximum likelihood estimation to ﬁnd the optimal parameters (Ogata  1999; Rasmussen  2013).
However  for short sequences  their lengths are insufﬁcient for reliable inference. One remedy is
that we treat the collection of short sequences as independent identically distributed realizations of
the same point process  since many subjects  e.g.  Twitter users  often share similar behaviors. This
makes the inference manageable. However  the learned pattern can be highly biased against certain
individuals  especially the non-mainstream users  since this method ignores the heterogeneity within
the collection.
An alternative is to recast the problem as a multitask learning problem (Zhang and Yang  2017) –
we target at multi-sequence analysis for multi-subjects. For each sequence  we consider a point

process model that slightly deviates from a common point process model  i.e.  efj = f0 + fj  where
f0 is the common model that captures the main effect  efj is the model for the j-th sequence  and fj

is the relatively small deviation. Such an assumption that there exists a universal common model
cross all subjects  however  is still strong  since the subjects’ patterns can differ dramatically. For
example  the job hopping history of a software engineer and a human resource manager should
have distinct characteristics. Furthermore  such method ignores the relationship of the subjects that
usually can be revealed by side information. For example  a social network often shows community
pattern (Girvan and Newman  2002) – across the communities the variation of the subjects is large 
while within the communities the variation is small. The connections in the social network  such as
"follow" or retweet relationship in Twitter data  can provide us valuable information to identify such
community pattern  but the aforementioned methods do not take into account such understanding to
help analyzing subjects’ behavior.
To this end  we propose a HAwkes Relational Meta LEarning method for Short Sequence (HARM-
LESS)  which can adaptively learn from a collection of short sequence. More speciﬁcally  in a social
network  each user often has multiple identities (Airoldi et al.  2008). For example  a Twitter user can
be both a military fan and a tech fan. Both his tweet history and social connections are based on his
identities. Motivated by above facts  we model each sequence as a hierarchical Bayesian mixture of
Hawkes processes – the weights of each Hawkes process are determined jointly by the hidden pattern
of sequences and the relational information  e.g.  social graphs.
We then propose a variational meta expectation maximization algorithm to efﬁciently perform
inference. Different from existing fully bayesian inference methods (Box and Tiao  2011; Rasmussen 
2013; Xu and Zha  2017)  we make no assumption on the prior distribution of the parameters of
Hawkes process. Instead  when inferring for the Hawkes process parameters of the same identity
for all the subjects  we perform a model-agnostic adaptation from a common model for this identity
(Finn et al. (2017)  see section 3 for more details). This is more ﬂexible since it does not restrict to a
speciﬁc form. We apply HARMLESS to both synthetic and real short event sequences  and achieve
competitive performance.
Notations: Throughout the paper  the unbold letters denote vectors or scalars  while the bold letters
denote the corresponding matrices or sequences. We refer the k-th entry of vector ai as ai k. We refer
the i-th subject as subject i.
2 Preliminaries
We brieﬂy introduce Hawkes Process and Model-Agnostic Meta Learning.
Hawkes processes (Hawkes  1971) is a doubly stochastic temporal point process H(✓) with condi-
tional intensity function  = (t; ✓  ⌧ ) deﬁned as

(t; ✓  ⌧ ) = µ + X⌧ (j)<t

g(t  ⌧ (j); ⇠) 

where ✓ = {µ  ⇠}  g is the nonnegative impact function with parameter ⇠  µ is the base intensity  and
⌧ = {⌧ (1) ⌧ (2) ···  ⌧ (M )} are the timestamps of the events occurring in a time interval [0  tend].
Function g indicates how past events affect current intensity. Existing works usually use pre-speciﬁed
impact functions in parametric form  e.g.  the exponential function in Rasmussen (2013); Zhou et al.
(2013) and the power-law function in Zhao et al. (2015).
Hawkes process captures an important property of real-world events – self-exciting  i.e.  the past
events always increase the chance of arrivals of new events. For example  selling a signiﬁcant quantity

2

of a stock can precipitate a trading ﬂurry. As a result  Hawkes process has been widely used in many
areas  e.g.  behavior analysis (Yang and Zha  2013; Luo et al.  2015)  ﬁnancial analysis (Bacry et al. 
2012)  and social network analysis (Blundell et al.  2012; Zhou et al.  2013).
Model-Agnostic Meta Learning (MAML  Finn et al.  2017) considers a set of tasks =
{T1 T2 ···  TN}  where each of the tasks only contains a very small amount of data which is
not enough to train a model. We want to exploit the shared structure of the tasks  to obtain models
that can perform well on each of the tasks. Speciﬁcally  MAML seeks to train a common model for
all tasks. From optimization perspective  MAML solves the following problem 

min

✓ XTi2

FTi(e✓i)   min

✓ XTi2

FTi(✓  ⌘D(FTi ✓ )) 

(1)

where D(· ·) is an operator  FTi is the loss function of task Ti  ✓ is the parameter of the common
model  and ⌘ is the step size. Here  D(FTi ✓ ) represents one or a small number of gradient update of
✓. For example  in cases of one gradient step  we take D(FTi ✓ ) = r✓FTi(✓). This optimization
problem aims to ﬁnd the common model that is expected to produce maximally effective behavior on
that task after performing update ✓  ⌘D(FTi ✓ ).
Solving (1) using gradient descent involves computing the Hessian matrices  which is computationally
prohibitive. To alleviate the computational burden  First Order MAML (FOMAML) (Finn et al. 
2017) and Reptile (Nichol et al.  2018) are then proposed. FOMAML drops the second order term in
the gradient of (1). Reptile further simpliﬁes the computation by relaxing the original update with
Hessian as a multi-step stochastic gradient descent updates. All three algorithms can be written in the
form of (1) with operator D deﬁned differently for different methods. Due to space limit  we defer
the deﬁnition of D to Appendix B.
3 HAwkes Relational Meta LEarning for Short Sequences (HARMLESS)

We next introduce the meta learning method
for analyzing short sequences. Suppose we
are given a collection of sequences T =
{⌧1  ⌧2 ···   ⌧N}. We also know some extra
relational information about the subjects. For
example  in social networks  we can have in-
formation on who is friend of whom; in crim-
inal data  we have the locations of the crimes 
and crimes happen near each other often have
Granger causality. Such relational information
can be described as a graph G = (E V)  where
E is the node set  V is the edge set. Denote its
adjacency matrix as Y .
Such social graphs often exhibit community pat-
terns (Girvan and Newman  2002; Xie et al. 
2013). Within the communities the variation of subjects are small  while across the communities
the variation is large. Moreover  the communities are overlapping with each other  i.e.  each subject
may belong to multiple communities and thus have multiple identities. The behaviors of the subject
is based on the identities. Motivated by this observation  we ﬁrst assign each subject a sum-to-one
identity proportion vector ⇡i 2 [0  1]K  whose k-th entry represents the probability of subject i
having the k-th identity. In this way  we associate each subject with multiple identities rather than a
single identity so that its different aspects is captured  which is more natural and ﬂexible.

Figure 1: Illustration of the suggested model.

is

k ) to model the timestamps of the
k   ⌧i). For a Hawkes
k )  the likelihood (Laub et al.  2015) of a sequence ⌧i to appear in time interval [0  tend]

For the k-th identity of subject i  we adopt Hawkes process H(e✓(i)
associated events. Denote the conditional intensity function of H(e✓(i)
process H(e✓(i)
k ; ⌧i) = exp⇣ Z tend
k   ⌧i)dt + X⌧j <tend
L(e✓(i)

k ) as (t;e✓(i)
log (⌧j;e✓(i)

k   ⌧i)⌘.

(t;e✓(i)

(2)

0

3

Here  the parametere✓(i)

model-agnostic adaptation  which we will elaborate in next section.
The identity of the i-th subject is then a combination of the K
identities with identity proportion ⇡i  and the models for indi-
vidual sequences are essentially mixtures of Hawkes process
k ; ⌧i). The likelihood for the

i-th sequence ⌧i is

models. Denote Li(e✓(i)

k ) = L(e✓(i)
KXk=1

p(⌧i) =

k ).

⇡i kLi(e✓(i)

(3)

✓

↵

B

˜✓(i)
1:K

⇡i

⌧i

zi!j

zi j

Yij

k is adapted from a common model with parameter ✓k using a relatively small

Moreover  the connections of the subjects are also based on their
identities. More speciﬁcally  for each connection to happen  one
subject i needs to approach another subject j  where the identities
of subjects i  j are based on ⇡i ⇡ j respectively. Based on this
observation  we adopt a Mixed Membership stochastic Block-
model (MMB) (Airoldi et al.  2008) to model the connections of
the subjects. For each subjects pair (i  j)  denote the identity of
subject i when subject i approaches subject j as random variable
zi!j  and the identity of subject j when j is approached by i as
zi j. The probability of zi!j represent the k-th identity is ⇡i k 
and the probability of zi j represent the k-th identity is ⇡j k.
The probability of whether subject i and j have a connection is
then a function dependent on this two identities - the random variable representing the existence of
connection Yij follows Bernoulli distribution with parameter zT
i!jBzi j  where B is a learnable
parameter.
Generative process: The above model can be summarized as the following generative process.

Figure 2: Probabilisitic graph of
the suggested model. The yel-
low nodes are parameters  white
nodes are latent variables  and the
gray nodes are observed variables.
The solid arrows represent proba-
bilistic mapping  while the hol-
low arrows represent the deter-
ministic mapping.

• For each node i 

– Draw a K dimensional identity proportion vector ⇡i ⇠ Dirichlet(↵).
– Sample the i-th sequence ⌧i from the mixture of Hawkes processes described in (3).

• For each pair of nodes i and j 

– Draw identity indicator for the initiator zi!j ⇠ Categorical(⇡i)
– Draw identity indicator for the receiver zi j ⇠ Categorical(⇡j)
– Sample whether there is an edge between i and j  Yij ⇠ Bernoulli(zT

i!jBzi j).

k   and B. The latent variables

Here  the observed variables are ⌧i and Yij. The parameters are ↵ e✓(i)

are ⇡i  zi  zi!j and zi j. The graph model is shown in Figure 2.
4 Variational Meta Expectation Maximization
We now introduce our variational meta expectation maximization algorithm. This algorithm incorpo-
rates model-agnostic adaptation into variational expectation maximization. In the rest of the paper 
we denote z! = {zi!j}N
To ease the computation we add one more latent variable z. For the i-th sequence  we sample
zi ⇠ Categorical(⇡i). We regard ⌧i as a Hawkes process with parameter ✓(i)
zi . Note that this is
equivalent to the mixture of Hawkes process described in previous section  since p(⌧i) =Pk p(zi =
k)Li(e✓(i)

close form.
Variational E step. The goal is to ﬁnd an approximation of the following posterior distribution

k ). This can ease the computation because now the update for ⇡ has

zi ) =Pk ⇡i kLi(e✓(i)

i j=1 e✓ = {e✓(i)

i j=1  z = {zi j}N

k }N K

i=1 k=1.

p(z  z!  z   ⇡|T   Y  ↵  e✓  B).

We aim to ﬁnd a distribution q(z  z!  z   ⇡) that minimizes the Kullback-Leibler (KL) divergence
to the above posterior distribution. This can be achieved by maximizing the Evidence Lower BOund
(ELBO  Blei et al.  2017) 

max
q2Q

Eq[log p(z  z!  z   ⇡  T   Y )]  Eq[log q(z  z!  z   ⇡)] 

(4)

4

where Q is a properly chosen distribution space. We adopt Q as the mean-ﬁeld variational family  i.e. 

q(z  z!  z   ⇡) = q1(⇡)Yi

q2(zi)Yj

q3(zi!j)q4(zi j).

where q1(⇡i) is the Probability Density Function (PDF) of Dirichlet(i)  q2(zi) is the Probability
Mass Function (PMF) of Categorical(i)  q3(zi!j) is the PMF of Categorical(ij)  q4(zi j) is the
PMF of Categorical( ij)  and i  i  ij  ij are variational parameters. By some derivation (see
Appendix C for detail)  the updates for the variational parameters for solving problem (4) are

 

ij k +

 ij k 

k ) 

(5)

(6)

NXj=1

i k ↵k + i k +

NXj=1
i k eEq[log ⇡i k]Li(e✓(i)
KY`=1⇣BYij
ij k eEq[log ⇡i k]
KYk=1(Bk`)Yij (1  Bk`)1Yijij k   ij ` 

k` (1  Bk`)1Yij⌘ ij `

 ij ` eEq[log ⇡j `]

i kP` i `

i k 

(7)

  ij k 

ij kP` ij `
 ij `Pk ij k
where Eq[log ⇡i k] = fdg(i k)  fdg(P` i `)  and fdg(·) is the digamma function.
Meta inference for ✓ and e✓. Recall that the Hawkes parameter of the k-th identity of subject i is
e✓(i)
k . Instead of specifying thate✓(i)
model H(✓k) to sequence i using MAML-type updates 
e✓(i)
individual models with parametere✓(i)

(9)
k = ✓k  ⌘D(log Li ✓ k).
Since MAML-type algorithms only perform one or few updates from the common model  the adapted
k within one community is close to each other  which meets our

expectation that the within-community variation should be small.
The gradient descent step on the log-likelihood of ✓ can then be written as

k is sampled from a prior distribution  we adapt the k-th common

(8)

 

 

✓k ✓k + ⌘✓r✓k NXi=1

i k log Li(✓k  ⌘D(log Li ✓ k))!  

where ⌘✓ is the step size. In this algorithm  we only need to estimate the common models with
parameter ✓k  k = 1  2 ···   K instead of all individual models. After we obtain ✓k  the individual
models can be easily obtained from Equation (9).
M step. We perform maximum likelihood estimation to ↵ and B  The updates are as follows 

(10)

(11)

(12)

↵`)  fdg(↵k) +

NXi=1fdg(i k)  fdg(Xl

i `)!  

↵k ↵k + ⌘↵ Nfdg(X`
Bk` Pij Yijij k ij `
Pij ij k ij `

 

where ⌘↵ is the step size. The detailed derivation can be found in Appendix C.
Algorithm. We perform updates (5)-(8)  (10)-(12) iteratively until convergence. Note that the
updates can also be implemented in stochastic fashion – at each iteration  we sample a mini-batch of
sequences  and update their associated parameters (Hoffman et al.  2013).
5 Experiments
We ﬁrst brieﬂy introduce oue experiment settings.
Impact function. Following Rasmussen (2013); Zhou et al. (2013)  we choose exponential impact
function g(t;{  !}) = !e!t. The conditional intensity function is

!e!(t⌧ (m)) 

(13)

(t; ✓  ⌧ ) = (t;{µ    !}  ⌧ ) = µ + X⌧ (m)<t

5

i

i

likelihood in update (10) 

where  and ! are parameters. Note that each Hawkes process model only contains three parameters 
µ    and !. This is because we target at short sequence. To avoid overﬁtting  each individual models
cannot have too many parameters.
Regularized likelihood function. Substitute Eq. (13) into Eq. (2)  we have

k ) with a regularized log-

k )   log Li(e✓(i)

k ) + ⌫ log(eµ(i)

!e!(⌧ (n)⌧ (m))⌘⌘.

k ) + ⌫R(e✓(i)
k )   log Li(e✓(i)
k  e!(i)
k  e↵(i)
k = {eµ(i)

L(✓; ⌧ ) = exp⇣  µtend X⌧ (n)<tend⇣(1  e!(tend⌧ (n)))  logµ + X⌧ (m)<⌧ (n)
To keep the parameters non-negative  in practice we replace log Li(e✓(i)
Qi(e✓(i)
k ) + log(e↵(i)
wheree✓(i)

k )  (14)
k } is the parameter of the i-th Hawkes process of the k-th identity  ⌫ is a
regularization coefﬁcient.
Evaluation metric. We hold out the last timestamp of each sequence  and split the hold-out times-
tamps into a validation set and a test set. Another option to do validation and test on event sequence
data is to hold out the last two timestamps – we ﬁrst use the former ones to do validation  then train a
new model together with the validation timestamps  and ﬁnally report the test result based on the later
ones. However  this is not suitable here. This is because the sequences we adopt for experiments are
usually very short  sometimes even no more than 5 events in one sequence. As a result  the models
trained without or with validation timestamps  e.g.  using 3 or 4 timestamps  can be signiﬁcantly
different  which makes the validation procedure very unreliable.
We report the Log-Likelihood (LL) of the test set. More speciﬁcally  for each sequence ⌧i =
{⌧ (1)

k ) + log(e!(i)

 ⌧ (2)

i

} and parameter ✓  the likelihood of next arrival ⌧ (Mi+1)
 ···  ⌧ (Mi)
KXk=1
i k⌧ (Mi+1)
(t;e✓(i)
eLi =

k   ⌧i) exp⇣ Z ⌧ (Mi+1)
;e✓(i)

⌧ (Mi)
i

k   ⌧i) dt⌘.

is

i

i

i

The reported score is the averaged log eLi over subjects. More details can be found in Appendix D.

To estimate of the variance of the estimated log-likelihood  we adopt a multi-split procedure for
evaluation. First  we train m candidate models with different hyper-parameters. Then we repeat
the following procedure for 30 times: 1). Randomly split a validation set and a test set; 2). Pick a
model with highest log-likelihood on the validation set from the m candidate models; 3). Compute
the log-likelihood on the test set. Accordingly  we obtain 30 estimates of the log-likelihood. We then
report the mean and standard error of the 30 estimates.
Baselines. We adopt four baselines as follows.
⇧ MLE-Sep: We consider each sequence as a realization of an individual Hawkes process. We perform
Maximum Likelihood Estimation (MLE) on each sequence separately  and obtain N models for N
sequences.
⇧ MLE-Com: We consider all sequences as realizations of the same Hawkes process and learn a
common model by MLE.
⇧ DMHP (Xu and Zha  2017): We model sequences as a mixture of Hawkes processes with a Dirichlet
distribution as the prior distribution of the mixtures.
⇧ MTL: We perform multi-task learning as described in Section 1. More speciﬁcally  we adopt
Hawkes process model for f0 and efj. Denote the parameters of f0 and efi as ⇢0 = [µ0  0 ! 0]T and
⇢i = [µi  i ! i]T   respectively. We solve

where k⇢i  ⇢0k2 is the `2 norm regularizer of ⇢i  ⇢0 to promote the difference between f0 and fj
to be small  ⌫mtl is a tuning parameter  and Qi(·) is the function deﬁned in Eq. (14).
We would like to remark that another possible baseline is the hierarchical Bayesian model  i.e.  we
modele✓(i)
1:K to have prior distribution with parameter ✓. However  such hierarchical Bayesian model

6

max
⇢0 ⇢i

(Qi(⇢i) + ⌫mtlk⇢i  ⇢0k2)  

NXi=1

Table 1: Visualizations of identities by HARMLESS(MAML).
S Ground Truth

does not have a closed-form update in variational EM algorithm. Therefore  Markov chain Monte
Carlo should be adopted for inference  which is not scalable. For our large scale real graphs we
consider here  the time cost is unrealistic. Therefore we leave out this baseline.
Parameter Tuning. The detailed tuning procedure and detailed settings of each experiment can be
found in Appendix E.
5.1 Synthetic Data
Data generation. We gener-
ate a dataset of 50 nodes with
K = 6 communities. For
each community  we generate
Hawkes meta parameters ✓k =
{µk  k ! k} using the follow-
ing uniform distributions:
µk ⇠ Uniform(0.15  10) 
k ⇠ Uniform(0.15  0.85) 
!k ⇠ Uniform(1  10).
We set ↵ = 1K  i.e.  the entries
of ↵ is all one. Then for the i-th
node  the identity proportion ⇡i
is sampled from Dirichlet(↵)
and the membership indicator
zi from the corresponding cate-

K0 = 10

K0 = 3

K0 = 6

2.0

0.5

1.0

zi

5

N   1

N   2

zi by

eµ(i)

e!(i)

#{i2[1 ···  N ]:zi=k}

zi ⇠ N(µzi  0.01) 

zi ⇠ N(!zi  0.05).

adding small perturbation to ✓zi:

e(i)
zi ⇠ N(zi  0.01) 
N   for any k 6= `  and Bkk =

gorical distribution Categorical(⇡i). Based on zi  we then generate the Hawkes parameterse✓(i)
The sequence is then sampled based on Hawkes process with parametere✓(i)
in time interval [0  20].
To ease the tuning we normalize the sequences by dividing by the largest timestamp. We set
. We sample the graph edges based
Bk` = 0.5
on B. Denote S = Bk` ⇥ N. The generated graphs are visualized in the second column of Table 1.
Visualization of communities. We visualize the communities learned by HARMLESS (MAML) in
Table 1. Denote K0 as the number of communities speciﬁed in HARMLESS. We adopt K0 colors
corresponding to the K0 communities in the graph. The color of each node shown in the Table 1 is
the linear combinations of the RGB values of the K0 colors weighted by identity proportions ⇡i.
HARMLESS produces reasonable identities even if K0 is mis-speciﬁed. If K0 < K  some of the
communities would merge. If K0 > K  some of the communities would split.
Beneﬁt of joint training. To validate the beneﬁt of
joint training on graphs and sequences  we compare
HARMLESS result with a two step procedure: We
ﬁrst train an MMB model and obtain the identities 
and train HARMLESS (MAML) with ﬁxed identities.
In Figure 3 we plot the obtained log-likelihood with
respect to K0.
HARMLESS (MAML) consistently achieves larger
log-likelihood than the two step procedure. This sug-
gests joint training of graphs and the sequences indeed improve the prediction of future events.
Log-likelihood with respect to K0. We also include the results of the baselines and HARMLESS
(FOMAML) in Figure 3. The performance of HARMLESS is consistently better than the baselines.
Besides  we ﬁnd the performance HARMLESS (Reptile) is very dependent on the dataset. For this
synthetic dataset  Reptile cannot perform well.
5.2 Real Data
We adopt four real datasets.

Figure 3: Plot of synthetic data. S = 1.

7

13610K05.005.055.105.155.205.25Log-LikelihoodMLE-SepMLE-ComMTLDMHPTwo StepHARMLESS(MAML)HARMLESS(FOMAML)Table 2: Log-likelihood of real datasets.

LinkedIn

N\A

911-Calls

4.0030 ± 0.3763
4.5111 ± 0.3192
4.4812 ± 0.3434
4.4621 ± 0.3173
4.5208 ± 0.3256
4.6362 ± 0.3241
4.4929 ± 0.3503

0.8419 ± 0.0251
0.8768 ± 0.0028
0.8348 ± 0.0030
0.9270 ± 0.0027
1.4070 ± 0.0105
1.0129 ± 0.004
0.9540 ± 0.0082

MathOverﬂow
0.5043 ± 0.0657
1.7805 ± 0.0345
1.5394 ± 0.0347
1.7225 ± 0.0336
1.8563 ± 0.0345
1.8344 ± 0.0348
1.8663 ± 0.0342

Dataset
MLE-Sep
MLE-Com
DMHP
MTL
HARMLESS (MAML)
HARMLESS (FOMAML)
HARMLESS (Reptile)

StackOverﬂow
0.2862 ± 0.0177
1.5594 ± 0.0134
1.4910 ± 0.0089
1.3886 ± 0.0082
1.5988 ± 0.0083
1.6017 ± 0.0097
911-Calls dataset: The 911-Calls dataset1 contains emergency phone call records of ﬁre  trafﬁc and
other emergencies for Montgomery County  PA. The county is divided into disjoint areas  each of
which has a unique ZIP Code. For each area  the timestamps of emergency phone calls in this area are
recorded as an event sequence. We consider each area as a subject  and two subjects are connected if
they are adjoint. We ﬁnally obtain 57 subjects and 81 connections among them. The average length
of the sequences is 219.1.
LinkedIn dataset: The LinkedIn dataset (Xu et al.  2017b) contains job hopping records of the users.
For each user  her/his check-in timestamps corresponding to different companies are recorded as an
event sequence. We consider each user as a subject  and two subjects are connected if the difference in
timestamps of two user joined the same company is less than 2 weeks. After removing the singleton
subjects  we have 1  369 subjects and 12  815 connections among them. The average length of the
sequences is 4.9.
MathOverﬂow dataset: The MathOverﬂow dataset (Paranjape et al.  2017) contains records of the
users posting and answering math questions. We adopt the records from May 2  2014 to March 6 
2016. For each user  her/his timestamps of answering questions are recorded as an event sequence.
We consider each user as a subject  and two subjects are connected if one user answers another user’s
question. After removing the singleton subjects  we have 1  529 subjects and 6  937 connections
among them. The average length of the sequences is 11.8.
StackOverﬂow dataset: StackOverﬂow is a question and answer site similar to MathOverﬂow. We
adopt the records from November 8  2015 to December 1  2015. We construct the sequences and
graphs in the same way as MathOverﬂow. After removing the singleton subjects  we have 13  434
users and 19  507 connections among them. The average length of the sequences is 7.7.
Result: The log-likelihood is summarized in Table 2. Note due to Markov chain Monte Carlo is
needed for DMHP  we cannot get reasonable result for large dataset  i.e.  StackOverﬂow. HARMLESS
performs consistently better than the baselines. Since the standard error of the results of 911-
Calls dataset are large  we also performed a paired t test. The test shows the difference in log-
likelihood between MLE-Com  i.e.  best of the baselines  and HARMLESS (FOMAML)  i.e.  best of
HARMLESS series  is statistically signiﬁcant (with p value= 1.3 ⇥ 105).
5.3 Ablation Study
We then perform ablation study using
LinkedIn dataset. Three sets of ablation
study are considered here:
Remove inner heterogeneity: We model
each community of sequences using the

Table 3: Results of ablation study.

Method
HARMLESS (MAML)
HARMLESS (FOMAML)
HARMLESS (Reptile)
Remove inner heterogeneity (K = 3)
Remove inner heterogeneity (K = 5)
Remove grouping (MAML)
Remove grouping (FOMAML)
Remove grouping (Reptile)
Remove graph (MAML)
Remove graph (FOMAML)
Remove graph (Reptile)

Log-Likelihood
1.4070 ± 0.0105
1.0129 ± 0.0042
0.9540 ± 0.0082
0.9405 ± 0.0032
0.9392 ± 0.0032
0.9432 ± 0.0031
0.9376 ± 0.0031
0.9455 ± 0.0041
0.9507 ± 0.0032
0.9446 ± 0.0032
0.9489 ± 0.0072

k = ✓k.

same parameters  i.e.  we sete✓(i)

Remove grouping: We set K = 1  so
that the whole graph is one community.
This equivalent to apply the MAML-type
algorithms on the sequences directly.
Remove graph: We do not consider the
graph information  i.e.  we remove z!  z   Y and B from the panel in Figure 2.
The results in Table 3 suggest that MAML-type adaptation  graph information  and using multiple
identities all contribute to the good performance of HARMLESS.
6 Discussions
The setting of meta learning. The goal of conventional settings of meta learning is to train a model
on a set of tasks  so that it can quickly adapt to a new task with only few training samples. Therefore 

1Data is provided by montcoalert.org.

8

people divide the tasks into meta training set and meta test set  where each of the task contains a
training set and a test set. The meta model is trained on the meta training set  aiming to minimize the
test errors  and validated on the meta test set (Vinyals et al.  2016; Santoro et al.  2016). This setting
is designed for supervised learning or reinforcement learning tasks that has accuracy or reward as a
clear evaluation metric. Extracting information from the event sequences  however  is essentially an
unsupervised learning task. Therefore  we do not separate meta training set and meta test set. Instead 
we pull the collection of tasks together  and aim to extract shared information of the collection to
help the training of models on individual tasks. Here  each short sequence is a task. We exploit the
shared pattern of the collection of the sequences to obtain the models for individual sequences.
Community Pattern. The target of Mixed Membership stochastic Blockmodels (MMB) is to
identify the communities in a social graph  e.g.  the classes in a school. However  real social graphs
cannot always be viewed as Erd˝os-Rényi (ER) graphs assumed by MMB. As argued in Karrer and
Newman (2011)  for real-world networks  MMB tends to assign nodes with similar degrees to same
communities  which is different from the popular interpretation of the community pattern. This
property  however  is actually very helpful in our case. As an example  Twitter users that are more
active tend to have similar behavior: They tend to make more connections and post tweets more
frequently. In contrast  users with very different node degrees often have the tweets histories of
different characteristics  and thus should be assigned to different identities. Such property of MMB
allows the identities in HARMLESS to represent this non-traditional community patterns in non-ER
graphs  i.e.  it assigns subjects with various activeness to different communities.
Mixture of Hawkes processes. Many existing works adopt mixture of Hawkes process to model
sequences that are generated from complicated mechanisms (Yang and Zha  2013; Li and Zha  2013;
Xu and Zha  2017). Those works are different from HARMLESS since they do not consider the
hierarchical heterogeneity of the sequences  and do not consider the relational information.
Variants of Hawkes process. Some attempts have been made to further enhance the ﬂexibility of
Hawkes processes. For example  the time-dependent Hawkes process (TiDeH) in Kobayashi and
Lambiotte (2016) and the neural network-based Hawkes process (N-SM-MPP) in Mei and Eisner
(2017) learn very ﬂexible Hawkes processes with complicated intensity functions. Those models
usually have more parameters than vanilla Hawkes processes. For longer sequences  HARMLESS can
also be naturally extended to TiDeHs or N-SM-MPP. However  this work focuses on short sequences.
These methods are not useful here  since they have too many degrees of freedom.

Acknowledgement
This work is partially supported by the grant NSF IIS 1717916 and NSF CMMI 1745382. Part of
the work done by Hongyuan Zha is supported by Shenzhen Institute of Artiﬁcial Intelligence and
Robotics for Society  and Shenzhen Research Institute of Big Data.

References
ACHAB  M.  BACRY  E.  GAÏFFAS  S.  MASTROMATTEO  I. and MUZY  J.-F. (2017). Uncovering
causality from multivariate hawkes integrated cumulants. The Journal of Machine Learning
Research  18 6998–7025.

AIROLDI  E. M.  BLEI  D. M.  FIENBERG  S. E. and XING  E. P. (2008). Mixed membership

stochastic blockmodels. Journal of machine learning research  9 1981–2014.

BACRY  E.  DAYRI  K. and MUZY  J.-F. (2012). Non-parametric kernel estimation for symmetric
hawkes processes. application to high frequency ﬁnancial data. The European Physical Journal B 
85 157.

BAUWENS  L. and HAUTSCH  N. (2009). Modelling ﬁnancial high frequency data using point

processes. In Handbook of ﬁnancial time series. Springer  953–979.

BENGIO  Y.  BENGIO  S. and CLOUTIER  J. (1990). Learning a synaptic learning rule. Université

de Montréal  Département d’informatique et de recherche ?

BLEI  D. M.  KUCUKELBIR  A. and MCAULIFFE  J. D. (2017). Variational inference: A review for

statisticians. Journal of the American Statistical Association  112 859–877.

9

BLUNDELL  C.  BECK  J. and HELLER  K. A. (2012). Modelling reciprocating relationships with

hawkes processes. In Advances in Neural Information Processing Systems.

BOX  G. E. and TIAO  G. C. (2011). Bayesian inference in statistical analysis  vol. 40. John Wiley

& Sons.

CHALMERS  D. J. (1991). The evolution of learning: An experiment in genetic connectionism. In

Connectionist Models. Elsevier  81–90.

CLEEREMANS  A. and MCCLELLAND  J. L. (1991). Learning the structure of event sequences.

Journal of Experimental Psychology: General  120 235.

EICHLER  M.  DAHLHAUS  R. and DUECK  J. (2017). Graphical modeling for multivariate hawkes

processes with nonparametric link functions. Journal of Time Series Analysis  38 225–242.

FARAJTABAR  M.  YANG  J.  YE  X.  XU  H.  TRIVEDI  R.  KHALIL  E.  LI  S.  SONG  L. and
ZHA  H. (2017). Fake news mitigation via point process based intervention. In Proceedings of the
34th International Conference on Machine Learning-Volume 70. JMLR. org.

FARAJTABAR  M.  YE  X.  HARATI  S.  SONG  L. and ZHA  H. (2016). Multistage campaigning in

social networks. In Advances in Neural Information Processing Systems.

FINN  C.  ABBEEL  P. and LEVINE  S. (2017). Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70. JMLR. org.

FINN  C.  XU  K. and LEVINE  S. (2018). Probabilistic model-agnostic meta-learning. In Advances

in Neural Information Processing Systems.

FOX  E. W.  SHORT  M. B.  SCHOENBERG  F. P.  CORONGES  K. D. and BERTOZZI  A. L. (2016).
Modeling e-mail networks and inferring leadership using self-exciting point processes. Journal of
the American Statistical Association  111 564–584.

GIRVAN  M. and NEWMAN  M. E. (2002). Community structure in social and biological networks.

Proceedings of the national academy of sciences  99 7821–7826.

GRANT  E.  FINN  C.  LEVINE  S.  DARRELL  T. and GRIFFITHS  T. (2018). Recasting gradient-

based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930.

HANSEN  N. R.  REYNAUD-BOURET  P.  RIVOIRARD  V. ET AL. (2015). Lasso and probabilistic

inequalities for multivariate point processes. Bernoulli  21 83–143.

HAWKES  A. G. (1971). Spectra of some self-exciting and mutually exciting point processes.

Biometrika  58 83–90.

HOFFMAN  M. D.  BLEI  D. M.  WANG  C. and PAISLEY  J. (2013). Stochastic variational inference.

The Journal of Machine Learning Research  14 1303–1347.

KARRER  B. and NEWMAN  M. E. (2011). Stochastic blockmodels and community structure in

networks. Physical review E  83 016107.

KOBAYASHI  R. and LAMBIOTTE  R. (2016). Tideh: Time-dependent hawkes process for predicting

retweet dynamics. In Tenth International AAAI Conference on Web and Social Media.

KOCH  G.  ZEMEL  R. and SALAKHUTDINOV  R. (2015). Siamese neural networks for one-shot

image recognition. In ICML deep learning workshop  vol. 2.

LAUB  P. J.  TAIMRE  T. and POLLETT  P. K. (2015). Hawkes processes. arXiv preprint

arXiv:1507.02822.

LI  L. and ZHA  H. (2013). Dyadic event attribution in social networks with mixtures of hawkes
processes. In Proceedings of the 22nd ACM international conference on Information & Knowledge
Management. ACM.

10

LINDERMAN  S. and ADAMS  R. (2014). Discovering latent network structure in point process data.

In International Conference on Machine Learning.

LUO  D.  XU  H.  ZHEN  Y.  NING  X.  ZHA  H.  YANG  X. and ZHANG  W. (2015). Multi-task
multi-dimensional hawkes processes for modeling event sequences. In Twenty-Fourth International
Joint Conference on Artiﬁcial Intelligence.

MACLAURIN  D.  DUVENAUD  D. and ADAMS  R. (2015). Gradient-based hyperparameter opti-

mization through reversible learning. In International Conference on Machine Learning.

MEI  H. and EISNER  J. M. (2017). The neural hawkes process: A neurally self-modulating

multivariate point process. In Advances in Neural Information Processing Systems.

MUNKHDALAI  T. and YU  H. (2017). Meta networks. In Proceedings of the 34th International

Conference on Machine Learning-Volume 70. JMLR. org.

NICHOL  A.  ACHIAM  J. and SCHULMAN  J. (2018). On ﬁrst-order meta-learning algorithms.

arXiv preprint arXiv:1803.02999.

NICHOL  A. and SCHULMAN  J. (2018). Reptile: a scalable metalearning algorithm. arXiv preprint

arXiv:1803.02999.

OGATA  Y. (1999). Seismicity analysis through point-process modeling: A review. In Seismicity

patterns  their statistical signiﬁcance and physical meaning. Springer  471–507.

PARANJAPE  A.  BENSON  A. R. and LESKOVEC  J. (2017). Motifs in temporal networks. In
Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. ACM.
RASMUSSEN  J. G. (2013). Bayesian inference for hawkes processes. Methodology and Computing

in Applied Probability  15 623–642.

RAVI  S. and BEATSON  A. (2018). Amortized bayesian meta-learning.
RAVI  S. and LAROCHELLE  H. (2016). Optimization as a model for few-shot learning.
REYNAUD-BOURET  P.  SCHBATH  S. ET AL. (2010). Adaptive estimation for hawkes processes;

application to genome analysis. The Annals of Statistics  38 2781–2822.

ROSS  S. M.  KELLY  J. J.  SULLIVAN  R. J.  PERRY  W. J.  MERCER  D.  DAVIS  R. M. 
WASHBURN  T. D.  SAGER  E. V.  BOYCE  J. B. and BRISTOW  V. L. (1996). Stochastic
processes  vol. 2. Wiley New York.

SANTORO  A.  BARTUNOV  S.  BOTVINICK  M.  WIERSTRA  D. and LILLICRAP  T. (2016).
Meta-learning with memory-augmented neural networks. In International conference on machine
learning.

SNELL  J.  SWERSKY  K. and ZEMEL  R. (2017). Prototypical networks for few-shot learning. In

Advances in Neural Information Processing Systems.

SUNG  F.  YANG  Y.  ZHANG  L.  XIANG  T.  TORR  P. H. and HOSPEDALES  T. M. (2018).
In Proceedings of the IEEE

Learning to compare: Relation network for few-shot learning.
Conference on Computer Vision and Pattern Recognition.

TRAN  L.  FARAJTABAR  M.  SONG  L. and ZHA  H. (2015). Netcodec: Community detection from
individual activities. In Proceedings of the 2015 SIAM International Conference on Data Mining.
SIAM.

TRIVEDI  R.  FARAJTABAR  M.  BISWAL  P. and ZHA  H. (2018). Dyrep: Learning representations

over dynamic graphs.

VINYALS  O.  BLUNDELL  C.  LILLICRAP  T.  WIERSTRA  D. ET AL. (2016). Matching networks

for one shot learning. In Advances in neural information processing systems.

XIE  J.  KELLEY  S. and SZYMANSKI  B. K. (2013). Overlapping community detection in networks:

The state-of-the-art and comparative study. Acm computing surveys (csur)  45 43.

11

XU  H.  LUO  D.  CHEN  X. and CARIN  L. (2017a). Beneﬁts from superposed hawkes processes.

arXiv preprint arXiv:1710.05115.

XU  H.  LUO  D. and ZHA  H. (2017b). Learning hawkes processes from short doubly-censored event
sequences. In Proceedings of the 34th International Conference on Machine Learning-Volume 70.
JMLR. org.

XU  H. and ZHA  H. (2017). A dirichlet mixture model of hawkes processes for event sequence

clustering. In Advances in Neural Information Processing Systems.

YANG  S.-H. and ZHA  H. (2013). Mixture of mutually exciting processes for viral diffusion. In

International Conference on Machine Learning.

ZAREZADE  A.  KHODADADI  A.  FARAJTABAR  M.  RABIEE  H. R. and ZHA  H. (2017). Corre-
lated cascades: Compete or cooperate. In Thirty-First AAAI Conference on Artiﬁcial Intelligence.
ZHANG  Y. and YANG  Q. (2017). A survey on multi-task learning. arXiv preprint arXiv:1707.08114.
ZHAO  Q.  ERDOGDU  M. A.  HE  H. Y.  RAJARAMAN  A. and LESKOVEC  J. (2015). Seismic: A
self-exciting point process model for predicting tweet popularity. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM.

ZHOU  K.  ZHA  H. and SONG  L. (2013). Learning social infectivity in sparse low-rank networks

using multi-dimensional hawkes processes. In Artiﬁcial Intelligence and Statistics.

12

,Yujia Xie
Haoming Jiang
Feng Liu
Tuo Zhao
Hongyuan Zha