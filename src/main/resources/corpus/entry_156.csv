2013,A Novel Two-Step Method for Cross Language Representation Learning,Cross language text classiﬁcation is an important learning task in natural language processing. A critical challenge of cross language learning lies in that words of different languages are in disjoint feature spaces. In this paper  we propose a two-step representation learning method to bridge the feature spaces of different languages by exploiting a set of parallel bilingual documents. Speciﬁcally  we ﬁrst formulate a matrix completion problem to produce a complete parallel document-term matrix for all documents in two languages  and then induce a cross-lingual document representation by applying latent semantic indexing on the obtained matrix. We use a projected gradient descent algorithm to solve the formulated matrix completion problem with convergence guarantees. The proposed approach is evaluated by conducting a set of experiments with cross language sentiment classiﬁcation tasks on Amazon product reviews. The experimental results demonstrate that the proposed learning approach outperforms a number of comparison cross language representation learning methods  especially when the number of parallel bilingual documents is small.,A Novel Two-Step Method for Cross Language

Representation Learning

Min Xiao and Yuhong Guo

Department of Computer and Information Sciences
Temple University  Philadelphia  PA 19122  USA

{minxiao  yuhong}@temple.edu

Abstract

Cross language text classiﬁcation is an important learning task in natural language
processing. A critical challenge of cross language learning arises from the fact that
words of different languages are in disjoint feature spaces. In this paper  we pro-
pose a two-step representation learning method to bridge the feature spaces of dif-
ferent languages by exploiting a set of parallel bilingual documents. Speciﬁcally 
we ﬁrst formulate a matrix completion problem to produce a complete parallel
document-term matrix for all documents in two languages  and then induce a low
dimensional cross-lingual document representation by applying latent semantic
indexing on the obtained matrix. We use a projected gradient descent algorithm
to solve the formulated matrix completion problem with convergence guarantees.
The proposed method is evaluated by conducting a set of experiments with cross
language sentiment classiﬁcation tasks on Amazon product reviews. The experi-
mental results demonstrate that the proposed learning method outperforms a num-
ber of other cross language representation learning methods  especially when the
number of parallel bilingual documents is small.

1

Introduction

Cross language text classiﬁcation is an important natural language processing task that exploits a
large amount of labeled documents in an auxiliary source language to train a classiﬁcation model for
classifying documents in a target language where labeled data is scarce. An effective cross language
learning system can greatly reduce the manual annotation effort in the target language for learning
good classiﬁcation models. Previous work in the literature has demonstrated successful performance
of cross language learning systems on various cross language text classiﬁcation problems  including
multilingual document categorization [2]  cross language ﬁne-grained genre classiﬁcation [14]  and
cross-lingual sentiment classiﬁcation [18  16].

The challenge of cross language text classiﬁcation lies in the language barrier. That is documents
in different languages are expressed with different word vocabularies and thus have disjoint feature
spaces. A variety of methods have been proposed in the literature to address cross language text
classiﬁcation by bridging the cross language gap  including transforming the training or test data
from one language domain into the other language domain by using machine translation tools or
bilingual lexicons [18  6  23]  and constructing cross-lingual representations by using readily avail-
able auxiliary resources such as bilingual word pairs [16]  comparable corpora [10  20  15]  and
other multilingual resources [3  14].

In this paper  we propose a two-step learning method to induce cross-lingual feature representa-
tions for cross language text classiﬁcation by exploiting a set of unlabeled parallel bilingual docu-
ments. First we construct a concatenated bilingual document-term matrix where each document is
represented in the concatenated vocabulary of two languages. In such a matrix  a pair of parallel

1

documents are represented as a row vector ﬁlled with observed word features from both the source
language domain and the target language domain  while a non-parallel document in a single lan-
guage is represented as a row vector ﬁlled with observed word features only from its own language
and has missing values for the word features from the other language. We then learn the unobserved
feature entries of this sparse matrix by formulating a matrix completion problem and solving it us-
ing a projected gradient descent optimization algorithm. By doing so  we expect to automatically
capture important and robust low-rank information based on the word co-occurrence patterns ex-
pressed both within each language and across languages. Next we perform latent semantic indexing
over the recovered document-term matrix and induce a low-dimensional dense cross-lingual repre-
sentation of the documents  on which standard monolingual classiﬁers can be applied. To evaluate
the effectiveness of the proposed learning method  we conduct a set of experiments with cross lan-
guage sentiment classiﬁcation tasks on multilingual Amazon product reviews. The empirical results
show that the proposed method signiﬁcantly outperforms a number of cross language learning meth-
ods. Moreover  the proposed method produces good performance even with a very small number of
unlabeled parallel bilingual documents.

2 Related Work

Many works in the literature address cross language text classiﬁcation by ﬁrst translating documents
from one language domain into the other one via machine translation tools or bilingual lexicons
and then applying standard monolingual classiﬁcation algorithms [18  23]  domain adaptation tech-
niques [17  9  21]  or multi-view learning methods [22  2  1  13  12]. For example  [17] proposed
an expectation-maximization based self-training method  which ﬁrst initializes a monolingual clas-
siﬁer in the target language with the translated labeled documents from the source language and
then retrains the model by adding unlabeled documents from the target language with automatically
predicted labels.
[21] proposed an instance and feature bi-weighting method by ﬁrst translating
documents from one language domain to the other one and then simultaneously re-weighting in-
stances and features to address the distribution difference across domains. [22] proposed to use
the co-training method for cross language sentiment classiﬁcation on parallel corpora.
[2] pro-
posed a multi-view majority voting method to categorize documents in multiple views produced
from machine translation tools. [1] proposed a multi-view co-classiﬁcation method for multilingual
document categorization  which minimizes both the training loss for each view and the prediction
disagreement between different language views. Our proposed approach in this paper shares similar-
ity with these approaches in exploiting parallel data produced by machine translation tools. But our
approach only requires a small set of unlabeled parallel documents  while these approaches require
at least translating all the training documents in one language domain.

Another important group of cross language text classiﬁcation methods in the literature con-
struct cross-lingual representations by exploiting bilingual word pairs [16  7]  parallel corpora
[10  20  15  19  8]  and other resources [3  14].
[16] proposed a cross-language structural cor-
respondence learning method to induce language-independent features by using pivot word pairs
produced by word translation oracles.
[10] proposed a cross-language latent semantic indexing
(CL-LSI) method to induce cross-lingual representations by performing LSI over a dual-language
document-term matrix  where each dual-language document contains its original words and the
corresponding translation text. [20] proposed a cross-lingual kernel canonical correlation analysis
(CL-KCCA) method. It ﬁrst learns two projections (one for each language) by conducting kernel
canonical correlation analysis over a paired bilingual corpus and then uses them to project doc-
uments from language-speciﬁc feature spaces to the shared multilingual semantic feature space.
[15] employed cross-lingual oriented principal component analysis (CL-OPCA) over concatenated
parallel documents to learn a multilingual projection by simultaneously minimizing the projected
distance between parallel documents and maximizing the projected covariance of documents across
languages. Some other work uses multilingual topic models such as the coupled probabilistic latent
semantic analysis and the bilingual latent Dirichlet allocation to extract latent cross-lingual topics
as interlingual representations [19]. [14] proposed to use language-speciﬁc part-of-speech (POS)
taggers to tag each word and then map those language-speciﬁc POS tags to twelve universal POS
tags as interlingual features for cross language ﬁne-grained genre classiﬁcation. Similar to the mul-
tilingual semantic representation learning approaches such as CL-LSI  CL-KCCA and CL-OPCA 
our two-step learning method exploits parallel documents. But different from these methods which
apply operations such as LSI  KCCA  and OPCA directly on the original concatenated document-

2

term matrix  our method ﬁrst ﬁlls the missing entries of the document-term matrix using matrix
completion  and then performs LSI over the recovered low-rank matrix.

3 Approach

In this section  we present the proposed two-step learning method for learning cross-lingual docu-
ment representations. We assume a subset of unlabeled parallel documents from the two languages
are given  which can be used to capture the co-occurrence of terms across languages and build con-
nections between the vocabulary sets of the two languages. We ﬁrst construct a uniﬁed document-
term matrix for all documents from the auxiliary source language domain and the target language
domain  whose columns correspond to the word features from the uniﬁed vocabulary set of the two
languages. In this matrix  each pair of parallel documents is represented as a fully observed row
vector  and each non-parallel document is represented as a partially observed row vector where only
entries corresponding to words in its own language vocabulary are observed. Instead of learning a
low-dimensional cross-lingual document representation from this matrix directly  we perform a two-
step learning procedure: First we learn a low-rank document-term matrix by automatically ﬁlling the
missing entries via matrix completion. Next we produce cross-lingual representations by applying
the latent semantic indexing method over the learned matrix.

Let M 0 ∈ Rt×d be the uniﬁed document-term matrix  which is partially ﬁlled with observed nonneg-
ative feature values  where t is the number of documents and d is the size of the uniﬁed vocabulary.
We use Ω to denote the index set of the observed features in M 0  such that (i  j) ∈ Ω if only if M 0
ij
is observed; and use bΩ to denote the index set of the missing features in M 0  such that (i  j) ∈ bΩ
ij is unobserved. For the i-th document in the data set from one language  if the doc-
if only if M 0
ument does not have a parallel translation in the other language  then all the features in row M 0
i:
corresponding to the words in the vocabulary of the other language are viewed as missing features.

3.1 Matrix Completion

Note that the document-term matrix M 0 has a large fraction of missing features and the only bridge
between the vocabulary sets of the two languages is the small set of parallel bilingual documents.
Learning from this partially observed matrix directly by treating missing features as zeros certainly
will lose a lot of information. On the other hand  a fully observed document-term matrix is naturally
low-rank and sparse  as the vocabulary set is typically very large and each document only contains
a small fraction of the words in the vocabulary. Thus we propose to automatically ﬁll the missing
entries of M 0 based on the feature co-occurrence information expressed in the observed data  by
conducting matrix completion to recover a low-rank and sparse matrix. Speciﬁcally  we formulate
the matrix completion as the following optimization problem

min
M

rank(M ) + µkM k1

subject to Mij = M 0

ij  ∀(i  j) ∈ Ω; Mij ≥ 0  ∀(i  j) ∈ bΩ

(1)

where k · k1 denotes a ℓ1 norm and is used to enforce sparsity. The rank function however is non-
convex and difﬁcult to optimize. We can relax it to its convex envelope  a convex trace norm kM k∗.
Moreover  instead of using the equality constraints in (1)  we propose to minimize a regulariza-
ij)  to cope with observation noise for all the observed feature entries.
tion loss function  c(Mij  M 0
Meanwhile  we also add regularization terms over the missing features  c(Mij  0)  ∀(i  j) ∈ bΩ  to
avoid overﬁtting. In particular  we use the least squared loss function c(x  y) = 1
2 kx − yk2. Hence
we obtain the following relaxed convex optimization problem for matrix completion

min
M

γkM k∗ + µkM k1 + X

c(Mij  M 0

ij) + ρ X

(i j)∈Ω

(i j)∈ bΩ

c(Mij  0)

subject to M ≥ 0

(2)

With nonnegativity constraints M ≥ 0  the non-smooth ℓ1 norm regularizer in the objective function
of (2) is equivalent to a smooth linear function kM k1 = Pij Mij. Nevertheless  with the non-
smooth trace norm kM k∗  the optimization problem (2) remains to be convex but non-smooth.
Moreover  the matrix M in cross-language learning tasks is typically very large  and thus a scalable
optimization algorithm needs to be developed to conduct efﬁcient optimization. In next section  we
will present a scalable projected gradient descent algorithm to solve this minimization problem.

3

Algorithm 1 Projected Gradient Descent Algorithm

Input: M 0  γ  ρ ≤ 1  0 < τ < min(2  2
Initialize M as the nonnegative projection of the rank-1 approximation of M 0.
while not converged do

ρ )  µ.

1. gradient descent: M = M − τ ∇g(M ).
2. shrink: M = Sτ γ(M ).
3. project onto feasible set: M = max(M  0).

end while

3.2 Latent Semantic Indexing

After solving (2) for an optimal low-rank solution M ∗  we can use each row of the sparse matrix
M ∗ as a vector representation for each document in the concatenated vocabulary space of the two
languages. However exploiting such a matrix representation directly for cross language text clas-
siﬁcation lacks sufﬁcient capacity of handling feature noise and sparseness  as each document is
represented using a very small set of words in the vocabulary set. We thus propose to apply a latent
semantic indexing (LSI) method on M ∗ to produce a low-dimensional semantic representation of
the data. LSI uses singular value decomposition to discover the important associative relationships
of word features [10]  and create a reduced-dimension feature space. Speciﬁcally  we ﬁrst perform
singular value decomposition over M ∗  M ∗ = U SV ⊤  and then obtain a low dimensional represen-
tation matrix Z via a projection Z = M ∗Vk  where Vk contains the top k right singular vectors of
M ∗. Cross-language text classiﬁcation can then be conducted over Z using monolingual classiﬁers.

4 Optimization Algorithm

4.1 Projected Gradient Descent Algorithm

A number of algorithms have been developed to solve matrix completion problems in the litera-
ture [4  11]. We use a projected gradient descent algorithm to solve the non-smooth convex opti-
mization problem in (2). This algorithm takes the objective function f (M ) in (2) as a composition
of a non-smooth term and a convex smooth term such as f (M ) = γkM k∗ + g(M ) where

g(M ) = µkM k1 + X

c(Mij  M 0

ij) + ρ X

c(Mij  0).

(3)

(i j)∈Ω

(i j)∈ bΩ

It ﬁrst initializes M as the nonnegative projection of the rank-1 approximation of M 0  and then
iteratively updates M using a projected gradient descent procedure. In each iteration  we perform
three steps to update M. First  we take a gradient descent step M = M − τ ∇g(M ) with stepsize τ
and gradient function

∇g(M ) = µE + (M − M 0) ◦ Y + ρM ◦ bY

(4)
where E is a t × d matrix with all 1s; Y and bY are t × d indicator matrices such that Yij = 1 if
and only if (i  j) ∈ Ω and bY = E − Y ; and “◦” denotes the Hadamard product. Next we perform a
shrinkage operation M = Sν(M ) over the resulting matrix from the ﬁrst step to minimize its rank.
The shrinkage operator is based on singular value decomposition

Sν(M ) = U Σ(ν)V ⊤  M = U ΣV ⊤  Σ(ν) = max(Σ − ν  0) 

(5)
where ν = τ γ. Finally we project the resulting matrix into the nonnegative feasible set by M =
max(M  0). This update procedure provably converges to an optimal solution. The overall algorithm
is given in Algorithm 1.

4.2 Convergence Analysis

Let h(·) = I(·) − τ ∇g(·) be the gradient descent operator used in the gradient descent step  and
let PC(·) = max(·  0) be the projection operator  while Sν(·) is the shrinkage operator. Below we
prove the convergence of the projected gradient descent algorithm.

4

Lemma 1. Let E be a t×d matrix with all 1s  and Q = E −τ (Y +ρbY ). For τ ∈ (0  min(2  2
ρ ))  the
operator h(·) is non-expansive  i.e.  for any M and M ′ ∈ Rt×d  kh(M )−h(M ′)kF ≤ kM −M ′kF .
Moreover  kh(M ) − h(M ′)kF = kM − M ′kF if and only if h(M ) − h(M ′) = M − M ′.

Proof. Note that for τ ∈ (0  min(2  2
gradient deﬁnition in (4)  we have

kh(M ) − h(M ′)kF = (cid:13)(cid:13)(M − M ′) ◦ QkF = (X

ij

ρ ))  we have −1 < Qij < 1  ∀(i  j). Then following the

(Mij − M ′

ij)2Q2

ij)

1

2 ≤ kM − M ′kF

The inequalities become equalities if only if h(M ) − h(M ′) = M − M ′.
Lemma 2. [11  Lemma 1] The shrinkage operator Sν(·) is non-expansive  i.e.  for any M and
M ′ ∈ Rt×d  kSν(M )−Sν(M ′)kF ≤ kM −M ′kF . Moreover  kSν(M )−Sν(M ′)kF = kM −M ′kF
if and only if Sν(M ) − Sν(M ′) = M − M ′.
Lemma 3. The projection operator PC(·) is non-expansive  i.e.  kPC(M ) − PC(M ′)kF ≤ kM −
M ′kF . Moreover  kPC(M )−PC(M ′)kF = kM −M ′kF if and only if PC(M )−PC(M ′) = M −M ′.

Proof. For any given entry index (i  j)  there are four cases:

• Case 1: Mij ≥ 0  M ′

• Case 2: Mij ≥ 0  M ′

ij ≥ 0. We have (PC(Mij) − PC(M ′
ij < 0. We have (PC(Mij) − PC(M ′

• Case 3: Mij < 0  M ′

• Case 4: Mij < 0  M ′

ij ≥ 0. We have (PC(Mij) − PC(M ′
ij < 0. We have (PC(Mij) − PC(M ′

ij)2.

ij < (Mij − M ′

ij))2 = (Mij − M ′
ij))2 = M 2
ij))2 = M ′2
ij))2 = 0 ≤ (Mij − M ′

ij < (Mij − M ′
ij)2.

ij)2.

ij)2.

Therefore 

kPC(M ) − PC(M ′)kF = (cid:0)X

(PC(Mij) − PC(M ′

ij))2(cid:1) 1

2 ≤ (cid:0)X

(Mij − M ′

2 = kM − M ′kF

ij)2(cid:1) 1

ij

ij

and kPC(M ) − PC(M ′)kF = kM − M ′kF if only if PC(M ) − PC(M ′) = M − M ′.
Theorem 1. The sequence {M k} generated by the projected gradient descent iterations in Algo-
rithm 1 with 0 < τ < min(2  2

ρ ) converges to M ∗  which is an optimal solution of (2).

Proof. Since h(·)  Sν(·) and PC(·) are all non-expansive  the composite operator PC(Sν(h(·))) is
non-expansive as well. This theorem can then be proved following [11  Theorem 4].

5 Experiments

In this section  we evaluate the proposed two-step learning method by conducting extensive cross
language sentiment classiﬁcation experiments on multilingual Amazon product reviews.

5.1 Experimental Setting

Dataset We used the multilingual Amazon product reviews dataset [16]  which contains three
categories (Books (B)  DVD (D)  Music (M)) of product reviews in four different languages (English
(E)  French (F)  German (G)  Japanese (J)). For each category of the product reviews  there are 2000
positive and 2000 negative English reviews  and 1000 positive and 1000 negative reviews for each
of the other three languages. In addition  there are another 2000 unlabeled parallel reviews between
English and each of the other three languages. Each review is preprocessed into a unigram bag-of-
word feature vector with TF-IDF values. We focused on cross-lingual learning between English and
the other three languages and constructed 18 cross language sentiment classiﬁcation tasks (EFB 
FEB  EFD  FED  EFM  FEM  EGB  GEB  EGD  GED  EGM  GEM  EJB  JEB  EJD  JED  EJM 
JEM)  each for one combination of selected source language  target language and category. For
example  the task EFB uses English Books reviews as the source language data and uses French
Books reviews as the target language data.

5

Table 1: Average classiﬁcation accuracies (%) and standard deviations (%) over 10 runs for the 18
cross language sentiment classiﬁcation tasks.

TBOW

TASK
67.31±0.96
EFB
FEB
66.82±0.43
67.80±0.94
EFD
66.15±0.65
FED
EFM 67.84±0.43
FEM 66.08±0.52
67.23±0.68
EGB
GEB
67.16±0.55
66.79±0.80
EGD
66.27±0.69
GED
EGM 67.65±0.45
GEM 66.74±0.55
63.15±0.69
EJB
66.85±0.68
JEB
EJD
65.47±0.50
66.42±0.55
JED
67.62±0.75
EJM
JEM
66.51±0.51

CL-LSI

79.56±0.21
76.66±0.34
77.82±0.66
76.61±0.25
75.39±0.40
76.33±0.27
77.59±0.21
77.64±0.19
79.22±0.22
77.78±0.26
73.81±0.49
77.28±0.51
72.68±0.35
74.63±0.42
72.55±0.28
75.18±0.27
73.44±0.50
72.38±0.50

CL-KCCA
77.56±0.14
73.45±0.13
78.19±0.09
74.93±0.07
78.24±0.12
73.38±0.12
79.14±0.12
74.15±0.09
76.73±0.10
74.26±0.08
79.18±0.05
72.31±0.08
69.46±0.11
67.99±0.18
74.79±0.11
72.44±0.16
73.54±0.11
70.00±0.18

CL-OPCA
76.55±0.31
74.43±0.53
70.54±0.41
72.49±0.47
73.69±0.49
73.46±0.50
74.72±0.54
74.78±0.39
74.59±0.66
74.83±0.45
74.45±0.59
74.15±0.42
71.41±0.48
73.41±0.41
71.84±0.41
75.42±0.52
74.96±0.86
72.64±0.66

TSL

81.92±0.20
79.51±0.21
81.97±0.33
78.09±0.32
79.30±0.30
78.53±0.46
79.22±0.31
78.65±0.23
81.34±0.24
79.34±0.23
79.39±0.39
79.02±0.34
72.57±0.52
77.17±0.36
76.60±0.49
79.01±0.50
76.21±0.40
77.15±0.58

Approaches We compared the proposed two-step learning (TSL) method with the following four
methods: TBOW  CL-LSI  CL-OPCA and CL-KCCA. The Target Bag-Of-Word (TBOW) baseline
method trains a supervised monolingual classiﬁer in the original bag-of-word feature space with the
labeled training data from the target language domain. The Cross-Lingual Latent Semantic Indexing
(CL-LSI) method [10] and the Cross-Lingual Oriented Principal Component Analysis (CL-OPCA)
method [15] ﬁrst learn cross-lingual representations with all data from both language domains by
performing LSI or OPCA and then train a monolingual classiﬁer with labeled data from both lan-
guage domains in the induced low-dimensional feature space. The Cross-Lingual Kernel Canonical
Component Analysis (CL-KCCA) method [20] ﬁrst induces two language projections by using un-
labeled parallel data and then trains a monolingual classiﬁer on labeled data from both language
domains in the projected low-dimensional space. For all experiments  we used linear support vector
machine (SVM) as the monolingual classiﬁcation model. For implementation  we used the libsvm
package [5] with default parameter setting.

5.2 Classiﬁcation Accuracy

For each of the 18 cross language sentiment classiﬁcation tasks  we used all documents from the two
languages and the additional 2000 unlabeled parallel documents for representation learning. Then
we used all documents in the auxiliary source language and randomly chose 100 documents from
the target language as labeled data for classiﬁcation model training  and used the remaining data in
the target language as test data. For the proposed method  TSL  we set µ = 10−6 and τ = 1  chose
γ value from {0.01  0.1  1  10}  chose ρ value from {10−5  10−4  10−3  10−2  10−1  1}  and chose
the dimension k value from {20  50  100  200  500}. We used the ﬁrst task EFB to perform model
parameter selection by running the algorithm 3 times based on random selections of 100 labeled
target training data. This gave us the following parameter setting: γ = 0.1  ρ = 10−4  k = 50. We
used the same procedure to select the dimensionality of the learned semantic representations for the
other three approaches  CL-LSI  CL-OPCA and CL-KCCA  which produced k = 50 for CL-LSI
and CL-OPCA  and k = 100 for CL-KCCA. We then used the selected model parameters for all
the 18 tasks and ran each experiment for 10 times based on random selections of 100 labeled target
documents. The average classiﬁcation accuracies and standard deviations are reported in Table 1.

We can see that the proposed two-step learning method  TSL  outperforms all other four comparison
methods in general. The target baseline TBOW performs poorly on all the 18 tasks  which implies
that 100 labeled target training documents are far from enough to obtain a robust sentiment classiﬁer

6

EFB

EFD

 

 

80

y
c
a
r
u
c
c
A

75

70

65

 

80

75

70

65

60

 

 

72

70

68

66

64

62

60

58

56

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

82

80

78

76

74

72

70

68

66

 

80

75

70

65

 

76

74

72

70

68

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

EGB

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

EJB

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

2000

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

EGD

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

EJD

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

2000

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

80

78

76

74

72

70

68

66

64

 

80

75

70

65

60

 

 

76

74

72

70

68

66

EFM

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

EGM

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

EJM

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

2000

Figure 1: Average test classiﬁcation accuracies (%) and standard deviations (%) over 10 runs with
different numbers of unlabeled parallel documents for adapting a classiﬁcation system from English
to French  German and Japanese.

in the target language domain. All the other three cross-lingual representation learning methods 
CL-LSI  CL-KCCA and CL-OPCA  consistently outperform this baseline method across all the
18 tasks  which demonstrates that the labeled training data from the source language domain is
useful for classifying the target language data under a uniﬁed data representation. Nevertheless  the
improvements achieved by these three methods over the baseline are much smaller than the proposed
TSL method. Across all the 18 tasks  TSL increases the average test accuracy over the baseline
TBOW method by at least 8.59 (%) on the EJM task and up to 14.61 (%) on the EFB task. Moreover 
TSL also outperforms both CL-KCCA and CL-OPCA across all the 18 tasks  outperforms CL-LSI
on 17 out of the 18 tasks and achieves comparable performance with CL-LSI on the remaining
one task (EJB). All these results demonstrate the efﬁcacy and robustness of the proposed two-step
representation learning method for cross language text classiﬁcation.

5.3

Impact of the Size of Unlabeled Parallel Data

All the four cross-lingual adaptation learning methods  CL-LSI  CL-KCCA  CL-OPCA and TSL 
exploit unlabeled parallel reviews for learning cross-lingual representations. Next we investigated
the performance of these methods with respect to different numbers of unlabeled parallel reviews.
We tested a set of different numbers  np ∈ {200  500  1000  2000}. For each number np in the set 
we randomly chose np parallel documents from all the 2000 unlabeled parallel reviews to conduct
experiments using the same setting from the previous experiments. Each experiment was repeated
10 times based on random selections of labeled target training data. The average test classiﬁcation
accuracies and standard deviations are plotted in Figure 1 and Figure 2. Figure 1 presents the results
for the 9 cross-lingual classiﬁcation tasks that adapt classiﬁcation systems from English to French 
German and Japanese  while Figure 2 presents the results for the other 9 cross-lingual classiﬁcation
tasks that adapt classiﬁcation systems from French  German and Japanese to English.

7

FEB

FED

FEM

 

 

 

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

80

78

76

74

72

 

78

76

74

72

70

 

78

76

74

72

70

68

66

 

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

78

76

74

72

70

68

 

80

78

76

74

72

70

 

80

78

76

74

72

70

68

 

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

y
c
a
r
u
c
c
A

78

76

74

72

70

 

80

78

76

74

72

70

78

76

74

72

70

68

66

 

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

GED

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

JED

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

2000

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

GEB

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

JEB

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

2000

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

GEM

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

JEM

2000

 

CL−LSI
CL−KCCA
CL−OPCA
TSL

500

1500
Unlabeled parallel data

1000

2000

Figure 2: Average test classiﬁcation accuracies and standard deviations over 10 runs with different
numbers of unlabeled parallel documents for adapting a classiﬁcation system from French  German
and Japanese to English.

From these results  we can see that the performance of all four methods in general improves with the
increase of the unlabeled parallel data. The proposed method  TSL  nevertheless outperforms the
other three cross-lingual adaptation learning methods across the range of different np values for 16
out of the 18 cross language sentiment classiﬁcation tasks. For the remaining two tasks  EFM and
EGM  it has similar performance with the CL-KCCA method while signiﬁcantly outperforming the
other two methods. Moreover  for the 9 tasks that make adaptation from English to the other three
languages  the TSL method achieves great performance with only 200 unlabeled parallel documents 
while the performance of the other three methods decreases signiﬁcantly with the decrease of the
number of unlabeled parallel documents. These results demonstrate the robustness and efﬁcacy of
the proposed method  comparing to other methods.

6 Conclusion

In this paper  we developed a novel two-step method to learn cross-lingual semantic data representa-
tions for cross language text classiﬁcation by exploiting unlabeled parallel bilingual documents. We
ﬁrst formulated a matrix completion problem to infer unobserved feature values of the concatenated
document-term matrix in the space of uniﬁed vocabulary set from the source and target languages.
Then we performed latent semantic indexing over the completed low-rank document-term matrix to
produce a low-dimensional cross-lingual representation of the documents. Monolingual classiﬁers
were then used to conduct cross language text classiﬁcation based on the learned document repre-
sentation. To investigate the effectiveness of the proposed learning method  we conducted extensive
experiments with tasks of cross language sentiment classiﬁcation on Amazon product reviews. Our
experimental results demonstrated that the proposed two-step learning method signiﬁcantly out-
performs the other four comparison methods. Moreover  the proposed approach needs much less
parallel documents to produce a good cross language text classiﬁcation system.

8

References

[1] M. Amini and C. Goutte. A co-classiﬁcation approach to learning from multilingual corpora.

Machine Learning  79:105–121  2010.

[2] M. Amini  N. Usunier  and C. Goutte. Learning from multiple partially observed views - an

application to multilingual text categorization. In NIPS  2009.

[3] B. A.R.  A. Joshi  and P. Bhattacharyya. Cross-lingual sentiment analysis for indian languages

using linked wordnets. In Proc. of COLING  2012.

[4] E. Cand´es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational Mathematics  9(6):717–772  2009.

[5] C. Chang and C. Lin. LIBSVM: A library for support vector machines. ACM Transactions on

Intelligent Systems and Technology  2:27:1–27:27  2011.

[6] W. Dai  Y. Chen  G. Xue  Q. Yang  and Y. Yu. Translated learning: Transfer learning across

different feature spaces. In NIPS  2008.

[7] A. Gliozzo. Exploiting comparable corpora and bilingual dictionaries for cross-language text

categorization. In Proc. of ICCL-ACL  2006.

[8] J. Jagarlamudi  R. Udupa  H. Daum´e III  and A. Bhole. Improving bilingual projections via

sparse covariance matrices. In Proc. of EMNLP  2011.

[9] X. Ling  G. Xue  W. Dai  Y. Jiang  Q. Yang  and Y. Yu. Can chinese web pages be classiﬁed

with English data source? In Proc. of WWW  2008.

[10] M. Littman  S. Dumais  and T. Landauer. Automatic cross-language information retrieval using
latent semantic indexing. In Cross-Language Information Retrieval  chapter 5  pages 51–62.
Kluwer Academic Publishers  1998.

[11] S. Ma  D. Goldfarb  and L. Chen. Fixed point and bregman iterative methods for matrix rank

minimization. Mathematical Programming: Series A and B archive  128  Issue 1-2  2011.

[12] X. Meng  F. Wei  X. Liu  M. Zhou  G. Xu  and H. Wang. Cross-lingual mixture model for

sentiment classiﬁcation. In Proc. of ACL  2012.

[13] J. Pan  G. Xue  Y. Yu  and Y. Wang. Cross-lingual sentiment classiﬁcation via bi-view non-

negative matrix tri-factorization. In Proc. of PAKDD  2011.

[14] P. Petrenz and B. Webber. Label propagation for ﬁne-grained cross-lingual genre classiﬁcation.

In Proc. of the NIPS xLiTe workshop  2012.

[15] J. Platt  K. Toutanova  and W. Yih. Translingual document representations from discriminative

projections. In Proc. of EMNLP  2010.

[16] P. Prettenhofer and B. Stein. Cross-language text classiﬁcation using structural correspondence

learning. In Proc. of ACL  2010.

[17] L. Rigutini and M. Maggini. An EM based training algorithm for cross-language text catego-

rization. In Proc. of the Web Intelligence Conference  2005.

[18] J. Shanahan  G. Grefenstette  Y. Qu  and D. Evans. Mining multilingual opinions through
classiﬁcation and translation. In AAAI Spring Symp. on Explor. Attit. and Affect in Text  2004.
[19] W. Smet  J. Tang  and M. Moens. Knowledge transfer across multilingual corpora via latent

topics. In Proc. of PAKDD  2011.

[20] A. Vinokourov  J. Shawe-taylor  and N. Cristianini. Inferring a semantic representation of text

via cross-language correlation analysis. In NIPS  2002.

[21] C. Wan  R. Pan  and J. Li. Bi-weighting domain adaptation for cross-language text classiﬁca-

tion. In Proc. of IJCAI  2011.

[22] X. Wan. Co-training for cross-lingual sentiment classiﬁcation. In Proc. of ACL-IJCNLP  2009.
[23] K. Wu  X. Wang  and B. Lu. Cross language text categorization using a bilingual lexicon. In

Proc. of IJCNLP  2008.

9

,Min Xiao
Yuhong Guo