2013,Which Space Partitioning Tree to Use for Search?,We consider the task of nearest-neighbor search with the class of binary-space-partitioning trees  which includes kd-trees  principal axis trees and random projection trees  and try to rigorously answer the question which tree to use for nearest-neighbor search?'' To this end  we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees. We also explore another factor affecting the search performance -- margins of the partitions in these trees. We demonstrate  both theoretically and empirically  that large margin partitions can improve the search performance of a space-partitioning tree. ",Which Space Partitioning Tree to Use for Search?

P. Ram

Georgia Tech. / Skytree  Inc.

Atlanta  GA 30308

p.ram@gatech.edu

A. G. Gray
Georgia Tech.

Atlanta  GA 30308

agray@cc.gatech.edu

Abstract

We consider the task of nearest-neighbor search with the class of binary-space-
partitioning trees  which includes kd-trees  principal axis trees and random projec-
tion trees  and try to rigorously answer the question “which tree to use for nearest-
neighbor search?” To this end  we present the theoretical results which imply that
trees with better vector quantization performance have better search performance
guarantees. We also explore another factor affecting the search performance –
margins of the partitions in these trees. We demonstrate  both theoretically and
empirically  that large margin partitions can improve tree search performance.

1 Nearest-neighbor search

Nearest-neighbor search is ubiquitous in computer science. Several techniques exist for nearest-
neighbor search  but most algorithms can be categorized into two following groups based on the in-
dexing scheme used – (1) search with hierarchical tree indices  or (2) search with hash-based indices.
Although multidimensional binary space-partitioning trees (or BSP-trees)  such as kd-trees [1]  are
widely used for nearest-neighbor search  it is believed that their performances degrade with increas-
ing dimensions. Standard worst-case analyses of search with BSP-trees in high dimensions usually
lead to trivial guarantees (such as  an Ω(n) search time guarantee for a single nearest-neighbor query
in a set of n points). This is generally attributed to the “curse of dimensionality” – in the worst case 
the high dimensionality can force the search algorithm to visit every node in the BSP-tree.
However  these BSP-trees are very simple and intuitive  and still used in practice with success.
The occasional favorable performances of BSP-trees in high dimensions are attributed to the low
“intrinsic” dimensionality of real data. However  no clear relationship between the BSP-tree search
performance and the intrinsic data properties is known. We present theoretical results which link the
search performance of BSP-trees to properties of the data and the tree. This allows us to identify
implicit factors inﬂuencing BSP-tree search performance — knowing these driving factors allows
us to develop successful heuristics for BSP-trees with improved search performance.
Each node in a BSP-tree represents a region of the space and
each non-leaf node has a left and right child representing a dis-
joint partition of this region with some separating hyperplane
and threshold (w  b). A search query on this tree is usually
answered with a depth-ﬁrst branch-and-bound algorithm. Al-
gorithm 1 presents a simpliﬁed version where a search query
is answered with a small set of neighbor candidates of any de-
sired size by performing a greedy depth-ﬁrst tree traversal to
a speciﬁed depth. This is known as defeatist tree search. We
are not aware of any data-dependent analysis of the quality of
the results from defeatist BSP-tree search. However  Verma et
al. (2009) [2] presented adaptive data-dependent analyses of
some BSP-trees for the task of vector quantization. These re-
sults show precise connections between the quantization per-
formance of the BSP-trees and certain properties of the data
(we will present these data properties in Section 2).

Query q  Desired depth l
Output: Candidate neighbor p
current tree depth lc ← 0
current tree node Tc ← T
while lc < l do

end if
Increment depth lc ← lc + 1
end while
p ← arg minr∈Tc∩S (cid:107)q − r(cid:107).

if (cid:104)Tc.w  q(cid:105) + Tc.b ≤ 0 then
else

Tc ← Tc.left child
Tc ← Tc.right child

Algorithm 1 BSP-tree search
Input: BSP-tree T on set S 

1

(a) kd-tree

(b) RP-tree

(c) MM-tree

Figure 1: Binary space-partitioning trees.

We establish search performance guarantees for BSP-trees by linking their nearest-neighbor perfor-
mance to their vector quantization performance and utilizing the recent guarantees on the BSP-tree
vector quantization. Our results provide theoretical evidence  for the ﬁrst time  that better quantiza-
tion performance implies better search performance1. These results also motivate the use of large
margin BSP-trees  trees that hierarchically partition the data with a large (geometric) margin  for
better nearest-neighbor search performance. After discussing some existing literature on nearest-
neighbor search and vector quantization in Section 2  we discuss our following contributions:
• We present performance guarantees for Algorithm 1 in Section 3  linking search performance
to vector quantization performance. Speciﬁcally  we show that for any balanced BSP-tree and a
depth l  under some conditions  the worst-case search error incurred by the neighbor candidate
returned by Algorithm 1 is proportional to a factor which is

(cid:18) 2l/2 exp(−l/2β)

(n/2l)1/O(d) − 2

O

(cid:19)

 

proportional to (1/γ) where γ is the smallest margin size of all the partitions in T .

where β corresponds to the quantization performance of the tree (smaller β implies smaller
quantization error) and d is closely related to the doubling dimension of the dataset (as opposed
to the ambient dimension D of the dataset). This implies that better quantization produces better
worst-case search results. Moreover  this result implies that smaller l produces improved worst-
case performance (smaller l does imply more computation  hence it is intuitive to expect less
error at the cost of computation). Finally  there is also the expected dependence on the intrinsic
dimensionality d – increasing d implies deteriorating worst-case performance. The theoretical
results are empirically veriﬁed in this section as well.
• In Section 3  we also show that the worst-case search error for Algorithm 1 with a BSP-tree T is
• We present the quantization performance guarantee of a large margin BSP tree in Section 4.
These results indicate that for a given dataset  the best BSP-tree for search is the one with the best
combination of low quantization error and large partition margins. We conclude with this insight
and related unanswered questions in Section 5.
2 Search and vector quantization
Binary space-partitioning trees (or BSP-trees) are hierarchical data structures providing a multi-
resolution view of the dataset indexed. There are several space-partitioning heuristics for a BSP-
tree construction. A tree is constructed by recursively applying a heuristic partition. The most
popular kd-tree uses axis-aligned partitions (Figure 1(a))  often employing a median split along the
coordinate axis of the data in the tree node with the largest spread. The principal-axis tree (PA-tree)
partitions the space at each node at the median along the principal eigenvector of the covariance
matrix of the data in that node [3  4]. Another heuristic partitions the space based on a 2-means
clustering of the data in the node to form the two-means tree (2M-tree) [5  6]. The random-projection
tree (RP-tree) partitions the space by projecting the data along a random standard normal direction
and choosing an appropriate splitting threshold [7] (Figure 1(b)). The max-margin tree (MM-tree) is
built by recursively employing large margin partitions of the data [8] (Figure 1(c)). The unsupervised
large margin splits are usually performed using max-margin clustering techniques [9].
Search. Nearest-neighbor search with a BSP-tree usually involves a depth-ﬁrst branch-and-bound
algorithm which guarantees the search approximation (exact search is a special case of approximate
search with zero approximation) by a depth-ﬁrst traversal of the tree followed by a backtrack up the
tree as required. This makes the tree traversal unpredictable leading to trivial worst-case runtime

1This intuitive connection is widely believed but never rigorously established to the best of our knowledge.

2

guarantees. On the other hand  locality-sensitive hashing [10] based methods approach search in a
different way. After indexing the dataset into hash tables  a query is answered by selecting candidate
points from these hash tables. The candidate set size implies the worst-case search time bound. The
hash table construction guarantees the set size and search approximation. Algorithm 1 uses a BSP-
tree to select a candidate set for a query with defeatist tree search. For a balanced tree on n points 
the candidate set size at depth l is n/2l and the search runtime is O(l + n/2l)  with l ≤ log2 n. For
any choice of the depth l  we present the ﬁrst approximation guarantee for this search process.
Defeatist BSP-tree search has been explored with the spill tree [11]  a binary tree with overlapping
sibling nodes unlike the disjoint nodes in the usual BSP-tree. The search involves selecting the can-
didates in (all) the leaf node(s) which contain the query. The level of overlap guarantees the search
approximation  but this search method lacks any rigorous runtime guarantee; it is hard to bound the
number of leaf nodes that might contain any given query. Dasgupta & Sinha (2013) [12] show that
the probability of ﬁnding the exact nearest neighbor with defeatist search on certain randomized
partition trees (randomized spill trees and RP-trees being among them) is directly proportional to
the relative contrast of the search task [13]  a recently proposed quantity which characterizes the
difﬁculty of a search problem (lower relative contrast makes exact search harder).
Vector Quantization. Recent work by Verma et al.  2009 [2] has established theoretical guarantees
for some of these BSP-trees for the task of vector quantization. Given a set of points S ⊂ RD of n
points  the task of vector quantization is to generate a set of points M ⊂ RD of size k (cid:28) n with
low average quantization error. The optimal quantizer for any region A is given by the mean µ(A)
of the data points lying in that region. The quantization error of the region A is then given by

VS(A) =

1

|A ∩ S|

(cid:107)x − µ(A)(cid:107)2
2  

(1)

(cid:88)

x∈A∩S

(cid:17)
(cid:17)

and the average quantization error of a disjoint partition of region A into Al and Ar is given by:

VS({Al  Ar}) = (|Al ∩ S|VS(Al) + |Ar ∩ S|VS(Ar)) /|A ∩ S|.

(2)
Tree-based structured vector quantization is used for efﬁcient vector quantization – a BSP-tree of
depth log2 k partitions the space containing S into k disjoint regions to produce a k-quantization of
S. The theoretical results for tree-based vector quantization guarantee the improvement in average
quantization error obtained by partitioning any single region (with a single quantizer) into two dis-
joints regions (with two quantizers) in the following form (introduced by Freund et al. (2007) [14]):
Deﬁnition 2.1. For a set S ⊂ RD  a region A par-
titioned into two disjoint regions {Al  Ar}  and a
data-dependent quantity β > 1  the quantization er-
ror improvement is characterized by:

/λ1

.
=

Tree
PA-tree
RP-tree
kd-tree
2M-tree
MM-tree∗ O(ρ) : ρ

Deﬁnition of β
O(2) : 
O(dc)
×
optimal (smallest possible)
/γ2

i=1 λi

.
=

(cid:16)(cid:80)D
(cid:16)(cid:80)D

VS({Al  Ar}) < (1 − 1/β)VS(A).

(3)

i=1 λi

The quantization performance depends inversely on
the data-dependent quantity β – lower β implies bet-
ter quantization. We present the deﬁnition of β for
different BSP-trees in Table 1. For the PA-tree  β
depends on the ratio of the sum of the eigenval-
ues of the covariance matrix of data (A ∩ S) to the
principal eigenvalue. The improvement rate β for
the RP-tree depends on the covariance dimension
of the data in the node A (β = O(dc)) [7]  which
roughly corresponds to the lowest dimensionality of
an afﬁne plane that captures most of the data covari-
ance. The 2M-tree does not have an explicit β but
it has the optimal theoretical improvement rate for a
single partition because the 2-means clustering ob-
jective is equal to |Al|V(Al) + |Ar|V(Ar) and minimizing this objective maximizes β. The 2-
means problem is NP-hard and an approximate solution is used in practice. These theoretical re-
sults are valid under the condition that there are no outliers in A ∩ S. This is characterized as
maxx y∈A∩S (cid:107)x − y(cid:107)2 ≤ ηVS(A) for a ﬁxed η > 0. This notion of the absence of outliers was
ﬁrst introduced for the theoretical analysis of the RP-trees [7]. Verma et al. (2009) [2] describe
outliers as “points that are much farther away from the mean than the typical distance-from-mean”.
In this situation  an alternate type of partition is used to remove these outliers that are farther away

Table 1: β for various trees. λ1  . . .   λD are
the sorted eigenvalues of the covariance matrix
of A ∩ S in descending order  and dc < D is
the covariance dimension of A ∩ S. The results
for PA-tree and 2M-tree are due to Verma et al.
(2009) [2]. The PA-tree result can be improved to
O() from O(2) with an additional assumption
[2]. The RP-tree result is in Freund et al. (2007)
[14]  which also has the precise deﬁnition of dc.
We establish the result for MM-tree in Section 4.
γ is the margin size of the large margin partition.
No such guarantee for kd-trees is known to us.

3

from the mean than expected. For η ≥ 8  this alternate partitioning is guaranteed to reduce the data
diameter (maxx y∈A∩S (cid:107)x − y(cid:107)) of the resulting nodes by a constant fraction [7  Lemma 12]  and
can be used until a region contain no outliers  at which point  the usual hyperplane partition can be
used with their respective theoretical quantization guarantees. The implicit assumption is that the
alternate partitioning scheme is employed rarely.
These results for BSP-tree quantization performance indicate that different heuristics are adaptive
to different properties of the data. However  no existing theoretical result relates this performance
of BSP-trees to their search performance. Making the precise connection between the quantization
performance and the search performance of these BSP-trees is a contribution of this paper.
3 Approximation guarantees for BSP-tree search
In this section  we formally present the data and tree dependent performance guarantees on the
search with BSP-trees using Algorithm 1. The quality of nearest-neighbor search can be quantized
in two ways – (i) distance error and (ii) rank of the candidate neighbor. We present guarantees for
both notions of search error2. For a query q and a set of points S and a neighbor candidate p ∈ S 
distance error (q) =

minr∈S(cid:107)q−r(cid:107) − 1  and rank τ (q) = |{r ∈ S : (cid:107)q − r(cid:107) < (cid:107)q − p(cid:107)}| + 1.

(cid:107)q−p(cid:107)

Algorithm 1 requires the query traversal depth l as an input. The search runtime is O(l + (n/2l)).
The depth can be chosen based on the desired runtime. Equivalently  the depth can be chosen based
on the desired number of candidates m; for a balanced binary tree on a dataset S of n points with leaf
nodes containing a single point  the appropriate depth l = log2 n − (cid:100)log2 m(cid:101). We will be building
on the existing results on vector quantization error [2] to present the worst case error guarantee for
Algorithm 1. We need the following deﬁnitions to precisely state our results:
Deﬁnition 3.1. An ω-balanced split partitioning a region A into disjoint regions {A1  A2} implies
||A1 ∩ S| − |A2 ∩ S|| ≤ ω|A ∩ S|.
For a balanced tree corresponding to recursive median splits  such as the PA-tree and the kd-tree 
ω ≈ 0. Non-zero values of ω (cid:28) 1  corresponding to approximately balanced trees  allow us to
potentially adapt better to some structure in the data at the cost of slightly losing the tree balance.
For the MM-tree (discussed in detail in Section 4)  ω-balanced splits are enforced for any speciﬁed
value of ω. Approximately balanced trees have a depth bound of O(log n) [8  Theorem 3.1]. For
. For the
a tree with ω-balanced splits  the worst case runtime of Algorithm 1 is O
2M-tree  ω-balanced splits are not enforced. Hence the actual value of ω could be high for a 2M-tree.
(p  ∆) = {r ∈ S : (cid:107)p − r(cid:107) < ∆} denote the points in S contained in a ball
Deﬁnition 3.2. Let B(cid:96)2
of radius ∆ around some p ∈ S with respect to the (cid:96)2 metric. The expansion constant of (S  (cid:96)2) is

(cid:1)l
l +(cid:0) 1+ω
(p  ∆)(cid:12)(cid:12) ∀p ∈ S and ∀∆ > 0.

deﬁned as the smallest c ≥ 2 such(cid:12)(cid:12)B(cid:96)2

(p  2∆)(cid:12)(cid:12) ≤ c(cid:12)(cid:12)B(cid:96)2

(cid:16)

(cid:17)

n

2

Bounded expansion constants correspond to growth-restricted metrics [15]. The expansion constant
characterizes the data distribution  and c ∼ 2O(d) where d is the doubling dimension of the set S
(cid:80)
with respect to the (cid:96)2 metric. The relationship is exact for points on a D-dimensional grid (i.e. 
c = Θ(2D)). Equipped with these deﬁnitions  we have the following guarantee for Algorithm 1:
x y∈S (cid:107)x − y(cid:107)2  the BSP
Theorem 3.1. Consider a dataset S ⊂ RD of n points with ψ = 1
tree T built on S and a query q ∈ RD with the following conditions :

(C2) Let T be complete till a depth L <(cid:0)log2

(C1) Let (A ∩ (S ∪ {q})  (cid:96)2) have an expansion constant at most ˜c for any convex set A ⊂ RD.
(C3) Let β∗ correspond to the worst quantization error improvement rate over all splits in T .
(C4) For any node A in the tree T   let maxx y∈A∩S (cid:107)x − y(cid:107)2 ≤ ηVS(A) for a ﬁxed η ≥ 8.

(cid:1) /(1 − log2(1 − ω)) with ω-balanced splits.

For α = 1/(1 − ω)  the upper bound du on the distance of q to the neighbor candidate p returned
by Algorithm 1 with depth l ≤ L is given by
√
2

2n2

n
˜c

(cid:107)q − p(cid:107) ≤ du =

ηψ · (2α)l/2 · exp(−l/2β∗)
(n/(2α)l)1/ log2 ˜c − 2

.

(4)

2The distance error corresponds to the relative error in terms of the actual distance values. The rank is one
more than the number of points in S which are better neighbor candidates than p. The nearest-neighbor of q
has rank 1 and distance error 0. The appropriate notion of error depends on the search application.

4

neighbor distance is inversely proportional to(cid:0)n/(2α)l(cid:1)1/ log2 ˜c  implying deteriorating bounds du

Now η is ﬁxed  and ψ is ﬁxed for a dataset S. Then  for a ﬁxed ω  this result implies that between
two types of BSP-trees on the same set and the same query  Algorithm 1 has a better worst-case guar-
antee on the candidate-neighbor distance for the tree with better quantization performance (smaller
β∗). Moreover  for a particular tree with β∗ ≥ log2 e  du is non-decreasing in l. This is expected
because as we traverse down the tree  we can never reduce the candidate neighbor distance. At the
root level (l = 0)  the candidate neighbor is the nearest-neighbor. As we descend down the tree 
the candidate neighbor distance will worsen if a tree split separates the query from its closer neigh-
bors. This behavior is implied in Equation (4). For a chosen depth l in Algorithm 1  the candidate
with increasing ˜c. Since log2 ˜c ∼ O(d)  larger intrinsic dimensionality implies worse guarantees as
expected from the curse of dimensionality. To prove Theorem 3.1  we use the following result:
Lemma 3.1. Under the conditions of Theorem 3.1  for any node A at a depth l in the BSP-tree T
on S  VS(A) ≤ ψ (2/(1 − ω))l exp(−l/β∗).
This result is obtained by recursively applying the quantization error improvement in Deﬁnition 2.1
over l levels of the tree (the proof is in Appendix A).
Proof of Theorem 3.1. Consider the node A at depth l in the tree containing q  and let m = |A ∩ S|.
Let D = maxx y∈A∩S (cid:107)x − y(cid:107)  let d = minx∈A∩S (cid:107)q − x(cid:107)  and let B(cid:96)2
(q  ∆) = {x ∈ A ∩ (S ∪
{q}) : (cid:107)q − x(cid:107) < ∆}. Then  by the Deﬁnition 3.2 and condition C1 
(q  d)| = ˜clog2(cid:100) D+d

d (cid:101)|B(cid:96)2
where the equality follows from the fact that B(cid:96)2
this above gives us m1/ log2 ˜c ≤ (D/d) + 2. By condition C2  m1/ log2 ˜c > 2. Hence we have

(q  D + d)(cid:12)(cid:12) ≥ m. Using
d ≤ D/(m1/ log2 ˜c − 2). By construction and condition C4  D ≤(cid:112)ηVS(A). Now m ≥ n/(2α)l.

(q  d) = {q}. Now(cid:12)(cid:12)B(cid:96)2

(q  D + d)(cid:12)(cid:12) ≤ ˜clog2(cid:100) D+d

d (cid:101) ≤ ˜clog2( D+2d

(cid:12)(cid:12)B(cid:96)2

) 

d

q )(cid:101)  where d∗

q = minr∈S (cid:107)q − r(cid:107).

(q  du)| ≤ ˜c(cid:100)log2(du/d∗

q)|. Using the fact that |B(cid:96)2

Plugging this above and utilizing Lemma 3.1 gives us the statement of Theorem 3.1.
Nearest-neighbor search error guarantees. Equipped with the bound on the candidate-neighbor
distance  we bound the worst-case nearest-neighbor search errors as follows:
Corollary 3.1. Under the conditions of Theorem 3.1  for any query q at a desired depth l ≤ L
q) − 1  and the rank τ (q) is
in Algorithm 1  the distance error (q) is bounded as (q) ≤ (du/d∗
bounded as τ (q) ≤ ˜c(cid:100)log2(du/d∗
Proof. The distance error bound follows from the deﬁnition of distance error. Let R = {r ∈
S : (cid:107)q − r(cid:107) < du}. By deﬁnition  τ (q) ≤ |R| + 1. Let B(cid:96)2
(q  ∆) = {x ∈ (S ∪ {q}) : (cid:107)q − x(cid:107) <
(q  du)| = |R| + 1 ≥ τ (q). From Deﬁnition
∆}. Since B(cid:96)2
q)| =
3.2 and Condition C1  |B(cid:96)2
(q  d∗
|{q}| = 1 gives us the upper bound on τ (q).
The upper bounds on both forms of search error are directly proportional to du. Hence  the BSP-
tree with better quantization performance has better search performance guarantees  and increasing
traversal depth l implies less computation but worse performance guarantees. Any dependence of
this approximation guarantee on the ambient data dimensionality is subsumed by the dependence
on β∗ and ˜c. While our result bounds the worst-case performance of Algorithm 1  an average case
performance guarantee on the distance error is given by Eq (q) ≤ du Eq

(q  du) contains q and R  and q /∈ S  |B(cid:96)2
q )(cid:101)|B(cid:96)2

(cid:1)−1  and on the rank

q)(cid:17)
is given by Eq τ (q) ≤ ˜c(cid:100)log2 du(cid:101)(cid:16)Eq c−(log2 d∗

  since the expectation is over the queries q and du
does not depend on q. For the purposes of relative comparison among BSP-trees  the bounds on the
expected error depend solely on du since the term within the expectation over q is tree independent.
Dependence of the nearest-neighbor search error on the partition margins. The search error
bounds in Corollary 3.1 depend on the true nearest-neighbor distance d∗
q of any query q of which we
have no prior knowledge. However  if we partition the data with a large margin split  then we can
say that either the candidate neighbor is the true nearest-neighbor of q or that d∗
q is greater than the
size of the margin. We characterize the inﬂuence of the margin size with the following result:
Corollary 3.2. Consider the conditions of Theorem 3.1 and a query q at a depth l ≤ L in Algorithm
1. Further assume that γ is the smallest margin size on both sides of any partition in the tree T . Then
the distance error is bounded as (q) ≤ du/γ − 1  and the rank is bounded as τ (q) ≤ ˜c(cid:100)log2(du/γ)(cid:101).
This result indicates that if the split margins in a BSP-tree can be increased without adversely affect-
ing its quantization performance  the BSP-tree will have improved nearest-neighbor error guarantees

(q  d∗

(cid:0)1/d∗

q

5

for the Algorithm 1. This motivated us to consider the max-margin tree [8]  a BSP-tree that explicitly
maximizes the margin of the split for every split in the tree.
Explanation of the conditions in Theorem 3.1. Condition C1 implies that for any convex set
A ⊂ RD  ((A ∩ (S ∪ {q}))  (cid:96)2) has an expansion constant at most ˜c. A bounded ˜c implies that no
subset of (S ∪ {q})  contained in a convex set  has a very high expansion constant. This condition
implies that ((S ∪{q})  (cid:96)2) also has an expansion constant at most ˜c (since (S ∪{q}) is contained in
its convex hull). However  if (S ∪ {q}  (cid:96)2) has an expansion constant c  this does not imply that the
data lying within any convex set has an expansion constant at most c. Hence a bounded expansion
constant assumption for (A∩(S∪{q})  (cid:96)2) for every convex set A ⊂ RD is stronger than a bounded
expansion constant assumption for (S ∪ {q}  (cid:96)2)3. Condition C2 ensures that the tree is complete
so that for every query q and a depth l ≤ L  there exists a large enough tree node which contains q.
Condition C3 gives us the worst quantization error improvement rate over all the splits in the tree.
Condition C4 implies that the squared data diameter of any node A (maxx y∈A∩S (cid:107)x − y(cid:107)2) is
within a constant factor of its quantization error VS(A). This refers to the assumption that the node
A contains no outliers as described in Section 3 and only hyperplane partitions are used and their
respective quantization improvement guarantees presented in Section 2 (Table 1) hold. By placing
condition C4  we ignore the alternate partitioning scheme used to remove outliers for simplicity
of analysis.
If we allow a small fraction of the partitions in the tree to be this alternate split  a
similar result can be obtained since the alternate split is the same for all BSP-tree. For two different
kinds of hyperplane splits  if alternate split is invoked the same number of times in the tree  the
difference in their worst-case guarantees for both the trees would again be governed by their worst-
case quantization performance (β∗). However  for any ﬁxed η  a harder question is whether one
type of hyperplane partition violates the inlier condition more often than another type of partition 
resulting in more alternate partitions. And we do not yet have a theoretical answer for this4.
Empirical validation. We examine our theoretical results with 4 datasets – OPTDIGITS (D = 64 
n = 3823  1797 queries)  TINY IMAGES (D = 384  n = 5000  1000 queries)  MNIST (D =
784  n = 6000  1000 queries)  IMAGES (D = 4096  n = 500  150 queries). We consider the
following BSP-trees: kd-tree  random-projection (RP) tree  principal axis (PA) tree  two-means (2M)
tree and max-margin (MM) tree. We only use hyperplane partitions for the tree construction. This is
S(A) > ηVS(A)) can be computationally
because  ﬁrstly  the check for the presence of outliers (∆2
expensive for large n  and  secondly  the alternate partition is mostly for the purposes of obtaining
theoretical guarantees. The implementation details for the different tree constructions are presented
in Appendix C. The performance of these BSP-trees are presented in Figure 2. Trees with missing
data points for higher depth levels (for example  kd-tree in Figure 2(a) and 2M-tree in Figures 2 (b)
& (c)) imply that we were unable to grow complete BSP-trees beyond that depth.
The quantization performance of the 2M-tree  PA-tree and MM-tree are signiﬁcantly better than the
performance of the kd-tree and RP-tree and  as suggested by Corollary 3.1  this is also reﬂected in
their search performance. The MM-tree has comparable quantization performance to the 2M-tree
and PA-tree. However  in the case of search  the MM-tree outperforms PA-tree in all datasets. This
can be attributed to the large margin partitions in the MM-tree. The comparison to 2M-tree is not
as apparent. The MM-tree and PA-tree have ω-balanced splits for small ω enforced algorithmically 
resulting in bounded depth and bounded computation of O(l + n(1 + ω)l/2l) for any given depth
l. No such balance constraint is enforced in the 2-means algorithm  and hence  the 2M-tree can be
heavily unbalanced. The absence of complete BSP 2M-tree beyond depth 4 and 6 in Figures 2 (b)
& (c) respectively is evidence of the lack of balance in the 2M-tree. This implies possibly more
computation and hence lower errors. Under these conditions  the MM-tree with an explicit balance
constraint performs comparably to the 2M-tree (slightly outperforming in 3 of the 4 cases) while
still maintaining a balanced tree (and hence returning smaller candidate sets on average).

3A subset of a growth-restricted metric space (S  (cid:96)2) may not be growth-restricted. However  in our case 
we are not considering all subsets; we only consider subsets of the form (A ∩ S) where A ⊂ RD is a convex
set. So our condition does not imply that all subsets of (S  (cid:96)2) are growth-restricted.

4We empirically explore the effect of the tree type on the violation of the inlier condition (C4) in Appendix
B. The results imply that for any ﬁxed value of η  almost the same number of alternate splits would be invoked
for the construction of different types of trees on the same dataset. Moreover  with η ≥ 8  for only one of the
datasets would a signiﬁcant fraction of the partitions in the tree (of any type) need to be the alternate partition.

6

(a) OPTDIGITS

(b) TINY IMAGES

(c) MNIST

(d) IMAGES

Figure 2: Performance of BSP-trees with increasing traversal depth. The top row corresponds to quanti-
zation performance of existing trees and the bottom row presents the nearest-neighbor error (in terms of mean
rank τ of the candidate neighbors (CN)) of Algorithm 1 with these trees. The nearest-neighbor search error
graphs are also annotated with the mean distance-error of the CN (please view in color).
4 Large margin BSP-tree
We established that the search error depends on the quantization performance and the partition mar-
gins of the tree. The MM-tree explicitly maximizes the margin of every partition and empirical
results indicate that it has comparable performance to the 2M-tree and PA-tree in terms of the quan-
tization performance. In this section  we establish a theoretical guarantee for the MM-tree quanti-
zation performance. The large margin split in the MM-tree is obtained by performing max-margin
clustering (MMC) with 2 clusters. The task of MMC is to ﬁnd the optimal hyperplane (w∗  b∗) from
the following optimization problem5 given a set of points S = {x1  x2  . . .   xm} ⊂ RD:

min
w b ξi

(cid:107)w(cid:107)2

2 + C

1
2

m(cid:88)

i=1

ξi

s.t.

|(cid:104)w  xi(cid:105) + b| ≥ 1 − ξi  ξi ≥ 0 ∀i = 1  . . .   m

sgn((cid:104)w  xi(cid:105) + b) ≤ ωm.

−ωm ≤ m(cid:88)

i=1

(5)

(6)

(7)

(8)

MMC ﬁnds a soft max-margin split in the data to obtain two clusters separated by a large (soft)
margin. The balance constraint (Equation (7)) avoids trivial solutions and enforces an ω-balanced
split. The margin constraints (Equation (6)) enforce a robust separation of the data. Given a solution
to the MMC  we establish the following quantization error improvement rate for the MM-tree:
Theorem 4.1. Given a set of points S ⊂ RD and a region A containing m points  consider an
ω-balanced max-margin split (w  b) of the region A into {Al  Ar} with at most αm support vectors
and a split margin of size γ = 1/(cid:107)w(cid:107). Then the quantization error improvement is given by:

1 − γ2 (1 − α)2(cid:16) 1−ω

(cid:80)D

1+ω

(cid:17)

VS(A) 

i=1 λi

VS({Al  Ar}) ≤

where λ1  . . .   λD are the eigenvalues of the covariance matrix of A ∩ S.
The result indicates that larger margin sizes (large γ values) and a smaller number of support vectors
(small α) implies better quantization performance. Larger ω implies smaller improvement  but ω is
generally restricted algorithmically in MMC. If γ = O(
λ1) then this rate matches the best possible
quantization performance of the PA-tree (Table 1). We do assume that we have a feasible solution to
the MMC problem to prove this result. We use the following result to prove Theorem 4.1:
Proposition 4.1. [7  Lemma 15] Give a set S  for any partition {A1  A2} of a set A 

√

VS(A) − VS({A1  A2}) =

|A ∩ S|2
where µ(A) is the centroid of the points in the region A.

|A1 ∩ S||A2 ∩ S|

(cid:107)µ(A1) − µ(A2)(cid:107)2  

(9)

5This is an equivalent formulation [16] to the original form of max-margin clustering proposed by Xu et al.
(2005) [9]. The original formulation also contains the labels yis and optimizes over it. We consider this form
of the problem since it makes our analysis easier to follow.

7

m(cid:88)

This result [7] implies that the improvement in the quantization error depends on the distance be-
tween the centroids of the two regions in the partition.
Proof of Theorem 4.1. For a feasible solution (w  b  ξi|i=1 ... m) to the MMC problem 

Let ˜xi = (cid:104)w  xi(cid:105)+b and mp = |{i : ˜xi > 0}| and mn = |{i : ˜xi ≤ 0}| and ˜µp = ((cid:80)
and ˜µn = ((cid:80)

|(cid:104)w  xi(cid:105) + b| ≥ m − m(cid:88)
i : ˜xi≤0 ˜xi)/mn. Then mp ˜µp − mn ˜µn ≥ m −(cid:80)

Without loss of generality  we assume that mp ≥ mn. Then the balance constraint (Equation (7))
tells us that mp ≤ m(1+ ω)/2 and mn ≥ m(1− ω)/2. Then ˜µp− ˜µn + ω(˜µp + ˜µn) ≥ 2− 2
i ξi.
Since ˜µp > 0 and µn ≤ 0  |˜µp + ˜µn| ≤ (˜µp − ˜µn). Hence (1 + ω)(˜µp − ˜µn) ≥ 2 − 2
i ξi. For
an unsupervised split  the data is always separable since there is no misclassiﬁcation. This implies
that ξ∗

i : ˜xi>0 ˜xi)/mp

i ≤ 1∀i. Hence 

(cid:80)

(cid:80)

i ξi.

ξi.

i=1

i=1

m

m

(cid:18)

(cid:19)

(cid:18) 1 − α

(cid:19)

1 + ω

˜µp − ˜µn ≥

2 − 2
m

|{i : ξi > 0}|

/(1 + ω) ≥ 2

 

(10)

.

1

i=1

λi.

1 + ω

1 + ω

(cid:19)

x∈A∩S

(cid:19)2

|A ∩ S|

VS(A) =

= γ2 (1 − α)2

(cid:18) 1 − α

(cid:107)x − µ(A)(cid:107)2 = tr (Cov(A ∩ S)) =

VS(A) − VS({Al  Ar}) ≥ (1 − ω2)γ2

since the term |{i : ξi > 0}| corresponds to the number of support vectors in the solution.
Cauchy-Schwartz implies that (cid:107)µ(Al) − µ(Ar)(cid:107) ≥ |(cid:104)w  µ(Al) − µ(Ar)(cid:105)|/(cid:107)w(cid:107) = (˜µp − ˜µn)γ 
since ˜µn = (cid:104)w  µ(Al)(cid:105) + b and ˜µp = (cid:104)w  µ(Ar)(cid:105) + b. From Equation (10)  we can say
that (cid:107)µ(Al) − µ(Ar)(cid:107)2 ≥ 4γ2 (1 − α)2 / (1 + ω)2. Also  for ω-balanced splits  |Al||Ar| ≥
(1 − ω2)m2/4. Combining these into Equation (9) from Proposition 4.1  we have

(11)
Let Cov(A ∩ S) be the covariance matrix of the data contained in region A and λ1  . . .   λD be the
(cid:88)
eigenvalues of Cov(A ∩ S). Then  we have:

(cid:18) 1 − ω
D(cid:88)
Then dividing Equation (11) by VS(A) gives us the statement of the theorem.
5 Conclusions and future directions
Our results theoretically verify that BSP-trees with better vector quantization performance and large
partition margins do have better search performance guarantees as one would expect. This means
that the best BSP-tree for search on a given dataset is the one with the best combination of good
quantization performance (low β∗ in Corollary 3.1) and large partition margins (large γ in Corollary
3.2). The MM-tree and the 2M-tree appear to have the best empirical performance in terms of the
search error. This is because the 2M-tree explicitly minimizes β∗ while the MM-tree explicitly
maximizes γ (which also implies smaller β∗ by Theorem 4.1). Unlike the 2M-tree  the MM-tree
explicitly maintains an approximately balanced tree for better worst-case search time guarantees.
However  the general dimensional large margin partitions in the MM-tree construction can be quite
expensive. But the idea of large margin partitions can be used to enhance any simpler space partition
heuristic – for any chosen direction (such as along a coordinate axis or along the principal eigen-
vector of the data covariance matrix)  a one dimensional large margin split of the projections of the
points along the chosen direction can be obtained very efﬁciently for improved search performance.
This analysis of search could be useful beyond BSP-trees. Various heuristics have been developed
to improve locality-sensitive hashing (LSH) [10]. The plain-vanilla LSH uses random linear projec-
tions and random thresholds for the hash-table construction. The data can instead be projected along
the top few eigenvectors of the data covariance matrix. This was (empirically) improved upon by
learning an orthogonal rotation of the projected data to minimize the quantization error of each bin in
the hash-table [17]. A nonlinear hash function can be learned using a restricted Boltzmann machine
[18]. If the similarity graph of the data is based on the Euclidean distance  spectral hashing [19]
uses a subset of the eigenvectors of the similarity graph Laplacian. Semi-supervised hashing [20]
incorporates given pairwise semantic similarity and dissimilarity constraints. The structural SVM
framework has also been used to learn hash functions [21]. Similar to the choice of an appropriate
BSP-tree for search  the best hashing scheme for any given dataset can be chosen by considering the
quantization performance of the hash functions and the margins between the bins in the hash tables.
We plan to explore this intuition theoretically and empirically for LSH based search schemes.

8

References
[1] J. H. Friedman  J. L. Bentley  and R. A. Finkel. An Algorithm for Finding Best Matches in

Logarithmic Expected Time. ACM Transactions in Mathematical Software  1977.

[2] N. Verma  S. Kpotufe  and S. Dasgupta. Which Spatial Partition Trees are Adaptive to Intrinsic
Dimension? In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence  2009.
[3] R.F. Sproull. Reﬁnements to Nearest-Neighbor Searching in k-dimensional Trees. Algorith-

mica  1991.

[4] J. McNames. A Fast Nearest-Neighbor Algorithm based on a Principal Axis Search Tree. IEEE

Transactions on Pattern Analysis and Machine Intelligence  2001.

[5] K. Fukunaga and P. M. Nagendra. A Branch-and-Bound Algorithm for Computing k-Nearest-

Neighbors. IEEE Transactions on Computing  1975.

[6] D. Nister and H. Stewenius. Scalable Recognition with a Vocabulary Tree. In IEEE Conference

on Computer Vision and Pattern Recognition  2006.

[7] S. Dasgupta and Y. Freund. Random Projection trees and Low Dimensional Manifolds. In

Proceedings of ACM Symposium on Theory of Computing  2008.

[8] P. Ram  D. Lee  and A. G. Gray. Nearest-neighbor Search on a Time Budget via Max-Margin

Trees. In SIAM International Conference on Data Mining  2012.

[9] L. Xu  J. Neufeld  B. Larson  and D. Schuurmans. Maximum Margin Clustering. Advances in

Neural Information Processing Systems  2005.

[10] P. Indyk and R. Motwani. Approximate Nearest Neighbors: Towards Removing the Curse of

Dimensionality. In Proceedings of ACM Symposium on Theory of Computing  1998.

[11] T. Liu  A. W. Moore  A. G. Gray  and K. Yang. An Investigation of Practical Approximate

Nearest Neighbor Algorithms. Advances in Neural Information Proceedings Systems  2005.

[12] S. Dasgupta and K. Sinha. Randomized Partition Trees for Exact Nearest Neighbor Search. In

Proceedings of the Conference on Learning Theory  2013.

[13] J. He  S. Kumar and S. F. Chang. On the Difﬁculty of Nearest Neighbor Search. In Proceedings

of the International Conference on Machine Learning  2012.

[14] Y. Freund  S. Dasgupta  M. Kabra  and N. Verma. Learning the Structure of Manifolds using

Random Projections. Advances in Neural Information Processing Systems  2007.

[15] D. R. Karger and M. Ruhl. Finding Nearest Neighbors in Growth-Restricted Metrics.

Proceedings of ACM Symposium on Theory of Computing  2002.

In

[16] B. Zhao  F. Wang  and C. Zhang. Efﬁcient Maximum Margin Clustering via Cutting Plane

Algorithm. In SIAM International Conference on Data Mining  2008.

[17] Y. Gong and S. Lazebnik. Iterative Quantization: A Procrustean Approach to Learning Binary

Codes. In IEEE Conference on Computer Vision and Pattern Recognition  2011.

[18] R. Salakhutdinov and G. Hinton. Learning a Nonlinear Embedding by Preserving Class Neigh-

bourhood Structure. In Artiﬁcial Intelligence and Statistics  2007.

[19] Y. Weiss  A. Torralba  and R. Fergus. Spectral Hashing. Advances of Neural Information

Processing Systems  2008.

[20] J. Wang  S. Kumar  and S. Chang. Semi-Supervised Hashing for Scalable Image Retrieval. In

IEEE Conference on Computer Vision and Pattern Recognition  2010.

[21] M. Norouzi and D. J. Fleet. Minimal Loss Hashing for Compact Binary Codes. In Proceedings

of the International Conference on Machine Learning  2011.

[22] S. Lloyd. Least Squares Quantization in PCM. IEEE Transactions on Information Theory 

28(2):129–137  1982.

9

,Parikshit Ram
Alexander Gray
Guillaume Papa
Aurélien Bellet
Stephan Clémençon
Xiaoxiao Guo
Hui Wu
Yu Cheng
Steven Rennie
Gerald Tesauro
Rogerio Feris
Xuanyi Dong
Yi Yang