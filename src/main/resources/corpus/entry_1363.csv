2019,Deep Learning without Weight Transport,Current algorithms for deep learning probably cannot run in the brain because
they rely on weight transport  where forward-path neurons transmit their synaptic
weights to a feedback path  in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights  but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms — a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 — both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks  without weight transport or complex wiring. Tested on the ImageNet visual-recognition task  these mechanisms outperform both feedback alignment and the newer sign-symmetry method  and nearly match backprop  the standard algorithm of deep learning  which uses weight transport.,Deep Learning without Weight Transport

Mohamed Akrout

University of Toronto  Triage

Collin Wilson

University of Toronto

Peter C. Humphreys

DeepMind

Timothy Lillicrap

DeepMind  University College London

Douglas Tweed

University of Toronto  York University

Abstract

Current algorithms for deep learning probably cannot run in the brain because
they rely on weight transport  where forward-path neurons transmit their synaptic
weights to a feedback path  in a way that is likely impossible biologically. An algo-
rithm called feedback alignment achieves deep learning without weight transport by
using random feedback weights  but it performs poorly on hard visual-recognition
tasks. Here we describe two mechanisms — a neural circuit called a weight mirror
and a modiﬁcation of an algorithm proposed by Kolen and Pollack in 1994 — both
of which let the feedback path learn appropriate synaptic weights quickly and accu-
rately even in large networks  without weight transport or complex wiring. Tested
on the ImageNet visual-recognition task  these mechanisms learn almost as well as
backprop (the standard algorithm of deep learning  which uses weight transport)
and they outperform feedback alignment and another  more-recent transport-free
algorithm  the sign-symmetry method.

1

Introduction

The algorithms of deep learning were devised to run on computers  yet in many ways they seem
suitable for brains as well; for instance  they use multilayer networks of processing units  each with
many inputs and a single output  like networks of neurons. But current algorithms can’t quite work
in the brain because they rely on the error-backpropagation algorithm  or backprop  which uses
weight transport: each unit multiplies its incoming signals by numbers called weights  and some
units transmit their weights to other units. In the brain  it is the synapses that perform this weighting 
but there is no known pathway by which they can transmit their weights to other neurons or to other
synapses in the same neuron [1  2].
Lillicrap et al. [3] offered a solution in the form of feedback alignment  a mechanism that lets
deep networks learn without weight transport  and they reported good results on several tasks. But
Bartunov et al. [4] and Moskovitz et al. [5] have found that feedback alignment does not scale to
hard visual recognition problems such as ImageNet [6].
Xiao et al. [7] achieved good performance on ImageNet using a sign-symmetry algorithm in which
only the signs of the forward and feedback weights  not necessarily their values  must correspond  and
they suggested a mechanism by which that correspondence might be set up during brain development.
Krotov and Hopﬁeld [8] and Guerguiev et al. [9] have explored other approaches to deep learning
without weight transport  but so far only in smaller networks and tasks.
Here we propose two different approaches that learn ImageNet about as well as backprop does  with
no need to initialize forward and feedback matrices so their signs agree. We describe a circuit called
a weight mirror and a version of an algorithm proposed by Kolen and Pollack in 1994 [10]  both of
which let initially random feedback weights learn appropriate values without weight transport.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

There are of course other questions about the biological implications of deep-learning algorithms 
some of which we touch on in Appendix C  but in this paper our main concern is with weight
transport.

2 The weight-transport problem

In a typical deep-learning network  some signals ﬂow along a forward path through multiple layers of
processing units from the input layer to the output  while other signals ﬂow back from the output layer
along a feedback path. Forward-path signals perform inference (e.g. they try to infer what objects are
depicted in a visual input) while the feedback path conveys error signals that guide learning. In the
forward path  signals ﬂow according to the equation

yl+1 = φ(Wl+1 yl + bl+1)

(1)
Here yl is the output signal of layer l  i.e. a vector whose i-th element is the activity of unit i in layer
l. Equation 1 shows how the next layer l + 1 processes its input yl: it multiplies yl by the forward
weight matrix Wl+1  adds a bias vector bl+1  and puts the sum through an activation function φ.
Interpreted as parts of a real neuronal network in the brain  the y’s might be vectors of neuronal ﬁring
rates  or some function of those rates  Wl+1 might be arrays of synaptic weights  and bl+1 and φ
bias currents and nonlinearities in the neurons.
In the feedback path  error signals δ ﬂow through the network from its output layer according to the
error-backpropagation [11] or backprop equation:

δl = φ(cid:48)(yl) WT

l+1 δl+1

(2)
Here φ(cid:48) is the derivative of the activation function φ from equation (1)  which can be computed from
yl. So feedback signals pass layer by layer through weights WT
l . Interpreted as a structure in the
brain  the feedback path might be another set of neurons  distinct from those in the forward path  or
the same set of neurons might carry inference signals in one direction and errors in the other [12  13].
Either way  we have the problem that the same weight matrix Wl appears in the forward equation (1)
and then again  transposed  in the feedback equation (2)  whereas in the brain  the synapses in the
forward and feedback paths are physically distinct  with no known way to coordinate themselves so
one set is always the transpose of the other [1  2].

3 Feedback alignment

In feedback alignment  the problem is avoided by replacing the transposed Wl’s in the feedback path
by random  ﬁxed (non-learning) weight matrices Bl 

(3)

δl = φ(cid:48)(yl) Bl+1 δl+1

∆Wl+1 = −ηW δl+1 yT

These feedback signals δ drive learning in the forward weights W by the rule

l

(4)
where ηW is a learning-rate factor. As shown in [3]  equations (1)  (3)  and (4) together drive the
forward matrices Wl to become roughly proportional to transposes of the feedback matrices Bl.
That rough transposition makes equation (3) similar enough to the backprop equation (2) that the
network can learn simple tasks as well as backprop does.
Can feedback alignment be augmented to handle harder tasks? One approach is to adjust the feedback
weights Bl as well as the forward weights Wl  to improve their agreement. Here we show two
mechanisms by which that adjustment can be achieved quickly and accurately in large networks
without weight transport.

4 Weight mirrors

4.1 Learning the transpose

The aim here is to adjust an initially random matrix B so it becomes proportional to the transpose
of another matrix W without weight transport  i.e. given only the input and output vectors x and

2

y = Wx (for this explanation  we neglect the activation function φ). We observe that E(cid:2)x yT(cid:3) =
E(cid:2)x xT WT(cid:3) = E(cid:2)x xT(cid:3)WT . In the simplest case  if the elements of x are independent and
zero-mean with equal variance  σ2   it follows that E(cid:2)x yT(cid:3) = σ2WT . Therefore we can push B

steadily in the direction σ2W using this transposing rule 

∆B = ηB x yT

(5)

So B integrates a signal that is proportional to WT on average. Over time  this integration may
cause the matrix norm (cid:107)B(cid:107) to increase  but if we add a mechanism to keep the norm small — such as
weight decay or synaptic scaling [14–16] — then the initial  random values in B shrink away  and B
converges to a scalar multiple of WT (see Appendix A for an account of this learning rule in terms
of gradient descent).

4.2 A circuit for transposition

Figure 1 shows one way the learning rule (5) might be implemented in a neural network. This network
alternates between two modes: an engaged mode  where it receives sensory inputs and adjusts its
forward weights to improve its inference  and a mirror mode  where its neurons discharge noisily
and adjust the feedback weights so they mimic the forward ones. Biologically  these two modes may
correspond to wakefulness and sleep  or simply to practicing a task and then setting it aside for a
moment.

Figure 1: Network modes for weight mirroring. Both panels show the same two-layer section of
a network. In both modes  the three neurons in layer l of the forward path (
) send their output
signal yl through the weight array Wl+1 (and other processing shown in equation (1)) to yield the
next-layer signal yl+1. And in the feedback path (
)  the two neurons in layer l + 1 send their signal
δl+1 through weight array Bl+1 to yield δl  as in (3). The ﬁgure omits the biases b  nonlinearities φ 
and  in the top panel  the projections that convey yl to the δl cells  allowing them to compute the
factor φ(cid:48)(yl) in equation (3). a) In engaged mode  cross-projections (
) convey the feedback signals
δ to the forward-path cells  so they can adjust the forward weights W using learning rule (4). b) In
mirror mode  one layer of forward cells  say layer l  ﬁres noisily. Its signal yl still passes through
Wl+1 to yield yl+1  but now the blue cross-projections (
) control ﬁring in the feedback path  so
δl = yl and δl+1 = yl+1  and the δl neurons adjust the feedback weights Bl+1 using learning
rule (7). We call the circuit yl  yl+1  δl+1  δl a weight mirror because it makes the weight array
Bl+1 resemble WT

l+1.

In mirror mode  the forward-path neurons in each layer l  carrying the signal yl  project strongly to
layer l of the feedback path — strongly enough that each signal δl of the feedback path faithfully
mimics yl  i.e.

δl = yl

(6)

Also in mirror mode  those forward-path signals yl are noisy. Multiple layers may ﬁre at once  but the
process is simpler to explain in the case where they take turns  with just one layer l driving forward-
path activity at any one time. In that case  all the cells of layer l ﬁre randomly and independently 
so their output signal yl has zero-mean and equal variance σ2. That signal passes through forward
weight matrix Wl+1 and activation function φ to yield yl+1 = φ(Wl+1 yl + bl). By equation (6) 

3

δl+1δ lylyl +1WlWl+2Wl+1BlBl+1 Bl2+ aδl+1δ lylyl +1WlWl+2Wl+1BlBl+1 Bl2+ bsignals yl and yl+1 are transmitted to the feedback path. Then the layer-l feedback cells adjust their
weights Bl+1 by Hebbian learning 

∆Bl+1 = ηB δl δT

l+1

(7)

This circuitry and learning rule together constitute the weight mirror.

4.3 Why it works

To see that (7) approximates the transposing rule (5)  notice ﬁrst that

δl δT

l+1 = yl yT

(8)
We will assume  for now  that the variance σ2 of yl is small enough that Wl+1 yl + bl+1 stays in a
roughly afﬁne range of φ  and that the diagonal elements of the derivative matrix φ(cid:48)(bl+1) are all
roughly similar to each other  so the matrix is approximately of the form φ(cid:48)
s is a positive
scalar and I is the identity. Then

l+1 = yl φ(Wl+1 yl + bl+1)T

sI  where φ(cid:48)

Therefore

and so

φ(Wl+1 yl + bl+1) ≈ φ(cid:48)(bl+1) Wl+1 yl + φ(bl+1)

≈ φ(cid:48)

l+1 ≈ yl

(cid:2) yT
(cid:0)E(cid:2)ylyT
(cid:3) ≈ ηB
(cid:3)WT
(cid:3)WT
= ηB E(cid:2)ylyT

s Wl+1 yl + φ(bl+1)
l+1 φ(cid:48)

s + φ(bl+1)T (cid:3)
s + E(cid:2)yl

l+1φ(cid:48)
l+1φ(cid:48)

l WT

s

l

l

(cid:3)φ(bl+1)T(cid:1)

δl δT

E(cid:2)∆Bl+1

= ηB σ2φ(cid:48)

sWT

l+1

(9)

(10)

(11)

Hence the weight matrix Bl+1 integrates a teaching signal (7) which approximates  on average  a
l+1. As in (5)  this integration may drive up the matrix norm (cid:107)Bl+1(cid:107) 
positive scalar multiple of WT
but if we add a mechanism such as weight decay to keep the norm small [15  16] then Bl+1 evolves
toward a reasonable-sized positive multiple of WT
We get a stronger result if we suppose that neurons are capable of bias-blocking — of closing off
their bias currents when in mirror mode  or preventing their inﬂuence on the axon hillock. Then

l+1.

E(cid:2)∆Bl+1

(cid:3) ≈ ηB σ2φ(cid:48)(0)WT

l+1

So again  Bl+1 comes to approximate a positive scalar multiple of WT
derivative around 0  but we no longer need to assume that φ(cid:48)(bl+1) ≈ φ(cid:48)
sI.
In one respect the weight mirror resembles difference target propagation [4]  because both mechanisms
shape the feedback path layer by layer  but target propagation learns layer-wise autoencoders (though
see [17])  and uses feedback weights to propagate targets rather than gradients.

(12)
l+1  so long as φ has a positive

5 The Kolen-Pollack algorithm

5.1 Convergence through weight decay

Kolen and Pollack [10] observed that we don’t have to transport weights if we can transport changes
in weights. Consider two synapses  W in the forward path and B in the feedback path (written
without boldface because for now we are considering individual synapses  not matrices). Suppose W
and B are initially unequal  but at each time step t they undergo identical adjustments A(t) and apply
identical weight-decay factors λ  so

∆W (t) = A(t) − λW (t)

(13)

and

(14)
Then W (t + 1)− B(t + 1) = W (t) + ∆W (t)− B(t)− ∆B(t) = W (t)− B(t)− λ[W (t)− B(t)] =
(1 − λ)[W (t) − B(t)] = (1 − λ)t+1[W (0) − B(0)]  and so with time  if 0 < λ < 1  W and B will
converge.

∆B(t) = A(t) − λB(t)

4

But biologically  it is no more feasible to transport weight changes than weights  and Kolen and
Pollack do not say how their algorithm might run in the brain. Their ﬂow diagram (Figure 2 in their
paper) is not at all biological: it shows weight changes being calculated at one locus and then traveling
to distinct synapses in the forward and feedback paths. In the brain  changes to different synapses are
almost certainly calculated separately  within the synapses themselves. But it is possible to implement
Kolen and Pollack’s method in a network without transporting weights or weight changes.

5.2 A circuit for Kolen-Pollack learning

The standard  forward-path learning rule (4) says that the matrix Wl+1 adjusts itself based on a
product of its input vector yl and a teaching vector δl+1. More speciﬁcally  each synapse Wl+1 ij
adjusts itself based on its own scalar input yl j and the scalar teaching signal δl+1 i sent to its neuron
from the feedback path.
We propose a reciprocal arrangement  where synapses in the feedback path adjust themselves based
on their own inputs and cell-speciﬁc  scalar teaching signals from the forward path 

If learning rates and weight decay agree in the forward and feedback paths  we get

∆Bl+1 = −η yl δT

l+1

and

i.e.

∆Wl+1 = −ηW δl+1 yT

∆Bl+1 = −ηW yl δT
l+1 = −ηW δl+1 yT

∆BT

l − λ Wl+1
l+1 − λ Bl+1
l − λ BT

l+1

(15)

(16)

(17)

(18)

In this network (drawn in Figure 2)  the only variables transmitted between cells are the activity
vectors yl and δl+1  and each synapse computes its own adjustment locally  but (16) and (18) have
the form of the Kolen-Pollack equations (13) and (14)  and therefore the forward and feedback weight
matrices converge to transposes of each other.

Figure 2: Reciprocal network for Kolen-Pollack learning. There is a single mode of operation.
Gold-colored cross-projections (
) convey feedback signals δ to forward-path cells  so they can
adjust the forward weights W using learning rule (16). Blue cross-projections (
) convey the signals
y to the feedback cells  so they can adjust the feedback weights B using (17).

released a Python version of

We have
the weight mirror and the KP reciprocal network that we used in our
github.com/makrout/Deep-Learning-without-Weight-Transport.

the proprietary TensorFlow/TPU code
tests;

for
see

6 Experiments

We compared our weight-mirror and Kolen-Pollack networks to backprop  plain feedback alignment 
and the sign-symmetry method [5  7]. For easier comparison with recent papers on biologically-
motivated algorithms [4  5  7]  we used the same types of networks they did  with convolution [18] 

5

δl+1δ lylyl +1WlWl+2Wl+1BlBl+1 Bl+2 batch normalization (BatchNorm) [19]  and rectiﬁed linear units (ReLUs) without bias-blocking. In
most experiments  we used a ResNet block variant where signals were normalized by BatchNorm
after the ReLU nonlinearity  rather than before (see Appendix D.3). More brain-like implementations
would have to replace BatchNorm with some kind of synaptic scaling [15  16]  ReLU with a bounded
function such as rectiﬁed tanh  and convolution with non-weight-sharing local connections.

Figure 3: ImageNet results. a) With ResNet-18 architecture  the weight-mirror network (— WM)
and Kolen-Pollack (— KP) outperformed plain feedback alignment (— FA) and the sign-symmetry
algorithm (— SS)  and nearly matched backprop (— BP). b) With the larger ResNet-50 architecture 
results were similar.

Run on the ImageNet visual-recognition task [6] with the ResNet-18 network (Figure 3a)  weight
mirrors managed a ﬁnal top-1 test error of 30.2(7)%  and Kolen-Pollack reached 29.2(4)%  versus
97.4(2)% for plain feedback alignment  39.2(4)% for sign-symmetry  and 30.1(4)% for backprop.
With ResNet-50 (Figure 3b)  the scores were: weight mirrors 23.4(5)%  Kolen-Pollack 23.9(7)% 
feedback alignment 98.9(1)%  sign-symmetry 33.8(3)%  and backprop 22.9(4)%. (Digits in paren-
theses are standard errors).
Sign-symmetry did better in other experiments where batch normalization was applied before the
ReLU nonlinearity. In those runs  it achieved top-1 test errors of 37.8(4)% with ResNet-18 (close to
the 37.91% reported in [7] for the same network) and 32.6(6)% with ResNet-50 (see Appendix D.1
for details of our hyperparameter selection  and Appendix D.3 for a ﬁgure of the best result attained
by sign-symmetry on our tests). The same change in BatchNorm made little difference to the other
four methods — backprop  feedback alignment  Kolen-Pollack  and the weight mirror.
Weight mirroring kept the forward and feedback matrices in agreement throughout training  as shown
in Figure 4. One way to measure this agreement is by matrix angles: in each layer of the networks 
we took the feedback matrix Bl and the transpose of the forward matrix  WT
  and reshaped them
l
into vectors. With backprop  the angle between those vectors was of course always 0. With weight
mirrors (Figure 4a)  the angle stayed < 12° in all layers  and < 6° later in the run for all layers except
the ﬁnal one. That ﬁnal layer was fully connected  and therefore its Wl received more inputs than
l harder to deduce. For closer alignment  we
those of the other  convolutional layers  making its WT
would have needed longer mirroring with more examples.
The matrix angles grew between epochs 2 and 10 and then held steady at relatively high levels till
epoch 32 because during this period the learning rate ηW was large (see Appendix D.1)  and mirroring
l ’s. That problem could also have been solved
didn’t keep the Bl’s matched to the fast-changing WT
with more mirroring  but it did no harm because at epoch 32  ηW shrank by 90%  and from then on 
the Bl’s and WT
We also computed the δ angles between the feedback vectors δl computed by the weight-mirror
network (using Bl’s) and those that would have been computed by backprop (using WT
l ’s). Weight
mirrors kept these angles < 25° in all layers (Figure 4b)  with worse alignment farther upstream 
because δ angles depend on the accumulated small discrepancies between all the Bl’s and WT
l ’s in
all downstream layers.

l ’s stayed better aligned.

6

020406080100020406080100Test top-1 error (%)EpochFAaBPKPWMSS020406080100020406080100Test top-1 error (%)EpochFAbSSBPKPWMFigure 4: Agreement of forward and feedback matrices in the ResNet-50 from Figure 3b. a) Weight
l small in all layers  from the input layer (—)
mirrors kept the angles between the matrices Bl and WT
to the output (—). b) Feedback vectors δl computed by the weight-mirror network were also well
aligned with those that would have been computed by backprop. c  d) The Kolen-Pollack network
kept the matrix and δ angles even smaller. e  f) The sign-symmetry method was less accurate.

The Kolen-Pollack network was even more accurate  bringing the matrix and δ angles to near zero
within 20 epochs and holding them there  as shown in Figures 4c and 4d.
The sign-symmetry method aligned matrices and δ’s less accurately (Figures 4e and 4f)  while with
feedback alignment (not shown)  both angles stayed > 80° for most layers in both the ResNet-18 and
ResNet-50 architectures.

7

InputOutput10120304050Layers0204060800153045607590Matrix angle (°)Epocheδ angle (°)f020406080100100Epoch0153045607590036912150510152025020406080Matrix angle (°)Epochcδ angle (°)d020406080100100Epoch036912150510152025Matrix angle (°)abδ angle (°)020406080100Epoch020406080100Epoch7 Discussion

Both the weight mirror and the Kolen-Pollack network outperformed feedback alignment and the
sign-symmetry algorithm  and both kept pace  at least roughly  with backprop. Kolen-Pollack has
some advantages over weight mirrors  as it doesn’t call for separate modes of operation and needn’t
proceed layer by layer. Conversely  weight mirrors don’t need sensory input but learn from noise  so
they could tune feedback paths in sleep or in utero. And while KP kept matrix and δ angles smaller
than WM did in Figure 4  that may not be the case in all learning tasks. With KP  the matrix B
converges to WT at a rate that depends on λ  the weight-decay factor in equation (17). A big λ
speeds up alignment  but may hamper learning  and at present we have no proof that a good balance
can always be found between λ and learning rate ηW . In this respect  WM may be more versatile than
KP  because if mirroring ever fails to yield small enough angles  we can simply do more mirroring 
e.g. in sleep. More tests are needed to assess the two mechanisms’ aptitude for different tasks 
their sensitivity to hyperparameters  and their effectiveness in non-convolutional networks and other
architectures.
Both methods may have applications outside biology  because the brain is not the only computing
device that lacks weight transport. Abstractly  the issue is that the brain represents information in two
different forms: some is coded in action potentials  which are energetically expensive but rapidly
transmissible to other parts of the brain  while other information is stored in synaptic weights  which
are cheap and compact but localized — they inﬂuence the transmissible signals but are not themselves
transmitted. Similar issues arise in certain kinds of technology  such as application-speciﬁc integrated
circuits (ASICs). Here as in the brain  mechanisms like weight mirroring and Kolen-Pollack could
allow forward and feedback weights to live locally  saving time and energy [20–22].

References
[1] Stephen Grossberg. Competitive learning: From interactive activation to adaptive resonance. Cognitive

Science  11(1):23–63  1987.

[2] Francis Crick. The recent excitement about neural networks. Nature  337(6203):129–132  1989.

[3] Timothy P Lillicrap  Daniel Cownden  Douglas B Tweed  and Colin J Akerman. Random synaptic feedback

weights support error backpropagation for deep learning. Nature Communications  7:13276  2016.

[4] Sergey Bartunov  Adam Santoro  Blake Richards  Luke Marris  Geoffrey E Hinton  and Timothy Lillicrap.
Assessing the scalability of biologically-motivated deep learning algorithms and architectures. In Advances
in Neural Information Processing Systems  pages 9390–9400  2018.

[5] Theodore H Moskovitz  Ashok Litwin-Kumar  and LF Abbott. Feedback alignment in deep convolutional

networks. arXiv preprint arXiv:1812.06488  2018.

[6] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
ImageNet large scale visual recognition

Andrej Karpathy  Aditya Khosla  Michael Bernstein  et al.
challenge. International Journal of Computer Vision  115(3):211–252  2015.

[7] Will Xiao  Honglin Chen  Qianli Liao  and Tomaso Poggio. Biologically-plausible learning algorithms can

scale to large datasets. arXiv preprint arXiv:1811.03567  2018.

[8] Dmitry Krotov and John Hopﬁeld. Unsupervised learning by competing hidden units. Proceedings of the

National Academy of Sciences  116(16):7723–7731  2019.

[9] Jordan Guerguiev  Konrad Kording  and Blake Richards. Spike-based causal inference for weight alignment.

arXiv preprint arXiv:1910.01689  2019.

[10] John F Kolen and Jordan B Pollack. Backpropagation without weight transport. In Proceedings of 1994
IEEE International Conference on Neural Networks (ICNN’94)  volume 3  pages 1375–1380. IEEE  1994.

[11] David E Rumelhart  Geoffrey E Hinton  Ronald J Williams  et al. Learning representations by back-

propagating errors. Nature  323:533–536  1986.

[12] Robert Urbanczik and Walter Senn. Learning by the dendritic prediction of somatic spiking. Neuron 

81(3):521–528  2014.

[13] Jordan Guergiuev  Timothy P Lillicrap  and Blake A Richards. Biologically feasible deep learning with

segregated dendrites. arXiv preprint arXiv:1610.00161  2016.

8

[14] Nathan Intrator and Leon N Cooper. Objective function formulation of the BCM theory of visual cortical

plasticity: Statistical connections  stability conditions. Neural Networks  5(1):3–17  1992.

[15] Gina G Turrigiano. The self-tuning neuron: synaptic scaling of excitatory synapses. Cell  135(3):422–435 

2008.

[16] Dhrubajyoti Chowdhury and Johannes W Hell. Homeostatic synaptic scaling: Molecular regulators of

synaptic AMPA-type glutamate receptors. F1000Research  7  2018.

[17] Daniel Kunin  Jonathan M Bloom  Aleksandrina Goeva  and Cotton Seed. Loss landscapes of regularized

linear autoencoders. arXiv preprint arXiv:1901.08168  2019.

[18] Yann LeCun  Bernhard Boser  John S Denker  Donnie Henderson  Richard E Howard  Wayne Hubbard  and
Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation 
1(4):541–551  1989.

[19] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arXiv:1502.03167  2015.

[20] Yu-Hsin Chen  Joel Emer  and Vivienne Sze. Eyeriss: A spatial architecture for energy-efﬁcient dataﬂow
for convolutional neural networks. In ACM SIGARCH Computer Architecture News  volume 44  pages
367–379. IEEE Press  2016.

[21] Hyoukjun Kwon  Michael Pellauer  and Tushar Krishna. Maestro: An open-source infrastructure for

modeling dataﬂows within deep learning accelerators. arXiv preprint arXiv:1805.02566  2018.

[22] Brian Crafton  Abhinav Parihar  Evan Gebhardt  and Arijit Raychowdhury. Direct feedback alignment

with sparse connections for local learning. arXiv preprint arXiv:1903.02083  2019.

[23] Andrey Gushchin and Ao Tang. Total wiring length minimization of C. elegans neural network: a

constrained optimization approach. PloS one  10(12):e0145029  2015.

[24] Marion Langen  Egemen Agi  Dylan J Altschuler  Lani F Wu  Steven J Altschuler  and Peter Robin
Hiesinger. The developmental rules of neural superposition in Drosophila. Cell  162(1):120–133  2015.

[25] SL Palay and V Chan-Palay. Cerebellar Cortex. Springer-Verlag. Berlin  Heiderberg  New York  1974.

[26] Jordan Guerguiev  Timothy P Lillicrap  and Blake A Richards. Towards deep learning with segregated

dendrites. Elife  6:e22901  2017.

[27] Richard Naud and Henning Sprekeler. Sparse bursts optimize information transmission in a multiplexed

neural code. Proceedings of the National Academy of Sciences  115(27):E6329–E6338  2018.

[28] Chris Eliasmith and Charles H Anderson. Neural Engineering: Computation  Representation  and

Dynamics in Neurobiological Systems. MIT Press  2004.

[29] RJ Leigh and DS Zee. The Neurology of Eye Movements. New York: Oxford University Press  3rd ed.

edition  1999.

[30] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778 
2016.

[31] Peter Buchlovsky  David Budden  Dominik Grewe  Chris Jones  John Aslanides  Frederic Besse  Andy
Brock  Aidan Clark  Sergio Gómez Colmenarejo  Aedan Pope  et al. Tf-replicator: Distributed machine
learning for researchers. arXiv preprint arXiv:1902.00465  2019.

[32] Ilya Sutskever  James Martens  George E Dahl  and Geoffrey E Hinton. On the importance of initialization

and momentum in deep learning. ICML (3)  28(1139-1147):5  2013.

9

,Nima Taghipour
Jesse Davis
Hendrik Blockeel
Marta Soare
Alessandro Lazaric
Remi Munos
Mohamed Akrout
Collin Wilson
Peter Humphreys
Timothy Lillicrap
Douglas Tweed