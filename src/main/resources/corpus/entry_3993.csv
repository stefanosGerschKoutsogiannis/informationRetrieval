2016,On the Recursive Teaching Dimension of VC Classes,The recursive teaching dimension (RTD) of a concept class $C \subseteq \{0  1\}^n$  introduced by Zilles et al. [ZLHZ11]  is a complexity parameter measured by the worst-case number of labeled examples needed to learn any target concept of $C$ in the recursive teaching model. In this paper  we study the quantitative relation between RTD and the well-known learning complexity measure VC dimension (VCD)  and improve the best known upper and (worst-case) lower bounds on the recursive teaching dimension with respect to the VC dimension.  Given a concept class $C \subseteq \{0  1\}^n$ with $VCD(C) = d$  we first show that $RTD(C)$ is at most $d 2^{d+1}$. This is the first upper bound for $RTD(C)$ that depends only on $VCD(C)$  independent of the size of the concept class $|C|$ and its~domain size $n$. Before our work  the best known upper bound for $RTD(C)$ is $O(d 2^d \log \log |C|)$  obtained by Moran et al. [MSWY15]. We remove the $\log \log |C|$ factor.  We also improve the lower bound on the worst-case ratio of $RTD(C)$ to $VCD(C)$. We present a family of classes $\{ C_k \}_{k \ge 1}$ with $VCD(C_k) = 3k$ and $RTD(C_k)=5k$  which implies that the ratio of $RTD(C)$ to $VCD(C)$ in the worst case can be as large as $5/3$. Before our work  the largest ratio known was $3/2$ as obtained by Kuhlmann [Kuh99]. Since then  no finite concept class $C$ has been known to satisfy $RTD(C) > (3/2) VCD(C)$.,On the Recursive Teaching Dimension

of VC Classes

Xi Chen

Department of Computer Science

Columbia University

xichen@cs.columbia.edu

Yu Cheng

Department of Computer Science
University of Southern California

yu.cheng.1@usc.edu

Bo Tang

Department of Computer Science

Oxford University

tangbonk1@gmail.com

Abstract

The recursive teaching dimension (RTD) of a concept class C ⊆ {0  1}n  introduced
by Zilles et al. [ZLHZ11]  is a complexity parameter measured by the worst-case
number of labeled examples needed to learn any target concept of C in the recursive
teaching model. In this paper  we study the quantitative relation between RTD and
the well-known learning complexity measure VC dimension (VCD)  and improve
the best known upper and (worst-case) lower bounds on the recursive teaching
dimension with respect to the VC dimension.
Given a concept class C ⊆ {0  1}n with VCD(C) = d  we ﬁrst show that RTD(C)
is at most d · 2d+1. This is the ﬁrst upper bound for RTD(C) that depends only on
VCD(C)  independent of the size of the concept class |C| and its domain size n.
Before our work  the best known upper bound for RTD(C) is O(d2d log log |C|) 
obtained by Moran et al. [MSWY15]. We remove the log log |C| factor.
We also improve the lower bound on the worst-case ratio of RTD(C) to VCD(C).
We present a family of classes {Ck}k≥1 with VCD(Ck) = 3k and RTD(Ck) = 5k 
which implies that the ratio of RTD(C) to VCD(C) in the worst case can be as
large as 5/3. Before our work  the largest ratio known was 3/2 as obtained by
Kuhlmann [Kuh99]. Since then  no ﬁnite concept class C has been known to satisfy
RTD(C) > (3/2) · VCD(C).

1

Introduction

In computational learning theory  one of the fundamental challenges is to understand how different
information complexity measures arising from different learning models relate to each other. These
complexity measures determine the worst-case number of labeled examples required to learn any
concept from a given concept class. For example  one of the most notable results along this line
of research is that the sample complexity in PAC-learning is linearly related to the VC dimension
[BEHW89]. Recall that the VC dimension of a concept class C ⊆ {0  1}n [VC71]  denoted by
VCD(C)  is the maximum size of a shattered subset of [n] = {1  . . .   n}  where we say Y ⊆ [n] is
shattered if for every binary string b of length |Y |  there is a concept c ∈ C such that c|Y = b. Here
we use c|X to denote the projection of c on X. As the best-studied information complexity measure 
VC dimension is known to be closely related to many other complexity parameters  and it serves as a
natural parameter to compare against across various models of learning and teaching.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Instead of the PAC-learning model where the algorithm takes random samples  we consider an
interactive learning model where a helpful teacher selects representative examples and present them
to the learner  with the objective of minimizing the number of examples needed. The notion of a
teaching set was introduced in mathematical models for teaching. The teaching set of a concept c ∈ C
is a set of indices (or examples) X ⊆ [n] that uniquely identiﬁes c from C. Formally  given a concept
class C ⊆ {0  1}n (a set of binary strings of length n)  X ⊆ [n] is a teaching set for a concept c ∈ C
(a binary string in C) if X satisﬁes

c|X (cid:54)= c(cid:48)|X  

for all other concepts c(cid:48) ∈ C.

The teaching dimension of a concept class C is the smallest number t such that every c ∈ C has a
teaching set of size no more than t [GK95  SM90]. However  teaching dimension does not always
capture the cooperation in teaching and learning (as we will see in Example 2)  and a more optimistic
and realistic notion of recursive teaching dimension has been introduced and studied extensively in
the literature [Kuh99  DSZ10  ZLHZ11  WY12  DFSZ14  SSYZ14  MSWY15].
Deﬁnition 1. The recursive teaching dimension of a class C ⊆ {0  1}n  denoted by RTD(C)  is the
smallest number t where one can order all the concepts of C as an ordered sequence c1  . . .   c|C| such
that every concept ci  i < |C|  has a teaching set of size no more than t in {ci  . . .   c|C|}.
Hence  RTD(C) measures the worst-case number of labeled examples needed to learn any target
concept in C  if the teacher and the learner are cooperative. We would like to emphasize that an
optimal ordered sequence (as in Deﬁnition 1) can be derived by the teacher and learner separately
without any communication: They can put all concepts in C that have the smallest teaching dimension
appear at the beginning of the sequence  then remove these concepts from C and proceeds recursively.
By deﬁnition  RTD(C) is always bounded from above by the teaching dimension of C but can be
much smaller than the teaching dimension. We use the following example to illustrate the difference
between the teaching dimension and the recursive teaching dimension.
Example 2. Consider the class C ⊆ {0  1}n with n + 1 concepts: the empty concept 0 and all the
singletons. For example when n = 3  C = {000  100  010  001}. Each singleton concept has teaching
dimension 1  while the teaching dimension for the empty concept 0 is n  because the teacher has to
reveal all labels to distinguish 0 from the other concepts. However  if the teacher and the learner
are cooperative  every concept can be taught with one label: If the teacher reveals a “0” label  the
learner can safely assume that the target concept must be 0  because otherwise the teacher would
present a “1” label instead for the other concepts. Equivalently  in the setting of Deﬁnition 1  the
teacher and the learner can order the concepts so that the singleton concepts appear before the empty
concept 0. Then every concept has a teaching set of size 1 to distinguish it from the later concepts in
the sequence  and thus the recursive teaching dimension of C is 1.
In this paper  we study the quantitative relationship between the recursive teaching dimension (RTD)
and the VC dimension (VCD). A bound on the RTD that depends only on the VCD would imply a
close connection between learning from random samples and teaching (under the recursive teaching
model). The same structural properties that make a concept class easy to learn would also give a
bound on the number of examples needed to teach it. Moreover  the recursive teaching dimension is
known to be closely related to sample compression schemes [LW86  War03  DKSZ16]  and a better
understanding of the relationship between RTD and VCD might help resolve the long-standing sample
compression conjecture [War03]  which states that every concept class has a sample compression
scheme of size linear in its VCD.

1.1 Our Results
Our main result (Theorem 3) is an upper bound of d · 2d+1 on RTD(C) when VCD(C) = d. This
is the ﬁrst upper bound for RTD(C) that depends only on VCD(C)  but not on |C|  the size of the
concept class  or n  the domain size. Previously  Moran et al. [MSWY15] showed an upper bound of
O(d2d log log |C|) for RTD(C); our result removes the log log |C| factor  and answers positively an
open problem posed in [MSWY15].
Our proof tries to reveal examples iteratively to minimize the number of the remaining concepts.
Given a concept class C ⊆ {0  1}n  we pick a set of examples Y ⊆ [n] and their labels b ∈ {0  1}Y   so
that the set of remaining concepts (with the projection c|Y = b) is nonempty and has the smallest size
among all choices of Y and b. We then prove by contradiction (with the assumption of VCD(C) = d)

2

that  if the size of Y is large enough (but still depends on only VCD(C))  the remaining concepts
must have VC dimension at most d − 1. This procedure gives us a recursive formula  which we solve
and obtain the claimed upper bound on RTD of classes of VC dimension d.
We also improve the lower bound on the worst-case factor by which RTD may exceed VCD. We
present a family of classes {Ck}k≥1 (Figure 4) with VCD(Ck) = 3k and RTD(Ck) = 5k  which shows
that the worst-case ratio between RTD(C) and VCD(C) is at least 5/3. Before our work  the largest
known multiplicative gap between RTD(C) and VCD(C) was a ratio of 3/2  given by Kuhlmann
[Kuh99]. (Later Doliwa et al. [DFSZ14] showed the smallest class CW with RTD(CW ) = (3/2) ·
VCD(CW ) (Warmuth’s class)). Since then  no ﬁnite concept class C with RTD(C) > (3/2) · VCD(C)
has been found.
Instead of exhaustively searching through all small concept classes  our improvement on the lower
bound is achieved by formulating the existence of a concept class with the desired RTD  VCD and
domain size  as a boolean satisﬁability problem. We then run the state-of-the-art SAT solvers on
these formulae to discover a concept class C0 with VCD(C0) = 3 and RTD(C0) = 5. Based on the
concept class C0  one can produce a family of concept classes {Ck}k≥1 with VCD(Ck) = 3k and
RTD(Ck) = 5k  by taking the Cartesian product of k copies of C0: Ck = C0 × . . . × C0.

2 Upper Bound on the Recursive Teaching Dimension
In this section  we prove the following upper bound on RTD(C) with respect to VCD(C).
Theorem 3. Let C ⊆ {0  1}n be a class with VCD(C) = d. Then RTD(C) ≤ 2d+1(d − 2) + d + 4.
Given a class C  we use TSmin(C) to denote the smallest integer t such that at least one concept c ∈ C
has a teaching set of size t. Notice that TSmin(C) is different from teaching dimension. Teaching
dimension is deﬁned as the smallest t such that every c ∈ C has a teaching set of size at most t.)
Theorem 3 follows directly from Lemma 4 and the observation that the VC dimension of a concept
class does not increase after a concept is removed. (After removing a concept from C  the new class
C(cid:48) still has VCD(C(cid:48)) ≤ d  and one can apply Lemma 4 again to obtain another concept that has a
teaching set of the desired size in C(cid:48) and repeat this process.)
Lemma 4. Let C ⊆ {0  1}n be a class with VCD(C) = d. Then TSmin(C) ≤ 2d+1(d − 2) + d + 4.
We start with some intuition by reviewing the proof of Kuhlmann [Kuh99] that every class C with
VCD(C) = 1 must have a concept c ∈ C with a teaching set of size 1. Given an index i ∈ [n] and a
b to denote the set of concepts c ∈ C such that ci = b. The proof starts by
bit b ∈ {0  1}  we use Ci
picking an index i and a bit b such that Ci
b is nonempty and has the smallest size among all choices of
i and b. The proof then proceeds to show that Ci
b contains a unique concept  which by the deﬁnition of
Ci
b has a teaching set {i} of size 1. To see why Ci
b must be a singleton set  we assume for contradiction
that it contains more than one concept. Then there exists an index j (cid:54)= i and two concepts c  c(cid:48) ∈ Ci
j = 1. Since C has VCD(C) = 1  {i  j} cannot be shattered and thus  all the
such that cj = 0 and c(cid:48)
concepts c∗ ∈ C with c∗
i = 1 − b must share the same c∗
j = 0. As a result  it is easy to verify
that Cj
Moran et al. [MSWY15] used a similar approach to show that every so-called (3  6)-class C has
TSmin(C) at most 3. They deﬁne a class C ⊆ {0  1}n to be a (3  6)-class if for any three indices
i  j  k ∈ [n]  the projection of C onto {i  j  k} has at most 6 patterns. (In contrast  VCD(C) = 2
means that the projection of C has at most 7 patterns. So C being a (3  6)-class is a stronger condition
than VCD(C) = 2.) The proof of [MSWY15] starts by picking two indices i  j ∈ [n] and two bits
b1  b2 ∈ {0  1} such that Ci j
  i.e.  the set of c ∈ C such that ci = b1 and cj = b2  is nonempty
and has the smallest size among all choices of i  j and b1  b2. They then prove by contradiction that
VCD(Ci j
Our proof extends this approach further. Given a concept class C ⊆ {0  1}n with VCD(C) = d  let
k = 2d(d − 1) + 1 and we pick a set Y ∗ ⊂ [n] of k indices and a string b
∗ ∈ {0  1}k such that CY ∗
b∗  
the set of c ∈ C such that the projection c|Y ∗ = b
∗  is nonempty and has the smallest size among all
choices of Y and b. We then prove by contradiction (with the assumption of VCD(C) = d) that CY ∗
b∗
must have VC dimension at most d − 1. This gives us a recursive formula that bounds the TSmin of
classes of VC dimension d  which we solve to obtain the upper bound stated in Lemma 4.
We now prove Lemma 4.

) = 1  and combine with [Kuh99] to conclude that TSmin(C) ≤ 3.

j   say c∗

1 is a nonempty proper subset of Ci

b  contradicting the choice of i and b at the beginning.

b

b1 b2

b1 b2

3

Proof of Lemma 4. We prove by induction on d. Let

f (d) = max

C : VCD(C)≤d

TSmin(C).

Our goal is to prove the following upper bound for f (d):
f (d) ≤ 2d+1(d − 2) + d + 4 
The base case of d = 1 follows directly from [Kuh99].
For the induction step  we show that condition (1) holds for some d > 1  assuming that it holds for
d − 1. Take any concept class C ⊆ {0  1}n with VCD(C) ≤ d. Let k = 2d(d − 1) + 1. If n ≤ k then
we are already done because

for all d ≥ 1.

(1)

TSmin(C) ≤ n ≤ k = 2d(d − 1) + 1 ≤ 2d+1(d − 2) + d + 4 

where the last inequality holds for all d ≥ 1. Assume in the rest of the proof that n > k. Then any set
of k indices Y ⊂ [n] partitions C into 2k (possibly empty) subsets  denoted by

CY
b = {c ∈ C : c|Y = b} 

for each b ∈ {0  1}k.

∗ ∈ {0  1}k such that CY ∗

We follow the approach of [Kuh99] and [MSWY15] to choose a set of k indices Y ∗ ⊂ [n] as well as
b∗ is nonempty and has the smallest size among all nonempty CY
b  
a string b
∗
over all choices of Y and b. Without loss of generality we assume below that Y ∗ = [k] and b
= 0
is the all-zero string. For notational convenience  we also write Cb to denote CY ∗
for b ∈ {0  1}k.
Notice that if Cb∗ = CY ∗

b∗ has VC dimension at most d − 1  then we have
TSmin(C) ≤ k + f (d − 1) ≤ 2d+1(d − 2) + d + 4 

b

using the inductive hypothesis. This is because according to the deﬁnition of f  one of the concepts
c ∈ Cb∗ has a teaching set T ⊆ [n]\ Y ∗ of size at most f (d− 1) to distinguish it from other concepts
of Cb∗. Thus  [k] ∪ T is a teaching set of c in the original class C  of size at most k + f (d − 1).

0

0

0

0

1

1

1

1

0

0
0
0
1
1
0
1
1
XXX0
0
XXX0
1
XXX1
0
XXX1
1
1 XXX0
0

∗

= 0. Note that CY (cid:48)

b∗   after ﬁxing ﬁve bits  has VCD(CY ∗

0 is indeed a nonempty proper subset of CY ∗
0 .

Figure 1: An illustration for the proof of Lemma 4  TSmin(C) ≤ 6 when d = 2. We prove by
contradiction that the smallest nonempty set CY ∗
b∗ ) = 1  where
= 0. In this example  we have Z = {6  7}  Y (cid:48) = {2  3  4  6  7} and
Y ∗ = {1  2  3  4  5} and b
(cid:48)
b
Finally  we prove by contradiction that Cb∗ has VC dimension at most d − 1. Assume that Cb∗ has
VC dimension d. Then by deﬁnition there exists a set Z ⊆ [n] \ Y ∗ of d indices that is shattered by
Cb∗ (i.e.  all the 2d possible strings appear in Cb∗ on Z). Observe that for each i ∈ Y ∗  the union of
all Cb with bi = 1 (recall that b
∗ is the all-zero string) must miss at least one string on Z  which we
denote by pi (choose one arbitrarily if more than one are missing); otherwise  C has a shattered set
of size d + 1  i.e.  Z ∪ {i}  contradicting with the assumption that VCD(C) ≤ d. (See Figure 1 for
an example when d = 2 and k = 5.) However  given that there are only 2d possibilities for each pi
(and |Y ∗| = k = 2d(d − 1) + 1)  it follows from the pigeonhole principle that there exists a subset
K ⊂ Y ∗ of size d such that pi = p for every i ∈ K  for some p ∈ {0  1}d. Let Y (cid:48) = (Y ∗ \ K) ∪ Z
is a nonempty and proper subset of CY ∗
(cid:48)
be a new set of k indices and let b
b∗   a
contradiction with our choice of Y ∗ and b
This ﬁnishes the induction and the proof of the lemma.

= 0k−d ◦ p. Then CY (cid:48)
b(cid:48)

∗.

4

3 Lower Bound on the Worst-Case Recursive Teaching Dimension

We also improve the lower bound on the worst-case factor by which RTD may exceed VCD.
In this section  we present an improved lower bound on the worst-case factor by which RTD(C) may
exceed VCD(C). Recall the deﬁnition of TSmin(C)  which denotes the number of examples needed
to teach some concept in c ∈ C. By deﬁnition we always have RTD(C) ≥ TSmin(C) for any class C.
Kuhlmann [Kuh99] ﬁrst found a class C such that RTD(C) = TSmin(C) = 3 and VCD(C) = 2  with
domain size n = 16 and |C| = 24. Since then  no class C with RTD(C) > (3/2) · VCD(C) has been
found. Recently  Doliwa et al. [DFSZ14] gave the smallest such class CW (Warmuth’s class  as shown
in Figure 2)  with RTD(CW ) = TSmin(CW ) = 3  VCD(CW ) = 2  n = 5  and |CW| = 10. We can
view CW as taking all ﬁve possible rotations of the two concepts (0  0  0  1  1) and (0  1  0  1  1).

x1

x2

x3

x4

x5

0
0
0
1
1
0
1
0
1
1

0
0
1
1
0
1
0
1
1
0

0
1
1
0
0
0
1
1
0
1

(a)

1
1
0
0
0
1
1
0
1
0

1
0
0
0
1
1
0
1
0
1

x1

x2

x3

x4

x5

0
0

0
1

0
0

(b)

1
1

1
1

Figure 2: (a) Warmuth’s class CW with RTD(CW ) = 3 and VCD(CW ) = 2; (b) The succinct
representation of CW with one concept selected from each rotation-equivalent set of concepts.
The teaching set of each concept is marked with underline.
Given CW one can obtain a family of classes {Ck}k≥1 by taking the Cartesian product of k copies:

Ck = Ck

W = CW × ··· × CW  

and it follows from the next lemma that RTD(Ck) = TSmin(Ck) = 3k and VCD(Ck) = 2k.
Lemma 5 (Lemma 16 of [DFSZ14]). Given two concept classes C1 and C2.
Let C1 × C2 = {(c1  c2) | c1 ∈ C1  c2 ∈ C2}. Then

TSmin(C1 × C2) = TSmin(C1) + TSmin(C2) 
RTD(C1 × C2) ≤ RTD(C1) + RTD(C2) 
VCD(C1 × C2) = VCD(C1) + VCD(C2).

and

Lemma 5 allows us to focus on ﬁnding small concept classes with RTD(C) > (3/2) · VCD(C). The
ﬁrst attempt to ﬁnd such classes is to exhaustively search over all possible binary matrices and then
compute and compare their VCD and RTD. But brute-force search quickly becomes infeasible as the
domain size n gets larger. For example  even the class CW has ﬁfty 0/1 entries. Instead  we formulate
the existence of a class with certain desired RTD  VCD  and domain size  as a boolean satisﬁability
problem  and then run state-of-the-art Boolean Satisﬁability (SAT) solvers to see whether the boolean
formula is satisﬁable or not.
We brieﬂy describe how to construct an equivalent boolean formula in conjunctive normal form
(CNF). For a ﬁxed domain size n  we have 2n basic variables xc  each describing whether a concept
c ∈ {0  1}n is included in C or not. We need VC dimension to be at most VCD  which is equivalent to
requiring that every set S ⊆ [n] of size |S| = VCD + 1 is not shattered by C. So we deﬁne auxiliary
variables y(S b) for each set S of size |S| = VCD + 1  and every string b ∈ {0  1}S  indicating
whether a speciﬁc pattern b appears in the projection of C on S or not. These auxiliary variables are
decided by the basic variables  and for every S  at least one of the 2|S| patterns must be missing on S.

5

For the minimum teaching dimension to be at least RTD  we cannot teach any row with RTD − 1
labels. So for every concept c  and every set of indices T ⊆ [n] of size |T| = RTD − 1  we need
at least one other concept c(cid:48) (cid:54)= c satisfying c|T = c(cid:48) |T so that c(cid:48) is there to “confuse” c on T . As
an example  we list one clause of each type  from the SAT instance with n = 5  VCD = 2  and
RTD = 3:

x01011 → y({1 2 3} 010) 

¬ y({1 2 3} b) 

x01011 → (cid:95)

x(01 b).

b(cid:54)=011

(cid:95)

b

Note that there are many ways to formulate our problem as a SAT instance. For example  we could
directly use a boolean variable for each entry of the matrix. But in our experiments  the SAT solvers
run faster using the formulation described above. The SAT solvers we use are Lingeling [Bie15]
and Glucose [AS14] (based on MiniSAT [ES03]). We are able to rediscover CW and rule out the
existence of concept classes for certain small values of (VCD  RTD  n); see Figure 3.

Satisﬁable Concept Class
CW (Figure 2)

VCD(C) RTD(C) n (domain size)

2
2
3
3
4
4

3

3
4
5
6
6
7

5

5
7
7
8
7
8

12

Yes
No
No
No
No
No
Yes

Figure 4

Figure 3: The satisﬁability of the boolean formulae for small values of VCD(C)  RTD(C)  and n.

Unfortunately for n > 8  even these SAT solvers are no longer feasible. We use another heuristic
to speed up the SAT solvers when we conjecture the formula to be satisﬁable — adding additional
clauses to the SAT formula so that it has fewer solutions (but hopefully still satisﬁable)  and faster
to solve. More speciﬁcally  we bundle all the rotation-equivalent concepts  that is if we include a
concept  we must also include all its rotations. Note that with this restriction  we can reduce the
number of variables by having one for each rotation-equivalent set; we can also reduce the number of
clauses  since if S is not shattered  then we know all rotations of S are also not shattered.
We manage to ﬁnd a class C0 with RTD(C0) = TSmin(C0) = 5 and VCD(C) = 3  and domain size
n = 12. A succinct representation of C0 is given in Figure 4  where all rotation-equivalent concepts
(i.e. rows) are omitted. The ﬁrst 8 rows each represents 12 concepts  and the last row represents 4
concepts (because it is more symmetric)  with a total of |C0| = 100 concepts. We also include a text
ﬁle with the entire concept class C0 (as a 100 × 12 matrix) in the supplemental material. Applying
Lemma 5  we obtain a family of concept classes {Ck}k≥1  where Ck = C0 × ··· × C0 is the Cartesian
product of k copies of C0  that satisfy RTD(Ck) = 5k and VCD(Ck) = 3k.

4 Conclusion and Open Problem

We improve the best known upper and lower bounds for the worst-case recursive teaching dimension
with respect to VC dimension. Given a concept class C with d = VCD(C) we improve the upper
bound RTD(C) = O(d2d log log |C|) of Moran et al. [MSWY15] to 2d+1(d − 2) + d + 4  removing
the log log |C| factor as well as the dependency on |C|. In addition  we improve the lower bound
maxC(RTD(C)/VCD(C)) ≥ 3/2 of Kuhlmann [Kuh99] to maxC(RTD(C)/VCD(C)) ≥ 5/3.
Our results are a step towards answering the following question:
Is RTD(C) = O(VCD(C))?

posed by Simon and Zilles [SZ15].
While Kuhlmann [Kuh99] showed that RTD(C) = 1 when VCD(C) = 1  the simplest case that is still
open is to give a tight bound on RTD(C) when VCD(C) = 2: Doliwa et al. [DFSZ14] presented a
concept class C (Warmuth’s class) with RTD(C) = 3  while our Theorem 3 shows that RTD(C) ≤ 6.

6

x1

x2

x3

x4

x5

x6

x7

x8

x9

x10

x11

x12

0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
1
1

0
0
0
0
0
1
1
0
1

0
0
0
1
1
1
1
1
1

0
0
1
0
1
0
0
0
0

1
1
1
1
1
1
1
1
1

0
1
0
1
0
0
1
1
1

1
1
1
1
1
1
1
1
1

0
0
0
0
0
0
0
0
0

1
1
1
1
1
1
1
1
1

0
0
0
0
0
0
0
1
1

1
1
1
1
1
1
1
1
1

Figure 4: The succinct representation of a concept class C0 with RTD(C0) = 5 and VCD(C0) = 3.
The teaching set of each concept is marked with underline.

Acknowledgments

We thank the anonymous reviewers for their helpful comments and suggestions. We also thank Joseph
Bebel for pointing us to the SAT solvers. This work was done in part while the authors were visiting
the Simons Institute for the Theory of Computing. Xi Chen is supported by NSF grants CCF-1149257
and CCF-1423100. Yu Cheng is supported in part by Shang-Hua Teng’s Simons Investigator Award.
Bo Tang is supported by ERC grant 321171.

References

[AS14] G. Audemard and L. Simon. Glucose 4.0. 2014. Available at

http://www.labri.fr/perso/lsimon/glucose.

[BEHW89] A. Blumer  A. Ehrenfeucht  D. Haussler  and M. K. Warmuth. Learnability and the

Vapnik-Chervonenkis dimension. J. ACM  36(4):929–965  1989.

[Bie15] A. Biere. Lingeling  Plingeling and Treengeling. 2015. Available at

http://fmv.jku.at/lingeling.

[DFSZ14] T. Doliwa  G. Fan  H.-U. Simon  and S. Zilles. Recursive teaching dimension 

VC-dimension and sample compression. Journal of Machine Learning Research 
15(1):3107–3131  2014.

[DKSZ16] M. Darnstädt  T. Kiss  H. U. Simon  and S. Zilles. Order compression schemes. Theor.

Comput. Sci.  620:73–90  2016.

[DSZ10] T. Doliwa  H.-U. Simon  and S. Zilles. Recursive teaching dimension  learning

complexity  and maximum classes. In Proceedings of the 21st International Conference
on Algorithmic Learning Theory  pages 209–223  2010.

[ES03] N. Eén and N. Sörensson. An extensible SAT-solver. In Theory and Applications of
Satisﬁability Testing  6th International Conference  SAT 2003.  pages 502–518  2003.

[GK95] S. A. Goldman and M. J. Kearns. On the complexity of teaching. Journal of Computer

and System Sciences  50(1):20–31  1995.

[Kuh99] C. Kuhlmann. On teaching and learning intersection-closed concept classes. In

Proceedings of the 4th European Conference on Computational Learning Theory  pages
168–182  1999.

[LW86] N. Littlestone and M. Warmuth. Relating data compression and learnability. Technical

report  University of California  Santa Cruz  1986.

7

[MSWY15] S. Moran  A. Shpilka  A. Wigderson  and A. Yehudayoff. Compressing and teaching

for low VC-dimension. In Proceedings of the 56th IEEE Annual Symposium on
Foundations of Computer Science  pages 40–51  2015.

[SM90] A. Shinohara and S. Miyano. Teachability in computational learning. In ALT  pages

247–255  1990.

[SSYZ14] R. Samei  P. Semukhin  B. Yang  and S. Zilles. Algebraic methods proving Sauer’s

bound for teaching complexity. Theoretical Computer Science  558:35–50  2014.

[SZ15] H.-U. Simon and S. Zilles. Open problem: Recursive teaching dimension versus VC

dimension. In Proceedings of the 28th Conference on Learning Theory  pages
1770–1772  2015.

[VC71] V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative

frequencies of events to their probabilities. Theory of Probability and Its Applications 
16:264–280  1971.

[War03] M. K. Warmuth. Compressing to VC dimension many points. In Proceedings of the

16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop 
COLT/Kernel 2003  pages 743–744  2003.

[WY12] A. Wigderson and A. Yehudayoff. Population recovery and partial identiﬁcation. In

Proceedings of the 53rd IEEE Annual Symposium on Foundations of Computer Science 
pages 390–399  2012.

[ZLHZ11] S. Zilles  S. Lange  R. Holte  and M. Zinkevich. Models of cooperative teaching and

learning. Journal of Machine Learning Research  12:349–384  2011.

8

,Franz Kiraly
Louis Theran
Xi Chen
Yu Cheng
Bo Tang
Liangpeng Zhang
Ke Tang
Xin Yao