2009,Gaussian process regression with Student-t likelihood,In the Gaussian process regression the observation model is commonly assumed to be Gaussian  which is convenient in computational perspective. However  the drawback is that the predictive accuracy of the model can be significantly compromised if the observations are contaminated by outliers. A robust observation model  such as the Student-t distribution  reduces the influence of outlying observations and improves the predictions. The problem  however  is the analytically intractable inference. In this work  we discuss the properties of a Gaussian process regression model with the Student-t likelihood and utilize the Laplace approximation for approximate inference. We compare our approach to a variational approximation and a Markov chain Monte Carlo scheme  which utilize the commonly used scale mixture representation of the Student-t distribution.,Gaussian process regression with Student-t likelihood

Jarno Vanhatalo

Department of Biomedical Engineering

and Computational Science

Helsinki University of Technology

Finland

Pasi Jyl¨anki

Department of Biomedical Engineering

and Computational Science

Helsinki University of Technology

Finland

jarno.vanhatalo@tkk.fi

pasi.jylanki@tkk.fi

Aki Vehtari

Department of Biomedical Engineering

and Computational Science

Finland

Helsinki University of Technology

aki.vehtari@tkk.fi

Abstract

In the Gaussian process regression the observation model is commonly assumed
to be Gaussian  which is convenient in computational perspective. However  the
drawback is that the predictive accuracy of the model can be signiﬁcantly com-
promised if the observations are contaminated by outliers. A robust observation
model  such as the Student-t distribution  reduces the inﬂuence of outlying obser-
vations and improves the predictions. The problem  however  is the analytically
intractable inference. In this work  we discuss the properties of a Gaussian process
regression model with the Student-t likelihood and utilize the Laplace approxima-
tion for approximate inference. We compare our approach to a variational approx-
imation and a Markov chain Monte Carlo scheme  which utilize the commonly
used scale mixture representation of the Student-t distribution.

1 Introduction

A commonly used observation model in the Gaussian process (GP) regression is the Normal distri-
bution. This is convenient since the inference is analytically tractable up to the covariance function
parameters. However  a known limitation with the Gaussian observation model is its non-robustness 
and replacing the normal distribution with a heavy-tailed one  such as the Student-t distribution  can
be useful in problems with outlying observations.
If both the prior and the likelihood are Gaussian  the posterior is Gaussian with mean between
the prior mean and the observations.
In conﬂict this compromise is not supported by either of
the information sources. Thus  outlying observations may signiﬁcantly reduce the accuracy of the
inference. For example  a single corrupted observation may pull the posterior expectation of the
unknown function value considerably far from the level described by the other observations (see
Figure 1). A robust  or outlier-prone  observation model would  however  weight down the outlying
observations the more  the further away they are from the other observations and prior mean.
The idea of robust regression is not new. Outlier rejection was described already by De Finetti [1]
and theoretical results were given by Dawid [2]  and O’Hagan [3]. Student-t observation model with
linear regression was studied already by West [4] and Geweke [5]  and Neal [6] introduced it for GP
regression. Other robust observation models include  for example  mixtures of Gaussians  Laplace

1

(a) Gaussian observation model.

(b) Student-t observation model.

Figure 1: An example of regression with outliers by Neal [6]. On the left Gaussian and on the right
the Student-t observation model. The real function is plotted with black line.

distribution and input dependent observation models [7–10]. The challenge with the Student-t model
is the inference  which is analytically intractable. A common approach has been to use the scale-
mixture representation of the Student-t distribution [5]  which enables Gibbs sampling [5  6]  and a
factorized variational approximation (VB) for the posterior inference [7  11].
Here  we discuss the properties of the GP regression with a Student-t likelihood and utilize the
Laplace approximation for the approximate inference. We discuss the known weaknesses of the
approximation scheme and show that in practice it works very well and quickly. We use several
different data sets to compare it to both a full MCMC and a factorial VB  which utilize the scale
mixture equivalent of the Student-t distribution. We show that the predictive performances are sim-
ilar and that the Laplace’s method approximates the posterior covariance somewhat better than VB.
We also point out some of the similarities between these two methods and discuss their differences.

2 Robust regression with Gaussian processes

Consider a regression problem  where the data comprise observations yi = f(xi) + i at input
locations X = {xi}n
i=1  where the observation errors 1  ...  n are zero-mean exchangeable random
variables. The object of inference is the latent function f  which is given a Gaussian process prior.
This implies that any ﬁnite subset of latent variables  f = {f(xi)}n
i=1  has a multivariate Gaussian
distribution. In particular  at the observed input locations X the latent variables have a distribution

p(f|X) = N (f|µ  Kf f) 

(1)

where Kf f is the covariance matrix and µ the mean function. For the notational simplicity  we will
use a zero-mean Gaussian process. Each element in the covariance matrix is a realization of covari-
ance function  [Kf f]ij = k(xi  xj)  which represents the prior assumptions of the smoothness of the
latent function (for a detailed introduction on GP regression see [12]). The covariance function used
d=1(xi d − xj d)2/l2
d) 
in this work is the stationary squared exponential kse(xi  xj) = σ2
where σ2
A formal deﬁnition of robustness is given  for example  in terms of an outlier-prone observation
model. The observation model is deﬁned to be outlier-prone of order n  if p(f|y1  ...  yn+1) →
p(f|y1  ...  yn) as yn+1 → ∞ [3  4]. That is  the effect of a single conﬂicting observation to the
posterior becomes asymptotically negligible as the observation approaches inﬁnity. This contrasts
heavily with the Gaussian observation model where each observation inﬂuences the posterior no
matter how far it is from the others. The zero-mean Student-t distribution

se is the scaling parameter and ld are the length-scales.

se exp(−(cid:80)D

p(yi|fi  σ  ν) =

Γ((ν + 1)/2)
√
Γ(ν/2)
νπσ

1 +

(yi − fi)2

νσ2

(cid:18)

(cid:19)−(ν+1)/2

where ν is the degrees of freedom and σ the scale parameter [13]  is outlier prone of order 1  and
it can reject up to m outliers if there are at least 2m observations in all [3]. From this on we will
collect all the hyperparameters into θ = {σ2

se  l1  ...  lD  σ  ν}.

2

 

(2)

3 Inference with the Laplace approximation

3.1 The conditional posterior of the latent variables

Our approach is motivated by the Laplace approximation in GP classiﬁcation [14]. A similar ap-
proximation has been considered by West [4] in the case of robust linear regression and by Rue
et al. [15] in their integrated nested Laplace approximation (INLA). Below we follow the notation
of Rasmussen and Williams [12].
A second order Taylor expansion of log p(f | y  θ) around the mode  gives a Gaussian approximation

p(f | y  θ) ≈ q(f | y  θ) = N(f |ˆf   Σ) 

where ˆf = arg maxf p(f | y  θ) and Σ−1 is the Hessian of the negative log conditional posterior at
the mode ˆf [12  13]:

Σ−1 = −∇∇ log p(f | y  θ)|f =ˆf = K-1

f f +W 

(3)

(4)

where

ri = (yi − fi)  and Wji = 0 if i (cid:54)= j.

i − νσ2
Wii = −(ν + 1) r2
i + νσ2)2  
(r2

(cid:90)

3.2 The maximum a posterior estimate of the hyperparameters
To ﬁnd a maximum a posterior estimate (MAP) for the hyperparameters  we write p(θ| y) ∝
p(y |θ)p(θ)  where

p(y |θ) =

p(y| f)p(f |X  θ)d f  

(5)
is the marginal likelihood. To ﬁnd an approximation  q(y |θ)  for the marginal likelihood one can
utilize the Laplace method second time [12]. A Taylor expansion of the logarithm of the integrand
in (5) around ˆf gives a Gaussian integral over f multiplied by a constant  giving
log | K-1

ˆf T K-1

log q(y |θ) = log p(y|ˆf) − 1
2

f f +W|.

(6)

ˆf − 1
2

log | Kf f | − 1
2

f f

The hyperparameters can then be optimized by maximizing the approximate log marginal posterior 
log q(θ| y) ∝ log q(y |θ) + log p(θ). This is differentiable with respect to θ  which enables the use
of gradient based optimization to ﬁnd ˆθ = arg maxθ q(θ| y) [12].

3.3 Making predictions

The approximate posterior distribution of a latent variable f∗ at a new input location x∗ is also
Gaussian  and therefore deﬁned by its mean and variance [12]

[f∗|X  y  x∗] = K∗ f K-1
[f∗|X  y  x∗] = K∗ ∗ − K∗ f(Kf f +W−1)−1 Kf ∗ .

ˆf = K∗ f ∇ log p(y |ˆf)

f f

(7)

(8)

E

q(f | y θ)

Var

q(f | y θ)

The predictive distribution of a new observation is obtained by marginalizing over the posterior
distribution of f∗

q(y∗|X  y  x∗) =

p(y∗|f∗)q(f∗|X  y  x∗)df∗ 

(9)

(cid:90)

which can be evaluated  for example  with a Gaussian quadrature integration.

3.4 Properties of the Laplace approximation

The Student-t distribution is not log-concave  and therefore the posterior distribution may be mul-
timodal. The immediate concern from this is that a unimodal Laplace approximation may give
a poor estimate for the posterior. This is  however  a problem for all unimodal approximations 

3

(a) Greater prior variance than likelihood variance

(b) Equal prior and likelihood variance

Figure 2: A comparison of the Laplace and VB approximation for p(f|θ  y) in the case of a single
observation with the Student-t likelihood and a Gaussian prior. The likelihood is centered at zero
and the prior mean is altered. The upper plots show the probability density functions and the lower
plots the variance of the true posterior and its approximations as a function of the posterior mean.

such as the VB in [7  11]. An other concern is that the estimate of the posterior precision 
Σ−1 = −∇∇ log p(f | y  θ)|f =ˆf   is essentially uncontrolled. However  at a posterior mode ˆf  the
Hessian Σ−1 is always positive deﬁnite and in practice approximates the truth rather well according
to our experiments. If the optimization for f ends up in a saddle point or the mode is very ﬂat  Σ−1
may be close to singular  which leads to problems in the implementation. In this section  we will
discuss these issues with simple examples and address the implementation in the section 4.
Consider a single observation yi = 0 from a Student-t distribution with a Gaussian prior for its
mean  fi. The behavior of the true posterior  the Laplace approximation  and VB as a function of
prior mean are illustrated in the upper plots of the Figure 2. The dotted lines represent the situation 
where the observation is a clear outlier in which case the posterior is very close to the prior (cf.
section 2). The solid lines represent a situation where the prior and data agree  and the dashed lines
represent a situation where the prior and data conﬂict moderately.
i + W (fi) > 0  for all fi ∈ (cid:60)  where τ 2
The posterior of the mean is unimodal if Σ(fi)−1 = τ−2
i is
(4)). With ν and σ ﬁxed  W (fi) reaches its (negative) minimum at |yi−fi| = ±√
the prior variance and W (fi) is the Hessian of the negative log likelihood at fi (see equations (3) and
3νσ  where Σ−1 =
i − (ν + 1)/(8νσ2). Therefore  the posterior distribution is unimodal if τ−2
τ−2
i > (ν + 1)/(8νσ2) 
or in terms of variances if Var[yi|fi  ν  σ]/τ 2
i > (ν + 1)/(8(ν − 2)) (for ν > 2). It follows that the
√
most problematic situation for the Laplace approximation is when the prior is much wider than the
likelihood. Then in the case of a moderate conﬂict (|yi − ˆfi| is close to
3νσ) the posterior may be
multimodal (see the Figure 2(a))  meaning that it is unclear whether the observation is an outlier or
not. In this case  W (fi) is negative and Σ−1 may be close to zero  which reﬂects uncertainty on the
location. In the implementation this may lead to numerical problems but in practice  the problem
becomes concrete only seldom as described in the section 4.
The negative values of W relate to a decrease in the posterior precision compared to the prior preci-
sion. As long as the total precision remains positive it approximates the behavior of the true posterior
rather well. The Student-t likelihood leads to a decrease in the variance from prior to posterior only
if the prior mean and the observation are consistent with each other as shown in the Figure 2. This
behavior is not captured with the factorized VB approximation [7]  where W in q(f |θ  y) is replaced
with a strictly positive diagonal that always increases the precision as illustrated in the Figure 2.

4

−15−10−50501latent value fp(f)  p(f|D)  p(y|f)  priorlikelihreal posteriorLaplace appVB approx−15−10−505024posterior mean of latent value fVar(f|D) Var(f)−15−10−5050latent value fp(f)  p(f|D)  p(y|f)−15−10−505posterior mean of latent value fVar(f|D) Var(f)0.30.64 On the implementation

4.1 Posterior mode of the latent variables
The mode of the latent variables  ˆf  can be found with general optimization methods such as the
scaled conjugate gradients. The most robust and efﬁcient method  however  proved to be the expec-
tation maximization (EM) algorithm that utilizes the scale mixture representation of the Student-t
distribution

yi|fi ∼ N(fi  Vi)

Vi ∼ Inv-χ2(ν  σ2)

(10)
(11)

where each observation has its own noise variance Vi that is Inv-χ2 distributed. Following Gelman
et al. [13]  p. 456 the E-step of the algorithm consists of evaluating the expectation

(cid:20) 1

Vi

(cid:12)(cid:12)(cid:12)yi  f old

i

E

(cid:21)

  ν  σ

=

ν + 1

νσ2 + (yi − f old
i )2

 

(12)

after which the latent variables are updated in the M-step as

ˆf new = (K-1

f f +V−1)−1V−1y 

(13)
where V−1 is a diagonal matrix of the expectations in (12). In practice  we do not invert Kf f and 
thus  ˆf is updated using the Woodbury-Sherman-Morrison [e.g. 16] lemma
ˆf new = (Kf f − Kf f V−1/2B−1V−1/2 Kf f)V−1y

(14)
where matrix B = I + V−1/2 Kf f V−1/2. This is numerically more stable than directly inverting
the covariance matrix  and gives as an intermediate result the vector a = K-1
f f

ˆf for later use.

= Kf f a

4.2 Approximate marginal likelihood

Rasmussen and Williams [12] discuss a numerically stable formulation to evaluate the approximate
marginal likelihood and its gradients with a classiﬁcation model. Their approach relies on W being
non-negative  for which reason it requires some modiﬁcation for our setting. With the Student-t
likelihood  we found the most stable formulation for (6) is

ˆf Ta − n(cid:88)

n(cid:88)

log Rii +

log Lii 

(15)

log q(y |θ) = log p(y|ˆf) − 1
2

i=1

i=1
f f +W)−1  and a is obtained
where R and L are the Cholesky decomposition of Kf f and Σ = (K-1
from the EM algorithm. The only problematic term is the last one  which is numerically unstable
if evaluated directly. We could evaluate ﬁrst Σ = Kf f − Kf f(W−1 + Kf f)−1 Kf f  but this is in
many cases even worse than the direct evaluation  since W−1 might have arbitrary large negative
values. For this reason  we evaluate LLT = Σ using a rank one Cholesky updates in a speciﬁc order.
After L is found it can also be used in the predictive variance (8) and in the gradients of (6) with
only minor modiﬁcation to equations given in [12]. We write ﬁrst the posterior covariance as

Σ = (K-1

f f +W)−1 = (K-1

f f +e1eT

1W11 + e2eT

2W22 + ...eneT

nWnn)−1 

(16)

where ei is the ith unit vector. The terms eieT
i Wii are added iteratively and the Cholesky decompo-
sition of Σ is updated accordingly. At the beginning L = chol(Kf f)  and at iteration step i+1 we
use the rank one Cholesky update to ﬁnd

L(i+1) = chol

where si is the ith column of Σ(i) and δi = Wii(Σ(i)
conduct a Cholesky downdate  and if Wii < 0 and (Σ(i)
which increases the covariance. The increase may be arbitrary large if (Σ(i)

 

L(i)(L(i))T − sisT
(17)
i δi
ii )−1 + Wii). If Wii is positive we
ii )−1/((Σ(i)
ii )−1 + Wii > 0 we have a Cholesky update
ii )−1 ≈ −Wii  but in

(cid:17)

(cid:16)

5

ii )−1 + Wii ≤ 0  since then the
practice it can be limited. Problems arise also if Wii < 0 and (Σ(i)
resulting Cholesky downdate is not positive deﬁnite. This should not happen if ˆf is at local maxima 
but in practice it may be in a saddle point or this happens because of numerical instability or the
iterative framework to update the Cholesky decomposition. The problem is prevented by adding the
diagonals in a decreasing order  that is  ﬁrst the ”normal” observations and last the outliers.
A single Cholesky update is analogous to the discussion in section 3.4 in that the posterior covariance
is updated using the result of the previous iteration as a prior. If we added the negative W values
ii )−1 + Wii ≤ 0 or
at the beginning  Σii  (the prior variance) could be so large that either (Σ(i)
(Σ(i)
could become singular or arbitrary
large and lead to problems in the later iterations (compare to the dashed black line in the Figure
2(a)). Adding ﬁrst the largest W we reduce Σ so that negative values of W are less problematic
(compare to the dashed black line in the Figure 2(b))  and the updates are numerically more stable.
ii )−1+Wii ≥ 0 that everything
During the Cholesky updates  we cross-check with the condition (Σ(i)
is ﬁne. If the condition is not fulﬁlled our code prints a warning and replaces Wii with −1/(2Σ(i)
ii ).
This ensures that the Cholesky update will remain positive deﬁnite and doubles the marginal vari-
ance instead. However  in practice we never encountered any warnings in our experiments if the
hyperparameters were initialized sensibly so that the prior was tight compared to the likelihood.

ii )−1 ≈ −Wii  in which case the posterior covariance Σ(i+1)

ii

5 Relation to other work

Neal [6] implemented the Student-t model for the Gaussian process via Markov chain Monte Carlo
utilizing the scale mixture representation. However  the most similar approaches to the Laplace
approximation are the VB approximation [7  11] and the one in INLA [15]. Here we will shortly
summarize them.
The difference between INLA and GP framework is that INLA utilizes Gaussian Markov random
ﬁelds (GMRF) in place of the Gaussian process. The Gaussian approximation for p(f | y  θ) in INLA
is the same as the Laplace approximation here with the covariance function replaced by a precision
matrix. Rue et al. [15] derive the approximation for the log marginal posterior  log p(θ| y)  from

p(θ| y) ≈ q(θ| y) ∝ p(y  f   θ)
q(f |θ  y)

= p(y | f)p(f |θ)p(θ)

q(f |θ  y)

(18)
The proportionality sign is due to the fact that the normalization constant for p(f   θ| y) is unknown.
This is exactly the same as the approximation derived in the section 3.2. Taking the logarithm of
(18) we end up in log q(θ| y) ∝ log q(y |θ) + log p(θ)  where log q(y |θ) is given in (6).
In the variational approximation [7]  the joint posterior of the latent variables and the scale param-
eters in the scale mixture representation (10)-(11) is approximated with a factorizing distribution
p(f   V| y  θ) ≈ q(f)q(V)  where q(f) = N(f |m  A) and q(V) = Πn
i=1Inv-χ2(Vii|˜ν/2  ˜σ2/2) 
where ˜θ = {m  A  ˜ν  ˜σ2} are the parameters of the variational approximation. The approximate
distributions and the hyperparameters are updated in turns so that ˜θ are updated with current esti-
mate for θ and after that θ is updated with ﬁxed ˜θ.
The variational approximation for the conditional posterior is p(f | y  ˆθ  ˆV) ≈ N(f |m  A). Here 
f f + ˆV−1)−1  and the iterative search for the posterior parameters m and A is the same as
A = (K-1

the EM algorithm described in section 4 except that the update of E(cid:2)V −1
E(cid:2)V −1

i )2). Thus  the Laplace and the variational approximation
are very similar. In practice  the posterior mode  m  is very close to the mode ˆf  and the main
difference between the approximations is in the covariance and the hyperparameter estimates.
(cid:20)
In the variational approximation ˆθ is searched by maximizing the variational lower bound
log p(y | f   V)p(f |θ)p(V|θ)p(θ)
V = Eq(f  V| y θ)
where we have made visible the implicit dependence of the approximations q(f) and q(V) to the
data and hyperparameters  and included prior for θ. The variational lower bound is similar to the ap-

ii +(yi− mold
(cid:21)

(cid:3) in (12) is replaced with

(cid:3) = (ν +1)/(σ2 + Aold

p(y  f   V  θ)

q(f | y  θ)q(V| y  θ)

q(f   V| y  θ)

(cid:20)

log

(cid:21)

 

(19)

ii

(cid:12)(cid:12)(cid:12)f =ˆf

(cid:12)(cid:12)(cid:12)f =ˆf

.

ii

= Eq(f  V| y θ)

6

Table 1: The RMSE and NLP statistics on the experiments.

G
T-lapl
T-vb
T-mcmc

Neal
0.393
0.028
0.029
0.055

0.324
0.220
0.220
0.253

The RMSE error

Friedman Housing Concrete

0.324
0.289
0.294
0.287

0.230
0.231
0.212
0.197

Neal
0.254
-2.181
-2.228
-1.907

The NLP statistics

Friedman Housing Concrete
0.0642
-0.116
-0.132
-0.241

0.227
-0.16
-0.049
-0.106

1.249
0.080
0.091
0.029

proximate log marginal posterior (18). Only the point estimate ˆf is replaced with averaging over the
approximating distribution q(f   V| y  θ). The other difference is that in the Laplace approximation
the scale parameters V are marginalized out and it approximates directly p(f | y  θ).

6 Experiments

We studied four data sets: 1) Neal data [6] with 100 data points and one input shown in Figure 1.
2) Friedman data with a nonlinear function of 10 inputs  from which we generated 10 data sets with
100 training points including 10 randomly selected outliers as described by Kuss [7]  p. 83. 3) The
Boston housing data that summarize median house prices in Boston metropolitan area for 506 data
points and 13 input variables [7]. 4) Concrete data that summarize the quality of concrete casting as
a function of 27 variables for 215 measurements [17]. In earlier experiments  the Student-t model
has worked better than the Gaussian observation model in all of these data sets.
The predictive performance is measured with a root mean squared error (RMSE) and a negative
log predictive density (NLP). With simulated data these are evaluated for a test set of 1000 latent
variables. With real data we use 10-fold cross-validation. The compared observation models are
Gaussian (G) and Student-t (T). The Student-t model is inferred using the Laplace approximation
(lapl)  VB (vb) [7] and full MCMC (mcmc) [6]. The Gaussian observation model  the Laplace
approximation and VB are evaluated at ˆθ  and in MCMC we sample θ. INLA is excluded from
the experiments since GMRF model can not be constructed naturally for these non-regularly dis-
tributed data sets. The results are summarized in the Table 1. The signiﬁcance of the differences in
performance is approximated using a Gaussian approximation for the distribution of the NLP and
RMSE statistics [17]. The Student-t model is signiﬁcantly better than the Gaussian with higher than
95% probability in all other tests but in the RMSE with the concrete data. There is no signiﬁcant
difference between the Laplace approximation  VB and MCMC.
The inference time was the shortest with Gaussian observation model and the longest with the
Student-t model utilizing full MCMC. The Laplace approximation for the Student-t likelihood took
in average 50% more time than the Gaussian model  and VB was in average 8-10 times slower than
the Laplace approximation. The reason for this is that in VB two sets of parameters  θ and ˜θ  are
updated in turns  which slows down the convergence of hyperparameters. In the Laplace approx-
imation we have to optimize only θ. Figure 3 shows the mean and the variance of p(f |ˆθ  y) for
MCMC versus the Laplace approximation and VB. The mean of the Laplace approximation and VB
match equally well the mean of the MCMC solution  but VB underestimates the variance more than
the Laplace approximation (see also the ﬁgure 2). In the housing data  both approximations under-
estimate the variance remarkably for few data points (40 of 506) that were located as clusters at
places where inputs  x are truncated along one or more dimension. At these locations  the marginal
posteriors were slightly skew and their tails were rather heavy  and thus a Gaussian approximation
presumably underestimates the variance.
The degrees of freedom of the Student-t likelihood were optimized only in Neal data and Boston
housing data using the Laplace approximation. In other data sets  there was not enough information
to infer ν and it was set to 4. Optimizing ν was more problematic for VB than for the Laplace
approximation probably because the factorized approximation makes it harder to identify ν. The
MAP estimates ˆθ found by the Laplace approximation and VB were slightly different. This is
reasonable since the optimized functions (18) and (19) are also different.

7

(a) Neal data

(b) Friedman data

(c) Boston housing data

(d) Concrete data

Figure 3: Scatter plot of the posterior mean and variance of the latent variables. Upper row consists
means  and lower row variances. In each ﬁgure  left plot is for MCMC (x-axis) vs the Laplace
approximation (y-axis) and the right plot is MCMC (x-axis) vs. VB (y-axis).

7 Discussion

In our experiments we found that the predictive performance of both the Laplace approximation and
the factorial VB is similar with the full MCMC. Compared to the MCMC the Laplace approximation
and VB estimate the posterior mean E[f |ˆθ  y] similarly but VB underestimates the posterior variance
Var[f |ˆθ  y] more than the Laplace approximation. Optimizing the hyperparameters is clearly faster
with the Laplace approximation than with VB.
Both the Laplace and the VB approximation estimate the posterior precision as a sum of a prior pre-
cision and a diagonal matrix. In VB the diagonal is strictly positive  whereas in the Laplace approx-
imation the diagonal elements corresponding to outlying observations are negative. The Laplace ap-
proximation is closer to the reality in that respect since the outlying observations have a negative ef-
fect on the (true) posterior precision. This happens because VB minimizes KL(q(f)q(V)||p(f   V)) 
which requires that the q(f   V) must be close to zero whenever p(f   V) is (see for example [18]).
Since a posteriori f and V are correlated  the marginal q(f) underestimates the effect of marginal-
izing over the scale parameters. The Laplace approximation  on the other hand  tries to estimate
directly the posterior p(f) of the latent variables. Recently  Opper and Archambeau [19] discussed
the relation between the Laplace approximation and VB  and proposed a variational approximation
directly for the latent variables and tried it with a Cauchy likelihood (they did not perform extensive
experiments though). Presumably their implementation would give better estimate for p(f) than the
factorized approximation. However  experiments on that respect are left for future.
The advantage of VB is that the objective function (19) is a rigorous lower bound for p(y |θ) 
whereas the Laplace approximation (18) is not. However  the marginal posteriors p(f | y  θ) in
our experiments (inferred with MCMC) were so close to Gaussian that the Laplace approximation
q(f |θ  y) should be very accurate and  thus  the approximation for p(θ| y) (18) should also be close
to the truth (see also justiﬁcations in [15]).
In recent years the expectation propagation (EP) algorithm [20] has been demonstrated to be very ac-
curate and efﬁcient method for approximate inference in many models with factorizing likelihoods.
However  the Student-t likelihood is problematic for EP since it is not log-concave  for which rea-
son EPs estimate for the posterior covariance may become singular during the site updates [21]. The
reason for this is that the variance parameters of the site approximations may become negative. As
demonstrated with Laplace approximation here  this reﬂects the behavior of the true posterior. We
assume that the problem can be overcome  but we are not aware of any work that would have solved
this problem.

Acknowledgments

This research was funded by the Academy of Finland  and the Graduate School in Electronics and
Telecommunications and Automation (GETA). The ﬁrst and second author thank also the Finnish
Foundation for Economic and Technology Sciences - KAUTE  Finnish Cultural Foundation  Emil
Aaltonen Foundation  and Finnish Foundation for Technology Promotion for supporting their post
graduate studies.

8

References
[1] Bruno De Finetti. The Bayesian approach to the rejection of outliers.

In Proceedings of
the fourth Berkeley Symposium on Mathematical Statistics and Probability  pages 199–210.
University of California Press  1961.

[2] A. Philip Dawid. Posterior expectations for large observations. Biometrika  60(3):664–667 

December 1973.

[3] Anthony O’Hagan. On outlier rejection phenomena in Bayes inference. Royal Statistical

Society. Series B.  41(3):358–367  1979.

[4] Mike West. Outlier models and prior distributions in Bayesian linear regression. Journal of

Royal Statistical Society. Serires B.  46(3):431–439  1984.

[5] John Geweke. Bayesian treatment of the independent Student-t linear model. Journal of

Applied Econometrics  8:519–540  1993.

[6] Radford M. Neal. Monte Carlo Implementation of Gaussian Process Models for Bayesian Re-
gression and Classiﬁcation. Technical Report 9702  Dept. of statistics and Dept. of Computer
Science  University of Toronto  January 1997.

[7] Malte Kuss. Gaussian Process Models for Robust Regression  Classiﬁcation  and Reinforce-

ment Learning. PhD thesis  Technische Universit¨at Darmstadt  2006.

[8] Paul W. Goldberg  Christopher K.I. Williams  and Christopher M. Bishop. Regression with
input-dependent noise: A Gaussian process treatment. In M. I. Jordan  M. J. Kearns  and S. A
Solla  editors  Advances in Neural Information Processing Systems 10. MIT Press  Cambridge 
MA  1998.

[9] Andrew Naish-Guzman and Sean Holden. Robust regression with twinned gaussian processes.
In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information
Processing Systems 20  pages 1065–1072. MIT Press  Cambridge  MA  2008.

[10] Oliver Stegle  Sebastian V. Fallert  David J. C. MacKay  and Søren Brage. Gaussian process
robust regression for noisy heart rate data. Biomedical Engineering  IEEE Transactions on  55
(9):2143–2151  September 2008. ISSN 0018-9294. doi: 10.1109/TBME.2008.923118.

[11] Michael E. Tipping and Neil D. Lawrence. Variational inference for Student-t models: Robust
bayesian interpolation and generalised component analysis. Neurocomputing  69:123–141 
2005.

[12] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine

Learning. The MIT Press  2006.

[13] Andrew Gelman  John B. Carlin  Hal S. Stern  and Donald B. Rubin. Bayesian Data Analysis.

Chapman & Hall/CRC  second edition  2004.

[14] Christopher K. I. Williams and David Barber. Bayesian classiﬁcation with Gaussian processes.

IEEE Transactions on Pattern Analysis and Machine Intelligence  20(12):1342–1351  1998.

[15] H˚avard Rue  Sara Martino  and Nicolas Chopin. Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations. Journal of Royal statis-
tical Society B  71(2):1–35  2009.

[16] David A. Harville. Matrix Algebra From a Statistician’s Perspective. Springer-Verlag  1997.
[17] Aki Vehtari and Jouko Lampinen. Bayesian model assessment and comparison using cross-

validation predictive densities. Neural Computation  14(10):2439–2468  2002.

[18] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer Science +Busi-

ness Media  LLC  2006.

[19] Manfred Opper and C´edric Archambeau. The variational Gaussian approximation revisited.

Neural Computation  21(3):786–792  March 2009.

[20] Thomas Minka. A family of algorithms for approximate Bayesian inference. PhD thesis 

Massachusetts Institute of Technology  2001.

[21] Matthias Seeger. Bayesian inference and optimal design for the sparse linear model. Journal

of Machine Learning Research  9:759–813  2008.

9

,Cong Han Lim
Stephen Wright