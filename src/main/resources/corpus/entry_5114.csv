2019,Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models,We characterize the effectiveness of a classical algorithm for recovering the Markov graph of a general discrete pairwise graphical model from i.i.d. samples. The algorithm is (appropriately regularized) maximum conditional log-likelihood  which involves solving a convex program for each node; for Ising models this is $\ell_1$-constrained logistic regression  while for more general alphabets an $\ell_{2 1}$ group-norm constraint needs to be used. We show that this algorithm can recover any arbitrary discrete pairwise graphical model  and also characterize its sample complexity as a function of model width  alphabet size  edge parameter accuracy  and the number of variables. We show that along every one of these axes  it matches or improves on all existing results and algorithms for this problem. Our analysis applies a sharp generalization error bound for logistic regression when the weight vector has an $\ell_1$ (or $\ell_{2 1}$) constraint and the sample vector has an $\ell_{\infty}$ (or $\ell_{2  \infty}$) constraint. We also show that the proposed convex programs can be efficiently solved in $\tilde{O}(n^2)$ running time (where $n$ is the number of variables) under the same statistical guarantees. We provide experimental results to support our analysis.,Sparse Logistic Regression Learns

All Discrete Pairwise Graphical Models

Shanshan Wu  Sujay Sanghavi  Alexandros G. Dimakis

Department of Electrical and Computer Engineering

shanshan@utexas.edu  sanghavi@mail.utexas.edu  dimakis@austin.utexas.edu

University of Texas at Austin

Abstract

We characterize the effectiveness of a classical algorithm for recovering the Markov
graph of a general discrete pairwise graphical model from i.i.d. samples. The algo-
rithm is (appropriately regularized) maximum conditional log-likelihood  which
involves solving a convex program for each node; for Ising models this is (cid:96)1-
constrained logistic regression  while for more general alphabets an (cid:96)2 1 group-
norm constraint needs to be used. We show that this algorithm can recover any
arbitrary discrete pairwise graphical model  and also characterize its sample com-
plexity as a function of model width  alphabet size  edge parameter accuracy  and
the number of variables. We show that along every one of these axes  it matches
or improves on all existing results and algorithms for this problem. Our analysis
applies a sharp generalization error bound for logistic regression when the weight
vector has an (cid:96)1 (or (cid:96)2 1) constraint and the sample vector has an (cid:96)∞ (or (cid:96)2 ∞)
constraint. We also show that the proposed convex programs can be efﬁciently
solved in ˜O(n2) running time (where n is the number of variables) under the same
statistical guarantees. We provide experimental results to support our analysis.

1

Introduction

Undirected graphical models provide a framework for modeling high dimensional distributions with
dependent variables and have many applications including in computer vision (Choi et al.  2010)  bio-
informatics (Marbach et al.  2012)  and sociology (Eagle et al.  2009). In this paper we characterize
the effectiveness of a natural  and already popular  algorithm for the structure learning problem.
Structure learning is the task of ﬁnding the dependency graph of a Markov random ﬁeld (MRF)
given i.i.d. samples; typically one is also interested in ﬁnding estimates for the edge weights as
well. We consider the structure learning problem in general (non-binary) discrete pairwise graphical
models. These are MRFs where the variables take values in a discrete alphabet  but all interactions are
pairwise. This includes the Ising model as a special case (which corresponds to a binary alphabet).
The natural and popular algorithm we consider is (appropriately regularized) maximum conditional
log-likelihood for ﬁnding the neighborhood set of any given node. For the Ising model  this becomes
(cid:96)1-constrained logistic regression; more generally for non-binary graphical models the regularizer
becomes an (cid:96)2 1 norm. We show that this algorithm can recover all discrete pairwise graphical
models  and characterize its sample complexity as a function of the parameters of interest: model
width  alphabet size  edge parameter accuracy  and the number of variables. We match or improve
dependence on each of these parameters  over all existing results for the general alphabet case when
no additional assumptions are made on the model (see Table 1). For the speciﬁc case of Ising models 
some recent work has better dependence on some parameters (see Table 2 in Appendix A).
We now describe the related work  and then outline our contributions.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Sample complexity (N)

O(exp( kO(d) exp(O(d2λ))

ηO(1)

) ln( nk

ρ ))

Assumptions
1. Alphabet size k ≥ 2
2. Model width ≤ λ
3. Degree ≤ d
4. Minimum edge weight ≥ η > 0
5. Probability of success ≥ 1 − ρ
1. Alphabet size k ≥ 2
2. Model width ≤ λ
3. Minimum edge weight ≥ η > 0
4. Probability of success ≥ 1 − ρ

η4

η4

Paper

Greedy
algorithm (Hamilton
et al.  2017)

O( λ2k5 exp(14λ)

Sparsitron (Klivans
and Meka  2017)
(cid:96)2 1-constrained
logistic regression
(this paper)
Table 1: Sample complexity comparison for different graph recovery algorithms. The pairwise
graphical model has alphabet size k. For k = 2 (i.e.  Ising models)  our algorithm reduces to the
(cid:96)1-constrained logistic regression (see Table 2 in Appendix A for related work on learning Ising
models). Our sample complexity has a better dependency on the alphabet size ( ˜O(k4) versus ˜O(k5))
than that in (Klivans and Meka  2017)2.

O( λ2k4 exp(14λ)

ln( nk

ρη ))

ln( nk

ρ ))

Related Work

In a classic paper  Ravikumar et al. (2010) considered the structure learning problem for Ising models.
They showed that (cid:96)1-regularized logistic regression provably recovers the correct dependency graph
with a very small number of samples by solving a convex program for each variable. This algorithm
was later generalized to multi-class logistic regression with (cid:96)2 1 group sparse regularization  for
learning MRFs with higher-order interactions and non-binary variables (Jalali et al.  2011). A well-
known limitation of (Ravikumar et al.  2010; Jalali et al.  2011) is that their theoretical guarantees
only work for a restricted class of models. Speciﬁcally  they require that the underlying learned model
satisﬁes technical incoherence assumptions  that are difﬁcult to validate or check.
A large amount of recent work has since proposed various algorithms to obtain provable learning
results for general graphical models without requiring the incoherence assumptions. We now describe
the (most related part of the extensive) related work  followed by our results and comparisons (see
Table 1). For a discrete pairwise graphical model  let n be the number of variables and k be the
alphabet size; deﬁne the model width λ as the maximum neighborhood weight (see Deﬁnition 1
and 2 for the precise deﬁnition). For structure learning algorithms  a popular approach is to focus
on the sub-problem of ﬁnding the neighborhood of a single node. Once this is correctly learned  the
overall graph structure is a simple union bound. Indeed all the papers we now discuss are of this
type. As shown in Table 1  Hamilton et al. (2017) proposed a greedy algorithm to learn pairwise (and
higher-order) MRFs with general alphabet. Their algorithm generalizes the approach of Bresler (2015)
to learning Ising models. The sample complexity in (Hamilton et al.  2017) grows logarithmically
in n  but doubly exponentially in the width λ (only single exponential is necessary (Santhanam
and Wainwright  2012)). Klivans and Meka (2017) provided a different algorithmic and theoretical
approach by setting this up as an online learning problem and leveraging results from the Hedge
algorithm therein. Their algorithm Sparsitron achieves single-exponential dependence on λ.
Our Contributions
• Our main result: We show that a classical algorithm (cid:96)2 1-constrained3 logistic regression can
recover the edge weights of a discrete pairwise graphical model from i.i.d. samples (see Theorem 2).
For the special case of Ising models (see Theorem 1)  this reduces to an (cid:96)1-constrained logistic
regression. For the general setting with non-binary alphabet  since each edge has a group of
parameters  it is natural to use an (cid:96)2 1 group sparse constraint to enforce sparsity at the level of

2Theorem 8.4 in (Klivans and Meka  2017) has a typo. The correct dependence should be k5 instead of k3.
In Section 8 of (Klivans and Meka  2017)  after re-writing the conditional distribution as a sigmoid function  the
weight vector w is a vector of length (n − 1)k + 1. Their derivation uses an incorrect bound (cid:107)w(cid:107)1 ≤ 2λ  while
it should be (cid:107)w(cid:107)1 ≤ 2kλ. This gives rise to an additional k2 factor on the ﬁnal sample complexity.

3It may be possible to prove a similar result for the regularized version of the optimization problem using
techniques from (Negahban et al.  2012). One needs to prove that the objective function satisﬁes restricted strong
convexity (RSC) when the samples are from a graphical model distribution (Vuffray et al.  2016; Lokhov et al. 
2018). It would be interesting to see if the proof presented in this paper is related to the RSC condition.

2

groups. We make no incoherence assumption on the graphical models. As shown in Table 1 
our sample complexity scales as ˜O(k4)  which improves4 the previous best result with ˜O(k5)
dependency5. Our analysis applies a sharp generalization error bound for logistic regression when
the weight vector has an (cid:96)2 1 (or (cid:96)1) constraint and the sample vector has an (cid:96)2 ∞ (or (cid:96)∞) constraint
(see Lemma 8 and 11 in Appendix E). Our key insight is that a generalization bound can be used to
control the squared distance between the predicted and true logistic functions (see Lemma 1 and 2
in Appendix B)  which then implies an (cid:96)∞ norm bound between the weight vectors (see Lemma 5
and 6 in Appendix B).
• We show that the proposed algorithms can run in ˜O(n2) time without affecting the statistical
guarantees (see Section 2.3). Note that ˜O(n2) is an efﬁcient runtime for graph recovery over n
nodes. Previous algorithms in (Hamilton et al.  2017; Klivans and Meka  2017) also require ˜O(n2)
runtime for structure learning of pairwise graphical models.
• We construct examples that violate the incoherence condition proposed in (Ravikumar et al.  2010)
(see Figure 1). We then run (cid:96)1-constrained logistic regression and show that it can recover the
graph structure as long as given enough samples. This veriﬁes our analysis and shows that our
conditions for graph recovery are weaker than those in (Ravikumar et al.  2010).
• We empirically compare the proposed algorithm with the Sparsitron algorithm in (Klivans and
Meka  2017) over different alphabet sizes  and show that our algorithm needs fewer samples for
graph recovery (see Figure 2).

denote its i-th coordinate. The (cid:96)p norm of a vector is deﬁned as (cid:107)x(cid:107)p = ((cid:80)
dot product between two vectors (cid:104)x  y(cid:105) =(cid:80)
i xiyi or two matrices (cid:104)A  B(cid:105) =(cid:80)

Notation. We use [n] to denote the set {1  2 ···   n}. For a vector x ∈ Rn  we use xi or x(i) to
i |xi|p)1/p. We use
x−i ∈ Rn−1 to denote the vector after deleting the i-th coordinate. For a matrix A ∈ Rn×k  we use
Aij or A(i  j) to denote its (i  j)-th entry. We use A(i  :) ∈ Rk and A(:  j) ∈ Rn to the denote the i-th
row vector and the j-th column vector. The (cid:96)p q norm of a matrix A ∈ Rn×k is deﬁned as (cid:107)A(cid:107)p q =
(cid:107)[(cid:107)A(1  :)(cid:107)p  ... (cid:107)A(n  :)(cid:107)p](cid:107)q. We deﬁne (cid:107)A(cid:107)∞ = maxij |A(i  j)|. We use (cid:104)· ·(cid:105) to represent the
ij A(i  j)B(i  j).

2 Main results

We start with the special case of binary variables (i.e.  Ising models)  and then move to the general
case with non-binary variables.

2.1 Learning Ising models

We ﬁrst give a deﬁnition of an Ising model distribution.
Deﬁnition 1. Let A ∈ Rn×n be a symmetric weight matrix with Aii = 0 for i ∈ [n]. Let θ ∈ Rn be
a mean-ﬁeld vector. The n-variable Ising model is a distribution D(A  θ) on {−1  1}n that satisﬁes

P

Z∼D(A θ)

[Z = z] ∝ exp

Aijzizj +

θizi

The dependency graph of D(A  θ) is an undirected graph G = (V  E)  with vertices V = [n] and
edges E = {(i  j) : Aij (cid:54)= 0}. Deﬁne the width of D(A  θ) as

(1)

(2)

1≤i<j≤n

 (cid:88)
(cid:88)

j∈[n]

 .

i∈[n]

(cid:88)
 .

λ(A  θ) = max
i∈[n]

|Aij| + |θi|

Let η(A  θ) be the minimum edge weight  i.e.  η(A  θ) = min(i j)∈E |Aij|.

4This improvement essentially comes from the fact that we are using an (cid:96)2 1 norm constraint instead of
an (cid:96)1 norm constraint for learning general (i.e.  non-binary) pairwise graphical models (see our remark after
Theorem 2). The Sparsitron algorithm proposed by Klivans and Meka (2017) learns a (cid:96)1-constrained generalized
linear model. This (cid:96)1-constraint gives rise to a k5 dependency for learning non-binary pairwise graphical models.
5In an independent and concurrent work  Vuffray et al. (2019) generalize the Interaction Screening algo-

rithm (Vuffray et al.  2016). Their sample complexity is O(k4(cid:98)γ4 exp(12λ) ln(nk)/η4) for learning pairwise
non-binary graphical models (see Corollary 4 in their paper)  where(cid:98)γ is an upper bound on the (cid:96)1 norm of the
node-wise weight vectors. Since(cid:98)γ can scale as k2λ  their dependence on k can be much worse than ours.

3

exp((cid:80)

exp((cid:80)

j(cid:54)=i Aijxj + θi) + exp(−(cid:80)

j(cid:54)=i Aijxj + θi)

One property of an Ising model distribution is that the conditional distribution of any variable given
the rest variables follows a logistic function. Let σ(z) = 1/(1 + e−z) be the sigmoid function.
Fact 1. Let Z ∼ D(A  θ) and Z ∈ {−1  1}n. For any i ∈ [n]  the conditional probability of the i-th
variable Zi ∈ {−1  1} given the states of all other variables Z−i ∈ {−1  1}n−1 is

= σ((cid:104)w  x(cid:48)(cid:105)) 

P[Zi = 1|Z−i = x] =

j(cid:54)=i Aijxj − θi)

(3)
where x(cid:48) = [x  1] ∈ {−1  1}n  and w = 2[Ai1 ···   Ai(i−1)  Ai(i+1) ···   Ain  θi] ∈ Rn. Moreover 
w satisﬁes (cid:107)w(cid:107)1 ≤ 2λ(A  θ)  where λ(A  θ) is the model width deﬁned in Deﬁnition 1.
Following Fact 1  the natural approach to estimating the edge weights Aij is to solve a logistic
regression problem for each variable. For ease of notation  let us focus on the n-th variable (the
algorithm directly applies to the rest variables). Given N i.i.d. samples {z1 ···   zN}  where
zi ∈ {−1  1}n from an Ising model D(A  θ)  we ﬁrst transform the samples into {(xi  yi)}N
i=1 
n ∈ {−1  1}. By Fact 1  we know that
where xi = [zi
P[yi = 1|xi = x] = σ((cid:104)w∗  x(cid:105)) where w∗ = 2[An1 ···   An(n−1)  θn] ∈ Rn satisﬁes (cid:107)w∗(cid:107)1 ≤
2λ(A  θ). Suppose that λ(A  θ) ≤ λ  we are then interested in recovering w∗ by solving the following
(cid:96)1-constrained logistic regression problem

n−1  1] ∈ {−1  1}n and yi = zi

1 ···   zi

(4)

ˆw ∈ arg min
1
w∈Rn
N
where (cid:96) : R → R is the loss function

(cid:96)(yi(cid:10)w  xi(cid:11)) = ln(1 + e−yi(cid:104)w xi(cid:105)) =

i=1

s.t. (cid:107)w(cid:107)1 ≤ 2λ 

(cid:26)− ln σ((cid:10)w  xi(cid:11)) 
− ln(1 − σ((cid:10)w  xi(cid:11))) 

if yi = 1
if yi = −1
Eq. (5) is essentially the negative log-likelihood of observing yi given xi at the current w.
Let ˆw be a minimizer of (4). It is worth noting that in the high-dimensional regime (N < n)  ˆw may
not be unique. In this case  we will show that any one of them would work. After solving the convex
problem in (4)  the edge weight is estimated as ˆAnj = ˆwj/2.
The pseudocode of the above algorithm is given in Algorithm 1. Solving the (cid:96)1-constrained logistic
regression problem will give us an estimator of the true edge weight. We then form the graph by
keeping the edge that has estimated weight larger than η/2 (in absolute value).

(5)

N(cid:88)

(cid:96)(yi(cid:10)w  xi(cid:11))

3

Algorithm 1: Learning an Ising model via (cid:96)1-constrained logistic regression
Input: N i.i.d. samples {z1 ···   zN}  where zm ∈ {−1  1}n for m ∈ [N ]; an upper bound on
Output: ˆA ∈ Rn×n  and an undirected graph ˆG on n nodes.
1 for i ← 1 to n do
2

λ(A  θ) ≤ λ; a lower bound on η(A  θ) ≥ η > 0.

∀m ∈ [N ]  xm ← [zm−i  1]  ym ← zm
ˆw ← arg minw∈Rn
s.t. (cid:107)w(cid:107)1 ≤ 2λ
∀j ∈ [n]  ˆAij ← ˆw˜j/2  where ˜j = j if j < i and ˜j = j − 1 if j > i

(cid:80)N
m=1 ln(1 + e−ym(cid:104)w xm(cid:105))

4
5 end
6 Form an undirected graph ˆG on n nodes with edges {(i  j) : | ˆAij| ≥ η/2  i < j}.
Theorem 1. Let D(A  θ) be an unknown n-variable Ising model distribution with dependency graph
G. Suppose that the D(A  θ) has width λ(A  θ) ≤ λ. Given ρ ∈ (0  1) and  > 0  if the number
of i.i.d. samples satisﬁes N = O(λ2 exp(12λ) ln(n/ρ)/4)  then with probability at least 1 − ρ 
Algorithm 1 produces ˆA that satisﬁes

1
N

i

|Aij − ˆAij| ≤ .

max
i j∈[n]

(6)

Corollary 1. In the setup of Theorem 1  suppose that the Ising model distribution D(A  θ) has
minimum edge weight η(A  θ) ≥ η > 0. If we set  < η/2 in (6)  which corresponds to sample
complexity N = O(λ2 exp(12λ) ln(n/ρ)/η4)  then with probability at least 1 − ρ  Algorithm 1
recovers the dependency graph  i.e.  ˆG = G.

4

2.2 Learning pairwise graphical models over general alphabet
Deﬁnition 2. Let k be the alphabet size. Let W = {Wij ∈ Rk×k : i (cid:54)= j ∈ [n]} be a set of weight
matrices satisfying Wij = W T
ji. Without loss of generality  we assume that every row (and column)
vector of Wij has zero mean. Let Θ = {θi ∈ Rk : i ∈ [n]} be a set of external ﬁeld vectors. Then the
n-variable pairwise graphical model D(W  Θ) is a distribution over [k]n where

P

Z∼D(W Θ)

[Z = z] ∝ exp

Wij(zi  zj) +

θi(zi)

(7)

The dependency graph of D(W  Θ) is an undirected graph G = (V  E)  with vertices V = [n] and
edges E = {(i  j) : Wij (cid:54)= 0}. The width of D(W  Θ) is deﬁned as

 .

(cid:88)

i∈[n]

 .

1≤i<j≤n

 (cid:88)
(cid:88)

λ(W  Θ) = max

|Wij(a  b)| + |θi(a)|

(8)

i a

j(cid:54)=i

b Wij(a  b)/k and θ(cid:48)

b Wij(a  b) = 0 and(cid:80)

ij(a  b) = Wij(a  b) −(cid:80)

b M (a  b) = 0 ∀a ∈ [k]} and {M ∈ Rk×k :(cid:80)

(i.e. (cid:80)
Fact 8.2 in (Klivans and Meka  2017)). If the a-th row of Wij is not centered  i.e. (cid:80)
i(a) = θi(a) +(cid:80)
(i.e.  {M ∈ Rk×k :(cid:80)

max
b∈[k]
We deﬁne η(W  Θ) = min(i j)∈E maxa b |Wij(a  b)|.
Remark (centered rows and columns). The assumption that Wij has centered rows and columns
a Wij(a  b) = 0 for any a  b ∈ [k]) is without loss of generality (see
b Wij(a  b) (cid:54)= 0 
we can deﬁne W (cid:48)
b Wij(a  b)/k  and
notice that D(W  Θ) = D(W(cid:48)  Θ(cid:48)). Because the sets of matrices with centered rows and columns
a M (a  b) = 0 ∀b ∈ [k]}) are
two linear subspaces  alternatively projecting Wij onto the two sets will converge to the intersection
of the two subspaces (Von Neumann  1949). As a result  the condition of centered rows and
columns is necessary for recovering the underlying weight matrices  since otherwise different
parameters can give the same distribution. Note that in the case of k = 2  Deﬁnition 2 is the same
as Deﬁnition 1 for Ising models. To see their connection  simply deﬁne Wij ∈ R2×2 as follows:
Wij(1  1) = Wij(2  2) = Aij  Wij(1  2) = Wij(2  1) = −Aij.
For a pairwise graphical model distribution D(W  Θ)  the conditional distribution of any variable
(when restricted to a pair of values) given all the other variables follows a logistic function  as shown
in Fact 2. This is analogous to Fact 1 for the Ising model distribution.
Fact 2. Let Z ∼ D(W  Θ) and Z ∈ [k]n. For any i ∈ [n]  any α (cid:54)= β ∈ [k]  and any x ∈ [k]n−1 
(9)

(Wij(α  xj) − Wij(β  xj)) + θi(α) − θi(β)).

P[Zi = α|Zi ∈ {α  β}  Z−i = x] = σ(

(cid:88)

j(cid:54)=i

Given N i.i.d. samples {z1 ···   zN}  where zm ∈ [k]n ∼ D(W  Θ) for m ∈ [N ]  the goal is to
estimate matrices Wij for all i (cid:54)= j ∈ [n]. For ease of notation and without loss of generality  let us
consider the n-th variable. Now the goal is to estimate matrices Wnj for all j ∈ [n − 1].
To use Fact 2  ﬁx a pair of values α (cid:54)= β ∈ [k]  let S be the set of samples satisfying zn ∈ {α  β}.
We next transform the samples in S to {(xt  yt)}|S|
t=1 as follows: xt = OneHotEncode([zt−n  1]) ∈
n = β. Here OneHotEncode(·) : [k]n → {0  1}n×k
{0  1}n×k  yt = 1 if zt
is a function that maps a value t ∈ [k] to the standard basis vector et ∈ {0  1}k  where et has a single
1 at the t-th entry. For each sample (x  y) in the set S  Fact 2 implies that P[y = 1|x] = σ((cid:104)w∗  x(cid:105)) 
where w∗ ∈ Rn×k satisﬁes

n = α  and yt = −1 if zt

w∗(j  :) = Wnj(α  :) − Wnj(β  :) ∀j ∈ [n − 1]; w∗(n  :) = [θn(α) − θn(β)  0  ...  0].

(10)
Suppose that the width of D(W  Θ) satisﬁes λ(W  Θ) ≤ λ  then w∗ deﬁned in (10) satisﬁes
√
(cid:107)w∗(cid:107)2 1 ≤ 2λ
j(cid:107)w∗(j  :)(cid:107)2. We can now form an (cid:96)2 1-constrained logistic
regression over the samples in S:

ln(1 + e−yt(cid:104)w xt(cid:105))

s.t. (cid:107)w(cid:107)2 1 ≤ 2λ

√

k.

(11)

k  where (cid:107)w∗(cid:107)2 1 :=(cid:80)
|S|(cid:88)

wα β ∈ arg min
w∈Rn×k

1
|S|

t=1

5

(cid:88)

a∈[k]

(cid:88)

1
k

Let wα β be a minimizer of (11). We then create a new matrix U α β ∈ Rn×k by centering the ﬁrst
n − 1 rows of wα β:

U α β(j  b) = wα β(j  b) − 1
k

wα β(j  a)  ∀j ∈ [n − 1]  ∀b ∈ [k];

(12)

U α β(n  b) = wα β(n  b) +

wα β(j  a)  ∀b ∈ [k].

(cid:10)U α β  x(cid:11) =(cid:10)wα β  x(cid:11)  which implies that U α β is also a minimizer of (11).

Since each row of the x matrix in (11) is a standard basis vector (i.e.  all zeros except a single one) 

j∈[n−1] a∈[k]

The key step in our proof (see (30) in Appendix D) is to show that given enough samples  the obtained
U α β ∈ Rn×k matrix is close to w∗ deﬁned in (10). Speciﬁcally  we will prove that

|Wnj(α  b) − Wnj(β  b) − U α β(j  b)| ≤  

(13)
Recall that our goal is to estimate the original matrices Wnj for all j ∈ [n − 1]. Summing (13) over

∀j ∈ [n − 1]  ∀α  β  b ∈ [k].

β ∈ [k] and using the fact that(cid:80)
(cid:88)
In other words  ˆWnj(α  :) =(cid:80)

|Wnj(α  b) − 1
k

β∈[k]

β Wnj(β  b) = 0 gives
U α β(j  b)| ≤  

∀j ∈ [n − 1]  ∀α  b ∈ [k].

(14)

β∈[k] U α β(j  :)/k is a good estimate of Wnj(α  :).

The above algorithm is given in Algorithm 2. Suppose that η(W  Θ) ≥ η  once we obtain the estimates
ˆWij  the last step is to form a graph by keeping the edge (i  j) that satisﬁes maxa b | ˆWij(a  b)| ≥ η/2.

bound on λ(W  Θ) ≤ λ; a lower bound on η(W  Θ) ≥ η > 0.

Algorithm 2: Learning a pairwise graphical model via (cid:96)2 1-constrained logistic regression
Input: alphabet size k; N i.i.d. samples {z1 ···   zN}  where zm ∈ [k]n for m ∈ [N ]; an upper
Output: ˆWij ∈ Rk×k for all i (cid:54)= j ∈ [n]; an undirected graph ˆG on n nodes.
1 for i ← 1 to n do
2
3
4

for each pair α (cid:54)= β ∈ [k] do
i ∈ {α  β}}
S ← {zm  m ∈ [N ] : zm
(cid:80)|S|
i = α; yt ← −1 if zt
∀zt ∈ S  xt ← OneHotEncode([zt−i  1])  yt ← 1 if zt
√
i = β
t=1 ln(1 + e−yt(cid:104)w xt(cid:105))
wα β ← arg minw∈Rn×k
s.t. (cid:107)w(cid:107)2 1 ≤ 2λ
Deﬁne U α β ∈ Rn×k by centering the ﬁrst n − 1 rows of wα β (see (12)).

end
for j ∈ [n]\i and α ∈ [k] do

6
7
8
9
10
11 end
12 Form graph ˆG on n nodes with edges {(i  j) : maxa b | ˆWij(a  b)| ≥ η/2  i < j}.

(cid:80)
β∈[k] U α β(˜j  :)  where ˜j = j if j < i and ˜j = j − 1 if j > i.

ˆWij(α  :) ← 1

end

1|S|

k

5

k

Theorem 2. Let D(W  Θ) be an n-variable pairwise graphical model distribution with width
λ(W  Θ) ≤ λ. Given ρ ∈ (0  1) and  > 0  if the number of i.i.d. samples satisﬁes N =
O(λ2k4 exp(14λ) ln(nk/ρ)/4)  then with probability at least 1 − ρ  Algorithm 2 produces ˆWij ∈
Rk×k that satisﬁes

|Wij(a  b) − ˆWij(a  b)| ≤  

∀i (cid:54)= j ∈ [n]  ∀a  b ∈ [k].

(15)

Corollary 2. In the setup of Theorem 2  suppose that the pairwise graphical model distribution
D(W  Θ) satisﬁes η(W  Θ) ≥ η > 0. If we set  < η/2 in (15)  which corresponds to sample
complexity N = O(λ2k4 exp(14λ) ln(nk/ρ)/η4)  then with probability at least 1 − ρ  Algorithm 2
recovers the dependency graph  i.e.  ˆG = G.

6

√

√

Remark ((cid:96)2 1 versus (cid:96)1 constraint). The w∗ ∈ Rn×k matrix deﬁned in (10) satisﬁes (cid:107)w∗(cid:107)2 1 ≤
k and (cid:107)w∗(cid:107)1 ≤ 2λk. Instead of solving the (cid:96)2 1-constrained logistic regression deﬁned in
√
2λ
(11)  we could solve an (cid:96)1-constrained logistic regression with (cid:107)w(cid:107)1 ≤ 2λk. This additional
k
k) will lead to a worse sample complexity ˜O(k5) .
dependence in the constraint (i.e.  2λk versus 2λ
Remark (dependence on the alphabet size). A simple lower bound of the sample complexity is
Ω(k2). To see why  consider a graph with two nodes (i.e.  n = 2). Let W be a k-by-k weight matrix
between the two nodes  deﬁned as follows: W (1  1) = W (2  2) = 1  W (1  2) = W (2  1) = −1 
and W (i  j) = 0 otherwise. This deﬁnition satisﬁes the condition that every row and column is
centered (Deﬁnition 2). Besides  we have λ = 1 and η = 1  which means that the two quantities
do not scale in k. To distinguish W from the zero matrix  we need to observe samples in the set
{(1  1)  (2  2)  (1  2)  (2  1)}. This requires Ω(k2) samples because any speciﬁc sample (a  b) (where
a ∈ [k] and b ∈ [k]) has a probability of approximately 1/k2 to show up.

2.3 Learning pairwise graphical models in ˜O(n2) time

Our results so far assume that the (cid:96)1-constrained logistic regression (in Algorithm 1) and the (cid:96)2 1-
constrained logistic regression (in Algorithm 2) is exactly solved. This would require ˜O(n4) com-
plexity if an interior-point based method is used (Koh et al.  2007). Our key result in this section is
Theorem 3  which says that the statistical guarantees in Theorem 1 and 2 still hold if the constrained
logistic regression is only approximately solved.
Theorem 3 (Informal). Suppose that the constrained logistic regression in Algorithm 1 and 2 is
optimized by the mirror descent method given in Appendix J. Given ρ ∈ (0  1) and  > 0  if the
number of mirror descent iterations satisﬁes T = O(λ2k3 exp(O(λ)) ln(n)/4)  then (6) and (15)
still hold with probability at least 1 − ρ. The time and space complexity of Algorithm 1 is O(T N n2)
and O(T N +n2). The time and space complexity of Algorithm 2 is O(T N n2k2) and O(T N +n2k2).

Proof of Theorem 3 requires bounding (cid:107) ¯w − w∗(cid:107)∞ (where ¯w is the value after T mirror descent
iterations). This is non-trivial because we are in the high-dimensional regime as the number of
samples N = O(ln(n))  the empirical loss functions in (11) and (4) are not strongly convex. Due to
the space limit  more details of this section can be found in Appendix I  J and K.
Note that ˜O(n2) is an efﬁcient time complexity for graph recovery over n nodes. Previous structural
learning algorithms of Ising models require either ˜O(n2) complexity (Bresler  2015; Klivans and
Meka  2017) or a worse time complexity (Ravikumar et al.  2010; Vuffray et al.  2016).
It is possible to improve the time complexity given in Theorem 3 (especially the dependence on
 and λ)  by using stochastic or accelerated versions of mirror descent algorithms (instead of the
batch version given in Appendix J). In fact  the Sparsitron algorithm proposed by Klivans and Meka
(2017) can be seen as an online mirror descent algorithm for optimizing the (cid:96)1-constrained logistic
regression (see Algorithm 3 in Appendix J). Furthermore  Algorithm 1 and 2 can be parallelized as
every node has an independent regression problem.
We would like to remark that our goal here is not to give the fastest ﬁrst-order optimization algorithm.
Instead  our goal is to provably show that it is possible to run Algorithm 1 and Algorithm 2 in ˜O(n2)
time without affecting the original statistical guarantees.

3 Proof outline

L(w) = E(x y)∼D ln(1 + e−y(cid:104)w x(cid:105)) and ˆL(w) =(cid:80)N

We give a proof outline for Theorem 1. The proof of Theorem 2 follows a similar outline. Let D be
a distribution over {−1  1}n × {−1  1}  where (x  y) ∼ D satisﬁes P[y = 1|x] = σ((cid:104)w∗  x(cid:105)). Let
i=1 ln(1 + e−yi(cid:104)w xi(cid:105))/N be the expected and
empirical logistic loss. Suppose (cid:107)w∗(cid:107)1 ≤ 2λ. Let ˆw ∈ arg minw
ˆL(w) s.t. (cid:107)w(cid:107)1 ≤ 2λ. Our goal is
to prove that (cid:107) ˆw − w∗(cid:107)∞ is small when the samples are constructed from an Ising model distribution.
Our proof can be summarized in three steps:

7

1. If the number of samples satisﬁes N = O(λ2 ln(n/ρ)/γ2)  then L( ˆw) − L(w∗) ≤ O(γ).
This is obtained using a sharp generalization bound when (cid:107)w(cid:107)1 ≤ 2λ and (cid:107)x(cid:107)∞ ≤ 1 (see
Lemma 8 in Appendix E).

2. For any w  we show that L(w)−L(w∗) ≥ Ex[σ((cid:104)w  x(cid:105))−σ((cid:104)w∗  x(cid:105))]2 (see Lemma 10 and
Lemma 9 in Appendix E). Hence  Step 1 implies that Ex[σ((cid:104) ˆw  x(cid:105)) − σ((cid:104)w∗  x(cid:105))]2 ≤ O(γ)
(see Lemma 1 in Appendix B).

3. We now use a result from (Klivans and Meka  2017) (see Lemma 5 in Appendix B) 
which says that if the samples are from an Ising model and if γ = O(2 exp(−6λ))  then
Ex[σ((cid:104) ˆw  x(cid:105)) − σ((cid:104)w∗  x(cid:105))]2 ≤ O(γ) implies that (cid:107) ˆw − w∗(cid:107)∞ ≤ . The required number
of samples is N = O(λ2 ln(n/ρ)/γ2) = O(λ2 exp(12λ) ln(n/ρ)/4).

For the general setting with non-binary alphabet (i.e.  Theorem 2)  the proof is similar to that
√
of Theorem 1. The main difference is that we need to use a sharp generalization bound when
k and (cid:107)x(cid:107)2 ∞ ≤ 1 (see Lemma 11 in Appendix E). This would give us Lemma 2 in
(cid:107)w(cid:107)2 1 ≤ 2λ
Appendix B which bounds the squared distance between the two sigmoid functions. The last step is
to use Lemma 6 to bound the inﬁnity norm between the two weight matrices.

4 Experiments

In both of the simulations below  the external ﬁeld is set to be zero. Sampling is done via exactly
computing the distribution. We implement the algorithm in Matlab. All experiments are done using a
personal desktop. Source code can be found at https://github.com/wushanshan/GraphLearn.
Learning Ising models. In Figure 1 we construct a diamond-shape graph and plot the incoherence
value at Node 1. This value becomes bigger than 1 (and hence violates the incoherence condition
in (Ravikumar et al.  2010)) when we increase the graph size n and edge weight a. We then run 100
times of Algorithm 1 and plot the fraction of runs that exactly recovers the underlying graph structure.
In each run we generate a different set of samples. The result shown in Figure 1 is consistent with our
analysis and also indicates that our conditions for graph recovery are weaker than those in (Ravikumar
et al.  2010).

Figure 1: Left: The graph structure used in this simulation. It has n nodes and 2(n − 2) edges. Every
edge has the same weight a > 0. Middle: Incoherence value at Node 1. The incoherence condition
required by (Ravikumar et al.  2010) is violated for n ≥ 10 and a ≥ 0.2. Right: We simulate 100
runs of Algorithm 1 for edge weight a = 0.2 across different n values.

Learning general pairwise graphical models. We compare our algorithm (Algorithm 2) with the
Sparsitron algorithm in (Klivans and Meka  2017) on a two-dimensional 3-by-3 grid (shown in
Figure 2). We test two alphabet sizes in the experiments: k = 4  6. For each value of k  we simulate
both algorithms 100 runs  and in each run we generate random Wij matrices with entries ±0.2.
To ensure that each row (as well as each column) of Wij is centered (Deﬁnition 2)  we randomly
choose Wij between two options: for example  if k = 2  then Wij = [0.2 −0.2;−0.2  0.2] or
Wij = [−0.2  0.2; 0.2 −0.2]. As shown in the Figure 2  our algorithm requires fewer samples for
successfully recovering the graphs. More details about this experiment can be found in Appendix L.

8

…123n-2n-1naaaaaaaa68101214Number of nodes (n)0.511.52Incoherence at Node 1a=0.15a=0.20a=0.251000200030004000Number of samples (N)00.51Prob succ in 100 runsn=6n=8n=10n=12n=14Figure 2: Left: A two-dimensional 3-by-3 grid graph used in the simulation. Middle and right: Our
algorithm needs fewer samples than the Sparsitron algorithm (Klivans and Meka  2017) for graph
recovery.

5 Conclusion

The main contribution of this paper is to show that an existing and popular algorithm (i.e.  group-sparse
regularized logistic regression) actually gives the state-of-the-art performance (in a setting where
alternative algorithms are being proposed). Speciﬁcally  we have shown that the (cid:96)2 1-constrained
logistic regression can recover the Markov graph of any discrete pairwise graphical model from i.i.d.
samples. For the special case of Ising model  the algorithm reduces to the (cid:96)1-constrained logistic
regression. This algorithm has better sample complexity than the previous state-of-the-art result (k4
versus k5)  and can be efﬁciently optimized in ˜O(n2) time. One interesting direction for future work
is to see if the 1/η4 dependency in the sample complexity can be improved. It is also interesting to
see a thorough empirical evaluation of different structural learning algorithms.
Another interesting direction is to consider MRFs with higher-order interactions. Intuitively  it should
not be difﬁcult to prove that (cid:96)1-constrained logistic regression can recover the structure of binary
t-wise MRFs. One can prove it by combining results from Section 7 of (Klivans and Meka  2017) and
the following fact: the Sparsitron algorithm can be viewed as an online mirror descent algorithm that
approximately solves an (cid:96)1-constrained logistic regression. This observation is actually the starting
point of our paper. For higher-order MRFs with non-binary alphabet  we conjecture that similar result
can be proved for group-sparse regularized logistic regression. Extending the current proof/method
to higher-order MRFs is deﬁnitely an interesting direction for future research.

6 Acknowledgements

This research has been supported by NSF Grants 1302435  1564000  and 1618689  DMS 1723052 
CCF 1763702  AF 1901292 and research gifts by Google  Western Digital and NVIDIA.

References
Agarwal  A.  Negahban  S.  and Wainwright  M. J. (2010). Fast global convergence rates of gradient
methods for high-dimensional statistical recovery. In Advances in Neural Information Processing
Systems  pages 37–45.

Aurell  E. and Ekeberg  M. (2012). Inverse ising inference using all the data. Physical review letters 

108(9):090201.

Banerjee  O.  Ghaoui  L. E.  and d’Aspremont  A. (2008). Model selection through sparse maximum
likelihood estimation for multivariate gaussian or binary data. Journal of Machine learning
research  9(Mar):485–516.

Bartlett  P. L. and Mendelson  S. (2002). Rademacher and gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3(Nov):463–482.

Ben-Tal  A. and Nemirovski  A. (Fall 2013). Lectures on modern convex optimization. https:

//www2.isye.gatech.edu/~nemirovs/Lect_ModConvOpt.pdf.

9

40006000800010000Number of samples (N)0.60.70.80.91Prob succ in 100 runsAlphabet size (k) = 4Our method[KM17]2461040.40.60.81k = 6Our method[KM17]Bento  J. and Montanari  A. (2009). Which graphical models are difﬁcult to learn? In Advances in

Neural Information Processing Systems  pages 1303–1311.

Bresler  G. (2015). Efﬁciently learning ising models on arbitrary graphs. In Proceedings of the

forty-seventh annual ACM symposium on Theory of computing (STOC)  pages 771–782. ACM.
Bubeck  S. (2015). Convex optimization: Algorithms and complexity. Foundations and Trends R(cid:13) in

Machine Learning  8(3-4):231–357.

Choi  M. J.  Lim  J. J.  Torralba  A.  and Willsky  A. S. (2010). Exploiting hierarchical context on
a large database of object categories. In Computer vision and pattern recognition (CVPR)  2010
IEEE conference on  pages 129–136. IEEE.

Eagle  N.  Pentland  A. S.  and Lazer  D. (2009). Inferring friendship network structure by using

mobile phone data. Proceedings of the national academy of sciences  106(36):15274–15278.

Hamilton  L.  Koehler  F.  and Moitra  A. (2017). Information theoretic properties of markov random
ﬁelds  and their algorithmic applications. In Advances in Neural Information Processing Systems 
pages 2463–2472.

Jalali  A.  Ravikumar  P.  Vasuki  V.  and Sanghavi  S. (2011). On learning discrete graphical models
using group-sparse regularization. In Proceedings of the Fourteenth International Conference on
Artiﬁcial Intelligence and Statistics  pages 378–387.

Kakade  S. M.  Shalev-Shwartz  S.  and Tewari  A. (2012). Regularization techniques for learning

with matrices. Journal of Machine Learning Research  13(Jun):1865–1890.

Kakade  S. M.  Sridharan  K.  and Tewari  A. (2009). On the complexity of linear prediction: Risk
bounds  margin bounds  and regularization. In Advances in neural information processing systems 
pages 793–800.

Klivans  A. R. and Meka  R. (2017). Learning graphical models using multiplicative weights. In
Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science (FOCS) 
pages 343–354. IEEE.

Koh  K.  Kim  S.-J.  and Boyd  S. (2007). An interior-point method for large-scale (cid:96)1-regularized

logistic regression. Journal of Machine learning research  8(Jul):1519–1555.

Lee  S.-I.  Ganapathi  V.  and Koller  D. (2007). Efﬁcient structure learning of markov networks using

l_1-regularization. In Advances in neural Information processing systems  pages 817–824.

Lokhov  A. Y.  Vuffray  M.  Misra  S.  and Chertkov  M. (2018). Optimal structure and parameter

learning of ising models. Science advances  4(3):e1700791.

Marbach  D.  Costello  J. C.  Küffner  R.  Vega  N. M.  Prill  R. J.  Camacho  D. M.  Allison  K. R. 
Consortium  T. D.  Kellis  M.  Collins  J. J.  and Stolovitzky  G. (2012). Wisdom of crowds for
robust gene network inference. Nature methods  9(8):796.

Negahban  S. N.  Ravikumar  P.  Wainwright  M. J.  and Yu  B. (2012). A uniﬁed framework for
high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science 
27(4):538–557.

Ravikumar  P.  Wainwright  M. J.  and Lafferty  J. D. (2010). High-dimensional ising model selection

using (cid:96)1-regularized logistic regression. The Annals of Statistics  38(3):1287–1319.

Rigollet  P. and Hütter  J.-C. (Spring 2017). Lectures notes on high dimensional statistics. http:

//www-math.mit.edu/~rigollet/PDFs/RigNotes17.pdf.

Santhanam  N. P. and Wainwright  M. J. (2012). Information-theoretic limits of selecting binary
graphical models in high dimensions. IEEE Transactions on Information Theory  58(7):4117–4134.

Shalev-Shwartz  S. and Ben-David  S. (2014). Understanding machine learning: From theory to

algorithms. Cambridge university press.

10

Von Neumann  J. (1949). On rings of operators. reduction theory. Annals of Mathematics  pages

401–485.

Vuffray  M.  Misra  S.  Lokhov  A.  and Chertkov  M. (2016). Interaction screening: Efﬁcient and
sample-optimal learning of ising models. In Advances in Neural Information Processing Systems 
pages 2595–2603.

Vuffray  M.  Misra  S.  and Lokhov  A. Y. (2019). Efﬁcient learning of discrete graphical models.

arXiv preprint arXiv:1902.00600.

Yang  E.  Allen  G.  Liu  Z.  and Ravikumar  P. K. (2012). Graphical models via generalized linear

models. In Advances in Neural Information Processing Systems  pages 1358–1366.

Yuan  M. and Lin  Y. (2007). Model selection and estimation in the gaussian graphical model.

Biometrika  94(1):19–35.

11

,Shanshan Wu
Sujay Sanghavi
Alexandros Dimakis