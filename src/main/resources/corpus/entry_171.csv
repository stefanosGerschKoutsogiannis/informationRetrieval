2016,A Probabilistic Framework for Deep Learning,We develop a probabilistic framework for deep learning based on the Deep Rendering Mixture Model (DRMM)  a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. We demonstrate that max-sum inference in the DRMM yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (DCNs)  providing a first principles derivation. Our framework provides new insights into the successes and shortcomings of DCNs as well as a principled route to their improvement. DRMM training via the Expectation-Maximization (EM) algorithm is a powerful alternative to DCN back-propagation  and initial training results are promising. Classification based on the DRMM and other variants outperforms DCNs in supervised digit classification  training 2-3x faster while achieving similar accuracy. Moreover  the DRMM is applicable to semi-supervised and unsupervised learning tasks  achieving results that are state-of-the-art in several categories on the MNIST benchmark and comparable to state of the art on the CIFAR10 benchmark.,A Probabilistic Framework for Deep Learning

Ankit B. Patel

Baylor College of Medicine  Rice University

ankitp@bcm.edu abp4@rice.edu

Tan Nguyen
Rice University
mn15@rice.edu

Richard G. Baraniuk

Rice University
richb@rice.edu

Abstract

We develop a probabilistic framework for deep learning based on the Deep Render-
ing Mixture Model (DRMM)  a new generative probabilistic model that explicitly
capture variations in data due to latent task nuisance variables. We demonstrate
that max-sum inference in the DRMM yields an algorithm that exactly reproduces
the operations in deep convolutional neural networks (DCNs)  providing a ﬁrst
principles derivation. Our framework provides new insights into the successes and
shortcomings of DCNs as well as a principled route to their improvement. DRMM
training via the Expectation-Maximization (EM) algorithm is a powerful alternative
to DCN back-propagation  and initial training results are promising. Classiﬁcation
based on the DRMM and other variants outperforms DCNs in supervised digit
classiﬁcation  training 2-3⇥ faster while achieving similar accuracy. Moreover  the
DRMM is applicable to semi-supervised and unsupervised learning tasks  achiev-
ing results that are state-of-the-art in several categories on the MNIST benchmark
and comparable to state of the art on the CIFAR10 benchmark.

Introduction

1
Humans are adept at a wide array of complicated sensory inference tasks  from recognizing objects
in an image to understanding phonemes in a speech signal  despite signiﬁcant variations such as
the position  orientation  and scale of objects and the pronunciation  pitch  and volume of speech.
Indeed  the main challenge in many sensory perception tasks in vision  speech  and natural language
processing is a high amount of such nuisance variation. Nuisance variations complicate perception
by turning otherwise simple statistical inference problems with a small number of variables (e.g. 
class label) into much higher-dimensional problems. The key challenge in developing an inference
algorithm is then how to factor out all of the nuisance variation in the input. Over the past few decades 
a vast literature that approaches this problem from myriad different perspectives has developed  but
the most difﬁcult inference problems have remained out of reach.
Recently  a new breed of machine learning algorithms have emerged for high-nuisance inference
tasks  achieving super-human performance in many cases. A prime example of such an architecture
is the deep convolutional neural network (DCN)  which has seen great success in tasks like visual
object recognition and localization  speech recognition and part-of-speech recognition.
The success of deep learning systems is impressive  but a fundamental question remains: Why do they
work? Intuitions abound to explain their success. Some explanations focus on properties of feature
invariance and selectivity developed over multiple layers  while others credit raw computational
power and the amount of available training data. However  beyond these intuitions  a coherent
theoretical framework for understanding  analyzing  and synthesizing deep learning architectures has
remained elusive.
In this paper  we develop a new theoretical framework that provides insights into both the successes
and shortcomings of deep learning systems  as well as a principled route to their design and improve-
ment. Our framework is based on a generative probabilistic model that explicitly captures variation
due to latent nuisance variables. The Rendering Mixture Model (RMM) explicitly models nuisance
variation through a rendering function that combines task target variables (e.g.  object class in an

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

object recognition) with a collection of task nuisance variables (e.g.  pose). The Deep Rendering
Mixture Model (DRMM) extends the RMM in a hierarchical fashion by rendering via a product of
afﬁne nuisance transformations across multiple levels of abstraction. The graphical structures of the
RMM and DRMM enable efﬁcient inference via message passing (e.g.  using the max-sum/product
algorithm) and training via the expectation-maximization (EM) algorithm. A key element of our
framework is the relaxation of the RMM/DRMM generative model to a discriminative one in order to
optimize the bias-variance tradeoff. Below  we demonstrate that the computations involved in joint
MAP inference in the relaxed DRMM coincide exactly with those in a DCN.
The intimate connection between the DRMM and DCNs provides a range of new insights into how
and why they work and do not work. While our theory and methods apply to a wide range of different
inference tasks (including  for example  classiﬁcation  estimation  regression  etc.) that feature a
number of task-irrelevant nuisance variables (including  for example  object and speech recognition) 
for concreteness of exposition  we will focus below on the classiﬁcation problem underlying visual
object recognition. The proofs of several results appear in the Appendix.

2 Related Work
Theories of Deep Learning. Our theoretical work shares similar goals with several others such
as the i-Theory [1] (one of the early inspirations for this work)  Nuisance Management [24]  the
Scattering Transform [6]  and the simple sparse network proposed by Arora et al. [2].
Hierarchical Generative Models. The DRMM is closely related to several hierarchical models 
including the Deep Mixture of Factor Analyzers [27] and the Deep Gaussian Mixture Model [29].
Like the above models  the DRMM attempts to employ parameter sharing  capture the notion of
nuisance transformations explicitly  learn selectivity/invariance  and promote sparsity. However 
the key features that distinguish the DRMM approach from others are: (i) The DRMM explicitly
models nuisance variation across multiple levels of abstraction via a product of afﬁne transformations.
This factorized linear structure serves dual purposes: it enables (ii) tractable inference (via the max-
sum/product algorithm)  and (iii) it serves as a regularizer to prevent overﬁtting by an exponential
reduction in the number of parameters. Critically  (iv) inference is not performed for a single variable
of interest but instead for the full global conﬁguration of nuisance variables. This is justiﬁed in low-
noise settings. And most importantly  (v) we can derive the structure of DCNs precisely  endowing
DCN operations such as the convolution  rectiﬁed linear unit  and spatial max-pooling with principled
probabilistic interpretations. Independently from our work  Soatto et al. [24] also focus strongly on
nuisance management as the key challenge in deﬁning good scene representations. However  their
work considers max-pooling and ReLU as approximations to a marginalized likelihood  whereas our
work interprets those operations differently  in terms of max-sum inference in a speciﬁc probabilistic
generative model. The work on the number of linear regions in DCNs [14] is complementary to our
own  in that it sheds light on the complexity of functions that a DCN can compute. Both approaches
could be combined to answer questions such as: How many templates are required for accurate
discrimination? How many samples are needed for learning? We plan to pursue these questions in
future work.
Semi-Supervised Neural Networks. Recent work in neural networks designed for semi-supervised
learning (few labeled data  lots of unlabeled data) has seen the resurgence of generative-like ap-
proaches  such as Ladder Networks [17]  Stacked What-Where Autoencoders (SWWAE) [31] and
many others. These network architectures augment the usual task loss with one or more regularization
term  typically including an image reconstruction error  and train jointly. A key difference with our
DRMM-based approach is that these networks do not arise from a proper probabilistic density and as
such they must resort to learning the bottom-up recognition and top-down reconstruction weights
separately  and they cannot keep track of uncertainty.

3 The Deep Rendering Mixture Model: Capturing Nuisance Variation
Although we focus on the DRMM in this paper  we deﬁne and explore several other interesting
variants  including the Deep Rendering Factor Model (DRFM) and the Evolutionary DRMM (E-
DRMM)  both of which are discussed in more detail in [16] and the Appendix. The E-DRMM
is particularly important  since its max-sum inference algorithm yields a decision tree of the type
employed in a random decision forest classiﬁer[5].

2

A

Rendering Mixture Model

a

I

g

c

Rendering Factor Model

z

I

a

g

c

B

Deep Rendering 
Mixture Model

C
Model

Deep Sparse Path 

cL

zL

gL

zL1

gL1

...

z1

I

g1

Figure 1: Graphical model depiction of (A) the Shallow Rendering Models and (B) the DRMM. All
dependence on pixel location x has been suppressed for clarity. (C) The Sparse Sum-over-Paths
formulation of the DRMM. A rendering path contributes only if it is active (green arrows).

3.1 The (Shallow) Rendering Mixture Model
The RMM is a generative probabilistic model for images that explicitly models the relationship
between images I of the same object c subject to nuisance g 2G   where G is the set of all nuisances
(see Fig. 1A for the graphical model depiction).

c ⇠ Cat({⇡c}c2C) 

g ⇠ Cat({⇡g}g2G) 
I = aµcg + noise.

a ⇠ Bern({⇡a}a2A) 

(1)
Here  µcg is a template that is a function of the class c and the nuisance g. The switching variable
a 2A = {ON  OFF} determines whether or not to render the template at a particular patch; a
sparsity prior on a thus encourages each patch to have a few causes. The noise distribution is from the
exponential family  but without loss of generality we illustrate below using Gaussian noise N (0  21).
We assume that the noise is i.i.d. as a function of pixel location x and that the class and nuisance
variables are independently distributed according to categorical distributions. (Independence is
merely a convenience for the development; in practice  g can depend on c.) Finally  since the world is
spatially varying and an image can contain a number of different objects  it is natural to break the
image up into a number of patches  that are centered on a single pixel x. The RMM described in (1)
then applies at the patch level  where c  g  and a depend on pixel/patch location x. We will omit the
dependence on x when it is clear from context.
Inference in the Shallow RMM Yields One Layer of a DCN. We now connect the RMM with the
computations in one layer of a deep convolutional network (DCN). To perform object recognition
with the RMM  we must marginalize out the nuisance variables g and a. Maximizing the log-posterior
over g 2G and a 2A and then choosing the most likely class yields the max-sum classiﬁer

ˆc(I) = argmax

c2C

max
g2G

max
a2A

ln p(I|c  g  a) + ln p(c  g  a)

(2)

max
g2G

max
a2A

that computes the most likely global conﬁguration of target and nuisance variables for the image.
Assuming that Gaussian noise is added to the template  the image is normalized so that kIk2 = 1 
and c  g are uniformly distributed  (2) becomes
ReLu(hwcg|Ii + bcg) + b0 (3)
ˆc(I) ⌘ argmax
where ReLU(u) ⌘ (u)+ = max{u  0} is the soft-thresholding operation performed by the rec-
tiﬁed linear units in modern DCNs. Here we have reparameterized the RMM model from the
moment parameters ✓ ⌘{ 2  µcg ⇡ a} to the natural parameters ⌘(✓) ⌘{ wcg ⌘ 1
2 µcg  bcg ⌘
p(a=0)⌘. The relationships ⌘(✓) are referred to as the
 1
22kµcgk2
generative parameter constraints.

2  ba ⌘ ln p(a) = ln ⇡a  b0 ⌘ ln⇣ p(a=1)

a(hwcg|Ii + bcg) + ba = argmax

max
g2G

c2C

c2C

3

We now demonstrate that the sequence of operations in the max-sum classiﬁer in (3) coincides exactly
with the operations involved in one layer of a DCN: image normalization  linear template matching 
thresholding  and max pooling. First  the image is normalized (by assumption). Second  the image is
ﬁltered with a set of noise-scaled rendered templates wcg. If we assume translational invariance in
the RMM  then the rendered templates wcg yield a convolutional layer in a DCN [10] (see Appendix
Lemma A.2). Third  the resulting activations (log-probabilities of the hypotheses) are passed through
a pooling layer; if g is a translational nuisance  then taking the maximum over g corresponds to max
pooling in a DCN. Fourth  since the switching variables are latent (unobserved)  we max-marginalize
over them during classiﬁcation. This leads to the ReLU operation (see Appendix Proposition A.3).
3.2 The Deep Rendering Mixture Model: Capturing Levels of Abstraction
Marginalizing over the nuisance g 2G in the RMM is intractable for modern datasets  since G will
contain all conﬁgurations of the high-dimensional nuisance variables g. In response  we extend the
RMM into a hierarchical Deep Rendering Mixture Model (DRMM) by factorizing g into a number of
different nuisance variables g(1)  g(2)  . . .   g(L) at different levels of abstraction. The DRMM image
generation process starts at the highest level of abstraction (` = L)  with the random choice of the
object class c(L) and overall nuisance g(L). It is then followed by random choices of the lower-level
details g(`) (we absorb the switching variable a into g for brevity)  progressively rendering more
concrete information level-by-level (` ! `1)  until the process ﬁnally culminates in a fully rendered
D-dimensional image I (` = 0). Generation in the DRMM takes the form:
(4)
c(L) ⇠ Cat({⇡c(L)})  g(`) ⇠ Cat({⇡g(`)}) 8` 2 [L]
µc(L)g ⌘ ⇤gµc(L) ⌘ ⇤(1)
(5)
g(L)µc(L)
(6)
where the latent variables  parameters  and helper variables are deﬁned in full detail in Appendix B.
The DRMM is a deep Gaussian Mixture Model (GMM) with special constraints on the latent variables.
Here  c(L) 2C L and g(`) 2G `  where CL is the set of target-relevant nuisance variables  and G` is the
set of all target-irrelevant nuisance variables at level `. The rendering path is deﬁned as the sequence
(c(L)  g(L)  . . .   g(`)  . . .   g(1)) from the root (overall class) down to the individual pixels at ` = 0.
µc(L)g is the template used to render the image  and ⇤g ⌘Q` ⇤g(`) represents the sequence of local

I ⇠N (µc(L)g  ⌘ 21D) 

g(2) ··· ⇤(L1)

g(L1)⇤(L)

g(1)⇤(2)

p a(1)
p

p a(L)

··· (1)

g(`) is an afﬁne transformation with a bias term ↵(`)

perspective on the DRMM  as each pixel intensity Ix =Pp (L)
every switch on the path is active i.e.Q` a(`)

nuisance transformations that partially render ﬁner-scale details as we move from abstract to concrete.
Note that each ⇤(`)
g(`) that we have suppressed for
clarity. Fig. 1B illustrates the corresponding graphical model. As before  we have suppressed the
dependence of g(`) on the pixel location x(`) at level ` of the hierarchy.
Sum-Over-Paths Formulation of the DRMM. We can rewrite the DRMM generation process
by expanding out the matrix multiplications into scalar products. This yields an interesting new
is the sum over all
active paths to that pixel  of the product of weights along that path. A rendering path p is active iff
p = 1 . While exponentially many possible rendering
paths exist  only a very small fraction  controlled by the sparsity of a  are active. Fig. 1C depicts the
sum-over-paths formulation graphically.
Recursive and Nonnegative Forms. We can rewrite the DRMM into a recursive form as z(`) =
⇤(`+1)
g(`+1)z(`+1)  where z(L) ⌘ µc(L) and z(0) ⌘ I. We refer to the helper latent variables z(`) as
intermediate rendered templates. We also deﬁne the Nonnegative DRMM (NN-DRMM) as a DRMM
with an extra nonnegativity constraint on the intermediate rendered templates  z(`)  08` 2 [L].
The latter is enforced in training via the use of a ReLu operation in the top-down reconstruction
phase of inference. Throughout the rest of the paper  we will focus on the NN-DRMM  leaving the
unconstrained DRMM for future work. For brevity  we will drop the NN preﬁx.
Factor Model. We also deﬁne and explore a variant of the DRMM that where the top-level latent
variable is Gaussian: z(L+1) ⇠N (0  1d) 2 Rd and the recursive generation process is otherwise
identical to the DRMM: z(`) =⇤ (`+1)
g(`+1)z(`+1) where g(L+1) ⌘ c(L). We call this the Deep Rendering
Factor Model (DRFM). The DRFM is closely related to the Spike-and-Slab Sparse Coding model
[22]. Below we explore some training results  but we leave most of the exploration for future work.
(see Fig. 3 in Appendix C for architecture of the RFM  the shallow version of the DRFM)

p

4

Number of Free Parameters. Compared to the shallow RMM  which has D |CL|Q` |G`| parameters 
the DRMM has onlyP` |G`+1|D`D`+1 parameters  an exponential reduction in the number of free
parameters (Here GL+1 ⌘C L and D` is the number of units in the `-th layer with D0 ⌘ D). This
enables efﬁcient inference  learning  and better generalization. Note that we have assumed dense
(fully connected) ⇤g’s here; if we impose more structure (e.g. translation invariance)  the number of
parameters will be further reduced.
Bottom-Up Inference. As in the shallow RMM  given an input image I the DRMM classiﬁer infers
the most likely global conﬁguration {c(L)  g(`)}  ` = 0  1  . . .   L by executing the max-sum/product
message passing algorithm in two stages: (i) bottom-up (from ﬁne-to-coarse) to infer the overall class
label ˆc(L) and (ii) top-down (from coarse-to-ﬁne) to infer the latent variables ˆg(`) at all intermediate
levels `. First  we will focus on the ﬁne-to-coarse pass since it leads directly to DCNs.
Using (3)  the ﬁne-to-coarse NN-DRMM inference algorithm for inferring the most likely cateogry
ˆcL is given by

argmax
c(L)2C

max
g2G

µT
c(L)

µT
c(L)gI = argmax
c(L)2C
µT
c(L) max
g(L)2GL

= argmax
c(L)2C

max
g2G
⇤T
g(L) ··· max
g(1)2G1

⇤T

g(`)I

1Y`=L
|

⌘ I 1

{z

}

⇤T
g(1)|I

= ··· ⌘ argmax
c(L)2C

µT
c(L)I (L).

(7)

|

⌘W (`+1)

{z

}

Here  we have assumed the bias terms ↵g(`) = 0. In the second line  we used the max-product
algorithm (distributivity of max over products i.e. for a > 0  max{ab  ac} = a max{b  c}). See
Appendix B for full details. This enables us to rewrite (7) recursively:

I (`+1) ⌘ max

g(`+1)2G`+1

(⇤g(`+1))T

I (`) = MaxPool(ReLu(Conv(I (`)))) 

(8)

n

.

where I (`) is the output feature maps of layer `  I (0) ⌘ I and W (`) are the ﬁlters/weights for layer `.
Comparing to (3)  we see that the `-th iteration of (7) and (8) corresponds to feedforward propagation
in the `-th layer of a DCN. Thus a DCN’s operation has a probabilistic interpretation as ﬁne-to-coarse
inference of the most probable conﬁguration in the DRMM.
Top-Down Inference. A unique contribution of our generative model-based approach is that we have
a principled derivation of a top-down inference algorithm for the NN-DRMM (Appendix B). The
resulting algorithm amounts to a simple top-down reconstruction term ˆIn =⇤ ˆgnµˆc(L)
Discriminative Relaxations: From Generative to Discriminative Classiﬁers. We have con-
structed a correspondence between the DRMM and DCNs  but the mapping is not yet complete.
In particular  recall the generative constraints on the weights and biases. DCNs do not have such
constraints — their weights and biases are free parameters. As a result  when faced with training data
that violates the DRMM’s underlying assumptions  the DCN will have more freedom to compensate.
In order to complete our mapping from the DRMM to DCNs  we relax these parameter constraints 
allowing the weights and biases to be free and independent parameters. We refer to this process as a
discriminative relaxation of a generative classiﬁer ([15  4]  see the Appendix D for details).
3.3 Learning the Deep Rendering Model via the Expectation-Maximization (EM) Algorithm
We describe how to learn the DRMM parameters from training data via the hard EM algorithm in
Algorithm 1. The DRMM E-Step consists of bottom-up and top-down (reconstruction) E-steps at
each layer ` in the model. The ncg ⌘ p(c  g|In; ✓) are the responsibilities  where for brevity we have
absorbed a into g. The DRMM M-step consists of M-steps for each layer ` in the model. The per-layer
M-step in turn consists of a responsibility-weighted regression  where GLS(yn ⇠ xn) denotes the
solution to a generalized Least Squares regression problem that predict targets yn from predictors

xn and is closely related to the SVD. The Iversen bracket is deﬁned asJbK ⌘ 1 if expression b is

true and is 0 otherwise. There are several interesting and useful features of the EM algorithm. First 
we note that it is a derivative-free alternative to the back propagation algorithm for training that is
both intuitive and potentially much faster (provided a good implementation for the GLS problem).
Second  it is easily parallelized over layers  since the M-step updates each layer separately (model
parallelism). Moreover  it can be extended to a batch version so that at each iteration the model is

5

Algorithm 1 Hard EM and EG Algorithms for the DRMM

E-step:

M-step:

G-step:

ˆcn  ˆgn = argmax

ncg

c g

ˆ⇤g(`) = GLS|{z}⇣I (`1)

⇠ ˆz(`)
ˆ⇤g(`) / r⇤g(`) `DRMM (✓)

n

n | g(`) = ˆg(`)

n ⌘ 8g(`)

simultaneously updated using separate subsets of the data (data parallelism). This will enable training
to be distributed easily across multiple machines. In this vein  our EM algorithm shares several
features with the ADMM-based Bregman iteration algorithm in [28]. However  the motivation there is
from an optimization perspective and so the resulting training algorithm is not derived from a proper
probabilistic density. Third  it is far more interpretable via its connections to (deep) sparse coding
and to the hard EM algorithm for GMMs. The sum-over-paths formulation makes it particularly clear
that the mixture components are paths (from root to pixels) in the DRMM.
G-step. For the training results in this paper  we use the Generalized EM algorithm wherein we
replace the M-step with a gradient descent based G-step (see Algorithm 1). This is useful for
comparison with backpropagation-based training and for ease of implementation.
Flexibility and Extensibility. Since we can choose different priors/types for the nuisances g  the
larger DRMM family could be useful for modeling a wider range of inputs  including scenes  speech
and text. The EM algorithm can then be used to train the whole system end-to-end on different
sources/modalities of labeled and unlabeled data. Moreover  the capability to sample from the model
allows us to probe what is captured by the DRMM  providing us with principled ways to improve the
model. And ﬁnally  in order to properly account for noise/uncertainty  it is possible in principle to
extend this algorithm into a soft EM algorithm. We leave these interesting extensions for future work.

3.4 New Insights into Deep Convnets
DCNs are Message Passing Networks. The convolution  Max-Pooling and ReLu operations in a
DCN correspond to max-sum/product inference in a DRMM. Note that by “max-sum-product” we
mean a novel combination of max-sum and max-product as described in more detail in the proofs in
the Appendix. Thus  we see that architectures and layer types commonly used in today’s DCNs can
be derived from precise probabilistic assumptions that entirely determine their structure. The DRMM
therefore uniﬁes two perspectives — neural network and probabilistic inference (see Table 2 in the
Appendix for details).
Shortcomings of DCNs. DCNs perform poorly in categorizing transparent objects [20]. This
might be explained by the fact that transparent objects generate pixels that have multiple sources 
conﬂicting with the DRMM sparsity prior on a  which encourages few sources. DCNs also fail to
classify slender and man-made objects [20]. This is because of the locality imposed by the locally-
connected/convolutional layers  or equivalently  the small size of the template µc(L)g in the DRMM.
As a result  DCNs fail to model long-range correlations.
Class Appearance Models and Activity Maximization. The DRMM enables us to understand how
trained DCNs distill and store knowledge from past experiences in their parameters. Speciﬁcally  the
DRMM generates rendered templates µc(L)g via a mixture of products of afﬁne transformations  thus
implying that class appearance models in DCNs are stored in a similar factorized-mixture form over
multiple levels of abstraction. As a result  it is the product of all the ﬁlters/weights over all layers
that yield meaningful images of objects (Eq. 6). We can also shed new light on another approach
to understanding DCN memories that proceeds by searching for input images that maximize the
activity of a particular class unit (say  class of cats) [23]  a technique we call activity maximization.
Results from activity maximization on a high performance DCN trained on 15 million images is
shown in Fig. 1 of [23]. The resulting images reveal much about how DCNs store memories. Using
the DRMM  the solution I⇤c(L) of the activity maximization for class c(L) can be derived as the sum
Pi  each of which is a function of the learned DRMM
of individual activity-maximizing patches I⇤
Pi) /PPi2P µ(c(L)  g⇤
Pi).

parameters (see Appendix E). In particular  I⇤c(L) ⌘PPi2P I⇤

Pi(c(L)  g⇤

6

&

&

e
t
a
R
y
c
a
r
u
c
c
A

Layer&

Figure 2: Information about latent nuisance variables at each layer (Left)  training results from EG
for RFM (Middle) and DRFM (Right) on MNIST  as compared to DCNs of the same conﬁguration.

This implies that I⇤c(L) contains multiple appearances of the same object but in various poses. Each
activity-maximizing patch has its own pose g⇤
Pi  consistent with Fig. 1 of [23] and our own extensive
experiments with AlexNet  VGGNet  and GoogLeNet (data not shown). Such images provide strong
conﬁrmational evidence that the underlying model is a mixture over nuisance parameters  as predcted
by the DRMM.
Unsupervised Learning of Latent Task Nuisances. A key goal of representation learning is to
disentangle the factors of variation that contribute to an image’s appearance. Given our formulation of
the DRMM  it is clear that DCNs are discriminative classiﬁers that capture these factors of variation
with latent nuisance variables g. As such  the theory presented here makes a clear prediction that for
a DCN  supervised learning of task targets will lead to unsupervised learning of latent task nuisance
variables. From the perspective of manifold learning  this means that the architecture of DCNs is
designed to learn and disentangle the intrinsic dimensions of the data manifolds.
In order to test this prediction  we trained a DCN to classify synthetically rendered images of
naturalistic objects  such as cars and cats  with variation in factors such as location  pose  and lighting.
After training  we probed the layers of the trained DCN to quantify how much linearly decodable
information exists about the task target c(L) and latent nuisance variables g. Fig. 2 (Left) shows that
the trained DCN possesses signiﬁcant information about latent factors of variation and  furthermore 
the more nuisance variables  the more layers are required to disentangle the factors. This is strong
evidence that depth is necessary and that the amount of depth required increases with the complexity
of the class models and the nuisance variations.

4 Experimental Results
We evaluate the DRMM and DRFM’s performance on the MNIST dataset  a standard digit classiﬁca-
tion benchmark with a training set of 60 000 28 ⇥ 28 labeled images and a test set of 10 000 labeled
images. We also evaluate the DRMM’s performance on CIFAR10  a dataset of natural objects which
include a training set of 50 000 32 ⇥ 32 labeled images and a test set of 10 000 labeled images. In all
experiments  we use a full E-step that has a bottom-up phase and a principled top-down reconstruction
phase. In order to approximate the class posterior in the DRMM  we include a Kullback-Leibler
divergence term between the inferred posterior p(c|I) and the true prior p(c) as a regularizer [9].
We also replace the M-step in the EM algorithm of Algorithm 1 by a G-step where we update
the model parameters via gradient descent. This variant of EM is known as the Generalized EM
algorithm [3]  and here we refer to it as EG. All DRMM experiments were done with the NN-DRMM.
Conﬁgurations of our models and the corresponding DCNs are provided in the Appendix I.
Supervised Training. Supervised training results are shown in Table 3 in the Appendix. Shallow
RFM: The 1-layer RFM (RFM sup) yields similar performance to a Convnet of the same conﬁguration
(1.21% vs. 1.30% test error). Also  as predicted by the theory of generative vs discriminative
classiﬁers  EG training converges 2-3x faster than a DCN (18 vs. 40 epochs to reach 1.5% test error 
Fig. 2  middle). Deep RFM: Training results from an initial implementation of the 2-layer DRFM
EG algorithm converges 2  3⇥ faster than a DCN of the same conﬁguration  while achieving a
similar asymptotic test error (Fig. 2  Right). Also  for completeness  we compare supervised training
for a 5-layer DRMM with a corresponding DCN  and they show comparable accuracy (0.89% vs
0.81%  Table 3).

7

Unsupervised Training. We train the RFM and the 5-layer DRMM unsupervised with NU images 
followed by an end-to-end re-training of the whole model (unsup-pretr) using NL labeled images. The
results and comparison to the SWWAE model are shown in Table 1. The DRMM model outperforms
the SWWAE model in both scenarios (Filters and reconstructed images from the RFM are available
in the Appendix 4.)

Table 1: Comparison of Test Error rates (%) between best DRMM variants and other best published
results on MNIST dataset for the semi-supervised setting (taken from [31]) with NU = 60K
unlabeled images  of which NL 2{ 100  600  1K  3K} are labeled.
NL = 600

NL = 100

NL = 3K

NL = 1K

Model

Convnet [10]
MTC [18]
PL-DAE [11]
WTA-AE [13]
SWWAE dropout [31]
M1+TSVM [8]
M1+M2 [8]
Skip Deep Generative Model [12]
LadderNetwork [17]
Auxiliary Deep Generative Model [12]
catGAN [25]
ImprovedGAN [21]
RFM
DRMM 2-layer semi-sup
DRMM 5-layer semi-sup
DRMM 5-layer semi-sup NN+KL
SWWAE unsup-pretr [31]
RFM unsup-pretr
DRMM 5-layer unsup-pretr

22.98
12.03
10.49

-

1.32

8.71 ± 0.34
11.82 ± 0.25
3.33 ± 0.14
1.06 ± 0.37
1.39 ± 0.28
0.93 ± 0.065

0.96

14.47
11.81
3.50
0.57

-

16.2
12.03

7.86
5.13
5.03
2.37

6.45
3.64
3.46
1.92

5.72

3.31 ± 0.40
2.59 ± 0.05

-
-
-
-
-

4.24

2.83 ± 0.10
2.40 ± 0.02
0.84 ± 0.08

-

-
-
-

3.49

2.10 ± 0.22
2.18 ± 0.04

3.35
2.57
2.69

-

-
-
-
-
-

5.61
3.73
1.56

9.80
5.65
3.61

4.67
2.88
1.67

6.135
4.64
2.73

2.96
1.72
0.91

4.41
2.95
1.68

Semi-Supervised Training. For semi-supervised training  we use a randomly chosen subset of
NL = 100  600  1K  and 3K labeled images and NU = 60K unlabeled images from the training
and validation set. Results are shown in Table 1 for a RFM  a 2-layer DRMM and a 5-layer DRMM
with comparisons to related work. The DRMMs performs comparably to state-of-the-art models.
Specially  the 5-layer DRMM yields the best results when NL = 3K and NL = 600 while results in
the second best result when NL = 1K. We also show the training results of a 9-layer DRMM on
CIFAR10 in Table 4 in Appendix H. The DRMM yields comparable results on CIFAR10 with the
best semi-supervised methods. For more results and comparison with other works  see Appendix H.
5 Conclusions
Understanding successful deep vision architectures is important for improving performance and
solving harder tasks. In this paper  we have introduced a new family of hierarchical generative
models  whose inference algorithms for two different models reproduce deep convnets and decision
trees  respectively. Our initial implementation of the DRMM EG algorithm outperforms DCN back-
propagation in both supervised and unsupervised classiﬁcation tasks and achieves comparable/state-
of-the-art performance on several semi-supervised classiﬁcation tasks  with no architectural hyperpa-
rameter tuning.
Acknowledgments. Thanks to Xaq Pitkow and Ben Poole for helpful feedback. ABP and RGB
were supported by IARPA via DoI/IBC contract D16PC00003. 1 RGB was supported by NSF
CCF-1527501  AFOSR FA9550-14-1-0088  ARO W911NF-15-1-0316  and ONR N00014-12-1-
0579. TN was supported by an NSF Graduate Reseach Fellowship and NSF IGERT Training Grant
(DGE-1250104).

1The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of
the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements  either
expressed or implied  of IARPA  DoI/IBC  or the U.S. Government.

8

References
[1] F. Anselmi  J. Z. Leibo  L. Rosasco  J. Mutch  A. Tacchetti  and T. Poggio. Magic materials: a theory of
deep hierarchical architectures for learning sensory representations. MIT CBCL Technical Report  2013.
[2] S. Arora  A. Bhaskara  R. Ge  and T. Ma. Provable bounds for learning some deep representations. arXiv

[3] C. M. Bishop. Pattern Recognition and Machine Learning  volume 4. Springer New York  2006.
[4] C. M. Bishop  J. Lasserre  et al. Generative or discriminative? getting the best of both worlds. Bayesian

preprint arXiv:1310.6343  2013.

Statistics  8:3–24  2007.

[5] L. Breiman. Random forests. Machine learning  45(1):5–32  2001.
[6] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Transactions on Pattern Analysis

and Machine Intelligence  35(8):1872–1886  2013.

[7] I. J. Goodfellow  D. Warde-Farley  M. Mirza  A. Courville  and Y. Bengio. Maxout networks. arXiv

preprint arXiv:1302.4389  2013.

[8] D. P. Kingma  S. Mohamed  D. J. Rezende  and M. Welling. Semi-supervised learning with deep generative

models. In Advances in Neural Information Processing Systems  pages 3581–3589  2014.

[9] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114  2013.
[10] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[11] D.-H. Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural

networks. In Workshop on Challenges in Representation Learning  ICML  volume 3  2013.

[12] L. Maaløe  C. K. Sønderby  S. K. Sønderby  and O. Winther. Auxiliary deep generative models. arXiv

[13] A. Makhzani and B. J. Frey. Winner-take-all autoencoders. In Advances in Neural Information Processing

preprint arXiv:1602.05473  2016.

Systems  pages 2773–2781  2015.

[14] G. F. Montufar  R. Pascanu  K. Cho  and Y. Bengio. On the number of linear regions of deep neural

networks. In Advances in Neural Information Processing Systems  pages 2924–2932  2014.

[15] A. Ng and M. Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic regression and

naive bayes. Advances in neural information processing systems  14:841  2002.

[16] A. B. Patel  T. Nguyen  and R. G. Baraniuk. A probabilistic theory of deep learning. arXiv preprint

arXiv:1504.00641  2015.

[17] A. Rasmus  M. Berglund  M. Honkala  H. Valpola  and T. Raiko. Semi-supervised learning with ladder

networks. In Advances in Neural Information Processing Systems  pages 3532–3540  2015.

[18] S. Rifai  Y. N. Dauphin  P. Vincent  Y. Bengio  and X. Muller. The manifold tangent classiﬁer. In Advances

in Neural Information Processing Systems  pages 2294–2302  2011.

[19] S. Rifai  P. Vincent  X. Muller  X. Glorot  and Y. Bengio. Contractive auto-encoders: Explicit invariance
In Proceedings of the 28th international conference on machine learning

during feature extraction.
(ICML-11)  pages 833–840  2011.

[20] O. Russakovsky  J. Deng  H. Su  J. Krause  S. Satheesh  S. Ma  Z. Huang  A. Karpathy  A. Khosla 
M. Bernstein  et al. Imagenet large scale visual recognition challenge. International Journal of Computer
Vision  115(3):211–252  2015.

[21] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. Improved techniques for

training gans. arXiv preprint arXiv:1606.03498  2016.

[22] A.-S. Sheikh  J. A. Shelton  and J. Lücke. A truncated em approach for spike-and-slab sparse coding.

Journal of Machine Learning Research  15(1):2653–2687  2014.

[23] K. Simonyan  A. Vedaldi  and A. Zisserman. Deep inside convolutional networks: Visualising image

classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034  2013.

[24] S. Soatto and A. Chiuso. Visual representations: Deﬁning properties and deep approximations.

In

International Conference on Learning Representations  2016.

[25] J. T. Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial

[26] J. T. Springenberg  A. Dosovitskiy  T. Brox  and M. Riedmiller. Striving for simplicity: The all convolu-

networks. arXiv preprint arXiv:1511.06390  2015.

tional net. arXiv preprint arXiv:1412.6806  2014.

[27] Y. Tang  R. Salakhutdinov  and G. Hinton. Deep mixtures of factor analysers.

arXiv preprint

arXiv:1206.4635  2012.

[28] G. Taylor  R. Burmeister  Z. Xu  B. Singh  A. Patel  and T. Goldstein. Training neural networks without

gradients: A scalable admm approach. arXiv preprint arXiv:1605.02026  2016.

[29] A. van den Oord and B. Schrauwen. Factoring variations in natural images with deep gaussian mixture

models. In Advances in Neural Information Processing Systems  pages 3518–3526  2014.
[30] V. N. Vapnik and V. Vapnik. Statistical learning theory  volume 1. Wiley New York  1998.
[31] J. Zhao  M. Mathieu  R. Goroshin  and Y. LeCun. Stacked what-where autoencoders. arXiv preprint

arXiv:1506.02351  2016.

9

,Ankit Patel
Minh Nguyen
Richard Baraniuk