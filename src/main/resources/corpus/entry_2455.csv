2015,High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality,We provide a general theory of the expectation-maximization (EM) algorithm for inferring high dimensional latent variable models.   In particular  we make two contributions: (i) For parameter estimation  we propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure into parameter estimation.  With an appropriate initialization  this algorithm converges at a geometric rate and attains an estimator with the (near-)optimal statistical rate of convergence.  (ii) Based on the obtained estimator  we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters.  For a broad family of statistical models   our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions.,High Dimensional EM Algorithm:

Statistical Optimization and Asymptotic Normality⇤

Zhaoran Wang

Princeton University

Quanquan Gu

University of Virginia

Yang Ning

Princeton University

Han Liu

Princeton University

Abstract

We provide a general theory of the expectation-maximization (EM) algorithm for
inferring high dimensional latent variable models. In particular  we make two con-
tributions: (i) For parameter estimation  we propose a novel high dimensional EM
algorithm which naturally incorporates sparsity structure into parameter estimation.
With an appropriate initialization  this algorithm converges at a geometric rate
and attains an estimator with the (near-)optimal statistical rate of convergence. (ii)
Based on the obtained estimator  we propose a new inferential procedure for testing
hypotheses for low dimensional components of high dimensional parameters. For
a broad family of statistical models  our framework establishes the ﬁrst computa-
tionally feasible approach for optimal estimation and asymptotic inference in high
dimensions.
Introduction

1
The expectation-maximization (EM) algorithm [12] is the most popular approach for calculating the
maximum likelihood estimator of latent variable models. Nevertheless  due to the nonconcavity of
the likelihood function of latent variable models  the EM algorithm generally only converges to a
local maximum rather than the global one [30]. On the other hand  existing statistical guarantees
for latent variable models are only established for global optima [3]. Therefore  there exists a gap
between computation and statistics.
Signiﬁcant progress has been made toward closing the gap between the local maximum attained
by the EM algorithm and the maximum likelihood estimator [2  18  25  30]. In particular  [30] ﬁrst
establish general sufﬁcient conditions for the convergence of the EM algorithm. [25] further improve
this result by viewing the EM algorithm as a proximal point method applied to the Kullback-Leibler
divergence. See [18] for a detailed survey. More recently  [2] establish the ﬁrst result that characterizes
explicit statistical and computational rates of convergence for the EM algorithm. They prove that 
given a suitable initialization  the EM algorithm converges at a geometric rate to a local maximum
close to the maximum likelihood estimator. All these results are established in the low dimensional
regime where the dimension d is much smaller than the sample size n.
In high dimensional regimes where the dimension d is much larger than the sample size n  there
exists no theoretical guarantee for the EM algorithm. In fact  when d  n  the maximum likelihood
estimator is in general not well deﬁned  unless the models are carefully regularized by sparsity-type
assumptions. Furthermore  even if a regularized maximum likelihood estimator can be obtained in a
computationally tractable manner  establishing the corresponding statistical properties  especially
asymptotic normality  can still be challenging because of the existence of high dimensional nuisance
parameters. To address such a challenge  we develop a general inferential theory of the EM algorithm
for parameter estimation and uncertainty assessment of high dimensional latent variable models. In
particular  we make two contributions in this paper:
• For high dimensional parameter estimation  we propose a novel high dimensional EM algorithm by
attaching a truncation step to the expectation step (E-step) and maximization step (M-step). Such a
⇤Research supported by NSF IIS1116730  NSF IIS1332109  NSF IIS1408910  NSF IIS1546482-BIGDATA 
NSF DMS1454377-CAREER  NIH R01GM083084  NIH R01HG06841  NIH R01MH102339  and FDA
HHSF223201000072C.

1

truncation step effectively enforces the sparsity of the attained estimator and allows us to establish
signiﬁcantly improved statistical rate of convergence.
• Based upon the estimator attained by the high dimensional EM algorithm  we propose a decorrelated
score statistic for testing hypotheses related to low dimensional components of the high dimensional
parameter.

Under a uniﬁed analytic framework  we establish simultaneous statistical and computational guar-
antees for the proposed high dimensional EM algorithm and the respective uncertainty assessment
procedure. Let ⇤ 2 Rd be the true parameter  s⇤ be its sparsity level and(t) T
t=0 be the iterative
solution sequence of the high dimensional EM algorithm with T being the total number of iterations.
In particular  we prove that:
• Given an appropriate initialization init with relative error upper bounded by a constant  2 (0  1) 
i.e. init  ⇤2/k⇤k2    the iterative solution sequence(t) T
+ 2 ·ps⇤ · log d/n
|
}

with high probability. Here ⇢ 2 (0  1)  and 1  2 are quantities that possibly depend on ⇢   and
⇤. As the optimization error term in (1.1) decreases to zero at a geometric rate with respect to
t  the overall estimation error achieves theps⇤ · log d/n statistical rate of convergence (up to an
• The proposed decorrelated score statistic is asymptotically normal. Moreover  its limiting variance
is optimal in the sense that it attains the semiparametric information bound for the low dimensional
components of interest in the presence of high dimensional nuisance parameters. See Theorem 4.6
for details.

(t)  ⇤2  1 · ⇢t/2
{z
}

extra factor of log n)  which is (near-)minimax-optimal. See Theorem 3.4 for details.

t=0 satisﬁes

Optimization Error

Statistical Error: Optimal Rate

|

{z

(1.1)

Our framework allows two implementations of the M-step: the exact maximization versus approximate
maximization. The former one calculates the maximizer exactly  while the latter one conducts an
approximate maximization through a gradient ascent step. Our framework is quite general. We
illustrate its effectiveness by applying it to two high dimensional latent variable models  that is 
Gaussian mixture model and mixture of regression model.
Comparison with Related Work: A closely related work is by [2]  which considers the low dimen-
sional regime where d is much smaller than n. Under certain initialization conditions  they prove

that the EM algorithm converges at a geometric rate to some local optimum that attains thepd/n

statistical rate of convergence. They cover both maximization and gradient ascent implementations of
the M-step  and establish the consequences for the two latent variable models considered in our paper
under low dimensional settings. Our framework adopts their view of treating the EM algorithm as
a perturbed version of gradient methods. However  to handle the challenge of high dimensionality 
the key ingredient of our framework is the truncation step that enforces the sparsity structure along
the solution path. Such a truncation operation poses signiﬁcant challenges for both computational
and statistical analysis. In detail  for computational analysis we need to carefully characterize the
evolution of each intermediate solution’s support and its effects on the evolution of the entire iterative
solution sequence. For statistical analysis  we need to establish a ﬁne-grained characterization of the
entrywise statistical error  which is technically more challenging than just establishing the `2-norm

error employed by [2]. In high dimensional regimes  we need to establish theps⇤ · log d/n statistical
rate of convergence  which is much sharper than theirpd/n rate when d  n. In addition to point

estimation  we further construct hypothesis tests for latent variable models in the high dimensional
regime  which have not been established before.
High dimensionality poses signiﬁcant challenges for assessing the uncertainty (e.g.  testing hypothe-
ses) of the constructed estimators. For example  [15] show that the limiting distribution of the Lasso
estimator is not Gaussian even in the low dimensional regime. A variety of approaches have been
proposed to correct the Lasso estimator to attain asymptotic normality  including the debiasing method
[13]  the desparsiﬁcation methods [26  32] as well as instrumental variable-based methods [4]. Mean-
while  [16  17  24] propose the post-selection procedures for exact inference. In addition  several
authors propose methods based on data splitting [20  29]  stability selection [19] and `2-conﬁdence
sets [22]. However  these approaches mainly focus on generalized linear models rather than latent
variable models. In addition  their results heavily rely on the fact that the estimator is a global optimum
of a convex program. In comparison  our approach applies to a much broader family of statistical
models with latent structures. For these latent variable models  it is computationally infeasible to

2

obtain the global maximum of the penalized likelihood due to the nonconcavity of the likelihood
function. Unlike existing approaches  our inferential theory is developed for the estimator attained
by the proposed high dimensional EM algorithm  which is not necessarily a global optimum to any
optimization formulation.
Another line of research for the estimation of latent variable models is the tensor method  which
exploits the structures of third or higher order moments. See [1] and the references therein. However 
existing tensor methods primarily focus on the low dimensional regime where d ⌧ n. In addition 
since the high order sample moments generally have a slow statistical rate of convergence  the
estimators obtained by the tensor methods usually have a suboptimal statistical rate even for d ⌧ n.
For example  [9] establish thepd6/n statistical rate of convergence for mixture of regression model 
which is suboptimal compared with thepd/n minimax lower bound. Similarly  in high dimensional

settings  the statistical rates of convergence attained by tensor methods are signiﬁcantly slower than
the statistical rate obtained in this paper.
The latent variable models considered in this paper have been well studied. Nevertheless  only a
few works establish theoretical guarantees for the EM algorithm. In particular  for Gaussian mixture
model  [10  11] establish parameter estimation guarantees for the EM algorithm and its extensions. For
mixture of regression model  [31] establish exact parameter recovery guarantees for the EM algorithm
under a noiseless setting. For high dimensional mixture of regression model  [23] analyze the gradient
EM algorithm for the `1-penalized log-likelihood. They establish support recovery guarantees for the
attained local optimum but have no parameter estimation guarantees. In comparison with existing
works  this paper establishes a general inferential framework for simultaneous parameter estimation
and uncertainty assessment based on a novel high dimensional EM algorithm. Our analysis provides
the ﬁrst theoretical guarantee of parameter estimation and asymptotic inference in high dimensional
regimes for the EM algorithm and its applications to a broad family of latent variable models.
Notation: The matrix (p  q)-norm  i.e.  k·k p q  is obtained by taking the `p-norm of each row and
then taking the `q-norm of the obtained row norms. We use C  C0  . . . to denote generic constants.
Their values may vary from line to line. We will introduce more notations in §2.2.
2 Methodology
We ﬁrst introduce the high dimensional EM Algorithm and then the respective inferential procedure.
As examples  we consider their applications to Gaussian mixture model and mixture of regression
model. For compactness  we defer the details to §A of the appendix. More models are included in the
longer version of this paper.
Algorithm 1 High Dimensional EM Algorithm

1: Parameter: Sparsity Parameterbs  Maximum Number of Iterations T
2: Initialization: bS init suppinit bs  (0) truncinit bS init
supp(· ·) and trunc(· ·) are deﬁned in (2.2) and (2.3) 
3: For t = 0 to T  1
E-step: Evaluate Qn; (t)
4:
Mn(·) is implemented as in Algorithm 2 or 3 
M-step: (t+0.5) Mn(t)
5:
T-step: bS (t+0.5) supp(t+0.5) bs  (t+1) trunc(t+0.5) bS (t+0.5)
6:
7: End For
Output: Mn(t) argmax Qn; (t)

8: Output: b (T )
1: Input: (t)  Qn; (t)
1: Input: (t)  Qn; (t)
2: Output: Mn(t) (t) + ⌘ ·r Qn(t); (t)

Algorithm 2 Maximization Implementation of the M-step

Algorithm 3 Gradient Ascent Implementation of the M-step

Parameter: Stepsize ⌘> 0

2.1 High Dimensional EM Algorithm
Before we introduce the proposed high dimensional EM Algorithm (Algorithm 1)  we brieﬂy review
the classical EM algorithm. Let h(y) be the probability density function of Y 2Y   where  2 Rd is
the model parameter. For latent variable models  we assume that h(y) is obtained by marginalizing
over an unobserved latent variable Z 2Z   i.e.  h(y) =RZ
f(y  z) dz. Let k(z | y) be the density

3

1
n

nXi=1ZZ

of Z conditioning on the observed variable Y = y  i.e.  k(z | y) = f(y  z)/h(y). We deﬁne
(2.1)

Qn(; 0) =

k0(z | yi) · log f(yi  z) dz.

See §B of the appendix for a detailed derivation. At the t-th iteration of the classical EM algorithm  we
evaluate Qn; (t) at the E-step and then perform max Qn; (t) at the M-step. The proposed
high dimensional EM algorithm (Algorithm 1) is built upon the E-step and M-step (lines 4 and 5)
of the classical EM algorithm. In addition to the exact maximization implementation of the M-step
(Algorithm 2)  we allow the gradient ascent implementation of the M-step (Algorithm 3)  which
performs an approximate maximization via a gradient ascent step. To handle the challenge of high
dimensionality  in line 6 of Algorithm 1 we perform a truncation step (T-step) to enforce the sparsity
structure. In detail  we deﬁne

supp(  s): The set of index j’s corresponding to the top s largest |j|’s.

Also  for an index set S✓{ 1  . . .   d}  we deﬁne the trunc(· ·) function in line 6 as

(2.3)
Note that (t+0.5) is the output of the M-step (line 5) at the t-th iteration of the high dimensional

⇥trunc( S)⇤j = j · 1{j 2S} .

(2.2)

(line 1). By iteratively performing the E-step  M-step and T-step  the high dimensional EM algorithm

EM algorithm. To obtain (t+1)  the T-step (line 6) preserves the entries of (t+0.5) with the topbs
large magnitudes and sets the rest to zero. Herebs is a tuning parameter that controls the sparsity level
attains anbs-sparse estimator b = (T ) (line 8). Here T is the total number of iterations.
2.2 Asymptotic Inference
Notation: Let r1Q(; 0) be the gradient with respect to  and r2Q(; 0) be the gradient with
respect to 0. If there is no confusion  we simply denote rQ(; 0) = r1Q(; 0) as in the previous
sections. We deﬁne the higher order derivatives in the same manner  e.g.  r2
1 2Q(; 0) is calculated
by ﬁrst taking derivative with respect to  and then with respect to 0. For  =>1   >2> 2 Rd with
1 2 Rd1  2 2 Rd2 and d1 + d2 = d  we use notations such as v1 2 Rd1 and A1 2 2 Rd1⇥d2
to denote the corresponding subvector of v 2 Rd and the submatrix of A 2 Rd⇥d.
We aim to conduct asymptotic inference for low dimensional components of the high dimensional
parameter ⇤. Without loss of generality  we consider a single entry of ⇤. In particular  we assume
⇤ = ⇥↵⇤  (⇤)>⇤>  where ↵⇤ 2 R is the entry of interest  while ⇤ 2 Rd1 is treated as the

nuisance parameter. In the following  we construct a high dimensional score test named decorrelated
score test. It is worth noting that  our method and theory can be easily generalized to perform statistical
inference for an arbitrary low dimensional subvector of ⇤.
Decorrelated Score Test: For score test  we are primarily interested in testing H0 : ↵⇤ = 0  since
this null hypothesis characterizes the uncertainty in variable selection. Our method easily generalizes
to H0 : ↵⇤ = ↵0 with ↵0 6= 0. For notational simplicity  we deﬁne the following key quantity
1 2Qn(; ) 2 Rd⇥d.
Tn() = r2
Let  =↵  >>. We deﬁne the decorrelated score function Sn(· ·) 2 R as
Sn(  ) =⇥r1Qn(; )⇤↵  w(  )> ·⇥r1Qn(; )⇤.
Here w(  ) 2 Rd1 is obtained using the following Dantzig selector [8]
w2Rd1 kwk1 

subject to ⇥Tn()⇤ ↵ ⇥Tn()⇤  · w1   

dimensional EM algorithm (Algorithm 1). We deﬁne the decorrelated score statistic as

where > 0 is a tuning parameter. Let b =b↵ b>>  where b is the estimator attained by the high
pn · Snb0 ⇥Tnb0⇤↵| 1/2 
where b0 =0 b>>  and⇥Tnb0⇤↵| =⇥1 wb0 >⇤ · Tnb0 ·⇥1 wb0 >⇤>.
Here we use b0 instead of b since we are interested in the null hypothesis H0 : ↵⇤ = 0. We can also
replace b0 with b and the theoretical results will remain the same. In §4 we will prove the proposed

decorrelated score statistic in (2.7) is asymptotically N (0  1). Consequently  the decorrelated score

1 1Qn(; ) + r2

w(  ) = argmin

(2.5)

(2.6)

(2.7)

(2.4)

4

test with signiﬁcance level  2 (0  1) takes the form

 S() = 1pn · Snb0 ⇥Tnb0⇤↵| 1/2 /2⇥1(1  /2)  1(1  /2)⇤  

where 1(·) is the inverse function of the Gaussian cumulative distribution function. If S() = 1 
we reject the null hypothesis H0 : ↵⇤ = 0. The intuition of this decorrelated score test is explained
in §D of the appendix. The key theoretical observation is Theorem 2.1  which connects r1Qn(·;·)
in (2.5) and Tn(·) in (2.7) with the score function and Fisher information in the presence of latent
structures. Let `n() be the log-likelihood. Its score function is r`n() and the Fisher information is
I(⇤) = E⇤⇥r2`n(⇤)⇤n  where E⇤(·) is the expectation under the model with parameter ⇤.
Theorem 2.1. For the true parameter ⇤ and any  2 Rd  it holds that

and E⇤⇥Tn(⇤)⇤ = I(⇤) = E⇤⇥r2`n(⇤)⇤n.

(2.8)

r1Qn(; ) = r`n()/n 

Proof. See §I.1 of the appendix for a detailed proof.
Based on the decorrelated score test  it is easy to establish the decorrelated Wald test  which allows
us to construct conﬁdence intervals. For compactness we defer it to the longer version of this paper.
3 Theory of Computation and Estimation
Before we present the main results  we introduce three technical conditions  which will signiﬁcantly
ease our presentation. They will be veriﬁed for speciﬁc latent variable models in §E of the appendix.
The ﬁrst two conditions  proposed by [2]  characterize the properties of the population version lower
bound function Q(·;·)  i.e.  the expectation of Qn(·;·) deﬁned in (2.1). We deﬁne the respective
population version M-step as follows. For the M-step in Algorithm 2  we deﬁne
(3.1)

M () = argmax

Q(0; ).

0

For the M-step in Algorithm 3  we deﬁne

(3.3)

M () =  + ⌘ ·r 1Q(; ) 

(3.2)
where ⌘> 0 is the stepsize in Algorithm 3. We use B to denote the basin of attraction  i.e.  the local
region where the high dimensional EM algorithm enjoys desired guarantees.
Condition 3.1. We deﬁne two versions of this condition.
• Lipschitz-Gradient-1(1 B). For the true parameter ⇤ and any  2B   we have

r1Q⇥M (); ⇤⇤  r1Q⇥M (); ⇤2  1 ·k   ⇤k2 
r1Q(; ⇤)  r1Q(; )2  2 ·k   ⇤k2.

where M (·) is the population version M-step (maximization implementation) deﬁned in (3.1).
• Lipschitz-Gradient-2(2 B). For the true parameter ⇤ and any  2B   we have

(3.4)
Condition 3.1 deﬁnes a variant of Lipschitz continuity for r1Q(·;·). In the sequel  we will use (3.3)
and (3.4) in the analysis of the two implementations of the M-step respectively.
Condition 3.2 Concavity-Smoothness(µ  ⌫ B). For any 1  2 2B   Q(·; ⇤) is µ-smooth  i.e. 

and ⌫-strongly concave  i.e. 

Q(1; ⇤)  Q(2; ⇤) + (1  2)> ·r 1Q(2; ⇤)  µ/2 ·k 2  1k2
2 
Q(1; ⇤)  Q(2; ⇤) + (1  2)> ·r 1Q(2; ⇤)  ⌫/2 ·k 2  1k2
2.

(3.6)
This condition indicates that  when the second variable of Q(·;·) is ﬁxed to be ⇤  the function is
‘sandwiched’ between two quadratic functions. The third condition characterizes the statistical error
between the sample version and population version M-steps  i.e.  Mn(·) deﬁned in Algorithms 2 and
3  and M (·) in (3.1) and (3.2). Recall k·k 0 denotes the total number of nonzero entries in a vector.
Condition 3.3 Statistical-Error(✏    s  n B). For any ﬁxed  2B with kk0  s  we have that
(3.7)
holds with probability at least 1  . Here ✏> 0 possibly depends on   sparsity level s  sample size
n  dimension d  as well as the basin of attraction B.
In (3.7) the statistical error ✏ quantiﬁes the `1-norm of the difference between the population version
and sample version M-steps. Particularly  we constrain the input  of M (·) and Mn(·) to be s-sparse.
Such a condition is different from the one used by [2]. In detail  they quantify the statistical error

M ()  Mn()1  ✏

(3.5)

5

1

· R

Optimization Error

{z

(3.10)

Statistical Error

with the `2-norm and do not constrain the input of M (·) and Mn(·) to be sparse. Consequently  our
subsequent statistical analysis is different from theirs. The reason we use the `1-norm is that  it
characterizes the more reﬁned entrywise statistical error  which converges at a fast rate ofplog d/n
(possibly with extra factors depending on speciﬁc models). In comparison  the `2-norm statistical
error converges at a slow rate ofpd/n  which does not decrease to zero as n increases with d  n.

Furthermore  the ﬁne-grained entrywise statistical error is crucial to our key proof for quantifying the
effects of the truncation step (line 6 of Algorithm 1) on the iterative solution sequence.
3.1 Main Results
To simplify the technical analysis of the high dimensional EM algorithm  we focus on its resampling
version  which is illustrated in Algorithm 4 in §C of the appendix.
Theorem 3.4. We deﬁne B = : k  ⇤k2  R   where R =  ·k ⇤k2 for some  2 (0  1).
We assume Condition Concavity-Smoothness(µ  ⌫ B) holds andinit  ⇤2  R/2.
• For the maximization implementation of the M-step (Algorithm 2)  we suppose that Condition
Lipschitz-Gradient-1(1 B) holds with ⇢1 := 1/⌫ 2 (0  1) and
bs =⌃C · max16/(1/⇢1  1)2  4 · (1 + )2/(1  )2 · s⇤⌥  
(3.8)
p
bs + C0/p1   · ps⇤ · ✏  min(1  p⇢1)2 · R  (1  )2/[2 · (1 + )] ·k ⇤k2 .
(3.9)
Here C  1 and C0 > 0 are constants. Under Condition Statistical-Error(✏  /T bs  n/T B) we
(t)  ⇤2  ⇢t/2
| {z }

bs + C0/p1   · ps⇤/(1  p⇢1) · ✏
}

have that  for t = 1  . . .   T  

+p
|

Proof. See §G.1 of the appendix for a detailed proof.

of the same order as the true sparsity level s⇤. This assumption ensures that the error incurred by the
truncation step can be upper bounded. In addition  as is shown for speciﬁc latent variable models in

holds with probability at least 1    where C0 is the same constant as in (3.9).
• For the gradient ascent implementation of the M-step (Algorithm 3)  we suppose that Condition
Lipschitz-Gradient-2(2 B) holds with ⇢2 := 1 2· (⌫  2)/(⌫ + µ) 2 (0  1) and the stepsize in
Algorithm 3 is set to ⌘ = 2/(⌫ + µ). Meanwhile  we assume (3.8) and (3.9) hold with ⇢1 replaced
holds with probability at least 1    in which ⇢1 is replaced with ⇢2.

by ⇢2. Under Condition Statistical-Error(✏  /T bs  n/T B) we have that  for t = 1  . . .   T   (3.10)
The assumption in (3.8) states that the sparsity parameterbs is chosen to be sufﬁciently large and also
§E of the appendix  the error term ✏ in Condition Statistical-Error(✏  /T bs  n/T B) decreases as
sample size n increases. By the assumption in (3.8) pbs + C0/p1   · ps⇤ is of the same order
as ps⇤. Therefore  the assumption in (3.9) suggests the sample size n is sufﬁciently large such that
ps⇤ · ✏ is sufﬁciently small. These assumptions guarantee that the entire iterative solution sequence
remains within the basin of attraction B in the presence of statistical error.
Theorem 3.4 illustrates that  the upper bound of the overall estimation error can be decomposed
into two terms. The ﬁrst term is the upper bound of optimization error  which decreases to zero at a
geometric rate of convergence  because we have ⇢1 ⇢ 2 < 1. Meanwhile  the second term is the upper
bound of statistical error  which does not depend on t. Sincepbs + C0/p1  ·ps⇤ is of the same
order as ps⇤  this term is proportional to ps⇤ · ✏  where ✏ is the entrywise statistical error between
M (·) and Mn(·). In §E of the appendix we prove that  for each speciﬁc latent variable model  ✏ is
roughly of the orderplog d/n. (There may be extra factors attached to ✏ depending on each speciﬁc
model.) Therefore  the statistical error term is roughly of the orderps⇤ · log d/n. Consequently  for
same order  the ﬁnal estimator b = (T ) attains a (near-)optimalps⇤ · log d/n (possibly with extra
factors) statistical rate. For compactness  we give the following example and defer the details to §E.
Implications for Gaussian Mixture Model: We assume y1  . . .   yn are the n i.i.d. realizations of
Y = Z · ⇤ + V . Here Z is a Rademacher random variable  i.e.  P(Z = +1) = P(Z = 1) = 1/2 
and V ⇠ N (0  2 · Id) is independent of Z  where  is the standard deviation. Suppose that we have
k⇤k2/  r  where r > 0 is a sufﬁciently large constant that denotes the minimum signal-to-noise
ratio. In §E of the appendix we prove that there exists some constant C > 0 such that Conditions

a sufﬁciently large t = T such that the optimization and statistical error terms in (3.10) are of the

6

Lipschitz-Gradient-1(1 B) and Concavity-Smoothness(µ  ⌫ B) hold with
1 = expC · r2  µ = ⌫ = 1  B = : k  ⇤k2  R with R =  ·k ⇤k2  = 1/4.
For a sufﬁciently large n  we have that Condition Statistical-Error(✏    s  n B) holds with
Then the ﬁrst part of Theorem 3.4 impliesb  ⇤2  C ·ps⇤ · log d · log n/n for a sufﬁciently
large T   which is near-optimal with respect to the minimax lower boundps⇤ log d/n.

✏ = C ·k⇤k1 +  ·q⇥log d + log(2/)⇤n.

4 Theory of Inference
To simplify the presentation of the uniﬁed framework  we lay out several technical conditions  which
will be veriﬁed for each model. Let ⇣EM  ⇣G  ⇣T and ⇣L be four quantities that scale with s⇤  d and n.
These conditions will be veriﬁed for speciﬁc latent variable models in §F of the appendix.
Condition 4.1 Parameter-Estimation⇣EM. We haveb  ⇤1 = OP⇣EM.
Condition 4.2 Gradient-Statistical-Error⇣G. We have
r1Qn(⇤; ⇤)  r1Q(⇤; ⇤)1
Condition 4.3 Tn(·)-Concentration⇣T. We haveTn(⇤)  E⇤⇥Tn(⇤)⇤1 1
Condition 4.4 Tn(·)-Lipschitz⇣L. For any   we have
Tn()  Tn(⇤)1 1
In the sequel  we lay out an assumption on several population quantities and the sample size n. Recall
that ⇤ = [↵⇤  (⇤)>]>  where ↵⇤ 2 R is the entry of interest  while ⇤ 2 Rd1 is the nuisance
parameter. By the notations in §2.2 ⇥I(⇤)⇤  2 R(d1)⇥(d1) and⇥I(⇤)⇤ ↵ 2 R(d1)⇥1 denote
the submatrices of the Fisher information matrix I(⇤) 2 Rd⇥d. We deﬁne w⇤  s⇤w and S⇤w as
and S⇤w = supp(w⇤).
We deﬁne 1⇥I(⇤)⇤ and d⇥I(⇤)⇤ as the largest and smallest eigenvalues of I(⇤)  and
  ·⇥I(⇤)⇤ ↵ 2 R.

= OP⇣G.
= OP⇣L ·k   ⇤k1.

⇥I(⇤)⇤↵| =⇥I(⇤)⇤↵ ↵ ⇥I(⇤)⇤>

s⇤w = kw⇤k0 
 ↵ ·⇥I(⇤)⇤1

  ·⇥I(⇤)⇤ ↵ 2 Rd1 

According to (4.1) and (4.2)  we can easily verify that

w⇤ =⇥I(⇤)⇤1

= OP⇣T.

(4.1)

(4.2)

(4.3)

⇥I(⇤)⇤↵| =⇥1 (w⇤)>⇤ · I(⇤) ·⇥1 (w⇤)>⇤>.

The following assumption ensures that d⇥I(⇤)⇤ > 0. Hence ⇥I(⇤)⇤  in (4.1) is invertible.
Also  according to (4.3) and the fact that d⇥I(⇤)⇤ > 0  we have⇥I(⇤)⇤↵| > 0.
Assumption 4.5 . We impose the following assumptions.
• For positive constants ⇢max and ⇢min  we assume
⇥I(⇤)⇤1
⇥I(⇤)⇤↵| = O(1) 
⇢max  1⇥I(⇤)⇤  d⇥I(⇤)⇤  ⇢min 
• The tuning parameter  of the Dantzig selector in (2.6) is set to
 = C ·⇣T + ⇣L · ⇣EM ·1 + kw⇤k1 
where C  1 is a sufﬁciently large constant. The sample size n is sufﬁciently large such that

maxkw⇤k1  1 · s⇤w ·  = o(1) ⇣

s⇤w ·  · ⇣G = o(1/pn) 
 · ⇣EM = o(1/pn)  max1  kw⇤k1 · ⇣L ·⇣EM2 = o(1/pn).

The assumption on d⇥I(⇤)⇤ guarantees that the Fisher information matrix is positive deﬁnite. The
other assumptions in (4.4) guarantee the existence of the asymptotic variance of pn · Snb0  in

the score statistic deﬁned in (2.7). Similar assumptions are standard in existing asymptotic inference
results. For example  for mixture of regression model  [14] impose variants of these assumptions.
For speciﬁc models  we will show that ⇣EM  ⇣G  ⇣T and  all decrease with n  while ⇣L increases
with n at a slow rate. Therefore  the assumptions in (4.6) ensure that the sample size n is sufﬁciently
large. We will make these assumptions more explicit after we specify ⇣EM  ⇣G  ⇣T and ⇣L for each

↵| = O(1). (4.4)

EM = o(1) 

(4.6)

(4.5)

7

model. Note the assumptions in (4.6) imply that s⇤w = kw⇤k0 needs to be small. For instance  for 
speciﬁed in (4.5)  maxkw⇤k1  1 · s⇤w ·  = o(1) in (4.6) implies s⇤w · ⇣T = o(1). In the following 
we will prove that ⇣T is of the orderplog d/n. Hence  we require that s⇤w = opn/ log d ⌧ d 1 
i.e.  w⇤ 2 Rd1 is sparse. Such a sparsity assumption can be understood as follows. According to
the deﬁnition of w⇤ in (4.1)  we have⇥I(⇤)⇤  · w⇤ =⇥I(⇤)⇤ ↵. Therefore  such a sparsity
assumption suggests⇥I(⇤)⇤ ↵ lies within the span of a few columns of⇥I(⇤)⇤ . Such a sparsity

assumption on w⇤ is necessary  because otherwise it is difﬁcult to accurately estimate w⇤ in high
dimensional regimes. In the context of high dimensional generalized linear models  [26  32] impose
similar sparsity assumptions.
4.1 Main Results
Decorrelated Score Test: The next theorem establishes the asymptotic normality of the decorrelated
score statistic deﬁned in (2.7).

pn · Snb0 ⇥Tnb0⇤↵| 1/2 D! N (0  1) 

Theorem 4.6. We consider ⇤ =⇥↵⇤  (⇤)>⇤> with ↵⇤ = 0. Under Assumption 4.5 and Conditions
4.1-4.4  we have that for n ! 1 
where b0 and⇥Tnb0⇤↵| 2 R are deﬁned in (2.7). The limiting variance of the decorrelated score
function pn · Snb0  is⇥I(⇤)⇤↵|  which is deﬁned in (4.2).
Proof. See §G.2 of the appendix for a detailed proof.
Optimality: [27] prove that for inferring ↵⇤ in the presence of nuisance parameter ⇤ ⇥I(⇤)⇤↵| is
the semiparametric efﬁcient information  i.e.  the minimum limiting variance of the (rescaled) score
function. Our proposed decorrelated score function achieves such a semiparametric information lower
bound and is therefore in this sense optimal.
In the following  we use Gaussian mixture model to illustrate the effectiveness of Theorem 4.6. We
defer the details and the implications for mixture of regression to §F of the appendix.
Implications for Gaussian Mixture Model: Under the same model considered in §3.1  if we assume
all quantities except s⇤w  s⇤  d and n are constant  then we have that Conditions 4.1-4.4 hold with
⇣EM = s⇤plog d · log n/n  ⇣G =plog d/n  ⇣T =plog d/n and ⇣L =log d + log n3/2. Thus 
under Assumption 4.5  (4.7) holds when n ! 1. Also  we can verify that (4.6) in Assumption 4.5
holds if maxs⇤w  s⇤ 2 · (s⇤)2 · (log d)5 = o⇥n/(log n)2⇤.

5 Conclusion
We propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure.
Our theory shows that  with a suitable initialization  the proposed algorithm converges at a geometric
rate and achieves an estimator with the (near-)optimal statistical rate of convergence. Beyond point
estimation  we further propose the decorrelated score and Wald statistics for testing hypotheses and
constructing conﬁdence intervals for low dimensional components of high dimensional parameters.
We apply the proposed algorithmic framework to a broad family of high dimensional latent variable
models. For these models  our framework establishes the ﬁrst computationally feasible approach for
optimal parameter estimation and asymptotic inference under high dimensional settings.
References
[1] A N A N D K U M A R   A .  G E   R .  H S U   D .  K A K A D E   S . M . and T E L G A R S K Y  M . (2014). Tensor
decompositions for learning latent variable models. Journal of Machine Learning Research 15 2773–2832.
[2] B A L A K R I S H N A N   S .  WA I N W R I G H T  M . J . and Y U   B . (2014). Statistical guarantees for the EM

(4.7)

algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156 .

[3] B A R T H O L O M E W  D . J .  K N O T T  M . and M O U S TA K I   I . (2011). Latent variable models and

factor analysis: A uniﬁed approach  vol. 899. Wiley.

[4] B E L L O N I   A .  C H E N   D .  C H E R N O Z H U K O V  V. and H A N S E N   C . (2012). Sparse models and

methods for optimal instruments with an application to eminent domain. Econometrica 80 2369–2429.

[5] B I C K E L   P. J .  R I T O V  Y. and T S Y B A K O V  A . B . (2009). Simultaneous analysis of Lasso and

Dantzig selector. Annals of Statistics 37 1705–1732.

8

[6] B O U C H E R O N   S .  L U G O S I   G . and M A S S A RT  P. (2013). Concentration inequalities: A nonasymp-

totic theory of independence. Oxford University Press.

[7] C A I   T.  L I U   W. and L U O   X . (2011). A constrained `1 minimization approach to sparse precision

matrix estimation. Journal of the American Statistical Association 106 594–607.

[8] C A N D `E S   E . and TA O   T. (2007). The Dantzig selector: Statistical estimation when p is much larger

than n. Annals of Statistics 35 2313–2351.

[9] C H A G A N T Y  A . T. and L I A N G   P. (2013). Spectral experts for estimating mixtures of linear regres-

sions. arXiv preprint arXiv:1306.3729 .

[10] C H A U D H U R I   K .  D A S G U P TA   S . and VAT TA N I   A . (2009). Learning mixtures of Gaussians

using the k-means algorithm. arXiv preprint arXiv:0912.0086 .

[11] D A S G U P TA   S . and S C H U L M A N   L . (2007). A probabilistic analysis of EM for mixtures of separated 

spherical Gaussians. Journal of Machine Learning Research 8 203–226.

[12] D E M P S T E R   A . P.  L A I R D   N . M . and R U B I N   D . B . (1977). Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Statistical
Methodology) 39 1–38.

[13] JAVA N M A R D   A . and M O N TA N A R I   A . (2014). Conﬁdence intervals and hypothesis testing for

high-dimensional regression. Journal of Machine Learning Research 15 2869–2909.

[14] K H A L I L I   A . and C H E N   J . (2007). Variables selection in ﬁnite mixture of regression models. Journal

of the American Statistical Association 102 1025–1038.

[15] K N I G H T  K . and F U   W. (2000). Asymptotics for Lasso-type estimators. Annals of Statistics 28

1356–1378.

[16] L E E   J . D .  S U N   D . L .  S U N   Y. and TAY L O R   J . E . (2013). Exact inference after model selection

via the Lasso. arXiv preprint arXiv:1311.6238 .

[17] L O C K H A RT  R .  TAY L O R   J .  T I B S H I R A N I   R . J . and T I B S H I R A N I   R . (2014). A signiﬁcance

test for the Lasso. Annals of Statistics 42 413–468.

[18] M C L A C H L A N   G . and K R I S H N A N   T. (2007). The EM algorithm and extensions  vol. 382. Wiley.
[19] M E I N S H A U S E N   N . and B ¨U H L M A N N   P. (2010). Stability selection. Journal of the Royal Statistical

Society: Series B (Statistical Methodology) 72 417–473.

[20] M E I N S H A U S E N   N .  M E I E R   L . and B ¨U H L M A N N   P. (2009). p-values for high-dimensional

regression. Journal of the American Statistical Association 104 1671–1681.

[21] N E S T E R O V  Y. (2004). Introductory lectures on convex optimization:A basic course  vol. 87. Springer.
[22] N I C K L   R . and VA N D E G E E R   S . (2013). Conﬁdence sets in sparse regression. Annals of Statistics

41 2852–2876.

[23] S T ¨A D L E R   N .  B ¨U H L M A N N   P. and VA N D E G E E R   S . (2010). `1-penalization for mixture

regression models. TEST 19 209–256.

[24] TAY L O R   J .  L O C K H A RT  R .  T I B S H I R A N I   R . J . and T I B S H I R A N I   R . (2014). Post-selection

adaptive inference for least angle regression and the Lasso. arXiv preprint arXiv:1401.3889 .

[25] T S E N G   P. (2004). An analysis of the EM algorithm and entropy-like proximal point methods. Mathe-

matics of Operations Research 29 27–44.

[26] VA N D E G E E R   S .  B ¨U H L M A N N   P.  R I T O V  Y. and D E Z E U R E   R . (2014). On asymptotically

optimal conﬁdence regions and tests for high-dimensional models. Annals of Statistics 42 1166–1202.

[27] VA N D E R VA A R T  A . W. (2000). Asymptotic statistics  vol. 3. Cambridge University Press.
[28] V E R S H Y N I N   R . (2010).

Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027 .

[29] WA S S E R M A N   L . and R O E D E R   K . (2009). High-dimensional variable selection. Annals of Statistics

37 2178–2201.

[30] W U   C . F. J . (1983). On the convergence properties of the EM algorithm. Annals of Statistics 11

95–103.

[31] Y I   X .  C A R A M A N I S   C . and S A N G H AV I   S . (2013). Alternating minimization for mixed linear

regression. arXiv preprint arXiv:1310.3745 .

[32] Z H A N G   C . - H . and Z H A N G   S . S . (2014). Conﬁdence intervals for low dimensional parameters in
high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology)
76 217–242.

9

,Romain Paulus
Richard Socher
Christopher Manning
Zhaoran Wang
Quanquan Gu
Yang Ning
Han Liu
Jie Cao
Yibo Hu
Hongwen Zhang
Ran He
Zhenan Sun