2018,Self-Supervised Generation of Spatial Audio for 360° Video,We introduce an approach to convert mono audio recorded by a 360° video camera into spatial audio  a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360° video viewing  but spatial audio microphones are still rare in current 360° video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere  conditioned on multi-modal analysis from the audio and 360° video frames. We introduce several datasets  including one filmed ourselves  and one collected in-the-wild from YouTube  consisting of 360° videos uploaded with spatial audio. During training  ground truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach we show that it is possible to infer the spatial localization of sounds based only on a synchronized 360° video and the mono audio track.,Self-Supervised Generation of Spatial Audio

for 360◦ Video

Pedro Morgado

University of California  San Diego∗

Nuno Vasconcelos

University of California  San Diego

Timothy Langlois

Adobe Research  Seattle

Oliver Wang

Adobe Research  Seattle

Abstract

We introduce an approach to convert mono audio recorded by a 360◦ video camera
into spatial audio  a representation of the distribution of sound over the full viewing
sphere. Spatial audio is an important component of immersive 360◦ video viewing 
but spatial audio microphones are still rare in current 360◦ video production. Our
system consists of end-to-end trainable neural networks that separate individual
sound sources and localize them on the viewing sphere  conditioned on multi-modal
analysis of audio and 360◦ video frames. We introduce several datasets  including
one ﬁlmed ourselves  and one collected in-the-wild from YouTube  consisting of
360◦ videos uploaded with spatial audio. During training  ground-truth spatial
audio serves as self-supervision and a mixed down mono track forms the input to
our network. Using our approach  we show that it is possible to infer the spatial
location of sound sources based only on 360◦ video and a mono audio track.

Introduction

1
360◦ video provides viewers an immersive viewing experience where they are free to look in any
direction  either by turning their heads with a Head-Mounted Display (HMD)  or by mouse-control
while watching the video in a browser (e.g.  YouTube). Capturing 360◦ video involves ﬁlming the
scene with multiple cameras and stitching the result together. While early systems relied on expensive
rigs with carefully mounted cameras  recent consumer-level devices combine multiple lenses in a
small ﬁxed-body frame that enables automatic stitching  allowing 360◦ video to be recorded with a
single push of a button.
As humans rely on audio localization cues for full scene awareness  spatial audio is a crucial
component of 360◦ video. Spatial audio enables viewers to experience sound in all directions  while
adjusting the audio in real time to match the viewing position. This gives users a more immersive
experience  as well as providing cues about which part of the scene might have interesting content
to look at. However  unlike 360◦ video  producing spatial audio content still requires a moderate
degree of expertise. Most consumer-level 360◦ cameras only record mono audio  and syncing an
external spatial audio microphone can be expensive and technically challenging. As a consequence 
while most video platforms (e.g.  YouTube and Facebook) support spatial audio  it is often ignored
by content creators  and at the time of submission  a random polling of 1000 YouTube 360◦ videos
yielded less than 5% with spatial audio.
In order to close this gap between the audio and visual experiences  we introduce three main
contributions: (1) we formalize the 360◦ spatialization problem; (2) design the ﬁrst 360◦ spatialization
procedure; and (3) collect two datasets and propose an evaluation protocol to benchmark ours and

∗Contact author: pmaravil@eng.ucsd.edu

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Architecture overview. Our approach is composed of four main blocks. The input video and audio
signals are fed into the analysis block (a)  which extracts high-level features. The separation block (b) then
learns k time-frequency attenuation maps ai(t  w) to modulate the input STFT and produce modiﬁed waveforms
f i(t). The localization block (c) computes a set of linear transform weights wi(t) that localize each source. In
the ambisonics generation step (d)  localization weights are then combined with the separated sound sources to
produce the ﬁnal spatial audio output.

future algorithms. 360◦ spatialization aims to upconvert a single mono recording into spatial
audio guided by full 360 view video. More speciﬁcally  we seek to generate spatial audio in the
form of a popular encoding format called ﬁrst-order ambisonics (FOA)  given the mono audio and
corresponding 360◦ video as inputs. In addition to formulating the 360◦ spatialization task  we design
the ﬁrst data-driven system to upgrade mono audio using self-supervision from 360◦ videos recorded
with spatial audio. The proposed procedure is based on a novel neural network architecture that
disentangles two fundamental challenges in audio spatialization: the separation of sound sources
from a mixed audio input and respective localization of these sources. In order to train and validate
our approach  we introduce two 360◦ video datasets with spatial audio  one recorded by ourselves
in a constrained domain  and a large-scale dataset collected in-the-wild from YouTube. During
training  the captured spatial audio serves as ground truth  with a mixed down mono version provided
as input to our system. Experiments conducted in both datasets show that the proposed neural
network can generate plausible spatial audio for 360◦ video. We further validate each component of
the proposed architecture and show its superiority over a state-of-the-art  but domain-independent
baseline architecture.
In the interest of reproducibility  code  data and trained models will be made available to the
community at https://pedro-morgado.github.io/spatialaudiogen.

2 Related Work

To the best of our knowledge  we propose the ﬁrst system for audio spatialization. In addition to
spatial audio  the ﬁelds most related to our work are self-supervised learning  audio generation  source
separation and audio-visual cross-modal learning  which we now brieﬂy describe.

Spatial audio Artiﬁcial environments  such as those rendered by game engines  can play sounds
from any location in the video. This capability requires recording sound sources separately and
mixing them according to the desired scene conﬁguration (i.e.  the positions of each source relative to
the user). In a real world recording  however  sound sources cannot be recorded separately. In this case
where sound sources are naturally mixed  spatial audio is often encoded using Ambisonics [13  9  30].
Ambisonics aim to approximate the sound pressure ﬁeld at a single point in space using a spherical
harmonic decomposition. More speciﬁcally  an audio signal f (θθθ  t) arriving from direction θθθ = (ϕ  ϑ)
(where ϕ is the zenith angle and ϑ the azimuth angle) at time t is represented by a truncated spherical
harmonic expansion of order N

f (θθθ  t) =(cid:80)N

(cid:80)n

n (ϕ  ϑ) is the real spherical harmonic of order n and degree m  and φm

where Y m
of the expansion. For ease of notation  Y m
(Eq. 1) written as f (θθθ  t) = yyyT

N (θθθ) φφφN (t).

n and φm

n=0

m=−n Y m

n (t)

n (ϕ  ϑ)φm

(1)
n (t) are the coefﬁcients
n can be stacked into vectors yyyN and φφφN   and

2

CopyCopyCopyXCNNCNNRGBFLOWSTFT𝑤𝑧𝑖𝑡𝑤𝑦𝑖𝑡𝑤𝑥𝑖𝑡𝜙𝑥𝑡𝜙𝑦𝑡𝜙𝑧𝑡𝑤𝑥𝑇𝑓𝑤𝑦𝑇𝑓𝑤𝑧𝑇𝑓iSTFT𝑎𝑖𝑡 𝜔𝚽𝑖𝑡 𝜔Tile & ConcatAudio𝑖𝑡Video𝑣𝑡𝚽𝑡 𝜔(a)(b)(c)(d)In a controlled environment  sound sources with known locations can be synthetically encoded into
ambisonics using their spherical harmonic projection. More speciﬁcally  given a set of k audio signals
s1(t)  . . .   sk(t) originating from directions θθθ1  . . .   θθθk 

φφφN (t) =(cid:80)k

i=1 yyyN (θθθi)si(t).

(2)

For ambisonics playback  φφφN is then decoded into a set of speakers or headphone signals in order
to provide a plane-wave reconstruction of the sound ﬁeld. In sum  the coefﬁcients φφφN   also known
as ambisonic channels  are sufﬁcient to encode and reproduce spatial audio. Hence  our goal is to
generate φφφN from non-spatial audio and the corresponding video.

Self-supervised learning Neural networks have been successfully trained through self-supervision
for tasks such as image super-resolution [10  27] and image colorization [20  46]. In the audio
domain  self-supervision has also enabled the detection of sound-video misalignment [37] and audio
super-resolution [31]. Inspired by these approaches  we propose a self-supervised technique for audio
spatialization. We show that the generation of ambisonic audio can be learned using a dataset of 360◦
video with spatial audio collected in-the-wild without additional human intervention.

Generative models Recent advances in generative models such as Generative Adversarial Networks
(GANs) [14] or Variational Auto-Encoders (VAE) [29] have enabled the generation of complex
patterns  such as images [14] or text [23]. In the audio domain  Wavenet [36] has demonstrated the
ability to produce high ﬁdelity audio samples of both speech and music  by generating a waveform
from scratch on a sample-by-sample basis. Furthermore  neural networks have also outperformed
prior solutions to audio super-resolution [31] (e.g. converting from 4kHz to 16kHz audio) using a
U-Net encoder-decoder architecture  and have enabled “automatic-Foley” type applications [41  38] 
i.e. generating sounds that correspond to image features  and vice-versa. In this work  instead of
generating audio from scratch  our goal is to augment the input audio channels so as to introduce
spatial information. Thus  unlike Wavenet  efﬁcient audio generation can be achieved without
sacriﬁcing audio ﬁdelity  by transforming the input audio. We also demonstrate the advantages of our
approach  inspired by the ambisonics encoding process in controlled environments  over a generic
U-Net architecture for spatial audio generation.

Source separation Source separation is a classic problem with an extensive literature. While early
methods present the problem as independent component analysis  and focused on maximizing the
statistical independence of the extracted signals [24  7  6  2]  recent approaches focus on data-driven
solutions. For example  [19] proposes a recurrent neural-network for monaural separation of two
speakers  [1  12  11] seek to isolate sound sources by leveraging synchronized visual information in
addition to the audio input  and [44] studies a wide range of frequency-based separation methods.
Similarly to recent trends  we rely on neural networks guided by cross-modal video analysis. However 
instead of only separating human speakers [44] or musical instruments [47]  we aim to separate
multiple unidentiﬁed types of sound sources. Also  unlike previous algorithms  no explicit supervision
is available to learn the separation block.

Source localization Sound source localization is a mature area of signal processing and robotics
research [3  35  34  42]. However  unlike the proposed 360◦ spatialization problem  these works rely
on microphone arrays using beamforming techniques [43] or binaural audio and HRTF cues similar
to those used by humans [18]. Furthermore  the need for carefully calibrated microphones limits the
applicability of these techniques to videos collected in-the-wild.

Cross visual-audio analysis Cross-modal analysis has been extensively studied in the vision and
graphics community  due to the inherently paired nature of video and audio. For example  [4] learns
audio feature representations in an unsupervised setting by leveraging synchronized video. [22]
segments and localizes dominant sound sources using clustering of video and sound features. Other
methods correlate repeated motions with sounds to identify sound sources such as the strumming of a
guitar using for example canonical correlation analysis [25  26]  joint embedding spaces [41  38] or
other temporal features [5].

3

3 Method
In this section  we deﬁne the 360◦ spatialization task to upconvert common audio recordings to
support spatial audio playback. We then introduce a deep learning architecture to address this task 
and two datasets to train the proposed architecture.

3.1 Audio spatialization
The goal of 360◦ spatialization is to generate ambisonic channels φφφN (t) from non-spatial audio i(t)
and corresponding video v(t). To handle the most common audio formats supported by commercial
360◦ cameras and video viewing platforms (e.g.  YouTube and Facebook)  we upgrade monaural
recordings (mono) into ﬁrst-order ambisonics (FOA). FOA consists of four channels that store the
ﬁrst-order coefﬁcients  φ0
1  of the spherical harmonic expansion in (Eq. 1). For ease
of notation  we refer to these tracks as φw  φy  φz and φx  respectively.

0  φ−1

1   φ0

1 and φ1

Self-supervised audio spatialization Converting mono to FOA ideally requires learning from
videos with paired mono and ambisonics recordings  which are difﬁcult to collect in-the-wild. In
order to learn from self-supervision  we assume that monaural audio is recorded with an omni-
directional microphone. Under this assumption  mono is equivalent to zeroth-order ambisonics (up
to an amplitude scale) and  as a consequence  the upconversion only requires the synthesis of the
missing higher-order channels. More speciﬁcally  we learn to predict the ﬁrst-order components
φx(t)  φy(t)  φz(t) from the (surrogate) mono audio i(t) = φw(t) and video input v(t). Note that
the proposed framework is also applicable to other conversion scenarios  e.g. FOA to second-order
ambisonics (SOA)  simply by changing the number of input and output audio tracks (see Sec 5).

3.2 Architecture

Audio spatialization requires solving two fundamental problems: source separation and localization.
In controlled environments  where the separated sound sources si(t) and respective localization θθθi
are known in advance  ambisonics can be generated using (Eq. 2). However  since si(t) and θθθi are
not known in practice  we design dedicated modules to isolate sources from the mixed audio input
and localize them in the video. Also  because audio and video are complementary for identifying
each source  both separation and localization modules are guided by a multi-modal audio-visual
analysis module. A schematic description of our architecture is shown in Fig. 1. We now describe
each component. Details of network architectures are provided in Appendix A.

Audio and visual analysis Audio features are extracted in the time-frequency domain  which has
produced successful audio representations for tasks such as audio classiﬁcation [17] and speaker
identiﬁcation [33]. More speciﬁcally  we extract a sequence of short-term Fourier transforms (STFT)
computed on 25ms segments of the input audio with 25% hop size and multiplied by Hann window
functions. Then  we apply a (two-dimensional) CNN encoder to the audio spectrogram  which
progressively reduces the spectrogram dimensionality and extracts high-level features.
Video features are extracted using a two-stream network  based on Resnet-18 [16]  to encode both
appearance (RGB frames) and motion (optical ﬂow predicted by FlowNet2 [21]). Both streams are
initialized with weights pre-trained on ImageNet [8] for classiﬁcation  and ﬁne-tuned on our task.
A joint audio-visual representation is then obtained by merging the three feature maps (audio  RGB
and ﬂow) produced at each time t. Since audio features are extracted at a higher frame rate than video
features  we ﬁrst synchronize the audio and video feature maps by nearest neighbor up-sampling of
video features. Each feature map is then projected into a feature vector (1024 for audio and 512 for
RGB and ﬂow)  and the outputs concatenated and fed to the separation and localization modules.

Audio separation
Although the number of sources may vary  this is often small in practice.
Furthermore  psycoaccoustic studies have shown that humans can only distinguish a small number of
simultaneous sources (three according to [39]). We thus assume an upper-bound of k simultaneous
sources  and implement a separation network that extracts k audio tracks f i(t) from the input audio
i(t). The separation module takes the form of a U-Net decoder that progressively restores the STFT
dimensionality through a series of transposed convolutions and skip connections from the audio

4

analysis stage of equivalent resolution. Furthermore  to visually guide the separation module  we
concatenate the multi-modal features to the lowest resolution layer of the audio encoder. In the last
up-sampling layer  we produce k sigmoid activated maps ai(t  ω)  which are used to modulate the
STFT of the mono input ΦΦΦ(t; ω). The STFT of the ith source ΦΦΦi(t; ω) is thus obtained through
the soft-attention mechanism ΦΦΦi(t; ω) = ai(t  ω) · ΦΦΦ(t; ω)  and the separated audio track f i(t)
reconstructed as the inverse STFT of ΦΦΦi(t; ω) using an overlap-add method.

Localization
To localize the sounds f i(t) extracted by the separation network  we implement
a module that generates  at each time t  the localization weights wi(t) = (wi
z(t))
associated with each of the k sources  through a series of fully-connected layers applied to the
multi-modal feature vectors of the analysis stage. In a parallel to the encoding mechanism of (Eq. 2)
used in controlled environments  wi(t) can be interpreted as the spherical harmonics yyyN (θθθi(t))
evaluated at the predicted position of the ith source θθθi(t).

x(t)  wi

y(t)  wi

(cid:80)k

Ambisonic generation
Given the localization weights wi(t) and separated wave-forms
f i(t)  the ﬁrst-order ambisonic channels φφφ(t) = (φx(t)  φy(t)  φz(t)) are generated by φφφ(t) =
i=1 wi(t)f i(t). In summary  we split the generation task into two components: generating the
attenuation maps ai(t  ω) for source separation  and the localization weights wi(t). As audio is not
generated from scratch  but through a transformation of the original input inspired by the encoding
framework of (Eq. 2)  we are able to achieve fast deployment speeds with high quality results.

3.3 Evaluation metrics

Let φφφ(t) and ˆφφφ(t) be the ground-truth and predicted ambisonics  and ΦΦΦ(t; ω) and ˆΦΦΦ(t; ω) their
respective STFTs. We now discuss several metrics used for evaluating the generated signals ˆφφφ(t).

STFT distance Our network is trained end-to-end to minimize errors between STFTs  i.e. 

M SEstft =(cid:80)

p∈{x y z}(cid:80)

(cid:80)
ω (cid:107)Φp(t  ω) − ˆΦp(t  ω)(cid:107)2 

(3)
where (cid:107) · (cid:107) is the euclidean complex norm. M SEstft has well-deﬁned and smooth partial derivatives
and  thus  it is a suitable loss function. Furthermore  unlike the euclidean distance between raw
waveforms  the STFT loss is able to separate the signal into its frequency components  which enables
the network to learn the easier parts of the spectrum without distraction from other errors.

t

Log-spectral distance (LSD) Distances that only compare the smoothed spectral behavior of audio
signals are widely used throughout the audio literature. We use the log-spectral distance [15] between
ΦΦΦ(t; ω) and ˆΦΦΦ(t; ω)  which measures the distance in dB between the two spectrograms using

LSD =(cid:80)

p∈{x y z}(cid:80)

t

(cid:114)

(cid:80)K

(cid:16)

(cid:12)(cid:12)(cid:12) Φp(t ω)

ˆΦp(t ω)

(cid:12)(cid:12)(cid:12)(cid:17)2

1
K

ω=1

10 log10

.

(4)

Envelope distance (ENV) Due to the high-frequency nature of audio and the human insensitivity to
phase differences  frame-by-frame comparison of raw waveforms do not capture perceptual similarity
of two audio signals. Instead  we measure the euclidean distance between envelopes of φφφ(t) and ˆφφφ(t) 
where the envelope of an audio wave is computed using the Hilbert transform method [40].

(cid:113) 1

T

(cid:80)

(cid:113)

(cid:80)

(cid:0)yyyT
N (θθθ) φφφN (τ )(cid:1)2

Earth Mover’s Distance (EMD) Ambisonics model the sound ﬁeld f (θθθ  t) over all directions θθθ.
The energy of the sound ﬁeld measured over a small window wt around time t along direction θθθ is

E(θθθ  t) =

τ∈wt

f (θθθ  τ )2 =

1
T

τ∈wt

.

(5)

Thus  E(θθθ  t) represents the directional energy map of φφφ(t). In order to measure the localization
accuracy of the generated spatial audio  we propose to compute the EMD [32] between the energy
maps E(θθθ  t) associated with φφφ(t) and ˆφφφ(t). In practice  we uniformly sample the maps E(θθθ  t) over
i E(θθθi  t) = 1  and measure the distance between

the sphere  normalize the sampled map so that(cid:80)

samples over the sphere’s surface using cosine (angular) distances for EMD calculation.

5

REC-STREET

YT-ALL

YT-MUSIC

YT-CLEAN

Figure 2: Representative images. Example video frames from each dataset.

3.4 Datasets
To train our model  we collected two datasets of 360◦ videos with FOA audio. The ﬁrst dataset 
denoted REC-STREET  was recorded by us using a Theta V 360◦ camera with an attached TA-1
spatial audio microphone. REC-STREET consists of 43 videos of outdoor street scenes  totaling 3.5
hours and 123k training samples (0.1s each). Due to the consistency of capture hardware and scene
content  the audio of REC-STREET videos is relatively easier to spatialize.
The second dataset  denoted YT-ALL  was collected in-the-wild by scraping 360◦ videos from
YouTube using queries related to spatial audio  e.g.  spatial audio  ambisonics  and ambix. To
clean the search results  we automatically removed videos that did not contain valid ambisonics  as
described by YouTube’s format  keeping only videos containing all 4 channels or with only the Z
channel missing (a common spatial audio capture scenario). Finally  we performed a manual curation
to remove videos containing 1) still images  2) computer generated content  or 3) post-processed
and non-visually indicated sounds such as background music or voice-overs. During this pruning
process  799 videos were removed  resulting in 1146 valid videos totaling 113.1 hours of content
(3976k training samples). YT-ALL was further separated into live musical performances  YT-MUSIC
(397 videos)  and videos with a small number of super-imposed sources which could be localized in
the image  YT-CLEAN (496 videos). Upgrading YT-MUSIC videos into spatial audio is especially
challenging due to the large number of mixed sources (voices and instruments). We also identiﬁed
489 videos that were recorded with a “horizontal” spatial audio microphone (i.e. only containing
φw(t) φx(t) and φy(t) channels). In this case  we simply ignore the Z channel φz(t) when computing
each metric including the STFT loss. Fig. 2 shows illustrative video frames and summarizes the most
common categories for each dataset.

4 Evaluation

For our experiments  we randomly sample three partitions  each containing 75% of all videos for
training and 25% for testing. Networks are trained to generate audio at 48kHz from input mono audio
processed at 48kHz and video at 10Hz. Each training sample consists of a chunk of 0.6s of mono
audio and a single frame of RGB and ﬂow  which are used to predict 0.1s of spatial audio at the
center of the 0.6s input window. To make the model more robust and remove any bias to content in
the center  we augment datasets during training by randomly rotating both video and spatial audio
around the vertical (z) axis. Spatial audio can be rotated by multiplying the ambisonic channels with
the appropriate rotation matrix as described in [30]  and video frames (in equirectangular format) can
be rotated using horizontal translations with wrapping. Networks are trained by back-propagation
using the Adam optimizer [28] for 150k iterations (roughly two days) with parameters β1 = 0.9 
β2 = 0.999 and  = 1e − 8  batch size of 32  learning rate of 1e − 4 and weight decay of 0.0005.
During evaluation  we predict a chunk of 0.1s for each second of the test video  and average the results
across all chunks. Also  to avoid bias towards longer videos  all evaluation metrics are computed for
each video separately  and averaged across videos.

6

0100200300CountsAutos &VehiclesSportsFilm &AnimationScience &TechnologyEntertainmentTravel &EventsMusicPeople &BlogsYT-AllYT-MusicYT-CleanREC-STREET

YT-CLEAN

YT-MUSIC

YT-ALL

STFT

0.187
0.180
0.178
0.158
0.172
0.152
0.158

ENV

0.958
0.935
0.973
0.779
0.784
0.790
0.767

EMD

0.492
0.449
0.450
0.425
0.440
0.422
0.419

STFT

1.394
1.361
1.370
1.339
1.349
1.381
1.379

ENV

2.063
2.039
2.081
1.847
1.778
1.773
1.776

EMD

1.478
1.403
1.428
1.405
1.402
1.415
1.417

STFT

4.652
4.338
4.220
3.664
3.615
3.627
3.524

ENV

4.355
4.678
4.591
3.569
3.467
3.602
3.366

EMD

3.479
2.855
2.654
2.432
2.403
2.447
2.350

STFT

2.691
2.658
2.635
2.546
2.455
2.435
2.447

ENV

3.394
3.239
3.200
2.907
2.665
2.694
2.649

EMD

2.246
2.137
2.117
2.063
2.023
2.050
2.019

SPATIAL PRIOR
U-NET BASELINE
OURS-NOVIDEO
OURS-NORGB
OURS-NOFLOW
OURS-NOSEP
OURS-FULL

Table 1: Quantitative comparisons. We report three quality metrics (Sec 3.3): Envelope distance (ENV) 
Log-spectral distance (LSD)  and earth-mover’s distance (EMD)  on test videos from different datasets (Sec 3.4).
Lower is better. All results within 0.01 of the top performer are shown in bold.

T
G

s
r
u
O

Figure 3: Qualitative Results. Comparison between predicted and recorded FOA. Spatial audio is visualized
as a color overlay over the frame  with darker red indicating locations with higher audio energy.

Real time performance
sampling rate in 103ms  using a single 12GB Titan Xp GPU (3840 cores running at 1.6GHz).

The proposed procedure can generate 1s of spatial audio at 48000Hz

Baselines Since spatial audio generation is a novel task  no established methods exist for comparison
purposes. Instead  we ablate our architecture to determine the relevance of each component  and
compare it to the prior spatial distribution of audio content and a popular  domain-independent
baseline architecture. Quantitative results are shown in Table 1.
To determine the role of the visual input  we remove the RGB encoder (NORGB)  the ﬂow encoder
(NOFLOW)  or both (NOVIDEO). We also remove the separation block entirely (NOSEP)  and
multiply the localization weights with the input mono directly. The results indicate that the network
is highly relying on visual features  with NOVIDEO being one of the worse performers overall.
Interestingly  most methods performed well on REC-STREET and YT-CLEAN. However  the visual
encoder and separation block are necessary for more complex videos as in YT-MUSIC and YT-ALL.
Since the main sound sources in 360◦ videos often appear in the center  we validate the need
for a complex model by directly using the prior distribution of audio content (SPATIAL-PRIOR).
We compute the spatial prior ¯E(θ) by averaging the energy maps E(θ  t) of (Eq. 5) over all
videos in the training set. Then  to induce the same distribution on test videos  we decompose
¯E(θ) into its spherical harmonics coefﬁcients (cw  cx  cy  cz) and upconvert the input mono using
(φw(t)  φx(t)  φy(t)  φz(t)) = (1  cx/cw  cy/cw  cz/cw) i(t). As shown in Table 1  relying solely on
the prior distribution is not enough for accurate ambisonic conversion.
We ﬁnally compare to a popular encoder-decoder U-NET architecture  which has been sucessfully
applied to audio tasks such as audio super-resolution [31]. This network consists of a number of
convolutional downsampling layers that progressively reduce the dimension of the signal  distilling
higher level features  followed by a number of upsampling layers to restore the signal’s resolution. In
each upsampling layer  a skip connection is added from the encoding layer of equivalent resolution.
To generate spatial audio  we modify the U-NET architecture by setting the number of units in the
output layer to the number of ambisonic channels  and concatenate video features to the U-Net
bottleneck (i.e.  the lowest resolution layer). Our approach signiﬁcantly outperforms the U-NET
architecture  which demonstrates the importance of an architecture tailored to the task of spatial audio
generation.

7

Ground-truth

U-NET

NOAUDIO

NOSEP

OURS

Figure 4: Comparisons. Predicted FOA produced by different procedures.

Figure 5: Mono recordings. Predicted FOA on
videos recorded with a real mono microphone (un-
known FOA).

Figure 6: User studies. Percentage of videos labeled as
"Real" when viewed with audio generated by various meth-
ods (GT  OURS  U-NET and MONO) under two viewing
experiences (using a HMD device  and in-browser view-
ing). Error bars represent Wilson score intervals [45] for a
95% conﬁdence level.

Qualitative results Designing robust metrics for comparing spatial audio is an open problem 
and we found that only so much can be determined by these metrics alone. For example  fully ﬂat
predictions can have a similar EMD to a mis-placed prediction  but perceptually be much worse.
Therefore  we also rely on qualitative evaluation and a user study. Fig. 3 shows illustrative examples
of the spatial audio output of our network  and Fig. 4 shows a comparison with other baselines. To
depict spatial audio  we overlay the directional energy map E(θθθ  t) of the predicted ambisonics (Eq. 5)
over the video frame at time t. As can be seen in most of these examples  our network generates
spatial audio that has a similar spatial distribution of energy as the ground truth. Furthermore  due to
the form of the audio generator  the sound ﬁdelity of the original mono input is carried over to the
synthesized audio. These and other examples  together with the predicted spatial audio  are provided
in Supp. material.
The results shown in Table 1 and Fig. 3 use videos recorded with ambisonic microphones and
converted to mono audio. To validate whether our method extends to real mono microphones  we
scraped additional videos from YouTube that were not recorded with ambisonics  and show that we
can still generate convincing spatial audio (see Fig. 5 and Supp. material).

User study The real criteria for success is whether viewers believe that the generated audio is
correctly spatialized. To evaluate this  we conducted a “real vs fake” user study  where participants
were shown a 360◦ video and asked to decide whether the perceived location of the audio matches
the location of its sources in the video (real) or not (fake). Two studies were conducted in different
viewing environments: a popular in-browser 360◦ video viewing platform (YouTube)  and with a
head-mounted display (HMD) in a controlled environment. We recruited 32 participants from Amazon
Mechanical Turk for the in-browser study. For the HMD study  we recruited 9 participants (aged
between 20 and 32  1 female) through an engineering school email list of a large university. In both
cases  participants were asked to have normal hearing  and to listen to the audio using headphones. In
the HMD study  participants were asked to wear a KAMLE VR Headset. To familiarize participants
with the spatial audio experience  each participant was ﬁrst asked to watch two versions of a pre-
selected video with and without correct spatial audio. After the practice round  participants watched
20 randomly selected videos whose audio was generated by one of four methods: GT  the original
ground-truth recorded spatial audio; MONO  just the mono track (no spatialization); U-NET  the
baseline method; and OURS  the result of our full method. After each video  participants were asked
to decide whether its audio was real or fake. In total  280 clips per method were watched for the
in-browser study  and 45 per method in the HMD study.
The results of both studies  shown in Fig 6  support several conclusions. First  our approach
outperforms the U-NET baseline and MONO by statistically signiﬁcant margins in both studies.

8

HMDIn-Browser020406080100% Real84.472.562.255.031.140.422.235.884.4444444444444472.53521126760563GTOursU-NetMonoFOA

SOA

MONO → FOA

FOA → SOA

ENV
LSD
EMD

1.870
3.228
1.400

0.333
0.513
0.232

Ground truth

Ours

Figure 7: Limitations. Our algorithm predicts
the wrong people who are laughing in a room
full of people (top)  and the wrong violin who is
currently playing in the live performance (right).

Figure 8: Higher order ambisonics.
(Top) Examples
from our synthetic FOA to SOA conversion experiment.
(Bottom) Comparison between Mono to FOA and FOA to
SOA conversion tasks.

Second  in comparison to in-browser video platforms  HMD devices offer a more realistic viewing
experience  which enables non-spatial audio to be identiﬁed more easily. Thus  participants were
convinced by the ambisonics predicted by our approach at higher rates while wearing an HMD device
(62% HMD vs. 55% in-browser). Finally  spatial audio may not always be experienced easily  e.g. 
when the video does not contain clean sound sources. As a consequence  even videos with GT
ambisonics were misclassiﬁed in both studies at a signiﬁcant rate.

5 Discussion

Limitations We observe several cases where sound sources are not correctly separated or localized.
This occurs with challenging examples such as those with many overlapping sources  reverberant
environments which are hard to separate  or where there is an ambiguous mapping from visual
appearance to sound source (such as multiple  similar looking cars). Fig. 7 shows a few examples.
While general purpose spatial audio generation is still an open problem  we provide a ﬁrst approach.
We hope that future advances in audio-visual analysis and audio generation will enable more robust
solutions. Also  while total amount of content (in hours) is on par with other video datasets  the
number of videos is still low  due to the limited number of 360◦ video with spatial audio available
from online sources. As this number increases  our method should also improve signiﬁcantly.
Future work Although hardware trends change and we begin to see commercial cameras that
include spatial audio microphone arrays capable of recording FOA  we believe that up-converting
to spatial audio will remain relevant for a number of reasons. Besides the spatialization of legacy
recordings with only mono or stereo audio  our method can be used to further increase the ambisonics
spatial resolution  for example by up-converting ﬁrst into second-order ambisonics (SOA). Unfortu-
nately  ground-truth SOA recordings are difﬁcult to collect in-the-wild  since SOA microphones are
rare and expensive. Instead  to demonstrate future potential  we applied our approach to the FOA to
SOA conversion task  using a small synthetic dataset where pre-recorded sounds are placed at chosen
locations  which move over time in random trajectories. These are accompanied by an artiﬁcially
constructed video consisting of a random background image with identifying icons synchronized
with the sound location (see Fig. 8). The results shown in Fig. 8 indicate that converting FOA into
SOA may be signiﬁcantly easier than ZOA to FOA. This is because FOA signals already contain
substantial spatial information  and partially separated sounds. Given these ﬁndings  a promising area
for future work is to synthesize a realistic large scale SOA dataset for learning to convert FOA into
high-order ambisonics in order to support more realistic viewing experience.
Conclusion We presented the ﬁrst approach for up-converting conventional mono recordings into
spatial audio given a 360◦ video  and introduced an end-to-end trainable network tailored to this
task. We also demonstrate the beneﬁts of each component of our network and show that the proposed
generator performs substantially better than a domain independent baseline.

Acknowledgments This work was partially funded by graduate fellowship SFRH/BD/109135/2015 from the
Portuguese Ministry of Sciences and Education and NRI Grant IIS-1637941.

9

References
[1] T. Afouras  J. S. Chung  and A. Zisserman. The conversation: Deep audio-visual speech enhancement. In

Interspeech  2018. 3

[2] S. Amari  A. Cichocki  and H. H. Yang. A new learning algorithm for blind signal separation. In Advances

in Neural Information and Processing Systems (NIPS)  1996. 3

[3] S. Argentieri  P. Danès  and P. Souères. A survey on sound source localization in robotics: From binaural

to array processing methods. Computer Speech & Language  34(1):87–112  2015. 3

[4] Y. Aytar  C. Vondrick  and A. Torralba. Soundnet: Learning sound representations from unlabeled video.

In Advances in Neural Information and Processing Systems (NIPS)  2016. 3

[5] Z. Barzelay and Y. Y. Schechner. Harmony in motion. In IEEE Conf. on Computer Vision and Pattern

Recognition (CVPR)  2007. 3

[6] A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and blind

deconvolution. Neural computation  7(6):1129–1159  1995. 3

[7] P. Comon. Independent component analysis  a new concept? Signal Processing  36(3):287 – 314  1994. 3
[8] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical image

database. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)  2009. 4

[9] G. Dickins and R. Kennedy. Towards optimal soundﬁeld representation. In Audio Engineering Society

Convention  1999. 2

[10] C. Dong  C. C. Loy  K. He  and X. Tang. Learning a deep convolutional network for image super-resolution.

In European Conference on Computer Vision (ECCV)  2014. 3

[11] A. Ephrat  I. Mosseri  O. Lang  T. Dekel  K. Wilson  A. Hassidim  W. Freeman  and M. Rubinstein.
Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation.
In ACM SIGGRAPH  2018. 3

[12] A. Gabbay  A. Shamir  and S. Peleg. Visual speech enhancement using noise-invariant training.

Interspeech  2018. 3

In

[13] M. A. Gerzon. Periphony: With-height sound reproduction. J. Audio Eng. Soc  21(1):2–10  1973. 2
[14] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.
Generative adversarial nets. In Advances in Neural Information and Processing Systems (NIPS)  2014. 3
[15] A. Gray and J. Markel. Distance measures for speech processing. IEEE Transactions on Acoustics  Speech 

and Signal Processing  24(5):380–391  1976. 5

[16] K. He  X. Zhang  S. Ren  and J. Sun. Identity mappings in deep residual networks. In European Conference

on Computer Vision (ECCV)  2016. 4

[17] S. Hershey  S. Chaudhuri  D. P. Ellis  J. F. Gemmeke  A. Jansen  R. C. Moore  M. Plakal  D. Platt  R. A.
Saurous  B. Seybold  et al. CNN architectures for large-scale audio classiﬁcation. In IEEE International
Conf. on Acoustics  Speech and Signal Processing (ICASSP)  2017. 4

[18] J. Hornstein  M. Lopes  J. Santos-Victor  and F. Lacerda. Sound localization for humanoid robots-building
audio-motor maps based on the HRTF. In IEEE/RSJ International Conf. on Intelligent Robots and Systems 
2006. 3

[19] P.-S. Huang  M. Kim  M. Hasegawa-Johnson  and P. Smaragdis. Deep learning for monaural speech
separation. In IEEE International Conf. on Acoustics  Speech and Signal Processing (ICASSP)  2014. 3
[20] S. Iizuka  E. Simo-Serra  and H. Ishikawa. Let there be color!: joint end-to-end learning of global and
local image priors for automatic image colorization with simultaneous classiﬁcation. ACM Transactions
on Graphics (TOG)  35(4):110  2016. 3

[21] E. Ilg  N. Mayer  T. Saikia  M. Keuper  A. Dosovitskiy  and T. Brox. Flownet 2.0: Evolution of optical
ﬂow estimation with deep networks. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) 
2017. 4

[22] H. Izadinia  I. Saleemi  and M. Shah. Multimodal analysis for identiﬁcation and segmentation of moving-

sounding objects. IEEE Transactions on Multimedia  15(2):378–390  2013. 3

[23] R. Jozefowicz  O. Vinyals  M. Schuster  N. Shazeer  and Y. Wu. Exploring the limits of language modeling.

arXiv preprint arXiv:1602.02410  2016. 3

[24] C. Jutten and J. Herault. Blind separation of sources  part I: An adaptive algorithm based on neuromimetic

architecture. Signal Processing  24(1)  1991. 3

[25] E. Kidron  Y. Y. Schechner  and M. Elad. Pixels that sound. In IEEE Conf. on Computer Vision and Pattern

Recognition (CVPR)  2005. 3

[26] E. Kidron  Y. Y. Schechner  and M. Elad. Cross-modal localization via sparsity. IEEE Trans. on Signal

Processing (TIP)  55(4):1390–1404  2007. 3

[27] J. Kim  J. Kwon Lee  and K. Mu Lee. Accurate image super-resolution using very deep convolutional

networks. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)  2016. 3

[28] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR  abs/1412.6980  2014. 6
[29] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on Learning

Representations (ICLR)  2014. 3

10

[30] M. Kronlachner. Spatial transformations for the alteration of ambisonic recordings. Master’s thesis  Graz

University of Technology  2014. 2  6

[31] V. Kuleshov  S. Z. Enam  and S. Ermon. Audio super resolution using neural networks. In Workshops at

International Conference on Learning Representations (ICLR)  2017. 3  7

[32] E. Levina and P. Bickel. The earth mover’s distance is the mallows distance: Some insights from statistics.

In IEEE International Conference on Computer Vision (ICCV)  2001. 5

[33] A. Nagrani  J. S. Chung  and A. Zisserman. Voxceleb: A large-scale speaker identiﬁcation dataset. In

Interspeech  2017. 4

[34] K. Nakadai  H. G. Okuno  and H. Kitano. Real-time sound source localization and separation for robot

audition. In International Conference on Spoken Language Processing  2002. 3

[35] K. Nakamura  K. Nakadai  F. Asano  and G. Ince. Intelligent sound source localization and its application
to multimodal human tracking. In IEEE/RSJ International Conf. on Intelligent Robots and Systems (IROS) 
2011. 3

[36] A. v. d. Oord  S. Dieleman  H. Zen  K. Simonyan  O. Vinyals  A. Graves  N. Kalchbrenner  A. Senior  and
K. Kavukcuoglu. Wavenet: A generative model for raw audio. In 9th ISCA Speech Synthesis Workshop 
2016. 3

[37] A. Owens and A. A. Efros. Audio-visual scene analysis with self-supervised multisensory features. In

European Conference on Computer Vision (ECCV)  2018. 3

[38] A. Owens  P. Isola  J. McDermott  A. Torralba  E. H. Adelson  and W. T. Freeman. Visually indicated

sounds. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)  2016. 3

[39] O. Santala and V. Pulkki. Directional perception of distributed sound sources. The Journal of the Acoustical

Society of America  129(3):1522–1530  2011. 4

[40] J. O. Smith. Mathematics of the discrete Fourier transform (DFT): with audio applications  chapter The

Analytic Signal and Hilbert Transform Filters. Julius Smith  2007. 5

[41] M. Soler  J.-C. Bazin  O. Wang  A. Krause  and A. Sorkine-Hornung. Suggesting sounds for images from

video collections. In European Conference on Computer Vision (ECCV)  2016. 3

[42] N. Strobel  S. Spors  and R. Rabenstein. Joint audio-video object localization and tracking. IEEE Signal

Processing Magazine  18(1):22–31  2001. 3

[43] J.-M. Valin  F. Michaud  and J. Rouat. Robust localization and tracking of simultaneous moving sound
sources using beamforming and particle ﬁltering. Robotics and Autonomous Systems  55(3):216–228  2007.
3

[44] D. Wang and J. Chen. Supervised speech separation based on deep learning: An overview. IEEE/ACM

Transactions on Audio  Speech  and Language Processing  2018. 3

[45] E. B. Wilson. Probable inference  the law of succession  and statistical inference. Journal of the American

Statistical Association  22(158):209–212  1927. 8

[46] R. Zhang  P. Isola  and A. A. Efros. Colorful image colorization. In European Conference on Computer

Vision (ECCV)  2016. 3

[47] H. Zhao  C. Gan  A. Rouditchenko  C. Vondrick  J. McDermott  and A. Torralba. The sound of pixels. In

European Conference on Computer Vision (ECCV)  2018. 3

11

,Pedro Morgado
Nuno Nvasconcelos
Timothy Langlois
Oliver Wang