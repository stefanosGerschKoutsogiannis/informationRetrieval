2015,A Nonconvex Optimization Framework for Low Rank Matrix Estimation,We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation  nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However  the understanding of its theoretical guarantees are limited. In this paper  we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing and completion. In particular  we prove that a broad class of nonconvex optimization algorithms  including alternating minimization and gradient-type methods  geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.,A Nonconvex Optimization Framework for Low Rank

Matrix Estimation⇤

Tuo Zhao

Johns Hopkins University

Zhaoran Wang

Han Liu

Princeton University

Abstract

We study the estimation of low rank matrices via nonconvex optimization. Com-
pared with convex relaxation  nonconvex optimization exhibits superior empirical
performance for large scale instances of low rank matrix estimation. However  the
understanding of its theoretical guarantees are limited. In this paper  we deﬁne the
notion of projected oracle divergence based on which we establish sufﬁcient condi-
tions for the success of nonconvex optimization. We illustrate the consequences
of this general framework for matrix sensing. In particular  we prove that a broad
class of nonconvex optimization algorithms  including alternating minimization
and gradient-type methods  geometrically converge to the global optimum and
exactly recover the true low rank matrices under standard conditions.

Introduction

1
Let M⇤ 2 Rm⇥n be a rank k matrix with k much smaller than m and n. Our goal is to estimate
M⇤ based on partial observations of its entires. For example  matrix sensing is based on linear
measurements hAi  M⇤i  where i 2{ 1  . . .   d} with d much smaller than mn and Ai is the sensing
matrix. In the past decade  signiﬁcant progress has been established on the recovery of low rank matrix
[4  5  23  18  15  16  12  22  7  25  19  6  14  11  13  8  9  10  27]. Among all these existing works  most
are based upon convex relaxation with nuclear norm constraint or regularization. Nevertheless  solving
these convex optimization problems can be computationally prohibitive in high dimensional regimes
with large m and n [27]. A computationally more efﬁcient alternative is nonconvex optimization. In
particular  we reparameterize the m ⇥ n matrix variable M in the optimization problem as U V >
with U 2 Rm⇥k and V 2 Rn⇥k  and optimize over U and V . Such a reparametrization automatically
enforces the low rank structure and leads to low computational cost per iteration. Due to this reason 
the nonconvex approach is widely used in large scale applications such as recommendation systems
[17].
Despite the superior empirical performance of the nonconvex approach  the understanding of its
theoretical guarantees is relatively limited in comparison with the convex relaxation approach. Only
until recently has there been progress on coordinate descent-type nonconvex optimization methods 
which is known as alternating minimization [14  8  9  10]. They show that  provided a desired
initialization  the alternating minimization algorithm converges at a geometric rate to U⇤ 2 Rm⇥k
and V ⇤ 2 Rn⇥k  which satisfy M = U⇤V ⇤>. Meanwhile  [15  16] establish the convergence of
gradient-type methods  and [27] further establish the convergence of a broad class of nonconvex
algorithms including both gradient-type and coordinate descent-type methods. However  [15  16  27]
only establish the asymptotic convergence for an inﬁnite number of iterations  rather than the explicit
rate of convergence. Besides these works  [18  12  13] consider projected gradient-type methods 
which optimize over the matrix variable M 2 Rm⇥n rather than U 2 Rm⇥k and V 2 Rn⇥k. These
methods involve calculating the top k singular vectors of an m ⇥ n matrix at each iteration. For
⇤Research supported by NSF IIS1116730  NSF IIS1332109  NSF IIS1408910  NSF IIS1546482-BIGDATA 
NSF DMS1454377-CAREER  NIH R01GM083084  NIH R01HG06841  NIH R01MH102339  and FDA
HHSF223201000072C.

1

k much smaller than m and n  they incur much higher computational cost per iteration than the
aforementioned methods that optimize over U and V . All these works  except [27]  focus on speciﬁc
algorithms  while [27] do not establish the explicit optimization rate of convergence.
In this paper  we propose a general framework that uniﬁes a broad class of nonconvex algorithms
for low rank matrix estimation. At the core of this framework is a quantity named projected oracle
divergence  which sharply captures the evolution of generic optimization algorithms in the presence
of nonconvexity. Based on the projected oracle divergence  we establish sufﬁciently conditions under
which the iteration sequences geometrically converge to the global optima. For matrix sensing  a direct
consequence of this general framework is that  a broad family of nonconvex algorithms  including
gradient descent  coordinate gradient descent and coordinate descent  converge at a geometric rate
to the true low rank matrices U⇤ and V ⇤. In particular  our general framework covers alternating
minimization as a special case and recovers the results of [14  8  9  10] under standard conditions.
Meanwhile  our framework covers gradient-type methods  which are also widely used in practice
[28  24]. To the best of our knowledge  our framework is the ﬁrst one that establishes exact recovery
guarantees and geometric rates of convergence for a broad family of nonconvex matrix sensing
algorithms.
To achieve maximum generality  our uniﬁed analytic framework signiﬁcantly differs from previous
works. In detail  [14  8  9  10] view alternating minimization as a perturbed version of the power
method. However  their point of view relies on the closed form solution of each iteration of alternating
minimization  which makes it hard to generalize to other algorithms  e.g.  gradient-type methods.
Meanwhile  [27] take a geometric point of view. In detail  they show that the global optimum of the
optimization problem is the unique stationary point within its neighborhood and thus a broad class of
algorithms succeed. However  such geometric analysis of the objective function does not characterize
the convergence rate of speciﬁc algorithms towards the stationary point. Unlike existing analytic
frameworks  we analyze nonconvex optimization algorithms as perturbed versions of their convex
counterparts. For example  under our framework we view alternating minimization as a perturbed
version of coordinate descent on convex objective functions. We use the key quantity  projected oracle
divergence  to characterize such a perturbation effect  which results from the local nonconvexity
at intermediate solutions. This framework allows us to establish explicit rate of convergence in an
analogous way as existing convex optimization analysis.
Notation: For a vector v = (v1  . . .   vd)T 2 Rd  let the vector `q norm be kvkq
j . For a
matrix A 2 Rm⇥n  we use A⇤j = (A1j  ...  Amj)> to denote the j-th column of A  and Ai⇤ =
(Ai1  ...  Ain)> to denote the i-th row of A. Let max(A) and min(A) be the largest and smallest
nonzero singular values of A. We deﬁne the following matrix norms: kAk2
2  kAk2 =
max(A). Moreover  we deﬁne kAk⇤ to be the sum of all singular values of A. Given another matrix
B 2 Rm⇥n  we deﬁne the inner product as hA  Bi = Pi j AijBij. We deﬁne ei as an indicator
vector  where the i-th entry is one  and all other entries are zero. For a bivariate function f (u  v)  we
deﬁne ruf (u  v) to be the gradient with respect to u. Moreover  we use the common notations of
⌦(·)  O(·)  and o(·) to characterize the asymptotics of two real sequences.

q = Pj vq
F =Pj kA⇤jk2

2 Problem Formulation and Algorithms

Let M⇤ 2 Rm⇥n be the unknown low rank matrix of interest. We have d sensing matrices Ai 2
Rm⇥n with i 2{ 1  . . .   d}. Our goal is to estimate M⇤ based on bi = hAi  M⇤i in the high
dimensional regime with d much smaller than mn. Under such a regime  a common assumption
is rank(M⇤) = k ⌧ min{d  m  n}. Existing approaches generally recover M⇤ by solving the
following convex optimization problem
(2.1)

min

subject to b = A(M ) 

M2Rm⇥n kMk⇤
A(M ) = [hA1  Mi  ... hAi  Mi]> 2 Rd.

where b = [b1  ...  bd]> 2 Rd  and A(M ) : Rm⇥n ! Rd is an operator deﬁned as
(2.2)
Existing convex optimization algorithms for solving (2.1) are computationally inefﬁcient  in the sense
that they incur high per-iteration computational cost  and only attain sublinear rates of convergence to
the global optimum [14]. Instead  in large scale settings we usually consider the following nonconvex

2

optimization problem

min

U2Rm⇥k V 2Rn⇥k F(U  V ). where F(U  V ) =

1
2kb A (U V >)k2
2.

(2.3)

i=1  {Ai}d

i=1

The reparametrization of M = U V >  though making the optimization problem in (2.3) nonconvex 
signiﬁcantly improves the computational efﬁciency. Existing literature [17  28  21  24] has established
convincing empirical evidence that (2.3) can be effectively solved by a board variety of gradient-based
nonconvex optimization algorithms  including gradient descent  alternating exact minimization (i.e. 
alternating least squares or coordinate descent)  as well as alternating gradient descent (i.e.  coordinate
gradient descent)  which are shown in Algorithm 1.
It is worth noting the QR decomposition and rank k singular value decomposition in Algorithm
1 can be accomplished efﬁciently. In particular  the QR decomposition can be accomplished in
O(k2 max{m  n}) operations  while the rank k singular value decomposition can be accomplished
in O(kmn) operations. In fact  the QR decomposition is not necessary for particular update schemes 
e.g.  [14] prove that the alternating exact minimization update schemes with or without the QR
decomposition are equivalent.
Algorithm 1 A family of nonconvex optimization algorithms for matrix sensing. Here (U   D  V ) 
KSVD(M ) is the rank k singular value decomposition of M. Here D is a diagonal matrix containing
the top k singular values of M in decreasing order  and U and V contain the corresponding top k left
and right singular vectors of M. Here (V   RV ) QR(V ) is the QR decomposition  where V is the
corresponding orthonormal matrix and RV is the corresponding upper triangular matrix.
Input: {bi}d
Parameter: Step size ⌘  Total number of iterations T
(U (0)  D(0)  V (0)) KSVD(Pd
For: t = 0  ....  T  1
Alternating Exact Minimization : V (t+0.5) argminV F(U (t)  V )
(V (t+1)  R(t+0.5)
) QR(V (t+0.5))
Alternating Gradient Descent : V (t+0.5) V (t)  ⌘rV F(U (t)  V (t))
) QR(V (t+0.5))  U (t) U (t)R(t+0.5)>
(V (t+1)  R(t+0.5)
Gradient Descent : V (t+0.5) V (t)  ⌘rV F(U (t)  V (t))
) QR(V (t+0.5))  U (t+1) U (t)R(t+0.5)>
(V (t+1)  R(t+0.5)
Alternating Exact Minimization : U (t+0.5) argminU F(U  V (t+1))
(U (t+1)  R(t+0.5)
Alternating Gradient Descent : U (t+0.5) U (t)  ⌘rUF(U (t)  V (t+1))
(U (t+1)  R(t+0.5)
Gradient Descent : U (t+0.5) U (t)  ⌘rUF(U (t)  V (t))
(U (t+1)  R(t+0.5)

) QR(U (t+0.5))
) QR(U (t+0.5))  V (t+1) V t+1R(t+0.5)>
) QR(U (t+0.5))  V (t+1) V tR(t+0.5)>

i=1 biAi)  V (0) V (0)D(0)  U (0) U (0)D(0)

End for
Output: M (T ) U (T0.5)V (T )> (for gradient descent we use U (T )V (T )>)
3 Theoretical Analysis
We analyze the convergence properties of the general family of nonconvex optimization algorithms
illustrated in §2. Before we present the main results  we ﬁrst introduce a uniﬁed analytic framework
based on a key quantity named projected oracle divergence. Such a uniﬁed framework equips our
theory with the maximum generality. Without loss of generality  we assume m  n throughout the
rest of this paper.

9>>>>>>>>>=>>>>>>>>>;
9>>>>>>>>>=>>>>>>>>>;

Updating U

Updating V

V

V

V

U

U

U

U

V

V

U

3.1 Projected Oracle Divergence
We ﬁrst provide an intuitive explanation for the success of nonconvex optimization algorithms  which
forms the basis of our later proof for the main results. Recall that (2.3) is a special instance of the
following optimization problem 

(3.1)
A key observation is that  given ﬁxed U  f (U ·) is strongly convex and smooth in V under suitable
conditions  and the same also holds for U given ﬁxed V correspondingly. For the convenience of

U2Rm⇥k V 2Rn⇥k

f (U  V ).

min

3

discussion  we summarize this observation in the following technical condition  which will be later
veriﬁed for matrix sensing under suitable conditions.

Condition 3.1 (Strong Biconvexity and Bismoothness). There exist universal constants µ+ > 0 and
µ > 0 such that
µ
2 kU0  Uk2
µ
2 kV 0  V k2

F  f (U0  V )  f (U  V )  hU0  U rU f (U  V )i 
F  f (U  V 0)  f (U  V )  hV 0  V rV f (U  V )i 

µ+
2 kU0  Uk2
µ+
2 kV 0  V k2

F for all U  U0 

F for all V  V 0.

For the simplicity of discussion  for now we assume U⇤ and V ⇤ are the unique global minimizers to
the generic optimization problem in (3.1). Assuming U⇤ is given  we can obtain V ⇤ by

Condition 3.1 implies the objective function in (3.2) is strongly convex and smooth. Hence  we can
choose any gradient-based algorithm to obtain V ⇤. For example  we can directly solve for V ⇤ in

V ⇤ = argmin
V 2Rn⇥k

f (U⇤  V ).

(3.2)

(3.3)

or iteratively solve for V ⇤ using gradient descent  i.e. 

rV f (U⇤  V ) = 0 

(3.4)
where ⌘ is the step size. For the simplicity of discussion  we put aside the renormalization issue for
now. In the example of gradient descent  by invoking classical convex optimization results [20]  it is
easy to prove that

V (t) = V (t1)  ⌘rV f (U⇤  V (t1)) 

kV (t)  V ⇤kF  kV (t1)  V ⇤kF for all t = 0  1  2  . . .  

where  2 (0  1) is a contraction coefﬁcient  which depends on µ+ and µ in Condition 3.1.
However  the ﬁrst-order oracle rV f (U⇤  V (t1)) is not accessible in practice  since we do not know
U⇤. Instead  we only have access to rV f (U  V (t1))  where U is arbitrary. To characterize the
divergence between the ideal ﬁrst-order oracle rV f (U⇤  V (t1)) and the accessible ﬁrst-order oracle
rV f (U  V (t1))  we deﬁne a key quantity named projected oracle divergence  which takes the form
D(V  V 0  U ) =⌦rV f (U⇤  V 0)  rV f (U  V 0)  V  V ⇤/(kV  V ⇤kF)↵ 
(3.5)
where V 0 is the point for evaluating the gradient. In the above example  it holds for V 0 = V (t1).
Later we will illustrate that  the projection of the difference of ﬁrst-order oracles onto a speciﬁc one
dimensional space  i.e.  the direction of V  V ⇤  is critical to our analysis. In the above example of
gradient descent  we will prove later that for V (t) = V (t1)  ⌘rV f (U  V (t1))  we have
(3.6)
In other words  the projection of the divergence of ﬁrst-order oracles onto the direction of V (t)  V ⇤
captures the perturbation effect of employing the accessible ﬁrst-order oracle rV f (U  V (t1)) instead
of the ideal rV f (U⇤  V (t1)). For V (t+1) = argminV f (U  V )  we will prove that
(3.7)
According to the update schemes shown in Algorithm 1  for alternating exact minimization  we set
U = U (t) in (3.7)  while for gradient descent or alternating gradient descent  we set U = U (t1) or
U = U (t) in (3.6) respectively. Correspondingly  similar results hold for kU (t)  U⇤kF.
To establish the geometric rate of convergence towards the global minima U⇤ and V ⇤  it remains to
establish upper bounds for the projected oracle divergence. In the example of gradient decent we will
prove that for some ↵ 2 (0  1  ) 

kV (t)  V ⇤kF  kV (t1)  V ⇤kF + 2/µ+ · D(V (t)  V (t1)  U ).

kV (t)  V ⇤kF  1/µ · D(V (t)  V (t)  U ).

which together with (3.6) (where we take U = U (t1)) implies

2/µ+ · D(V (t)  V (t1)  U (t1))  ↵kU (t1)  U⇤kF 

kV (t)  V ⇤kF  kV (t1)  V ⇤kF + ↵kU (t1)  U⇤kF.

Correspondingly  similar results hold for kU (t)  U⇤kF  i.e. 
Combining (3.8) and (3.9) we then establish the contraction

kU (t)  U⇤kF  kU (t1)  U⇤kF + ↵kV (t1)  V ⇤kF.

(3.8)

(3.9)

max{kV (t)  V ⇤kF kU (t)  U⇤kF} (↵ + ) · max{kV (t1)  V ⇤kF kU (t1)  U⇤kF} 

4

which further implies the geometric convergence  since ↵ 2 (0  1 ). Respectively  we can establish
similar results for alternating exact minimization and alternating gradient descent. Based upon such a
uniﬁed analytic framework  we now simultaneously establish the main results.
Remark 3.2. Our proposed projected oracle divergence is inspired by previous work [3  2  1] 
which analyzes the Wirtinger Flow algorithm for phase retrieval  the expectation maximization (EM)
Algorithm for latent variable models  and the gradient descent algorithm for sparse coding. Though
their analysis exploits similar nonconvex structures  they work on completely different problems  and
the delivered technical results are also fundamentally different.

3.2 Matrix Sensing
Before we present our main results  we ﬁrst introduce an assumption known as the restricted isometry
property (RIP). Recall that k is the rank of the target low rank matrix M⇤.
Assumption 3.3. The linear operator A(·) : Rm⇥n ! Rd deﬁned in (2.2) satisﬁes 2k-RIP with
parameter 2k 2 (0  1)  i.e.  for all  2 Rm⇥n such that rank()  2k  it holds that

(1  2k)kk2

F  kA()k2

2  (1 + 2k)kk2
F.

2k kn log n).

Several random matrix ensembles satisfy k-RIP for a sufﬁciently large d with high probability. For
example  suppose that each entry of Ai is independently drawn from a sub-Gaussian distribution 
A(·) satisﬁes 2k-RIP with parameter 2k with high probability for d =⌦( 2
The following theorem establishes the geometric rate of convergence of the nonconvex optimization
algorithms summarized in Algorithm 1.
Theorem 3.4. Assume there exists a sufﬁciently small constant C1 such that A(·) satisﬁes 2k-RIP
with 2k  C1/k  and the largest and smallest nonzero singular values of M⇤ are constants  which do
not scale with (d  m  n  k). For any pre-speciﬁed precision ✏  there exist an ⌘ and universal constants
C2 and C3 such that for all T  C2 log(C3/✏)  we have kM (T )  M⇤kF  ✏.
The proof of Theorems 3.4 is provided in Appendices 4.1  A.1  and A.2. Theorem 3.4 implies that all
three nonconvex optimization algorithms geometrically converge to the global optimum. Moreover 
assuming that each entry of Ai is independently drawn from a sub-Gaussian distribution with mean
zero and variance proxy one  our result further suggests  to achieve exact low rank matrix recovery 
our algorithm requires the number of measurements d to satisfy

d =⌦( k3n log n) 

(3.10)
since we assume that 2k  C1/k. This sample complexity result matches the state-of-the-art result
for nonconvex optimization methods  which is established by [14]. In comparison with their result 
which only covers the alternating exact minimization algorithm  our results holds for a broader variety
of nonconvex optimization algorithms.
Note that the sample complexity in (3.10) depends on a polynomial of max(M⇤)/min(M⇤)  which
is treated as a constant in our paper. If we allow max(M⇤)/min(M⇤) to increase with the dimension 
we can plug the nonconvex optimization algorithms into the multi-stage framework proposed by [14].
Following similar lines to the proof of Theorem 3.4  we can derive a new sample complexity  which
is independent of max(M⇤)/min(M⇤). See more details in [14].

4 Proof of Main Results
Due to space limitation  we only sketch the proof of Theorem 3.4 for alternating exact minimization.
The proof of Theorem 3.4 for alternating gradient descent and gradient descent  and related lemmas
are provided in the appendix. For notational simplicity  let 1 = max(M⇤) and k = min(M⇤).
Before we proceed with the main proof  we ﬁrst introduce the following lemma  which veriﬁes
Condition 3.1.
Lemma 4.1. Suppose that A(·) satisﬁes 2k-RIP with parameter 2k. Given an arbitrary orthonormal
matrix U 2 Rm⇥k  for any V  V 0 2 Rn⇥k  we have
1 + 2k

kV 0  V k2

F F (U   V 0) F (U   V )  hrV F(U   V )  V 0  V i 

kV 0  V k2
F.

2

1  2k

2

The proof of Lemma 4.1 is provided in Appendix B.1. Lemma 4.1 implies that F(U  ·) is strongly
convex and smooth in V given a ﬁxed orthonormal matrix U  as speciﬁed in Condition 3.1. Equipped
with Lemma 4.1  we now lay out the proof for each update scheme in Algorithm 1.

5

V ⇤(t)>  where U⇤(t) is orthonormal. We choose the projected oracle divergence as
V (t+0.5)V ⇤(t)

4.1 Proof of Theorem 3.4 (Alternating Exact Minimization)
Proof. Throughout the proof of alternating exact minimization  we deﬁne a constant ⇠ 2 (1 1)
for notational simplicity. We assume that at the t-th iteration  there exists a matrix factorization of
M⇤ = U⇤(t)
kV (t+0.5)V ⇤(t)kF.
D(V (t+0.5)  V (t+0.5)  U
Remark 4.2. Note that the matrix factorization is not necessarily unique. Because given a factoriza-
tion of M⇤ = U V >  we can always obtain a new factorization of M⇤ = eUeV >  where eU = U O and
eV = V O for an arbitrary unitary matrix O 2 Rk⇥k. However  this is not a issue to our convergence

analysis. As will be shown later  we can prove that there always exists a factorization of M⇤ satisfying
the desired computational properties for each iteration (See Lemma 4.5  Corollaries 4.7 and 4.8).

)=⌧rV F(U⇤(t)

  V (t+0.5))rV F(U

  V (t+0.5)) 

(t)

(t)

2k 

and kU
) 

The following lemma establishes an upper bound for the projected oracle divergence.
Lemma 4.3. Suppose that 2k and U
p2(1  2k)2k

(t) satisfy

.

(t)  U⇤(t)kF 

(1  2k)k
4⇠(1 + 2k)1

(t)

2⇠

kU

(1  2k)k

(t)  U⇤(t)kF.

4⇠k(1 + 2k)1
Then we have D(V (t+0.5)  V (t+0.5)  U
The proof of Lemma 4.3 is provided in Appendix B.2. Lemma 4.3 shows that the projected oracle di-
(t).The following lemma quantiﬁes
vergence for updating V diminishes with the estimation error of U
the progress of an exact minimization step using the projected oracle divergence.
Lemma 4.4. We have kV (t+0.5)  V ⇤(t)kF  1/(1  2k) · D(V (t+0.5)  V (t+0.5)  U
The proof of Lemma 4.4 is provided in Appendix B.3. Lemma 4.4 illustrates that the estimation error
of V (t+0.5) diminishes with the projected oracle divergence. The following lemma characterizes the
effect of the renormalization step using QR decomposition.
Lemma 4.5. Suppose that V (t+0.5) satisﬁes

).

(t)

(4.1)

(0).

kV (t+0.5)  V ⇤(t)kF  k/4.
(t+1)  V ⇤(t+1)kF  2/k ·k V (t+0.5)  V ⇤(t)kF.

(4.2)
Then there exists a factorization of M⇤ = U⇤(t+1)V ⇤(t+1) such that V ⇤(t+0.5) 2 Rn⇥k is an
orthonormal matrix  and satisﬁes kV
The proof of Lemma 4.5 is provided in Appendix B.4. The next lemma quantiﬁes the accuracy of the
initialization U
Lemma 4.6. Suppose that 2k satisﬁes
2k 

(1  2k)24
(4.3)
V ⇤(0)> such that U⇤(0) 2 Rm⇥k is an orthonormal
.

Then there exists a factorization of M⇤ = U⇤(0)
(0)  U⇤kF  (12k)k
matrix  and satisﬁes kU
The proof of Lemma 4.6 is provided in Appendix B.5. Lemma 4.6 implies that the initial solution
U

(0) attains a sufﬁciently small estimation error.

192⇠2k(1 + 2k)24
1

4⇠(1+2k)1

k

.

Combining the above Lemmas  we obtain the next corollary for a complete iteration of updating V .
Corollary 4.7. Suppose that 2k and U
(1  2k)24

(t) satisfy

k

and kU

(t)  U⇤(t)kF 

(1  2k)k
4⇠(1 + 2k)1

192⇠2k(1 + 2k)24
1

2k 

.

(4.4)
(t+1)  V ⇤(t+1)kF 

1

We then have kV
⇠kU

(t+1)  V ⇤(t+1)kF  (12k)k
(t)  U⇤(t)kF and kV (t+0.5)  V ⇤(t)kF  k
6

4⇠(1+2k)1

. Moreover  we also have kV
2⇠ kU

(t)  U⇤(t)kF.

k

2k 

(1  2k)24

192⇠2k(1 + 2k)24
1

The proof of Corollary 4.7 is provided in Appendix B.6. Since the alternating exact minimization
algorithm updates U and V in a symmetric manner  we can establish similar results for a complete
iteration of updating U in the next corollary.
Corollary 4.8. Suppose that 2k and V

(t+1)  V ⇤(t+1)kF 
V ⇤(t+1)> such U⇤(t+1) is an orthonormal matrix 
(t+1)  U⇤(t+1)kF 
. Moreover  we also have kU

(t+1) satisfy
and
kV
Then there exists a factorization of M⇤ = U⇤(t+1)
(t+1)  U⇤(t+1)kF  (12k)k
and satisﬁes kU
(t+1)  V ⇤(t+1)kF.
⇠kV
The proof of Corollary 4.8 directly follows Appendix B.6  and is therefore omitted.
We then proceed with the proof of Theorem 3.4 for alternating exact minimization. Lemma 4.6
(0). Then Corollary 4.7 ensures that (4.5) of Corollary
ensures that (4.4) of Corollary 4.7 holds for U
(1). By induction  Corollaries 4.7 and 4.8 can be applied recursively for all T iterations.
4.8 holds for V
Thus we obtain
kV

(t+1)  V ⇤(t+1)kF and kU (t+0.5)  U⇤(t+1)kF  k

(T )  V ⇤(T )kF 

(1  2k)k
4⇠(1 + 2k)1

2⇠ kV

4⇠(1+2k)1

(4.5)

1

.

(T1)  V ⇤(T1)kF
(1  2k)k

4⇠2T (1 + 2k)1

 

(4.6)

where the last inequality comes from Lemma 4.6. Therefore  for a pre-speciﬁed accuracy ✏  we need

1

1
⇠kU

···

(T1)  U⇤(T1)kF 
⇠2T1kU

1
⇠2kV
(0)  U⇤(0)kF 
2✏(1+2k)1⌘ log1 ⇠m iterations such that

kV

(T )  V ⇤(T )kF 

(1  2k)k

4⇠2T (1 + 2k)1 

✏
2

.

at most T =l1/2 · log⇣ (12k)k

Moreover  Corollary 4.8 implies

 

(4.7)

(4.8)

where the last inequality comes from (4.6). Therefore  we need at most

(1  2k)2

k

8⇠2T +1(1 + 2k)1

kU (T0.5)  U⇤(T )kF 

k
2⇠ kV

(T )  V ⇤(T )kF 
4⇠✏(1 + 2k)◆ log1 ⇠⇡

k

T =⇠1/2 · log✓ (1  2k)2
kU (T0.5)  U⇤kF 

(1  2k)2

8⇠2T +1(1 + 2k)1 

k

✏
21

.

iterations such that

Then combining (4.7) and (4.8)  we obtain

kM (T )  M⇤k = kU (T0.5)V

(T )>  U⇤(T )V ⇤(T )>kF

where the last inequality is from kV
1 (since U⇤(T )V ⇤(T )> = M⇤ and V ⇤(T ) is orthonormal). Thus we complete the proof.

(T )k2 = 1 (since V

 kV

(T )k2kU (T0.5)  U⇤(T )kF + kU⇤(T )k2kV

(T )  V ⇤(T )kF  ✏ 

(4.9)
(T ) is orthonormal) and kU⇤k2 = kM⇤k2 =

5 Extension to Matrix Completion
Under the same setting as matrix sensing  we observe a subset of the entries of M⇤  namely  W✓
{1  . . .   m}⇥{ 1  . . .   n}. We assume that W is drawn uniformly at random  i.e.  M⇤i j is observed
independently with probability ¯⇢ 2 (0  1]. To exactly recover M⇤  a common assumption is the
incoherence of M⇤  which will be speciﬁed later. A popular approach for recovering M⇤ is to solve
the following convex optimization problem

(5.1)
where PW (M ) : Rm⇥n ! Rm⇥n is an operator deﬁned as [PW (M )]ij = Mij if (i  j) 2W   and
0 otherwise. Similar to matrix sensing  existing algorithms for solving (5.1) are computationally

subject to PW (M⇤) = PW (M ) 

M2Rm⇥n kMk⇤

min

7

inefﬁcient. Hence  in practice we usually consider the following nonconvex optimization problem

min

U2Rm⇥k V 2Rn⇥k FW (U  V )  where FW (U  V ) = 1/2 · kPW (M⇤) P W (U V >)k2
F.

Similar to matrix sensing  (5.2) can also be efﬁciently solved by gradient-based algorithms. Due to
space limitation  we present these matrix completion algorithms in Algorithm 2 of Appendix D. For
the convenience of later convergence analysis  we partition the observation set W into 2T + 1 subsets
W0 ... W2T using Algorithm 4 in Appendix D. However  in practice we do not need the partition
scheme  i.e.  we simply set W0 = ··· = W2T = W.
Before we present the main results  we introduce an assumption known as the incoherence property.
Assumption 5.1. The target rank k matrix M⇤ is incoherent with parameter µ  i.e.  given the rank k
singular value decomposition of M⇤ = U⇤⌃⇤V ⇤>  we have

(5.2)

max

i kU⇤

i⇤k2  µpk/m and max
j kV ⇤

j⇤k2  µpk/n.

The incoherence assumption guarantees that M⇤ is far from a sparse matrix  which makes it feasible
to complete M⇤ when its entries are missing uniformly at random. The following theorem establishes
the iteration complexity and the estimation error under the Frobenius norm.
Theorem 5.2. Suppose that there exists a universal constant C4 such that ¯⇢ satisﬁes

¯⇢  C4µ2k3 log n log(1/✏)/m 

(5.3)
where ✏ is the pre-speciﬁed precision. Then there exist an ⌘ and universal constants C5 and C6 such
that for any T  C5 log(C6/✏)  we have kM (T )  MkF  ✏ with high probability.
Due to space limit  we defer the proof of Theorem 5.2 to the longer version of this paper. Theorem
5.2 implies that all three nonconvex optimization algorithms converge to the global optimum at a
geometric rate. Furthermore  our results indicate that the completion of the true low rank matrix M⇤
up to ✏-accuracy requires the entry observation probability ¯⇢ to satisfy

(5.4)
This result matches the result established by [8]  which is the state-of-the-art result for alternating
minimization. Moreover  our analysis covers three nonconvex optimization algorithms.

¯⇢ =⌦( µ2k3 log n log(1/✏)/m).

6 Experiments
We present numerical experiments for matrix sensing to support our theoretical analysis. We choose
m = 30  n = 40  and k = 5  and vary d from 300 to 900. Each entry of Ai’s are independent sampled

from N (0  1). We then generate M = U V >  where eU 2 Rm⇥k and eV 2 Rn⇥k are two matrices
with all their entries independently sampled from N (0  1/k). We then generate d measurements by
bi = hAi  Mi for i = 1  ...  d. Figure 1 illustrates the empirical performance of the alternating exact
minimization and alternating gradient descent algorithms for a single realization. The step size for the
alternating gradient descent algorithm is determined by the backtracking line search procedure. We
see that both algorithms attain linear rate of convergence for d = 600 and d = 900. Both algorithms
fail for d = 300  because d = 300 is below the minimum requirement of sample complexity for the
exact matrix recovery.

r
o
r
r
E
 
n
o
i
t
a
m
i
t
s
E

100

10-5

0

d = 300
d = 600
d = 900
10
30
Number of Iterations

20

40

r
o
r
r
E
 
n
o
i
t
a
m
i
t
s
E

100

10-5

0

d = 300
d = 600
d = 900
10
30
Number of Iterations

20

40

(a) Alternating Exact Minimization

(b) Alternating Gradient Descent

Figure 1: Two illustrative examples for matrix sensing. The vertical axis corresponds to estimation
error kM (t)  MkF. The horizontal axis corresponds to numbers of iterations. Both the alternating
exact minimization and alternating gradient descent algorithms attain linear rate of convergence for
d = 600 and d = 900. But both algorithms fail for d = 300  because d = 300 is below the minimum
requirement of sample complexity for the exact matrix recovery.

8

References
[1] Sanjeev Arora  Rong Ge  Tengyu Ma  and Ankur Moitra. Simple  efﬁcient  and neural algorithms for sparse

coding. arXiv preprint arXiv:1503.00778  2015.

[2] Sivaraman Balakrishnan  Martin J Wainwright  and Bin Yu. Statistical guarantees for the EM algorithm:

From population to sample-based analysis. arXiv preprint arXiv:1408.2156  2014.

[3] Emmanuel J Cand`es  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory

and algorithms. IEEE Transactions on Information Theory  61(4):1985–2007  2015.

[4] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational Mathematics  9(6):717–772  2009.

[5] Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix completion.

IEEE Transactions on Information Theory  56(5):2053–2080  2010.

[6] Yudong Chen. Incoherence-optimal matrix completion. arXiv preprint arXiv:1310.0154  2013.
[7] David Gross. Recovering low-rank matrices from few coefﬁcients in any basis. IEEE Transactions on

Information Theory  57(3):1548–1566  2011.

[8] Moritz Hardt. Understanding alternating minimization for matrix completion. In Symposium on Founda-

tions of Computer Science  pages 651–660  2014.

[9] Moritz Hardt  Raghu Meka  Prasad Raghavendra  and Benjamin Weitz. Computational limits for matrix

completion. arXiv preprint arXiv:1402.2331  2014.

[10] Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. arXiv preprint

arXiv:1407.4070  2014.

[11] Trevor Hastie  Rahul Mazumder  Jason Lee  and Reza Zadeh. Matrix completion and low-rank SVD via

fast alternating least squares. arXiv preprint arXiv:1410.2596  2014.

[12] Prateek Jain  Raghu Meka  and Inderjit S Dhillon. Guaranteed rank minimization via singular value

projection. In Advances in Neural Information Processing Systems  pages 937–945  2010.

[13] Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with ﬁnite samples. arXiv preprint

arXiv:1411.1087  2014.

[14] Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank matrix completion using alternating

minimization. In Symposium on Theory of Computing  pages 665–674  2013.

[15] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from a few entries.

IEEE Transactions on Information Theory  56(6):2980–2998  2010.

[16] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from noisy entries.

Journal of Machine Learning Research  11:2057–2078  2010.

[17] Yehuda Koren. The Bellkor solution to the Netﬂix grand prize. Netﬂix Prize Documentation  81  2009.
[18] Kiryung Lee and Yoram Bresler. Admira: Atomic decomposition for minimum rank approximation. IEEE

Transactions on Information Theory  56(9):4402–4416  2010.

[19] Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and

high-dimensional scaling. The Annals of Statistics  39(2):1069–1097  2011.

[20] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer  2004.
[21] Arkadiusz Paterek. Improving regularized singular value decomposition for collaborative ﬁltering. In

Proceedings of KDD Cup and workshop  volume 2007  pages 5–8  2007.

[22] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research 

12:3413–3430  2011.

[23] Benjamin Recht  Maryam Fazel  and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix

equations via nuclear norm minimization. SIAM Review  52(3):471–501  2010.

[24] Benjamin Recht and Christopher R´e. Parallel stochastic gradient algorithms for large-scale matrix comple-

tion. Mathematical Programming Computation  5(2):201–226  2013.

[25] Angelika Rohde and Alexandre B Tsybakov. Estimation of high-dimensional low-rank matrices. The

Annals of Statistics  39(2):887–930  2011.

[26] Gilbert W Stewart  Ji-guang Sun  and Harcourt B Jovanovich. Matrix perturbation theory  volume 175.

Academic press New York  1990.

[27] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. arXiv preprint

arXiv:1411.8003  2014.

[28] G´abor Tak´acs  Istv´an Pil´aszy  Botty´an N´emeth  and Domonkos Tikk. Major components of the gravity

recommendation system. ACM SIGKDD Explorations Newsletter  9(2):80–83  2007.

9

,Sebastien Bubeck
Che-Yu Liu
Zhaoran Wang
Han Liu
Kirill Struminsky
Simon Lacoste-Julien
Anton Osokin