2017,Is the Bellman residual a bad proxy?,This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose  we place ourselves in the framework of policy search algorithms  that are usually designed to maximize the mean value  and derive a method that minimizes the residual $\|T_* v_\pi - v_\pi\|_{1 \nu}$ over policies. A theoretical analysis shows how good this proxy is to policy optimization  and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes  specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better  despite the current lack of deep theoretical analysis. This might seem obvious  as directly addressing the problem of interest is usually better  but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning  we believe that this question is worth to be considered.,Is the Bellman residual a bad proxy?

Matthieu Geist1  Bilal Piot2 3 and Olivier Pietquin 2 3

1 Université de Lorraine & CNRS  LIEC  UMR 7360  Metz  F-57070 France

2 Univ. Lille  CNRS  Centrale Lille  Inria  UMR 9189 - CRIStAL  F-59000 Lille  France

3 Now with Google DeepMind  London  United Kingdom

matthieu.geist@univ-lorraine.fr

bilal.piot@univ-lille1.fr  olivier.pietquin@univ-lille1.fr

Abstract

This paper aims at theoretically and empirically comparing two standard optimiza-
tion criteria for Reinforcement Learning: i) maximization of the mean value and
ii) minimization of the Bellman residual. For that purpose  we place ourselves in
the framework of policy search algorithms  that are usually designed to maximize
the mean value  and derive a method that minimizes the residual (cid:107)T∗vπ − vπ(cid:107)1 ν
over policies. A theoretical analysis shows how good this proxy is to policy op-
timization  and notably that it is better than its value-based counterpart. We also
propose experiments on randomly generated generic Markov decision processes 
speciﬁcally designed for studying the inﬂuence of the involved concentrability
coefﬁcient. They show that the Bellman residual is generally a bad proxy to policy
optimization and that directly maximizing the mean value is much better  despite
the current lack of deep theoretical analysis. This might seem obvious  as directly
addressing the problem of interest is usually better  but given the prevalence of
(projected) Bellman residual minimization in value-based reinforcement learning 
we believe that this question is worth to be considered.

1

Introduction

Reinforcement Learning (RL) aims at estimating a policy π close to the optimal one  in the sense
that its value  vπ (the expected discounted return)  is close to maximal  i.e (cid:107)v∗ − vπ(cid:107) is small (v∗
being the optimal value)  for some norm. Controlling the residual (cid:107)T∗vθ − vθ(cid:107) (where T∗ is the
optimal Bellman operator and vθ a value function parameterized by θ) over a class of parameterized
value functions is a classical approach in value-based RL  and especially in Approximate Dynamic
Programming (ADP). Indeed  controlling this residual allows controlling the distance to the optimal
value function: generally speaking  we have that
(cid:107)v∗ − vπvθ(cid:107) ≤

(1)

C

with the policy πvθ being greedy with respect to vθ [17  19].
Some classical ADP approaches actually minimize a projected Bellman residual  (cid:107)Π(T∗vθ − vθ)(cid:107) 
where Π is the operator projecting onto the hypothesis space to which vθ belongs: Approximate Value
Iteration (AVI) [11  9] tries to minimize this using a ﬁxed-point approach  vθk+1 = ΠT∗vθk  and it has
been shown recently [18] that Least-Squares Policy Iteration (LSPI) [13] tries to minimize it using
a Newton approach1. Notice that in this case (projected residual)  there is no general performance
bound2 for controlling (cid:107)v∗ − vπvθ(cid:107).

1(Exact) policy iteration actually minimizes (cid:107)T∗v − v(cid:107) using a Newton descent [10].
2With a single action  this approach reduces to LSTD (Least-Squares Temporal Differences) [5]  that can be

1 − γ (cid:107)T∗vθ − vθ(cid:107) 

arbitrarily bad in an off-policy setting [20].

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Despite the fact that (unprojected) residual approaches come easily with performance guarantees 
they are not extensively studied in the (value-based) literature (one can mention [3] that considers
a subgradient descent or [19] that frames the norm of the residual as a delta-convex function). A
reason for this is that they lead to biased estimates when the Markovian transition kernel is stochastic
and unknown [1]  which is a rather standard case. Projected Bellman residual approaches are more
common  even if not introduced as such originally (notable exceptions are [16  18]).
An alternative approach consists in maximizing directly the mean value Eν[vπ(S)] for a user-
deﬁned state distribution ν  this being equivalent to directly minimizing (cid:107)v∗ − vπ(cid:107)1 ν  see Sec. 2.
This suggests deﬁning a class of parameterized policies and optimizing over them  which is the
predominant approach in policy search3 [7].
This paper aims at theoretically and experimentally studying these two approaches: maximizing the
mean value (related algorithms operate on policies) and minimizing the residual (related algorithms
operate on value functions). In that purpose  we place ourselves in the context of policy search
algorithms. We adopt this position because we could derive a method that minimizes the residual
(cid:107)T∗vπ − vπ(cid:107)1 ν over policies and compare to other methods that usually maximize the mean value.
On the other hand  adapting ADP methods so that they maximize the mean value is way harder4. This
new approach is presented in Sec. 3  and we show theoretically how good this proxy.
In Sec. 4  we conduct experiments on randomly generated generic Markov decision processes to
compare both approaches empirically. The experiments are speciﬁcally designed to study the inﬂuence
of the involved concentrability coefﬁcient. Despite the good theoretical properties of the Bellman
residual approach  it turns out that it only works well if there is a good match between the sampling
distribution and the discounted state occupancy distribution induced by the optimal policy  which is a
very limiting requirement. In comparison  maximizing the mean value is rather insensitive to this
issue and works well whatever the sampling distribution is  contrary to what suggests the sole related
theoretical bound. This study thus suggests that maximizing the mean value  although it doesn’t
provide easy theoretical analysis  is a better approach to build efﬁcient and robust RL algorithms.

2 Background

2.1 Notations

γ ∈ (0  1) is the discount factor. For v ∈ RS  we write (cid:107)v(cid:107)1 ν =(cid:80)

Let ∆X be the set of probability distributions over a ﬁnite set X and Y X the set of applications
from X to the set Y . By convention  all vectors are column vectors  except distributions (for left
multiplication). A Markov Decision Process (MDP) is a tuple {S A  P R  γ}  where S is the
ﬁnite state space5  A is the ﬁnite action space  P ∈ (∆S )S×A is the Markovian transition kernel
|s  a) denotes the probability of transiting to s(cid:48) when action a is applied in state s)  R ∈ RS×A
(P (s(cid:48)
is the bounded reward function (R(s  a) represents the local beneﬁt of doing action a in state s) and
s∈S ν(s)|v(s)| the ν-weighted
(cid:96)1-norm of v.
Notice that when the function v ∈ RS is componentwise positive  that is v ≥ 0  the ν-weighted
(cid:96)1-norm of v is actually its expectation with respect to ν: if v ≥ 0  then (cid:107)v(cid:107)1 ν = Eν[v(S)] = νv.
We will make an intensive use of this basic property in the following.
A stochastic policy π ∈ (∆A)S associates a distribution over actions to each state. The policy-induced
reward and transition kernels  Rπ ∈ RS and Pπ ∈ (∆S )S  are deﬁned as
|s) = Eπ(.|s)[P (s(cid:48)
|s  A)].
The quality of a policy is quantiﬁed by the associated value function vπ ∈ RS:
γtRπ(St)|S0 = s  St+1 ∼ Pπ(.|St)].

Rπ(s) = Eπ(.|s)[R(s  A)] and Pπ(s(cid:48)

vπ(s) = E[

(cid:88)

t≥0

3A remarkable aspect of policy search is that it does not necessarily rely on the Markovian assumption  but
this is out of the scope of this paper (residual approaches rely on it  through the Bellman equation). Some recent
and effective approaches build on policy search  such as deep deterministic policy gradient [15] or trust region
policy optimization [23]. Here  we focus on the canonical mean value maximization approach.

4Approximate linear programming could be considered as such but is often computationally intractable [8  6].
5This choice is done for ease and clarity of exposition  the following results could be extended to continuous

state and action spaces.

2

(cid:88)

t≥0

The value vπ is the unique ﬁxed point of the Bellman operator Tπ  deﬁned as Tπv = Rπ + γPπv for
any v ∈ RS. Let deﬁne the second Bellman operator T∗ as  for any v ∈ RS  T∗v = maxπ∈(∆A)S Tπv.
A policy π is greedy with respect to v ∈ RS  denoted π ∈ G(v) if Tπv = T∗v. There exists an
optimal policy π∗ that satisﬁes componentwise vπ∗ ≥ vπ  for all π ∈ (∆A)S. Moreover  we have
that π∗ ∈ G(v∗)  with v∗ being the unique ﬁxed point of T∗.
Finally  for any distribution µ ∈ ∆S  the γ-weighted occupancy measure induced by the policy π
when the initial state is sampled from µ is deﬁned as

dµ π = (1 − γ)µ

γtP t

π = (1 − γ)µ(I − γPπ)−1 ∈ ∆S .
ν (cid:107)∞ the smallest constant C satisfying  for all s ∈ S 

For two distributions µ and ν  we write (cid:107) µ
µ(s) ≤ Cν(s). This quantity measures the mismatch between the two distributions.
2.2 Maximizing the mean value
Let P be a space of parameterized stochastic policies and let µ be a distribution of interest. The
optimal policy has a higher value than any other policy  for any state. If the MDP is too large 
satisfying this condition is not reasonable. Therefore  a natural idea consists in searching for a policy
such that the associated value function is as close as possible to the optimal one  in expectation 
according to a distribution of interest µ. More formally  this means minimizing (cid:107)v∗ − v(cid:107)1 µ =
Eµ[v∗(S) − vπ(S)] ≥ 0. The optimal value function being unknown  one cannot address this
problem directly  but it is equivalent to maximizing Eµ[vπ(S)].
This is the basic principle of many policy search approaches:

max

π∈P Jν(π) with Jν(π) = Eν[vπ(S)] = νvπ.

Notice that we used a sampling distribution ν here  possibly different from the distribution of interest
µ. Related algorithms differ notably by the considered criterion (e.g.  it can be the mean reward rather
than the γ-discounted cumulative reward considered here) and by how the corresponding optimization
problem is solved. We refer to [7] for a survey on that topic.
Contrary to ADP  the theoretical efﬁciency of this family of approaches has not been studied a lot.
Indeed  as far as we know  there is a sole performance bound for maximizing the mean value.
Theorem 1 (Scherrer and Geist [22]). Assume that the policy space P is stable by stochastic mixture 
that is ∀π  π(cid:48)
∈ P. Deﬁne the ν-greedy-complexity of the policy
space P as

∈ P ∀α ∈ (0  1) 

π(cid:48)∈P dν π(T∗vπ − Tπ(cid:48)vπ).
Then  any policy π that is an -local optimum of Jν  in the sense that

Eν(P) = max

(1− α)π + απ(cid:48)
π∈P min

enjoys the following global performance guarantee:

∀π(cid:48)

∈ Π 

lim
α→0

νv(1−α)π+απ(cid:48) − νvπ

≤  

α

(cid:13)(cid:13)(cid:13)(cid:13) dµ π∗

ν

(cid:13)(cid:13)(cid:13)(cid:13)∞

µ(v∗ − vπ) ≤

1

(1 − γ)2

(Eν(P) + ) .

This bound (as all bounds of this kind) has three terms: an horizon term  a concentrability term and
an error term. The term 1
1−γ is the average optimization horizon. This concentrability coefﬁcient
((cid:107)dµ π∗/ν(cid:107)∞) measures the mismatch between the used distribution ν and the γ-weighted occupancy
measure induced by the optimal policy π∗ when the initial state is sampled from the distribution of
interest µ. This tells that if µ is the distribution of interest  one should optimize Jdµ π∗   which is
not feasible  π∗ being unknown (in this case  the coefﬁcient is equal to 1  its lower bound). This
coefﬁcient can be arbitrarily large: consider the case where µ concentrates on a single starting
state (that is µ(s0) = 1 for a given state s0) and such that the optimal policy leads to other states
(that is  dµ π∗ (s0) < 1)  the coefﬁcient is then inﬁnite. However  it is also the best concentrability
coefﬁcient according to [21]  that provides a theoretical and empirical comparison of Approximate
Policy Iteration (API) schemes. The error term is Eν(P) +   where Eν(P) measures the capacity of

3

the policy space to represent the policies being greedy with respect to the value of any policy in P
and  tells how the computed policy π is close to a local optimum of Jν.
There exist other policy search approches  based on ADP rather than on maximizing the mean value 
such as Conservative Policy Iteration (CPI) [12] or Direct Policy Iteration (DPI) [14]. The bound of
Thm. 1 matches the bounds of DPI or CPI. Actually  CPI can be shown to be a boosting approach
maximizing the mean value. See the discussion in [22] for more details. However  this bound is also
based on a very strong assumption (stability by stochastic mixture of the policy space) which is not
satisﬁed by all commonly used policy parameterizations.

3 Minimizing the Bellman residual

Direct maximization of the mean value operates on policies  while residual approaches operate on
value functions. To study these two optimization criteria together  we introduce a policy search
method that minimizes a residual. As noted before  we do so because it is much simpler than
introducing a value-based approach that maximizes the mean value. We also show how good this
proxy is to policy optimization. Although this algorithm is new  it is not claimed to be a core
contribution of the paper. Yet it is clearly a mandatory step to support the comparison between
optimization criteria.

3.1 Optimization problem
We propose to search a policy in P that minimizes the following Bellman residual:

π∈P Jν(π) with Jν(π) = (cid:107)T∗vπ − vπ(cid:107)1 ν.
min

Notice that  as for the maximization of the mean value  we used a sampling distribution ν  possibly
different from the distribution of interest µ.
From the basic properties of the Bellman operator  for any policy π we have that T∗vπ ≥ vπ.
Consequently  the ν-weighted (cid:96)1-norm of the residual is indeed the expected Bellman residual:

Jν(π) = Eν[[T∗vπ](S) − vπ(S)] = ν(T∗vπ − vπ).

Therefore  there is naturally no bias problem for minimizing a residual here  contrary to other residual
approaches [1]. This is an interesting result on its own  as removing the bias in value-based residual
approaches is far from being straightforward. This results from the optimization being done over
policies and not over values  and thus from vπ being an actual value (the one of the current policy)
obeying to the Bellman equation6.
Any optimization method can be envisioned to minimize Jν. Here  we simply propose to apply a
subgradient descent (despite the lack of convexity).
Theorem 2 (Subgradient of Jν). Recall that given the considered notations  the distribution νPG(vπ)
is the state distribution obtained by sampling the initial state according to ν  applying the action
being greedy with respect to vπ and following the dynamics to the next state. This being said  the
subgradient of Jν is given by
1
1 − γ

(cid:0)dν π(s) − γdνPG(vπ ) π(s)(cid:1) π(a|s)∇ ln π(a|s)qπ(s  a) 

s a

(cid:88)
with qπ(s  a) = R(s  a) + γ(cid:80)
s(cid:48)∈S P (s(cid:48)

−∇Jν(π) =

|s  a)vπ(s(cid:48)) the state-action value function.
Proof. The proof relies on basic (sub)gradient calculus  it is given in the appendix.

There are two terms in the negative subgradient −∇Jν: the ﬁrst one corresponds to the gradient of
Jν  the second one (up to the multiplication by −γ) is the gradient of JνPG(vπ ) and acts as a kind of
correction. This subgradient can be estimated using Monte Carlo rollouts  but doing so is harder than
for classic policy search (as it requires additionally sampling from νPG(vπ)  which requires estimating
6The property T∗v ≥ v does not hold if v is not the value function of a given policy  as in value-based

approaches.

4

the state-action value function). Also  this gradient involves computing the maximum over actions
(as it requires sampling from νPG(vπ)  that comes from explicitly considering the Bellman optimality
operator)  which prevents from extending easily this approach to continuous actions  contrary to
classic policy search.
Thus  from an algorithmic point of view  this approach has drawbacks. Yet  we do not discuss
further how to efﬁciently estimate this subgradient since we introduced this approach for the sake
of comparison to standard policy search methods only. For this reason  we will consider an ideal
algorithm in the experimental section where an analytical computation of the subgradient is possible 
see Sec. 4. This will place us in an unrealistically good setting  which will help focusing on the main
conclusions. Before this  we study how good this proxy is to policy optimization.

3.2 Analysis

Theorem 3 (Proxy bound for residual policy search). We have that

(cid:13)(cid:13)(cid:13)(cid:13) dµ π∗

ν

(cid:13)(cid:13)(cid:13)(cid:13)∞ Jν(π) =

(cid:107)v∗ − vπ(cid:107)1 µ ≤

1
1 − γ

(cid:13)(cid:13)(cid:13)(cid:13) dµ π∗

ν

(cid:13)(cid:13)(cid:13)(cid:13)∞ (cid:107)T∗vπ − vπ(cid:107)1 ν.

1
1 − γ

Proof. The proof can be easily derived from the analyses of [12]  [17] or [22]. We detail it for
completeness in the appendix.

This bound shows how controlling the residual helps in controlling the error. It has a linear dependency
on the horizon and the concentrability coefﬁcient is the best one can expect (according to [21]). It has
the same form has the bounds for value-based residual minimization [17  19] (see also Eq. (1)). It is
even better due to the involved concentrability coefﬁcient (the ones for value-based bounds are worst 
see [21] for a comparison).
Unfortunately  this bound is hardly comparable to the one of Th. 1  due to the error terms. In Th. 3 
the error term (the residual) is a global error (how good is the residual as a proxy)  whereas in Th. 1
the error term is mainly a local error (how small is the gradient after minimizing the mean value).
Notice also that Th. 3 is roughly an intermediate step for proving Th. 1  and that it applies to any
policy (suggesting that searching for a policy that minimizes the residual makes sense). One could
argue that a similar bound for mean value maximization would be something like: if Jµ(π) ≥ α  then
(cid:107)v∗ − vπ(cid:107)1 µ ≤ µv∗ − α. However  this is an oracle bound  as it depends on the unknown solution
v∗. It is thus hardly exploitable.
The aim of this paper is to compare these two optimization approaches to RL. At a ﬁrst sight 
maximizing directly the mean value should be better (as a more direct approach). If the bounds of
Th. 1 and 3 are hardly comparable  we can still discuss the involved terms. The horizon term is better
(linear instead of quadratic) for the residual approach. Yet  an horizon term can possibly be hidden in
the residual itself. Both bounds imply the same concentrability coefﬁcient  the best one can expect.
This is a very important term in RL bounds  often underestimated: as these coefﬁcients can easily
explode  minimizing an error makes sense only if it’s not multiplied by inﬁnity. This coefﬁcient
suggests that one should use dµ π∗ as the sampling distribution. This is rarely reasonable  while using
instead directly the distribution of interest is more natural. Therefore  the experiments we propose on
the next section focus on the inﬂuence of this concentrability coefﬁcient.

4 Experiments

We consider Garnet problems [2  4]. They are a class of randomly built MDPs meant to be totally
abstract while remaining representative of the problems that might be encountered in practice. Here 
a Garnet G(|S| |A|  b) is speciﬁed by the number of states  the number of actions and the branching
factor. For each (s  a) couple  b different next states are chosen randomly and the associated
probabilities are set by randomly partitioning the unit interval. The reward is null  except for 10% of
states where it is set to a random value  uniform in (1  2). We set γ = 0.99.
For the policy space  we consider a Gibbs parameterization: P = {πw : πw(a|s) ∝ ew(cid:62)φ(s a)}.
The features are also randomly generated  F (d  l). First  we generate binary state-features ϕ(s) of
dimension d  such that l components are set to 1 (the others are thus 0). The positions of the 1’s are

5

(cid:62)

0 . . . 0)

selected randomly such that no two states have the same feature. Then  the state-action features  of
  the position of the
dimension d|A|  are classically deﬁned as φ(s  a) = (0 . . . 0 ϕ(s)
zeros depending on the action. Notice that in general this policy space is not stable by stochastic
mixture  so the bound for policy search does not formally apply.
We compare classic policy search (denoted as PS(ν))  that maximizes the mean value  and residual
policy search (denoted as RPS(ν))  that minimizes the mean residual. We optimize the relative
objective functions with a normalized gradient ascent (resp. normalized subgradient descent) with
a constant learning rate α = 0.1. The gradients are computed analytically (as we have access to
the model)  so the following results represent an ideal case  when one can do an inﬁnite number of
rollouts. Unless said otherwise  the distribution µ ∈ ∆S of interest is the uniform distribution.
4.1 Using the distribution of interest

First  we consider ν = µ. We generate randomly 100 Garnets G(30  4  2) and 100 features F (8  3).
For each Garnet-feature couple  we run both algorithms for T = 1000 iterations. For each algorithm 
we measure two quantities: the (normalized) error (cid:107)v∗−vπ(cid:107)1 µ
(notice that as rewards are positive  we
(cid:107)v∗(cid:107)1 µ
have (cid:107)v∗(cid:107)1 µ = µv∗) and the Bellman residual (cid:107)T∗vπ − vπ(cid:107)1 µ  where π depends on the algorithm
and on the iteration. We show the results (mean±standard deviation) on Fig. 1.

a. Error for PS(µ).

b. Error for RPS(µ).

c. Residual for PS(µ). d. Residual for RPS(µ).

Figure 1: Results on the Garnet problems  when ν = µ.

Fig. 1.a shows that PS(µ) succeeds in decreasing the error. This was to be expected  as it is the
criterion it optimizes. Fig. 1.c shows how the residual of the policies computed by PS(µ) evolves.
By comparing this to Fig. 1.a  it can be observed that the residual and the error are not necessarily
correlated: the error can decrease while the residual increases  and a low error does not necessarily
involves a low residual.
Fig. 1.d shows that RPS(µ) succeeds in decreasing the residual. Again  this is not surprising  as it is
the optimized criterion. Fig. 1.b shows how the error of the policies computed by RPS(µ) evolves.
Comparing this to Fig. 1.d  it can be observed that decreasing the residual lowers the error: this is
consistent with the bound of Thm. 3.
Comparing Figs. 1.a and 1.b  it appears clearly that RPS(µ) is less efﬁcient than PS(µ) for decreasing
the error. This might seem obvious  as PS(µ) directly optimizes the criterion of interest. However 
when comparing the errors and the residuals for each method  it can be observed that they are not
necessarily correlated. Decreasing the residual lowers the error  but one can have a low error with a
high residual and vice versa.
As explained in Sec. 1  (projected) residual-based methods are prevalent for many reinforcement
learning approaches. We consider a policy-based residual rather than a value-based one to ease the
comparison  but it is worth studying the reason for such a different behavior.

4.2 Using the ideal distribution

The lower the concentrability coefﬁcient (cid:107) dµ π∗
ν (cid:107)∞ is  the better the bounds in Thm. 1 and 3 are.
This coefﬁcient is minimized for ν = dµ π∗. This is an unrealistic case (π∗ is unknown)  but since
we work with known MDPs we can compute this quantity (the model being known)  for the sake
of a complete empirical analysis. Therefore  PS(dµ π∗) and RPS(dµ π∗) are compared in Fig. 2. We
highlight the fact that the errors and the residuals shown in this ﬁgure are measured respectively to
the distribution of interest µ  and not the distribution dµ π∗ used for the optimization.

6

02004006008001000numberofiterations0.00.20.40.60.8kv∗−vπk1 µ·kv∗k−11 µ02004006008001000numberofiterations0.00.20.40.60.8kv∗−vπk1 µ·kv∗k−11 µ02004006008001000numberofiterations0.00.20.40.60.81.01.21.4kT∗vπ−vπk1 µ02004006008001000numberofiterations0.00.20.40.60.81.01.21.4kT∗vπ−vπk1 µa. Error for PS(dµ π∗). b. Error for RPS(dµ π∗).

c. Residual for

PS(dµ π∗).

d. Residual for
RPS(dµ π∗).

Figure 2: Results on the Garnet problems  when ν = dµ π∗.

Fig. 2.a shows that PS(dµ π∗) succeeds in decreasing the error (cid:107)v∗ − vπ(cid:107)1 µ. However  comparing
Fig. 2.a to Fig. 1.a  there is no signiﬁcant gain in using ν = dµ π∗ instead of ν = µ. This suggests
that the dependency of the bound in Thm. 1 on the concentrability coefﬁcient is not tight. Fig. 2.c
shows how the corresponding residual evolves. Again  there is no strong correlation between the
residual and the error.
Fig. 2.d shows how the residual (cid:107)T∗vπ − vπ(cid:107)1 µ evolves for RPS(dµ π∗). It is not decreasing  but it
is not what is optimized (the residual (cid:107)T∗vπ − vπ(cid:107)1 dµ π∗   not shown  decreases indeed  in a similar
fashion than Fig. 1.d). Fig. 2.b shows how the related error evolves. Compared to Fig. 2.a  there is no
signiﬁcant difference. The behavior of the residual is similar for both methods (Figs. 2.c and 2.d).
Overall  this suggests that controlling the residual (RPS) allows controlling the error  but that this
requires a wise choice for the distribution ν. On the other hand  controlling directly the error (PS)
is much less sensitive to this. In other words  this suggests a stronger dependency of the residual
approach to the mismatch between the sampling distribution and the discounted state occupancy
measure induced by the optimal policy.

4.3 Varying the sampling distribution

This experiment is designed to study the effect of the mismatch between the distributions. We sample
100 Garnets G(30  4  2)  as well as associated feature sets F (8  3). The distribution of interest is no
longer the uniform distribution  but a measure that concentrates on a single starting state of interest
s0: µ(s0) = 1. This is an adverserial case  as it implies that (cid:107) dµ π∗
µ (cid:107)∞ = ∞: the branching factor
being equal to 2  the optimal policy π∗ cannot concentrate on s0.
The sampling distribution is deﬁned as being a mixture between the distribution of interest and the
ideal distribution. For α ∈ [0  1]  να is deﬁned as να = (1 − α)µ + αdµ π∗. It is straightforward to
show that in this case the concentrability coefﬁcient is indeed 1
0 = ∞):

α (with the convention that 1

(cid:19)

(cid:13)(cid:13)(cid:13)(cid:13) dµ π∗

να

(cid:13)(cid:13)(cid:13)(cid:13)∞

(cid:18)

= max

dµ π∗ (s0)

(1 − α) + αdµ π∗ (s0)

;

1
α

=

1
α

.

For each MDP  the learning (for PS(να) and RPS(να)) is repeated  from the same initial policy  by
setting α = 1
k   for k ∈ [1; 25]. Let πt x be the policy learnt by algorithm x (PS or RPS) at iteration t 
the integrated error (resp. integrated residual) is deﬁned as

T(cid:88)

t=1

1
T

(cid:107)v∗ − vπt x(cid:107)1 µ

(cid:107)v∗(cid:107)1 µ

(resp. 1
T

T(cid:88)
t=1 (cid:107)T∗vπt x − vπt x(cid:107)1 µ).

Notice that here again  the integrated error and residual are deﬁned with respect to µ  the distribution
of interest  and not να  the sampling distribution used for optimization. We get an integrated error
(resp. residual) for each value of α = 1
να (cid:107)∞  the
concentrability coefﬁcient. Results are presented in Fig. 3  that shows these functions averaged across
the 100 randomly generated MDPs (mean±standard deviation as before  minimum and maximum
values are shown in dashed line).
Fig. 3.a shows the integrated error for PS(να). It can be observed that the mismatch between measures
has no inﬂuence on the efﬁciency of the algorithm. Fig. 3.b shows the same thing for RPS(να). The
integrated error increases greatly as the mismatch between the sampling measure and the ideal one

k   and represent it as a function of k = (cid:107) dµ π∗

7

02004006008001000numberofiterations0.00.20.40.60.8kv∗−vπk1 µ·kv∗k−11 µ02004006008001000numberofiterations0.00.20.40.60.8kv∗−vπk1 µ·kv∗k−11 µ02004006008001000numberofiterations0.00.20.40.60.81.01.21.4kT∗vπ−vπk1 µ02004006008001000numberofiterations0.00.20.40.60.81.01.21.4kT∗vπ−vπk1 µa. Integrated error for

PS(να).

b. Integrated error for

RPS(να).

c. Integrated residual

for PS(να).

d. Integrated residual

for RPS(να).

Figure 3: Results for the sampling distribution να.

increases (the value to which the error saturates correspond to no improvement over the initial policy).
Comparing both ﬁgures  it can be observed that RPS performs as well as PS only when the ideal
distribution is used (this corresponds to a concentrability coefﬁcient of 1). Fig. 3.c and 3.d show the
integrated residual for each algorithm. It can be observed that RPS consistently achieves a lower
residual than PS.
Overall  this suggests that using the Bellman residual as a proxy is efﬁcient only if the sampling
distribution is close to the ideal one  which is difﬁcult to achieve in general (the ideal distribution
dµ π∗ being unknown). On the other hand  the more direct approach consisting in maximizing the
mean value is much more robust to this issue (and can  as a consequence  be considered directly with
the distribution of interest).
One could argue that the way we optimize the considered objective function is rather naive (for
example  considering a constant learning rate). But this does not change the conclusions of this
experimental study  that deals with how the error and the Bellman residual are related and with how
the concentrability inﬂuences each optimization approach. This point is developed in the appendix.

5 Conclusion

The aim of this article was to compare two optimization approaches to reinforcement learning:
minimizing a Bellman residual and maximizing the mean value. As said in Sec. 1  Bellman residuals
are prevalent in ADP. Notably  value iteration minimizes such a residual using a ﬁxed-point approach
and policy iteration minimizes it with a Newton descent. On another hand  maximizing the mean
value (Sec. 2) is prevalent in policy search approaches.
As Bellman residual minimization methods are naturally value-based and mean value maximization
approaches policy-based  we introduced a policy-based residual minimization algorithm in order to
study both optimization problems together. For the introduced residual method  we proved a proxy
bound  better than value-based residual minimization. The different nature of the bounds of Th. 1
and 3 made the comparison difﬁcult  but both involve the same concentrability coefﬁcient  a term
often underestimated in RL bounds.
Therefore  we compared both approaches empirically on a set of randomly generated Garnets 
the study being designed to quantify the inﬂuence of this concentrability coefﬁcient. From these
experiments  it appears that the Bellman residual is a good proxy for the error (the distance to the
optimal value function) only if  luckily  the concentrability coefﬁcient is small for the considered
MDP and the distribution of interest  or one can afford a change of measure for the optimization
problem  such that the sampling distribution is close to the ideal one. Regarding this second point 
one can change to a measure different from the ideal one  dµ π∗ (for example  using for ν a uniform
distribution when the distribution of interest concentrates on a single state would help)  but this is
difﬁcult in general (one should know roughly where the optimal policy will lead to). Conversely 
maximizing the mean value appears to be insensitive to this problem. This suggests that the Bellman
residual is generally a bad proxy to policy optimization  and that maximizing the mean value is more
likely to result in efﬁcient and robust reinforcement learning algorithms  despite the current lack of
deep theoretical analysis.
This conclusion might seems obvious  as maximizing the mean value is a more direct approach  but
this discussion has never been addressed in the literature  as far as we know  and we think it to be
important  given the prevalence of (projected) residual minimization in value-based RL.

8

0510152025concentrabilitycoefﬁcient0.00.20.40.60.81.0integratederror0510152025concentrabilitycoefﬁcient0.00.20.40.60.81.0integratederror0510152025concentrabilitycoefﬁcient0.00.51.01.5integratedresidual0510152025concentrabilitycoefﬁcient0.00.51.01.5integratedresidualReferences
[1] András Antos  Csaba Szepesvári  and Rémi Munos. Learning near-optimal policies with
Bellman-residual minimization based ﬁtted policy iteration and a single sample path. Machine
Learning  71(1):89–129  2008.

[2] TW Archibald  KIM McKinnon  and LC Thomas. On the generation of Markov decision

processes. Journal of the Operational Research Society  pages 354–361  1995.

[3] Leemon C. Baird. Residual Algorithms: Reinforcement Learning with Function Approximation.

In International Conference on Machine Learning (ICML)  pages 30–37  1995.

[4] Shalabh Bhatnagar  Richard S Sutton  Mohammad Ghavamzadeh  and Mark Lee. Natural

actor-critic algorithms. Automatica  45(11):2471–2482  2009.

[5] Steven J. Bradtke and Andrew G. Barto. Linear Least-Squares algorithms for temporal difference

learning. Machine Learning  22(1-3):33–57  1996.

[6] Daniela Pucci de Farias and Benjamin Van Roy. The linear programming approach to approxi-

mate dynamic programming. Operations research  51(6):850–865  2003.

[7] Marc Peter Deisenroth  Gerhard Neumann  Jan Peters  et al. A Survey on Policy Search for

Robotics. Foundations and Trends in Robotics  2(1-2):1–142  2013.

[8] Vijay V. Desai  Vivek F. Farias  and Ciamac C. Moallemi. Approximate dynamic programming

via a smoothed linear program. Oper. Res.  60(3):655–674  May 2012.

[9] Damien Ernst  Pierre Geurts  and Louis Wehenkel. Tree-Based Batch Mode Reinforcement

Learning. Journal of Machine Learning Research  6:503–556  2005.

[10] Jerzy A Filar and Boleslaw Tolwinski. On the Algorithm of Pollatschek and Avi-ltzhak.

Stochastic Games And Related Topics  pages 59–70  1991.

[11] Geoffrey Gordon. Stable Function Approximation in Dynamic Programming. In International

Conference on Machine Learning (ICML)  1995.

[12] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning.

In International Conference on Machine Learning (ICML)  2002.

[13] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine

Learning Research  4:1107–1149  2003.

[14] Alessandro Lazaric  Mohammad Ghavamzadeh  and Rémi Munos. Analysis of a classiﬁcation-
based policy iteration algorithm. In International Conference on Machine Learning (ICML) 
2010.

[15] Timothy P Lillicrap  Jonathan J Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval Tassa 
David Silver  and Daan Wierstra. Continuous control with deep reinforcement learning. In
International Conference on Learning Representations (ICLR)  2016.

[16] Hamid R Maei  Csaba Szepesvári  Shalabh Bhatnagar  and Richard S Sutton. Toward off-policy
learning control with function approximation. In International Conference on Machine Learning
(ICML)  2010.

[17] Rémi Munos. Performance bounds in (cid:96)p-norm for approximate value iteration. SIAM journal

on control and optimization  46(2):541–561  2007.

[18] Julien Pérolat  Bilal Piot  Matthieu Geist  Bruno Scherrer  and Olivier Pietquin. Softened
Approximate Policy Iteration for Markov Games. In International Conference on Machine
Learning (ICML)  2016.

[19] Bilal Piot  Matthieu Geist  and Olivier Pietquin. Difference of Convex Functions Programming
for Reinforcement Learning. In Advances in Neural Information Processing Systems (NIPS) 
2014.

9

[20] Bruno Scherrer. Should one compute the Temporal Difference ﬁx point or minimize the
In International Conference on

Bellman Residual? The uniﬁed oblique projection view.
Machine Learning (ICML)  2010.

[21] Bruno Scherrer. Approximate Policy Iteration Schemes: A Comparison. In International

Conference on Machine Learning (ICML)  pages 1314–1322  2014.

[22] Bruno Scherrer and Matthieu Geist. Local Policy Search in a Convex Space and Conservative
Policy Iteration as Boosted Policy Search. In European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD)  2014.

[23] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region

policy optimization. In International Conference on Machine Learning (ICML)  2015.

10

,Sergey Plis
David Danks
Cynthia Freeman
Vince Calhoun
Matthieu Geist
Bilal Piot
Olivier Pietquin
Jiaqi Ma
Weijing Tang
Ji Zhu
Qiaozhu Mei