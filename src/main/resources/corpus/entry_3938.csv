2014,On a Theory of Nonparametric Pairwise Similarity for Clustering: Connecting Clustering to Classification,Pairwise clustering methods partition the data space into clusters by the pairwise similarity between data points. The success of pairwise clustering largely depends on the pairwise similarity function defined over the data points  where kernel similarity is broadly used. In this paper  we present a novel pairwise clustering framework by bridging the gap between clustering and multi-class classification. This pairwise clustering framework learns an unsupervised nonparametric classifier from each data partition  and search for the optimal partition of the data by minimizing the generalization error of the learned classifiers associated with the data partitions. We consider two nonparametric classifiers in this framework  i.e. the nearest neighbor classifier and the plug-in classifier. Modeling the underlying data distribution by nonparametric kernel density estimation  the generalization error bounds for both unsupervised nonparametric classifiers are the sum of nonparametric pairwise similarity terms between the data points for the purpose of clustering. Under uniform distribution  the nonparametric similarity terms induced by both unsupervised classifiers exhibit a well known form of kernel similarity. We also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary for Low Density Separation  a widely used criteria for semi-supervised learning and clustering. Based on the derived nonparametric pairwise similarity using the plug-in classifier  we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability  whose superiority is evidenced by the experimental results.,On a Theory of Nonparametric Pairwise Similarity

for Clustering: Connecting Clustering to

Classiﬁcation

Yingzhen Yang1 Feng Liang1 Shuicheng Yan2 Zhangyang Wang1 Thomas S. Huang1

1 University of Illinois at Urbana-Champaign  Urbana  IL 61801  USA
{yyang58 liangf zwang119 t-huang1}@illinois.edu

2 National University of Singapore  Singapore  117576

eleyans@nus.edu.sg

Abstract

Pairwise clustering methods partition the data space into clusters by the pairwise
similarity between data points. The success of pairwise clustering largely de-
pends on the pairwise similarity function deﬁned over the data points  where ker-
nel similarity is broadly used. In this paper  we present a novel pairwise clustering
framework by bridging the gap between clustering and multi-class classiﬁcation.
This pairwise clustering framework learns an unsupervised nonparametric classi-
ﬁer from each data partition  and search for the optimal partition of the data by
minimizing the generalization error of the learned classiﬁers associated with the
data partitions. We consider two nonparametric classiﬁers in this framework  i.e.
the nearest neighbor classiﬁer and the plug-in classiﬁer. Modeling the underly-
ing data distribution by nonparametric kernel density estimation  the generaliza-
tion error bounds for both unsupervised nonparametric classiﬁers are the sum of
nonparametric pairwise similarity terms between the data points for the purpose
of clustering. Under uniform distribution  the nonparametric similarity terms in-
duced by both unsupervised classiﬁers exhibit a well known form of kernel simi-
larity. We also prove that the generalization error bound for the unsupervised plug-
in classiﬁer is asymptotically equal to the weighted volume of cluster boundary
[1] for Low Density Separation  a widely used criteria for semi-supervised learn-
ing and clustering. Based on the derived nonparametric pairwise similarity using
the plug-in classiﬁer  we propose a new nonparametric exemplar-based clustering
method with enhanced discriminative capability  whose superiority is evidenced
by the experimental results.

1 Introduction

Pairwise clustering methods partition the data into a set of self-similar clusters based on the pair-
wise similarity between the data points. Representative clustering methods include K-means [2]
which minimizes the within-cluster dissimilarities  spectral clustering [3] which identiﬁes clusters
of more complex shapes lying on low dimensional manifolds  and the pairwise clustering method
[4] using message-passing algorithm to inference the cluster labels in a pairwise undirected graph-
ical model. Utilizing pairwise similarity  these pairwise clustering methods often avoid estimating
complex hidden variables or parameters  which is difﬁcult for high dimensional data.
However  most pairwise clustering methods assume that the pairwise similarity is given [2  3]  or
they learn a more complicated similarity measure based on several given base similarities [4]. In
this paper  we present a new framework for pairwise clustering where the pairwise similarity is
derived as the generalization error bound for the unsupervised nonparametric classiﬁer. The un-

1

supervised classiﬁer is learned from unlabeled data and the hypothetical labeling. The quality of
the hypothetical labeling is measured by the associated generalization error of the learned classi-
ﬁer  and the hypothetical labeling with minimum associated generalization error bound is preferred.
We consider two nonparametric classiﬁers  i.e. the nearest neighbor classiﬁer (NN) and the plug-in
classiﬁer (or the kernel density classiﬁer). The generalization error bounds for both unsupervised
classiﬁers are expressed as sum of pairwise terms between the data points  which can be interpreted
as nonparametric pairwise similarity measure between the data points. Under uniform distribution 
both nonparametric similarity measures exhibit a well known form of kernel similarity. We also
prove that the generalization error bound for the unsupervised plug-in classiﬁer is asymptotically
equal to the weighted volume of cluster boundary [1] for Low Density Separation  a widely used
criteria for semi-supervised learning and clustering.
Our work is closely related to discriminative clustering methods by unsupervised classiﬁcation 
which search for the cluster boundaries with the help of unsupervised classiﬁer. For example  [5]
learns a max-margin two-class classiﬁer to group unlabeled data in an unsupervised manner  known
as unsupervised SVM whose theoretical property is further analyzed in [6]. Also  [7] learns the
kernel logistic regression classiﬁer  and uses the entropy of the posterior distribution of the class
label by the classiﬁer to measure the quality of the learned classiﬁer. More recent work presented in
[8] learns an unsupervised classiﬁer by maximizing the mutual information between cluster labels
and the data  and the Squared-Loss Mutual Information is employed to produce a convex optimiza-
tion problem. Although such discriminative methods produce satisfactory empirical results  the
optimization of complex parameters hampers their application in high-dimensional data. Following
the same principle of unsupervised classiﬁcation using nonparametric classiﬁers  we derive non-
parametric pairwise similarity and eliminate the need of estimating complicated parameters of the
unsupervised classifer. As an application  we develop a new nonparametric exemplar-based cluster-
ing method with the derived nonparametric pairwise similarity induced by the plug-in classiﬁer  and
our new method demonstrates better empirical clustering results than the existing exemplar-based
clustering methods.
It should be emphasized that our generalization bounds are essentially different from the litera-
ture. As nonparametric classiﬁcation methods  the generalization properties of the nearest neighbor
classiﬁer (NN) and the plug-in classiﬁer are extensively studied. Previous research focuses on the
average generalization error of the NN [9  10]  which is the average error of the NN over all the
random training data sets  or the excess risk of the plug-in classiﬁer [11  12]. In [9]  it is shown that
the average generalization error of the NN is bounded by twice of the Bayes error. Assuming that
the class of the regression functions has a smooth parameter β  [11] proves that the excess risk of
−(cid:12)
2(cid:12)+d where d is the dimension of the data. [12]
the plug-in classiﬁer converges to 0 of the order n
further shows that the plug-in classiﬁer attains faster convergence rate of the excess risk  namely
− 1
2   under some margin assumption on the data distribution. All these generalization error bounds
n
depend on the unknown Bayes error. By virtue of kernel density estimation and generalized ker-
nel density estimation [13]  our generalization bounds are represented mostly in terms of the data 
leading to the pairwise similarities for clustering.

2 Formulation of Pairwise Clustering by Unsupervised Nonparametric

Classiﬁcation

The discriminative clustering literature [5  7] has demonstrated the potential of multi-class clas-
siﬁcation for the clustering problem.
Inspired by the natural connection between clustering and
classiﬁcation  we model the clustering problem as a multi-class classiﬁcation problem: a classiﬁer
is learned from the training data built by a hypothetical labeling  which is a possible cluster labeling.
The optimal hypothetical labeling is supposed to be the one such that its associated classiﬁer has the
minimum generalization error bound. To study the generalization bound for the classiﬁer learned
from the hypothetical labeling  we deﬁne the concept of classiﬁcation model. Given unlabeled data
{xl}n
l=1 as
below:
Deﬁnition 1. The classiﬁcation model corresponding to the hypothetical labeling Y = {yl}n
is deﬁned as MY =
l=1 are the labeled data by the

l=1  a classiﬁcation model MY is constructed for any hypothetical labeling Y = {yl}n

. S = {xl  yl}n

S  PXY  {πi  fi}Q

i=1  F

l=1

(

)

2

hypothetical labeling  and S are assumed to be i.i.d.
samples drawn from the joint distribu-
tion PXY = PX|Y PY   where (X  Y ) is a random couple  X ∈ IRd represents the data and
Y ∈ {1  2  ...  Q} is the class label of X  Q is the number of classes determined by the hypothetical
labeling. Furthermore  PXY is speciﬁed by {π(i)  f (i)}Q
i=1 as follows: π(i) is the class prior for
class i  i.e. Pr [Y = i] = π(i); the conditional distribution PX|Y =i has probabilistic density func-
tion f (i)  i = 1  . . .   Q. F is a classiﬁer trained using the training data S. The generalization error
of the classiﬁcation model MY is deﬁned as the generalization error of the classiﬁer F in MY.

In this paper  we study two types of classiﬁcation models with the nearest neighbor classiﬁer and
the plug-in classiﬁer respectively  and derive their generalization error bounds as sum of pairwise
similarity between the data. Given a speciﬁc type of classiﬁcation model  the optimal hypothetical
labeling corresponds to the classiﬁcation model with minimum generalization error bound. The
optimal hypothetical labeling also generates a data partition where the sum of pairwise similarity
between the data from different clusters is minimized  which is a common criteria for discriminative
clustering.
In the following text  we derive the generalization error bounds for the two types of classiﬁcation
models. Before that  we introduce more notations and assumptions for the classiﬁcation model.
Denote by PX the induced marginal distribution of X  and f is the probabilistic density function of
PX which is a mixture of Q class-conditional densities: f =
π(i)f (i). η(i) (x) is the regression
function of Y on X = x  i.e. η(i) (x) = Pr [Y = i|X = x ] = (cid:25)(i)f (i)(x)
. For the sake of the
consistency of the kernel density estimators used in the sequel  there are further assumptions on
the marginal density and class-conditional densities in the classiﬁcation model for any hypothetical
labeling:
(A) f is bounded from below  i.e. f ≥ fmin > 0
(B) {f (i)} is bounded from above  i.e. f (i) ≤ f (i)
where Σ(cid:13);c is the class of H¨older-γ smooth functions with H¨older constant c:

max  and f (i) ∈ Σ(cid:13);ci  1 ≤ i ≤ Q.

Q∑

f (x)

i=1

∑

(cid:6)(cid:13);c   {f : IRd → IR|∀x; y;|f (x) − f (y)| ≤ c∥x − y∥(cid:13)}; (cid:13) > 0

It follows from assumption (B) that f ∈ Σ(cid:13);c where c =
π(i)ci. Assumption (A) and (B) are
mild. The upper bound for the density functions is widely required for the consistency of kernel
density estimators [14  15]; H¨older-γ smoothness is required to bound the bias of such estimators 
and it also appears in [12] for estimating the excess risk of the plug-in classiﬁer. The lower bound
for the marginal density is used to derive the consistency of the estimator of the regression function
η(i) (Lemma 2) and the consistency of the generalized kernel density estimator (Lemma 3). We
denote by PX the collection of marginal distributions that satisfy assumption (A)  and denote by
PX|Y the collection of class-conditional distributions that satisfy assumption (B). We then deﬁne
the collection of joint distributions PXY that PXY belongs to  which requires the marginal density
and class-conditional densities satisfy assumption (A)-(B):

i

PXY   {PXY | PX ∈ PX ;{PX|Y =i} ∈ PX|Y ; min

i

{(cid:25)(i)} > 0}

(1)

Given the joint distribution PXY   the generalization error of the classiﬁer F learned from the train-
ing data S is:

(2)
Nonparametric kernel density estimator (KDE) serves as the primary tool of estimating the under-
lying probabilistic density functions in our generalization analysis  and we introduce the KDE of f
as below:

R (FS )   Pr [(X; Y ) : F (X) ̸= Y ]

n∑

l=1

3

^fn;hn (x) =

1
n

Khn (x − xl)

(3)

(

)

where Kh (x) = 1
(2(cid:25))d/2 e

− ∥x∥2

1

2

is the isotropic Gaussian kernel with bandwidth h and K (x)  
. We have the following VC property of the Gaussian kernel K. Deﬁne the class

hd K

x
h

of functions

)

(

t − ·
h

F   {K

; t ∈ IRd; h ̸= 0}

− d

∫

(4)
The VC property appears in [14  15  16  17  18]  and it is proved that F is a bounded VC class of
measurable functions with respect to the envelope function F such that |u| ≤ F for any u ∈ F (e.g.
F ≡ (2π)
2 ). It follows that there exist positive numbers A and v such that for every probability
measure P on IRd for which
(

)
where N
T in the metric space
The VC property of K is required for the consistency of kernel density estimators shown in
Lemma 2. Also  we adopt the kernel estimator of η(i) below

is deﬁned as the minimal number of open ˆd-balls of radius ϵ required to cover

. A and v are called the VC characteristics of F.

(
(
)
F 2dP < ∞ and any 0 < τ < 1 
≤
F ;∥·∥

L2(P ) ; (cid:28) ∥F∥

L2(P )

T   ˆd  ϵ

(

)
T   ˆd

)

v

A
(cid:28)

(5)

N

^(cid:17)(i)
n;hn

(x) =

l=1

Khn (x − xl)1I{yl=i}

n ^fn;hn (x)

(6)

Before stating Lemma 2  we introduce several frequently used quantities throughout this paper. Let
L  C > 0 be constants which only depend on the VC characteristics of the Gaussian kernel K. We
deﬁne

n∑

Q∑

f0  

(cid:25)(i)f (i)

max (cid:27)2

0   ∥K∥2
2f0

Also  for all positive numbers λ ≥ C and σ > 0  we deﬁne

i=1

E(cid:27)2   log (1 + (cid:21)=4L)

(cid:21)L(cid:27)2

(7)

(8)

Based on Corollary 2.2 in [14]  Lemma 2 and Lemma 3 in the Appendix (more complete version
in the supplementary) show the strong consistency (almost sure uniformly convergence) of several
kernel density estimators  i.e. ˆfn;hn  {ˆη(i)
} and the generalized kernel density estimator  and they
form the basis for the derivation of the generalization error bounds for the two types of classiﬁcation
models.

n;hn

3 Generalization Bounds

We derive the generalization error bounds for the two types of classiﬁcation models with the nearest
neighbor classiﬁer and the plug-in classiﬁer respectively. Substituting these kernel density estima-
tors for the corresponding true density functions  Theorem 1 and Theorem 2 present the generaliza-
tion error bounds for the classiﬁcation models with the plug-in classiﬁer and the nearest neighbor
classiﬁer. The dominant terms of both bounds are expressed as sum of pairwise similarity depend-
ing solely on the data  which facilitates the application of clustering. We also show the connection
between the error bound for the plug-in classiﬁer and Low Density Separation in this section. The
detailed proofs are included in the supplementary.

3.1 Generalization Bound for the Classiﬁcation Model with Plug-In Classiﬁer

The plug-in classiﬁer resembles the Bayes classiﬁer while it uses the kernel density estimator of the
regression function η(i) instead of the true η(i). It has the form

PI (X) = arg max
1≤i≤Q

^(cid:17)(i)
n;hn

(X)

(9)

where ˆη(i)
is the nonparametric kernel estimator of the regression function η(i) by (6). The
generalization capability of the plug-in classiﬁer has been studied by the literature[11  12]. Let

n;hn

4

∗ be the Bayes classiﬁer  it is proved that the excess risk of PIS  namely IESR (PIS) − R (F
) 
F
−(cid:12)
converges to 0 of the order n
2(cid:12)+d under some complexity assumption on the class of the regression
functions with smooth parameter β that {η(i)} belongs to [11  12]. However  this result cannot be
used to derive the generalization error bound for the plug-in classiﬁer comprising of nonparametric
pairwise similarities in our setting.
We show the upper bound for the generalization error of PIS in Lemma 1.
Lemma 1. For any PXY ∈ PXY   there exists a n0 which depends on σ0 and VC characteristics
of K  when n > n0  with probability greater than 1 − 2QLh
  the generalization error of the
plug-in classiﬁer satisﬁes

E(cid:27)2
0
n

∗

n + O

R (PIS) ≤ RPI
∑

−1
log h
n
nhd
n

+ h(cid:13)
n

RPI

n =

i;j=1;:::;Q;i̸=j

IEX

^(cid:17)(i)
n;hn

(X) ^(cid:17)(j)

n;hn

(X)

)

]

(√
[

(10)

(11)

(12)

−1
n
nhd
n

→ 0  ˆη(i)

is the kernel
Q for

where E(cid:27)2 is deﬁned by (8)  hn is chosen such that hn → 0  log h
n;hn
estimator of the regression function. Moreover  the equality in (10) holds when ˆη(i)
1 ≤ i ≤ Q.
Based on Lemma 1  we can bound the error of the plug-in classiﬁer from above by RPI
n . Theorem 1
then gives the bound for the error of the plug-in classiﬁer in the corresponding classiﬁcation model
using the generalized kernel density estimator in Lemma 3. The bound has a form of sum of pairwise
(
similarity between the data from different classes.
Theorem 1.
S  PXY  {πi  fi}Q
and the VC characteristics of K  when n > n1  with probability greater than 1 − 2QLh
√
L(

the Plug-In Classiﬁer) Given the classiﬁcation model MY =
such that PXY ∈ PXY   there exists a n1 which depends on σ0  σ1
n −

  the generalization error of the plug-in classiﬁer satisﬁes

(Error of
i=1  PI

0 − QLh

≡ 1

)

E(cid:27)2
0

n;hn

E(cid:27)2

E(cid:27)2
1
n

2hn)

R (PIS) ≤ ^Rn (PIS) + O

(√

)

−1
log h
n
nhd
n

+ h(cid:13)
n

∑

l;m

where ˆRn (PIS) = 1
n2
function and

θlmGlm;

√

2hn

  σ2

1 =

∥K∥2
fmin

2fmax

  θlm = 1I{yl̸=ym} is a class indicator

1
2

Glm;h = Gh (xl; xm) ; Gh (x; y) =

Kh (x − y)
^f
n;h (x) ^f
n;h (y)
→ 0  ˆfn;hn is the kernel density
E(cid:27)2 is deﬁned by (8)  hn is chosen such that hn → 0  log h
)
estimator of f deﬁned by (3).
ˆRn is the dominant term determined solely by the data and the excess error O
goes to 0 with inﬁnite n. In the following subsection  we show the close connection between the
error bound for the plug-in classiﬁer and the weighted volume of cluster boundary  and the latter is
proposed by [1] for Low Density Separation.

(√

−1
log h
n
nhd
n

−1
n
nhd
n

+ h(cid:13)
n

(13)

1
2

;

3.1.1 Connection to Low Density Separation

Low Density Separation [19]  a well-known criteria for clustering  requires that the cluster boundary
should pass through regions of low density. It has been extensively studied in unsupervised learning
and semi-supervised learning [20  21  22]. Suppose the data {xl}n
l=1 lies on a domain Ω ⊆ Rd.
Let f be the probability density function on Ω  S be the cluster boundary which separates Ω into
two parts S1 and S2. Following the Low Density Separation assumption  [1] suggests that the

5

cluster boundary S with low weighted volume

∫

f (s)ds should be preferable. [1] also proves that

S

a particular type of cut function converges to the weighted volume of S. Based on their study  we
obtain the following result relating the error of the plug-in classiﬁer to the weighted volume of the
cluster boundary.
Corollary 1. Under the assumption of Theorem 1  for any kernel bandwidth sequence {hn}∞
such that

2d+2   with probability 1 

n→∞ hn = 0 and hn > n
lim

n=1

∫
−(cid:11) where 0 < α < 1

√

lim
n→∞

(cid:25)
2hn

^Rn (PIS) =

f (s)ds

S

(14)

3.2 Generalization Bound for the Classiﬁcation Model with Nearest Neighbor Classiﬁer

)
Theorem 2 shows the generalization error bound for the classiﬁcation model with nearest neighbor
classiﬁer (NN)  which has a similar form as (12).
Theorem 2. (Error of the NN) Given the classiﬁcation model MY =
i=1  NN
such that PXY ∈ PXY and the support of PX is bounded by [−M0  M0]d  there exists a n0 which
depends on σ0 and VC characteristics of K  when n > n0  with probability greater than 1 −

S  PXY  {πi  fi}Q

(

2QLh

E(cid:27)2
0

n − (2M0)dndd0 e
∑

)
−n1−dd0 fmin  the generalization error of the NN satisﬁes:

)

(√

R (NNS) ≤ ^Rn (NNS) + c0

(cid:13)

−d0(cid:13) + O
n

d

−1
log h
n
nhd
n

+ h(cid:13)
n

(√

where ˆRn (NN) = 1
n

Hlm;hnθlm 

1≤l<m≤n

(∫

Hlm;hn = Khn (xl − xm)

Vl

^fn;hn (x) dx
^fn;hn (xl)

∫

Vm

^fn;hn (x) dx

^fn;hn (xm)

)

;

+

(15)

(16)

E(cid:27)2 is deﬁned by (8)  d0 is a constant such that dd0 < 1  ˆfn;hn is the kernel density estimator of
f deﬁned by (3) with the kernel bandwidth hn satisfying hn → 0  log h
→ 0  Vl is the Voronoi
cell associated with xl  c0 is a constant  θlm = 1I{yl̸=ym} is a class indicator function such that
θlm = 1 if xl and xm belongs to different classes  and 0 otherwise. Moreover  the equality in (15)
holds when η(i) ≡ 1

−1
n
nhd
n

Q for 1 ≤ i ≤ Q.

√

2hn

in (13) and Hlm;hn in (16) are the new pairwise similarity functions induced by the plug-
Glm;
in classiﬁer and the nearest neighbor classiﬁer respectively. According to the proof of Theorem 1 and
Theorem 2  the kernel density estimator ˆf can be replaced by the true density f in the denominators
√
of (13) and (16)  and the conclusions of Theorem 1 and 2 still hold. Therefore  both Glm;
and
Hlm;hn are equal to ordinary Gaussian kernels (up to a scale) with different kernel bandwidth under
uniform distribution  which explains the broadly used kernel similarity in data clustering from an
angle of supervised learning.

2hn

4 Application to Exemplar-Based Clustering

We propose a nonparametric exemplar-based clustering algorithm using the derived nonparametric
pairwise similarity by the plug-in classiﬁer. In exemplar-based clustering  each xl is associated with
a cluster indicator el (l ∈ {1  2  ...n}   el ∈ {1  2  ...n})  indicating that xl takes xel as the cluster
exemplar. Data from the same cluster share the same cluster exemplar. We deﬁne e   {el}n
l=1.
Moreover  a conﬁguration of the cluster indicators e is consistent iff el = l when em = l for any
l  m ∈ 1..n  meaning that xl should take itself as its exemplar if any xm take xl as its exemplar. It is
required that the cluster indicators e should always be consistent. Afﬁnity Propagation (AP) [23]  a
representative of the exemplar-based clustering methods  solves the following optimization problem

n∑

min

e

l=1

Sl;el

s:t: e is consistent

(17)

6

Sl;el is the dissimilarity between xl and xel  and note that Sl;l is set to be nonzero to avoid the trivial
minimizer of (17).
Now we aim to improve the discriminative capability of the exemplar-based clustering (17) using
the nonparametric pairwise similarity derived by the unsupervised plug-in classiﬁer. As mentioned
before  the quality of the hypothetical labeling ˆy is evaluated by the generalization error bound for
the nonparametric plug-in classiﬁer trained by S^y  and the hypothetical labeling ˆy with minimum
associated error bound is preferred  i.e. arg min^y
where
√
θlm = 1I^yl̸=^ym and Glm;
enforces minimization of the weighted volume of cluster boundary asymptotically. To avoid the
trivial clustering where all the data are grouped into a single cluster  we use the sum of within-
cluster dissimilarities term
to control the size of clusters. Therefore  the
objective function of our pairwise clustering method is below:

is deﬁned in (13). By Lemma 3  minimizing

ˆRn (PIS) = arg min^y

∑
∑

θlmGlm;

θlmGlm;

2hn
√

(

also

2hn

2hn

2hn

l;m

l;m

√

exp

n∑
(
−Glel;

l=1

−Glel;
√
)

√

+ (cid:21)

)
∑

(

n∑

(cid:9) (e) =

l=1

exp

where ρlm is a function to enforce the consistency of the cluster indicators:

√
~(cid:18)lmGlm;

+ (cid:26)lm (el; em)

2hn

2hn

l;m

{ ∞ em = l; el ̸= l or el = m; em ̸= m

;

(cid:26)lm (el; em) =

0 otherwise

)

(18)

√

2hn

λ is a balancing parameter. Due to the form of (18)  we construct a pairwise Markov Random
Field (MRF) representing the unary term ul and the pairwise term ˜θlmGlm;
+ ρlm as the data
likelihood and prior respectively. The variables e are modeled as nodes and the unary term and
pairwise term in (18) are modeled as potential functions in the pairwise MRF. The minimization of
the objective function is then converted to a MAP (Maximum a Posterior) problem in the pairwise
MRF. (18) is minimized by Max-Product Belief Propagation (BP).
The computational complexity of our clustering algorithm is O(T EN )  where E is the number of
edges in the pairwise MRF  T is the number of iterations of message passing in the BP algorithm.
We call our new algorithm Plug-In Exemplar Clustering (PIEC)  and compare it to representative
exemplar-based clustering methods  i.e. AP and Convex Clustering with Exemplar-Based Model
(CEB) [24]  for clustering on three real data sets from UCI repository  i.e. Iris  Vertebral Column
(VC) and Breast Tissue (BT). We record the average clustering accuracy (AC) and the standard
deviation of AC for all the exemplar-based clustering methods when they produce the correct number
of clusters for each data set with different values of hn and λ  and the results are shown in Table 1.
Although AP produces better clustering accuracy on the VC data set  PIEC generates the correct
cluster numbers for much more times. The dash in Table 1 indicates that the corresponding clustering
method cannot produce the correct cluster number. The default value for the kernel bandwidth hn is
∗
n  which is set as the variance of the pairwise distance between data points
. The
h
∗
n  λ varies between [0.2  1] and
default value for the balancing parameter λ is 1. We let hn = αh
α varies between [0.2  1.9] with step 0.2 and 0.05 respectively  resulting in 170 different parameter
settings. We also generate the same number of parameter settings for AP and CEB.
Table 1: Comparison Between Exemplar-Based Clustering Methods. The number in the bracket is
the number of times when the corresponding algorithm produces correct cluster numbers.

{∥xl − xm∥

}

l<m

Data sets

AP
CEB
PIEC

Iris

0.8933 ± 0.0138 (16)
0.6929 ± 0.0168 (15)
0.9089 ± 0.0033 (15)

VC

BT

0.6677 (14)

0.4748 ± 0.0014 (5)
0.5263 ± 0.0173 (35)

0.4906 (1)

0.3868 ± 0.08 (2)
0.6585 ± 0.0103 (5)

5 Conclusion

We propose a new pairwise clustering framework where nonparametric pairwise similarity is de-
rived by minimizing the generalization error unsupervised nonparametric classiﬁer. Our framework
bridges the gap between clustering and multi-class classiﬁcation  and explains the widely used ker-
nel similarity for clustering. In addition  we prove that the generalization error bound for the unsu-
pervised plug-in classiﬁer is asymptotically equal to the weighted volume of cluster boundary for

7

Low Density Separation. Based on the derived nonparametric pairwise similarity using the plug-in
classiﬁer  we propose a new nonparametric exemplar-based clustering method with enhanced dis-
criminative capability compared to the exiting exemplar-based clustering methods.

Appendix

Lemma 2. (Consistency of Kernel Density Estimator) Let the kernel bandwidth hn of the Gaussian
kernel K be chosen such that hn → 0  log h
→ 0. For any PX ∈ PX  there exists a n0 which
depends on σ0 and VC characteristics of K  when n > n0  with probability greater than 1− Lh
over the data {xl} 

−1
n
nhd
n

E(cid:27)2
0
n

(cid:13)(cid:13)(cid:13) ^fn;hn (x) − f (x)
(cid:13)(cid:13)(cid:13)

(√

∞ = O

−1
log h
n
nhd
n

(19)
where ˆfn;hn is the kernel density estimator of f. Furthermore  for any PXY ∈ PXY   when n > n0 
then with probability greater than 1 − 2Lh

+ h(cid:13)
n

E(cid:27)2
0
n

(cid:13)(cid:13)(cid:13)^(cid:17)(i)

n;hn

(x) − (cid:17)(i) (x)

(cid:13)(cid:13)(cid:13)

(√

over the data {xl} 
−1
∞ = O
log h
n
nhd
n

+ h(cid:13)
n

for each 1 ≤ i ≤ Q.
Lemma 3. (Consistency of the Generalized Kernel Density Estimator) Suppose f is the probabilistic
density function of PX ∈ PX. Let g be a bounded function deﬁned on X and g ∈ Σ(cid:13);g0  0 < gmin ≤
g ≤ gmax  and e = f

g . Deﬁne the generalized kernel density estimator of e as

(20)

)

)

^en;h   1
n

Kh (x − xl)

g (xl)

(21)

∥K∥2
g2

g =

Let σ2
n > ng  with probability greater than 1 − Lh

2fmax
min

. There exists ng which depends on σg and the VC characteristics of K  When

n∑

l=1

(
t−·
g (·)
h

E(cid:27)2
g
n

over the data {xl} 

(√

)

−1
log h
n
nhd
n

+ h(cid:13)
n

where hn is chosen such that hn → 0  log h
Sketch of proof: For ﬁxed h ̸= 0  we consider the class of functions

−1
n
nhd
n

∥^en;hn (x) − e (x)∥∞ = O
→ 0.
)

It can be veriﬁed that Fg is also a bounded VC class with the envelope function Fg = F

gmin

Fg   { K

(
Fg;∥·∥

N

; t ∈ IRd; h ̸= 0}
(

)

)

v

A
(cid:28)

L2(P ) ; (cid:28) ∥Fg∥

L2(P )

≤

(22)

(23)

  and

Then (22) follows from similar argument in the proof of Lemma 2 and Corollary 2.2 in [14].

The generalized kernel density estimator (21) is also used in [13] to estimate the Laplacian PDF
Distance between two probabilistic density functions  and the authors only provide the proof of
pointwise weak consistency of this estimator in [13]. Under mild conditions  our Lemma 3 and
Lemma 2 show the strong consistency of the generalized kernel density estimator and the traditional
kernel density estimator under the same theoretical framework of the VC property of the kernel.
Acknowledgements. This material is based upon work supported by the National Science Founda-
tion under Grant No. 1318971.

8

References
[1] Hariharan Narayanan  Mikhail Belkin  and Partha Niyogi. On the relation between low density separation 

spectral clustering and graph cuts. In NIPS  pages 1025–1032  2006.

[2] J. A. Hartigan and M. A. Wong. A K-means clustering algorithm. Applied Statistics  28:100–108  1979.
[3] Andrew Y. Ng  Michael I. Jordan  and Yair Weiss. On spectral clustering: Analysis and an algorithm. In

NIPS  pages 849–856  2001.

[4] Noam Shental  Assaf Zomet  Tomer Hertz  and Yair Weiss. Pairwise clustering and graphical models. In

NIPS  2003.

[5] Linli Xu  James Neufeld  Bryce Larson  and Dale Schuurmans. Maximum margin clustering. In NIPS 

2004.

[6] Zohar Karnin  Edo Liberty  Shachar Lovett  Roy Schwartz  and Omri Weinstein. Unsupervised svms: On
the complexity of the furthest hyperplane problem. Journal of Machine Learning Research - Proceedings
Track  23:2.1–2.17  2012.

[7] Ryan Gomes  Andreas Krause  and Pietro Perona. Discriminative clustering by regularized information

maximization. In NIPS  pages 775–783  2010.

[8] Masashi Sugiyama  Makoto Yamada  Manabu Kimura  and Hirotaka Hachiya. On information-
maximization clustering: Tuning parameter selection and analytic solution. In ICML  pages 65–72  2011.
[9] T. Cover and P. Hart. Nearest neighbor pattern classiﬁcation. Information Theory  IEEE Transactions on 

13(1):21–27  January 1967.

[10] Luc Devroye. A probabilistic theory of pattern recognition  volume 31. springer  1996.
[11] Yuhong Yang. Minimax nonparametric classiﬁcation - part i: Rates of convergence. IEEE Transactions

on Information Theory  45(7):2271–2284  1999.

[12] Jean-Yves Audibert and Alexandre B. Tsybakov. Fast learning rates for plug-in classiﬁers. The Annals of

Statistics  35(2):pp. 608–633  2007.

[13] Robert Jenssen  Deniz Erdogmus  Jos´e Carlos Pr´ıncipe  and Torbjørn Eltoft. The laplacian pdf distance:

A cost function for clustering in a kernel feature space. In NIPS  2004.

[14] Evarist Gin´e and Armelle Guillou. Rates of strong uniform consistency for multivariate kernel density

estimators. Ann. Inst. H. Poincar´e Probab. Statist.  38(6):907–921  November 2002.

[15] Uwe Einmahl and David M. Mason. Uniform in bandwidth consistency of kernel-type function estimators.

The Annals of Statistics  33:1380C1403  2005.

[16] R. M. Dudley. Uniform Central Limit Theorems. Cambridge University Press  1999.
[17] A.W. van der Vaart and J.A. Wellner. Weak Convergence and Empirical Processes. Springer series in

statistics. Springer  1996.

[18] Deborah Nolan and David Pollard. U-Processes: Rates of convergence. The Annals of Statistics  15(2) 

1987.

[19] Olivier Chapelle and Alexander Zien. Semi-Supervised Classiﬁcation by Low Density Separation. In

AISTATS  2005.

[20] Markus Maier  Ulrike von Luxburg  and Matthias Hein. Inﬂuence of graph construction on graph-based

clustering measures. In NIPS  pages 1025–1032  2008.

[21] Zenglin Xu  Rong Jin  Jianke Zhu  Irwin King  Michael R. Lyu  and Zhirong Yang. Adaptive regulariza-

tion for transductive support vector machine. In NIPS  pages 2125–2133  2009.

[22] Xiaojin Zhu  John Lafferty  and Ronald Rosenfeld. Semi-supervised learning with graphs. PhD thesis 

Carnegie Mellon University  Language Technologies Institute  School of Computer Science  2005.

[23] Brendan J. Frey and Delbert Dueck. Clustering by passing messages between data points. Science 

315:972–977  2007.

[24] Danial Lashkari and Polina Golland. Convex clustering with exemplar-based models. In NIPS  2007.

9

,Yingzhen Yang
Feng Liang
Shuicheng Yan
Zhangyang Wang
Thomas Huang