2019,Classification Accuracy Score for Conditional Generative Models,Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results  especially on large-scale datasets such as ImageNet  suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis  we use class-conditional generative models from a number of model classes—variational autoencoders  autoregressive models  and generative adversarial networks (GANs)—to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task  which we call Classification Accuracy Score (CAS)  reveals some surprising results not identified by traditional metrics and constitute our contributions. First  when using a state-of-the-art GAN (BigGAN-deep)  Top-1 and Top-5 accuracy decrease by 27.9% and 41.6%  respectively  compared to the original data; and conditional generative models from other model classes  such as Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical Autoregressive Models (HAMs)  substantially outperform GANs on this benchmark. Second  CAS automatically surfaces particular classes for which generative models failed to capture the data distribution  and were previously unknown in the literature. Third  we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore  in order to facilitate better diagnoses of generative models  we open-source the proposed metric.,ClassiﬁcationAccuracyScoreforConditionalGenerativeModelsSumanRavuri&OriolVinyals∗DeepMindLondon UKN1C4AGravuris vinyals@google.comAbstractDeepgenerativemodels(DGMs)ofimagesarenowsufﬁcientlymaturethattheyproducenearlyphotorealisticsamplesandobtainscoressimilartothedatadis-tributiononheuristicssuchasFrechetInceptionDistance(FID).Theseresults especiallyonlarge-scaledatasetssuchasImageNet suggestthatDGMsarelearn-ingthedatadistributioninaperceptuallymeaningfulspaceandcanbeusedindownstreamtasks.Totestthislatterhypothesis weuseclass-conditionalgenerativemodelsfromanumberofmodelclasses—variationalautoencoders autoregressivemodels andgenerativeadversarialnetworks(GANs)—toinfertheclasslabelsofrealdata.Weperformthisinferencebytraininganimageclassiﬁerusingonlysyntheticdataandusingtheclassiﬁertopredictlabelsonrealdata.Theperfor-manceonthistask whichwecallClassiﬁcationAccuracyScore(CAS) revealssomesurprisingresultsnotidentiﬁedbytraditionalmetricsandconstituteourcontributions.First whenusingastate-of-the-artGAN(BigGAN-deep) Top-1andTop-5accuracydecreaseby27.9%and41.6% respectively comparedtotheoriginaldata;andconditionalgenerativemodelsfromothermodelclasses suchasVector-QuantizedVariationalAutoencoder-2(VQ-VAE-2)andHierarchicalAutoregressiveModels(HAMs) substantiallyoutperformGANsonthisbench-mark.Second CASautomaticallysurfacesparticularclassesforwhichgenerativemodelsfailedtocapturethedatadistribution andwerepreviouslyunknownintheliterature.Third weﬁndtraditionalGANmetricssuchasInceptionScore(IS)andFIDneitherpredictiveofCASnorusefulwhenevaluatingnon-GANmodels.Furthermore inordertofacilitatebetterdiagnosesofgenerativemodels weopen-sourcetheproposedmetric.1IntroductionEvaluatinggenerativemodelsofhigh-dimensionaldataremainsanopenproblem.Despiteanumberofsubtletiesingenerativemodelassessment[1] inaquesttoimprovegenerativemodelsofimages researchers andparticularlythosewhohavefocusedonGenerativeAdversarialNetworks[2] haveidentiﬁeddesirablepropertiessuchas“samplequality”and“diversity”andproposedautomaticmetricstomeasurethesedesiderata.Asaresult recentyearshavewitnessedarapidimprovementinthequalityofdeepgenerativemodels.Whileultimatelytheutilityofthesemodelsistheirperformanceindownstreamtasks thefocusonthesemetricshasledtomodelswhosesamplersnowgeneratenearlyphotorealisticimages[3–5].Foronemodelinparticular BigGAN-deep[3] resultsonstandardGANmetricssuchasInceptionScore(IS)[6]andFrechetInceptionDistance(FID)[7]approachthoseofthedatadistribution.TheresultsonFID whichpurportstobetheWasserstein-2metricinaperceptualfeaturespace inparticularsuggestthatBigGANsarecapturingthedatadistribution.∗Correspondingauthor:SumanRavuri(ravuris@google.com).33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.BalloonPaddlewheelPencilSharpenerSpatulaFigure1:CASidentiﬁesclassesforwhichBigGAN-deepfailstocapturethedatadistribution.Toprowarerealimages andthebottomtworowsaresamplesfromBigGAN-deep.Asimilar thoughlessheralded improvementhasoccurredformodelswhoseobjectivesare(boundsof)likelihood withtheresultthatmanyofthesemodelsnowalsoproducephotorealisticsamples.Examplesinclude:SubscalePixelNetworks[8] unconditionalautoregressivemodelsof128×128ImageNetthatachievestate-of-the-arttestsetlog-likelihoods;HierarchicalAutoregressiveModels(HAMs)[9] class-conditionalautoregressivemodelsof128×128and256×256ImageNet;andtherecentlyintroducedVector-QuantizedVariationalAutoencoder-2(VQ-VAE-2)[10] avariationalautoencoderthatusesvectorquantizationandanautoregressivepriortoproducehigh-qualitysamples.Notably thesemodelsmeasurediversityusingtestsetlikelihoodandassesssamplequalitythroughvisualinspection eschewingthemetricstypicallyusedinGANresearch.Asthesemodelsincreasinglyseem“tolearnthedistribution”accordingtothesemetrics itisnaturaltoconsidertheiruseindownstreamtasks.Suchaviewcertainlyhasaprecedent:improvedtestsetlikelihoodsinlanguagemodels unconditionalmodelsoftext alsoimproveperformanceintaskssuchasspeechrecognition[11].Whileagenerativemodelneednotlearnthedatadistributiontoperformwellonadownstreamtask poorperformanceonsuchtasksallowsustodiagnosespeciﬁcproblemswithbothourgenerativemodelsandthetask-agnosticmetricsweusetoevaluatethem.Tothatend weuseageneralframework(ﬁrstposedin[12]andfurtherstudiedin[13])inwhichweuseconditionalgenerativemodelstoperformapproximateinferenceandmeasurethequalityofthatinference.Theideaissimple:foranygenerativemodeloftheformpθ(x|y) welearnaninferencenetworkˆp(y|x)usingonlysamplesfromtheconditionalgenerativemodelandmeasuretheperformanceoftheinferencenetworkonadownstreamtask.Wethencompareperformancetothatofaninferencenetworktrainedonrealdata.Weapplythisframeworktoconditionalimagemodelswhereyistheimagelabel xistheimage andtaskisimageclassiﬁcation.(N.B.thisapproachhasbeenusedforevaluatingsmallerscaleGANs[12–16]).Theperformancemeasureweuse Top-1andTop-5accuracy denoteaClassiﬁcationAccuracyScore(CAS).Thegapinperformancebetweennetworkstrainedonrealandsyntheticdataallowsustounderstandspeciﬁcdeﬁcienciesinthegenerativemodel.Althoughasimplemetric CASrevealssomesurprisingresults:•Whenusingastate-of-the-artGAN(BigGAN-deep)andanoff-the-shelfResNet-50classiﬁerastheinferencenetwork wefoundthatTop-1andTop-5accuraciesdecreaseby27.9%and41.6% respectively comparedtousingrealdata.•Conditionalgenerativemodelsbasedonlikelihood suchasVQ-VAE-2andHAM performwellcomparedtoBigGAN-deep despiteachievingrelativelypoorInceptionScoresandFrechetInceptionDistances.Sincethesemodelsproducevisuallyappealingsamples theresultsuggeststhatISandFIDarepoormeasuresofnon-GANmodelperformance.•CASautomaticallysurfacesparticularclassesforwhichBigGAN-deepandVQ-VAE-2failtocapturethedatadistributionandwerepreviouslyunknownintheliterature.Figure1showsfoursuchclassesforBigGAN-deep.•WeﬁndthatneitherIS norFID norcombinationsthereofarepredictiveofCAS.Asgenerativemodelsmaysoonbedeployedindownstreamtasks theseresultssuggestthatweshouldcreatemetricsthatbettermeasuretaskperformance.•WecalculateaNaiveAugmentationScore(NAS) avariantofCASwheretheimageclassiﬁeristrainedonbothrealandsyntheticimages todemonstratethatclassiﬁcationperformanceimprovesinlimitedcircumstances.AugmentingtheImageNettrainingsetwithlow-diversityBigGAN-deepimagesimprovesTop-5accuracyby0.2% whileaugmentingthedatasetwithanyothersyntheticimagesdegradesclassiﬁcationperformance.2InSection2weprovideafewdeﬁnitions desiderataofmetrics andshortcomingsofthemostpopularmetricsinrelationtodifferentresearchdirectionsforgenerativemodeling.Section3deﬁnesCAS.Finally Section4providesalarge-scalestudyofcurrentstate-of-the-artgenerativemodelsusingFID IS andCASonboththeImageNetandCIFAR-10datasets.2MetricsforGenerativeModelsMuchofthedifﬁcultyinevaluatinganygenerativemodelisnotknowingthetaskforwhichthemodelwillbeused.Understandinghowthemodelwillbedeployed however hasimportantimplicationsonitsdesiredproperties.Forexample considertheseeminglysimilartasksofautomaticspeechrecognitionandspeechsynthesis.Whilebothtasksmaysharethesamegenerativemodelofspeech—suchasahiddenMarkovModelpθ(o l)withtheobservedandlatentvariablesbeingthewaveformoandwordsequencel respectively—theimplicationsofmodelmisspeciﬁcationarevastlydifferent.Inspeechrecognition themodelshouldbeabletoinferwordsforallpossiblespeechwaveforms evenifthewaveformsthemselvesaredegraded.Inspeechsynthesis however themodelshouldproducethemostrealistic-soundingsamples evenifitcannotproduceallpossiblespeechwaveforms.Inparticular forautomaticspeechrecognition wecareaboutpθ(l|o) whileforspeechsynthesis wecareabouto∼pθ(o|l).Inabsenceofaknowndownstreamtask weassesstowhatextentthemodeldistributionpθ(x)matchesthedatadistributionpdata(x) alessspeciﬁcandoftenmoredifﬁcultgoal.Twoconsequencesofthetrivialobservationthatpθ(x)=pdata(x)are:1)eachsamplex∼pθ(x)“comes”fromthedatadistribution(i.e. itisa“plausible”samplefromthedatadistribution) and2)thatallpossibleexamplesfromthedatadistributionarerepresentedbythemodel.Differentmetricsthatevaluatethedegreeofmodelmismatchweighthesecriteriadifferently.Furthermore weexpectourmetricstoberelativelyfasttocalculate.Thislastdesideratumoftendependsonthemodelclass.Themostpopularseemtobe:•(Inexact)Likelihoodmodelsusingvariationalinference(e.g. VAE[17 18])•Likelihoodusingautoregressivemodels(e.g. PixelCNN[19])•Likelihoodmodelsbasedonbijections(e.g. GLOW[20] rNVP[21])•(Possiblyinexact)likelihoodusingenergy-basedmodels(e.g. RBM[22])•Implicitgenerativemodels(e.g. GANs)Fortheﬁrstfouroftheseclasses thelikelihoodobjectiveprovidesusscaledestimatesoftheKL-divergencebetweenthedataandmodel.Furthermore testsetlikelihoodisalsoanimplicitmeasureofdiversity.Thelikelihood however isafairlypoormeasureofsamplequality[1]andoftenscoresout-of-domaindatamorehighlythanin-domaindata[23].Forimplicitmodels theobjectiveprovidesneitheranaccurateestimateofastatisticaldivergenceordistancenoranaturalevaluationmetric.Thelackofanysuchmetricslikelyforcedresearcherstoproposeheuristicsthatmeasureversionsofboth1and2(samplequalityanddiversity)simultaneously.InceptionScore(IS)[6](exp(Ex[p(y|x)kp(y)])measures1byhowconﬁdentlyaclassiﬁerassignsanimagetoaparticularclass(p(y|x)) and2bypenalizingiftoomanyimageswereclassiﬁedtothesameclass(p(y)).MoreprincipledversionsofthisprocedureareFrechetInceptionDistance(FID)[7]andKernelInceptionDistance(KID)[24] whichbothusevariantsoftwo-sampletestsinalearned“perceptual”featurespace theInceptionpool3space toassessdistributionmatching.Eventhoughthisspacewasanad-hocproposition recentwork[25]suggeststhatdeepfeaturescorrelatewithhumanperceptionofsimilarity.Evenmorerecentwork[26 27]calculate1and2independentlybycalculatingprecisionandrecall.RelianceonISandFIDinparticularhasledtoimprovementinGANmodelsbuthascertaindeﬁciencies.ISdoesnotpenalizealackofintra-classdiversity andcertainout-of-distributionsamplesproduceInceptionScoresthreetimeshigherthanthatofthedata[28].FID ontheotherhand suffersfromahighdegreeofbias[24].Moreover thepool3featurelayermaynotevencorrelatewellwithhumanjudgmentofsamplequality[29].Inthiswork wealsoﬁndthatnon-GANmodelshaveratherpoorInceptionScoresandFrechetInceptionDistances eventhoughthesamplesarevisuallyappealing.3Ratherthancreatingad-hocheuristicsaimedatbroadlymeasuringsamplequalityanddiversity weinsteadevaluategenerativemodelsbyassessingtheirperformanceonadownstreamtask.Thisisakintomeasuringagenerativemodelofspeechbyevaluatingitonautomaticspeechrecognition.Sincemodelsconsideredhereareimplicitordonotadmitexactlikelihoods exactinferenceisdifﬁcult.Tocircumventthisissue wetrainaninferencenetworkonsamplesfromthemodel.Ifthegenerativemodelisindeedcapturingthedatadistribution thenwecouldreplacetheoriginaldistributionwithamodel-generatedone performanydownstreamtask andobtainthesameresult.Inthiswork westudyperhapsthesimplestdownstreamtask:imageclassiﬁcation.Thisideaisnotnecessarilynew:forGANevaluation ithasbeenindependentlydiscoveredatleastfourtimes.[12]ﬁrstintroducedthemetric(denoted“adversarialaccuracy”)tomeasuretheirproposedLayer-RecursiveGANandconnectedimageclassiﬁcationtoapproximateinference.[13]moresystematicallystudiedthisideaofapproximateinferencetomeasuretheboundarydistortioninducedbyGANs.Theydidthisbytrainingseparateper-labelunconditionalgenerativemodels andthentrainedclassiﬁersonsyntheticdatatounderstandhowtheboundaryshiftedandtomeasurethesamplediversityofGANs.Predating[13] [14]used“TrainonSynthetic TestonReal”tomeasurearecurrentconditionalGANformedicaldata.[16]trainedonsyntheticdata testedonreal(denoted“GAN-train”)asanapproximaterecallmetricforGANs.Theyalsotrainedonrealdataandtestedonsynthetic(denoted“GAN-test”)asanapproximateprecisiontest.Unlikepreviouswork theytestedonlargerdatasetssuchas128×128ImageNet butwithsmallerscalemodelssuchasSNGAN[30].Themetricsmentionedabovearebynomeanstheonlyones andresearchershaveproposedmethodstoevaluateotherpropertiesofgenerativemodels.[31]constructsapproximatemanifoldsfromdataandsamples andappliesthemethodtoGANsamplestodeterminewhethermodecollapseoccurred.[32]attemptstodeterminethesupportsizeofGANsbyusingaBirthdayParadoxtest thoughtheprocedurerequiresahumantoidentifytwonearly-identicalsamples.MaximumMeanDiscrepancy[33]isatwo-sampletestthathasmanynicetheoreticalpropertiesbutseemstobelessusedbecausethechoiceofkernelsdonotnecessarilycoincidewithhumanjudgment.Procedurallysimilartoourmethod [34]proposesa“reverseLMscore” whichtrainsalanguagemodelonGANdataandtestsonarealheld-outset.[35]measuresthequalityofgenerativemodelsoftextbytrainingasentimentanalysisclassiﬁer.Finally [36]classiﬁesrealdatausingastudentnetworkmimickingateachernetworkpretrainedonrealdatabutdistilledonGANdata.Ourworkmostcloselymirrors[16] butdiffersinasomekeyrespects.First sinceweviewimageclassiﬁcationasapproximateinference weareabletodescribeitslimitationsinSection3 andverifytheapproximationinSection4.5.Second whilein[16]performanceonGAN-traincorrelateswithimprovedISandFID wefocusmoreonlarge-scaleandnon-GANmodels suchasVQ-VAE-2andHAMs whereFIDandISarenotindicativeofclassiﬁcationperformance.Third bypollingtheinferencenetwork wecanidentifyclassesforwhichthemodelfailedtocapturethedatadistribution.Finally weopen-sourcethemetricforImageNetforeaseofevaluatinglarge-scalegenerativemodels.3ClassiﬁcationAccuracyScoreAttheheartofCASliesaverysimpleidea:ifthemodelcapturesthedatadistribution performanceonanydownstreamtaskshouldbesimilarwhetherusingtheoriginalormodeldata.Tomakethisintuitionmoreprecise supposethatdatacomesfromadistributionp(x y) thetaskistoinferyfromx andwesufferalossL(y ˆy)forpredictingˆywhenthetruelabelisy.Theriskassociatedwithaclassiﬁerˆy=f(x)is:Ep(x y)[L(y ˆy)]=Ep(x)[Ep(y|x)[L(y ˆy)|X]](1)Asweonlyhavesamplesfromp(x y) wemeasuretheempiricalrisk1NL(yi f(xi)).FromtherighthandsideofEquation1 ofthesetofpredictionsY theoptimaloneˆyminimizestheexpectedposteriorloss:ˆy=argminy0∈YEp(y|x)[L(y y0)|X](2)Assumingweknowthelabeldistributionp(y) agenerativemodelingapproachtothisproblemistomodeltheconditionaldistributionpθ(x|y) andinferlabelsusingBayesrule:pθ(y|x)=pθ(x|y)p(y)pθ(x).Ifpθ(y|x)=p(y|x) thenwecanmakepredictionsthatminimizetheriskforanylossfunction.Iftheriskisnotminimized however thenwecanconcludethatdistributionsarenotmatched andwecaninterrogatepθ(y|x)tobetterunderstandhowourgenerativemodelsfailed.4Formostmoderndeepgenerativemodels however wehaveaccesstoneitherpθ(x|y) theprobabilityofthedatagiventhelabel norpθ(y|x) themodelconditionaldistribution norp(y|x) thetrueconditionaldistribution.Instead fromsamplesx y∼p(y)pθ(x|y) wetrainadiscriminativemodelˆp(y|x)tolearnpθ(y|x) anduseittoestimatetheexpectedposteriorlossEˆp(y|x)[L(y ˆy)|X].WedeﬁnethegenerativeriskasEp(x y)[L(y ˆyg)] whereˆygistheclassiﬁerthatminimizestheexpectedposteriorlossunderˆp(y|x).Thenwecomparetheperformanceoftheclassiﬁertotheperformanceoftheclassiﬁertrainedonsamplesfromp(x y).Inthecaseofconditionalgenerativemodelsofimages yistheclasslabelforimagex andthemodelofˆp(y|x)isanimageclassiﬁer.WeuseResNets[37]inthiswork.ThelossfunctionsLweexplorearethestandardonesforimageclassiﬁcation.Oneis0-1 whichyieldsTop-1accuracy andtheotheris0-1intheTop-5 whichyieldsTop-5accuracy.2Procedurally wetrainaclassiﬁeronsyntheticdata andevaluatetheperformanceoftheclassiﬁeronrealdata.WecalltheaccuracytheClassiﬁcationAccuracyScore(CAS).NotethataCASclosetothatforthedatadoesnotimplythatthegenerativemodelaccuratelymodeledthedatadistribution.Thismayhappenforafewreasons.First pθ(y|x)=p(y|x)foranygenerativemodelthatsatisﬁespθ(x|y)pθ(x)=p(x|y)p(x)forallx y∼p(x y).Oneexampleisagenerativemodelthatsamplesfromthetruedistributionwithprobabilityp andfromanoisedistributionwithasupportdisjointfromthetruedistributionwithprobability1−p.Inthiscase ourinferencemodelisgoodbuttheunderlyinggenerativemodelispoor.Second sincethelossesconsideredherearenotproperscoringrules[38] onecouldobtainreasonableCASfromsuboptimalinferencenetworks.Forexample supposethatp(y|x)=1.0forthecorrectclasswhileˆp(y|x)=0.51forthecorrectclassduetopoorsyntheticdata.CASforbothis100%.Usingaproperscoringrule suchasBrierScore eliminatesthisissue butexperimentallywefoundlimitedpracticalbeneﬁtfromusingone.Finally agenerativemodelthatmemorizesthetrainingsetwillachievethesameCASastheoriginaldata.3Ingeneral however wehopethatgenerativemodelsproducesamplesdisjointfromthesetonwhichtheyaretrained.Ifthesamplesaresufﬁcientlydifferent wecantrainaclassiﬁeronboththeoriginaldataandmodeldataandexpectimprovedaccuracy.Wedenotetheperformanceofclassiﬁerstrainedonthis“naiveaugmentation”NaiveAugmentationScore(NAS).OurCASresults however indicatethatthecurrentmodelsstillsigniﬁcantlyunderﬁt renderingtheconclusionslesscompelling.Forcompleteness weincluderesultsonaugmentationinSection4.4.Despitethesetheoreticalissues weﬁndthatgenerativemodelshaveClassiﬁcationAccuracyScoreslowerthantheoriginaldata indicatingthattheyfailtocapturethedatadistribution.3.1ComputationandOpen-SourcingMetricComputationally trainingclassiﬁersissigniﬁcantlymoredemandingthancalculatingFIDorISover50 000samples.Webelieve however thatnowistherighttimeforsuchametricduetoafewkeyadvancesintrainingclassiﬁers:1)thetrainingofImageNetclassiﬁershasbeenreducedtominutes[39] 2)withcloudservices thevarianceduetoimplementationdetailsofsuchametricislargelymitigated and3)thepriceandtimecostoftrainingclassiﬁersoncloudservicesisreasonableandwillonlyimproveovertime.Moreover manyclass-conditionalgenerativemodelsarecomputationallyexpensivetotrain andthus evenarelativelyexpensivemetricsuchasCAScomprisesasmallpercentageofthetrainingbudget.Weopen-sourceourmetriconGoogleCloudforotherstouse.TheinstructionsaregiveninAppendixB.Atthetimeofwriting onecancomputethemetricin10hoursforroughly$15 orin45minutesforroughly$85usingTPUs.Moreover dependingonafﬁliation onemaybeabletoaccessTPUsforfreeusingtheTensorﬂowResearchCloud(TFRC)(https://www.tensorflow.org/tfrc/).2Itismorecorrecttostatethatthelossesyielderrors butwepresentresultsasaccuraciesinsteadastheyarestandardincomputervisionliterature.3N.B.ISandFIDalsosufferthesamefailuremode.5Table1:CASfordifferentmodelsat128×128and256×256resolutions.BigGAN-deepsamplesaretakenfrombesttruncationparameterof1.5.TrainingSetResolutionTop-5Top-1ISFID-50KAccuracyAccuracyReal128×12888.79%68.82%165.38±2.841.61BigGAN-deep128×12864.44%40.64%71.31±1.574.22HAM128×12877.33%54.05%17.02±0.7946.05Real256×25691.47%73.09%331.83±5.002.47BigGAN-deep256×25665.92%42.65%109.39±1.5611.78VQ-VAE-2256×25677.59%54.83%43.44±0.8738.05Figure2:Comparisonofper-classaccuracyofdata(blue)vs.model(red).Left:BigGAN-deep256×256at1.5truncationlevel.Middle:VQ-VAE-2256×256.Right:HAM128×128.4ExperimentsOurexperimentsaresimple:onImageNet weusethreegenerativemodels—BigGAN-deepat256×256and128×128resolutions HAMwithmaskedself-predictionauxiliarydecoderat128×128resolution andVQ-VAE-2at256×256resolution—toreplacetheImageNettrainingsetwithamodel-generatedone trainanimageclassiﬁer andevaluateperformanceontheImageNetvalidationset.TocalculateCAS wereplacetheImageNettrainingsetwithonesampledfromthemodel andeachexamplefromtheoriginaltrainingsetisreplacedwithamodelsamplefromthesameclass.Inaddition wecompareCAStotwotraditionalGANmetrics:ISandFID asthesemetricsarethecurrentgoldstandardforGANcomparisonandarebeingusedtocomparenon-GANtoGANmodels.BothrelyonafeaturespacefromaclassiﬁertrainedonImageNet suggestingthatifmetricsareusefulatpredictingperformanceonadownstreamtask itwouldindeedbethisone.FurtherdetailsabouttheexperimentcanbefoundinAppendixA.1.4.1ModelComparisononImageNetTable1showstheperformanceofclassiﬁerstrainedonmodel-generateddatasetscomparedtothoseontherealdatasetfor256×256and128×128 respectively.At256×256resolution BigGAN-deepachievesaCASTop-5of65.92% suggestingthatBigGANsarelearningnontrivialdistributions.Perhapssurprisingly VQ-VAE-2 thoughperformingquitepoorlycomparedtoBigGAN-deeponbothFIDandIS obtainsaCASTop-5accuracyof77.59%.Bothmodels however lagtheoriginal256×256dataset whichachievesaCASTop-5Accuracyof91.47%.Weﬁndnearlyidenticalresultsforthe128×128models.BigGAN-deepachievesCASTop-5andTop-1similartothe256×256model(notethatISandFIDresultsfor128×128and256×256BigGAN-deeparevastlydifferent).HAMs similartoVQ-VAE-2 performpoorlyonFIDandInceptionScorebutoutperformBigGAN-deeponCAS.Moreover bothmodelsunderperformrelativetotheoriginal128×128dataset.4.2UncoveringModelDeﬁcienciesTobetterunderstandwhataccountsforthegapbetweengenerativemodelanddatasetCAS webrokedowntheperformancebyclass(Figure2).Asshownintheleftpane nearlyeveryclassofBigGAN-deepsuffersadropinperformancecomparedtotheoriginaldataset thoughsixclasses—6BigGAN-deepVQ-VAE-2HAMFigure3:Thetoptworowsaresamplesfromclassesthatachievedthebesttestsetperformancerelativetooriginaldataset.Thebottomtworowsarethosefromclassesthatachievedtheworst.Left:BigGAN-deeptoptwo—squirrelmonkeyandredfox—andbottomtwo—(hotair)balloonandpaddlewheel.Middle:VQ-VAE-2toptwo—redfoxandAfricanelephant—andbottomtwo—guillotineandfurcoat.Right:HAMtoptwo—huskyandgong/tim-tam—andbottomtwo—hermitcrabandharmonica.Figure4:Top:Originals Bottom:ReconstructionsusingVQ-VAE-2.partridge redfox jaguar/panther squirrelmonkey Africanelephant andstrawberry—showmarginalimprovementovertheoriginaldataset.TheleftpaneofFigure3showsthetwobestandtwoworstperformingcategories asmeasuredbythedifferenceinclassiﬁcationperformance.Notably forthetwoworstperformingcategoriesandtwoothers—balloon paddlewheel pencilsharpener andspatula—classiﬁcationaccuracywas0%onthevalidationset.Theper-classbreakdownofVQ-VAE-2(middlepaneofFigure2)showsthatthismodelalsounderperformstherealdataformostclasses(only31classesperformbetterthantheoriginaldata) thoughthegapisnotaslargeasforBigGAN-deep.Furthermore VQ-VAE-2hasbettergeneralizationperformancein87.6%ofclassescomparedtoBigGAN-deep andsuffers0%classiﬁcationaccuracyfornoclasses.ThemiddlepaneofFigure3showsthetwobestandtwoworstperformingcategories.TherightpanesofFigures2and3showtheper-classbreakdownandtopandbottomtwoclasses respectively forHAMs.ResultsbroadlymirrorthoseofVQ-VAE-2.4.3ANoteonFIDandaSecondNoteonISWenotethatISandFIDhaveverylittlecorrelationwithCAS suggestingthatalternativemetricsareneededifweintendtodeployourmodelsondownstreamtasks.Asacontrolledexperiment wecalculateCAS IS andFIDforBigGAN-deepmodelswithinputnoisedistributionstruncatedatdifferentvalues(knownasthe“truncationtrick”).Asnotedin[3] lowertruncationvaluesseemtoimprovesamplequalityattheexpenseofdiversity.ForCAS thecorrelationcoefﬁcientbetweenTop-1AccuracyandFIDis0.16 andIS-0.86 thelatterresultincorrectlysuggestingthatimprovedISishighlycorrelatedwithpoorerperformance.Moreover thebest-performingtruncationvalues(1.5and2.0)haveratherpoorISsandFIDs.ThatthesepoorIS/FIDalsoseemtoindicatepoorperformanceonthismetricisnosurprise;thatothermodels withwell-performingISsandFIDsyieldpoorperformanceonCASsuggeststhatalternativemetricsareneeded.OnecaneasilydiagnosetheissuewithIS:asnotedin[28] ISdoesnotaccountforintra-classdiversity andatrainingsetwithlittleintra-classdiversitymaymaketheclassiﬁerfailtogeneralizetoamorediversetestset.7Table2:CASforVQ-VAE-2modelreconstructionsandBigGAN-deepmodelsatdifferenttruncationlevelsat256×256resolution.TrainingSetTruncationTop-5Top-1ISFID-50KAccuracyAccuracyBigGAN-deep0.2013.24%5.11%339.06±3.1420.75BigGAN-deep0.4228.68%13.30%324.62±3.2915.93BigGAN-deep0.5032.88%15.66%316.31±3.7014.37BigGAN-deep0.6045.01%25.51%299.51±3.2012.41BigGAN-deep0.8056.68%32.88%258.72±2.869.24BigGAN-deep1.0062.97%39.07%214.64±2.017.42BigGAN-deep1.5065.92%42.65%109.39±1.5611.78BigGAN-deep2.0064.37%40.98%49.54±0.9828.67VQ-VAE-2reconstructions-89.46%69.90%203.89±2.558.69Real-91.47%73.09%331.83±5.002.47FIDshouldbetteraccountforthislackofdiversityatleastgrossly asthemetric calculatedasFID(Px Py)=kµx−µyk2+tr(Σx+Σy−2(ΣxΣy)1/2) comparesthecovariancematricesofthedataandmodeldistribution.Bycomparison CASoffersaﬁnermeasureofmodelperformance asitprovidesusaper-classmetrictoidentifywhichclasseshavebetterorworseperformance.Whileintheoryonecouldcalculateaper-classFID FIDisknowntosufferfromhighbias[24]foralownumberofsamples suggestingthatper-classestimateswouldbeunreliable.4PerhapsalargerissueisthatISandFIDheavilypenalizenon-GANmodels suggestingthattheseheuristicsarenotsuitableforinter-model-classcomparisons.AparticularlyegregiousfailureisthatISandFIDaggressivelypenalizecertaintypesofsamplesthatlooknearlyidenticaltotheoriginaldataset.Forexample wecomputedCAS IS andFIDontheImageNettrainingsetat256×256resolutionandonreconstructionsfromVQ-VAE-2.AsshowninFigure4 thereconstructionslooknearlyidenticaltotheoriginaldata.AsnotedinTable2 however ISdecreasesby128pointsandFIDincreasesby3.5×.ThedropinperformanceissogreatthatBigGAN-deepat1.00truncationachievesbetterISandFIDthannearly-identicalreconstructions.CASTop-1andTop-5forthereconstructions however dropsby2.2%and4.4% respectively relativetotheoriginaldataset.CASforBigGAN-deepmodelat1.00truncation ontheotherhand dropsby31.1%and46.5%relative.4.4NaiveAugmentationScoreTocalculateNAS weaddtotheoriginalImageNettrainingset25% 50% or100%moredatafromeachofourmodels.TheoriginalImageNettrainingsetachievesaTop-5accuracyof92.97%.AlthoughtheCASresultsforBigGAN-deep andtoalesserextentVQ-VAE-2 suggestthataugment-ingtheoriginaltrainingsetwithmodelsampleswillnotresultinimprovedclassiﬁcationperformance wewantedtostudywhethertherelativeorderingontheCASexperimentswouldholdfortheNASones.Figure5illustratestheperformanceoftheclassiﬁersasweincreasetheamountofsynthetictrainingdata.Perhapssomewhatsurprisingly BigGAN-deepmodelsthatsamplefromlowertrun-cationvalues andhavelowersamplediversity areabletoperformbetterfordataaugmentationcomparedtothosemodelsthatperformedwellonCAS.Infact forsomeofthelowesttruncationvalues wefoundamodestimprovementinclassiﬁcationperformance:roughly0.2%.Moreover VQ-VAE-2underperformsrelativetoBigGAN-deepmodels.Ofcourse thecaveatisthattheformermodeldoesnotyethaveamechanismtotradeoffsamplequalityfromsamplediversity.Theresultsonaugmentationhighlightdifferentdesiderataforsamplesthatareaddedtothedatasetratherthanreplaced.Clearly thesamplesaddedshouldbesufﬁcientlydifferentfromthedatatoallowtheclassiﬁertobettergeneralize yetpoorersamplequalitymayleadtopoorergeneralizationcomparedtotheoriginaldataset.Thismaybethereasonwhyextendingthedatasetwithsamplesgeneratedfromalowertruncationvalue—whicharehigher-quality butlessdiverse—performbetter4[24]proposedKID anunbiasedalternativetoFID butthevarianceofthismetricistoolargetobereliablewhenusingthenumberofper-classsamplesintheImageNettrainingset(roughly1 000perclass) andisworsewhenusingthe50inthevalidationset.Inadditiontosufferinghighbias per-classFIDrequiresestimationoftherealdatacovariancematrixofrank2048usingfarfewersamples leadingtorankdeﬁciency.8Figure5:Top-5accuracyastrainingdataisaugmentedbyx%examplesfromBigGAN-deepfordifferenttruncationlevels.Lowertruncationgeneratesdatasetswithlesssamplediversity.Table3:CASfordifferentmodelsofCIFAR-10.PixelCNN-Bayesdenotesclassiﬁcationaccuracyusingexactinferenceusingthegenerativemodel.RealBigGANcGANPixelCNNPixelCNN-BayesPixelIQNAccuracy92.58%71.87%76.35%64.02%60.05%74.26%onNASthanCAS.Furthermore thismayalsoexplainwhyIS FID andCASarenotpredictiveofNAS.4.5ModelComparisononCIFAR-10Finally wealsocompareCASfordifferentmodelclassesonCIFAR-10.Wecomparefourmodels:BigGAN cGANwithProjectionDiscriminator[30] PixelCNN[19] andPixelIQN[40].WetrainaResNet-56followingthetrainingprocedureof[37].MoredetailscanbefoundinAppendixA.2.SimilartotheImageNetexperiments weﬁndthatbothGANsproducesampleswithacertaindegreeofgeneralization.GANsalsosigniﬁcantlyoutperformPixelCNNonthisbenchmark.Furthermore sincePixelCNNisanexactlikelihoodmodel wecancompareclassiﬁcationperformancewithexactinferenceusingBayesruletothatwithapproximateinferenceusingaclassiﬁer.Perhapssurprisingly CASforPixelCNNismodestlybetterthanclassiﬁcationaccuracyusingexactinference thoughbothresultsaresimilar.Finally PixelIQNhassimilarperformancetothenewerGANs.5ConclusionGoodmetricshavelongbeenanimportant andperhapsunderappreciated componentindrivingimprovementsinmodels.Itmaybeparticularlyimportantnow asgenerativemodelshavereachedamaturitythattheymaybedeployedindownstreamtasks.Weproposedone ClassiﬁcationAccuracyScore forconditionalgenerativemodelsofimagesandfoundthemetricpracticallyusefulinuncov-eringmodeldeﬁciencies.Furthermore weﬁndthatGANmodelsofImageNet despitehighsamplequality tendtounderperformmodelsbasedonlikelihood.Finally weﬁndthatISandFIDunfairlypenalizenon-GANmodels.Anopenquestioninthisworkisunderstandingtowhatextentthesemodelsgeneralizebeyondthetrainingset.Whilecurrentresultssuggestthatevenstate-of-the-artmodelscurrentlyunderﬁt recentprogressindicatesthatunderﬁttingmaybeatemporaryissue.Measuringgeneralizationwillthenbeofprimaryimportance especiallyifmodelsaredeployedondownstreamtasks.9AcknowledgmentsWewouldliketothankAliRazavi AaronvandenOord AndyBrock Jean-BaptisteAlayrac JeffreyDeFauw SanderDieleman JeffDonahue KarenSimonyan TakeruMiyato andGeorgOstrovskifordiscussionandhelpwithmodels.Furthermore wewouldliketothankthosewhocontactedus pointingustopriorwork.References[1]L.Theis A.vandenOord andM.Bethge.Anoteontheevaluationofgenerativemodels.InInternationalConferenceonLearningRepresentations 2016.arXiv:1511.01844.[2]IanGoodfellow JeanPouget-Abadie MehdiMirza BingXu DavidWarde-Farley SherjilOzair AaronCourville andYoshuaBengio.Generativeadversarialnets.InAdvancesinneuralinformationprocessingsystems pages2672–2680 2014.[3]AndrewBrock JeffDonahue andKarenSimonyan.Largescalegantrainingforhighﬁdelitynaturalimagesynthesis.arXivpreprintarXiv:1809.11096 2018.[4]TeroKarras TimoAila SamuliLaine andJaakkoLehtinen.Progressivegrowingofgansforimprovedquality stability andvariation.arXivpreprintarXiv:1710.10196 2017.[5]TeroKarras SamuliLaine andTimoAila.Astyle-basedgeneratorarchitectureforgenerativeadversarialnetworks.arXivpreprintarXiv:1812.04948 2018.[6]TimSalimans IanGoodfellow WojciechZaremba VickiCheung AlecRadford andXiChen.Improvedtechniquesfortraininggans.InAdvancesinNeuralInformationProcessingSystems pages2234–2242 2016.[7]MartinHeusel HubertRamsauer ThomasUnterthiner BernhardNessler andSeppHochreiter.Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium.InAdvancesinNeuralInformationProcessingSystems pages6626–6637 2017.[8]JacobMenickandNalKalchbrenner.Generatinghighﬁdelityimageswithsubscalepixelnetworksandmultidimensionalupscaling.arXivpreprintarXiv:1812.01608 2018.[9]JeffreyDeFauw SanderDieleman andKarenSimonyan.Hierarchicalautoregressiveimagemodelswithauxiliarydecoders.arXivpreprintarXiv:1903.04933 2019.[10]AliRazavi AaronvandenOord andOriolVinyals.Generatingdiversehigh-ﬁdelityimageswithvq-vae-2.arXivpreprintarXiv:1906.00446 2019.[11]StanleyFChen DouglasBeeferman andRoniRosenfeld.Evaluationmetricsforlanguagemodels.1998.[12]JianweiYang AnithaKannan DhruvBatra andDeviParikh.Lr-gan:Layeredrecursivegenerativeadversarialnetworksforimagegeneration.arXivpreprintarXiv:1703.01560 2017.[13]ShibaniSanturkar LudwigSchmidt andAleksanderM˛adry.Aclassiﬁcation-basedstudyofcovariateshiftingandistributions.arXivpreprintarXiv:1711.00970 2017.[14]CristóbalEsteban StephanieLHyland andGunnarRätsch.Real-valued(medical)timeseriesgenerationwithrecurrentconditionalgans.arXivpreprintarXiv:1706.02633 2017.[15]TimothéeLesort Jean-FrançoisGoudou andDavidFilliat.Trainingdiscriminativemodelstoevaluategenerativeones.arXivpreprintarXiv:1806.10840 2018.[16]KonstantinShmelkov CordeliaSchmid andKarteekAlahari.Howgoodismygan?InProceedingsoftheEuropeanConferenceonComputerVision(ECCV) pages213–229 2018.[17]DiederikPKingmaandMaxWelling.Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114 2013.10[18]DaniloJimenezRezende ShakirMohamed andDaanWierstra.Stochasticbackpropagationandapproximateinferenceindeepgenerativemodels.arXivpreprintarXiv:1401.4082 2014.[19]AaronVandenOord NalKalchbrenner LasseEspeholt OriolVinyals AlexGraves etal.Conditionalimagegenerationwithpixelcnndecoders.InAdvancesinneuralinformationprocessingsystems pages4790–4798 2016.[20]DurkPKingmaandPrafullaDhariwal.Glow:Generativeﬂowwithinvertible1x1convolutions.InAdvancesinNeuralInformationProcessingSystems pages10215–10224 2018.[21]LaurentDinh JaschaSohl-Dickstein andSamyBengio.Densityestimationusingrealnvp.arXivpreprintarXiv:1605.08803 2016.[22]PaulSmolensky.Informationprocessingindynamicalsystems:Foundationsofharmonytheory.Technicalreport ColoradoUnivatBoulderDeptofComputerScience 1986.[23]EricNalisnick AkihiroMatsukawa YeeWhyeTeh DilanGorur andBalajiLakshminarayanan.Dodeepgenerativemodelsknowwhattheydon’tknow?arXivpreprintarXiv:1810.09136 2018.[24]MikołajBi´nkowski DougalJSutherland MichaelArbel andArthurGretton.Demystifyingmmdgans.arXivpreprintarXiv:1801.01401 2018.[25]RichardZhang PhillipIsola AlexeiAEfros EliShechtman andOliverWang.Theunrea-sonableeffectivenessofdeepfeaturesasaperceptualmetric.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition pages586–595 2018.[26]MehdiSMSajjadi OlivierBachem MarioLucic OlivierBousquet andSylvainGelly.Assess-inggenerativemodelsviaprecisionandrecall.InAdvancesinNeuralInformationProcessingSystems pages5228–5237 2018.[27]TuomasKynkäänniemi TeroKarras SamuliLaine JaakkoLehtinen andTimoAila.Improvedprecisionandrecallmetricforassessinggenerativemodels.arXivpreprintarXiv:1904.06991 2019.[28]ShaneBarrattandRishiSharma.Anoteontheinceptionscore.arXivpreprintarXiv:1801.01973 2018.[29]SharonZhou MitchellGordon RanjayKrishna AustinNarcomey DurimMorina andMichaelSBernstein.Hype:Humaneyeperceptualevaluationofgenerativemodels.arXivpreprintarXiv:1904.01121 2019.[30]TakeruMiyatoandMasanoriKoyama.cganswithprojectiondiscriminator.arXivpreprintarXiv:1802.05637 2018.[31]ValentinKhrulkovandIvanOseledets.Geometryscore:Amethodforcomparinggenerativeadversarialnetworks.arXivpreprintarXiv:1802.02664 2018.[32]SanjeevAroraandYiZhang.Dogansactuallylearnthedistribution?anempiricalstudy.arXivpreprintarXiv:1706.08224 2017.[33]ArthurGretton KarstenMBorgwardt MalteJRasch BernhardSchölkopf andAlexanderSmola.Akerneltwo-sampletest.JournalofMachineLearningResearch 13(Mar):723–773 2012.[34]StanislauSemeniuta AliakseiSeveryn andSylvainGelly.Onaccurateevaluationofgansforlanguagegeneration.arXivpreprintarXiv:1806.04936 2018.[35]ZhitingHu ZichaoYang XiaodanLiang RuslanSalakhutdinov andEricPXing.Towardcontrolledgenerationoftext.InProceedingsofthe34thInternationalConferenceonMachineLearning-Volume70 pages1587–1596.JMLR.org 2017.[36]RuishanLiu NicoloFusi andLesterMackey.Modelcompressionwithgenerativeadversarialnetworks.arXivpreprintarXiv:1812.02271 2018.11[37]KaimingHe XRSSJZhang SRen andJSun.Deepresiduallearningforimagerecognition.eprint.arXivpreprintarXiv:0706.1234 2015.[38]TilmannGneitingandAdrianERaftery.Strictlyproperscoringrules prediction andestimation.JournaloftheAmericanStatisticalAssociation 102(477):359–378 2007.[39]PriyaGoyal PiotrDollár RossGirshick PieterNoordhuis LukaszWesolowski AapoKyrola AndrewTulloch YangqingJia andKaimingHe.Accurate largeminibatchsgd:Trainingimagenetin1hour.arXivpreprintarXiv:1706.02677 2017.[40]GeorgOstrovski WillDabney andRémiMunos.Autoregressivequantilenetworksforgenera-tivemodeling.arXivpreprintarXiv:1806.05575 2018.12,Suman Ravuri
Oriol Vinyals