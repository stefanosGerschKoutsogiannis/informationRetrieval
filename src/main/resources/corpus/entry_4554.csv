2018,Randomized Prior Functions for Deep Reinforcement Learning,Dealing with uncertainty is essential for efficient reinforcement learning.
There is a growing literature on uncertainty estimation for deep learning from fixed datasets  but many of the most popular approaches are poorly-suited to sequential decision problems.
Other methods  such as bootstrap sampling  have no mechanism for uncertainty that does not come from the observed data.
We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable `prior' network to each ensemble member.
We prove that this approach is efficient with linear representations  provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.,Randomized Prior Functions

for Deep Reinforcement Learning

Ian Osband
DeepMind

iosband@google.com

John Aslanides

DeepMind

jaslanides@google.com

Albin Cassirer

DeepMind

cassirer@google.com

Abstract

Dealing with uncertainty is essential for ecient reinforcement learning.
There is a growing literature on uncertainty estimation for deep learning
from ﬁxed datasets  but many of the most popular approaches are poorly-
suited to sequential decision problems. Other methods  such as bootstrap
sampling  have no mechanism for uncertainty that does not come from the
observed data. We highlight why this can be a crucial shortcoming and
propose a simple remedy through addition of a randomized untrainable
‘prior’ network to each ensemble member. We prove that this approach
is ecient with linear representations  provide simple illustrations of its
ecacy with nonlinear representations and show that this approach scales
to large-scale problems far better than previous attempts.

1 Introduction

Deep learning methods have emerged as the state of the art approach for many challenging
problems [30  70]. This is due to the statistical ﬂexibility and computational scalability
of large and deep neural networks  which allows them to harness the information in large
and rich datasets. Deep reinforcement learning combines deep learning with sequential
decision making under uncertainty. Here an agent takes actions inside an environment in
order to maximize some cumulative reward [63]. This combination of deep learning with
reinforcement learning (RL) has proved remarkably successful [67  42  60].
At the same time  elementary decision theory shows that the only admissible decision rules
are Bayesian [12  71]. Colloquially  this means that any decision rule that is not Bayesian
can be improved (or even exploited) by some Bayesian alternative [14]. Despite this fact 
the majority of deep learning research has evolved outside of Bayesian (or even statistical)
analysis [55  32]. This disconnect extends to deep RL  where the majority of state of the art
algorithms have no concept of uncertainty [42  41] and can fail spectacularly even in simple
problems where success requires its consideration [40  45].
There is a long history of research in Bayesian neural networks that never quite became
mainstream practice [37  43]. Recently  Bayesian deep learning has experienced a resurgence
of interest with a myriad of approaches for uncertainty quantiﬁcation in ﬁxed datasets and
also sequential decision problems [29  11  20  47]. In this paper we highlight the surprising
fact that many of these well-cited and popular methods for uncertainty estimation in deep
learning can be poor choices for sequential decision problems. We show that this disconnect
is more than a technical detail  but a serious shortcoming that can lead to arbitrarily poor
performance. We support our claims by a series of simple lemmas for simple environments 
together with experimental evidence in more complex settings.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Our approach builds on an alternative method for uncertainty in deep RL inspired by the
statistical bootstrap [15]. This approach trains an ensemble of models  each on perturbed
versions of the data. The resulting distribution of the ensemble is used to approximate
the uncertainty in the estimate [47]. Although typically regarded as a frequentist method 
bootstrapping gives near-optimal convergence rates when used as an approximate Bayesian
posterior [19  18]. However  these ensemble-based approaches to uncertainty quantiﬁcation
approximate a ‘posterior’ without an eective methodology to inject a ‘prior’. This can be a
crucial shortcoming in sequential decision problems.
In this paper  we present a simple modiﬁcation where each member of the ensemble is
initialized together with a random but ﬁxed prior function. Predictions in each ensemble
member are then taken as the sum of the trainable neural network and its prior function.
Learning/optimization is performed so that this sum (network plus prior) minimizes training
loss. Therefore  with sucient network capacity and optimization  the ensemble members
will agree at observed data. However  in regions of the space with little or no training
data  their predictions will be determined by the generalization of their networks and priors.
Surprisingly  we show that this approach is equivalent to exact Bayesian inference for the
special case of Gaussian linear models. Following on from this ‘sanity check’  we present a
series of simple experiments designed to extend this intuition to deep learning. We show
that many of the most popular approaches for uncertainty estimation in deep RL do not
pass these sanity checks  and crystallize these shortcomings in a series of lemmas and small
examples. We demonstrate that our simple modiﬁcation can facilitate aspiration in dicult
tasks where previous approaches for deep RL fail. We believe that this work presents a
simple and practical approach to encoding prior knowledge with deep reinforcement learning.

2 Why do we need a ‘prior’ mechanism for deep RL?
We model the environment as a Markov decision process M = (S A  R  P) [10]. Here S is
the state space and A is the action space. At each time step t  the agent observes state
st œS   takes action at œA   receives reward rt ≥ R(st  at) and transitions to st+1 ≥ P(st  at).
A policy ﬁ : SæA maps states to actions and let Ht denote the history of observations
before time t. An RL algorithm maps Ht to a distribution over policies; we assess its
quality through the cumulative reward over unknown environments. To perform well  an RL
algorithm must learn to optimize its actions  combining both learning and control [63]. A
‘deep’ RL algorithm uses neural networks for nonlinear function approximation [32  42].
The scale and scope of problems that might be approached through deep RL is vast  but
there are three key aspects an ecient (and general) agent must address [63]:

1. Generalization: be able to learn from data it collects.
2. Exploration: prioritize the best experiences to learn from.
3. Long-term consequences: consider external eects beyond a single time step.

In this paper we focus on the importance of some form of ‘prior’ mechanism for ecient
exploration. As a motivating example we consider a sparse reward task where random actions
are very unlikely to ever see a reward. If an agent has never seen a reward then it is essential
that some other form of aspiration  motivation  drive or curiosity direct its learning. We
call this type of drive a ‘prior’ eect  since it does not come from the observed data  but are
ambivalent as to whether this eect is philosophically ‘Bayesian’. Agents that do not have
this prior drive will be left ﬂoundering aimlessly and thus may require exponentially large
amounts of data in order to learn even simple problems [27].
To solve a speciﬁc task  it can be possible to attain superhuman performance without
signiﬁcant prior mechanism [42  41]. However  if our goal is artiﬁcial general intelligence 
then it is disconcerting that our best agents can perform very poorly even in simple problems
[33  39]. One potentially general approach to decision making is given by the Thompson
sampling heuristic1: ‘randomly take action according to the probability you believe it is the
optimal action’ [68]. In recent years there have been several attempts to apply this heuristic

1This heuristic is general in the sense that Thompson sampling can be theoretically justiﬁed in

many of the domains where these other approaches fail [1  48  34  58].

2

to deep reinforcement learning  each attaining signiﬁcant outperformance over deep RL
baselines on certain tasks [20  47  35  11  17]. In this section we outline crucial shortcomings
for the most popular existing approaches to posterior approximation; these outlines will be
brief  but more detail can be found in Appendix C. These shortcomings set the scene for
Section 3  where we introduce a simple and practical alternative that passes each of our
simple sanity checks: bootstrapped ensembles with randomized prior functions. In Section 4
we demonstrate that this approach scales gracefully to complex domains with deep RL.

2.1 Dropout as posterior approximation
One of the most popular modern approaches to regularization in deep learning is dropout
sampling [61]. During training  dropout applies an independent random Bernoulli mask to
the activations and thus guards against excessive co-adaptation of weights. Recent work
has sought to understand dropout through a Bayesian lens  highlighting the connection to
variational inference and arguing that the resultant dropout distribution approximates a
Bayesian posterior [20]. This narrative has proved popular despite the fact that dropout
distribution can be a poor approximation to most reasonable Bayesian posteriors [22  46]:
Lemma 1 (Dropout distribution does not concentrate with observed data).
Consider any loss function L  regularizer R and data D={(x y)}. Let ◊ be parameters of
any neural network architecture f trained with dropout rate pœ(0 1) and dropout masks W 
(1)

EW≥Ber(p) (x y)≥D [L(x  y | ◊  W ) + R(◊)] .

◊úp œ arg min

Then the dropout distribution f◊úp W is invariant to duplicates of the dataset D.
Lemma 1 is somewhat contrived  but highlights a clear shortcoming of dropout as posterior
sampling: the dropout rate does not depend on the data. Lemma 1 means no agent employing
dropout for posterior approximation can tell the dierence between observing a set of data
once and observing it N ∫ 1 times. This can lead to arbitrarily poor decision making  even
when combined with an ecient strategy for exploration [45].

◊

2.2 Variational inference and Bellman error
Dropout as posterior is motivated by its connection to variational inference (VI) [20]  and
recent work to address Lemma 1 improves the quality of this variational approximation
by tuning the dropout rate from data [21].2 However  there is a deeper problem to this
line of research that is common across many works in this ﬁeld: even given access to an
oracle method for exact inference  applying independent inference to the Bellman error does
not propagate uncertainty correctly for the value function as a whole [44]. To estimate
the uncertainty in Q from the Bellman equation Q(st at)=E[rt+1+“max–Q(st+1 –)]
it is
crucial that the two sides of this equation are not independent random variables. Ignoring
this dependence can lead to very bad estimates  even with exact inference.
Lemma 2 (Independent VI on Bellman error does not propagate uncertainty).
Let Y ≥N(µY  ‡2

Y ) be a target value. If we train X≥N(µ ‡2) according to the squared error
(2)

for X  Y independent 

µú ‡ ú œ arg min

µ ‡

E#(X ≠ Y )2$

then the solution µú = µY  ‡ ú = 0 propagates zero uncertainty from Y to X.
To understand the signiﬁcance of Lemma 2  imagine a deterministic system that transitions
from s1 to s2 without reward. Suppose an agent is able to correctly quantify their posterior
uncertainty for the value V (s2)=Y ≥N(µY  ‡2
Y ). Training V (s1)=X according to (2) will
lead to zero uncertainty estimates at s1  when in fact V (s1)≥N(µY  ‡2
Y ). This observation
may appear simplistic  and may not say much more than ‘do not use the squared loss’ for
VI in this setting. However  despite this clear failing (2) is precisely the loss used by the
majority of approaches to VI for RL [17  35  65  69  20]. Note that this failure occurs even
without decision making  function approximation and even when the true posterior lies
within the variational class.

2Concrete dropout assymptotically improves the quality of the variational approximation  but
provides no guarantees on its rate of convergence or error relative to exact Bayesian inference [21].

3

‘Distributional reinforcement learning’

2.3
The key ingredient for a Bayesian formulation for sequential decision making is to consider
beliefs not simply as a point estimate  but as a distribution. Recently an approach called
‘distributional RL’ has shown great success in improving stability and performance in
deep RL benchmark algorithms [8]. Despite the name  these two ideas are quite distinct.
‘Distributional RL’ replaces a scalar estimate for the value function by a distribution that is
trained to minimize a loss against the distribution of data it observes. This distribution of
observed data is an orthogonal concept to that of Bayesian uncertainty.

(a) Posterior beliefs concentrate around p = 0.5.

(b) ‘Distributional’ tends to mass at 0 and 1.

Figure 1: Output distribution after observing n heads and n tails of a coin.

Figure 1 presents an illustration of these two distributions after observing ﬂips of a coin.
As more data is gathered the posterior distribution concentrates around the mean whereas
the ‘distributional RL’ estimate approaches that of the generating Bernoulli. Although
both approaches might reasonably claim a ‘distributional perspective’ on RL  these two
distributions have orthogonal natures and behave quite dierently. Conﬂating one for the
other can lead to arbitrarily poor decisions; it is the uncertainty in beliefs (‘epistemic’)  not
the distributional noise (‘aleatoric’) that is important for exploration [27].

2.4 Count-based uncertainty estimates
Another popular method for incentivizing exploration is with a density model or ‘pseudocount’
[6]. Inspired by the analysis of tabular systems  these models assign a bonus to states and
actions that have been visited infrequently according to a density model. This method can
perform well  but only when the generalization of the density model is aligned with the task
objective. Crucially  this generalization is not learned from the task [53].
Even with an optimal state representation and density  a count-based bonus on states can be
poorly suited for ecient exploration. Consider a linear bandit with reward rt(xt) = xT
t ◊ú+‘t
for some ◊ú œ Rd and ‘t ≥ N(0  1) [56]. Figure 2 compares the uncertainty in the expected
reward E[xT ◊ú] with that obtained by density estimation on the observed xt. A bonus based
upon the state density does not correlate with the uncertainty over the unknown optimal
action. This disconnect can lead to arbitrarily poor decisions [49].

(a) Uncertainty bonus from posterior over xT ◊ú.
Figure 2: Count-based uncertainty leads to a poorly aligned bonus even in a linear system.

(b) Bonus from Gaussian pseudocount p(x).

4

⁄

⁄

.

3 Randomized prior functions for deep ensembles
Section 2 motivates the need for eective uncertainty estimates in deep RL. We note that
crucial failure cases of several popular approaches can arise even with simple linear models.
As a result  we take a moment to review the setting of Bayesian linear regression. Let
i=1 for xi œ Rd and yi = ◊T xi + ‘i with
◊ œ Rd with prior N(◊  ⁄I ) and data D = {(xi  yi)}n
‘i ≥ N(0 ‡ 2) iid. Then  conditioned on D  the posterior for ◊ is Gaussian:
◊4  Cov[◊|D]=3 1

E[◊|D]=3 1

I4≠13 1

‡2 X T X+ 1

‡2 X T X+ 1

‡2 X T y+ 1

I4≠1

(3)

⁄

Equation (3) relies on Gaussian conjugacy and linear models  which cannot easily be extended
to deep neural networks. The following result shows that we can replace this analytical result
with a simple computational procedure.
Lemma 3 (Computational generation of posterior samples).
Let f◊(x) = xT ◊  ˜yi ≥ N(yi ‡ 2) and ˜◊ ≥ N(◊  ⁄I ). Then the either of the following
optimization problems generate a sample ◊ | D according to (3):
⁄ Î˜◊ ≠ ◊Î2
2 
2 + ‡2
⁄ Î◊Î2
2.

2 + ‡2
nÿi=1 Î˜yi ≠ f◊(xi)Î2
nÿi=1 Î˜yi ≠ (f˜◊ + f◊) (xi)Î2

Proof. To prove (4) note that the solution is Gaussian and then match moments; equation
(5) then follows by relabeling [49].

˜◊ + arg min

arg min

(5)

(4)

◊

◊

Lemma 3 is revealing since it allows us to view Bayesian regression through a purely
computational lens: ‘generate posterior samples by training on noisy versions of the data 
together with some random regularization’. Even for nonlinear f◊  we can still compute (4)
or (5). Although the resultant f◊ will no longer be an exact posterior  at least it passes the
‘sanity check’ in this simple linear setting (unlike the approaches of Section 2). We argue
this method is quite intuitive: the perturbed data ˜D = {(xi  ˜yi)}n
i=1 is generated according
to the estimated noise process ‘t and the sample ˜◊ is drawn from prior beliefs. Intuitively (4)
says to ﬁt to ˜D and regularize weights to a prior sample of weights ˜◊; (5) says to generate a
prior function f˜◊ and then ﬁt an additive term to noisy data ˜D with regularized complexity.
This paper explores the performance of each of these methods for uncertainty estimation
with deep learning. We ﬁnd empirical support that method (5) coupled with a randomized
prior function signiﬁcantly outperforms ensemble-based approaches without prior mechanism.
We also ﬁnd that (5) signiﬁcantly outperforms (4) in deep RL. We suggest a major factor in
this comes down to the huge dimensionality of neural network weights  whereas the output
policy or value is typically far smaller. In this case  it makes sense to enforce prior beliefs in
the low dimensional space. Further  the initialization of neural network weights plays an
important role in their generalization properties and optimization via stochastic gradient
descent (SGD) [23  38]. As such  (5) may help to decouple the dual roles of initial weights as
both ‘prior’ and training initializer. Algorithm 1 describes our approach applied to modern
deep learning architectures.

Ensemble size KœN  noise procedure data_noise  distribution over priors P™{ P(p)|p:XæY} .

Algorithm 1 Randomized prior functions for ensemble posterior.
Require: Data D™{ (x y)|xœX  yœY}  loss function L  neural model f◊:XæY  
1: for k = 1  ..  K do
2:
3:
4:
5:
6: return ensemble {f◊k + pk}K

initialize ◊k ≥ Glorot initialization [23].
form Dk = data_noise(D) (e.g. Gaussian noise or bootstrap sampling [50]).
sample prior function pk ≥P .
optimize Ò◊|◊=◊kL(f◊ + pk;Dk) via ADAM [28].
k=1.

5

L“(◊; ◊≠  p D) :=ÿtœD

Qcart + “ max

aÕœA

˝¸
˙
(f◊≠ + p)(sÕt  aÕ) ≠

˚

online Q

(f◊ + p)(st  at)Rdb
˙ ˝¸ ˚

.

(6)

4 Deep reinforcement learning
Algorithm 1 might be applied to model or policy learning approaches  but this paper focuses
on value learning. We apply Algorithm 1 to deep Q networks (DQN) [42] on a series of tasks
designed to require good uncertainty estimates. We train an ensemble of K networks {Qk}K
k=1
in parallel  each on a perturbed version of the observed data Ht and each with a distinct
random  but ﬁxed  prior function pk. Each episode  the agent selects j ≥ Unif([1  ..  K])
and follows the greedy policy w.r.t. Qj for the duration of the episode. This algorithm
is essentially bootstrapped DQN (BootDQN) except for the addition of the prior function
pk [47]. We use the statistical bootstrap rather than Gaussian noise (5) to implement a
state-speciﬁc variance [19].
Let “ œ [0  1] be a discount factor that induces a time preference over future rewards. For
a neural network family f◊  prior function p  and data D = {(st  at  rt  sÕt) we deﬁne the
“-discounted empirical temporal dierence (TD) loss 
target Q

2

Using this notation  the learning update for BootDQN with prior functions is a simple
application of Algorithm 1  which we outline below. To complete the RL algorithm we
implement a 50-50 ensemble_buffer  where each transition has a 50% chance of being
included in the replay for model k = 1  ..  K. For a complete description of BootDQN+prior
agent  see Appendix A.

Algorithm 2 learn_bootstrapped_dqn_with_prior
Agent:

trainable network parameters
◊1  .. ◊ K
ﬁxed prior functions
p1  ..  pK
L“(◊=· ; ◊≠=·   p=·  D=· ) TD error loss function
ensemble_buffer
◊1  .. ◊ K

replay buer of K-parallel perturbed data
agent value function estimate

Updates:
1: for k in (1  . . .   K) do
2:
3:

Data Dk Ω ensemble_buffer[k].sample_minibatch()
optimize Ò◊|◊=◊kL(◊; ◊k  pk Dk) via ADAM [28].

4.1 Does BootDQN+prior address the shortcomings from Section 2?
Algorithm 2 is a simple modiﬁcation of vanilla Q-learning: rather than maintain a single
point estimate for Q  we maintain K estimates in parallel  and rather than regularize each
estimate to a single value  each is individually regularized to a distinct random prior function.
We show that that this simple and scalable algorithm overcomes the crucial shortcomings
that aict existing methods  as outlined in Section 2.
X Posterior concentration (Section 2.1): Prior function + noisy data means the ensemble
is initially diverse  but concentrates as more data is gathered. For linear-gaussian systems
this matches Bayes posterior  bootstrap oers a general  non-parametric approach [16  18].
X Multi-step uncertainty (Section 2.2): Since each network k trains only on its own
target value  BootDQN+prior propagates a temporally-consistent sample of Q-value [49].
X Epistemic vs aleatoric (Section 2.3): BootDQN+prior optimises the mean TD loss (6)
and does not seek to ﬁt the noise in returns  unlike ‘distributional RL’ [7].
X Task-appropriate generalization (Section 2.4): We explore according to our uncer-
tainty in the value Q  rather than density on state. As such  our generalization naturally
occurs in the space of features relevant to the task  rather than pixels or noise [6].
X Intrinsic motivation (comparison to BootDQN without prior): In an environment with
zero rewards  a bootstrap ensemble may simply learn to predict zero for all states. The prior
pk can make this generalization unlikely for Qk at unseen states ˜s so E[max–Qk(˜s –)]>0;
thus BootDQN+prior seeks novelty even with no observed rewards.

Another source of justiﬁcation comes from the observation that BootDQN+prior is an
instance of randomized least-squares value iteration (RLSVI)  with regularization via ‘prior

6

function’ for an ensemble of neural networks. RLSVI with linear function approximation
and Gaussian noise guarantees a bound on expected regret of ˜O(|S||A|T) in the tabular
setting [49].3 Similarly  analysis for the bandit setting establishes that K = ˜O(|A|) models
trained online can attain similar performance to full resampling each episode [36]. Our work
in this paper pushes beyond the boundaries of these analyses  which are presented as ‘sanity
checks’ that our algorithm is at least sensible in simple settings  rather than a certiﬁcate
of correctness for more complex ones. The rest of this paper is dedicated to an empirical
investigation of our algorithm through computational experiments. Encouragingly  we ﬁnd
that many of the insights born out of simple problems extend to more complex ‘deep RL’
settings and good evidence for the ecacy of our algorithm.
4.2 Computational experiments
Our experiments focus on a series of environments that require deep exploration together
with increasing state complexity [27  49]. In each of our domains  random actions are very
unlikely to achieve a reward and exploratory actions may even come at a cost. Any algorithm
without prior motivation will have no option but to explore randomly  or worse  eschew
exploratory actions completely in the name of premature and sub-optimal exploitation. In
our experiments we focus on a tabula rasa setting in which the prior function is drawn as
a random neural network. Although our prior distribution P could encode task-speciﬁc
knowledge (e.g. through sampling the true Qú)  we leave this investigation to future work.
4.2.1 Chain environments
We begin our experiments with a family of chain-like environments that highlight the need
for deep exploration [62]. The environments are indexed by problem scale NœN and action
mask W ≥Ber(0.5)N◊N  with S={0 1}N◊N and A={0 1}. The agent begins each episode in
the upper left-most state in the grid and deterministically falls one row per time step. The
state encodes the agent’s row and column as a one-hot vector stœS. The actions {0 1} move
the agent left or right depending on the action mask W at state st  which remains ﬁxed.
The agent incurs a cost of 0.01/N for moving right in all states except for the right-most  in
which the reward is 1. The reward for action left is always zero. An episode ends after N
time steps so that the optimal policy is to move right each step and receive a total return of
0.99; all other policies receive zero or negative return. Crucially  algorithms without deep
exploration take (2N) episodes to learn the optimal policy [52].4

Figure 3: Only bootstrap with additive prior network (BSP) scales gracefully to large problems.
Plotting BSP on a log-log scale suggests an empirical scaling Tlearn = ˜O(N 3); see Figure 8.
Figure 3 presents the average time to learn for N = 5  ..  60 up to 500K episodes over 5
seeds and ensemble K = 20. We say that an agent has learned the optimal policy when
the average regret per episode drops below 0.9. We compare three variants of BootDQN 
depending on their mechanism for ‘prior’ eects. BS is bootstrap without prior mechanism.
BSR is bootstrap with l2 regularization on weights per (4). BSP is bootstrap with additive
prior function per (5). In each case we initialize a random 20-unit MLP; BSR regularizes to
these initial weights and BSP trains an additive network. Although all bootstrap methods
signiﬁcantly outperform ‘-greedy  only BSP successfully scales to large problem sizes.
Figure 4 presents a more detailed analysis of the sensitivity of our approach to the tuning
parameters of dierent regularization approaches. We repeat the experiments of Figure 3
3Regret measures the shortfall in cumulative rewards compared to that of the optimal policy.
4The dashed lines indicate the 2N dithering lower bound. The action mask W means this cannot
be solved easily by evolution or policy search evolution  unlike previous ‘chain’ examples [47  54].

7

and examine the size of the largest problem solved before 50K episodes. In each case larger
ensembles lead to better performance  but this eect tends to plateau relatively early. Figure
4a shows that regularization provides little or no beneﬁt to BSR. Figure 4b examines the
eect of scaling the randomly initialized MLP by a scalar hyperparameter —.

(a) l2 regularization has a very limited eect.
(b) Additive prior greatly improves performance.
Figure 4: Comparing eects of dierent styles of prior regularization in Bootstrapped DQN.
How does BSP solve this exponentially-dicult problem?
At ﬁrst glance this ‘chain’ problem may seem like an impossible task. Finding the single
rewarding policy out of 2N is not simply a needle-in-a-haystack  but more akin to looking for
a piece of hay in a needle-stack! Since every policy apart from the rewarding one is painful 
it’s very tempting for an agent to give up and receive reward zero. We now provide some
intuition for how BSP is able to consistently and scalably solve such a dicult task.
One way to interpret this result is through analysing BSP with linear function approximation
via Lemma 3. As outlined in Section 4.1  BSP with linear function approximation satisﬁes a
polynomial regret bound [49]. Further  this empirical scaling matches that predicted by the
regret bound tabular domain [51] (see Figure 8). Here  the prior function plays a crucial
role - it provides motivation for the agent to explore even when the observed data has low
(or no) reward. Note that it is not necessary the sampled prior function leads to a good
policy itself; in fact this is exponentially unlikely according to our initialization scheme. The
crucial element is that when a new state sÕ is reached there is some ensemble member that
estimates maxaÕ Qk(sÕ  aÕ) is suciently positive to warrant visiting  even if it causes some
negative reward along the way. In that case  when network k is active it will seek out the
potentially-informative sÕ even if it is multiple timesteps away; this eect is sometimes called
deep exploration. We present an accompanying visualization at http://bit.ly/rpf_nips.
However  this connection to linear RLSVI does not inform why BSP should outperform BSR.
To account for this  we appeal to the functional dynamics of deep learning architectures (see
Section 3). In large networks weight decay (per BSR) may be an ineective mechanism on
the output Q-values. Instead  training an additive network via SGD (per BSP) may provide a
more eective regularization on the output function [73  38  5]. We expand on this hypothesis
and further details of these experiments in Appendix B.1. This includes investigation of
NoisyNets [17] and dropout [20]  which both perform poorly  and a comparison to UCB-based
algorithms  which scale much worse than BSP  even with oracle access to state visit counts.
4.2.2 Cartpole swing-up
The experiments of Section 4.2.1 show that the choice of prior mechanism can be absolutely
essential for ecient exploration via randomized value functions. However  since the under-
lying system is a small ﬁnite MDP we might observe similar performance through a tabular
algorithm. In this section we investigate a classic benchmark problem that necessitates
nonlinear function approximation: cartpole [63]. We modify the classic formulation so
that the pole begins hanging down and the agent only receives a reward when the pole
is upright  balanced  and centered5. We also add a cost of 0.1 for moving the cart. This
problem embodies many of the same aspects of 4.2.1  but since the agent interacts with the
environment through state st=(cos(◊t) sin(◊t)  ˙◊t xt  ˙xt)  the agent must also learn nonlinear
generalization. Tabular approaches are not practical due to the curse of dimensionality.

5We use the DeepMind control suite [66] with reward +1 only when cos(◊)>0.95  |x|<0.1  | ˙◊|<1 

and | ˙x|<1. Each episode lasts 1 000 time steps  simulating 10 seconds of interaction.

8

(a) Only BSP learns a performant policy.

(b) Inspecting the ﬁrst 500 episodes.

Figure 5: Learning curves for the modiﬁed cartpole swing-up task.

Figure 5 compares the performance of DQN with ‘-greedy  bootstrap without prior (BS) 
bootstrap with prior networks (BSP) and the state-of-the-art continuous control algorithm
D4PG  itself an application of ‘distributional RL’ [4]. Only BSP learns a performant policy;
no other approach ever attains any positive reward. We push experimental details  including
hyperparameter analysis  to Appendix B.2. These results are signiﬁcant in that they show
that our intuitions translate from simple domains to more complex nonlinear settings 
although the underlying state is relatively low dimensional. Our next experiments investigate
performance in a high dimensional and visually rich domain.

4.2.3 Montezuma’s revenge
Our ﬁnal experiment comes from the Arcade Learning Environment and the canonical
sparse reward game  Montezuma’s Revenge [9]. The agent interacts directly with the pixel
values and  even under an optimal policy  there can be hundreds of time steps between
rewarding actions. This problem presents a signiﬁcant exploration challenge in a visually rich
environment; many published algorithms are essentially unable to attain any reward here
[42  41]. We compare performance against a baseline distributed DQN agent with double
Q-learning  prioritized experience replay and dueling networks [25  24  59  72]. To save
computation we follow previous work and use a shared convnet for the ensemble uncertainty
[47  3]. Figure 6 presents the results for varying prior scale — averaged over three seeds. Once
again  we see that the prior network can be absolutely critical to successful exploration.

Figure 6: The prior network qualitatively changes behavior on Montezuma’s revenge.

5 Conclusion
This paper highlights the importance of uncertainty estimates in deep RL  the need for
an eective ‘prior’ mechanism  and its potential beneﬁts towards ecient exploration. We
present some alarming shortcomings of existing methods and suggest bootstrapped ensembles
with randomized prior functions as a simple  practical alternative. We support our claims
through an analysis of this method in the linear setting  together with a series of simple
experiments designed to highlight the key issues. Our work leaves several open questions.
What kinds of prior functions are appropriate for deep RL? Can they be optimized or
‘meta-learned’? Can we distill the ensemble process to a single network? We hope this work
helps to inspire solutions to these problems  and also build connections between the theory
of ecient learning and practical algorithms for deep reinforcement learning.

9

Acknowledgements
We would like to thank many people who made important contributions to this paper.
This paper can be thought of as a speciﬁc type of ‘deep exploration via randomized value
functions’  whose line of research has been crucially driven by the contributions of (and
conversations with) Benjamin Van Roy  Daniel Russo and Zheng Wen. Further  we would
like to acknowledge the many helpful comments and support from Mohammad Gheshlaghi
Azar  David Budden  David Silver and Justin Sirignano. Finally  we would like to make
a special mention for Hado Van Hasselt  who coined the term ‘hay in a needle-stack’ to
describe our experiments from Section 4.2.1.

References
[1] Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit

problem. In Conference on Learning Theory  pages 39–1  2012.

[2] Shipra Agrawal and Navin Goyal. Further optimal regret bounds for Thompson sampling. In

Artiﬁcial Intelligence and Statistics  pages 99–107  2013.

[3] Kamyar Azizzadenesheli  Emma Brunskill  and Animashree Anandkumar. Ecient exploration

through bayesian deep q-networks. arXiv preprint arXiv:1802.04412  2018.

[4] Gabriel Barth-Maron  Matthew W Homan  David Budden  Will Dabney  Dan Horgan  Alistair
Muldal  Nicolas Heess  and Timothy Lillicrap. Distributed distributional deterministic policy
gradients. arXiv preprint arXiv:1804.08617  2018.

[5] Peter L Bartlett  Dylan J Foster  and Matus J Telgarsky. Spectrally-normalized margin bounds
for neural networks. In Advances in Neural Information Processing Systems 30  pages 6241–6250 
2017.

[6] Marc Bellemare  Sriram Srinivasan  Georg Ostrovski  Tom Schaul  David Saxton  and Remi
Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural
Information Processing Systems 29  pages 1471–1479. 2016.

[7] Marc G Bellemare  Will Dabney  and Rémi Munos. A Distributional Perspective on Reinforce-
ment Learning. Proceedings of the 34th International Conference on Machine Learning (ICML) 
2017.

[8] Marc G Bellemare  Will Dabney  and Rémi Munos. A distributional perspective on reinforcement

learning. In International Conference on Machine Learning  pages 449–458  2017.

[9] Marc G Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The arcade learning
environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR)  47:253–279 
2013.

[10] Dimitri P. Bertsekas and John Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc 

September 1996.

[11] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra. Weight uncertainty

in neural networks. arXiv preprint arXiv:1505.05424  2015.

[12] David Roxbee Cox and David Victor Hinkley. Theoretical statistics. CRC Press  1979.
[13] Will Dabney  Mark Rowland  Marc G Bellemare  and Rémi Munos. Distributional reinforce-
ment learning with quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence  2018.

[14] Bruno De Finetti. La prévision: ses lois logiques  ses sources subjectives. In Annales de l’institut

Henri Poincaré  volume 7  pages 1–68  1937.

[15] Bradley Efron. The jackknife  the bootstrap and other resampling plans  volume 38. SIAM 

1982.

[16] Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press  1994.
[17] Meire Fortunato  Mohammad Gheshlaghi Azar  Bilal Piot  Jacob Menick  Ian Osband  Alex
Graves  Vlad Mnih  Remi Munos  Demis Hassabis  Olivier Pietquin  et al. Noisy networks for
exploration. In Proc. of ICLR  2018.

[18] Tadayoshi Fushiki. Bootstrap prediction and bayesian prediction under misspeciﬁed models.

Bernoulli  pages 747–758  2005.

[19] Tadayoshi Fushiki  Fumiyasu Komaki  Kazuyuki Aihara  et al. Nonparametric bootstrap

prediction. Bernoulli  11(2):293–307  2005.

[20] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model

uncertainty in deep learning. In International Conference on Machine Learning  2016.

10

[21] Yarin Gal  Jiri Hron  and Alex Kendall. Concrete dropout. In Advances in Neural Information

Processing Systems  pages 3584–3593  2017.

[22] Yarin Gal  Rowan McAllister  and Carl Edward Rasmussen. Improving pilco with bayesian
neural network dynamics models. In Data-Ecient Machine Learning workshop  ICML  2016.
[23] Xavier Glorot and Yoshua Bengio. Understanding the diculty of training deep feedforward
neural networks. In Proceedings of the 13th international conference on artiﬁcial intelligence
and statistics  pages 249–256  2010.

[24] Hado van Hasselt  Arthur Guez  and David Silver. Deep reinforcement learning with double
q-learning. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence  AAAI’16 
pages 2094–2100. AAAI Press  2016.

[25] Daniel Horgan  John Quan  David Budden  Gabriel Barth-Maron  Matteo Hessel  Hado Van
Hasselt  and David Silver. Distributed prioritized experience replay. In 6th International
Conference on Learning Represenations  2018.

[26] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11(Apr):1563–1600  2010.

[27] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine

Learning  49  2002.

[28] Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. Proceedings

of the International Conference on Learning Representations  2015.

[29] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. International Conference

on Learning Representations  2014.

[30] Alex Krizhevsky  Ilya Sutskever  and Georey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in Neural Information Processing Systems 25  pages
1097–1105  2012.

[31] Balaji Lakshminarayanan  Alexander Pritzel  and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems  pages 6405–6416  2017.

[32] Yann LeCun  Yoshua Bengio  and Georey Hinton. Deep learning. Nature  521(7553):436  2015.
[33] Shane Legg  Marcus Hutter  et al. A collection of deﬁnitions of intelligence. Frontiers in

Artiﬁcial Intelligence and applications  157:17  2007.

[34] Jan Leike  Tor Lattimore  Laurent Orseau  and Marcus Hutter. Thompson sampling is
asymptotically optimal in general environments. Uncertainty in Artiﬁcial Intelligence  2016.
[35] Zachary C Lipton  Jianfeng Gao  Lihong Li  Xiujun Li  Faisal Ahmed  and Li Deng. Ecient
exploration for dialogue policy learning with bbq networks & replay buer spiking. arXiv
preprint arXiv:1608.05081  2016.

[36] Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. In Advances in Neural Information

Processing Systems  pages 3260–3268  2017.

[37] David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural

computation  4(3):448–472  1992.

[38] Hartmut Maennel  Olivier Bousquet  and Sylvain Gelly. Gradient descent quantizes ReLU

network features. arXiv preprint arXiv:1803.08367  2018.

[39] Horia Mania  Aurelia Guy  and Benjamin Recht. Simple random search provides a competitive

approach to reinforcement learning. arXiv preprint arXiv:1803.07055  2018.

[40] Oliver Mihatsch and Ralph Neuneier. Risk-sensitive reinforcement learning. Machine learning 

49(2-3):267–290  2002.

[41] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lilli-
crap  Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. In Proc. of ICML  2016.

[42] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529–533  2015.
[43] Radford M Neal. Bayesian learning for neural networks  volume 118. Springer Science &

Business Media  2012.

[44] Brendan O’Donoghue  Ian Osband  Remi Munos  and Volodymyr Mnih. The uncertainty

bellman equation and exploration. arXiv preprint arXiv:1709.05380  2017.

11

[45] Ian Osband. Deep Exploration via Randomized Value Functions. PhD thesis  Stanford University 

2016.

[46] Ian Osband. Risk versus uncertainty in deep learning: Bayes  bootstrap and the dangers of

dropout. 2016.

[47] Ian Osband  Charles Blundell  Alexander Pritzel  and Benjamin Van Roy. Deep exploration
via bootstrapped DQN. In Advances In Neural Information Processing Systems 29  pages
4026–4034  2016.

[48] Ian Osband  Daniel Russo  and Benjamin Van Roy. (More) ecient reinforcement learning via
posterior sampling. In Advances in Neural Information Processing Systems 26  pages 3003–3011.
2013.

[49] Ian Osband  Daniel Russo  Zheng Wen  and Benjamin Van Roy. Deep exploration via randomized

value functions. arXiv preprint arXiv:1703.07608  2017.

[50] Ian Osband and Benjamin Van Roy. Bootstrapped Thompson sampling and deep exploration.

arXiv preprint arXiv:1507.00300  2015.

[51] Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for
In Proceedings of the 34th International Conference on Machine

reinforcement learning?
Learning  pages 2701–2710  2017.

[52] Ian Osband  Benjamin Van Roy  and Zheng Wen. Generalization and exploration via randomized
value functions. In Proceedings of The 33rd International Conference on Machine Learning 
pages 2377–2386  2016.

[53] Georg Ostrovski  Marc G Bellemare  Aaron van den Oord  and Rémi Munos. Count-based

exploration with neural density models. In Proc. of ICML  2017.

[54] Matthias Plappert  Rein Houthooft  Prafulla Dhariwal  Szymon Sidor  Richard Y Chen  Xi Chen 
Tamim Asfour  Pieter Abbeel  and Marcin Andrychowicz. Parameter space noise for exploration.
arXiv preprint arXiv:1706.01905  2017.

[55] David E Rumelhart  Georey E Hinton  and Ronald J Williams. Learning internal representa-

tions by error propagation. Technical report  DTIC Document  1985.

[56] Paat Rusmevichientong and John N. Tsitsiklis. Linearly parameterized bandits. Math. Oper.

Res.  35(2):395–411  2010.

[57] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics

of Operations Research  39(4):1221–1243  2014.

[58] Daniel Russo  Benjamin Van Roy  Abbas Kazerouni  and Ian Osband. A tutorial on Thompson

sampling. arXiv preprint arXiv:1707.02038  2017.

[59] Tom Schaul  John Quan  Ioannis Antonoglou  and David Silver. Prioritized experience replay.

CoRR  abs/1511.05952  2015.

[60] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driess-
che  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  et al. Mas-
tering the game of go with deep neural networks and tree search. Nature  529(7587):484–489 
2016.

[61] Nitish Srivastava  Georey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine
Learning Research  15(1):1929–1958  2014.

[62] Malcolm Strens. A Bayesian framework for reinforcement learning. In International Conference

on Machine Learning  pages 943–950  2000.

[63] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press 

2017.

[64] Richard S Sutton  Joseph Modayil  Michael Delp  Thomas Degris  Patrick M Pilarski  Adam
White  and Doina Precup. Horde: A scalable real-time architecture for learning knowledge
from unsupervised sensorimotor interaction. In The 10th International Conference on Au-
tonomous Agents and Multiagent Systems-Volume 2  pages 761–768. International Foundation
for Autonomous Agents and Multiagent Systems  2011.

[65] Yunhao Tang and Alp Kucukelbir. Variational deep q network. arXiv preprint arXiv:1711.11225 

2017.

[66] Yuval Tassa  Yotam Doron  Alistair Muldal  Tom Erez  Yazhe Li  Diego de Las Casas  David
Budden  Abbas Abdolmaleki  Josh Merel  Andrew Lefrancq  et al. Deepmind control suite.
arXiv preprint arXiv:1801.00690  2018.

12

[67] Gerald Tesauro. Temporal dierence learning and TD-gammon. Communications of the ACM 

38(3):58–68  1995.

[68] William R Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika  25(3/4):285–294  1933.

[69] Ahmed Touati  Harsh Satija  Joshua Romo  Joelle Pineau  and Pascal Vincent. Randomized
value functions via multiplicative normalizing ﬂows. arXiv preprint arXiv:1806.02315  2018.
[70] Aaron Van Den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex
Graves  Nal Kalchbrenner  Andrew Senior  and Koray Kavukcuoglu. Wavenet: A generative
model for raw audio. arXiv preprint arXiv:1609.03499  2016.

[71] Abraham Wald. Statistical decision functions. In Breakthroughs in Statistics  pages 342–357.

Springer  1992.

[72] Ziyu Wang  Nando de Freitas  and Marc Lanctot. Dueling network architectures for deep

reinforcement learning. CoRR  abs/1511.06581  2015.

[73] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. CoRR  abs/1611.03530  2016.

13

,Ian Osband
John Aslanides
Albin Cassirer