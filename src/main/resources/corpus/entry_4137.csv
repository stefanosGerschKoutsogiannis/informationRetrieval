2014,Optimal prior-dependent neural population codes under shared input noise,The brain uses population codes to form distributed  noise-tolerant representations of sensory and motor variables. Recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain. However  the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix. Here we analyze population coding under a simple alternative model in which latent input noise" corrupts the stimulus before it is encoded by the population. This provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons  and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations. We examine prior-dependent  Bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately Gaussian. These analyses extend previous results on independent Poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information. We show that  for homogeneous populations that tile the input domain  optimal tuning curve width depends on the prior  the loss function  the resource constraint  and the amount of input noise. This framework provides a practical testbed for examining issues of optimality  noise  correlation  and coding fidelity in realistic neural populations.",Optimal prior-dependent neural population codes

under shared input noise

Agnieszka Grabska-Barwi´nska

Gatsby Computational Neuroscience Unit

University College London

agnieszka@gatsby.ucl.ac.uk

Jonathan W. Pillow

Princeton Neuroscience Institute

Department of Psychology

Princeton University

pillow@princeton.edu

Abstract

The brain uses population codes to form distributed  noise-tolerant representa-
tions of sensory and motor variables. Recent work has examined the theoretical
optimality of such codes in order to gain insight into the principles governing
population codes found in the brain. However  the majority of the population
coding literature considers either conditionally independent neurons or neurons
with noise governed by a stimulus-independent covariance matrix. Here we an-
alyze population coding under a simple alternative model in which latent “input
noise” corrupts the stimulus before it is encoded by the population. This provides
a convenient and tractable description for irreducible uncertainty that cannot be
overcome by adding neurons  and induces stimulus-dependent correlations that
mimic certain aspects of the correlations observed in real populations. We ex-
amine prior-dependent  Bayesian optimal coding in such populations using exact
analyses of cases in which the posterior is approximately Gaussian. These anal-
yses extend previous results on independent Poisson population codes and yield
an analytic expression for squared loss and a tight upper bound for mutual infor-
mation. We show that  for homogeneous populations that tile the input domain 
optimal tuning curve width depends on the prior  the loss function  the resource
constraint  and the amount of input noise. This framework provides a practical
testbed for examining issues of optimality  tuning width  noise  and correlations
in realistic neural populations.

1

Introduction

A substantial body of work has examined the optimality of neural population codes [1–18]. How-
ever  the classical literature has focused predominantly on codes with independent Poisson neurons
and on analyses of unbiased decoders using Fisher information. Real neurons  by contrast  ex-
hibit noise correlations (dependencies not introduced by the stimulus)  and Fisher information does
not accurately quantify information when performance is biased or close to threshold [6  14  17].
Moreover  the classical population codes with independent Poisson noise predict unreasonably good
performance with even a small number of neurons. A variety of studies have shown cases where
a small number of independent neurons can outperform an entire animal [19  20]. For example 
a population of only 220 Poisson neurons with tuning width of 60 deg (full width at half height)
and tuning amplitude of 10 spikes can match the human orientation discrimination threshold of ⇡ 1
deg. (See Supplement S1 for derivation.) Even fewer neurons would be required if tuning curve
amplitude were higher.
The mismatch between this predicted efﬁciency and animals’ actual behaviour has been attributed
to the presence of information-limiting correlations between neurons [21  22]. However  deviation

1

input noise

tuning curves

Poisson
noise

t

n
u
o
c
 

e
k
p
s

i

population response

posterior

likelihood

)
s
u
u
m

l

i
t
s
(
p

stimulus

preferred stimulus

stimulus

stimulus prior

)
s
u
u
m

l

i
t
s
(
p

stimulus

+

t

n
u
o
c
 

e
k
p
s

i

0

Figure 1: Bayesian formulation of neural population coding with input noise.

from independence renders most analytical treatments infeasible  necessitating the use of numerical
methods (Monte Carlo simulations) for quantifying the performance of such codes [6  14].
Here we examine a family of population codes for which the posterior is approximately Gaussian 
which makes it feasible to perform a variety of analytical treatments. In particular  we consider a
population with Gaussian tuning curves that “tile” the input domain  and Gaussian stimulus priors.
This yields a Gaussian-shaped likelihood and a Gaussian posterior with variance that depends only
on the total spike count [2  15]. We use this formulation to derive tractable expressions for neuro-
metric functions such as mean squared error (MSE) and mutual information (MI)  and to analyze
optimality without resorting to Fisher information  which can be inaccurate for short time windows
or small spike counts [6  14  17]. Secondly  we extend this framework to incorporate shared “input
noise” in the stimulus variable of interest (See Fig. 1). This form of noise differs from many existing
models  which assume noise to arise from shared connectivity  but with no direct relationship to the
stimulus coding [5  14  17  23]. (See [15  24] for related approaches).
This paper is organised as follows. In Sec. 2  we describe an idealized Poisson population code with
tractable posteriors  and review classical results based on Fisher Information. In Sec. 3  we provide
a Bayesian treatment of these codes  deriving expressions for mean squared error (MSE) and mutual
information (MI). In Sec. 4  we extend these analyses to a population with input noise. Finally  in
Sec. 5  we examine the patterns of pairwise dependencies introduced by input noise.

2

Independent Poisson population codes

Consider an idealized population of Poisson neurons that encode a scalar stimulus s with Gaussian-
shaped tuning curves. Under this (classical) model  the response vector r = (r1  . . . rN )> is condi-
tionally Poisson distributed:

ri|s ⇠ Poiss(fi(s)) 

p(r|s) =

ri! fi(s)riefi(s) 

with equally-spaced centers or “preferred stimuli” ?s = ( ?s 1  . . . ?s N )  tuning width t  amplitude
A  and time window for counting spikes ⌧. We assume that the tuning curves “tile”  i.e.  sum to a
constant over the relevant stimulus range:

NXi=1

fi(s) ⇡ .

(tiling property) (3)

We can determine  by integrating the summed tuning curves (eq. 3) over the stimulus space  giving

i=1 fi(s) = N Ap2⇡t = S  which gives:

R dsPN
(expected total spike count) (4)
where  = S/N is the spacing between tuning curve centers  and a = p2⇡A⌧ is an “amplitude
constant” that depends on true tuning curve amplitude and the time window for integrating spikes.

 = at/ 

2

where tuning curves fi(s) take the form

fi(s) = ⌧ A exp⇣ 1

22
t

1

NYi=1
(s  ?s i)2⌘  

(Poisson encoding) (1)

(tuning curves) (2)

Note  that tiling holds almost perfectly if tuning curves are broad compared to their spacing (e.g.
t > ). However  our results hold for a much broader range of t (see Supplementary Figs S3 and
S4 for a numerical analysis.)

Let R =P ri denote the total spike count from the entire population. Due to tiling  R is a Poisson
random variable with rate   regardless of the stimulus: p(R|s) = 1
For simplicity  we will consider stimuli drawn from a zero-mean Gaussian prior with variance 2
s:
(stimulus prior) (5)

R! Re.

e s2
22
s .

SinceQi efi(s) = e due to the tiling assumption  the likelihood (eq. 1 as a function of s) and

posterior both take Gaussian forms:

p(s) = 1p2⇡s

s ⇠ N (0  2
s ) 
p(r|s) /Yi

(likelihood) (6)

(posterior) (7)

R r> ?s  1

fi(s)ri / N 1
p(s|r) = N⇣ r> ?s

R + ⇢

R 2

t
R + ⇢⌘ 

2
t

 

t /2

s denotes the ratio of the tuning curve variance to prior variance. The maximum of
where ⇢ = 2
the likelihood (eq. 6) is the so-called center-of-mass estimator estimator  1
R r> ?s  while the mean of
the posteror (eq. 7) is biased toward zero by an amount that depends on ⇢. Note that the posterior
variance does not depend on which neurons emitted spikes  only the total spike count R  a fact that
will be important for our analyses below.

2.1 Capacity constraints for deﬁning optimality

Deﬁning optimality for a population code requires some form of constraint on the capacity of the
neural population  since clearly we can achieve arbitrarily narrow posteriors if we allow arbitrarily
large total spike count R. In the following  we will consider two different biologically plausible
constraints:

• An amplitude constraint  in which we constrain the tuning curve amplitude. Under this
constraint  expected spike count  will grow as the tuning width t increases (see eq. 4) 
since more neurons will respond to any stimulus when tuning is broader.

• An energy constraint  in which we ﬁx  while allowing t and amplitude A to vary. Here 
we can make tuning curves wider so that more neurons respond  but must reduce the am-
plitude so that total expected spike count remains ﬁxed.

We will show that the optimal tuning depends strongly on which kind of constraint we apply.

2.2 Analyses based on Fisher Information

NXi=1

NXi=1

⌘ =

exp⇣

The Fisher information provides a popular  tractable metric for quantifying the efﬁciency of a neural
code  given by E[ @2
@s2 log p(r|s)]  where expectation is taken with respect to encoding distribution
p(r|s). For our idealized Poisson population  the total Fisher information is:
f0i(s)2

=
2
fi(s)
t

(Fisher info) (8)

(s  ?s i)2

(s  ?s i)2

IF (s) =

22
t

t

4
t

=

A

a

 

which we can derive  as before  using the tiling property (eq. 3). (See Supplemental Sec. S2 for
derivation.) The ﬁrst of the two expressions at right reﬂects IF for the amplitude constraint  where
 varies implicitly as we vary t. The second expresses IF under the energy constraint  where  is
constant so that the amplitude constant a varies implicitly with t. For both constraints  IF increases
with decreasing t [5].
Fisher information provides a well-known bound on the variance of an unbiased estimator ˆs(r)
known as the Cram´er-Rao (CR) bound  namely var(ˆs|s)  1/IF (s). Since FI is constant over s in
our idealized setting  this leads to a bound on the mean squared error ([6  11]):
2
t


MSE   E⇥(ˆs(r)  s)2⇤p(r s)  E 1

IF (s)p(s)

t

(9)

=

=

a

 

3

effects of prior stdev

effects of time window (ms)

 

=

 3
2

amplitude
constraint: M

E
S

energy
constraint: M

E
S

103

101
10(cid:239)(cid:20)

101
10(cid:239)(cid:20)
10(cid:239)(cid:22)

16
8
4
2
1

 

0

102
100
10(cid:239)(cid:21)

100
10(cid:239)(cid:21)
10(cid:239)(cid:23)

CR bound

2

4

tuning width 

6

8

= 25
501

0

02

4

0

0

0

0

CR bound

 

0

2

4

tuning width 

6

 

8

Figure 2: Mean squared error as a function of the tuning width t  under amplitude constraint (top
row) and energy constraint (bottom row)  for spacing  = 1 and amplitude A = 20 sp/s. and
Top left: MSE for different prior widths s (with ⌧ = 100ms)  showing that optimal t increases
with larger prior variance. Cram´er-Rao bound (gray solid) is minimized at t = 0  whereas bound
(eq. 12  gray dashed) accurately captures shape and location of the minimum. Top right: Similar
curves for different time windows ⌧ for counting spikes (with s=32)  showing that optimal t in-
creases for lower spike counts. Bottom row: Similar traces under energy constraint (where A scales
inversely with t so that  = p2⇡⌧ At is constant). Although the CR bound grossly understates
the true MSE for small counting windows (right)  the optimal tuning is maximally narrow in this
conﬁguration  consistent with the CR curve.

which is simply the inverse of Fisher Information (eq. 8).
Fisher information also provides a (quasi) lower bound on the mutual information  since an efﬁcient
estimator (i.e.  one that achieves the CR bound) has entropy upper-bounded by that of a Gaussian
with variance 1/IF (see [3]). In our setting this leads to the lower bound:

MI(s  r)   H(s)  H(s|r)  1

2 log⇣2

s

a

t⌘ = 1

2 log⇣2

s


2

t⌘.

(10)

Note that neither of these FI-based bounds apply exactly to the Bayesian setting we consider here 
since Bayesian estimators are generally biased  and are inefﬁcient in the regime of low spike counts
[6]. We examine them here nonetheless (gray traces in Figs. 2 and 3) due to their prominence in the
prior literature ([5  11  13])  and to emphasize their limitations for characterizing optimal codes.

2.3 Exact Bayesian analyses

In our idealized population  the total spike count R is a Poisson random variable with mean   which
allows us to compute the MSE and MI by taking expectations w.r.t. this distribution.

Mean Squared Error (MSE)
The mean squared error  which equals the average posterior variance (eq. 7)  can be computed
analytically for this model:

MSE = E 2

R + ⇢p(R)

t

=

e = 2

t e (⇢) ⇤ (⇢ )  

(11)

t /2

s and ⇤(a  z) = za 1

0 ta1etdt is the holomorphic extension of the lower
where ⇢ = 2
incomplete gamma function [25] (see SI for derivation). When the tuning curve is narrower than the
prior (i.e.  2

s )  we can obtain a relatively tight lower bound:

t  2

MSE 

s  2

t )e.

(12)

t

R!

R + ⇢◆ R

1XR=0✓ 2
(a)R z
 1  e + (2

2
t

4

effects of prior stdev

effects of time window (ms)

amplitude
constraint:

)
s
t
i

b
(
 
I

M

energy
constraint:

)
s
t
i

b
(
 
I

M

 

6
4
2
0

6
4
2
0

FI-based bound

 

 = 32
16
8
4
2
1

2

4

tuning width 

6

8

 

6
4
2
0

6
4
2
0

 

= 400

= 25

2

4

tuning width 

6

8

Figure 3: Mutual information as a function of tuning width t  directly analogous to plots in Fig. 2.
Note the problems with the lower bound on MI derived from Fisher information (top  gray traces)
and the close match of the derived bound (eq. 14  dashed gray traces). The effects are similar
to Fig. 2  except that MI-optimal tuning widths are slightly smaller (upper left and right) than for
MSE-optimal codes. For both loss functions  optimal width is minimal under an energy constraint.

Figure 2 shows the MSE (and derived bound) as a function of the tuning width t over the range
where tiling approximately holds. Note the high accuracy of the approximate formula (12  dashed
gray traces) and that the FI-based bound does not actually lower-bound the MSE in the case of
narrow priors (darker traces).
For the amplitude-constrained setting (top row  obtained by substituting  = at/ in eqs. 11 and
12)  we observe substantial discrepancies between the true MSE and FI-based analysis. While FI
suggests that optimal tuning width is near zero (down to the limits of tiling)  analyses reveal that the
optimal t grows with prior variance (left) and decreasing time window (right). These observations
agree well with the existing literature (e.g. [14  15]). However  if we restrict the average population
ﬁring rate (energy constraint  bottom plots)  the optimal tuning curves once again approach zero. In
this case  FI provides correct intuitions and better approximation of the true MSE.

Mutual Information (MI)

For a tiling population and Gaussian prior  mutual information between stimulus and response is:

MI(s  r) = 1

2Ehlog⇣1 + R 2

t⌘ip(R)

s
2

 

(13)

which has no closed-form solution  but can be calculated efﬁciently with a discrete sum over R from
0 to some large integer (e.g.  R =  + np to capture n standard deviations above the mean). We
can derive an upper bound using the Taylor expansion to log while preserving the exact zeroth order
term:

MI(s  r)  1e

2

log⇣1 + (



1e ) 2

s
2

t⌘ = 1eat/

2

log⇣1 +

a

1eat/

2
s

t⌘

(14)

Once again  we investigate the efﬁciency of population coding  but in terms of the maximal MI.
Figure 3 shows MI as a function of the neural tuning width t. We observe a similar effect as for the
MSE: the optimal tuning widths are now different from zero  but only for the amplitude constraint.
Under the energy constraint (as with FI) the optimum is maximally narrow tuning.

5

3 Poisson population coding with input noise

We can obtain a more general family of correlated population codes by considering “input noise” 
where the stimulus s is corrupted by an additive noise n (see Fig. 1):

(prior) (15)
(input noise) (16)
(population response) (17)
The use of Gaussians allows us to marginalise over n analytically  resulting in a Gaussian form for
the likelihood and Gaussian posterior:

s ⇠ N (0  2
s )
n ⇠ N (0  2
n)

ri|s  n ⇠ Poiss(fi(s + n))

p(r|s) / N 1

r> ?s
s + R(2

2
t /2

p(s|r) = N✓

R r> ?s  1

R 2

n/2

s + 1)

n

t + 2
(2
t + R2
2
t + R(2

 

n)2
s
n + 2

s )◆

(likelihood) (18)

(posterior) (19)

Note that even in the limit of many neurons and large spike counts  the posterior variance is non-zero 
converging to 2

s )  a limit deﬁned by the prior and input noise variance. [22].

n + 2

s /(2

n2

3.1 Population coding characteristics: FI  MSE  & MI

Fisher information for a population with input noise can be computed using the fact that the likeli-
hood (eq. 18) is Gaussian:

IF (s) = E
where ⇢ = 2
n and ⇤(· ·) once again denotes holomorphic extension of lower incomplete
gamma function. Note that for n = 0  this reduces to (eq. 8).
It is straightforward to employ the results from Sec. 2.3 for the exact Bayes analyses of a Gaussian
posterior (19):

(1 + ⇢)⇤(1 + ⇢ )

np(R)

e
2
n

2
t + R2

t /2

(20)

=

R

MSE = 2

sE

2
t + R2
n

2
t + R(2

n + 2

= ⇥()⇤( ) + 2
2Elog✓1 +

R2
s
2
t + R2

MI = 1

n

1

= 2

s )p(R)

s  E

 + Rp(R)
(1 + )⇤(1 +  )⇤2
n◆p(R)

 

2
s +2
n

s 2
+ 2
n
s +2
2

n E R

 + Rp(R)

s e  and

(21)

(22)

t /(2

s + 2

where  = 2
n). Although we could not determine closed-form analytical expressions
for MI  it can be computed efﬁciently by summing over a range of integers [0  . . . Rmax] for which
P (R) has sufﬁcient support. This is still a much faster procedure than estimating these values from
Monte Carlo simulations.

3.2 Optimal tuning width under input noise

Fig. 4 shows the optimal tuning width under the amplitude constraint  that is  the value of t that
achieves minimal MSE (left) or maximal MI (right) as a function of the prior width s  for several
different time windows ⌧. Blue traces show results for a Poisson population  while green traces
correspond to a population with input noise (n = 1).
For both MSE and MI loss functions  optimal tuning width decreases for narrower priors. However 
under input noise (green traces)  the optimal tuning width saturates at the value that depends on
the available number of spikes. As the prior grows wider  the growth of the optimal tuning width
depends strongly on the choice of loss function: optimal t grows approximately logarithmically
with s for minimizing MSE (left)  but it grows much slower for maximizing MI (right). Note that
for realistic prior widths (i.e. for s > n)  the effects of input noise on optimal tuning width are far
more substantial under MI than under MSE.

6

8

6

4

2

0

MSE

Poisson 
noise only

w/ input noise

0.1
prior stdev 

1

= 25

= 5 0
0
= 1

0

= 200
10

8

6

4

2

0

 

mutual information

= 25

= 200

0.1
prior stdev 

1

10

 

h

t

i

d
w
C
T

 

 
l

a
m

i
t

p
o

Figure 4: Optimal tuning width t (under amplitude constraint only) as a function of prior width s 
for classic Poisson populations (blue) and populations with input-noise (green  2
n = 1). Different
traces correspond to different time windows of integration  for  = 1 and A = 20 sp/s. As n
increases  the optimal tuning width increases under MI  and under MSE when s < n (traces not
shown). For MSE  predictions of the Poisson and input-noise model converge for priors s > n.

We have not shown plots for energy-constrained population codes because the optimal tuning width
sits at the minimum of the range over which tiling can be said to hold  regardless of prior width 
input noise level  time window  or choice of loss function. This can be seen easily in the expressions
for MI (eqs. 13 and 22)  in which each term in the expectation is a decreasing function of t for
all R > 0. This suggests that  contrary to some recent arguments (e.g.  [14  15])  narrow tuning (at
least down to the limit of tiling) really is best if the brain has a ﬁxed energetic budget for spiking  as
opposed to a mere constraint on the number of neurons.

4 Correlations induced by input noise

Input noise alters the mean  variance  and pairwise correlations of population responses in a system-
atic manner that we can compute directly (see Supplement for derivations). In Fig. 5 we show the
effects of input noise with standard deviation n = 0.5  for neurons with the tuning amplitude of
A = 10. The tuning curve (mean response) becomes slightly ﬂatter (Fig. 5A)  while the variance in-
creases  especially at the ﬂanks (Fig. 5B). Fig. 5C shows correlations between the two neurons with
tuning curves and variance are shown in panels A-B: one pair with the same preferred orientation at
zero (red) and a second with a 4 degree difference in preferred orientation (blue). From these plots 
it is clear that the correlation structure depends on both the tuning as well as the stimulus. Thus  in
order to describe such correlations one needs to consider the entire stimulus range  not simply the
average correlation marginalized over stimuli.
Figure 5D shows the pairwise correlations across an entire population of 21 neurons given a stim-
ulus at s = 0. Although we assumed Gaussian tuning curves here  one can obtain similar plots
for arbitrary unimodal tuning curves (see Supplement)  which should make it feasible to test our
predictions in real data. However  the time scale of the input noise and basic neural computations
is about 10 ms. At such short spike count windows  available number of spikes is low  and so are
correlations induced by input noise. With other sources of second order statistics  such as common
input gains (e.g. by contrast or adaptation)  these correlations might be too subtle to recover [22].

5 Discussion

We derived exact expressions for mean squared error and mutual information in a Bayesian analysis
of: (1) an idealized Poisson population coding model; and (2) a correlated  conditionally Poisson
population coding model with shared input noise. These expressions allowed us to examine the
optimal tuning curve width under both loss functions  under two kinds of resource constraints. We
have conﬁrmed that optimal t diverges from predictions based on Fisher information  if the overall
spike count is allowed to grow with tuning width (i.e.  because more neurons respond to the stim-
ulus when tuning curves become broader). We refer to this as an “amplitude constraint”  because
the amplitude is ﬁxed independently of tuning width. This differs from an “energy constraint”  in

7

A

(cid:20)(cid:19)

5

(cid:19)

s
 
/
 

p
s

C

r

(cid:19)(cid:17)(cid:21)
(cid:19)
(cid:239)(cid:19)(cid:17)(cid:21)
(cid:239)(cid:19)(cid:17)(cid:23)

mean

(cid:239)(cid:20)(cid:19)

(cid:19)

stimulus s

(cid:20)(cid:19)

correlation 

(cid:239)(cid:20)(cid:19)

(cid:19)

stimulus s

(cid:20)(cid:19)

variance

(cid:239)(cid:20)(cid:19)

(cid:19)

stimulus s

(cid:20)(cid:19)

 
 

B

(cid:20)(cid:19)

2

)
s
 
/
 

p
s
(

5

(cid:19)

(cid:20)(cid:19)

(cid:19)

D

m

i
t
s
 

d
e
r
r
e
e
r
p

f

(cid:239)(cid:20)(cid:19)

 
(cid:239)(cid:20)(cid:19)

(cid:19)

preferred stim

(cid:20)(cid:19)

r
(cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:20)
(cid:19)
(cid:239)(cid:19)(cid:17)(cid:20)
(cid:239)(cid:19)(cid:17)(cid:21)

Figure 5: Response statistics of neural population with input noise  for standard deviation n = 0.5.
(A) Expected spike responses of two neurons: ?s 1 = 0 (red) and ?s 2 = 4 (blue). The common
noise effectively smooths blurs the tuning curves with a Gaussian kernel of width n. (B) Variance
of neuron 1  its tuning curve replotted in black for reference. Input noise has largest inﬂuence on
variance at the steepest parts of the tuning curve. (C) Cross-correlation of the neuron 1 with two
others: one sharing the same preference (red)  and one with ?s = 4 (blue). Note that correlation
of two identically tuned neurons is largest at the steepest part of the tuning curve. (D) Spike count
correlations for entire population of 21 neurons given a ﬁxed stimulus s = 0  illustrating that the
pattern of correlations is signal dependent.

which tuning curve amplitude scales with tuning width so that average total spike count is constant.
Under an energy constraint  predictions from Fisher information match those of an exact Bayesian
analysis  and we ﬁnd that optimal tuning width should be narrow (down to the limit at which the
tiling assumption applies)  regardless of the duration  prior width  or input noise level.
We also derived explicit expressions for the response correlations induced by the input noise. These
correlations depend on the shape and amplitude of tuning curves  and on the amount of input noise
(n). However  for a reasonable assumption that noise distribution is much narrower than the width
of the prior (and tuning curves)  under which the mean ﬁring rate changes little  we can derive pre-
dictions for the covariances directly from the measured tuning curves. An important direction for
future work will be to examine the detailed structure of correlations measured in large populations.
We feel that the input noise model — which describes exactly those correlations that are most harm-
ful for decoding — has the potential to shed light on the factors affecting the coding capacity in
optimal neural populations [22].
Finally  we can return to the introductory example involving orientation discrimination  to ask
how the number of neurons necessary to reach the human discrimination threshold of s=1 de-
gree changes in the presence of input noise. As n approaches s  the number of neurons required
goes rapidly to inﬁnity (See Supp. Fig. S1).

Acknowledgments
This work was supported by the McKnight Foundation (JP)  NSF CAREER Award IIS-1150186
(JP)  NIMH grant MH099611 (JP) and the Gatsby Charitable Foundation (AGB).

References
[1] HS Seung and H. Sompolinsky. Simple models for reading neuronal population codes. Proceedings of

the National Academy of Sciences  90(22):10749–10753  1993.

[2] R. S. Zemel  P. Dayan  and A. Pouget. Probabilistic interpretation of population codes. Neural Comput 

10(2):403–430  Feb 1998.

8

[3] Nicolas Brunel and Jean-Pierre Nadal. Mutual information  ﬁsher information  and population coding.

Neural Computation  10(7):1731–1757  1998.

[4] Kechen Zhang and Terrence J. Sejnowski. Neuronal tuning: To sharpen or broaden? Neural Computation 

11(1):75–84  1999.

[5] A. Pouget  S. Deneve  J. Ducom  and P. E. Latham. Narrow versus wide tuning curves: What’s best for a

population code? Neural Computation  11(1):85–90  1999.

[6] M. Bethge  D. Rotermund  and K. Pawelzik. Optimal short-term population coding: When ﬁsher infor-

mation fails. Neural computation  14(10):2317–2351  2002.

[7] P. Series  P. E. Latham  and A. Pouget. Tuning curve sharpening for orientation selectivity: coding

efﬁciency and the impact of correlations. Nature Neuroscience  7(10):1129–1135  2004.

[8] W. J. Ma  J. M. Beck  P. E. Latham  and A. Pouget. Bayesian inference with probabilistic population

codes. Nature Neuroscience  9:1432–1438  2006.

[9] Marcelo A Montemurro and Stefano Panzeri. Optimal tuning widths in population coding of periodic

variables. Neural computation  18(7):1555–1576  2006.

[10] R. Haefner and M. Bethge. Evaluating neuronal codes for inference using ﬁsher information. Neural

Information Processing Systems  2010.

[11] D. Ganguli and E. P. Simoncelli. Implicit encoding of prior probabilities in optimal neural populations.

In Adv. Neural Information Processing Systems  volume 23  May 2010.

[12] Xue-Xin Wei and Alan Stocker. Efﬁcient coding provides a direct link between prior and likelihood in

perceptual bayesian inference. In Adv. Neur. Inf. Proc. Sys. 25  pages 1313–1321  2012.

[13] Z Wang  A Stocker  and A Lee. Optimal neural tuning curves for arbitrary stimulus distributions: Dis-

crimax  infomax and minimum lp loss. In Adv. Neur. Inf. Proc. Sys. 25  pages 2177–2185  2012.

[14] P. Berens  A.S. Ecker  S. Gerwinn  A.S. Tolias  and M. Bethge. Reassessing optimal neural population
codes with neurometric functions. Proceedings of the National Academy of Sciences  108(11):4423  2011.
[15] Steve Yaeli and Ron Meir. Error-based analysis of optimal tuning functions explains phenomena observed

in sensory neurons. Frontiers in computational neuroscience  4  2010.

[16] J. M. Beck  P. E. Latham  and A. Pouget. Marginalization in neural circuits with divisive normalization.

J Neurosci  31(43):15310–15319  Oct 2011.

[17] Stuart Yarrow  Edward Challis  and Peggy Seri`es. Fisher and shannon information in ﬁnite neural popu-

lations. Neural Computation  24(7):1740–1780  2012.

[18] D Ganguli and E P Simoncelli. Efﬁcient sensory encoding and Bayesian inference with heterogeneous
neural populations. Neural Computation  26(10):2103–2134  Oct 2014. Published online: 24 July 2014.
[19] E. Zohary  M. N. Shadlen  and W. T. Newsome. Correlated neuronal discharge rate and its implications

for psychophysical performance. Nature  370(6485):140–143  Jul 1994.

[20] Keiji Miura  Zachary Mainen  and Naoshige Uchida. Odor representations in olfactory cortex: distributed

rate coding and decorrelated population activity. Neuron  74(6):1087–1098  2012.

[21] Jakob H. Macke  Manfred Opper  and Matthias Bethge. Common input explains higher-order correlations
and entropy in a simple model of neural population activity. Phys. Rev. Lett.  106(20):208102  May 2011.
[22] R. Moreno-Bote  J. Beck  I. Kanitscheider  X. Pitkow  P.E. Latham  and A. Pouget. Information-limiting

correlations. Nat Neurosci  17(10):1410–1417  Oct 2014.

[23] K. Josic  E. Shea-Brown  B. Doiron  and J. de la Rocha. Stimulus-dependent correlations and population

codes. Neural Comput  21(10):2774–2804  Oct 2009.

[24] G. Dehaene  J. Beck  and A. Pouget. Optimal population codes with limited input information have ﬁnite

tuning-curve widths. In CoSyNe  Salt Lake City  Utah  February 2013.

[25] NIST Digital Library of Mathematical Functions. http://dlmf.nist.gov/  Release 1.0.9 of 2014-08-29.
[26] David C Burr and Sally-Ann Wijesundra. Orientation discrimination depends on spatial frequency. Vision

Research  31(7):1449–1452  1991.

[27] P. Seri`es  A. A. Stocker  and E. P. Simoncelli. Is the homunculus ’aware’ of sensory adaptation? Neural

Computation  21(12):3271–3304  Dec 2009.

[28] R. L. De Valois  E. W. Yund  and N. Hepler. The orientation and direction selectivity of cells in macaque

visual cortex. Vision research  22(5):531–544  1982.

9

,Andreas Ruttor
Philipp Batz
Manfred Opper
Agnieszka Grabska-Barwinska
Jonathan Pillow