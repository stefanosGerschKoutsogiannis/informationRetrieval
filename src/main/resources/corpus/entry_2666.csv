2019,Inherent Tradeoffs in Learning Fair Representations,With the prevalence of machine learning in high-stakes applications  especially the ones regulated by anti-discrimination laws or societal norms  it is crucial to ensure that the predictive models do not propagate any existing bias or discrimination. Due to the ability of deep neural nets to learn rich representations  recent advances in algorithmic fairness have focused on learning fair representations with adversarial techniques to reduce bias in data while preserving utility simultaneously. In this paper  through the lens of information theory  we provide the first result that quantitatively characterizes the tradeoff between demographic parity and the joint utility across different population groups. Specifically  when the base rates differ between groups  we show that any method aiming to learn fair representations admits an information-theoretic lower bound on the joint error across these groups. To complement our negative results  we also prove that if the optimal decision functions across different groups are close  then learning fair representations leads to an alternative notion of fairness  known as the accuracy parity  which states that the error rates are close between groups. Finally  our theoretical findings are also confirmed empirically on real-world datasets.,Inherent Tradeoffs in Learning Fair Representations

Han Zhao∗

Machine Learning Department
School of Computer Science
Carnegie Mellon University
han.zhao@cs.cmu.edu

Geoffrey J. Gordon

Microsoft Research  Montreal
Machine Learning Department
Carnegie Mellon University

geoff.gordon@microsoft.com

Abstract

With the prevalence of machine learning in high-stakes applications  especially the
ones regulated by anti-discrimination laws or societal norms  it is crucial to ensure
that the predictive models do not propagate any existing bias or discrimination. Due
to the ability of deep neural nets to learn rich representations  recent advances in
algorithmic fairness have focused on learning fair representations with adversarial
techniques to reduce bias in data while preserving utility simultaneously. In this
paper  through the lens of information theory  we provide the ﬁrst result that
quantitatively characterizes the tradeoff between demographic parity and the joint
utility across different population groups. Speciﬁcally  when the base rates differ
between groups  we show that any method aiming to learn fair representations
admits an information-theoretic lower bound on the joint error across these groups.
To complement our negative results  we also prove that if the optimal decision
functions across different groups are close  then learning fair representations leads
to an alternative notion of fairness  known as the accuracy parity  which states that
the error rates are close between groups. Finally  our theoretical ﬁndings are also
conﬁrmed empirically on real-world datasets.

1

Introduction

With the prevalence of machine learning applications in high-stakes domains  e.g.  criminal judgement 
medical testing  online advertising  etc.  it is crucial to ensure that the automated decision making
systems do not propagate existing bias or discrimination that might exist in historical data [3  4  28].
Among many recent proposals for achieving different notions of algorithmic fairness [10  14  31–33] 
learning fair representations has received increasing attention due to recent advances in learning
rich representations with deep neural networks [5  11  24  26  30  34]. In fact  a line of work has
proposed to learn group-invariant representations with adversarial learning techniques in order to
achieve statistical parity  also known as the demographic parity in the literature. This line of work
dates at least back to Zemel et al. [33] where the authors proposed to learn predictive models that
are independent of the group membership attribute. At a high level  the underlying idea is that if
representations of instances from different groups are similar to each other  then any predictive model
on top of them will certainly make decisions independent of group membership.
On the other hand  it has long been observed that there is an underlying tradeoff between utility and
demographic parity:

“All methods have in common that to some extent accuracy must be traded-off for
lowering the dependency.” [6]

In particular  it is easy to see that in an extreme case where the group membership coincides with
the target task  a call for exact demographic parity will inevitably remove the perfect predictor [14].

∗Part of this work was done when Han Zhao was visiting Microsoft Research  Montreal.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Empirically  it has also been observed that a tradeoff exists between accuracy and fairness in binary
classiﬁcation [38]. Clearly  methods based on learning fair representations are also bound by such
inherent tradeoff between utility and fairness. But how does the fairness constraint trade for utility?
Will learning fair representations help to achieve other notions of fairness besides the demographic
parity? If yes  what is the fundamental limit of utility that we can hope to achieve under such
constraint?
To answer the above questions  through the lens of information theory  in this paper we provide the
ﬁrst result that quantitatively characterizes the tradeoff between demographic parity and the joint
utility across different population groups. Speciﬁcally  when the base rates differ between groups 
we provide a tight information-theoretic lower bound on the joint error across these groups. Our
lower bound is algorithm-independent so it holds for all methods aiming to learn fair representations.
When only approximate demographic parity is achieved  we also present a family of lower bounds to
quantify the tradeoff of utility introduced by such approximate constraint. As a side contribution  our
proof technique is simple but general  and we expect it to have broader applications in other learning
problems using adversarial techniques  e.g.  unsupervised domain adaptation [12  36]  privacy-
preservation under attribute inference attacks [13  35] and multilingual machine translation [16].
To complement our negative results  we show that if the optimal decision functions across different
groups are close  then learning fair representations helps to achieve an alternative notion of fairness 
i.e.  the accuracy parity  which states that the error rates are close between groups. Empirically 
we conduct experiments on a real-world dataset that corroborate both our positive and negative
results. We believe our theoretical insights contribute to better understanding of the tradeoff between
utility and different notions of fairness  and they are also helpful in guiding the future design of
representation learning algorithms to achieve algorithmic fairness.

2 Preliminary

We ﬁrst introduce the notation used throughout the paper and formally describe the problem setup.
We then brieﬂy discuss some information-theoretic concepts that will be used in our analysis.
Notation We use X ⊆ Rd and Y = {0  1} to denote the input and output space. Accordingly  we
use X and Y to denote the random variables which take values in X and Y  respectively. Lower case
letters x and y are used to denote the instantiation of X and Y . To simplify the presentation  we
use A ∈ {0  1} as the sensitive attribute  e.g.  race  gender  etc. 2 Let H be the hypothesis class of
classiﬁers. In other words  for h ∈ H  h : X → Y is the predictor that outputs a prediction. Note that
even if the predictor does not explicitly take the sensitive attribute A as input  this fairness through
blindness mechanism can still be biased due to the potential correlations between X and A. In this
work we study the stochastic setting where there is a joint distribution D over X  Y and A from
which the data are sampled. To keep the notation consistent  for a ∈ {0  1}  we use Da to mean
the conditional distribution of D given A = a. For an event E  D(E) denotes the probability of E
under D. In particular  in the literature of fair machine learning  we call D(Y = 1) the base rate
of distribution D and we use ∆BR(D D(cid:48)) := |D(Y = 1) − D(cid:48)(Y = 1)| to denote the difference
of the base rates between two distributions D and D(cid:48) over the same sample space. Given a feature
transformation function g : X → Z that maps instances from the input space X to feature space Z 
we deﬁne g(cid:93)D := D ◦ g−1 to be the induced (pushforward) distribution of D under g  i.e.  for any
event E(cid:48) ⊆ Z  g(cid:93)D(E(cid:48)) := D(g−1(E(cid:48))) = D({x ∈ X | g(x) ∈ E(cid:48)}).
Problem Setup Given a joint distribution D  the error of a predictor h under D is deﬁned as
ErrD(h) := ED[|Y − h(X)|]. Note that for binary classiﬁcation problems  when h(X) ∈ {0  1} 
ErrD(h) reduces to the true error rate of binary classiﬁcation. To make the notation more compact 
we may drop the subscript D when it is clear from the context. In this work we focus on group
fairness where the group membership is given by the sensitive attribute A. Even in this context there
are many possible deﬁnitions of fairness [27]  and in what follows we provide a brief review of the
ones that are mostly relevant to this work.

Deﬁnition 2.1 (Demographic Parity). Given a joint distribution D  a classiﬁer (cid:98)Y satisﬁes demo-
graphic parity if (cid:98)Y is independent of A.

2Our main results could also be straightforwardly extended to the setting where A is a categorical variable.

2

Demographic parity reduces to the requirement that D0((cid:98)Y = 1) = D1((cid:98)Y = 1)  i.e.  positive outcome
Deﬁnition 2.2 (DP Gap). Given a joint distribution D  the demographic parity gap of a classiﬁer(cid:98)Y
is ∆DP((cid:98)Y ) := |D0((cid:98)Y = 1) − D1((cid:98)Y = 1)|.

is given to the two groups at the same rate. When exact equality does not hold  we use the absolute
difference between them as an approximate measure:

Demographic parity is also known as statistical parity  and it has been adopted as deﬁnition of fairness
in a series of work [6  11  15  17  18  24  26  33]. However  as we shall quantify precisely in Section 3 
demographic parity may cripple the utility that we hope to achieve  especially in the common scenario
where the base rates differ between two groups  e.g.  D0(Y = 1) (cid:54)= D1(Y = 1) [14]. In light of this 
an alternative deﬁnition is accuracy parity:
Deﬁnition 2.3 (Accuracy Parity). Given a joint distribution D  a classiﬁer h satisﬁes accuracy parity
if ErrD0(h) = ErrD1(h).

In the literature  a break of accuracy parity is also known as disparate mistreatment [32]. Again  when
h is a deterministic binary classiﬁer  accuracy parity reduces to D0(h(X) = Y ) = D1(h(X) = Y ).
Different from demographic parity  the deﬁnition of accuracy parity does not eliminate the perfect
predictor when Y = A when the base rates differ between two groups. When costs of different error
types matter  more reﬁned deﬁnitions exist:
Deﬁnition 2.4 (Positive Rate Parity). Given a joint distribution D  a deterministic classiﬁer h satisﬁes
positive rate parity if D0(h(X) = 1 | Y = y) = D1(h(X) = 1 | Y = y)  ∀y ∈ {0  1}.
Positive rate parity is also known as equalized odds [14]  which essentially requires equal true positive
and false positive rates between different groups. Furthermore  Hardt et al. [14] also deﬁned true
positive parity  or equal opportunity  to be D0(h(X) = 1 | Y = 1) = D1(h(X) = 1 | Y = 1) when
positive outcome is desirable. Last but not least  predictive rate parity  also known as test fairness [7] 
asks for equal chance of positive outcomes across groups given predictions:
Deﬁnition 2.5 (Predictive Rate Parity). Given a joint distribution D  a probabilistic classiﬁer h
satisﬁes predictive rate parity if D0(Y = 1 | h(X) = c) = D1(Y = 1 | h(X) = c)  ∀c ∈ [0  1].
When h is a deterministic binary classiﬁer that only takes value in {0  1}  Chouldechova [7] showed
an intrinsic tradeoff between predictive rate parity and positive rate parity:
Theorem 2.1 (Chouldechova [7]). Assume D0(Y = 1) (cid:54)= D1(Y = 1)  then for any deterministic
classiﬁer h : X → {0  1} that is not perfect  i.e.  h(X) (cid:54)= Y   positive rate parity and predictive rate
parity cannot hold simultaneously.

Similar tradeoff result for probabilistic classiﬁer has also been observed by Kleinberg et al. [21] 
where the authors showed that for any non-perfect predictors  calibration and positive rate parity
cannot be achieved simultaneously if the base rates are different across groups. Here a classiﬁer h is
said to be calibrated if D(Y = 1 | h(X) = c) = c ∀c ∈ [0  1]  i.e.  if we look at the set of data that
receive a predicted probability of c by h  we would like c-fraction of them to be positive instances
according to Y [29].

f-divergence
Introduced by Ali and Silvey [2] and Csiszár [8  9]  f-divergence  also known as the
Ali-Silvey distance  is a general class of statistical divergences to measure the difference between
two probability distributions P and Q over the same measurable space.
Deﬁnition 2.6 (f-divergence). Let P and Q be two probability distributions over the same space and
assume P is absolutely continuous w.r.t. Q (P (cid:28) Q). Then for any convex function f : (0 ∞) → R
that is strictly convex at 1 and f (1) = 0  the f-divergence of Q from P is deﬁned as

(cid:20)

(cid:18) dP

(cid:19)(cid:21)

Df (P || Q) := EQ

dQ
The function f is called the generator function of Df (· || ·).
Different choices of the generator function f recover popular statistical divergence as special cases 
e.g.  the KL-divergence. From Jensen’s inequality it is easy to verify that Df (P || Q) ≥ 0 and
Df (P || Q) = 0 iff P = Q almost surely. Note that f-divergence does not necessarily leads to

f

.

(1)

3

a distance metric  and it is not symmetric in general  i.e.  Df (P || Q) (cid:54)= Df (Q || P) provided
that P (cid:28) Q and Q (cid:28) P. We list some common choices of the generator function f and their
corresponding properties in Table 1. Notably  Khosravifard et al. [20] proved that total variation is
the only f-divergence that serves as a metric  i.e.  satisfying the triangle inequality.

Table 1: List of different f-divergences and their corresponding properties. DKL(P || Q) denotes the
KL-divergence of Q from P and M := (P + Q)/2 is the average distribution of P and Q. Symm.
stands for Symmetric and Tri. stands for Triangle Inequality.

Df (P || Q)
Name
Kullback-Leibler DKL(P || Q)
DKL(Q || P)
Reverse-KL
Jensen-Shannon DJS(P Q) := 1
Squared Hellinger H 2(P Q) := 1
Total Variation

(cid:82) (

dQ)2
dTV(P Q) := supE |P(E) − Q(E)|

2

√

dP − √

2 (DKL(P||M) + DKL(Q||M)) t log t − (t + 1) log( t+1

Generator f (t)

t log t
− log t
(1 − √
|t − 1|/2

t)2/2

Symm. Tri.


2 ) 









3 Main Results

As we brieﬂy mentioned in Section 2  it is impossible to have imperfect predictor that is both
calibrated and preserves positive rate parity when the base rates differ between two groups. Similar
impossibility result also holds between positive rate parity and predictive rate parity. On the other
hand  while it has long been observed that demographic parity may eliminate perfect predictor [14] 
and previous work has empirically veriﬁed that tradeoff exists between accuracy and demographic
parity [6  17  38] on various datasets  so far a quantitative characterization on the exact tradeoff
between accuracy and various notions of parity is still missing. In what follows we shall prove a
family of information theoretic lower bounds on the accuracy that hold for all algorithms.

3.1 Tradeoff between Fairness and Utility

g−→ Z h−→ (cid:98)Y   where g is the
feature transformation  h is the classiﬁer on feature space  Z is the feature and(cid:98)Y is the predicted target
Essentially  every prediction function induces a Markov chain: X
variable by h ◦ g. Note that simple models  e.g.  linear classiﬁers  are also included by specifying
g to be the identity map. With this notation  we ﬁrst state the following theorem that quantiﬁes an
Theorem 3.1. Let(cid:98)Y = h(g(X)) be the predictor. If(cid:98)Y satisﬁes demographic parity  then ErrD0(h ◦
inherent tradeoff between fairness and utility.
g) + ErrD1 (h ◦ g) ≥ ∆BR(D0 D1).
Remark First of all  ∆BR(D0 D1) is the difference of base rates across groups  and it achieves
its maximum value of 1 iff Y = A almost surely  i.e.  Y indicates group membership. On the other
hand  if Y is independent of A  then ∆BR(D0 D1) = 0 so the lower bound does not make any
constraint on the joint error. Second  Theorem 3.1 applies to all possible feature transformation g
and predictor h. In particular  if we choose g to be the identity map  then Theorem 3.1 says that
when the base rates differ  no algorithm can achieve a small joint error on both groups  and it also
recovers the previous observation that demographic parity can eliminate the perfect predictor [14].
Third  the lower bound in Theorem 3.1 is insensitive to the marginal distribution of A  i.e.  it treats
the errors from both groups equally. As a comparison  let α := D(A = 1)  then ErrD(h ◦ g) =
(1 − α)ErrD0 (h ◦ g) + αErrD1 (h ◦ g). In this case ErrD(h ◦ g) could still be small even if the
minority group suffers a large error.
Corollary 3.1. If the predictor (cid:98)Y = h(g(X)) satisﬁes demographic parity  then max{ErrD0(h ◦
Furthermore  by the pigeonhole principle  the following corollary holds:
g)  ErrD1(h ◦ g)} ≥ ∆BR(D0 D1)/2.
In words  this means that for fair predictors in the demographic parity sense  at least one of the
subgroups has to incur an error of at least ∆BR(D0 D1)/2 which could be large in settings like
criminal justice where ∆BR(D0 D1) is large.

4

Before we give the proof  we ﬁrst present a useful lemma that lower bounds the prediction error by
Lemma 3.1. Let (cid:98)Y = h(g(X)) be the predictor  then for a ∈ {0  1}  dTV(Da(Y ) Da((cid:98)Y )) ≤
the total variation distance.
ErrDa (h ◦ g).
Proof. For a ∈ {0  1}  we have:

dTV(Da(Y ) Da((cid:98)Y )) = |Da(Y = 1) − Da(h(g(X)) = 1)| = |EDa[Y ] − EDa [h(g(X))]|

≤ EDa [|Y − h(g(X))|] = ErrDa (h ◦ g).

Now we are ready to prove Theorem 3.1:

Proof of Theorem 3.1. First of all  we show that if (cid:98)Y = h(g(X)) satisﬁes demographic parity  then:
dTV(D0((cid:98)Y ) D1((cid:98)Y )) = max(cid:8)|D0((cid:98)Y = 0) − D1((cid:98)Y = 0)|  |D0((cid:98)Y = 1) − D1((cid:98)Y = 1)|(cid:9)

= |D0((cid:98)Y = 1) − D1((cid:98)Y = 1)|
= |D((cid:98)Y = 1 | A = 0) − D((cid:98)Y = 1 | A = 1)| = 0 

(cid:4)

(cid:4)

where the last equality follows from the deﬁnition of demographic parity. Now from Table 1  dTV(· ·)
is symmetric and satisﬁes the triangle inequality  we have:

dTV(D0(Y ) D1(Y )) ≤ dTV(D0(Y ) D0((cid:98)Y )) + dTV(D0((cid:98)Y ) D1((cid:98)Y )) + dTV(D1((cid:98)Y ) D1(Y ))
The last step is to bound dTV(Da(Y ) Da((cid:98)Y )) in terms of ErrDa (h ◦ g) for a ∈ {0  1} using

= dTV(D0(Y ) D0((cid:98)Y )) + dTV(D1((cid:98)Y ) D1(Y )).

(2)

dTV(D0(Y ) D0((cid:98)Y )) ≤ ErrD0(h ◦ g) 

dTV(D1(Y ) D1((cid:98)Y )) ≤ ErrD1 (h ◦ g).

Lemma 3.1:

Combining the above two inequalities and (2) completes the proof.

predictor (cid:98)Y ≡ 1 or (cid:98)Y ≡ 0  which clearly satisﬁes demographic parity by deﬁnition. But in this
It is not hard to show that our lower bound in Theorem 3.1 is tight. To see this  consider the
case A = Y   where the lower bound achieves its maximum value of 1. Now consider a constant
case either ErrD0(h ◦ g) = 1  ErrD1(h ◦ g) = 0 or ErrD0 (h ◦ g) = 0  ErrD1(h ◦ g) = 1  hence
ErrD0(h ◦ g) + ErrD1 (h ◦ g) ≡ 1  achieving the lower bound.
To conclude this section  we point out that the choice of total variation in the lower bound is not
unique. As we will see shortly in Section 3.2  similar lower bounds could be attained using speciﬁc
choices of the general f-divergence with some desired properties.

3.2 Tradeoff in Fair Representation Learning

In the last section we show that there is an inherent tradeoff between fairness and utility when a
predictor exactly satisﬁes demographic parity. In practice we may not be able to achieve demographic
parity exactly. Instead  most algorithms [1  5  11  24] build an adversarial discriminator that takes as
input the feature vector Z = g(X)  and the goal is to learn fair representations such that it is hard for
the adversarial discriminator to infer the group membership from Z. In this sense due to the limit on
the capacity of the adversarial discriminator  only approximate demographic parity can be achieved
in the equilibrium. Hence it is natural to ask what is the tradeoff between fair representations and
accuracy in this scenario? In this section we shall answer this question by generalizing our previous
analysis with f-divergence to prove a family of lower bounds on the joint target prediction error.
Our results also show how approximate DP helps to reconcile but not remove the tradeoff between
fairness and utility. Before we state and prove the main results in this section  we ﬁrst introduce the
following lemma by Liese and Vajda [22] as a generalization of the data processing inequality for
f-divergence:
Lemma 3.2 (Liese and Vajda [22]). Let µ(Z) be the space of all probability distributions over Z.
Then for any f-divergence Df (· || ·)  any stochastic kernel κ : X → µ(Z)  and any distributions P
and Q over X   Df (κP || κQ) ≤ Df (P || Q).

5

Deﬁne dJS(P Q) :=(cid:112)DJS(P Q) and H(P Q) :=(cid:112)H 2(P Q). It is well-known in information

Roughly speaking  Lemma 3.2 says that data processing cannot increase discriminating information.
theory that both dJS(· ·) and H(· ·) form a bounded distance metric over the space of probability
distributions. Realize that dTV(· ·)  H 2(· ·) and DJS(· ·) are all f-divergence. The following
corollary holds:
Corollary 3.2. Let h : Z → Y be any hypothesis  and g(cid:93)Da be the pushforward distribution of Da

by g  ∀a ∈ {0  1}. Let (cid:98)Y = h(g(X)) be the predictor  then all the following inequalities hold:

1. dTV(D0((cid:98)Y ) D1((cid:98)Y )) ≤ dTV(g(cid:93)D0  g(cid:93)D1)
2. H(D0((cid:98)Y ) D1((cid:98)Y )) ≤ H(g(cid:93)D0  g(cid:93)D1)
3. dJS(D0((cid:98)Y ) D1((cid:98)Y )) ≤ dJS(g(cid:93)D0  g(cid:93)D1)

Now we are ready to present the following main theorem of this section:

Theorem 3.2. Let (cid:98)Y = h(g(X)) be the predictor. Assume dJS(g(cid:93)D0  g(cid:93)D1) ≤ dJS(D0(Y ) D1(Y ))

and H(g(cid:93)D0  g(cid:93)D1) ≤ H(D0(Y ) D1(Y ))  then the following three inequalities hold:

1. Total variation lower bound:

2. Jensen-Shannon lower bound:

ErrD0(h ◦ g) + ErrD1(h ◦ g) ≥ dTV(D0(Y ) D1(Y )) − dTV(g(cid:93)D0  g(cid:93)D1).

ErrD0 (h ◦ g) + ErrD1(h ◦ g) ≥(cid:0)dJS(D0(Y ) D1(Y )) − dJS(g(cid:93)D0  g(cid:93)D1)(cid:1)2
ErrD0 (h ◦ g) + ErrD1 (h ◦ g) ≥(cid:0)H(D0(Y ) D1(Y )) − H(g(cid:93)D0  g(cid:93)D1)(cid:1)2

/2.

/2.

3. Hellinger lower bound:

Remark All the three lower bounds in Theorem 3.2 imply a tradeoff between the joint error across
demographic subgroups and learning group-invariant feature representations. In a nutshell:
For fair representations  it is not possible to construct a predictor that simultane-
ously minimizes the errors on both demographic subgroups.

When g(cid:93)D0 = g(cid:93)D1  which also implies D0((cid:98)Y ) = D1((cid:98)Y )  all three lower bounds get larger  in this

case we have

(cid:26)

max

dTV(D0(Y ) D1(Y )) 

JS(D0(Y ) D1(Y )) 
d2

1
2

1
2

H 2(D0(Y ) D1(Y ))

(cid:27)

= dTV(D0(Y ) D1(Y ))
= ∆BR(D0 D1) 

and this reduces to Theorem 3.1. Now we give a sketch of the proof for Theorem 3.2:

Proof Sketch of Theorem 3.2. We prove the three inequalities respectively. The total variation lower
dTV(g(cid:93)D0  g(cid:93)D1) from Corollary 3.2. To prove the Jensen-Shannon lower bound  realize that
dJS(· ·) is a distance metric over probability distributions. Combining with the inequality

bound follows the same idea as the proof of Theorem 3.1 and the inequality dTV(D0((cid:98)Y ) D1((cid:98)Y )) ≤
dJS(D0((cid:98)Y ) D1((cid:98)Y )) ≤ dJS(g(cid:93)D0  g(cid:93)D1) from Corollary 3.2  we have:

Now by Lin’s lemma [23  Theorem 3]  for any two distributions P and Q  we have d2
dTV(P Q). Combine Lin’s lemma with Lemma 3.1  we get the following lower bound:

dJS(D0(Y ) D1(Y )) ≤ dJS(D0(Y ) D0((cid:98)Y )) + dJS(g(cid:93)D0  g(cid:93)D1) + dJS(D1((cid:98)Y ) D1(Y )).
(cid:112)ErrD0(h ◦ g) +(cid:112)ErrD1(h ◦ g) ≥ dJS(D0(Y ) D1(Y )) − dJS(g(cid:93)D0  g(cid:93)D1).
(cid:113)
2(cid:0)ErrD0 (h ◦ g) + ErrD1 (h ◦ g)(cid:1) ≥(cid:112)ErrD0 (h ◦ g) +(cid:112)ErrD1(h ◦ g).

Apply the AM-GM inequality  we can further bound the L.H.S. by

JS(P Q) ≤

Under the assumption that dJS(g(cid:93)D0  g(cid:93)D1) ≤ dJS(D0(Y ) D1(Y ))  taking a square at both sides
then completes the proof for the second inequality. The proof for Hellinger’s lower bound follows
use the fact that H 2(P Q) ≤ dTV(P Q) ≤ √
exactly as the one for Jensen-Shannon’s lower bound  except that instead of Lin’s lemma  we need to
(cid:4)

2H(P Q)  ∀P Q.

6

As a simple corollary of Theorem 3.2  the following result shows how approximate DP (in terms of
the DP gap) helps to reconcile the tradeoff between fairness and utility:

Corollary 3.3. Let (cid:98)Y = h(g(X)) be the predictor  then ErrD0(h ◦ g) + ErrD1 (h ◦ g) ≥
∆BR(D0 D1) − ∆DP((cid:98)Y ).

In a sense Corollary 3.3 means that in order to lower the joint error  the DP gap of the predictor cannot
be too small. Of course  since the above inequality is a lower bound  it only serves as a necessary
condition for small joint error. Hence an interesting question would be to ask whether it is possible to
have a sufﬁcient condition that guarantees a small joint error such that the DP gap of the predictor is
no larger than that of the perfect predictor  i.e.  ∆BR(D0 D1). We leave this as a future work.

3.3 Fair Representations Lead to Accuracy Parity

In the previous sections we prove a family of information-theoretic lower bounds that demonstrate an
inherent tradeoff between fair representations and joint error across groups. A natural question to ask
then  is  what kind of parity can fair representations bring us? To complement our negative results 
in this section we show that learning group-invariant representations help to reduce discrepancy of
errors (utilities) across groups.
First of all  since we work under the stochastic setting where Da is a joint distribution over X and
Y conditioned on A = a  then any function mapping h : X → Y will inevitably incur an error due
to the noise existed in the distribution Da. Formally  for a ∈ {0  1}  deﬁne the optimal function
a : X → Y under the absolute error to be h∗
a(X) := mDa (Y | X)  where mDa (Y | X) denotes
h∗
the median of Y given X under distribution Da. Now deﬁne the noise of distribution Da to be
nDa := EDa [|Y − h∗
a(X)|]. With these notations  we are now ready to present the following theorem:
Theorem 3.3. For any hypothesis H (cid:51) h : X → Y  the following inequality holds:

|ErrD0(h) − ErrD1(h)| ≤ nD0 + nD1 + dTV(D0(X) D1(X))
1|]  ED1[|h∗

+ min{ED0[|h∗

0 − h∗

0 − h∗

1|]} .

|ErrD0(h) − ErrD1(h)| ≤ dTV(D0(X) D1(X)) + min{ED0[|h∗
0 and h∗

Remark Theorem 3.3 upper bounds the discrepancy of accuracy across groups by three terms: the
noise  the distance of representations across groups and the discrepancy of optimal decision functions.
In an ideal setting where both distributions are noiseless  i.e.  same people in the same group are
always treated equally  the upper bound simpliﬁes to the latter two terms:
0 − h∗
If we further require that the optimal decision functions h∗
1 are close to each other  i.e. 
optimal decisions are insensitive to the group membership  then Theorem 3.3 implies that a sufﬁcient
condition to guarantee accuracy parity is to ﬁnd group-invariant representation that minimizes
dTV(D0(X) D1(X)). We now present the proof for Theorem 3.3:
Proof of Theorem 3.3. First  we show that for a ∈ {0  1}  ErrDa (h) cannot be too large if h is close
to h∗
a:
|ErrDa (h) − nDa| = |ErrDa (h) − ErrDa (h∗
a(X)|] 

a)| =(cid:12)(cid:12)EDa [|Y − h(X)|] − EDa [|Y − h∗

≤ EDa [|h(X) − h∗

1|]  ED1[|h∗

where the inequality is due to triangular inequality. Next  we bound |ErrD0(h) − ErrD1(h)| by:

0 − h∗

1|]} .

0(X)|] − ED1 [|h(X) − h∗

In order to show this  deﬁne εa(h  h(cid:48)) := EDa [|h(X) − h(cid:48)(X)|] so that

|ErrD0 (h) − ErrD1(h)| ≤ nD0 + nD1 +(cid:12)(cid:12)ED0[|h(X) − h∗
(cid:12)(cid:12)ED0[|h(X) − h∗
1)(cid:12)(cid:12)  realize that |h(X) − h∗
To bound(cid:12)(cid:12)ε0(h  h∗
(cid:12)(cid:12)ε0(h  h∗
1)(cid:12)(cid:12) =(cid:12)(cid:12)ε0(h  h∗
≤(cid:12)(cid:12)ε0(h  h∗

0) − ε1(h  h∗
0) − ε1(h  h∗

1(X)|](cid:12)(cid:12) =(cid:12)(cid:12)ε0(h  h∗
1)(cid:12)(cid:12) +(cid:12)(cid:12)ε0(h  h∗

0(X)|] − ED1 [|h(X) − h∗

0) − ε0(h  h∗
0) − ε0(h  h∗
1) + dTV(D0(X) D1(X)) 

≤ ε0(h∗

0  h∗

0) − ε1(h  h∗

a(X)| ∈ {0  1}. On one hand  we have:
1) + ε0(h  h∗

1) − ε1(h  h∗
1) − ε1(h  h∗

a(X)|](cid:12)(cid:12)
1(X)|](cid:12)(cid:12).
1)(cid:12)(cid:12).
1)(cid:12)(cid:12)
1)(cid:12)(cid:12)

7

where the last inequality is due to(cid:12)(cid:12)ε0(h  h∗
1)(cid:12)(cid:12) ≤ supE |D0(E) − D1(E)| = dTV(D0 D1). Similarly  by subtracting and adding back ε1(h  h∗
instead  we can also show that(cid:12)(cid:12)ε0(h  h∗
1)(cid:12)(cid:12) ≤ min{ε0(h∗

1)(cid:12)(cid:12) =(cid:12)(cid:12)D0(|h − h∗
1)(cid:12)(cid:12) ≤ ε1(h∗

Combine the above two inequalities yielding:

1) + dTV(D0(X) D1(X)).

1| = 1) − D1(|h − h∗

(cid:12)(cid:12)ε0(h  h∗

1)} + dTV(D0(X) D1(X)).
Incorporating the two noise terms back to the above inequality then completes the proof.

0) − ε1(h  h∗

1)  ε1(h∗

0  h∗

0  h∗

0) − ε1(h  h∗

0  h∗

1) − ε1(h  h∗

1| =
0)

(cid:4)

4 Empirical Validation

Our theoretical results on the lower bound imply that over-training the feature transformation function
to achieve group-invariant representations will inevitably lead to large joint errors. On the other
hand  our upper bound also implies that group-invariant representations help to achieve accuracy
parity. To verify these theoretical implications  in this section we conduct experiments on a real-world
benchmark dataset  the UCI Adult dataset  to present empirical results with various metrics.

Dataset The Adult dataset contains 30 162/15 060 training/test instances for income prediction.
Each instance in the dataset describes an adult from the 1994 US Census. Attributes include gender 
education level  age  etc. In this experiment we use gender (binary) as the sensitive attribute  and we
preprocess the dataset to convert categorical variables into one-hot representations. The processed
data contains 114 attributes. The target variable (income) is also binary: 1 if ≥ 50K/year otherwise 0.
For the sensitive attribute A  A = 0 means Male otherwise Female. In this dataset  the base rates
across groups are different: Pr(Y = 1 | A = 0) = 0.310 while Pr(Y = 1 | A = 1) = 0.113. Also 
the group ratios are different: Pr(A = 0) = 0.673.

Experimental Protocol To validate the effect of learning group-invariant representations with
adversarial debiasing techniques [5  26  34]  we perform a controlled experiment by ﬁxing the
baseline network architecture to be a three hidden-layer feed-forward network with ReLU activations.
The number of units in each hidden layer are 500  200  and 100  respectively. The output layer
corresponds to a logistic regression model. This baseline without debiasing is denoted as NoDebias.
For debiasing with adversarial learning techniques  the adversarial discriminator network takes
the feature from the last hidden layer as input  and connects it to a hidden-layer with 50 units 
followed by a binary classiﬁer whose goal is to predict the sensitive attribute A. This model is
denoted as AdvDebias. Compared with NoDebias  the only difference of AdvDebias in terms of
objective function is that besides the cross-entropy loss for target prediction  the AdvDebias also
contains a classiﬁcation loss from the adversarial discriminator to predict the sensitive attribute
A. In the experiment  all the other factors are ﬁxed to be the same between these two methods 
including learning rate  optimization algorithm  training epoch  and also batch size. To see how the
adversarial loss affects the joint error  the demographic parity as well as the accuracy parity  we vary
the coefﬁcient ρ for the adversarial loss between 0.1  1.0  5.0 and 50.0.
Results and Analysis The experimental results are listed in Table 2. Note that in the table |ErrD0 −
∆DP((cid:98)Y ) measures the closeness of the classiﬁer to satisfy demographic parity. From the table  it is
ErrD1| could be understood as measuring an approximate version of accuracy parity  and similarly
∆DP((cid:98)Y ) is drastically decreasing with the increasing of ρ. Furthermore  |ErrD0 − ErrD1| is also
gradually decreasing  but much slowly than ∆DP((cid:98)Y ). This is due to the existing noise in the data

then clear that with increasing ρ  both the overall error ErrD (sensitive to the marginal distribution of
A) and the joint error ErrD0 + ErrD1 (insensitive to the imbalance of A) are increasing. As expected 

as well as the shift between the optimal decision functions across groups  as indicated by our upper
bound. To conclude  all the empirical results are consistent with our theoretical ﬁndings.

5 Related Work

Fairness Frameworks Two central notions of fairness have been extensively studied  i.e.  group
fairness and individual fairness. In a seminal work  Dwork et al. [10] deﬁne individual fairness
as a measure of smoothness of the classiﬁcation function. Under the assumption that number of

8

Table 2: Adversarial debiasing on demographic parity  joint error across groups  and accuracy parity.

NoDebias
AdvDebias  ρ = 0.1
AdvDebias  ρ = 1.0
AdvDebias  ρ = 5.0
AdvDebias  ρ = 50.0

ErrD
0.157
0.159
0.162
0.166
0.201

ErrD0 + ErrD1

0.275
0.278
0.286
0.295
0.360

|ErrD0 − ErrD1| ∆DP((cid:98)Y )

0.115
0.116
0.106
0.106
0.112

0.189
0.190
0.113
0.032
0.028

individuals is ﬁnite  the authors proposed a linear programming framework to maximize the utility
under their fairness constraint. However  their framework requires apriori a distance function that
computes the similarity between individuals  and their optimization formulation does not produce
an inductive rule to generalize to unseen data. Based on the deﬁnition of positive rate parity  Hardt
et al. [14] proposed a post-processing method to achieve fairness by taking as input the prediction
and the sensitive attribute. In a concurrent work  Kleinberg et al. [21] offer a calibration technique
to achieve the corresponding fairness criterion as well. However  both of the aforementioned two
approaches require sensitive attribute during the inference phase  which is not available in many
real-world scenarios.
Regularization Techniques The line of work on fairness-aware learning through regularization
dates at least back to Kamishima et al. [19]  where the authors argue that simple deletion of sensitive
features in data is insufﬁcient for eliminating biases in automated decision making  due to the possible
correlations among attributes and sensitive information [25]. In light of this  the authors proposed a
prejudice remover regularizer that essentially penalizes the mutual information between the predicted
goal and the sensitive information. In a more recent approach  Zafar et al. [31] leveraged a measure
of decision boundary fairness and incorporated it via constraints into the objective function of logistic
regression as well as support vector machines. As discussed in Section 2  both approaches essentially
reduce to achieving demographic parity through regularization.
Representation Learning In a pioneer work  Zemel et al. [33] proposed to preserve both group
and individual fairness through the lens of representation learning  where the main idea is to ﬁnd a
good representation of the data with two competing goals: to encode the data for utility maximization
while at the same time to obfuscate any information about membership in the protected group.
Due to the power of learning rich representations offered by deep neural nets  recent advances in
building fair automated decision making systems focus on using adversarial techniques to learn
fair representation that also preserves enough information for the prediction vendor to achieve his
utility [1  5  11  24  30  34  37]. Madras et al. [26] further extended this approach by incorporating
reconstruction loss given by an autoencoder into the objective function to preserve demographic
parity  equalized odds  and equal opportunity.

6 Conclusion

In this paper we theoretically and empirically study the important problem of quantifying the tradeoff
between utility and fairness in learning group-invariant representations. Speciﬁcally  we prove a
novel lower bound to characterize the tradeoff between demographic parity and the joint utility across
different population groups when the base rates differ between groups. In particular  our results
imply that any method aiming to learn fair representations admits an information-theoretic lower
bound on the joint error  and the better the representation  the larger the joint error. Complementary
to our negative results  we also show that learning fair representations leads to accuracy parity if
the optimal decision functions across different groups are close. These theoretical ﬁndings are also
conﬁrmed empirically on real-world datasets. We believe our results take an important step towards
better understanding the tradeoff between utility and different notions of fairness. Inspired by our
lower bound  one interesting direction for future work is to design instance-weighting algorithm to
balance the base rates during representation learning.

Acknowledgments
HZ and GG would like to acknowledge support from the DARPA XAI project  contract
#FA87501720152 and an Nvidia GPU grant. HZ would also like to thank Jianfeng Chi for helpful
discussions on the relationship between algorithmic fairness and privacy-preservation learning.

9

References
[1] Tameem Adel  Isabel Valera  Zoubin Ghahramani  and Adrian Weller. One-network adversarial

fairness. In 33rd AAAI Conference on Artiﬁcial Intelligence  2019.

[2] Syed Mumtaz Ali and Samuel D Silvey. A general class of coefﬁcients of divergence of one
distribution from another. Journal of the Royal Statistical Society: Series B (Methodological) 
28(1):131–142  1966.

[3] Solon Barocas and Andrew D Selbst. Big data’s disparate impact. Calif. L. Rev.  104:671  2016.

[4] Richard Berk  Hoda Heidari  Shahin Jabbari  Michael Kearns  and Aaron Roth. Fairness in
criminal justice risk assessments: The state of the art. Sociological Methods & Research  page
0049124118782533  2018.

[5] Alex Beutel  Jilin Chen  Zhe Zhao  and Ed H Chi. Data decisions and theoretical implications

when adversarially learning fair representations. arXiv preprint arXiv:1707.00075  2017.

[6] Toon Calders  Faisal Kamiran  and Mykola Pechenizkiy. Building classiﬁers with independency
constraints. In 2009 IEEE International Conference on Data Mining Workshops  pages 13–18.
IEEE  2009.

[7] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. Big data  5(2):153–163  2017.

[8] Imre Csiszár. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der
ergodizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl.  8:85–108 
1964.

[9] Imre Csiszár. Information-type measures of difference of probability distributions and indirect

observation. studia scientiarum Mathematicarum Hungarica  2:229–318  1967.

[10] Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science
conference  pages 214–226. ACM  2012.

[11] Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv

preprint arXiv:1511.05897  2015.

[12] Yaroslav Ganin  Evgeniya Ustinova  Hana Ajakan  Pascal Germain  Hugo Larochelle  François
Laviolette  Mario Marchand  and Victor Lempitsky. Domain-adversarial training of neural
networks. The Journal of Machine Learning Research  17(1):2096–2030  2016.

[13] Jihun Hamm. Minimax ﬁlter: Learning to preserve privacy from inference attacks. The Journal

of Machine Learning Research  18(1):4704–4734  2017.

[14] Moritz Hardt  Eric Price  Nati Srebro  et al. Equality of opportunity in supervised learning. In

Advances in neural information processing systems  pages 3315–3323  2016.

[15] James E Johndrow  Kristian Lum  et al. An algorithm for removing sensitive information:
application to race-independent recidivism prediction. The Annals of Applied Statistics  13(1):
189–220  2019.

[16] Melvin Johnson  Mike Schuster  Quoc V Le  Maxim Krikun  Yonghui Wu  Zhifeng Chen  Nikhil
Thorat  Fernanda Viégas  Martin Wattenberg  Greg Corrado  et al. Google’s multilingual neural
machine translation system: Enabling zero-shot translation. Transactions of the Association for
Computational Linguistics  5:339–351  2017.

[17] Faisal Kamiran and Toon Calders. Classifying without discriminating. In 2009 2nd International

Conference on Computer  Control and Communication  pages 1–6. IEEE  2009.

[18] Toshihiro Kamishima  Shotaro Akaho  and Jun Sakuma. Fairness-aware learning through regu-
larization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops 
pages 643–650. IEEE  2011.

10

[19] Toshihiro Kamishima  Shotaro Akaho  Hideki Asoh  and Jun Sakuma. Fairness-aware classiﬁer
with prejudice remover regularizer. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases  pages 35–50. Springer  2012.

[20] Mohammadali Khosravifard  Dariush Fooladivanda  and T Aaron Gulliver. Conﬂiction of the
convexity and metric properties in f-divergences. IEICE Transactions on Fundamentals of
Electronics  Communications and Computer Sciences  90(9):1848–1853  2007.

[21] Jon Kleinberg  Sendhil Mullainathan  and Manish Raghavan. Inherent trade-offs in the fair

determination of risk scores. arXiv preprint arXiv:1609.05807  2016.

[22] Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information

theory. IEEE Transactions on Information Theory  52(10):4394–4412  2006.

[23] Jianhua Lin. Divergence measures based on the shannon entropy.

Information theory  37(1):145–151  1991.

IEEE Transactions on

[24] Christos Louizos  Kevin Swersky  Yujia Li  Max Welling  and Richard Zemel. The variational

fair autoencoder. arXiv preprint arXiv:1511.00830  2015.

[25] Kristian Lum and James Johndrow. A statistical framework for fair predictive algorithms. arXiv

preprint arXiv:1610.08077  2016.

[26] David Madras  Elliot Creager  Toniann Pitassi  and Richard Zemel. Learning adversarially
fair and transferable representations. In International Conference on Machine Learning  pages
3381–3390  2018.

[27] Arvind Narayanan. Translation tutorial: 21 fairness deﬁnitions and their politics. In Proc. Conf.

Fairness Accountability Transp.  New York  USA  2018.

[28] Executive Ofﬁce of the President. Big data: A report on algorithmic systems  opportunity  and

civil rights. Executive Ofﬁce of the President  2016.

[29] Geoff Pleiss  Manish Raghavan  Felix Wu  Jon Kleinberg  and Kilian Q Weinberger. On fairness
and calibration. In Advances in Neural Information Processing Systems  pages 5680–5689 
2017.

[30] Jiaming Song  Pratyusha Kalluri  Aditya Grover  Shengjia Zhao  and Stefano Ermon. Learning
controllable fair representations. In Artiﬁcial Intelligence and Statistics  pages 2164–2173 
2019.

[31] Muhammad Bilal Zafar  Isabel Valera  Manuel Gomez Rodriguez  and Krishna P Gummadi.
Fairness constraints: Mechanisms for fair classiﬁcation. arXiv preprint arXiv:1507.05259 
2015.

[32] Muhammad Bilal Zafar  Isabel Valera  Manuel Gomez Rodriguez  and Krishna P Gummadi.
Fairness beyond disparate treatment & disparate impact: Learning classiﬁcation without dis-
parate mistreatment. In Proceedings of the 26th International Conference on World Wide Web 
pages 1171–1180. International World Wide Web Conferences Steering Committee  2017.

[33] Rich Zemel  Yu Wu  Kevin Swersky  Toni Pitassi  and Cynthia Dwork. Learning fair representa-

tions. In International Conference on Machine Learning  pages 325–333  2013.

[34] Brian Hu Zhang  Blake Lemoine  and Margaret Mitchell. Mitigating unwanted biases with
adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI  Ethics  and
Society  pages 335–340. ACM  2018.

[35] Han Zhao  Jianfeng Chi  Yuan Tian  and Geoffrey J. Gordon. Adversarial privacy preservation

under attribute inference attack. arXiv preprint arXiv:1906.07902  2019.

[36] Han Zhao  Remi Tachet des Combes  Kun Zhang  and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. In International Conference on Machine Learning  2019.
[37] Han Zhao  Amanda Coston  Tameem Adel  and Geoffrey J. Gordon. Conditional learning of

fair representations. arXiv preprint arXiv:1910.07162  2019.

[38] Indre Zliobaite. On the relation between accuracy and fairness in binary classiﬁcation. arXiv

preprint arXiv:1505.05723  2015.

11

,Han Zhao
Geoff Gordon