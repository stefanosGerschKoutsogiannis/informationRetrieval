2010,Scrambled Objects for Least-Squares Regression,We consider least-squares regression using a randomly generated subspace G_P\subset F of finite dimension P  where F is a function space of infinite dimension  e.g.~L_2([0 1]^d).  G_P is defined as the span of P random features  that are linear combinations of the basis functions of F weighted by random Gaussian i.i.d.~coefficients. In particular  we consider multi-resolution random combinations at all scales of a given mother function   such as a hat function or a wavelet. In this latter case  the resulting Gaussian objects are called {\em scrambled wavelets} and we show that they enable to approximate functions in Sobolev spaces H^s([0 1]^d). As a result  given N data  the least-squares estimate \hat g built from P scrambled wavelets has excess risk ||f^* - \hat g||_\P^2 = O(||f^*||^2_{H^s([0 1]^d)}(\log N)/P + P(\log N )/N) for target functions f^*\in H^s([0 1]^d) of smoothness order s>d/2. An interesting aspect of the resulting bounds is that they do not depend on the distribution \P from which the data are generated  which is important in a statistical regression setting considered here. Randomization enables to adapt to any possible distribution.   We conclude by describing an efficient numerical implementation using lazy expansions with numerical complexity \tilde O(2^d N^{3/2}\log N + N^2)  where d is the dimension of the input space.,Scrambled Objects for Least-Squares Regression

Odalric-Ambrym Maillard and R´emi Munos

SequeL Project  INRIA Lille - Nord Europe  France

{odalric.maillard  remi.munos}@inria.fr

Abstract

We consider least-squares regression using a randomly generated subspace GP ⊂
F of ﬁnite dimension P   where F is a function space of inﬁnite dimension 
e.g. L2([0  1]d). GP is deﬁned as the span of P random features that are linear
combinations of the basis functions of F weighted by random Gaussian i.i.d. co-
efﬁcients. In particular  we consider multi-resolution random combinations at all
scales of a given mother function  such as a hat function or a wavelet. In this latter
case  the resulting Gaussian objects are called scrambled wavelets and we show
that they enable to approximate functions in Sobolev spaces H s([0  1]d). As a

result  given N data  the least-squares estimate(cid:98)g built from P scrambled wavelets
has excess risk ||f∗ −(cid:98)g||2P = O(||f∗||2

H s([0 1]d)(log N)/P + P (log N)/N) for
target functions f∗ ∈ H s([0  1]d) of smoothness order s > d/2. An interesting
aspect of the resulting bounds is that they do not depend on the distribution P from
which the data are generated  which is important in a statistical regression setting
considered here. Randomization enables to adapt to any possible distribution.
We conclude by describing an efﬁcient numerical implementation using lazy ex-
pansions with numerical complexity ˜O(2dN 3/2 log N + N 2)  where d is the di-
mension of the input space.

1 Introduction

We consider ordinary least-squares regression using randomly generated feature spaces. Let us ﬁrst
describe the general regression problem: we observe data DN = ({xn  yn}1≤n≤N ) (with xn ∈ X a
compact subset of Rd  and yn ∈ R)  assumed to be independently and identically distributed (i.i.d.)
with xn ∼ P and
where f∗ is the (unknown) target function  such that ||f∗||∞ ≤ L  and ηn is a centered  independent
noise of variance bounded by σ2. We assume that L and σ are known.
Now  for a given class of functions F  and f ∈ F  we deﬁne the empirical ‘2-error

yn = f∗(xn) + ηn 

N(cid:88)

n=1

LN (f) def=

1
N

[yn − f(xn)]2 

and the generalization error

The goal is to return a regression function (cid:98)f ∈ F with lowest possible generalization error L((cid:98)f).
The excess risk L((cid:98)f)−L(f∗) = ||f∗−(cid:98)f||P (where ||g||2P = EX∼P[g(X)2]) measures the closeness

L(f) def= EX Y [(Y − f(X))2].

to optimality.
In this paper we consider inﬁnite dimensional spaces F that are generated by a denumerable family
of functions {ϕi}i≥1  called initial features (such as wavelets). We will assume that f∗ ∈ F.

1

Since F is an inﬁnite dimensional space  the empirical risk minimizer in F is certainly subject to
overﬁtting. Traditional methods to circumvent this problem have considered penalization  i.e. one
searches for a function in F which minimizes the empirical error plus a penalty term  for example
p for p = 1 or 2  where λ is a parameter and usual choices for the

(cid:98)f = arg minf∈F LN (f) + λ||f||p

norm are ‘2 (ridge-regression [17]) and ‘1 (LASSO [16]).
In this paper we follow an alternative approach introduced in [10]  called Compressed Least Squares
Regression  which considers generating randomly a subspace GP (of ﬁnite dimension P ) of F  and
then returning the empirical risk minimizer in GP   i.e. arg ming∈GP LN (g). This previous work
considered the case when F is of ﬁnite dimension. Here we consider speciﬁc cases of inﬁnite
dimensional spaces F and provide a characterization of the resulting approximation spaces.

2 Regression with random spaces

Let us brieﬂy recall the method described in [10] and extend it to the case of inﬁnite dimensional
spaces F. In this paper we assume that the set of features (ϕi)i≥1 are continuous and are such that 
(1)

||ϕ(x)||2 < ∞  where ||ϕ(x)||2 def=

ϕi(x)2.

sup
x∈X

(cid:88)

i≥1

Examples of feature spaces satisfying this property include rescaled wavelets and will be described
in Section 3.
The random subspace GP is generated by building a set of P random features (ψp)1≤p≤P deﬁned
as linear combinations of the initial features {ϕi}1≥1 weighted by random coefﬁcients:

ψp(x) def=

Ap iϕi(x)  for 1 ≤ p ≤ P 

(2)

(cid:88)

i≥1

where the (inﬁnitely many) coefﬁcients Ap i are drawn i.i.d. from a centered distribution with vari-
ance 1/P . Here we explicitly choose a Gaussian distribution N (0  1/P ). Such a deﬁnition of the
features ψp as an inﬁnite sum of random variable is not obvious (this is called an expansion of a
Gaussian object) and we refer to [11] for elements of theory about Gaussian objects and for the
expansion of a Gaussian object. It is shown that under assumption (1)  the random features are well
deﬁned. Actually  they are random samples of a centered Gaussian process indexed by the space X
with covariance structure given by 1
i uivi for
two square-summable sequences u and v. Indeed  EAp[ψp(x)] = 0  and

P hϕ(x)  ϕ(x0)i  where we use the notation hu  vi =

(cid:80)

CovAp(ψp(x)  ψp(x0)) = EAp[ψp(x)ψp(x0)] =

1
P

ϕi(x)ϕi(x0) =

1
P

hϕ(x)  ϕ(x0)i .

(cid:88)

i≥1

The continuity of the initial features (ϕi) guarantees that there exists a continuous version of the
process ψp which is thus a Gaussian process.
Then we deﬁne GP ⊂ F to be the (random) vector space spanned by those features  i.e.

P(cid:88)

GP

def= {gβ(x) def=

βpψp(x)  β ∈ RP}.

p=1

Now  the least-squares estimate gbβ ∈ GP is the function in GP with minimal empirical error  i.e.
and is the solution of a least-squares regression problem  i.e. (cid:98)β = Ψ†Y ∈ RP   where Ψ is the
inverse of Ψ1. The ﬁnal prediction function(cid:98)g(x) is the truncation (to the threshold ±L) of gbβ  i.e.
(cid:98)g(x) def= TL[gbβ(x)]  where TL(u) def=

gbβ = arg min
(cid:189)

N × P -matrix composed of the elements: Ψn p

def= Ψp(xn)  and Ψ† is the Moore-Penrose pseudo-

if |u| ≤ L 
u
L sign(u) otherwise.

LN (gβ) 

gβ∈GP

(3)

Next  we provide bounds on the approximation error of f∗ in GP and deduce excess risk bounds.

1In the full rank case when N ≥ P   Ψ† = (ΨT Ψ)−1ΨT

2

2.1 Approximation error

We now extend the result of [10] and derive approximation error bounds both in expectation and in
high probability. We restrict the set of target functions to belong to the approximation space K ⊂ F
(also identiﬁed to the kernel space associated to the expansion of a Gaussian object):

K def= {fα ∈ F ||α||2 def=

i < ∞}.
α2

(4)

(cid:88)

i≥1

(cid:112)
(cid:161)

Remark 1. This space may be seen from two equivalent points of view: either as a set of functions
that are random linear combinations of the initial features  or a set of functions that are the expec-
tation of some random processes (interpretation in terms of kernel space). We will not develop the
related theory of Gaussian processes here but we refer the reader interested in the construction of
kernel spaces to [11]

(cid:80)
i αiϕi ∈ K. Write g∗ the projection of fα onto GP w.r.t. the norm || · ||P  i.e. g∗ =
Let fα =
arg ming∈GP ||fα − g||P  and ¯g∗ = TLg∗ its truncation at the threshold L ≥ ||fα||∞. Notice that
due to the randomness of the features (ψp)1≤p≤P of GP   the space GP is also random  and so is ¯g∗.
The following result provides bounds for the approximation error ||fα − ¯g∗||P both in expectation
and in high probability.
Theorem 1. For any η > 0  whenever P ≥ c1 log(P γ2
1 − η (w.r.t. the choice of the random subspace GP ) 
||α||2 supx ||ϕ(x)||2

log(1/η)/η)  we have with probability

g∈G ||f∗ − TL(g)||2P ≤ c2
inf
||α|| supx ||ϕ(x)|| and c1  c2 are some universal constants (see [11]). A similar result holds

where γ =
in expectation.
This result relies on the property that inf g∈GP ||fα − g||P ≤ ||fα − gAα||P and that gAα  considered
as a random variable w.r.t. the choice of the random elements A  concentrates around fα (in || · ||P-
norm) when P increases. Indeed  gAα(x) = (Aα) · ψ(x) = (Aα) · (Aϕ(x)) which is close to α ·
ϕ(x) = fα(s)  since inner-products are approximately preserved through random projections (from
a variant of Johnson-Lindenstrauss (JL) Lemma). The proof of Theorem 1 (provided in Appendix of
J from P  applying JL Lemma at those points
[11]) relies in generating auxiliary samples X0
and combining it with a Chernoff-Hoeffding bound for generalizing the result to hold in ||·||P-norm.
Remark 2. An interesting property of this result is that the bound does not depend on the distribution
P. This distribution is used in the deﬁnition of the norm || · ||P to assess how well a function space
GP can approximate a function fα. It is thus surprising that the measure P does not appear in the
bound. Actually  the fact that GP is random enables it to be close to fα (in high probability or in
expectation) whatever the measure P is. This is especially interesting in a regression setting where
the distribution P from which the data are generated is not known in advance.

1  . . .   X0

1 + log(P γ2

log(1/η)/η)

(cid:112)

(cid:162)

P

L

 

2.2 Excess risk bounds

We now combine the approximation error bound from Theorem 1 with usual estimation error bounds
that our prediction function(cid:98)g is the truncation(cid:98)g def= TL[gbβ] of the (ordinary) least-squares estimate
i ϕi ∈ K. Remember
for linear spaces (see e.g. [7]). Let us consider a target function f∗ =
gbβ (empirical risk minimizer in the random space GP ) deﬁned by (3).

(cid:80)
i α∗

We now provide upper bounds (both in expectation and in high probability) on the excess risk for
the least-squares estimate using random subspaces (the proof is given in [11]).
Theorem 2. Whenever P ≥ c3 log N  we have the following bound in expectation (w.r.t. all sources
of randomness  i.e. input data  noise  and the choice of the random features):
||α∗||2 sup

(5)
Now  for any η > 0  whenever P ≥ c5 log(N/η)  we have the following bound in high probability
(w.r.t. the choice of the random features)  where c3  c4  c5  c6 are universal constant (see [11]):

EGP  X Y ||f∗ −(cid:98)g||2P ≤ c4
(cid:161)
EX Y ||f∗ −(cid:98)g||2P ≤ c6

||ϕ(x)||2(cid:162)
||ϕ(x)||2(cid:162)

+ L2 P log N

+ L2 P log N

σ2 P
N

log N/η

log N

(cid:161)

(6)

+

+

N

P

 

.

x

||α∗||2 sup

x

P

σ2 P
N

N

3

The results of Theorems 1 and 2 say that if the term ||α∗||2 supx ||ϕ(x)||2 is small  then the least-
squares estimate in the random subspace GP has low excess risk. The question we wish to address
now is whether we can deﬁne spaces for which this is the case. In the next section we provide two
examples of feature spaces and characterize the space of functions for which this term is controlled.

3 Regression with Scrambled Objects

In the two examples provided below we consider (inﬁnitely many) initial features that are trans-
lations and rescaling of a given mother function (which is assumed to be continuous) at all scales.
Thus each random feature ψp is a Gaussian object based on a multi-scale scheme built from an object
(the mother function)  and will be called a “scrambled object”  to refer to the disorderly construction
of this multi-resolution random process.
We thus propose to solve the regression problem by ordinary Least Squares on the (random) approx-
imation space deﬁned by the span of P such scrambled objects. In the next sections we provide two
examples. The ﬁrst one considers the case when the mother function is a hat function and we show
that the corresponding scrambled objects are Brownian motions. The second example considers
wavelets. The proof of bounds (7) and (8) can be found in [11].

3.1 Brownian motions and Brownian Sheets
Dimension 1: We start with the 1-dimensional case where X = [0  1]. Let us choose as object
(mother function) the hat function Λ(x) = xI[0 1/2[ + (1 − x)I[1/2 1[. We deﬁne the (inﬁnite) set
of initial features as translated and rescaled hat functions: Λj l(x) = 2−j/2Λ(2jx − l) for any scale
j ≥ 1 and translation index 0 ≤ l ≤ 2j − 1. We also write Λ0 0(x) = x. This deﬁnes a basis of the
space of continuous functions C0([0  1]) equal to 0 at 0 (introduced by Faber in 1910  and known as
the Schauder basis  see [8] for an interesting overview). Those functions are indexed by the scale j
and translation index l  but all functions may be equivalently indexed by a unique index i ≥ 1.
We have the property that the random features ψp(x)  deﬁned as linear combinations of those hat
functions weighted by Gaussian i.i.d. random numbers  are Brownian motions (See Example 1 of
[11] for the proof). In addition  we can characterize the corresponding kernel space K  which is the
Sobolev space H 1([0  1]) of order 1 (space of functions which have a weak derivative in L2([0  1])).

Dimension d: For the extension to dimension d  we deﬁne the initial features as the tensor
product ϕj l of one-dimensional hat functions (thus j and l are multi-indices). The random fea-
tures ψp(x) are Brownian sheets (extensions of Brownian motions to several dimensions) and
the corresponding kernel K is the so-called Cameron-Martin space [9]  endowed with the norm
||f||K = ||
||L2([0 1]d) (see also Example 1 of [11] for the proof). One may interpret
this space as the set of functions which have a d-th order crossed (weak) derivative
in
L2([0  1]d)  vanishing on the “left” boundary (edges containing 0) of the unit d-dimensional cube.
Note that in dimension d > 1  this space differs from the Sobolev space H 1.

∂x1...∂xd

∂df

∂df

∂x1...∂xd

Regression with Brownian Sheets: When one uses Brownian sheets for regression with a target
i ϕi that lies in the Cameron-Martin space K deﬁned previously (i.e. such that
function f∗ =
||α∗|| < ∞)  then the term ||α∗||2 supx∈X ||ϕ(x)||2 that appears in Theorems 1 and 2 is bounded
as:

||α∗||2 sup
x∈X

||ϕ(x)||2 ≤ 2−d||f∗||2K.

(cid:80)
i α∗

Thus  from Theorem 2  ordinary least-squares performed on random subspaces spanned by P Brow-
nian sheets has an expected excess risk

(cid:180)

P +

log N

P

||f∗||2K

 

(7)

EGP  X Y ||f∗ −(cid:98)g||2P = O

(and a similar bound holds in high probability).

(cid:179)log N

N

4

3.2 Scrambled Wavelets in [0  1]d
We now introduce a second example built from a family of orthogonal wavelets ( ˜ϕε j l) ∈
C q([0  1]d) (where ε ∈ {0  1}d is a multi-index  j is a scale index  l a multi-index  see [2  12]
for details of the notations) with at least q > d/2 vanishing moments. Now for s ∈ (d/2  q)  we de-
ﬁne the initial features (ϕε j l) as the rescaled wavelets ( ˜ϕε j l)  i.e. ϕε j l
|| ˜ϕε j l||2 . Again 
the initial features may equivalently be indexed by a unique index i ≥ 1. The random features ψp
deﬁned from (2) are called “scrambled wavelets”. It can be shown that the resulting approximation
space K (i.e. {fα =
Regression with Scambled Wavelets: Assume that the mother wavelet ˜ϕ has compact support
i α∗
[0  1]d and is bounded by λ  and assume that the target function f∗ =
i ϕi lies in the Sobolev
space H s([0  1]d) with s > d/2 (i.e. such that ||α∗|| < ∞). Then  we have 

i αiϕi ||α|| < ∞) is the Sobolev space H s([0  1]d).
(cid:80)

def= 2−js

(cid:80)

˜ϕε j l

||α∗||2 sup
x∈X

||ϕ(x)||2 ≤ λ2d(2d − 1)
1 − 2−2(s−d/2)

||f∗||2

H s([0 1]d).

Thus from Theorem 2  ordinary least-squares performed on random subspaces spanned by P scram-
bled wavelets has an expected excess risk

EGP  X Y ||f∗ −(cid:98)g||2P = O

(cid:179)log N

N

(and a similar bound holds in high probability).

In both examples  by choosing P of order

N||f∗||K  one deduces the excess risk

√

E||f∗ −(cid:98)g||2P = O

(cid:179)||f∗||K log N

(cid:180)

.

√

N

(cid:180)

 

P +

log N

P

||f∗||2

H s([0 1]d)

(8)

(9)

3.3 Remark about randomized spaces

Note that the bounds on the excess risk obtained in (7)  (8)  and (9) do not depend on the distribution
P under which the data are generated. This is crucial in our setting since P is usually unknown. It
should be noticed that this property does not hold when one considers non-randomized approxima-
tion spaces. Indeed  it is relatively easy to exhibit a particularly well-chosen set of features ϕi that
will approximate functions in a given class using a particular measure P. For example when P = λ 
the Lebesgue measure  and f∗ ∈ H s([0  1]d) (with s > d/2)  then linear regression using wavelets
(with at least d/2 vanishing moments)  which form an orthonormal basis of L2 λ([0  1]d)  enables
to achieve a bound similar to (8). However  this is no more the case when P is not the Lebesgue
measure and it seems difﬁcult to modify the features ϕi in order to recover the same bound  even
when P is known. This seems to be even harder when P is arbitrary and not known in advance.
Randomization enables to deﬁne approximation spaces such that the approximation error (either in
expectation or in high probability on the choice of the random space) is controlled  whatever the
measure P used to assess the performance (even when P is unknown) is.
For illustration  consider a very peaky (a spot) distribution P in a high-dimensional space X . Reg-
ular linear approximation  say with wavelets (see e.g. [6])  will most probably miss the speciﬁc
characteristics of f∗ at the spot  since the ﬁrst wavelets have large support. On the contrary  scram-
bled wavelets  which are functions that contain (random combinations of) all wavelets  will be able
to detect correlations between the data and some high frequency wavelets  and thus discover relevant
features of f∗ at the spot. This is illustrated in the numerical experiment below.
Here P is a very peaky Gaussian distribution and f∗ is a 1-dimensional periodic function. We con-
sider as initial features (ϕi)i≥1 the set of hat functions deﬁned in Section 3.1. Figure 3.3 shows the
target function f∗  the distribution P  and the data (xn  yn)1≤n≤100 (left plots). The middle plots
motions). The right plots shows the least-squares estimate using the initial features (ϕi)1≤i≤40. The
top ﬁgures represent a high level view of the whole domain [0  1]. No method is able to learn f∗ on
the whole space (this is normal since the available data are only generated from a peaky distribu-
tion). The bottom ﬁgures shows a zoom [0.45  0.51] around the data. Least-squares regression using
scrambled objects is able to learn the structure of f∗ in terms of the measure P.

represents the least-squares estimate(cid:98)g using P = 40 scrambled objects (ψp)1≤p≤40 (here Brownian

5

Figure 1: LS estimate of f∗ using N = 100 data generated from a peaky distribution P (left plots) 
using 40 Brownian motions (ψp) (middle plots) and 40 hat functions (ϕi) (right plots). The bottom
row shows a zoom around the data.

4 Discussion

Minimax optimality: Note that although the rate ˜O(N−1/2) deduced in (9)  does not depend on
the dimension d of the input data X   it does not contradict the known minimax lower bounds  which
are Ω(N−2s/(2s+d)) for functions deﬁned over [0  1]d that possess s-degrees of smoothness (e.g. that
are s-times differentiable)  see e.g. Chapter 3 of [7]. Indeed  the kernel space K is composed of
functions whose order of smoothness may depend on d. For illustration  in the case of scrambled
wavelets  the kernel space is the Sobolev space H s([0  1]d) with s > d/2. Thus 2s/(2s + d) > 1/2.
Notice that if one considers wavelets with q vanishing moments  where q > d/2  then one may
choose s (such that q > s > d/2) arbitrarily close to d/2  and deduce that the excess risk rate
˜O(N−1/2) deduced from Theorem 2 is arbitrarily close to the minimax lower rate. Thus regression
using scrambled wavelets is minimax optimal (up to logarithmic factors).
Now  concerning Brownian sheets  we are not aware of minimax lower bounds for Cameron-Martin
spaces  thus we do not know whether regression using Brownian sheets is minimax optimal or not.

Links with RKHS Theory: There are strong links between the kernel space of Gaussian objects
(see eq.(4)) and Reproducing Kernel Hilbert Spaces (RKHS). We now remind two properties that
illustrate those links:

• Kernel spaces of Gaussian objects can be built using a Carleman operator  i.e. a linear injec-
tive mapping J : H 7→ S (where H is a Hilbert space) such that J(h)(t) =
Γt(s)h(s)ds
where (Γt)t is a collection of functions of H. There is a bijection between Carleman oper-
ators and the set of RKHSs [4  15].

(cid:82)

(cid:80)∞

• Expansion of a Mercer kernel. The expansion of a Mercer kernel k (i.e. when X is com-
i=1 λiei(x)ei(y) 
pact Haussdorff and k is a continuous kernel) is given by k(x  y) =
where (λi)i and (ei)i are the eigenvalues and eigenfunctions of the integral operator
Lk : L2 µ(X ) → L2 µ(X ) deﬁned by (Lk(f))(x) =
X k(x  y)f(y)dµ(y). The asso-
√
ciated RKHS is K = {f =
λiei  endowed
with the inner product hfα  fβi = hα  βil2. This space is thus also the kernel space of the
Gaussian object as deﬁned by (4).

i < ∞}  where ϕi =

(cid:80)

i αiϕi;

i α2

(cid:82)

(cid:80)

6

0.00.20.40.60.81.0-0.4-0.20.00.20.40.60.81.0Target function0.00.20.40.60.81.0-1.0-0.50.00.51.0Predicted function: BLSR_Hat0.00.20.40.60.81.0-0.6-0.4-0.20.00.20.40.60.81.0Predicted function: LSR_Hat0.450.460.470.480.490.500.51-0.4-0.20.00.20.40.60.81.0Target function0.450.460.470.480.490.500.51-1.0-0.50.00.51.0Predicted function: BLSR_Hat0.450.460.470.480.490.500.51-0.6-0.4-0.20.00.20.40.60.81.0Predicted function: LSR_HatThe expansion of a Mercer kernel gives an explicit construction of the functions of the RKHS.
However it may not be straightforward to compute the eigenvalues and eigenfunctions of the integral
operator Lk and thus the basis functions ϕi in the general case.
The approach described in this paper enables to choose explicitly the initial basis functions  and build
the corresponding kernel space. For example we have presented examples of expansions using multi-
resolution bases (such as hat functions and wavelets)  which is not easy to obtain from the Mercer
expansion. This is interesting because from the choice of the initial basis  we can characterize the
corresponding approximations spaces (e.g. Sobolev space in the case of wavelets). Another more
practical beneﬁt is that by using multi-resolution bases (with compact mother function)  we can
derive efﬁcient numerical implementations  as described in Section 5.

(cid:80)P

P

i.i.d∼ µ  there exist coefﬁcients (cp)p≤P such that (cid:98)f(x) =

(cid:82)
In [14  13]  the authors consider  for a given parameterized function Φ : X ×
Related works
(cid:82)
Θ → R bounded by 1  and a probability measure µ over Θ  the space F of functions f(x) =
µ(θ)| < ∞. They show that this is a dense subset
Θ α(θ)Φ(x  θ)dθ such that ||f||µ = supθ | α(θ)
Θ µ(θ)Φ(x  θ)Φ(y  θ)dθ  and that if f ∈ F  then with high
of the RKHS with kernel k(x  y) =
satisﬁes ||(cid:98)f − f||2
probability over (θp)p≤P
p=1 cpΦ(x  θp)
(cid:80)
2 ≤ O(||f||µ√
). The method is analogous to the construction of the empirical
estimates gAα ∈ GP of function fα ∈ K in our setting. Indeed we may formally identify Φ(x  θp)
with ψp(x) =
i Ap iϕi(x)  θp with the sequence (Ap i)i  and the law µ with the law of this
inﬁnite sequence. However  in our setting we do not require the condition supx θ Φ(x  θ) ≤ 1 to
hold and the fact that Θ is a set of inﬁnite sequences makes the identiﬁcation tedious without the
Gaussian random functions theory used here. Anyway  we believe that this link provides a better
mutual understanding of both approaches (i.e. [14] and this paper).
In the work [1]  the authors provide excess risk bounds for greedy algorithms (i.e. in a non-linear
approximation setting). The bounds derived in their Theorem 3.1 is similar to the result stated in
our Theorem 2. The main difference is that their bound makes use of the l1 norm of the coefﬁcients
α∗ instead of the l2 norm in our setting. It would be interesting to further investigate whether this
difference is a consequence of the non-linear aspect of their approximation or if it results from the
different assumptions made about the approximation spaces  in terms of rate of decrease of the
coefﬁcients.

5 Efﬁcient implementation using a lazy multi-resolution expansion

In practice  in order to build the least-squares estimate  one needs to compute the values of the
random features (ψp)1≤p≤P at the data points (xn)1≤n≤N   i.e. the matrix Ψ = (ψp(xn))p≤P n≤N .
Due to ﬁnite memory and precision of computers  numerical implementations can only handle a
ﬁnite number F of initial features (ϕi)1≤i≤F . In [10] it was mentioned that the computation of Ψ 
which makes use of the random matrix A = (Ap i)p≤P i≤F   has a complexity O(F P N). How-
ever  in the multi-resolution schemes described here  provided that the mother function has compact
support (such as the hat functions or the Daubechie wavelets)  we can signiﬁcantly speed up the
computation of the matrix Ψ by using a tree-based lazy expansion  i.e. where the expansion of the
random features (ψp)p≤P is built only when needed for the evaluation at the points (xn)n.
Consider the example of the scrambled wavelets. In dimension 1  using a wavelet dyadic-tree of
depth H (i.e. F = 2H+1)  the numerical cost for computing Ψ is O(HP N) (using one tree per
random feature). Now  in dimension d the classical extension of one-dimensional wavelets uses a
family of 2d − 1 wavelets  thus requires 2d − 1 trees each one having 2dH nodes. While the resulting
number of initial features F is of order 2d(H+1)  thanks to the lazy evaluation (notice that one never
computes all the initial features)  one needs to expand at most one path of length H per training
point  and the resulting complexity to compute Ψ is O(2dHP N).
Note that one may alternatively use the so-called sparse-grids instead of wavelet trees  which have
been introduced by Griebel and Zenger (see [18  3]). The main result is that one can reduce signif-
icantly the total number of features to F = O(2H H d) (while preserving a good approximation for
sufﬁciently smooth functions). Similar lazy evaluation techniques can be applied to sparse-grids.

7

Now  using a ﬁnite F introduces an additional approximation (squared) error term in the ﬁnal excess
risk bounds or order O(F − 2s
d ) for a wavelet basis adapted to H s([0  1]d). This additional error (due
to the numerical approximation) can be made arbitrarily small  e.g. o(N−1/2)  whenever H ≥ log N
.
√
Thus  using P = O(
N) random features  we deduce that the complexity of building the matrix
Ψ is O(2dN 3/2 log N). Then in order to solve the least squares system  one has to compute ΨT Ψ 
that has numerical cost O(P 2N)  and then solve the system by inversion  which has numerical cost
O(P 2.376) by [5]. Thus  the overall cost of the algorithm is O(2dN 3/2 log N + N 2).

d

N P + log N

P ||f∗||2K).

6 Conclusion and future works
We analyzed least-squares regression using sub-spaces GP that are generated by P random lin-
ear combinations of inﬁnitely many initial features. We showed that the approximation space
K = {fα ||α|| < ∞} (which is also the kernel space of the related Gaussian object) provides a
characterization of the set of target functions f∗ for which this random regression works. We il-
lustrated the approach on two examples for which the approximation space is a known functional
space  namely a Cameron-Martin space when the random features are Brownian sheets (generated
by random combinations at all scales of a hat function)  and a Sobolev space in the case of scram-
bled wavelets. We derived a general approximation error result from which we deduced excess risk
bounds of order O( log N
We showed that least-squares regression with scrambled wavelets provides rates that are arbitrarily
close to minimax optimality. However in the case of regression with Brownian sheets  we are not
aware of minimax lower bounds for Cameron-Martin spaces in dimension d > 1.
We discussed a key aspect of randomized approximation spaces which is that the approximation
error can be controlled independently of the measure P used to assess the performance. This is
essential in a regression setting where P is unknown  and excess risk rates independent of P are
obtained.
We concluded by mentioning a nice property of using multiscale objects like Brownian sheets and
scrambled wavelets (with compact mother wavelet) which is the possibility to be efﬁciently imple-
mented. We described a lazy expansion approach for computing the regression function which has
a numerical complexity O(N 2 + 2dN 3/2 log N).
A limitation of the current scrambled wavelets is that  so far  we did not consider reﬁned analysis
for spaces H s with large smoothness s (cid:192) d/2. Possible directions for better handling such spaces
may involve reﬁned covering number bounds which will be the object of future works.

Acknowledgment

This work has been supported by French National Research Agency (ANR) through COSINUS
program (project EXPLO-RA number ANR-08-COSI-004).

8

References
[1] Andrew Barron  Albert Cohen  Wolfgang Dahmen  and Ronald Devore. Approximation and

learning by greedy algorithms. 36:1:64–94  2008.

[2] Gerard Bourdaud. Ondelettes et espaces de besov. Rev. Mat. Iberoamericana  11:3:477–512 

1995.

[3] Hans-Joachim Bungartz and Michael Griebel. Sparse grids.

Numerica  volume 13. University of Cambridge  2004.

In Arieh Iserles  editor  Acta

[4] St´ephane Canu  Xavier Mary  and Alain Rakotomamonjy. Functional learning through kernel.

arXiv  2009  October.

[5] D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic progressions. In STOC
’87: Proceedings of the nineteenth annual ACM symposium on Theory of computing  pages
1–6  New York  NY  USA  1987. ACM.

[6] R. DeVore. Nonlinear Approximation. Acta Numerica  1997.
[7] L. Gy¨orﬁ  M. Kohler  A. Krzy˙zak  and H. Walk. A distribution-free theory of nonparametric

regression. Springer-Verlag  2002.

[8] St´ephane Jaffard. D´ecompositions en ondelettes. In Development of mathematics 1950–2000 

pages 609–634. Birkh¨auser  Basel  2000.

[9] Svante Janson. Gaussian Hilbert spaces. Cambridge Univerity Press  Cambridge  UK  1997.
[10] Odalric-Ambrym Maillard and R´emi Munos. Compressed Least-Squares Regression. In NIPS

2009  Vancouver Canada  2009.

[11] Odalric-Ambrym Maillard and R´emi Munos. Linear regression with random projections. Tech-

nical report  Hal INRIA: http://hal.archives-ouvertes.fr/inria-00483014/  2010.
[12] Stephane Mallat. A Wavelet Tour of Signal Processing. Academic Press  1999.
[13] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In John C.
Platt  Daphne Koller  Yoram Singer  Sam T. Roweis  John C. Platt  Daphne Koller  Yoram
Singer  and Sam T. Roweis  editors  NIPS. MIT Press  2007.

[14] Ali Rahimi and Benjamin Recht. Uniform approximation of functions with random bases.

2008.

[15] S. Saitoh. Theory of reproducing Kernels and its applications. Longman Scientiﬁc & Techni-

cal  Harlow  UK  1988.

[16] Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal

Statistical Society  Series B  58:267–288  1994.

[17] A. N. Tikhonov. Solution of incorrectly formulated problems and the regularization method.

Soviet Math Dokl 4  pages 1035–1038  1963.

[18] C. Zenger. Sparse grids. In W. Hackbusch  editor  Parallel Algorithms for Partial Differen-
tial Equations  Proceedings of the Sixth GAMM-Seminar  volume 31 of Notes on Num. Fluid
Mech.  Kiel  1990. Vieweg-Verlag.

9

,Mohammad Saberian
Jose Costa Pereira
Can Xu
Jian Yang
Nuno Nvasconcelos
Karol Hausman
Yevgen Chebotar
Stefan Schaal
Gaurav Sukhatme
Joseph Lim