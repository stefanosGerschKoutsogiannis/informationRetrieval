2019,What Can ResNet Learn Efficiently  Going Beyond Kernels?,How can neural networks such as ResNet \emph{efficiently} learn CIFAR-10 with test accuracy more than $96 \%$  while other methods  especially kernel methods  fall relatively behind? Can we more provide theoretical justifications for this gap?

Recently  there is an influential line of work relating neural networks to kernels in the over-parameterized regime  proving they can learn certain concept class that is also learnable by kernels with similar test error. Yet  can neural networks provably learn some concept class \emph{better} than kernels?


We answer this positively in the distribution-free setting. We prove neural networks can efficiently learn a notable class of functions  including those defined by three-layer residual networks with smooth activations  without any distributional assumption.
At the same time  we prove there are simple functions in this class such that with the same number of training examples  the test error obtained by neural networks can be \emph{much smaller} than \emph{any} kernel method  including neural tangent kernels (NTK).

The main intuition is that \emph{multi-layer} neural networks can implicitly perform hierarchal learning using different layers  which reduces the sample complexity comparing to ``one-shot'' learning algorithms such as kernel methods.

In the end  we also prove a computation complexity advantage of ResNet with respect to other learning methods including linear regression over arbitrary feature mappings.,What Can ResNet Learn Efﬁciently 

Going Beyond Kernels?∗

Zeyuan Allen-Zhu
Microsoft Research AI

zeyuan@csail.mit.edu

Yuanzhi Li

Carnegie Mellon University
yuanzhil@andrew.cmu.edu

Abstract

How can neural networks such as ResNet efﬁciently learn CIFAR-10 with test
accuracy more than 96%  while other methods  especially kernel methods  fall
relatively behind? Can we more provide theoretical justiﬁcations for this gap?
Recently  there is an inﬂuential line of work relating neural networks to kernels in
the over-parameterized regime  proving they can learn certain concept class that is
also learnable by kernels with similar test error. Yet  can neural networks provably
learn some concept class better than kernels?
We answer this positively in the distribution-free setting. We prove neural net-
works can efﬁciently learn a notable class of functions  including those deﬁned by
three-layer residual networks with smooth activations  without any distributional
assumption. At the same time  we prove there are simple functions in this class
such that with the same number of training examples  the test error obtained by
neural networks can be much smaller than any kernel method  including neural
tangent kernels (NTK).
The main intuition is that multi-layer neural networks can implicitly perform hi-
erarchical learning using different layers  which reduces the sample complexity
comparing to “one-shot” learning algorithms such as kernel methods. In a follow-
up work [2]  this theory of hierarchical learning is further strengthened to incor-
porate the “backward feature correction” process when training deep networks.
In the end  we also prove a computation complexity advantage of ResNet with
respect to other learning methods including linear regression over arbitrary feature
mappings.

Introduction

1
Neural network learning has become a key practical machine learning approach and has achieved
remarkable success in a wide range of real-world domains  such as computer vision  speech recog-
nition  and game playing [18  19  22  31]. On the other hand  from a theoretical standpoint  it is
less understood that how large-scale  non-convex  non-smooth neural networks can be optimized
efﬁciently over the training data and generalize to the test data with relatively few training examples.
There has been a sequence of research trying to address this question  showing that under certain
conditions neural networks can be learned efﬁciently [8–10  15  16  21  24  25  32–35  37  40]. These
provable guarantees typically come with strong assumptions and the proofs heavily rely on them.
One common assumption from them is on the input distribution  usually being random Gaussian
or sufﬁciently close to Gaussian. While providing great insights to the optimization side of neural
networks  it is not clear whether these works emphasizing on Gaussian inputs can coincide with
the neural network learning process in practice. Indeed  in nearly all real world data where deep
learning is applied to  the input distributions are not close to Gaussians; even worse  there may be

∗Full version and future updates can be found on https://arxiv.org/abs/1905.10337.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

no simple model to capture such distributions.
The difﬁculty of modeling real-world distributions brings us back to the traditional PAC-learning
language which is distribution-free. In this language  one of the most popular  provable learning
methods is the kernel methods  deﬁned with respect to kernel functions K(x  x(cid:48)) over pairs of data
(x  x(cid:48)). The optimization task associated with kernel methods is convex  hence the convergence rate
and the generalization error bound are well-established in theory.
Recently  there is a line of work studying the convergence of neural networks in the PAC-learning
language  especially for over-parameterized neural networks [1  3–7  12–14  20  23  41]  putting
neural network theory back to the distribution-free setting. Most of these works rely on the so-called
Neural Tangent Kernel (NTK) technique [12  20]  by relating the training process of sufﬁciently
over-parameterized (or even inﬁnite-width) neural networks to the learning process over a kernel
whose features are deﬁned by the randomly initialized weights of the neural network.
In other
words  on the same training data set  these works prove that neural networks can efﬁciently learn a
concept class with as good generalization as kernels  but nothing more is known.2
In contrast  in many practical tasks  neural networks
give much better generalization error compared to ker-
nels  although both methods can achieve zero training
error. For example  ResNet achieves 96% test accuracy
on the CIFAR-10 data set  but NTKs achieve 77% [6]
and random feature kernels achieve 85% [29]. This gap
becomes larger on more complicated data sets.
To separate the generalization power of neural networks
from kernel methods  the recent work [36] tries to iden-
tify conditions where the solutions found by neural net-
works provably generalize better than kernels. This ap-
proach assumes that the optimization converges to min-
imal complexity solutions (i.e.
the ones minimizing
the value of the regularizer  usually the sum of squared
Frobenius norms of weight matrices) of the training ob-
jective. However  for most practical applications  it is
unclear how  when training neural networks  minimal
complexity solutions can be found efﬁciently by local search algorithms such as stochastic gradient
descent. In fact  it is not true even for rather simple problems (see Figure 1).3 Towards this end  the
following fundamental question is largely unsolved:

Figure 1: d = 40  N = 5000  after exhaustive
search in network size  learning rate 
weight decay  randomly initialized
SGD still cannot ﬁnd solutions with
Frobenius norm comparable to what
we construct by hand. Details and
more experiments in Section 9.2.

Can neural networks efﬁciently and distribution-freely learn a concept class 

with better generalization than kernel methods?

In this paper  we give arguably the ﬁrst positive answer to this question for neural networks with
ReLU activations. We show without any distributional assumption  a three-layer residual network
(ResNet) can (improperly) learn a concept class that includes three-layer ResNets of smaller size
and smooth activations. This learning process can be efﬁciently done by stochastic gradient descent
(SGD)  and the generalization error is also small if polynomially many training examples are given.
More importantly  we give a provable separation between the generalization error obtained by neu-
ral networks and kernel methods. For some δ ∈ (0  1)  with N = O(δ−2) training samples  we
prove that neural networks can efﬁciently achieve generalization error δ for this concept class over
any distribution; in contrast  there exists rather simple distributions such that any kernel method (in-

2Technically speaking  the three-layer learning theorem of [3] is beyond NTK  because the learned weights
across different layers interact with each other  while in NTK the learned weights of each layer only interact
with random weights of other layers. However  there exist other kernels — such as recursive kernels [39] —
that can more or less efﬁciently learn the same concept class proposed in [3].

√
3Consider the class of degree-6 polynomials over 6 coordinates of the d-dimensional input. There exist
two-layer networks with F-norm O(
d) implementing this function (thus have near-zero training and testing
error). By Rademacher complexity  O(d) samples sufﬁce to learn if we are able to ﬁnd a minimal complexity
solution. Unfortunately  due to the non-convexity of the optimization landscape  two-layer networks can not be
trained to match this F-norm even with O(d2) samples  see Figure 1.

2

01002003004000.0040.0160.0630.251.Frobenius normerrorwe construct by hand in train/test errorbest found by SGD in train errorbest found by SGD in test errorδ for this class. To
cluding NTK  recursive kernel  etc) cannot have generalization error better than
the best of our knowledge  this is the ﬁrst work that gives provable  efﬁciently achievable separation
between neural networks with ReLU activations and kernels in the distribution-free setting. In the
end  we also prove a computation complexity advantage of neural networks with respect to linear
regression over arbitrary feature mappings as well.
Roadmap. We present detailed overview of our positive and negative results in Section 2 and 3.
Then  we introduce notations in Section 4  formally deﬁne our concept class in Section 5  and give
proof overviews in Section 6 and 8.

√

2 Positive Result: The Learnability of Three-Layer ResNet
In this paper  we consider learner networks that are single-skip three-layer ResNet
with ReLU activation  deﬁned as a function out : Rd → Rk:

out(x) = A (σ (Wx + b1) + σ (Uσ (Wx + b1) + b2))

(2.1)
Here  σ is the ReLU function  W ∈ Rm×d and U ∈ Rm×m are the hidden
weights  A ∈ Rk×m is the output weight  and b1  b2 ∈ Rm are two bias vectors.
We wish to learn a concept class given by target functions that can be written as

H(x) = F(x) + αG (F(x))

(2.2)
where α ∈ [0  1) and G : Rk → Rk F : Rd → Rk are two functions that can be written as two-layer
networks with smooth activations (see Section 5 for the formal deﬁnition). Intuitively  the target
function is a mixture of two parts: the base signal F  which is simpler and contributes more to
the target  and the composite signal G (F)  which is more complicated but contributes less. As an
analogy  F could capture the signal in which “85%” examples in CIFAR-10 can be learned by kernel
methods  and G (F) could capture the additional “11%” examples that are more complicated.
The goal is to use three-layer ResNet (2.1) to improperly learn this concept class (2.2)  meaning
learning “both” the base and composite signals  with as few samples as possible. In this paper  we
consider a simple (cid:96)2 regression task where the features x ∈ Rd and labels y ∈ Rk are sampled from
some unknown distribution D. Thus  given a network out(x)  the population risk is

E

(x y)∼D

1
2

(cid:107)out (x) − y(cid:107)2

2 .

To illustrate our result  we ﬁrst assume for simplicity that y = H (x) for some H of the form (2.2)
(so the optimal target has zero regression error). Our main theorem can be sketched as follows.
Let CF and CG respectively be the individual “complexity” of F and G  which at a high level 
capture the size and smoothness of F and G. This complexity notion shall be formally introduced
in Section 4  and is used by prior works such as [3  7  39].

Theorem 1 (ResNet  sketched). For any distribution over x  for every δ ∈ (cid:0)(αCG)4  1(cid:1)  with

probability at least 0.99  SGD efﬁciently learns a network out(x) in the form (2.1) satisfying

E

(x y)∼D

1
2

(cid:107)out (x) − y(cid:107)2

2 ≤ δ

samples

using N = (cid:101)O

(cid:16) C 2F

(cid:17)

δ2

The running time of SGD is polynomial in poly(CG  CF   α−1).
In other words  ResNet is capable of achieving population risk α4  or equivalently learning the
output H(x) up to α2 error. In our full theorem  we also allow label y to be generated from H(x)
with error  thus our result also holds in the agnostic learning framework.
2.1 Our Contributions
Our main contribution is to obtain time and sample complexity in CF and CG without any depen-
dency on the composed function G(F) as in prior work [3  39]. We illustrate this crucial difference
with an example. Suppose x ∼ N (0  I/d)  k = 2 and F ∈ Rd → R2 consists of two linear func-
d  and G is degree-10 polynomial with

tion: F(x) = (cid:0)(cid:104)w∗
d) and CG = (cid:101)O(1). Theorem 1 implies
2(cid:107)2 =
• we need (cid:101)O(d) samples to efﬁciently learn H = F + αG(F) up to accuracy (cid:101)O(α2).

1(cid:107)2 (cid:107)w∗
√
constant coefﬁcient. As we shall see  CF = O(

2  x(cid:105)(cid:1) with (cid:107)w∗

1  x(cid:105) (cid:104)w∗

√

3

𝑊𝑥ReLU𝑈ReLU𝐴In contrast  the complexity of G(F) is (cid:101)O((
• prior works [3  39] need(cid:101)Ω(d10) samples to efﬁciently learn H up to any accuracy o(α) 

d)10)  so

√

2  x(cid:105)10.4

1  x(cid:105)10 − (cid:104)w∗

even if G(x) is of some simple form such as (cid:104)w∗
Inductive Bias. Our network is over-parameterized  thus intuitively in the example above  with
only O(d) training examples  the learner network could over-ﬁt to the training data since it has
to decide from a set of d10 many possible coefﬁcients to learn the degree 10 polynomial G. This is
indeed the case if we learn the target function using kernels  or possibly even learn it with a two-layer
network. However  three-layer ResNet posts a completely different inductive bias  and manages to
avoid over-ﬁtting to G(F) with the help from F.
Implicit Hierarchical Learning. Since H(x) = F(x) + αG (F(x))  if we only learn F but not
G (F)  we will have regression error ≈ (αCG)2. Thus  to get to regression error (αCG)4  Theorem 1
shows that ResNet is also capable of learning G (F) up to some good accuracy with relatively few
training examples. This is also observed in practice  where with this number of training examples 
three-layer fully-connected networks and kernel methods can indeed fail to learn G (F) up to any
non-trivial accuracy  see Figure 2.
Intuitively  there is a hierarchy of the learning process: we would like to ﬁrst learn F  and then we
could learn G(F) much easier with the help of F using the residual link. In our learner network (2.1) 
the ﬁrst hidden layer serves to learn F and the second hidden layer serves to learn G with the help of
F  which reduces the sample complexity. However  the important message is that F and G are not
given as separate data to the network  rather the learning algorithm has to disentangle them from the
“combined” function H = F + αG(F) automatically during the training process. Moreover  since
we train both layers simultaneously  the learning algorithm also has to distribute the learning task
of F and G onto different layers automatically.
We also emphasize that our result cannot be obtained by layer-wise training of the ResNet  that is 
ﬁrst training the hidden layer close to the input  and then training the hidden layer close to the output.
Since it could be the case the ﬁrst layer incurs some α error (since it cannot learn G(F) directly) 
then it could be really hard  or perhaps impossible  for the second layer to ﬁx it only using inputs
of the form F(x) ± α. In other words  it is crucial that the two hidden layers are simultaneously
trained. 5
A follow-up work.
In a follow-up work [2]  this theory of hierarchical learning is signiﬁcantly
strengthened to further incorporate the “backward feature correction” when training deep neural
networks. In the language of this paper  when the two layers trained together  given enough samples 
the accuracy in the ﬁrst layer can actually be improved from F ± α to arbitrarily close to F during
the training process. As a consequence  the ﬁnal training and generalization error can be arbitrarily
small as well  as opposite to α4 in this work. The new “backward feature correction” is also critical
to extend the hierarchical learning process from 3 layers to arbitrarily number of layers.

3 Negative Results
3.1 Limitation of Kernel Methods
Given (Mercer) kernels K1  . . .   Kk : Rd×d → R and training examples {(x(i)  y(i))}i∈[N ] from D 
a kernel method tries to learn a function K : Rd → Rk where each
n∈[N ] Kj(x  x(n)) · wj n

Kj(x) =(cid:80)

(3.1)

4Of course  if one knew a priori the form H(x) = (cid:104)w∗

2   x(cid:105)10  one could also try to solve it
directly by minimizing objective ((cid:104)w∗
2   x(cid:105)10 + (cid:104)w2  x(cid:105)10 − (cid:104)w1  x(cid:105)10)2 over w1  w2 ∈ Rd. Un-
fortunately  the underlying optimization process is highly non-convex and it remains unclear how to minimize

it efﬁciently. Using matrix sensing [25]  one can efﬁciently learn such H(x) in sample complexity (cid:101)O(d5).

1   x(cid:105)10 − (cid:104)w∗

1   x(cid:105)10 − (cid:104)w∗

5However  this does not mean that the error of the ﬁrst layer can be reduced by its own  since it is still
possible for the ﬁrst layer to learn F + αR(x) ± α2 and the second layer to learn G(F)(x) − R(x) ± α  for
an arbitrary (bounded) function R.

4

i=1

j∈[k]

j

1
N

(cid:1)2

+ R(w)

(cid:80)

(cid:80)N

(cid:0)(cid:80)
n∈[N ] Kj(x(i)  x(n))wj n − y(i)

2/h; (2) arcsin kernel K(x  y) = arcsin(cid:0)(cid:104)x  y(cid:105)/((cid:107)x(cid:107)2(cid:107)y(cid:107)2)(cid:1); (3) recursive kernel with any

is parameterized by a weight vector wj ∈ RN . Usually  for the (cid:96)2 regression task  a kernel method
ﬁnds the optimal weights w1  . . .   wk ∈ RN by solving the following convex minimization problem
(3.2)
for some convex regularizer R(w).6 In this paper  however  we do not make assumptions about how
K(x) is found as the optimal solution of the training objective. Instead  we focus on any kernel
regression function that can be written in the form (3.1).
Most of the widely-used kernels are Mercer kernels.7 This includes (1) Gaussian kernel K(x  y) =
e−(cid:107)x−y(cid:107)2
recursive function [39]; (4) random feature kernel K(x  y) = Ew∼W φw(x)φw(y) for any function
φw(·) and distribution W; (5) the conjugate kernel deﬁned by the last hidden layer of random initial-
ized neural networks [11]; (6) the neural tangent kernels (NTK) for fully-connected [20] networks 
convolutional networks [6  38] or more generally for any architectures [38].
Our theorem can be sketched as follows:
Theorem 3 (kernel  sketched). For every constant k ≥ 2  for every sufﬁciently large d ≥ 2  there
exist concept classes consisting of functions H(x) = F(x) + αG (F(x)) with complexities CF   CG
and α ∈ (0  1

CG ) such that  letting

Nres be the sample complexity from Theorem 1 to achieve α3.9 population risk 

then there exists simple distributions D over (x H(x)) such that  for at least 99% of the functions

H in this concept class  even given N = O(cid:0)(Nres)k/2(cid:1) training samples from D  any function K(x)

of the form (3.1) has to suffer population risk
2 > α2

2 (cid:107)K(x) − y(cid:107)2

E(x y)∼D 1

even if the label y = H(x) has no error.

√

Contribution and Intuition. Let us compare this to Theorem 1. While both algorithms are ef-
ﬁcient  neural networks (trained by SGD) achieve population risk α3.9 using Nres samples for any
distribution over x  while kernel methods cannot achieve any population risk better than α2 for some
simple distributions even with N = (Nres)k/2 (cid:29) Nres samples.8 Our two theorems together gives a
provable separation between the generalization error of the solutions found by neural networks and
kernel methods  in the efﬁciently computable regime.
More speciﬁcally  recall CF and CG only depend on individual complexity of G F  but not on G(F).
In Theorem 3  we will construct F as linear functions and G as degree-k polynomials. This ensures
d) and CG = O(1) for k being constant  but the combined complexity of G(F) is as
CF = O(
high as Ω(dk/2). Since ResNet can perform hierarchical learning  it only needs sample complexity
Nres = O(d/α8) instead of paying (square of) the combined complexity Ω(dk).
In contrast  a kernel method is not hierarchical: rather than discovering F ﬁrst and then learning
G(F) with the guidance of it  kernel method tries to learn everything in one shot. This unavoidably
requires the sample complexity to be at least Ω(dk). Intuitively  as the kernel method tries to learn
G(F) from scratch  this means that it has to take into account all Ω(dk) many possible choices
of G(F) (recall that G is a degree k polynomial over dimension d). On the other hand  a kernel
method with N samples only has N-degrees of freedom (for each output dimension). This means  if
N (cid:28) o(dk)  kernel method simply does not have enough degrees of freedom to distinguish between
different G(F)  so has to pay Ω(α2) in population risk. Choosing for instance α = d−0.1  we have

the desired negative result for all N ≤ O(cid:0)(Nres)k/2(cid:1) (cid:28) o(dk).
6In many cases  R(w) = λ ·(cid:80)

j Kjwj is the norm associated with the kernel  for matrix Kj ∈
7Recall a Mercer kernel K : Rd×d → R can be written as K(x  y) = (cid:104)φ(x)  φ(y)(cid:105) where φ : Rd → V is a

j∈[k] w(cid:62)
RN×N deﬁned as [Kj]i n = Kj(x(i)  x(n)).
feature mapping to some inner product space V.

8It is necessary the negative result of kernel methods is distribution dependent  since for trivial distributions
where x is non-zero only on the ﬁrst constantly many coordinates  both neural networks and kernel methods
can learn it with constantly many samples.

5

3.2 Limitation of Linear Regression Over Feature Mappings
Given an arbitrary feature mapping φ : Rd → RD  one may consider learning a linear function over
φ. Namely  to learn a function F : Rd → Rk where each

(3.3)
is parameterized by a weight vector wj ∈ RD. Usually  these weights are determined by minimizing
the following regression objective:9

j φ(x)

Fj(x) = w(cid:62)

(cid:80)

(cid:80)

1
N

i∈[N ]

j∈[k]

(cid:16)

j φ(cid:0)x(i)(cid:1) − y(i)

j

w(cid:62)

(cid:17)2

+ R(w)

for some regularizer R(w). In this paper  we do not make assumptions about how the weighted are
found. Instead  we focus on any linear function over such feature mapping in the form (3.3).
Theorem 4 (feature mapping  sketched). For sufﬁciently large integers d  k  there exist concept
classes consisting of functions H(x) = F(x) + αG (F(x)) with complexities CF   CG and α ∈
(0  1

CG ) such that  letting

Tres be the time complexity from Theorem 1 to achieve α3.9 population risk 

then for at least 99% of the functions H in this concept class  even with arbitrary D = (Tres)2
dimensional feature mapping  any function F(x) of the form (3.3) has to suffer population risk

E(x y)∼D 1

2 (cid:107)F(x) − y(cid:107)2

2 > α2

even if the label y = H(x) has no error.

Interpretation. Since any algorithm that optimizes linear functions over D-dimensional feature
mapping has to run in time Ω(D)  this proves a time complexity separation between neural networks
(say  for achieving population risk α3.9) and linear regression over feature mappings (for achieving
even any population risk better than α2 (cid:29) α3.9). Usually  such an algorithm also has to suffer from
Ω(D) space complexity. If that happens  we also have a space complexity separation. Our hard
instance in proving Theorem 4 is the same as Theorem 3  and the proof is analogous.

4 Notations
We denote by (cid:107)w(cid:107)2 and (cid:107)w(cid:107)∞ the Euclidean and inﬁnity norms of vectors w  and (cid:107)w(cid:107)0 the number
of non-zeros of w. We also abbreviate (cid:107)w(cid:107) = (cid:107)w(cid:107)2 when it is clear from the context. We denote
the row (cid:96)p norm for W ∈ Rm×d (for p ≥ 1) as

(cid:107)W(cid:107)2 p :=(cid:0)(cid:80)

(cid:1)1/p

.

i∈[m] (cid:107)wi(cid:107)p

2

By deﬁnition  (cid:107)W(cid:107)2 2 = (cid:107)W(cid:107)F is the Frobenius norm of W. We use (cid:107)W(cid:107)2 to denote the matrix
spectral norm. For a diagonal matrix D we use (cid:107)D(cid:107)0 to denote its sparsity. For a matrix W ∈ Rm×d 
we use Wi or wi to denote the i-th row of W.
We use N (µ  σ) to denote Gaussian distribution with mean µ and variance σ; or N (µ  Σ) to denote
Gaussian vector with mean µ and covariance Σ. We use 1event or 1[event] to denote the indicator
function of whether event is true. We use σ(·) to denote the ReLU function  namely σ(x) =
max{x  0} = 1x≥0 · x. Given univariate function f : R → R  we also use f to denote the same
function over vectors: f (x) = (f (x1)  . . .   f (xm)) if x ∈ Rm.
For notation simplicity  throughout this paper “with high probability” (or w.h.p.) means with prob-

ability 1 − e−c log2 m for a sufﬁciently large constant c. We use (cid:101)O to hide polylog(m) factors.
inﬁnite-order smooth function φ : R → R. Suppose φ(z) =(cid:80)∞
(C∗)i +(cid:0)√

Function complexity. The following notions introduced in [3] measure the complexity of any

i=0 cizi is its Taylor expansion.
√
log(1/ε)

C∗(cid:1)i(cid:17)|ci|

(cid:16)
Cε(φ) = Cε(φ  1) :=(cid:80)∞
Cs(φ) = Cs(φ  1) := C∗(cid:80)∞
i=0(i + 1)|ci|

i=0

where C∗ is a sufﬁciently large constant (e.g.  104).
Example 4.1. If φ(z) = ec·z − 1  sin(c · z)  cos(c · z) or degree-c polynomial for constant c  then
Cε(φ  1) = o(1/ε) and Cs(φ  1) = O(1). If φ(z) = sigmoid(z) or tanh(z)  to get ε approximation
9If R(w) is the (cid:96)2 regularizer  then this becomes a kernel method again since the minimizer can be written

i

in the form (3.1). For other regularizers  this may not be the case.

6

we can truncate their Taylor series at degree Θ(log 1
the fact that (log(1/ε)/i)i ≤ poly(ε−1) for every i ≥ 1  and Cs(φ  1) ≤ O(1).

ε ). One can verify that Cε(φ  1) ≤ poly(1/ε) by

5 Concept Class
We consider learning some unknown distribution D of data points z = (x  y) ∈ Rd × Rk  where
x ∈ Rd is the input vector and y is the associated label. Let us consider target functions H : Rd →
Rk coming from the following concept class.
Concept 1. H is given by two smooth functions F G : Rk → Rk and a value α ∈ R+:

(cid:0)(cid:104)v∗
r i  h(cid:105)(cid:1)

(5.1)

(5.2)

where for each output coordinate r 

Fr(x) =

F  r i · Fr i
a∗

(cid:88)

i∈[pF ]

H(x) = F(x) + αG (F(x))  

(cid:0)(cid:104)w∗
r i  x(cid:105)(cid:1)

and Gr(h) =

(cid:88)
r i ∈ Rd and v∗

i∈[pG ]

G r i · Gr i
a∗

F  r i  a∗
√
r i(cid:107)2 = 1/

G r i ∈ [−1  1] and vectors w∗

r i ∈ Rk. We assume for
2.10 For simplicity  we assume (cid:107)x(cid:107)2 = 1 and (cid:107)F(x)(cid:107)2 = 1 for

for some parameters a∗
simplicity (cid:107)w∗
r i(cid:107)2 = (cid:107)v∗
(x  y) ∼ D and in Appendix A we state a more general Concept 2 without these assumptions.11
We denote by Cε(F) = maxr i{Cε(Fr i)} and Cs(F) = maxr i{Cs(Fr i)}. Intuitively  F and G
are both generated by two-layer neural networks with smooth activation functions Fr i and Gr i.
Borrowing the agnostic PAC-learning language  our concept class consists of all functions H(x)
in the form of Concept 1 with complexity bounded by tuple (pF   CF   pG  CG). Let OPT be the
population risk achieved by the best target function in this concept class. Then  our goal is to learn
this concept class with population risk O(OPT)+ε using sample and time complexity polynomial in
pF   CF   pG  CG and 1/ε. In the remainder of this paper  to simplify notations  we do not explicitly
deﬁne this concept class parameterized by (pF   CF   pG  CG). Instead  we equivalently state our
theorem with respect to any (unknown) ﬁxed target function H with with population risk OPT:

E(x y)∼D(cid:2) 1
In the analysis we adopt the following notations. For every (x  y) ∼ D  it satisﬁes (cid:107)F(x)(cid:107)2 ≤ BF
and (cid:107)G(F(x))(cid:107)2 ≤ BF◦G. We assume G(·) is LG-Lipschitz continuous. It is a simple exercise
(see Fact A.3) to verify that LG ≤ √
kpF Cs(F) and BF◦G ≤ LGBF +
√
kpGC(G) ≤ kpF Cs(F)pGCs(G).

kpGCs(G)  BF ≤ √

(cid:3) ≤ OPT .

2(cid:107)H(x) − y(cid:107)2

2

6 Overview of Theorem 1
We learn the unknown distribution D with three-layer ResNet with ReLU activation (2.1) as learners.
For notation simplicity  we absorb the bias vector into weight matrix: that is  given W ∈ Rm×d and
bias b1 ∈ Rm  we rewrite Wx + b as W(x  1) for a new weight matrix W ∈ Rm×(d+1). We
also re-parameterize U as U = VA and we ﬁnd this parameterization (similar to the “bottleneck”
structure in ResNet) simpliﬁes the proof and also works well empirically for our concept class. After
such notation simpliﬁcation and re-parameterization  we can rewrite out(x) : Rd → Rk as

(cid:16)

(cid:17)

out(W  V; x) = out(x) = out1(x) + Aσ

(V(0) + V)(out1(x)  1)

out1(W  V; x) = out1(x) = Aσ(W(0) + W)(x  1) .

Above  A ∈ Rk×m  V(0) ∈ Rm×(k+1)  W(0) ∈ Rm×(d+1) are weight matrices corresponding
to random initialization  and W ∈ Rm×(k+1)  W ∈ Rm×(d+1) are the additional weights to be
learned by the algorithm. To prove the strongest result  we only train W  V and do not train A.12
10For general (cid:107)w∗
r i| ≤ B  the scaling factor B can be absorbed into the
11Since we use ReLU networks as learners  they are positive homogeneous so to learn general functions F G

activation function φ(cid:48)(x) = φ(Bx). Our results then hold by replacing the complexity of φ with φ(cid:48).

1 i(cid:107)2 ≤ B (cid:107)w∗

2 i(cid:107)2 ≤ B  |a∗

which may not be positive homogenous  it is in some sense necessary that the inputs are scaled properly.

12This can be more meaningful than training all the layers together  in which if one is not careful with
parameter choices  the training process can degenerate as if only the last layer is trained [11]. (That is a convex

7

We consider random Gaussian initialization where the entries of A  W(0)  V(0) are independently
generated as follows:

[W(0)]i j ∼ N(cid:0)0  σ2

w

(cid:1)

[V(0)]i j ∼ N(cid:0)0  σ2
v/m(cid:1)

Ai j ∼ N(cid:0)0  1

(cid:1)

m

In this paper we focus on the (cid:96)2 loss function between H and out  given as:

(cid:107)y − out(W  V; x)(cid:107)2

1
2

2

∂Obj(W V;(xt yt))

Obj(W  V; (x  y)) =

Wt+1 ← Wt − ηw
Vt+1 ← Vt − ηv

(6.1)
We consider the vanilla SGD algorithm. Starting from W0 = 0  V0 = 0  in each iteration t =
0  1  . . .   T − 1  it receives a random sample (xt  yt) ∼ D and performs SGD updates13

(cid:12)(cid:12)W=Wt V=Vt
(cid:12)(cid:12)W=Wt V=Vt
Theorem 1. Under Concept 1 or Concept 2  for every α ∈ (cid:0)0 (cid:101)Θ(
kpG Cs(G) )(cid:1) and δ ≥ OPT +
(cid:101)Θ(cid:0)α4(kpGCs(G))4(1 + BF )2(cid:1). There exist M = poly(Cα(F)  Cα(G)  pF   α−1) satisfying that for
(cid:16) αpG Cs(G)

every m ≥ M  with high probability over A  W(0)  V(0)  for a wide range of random initialization
parameters σw  σv (see Table 1)  choosing

ηw = (cid:101)Θ (min{1  δ})

ηv = ηw ·(cid:101)Θ

T = (cid:101)Θ

∂Obj(W V;(xt yt))

(cid:17)2

(cid:17)

∂W

∂V

1

pF Cs(F )

With high probability  the SGD algorithm satisﬁes

E(x y)∼D (cid:107)H(x) − out(Wt  Vt; x)(cid:107)2

2 ≤ O(δ) .

As a corollary  under Concept 1  we can archive population risk

t=0

O(OPT) +(cid:101)Θ(cid:0)α4(kpGCs(G))4(cid:1)

(6.2)
Remark 6.1. Our Theorem 1 is almost in the PAC-learning language  except that the ﬁnal error has
an additive α4 term that can not be arbitrarily small.

using sample complexity T .

(cid:16) (kpF Cs(F ))2
(cid:80)T−1

min{1 δ2}

1
T

6.1 Proof Overview
In the analysis  let us deﬁne diagonal matrices

DW(0) = diag{1
DW = diag{1

W(0)(x 1)≥0}
(W(0)+W)(x 1)≥0}

DV(0) W = diag{1
DV W = diag{1

V(0)(out1(x) 1)≥0}
(V(0)+V)(out1(x) 1)≥0}

which satisfy out1(x) = ADW(W(0) + W)(x  1) and out(x) = ADV W(V(0) + V)(out1(x)  1).
The proof of Theorem 1 can be divided into three simple steps with parameter choices in Table 1.

In this paper  we assume 0 < α ≤ (cid:101)O(

1

kpG Cs(G) ) and choose parameters

σw ∈ [m−1/2+0.01  m−0.01]

τw := (cid:101)Θ(kpF Cs(F)) ≥ 1
(cid:3)
τw ∈(cid:2)m1/8+0.001σw  m1/8−0.001σ1/4
σw  σv: recall entries of W(0) and V(0) are from N(cid:0)0  σ2

w

σv ∈ [polylog(m)  m3/8−0.01]

τv := (cid:101)Θ(αkpGCs(G)) ≤
τv ∈(cid:2)σv · (k/m)3/8 
v/m(cid:1).
(cid:1) and N(cid:0)0  σ2

τw  τv: the proofs work with respect to (cid:107)W(cid:107)2 ≤ τw and (cid:107)V(cid:107)2 ≤ τv.

w

1

(cid:3)

polylog(m)
σv

polylog(m)

Table 1: Three-layer ResNet parameter choices.

and they satisfy

In the ﬁrst step  we prove that for all weight matrices not very far from random initialization (namely 
all (cid:107)W(cid:107)2 ≤ τw and (cid:107)V(cid:107)2 ≤ τv)  many good “coupling properties” occur. This includes upper
kernel method.) Of course  as a simple corollary  our result also applies to training all the layers together  with
appropriately chosen random initialization and learning rate.

13Performing SGD with respect to W(0) + W and V(0) + V is the same as that with respect to W and
V; we introduce W(0)  V(0) notation for analysis purpose. Note also  one can alternatively consider having a
training set and then performing SGD on this training set with multiple passes; similar results can be obtained.

8

bounds on the number of sign changes (i.e.  on (cid:107)DW(0) − DW(cid:107)0 and(cid:13)(cid:13)DV(0) W − DV W

(cid:13)(cid:13)0) as

(cid:62)

(cid:62)

(cid:62)

(cid:62)

ADWW

(cid:62)(cid:107)F ≤ τv

  V
(x  1) ≈ F(x) and ADV(0) WV

well as vanishing properties such as ADWW(0)  ADV WV(0) being negligible. We prove such
properties using techniques from prior works [3  5]. Details are in Section C.1.
In the second step  we prove the existence of W
10 sat-
(out1(x)  1) ≈ αG (out1(x)). This existen-
isfying ADW(0)W
tial proof relies on an “indicator to function” lemma from [3]; for the purpose of this paper we have
to revise it to include a trainable bias term (or equivalently  to support vectors of the form (x  1)).
Combining it with the aforementioned vanishing properties  we derive (details are in Section C.2):
(6.3)
In the third step  consider iteration t of SGD with sample (xt  yt) ∼ D. For simplicity we assume
OPT = 0 so yt = H(xt). One can carefully write down gradient formula  and plug in (6.3) to derive

(out1(x)  1) ≈ αG (out1(x)) .

(cid:62) with (cid:107)W
(cid:62)

(x  1) ≈ F(x)

and ADV WV

10 and (cid:107)V

(cid:62)(cid:107)F ≤ τw

≥ 1

  Vt − V

2 is as small as E[(cid:107)Errt(cid:107)2

2(cid:107)H(xt) − out(Wt  Vt; xt)(cid:107)2

Ξt := (cid:104)∇W VObj(Wt  Vt; (xt  yt))  (Wt − W
(cid:62)
2 − 2(cid:107)Errt(cid:107)2

(cid:3) ≤ (cid:101)Θ(cid:0)α4(kpGCs(G))4(cid:1). This quantity Ξt is quite famous in classical mirror

descent analysis: for appropriately chosen learning rates  Ξt must converge to zero.14
In other
words  by concentration  SGD is capable of ﬁnding solutions Wt  Vt so that the population risk
(cid:107)H(xt) − out(Wt  Vt; xt)(cid:107)2
2]. This is why we can obtain population risk

with E(cid:2)(cid:107)Errt(cid:107)2
(cid:101)Θ(cid:0)α4(kpGCs(G))4(cid:1) in (6.2). Details are in Section C.3 and C.4.
F(x) = W∗x and G(y) =(cid:0)(cid:81)
every α ∈(cid:2)Ω(d−k/4)  1(cid:1)  let X = {x(1)  . . .   x(N )} be i.i.d. drawn from the uniform distribution

7 Overview of Theorem 3
Let us consider the following simple distribution of functions H(x) = F(x) + αG(F(x))  with
d(ei1   ei2  ··· eik ) for i1 (cid:54)= i2 ··· (cid:54)= ik

being uniformly at random chosen from [d].
Theorem 2. For every constant k ≥ 4  for sufﬁciently large d  for every N ≤ O( dk/2

logk d )  for
over { ±1√
}d and y(i) = H(x(i))  and let K(x) represent the optimal solution to (3.2) using any
correlation kernel  then we have with probability at least 0.99 over the randomness of W∗ and X

i∈[k]. Here  W∗ =

j∈[k] yj

))(cid:105)

(cid:1)

√

(cid:62)

d

2

2

x∼U ({−1/

E
√
d  +1/

√

d}d)

(cid:107)H(x) − K(x)(cid:107)2

2 > α2/4

√

2 ≤ (cid:101)O(α4)

(cid:1) samples .

in N = (cid:101)Θ(cid:0) d

Remark. One can relax the assumption on K(x) to being any approximate minimizer of (3.2).
In contrast  using Theorem 1  one can show Cs(F) = O(

d) and Cs(G) = O(1) so SGD can ﬁnd

Ex (cid:107)H(x) − out(x)(cid:107)2

As an example  when k ≥ 4 and α = d−0.01  ResNet achieves regression error α4 in N = (cid:101)O(d1.08)
samples  but kernel methods cannot achieve α2 error even with N = (cid:101)Θ(dk/2) = (cid:101)Ω(d2) samples.

We sketch the proof of Theorem 3 in half a page on Page 13.
Conclusion. We give the ﬁrst provable separation between neural networks and kernel methods 
in the efﬁcient and distribution-free learning regime. We show that neural networks can implicitly
hierarchical learn functions G(F(x)) with the help of F(x) using residual links  without paying
sample complexity comparing to “one-shot” learning algorithms that directly learns G(F(x)). Fi-
nally  we would like to point out there are kernels not captured by our correlation kernels  such as
convolutional networks with “global average pooling.” To prove bounds for them is an interesting
future direction.

α8

14Indeed  one can show(cid:80)T−1

√
be made O(

T ) ignoring other factors.

9

t=0 Ξt ≤ O(ηw + ηv)· T +

(cid:107)W

(cid:62)(cid:107)2
ηw

F

+

(cid:107)V

(cid:62)(cid:107)2
ηv

F

  and thus the right hand side can

References
[1] Zeyuan Allen-Zhu and Yuanzhi Li. Can SGD learn recurrent neural networks with provable
generalization? CoRR  abs/1902.01028  2019. URL http://arxiv.org/abs/1902.01028.

[2] Zeyuan Allen-Zhu and Yuanzhi Li. Backward Feature Correction: How Deep Learning Per-

forms Deep Learning. arXiv preprint  January 2020.

[3] Zeyuan Allen-Zhu  Yuanzhi Li  and Yingyu Liang. Learning and Generalization in Overpa-
rameterized Neural Networks  Going Beyond Two Layers. In NeurIPS  2019. Full version
available at http://arxiv.org/abs/1811.04918.

[4] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. On the convergence rate of training recurrent
neural networks. In NeurIPS  2019. Full version available at http://arxiv.org/abs/1810.
12065.

[5] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning via
over-parameterization. In ICML  2019. Full version available at http://arxiv.org/abs/
1811.03962.

[6] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  Ruslan Salakhutdinov  and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 
2019.

[7] Sanjeev Arora  Simon S. Du  Wei Hu  Zhiyuan Li  and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. CoRR 
abs/1901.08584  2019. URL http://arxiv.org/abs/1901.08584.

[8] Ainesh Bakshi  Rajesh Jayaram  and David P Woodruff. Learning two layer rectiﬁed neural

networks in polynomial time. arXiv preprint arXiv:1811.01885  2018.

[9] Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer

neural network. arXiv preprint arXiv:1710.11241  2017.

[10] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with

gaussian inputs. arXiv preprint arXiv:1702.07966  2017.

[11] Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural

Information Processing Systems  pages 2422–2430  2017.

[12] Amit Daniely  Roy Frostig  and Yoram Singer. Toward deeper understanding of neural net-
works: The power of initialization and a dual view on expressivity. In Advances in Neural
Information Processing Systems (NIPS)  pages 2253–2261  2016.

[13] Simon S Du  Jason D Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent ﬁnds

global minima of deep neural networks. arXiv preprint arXiv:1811.03804  2018.

[14] Simon S Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably opti-

mizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054  2018.

[15] Rong Ge  Jason D Lee  and Tengyu Ma. Learning one-hidden-layer neural networks with

landscape design. arXiv preprint arXiv:1711.00501  2017.

[16] Rong Ge  Rohith Kuditipudi  Zhize Li  and Xiang Wang. Learning two-layer neural networks

with symmetric inputs. In International Conference on Learning Representations  2019.

[17] Noah Golowich  Alexander Rakhlin  and Ohad Shamir. Size-independent sample complexity

of neural networks. In Proceedings of the Conference on Learning Theory  2018.

[18] Alex Graves  Abdel-rahman Mohamed  and Geoffrey Hinton. Speech recognition with deep
In Acoustics  speech and signal processing (icassp)  2013 ieee

recurrent neural networks.
international conference on  pages 6645–6649. IEEE  2013.

10

[19] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion  pages 770–778  2016.

[20] Arthur Jacot  Franck Gabriel  and Cl´ement Hongler. Neural tangent kernel: Convergence
and generalization in neural networks. In Advances in neural information processing systems 
pages 8571–8580  2018.

[21] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Informa-

tion Processing Systems  pages 586–594  2016.

[22] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[23] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. In Advances in Neural Information Processing Systems 
2018.

[24] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu

activation. In Advances in Neural Information Processing Systems  pages 597–607  2017.

[25] Yuanzhi Li  Tengyu Ma  and Hongyang Zhang.

Algorithmic regularization in over-
parameterized matrix sensing and neural networks with quadratic activations. In COLT  2018.

[26] Tengyu Ma. CS229T/STAT231: Statistical Learning Theory (Fall 2017). https://web.
ac-

stanford.edu/class/cs229t/scribe_notes/10_17_final.pdf  October 2017.
cessed May 2019.

[27] Martin J. Wainwright. Basic tail and concentration bounds. https://www.stat.berkeley.
edu/~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf  2015. Online; accessed
Oct 2018.

[28] Behnam Neyshabur  Ryota Tomioka  and Nathan Srebro. Norm-based capacity control in

neural networks. In Conference on Learning Theory  pages 1376–1401  2015.

[29] Benjamin Recht  Rebecca Roelofs  Ludwig Schmidt  and Vaishaal Shankar. Do CIFAR-10

Classiﬁers Generalize to CIFAR-10? arXiv preprint arXiv:1806.00451  2018.

[30] Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme
singular values. In Proceedings of the International Congress of Mathematicians 2010 (ICM
2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II–IV: Invited Lectures 
pages 1576–1602. World Scientiﬁc  2010.

[31] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van
Den Driessche  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanc-
tot  et al. Mastering the game of go with deep neural networks and tree search. nature  529
(7587):484  2016.

[32] Mahdi Soltanolkotabi  Adel Javanmard  and Jason D Lee. Theoretical insights into the
arXiv preprint

optimization landscape of over-parameterized shallow neural networks.
arXiv:1707.04926  2017.

[33] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guar-

antees for multilayer neural networks. arXiv preprint arXiv:1605.08361  2016.

[34] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and
its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560 
2017.

[35] Santosh Vempala and John Wilmes. Polynomial convergence of gradient descent for training

one-hidden-layer neural networks. arXiv preprint arXiv:1805.02677  2018.

[36] Colin Wei  Jason D Lee  Qiang Liu  and Tengyu Ma. On the margin theory of feedforward

neural networks. arXiv preprint arXiv:1810.05369  2018.

11

[37] Bo Xie  Yingyu Liang  and Le Song. Diversity leads to generalization in neural networks.

arXiv preprint Arxiv:1611.03131  2016.

[38] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian pro-
cess behavior  gradient independence  and neural tangent kernel derivation. arXiv preprint
arXiv:1902.04760  2019.

[39] Yuchen Zhang  Jason D Lee  and Michael I Jordan. l1-regularized neural networks are improp-
erly learnable in polynomial time. In International Conference on Machine Learning  pages
993–1001  2016.

[40] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery guar-

antees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175  2017.

[41] Difan Zou  Yuan Cao  Dongruo Zhou  and Quanquan Gu. Stochastic gradient descent opti-

mizes over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888  2018.

12

,Zeyuan Allen-Zhu
Yuanzhi Li