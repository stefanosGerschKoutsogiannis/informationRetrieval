2016,Finite-Dimensional BFRY Priors and Variational Bayesian Inference for Power Law Models,Bayesian nonparametric  methods based on the Dirichlet process (DP)  gamma process and beta process  have proven effective in capturing aspects of various datasets arising in machine learning.  However  it is now recognized that such processes have their limitations in terms of the ability to capture power law behavior. As such there is now considerable interest in models based on the Stable Processs (SP)  Generalized Gamma process (GGP) and Stable-beta process (SBP). These models present new challenges in terms of practical statistical implementation. In analogy to tractable processes such as the finite-dimensional Dirichlet process  we describe a class of random processes  we call iid finite-dimensional BFRY processes  that enables one to begin to develop efficient posterior inference algorithms such as variational Bayes that readily scale to massive datasets. For illustrative purposes  we describe a simple variational Bayes algorithm for normalized SP mixture models  and demonstrate its usefulness with experiments on synthetic and real-world datasets.,Finite-Dimensional BFRY Priors and Variational

Bayesian Inference for Power Law Models

Juho Lee

POSTECH  Korea

stonecold@postech.ac.kr

Lancelot F. James
HKUST  Hong Kong
lancelot@ust.hk

Seungjin Choi
POSTECH  Korea

seungjin@postech.ac.kr

Abstract

Bayesian nonparametric methods based on the Dirichlet Process (DP)  gamma pro-
cess and beta process  have proven effective in capturing aspects of various datasets
arising in machine learning. However  it is now recognized that such processes
have their limitations in terms of the ability to capture power law behavior. As such
there is now considerable interest in models based on the Stable Processs (SP) 
Generalized Gamma process (GGP) and Stable-Beta Process (SBP). These models
present new challenges in terms of practical statistical implementation. In analogy
to tractable processes such as the ﬁnite-dimensional Dirichlet process  we describe
a class of random processes  we call iid ﬁnite-dimensional BFRY processes  that
enables one to begin to develop efﬁcient posterior inference algorithms such as
variational Bayes that readily scale to massive datasets. For illustrative purposes 
we describe a simple variational Bayes algorithm for normalized SP mixture mod-
els  and demonstrate its usefulness with experiments on synthetic and real-world
datasets.

1

Introduction

the limit  as K → ∞  of a ﬁnite-dimensional Dirichlet process  PK(A) =(cid:80)K
variable  leads to a ﬁnite-dimensional Gamma process ΓK(A) =(cid:80)K

Bayesian non-parametric ideas have played a major role in various intricate applications in statistics
and machine learning. The Dirichlet process (DP) [1]  due to its remarkable properties and relative
tractability  has been the primary choice for many applications. It has also inspired the development of
variants such as the HDP [2] which can be seen as an inﬁnite-dimensional extension of latent Dirichlet
allocation [3]. While there are many possible descriptions of a DP  a most intuitive one is its view as
k=1 DkI{Vk∈A}  where
one can take (D1  . . .   DK) to be a K-variate symmetric Dirichlet vector on the (K − 1)-simplex
with parameters (θ/K  . . .   θ/K)  for θ > 0 and {Vk} are an arbitrary i.i.d. sequence of variables
over a space Ω  with law H(A) = Pr(Vk ∈ A). Multiplying by a Gθ  an independent Gamma(θ  1) 
k=1 GkI{Vk∈A} := GθPK(A) 
where {Gk} are i.i.d. Gamma(θ/K  1) variables  and one may set ΓK(Ω) = Gθ. It was shown
that limK→∞(PK  ΓK) d= ( ˜F0 θ  ˜µ0 θ)  where the limits correspond to a DP and a Gamma process
(GP) [4]. While (PK  ΓK) are often viewed as approximations to the DP and Gamma process (GP) 
the works of [5  6  7] and references therein demonstrate the general utility of these models.
The relationship between the GP and DP shows that the GP is a more ﬂexible random process. This
is borne out by its recognized applicability for a wider range of data structures. As such  it sufﬁces
to focus on ΓK as a tractable instance of what we refer to as an i.i.d. ﬁnite-dimensional process. In
k=1 JkδVk  is an i.i.d. ﬁnite-dimensional process if
d= ˜µ  where ˜µ is a
[(i)] For each ﬁxed K  (J1  . . .   JK) are i.i.d. random variables [(ii)] limK→∞ µK
completely random measure (CRM) [8]. In fact  from [9] (Theorem 14)  it follows that if the limit
exists ˜µ must be a CRM and therefore T := ˜µ(Ω) < ∞ is a non-negative inﬁnitely divisible random

general  we say a random process  µK(·) :=(cid:80)K

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

variable. On the other hand  it is important to note that  {Jk} and TK = µK(Ω) =(cid:80)K

k=1 Jk need
not be inﬁnitely divisible. We also point out there are many constructions of µK that converge to
the same ˜µ. According to [4]  for every CRM ˜µ one can construct special cases of µK that always
converge as follow: Let (C1  . . .   CK) denote a disjoint partition of Ω such that H(Ck) = 1/K for
d= ˜µ(Ck)  where the {Jk} are iid inﬁnitely divisible variables
k = 1  . . .   K  then one can set Jk
d= T. For reference we shall call such µK ﬁnite-dimensional Kingman processes or simply
and TK
Kingman proceses. It is clear that the ﬁnite-dimensional gamma process satisﬁes such a construction
with Jk = Gk and TK = Gθ. However  the nice tractable features of this special case  do not carry
over in general. This is due to the fact that there are many cases where the distribution of ˜µ(Ck)  is
not tractable either in the sense of not being easily simulated or having a relatively simple density.
The latter is of particular importance if one wants to consider developing inferential techniques for
CRM models that scale readily to large or massive data sets. An example of this would be variational
Bayes type methods  which would otherwise be well suited to the i.i.d. based models [10]. As such
we consider a ﬁner class of i.i.d. ﬁnite-dimensional processes as follows: We say µK is ideal if in
addition to [(i)] and [(ii)] it satisﬁes [(iii)] the Jk are easily simulated [(iv)] the density of Jk has
an explicit closed form suitable for application of techniques such as variational Bayes. We do not
attempt to specify any formal structure on what we mean by ideal  except to note that one can easily
recognize a choice of µK that is not ideal.
Our focus in this paper is not to explore the generalities of ﬁnite-dimensional processes. Rather
it is to identify speciﬁc ideal processes which are suitable for important cases where ˜µ is a Stable
process (SP)  or Generalized Gamma process (GGP). Furthermore by a simple transformation we
can construct processes that have behaviour similar to a Stable-Beta process (SBP). The SP  GGP 
SBP  and processes constructed from them  are now regarded as important alternatives to the DP 
GP and beta process (BP)  as they  unlike the (DP  GP  BP)  are better able to capture power law
behavior inherent in many types of datasets [11  12  13  14  15]. Unfortunately Kingman processes
based on SP  GGP or SBP are clearly not ideal. Indeed  if one considers for 0 < α < 1  T = Sα
a positive stable random variable  with density fα  then the corresponding stable process ˜µα 0  is
d= µα 0(Ck) d= K−1/αSα. While it is fairly easy to sample Sα and hence each Jk  it is
such that Jk
well-known that the density fα does not have generally a tractable form. Things become worse in
the GGP setting as the relevant density is formed by exponentially tilting the density fα. Finally it is
neither clear from the literature how to sample T for SBP  and much less have a simple form for its
corresponding density. Here we shall construct ideal processes based on various manipulations of a
class of µK we call ﬁnite-dimensional BFRY [16] processes. We note that BFRY random variables
appear in machine learning contexts in recent work [17]  albeit in a very different role.
Based on ﬁnite-dimensional BFRY processes  we provide simple variational Bayes algorithms for
mixture models based on normalized SP and GGP. We also derive collapsed variational Bayes
algorithms where the jumps are marginalized out. We demonstrate the effectiveness of our approach
on both synthetic and real-world datasets. Our intent here is to demonstrate how these processes can
be used within the context of variational inference. This in turn hopefully helps to elucidate how to
implement such procedures  or other inference techniques that beneﬁt from explicit densities  such as
hybrid Monte Carlo [18] or stochastic gradient MCMC algorithms [19].

2 Background

2.1 Completely random measure and Laplace functionals
Let (Ω F) be a measurable space  A random measure µ on Ω is completely random [8] if for any
disjoint A  B ∈ F  µ(A) and µ(B) are independent. It is known that any CRM can be written as
the sum of a deterministic measure  a measure with ﬁxed atoms  and a random measure represented
as a linear functional of the Poisson process [8]. In this paper  we focus on CRMs with only the
third component. Let Π be a Poisson process on R+ × Ω with mean intensity decomposed as
ν(ds  dω) = ρ(ds)H(dω). A realization of Π and corresponding CRM is written as

Π =

δ(sk ωk)  µ =

sΠ(ds  dω) =

skδωk .

(1)

Π(R+ Ω)(cid:88)

k=1

(cid:90) ∞

Π(R+ Ω)(cid:88)

0

2

k=1

We refer to ρ as the Lévy measure of µ and H as the base measure  and write µ ∼ CRM(ρ  H).
Examples of CRMs include the gamma process GP(θ  H) with Lévy measure ρ(ds) = θs−1e−sds
or the beta process BP(c  θ  H) with Lévy measure ρ(du) = θcu−1(1 − u)c−1I{0≤u≤1}du. Stable 
generalized gamma  and stable beta are also CRMs  and we will discuss them later.
A CRM is identiﬁed by its Laplace functional  just as a random variable is identiﬁed by its charac-
teristic function [20]. For a random measure µ and a measurable function f  the Laplace functional
Lµ(f ) is deﬁned as

Lµ(f ) := E[e−µ(f )]  µ(f ) :=

f (ω)µ(dω).

(cid:90)

Θ

(2)

(3)

(4)

(5)

When µ ∼ CRM(ρ  H)  the Laplace functional can be computed using the following theorem.
Theorem 1. (Lévy-Khintchine Formula [21]) For µ ∼ CRM(ρ  H) and measurable f on Ω 

(cid:26)

(cid:90)

(cid:90) ∞

(cid:27)

Lµ(f ) = exp

−

(1 − e−sf (ω))ρ(ds)H(dω)

.

2.2 Stable and related processes

Ω

0

A Stable Process SP(θ  α  H) is a CRM with Lévy measure
s−α−1ds 

ρ(ds) =

θ

Γ(1 − α)

and a Generalized Gamma Process GGP(θ  α  τ  H) is a CRM with Lévy measure

ρ(ds) =

θ

Γ(1 − α)

s−α−1e−τ sds 

where θ > 0  0 < α < 1  and τ > 0.
GGP is general in the sense that we can get many other processes from it. For example  by letting
α → 0 we get GP  and by setting τ = 0 we get SP. Furthermore  while it is well-known that the
Pitman-Yor process (see [22] and [23]) can be derived from SP  there is also a construction based
on GGP as follows. In particular as a consequence of ([23]  Proposition 21)  if we randomize θ =
Gamma(θ(cid:48)/α  1) in SP and normalize the jumps  then we get the Pitman-Yor process PYP(θ(cid:48)  α)
for θ(cid:48) > 0. The jumps of SP and GGP are known to be heavy-tailed  and this results in power-law
behaviour of data drawn from models having those processes as priors.
The stable beta process SBP(θ  α  c  H) is a CRM with Lévy measure

θΓ(1 + c)

ρ(du) =

(6)
where θ > 0  0 < α < 1  and c > −α. SBP can be viewed as a heavy-tailed extension of BP  and the
special case of c = 0 can be obtained by applying the transformation u = s/(s + 1) in SP.

Γ(1 − α)Γ(c + α)

u−α−1(1 − u)c+α−1I{0≤u≤1}du 

2.3 BFRY distributions

The BFRY distribution with parameter 0 < α < 1  written as BFRY(α)  is a random variable with
density

gα(s) =

α

Γ(1 − α)

s−α−1(1 − e−s).

(7)

We can simulate S ∼ BFRY(α) with S d= G/B  where G ∼ Gamma(1−α  1) and B ∼ Beta(α  1).
One can easily verify this by computing the density of the ratio distribution.
The name BFRY was coined in [16] after the work of Bertoin  Fujita  Roynette  and Yor [24]
who obtained explicit descriptions of the inﬁnitely divisible random variable and subordinator
corresponding to the density. However  the density arises much earlier  and can be found in a variety
of contexts  for instance in [23] (Proposition 12  Corollary 13 and see also Eq.(124)) and [25]. See

3

[17] for the use of BFRY distributions to induce the closed form Indian buffet process type generative
processes that have a type III power law behaviour.
We also explain some variations of BFRY distributions needed for the construction of ﬁnite-
dimensional BFRY processes for SP and GGP. First  we can scale the BFRY random variables
by some scale c > 0. In that case  we write S ∼ BFRY(c  α)  and the density is given as

gc α(s) =

c

Γ(1 − α)

s−α−1(1 − e−(α/c)1/αs).

(8)

We can easily sample S ∼ BFRY(c  α) as S d= (α/c)−1/αT where T ∼ BFRY(α). We can also
exponentially tilt the scaled BFRY random variable  with a parameter τ > 0. For that we write
S ∼ BFRY(c  τ  α)  and the density is given as

gc τ α(s) =

αs−α−1e−τ s(1 − e−(α/c)1/α
Γ(1 − α){(τ + (α/c)1/α)α − τ α} .

s)

(9)

We can simulate S ∼ BFRY(c  τ  α) as S d= GT where G ∼ Gamma(1 − α  1) and T is a random
variable with density 

h(t) =

αt−α−1

(τ + (α/c)1/α)α − τ α

I{(τ +(α/c)1/α)−1≤t≤τ−1} 

(10)

which can easily be sampled using inverse transform sampling.

3 Main Contributions

3.1 A Motivating example

beta process µK = (cid:80)K

Before we jump into our method  we ﬁrst revisit an example of ideal ﬁnite-dimensional processes.
Inspired by constructions of DP and GP  the Indian buffet process (IBP  [26]) was developed as a
model for feature selection  by considering the limit K → ∞ of an M × K binary matrix whose en-
tries {Zm k} are conditionally independent Bern(Uk) variables where {Uk} are i.i.d. Beta(θ/K  1)
variables. Although not explicitly described as such  this leads to the notion of a ﬁnite-dimensional
In [26]  IBP was obtained as the limit of the marginal dis-
tribution where µK was marginalized out  and this result coupled with [27] show indirectly that
limK→∞ µK → µ ∼ BP(θ  H). Here  we show another proof of this convergence  by inspecting the
Laplace functional of µK. The Laplace functional of µK is computed as follows:

k=1 UkδVk.

(cid:90) 1

θ
K

u

Ω

0

(cid:20)(cid:90)
LµK (f ) = E[e−µK (f )] =
(cid:90)
(cid:90) 1
(cid:90) ∞

(cid:20)
(cid:26)

1 − 1
K

(cid:90)

=

Ω

0

θ

K −1e−uf (ω)duH(dω)

θ

K −1(1 − e−uf (ω))duH(dω)

θu

(cid:21)K

(cid:21)K

.

(11)

(cid:27)

 

(12)

Since uθ/K is bounded by 1  the bounded convergence theorem implies

K→∞LµK (f ) = exp

lim

−

Ω

0

(1 − e−uf (ω))θu−1I{0≤u≤1}duH(dω)

which exactly matches the Laplace functional of µ computed by Eq. (3). In contrast to the marginal
likelihood arguments  in our proof  we illustrate the direct relationship between the random measures
and suggest a blueprint that can be applied to other CRMs. Note that the ﬁnite-dimensional beta
process is not a Kingman process  since the beta variables are not inﬁnitely divisible and the total
mass T is a Dickman variable. We can also apply our argument to the case of the ﬁnite-dimensional
gamma process  the proof of which is given in our supplementary material.

3.2 Finite-dimensional BFRY processes

Inspired by the ﬁnite-dimensional beta and gamma process examples  we propose ﬁnite-dimensional
BFRY processes  which converge to SP  GGP  and SBP as K → ∞.

4

Theorem 2. (Finite-dimensional BFRY processes)

(i) Let µ ∼ SP(θ  α  H). Construct µK as follows:

J1  . . .   JK

i.i.d.∼ BFRY(θ/K  α) 

V1  . . .   VK

i.i.d.∼ H  µK =

JkδVk .

(13)

(ii) Let µ ∼ GGP(θ  α  τ  H). Construct µK as follows:

J1  . . .   JK

i.i.d.∼ BFRY(θ/K  τ  α) 

V1  . . .   VK

(iii) Let µ ∼ SBP(θ  α  0  H). Construct µK as follows:

S1  . . .   SK

i.i.d.∼ BFRY(θ/K  α) 

V1  . . .   VK

i.i.d.∼ H  µK =

i.i.d.∼ H  µK =

JkδVk .

(14)

k=1

for k = 1  . . .   K 

JkδVk .

(15)

Jk = Sk
Sk+1

K(cid:88)

K(cid:88)
K(cid:88)

k=1

For all three cases  limK→∞ Lf (µK) = Lf (µ) for an arbitrary measurable f.

k=1

(cid:21)K

(cid:20)(cid:90)
(cid:20)

(cid:90) ∞
(cid:90)

0

Ω

1 − 1
K

Ω

=

(cid:90) ∞
(cid:26)

0

KΓ(1 − α)

Γ(1 − α)

θ

(cid:90) ∞

(cid:90)

Proof. We ﬁrst provide a proof for SP case (i)  and the proof for GGP (ii) is almost identical. The
Laplace functional of µK is written as
LµK (f ) =

s−α−1(1 − e−(αK/θ)1/αs)dsH(dω)

e−sf (ω)

θ

(cid:21)K

(1 − e−sf (ω))s−α−1(1 − e−(αK/θ)1/αs)dsH(dω)

Since 1 − e−(αK/θ)s is bounded by 1  the bounded convergence theorem implies 
s−α−1dsH(dω)

(1 − e−sf (ω))

−

θ

K→∞LµK (f ) = exp

lim

Γ(1 − α)

Ω

0

which exactly matches the Laplace functional of SP. The proof of (iii) is trivial from (i) and the
relationship between SP and SBP.
Corollary 1. Let τ = 1 and α → 0 in (14). Then µK will converge to µ ∼ GP(θ  H).
Proof. The result is trivial by letting α → 0 in Lf (µK).

(cid:27)

 

Finite-dimensional BFRY processes are certainly ideal processes  since
we can easily sample the jumps {Jk}  and we have explicit closed form
densities written as (8) and (9). Hence  based on those processes  we
can develop efﬁcient inference algorithms such as variational Bayes for
power-law models related to SP  GGP  and SBP that require explicit
densities of jumps. Figure 1 illustrates the log of average jump sizes
of 100 normalized SPs drawn using ﬁnite-dimensional BFRY processes 
with θ = 1  K = 1000  and varying α. As expected  the jumps generated
with bigger α are more heavy-tailed.

Figure 1: Log of average
jump sizes of NSPs

3.3 Finite-dimensional normalized random measure mixture models

A normalized random measure (NRM) is obtained by normalizing a CRM by its total mass. A NRM
mixture model (NRMM) is then deﬁned as a mixture model with NRM prior  and its generative
process is written as follows:

µ ∼ CRM(ρ  H)  φ1  . . . φN

(16)
where L is a likelihood distribution. One can easily do posterior inferences by marginalizing out µ 
with an auxiliary variable. Once µ is marginalized out we can develop a Gibbs sampler [28]. However 
this scales poorly as mentioned earlier. On the other hand  one may replace µ with µK  yielding
the ﬁnite-dimensional NRMM (FNRMM)  for which efﬁcient variational Bayes can be developed
provided that the ﬁnite-dimensional process is ideal.

i.i.d.∼ µ/µ(Ω)  Xn|φn ∼ L(φn) 

5

123456atom indices (sorted)-6-4-20log (jumps)alpha=0.8alpha=0.4alpha=0.23.4 Variational Bayes for ﬁnite-dimensional mixture models

We ﬁrst introduce a variational Bayes algorithm for ﬁnite-dimensional normalized SP mixture
(FNSPM). The joint likelihood of the model is written as

where s· := (cid:80)

Pr({Xn ∈ dxn  zn} {Jk ∈ dsk  Vk ∈ dωk}) = s

−N·

sNk
k gθ/K α(dsk)

k=1

zn=k

L(dxn|ωk)H(dωk) 

(17)

convenient to introduce an auxiliary variable U ∼ Gamma(N  s·) as in [20] to remove s−N·

k sk  and zn is an indicator variable such that zn = k if φn = ωk. We found it

:

Pr({Xn ∈ dxn  zn} {Jk ∈ dsk  Vk ∈ dωk}  U ∈ du)

K(cid:89)

(cid:89)

K(cid:89)

k=1

(cid:89)

zn=k

∝ uN−1du

sNk
k e

−usk gθ/K α(sk)dsk

L(dxn|ωk)H(dωk).

(18)

Now we introduce variational distributions for {z  s  ω  u} and optimize the Evidence Lower BOund
(ELBO) with respect to the parameters of the variational distributions. The posterior statistics can be
simulated using the optimized variational distributions. We can also optimize the hyperparamters θ
and α with ELBO. The detailed optimization procedure is described in the supplementary material.

3.5 Collapsed Gibbs sampler for ﬁnite-dimensional mixture models
As with the NRMM  we can also marginalize out the jumps {Jk} to get the collapsed model.
(cid:21)I{Nk >0}
Marginalizing out s in (18) gives

(cid:20) θ(1 − ξNk−α)

Pr({Xn ∈ dxn  zn} {Vk ∈ dωk}  U ∈ du) ∝ uN−1du

K(cid:89)

uNk−α

Γ(Nk − α)
Γ(1 − α)

(cid:20) θuα

α

×

−α − 1)

(ξ

(cid:21)I{Nk =0} (cid:89)

zn=k

k=1

L(dxn|ωk)H(dωk) 

(19)

where ξ :=
and the detailed equations are in the supplementary material.

u+(αK/θ)1/α . Based on this  we can derive the collapsed Gibbs sampler for FNSPM 

u

3.6 Collapsed variational Bayes for ﬁnite-dimensional mixture models

Based on the marginalized log likelihood (19)  we can develop a collapsed variational Bayes algorithm
for FNSPM  following the collapsed variational inference algorithm for DPM [29]. We introduce
variational distributions for {u  z  ω}  and then the update equation for q(z) is computed using the
conditional posterior p(z|x). The hyperparamters can also be optimized  the detailed procedures for
which are explained in the supplementary material.

4 Experiments

4.1 Experiments on synthetic datasets

4.1.1 Data generation

We generated 10 datasets from PYP mixture models. Each dataset was generated as follows. We
ﬁrst generated cluster labels for 2 000 data points from PYP(θ  α) with θ = 1 and α = 0.7. Given
the cluster labels  we generated data points from Mult(M  ω)  where the number of trials M was
chosen uniformly from [1  50] and ω was sampled from Dir(0.05 · 1200). We also generated another
10 datasets from CRP mixture models CRP(θ) with θ = 1  to see if FNSPM adapts to the change of
the underlying random measure. For each dataset  we used 80% of data points for training and the
remaining 20% for testing.

4.1.2 Algorithm settings and performance measure

We compared six algorithms - Collapsed Gibbs (CG) for FDPM (CG/D)  Variational Bayes (VB) for
FDPM (VB/D)  Collapsed Variational Bayes (CVB) for FDPM (CVB/D)  CG for FNSPM (CG/S) 

6

Figure 2: (Left) comparison between the inﬁnite-dimensional algorithm and the ﬁnite dimensional algorithms.
(Middle) Average times per iteration of the inﬁnite and the ﬁnite dimensional algorithms. (Right) Average
number of iterations need to converge for variational algorithms.

Table 1: Comparison between six ﬁnite-dimensional algorithms on synthetic PYP  synthetic CRP  AP corpus and
NIPS corpus. Average test log-likelihood values and α estimates are shown with standard deviations.

PYP

CRP

AP

NIPS

loglikel
CG/D -33.2078
(1.5557)
VB/D -33.4480
(1.6495)
CVB/D -33.4278
(1.6525)
-33.1039
(1.5676)
-33.1861
(1.5873)
-33.2031
(1.5858)

CVB/S

CG/S

VB/S

α

-

-

-

0.6940
(0.0235)
0.4640
(0.0085)
0.7041
(0.0322)

loglikel
-25.4076
(1.9081)
-25.4148
(1.9120)
-25.4150
(1.9115)
-25.4079
(1.9077)
-25.5076
(1.9122)
-25.4080
(1.9085)

α

-

-

-

0.2867
(0.0762)
0.4770
(0.0041)
0.2925
(0.0608)

loglikel
-157.2228
(0.0189)
-157.2379
(0.0304)
-157.2302
(0.0280)
-157.1920
(0.0036)
-157.1391
(0.1154)
-157.2182
(0.0282)

α

-

-

-

0.5261
(0.0032)
0.4748
(0.0434)
0.5327
(0.0060)

loglikel
-352.8909
(0.0070)
-352.9104
(0.0172)
-352.8692
(0.0321)
-352.7487
(0.0037)
-352.6078
(0.2599)
-352.7544
(0.0088)

α

-

-

-

0.5857
(0.0032)
0.4945
(0.0324)
0.5899
(0.0070)

VB for FNSPM (VB/S) and CVB for FNSPM (CVB/S). All the algorithms were initialized with a
single run of sequential collapsed Gibbs sampling starting from zero clusters  and afterwards ran for
100 iterations. The variational algorithms were terminated if the improvements of the ELBO were
smaller than a threshold. The hyperparameters θ and α were initialized as θ = 1 and α = 0.5 for all
algorithms. The performances were measured by average test log-likelihood 

Ntest(cid:88)

n=1

1

Ntest

log p(xn|xtrain).

(20)

For CG  we computed the average of samples collected every 10 iterations. For VB and CVB  we
computed the log-likelihood using the expectations of the variational distributions.

4.1.3 Effect of K on predictive performance and running time

To see the effect of K on predictive performance  we ﬁrst compared the ﬁnite-dimensional algorithms
(CG for FNSPM  VB for FNSPM and CVB for FNSPM) to the inﬁnite-dimensional algorithm
(CG for NSPM [28]). We tested the four algorithms on 10 synthetic datasets generated from PYP
mixtures  with K ∈ {200  400  600  800  1000} for ﬁnite algorithms  and measured the difference
of average test log likelihood compared to the inﬁnite-dimensional algorithm. We also measured
the average running time per iteration of the four algorithms  and the average number of iterations
to converge of the variational algorithms. Figure 2 shows the results. As expected  the difference
between ﬁnite-dimensional algorithms and the inﬁnite-dimensional algorithm decreases as K grows.
The ﬁnite-dimensional algorithms have O(N K) time complexity per iteration  and the inﬁnite-
dimensional algorithm has O(N ˜K) where ˜K is the maximum number of clusters created during
clustering. However  in practice  variational algorithms can be implemented with efﬁcient matrix
multiplications  and this makes them much faster than sampling algorithms. Moreover  as shown in
Figure 2  variational algorithms usually converge in 50 iterations.

7

2004006008001000dimension K00.511.5test log likel diffCGNSPM - CGFNSPMCGNSPM - VBFNSPMCGNSPM - CVBFNSPM2004006008001000dimension K0123time / iter [sec]CGNSPMCGFNSPMVBFNSPMCVBFNSPM2004006008001000dimension K02040iter to convergeVBFNSPMCVBFNSPM4.1.4 Comparing ﬁnite-dimensional algorithms on PYP and CRP datasets

We compared six algorithms for ﬁnite mixture models (CG/D  VB/D  CVB/D  CG/S  VB/S and
CVB/S) on PYP mixture datasets and CRP mixture datasets  with K = 1000. The results are summa-
rized in Table 1. On PYP datasets  in general  FNSPM outperformed FDPM and CG outperformed
VB and CVB. CG/S consistently outperformed CG/D  and the same relationship applied to VB/S
and VB/D and CVB/S and CVB/D. Even though VB/S and CVB/S were variational algorithms  the
performance gap between them and CG/S was not signiﬁcant. Table 1 shows the estimated α values
for CG/S  VB/S and CVB/S. CG/S and CVB/S seemed to recover the true value α = 0.7  but VB/S
didn’t. We found that VB/S tends to control the other parameter θ while holding α near its initial
value 0.5. On CRP datasets  all the algorithms showed similar performances except for VB/S  which
was consistently worse than other algorithms. This is probably due to the bad estimates of α.

4.2 Experiments on real-world documents

We compared the six algorithms on real-world document clustering task by clustering AP corpus 1 and
NIPS corpus 2. We preprocessed the corpora using latent Dirichlet allocation (LDA) [3]. We ran LDA
with 300 topics  and then gave each document a bag-of-words representation of topic assignments
to those 300 topics. We assumed that those representations were generated from the multinomial-
Dirichlet model  and clustered them using FDPM and FNSPM. We used 80% of documents for
training and the remaining 20% for computing average test log-likelihood. We set K = 2  000 and ran
each algorithm for 200 iterations. We repeated this 10 times to measure the average performance.The
results are summarized in Table 1. In general  the algorithms based on FNSPM showed better
performance than those of FDPM based ones  implying that FNSPM based algorithms are well
capturing the heavy-tailed cluster distributions of the corpora. VB/S performed the best  even though
it sometimes converged to poor values.

5 Conclusion

In this paper  we proposed ﬁnite-dimensional BFRY processes that converge to SP  GGP and SBP. The
jumps of the ﬁnite-dimensional BFRY processes have nice closed-form densities  and this leads to
the efﬁcient posterior inference algorithms. With ﬁnite-dimensional BFRY processes  we developed
variational Bayes and collapsed variational Bayes for ﬁnite-dimensional normalized SP mixture
models  and demonstrated its performance both on synthetic and real-world datasets. As mentioned
earlier  with ﬁnite dimensional BFRY processes one can develop variational Bayes or other posterior
inference algorithms for a variety of models with SP  GGP and SBP priors. This fact  along with
more theoretical properties of ﬁnite-dimensional processes  presents interesting avenues for future
research.
Acknowledgements: This work was supported by IITP (No. B0101-16-0307  Basic Software
Research in Human-Level Lifelong Machine Learning (Machine Learning Center)) and by National
Research Foundation (NRF) of Korea (NRF-2013R1A2A2A01067464)  and supported in part by the
grant RGC-HKUST 601712 of the HKSAR.

References
[1] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of Statistics  1(2):209–

230  1973.

[2] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. Journal of the

American Statistical Association  101(476):1566–1581  2006.

[3] D. M. Blei  A. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research 

3:993–1022  2003.

[4] J. F. C. Kingman. Random discrete distributions. Journal of the Royal Statistical Society. Series B

(Methodological)  37(1):1–22  1975.

1http://www.cs.princeton.edu/~blei/lda-c/
2https://archive.ics.uci.edu/ml/datasets/Bag+of+Words

8

[5] H. Ishwaran and L. F. James. Computational methods for multiplicative intensity models using weighted
gamma processes: proportional hazards  marked point processes  and panel count data. Journal of the
American Statistical Association  99:175–190  2004.

[6] H. Ishwaran  L. F. James  and J. Sun. Bayesian model selection in ﬁnite mixtures by marginal density

decompositions. Journal of the American Statistical Association  96:1316–1332  2001.

[7] H. Ishwaran and M. Zarepour. Dirichlet prior seives in ﬁnite normal mixtures. Statistica Sinica  12(3):941–

963  2002.

[8] J. F. C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics  21(1):59–78  1967.

[9] J. Pitman and N. M. Tran. Size-biased permutation of a ﬁnite sequence with independent and identically

distributed terms. Bernoulli  21(4):2484–2512  2015.

[10] D. M. Blei  A. Kucukelbir  and J. D. McAuliffe. Variational inference: A review for statisticians. 2016.

arXiv:1601.00670.

[11] C. Chen  N. Ding  and W. Buntine. Dependent hierarchical normalized random measures for dynamic

topic modeling. ICML  2012.

[12] Y. W. Teh and D. Görür. Indian buffet processes with power-law behavior. NIPS  2009.

[13] F. Caron and E. B. Fox. Sparse graphs using exchangeable random measures. 2014. arXiv:1401.1137.

[14] F. Caron. Bayesian nonparametric models for bipartite graphs. NIPS  2012.

[15] V. Veitch and D. M. Roy. The class of random graphs arising from exchangeable random measures. 2015.

arXiv:1512.03099.

[16] L. Devroye and L. F. James. On simulation and properties of the stable law. Statistical Methods and

Applications  23(3):307–343  2014.

[17] L. F. James  P. Orbanz  and Y. W. Teh. Scaled subordinators and generalizations of the Indian buffet

process. 2015. arXiv:1510.07309.

[18] S. Duane  A. D. Kennedy  B. J. Pendleton  and D. Roweth. Hybrid Monte Carlo. Physics Letters B 

195(2):216–222  1987.

[19] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. ICML  2011.

[20] L. F. James. Bayesian Poisson process partition calculus with an application to Bayesian Lévy moving

averages. The Annals of Statistics  33(4):1771–1799  2005.

[21] E. Çinlar. Probability and stochastics. 2010.

[22] H. Ishwaran and L. F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American

Statistical Association  96(453):161–173  2001.

[23] J. Pitman and M. Yor. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator.

The Annals of Probability  25(2):855–900  1997.

[24] J. Bertoin  T. Fujita  B. Roynette  and M. Yor. On a particular class of self-decomposable random variables:
the durations of Bessel excursions straddling independent exponential times. Probability and Mathematical
Statistics  26:315–366  2006.

[25] M. Winkel. Electronic foreign-exchange markets and passage events of independent subordinators. Journal

of Applied Probability  42(1):138–152  2005.

[26] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. NIPS  2006.

[27] R. Thibaux and M. I. Jordan. Hierarchcial beta processes and the Indian buffet process. AISTATS  2007.

[28] S. Favaro and Y. W. Teh. MCMC for normalized random measure mixture models. Statistical Science 

28(3):335–359  2013.

[29] K. Kurihara  M. Welling  and Y. W. Teh. Collapsed variational dirichlet process mixture models. IJCAI 

2007.

9

,Tzu-Kuo Huang
Alekh Agarwal
Daniel Hsu
John Langford
Robert Schapire
Juho Lee
Lancelot James
Seungjin Choi