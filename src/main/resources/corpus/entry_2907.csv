2011,Learning Eigenvectors for Free,We extend the classical problem of predicting a sequence  of outcomes from a finite alphabet to the matrix domain.   In this extension  the alphabet of $n$ outcomes is replaced by the set of all dyads  i.e.   outer products $\u\u^\top$ where $\u$ is a vector in $\R^n$ of unit length.  Whereas in the classical case the goal is to learn   (i.e. sequentially predict as well as) the best multinomial distribution   in the matrix case we desire to learn the density matrix  that best explains the observed sequence of dyads. We show  how popular online algorithms for learning a multinomial  distribution can be extended to learn density matrices.  Intuitively  learning the $n^2$ parameters of a density matrix is much harder than learning the $n$ parameters of a multinomial distribution. Completely surprisingly  we prove that the worst-case regrets of   certain classical algorithms and   their matrix generalizations are identical.   The reason is that the worst-case sequence of dyads share a common   eigensystem  i.e. the worst case regret is achieved in the  classical case. So these matrix algorithms learn the eigenvectors  without any regret.,Learning Eigenvectors for Free

Wouter M. Koolen

Royal Holloway and CWI
wouter@cs.rhul.ac.uk

Wojtek Kotłowski

Centrum Wiskunde & Informatica

Manfred K. Warmuth

UC Santa Cruz

kotlowsk@cwi.nl

manfred@cse.ucsc.edu

Abstract

We extend the classical problem of predicting a sequence of outcomes from a ﬁ-
nite alphabet to the matrix domain. In this extension  the alphabet of n outcomes is
replaced by the set of all dyads  i.e. outer products uu(cid:62) where u is a vector in Rn
of unit length. Whereas in the classical case the goal is to learn (i.e. sequentially
predict as well as) the best multinomial distribution  in the matrix case we desire
to learn the density matrix that best explains the observed sequence of dyads. We
show how popular online algorithms for learning a multinomial distribution can
be extended to learn density matrices. Intuitively  learning the n2 parameters of a
density matrix is much harder than learning the n parameters of a multinomial dis-
tribution. Completely surprisingly  we prove that the worst-case regrets of certain
classical algorithms and their matrix generalizations are identical. The reason is
that the worst-case sequence of dyads share a common eigensystem  i.e. the worst
case regret is achieved in the classical case. So these matrix algorithms learn the
eigenvectors without any regret.

1

Introduction

We consider the extension of the classical online problem of predicting outcomes from a ﬁnite
alphabet to the matrix domain. In this extension  the alphabet of n outcomes is replaced by a set of
all dyads  i.e. outer products uu(cid:62) where u is a unit vector in Rn. Whereas classically the goal is
to learn as well as the best multinomial distribution over outcomes  in the matrix case we desire to
learn the distribution over dyads that best explains the sequence of dyads seen so far. A distribution
on dyads is summarized as a density matrix  i.e. a symmetric positive-deﬁnite1 matrix of unit trace.
Such matrices are heavily used in quantum physics  where dyads represent states. We will show
how popular online algorithms for learning multinomials can be extended to learn density matrices.
Considerable attention has been placed recently on generalizing algorithms for learning and opti-
mization problems from probability vector parameters to density matrices [17  19]. Efﬁcient semi-
deﬁnite programming algorithms have been devised [1] and better approximation algorithms for
NP-hard problems have been obtained [2] by employing on-line algorithms that update a density
matrix parameter. Also two important quantum complexity classes were shown to collapse based on
these algorithms [8]. Even though the matrix generalization led to progress in many contexts  in the
original domain of on-line learning  the regret bounds proven for the algorithms in the matrix case
are often the same as those provable for the original classical ﬁnite alphabet case [17  19]. Therefore
it was posed as an open problem to determine whether this is just a case of loose classical bound or
whether there truly exists a “free matrix lunch” for some of these algorithms [18]. Such algorithms
essentially would learn the eigensystem of the data for free without incurring any additional regret.
This is non-intuitive  since one would expect a matrix to have n2 parameters and be much harder to
learn than an n dimensional parameter vector.

1We use positive in the non-strict sense  and omit ‘symmetric’ and ‘deﬁnite’. Our matrices are real-valued.

1

for trial t = 1  2  . . .   T do

Algorithm predicts with probability vector ωt−1
Nature responds with outcome xt.
Algorithm incurs loss − log ωt−1 xt.

end for

for trial t = 1  2  . . .   T do

Algorithm predicts with density matrix Wt−1
Nature responds with density matrix Xt.

Algorithm incurs loss − tr(cid:0)Xt log(Wt−1)(cid:1).

end for

Probability vector prediction

Density matrix prediction

Figure 1: Protocols

probability vector as a mixture of these events: ω =(cid:80)

In this paper we investigate this frivolously named but deep “free matrix lunch” question in arguably
the simplest context: learning a multinomial distribution. In the classical case  there are n ≥ 2
outcomes and a distribution is parametrized by an n-dimensional probability vector ω  where ωi is
the probability of outcome i. One can view the base vectors ei as the elementary events and the
i ωiei. We deﬁne a “matrix generalization”
of a multinomial which is parametrized by a density matrix W (positive matrix of unit trace). Now
the elementary events are dyads of the form uu(cid:62)  where u is a unit vector in Rn. Dyads are the
representations of states used in quantum physics [20]. A density matrix is a mixture of dyads.
Whereas probability vectors represent uncertainty over n basis vectors  density matrices can be
viewed as representing uncertainty over inﬁnitely many dyads in Rn.
In the classical case  the algorithm predicts at trial t with multinomial ωt−1. Nature produces an
outcome xt ∈ {1  . . .   n}  and the algorithm incurs loss − log(ωt−1 xt). The most common heuristic
(a.k.a. the Laplace estimator) chooses ωt−1 i proportional to 1 plus the number of previous trials in
which outcome i was observed. The on-line algorithms are evaluated by their worst-case regret over
data sequences  where the regret is the additional loss of the algorithm over the total loss of the best
probability vector chosen in hindsight.
In this paper we develop the corresponding matrix setting  where the algorithm predicts with a den-
sity matrix Wt−1  Nature produces a dyad xtx(cid:62)
t log(Wt−1)xt.
Here log denotes the matrix logarithm. We are particularly interested in how the regret changes
when the algorithms are generalized to the matrix case. Surprisingly we can show that for the
Laplace as well as the Krichevsky-Troﬁmov [10] estimators the worst-case regret is the same in the
matrix case as it is in the classical case. For the Last-Step Minimax algorithm [16]  we can prove
the same regret bound for the matrix case that was proven for the classical case.
Why are we doing this? Most machine learning algorithms deal with vector parameters. The goal of
this line of research is to develop methods for handling matrix parameters. We are used to dealing
with probability vectors. Recently a probability calculus was developed for density matrices [20]
including various Bayes rules for updating generalized conditionals. The vector problems are typi-
cally retained as special cases of the matrix problems  where the eigensystem is ﬁxed and only the
vectors of eigenvalues has to be learned. We exhibit for the ﬁrst time a basic fundamental prob-
lem  for which the regret achievable in the matrix case is no higher than the regret achievable in the
original vector setting.

t   and the algorithm incurs loss −x(cid:62)

Paper outline Deﬁnitions and notation are given in the next section  followed by proofs of the
free matrix lunch for the three discussed algorithms in Section 3. At the core of our proofs is
a new technical lemma for mixing quantum entropies. We also discuss the minimax algorithm
for multinomials due to Shtarkov  and corresponding minimax algorithm for density matrices. We
provide strong experimental evidence that the free matrix lunch holds for this algorithm as well. To
put the results into context  we motivate and discuss our choice of the loss function  and compare it
to several alternatives in Section 4. More discussion and perspective is provided in the Section 5.

2 Setup

The protocol for the classical probability vector prediction problem and the new density matrix
prediction problem are displayed side-by-side in Figure 1. We explain the latter problem. Learning
proceeds in trials. During trial t the algorithm predicts with a density matrix Wt−1. We use index
t− 1 to indicate that is based on the t− 1 previous outcomes. Then nature responds with an outcome

2

(cid:96)(Wt−1  Xt) := − tr(cid:0)Xt log(Wt−1)(cid:1) 

density matrix Xt. The discrepancy between prediction and outcome is measured by the matrix
entropic loss

(cid:80)
i ωt−1 i eie(cid:62)

i for some probability vector ωt−1  and the outcome Xt is an eigendyad eje(cid:62)

(1)
where log denotes matrix logarithm2. When the outcome density matrix Xt is a dyad xtx(cid:62)
t   then
this loss becomes −x(cid:62)
t log(Wt−1)xt  which is the simpliﬁed form of the entropic loss discussed
in the introduction. Also if the prediction density matrix is diagonal  i.e. it has the form Wt−1 =
j of the
same eigensystem  then this loss simpliﬁes to the classical log loss: (cid:96)(Wt−1  Xt) = − log(ωt−1 j).
The above deﬁnition is not the only way to promote the log loss to the matrix domain. Yet  in Section
4 we justify this choice.
We aim to design algorithms with low regret compared to the best ﬁxed density matrix in hindsight.
The loss of the best ﬁxed density matrix can be expressed succinctly in terms of the von Neumann
entropy  which is deﬁned for any density matrix D as H(D) := − tr(D log D)  and the sufﬁ-

cient statistic ST = (cid:80)T

(cid:1) . For ﬁxed data

t=1 Xt as follows: infW

X1  . . .   XT   the regret of a strategy that issues prediction Wt after observing X1  . . .   Xt is

t=1 (cid:96)(W   Xt) = T H(cid:0) ST
(cid:80)T
(cid:18) ST

(cid:19)

T

(cid:96)(Wt−1  Xt) − T H

 

T

(2)

T(cid:88)

t=1

and the worst-case regret on T trials is obtained by taking supX1 ... XT over (2). Our aim is to
design strategies for density matrix prediction that have low worst-case regret.

3 Free Matrix Lunches

In this section  we will show how four popular online algorithms for learning multinomials can be
extended to learning density matrices. We start with the simple Laplace estimator  continue with
its improved version known as the Krichevsky-Troﬁmov estimator  and also extend the less known
Last Step Minimax strategy which has even less regret. We will prove a version of the free matrix
lunch (FML) for all three algorithms. Finally we discuss the minimax algorithm for which we have
experimental evidence that the free matrix lunch holds as well.

3.1 Laplace

predicts with the probability vector ωt := σt+1

After observing classical data with sufﬁcient statistic vector σt = (cid:80)t
analogy  after observing matrix data with sufﬁcient statistic St =(cid:80)t

q=1 exq  classical Laplace
t+n consisting of the normalized smoothed counts. By
q=1 Xt  matrix Laplace predicts
with the correspondingly smoothed matrix Wt := St+I
t+n . Classical Laplace is commonly motivated
as either the Bayes predictive distribution w.r.t. the uniform prior or as a loss minimization with
virtual outcomes [3]. The latter motivation can be “lifted” to the matrix domain by adding n virtual
outcomes at I/n:

(cid:40)

t(cid:88)

(cid:41)

Wt = argmin
W dens. mat.

The worst-case regret of classical Laplace after T iterations equals log(cid:0)T +n−1

n (cid:96)(W   I/n) +

(cid:96)(W   Xq)

q=1

=

n−1

St + I
t + n

.

(3)

(cid:1) ≤ (n−1) log(T +1)

(see e.g. [6]). We now show that in the matrix case  no additional regret is incurred.
Theorem 1 (Laplace FML). The worst-case regrets of classical and matrix Laplace coincide.
Proof. Let W ∗
Laplace can be bounded as follows:

t denote the best density matrix for the ﬁrst t outcomes. The regret (2) of matrix

T(cid:88)

(cid:96)(Wt−1  Xt) − T(cid:88)

T   Xt) ≤ T(cid:88)
2For any positive matrix with eigendecomposition A =(cid:80)

(cid:96)(W ∗

t=1

t=1

t=1

(cid:16)

(cid:17)

(cid:96)(Wt−1  Xt) − (cid:96)(W ∗

i   log(A) :=(cid:80)

i αi aia(cid:62)

t   Xt)

.

(4)

i log(αi) aia(cid:62)
i .

3

(cid:18)

(cid:18)

Now consider each term in the right-hand sum separately. The tth term equals

(cid:19)(cid:19)

(cid:18) t − 1 + n

(cid:19)

− tr(cid:0)Xt

(cid:0)log(St−1 + I) − log St

(cid:1)(cid:1) .

− tr

Xt

log

St−1 + I
t − 1 + n

− log

St
t

= log

t

Note that the ﬁrst term constitutes the “classical” part of the per-round regret  while the second term
is the “matrix” part. The matrix part is non-positive since St−1 + I (cid:23) St  and the logarithm is a
matrix monotone operation (i.e. A (cid:23) B implies log A (cid:23) log B). By omitting it  we obtain an
upper bound on the regret of matrix Laplace  that is tight: for any sequence of identical dyads the
T for all t ≤ T . The same upper
matrix part is zero and (4) holds with equality since W ∗
bound is also met by classical Laplace on any sequence of identical outcomes [6].

t = W ∗

We just showed that matrix Laplace has the same worst-case regret as classical Laplace  albeit matrix
Laplace learns a matrix of n2 parameters whereas classical Laplace only learns n probabilities. No
additional regret is incurred for learning the eigenvectors. Matrix Laplace can update Wt in O(n2)
time per trial. The same will be true for our next algorithm.

3.2 Krichevsky-Troﬁmov (KT)

2 to each count  i.e. ωt := σt+1/2

t+n/2 and Wt := St+I/2
Classical and matrix KT smooth by adding 1
t+n/2 .
The former can again be obtained as the Bayes predictive distribution w.r.t. Jeffreys’ prior  the latter
as the solution to the matrix entropic loss minimization problem (3) with n/2 virtual outcomes
instead of n for Laplace.
The leading term in the worst-case regret for classical KT is the optimal 1
2 log(T ) rate per parameter
instead of the log(T ) rate for Laplace. More precisely  classical KT’s worst-case regret after T
iterations is known to be log Γ(T +n/2)
Again we show that no additional regret is incurred in the matrix case.
Theorem 2 (KT FML). The worst-case regrets of classical and matrix KT coincide.

(cid:0)log(T + 1) + log(π)(cid:1) (see e.g. [6]).

Γ(T +1/2) + log Γ(1/2)

Γ(n/2) ≤ n−1

2

Lemma 1. For positive matrices A  B with A =(cid:80)

The proof uses the following key entropy decomposition lemma (proven in Appendix A):

the eigendecomposition of A:

i

i αi aia(cid:62)

H(cid:0)A + tr(B) aia(cid:62)

i

a(cid:62)
i Bai
tr(B)

(cid:18) St−1 + Xt

(cid:19)

t

(cid:1) 
(cid:18) St−1

t − 1

(cid:19)(cid:19)

+ (t − 1)H

.

(5)

(cid:80)n
i=1 σi sis(cid:62)

i=1

t=1

T(cid:88)

H(A + B) ≥ n(cid:88)
(cid:18)
− tr(cid:0)Xt log(Wt−1)(cid:1) − tH
n(cid:88)
≥ n(cid:88)

− tr(cid:0)Xt log(Wt−1)(cid:1) = − tr(cid:0)Xt
(cid:19)
(cid:18) St−1 + Xt
(cid:18)

i=1

i=1

H

t

s(cid:62)
i Xtsi

− log(ωt−1 i) − tH

δt :=

n(cid:88)

i=1

Moreover  it follows from Lemma 1 that:

which  in turn  is at most:

(cid:18)

Proof of Theorem 2. We start by telescoping the regret (2) of matrix KT as follows

We bound each term separately. Let us denote the eigendecomposition of St−1 by St−1 =

i . Notice that since Wt−1 plays in the eigensystem of St−1  we have:

log(ωt−1 i) sis(cid:62)

s(cid:62)
i Xtsi log(ωt−1 i).

i

(cid:1) = − n(cid:88)
(cid:18) St−1 + sis(cid:62)

i=1

i

(cid:19)

.

t

(cid:19)

s(cid:62)
i XtsiH

(cid:18) St−1 + sis(cid:62)
(cid:19)
(cid:18) St−1 + sis(cid:62)

t

i

i

t

4

+ (t − 1)H

 

(6)

(cid:19)(cid:19)

(cid:18) St−1
(cid:19)(cid:19)
(cid:18) St−1

t − 1

t − 1

.

Taking this equality and inequality into account  the tth term in (5) is bounded above by:

δt ≤ sup

i

− log(ωt−1 i) − tH

+ (t − 1)H

In other words the per-round regret increase is largest for one of the eigenvectors of the sufﬁcient
statistic St−1  i.e. for classical data. To get an upper bound  maximize over S0  . . .   ST−1 indepen-
dently  each with the constraint that tr(St) = t. A particular maximizer is St = t e1e(cid:62)
1   which is
the sufﬁcient statistic of the sequence of outcomes all equal to Xt = e1e(cid:62)
1 . For this sequence all
bounding steps hold with equality. Hence the matrix KT regret is below the classical KT regret. The
reverse is obvious.

3.3 Last Step Minimax

The bounding technique  developed using Lemma 1 and applied to KT can be used to prove bounds
for a much broader class of prediction strategies. The crucial part of the KT proof was showing
that each term in the telescoped regret (5) can be bounded above by δt as deﬁned in (6)  in which
all matrices share the same eigensystem  and which is hence equivalent to the corresponding clas-
sical expression. The only property of the prediction strategy that we used was that it plays in the
eigensystem of the past sufﬁcient statistic. Therefore  using the same line of argument  we can show
that if for some classical prediction strategy we can obtain a meaningful regret bound by bounding
each term in the regret δt independently  we can obtain the same bound for the corresponding matrix
strategy  i.e. its spectral promotion.
In particular  we can push this argument to its limit by considering the algorithm designed to mini-
mize δt in each iteration. This algorithm is known as Last Step Minimax.
In fact  the Last Step Minimax (LSM) principle is a general recipe for online prediction  which states
that the algorithm should minimize the worst-case regret with respect to the next outcome [16]. In
other words  it should act as the minimax algorithm given that the time horizon is one iteration
ahead. In the classical case for the multinomial distribution  after observing data with sufﬁcient
statistic σt−1  classical LSM predicts with

(cid:40)

− t(cid:88)
(cid:124)

q=1

(cid:96)(ω∗

(cid:123)(cid:122)

tH( σt

t )

(cid:96)(ω  xt)

(cid:124) (cid:123)(cid:122) (cid:125)

− log(ωt−1 xt )

(cid:41)
(cid:125)

t   xq)

=

exp(cid:0)−tH( σt−1+ei
)(cid:1)
j exp(cid:0)−tH( σt−1+ej
(cid:80)

t

t

n(cid:88)

i=1

)(cid:1) ei.

(7)

ωt−1 := argmin

ω

max

xt

Classical LSM is analyzed in [16] for the Bernoulli (n = 2) case. For our straightforward gener-
alization to the classical multinomial case  the regret is bounded by n−1
ln(T + 1) + 1. LSM is
therefore slightly better than KT.
Applying the Last Step Minimax principle to density prediction  we obtain matrix LSM which issues
prediction:

2

(cid:26)

− tr(cid:0)Xt log(W )(cid:1) − tH

(cid:18) St

(cid:19)(cid:27)

Wt−1 := argmin

max
Xt

t
We show that matrix LSM learns the eigenvectors without additional regret.
Theorem 3 (LSM FML). The regrets of classical and matrix LSM are at most n−1

W

.

2

Proof. We determine the form of Wt−1. By Sion’s minimax theorem [15]:

max
Xt

min
W
where P ranges over probability distribution on density matrices Xt. Plugging in the minimizer
W = EP [Xt]  the right hand side becomes:

= max

min
W

t

t

P

 

ln(T + 1) + 1.

(cid:18) St

(cid:19)(cid:21)

(cid:20)

− tr(cid:0)Xt log(W )(cid:1) − tH
(cid:19)(cid:21)(cid:27)

tH

.

(8)

(cid:18) St

(cid:19)(cid:27)

(cid:26)

− tr(cid:0)Xt log(W )(cid:1) − tH
(cid:26)

EP

(cid:18) St

t

n(cid:88)

i=1

(cid:20)

H(cid:0)EP [Xt](cid:1) − EP
(cid:19)(cid:35)
(cid:18) St−1 + sis(cid:62)

i

P

max
i=1 σi sis(cid:62)

(cid:62)
i XtsiH

t

5

Now decompose St−1 as(cid:80)n
(cid:20)
n(cid:88)

inside the maximum:
≥ EP

(cid:18) St

(cid:19)(cid:21)

(cid:34)

EP

tH

s

t

t

i=1

i . Using Lemma 1  we can bound the second expression

= t

(cid:62)
i EP [Xt] siH

s

(cid:18) St−1 + sis(cid:62)

i

(cid:19)

.

t

EP [Xt] by its pinching (a.k.a. projective measurement) (cid:80)n

On the other hand  we know that the entropy does not decrease when we replace the argument
i w.r.t. any

EP [Xt]ui) uiu(cid:62)

eigensystem ui [12]. Therefore  we have:

i

i=1(u(cid:62)
(cid:33)

(s(cid:62)

i

EP [Xt]si) sis(cid:62)

i

= H(p) 

H(cid:0)EP [Xt](cid:1) ≤ H
(cid:18) St
(cid:20)

H(cid:0)EP [Xt](cid:1) − EP

tH

(cid:32) n(cid:88)

i=1

(cid:19)(cid:21)

t

where the last entropy is a classical entropy and p is a vector such that pi = s(cid:62)
bining those two results together  we have:

i

EP [Xt]si. Com-

n(cid:88)

i=1

(cid:18) σt−1 + ei

(cid:19)

.

t

≤ H(p) − t

piH

Note that we have equality only when the distribution P puts nonzero mass only on the eigenvectors
s1  . . .   sn. This means that when p is ﬁxed  we will maximize (8) by using a distribution with such
a property  i.e. P is restricted to the eigensystem of St−1. This  in turn  means that Wt−1 = EP [Xt]
will play in the eigensystem of St−1 as well. It follows that Wt−1 is the classical LSM strategy in

the eigensystem of St−1  i.e. Wt−1 =(cid:80)

i   where ωt−1 are taken as in (7).

i ωt−1 i sis(cid:62)

The proof of the classical LSM guarantee is based on bounding the per-round regret increase:

δt := − log(ωt−1 xt) − tH

+ (t − 1)H

(cid:18) σt−1 + ext

(cid:19)

t

(cid:19)

(cid:18) σt−1

t − 1

 

by choosing the worst case w.r.t. xt and σt−1. Since  for matrices  the worst case for the correspond-
ing matrix version of δt  see (6)  is the diagonal case  the whole analysis immediately goes through
and we get the same bound as for classical LSM.

Note that the bound for LSM is not tight  i.e. there exists no data sequence for which the bound is
achieved. Therefore  the bound for matrix LSM is also not tight. This theorem is a weaker FML
because it only relates worst-case regret bounds. We have veriﬁed experimentally that the actual
regrets coincide in dimension n = 2 for up to T = 5 outcomes  using a grid of 30 dyads per trial 
with uniformly spaced (x(cid:62)e1)2. So we believe that in fact
Conjecture 1 (LSM FML). The worst-case regrets of classical and matrix LSM coincide.

To execute the LSM matrix strategy  we need to have the eigendecomposition of the sufﬁcient statis-
tic. For density matrix data Xt  we may need to recompute it each trial in Ω(n3) time. For dyadic
data xtx(cid:62)
it can be incrementally updated in O(n2) per trial with methods along the lines of [11].

t

3.4 Shtarkov

T(cid:88)

t=1

(cid:16) σT

(cid:17)

.

Fix horizon T . The minimax algorithm for multinomials  due to Shtarkov [14]  minimizes the worst-
case regret

(cid:96)(ωt−1  xt) − T H

T

. . .

ωt :=

inf
ω0

sup
x1

inf
ωT −1

sup
xT

n(cid:88)

φr−1(σt + ei)

(9)
After observing data with sufﬁcient statistic σt and hence with r := T − t rounds remaining 
(cid:17)(cid:17)
classical Shtarkov predicts with
The so-called Shtarkov sum φr can be evaluated in time O(cid:0)n r log(r)(cid:1) using a straightforward exten-
(cid:0)log(T ) − log(n − 2) + 1(cid:1) [6]. This is

sion of the method described in [9] for computing φT (0)  which is based on dynamic programming
and Fast Fourier Transforms.
The regret of classical Shtarkov equals log φT (0) ≈ n−1
again better than Last Step Minimax  which is in turn better than KT which dominates Laplace.

(cid:88)
(cid:80)n

(cid:16)−T H

(cid:16) σ + c

ei where φr(σ) :=

c1 ... cn
i=1 ci=r

c1  . . .   cn

φr(σt)

(cid:33)

(cid:32)

(10)

exp

i=1

T

r

2

.

6

The minimax algorithm for density matrices  called matrix Shtarkov  optimizes the worst-case regret

T(cid:88)

t=1

(cid:18) ST

(cid:19)

T

inf
W0

sup
X1

. . .

inf

WT −1

sup
XT

(cid:96)(Wt−1  Xt) − T H

.

(11)

To this end  after observing data with sufﬁcient statistic St  with r rounds remaining  it predicts with

Wt := argmin

W

sup
X

(cid:96)(W   X) + Rr−1(St + X) 

where Rr is the tail sequence of inf/sups of (11) of length r. We now argue that the FML holds
for matrix Shtarkov. Matrix Shtarkov is surprisingly difﬁcult to analyze. However  we provide a
simplifying conjecture that we veriﬁed experimentally. A rigorous proof remains an open problem.
Our conjecture is that Lemma 1 holds with the entropy H replaced by the minimax regret tail Rr:
Conjecture 2. For each integer r  for each pair of positive matrices A and B

Rr(A + B) ≥ (cid:88)

a(cid:62)
i Bai
tr(B)

Rr

i

(cid:0)A + tr(B) aia(cid:62)

(cid:1).

i

Note that this conjecture generalizes Lemma 1  which is retained as the case r = 0. It follows
from this conjecture  using the same argument as for LSM  that matrix Shtarkov predicts in the

eigensystem of St  i.e. with Wt =(cid:80)

i ωt i sis(cid:62)

i   where ωt as in (10)  and furthermore that

Conjecture 3 (Shtarkov FML). The worst-case regrets of classical and matrix Shtarkov coincide.

We have veriﬁed Conjecture 3 for the matrix Bernoulli case (n = 2) up to T = 5 outcomes  using a
grid of 30 dyads per trial  with uniformly spaced (x(cid:62)e1)2. Then assuming that Rr(S) = log(φ(σ)) 
where σ are the eigenvalues of S  for each n from 2 to 5 we drew 105 trace pairs uniformly from
[0  10]  then drew matrix pairs A and B uniformly at random with those traces. Conjecture 2 always
held.
Obtaining the FML for the minimax algorithm is mathematically challenging and of academic inter-
est but of minor practical relevance. First  the time horizon T must be speciﬁed in advance  so the
minimax algorithm can not be used in a purely online fashion. Secondly  the running time is super-
linear in the number of rounds remaining  while it is constant for the previous three algorithms.

4 Motivation and Discussion of the Loss Function

The matrix entropic loss (1) that we choose as our loss function has a coding interpretation and it is
a proper scoring rule. The latter seems to be a necessary condition for the free matrix lunch.

Quantum coding Classical log-loss forecasting can be motivated from the point of view of data
compression and variable-length coding [7]. In information theory  the Kraft-McMillan inequality
states that  ignoring rounding issues  for every uniquely decodable code with a code length function
λ  there is a probability distribution ω such that λi = − log ωi for all symbols i = 1  . . .   n  and
vice versa. Therefore  the log loss can be interpreted as the code length assigned to the observed out-
come. Quantum information theory[13  5] generalizes variable length coding to the quantum/density
matrix case. Instead of messages composed of bits  the sender and the receiver exchange messages
described by density matrices  and the role analogous to the message length is now played by the
dimension of the density matrix. Variable-length quantum coding requires the deﬁnition of a code
length operator L  which is a positive matrix such that for any density matrix X  tr(XL) gives
the expected dimension (“length”) of the message assigned to X. The quantum version of Kraft’s
inequality states that  ignoring rounding issues  for every variable-length quantum code with code-
length operator L  there exists a density matrix W such that L = − log W . Therefore  the matrix
entropic loss can be interpreted as the (expected) code length of the observed outcome.

Proper score function In decision theory  the loss function (cid:96)(ω  x) assessing the quality of pre-
dictions is also referred to as a score function. A score function is said to be proper  if for
any distribution p on outcomes  the expected loss is minimized by predicting with p itself  i.e.
Ex∼p[(cid:96)(ω  x)] = p. Minimization of a proper score function leads to well-calibrated
argminω
forecasting. The log loss is known to be a proper score function [4].

7

We will say that a matrix loss function (cid:96)(W   X) is proper if for any distribution P on density
matrix outcomes  the expected loss with respect to P is minimized by predicting with the mean out-
EX∼P [(cid:96)(W   X)] = EX∼P [X]. The matrix entropic loss (1) is proper 
come of P   i.e. argminW

for EX∼P [− tr(X log W )] = − tr(cid:0)EX∼P [X] log W(cid:1) is minimized at W = EX∼P [X] [12].
log trace loss (cid:96)(W   X) := − log(cid:0)tr(XW )(cid:1). Note that here the trace and the logarithm are ex-

Therefore  minimization of the matrix entropic loss leads to well-calibrated forecasting  as in the
classical case.
A second generalization of the log loss to the matrix domain used in quantum physics [12] is the

changed compared to (1). The expression tr(XW ) plays an important role in quantum physics
as the expected value of a measurement outcome  and for X = xx(cid:62)  tr(xx(cid:62)W ) is interpreted
as a probability. However  log trace loss is not proper. The counterexample is straightforward:
2 }  then the minimizer of the expected log trace loss is
if we take P uniform on {x1x(cid:62)
W ∝ (x1 + x2)(x1 + x2)(cid:62)  which differs from EX∼P [X] = 1
2 ). Also for
log trace loss we found an example (not presented) against the FML for the minimax algorithm.

A third generalization of the loss is (cid:96)(W   X) := − log(cid:0)tr(X (cid:12) W )(cid:1)  where (cid:12) denotes the commu-

1 + x2x(cid:62)

1   x2x(cid:62)

2 (x1x(cid:62)

tative “product” between matrices that underlies the probability calculus of [20].3 This loss upper
bounds the log trace loss. We don’t know whether it is a proper scoring function. However  it equals
the matrix entropic loss when X is a dyad.
Finally  another loss explored in the on-line learning community is the trace loss (cid:96)(W   X) :=
tr(W X). This loss is not a proper scoring function (it behaves like the absolute loss in the vector
case) and we have an example that shows that there is no FML for the minimax algorithm in this
case (not presented).
In summary  for there to exist a FML  properness of the loss function seems to be required.

5 Conclusion

We showed that the free matrix lunch holds for the matrix version of the KT estimator. Thus the
conjectured free matrix lunch [18] is realized. Our paper raises many open questions. Perhaps the
main one is whether the free matrix lunch holds for the matrix minimax algorithm. Also we would
like to know what properties of the loss function and algorithm cause the free matrix lunch to occur.
From the examples given in this paper it is tempting to believe that you always get a free matrix
lunch when upgrading any classical sufﬁcient-statistics-based predictor to a matrix version by just
playing this predictor in the eigensystem of the current matrix sufﬁcient statistics. However the
following counter example shows that a general reduction must be more subtle: Consider ﬂoored
KT  which predicts with ωt i ∝ (cid:98)σt i(cid:99) + 1/2. For T = 5 trials in dimension n = 2  the worst-case
regret is 1.297 for the classical log loss and 1.992 for matrix entropic loss.

A Proof of Lemma 1
We prove the following slightly stronger inequality for all γ ≥ 0. The lemma is the case γ = 1.

logarithm implies that log(cid:0)A + γ tr(B)I(cid:1) (cid:23) log(A + γB)  so that f(cid:48)(γ) ≥ 0.

Since tr(B)I (cid:23) B  we have A + γ tr(B)I (cid:23) A + γB  and hence the matrix monotonicity of the

B log(A + γB)

.

3We can compute A (cid:12) B as the matrix exponential of the sum of matrix logarithms of A and B.

8

Since f (0) = 0  it sufﬁces to show that f(cid:48)(γ) ≥ 0. Since ∂H(D)

f (γ) := H(A + γB) − n(cid:88)
n(cid:88)
f(cid:48)(γ) = − tr(cid:0)B log(A + γB)(cid:1) +
(cid:16)
B log(cid:0)A + γ tr(B)I(cid:1)(cid:17) − tr

= tr

i=1

i=1

(cid:16)

a(cid:62)
i Bai
tr(B)

H(A + γ tr(B)aia(cid:62)

i ) ≥ 0.

a(cid:62)
i Bai tr

∂D = − log(D) − I 
(cid:16)
aia(cid:62)

i log(cid:0)A + γ tr(B) aia(cid:62)
(cid:17)

i

(cid:1)(cid:17)

References
[1] S. Arora  E. Hazan  and S. Kale. Fast algorithms for approximate semideﬁnite programming

using the multiplicative weights update method. In FOCS  pages 339–348  2005.

[2] S. Arora and S. Kale. A combinatorial  primal-dual approach to semideﬁnite programs. In

STOC  pages 227–236  2007.

[3] K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the

exponential family of distributions. Machine Learning  43(3):211–246  2001.

[4] J. M. Bernardo and A. F. M. Smith. Bayesian Theory. Wiley  1994.
[5] K. Bostroem and T. Felbinger. Lossless quantum data compression and variable-length coding.

Phys. Rev. A  65(3):032313  2002.

[6] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University

Press  New York  NY  USA  2006.

[7] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons  1991.
[8] R. Jain  Z. Ji  S. Upadhyay  and J. Watrous. QIP = PSPACE. In Proceedings of the 42nd ACM

Symposium on Theory of Computing  STOC  pages 573–582  2010.

[9] P. Kontkanen and P. Myllym¨aki. A fast normalized maximum likelihood algorithm for multi-
In Proceedings of the Nineteenth International Joint Conference on Artiﬁcial

nomial data.
Intelligence (IJCAI-05)  pages 1613–1616  2005.

[10] R. E. Krichevsky and V. K. Troﬁmov. The performance of universal encoding. IEEE Transac-

tions on Information Theory  27(2):199–207  Mar. 1981.

[11] J. T. Kwok and H. Zhao.

270–273  2003.

Incremental eigen decomposition.

In IN PROC. ICANN  pages

[12] M. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information. Cambridge

University Press  2000.

[13] B. Schumacher and M. D. Westmoreland. Indeterminate-length quantum coding. Phys. Rev.

A  64(4):042304  2001.

[14] Y. M. Shtarkov. Universal sequential coding of single messages. Problems of Information

Transmission  23(3):3–17  1987.

[15] M. Sion. On general minimax theorems. Paciﬁc Jouronal of Mathematics  8(1):171–176  1958.
[16] E. Takimoto and M. Warmuth. The last-step minimax algorithm. In Proceedings of the 13th

Annual Conference on Computational Learning Theory  pages 100–106  2000.

[17] K. Tsuda  G. R¨atsch  and M. K. Warmuth. Matrix exponentiated gradient updates for on-line
learning and Bregman projections. Journal of Machine Learning Research  6:995–1018  June
2005.

[18] M. K. Warmuth. When is there a free matrix lunch. In Proc. of the 20th Annual Conference on

Learning Theory (COLT ’07). Springer-Verlag  June 2007. Open problem.

[19] M. K. Warmuth and D. Kuzmin. Online variance minimization. In Proceedings of the 19th

Annual Conference on Learning Theory (COLT ’06)  Pittsburg  June 2006. Springer-Verlag.

[20] M. K. Warmuth and D. Kuzmin. Bayesian generalized probability calculus for density matri-

ces. Journal of Machine Learning  78(1-2):63–101  January 2010.

9

,Mehrdad Mahdavi
Tianbao Yang
Rong Jin
Nicki Skafte
Søren Hauberg