2018,Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling,Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-problem in planning  game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds  which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima  we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities  fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice.,Sequential Test for the Lowest Mean:
From Thompson to Murphy Sampling

Emilie Kaufmann1 Wouter M. Koolen2 Aurélien Garivier3

1 CNRS & U. Lille  CRIStAL / SequeL Inria Lille  emilie.kaufmann@univ-lille.fr

2 Centrum Wiskunde & Informatica  Amsterdam  wmkoolen@cwi.nl

3 UMPA  École normale supérieure de Lyon  aurelien.garivier@ens-lyon.fr

Abstract

Learning the minimum/maximum mean among a ﬁnite set of distributions is a
fundamental sub-task in planning  game tree search and reinforcement learning.
We formalize this learning task as the problem of sequentially testing how the
minimum mean among a ﬁnite set of distributions compares to a given threshold.
We develop reﬁned non-asymptotic lower bounds  which show that optimality
mandates very different sampling behavior for a low vs high true minimum. We
show that Thompson Sampling and the intuitive Lower Conﬁdence Bounds policy
each nail only one of these cases. We develop a novel approach that we call Murphy
Sampling. Even though it entertains exclusively low true minima  we prove that
MS is optimal for both possibilities. We then design advanced self-normalized
deviation inequalities  fueling more aggressive stopping rules. We complement our
theoretical guarantees by experiments showing that MS works best in practice.

1

Introduction

∗= mina µa from adaptive samples Xt∼ µAt  where At indicates the distribution sampled
∗ = mina µa was studied in [34] and subsequently [7  31  8]. It is
∗  and that estimators face an intricate bias-variance

We consider a collection of core problems related to minimums of means. For a given ﬁnite collection
of probability distributions parameterized by their means µ1  . . .   µK  we are interested in learning
about µ
at time t. We shall refer to these distributions as arms in reference to a multi-armed bandit model
[28  26]. Knowing about minima/maxima is crucial in reinforcement learning or game-playing  where
the value of a state for an agent is the maximum over actions of the (expected) successor state value
or the minimum over adversary moves of the next state value.
The problem of estimating µ
known that no unbiased estimator exists for µ
trade-off. Beyond estimation  the problem of constructing conﬁdence intervals on minima/maxima
naturally arises in (Monte Carlo) planning in Markov Decision Processes [15] and games [25]. Such
conﬁdence intervals are used hierarchically for Monte Carlo Tree Search (MCTS) in [32  11  17  20].
The open problem of designing asymptotically optimal algorithms for MCTS led us to isolate one
core difﬁculty that we study here  namely the construction of conﬁdence intervals and associated
sampling/stopping rules for learning minima (and  by symmetry  maxima).
Conﬁdence intervals (that are uniform over time) can be naturally obtained from a (sequential)

∗< γ} versus{µ

∗> γ}  given a threshold γ. The main focus of the paper goes even
∗> γ}  that is sequentially sampling the arms in order to decide for one hypothesis

test of{µ
{µ
∗< γ} or{µ

further and investigates the minimum number of samples required for adaptively testing whether

as quickly as possible. Such a problem is interesting in its own right as it naturally arises in several
statistical certiﬁcation applications. As an example we may consider quality control testing in
manufacturing  where we want to certify that in a batch of machines each has a guaranteed probability
of successfully producing a widget. In e-learning  we may want to certify that a given student has

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

max

min

µ1 µ2

. . . µK

γ

“depth 11~2”. We consider the scenario

Figure 1: Game tree search problem of

where it has been established that the
right subtree (grey) of the root has
value γ. Learning the optimal action
at the root (orange) is equivalent to de-
termining how the minimum (blue) of
the leaf means (green) compares to γ.

sufﬁcient understanding of a range of subjects  asking as few questions as possible about the different
subjects. Then in anomaly detection  we may want to ﬂag the presence of an anomaly faster the more
anomalies are present. Finally  in a crowdsourcing system  we may need to establish as quickly as
possible whether a cohort of workers contains at least one unacceptably careless worker. Our own
motivation for studying this problem is that it corresponds to an especially simple instance of the
depth-two game tree search problem  as illustrated in Figure 1.
We thus study a particular example of sequential adaptive
hypothesis testing problem  as introduced by Chernoff [5] 
in which multiple experiments (sampling from one arm)
are available to the experimenter  each of which allows
to gain different information about the hypotheses. The
experimenter sequentially selects which experiment to per-
form  when to stop and then which hypothesis to recommend.
Several recent works from the bandit literature ﬁt into this
framework  with the twist that they consider continuous 
composite hypotheses and aim for δ-correct testing:
the
probability of guessing a wrong hypothesis has to be smaller
than δ  while performing as few experiments as possible.
The ﬁxed-conﬁdence Best Arm Identiﬁcation problem (con-
cerned with ﬁnding the arm with largest mean) is one such
example [9  23]  of which several variants have been studied
[19  17  12]. For example the Thresholding Bandit Problem
[27] aims at ﬁnding the set of arms above a threshold  which
is strictly harder than our testing problem. In the Ranking
and Selection literature (see e.g.  [14] for a survey) the related problem of ﬁnding systems whose
expected performance is smaller than a known standard has been studied by [24]  but if such system
exist  the goal was to additionnaly identify the one with smallest expectation  which is strictly harder
than our problem.
A full characterization of the asymptotic complexity of the BAI problem was recently given in
[11]  highlighting the existence of an optimal allocation of samples across arms. The lower bound
technique introduced therein can be generalized to virtually any testing problem in a bandit model
(see  e.g. [20  12]). Such an optimal allocation is also presented by [4] in the GENERAL-SAMP
framework  which is quite generic and in particular encompasses testing on which side of γ the
minimum falls. The proposed LPSample algorithm is thus a candidate to be applied to our testing
problem. However  this algorithm is only proved to be order-optimal  that is to attain the minimal
sample complexity up to a (large) multiplicative constant. Moreover  like other algorithms for special
cases (e.g. Track-and-Stop for BAI [11])  it relies on forced exploration  which may be harmful
in practice and leads to unavoidably asymptotic analysis.
Our ﬁrst contribution is a tight lower bound on the sample complexity that provides an oracle sample
allocation  but also aims at reﬂecting the moderate-risk behavior of a δ-correct algorithm. Our second
contribution is a new sampling rule for the minimum testing problem  under which the empirical
fraction of selections converges to the optimal allocation without forced exploration. The algorithm
is a variant of Thompson Sampling [33  1] that is conditioning on the “worst” outcome µ
the name Murphy Sampling. This conditioning is inspired by the Top Two Thompson Sampling
recently proposed by [29] for Best Arm Identiﬁcation. As we shall see  the optimal allocation is
very different whether µ
behavior in each case. Our third contribution is a new stopping rule  that by aggregating samples from
several arms that look small may lead to early stopping whenever µ
on a new self-normalized deviation inequality for exponential families (Theorem 7) of independent
interest. It generalizes results obtained by [18  23] in the Gaussian case and by [3] without the
uniformity in time  and also handles subsets of arms.
The rest of the paper is structured as follows. In Section 2 we introduce our notation and formally
deﬁne our objective. In Section 3  we present lower bounds on the sample complexity of sequential
tests for minima. In particular  we compute the optimal allocations for this problem and discuss
the limitation of naive benchmarks to attain them. In Section 4 we introduce Murphy sampling 
and prove its optimality in conjunction with a simple stopping rule. Improved stopping rules (and

∗< γ  hence
∗> γ and yet Murphy Sampling automatically adopts the right
∗< γ. This stopping rule is based

∗< γ or µ

2

conﬁdence intervals) are presented in Section 5. Finally  numerical experiments reported in Section 6
demonstrate the efﬁciency of Murphy Sampling paired with our new stopping rule.

2 Setup

µ

∗ = min
∗< γ}

In this paper  we are interested in the smallest mean (and the arm where it is attained)

We consider a family of K probability distributions that belong to a one-parameter canonical expo-
nential family  that we shall call arms in reference to a multi-armed bandit model. Such exponential
families include Gaussian with known variance  Bernoulli  Poisson  see [3] for details. For natural

ρ(dx) 
parameter ν  the density of the distribution w.r.t. carrier measure ρ on R is given by exν−b(ν)
where the cumulant generating function b(ν)= ln Eρ[eXν] induces a bijection ν ˙b(ν) to the mean
parameterization. We write KL(ν  λ) and d(µ  θ) for the Kullback-Leibler divergence from natural
parameters ν to λ and from mean parameters µ to θ. Speciﬁcally  with convex conjugate b∗ 
KL(ν  λ) = b(λ)− b(ν)+(ν− λ) ˙b(ν)
and d(µ  θ) = b∗(µ)− b∗(θ)−(µ− θ)˙b∗(θ).
We denote by µ=(µ1  . . .   µK)∈I K the vector of arm means  which fully characterizes the model.
∗(µ) = arg min
∗ = a
∗< γ or µ
Given a threshold γ∈I  our goal is to decide whether µ
∗> γ. We introduce the hypotheses
and their union H = H<∪H>.
∗> γ} 
and H> = {µ∈I K µ
H< = {µ∈I K µ
At  a stopping rule τ and a decision rule ˆm ∈ {< >}. The algorithm samples Xt ∼ µAt while
t ≤ τ  and then outputs a decision ˆm. We denote the information available after t rounds by
Ft= σ(A1  X1  . . .   At  Xt). At is measurable with respect toFt−1 an possibly some exogenous
random variable  τ is a stopping time with respect to this ﬁltration and ˆm isFτ -measurable.
Given a risk parameter δ∈(0  1]  we aim for a δ-correct algorithm  that satisﬁes Pµ(µ∈H ˆm)≥ 1− δ
for all µ∈H. Our goal is to build δ-correct algorithms that use a small number of samples τδ in order
to reach a decision. In particular  we want the sample complexity Eµ[τ] to be small.
1(As=a) be the number of selections of arm a up to round t  Sa(t)=
Notation We let Na(t)=∑t
s=1
s=1 Xs1(As=a) be the sum of the gathered observations from that arm and ˆµa(t)= Sa(t)~Na(t)
∑t

We want to propose a sequential and adaptive testing procedure  that consists in a sampling rule

and

µa.

µa

a

a

a

their empirical mean.

3 Lower Bounds

where

In this section we study information-theoretic sample complexity lower bounds  in particular to ﬁnd
out what the problem tells us about the behavior of oracle algorithms. [10] prove that for any δ-correct
algorithm

T∗(µ) = max
Eµ[τ] ≥ T
∗(µ)kl(δ  1− δ)
wad(µa  λa)
λ∈Alt(µ)Q
w∈△ min
kl(x  y)= x ln x
1−y and Alt(µ) is the set of bandit models where the correct recom-
y+(1− x) ln 1−x
mendation differs from that on µ. The following result specialises the above to the case of testingH<
vsH>  and gives explicit expressions for the characteristic time T
∗(µ).
∗(µ) and oracle weights w
a(µ) = 1a=a∗
∗(µ) = 
∗< γ 
d(µ∗ γ)
∗> γ.
∑a
d(µa  γ)
d(µa γ) µ
∑j
d(µj  γ)

Lemma 1. Any δ-correct strategy satisﬁes (1) with

∗< γ 
∗> γ 

and

(1)

∗

w

µ

µ

µ

T

1

1

1

1

1

a

Lemma 1 is proved in Appendix B. As explained by [10] the oracle weights correspond to the fraction
of samples that should be allocated to each arm under a strategy matching the lower bound. The
interesting feature here is that the lower bound indicates that an oracle algorithm should have very

different behavior onH< andH>. OnH< it should sample a
several) exclusively  while onH> it should sample all arms with certain speciﬁc proportions.

∗ (or all lowest means  if there are

3

3.1 Boosting the Lower Bounds

Following [13] (see also [30] and references therein)  Lemma 1 can be improved under very mild
assumptions on the strategies. We call a test symmetric if its sampling and stopping rules are invariant
by conjugation under the action of the group of permutations on the arms. In that case  if all the arms

are equal  then their expected numbers of draws are equal. For simplicity we assume µ1≤ . . .≤ µK.
Proposition 2. Let k= maxa d(µa  γ)= maxd(µ1  γ)  d(µK  γ). For any symmetric  δ-correct
test  for all arms a∈{1  . . .   K}  the expected number of selections of arm a satisﬁes

Eµ[Na(τ)]≥ 21− 2δK 3

27K 2k

.

may suggest otherwise. Second  this lower bound on the number of draws of each arm can be used to

Proposition 2 is proved in Appendix B. It is an open question to improve the dependency in K in this

bound; moreover  one may expect a bound decreasing with δ  maybe in ln(ln(1~δ)) (but certainly
not in ln(1~δ)). This result already has two important consequences: ﬁrst  it shows that even an
optimal algorithm needs to draw all the arms a certain number of times  even onH< where Lemma 1
“boost” the lower bound on Eµ[τ]: the following result is also proved in Appendix B.
 .

∗< γ  for any symmetric  δ-correct strategy 
d(µ1  γ) + 21− 2δK 3
Eµ[τ]≥ kl(δ  1− δ)
27K 2k Q

1− d(µa  γ)1(µa≤γ)
d(µ1  γ)

Theorem 3. When µ

a

3.2 Lower Bound Inspired Matching Algorithms

where

decision to stop relies on individual “box” conﬁdence intervals for each arm  whose endpoints are

Ua(t) = max{q∶ Na(t)d
La(t) = min{q∶ Na(t)d

+(ˆµa(t)  q)≥ C<(δ  Na(t))} 
−(ˆµa(t)  q)≥ C>(δ  Na(t))}.

−(u  v)= d(u  v)1(u≥v)  we let
τ< = inf{t∈ N∗∶∃a Na(t)d
τ> = inf{t∈ N∗∶∀a Na(t)d

In light of the lower bound in Lemma 1  we now investigate the design of optimal learning algorithms
(sampling rule At and stopping rule τ). We start with the stopping rule. The ﬁrst stopping rule that
comes to mind consists in comparing separately each arm to the threshold and stopping when either
one arm looks signiﬁcantly below the threshold or all arms look signiﬁcantly above. Introducing
d

+(u  v)= d(u  v)1(u≤v) and d
+(ˆµa(t)  γ)≥ C<(δ  Na(t))}  
τBox = τ<∧ τ>
−(ˆµa(t)  γ)≥ C>(δ  Na(t))}  
and C<(δ  r) and C>(δ  r) are two threshold functions to be speciﬁed. Box refers to the fact that the
Indeed  τBox= inf{t∈ N∗∶ mina Ua(t)≤ γ or mina La(t)≥ γ}. In particular  if∀a ∀t∈ N∗
  µa∈
[La(t)  Ua(t)]  any algorithm that stops using τBox is guaranteed to output a correct decision. In
the Gaussian case  existing work [18  23] permits to exhibit thresholds of the form Cࣤ(δ  r) =
ln(1~δ)+ a ln ln(1~δ)+ b ln(1+ ln(r)) for which this sufﬁcient correctness condition is satisﬁed
with probability larger than 1− δ. Theorem 7 below generalizes this to exponential families.
show that a simple algorithm  called LCB  can do that for all µ∈H>. LCB selects at each round the
which is intuitively designed to attain the stopping condition mina La(t)≥ γ faster. In Appendix E
we prove (Proposition 15) that LCB is optimal for µ∈H> however we show (Proposition 16) that on
instances ofH< it draws all arms a≠ a
For µ∈H<  the lower bound Lemma 1 can actually be a good guideline to design a matching
∗ with smallest

Given that τBox can be proved to be δ-correct whatever the sampling rule  the next step is to propose
sampling rules that  coupled with τBox  would attain the lower bound presented in Section 3. We now

∗ too much and cannot match our lower bound.

LCB: Play At= argmina La(t)  

algorithm: under such an algorithm  the empirical proportion of draws of the arm a
mean should converge to 1. The literature on regret minimization in bandit models (see [2] for a
survey) provides candidate algorithms that have this type of behavior  and we propose to use the

arm with smallest Lower Conﬁdence Bound:

(2)

(3)

4

Thompson Sampling (TS) algorithm [1  22]. Given independent prior distribution on the mean of
each arm  this Bayesian algorithm selects an arm at random according to its posterior probability of
being optimal (in our case  the arm with smallest mean). Letting πt
a refer to the posterior distribution
of µa after t samples  this can be implemented as

TS: Sample∀a∈{1  . . .   K}  θa(t)∼ πt−1

a   then play At= arg mina∈{1 ... K} θa(t).

the following: they are non-decreasing in r and there exists a function f such that 

δ

It follows from Theorem 12 in Appendix 5 that if Thompson Sampling is run without stopping 

To summarize  we presented a simple stopping rule  τBox  that can be asymptotically optimal for

in combination with LCB. But neither of these two sampling rules are good for the other type of
instances  which is a big limitation for a practical use of either of these. In the next section  we

We now argue that ensuring the sampling proportions converge to w
optimal sample complexity  at least in an asymptotic sense. The proof can be found in Appendix C.

Na∗(t)~t converges almost surely to 1  for every µ. As TS is an anytime sampling strategy (i.e. that
does not depend on δ)  Lemma 4 below permits to justify that on every instance ofH< with a unique
optimal arm  under this algorithm τBoxࣃ(1~d(µ1  θ)) ln(1~δ). However  TS cannot be optimal for
µ∈H>  as the empirical proportions of draws cannot converge to w
every µ∈H< if it is used in combination with Thompson Sampling and for µ∈H> if it is used
propose a new Thompson Sampling like algorithm that ensures the right exploration under bothH<
andH>. In Section 5  we further present an improved stopping rule that may stop signiﬁcantly earlier
than τBox on instances ofH<  by aggregating samples from multiple arms that look small.
∗ is sufﬁcient for reaching the
Lemma 4. Fix µ∈H. Fix an anytime sampling strategy (At) ensuring Nt
∗(µ). Let τδ be a
t → w
stopping rule such that τδ≤ τ Box
  for a Box stopping rule (2) whose threshold functions Cࣤ satisfy
∀r≥ r0  Cࣤ(δ  r)≤ f(δ)+ ln r  where f(δ)= ln(1~δ)+ o(ln(1~δ)).
Then lim supδ→0
4 Murphy Sampling
In this section we denote by Πn= P(⋅Fn) the posterior distribution of the mean parameters after n
Law  as it performs some conditionning to the “worst event”(µ∈H<):
MS: Sample θt∼ Πt−1(⋅H<)  then play At = a
As we will argue below  the subtle difference of sampling from Πn−1(⋅H<) instead of Πn−1 (regular
MS always conditions onH< (and never onH>) regardless of the position of µ w.r.t. γ. This is
∗(θ)≠ a
∗(µ) a ﬁxed fraction 1− β of the time  where β is a parameter that needs to be tuned with
Also note that MS is an anytime sampling algorithm  being independent of the conﬁdence level 1− δ.
MS is technically an instance of Thompson Sampling with a joint prior Π supported only onH<.
This viewpoint is conceptually funky  as we will apply MS identically toH< andH>. To implement

different from the symmetric Top Two Thompson Sampling [29]  which essentially conditions on
a
knowledge of µ. MS on the other hand needs no parameters.

rounds. We introduce a new (randomised) sampling rule called Murphy Sampling after Murphy’s

Thompson Sampling) ensures the required split personality behavior (see Lemma 1). Note that

≤ T

∗(µ) almost surely.

τδ
ln 1
δ

∗(θt) .

(4)

∗(µ)≠ 1a∗.

The conﬁdence will manifest only in the stopping rule.

MS  we use that independent conjugate per-arm priors induce likewise posteriors  admitting efﬁcient
(unconditioned) posterior sampling. Rejection sampling then achieves the required conditioning. Its
computational cost is limited: the acceptance probability cannot be much smaller than the risk δ
provided to the algorithm. Indeed  the fact that the stopping rule (see Section 5) has not yet ﬁred 
combined with the posterior concentration (Proposition 6) and the convergence of the sampling efforts
to track the sampling proportions (Theorem 5) reveals that the MS rejection sampling step accepts

with probability at least of order δ~(ln t)3. So for reasonable values of δ  this can be small and require

a few thousands of draws (not a big deal for today’s computers)  but it cannot be prohibitively small.
The rest of this section is dedicated to the analysis of MS. First  we argue that the MS sampling
proportions converge to the oracle weights of Lemma 1.

5

Assumption For purpose of analysis  we need to assume that the parameter space Θ∋ µ (or the
support of the prior) is the interior of a bounded subset of RK. This ensures that supµ θ∈Θ d(µ  θ)<∞
and supµ θ∈Θµ− θ<∞. This assumption is common [16  Section 7.1]  [29  Assumption 1]. We
also assume that the prior Π has a density π with bounded ratio supµ θ∈Θ

π(µ)<∞.
π(θ)
∗(µ) a.s. for any µ∈H.

t → w

Theorem 5. Under the above assumption  MS ensures Nt

The main intuition is provided by

In this case the conditioning in MS is asymptotically

proof in the appendix is to show the convergence occurs almost surely.

We give a sketch of the proof below  the detailed argument can be found in Appendix D  Theorems 12
and 13. Given the convergence of the weights  the asymptotic optimality in terms of sample
complexity follows by Lemma 4  if MS is used with an appropriate stopping rule (Box (2) or the
improved Aggregate stopping rule discussed in Section 5).

Proof Sketch First  consider µ ∈ H<.
immaterial as Πn(H<) → 1  and the algorithm behaves like regular Thompson Sampling. As
Thompson sampling has sublinear pseudo-regret [1]  we must have E[N1(t)]~t→ 1. The crux of the
Next  consider µ ∈ H>. Following [29]  we denote the sampling probabilities in round n by
ψa(n)= Πn−1(a= arg minj θjH<)  and abbreviate Ψa(n)=∑n
t=1 ψa(t) and ¯ψa(n)= Ψa(n)~n.
Proposition 6 ([29  Proposition 4]). For any open subset ˜Θ⊆ Θ  the posterior concentrates at rate
Πn( ˜Θ)࣊ exp−n minλ∈ ˜Θ∑a
Let us use this to analyze ψa(n). As we are onH>  the posterior Πn(H<)→ 0 vanishes. More-
over  Πn(a= arg minj θj H<)∼ Πn(θa< γ) as the probability that multiple arms fall below γ is

¯ψa(n)d(µa  λa) a.s. where an࣊ bn means 1

exp−n ¯ψa(n)d(µa  γ)
∑j exp−n ¯ψj(n)d(µj  γ) .

To get a good sense for what this means  let’s analyse the version with equality. Using that w
is constant (Lemma 1)  we see

ψa(n+ 1) ∼ Πn(µa< γ)
∑j Πn(µj< γ) ࣊
ad(µa  γ)
∗
ψa(n+ 1) ≤ e
Now this means that whenever ¯ψa(n)≥ w
−nda≈ 0 is exponentially
a+   we ﬁnd that ψa(n+ 1)≤ e
∗
¯ψa(n) decays hyperbolically (i.e. without lower bound). Hence
small  and hence ¯ψa(n+ 1)≈ n
n+1
a+ . As this holds for all arms a and > 0  we must have limn ψa(n)= w
lim supn→∞ ¯ψa(n)≤ w
∗
∗

−n ¯ψa(n)−w

bn → 0.

d(µa γ)

negligible. Hence

n ln an

a.

∗

a

.

5

Improved Stopping Rule and Conﬁdence Intervals

and

NS(t) = Q

and recall d
one-parameter exponential families.

ˆµS(t) = ∑a∈S Na(t)ˆµa(t)
NS(t)

Theorem 7 below provides a new self-normalized deviation inequality that given a subset of arms
controls uniformly over time how the aggregated mean of the samples obtained from those arms can

deviate from the smallest (resp. largest) mean in the subset. More formally forS⊆[K]  we introduce
−(u  v) = d(u  v)1(u≥v). We prove the following for
Theorem 7. Let T∶ R+→ R+ be the function deﬁned by
−1(1+ x)+ ln ζ(2)
−11+ h
−s. For every subsetS of arms and x≥ 0.04 
where h(u)= u− ln(u) for u≥ 1 and ζ(s)=∑∞
n=1 n
a∈S µa≥ 3 ln(1+ ln(NS(t)))+ T(x) ≤ e
−x 
a∈S µa≥ 3 ln(1+ ln(NS(t)))+ T(x) ≤ e
−x.

a∈S Na(t)
+(u  v) = d(u  v)1(u≤v) and d
T(x) = 2h
+ˆµS(t)  min
−ˆµS(t)  max

P∃t∈ N∶ NS(t)d
P∃t∈ N∶ NS(t)d



(5)

(6)

(7)

2

6

5.1 An Improved Stopping Rule

increasing empirical mean and smaller than γ.

The proof of this theorem can be found in Section F and is sketched below. It generalizes in several

beyond subsets of size 1 will be crucial here to obtain better conﬁdence intervals on minimums  or
stop earlier in tests. Note that the threshold function T introduced in (5) does not depend on the

directions the type of results obtained by [18  23] for Gaussian distributions andS= 1. Going
cardinality of the subsetS to which the deviation inequality is applied. Tight upper bounds on T can
be given using Lemma 21 in Appendix F.3  which support the approximation T(x)ࣃ x+ 3 ln(x).
Fix a subset prior π∶·({1  . . .   K})→ R+ such that∑S⊆{1 ... K} π(S)= 1 and let T be the threshold
function deﬁned in Theorem 7. We deﬁne the stopping rule τ π∶= τ>∧ τ π<   where
τ> = inf{t∈ N∗∶∀a∈{1  . . .   K}Na(t)d
−(ˆµa(t)  γ)≥ 3 ln(1+ ln(Na(t)))+ T(ln(1~δ))}  
+(ˆµS(t)  γ)≥ 3 ln(1+ ln(NS(t)))+ T(ln(1~(δπ(S)))} .
= inf{t∈ N∗∶∃S∶ NS(t)d
τ π<
The associated recommendation rule selectsH> if τ π = τ> andH< if τ π = τ π< . For the practical
computation of τ π<   the search over subsets can be reduced to nested subsets including arms sorted by
Lemma 8. Any algorithm using the stopping rule τ π and selecting ˆm=> iff τ π= τ>  is δ-correct.
is uniform over subset of size 1  i.e. π(S)= K
−11(S= 1)  one obtain a δ-correct τBox stopping rule
−1 KS−1  that puts the
which NS(t)d
Links with Generalized Likelihood Ratio Tests (GLRT). Assume we want to testH0 againstH1
parameter x rejectsH0 if the test statistic maxx∈H1 (cid:96)(X1  . . .   Xt; x)~ maxx∈H0∪H1 (cid:96)(X1  . . .   Xt; x)
has large values (where (cid:96)(⋅; x) denotes the likelihood of the observations under the model parameter-
ized by x). In our testing problem  the GLRT statistic for rejectingH< is mina Na(t)d
−(ˆµa(t)  γ)
hence τ> is very close to a sequential GLRT test. However  the GLRT statistic for rejectingH> is
+(ˆµa(t)  γ)  which is quite different from the stopping statistic used by τ π< . Rather than
a=1 Na(t)d
∑K
Using similar martingale techniques as for proving Theorem 7  one can show that replacing τ π< by
K 
t∈ N∗∶ Q
+(ˆµa(t)  γ)− 3 ln(1+ ln(Na(t)))]+≥ KT ln(1~δ)
= inf
<
also yields a δ-correct algorithm (see [21])1. At ﬁrst sight  τ π< and τ GLRT

+(ˆµS(t)  γ) may be larger. We advocate the use of π(S)= K

with thresholds functions satisfying the assumptions of Lemma 4. However  in practice (especially
more moderate δ)  it may be more interesting to include in the support of π subsets of larger sizes  for

From Lemma 8  proved in Appendix G  the prior π doesn’t impact the correctness of the algorithm.
However it may impact its sample complexity signiﬁcantly. First it can be observed that picking π that

are hard to compare: the
stopping statistic used by the latter can be larger than that used by the former  but it is compared to a
smaller threshold. In Section 6 we will provide empirical evidence in favor of aggregating samples.

aggregating samples from arms  the GLRT statistic is summing evidence for exceeding the threshold.

for composite hypotheses. A GLRT test based on t observations whose distribution depends on some

same weight on the set of subsets of each possible size.

τ GLRT

[Na(t)d

a∶ˆµa(t)≤γ

<

5.2 A Conﬁdence Intervals Interpretation

Uπ

Inequality (6) (and a union bound over subsets) also permits building a tight upper conﬁdence bound
on the minimum µ

∗. Indeed  deﬁning
S⊆{1 ... K}[NS(t)d
min(t)∶= maxq∶ max
+(ˆµS(t)  q)− 3 ln(1+ ln(1+ NS(t)))]≤ Tln
δπ(S)  
it is easy to show that P(∀t∈ N  µ
min(t)) ≥ 1− δ. For general choices of π  this upper
∗≤ Uπ
conﬁdence bound may be much smaller than the naive bound mina Ua(t) which corresponds to
composite arm  allowing us to replace (5) by T(x)= 2h−11+ x+ln ζ(2)

choosing π uniform over subset of size 1. We provide an illustration supporting this claim in Figure 2

1In fact  we can slightly sharpen the bound by observing that we are controlling the deviation of a single

  see [21  Appendix A.1]

1

2

7

−1 KS−1
below. The two type of upper conﬁdence bounds (Aggregate corresponding to π(S)= K
and Box corresponding to π(S)= K
−11(S=1)) are compared under uniform sampling in a Bernoulli
bandit model that has k arms with mean 0.1 plus 4 arms with means[0.2 0.3 0.4 0.5]. The larger the

number of arms close to minimum (here equal to it) is  the more UCB Aggregate beats UCB Box.
Observe that using inequality (7) in Theorem 7 similarly allows to derive tighter lower conﬁdence
bounds on the maximum of several means.

5.3 Sketch of the Proof of Theorem 7

Figure 2: Illustration of the Box versus Aggregate Upper Conﬁdence Bounds as a function of time on

Bernoulli instance for k= 1 (left)  k= 3 (middle) and k= 10 (right) minimal arms.
+(ˆµS(t)  mina∈S µa)− 2(1+ η) ln(1+ ln NS(t))] 
Fix η∈[0  1+ e[. Introducing Xη(t)=[NS(t)d
the cornerstone of the proof (Lemma 17) consists in proving that for all λ∈[0 (1+ η)−1[  there
t that “almost” upper bounds eλXη(t): there exists a function gη such that
t ≥ eλXη(t)−gη(λ)
0]= 1 and ∀t∈ N∗
E[M λ
t > eλu−gη(λ)≤ exp(−[λu− gη(λ)]) .
P(∃t∈ N∗∶ Xη(t)> u) ≤ P∃t∈ N∗∶ M λ

From there  the proof easily follows from a combination of Chernoff method and Doob inequality:

exists a martingale M λ

  M λ

(8)

.

Inequality (6) is then obtained by optimizing over λ  carefully picking η and inverting the bound.
The interesting part of the proof is to actually build a martingale satisfying (8). First  using the so-
called method of mixtures [6] and some speciﬁc fact about exponential families already exploited by
[3]  we can prove that there exists a martingale ˜W x
t such that for some function f (see Equation (14))

From there it follows that  for every λ and z> 1 eλ(Xη(t)−f(η))≥ z⊆{e

x

− ln(z)
λ(1+η) ˜W

λ ln(z)

1

t

≥ 1} and

the trick is to introduce another mixture martingale 

1+η .
{Xη(t)− f(η)≥ x}⊆ ˜W x
t ≥ e
t = 1+S ∞
λ ln(z)
− ln(z)
λ(1+η) ˜W
t~E[M
t = M
t ≥ eλ[Xη(t)−f(η)]. We let M λ

dz 

M

e

λ

λ

λ

1

t

1

t].

λ

that is proved to satisfy M

6 Experiments

We discuss the results of numerical experiments performed on Gaussian bandits with variance 1 
R  which leads to a conjugate Gaussian posterior. The experiments demonstrate the ﬂexibility of our

using the threshold γ= 0. Thompson and Murphy sampling are run using a ﬂat (improper) prior on
MS sampling rule  which attains optimal performance on instances from bothH< andH>. Moreover 
µ∈H<. This aggregating stopping rule  that we refer to as τ Agg is an instance of the τ π stopping rule
−1 KS−1. We investigate the combined use of three sampling
presented in Section 5 for π(S)= K

they show the advantage of using a stopping rule aggregating samples from subsets of arms when

rules  MS  LCB and Thompson Sampling with three stopping rules  τ Agg  τ Box and τ GLRT.

8

1002003004005000.20.40.60.81.0UCB BoxUCB AggregateMinimum value1002003004005000.20.40.60.81.0UCB BoxUCB AggregateMinimum value1002003004005000.20.40.60.81.0UCB BoxUCB AggregateMinimum valuerun the different algorithms (excluding the TS sampling rule  that essentially coincides with MS

MS is outperforming LCB  with a sample complexity of order T

We ﬁrst study an instance µ∈H< with K= 10 arms that are linearly spaced between−1 and 1. We
onH<) for different values of δ and report the estimated sample complexity in Figure 3 (left). For
each sampling rule  it appears that E[τ Agg]≤ E[τ Box]≤ E[τ GLRT]. Moreover  for each stopping rule
∗(µ) ln(1~δ)+ C. Then we study
an instance µ∈H> with K= 5 arms that are linearly spaced between 0.5 and 1  with τ Agg as the
sampling rule (which matters little as the algorithm mostly stops because of τ> onH>). Results are
also proved optimal onH>)  while vanilla TS fails dramatically. On those experiments  the empirical
MS sampling rule as well as a larger-scale comparison of stopping rules underH<.

error was always zero  which shows that our theoretical thresholds are still quite conservative. More
experimental results can be found in Appendix A: an illustration of the convergence properties of the

reported in Figure 3 (right)  in which we see that MS is performing very similarly to LCB (that is

Figure 3: E[τδ] as a function of ln(1~δ) for several algorithms on an instance µ∈H< (left) and
µ∈H> (right)  estimated using N= 5000 (resp. 500) repetitions.

7 Discussion

∗  and develop the Murphy Sampling strategy to match it asymptotically.

We propose new sampling and stopping rules for sequentially testing the minimum of means. As
our guiding principle  we ﬁrst prove sample complexity lower bounds  characterized the emerging
oracle sample allocation w
We observe in the experiments that the asymptotic regime does not necessarily kick in at moderate
conﬁdence δ (Figure 4  left) and that there is an important lower-order term to the practical sample
complexity (Figure 3). It is an intriguing open problem of theoretical and practical importance
to characterize and match optimal behavior at moderate conﬁdence. We make ﬁrst contributions
in both directions: we prove tighter sample complexity lower bounds for symmetric algorithms
(Proposition 2  Theorem 3) and we design aggregating conﬁdence intervals which are tighter in
practice (Figure 2).
The importance of this perspective arises  as highlighted in the introduction  from the hierarchical
application of maxima/minima in learning applications. A better understanding of the moderate
conﬁdence regime for learning minima will very likely translate into new insights and methods for
learning about hierarchical structures  where the beneﬁts accumulate with depth.

References
[1] S. Agrawal and N. Goyal. Analysis of Thompson Sampling for the multi-armed bandit problem.

In Proceedings of the 25th Conference On Learning Theory  2012.

[2] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Fondations and Trends in Machine Learning  5(1):1–122  2012.

[3] O. Cappé  A. Garivier  O.-A. Maillard  R. Munos  and G. Stoltz. Kullback-Leibler upper
conﬁdence bounds for optimal sequential allocation. Annals of Statistics  41(3):1516–1541 
2013.

9

5101520-log(delta)0100200300400500600700mean sample complexitySample Complexity as a function of -log(delta) (N=5000 repetitions)LCB + AggregateMS + AggregateLCB + BoxMS + BoxLCB + GLRTMS + GLRTLower Bound34567-log(delta)0100200300400500600700mean sample complexitySample Complexity as a function of -log(delta) (N=500 repetitions)LCB + AGGTS + AGGMS + AGGLower Bound[4] L. Chen  A. Gupta  J. Li  M. Qiao  and R. Wang. Nearly optimal sampling algorithms for
combinatorial pure exploration. In Proceedings of the 30th Conference on Learning Theory
(COLT)  2017.

[5] H. Chernoff. Sequential design of Experiments. The Annals of Mathematical Statistics  30(3):

755–770  1959.

[6] V. H. de la Peña  T. L. Lai  and S. Q. Self-normalized processes. Limit Theory and Statistical

applications. Springer  2009.

[7] C. D’Eramo  M. Restelli  and A. Nuara. Estimating maximum expected value through Gaussian

approximation. In International Conference on Machine Learning (ICML)  2016.

[8] C. D’Eramo  A. Nuara  M. Pirotta  and M. Restelli. Estimating the maximum expected value in

continuous reinforcement learning problems. In AAAI  2017.

[9] E. Even-Dar  S. Mannor  and Y. Mansour. Action Elimination and Stopping Conditions for
the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning
Research  7:1079–1105  2006.

[10] A. Garivier and E. Kaufmann. Optimal best arm identiﬁcation with ﬁxed conﬁdence.

Proceedings of the 29th Conference On Learning Theory (COLT)  2016.

In

[11] A. Garivier  E. Kaufmann  and W. M. Koolen. Maximin action identiﬁcation: A new bandit
framework for games. In Proceedings of the 29th Conference On Learning Theory (COLT) 
2016.

[12] A. Garivier  P. Ménard  and L. Rossi. Thresholding bandit for dose-ranging: The impact of

monotonicity. arXiv:1711.04454  2017.

[13] A. Garivier  P. Ménard  and G. Stoltz. Explore ﬁrst  exploit next: The true shape of regret in

bandit problems. Mathematics of Operations Research  Jun. 2018.

[14] D. Goldsman and B. L. Nelson. Comparing Systems via Simulation  Chapter 9 in the Handbook

of Simulation. Wiley  1998.

[15] J.-B. Grill  M. Valko  and R. Munos. Blazing the trails before beating the path: Sample-efﬁcient
Monte-Carlo planning. In Advances in Neural Information Processing Systems (NIPS)  pages
4680–4688  2016.

[16] P. D. Grünwald. The minimum description length principle. MIT press  2007.

[17] R. Huang  M. M. Ajallooeian  C. Szepesvári  and M. Müller. Structured best arm identiﬁcation
with ﬁxed conﬁdence. In International Conference on Algorithmic Learning Theory (ALT) 
2017.

[18] K. Jamieson  M. Malloy  R. Nowak  and S. Bubeck. lil’UCB: an Optimal Exploration Algorithm
for Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory  2014.

[19] S. Kalyanakrishnan  A. Tewari  P. Auer  and P. Stone. PAC subset selection in stochastic

multi-armed bandits. In International Conference on Machine Learning (ICML)  2012.

[20] E. Kaufmann and W. M. Koolen. Monte-Carlo tree search by best arm identiﬁcation.

Advances in Neural Information Processing Systems (NIPS)  2017.

In

[21] E. Kaufmann and W. M. Koolen. Mixture Martingales Revisited with Applications
Preprint  2018. URL https://hal.

to Sequential Tests and Conﬁdence Intervals.
archives-ouvertes.fr/hal-01886612v1/.

[22] E. Kaufmann  N. Korda  and R. Munos. Thompson Sampling : an Asymptotically Optimal
Finite-Time Analysis. In Proceedings of the 23rd conference on Algorithmic Learning Theory 
2012.

[23] E. Kaufmann  O. Cappé  and A. Garivier. On the Complexity of Best Arm Identiﬁcation in

Multi-Armed Bandit Models. Journal of Machine Learning Research  17(1):1–42  2016.

10

[24] S.-H. Kim. Comparison with a standard via fully sequential procedures. ACM Transactions on

Modeling and Computer Simulation (TOMACS)  15(2):155–174  2005.

[25] L. Kocsis and C. Szepesvári. Bandit based Monte-Carlo planning. In Proceedings of the 17th
European Conference on Machine Learning  ECML’06  pages 282–293  Berlin  Heidelberg 
2006. Springer-Verlag. ISBN 3-540-45375-X  978-3-540-45375-8.

[26] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics  6(1):4–22  1985.

[27] A. Locatelli  M. Gutzeit  and A. Carpentier. An optimal algorithm for the thresholding bandit

problem. In International Conference on Machine Learning (ICML)  2016.

[28] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American

Mathematical Society  58(5):527–535  1952.

[29] D. Russo. Simple Bayesian algorithms for best arm identiﬁcation. CoRR  abs/1602.08448 

2016. URL http://arxiv.org/abs/1602.08448.

[30] M. Simchowitz  K. Jamieson  and B. Recht. The simulator: Understanding adaptive sampling
in the moderate-conﬁdence regime. In Proceedings of the 30th Conference on Learning Theory
(COLT)  2017.

[31] I. Takahisa and T. Kaneko. Estimating the maximum expected value through upper conﬁdence
bound of likelihood. In Conference on Technologies and Applications of Artiﬁcial Intelligence
(TAAI)  pages 202–207. IEEE  2017.

[32] K. Teraoka  K. Hatano  and E. Takimoto. Efﬁcient sampling method for Monte Carlo tree search

problem. IEICE Transactions on Infomation and Systems  pages 392–398  2014.

[33] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25:285–294  1933.

[34] H. van Hasselt. Estimating the maximum expected value: An analysis of (nested) cross
validation and the maximum sample average. CoRR  abs/1302.7175  2013. URL http:
//arxiv.org/abs/1302.7175.

11

,Emilie Kaufmann
Wouter Koolen
Aurélien Garivier