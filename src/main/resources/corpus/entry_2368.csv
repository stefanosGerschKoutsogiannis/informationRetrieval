2009,Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models,Little work has been done to directly combine the outputs of multiple supervised and unsupervised models. However  it can increase the accuracy and applicability of ensemble methods. First  we can boost the diversity of classification ensemble by incorporating multiple clustering outputs  each of which provides grouping constraints for the joint label predictions of a set of related objects. Secondly  ensemble of supervised models is limited in applications which have no access to raw data but to the meta-level model outputs. In this paper  we aim at calculating a consolidated classification solution for a set of objects by maximizing the consensus among both supervised predictions and unsupervised grouping constraints. We seek a global optimal label assignment for the target objects  which is different from the result of traditional majority voting and model combination approaches. We cast the problem into an optimization problem on a bipartite graph  where the objective function favors smoothness in the conditional probability estimates over the graph  as well as penalizes deviation from initial labeling of supervised models. We solve the problem through iterative propagation of conditional probability estimates among neighboring nodes  and interpret the method as conducting a constrained embedding in a transformed space  as well as a ranking on the graph. Experimental results on three real applications demonstrate the benefits of the proposed method over existing alternatives.,Graph-based Consensus Maximization among
Multiple Supervised and Unsupervised Models

Jing Gao†  Feng Liang†  Wei Fan‡  Yizhou Sun†  and Jiawei Han†

†University of Illinois at Urbana-Champaign  IL USA
‡IBM TJ Watson Research Center  Hawthorn  NY USA

†{jinggao3 liangf sun22 hanj}@illinois.edu  ‡weifan@us.ibm.com

Abstract

Ensemble classiﬁers such as bagging  boosting and model averaging are known
to have improved accuracy and robustness over a single model. Their potential 
however  is limited in applications which have no access to raw data but to the
meta-level model output. In this paper  we study ensemble learning with output
from multiple supervised and unsupervised models  a topic where little work has
been done. Although unsupervised models  such as clustering  do not directly
generate label prediction for each individual  they provide useful constraints for
the joint prediction of a set of related objects. We propose to consolidate a classi-
ﬁcation solution by maximizing the consensus among both supervised predictions
and unsupervised constraints. We cast this ensemble task as an optimization prob-
lem on a bipartite graph  where the objective function favors the smoothness of the
prediction over the graph  as well as penalizing deviations from the initial labeling
provided by supervised models. We solve this problem through iterative propaga-
tion of probability estimates among neighboring nodes. Our method can also be
interpreted as conducting a constrained embedding in a transformed space  or a
ranking on the graph. Experimental results on three real applications demonstrate
the beneﬁts of the proposed method over existing alternatives1.

1 Introduction

We seek to integrate knowledge from multiple information sources. Traditional ensemble methods
such as bagging  boosting and model averaging are known to have improved accuracy and robustness
over a single model. Their potential  however  is limited in applications which have no access to raw
data but to the meta-level model output. For example  due to privacy  companies or agencies may
not be willing to share their raw data but their ﬁnal models. So information fusion needs to be
conducted at the decision level. Furthermore  different data sources may have different formats  for
example  web video classiﬁcation based on image  audio and text features. In these scenarios  we
have to combine incompatible information sources at the coarser level (predicted class labels) rather
than learn the joint model from raw data.
In this paper  we consider the general problem of combining output of multiple supervised and unsu-
pervised models to improve prediction accuracy. Although unsupervised models  such as clustering 
do not directly generate label predictions  they provide useful constraints for the classiﬁcation task.
The rationale is that objects that are in the same cluster should be more likely to receive the same
class label than the ones in different clusters. Furthermore  incorporating the unsupervised clustering
models into classiﬁcation ensembles improves the base model diversity  and thus has the potential
of improving prediction accuracy.

1More information  data and codes are available at http://ews.uiuc.edu/∼jinggao3/nips09bgcm.htm

1

Figure 1: Groups

Figure 2: Bipartite Graph Figure 3: Position of Consensus Maximization
Suppose we have a set of data points X = {x1  x2  . . .   xn} from c classes. There are m models
that provide information about the classiﬁcation of X  where the ﬁrst r of them are (supervised)
classiﬁers  and the remaining are (unsupervised) clustering algorithms. Consider an example where
X = {x1  . . .   x7}  c = 3 and m = 4. The output of the four models are:
M1 = {1  1  1  2  3  3  2} M2 = {1  1  2  2  2  3  1} M3 = {2  2  1  3  3  1  3} M4 = {1  2  3  1  2  1  1}
where M1 and M2 assign each object a class label  whereas M3 and M4 simply partition the objects
into three clusters and assign each object a cluster ID. Each model  no matter it is supervised or
unsupervised  partitions X into groups  and objects in the same group share either the same predicted
class label or the same cluster ID. We summarize the data  models and the corresponding output by
a bipartite graph. In the graph  nodes at the left denote the groups output by the m models with
some labeled ones from the supervised models  nodes at the right denote the n objects  and a group
and an object are connected if the object is assigned to the group by one of the models. For the
aforementioned toy example  we show the groups obtained from a classiﬁer M1 and a clustering
model M3 in Figure 1  as well as the group-object bipartite graph in Figure 2.
The objective is to predict the class label of xi ∈ X  which agrees with the base classiﬁers’ pre-
dictions  and meanwhile  satisﬁes the constraints enforced by the clustering models  as much as
possible. To reach maximum consensus among all the models  we deﬁne an optimization problem
over the bipartite graph whose objective function penalizes deviations from the base classiﬁers’ pre-
dictions  and discrepancies of predicted class labels among nearby nodes. In the toy example  the
consensus label predictions for X should be {1  1  1  2  2  3  2}.
Related Work. We summarize various learning problems in Figure 3  where one dimension repre-
sents the goal – from unsupervised to supervised  and the other dimension represents the method –
single models  ensembles at the raw data  or ensembles at the output level. Our proposed method is
a semi-supervised ensemble working at the output level  where little work has been done.
Many efforts have been devoted to develop single-model learning algorithms  such as Support Vector
Machines and logistic regression for classiﬁcation  K-means and spectral clustering for clustering.
Recent studies reveal that unsupervised information can also be utilized to improve the accuracy of
supervised learning  which leads to semi-supervised [29  8] and transductive learning [21]. Although
our proposed algorithm works in a transductive setting  existing semi-supervised and transductive
learning methods cannot be easily applied to our problem setting and we discuss this in more de-
tail at the end of Section 2. Note that all methods listed in Figure 3 are for single task learning.
On the contrary  multi-task learning [6  9] deals with multiple tasks simultaneously by exploiting
dependence among tasks  which has a different problem setting and thus is not discussed here.
In Figure 3  we divide ensemble methods into two categories depending on whether they require
access to raw data. In unsupervised learning  many clustering ensemble methods [12  17  25  26]
have been developed to ﬁnd a consensus clustering from multiple partitionings without accessing the
features. In supervised learning  however  only majority voting type algorithms work on the model
output level  and most well-known classiﬁcation ensemble approaches [2  11  19] (eg. bagging 
boosting  bayesian model averaging) involve training diversiﬁed classiﬁers from raw data. Methods
such as mixture of experts [20] and stacked generalization [27] try to obtain a meta-learner on
top of the model output  however  they still need the labels of the raw data as feedbacks  so we
position them as an intermediate between raw data ensemble and output ensemble. In multi-view

2

15243671524367123798g1…...[0 0 1]M1Classifier[1 0 0][0 0 1]…...…...…...[1 0 0]g3g4g6g7g9g10g12x1x2x3x4x5x6x7M2ClassifierM3ClusteringM4ClusteringSingleModelsEnsemble atRaw DataEnsembleat Output LevelK-means Spectral Clustering  …...Semi-supervised TransductiveLearningSVM Logistic Regression …...Multi-view LearningBagging Boosting Bayesianmodelaveraging …...UnsupervisedLearningSupervisedLearningSemi-supervisedLearningClustering EnsembleConsensusMaximizationMajorityVotingMixture of Experts StackedGeneralizationlearning [4  13]  a joint model is learnt from both labeled and unlabeled data from multiple sources.
Therefore  it can be regarded as a semi-supervised ensemble requiring access to the raw data.
Summary. The proposed consensus maximization problem is a challenging problem that cannot
be solved by simple majority voting. To achieve maximum agreement among various models  we
must seek a global optimal prediction for the target objects. In Section 2  we formally deﬁne the
graph-based consensus maximization problem and propose an iterative algorithm to solve it. The
proposed solution propagates labeled information among neighboring nodes until stabilization. We
also present two different interpretations of the proposed method in Section 3  and discuss how
to incorporate feedbacks obtained from a few labeled target objects into the framework in Section
4. An extensive experimental study is carried out in Section 5  where the beneﬁts of the proposed
approach are illustrated on 20 Newsgroup  Cora research papers  and DBLP publication data sets.

2 Methodology
Suppose we have the output of r classiﬁcation algorithms and (m − r) clustering algorithms on a
data set X. For the sake of simplicity  we assume that each point is assigned to only one class or
cluster in each of the m algorithms  and the number of clusters in each clustering algorithm is c 
same as the number of classes. Note that cluster ID z may not be related to class z. So each base
algorithm partitions X into c groups and there are totally v = mc groups  where the ﬁrst s = rc
groups are generated by classiﬁers and the remaining v − s groups are from clustering algorithms.
Before proceeding further  we introduce some notations that will be used in the following discussion:
Bn×m denotes an n× m matrix with bij representing the (ij)-th entry  and (cid:126)bi· and (cid:126)b·j denote vectors
of row i and column j  respectively. See Table 1 for a summary of important symbols.
We represent the objects and groups in a bipartite graph as shown in Figure 2  where the object nodes
x1  . . .   xn are on the right  the group nodes g1  . . .   gv are on the left. The afﬁnity matrix An×v of
this graph summarizes the output of m algorithms on X:

aij = 1 

if xi is assigned to group gj by one of the algorithms;

0 

otherwise.

We aim at estimating the conditional probability of each object node xi belonging to c classes. As
a nuisance parameter  the conditional probabilities at each group node gj are also estimated. These
conditional probabilities are denoted by Un×c for object nodes and Qv×c for group nodes:

uiz = ˆP (y = z|xi)

and

qjz = ˆP (y = z|gj).

Since the ﬁrst s = rc groups are obtained from supervised learning models  they have some initial
class label estimates denoted by Yv×c where

if gj’s predicted label is z  j = 1  . . .   s;

yjz = 1 
z=1 yjz  and we formulate the consensus agreement as the following optimization

otherwise.

0 

(cid:80)c

Let kj =
problem on the graph:

(cid:179) n(cid:88)

v(cid:88)

v(cid:88)

(cid:180)

min
Q U

f(Q  U) = min
Q U

aij||(cid:126)ui· − (cid:126)qj·||2 + α

kj||(cid:126)qj· − (cid:126)yj·||2

(1)

i=1

j=1

j=1

s.t. (cid:126)ui· ≥ (cid:126)0  |(cid:126)ui·| = 1  i = 1 : n

(cid:126)qj· ≥ (cid:126)0  |(cid:126)qj·| = 1  j = 1 : v

where ||.|| and |.| denote a vector’s L2 and L1 norm respectively. The ﬁrst term ensures that if an
object xi is assigned to group gj by one of the algorithm  their conditional probability estimates
must be close. When j = 1  . . .   s  the group node gj is from a classiﬁer  so kj = 1 and the second
term puts the constraints that a group gj’s consensus class label estimate should not deviate much
from its initial class label prediction. α is the shadow price payment for violating the constraints.
When j = s + 1  . . .   v  gj is a group from an unsupervised model with no such constraints  and
thus kj = 0 and the weight of the constraint is 0. Finally  (cid:126)ui· and (cid:126)qj· are probability vectors  and
therefore each component must be greater than or equal to 0 and the sum equals to 1.
We propose to solve this problem using block coordinate descent methods as shown in Algorithm 1.
At the t-th iteration  if we ﬁx the value of U  the objective function is a summation of v quadratic
components with respect to (cid:126)qj·. The corresponding Hessian matrix is diagonal with entries equal to

3

(cid:80)n
(cid:80)n
i=1 aij (cid:126)u (t−1)

i·

(cid:80)v
(cid:80)v

Table 1: Important Notations

Algorithm 1 BGCM algorithm
Input: group-object afﬁnity matrix A  initial la-
beling matrix Y ; parameters α and ;
Output: consensus matrix U;
Algorithm:

Initialize U 0 U 1 randomly
t ← 1
while ||U t − U t−1|| >  do

class indexes
object indexes
indexes of groups from supervised models
indexes of groups from unsupervised models
aij-indicator of object i in group j
uiz-probability of object i wrt class z
(cid:80)n
qjz-probability of group j wrt class z
return U t
yjz-indicator of group j predicted as class z
i=1 aij + αkj > 0. Therefore it is strictly convex and ∇(cid:126)qj·f(Q  U (t−1)) = 0 gives the unique
global minimum of the cost function with respect to (cid:126)qj· in Eq. (2). Similarly  ﬁxing Q  the unique
global minimum with respect to (cid:126)ui· is also obtained.

Symbol Deﬁnition
1  . . .   c
1  . . .   n
1  . . .   s
s + 1  . . .   v
An×v = [aij]
Un×c = [uiz]
Qv×c = [qjz]
Yv×c = [yjz]

Qt = (Dv +αKv)−1(AT U t−1+αKvY )
U t = D−1

n AQt

(

(2)

(cid:169)

(cid:170)

(cid:126)q (t)
j· =

(cid:169)

(

(cid:126)u (t)

i· =

j=1 aij)

+ αkj(cid:126)yj·

(cid:170)
(cid:170)

(cid:80)v

i=1 aij + αkj

i=1 aij)
z=1 yjz)

(cid:169)
(cid:80)n
(cid:80)c

j=1 aij(cid:126)q (t)
j·
j=1 aij
The update formula in matrix forms are given in Algorithm 1. Dv = diag
v×v and
Dn = diag
n×n act as the normalization factors. Kv = diag
(
v×v indi-
cates the existence of constraints on the group nodes. During each iteration  the probability estimate
at each group node (i.e.  Q) receives the information from its neighboring object nodes while retains
its initial value Y   and in return the updated probability estimates at group nodes propagate the in-
formation back to its neighboring object nodes when updating U. It is straightforward to prove that
(Q(t)  U (t)) converges to a stationary point of the optimization problem [3].
In [14]  we proposed a heuristic method to combine heterogeneous information sources. In this pa-
per  we bring up the concept of consensus maximization and solve the problem over a bipartite graph
representation. Our proposed method is related to graph-based semi-supervised learning (SSL). But
existing SSL algorithms only take one supervised source (i.e.  the labeled objects) and one unsu-
pervised source (i.e.  the similarity graph) [29  8]  and thus cannot be applied to combine multiple
models. Some SSL methods [16] can incorporate results from an external classiﬁer into the graph 
but obviously they cannot handle multiple classiﬁers and multiple unsupervised sources. To apply
SSL algorithms on our problem  we must ﬁrst fuse all supervised models into one by some ensem-
ble approach  and fuse all unsupervised models into one by deﬁning a similarity function. Such a
compression may lead to information loss  whereas the proposed method retains all the information
and thus consensus can be reached among all the based model output.

3 Interpretations

In this part  we explain the proposed method from two independent perspectives.
Constrained Embedding. Now we focus on the “hard” consensus solution  i.e.  each point is
assigned to exactly one class. So U and Q are indicator matrices: uiz = 1 if the ensemble assigns
xi to class z  and 0 otherwise; similar for qjz’s. For group nodes from classiﬁcation algorithms 
we will treat their entries in Q as known since they have been assigned a class label by one of the
classiﬁers  that is  qjz = yjz for 1 ≤ j ≤ s.
Because U represents the consensus  we should let group gj correspond to class z if majority of the
objects in group gj correspond to class z in the consensus solution. The optimization is thus:

(3)

c(cid:88)

s.t.

z=1

uiz = 1∀i ∈ {1  . . .   n}

qjz = 1∀j ∈ {s+1  . . .   v} uiz ∈ {0  1} qjz ∈ {0  1} (4)

qjz = 1 ∀j ∈ {1  . . .   s} if gj’s label is z qjz = 0 ∀j ∈ {1  . . .   s} if gj’s label is not z

(5)

(cid:175)(cid:175)(cid:175)(cid:175)qjz −

(cid:80)n
(cid:80)n

i=1 aijuiz

i=1 aij

(cid:175)(cid:175)(cid:175)(cid:175)

min
Q U

c(cid:88)

z=1

v(cid:88)
c(cid:88)

j=1

z=1

4

i=1 aij

i=1 aij (cid:126)ui·

Here  the two indicator matrices U and Q can be viewed as embedding x1  . . .   xn (object nodes)
and g1  . . .   gv (group nodes) into a c-dimensional cube. Due to the constraints in Eq. (4)  (cid:126)ui· and
(cid:126)qj· reside on the boundary of the (c − 1)-dimensional hyperplane in the cube. (cid:126)a·j denotes the
objects group gj contains  (cid:126)qj· can be regarded as the group representative in this new space  and
. For the s groups obtained from classiﬁcation
thus it should be close to the group mean:
algorithms  we know their “ideal” embedding  as represented in the constraints in Eq. (5).
We now relate this problem to the optimization framework discussed in Section 2. aij can only take
value of 0 or 1  and thus Eq. (3) just depends on the cases when aij = 1. When aij = 1  no matter
qjz is 1 or 0  we have |qjz
c(cid:88)
(cid:88)
i=1 aijuiz

(cid:80)n
(cid:80)n
i=1 aij −(cid:80)n
(cid:80)n
(cid:175)(cid:175)qjz
(cid:175)(cid:175)(cid:175)(cid:175) =
c(cid:88)
(cid:88)

(cid:80)n
(cid:80)n
i=1 aij −(cid:80)n
(cid:80)n
i=1 |aij(qjz − uiz)|. Therefore 
i=1 aijuiz| =
(cid:80)n
(cid:80)n
i=1 |aij(qjz − uiz)|
i=1 aijuiz
c(cid:88)

(cid:88)
(cid:80)n
v(cid:88)
n(cid:88)

Suppose the groups found by the base models have balanced size  i.e. 
constant for ∀j. Then the objective function can be approximated as:
|qjz − uiz| =

|aij(qjz − uiz)| =

i=1 aij = γ where γ is a

|qjz − uiz|

n(cid:88)

(cid:88)

c(cid:88)

c(cid:88)

j:aij =1

z=1

i=1 aij

i=1 aij

i=1 aij

j:aij =1

aij

aij

(cid:175)(cid:175)

z=1

=

j:aij =1

z=1

(cid:80)n
(cid:175)(cid:175)(cid:175)(cid:175)qjz −
(cid:80)n
c(cid:88)
(cid:88)
n(cid:88)
(cid:80)n

z=1

i=1

j:aij =1

i=1

j=1

z=1

i=1

j:aij =1

z=1

i=1

j=1 aij

(cid:80)v

(cid:80)c
Therefore  when the classiﬁcation and clustering algorithms generate balanced groups  with the same
set of constraints in Eq. (4) and Eq. (5)  the constrained embedding problem in Eq. (3) is equivalent
z=1 |qjz − uiz|. It is obvious that this is the same as the optimization
to: min Q U
problem we propose in Section 2 with two relaxations: 1) We transform hard constraints in Eq. (5)
to soft constraints where the ideal embedding is expressed in the initial labeling matrix Y and the
price for violating the constraints is set to α. 2) uiz and qjz are relaxed to have values between 0 and
1  instead of either 0 or 1  and quadratic cost functions replace the L1 norms. So they are probability
estimates rather than class membership indicators  and we can embed them anywhere on the plane.
Though with these relaxations  we build connections between the constrained embedding framework
as discussed in this section with the one proposed in Section 2. Therefore  we can view our proposed
method as embedding both object nodes and group nodes into a hyperlane so that object nodes are
close to the group nodes they link to. The constraints are put on the group nodes from supervised
models to penalize the embedding that are far from the “ideal” ones.
Ranking on Consensus Structure. Our method can also be viewed as conducting ranking with re-
spect to each class on the bipartite graph  where group nodes from supervised models act as queries.
Suppose we wish to know the probability of any group gj belonging to class 1  which can be regarded
as the relevance score of gj with respect to example queries from class 1. Let wj =
i=1 aij. In
Algorithm 1  the relevance scores of all the groups are learnt using the following equation:

(cid:80)n

n A)(cid:126)q·1 + D1−λ(cid:126)y·1
wj
.

and

(cid:126)q·1 = (Dv + αKv)−1(AT D−1

n A(cid:126)q·1 + αKv(cid:126)y·1) = Dλ(D−1

v AT D−1

αkj

wj +αkj

wj +αkj

where the v × v diagonal matrices Dλ and D1−λ have (j  j) entries as
Consider collapsing the original bipartite graph into a graph with group nodes only  then AT A is its
afﬁnity matrix. After normalizing it to be a probability matrix  we have pij in P = D−1
v AT D−1
n A
represent the probability of jumping to node j from node i. The groups that are predicted to be in
class 1 by one of the supervised models have 1 at the corresponding entries in (cid:126)y·1  therefore these
group nodes are “queries” and we wish to rank the group nodes according to their relevance to them.
Comparing our ranking model with PageRank model [24]  there are the following relationships: 1)
In PageRank  a uniform vector with entries all equal to 1 replaces (cid:126)y·1. In our model  we use (cid:126)y·1 to
show our preference towards the query nodes  so the resulting scores would be biased to reﬂect the
relevance regarding class 1. 2) In PageRank  the weights Dλ and D1−λ are ﬁxed constants λ and
1 − λ  whereas in our model Dλ and D1−λ give personalized damping factors  where each group
has a damping factor λj = wj
. 3) In PageRank  the value of link-votes are normalized by the
number of outlinks at each node  whereas our ranking model does not normalize pij on its outlinks 
and thus can be viewed as an un-normalized version of personalized PageRank [18  28]. When each
base model generates balanced groups  both λj and outlinks at each node become constants  and the
proposed method simulates the standard personalized PageRank.

wj +αkj

5

Data

Newsgroup

Cora

DBLP

ID
1
2
3
4
5
6
1
2
3
4
1

Table 2: Data Sets Description

Category Labels

comp.graphics comp.os.ms-windows.misc sci.crypt sci.electronics

rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey

sci.cypt sci.electronics sci.med sci.space

misc.forsale rec.autos rec.motorcycles talk.politics.misc
rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics

alt.atheism rec.sport.baseball rec.sport.hockey soc.religion.christian

Operating Systems Programming Data Structures Algorithms and Theory

Databases Hardware and Architecture Networking Human Computer Interaction

Distributed Memory Management Agents Vision and Pattern Recognition

Graphics and Virtual Reality Object Oriented Planning Robotics Compiler Design Software Development

Databases Data Mining Machine Learning Information Retrieval

#target
1408
1428
1413
1324
1424
1352
603
897
1368
875
3836

#labeled

160
160
160
160
160
160
60
80
100
100
400

v AT D−1

The relevance scores with respect to class 1 for group and object nodes will converge to
(cid:126)q·1 = (Iv − DλD−1
v AT )−1D−1
n AD1−λ(cid:126)y·1
respectively. Iv and In are identity matrices with size v × v and n × n. The above arguments hold
for the other classes as well  and thus each column in U and Q represents the ranking of the nodes
with respect to each class. Because each row sums up to 1  they are conditional probability estimates
of the nodes belonging to one of the classes.

(cid:126)u·1 = (In − D−1

n A)−1D1−λ(cid:126)y·1

n ADλD−1

4 Incorporating Labeled Information

Thus far  we propose to combine the output of supervised and unsupervised models by consensus.
When the true labels of the objects are unknown  this is a reliable approach. However  incorporating
labels from even a small portion of the objects may greatly reﬁne the ﬁnal hypothesis. We assume
that labels of the ﬁrst l objects are known  which is encoded in an n × c matrix F :
otherwise.

xi’s observed label is z  i = 1  . . .   l;

fiz = 1 

0 

We modify the objection function in Eq. (1) to penalize the deviation of (cid:126)ui· of labeled objects from
the observed label:

v(cid:88)

n(cid:88)

n(cid:88)

v(cid:88)

f(Q  U) =

(cid:80)c

aij||(cid:126)ui· − (cid:126)qj·||2 + α

kj||(cid:126)qj· − (cid:126)yj·||2 + β

hi||(cid:126)ui· − (cid:126)fi·||2

(6)

i=1

j=1

j=1

i=1

(cid:80)v
(cid:80)v

where hi =
z=1 fiz. When i = 1  . . .   l  hi = 1  so we enforce the constraints that an object xi’s
consensus class label estimate should be close to its observed label with a shadow price β. When
i = l + 1  . . .   n  xi is unlabeled. Therefore  hi = 0 and the constraint term is eliminated from the
objective function. To update the condition probability for the objects  we incorporate their prior
labeled information:

j=1 aij(cid:126)q t

j· + βhi
j=1 aij + βhi

(cid:126)fi·

(cid:126)u t

i· =

(

(cid:170)

(cid:169)

(cid:80)c

(7)
it would be U t = (Dn + βHn)−1(AQt + βHnF ) with Hn =
In matrix forms 
z=1 fiz)
diag
n×n. Note that the initial conditional probability of a labeled object is 1 at its
observed class label  and 0 at all the others. However  this optimistic estimate will be changed
during the updates  with the rationale that the observed labels are just random samples from some
multinomial distribution. Thus we only use the observed labels to bias the updating procedure 
instead of totally relying on them.

5 Experiments

We evaluate the proposed algorithms on eleven classiﬁcation tasks from three real world applica-
tions.
In each task  we have a target set on which we wish to predict class labels. Clustering
algorithms are performed on this target set to obtain the grouping results. On the other hand  we
learn classiﬁcation models from some training sets that are in the same domain or a relevant domain
with respect to the target set. These classiﬁcation models are applied to the target set as well. The
proposed algorithm generates a consolidated classiﬁcation solution for the target set based on both
classiﬁcation and clustering results. We elaborate details of each application in the following.

6

Table 3: Classiﬁcation Accuracy Comparison on a Series of Data Sets

Methods

M1
M2
M3
M4
MCLA
HBGF
BGCM
2-L
3-L
BGCM-L
STD

1

0.7967
0.7721
0.8056
0.7770
0.7592
0.8199
0.8128
0.7981
0.8188
0.8316
0.0040

2

0.8855
0.8611
0.8796
0.8571
0.8173
0.9244
0.9101
0.9040
0.9206
0.9197
0.0038

20 Newsgroups
3

4

0.8557
0.8134
0.8658
0.8149
0.8253
0.8811
0.8608
0.8511
0.8820
0.8859
0.0037

0.8826
0.8676
0.8983
0.8467
0.8686
0.9152
0.9125
0.8728
0.9158
0.9240
0.0040

5

0.8765
0.8358
0.8716
0.8543
0.8295
0.8991
0.8864
0.8830
0.8989
0.9016
0.0027

6

0.8880
0.8563
0.9020
0.8578
0.8546
0.9125
0.9088
0.8977
0.9121
0.9177
0.0030

1

0.7745
0.7797
0.7779
0.7476
0.8703
0.7834
0.8687
0.8066
0.8557
0.8891
0.0096

Cora

2

0.8858
0.8594
0.8833
0.8594
0.8388
0.9111
0.9155
0.8798
0.9086
0.9181
0.0027

3

0.8671
0.8508
0.8646
0.7810
0.8892
0.8481
0.8965
0.8932
0.9202
0.9246
0.0052

4

0.8841
0.8879
0.8813
0.9016
0.8716
0.8943
0.9090
0.8951
0.9141
0.9206
0.0044

DBLP

1

0.9337
0.8766
0.9382
0.7949
0.8953
0.9357
0.9417
0.9054
0.9332
0.9480
0.0020

20 Newsgroup categorization. We construct six learning tasks  each of which involves four classes.
The objective is to classify newsgroup messages according to topics. We used the version [1] where
the newsgroup messages are sorted by date  and separated into training and test sets. The test sets
are our target sets. We learn logistic regression [15] and SVM models [7] from the training sets  and
apply these models  as well as K-means and min-cut clustering algorithms [22] on the target sets.
Cora research paper classiﬁcation. We aim at classifying a set of research papers into their areas
[23]. We extract four target sets  each of which includes papers from around four areas. The train-
ing sets contain research papers that are different from those in the target sets. Both training and
target sets have two views  the paper abstracts  and the paper citations. We apply logistic regression
classiﬁers and K-means clustering algorithms on the two views of the target sets.
DBLP data. We retrieve around 4 000 authors from DBLP network [10]  and try to predict their
research areas. The training sets are drawn from a different domain  i.e.  the conferences in each
research ﬁeld. There are also two views for both training and target sets  the publication network  and
the textual content of the publications. The amount of papers an author published in the conference
can be regarded as link feature  whereas the pool of titles that an author published is the text feature.
Logistic regression and K-means clustering algorithms are used to derive the predictions on the
target set. We manually label the target set for evaluation.
The details of each learning task are summarized in Table 2. On each target set  we apply four
models M1 to M4  where the ﬁrst two are classiﬁcation models and the remaining two are clus-
tering models. We denote the proposed method as Bipartite Graph-based Consensus Maximization
(BGCM)  which combines the output of the four models. As shown in Figure 3  only clustering
ensembles  majority voting methods  and the proposed BGCM algorithm work at the meta output
level where raw data are discarded and only prediction results from multiple models are available.
However  majority voting can not be applied when there are clustering models because the cor-
respondence between clusters and classes is unknown. Therefore  we compare BGCM with two
clustering ensemble approaches (MCLA [26] and HBGF [12])  which ignore the label information
from supervised models  regard all the base models as unsupervised clustering  and integrate the
output of the base models. So they only give clustering solutions  not classiﬁcation results.
To evaluate classiﬁcation accuracy  we map the output of all the clustering algorithms (the base
models  and the ensembles) to the best possible class predictions with the help of hungarian method
[5]  where cluster IDs are matched with class labels. Actually  it is “cheating” because the true class
labels are used to do the mapping  and thus it should be able to generate the best accuracy from
these unsupervised models. As discussed in Section 4  we can incorporate a few labeled objects 
which are drawn from the same domain of the target set  into the framework and improve accuracy.
This improved version of the BGCM algorithm is denoted as BGCM-L  and the number of labeled
objects used in each task is shown in Table 2. On each task  we repeat the experiments 50 times 
each of which has randomly chosen target and labeled objects  and report the average accuracy. Due
to space limit  we only show the standard deviation (STD) for BGCM-L method. The baselines
share very similar standard deviation with the reported one on each task.
Accuracy. In Table 3  we summarized the classiﬁcation accuracy of all the baselines and the pro-
posed approach on the target sets of eleven tasks. The two single classiﬁers (M1 and M2)  and the
two clustering single models (M3 and M4) usually have low accuracy. By combining all the base
models  the clustering ensemble approaches (MCLA and HBGF) can improve the performance over
each single model. However  these two methods are not designed for classiﬁcation  and the reported

7

Figure 4: Sensitivity Analysis

accuracy is the upper bound of their “true” accuracy. The proposed BGCM method always outper-
forms the base models  and achieves better or comparable performances compared with the upper
bound of the baseline ensembles. By incorporating a small portion (around 10%) of labeled objects 
the BGCM-L method further improves the performances. The consistent increase in accuracy can
be observed in all the tasks  where the margin between the accuracy of the best single model and
that of the BGCM-L method is from 2% to 10%. Even when taking variance into consideration  the
results demonstrate the power of consensus maximization in accuracy improvements.
Sensitivity. As shown in Figure 4 (a) and (b)  the proposed BGCM-L method is not sensitive to the
parameters α and β. To make the plots clear  we just show the performance on the ﬁrst task of each
application. α and β are the shadow prices paid for deviating from the estimated labels of groups
and observed labels of objects  so they should be greater than 0. α and β represent the conﬁdence
of our belief in the labels of the groups and objects compared with 1. The labels of group nodes are
obtained from supervised models and may not be correct  therefore  a smaller α usually achieves
better performance. On the other hand  the labels of objects can be regarded as groundtruths  and
thus the larger β the better. In experiments  we ﬁnd that when α is below 4  and β greater than 4 
good performance can be achieved. We let α = 2 and β = 8 to get the experimental results shown
in Table 3. Also  we ﬁx the target set as 80% of all the objects  and use 1% to 20% as the labeled
objects to see how the performance varies  and the results are summarized in Figure 4 (c). In general 
more labeled objects would help the classiﬁcation task where the improvements are more visible on
Cora data set. When the percentage reaches 10%  BGCM-L’s performance becomes stable.
Number of Models. We vary the number of base models incorporated into the consensus frame-
work. The BGCM-L method on two models is denoted as 2-L  where we average the performance
of the combined model obtained by randomly choosing one classiﬁer and one clustering algorithm.
Similarly  the BGCM-L method on three models is denoted as 3-L. From Table 3  we can see that
BGCM-L method using all the four models outperforms the method incorporating only two or three
models. When the base models are independent and each of them obtains reasonable accuracy 
combining more models would beneﬁt more because the chances of reducing independent errors
increase. However  when the new model cannot provide additional information to the current pool
of models  incorporating it may not improve the performance anymore. In the future  we plan to
identify this upper bound through experiments with more input sources.

6 Conclusions

In this work  we take advantage of the complementary predictive powers of multiple supervised
and unsupervised models to derive a consolidated label assignment for a set of objects jointly. We
propose to summarize base model output in a group-object bipartite graph  and maximize the con-
sensus by promoting smoothness of label assignment over the graph and consistency with the initial
labeling. The problem is solved by propagating labeled information between group and object nodes
through their links iteratively. The proposed method can be interpreted as conducting an embedding
of object and group nodes into a new space  as well as an un-normalized personalized PageRank.
When a few labeled objects are available  the proposed method uses them to guide the propagation
and reﬁne the ﬁnal hypothesis. In the experiments on 20 newsgroup  Cora and DBLP data  the
proposed consensus maximization method improves the best base model accuracy by 2% to 10%.
Acknowledgement The work was supported in part by the U.S. National Science Foundation grants
IIS-08-42769  IIS-09-05215 and DMS-07-32276  and the Air Force Ofﬁce of Scientiﬁc Research
MURI award FA9550-08-1-0265.

8

051015200.80.850.90.951αAccuracy(a) Performance w.r.t. α  Newsgroup1Cora1DBLP1051015200.80.850.90.951βAccuracy(b) Performance w.r.t. β  Newsgroup1Cora1DBLP10  0.020.040.060.080.10.80.850.90.951% Labeled ObjectsAccuracy(c) Performance w.r.t. % Labeled Objects  Newsgroup1Cora1DBLP1References
[1] 20 Newsgroups Data Set. http://people.csail.mit.edu/jrennie/20Newsgroups/.
[2] E. Bauer and R. Kohavi. An Empirical Comparison of Voting Classiﬁcation Algorithms: Bagging  Boost-

ing  and Variants. Machine Learning  36:105–139  2004.

[3] Dimitri P. Bertsekas. Non-Linear Programming (2nd Edition). Athena Scientiﬁc  1999.
[4] A. Blum and T. Mitchell. Combining Labeled and Unlabeled Data with Co-training. In Proc. of COLT’

98  pages 92–100  1998.

[5] N. Borlin. Implementation of Hungarian Method. http://www.cs.umu.se/∼niclas/matlab/assignprob/.
[6] R. Caruana. Multitask Learning. Machine Learning  28:41–75  1997.
[7] C.-C. Chang and C.-J. Lin. LibSVM: a Library for Support Vector Machines  2001. Software available

at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.

[8] O. Chapelle  B. Sch¨olkopf and A. Zien (eds). Semi-Supervised Learning. MIT Press  2006.
[9] K. Crammer  M. Kearns and J. Wortman. Learning from Multiple Sources. Journal of Machine Learning

Research  9:1757-1774   2008.

[10] DBLP Bibliography. http://www.informatik.uni-trier.de/∼ley/db/.
[11] T. Dietterich. Ensemble Methods in Machine Learning. In Proc. of MCS ’00  pages 1–15  2000.
[12] X. Z. Fern and C. E. Brodley. Solving Cluster Ensemble Problems by Bipartite Graph Partitioning. In

Proc. of ICML’ 04  pages 281–288  2004.

[13] K. Ganchev  J. Graca  J. Blitzer  and B. Taskar. Multi-view Learning over Structured and Non-identical

Outputs. In Proc. of UAI’ 08  pages 204–211  2008.

[14] J. Gao  W. Fan  Y. Sun  and J. Han. Heterogeneous source consensus learning via decision propagation

and negotiation. In Proc. of KDD’ 09  pages 339–347  2009.

[15] A. Genkin  D. D. Lewis  and D. Madigan. BBR: Bayesian Logistic Regression Software.

http://stat.rutgers.edu/∼madigan/BBR/.

[16] A. Goldberg and X. Zhu. Seeing stars when there aren’t many stars: Graph-based semi-supervised

learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs.

[17] A. Gionis  H. Mannila  and P. Tsaparas. Clustering Aggregation. ACM Transactions on Knowledge

Discovery from Data  1(1)  2007.

[18] T. Haveliwala. Topic-Sensitive PageRank: A Context-Sensitive Ranking Algorithm for Web Search.

IEEE Transactions on Knowledge and Data Engineering  15(4):1041-4347  2003.

[19] J. Hoeting  D. Madigan  A. Raftery  and C. Volinsky. Bayesian Model Averaging: a Tutorial. Statistical

Science  14:382–417  1999.

[20] R. Jacobs  M. Jordan  S. Nowlan  and G. Hinton. Adaptive Mixtures of Local Experts. Neural Computa-

tion  3:79-87  1991.

[21] T. Joachims. Transductive Learning via Spectral Graph Partitioning. In Proc. of ICML’ 03  pages 290–

297  2003.

[22] G. Karypis. CLUTO – Family of Data Clustering Software Tools.

http://glaros.dtc.umn.edu/gkhome/views/cluto.

[23] A. McCallum  K. Nigam  J. Rennie  and K. Seymore. Automating the Construction of Internet Portals

with Machine Learning. Information Retrieval Journal  3:127–163  2000.

[24] L. Page  S. Brin  R. Motwani  and T. Winograd. The PageRank Citation Ranking: Bringing Order to the

Web. Technical Report  Stanford InfoLab  1999.

[25] V. Singh  L. Mukherjee  J. Peng  and J. Xu. Ensemble Clustering using Semideﬁnite Programming. In

Proc. of NIPS’ 07  2007.

[26] A. Strehl and J. Ghosh. Cluster Ensembles – a Knowledge Reuse Framework for Combining Multiple

Partitions. Journal of Machine Learning Research  3:583–617  2003.

[27] D. Wolpert. Stacked Generalization. Neural Networks  5:241–259  1992.
[28] D. Zhou   J. Weston  A. Gretton  O. Bousquet and B. Scholkopf. Ranking on Data Manifolds. In Proc.

of NIPS’ 03  pages 169–176  2003.

[29] X. Zhu. Semi-supervised Learning Literature Survey. Technical Report 1530  Computer Sciences  Uni-

versity of Wisconsin-Madison  2005.

9

,Hedi Hadiji