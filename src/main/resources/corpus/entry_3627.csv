2010,Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers,As increasing amounts of sensitive personal information finds its way into data repositories  it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model  provides a framework to analyze such mechanisms for databases belonging to a single party  this framework has not yet been considered in a multi-party setting. In this paper  we propose a privacy-preserving protocol for composing a differentially private aggregate classifier  using classifiers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classifier. We also present a detailed theoretical analysis containing a proof of differential privacy  of the perturbed aggregate classifier and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset.,Multiparty Differential Privacy via Aggregation of

Locally Trained Classiﬁers

Manas A. Pathak

Carnegie Mellon University

Pittsburgh  PA

manasp@cs.cmu.edu

Shantanu Rane

Cambridge  MA

rane@merl.com

Mitsubishi Electric Research Labs

Carnegie Mellon University

Bhiksha Raj

Pittsburgh  PA

bhiksha@cs.cmu.edu

Abstract

As increasing amounts of sensitive personal information ﬁnds its way into data
repositories  it is important to develop analysis mechanisms that can derive ag-
gregate information from these repositories without revealing information about
individual data instances. Though the differential privacy model provides a frame-
work to analyze such mechanisms for databases belonging to a single party  this
framework has not yet been considered in a multi-party setting. In this paper  we
propose a privacy-preserving protocol for composing a differentially private ag-
gregate classiﬁer using classiﬁers trained locally by separate mutually untrusting
parties. The protocol allows these parties to interact with an untrusted curator to
construct additive shares of a perturbed aggregate classiﬁer. We also present a
detailed theoretical analysis containing a proof of differential privacy of the per-
turbed aggregate classiﬁer and a bound on the excess risk introduced by the per-
turbation. We verify the bound with an experimental evaluation on a real dataset.

1

Introduction

In recent years  individuals and corporate entities have gathered large quantities of personal data.
Often  they may wish to contribute the data towards the computation of functions such as various
statistics  responses to queries  classiﬁers etc.
In the process  however  they risk compromising
the privacy of the individuals by releasing sensitive information such as their medical or ﬁnancial
records  addresses and telephone numbers  preferences of various kinds which the individuals may
not want exposed. Merely anonymizing the data is not sufﬁcient – an adversary with access to
publicly available auxiliary information can still recover the information about individual  as was
the case with the de-anonymization of the Netﬂix dataset [1].
In this paper  we address the problem of learning a classiﬁer from a multi-party collection of such
private data. A set of parties P1  P2  . . .   PK each possess data D1  D2  . . .   DK. The aim is to
learn a classiﬁer from the union of all the data D1∪ D2 . . .∪ DK. We speciﬁcally consider a logistic
regression classiﬁer  but as we shall see  the techniques are generally applicable to any classiﬁcation
algorithm. The conditions we impose are that (a) None of the parties are willing to share the data
with one another or with any third party (e.g. a curator). (b) The computed classiﬁer cannot be
reverse engineered to learn about any individual data instance possessed by any contributing party.
The conventional approach to learning functions in this manner is through secure multi-party com-
putation (SMC) [2]. Within SMC individual parties use a combination of cryptographic techniques
and oblivious transfer to jointly compute a function of their private data [3  4  5]. The techniques
typically provide guarantees that none of the parties learn anything about the individual data besides
what may be inferred from the ﬁnal result of the computation. Unfortunately  this does not satisfy
condition (b) above. For instance  when the outcome of the computation is a classiﬁer  it does not
prevent an adversary from postulating the presence of data instances whose absence might change

1

the decision boundary of the classiﬁer  and verifying the hypothesis using auxiliary information if
any. Moreover  for all but the simplest computational problems  SMC protocols tend to be highly
expensive  requiring iterated encryption and decryption and repeated communication of encrypted
partial results between participating parties.
An alternative theoretical model for protecting the privacy of individual data instances is differential
privacy [6]. Within this framework  a stochastic component is added to any computational mecha-
nism  typically by the addition of noise. A mechanism evaluated over a database is said to satisfy
differential privacy if the probability of the mechanism producing a particular output is almost the
same regardless of the presence or absence of any individual data instance in the database. Dif-
ferential privacy provides statistical guarantees that the output of the computation does not carry
information about individual data instances. On the other hand  in multiparty scenarios where the
data used to compute a function are distributed across several parties  it does not provide any mech-
anism for preserving the privacy of the contributing parties from one another or alternately  from a
curator who computes the function from the combined data.
We provide an alternative solution: within our approach the individual parties locally compute an
optimal classiﬁer with their data. The individual classiﬁers are then averaged to obtain the ﬁnal ag-
gregate classiﬁer. The aggregation is performed through a secure protocol that also adds a stochastic
component to the averaged classiﬁer  such that the resulting aggregate classiﬁer is differentially
private  i.e.  no inference may be made about individual data instances from the classiﬁer. This
procedure satisﬁes both criteria (a) and (b) mentioned above. Furthermore  it is signiﬁcantly less
expensive than any SMC protocol to compute the classiﬁer on the combined data.
We also present theoretical guarantees on the classiﬁer. We provide a fundamental result that the
excess risk of an aggregate classiﬁer obtained by averaging classiﬁers trained on individual subsets 
compared to the optimal classiﬁer computed on the combined data in the union of all subsets  is
bounded by a quantity that depends on the size of the smallest subset. We prove that the addition of
the noise does indeed result in a differentially private classiﬁer. We also provide a bound on the true
excess risk of the differentially private averaged classiﬁer compared to the optimal classiﬁer trained
on the combined data. Finally  we present experimental evaluation of the proposed technique on
a UCI Adult dataset which is a subset of the 1994 census database and empirically show that the
differentially private classiﬁer trained using the proposed method provides the performance close to
the optimal classiﬁer when the distribution of data across parties is reasonably equitable.

2 Differential Privacy

In this paper  we consider the differential privacy model introduced by Dwork [6]. Given any two
databases D and D(cid:48) differing by one element  which we will refer to as adjacent databases  a
randomized query function M is said to be differentially private if the probability that M produces
a response S on D is close to the probability that M produces the same response S on D(cid:48). As
the query output is almost the same in the presence or absence of an individual entry with high
probability  nothing can be learned about any individual entry from the output.

Deﬁnition A randomized function M with a well-deﬁned probability density P satisﬁes -
differential privacy if  for all adjacent databases D and D(cid:48) and for any S ∈ range(M ) 

(1)

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:12)(cid:12)(cid:12)(cid:12) ≤ .

P (M (D) = S)
P (M (D(cid:48)) = S)

In a classiﬁcation setting  the training dataset may be thought of as the database and the algorithm
learning the classiﬁcation rule as the query mechanism. A classiﬁer satisfying differential privacy
implies that no additional details about the individual training data instances can be obtained with
certainty from output of the learning algorithm  beyond the a priori background knowledge. Differ-
ential privacy provides an ad omnia guarantee as opposed to most other models that provide ad hoc
guarantees against a speciﬁc set of attacks and adversarial behaviors. By evaluating the differentially
private classiﬁer over a large number of test instances  an adversary cannot learn the exact form of
the training data.

2

2.1 Related Work

Dwork et al. [7] proposed the exponential mechanism for creating functions satisfying differential
privacy by adding a perturbation term from the Laplace distribution scaled by the sensitivity of the
function. Chaudhuri and Monteleoni [8] use the exponential mechanism [7] to create a differen-
tially private logistic regression classiﬁer by perturbing the estimated parameters with multivariate
Laplacian noise scaled by the sensitivity of the classiﬁer. They also propose another method to learn
classiﬁers satisfying differential privacy by adding a linear perturbation term to the objective func-
tion which is scaled by Laplacian noise. Nissim  et al. [9] show we can create a differentially private
function by adding noise from Laplace distribution scaled by the smooth sensitivity of the function.
While this mechanism results in a function with lower error  the smooth sensitivity of a function
can be difﬁcult to compute in general. They also propose the sample and aggregate framework
for replacing the original function with a related function for which the smooth sensitivity can be
easily computed. Smith [10] presents a method for differentially private unbiased MLE using this
framework.
All the previous methods are inherently designed for the case where a single curator has access
to the entire data and is interested in releasing a differentially private function computed over the
data. To the best of our knowledge and belief  ours is the ﬁrst method designed for releasing a
differentially private classiﬁer computed over training data owned by different parties who do not
wish to disclose the data to each other. Our technique was principally motivated by the sample and
aggregate framework  where we considered the samples to be owned by individual parties. Similar
to [10]  we choose a simple average as the aggregation function and the parties together release the
perturbed aggregate classiﬁer which satisﬁes differential privacy. In the multi-party case  however 
adding the perturbation to the classiﬁer is no longer straightforward and it is necessary to provide a
secure protocol to do this.

3 Multiparty Classiﬁcation Protocol

The problem we address is as follows: a number of parties P1  . . .   PK possess data sets D1  . . .   DK
where Di = (x  y)|j includes a set of instances x and their binary labels y. We want to train a logistic
regression classiﬁer on the combined data such that no party is required to expose any of its data 
and the no information about any single data instance can be obtained from the learned classiﬁer.
The protocol can be divided into the three following phases:

3.1 Training Local Classiﬁers on Individual Datasets
Each party Pj uses their data set (x  y)|j to learn an (cid:96)2 regularized logistic regression classiﬁer with
weights ˆwj. This is obtained by minimizing the following objective function

ˆwj = argmin

w

J(w) = argmin

w

1
nj

log

1 + e−yiwT xi

+ λwT w 

(2)

(cid:17)

where λ > 0 is the regularization parameter. Note that no data or information has been shared yet.

3.2 Publishing a Differentially Private Aggregate Classiﬁer

The proposed solution  illustrated by Figure 1  proceeds as follows. The parties then collaborate
j ˆwj + η  where η is a d-dimensional
to compute an aggregate classiﬁer given by ˆws = 1
K
n(1)λ and n(1) =
random variable sampled from a Laplace distribution scaled with the parameter
minj nj. As we shall see later  composing an aggregate classiﬁer in this manner incurs only a well-
bounded excess risk over training a classiﬁer directly on the union of all data while enabling the
parties to maintain their privacy. We also show in Section 4.1 that the noise term η ensures that
the classiﬁer ˆws satisﬁes differential privacy  i.e.  that individual data instances cannot be discerned
from the aggregate classiﬁer. The deﬁnition of the noise term η above may appear unusual at this
stage  but it has an intuitive explanation: A classiﬁer constructed by aggregating locally trained
classiﬁers is limited by the performance of the individual classiﬁer that has the least number of data
instances. This will be formalized in Section 4.2. We note that the parties Pj cannot simply take

2

(cid:88)

i

(cid:16)

(cid:80)

3

classiﬁers  because aggregating such classiﬁers will not give the correct η ∼ Lap(cid:0)2/(n(1)λ)(cid:1) in

their individually trained classiﬁers ˆwj  perturb them with a noise vector and publish the perturbed

general. Since individual parties cannot simply add noise to their classiﬁers to impose differential
privacy  the actual averaging operation must be performed such that the individual parties do not
expose their own classiﬁers or the number of data instances they possess. We therefore use a private
multiparty protocol  interacting with an untrusted curator “Charlie” to perform the averaging. The
outcome of the protocol is such that each of the parties obtain additive shares of the ﬁnal classiﬁer
ˆws  such that these shares must be added to obtain ˆws.

Figure 1: Multiparty protocol to securely compute additive shares of ˆws.

Privacy-Preserving Protocol

We use asymmetric key additively homomorphic encryption [11]. A desirable property of such
schemes is that we can perform operations on the ciphertext elements which map into known op-
erations on the same plaintext elements. For an additively homomorphic encryption function ξ(·) 
ξ(a) ξ(b) = ξ(a + b)  ξ(a)b = ξ(ab). Note that the additively homomorphic scheme employed
here is semantically secure  i.e.  repeated encryption of the same plaintext will result in different
ciphertexts. For the ensuing protocol  encryption keys are considered public and decryption keys are
privately owned by the speciﬁed parties. Assuming the parties to be honest-but-curious  the steps of
the protocol are as follows.

Stage 1. Finding the index of the smallest database obfuscated by permutation.

1. Each party Pj computes nj = aj + bj  where aj and bj are integers representing
additive shares of the database lengths nj for j = 1  2  ...  K. Denote the K-length
vectors of additive shares as a and b respectively.

2. The parties Pj mutually agree on a permutation π1 on the index vector (1  2  ...  K).
This permutation is unknown to Charlie. Then  each party Pj sends its share aj to
party Pπ1(j)  and sends its share bj to Charlie with the index changed according to the
permutation. Thus  after this step  the parties have permuted additive shares given by
π1(a) while Charlie has permuted additive shares π1(b).

3. The parties Pj generate a key pair (pk sk) where pk is a public key for homomorphic
encryption and sk is the secret decryption key known only to the parties but not to
Charlie. Denote element-wise encryption of a by ξ(a). The parties send ξ(π1(a)) =
π1(ξ(a)) to Charlie.
4. Charlie generates a random vector r = (r1  r2 ···   rK) where the elements ri are
integers chosen uniformly at random and are equally likely to be positive or negative.
Then  he computes ξ(π1(aj))ξ(rj) = ξ(π1(aj) + rj). In vector notation  he computes
ξ(π1(a) + r). Similarly  by subtracting the same random integers in the same order
to his own shares  he obtains π1(b) − r where π1 was the permutation unknown to
him and applied by the parties. Then  Charlie selects a permutation π2 at random

4

curatoradditive secret sharing& blind-and-permuteIndicator vector with permuted index of smallest databasecuratorencryption& reverse permutations&add noise vectorsEncrypted Laplaciannoise vectoradded obliviously by smallest databasecuratoradditive secret sharing of noise term& additive secret sharing of local classifersperfectly privateadditive shares ofNullStage 1Stage 2Stage 3and obtains π2(ξ(π1(a) + r)) = ξ(π2(π1(a) + r)) and π2(π1(b) − r). He sends
ξ(π2(π1(a) + r)) to the individual parties in the following order: First element to P1 
second element to P2 ... Kth element to PK.

5. Each party decrypts the signal received from Charlie. At this point  the parties
P1  P2  ...  PK respectively possess the elements of the vector π2(π1(a) + r) while
Charlie possesses the vector π2(π1(b) − r). Since π1 is unknown to Charlie and π2
is unknown to the parties  the indices in both vectors have been complete obfuscated.
Note also that  adding the vector collectively owned by the parties and the vector
owned by Charlie would give π2(π1(a) + r) + π2(π1(b) − r) = π2(π1(a + b)) =
π2(π1(n)). This situation in this step is similar to that encountered in the “blind and
permute” protocol used for minimum-ﬁnding by Du and Atallah [12].
6. Let π2(π1(a) +r) = ˜a and π2(π1(b)−r) = ˜b. Then ni > nj ⇒ ˜ai +˜bi > ˜aj +˜bj ⇒
˜ai − ˜aj > ˜bj − ˜bi. For each (i  j) pair with i  j ∈ {1  2  ...  K}  these comparisons
can be solved by any implementation of a secure millionaire protocol [2]. When all
the comparisons are done  Charlie ﬁnds the index ˜j such that ˜a˜j + ˜b˜j = minj nj. The
true index corresponding to the smallest database has already been obfuscated by the
steps of the protocol. Charlie holds only an additive share of minj nj and thus cannot
know the true length of the smallest database.

Stage 2. Obliviously obtaining encrypted noise vector from the smallest database.

2 (u) where π−1

1. Charlie constructs an K indicator vector u such that u˜j = 1 and all other elements are
0. He then obtains the permuted vector π−1
inverts π2. He generates a
key-pair (pk(cid:48) sk(cid:48)) for additive homomorphic function ζ(·) where only the encryption
key pk(cid:48) is publicly available to the parties Pj. Charlie then transmits ζ(π−1
2 (u)) =
π−1
2 (ζ(u)) to the parties Pj.
2 (ζ(u))) = ζ(v) where π−1
inverts the permutation π1 originally applied by the parties Pj in Stage I. Now that
both permutations have been removed  the index of the non-zero element in the indi-
cator vector v corresponds to the true index of the smallest database. However  since
the parties Pj cannot decrypt ζ(·)  they cannot ﬁnd out this index.

2. The parties mutually obtain a permuted vector π−1

1 (π−1

2

1

3. For j = 1  . . .   K  party Pj generates ηj  a d-dimensional noise vector sampled from
nj λ. Then  it obtains a d-dimensional vector

a Laplace distribution with parameter
ψj where for i = 1  . . .   d  ψj(i) = ζ(v(j))ηj (i) = ζ(v(j) ηj(i)).

4. All parties Pj now compute a d-dimensional noise vector ψ such that  for i = 1  . . .   d 

2

ψ(i) =(cid:81)

j ψj(i) =(cid:81)

j ζ(v(j)ηj(i)) = ζ

j v(j)ηj(i)

(cid:16)(cid:80)

(cid:17)

.

The reader will notice that  by construction  the above equation selects only the
Laplace noise terms for the smallest database  while rejecting the noise terms for all
other databases. This is because v has an element with value 1 at the index correspond-
ing to the smallest database and has zeroes everywhere else. Thus  the decryption of
ψ is equal to η which was the desired perturbation term deﬁned at the beginning of
this section.

Stage 3. Generating secret additive shares of ˆws.

1. One of the parties  say P1  generates a d-dimensional random integer noise vector s 
and transmits ψ(i)ζ(s(i)) for all i = 1  . . .   d to Charlie. Using s effectively prevents
Charlie from discovering η  and therefore still ensures that no information is leaked
about the database owners Pj. P1 computes w1 − Ks.

2. Charlie decrypts ψ(i)ζ(s(i)) to obtain η(i) + s(i) for i = 1  . . .   d. At this stage  the
parties and Charlie have the following d-dimensional vectors: Charlie has K(η + s) 
P1 has ˆw1 − Ks  and all other parties Pj  j = 2  . . .   K have ˆwj. None of the K + 1
participants can share this data for fear of compromising differential privacy.

3. Finally  Charlie and the K database-owning parties run a simple secure function eval-
uation protocol [13]  at the end of which each of the K + 1 participants obtains an
additive share of K ˆws. This protocol is provably private against honest but curious
participants when there are no collisions. The resulting shares are published.

5

The above protocol ensures the following (a) None of the K+1 participants  or users of the perturbed
aggregate classiﬁer can ﬁnd out the size of any database  and therefore none of the parties knows
who contributed η (b) Neither Charlie nor any of the parties Pj can individually remove the noise η
after the additive shares are published. This last property is important because if anyone knowingly
could remove the noise term  then the resulting classiﬁer no longer provides differential privacy.

3.3 Testing Phase
A test participant Dave having a test data instance x(cid:48) ∈ Rd is interested in applying the trained
classiﬁer adds the published shares and divides by K to get the differentially private classiﬁer ˆws.
and decide to classify x(cid:48) with label −1 if
He can then compute the sigmoid function t =
t ≤ 1

2 and with label 1 if t > 1
2.

1+e−ˆwsT xi

1

4 Theoretical Analysis

4.1 Proof of Differential Privacy

We show that the perturbed aggregate classiﬁer satisﬁes differential privacy. We use the follow-
ing bound on the sensitivity of the regularized regression classiﬁer as proved in Corollary 2 in [8]
restated in the appendix as Theorem 6.1.
Theorem 4.1. The classiﬁer ˆws preserves -differential privacy. For any two adjacent datasets D
and D(cid:48) 

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:12)(cid:12)(cid:12)(cid:12) ≤ .

P (ˆws|D)
P (ˆws|D(cid:48))

Proof. Consider the case where one instance of the training dataset D is changed to result in an
adjacent dataset D(cid:48). This would imply a change in one element in the training dataset of one party
and thereby a change in the corresponding learned vector ˆws
j. Assuming that the change is in the
dataset of the party Pj  the change in the learned vector is only going to be in ˆwj; let denote the new
classiﬁer by ˆw(cid:48)
nj λ. Following
an argument similar to [7]  considering that we learn the same vector ˆws using either the training
datasets D and D(cid:48)  we have

j. In Theorem 6.1  we bound the sensitivity of ˆwj as (cid:107) ˆwj − ˆw(cid:48)

j(cid:107)1 ≤ 2

P ( ˆws|D)
P ( ˆws|D(cid:48))

=

P ( ˆwj + η|D)

j + η|D(cid:48)(cid:1) =
P(cid:0) ˆw(cid:48)
(cid:20) n(1)λ
(cid:21)

2

≤ exp

(cid:104) n(1)λ
(cid:104) n(1)λ
(cid:20) n(1)

2

2

(cid:105)
(cid:105) ≤ exp

(cid:107) ˆwj(cid:107)1
(cid:21)
(cid:107) ˆw(cid:48)
j(cid:107)1
≤ exp() 



exp

exp
≤ exp

(cid:20) n(1)λ

2

(cid:21)

(cid:107) ˆwj − ˆw(cid:48)

j(cid:107)1

by the deﬁnition of function sensitivity. Similarly  we can lower bound the the ratio by exp(−).

2

njλ

nj

4.2 Analysis of Excess Error

In the following discussion  we consider how much excess error is introduced when using a per-
turbed aggregate classiﬁer ˆws satisfying differential privacy as opposed to the unperturbed classiﬁer
w∗ trained on the entire training data while ignoring the privacy constraints as well as the unper-
turbed aggregate classiﬁer ˆw.
We ﬁrst establish a bound on the (cid:96)2 norm of the difference between the aggregate classiﬁer ˆw and the
classiﬁer w∗ trained over the entire training data. To prove the bound we apply Lemma 1 from [8]
restated as Lemma 6.2 in the appendix. Please refer to the appendix for the proof of the following
theorem.
Theorem 4.2. Given the aggregate classiﬁer ˆw  the classiﬁer w∗ trained over the entire training
data and n(1) is the size of the smallest training dataset 

(cid:107)ˆw − w∗(cid:107)2 ≤ K − 1

.

n(1)λ

6

The bound is inversely proportional to the number of instances in the smallest dataset. This indicates
that when the datasets are of disparate sizes  ˆw will be a lot different from w∗. The largest possible
K in which case all parties having an equal amount of training data and ˆw will
value for n(1) is n
be closest to w∗. In the one party case for K = 1  the bound indicates that norm of the difference
would be upper bounded by zero  which is a valid sanity check as the aggregate classiﬁer ˆw is the
same as w∗.
We use this result to establish a bound on the empirical risk of the perturbed aggregate classiﬁer
ˆws = ˆw + η over the empirical risk of the unperturbed classiﬁer w∗ in the following theorem.
Please refer to the appendix for the proof.
Theorem 4.3. If all data instances xi lie in a unit ball  with probability at least 1 − δ  the empirical
regularized excess risk of the perturbed aggregate classiﬁer ˆws over the classiﬁer w∗ trained over
entire training data is
J(ˆws) ≤ J(w∗) +

2d(K − 1)(λ + 1)

(K − 1)2(λ + 1)

(cid:18) d

(cid:18) d

(cid:19)

(cid:19)

log2

log

+

+

2n2

(1)λ2

2d2(λ + 1)
n2
(1)2λ2

δ

n2
(1)λ2

.

δ

The bound suggests an error because of two factors: aggregation and perturbation. The bound
increases for smaller values of  implying a tighter deﬁnition of differential privacy  indicating a
clear trade-off between privacy and utility. The bound is also inversely proportional to n2
(1) implying
an increase in excess risk when the parties have training datasets of disparate sizes.
In the limiting case  → ∞  we are adding a perturbation term η sampled from a Laplacian distri-
bution of inﬁnitesimally small variance resulting in the perturbed classiﬁer being almost as same as
using the unperturbed aggregate classiﬁer ˆw satisfying a very loose deﬁnition of differential privacy.
With such a value of   our bound becomes

J( ˆw) ≤ J(w∗) +

(K − 1)2(λ + 1)

2n2

(1)λ2

.

(3)

Similar to the analysis of Theorem 4.2  the excess error in using an aggregate classiﬁer is inversely
proportional to the size of the smallest dataset n(1) and in the one party case K = 1  the bound
becomes zero as the aggregate classiﬁer ˆw is the same as w∗. Also  for a small value of  in the one
party case K = 1 and n(1) = n  our bound reduces to that in Lemma 3 of [8] 

J( ˆws) ≤ J(w∗) +

2d2(λ + 1)

n22λ2

log2

.

(4)

While the previous theorem gives us a bound on the empirical excess risk over a given training
dataset  it is important to consider a bound on the true excess risk of ˆws over w∗. Let us denote the
true risk of the classiﬁer ˆws by ˜J( ˆws) = E[J( ˆws)] and similarly  the true risk of the classiﬁer w∗ by
˜J(w∗) = E[J(w∗)]. In the following theorem  we apply the result from [14] which uses the bound
on the empirical excess risk to form a bound on the true excess risk. Please refer to the appendix for
the proof.
Theorem 4.4. If all training data instances xi lie in a unit ball  with probability at least 1 − δ  the
true excess risk of the perturbed aggregate classiﬁer ˆws over the classiﬁer w∗ trained over entire
training data is

(cid:18) d

(cid:19)

δ

˜J(ˆws) ≤ ˜J(w∗) +

+

5 Experiments

2(K − 1)2(λ + 1)

2n2

(1)λ2

4d(K − 1)(λ + 1)

n2
(1)λ2

(cid:18) d

(cid:19)

δ

32 + log

(cid:18) 1

(cid:19)(cid:21)

δ

.

+

4d2(λ + 1)
n2
(1)2λ2

(cid:18) d

(cid:19)

log2

(cid:20)

+

16
λn

log

δ

We perform an empirical evaluation of the proposed differentially private classiﬁer to obtain a char-
acterization of the increase in the error due to perturbation. We use the Adult dataset from the
UCI machine learning repository [15] consisting of personal information records extracted from

7

Figure 2: Classiﬁer performance evaluated for w∗  w∗ + η  and ˆws for different data splits vs. 

the census database and the task is to predict whether a given person has an annual income over
$50 000. The choice of the dataset is motivated as a realistic example for application of data pri-
vacy techniques. The original Adult data set has six continuous and eight categorical features. We
use pre-processing similar to [16]  the continuous features are discretized into quintiles  and each
quintile is represented by a binary feature. Each categorical feature is converted to as many binary
features as its cardinality. The dataset contains 32 561 training and 16 281 test instances each with
123 features.1 In Figure 2  we compare the test error of perturbed aggregate classiﬁers trained over
data from ﬁve parties for different values of . We consider three situations: all parties with equal
datasets containing 6512 instances (even split  n(1) = 20% of n)  parties with datasets containing
4884  6512  6512  6512  8141 instances (n(1) = 15% of n)  and parties with datasets containing
3256  6512  6512  6512  9769 instances (n(1) = 10% of n). We also compare with the error of
the classiﬁer trained using combined training data and its perturbed version satisfying differential
privacy. We chose the value of the regularization parameter λ = 1 and the results displayed are
averaged over 200 executions.
The perturbed aggregate classiﬁer which is trained using maximum n(1) = 6512 does consistently
better than for lower values of n(1) which is same as our theory suggested. Also  the test error for
all perturbed aggregate classiﬁers drops with   but comparatively faster for even split and converges
to the test error of the classiﬁer trained over the combined data. As expected  the differentially
private classiﬁer trained over the entire training data does much better than the perturbed aggregate
classiﬁers with an error equal to the unperturbed classiﬁer except for small values of . The lower
error of this classiﬁer is at the cost of the loss in privacy of the parties as they would need to share
the data in order to train the classiﬁer over combined data.

6 Conclusion

We proposed a method for composing an aggregate classiﬁer satisfying -differential privacy from
classiﬁers locally trained by multiple untrusting parties. The upper bound on the excess risk of
the perturbed aggregate classifer as compared to the optimal classiﬁer trained over the complete
data without privacy constraints is inversely proportional to the privacy parameter   suggesting an
inherent tradeoff between privacy and utility. The bound is also inversely proportional to the size
of the smallest training dataset  implying the best performance when the datasets are of equal sizes.
Experimental results on the UCI Adult data also show the behavior suggested by the bound and
we observe that the proposed method provides classiﬁcation performance close to the optimal non-
private classiﬁer for appropriate values of . In future work  we seek to generalize the theoretical
analysis of the perturbed aggregate classiﬁer to the setting in which each party has data generated
from a different distribution.

1The dataset can be download from http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html#a9a

8

0.20.250.30.350.40.450.500.050.10.150.20.250.30.350.4test errorεnon-private all dataDP all dataDP aggregate n(1)=6512DP aggregate n(1)=4884DP aggregate n(1)=3256References
[1] Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In IEEE Sympo-

sium on Security and Privacy  pages 173–187  2009.

[2] Andrew Yao. Protocols for secure computations (extended abstract). In IEEE Symposium on

Foundations of Computer Science  1982.

[3] Jaideep Vaidya  Chris Clifton  Murat Kantarcioglu  and A. Scott Patterson. Privacy-preserving

decision trees over vertically partitioned data. TKDD  2(3)  2008.

[4] Jaideep Vaidya  Murat Kantarcioglu  and Chris Clifton. Privacy-preserving naive bayes classi-

ﬁcation. VLDB J  17(4):879–898  2008.

[5] Jaideep Vaidya  Hwanjo Yu  and Xiaoqian Jiang. Privacy-preserving svm classiﬁcation.

Knowledge and Information Systems  14(2):161–178  2008.

[6] Cynthia Dwork. Differential privacy. In International Colloquium on Automata  Languages

and Programming  2006.

[7] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise to sensi-

tivity in private data analysis. In Theory of Cryptography Conference  pages 265–284  2006.

[8] Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Neural

Information Processing Systems  pages 289–296  2008.

[9] Kobbi Nissim  Sofya Raskhodnikova  and Adam Smith. Smooth sensitivity and sampling in

private data analysis. In ACM Symposium on Theory of Computing  pages 75–84  2007.

[10] Adam Smith. Efﬁcient  differentially private point estimators. arXiv:0809.4794v1 [cs.CR] 

2008.

[11] Pascal Paillier. Public-key cryptosystems based on composite degree residuosity classes. In

EUROCRYPT  1999.

[12] Mikhail Atallah and Jiangtao Li. Secure outsourcing of sequence comparisons. International

Journal of Information Security  4(4):277–287  2005.

[13] Michael Ben-Or  Shari Goldwasser  and Avi Widgerson. Completeness theorems for non-
cryptographic fault-tolerant distributed computation. In Proceedings of the ACM Symposium
on the Theory of Computing  pages 1–10  1988.

[14] Karthik Sridharan  Shai Shalev-Shwartz  and Nathan Srebro. Fast rates for regularized objec-

tives. In Neural Information Processing Systems  pages 1545–1552  2008.

[15] A. Frank and A. Asuncion. UCI machine learning repository  2010.
[16] John Platt. Fast training of support vector machines using sequential minimal optimization. In

Advances in Kernel Methods — Support Vector Learning  pages 185–208  1999.

9

,Han Liu
Zhizhong Han
Yu-Shen Liu
Ming Gu