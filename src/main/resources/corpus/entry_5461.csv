2019,The Impact of Regularization on High-dimensional Logistic Regression,Logistic regression is commonly used for modeling dichotomous outcomes. In the classical setting  where the number of observations is much larger than the number of parameters  properties of the maximum likelihood estimator in logistic regression are well understood. Recently  Sur and Candes~\cite{sur2018modern} have studied logistic regression in the high-dimensional regime  where the number of observations and parameters are comparable  and show  among other things  that the maximum likelihood estimator is biased. In the high-dimensional regime the underlying parameter vector is often structured (sparse  block-sparse  finite-alphabet  etc.) and so in this paper we study regularized logistic regression (RLR)  where a convex regularizer that encourages the desired structure is added to the negative of the log-likelihood function. An advantage of RLR is that it allows parameter recovery even for instances where the (unconstrained) maximum likelihood estimate does not exist. We provide a precise analysis of the performance of RLR via the solution of a system of six nonlinear equations  through which any performance metric of interest (mean  mean-squared error  probability of support recovery  etc.) can be explicitly computed. Our results generalize those of Sur and Candes and we provide a detailed study for the cases of $\ell_2^2$-RLR and sparse ($\ell_1$-regularized) logistic regression. In both cases  we obtain explicit expressions for various performance metrics and can find the values of the regularizer parameter that optimizes the desired performance. The theory is validated by extensive numerical simulations across a range of parameter values and problem instances.,The Impact of Regularization on High-dimensional

Logistic Regression

Fariborz Salehi  Ehsan Abbasi  and Babak Hassibi∗

Pasadena  CA  USA.

Department of Electrical Engineering

California Institute of Technology

Abstract

Logistic regression is commonly used for modeling dichotomous outcomes. In
the classical setting  where the number of observations is much larger than the
number of parameters  properties of the maximum likelihood estimator in logistic
regression are well understood. Recently  Sur and Candes [26] have studied logistic
regression in the high-dimensional regime  where the number of observations and
parameters are comparable  and show  among other things  that the maximum
likelihood estimator is biased. In the high-dimensional regime the underlying
parameter vector is often structured (sparse  block-sparse  ﬁnite-alphabet  etc.) and
so in this paper we study regularized logistic regression (RLR)  where a convex
regularizer that encourages the desired structure is added to the negative of the
log-likelihood function. An advantage of RLR is that it allows parameter recovery
even for instances where the (unconstrained) maximum likelihood estimate does
not exist. We provide a precise analysis of the performance of RLR via the solution
of a system of six nonlinear equations  through which any performance metric
of interest (mean  mean-squared error  probability of support recovery  etc.) can
be explicitly computed. Our results generalize those of Sur and Candes and we
provide a detailed study for the cases of (cid:96)2
2-RLR and sparse ((cid:96)1-regularized) logistic
regression. In both cases  we obtain explicit expressions for various performance
metrics and can ﬁnd the values of the regularizer parameter that optimizes the
desired performance. The theory is validated by extensive numerical simulations
across a range of parameter values and problem instances.

Introduction

1
Logistic regression is the most commonly used statistical model for predicting dichotomous out-
comes [11]. It has been extensively employed in many areas of engineering and applied sciences 
such as in the medical [3  32] and social sciences [14]. As an example  in medical studies logistic
regression can be used to predict the risk of developing a certain disease (e.g. diabetes) based on a set
of observed characteristics from the patient (age  gender  weight  etc.)
Linear regression is a very useful tool for predicting a quantitive response. However  in many
situations the response variable is qualitative (or categorical) and linear regression is no longer appro-
priate [12]. This is mainly due to the fact that least-squares often succeeds under the assumption that
the error components are independent with normal distribution. In categorical predictions  however 
the error components are neither inependent nor normally distributed [19].
In logistic regression we model the probability that the label  Y   belongs to a certain category. When
no prior knowledge is available regarding the structure of the parameters  maximum likelihood is
often used for ﬁtting the model. Maximum likelihood estimation (MLE) is a special case of maximum
∗This work was supported in part by the National Science Foundation under grants CNS-0932428  CCF-
1018927  CCF-1423663 and CCF-1409204  by a grant from Qualcomm Inc.  by a grant from Futurewei Inc.  by
NASA’s Jet Propulsion Laboratory through the President and Director’s Fund  and by King Abdullah University
of Science and Technology.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

a posteriori estimation (MAP) that assumes a uniform prior distribution on the parameters.
In many applications in statistics  machine learning  signal processing  etc.  the underlying parameter
obeys some sort of structure (sparse  group-sparse  low-rank  ﬁnite-alphabet  etc.). For instance 
in modern applications where the number of features far exceeds the number of observations  one
typically enforces the solution to contain only a few non-zero entries. To exploit such structural
information  inspired by the Lasso [31] algorithm for linear models  researchers have studied reg-
ularization methods for generalized linear models [24  9]. From a statistical viewpoint  adding a
regularization term provides a MAP estimate with a non-uniform prior distribution that has higher
densities in the set of structured solutions.

1.1 Prior work
Classical results in logistic regression mainly concern the regime where the sample size  n  is over-
whelmingly larger than the feature dimension p. It can be shown that in the limit of large samples
when p is ﬁxed and n → ∞  the maximum likelihood estimator provides an efﬁcient estimate of
the underlying parameter  i.e.  an unbiased estimate with covariance matrix approaching the inverse
of the the Fisher information [34  17]. However  in most modern applications in data science  the
p (cid:29) 1 is not valid. Sur
datasets often have a huge number of features  and therefore  the assumption n
and Candes [5  26  27] have recently studied the performance of the maximum likeliood estimator for
logistic regression in the regime where n is proportional to p. Their ﬁndings challenge the conven-
tional wisdom  as they have shown that in the linear asymptotic regime the maximum likelikehood
estimate is not even unbiased. Their analysis provides the precise performance of the maximum
likelihood estimator.
There have been many studies in the literature on the performance of regularized (penalized) lo-
gistic regression  where a regularizer is added to the negative log-likelihood function (a partial list
includes [4  13  33]). These studies often require the underlying parameter to be heavily structured.
For example  if the parameters are sparse the sparsity is taken to be o(p). Furthermore  they provide
orderwise bounds on the performance but do not give a precise characterization of the quality of the
resulting estimate. A major advantage of adding a regularization term is that it allows for recovery of
the parameter vector even in regimes where the maximum likelihood estimate does not exist (due to
an insufﬁcient number of observations.)

1.2 Summary of contributions
In this paper  we study regularized logistic regression (RLR) for parameter estimation in high-
dimensional logistic models. Inspired by recent advances in the performance analysis of M-estimators
for linear models [7  8  28]  we precisely characterize the assymptotic performance of the RLR esti-
mate. Our characterization is through a system of six nonlinear equations in six unknowns  through
whose solution all locally-Lipschitz performance measures such as the mean  mean-squared error 
probability of support recovery  etc.  can be determined. In the special case when the regularization
term is absent  our 6 nonlinear equations reduce to the 3 nonlinear equations reported in [26]. When
the regularizer is quadratic in parameters  the 6 equations also simpliﬁes to 3. When the regularizer is
the (cid:96)1 norm  which corresponds to the popular sparse logistic regression [15  16]  our equations can
be expressed in terms of q-functions  and quantities such as the probability of correct support recovery
can be explicitly computed. Numerous numerical simulations validate the theoretical ﬁndings across
a range of problem settings. To the extent of our knowledge  this is the ﬁrst work that precisely
characterizes the performance of the regularized logistic regression in high dimensions.
For our analysis  we utilize the recently developed Convex Gaussian Min-max Theorem
(CGMT) [29] which is a strengthened version of a classical Gaussian comparison inequality due to
Gordon [10]  and whose origins are in [25]. Previously  the CGMT has been successfully applied to
derive the precise performance in a number of applications such as regularized M-estimators [28] 
analysis of the generalized lasso [18  29]  data detection in massive MIMO [1  2  30]  and PhaseMax
in phase retrieval [6  23  22].
2 Preliminaries
2.1 Notations
We gather here the basic notations that are used throughout this paper. N (µ  σ2) denotes the normal
distribution with mean µ and variance σ2. X ∼ pX implies that the random variable X has a density
P−→ and d−→ represent convergence in probability and in distribution  respectively. Lower letters
pX.
are reserved for vectors and upper letters are for matrices. 1d  and Id respectively denote the all-one

2

(invariantly) separable if f (w) =(cid:80)p

vector and the identity matrix in dimension d. For a vector v  vi denotes its ith entry  and ||v||p (for
p ≥ 1)  is its (cid:96)p norm  where we remove the subscript when p = 2. A function f : Rp → R is called
˜f (wi) for all w ∈ Rp  where ˜f (·) is a real-valued function.
For a function Φ : Rd → R  the Moreau envelope associated with Φ(·) is deﬁned as 

i=1

and the proximal operator is the solution to this optimization  i.e. 

MΦ(v  t) = min
x∈Rd

||v − x||2 + Φ(x)  

1
2t

ProxtΦ(·)(v) = arg min
x∈Rd

||v − x||2 + Φ(x) .

1
2t

2.2 Mathematical Setup
Assume we have n samples from a logistic model with parameter β∗ ∈ Rp. Let D = {(xi  yi)}n
denote the set of samples (a.k.a. the training data)  where for i = 1  2  . . .   n  xi ∈ Rp is the feature
vector and the label yi ∈ {0  1} is a Bernouli random variable with 

i=1

P[yi = 1|xi] = ρ(cid:48)(xT

i β∗)  

(3)
1+et is the standard logistic function. The goal is to compute an estimate for β∗ from

where ρ(cid:48)(t) := et
the training data D. The maximum likelihood estimator  ˆβM L  is deﬁned as 

for i = 1  2  . . .   n  

(1)

(2)

n(cid:89)

i=1

ˆβM L = arg max
β∈Rp

Pβ(yi|xi) = arg max
β∈Rp

= arg min
β∈Rp

eyi(xT
i β)
1 + exT

i β

ρ(xT

i β) − yi(xT

i β) .

(4)

n(cid:89)
n(cid:88)

i=1

i=1

·(cid:2) n(cid:88)

1
n

Where ρ(t) := log(1 + et) is the link function which has the standard logistic function as its
derivative. The last optimization is simply minimization over the negative log-likelihood. This is a
convex optimization program as the log-likelihood is concave with respect to β.
As explained earlier in Section 1  in many interesting settings the underlying parameter possesses
cerain structure(s) (sparse  low-rank  ﬁnite-alphabet  etc.). In order to exploit this structure we assume
f : Rp → R is a convex function that measures the (so-called) "complexity" of the structured solution.
We ﬁt this model by the regularized maximum (binomial) likelihood deﬁned as follows 

ρ(xT

i β) − yi(xT

i=1

f (β) .

ˆβ = arg min
β∈Rp

f (w) = (cid:80)

(5)
Here  λ ∈ R+ is the regularization parameter that must be tuned properly. In this paper  we study
the linear asymptotic regime in which the problem dimensions p  n grow to inﬁnity at a proportional
p > 0. Our main result characterizes the performance of ˆβ in terms of the ratio  δ  and
rate  δ := n
p . For our analysis we assume that the regularizer f (·) is separable 
the signal strength  κ =
˜f (wi)  and the data points are drawn independently from the Gaussian distribution 
{xi}n
p Ip). Note that the assumptions considered in the analysis of the We further
assume that the entries of β∗ are drawn from a distribution Π. Our main result characterizes the
performance of the resulting estimator through the solution of a system of six nonlinear equations
with six unknowns. In particular  we use the solution to compute some common descriptive statistics
of the estimate  such as the mean and the variance.

i.i.d.∼ N (0  1

||β∗||√

i=1

i

i β)(cid:3) +

λ
p

3 Main Results

In this section  we present the main result of the paper  that is the characterization of the asymptotic
performance of regularized logistic regression (RLR). When the estimation performance is measured
via a locally- Lipschitz function (e.g. mean-squared error)  Theorem 1 precisely predicts the asymp-
totic behavior of the error. The derived expression captures the role of the regularizer  f (·)  and
the particular distribution of β∗  through a set of scalars derived by solving a system of nonlinear
equations. In Section 3.1 we present this system of nonlinear equations along with some insights on
how to numerically compute its solution. After formally stating our result in Section 3.2  we use that
to predict the general behavior of ˆβ. In particular  in Section 3.3 we compute its correlation with the
true signal as well as its mean-squared error.

3

3.1 A nonlinear system of equations
As we will see in Theorem 1  given the signal strength κ  and the ratio δ  the asymptotic performance
of RLR is characterized by the solution to the following system of nonlinear equations with six
unknowns (α  σ  γ  θ  τ  r).



γ =

√
1

r

δ

r√
δ

Z)(cid:1)(cid:3)  

Z)(cid:1)(cid:3)  
κ2α = E(cid:2)β Proxλστ ˜f (·)
(cid:0)στ (θβ +
E(cid:2)Z Proxλστ ˜f (·)
(cid:0)στ (θβ +
Z)(cid:1)2(cid:3)  
κ2α2 + σ2 = E(cid:2)Proxλστ ˜f (·)
(cid:0)στ (θβ +
E(cid:2)ρ(cid:48)(−κZ1)(cid:0)καZ1 + σZ2 − Proxγρ(·)(καZ1 + σZ2)(cid:1)2(cid:3)  
θγ = −2 E(cid:2)ρ(cid:48)(cid:48)(−κZ1)Proxγρ(·)
(cid:0)καZ1 + σZ2
(cid:1)(cid:3)  
1 + γρ(cid:48)(cid:48)(cid:0)Proxγρ(·)(καZ1 + σZ2)(cid:1)(cid:3) .
= E(cid:2)

2ρ(cid:48)(−κZ1)

1 − γ
στ

r√
δ

r√
δ

γ2 =

2
r2

(6)

Here Z  Z1  Z2 are standard normal variables  and β ∼ Π  where Π denotes the distribution on the
entries of β∗. The following remarks provide some insights on solving the nonlinear system.
Remark 1 (Proximal Operators). It is worth noting that the equations in (6) include the expectation
of functionals of two proximal operators. The ﬁrst three equations are in terms of Prox ˜f (·)  which
can be computed explicitly for most widely used regularizers. For instance  in (cid:96)1-regularization  the
proximal operator is the well-known shrinkage function deﬁned as η(x  t) := x|x| (|x| − t)+. The
remaining equations depend on computing the proximal operator of the link function ρ(·). For x ∈ R 
Proxtρ(·)(x) is the unique solution of z + tρ(cid:48)(z) = x.
Remark 2 (Numerical Evaluation). Deﬁne v := [α  σ  γ  θ  τ  r]T as the vector of unknonws. The
nonlinear system (6) can be reformulated as v = S(v) for a properly deﬁned S : R6 → R6. We have
empirically observed in our numerical simulations that a ﬁxed-point iterative method  vt+1 = S(vt) 
converges to v∗  such that v∗ = S(v∗).
3.2 Asymptotic performance of regularized logistic regression
We are now able to present our main result. Theorem 1 below describes the average behavior of
the entries of ˆβ  the solution of the RLR. The derived expression is in terms of the solution of the
nonlinear system (6)  denoted by (¯α  ¯σ  ¯γ  ¯θ  ¯τ   ¯r). An informal statement of our result is that as
n → ∞  the entries of ˆβ converge as follows 
j   Z)  

for j = 1  2  . . .   p  

d→ Γ(β∗

(7)

ˆβj

where Z is a standard normal random variable  and Γ : R2 → R is deﬁned as 

Γ(c  d) := Proxλ¯σ ¯τ ˜f (·)

(cid:0)¯σ¯τ (¯θc +

d)(cid:1) .

¯r√
δ

(8)

(9)

In other words  the RLR solution has the same behavior as applying the proximal operator on the
"perturbed signal"  i.e.  the true signal added with a Gaussian noise.
Theorem 1. Consider the optimization program (5)  where for i = 1  2  . . .   n  xi has the multi-
variate Gaussian distribution N (0  1
i β∗)  and the entries of β∗ are drawn
independently from a distribution Π. Assume the parameters δ  κ  and λ are such that the nonlinear
system (6) has a unique solution (¯α  ¯σ  ¯γ  ¯θ  ¯τ   ¯r). Then  as p → ∞  for any locally-Lipschitz2
function Ψ : R × R → R   we have 

p Ip)  and yi = Ber(xT

p(cid:88)

j=1

1
p

P−→ E(cid:2)Ψ(cid:0)Γ(β  Z)  β(cid:1)(cid:3)  

Ψ( ˆβj  β∗
j )

where Z ∼ N (0  1)  β ∼ Π is independent of Z  and the function Γ(· ·) is deﬁned in (8).

2A function Φ : Rd → R is said to be locally-Lipschitz if 

∀M > 0  ∃LM ≥ 0  such that ∀x  y ∈(cid:2) − M  +M(cid:3)d :

|Φ(x) − Φ(y)| ≤ LM||x − y|| .

4

We defer the detailed proof to the Appendix. In short  to show this result we ﬁrst represnt the
optimization as a bilinear form uT Xv  where X is the measurement matrix. Applying the CGMT
to derive an equivalent optimization  we then simplify this optimization to obtain an unconstrained
optimization with six scalar variables. The nonlinear system (6) represents the ﬁrst-order optimality
condition of the resulting scalar optimization.
Before stating the consequences of this result  a few remarks are in order.
Remark 3 (Assumptions). The assumptions in Theorem 1 are chosen in a conservative manner. In
particular  we could relax the separability condition on f (·)  to some milder condition in terms of
asymptotic convergence of its proximal operator. Furthermore  one can relax the assumption on the
entries of β∗ being i.i.d. to a weaker assumption on the empirical distribution of its entries. However 
for the applications of this paper  the theorem in its current form is adequate.
Remark 4 (Choosing Ψ). The performance measure in Theorem 1 is computed in terms of evaluation
of a locally-Lipschitz function  Ψ(· ·) . As an example  Ψ(u  v) = (u − v)2 can be used to compute
the mean-squared error. Later on  we will appeal to this theorem with various choices of Ψ to evaluate
different performance measures on ˆβ.
3.3 Correlation and variance of the RLR estimate
As the ﬁrst application of Theorem 1 we compute common descriptive statistics of the estimate ˆβ. In
the following corollaries  we establish that the parametrs ¯α  and ¯σ in (6) correspond to the correlation
and the mean-squared error of the resulting estimate.
Corollary 1. As p → ∞ 
Proof. Recall that ||β∗||2 = pκ2. Applying Theorem 1 with Ψ(u  v) = uv gives 

ˆβT β∗ P−→ ¯α .

1||β∗||2

E(cid:2)β Proxλ¯σ ¯τ ˜f (·)

(cid:0)¯σ¯τ (¯θβ +

Z)(cid:1)(cid:3) = ¯α  

¯r√
δ

(10)

1

||β∗||2

ˆβT β∗ =

1
κ2p

ˆβjβ∗

j

P−→ 1
κ2

p(cid:88)

j=1

ˆβ
¯α and compute its mean-squared error in the following corollary.

where the last equality is derived from the ﬁrst equation in the nonlinear system (6)  along with the
fact that (¯α  ¯σ  ¯γ  ¯θ  ¯τ   ¯r) is a solution to this system.
Corollary 1 states that upon centering ˆβ around ¯αβ∗  it becomes decorrelated from β∗. Therefore 
we deﬁne a new estimate ˜β :=
Corollary 2. As p → ∞  1
Proof. We appeal to Theorem 1 with Ψ(u  v) = (u − ¯αv)2 
1
p

¯σ2
¯α2  
(11)
where the last equality is derived from the third equation in the nonlinear system (6) together with the
result of Corollary 1.

|| ˆβ − ¯αβ∗||2(cid:1) P−→ 1

E(cid:2)(cid:0)Proxλ¯σ ¯τ ˜f (·)

Z)(cid:1) − ¯αβ(cid:1)2(cid:3) =

p|| ˜β − β∗||2 P−→ ¯σ2
¯α2 .

(cid:0)¯σ¯τ (¯θβ +

|| ˜β − β∗||2 =

(cid:0) 1

p

¯r√
δ

1
¯α2

¯α2

In the next two sections  we investigate other properties of the estimate ˆβ under (cid:96)1 and (cid:96)2 regulariza-
tion.
4 RLR with (cid:96)2
The (cid:96)2 norm regularization is commonly used in machine learning applications to stabilize the model.
Adding this regularization would simply shrink all the parameters toward the origin and hence
decrease the variance of the resulting model. Here  we provide a precise performance analysis of the
RLR with (cid:96)2

2-regularization

2-regularization  i.e. 

ˆβ = arg min
β∈Rp

1
n

ρ(xT

i β) − yi(xT

β2
i .

(12)

·(cid:2) n(cid:88)

i=1

i β)(cid:3) +

p(cid:88)

i=1

λ
2p

To analyze (12)  we use the result of Theorem 1. It can be shown that in the nonlinear system (6)  ¯θ 
¯τ  ¯r can be derived explicitely from solving the ﬁrst three equations. This is due to the fact that the
proximal operator of ˜f (·) = 1

2 (·)2 can be expressed in the following closed-form 

Proxt ˜f (·)(x) = arg min
y∈R

(y − x)2 +

1
2t

1
2

y2 =

x

1 + t

.

(13)

5

(a)

(b)

(c)

Figure 1: The performance of the regularized logistic regression under (cid:96)2
2 penalty (a) the correlation factor ¯α
p|| ˆβ − β∗||2. The dashed lines depict the theoretical result
(b) the variance ¯σ2  and (c) the mean-squared error 1
derived from Theorem 2  and the dots are the result of empirical simulations. The empirical results is the average
over 100 independent trials with p = 250 and κ = 1 .

This indicates that the proximal operator in this case is just a simple rescaling. Substituting (13) in
the nonlinear system (6)  we can rewrite the ﬁrst three equations as follows 



θ =

τ =

α
γδ

 

σ(cid:0)1 − λδγ(cid:1)  

δγ

r =

γ

√
σ

.

δ

(14)

Therefore we can state the following Theorem for (cid:96)2
Theorem 2. Consider the optimization (12) with parameters κ  δ  and γ  and the same assumptions
as in Theorem 1. As p → ∞  for any locally-Lipschitz function Ψ(· ·)  the following convergence
holds 

2-regularization:

p(cid:88)

j=1

1
p

P−→ E(cid:2)Ψ(cid:0)¯σZ  β(cid:1)(cid:3)  

Ψ( ˆβj − ¯αβ∗

j   β∗
j )

where Z is standard norma  β ∼ Π  and ¯α ¯σ are the unique solution to the following nonlinear
system of equations 



= E(cid:2)ρ(cid:48)(−κZ1)(cid:0)καZ1 + σZ2 − Proxγρ(·)(καZ1 + σZ2)(cid:1)2(cid:3)  
= E(cid:2)ρ(cid:48)(cid:48)(−κZ1)Proxγρ(·)

(cid:0)καZ1 + σZ2
(cid:1)(cid:3)  
1 + γρ(cid:48)(cid:48)(cid:0)Proxγρ(·)(καZ1 + σZ2)(cid:1)(cid:3) .

2ρ(cid:48)(−κZ1)

σ2
2δ
− α
2δ

+ λγ = E(cid:2)

1 − 1
δ

(15)

(16)

6

The proof is deferred to the Appendix. Theorem 2 states that upon centering the estimate ˆβ  it
becomes decorrelated from β∗ and the distribution of the entries approach a zero-mean Gaussian
distribution with variance ¯σ2.
Figure 1 depicts the performance of the regularized estimate for different values of λ. As observed
in the ﬁgure  increasing the value of λ reduces the correlation factor ¯α (Figure 1a) and the variance
¯σ2 (Figure 1b). Figure 1c shows the mean-squared-error of the estimate as a function of λ . It
indicates that for different values of δ there exist an optimal value λopt that achieves the minimum
mean-squared error.

4.1 Unstructured case
When λ = 0 in (12)  we obtain the optimization with no regularization  i.e.  the maximum likelihood
estimate. When we set λ to zero in (16)  Theorem 2 gives the same result as Sur and Candes reported
in [26]. In their analysis  they have also provided an interesting interpretation of ¯γ in terms of the
likelihood ratio statistics. Studying the likelihood ratio test is beyond the scope of this paper.
5 Sparse Logistic Regression
In this section we study the performance of our estimate when the regularizer is the (cid:96)1 norm. In
modern machine learning applications the number of features  p  is often overwhelmingly large.
Therefore  to avoid overﬁtting one typically needs to perform feature selection  that is  to exclude
irrelevent variables from the regression model [12]. Adding an (cid:96)1 penalty to the loss function is the
most popular approach for feature selection.
As a natural consequence of the result of Theorem 1  we study the performance of RLR with (cid:96)1
regularizer (referred to as "sparse LR") and evaluate its success in recovery of the sparse signals. In
Section 5.1  we extend our general analysis to the case of sparse LR. In other words  we will precisely
analyze the performance of the solution of the following optimization 

ˆβ = arg min
β∈Rp

1
n

ρ(xT

i β) − yi(xT

||β||1 .

λ
p

(17)

·(cid:2) n(cid:88)

i=1

i β)(cid:3) +

In Section 5.1  we explicitly describe the expectations in the nonlinear system (6) using two q-
functions3. In Section 5.2  we analyze the support recovery in the resulting estimate and show that
the two q-functions represent the probability of on and off support recovery.

5.1 Convergence behavior of sparse LR
For our analysis in this section  we assume each entry β∗
distribution 

Π(β) = (1 − s) · δ0(β) + s ·(cid:0) φ( β

(cid:1) 

)

κ√
s
κ√
s

i   for i = 1  . . .   p  is sampled i.i.d. from a

(18)

where s ∈ (0  1) is the sparsity factor  φ(t) := e−t2/2√
is the density of the standard normal
distribution  and δ0(·) is the Dirac delta function.
In other words  entries of β∗ are zero with
probability 1 − s  and the non-zero entries have a Gaussian distribution with appropriately deﬁned
variance. Although our analysis can be extended further  here we only present the result for a
Gaussian distribution on the non-zero entries. The proximal operator of ˜f (·) = | · | is the soft-
thresholding operator deﬁned as  η(x  t) = x|x| (x− t)+ . Therefore  we are able to explicitly compute
the expectations with respect to ˜f (·) in the nonlinear system (6). To streamline the representation  we
deﬁne the following two proxies 

2π

λ(cid:113) r2

δ + θ2κ2

s

t1 =

  t2 =

λ
r√
δ

.

(19)

In the next section  we provide an interpretation for t1 and t2. In particular  we will show that Q( ¯t1) 
and Q( ¯t2) are related to the probabilities of on and off support recovery. We can rewrite the ﬁrst three

3The q-function is the tail distribution of the standard normal r.v. deﬁned as  Q(t) :=(cid:82) ∞

e−x2 /2√

2π

dx .

t

7

(a)

(b)

(c)

Figure 2: The performance of the regularized logistic regression under (cid:96)1 penalty (a) the correlation factor ¯α
p|| ˆβ − β∗||2. The dashed lines are the theoretical result
(b) the variance ¯σ2  and (c) the mean-squared error 1
derived from Theorem 1  and the dots are the result of empirical simulations. For the numerical simulations  the
result is the average over 100 independent trials with p = 250 and κ = 1 .



equations in (6) as follows 

α
2στ
δγ
2στ
κ2α2 + σ2

= θ · Q(t1)  

= s · Q(cid:0)t1

(cid:1) + (1 − s) · Q(cid:0)t2
(cid:1)  
(cid:1) − λ2(s · φ(t1)
+ κ2θ2 · Q(cid:0)t1

2σ2τ 2 =

δγλ2
2στ

+

γr2
2στ

+ (1 − s) · φ(t2)
t2

t1

(20)

) .

Appending the three equations in (20) to the last three equations in (6) gives the nonlinear system for
sparse LR. Upon solving these equations  we can use the result of Theorem 1 to compute various
performance measure on the estimate ˆβ. Figure 2 shows the performance of our estimate as a
function of λ. It can be seen that the bound derived from our theoretical result matches the empirical
simulations. Also  it can be inferrred from Figure 2c that the optimal value of λ (λopt that achieves
the minimum mean-squared error) is a decreasing function of δ.
5.2 Support recovery
In this section  we study the support recovery in sparse LR. As mentioned earlier  sparse LR is
often used when the underlying paramter has few non-zero entries. We deﬁne the support of β∗ as
Ω := {j|1 ≤ j ≤ p  β∗
j (cid:54)= 0}. Here  we would like to compute the probability of success in recovery
of the support of β∗.
Let ˆβ denote the solution of the optimization (17). We ﬁx the value  > 0 as a hard-threshold based
on which we decide whether an entry is on the support or not. In other words  we form the following
set as our estimate of the support given ˆβ 

ˆΩ = {j|1 ≤ j ≤ p | ˆβj| > }

In order to evaluate the success in support recovery  we deﬁne the following two error measures 

E1() = Prob{j ∈ ˆΩ|j (cid:54)∈ Ω}   E2() = Prob{j (cid:54)∈ ˆΩ|j ∈ Ω} .

(21)

(22)

8

(a)

(b)

Figure 3: The support recovery in the regularized logistic regression with (cid:96)1 penalty for (a) E1: the probability
of false detection  (b) E2: the probability of missing an entry of the support. The dashed lines are the theoretical
results derived from Lemma 1  and the dots are the result of empirical simulations. For the numerical simulations 
the result is the average over 100 independent trials with p = 250 and κ = 1 and  = 0.001 .

In our estimation  E1 represents the probability of false alarm  and E2 is the probability of misdetec-
tion of an entry of the support. The following lemma indicates the asymptotic behavior of both errors
as  approcahes zero .
Lemma 1 (Support Recovery). Let ˆβ be the solution to the optimization (17)  and the entries of β∗
have distribution Π deﬁned in (18). Assume λ is chosen such that the nonlinear system (6) has a
unique solution (¯α  ¯σ  ¯γ  ¯θ  ¯τ   ¯r). As p → ∞ we have 

p−→ 2 Q(cid:0)¯t1
p−→ 1 − 2 Q(cid:0)¯t2

(cid:1) where  ¯t1 =
(cid:1) where  ¯t2 =

lim
↓0

E1()

lim
↓0

E2()

  and 

λ
¯r√
δ

λ(cid:113)

¯r2

δ + ¯θ2κ2

s

(23)

.

6 Conclusion and Future Directions

In this paper  we analyzed the performance of the regularized logistic regression (RLR)  which is
often used for parameter estimation in binary classiﬁcation. We considered the setting where the
underlying parameter has certain structure (e.g. sparse  group-sparse  low-rank  etc.) that can be
enforced via a convex penalty function f (·). We precisely characterized the performance of the
regularized maximum likelihood estimator via the solution to a nonlinear system of equations. Our
main results can be used to measure the performance of RLR for a general convex penalty function
f (·). In particular  we apply our ﬁndings to two important special cases  i.e.  (cid:96)2
2-RLR and (cid:96)1-RLR.
When the regularizer is quadratic in parameters  we have shown that the nonlinear system can be
simpliﬁed to three equations. When the regularization parameter  λ  is set to zero  which corresponds
to the maximum likelihood estimator  we simply derived the results reported by Sur and Candes [26].
For sparse logistic regression  we established that the nonlinear system can be represented using
two q-functions. We further show that these two q-functions represent the probability of the support
recovery.
For our analysis  we assumed the datapoints are drawn independently from a gaussian distribution
and utilized the CGMT framework. An interesting future work is to extend our analysis to non-
gaussian distributions. To this end  we can exploit the techniques that have been used to establish
the universality law (see [20  21] and the references therein). As mentioned earlier in Section 1  an
advantage of RLR is that it allows parameter recovery even for instances where the (unconstrained)
maximum likelihood estimate does not exist. Therefore  another interesting future direction is to
analyze the conditions on λ (as a function of δ and κ) that guarantees the existence of the solution
to the RLR optimization (5). In the unstructured setting  this has been studied in a recent work by
Candes and Sur [5].

9

References
[1] Ehsan Abbasi  Fariborz Salehi  and Babak Hassibi. Performance analysis of convex data
detection in mimo. In ICASSP 2019-2019 IEEE International Conference on Acoustics  Speech
and Signal Processing (ICASSP)  pages 4554–4558. IEEE  2019.

[2] Ismail Ben Atitallah  Christos Thrampoulidis  Abla Kammoun  Tareq Y Al-Naffouri  Babak
Hassibi  and Mohamed-Slim Alouini. Ber analysis of regularized least squares for bpsk recovery.
In 2017 IEEE International Conference on Acoustics  Speech and Signal Processing (ICASSP) 
pages 4262–4266. IEEE  2017.

[3] Carl R Boyd  Mary Ann Tolson  and Wayne S Copes. Evaluating trauma care: the triss method.

trauma score and the injury severity score. The Journal of trauma  27(4):370–378  1987.

[4] Florentina Bunea et al. Honest variable selection in linear and logistic regression models via 1

and 1+ 2 penalization. Electronic Journal of Statistics  2:1153–1194  2008.

[5] Emmanuel J Candès and Pragya Sur. The phase transition for the existence of the maximum
likelihood estimate in high-dimensional logistic regression. arXiv preprint arXiv:1804.09753 
2018.

[6] Oussama Dhifallah  Christos Thrampoulidis  and Yue M Lu. Phase retrieval via polytope opti-
mization: Geometry  phase transitions  and new algorithms. arXiv preprint arXiv:1805.09555 
2018.

[7] David Donoho and Andrea Montanari. High dimensional robust m-estimation: Asymptotic
variance via approximate message passing. Probability Theory and Related Fields  166(3-
4):935–969  2016.

[8] Noureddine El Karoui  Derek Bean  Peter J Bickel  Chinghway Lim  and Bin Yu. On robust
regression with high-dimensional predictors. Proceedings of the National Academy of Sciences 
110(36):14557–14562  2013.

[9] Jerome Friedman  Trevor Hastie  and Rob Tibshirani. Regularization paths for generalized

linear models via coordinate descent. Journal of statistical software  33(1):1  2010.

[10] Yehoram Gordon. Some inequalities for gaussian processes and applications. Israel Journal of

Mathematics  50(4):265–289  1985.

[11] David W Hosmer Jr  Stanley Lemeshow  and Rodney X Sturdivant. Applied logistic regression 

volume 398. John Wiley & Sons  2013.

[12] Gareth James  Daniela Witten  Trevor Hastie  and Robert Tibshirani. An introduction to

statistical learning  volume 112. Springer  2013.

[13] Sham Kakade  Ohad Shamir  Karthik Sindharan  and Ambuj Tewari. Learning exponential
families in high-dimensions: Strong convexity and sparsity. In Proceedings of the thirteenth
international conference on artiﬁcial intelligence and statistics  pages 381–388  2010.

[14] Gary King and Langche Zeng. Logistic regression in rare events data. Political analysis 

9(2):137–163  2001.

[15] Kwangmoo Koh  Seung-Jean Kim  and Stephen Boyd. An interior-point method for large-scale
l1-regularized logistic regression. Journal of Machine learning research  8(Jul):1519–1555 
2007.

[16] Balaji Krishnapuram  Lawrence Carin  Mario AT Figueiredo  and Alexander J Hartemink.
IEEE

Sparse multinomial logistic regression: Fast algorithms and generalization bounds.
transactions on pattern analysis and machine intelligence  27(6):957–968  2005.

[17] Erich L Lehmann and Joseph P Romano. Testing statistical hypotheses. Springer Science &

Business Media  2006.

[18] Léo Miolane and Andrea Montanari. The distribution of the lasso: Uniform control over sparse

balls and adaptive parameter tuning. arXiv preprint arXiv:1811.01212  2018.

10

[19] John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the

Royal Statistical Society: Series A (General)  135(3):370–384  1972.

[20] Samet Oymak and Joel A Tropp. Universality laws for randomized dimension reduction  with

applications. Information and Inference: A Journal of the IMA  7(3):337–446  2017.

[21] Ashkan Panahi and Babak Hassibi. A universal analysis of large-scale regularized least squares

solutions. In Advances in Neural Information Processing Systems  pages 3381–3390  2017.

[22] Fariborz Salehi  Ehsan Abbasi  and Babak Hassibi. Learning without the phase: Regularized
phasemax achieves optimal sample complexity. In Advances in Neural Information Processing
Systems  pages 8641–8652  2018.

[23] Fariborz Salehi  Ehsan Abbasi  and Babak Hassibi. A precise analysis of phasemax in phase
retrieval. In 2018 IEEE International Symposium on Information Theory (ISIT)  pages 976–980.
IEEE  2018.

[24] Shirish Krishnaj Shevade and S Sathiya Keerthi. A simple and efﬁcient algorithm for gene

selection using sparse logistic regression. Bioinformatics  19(17):2246–2253  2003.

[25] Mihailo Stojnic. A framework to characterize performance of lasso algorithms. arXiv preprint

arXiv:1303.7291  2013.

[26] Pragya Sur and Emmanuel J Candès. A modern maximum-likelihood theory for high-

dimensional logistic regression. arXiv preprint arXiv:1803.06964  2018.

[27] Pragya Sur  Yuxin Chen  and Emmanuel J Candès. The likelihood ratio test in high-dimensional
logistic regression is asymptotically a rescaled chi-square. Probability Theory and Related
Fields  pages 1–72  2017.

[28] Christos Thrampoulidis  Ehsan Abbasi  and Babak Hassibi. Precise error analysis of regularized
m-estimators in high dimensions. IEEE Transactions on Information Theory  64(8):5592–5628 
2018.

[29] Christos Thrampoulidis  Samet Oymak  and Babak Hassibi. Regularized linear regression: A
precise analysis of the estimation error. In Conference on Learning Theory  pages 1683–1709 
2015.

[30] Christos Thrampoulidis  Ilias Zadik  and Yury Polyanskiy. A simple bound on the ber of the

map decoder for massive mimo systems. arXiv preprint arXiv:1903.03949  2019.

[31] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological)  pages 267–288  1996.

[32] Jack V Tu. Advantages and disadvantages of using artiﬁcial neural networks versus logistic
regression for predicting medical outcomes. Journal of clinical epidemiology  49(11):1225–
1231  1996.

[33] Sara A Van de Geer et al. High-dimensional generalized linear models and the lasso. The

Annals of Statistics  36(2):614–645  2008.

[34] Aad W Van der Vaart. Asymptotic statistics  volume 3. Cambridge university press  2000.

11

,Fariborz Salehi
Ehsan Abbasi
Babak Hassibi