2019,Re-examination of the Role of Latent Variables in Sequence Modeling,With latent variables  stochastic recurrent models have achieved state-of-the-art performance in modeling sound-wave sequence.
However  opposite results are also observed in other domains  where standard recurrent networks often outperform stochastic models.
To better understand this discrepancy  we re-examine the roles of latent variables in stochastic recurrent models for speech density estimation.
Our analysis reveals that under the restriction of fully factorized output distribution in previous evaluations  the stochastic variants were implicitly leveraging intra-step correlation but the deterministic recurrent baselines were prohibited to do so  resulting in an unfair comparison.
To correct the unfairness  we remove such restriction in our re-examination  where all the models can explicitly leverage intra-step correlation with an auto-regressive structure.
Over a diverse set of univariate and multivariate sequential data  including human speech  MIDI music  handwriting trajectory  and frame-permuted speech  our results show that stochastic recurrent models fail to deliver the performance advantage claimed in previous work. 
%exhibit any practical advantage despite the claimed theoretical superiority. 
In contrast  standard recurrent models equipped with an auto-regressive output distribution consistently perform better  dramatically advancing the state-of-the-art results on three speech datasets.,Re-examination of the Role of Latent Variables in

Sequence Modeling

Guokun Lai⇤1  Zihang Dai⇤1  Yiming Yang1  Shinjae Yoo2
1Carnegie Mellon University  2Brookhaven National Laboratory
1{guokun dzihang yiming}@cs.cmu.edu  2sjyoo@bnl.gov

Abstract

With latent variables  stochastic recurrent models have achieved state-of-the-art
performance in modeling sound-wave sequence. However  opposite results are also
observed in other domains  where standard recurrent networks often outperform
stochastic models. To better understand this discrepancy  we re-examine the roles
of latent variables in stochastic recurrent models for speech density estimation.
Our analysis reveals that under the restriction of fully factorized output distribution
in previous evaluations  the stochastic variants were implicitly leveraging intra-
step correlation but the deterministic recurrent baselines were prohibited to do
so  resulting in an unfair comparison. To correct the unfairness  we remove such
restriction in our re-examination  where all the models can explicitly leverage
intra-step correlation with an auto-regressive structure. Over a diverse set of
univariate and multivariate sequential data  including human speech  MIDI music 
handwriting trajectory and frame-permuted speech  our results show that stochastic
recurrent models fail to deliver the performance advantage claimed in previous
work. In contrast  standard recurrent models equipped with an auto-regressive
output distribution consistently perform better  dramatically advancing the state-of-
the-art results on three speech datasets.

1 Introduction

As a fundamental problem in machine learning  probabilistic sequence modeling aims at capturing
the sequential correlations in both short and long ranges. Among many possible model choices  deep
auto-regressive models [1  2] have become one of the most widely adopted solutions. Typically 
a deep auto-regressive model factorizes the likelihood function of sequences in an auto-regressive

manner  i.e.  p(x) =Q|x|t=1 p(xt | x<t). Then  a neural network (e.g. RNN) is employed to encode
the conditional context x<t into a compact hidden representation ht = f (x<t)  which is then used to
deﬁne the output distribution p(xt | x<t)   p(xt | ht).
Despite the state-of-the-art (SOTA) performance in many domains [3  4  5  6]  the hidden representa-
tions of standard auto-regressive models are produced in a completely deterministic way. Hence  the
stochastic aspects of the observed sequences can only be modeled by the output distribution  which
however  usually has a simple parametric form such as a unimodal distribution or a ﬁnite mixture of
unimodal distributions. A potential weakness of such simple forms is that they may not be sufﬁciently
expressive for modeling real-world sequential data with complex stochastic dynamics.
Recently  many efforts have been made to enrich the expressive power of auto-regressive models
by injecting stochastic latent variables into the computation of hidden states. Notably  relying
on the variational auto-encoding (VAE) framework [7  8]  stochastic recurrent models (SRNN)
have outperformed standard RNN-based auto-regressive models by a large margin in modeling raw
sound-wave sequences [9  10  11  12  13  14].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

However  the success of stochastic latent variables does not necessarily generalize to other domains
such as text and images. For instance  the authors [12] report that an SRNN trained by Z-Forcing
lags behind a baseline RNN in language modeling. Similarly  for the density estimation of natural
images  PixelCNN [15  16  17] consistently outperforms generative models with latent variables [18 
19  20  21  22].
To better understand the discrepancy  we perform a re-examination on the role of stochastic variables
in SRNN models. By carefully inspecting of the previous experiment settings for sound-wave density
estimation  and systematically analyzing the properties of SRNN  we identify two potential causes
of the performance gap between SRNN and RNN. Controlled experiments are designed to test
each hypothesis  where we ﬁnd that previous evaluations impose an unnecessary restriction of fully
factorized output distributions  which has led to an unfair comparison between SRNN and RNN.
Speciﬁcally  under the factorized parameterization  SRNN can still implicitly leverage the intra-step
correlation  i.e.  the simultaneity [23]  while the RNN baselines are prohibited to do so. Meanwhile 
we also observe that the posterior learned by SRNN can get outperformed by a simple hand-crafted
posterior  raising serious doubt about the general effectiveness of injecting latent variables.
To provide a fair comparison  we propose an evaluation setting where both the SRNN and RNN can
utilize an auto-regressive output distribution to model the intra-step correlation explicitly. Under the
new setting  we re-evaluate SRNN and RNN on a diverse collection of sequential data  including
human speech  MIDI music  handwriting trajectory and frame-permuted speech. Empirically  we ﬁnd
that sequential models with continuous latent variables fail to offer any practical beneﬁts  despite their
widely believed theoretical superiority. On the contrary  explicitly capturing the intra-step correlation
with an auto-regressive output distribution consistently performs better  substantially improving the
SOTA performances in modeling speech signals. Overall  these observations show that the previously
reported performance “advantage” of SRNN is merely the result of a long-existing experiment bias
of using factorized output distributions.

2 Background

In this section  we brieﬂy review SRNN and RNN for probabilistic sequence modeling. The other
approaches are summarized in appendix E. Throughout the paper  we will use bold font x to denote a
sequence  x<t and xt to indicate the sub-sequence of ﬁrst t  1 and t elements respectively  and xt
to represent the t-th element. Note that xt can either be a scalar or a multivariate vector. In the latter
case  xt i denotes the i-th element of the vector xt.
Given a set of sequences D =x1  x2 ···   x|D|   we are interested in building a density estimation

model for sequences. A widely adapted solution is to employ an auto-regressive model powered by a
neural network  and utilize MLE to perform the training:

✓ LD = E
max

x⇠D" TxXt=1

log p✓(xt | x<t)# 

(1)

ht;

Categorical or Gaussian Mixture;

where Tx is the length of the sequence x. More concretely  the conditional distribution p✓(xt | x<t)
is usually jointly modeled by two sub-modules:
• The pre-deﬁned distribution family of the output distribution p✓(xt | x<t)  such as a Gaussian 
• The sequence model f✓  which encodes the contextual sequence x<t into a compact hidden vector
Under this general framework  RNN and SRNN can be seen as two different instantiations of
the sequence model. As we have discussed in Section 1  the computation inside RNN is fully
deterministic.
To improve the model expressiveness  SRNN takes an alternative route and incorporates continuous
latent variables into the sequence model. Typically  SRNN associates the observed data sequence
x with a sequence of latent variables z = [z1  . . .   zTx]  one for each step. With latent variables 
the internal dynamics of the sequence model is not deterministic any more  offering a theoretical
possibility to capture more complex stochastic patterns. However  the improved capacity comes with

2

a computational burden — the log-likelihood is generally intractable due to the integral:

LSRNN
D

= E

x⇠DlogZ p✓(x  z)dz.

Hence  standard MLE training cannot be performed.
To handle the intractability  SRNN utilizes the VAE framework and maximizes the evidence lower
bound (ELBO) of the log-likelihood (1) for training:

✓  FD = E
max

x⇠D" E

q(z|x) TxXt=1

log

p✓(xt | zt  x<t)p✓(zt|z<t  x<t)

q(zt | z<t  x)

!# L SRNN

D

 

(2)

where q(z | x) is the approximate posterior distribution modeled by an encoder network. Compu-
tationally  several SRNN variants have been proposed [9  10  11  12]  mostly differing in how the
generative distribution p✓(x  z) and the variational posterior q(z | x) are parameterized. In this
work  we follow the parameterization and optimization in Z-forcing SRNN method [12]  which is
the one with the best performance. We include a detailed introduction for related SRNN models in
appendix B.

3 Revisiting SRNN for Speech Modeling

3.1 Previous Setting for Speech Density Estimation

To compare SRNN and RNN  previous studies largely rely on the density estimation of sound-wave
sequences. Usually  a sound-wave dataset consists of a collection of audio sequences with a sample
rate of 16Hz  where each frame (element) of the sequence is a scalar in [1  1]  representing the
normalized amplitude of the sound. Instead of treating each frame as a single step  the authors
[10] propose a multi-frame setting  where every 200 consecutive frames are taken as a single step.
Effectively  the data can be viewed as a sequence of 200-dimensional real-valued vectors  i.e. 
xt 2 RL with L = 200. During training  every T = 40 steps (8 000 frames) are taken as an i.i.d.
sequence to form the training set.
Under this data format  notice that the output distributions p✓(xt | x<t) and p✓(xt | zt  x<t) now
correspond to an L-dimensional random vector xt. Therefore  how to parameterize this multivariate
distribution can largely inﬂuence empirical performance. That said  recent approaches [11  12] have
all followed [10] to employ a fully factorized parametric form which ignores the inner dependency:

p✓(xt | x) ⇡

p✓(xt | zt  x<t) ⇡

LYi=1
LYi=1

p✓(xt i | x<t) 

p✓(xt i | zt  x<t).

(3)

(4)

Here  we have used the ⇡ to emphasize this choice effectively poses an independent assumption.
Despite this convenience  note that the restriction of a fully factorized form is not necessary at all.
Nevertheless  we will refer to the models in Eqn. (3) and Eqn. (4)  respectively  as factorized RNN
(F-RNN) and factorized SRNN (F-SRNN) in the following.
To provide a baseline for further discussion  we replicate the experiments under the setting introduced
above and evaluate them on three speech datasets  namely TIMIT  VCTK  and Blizzard. Following the
previous work [10]  we choose a Gaussian mixture to model the per-frame distribution p✓(xt i | x<t)
of F-RNN  which enables a basic multi-modality.
We report the averaged test log-likelihood in Table 1. For consistency with previous results in the
literature  the results of TIMIT and Blizzard are based on sequence-level average  while the result of
VCTK is frame-level average. As we can see  similar to previous observations  F-SRNN outperforms
F-RNN on all three datasets by a dramatic margin.

3

TIMIT VCTK Blizzard
Models
7 610
F-RNN
32 745
F-SRNN 69 296
15 258

0.786
2.383

Table 1: Performance comparison on three benchmark datasets.

3.2 Decomposing the Advantages of Factorized SRNN

To understand why the F-SRNN outperforms F-RNN by such a large margin  it is helpful to examine
the effective output distribution p✓(xt | x<t) of F-SRNN after marginalizing out the latent variables:
(5)

p✓(xt i| zt  x<t)dzt.

p✓(xt| x<t) =Z p✓(zt| x<t)

LYi=1

From this particular form  we can see two potential causes of the performance gap between F-SRNN
and F-RNN in the multi-frame setting:
• Advantage under High Volatility: By incorporating the continuous latent variable  the distribu-
tion p✓(xt | x<t) of F-SRNN essentially forms an inﬁnite mixture of simpler distributions (see
ﬁrst line of Eqn. (5)). As a result  the distribution is signiﬁcantly more expressive and ﬂexible  and
it is believed to be particularly suitable for modeling high-entropy sequential dynamics [10].
The multi-frame setting introduced above well matches this description. Concretely  since the
model is required to predict the next L frames all together in this setting  the long prediction horizon
will naturally involve a higher uncertainty. Therefore  the high volatility of the multi-frame setting
may provide a perfect scenario for SRNN to exhibit its theoretical advantage in expressiveness.
• Utilizing the Intra-Step Correlation: From Eqn. (5)  notice that the distribution p✓(xt | x<t)
after marginalization is generally not factorized any more  due to the coupling with z. In con-
trast  recall the same distribution of the F-RNN (Eqn. (3)) is fully factorized p✓(xt | x<t) =
QL
i=1 p✓(xt i | x<t). Therefore  in theory  a factorized SRNN could still model the correlation
among the L frames within each step  if properly trained  while the factorized RNN has no means
to do so at all. Thus  SRNN may also beneﬁt from this difference.

While both advantages could have jointly led to the performance gap in Table 1  the implications
are totally different. The ﬁrst advantage under high volatility is a unique property of latent-variable
models that other generative models without latent variables can hardly to obtain. Therefore  if this
property signiﬁcantly contributes to the superior performance of F-SRNN over F-RNN  it suggests
more general effectiveness of incorporating stochastic latent variables.
Quite the contrary  being able to utilize the intra-step correlation is more like an unfair beneﬁt to
SRNN  since it is the unnecessary restriction of fully factorized output distributions in previous
experimental design that prevents RNNs from modeling the correlation. In practice  one can easily
enable RNNs to do so by employing a non-factorized output distribution. In this case  it remains
unclear whether this particular advantage will sustain. Motivated by the distinct implications  in the
sequel  we will try to ﬁgure out how much each of the two hypotheses above actually contributes to
the performance gap.

3.3 Advantage under High Volatility

In order to test the advantage of F-SRNN in modeling high-volatile data in isolation  the idea is to
construct a sequential dataset where each step consists of a single frame (i.e.  a uni-variate variable) 
while there exists high volatility between every two consecutive steps.
Concretely  for each sequence x 2D   we create a sub-sequence by selecting one frame from every
M consecutive frames  i.e.  ˆx = [x1  xM +1  x2M +1  . . .] with xt 2 R. Intuitively  a larger stride M
will lead to a longer horizon between two selected frames and hence a higher uncertainty. Moreover 
since each step corresponds to a single scalar  the second advantage (i.e.  the potential confounding
factor) automatically disappears.
Following this idea  from the original datasets  we derive the stride-TIMIT  stride-VCTK and stride-
Blizzard with different stride values M  and evaluate the RNN and SRNN on each of them. Again 
we report the sequence- or frame-average test likelihood in Table 2.

4

Stride = 50

Stride = 200

Model TIMIT VCTK Blizzard TIMIT VCTK Blizzard
RNN 20 655
SRNN 14 469

4 124
0.177
-1 137 0.0187

-320
-1 231

4 607
3 603

0.668
0.605

Table 2: Performance comparison on high-volatility datasets.

Surprisingly  RNN consistently achieves a better performance than SRNN in this setting. It suggests
the theoretically better expressiveness of SRNN does not help that much in high-volatility scenarios.
Hence  this potential advantage does not really contribute to the performance gap observed in Table 1.

3.4 Utilizing the Intra-Step Correlation
After ruling out the ﬁrst hypothesis  it becomes more likely that being able to utilize the intra-step
correlation actually leads to the superior performance of F-SRNN. However  despite the non-factorized
form in Eqn. (5)  it is still not clear how F-SRNN computationally captures the correlation in practice.
Here  we provide a particular possibility.
Recall that in ELBO function of SRNN method (Eqn. (2))  the vector xt  we hope to reconstruct at
step t  is included in the conditional input to the posterior q(zt | z<t  x). With this computational
structure  the encoder could theoretically leak a subset of the vector xt into the latent variable zt  and
leverage the leaked subset to predict (reconstruct) the rest elements in xt. Intuitively  the procedure
of using the leaked subset to predict the remained subset is essentially exploiting the dependency
between the two subsets  or in other words  the correlation within xt.
Proposition 1. Given a vector xt  we split its elements into two arbitrary disjoint subsets  the leaked
subset xa
t . Assume that the latent variables and leaked subset have
the same dimensionality  |zt| = |xa

t |. Deﬁne the posterior distribution as a delta function:

t and its complement xb

t = xt\xa

q(zt | z<t  x) = zt=xa

t =⇢1 

0 

if zt = xa
t
otherwise  

We further assume p✓(xt | zt  x<t) ⇡ p✓(xt | zt  x<t). The ELBO function (Eqn. (2)) would
reduce to a special case of auto-regressive factorization:

(6)

(7)

✓ LD = E
max

x⇠D" TxXt=1hlog p✓(xa

t | x<t) + log p✓(xb

t | xa

t   x<t)i#.

t to predict xb

This proposition can be proved by substituting the posterior distribution into the ELBO function and
the detail derivation is provided in the supplementary material. Now  the second term in Eqn. (7)
is conditioned on the leaked subset of xa
t  which is exactly utilizing the correlation
between the two subsets. In other words  with a proper posterior  F-SRNN can recover a certain
auto-regressive parameterization  making it possible to utilize the intra-step correlation  even with a
fully factorized output distribution.
Although the analysis and construction above provide a theoretical possibility  we still lack concrete
evidence to support the hypothesis that F-SRNN has signiﬁcantly beneﬁted from modeling the
intra-step correlation. While it is difﬁcult to verify this hypothesis in general  we can parameterize an
RNN according to Eqn. (7)  which is equivalent to an F-SRNN with a delta posterior. Therefore  by
measuring the performance of this special RNN  we can get a conservative estimate of how much
modeling the intra-step correlation can contribute to the performance of F-SRNN.
To ﬁnish the special RNN idea  we still need to specify how xt is split into xa
consider two methods with different intuitions:
• Interleaving: The ﬁrst method takes one out of every U elements to construct xa

t =
t. In the
t the even ones. Hence  when
t  the output distribution is conditioned on both the elements
• Random: The second method simply uniformly selects V random elements from xt to form
t. Intuitively  this can be viewed as an informal “lower bound” of

{xt 1  xt U +1  xt 2U +1  . . .}. Essentially  this method interleaves the two subsets xa
extreme case of U = 2  xa
predicting an even element xt 2k 2 xb
to the left xt 2k1 and to the right xt 2k+1  making the problem much easier.
t   and leaves the rest for xb
xa
performance gain through modeling the intra-step correlation.

t includes the odd elements of xt and xb

t. Here  we

t and xb

t and xb

5

Models
F-RNN
F-SRNN
-RNN (U = 2)
-RNN (U = 3)
-RNN (V = 50)
-RNN (V = 75)

TIMIT VCTK Blizzard
7 610
32 745
15 258
69 296
15 306
70 900
72 067
15 284
14 389
66 122
66 453
14 585

0.786
2.383
2.027
2.262
2.199
2.120

Table 3: Performance comparison between -RNN and F-SRNN. Note that a smaller U corresponds
to leaking more elements.

Since the parametric form Eqn. (7) is derived from a delta posterior  we will refer to the special
RNN model as -RNN. Based on the two split methods  we train -RNN on TIMIT  VCTK and
Blizzard with different values of U and V . The results are summarized in Table 3. As we can see 
when the interleaving split scheme is used  -RNN signiﬁcantly improves upon F-RNN and becomes
very competitive with F-SRNN. Speciﬁcally  on TIMIT and Blizzard  -RNN can even outperform
F-SRNN in certain cases. More surprisingly  the -RNN with the random-copy scheme can also
achieve a performance that is very close to that of F-SRNN  especially compared to F-RNN.
Recall that -RNN is equivalent to employing a manually designed delta posterior that can only copy
but never compresses (auto-encodes) the information in xt. As a result  compared to a posterior that
can learn to compress information  the delta posterior will involve a higher KL cost when leaking
information through the posterior. Furthermore  the correlation between historical latent variables
and outputs is ignored. It would decrease the model capacity of -RNN. Despite these disadvantages 
-RNN is still able to match or even surpasses the performance of F-SRNN  suggesting the learned
posterior in F-SRNN is far from satisfying. Quite contrary to that  the limited performance gap
between F-SRNN and the random copy baseline raises a serious concern about the effectiveness of
current variational inference techniques.
Nevertheless  putting the analysis and empirical evidence together  we can conclude that the perfor-
mance advantage of F-SRNN in the multi-frame setting can be entirely attributed to the second cause.
That is  under the factorized constraint in previous experiments  F-SRNN can still implicitly leverage
the intra-step correlation  while F-RNN is prohibited to do so. However  as we have discussed earlier
in Section 3.2  this is essentially an unfair comparison. More importantly  the claimed superiority
of SRNN over RNN may be misleading  as it is unclear whether performance advantage of SRNN
will sustain or not when a non-factorized output distribution is employed to capture the intra-step
correlation explicitly.
As far as we know  no previous work has carefully compared the performance of SRNN and RNN
when non-factorized output distribution is allowed. On the other hand  as shown in Table 3  by
modeling the multivariate simultaneity in the simplest way  -RNN can achieve dramatic performance
improvement. Motivated by the huge potential as well as the lack of a systematic study  we will next
include non-factorized output distributions in our consideration  and properly re-evaluate SRNN and
RNN for multivariate sequence modeling.

4 Proper Multivariate Sequence Modeling with or without Latent Variables

4.1 Avoiding the Implicit Data Bias
In this section  we aim to eliminate any experimental bias and provide a proper evaluation of SRNN
and RNN for multivariate sequence modeling. Apart from the “model bias” of employing fully
factorized output distributions we have discussed  another possible source of bias is actually the
experimental data. For example  as we discussed in Section 3.1  the multi-frame speech sequences are
constructed by reshaping L consecutive real-valued frames into L-dimensional vectors. Consequently 
elements within each step xt are simply temporally correlated with a natural order  which would
favor a model that recurrently process each element from xt 1 to xt L with parameter sharing.
Thus  to avoid such “data bias”  besides speech sequences  we additionally consider three more
types of multivariate sequences with different patterns of intra-step correlation  they are MIDI sound
sequence data (including Muse and Nottingham datasets)  handwriting trajectory data (IAM-OnDB)

6

Models
VRNN†
SRNN†
Z-Forcing†
SWaveNet†‡
STCN†‡
F-RNN
F-SRNN
-RNN-random

RNN-ﬂat
SRNN-ﬂat
RNN-hier
SRNN-hier

TIMIT
28 982
60 550
70 469
72 463
77 438
32 745
69 296
66 453
117 721?
109 284
109 641
107 912

VCTK Blizzard Muse

Nottingham IAM-OnDB Perm-TIMIT

-
-
-
-
-

0.786
2.383
2.199
3.2173?
3.2062
3.1822
3.1423

9 392
11 991
15 430
15 708
17 670
7 610
15 258
14 585
22 714?
22 290
21 950
21 845

-6.28

-

-
-
-

-2.94

-

-
-
-

-6.991
-6.438
-6.252

-5.251
-5.616
-5.161
-5.483

-3.400
-2.811
-2.834

-2.180
-2.324
-2.028
-2.065

1384

-
-

1301
1796
1397
1402
N/A

N/A
N/A
1440
1395

-
-
-
-
-

25 679
67 613
61 103

15 763
14 278
95 161
94 402

Table 4: Performance comparison on a diverse set of datasets. The models with † indicate that the
performances are directly copied from previous publications. Numbers with ? indicate the state-of-
the-art performances. N/A suggests the model is not application on the dataset. The models with ‡
have other architectures than recurrent neural network as the backbone.
and the Perm-TIMIT dataset. The Perm-TIMIT is a variant of multivariate TIMIT dataset. It permutes
the elements within each time step  which is designed to remove the temporal bias. We include the
detail information of these datasets in Appendix C.

4.2 Modeling Simultaneity with Auto-Regressive Decomposition

With proper datasets  we now consider how to construct a family of non-factorized distributions that
(1) can be easily integrated into RNN and SRNN as the output distribution  and (2) are reasonably
expressive for modeling multivariate correlations. Among many possible choices  the most straight-
forward choice would be the auto-regressive parameterization. Compared to other options such as the
normalizing ﬂow or Markov Random Field (e.g. RBM)  the auto-regressive structure is conceptually
simpler and can be applied to both discrete and continuous data with full tractability. In light of these
beneﬁts  we choose to follow this simple idea  and decompose the output distribution of the RNN and
SRNN  respectively  as

p✓(xt | x<t) =

p✓(xt | zt  x<t) =

p✓(xt i | x<t  xt <i) 

p✓(xt i | zt  x<t  xt <i).

(8)

(9)

LYi=1
LYi=1

Notice that although we use the natural decomposition order from smallest index to largest one  this
particular order is generally not optimal for modeling multivariate distributions. A better choice
could be adapting the orderless training previously explored in literature [2]. But for simplicity  we
will stick to this simple approach.
Given the auto-regressive decomposition  a natural neural instantiation would be a recurrent hierar-
chical model that utilizes a two-level architecture to process the sequence:
• Firstly  a high-level RNN or SRNN is employed to encode the multivariate steps x = [x1  . . .   xT ]
into a sequence of high-level hidden vectors h = [h1  . . .   hT ]  which follows exactly the same as
the computational procedure used in F-RNN and F-SRNN . Recall that  in the case of SRNN  the
computation of high-level vectors involves sampling the latent variables.

• Based on the high-level representations  for each multivariate step xt  another neural model flow will
take both the elements [xt 1 ···   xt L] and the high-level vector ht as input  and auto-regressively
produce a sequence of low-level hidden vectors [gt 1 ···   gt L] where gt i = flow(xt <i  ht). They
can be then used to form the per-element output distributions in Eqn. (8) and (9).
In practice  the low-level model could simply be an RNN or a causally masked MLP [24]  depending
on our prior about the data. For convenience  we will refer to the hierarchical models as RNN-hier
and SRNN-hier.

7

In some cases where all the elements within a step share the same statistical type  such as on the
speech or MIDI dataset  one may alternatively consider a ﬂat model. As the name suggests  the ﬂat
model will break the boundary between steps and ﬂatten the data into a new uni-variate sequence 
where each step is simply a single element. Then  the new uni-variate sequence can be directly fed
into a standard RNN or SRNN model  producing each conditional factor in Eqn. (8) and (9) in an
auto-regressive manner. Similarly  this class of RNN and SRNN will be referred to as RNN-ﬂat and
SRNN-ﬂat  respectively. Compared to the hierarchical model  the ﬂat variant implicitly assumes a
sequential continuity between xt L and xt+1 1. Since this inductive bias matches the characteristics
of multi-frame speech sequences  we expect the ﬂat model to perform better in this case.

4.3 Experiment Results

Based on the seven datasets  we compare the performance of the models introduced above. To provide
a random baseline  we include the -RNN with the random split scheme in the comparison. Moreover 
previous results  if exist  are also presented to provide additional information. For a fair comparison 
we make sure all models share the same parameter size. For more implementation details  please refer
to the Supplementary D as well as the source code1. We also include the running time comparison
in Appendix 4.4. Finally  the results are summarized in Table 4  where we make several important
observations.
Firstly  on the speech and MIDI datasets  models with auto-regressive (lower-half) output distribu-
tions obtain a dramatic advantage over models with fully factorized output distributions (upper-half) 
achieving new SOTA results on three speech datasets. This observation reminds us that  besides cap-
turing the long-term temporal structure across steps  how to properly model the intra-step dependency
is equally  if not more  crucial to the practical performance.
Secondly  when the auto-regressive output distribution is employed (lower-half)  the non-stochastic
recurrent models consistently outperform their stochastic counterparts across all datasets. In other
words  the advantage of SRNN completely disappears once a powerful output distribution is used.
Combined with the previous observation  it veriﬁes our earlier concern that the so-called superiority
of F-SRNN over F-RNN is merely a result of the biased experiment design in previous work.
In addition  as we expected  when the inductive bias of the ﬂat model matches the characteristics of
speech data  it will achieve a better performance than the hierarchical model. Inversely  when the
prior does not match data property on the other datasets  the hierarchical model is always better. In
the extreme case of permuted TIMIT  the ﬂat model even falls behind factorized models  while the
hierarchical model achieves a very decent performance that is even much better than what F-SRNN
can achieve on the original TIMIT. This shows that the hierarchical model is usually more robust 
especially when we don’t have a good prior.
Overall  we don’t ﬁnd any advantage of employing stochastic latent variables for multivariate
sequence modeling. Instead  relying on a full auto-regressive solution yields better or even state-
of-the-art performances. Combined with the observation that -RNN-random can often achieve a
competitive performance to F-SRNN  we believe that the theoretical advantage of latent-variable
models in sequence modeling is still far from fulﬁlled  if ultimately possible. In addition  we suggest
future development along this line compare with the simple but extremely robust baselines with an
auto-regressive output distribution.

4.4 Training Time Comparison

Here  we report the training time of different methods in TIMIT dataset. The running times of training
models for 40k updating steps on TIMIT are summarized in Table 5. The input length indicates the
sample length of input during the training phrase. Admittedly  modeling the intra-step correlation
(*-hier and *-ﬂat model) would require extra computation time. Hence  this leads to a trade-off
between quality and speed. Ideally  latent-variable models would provide a solution close to the sweet
point of this trade-off. However  in our experiment  we ﬁnd a simple hierarchical auto-regressive
model trained with a shorter input length could already achieve signiﬁcantly better performance with
a comparable computation time (RNN-hier vs. F-SRNN in Table 5).

1https://github.com/zihangdai/reexamine-srnn

8

Input Length
Model Name
Training Time
Log-Likelihood

8000

1000

F-RNN F-SRNN -RNN RNN-hier
0.54h
32 745

9.92h
109 641

0.94h
69 296

0.90h
66 453

SRNN-hier RNN-ﬂat
37.48h
117 721

12.52h
107 912

SRNN-ﬂat RNN-hier

42.26h
109 284

1.7h

101 713

Table 5: Training time comparison between various models.

5 Conclusion and Discussion

In summary  our re-examination reveals a misleading impression on the beneﬁts of latent variables
in sequence modeling. From our empirical observation  the main effect of latent variables is only
to provide a mechanism to leverage the intra-step correlation  which is however  not as powerful as
employing the straightforward auto-regressive decomposition. It remains unclear what leads to the
signiﬁcant gap between the theoretical potential of latent variables and their practical effectiveness 
which we believe deserves more research attention. Meanwhile  given the large gain of modeling
simultaneity  using sequential structures to better capture local patterns is another good future
direction in sequence modeling.

Acknowledgment

This work is supported in part by the National Science Foundation (NSF) under grant IIS-1546329
and by DOE-Ofﬁce of Science under grant ASCR #KJ040201.

References
[1] Alex Graves. Generating sequences with recurrent neural networks.

arXiv:1308.0850  2013.

arXiv preprint

[2] Benigno Uria  Marc-Alexandre Côté  Karol Gregor  Iain Murray  and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research  17(1):7184–
7220  2016.

[3] Valentin Flunkert  David Salinas  and Jan Gasthaus. Deepar: Probabilistic forecasting with

autoregressive recurrent networks. arXiv preprint arXiv:1704.04110  2017.

[4] Xi Chen  Nikhil Mishra  Mostafa Rohaninejad  and Pieter Abbeel. Pixelsnail: An improved

autoregressive generative model. arXiv preprint arXiv:1712.09763  2017.

[5] Niki Parmar  Ashish Vaswani  Jakob Uszkoreit  Łukasz Kaiser  Noam Shazeer  and Alexander

Ku. Image transformer. arXiv preprint arXiv:1802.05751  2018.

[6] Zihang Dai  Zhilin Yang  Yiming Yang  William W Cohen  Jaime Carbonell  Quoc V Le 
and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length
context. arXiv preprint arXiv:1901.02860  2019.

[7] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[8] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082  2014.
[9] Justin Bayer and Christian Osendorfer. Learning stochastic recurrent networks. arXiv preprint

arXiv:1411.7610  2014.

[10] Junyoung Chung  Kyle Kastner  Laurent Dinh  Kratarth Goel  Aaron C Courville  and Yoshua
Bengio. A recurrent latent variable model for sequential data. In Advances in neural information
processing systems  pages 2980–2988  2015.

[11] Marco Fraccaro  Søren Kaae Sønderby  Ulrich Paquet  and Ole Winther. Sequential neural
models with stochastic layers. In Advances in neural information processing systems  pages
2199–2207  2016.

9

[12] Anirudh Goyal Alias Parth Goyal  Alessandro Sordoni  Marc-Alexandre Côté  Nan Rosemary
Ke  and Yoshua Bengio. Z-forcing: Training stochastic recurrent networks. In Advances in
neural information processing systems  pages 6713–6723  2017.

[13] Guokun Lai  Bohan Li  Guoqing Zheng  and Yiming Yang. Stochastic wavenet: A generative

latent variable model for sequential data. arXiv preprint arXiv:1806.06116  2018.

[14] Emre Aksan and Otmar Hilliges. Stcn: Stochastic temporal convolutional networks. arXiv

preprint arXiv:1902.06568  2019.

[15] Aaron van den Oord  Nal Kalchbrenner  and Koray Kavukcuoglu. Pixel recurrent neural

networks. arXiv preprint arXiv:1601.06759  2016.

[16] Aaron van den Oord  Nal Kalchbrenner  Lasse Espeholt  Oriol Vinyals  Alex Graves  et al.
Conditional image generation with pixelcnn decoders. In Advances in Neural Information
Processing Systems  pages 4790–4798  2016.

[17] Tim Salimans  Andrej Karpathy  Xi Chen  and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modiﬁcations. arXiv preprint
arXiv:1701.05517  2017.

[18] Karol Gregor  Ivo Danihelka  Alex Graves  Danilo Jimenez Rezende  and Daan Wierstra. Draw:

A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623  2015.

[19] Karol Gregor  Frederic Besse  Danilo Jimenez Rezende  Ivo Danihelka  and Daan Wierstra.
Towards conceptual compression. In Advances In Neural Information Processing Systems 
pages 3549–3557  2016.

[20] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows.

arXiv preprint arXiv:1505.05770  2015.

[21] Laurent Dinh  Jascha Sohl-Dickstein  and Samy Bengio. Density estimation using real nvp.

arXiv preprint arXiv:1605.08803  2016.

[22] Durk P Kingma  Tim Salimans  Rafal Jozefowicz  Xi Chen  Ilya Sutskever  and Max Welling.
Improved variational inference with inverse autoregressive ﬂow. In Advances in neural informa-
tion processing systems  pages 4743–4751  2016.

[23] Nicolas Boulanger-Lewandowski  Yoshua Bengio  and Pascal Vincent. Modeling temporal
dependencies in high-dimensional sequences: Application to polyphonic music generation and
transcription. arXiv preprint arXiv:1206.6392  2012.

[24] Mathieu Germain  Karol Gregor  Iain Murray  and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In International Conference on Machine Learning  pages 881–889 
2015.

[25] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[26] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv

preprint arXiv:1608.03983  2016.

[27] Sam Roweis and Zoubin Ghahramani. A unifying review of linear gaussian models. Neural

computation  11(2):305–345  1999.

[28] Lawrence R Rabiner and Biing-Hwang Juang. An introduction to hidden markov models. ieee

assp magazine  3(1):4–16  1986.

[29] Rudolph Emil Kalman. A new approach to linear ﬁltering and prediction problems. Journal of

basic Engineering  82(1):35–45  1960.

[30] Ilya Sutskever  Geoffrey E Hinton  and Graham W Taylor. The recurrent temporal restricted
boltzmann machine. In Advances in neural information processing systems  pages 1601–1608 
2009.

10

[31] Zhe Gan  Chunyuan Li  Ricardo Henao  David E Carlson  and Lawrence Carin. Deep temporal
sigmoid belief networks for sequence modeling. In Advances in Neural Information Processing
Systems  pages 2467–2475  2015.

[32] Rahul G Krishnan  Uri Shalit  and David Sontag. Deep kalman ﬁlters. arXiv preprint

arXiv:1511.05121  2015.

[33] Rahul G Krishnan  Uri Shalit  and David Sontag. Structured inference networks for nonlinear

state space models. In AAAI  pages 2101–2109  2017.

[34] Marco Fraccaro  Simon Kamronn  Ulrich Paquet  and Ole Winther. A disentangled recognition
and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information
Processing Systems  pages 3601–3610  2017.

[35] Matthew Johnson  David K Duvenaud  Alex Wiltschko  Ryan P Adams  and Sandeep R Datta.
Composing graphical models with neural networks for structured representations and fast
inference. In Advances in neural information processing systems  pages 2946–2954  2016.

11

,Guokun Lai
Zihang Dai
Yiming Yang
Shinjae Yoo