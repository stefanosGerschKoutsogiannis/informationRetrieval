2017,Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations,We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy.  Our method is based on a soft (continuous) relaxation of quantization and entropy  which we anneal to their discrete counterparts throughout training.  We showcase this method for two challenging applications: Image compression and neural network compression.  While these tasks have typically been approached with different methods  our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.,Soft-to-Hard Vector Quantization for End-to-End

Learning Compressible Representations

Eirikur Agustsson

ETH Zurich

Fabian Mentzer

ETH Zurich

Michael Tschannen

ETH Zurich

aeirikur@vision.ee.ethz.ch

mentzerf@vision.ee.ethz.ch

michaelt@nari.ee.ethz.ch

Lukas Cavigelli

ETH Zurich

cavigelli@iis.ee.ethz.ch

Radu Timofte

ETH Zurich & Merantix

timofter@vision.ee.ethz.ch

Luca Benini
ETH Zurich

benini@iis.ee.ethz.ch

Luc Van Gool

KU Leuven & ETH Zurich
vangool@vision.ee.ethz.ch

Abstract

We present a new approach to learn compressible representations in deep archi-
tectures with an end-to-end training strategy. Our method is based on a soft
(continuous) relaxation of quantization and entropy  which we anneal to their
discrete counterparts throughout training. We showcase this method for two chal-
lenging applications: Image compression and neural network compression. While
these tasks have typically been approached with different methods  our soft-to-hard
quantization approach gives results competitive with the state-of-the-art for both.

Introduction

1
In recent years  deep neural networks (DNNs) have led to many breakthrough results in machine
learning and computer vision [20  28  10]  and are now widely deployed in industry. Modern DNN
models often have millions or tens of millions of parameters  leading to highly redundant structures 
both in the intermediate feature representations they generate and in the model itself. Although
overparametrization of DNN models can have a favorable effect on training  in practice it is often
desirable to compress DNN models for inference  e.g.  when deploying them on mobile or embedded
devices with limited memory. The ability to learn compressible feature representations  on the other
hand  has a large potential for the development of (data-adaptive) compression algorithms for various
data types such as images  audio  video  and text  for all of which various DNN architectures are now
available.
DNN model compression and lossy image compression using DNNs have both independently attracted
a lot of attention lately. In order to compress a set of continuous model parameters or features  we
need to approximate each parameter or feature by one representative from a set of quantization
levels (or vectors  in the multi-dimensional case)  each associated with a symbol  and then store the
assignments (symbols) of the parameters or features  as well as the quantization levels. Representing
each parameter of a DNN model or each feature in a feature representation by the corresponding
quantization level will come at the cost of a distortion D  i.e.  a loss in performance (e.g.  in
classiﬁcation accuracy for a classiﬁcation DNN with quantized model parameters  or in reconstruction
error in the context of autoencoders with quantized intermediate feature representations). The rate
R  i.e.  the entropy of the symbol stream  determines the cost of encoding the model or features in a
bitstream.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

To learn a compressible DNN model or feature representation we need to minimize D + βR  where
β > 0 controls the rate-distortion trade-off. Including the entropy into the learning cost function can
be seen as adding a regularizer that promotes a compressible representation of the network or feature
representation. However  two major challenges arise when minimizing D + βR for DNNs: i) coping
with the non-differentiability (due to quantization operations) of the cost function D + βR  and ii)
obtaining an accurate and differentiable estimate of the entropy (i.e.  R). To tackle i)  various methods
have been proposed. Among the most popular ones are stochastic approximations [39  19  7  32  5]
and rounding with a smooth derivative approximation [15  30]. To address ii) a common approach
is to assume the symbol stream to be i.i.d. and to model the marginal symbol distribution with a
parametric model  such as a Gaussian mixture model [30  34]  a piecewise linear model [5]  or a
Bernoulli distribution [33] (in the case of binary symbols).
In this paper  we propose a uniﬁed end-to-end learning frame-
work for learning compressible representations  jointly op-
timizing the model parameters  the quantization levels  and
the entropy of the resulting symbol stream to compress ei-
ther a subset of feature representations in the network or the
model itself (see inset ﬁgure). We address both challenges i)
and ii) above with methods that are novel in the context DNN
model and feature compression. Our main contributions are:
• We provide the ﬁrst uniﬁed view on end-to-end learned compression of feature representations and
DNN models. These two problems have been studied largely independently in the literature so far.
• Our method is simple and intuitively appealing  relying on soft assignments of a given scalar
or vector to be quantized to quantization levels. A parameter controls the “hardness” of the
assignments and allows to gradually transition from soft to hard assignments during training. In
contrast to rounding-based or stochastic quantization schemes  our coding scheme is directly
differentiable  thus trainable end-to-end.

z: vector to be compressed

DNN model compression

FK ◦ ... ◦ Fb+1

x(1)

x(K−1)

data compression

z = x(b)

z = [w1  w2  . . .   wK]

F1(· ; w1)

x(K)

FK(· ; wK)

x

x

Fb ◦ ... ◦ F1

x(K)

• Our method does not force the network to adapt to speciﬁc (given) quantization outputs (e.g. 
integers) but learns the quantization levels jointly with the weights  enabling application to a wider
set of problems. In particular  we explore vector quantization for the ﬁrst time in the context of
learned compression and demonstrate its beneﬁts over scalar quantization.

• Unlike essentially all previous works  we make no assumption on the marginal distribution of
the features or model parameters to be quantized by relying on a histogram of the assignment
probabilities rather than the parametric models commonly used in the literature.

• We apply our method to DNN model compression for a 32-layer ResNet model [13] and full-
resolution image compression using a variant of the compressive autoencoder proposed recently
in [30]. In both cases  we obtain performance competitive with the state-of-the-art  while making
fewer model assumptions and signiﬁcantly simplifying the training procedure compared to the
original works [30  6].

The remainder of the paper is organized as follows. Section 2 reviews related work  before our
soft-to-hard vector quantization method is introduced in Section 3. Then we apply it to a compres-
sive autoencoder for image compression and to ResNet for DNN compression in Section 4 and 5 
respectively. Section 6 concludes the paper.

2 Related Work
There has been a surge of interest in DNN models for full-resolution image compression  most
notably [32  33  4  5  30]  all of which outperform JPEG [35] and some even JPEG 2000 [29]
The pioneering work [32  33] showed that progressive image compression can be learned with
convolutional recurrent neural networks (RNNs)  employing a stochastic quantization method during
training. [4  30] both rely on convolutional autoencoder architectures. These works are discussed in
more detail in Section 4.
In the context of DNN model compression  the line of works [12  11  6] adopts a multi-step procedure
in which the weights of a pretrained DNN are ﬁrst pruned and the remaining parameters are quantized
using a k-means like algorithm  the DNN is then retrained  and ﬁnally the quantized DNN model
is encoded using entropy coding. A notable different approach is taken by [34]  where the DNN

2

compression task is tackled using the minimum description length principle  which has a solid
information-theoretic foundation.
It is worth noting that many recent works target quantization of the DNN model parameters and
possibly the feature representation to speed up DNN evaluation on hardware with low-precision
arithmetic  see  e.g.  [15  23  38  43]. However  most of these works do not speciﬁcally train the DNN
such that the quantized parameters are compressible in an information-theoretic sense.
Gradually moving from an easy (convex or differentiable) problem to the actual harder problem during
optimization  as done in our soft-to-hard quantization framework  has been studied in various contexts
and falls under the umbrella of continuation methods (see [3] for an overview). Formally related but
motivated from a probabilistic perspective are deterministic annealing methods for maximum entropy
clustering/vector quantization  see  e.g.  [24  42]. Arguably most related to our approach is [41] 
which also employs continuation for nearest neighbor assignments  but in the context of learning a
supervised prototype classiﬁer. To the best of our knowledge  continuation methods have not been
employed before in an end-to-end learning framework for neural network-based image compression
or DNN compression.

L(X  Y; F ) =

1
N

(1)

and

N(cid:88)

i=1

(cid:96)(F (xi)  yi) + λR(W) 

3 Proposed Soft-to-Hard Vector Quantization
3.1 Problem Formulation
Preliminaries and Notations. We consider the standard model for DNNs  where we have an
architecture F : Rd1 (cid:55)→ RdK+1 composed of K layers F = FK ◦ ··· ◦ F1  where layer Fi
maps Rdi → Rdi+1  and has parameters wi ∈ Rmi. We refer to W = [w1 ···   wK] as the
parameters of the network and we denote the intermediate layer outputs of the network as x(0) :=
x(i) := Fi(x(i−1))  such that F (x) = x(K) and x(i) is the feature vector produced
x
by layer Fi.
The parameters of the network are learned w.r.t. training data X = {x1 ···   xN} ⊂ Rd1 and labels
Y = {y1 ···   yN} ⊂ RdK+1  by minimizing a real-valued loss L(X  Y; F ). Typically  the loss can
be decomposed as a sum over the training data plus a regularization term 
(e.g.  R(W) =(cid:80)

where (cid:96)(F (x)  y) is the sample loss  λ > 0 sets the regularization strength  and R(W) is a regularizer
i (cid:107)wi(cid:107)2 for l2 regularization). In this case  the parameters of the network can be
learned using stochastic gradient descent over mini-batches. Assuming that the data X  Y on which
the network is trained is drawn from some distribution PX Y  the loss (1) can be thought of as an
estimator of the expected loss E[(cid:96)(F (X)  Y) + λR(W)]. In the context of image classiﬁcation  Rd1
would correspond to the input image space and RdK+1 to the classiﬁcation probabilities  and (cid:96) would
be the categorical cross entropy.
We say that the deep architecture is an autoencoder when the network maps back into the input space 
with the goal of reproducing the input. In this case  d1 = dK+1 and F (x) is trained to approximate x 
e.g.  with a mean squared error loss (cid:96)(F (x)  y) = (cid:107)F (x) − y(cid:107)2. Autoencoders typically condense
the dimensionality of the input into some smaller dimensionality inside the network  i.e.  the layer
with the smallest output dimension  x(b) ∈ Rdb   has db (cid:28) d1  which we refer to as the “bottleneck”.
Compressible representations. We say that a weight parameter wi or a feature x(i) has a compress-
ible representation if it can be serialized to a binary stream using few bits. For DNN compression  we
want the entire network parameters W to be compressible. For image compression via an autoencoder 
we just need the features in the bottleneck  x(b)  to be compressible.
Suppose we want to compress a feature representation z ∈ Rd in our network (e.g.  x(b) of an
autoencoder) given an input x. Assuming that the data X  Y is drawn from some distribution PX Y  z
will be a sample from a continuous random variable Z.
To store z with a ﬁnite number of bits  we need to map it to a discrete space. Speciﬁcally  we map
z to a sequence of m symbols using a (symbol) encoder E : Rd (cid:55)→ [L]m  where each symbol is an
index ranging from 1 to L  i.e.  [L] := {1  . . .   L}. The reconstruction of z is then produced by a
(symbol) decoder D : [L]m (cid:55)→ Rd  which maps the symbols back to ˆz = D(E(z)) ∈ Rd. Since z is

3

a sample from Z  the symbol stream E(z) is drawn from the discrete probability distribution PE(Z).
Thus  given the encoder E  according to Shannon’s source coding theorem [8]  the correct metric for
compressibility is the entropy of E(Z):

P (E(Z) = e) log(P (E(Z) = e)).

(2)

(cid:88)

H(E(Z)) = −

e∈[L]m

Our generic goal is hence to optimize the rate distortion trade-off between the expected loss and the
entropy of E(Z):

EX Y[(cid:96)( ˆF (X)  Y) + λR(W)] + βH(E(Z)) 

min

E D W

(3)

where ˆF is the architecture where z has been replaced with ˆz  and β > 0 controls the trade-off
between compressibility of z and the distortion it imposes on ˆF .
However  we cannot optimize (3) directly. First  we do not know the distribution of X and Y.
Second  the distribution of Z depends in a complex manner on the network parameters W and the
distribution of X. Third  the encoder E is a discrete mapping and thus not differentiable. For our ﬁrst
approximation we consider the sample entropy instead of H(E(Z)). That is  given the data X and
some ﬁxed network parameters W  we can estimate the probabilities P (E(Z) = e) for e ∈ [L]m
via a histogram. For this estimate to be accurate  we however would need |X| (cid:29) Lm. If z is the
bottleneck of an autoencoder  this would correspond to trying to learn a single histogram for the
entire discretized data space. We relax this by assuming the entries of E(Z) are i.i.d. such that we
can instead compute the histogram over the L distinct values. More precisely  we assume that for
l=1 pel   where pj is the histogram
e = (e1 ···   em) ∈ [L]m we can approximate P (E(Z) = e) ≈
estimate

(cid:81)m

pj := |{el(zi)|l ∈ [m]  i ∈ [N ]  el(zi) = j}|

mN

 

(4)

where we denote the entries of E(z) = (e1(z) ···   em(z)) and zi is the output feature z for training
data point xi ∈ X . We then obtain an estimate of the entropy of Z by substituting the approximation
(3.1) into (2) 

H(E(Z)) ≈ −

pel

log

pel

= −m

pj log pj = mH(p) 

(5)

where the ﬁrst (exact) equality is due to [8]  Thm. 2.6.6  and H(p) := −
entropy for the (i.i.d.  by assumption) components of E(Z) 1.
We now can simplify the ideal objective of (3)  by replacing the expected loss with the sample mean
over (cid:96) and the entropy using the sample entropy H(p)  obtaining

j=1 pj log pj is the sample

(cid:33)

(cid:32) m(cid:89)

(cid:88)

e∈[L]m

l=1

(cid:33)

(cid:32) m(cid:89)

l=1

L(cid:88)

j=1

(cid:80)L

N(cid:88)

i=1

1
N

(cid:96)(F (xi)  yi) + λR(W) + βmH(p).

(6)

We note that so far we have assumed that z is a feature output in F   i.e.  z = x(k) for some k ∈ [K].
However  the above treatment would stay the same if z is the concatenation of multiple feature
outputs. One can also obtain a separate sample entropy term for separate feature outputs and add
them to the objective in (6).
In case z is composed of one or more parameter vectors  such as in DNN compression where z = W 
z and ˆz cease to be random variables  since W is a parameter of the model. That is  opposed to the
case where we have a source X that produces another source ˆZ which we want to be compressible 
we want the discretization of a single parameter vector W to be compressible. This is analogous to
compressing a single document  instead of learning a model that can compress a stream of documents.
In this case  (3) is not the appropriate objective  but our simpliﬁed objective in (6) remains appropriate.
This is because a standard technique in compression is to build a statistical model of the (ﬁnite) data 
which has a small sample entropy. The only difference is that now the histogram probabilities in (4)
are taken over W instead of the dataset X   i.e.  N = 1 and zi = W in (4)  and they count towards
storage as well as the encoder E and decoder D.

1In fact  from [8]  Thm. 2.6.6  it follows that if the histogram estimates pj are exact  (5) is an upper bound

for the true H(E(Z)) (i.e.  without the i.i.d. assumption).

4

Challenges. Eq. (6) gives us a uniﬁed objective that can well describe the trade-off between com-
pressible representations in a deep architecture and the original training objective of the architecture.
However  the problem of ﬁnding a good encoder E  a corresponding decoder D  and parameters W
that minimize the objective remains. First  we need to impose a form for the encoder and decoder 
and second we need an approach that can optimize (6) w.r.t. the parameters W. Independently of the
choice of E  (6) is challenging since E is a mapping to a ﬁnite set and  therefore  not differentiable.
This implies that neither H(p) is differentiable nor ˆF is differentiable w.r.t. the parameters of z and
layers that feed into z. For example  if ˆF is an autoencoder and z = x(b)  the output of the network
will not be differentiable w.r.t. w1 ···   wb and x(0) ···   x(b−1).
These challenges motivate the design decisions of our soft-to-hard annealing approach  described in
the next section.

3.2 Our Method
Encoder and decoder form. For the encoder E : Rd (cid:55)→ [L]m we assume that we have L centers
vectors C = {c1 ···   cL} ⊂ Rd/m. The encoding of z ∈ Rd is then performed by reshaping it
into a matrix Z = [¯z(1) ···   ¯z(m)] ∈ R(d/m)×m  and assigning each column ¯z(l) to the index of its
nearest neighbor in C. That is  we assume the feature z ∈ Rd can be modeled as a sequence of m
points in Rd/m  which we partition into the Voronoi tessellation over the centers C. The decoder
D : [L]m (cid:55)→ Rd then simply constructs ˆZ ∈ R(d/m)×m from a symbol sequence (e1 ···   em) by
picking the corresponding centers ˆZ = [ce1  ···   cem ]  from which ˆz is formed by reshaping ˆZ back
into Rd. We will interchangeably write ˆz = D(E(z)) and ˆZ = D(E(Z)).
The idea is then to relax E and D into continuous mappings via soft assignments instead of the hard
nearest neighbor assignment of E.
Soft assignments. We deﬁne the soft assignment of ¯z ∈ Rd/m to C as
where softmax(y1 ···   yL)j :=
positive entries and (cid:107)φ(¯z)(cid:107)1 = 1. We denote the j-th entry of φ(¯z) with φj(¯z) and note that

(7)
ey1 +···+eyL is the standard softmax operator  such that φ(¯z) has

φ(¯z) := softmax(−σ[(cid:107)¯z − c1(cid:107)2  . . .  (cid:107)¯z − cL(cid:107)2]) ∈ RL 

eyj

(cid:40)

σ→∞ φj(¯z) =
lim

1
0

if j = arg minj(cid:48)∈[L](cid:107)¯z − cj(cid:48)(cid:107)
otherwise

such that ˆφ(¯z) := limσ→∞ φ(¯z) converges to a one-hot encoding of the nearest center to ¯z in C. We
therefore refer to ˆφ(¯z) as the hard assignment of ¯z to C and the parameter σ > 0 as the hardness of
the soft assignment φ(¯z).
Using soft assignment  we deﬁne the soft quantization of ¯z as

L(cid:88)

˜Q(¯z) :=

cjφi(¯z) = Cφ(¯z) 

where we write the centers as a matrix C = [c1 ···   cL] ∈ Rd/m×L. The corresponding hard
assignment is taken with ˆQ(¯z) := limσ→∞ ˜Q(¯z) = ce(¯z)  where e(¯z) is the center in C nearest to ¯z.
Therefore  we can now write:

j=1

ˆZ = D(E(Z)) = [ ˆQ(¯z(1)) ···   ˆQ(¯z(m))] = C[ ˆφ(¯z(1)) ···   ˆφ(¯z(m))].

Now  instead of computing ˆZ via hard nearest neighbor assignments  we can approximate it with
a smooth relaxation ˜Z := C[φ(¯z(1)) ···   φ(¯z(m))] by using the soft assignments instead of the
hard assignments. Denoting the corresponding vector form by ˜z  this gives us a differentiable
approximation ˜F of the quantized architecture ˆF   by replacing ˆz in the network with ˜z.
Entropy estimation. Using the soft assignments  we can similarly deﬁne a soft histogram  by
summing up the partial assignments to each center instead of counting as in (4):

qj :=

1

mN

φj(¯z(l)

i ).

N(cid:88)

m(cid:88)

i=1

l=1

5

This gives us a valid probability mass function q = (q1 ···   qL)  which is differentiable but converges
to p = (p1 ···   pL) as σ → ∞.
We can now deﬁne the “soft entropy” as the cross entropy between p and q:

where DKL(p||q) = (cid:80)

˜H(φ) := H(p  q) = −

pj log qj = H(p) + DKL(p||q)

L(cid:88)

j=1

L(cid:88)

j=1

N(cid:88)

m(cid:88)

L(cid:88)

i=1

l=1

j=1

j pj log(pj/qj) denotes the Kullback–Leibler divergence.

Since
DKL(p||q) ≥ 0  this establishes ˜H(φ) as an upper bound for H(p)  where equality is obtained
when p = q.
We have therefore obtained a differentiable “soft entropy” loss (w.r.t. q)  which is an upper bound on
the sample entropy H(p). Hence  we can indirectly minimize H(p) by minimizing ˜H(φ)  treating
the histogram probabilities of p as constants for gradient computation. However  we note that while
qj is additive over the training data and the symbol sequence  log(qj) is not. This prevents the use
of mini-batch gradient descent on ˜H(φ)  which can be an issue for large scale learning problems.
In this case  we can instead re-deﬁne the soft entropy ˜H(φ) as H(q  p). As before  ˜H(φ) → H(p)
as σ → ∞  but ˜H(φ) ceases to be an upper bound for H(p). The beneﬁt is that now ˜H(φ) can be
decomposed as

˜H(φ) := H(q  p) = −

qj log pj = −

1

mN

φj(¯z(l)

i ) log pj 

(8)

such that we get an additive loss over the samples xi ∈ X and the components l ∈ [m].
Soft-to-hard deterministic annealing. Our soft assignment scheme gives us differentiable ap-
proximations ˜F and ˜H(φ) of the discretized network ˆF and the sample entropy H(p)  respectively.
However  our objective is to learn network parameters W that minimize (6) when using the encoder
and decoder with hard assignments  such that we obtain a compressible symbol stream E(z) which
we can compress using  e.g.  arithmetic coding [40].
To this end  we anneal σ from some initial value σ0 to inﬁnity during training  such that the soft
approximation gradually becomes a better approximation of the ﬁnal hard quantization we will use.
Choosing the annealing schedule is crucial as annealing too slowly may allow the network to invert
the soft assignments (resulting in large weights)  and annealing too fast leads to vanishing gradients
too early  thereby preventing learning. In practice  one can either parametrize σ as a function of the
iteration  or tie it to an auxiliary target such as the difference between the network losses incurred by
soft quantization and hard quantization (see Section 4 for details).
For a simple initialization of σ0 and the centers C  we can sample the centers from the set Z :=
{¯z(l)
¯z∈Z (cid:107)¯z − ˜Q(¯z)(cid:107)2
using SGD.

|i ∈ [N ]  l ∈ [m]} and then cluster Z by minimizing the cluster energy(cid:80)

i

Image Compression

4
We now show how we can use our framework to realize a simple image compression system. For
the architecture  we use a variant of the convolutional autoencoder proposed recently in [30] (see
Appendix A.1 for details). We note that while we use the architecture of [30]  we train it using our
soft-to-hard entropy minimization method  which differs signiﬁcantly from their approach  see below.
Our goal is to learn a compressible representation of the features in the bottleneck of the autoencoder.
Because we do not expect the features from different bottleneck channels to be identically distributed 
we model each channel’s distribution with a different histogram and entropy loss  adding each entropy
term to the total loss using the same β parameter. To encode a channel into symbols  we separate the
channel matrix into a sequence of pw × ph-dimensional patches. These patches (vectorized) form the
columns of Z ∈ Rd/m×m  where m = d/(pwph)  such that Z contains m (pwph)-dimensional points.
Having ph or pw greater than one allows symbols to capture local correlations in the bottleneck 
which is desirable since we model the symbols as i.i.d. random variables for entropy coding. At test
time  the symbol encoder E then determines the symbols in the channel by performing a nearest
neighbor assignment over a set of L centers C ⊂ Rpwph  resulting in ˆZ  as described above. During
training we instead use the soft quantized ˜Z  also w.r.t. the centers C.

6

0.20bpp / 0.91 / 0.69 / 23.88dB

SHVQ (ours)

0.20bpp / 0.90 / 0.67 / 24.19dB

BPG

0.20bpp / 0.88 / 0.63 / 23.01dB

JPEG 2000

0.22bpp / 0.77 / 0.48 / 19.77dB

JPEG

Figure 1: Top: MS-SSIM as a function of rate for SHVQ (Ours)  BPG  JPEG 2000  JPEG  for each
data set. Bottom: A visual example from the Kodak data set along with rate / MS-SSIM / SSIM /
PSNR.

We trained different models using Adam [17]  see Appendix A.2. Our training set is composed
similarly to that described in [4]. We used a subset of 90 000 images from ImageNET [9]  which
we downsampled by a factor 0.7 and trained on crops of 128 × 128 pixels  with a batch size of 15.
To estimate the probability distribution p for optimizing (8)  we maintain a histogram over 5 000
images  which we update every 10 iterations with the images from the current batch. Details about
other hyperparameters can be found in Appendix A.2.
The training of our autoencoder network takes place in two stages  where we move from an identity
function in the bottleneck to hard quantization. In the ﬁrst stage  we train the autoencoder without any
quantization. Similar to [30] we gradually unfreeze the channels in the bottleneck during training (this
gives a slight improvement over learning all channels jointly from the start). This yields an efﬁcient
weight initialization and enables us to then initialize σ0 and C as described above. In the second stage 
we minimize (6)  jointly learning network weights and quantization levels. We anneal σ by letting the
gap between soft and hard quantization error go to zero as the number of iterations t goes to inﬁnity.
Let eS = (cid:107) ˜F (x)−x(cid:107)2 be the soft error  eH = (cid:107) ˆF (x)−x(cid:107)2 be the hard error. With gap(t) = eH−eS
we can denote the error between the actual the desired gap with eG(t) = gap(t) − T /(T + t) gap(0) 
such that the gap is halved after T iterations. We update σ according to σ(t + 1) = σ(t) + KG eG(t) 
where σ(t) denotes σ at iteration t. Fig. 3 in Appendix A.4 shows the evolution of the gap  soft and
hard loss as sigma grows during training. We observed that both vector quantization and entropy loss
lead to higher compression rates at a given reconstruction MSE compared to scalar quantization and
training without entropy loss  respectively (see Appendix A.3 for details).

Evaluation. To evaluate the image compression performance of our Soft-to-Hard Vector Quantiza-
tion Autoencoder (SHVQ) method we use four datasets  namely Kodak [2]  B100 [31]  Urban100 [14] 
ImageNET100 (100 randomly selected images from ImageNET [25]) and three standard quality
measures  namely peak signal-to-noise ratio (PSNR)  structural similarity index (SSIM) [37]  and
multi-scale SSIM (MS-SSIM)  see Appendix A.5 for details. We compare our SHVQ with the
standard JPEG  JPEG 2000  and BPG [1]  focusing on compression rates < 1 bits per pixel (bpp) (i.e. 
the regime where traditional integral transform-based compression algorithms are most challenged).
As shown in Fig. 1  for high compression rates (< 0.4 bpp)  our SHVQ outperforms JPEG and JPEG
2000 in terms of MS-SSIM and is competitive with BPG. A similar trend can be observed for SSIM
(see Fig. 4 in Appendix A.6 for plots of SSIM and PSNR as a function of bpp). SHVQ performs
best on ImageNET100 and is most challenged on Kodak when compared with JPEG 2000. Visually 
SHVQ-compressed images have fewer artifacts than those compressed by JPEG 2000 (see Fig. 1 
and Fig. 5–12 in Appendix A.7).

Related methods and discussion.
JPEG 2000 [29] uses wavelet-based transformations and adap-
tive EBCOT coding. BPG [1]  based on a subset of the HEVC video compression standard  is the

7

0.20.40.6rate[bpp]0.860.880.900.920.940.960.981.00MS-SSIMImageNET1000.20.40.6rate[bpp]0.860.880.900.920.940.960.981.00MS-SSIMB1000.20.40.6rate[bpp]0.860.880.900.920.940.960.981.00MS-SSIMUrban1000.20.40.6rate[bpp]0.860.880.900.920.940.960.981.00MS-SSIMKodakSHVQ(ours)BPGJPEG2000JPEGACC COMP.
[%] RATIO
METHOD
1.00
92.6
ORIGINAL MODEL
92.6
4.52
PRUNING + FT. + INDEX CODING + H. CODING [12]
PRUNING + FT. + K-MEANS + FT. + I.C. + H.C. [11]
92.6 18.25
PRUNING + FT. + HESSIAN-WEIGHTED K-MEANS + FT. + I.C. + H.C. 92.7 20.51
92.7 22.17
PRUNING + FT. + UNIFORM QUANTIZATION + FT. + I.C. + H.C.
PRUNING + FT. + ITERATIVE ECSQ + FT. + I.C. + H.C.
92.7 21.01
SOFT-TO-HARD ANNEALING + FT. + H. CODING (OURS)
92.1 19.15
SOFT-TO-HARD ANNEALING + FT. + A. CODING (OURS)
92.1 20.15

Table 1: Accuracies and compression factors for different DNN compression techniques  using a
32-layer ResNet on CIFAR-10. FT. denotes ﬁne-tuning  IC. denotes index coding and H.C. and A.C.
denote Huffman and arithmetic coding  respectively. The pruning based results are from [6].

Quantization
Backpropagation
Entropy estimation (soft) histogram
Training material
Operating points

ImageNET
single model

Theis et al. [30]
rounding to integers

SHVQ (ours)
vector quantization
grad. of soft relaxation grad. of identity mapping
Gaussian scale mixtures
high quality Flickr images
ensemble

current state-of-the art for image compression. It uses context-adaptive binary arithmetic coding
(CABAC) [21].
The recent works of [30  5]
also showed competitive perfor-
mance with JPEG 2000. While
we use the architecture of [30] 
there are stark differences be-
tween the works  summarized
in the inset table. The work of [5] build a deep model using multiple generalized divisive normaliza-
tion (GDN) layers and their inverses (IGDN)  which are specialized layers designed to capture local
joint statistics of natural images. Furthermore  they model marginals for entropy estimation using
linear splines and also use CABAC[21] coding. Concurrent to our work  the method of [16] builds on
the architecture proposed in [33]  and shows that impressive performance in terms of the MS-SSIM
metric can be obtained by incorporating it into the optimization (instead of just minimizing the MSE).
In contrast to the domain-speciﬁc techniques adopted by these state-of-the-art methods  our framework
for learning compressible representation can realize a competitive image compression system  only
using a convolutional autoencoder and simple entropy coding.

5 DNN Compression
For DNN compression  we investigate the ResNet [13] architecture for image classiﬁcation. We adopt
the same setting as [6] and consider a 32-layer architecture trained for CIFAR-10 [18]. As in [6]  our
goal is to learn a compressible representation for all 464 154 trainable parameters of the model.
We concatenate the parameters into a vector W ∈ R464 154 and employ scalar quantization (m = d) 
such that ZT = z = W. We started from the pre-trained original model  which obtains a 92.6%
accuracy on the test set. We implemented the entropy minimization by using L = 75 centers and
chose β = 0.1 such that the converged entropy would give a compression factor ≈ 20  i.e.  giving
≈ 32/20 = 1.6 bits per weight. The training was performed with the same learning parameters as
the original model was trained with (SGD with momentum 0.9). The annealing schedule used was a
simple exponential one  σ(t + 1) = 1.001 · σ(t) with σ(0) = 0.4. After 4 epochs of training  when
σ(t) has increased by a factor ≈ 20  we switched to hard assignments and continued ﬁne-tuning at
a 10× lower learning rate. 2 Adhering to the benchmark of [6  12  11]  we obtain the compression
factor by dividing the bit cost of storing the uncompressed weights as ﬂoats (464  154 × 32 bits) with
the total encoding cost of compressed weights (i.e.  L × 32 bits for the centers plus the size of the
compressed index stream).
Our compressible model achieves a comparable test accuracy of 92.1% while compressing the DNN
by a factor 19.15 with Huffman and 20.15 using arithmetic coding. Table 1 compares our results with
state-of-the-art approaches reported by [6]. We note that while the top methods from the literature
also achieve accuracies above 92% and compression factors above 20×  they employ a considerable
amount of hand-designed steps  such as pruning  retraining  various types of weight clustering  special
encoding of the sparse weight matrices into an index-difference based format and then ﬁnally use

2 We switch to hard assignments since we can get large gradients for weights that are equally close to two
centers as ˜Q converges to hard nearest neighbor assignments. One could also employ simple gradient clipping.

8

entropy coding. In contrast  we directly minimize the entropy of the weights in the training  obtaining
a highly compressible representation using standard entropy coding.
In Fig. 13 in Appendix A.8  we show how the sample entropy H(p) decays and the index histograms
develop during training  as the network learns to condense most of the weights to a couple of centers
when optimizing (6). In contrast  the methods of [12  11  6] manually impose 0 as the most frequent
center by pruning ≈ 80% of the network weights. We note that the recent works by [34] also manages
to tackle the problem in a single training procedure  using the minimum description length principle.
In contrast to our framework  they take a Bayesian perspective and rely on a parametric assumption
on the symbol distribution.

6 Conclusions
In this paper we proposed a uniﬁed framework for end-to-end learning of compressed representations
for deep architectures. By training with a soft-to-hard annealing scheme  gradually transferring
from a soft relaxation of the sample entropy and network discretization process to the actual non-
differentiable quantization process  we manage to optimize the rate distortion trade-off between the
original network loss and the entropy. Our framework can elegantly capture diverse compression
tasks  obtaining results competitive with state-of-the-art for both image compression as well as DNN
compression. The simplicity of our approach opens up various directions for future work  since our
framework can be easily adapted for other tasks where a compressible representation is desired.

Acknowledgments
This work was supported by EUs Horizon 2020 programme under grant agreement No 687757 –
REPLICATE  by NVIDIA Corporation through the Academic Hardware Grant  by ETH Zurich  and
by Armasuisse.

References
[1] BPG Image format. https://bellard.org/bpg/.
[2] Kodak PhotoCD dataset. http://r0k.us/graphics/kodak/.
[3] Eugene L Allgower and Kurt Georg. Numerical continuation methods: an introduction 

volume 13. Springer Science & Business Media  2012.

[4] Johannes Ballé  Valero Laparra  and Eero P Simoncelli. End-to-end optimization of nonlinear

transform codes for perceptual quality. arXiv preprint arXiv:1607.05006  2016.

[5] Johannes Ballé  Valero Laparra  and Eero P Simoncelli. End-to-end optimized image compres-

sion. arXiv preprint arXiv:1611.01704  2016.

[6] Yoojin Choi  Mostafa El-Khamy  and Jungwon Lee. Towards the limit of network quantization.

arXiv preprint arXiv:1612.01543  2016.

[7] Matthieu Courbariaux  Yoshua Bengio  and Jean-Pierre David. Binaryconnect: Training deep
neural networks with binary weights during propagations. In Advances in Neural Information
Processing Systems  pages 3123–3131  2015.

[8] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons 

2012.

[9] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei.

Hierarchical Image Database. In CVPR09  2009.

ImageNet: A Large-Scale

[10] Andre Esteva  Brett Kuprel  Roberto A Novoa  Justin Ko  Susan M Swetter  Helen M Blau  and
Sebastian Thrun. Dermatologist-level classiﬁcation of skin cancer with deep neural networks.
Nature  542(7639):115–118  2017.

[11] Song Han  Huizi Mao  and William J Dally. Deep compression: Compressing deep neural net-
works with pruning  trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 
2015.

[12] Song Han  Jeff Pool  John Tran  and William Dally. Learning both weights and connections
for efﬁcient neural network. In Advances in Neural Information Processing Systems  pages
1135–1143  2015.

[13] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June
2016.

9

[14] Jia-Bin Huang  Abhishek Singh  and Narendra Ahuja. Single image super-resolution from
transformed self-exemplars. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 5197–5206  2015.

[15] Itay Hubara  Matthieu Courbariaux  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Quan-
tized neural networks: Training neural networks with low precision weights and activations.
arXiv preprint arXiv:1609.07061  2016.

[16] Nick Johnston  Damien Vincent  David Minnen  Michele Covell  Saurabh Singh  Troy Chinen 
Sung Jin Hwang  Joel Shor  and George Toderici. Improved lossy image compression with
priming and spatially adaptive bit rates for recurrent networks. arXiv preprint arXiv:1703.10114 
2017.

[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR 

abs/1412.6980  2014.

[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[19] Alex Krizhevsky and Geoffrey E Hinton. Using very deep autoencoders for content-based

image retrieval. In ESANN  2011.

[20] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[21] Detlev Marpe  Heiko Schwarz  and Thomas Wiegand. Context-based adaptive binary arithmetic
coding in the h. 264/avc video compression standard. IEEE Transactions on circuits and systems
for video technology  13(7):620–636  2003.

[22] D. Martin  C. Fowlkes  D. Tal  and J. Malik. A database of human segmented natural images
and its application to evaluating segmentation algorithms and measuring ecological statistics.
In Proc. Int’l Conf. Computer Vision  volume 2  pages 416–423  July 2001.

[23] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet
classiﬁcation using binary convolutional neural networks. In European Conference on Computer
Vision  pages 525–542. Springer  2016.

[24] Kenneth Rose  Eitan Gurewitz  and Geoffrey C Fox. Vector quantization by deterministic

annealing. IEEE Transactions on Information theory  38(4):1249–1257  1992.

[25] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV)  115(3):211–252  2015.

[26] Wenzhe Shi  Jose Caballero  Ferenc Huszár  Johannes Totz  Andrew P Aitken  Rob Bishop 
Daniel Rueckert  and Zehan Wang. Real-time single image and video super-resolution using an
efﬁcient sub-pixel convolutional neural network. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 1874–1883  2016.

[27] Wenzhe Shi  Jose Caballero  Lucas Theis  Ferenc Huszar  Andrew Aitken  Christian Ledig  and
Zehan Wang. Is the deconvolution layer the same as a convolutional layer? arXiv preprint
arXiv:1609.07009  2016.

[28] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driess-
che  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  et al. Mas-
tering the game of go with deep neural networks and tree search. Nature  529(7587):484–489 
2016.

[29] David S. Taubman and Michael W. Marcellin. JPEG 2000: Image Compression Fundamentals 

Standards and Practice. Kluwer Academic Publishers  Norwell  MA  USA  2001.

[30] Lucas Theis  Wenzhe Shi  Andrew Cunningham  and Ferenc Huszar. Lossy image compression

with compressive autoencoders. In ICLR 2017  2017.

[31] Radu Timofte  Vincent De Smet  and Luc Van Gool. A+: Adjusted Anchored Neighborhood
Regression for Fast Super-Resolution  pages 111–126. Springer International Publishing  Cham 
2015.

[32] George Toderici  Sean M O’Malley  Sung Jin Hwang  Damien Vincent  David Minnen  Shumeet
Baluja  Michele Covell  and Rahul Sukthankar. Variable rate image compression with recurrent
neural networks. arXiv preprint arXiv:1511.06085  2015.

10

[33] George Toderici  Damien Vincent  Nick Johnston  Sung Jin Hwang  David Minnen  Joel Shor 
and Michele Covell. Full resolution image compression with recurrent neural networks. arXiv
preprint arXiv:1608.05148  2016.

[34] Karen Ullrich  Edward Meeds  and Max Welling. Soft weight-sharing for neural network

compression. arXiv preprint arXiv:1702.04008  2017.

[35] Gregory K Wallace. The JPEG still picture compression standard.

consumer electronics  38(1):xviii–xxxiv  1992.

IEEE transactions on

[36] Z. Wang  E. P. Simoncelli  and A. C. Bovik. Multiscale structural similarity for image quality
assessment. In Asilomar Conference on Signals  Systems Computers  2003  volume 2  pages
1398–1402 Vol.2  Nov 2003.

[37] Zhou Wang  A. C. Bovik  H. R. Sheikh  and E. P. Simoncelli. Image quality assessment: from
error visibility to structural similarity. IEEE Transactions on Image Processing  13(4):600–612 
April 2004.

[38] Wei Wen  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li. Learning structured sparsity in
deep neural networks. In Advances in Neural Information Processing Systems  pages 2074–2082 
2016.

[39] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-

ment learning. Machine learning  8(3-4):229–256  1992.

[40] Ian H. Witten  Radford M. Neal  and John G. Cleary. Arithmetic coding for data compression.

Commun. ACM  30(6):520–540  June 1987.

[41] Paul Wohlhart  Martin Kostinger  Michael Donoser  Peter M. Roth  and Horst Bischof. Optimiz-
ing 1-nearest prototype classiﬁers. In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR)  June 2013.

[42] Eyal Yair  Kenneth Zeger  and Allen Gersho. Competitive learning and soft competition for

vector quantizer design. IEEE transactions on Signal Processing  40(2):294–309  1992.

[43] Aojun Zhou  Anbang Yao  Yiwen Guo  Lin Xu  and Yurong Chen. Incremental network quanti-
zation: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044 
2017.

11

,Eirikur Agustsson
Fabian Mentzer
Michael Tschannen
Lukas Cavigelli
Radu Timofte
Luca Benini
Luc Gool