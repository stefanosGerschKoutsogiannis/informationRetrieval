2019,MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization,Tremendous amount of parameters make deep neural networks impractical to be deployed for edge-device-based real-world applications due to the limit of computational power and storage space. Existing studies have made progress on learning quantized deep models to reduce model size and energy consumption  i.e. converting full-precision weights ($r$'s) into discrete values ($q$'s) in a supervised training manner. However  the training process for quantization is non-differentiable  which leads to either infinite or zero gradients ($g_r$) w.r.t. $r$. To address this problem  most training-based quantization methods use the gradient w.r.t. $q$ ($g_q$) with clipping to approximate $g_r$ by Straight-Through-Estimator (STE) or manually design their computation. However  these methods only heuristically make training-based quantization applicable  without further analysis on how the approximated gradients can assist training of a quantized network. In this paper  we propose to learn $g_r$ by a neural network. Specifically  a meta network is trained using $g_q$ and $r$ as inputs  and outputs $g_r$ for subsequent weight updates. The meta network is updated together with the original quantized network. Our proposed method alleviates the problem of non-differentiability  and can be trained in an end-to-end manner. Extensive experiments are conducted with CIFAR10/100 and ImageNet on various deep networks to demonstrate the advantage of our proposed method in terms of a faster convergence rate and better performance. Codes are released at: \texttt{https://github.com/csyhhu/MetaQuant},MetaQuant: Learning to Quantize by Learning to

Penetrate Non-differentiable Quantization

Shangyu Chen

Wenya Wang

Nanyang Technological University  Singapore

Nanyang Technological University  Singapore

schen025@e.ntu.edu.sg

wangwy@ntu.edu.sg

Sinno Jialin Pan

Nanyang Technological University  Singapore

sinnopan@ntu.edu.sg

Abstract

Tremendous amount of parameters make deep neural networks impractical to be
deployed for edge-device-based real-world applications due to the limit of compu-
tational power and storage space. Existing studies have made progress on learning
quantized deep models to reduce model size and energy consumption  i.e. convert-
ing full-precision weights (r’s) into discrete values (q’s) in a supervised training
manner. However  the training process for quantization is non-differentiable  which
leads to either inﬁnite or zero gradients (gr) w.r.t. r. To address this problem  most
training-based quantization methods use the gradient w.r.t. q (gq) with clipping
to approximate gr by Straight-Through-Estimator (STE) or manually design their
computation. However  these methods only heuristically make training-based
quantization applicable  without further analysis on how the approximated gra-
dients can assist training of a quantized network. In this paper  we propose to
learn gr by a neural network. Speciﬁcally  a meta network is trained using gq
and r as inputs  and outputs gr for subsequent weight updates. The meta network
is updated together with the original quantized network. Our proposed method
alleviates the problem of non-differentiability  and can be trained in an end-to-end
manner. Extensive experiments are conducted with CIFAR10/100 and ImageNet
on various deep networks to demonstrate the advantage of our proposed method in
terms of a faster convergence rate and better performance. Codes are released at:
https://github.com/csyhhu/MetaQuant

1

Introduction

Deep neural networks have shown promising results in various computer vision tasks. However 
modern deep learning models usually contain many layers and enormous amount of parameters [9] 
which limits their applications on edge devices. To reduce parameters redundancy  continuous effects
in architecture reﬁnement have been made  such as using small kernel convolutions [14] and reusing
features [6]. Consider a very deep model which is fully-trained. To use it for making predictions 
most of the computations involve multiplications of a real-valued weight by a real-valued activation
in a forward pass. These multiplications are expensive as they are all ﬂoat-point to ﬂoat-point
multiplication operations. To alleviate this problem  a number of approaches have been proposed to
quantize deep models. Courbariaux et al. [4] and Hubara et al. [7] proposed to binarize weights of the
deep model to be in {±1}. To provide more ﬂexibility for quantized values in each layer  Rastegari et
al. [13] introduced a ﬂoat value αl known as the scaling factor for layer l to turn binarized weights

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

into αl × {±1}. Li et al. [11] extended binary weights to ternary values  and Zhou et al. [17] further
incorporated activation and gradient quantization.
Training-based quantization methods generate quantized neural networks under the training mech-
anism. Existing training-based quantization methods can be roughly categorized into “STE” and
“Non-STE” methods. “STE” methods contain a non-differentiable discrete quantization function 
connecting the full-precision weights and quantized weights. During backpropagation  STE is used
to penetrate this non-differentiable function. (e.g.[7]  [13]  [17]). “Non-STE” methods are referred to
as learning without STE by directly working on full-precision weights with a regularizer to obtain
feasible quantization ([2]) or weights projection using proximal gradient methods ([10]  [5]). The
training process in Non-STE quantization suffers from heavy hyper-parameters tuning  such as
weights partition portion in each step [15] and penalty setting in [10].
Speciﬁcally  STE quantization methods follow a rather simple and standard training protocol. Given
a neural network f with full-precision weights W  a quantization function Q(·) (without loss of
generalization  Q(r) is set as a mapping from r to 1 if r ≥ 0  otherwise −1)  and labeled data
(x  y)  the objective is to minimize the training loss: (cid:96)(f (Q(W); x)  y). However  due to the
non-differentiability of Q  the gradient of (cid:96) w.r.t W cannot be computed using the chain rule:
∂W is inﬁnite when W = 0 and 0 elsewhere. To enable a stable
∂W = ∂l
∂(cid:96)
quantization training  Hubara et al. [7] proposed Straight-Through-Estimator (STE) to redeﬁne
∂Q(r)

∂W   where ∂Q(W)

∂Q(W)

∂Q(W)

:

∂r

(cid:26)1

0

∂Q(r)

∂r

=

|r| ≤ 1 

if
otherwise.

.

∂(cid:96)

∂r

STE is widely used in training-based quantization methods1 as it provides an approximated gradient
for penetration of Q with an simple implementation. However  it inevitably brings the problem of
gradient mismatch: the gradients of the weights are not generated using the value of weights  but
rather its quantized value. Although STE provides an end-to-end training beniﬁt under discrete
constraints  few works have progressed to investigate how to obtain better gradients for quantization
training. In the methods HWGQ [3] and Bi-real [12]  ∂Q(r)
is manually deﬁned  but they focused on
activation quantization.
To overcome the problem of gradient mismatch and explore better gradients in training-based methods 
∂W by a neural network (M) in quantization training. This
inspired by [1]  we propose to learn ∂Q(W)
additional neural network is referred to as meta quantizer and trained together with the base quantized
model. The whole process is denoted by Meta Quantization (MetaQuant). Specially  in each
backward propagation  M takes
∂Q(W) and W as inputs in a coordinate-wise manner  then its
output is used to compute ∂(cid:96)
∂W for updating weights W using common optimization methods such
as SGD or Adam [8]. In a forward pass  inference is performed using the quantized version of
the updated weights  which produces the ﬁnal outputs to be compared with the ground-truth labels
for backward computation. During this process  gradient propagation from the quantized weights
to the full-precision weights is handled by M  which avoids the problem of non-differentiability
and gradient mismatch. Besides  the gradients generated by the meta quantizer are loss-aware 
contributing to better performance of the quantization training.
Compared with commonly-used STE and manually designed gradient propagation in quantization
training  MetaQuant learns to generate proper gradients without any manually designed knowledge.
The whole process is end-to-end. meta quantizer can be viewed as a plug-in to any base model  making
it easy and general to be implemented in modern architectures. After quantization training is ﬁnished 
meta quantizer can be removed and consumes no extra space for inference. We compare MetaQuant
with STE under different quantization functions (dorefa [17]  BWN [13]) and optimization techniques
(SGD  Adam) with CIFAR10/100 and ImageNet on various base models to verify MetaQuant’s
generalizability. Extensive experiments show that MetaQuant achieves a faster convergence speed
under SGD and better performance under SGD/Adam.

1In the following description  training-based quantization refers to STE training-based quantization

2

2 Related Work

Courbariaux et al. [4] proposed to train binarized networks through deterministic and stochastic
rounding on parameters update after backpropagation. This idea was further extended in [7] and
[13] by introducing binary activation. Nevertheless  these pioneer attempts face the problem of
non-differentiable rounding operator during back-propagation  which is solved by directly penetration
of rounding with unchanged gradient. To bypass non-differentiability  Leng et al. [10] modiﬁed the
quantization training objective function using ADMM  which separates the processes on training real-
valued parameters and quantizing the updated parameters. Zhou et al. [15] proposed to incrementally
quantize a portion of parameters based on weight partition and update the un-quantized parameters
by normal training. However  this kind of methods introduced more hyper-parameters tuning such
as determining the procedure of partial quantization  thus complicating quantization. Bai et al. [2]
added a regularizer in quantization training to transform full-precision weights to quantized values.
Though this method simpliﬁes quantization training procedure  but its optimization process involves
the proximal method  which makes the training cost expensive.

3 Problem Statement
Given a training set of n labeled instances {x  y}’s  a pre-trained full-precision base model f with
L layers is parameterized by W = [W1  ...  WL]. We deﬁne a pre-processing function A(·) and a
quantization function Q(·). A(·) converts W into ˜W  which is rescaled and centralized to make it
easier for quantization. Q(·) discretizes ˜W to ˆW using k-bits. Specially  2 pre-processing functions
and corresponding quantization methods (dorefa2  BWN) are studied in this work:

(cid:105)

(cid:104)

(2k − 1) ˜W
2k − 1

−1. (1)

(2)

dorefa : ˜W =A(W) =

tanh(W)

2max(|tanh(W)|)

+

1
2

 

ˆW = Q( ˜W) = 2

round

BWN : ˜W = A(W) = W 

ˆW = Q( ˜W) =

|| ˜W||l1 × sign( ˜W).

1
n

Training-based quantization aims at training a quantized version of W  i.e.  ˆW  such that the loss of
the quantized f is minimized: min ˆW (cid:96)(f ( ˆW; x)  y).

4 Meta Quantization

4.1 Generation of Meta Gradient
Our proposed MetaQuant incorporates a shared meta quantizer Mφ parameterized by φ across layers
into quantization training. After W is quantized as ˆW (subscript l is omitted for ease of notation)  a
loss (cid:96) is generated by comparing f ( ˆW; x) with the ground-truth.
In back-propagation  the gradient of (cid:96) w.r.t ˆW is then computed by chain rules  which is denoted by
. The meta quantizer Mφ receives g ˆW and ˜W as inputs  and outputs the gradient of (cid:96)
g ˆW = ∂(cid:96)
∂ ˆW
w.r.t. ˜W  denoted by g ˜W  as:

g ˜W =

∂(cid:96)
∂ ˜W

= Mφ(g ˆW  ˜W).

(3)

The gradient g ˜W is further used to compute the gradient of (cid:96) w.r.t. W  denoted by gW  where gW is
computed via:

gW =

∂(cid:96)
∂ ˜W

∂ ˜W
∂W

= g ˜W

∂ ˜W
∂W

= Mφ(g ˆW  ˜W)

∂ ˜W
∂W

 

(4)

where ∂ ˜W
dorefa according to (1)  and ∂ ˜W
calibration.

∂W depends on the pre-processing function between W and ˜W: ∂ ˜W

max(|tanh(W)|) for
∂W = 1 for BWN according to (2). This process is referred to as

∂W = 1−tanh2(W)

2In this work  we only consider the forward quantization function for weights quantization used in [17]  and

denote it as “dorefa”

3

Figure 1: The overﬂow of MetaQuant. During backward propagation  gradients are represented as
blue line. Dash blue line means this propagation is non-differentiable and requires special handling.
A shared meta network M is constructed which takes g ˆW and ˜W as input  and outputs the gradient
of ˜W (g ˜W). With g ˜W  the gradient of the weights W can be computed using (4). Finally  W is
updated with (5)  with the assistance of different optimization methods reﬂected in π(·).

Before using gW to update W  gW is ﬁrstly processed according to different optimization methods
to produce the ﬁnal update value for each weight. This process is named gradient reﬁnement  which
is denoted by π(·) in the sequent. Speciﬁcally  for SGD  π(gW) = gW. For other optimization
methods such as Adam  π(·) can be implemented as π(gW) = gW + residual  where “residual” is
computed according to different gradient reﬁnement methods. Finally  the full-precision weights W
is updated as:

Wt+1 = Wt − απ(gt

(5)
where t denotes the t-th training iteration and α is the learning rate. Fig.1 illustrates the overall
procedure of MetaQuant.
Compared with [1]  which directly learns gW  MetaQuant construct a neural network to learn g ˜W 
which cannot be directly computed in quantization training due to the property of non-differentiability
of the quantization functions. Our work resolves the issue of non-differentiability and is general to
different optimization methods. Insight of how and why MetaQuant works is studied at Appendix.7.2.

W) 

4.2 Training of Meta Quantizer

= Mφ(g ˆWi

Similar to [1]  our proposed meta quantizer is a
coordinate-wise neural network  which means
that each weight parameter is processed inde-
pendently. For a single weight index i in g ˆWi
 
˜Wi receives its corresponding gradient g ˜Wi
via
  ˜Wi). For efﬁcient process-
g ˜Wi
ing  during inference  the inputs in (3) are ar-
ranged as batches with size 1. Specially  suppose
W comes from a convolution layer with shape
Ro×i×k×k  where o  i and k denote the number
of output channels  input channels and kernel
size  respectively. Then ˜W  ˆW and the corre-
sponding gradient share the same shape  which
is a reshaping of inputs in (3) to R(o×i×k2)×1.
Recall from (5) and (4)  the output of Mφ is in-
corporated into the value of updated Wt  which
is then quantized in next iteration’s inference.

Figure 2: Incorporation of meta quantizer into
quantization training. ∆W is composed of cal-
ibration  gradient reﬁnement and multiplication
of learning rate α. Output of meta quantizer is
involved in W’s update and contributes to ﬁnal
loss  constructing a differential path from loss to
φ-parameterized meta quantizer.

4

Therefore  Mφ is associated to the ﬁnal quantization training loss  which receives gradient update on
φ backpropagated from the ﬁnal loss. By introducing the meta quantizer to produce g ˜W  MetaQuant
not only addresses the non-differentiability issue for parameters in the base model  but also provides
an end-to-end training beneﬁt throughout the whole network. Moreover  the meta quantizer is loss-
aware  hence it is trained to generate more accurate update for W for reducing the ﬁnal loss  which
explores how gradient can be modiﬁed to assist quantization training. Figure.2 illustrates the detailed
process when incorporating the meta quantizer into the quantization training of the base model  which
forms a differentiable path from the ﬁnal loss to φ. In the meantime of quantization training in W  φ
is also learned in each training iteration t:

(cid:34)

(cid:35)

∂ ˜Wt−1
∂Wt−1 )

 

(6)

(7)

Forward:

˜Wt = A(Wt) = A

Wt−1 − α × π(Mφ(gt−1

  ˜Wt−1)

ˆW

(cid:16)

(cid:104)

(cid:105)

(cid:17)

Loss = (cid:96)

f

∂(cid:96)
∂φt =

∂(cid:96)
∂ ˜Wt

 

  y

Q( ˜Wt); x
∂ ˜Wt
∂φt = Mφ(g ˆWt  ˜Wt)

∂ ˜Wt
∂φt .

Backward:

(8)
In Forward  we use a combination of Wt−1 and meta gradient to represent Wt  in order to incorporate
Mφ. Specially in (6)  meta gradient is derived from M’s output  which is ﬁrstly multiplied to achieve
gradient of W  then is reﬁned by optimization π. Finally  it is adjusted by learning rate to become
∂φ is differentiable because A is differentiable.
meta gradient. (8) calculates gradient of φ  here ∂ ˜W
Furthermore  a differentiable meta neural network is chosen. Wt will be actually updated after
Backward  which can be regarded as late weights update.

4.3 Design of Meta Quantizer
The meta quantizer Mφ is a parameterized and differentiable neural network to generate the meta
gradient. It can be viewed as a generalization of STE. For example  Mφ reduces to STE if it clips
g ˜W according to the absolute magnitude of ˜W: g ˜W = Mφ(g ˆW  ˜W) = g ˆW · 1| ˜W|≤1.
We design 3 different architectures of the meta quantizer. The ﬁrst architecture simply uses a neural
network composing of 2 or multiple layers of fully-connected layer. It only requires g ˆW as input:

FCGrad : Mφ(g ˆW) = FCs(φ  σ  g ˆW) 

(9)
where σ represents the nonlinear activation. Since previous successful experimental results brought
by STE show that a good g ˜W should be generated by considering the value of ˜W. Based on this
observation  we construct another 2 architectures of meta quantizer with ˜W fed as input and multiply
the output of these neural networks with g ˆW to incorporate gradient information from its subsequent
step. Speciﬁcally  one is based on fully-connected (FC) layers:

MultiFC : Mφ(g ˆW  ˜W) = g ˆW · FCs(φ  σ  ˜W).

(10)
Another network incorporates LSTM and FC to construct M  which is inspired by [1] that uses
memory-based neural network as the meta learner:

LSTMFC : Mφ(g ˆW  ˜W) = g ˆW · FCs(φF Cs  σ  (LSTM(φLST M   ˜W))).

(11)
When using LSTM as the meta quantizer  each coordinate of the weights keeps a track of the hidden
states generated by LSTM  which contains the memory of historical information of g ˆW and ˜W. Meta
quantizer’s memory consumption and detailed hyper-parameter is studied at Appendix.7.1  7.3.

4.4 Algorithm and Implementation Details
The detailed process of MetaQuant is illustrated in Algorithm 1. A shared meta quantizer Mφ is ﬁrstly
constructed and randomely initialized. During each training iteration  line 2-6 describes the forward
process: for each layer  g ˆW and ˜W from the previous iteration are fed into Mφ to generate the meta
gradient g ˜W to perform inference  as indicated from line 3-5. Since g ˆW is not calculated in the ﬁrst
iteration  normal quantization training is conducted at the ﬁrst iteration: ˆW = Q( ˜W) = Q [A(W)]
to replace line 4. Line 7-9 shows the backward process: ˆW’s gradient can be attained through

5

error backpropagation  which is shown in line 7. During the backward process  g ˆW and ˜W of the
current iteration are obtained and their outputs from Mφ are saved for computation in the next
iteration  denoted by g ˜Wt+1 as described in line 7-8. By incorporating Mφ into the inference graph 
its gradient is obtained in line 9. Finally  g ˜W is used to calculate gW  which is then processed by
different optimization methods using π(·)  leading to the update of W shown in line 10-12. In the
ﬁrst iteration  due to the lack of g ˜W  weights update of W is not conducted. Note that φ from the
meta quantizer is updated in line 13.

(cid:21)(cid:27)

)

ˆWl

) · ∂ ˜Wt−1
∂Wt−1

l

l

(cid:20)
(cid:104)

(cid:26)
(cid:110)

Wt−1

l − α × π(Mφ(gt−1

for Layer l from 1 to L do
A

l = Q( ˜Wt

l ) = Q

Algorithm 1 MetaQuant
Require: Training dataset {x  y}n  well-trained full-precision base model W.
Ensure: Quantized base model ˆW.
1: Construct shared meta quantizer Mφ  training iteration t = 0.
2: while not optimal do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end while

(cid:105)
ˆWt
end for
Q( ˜Wt); x
Calculate loss: (cid:96) = Loss
Generate g ˆWt using chain rules.
Calculate meta gradient g ˜Wt using Mφ.
Calculate ∂(cid:96)
for Layer l from 1 to L do

Wt
end for
φt+1 = φt − γ × ∂(cid:96)
t = t + 1

∂φt (γ is the learning rate of the meta quantizer)

l = Wt−1

l − α × π(Mφ(gt−1

) · ∂ ˜Wt−1
∂Wt−1

l

∂φt by (8)

  ˜Wt−1

l

  ˜Wt−1

l

(cid:111)

  y

f

ˆWl

)

l

5 Experiment

5.1 Experiment Setup

MetaQuant focuses on the penetration of non-differentiable quantization function during training-
based methods. We conduct comparison experiments with STE under the following 2 forward
quantization methods: 1) dorefa [17]   2) BWN [13] and 2 optimization methods: 1) SGD 2) Adam
[8]. When quantization training is conducted with dorefa or BWN as forward quantization function
and STE as backward method  it becomes a weight-quantization version of [17] or the proposed
method in [13]  respectively. Three benchmark datasets are used including ImageNet ILSVRC-2012
and CIFAR10/100. Regarding deep architectures  we experiment with ResNet20/32/44 on CIFAR10.
Since CIFAR10/100 share the same input dimension  we modify the output dimension of the last
fully-connected layer from 10 to 100 in ResNet56/110 for CIFAR100. For ImageNet  ResNet18 is
utilized for comparison. For all the experiments conducted and compared  all layers in the networks
are quantized using 1 bit: each layer contains only 2 values. For experiments on CIFAR10/100  we set
the initial learning rate as α = 1e−3 for base models and the initial learning rate as γ = 1e−3 for the
meta quantizer. For fair comparison  we set total training epochs as 100 for all experiments  α and γ
will be divided by 10 after every 30 epochs. For ImageNet  the initial learning rate is set as α = 1e−4
for the base model using dorefa and BWN. Initial γ is set as 1e−3. α decreases to {1e−5  1e−6}
when training comes to 10 / 20 epochs. γ reduces to {1e−4  1e−5} in accordance to the change of
the learning rate in base models with total epoch as 30. Batch size is 128 for CIFAR/ImageNet. All
experiments are conducted for 5 times  the statistics of last 10/5 epochs’ test accuracy are reported
as the performance of both proposed and baseline methods in CIFAR/ImageNet datasets. We also
demonstrate the empirical convergence speed among different methods through training loss curves.
Detailed hyper-parameters in different realizations of MetaQuant in CIFAR experiments are the
following: In MultiFC  a 2-layer fully-connected layer is used with hidden size as 100  no non-linear
activation is used. In LSTMFC  a 1-layer LSTM and a fully-connected layer are utilized  with the
hidden dimension set as 100. In FCGrad  a 2-layer fully-connected meta model is used with hidden

6

MultiFC
LSTMFC
FCGrad

STE

MultiFC
LSTMFC
FCGrad

STE

LSTMFC
FCGrad

STE

LSTMFC
FCGrad

STE

MultiFC
LSTMFC
FCGrad

STE

MultiFC
LSTMFC
FCGrad

STE

LSTMFC
FCGrad

STE

LSTMFC
FCGrad

STE

MultiFC
LSTMFC
FCGrad

STE

MultiFC
LSTMFC
FCGrad

STE

LSTMFC
FCGrad

STE

dorefa

BWN

dorefa

BWN

dorefa

BWN

ResNet20

ResNet32

ResNet44

LSTMFC
FCGrad

SGD

Adam

SGD

Adam

SGD

Adam

SGD

Adam

SGD

Adam

SGD

Adam

FP Acc (%)

91.5

92.13

93.56

Test Acc (%)
80.745(2.113)
88.942(0.466)
88.305(0.810)
88.840(0.291)
89.782(0.172)
89.941(0.068)
89.979(0.103)
89.962(0.068)
75.913(3.495)
89.289(0.212)
88.949(0.231)
89.896(0.182)
90.036(0.109)
90.042(0.098)
82.911(1.680)
89.637(0.380)
90.397(0.149)
89.934(0.246)
90.172(0.077)
90.966(0.064)
90.948(0.074)
90.976(0.068)
79.768(2.062)
90.568(0.169)
90.241(0.316)
91.015(0.087)
91.002(0.077)
91.034(0.067)
86.686(1.020)
90.546(0.218)
91.494(0.163)
91.539(0.097)
91.079(0.064)
91.772(0.073)
91.870(0.022)
91.989(0.067)
82.647(0.334)
91.498(0.057)
91.614(0.081)
91.121(0.023)
91.498(0.271)
92.107(0.059)

size as 100 without non-linear activation. In ImageNet experiments  we use MultiFC/FCGrad with
2/1-layer fully-connected layer  whose hidden dimension is 100.

5.2 Experimental Results and Analysis

Network

Forward Backward Optimization

STE

Table 1: Experimental result of MetaQuant and STE using dorefa  BWN on CIFAR10

Table.1 shows the overall experimental results on CIFAR10 for MetaQuant and STE using dif-
ferent forward quantization methods and optimizations. Variants of MetaQuant shows signiﬁcant
improvement over STE baseline  especially SGD is used.
CIFAR100 is a more difﬁcult task than CIFAR10  which contains much more ﬁne-grained classes
with a total number of 100 classes. Table.2 shows the overall experimental results on CIFAR100
for MetaQuant and STE using different forward quantization methods and optimizations. Similar to
CIFAR10  MetaQuant out-performs by a large margin than STE in all cases  showing that MetaQuant
has signiﬁcant improvement in more challenging tasks than traditional methods.

5.3 Empirical Convergence Analysis

In this experiment  we compare the performances of variants of MetaQuant and STE during the
training process to demonstrate their convergence speeds. ResNet20 using dorefa is utilized as an
example. As Fig.3 shows  under the same task and forward quantization method  MetaQuant shows
tremendous convergence advantage over STE using SGD  including much faster descending speed of
loss and obviously lower loss values. In Adam  although all the methods show similar decreasing

7

Network

Forward Backward Optimization

STE

ResNet56

ResNet110

MultiFC
LSTMFC
FCGrad

STE

MultiFC
LSTMFC
FCGrad

STE

LSTMFC
FCGrad

STE

LSTMFC
FCGrad

STE

MultiFC
LSTMFC
FCGrad

STE

MultiFC
LSTMFC
FCGrad

STE

LSTMFC
FCGrad

STE

dorefa

BWN

dorefa

BWN

LSTMFC
FCGrad

SGD

Adam

SGD

Adam

SGD

Adam

SGD

Adam

FP Acc (%)

71.22

72.54

Test Acc (%)
42.265(8.143)
65.791(0.415)
63.645(2.183)
64.351(0.935)
66.419(0.533)
66.588(0.375)
66.483(0.793)
66.564(0.351)
34.479(11.737)
63.346(2.253)
64.402(1.434)
64.297(1.309)
66.584(0.349)
67.018(0.329)
43.419(18.902)
68.269(0.136)
64.753(2.850)
66.145(2.490)
66.836(1.198)
68.418(0.235)
67.138(1.286)
68.741(0.363)
35.227(19.408)
66.242(2.979)
64.791(4.096)
66.265(1.429)
67.767(1.391)
69.114(0.181)

Table 2: Experimental result of MetaQuant and STE using dorefa  BWN on CIFAR100

Network

Forward Backward Optimization

FP Top1/Top5(%)

Quant Top1/Top5 (%)

dorefa

ResNet18

58.349(2.072)/81.477(1.567)
59.472(0.025)/82.410(0.010)
59.835(0.359)/82.671(0.232)
59.503(0.835)/82.549(0.506)
60.328(0.391)/83.025(0.234)
Table 3: Experimental result of MetaQuant and STE using dorefa  BWN on ImageNet.

MultiFC
FCGrad

69.76/89.08

STE

FCGrad

BWN

STE

Adam

(a) SGD

(b) Adam

Figure 3: Convergence speed of MetaQuant V.S STE using SGD/Adam in ResNet20  CIFAR10 
dorefa.

speed  MetaQuant methods ﬁnally reach to lower loss values  which is also reﬂected in the test
accuracy reported in Table.1. Overall  MetaQuant shows better convergence than STE using different
forward quantizations and optimizations. The improvement is more obvious when SGD is chosen.

8

020406080100epoch100lossBaselineLSTMFCFC-GradMultiFC020406080100epoch100lossBaselineLSTMFCFC-GradMultiFCWe conjecture that the performance difference between SGD and Adam is due to the following reason:
SGD simply updates full-precision weights using the calibrated gradient from g ˜W  which directly
reﬂects the output of meta quantizercompared to STE. Adam aggregates the historical information
of gW and normalizes the current gradient  which to a certain degree shrinks the difference of meta
quantizer and STE. More comparisons in training accuracy  test accuracy on more tasks are listed in
Appendix.7.4.

5.4 Performance Comparison with Non-STE Training-based Quantization

Network
ResNet20

ResNet44

Method

ProxQuant
MetaQuant
ProxQuant
MetaQuant

ELQ

Acc Drop (%)

1.29
0.7
0.99
0.08

3.55/2.65
6.32/4.31

ResNet18
Table 4: Experimental result of MetaQuant V.S ProxQuant  LAB  ELQ  TTQ.

ResNet18-2bits

MetaQuant

Network
ResNet32

LABNet

Method

ProxQuant
MetaQuant

LAB

MetaQuant
TTQ [18]
MetaQuant

Acc Drop (%)

1.28
0.39
1.4
-0.2

3.00/2.00
5.17/3.59

MetaQuant aims at improving training-based quantization by learning better gradients for penetration
of non-differentiable quantization functions. Some advanced quantization methods avoid discrete
quantization. In this section  we compare MetaQuant with Non-STE training-based quantization:
ProxQuant ([2])  LAB ([5]) to demonstrate that traditional STE training-based quantization is able to
achieve better performance by using MetaQuant.
Due to the difference of the initial full-precision model used  we only report the performance drop
in terms of test accuracy after quantization (the smaller the better). We compare MetaQuant with
ProxQuant using ResNet20/32/44  LAB using its proposed architecture3 on CIFAR10 with all layers
quantized to binary values. As shown in Table.4  MetaQuant shows better performance than both
baselines.
ELQ ([16]) and TTQ ([18]) are compared in 3rd row in Table.4 using ImageNet datasets. Although
over-performance  ELQ is a combination of a series of previous quantization methods and tricks
on incremental quantization. MetaQuant focuses more on how to improve STE-based training
quantization  without any extra loss and training tricks. TTQ is a non-symmetric ternarization with
{0  α −β} as ternary points. MetaQuant follows dorefa using a symmetric quantization which leads
to efﬁcient inference.

5.5 MetaQuant Training Analysis

Training of MetaQuant involves computation in training of meta quantizer. To analyze the additional
training time  training time per iteration as for MetaQuant using MultiFC and DoReFa with STE
using ResNet20 in CIFAR10 (Intel Xeon CPU E5-1650 with GeForce GTX 750 Ti). MetaQuant
costs 51.15 seconds to ﬁnish one iteration of training while baseline method uses 38.17s. However 
In real deployment meta quantizer is removed  MetaQuant is able to provide better test performance
without any extra inference time.

6 Conclusion

In this paper  we propose a novel method (MetaQuant) to learn the gradient for penetration of the
non-differentiable quantization function in training-based quantization by a meta quantizer. This
meta network is general enough to be incorporated into various base models and can be updated using
the loss of the base models. We propose 3 types of meta quantizer and show that the meta gradients
generated through these modules are able to provide better convergence speed and ﬁnal quantization
performance  under different forward quantization functions and optimization methods.

3(2x128C3)-MP2-(2x256C3)-MP2-(2x512C3)-MP2-(2x1024FC)-10FC

9

Acknowledgement

This work is supported by NTU Singapore Nanyang Assistant Professorship (NAP) grant
M4081532.020  and Singapore MOE AcRF Tier-2 grant MOE2016-T2-2-06.

Reference
[1] Marcin Andrychowicz  Misha Denil  Sergio Gomez  Matthew W Hoffman  David Pfau  Tom
Schaul  Brendan Shillingford  and Nando De Freitas. Learning to learn by gradient descent by
gradient descent. In Advances in Neural Information Processing Systems  pages 3981–3989 
2016.

[2] Yu Bai  Yu-Xiang Wang  and Edo Liberty. Proxquant: Quantized neural networks via proximal

operators. In International Conference of Learning Representation  2019.

[3] Zhaowei Cai  Xiaodong He  Jian Sun  and Nuno Vasconcelos. Deep learning with low precision
by half-wave gaussian quantization. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 5918–5926  2017.

[4] Matthieu Courbariaux  Yoshua Bengio  and Jean-Pierre David. Binaryconnect: Training deep
neural networks with binary weights during propagations. In Advances in neural information
processing systems  pages 3123–3131  2015.

[5] Lu Hou  Quanming Yao  and James T Kwok. Loss-aware binarization of deep networks. In

International Conference of Learning Representation  2017.

[6] Gao Huang  Zhuang Liu  Kilian Q Weinberger  and Laurens van der Maaten. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition  volume 1  page 3  2017.

[7] Itay Hubara  Matthieu Courbariaux  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Binarized
neural networks. In Advances in neural information processing systems  pages 4107–4115 
2016.

[8] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-

tional Conference of Learning Representation  2015.

[9] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[10] Cong Leng  Hao Li  Shenghuo Zhu  and Rong Jin. Extremely low bit neural network: Squeeze

the last bit out with admm. In AAAI Conference on Artiﬁcial Intelligence  2017.

[11] Fengfu Li  Bo Zhang  and Bin Liu. Ternary weight networks. International Workshop on
Efﬁcient Methods for Deep Neural Networks  Advances in Neural Information Processing
Systems  2016.

[12] Zechun Liu  Baoyuan Wu  Wenhan Luo  Xin Yang  Wei Liu  and Kwang-Ting Cheng. Bi-real
net: Enhancing the performance of 1-bit cnns with improved representational capability and
advanced training algorithm. In Proceedings of the European Conference on Computer Vision 
pages 722–737  2018.

[13] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet
classiﬁcation using binary convolutional neural networks. In European Conference on Computer
Vision  pages 525–542. Springer  2016.

[14] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. In International Conference on Learning Representations  2015.

[15] Aojun Zhou  Anbang Yao  Yiwen Guo  Lin Xu  and Yurong Chen.

Incremental network
quantization: Towards lossless cnns with low-precision weights. In International Conference of
Learning Representation  2017.

10

[16] Aojun Zhou  Anbang Yao  Kuan Wang  and Yurong Chen. Explicit loss-error-aware quantization
for low-bit deep neural networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 9426–9435  2018.

[17] Shuchang Zhou  Yuxin Wu  Zekun Ni  Xinyu Zhou  He Wen  and Yuheng Zou. Dorefa-net:
Training low bitwidth convolutional neural networks with low bitwidth gradients. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition  2016.

[18] Chenzhuo Zhu  Song Han  Huizi Mao  and William J Dally. Trained ternary quantization. In

International Conference of Learning Representation  2016.

11

,Song Zhou
Swati Gupta
Madeleine Udell
Shangyu Chen
Wenya Wang
Sinno Jialin Pan