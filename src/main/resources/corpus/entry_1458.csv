2019,Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks,We present a comprehensive study of multilayer neural networks with binary activation  relying on the PAC-Bayesian theory. Our contributions are twofold: (i) we develop an end-to-end framework to train a binary activated deep neural network  (ii) we provide nonvacuous PAC-Bayesian generalization bounds for binary activated deep neural networks. Our results are obtained by minimizing the expected loss of an architecture-dependent aggregation of binary activated deep neural networks. Our analysis inherently overcomes the fact that binary activation function is non-differentiable. The performance of our approach is assessed on a thorough numerical experiment protocol on real-life datasets.,Dichotomize and Generalize: PAC-Bayesian Binary

Activated Deep Neural Networks

Gaël Letarte
Université Laval

Canada

gael.letarte.1@ulaval.ca

Benjamin Guedj

Inria and University College London

France and United Kingdom
benjamin.guedj@inria.fr

Pascal Germain

Inria
France

pascal.germain@inria.fr

François Laviolette

Université Laval

Canada

francois.laviolette@ift.ulaval.ca

Abstract

We present a comprehensive study of multilayer neural networks with binary
activation  relying on the PAC-Bayesian theory. Our contributions are twofold:
(i) we develop an end-to-end framework to train a binary activated deep neural
network  (ii) we provide nonvacuous PAC-Bayesian generalization bounds for
binary activated deep neural networks. Our results are obtained by minimizing the
expected loss of an architecture-dependent aggregation of binary activated deep
neural networks. Our analysis inherently overcomes the fact that binary activation
function is non-differentiable. The performance of our approach is assessed on a
thorough numerical experiment protocol on real-life datasets.

1

Introduction

The remarkable practical successes of deep learning make the need for better theoretical understanding
all the more pressing. The PAC-Bayesian theory has recently emerged as a fruitful framework to
analyze generalization abilities of deep neural network. Inspired by precursor work of Langford and
Caruana [2001]  nonvacuous risk bounds for multilayer architectures have been obtained by Dziugaite
and Roy [2017]  Zhou et al. [2019]. Although informative  these results do not explicitly take into
account the network architecture (number of layers  neurons per layer  type of activation function). A
notable exception is the work of Neyshabur et al. [2018] which provides a PAC-Bayesian analysis
relying on the network architecture and the choice of ReLU activation function. The latter bound
arguably gives insights on the generalization mechanism of neural networks (namely in terms of the
spectral norms of the learned weight matrices)  but their validity hold for some margin assumptions 
and they are likely to be numerically vacuous.
We focus our study on deep neural networks with a sign activation function. We call such networks
binary activated multilayer (BAM) networks. This specialization leads to nonvacuous generalization
bounds which hold under the sole assumption that training samples are iid. We provide a PAC-
Bayesian bound holding on the generalization error of a continuous aggregation of BAM networks.
This leads to an original approach to train BAM networks  named PBGNet. The building block of
PBGNet arises from the specialization of PAC-Bayesian bounds to linear classiﬁers [Germain et al. 
2009]  that we adapt to deep neural networks. The term binary neural networks has been coined by
Bengio [2009]  and further studied in Hubara et al. [2016  2017]  Soudry et al. [2014]: it refers to
neural networks for which both the activation functions and the weights are binarized (in contrast

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

with BAM networks). These architectures are motivated by the desire to reduce the computation and
memory footprints of neural networks.
Our theory-driven approach is validated on real life datasets  showing competitive accuracy with
tanh-activated multilayer networks  and providing nonvacuous generalization bounds.
Organisation of the paper. We formalize our framework and notation in Section 2  along with
a presentation of the PAC-Bayes framework and its specialization to linear classiﬁers. Section 3
illustrates the key ideas we develop in the present paper  on the simple case of a two-layers neural
network. This is then generalized to deep neural networks in Section 4. We present our main
theoretical result in Section 5: a PAC-Bayesian generalization bound for binary activated deep neural
networks  and the associated learning algorithm. Section 6 presents the numerical experiment protocol
and results. The paper closes with avenues for future work in Section 7.

2 Framework and notation

and D :=(cid:80)L

We stand in the supervised binary classiﬁcation setting: given a real input vector1 x ∈ Rd0  one wants
to predict a label y ∈ {−1  1}. Let us consider a neural network of L fully connected layers with a
(binary) sign activation function: sgn(a) = 1 if a > 0 and sgn(a) = −1 otherwise.2 We let dk denote
the number of neurons of the kth layer  for k ∈ {1  . . .   L}; d0 is the input data point dimension 
k=1 dk−1dk is the total number of parameters. The output of the (deterministic) BAM
network on an input data point x ∈ Rd0 is given by

fθ(x) = sgn(cid:0)WLsgn(cid:0)WL−1sgn(cid:0) . . . sgn(cid:0)W1x(cid:1)(cid:1)(cid:1)(cid:1)  

{Wk}L

(1)
θ = vec(cid:0)
(cid:1)
where Wk ∈ Rdk×dk−1 denotes the weight matrices. The network is thus parametrized by
k. For binary classiﬁcation 
the BAM network ﬁnal layer WL∈R1×dL−1 has one line (dL=1)  that is a vector wL∈RdL−1  and
fθ : Rd0→{−1  1}.
2.1 Elements from the PAC-Bayesian theory

∈RD. The ith line of matrix Wk will be denoted wi

k=1

The Probably Approximately Correct (PAC) framework [introduced by Valiant  1984] holds under
the frequentist assumption that data is sampled in an iid fashion from a data distribution D over the
input-output space. The learning algorithm observes a ﬁnite training sample S = {(xi  yi)}n
i=1 ∼ Dn
and outputs a predictor f : Rd0 → [−1  1]. Given a loss function (cid:96) : [−1  1]2 → [0  1]  we deﬁne
error on the training set  given by

LD(f ) as the generalization loss on the data generating distribution D  and (cid:98)LS(f ) as the empirical

LD(f ) = E

(x y)∼D (cid:96)(f (x)  y)  

and

(cid:96)(f (xi)  yi) .

(cid:98)LS(f ) =

1
n

n(cid:88)

i=1

PAC-Bayes considers the expected loss of an aggregation of predictors: considering a distribution Q
(called the posterior) over a family of predictors F  one obtains PAC upper bounds on Ef∼Q LD(f ).
2 (1− yy(cid:48))  for which the aggregated loss is equivalent
Our work focuses on the linear loss (cid:96)(y(cid:48)  y) := 1
LD(FQ) = Ef∼Q LD(f )  by its empirical counterpart (cid:98)LS(FQ) = Ef∼Q (cid:98)LS(f ) and a complexity
to the loss of the predictor FQ(x) := Ef∼Q f (x)  performing a Q-aggregation of all predictors in F.
In other words  we may upper bound with an arbitrarily high probability the generalization loss
distribution) chosen independently of the training set S  given by KL(Q(cid:107)P ) := (cid:82) ln Q(θ)

term  the Kullback-Leibler divergence between Q and a reference measure P (called the prior
P (θ) Q(dθ).
Since the seminal works of Shawe-Taylor and Williamson [1997]  McAllester [1999  2003] and
Catoni [2003  2004  2007]  the celebrated PAC-Bayesian theorem has been declined in many forms
[see Guedj  2019  for a survey]. The following Theorems 1 and 2 will be useful in the sequel.

1Bold uppercase letters denote matrices  bold lowercase letters denote vectors.
2We consider the activation function as an element-wise operator when applied to vectors or matrices.

2

 

n

≤

n

δ

for all Q on F : kl

p + (1 − q) ln 1−q

√
KL(Q(cid:107)P ) + ln 2

Theorem 1 (Seeger [2002]  Maurer [2004]). Given a prior P on F  with probability at least 1 − δ
over S ∼ Dn 

(cid:16)(cid:98)LS(FQ)(cid:13)(cid:13)LD(FQ)
(cid:17)
(cid:18)
−C (cid:98)LS(FQ) −

(2)
1−p is the Kullback-Leibler divergence between Bernoulli

where kl(q(cid:107)p) := q ln q
distributions with probability of success p and q  respectively.
Theorem 2 (Catoni [2007]). Given P on F and C > 0  with probability at least 1 − δ over S ∼ Dn 
for all Q on F : LD(FQ) ≤
(3)
From Theorems 1 and 2  we obtain PAC-Bayesian bounds on the linear loss of the Q-aggregated
predictor FQ. Given our binary classiﬁcation setting  it is natural to predict a label by taking the
sign of FQ(·). Thus  one may also be interested in the zero-one loss (cid:96)01(y(cid:48)  y) := 1[sgn(y(cid:48)) (cid:54)= y];
the bounds obtained from Theorems 1 and 2 can be turned into bounds on the zero-one loss with an
extra 2 multiplicative factor  using the elementary inequality (cid:96)01(FQ(x)  y) ≤ 2(cid:96)(FQ(x)  y).
2.2 Elementary building block: PAC-Bayesian learning of linear classiﬁers

KL(Q(cid:107)P ) + ln 1

1 − e−C

1 − exp

(cid:19)(cid:19)

(cid:18)

δ

.

n

1

The PAC-Bayesian specialization to linear classiﬁers has been proposed by Langford and Shawe-
Taylor [2002]  and used for providing tight generalization bounds and a model selection criteria
[further studied by Ambroladze et al.  2006  Langford  2005  Parrado-Hernández et al.  2012].
This paved the way to the PAC-Bayesian bound minimization algorithm of Germain et al. [2009] 
that learns a linear classiﬁer fw(x) := sgn(w · x)  with w ∈ Rd. The strategy is to consider a
Gaussian posterior Qw := N (w  Id) and a Gaussian prior Pw0 := N (w0  Id) over the space of all
linear predictors Fd := {fv|v ∈ Rd} (where Id denotes the d × d identity matrix). The posterior
is used to deﬁne a linear predictor fw and the prior may have been learned on previously seen
data; a common uninformative prior being the null vector w0 = 0. With such parametrization 
2(cid:107)w − w0(cid:107)2. Moreover  the Qw-aggregated output can be written in terms of the
KL(Qw(cid:107)Pw0 ) = 1
Gauss error function erf(·). In Germain et al. [2009]  the erf function is introduced as a loss function
to be optimized. Here we interpret it as the predictor output  to be in phase with our neural network
approach. Likewise  we study the linear loss of an aggregated predictor instead of the Gibbs risk of a
stochastic classiﬁer. We obtain (explicit calculations are provided in Appendix A.1 for completeness)
(4)
Given a training set S ∼ Dn  Germain et al. [2009] propose to minimize a PAC-Bayes upper bound
on LD(Fw) by gradient descent on w. This approach is appealing as the bounds are valid uniformly
for all Qw (see Equations 2 and 3). In other words  the algorithm provides both a learned predictor
and a generalization guarantee that is rigorously valid (under the iid assumption) even when the
optimization procedure did not ﬁnd the global minimum of the cost function (either because it
converges to a local minimum  or early stopping is used). Germain et al. [2009] investigate the
optimization of several versions of Theorems 1 and 2. The minimization of Theorem 1 generally leads
to tighter bound values  but empirical studies show lowest accuracy as the procedure conservatively
prevents overﬁtting. The best empirical results are obtained by minimizing Theorem 2 for a ﬁxed
hyperparameter C  selected by cross-validation. Minimizing Equation (3) amounts to minimizing

(cid:82) x
0 e−t2

  with erf(x) := 2√

(cid:16) w·x√

Fw(x) := E

fv(x) = erf

v∼Qw

(cid:17)

2(cid:107)x(cid:107)

dt .

π

C n(cid:98)LS(Fw) + KL(Qw(cid:107)Pw0) = C

1
2

(cid:18)

n(cid:88)

i=1

erf

−yi

w · xi
√2(cid:107)xi(cid:107)

(cid:19)

+

1
2(cid:107)w − w0(cid:107)2 .

(5)

In their discussion  Germain et al. [2009] observe that the objective in Equation (5) is similar to
the one optimized by the soft-margin Support Vector Machines [Cortes and Vapnik  1995]  by
roughly interpreting the hinge loss max(0  1− yy(cid:48)) as a convex surrogate of the probit loss erf(−yy(cid:48)).
Likewise  Langford and Shawe-Taylor [2002] present this parameterization of the PAC-Bayes theorem
as a margin bound. In the following  we develop an original approach to neural networks based on a
slightly different observation: the predictor output given by Equation (4) is reminiscent of the tanh
activation used in classical neural networks (see Figure 3 in the appendix for a visual comparison).
Therefore  as the linear perceptron is viewed as the building block of modern multilayer neural
networks  the PAC-Bayesian specialization to binary classiﬁers is the cornerstone of our theoretical
and algorithmic framework for BAM networks.

3

3 The simple case of a one hidden layer network

Let us ﬁrst consider a network with one hidden layer of size d1. Hence  this network is parameterized
by weights θ = vec({W1  w2})  with W1 ∈ Rd1×d0 and w2 ∈ Rd1. Given an input x ∈ Rd0  the
output of the network is
(6)

fθ(x) = sgn(cid:0)w2 · sgn(W1x)(cid:1) .

Following Section 2  we consider an isotropic Gaussian posterior distribution centered in θ  denoted
Qθ = N (θ  ID)  over the family of all networks FD = {f˜θ | ˜θ ∈ RD}. Thus  the prediction
of the Qθ-aggregate predictor is given by Fθ(x) = E˜θ∼Qθ
f˜θ(x). Note that Dziugaite and Roy
[2017]  Langford and Caruana [2001] also consider Gaussian distributions over neural networks
parameters. However  as their analysis is not speciﬁc to a particular activation function—experiments
are performed with typical activation functions (sigmoid  ReLU)—the prediction relies on sampling
the parameters according to the posterior. An originality of our approach is that  by studying the sign
activation function  we can calculate the exact form of Fθ(x)  as detailed below.

3.1 Deterministic network

Prediction. To compute the value of Fθ(x)  we ﬁrst need to decompose the probability of each
˜θ=vec({V1  v2})∼ Qθ as Qθ(˜θ)=Q1(V1)Q2(v2)  with Q1=N (W1  Id0d1 ) and Q2=N (w2  Id1).

Fθ(x) =

Q1(V1)

Rd1×d0

Rd1

(cid:90)
(cid:90)

Q1(V1) erf

(cid:90)
(cid:16) w2·s√
(cid:16) w2·s√

2d1

2d1

erf

erf

(cid:17)

2(cid:107)sgn(V1x)(cid:107)

(cid:16) w2·sgn(V1x)
(cid:17)(cid:90)
(cid:17)

Ψs (x  W1)  

Q2(v2)sgn(v2 · sgn(V1x))dv2dV1
√

dV1

1[s = sgn(V1x)]Q1(V1) dV1
Rd1×d0

Rd1×d0

=

=

(cid:88)
(cid:88)
where  from Q1(V1) =(cid:81)d1
(cid:90)
d1(cid:89)

=

Ψs (x  W1) :=

s∈{−1 1}d1

s∈{−1 1}d1

i=1 Qi

1(vi

1) with Qi

1 := N (wi

1  Id0)  we obtain

1[si x · vi

Rd0

1 > 0]Qi

1(vi

1) dvi

1 =

i=1

d1(cid:89)

i=1

(cid:20) 1
(cid:124)

2

(cid:18) wi
(cid:123)(cid:122)
1 · x
√2(cid:107)x(cid:107)

(cid:19)(cid:21)
(cid:125)

+

si
2

erf

(7)

(8)

(9)

.

(10)

ψsi (x wi
1)

Line (7) states that the output neuron is a linear predictor over the hidden layer’s activation values

s = sgn(V1x); based on Equation (4)  the integral on v2 becomes erf(cid:0)w2 · s/(√2(cid:107)s(cid:107))(cid:1). As a

function of s  the latter expression is piecewise constant. Thus  Line (8) discretizes the integral on V1
as a sum of the 2d1 different values of s = (si)d1
Finally  one can compute the exact output of Fθ(x)  provided one accepts to compute a sum combi-
natorial in the number of hidden neurons (Equation 9). We show in forthcoming Section 3.2 that it is
possible to circumvent this computational burden and approximate Fθ(x) by a sampling procedure.
Derivatives. Following contemporary approaches in deep neural networks [Goodfellow et al.  2016] 

we minimize the empirical loss (cid:98)LS(Fθ) by stochastic gradient descent (SGD). This requires to

i=1  si ∈ {−1  1}. Note that (cid:107)s(cid:107)2 = d1.

compute the partial derivative of the cost function according to the parameters θ:

∂(cid:98)LS(Fθ)

∂θ

=

1
n

n(cid:88)

n(cid:88)

i=1

∂(cid:96)(Fθ(xi)  yi)

i=1

∂θ

=

1
n

∂Fθ(xi)

∂θ

(cid:48)
(cid:96)

(Fθ(xi)  yi)  

(11)

with the derivative of the linear loss (cid:96)(cid:48)(Fθ(xi)  yi) = − 1
2 y.

4

Figure 1: Illustration of the proposed method for a one hidden layer network of size d1=3  interpreted
as a majority vote over 8 binary representations s ∈ {−1  1}3. For each s  a plot shows the values
of Fw2(s)Ψs(x  W1). The sum of these values gives the deterministic network output Fθ(x) (see
Eq. 9). We also plot the BAM network output fθ(x) for the same parameters θ (see Eq. 6).

The partial derivatives of the prediction function (Equation 9) according to the hidden layer parameters
wk
1 ∈ {w1

1  . . .   wd1

(cid:48)(cid:18) wk
1 · x
√2(cid:107)x(cid:107)
(cid:88)

1 } and the output neuron parameters w2 are
x
2(cid:107)x(cid:107)
1
√2d1

(cid:19) (cid:88)
(cid:48)(cid:18) w2 · s

(cid:18) w2 · s

s∈{−1 1}d1

√2d1

√2d1

(cid:19)

sk erf

s erf

3

2

erf

s∈{−1 1}d1

∂

∂wk
1

∂

∂w2

Fθ(x) =

Fθ(x) =

(cid:19)(cid:20) Ψs(x  W1)

(cid:21)

ψsk (x  wk
1 )

 

(12)

Ψs(x  W1)   with erf

(cid:48)

(x) := 2√

π e−x2 .

(13)

Note that this is an exact computation. A salient fact is that even though we work on non-differentiable
Indeed (cid:80)
BAM networks  we get a structure trainable by (stochastic) gradient descent by aggregating networks.
Majority vote of learned representations. Note that Ψs (Equation 10) deﬁnes a distribution on s.
s Ψs(x  W1)=1  as Ψs(x  W1) + Ψ¯s(x  W1) = 2−d1 for every ¯s =−s. Thus  by Equa-
tion (9) we can interpret Fθ akin to a majority vote predictor  which performs a convex combination
of a linear predictor outputs Fw2 (s) := erf(w2 · s/√2d1). The vote aggregates the predictions on
the 2d1 possible binary representations. Thus  the algorithm does not learn the representations per se 
but rather the weights Ψs(x  W1) associated to every s given an input x  as illustrated by Figure 1.

3.2 Stochastic approximation

Since Ψs (Equation 10) deﬁnes a distribution  we can interpret the function value as the probability
of mapping input x into the hidden representation s given the parameters W1. Using a different
formalism  we could write Pr(s|x  W1) = Ψs(x  W1). This viewpoint suggests a sampling scheme
to approximate both the predictor output (Equation 9) and the partial derivatives (Equations 12
and 13)  that can be framed as a variant of the REINFORCE algorithm [Williams  1992] (see the
discussion below): We avoid computing the 2d1 terms by resorting to a Monte Carlo approximation
of the sum. Given an input x and a sampling size T   the procedure goes as follows.
(cid:80)T
Prediction. We generate T random binary vectors Z:={st}T
t=1 according to the Ψs(x  W1)-
A stochastic approximation of Fθ(x) is given by (cid:98)Fθ(Z) := 1
1)−zt
distribution. This can be done by uniformly sampling zt
i ).
deep learning frameworks while evaluating (cid:98)Fθ(Z) [e.g.  Paszke et al.  2017]. However  we need the
Derivatives. Note that for a given sample {st}T
t=1  the approximate derivatives according to w2
(Equation 15 below) can be computed numerically by the automatic differentiation mechanism of
following Equation (14) to approximate the gradient according to W1 because ∂(cid:98)Fθ(Z)/∂wk

i∈[0  1]  and setting st

(cid:16) w2·st√

i=sgn(ψ1(x  wi

t=1 erf

(cid:17)

2d1

1 = 0.

T

.

5

s=(−1 −1 −1)s=(−1 −1 1)s=(−1 1 −1)s=(−1 1 1)DeterministicNetworkFθs=(1 −1 −1)s=(1 −1 1)s=(1 1 −1)s=(1 1 1)BAMNetworkfθ−1.00−0.75−0.50−0.250.000.250.500.751.00ˆy

ˆy

x1

x2

x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2

Figure 2: Illustration of the BAM to tree architecture map on a three layers network.

∂

∂wk
1

∂

∂w2

Fθ(x) ≈

Fθ(x) ≈

T 2

x
3
2(cid:107)x(cid:107)
1
T√2d1

erf

T(cid:88)

t=1

(cid:19) T(cid:88)
(cid:48)(cid:18) wk
1 · x
(cid:48)(cid:18) w2 · st
(cid:19)
√2(cid:107)x(cid:107)

st erf

t=1

√2d1

=

∂

∂w2

(cid:18) w2 · st

√2d1

(cid:19)

;

(14)

(15)

st
k
(x  wk
1 )

ψst

k

erf

(cid:98)Fθ(Z) .

Similar approaches to stochastic networks. Random activation functions are commonly used in
generative neural networks  and tools have been developed to train these by gradient descent (see
Goodfellow et al. [2016  Section 20.9] for a review). Contrary to these approaches  our analysis
differs as the stochastic operations are introduced to estimate a deterministic objective. That being
said  Equation (14) can be interpreted as a variant of REINFORCE algorithm [Williams  1992]
to apply the back-propagation method along with discrete activation functions. Interestingly  the
formulation we obtain through our PAC-Bayes objective is similar to a commonly used REINFORCE
variant [e.g.  Bengio et al.  2013  Yin and Zhou  2019]  where the activation function is given by a
Bernoulli variable with probability of success σ(a)  where a is the neuron input  and σ is the sigma is
the sigmoid function. The latter can be interpreted as a surrogate of our ψsi(x  wi

1).

4 Generalization to multilayer networks

In the following  we extend the strategy introduced in Section 3 to BAM architectures with an arbitrary
number of layers L ∈ N∗ (Equation 1). An apparently straightforward approach to achieve this
sums of(cid:81)L
generalization would have been to consider a Gaussian posterior distribution N (θ  ID) over the BAM
family {f˜θ|˜θ ∈ RD}. However  doing so leads to a deterministic network relying on undesirable
k=1 2dk elements (see Appendix A.2 for details). Instead  we deﬁne a mapping fθ (cid:55)→ gζ(θ)
which transforms the BAM network into a computation tree  as illustrated by Figure 4.
graph nodes): the tree leaves contain(cid:81)L
BAM to tree architecture map. Given a BAM network fθ of L layers with sizes d0  d1  . . .   dL
(reminder: dL=1)  we obtain a computation tree by decoupling the neurons (i.e.  the computation
k=1 dk copies of each of the d0 BAM input neurons  and
has its own parameter (a real-valued scalar); the total number of edges is D† := (cid:80)L−1
the tree root node corresponds to the single BAM output neuron. Each input-output path of the
k :=(cid:81)L
original BAM network becomes a path of length L from one leaf to the tree root. Each tree edge
†
k  with
k=0 d
i=k di. We deﬁne a set of tree parameters η recursively according to the tree structure. From
d
†
k edges. That is  each node at level k+1 has its own parameters subtree
level k to k+1  the tree has d
i }dk
i=0  where each ηk
ηk+1 := {ηk
i is either a weight vector containing the input edges parameters (by
0 ∈ Rdk−1) or a parameter set (thus  ηk
convention  ηk
dk−1 are themselves parameter subtrees).

1   . . .   ηk

†

6

Hence  the deepest elements of the recursive parameters set η are weight vectors η1 ∈ Rd0. Let us
now deﬁne the output tree gη(x) := gL(x  η) on an input x ∈ Rd0 as a recursive function:

(cid:124)

g1(x {w}) = sgn (w · x)  
1   . . .   ηk

) = sgn

(cid:125)

dk}

(cid:123)(cid:122)

(cid:16)

(cid:124)

(cid:123)(cid:122)

(cid:17)

(cid:125)

gk+1(x {w  ηk

w · (gk(x  η1)  . . .   gk(x  ηdk ))

for k = 1  . . .   L−1 .

ηk

gk(x ηk)

BAM to tree parameters map. Given BAM parameters θ  we denote θ1:k := vec(cid:0)

{Wk}k
i=1
is ζ(θ) =
k  ζ1(θ1:k−1)  . . .   ζdk−1(θ1:k−1)} 
1}. Note that the parameters tree obtained by the transformation ζ(θ) is highly
†
k (the ith line of the Wk matrix from θ) is replicated d
k+1 times.

The mapping from θ into the corresponding (recursive) tree parameters set
{wL  ζ1(θ1:L−1)  . . .   ζdL−1(θ1:L−1)}  such that ζi(θ1:k) = {wi
and ζi(θ1:1) = {wi
redundant  as each weight vector wi
This construction is such that fθ(x) = gζ(θ)(x) for all x ∈ Rd0.
Deterministic network. With a slight abuse of notation  we let ˜η ∼ Qη := N (η  ID†) denote a
parameter tree of the same structure as η  where every weight is sampled iid from a normal distribution.
We denote Gθ(x) := E˜η∼Qζ(θ) g˜η(x)  and we compute the output value of this predictor recursively.
In the following  we denote G(j)
(x) the function returning the jth neuron value of the layer k+1.
Hence  the output of this network is Gθ(x) = G(1)

(cid:1).

θ1:k+1

θ1:L(x). As such 

(cid:90)
(cid:88)

Rd0

(cid:17)

(cid:16) wj

1·x√
2(cid:107)x(cid:107)

 

s (x  θ) =

G(j)

θ1:1(x) =

G(j)

θ1:k+1

(x) =

erf
s∈{−1 1}dk

Qwj

1

(cid:18) wj

(cid:19)

(v)sgn(v · x)dv = erf
k+1·s√

s (x  θ)  with Ψk

Ψk

2dk

(cid:18) 1

2

dk(cid:89)

i=1

(cid:19)

(x)

.

(16)

+

1
2

si × G(i)

θ1:k

The complete mathematical calculations leading to the above results are provided in Appendix A.3.
The computation tree structure and the parameter mapping ζ(θ) are crucial to obtain the recursive
expression of Equation (16). However  note that this abstract mathematical structure is never manipu-
lated explicitly. Instead  it allows computing each hidden layer vector (G(j)
j=1 sequentially; a
θ1:k
summation of 2dk terms is required for each layer k = 1  . . .   L−1.
Stochastic approximation. Following the Section 3.2 sampling procedure trick for the one hidden
layer network  we propose to perform a stochastic approximation of the network prediction output 
by a Monte Carlo sampling for each layer. Likewise  we recover exact and approximate derivatives in
a layer-by-layer scheme. The related equations are given in Appendix A.4.

(x))dk

5 PBGNet: PAC-Bayesian SGD learning of binary activated networks
We design an algorithm to learn the parameters θ ∈ RD of the predictor Gθ by minimizing a
PAC-Bayesian upper bound on the generalization loss LD(Gθ). We name our algorithm PBGNet
(PAC-Bayesian Binary Gradient Network)  as it is a generalization of the PBGD (PAC-Bayesian
Gradient Descent) learning algorithm for linear classiﬁers [Germain et al.  2009] to deep binary
activated neural networks.
Kullback-Leibler regularization. The computation of a PAC-Bayesian bound value relies on two
key elements: the empirical loss on the training set and the Kullback-Leibler divergence between
the prior and the posterior. Sections 3 and 4 present exact computation and approximation schemes

KL-divergence associated to the parameter maps of Section 4. We use the shortcut notation K(θ  µ)
to refer to the divergence between two multivariate Gaussians of D† dimensions  corresponding to

for the empirical loss (cid:98)LS(Gθ) (which is equal to (cid:98)LS(Fθ) when L=2). Equation (17) introduces the
learned parameters θ = vec(cid:0)
(cid:16)
k+1 =(cid:81)L

K(θ  µ) := KL
†
where the factors d
i=k+1 di are due to the redundancy introduced by transformation ζ(·).
This has the effect of penalizing more the weights on the ﬁrst layers. It might have a considerable

(cid:1) and prior parameters µ = vec(cid:0)
(cid:32)
(cid:107)wL − uL(cid:107)2 +

(cid:17)
{Wk}L

(cid:13)(cid:13) Pζ(µ)

(cid:1).
(cid:13)(cid:13)Wk − Uk
(cid:13)(cid:13)2

{Uk}L

L−1(cid:88)

†
d
k+1

(cid:33)

Qζ(θ)

(17)

1
2

k=1

k=1

k=1

=

F

 

7

inﬂuence on the bound value for very deep networks. On the other hand  we observe that this is
consistent with the ﬁne-tuning practice performed when training deep neural networks for a transfer
learning task: prior parameters are learned on a ﬁrst dataset  and the posterior weights are learned by
adjusting the last layer weights on a second dataset [see Bengio  2009  Yosinski et al.  2014].
Bound minimization. PBGNet minimizes the bound of Theorem 1 (rephrased as Equation 18).
However  this is done indirectly by minimizing a variation on Theorem 2 and used in a deep learning
context by Zhou et al. [2019] (Equation 19). Theorem 3 links both results (proof in Appendix A.5).
Theorem 3. Given prior parameters µ ∈ RD  with probability at least 1 − δ over S ∼ Dn  we have
for all θ on RD :

(cid:27)

(cid:19)(cid:19)(cid:27)

(18)

.

(19)

√
n
[K(θ  µ) + ln 2
δ ]

1
n

(cid:26)

(cid:26)
LD(Gθ) ≤ sup
0≤p≤1

= inf
C>0

p : kl((cid:98)LS(Gθ)(cid:107)p) ≤
(cid:18)
(cid:18)
−C (cid:98)LS(Gθ) −

1 − exp

1
n

1

1−e−C

√
n
[K(θ  µ) + ln 2
δ ]

We use stochastic gradient descent (SGD) as the optimization procedure to minimize Equation (19)
with respect to θ and C. It optimizes the same trade-off as in Equation (5)  but choosing the C value
which minimizes the bound.3 The originality of our SGD approach is that not only do we induce
gradient randomness by selecting mini-batches among the training set S  we also approximate the
loss gradient by sampling T elements for the combinatorial sum at each layer. Our experiments show
that  for some learning problems  reducing the sample size of the Monte Carlo approximation can be
beneﬁcial to the stochastic gradient descent. Thus the sample size value T has an inﬂuence on the
cost function space exploration during the training procedure (see Figure 7 in the appendix). Hence 
we consider T as a PBGNet hyperparameter.

6 Numerical experiments

Experiments were conducted on six binary classiﬁcation datasets  described in Appendix B.
Learning algorithms. In order to get insights on the trade-offs promoted by the PAC-Bayes bound
minimization  we compared PBGNet to variants focusing on empirical loss minimization. We train the
models using multiple network architectures (depth and layer size) and hyperparameter choices. The
objective is to evaluate the efﬁciency of our PAC-Bayesian framework both as a learning algorithm
design tool and a model selection criterion. For all methods  the network parameters are trained using
the Adam optimizer [Kingma and Ba  2015]. Early stopping is used to interrupt the training when the
cost function value is not improved for 20 consecutive epochs. Network architectures explored range
from 1 to 3 hidden layers (L) and a hidden size h ∈ {10  50  100} (dk = h for 1 ≤ k < L). Unless
otherwise speciﬁed  the same randomly initialized parameters are used as a prior in the bound and as
a starting point for SGD optimization [as in Dziugaite and Roy  2017]. Also  for all models except
MLP  we select the binary activation sampling size T in a range going from 10 to 10000. More details
about the experimental setting are given in Appendix B.
MLP. We compare to a standard network with tanh activation  as this activation resembles the erf
function of PBGNet. We optimize the linear loss as the cost function and use 20% of training data
as validation for hyperparameters selection. A weight decay parameter ρ is selected between 0 and
10−4. Using weight decay corresponds to adding an L2 regularizer ρ
2(cid:107)θ(cid:107)2 to the cost function  but
contrary to the regularizer of Equation (17) promoted by PBGNet  this regularization is uniform for
all layers.

PBGNet(cid:96). This variant minimizes the empirical loss (cid:98)L(Gθ)  with an L2 regularization term ρ
PBGNet(cid:96)-bnd. Again  the empirical loss (cid:98)L(Gθ) with an L2 regularization term ρ

2(cid:107)θ(cid:107)2.
The corresponding weight decay ρ  as well as other hyperparameters  are selected using a validation
set  exactly as the MLP does. The bound expression is not involved in the learning process and is
computed on the model selected by the validation set technique.

2(cid:107)θ(cid:107)2 is minimized.
However  only the weight decay hyperparameter ρ is selected on the validation set  the other ones are
selected by the bound. This method is motivated by an empirical observation: our PAC-Bayesian
bound is a great model selection tool for most hyperparameters  except the weight decay term.

3We also note that our training objective can be seen as a generalized Bayesian inference one [Knoblauch

et al.  2019]  where the tradeoff between the loss and the KL divergence is given by the PAC-Bayes bound.

8

Table 1: Experiment results for the considered models on the binary classiﬁcation datasets: error rates
on the train and test sets (ES and ET )  and generalization bounds on the linear loss LD (Bnd). The
PAC-Bayesian bounds hold with probability 0.95. Bound values for PBGNet(cid:96) are trivial  excepted
Adult with a bound value of 0.606  and are thus not reported. A visual representation of this table is
presented in the appendix (Figure 5).

MLP
ES

ET

PBGNet(cid:96)
ES
ET

PBGNet(cid:96)-bnd
ES

PBGNet

PBGNetpre

Dataset

ET Bnd

ET Bnd
0.021 0.035 0.018 0.030 0.028 0.047 0.763 0.131 0.168 0.205 0.033 0.033 0.060
ads
0.137 0.152 0.133 0.149 0.147 0.155 0.281 0.154 0.163 0.214 0.149 0.154 0.164
adult
mnist17 0.002 0.004 0.003 0.004 0.004 0.006 0.096 0.005 0.007 0.040 0.004 0.004 0.010
mnist49 0.004 0.013 0.003 0.018 0.029 0.035 0.311 0.035 0.040 0.139 0.016 0.017 0.028
mnist56 0.004 0.013 0.003 0.011 0.022 0.024 0.172 0.022 0.025 0.090 0.009 0.009 0.018
mnistLH 0.006 0.018 0.004 0.019 0.046 0.051 0.311 0.049 0.052 0.160 0.026 0.027 0.033

ES

ET Bnd

ES

PBGNet. As described in Section 5  the generalization bound is directly optimized as the cost
function during the learning procedure and used solely for hyperparameters selection: no validation
set is needed and all training data S are exploited for learning.
PBGNetpre. We also explore the possibility of using a part of the training data as a pre-training step.
To do so  we split the training set into two halves. First  we minimize the empirical loss for a ﬁxed
number of 20 epochs on the ﬁrst 50% of the training set. Then  we use the learned parameters as
initialization and prior for PBGNet and learn on the second 50% of the training set.
Analysis. Results are summarized in Table 1  which highlights the strengths and weaknesses of the
models. Both MLP and PBGNet(cid:96) obtain competitive error scores but lack generalization guarantees.
By introducing the bound value in the model selection process  even with the linear loss as the cost
function  PBGNet(cid:96)-bnd yields non-vacuous generalization bound values although with an increase in
error scores. Using the bound expression for the cost function in PBGNet improves bound values
while keeping similar performances. The Ads dataset is a remarkable exception where the small
amount of training examples seems to radically constrain the network in the learning process as it
hinders the KL divergence growth in the bound expression. With an informative prior from pre-
training  PBGNetpre is able to recover competitive error scores while offering tight generalization
guarantees. All selected hyperparameters are presented in the appendix (Table 4).
A notable observation is the impact of the bound exploitation for model selection on the train-test
error gap. Indeed  PBGNet(cid:96)-bnd  PBGNet and PBGNetpre display test errors closer to their train errors 
as compared to MLP and PBGNet(cid:96). This behavior is more noticeable as the dataset size grows and
suggests potential robustness to overﬁtting when the bound is involved in the learning process.

7 Conclusion and perspectives

We made theoretical and algorithmic contributions towards a better understanding of generalization
abilities of binary activated multilayer networks  using PAC-Bayes. Note that the computational
complexity of a learning epoch of PBGNet is higher than the cost induced in binary neural networks
[Bengio  2009  Hubara et al.  2016  2017  Soudry et al.  2014]. Indeed  we focus on the optimization
of the generalization guarantee more than computational complexity. Although we also propose a
sampling scheme that considerably reduces the learning time required by our method  achieving a
nontrivial tradeoff.
We intend to investigate how we could leverage the bound to learn suitable priors for PBGNet. Or
equivalently  ﬁnding (from the bound point of view) the best network architecture. We also plan to
extend our analysis to multiclass and multilabel prediction  and convolutional networks. We believe
that this line of work is part of a necessary effort to give rise to a better understanding of the behavior
of deep neural networks.

9

Acknowledgments
We would like to thank Mario Marchand for the insight leading to the Theorem 3  Gabriel Dubé and
Jean-Samuel Leboeuf for their input on the theoretical aspects  Frédérik Paradis for his help with the
implementation  and Robert Gower for his insightful comments. This work was supported in part by
the French Project APRIORI ANR-18-CE23-0015  in part by NSERC and in part by Intact Financial
Corporation. We gratefully acknowledge the support of NVIDIA Corporation with the donation of
Titan Xp GPUs used for this research.

References
Amiran Ambroladze  Emilio Parrado-Hernández  and John Shawe-Taylor. Tighter PAC-Bayes bounds.

In NIPS  2006.

Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning  2

(1):1–127  2009.

Yoshua Bengio  Nicholas Léonard  and Aaron C. Courville. Estimating or propagating gradients

through stochastic neurons for conditional computation. CoRR  abs/1308.3432  2013.
Olivier Catoni. A PAC-Bayesian approach to adaptive classiﬁcation. preprint  840  2003.
Olivier Catoni. Statistical learning theory and stochastic optimization: Ecole d’Eté de Probabilités

de Saint-Flour XXXI-2001. Springer  2004.

Olivier Catoni. PAC-Bayesian supervised classiﬁcation: the thermodynamics of statistical learning 

volume 56. Inst. of Mathematical Statistic  2007.

Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning  20(3)  1995.

Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In UAI. AUAI
Press  2017.

Pascal Germain  Alexandre Lacasse  François Laviolette  and Mario Marchand. PAC-Bayesian

learning of linear classiﬁers. In ICML  pages 353–360. ACM  2009.

Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016. http:

//www.deeplearningbook.org.

Benjamin Guedj. A primer on PAC-Bayesian learning. arXiv preprint arXiv:1901.05353  2019.

Itay Hubara  Matthieu Courbariaux  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Binarized

neural networks. In NIPS  pages 4107–4115  2016.

Itay Hubara  Matthieu Courbariaux  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. JMLR  18
(1):6869–6898  2017.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR  2015.

Jeremias Knoblauch  Jack Jewson  and Theodoros Damoulas. Generalized variational inference 

2019.

Alexandre Lacasse. Bornes PAC-Bayes et algorithmes d’apprentissage. PhD thesis  Université Laval 

2010. URL http://www.theses.ulaval.ca/2010/27635/.

John Langford. Tutorial on practical prediction theory for classiﬁcation. JMLR  6  2005.
John Langford and Rich Caruana. (Not) Bounding the True Error. In NIPS  pages 809–816. MIT

Press  2001.

John Langford and John Shawe-Taylor. PAC-Bayes & margins. In NIPS  2002.
Andreas Maurer. A note on the PAC-Bayesian theorem. CoRR  cs.LG/0411099  2004.

10

David McAllester. Some PAC-Bayesian theorems. Machine Learning  37(3)  1999.
David McAllester. PAC-Bayesian stochastic model selection. Machine Learning  51(1)  2003.

Behnam Neyshabur  Srinadh Bhojanapalli  and Nathan Srebro. A PAC-Bayesian approach to

spectrally-normalized margin bounds for neural networks. In ICLR  2018.

Frédérik Paradis. Poutyne: A Keras-like framework for PyTorch  2018. https://poutyne.org.

Emilio Parrado-Hernández  Amiran Ambroladze  John Shawe-Taylor  and Shiliang Sun. PAC-Bayes

bounds with data dependent priors. JMLR  13  2012.

Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop  2017.

Matthias Seeger. PAC-Bayesian generalization bounds for gaussian processes. JMLR  3  2002.
John Shawe-Taylor and Robert C. Williamson. A PAC analysis of a Bayesian estimator. In COLT 

1997.

Daniel Soudry  Itay Hubara  and Ron Meir. Expectation backpropagation: Parameter-free training of
multilayer neural networks with continuous or discrete weights. In NIPS  pages 963–971  2014.
Leslie G Valiant. A theory of the learnable. In Proceedings of the sixteenth annual ACM symposium

on Theory of computing  pages 436–445. ACM  1984.

Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine Learning  8(3):229–256  May 1992.

Mingzhang Yin and Mingyuan Zhou. ARM: augment-reinforce-merge gradient for stochastic binary

networks. In ICLR (Poster)  2019.

Jason Yosinski  Jeff Clune  Yoshua Bengio  and Hod Lipson. How transferable are features in deep

neural networks? In NIPS  pages 3320–3328  2014.

Wenda Zhou  Victor Veitch  Morgane Austern  Ryan P. Adams  and Peter Orbanz. Non-vacuous
generalization bounds at the imagenet scale: a PAC-bayesian compression approach. In ICLR 
2019.

11

,Gaël Letarte
Pascal Germain
Benjamin Guedj
Francois Laviolette