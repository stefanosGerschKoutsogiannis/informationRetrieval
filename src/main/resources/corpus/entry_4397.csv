2019,Primal-Dual Block Generalized Frank-Wolfe,We propose a generalized variant of Frank-Wolfe algorithm for solving a class of sparse/low-rank optimization problems. Our formulation includes Elastic Net  regularized SVMs and phase retrieval as special cases. The proposed Primal-Dual Block Generalized Frank-Wolfe algorithm reduces the per-iteration cost while maintaining linear convergence rate.
The per iteration cost of our method depends on the structural complexity of the solution (i.e. sparsity/low-rank) instead of the ambient dimension.
We empirically show that our algorithm outperforms the state-of-the-art methods on (multi-class) classification tasks.,Primal-Dual Block Generalized Frank-Wolfe

Qi Lei†∗

  Jiacheng Zhuo†∗

Inderjit S. Dhillon†‡  and

  Constantine Caramanis† 
Alexandros G. Dimakis†
† UT Austin ‡ Amazon

{leiqi@oden.  jzhuo@  constantine@  inderjit@cs. 

dimakis@austin.}utexas.edu

Abstract

We propose a generalized variant of Frank-Wolfe algorithm for solving a class
of sparse/low-rank optimization problems. Our formulation includes Elastic Net 
regularized SVMs and phase retrieval as special cases. The proposed Primal-Dual
Block Generalized Frank-Wolfe algorithm reduces the per-iteration cost while
maintaining linear convergence rate. The per iteration cost of our method depends
on the structural complexity of the solution (i.e. sparsity/low-rank) instead of
the ambient dimension. We empirically show that our algorithm outperforms the
state-of-the-art methods on (multi-class) classiﬁcation tasks.

Introduction

1
We consider optimization problems of the form:

(cid:88)

i

min
x∈C

:

fi(a(cid:62)

i x) + g(x) 

directly motivated by regularized and constrained Empirical Risk Minimization (ERM). Particularly 
we are interested in problems whose solution has special “simple” structure like low-rank or spar-
sity. The sparsity constraint applies to large-scale multiclass/multi-label classiﬁcation  low-degree
polynomial data mapping [5]  random feature kernel machines [32]  and Elastic Net [39]. Motivated
by recent applications in low-rank multi-class SVM  phase retrieval  matrix completion  afﬁne rank
minimization and other problems (e.g.  [9  31  2  3])  we also consider settings where the constraint
x ∈ C (e.g.  trace norm ball) while convex  may be difﬁcult to project onto. A wish-list for this class
of problems would include an algorithm that (1) exploits the function ﬁnite-sum form and the simple
structure of the solution  (2) achieves linear convergence for smooth and strongly convex problems 
(3) does not pay a heavy price for the projection step.
We propose a Frank-Wolfe (FW) type method that attains these three goals. This does not come
without challenges: Although it is currently well-appreciated that FW type algorithms avoid the cost
of projection [14  1]  the beneﬁts are limited to constraints that are hard to project onto  like the trace
norm ball. For problems like phase retrieval and ERM for multi-label multi-class classiﬁcation  the
gradient computation requires large matrix multiplications. This dominates the per-iteration cost 
and the existing FW type methods do not asymptotically reduce time complexity per iteration  even
without paying the expensive projection step. Meanwhile  for simpler constraints like the (cid:96)1 norm ball
or the simplex  it is unclear if FW can offer any beneﬁts compared to other methods. Moreover  as is
generally known  FW suffers from sub-linear convergence rate even for well-conditioned problems
that enjoy strong convexity and smoothness.
Our contributions. In this paper we tackle the challenges by exploiting the special structure induced
by the constraints and FW steps. We propose a generalized variant of FW that we call Primal-Dual
Block Generalized Frank Wolfe. The main advantage is that the computational complexity depends

∗Both authors contribute equally.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

only on the sparsity of the solution  rather than the ambient dimension  i.e. it is dimension free. This
is achieved by conducting partial updates in each iteration  i.e.  sparse updates for (cid:96)1 and low-rank
updates for the trace norm ball. While the beneﬁts of partial updates is unclear for the original
problem  we show in this work how they signiﬁcantly beneﬁt a primal-dual reformulation. This
reduces the per iteration cost to roughly a ratio of s
d compared to naive Frank-Wolfe  where s is
the sparsity (or rank) of the optimal solution  and d is the feature dimension. Meanwhile  the per
iteration progress of our proposal is comparable to a full gradient descent step  thus retaining linear
convergence rate.
For strongly convex and smooth f and g we show that our algorithm achieves linear convergence
with per-iteration cost sn over (cid:96)1-norm ball  where s upper bounds the sparsity of the primal optimal.
Speciﬁcally  for sparse ERM with smooth hinge loss or quadratic loss with (cid:96)2 regularizer  our
algorithm yields an overall O(s(n + κ) log 1
 ) time complexity to reach  duality gap  where κ is the
condition number (smoothness divided by strong convexity). Our theory has minimal requirements
on the data matrix A.
Experimentally we observe our method yields signiﬁcantly better performance compared to prior
work  especially when the data dimension is large and the solution is sparse. Therefore we achieve
the state-of-the-art performance both in time complexity and in practice measured by CPU time  for
regularized ERM with smooth hinge loss and matrix sensing problems.
2 Related Work
We review relevant algorithms that improve the overall performance of Frank-Wolfe type methods.
Such improvements are roughly obtained for two reasons: the enhancement on convergence speed
and the reduction on iteration cost. Very few prior works beneﬁt in both.
Nesterov’s acceleration has proven effective as in Stochastic Condition Gradient Sliding (SCGS) [23]
and other variants [36  26  10]. Restarting techniques dynamically adapt to the function geometric
properties and ﬁlls in the gap between sublinear and linear convergence for FW method [18]. Some
variance reduced algorithms obtain linear convergence as in [13]  however  the number of inner loops
grows signiﬁcantly and hence the method is not computationally efﬁcient.
Linear convergence has been obtained speciﬁcally for polytope constraints like [27  20]  as well
as the work proposed in [21  11] that use the Away-step Frank Wolfe and Pair-wise Frank Wolfe 
and their stochastic variants. One recent work [1] focuses on trace norm constraints and proposes
a FW-type algorithm that yields similar progress as projected gradient descent per iteration but is
almost projection free. However  in many applications where gradient computation dominates the
iteration complexity  the reduction on projection step doesn’t necessarily produce asymptotically
better iteration costs.
The sparse update introduced by FW steps was also appreciated by [22]  where they conducted dual
updates with a focus on SVM with polytope constraint. Their algorithm yields low iteration costs but
still suffer from sub-linear convergence.
On the other hand  the recently popularized primal-dual formulation minx maxy{g(x) + y(cid:62)Ax −
f (y)} has proven useful for different machine learning tasks like reinforcement learning  ERM  and
robust optimization [8]. Especially for the ERM related problems  the primal-dual formulation still
inherits the ﬁnite-sum structure from the primal form  and could be used to reduce variance [38  35]
or reduces communication complexity in the distributed setting [37]. One issue lies in this line of
prior work: they do not achieve any better performance than that with the primal formulation. A
notable exception is [24] where they also attempt to exploit sparsity of the primal variables with
the primal-dual formulation. However  this work is for unconstrained problem so it’s not directly
comparable to ours. On the other hand  the analysis of [24] relies on the sparsity of the whole iterate
trajectory  which has no obvious guarantee to be small. While our analysis only depends on primal
optimal’s sparsity or rank  and is guaranteed by the (cid:96)1 or nuclear norm constraints.
3 Setup
Notation. We brieﬂy introduce the notation used throughout the paper. We use bold lower case letter
to denote vectors  capital letter to represent matrices. (cid:107) · (cid:107) is (cid:96)2 norm for vectors and Frobenius norm
for matrices unless speciﬁed otherwise. (cid:107) · (cid:107)∗ indicates the trace norm for a matrix.

2

We say a function f is α strongly convex if f (y) ≥ f (x)+(cid:104)g  y−x(cid:105)+ α
is any sub-gradient of f. Similarly  f is β-smooth when f (y) ≤ f (x) + (cid:104)g  y − x(cid:105) + β
We use f∗ to denote the convex conjugate of f  i.e.  f∗(y)
parameters are problem-speciﬁc and are deﬁned when needed.
3.1 A Theoretical Vignette
To elaborate the techniques we use to obtain the linear convergence for our Frank-Wolfe type
algorithm  we consider the (cid:96)1 norm constrained problem as an illustrating example:

2 (cid:107)y−x(cid:107)2  where g ∈ ∂f (x)
2(cid:107)y − x(cid:107)2.
= maxx(cid:104)x  y(cid:105) − f (x). Some more

def

arg min

x∈Rd (cid:107)x(cid:107)1≤τ

f (x) 

(1)

where f is L-smooth and µ-strongly convex. If we invoke the Frank Wolfe algorithm  we compute
(2)

x(t) ← (1 − η)x(t−1) + η ˜x  where ˜x ← arg min
(cid:107)x(cid:107)1≤τ

(cid:104)∇f (x(t−1))  x(cid:105).

Even when the function f is smooth and strongly convex  (2) converges sublinearly. As inspired by
[1]  if we assume the optimal solution is s-sparse  we can enforce a sparse update while maintaining
linear convergence by a mild modiﬁcation on (2):
x(t) ← (1−η)x(t−1)+η ˜x  where ˜x ← arg min
2}. (3)
We also call this new practice block Frank-Wolfe as in [1]. The proof of convergence can be completed
within three lines. Let ht = f (x(t)) − f∗.

{(cid:104)∇f (x(t−1))  x(cid:105)+

η(cid:107)x(t−1)−x(cid:107)2

(cid:107)x(cid:107)1≤τ (cid:107)x(cid:107)0≤s

L
2

ht = f (x(t−1) + η( ˜x − x(t−1))) − f∗

η2)ht−1

2L  ht+1 ≤ (1 − µ

(by convexity and µ-strong convexity of f)

(4)
µ log(1/)) to

4L )th1 and the iteration complexity is O( L

≤ ht−1 + η(cid:104)∇f (x(t−1))  ˜x − x(t−1)(cid:105) +
η2(cid:107) ˜x − x(t−1)(cid:107)2
L
(Smoothness of f)
2
≤ ht−1 + η(cid:104)∇f (x(t−1))  x∗ − x(t−1)(cid:105) +
η2(cid:107)x∗ − x(t−1)(cid:107)2 (Deﬁnition of ˜x)
L
2
≤ (1 − η +
L
µ
Therefore  when η = µ
achieve  error.
Although we achieve linear convergence  the advantage of overall complexity against classical
methods (e.g. Projected Gradient Descend (PGD)) is not shown yet. Luckily  with the update ˜x being
sparse  it is possible to improve the iteration complexity  while maintaining the linear convergence
rate. In order to differentiate  we name the sparse update nature of (3) as partial update.
Next we elaborate the situations when one beneﬁts from partial updates. Consider a quadratic
2 x(cid:62)Ax  whose gradient is Ax for symmetric A. As ˜x is sparse  One can maintain
function: f (x) = 1
the value of the gradient efﬁciently: Ax(t) ≡ (1 − η)Ax(t−1) + ηAI : ˜x  where I is the support
set of ˜x. We therefore reduce the complexity of one iteration to O(sd)  compared to O(d2) with
PGD. Similar beneﬁts hold when we replace x by a matrix X and conduct a low-rank update on
X. The beneﬁt of partial update is not limited to quadratic functions. Next we show that for a
class of composite function  we are able to take the full advantage of the partial update  by taking a
primal-dual re-formulation.
4 Methodology
Primal-Dual Formulation. Note that the problem we are tackling is as follows:

P (x) ≡ 1
n

fi(a(cid:62)

min
x∈C

(5)
We ﬁrst focus on the setting where x ∈ Rd is a vector and C is the (cid:96)1-norm ball. This form covers
general classiﬁcation or regression tasks with fi being some loss function and g being a regularizer.
Extension to matrix optimization over a trace norm ball is introduced in Section 4.3.
Even with the constraint  we could reform (5) as a primal-dual convex-concave saddle point problem:

i x) + g(x)

i=1

 

(cid:41)

(cid:40)

(cid:40)

n(cid:88)

3

(5) ⇔ min
x∈C

max

y

L(x  y) ≡ g(x) +

1
n

(cid:104)y  Ax(cid:105) − 1
n

f∗
i (yi)

 

(6)

(cid:41)

n(cid:88)

i=1

i=1

or its dual formulation:
(5) ⇔ max

y

(cid:40)

(cid:26)

D(y) := min
x∈C

g(x) +

(cid:27)

− 1
n

(cid:104)y  Ax(cid:105)

1
n

n(cid:88)

f∗
i (yi)

(cid:41)

.

(7)

Notice (7) is not guaranteed to have an explicit form. Therefore some existing FW variants like [22]
that optimizes over (7) may not apply. Instead  we directly solve the convex concave problem (6) and
could therefore solve more general problems  including complicated constraint like trace norm.
Since the computational cost of the gradient ∇xL and ∇yL is dominated by computing A(cid:62)y and
Ax respectively  sparse updates could reduce computational costs by a ratio of roughly O(d/s) for
updating x and y while achieving good progress.
4.1 Primal-Dual Block Generalized Frank-Wolfe
With the primal-dual formulation  we are ready to introduce our algorithm. The idea is simple: since
the primal variable x is constrained over (cid:96)1 norm ball  we conduct block Frank-Wolfe algorithm
and achieve an s-sparse update. Meanwhile  for the dual variable y we conduct greedy coordinate
ascent method to select and update k coordinates (k determined later). We selected coordinates that
allow the largest step  which is usually referred as a Gauss-Southwell rule denoted by GS-r [30]. Our
algorithm is formally presented in Algorithm 1. We have the following assumptions on f and g:
Assumption 4.1. We assume the functions satisfy the following properties:

• Each loss function fi is convex and β-smooth  and is α strongly convex over some convex
set (could be R)  and linear otherwise.
• maxi (cid:107)ai(cid:107)2
• g is µ-strongly convex and L-smooth.

(cid:80)n
i=1 fi(a(cid:62)

2 ≤ R. Therefore 1

i x) is βR-smooth.

n

Suitable loss functions fi include smooth hinge loss [33] and quadratic loss function. Relevant
applications covered are Support Vector Machine (SVM) with smooth hinge loss  elastic net [39] 
and linear regression problem with quadratic loss.

Algorithm 1 Primal-Dual Block Generalized Frank-Wolfe Method for (cid:96)1 Norm Ball
1: Input: Training data A ∈ Rn×d  primal and dual step size η  δ > 0.
2: Initialize: x(0) ← 0 ∈ Rd  y(0) ← 0 ∈ Rn  w(0) ≡ Ax = 0 ∈ Rn  z(0) ≡ A(cid:62)y = 0 ∈ Rd
3: for t = 1  2 ···   T do
4:

Use Block Frank Wolfe to update the primal variable:

˜x ← arg min

(cid:107)x(cid:107)1≤λ (cid:107)x(cid:107)0≤s

{(cid:104) 1
z(t−1) + ∇g(x(t−1))  x(cid:105) +
n
x(t) ← (1 − η)x(t−1) + η ˜x

L
2

η(cid:107)x − x(t−1)(cid:107)2}

(8)

Update w to maintain the value of Ax:

w(t) ← (1 − η)w(t−1) + ηA∆x

Consider the potential dual update:

(cid:26) 1

˜y = arg max

y(cid:48)

(cid:104)w(t)  y

(cid:48)(cid:105) − f

∗

(y

(cid:48)

) − 1
2δ

(cid:107)y

(cid:48) − y(t−1)(cid:107)2

n

(cid:27)

.

5:

6:

7:

8:

(9)

(10)

(11)

(12)

Choose greedily the dual coordinates to update: let I (t) be the top k coordinates that maximize

Update the dual variable accordingly:

i ←
y(t)
Update z to maintain the value of A(cid:62)y

|˜yi − y(t−1)

|  i ∈ [n].

i

(cid:26) ˜yi

y(t−1)

i

if i ∈ I (t)
otherwise.

z(t) ← z(t−1) + A

: I(t) (y(t) − y(t−1))
(cid:62)

9: end for
10: Output: x(T )  y(T )
As L(x  y) is µ-strongly convex and L-smooth with respect to x  we set the primal learning rate
η = µ
2L according to Section 3.1. Meanwhile  the dual learning rate δ is set to balance its effect on
the dual progress as well as the primal progress. We specify it in the theoretical analysis part.

4

The computational complexity for each iteration in Algorithm 1 is O(ns). Both primal and dual
update could be viewed as roughly three steps: coordinate selection  variable update  and maintaining
AT y or Ax. The coordinate selection as Eqn. (8) for primal and the choice of I (t) for dual variable
respectively take O(d) and O(n) on average if implemented with the quick selection algorithm. The
variable update costs O(d) and O(n). The dominating cost is to maintain Ax as in Eqn. (9) that
takes O(ns)  and O(dk) of maintaining A(cid:62)y as in Eqn. (12). To balance the time budget for primal
and dual step  we set k = ns/d and achieve an overall complexity of O(ns) per iteration.
4.2 Theoretical Analysis
We derive convergence analysis under Assumption 4.1. The derivation consists of the analysis on the
primal progress  the balance of the dual progress  and their overall effect.

def

= L(x(t+1)  y(t)) − L( ¯x(t)  y(t))  where ¯x(t) is the primal optimal
Deﬁne the primal gap as ∆(t)
p
solution such that the dual D(y(t)) = L( ¯x(t)  y(t))  and is sparse enforced by the (cid:96)1 constraint.
= D∗ − D(y(t)). We analyze the convergence rate of duality gap ∆(t) ≡
The dual gap is ∆(t)
d
max{1  (β/α − 1)}∆(t)
p + ∆(t)
d .
Primal progress: Firstly  similar to the analysis in Section 3.1  we could derive that primal update
introduces a sufﬁcient descent as in Lemma A.2.

def

L(x(t+1)  y(t)) − L(x(t)  y(t)) ≤ − η
2

∆(t)
p .

Dual progress: With the GS-r rule to carefully select and update the most important k coordinates
in the dual variable in (10)  we are able to derive the following result on dual progress that diminishes
dual gap as well as inducing error.

2

kδ

n2 R(cid:107) ¯x(t) − x(t)(cid:107)2
(cid:123)

(cid:125)(cid:124)

dual progress

(cid:122)

(cid:122)

(cid:125)(cid:124)

primal hindrance

(cid:123)

−(cid:107)y(t) − y(t−1)(cid:107)2 ≤ − kδ
nβ

∆(t)

d +

Refer to Lemma A.5 for details.
Primal Dual progress: The overall progress evolves as:

(cid:122)

(cid:125)(cid:124)

primal progress

(cid:123)

(cid:107) ¯x(t) − x(t)(cid:107)2 .
∆(t) − ∆(t−1) ≤
In this way  we are able to connect the progress on duality gap with constant fraction of its value  and
achieve linear convergence:

L(x(t+1)  y(t)) − L(x(t)  y(t))− 1
4δ

Theorem 4.1. Given a function P (x) = (cid:80)n

(cid:107)y(t) − y(t−1)(cid:107)2 +

Set s to upper bound the sparsity of the primal optimal ¯x(t)  and learning rates η = µ
k ( L
2αµn2 (1 + 4 L
1
Algorithm 1 takes O( L
O(ns L
nµ ) log 1

µ ))−1. The duality gap ∆(t) = max{1  β
µ (1 + β
 ).

i x) + g(x) that satisﬁes Assumption 4.1.
2L   δ =
d generated by
 ) iterations to achieve  error. The overall complexity is

α − 1}∆(t)

µnβ + 5βR

i=1 fi(a(cid:62)

p + ∆(t)

3δRk
2n2

nµ ) log 1

µ (1 + β

Rβ

Rβ

α

α

n ) log 1

 ) iterations to achieve  error. The overall complexity is O(s(n + κ) log 1
 ).

2(cid:107)x(cid:107)2. Deﬁne the condition number κ = βR
2   δ = 1

For our target applications like elastic net  or ERM with smooth hinge loss  we are able to connect
the time complexity to the condition number of the primal form.
Corollary 4.1.1. Given a smooth hinge loss or quadratic loss fi that is β-smooth  and (cid:96)2 regularizer
g = µ
µ . Setting s upper bounds the sparsity of the
2µn2 )−1  the duality gap ∆(t) takes
primal optimal ¯x(t)  and learning rates η = 1
O((1 + κ
Our derivation of overall complexity implicitly requires ns ≥ d by setting k = sd/n ≥ 1. This
is true for our considered applications like SVM. Otherwise we choose k = 1 and the complexity
becomes O(max{d  ns}(1 + κ
In Table 1  we brieﬂy compare the time complexity of our algorithm with some benchmark algorithms:
(1) Accelerated Projected Gradient Descent (PGD) (2) Frank-Wolfe algorithm (FW) (3) Stochastic
Variance Reduced Gradient (SVRG) [15] (4) Stochastic Conditional Gradient Sliding (SCGS) [23]
and (5) Stochastic Variance-Reduced Conditional Gradient Sliding (STORC) [13]. The comparison
is not thorough but intends to select constrained optimization that improves the overall complexity
from different perspective. Among them  accelerated PGD improves conditioning of the problem 

nβ + 25R

n ) log 1

k ( 1

 ).

5

while SCGS and STORC reduces the dependence on number of samples. In the experimental session
we show that our proposal outperforms the listed algorithms under various conditions.

Algorithm
Frank Wolfe

Accelerated PGD [29]

SVRG [15]
SCGS [23]
STORC [13]

Primal Dual FW (ours)

Per Iteration Cost

O(nd)
O(nd)
O(nd)
O(κ2 #iter3
2 d)
O(κ2d + nd)
O(ns)

Iteration Complexity

O( 1
√
 )
κ log 1
 )

O(

O((1 + κ/n) log 1
 )

O( 1
 )
O(log 1
 )

O((1 + κ/n) log 1
 )

Table 1: Time complexity comparisons on the setting of Corollary 4.1.1. For clear comparison  we refer the per
iteration cost as the time complexity of outer iterations.

4.3 Extension to the Trace Norm Ball

Algorithm 2 Primal-Dual Block Generalized Frank-Wolfe Method for Trace Norm Ball
1: Input: Training data A ∈ Rn×d  primal and dual step size η  δ > 0. Target accuracy .
2: Initialize: X (0) ← 0 ∈ Rd×c  Y (0) ← 0 ∈ Rn×c  W (0) ≡ AX = 0 ∈ Rn×c  Z (0) ≡ A(cid:62)Y = 0 ∈
3: for t = 1  2 ···   T do
4:

Use Frank Wolfe to Update the primal variable:

Rd×c

X (t) ← (1 − η)X (t−1) + η ˜X  where ˜X ← (

1
2

 


8

)-approximation of Eqn. (18).

5:

Update W to maintain the value of AX:

W (t) ← (1 − η)W (t−1) + ηA ˜X

6:

Consider the potential dual update:

(cid:27)

(13)

(14)

(cid:104)W  Y (cid:105) − f

(cid:26)
(cid:13)(cid:13)(cid:13) ˜Yi : − Y (t−1)
(cid:26) ˜Yi :

i :

Y (t−1)

i :

(cid:13)(cid:13)(cid:13)2

  i ∈ [n].

˜Y (t) ← arg max

Y

∗

(Y ) − 1
2δ

(cid:107)Y − Y (t−1)(cid:107)2

Choose greedily the rows of the dual variable to update: let I (t) be the top k coordinates that maximize

Update the dual variable accordingly:

i : ←
Y (t)
Update Z to maintain the value of A(cid:62)Y

if i ∈ I (t)
otherwise.

(Y (t) − Y (t−1))

(15)

(16)

Z (t) ← Z (t−1) + A
(cid:62)

7:

8:

9: end for
10: Output: X (T )  Y (T )

(cid:40)

n(cid:88)

i=1

1
n

(cid:41)

We also extend our algorithm to matrix optimization over trace norm constraints:

min

(cid:107)X(cid:107)∗≤λ X∈Rd×c

fi(a(cid:62)

i X) + g(X)

.

(17)

(cid:40)

This formulation covers multi-label multi-class problems  matrix completion  afﬁne rank minimiza-
tion  and phase retrieval problems (see reference therein [3  1]). Equivalently  we solve the following
primal-dual problem:

min

(cid:107)X(cid:107)∗≤λ X∈Rd×c

max
Y ∈Rn×c

L(X  Y ) ≡ g(X) +

1
n

(cid:104)AX  Y (cid:105) − 1
n

f∗
i (yi)

.

Here yi is the i-th row of the dual matrix Y . For this problem  the partial update we enforced on the
primal matrix is to keep the update matrix low rank:
Z + ∇g(X (t−1))  X(cid:105) +
˜X ← arg min

  Z ≡ A(cid:62)Y (t−1). (18)

η(cid:107)X − X (t−1)(cid:107)2

(cid:107)X(cid:107)∗≤λ rank(X)≤s

(cid:26)

(cid:104) 1
n

(cid:41)

n(cid:88)
(cid:27)

i=1

L
2

6

F be the objective function in (18)  and let l∗

However  an exact solution to (18) requires computing the top s left and right singular vectors of
the matrix X (t−1) − 1
ηL (Z + ∇g(X (t−1)) ∈ Rd×c. Therefore we loosely compute an ( 1
2   /2)-
approximation  where  is the target accuracy  based on the following deﬁnition:
Deﬁnition 4.2 (Restated Deﬁnition 3.2 in [1]). Let lt(V ) = (cid:104)∇XL(X (t)  Y (t))  V −X (t)(cid:105)+ L
2 η(cid:107)V −
t = lt( ¯X (t)). Given parameters γ ≥ 0 and  ≥ 0 
X (t)(cid:107)2
a feasible solution V to (18) is called (γ  )-approximate if it satisﬁes l(V ) ≤ (1 − γ)l∗
The time dependence on the data size n  c  d  s is ncs + s2(n + c) [1]  and is again independent of
d. Meanwhile  the procedures to keep track of W (t) ≡ AX (t) requires complexity of nds + ncs 
while updating Y (t) requires dck operations. Therefore  by setting k ≤ ns(1/c + 1/d)  the iteration
complexity’s dependence on the data size becomes O(n(d + c)s) operations  instead of O(ndc) for
conducting a full projected gradient step. Recall that s upper bounds the rank of ¯X (t) ≤ min{d  c}.
The trace norm version mostly inherits the convergence guarantees for vector optimization. Refer to
the Appendix for details.
Assumption 4.2. We assume the following property on the primal form (17):

t + .

i exists and satisﬁes 1

α -smooth on some

• fi is convex  and β-smooth. Its convex conjugate f∗
convex set (could be Rc) and inﬁnity otherwise.
• Data matrix A satisﬁes R = max|I|≤k I⊂[n] σ2
• g is µ-strongly convex and L-smooth.

the largest singular value of X.

max(AI :) (≤ (cid:107)A(cid:107)2

2). Here σmax(X) denotes

The assumptions also cover smooth hinge loss as well as quadratic loss. With the similar assumptions 
the convergence analysis for Algorithm 2 is almost the same as Algorithm 1. The only difference
comes from the primal step where approximated update produces some error:
Primal progress: With the primal update rule in Algorithm 2  it satisﬁes L(X (t+1)  Y (t)) −
L(X (t)  Y (t)) ≤ − µ
16. (See Lemma A.7.) With no much modiﬁcation in the proof  we are
able to derive similar convergence guarantees for the trace norm ball.
Theorem 4.3. Given a function 1
n
s ≥rank( ¯X (t))  and learning rate η = µ
2L   δ ≤ 1
∆(t) generated by Algorithm 2 satisﬁes ∆(t) ≤ kδ

i X) + g(X) that satisﬁes Assumption 4.2. Setting
µ ))−1  the duality gap
α (1 +

µnβ + 5βR
k ( L
kδ+8βn ∆(t−1) + 

16 . Therefore it takes O( L

(cid:80)n
i=1 fi(a(cid:62)

2αµn2 (1 + 8 L

8L ∆(t)

p + 

β
α

Rβ

nµ ) log 1

 ) iterations to achieve  error.

We also provide a brief analysis on the difﬁculty to extend our algorithm to polytope-type constraints
in the Appendix A.9.
5 Experiments
We evaluate the Primal-Dual Block Generalized Frank-Wolfe algorithm by its performance on binary
classiﬁcation with smoothed hinge loss2. We refer the readers to Appendix A.7 for details about
smoothed hinge loss.
We compare the proposed algorithm against ﬁve benchmark algorithms: (1) Accelerated Projected
Gradient Descent (Acc PG) (2) Frank-Wolfe algorithm (FW) (3) Stochastic Variance Reduced
Gradient (SVRG) [15] (4) Stochastic Conditional Gradient Sliding (SCGS) [23] and (5) Stochastic
Variance-Reduced Conditional Gradient Sliding (STORC) [13]. We presented the time complexity
for each algorithm in Table 1. Three of them (FW  SCGS  STORC) are projection-free algorithms 
and the other two (Acc PG  SVRG) are projection-based algorithms. Algorithms are implemented in
C++  with the Eigen linear algebra library [12].
The six datasets used here are summarized in Table 2. All of them can be found in LIBSVM datasets
[4]. We augment the features of MNIST  ijcnn  and cob-rna by random binning [32]  which is a
standard technique for kernel approximation. Data is normalized. We set the (cid:96)1 constraint to be 300
and the (cid:96)2 regularize parameter to 10/n to achieve reasonable prediction accuracy. We refer the

2The codes to reproduce our results could be found in https://github.com/CarlsonZhuo/

primal_dual_frank_wolfe.

7

Figure 1: Convergence result comparison of different algorithms on smoothed hinge loss. For
six different datasets  we show the decrease of relative primal objective: (P (x(t)) − P ∗)/P ∗ over
CPU time. Our algorithm (brown) achieves around 10 times speedup over all other methods except
for the smallest dataset duke.

Dataset Name

duke breast-cancer [4]

rcv1 [4]

news20.binary [4]

MNIST.RB 0 VS 9 [4  32]

ijcnn.RB [4  32]
cob-rna.RB [4  32]

# Features

# Samples

7 129
47 236

1 355 191
894 499
58 699
81 398

44

20 242
19 996
11 872
49 990
59 535

# Non-Zero

313 676
1 498 952
9 097 916
1 187 200
14 997 000
5 953 500

Solution Sparsity (Ratio)

423 (5.9%)
1 169 (2.5%)
1 365 (0.1%)
8 450 (0.9%)
715 (1.2%)
958 (1.2%)

Table 2: Summary of the properties of the datasets.

readers to the Appendix C.1 for results of other choice of parameters. These datasets have various
scale of features  samples  and solution sparsity ratio.
The results are shown in Fig 1. To focus on the convergence property  we show the decrease of loss
function instead of prediction accuracy. From Fig 1  our proposed algorithm consistently outperforms
the benchmark algorithms. The winning margin is roughly proportional to the solution sparsity ratio 
which is consistent with our theory.
We also implement Algorithm 2 for trace norm ball and compare it with some prior work in the
Appendix C.2  especially Block FW [1]. We generated synthetic data with optimal solutions of
different ranks  and show that our proposal is consistently faster than others.
6 Conclusion
In this paper we consider a class of problems whose solutions enjoy some simple structure induced by
the constraints. We propose a FW type algorithm to exploit the simple structure and conduct partial
updates  reducing the time cost for each update remarkably while attaining linear convergence. For
a class of ERM problems  our running time depends on the sparsity/rank of the optimal solutions
rather than the ambient feature dimension. Our empirical studies verify the improved performance
compared to various state-of-the-art algorithms.
Acknowledgements. This work is supported by NSF Grants 1618689  EECS-1609279  CCF-
1302435  CNS-1704778  IIS-1546452  CCF-1564000  DMS 1723052  CCF 1763702  AF 1901292
and research gifts by Google  Western Digital and NVIDIA.

8

References
[1] Zeyuan Allen-Zhu  Elad Hazan  Wei Hu  and Yuanzhi Li. Linear convergence of a Frank-Wolfe
type algorithm over trace-norm balls. In Advances in Neural Information Processing Systems 
2017.

[2] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Convex multi-task feature

learning. Machine Learning  2008.

[3] Emmanuel J Candes  Yonina C Eldar  Thomas Strohmer  and Vladislav Voroninski. Phase

retrieval via matrix completion. SIAM review  2015.

[4] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM

transactions on intelligent systems and technology (TIST)  2011.

[5] Yin-Wen Chang  Cho-Jui Hsieh  Kai-Wei Chang  Michael Ringgaard  and Chih-Jen Lin. Train-
ing and testing low-degree polynomial data mappings via linear SVM. Journal of Machine
Learning Research  2010.

[6] Inderjit S Dhillon  Pradeep K Ravikumar  and Ambuj Tewari. Nearest neighbor based greedy

coordinate descent. In Advances in Neural Information Processing Systems  2011.

[7] Ding-Zhu Du and Panos M Pardalos. Minimax and applications. Springer Science & Business

Media  2013.

[8] Simon S Du and Wei Hu. Linear convergence of the primal-dual gradient method for convex-
concave saddle point problems without strong convexity. arXiv preprint arXiv:1802.01504 
2018.

[9] Miroslav Dudik  Zaid Harchaoui  and Jérôme Malick. Lifted coordinate descent for learning

with trace-norm regularization. In Artiﬁcial Intelligence and Statistics  2012.

[10] Dan Garber and Elad Hazan. Faster rates for the Frank-Wolfe method over strongly-convex

sets. In 32nd International Conference on Machine Learning  ICML 2015  2015.

[11] Donald Goldfarb  Garud Iyengar  and Chaoxu Zhou. Linear convergence of stochastic Frank

Wolfe variants. arXiv preprint arXiv:1703.07269  2017.

[12] Gaël Guennebaud  Benoît Jacob  et al. Eigen v3. http://eigen.tuxfamily.org  2010.

[13] Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization. In

International Conference on Machine Learning  2016.

[14] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1) 

2013.

[15] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  2013.

[16] Sham Kakade  Shai Shalev-Shwartz  and Ambuj Tewari. On the duality of strong convexity and

strong smoothness: Learning applications and matrix regularization.

[17] Sai Praneeth Karimireddy  Anastasia Koloskova  Sebastian U Stich  and Martin Jaggi. Efﬁcient

greedy coordinate descent for composite problems. arXiv preprint arXiv:1810.06999  2018.

[18] Thomas Kerdreux  Alexandre d’Aspremont  and Sebastian Pokutta. Restarting Frank-Wolfe.

International Conference on Machine Learning  2019.

[19] Subhash Khot. Hardness of approximating the shortest vector problem in lattices. Journal of

the ACM (JACM)  2005.

[20] Piyush Kumar and E Alper Yıldırım. A linearly convergent linear-time ﬁrst-order algorithm for

support vector classiﬁcation with a core set result. INFORMS Journal on Computing  2011.

[21] Simon Lacoste-Julien and Martin Jaggi. On the global linear convergence of Frank-Wolfe

optimization variants. In Advances in Neural Information Processing Systems  2015.

9

[22] Simon Lacoste-Julien  Martin Jaggi  Mark Schmidt  and Patrick Pletscher. Block-coordinate

Frank-Wolfe optimization for structural SVMs. arXiv preprint arXiv:1207.4747  2012.

[23] Guanghui Lan and Yi Zhou. Conditional gradient sliding for convex optimization. SIAM

Journal on Optimization  2016.

[24] Qi Lei  Ian EH Yen  Chao-yuan Wu  Inderjit S Dhillon  and Pradeep Ravikumar. Doubly greedy
primal-dual coordinate descent for sparse empirical risk minimization. In Proceedings of the
34th International Conference on Machine Learning-Volume 70. JMLR. org  2017.

[25] Qi Lei  Kai Zhong  and Inderjit S Dhillon. Coordinate-wise power method. In Advances in

Neural Information Processing Systems  2016.

[26] Gerard GL Meyer. Accelerated Frank-Wolfe algorithms. SIAM Journal on Control  1974.

[27] Ricardo Ñanculef  Emanuele Frandi  Claudio Sartori  and Héctor Allende. A novel Frank-Wolfe
algorithm. analysis and applications to large-scale SVM training. Information Sciences  2014.

[28] Yu Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization  2012.

[29] Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Springer Science

& Business Media  2013.

[30] Julie Nutini  Mark Schmidt  Issam Laradji  Michael Friedlander  and Hoyt Koepke. Coordinate
descent converges faster with the Gauss-Southwell rule than random selection. In International
Conference on Machine Learning  2015.

[31] Ting Kei Pong  Paul Tseng  Shuiwang Ji  and Jieping Ye. Trace norm regularization: Reformu-

lations  algorithms  and multi-task learning. SIAM Journal on Optimization  2010.

[32] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems  2008.

[33] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research  2013.

[34] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for sublinear time maximum

inner product search (MIPS). In Advances in Neural Information Processing Systems  2014.

[35] Jialei Wang and Lin Xiao. Exploiting strong convexity from data with primal-dual ﬁrst-order
algorithms. In Proceedings of the 34th International Conference on Machine Learning-Volume
70. JMLR. org  2017.

[36] Andrés Weintraub  Carmen Ortiz  and Jaime González. Accelerating convergence of the

Frank-Wolfe algorithm. Transportation Research Part B: Methodological  1985.

[37] Lin Xiao  Adams Wei Yu  Qihang Lin  and Weizhu Chen. Dscovr: Randomized primal-dual
block coordinate algorithms for asynchronous distributed optimization. Journal of Machine
Learning Research  2019.

[38] Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized empirical

risk minimization. arXiv preprint arXiv:1409.3257  2014.

[39] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of

the royal statistical society: series B (statistical methodology)  2005.

10

,Qi Lei
JIACHENG ZHUO
Constantine Caramanis
Inderjit Dhillon
Alexandros Dimakis