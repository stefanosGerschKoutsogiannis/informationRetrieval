2014,Probabilistic Differential Dynamic Programming,We present a data-driven  probabilistic trajectory optimization framework for systems with unknown dynamics  called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function  PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradient-based policy search methods  PDDP does not require a policy parameterization and learns a locally optimal  time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method  PDDP offers a superior combination of data-efficiency  learning speed  and applicability.,Probabilistic Differential Dynamic Programming

Yunpeng Pan and Evangelos A. Theodorou

Autonomous Control and Decision Systems Laboratory
Daniel Guggenheim School of Aerospace Engineering

Institute for Robotics and Intelligent Machines

Georgia Institute of Technology

Atlanta  GA 30332

ypan37@gatech.edu  evangelos.theodorou@ae.gatech.edu

Abstract

We present a data-driven  probabilistic trajectory optimization framework for sys-
tems with unknown dynamics  called Probabilistic Differential Dynamic Program-
ming (PDDP). PDDP takes into account uncertainty explicitly for dynamics mod-
els using Gaussian processes (GPs). Based on the second-order local approxi-
mation of the value function  PDDP performs Dynamic Programming around a
nominal trajectory in Gaussian belief spaces. Different from typical gradient-
based policy search methods  PDDP does not require a policy parameterization
and learns a locally optimal  time-varying control policy. We demonstrate the ef-
fectiveness and efﬁciency of the proposed algorithm using two nontrivial tasks.
Compared with the classical DDP and a state-of-the-art GP-based policy search
method  PDDP offers a superior combination of learning speed  data efﬁciency
and applicability.

1

Introduction

Differential Dynamic Programming (DDP) is a powerful trajectory optimization approach. Origi-
nally introduced in [1]  DDP generates locally optimal feedforward and feedback control policies
along with an optimal state trajectory. Compared with global optimal control approaches  the lo-
cal optimal DDP shows superior computational efﬁciency and scalability to high-dimensional prob-
lems. In the last decade  variations of DDP have been proposed in both control and machine learning
communities [2][3][4][5][6]. Recently  DDP was applied for high-dimensional policy search which
achieved promising results in challenging control tasks [7].
DDP is derived based on linear approximations of the nonlinear dynamics along state and control
trajectories  therefore it relies on accurate and explicit dynamics models. However  modeling a
dynamical system is in general a challenging task and model uncertainty is one of the principal
limitations of model-based methods. Various parametric and semi-parametric approaches have been
developed to address these issues  such as minimax DDP using Receptive Field Weighted Regression
(RFWR) by Morimoto and Atkeson [8]  and DDP using expert-demonstrated trajectories by Abbeel
et al. [9]. Motivated by the complexity of the relationships between states  controls and observations
in autonomous systems  in this work we take a Bayesian non-parametric approach using Gaussian
Processes (GPs).
Over last few years  GP-based control and Reinforcement Learning (RL) algorithms have increas-
ingly drawn more attention in control theory and machine learning communities. For instance 
the works by Rasmussen et al.[10]  Nguyen-Tuong et al.[11]  Deisenroth et al.[12][13][14] and
Hemakumara et al.[15] have demonstrated the remarkable applicability of GP-based control and RL
methods in robotics. In particular  a recently proposed GP-based policy search framework called
PILCO  developed by Deisenroth and Rasmussen [13] (an improved version has been developed by

1

Deisenroth  Fox and Rasmussen [14]) has achieved unprecedented performances in terms of data-
efﬁciency and policy learning speed. PILCO as well as most gradient-based policy search algorithms
require iterative methods (e.g. CG or BFGS) for solving non-convex optimization to obtain optimal
policies.
The proposed approach does not require a policy parameterization. Instead PDDP ﬁnds a linear  time
varying control policy based on Bayesian non-parametric representation of the dynamics and out-
performs PILCO in terms of control learning speed while maintaining a comparable data-efﬁciency.

2 Proposed Approach

The proposed PDDP framework consists of 1) a Bayesian non-parametric representation of the un-
known dynamics; 2) local approximations of the dynamics and value functions; 3) locally optimal
controller learning.

2.1 Problem formulation

We consider a general unknown dynamical system described by the following differential equation
(1)
where x ∈ Rn is the state  u ∈ Rm is the control and ω ∈ Rp is standard Brownian motion
noise. The trajectory optimization problem is deﬁned as ﬁnding a sequence of state and controls that
minimize the expected cost

dω ∼ N (0  Σω) 

dx = f (x  u)dt + Cdω 

x(t0) = x0 

(cid:20)

(cid:16)

(cid:17)

(cid:90) T

L(cid:16)

(cid:17)

(cid:21)

 

J π(x(t0)) = Ex

h

x(T )

(2)
where h(x(T )) is the terminal cost  L(x(t)  π(x(t))  t) is the instantaneous cost rate  u(t) =
π(x(t)) is the control policy. The cost J π(x(t0)) is deﬁned as the expectation of the total cost
accumulated from t0 to T . For the rest of our analysis  we denote xk = x(tk) in discrete-time
where k = 0  1  ...  H is the time step  we use this subscript rule for other variables as well.

x(t)  π(x(t))  t

dt

+

t0

2.2 Probabilistic model learning
The continuous functional mapping from state-control pair ˜x = (x  u) ∈ Rn+m to state transition
dx can be viewed as an inference with the goal of inferring dx given ˜x. We view this inference
as a nonlinear regression problem. In this subsection  we introduce the Gaussian processes (GP)
approach to learning the dynamics model in (1). A GP is deﬁned as a collection of random vari-
ables  any ﬁnite number subset of which have a joint Gaussian distribution. Given a sequence of
state-control pairs ˜X = {(x0  u0)  . . . (xH   uH )}  and the corresponding state transition dX =
{dx0  . . .   dxH}  a GP is completely deﬁned by a mean function and a covariance function. The
joint distribution of the observed output and the output corresponding to a given test state-control
pair ˜x∗ = (x∗  u∗) can be written as p
. The co-
variance of this multivariate Gaussian distribution is deﬁned via a kernel matrix K(xi  xj). In partic-
2 (xi−xj)TW(xi−xj)) 
ular  in this paper we consider the Gaussian kernel K(xi  xj) = σ2
with σs  σn  W the hyper-parameters. The kernel function can be interpreted as a similarity measure
of random variables. More speciﬁcally  if the training pairs ˜Xi and ˜Xj are close to each other in the
kernel space  their outputs dxi and dxj are highly correlated. The posterior distribution  which is
also a Gaussian  can be obtained by constraining the joint distribution to contain the output dx∗ that
is consistent with the observations. Assuming independent outputs (no correlation between each
output dimension) and given a test input ˜xk = (xk  uk) at time step k  the one-step predictive mean
and variance of the state transition are speciﬁed as

(cid:104) K( ˜X  ˜X) + σnI K( ˜X  ˜x∗)

K(˜x∗  ˜X)
s exp(− 1

(cid:17) ∼ N(cid:16)

(cid:16) dX

(cid:105)(cid:17)

K(˜x∗  ˜x∗)

dx∗

0 

Ef [dxk] = K(˜xk  ˜X)(K( ˜X  ˜X) + σnI)−1dX 

VARf [dxk] = K(˜xk  ˜xk) − K(˜xk  ˜X)(K( ˜X  ˜X) + σnI)−1K( ˜X  ˜xk).

(3)

The state distribution at k = 1 is p(x1) ∼ N (µ1  Σ1) where the state mean and variance are
µ1 = x0 +Ef [dx0]  Σ1 = VARf [dx0]. When propagating the GP-based dynamics over a trajectory

2

Next  we compute the predictive covariance matrix

(cid:82) p(f (˜xk)|˜xk)p(˜xk)d˜xk. Generally  this predictive distribution cannot be computed analytically

of time horizon H  the input state-control pair ˜xk becomes uncertain with a Gaussian distribution
(initially ˜x0 is deterministic). Here we deﬁne the joint distribution over state-control pair at k as
p(˜xk) = p(xk  uk) ∼ N ( ˜µk  ˜Σk). Thus the distribution over state transition becomes p(dxk) =
because the nonlinear mapping of an input Gaussian distribution leads to a non-Gaussian predictive
distribution. However  the predictive distribution can be approximated by a Gaussian p(dxk) ∼
N (dµk  dΣk) [16]. Thus the state distribution at k + 1 is also a Gaussian N (µk+1  Σk+1) [14]

µk+1 = µk + dµk 

(4)
Given an input joint distribution N ( ˜µk  ˜Σk)  we employ the moment matching approach [16][14]
to compute the predictive distribution. The predictive mean dµk is evaluated as

f  ˜xk [dxk  xk].

Σk+1 = Σk + dΣk + COV

f  ˜xk [xk  dxk] + COV

dµk = E˜xk

(cid:34)

dΣk =

COVf  ˜xk

(cid:90)

]

]

. . .

. . .

. . .

 

[dxk1

[dxk1

  dxkn ]

[dxkn   dxk1

COVf  ˜xk

.
.
.
VARf  ˜xk

VARf  ˜xk
.
.
.

(cid:2)Ef [dxk](cid:3) =

Ef [dxk]N(cid:0) ˜µk  ˜Σk

(cid:1)d˜xk.
(cid:35)
(cid:2)Ef [dxki](cid:3)2
(cid:2)VARf [dxki](cid:3) + E˜xk
(cid:2)Ef [dxki]Ef [dxkj ](cid:3) − E˜xk [Ef [dxki]]E˜xk [Ef [dxkj ]].
(cid:2)˜xkEf [dxk]T(cid:3) − E˜xk [˜xk]Ef  ˜xk [dxk]T.
(cid:26)

(cid:2)Ef [dxki]2(cid:3) − E˜xk

(cid:18)

[dxkn ]

(cid:17)(cid:19)(cid:27)

(cid:16)

Θ∗ = argmax

log

p

dX| ˜X  Θ

.

where the variance term on the diagonal for output dimension i is obtained as

VARf  ˜xk [dxki] = E˜xk

 
and the off-diagonal covariance term for output dimension i  j is given by the expression

COV

f  ˜xk [dxki   dxkj ] = E˜xk

The input-output cross-covariance is formulated as

COV

f  ˜xk [˜xk  dxk] = E˜xk

(7)
COV
f  ˜xk [xk  dxk] can be easily obtained as a sub-matrix of (7). The kernel or hyper-parameters
Θ = (σn  σs  W) are learned by maximizing the log-likelihood of the training outputs given the
inputs

(5)

(6)

(8)

(10)

(11)

This optimization problem can be solved using numerical methods such as conjugate gradient [17].

Θ

2.3 Local dynamics model

In DDP-related algorithms  a local model along a nominal trajectory (¯xk  ¯uk)  is created based
on: i) a ﬁrst or second-order linear approximation of the dynamics model; ii) a second-order local
approximation of the value function. In our proposed PDDP framework  we will create a local model
along a trajectory of state distribution-control pair (p(¯xk)  ¯uk). In order to incorporate uncertainty
explicitly in the local model  we introduce the Gaussian augmented state (or the belief over xk)
zk = [µk vec(Σk)]T ∈ Rn+n×n where vec(Σk) is the vectorization of Σk. Now we create a local
linear model of the dynamics. Based on eq.(4)  the dynamics model with the augmented state is
(9)
Deﬁne the control and state variations δzk = zk − ¯zk and δuk = uk − ¯uk. In this work we consider
the ﬁrst-order expansion of the dynamics. More precisely we have

zk+1 = F(zk  uk).

where the Jacobian matrices F x

k and F u

 ∈ R(n+n2)×(n+n2) 

k δuk 

δzk+1 = F z

 ∂µk+1
(cid:34) ∂µk+1

k δzk + F u
k are speciﬁed as
∂µk+1
∂Σk
Σk+1
∂Σk
∈ R(n+n2)×m.

∂µk
∂Σk+1
∂µk

(cid:35)

∂uk

∂Σk+1

F z
k = ∇zkF =

k = ∇ukF =
F u

∂uk

The partial derivatives ∂µk+1
are computed analytically.
∂µk
Their forms are provided in the supplementary document of this work. For numerical implementa-
tion  the dimension of the augmented state can be reduced by eliminating the redundancy of Σk and
the principle square root of Σk may be used for numerical robustness.

  ∂Σk+1
∂Σk

  ∂Σk+1
∂µk

  ∂Σk+1
∂uk

∂µk+1
∂Σk

∂µk+1
∂uk

 

 

3

2.4 Cost function

(cid:104)L(xk  uk)
(cid:105)

In the classical DDP and many optimal control problems  the following quadratic cost function is
used

L(xk  uk) = (xk − xgoal

)TQ(xk − xgoal

(12)
is the target state. Given the distribution p(xk) ∼ N (µk  Σk)  the expectation of

k Ruk 

) + uT

k

k

where xgoal
original quadratic cost function is formulated as

k

Ex

= tr(QΣk) + (µk − xgoal

)TQ(µk − xgoal

(13)
In PDDP  we use the cost function L(zk  uk) = Exk [L(xk  uk)]. The analytic expressions of partial
L(zk  uk) can be easily obtained. The cost function (13) scales
derivatives ∂
∂zk
linearly with the state covariance  therefore the exploration strategy of PDDP is balanced between
the distance from the target and the variance of the state. This strategy ﬁts well with DDP-related
frameworks that rely on local approximations of the dynamics. A locally optimal controller obtained
from high-risk explorations in uncertain regions might be highly undesirable.

L(zk  uk) and ∂

k Ruk.

) + uT

∂uk

k

k

2.5 Control policy

The Bellman equation for the value function in discrete-time is speciﬁed as follows

(cid:34)

(cid:124)

(cid:35)

|xk

(cid:17)
(cid:125)

(cid:16)F(zk  uk)  k + 1
(cid:123)(cid:122)
(cid:20) δzk

Q(zk uk)

V (zk  k) = min
uk

E

L(zk  uk) + V

.

(14)

We create a quadratic local model of the value function by expanding the Q-function up to the
second order
Qk(zk +δzk  uk +δuk) ≈ Q0

k Qzu
  (15)
k
k Quu
k
k = ∇zQk(zk  uk).
where the superscripts of the Q-function indicate derivatives. For instance  Qz
For the rest of the paper  we will use this superscript rule for L and V as well. To ﬁnd the optimal
control policy  we compute the local variations in control δ ˆuk that maximize the Q-function

(cid:21)T(cid:20) Qzz

(cid:21)(cid:20) δzk

kδzk +Qu

k +Qz

kδuk +

(cid:21)

Quz

δuk

δuk

1
2

(cid:104)

(cid:105)

δ ˆuk = arg max
uk

Qk(zk + δzk  uk + δuk)

(cid:124)

= −(Quu

(cid:123)(cid:122)
(cid:125)
k )−1Qu

k

(cid:124)

Ik

−(Quu

(cid:123)(cid:122)
(cid:125)
k )−1Quz

k

Lk

δzk = Ik + Lkδzk.

(16)
The optimal control can be found as ˆuk = ¯uk + δ ˆuk. The quadratic expansion of the value function
is backward propagated based on the equations that follow

k F z
k = Lz
k = Lu
k   Qu
k + V x
Qz
k + (F z
k = Lzz
k F z
k   Quz
Qzz
k )TV zz
V z
Vk−1 = Vk + Qu
k−1 = Qz
kIk 

k F u
k + V x
k  
k = Luz
k + (F u
k + Qu
kLk 

k F u
k  
(17)
The second-order local approximation of the value function is propagated backward in time iter-
atively. We use the learned controller to generate a locally optimal trajectory by propagating the
dynamics forward in time. The control policy is a linear function of the augmented state zk  there-
fore the controller is deterministic. The state propagations have been discussed in Sec. 2.2.

k F z
V zz
k−1 = Qzz

k = Luu
k Lk.

k + (F u

k + Qzu

k   Quu

k )TV zz

k )TV zz

2.6 Summary of algorithm

The proposed algorithm can be summarized in Algorithm 1. The algorithm consists of 8 modules.
In Model learning (Step 1-2) we sample trajectories from the original physical system in order to
collect training data and learn a probabilistic model. In Local approximation (Step 4) we obtain
a local linear approximation (10) of the learned probabilistic model along a nominal trajectory by
computing Jacobian matrices (11). In Controller learning (Step 5) we compute a local optimal con-
trol sequence (16) by backward-propagation of the value function (17). To ensure convergence  we
employ the line search strategy as in [2]. We compute the control law as δ ˆuk = αIk + Lkδzk.

4

Initially α = 1  then decrease it until the expected cost is smaller than the previous one. In Forward
propagation (Step 6)  we apply the control sequence from last step and obtain a new nominal trajec-
tory for the next iteration. In Convergence condition (Step 7)  we set a threshold on the accumulated
cost J∗ such that when J π < J∗  the algorithm is terminated with the optimized state and control
trajectory. In Interaction condition (Step 8)  when the state covariance Σk exceeds a threshold Σtol 
we sample new trajectories from the physical system using the control obtained in step 5  and go
back to step 2 to learn a new model. The old GP training data points are removed from the training
set to keep its size ﬁxed. Finally in Nominal trajectory update (step 9)  the trajectory obtained in
Step 6 or 8 becomes the new nominal trajectory for the next iteration. An simple illustration of the
algorithm is shown in Fig. 3a. Intuitively  PDDP requires interactions with the physical systems
only if the GP model no longer represents the true dynamics around the nominal trajectory.

Given: A system with unknown dynamics  target states
Goal : An optimized trajectory of state and control

1 Generate N state trajectories by applying random control sequences to the physical system (1);
2 Obtain state and control training pairs from sampled trajectories and optimize the

hyper-parameters of GP (8);

3 for i = 1 to Imax do
4
5

6

7
8

Compute a linear approximation of the dynamics along (¯zk  ¯uk) (10);
Backpropagate in time to get the locally optimal control ˆuk = ¯uk + δ ˆuk and value function
V (zk  k) according to (16) (17);
Forward propagate the dynamics (9) by applying the optimal control ˆuk  obtain a new
trajectory (zk  uk);
if Converge then Break the for loop;
if Σk > Σtol then Apply the optimal control to the physical system to generate a new
nominal trajectory (zk  uk) and N − 1 additional trajectories by applying small variations
of the learned controller  and go back to step 2;
Set ¯zk = zk  ¯uk = uk and i = i + 1  go back to step 4;

9
10 end
11 Apply the optimized controller to the physical system  obtain the optimized trajectory.

Algorithm 1: PDDP algorithm

2.7 Computational complexity

the complexity of one-step moment matching (2.2) is O(cid:0)(N )2n2(n+m)(cid:1) [14]  which is ﬁxed during

Dynamics propagation: The major computational effort is devoted to GP inferences. In particular 
the iterative process of PDDP. We found a small number of sampled trajectories (N ≤ 5) are able
to provide good performances for a system of moderate size (6-12 state dimensions). However  for
higher dimensional problems  sparse or local approximation of GP (e.g. [11][18][19]  etc) may be
used to reduce the computational cost of GP dynamics propagation.
Controller learning: According to (16)  learning policy parameters Ik and Lk requires computing
k   which has the computational complexity of O(m3)  where m is the dimension
the inverse of Quu
of control input. As a local trajectory optimization method  PDDP offers comparable scalability to
the classical DDP.

2.8 Relation to existing works

Here we summarize the novel features of PDDP in comparison with some notable DDP-related
frameworks for stochastic systems (see also Table 1). First  PDDP shares some similarities with
the belief space iLQG [6] framework  which approximates the belief dynamics using an extended
Kalman ﬁlter. Belief space iLQG assumes a dynamics model is given and the stochasticity comes
from the process noises. PDDP  however  is a data-driven approach that learns the dynamics models
and controls from sampled data  and it takes into account model uncertainties by using GPs. Second 
PDDP is also comparable with iLQG-LD [5]  which applies Locally Weighted Projection Regression
(LWPR) to represent the dynamics.
iLQG-LD does not incorporate model uncertainty therefore
requires a large amount of data to learn an accurate model. Third  PDDP does not suffer from the
high computational cost of ﬁnite differences used to numerically compute the ﬁrst-order expansions

5

[2][6] and second-order expansions [4] of the underlying stochastic dynamics. PDDP computes
Jacobian matrices analytically (11).

State

Dynamics model

Linearization

Belief space iLQG[6]

iLQG-LD[5]

iLQG[2]/sDDP[4]

PDDP
µk  Σk
Unknown

µk  Σk
Known

Analytic Jacobian Finite differences Analytic Jacobian Finite differences

xk

Unknown

xk

Known

Table 1: Comparison with DDP-related frameworks

3 Experimental Evaluation

We evaluate the PDDP framework using two nontrivial simulated examples: i) cart-double inverted
pendulum swing-up; ii) six-link robotic arm reaching. We also compare the learning efﬁciency
of PDDP with the classical DDP [1] and PILCO [13][14]. All experiments were performed in
MATLAB.

3.1 Cart-double inverted pendulum swing-up

Cart-Double Inverted Pendulum (CDIP) swing-up is a challenging control problem because the sys-
tem is highly underactuated with 3 degrees of freedom and only 1 control input. The system has
6 state-dimensions (cart position/velocity  link 1 2 angles and angular velocities). The goal of the
swing-up problem is to ﬁnd a sequence of control input to force both pendulums from initial position
(π π) to the inverted position (2π 2π). The balancing task requires the velocity of the cart  angular
velocities of both pendulums to be zero. We sample 4 initial trajectories with time horizon H = 50.
The CDIP swing-up problem has been solved by two controllers for swing-up and balancing  re-
spectively [20]. PILCO [14] is one of the few RL methods that is able to complete this task without
knowing the dynamics. The results are shown in Fig.1.

(a)

(b)

Figure 1: Results for the CDIP task. (a) Optimized state trajectories of PDDP. Solid lines indicate
means  errorbars indicate variances. (b) Cost comparison of PDDP  DDP and PILCO. Costs (eq. 13)
were computed based on sampled trajectories by applying the ﬁnal controllers.

3.2 Six-link robotic arm

The six-link robotic arm model consists of six links of equal length and mass  connected in an open
chain with revolute joints. The system has 6 degrees of freedom  and 12 state dimensions (angle
and angular velocity for each joint). The goal for the ﬁrst 3 joints is to move to the target angle π
and for the rest 3 joints to − π
4
4 . The desired velocities for all 6 joints are zeros. We sample 2 initial
trajectories with time horizon H = 50. The results are shown in Fig. 2.

3.3 Comparative analysis

DDP: Originally introduced in the 70’s  the classical DDP [1] is still one of the most effective and
efﬁcient trajectory optimization approaches. The major differences between DDP and PDDP can

6

05101520253035404550−4−2024681012CDIP state trajectoriesTime steps Cart positionCart velocityLink1 angular velocityLink2 angular velocityLink1 angleLink2 angle0510152025303540455000.20.40.60.81Time stepsCDIP cost PDDPDDPPILCO(a)

(b)

Figure 2: Results for the 6-link arm task.
(a) Optimized state trajectories of PDDP. Solid lines
indicate means  errorbars indicate variances. (b) Cost comparison of PDDP  DDP and PILCO. Costs
(eq. 13) were computed based on sampled trajectories by applying the ﬁnal controllers.

be summarized as follow: ﬁrstly  DDP relies on a given accurate dynamics model  while PDDP is
a data-driven framework that learns a locally accurate model by forward sampling; secondly  DDP
does not deal with model uncertainty  PDDP takes into account model uncertainty using GPs and
perform local dynamic programming in Gaussian belief spaces; thirdly  generally in applications
of DDP linearizations are performed using ﬁnite differences while in PDDP Jacobian matrices are
computed analytically (11).
PILCO: The recently proposed PILCO [14] framework has demonstrated state-of-the-art learning
efﬁciency compared with other methods such as [21][22]. The proposed PDDP is different from
PILCO in several ways. Firstly  based on local linear approximation of dynamics and quadratic
approximation of the value function  PDDP ﬁnds linear  time-varying feedforward and feedback
policy  PILCO requires an a priori policy parameterization and an extra optimization solver. Sec-
ondly  PDDP keeps a ﬁxed size of training data for GP inferences  while PILCO adds new data to
the training set after each trial (recently  the authors applied sparse GP approximation [19] in an
improved version of PILCO when the data size reached a threshold). Thirdly  by using the Gaussian
belief and cost function (13)  PDDP’s exploration scheme is balanced between the distance from
the target and the variance of the state. PILCO employs a saturating cost function which leads to
automatic explorations in the high-variance regions in the early stages of learning.
In both tasks  PDDP  DDP and PILCO bring the system to the desired states. The resulting tra-
jectories for PDDP are shown in Fig.1a and 2a. The reason for low variances of some optimized
trajectories is that during ﬁnal stage of learning  interactions with the physical systems (forward
samplings using the locally optimal controller) would reduce the variances signiﬁcantly. The costs
are shown in Fig. 1b and 2b. For both tasks  PDDP and DDP performs similarly and slightly dif-
ferent from PILCO in terms of cost reduction. The major reasons for this difference are: i) different
cost functions used by these methods; ii) we did not impose convergence condition for the optimized
trajectories on PILCO. We now compare PDDP with DDP and PILCO in terms of data-efﬁciency
and controller learning speed.
Data efﬁciency: As shown in Fig.4a  in both tasks  PDDP performs slightly worse than PILCO in
terms of data efﬁciency based on the number of interactions required with the physical systems. For
the systems used for testing  PDDP requires around 15% − 25% more interactions than PILCO.
The number of interactions indicates the amount of sampled trajectories required from the physical
system. At each trial we sample N trajectories from the physical systems (algorithm 1). Possible
reasons for the slightly worse performances are:
i) PDDP’s policy is linear which is restrictive 
while PILCO yields nonlinear policy parameterizations; ii) PDDP’s exploration scheme is more
conservative than PILCO in the early stages of learning. We believe PILCO is the more data efﬁcient
for these tasks. However  PDDP is able to offer close performances thanks to the probabilistic
representation of the dynamics as well as the use of Gaussian belief (augmented state).
Learning speed: In terms of total computational time required to obtain the ﬁnal controller  PDDP
outperforms PILCO signiﬁcantly as shown in Fig.4b. For the 6 and 12 dimensional systems used
for testing  PILCO requires an iterative method (e.g. CG or BFGS) to solve high dimensional opti-
mization problems (depending on the policy parameterization)  while PDDP computes local optimal
controls (16) without an extra optimizer. In terms of computational time per iteration  as shown in

7

5101520253035404550−101Angle5101520253035404550−101Angular velocityTime steps0510152025303540455000.511.522.53Time steps6−link arm Cost PDDPDDPPILCOFig.3b  PDDP is slower than the classical DDP due to the high computational cost of GP dynamics
propagations. However  for DDP  the time dedicated to linearizing the dynamics model is around
70% − 90% of the total time per iteration for the two tasks considered in this work. PDDP avoids
the high computational cost of ﬁnite differences by evaluating all Jacobian matrices analytically  the
time dedicated to linearization is less than 10% of the total time per iteration.

(a)

(b)

Figure 3: (a) An intuitive illustration of the PDDP framework. (b) Comparison of PDDP and DDP
in terms of the computational time per iteration (in seconds) for the CDIP (left subﬁgure) and 6-link
arm (right subﬁgure) tasks. Green indicates time for performing linearization  cyan indicates time
for forward and backward sweeps (Sec. 2.6).

(a)

(b)

Figure 4: Comparison of PDDP and PILCO in terms of data efﬁciency and controller learning speed.
(a) Number of interactions with the physical systems required to obtain the ﬁnal results in Fig. 1
and 2. (b) Total computational time (in minutes) consumed to obtain the ﬁnal controllers.

4 Conclusions

In this work we have introduced a probabilistic model-based control and trajectory optimization
method for systems with unknown dynamics based on Differential Dynamic Programming (DDP)
and Gaussian processes (GPs)  called Probabilistic Differential Dynamic Programming (PDDP).
PDDP takes model uncertainty into account explicitly by representing the dynamics using GPs and
performing local Dynamic Programming in Gaussian belief spaces. Based on the quadratic approxi-
mation of the value function  PDDP yields a linear  locally optimal control policy and features a more
efﬁcient control improvement scheme compared with typical gradient-based policy search methods.
Thanks to the probabilistic representation of the dynamics  PDDP offers reasonable data efﬁciency
comparable to a state of the art GP-based policy search method [14]. In general  local trajectory op-
timization is a powerful approach to challenging control and RL problems. Due to its model-based
nature  model inaccuracy has always been the major obstacle for advanced applications. Grounded
on the solid developments of classical trajectory optimization and Bayesian machine learning  the
proposed PDDP has demonstrated encouraging performance and potential for many applications.

Acknowledgments

We thank reviewers for their constructive feedback and helpful comments.

8

Control policyGP dynamicsLocal Model  Cost function Physical systemDDPPDDP0246810121416Time per iteration (sec) for CDIP DDPPDDP01020304050Time per iteration (sec) for 6−link arm Dynamics linearizationForward/backward passDyanmics linearizationForward/backward passCDIP6−Link arm05101520253035Number of interactions PDDPPILCOCDIP6−Link arm050010001500Total time (minutes) PDDPPILCOReferences
[1] D.H. Jacobson and D.Q. Mayne. Differential dynamic programming. Elsevier Sci. Publ.  1970.
[2] E. Todorov and W. Li. A generalized iterative lqg method for locally-optimal feedback control
of constrained nonlinear stochastic systems. In American Control Conference  pages 300–306 
June 2005.

[3] Y. Tassa  T. Erez  and W. D. Smart. Receding horizon differential dynamic programming. In

NIPS  pages 1465–1472.

[4] E. Theodorou  Y. Tassa  and E. Todorov. Stochastic differential dynamic programming. In

American Control Conference  pages 1125–1132  June 2010.

[5] D. Mitrovic  S. Klanke  and S. Vijayakumar. Adaptive optimal feedback control with learned
internal dynamics models. In From Motor Learning to Interaction Learning in Robots  pages
65–84. Springer  2010.

[6] J. Van Den Berg  S. Patil  and R. Alterovitz. Motion planning under uncertainty using it-
erative local optimization in belief space. The International Journal of Robotics Research 
31(11):1263–1278  2012.

[7] S. Levine and V. Koltun. Variational policy search via trajectory optimization. In NIPS  pages

207–215. 2013.

[8] J. Morimoto and C.G. Atkeson. Minimax differential dynamic programming: An application

to robust biped walking. In NIPS  pages 1539–1546  2002.

[9] P. Abbeel  A. Coates  M. Quigley  and A. Y. Ng. An application of reinforcement learning to

aerobatic helicopter ﬂight. In NIPS  pages 1–8  2007.

[10] C. E. Rasmussen and M. Kuss. Gaussian processes in reinforcement learning. In NIPS  pages

751–759  2003.

[11] D. Nguyen-Tuong  J. Peters  and M. Seeger. Local gaussian process regression for real time

online model learning. In NIPS  pages 1193–1200  2008.

[12] M. P. Deisenroth  C. E. Rasmussen  and J. Peters. Gaussian process dynamic programming.

Neurocomputing  72(7):1508–1524  2009.

[13] M. P. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efﬁcient approach to

policy search. In ICML  pages 465–472  2011.

[14] M. P. Deisenroth  D. Fox  and C. E. Rasmussen. Gaussian processes for data-efﬁcient learning
in robotics and control. IEEE Transsactions on Pattern Analysis and Machine Intelligence 
27:75–90  2014.

[15] P. Hemakumara and S. Sukkarieh. Learning uav stability and control derivatives using gaussian

processes. IEEE Transactions on Robotics  29:813–824  2013.

[16] J. Quinonero Candela  A. Girard  J. Larsen  and C. E. Rasmussen. Propagation of uncertainty in
bayesian kernel models-application to multiple-step ahead forecasting. In IEEE International
Conference on Acoustics  Speech  and Signal Processing  2003.

[17] C.K.I Williams and C.E. Rasmussen. Gaussian processes for machine learning. MIT Press 

2006.

[18] L. Csat´o and M. Opper. Sparse on-line gaussian processes. Neural Computation  14(3):641–

668  2002.

[19] E. Snelson and Z. Ghahramani. Sparse gaussian processes using pseudo-inputs. In NIPS  pages

1257–1264  2005.

[20] W. Zhong and H. Rock. Energy and passivity based control of the double inverted pendulum

on a cart. In International Conference on Control Applications  pages 896–901  Sept 2001.

[21] T. Raiko and M. Tornio. Variational bayesian learning of nonlinear hidden state-space models

for model predictive control. Neurocomputing  72(16):3704–3712  2009.

[22] H. van Hasselt. Insights in reinforcement learning. Hado van Hasselt  2011.

9

,Misha Denil
Babak Shakibi
Laurent Dinh
Marc'Aurelio Ranzato
Nando de Freitas
Yunpeng Pan
Evangelos Theodorou
Yossi Arjevani
Ohad Shamir
Ji Hyun Bak
Jonathan Pillow