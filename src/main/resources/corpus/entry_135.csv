2014,A Multiplicative Model for Learning Distributed Text-Based Attribute Representations,In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to a wide variety of concepts  such as document indicators (to learn sentence vectors)  language indicators (to learn distributed language representations)  meta-data and side information (such as the age  gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification  cross-lingual document classification  and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.,A Multiplicative Model for Learning Distributed

Text-Based Attribute Representations

Ryan Kiros  Richard S. Zemel  Ruslan Salakhutdinov

University of Toronto

Canadian Institute for Advanced Research

{rkiros  zemel  rsalakhu}@cs.toronto.edu

Abstract

In this paper we propose a general framework for learning distributed represen-
tations of attributes: characteristics of text whose representations can be jointly
learned with word embeddings. Attributes can correspond to a wide variety of
concepts  such as document indicators (to learn sentence vectors)  language in-
dicators (to learn distributed language representations)  meta-data and side infor-
mation (such as the age  gender and industry of a blogger) or representations of
authors. We describe a third-order model where word context and attribute vectors
interact multiplicatively to predict the next word in a sequence. This leads to the
notion of conditional word similarity: how meanings of words change when con-
ditioned on different attributes. We perform several experimental tasks including
sentiment classiﬁcation  cross-lingual document classiﬁcation  and blog author-
ship attribution. We also qualitatively evaluate conditional word neighbours and
attribute-conditioned text generation.

1

Introduction

Distributed word representations have enjoyed success in several NLP tasks [1  2]. More recently 
the use of distributed representations have been extended to model concepts beyond the word level 
such as sentences  phrases and paragraphs [3  4  5  6]  entities and relationships [7  8] and embed-
dings of semantic categories [9  10].
In this paper we propose a general framework for learning distributed representations of attributes:
characteristics of text whose representations can be jointly learned with word embeddings. The use
of the word attribute in this context is general. Table 1 illustrates several of the experiments we
perform along with the corresponding notion of attribute. For example  an attribute can represent
an indicator of the current sentence or language being processed. This allows us to learn sentence
and language vectors  similar to the proposed model of [6]. Attributes can also correspond to side
information  or metadata associated with text. For instance  a collection of blogs may come with
information about the age  gender or industry of the author. This allows us to learn vectors that can
capture similarities across metadata based on the associated body of text. The goal of this work
is to show that our notion of attribute vectors can achieve strong performance on a wide variety of
NLP related tasks. In particular  we demonstrate strong quantitative performance on three highly
diverse tasks: sentiment classiﬁcation  cross-lingual document classiﬁcation  and blog authorship
attribution.
To capture these kinds of interactions between attributes and text  we propose to use a third-order
model where attribute vectors act as gating units to a word embedding tensor. That is  words are
represented as a tensor consisting of several prototype vectors. Given an attribute vector  a word
embedding matrix can be computed as a linear combination of word prototypes weighted by the
attribute representation. During training  attribute vectors reside in a separate lookup table which
can be jointly learned along with word features and the model parameters. This type of three-way

1

Table 1: Summary of tasks and attribute types used in our experiments. The ﬁrst three are quantita-
tive while the second three are qualitative.

Task

Sentiment Classiﬁcation

Cross-Lingual Classiﬁcation

Authorship Attribution

Conditional Text Generation
Structured Text Generation
Conditional Word Similarity

Dataset

Sentiment Treebank

RCV1/RCV2
Blog Corpus

Gutenberg Corpus
Gutenberg Corpus
Blogs & Europarl

Attribute type
Sentence Vector
Language Vector
Author Metadata

Book Vector

Part of Speech Tags

Author Metadata / Language

interaction can be embedded into a neural language model  where the three-way interaction consists
of the previous context  the attribute and the score (or distribution) of the next word after the context.
Using a word embedding tensor gives rise to the notion of conditional word similarity. More specif-
ically  the neighbours of word embeddings can change depending on which attribute is being con-
ditioned on. For example  the word ‘joy’ when conditioned on an author with the industry attribute
‘religion’ appears near ‘rapture’ and ‘god’ but near ‘delight’ and ‘comfort’ when conditioned on
an author with the industry attribute ‘science’. Another way of thinking of our model would be
the language analogue of [11]. They used a factored conditional restricted Boltzmann machine for
modelling motion style deﬁned by real or continuous valued style variables. When our factorization
is embedded into a neural language model  it allows us to generate text conditioned on different
attributes in the same manner as [11] could generate motions from different styles. As we show in
our experiments  if attributes are represented by different books  samples generated from the model
learn to capture associated writing styles from the author. Furthermore  we demonstrate a strong
performance gain for authorship attribution when conditional word representations are used.
Multiplicative interactions have also been previously incorporated into neural language models. [12]
introduced a multiplicative model where images are used for gating word representations. Our
framework can be seen as a generalization of [12] and in the context of their work an attribute would
correspond to a ﬁxed representation of an image. [13] introduced a multiplicative recurrent neural
network for generating text at the character level. In their model  the character at the current timestep
is used to gate the network’s recurrent matrix. This led to a substantial improvement in the ability to
generate text at the character level as opposed to a non-multiplicative recurrent network.

2 Methods
In this section we describe the proposed models. We ﬁrst review the log-bilinear neural language
model of [14] as it forms the basis for much of our work. Next  we describe a word embedding
tensor and show how it can be factored and introduced into a multiplicative neural language model.
This is concluded by detailing how our attribute vectors are learned.

2.1 Log-bilinear neural language models
The log-bilinear language model (LBL) [14] is a deterministic model that may be viewed as a feed-
forward neural network with a single linear hidden layer. Each word w in the vocabulary is rep-
resented as a K-dimensional real-valued vector rw ∈ RK. Let R denote the V × K matrix of
word representation vectors where V is the vocabulary size. Let (w1  . . . wn−1) be a tuple of n − 1
words where n − 1 is the context size. The LBL model makes a linear prediction of the next word
representation as

(1)
where C(i)  i = 1  . . .   n − 1 are K × K context parameter matrices. Thus  ˆr is the predicted
representation of rwn. The conditional probability P (wn = i|w1:n−1) of wn given w1  . . .   wn−1 is

C(i)rwi 

ˆr =

i=1

n−1(cid:88)

P (wn = i|w1:n−1) =

 

(2)

(cid:80)V

exp(ˆrT ri + bi)
j=1 exp(ˆrT rj + bj)

where b ∈ RV is a bias vector. Learning can be done using backpropagation.

2

(a) NLM

(b) Multiplicative NLM

(c) Multiplicative NLM with lan-
guage switch

Figure 1: Three different formulations for predicting the next word in a neural language model. Left:
A standard neural language model (NLM). Middle: The context and attribute vectors interact via
a multiplicative interaction. Right: When words are unshared across attributes  a one-hot attribute
vector gates the factors-to-vocabulary matrix.

x ∈ RD  we can compute attribute-gated word representations as T x = (cid:80)D

2.2 A word embedding tensor
Traditionally  word representation matrices are represented as a matrix R ∈ RV ×K  such as in
the case of the log-bilinear model. Throughout this work  we instead represent words as a tensor
T ∈ RV ×K×D where D corresponds to the number of tensor slices. Given an attribute vector
i=1 xiT (i) i.e. word
representations with respect to x are computed as a linear combination of slices weighted by each
component xi of x.
It is often unnecessary to use a fully unfactored tensor. Following [15  16]  we re-represent T in
terms of three matrices Wf k ∈ RF×K  Wf d ∈ RF×D and Wf v ∈ RF×V   such that

(3)
where diag(·) denotes the matrix with its argument on the diagonal. These matrices are parametrized
by a pre-chosen number of factors F .

T x = (Wf v)(cid:62) · diag(Wf dx) · Wf k 

2.3 Multiplicative neural language models
We now show how to embed our word representation tensor T into the log-bilinear neural language
model. Let E = (Wf k)(cid:62)Wf v denote a ‘folded’ K × V matrix of word embeddings. Given the
context w1  . . .   wn−1  the predicted next word representation ˆr is given by

n−1(cid:88)

i=1

(4)
where E(:  wi) denotes the column of E for the word representation of wi and C(i)  i = 1  . . .   n−1
are K × K context matrices. Given a predicted next word representation ˆr  the factor outputs are

C(i)E(:  wi) 

ˆr =

f = (Wf kˆr) • (Wf dx) 

(5)
where • is a component-wise product. The conditional probability P (wn = i|w1:n−1  x) of wn
given w1  . . .   wn−1 and x can be written as
P (wn = i|w1:n−1  x) =

exp(cid:0)(Wf v(:  i))(cid:62)f + bi
(cid:1)
j=1 exp(cid:0)(Wf v(:  j))(cid:62)f + bj
(cid:80)V

(cid:1) .

Here  Wf v(:  i) denotes the column of Wf v corresponding to word i. In contrast to the log-bilinear
model  the matrix of word representations R from before is replaced with the factored tensor T   as
shown in Fig. 1.

2.4 Unshared vocabularies across attributes
Our formulation for T assumes that word representations are shared across all attributes. In some
cases  words may only be speciﬁc to certain attributes and not others. An example of this is cross-
lingual modelling  where it is necessary to have language speciﬁc vocabularies. As a running ex-
ample  consider the case where each attribute corresponds to a language representation vector. Let

3

Table 2: Samples generated from the model when conditioning on various attributes. For the last
example  we condition on the average of the two vectors (symbol <#> corresponds to a number).

Attribute

Bible

Caesar

1

2 (Bible +
Caesar)

Sample
<#> : <#> for thus i enquired unto thee   saying   the lord had not come unto
him . <#> : <#> when i see them shall see me greater am that under the name
of the king on israel .
to tell vs pindarus : shortly pray   now hence   a word . comes hither   and
let vs exclaim once by him fear till loved against caesar . till you are now which
have kept what proper deed there is an ant ? for caesar not wise cassi
let our spring tiger as with less ; for tucking great fellowes at ghosts of broth .
industrious time with golden glory employments . <#> : <#> but are far in men
soft from bones   assur too   set and blood of smelling   and there they cost  
i learned : love no guile his word downe the mystery of possession

x denote the attribute vector for language (cid:96) and x(cid:48) for language (cid:96)(cid:48) (e.g. English and French). We
can then compute language-speciﬁc word representations T (cid:96) by breaking up our decomposition into
language dependent and independent components (see Fig. 1c):

T (cid:96) = (Wf v

(cid:96) )(cid:62) · diag(Wf dx) · Wf k 

(6)
(cid:96) )(cid:62) is a V(cid:96) × F language speciﬁc matrix. The matrices Wf d and Wf k do not depend
where (Wf v
(cid:96) )(cid:62) is language speciﬁc. Moreover  since each lan-
on the language or the vocabulary  whereas (Wf v
guage may have a different sized vocabulary  we use V(cid:96) to denote the vocabulary size of language (cid:96).
Observe that this model has an interesting property in that it allows us to share statistical strength
across word representations of different languages. In particular  we show in our experiments how
we can improve cross-lingual classiﬁcation performance between English and German when a large
amount of parallel data exists between English and French and only a small amount of parallel data
exists between English and German.

2.5 Learning attribute representations
We now discuss how to learn representation vectors x. Recall that when training neural language
models  the word representations of w1  . . .   wn−1 are updated by backpropagating through the
word embedding matrix. We can think of this as being a linear layer  where the input to this layer
is a one-hot vector with the i-th position active for word wi. Then multiplying this vector by the
embedding matrix results in the word vector for wi. Thus the columns of the word representations
matrix consisting of words from w1  . . .   wn−1 will have non-zero gradients with respect to the loss.
This allows us to consistently modify the word representations throughout training.
We construct attribute representations in a similar way. Suppose that L is an attribute lookup table 
where x = f (L(:  x)) and f is an optional non-linearity. We often use a rectiﬁer non-linearity in
order to keep x sparse and positive  which we found made training much more stable. Initially  the
entries of L are generated randomly. During training  we treat L in the same way as the word em-
bedding matrix. This way of learning language representations allows us to measure how ‘similar’
attributes are as opposed to using a one-hot encoding of attributes for which no such similarity could
be computed.
In some cases  attributes that are available during training may not also be available at test time.
An example of this is when attributes are used as sentence indicators for learning representations
of sentences. To accommodate for this  we use an inference step similar to that proposed by [6].
That is  at test time all the network parameters are ﬁxed and stochastic gradient descent is used for
inferring the representation of an unseen attribute vector.

3 Experiments
In this section we describe our experimental evaluation and results. Throughout this section we refer
to our model as Attribute Tensor Decomposition (ATD). All models are trained using stochastic gra-
dient descent with an exponential learning rate decay and linear (per epoch) increase in momentum.
We ﬁrst demonstrate initial qualitative results to get a sense of the tasks our model can perform. For
these  we use the small project Gutenberg corpus which consists of 18 books  some of which have
the same author. We ﬁrst trained a multiplicative neural language model with a context size of 5 

4

Table 3: A modiﬁed version of the game Mad Libs. Given an initialization  the model is to generate
the next 5 words according to the part-of-speech sequence (note that these are not hard constraints).

[DT  NN  IN  DT  JJ]
the meaning of life is...

the cure of the bad
the truth of the good
a penny for the fourth

the globe of those modern

all man upon the same

to keep sold most wishes

to make manned most magniﬁcent

[TO  VB  VBD  JJS  NNS]

my greatest accomplishment is...

to keep wounded best nations
to be allowed best arguments
to be mentioned most people

[PRP  NN  ’ ’   JJ  NN]

i could not live without...

his regard   willing tenderness

her french   serious friend

her father   good voice
her heart   likely beauty
her sister   such character

Table 4: Classiﬁcation accuracies on various tasks. Left: Sentiment classiﬁcation on the tree-
bank dataset. Competing methods include the Neural Bag of words (NBoW) [5]  Recursive Net-
work (RNN) [17]  Matrix-Vector Recursive Network (MV-RNN) [18]  Recursive Tensor Network
(RTNN) [3]  Dynamic Convolutional Network (DCNN) [5] and Paragraph Vector (PV) [6]. Right:
Cross-lingual classiﬁcation on RCV2. Methods include statistical machine translation (SMT)  I-
Matrix [19]  Bag-of-words autoencoders (BAE-*) [20] and BiCVM  BiCVM+ [21]. The use of ‘+’
on cross-lingual tasks indicate the use of a third language (French) for learning embeddings.

Fine-grained Positive / Negative

Method
SMT

I-Matrix
BAE-cr
BAE-tree
BiCVM
BiCVM+
BAE-corr

ATD
ATD+

EN → DE DE → EN
67.4%
68.1%
71.1%
77.6%
63.6%
78.2%
68.2%
80.2%
83.7%
71.4%
76.9%
86.2%
91.8%
72.8%
71.8%
80.8%
83.4%
72.9%

Method
SVM
BiNB
NBoW
RNN

MVRNN
RTNN
DCNN

PV
ATD

40.7%
41.9%
42.4%
43.2%
44.4%
45.7%
48.5%
48.7%
45.9%

79.4%
83.1%
80.5%
82.4%
82.9%
85.4%
86.8%
87.8%
83.3%

where each attribute is represented as a book. This results in 18 learned attribute vectors  one for
each book. After training  we can condition on a book vector and generate samples from the model.
Table 2 illustrates some the generated samples. Our model learns to capture the ‘style’ associated
with different books. Furthermore  by conditioning on the average of book representations  the
model can generate reasonable samples that represent a hybrid of both attributes  even though such
attribute combinations were not observed during training.
Next  we computed POS sequences from sentences that occur in the training corpus. We trained
a multiplicative neural language model with a context size of 5 to predict the next word from its
context  given knowledge of the POS tag for the next word. That is  we model P (wn = i|w1:n−1  x)
where x denotes the POS tag for word wn. After training  we gave the model an initial input and
a POS sequence and proceeded to generate samples. Table 3 shows some results for this task.
Interestingly  the model can generate rather funny and poetic completions to the initial context.

3.1 Sentiment classiﬁcation
Our ﬁrst quantitative experiments are performed on the sentiment treebank of [3]. A common chal-
lenge for sentiment classiﬁcation tasks is that the global sentiment of a sentence need not correspond
to local sentiments exhibited in sub-phrases of the sentence. To address this issue  [3] collected an-
notations from the movie reviews corpus of [22] of all subphrases extracted from a sentence parser.
By incorporating local sentiment into their recursive architectures  [3] was able to obtain signiﬁcant
performance gains with recursive networks over bag of words baselines.
We follow the same experimental procedure proposed by [3] for which evaluation is reported on
two tasks: ﬁne-grained classiﬁcation of categories {very negative  negative  neutral  positive  very
positive } and binary classiﬁcation {positive  negative }. We extracted all subphrases of sentences
that occur in the training set and used these to train a multiplicative neural language model. Here 
each attribute is represented as a sentence vector  as in [6]. In order to compute subphrases for
unseen sentences  we apply an inference procedure similar to [6]  where the weights of the network
are frozen and gradient descent is used to infer representations for each unseen vector. We trained a
logistic regression classiﬁer using all training subphrases in the training set. At test time  we infer a
representation for a new sentence which is used for making a review prediction. We used a context

5

size of 8  100 dimensional word vectors initialized from [2] and 100 dimensional sentence vectors
initialized by averaging vectors of words from the corresponding sentence.
Table 4  left panel  illustrates our results on this task in comparison to all other proposed approaches.
Our results are on par with the highest performing recursive network on the ﬁne-grained task and
outperforms all bag-of-words baselines and recursive networks with the exception of the RTNN on
the binary task. Our method is outperformed by the two recently proposed approaches of [5] (a
convolutional network trained on sentences) and Paragraph Vector [6].

3.2 Cross-lingual document classiﬁcation
We follow the experimental procedure of [19]  for which several existing baselines are available to
compare our results. The experiment proceeds as follows. We ﬁrst use the Europarl corpus [23] for
inducing word representations across languages. Let S be a sentence with words w in language (cid:96)
and let x be the corresponding language vector. Let

v(cid:96)(S) =

T (cid:96)(:  w) =

(Wf v

(cid:96) (:  w))(cid:62) · diag(Wf dx) · Wf k

(7)

(cid:88)

w∈S

(cid:88)

w∈S

denote the sentence representation of S  deﬁned as the sum of language conditioned word represen-
tations for each w ∈ S. Equivalently we deﬁne a sentence representation for the translation S(cid:48) of S
denoted as v(cid:96)(cid:48)(S(cid:48)). We then optimize the following ranking objective:

0  α +(cid:13)(cid:13)v(cid:96)(S) − v(cid:96)(cid:48)(S(cid:48))(cid:13)(cid:13)2

2 −(cid:13)(cid:13)v(cid:96)(S) − v(cid:96)(cid:48)(Ck)(cid:13)(cid:13)2

2

(cid:27)

+ λ(cid:13)(cid:13)θ(cid:13)(cid:13)2

2

(cid:26)

max

(cid:88)

(cid:88)

S

k

minimize

θ

subject to the constraints that each sentence vector has unit norm. Each Ck is a constrastive (non-
translation) sentence of S and θ denotes all model parameters. This type of cross-language ranking
loss was ﬁrst used by [21] but without the norm constraint which we found signiﬁcantly improved
the stability of training. The Europarl corpus contains roughly 2 million parallel sentence pairs
between English and German as well as English and French  for which we induce 40 dimensional
word representations. Evaluation is then performed on English and German sections of the Reuters
RCV1/RCV2 corpora. Note that these documents are not parallel. The Reuters dataset contains
multiple labels for each document. Following [19]  we only consider documents which have been
assigned to one of the top 4 categories in the label hierarchy. These are CCAT (Corporate/Industrial) 
ECAT (Economics)  GCAT (Government/Social) and MCAT (Markets). There are a total of 34 000
English documents and 42 753 German documents with vocabulary sizes of 43614 English words
and 50 110 German words. We consider both training on English and evaluating on German and
vice versa. To represent a document  we sum over the word representations of words in that doc-
ument followed by a unit-ball projection. Following [19] we use an averaged perceptron classiﬁer.
Classiﬁcation accuracy is then evaluated on a held-out test set in the other language. We used a
monolingual validation set for tuning the margin α  which was set to α = 1. Five contrastive terms
were used per example which were randomly assigned per epoch.
Table 4  right panel  shows our results compared to all proposed methods thus far. We are com-
petitive with the current state-of-the-art approaches  being outperformed only by BiCVM+ [21] and
BAE-corr [20] on EN → DE. The BAE-corr method combines both a reconstruction term and a
correlation regularizer to match sentences  while our method does not consider reconstruction. We
also performed experimentation on a low resource task  where we assume the same conditions as
above with the exception that we only use 10 000 parallel sentence pairs between English and Ger-
man while still incorporating all English and French parallel sentences. For this task  we compare
against a separation baseline  which is the same as our model but with no parameter sharing across
languages (and thus resembles [21]). Here we achieve 74.7% and 69.7% accuracies (EN→DE and
DE→EN) while the separation baseline obtains 63.8% and 67.1%. This indicates that parame-
ter sharing across languages can be useful when only a small amount of parallel data is available.
Figure 2 further shows t-SNE embeddings of English-German word pairs.1
Another interesting consideration is whether or not the learned language vectors can capture any
interesting properties of various languages. To look into this  we trained a multiplicative neural
language model simultaneously on 5 languages: English  French  German  Czech and Slovak. To
our knowledge  this is the most languages word representations have been jointly learned on. We

1We note that Germany and Deutschland are nearest neighbours in the original space.

6

(a) Months

(b) Countries

Figure 2: t-SNE embeddings of English-German word pairs learned from Europarl.

(a) Correlation matrix

(b) Effect of conditional embeddings(c) Effect of inferring attribute vec-

tors

Figure 3: Results on the Blog classiﬁcation corpus. For the middle and right plots  each pair of same
coloured bars corresponds to the non-inclusion or inclusion of inferred attribute vectors  respectively.

computed a correlation matrix from the language vectors  illustrated in Fig. 3a. Interestingly  we
observe high correlation between Czech and Slovak representations  indicating that the model may
have learned some notion of lexical similarity. That being said  additional experimentation for future
work is necessary to better understand the similarities exhibited through language vectors.

3.3 Blog authorship attribution
For our ﬁnal task  we use the Blog corpus of [24] which contains 681 288 blog posts from 19 320
authors. For our experiments  we break the corpus into two separate datasets: one containing the
1000 most proliﬁc authors (most blog posts) and the other containing all the rest. Each author comes
with an attribute tag corresponding to a tuple (age  gender  industry) indicating the age range of the
author (10s  20s or 30s)  whether the author is male or female  and what industry the author works
in. Note that industry does not necessary correspond to the topic of blog posts. We use the dataset
of non-proliﬁc authors to train a multiplicative language model conditioned on an attribute tuple
of which there are 234 unique tuples in total. We used 100 dimensional word vectors initialized
from [2]  100 dimensional attribute vectors with random initialization and a context size of 5. A
1000-way classiﬁcation task is then performed on the proliﬁc author subset and evaluation is done
using 10-fold cross-validation. Our initial experimentation with baselines found that tf-idf performs
well on this dataset (45.9% accuracy). Thus  we consider how much we can improve on the tf-idf
baseline by augmenting word and attribute features.
For the ﬁrst experiment  we determine the effect conditional word embeddings have on classiﬁcation
performance  assuming attributes are available at test time. For this  we compute two embedding
matrices from a trained ATD model  one without and with attribute knowledge:

unconditioned ATD :
conditioned ATD :

(8)
(9)
We represent a blog post as the sum of word vectors projected to unit norm and augment these with
tf-idf features. As an additional baseline we include a log-bilinear language model [14]. 2 Figure
3b illustrates the results from which we observe that conditioned word embeddings are signiﬁcantly
more discriminative over word embeddings computed without knowledge of attribute vectors.

(Wf v)(cid:62)Wf k
(Wf v)(cid:62) · diag(Wf dx) · Wf k.

2The log-bilinear model has no concept of attributes.

7

5102550100382#Documents(thousands)0123456ImprovementoverinitialmodelunconditionedATDLBLconditionedATD5102550100382#Documents(thousands)−0.2−0.10.00.10.20.3InferredattributesdifferenceunconditionedATDTable 5: Results from a conditional word similarity task using Blog attributes and language vectors.
German
januar

Common Unique to A Unique to B

English
january

Query A B

therapy

school

French
janvier
decembre

juin

marche
marches
interne
guerre

terrorisme
mondaile

dit
disait
declare
deux

june

october
market
markets
internal

war

weapons
global
said
stated
told
two

two-thirds

both

deuxieme
seconde

f/10/student
m/20/tech
journal

f/10/student
m/30/adv.

create
f/30/arts

f/30/internet

joy

m/30/religion
m/20/science

cool

m/10/student
f/10/student

work
church
college
diary
blog

webpage

build
develop
maintain
happiness
sadness

pain
nice
funny

awesome

choir
prom
skool
project
book

yearbook
provide
acquire
generate
rapture

god

heartbreak
beautiful
amazing

neat

tech
job
zine
app

referral
compile
follow
analyse
delight
comfort

soul
sexy
hott
lame

dezember

juni
markt

binnenmarktes

marktes
krieg
globale
krieges
sagte
gesagt
sagten
zwei
beiden
zweier

For the second experiment  we determine the effect of inferring attribute vectors at test time if they
are not assumed to be available. To do this  we train a logistic regression classiﬁer within each fold
for predicting attributes. We compute an inferred vector by averaging each of the attribute vectors
weighted by the log-probabilities of the classiﬁer. In Fig. 3c we plot the difference in performance
when an inferred vector is augmented vs. when it is not. These results show consistent  albeit small
improvement gains when attribute vectors are inferred at test time.
To get a better sense of the attribute features learned from the model  the supplementary material
contains a t-SNE embedding of the learned attribute vectors. Interestingly  the model learns features
which largely isolate the vectors of all teenage bloggers independent of gender and topic.

3.4 Conditional word similarity
One of the key properties of our tensor formulation is the notion of conditional word similarity 
namely how neighbours of word representations change depending on the attributes that are condi-
tioned on. In order to explore the effects of this  we performed two qualitative comparisons: one
using blog attribute vectors and the other with language vectors. These results are illustrated in
Table 5. For the ﬁrst comparison on the left  we chose two attributes from the blog corpus and a
query word. We identify each of these attribute pairs as A and B. Next  we computed a ranked list of
the nearest neighbours (by cosine similarity) of words conditioned on each attribute and identiﬁed
the top 15 words in each. Out of these 15 words  we display the top 3 words which are common
to both ranked lists  as well as 3 words that are unique to a speciﬁc attribute. Our results illustrate
that the model can capture distinctive notions of word similarities depending on which attributes
are being conditioned. On the right of Table 5  we chose a query word in English (italicized) and
computed the nearest neighbours when conditioned on each language vector. This results in neigh-
bours that are either direct translations of the query word or words that are semantically similar. The
supplementary material includes additional examples with nearest neighbours of collocations.

4 Conclusion
There are several future directions from which this work can be extended. One application area
of interest is in learning representations of authors from papers they choose to review as a way of
improving automating reviewer-paper matching [25]. Since authors contribute to different research
topics  it might be more useful to instead consider a mixture of attribute vectors that can allow for
distinctive representations of the same author across research areas. Another interesting application
is learning representations of graphs. Recently  [26] proposed an approach for learning embeddings
of nodes in social networks. Introducing network indicator vectors could allow us to potentially
learn representations of full graphs. Finally  it would be interesting to train a multiplicative neural
language model simultaneously across dozens of languages.

Acknowledgments

We would also like to thank the anonymous reviewers for their valuable comments and suggestions.
This work was supported by NSERC  Google  Samsung  and ONR Grant N00014-14-1-0232.

8

References
[1] Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural

networks with multitask learning. In ICML  pages 160–167  2008.

[2] Joseph Turian  Lev Ratinov  and Yoshua Bengio. Word representations: a simple and general method for

semi-supervised learning. In ACL  pages 384–394  2010.

[3] Richard Socher  Alex Perelygin  Jean Y Wu  Jason Chuang  Christopher D Manning  Andrew Y Ng  and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In
EMNLP  pages 1631–1642  2013.

[4] Tomas Mikolov  Ilya Sutskever  Kai Chen  Greg S Corrado  and Jeff Dean. Distributed representations of

words and phrases and their compositionality. In NIPS  pages 3111–3119  2013.

[5] Phil Blunsom  Edward Grefenstette  Nal Kalchbrenner  et al. A convolutional neural network for mod-

elling sentences. In ACL  2014.

[6] Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. ICML  2014.
[7] Antoine Bordes  Nicolas Usunier  Alberto Garcia-Duran  Jason Weston  and Oksana Yakhnenko. Trans-

lating embeddings for modeling multi-relational data. In NIPS  pages 2787–2795  2013.

[8] Richard Socher  Danqi Chen  Christopher D Manning  and Andrew Ng. Reasoning with neural tensor

networks for knowledge base completion. In NIPS  pages 926–934  2013.

[9] Yann N Dauphin  Gokhan Tur  Dilek Hakkani-Tur  and Larry Heck. Zero-shot learning for semantic

utterance classiﬁcation. ICLR  2014.

[10] Andrea Frome  Greg S Corrado  Jon Shlens  Samy Bengio  Jeffrey Dean  and Tomas Mikolov MarcAure-

lio Ranzato. Devise: A deep visual-semantic embedding model. NIPS  2013.

[11] Graham W Taylor and Geoffrey E Hinton. Factored conditional restricted boltzmann machines for mod-

eling motion style. In ICML  pages 1025–1032  2009.

[12] Ryan Kiros  Richard S Zemel  and Ruslan Salakhutdinov. Multimodal neural language models. ICML 

2014.

[13] Ilya Sutskever  James Martens  and Geoffrey E Hinton. Generating text with recurrent neural networks.

In ICML  pages 1017–1024  2011.

[14] Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language modelling. In

ICML  pages 641–648  2007.

[15] Roland Memisevic and Geoffrey Hinton. Unsupervised learning of image transformations. In CVPR 

pages 1–8  2007.

[16] Alex Krizhevsky  Geoffrey E Hinton  et al. Factored 3-way restricted boltzmann machines for modeling

natural images. In AISTATS  pages 621–628  2010.

[17] Richard Socher  Jeffrey Pennington  Eric H Huang  Andrew Y Ng  and Christopher D Manning. Semi-
In EMNLP  pages 151–161 

supervised recursive autoencoders for predicting sentiment distributions.
2011.

[18] Richard Socher  Brody Huval  Christopher D Manning  and Andrew Y Ng. Semantic compositionality

through recursive matrix-vector spaces. In EMNLP  pages 1201–1211  2012.

[19] Alexandre Klementiev  Ivan Titov  and Binod Bhattarai. Inducing crosslingual distributed representations

of words. In COLING  pages 1459–1474  2012.

[20] Sarath Chandar A P  Stanislas Lauly  Hugo Larochelle  Mitesh M Khapra  Balaraman Ravindran  Vikas
Raykar  and Amrita Saha. An autoencoder approach to learning bilingual word representations. NIPS 
2014.

[21] Karl Moritz Hermann and Phil Blunsom. Multilingual distributed representations without word alignment.

ICLR  2014.

[22] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with

respect to rating scales. In ACL  pages 115–124  2005.

[23] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In MT summit  volume 5 

pages 79–86  2005.

[24] Jonathan Schler  Moshe Koppel  Shlomo Argamon  and James W Pennebaker. Effects of age and gender
on blogging. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs  volume 6 
pages 199–205  2006.

[25] Laurent Charlin  Richard S Zemel  and Craig Boutilier. A framework for optimizing paper matching.

UAI  2011.

[26] Bryan Perozzi  Rami Al-Rfou  and Steven Skiena. Deepwalk: Online learning of social representations.

KDD  2014.

9

,Shane Griffith
Kaushik Subramanian
Jonathan Scholz
Charles Isbell
Andrea Thomaz
Ryan Kiros
Richard Zemel
Russ Salakhutdinov
Mahdi Soltanolkotabi
Piotr Mirowski
Matt Grimes
Mateusz Malinowski
Karl Moritz Hermann
Keith Anderson
Denis Teplyashin
Karen Simonyan
koray kavukcuoglu
Andrew Zisserman
Raia Hadsell