2014,Clustering from Labels and Time-Varying Graphs,We present a general framework for graph clustering where a label is observed to each pair of nodes. This allows a very rich encoding of various types of pairwise interactions between nodes. We propose a new tractable approach to this problem based on maximum likelihood estimator and convex optimization. We analyze our algorithm under a general generative model  and provide both necessary and sufficient conditions for successful recovery of the underlying clusters. Our theoretical results cover and subsume a wide range of existing graph clustering results including planted partition  weighted clustering and partially observed graphs. Furthermore  the result is applicable to novel settings including time-varying graphs such that new insights can be gained on solving these problems. Our theoretical findings are further supported by empirical results on both synthetic and real data.,Clustering from Labels and Time-Varying Graphs

Shiau Hong Lim

National University of Singapore

mpelsh@nus.edu.sg

Yudong Chen

EECS  University of California  Berkeley
yudong.chen@eecs.berkeley.edu

Huan Xu

National University of Singapore

mpexuh@nus.edu.sg

Abstract

We present a general framework for graph clustering where a label is observed to
each pair of nodes. This allows a very rich encoding of various types of pairwise
interactions between nodes. We propose a new tractable approach to this problem
based on maximum likelihood estimator and convex optimization. We analyze our
algorithm under a general generative model  and provide both necessary and sufﬁ-
cient conditions for successful recovery of the underlying clusters. Our theoretical
results cover and subsume a wide range of existing graph clustering results includ-
ing planted partition  weighted clustering and partially observed graphs. Further-
more  the result is applicable to novel settings including time-varying graphs such
that new insights can be gained on solving these problems. Our theoretical ﬁnd-
ings are further supported by empirical results on both synthetic and real data.

1

Introduction

In the standard formulation of graph clustering  we are given an unweighted graph and seek a par-
titioning of the nodes into disjoint groups such that members of the same group are more densely
connected than those in different groups. Here  the presence of an edge represents some sort of
afﬁnity or similarity between the nodes  and the absence of an edge represents the lack thereof.
In many applications  from chemical interactions to social networks  the interactions between nodes
are much richer than a simple “edge” or “non-edge”. Such extra information may be used to improve
the clustering quality. We may represent each type of interaction by a label. One simple setting of
this type is weighted graphs  where instead of a 0-1 graph  we have edge weights representing
the strength of the pairwise interaction.
In this case the observed label between each pair is a
real number. In a more general setting  the label need not be a number. For example  on social
networks like Facebook  the label between two persons may be “they are friends”  “they went to
different schools”  “they liked 21 common pages”  or the concatenation of these. In such cases
different labels carry different information about the underlying community structure. Standard
approaches convert these pairwise interactions into a simple edge/non-edge  and then apply standard
clustering algorithms  which might lose much of the information. Even in the case of a standard
weighted/unweighted graph  it is not immediately clear how the graph should be used. For example 
should the absence of an edge be interpreted as a neutral observation carrying no information  or as
a negative observation which indicates dissimilarity between the two nodes?
We emphasize that the forms of labels can be very general. In particular  a label can take the form
of a time series  i.e.  the record of time varying interaction such as “A and B messaged each other
on June 1st  4th  15th and 21st”  or “they used to be friends  but they stop seeing each other since
2012”. Thus  the labeled graph model is an immediate tool for analyzing time-varying graphs.

1

In this paper  we present a new and principled approach for graph clustering that is directly based on
pairwise labels. We assume that between each pair of nodes i and j  a label Lij is observed which
is an element of a label set L. The set L may be discrete or continuous  and need not have any
structure. The standard graph model corresponds to a binary label set L = {edge  non-edge}  and
a weighted graph corresponds to L = R. Given the observed labels L = (Lij) ∈ Ln×n  the goal
is to partition the n nodes into disjoint clusters. Our approach is based on ﬁnding a partition that
optimizes a weighted objective appropriately constructed from the observed labels. This leads to a
combinatorial optimization problem  and our algorithm uses its convex relaxation.
To systematically evaluate clustering performance  we consider a generalization of the stochastic
block model [1] and the planted partition model [2]. Our model assumes that the observed labels
are generated based on an underlying set of ground truth clusters  where pairs from the same clus-
ter generate labels using a distribution µ over L and pairs from different clusters use a different
distribution ν. The standard stochastic block model corresponds to the case where µ and ν are two-
point distributions with µ(edge) = p and ν(edge) = q. We provide theoretical guarantees for our
algorithm under this generalized model.
Our results cover a wide range of existing clustering settings—with equal or stronger theoretical
guarantees—including the standard stochastic block model  partially observed graphs and weighted
graphs. Perhaps surprisingly  our framework allows us to handle new classes of problems that are not
a priori obvious to be a special case of our model  including the clustering of time-varying graphs.

1.1 Related work

The planted partition model/stochastic block model [1  2] are standard models for studying graph
clustering. Variants of the models cover partially observed graphs [3  4] and weighted graphs [5  6].
All these models are special cases of ours. Various algorithms have been proposed and analyzed
under these models  such as spectral clustering [7  8  1]  convex optimization approaches [9  10  11]
and tensor decomposition methods [12]. Ours is based on convex optimization; we build upon and
extend the approach in [13]  which is designed for clustering unweighted graphs whose edges have
different levels of uncertainty  a special case of our problem (cf. Section 4.2 for details).
Most related to our setting is the labelled stochastic block model proposed in [14] and [15]. A
main difference in their model is that they assume each observation is a two-step process: ﬁrst
an edge/non-edge is observed; if it is an edge then a label is associated with it.
In our model
all observations are in the form of labels; in particular  an edge or no-edge is also a label. This
covers their setting as a special case. Our model is therefore more general and natural—as a result
our theory covers a broad class of subproblems including time-varying graphs. Moreover  their
analysis is mainly restricted to the two-cluster setting with edge probabilities on the order of Θ(1/n) 
while we allow for an arbitrary number of clusters and a wide range of edge/label distributions.
In addition  we consider the setting where the distributions of the labels are not precisely known.
Algorithmically  they use belief propagation [14] and spectral methods [15].
Clustering time-varying graphs has been studied in various context; see [16  17  18  19  20] and
the references therein. Most existing algorithms use heuristics and lack theoretical analysis. Our
approach is based on a generative model and has provable performance guarantees.

2 Problem setup and algorithms

We assume n nodes are partitioned into r disjoint clusters of size at least K  which are unknown and
considered as the ground truth. For each pair (i  j) of nodes  a label Lij ∈ L is observed  where L is
the set of all possible labels.1 These labels are generated independently across pairs according to the
distributions µ and ν. In particular  the probability of observing the label Lij is µ(Lij) if i and j are
in the same cluster  and ν(Lij) otherwise. The goal is to recover the ground truth clusters given the
labels. Let L = (Lij) ∈ Ln×n be the matrix of observed labels. We represent the true clusters by
an n × n cluster matrix Y ∗  where Y ∗
ij = 0
otherwise (we use the convention Y ∗

ij = 1 if nodes i and j belong to the same cluster and Y ∗
ii = 1 for all i). The problem is therefore to ﬁnd Y ∗ given L.

1Note that L does not have to be ﬁnite. Although some of the results are presented for ﬁnite L  they can be

easily adapted to the other cases  for instance  by replacing summation with integration.

2

We take an optimization approach to this problem. To motivate our algorithm  ﬁrst consider the
case of clustering a weighted graph  where all labels are real numbers. Positive weights indicate
in-cluster interaction while negative weights indicate cross-cluster interaction. A natural approach
is to cluster the nodes in a way that maximizes the total weight inside the clusters (this is equivalent
to correlation clustering [21]). Mathematically  this is to ﬁnd a clustering  represented by a cluster
i j LijYij is maximized. For the case of general labels  we pick a weight
function w : L (cid:55)→ R  which assigns a number Wij = w(Lij) to each label  and then solve the
following max-weight problem:

(cid:104)W  Y (cid:105)

max

Y

s.t. Y is a cluster matrix;

(1)

matrix Y   such that(cid:80)
here (cid:104)W  Y (cid:105) :=(cid:80)

ij WijYij is the standard trace inner product. Note that this effectively converts

the problem of clustering from labels into a weighted clustering problem.
The program (1) is non-convex due to the constraint. Our algorithm is based on a convex relaxation
of (1)  using the now well-known fact that a cluster matrix is a block-diagonal 0-1 matrix and thus
has nuclear norm2 equal to n [22  3  23]. This leads to the following convex optimization problem:
(2)

max

(cid:104)W  Y (cid:105)
(cid:107)Y (cid:107)∗ ≤ n; 0 ≤ Yij ≤ 1 ∀(i  j).

Y
s.t.

We say that this program succeeds if it has a unique optimal solution equal to the true cluster matrix
Y ∗. We note that a related approach is considered in [13]  which is discussed in section 4.
One has the freedom of choosing the weight function w. Intuitively  w should assign w(Lij) > 0
to a label Lij with µ(Lij) > ν(Lij)  so the program (2) is encouraged to place i and j in the same
cluster  the more likely possibility; similarly we should have w(Lij) < 0 if µ(Lij) < ν(Lij). A
good weight function should further reﬂect the information in µ and ν. Our theoretical results in
section 3 characterize the performance of the program (2) for any given weight function; building
on this  we further derive the optimal choice for the weight function.

3 Theoretical results

of the true clusters K  and any given weight function w. Deﬁne Eµw := (cid:80)
Varµw :=(cid:80)

In this section  we provide theoretical analysis for the performance of the convex program (2) under
the probabilistic model described in section 2. The proofs are given in the supplementary materials.
Our main result is a general theorem that gives sufﬁcient conditions for (2) to recover the true cluster
matrix Y ∗. The conditions are stated in terms of the label distribution µ and ν  the minimum size
l∈L w(l)µ(l) and
Theorem 1 (Main). Suppose b is any number that satisﬁes |w(l)| ≤ b ∀l ∈ L almost surely. There
exists a universal constant c > 0 such that if
√

l∈L[w(l) − Eµw]2µ(l); Eνw and Varνw are deﬁned similarly.

√

−Eνw ≥ c
Eµw ≥ c

b log n +

√

b log n +

Varνw

K log n

n log n(cid:112)max(Varµw  Varνw)

K

 

(3)

(4)

 

then Y ∗ is the unique solution to (2) with probability at least 1 − n−10. 3
The theorem holds for any given weight function w. In the next two subsections  we show how to
choose w optimally  and then address the case where w deviates from the optimal choice.

K

3.1 Optimal weights

A good candidate for the weight function w can be derived from the maximum likelihood estima-
tor (MLE) of Y ∗. Given the observed labels L  the log-likelihood of the true cluster matrix taking
2The nuclear norm of a matrix is deﬁned as the sum of its singular values. A cluster matrix is positive
3In all our results  the choice n−10 is arbitrary. In particular  the constant c scales linearly with the exponent.

semideﬁnite so its nuclear norm is equal to its trace.

3

the value Y is

log Pr(L|Y ∗ = Y ) =

log(cid:2)µ(Lij)Yij ν(Lij)1−Yij(cid:3) = (cid:104)W  Y (cid:105) + c

(cid:88)

i j

where c is independent of Y and W is given by the weight function w(l) = wMLE(l) := log µ(l)
ν(l) .
The MLE thus corresponds to using the log-likelihood ratio wMLE(·) as the weight function. The
following theorem is a consequence of Theorem 1 and characterizes the performance of using the
MLE weights. In the sequel  we use D(·(cid:107)·) to denote the KL divergence between two distributions.
Theorem 2 (MLE). Suppose wMLE is used  and b and ζ are any numbers which satisfy with
D(ν||µ) ≤ ζD(µ||ν) and
that Y ∗ is the unique solution to (2) with probability at least 1 − n−10 if

(cid:12)(cid:12)(cid:12) ≤ b ∀l ∈ L. There exists a universal constant c > 0 such

(cid:12)(cid:12)(cid:12)log µ(l)

ν(l)

log n

D(ν||µ) ≥ c(b + 2)
 
D(µ||ν) ≥ c(ζ + 1)(b + 2)

K

(cid:18) n log n

(cid:19)

K 2

.

(5)

(6)

Moreover  we always have D(ν||µ) ≤ (2b + 3)D(µ||ν)  so we can take ζ = 2b + 3.
Note that the theorem has the intuitive interpretation that the in/cross-cluster label distributions µ and
ν should be sufﬁciently different  measured by their KL divergence. Using a classical result in infor-
mation theory [24]  we may replace the KL divergences with a quantity that is often easier to work
with  as summarized below. The LHS of (7) is sometimes called the triangle discrimination [24].
Corollary 1 (MLE 2). Suppose wMLE is used  and b  ζ are deﬁned as in Theorem 2. There exists a
universal constant c such that Y ∗ is the unique solution to (2) with probability at least 1 − n−10 if

(cid:88)

(µ(l) − ν(l))2
µ(l) + ν(l)

≥ c(ζ + 1)(b + 2)

(cid:18) n log n

(cid:19)

K 2

.

(7)

We may take ζ = 2b + 3.

l∈L

The MLE weight wMLE turns out to be near-optimal  at least in the two-cluster case  in the sense that
no other weight function (in fact  no other algorithm) has signiﬁcantly better performance. This is
shown by establishing a necessary condition for any algorithm to recover Y ∗. Here  an algorithm is
a measurable function ˆY that maps the data L to a clustering (represented by a cluster matrix).
Theorem 3 (Converse). The following holds for some universal constants c  c(cid:48) > 0. Suppose K =
2   and b deﬁned in Theorem 2 satisﬁes b ≤ c(cid:48). If

n

(µ(l) − ν(l))2
µ(l) + ν(l)

≤ c log n

n

 

(8)

(cid:88)

l∈L

then inf ˆY supY ∗ P( ˆY (cid:54)= Y ∗) ≥ 1
2   where the supremum is over all possible cluster matrices.
Under the assumption of Theorem 3  the conditions (7) and (8) match up to a constant factor.
Remark. The MLE weight |wMLE(l)| becomes large if µ(l) = o(ν(l)) or ν(l) = o(µ(l))  i.e.  when
the in-cluster probability is negligible compared to the cross-cluster one (or the other way around).
It can be shown that in this case the MLE weight is actually order-wise better than a bounded weight
function. We give this result in the supplementary material due to space constraints.

3.2 Monotonicity

We sometimes do not know the exact true distributions µ and ν to compute wMLE. Instead  we might
compute the weight using the log likelihood ratios of some “incorrect” distribution ¯µ and ¯ν. Our
algorithm has a nice monotonicity property: as long as the divergence of the true µ and ν is larger
than that of ¯µ and ¯ν (hence an “easier” problem)  then the problem should still have the same  if not
better probability of success  even though the wrong weights are used.
We say that (µ  ν) is more divergent then (¯µ  ¯ν) if  for each l ∈ L  we have that either

µ(l)
ν(l)

≥ µ(l)
¯ν(l)

≥ ¯µ(l)
¯ν(l)

≥ 1

or

4

ν(l)
µ(l)

≥ ν(l)
¯µ(l)

≥ ¯ν(l)
¯µ(l)

≥ 1.

¯ν(l)  ∀l  while the
Theorem 4 (Monotonicity). Suppose we use the weight function w(l) = log ¯µ(l)
actual label distributions are µ and ν. If the conditions in Theorem 2 or Corollary 1 hold with µ  ν
replaced by ¯µ  ¯ν  and (µ  ν) is more divergent than (¯µ  ¯ν)  then with probability at least 1 − n−10
Y ∗ is the unique solution to (2).
This result suggests that one way to choose the weight function is by using the log-likelihood ratio
based on a “conservative” estimate (i.e.  a less divergent one) of the true label distribution pair.

3.3 Using inaccurate weights

In the previous subsection we consider using a conservative log-likelihood ratio as the weight. We
now consider a more general weight function w which need not be conservative  but is only required
to be not too far from the true log-likelihood ratio wMLE. Let

be the error for each label l ∈ L. Accordingly  let ∆µ :=(cid:80)

ε(l) := w(l) − wMLE(l) = w(l) − log

µ(l)
ν(l)

l∈L µ(l)ε(l) and ∆ν :=(cid:80)

l∈L ν(l)ε(l)
be the average errors with respect to µ and ν. Note that ∆µ and ∆ν can be either positive or negative.
The following characterizes the performance of using such a w.
Theorem 5 (Inaccurate Weights). Let b and ζ be deﬁned as in Theorem 2. If the weight w satisﬁes

|w(l)| ≤ λ

|∆µ| ≤ γD(µ||ν) 

|∆ν| ≤ γD(ν||µ)

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:12)(cid:12)(cid:12)(cid:12)  ∀l ∈ L 

µ(l)
ν(l)

for some γ < 1 and λ > 0. Then Y ∗ is unique solution to (2) with probability at least 1 − n−10 if

D(ν||µ) ≥ c

λ2

(1 − γ)2 (b + 2)

log n

K

and D(µ||ν) ≥ c

λ2

(1 − γ)2 (ζ + 1)(b + 2)

(cid:18) n log n

(cid:19)

.

K 2

Therefore  as long as the errors ∆µ and ∆ν in w are not too large  the condition for recovery will be
order-wise similar to that in Theorem 2 for using the MLE weight. The numbers λ and γ measure
the amount of inaccuracy in w w.r.t. wMLE. The last two conditions in Theorem 5 thus quantify the
relation between the inaccuracy in w and the price we need to pay for using such a weight.

4 Consequences and applications

We apply the general results in the last section to different special cases. In sections 4.1 and 4.2  we
consider two simple settings and show that two immediate corollaries of our main theorems recover 
and in fact improve upon  existing results. In sections 4.3 and 4.4  we turn to the more complicated
setting of clustering time-varying graphs and derive several novel results.

4.1 Clustering a Gaussian matrix with partial observations

Analogous to the planted partition model for unweighted graphs  the bi-clustering [5] or submatrix-
localization [6  23] problem concerns with weighted graph whose adjacency matrix has Gaussian
entries. We consider a generalization of this problem where some of the entries are unobserved.
Speciﬁcally  we observe a matrix L ∈ (R ∪ {?})n×n  which has r submatrices of size K × K with
disjoint row and column support  such that Lij =? (meaning unobserved) with probability 1− s and
otherwise Lij ∼ N (uij  1). Here the means of the Gaussians satisfy: uij = ¯u if (i  j) is inside the
submatrices and uij = u if outside  where ¯u > u ≥ 0. Clustering is equivalent to locating these
submatrices with elevated mean  given the large Gaussian matrix L with partial observations.4
This is a special case of our labeled framework with L = R ∪ {?}. Computing the log-likelihood
ratios for two Gaussians  we obtain wMLE(Lij) = 0 if Lij =?  and wMLE(Lij) ∝ Lij − (¯u + u)/2
otherwise. This problem is interesting only when ¯u − u (cid:46) √
log n (otherwise simple element-wise
thresholding [5  6] ﬁnds the submatrices)  which we assume to hold. Clearly D (µ(cid:107)ν) = D (ν(cid:107)µ) =
4 s(¯u − u)2. The following can be proved using our main theorems (proof in the appendix).

1

4Here for simplicity we consider the clustering setting instead of bi-clustering. The latter setting corresponds

to rectangular L and submatrices. Extending our results to this setting is relatively straightforward.

5

Corollary 2 (Gaussian Graphs). Under the above setting  Y ∗ is the unique solution to (2) with
weights w = wMLE with probability at least 1 − 2n−10 provided

s (¯u − u)2 ≥ c

n log3 n

K 2

.

In the fully observed case  this recovers the results in [23  5  6] up to log factors. Our results are
more general as we allow for partial observations  which is not considered in previous work.

4.2 Planted Partition with non-uniform edge densities

The work in [13] considers a variant of the planted partition model with non-uniform edge densities 
where each pair (i  j) has an edge with probability 1− uij > 1/2 if they are in the same cluster  and
with probability uij < 1/2 otherwise. The number uij can be considered as a measure of the level of
uncertainty in the observation between i and j  and is known or can be estimated in applications like
cloud-clustering. They show that using the knowledge of {uij} improves clustering performance 
and such a setting covers clustering of partially observed graphs that is considered in [11  3  4].
Here we consider a more general setting that does not require the in/cross-cluster edge density to be
symmetric around 1
2. Suppose each pair (i  j) is associated with two numbers pij and qij  such that
if i and j are in the same cluster (different clusters  resp.)  then there is an edge with probability pij
(qij  resp.); we know pij and qij but not which of them is the probability that generates the edge.
The values of pij and qij are generated i.i.d. randomly as (pij  qij) ∼ D by some distribution D on
[0  1] × [0  1]. The goal is to ﬁnd the clusters given the graph adjacency matrix A  (pij) and (qij).
This model is a special case of our labeled framework. The labels have the form Lij =
(Aij  pij  qij) ∈ L = {0  1} × [0  1] × [0  1]  generated by the distributions

(cid:26)pD(p  q) 

µ(l) =

(1 − p)D(p  q) 

l = (1  p  q)
l = (0  p  q)

(cid:26)qD(p  q) 

ν(l) =

(1 − q)D(p  q) 
+(1−Aij) log 1−pij
1−qij

l = (1  p  q)
l = (0  p  q).

The MLE weight has the form wMLE(Lij) = Aij log pij
qij
convenient to use a conservative weight in which we replace pij and qij with ¯pij = 3
¯qij = 1
Corollary 3 (Non-uniform Density). Program (2) recovers Y ∗ with probability at least 1 − n−10 if

4 qij. Applying Theorem 4 and Corollary 1  we immediately obtain the following.

. It turns out it is more
4 qij and

4 pij + 3

4 pij + 1

(cid:20) (pij − qij)2

pij(1 − qij)

(cid:21)

ED

≥ c

n log n

K 2

 ∀(i.j).

Here ED is the expectation w.r.t. the distribution D  and LHS above is in fact independent of (i  j).

Corollary 3 improves upon existing results for several settings.
• Clustering partially observed graphs. Suppose D is such that pij = p and qij = q with proba-
bility s  and pij = qij otherwise  where p > q. This extends the standard planted partition model:
each pair is unobserved with probability 1 − s. For this setting we require

s(p − q)2
p(1 − q)

(cid:38) n log n

K 2

.

When s = 1. this matches the best existing bounds for standard planted partition [9  12] up to a
log factor. For the partial observation setting with s ≤ 1  the work in [4] gives a similar bound
under the additional assumption p > 0.5 > q  which is not required by our result. For general
p and q  the best existing bound is given in [3  9]  which replaces unobserved entries with 0 and
requires the condition s(p−q)2
p(1−sq)

with symmetric densities pij ≡ 1− qij  for which we recover their result ED(cid:2)(1−2qij)2(cid:3)(cid:38) nlogn

• Planted partition with non-uniformity. The model and algorithm in [13] is a special case of ours
K2 .

. Our result is tighter when p and q are close to 1.

(cid:38) n log n
K2

Corollary 3 is more general as it removes the symmetry assumption.

6

4.3 Clustering time-varying multiple-snapshot graphs

Standard graph clustering concerns with clustering on a single  static graph. We now consider a
setting where the graph can be time-varying. Speciﬁcally  we assume that for each time interval
t = 1  2  . . .   T   we observed a snapshot of the graph L(t) ∈ Ln×n. We assume each snapshot is
generated by the distributions µ and ν  independent of other snapshots.
We can map this problem into our original labeled framework  by considering the whole time se-
quence of ¯Lij := (L(1)
ij ) observed at the pair (i  j) as a single label. In this case the label
set is thus the set of all possible sequences  i.e.  ¯L = (L)T   and the label distributions are (with a
ij )  with ν(·) given similarly. The MLE weight
slight abuse of notation) µ( ¯Lij) = µ(L(1)
(normalized by T ) is thus the average log-likelihood ratio:

ij ) . . . µ(L(T )

ij   . . .   L(T )

wMLE( ¯Lij) =

1
T

log

µ(L(1)
ν(L(1)

ij ) . . . µ(L(T )
ij )
ij ) . . . ν(L(T )
ij )

=

1
T

log

µ(L(t)
ij )
ν(L(t)
ij )

.

T(cid:88)

t=1

Since wMLE( ¯Lij) is the average of T independent random variables  its variance scales with 1
T .
Applying Theorem 1  with almost identical proof as in Theorem 2 we obtain the following:
ν(l)| ≤ b ∀l ∈ L and D(ν||µ) ≤ ζD(µ||ν).
Corollary 4 (Independent Snapshots). Suppose | log µ(l)
The program (2) with MLE weights given recovers Y ∗ with probability at least 1 − n−10 provided
(cid:110) log n

D(ν||µ) ≥ c(b + 2)
K
D(µ||ν) ≥ c(b + 2) max

  (ζ + 1)

(cid:111)

log n

(9)

(10)

.

 

K

n log n
T K 2

Setting T = 1 recovers Theorem 2. When the second term in (10) dominates  the corollary says that
the problem becomes easier if we observe more snapshots  with the tradeoff quantiﬁed precisely.

4.4 Markov sequence of snapshots

We now consider the more general and useful setting where the snapshots form a Markov chain. For
simplicity we assume that the Markov chain is time-invariant and has a unique stationary distribution
which is also the initial distribution. Therefore  the observations L(t)
ij at each (i  j) are generated by
ﬁrst drawing a label from the stationary distribution ¯µ (or ¯ν) at t = 1  then applying a one-step
transition to obtain the label at each subsequent t. In particular  given the previously observed label
l  let the intra-cluster and inter-cluster conditional distributions be µ(·|l) and ν(·|l). We assume that
the Markov chains with respect to both µ and ν are geometrically ergodic such that for any τ ≥ 1 
and label-pair L(1)  L(τ +1) 

κ

and

| Prν(L(τ +1)|L(1)) − ¯ν(L(τ +1))| ≤ κγτ

| Prµ(L(τ +1)|L(1)) − ¯µ(L(τ +1))| ≤ κγτ

for some constants κ ≥ 1 and γ < 1 that only depend on µ and ν. Let Dl(µ||ν) be the KL-divergence
l∈L ¯µ(l)Dl(µ||ν) and
similarly for E¯νDl(ν||µ). As in the previous subsection  we use the average log-likelihood ratio as
(1−γ) minl{¯µ(l) ¯ν(l)}. Applying Theorem 1 gives the following corollary.
the weight. Deﬁne λ =
See sections H–I in the supplementary material for the proof and additional discussion.
Corollary 5 (Markov Snapshots). Under the above setting  suppose for each label-pair (l  l(cid:48)) 

between µ(·|l) and ν(·|l); Dl(ν||µ) is similarly deﬁned. Let E¯µDl(µ||ν) =(cid:80)
(cid:12)(cid:12)(cid:12)log ¯µ(l)

(cid:12)(cid:12)(cid:12) ≤ b  D(¯ν||¯µ) ≤ ζD(¯µ||¯ν) and E¯νDl(ν||µ) ≤ ζE¯µDl(µ||ν). The
(cid:17)E¯νDl(ν||µ) ≥ c(b + 2)
(cid:17)E¯µDl(µ||ν) ≥ c(b + 2) max

program (2) with MLE weights recovers Y ∗ with probability at least 1 − n−10 provided

(cid:12)(cid:12)(cid:12)log µ(l(cid:48)|l)
(cid:16)
(cid:16)

D(¯ν||¯µ) +
D(¯µ||¯ν) +

(cid:12)(cid:12)(cid:12) ≤ b 

(cid:110) log n

  (ζ + 1)λ

ν(l(cid:48)|l)

log n

(cid:111)

.

(11)

(12)

¯ν(l)

K

K

n log n
T K 2

1
T
1
T

1 − 1
T
1 − 1
T

7

As an illuminating example  consider the case where ¯µ ≈ ¯ν  i.e.  the marginal distributions for
individual snapshots are identical or very close. It means that the information is contained in the
change of labels  but not in the individual labels  as made evident in the LHSs of (11) and (12).
In this case  it is necessary to use the temporal information in order to perform clustering. Such
information would be lost if we disregard the ordering of the snapshots  for example  by aggregating
or averaging the snapshots then apply a single-snapshot clustering algorithm. This highlights an
essential difference between clustering time-varying graphs and static graphs.

5 Empirical results

To solve the convex program (2)  we follow [13  9] and adapt the ADMM algorithm by [25]. We
perform 100 trials for each experiment  and report the success rate  i.e.  the fraction of trials where
the ground-truth clustering is fully recovered. Error bars show 95% conﬁdence interval. Additional
empirical results are provided in the supplementary material.
We ﬁrst test the planted partition model with partial observations under the challenging sparse (p
and q close to 0) and dense settings (p and q close to 1); cf. section 4.2. Figures 1 and 2 show the
results for n = 1000 with 4 equal-size clusters. In both cases  each pair is observed with probability
0.5. For comparison  we include results for the MLE weights as well as the linear weights (based on
linear approximation of the log-likelihood ratio)  uniform weights and a imputation scheme where
all unobserved entries are assumed to be “no-edge”.

Figure 1: Sparse graphs

Figure 2: Dense graphs

Figure 3: Reality Mining dataset

Corollary 3 predicts more success as the ratio s(p−q)2
p(1−q) gets larger. All else being the same  distribu-
tions with small ζ (sparse) are “easier” to solve. Both predictions are consistent with the empirical
results in Figs. 1 and 2. The results also show that the MLE weights outperform the other weights.
For real data  we use the Reality Mining dataset [26]  which contains individuals from two main
groups  the MIT Media Lab and the Sloan Business School  which we use as the ground-truth
clusters. The dataset records when two individuals interact  i.e.  become proximal of each other or
make a phone call  over a 9-month period. We choose a window of 14 weeks (the Fall semester)
where most individuals have non-empty interaction data. These consist of 85 individuals with 25 of
them from Sloan. We represent the data as a time-varying graph with 14 snapshots (one per week)
and two labels—an “edge” if a pair of individuals interact within the week  and “no-edge” otherwise.
We compare three models: Markov sequence  independent snapshots  and the aggregate (union)
graphs. In each trial  the in/cross-cluster distributions are estimated from a fraction of randomly
selected pairwise interaction data. The vertical axis in Figure 3 shows the fraction of pairs whose
cluster relationship are correctly identiﬁed. From the results  we infer that the interactions between
individuals are likely not independent across time  and are better captured by the Markov model.

Acknowledgments

S.H. Lim and H. Xu were supported by the Ministry of Education of Singapore through AcRF Tier
Two grant R-265-000-443-112. Y. Chen was supported by NSF grant CIF-31712-23800 and ONR
MURI grant N00014-11-1-0688.

8

0.10.20.30.40.500.20.40.60.81p − qsuccess rateq = 0.02  s = 0.5 MLElinearuniformno partial0.10.20.30.40.500.20.40.60.81p − qsuccess ratep = 0.98  s = 0.5 MLElinearuniformno partial00.0050.010.0150.020.0250.70.750.80.850.914 weeksfraction of data used in estimationaccuracy MarkovindependentaggregateReferences
[1] K. Rohe  S. Chatterjee  and B. Yu. Spectral clustering and the high-dimensional stochastic

block model. Annals of Statistics  39:1878–1915  2011.

[2] A. Condon and R. M. Karp. Algorithms for graph partitioning on the planted partition model.

Random Structures and Algorithms  18(2):116–140  2001.

[3] S. Oymak and B. Hassibi. Finding dense clusters via low rank + sparse decomposition.

arXiv:1104.5186v1  2011.

[4] Y. Chen  A. Jalali  S. Sanghavi  and H. Xu. Clustering partially observed graphs via convex

optimization. Journal of Machine Learning Research  15:2213–2238  June 2014.

[5] Sivaraman Balakrishnan  Mladen Kolar  Alessandro Rinaldo  Aarti Singh  and Larry Wasser-
man. Statistical and computational tradeoffs in biclustering. In NIPS Workshop on Computa-
tional Trade-offs in Statistical Learning  2011.

[6] Mladen Kolar  Sivaraman Balakrishnan  Alessandro Rinaldo  and Aarti Singh. Minimax local-

ization of structural information in large noisy matrices. In NIPS  pages 909–917  2011.

[7] F. McSherry. Spectral partitioning of random graphs. In FOCS  pages 529–537  2001.
[8] K. Chaudhuri  F. Chung  and A. Tsiatas. Spectral clustering of graphs with general degrees in

the extended planted partition model. COLT  2012.

[9] Y. Chen  S. Sanghavi  and H. Xu. Clustering sparse graphs. In NIPS 2012.  2012.
[10] B. Ames and S. Vavasis. Nuclear norm minimization for the planted clique and biclique prob-

lems. Mathematical Programming  129(1):69–89  2011.

[11] C. Mathieu and W. Schudy. Correlation clustering with noisy input. In SODA  page 712  2010.
[12] Anima Anandkumar  Rong Ge  Daniel Hsu  and Sham M Kakade. A tensor spectral approach

to learning mixed membership community models. arXiv preprint arXiv:1302.2684  2013.

[13] Y. Chen  S. H. Lim  and H. Xu. Weighted graph clustering with non-uniform uncertainties. In

ICML  2014.

[14] Simon Heimlicher  Marc Lelarge  and Laurent Massouli´e. Community detection in the labelled
In NIPS Workshop on Algorithmic and Statistical Approaches for

stochastic block model.
Large Social Networks  2012.

[15] Marc Lelarge  Laurent Massouli´e  and Jiaming Xu. Reconstruction in the Labeled Stochastic

Block Model. In IEEE Information Theory Workshop  Seville  Spain  September 2013.
[16] S. Fortunato. Community detection in graphs. Physics Reports  486(3-5):75–174  2010.
[17] Jimeng Sun  Christos Faloutsos  Spiros Papadimitriou  and Philip S. Yu. Graphscope:

parameter-free mining of large time-evolving graphs. In ACM KDD  2007.

[18] D. Chakrabarti  R. Kumar  and A. Tomkins. Evolutionary clustering. In ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining  pages 554–560. ACM  2006.
[19] Vikas Kawadia and Sameet Sreenivasan. Sequential detection of temporal communities by

estrangement conﬁnement. Scientiﬁc Reports  2  2012.

[20] N.P. Nguyen  T.N. Dinh  Y. Xuan  and M.T. Thai. Adaptive algorithms for detecting commu-

nity structure in dynamic social networks. In INFOCOM  pages 2282–2290. IEEE  2011.

[21] N. Bansal  A. Blum  and S. Chawla. Correlation clustering. Machine Learning  56(1)  2004.
[22] A. Jalali  Y. Chen  S. Sanghavi  and H. Xu. Clustering partially observed graphs via convex

optimization. In ICML  2011.

[23] Brendan P.W. Ames. Guaranteed clustering and biclustering via semideﬁnite programming.

Mathematical Programming  pages 1–37  2013.

[24] F. Topsoe. Some inequalities for information divergence and related measures of discrimina-

tion. IEEE Transactions on Information Theory  46(4):1602–1609  Jul 2000.

[25] Z. Lin  M. Chen  L. Wu  and Y. Ma. The augmented lagrange multiplier method for exact
recovery of corrupted low-rank matrices. Technical Report UILU-ENG-09-2215  UIUC  2009.
[26] Nathan Eagle and Alex (Sandy) Pentland. Reality mining: Sensing complex social systems.

Personal Ubiquitous Comput.  10(4):255–268  March 2006.

[27] Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer  2009.

9

,Shiau Hong Lim
Yudong Chen
Huan Xu
Jianqiao Wangni
Jialei Wang
Ji Liu
Tong Zhang