2015,Color Constancy by Learning to Predict Chromaticity from Luminance,Color constancy is the recovery of true surface color from observed color  and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper  we show that the per-pixel color statistics of natural scenes---without any spatial or semantic context---can by themselves be a powerful cue for color constancy. Specifically  we describe an illuminant estimation method that is built around a classifier for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference  each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants  and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions  and for achromatic colors in brighter pixels. Despite its simplicity  the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next  we propose a method to learn the luminance-to-chromaticity classifier end-to-end. Using stochastic gradient descent  we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy  most significantly in the tail of the error distribution.,Color Constancy by Learning to Predict

Chromaticity from Luminance

Ayan Chakrabarti

Toyota Technological Institute at Chicago
6045 S. Kenwood Ave.  Chicago  IL 60637

ayanc@ttic.edu

Abstract

Color constancy is the recovery of true surface color from observed color  and
requires estimating the chromaticity of scene illumination to correct for the bias
it induces.
In this paper  we show that the per-pixel color statistics of natural
scenes—without any spatial or semantic context—can by themselves be a pow-
erful cue for color constancy. Speciﬁcally  we describe an illuminant estimation
method that is built around a “classiﬁer” for identifying the true chromaticity of
a pixel given its luminance (absolute brightness across color channels). During
inference  each pixel’s observed color restricts its true chromaticity to those val-
ues that can be explained by one of a candidate set of illuminants  and applying
the classiﬁer over these values yields a distribution over the corresponding illumi-
nants. A global estimate for the scene illuminant is computed through a simple
aggregation of these distributions across all pixels. We begin by simply deﬁn-
ing the luminance-to-chromaticity classiﬁer by computing empirical histograms
over discretized chromaticity and luminance values from a training set of natural
images. These histograms reﬂect a preference for hues corresponding to smooth
reﬂectance functions  and for achromatic colors in brighter pixels. Despite its
simplicity  the resulting estimation algorithm outperforms current state-of-the-art
color constancy methods. Next  we propose a method to learn the luminance-
to-chromaticity classiﬁer “end-to-end”. Using stochastic gradient descent  we set
chromaticity-luminance likelihoods to minimize errors in the ﬁnal scene illumi-
nant estimates on a training set. This leads to further improvements in accuracy 
most signiﬁcantly in the tail of the error distribution.

1

Introduction

The spectral distribution of light reﬂected off a surface is a function of an intrinsic material property
of the surface—its reﬂectance—and also of the spectral distribution of the light illuminating the
surface. Consequently  the observed color of the same surface under different illuminants in different
images will be different. To be able to reliably use color computationally for identifying materials
and objects  researchers are interested in deriving an encoding of color from an observed image that
is invariant to changing illumination. This task is known as color constancy  and requires resolving
the ambiguity between illuminant and surface colors in an observed image. Since both of these
quantities are unknown  much of color constancy research is focused on identifying models and
statistical properties of natural scenes that are informative for color constancy. While pschophysical
experiments have demonstrated that the human visual system is remarkably successful at achieving
color constancy [1]  it remains a challenging task computationally.

Early color constancy algorithms were based on relatively simple models for pixel colors. For
example  the gray world method [2] simply assumed that the average true intensities of different
color channels across all pixels in an image would be equal  while the white-patch retinex method [3]

1

assumed that the true color of the brightest pixels in an image is white. Most modern color constancy
methods  however  are based on more complex reasoning with higher-order image features. Many
methods [4  5  6] use models for image derivatives instead of individual pixels. Others are based on
recognizing and matching image segments to those in a training set to recover true color [7]. A recent
method proposes the use of a multi-layer convolutional neural network (CNN) to regress from image
patches to illuminant color. There are also many “combination-based” color constancy algorithms 
that combine illuminant estimates from a number of simpler “unitary” algorithms [8  9  10  11] 
sometimes using image features to give higher weight to the outputs of some subset of methods.

In this paper  we demonstrate that by appropriately modeling and reasoning with the statistics of
individual pixel colors  one can computationally recover illuminant color with high accuracy. We
consider individual pixels in isolation  where the color constancy task reduces to discriminating be-
tween the possible choices of true color for the pixel that are feasible given the observed color and a
candidate set of illuminants. Central to our method is a function that gives us the relative likelihoods
of these true colors  and therefore a distribution over the corresponding candidate illuminants. Our
global estimate for the scene illuminant is then computed by simply aggregating these distributions
across all pixels in the image.

We formulate the likelihood function as one that measures the conditional likelihood of true pixel
chromaticity given observed luminance  in part to be agnostic to the scalar (i.e.  color channel-
independent) ambiguity in observed color intensities. Moreover  rather than committing to a para-
metric form  we quantize the space of possible chromaticity and luminance values  and deﬁne the
function over this discrete domain. We begin by setting the conditional likelihoods purely empir-
ically  based simply on the histograms of true color values over all pixels in all images across a
training set. Even with this purely empirical approach  our estimation algorithm yields estimates
with higher accuracy than current state-of-the-art methods. Then  we investigate learning the per-
pixel belief function by optimizing an objective based on the accuracy of the ﬁnal global illuminant
estimate. We carry out this optimization using stochastic gradient descent  and using a sub-sampling
approach (similar to “dropout” [12]) to improve generalization beyond the training set. This further
improves estimation accuracy  without adding to the computational cost of inference.

2 Preliminaries

Assuming Lambertian reﬂection  the spectral distribution of light reﬂected by a material is a product
of the distribution of the incident light and the material’s reﬂectance function. The color intensity
vector v(n) ∈ R3 recorded by a tri-chromatic sensor at each pixel n is then given by

v(n) =Z κ(n  λ)ℓ(n  λ) s(n) Π(λ) dλ 

(1)

where κ(n  λ) is the reﬂectance at n  ℓ(n  λ) is the spectral distribution of the incident illumination 
s(n) is a geometry-dependent shading factor  and Π(λ) ∈ R3 denotes the spectral sensitivities of
the color sensors. Color constancy is typically framed as the task of computing from v(n) the
corresponding color intensities x(n) ∈ R3 that would have been observed under some canonical
illuminant ℓref (typically chosen to be ℓref(λ) = 1). We will refer to x(n) as the “true color” at n.

Since (1) involves a projection of the full incident light spectrum on to the three ﬁlters Π(λ)  it is not
generally possible to recover x(n) from v(n) even with knowledge of the illuminant ℓ(n  λ). How-
ever  a commonly adopted approximation (shown to be reasonable under certain assumptions [13])
is to relate the true and observed colors x(n) and v(n) by a simple per-channel adaptation:

v(n) = m(n) ◦ x(n) 

(2)

where ◦ refers to the element-wise Hadamard product  and m(n) ∈ R3 depends on the illuminant
ℓ(n  λ) (for ℓref  m = [1  1  1]T ). With some abuse of terminology  we will refer to m(n) as the
illuminant in the remainder of the paper. Moreover  we will focus on the single-illuminant case in
this paper  and assume m(n) = m  ∀n in an image. Our goal during inference will be to estimate
this global illuminant m from the observed image v(n). The true color image x(n) can then simply
be recovered as m−1 ◦ v(n)  where m−1 ∈ R3 denotes the element-wise inverse of m.

Note that color constancy algorithms seek to resolve the ambiguity between m and x(n) in (2) only
up to a channel-independent scalar factor. This is because scalar ambiguities show up in m between

2

ℓ and ℓref due to light attenuation  between x(n) and κ(n) due to the shading factor s(n)  and in
the observed image v(n) itself due to varying exposure settings. Therefore  the performance metric

typically used is the angular error cos−1(cid:16) m

vectors m and ¯m.

T ¯m

kmk2k ¯mk2(cid:17) between the true and estimated illuminant

Database For training and evaluation  we use the database of 568 natural indoor and outdoor
images captured under various illuminants by Gehler et al. [14]. We use the version from Shi and
Funt [15] that contains linear images (without gamma correction) generated from the RAW camera
data. The database contains images captured with two different cameras (86 images with a Canon
1D  and 482 with a Canon 5D). Each image contains a color checker chart placed in the image  with
its position manually labeled. The colors of the gray squares in the chart are taken to be the value
of the true illuminant m for each image  which can then be used to correct the image to get true
colors at each pixel (of course  only up to scale). The chart is masked out during evaluation. We use
k-fold cross-validation over this dataset in our experiments. Each fold contains images from both
cameras corresponding to one of k roughly-equal partitions of each camera’s image set (ordered
by ﬁle name/order of capture). Estimates for images in each fold are based on training only with
data from the remaining folds. We report results with three- and ten-fold cross-validation. These
correspond to average training set sizes of 379 and 511 images respectively.

3 Color Constancy with Pixel-wise Chromaticity Statistics

A color vector x ∈ R3 can be characterized in terms of (1) its luminance kxk1  or absolute brightness
across color channels; and (2) its chromaticity  which is a measure of the relative ratios between
intensities in different channels. While there are different ways of encoding chromaticity  we will
do so in terms of the unit vector ˆx = x/kxk2 in the direction of x. Note that since intensities can
not be negative  ˆx is restricted to lie on the non-negative eighth of the unit sphere S2
+. Remember
from Sec. 2 that our goal is to resolve the ambiguity between the true colors x(n) and the illuminant
m only up to scale. In other words  we need only estimate the illuminant chromaticity ˆm and true
chromaticities ˆx(n) from the observed image v(n)  which we can relate from (2) as

ˆx(n) =

x(n)

kx(n)k2

=

ˆm−1 ◦ v(n)

k ˆm−1 ◦ v(n)k2

∆= g(v(n)  ˆm).

(3)

A key property of natural illuminant chromaticities is that they are known to take a fairly restricted
set of values  close to a one-dimensional locus predicted by Planck’s radiation law [16]. To be able
to exploit this  we denote M = { ˆmi}M
i=1 as the set of possible values for illuminant chromaticity ˆm 
and construct it from a training set. Speciﬁcally  we quantize1 the chromaticity vectors { ˆmt}T
t=1 of
the illuminants in the training set  and let M be the set of unique chromaticity values. Additionally 
we deﬁne a “prior” bi = log(ni/T ) over this candidate set  based on the number ni of training
illuminants that were quantized to ˆmi.

Given the observed color v(n) at a single pixel n  the ambiguity in ˆm across the illuminant set M
translates to a corresponding ambiguity in the true chromaticity ˆx(n) over the set {g(v(n)  ˆmi)}i.
Figure 1(a) illustrates this ambiguity for a few different observed colors v. We note that while there
is signiﬁcant angular deviation within the set of possible true chromaticity values for any observed
color  values in each set lie close to a one dimensional locus in chromaticity space. This suggests
that the illuminants in our training set are indeed a good ﬁt to Planck’s law2.

The goal of our work is to investigate the extent to which we can resolve the above ambiguity
in true chromaticity on a per-pixel basis  without having to reason about the pixel’s spatial neigh-
borhood or semantic context. Our approach is based on computing a likelihood distribution over
the possible values of ˆx(n)  given the observed luminance kv(n)k1. But as mentioned in Sec. 2 
there is considerable ambiguity in the scale of observed color intensities. We address this par-
tially by applying a simple per-image global normalization to the observed luminance to deﬁne

1Quantization is over uniformly sized bins in S2
2In fact  the chromaticities appear to lie on two curves  that are slightly separated from each other. This

+. See supplementary material for details.

separation is likely due to differences in the sensor responses of the two cameras in the Gehler-Shi dataset.

3

Figure 1: Color Constancy with Per-pixel Chromaticity-luminance distributions of natural scenes.
(a) Ambiguity in true chromaticity given observed color: each set of points corresponds to the
possible true chromaticity values (location in S2
+  see legend) consistent with the pixel’s observed
chromaticity (color of the points) and different candidate illuminants ˆmi. (b) Distributions over
different values for true chromaticity of a pixel conditioned on its observed luminance  computed
as empirical histograms over the training set. Values y are normalized per-image by the median
luminance value over all pixels. (c) Corresponding distributions learned with end-to-end training to
maximize accuracy of overall illuminant estimation.

y(n) = kv(n)k1/median{kv(n′)k1}n′ . This very roughly compensates for variations across im-
ages due to exposure settings  illuminant brightness  etc. However  note that since the normalization
is global  it does not compensate for variations due to shading.

The central component of our inference method is a function L[ˆx  y] that encodes the belief that a
pixel with normalized observed luminance y has true chromaticity ˆx. This function is deﬁned over
a discrete domain by quantizing both chromaticity and luminance values: we clip luminance values
y to four (i.e.  four times the median luminance of the image) and quantize them into twenty equal
sized bins; and for chromaticity ˆx  we use a much ﬁnger quantization with 214 equal-sized bins in
S 2
+ (see supplementary material for details). In this section  we adopt a purely empirical approach

4

(b) from Empirical Statistics(c) from End-to-end Learningy ∈ [0 0.2)y ∈ [0.2 0.4)y ∈ [0.8 1.0)y ∈ [1.4 1.6)y ∈ [1.8 2.0)y ∈ [2.8 3.0)y ∈ [3.4 3.6)y ∈ [3.8 ∞)y ∈ [0 0.2)y ∈ [0.2 0.4)y ∈ [0.8 1.0)y ∈ [1.4 1.6)y ∈ [1.8 2.0)y ∈ [2.8 3.0)y ∈ [3.4 3.6)y ∈ [3.8 ∞)(a) Ambiguity with Observed ColorRedBlueGreenGreen + BlueBlue + RedRed + Green-15.1-3.3LegendTrue ChromaticitySet of possible truechromaticitiesfor a speciﬁc observedcolor v.′ Nˆx

′ y)   where Nˆx y is the number of pixels across
all pixels in a set of images in a training set that have true chromaticity ˆx and observed luminance y.

and deﬁne L[ˆx  y] as L[ˆx  y] = log (Nˆx y/Pˆx

We visualize these empirical versions of L[ˆx  y] for a subset of the luminance quantization levels
in Fig. 1(b). We ﬁnd that in general  desaturated chromaticities with similar intensity values in all
color channels are most common. This is consistent with ﬁndings of statistical analysis of natural
spectra [17]  which shows the “DC” component (ﬂat across wavelength) to be the one with most
variance. We also note that the concentration of the likelihood mass in these chromaticities increas-
ing for higher values of luminance y. This phenomenon is also predicted by traditional intuitions
in color science: materials are brightest when they reﬂect most of the incident light  which typi-
cally occurs when they have a ﬂat reﬂectance function with all values of κ(λ) close to one. Indeed 
this is what forms the basis of the white-patch retinex method [3]. Amongst saturated colors  we
ﬁnd that hues which combine green with either red or blue occur more frequently than primary col-
ors  with pure green and combinations of red and blue being the least common. This is consistent
with ﬁndings that reﬂectance functions are usually smooth (PCA on pixel spectra in [17] revealed a
Fourier-like basis). Both saturated green and red-blue combinations would require the reﬂectance to
have either a sharp peak or crest  respectively  in the middle of the visible spectrum.

We now describe a method that exploits the belief function L[ˆx  y] for illuminant estimation. Given
the observed color v(n) at a pixel n  we can obtain a distribution {L[g(v(n)  ˆmi)  y(n)]}i over
the set of possible true chromaticity values {g(v(n)  ˆmi)}i  which can also be interpreted as a
distribution over the corresponding illuminants ˆmi. We then simply aggregate these distributions
across all pixels n in the image  and deﬁne the global probability of ˆmi being the scene illuminant

m as pi = exp(li)/ (Pi′ exp(li′ ))  where
N Xn

li =

α

L[g(v(n)  ˆmi)  y(n)] + βbi 

(4)

N is the total number of pixels in the image  and α and β are scalar parameters. The ﬁnal illuminant
chromaticity estimate ¯m is then computed as

¯m = arg min

m

′ km

′k2=1

E [cos−1(mT m′)] ≈ arg max
′k2=1

′ km

m

E[mT m′] = Pi pimi
kPi pimik2

.

(5)

Note that (4) also incorporates the prior bi over illuminants. We set the parameters α and β using
a grid search  to values that minimize mean illuminant estimation error over the training set. The
primary computational cost of inference is in computing the values of {li}. We pre-compute values
of g(ˆx  ˆm) using (3) over the discrete domain of quantized chromaticity values for ˆx and the candi-
date illuminant set M for ˆm. Therefore  computing each li essentially only requires the addition of
N numbers from a look-up table. We need to do this for all M = |M| illuminants  where summa-
tions for different illuminants can be carried out in parallel. Our implementation takes roughly 0.3
seconds for a 9 mega-pixel image  on a modern Intel 3.3GHz CPU with 6 cores  and is available at
http://www.ttic.edu/chakrabarti/chromcc/.

This empirical version of our approach bears some similarity to the Bayesian method of [14] that
is based on priors for illuminants  and for the likelihood of different true reﬂectance values being
present in a scene. However  the key difference is our modeling of true chromaticity conditioned
on luminance that explicitly makes estimation agnostic to the absolute scale of intensity values. We
also reason with all pixels  rather than the set of unique colors in the image.

Experimental Results. Table 1 compares the performance of illuminant estimation with our
method (see rows labeled “Empirical”) to the current state-of-the-art  using different quantiles of
angular error across the Gehler-Shi database [14  15]. Results for other methods are from the survey
by Li et al. [18]. (See the supplementary material for comparisons to some other recent methods).

We show results with both three- and ten-fold cross-validation. We ﬁnd that our errors with three-
fold cross-validation have lower mean  median  and tri-mean values than those of the best performing
state-of-the-art method from [8]  which combines illuminant estimates from twelve different “uni-
tary” color-constancy method (many of which are also listed in Table 1) using support-vector regres-
sion. The improvement in error is larger with respect to the other combination methods [8  9  10  11] 
as well as those based the statistics of image derivatives [4  5  6]. Moreover  since our method has
more parameters than most previous algorithms (L[ˆx  y] has 214 × 20 ≈ 300k entries)  it is likely

5

Table 1: Quantiles of Angular Error for Different Methods on the Gehler-Shi Database [14  15]

Method Mean Median Tri-mean

Bayesian [14]

Gamut Mapping [20]

Deriv. Gamut Mapping [4]

Gray World [2]
Gray Edge(1 1 6) [5]
SV-Regression [21]

Spatio-Spectral [6]

Scene Geom. Comb. [9]

Nearest-30% Comb. [10]

Classiﬁer-based Comb. [11]

Neural Comb. (ELM) [8]

SVR-based Comb. [8]

Proposed

(3-Fold)

Empirical
End-to-end Trained

(10-Fold)

Empirical
End-to-end Trained

6.74◦
6.00◦
5.96◦
4.77◦
4.19◦
4.14◦
3.99◦
4.56◦
4.26◦
3.83◦
3.43◦
2.98◦

2.89◦
2.56◦
2.55◦
2.20◦

5.14◦
3.98◦
3.83◦
3.63◦
3.28◦
3.23◦
3.24◦
3.15◦
2.95◦
2.75◦
2.37◦
1.97◦

1.89◦
1.67◦
1.58◦
1.37◦

5.54◦
4.52◦
4.32◦
3.92◦
3.54◦
3.35◦
3.45◦
3.46◦
3.19◦
2.93◦
2.62◦
2.35◦

2.15◦
1.89◦
1.83◦
1.53◦

25%-ile
2.42◦
1.71◦
1.68◦
1.81◦
1.87◦
1.68◦
2.38◦
1.41◦
1.49◦
1.34◦
1.21◦
1.13◦

75%-ile
9.47◦
8.42◦
7.95◦
6.63◦
5.72◦
5.27◦
4.97◦
6.12◦
5.39◦
4.89◦
4.53◦
4.33◦

90%-ile
14.71◦
14.74◦
14.72◦
10.59◦
8.60◦
8.87◦
7.50◦
10.39◦
9.67◦
8.19◦
6.97◦
6.37◦

1.15◦
0.91◦
0.85◦
0.69◦

3.68◦
3.30◦
3.30◦
2.68◦

6.24◦
5.56◦
5.74◦
4.89◦

to beneﬁt from more training data. We ﬁnd this to indeed be the case  and observe a considerable
decrease in error quantiles when we switch to ten-fold cross-validation.

Figure. 2 shows estimation results with our method for a few sample images. For each image  we
show the input image (indicating the ground truth color chart being masked out) and the output image
with colors corrected by the global illuminant estimate. To visualize the quality of contributions from
individual pixels  we also show a map of angular errors for illuminant estimates from individual
pixels. These estimates are based on values of li computed by restricting the summation in (4) to
individual pixels. We ﬁnd that even these pixel-wise estimates are fairly accurate for a lot of pixels 
even when it’s true color is saturated (see cart in ﬁrst row). Also  to evaluate the weight of these
per-pixel distributions to the global li  we show a map of their variance on a per-pixel basis. As
expected from Fig. 1(b)  we note higher variances in relatively brighter pixels. The image in the last
row represents one of the poorest estimates across the entire dataset (higher than 90%−ile). Note
that much of the image is in shadow  and contain only a few distinct (and likely atypical) materials.

4 Learning L[ˆx  y] End-to-end

While the empirical approach in the previous section would be optimal if pixel chromaticities in a
typical image were infact i.i.d.  that is clearly not the case. Therefore  in this section we propose an
alternate approach method to setting the beliefs in L[ˆx  y]  that optimizes for the accuracy of the ﬁnal
global illuminant estimate. However  unlike previous color constancy methods that explicitly model
statistical co-dependencies between pixels—for example  by modeling spatial derivatives [4  5  6] 
or learning functions on whole-image histograms [21]—we retain the overall parametric “form” by
which we compute the illuminant in (4). Therefore  even though L[ˆx  y] itself is learned through
knowledge of co-occurence of chromaticities in natural images  estimation of the illuminant during
inference is still achieved through a simple aggregation of per-pixel distributions.

Speciﬁcally  we set the entries of L[ˆx  y] to minimize a cost function C over a set of training images:

C(L) =

C t(L) 

T

Xt=1

C t =Xi

6

cos−1( ˆmT

i ˆmt) pt
i 

(6)

Global Estimate

Per-Pixel Estimate Error

Belief Variance

Input+Mask

Ground Truth

Input+Mask

Ground Truth

Input+Mask

l
a
c
i
r
i
p
m
E

d
n
e
-
o
t
-
d
n
E

l
a
c
i
r
i
p
m
E

d
n
e
-
o
t
-
d
n
E

l
a
c
i
r
i

p
m
E

d
n
e
-
o
t
-
d
n
E

Error = 0.56◦

Error = 0.24◦

Error = 4.32◦

Error = 3.15◦

Error = 16.22◦

Ground Truth

Error = 10.31◦

Figure 2: Estimation Results on Sample Images. Along with output images corrected with the global
illuminant estimate from our methods  we also visualize illuminant information extracted at a local
level. We show a map of the angular error of pixel-wise illuminant estimates (i.e.  computed with li
based on distributions from only a single pixel). We also show a map of the variance Var({li}i) of
these beliefs  to gauge the weight of their contributions to the global illuminant estimate.

where ˆmt is the true illuminant chromaticity of the tth training image  and pt
i is computed from the
observed colors vt(n) using (4). We augment the training data available to us by “re-lighting” each
image with different illuminants from the training set. We use the original image set and six re-lit
copies for training  and use a seventh copy for validation.

We use stochastic gradient descent to minimize (6). We initialize L to empirical values as described
in the previous section (for convenience  we multiply the empirical values by α  and then set α = 1
for computing li)  and then consider individual images from the training set at each iteration. We
make multiple passes through the training set  and at each iteration  we randomly sub-sample the
pixels from each training image. Speciﬁcally  we only retain 1/128 of the total pixels in the image
by randomly sub-sampling 16 × 16 patches at a time. This approach  which can be interpreted as
being similar to “dropout” [12]  prevents over-ﬁtting and improves generalization.

7

Derivatives of the cost function C t with respect to the current values of beliefs L[ˆx  y] are given by

∂C t

∂L[ˆx  y]

=

1

where

∂C t
∂lt
i

= pt

N Xi Xn
i(cid:0)cos−1( ˆmT

δ(cid:0)g(vt(n)  ˆmi) = ˆx(cid:1) δ(cid:0)yt(n) = y(cid:1)! ×
i ˆmt) − C t(cid:1) .

∂C t
∂lt
i

 

We use momentum to update the values of L[ˆx  y] at each iteration based on these derivative as

L[ˆx  y] = L[ˆx  y] − L∇[ˆx  y]  L∇[ˆx  y] = r

∂C t

∂L[ˆx  y]

+ µL∇

∗ [ˆx  y] 

(7)

(8)

(9)

where L∇
∗ [ˆx  y] is the previous update value  r is the learning rate  and µ is the momentum factor. In
our experiments  we set µ = 0.9  run stochastic gradient descent for 20 epochs with r = 100  and
another 10 epochs with r = 10. We retain the values of L from each epoch  and our ﬁnal output is
the version that yields the lowest mean illuminant estimation error on the validation set.

We show the belief values learned in this manner in Fig. 1(c). Notice that although they retain the
overall biases towards desaturated colors and combined green-red and green-blue hues  they are less
“smooth” than their empirical counterparts in Fig. 1(b)—in many instances  there are sharp changes
in the values L[ˆx  y] for small changes in chromaticity. While harder to interpret  we hypothesize
that these variations result from shifting beliefs of speciﬁc (ˆx  y) pairs to their neighbors  when they
correspond to incorrect choices within the ambiguous set of speciﬁc observed colors.

Experimental Results. We also report errors when using these end-to-end trained versions of the
belief function L in Table 1  and ﬁnd that they lead to an appreciable reduction in error in comparison
to their empirical counterparts. Indeed  the errors with end-to-end training using three-fold cross-
validation begin to approach those of the empirical version with ten-fold cross-validation  which
has access to much more training data. Also note that the most signiﬁcant improvements (for both
three- and ten-fold cross-validation) are in “outlier” performance  i.e.  in the 75 and 90%-ile error
values. Color constancy methods perform worst on images that are dominated by a small number of
materials with ambiguous chromaticity  and our results indicate that end-to-end training increases
the reliability of our estimation method in these cases.

We also include results for the end-to-end case for the example images in Figure. 2. For all three
images  there is an improvement in the global estimation error. More interestingly  we see that the
per-pixel error and variance maps now have more high-frequency variation  since L now reacts more
sharply to slight chromaticity changes from pixel to pixel. Moreover  we see that a larger fraction of
pixels generate fairly accurate estimates by themselves (blue shirt in row 2). There is also a higher
disparity in belief variance  including within regions that visually look homogeneous in the input 
indicating that the global estimate is now more heavily inﬂuenced by a smaller fraction of pixels.

5 Conclusion and Future Work

In this paper  we introduced a new color constancy method that is based on a conditional likelihood
function for the true chromaticity of a pixel  given its luminance. We proposed two approaches to
learning this function. The ﬁrst was based purely on empirical pixel statistics  while the second
was based on maximizing accuracy of the ﬁnal illuminant estimate. Both versions were found to
outperform state-of-the-art color constancy methods  including those that employed more complex
features and semantic reasoning. While we assumed a single global illuminant in this paper  the
underlying per-pixel reasoning can likely be extended to the multiple-illuminant case  especially
since  as we saw in Fig. 2  our method was often able to extract reasonable illuminant estimates
from individual pixels. Another useful direction for future research is to investigate the beneﬁts of
using likelihood functions that are conditioned on lightness—estimated using an intrinsic image de-
composition method—instead of normalized luminance. This would factor out the spatially-varying
scalar ambiguity caused by shading  which could lead to more informative distributions.

Acknowledgments

We thank the authors of [18] for providing estimation results of other methods for comparison. The
author was supported by a gift from Adobe.

8

References

[1] D.H. Brainard and A Radonjic. Color constancy. In The new visual neurosciences  2014.

[2] G. Buchsbaum. A spatial processor model for object colour perception. J. Franklin Inst  1980.

[3] E.H. Land. The retinex theory of color vision. Scientiﬁc American  1971.

[4] A. Gijsenij  T. Gevers  and J. van de Weijer. Generalized gamut mapping using image derivative structures

for color constancy. IJCV  2010.

[5] J. van de Weijer  T. Gevers  and A. Gijsenij. Edge-Based Color Constancy. IEEE Trans. Image Proc. 

2007.

[6] A. Chakrabarti  K. Hirakawa  and T. Zickler. Color Constancy with Spatio-spectral Statistics. PAMI 

2012.

[7] H. R. V. Joze and M. S. Drew. Exemplar-based color constancy and multiple illumination. PAMI  2014.

[8] B. Li  W. Xiong  and D. Xu. A supervised combination strategy for illumination chromaticity estimation.

ACM Trans. Appl. Percept.  2010.

[9] R. Lu  A. Gijsenij  T. Gevers  V. Nedovic  and D. Xu. Color constancy using 3D scene geometry. In

ICCV  2009.

[10] S. Bianco  F. Gasparini  and R. Schettini. Consensus-based framework for illuminant chromaticity esti-

mation. J. Electron. Imag.  2008.

[11] S. Bianco  G. Ciocca  C. Cusano  and R. Schettini. Automatic color constancy algorithm selection and

combination. Pattern recognition  2010.

[12] N. Srivastava  G. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: A simple way to

prevent neural networks from overﬁtting. JMLR  2014.

[13] H. Chong  S. Gortler  and T. Zickler. The von Kries hypothesis and a basis for color constancy. In Proc.

ICCV  2007.

[14] P.V. Gehler  C. Rother  A. Blake  T. Minka  and T. Sharp. Bayesian Color Constancy Revisited. In CVPR 

2008.

[15] L. Shi and B. Funt. Re-processed version of the gehler color constancy dataset of 568 images. Accessed

from http://www.cs.sfu.ca/~colour/data/.

[16] D.B. Judd  D.L. MacAdam  G. Wyszecki  H.W. Budde  H.R. Condit  S.T. Henderson  and J.L. Simonds.

Spectral distribution of typical daylight as a function of correlated color temperature. JOSA  1964.

[17] A. Chakrabarti and T. Zickler. Statistics of Real-World Hyperspectral Images. In Proc. CVPR  2011.

[18] B. Li  W. Xiong  W. Hu  and B. Funt. Evaluating combinational illumination estimation methods on
real-world images. IEEE Trans. Imag. Proc.  2014. Data at http://www.escience.cn/people/
BingLi/Data_TIP14.html.

[19] S. Bianco  C. Cusano  and R. Schettini. Color constancy using cnns. arXiv:1504.04548[cs.CV]  2015.

[20] D. Forsyth. A novel algorithm for color constancy. IJCV  1990.

[21] W. Xiong and B. Funt.

Estimating illumination chromaticity via support vector

regression.

J. Imag. Sci. Technol.  2006.

[22] A. Gijsenij  T. Gevers  and J. van de Weijer. Computational color constancy: Survey and experiments.

IEEE Trans. Image Proc.  2011. Data at http://www.colorconstancy.com/.

9

,Ayan Chakrabarti
Mark Bun
Gautam Kamath
Thomas Steinke
Steven Wu