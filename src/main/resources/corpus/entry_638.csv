2012,Augment-and-Conquer Negative Binomial Processes,By developing data augmentation methods unique to the negative binomial (NB) distribution  we unite seemingly disjoint count and mixture models  under the  NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the  gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization  highlighting its unique theoretical  structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling  with connections to existing algorithms  showing the importance of inferring both the NB dispersion and probability parameters.,Augment-and-Conquer Negative Binomial Processes

Mingyuan Zhou

Lawrence Carin

Dept. of Electrical and Computer Engineering

Dept. of Electrical and Computer Engineering

Duke University  Durham  NC 27708

mz1@ee.duke.edu

Duke University  Durham  NC 27708

lcarin@ee.duke.edu

Abstract

By developing data augmentation methods unique to the negative binomial (NB)
distribution  we unite seemingly disjoint count and mixture models under the NB
process framework. We develop fundamental properties of the models and derive
efﬁcient Gibbs sampling inference. We show that the gamma-NB process can
be reduced to the hierarchical Dirichlet process with normalization  highlighting
its unique theoretical  structural and computational advantages. A variety of NB
processes with distinct sharing mechanisms are constructed and applied to topic
modeling  with connections to existing algorithms  showing the importance of
inferring both the NB dispersion and probability parameters.

1

Introduction

There has been increasing interest in count modeling using the Poisson process  geometric process
[1  2  3  4] and recently the negative binomial (NB) process [5  6]. Notably  it has been independently
shown in [5] and [6] that the NB process  originally constructed for count analysis  can be naturally
applied for mixture modeling of grouped data x1 ···   xJ  where each group xj = {xji}i=1 Nj .
For a territory long occupied by the hierarchical Dirichlet process (HDP) [7] and related models 
the inference of which may require substantial bookkeeping and suffer from slow convergence [7] 
the discovery of the NB process for mixture modeling can be signiﬁcant. As the seemingly distinct
problems of count and mixture modeling are united under the NB process framework  new opportu-
nities emerge for better data ﬁtting  more efﬁcient inference and more ﬂexible model constructions.
However  neither [5] nor [6] explore the properties of the NB distribution deep enough to achieve
fully tractable closed-form inference. Of particular concern is the NB dispersion parameter  which
was simply ﬁxed or empirically set [6]  or inferred with a Metropolis-Hastings algorithm [5]. Under
these limitations  both papers fail to reveal the connections of the NB process to the HDP  and thus
may lead to false assessments on comparing their modeling abilities.
We perform joint count and mixture modeling under the NB process framework  using completely
random measures [1  8  9] that are simple to construct and amenable for posterior computation.
We propose to augment-and-conquer the NB process: by “augmenting” a NB process into both
the gamma-Poisson and compound Poisson representations  we “conquer” the uniﬁcation of count
and mixture modeling  the analysis of fundamental model properties  and the derivation of efﬁcient
Gibbs sampling inference. We make two additional contributions: 1) we construct a gamma-NB
process  analyze its properties and show how its normalization leads to the HDP  highlighting its
unique theoretical  structural and computational advantages relative to the HDP. 2) We show that
a variety of NB processes can be constructed with distinct model properties  for which the shared
random measure can be selected from completely random measures such as the gamma  beta  and
beta-Bernoulli processes; we compare their performance on topic modeling  a typical example for
mixture modeling of grouped data  and show the importance of inferring both the NB dispersion and
probability parameters  which respectively govern the overdispersion level and the variance-to-mean
ratio in count modeling.

1

1.1 Poisson process for count and mixture modeling
Before introducing the NB process  we ﬁrst illustrate how the seemingly distinct problems of count
and mixture modeling can be united under the Poisson process. Denote Ω as a measure space and
for each Borel set A ⊂ Ω  denote Xj(A) as a count random variable describing the number of obser-
vations in xj that reside within A. Given grouped data x1 ···   xJ  for any measurable disjoint par-
tition A1 ···   AQ of Ω  we aim to jointly model the count random variables {Xj(Aq)}. A natural
choice would be to deﬁne a Poisson process Xj ∼ PP(G)  with a shared completely random mea-
q=1 G(Aq) and

sure G on Ω  such that Xj(A) ∼ Pois(cid:0)G(A)(cid:1) for each A ⊂ Ω. Denote G(Ω) =(cid:80)Q
(cid:101)G = G/G(Ω). Following Lemma 4.1 of [5]  the joint distributions of Xj(Ω)  Xj(A1) ···   Xj(AQ)
[Xj(A1) ···   Xj(Aq)] ∼ Mult(cid:0)Xj(Ω);(cid:101)G(A1) ···  (cid:101)G(AQ)(cid:1). (2)

Xj(Aq) ∼ Pois(cid:0)G(Aq)(cid:1);

are equivalent under the following two expressions:

Xj(Ω) =(cid:80)Q

Xj(Ω) ∼ Poisson(G(Ω)) 

q=1 Xj(Aq) 

(1)

Thus the Poisson process provides not only a way to generate independent counts from each Aq 
but also a mechanism for mixture modeling  which allocates the observations into any measurable

disjoint partition {Aq}1 Q of Ω  conditioning on Xj(Ω) and the normalized mean measure (cid:101)G.
both. Note that (cid:101)G = G/G(Ω) now becomes a Dirichlet process (DP) as (cid:101)G ∼ DP(γ0 (cid:101)G0)  where
γ0 = G0(Ω) and (cid:101)G0 = G0/γ0. The normalized gamma representation of the DP is discussed in

To complete the model  we may place a gamma process [9] prior on the shared measure as
G ∼ GaP(c  G0)  with concentration parameter c and base measure G0  such that G(A) ∼
Gamma(G0(A)  1/c) for each A⊂ Ω  where G0 can be continuous  discrete or a combination of

[10  11  9] and has been used to construct the group-level DPs for an HDP [12]. The Poisson process
has an equal-dispersion assumption for count modeling. As shown in (2)  the construction of Poisson
processes with a shared gamma process mean measure implies the same mixture proportions across
groups  which is essentially the same as the DP when used for mixture modeling when the total
counts {Xj(Ω)}j are not treated as random variables. This motivates us to consider adding an ad-
ditional layer or using a different distribution other than the Poisson to model the counts. As shown
below  the NB distribution is an ideal candidate  not only because it allows overdispersion  but also
because it can be augmented into both a gamma-Poisson and a compound Poisson representations.
2 Augment-and-Conquer the Negative Binomial Distribution
The NB distribution m ∼ NB(r  p) has the probability mass function (PMF) fM (m) = Γ(r+m)
m!Γ(r) (1−
p)rpm. It has a mean µ = rp/(1−p) smaller than the variance σ2 = rp/(1 − p)2 = µ+r−1µ2  with
the variance-to-mean ratio (VMR) as (1−p)−1 and the overdispersion level (ODL  the coefﬁcient of
the quadratic term in σ2) as r−1. It has been widely investigated and applied to numerous scientiﬁc
studies [13  14  15]. The NB distribution can be augmented into a gamma-Poisson construction as
m ∼ Pois(λ)  λ ∼ Gamma (r  p/(1 − p))  where the gamma distribution is parameterized by its
shape r and scale p/(1 − p). It can also be augmented under a compound Poisson representation
t=1 ut  ut ∼ Log(p)  l ∼ Pois(−r ln(1 − p))  where u ∼ Log(p) is the logarithmic
distribution [17] with probability-generating function (PGF) CU (z) = ln(1 − pz)/ln(1 − p) 
|z| <
p−1. In a slight abuse of notation  but for added conciseness  in the following discussion we use

[16] as m =(cid:80)l
m ∼(cid:80)l

t=1 Log(p) to denote m =(cid:80)l

t=1 ut  ut ∼ Log(p).

The inference of the NB dispersion parameter r has long been a challenge [13  18  19]. In this paper 
we ﬁrst place a gamma prior on it as r ∼ Gamma(r1  1/c1). We then use Lemma 2.1 (below) to
infer a latent count l for each m ∼ NB(r  p) conditioning on m and r. Since l ∼ Pois(−r ln(1− p))
by construction  we can use the gamma Poisson conjugacy to update r. Using Lemma 2.2 (below) 
we can further infer an augmented latent count l(cid:48) for each l  and then use these latent counts to
update r1  assuming r1 ∼ Gamma(r2  1/c2). Using Lemmas 2.1 and 2.2  we can continue this
process repeatedly  suggesting that we may build a NB process to model data that have subgroups
within groups. The conditional posterior of the latent count l was ﬁrst derived by us but was not
given an analytical form [20]. Below we explicitly derive the PMF of l  shown in (3)  and ﬁnd that
it exactly represents the distribution of the random number of tables occupied by m customers in a
Chinese restaurant process with concentration parameter r [21  22  7]. We denote l ∼ CRT(m  r)
as a Chinese restaurant table (CRT) count random variable with such a PMF and as proved in the

supplementary material  we can sample it as l =(cid:80)m

n=1 bn  bn ∼ Bernoulli (r/(n − 1 + r)).

2

n!

n=j

s(n j)xn

C (m)
Wj

Pr(l = j|m  r) = Γ(r)

the conditional posterior of l has PMF

Γ(m+r)|s(m  j)|rj  j = 0  1 ···   m.

random variables  the PGF of wj becomes CWj (z) = C j

Both the gamma-Poisson and compound-Poisson augmentations of the NB distribution and Lemmas
2.1 and 2.2 are key ingredients of this paper. We will show that these augment-and-concur methods
not only unite count and mixture modeling and provide efﬁcient inference  but also  as shown in
Section 3  let us examine the posteriors to understand fundamental properties of the NB processes 
clearly revealing connections to previous nonparametric Bayesian mixture models.
Lemma 2.1. Denote s(m  j) as Stirling numbers of the ﬁrst kind [17]. Augment m ∼ NB(r  p)
t=1 Log(p)  l ∼ Pois(−r ln(1 − p))  then

(3)
t=1 Log(p)  j = 1 ···   m. Since wj is the summation of j iid Log(p)
U (z) = [ln(1 − pz)/ln(1 − p)]j   |z| <
[17]  we have Pr(wj = m) =
(0)/m! = (−1)mpjj!s(m  j)/(m![ln(1 − p)]j). Thus for 0 ≤ j ≤ m  we have Pr(L =
j=0 |s(m  j)|rj  we

under the compound Poisson representation as m ∼(cid:80)l
Proof. Denote wj ∼ (cid:80)j
p−1. Using the property that [ln(1 + x)]j = j!(cid:80)∞
j|m  r) ∝ Pr(wj = m)Pois(j;−r ln(1−p)) ∝ |s(m  j)|rj. Denote Sr(m) =(cid:80)m
have Sr(m) = (m−1+r)Sr(m−1) = ··· =(cid:81)m−1
t=1 Log(p)  l ∼(cid:80)l(cid:48)
Proof. Augmenting m leads to m ∼(cid:80)l

Lemma 2.2. Let m ∼ NB(r  p)  r ∼ Gamma(r1  1/c1)  denote p(cid:48) =
be generated from a compound distribution as

t(cid:48)=1 Log(p(cid:48))  l(cid:48) ∼ Pois(−r1 ln(1 − p(cid:48))).
(4)
t=1 Log(p)  l ∼ Pois(−r ln(1 − p)). Marginalizing out r

n=1 (r +n)Sr(1) =(cid:81)m−1

n=0 (r +n) = Γ(m+r)
− ln(1−p)
c1−ln(1−p)   then m can also

leads to l ∼ NB (r1  p(cid:48)). Augmenting l using its compound Poisson representation leads to (4).
3 Gamma-Negative Binomial Process
We explore sharing the NB dispersion across groups while the probability parameters are group
dependent. We deﬁne a NB process X ∼ NBP(G  p) as X(A) ∼ NB(G(A)  p) for each A ⊂ Ω and
construct a gamma-NB process for joint count and mixture modeling as Xj ∼ NBP(G  pj)  G ∼
GaP(c  G0)  which can be augmented as a gamma-gamma-Poisson process as
(5)
In the above PP(·) and GaP(·) represent the Poisson and gamma processes  respectively  as deﬁned
in Section 1.1. Using Lemma 2.2  the gamma-NB process can also be augmented as
t=1 Log(pj)  Lj ∼ PP(−G ln(1 − pj))  G ∼ GaP(c  G0);

Xj ∼ PP(Λj)  Λj ∼ GaP((1 − pj)/pj  G)  G ∼ GaP(c  G0).

m ∼(cid:80)l

(6)

Γ(r)

.

(cid:1).

j ln(1−pj )
j ln(1−pj ) .

t=1 Log(p(cid:48))  L(cid:48) ∼ PP(−G0 ln(1 − p(cid:48)))  p(cid:48) =

L(cid:48)(Ω)|L  G0 =(cid:80)

Lj|Xj  G ∼ CRTP(Xj  G)  L(cid:48)|L  G0 ∼ CRTP(L  G0).

(8)
ω∈A T (ω)  T (ω) ∼ CRT(X(ω)  G(ω))

Λj|G  Xj  pj ∼ GaP(cid:0)1/pj  G + Xj

(7)
These three augmentations allow us to derive a sequence of closed-form update equations for infer-
ence with the gamma-NB process. Using the gamma Poisson conjugacy on (5)  for each A ⊂ Ω  we
have Λj(A)|G  Xj  pj ∼ Gamma (G(A) + Xj(A)  pj)  thus the conditional posterior of Λj is

Deﬁne T ∼ CRTP(X  G) as a CRT process that T (A) =(cid:80)
ω∈Ω δ(L(ω) > 0) =(cid:80)
K ) ≥ 1 if(cid:80)
γ0|{L(cid:48)(Ω)  p(cid:48)} ∼ Gamma(cid:0)e0 + L(cid:48)(Ω) 
G|G0 {Lj  pj} ∼ GaP(cid:0)c −(cid:80)

for each A ⊂ Ω. Applying Lemma 2.1 on (6) and (7)  we have
(9)
If G0 is a continuous base measure and γ0 = G0(Ω) is ﬁnite  we have G0(ω)→ 0 ∀ ω ∈ Ω and thus
(10)
if G0 is discrete as G0 =
which is equal to K +  the total number of used discrete atoms;
j Xj(ωk) > 0  thus L(cid:48)(Ω) ≥ K +. In
either case  let γ0 ∼ Gamma(e0  1/f0)  with the gamma Poisson conjugacy on (6) and (7)  we have
(11)
(12)
Since the data {xji}i are exchangeable within group j  the predictive distribution of a point Xji 
conditioning on X−i

j ln(1 − pj)  G0 +(cid:80)

j = {Xjn}n:n(cid:54)=i and G  with Λj marginalized out  can be expressed as
Xji|G  X−i

K δωk  then L(cid:48)(ωk) = CRT(L(ωk)  γ0

ω∈Ω δ((cid:80)

j Xj(ω) > 0)

(cid:80)K

f0−ln(1−p(cid:48))

(cid:1).

(cid:1);

−i
j ∼ E[Λj|G X
j
E[Λj (Ω)|G X

Xj ∼(cid:80)Lj
j Lj ∼(cid:80)L(cid:48)

L =(cid:80)

−(cid:80)
c−(cid:80)

=

G(Ω)+Xj (Ω)−1 +

G(Ω)+Xj (Ω)−1 .

G

γ0

k=1

j Lj

−i
X
j

]
−i
j

]

3

1

(13)

3.1 Relationship with the hierarchical Dirichlet process
Using the equivalence between (1) and (2) and normalizing all the gamma processes in (5)  denoting

(cid:101)Λj = Λj/Λj(Ω)  α = G(Ω)  (cid:101)G = G/α  γ0 = G0(Ω) and (cid:101)G0 = G0/γ0  we can re-express (5) as

Xji ∼(cid:101)Λj  (cid:101)Λj ∼ DP(α (cid:101)G)  α ∼ Gamma(γ0  1/c)  (cid:101)G ∼ DP(γ0 (cid:101)G0)

(14)
which is an HDP [7]. Thus the normalized gamma-NB process leads to an HDP  yet we can-
not return from the HDP to the gamma-NB process without modeling Xj(Ω) and Λj(Ω) as ran-
dom variables. Theoretically  they are distinct in that the gamma-NB process is a completely
random measure  assigning independent random variables into any disjoint Borel sets {Aq}1 Q
of Ω; whereas the HDP is not. Practically  the gamma-NB process can exploit conjugacy to
achieve analytical conditional posteriors for all latent parameters. The inference of the HDP is
a major challenge and it is usually solved through alternative constructions such as the Chinese
restaurant franchise (CRF) and stick-breaking representations [7  23].
In particular  without an-
alytical conditional posteriors  the inference of concentration parameters α and γ0 is nontrivial
[7  24] and they are often simply ﬁxed [23]. Under the CRF metaphor α governs the random
number of tables occupied by customers in each restaurant independently; further  if the base

probability measure (cid:101)G0 is continuous  γ0 governs the random number of dishes selected by ta-
However  if (cid:101)G0 is discrete as (cid:101)G0 = (cid:80)K

bles of all restaurants. One may apply the data augmentation method of [22] to sample α and γ0.
K δωk  which is of practical value and becomes a con-
tinuous base measure as K → ∞ [11  7  24]  then using the method of [22] to sample γ0 is only
approximately correct  which may result in a biased estimate in practice  especially if K is not large
enough. By contrast  in the gamma-NB process  the shared gamma process G can be analytically
updated with (12) and G(Ω) plays the role of α in the HDP  which is readily available as

k=1

1

G(Ω)|G0 {Lj  pj}j=1 N ∼ Gamma

(cid:16)

γ0 +(cid:80)

c−(cid:80)

j Lj(Ω) 

1
j ln(1−pj )

(15)

(cid:17)

k=1

and as in (11)  regardless of whether the base measure is continuous  the total mass γ0 has an analyt-
ical gamma posterior whose shape parameter is governed by L(cid:48)(Ω)  with L(cid:48)(Ω) = K + if G0 is con-
γ0
K δωk. Equation (15) also intuitively shows how

tinuous and ﬁnite and L(cid:48)(Ω) ≥ K + if G0 =(cid:80)K
the NB probability parameters {pj} govern the variations among {(cid:101)Λj} in the gamma-NB process.

In the HDP  pj is not explicitly modeled  and since its value becomes irrelevant when taking the nor-
malized constructions in (14)  it is usually treated as a nuisance parameter and perceived as pj = 0.5
when needed for interpretation purpose. Fixing pj = 0.5 is also considered in [12] to construct an
HDP  whose group-level DPs are normalized from gamma processes with the scale parameters as
pj
= 1; it is also shown in [12] that improved performance can be obtained for topic modeling by
1−pj
learning the scale parameters with a log Gaussian process prior. However  no analytical conditional
posteriors are provided and Gibbs sampling is not considered as a viable option [12].
3.2 Augment-and-conquer inference for joint count and mixture modeling
Since the Poisson intensity ν+ = ν(R+×Ω) = ∞ and(cid:82)(cid:82)
For a ﬁnite continuous base measure  the gamma process G ∼ GaP(c  G0) can also be deﬁned
with its L´evy measure on a product space R+ × Ω  expressed as ν(drdω) = r−1e−crdrG0(dω) [9].
process can be expressed as G =(cid:80)∞
Here we consider a discrete base measure as G0 =(cid:80)K
R+×Ω rν(drdω) is ﬁnite  a draw from this
k=1 rkδωk   (rk  ωk) ∼ π(drdω)  π(drdω)ν+ ≡ ν(drdω) [9].
(cid:80)K
K δωk   ωk ∼ g0(ωk)  then we have G =
k=1 rkδωk  rk ∼ Gamma(γ0/K  1/c)  ωk ∼ g0(ωk)  which becomes a draw from the gamma
linked to a mixture component ωzji ∈ Ω through a distribution F . Denote njk =(cid:80)Nj
process with a continuous base measure as K → ∞. Let xji ∼ F (ωzji) be observation i in group j 
i=1 δ(zji = k) 

k=1

γ0

we can express the gamma-NB process with the discrete base measure as

k=1 njk  njk ∼ Pois(λjk)  λjk ∼ Gamma(rk  pj/(1 − pj))

rk ∼ Gamma(γ0/K  1/c)  pj ∼ Beta(a0  b0)  γ0 ∼ Gamma(e0  1/f0)

(16)
where marginally we have njk ∼ NB(rk  pj). Using the equivalence between (1) and (2)  we
can equivalently express Nj and njk in the above model as Nj ∼ Pois (λj)   [nj1 ···   njK] ∼
k=1 λjk. Since the data {xji}i=1 Nj are fully
exchangeable  rather than drawing [nj1 ···   njK] once  we may equivalently draw the index

Mult (Nj; λj1/λj ···   λjK/λj)  where λj = (cid:80)K

ωk ∼ g0(ωk)  Nj =(cid:80)K

zji ∼ Discrete (λj1/λj ···   λjK/λj)

(17)

4

i=1 δ(zji = k). This provides further insights on how the seem-
ingly disjoint problems of count and mixture modeling are united under the NB process framework.
Following (8)-(12)  the block Gibbs sampling is straightforward to write as

for each xji and then let njk =(cid:80)Nj

(pj|−) ∼ Beta

p(ωk|−) ∝(cid:81)
(cid:18)
a0 + Nj  b0 +(cid:80)
k|−) ∼ CRT((cid:80)
(cid:18)
γ0/K +(cid:80)

(l(cid:48)

(rk|−) ∼ Gamma

(cid:19)

  p(cid:48) =

k rk

e0 +(cid:80)

zji=k F (xji; ωk)g0(ωk)  Pr(zji = k|−) ∝ F (xji; ωk)λjk

−(cid:80)
c−(cid:80)
(cid:18)
(cid:19)
j ln(1−pj )
j ln(1−pj )   (ljk|−) ∼ CRT(njk  rk)
(cid:19)
j ljk  γ0/K)  (γ0|−) ∼ Gamma
k|−) = δ((cid:80)

j ljk > 0) = δ((cid:80)

k l(cid:48)
k 

1

f0−ln(1−p(cid:48))

  (λjk|−) ∼ Gamma(rk + njk  pj). (18)

c−(cid:80)

j ljk 

1
j ln(1−pj )

which has similar computational complexity as that of the direct assignment block Gibbs sampling
of the CRF-HDP [7  24]. If g0(ω) is conjugate to the likelihood F (x; ω)  then the posterior p(ω|−)
would be analytical. Note that when K → ∞  we have (l(cid:48)
j njk > 0).
Using (1) and (2) and normalizing the gamma distributions  (16) can be re-expressed as

zji ∼ Discrete(˜λj)  ˜λj ∼ Dir(α˜r)  α ∼ Gamma(γ0  1/c)  ˜r ∼ Dir(γ0/K ···   γ0/K) (19)
which loses the count modeling ability and becomes a ﬁnite representation of the HDP  the inference
of which is not conjugate and has to be solved under alternative representations [7  24]. This also
implies that by using the Dirichlet process as the foundation  traditional mixture modeling may
discard useful count information from the beginning.
4 The Negative Binomial Process Family and Related Algorithms
The gamma-NB process shares the NB dispersion across groups. Since the NB distribution has two
adjustable parameters  we may explore alternative ideas  with the NB probability measure shared
across groups as in [6]  or with both the dispersion and probability measures shared as in [5]. These
constructions are distinct from both the gamma-NB process and HDP in that Λj has space dependent

scales  and thus its normalization(cid:101)Λj = Λj/Λj(Ω) no longer follows a Dirichlet process.
can be expressed as B =(cid:80)∞
NBP(rj  B)  with a random draw expressed as Xj = (cid:80)∞
k=1(rk  pk)δωk  and the NB process Xj ∼ NBP(R  B) becomes Xj = (cid:80)∞
(cid:80)∞
Zj ∼ BeP(B) is drawn from the Bernoulli process [26] and (R  B) =(cid:80)∞

It is natural to let the probability measure be drawn from a beta process [25  26]  which can be
deﬁned by its L´evy measure on a product space [0  1]× Ω as ν(dpdω) = cp−1(1− p)c−1dpB0(dω).
A draw from the beta process B ∼ BP(c  B0) with concentration parameter c and base measure B0
k=1 pkδωk . A beta-NB process [5  6] can be constructed by letting Xj ∼
k=1 njkδωk   njk ∼ NB(rj  pk). Under
this construction  the NB probability measure is shared and the NB dispersion parameters are group
dependent. As in [5]  we may also consider a marked-beta-NB1 process that both the NB probability
and dispersion measures are shared  in which each point of the beta process is marked with an
independent gamma random variable. Thus a draw from the marked-beta process becomes (R  B) =
k=1 njkδωk   njk ∼
NB(rk  pk). Since the beta and NB processes are conjugate  the posterior of B is tractable  as shown
in [5  6]. If it is believed that there are excessive number of zeros  governed by a process other
than the NB process  we may introduce a zero inﬂated NB process as Xj ∼ NBP(RZj  pj)  where
k=1(rk  πk)δωk is drawn
from a marked-beta process  thus njk ∼ NB(rkbjk  pj)  bjk = Bernoulli(πk). This construction
can be linked to the model in [27] with appropriate normalization  with advantages that there is no
need to ﬁx pj = 0.5 and the inference is fully tractable. The zero inﬂated construction can also be
linked to models for real valued data using the Indian buffet process (IBP) or beta-Bernoulli process
spike-and-slab prior [28  29  30  31].
4.1 Related Algorithms
To show how the NB processes can be diversely constructed and to make connections to previous
parametric and nonparametric mixture models  we show in Table 1 a variety of NB processes  which
differ on how the dispersion and probability measures are shared. For a deeper understanding on
how the counts are modeled  we also show in Table 1 both the VMR and ODL implied by these

1We may also consider a beta marked-gamma-NB process  whose performance is found to be very similar.

5

Table 1: A variety of negative binomial processes are constructed with distinct sharing mechanisms  reﬂected
with which parameters from rk  rj  pk  pj and πk (bjk) are inferred (indicated by a check-mark (cid:88))  and the
implied VMR and ODL for counts {njk}j k. They are applied for topic modeling of a document corpus  a
typical example of mixture modeling of grouped data. Related algorithms are shown in the last column.
Related Algorithms

VMR

pk

rk

rj
(cid:88)

(cid:88)
(cid:88)

(cid:88) (cid:88)

(cid:88)

πk

pj
(cid:88)
0.5
0.5 (cid:88)

(cid:88)

(1 − pj)−1

2
2

(1 − pk)−1
(1 − pj)−1
(1 − pk)−1

(rk)−1bjk

ODL
−1
r
j
−1
r
k

−1
j
−1
k
−1
k

r
r
r

Algorithms
NB-LDA
NB-HDP
NB-FTM
Beta-NB

Gamma-NB

(cid:88)
Marked-Beta-NB (cid:88)

LDA [32]  Dir-PFA [5]
HDP[7]  DILN-HDP [12]
FTM [27]  SγΓ-PFA [5]

BNBP [5]  BNBP [6]

CRF-HDP [7  24]

BNBP [5]

settings. We consider topic modeling of a document corpus  a typical example of mixture mod-
eling of grouped data  where each a-bag-of-words document constitutes a group  each word is an
exchangeable group member  and F (xji; ωk) is simply the probability of word xji in topic ωk.
We consider six differently constructed NB processes in Table 1: (i) Related to latent Dirichlet
allocation (LDA) [32] and Dirichlet Poisson factor analysis (Dir-PFA) [5]  the NB-LDA is also a
parametric topic model that requires tuning the number of topics. However  it uses a document de-
pendent rj and pj to automatically learn the smoothing of the gamma distributed topic weights  and
it lets rj ∼ Gamma(γ0  1/c)  γ0 ∼ Gamma(e0  1/f0) to share statistical strength between docu-
ments  with closed-form Gibbs sampling inference. Thus even the most basic parametric LDA topic
model can be improved under the NB count modeling framework. (ii) The NB-HDP model is re-
lated to the HDP [7]  and since pj is an irrelevant parameter in the HDP due to normalization  we set
it in the NB-HDP as 0.5  the usually perceived value before normalization. The NB-HDP model is
comparable to the DILN-HDP [12] that constructs the group-level DPs with normalized gamma pro-
cesses  whose scale parameters are also set as one. (iii) The NB-FTM model introduces an additional
beta-Bernoulli process under the NB process framework to explicitly model zero counts. It is the
same as the sparse-gamma-gamma-PFA (SγΓ-PFA) in [5] and is comparable to the focused topic
model (FTM) [27]  which is constructed from the IBP compound DP. Nevertheless  they apply about
the same likelihoods and priors for inference. The Zero-Inﬂated-NB process improves over them by
allowing pj to be inferred  which generally yields better data ﬁtting. (iv) The Gamma-NB process
explores the idea that the dispersion measure is shared across groups  and it improves over the NB-
HDP by allowing the learning of pj. It reduces to the HDP [7] by normalizing both the group-level
and the shared gamma processes. (v) The Beta-NB process explores sharing the probability measure
across groups  and it improves over the beta negative binomial process (BNBP) proposed in [6] 
allowing inference of rj. (vi) The Marked-Beta-NB process is comparable to the BNBP proposed
in [5]  with the distinction that it allows analytical update of rk. The constructions and inference
of various NB processes and related algorithms in Table 1 all follow the formulas in (16) and (18) 
respectively  with additional details presented in the supplementary material.
Note that as shown in [5]  NB process topic models can also be considered as factor analysis of
the term-document count matrix under the Poisson likelihood  with ωk as the kth factor loading
that sums to one and λjk as the factor score  which can be further linked to nonnegative matrix
factorization [33] and a gamma Poisson factor model [34]. If except for proportions ˜λj and ˜r  the
absolute values  e.g.  λjk  rk and pk  are also of interest  then the NB process based joint count and
mixture models would apparently be more appropriate than the HDP based mixture models.
5 Example Results
Motivated by Table 1  we consider topic modeling using a variety of NB processes  which differ on
which parameters are learned and consequently how the VMR and ODL of the latent counts {njk}j k
are modeled. We compare them with LDA [32  35] and CRF-HDP [7  24]. For fair comparison  they
are all implemented with block Gibbs sampling using a discrete base measure with K atoms  and
for the ﬁrst ﬁfty iterations  the Gamma-NB process with rk ≡ 50/K and pj ≡ 0.5 is used for
initialization. For LDA and NB-LDA  we search K for optimal performance and for the other
models  we set K = 400 as an upper-bound. We set the parameters as c = 1  η = 0.05 and
a0 = b0 = e0 = f0 = 0.01. For LDA  we set the topic proportion Dirichlet smoothing parameter
as 50/K  following the topic model toolbox2 provided for [35]. We consider 2500 Gibbs sampling
iterations  with the last 1500 samples collected. Under the NB processes  each word xji would

6

j

s=1

s=1

v=1

j

vk λ(s)

jk

k=1 ω(s)

vk λ(s)

Figure 1: Comparison of per-word perplexities on the held-out words between various algorithms. (a) With
60% of the words in each document used for training  the performance varies as a function of K in both LDA
and NB-LDA  which are parametric models  whereas the NB-HDP  NB-FTM  Beta-NB  CRF-HDP  Gamma-
NB and Marked-Beta-NB all infer the number of active topics  which are 127  201  107  161  177 and 130 
respectively  according to the last Gibbs sampling iteration. (b) Per-word perplexities of various models as a
function of the percentage of words in each document used for training. The results of the LDA and NB-LDA
are shown with the best settings of K under each training/testing partition.
be assigned to a topic k based on both F (xji; ωk) and the topic weights {λjk}k=1 K; each topic is
drawn from a Dirichlet base measure as ωk ∼ Dir(η ···   η) ∈ RV   where V is the number of unique
terms in the vocabulary and η is a smoothing parameter. Let vji denote the location of word xji in the
i δ(zji =

vocabulary  then we have (ωk|−) ∼ Dir(cid:0)η +(cid:80)
k  vji = V )(cid:1). We consider the Psychological Review2 corpus  restricting the vocabulary to terms
fjv = (cid:80)S
(cid:80)K

i δ(zji = k  vji = 1) ···   η +(cid:80)

(cid:80)K

(cid:80)

(cid:80)

k=1 ω(s)

(cid:80)V

(cid:14)(cid:80)S

that occur in ﬁve or more documents. The corpus includes 1281 abstracts from 1967 to 2003  with
2 566 unique terms and 71 279 total word counts. We randomly select 20%  40%  60% or 80%
of the words from each document to learn a document dependent probability for each term v as
jk   where ωvk is the probability of term v
in topic k and S is the total number of collected samples. We use {fjv}j v to calculate the per-
word perplexity on the held-out words as in [5]. The ﬁnal results are averaged from ﬁve random
training/testing partitions. Note that the perplexity per test word is the fair metric to compare topic
models. However  when the actual Poisson rates or distribution parameters for counts instead of the
mixture proportions are of interest  it is obvious that a NB process based joint count and mixture
model would be more appropriate than an HDP based mixture model.
Figure 1 compares the performance of various algorithms. The Marked-Beta-NB process has the
best performance  closely followed by the Gamma-NB process  CRF-HDP and Beta-NB process.
With an appropriate K  the parametric NB-LDA may outperform the nonparametric NB-HDP and
NB-FTM as the training data percentage increases  somewhat unexpected but very intuitive results 
showing that even by learning both the NB dispersion and probability parameters rj and pj in a
document dependent manner  we may get better data ﬁtting than using nonparametric models that
share the NB dispersion parameters rk across documents  but ﬁx the NB probability parameters.
Figure 2 shows the learned model parameters by various algorithms under the NB process frame-
work  revealing distinct sharing mechanisms and model properties. When (rj  pj) is used  as in the
NB-LDA  different documents are weakly coupled with rj ∼ Gamma(γ0  1/c)  and the modeling
results show that a typical document in this corpus usually has a small rj and a large pj  thus a large
ODL and a large VMR  indicating highly overdispersed counts on its topic usage. When (rj  pk) is
used to model the latent counts {njk}j k  as in the Beta-NB process  the transition between active
and non-active topics is very sharp that pk is either close to one or close to zero. That is because pk
j rj and the VMR as (1 − pk)−1 on topic k  thus
a popular topic must also have large pk and thus large overdispersion measured by the VMR; since
the counts {njk}j are usually overdispersed  particularly true in this corpus  a middle range pk indi-
(cid:80)
cating an appreciable mean and small overdispersion is not favored by the model and thus is rarely
observed. When (rk  pj) is used  as in the Gamma-NB process  the transition is much smoother that
j pj/(1 − pj)
and the ODL r−1
k on topic k  thus popular topics must also have large rk and thus small overdisper-
sion measured by the ODL  and unpopular topics are modeled with small rk and thus large overdis-
persion  allowing rarely and lightly used topics. Therefore  we can expect that (rk  pj) would allow

controls the mean as E[(cid:80)
rk gradually decreases. The reason is that rk controls the mean as E[(cid:80)

j njk] = pk/(1 − pk)(cid:80)

j njk] = rk

2http://psiexp.ss.uci.edu/research/programs data/toolbox.htm

7

0501001502002503003504008008509009501000105011001150120012501300K+=127K+=201K+=107K+=161K+=177K+=130(a)Number of topicsPerplexity0.20.30.40.50.60.70.8700800900100011001200130014001500Training data percentagePerplexity(b)  LDANB−LDANB−HDPNB−FTMBeta−NBCRF−HDPGamma−NBMarked−Beta−NBFigure 2: Distinct sharing mechanisms and model properties are evident between various NB processes  by
comparing their inferred parameters. Note that the transition between active and non-active topics is very sharp
when pk is used and much smoother when rk is used. Both the documents and topics are ordered in a decreasing
order based on the number of words associated with each of them. These results are based on the last Gibbs
sampling iteration. The values are shown in either linear or log scales for convenient visualization.

overdispersion would be allowed as both rk and pk are now responsible for the mean E[(cid:80)

more topics than (rj  pk)  as conﬁrmed in Figure 1 (a) that the Gamma-NB process learns 177 active
topics  signiﬁcantly more than the 107 ones of the Beta-NB process. With these analysis  we can
conclude that the mean and the amount of overdispersion (measure by the VMR or ODL) for the
usage of topic k is positively correlated under (rj  pk) and negatively correlated under (rk  pj).
When (rk  pk) is used  as in the Marked-Beta-NB process  more diverse combinations of mean and
j njk] =
Jrkpk/(1−pk). For example  there could be not only large mean and small overdispersion (large rk
and small pk)  but also large mean and large overdispersion (small rk and large pk). Thus (rk  pk)
may combine the advantages of using only rk or pk to model topic k  as conﬁrmed by the superior
performance of the Marked-Beta-NB over the Beta-NB and Gamma-NB processes. When (rk  πk)
is used  as in the NB-FTM model  our results show that we usually have a small πk and a large rk 
indicating topic k is sparsely used across the documents but once it is used  the amount of variation
on usage is small. This modeling properties might be helpful when there are excessive number of
zeros which might not be well modeled by the NB process alone. In our experiments  we ﬁnd the
more direct approaches of using pk or pj generally yield better results  but this might not be the
case when excessive number of zeros are better explained with the underlying beta-Bernoulli or IBP
processes  e.g.  when the training words are scarce.
It is also interesting to compare the Gamma-NB and NB-HDP. From a mixture-modeling viewpoint 
ﬁxing pj = 0.5 is natural as pj becomes irrelevant after normalization. However  from a count mod-
eling viewpoint  this would make a restrictive assumption that each count vector {njk}k=1 K has
the same VMR of 2  and the experimental results in Figure 1 conﬁrm the importance of learning pj
together with rk. It is also interesting to examine (15)  which can be viewed as the concentration pa-
rameter α in the HDP  allowing the adjustment of pj would allow a more ﬂexible model assumption
on the amount of variations between the topic proportions  and thus potentially better data ﬁtting.
6 Conclusions
We propose a variety of negative binomial (NB) processes to jointly model counts across groups 
which can be naturally applied for mixture modeling of grouped data. The proposed NB processes
are completely random measures that they assign independent random variables to disjoint Borel sets
of the measure space  as opposed to the hierarchical Dirichlet process (HDP) whose measures on
disjoint Borel sets are negatively correlated. We discover augment-and-conquer inference methods
that by “augmenting” a NB process into both the gamma-Poisson and compound Poisson repre-
sentations  we are able to “conquer” the uniﬁcation of count and mixture modeling  the analysis of
fundamental model properties and the derivation of efﬁcient Gibbs sampling inference. We demon-
strate that the gamma-NB process  which shares the NB dispersion measure across groups  can be
normalized to produce the HDP and we show in detail its theoretical  structural and computational
advantages over the HDP. We examine the distinct sharing mechanisms and model properties of
various NB processes  with connections to existing algorithms  with experimental results on topic
modeling showing the importance of modeling both the NB dispersion and probability parameters.
Acknowledgments
The research reported here was supported by ARO  DOE  NGA  and ONR  and by DARPA under
the MSEE and HIST programs.

8

0500100010−410−2100102NB−LDArjDocument Index050010000.20.40.60.81pjDocument Index020040010−410−2100NB−HDPrkTopic Index0500100000.51pjDocument Index02004000102030NB−FTMrkTopic Index020040010−310−210−1100πkTopic Index0500100010−410−2100102Beta−NBrjDocument Index020040000.51pkTopic Index020040010−410−2100Gamma−NBrkTopic Index0500100000.51pjDocument Index020040010−410−2100Marked−Beta−NBrkTopic Index020040000.51pkTopic IndexReferences
[1] J. F. C. Kingman. Poisson Processes. Oxford University Press  1993.
[2] M. K. Titsias. The inﬁnite gamma-Poisson feature model. In NIPS  2008.
[3] R. J. Thibaux. Nonparametric Bayesian Models for Machine Learning. PhD thesis  UC Berkeley  2008.
[4] K. T. Miller. Bayesian Nonparametric Latent Feature Models. PhD thesis  UC Berkeley  2011.
[5] M. Zhou  L. Hannah  D. Dunson  and L. Carin. Beta-negative binomial process and Poisson factor anal-

ysis. In AISTATS  2012.

[6] T. Broderick  L. Mackey  J. Paisley  and M. I. Jordan. Combinatorial clustering and the beta negative

binomial process. arXiv:1111.1802v3  2012.

[7] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. JASA  2006.
[8] M. I. Jordan. Hierarchical models  nested models and completely random measures. 2010.
[9] R. L. Wolpert  M. A. Clyde  and C. Tu. Stochastic expansions using continuous dictionaries: L´evy

Adaptive Regression Kernels. Annals of Statistics  2011.

[10] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Ann. Statist.  1973.
[11] H. Ishwaran and M. Zarepour. Exact and approximate sum-representations for the Dirichlet process. Can.

J. Statist.  2002.

[12] J. Paisley  C. Wang  and D. M. Blei. The discrete inﬁnite logistic normal distribution. Bayesian Analysis 

2012.

[13] C. I. Bliss and R. A. Fisher. Fitting the negative binomial distribution to biological data. Biometrics 

1953.

[14] A. C. Cameron and P. K. Trivedi. Regression Analysis of Count Data. Cambridge  UK  1998.
[15] R. Winkelmann. Econometric Analysis of Count Data. Springer  Berlin  5th edition  2008.
[16] M. H. Quenouille. A relation between the logarithmic  Poisson  and negative binomial series. Biometrics 

1949.

[17] N. L. Johnson  A. W. Kemp  and S. Kotz. Univariate Discrete Distributions. John Wiley & Sons  2005.
[18] S. J. Clark and J. N. Perry. Estimation of the negative binomial parameter κ by maximum quasi-likelihood.

Biometrics  1989.

[19] M. D. Robinson and G. K. Smyth. Small-sample estimation of negative binomial dispersion  with appli-

cations to SAGE data. Biostatistics  2008.

[20] M. Zhou  L. Li  D. Dunson  and L. Carin. Lognormal and gamma mixed negative binomial regression. In

ICML  2012.

[21] C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.

Ann. Statist.  1974.

[22] M. D. Escobar and M. West. Bayesian density estimation and inference using mixtures. JASA  1995.
[23] C. Wang  J. Paisley  and D. M. Blei. Online variational inference for the hierarchical Dirichlet process. In

AISTATS  2011.

[24] E. Fox  E. Sudderth  M. Jordan  and A. Willsky. Developing a tempered HDP-HMM for systems with

state persistence. MIT LIDS  TR #2777  2007.

[25] N. L. Hjort. Nonparametric Bayes estimators based on beta processes in models for life history data. Ann.

Statist.  1990.

[26] R. Thibaux and M. I. Jordan. Hierarchical beta processes and the Indian buffet process. In AISTATS 

2007.

[27] S. Williamson  C. Wang  K. A. Heller  and D. M. Blei. The IBP compound Dirichlet process and its

application to focused topic modeling. In ICML  2010.

[28] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In NIPS 

2005.

[29] M. Zhou  H. Chen  J. Paisley  L. Ren  L. Li  Z. Xing  D. Dunson  G. Sapiro  and L. Carin. Nonparametric

Bayesian dictionary learning for analysis of noisy and incomplete images. IEEE TIP  2012.

[30] M. Zhou  H. Yang  G. Sapiro  D. Dunson  and L. Carin. Dependent hierarchical beta process for image

interpolation and denoising. In AISTATS  2011.

[31] L. Li  M. Zhou  G. Sapiro  and L. Carin. On the integration of topic modeling and dictionary learning. In

ICML  2011.

[32] D. Blei  A. Ng  and M. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res.  2003.
[33] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS  2000.
[34] J. Canny. Gap: a factor model for discrete data. In SIGIR  2004.
[35] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. PNAS  2004.

9

,Vincent Dutordoir
Hugh Salimbeni
James Hensman
Marc Deisenroth
Difan Zou
Quanquan Gu