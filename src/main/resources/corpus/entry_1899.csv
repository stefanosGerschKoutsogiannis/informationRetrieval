2019,Exact sampling of determinantal point processes with sublinear time preprocessing,We study the complexity of sampling from a distribution over all index subsets of the set {1  ...  n} with the probability of a subset S proportional to the determinant of the submatrix L_S of some n x n positive semidefinite matrix L  where L_S corresponds to the entries of L indexed by S. Known as a determinantal point process (DPP)  this distribution is used in machine learning to induce diversity in subset selection. When sampling from DDPs  we often wish to sample multiple subsets S with small expected size k = E[|S|] << n from a very large matrix L  so it is important to minimize the preprocessing cost of the procedure (performed once) as well as the sampling cost (performed repeatedly). For this purpose we provide DPP-VFX  a new algorithm which  given access only to L  samples exactly from a determinantal point process while satisfying the following two properties: (1) its preprocessing cost is n poly(k)  i.e.  sublinear in the size of L  and (2) its sampling cost is poly(k)  i.e.  independent of the size of L. Prior to our results  state-of-the-art exact samplers required O(n^3) preprocessing time and sampling time linear in n or dependent on the spectral properties of L. We furthermore give a reduction which allows using our algorithm for exact sampling from cardinality constrained determinantal point processes with n poly(k) time preprocessing. Our implementation of DPP-VFX is provided at https://github.com/guilgautier/DPPy/.,Exact sampling of determinantal point processes

with sublinear time preprocessing

Michał Derezi´nski⇤
Department of Statistics

University of California  Berkeley

mderezin@berkeley.edu

Daniele Calandriello⇤

LCSL

Istituto Italiano di Tecnologia  Italy
daniele.calandriello@iit.it

Michal Valko
DeepMind Paris

valkom@deepmind.com

Abstract

We study the complexity of sampling from a distribution over all index subsets of
the set {1  ...  n} with the probability of a subset S proportional to the determinant
of the submatrix LS of some n ⇥ n positive semideﬁnite matrix L  where LS
corresponds to the entries of L indexed by S. Known as a determinantal point
process (DPP)  this distribution is used in machine learning to induce diversity in
subset selection. When sampling from DDPs  we often wish to sample multiple
subsets S with small expected size k   E[|S|] ⌧ n from a very large matrix L 
so it is important to minimize the preprocessing cost of the procedure (performed
once) as well as the sampling cost (performed repeatedly). For this purpose we
provide DPP-VFX  a new algorithm which  given access only to L  samples
exactly from a determinantal point process while satisfying the following two
properties: (1) its preprocessing cost is n · poly(k)  i.e.  sublinear in the size
of L  and (2) its sampling cost is poly(k)  i.e.  independent of the size of L.
Prior to our results  state-of-the-art exact samplers required O(n3) preprocessing
time and sampling time linear in n or dependent on the spectral properties of
L. We furthermore give a reduction which allows using our algorithm for exact
sampling from cardinality constrained determinantal point processes with n ·
poly(k) time preprocessing. Our implementation of DPP-VFX is provided at
https://github.com/guilgautier/DPPy/.

Introduction

1
Given a positive semi-deﬁnite (psd) n ⇥ n matrix L  a determinantal point process DPP(L)  also
known as an L-ensemble  is a distribution over all 2n index subsets S ✓{ 1  . . .   n} such that

Pr(S)   det(LS)
det(I + L)

 

where LS denotes the |S|⇥| S| submatrix of L with rows and columns indexed by S. Determinantal
point processes naturally appear across many scientiﬁc domains [Mac75  BLMV17  Gue83]  and
they have emerged as an important tool in machine learning [KT12] for inducing diversity in subset
selection and as a variance reduction approach. DPP sampling has been successfully applied in
core ML problems such as recommender systems [GKVM18  CZZ18  Bru18]  stochastic optimiza-
tion [ZKM17  ZÖMS19]  data summarization [CKS+18]  Gaussian processes [MS16  BRVDW19] 

⇤Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

[HKP+06  KT11]
[AGR16]
[LJS16b]
[LGD18]
[Der19]
DPP-VFX (this paper)

exact DPP

k-DPP

x

x
x

x
x
x

ﬁrst sample
n3
n · poly(k)
n2 · poly(k)
n3
n3
n · poly(k)

subsequent samples
nk2
n · poly(k)
n2 · poly(k)
poly(k · (1 + kLk))
poly(rank(L))
poly(k)

Table 1: Comparison of DPP and k-DPP algorithms using the L-ensemble representation. For a DPP 
k denotes the expected subset size. Note that k  rank(L)  n. We omit log terms for clarity.
experimental design [DW17  DWH18]  and many more. In these applications  we often wish to
efﬁciently produce many DPP samples of small expected size2 k   E[|S|] given a large matrix L.
Sometimes  the distribution is restricted to subsets of ﬁxed size |S| = k ⌧ n  denoted k-DPP(L).
[HKP+06] gave an algorithm for drawing samples from DPP(L) distributed exactly  later adapted to
k-DPP(L) by [KT11]  which can be implemented to run in polynomial time. In many applications 
however  sampling is still a computational bottleneck because the algorithm requires performing
the eigendecomposition of matrix L at the cost of O(n3). In addition to that initial cost  producing
many independent samples S1  S2  . . . at high frequency poses a challenge because the cost of each
sample is at least linear in n. Many alternative algorithms exist for both DPPs and k-DPPs to reduce
the computational cost of preprocessing and/or sampling  including many approximate and heuristic
approaches. Contrary to approximate solutions  we present an algorithm which samples exactly from
a DPP or a k-DPP with the initial preprocessing cost sublinear in the size of L and the sampling cost
independent of the size of L.
Theorem 1 For a psd n ⇥ n matrix L  let S1  S2  . . . be i.i.d. random sets from DPP(L) or from
any k-DPP(L). Then  there is an algorithm which  given access to L  returns

a) the ﬁrst subset  S1  in: n · poly(k) polylog(n) time 
b) each subsequent Si in:

poly(k) time.

We refer to this algorithm as the Very Fast and eXact DPP sampler  or DPP-VFX. Table 1 compares
DPP-VFX with other DPP and k-DPP sampling algorithms. In this comparison  we feature the
methods that provide strong accuracy guarantees. As seen from the table  our algorithm is the
ﬁrst exact sampler to achieve sublinear overall runtime. Only the approximate MCMC sampler of
[AGR16] matches our n · poly(k) complexity (and only for a k-DPP)  but for this method every next
sample is equally expensive  making it less practical when repeated sampling is needed. In fact  to
our knowledge  no other exact or approximate method (with rigorous approximation guarantees)
achieves poly(k) sampling time of the present paper.
Our method is based on a technique developed recently by [DWH18  DWH19] and later extended
by [Der19]. In this approach  we carefully downsample the index set [n] = {1  ...  n} to a sample
 = (1  ...  t) 2 [n]t that is small but still sufﬁciently larger than the expected target size k  and then
run a DPP on . As the downsampling distribution we use a regularized determinantal point process
(R-DPP)  proposed by [Der19]  which (informally) samples  with probability Pr() ⇠ det(I +eL) 
whereeL is a rescaled version of L. We can summarize this approach as follows  where |S| t ⌧ n 
The DPP algorithm proposed by [Der19] follows the same diagram  however it requires that the size
of the intermediate sample  be ⌦(rank(L)· k). This means that their method provides improvement
over [HKP+06] only when L can be decomposed as XX> for some n ⇥ r matrix X  with r ⌧ n.
However  in practice  matrix L is often only approximately low-rank  i.e.  it exhibits some form of
eigenvalue decay but it does not have a low-rank factorization. In this case  the results of [Der19] are
vacuous both in terms of the preprocessing cost and the sampling cost  in that obtaining every sample
would take ⌦(n3). We propose a different R-DPP implementation (see DPP-VFX as Algorithm 1)
2 To avoid complicating the exposition with edge cases  we assume k  1. Note that this can be always
satisﬁed without distorting the distribution by rescaling L by a constant  and is without loss of generality  as our
analysis can be trivially extended to the case 0  k < 1 with some additional notation.

eS⇠DPP! S = {i : i 2 eS}.

{1  ...  n}

⇠R-DPP
! (1  ...  t)

2

Deﬁnition 1 Given a psd matrix L  its ith -ridge leverage score (RLS) ⌧i() is the ith diagonal

where the expected size of  is O(k2). To make the algorithm efﬁcient  we use new connections
between determinantal point processes  ridge leverage scores  and Nyström approximation.
entry of L(I + L)1. The -effective dimension deff() is the sum of the leverage scores Pi ⌧i().
An important connection between RLSs and DPPs is that when S ⇠ DPP(L)  the marginal probabil-
ity of index i being sampled into S is equal to the ith 1-ridge leverage score of L  and the expected
size k of S is equal to the 1-effective dimension 

Pr(i 2 S) =⇥L(I + L)1⇤ii = ⌧i(1) 

k   E⇥|S|⇤ = trL(I + L)1 = deff(1).

Intuitively  if the marginal probability of i is high  then this index should likely make it into the
intermediate sample . This suggests that i.i.d. sampling of the indices 1  ...  t proportionally to
1-ridge leverage scores  i.e.  Pr(1 = i) / ⌧i(1)  should serve as a reasonable and cheap heuristic
for constructing . In fact  we can show that this distribution can be easily corrected by rejection
sampling to become the R-DPP that we need. Computing ridge leverage scores exactly costs O(n3) 
so instead we compute them approximately by ﬁrst constructing a Nyström approximation of L.
Deﬁnition 2 Let L be a psd matrix and C a subset of its row/column indices with size m   |C|. Then
we deﬁne the Nyström approximation of L based on C as the n ⇥ n matrixbL   (LC I)>L+
CLC I.
Here  LC I denotes an m ⇥ n matrix consisting of (entire) rows of L indexed by C and (·)+ denotes
the Moore-Penrose pseudoinverse. Since we use rejection sampling to achieve the right intermediate
distribution  the correctness of our algorithm does not depend on which Nyström approximation is
chosen. However  the subset C greatly inﬂuences the computational cost of the sampling through

multiplication and inversion involving the Nyström approximation will scale with m  and therefore a

the sample will be very high and the algorithm inefﬁcient. In this case  a slightly larger subset could

the rank ofbL and the probability of rejecting a sample. Since rank(bL) = |C|  operations such as
small subset increases efﬁciency. However  ifbL is too different from L  the probability of rejecting
improve accuracy and acceptance rate without increasing too much the cost of handlingbL. Therefore 

subset C has to be selected so that it is both small and accurately represents the matrix L. Here  we
once again rely on ridge leverage score sampling which has been effectively used for obtaining good
Nyström approximations in a number of prior works such as [AM15  CLV17  RCCR18].
While our main algorithm can sample only from the random-size DPP  and not from the ﬁxed-size
k-DPP  we present a rigorous reduction argument which lets us use our DPP algorithm to sample
exactly from a k-DPP (for any k) with a small computational overhead.

Related work Prior to our work  fast exact sampling from generic DPPs has been considered
out of reach. The ﬁrst procedure to sample general DPPs was given by [HKP+06] and even most
recent exact reﬁnements [LGD18  Der19  Pou19]  when the DPP is represented in the form of an
L-ensemble  require preprocessing that amounts to an expensive n ⇥ n matrix diagonalization at the
cost O(n3)  which is shown as the ﬁrst-sample complexity column in Table 1.
Nonetheless  there are well-known samplers for very speciﬁc DPPs that are both fast and exact  for
instance for sampling uniform spanning trees [Ald90  Bro89  PW98]  which leaves the possibility of
a more generic fast sampler open. Since the sampling from DPPs has several practical large scale
machine learning applications [KT12]  there are now a number of methods known to be able to
sample from a DPP approximately  outlined in the following paragraphs.
As DPPs can be speciﬁed by kernels (L-kernels or K-kernels)  a natural approximation strategy is
to resort to low-rank approximations [KT11  GKT12  AKFT13  LJS16a]. For example  [AKFT13]
provides approximate guarantee for the probability of any subset being sampled as a function of
eigengaps of the L-kernel. Next  [LJS16a] construct coresets approximating a given k-DPP and then
use them for sampling. In their Section 4.1  [LJS16a] show in which cases we can hope for a good
approximation. These guarantees become tight if these approximations (Nyström subspace  coresets)
are aligned with data. In our work  we aim for an adaptive approach that is able to provide a good
approximation for any DPP.
The second class of approaches are based on Markov chain Monte-Carlo [MU49] techniques [Kan13 
RK15  AGR16  LJS16b  GBV17]. There are known polynomial bounds on the mixing rates [DS91]

3

of MCMC chains with arbitrary DPPs as their limiting measure. In particular  [AGR16] showed
them for cardinality-constrained DPPs and [LJS16b] for the general case. The two chains have
mixing times which are  respectively  linear and quadratic in n (see Table 1). Unfortunately  for any
subsequent sample we need to wait until the chain mixes again.
Neither the known low-rank approximations or the known MCMC methods are able to provide
samples that are exactly distributed according to a DPP (also called perfect sampling). This is not
surprising as having scalable and exact sampling is very challenging in general. For example  methods
based on rejection sampling are always exact  but they typically scale poorly to high-dimensional data
and are adversely affected by the spikes in the distribution [EVCM16]  resulting in high rejection
rate and inefﬁciency. Surprisingly  our method is based on both low-rank approximation (a source of
inaccuracy) and rejection sampling (a common source of inefﬁciency). In the following section  we
show how to obtain a perfect DPP sampler from a Nyström approximation of the L-kernel. Then  to
guarantee efﬁciency  in Section 3 we bound the number of rejections  which is possible thanks to the
use of intermediate downsampling.

2 Exact sampling using any Nyström approximation
Notation We use [n] to denote the set {1  . . .   n}. For a matrix B 2 Rn⇥m and index sets C  D 
we use BC D to denote the submatrix of B consisting of the intersection of rows indexed by C with
columns indexed by D. If C = D  we use a shorthand BC and if D = [m]  we may write BC I.
Finally  we also allow C  D to be multisets or sequences  in which case each row/column is duplicated
in the matrix according to its multiplicity (and in the case of sequences  we order the rows/columns
as they appear in the sequence). Note that with this notation if L = BB> then LC D = BC IB>D I
.
As discussed in the introduction  our method relies on an intermediate downsampling distribution
to reduce the size of the problem. The exactness of our sampler relies on the careful choice of that
intermediate distribution. To that end  we use regularized determinantal processes  introduced by
[Der19]. In the below deﬁnition  we adapt them to the kernel setting.

Deﬁnition 3 Given an n⇥ n psd matrix L  distribution p   (p1  . . .   pn) and r > 0  leteL denote an
n ⇥ n matrix such thateLij   1
over events A ✓S1k=0[n]k  where

Lij for all i  j 2 [n]. We deﬁne R-DPPr

p(L) as a distribution

rppipj

Pr(A)   E⇥1[2A] det(I +eL)⇤

det(I + L)

 

for  = (1  . . .   t) i.i.d.⇠ p 

t ⇠ Poisson(r).

if  ⇠ R-DPPr

p(L)

and S ⇠ DPP(eL)

calculation shows that the R-DPP can be used as an intermediate distribution in our algorithm without
introducing any distortion in the sampling.

Since the term det(I +eL) has the same form as the normalization constant of DPP(eL)  an easy
Proposition 1 (Der19  Theorem 8) For any L  p  r  andeL deﬁned as in Deﬁnition 3 
then {i : i2 S}⇠ DPP(L).
To sample from the R-DPP  our algorithm uses rejection sampling  where the proposal distribution is
sampling i.i.d. proportionally to the approximate 1-ridge leverage scores li ⇡ ⌧i(1) (see Deﬁnition 1
and the following discussion)  computed using any Nyström approximationbL of matrix L. Apart
frombL  the algorithm also requires an additional parameter q  which controls the size of the interme-
not depend on the choice ofbL and q  as demonstrated in the following result. The key part of the
Theorem 2 Given a psd matrix L  any one of its Nyström approximations bL and any positive q 
DPP-VFX (Algorithm 1) returns S ⇠ DPP(L).
An important implication of Theorem 2 is that even though the choice of bL affects the overall

proof involves showing that the acceptance probability in Line 6 is bounded by 1. Here  we obtain a
considerably tighter bound than the one achieved by [Der19]  which allows us to use a much smaller
intermediate sample  (see Section 3) while maintaning the efﬁciency of rejection sampling.

execution of the algorithm  it does not affect the distribution of the output. Therefore we can reuse

diate sample. Because of rejection sampling and Proposition 1  the correctness of the algorithm does

4

Algorithm 1 DPP-VFX sampling S ⇠ DPP(L)
Input: L 2 Rn⇥n  its Nyström approximationbL  q > 0
1: Compute li =⇥(L bL) +bL(I +bL)1⇤ii ⇡ Pr(i 2 S)
qh 1plilj
z = trbL(I +bL)1  eL = s
2: Initialize s =Pi li 
sample t ⇠ Poisson(q es/q)
sample 1  . . .  t
sample Acc ⇠Bernoulli⇣ ez det(I+eL)
ets/q det(I+bL)⌘

6:
7: until Acc = true

3: repeat
4:
5:

  · · ·   ln
s ) 

i.i.d.⇠ ( l1

s

Lijiij

8: sample eS ⇠ DPPeL
9: return S = {i : i2eS}
the same bL to produce multiple independent samples S1  S2  ... ⇠ DPP(L)  which we prove in
Appendix A. This is particularly important because  as we will see in Section 3  DPP-VFX can
generate successive samples S2  S3  . . . much more cheaply than the ﬁrst sample S1.
Lemma 1 Let C ✓ [n] be a random set variable with any distribution. Suppose that S1 and S2
are returned by two executions of DPP-VFX  both using inputs constructed from the same L and
bL = LI CL+
As a result  the deﬁnition ofeL (Line 2) is not exactly the one that would be suggested by Deﬁnition 3.

In particular  DPP-VFX uses q es/q instead of just q as the mean parameter for t. The extra es/q
factor is necessary to correct for the rejection sampling step in Line 6 during the sampling loop  see
the proof below.

Before proceeding with the proof  we highlight for clarity that the Poisson r.v. t in DPP-VFX
(Algorithm 1  Line 4) has a different role than the the Poisson r.v. used in the deﬁnition of R-DPPs.

CLC I. Then S1 and S2 are (unconditionally) independent.

Proof of Theorem 2 We start by showing that the Bernoulli probability in Line 6 is bounded by 1.
Note that this is important not only to sample correctly  but also when we later establish the efﬁciency
of the algorithm. If we showed a weaker upper bound  say c > 1  we could always divide the
expression by c and retain the correctness  however it would also be c times less likely that Acc = 1.

CBC IB> = BPB >
CBC I is a projection  so that P2 = P. Let
b>i . Then  we have

SincebL is a Nyström approximation for some C ✓I   it can be written as
bL = LI CL+
CLC I = BB>C IL+
for any B such that L = BB > where P   B>C IL+
eL   eBeB > where the ith row of eB is the rescaled ith row of B  i.e.eb>i  q s
det(I +eB> IeB I)
= detI + (eB> IeB I  PB>BP)(I + PB>BP)1
 exp⇣tr(eB> IeB I  PB>BP)(I + PB>BP)1⌘
= exp✓ tXi=1

det(I +eL)
det(I +bL)

det(I + PB>BP)

qli⇥B(I + PB>BP)1B>⇤ii◆ · ez = ets/qez 

= det(I +eB> IeB I)(I + PB>BP)1

=

qli

s

where the last equality follows because

B(I + PB>BP)1B> = BI  PB>(I + BPB>)1BPB>

= L bL(I +bL)1bL = L bL +bL(I +bL)1.

5

as  is after exiting the repeat loop. It follows that

Thus  we showed that the expression in Line 6 is valid. Lete denote the random variable distributed
Pr(e 2 A) / E1[2A]
eq es/q t! · ets/q E⇥1[2A] det(I +eL) | t⇤
which shows thate ⇠ R-DPPq

ets/q det(I +bL) /
1Xt=0
ez det(I +eL)
/ Et0hE⇥1[2A] det(I +eL) | t = t0⇤i

s ). The claim follows from Proposition 1.

3 Conditions for fast sampling

for t0 ⇠ Poisson(q) 

l (L) for l = ( l1
s

  · · ·   ln

(q es/q)t

The complexity cost of DPP-VFX can be roughly summarized as follows: we pay a large one-

rejection sampling scheme which must be multiplied by the number of times we repeat the loop

time cost to precomputebL and all its associated quantities  and then we pay a smaller cost in the
until acceptance. We ﬁrst show that ifbL is sufﬁciently close to L then we will exit the loop with

high probability. We then analyze how accurate the precomputing step needs to be to satisfy this
condition. Finally  we bound the overall computational cost  which includes also the ﬁnal step where
we sample S out of the intermediate sample  using any off-the-shelf exact DPP sampler.
1) Bounding the number of rejections The following result presents the two conditions needed
for achieving efﬁcient rejection sampling in DPP-VFX. First  the Nyström approximation needs to
be accurate enough  and second  the intermediate sample size (controlled by parameter q) needs to
be ⌦(k2). This is a signiﬁcant improvement over the guarantee of [Der19] where the intermediate
sample size is ⌦(rank(L) · k)  which is only meaningful for low-rank kernels. The main novelty
in this proof comes from showing the following lower bound on the ratio of the determinants of
I + L and I +bL  whenbL is a Nyström approximation: det(I + L)/ det(I +bL)  ekz  where
z   tr(bL(I +bL)1). Remarkably  this bound exploits the fact that any Nyström approximation of
L = BB> can be written asbL = BPB>  where P is a projection matrix. Note that while our result
holds in the worst case  in deployement  the conditions onbL and on q can be considerably relaxed.
Theorem 3 If the Nyström approximationbL and the intermediate sample size parameter q satisfy
then Pr(Acc = true)  e2. Therefore  with probability 1    Algorithm 1 exits the rejection
sampling loop after at most O(log 1) iterations and  after precomputing all of the inputs  the time
complexity of the rejection sampling loop is Ok6 log 1 + log41.
P   E ez det(I +eL)
ets/q det(I +bL) =

det(I +bL)
1Xt=0
eqt!E⇥det(I +eL) | t⇤ = eqq es/q+z det(I + L)
det(I +bL)
where the last equality follows because the inﬁnite series computes the normalization constant of
R-DPPq
l (L) given in Deﬁnition 3. If s  1 then q = s2 and the inequality ex  1 + x + x2 for
x 2 [0  1] implies that q  q es/q + z = s2(1  e1/s + 1/s) + z  s  1 + z  s. On the other
hand  if q = s 2 [0  1]  then q  qes/q + z = (1 e)s + z  1 + z  s. We proceed to lower bound
the determinantal ratio. Here  let L = BB> andbL = BPB > where P is a projection matrix. Then 

Proof We ﬁrst bound the number of rejections  and then discuss the complexity of each loop iteration.
Let  be distributed as in Line 5. The probability of exiting the repeat loop at each iteration is

trL(I + L)1L bL(I + L)1bL  1

1Xt=0
det(I +bL)

E⇥ det(I +eL) | t⇤

q  max{s2  s} 

(q es/q)t
eq es/q t! ·

det(I + B>B)

eqq es/q+z

qt

ezts/q

and

=

det(I + L)

det(I +bL)

det(I + PB>BP)

= detI  (B>B  PB>BP)(I + B>B)11

 exp⇣tr(B>B  PB>BP)(I + B>B)1⌘
= exp⇣trB(I + B>B)1B>  trBP(I + B>B)1PB>⌘ 

=

 

6

where we can reformulate k = trB(I + B>B)1B> and the other elements in terms ofbL and L as

exp⇣k  trBP(I + B>B  B>B)(I + B>B)1PB>⌘
= exp⇣k  tr(bL) + trBPB>B(I + B>B)1PB>⌘
= exp⇣k  tr(bL) + trBPB>(I + BB>)1BPB>⌘
= exp⇣k  tr(bL) + trbL(I + L)1bL⌘.
det(I +bL)  exp⇣  1 + z  s + k  tr(bL) + trbL(I + L)1bL⌘ 

Putting all together 

eqq es/q+z det(I + L)

  · · ·   ln

and therefore 

eqq es/q+z det(I + L)

and using the deﬁnitions k = trL(I+L)1  s = tr(LbL+bL(bL+I)1)  and z = tr(bL(bL+I)1)
on the middle term z  s + k  tr(bL) we have
trbL(bL + I)1 + L(I + L)1  L +bL bL(bL + I)1 bL
= trL(I + L)1  L = trL(I + L)1  L(I + L)1(I + L) = trL(I + L)1L 
det(I +bL)  exp⇣  1 + trbL(I + L)1bL  trL(I + L)1L⌘ 

and we obtain our condition. Thus  with probability 1    the main loop will be repeated O(log 1)
times. We now quantify the cost of a single loop iteration. First note that since the number of samples
ti drawn from l in the ith iteration of the loop is a Poisson distributed random variable  a standard
Poisson tail bound implies that with probability 1   all iterations will satisfy ti = O(k2 + log 1).
Then  drawing the Poisson r.v. ti (Line 4) requires O(k2 + log 1) time. Drawing  from the
multinomial ( l1
s ) can be done by ﬁrst sorting the li  which is a one-time O(n log(n)) cost 
s
and then using specialized samplers which require O(1) time [BP12]. Therefore  the dominant cost
is computing the determinant of the matrix I +eL in O(t3
We separate the analysis into two steps: how much it costs to choosebL to satisfy the assumption of
Theorem 3  and how much it costs to compute everything else givenbL  see Appendix A.
Lemma 2 LetbL be constructed by sampling m = O(k3 log n
Then  with probability 1   bL satisﬁes the assumption of Theorem 3.
There exist many algorithms to sample columns proportionally to their RLSs. For example  we can
take the BLESS [RCCR18] with the following guarantee.
Proposition 2 (RCCR18  Theorem 1) There exists an algorithm that with probability 1   sam-
ples m columns proportionally to their RLSs in O(nk2 log2 n
We can now compute the remaining preprocessing costs  given a Nyström approximationbL.
Lemma 3 GivenbL with rank m  we can compute li  s  z  andeL in O(nm2 + m3) time.
3) Bounding the overall cost We can now fully characterize the computational cost.
Theorem 1 (restated for DPPs only) For a psd n⇥ n matrix L  let S1  S2 be i.i.d. random sets from
DPP(L). Denote withbL a Nyström approximation of L obtained by sampling m = O(k3 log(n/))
of its columns proportionally to their RLSs. If q  max{s2  s}  then w.p. 1    DPP-VFX returns

2) Bounding the precompute cost All that is left is to control the cost of the precomputation phase.

 ) columns proportionally to their RLS.

 + k3 log4 n

 + m) time.

i )  and the result follows.

 + k9 log3 n

 + k3 log4 n

 ) time 

a) subset S1 in: O(nk6 log2 n
b) then  S2 in: Ok6 log 1

 + log4 1

 time.

7

Discussion The above result follows from the bounds on the precompute costs  on the number of

iteratios  on the iterations cost  and on the fact that the ﬁnal DPP sampling step eS ⇠ DPPeL

requires O(t3) O ((k2 +log 1)3) time. Note however that due to the nature of rejection sampling 
as long as we exit the loop  i.e.  we accept the sample  the output of DPP-VFX is guaranteed to
follow the DPP distribution for any value of m and q. In Theorem 1 we set m = (k3 log n
 ) and
q  max{s2  s} to satisfy Theorem 3 and guarantee a constant acceptance probability in the rejection
sampling loop  but this might not be necessary or even desirable in practice. Experimentally  much
smaller values of m  starting from m =⌦( k log n
 ) seem to be sufﬁcient to accept the sample  while
at the same time a smaller m greatly reduces the preprocessing costs. In general  we recommend to
separate DPP-VFX in three phases. First  compute an accurate estimate of the RLS using off-the-
shelf algorithms in O(nk2 log2 n
 ) time. Then  sample a small number m of columns
to construct an explorativebL  and try to run DPP-VFX. If the rejection sampling loop does not
terminate sufﬁciently fast  then we can reuse the RLS estimates to compute a more accuratebL for

a larger m. Using a simple doubling schedule for m  this strategy quickly reaches a regime where
DPP-VFX is w.h.p. guaranteed to accept  resulting in faster sampling.

 + k3 log4 n

4 Reduction from DPPs to k-DPPs

We next show that with a simple extra rejection sampling step we can efﬁciently transform any exact
DPP sampler into an exact k-DPP sampler.
A common heuristic to sample S from a k-DPP is to ﬁrst sample S from a DPP  and then reject the
sample if the size of S is not exactly k. As we show in this section  the success probability of this
procedure can be improved by appropriately rescaling L by a constant factor ↵ 

sample S↵ ⇠ DPP(↵L) 

accept if |S↵| = k.

Note that rescaling the DPP by a constant ↵ as above only changes the expected size of the set S↵ 
and not its distribution. Therefore  if we accept only sets with size k  we will be sampling exactly
from our k-DPP. Moreover  if k↵ = E[|S↵|] is close to k  the success probability will improve. With
a slight abuse of notation  in the context of k-DPPs we will indicate with k the desired size of S↵ 
i.e.  the ﬁnal output size at acceptance  and with k↵ = E[|S↵|] the expected size of the scaled DPP.
While the above rejection sampling heuristic is widespread  until now there has been no proof that
this heuristic can provably succeed in few rejections We solve this open question with two new results.
First  we show that for an appropriate rescaling ↵? we only reject S↵? roughly O(pk) times. Then 
we show how to ﬁnd such an ↵? with a eO(n · poly(k)) time preprocessing step.
Theorem 4 There exists constant C > 0 such that for any rank n psd matrix L and k 2 [n]  there is
↵? > 0 with the following property: if we sample S↵? ⇠ DPP(↵?L)  then Pr(|S↵?| = k)  1
Cpk·
The proof in Appendix B. relies on a known Chernoff bound for the sample size |S↵| of a DPP. When
applied naïvely  the inequality does not offer a lower bound on the probability of any single sample
size. However we show that the probability mass is concentrated on O(pk↵) sizes. This leads to a
lower bound on the sample size with the largest probability  i.e.  the mode of the distribution. Then  it
remains to observe that for any k 2 [n] we can always ﬁnd ↵? for which k is the mode of |S↵?|. We
conclude that given ↵?  the rejection sampling scheme described above transforms any poly(k) time
DPP sampler into a poly(k) time k-DPP sampler. It remains to efﬁciently ﬁnd ↵?  which once again
relies on using a Nyström approximation of L.

Lemma 4 If k  1  then there is an algorithm that ﬁnds ↵? in eO(n · poly(k)) time.

While the existence proof of ↵? is general and based on simple unimodality  this characterization
is not sufﬁcient to control the mode of the DPP when ↵? is perturbed  as it happens during an
approximate optimization of ↵. However  for DPPs the mode can be expressed as a Poisson binomial
random variable [H+56] based on the spectrum of L  which can be controlled [D+64] using the

is necessary compared to Theorem 3  in practice the same m =⌦( k log n

approximate spectrum ofbL. While our analysis shows that in the worst case a much more accuratebL

 ) seems to sufﬁce.

8

Figure 1: First-sample cost for DPP-VFX against
other DPP samplers (n is data size).

Figure 2: Resampling cost for DPP-VFX com-
pared to the exact sampler of [HKP+06].

5 Experiments

In this section  we experimentally evaluate3 the performance of DPP-VFX and compare it to exact
sampling [HKP+06] and MCMC-based approaches [AGR16]. In particular  we are only interested
in evaluating computational performance  i.e.  how DPP-VFX and baselines scale with the size n
of the matrix L. Note that an inexact DPP sampler  e.g.  MCMC samplers or previous approximate
rejection samplers [Der19]  also had another metric to validate  i.e.  they had to empirically show that
their samples were close enough to a DPP distribution. However  Section 2 proves that DPP-VFX’s
output is distributed exactly according to the DPP and therefore it is strictly equivalent to any other
exact DPP sampler  e.g.  the Gram-Schmidt [Gil14] or dual [KT12] samplers implemented in DPPy.
We use the inﬁnite MNIST digits dataset (i.e.  MNIST8M [LCB07]) as input data  where n varies up
to 106 and d = 784. All algorithms  including DPP-VFX  are implemented in python as part of the
DPPy library [GBV19]. All experiments are carried out on a 24-core CPU and fully take advantage
of potential parallelization. For each experiment we report the mean and a 95% conﬁdence interval
over 10 runs of each algorithm’s runtime.
This ﬁrst experiment compares the time required to generate a ﬁrst sample. We consider subsets of
MNIST8M that go from n = 103 to n = 7 · 104  i.e.  the original MNIST dataset  and use an RBF
kernel with  = p3d to construct L. For the Nyström approximation we set m = 10deff(1) ⇡ 10k.
While this is much lower than the O(k3) value suggested by the theory  as we will see it is already
accurate enough to result in drastic runtime improvements over exact and MCMC. Following the
strategy of Section 4  for each algorithm we control4 the size of the output set by rescaling the input
matrix L by a constant ↵? such that E[|S↵?|] = 10. Results are reported in Figure 1.
Exact sampling is performed using eigendecomposition and DPPy’s default Gram-Schmidt [Gil14]
sampler. It is clearly cubic in n  and we could not push it beyond n = 1.5 · 104. For MCMC 
we enforce mixing by runnning the chain for nk steps  the minimum recommended by [AGR16].
However  for n = 7 · 105 the MCMC runtime is 160s and exceeds the plot’s limit  while DPP-VFX
completes in 16s  an order of magnitude faster. Moreover  DPP-VFX rarely rejects more than 10
times  and the mode of the rejections up to n = 7 · 105 is 1  i.e.  we mostly accept at the ﬁrst iteration.
Since in Figure 1  the resampling time of both exact and DPP-VFX are negligible  we investigate the
cost of a second sample  i.e.  of resampling  on a much larger subset of MNIST8M up to n = 106 
normalized to have maximum row norm equal to 1. In this case it is not possible to perform an
eigendecomposition of L  so we replace RBFs with a linear kernel  for which the input X represents
a factorization L = XX> and we use an exact dual sampler on X [KT12]. However  as Figure 2
shows  even with a special algorithm for this simpler setting the resampling process still scales with n.
On the other hand  DPP-VFX’s complexity (after preprocessing) is light years better as it scales only
with k and remains constant regardless of n.

3The code used for these experiments is available at https://github.com/LCSL/dpp-vfx.
4For simplicity we do not perform the full k-DPP rejection step  but only adjust the expected size of the set.

9

Acknowledgements
MD thanks the NSF for funding via the NSF TRIPODS program. This material is based upon work
supported by the Center for Brains  Minds and Machines (CBMM)  funded by NSF STC award
CCF-1231216  and the Italian Institute of Technology. We gratefully acknowledge the support of
NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this
research.

References

[AGR16] Nima Anari  Shayan Oveis Gharan  and Alireza Rezaei. Monte carlo markov chain
algorithms for sampling strongly rayleigh distributions and determinantal point pro-
cesses. In Vitaly Feldman  Alexander Rakhlin  and Ohad Shamir  editors  29th Annual
Conference on Learning Theory  volume 49 of Proceedings of Machine Learning
Research  pages 103–115  Columbia University  New York  New York  USA  23–26
Jun 2016. PMLR.

[AKFT13] Raja Haﬁz Affandi  Alex Kulesza  Emily Fox  and Ben Taskar. Nystrom approximation
for large-scale determinantal processes. In Carlos M. Carvalho and Pradeep Ravikumar 
editors  Proceedings of the Sixteenth International Conference on Artiﬁcial Intelligence
and Statistics  volume 31 of Proceedings of Machine Learning Research  pages 85–98 
Scottsdale  Arizona  USA  29 Apr–01 May 2013. PMLR.

[Ald90] David J Aldous. The random walk construction of uniform spanning trees and uniform

labelled trees. SIAM Journal on Discrete Mathematics  3(4):450–465  1990.

[AM15] Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel ridge regression
with statistical guarantees. In Proceedings of the 28th International Conference on
Neural Information Processing Systems  pages 775–783  Montreal  Canada  December
2015.

[BLMV17] Rémi Bardenet  Frédéric Lavancier  Xavier Mary  and Aurélien Vasseur. On a few
statistical applications of determinantal point processes. ESAIM: Procs  60:180–202 
2017.

[BP12] Karl Bringmann and Konstantinos Panagiotou. Efﬁcient sampling methods for discrete
distributions. In International Colloquium on Automata  Languages  and Programming 
pages 133–144. Springer  2012.

[Bra14] Petter Branden. Unimodality  log-concavity  real-rootedness and beyond. Handbook

of Enumerative Combinatorics  10 2014.

[Bro89] A Broder. Generating random spanning trees. In Foundations of Computer Science 

1989.  30th Annual Symposium on  pages 442–447. IEEE  1989.

[Bru18] Victor-Emmanuel Brunel. Learning signed determinantal point processes through
the principal minor assignment problem. In S. Bengio  H. Wallach  H. Larochelle 
K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information
Processing Systems 31  pages 7365–7374. Curran Associates  Inc.  2018.

[BRVDW19] David Burt  Carl Edward Rasmussen  and Mark Van Der Wilk. Rates of convergence
for sparse variational Gaussian process regression. In Kamalika Chaudhuri and Ruslan
Salakhutdinov  editors  Proceedings of the 36th International Conference on Machine
Learning  volume 97 of Proceedings of Machine Learning Research  pages 862–871 
Long Beach  California  USA  09–15 Jun 2019. PMLR.

[CCL+19] Daniele Calandriello  Luigi Carratino  Alessandro Lazaric  Michal Valko  and Lorenzo
Rosasco. Gaussian process optimization with adaptive sketching: Scalable and no
regret. In Alina Beygelzimer and Daniel Hsu  editors  Proceedings of the Thirty-Second
Conference on Learning Theory  volume 99 of Proceedings of Machine Learning
Research  pages 533–557  Phoenix  USA  25–28 Jun 2019. PMLR.

10

[CKS+18] Elisa Celis  Vijay Keswani  Damian Straszak  Amit Deshpande  Tarun Kathuria  and
Nisheeth Vishnoi. Fair and diverse DPP-based data summarization. In Jennifer Dy
and Andreas Krause  editors  Proceedings of the 35th International Conference on
Machine Learning  volume 80 of Proceedings of Machine Learning Research  pages
716–725  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018. PMLR.

[CLV17] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Distributed adaptive

sampling for kernel matrix approximation. In AISTATS  2017.

[CZZ18] Laming Chen  Guoxin Zhang  and Eric Zhou. Fast greedy map inference for determi-
nantal point process to improve recommendation diversity. In S. Bengio  H. Wallach 
H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances in
Neural Information Processing Systems 31  pages 5622–5633. Curran Associates  Inc. 
2018.

[D+64] John N Darroch et al. On the distribution of the number of successes in independent

trials. The Annals of Mathematical Statistics  35(3):1317–1321  1964.

[Der19] Michał Derezi´nski. Fast determinantal point processes via distortion-free intermediate

sampling. In Proceedings of the 32nd Conference on Learning Theory  2019.

[DS91] Persi Diaconis and Daniel Stroock. Geometric Bounds for Eigenvalues of Markov

Chains. The Annals of Applied Probability  1991.

[DW17] Michał Derezi´nski and Manfred K. Warmuth. Unbiased estimates for linear regression
via volume sampling. In Advances in Neural Information Processing Systems 30 
pages 3087–3096  Long Beach  CA  USA  December 2017.

[DWH18] Michał Derezi´nski  Manfred K. Warmuth  and Daniel Hsu. Leveraged volume sampling
for linear regression. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-
Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems
31  pages 2510–2519. Curran Associates  Inc.  2018.

[DWH19] Michał Derezi´nski  Manfred K. Warmuth  and Daniel Hsu. Correcting the bias in
least squares regression with volume-rescaled sampling. In Proceedings of the 22nd
International Conference on Artiﬁcial Intelligence and Statistics  2019.

[EVCM16] Akram Erraqabi  Michal Valko  Alexandra Carpentier  and Odalric-Ambrym Maillard.
Pliable rejection sampling. In International Conference on Machine Learning  2016.
[GBV17] Guillaume Gautier  Rémi Bardenet  and Michal Valko. Zonotope hit-and-run for
efﬁcient sampling from projection DPPs. In International Conference on Machine
Learning  2017.

[GBV19] Guillaume Gautier  Rémi Bardenet  and Michal Valko. DPPy: Sampling determinantal
point processes with Python. Journal of Machine Learning Research - Machine
Learning Open Source Software (JMLR-MLOSS)  2019.

[Gil14] Jennifer Ann Gillenwater. Approximate inference for determinantal point processes.

2014.

[GKT12] Jennifer Gillenwater  Alex Kulesza  and Ben Taskar. Discovering diverse and salient
threads in document collections. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing and Computational Natural
Language Learning  EMNLP-CoNLL ’12  pages 710–720  Stroudsburg  PA  USA 
2012. Association for Computational Linguistics.

[GKVM18] Jennifer A Gillenwater  Alex Kulesza  Sergei Vassilvitskii  and Zelda E. Mariet. Maxi-
mizing induced cardinality under a determinantal point process. In S. Bengio  H. Wal-
lach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances
in Neural Information Processing Systems 31  pages 6911–6920. Curran Associates 
Inc.  2018.

[Gue83] A Guenoche. Random spanning tree. Journal of Algorithms  4(3):214 – 220  1983.

11

[H+56] Wassily Hoeffding et al. On the distribution of the number of successes in independent

trials. The Annals of Mathematical Statistics  27(3):713–721  1956.

[HKP+06] J Ben Hough  Manjunath Krishnapur  Yuval Peres  Bálint Virág  et al. Determinantal

processes and independence. Probability surveys  3:206–229  2006.

[Kan13] Byungkon Kang. Fast determinantal point process sampling with application to
clustering. In Proceedings of the 26th International Conference on Neural Information
Processing Systems  NIPS’13  pages 2319–2327  USA  2013.

[KT11] Alex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes.
In Proceedings of the 28th International Conference on Machine Learning  pages
1193–1200  Bellevue  WA  USA  June 2011.

[KT12] Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning.

Now Publishers Inc.  Hanover  MA  USA  2012.

[LCB07] Gaëlle Loosli  Stéphane Canu  and Léon Bottou. Training invariant support vector
machines using selective sampling. In Léon Bottou  Olivier Chapelle  Dennis DeCoste 
and Jason Weston  editors  Large Scale Kernel Machines  pages 301–320. MIT Press 
Cambridge  MA.  2007.

[LGD18] Claire Launay  Bruno Galerne  and Agnès Desolneux. Exact Sampling of De-
arXiv e-prints  page

terminantal Point Processes without Eigendecomposition.
arXiv:1802.08429  Feb 2018.

[LJS16a] Chengtao Li  Stefanie Jegelka  and Suvrit Sra. Efﬁcient sampling for k-determinantal
point processes. In Arthur Gretton and Christian C. Robert  editors  Proceedings of the
19th International Conference on Artiﬁcial Intelligence and Statistics  volume 51 of
Proceedings of Machine Learning Research  pages 1328–1337  Cadiz  Spain  09–11
May 2016. PMLR.

[LJS16b] Chengtao Li  Stefanie Jegelka  and Suvrit Sra. Fast mixing markov chains for strongly
rayleigh measures  dpps  and constrained sampling.
In Proceedings of the 30th
International Conference on Neural Information Processing Systems  NIPS’16  pages
4195–4203  USA  2016. Curran Associates Inc.

[Mac75] Odile Macchi. The coincidence approach to stochastic point processes. Advances in

Applied Probability  7(1):83–122  1975.

[MS16] Zelda E. Mariet and Suvrit Sra. Kronecker determinantal point processes. In D. D. Lee 
M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural
Information Processing Systems 29  pages 2694–2702. Curran Associates  Inc.  2016.
[MU49] Nicholas Metropolis and S. Ulam. The Monte Carlo method. Journal of the American

Statistical Association  44(247):335–341  1949.

[Pou19] Jack Poulson. High-performance sampling of generic determinantal point processes.

ArXive:1905.00165v1  2019.

[PP14] Robin Pemantle and Yuval Peres. Concentration of lipschitz functionals of determinan-
tal and other strong rayleigh measures. Combinatorics  Probability and Computing 
23(1):140–160  2014.

[PW98] J G Propp and D B Wilson. How to get a perfectly random sample from a generic
Markov chain and generate a random spanning tree of a directed graph. Journal of
Algorithms  27(2):170–217  1998.

[RCCR18] Alessandro Rudi  Daniele Calandriello  Luigi Carratino  and Lorenzo Rosasco. On
fast leverage score sampling and optimal learning. In Advances in Neural Information
Processing Systems 31  pages 5672–5682. 2018.

[RK15] P Rebeschini and A Karbasi. Fast mixing for discrete point processes. In Conference

on Learning Theory  pages 1480–1500  2015.

12

[ZKM17] Cheng Zhang  Hedvig Kjellström  and Stephan Mandt. Determinantal point processes
for mini-batch diversiﬁcation. In 33rd Conference on Uncertainty in Artiﬁcial Intelli-
gence  UAI 2017  Sydney  Australia  11 August 2017 through 15 August 2017. AUAI
Press Corvallis  2017.

[ZÖMS19] Cheng Zhang  Cengiz Öztireli  Stephan Mandt  and Giampiero Salvi. Active mini-

batch sampling using repulsive point processes. In AAAI  2019.

13

,Michal Derezinski
Daniele Calandriello
Michal Valko