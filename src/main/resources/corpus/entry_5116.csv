2009,Sparse Metric Learning via Smooth Optimization,In this paper we study the problem of learning a low-dimensional (sparse)  distance matrix.  We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is  non-convex.  We then show that it can be equivalently formulated  as a convex saddle (min-max) problem. From this saddle representation  we develop an efficient smooth optimization approach for sparse metric learning although the learning model is based on a non-differential loss function. This smooth optimization approach has an optimal convergence rate of $O(1 /\ell^2)$ for smooth problems where $\ell$ is the iteration number. Finally  we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets.,Sparse Metric Learning via Smooth Optimization

Yiming Ying†  Kaizhu Huang‡  and Colin Campbell†

†Department of Engineering Mathematics  University of Bristol 

The Chinese Academy of Sciences  100190 Beijing  China

‡National Laboratory of Pattern Recognition  Institute of Automation 

Bristol BS8 1TR  United Kingdom

Abstract

In this paper we study the problem of learning a low-rank (sparse) distance ma-
trix. We propose a novel metric learning model which can simultaneously con-
duct dimension reduction and learn a distance matrix. The sparse representation
involves a mixed-norm regularization which is non-convex. We then show that
it can be equivalently formulated as a convex saddle (min-max) problem. From
this saddle representation  we develop an efﬁcient smooth optimization approach
[17] for sparse metric learning  although the learning model is based on a non-
differentiable loss function. Finally  we run experiments to validate the effective-
ness and efﬁciency of our sparse metric learning model on various datasets.

1 Introduction

For many machine learning algorithms  the choice of a distance metric has a direct impact on their
success. Hence  choosing a good distance metric remains a challenging problem. There has been
much work attempting to exploit a distance metric in many learning settings  e.g. [8  9  10  12  20 
22  23  25]. These methods have successfully indicated that a good distance metric can signiﬁcantly
improve the performance of k-nearest neighbor classiﬁcation and k-means clustering  for example.
A good choice of a distance metric generally preserves the distance structure of the data: the dis-
tance between examples exhibiting similarity should be relatively smaller  in the transformed space 
than between examples exhibiting dissimilarity. For supervised classiﬁcation  the label information
indicates whether the pair set is in the same class (similar) or in the different classes (dissimilar). In
semi-supervised clustering  the side information conveys the information that a pair of samples are
similar or dissimilar to each other. Since it is very common that the presented data is contaminated
by noise  especially for high-dimensional datasets  a good distance metric should also be minimally
inﬂuenced by noise. In this case  a low-rank distance matrix would produce a better generalization
performance than non-sparse counterparts and provide a much faster and efﬁcient distance calcula-
tion for test samples. Hence  a good distance metric should also pursue dimension reduction during
the learning process.
In this paper we present a novel approach to learn a low-rank (sparse) distance matrix. We
ﬁrst propose in Section 2 a novel metric learning model for estimating the linear transforma-
tion (equivalently distance matrix) that combines and retains the advantages of existing methods
[8  9  12  20  22  23  25]. Our method can simultaneously conduct dimension reduction and learn a
low-rank distance matrix. The sparse representation is realized by a mixed-norm regularization used
in various learning settings [1  18  21]. We then show that this non-convex mixed-norm regulariza-
tion framework is equivalent to a convex saddle (min-max) problem. Based on this equivalent rep-
resentation  we develop  in Section 3  Nesterov’s smooth optimization approach [16  17] for sparse
metric learning using smoothing approximation techniques  although the learning model is based on
a non-differentiable loss function. In Section 4  we demonstrate the effectiveness and efﬁciency of
our sparse metric learning model with experiments on various datasets.

1

2 Sparse Distance Matrix Learning Model
We begin by introducing necessary notation. Let Nn = {1  2  . . .   n} for any n ∈ N. The space
of symmetric d times d matrices will be denoted by S d. If S ∈ S d is positive deﬁnite  we write
it as S (cid:186) 0. The cone of positive semi-deﬁnite matrices is denoted by S d
+ and denote by O d
the set of d times d orthonormal matrices. For any X  Y ∈ Rd×q  (cid:104)X  Y (cid:105) := Tr(X(cid:62)Y ) where
Tr(·) denotes the trace of a matrix. The standard Euclidean norm is denoted by (cid:107) · (cid:107). Denote by
i ) ∈ Rd 
z := {(xi  yi) : i ∈ Nn} a training set of n labeled examples with input xi = (x1
class label yi (not necessary binary) and let xij = xi − xj.
Let P = (P(cid:96)k)(cid:96) k∈Nd ∈ Rd×d be a transformation matrix. Denote by ˆxi = P xi for any i ∈ Nn and
by ˆx = { ˆxi : i ∈ Nn} the transformed data matrix. The linear transformation matrix P induces a
distance matrix M = P (cid:62)P which deﬁnes a distance between xi and xj given by

i   . . .   xd

dM (xi  xj) = (xi − xj)(cid:62)M(xi − xj).

Our sparse metric learning model is based on two principal hypotheses: 1) a good choice of distance
matrix M should preserve the distance structure  i.e. the distance between similar examples should
be relatively smaller than between dissimilar examples; 2) a good distance matrix should also be
able to effectively remove noise leading to dimension reduction.
For the ﬁrst hypothesis  the distance structure in the transformed space can be speciﬁed  for example 
by the following constraints: (cid:107)P (xj − xk)(cid:107)2 ≥ (cid:107)P (xi − xj)(cid:107)2 + 1 ∀(xi  xj) ∈ S and (xj  xk) ∈
D  where S denotes the similarity pairs and D denotes the dissimilarity pairs based on the label
information. Equivalently 

(cid:107)ˆxj − ˆxk)(cid:107)2 ≥ (cid:107)ˆxi − ˆxj(cid:107)2 + 1 ∀(xi  xj) ∈ S and (xj  xk) ∈ D.

(1)
For the second hypothesis  we use a sparse regularization to give a sparse solution. This regu-
(cid:80)
larization ranges from element-sparsity for variable selection to a low-rank matrix for dimension
reduction [1  2  3  13  21]. In particular  for any (cid:96) ∈ Nd  denote the (cid:96)-th row vector of P by P(cid:96)
and (cid:107)P(cid:96)(cid:107) = (
2 . If (cid:107)P(cid:96)(cid:107) = 0 then the (cid:96)-th variable in the transformed space becomes
i = P(cid:96)xi = 0 which means that (cid:107)P(cid:96)(cid:107) = 0 has the effect of eleminating (cid:96)-th variable.
zero  i.e. x(cid:96)
Motivated by the above observation  a direct way would be to enforce a L1-norm across the vector
((cid:107)P1(cid:107)  . . .  (cid:107)Pd(cid:107))  i.e.
(cid:107)P(cid:96)(cid:107). This L1-regularization yields row-vector (feature) sparsity of
ˆx which plays the role of feature selection. Let W = P (cid:62)P = (W1  . . .   Wd) and we can easily
show that
Motivated by this observation  instead of L1-regularization over vector ((cid:107)P1(cid:107)  . . .  (cid:107)Pd(cid:107)) we can
enforce L1-norm regularization across the vector ((cid:107)W1(cid:107)  . . .  (cid:107)Wd(cid:107)). However  a low-dimensional
projected space ˆx does not mean that its row-vector (feature) should be sparse.
Ideally  we ex-
pect that the principal component of ˆx can be sparse. Hence  we introduce an extra orthonormal
transformation U ∈ O d and let ˆxi = P U xi. Denote a set of triplets T by

W(cid:96) ≡ 0 ⇐⇒ P(cid:96) ≡ 0.

T = {τ = (i  j  k) : i  j  k ∈ Nn   (xi  xj) ∈ S and (xj  xk) ∈ D}.

(2)
By introducing slack variables ξ in constraints (1)  we propose the following sparse (low-rank)
distance matrix learning formulation:

(cid:80)

(cid:96)k) 1
P 2

k∈Nd

(cid:96)∈Nd

(cid:80)
τ ξτ + γ||W||2

(2 1)

min
U∈Od

min
W∈S d
+
s.t.

(cid:80)

(cid:80)

1 + x(cid:62)

ijU(cid:62)W U xij ≤ x(cid:62)
ξτ ≥ 0  ∀τ = (i  j  k) ∈ T   and W ∈ S d
+.

kjU(cid:62)W U xkj + ξτ  

(3)

where ||W||(2 1) =
2 denotes the (2  1)-norm of W . A similar mixed (2  1)-norm
regularization was used in [1  18] for multi-task learning and multi-class classiﬁcation to learn the
sparse representation shared across different tasks or classes.

k(cid:96)) 1

k w2

(cid:96)(

2.1 Equivalent Saddle Representation

We now turn our attention to an equivalent saddle (min-max) representation for sparse metric learn-
ing (3) which is essential for developing optimization algorithms in the next section. To this end  we
need the following lemma which develops and extends a similar version in multi-task learning [1  2]
to the case of learning a positive semi-deﬁnite distance matrix.

2

(cid:88)
min
U∈Od
ijM xij ≤ x(cid:62)
ξτ ≥ 0 ∀τ = (i  j  k) ∈ T   and M ∈ S d
+.

ξτ + γ||U(cid:62)M U||2

kjM xkj + ξτ  

(2 1)

τ

min
M∈S d
+
s.t. x(cid:62)

(5)

Lemma 1. Problem (3) is equivalent to the following convex optimization problem

(1 + x(cid:62)

ijM xij − x(cid:62)

kjM xkj)+ + γ(Tr(M))2

(4)

(cid:88)

min
M(cid:186)0

τ =(i j k)∈T

Proof. Let M = U W U(cid:62) in equation (3) and then W = U(cid:62)M U. Hence  (3) is reduced to the
following

Now  for any ﬁxed M in equation (5)  by the eigen-decomposition of M there exists (cid:101)U ∈ O d
such that M = (cid:101)U(cid:62)λ(M)(cid:101)U . Here  the diagonal matrix λ(M) = diag(λ1  λ2  . . .   λd) where λi is
the i-th eigenvalue of M. Let V = (cid:101)U U ∈ O d  and then we have minU∈Od ||U(cid:62)M U||(2 1) =
minU∈Od ||((cid:101)U U)(cid:62)λ(M)(cid:101)U U||(2 1) = minV ∈Od ||V (cid:62)λ(M)V ||(2 1). Observe that
(cid:162) 1

(cid:80)
(cid:80)
(cid:80)
ki ≤(cid:161)(cid:80)
this back into (6) yields ||V (cid:62)λ(M)V ||(2 1) ≥(cid:80)
(cid:80)

j VkiVk(cid:48)i)λkVkjλk(cid:48) Vk(cid:48)j
where  in the last equality  we use the fact that V ∈ O d  i.e.
j VkjVk(cid:48)j = δkk(cid:48) . Applying Cauchy-
2 . Putting
Schwartz’s inequality implies that
k V 2
k λ2
kV 2
ki
k λk = Tr(M)  where we use the
k λkV 2
fact V ∈ O d again. However  if we select V to be identity matrix Id  ||V (cid:62)λ(M)V ||(2 1) = Tr(M).
Hence  minU∈Od ||U(cid:62)M U||(2 1) = minV ∈Od ||V (cid:62)λ(M)V ||(2 1) = Tr(M). Putting this back into
equation (5) the result follows.

||V (cid:62)λ(M)V ||(2 1) =
=

(cid:80)
(cid:80)
(cid:80)

(cid:80)
(cid:80)

(cid:162) 1
(cid:80)
(cid:162) 1

k VkiλkVkj)2) 1

(cid:161)(cid:80)

(cid:161)(cid:80)

(cid:161)(cid:80)

2 (
ki =

k λkV 2

(cid:80)

(cid:162) 1

k k(cid:48)(

ki) 1

kV 2
ki

k λ2

kV 2
ki

k λ2

2 =

2 =

(6)

2

i

i(

j(

2

i

i

From the above lemma  we are ready to present an equivalent saddle (min-max) representation of
T /γ }
problem (3). First  let Q1 = {uτ : τ ∈ T   0 ≤ uτ ≤ 1} and Q2 = {M ∈ S d
where T is the cardinality of triplet set T i.e. T = #{τ ∈ T }.
Theorem 1. Problem (4) is equivalent to the following saddle representation
ij)  M(cid:105) − γ(Tr(M))2

jk − xijx(cid:62)

uτ (xjkx(cid:62)

(cid:88)

(cid:110)

(7)

−

uτ

+ : Tr(M) ≤(cid:112)
(cid:111)
(cid:88)

t∈T

(cid:104)
τ =(i j k)∈T

min
u∈Q1

max
M∈Q2

γ(Tr(M∗))2 ≤(cid:80)
yields that Tr(M∗) ≤(cid:112)

kjM xik − x(cid:62)
τ∈T (1 + x(cid:62)
(cid:88)

min
M∈Q2

Proof. Suppose that M∗ is an optimal solution of problem (4). By its deﬁnition  there holds
kjM xkj)+ + γ(Tr(M))2 for any M (cid:186) 0. Letting M = 0

T /γ. Hence  problem (4) is identical to

(1 + x(cid:62)

ijM xij − x(cid:62)

kjM xkj)+ + γ(Tr(M))2.

(8)

τ =(i j k)∈T

(cid:80)
Observe that s+ = max{0  s} = maxα{sα : 0 ≤ α ≤ 1}.
(cid:111)
kjM xik − x(cid:62)
τ∈T uτ (1 + x(cid:62)
above equation can be written as minM∈Q2 max0≤u≤1
γ(Tr(M))2. By the min-max theorem (e.g.
the above problem is equivalent
[5]) 
ijM xij + x(cid:62)
minu∈Q1 maxM∈Q2
ijM xij = (cid:104)xjkx(cid:62)
this with the fact that x(cid:62)
theorem.

the
ijM xij) +
to
τ∈T ut. Combining
ij  M(cid:105) completes the proof of the

τ∈T uτ (−x(cid:62)
jkM xjk − x(cid:62)

jkM xjk) − γ(Tr(M))2

jk − xijx(cid:62)

−(cid:80)

(cid:110)(cid:80)

Consequently 

2.2 Related Work

There is a considerable amount of work on metric learning. In [9]  an information-theoretic approach
to metric learning (ITML) is developed which equivalently transforms the metric learning problem

3

to that of learning an optimal Gaussian distribution with respect to an relative entropy. The method
of Relevant Component analysis (RCA)[7] attempts to ﬁnd a distance metric which can minimize
the covariance matrix imposed by the equivalence constraints. In [25]  a distance metric for k-means
clustering is then learned to shrink the averaged distance within the similar set while enlarging the
average distance within the dissimilar set simultaneously. All the above methods generally do not
yield sparse solutions and only work within their special settings. Maximally Collapsing Metric
Learning (MCML) tries to map all points in a same class to a single location in the feature space via
a stochastic selection rule. There are many other metric learning approaches in either unsupervised
or supervised learning setting  see [26] for a detailed review. We particularly mention the following
work which is more related to our sparse metric learning model (3).
• Large Margin Nearest Neighbor (LMNN) [23  24]: LMNN aims to explore a large margin nearest
neighbor classiﬁer by exploiting nearest neighbor samples as side information in the training set.
Speciﬁcally  let Nk(x) denotes the k-nearest neighbor of sample x and deﬁne the similar set S =
{(xi  xj) : xi ∈ N (xj)  yi = yj} and D = {(xj  xk) : xk ∈ N (xj)  yk (cid:54)= yj}. Then  recall that the
triplet set T is given by equation (2)  the framework LMNN can be rewritten as the following:

min
M(cid:186)0

τ =(i j k)∈T

(1 + x(cid:62)

ijM xij − x(cid:62)

kjM xkj)+ + γTr(CM)
(cid:80)
where the covariance matrix C over the similar set S is deﬁned by C =
(xi xj )∈S(xi − xj)(xi −
xj)(cid:62). From the above reformulation  we see that LMNN also involves a sparse regularization term
Tr(CM). However  the sparsity of CM does not imply the sparsity of M  see the discussion in the
experimental section. Large Margin Component Analysis (LMCA) [22] is designed for conducting
classiﬁcation and dimensionality reduction simultaneously. However  LMCA controls the sparsity
by directly specifying the dimensionality of the transformation matrix and it is an extended version
of LMNN. In practice  this low dimensionality is tuned by ad hoc methods such as cross-validation.
• Sparse Metric Learning via Linear Programming (SMLlp) [20]: the spirit of this approach is
closer to our method where the following sparse framework was proposed:

(9)

(cid:88)

(cid:88)

(cid:88)

min
M(cid:186)0

(cid:80)

(1 + x(cid:62)

ijM xij − x(cid:62)

kjM xkj)+ + γ

|M(cid:96)k|

(10)

t=(i j k)∈T
|M(cid:96)k| can only enforce the element sparsity of M. The
However  the above 1-norm term
learned sparse model would not generate an appropriate low-ranked principal matrix M for metric
learning. In order to solve the above optimization problem  [10] further proposed to restrict M to the
space of diagonal dominance matrices: a small subspace of the positive semi-deﬁnite cone. Such a
restriction would only result in a sub-optimal solution  although the ﬁnal optimization is an efﬁcient
linear programming problem.

(cid:96) k∈Nd

(cid:96) k∈Nd

3 Smooth Optimization Algorithms

Nesterov [17  16] developed an efﬁcient smooth optimization method for solving convex program-
ming problems of the form minx∈Q f(x) where Q is a bounded closed convex set in a ﬁnite-
dimensional real vector space E. This smooth optimization usually requires f to be differentiable
with Lipschitz continuous gradient and it has an optimal convergence rate of O(1/t2) for smooth
problems where t is the iteration number. Unfortunately  we can not directly apply the smooth opti-
mization method to problem (4) since the hinge loss there is not continuously differentiable. Below
we show the smooth approximation method [17] can be approached through the saddle representa-
tion (7).

3.1 Nesterov’s Smooth Approximation Approach

We brieﬂy review Nesterov’s approach [17] in the setting of a general min-max problem using
smoothing techiniques. To this end  we introduce some useful notation. Let Q1 (resp. Q2) be non-
empty convex compact sets in ﬁnite-dimensional real vector spaces E1 (resp. E2) endowed with
norm (cid:107) · (cid:107)1 (resp. (cid:107) · (cid:107)2). Let E∗
2 be the dual space of E2 with standard norm deﬁned  for any
2 = max{(cid:104)s  x(cid:105)2 : (cid:107)x(cid:107)2 = 1}  where the scalar product (cid:104)· ·(cid:105)2 denotes the value
s ∈ E∗
of s at x. Let A : E1 → E∗
1 is deﬁned 

2 be a linear operator. Its adjoint operator A∗ : E2 → E∗

2  by (cid:107)s(cid:107)∗

4

(cid:80)
Smooth Optimization Algorithm for Sparse Metric Learning (SMLsm)
1. Let ε > 0  t = 0 and initialize u(0) ∈ Q1  M (−1) = 0 and let L = 1
τ∈T (cid:107)Xτ(cid:107)2
(cid:111)
2. Compute Mµ(u(t)) and ∇φµ(u(t)) = (−1 + (cid:104)Xτ   Mµ(u(t))(cid:105) : τ ∈ T )
and let M (t) = t
t+2 Mµ(ut)
2 (cid:107)u(t) − z(cid:107)2 + ∇φµ(u(t))(cid:62)(z − u(t))
2 (cid:107)u(0) − v(cid:107)2 +

3. Compute z(t) = arg minz∈Q1
4. Compute v(t) = arg minv∈Q1
5. Set u(t+1) = 2
6. Set t ← t + 1. Go to step 2 until the stopping criterion less than ε

(cid:110)
t+2 M (t−1) + 2
(cid:110)

t+3 v(t) + t+1

i=0( i+1
2 )

(cid:80)t

t+3 z(t)

(cid:161)

2µ

L

L

2

(cid:162)(cid:111)

φµ(u(i)) + ∇φµ(u(i))(cid:62)(v − u(i))

Table 1: Pseudo-code of ﬁrst order Nesterov’s method

(cid:111)

min
u∈Q1

for any x ∈ E2 and u ∈ E1  by (cid:104)Au  x(cid:105)2 = (cid:104)A∗x  u(cid:105)1. The norm of such a operator is deﬁned by
(cid:107)A(cid:107)1 2 = maxx u {(cid:104)Au  x(cid:105)2 : (cid:107)x(cid:107)2 = 1 (cid:107)u(cid:107)1 = 1} .
Now  the min-max problem considered in [17  Section 2] has the following special structure:

(cid:110)
φ(u) = (cid:98)φ(u) + max{(cid:104)Au  x(cid:105)2 − ˆf(x) : x ∈ Q2}

Here  (cid:98)φ(u) is assumed to be continuously differentiable and convex with Lipschitz continuous gra-

dient and ˆf(x) is convex and differentiable. The above min-max problem is usually not smooth and
Nesterov [17] proposed a smoothing approximation approach to solve the above problem:

(cid:110)
(cid:111)
φµ(u) = (cid:98)φ(u) + max{(cid:104)Au  x(cid:105)2 − ˆf(x) − µd2(x) : x ∈ Q2}

(12)
Here  d2(·) is a continuous proxy-function  strongly convex on Q2 with some convexity parameter
σ2 > 0 and µ > 0 is a small smoohting parameter. Let x0 = arg minx∈Q2 d2(x). Without loss
of generality  assume d2(x0) = 0. The strong convexity of d2(·) with parameter σ2 means that
d2(x) ≥ 1
2. Since d2(·) is strongly convex  the solution of the maximization problem
ˆφµ(u) := max{(cid:104)Au  x(cid:105)2 − ˆf(x) − µd2(x) : x ∈ Q2} is unique and differentiable  see [6  Theorem
4.1]. Indeed  it was established in [17  Theorem 1 ] that the gradient of φµ is given by

2 σ2(cid:107)x − x0(cid:107)2

min
u∈Q1

(11)

.

.

∇ ˆφµ(u) = A∗xµ(u)
(cid:107)A(cid:107)2
1 2
µσ2

  i.e. (cid:107)A∗xµ(u1) − A∗xµ(u2)(cid:107)∗

(13)
(cid:107)u1 − u2(cid:107)1.
and it has a Lipschitz constant L =
Hence  the proxy-function d2 can be regarded as a generalized Moreau-Yosida regularization term
to smooth out the objective function.
As mentioned above  function φµ in problem (12) is differentiable with Lipschitz continuous gra-
dients. Hence  we can apply the optimal smooth optimization scheme [17  Section 3] to the
smooth approximate problem (12). The optimal scheme needs another proxy-function d(u) as-
sociated with Q1. Assume that d(u0) = minu∈Q1 d(u) = 0 and it has convexity parameter σ i.e.
d(u) ≥ 1
2 σ(cid:107)u − u0(cid:107)1. For this special problem (12)  the primal solution u∗ ∈ Q1 and dual solution
x∗ ∈ Q2 can be simultaneously obtained  see [17  Theorem 3]. Below  we will apply this general
scheme to solve the min-max representation (7) of the sparse metric learning problem (3)  and hence
solves the original problem (4).

1 ≤ (cid:107)A(cid:107)2

µσ2

1 2

3.2 Smooth Optimization Approach for Sparse Metric Learning

We now turn our attention to developing a smooth optimization approach for problem (4). Our main
idea is to connect the saddle representation (7) in Theorem 1 with the special formulation (11).
To this end  ﬁrstly let E1 = RT with standard Euclidean norm (cid:107) · (cid:107)1 = (cid:107) · (cid:107) and E2 = S d with
Frobenius norm deﬁned  for any S ∈ S d  by (cid:107)S(cid:107)2
ij. Secondly  the closed convex sets
S2
are respectively given by Q1 = {u = (uτ : τ ∈ T ) ∈ [0  1]T} and Q2 = {M ∈ S d
+ : Tr(M) ≤
T /γ}. Then  deﬁne the proxy-function d2(M) = (cid:107)M(cid:107)2. Consequently  the proxy-function d2(·)
is strongly convex on Q2 with convexity parameter σ2 = 2. Finally  for any τ = (i  j  k) ∈ T   let

(cid:80)

(cid:112)

i j∈Nd

2 =

5

ij. In addition  we replace the variable x by M and (cid:98)φ(u) = −(cid:80)

Xτ = xjkx(cid:62)
τ∈T uτ in
(12)  ˆf(M) = γ(Tr(M))2. Finally  deﬁne the linear operator A : RT → (S d)∗  for any u ∈ RT   by
(14)

jk − xijx(cid:62)

(cid:88)

Au =

uτ Xτ .

τ∈T

With the above preparations  the saddle representation (7) exactly matches the special structure (11)
which can be approximated by problem (12) with µ sufﬁciently small. The norm of the linear
operator A can be estimated as follows.
Lemma 2. Let the linear operator A be deﬁned as above  then (cid:107)A(cid:107)1 2 ≤
for any M ∈ S d  (cid:107)M(cid:107)2 denotes the Frobenius norm of M.
Proof. For any u ∈ Q1 and M ∈ S d  we have that
Tr

(cid:162) ≤(cid:161)(cid:80)

τ∈T (cid:107)Xτ(cid:107)2

(cid:162)(cid:107)M(cid:107)2
(cid:162) 1
(cid:161)(cid:80)
(cid:161)(cid:80)
τ∈T uτ(cid:107)Xτ(cid:107)2
τ∈T (cid:107)Xτ(cid:107)2
τ∈T u2
τ

2 = (cid:107)M(cid:107)2(cid:107)u(cid:107)1

(cid:161)(cid:161)(cid:80)

(cid:179)(cid:80)

≤ (cid:107)M(cid:107)2

τ∈T uτ Xτ

2 where 

(cid:180) 1

(cid:162) 1

(cid:162)

M

2

2

2

(cid:162) 1

(cid:161)(cid:80)
(cid:180)
τ∈T (cid:107)Xτ(cid:107)2
τ∈T uτ Xτ )M

2

2 .

:

(cid:169)

(cid:179)

(cid:80)

Tr

(

Combining the above inequality with the deﬁnition that (cid:107)A(cid:107)1 2 = max
(cid:107)u(cid:107)1 = 1 (cid:107)M(cid:107)2 = 1

yields the desired result.

(cid:170)

τ∈T

We now can adapt the smooth optimization [17  Section 3 and Theorem 3] to solve the smooth
approximation formulation (12) for metric learning. To this end  let the proxy-function d in Q1 be
the standard Euclidean norm i.e. for some u(0) ∈ Q1 ⊆ RT   d(u) = (cid:107)u − u(0)(cid:107)2. The smooth
optimization pseudo-code for problem (7) (equivalently problem (4)) is outlined in Table 1. One can
stop the algorithm by monitoring the relative change of the objective function or change in the dual
gap.
The efﬁciency of Nesterov’s smooth optimization largely depends on Steps 2  3  and 4 in Table 1.
Steps 3 and 4 can be solved straightforward where z(t) = min(max(0  u(t) −∇φµ(u(t))/L)  1) and
(cid:88)
i=0(i + 1)∇φµ(u(i))/2L)  1). The solution Mγ(u) in Step 2 involves

v(t) = min(max(0  u(0) −(cid:80)t

the following problem

Mµ(u) = arg max{(cid:104)

uτ Xτ   M(cid:105) − γ(Tr(M))2 − µ(cid:107)M(cid:107)2

2 : M ∈ Q2}.

(15)

(16)

i :
s2

i∈Nd

i∈Nd

(cid:111)

si ≤

(cid:112)

(cid:80)

(cid:88)

(cid:88)

si)2− µ

λisi− γ(

t∈T utXt by

T /γ  and si ≥ 0 ∀i ∈ Nd

(cid:110)(cid:88)
(cid:80)
that Tr(XY ) ≤ (cid:80)

The next lemma shows it can be efﬁciently solved by quadratic programming (QP).
Lemma 3. Problem (15) is equivalent to the following
s∗ = arg max

(cid:88)
(cid:80)
i∈Nd
t∈T utXt. Moreover  if we denotes the eigen-
t∈T utXt = Udiag(λ)U(cid:62) with some U ∈ O d then the optimal

i∈Nd
where λ = (λ1  . . .   λd) are the eigenvalues of
decomposition
solution of problem (15) is given by Mµ(u) = Udiag(s∗)U(cid:62).
Proof. We know from Von Neumann’s inequality (see [14] or [4  Page 10])  for all X  Y ∈ S d 
λi(X)λi(Y ) where λi(X) and λi(Y ) are the eigenvalues of X and Y in
non-decreasing order  respectively. The equality is attained whenever X = Udiag(λ(X))U(cid:62)  Y =
Udiag(λ(Y ))U(cid:62) for some U ∈ O d. The desired result follows by applying the above inequality
with X =
It was shown in [17  Theorem 3] that the iteration complexity is of O(1/ε) for ﬁnding a ε-optimal
solution if we choose µ = O(ε). This is usually much better than the standard sub-gradient descent
method with iteration complexity typically O(1/ε2). As listed in Table 1  the complexity for each
iteration mainly depends on the eigen-decomposition on
utXt and the quadratic program-
(cid:80)
ming to solve problem (15) which has complexity O(d3). Hence  the overall iteration complexity of
the smooth optimization approach for sparse metric learning is of the order O(d3/ε) for ﬁnding an
τ (cid:107)Xτ(cid:107)2 could be too
ε-optimal solution. As a ﬁnal remark  the Lipschitz given by the L = 1
2µ
loose in reality. One can use the line search scheme [15] to further accelerate the algorithm.

τ∈T uτ Xτ and Y = M.

(cid:80)

(cid:80)

i∈Nd

t∈Nt

6

4 Experiments

In this section we compared our proposed method with four other methods including (1) the
LMNN method [23]  (2) the Sparse Metric Learning via Linear Programming (SMLlp) [20]  (3)
the information-theoretic approach for metric learning (ITML) [9]  and (4) the Euclidean distance
based k-Nearest Neighbor (KNN) method (called Euc for brevity). We also implemented the iter-
ative sub-gradient descent algorithm [24] to solve the proposed framework (4) (called SMLgd) in
order to evaluate the efﬁciency of the proposed smooth optimization algorithm SMLsm. We try to
exploit all these methods to learn a good distance metric and a KNN classiﬁer is used to examine
the performance of these different learned metrics.
The comparison is done on four benchmark data sets: Wine  Iris  Balance Scale  and Ionosphere 
which were obtained from the UCI machine learning repository. We randomly partitioned the
data sets into a training and test sets by using a ratio 0.85. We then trained each approach on
the training set  and performed evaluation on the test sets. We repeat the above process 10 times
and then report the averaged result as the ﬁnal performance. All the approaches except the Eu-
clidean distance need to deﬁne a triplet set T before training. Following [20]  we randomly gen-
erated 1500 triplets for SMLsm  SMLgd  SMLlp  and LMNN. The number of nearest neighbors
was adapted via cross validation for all the methods in the range of {1  3  5  7}. The trade-off
parameter for SMLsm  SMLgd  SMLlp  and LMNN was also tuned via cross validation from
{10−5  10−4  10−3  10−2  10−1  100  101  102}.
The ﬁrst part of our evaluations focuses on testing the learning accuracy. The result can be seen
in Figure 1 (a)-(d) respectively for the four data sets. Clearly  the proposed SMLsm demonstrates
best performance. Speciﬁcally  SMLsm outperforms the other four methods in Wine and Iris  while
it ranks the second in Balance Scale and Ionosphere with slightly lower accuracy than the best
method. SMLgd showed different results with SMLsm due to the different optimization methods 
which we will discuss shortly in Figure 1 (i)-(l). We also report the dimension reduction Figure 1(e)-
(h). It is observed that our model outputs the most sparse metric. This validates the advantages
of our approach. That is  our method directly learns both an accurate and sparse distance metric
simultaneously. In contrast  other methods only touch this topic marginally: SMLlp is not optimal 
as they exploited the one-norm regularization term and also relaxed the learning problem; LMNN
aims to learn a metric with a large-margin regularization term  which is not directly related to sparsity
of the distance matrix. ITML and Euc do not generate a sparse metric at all. Finally  in order to
examine the efﬁciency of the proposed smooth optimization algorithm  we plot the convergence
graphs of SMLsm versus those of SMLgd in Figure 1(i)-(l). As observed  SMLsm converged much
faster than SMLgd in all the data sets. SMLgd sometimes oscillated and may incur a long tail due
to the non-smooth nature of the hinge loss. For some data sets  it converged especially slow  which
can be observed in Figure (k) and (l).

5 Conclusion

In this paper we proposed a novel regularization framework for learning a sparse (low-rank) distance
matrix. This model was realized by a mixed-norm regularization term over a distance matrix which
is non-convex. Using its special structure  it was shown to be equivalent to a convex min-max
(saddle) representation involving a trace norm regularization. Depart from the saddle representation 
we successfully developed an efﬁcient Nesterov’s ﬁrst-order optimization approach [16  17] for our
metric learning model. Experimental results on various datasets show that our sparse metric learning
framework outperforms other state-of-the-art methods with higher accuracy and signiﬁcantly smaller
dimensionality. In future  we are planning to apply our model to large-scale datasets with higher
dimensional features and use the line search scheme [15] to further accelerate the algorithm.

Acknowledgements

The second author is partially supported by the Excellent SKL Project of NSFC (No.60723005) 
China. The ﬁrst and third author is supported by EPSRC grant EP/E027296/1.

7

(a)

(d)

(b)

(e)

(g)

(h)

(c)

(f)

(i)

(j)

(k)

(l)

Figure 1: Performance comparison among different methods. Subﬁgures (a)-(d) present the aver-
age error rates; (e)-(h) plots the average dimensionality used in different methods; (i)-(l) give the
convergence graph for the sub-gradient algorithm and the proposed smooth optimization algorithm.

8

0.74SMLsm1.95SMLgd1.48SMLlp2.48  ITML1.48  LMNN 3.7   EUC Wine−Average Error Rate (%)  1.3SMLsm1.4SMLgd2.17SMLlp1.74  ITML1.74  LMNN 3.04   EUC Iris−Average Error Rate (%)  8.19SMLsm9.21SMLgd18.62SMLlp6.81  ITML8.09  LMNN 20.11   EUC Bal−Average Error Rate (%)  9.06SMLsm8.87SMLgd10.2SMLlp12.45  ITML10.19  LMNN 15.28   EUC Iono−Average Error Rate (%)  5.7SMLsm9.3SMLgd12.1SMLlp13  ITML10.7  LMNN 13   EUC Wine−Average Dim  1SMLsm1SMLgd4SMLlp4  ITML2  LMNN 4   EUC Iris−Average Dim  3.3SMLsm3.7SMLgd4SMLlp4  ITML3.3  LMNN 4   EUC Bal−Average Dim  4.9SMLsm11SMLgd15.1SMLlp33  ITML9.3  LMNN 33   EUC Iono−Average Dim  010020030040050000.10.20.30.40.50.60.70.80.91EpochNormalized Objective ValuesConvergence Curve for Wine  SMLgd SMLsm010020030040050000.10.20.30.40.50.60.70.80.91EpochNormalized Objective ValuesConvergence curves for Iris  SMLgdSMLsm05010015020025030035040045000.10.20.30.40.50.60.70.80.91EpochNormalized Objective ValuesConvergence Curves for Balance  SMLgdSMLsm010020030040050000.10.20.30.40.50.60.70.80.91EpochNormalized Objective ValuesConvergence Curves for Ionosphere  SMLgdSMLsmReferences

[1] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning. NIPS  2007.
[2] A. Argyriou  C. A. Micchelli  M. Pontil  and Y. Ying. A spectral regularization framework

for multi-task structure learning. NIPS  2008.

[3] F. R. Bach. Consistency of trace norm minimization. J. of Machine Learning Research  9:

1019–1048  2008.

[4] J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and

Examples. CMS Books in Mathematics. Springer  2005.

[5] S. Boyd and L . Vandenberghe. Convex optimization. Cambridge University Press  2004.
[6] J. F. Bonnans and A. Shapiro. Optimization problems with perturbation: A guided tour. SIAM

Review  40:202–227  1998.

[7] A. Bar-Hillel  T. Hertz  N. Shental  and D. Weinshall. Learning a mahalanobis metric from

equivalence constraints. J. of Machine Learning Research  6: 937-965  2005.

[8] S. Chopra  R. Hadsell  and Y. LeCun. Learning a similarity metric discriminatively with ap-

plication to face veriﬁcation. CVPR  2005.

[9] J. Davis  B. Kulis  P. Jain  S. Sra  and I. Dhillon. Information-theoretic metric learning. ICML 

2007.

[10] G. M. Fung  O. L. Mangasarian  and A. J. Smola. Minimal kernel classiﬁers. J. of Machine

Learning Research  3: 303–321  2002.

[11] A. Globerson  S. Roweis  Metric learning by collapsing classes. NIPS  2005.
[12] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighbourhood component anal-

ysis. NIPS  2004.

[13] T. Hastie  R.Tibshirani  and Robert Friedman. The Elements of Statistical Learning. Springer-

Verlag New York  LLC  2003.

[14] R.A. Horn and C.R. Johhnson. Topics in Matrix Analysis. Cambridge University Press  1991.
[15] A. Nemirovski. Efﬁcient methods in convex programming. Lecture Notes  1994.
[16] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer  2003.
[17] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming 

103:127-152  2005.

[18] Obozinski  B. Taskar  and M. I. Jordan. Joint covariate selection and joint subspace selection

for multiple classiﬁcation problems. Statistics and Computing. In press  2009.

[19] J. D. M. Rennie  and N. Srebro. Fast maximum margin matrix factorization for collaborative

prediction. ICML  2005.

[20] R. Rosales and G. Fung. Learning sparse metrics via linear programming. KDD  2006.
[21] N. Srebro  J.D. M. Rennie  and T. S. Jaakkola. Maximum-margin matrix factorization. NIPS 

2005.

[22] L. Torresani and K. Lee. Large margin component analysis. NIPS  2007.
[23] K. Q. Weinberger  J. Blitzer  and L. Saul. Distance metric learning for large margin nearest

neighbour classiﬁcation. NIPS  2006.

[24] K. Q. Weinberger and L. K. Saul. Fast solvers and efﬁcient implementations for distance

metric learning. ICML  2008.

[25] E. Xing  A. Ng  M. Jordan  and S. Russell. Distance metric learning with application to

clustering with side information. NIPS  2002.

[26] L. Yang and R. Jin. Distance metric learning: A comprehensive survey. In Technical report 

Department of Computer Science and Engineering  Michigan State University  2007.

9

,Andreas Veit
Michael Wilber
Serge Belongie