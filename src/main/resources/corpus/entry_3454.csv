2019,Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases,Differential privacy has become a widely accepted notion of privacy  leading to the introduction and deployment of numerous privatization mechanisms. However  ensuring the privacy guarantee is an error-prone process  both in designing mechanisms and in implementing those mechanisms. Both types of errors will be greatly reduced  if we have a data-driven approach to verify privacy guarantees  from a black-box access to a mechanism. We pose it as a property estimation problem  and study the fundamental trade-offs involved in the accuracy in estimated privacy guarantees and the number of samples required. We introduce a novel estimator that uses polynomial approximation of a carefully chosen degree to optimally trade-off bias and variance. With n samples  we show that this estimator achieves performance of a straightforward plug-in estimator with n*log(n) samples  a phenomenon referred to as effective sample size amplification. The minimax optimality of the proposed estimator is proved by comparing it to a matching fundamental lower bound.,Minimax Optimal Estimation of Approximate
Differential Privacy on Neighboring Databases

Xiyang Liu

Sewoong Oh

Allen School of Computer Science and Engineering 

University of Washington

{xiyangl  sewoong}@cs.washington.edu

Abstract

Differential privacy has become a widely accepted notion of privacy  leading to
the introduction and deployment of numerous privatization mechanisms. How-
ever  ensuring the privacy guarantee is an error-prone process  both in designing
mechanisms and in implementing those mechanisms. Both types of errors will be
greatly reduced  if we have a data-driven approach to verify privacy guarantees 
from a black-box access to a mechanism. We pose it as a property estimation prob-
lem  and study the fundamental trade-offs involved in the accuracy in estimated
privacy guarantees and the number of samples required. We introduce a novel
estimator that uses polynomial approximation of a carefully chosen degree to op-
timally trade-off bias and variance. With n samples  we show that this estimator
achieves performance of a straightforward plug-in estimator with n ln n samples 
a phenomenon known as sample size ampliﬁcation. The minimax optimality of
the estimator is proved by comparing it to a matching fundamental lower bound.

1

Introduction

Differential privacy is gaining popularity as an agreed upon measure of privacy leakage  widely
used by the government to publish Census statistics [1]  Google to aggregate user’s choices in web-
browser features [2  3]  Apple to aggregate mobile user data [4]  and smart meters in telemetry [5].
As increasing number of privatization mechanisms are introduced and deployed in the wild  it is
critical to have countermeasures to check the ﬁdelity of those mechanisms. Such techniques will
allow us to hold accountable the deployment of privatization mechanisms if the claimed privacy
guarantees are not met  and help us ﬁnd and ﬁx bugs in implementations of those mechanisms.
A user-friendly tool for checking privacy guarantees is necessary for several reasons. Writing a
program for a privatization mechanism is error-prone  as it involves complex probabilistic compu-
tations. Even with customized languages for differential privacy  checking the end-to-end privacy
guarantee of an implementation remains challenging [6  7]. Furthermore  even when the implemen-
tation is error-free  there have been several cases where the mechanism designers have made errors
in calculating the privacy guarantees  and falsely reported higher level of privacy [8  9]. This is
evidence of an alarming issue that analytically checking the proof of a privacy guarantee is a chal-
lenging process even for an expert. An automated and data-driven algorithm for checking privacy
guarantees will signiﬁcantly reduce such errors in the implementation and the design. On other
cases  we are given very limited information about how the mechanism works  like Apple’s white
paper [4]. The users are left to trust the claimed privacy guarantees.
To address these issues  we propose a data-driven approach to estimate how much privacy is guar-
anteed  from a black-box access to a purportedly private mechanism. Our approach is based on
an optimal polynomial approximation that gracefully trades off bias and variance. We study the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

fundamental limit of how many samples are necessary to achieve a desired level of accuracy in the
estimation  and show that the proposed approach achieves this fundamental bound.
Problem formulation. Differential privacy (DP) introduced in [10] is a formal mathematical notion
of privacy that is widespread  due to several key advantages. It gives one of the strongest guarantees 
allows for precise mathematical analyses  and is intuitive to explain even to non-technical end-users.
When accessing a database through a query  we say the query output is private if the output did not
reveal whether a particular person’s entry is in the database or not. Formally  we say two databases
are neighboring if they only differ in one entry (one row in a table  for example). Let PQ D denote
the distribution of the randomized output to a query Q on a database D. We consider discrete valued
mechanisms taking one of S values  i.e. the response to a query is in [S] = {1  . . .   S} for some
integer S. We say a mechanism guarantees (ε  δ)-DP  [10]  if the following holds

(1)
for some ε ≥ 0  δ ∈ [0  1]  and all subset E ⊆ [S] and for all neighboring databases D and D′.
When δ = 0  (ε  0)-DP is referred to as (pure) differential privacy  and the general case of δ ≥ 0 is
referred to as approximate differential privacy. For pure DP  the above condition can be relaxed as

PQ D(E) ≤ eεPQ D′ (E) + δ  

PQ D(x) ≤ eεPQ D′ (x)  

(2)
for all output symbol x ∈ [S]  and for all neighboring databases D and D′. This condition can now
be checked  one symbol x at a time from [S]  without having to enumerate all subsets E ⊆ [S]. This
naturally leads to the following algorithm.
For a query Q and two neighboring databases D and D′ of interest  we need to verify the condition
in Eq. (2). As we only have a black-box access to the mechanism  we collect n responses from
the mechanism on the two databases. We check the condition on the empirical distribution of those
collected samples  for each x ∈ [S]. If it is violated for any x  we assert the mechanism to be
not (ε  0)-DP and present x as an evidence. Focusing only on pure DP  [11] proposed an approach
similar to this  where they also give guidelines for choosing the databases D and D′ to test. However 
their approach is only evaluated empirically  no statistical analysis is provided  and a more general
case of approximate DP is left as an open question  as the condition in Eq. (1) cannot be decoupled
like Eq. (2) when δ > 0.
We propose an alternative approach from ﬁrst principles to check the general approximate DP guar-
antees  and prove its minimax optimality. Given two probability measures P = [p1  . . .   pS] and
Q = [q1  . . .   qS] over [S] = {1  . . .   S}  we deﬁne the following approximate DP divergence with
respect to ε as

dε(P%Q) ≜

S!i=1

[pi − eεqi]+ = Ex∼P""1 − eε qx

px#+# .

(3)

where [x]+ = max{x  0}. The last representation indicates that this metric falls under a broader
class of metrics known as f-divergences  with a special choice of f (x) = [1 − eεx]+. From the
deﬁnition of DP  it follows that a mechanism is (ε  δ)-DP if and only if dε(PQ D%PQ D′ ) ≤ δ for
all neighboring databases D and D′. We propose estimating this divergence dε(PQ D%PQ D′ ) from
samples  and comparing it to the target δ. This only requires number of operations scaling as S ln n
where n is the sample size.
In this paper  we suppose there is a speciﬁc query Q of interest  and two neighboring databases D and
D′ have been already selected either by a statistician who has some side information on the structure
of the mechanism or by some algorithm  such as those from [11  12  13]. Without exploiting the
structure (such as symmetry  exchangeability  or invariance to the entries of the database conditioned
on the true output of the query)  one cannot avoid having to check all possible combinations of
neighboring databases. As a remedy  [12] proposes checking randomly selected databases. This
in turn ensures a relaxed notion of privacy known as random differential privacy. Similarly  [13]
proposed checking the typical databases  assuming there we have access to a prior distribution over
the databases. Our framework can be seamlessly incorporated with such higher-level routines to
select databases.
Contributions. We study the problem of estimating the approximate differential privacy guaranteed
by a mechanism  from a black-box access where we can sample from the mechanism output given a

2

query Q  a database D  and a target (ε  δ). We ﬁrst show that a straightforward plug-in estimator of
dε(P%Q) achieves mean squared error scaling as (eεS)/n  where S is the size of the alphabet and
n is the number of samples used (Section 2.1.1).
In the regime where we ﬁx S and increase the sample size  this achieves the parametric rate of 1/n 
and cannot be improved upon. However  in many cases of practical interest where S is comparable
to n  we show that this can be improved upon with a more sophisticated estimator. To this end  we
introduce a novel estimator of dε(P%Q). The main idea is to identify the regimes of non-smoothness
in [pi − eεqi]+ where the plug-in estimator has a large bias. We replace it by the uniformly best
polynomial approximation of the non-smooth regime of the function  and estimate those polynomial
from samples. By selecting appropriate degree of the polynomial  we can optimally trade off the
bias and variance. We provide an upper bound on the error scaling as (eεS)/(n ln n)  when S and
n are comparable. We prove that this is the best one can hope for  by providing a matching lower
bound.
We ﬁrst show this for the case when we know P and sample from Q in Section 2.1  to explain
the main technical insights while maintaining simple exposition. Then  we consider the practical
scenario where both P and Q are accessed via samples  and provide an minimax optimal estimator
in Section 2.2. This phenomenon is referred to as effective sample size ampliﬁcation; one can achieve
with n samples a desired error rate  that would require n ln n samples for a plug-in estimator. We
present numerical experiments supporting our theoretical predictions in Section 3.
Related work. A formal investigation into verifying DP guarantees of a given mechanism was
addressed in [13]. DP condition is translated into a certain Lipschitz condition on PQ D over the
databases D  and a Lipschitz tester is proposed to check the conditions. However  this approach
is not data driven  as it requires the knowledge of the distribution PQ D and no sampling of the
mechanism outputs is involved. [12] analyzes tradeoffs involved in testing DP guarantees. It is
shown that one cannot get accurate testing without sacriﬁcing the privacy of the databases used in
the testing. Hence  when testing DP guarantees  one should not use databases that contain sensitive
data. We compare some of the techniques involved in Section 2.1.1.
Our techniques are inspired by a long line of research in property estimation of a distribution from
samples. There has been signiﬁcant recent advances for high-dimensional estimation problems 
starting from entropy estimation in [14  15  16]. The general recipe is to identify the regime where
the property to be estimated is not smooth  and use functional approximation to estimate a smoothed
version of the property. This has been widely successful in support recovery [17]  density estimation
with ℓ1 loss [18]  and estimating Renyi entropy [19]. More recently  this technique has been applied
to estimate certain divergences between two unknown distributions  for Kullback-Leibler divergence
[20]  total variation distance [21]  and identity testing [22]. With carefully designed estimators  these
approximation-based approaches can achieve improvement over typical parametric rate of 1/n error
rate  sometimes referred to as effective sample size ampliﬁcation.
Notations. We let the alphabet of a discrete distribution be [S] = {1  . . .   S} for some positive
integer S denoting the size of the alphabet. We let MS denote the set of probability distributions
over [S]. We use f (n) ≳ g(n) to denote that supn f (n)/g(n) ≥ C for some constant C  and
f (n) ≲ g(n) is analogously deﬁned. f (n) ≍ g(n) denotes that f (n) ≳ g(n) and f (n) ≲ g(n).
2 Estimating differential privacy guarantees from samples

We want to estimate dε(P%Q) from a blackbox access to the mechanism outputs accessing two
databases  i.e. P = PQ D and Q = PQ D′. We ﬁrst consider a simpler case  where P = [p1  . . .   pS]
is known and we observe samples from an unknown distribution Q = [q1  . . .   qS] in Section 2.1.
We cover this simpler case ﬁrst to demonstrate the main ideas on the algorithm design and analysis
technique while maintaining the exposition simple. This paves the way for our main algorithmic and
theoretical results in Section 2.2  where we only have access to samples from both P and Q.

2.1 Estimating dε(P%Q) with known P
For a given budget n  representing an upper bound on the expected number of samples we can
collect  we propose sampling a random number N of samples from Poisson distribution with mean
n  i.e. N ∼ Poi (n). Then  each sample Xj ∈ [S] is drawn from Q for j ∈ {1  . . .   N}  and we let

3

i=1  making the analysis simpler.

Qn = [ˆq1  . . .   ˆqS] denote the resulting histogram divided by n  such that ˆqi ≜ |{j ∈ [N ] : Xi =
j}|/n. Note that Qn is not the standard empirical distribution  as$i ˆqi ∕= 1 with high probability.
However  in this paper we refer to Qn as empirical distribution of the samples. The empirical
distribution would have been divided by N instead of n. Instead  Qn is the maximum likelihood
estimate of the true distribution Q. This Poisson sampling  together with the MLE construction of
Qn  ensures independence among {ˆqi}S
2.1.1 Performance of the plug-in estimator
The following result shows that it is necessary and sufﬁcient to have n ≈ eε S samples to achieve an
arbitrary desired error rate  if we use this plug-in estimator dε(P%Qn)  under the worst-case P and
Q. Some assumption on (P  Q) is inevitable as it is trivial to achieve zero error for any sample size 
for example if P and Q have disjoint supports. Both dε(P%Q) and dε(P%Qn) are 1 with probability
one. We provide a proof in Appendix C.2. The bound in Eq. (4) also holds for dε(Pn%Q).
Theorem 1. For any ε ≥ 0 and support size S ∈ Z+  if n ≥ eεS  then the plug-in estimator satisﬁes
(4)

sup

.

EQ%|dε(P%Qn) − dε(P%Q)|2& ≍

eεS
n

P Q∈MS

A similar analysis was done in [12]  which gives an upper bound scaling as e2εS/n. We tighten the
analysis by a factor of eε  and provide a matching lower bound.

2.1.2 Achieving optimal sample complexity with a polynomial approximation

We construct a minimax optimal estimator using techniques ﬁrst introduced in [16  15] and adopted
in several property estimation problems including [18  20  19  21  22  17].

Algorithm 1 Differential Privacy (DP) estimator with known P
Input: target privacy ε ∈ R+  query Q  neighboring databases (D D′)  pmf of PQ D
samples from PQ D′  degree K ∈ Z+  constants c1  c2 ∈ R+  expected sample size 2n
Output: estimate 'dε K c1 c2 (P%Qn) of dε(PQ D%PQ D′ )
P ← PQ D
Draw two independent sample sizes: N1 ← Poi (n) and N2 ← Poi (n)
i=1 ∈ [S]N1 and {Xi 2}N2
Sample from PQ D′: {Xi 1}N1
ˆqi j ← |{ℓ∈[Nj ]:Xℓ j =i}|
for all i ∈ [S] and j ∈ {1  2}
Qn 1 ← [ˆq1 1  . . .   ˆqS 1] and Qn 2 ← [ˆq1 2  . . .   ˆqS 2]
for i = 1 to S do

i=1 ∈ [S]N2

n

0
 
˜DK(ˆqi 2; pi)
 
[pi − eε ˆqi 2]+  

if ˆqi 1 > U (pi; c1  c2)
if ˆqi 1 ∈ U (pi; c1  c2)
if ˆqi 1 < U (pi; c1  c2)

(deﬁned in Appendix A)

δi ←()*

end for

'dε K c1 c2 (P%Qn) ← 0 ∨ (1 ∧$S

i=1 δi)

To simplify the analysis  we split the samples randomly into two partitions  each having an inde-
pendent and identical distribution of Poi (n) samples from the multinomial distribution Q. We let
Qn 1 = [ˆq1 1  . . .   ˆqS 1] denote the count of the ﬁrst set of N1 ∼ Poi (n) samples (normalized by
n)  and Qn 2 = [ˆq1 1  . . .   ˆqS 1] the second set of N2 ∼ Poi (n) samples. See Algorithm 1 for a
formal deﬁnition. Note that for the analysis we are collecting 2n samples in total on average. In
all the experiments  however  we apply our estimator without partitioning the samples. A major
challenge in achieving the minimax optimality is in handling the non-smoothness of the function
f (ˆqi; pi) ≜ [pi − eε ˆqi]+ at pi ≃ eε ˆqi. We use one set of samples to identify whether an out-
come i ∈ [S] is in the smooth regime (ˆqi 1 /∈ U (pi; c1  c2)) or not (ˆqi 1 ∈ U (pi; c1  c2))  with an
appropriately deﬁned set function:
U (p; c1  c2) ≜ +

if p ≤ c1eε ln n
 
n
  otherwise 

[0  (c1+c2) ln n

(5)

]

 

"e−εp −  c2e−εp ln n

n

n

  e−εp +  c2e−εp ln n

n

#

4

for c1 ≥ c2 > 0 and p ∈ [0  1]. The scaling of the interval is chosen carefully such that (a) it is large
enough for the probability of making a mistake on the which regime (pi  qi) falls into to vanishes
(Lemma 13); and (b) it is small enough for the variance of the polynomial approximation in the
non-smooth regime to match that of the other regimes (Lemma 14). In the smooth regime  we use
the plug-in estimator. In the non-smooth regime  we can improve the estimation error by using the
best polynomial approximation of f (x; p) = [p − eε x]+  which has a smaller bias:

(6)

DK(x; p) ≜ arg min
P∈polyK

max

˜x∈U (p;c1 c1)-- [p − eε ˜x]+ − P (˜x)--  

where polyK is the set of polynomial functions of degree at most K  and we approximate f (x; p)
in an interval U (p; c1  c1) ⊃ U (p; c1  c2) for any c1 > c2. Having this slack of c1 > c2 in the
approximation allows us to guarantee the approximation quality  even if the actual q is not exactly
in the non-smooth regime U (p; c1  c2). Once we have the polynomial approximation  we estimate
this polynomial function DK(x; p) from samples  using the uniformly minimum variance unbiased
estimator (MVUE).
There are several advantages that makes this two-step process attractive. As we use an unbiased
estimate of the polynomial  the bias is exactly the polynomial approximation error of DK(x; p) 

which scales as (1/K).(pi ln n)/n. Larger degree K reduces the approximation error  and larger

n reduces the support of the domain we apply the approximation to in U (p; c1  c1) (Lemma 14).
The variance is due to the sample estimation of the polynomial DK(x; p)  which scales as
(BKpi ln n)/n for some universal constant B (Lemma 14). Larger degree K increases the vari-
ance. We prescribe choosing K = c3 ln n for appropriate constant c3 to optimize the bias-variance
tradeoff in Algorithm 1. The methods of constructing the polynomial approximation DK(x; p) and
corresponding unbiased estimator ˜DK(x; p) are described in details at Appendix A.
Theorem 2. Suppose ln n ≤ C′(ln S − ε) for some constants C′  then there exist constants c1  c2
and c3 that only depends on C′ and ε such that

sup

P Q∈MS

EQ%--'dε K c1 c2 (P%Qn) − dε(P%Q)--2& ≲ eεS

n ln n

.

(7)

for K = c3 ln n and where 'dε K c1 c2 is deﬁned in Algorithm 1.

We provide a proof in Appendix C.3  and a matching lower bound in Theorem 3. Note that the
plug-in estimator in Theorem 1 achieves the parametric rate of 1/n. In the low-dimensional regime 
where we ﬁx S and grow n  this cannot be improved upon. To go beyond the parametric rate  we
need to consider a high-dimensional regime  where S grows with n. Hence  a condition similar to
ln n ≤ C′ ln S is necessary  although it might be possible to further relax it.
2.1.3 Matching minimax lower bound
In the high-dimensional regime  where S grows with n sufﬁciently fast  we can get a tighter lower
bound then Theorem 1  that matches the upper bound in Theorem 2. Again  supremum over Q is
necessary as there exists (P  Q) where it is trivial to achieve zero error  for any sample size (see
Section 2.1.1 for an example). For any given P we provide a minimax lower bound in the following.
A proof is provided in Appendix C.4.
Theorem 3. Suppose S ≥ 2 and there exists constants c  C1  C2 > 0 such that C1 ln S ≤ ln n ≤
C2 ln S and n ≥ c(eεS)/ln S  then
sup
Q∈MS

EQ%--'dε(P%Qn) − dε(P%Q)--2& ≳ eεS

where the inﬁmum is taken over all possible estimators.

!dε(P%Qn)

sup
P∈MS

.

n ln n

(8)

inf

2.2 Estimating dε(P%Q) from samples
We now consider the general case where P = PQ D and Q = PQ D′ are both unknown  and we
access them through samples. We propose sampling a random number of samples N1 ∼ Poi (n)
and N2 ∼ Poi (n) from each distribution  respectively. Deﬁne the empirical distributions Pn =

5

[ˆp1  . . .   ˆpS] and Qn = [ˆq1  . . .   ˆqS] as in the previous section. From the proof of Theorem 1  we get
the same sample complexity for the plug-in estimator: If n ≥ eεS and S ≥ 2  we have

sup

P Q∈MS

EQ%|dε(Pn%Qn) − dε(P%Q)|2& ≍

eεS
n

.

(9)

Using the same two-step process  we construct an estimator that improves upon this parametric rate
of plug-in estimator.

2.2.1 Estimator for dε(P%Q)
We present an estimator using similar techniques as in Algorithm 1  but there are several challenges
in moving to a multivariate case. The multivariate function f (x  y) = [x − eεy]+ is non-smooth in
a region x = eεy. We ﬁrst deﬁne a two-dimensional non-smooth set U (c1  c2) ⊂ [0  1] × [0  eε] as
(√p + √eεq)  p ∈ [0  1]  q ∈ [0  1]0   (10)

U (c1  c2) = +(p  eεq) : |p − eεq | ≤/ (c1 + c2) ln n

n

where 0 < c2 < c1. As before  the plug-in estimator is good enough in the smooth regime 
i.e. (p  eεq) /∈ U (c1  c2).
We construct a polynomial approximation of this function with order K  in this non-smooth regime.
We will set K = c3 ln n again to achieve the optimal tradeoff. We split the samples randomly into
four partitions  each having an independent and identical distribution of Poi (n) samples  two from
the multinomial distributions P and other two from Q. See Algorithm 2 for a formal deﬁnition. We
use one set of samples to identify the regime  and the other for estimation. We give a full description
and justiﬁcation of the algorithm in the longer version of this paper [23].

Algorithm 2 Differential Privacy (DP) estimator
Input: target privacy ε ∈ R+  query Q  neighboring databases (D D′) 
samples from PQ D and PQ D′  degree K ∈ Z+  constants c1  c2 ∈ R+  expected sample size 2n
Output: estimate 'dε K c1 c2 (Pn%Qn) of dε(PQ D%PQ D′ )
P ← PQ D  Q ← PQ D′
Draw four independent sample sizes: N1 1  N1 2  N2 1  N2 2 ∼ Poi (n)
i=1 ∈ [S]N1 1 and {Xi 2}N1 2
Sample from PQ D: {Xi 1}N1 1
Sample from PQ D′: {Yi 1}N2 1
i=1 ∈ [S]N2 1 and {Yi 2}N2 2
ˆpi j ← |{ℓ∈[N1 j ]:Xℓ j =i}|
and ˆqi j ← |{ℓ∈[N2 j ]:Yℓ j =i}|
Pn 1 ← [ˆp1 1  . . .   ˆpS 1]  Pn 2 ← [ˆp1 2  . . .   ˆpS 2]  Qn 1 ← [ˆq1 1  . . .   ˆqS 1] and Qn 2 ←
[ˆq1 2  . . .   ˆqS 2]
for i = 1 to S do
if ˆpi 1 − eε ˆqi 1 < −  (c1+c2) ln n
(.ˆpi 1 +.eε ˆqi 1)
if ˆpi 1 − eε ˆqi 1 >  (c1+c2) ln n
(.ˆpi 1 +.eε ˆqi 1)
if ˆpi 1 + eε ˆqi 1 < c1 ln n
if (ˆpi 1  eε ˆqi 1) ∈ U (c1  c2)  ˆpi 1 + eε ˆqi 1 ≥ c1 ln n

i=1 ∈ [S]N1 2
i=1 ∈ [S]N2 2
for all i ∈ [S] and j ∈ {1  2}

δi ←

0
ˆpi 2 − eε ˆqi 2
K (ˆpi 2  ˆqi 2)
K (ˆpi 2  ˆqi 2; ˆpi 1  ˆqi 1)

˜D(1)

˜D(2)

 

 

 
 

n

n

n

n

n

n

(1111)1111*

end for

'dε K c1 c2 (Pn%Qn) ← 0 ∨ (1 ∧$S

i=1 δi)

case 1: For (x  eεy) ∈ [0  (2c1 ln n)/n]2. A straightforward polynomial approximation of [x −
eεy]+ on [0  (2c1 ln n)/n]2 cannot achieve approximation error smaller than (1/K)((2c1 ln n)/n).
As K = c3 ln n  this gives a bias of 1/n for each symbol in [S]  resulting in total bias of S/n. This
requires n ≫ S to achieve arbitrary small error  as opposed to n ≫ S/ ln S which is what we are
targeting. This is due to the fact that we are requiring multivariate approximation  and the bias is
dominated by the worst case y for each x. If y is ﬁxed  as in the case of univariate approximation in
Lemma 14  the bias would have been (1/K).(eεy2c1 ln n)/n  with y = qi  where total bias scales
as.S/n ln n when summed over all symbols i.

6

Our strategy is to use the decomposition [x− eεy]+ = (√x +√eεy) [√x − √eεy]+. Each function
can be approximated up to a bias of (1/K).(ln n)/n  and the dominant term in the bias becomes
(1/K).(eεqi ln n)/n. This gives the desired bias. Concretely  we use two bivariate polynomials
uK(x  y) and vK(x  y) to approximate √x + √y and [√x − √y]+ in [0  1]2  respectively. Namely 
(x′ y′)∈[0 1]2---P (x′  y′) − (√x′ +.y′)---   and (11)
(x y)∈[0 1]2--uK(x  y) − (√x + √y)-- =
(x′ y′)∈[0 1]2---P (x′  y′) − [√x′ −.y′]+--- . (12)
(x y)∈[0 1]2--vK(x  y) − [√x − √y]+-- =

P∈poly2

P∈poly2

sup

sup

sup

sup

inf

inf

K

K

Denote h2K(x  y) = uK(x  y)vK(x  y) − uK(0  0)vK(0  0). Deﬁne
eεyn

2c1 ln n

h2K2 xn

2c1 ln n

 

2c1 ln n3  

n

D(1)

K (x  y) =

for (x  eεy) ∈ [0  (2c1 ln n)/n]2. In practice  one can use the best Chebyshev polynomial expansion
to achieve the same uniform error rate  efﬁciently [24].
case 2: For (x  eεy) ∈ U (c1  c1) and x + eεy ≥ (c1 ln n)/2n. We utilize the best polynomial
approximation of |t| on [−1  1] with order K. Denote it as RK(t) =$K
rjW −j+1(eεy − x)j +

K (x  y; ˆpi 1  ˆqi 1) =

j=0 rjtj. Deﬁne

x − eεy

D(2)

(14)

1
2

2

 

where W =.(8c1 ln n)/n4.ˆpi 1 + eε ˆqi 15. Finally  we use second part of samples to construct

K (x  y; ˆpi 1  ˆqi 1) by Lemma 11 and 12 . Namely 

unbiased estimator for D(1)

K (x  y) and D(2)

K!j=0

(13)

(15)

(16)

E" ˜D(1)

K (ˆpi 2  ˆqi 2)# = D(1)
K (ˆpi 2  ˆqi 2; ˆpi 1  ˆqi 1)|ˆpi 1  ˆqi 1# = D(2)

E" ˜D(2)

K (p  q)   and

K (p  q; ˆpi 1  ˆqi 1) .

The formula for the unbiased estimators can be found in the Appendix in Eqs. (180) and (187).

2.2.2 Minimax optimal upper bound
We provide an upper bound on the error achieved by the proposed estimator. The analysis uses
similar techniques as the proof of Theorem 2. We provide a proof in Appendix C.5.
Theorem 4. Suppose there exists a constant C > 0 such that ln n ≤ C ln S. Then there exist
constants c1  c2 and c3 that only depends on C and ε such that

sup

P Q∈MS

EP×Q%--'dε K c1 c2 (Pn%Qn) − dε(P%Q)--2& ≲ eεS

n ln n

for K = c3 ln n where 'dε K c1 c2 is deﬁned in Algorithm 2.

It follows from the proof of Theorem 3 that

 

(17)

inf

!dε(Pn%Qn)

sup

P Q∈MS

EP×Q%--'dε(Pn%Qn) − dε(P%Q)--2& ≳ eεS

n ln n

.

Together  the above upper and lower bounds prove that the proposed estimator in Algorithm 2 is
minimax optimal and cannot be improved upon in terms of sample complexity. We want to em-
phasize that we do not require to know the size of the support S  as opposed to exiting methods in
[11]  which requires collecting enough samples to identify the support. Comparing it to the error
rate of plug-in estimator in Theorem 1  this minimax rate of eεS/(n ln n) demonstrates the effective
sample size ampliﬁcation holds; with n samples  a sophisticated estimator can achieve the error rate
equivalent to a plug-in estimator with n ln n samples.

7

3 Experiments

We present the experiment details in Appendix B and the code to reproduce our experiments at
https://github.com/xiyangl3/adp-estimator. Figure 1 (a) illustrates the Mean Square Er-
ror (MSE) for estimating dε(P%Q) between uniform distribution P and Zipf distribution Q  where
the support size is ﬁxed to be S = 100  Zipf(α) ∝ 1/iα  and α = −0.6 for i ∈ [S]. The ε is ﬁxed
to be ε = 0.4. This suggests that the Algorithm 2 consistently improves upon the plug-in estimator 
as predicted by Theorem 4.
We demonstrate how we can use Algorithm 2 to detect mechanisms with false claim of DP guaran-
tees on four types of mechanisms: Report Noisy Max [25]  Histogram [26]  Sparse Vector Technique
[8] and Mixture of Truncated Geometric Mechanism. We closely following the experimental set-up
of [11]  and the settings and discussions are provided in Appendix B.2.
In [11]  the test query and databases deﬁning (Q D D′) are chosen by some heuristics. Figure 1 (b)
and (c) show (ε  δ) regions for the variations of noisy max mechanisms for privacy budget ε0 = 0.3.
From the ﬁgures  one can easily conﬁrm that RNM+Lap and RNM+Exp have ˆδ ≫ 0 at ε = 0.3 (blue
lines)  conﬁrming that these two mechanisms do not guarantee the claimed (0.3  0)-DP  as known
in the literature [11]. For those faulty mechanisms  Algorithm 2 also provides a certiﬁcate in the
form of a set T ⊆ [S] such that
[P (T ) − eεQ(T )]+ − δ > 0. With the setting of privacy budget
ε0 = 0.5  Figure 1 (d) shows that the incorrect histogram with incorrect Lap(ε0) noise is likely to
be (1/ε0  0)-DP as known from [11]. Both mechanisms claim (0.5  0)-DP  but the ﬁgure shows that
the incorrect mechanism ensures (1/0.5  0)-DP instead. Figure 1 (e) shows that SVT is likely to be
(ε0  0)-DP with ε0 = 0.5. However  iSVT1  iSVT2  and iSVT3 do not meet the claimed (0.5  0)-DP.
Figure 1 (f) conﬁrms that MTGM satisﬁed the claimed (ε0  δ0) differential privacy.

MSE

ˆδ

(a)

sample size n

(d)

ε

ˆδ

ˆδ

(b)

ε
(e)

ε

ˆδ

ˆδ

(c)

ε
(f)

ε

Figure 1: (a) shows the proposed minimax optimal estimator in Algorithm 2 consistently improves
upon the plug-in estimator on synthetic data. Each data point represents 100 random trials  with
standard error (SE) error bars smaller than the plot marker. (b)  (c)  (d)  (e)  (f) estimate ˆδ from Al-
gorithm 2 of δ given ε  and privacy budget ε0 for DP mechanisms. Each point is showing an average
over 10 random trials with standard error. The red lines represent the original correct mechanisms.
Algorithm 2 allows us to detect violation of claimed DP guarantees.

4 Conclusion

We investigate the fundamental trade-off between accuracy and sample size in estimating differential
privacy guarantees from a black-box access to a purportedly private mechanism. Such a data-driven

8

approach to verifying privacy guarantees will allow us to hold accountable the mechanisms in the
wild that are not faithful to the claimed privacy guarantees  and help ﬁnd and ﬁx bugs in either
the design or the implementation. To this end  we propose a polynomial approximation based ap-
proach to estimate the differential privacy guarantees. We show that in the high-dimensional regime 
the proposed estimator achieves sample size ampliﬁcation effect. Compared to the parametric rate
achieved by the plug-in estimator  we achieve a factor of ln n gain in the sample size. A matching
lower bound proves the minimax optimality of our approach. Here  we list important remaining
challenges that are outside the scope of this paper.
Since the introduction of differential privacy  there have been several innovative notions of privacy 
such as pufferﬁsh  concentrated DP  zCDP  and Renyi DP  proposed in [27  28  29  30]. Our estimator
builds upon the fact that differential privacy guarantee is a divergence between two random outputs.
This is no longer true for the other notions of privacy  which makes it more challenging.
Characterizing the fundamental tradeoff for continuous mechanisms is an important problem  as
several popular mechanisms output continuous random variables  such as Laplacian and Gaussian
mechanisms. One could use non-parametric estimators such as k-nearest neighbor methods and
kernel methods  popular for estimating information theoretic quantities and divergences [31  32  33 
34]. Further  when the output is a mixture of discrete and continuous variables  recent advances in
estimating mutual information for mixed variables provide a guideline for such complex estimation
process [35].
There is a fundamental connection between differential privacy and ROC curves  as investigated
in [30  36  37]. Binary hypothesis testing and ROC curves provide an important measure of per-
formance in generative adversarial networks (GAN) [38]. This fundamental connection between
differential privacy and GAN was ﬁrst investigated in [39]  where it was used to provide an implicit
bias for mitigating mode collapse  a fundamental challenge in training GANs. A DP estimator  like
the one we proposed  provides valuable tools to measure performance of GANs. The main challenge
is that GAN outputs are extremely high-dimensional (popular examples being 1  024 × 1  024 ×3
dimensional images). Non-parametric methods have exponential dependence in the dimension  ren-
dering them useless. Even some recent DP approaches have output dimensions that are equally
large [40]. We need fundamentally different approach to deal with such high dimensional continu-
ous mechanisms.
We considered a setting where we create synthetic databases D and D′ and test the guarantees of
a mechanism of interest.
Instead  [12] assumes we do not have such a control  and the privacy
of the real databases used in the testing needs to also be preserved. It is proven that one cannot
test the privacy guarantee of a mechanism without revealing the contents of the test databases. Such
fundamental limits suggest that the samples used in estimating DP needs to be destroyed after the es-
timation. However  the estimated dε(PQ D%PQ D′ ) still leaks some information about the databases
used  although limited. This is related to a challenging task of designing mechanisms with (ε  δ)-DP
guarantees when (ε  δ) also depends on the databases. Without answering any queries  just pub-
lishing the guarantee of the mechanism on a set of databases reveal something about the database.
Detection and estimation under such complicated constraints is a challenging open question.

Acknowledgement

This work is partially supported by NSF awards CNS-1527754  CNS-1705007  CCF-1927712  RI-
1929955 and generous gift from Google.

References
[1] John Abowd. The u.s. census bureau adopts differential privacy. KDD Invited Talk  2018.

[2] Úlfar Erlingsson  Vasyl Pihur  and Aleksandra Korolova. Rappor: Randomized aggregatable
privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC conference on
computer and communications security  pages 1054–1067. ACM  2014.

[3] Giulia Fanti  Vasyl Pihur  and Úlfar Erlingsson. Building a rappor with the unknown: Privacy-
preserving learning of associations and data dictionaries. Proceedings on Privacy Enhancing
Technologies  2016(3):41–61  2016.

9

[4] Apple Differential Privacy Team. Learning with privacy at scale. Apple Machine Learning

Journal  2017.

[5] Bolin Ding  Janardhan Kulkarni  and Sergey Yekhanin. Collecting telemetry data privately. In

Advances in Neural Information Processing Systems  pages 3571–3580  2017.

[6] Gilles Barthe  Boris Köpf  Federico Olmedo  and Santiago Zanella Beguelin. Probabilistic

relational reasoning for differential privacy. ACM SIGPLAN Notices  47(1):97–110  2012.

[7] Yuxin Wang  Zeyu Ding  Guanhong Wang  Daniel Kifer  and Danfeng Zhang. Proving differ-

ential privacy with shadow execution. arXiv preprint arXiv:1903.12254  2019.

[8] Min Lyu  Dong Su  and Ninghui Li. Understanding the sparse vector technique for differential

privacy. Proceedings of the VLDB Endowment  10(6):637–648  2017.

[9] Yan Chen and Ashwin Machanavajjhala. On the privacy properties of variants on the sparse

vector technique. arXiv preprint arXiv:1508.07306  2015.

[10] Cynthia Dwork. Differential privacy. Encyclopedia of Cryptography and Security  pages 338–

340  2011.

[11] Zeyu Ding  Yuxin Wang  Guanhong Wang  Danfeng Zhang  and Daniel Kifer. Detecting viola-
tions of differential privacy. In Proceedings of the 2018 ACM SIGSAC Conference on Computer
and Communications Security  pages 475–489. ACM  2018.

[12] Anna C Gilbert and Audra McMillan. Property testing for differential privacy. In 2018 56th
Annual Allerton Conference on Communication  Control  and Computing (Allerton)  pages
249–258. IEEE  2018.

[13] Kashyap Dixit  Madhav Jha  Sofya Raskhodnikova  and Abhradeep Thakurta. Testing the
lipschitz property over product distributions with applications to data privacy. In Theory of
Cryptography Conference  pages 418–436. Springer  2013.

[14] Paul Valiant and Gregory Valiant. Estimating the unseen: improved estimators for entropy and
other properties. In Advances in Neural Information Processing Systems  pages 2157–2165 
2013.

[15] Jiantao Jiao  Kartik Venkat  Yanjun Han  and Tsachy Weissman. Minimax estimation of func-
tionals of discrete distributions. IEEE Transactions on Information Theory  61(5):2835–2885 
2015.

[16] Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via
best polynomial approximation. IEEE Transactions on Information Theory  62(6):3702–3720 
2016.

[17] Yihong Wu and Pengkun Yang. Chebyshev polynomials  moment matching  and optimal esti-

mation of the unseen. The Annals of Statistics  47(2):857–883  2019.

[18] Yanjun Han  Jiantao Jiao  and Tsachy Weissman. Minimax estimation of discrete distributions

under ℓ1 loss. IEEE Transactions on Information Theory  61(11):6343–6354  2015.

[19] Jayadev Acharya  Alon Orlitsky  Ananda Theertha Suresh  and Himanshu Tyagi. Estimating
rényi entropy of discrete distributions. IEEE Transactions on Information Theory  63(1):38–
56  2017.

[20] Yanjun Han  Jiantao Jiao  and Tsachy Weissman. Minimax rate-optimal estimation of diver-

gences between discrete distributions. arXiv preprint arXiv:1605.09124  2016.

[21] Jiantao Jiao  Yanjun Han  and Tsachy Weissman. Minimax estimation of the l1 distance. IEEE

Transactions on Information Theory  2018.

[22] Constantinos Daskalakis  Gautam Kamath  and John Wright. Which distribution distances are
sublinearly testable? In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
Discrete Algorithms  pages 2747–2764. SIAM  2018.

10

[23] Xiyang Liu and Sewoong Oh. Minimax rates of estimating approximate differential privacy.

arXiv preprint arXiv:1905.10335  2019.

[24] Hrushikesh N Mhaskar  Paul Nevai  and Eugene Shvarts. Applications of classical approxima-
tion theory to periodic basis function networks and computational harmonic analysis. Bulletin
of Mathematical Sciences  3(3):485–549  2013.

[25] Cynthia Dwork  Aaron Roth  et al. The algorithmic foundations of differential privacy. Foun-

dations and Trends R© in Theoretical Computer Science  9(3–4):211–407  2014.

[26] Cynthia Dwork. Differential privacy.

In Proceedings of the 33rd International Conference
on Automata  Languages and Programming - Volume Part II  ICALP’06  pages 1–12  Berlin 
Heidelberg  2006. Springer-Verlag.

[27] Daniel Kifer and Ashwin Machanavajjhala. Pufferﬁsh: A framework for mathematical privacy

deﬁnitions. ACM Transactions on Database Systems (TODS)  39(1):3  2014.

[28] Cynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint

arXiv:1603.01887  2016.

[29] Shuang Song  Yizhen Wang  and Kamalika Chaudhuri. Pufferﬁsh privacy mechanisms for
correlated data. In Proceedings of the 2017 ACM International Conference on Management of
Data  pages 1291–1306. ACM  2017.

[30] Peter Kairouz  Sewoong Oh  and Pramod Viswanath. The composition theorem for differential

privacy. IEEE Transactions on Information Theory  63(6):4037–4049  2017.

[31] Thomas B Berrett  Richard J Samworth  and Ming Yuan. Efﬁcient multivariate entropy esti-

mation via k-nearest neighbour distances. The Annals of Statistics  47(1):288–318  2019.

[32] Weihao Gao  Sewoong Oh  and Pramod Viswanath. Demystifying ﬁxed k-nearest neighbor

information estimators. IEEE Transactions on Information Theory  64(8):5629–5661  2018.

[33] Weihao Gao  Sewoong Oh  and Pramod Viswanath. Breaking the bandwidth barrier: Geo-
metrical adaptive entropy estimation. In Advances in Neural Information Processing Systems 
pages 2460–2468  2016.

[34] Jiantao Jiao  Weihao Gao  and Yanjun Han. The nearest neighbor information estimator is
adaptively near minimax rate-optimal. In Advances in neural information processing systems 
pages 3156–3167  2018.

[35] Weihao Gao  Sreeram Kannan  Sewoong Oh  and Pramod Viswanath. Estimating mutual in-
In Advances in Neural Information Processing

formation for discrete-continuous mixtures.
Systems  pages 5986–5997  2017.

[36] Peter Kairouz  Sewoong Oh  and Pramod Viswanath. Extremal mechanisms for local differen-

tial privacy. In Advances in neural information processing systems  pages 2879–2887  2014.

[37] Peter Kairouz  Sewoong Oh  and Pramod Viswanath. Secure multi-party differential privacy.

In Advances in neural information processing systems  pages 2008–2016  2015.

[38] Mehdi SM Sajjadi  Olivier Bachem  Mario Lucic  Olivier Bousquet  and Sylvain Gelly. Assess-
ing generative models via precision and recall. In Advances in Neural Information Processing
Systems  pages 5228–5237  2018.

[39] Zinan Lin  Ashish Khetan  Giulia Fanti  and Sewoong Oh. Pacgan: The power of two samples
in generative adversarial networks. In Advances in Neural Information Processing Systems 
pages 1498–1507  2018.

[40] Chong Huang  Peter Kairouz  Xiao Chen  Lalitha Sankar  and Ram Rajagopal. Generative

adversarial privacy. arXiv preprint arXiv:1807.05306  2018.

[41] T. A Driscoll  N. Hale  and L. N. Trefethen. Chebfun Guide. Pafnuty Publications  2014.

11

[42] Ben Stoddard  Yan Chen  and Ashwin Machanavajjhala. Differentially private algorithms for

empirical machine learning. arXiv preprint arXiv:1411.5428  2014.

[43] Rui Chen  Qian Xiao  Yu Zhang  and Jianliang Xu. Differentially private high-dimensional
data publication via sampling-based inference.
In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  pages 129–138. ACM 
2015.

[44] Jaewoo Lee and Christopher W Clifton. Top-k frequent itemsets via differentially private
fp-trees. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining  pages 931–940. ACM  2014.

[45] Arpita Ghosh  Tim Roughgarden  and Mukund Sundararajan. Universally utility-maximizing

privacy mechanisms. SIAM Journal on Computing  41(6):1673–1693  2012.

[46] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms

and probabilistic analysis. 2005.

[47] Z Ditzian. Totik  v.: Moduli of smoothness. Springer series in computational math  Springer-

Verleg  1987.

[48] Ronald A DeVore and George G Lorentz. Constructive approximation  volume 303. Springer

Science & Business Media  1993.

[49] T Tony Cai  Mark G Low  et al. Testing composite hypotheses  hermite polynomials and
optimal estimation of a nonsmooth functional. The Annals of Statistics  39(2):1012–1041 
2011.

[50] A.B. Tsybakov.

Introduction to Nonparametric Estimation. Springer Series in Statistics.

Springer New York  2008.

[51] Serge Bernstein. Sur la meilleure approximation de| x| par des polynomes de degrés donnés.

Acta Mathematica  37(1):1–57  1914.

12

,Xiyang Liu
Sewoong Oh