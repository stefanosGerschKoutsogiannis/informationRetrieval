2013,Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis,The performance of standard algorithms for Independent Component Analysis quickly deteriorates under the addition of Gaussian noise. This is partially due to a common first step that typically consists of whitening  i.e.  applying Principal Component Analysis (PCA) and rescaling the components to have identity covariance  which is not invariant under Gaussian noise.   In our paper we develop the first practical algorithm for Independent Component Analysis that is provably invariant under Gaussian noise. The two main contributions of this work are as follows: 1. We develop and implement a more efficient version of a Gaussian noise invariant decorrelation (quasi-orthogonalization) algorithm using Hessians of the cumulant functions. 2. We propose a very simple and efficient fixed-point GI-ICA (Gradient Iteration ICA) algorithm  which is compatible with quasi-orthogonalization  as well as with the usual PCA-based whitening in the noiseless case.  The algorithm is based on a special form of gradient iteration (different from gradient descent).   We provide an analysis of our algorithm demonstrating fast convergence following from the basic properties of cumulants. We also present a number of experimental comparisons with the existing methods  showing superior results on noisy data and very competitive performance in the noiseless case.,Fast Algorithms for Gaussian Noise Invariant

Independent Component Analysis

James Voss

Ohio State University

Computer Science and Engineering 
2015 Neil Avenue  Dreese Labs 586.

Columbus  OH 43210

vossj@cse.ohio-state.edu

Luis Rademacher

Ohio State University

Computer Science and Engineering 
2015 Neil Avenue  Dreese Labs 495.

Columbus  OH 43210

lrademac@cse.ohio-state.edu

Mikhail Belkin

Ohio State University

Computer Science and Engineering 
2015 Neil Avenue  Dreese Labs 597.

Columbus  OH 43210

mbelkin@cse.ohio-state.edu

Abstract

The performance of standard algorithms for Independent Component Analysis
quickly deteriorates under the addition of Gaussian noise. This is partially due
to a common ﬁrst step that typically consists of whitening  i.e.  applying Prin-
cipal Component Analysis (PCA) and rescaling the components to have identity
covariance  which is not invariant under Gaussian noise.
In our paper we develop the ﬁrst practical algorithm for Independent Component
Analysis that is provably invariant under Gaussian noise. The two main contribu-
tions of this work are as follows:
1. We develop and implement an efﬁcient  Gaussian noise invariant decorrelation
(quasi-orthogonalization) algorithm using Hessians of the cumulant functions.
2. We propose a very simple and efﬁcient ﬁxed-point GI-ICA (Gradient Iteration
ICA) algorithm  which is compatible with quasi-orthogonalization  as well as with
the usual PCA-based whitening in the noiseless case. The algorithm is based on
a special form of gradient iteration (different from gradient descent). We provide
an analysis of our algorithm demonstrating fast convergence following from the
basic properties of cumulants. We also present a number of experimental compar-
isons with the existing methods  showing superior results on noisy data and very
competitive performance in the noiseless case.

Introduction and Related Works

1
In the Blind Signal Separation setting  it is assumed that observed data is drawn from an unknown
distribution. The goal is to recover the latent signals under some appropriate structural assumption.
A prototypical setting is the so-called cocktail party problem: in a room  there are d people speaking
simultaneously and d microphones  with each microphone capturing a superposition of the voices.
The objective is to recover the speech of each individual speaker. The simplest modeling assumption
is to consider each speaker as producing a signal that is a random variable independent of the others 
and to take the superposition to be a linear transformation independent of time. This leads to the
following formalization: We observe samples from a random vector x distributed according to the
equation x = As + b + η where A is a linear mixing matrix  b ∈ Rd is a constant vector  s is a
latent random vector with independent coordinates  and η is an unknown random noise independent

1

of s. For simplicity  we assume A ∈ Rd×d is square and of full rank. The latent components of s
are viewed as containing the information describing the makeup of the observed signal (voices of
individual speakers in the cocktail party setting). The goal of Independent Component Analysis is
to approximate the matrix A in order to recover the latent signal s. In practice  most methods ignore
the noise term  leaving the simpler problem of recovering the mixing matrix A when x = As is
observed.
Arguably the two most widely used ICA algorithms are FastICA [13] and JADE [6]. Both of these
algorithms are based on a two step process:
(1) The data is centered and whitened  that is  made to have identity covariance matrix. This is
typically done using principal component analysis (PCA) and rescaling the appropriate components.
In the noiseless case this procedure orthogonalizes and rescales the independent components and
thus recovers A up to an unknown orthogonal matrix R.
(2) Recover the orthogonal matrix R.
Most practical ICA algorithms differ only in the second step. In FastICA  various objective functions
are used to perform a projection pursuit style algorithm which recovers the columns of R one at a
time. JADE uses a fourth-cumulant based technique to simultaneously recover all columns of R.
Step 1 of ICA is affected by the addition of a Gaussian noise. Even if the noise is white (has a scalar
times identity covariance matrix) the PCA-based whitening procedure can no longer guarantee the
whitening of the underlying independent components. Hence  the second step of the process is no
longer justiﬁed. This failure may be even more signiﬁcant if the noise is not white  which is likely to
be the case in many practical situations. Recent theoretical developments (see  [2] and [3]) consider
the case where the noise η is an arbitrary (not necessarily white) additive Gaussian variable drawn
independently from s.
In [2]  it was observed that certain cumulant-based techniques for ICA can still be applied for the
second step if the underlying signals can be orthogonalized.1 Orthogonalization of the latent sig-
nals (quasi-orthogonalization) is a signiﬁcantly less restrictive condition as it does not force the
underlying signal to have identity covariance (as in whitening in the noiseless case). In the noisy
setting  the usual PCA cannot achieve quasi-orthogonalization as it will whiten the mixed signal  but
not the underlying components. In [3]  we show how quasi-orthogonalization can be achieved in a
noise-invariant way through a method based on the fourth-order cumulant tensor. However  a direct
implementation of that method requires estimating the full fourth-order cumulant tensor  which is
computationally challenging even in relatively low dimensions. In this paper we derive a practical
version of that algorithm based on directional Hessians of the fourth univariate cumulant  thus re-
ducing the complexity dependence on the data dimensionality from d4 to d3  and also allowing for
a fully vectorized implementation.
We also develop a fast and very simple gradient iteration (not to be confused with gradient descent)
algorithm  GI-ICA  which is compatible with the quasi-orthogonalization step and can be shown to
have convergence of order r − 1  when implemented using a univariate cumulant of order r. For the
cumulant of order four  commonly used in practical applications  we obtain cubic convergence. We
show how these convergence rates follow directly from the properties of the cumulants  which sheds
some light on the somewhat surprising cubic convergence seen in fourth-order based ICA methods
[13  18  22]. The update step has complexity O(N d) where N is the number of samples  giving a
total algorithmic complexity of O(N d3) for step 1 and O(N d2t) for step 2  where t is the number
of iterations for convergence in the gradient iteration.
Interestingly  while the techniques are quite different  our gradient iteration algorithm turns out to
be closely related to Fast ICA in the noiseless setting  in the case when the data is whitened and the
cumulants of order three or four are used. Thus  GI-ICA can be viewed as a generalization (and a
conceptual simpliﬁcation) of Fast ICA for more general quasi-orthogonalized data.
We present experimental results showing superior performance in the case of data contaminated
by Gaussian noise and very competitive performance for clean data. We also note that the GI-
ICA algorithms are fast in practice  allowing us to process (decorrelate and detect the independent

1This process of orthogonalizing the latent signals was called quasi-whitening in [2] and later in [3]. How-
ever  this conﬂicts with the deﬁnition of quasi-whitening given in [12] which requires the latent signals to be
whitened. To avoid the confusion we will use the term quasi-orthogonalization for the process of orthogonal-
izing the latent signals.

2

components) 100 000 points in dimension 5 in well under a second on a standard desktop computer.
Our Matlab implementation of GI-ICA is available for download at http://sourceforge.
net/projects/giica/.
Finally  we observe that our method is partially compatible with the robust cumulants introduced
in [20]. We brieﬂy discuss how GI-ICA can be extended using these noise-robust techniques for
ICA to reduce the impact of sparse noise.
The paper is organized as follows. In section 2  we discuss the relevant properties of cumulants 
and discuss results from prior work which allows for the quasi-orthogonalization of signals with
non-zero fourth cumulant. In section 3  we discuss the connection between the fourth-order cumu-
lant tensor method for quasi-orthogonalization discussed in section 2 with Hessian-based techniques
seen in [2] and [11]. We use this connection to create a more computationally efﬁcient and prac-
tically implementable version of the quasi-orthogonalization algorithm discussed in section 2. In
section 4  we discuss new  fast  projection-pursuit style algorithms for the second step of ICA which
are compatible with quasi-orthogonalization. In order to simplify the presentation  all algorithms
are stated in an abstract form as if we have exact knowledge of required distribution parameters.
Section 5 discusses the estimators of required distribution parameters to be used in practice. Section
6 discusses numerical experiments demonstrating the applicability of our techniques.
Related Work. The name Independent Component Analysis refers to a broad range of algorithms
addressing the blind signal separation problem as well as its variants and extensions. There is an
extensive literature on ICA in the signal processing and machine learning communities due to its
applicability to a variety of important practical situations. For a comprehensive introduction see
the books [8  14]. In this paper we develop techniques for dealing with noisy data by introducing
new and more efﬁcient techniques for quasi-orthogonalization and subsequent component recovery.
The quasi-orthogonalization step was introduced in [2]  where the authors proposed an algorithm
for the case when the fourth cumulants of all independent components are of the same sign. A
general algorithm with complete theoretical analysis was provided in [3]. That algorithm required
estimating the full fourth-order cumulant tensor.
We note that Hessian based techniques for ICA were used in [21  2  11]  with [11] and [2] using the
Hessian of the fourth-order cumulant. The papers [21] and [11] proposed interesting randomized
one step noise-robust ICA algorithms based on the cumulant generating function and the fourth
cumulant respectively in primarily theoretical settings. The gradient iteration algorithm proposed is
closely related to the work [18]  which provides a gradient-based algorithm derived from the fourth
moment with cubic convergence to learn an unknown parallelepiped in a cryptographic setting. For
the special case of the fourth cumulant  the idea of gradient iteration has appeared in the context
of FastICA with a different justiﬁcation  see e.g. [16  Equation 11 and Theorem 2]. We also note
the work [12]  which develops methods for Gaussian noise-invariant ICA under the assumption that
the noise parameters are known. Finally  there are several papers that considered the problem of
performing PCA in a noisy framework.
[5] gives a provably robust algorithm for PCA under a
sparse noise model. [4] performs PCA robust to white Gaussian noise  and [9] performs PCA robust
to white Gaussian noise and sparse noise.

2 Using Cumulants to Orthogonalize the Independent Components
Properties of Cumulants: Cumulants are similar to moments and can be expressed in terms of
certain polynomials of the moments. However  cumulants have additional properties which allow
independent random variables to be algebraically separated. We will be interested in the fourth order
multi-variate cumulants  and univariate cumulants of arbitrary order. Denote by Qx the fourth order
cumulant tensor for the random vector x. So  (Qx)ijkl is the cross-cumulant between the random
variables xi  xj  xk  and xl  which we alternatively denote as Cum(xi  xj  xk  xl). Cumulant tensors
are symmetric  i.e. (Qx)ijkl is invariant under permutations of indices. Multivariate cumulants have
the following properties (written in the case of fourth order cumulants):
1. (Multilinearity) Cum(αxi  xj  xk  xl) = α Cum(xi  xj  xk  xl) for random vector x and scalar α.
If y is a random variable  then Cum(xi +y  xj  xk  xl) = Cum(xi  xj  xk  xl)+Cum(y  xj  xk  xl).
2. (Independence) If xi and xj are independent random variables  then Cum(xi  xj  xk  xl) = 0.
When x and y are independent  Qx+y = Qx + Qy.
3. (Vanishing Gaussian) Cumulants of order 3 and above are zero for Gaussian random variables.

3

The ﬁrst order cumulant is the mean  and the second order multivariate cumulant is the covariance
matrix. We will denote by κr(x) the order-r univariate cumulant  which is equivalent to the cross-
cumulant of x with itself r times: κr(x) := Cum(x  x  . . .   x) (where x appears r times). Univariate
r-cumulants are additive for independent random variables  i.e. κr(x + y) = κr(x) + κr(y)  and
homogeneous of degree r  i.e. κr(αx) = αrκr(x).
Quasi-Orthogonalization Using Cumulant Tensors. Recalling our original notation  x = As +
b + η gives the generative ICA model. We deﬁne an operation of fourth-order tensors on matrices:
For Q ∈ Rd×d×d×d and M ∈ Rd×d  Q(M ) is the matrix such that

d(cid:88)

d(cid:88)

Q(M )ij :=

Qijklmlk .

(1)

We can use this operation to orthogonalize the latent random signals.
Deﬁnition 2.1. A matrix W is called a quasi-orthogonalization matrix if there exists an orthogonal
matrix R and a nonsingular diagonal matrix D such that W A = RD.

k=1

l=1

We will need the following results from [3]. Here we use Aq to denote the qth column of A.
Lemma 2.2. Let M ∈ Rd×d be an arbitrary matrix. Then  Qx(M ) = ADAT where D is a
diagonal matrix with entries dqq = κ4(sq)AT
Theorem 2.3. Suppose that each component of s has non-zero fourth cumulant. Let M = Qx(I) 
and let C = Qx(M−1). Then C = ADAT where D is a diagonal matrix with entries dqq =
1/(cid:107)Aq(cid:107)2
2. In particular  C is positive deﬁnite  and for any factorization BBT of C  B−1 is a quasi-
orthogonalization matrix.

q M Aq.

3 Quasi-Orthogonalization using Cumulant Hessians
We have seen in Theorem 2.3 a tensor-based method which can be used to quasi-orthogonalize
observed data. However  this method na¨ıvely requires the estimation of O(d4) terms from data.
There is a connection between the cumulant Hessian-based techniques used in ICA [2  11] and
the tensor-based technique for quasi-orthogonalization described in Theorem 2.3 that allows the
tensor-method to be rewritten using a series of Hessian operations. We make this connection precise
below. The Hessian version requires only O(d3) terms to be estimated from data and simpliﬁes the
computation to consist of matrix and vector operations.
Let Hu denote the Hessian operator with respect to a vector u ∈ Rd. The following lemma connects
Hessian methods with our tensor-matrix operation (a special case is discussed in [2  Section 2.1]).
Lemma 3.1. Hu(κ4(uT x)) = ADAT where dqq = 12(uT Aq)2κ4(sq).

q (uuT )Aq). By com-
In Lemma 3.1  the diagonal entries can be rewritten as dqq = 12κ4(sq)(AT
paring with Lemma 2.2  we see that applying Qx against a symmetric  rank one matrix uuT can be
12Hu(κ4(uT x)). This formula extends
rewritten in terms of the Hessian operations: Qx(uuT ) = 1
to arbitrary symmetric matrices by the following Lemma.
Lemma 3.2. Let M be a symmetric matrix with eigen decomposition U ΛU T such that U =
(u1  u2  . . .   ud) and Λ = diag(λ1  λ2  . . .   λd). Then  Qx(M ) = 1
12
The matrices I and M−1 in Theorem 2.3 are symmetric. As such  the tensor-based method for
quasi-orthogonalization can be rewritten using Hessian operations. This is done in Algorithm 1.

(cid:80)d
i=1 λiHuiκ4(uT

i x).

4 Gradient Iteration ICA
In the preceding sections  we discussed techniques to quasi-orthogonalize data. For this sec-
tion  we will assume that quasi-orthogonalization is accomplished  and discuss deﬂationary ap-
proaches that can quickly recover the directions of the independent components. Let W be a quasi-
orthogonalization matrix. Then  deﬁne y := W x = W As + W η. Note that since η is Gaussian
noise  so is W η. There exists a rotation matrix R and a diagonal matrix D such that W A = RD.
Let ˜s := Ds. The coordinates of ˜s are still independent random variables. Gaussian noise makes
recovering the scaling matrix D impossible. We aim to recover the rotation matrix R.

4

(cid:80)d
i=1 Huκ4(uT x)|u=ei. See Equation (4) for the estimator.
i=1 λiHuκ4(uT x)|u=Ui. See Equation (4) for the estimator.

Algorithm 1 Hessian-based algorithm to generate a quasi-orthogonalization matrix.
1: function FINDQUASIORTHOGONALIZATIONMATRIX(x)
2:
3:
4:
Factorize C as BBT .
5:
return B−1
6:
7: end function

Let M = 1
12
Let U ΛU T give the eigendecomposition of M−1

Let C =(cid:80)d

To see why recovery of D is impossible  we note that a white Gaussian random variable η1 has
independent components. It is impossible to distinguish between the case where η1 is part of the
signal  i.e. W A(s + η1) + W η  and the case where Aη1 is part of the additive Gaussian noise  i.e.
W As + W (Aη1 + η)  when s  η1  and η are drawn independently. In the noise-free ICA setting  the
latent signal is typically assumed to have identity covariance  placing the scaling information in the
columns of A. The presence of additive Gaussian noise makes recovery of the scaling information
impossible since the latent signals become ill-deﬁned. Following the idea popularized in FastICA 
we will discuss a deﬂationary technique to recover the columns of R one at a time.
Fast Recovery of a Single Independent Component. In the deﬂationary approach  a function f is
ﬁxed that acts upon a directional vector u ∈ Rd. Based on some criterion (typically maximization
or minimization of f)  an iterative optimization step is performed until convergence. This technique
was popularized in FastICA  which is considered fast for the following reasons:
1. As an approximate Newton method  FastICA requires computation of ∇uf and a quick-to-
compute estimate of (Hu(f ))−1 at each iterative step. Due to the estimate  the computation runs in
O(N d) time  where N is the number of samples.
2. The iterative step in FastICA has local quadratic order convergence using arbitrary functions  and
global cubic-order convergence when using the fourth cumulant [13].
We note that cubic convergence rates are not unique to FastICA and have been seen using gradient
descent (with the correct step-size) when choosing f as the fourth moment [18]. Our proposed
deﬂationary algorithm will be comparable with FastICA in terms of computational complexity  and
the iterative step will take on a conceptually simpler form as it only relies on ∇uκr. We provide a
derivation of fast convergence rates that relies entirely on the properties of cumulants. As cumulants
are invariant with respect to the additive Gaussian noise  the proposed methods will be admissible
for both standard and noisy ICA.
While cumulants are essentially unique with the additivity and homogeneity properties [17] when
no restrictions are made on the probability space  the preprocessing step of ICA gives additional
structure (like orthogonality and centering)  providing additional admissible functions. In particular 
[20] designs “robust cumulants” which are only minimally effected by sparse noise. Welling’s robust
cumulants have versions of the additivity and homogeneity properties  and are consistent with our
update step. For this reason  we will state our results in greater generality.
Let G be a function of univariate random variables that satisﬁes the additivity  degree-r (r ≥ 3)
homogeneity  and (for the noisy case) the vanishing Gaussians properties of cumulants. Then for a
generic choice of input vector v  Algorithm 2 will demonstrate order r−1 convergence. In particular 
if G is κ3  then we obtain quadratic convergence; and if G is κ4  we obtain cubic convergence.
Lemma 4.1 helps explain why this is true.

Lemma 4.1. ∇vG(v · y) = r(cid:80)d

i=1(v · Ri)r−1G(˜si)Ri.

If we consider what is happening in the basis of the columns of R  then up to some multiplicative
constant  each coordinate is raised to the r − 1 power and then renormalized during each step of
Algorithm 2. This ultimately leads to the order r − 1 convergence.
Theorem 4.2. If for a unit vector input v to Algorithm 2 h = arg maxi |(v · Ri)r−2G(˜si)| has a
unique answer  then v has order r − 1 convergence to Rh up to sign. In particular  if the following
conditions are met: (1) There exists a coordinate random variable si of s such that G(si) (cid:54)= 0. (2) v
inputted into Algorithm 2 is chosen uniformly at random from the unit sphere Sd−1. Then Algorithm
2 converges to a column of R (up to sign) almost surely  and convergence is of order r − 1.

5

Algorithm 2 A fast algorithm to recover a single column of R when v is drawn generically from
the unit sphere. Equations (2) and (3) provide k-statistic based estimates of ∇vκ3 and ∇vκ4  which
can be used as practical choices of ∇vG on real data.
1: function GI-ICA(v  y)
2:
v ← ∇vG(vT y)
3:
v ← v/(cid:107)v(cid:107)2
4:
5:
6: end function

until Convergence return v

repeat

Algorithm 3 Algorithm for ICA in the presence of Gaussian noise. ˜A recovers A up to column
order and scaling. RT W is the demixing matrix for the observed random vector x.

function GAUSSIANROBUSTICA(G  x)

W = FINDQUASIORTHOGONALIZATIONMATRIX(x)
y = W x
R columns = ∅
for i = 1 to d do

Draw v from Sd−1 ∩ span(R columns)⊥ uniformly at random.
R columns = R columns ∪ {GI-ICA(v  y)}

end for
Construct a matrix R using the elements of R columns as columns.
˜s = RT y
˜A = (RT W )−1
return ˜A  ˜s

end function

By convergence up to sign  we include the possibility that v oscillates between Rh and −Rh on
alternating steps. This can occur if G(˜si) < 0 and r is odd. Due to space limitations  the proof is
omitted.
Recovering all Independent Components. As a Corollary to Theorem 4.2 we get:
Corollary 4.3. Suppose R1  R2  . . .   Rk are known for some k < d. Suppose there exists i > k
such that G(si) (cid:54)= 0. If v is drawn uniformly at random from Sd−1 ∩ span(R1  . . .   Rk)⊥ where
Sd−1 denotes the unit sphere in Rd  then Algorithm 2 with input v converges to a new column of R
almost surely.

Since the indexing of R is arbitrary  Corollary 4.3 gives a solution to noisy ICA  in Algorithm
3. In practice (not required by the theory)  it may be better to enforce orthogonality between the
columns of R  by orthogonalizing v against previously found columns of R at the end of each step
in Algorithm 2. We expect the fourth or third cumulant function will typically be chosen for G.

5 Time Complexity Analysis and Estimation of Cumulants
To implement Algorithms 1 and 2 requires the estimation of functions from data. We will limit
our discussion to estimation of the third and fourth cumulants  as lower order cumulants are more
statistically stable to estimate than higher order cumulants. κ3 is useful in Algorithm 2 for non-
symmetric distributions. However  since κ3(si) = 0 whenever si is a symmetric distribution  it is
plausible that κ3 would not recover all columns of R. When s is suspected of being symmetric  it
is prudent to use κ4 for G. Alternatively  one can fall back to κ4 from κ3 when κ3 is detected to be
near 0.
(cid:80)N
Denote by z(1)  z(2)  . . .   z(N ) the observed samples of a random variable z. Given a sample  each
cumulant can be estimated in an unbiased fashion by its k-statistic. Denote by kr(z(i)) the k-
i=1(z(i) − ¯z)r give the rth sample
statistic sample estimate of κr(z). Letting mr(z(i)) := 1
N
central moment  then

k3(z(i)) :=

N 2m3(z(i))

(N − 1)(N − 2)

  k4(z(i)) := N 2 (N + 1)m4(z(i)) − 3(N − 1)m2(z(i))2

(N − 1)(N − 2)(N − 3)

6

gives the third and fourth k-statistics [15]. However  we are interested in estimating the gradients (for
Algorithm 2) and Hessians (for Algorithm 1) of the cumulants rather than the cumulants themselves.
The following Lemma shows how to obtain unbiased estimates:
Lemma 5.1. Let z be a d-dimensional random vector with ﬁnite moments up to order r. Let z(i) be
an iid sample of z. Let α ∈ Nd be a multi-index. Then ∂α
u kr(u · z(i)) is an unbiased estimate for
u κr(u · z).
∂α
If we mean-subtract (via the sample mean) all observed random variables  then the resulting esti-
mates are:

∇uk3(u · y) = (N − 1)−1(N − 2)−13N

∇uk4(u · y) =

N 2

(N − 1)(N − 2)(N − 3)

(cid:33)

((u · y(i)))3y(i)

(cid:33)(cid:41)

−12

N − 1
N 2

(u · y(i))2

(u · y(i))y(i)

(cid:32) N(cid:88)
(cid:40)

i=1

N(cid:88)
(cid:40)

i=1

4

N

(u · y(i))2y(i)

i=1

N + 1

(cid:32) N(cid:88)
(cid:33)(cid:32) N(cid:88)
N(cid:88)
(cid:32) N(cid:88)

(u · x(i))x(i)

i=1

i=1

(2)

(3)

(4)

(cid:33)T

Huk4(u · x) =

− N − 1

N 2

12N 2

N(cid:88)

(N − 1)(N − 2)(N − 3)

N(cid:88)
(xxT )(i) − 2N − 2

N

(u · x(i))2

N 2

N + 1

((u · x(i)))2(xxT )(i)

(cid:33)(cid:32) N(cid:88)

(u · x(i))x(i)

i=1

i=1

i=1

i=1

Using (4) to estimate Huκ4(uT x) from data when implementing Algorithm 1  the resulting quasi-
orthogonalization algorithm runs in O(N d3) time. Using (2) or (3) to estimate ∇uG(vT y) (with G
chosen to be κ3 or κ4 respectively) when implementing Algorithm 2 gives an update step that runs
in O(N d) time. If t bounds the number of iterations to convergence in Algorithm 2  then O(N d2t)
steps are required to recover all columns of R once quasi-orthogonalization has been achieved.

6 Simulation Results
In Figure 1  we compare our algorithms to the baselines JADE [7] and versions of FastICA [10] 
using the code made available by the authors. Except for the choice of the contrast function for
FastICA the baselines were run using default settings. All tests were done using artiﬁcially generated
data. In implementing our algorithms (available at [19])  we opted to enforce orthogonality during
the update step of Algorithm 2 with previously found columns of R. In Figure 1  comparison on
ﬁve distributions indicates that each of the independent coordinates was generated from a distinct
distribution among the Laplace distribution  the Bernoulli distribution with parameter 0.5  the t-
distribution with 5 degrees of freedom  the exponential distribution  and the continuous uniform
distribution. Most of these distributions are symmetric  making GI-κ3 inadmissible.
When generating data for the ICA algorithm  we generate a random mixing matrix A with condition
number 10 (minimum singular value 1 and maximum singular value 10)  and intermediate singular
values chosen uniformly at random. The noise magnitude indicates the strength of an additive white
Gaussian noise. We deﬁne 100% noise magnitude to mean variance 10  with 25% noise and 50%
noise indicating variances 2.5 and 5 respectively. Performance was measured using the Amari Index
introduced in [1]. Let ˆB denote the approximate demixing matrix returned by an ICA algorithm 
+

and let M = ˆBA. Then  the Amari index is given by: E := (cid:80)n
(cid:80)n

(cid:16) |mij|
(cid:17)
maxk |mik| − 1

. The Amari index takes on values between 0 and the dimensionality
d. It can be roughly viewed as the distance of M from the nearest scaled permutation matrix P D
(where P is a permutation matrix and D is a diagonal matrix).
From the noiseles data  we see that quasi-orthogonalization requires more data than whitening in
order to provide accurate results. Once sufﬁcient data is provided  all fourth order methods (GI-κ4 
JADE  and κ4-FastICA) perform comparably. The difference between GI-κ4 and κ4-FastICA is not

(cid:80)n

|mij|

maxk |mkj| − 1

(cid:80)n

(cid:16)

(cid:17)

j=1

j=1

i=1

i=1

7

Figure 1: Comparison of ICA algorithms under various levels of noise. White and quasi-orthogonal
refer to the choice of the ﬁrst step of ICA. All baseline algorithms use whitening. Reported Amari
indices denote the mean Amari index over 50 runs on different draws of both A and the data. d gives
the data dimensionality  with two copies of each distribution used when d = 10.

statistically signiﬁcant over 50 runs with 100 000 samples. We note that GI-κ4 under whitening and
κ4-FastICA have the same update step (up to a slightly different choice of estimators)  with GI-κ4
differing to allow for quasi-orthogonalization. Where provided  the error bars give a 2σ conﬁdence
interval on the mean Amari index. In all cases  error bars for our algorithms are provided  and error
bars for the baseline algorithms are provided when they do not hinder readability.
It is clear that all algorithms degrade with the addition of Gaussian noise. However  GI-κ4 un-
der quasi-orthogonalization degrades far less when given sufﬁcient samples. For this reason  the
quasi-orthogonalized GI-κ4 outperforms all other algorithms (given sufﬁcient samples) including
the log cosh-FastICA  which performs best in the noiseless case. Contrasting the performance of GI-
κ4 under whitening with itself under quasi-orthogonalization  it is clear that quasi-orthogonalization
is necessary to be robust to Gaussian noise.
Run times were indeed reasonably fast. For 100 000 samples on the varied distributions (d = 5) with
50% Gaussian noise magnitude  GI-κ4 (including the orthogonalization step) had an average running
time2 of 0.19 seconds using PCA whitening  and 0.23 seconds under quasi-orthogonalization. The
corresponding average number of iterations to convergence per independent component (at 0.0001
error) were 4.16 and 4.08. In the following table  we report the mean number of steps to convergence
(per independent component) over the 50 runs for the 50% noise distribution (d = 5)  and note that
once sufﬁciently many samples were taken  the number of steps to convergence becomes remarkably
small.

Number of data pts

whitening+GI-κ4: mean num steps
quasi-orth.+GI-κ4: mean num steps

500
11.76
213.92

1000
5.92
65.95

5000
4.99
4.48

10000
4.59
4.36

50000
4.35
4.06

100000

4.16
4.08

7 Acknowledgments

This work was supported by NSF grant IIS 1117707.

2 Using a standard desktop with an i7-2600 3.4 GHz CPU and 16 GB RAM.

8

 100 1000 10000 1000000.010.101.00Number of SamplesAmari IndexICA Comparison on 5 distributions (d=5  noisless data) GI−κ4 (white)GI−κ4 (quasi−orthogonal)κ4−FastICAlog cosh−FastICAJADE 100 1000 10000 1000000.010.101.00Number of SamplesAmari IndexICA Comparison on 5 distributions (d=5  25% noise magnitude) GI−κ4 (white)GI−κ4 (quasi−orthogonal)κ4−FastICAlog cosh−FastICAJADE 100 1000 10000 1000000.010.101.00Number of SamplesAmari IndexICA Comparison on 5 distributions (d=5  50% noise magnitude) GI−κ4 (white)GI−κ4 (quasi−orthogonal)κ4−FastICAlog cosh−FastICAJADE 100 1000 10000 100000 0.01 0.10 1.0010.00Number of SamplesAmari IndexICA Comparison on 5 distributions (d=10  noisless data) GI−κ4 (white)GI−κ4 (quasi−orthogonal)κ4−FastICAlog cosh−FastICAJADE 100 1000 10000 100000 0.01 0.10 1.0010.00Number of SamplesAmari IndexICA Comparison on 5 distributions (d=10  25% noise magnitude) GI−κ4 (white)GI−κ4 (quasi−orthogonal)κ4−FastICAlog cosh−FastICAJADE 100 1000 10000 100000 0.01 0.10 1.0010.00Number of SamplesAmari IndexICA Comparison on 5 distributions (d=10  50% noise magnitude) GI−κ4 (white)GI−κ4 (quasi−orthogonal)κ4−FastICAlog cosh−FastICAJADEReferences
[1] S. Amari  A. Cichocki  H. H. Yang  et al. A new learning algorithm for blind signal separation.

Advances in neural information processing systems  pages 757–763  1996.

[2] S. Arora  R. Ge  A. Moitra  and S. Sachdeva. Provable ICA with unknown Gaussian noise 
with implications for Gaussian mixtures and autoencoders. In NIPS  pages 2384–2392  2012.
[3] M. Belkin  L. Rademacher  and J. Voss. Blind signal separation in the presence of Gaussian

noise. In JMLR W&CP  volume 30: COLT  pages 270–287  2013.

[4] C. M. Bishop. Variational principal components. Proc. Ninth Int. Conf. on Articial Neural

Networks. ICANN  1:509–514  1999.

[5] E. J. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? CoRR 

abs/0912.3599  2009.

[6] J. Cardoso and A. Souloumiac. Blind beamforming for non-Gaussian signals. In Radar and

Signal Processing  IEE Proceedings F  volume 140  pages 362–370. IET  1993.

[7] J.-F. Cardoso and A. Souloumiac. Matlab JADE for real-valued data v 1.8. http://
[On-

perso.telecom-paristech.fr/˜cardoso/Algo/Jade/jadeR.m  2005.
line; accessed 8-May-2013].

[8] P. Comon and C. Jutten  editors. Handbook of Blind Source Separation. Academic Press  2010.
[9] X. Ding  L. He  and L. Carin. Bayesian robust principal component analysis. Image Process-

ing  IEEE Transactions on  20(12):3419–3430  2011.

[10] H. G¨avert  J. Hurri  J. S¨arel¨a  and A. Hyv¨arinen. Matlab FastICA v 2.5. http://
[Online;

research.ics.aalto.fi/ica/fastica/code/dlcode.shtml  2005.
accessed 1-May-2013].

[11] D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: Moment methods and

spectral decompositions. In ITCS  pages 11–20  2013.

[12] A. Hyv¨arinen. Independent component analysis in the presence of Gaussian noise by maxi-

mizing joint likelihood. Neurocomputing  22(1-3):49–67  1998.

[13] A. Hyv¨arinen. Fast and robust ﬁxed-point algorithms for independent component analysis.

IEEE Transactions on Neural Networks  10(3):626–634  1999.

[14] A. Hyv¨arinen and E. Oja.

Independent component analysis: Algorithms and applications.

Neural Networks  13(4-5):411–430  2000.

[15] J. F. Kenney and E. S. Keeping. Mathematics of Statistics  part 2. van Nostrand  1962.
[16] H. Li and T. Adali. A class of complex ICA algorithms based on the kurtosis cost function.

IEEE Transactions on Neural Networks  19(3):408–420  2008.

[17] L. Mafttner. What are cumulants. Documenta Mathematica  4:601–622  1999.
[18] P. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU

signatures. J. Cryptology  22(2):139–160  2009.

[19] J. Voss  L. Rademacher  and M. Belkin. Matlab GI-ICA implementation. http://

sourceforge.net/projects/giica/  2013. [Online].

[20] M. Welling. Robust higher order statistics.

In Tenth International Workshop on Artiﬁcial

Intelligence and Statistics  pages 405–412  2005.

[21] A. Yeredor. Blind source separation via the second characteristic function. Signal Processing 

80(5):897–902  2000.

[22] V. Zarzoso and P. Comon. How fast is FastICA. EUSIPCO  2006.

9

,James Voss
Luis Rademacher
Mikhail Belkin
Aurko Roy
Sebastian Pokutta