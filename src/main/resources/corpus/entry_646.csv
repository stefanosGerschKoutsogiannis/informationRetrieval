2019,AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters,Reducing the model redundancy is an important task to deploy complex deep learning models to resource-limited or time-sensitive devices. Directly regularizing or modifying weight values makes pruning procedure less robust and sensitive to the choice of hyperparameters  and it also requires prior knowledge to tune different hyperparameters for different models. To build a better generalized and easy-to-use pruning method  we propose AutoPrune  which prunes the network through optimizing a set of trainable auxiliary parameters instead of original weights. The instability and noise during training on auxiliary parameters will not directly affect weight values  which makes pruning process more robust to noise and less sensitive to hyperparameters. Moreover  we design gradient update rules for auxiliary parameters to keep them consistent with pruning tasks. Our method can automatically eliminate network redundancy with recoverability  relieving the complicated prior knowledge required to design thresholding functions  and reducing the time for trial and error. We evaluate our method with LeNet and VGG-like on MNIST and CIFAR-10 datasets  and with AlexNet  ResNet and MobileNet on ImageNet to establish the scalability of our work. Results show that our model achieves state-of-the-art sparsity  e.g. 7%  23% FLOPs and 310x  75x compression ratio for LeNet5 and VGG-like structure without accuracy drop  and 200M and 100M FLOPs for MobileNet V2 with accuracy 73.32% and 66.83% respectively.,AutoPrune: Automatic Network Pruning by

Regularizing Auxiliary Parameters

Xia Xiao  Zigeng Wang  Sanguthevar Rajasekaran∗
Department of Computer Science and Engineering

University of Connecticut
Storrs  CT  USA  06269

{xia.xiao  zigeng.wang  sanguthevar.rajasekaran}@uconn.edu

Abstract

Reducing the model redundancy is an important task to deploy complex deep
learning models to resource-limited or time-sensitive devices. Directly regularizing
or modifying weight values makes pruning procedure less robust and sensitive
to the choice of hyperparameters  and it also requires prior knowledge to tune
different hyperparameters for different models. To build a better generalized and
easy-to-use pruning method  we propose AutoPrune  which prunes the network
through optimizing a set of trainable auxiliary parameters instead of original
weights. The instability and noise during training on auxiliary parameters will not
directly affect weight values  which makes pruning process more robust to noise
and less sensitive to hyperparameters. Moreover  we design gradient update rules
for auxiliary parameters to keep them consistent with pruning tasks. Our method
can automatically eliminate network redundancy with recoverability  relieving
the complicated prior knowledge required to design thresholding functions  and
reducing the time for trial and error. We evaluate our method with LeNet and VGG-
like on MNIST and CIFAR-10 datasets  and with AlexNet  ResNet and MobileNet
on ImageNet to establish the scalability of our work. Results show that our model
achieves state-of-the-art sparsity  e.g. 7%  23% FLOPs and 310x  75x compression
ratio for LeNet5 and VGG-like structure without accuracy drop  and 200M and
100M FLOPs for MobileNet V2 with accuracy 73.32% and 66.83% respectively.

1

Introduction

Deep neural networks (DNNs) have achieved a signiﬁcant success in many applications  ranging from
image classiﬁcation He et al. [2016] and object detection Ren et al. [2015] to self driving Maqueda et
al. [2018] and machine translation Sutskever et al. [2014]. However  the computationally expensive
and memory intensive properties of DNNs prevent their direct deployment to devices such as mobile
phones and auto-driving cars. To overcome these challenges  learning compressed light-weight DNNs
has attracted growing research attention Han et al. [2015]; Dong et al. [2017]; Zhuang et al. [2018].
For recent pruning methods  prior knowledge plays an important role in improving the performance
and reducing the training time  in which a large number of hyperparameters need to be individually
designed for different architectures and datasets. In magnitude-based pruning  where weights lower
than thresholds will be removed  the chosen thresholds majorly affect the pruning performance Han
et al. [2015]; Guo et al. [2016]. Moreover  for the layer-wise pruning Dong et al. [2017]; Aghasi
et al. [2017]  the searching space for layer-wise threshold combinations can be exponential in the
number of layers. As another branch of pruning  sensitivity-based method Tartaglione et al. [2018]
∗Corresponding author. This work has been supported in part by the following NSF grants: 1447711 

1514357  1743418  and 1843025.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

removes the less sensitive weights from the network  while further hyperparameter/function design is
required to avoid undesired weight shrinkage or updates.
Recently research on pruning Liu et al. [2019b] implies that the pruning process is actually ﬁnding the
right network structure  thus bridging the gap between pruning and neuron architecture search(NAS).
However  state-of-art NAS methods cannot be directly applied to pruning task. For example  gradient
based search algorithm DART Liu et al. [2019a] introduces auxiliary parameters acting as indicators
to select the appropriate network structure optimized through a gradient-descent procedure. But  dis-
crepancy between continuous over-parameterized graph and the discretized sub-graph is unavoidable
during the model evaluation procedure  and zero operation is eliminated in the search space. Our
method is similar to DART such that we employ smooth  approximated  gradient-based search to
pruning task  but the discrepancy is reduced by iteratively evaluating recoverable sub-graph during
the pruning procedure.
The advantage of introducing auxiliary parameters to pruning task is hyperparameter insensitive.
Instead of directly regularizing weights  our method regularizes auxiliary parameters which aggregate
gradient perturbations such as batch noise  dead neuron or dropout during pruning. In this way 
temporarily incorrect pruning induced by the instability and non-optimal hyperparameters can be
recovered  which greatly contributes to the pruning performance and efﬁciency. Different from
updating auxiliary parameters with vanilla unstable linear coarse gradient in Srinivas et al. [2017] 
in order to stabilize the pruning procedure  we analyze and decouple the gradient between weight
In contrast to Louizos et al. [2018]  our method avoids
parameters and auxiliary parameters.
inefﬁcient and high variance single-step Monte-Carlo sampling and places no assumptions on the
prior distribution. In comparison with Carreira-Perpinán and Idelbayev [2018]  we add no constraints
on model parameters  maintaining the ﬂexibility and capacity of the model. In addition  we design a
sparse regularizer working with the original loss function and weight decay. In order to evaluate the
proposed method  we conduct extensive experiments on different datasets and models  and the results
show that our method achieves state-of-the-art performance.
Contributions and novelty of our work are: 1) we offer a gradient based automatic network pruning
model; 2) we propose novel and weakly coupled update rule for auxiliary parameters to stabilize
pruning procedure; 3) we reduce the sub-graph discrepancy by iteratively evaluating recoverable
sub-graph; 4) we evaluate different smooth approximations of the derivative of the rectiﬁer; 5) we
obtain the state-of-art results on both structure and weight pruning and our method is scalable on
modern models and datasets.

2 Related Work

Neural network pruning can be mainly classiﬁed into two categories: unstructured pruning and
structured pruning. Unstructured pruning compresses neural networks by dropping redundant/less-
meaningful weights  while structured pruning is by dropping neurons. Both pruning methods shrink
the storage space of the targeted neural network  but  comparatively speaking  structured pruning has
a directly beneﬁt in reducing the computational cost of DNNs.
LeCun et al. [1990] pioneers neural network pruning and proposes optimal brain damage method
for shallow neural network unstructured pruning. For DNNs  Han et al. [2015] presents global
magnitude-based weight pruning and Guo et al. [2016] introduces recoverability into the global
pruning. Similar idea has then been applied to structured pruning. Hu et al. [2016] removes neurons
with high average zero output ratio and Li et al. [2017] prunes neurons with low absolute summation
values of incoming weights  which are all replying on predeﬁned thresholds.
In order to further improve the compression rate  different layer-wise pruning methods have been
proposed  either by weighting connections based on a layer-wise loss function(Dong et al. [2017])
or by solving a specially designed convex optimization program(Aghasi et al. [2017]). These layer-
wise schemes provide theoretical error bounds for speciﬁc activation functions but leave many
hyperparameters to be carefully designed. Due to this issue  Li et al. [2018] presents a relatively
efﬁcient comprehensive optimization algorithm for tuning layer-wise hyperparameters.
Besides layer-wise schemes  Gordon et al. [2018] scales efﬁcient structured pruning on large networks
by applying resource weighted sparsifying regularizers on activations. Zhu et al. [2018] improves
neural network sparsity by explicitly forcing the network to learn a set of less correlated ﬁlters via

2

decorrelation regularization. Zhuang et al. [2018] designs a discrimination-aware channel pruning
method to locate most discriminative channels. But after ranking the ﬁlters or channels  we still have
to pinpoint their optimal combinations for each layer  which highly relies on expertise. Gomez et al.
[2019] proposed to keep neurons with high magnitude and prune neurons with smaller magnitude in
a stochastic way. The accuracy is maintained by reducing the dependency of important neurons on
unimportant neurons.
Liu et al. [2019b] does comprehensive experiments showing that training-from-scratch on the right
sparse architecture yields better results than pruning from pre-trained models. Searching for a spare
architecture is more important than the weight values. Liu et al. [2019a] employs continuous indicator
parameters to relax the non-differentiable architecture searching problem. The relaxation is then
removed by dropping weak connections and selecting the single choice of the k options with the
highest weight. However  the gap between the continuous solution and the discretized architecture
remain unknown. More importantly  zero operations are omitted during the derivation process 
making is unsuitable for network pruning. Yu and Huang [2019a] implements greedy search for
width multipliers of slimmable network(Yu et al. [2018]) to reduce kernel number. Multiple batch
normalization layers are trained under different channel settings. However  a signiﬁcant accuracy
drop is observed in extreme sparse cases.

3 Methods

In this section  we ﬁrst formulate the problem and discuss the indicator function and auxiliary
parameters. Then  we introduce the update rule for auxiliary parameters for stable and efﬁcient
network pruning. Without losing generality  our method is formulated on weight pruning  but it can
be directly extended to neuron pruning.

3.1 Problem Formulation
Let fw : Rm×n → Rd be a continuous and differentiable neural network parametrized by W
mapping input X ∈ Rm×n to target Y ∈ Rd. The pruning problem can be formulated as:

(cid:18) N(cid:88)

i=1

argmin

w

1
N

(cid:19)

L(f (xi  W )  yi)

+ µ||W||0 

(1)

where ||W||0 denotes zero norm  or number of non-zero weights. The goal is to ﬁnd the sparse archi-
tecture with minimum subset w ∈ W that preserves the model accuracy. However  the second term is
non-differentiable  making the problem not solvable using gradient descent. Direct regularization on
wij will lead to sensitivity on hyperparameter µ and instability with batched training. We relax this
problem by introducing a indicator function deﬁned as:

(cid:40)

hij =

if wij is pruned;

0 
1  otherwise.

(2)

(cid:19)

(cid:18) N(cid:88)

Instead of designing an indicator function for each wij manually  we propose to parameterized a uni-
versal indicator function by a set of trainable auxiliary parameters M. Due to the non-differentiable
property of the indicator function  we will discuss how to update auxiliary parameters in subsec-
tions 3.2 and 3.3. Then the network sparsiﬁcation problem can be re-formulated as an optimization
problem:

1
N

L(f (xi  W (cid:12) h(M ))  yi)

+ λR(W ) + µR(h(M )) 

i=1

w m

argmin

(3)
where R(·) denotes a regularization function. We also denote the element-wise product T =
W (cid:12) h(M ) as the weight matrix after pruning. The advantage of regularizing on auxiliary parameters
instead of original weights is that any change in mij does not directly inﬂuence the gradient update
of wij  leading to a less sensitive pruning process with respect to hyperparameter µ.
As done by Han et al. [2015] and Carreira-Perpinán and Idelbayev [2018]  in order to enhance the
stability and performance  we also implement a multi-step training through iteratively training the
sparsity structure and retraining the original weights. More speciﬁcally  we employ the bi-level
optimization used in Liu et al. [2019a] for the optimization problem. The training set will be split

3

into Xtrain and Xval  and we can further re-formulate the problem from minimizing a single loss
function to minimizing the following loss functions iteratively.

L1 = min

w

min

w

L2 = min

m

min
m

N(cid:88)
N(cid:88)

i=1

i=1

L(f (xi  W (cid:12) h(M ))  yi) + λR(W )  xi ∈ Xtrain 

L(f (xi  W (cid:12) h(M ))  yi) + µR(h(M ))  xi ∈ Xval 

(4)

(5)

The ﬁrst term in both loss functions is the regular accuracy loss for neural network training. Note that
the regularization of W is not necessarily required but we add the term to show that our method is
consistent with traditional regularizers.

3.2 Coarse Gradient for Indicator Function

The indicator function hij contains only zero and one values and thus is non-smooth and non-
differentiable. Inspired by Hubara et al. [2016] where binary weights are represented using step
functions and trained with hard sigmoid straight through estimator (STE)  we use a simple step
function for indicator function hij with trainable parameter mij.
Binarized neural networks (BNNs) with proper STE have been demonstrated to be quite effective in
ﬁnding optimal binary parameters and can achieve promising results in complex tasks. The vanilla
BNNs are optimized by updating continuous variables mij:

∂L
∂mij

=

∂L

∂σ(mij)

  where σ(mij) = max(0  min(1 

mij + 1

2

)).

(6)

∂σ(mij)

∂mij

The output of each weight is the output of the hard sigmoid binary function. Note that the gradients
of ∂σ(mij)
∂mij

can be estimated in multiple ways.

Figure 1: Coarse Gradients for STEs

Srinivas et al. [2017] discuss using BNNs to learn sparse networks  however  the authors suggest
using linear STE to quickly estimate the gradient of the heaviside function. Recent result Yin et
al. [2019] shows that ReLU or clipped ReLU STEs yield better convergence while linear STE is
unstable at minima. Unfortunately  as shown in Fig. 1  the gradient of ReLU is zero if the input m is
smaller than zero. In other words  if we apply auxiliary parameters directly to any weight  without
any regularization  the weight will permanently die once the corresponding weight has been pruned.
Considering the pruning recoverability  we suggest using Leaky ReLU or Softplus instead of ReLU.

3.3 Updating Auxiliary Parameters

(cid:18) ∂Lacc

(cid:19)

Instead of directly applying the gradient update as described in Eq. 6  we propose a modiﬁed update
rule of auxiliary parameters to be consistent with (1) the magnitude of weights; (2) the change of
weights; and (3) the directions of BNN gradients. The update rule of mij is deﬁned as:

mij := mij − η

∂h(mij)

− µ

∂h(mij)

∂tij

∂mij

sgn(wij)

(7)
where Lacc denotes L(f (xi  W (cid:12) h(M ))  yi)  η is the learning rate of mij  tij = wij (cid:12) h(mij)  the
second term can be considered as the gradient of mij  ∂tij
  and the third term is related to the sparse
∂mij
regularizer. The proposed update rule is motivated from three advantages:
Sensitivity Consistency: The gradient of a vanilla BNN is correlated with wij  i.e.  ∂Lacc
f (|wij|) 
which means that mij is more sensitive if the magnitude of the corresponding wij is large. Such a
sensitive correlation is counter-intuitive since a larger wij is more likely to be pruned with a small
turbulence which reduces the robustness of the pruning. In the proposed update rule  we decouple

∂mij

∝

∂mij

1

4

202m012h(m)202m012h(m)202m012h(m)such a correlation to increase the stability of the pruning procedure. Practically  in order to boost the
sensitivity of mij associated with smaller weight magnitude(i.e. sensitivity consistency)  we use a
multiplier wij to Eq. 7.
Correlation Consistency: The second advantage of the update rule is that the direction of the
gradient of an arbitrary auxiliary parameter mij is the same as the direction of the gradient of its
corresponding |wij|  when ignoring the regularizers  i.e.  sgn( ∂L2
Proof. We can expand the gradient for wij and mij as follows:

) = sgn( ∂L1

∂|wij| ).

∂mij

∂L1
∂wij

∂Lacc
∂tij

∂tij
∂wij

=

∂R(wij)
∂wij

∂Lacc
∂tij

=

+ λ

h(mij) + λ

∂R(wij)
∂wij

∂L2
∂mij

∂Lacc
∂tij

∂tij
∂mij

=

∂R(h(mij))

∂mij

∂Lacc
∂tij

=

+ µ

wij

∂h(mij)

∂mij

+ µ

∂h(mij)

∂mij

If we consider the direction of the ﬁrst term of both gradients while ignoring the regularizers:

sgn(

∂Lacc
∂tij
∂Lacc
∂tij
Given the conditions that h(mij) ≥ 0 and ∂h(mij )

∂L1
∂wij
∂L2
∂mij

) = sgn(

) = sgn(

sgn(

)sgn(h(mij))

)sgn(wij)sgn(

∂h(mij)

∂mij

).

≥ 0  we can conclude that

∂mij

∂L2
∂mij

sgn(

) = sgn(

∂L1
∂|wij| ).

(8)

(9)

(10)

(11)

In other words  the auxiliary parameter mij tracks the changing of the magnitude of wij. For the
pruning task  when the absolute value of a weight/neuron keeps moving towards zero  we should
accelerate the pruning process of the weight/neuron.
Direction Consistency: The third advantage of the update rule is that the inner product between
the expected coarse and population gradients with respect to m is greater than zero  i.e.  the update
gradient and the population gradient form an acute angle. Updating in this way actually reduces
the loss of vanilla BNNs. We refer to Eq. 5  Lemma4 and Lemma10 from Yin et al. [2019]  where
the ReLU and linear STE form acute angle with population gradient. Since(cid:104)gσ  g(cid:105) = σ(cid:48)q(w  w∗) 
where q(w  w∗) is a deterministic function for both cases and σ represent the STE function. Since
relu ≤ σ(cid:48)
Linear  we can then retain 0 ≤ (cid:104)grelu  g(cid:105) ≤ (cid:104)gLeakyRelu  g(cid:105) ≤ (cid:104)gLinear  g(cid:105).
σ(cid:48)

LeakyRelu ≤ σ(cid:48)

3.4 Recoverable Pruning

Pruning with recoverability is important to reduce the gap between the original network graph and the
sparse network graph  which helps to achieve better sparsity. We design the pruning step following the
idea of Dynamic Network Surgery(Guo et al. [2016])  that once some important weights are pruned
and a large discrepancy occurs  the incorrectly pruned weights will be recovered to compensate for the
increase of loss. Different from previous works with hard thresholding  for a speciﬁc weight/neuron 
its opportunity to be pruned is determined automatically during optimization. The pruning step in our
model is soft  the pruned weight will hold its value  and ready to be spliced back to the network if
large discrepancy is observed.
Based on the multi-step training framework  after mij is updated by Eq. 7  the unpruned network
parameters wij will be updated based on the newly learned structure. If no regularization is applied
on wij  the corresponding mij could be recovered by the accuracy loss. Note that a weight will be
recovered if the damage made by the pruned weight cannot be restored by updating other unpruned
weights. If weight decay is applied  any pruned weight will gradually lose recoverability with a ﬁxed
rate. The weight decay will decrease the magnitude of wij and provide a negative gradient to mij 
which reduces the recoverability. Whether a weight will be recovered under weight decay depends on
(1) the absolute value of wij  and (2) the damage it made when removing it from the network. More
speciﬁcally  recovering a weight wij requires the gradient of mij moving toward positive direction.
With L1 regularization  a weight will be permanently pruned when its absolute value drops to zero.

5

Algorithm 1 AutoPrun
Input: Data set X and iter
Parameter: W   M  λ and µ
Output: Auxiliary parameter M and W

1: Randomly split X into Xtrain and Xval.
2: if Pre-trained then
3:
4: else
5:
6: end if

Initialize M based on pre-trained W ;
Initialize M ∼ Gaussian(µ  σ2);

3.5 Acceleration by Regularizers

Sample a mini batch from Xval;
Compute gradw with L1;
Compute gradm and gradmr by Eq. 7;
Update M with gradm and gradmr;
Sample a mini batch from Xtrain;
Compute gradw with L1;
Update W with gradw;
Update iter  λ  µ (if scheduling);

7: while iter!=0 do
8:
9:
10:
11:
12:
13:
14:
15:
16: end while
17: return solution;

3.5.1 Sparse Regularizer
Without any regularizer  our model can gradually converge to a sparse model  but with relatively
slow speed  especially when the weights are close to optimal and the gradients with respect to
T = W (cid:12) h(M ) are almost zero. In order to accelerate the pruning process  we bring in regularizers
to force the mask values to approach zero. The sparse regularizer is deﬁned as:

R(h(M )) =

|h(mij)| = count(h(M )).

(12)

(cid:88)

i j

Note that the L1 regularizer applied on h(M ) directly counts the number of gates that are open 
which is equivalent to applying L0 regularizer on h(M ). With the regularizer  M will be pushed
towards zero since the gradient with respect to mij is the positive STE gradient. Another beneﬁt of
this regularizer is to ﬁlter out the noise when updating W with SGD or dropout  i.e.  µ∂L2/∂mij > 0
when ∆|w| < δ and mij still decreases when wij increases by only a small amount.
3.5.2 Working with Weight Decay Regularizer

Our model can also work with general 1-norm or 2-norm regularizers on weights W . Since the auxil-
iary parameters M follow |W|  any weight decay regularizer will help to increase the sparsiﬁcation
speed. An important side effect of weight decay regularizer is that after pruning a certain weight  the
only source that can change |wij| ∈ |W| s.t. h(mij) = 0 will be the weight decay regularizers. A
large weight decay hyperparameter will decrease the pruned weight fast and hamper the recoverability
discussed in the previous subsection.

3.6 Hyperparameters Sensitivity and Robustness

By proposing auxiliary parameters and an indicator function  we introduce two new hyperparameters 
learning rate hyperparameter η and regularization hyperparameter µ. However  the pruning procedure
is not sensitive to those hyperparameters based on the following reasons: 1) We are not directly
regularizing W   so the bias of STE and hyperparameter will not directly inﬂuence weights; 2) The
indicator function is tolerant to the turbulence of auxiliary parameters mij; and 3) The pruning is
recoverable when an incorrect pruning happens and the damage is made. Practically  as shown in the
experimental part  the learning rate η is scheduled to be the same as for learning the original weights
W   and the regularization hyperparameter µ is set to be the same in all test cases. To conclude  our
method reduces a set of hyperparameters to one single  non-sensitive hyperparameter.

3.7 Convergence Discussion

Similar to Gordon et al. [2018]  our framework doesn’t guarantee convergence when optimized with
regularizers. But since the sparsiﬁcation procedure is emperically fast and a good structure can be
obtained with fewer epochs  we do not always need to wait until convergence. But  in order to give a
guidance to hyperparameter tuning  we will brieﬂy discuss the necessary condition for convergence.
At convergence  if no regularization is applied  ∂Lacc

= 0. We can further conclude:

∂tij

∂Lacc
∂tij

h(mij) =

∂Lacc
∂tij

sgn(wij)

∂h(mij)

∂mij

= 0.

(13)

6

= ∂L2

If both weight decay and sparse regularizers are applied  we need ∂L1
= 0. Assuming that
pruned weights are sufﬁciently small and make no contribution to both gradients  we only consider
the gradients w.r.t. mij ∈ M s.t. mij > 0  and h(M ) = 1. When taking into account the learning
rate compensation  we have:
∂Lacc
∂tij

(14)
If L2 is applied  we have the necessary condition 2λ|wij| = cµ  where c is the non-linear factor by
different STEs. If L1 is applied  we have the necessary condition λ = cµ. Under both cases  λ and µ
should be reduced to the same level when convergence.

∂R(wij)
∂wij

∂Lacc
∂tij

sgn(wij)

∂h(mij)

∂h(mij)

∂mij

∂wij

∂mij

∂mij

+ µ

=

0 =

+ λ

.

4 Experiments

In this section  we introduce our experiment settings  and compare the neuron pruning and weight
pruning performance with existing approaches.

4.1 Settings

To ensure a fair comparison  we follow the same backend packages as described in other papers.
Except for LeNets  all the other pre-trained parameters are downloaded from commonly available
sources and the auxiliary parameters are either initialized randomly or by pre-trained weights. All the
accuracy results are the average of 10 runs and the spare structure is picked from the best accurate
model. Our models are implemented by Tensorﬂow and run on Ubuntu Linux 16.04 with 32G memory
and a single NVIDIA Titan Xp GPU. To show the insensitivity of the introduced hyperparameter  we
set the learning rate of auxiliary parameters to 1.5e-2 and µ to 5e-2 for all test cases.

Table 1: Comparison of Different Neuron Pruning Techniques
Methods

Neurons per Layer

Base Error

Epochs

Model

LeNet-300-100
784-300-100

LeNet5
(MNIST)
20-50-
800-500

VGG-like
(CIFAR-10)
64x2-128x2-
256x3-512x7

Louizos et al. [2017]
Louizos et al. [2018]
Louizos et al. [2018]

Our method

Wen et al. [2016]

Neklyudov et al. [2017]
Louizos et al. [2017]
Louizos et al. [2018]
Louizos et al. [2018]

Our method

Li et al. [2017]

Neklyudov et al. [2017]
Neklyudov et al. [2017]

Our method

-
-

-
-

-
-

1.60%

1.60%

0.90%

0.78%
6.75%
7.20%
7.20%
7.60%

Error
1.80%
-
1.40% 200
200
1.80%
100
1.82%
-
1.00%
-
0.86%
-
1.00%
200
0.90%
1.00%
200
0.80% 100
40
6.60%
-
7.50%
-
9.00%
8.50%
150

278-98-13
219-214-100
266-88-33
244-85-37

3-12-800-500
2-18-284-283
5-10-76-16
20-25-45-462
9-18-65-25
4-16-86-87

32-64-128-128-256-256-256-256-256-256-256-256-256-512

64-62-128-126-234-155-31-79-73-9-59-73-56-27
44-54-92-115-234-155-31-76-55-9-34-35-21-280
37-41-91-89-156-140-74-81-54-51-44-46-48-52

NCR FLOPs
11%
3.04
26%
2.22
10%
3.06
3.23
9%
25%
1.04
9%
2.33
7%
12.8
50%
2.48
11.71
17%
7%
9.86
66%
1.49
43%
4.03
32%
3.83
4.72
23%

4.2 LeNet-300-100 and LeNet5 on MNIST Database

We ﬁrst use MNIST dataset to evaluate the performance. Layer structure of LeNet-300-100 is [784 
300  100  10] and of LeNet5 is two [20 50] convolution layers  followed by two FC layers. The total
number of trainable parameters of LeNet-300-100 and LeNet5 are 267K and 431K  respectively.
Similar to previous works  we train reference models with standard training method with SGD
optimizer  achieving accuracy of 1.72% and 0.78% respectively. In the pruning process  we use the
softplus STE. The learning rate for L1 is scheduled from 1e-2 to 1e-3. During the training procedure 
we observe that the ﬁnal result is not sensitive to λ and µ but the sparsiﬁcation speed relies on µ.
For neuron pruning  from Table 1  we can achieve the highest neuron compression rate(NCR) as 3.23
and the lowest FLOP usage percentage 9% comparing to original LeNet-300-100. For LeNet5  we
are taking the lead in both the model accuracy 99.20% and the FLOP reduction rate 93%. For weight
pruning  as we show in Table 4  our method applied to the LeNet-300-100 structure achieves the best
compression rate of up to 80x while a 0.06% error increase. Note that all the other methods with
compression rates greater than 60 have a minor accuracy drop while our method reaches the best
accuracy. For LeNet5 model  we compare existing works with two reference models. For the ﬁrst
model with 0.78% error  we achieve 260x compression rate and 0.8% error. For the second model
with 0.91% error  our method obtains a 310x compression rate with no accuracy drop.

7

Table 2: VGG-like CIFAR-10 Neuron Pruning

57%

Layer Conv1 Conv2 Conv3 Conv4
Sparsity 57.03% 17.36% 20.95% 16.06%
FLOP
49%
Conv5 Conv6 Conv7 Conv8 Conv9
10.76% 4.67% 5.30% 1.52% 0.39%
15% 4.50% 1.60%

37%

45%

42%

33%

Methods

Sandler et al. [2018]

Table 3: MobileNetV2(Top 1 Accuracy)
FLOPs
FLOPs Accuracy
97M 65.40%
97M 64.40%
97M 65.10%
102M 66.83%
209M 69.80%
216M 71.5%
Yu and Huang [2019b] 209M 69.60%

Yu and Huang [2019b]

Sandler et al. [2018]

Tan et al. [2019]

Yu et al. [2018]

Our method

100M

200M

Wu et al. [2019]

246M
Yu and Huang [2019a] 207M

Conv10 Conv11 Conv12 Conv13
0.35% 0.28% 0.27% 0.33%
0.85% 0.77% 0.84%

1%

300M

Our method

Sandler et al. [2018]

Tan et al. [2019]

73%
73%
209M 73.32%
300M 69.80%
317M

74%

Yu and Huang [2019a] 305M 74.20%
305M 74.0%

Our method

4.3 VGG-like on CIFAR-10

For VGG-like model  we use CIFAR-10 dataset to evaluate the performance. VGG-like is a standard
convolution neural network with 13 convolutional layers followed by 2 FC layers (512 and 10
respectively). The total number of trainable parameters is 15M. Similar to previous works  we use
the reference VGG-like model pre-trained with SGD with testing error 7.60%.
In this structure  we use L2-norm and L1-norm for L1 with hyperparameters 5e-5 and 1e-6  respec-
tively. We evaluate both Leaky ReLU and Softplus STEs. Leaky ReLU gives a fast sparsiﬁcation
speed while Softplus shows a smooth convergence with approximately 1.5x running time. We suggest
selecting the proper STE based on the time constraint.
For neuron pruning task  as shown in Table 1  our method reaches 23% FLOPs within 150 epochs.
In Table 2  we show the layer-wise percentage FLOPs of VGG-16 structure. Our model achieves a
higher sparsity at any layer compared to Li et al. [2017]. For weight pruning  our model reaches the
highest 75x compression rate  with only moderate accuracy drop within 150 epochs of training.

4.4 AlexNet  ResNet-50 and MobileNet on ImageNet

Three models with ILSVRC12 dataset are also tested with our pruning method including 1M training
images and 0.5M validation and testing images. AlexNet can be considered as deep since it contains
5 convolution layers and 3 FC layers. ResNet-50 consists of 16 convolution blocks with structure
cfg=[3 4 6 3]  plus one input and one output layer  and in total 25M parameters. For MobileNet  we

Table 4: Comparison of Different Weight Pruning Techniques

Model

LeNet300-100

(MNIST)

LeNet5
(MNIST)

VGG-like
(CIFAR-10)

AlexNet

(ILSVRC12)

Methods

Dong et al. [2017]
Ullrich et al. [2017]

Molchanov et al. [2017]

Our method

Guo et al. [2016]
Ullrich et al. [2017]

Molchanov et al. [2017]

Li et al. [2018]

Our method
Our method

Zhuang et al. [2018]

Zhu et al. [2018]

Molchanov et al. [2017]

Our method

Guo et al. [2016]

Srinivas et al. [2017]
Dong et al. [2017]

Our method

ResNet50

(ILSVRC12)

Zhuang et al. [2018]

Our method

8

Error

CR
1.76%→2.43% 66.7
1.89%→1.94%
64
1.64%→1.92%
68
1.72%→ 1.78% 80
0.91%→0.91%
108
0.88%→0.97%
162
0.80%→ 0.75% 280
0.91%→0.91%
298
0.78%→0.80%
260
0.91%→0.91%
310
6.01%→5.43% 15.58
6.42%→6.69%
8.5
7.55%→7.55%
65
7.60%→7.82%
75
43.42%→43.09% 17.7
42.80%→43.04% 10.3
43.30%→50.04% 9.1
43.26%→44.10% 18.5
23.99%→25.05% 2.06
25.10%→25.50% 2.2

use its conventional MobileNet V2 (224×224) model with 310M FLOPs. The size of the dataset and
also the complexity of the model clearly reveals the scalability of our method.
ResNet-50 is trained with a learning rate schedule from 1e-5 to 1e-6. Only L2 norm is applied  with
λ = 1e − 5. Note that the identity connections alleviate the need to add layer-wise learning rate since
the gradient to the ﬁrst several layers is enough to pull the auxiliary parameters. The learning rate for
AlexNet is 1e-3 and for MobileNet V2 is 1e-5. We split the training data into 1:1 for weight update
and auxiliary parameter update respectively. Once the desired FLOPs is reached  we use all training
data to ﬁne tune the model.
For neuron pruning  we evaluate our method on compact MobileNet V2 with less redundancy  and
compare with the state-of-art methods in different FLOPs levels  in Table 3. Our method achieves
similar error at 300M level and outperforms others at extreme level(200M and 100M). For ResNet
at 600M FLOPs  the top-1 error is 27.6%. For weight pruning  the results in Table 4 show that our
method on AlexNet model achieves 18.5x compression rate and 0.84% accuracy drop. For ResNet-50 
we get 2.2x compression rate with only 0.4% accuracy drop.

4.5 Ablation Study

We show the sparsity and accuracy are not sensitive to hyperparameters  taking weight pruning with
VGG-like on CIFAR-10 as an example. In Fig. 2(a)  we set the learning rate of auxiliary parameters to
1e-2  1e-1 and 5e-1. From the result we observe that all three settings converge to similar compression
ratio with different sparsiﬁcation speed. In Fig. 2(b)  the accuracy with higher learning rate drops
faster  but the ﬁnal gap is less than 0.1%. In Fig. 2(c)  we show the compression ratio versus accuracy
plot with proposed update in Eq. 7 and regular BNN update. The regular BNN update becomes
non-stable after 30x CR  and accuracy drops sharply afterward. With the proposed update rule 
accuracy is more stable and with lower variance until 80x. We’ve also included the comparison on
choosing different STE functions and learning rates for VGG like model on CIFAR10 in Fig. 2(d).
Softplus STE achieves the best result while converges slower than LeakyReLU STE  which achieves
slightly lower CR. The linear STE however  yields worst CR and slower convergence speed.

(a) CR vs Iter

(b) CR vs accuracy

(c) Training CR vs accuracy(d) Comparison on different

STEs and learning rate

Figure 2: Illustration of Hyperparameter Sensitivity

4.6 Training From Scratch

Apart from sparsiﬁcation on pre-trained models  our method can support training sparse network from
scratch. We evaluate our method through training LeNet5 from scratch. All the weights are randomly
initialized as usual while the auxiliary parameters are initialized as mij ∼ Gaussian(0.1  0.05). The
initial learning rate is set to 1e-3 and gradually decreased to 1e-5. The ﬁnal model we obtain has an
error of 0.95% with a 168x compression rate.

5 Conclusions

In this paper  we propose to automatically prune deep neural networks by regularizing auxiliary
parameters instead of original weights values. The auxiliary parameters are not sensitive to hyperpa-
rameters and are more robust to noise during training. We also design a gradient-based update rule
for auxiliary parameters and analyze the beneﬁts. In addition  we combine sparse regularizers and
weight regularization to accelerate the sparsiﬁcation process. Extensive experiments show that our
method achieves the state-of-the-art sparsity in both weight pruning and neuron pruning compared
with existing approaches. Moreover  our model also supports training from scratch and can reach a
comparable sparsity.

9

01000020000300004000050000Iteration01020304050607080Compression Rate(CR)lr=1e-2lr=1e-1lr=5e-101020304050607080Compression Rate(CR)91.491.691.892.092.292.492.6Accuracy in %lr=1e-2lr=1e-1lr=5e-1020406080100Compression Rate(CR)888990919293Accuracy in %Proposed UpdateRegular BNN Update01000020000300004000050000Iteration01020304050607080Compression Rate(CR)lr=1e-2lr=1e-1lr=5e-1References
Alireza Aghasi  Afshin Abdi  Nam Nguyen  and Justin Romberg. Net-trim: Convex pruning of deep neural
In Advances in Neural Information Processing Systems  pages

networks with performance guarantee.
3177–3186  2017.

Miguel A Carreira-Perpinán and Yerlan Idelbayev. “learning-compression” algorithms for neural net pruning. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 8532–8541  2018.

Xin Dong  Shangyu Chen  and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain

surgeon. In Advances in Neural Information Processing Systems  pages 4857–4867  2017.

Aidan N Gomez  Ivan Zhang  Kevin Swersky  Yarin Gal  and Geoffrey E Hinton. Learning sparse networks

using targeted dropout. arXiv preprint arXiv:1905.13678  2019.

Ariel Gordon  Elad Eban  Oﬁr Nachum  Bo Chen  Hao Wu  Tien-Ju Yang  and Edward Choi. Morphnet: Fast &
simple resource-constrained structure learning of deep networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 1586–1595  2018.

Yiwen Guo  Anbang Yao  and Yurong Chen. Dynamic network surgery for efﬁcient dnns. In Advances In Neural

Information Processing Systems  pages 1379–1387  2016.

Song Han  Jeff Pool  John Tran  and William Dally. Learning both weights and connections for efﬁcient neural

network. In Advances in neural information processing systems  pages 1135–1143  2015.

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition. In

Proceedings of the IEEE conference on computer vision and pattern recognition  pages 770–778  2016.

Hengyuan Hu  Rui Peng  Yu-Wing Tai  and Chi-Keung Tang. Network trimming: A data-driven neuron pruning

approach towards efﬁcient deep architectures. arXiv preprint arXiv:1607.03250  2016.

Itay Hubara  Matthieu Courbariaux  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Binarized neural

networks. In Advances in neural information processing systems  pages 4107–4115  2016.

Yann LeCun  John S Denker  and Sara A Solla. Optimal brain damage. In Advances in neural information

processing systems  pages 598–605  1990.

Hao Li  Asim Kadav  Igor Durdanovic  Hanan Samet  and Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets.

In International Conference on Learning Representations  2017.

Guiying Li  Chao Qian  Chunhui Jiang  Xiaofen Lu  and Ke Tang. Optimization based layer-wise magnitude-

based pruning for dnn compression. In IJCAI  pages 2383–2389  2018.

Hanxiao Liu  Karen Simonyan  and Yiming Yang. DARTS: Differentiable architecture search. In International

Conference on Learning Representations  2019.

Zhuang Liu  Mingjie Sun  Tinghui Zhou  Gao Huang  and Trevor Darrell. Rethinking the value of network

pruning. In International Conference on Learning Representations  2019.

Christos Louizos  Karen Ullrich  and Max Welling. Bayesian compression for deep learning. In Advances in

Neural Information Processing Systems  pages 3288–3298  2017.

Christos Louizos  Max Welling  and Diederik P. Kingma. Learning sparse neural networks through l_0

regularization. In International Conference on Learning Representations  2018.

Ana I Maqueda  Antonio Loquercio  Guillermo Gallego  Narciso García  and Davide Scaramuzza. Event-based
vision meets deep learning on steering prediction for self-driving cars. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition  pages 5419–5427  2018.

Dmitry Molchanov  Arsenii Ashukha  and Dmitry Vetrov. Variational dropout sparsiﬁes deep neural networks.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages 2498–2507.
JMLR. org  2017.

Kirill Neklyudov  Dmitry Molchanov  Arsenii Ashukha  and Dmitry P Vetrov. Structured bayesian pruning via
log-normal multiplicative noise. In Advances in Neural Information Processing Systems  pages 6775–6784 
2017.

Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster r-cnn: Towards real-time object detection with

region proposal networks. In Advances in neural information processing systems  pages 91–99  2015.

10

Mark Sandler  Andrew Howard  Menglong Zhu  Andrey Zhmoginov  and Liang-Chieh Chen. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 4510–4520  2018.

Suraj Srinivas  Akshayvarun Subramanya  and R Venkatesh Babu. Training sparse neural networks.

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 138–145  2017.

In

Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural networks. In Advances

in neural information processing systems  pages 3104–3112  2014.

Mingxing Tan  Bo Chen  Ruoming Pang  Vijay Vasudevan  Mark Sandler  Andrew Howard  and Quoc V Le.
Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 2820–2828  2019.

Enzo Tartaglione  Skjalg Lepsøy  Attilio Fiandrotti  and Gianluca Francini. Learning sparse neural networks via
sensitivity-driven regularization. In Advances in Neural Information Processing Systems  pages 3878–3888 
2018.

Karen Ullrich  Edward Meeds  and Max Welling. Soft weight-sharing for neural network compression. In

International Conference on Learning Representations  2017.

Wei Wen  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li. Learning structured sparsity in deep neural

networks. In Advances in neural information processing systems  pages 2074–2082  2016.

Bichen Wu  Xiaoliang Dai  Peizhao Zhang  Yanghan Wang  Fei Sun  Yiming Wu  Yuandong Tian  Peter Vajda 
Yangqing Jia  and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via differentiable neural
architecture search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 10734–10742  2019.

Penghang Yin  Jiancheng Lyu  Shuai Zhang  Stanley J. Osher  Yingyong Qi  and Jack Xin. Understanding
In International Conference on

straight-through estimator in training activation quantized neural nets.
Learning Representations  2019.

Jiahui Yu and Thomas Huang. Network slimming by slimmable networks: Towards one-shot architecture search

for channel numbers. arXiv preprint arXiv:1903.11728  2019.

Jiahui Yu and Thomas Huang. Universally slimmable networks and improved training techniques. arXiv preprint

arXiv:1903.05134  2019.

Jiahui Yu  Linjie Yang  Ning Xu  Jianchao Yang  and Thomas Huang. Slimmable neural networks. arXiv

preprint arXiv:1812.08928  2018.

Xiaotian Zhu  Wengang Zhou  and Houqiang Li. Improving deep neural network sparsity through decorrelation

regularization. In IJCAI  pages 3264–3270  2018.

Zhuangwei Zhuang  Mingkui Tan  Bohan Zhuang  Jing Liu  Yong Guo  Qingyao Wu  Junzhou Huang  and Jinhui
Zhu. Discrimination-aware channel pruning for deep neural networks. In Advances in Neural Information
Processing Systems  pages 883–894  2018.

11

,XIA XIAO
Zigeng Wang
Sanguthevar Rajasekaran