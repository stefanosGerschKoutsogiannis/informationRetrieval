2013,Global Solver and Its Efficient Approximation for Variational Bayesian Low-rank Subspace Clustering,When a probabilistic model and its  prior are given  Bayesian learning offers inference with automatic parameter tuning. However  Bayesian learning is often obstructed by computational difficulty: the rigorous Bayesian learning is  intractable in many models  and its variational Bayesian (VB) approximation is prone to suffer from local minima. In this paper  we overcome this difficulty for low-rank subspace clustering (LRSC) by providing an exact global solver and its efficient approximation. LRSC extracts a low-dimensional structure of data by embedding  samples into the union of low-dimensional subspaces  and its variational Bayesian variant has shown good performance. We first prove a key property that the VB-LRSC model is highly redundant. Thanks to this property  the optimization problem of VB-LRSC can be separated into small subproblems  each of which has only a small number of unknown variables. Our exact global solver relies on another key property that the stationary condition of each subproblem is written as a set of polynomial equations  which is solvable with the homotopy method. For further computational efficiency   we also propose an efficient approximate variant  of which the stationary condition can be written as a polynomial equation with a single variable. Experimental results show the usefulness of our approach.,Global Solver and Its Efﬁcient Approximation for
Variational Bayesian Low-rank Subspace Clustering

Shinichi Nakajima
Nikon Corporation

Tokyo  140-8601 Japan

Akiko Takeda

The University of Tokyo
Tokyo  113-8685 Japan

nakajima.s@nikon.co.jp

takeda@mist.i.u-tokyo.ac.jp

S. Derin Babacan

Google Inc.

Mountain View  CA 94043 USA

Masashi Sugiyama

Tokyo Institute of Technology

Tokyo 152-8552  Japan

dbabacan@gmail.com

sugi@cs.titech.ac.jp

Ichiro Takeuchi

Nagoya Institute of Technology

Aichi  466-8555  Japan

takeuchi.ichiro@nitech.ac.jp

Abstract

When a probabilistic model and its prior are given  Bayesian learning offers infer-
ence with automatic parameter tuning. However  Bayesian learning is often ob-
structed by computational difﬁculty: the rigorous Bayesian learning is intractable
in many models  and its variational Bayesian (VB) approximation is prone to suf-
fer from local minima. In this paper  we overcome this difﬁculty for low-rank
subspace clustering (LRSC) by providing an exact global solver and its efﬁcient
approximation. LRSC extracts a low-dimensional structure of data by embedding
samples into the union of low-dimensional subspaces  and its variational Bayesian
variant has shown good performance. We ﬁrst prove a key property that the VB-
LRSC model is highly redundant. Thanks to this property  the optimization prob-
lem of VB-LRSC can be separated into small subproblems  each of which has
only a small number of unknown variables. Our exact global solver relies on an-
other key property that the stationary condition of each subproblem consists of
a set of polynomial equations  which is solvable with the homotopy method. For
further computational efﬁciency  we also propose an efﬁcient approximate variant 
of which the stationary condition can be written as a polynomial equation with a
single variable. Experimental results show the usefulness of our approach.

1 Introduction

Principal component analysis (PCA) is a widely-used classical technique for dimensionality reduc-
tion. This amounts to globally embedding the data points into a low-dimensional subspace. As more
ﬂexible models  the sparse subspace clustering (SSC) [7  20] and the low-rank subspace clustering
(LRSC) [8  13  15  14] were proposed. By inducing sparsity and low-rankness  respectively  SSC
and LRSC locally embed the data into the union of subspaces. This paper discusses a probabilistic
model for LRSC.
As the classical PCA requires users to pre-determine the dimensionality of the subspace  LRSC re-
quires manual parameter tuning for adjusting the low-rankness of the solution. On the other hand 

1

Bayesian formulations enable us to estimate all unknown parameters without manual parameter
tuning [5  4  17]. However  in many problems  the rigorous application of Bayesian inference is
computationally intractable. To overcome this difﬁculty  the variational Bayesian (VB) approxima-
tion was proposed [1]. A Bayesian formulation and its variational inference have been proposed for
LRSC [2]. There  to avoid computing the inverse of a prohibitively large matrix  the posterior is
approximated with the matrix-variate Gaussian (MVG) [11].
Typically  the VB solution is computed by the iterated conditional modes (ICM) algorithm [3  5] 
which is derived through the standard procedure for the VB approximation. Since the objective
function for the VB approximation is generally non-convex  the ICM algorithm is prone to suffer
from local minima. So far  the global solution for the VB approximation is not attainable except PCA
(or the fully-observed matrix factorization)  for which the global VB solution has been analytically
obtained [17]. This paper makes LRSC another exception with proposed global VB solvers.
Two common factors make the global VB solution attainable in PCA and LRSC: ﬁrst  a large portion
of the degrees of freedom that the VB approximation learns are irrelevant  and the optimization
problem can be decomposed into subproblems  each of which has only a small number of unknown
variables; second  the stationary condition of each subproblem is written as a polynomial system (a
set of polynomial equations).
Based on these facts  we propose an exact global VB solver (EGVBS) and an approximate global
VB solver (AGVBS). EGVBS ﬁnds all stationary points by solving the polynomial system with the
homotopy method [12  10]  and outputs the one giving the lowest free energy. Although EGVBS
solves subproblems with much less complexity than the original VB problem  it is still not efﬁcient
enough for handling large-scale data. For further computational efﬁciency  we propose AGVBS  of
which the stationary condition is written as a polynomial equation with a single variable. Our ex-
periments on artiﬁcial and benchmark datasets show that AGVBS provides a more accurate solution
than the MVG approximation [2] with much less computation time.

2 Background

In this section  we introduce the low-rank subspace clustering and its variational Bayesian formula-
tion.

2.1 Subspace Clustering Methods
Let Y 2 RL(cid:2)M = (y1; : : : ; yM ) be L-dimensional observed samples of size M. We generally
denote a column vector of a matrix by a bold-faced small letter. We assume that each ym is ap-
′ words in a dictionary  D = (d1; : : : ; dM′ ) 
proximately expressed as a linear combination of M
i.e. 

Y = DX + E;

′(cid:2)M is unknown coefﬁcients  and E 2 RL(cid:2)M is noise. In subspace clustering 
where X 2 RM
the observed matrix Y itself is often used as a dictionary D. The convex formulation of the sparse
subspace clustering (SSC) [7  20] is given by

∥Y (cid:0) Y X∥2

Fro + (cid:21)∥X∥1; s.t. diag(X) = 0;

min
X

obtained subspace. After the minimizer bX is obtained  abs(bX) + abs(bX

(1)
where X 2 RM(cid:2)M is a parameter to be estimated  (cid:21) > 0 is a regularization coefﬁcient to be
manually tuned. ∥ (cid:1) ∥Fro and ∥ (cid:1) ∥1 are the Frobenius norm and the (element-wise) ℓ1-norm of a
matrix  respectively. The ﬁrst term in Eq.(1) requires that each data point ym can be expressed as
a linear combination of a small set of other data points fdm′g for m
′ ̸= m. This smallness of the
set is enforced by the second (ℓ1-regularization) term  and leads to the low-dimensionality of each
)  where abs((cid:1)) takes the
absolute value element-wise  is regarded as an afﬁnity matrix  and a spectral clustering algorithm 
such as normalized cuts [19]  is applied to obtain clusters.
In the low-rank subspace clustering (LRSC) or low-rank representation [8  13  15  14]  low-
dimensional subspaces are sought by enforcing the low-rankness of X:

⊤

(2)

∥Y (cid:0) Y X∥2

Fro + (cid:21)∥X∥tr:

min
X

2

Thanks to the simplicity  the global solution of Eq.(2) has been analytically obtained [8].

2.2 Variational Bayesian Low-rank Subspace Clustering

((cid:0) 1

((cid:0) 1

We formulate the probabilistic model of LRSC  so that the maximum a posteriori (MAP) estimator
coincides with the solution of the problem (2) under a certain hyperparameter setting:

p(Y jA
′
) / exp

) / exp
(3)
(cid:0)1
(cid:0)1
′
(4)
2tr(A
A A
C
B B
′⊤  as in [2]  to induce low-rankness through the model-induced
′
Here  we factorized X as X = B
A
′ 2 RM(cid:2)H for H (cid:20)
regularization mechanism [17].
In this formulation  A
min(L; M ) are the parameters to be estimated. We assume that hyperparameters

2tr(B
′ 2 RM(cid:2)H and B

)
2(cid:27)2∥Y (cid:0) DB
′⊤

((cid:0) 1

′
p(A

)
′⊤∥2
;
A
) / exp
′

)

p(B

; B

′⊤

Fro

C

)

)

′

′

′

:

;

CA = diag(c2
a1

; : : : ; c2

aH

);

CB = diag(c2
b1

; : : : ; c2
bH

):

are diagonal and positive deﬁnite. The dictionary D is treated as a constant  and set to D = Y   once
Y is observed.1
The Bayes posterior is written as

′

′jY ) = p(Y jA

′

′

′

′

;B

)p(A

)p(B

′

′

; B

p(A

(5)
)⟩p(A′)p(B′) is the marginal likelihood. Here  ⟨(cid:1)⟩p denotes the expecta-
where p(Y ) = ⟨p(Y jA
tion over the distribution p. Since the Bayes posterior (5) is computationally intractable  we adopt
the variational Bayesian (VB) approximation [1  5].
′
Let r(A
called the free energy:

)  or r for short  be a trial distribution. The following functional with respect to r is

; B

; B

p(Y )

′

;

)

F (r) =

log

′
r(A

′

p(Y jA′;B′);p(A′)p(B′)

;B

)

r(A′;B′)

=

′
log r(A

)
p(A′;B′jY )

;B

′

(cid:0) log p(Y ):

(6)

r(A′;B′)

⟩

⟩

⟨

⟨

In the last equation of Eq.(6)  the ﬁrst term is the Kullback-Leibler (KL) distance from the trial
distribution to the Bayes posterior  and the second term is a constant. Therefore  minimizing the free
energy (6) amounts to ﬁnding a distribution closest to the Bayes posterior in the sense of the KL
distance. In the VB approximation  the free energy (6) is minimized over some restricted function
space.

2.2.1 Standard VB (SVB) Iteration

The standard procedure for the VB approximation imposes the following constraint on the posterior:

′
r(A

; B

′

′
) = r(A

′

)r(B

):

By using the variational method  we can show that the VB posterior is Gaussian  and has the follow-
ing form:

(

′(cid:0)bA

′

)

′(cid:0)bA

′

⊤
)

)

′
r(A

) / exp

(cid:0) tr((A

)(cid:6)

(cid:0)1
A′ (A
2

;
) 2 RM H. The means and the covariances satisfy the following equations:

r(B

;

′

′

) / exp

(cid:0) ((cid:21)b

(cid:0)1
⊤ (cid:21)(cid:6)
B′ ((cid:21)b
)
2

(

′

′(cid:0)b(cid:21)b
)(cid:0)1

)

′(cid:0)b(cid:21)b

′

)

(7)

(8)

)(cid:0)1

′⟩

(⟨
(
(bA
′⊤bA

′⊤

B

Y

where (cid:21)b

bA
b(cid:21)b

′

′

′

= vec(B
′

⊤

Y bB
(

(cid:27)2 Y
(cid:21)(cid:6)B′
(cid:27)2 vec

= 1

)

(cid:6)A′;
⊤

Y bA

′

(cid:6)A′ = (cid:27)2

⊤

Y B

r(B′) + (cid:27)2C

(cid:0)1
A

;

′

+ M (cid:6)A′) (cid:10) Y

⊤

(cid:0)1
B

(cid:10) IM )

;

Y

=

(cid:21)(cid:6)B′ = (cid:27)2

(9)
where (cid:10) denotes the Kronecker product of matrices  and IM is the M-dimensional identity matrix.
1 Our formulation is slightly different from the one proposed in [2]  where a clean version of Y is introduced
as an additional parameter to cope with outliers. Since we focus on the LRSC model without outliers in this
paper  we simpliﬁed the model. In our formulation  the clean dictionary corresponds to Y BA
  where
y denotes the pseudo-inverse of a matrix.

Y + (cid:27)2(C

(BA

y
)

⊤

⊤

;

3

(

∑

′
h

c2
ah

= ∥ba
(

For empirical VB learning  where the hyperparameters are also estimated from observation  the
following are obtained from the derivatives of the free energy (6):
∥2 +
′⊤⟩
+M (cid:6)A′ )B

∥2=M + (cid:27)2
(
′bA
IM(cid:0)2bB

(11)
;
)) are the diagonal entries
where ((cid:27)2
′
a
1
of (cid:6)A′ and (cid:21)(cid:6)B′  respectively. Eqs.(8)–(11) form an ICM algorithm  which we call the standard VB
(SVB) iteration.

))
M
m=1 (cid:27)2
B

(bA
′⊤bA

) and (((cid:27)2
B

; : : : ; (cid:27)2
′
a
H

); : : : ; ((cid:27)2
B

; : : : ; (cid:27)2
B

; : : : ; (cid:27)2
B

∥bb

+⟨B

(cid:27)2 =

;
′⊤

′
M;H

(10)

r(B′ )

=
′

′
m;h

′
M;1

=M;

c2
bh

′
1;H

′
1;1

′
a
h

′
h

LM

⊤

tr

′

Y

Y

)

2.2.2 Matrix-Variate Gaussian Approximate (MVGA) Iteration

Actually  the SVB iteration cannot be applied to a large-scale problem  because Eq.(9) requires the
inversion of a huge M H (cid:2) M H matrix. This difﬁculty can be avoided by restricting r(B
) to be
the matrix-variate Gaussian (MVG) [11]  i.e. 

′

(

(

′

) / exp

r(B

(cid:0) 1
2tr

(cid:0)1
B′ (B

(cid:2)

′

(cid:0)1
B′ (B

)(cid:6)

:

(12)

′ (cid:0) bB

))

′ (cid:0) bB

⊤

′

)

Under this additional constraint  a gradient-based computationally tractable algorithm can be derived
[2]  which we call the MVG approximate (MVGA) iteration.

3 Global Variational Bayesian Solvers

In this section  we ﬁrst show that a large portion of the degrees of freedom in the expression (7)
are irrelevant  which signiﬁcantly reduces the complexity of the optimization problem without the
MVG approximation. Then  we propose an exact global VB solver and its approximation.

3.1

Irrelevant Degrees of Freedom of VB-LRSC

Consider the following transforms:

;

;

′

Y

Y

Y

B

′
A

(13)

where

Y = Ωleft

A = Ωright⊤

B = Ωright⊤

Y (cid:0)Y Ωright⊤
is the singular value decomposition (SVD) of Y . Then  we obtain the following theorem:

Theorem 1 The global minimum of the VB free energy (6) is achieved with a solution such that

bA; bB; (cid:6)A; (cid:21)(cid:6)B are diagonal.
i.e.  Y ! (cid:0)Y . Since we assume Gaussian priors with no correlation  the solution bBbA
′⊤bA
investigating perturbations around any solution. We ﬁrst show that bA

(Sketch of proof) After the transform (13)  we can regard the observed matrix as a diagonal matrix 
⊤ is naturally
expected to be diagonal. To prove this intuition  we apply a similar approach to [17]  where the
diagonalities of the VB posterior covariances were proved in fully-observed matrix factorization by
+M (cid:6)A′ is diagonal  with
2

which Eq.(9) implies the diagonality of (cid:21)(cid:6)B. Other diagonalities can be shown similarly.
Theorem 1 does not only reduce the complexity of the optimization problem greatly  but also makes
the problem separable  as shown in the following.

′

3.2 Exact Global VB Solver (EGVBS)

Thanks to Theorem 1  the free energy minimization problem can be decomposed as follows:
(ba1; : : : ;baH ); ((cid:27)2
Lemma 1 Let J((cid:20) min(L; M )) be the rank of Y   (cid:13)m be the m-th largest singular value of Y   and
be the diagonal entries of bA; (cid:6)A; bB; (cid:21)(cid:6)B  respectively. Then  the free energy (6) is written as
(

); (bb1; : : : ;bbH ); (((cid:27)2
∑
∑

; : : : ; (cid:27)2
aH

); : : : ; ((cid:27)2

; : : : ; (cid:27)2

; : : : ; (cid:27)2

)

BM;H

BM;1

B1;H

B1;1

))

a1

F = 1
2

LM log(2(cid:25)(cid:27)2) +

J

h=1 (cid:13)2
(cid:27)2 +

h

H
h=1 2Fh

;

where

(14)

4

h
(cid:27)2

bah = (cid:13)2
bbh =
=ba2

(cid:13)2
h
(cid:27)2

c2
ah

(
8<:(cid:27)2
(bb2

c2
bh

bb2
(
∑

(cid:13)2
m

;
(m (cid:20) J);
(m > J);

)

Y (cid:0)Y Ωright⊤

Algorithm 1 Exact Global VB Solver (EGVBS) for LRSC.
1: Calculate the SVD of Y = Ωleft
2: for h = 1 to H do
3:
4:
5:
6:

Find all the solutions of the polynomial system (16)–(18) by the homotopy method.
Discard prohibitive solutions with complex numbers or with negative variances.
Select the stationary point giving the smallest Fh (deﬁned by Eq.(15)).
The global solution for h is the selected stationary point if it satisﬁes Fh < 0  otherwise the
null solution (19).

Y

.

Y

⊤

Y

7: end for

2Fh = M log

bBbA
Ωright⊤
∑
bbh +bb2
(cid:0)2bah
h(ba2

8: Calculate bX = Ωright
9: Apply spectral clustering with the afﬁnity matrix equal to abs(bX) + abs(bX
∑
bb2
(ba2
)(cid:0)1
)(cid:0)1

m(cid:27)2
and its stationary condition is given as follows: for each h = 1; : : : ; H 

∑
(cid:0) (M + J) +
∑
(ba2

{
bbh(cid:27)2
bah(cid:27)2

Bm;h
h + M (cid:27)2
ah

h + M (cid:27)2
ah

;

(cid:27)2
Bm;h

=

J
m=1 log

+ (cid:27)2
c2
bh

J
m=1 (cid:13)2

J
m=1 (cid:13)2

+ (cid:27)2
c2
ah

h+M (cid:27)2
ah

+ 1
(cid:27)2

m(cid:27)2

= (cid:27)2

c2
ah
(cid:27)2

)

(

ba2

(cid:27)2
ah

h +

)

+

)

;

ah

c2
bh

(cid:27)2

+

ah

Bm;h

Bm;h

(cid:13)2
h

(cid:13)2
h

Bh;h

c2
ah

+

⊤

).

Bm;h

}

J

h+

m=1 (cid:27)2
c2
bh
h + M (cid:27)2
ah

)

;

(15)

If no stationary point gives Fh < 0  the global solution is given by

c2
bh

=

h +

J
m=1 (cid:27)2

Bm;h

=J:

h=M + (cid:27)2
ah

;

bah =bbh = 0;

(cid:27)2
ah

; (cid:27)2

Bm;h

; c2
ah

; c2
bh

for m = 1; : : : ; M:

! 0

(bah; (cid:27)2

;bbh;f(cid:27)2

Bm;h

= (cid:27)2

for m > J  Eqs.(16)–(18) for each h can be seen
Taking account of the trivial relations c2
bh
.
as a polynomial system with 5 + J unknown variables  i.e. 
Thus  Lemma 1 has decomposed the original problem (8)–(10) with O(M 2H 2) unknown variables
into H subproblems with O(J) variables each.
Given the noise variance (cid:27)2  our exact global VB solver (EGVBS) ﬁnds all stationary points that sat-
isfy the polynomial system (16)–(18) by using the homotopy method [12  10] 2 After that  it discards
the prohibitive solutions with complex numbers or with negative variances  and then selects the one
giving the smallest Fh  deﬁned by Eq.(15). The global solution is the selected stationary point if
it satisﬁes Fh < 0  or the null solution (19) otherwise. Algorithm 1 summarizes the procedure of
EGVBS. If (cid:27)2 is unknown  we conduct a naive 1-D search by iteratively applying EGVBS  as for
VB matrix factorization [17].

gJ
m=1; c2
bh

; c2
ah

Bm;h

ah

3.3 Approximate Global VB Solver (AGVBS)

Although Lemma 1 signiﬁcantly reduced the complexity of the optimization problem  EGVBS is
not applicable to large-scale data  since the homotopy method is not guaranteed to ﬁnd all the solu-
tions in polynomial time in J  when the polynomial system involves O(J) unknown variables. For
large-scale data  we propose a scalable approximation by introducing an additional constraint that
(cid:13)2
m(cid:27)2

are constant over m = 1; : : : ; J  i.e. 

Bm;h

(cid:13)2
m(cid:27)2

(20)
2 The homotopy method is a reliable and efﬁcient numerical method to solve a polynomial system [6  9]. It
provides all the isolated solutions to a system of n polynomials f (x) (cid:17) (f1(x); : : : ; fn(x)) = 0 by deﬁning a
smooth set of homotopy systems with a parameter t 2 [0; 1]: g(x; t) (cid:17) (g1(x; t); g2(x; t); : : : ; gn(x; t)) = 0 
such that one can continuously trace the solution path from the easiest (t = 0) to the target (t = 1). We use
HOM4PS-2.0 [12]  which is one of the most successful polynomial system solvers.

= (cid:27)2
bh

Bm;h

for all m (cid:20) J:

5

(16)

(17)

(18)

(19)

)

Under this constraint  we obtain the following theorem (the proof is omitted):

ﬁes the following polynomial equation with a single variablebb(cid:13)h:

Theorem 2 Under the constraint (20)  any stationary point of the free energy (15) for each h satis-

bb(cid:13)

bb(cid:13)

bb(cid:13)

bb(cid:13)h + (cid:24)0 = 0;

bb(cid:13)

bb(cid:13)

(cid:24)6

6
h + (cid:24)5

5
h + (cid:24)4

4
h + (cid:24)3

3
h + (cid:24)2

2
h + (cid:24)1

where

;

(cid:13)3
h

(cid:13)h

(cid:13)4
h

(cid:13)h

h)

(cid:13)3
h

(cid:13)2
h

h)

(cid:13)h

(cid:13)2
h

(cid:13)h

(cid:13)2
h
h)

(cid:24)4 = ϕ2

+ 1 + ϕ2

Here  (cid:13) = (

+ ϕhM J(cid:27)4

(cid:24)5 = (cid:0)2 ϕ2

(cid:0) ϕhM (cid:27)2((M +J)(cid:27)2(cid:0)(cid:13)2

hM (cid:27)2
+ 2ϕh
(cid:13)3
(cid:13)h
h
(cid:0) 2(M(cid:0)J)(cid:27)2

(cid:24)6 = ϕ2
h
(cid:13)2
h
(cid:24)3 = 2ϕhM (M(cid:0)J)(cid:27)4
(cid:24)2 = (M(cid:0)J)2(cid:27)4
∑
(cid:24)1 = (cid:0) (M(cid:0)J)(cid:27)2((M +J)(cid:27)2(cid:0)(cid:13)2
b(cid:13)h =bb(cid:13) + (cid:13)h (cid:0) M (cid:27)2
; b(cid:20) = (cid:13)2
√b(cid:20)2 (cid:0) 4M J(cid:27)4
(b(cid:20) +
b(cid:28) = 1
)
(bah; (cid:27)2
;bbh; (cid:27)2

(cid:13)h
(cid:0)2
m =J)

;
1 (cid:0) (cid:13)2

+ ((M + J)(cid:27)2 (cid:0) (cid:13)2

(cid:0) 2ϕh(2M(cid:0)J)(cid:27)2
(cid:0) ϕ2

h(M (cid:27)2(cid:0)(cid:13)2
h)
hM (cid:27)2(M (cid:27)2(cid:0)(cid:13)2
+ ϕh(M (cid:27)2(cid:0)(cid:13)2
h)
h) (cid:0) ϕh(M(cid:0)J)(cid:27)2(M (cid:27)2(cid:0)(cid:13)2

hM 2(cid:27)4
;
+ ϕh((M +J)(cid:27)2(cid:0)(cid:13)2
)
(
. For each real solutionbb(cid:13)h such that
(cid:0) (M + J)(cid:27)2 (cid:0)(
))
)(cid:0)1
(
; b(cid:14)h = (cid:27)2pb(cid:28)
(cid:0)b(cid:13)h
bb(cid:13)
)
(√b(cid:13)b(cid:14); (cid:27)2b(cid:14)h
√b(cid:13)=b(cid:14)=(cid:13)h;
pb(cid:28) =(cid:13)2
pb(cid:28) ;

bb(cid:13)
ϕh
(cid:13)h (cid:0) M (cid:27)2

M (cid:27)2 (cid:0) (cid:13)2

are real and positive  the corresponding stationary point candidate is given by

(cid:0)1 and ϕh =

(cid:24)0 = M J(cid:27)4:

)
(

J
m=1 (cid:13)

1 + ϕh

;

(cid:13)h

h)

;

2M J

h
(cid:13)2

(cid:13)2
h

; c2
ah

ah

; c2
bh

bh

=

(cid:13)h

h

(cid:13)h

;

(cid:13)h

(cid:13)h

;

(cid:13)2
h

h

b(cid:14)h(cid:0)ϕh

(cid:27)2

(cid:13)h

;

(cid:27)2pb(cid:28)

h)

(21)

;

(22)

;

(23)

(24)

(25)

(26)

(27)

:

(28)

h

Given the noise variance (cid:27)2  obtaining the coefﬁcients (22)–(25) is straightforward. Our approx-
imate global VB solver (AGVBS) solves the sixth-order polynomial equation (21)  e.g.  by the
‘roots’ function in MATLAB R⃝  and obtain all candidate stationary points by using Eqs.(26)–(28).
Then  it selects the one giving the smallest Fh  and the global solution is the selected stationary point
if it satisﬁes Fh < 0  otherwise the null solution (19). Note that  although a solution of Eq.(21) is not
necessarily a stationary point  selection based on the free energy discards all non-stationary points
and local maxima. As in EGVBS  a naive 1-D search is conducted for estimating (cid:27)2.
In Section 4  we show that AGVBS is practically a good alternative to the MVGA iteration in terms
of accuracy and computation time.

4 Experiments

In this section  we experimentally evaluate the proposed EGVBS and AGVBS. We assume that the
hyperparameters (CA; CB) and the noise variance (cid:27)2 are unknown and estimated from observations.
We use the full-rank model (i.e.  H = min(L; M ))  and expect VB-LRSC to automatically ﬁnd the
true rank without any parameter tuning.
We ﬁrst conducted an experiment with a small artiﬁcial dataset (‘artiﬁcial small’)  on which the
exact algorithms  i.e.  the SVB iteration (Section 2.2.1) and EGVBS (Section 3.2)  are computation-
ally tractable. Through this experiment  we can measure the accuracy of the efﬁcient approximate
variants  i.e.  the MVGA iteration (Section 2.2.2) and AGVBS (Section 3.3). We randomly cre-
ated M = 75 samples in L = 10 dimensional space. We assumed K = 2 clusters: M (1)(cid:3)
= 50
samples lie in a H (1)(cid:3)
= 25 samples lie in a
= 1 dimensional subspace. For each cluster k  we independently drew M (k)(cid:3) samples from
H (2)(cid:3)
N
H (k)(cid:3)(0; 10IH (k)(cid:3) )  where Nd((cid:22); (cid:6)) denotes the d-dimensional Gaussian  and projected them
into the observed L-dimensional space by R(k) 2 RL(cid:2)H (k)(cid:3)
  each entry of which follows N1(0; 1).
∑
Thus  we obtained a noiseless matrix Y (k)(cid:3) 2 RL(cid:2)M (k)(cid:3)
for the k-th cluster. Concatenating all
)  and adding random noise subject to N1(0; 1) to each entry gave
clusters  Y
an artiﬁcial observed matrix Y 2 RL(cid:2)M   where M =
(cid:3)
k=1 M (k)(cid:3)
= 75. The true rank of Y

= 3 dimensional subspace  and the other M (2)(cid:3)

; : : : ; Y (K)(cid:3)

= (Y (1)(cid:3)

(cid:3)

K

6

(a) Free energy

(b) Computation time

(c) Estimated rank
(cid:3)

Figure 1: Results on the ‘artiﬁcial small’ dataset (L = 10; M = 75; H
were 1:3% for EGVBS  AGVBS  and the SVB iteration  and 2:4% for the MVGA iteration.

= 4). The clustering errors

(a) Free energy

(b) Computation time

Figure 2: Results on the ‘artiﬁcial large’ dataset (L = 50; M = 225; H
were 4:0% for AGVBS and 11:2% for the MVGA iteration.

(c) Estimated rank
(cid:3)

= 5). The clustering errors

∑
k=1 H (k)(cid:3)

K

(a) Free energy

(b) Computation time

(c) Estimated rank

Figure 3: Results on the ‘1R2RC’ sequence (L = 59; M = 459) of the Hopkins 155 motion
database. The clustering errors are shown in Figure 4.

(cid:3)

= min(

(cid:3) is different from the rank J of the

′bA

is given by H
observed matrix Y   which is almost surely equal to min(L; M ) = 10 under the Gaussian noise.

Figure 1 shows the free energy  the computation time  and the estimated rank of bX = bB

; L; M ) = 4. Note that H

′⊤
over iterations. For the iterative methods  we show the results of 10 trials starting from different
random initializations. We can see that AGVBS gives almost the same free energy as the exact
methods (EGVBS and the SVB iteration). The exact method requires a large computation cost:
EGVBS took 621 sec to obtain the global solution  and the SVB iteration took (cid:24) 100 sec to achieve
almost the same free energy. The approximate methods are much faster: AGVBS took less than 1
sec  and the MVGA iteration took (cid:24) 10 sec. Since the MVGA iteration had not converged after
250 iterations  we continued the MVGA iteration until 2500 iterations  and found that the MVGA
iteration sometimes converges to a local solution with signiﬁcantly higher free energy than the other
methods. EGVBS  AGVBS  and the SVB iteration successfully found the true rank H
= 4  while
the MVGA iteration sometimes failed. This difference is actually reﬂected to the clustering error 
i.e.  the misclassiﬁcation rate with all possible cluster correspondences taken into account  after
spectral clustering [19] is performed: 1:3% for EGVBS  AGVBS  and the SVB iteration  and 2:4%
for the MVGA iteration.
Next we conducted the same experiment with a larger artiﬁcial dataset (‘artiﬁcial large’) (L =
50; K = 4; (M (1)(cid:3)
) = (2; 1; 1; 1))  on which
EGVBS and the SVB iteration are computationally intractable. Figure 2 shows results with AGVBS
and the MVGA iteration. An advantage in computation time is clear: AGVBS took (cid:24) 0:1 sec  while
the MVGA iteration took more than 100 sec. The clustering errors were 4:0% for AGVBS and
11:2% for the MVGA iteration.
Finally  we applied AGVBS and the MVGA iteration to the Hopkins 155 motion database [21].
In this dataset  each sample corresponds to a trajectory of a point in a video  and clusteirng the
trajectories amounts to ﬁnding a set of rigid bodies. Figure 3 shows the results on the ‘1R2RC’

) = (100; 50; 50; 25); (H (1)(cid:3)

; : : : ; M (K)(cid:3)

; : : : ; H (K)(cid:3)

(cid:3)

7

0501001502002501.81.922.12.22.3IterationF/LM EGVBSAGVBSSVBIterationMVGAIteration050100150200250100102104IterationTime(sec) EGVBSAGVBSSVBIterationMVGAIteration0501001502002500246810IterationbH EGVBSAGVBSSVBIterationMVGAIteration050010001500200025001.611.6151.621.6251.631.635IterationF/LM AGVBSMVGAIteration05001000150020002500100102104IterationTime(sec) AGVBSMVGAIteration05001000150020002500051015IterationbH AGVBSMVGAIteration05001000150020002500234567IterationF/LM AGVBSMVGAIteration05001000150020002500100102104IterationTime(sec) AGVBSMVGAIteration0500100015002000250001020304050IterationbH AGVBSMVGAIterationFigure 4: Clustering errors on the ﬁrst 20 sequences of Hopkins 155 dataset.

(L = 59; M = 459) sequence.3 We see that AGVBS gave a lower free energy with much less
computation time than the MVGA iteration. Figure 4 shows the clustering errors over the ﬁrst 20
sequences. We ﬁnd that AGVBS generally outperforms the MVGA iteration. Figure 4 also shows
the results with MAP estimation (2) with the tuning parameter (cid:21) optimized over the 20 sequences
(we performed MAP with different values for (cid:21)  and selected the one giving the lowest average
clustering error). We see that AGVBS performs comparably to MAP with optimized (cid:21)  which
implies that VB estimates the hyperparameters and the noise variance reasonably well.

5 Conclusion

In this paper  we proposed a global variational Bayesian (VB) solver for low-rank subspace clus-
tering (LRSC)  and its approximate variant. The key property that enabled us to obtain a global
solver is that we can signiﬁcantly reduce the degrees of freedom of the VB-LRSC model  and the
optimization problem is separable.
Our exact global VB solver (EGVBS) provides the global solution of a non-convex minimization
problem by using the homotopy method  which solves the stationary condition written as a poly-
nomial system. On the other hand  our approximate global VB solver (AGVBS) ﬁnds the roots of
a polynomial equation with a single unknown variable  and provides the global solution of an ap-
proximate problem. We experimentally showed advantages of AGVBS over the previous scalable
method  called the matrix-variate Gaussian approximate (MVGA) iteration  in terms of accuracy and
computational efﬁciency. In AGVBS  SVD dominates the computation time. Accordingly  applying
additional tricks  e.g.  parallel computation and approximation based on random projection  to the
SVD calculation would be a vital option for further computational efﬁciency.
LRSC can be equipped with an outlier term  which enhances robustness [7  8  2]. With the outlier
term  much better clustering error on Hopkins 155 dataset was reported [2]. Our future work is to
extend our approach to such robust variants. Theorem 2 enables us to construct the mean update
(MU) algorithm [16]  which ﬁnds the global solution with respect to a large number of unknown
variables in each step. We expect that the MU algorithm tends to converge to a better solution than
the standard VB iteration  as in robust PCA and its extensions. EGVBS and AGVBS cannot be
applied to the applications where Y has missing entries. Also in such cases  Theorem 2 might be
used to derive a better algorithm  as the VB global solution of fully-observed matrix factorization
(MF) was used as a subroutine for partially-observed MF [18].
In many probabilistic models  the Bayesian learning is often intractable  and its VB approximation
has to rely on a local search algorithm. Exceptions are the fully-observed MF  for which an analytic-
form of the global solution has been derived [17]  and LRSC  for which this paper provided global
VB solvers. As in EGVBS  the homotopy method can solve a stationary condition if it can be written
as a polynomial system. We expect that such a tool would extend the attainability of global solutions
of non-convex problems  with which machine learners often face.

Acknowledgments

The authors thank the reviewers for helpful comments. SN  MS  and IT thank the support from
MEXT Kakenhi 23120004  the FIRST program  and MEXT KAKENHI 23700165  respectively.

3Peaks in free energy curves are due to pruning  which is necessary for the gradient-based MVGA iteration.

The free energy can jump just after pruning  but immediately get lower than the value before pruning.

8

00.20.40.6ClusteringError 1R2RC1R2RCR1R2RCR_g121R2RCR_g131R2RCR_g231R2RCT_A1R2RCT_A_g121R2RCT_A_g131R2RCT_A_g231R2RCT_B1R2RCT_B_g121R2RCT_B_g131R2RCT_B_g231R2RC_g121R2RC_g131R2RC_g231R2TCR1R2TCRT1R2TCRT_g121R2TCRT_g13MAP (with optimized lambda)AGVBSMVGAIterationReferences
[1] H. Attias. Inferring parameters and structure of latent variable models by variational Bayes. In Proc. of

UAI  pages 21–30  1999.

[2] S. D. Babacan  S. Nakajima  and M. N. Do. Probabilistic low-rank subspace clustering. In Advances in

Neural Information Processing Systems 25  pages 2753–2761  2012.

[3] J. Besag. On the statistical analysis of dirty pictures. Journal of the Royal Statistical Society B  48:259–

302  1986.

[4] C. M. Bishop. Variational principal components. In Proc. of International Conference on Artiﬁcial Neural

Networks  volume 1  pages 514–509  1999.

[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer  New York  NY  USA  2006.
[6] F. J. Drexler. A homotopy method for the calculation of all zeros of zero-dimensional polynomial ideals.

In H. J. Wacker  editor  Continuation methods  pages 69–93  New York  1978. Academic Press.

[7] E. Elhamifar and R. Vidal. Sparse subspace clustering. In Proc. of CVPR  pages 2790–2797  2009.
[8] P. Favaro  R. Vidal  and A. Ravichandran. A closed form solution to robust subspace estimation and

clustering. In Proceedings of CVPR  pages 1801–1807  2011.

[9] C. B. Garcia and W. I. Zangwill. Determining all solutions to certain systems of nonlinear equations.

Mathematics of Operations Research  4:1–14  1979.

[10] T. Gunji  S. Kim  M. Kojima  A. Takeda  K. Fujisawa  and T. Mizutani. Phom—a polyhedral homotopy

continuation method. Computing  73:57–77  2004.

[11] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions. Chapman and Hall/CRC  1999.
[12] T. L. Lee  T. Y. Li  and C. H. Tsai. Hom4ps-2.0: a software package for solving polynomial systems by

the polyhedral homotopy continuation method. Computing  83:109–133  2008.

[13] G. Liu  Z. Lin  and Y. Yu. Robust subspace segmentation by low-rank representation. In Proc. of ICML 

pages 663–670  2010.

[14] G. Liu  H. Xu  and S. Yan. Exact subspace segmentation and outlier detection by low-rank representation.

In Proc. of AISTATS  2012.

[15] G. Liu and S. Yan. Latent low-rank representation for subspace segmentation and feature extraction. In

Proc. of ICCV  2011.

[16] S. Nakajima  M. Sugiyama  and S. D. Babacan. Variational Bayesian sparse additive matrix factorization.

Machine Learning  92:319–1347  2013.

[17] S. Nakajima  M. Sugiyama  S. D. Babacan  and R. Tomioka. Global analytic solution of fully-observed

variational Bayesian matrix factorization. Journal of Machine Learning Research  14:1–37  2013.

[18] M. Seeger and G. Bouchard. Fast variational Bayesian inference for non-conjugate matrix factorization
models. In Proceedings of International Conference on Artiﬁcial Intelligence and Statistics  La Palma 
Spain  2012.

[19] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Machine Intell. 

22(8):888–905  2000.

[20] M. Soltanolkotabi and E. J. Cand`es. A geometric analysis of subspace clustering with outliers. CoRR 

2011.

[21] R. Tron and R. Vidal. A benchmark for the comparison of 3-D motion segmentation algorithms. In Proc.

of CVPR  2007.

9

,Shinichi Nakajima
Akiko Takeda
S. Derin Babacan
Masashi Sugiyama
Ichiro Takeuchi