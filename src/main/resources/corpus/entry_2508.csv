2018,Hunting for Discriminatory Proxies in Linear Regression Models,A machine learning model may exhibit discrimination when used to make decisions involving people. One potential cause for such outcomes is that the model uses a statistical proxy for a protected demographic attribute. In this paper we formulate a definition of proxy use for the setting of linear regression and present algorithms for detecting proxies. Our definition follows recent work on proxies in classification models  and characterizes a model's constituent behavior that: 1) correlates closely with a protected random variable  and 2) is causally influential in the overall behavior of the model. We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program  and further extend this result to account for situations where the use of a certain input variable is justified as a ``business necessity''. Finally  we present empirical results on two law enforcement datasets that exhibit varying degrees of racial disparity in prediction outcomes  demonstrating that proxies shed useful light on the causes of discriminatory behavior in models.,Hunting for Discriminatory Proxies

in Linear Regression Models

Samuel Yeom

Carnegie Mellon University

Anupam Datta

Carnegie Mellon University

syeom@cs.cmu.edu

danupam@cmu.edu

Matt Fredrikson

Carnegie Mellon University
mfredrik@cs.cmu.edu

Abstract

A machine learning model may exhibit discrimination when used to make
decisions involving people. One potential cause for such outcomes is that the
model uses a statistical proxy for a protected demographic attribute. In this paper
we formulate a deﬁnition of proxy use for the setting of linear regression and
present algorithms for detecting proxies. Our deﬁnition follows recent work on
proxies in classiﬁcation models  and characterizes a model’s constituent behavior
that: 1) correlates closely with a protected random variable  and 2) is causally
inﬂuential in the overall behavior of the model. We show that proxies in linear
regression models can be efﬁciently identiﬁed by solving a second-order cone
program  and further extend this result to account for situations where the use of
a certain input variable is justiﬁed as a “business necessity”. Finally  we present
empirical results on two law enforcement datasets that exhibit varying degrees
of racial disparity in prediction outcomes  demonstrating that proxies shed useful
light on the causes of discriminatory behavior in models.

1

Introduction

The use of machine learning in domains like insurance [23]  criminal justice [18]  and child wel-
fare [28] raises concerns about fairness  as decisions based on model predictions may discriminate
on the basis of demographic attributes like race and gender. These concerns are driven by high-
proﬁle examples of models that appear to have discriminatory effect  ranging from gender bias in
job advertisements [10] to racial bias in same-day delivery services [21] and predictive policing [3].
Meanwhile 
laws and regulations in various jurisdictions prohibit certain practices that have
discriminatory effect  regardless of whether the discrimination is intentional. For example  the
U.S. has recognized the doctrine of disparate impact since 1971  when the Supreme Court held in
Griggs v. Duke Power Co. [25] that the Duke Power Company had discriminated against its black
employees by requiring a high-school diploma for promotion when the diploma had little to do
with competence in the new job. These regulations pose a challenge for machine learning models 
which may give discriminatory predictions as an unintentional side effect of misconﬁguration or
biased training data. Many competing deﬁnitions of disparate impact [3  14] have been proposed in
efforts to address this challenge  but it has been shown that some of these deﬁnitions are impossible
to satisfy simultaneously [8]. Therefore  it is important to ﬁnd a workable standard for detecting
discriminatory behavior in models.
Much prior work [19  30] has focused on the four-ﬁfths rule [17] or variants thereof  which are
relaxed versions of the demographic parity requirement that different demographic groups should
receive identical outcomes on average. However  demographic parity does not necessarily make
a model fair. For example  consider an attempt to “repair” a racially discriminatory predictive
policing model by arbitrarily lowering the risk scores of some members of the disadvantaged race
until demographic parity is reached. The resulting model is still unfair to individual members of

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

the disadvantaged race that did not have their scores adjusted. In fact  this is why the U.S. Supreme
Court ruled that demographic parity is not a complete defense to claims of disparate impact [26].
In addition  simply enforcing demographic parity without regard for possible justiﬁcations for
disparate impact may be prohibited on the grounds of intentional discrimination [27].
Recent work on proxy use [11] addresses these issues by considering the causal factors behind
discriminatory behavior. A proxy for a protected attribute is deﬁned as a portion of the model that
is both causally inﬂuential [13] on the model’s output and statistically associated with the protected
variable. This means that  in the repair example above  the original discriminatory model is a proxy
for a protected demographic attribute  indicating the presence of discriminatory behavior in the
“repaired” model. However  prior treatment of proxy use has been limited to classiﬁcation models 
so regression models remain out of reach of these techniques.
In this paper  we deﬁne a notion of proxy use (Section 2) for linear regression models  and show how
it can be used to inform considerations of fairness and discrimination. While the previous notion of
proxy use is prohibitively expensive to apply at scale to real-world models [11]  our deﬁnition admits
a convex optimization procedure that leads to an efﬁcient detection algorithm (Section 3). Because
disparate impact is not always forbidden  we extend our deﬁnition to account for an exempt input
variable whose use for a particular problem is justiﬁed. We show that slight modiﬁcations to our
detection algorithm allow us to effectively “ignore” proxies based on the exempt variable (Section 4).
Finally  in Section 5 we evaluate our algorithm with two real-world predictive policing applications.
We ﬁnd that the algorithm  despite taking little time to run  accurately identiﬁes parts of the model
that are the most problematic in terms of disparate impact. Moreover  in one of the datasets  the
strongest nonexempt proxy is signiﬁcantly weaker than the strongest general proxy  suggesting
that proxy use can sometimes be attributed to a single input variable. In other words  the proxies
identiﬁed by our approach effectively explain the cause of discriminatory model predictions 
informing the consideration of whether the disparate impact is justiﬁed.
Proofs of all theorems are given in the extended version of this paper [29].

1.1 Related Work

We refer the reader to [6] for a detailed discussion of discrimination in machine learning from a
legal perspective. One legal development of note is the adoption of the four-ﬁfths rule by the U.S.
Equal Employment Opportunities Commission in 1978 [17]. The four-ﬁfths “rule” is a guideline
that compares the rates of favorable outcomes among different demographic groups  requiring that
the ratio of these rates be no less than four-ﬁfths. This guideline motivated the work of Feldman et
al. [19]  who guarantee that no classiﬁer will violate the four-ﬁfths rule by removing the association
between the input variables and the protected attribute. Zafar et al. [30] use convex optimization to
ﬁnd linear models that are both accurate and fair  but their fairness deﬁnition  unlike ours  is derived
from the four-ﬁfths rule. We show in Section 2.5 that proxy use is a stronger notion of fairness than
demographic parity  of which the four-ﬁfths rule is a relaxation.
Other notions of fairness have been proposed as well. Dwork et al. [16] argue that demographic
parity is insufﬁcient as a fairness constraint  and instead deﬁne individual fairness  which requires
that similar individuals have similar outcomes. While individual fairness is important  it is not
well-suited for characterizing disparate impact  which inherently involves comparing different de-
mographic groups to each other. Hardt et al. [20] propose a notion of group fairness called equalized
odds. Notably  equalized odds does not require demographic parity  i.e.  groups can have unequal
outcomes as long as the response variable is also unequally distributed. For example  in the context
of predictive policing  it would be acceptable to categorize members of a certain racial group as a
higher risk on average  provided that they are in fact more likely to reoffend. This is consistent with
the current legal standard  wherein disparate impact can be justiﬁed if there is an acceptable reason.
However  some have observed that the response variable could be tainted by past discrimination [6 
Section I.B.1]  in which case equalized odds may end up perpetuating the discrimination.
Our treatment of exempt input variables is similar to that of resolving variables by Kilbertus et
al. [22] in their work on causal analysis of proxy use and discrimination. A key difference is that
they assume a causal model and only consider causal relationships between the protected attribute
and the output of the model  whereas we view any association with the protected attribute as suspect.
Our notion of proxy use extends that of Datta et al. [11  12]  who take into consideration both

2

association and inﬂuence. An alternative measure of proxy strength has been proposed by Adler et
al. [1]  who deﬁne a single real-valued metric called indirect inﬂuence. As we show in the rest of this
paper  the two-metric-based approach of Datta et al. leads to an efﬁcient proxy detection algorithm.

2 Proxy Use

In this section we present a deﬁnition of proxy use that is suited to linear regression models. We
ﬁrst review the original deﬁnition of Datta et al. [11] for classiﬁcation models and then show how
to modify this deﬁnition to get one that is applicable to the setting of linear regression.

2.1 Setting

We work in the standard machine learning setting  where a model is given several inputs that
correspond to a data point. Throughout this paper  we will use X = (X1  . . .   Xn) to denote
these inputs  where X1  . . .   Xn are random variables. We consider a linear regression model
ˆY = β1X1 + ··· + βnXn  where βi represents the coefﬁcient for the input variable Xi. We will
abuse notation by using ˆY to represent either the model or its output.
In the case where each data point represents a person  care must be taken to avoid disparate impact
on the basis of a protected demographic attribute  such as race or gender. We will denote such
protected attribute by the random variable Z. In practice  Z is usually binary (i.e.  Z ∈ {0  1})  but
our results are general and apply to arbitrary numerical random variables.

2.2 Proxy Use in Prior Work

Datta et al. [11] deﬁne proxy use of a random variable Z as the presence of an intermediate
computation in a program that is both statistically associated with Z and causally inﬂuential on
the ﬁnal output of the program. Instantiating this deﬁnition to a particular setting therefore entails
specifying an appropriate notion of “intermediate computation”  a statistical association measure 
and a causal inﬂuence measure.
Datta et al. identify intermediate computations in terms of syntactic decompositions into subpro-
(cid:48)(X   P (X )). Then the association between P and Z is given
grams P   ˆY
by an appropriate measure such as mutual information  and the inﬂuence of P on ˆY is deﬁned as
shown in Equation 1  where X and X (cid:48) are drawn independently from the population distribution.

(cid:48) such that ˆY (X ) ≡ ˆY

Inﬂ ˆY (P ) = PrX  X (cid:48)[ ˆY (X ) (cid:54)= ˆY

(cid:48)

(X   P (X (cid:48)

))] 

(1)

Intuitively  inﬂuence is characterized by the likelihood that an independent change in the value of
P will cause a change in ˆY . This makes sense for classiﬁcation models because a change in the
model’s output corresponds to a change in the predicted class of a point  as reﬂected by the use of
0-1 loss in that setting. On the other hand  regression models have real-valued outputs  so the square
loss is more appropriate for these models. Therefore  we are motivated to transform Equation 1 
which is simply the expected 0-1 loss between ˆY (X ) and ˆY
(cid:48)(X   P (X (cid:48)))  into Equation 2  which is
the expected square loss between these two quantities.

X  X (cid:48)[( ˆY (X ) − ˆY
E

(cid:48)

(X   P (X (cid:48)

)))2]

(2)

Before we can reason about the suitability of this measure  we must ﬁrst deﬁne an appropriate
notion of intermediate computation for linear models.

2.3 Linear components

The notion of subprogram used for discrete models [11] is not well-suited to linear regression. To
see why  consider the model ˆY = β1X1 + β2X2 + β3X3. Suppose that this is computed using
the grouping (β1X1 + β2X2) + β3X3 and that the deﬁnition of subprogram honors this ordering.
Then  β1X1 + β2X2 would be a subprogram  but β1X1 + β3X3 would not be even though ˆY could
have been computed equivalently as (β1X1 + β3X3) + β2X2. We might attempt to address this
by allowing any subset of the terms used in the model to deﬁne a subprogram  thus capturing the
commutativity and associativity of addition. However  this deﬁnition still excludes expressions such

3

as β1X1 + 0.5β3X3  which may be a stronger proxy than either β1X1 or β1X1 + β3X3. To include
such expressions  we present Deﬁnition 1 as the notion of subprogram that we use to deﬁne proxy
use in the setting of linear regression.
Deﬁnition 1 (Component). Let ˆY = β1X1 + ··· + βnXn be a linear regression model. A
random variable P is a component of ˆY if and only if there exist α1  . . .   αn ∈ [0  1] such that
P = α1β1X1 + ··· + αnβnXn.

2.4 Linear association and inﬂuence

Having deﬁned a component as the equivalent of a subprogram in a linear regression model  we
now formalize the association and inﬂuence conditions given by Datta et al. [11].

Association. A linear model only uses linear relationships between variables  so our association
measure only captures linear relationships. In particular  we use the Pearson correlation coefﬁcient 
and we square it so that a higher association measure always represents a stronger proxy.
Deﬁnition 2 (Association). The association of two nonconstant random variables P and Z is
deﬁned as Asc(P  Z) = Cov(P Z)2
Note that Asc(P  Z) ∈ [0  1]  with 0 representing no linear correlation and 1 representing a fully
linear relationship.

Var(P )Var(Z) .

(cid:48)(X   P (X (cid:48))) = (cid:80)n

Substituting these into Equation 2 gives
)))2] = E

inition 1 gives us ˆY (X ) = (cid:80)n
Inﬂuence. To formalize inﬂuence  we continue from where we left off with Equation 2. Def-
i=1(1 − αi)βiXi + αiβiX(cid:48)
i.
i)2] = Var(P (X ) − P (X (cid:48))) 
X  X (cid:48)[( ˆY (X ) − ˆY
E
which is proportional to Var(P (X )) since X and X (cid:48) are i.i.d. Deﬁnition 3 captures this reasoning 
normalizing the variance so that Inﬂ ˆY (P ) = 1 when P = ˆY (i.e.  α1 = ··· = αn = 1). In the
extended version of this paper [29]  we also show that variance is the unique inﬂuence measure
(up to a constant factor) satisfying some natural axioms that we call nonnegativity  nonconstant
positivity  and zero-covariance additivity.
Deﬁnition 3 (Inﬂuence). Let P be a component of a linear regression model ˆY . The inﬂuence of P
is deﬁned as Inﬂ ˆY (P ) = Var(P )
Var( ˆY )

i=1 αiβiXi − αiβiX(cid:48)

.

(cid:48)

(X   P (X (cid:48)

i=1 βiXi and ˆY

X  X (cid:48)[((cid:80)n

When it is obvious from the context  the subscript ˆY may be omitted. Note that inﬂuence can
exceed 1 because the inputs to a model can cancel each other out  leaving the ﬁnal model less
variable than some of its components.
Finally  the deﬁnition of proxy use for linear models is given in Deﬁnition 4.
Deﬁnition 4 ((  δ)-Proxy Use). Let   δ ∈ (0  1]. A model ˆY = β1X1 + ··· + βnXn has
(  δ)-proxy use of Z if there exists a component P such that Asc(P  Z) ≥  and Inﬂ ˆY (P ) ≥ δ.

2.5 Connection to Demographic Parity

We now discuss the relationship between proxy use and demographic parity  and argue that proxy
use is a stronger deﬁnition that provides more useful information than demographic parity. For
binary classiﬁcation models with two demographic groups  demographic parity is deﬁned by the
equation Pr[ ˆY = 1|Z = 0] = Pr[ ˆY = 1|Z = 1]  i.e.  two demographic groups must have the same
rates of favorable outcomes. We adapt this notion to regression models by replacing the constraint
on the positive classiﬁcation outcome with the expectation of the response  as shown in Deﬁnition 5.
Deﬁnition 5 (Demographic Parity  Regression). Let ˆY be a regression model  and let Z be a binary
random variable. ˆY satisﬁes demographic parity if E[ ˆY |Z = 0] = E[ ˆY |Z = 1].
Equation 3 shows that our association measure is related to demographic parity in regression models.

Asc( ˆY   Z) =

Cov( ˆY   Z)2

Var( ˆY )Var(Z)

= (E[ ˆY |Z = 0] − E[ ˆY |Z = 1])2 · Var(Z)
Var( ˆY )

 

(3)

4

(a) z is a vector representation of the protected
attribute Z  and components of the model can
If a component
also be represented as vectors.
is inside the red double cone 
it exceeds the
association threshold   where the angle θ is set
such that  = cos2 θ. The cone on the right side
corresponds to positive correlation with Z  and
the left cone negative correlation. Components
in the blue shaded area exceed some inﬂuence
threshold δ. If any component exceeds both the
association and the inﬂuence thresholds  it is a
proxy and may be disallowed.

(b) x1 and x2 are vector representations of X1
and X2  which are inputs to the model ˆY =
β1X1 +β2X2. The gray shaded area indicates the
space of all possible components of the model.
β1X1 is a component  but it is not a proxy because
it does not have strong enough association with
Z. Although β2X2 is strongly associated with Z 
it is not inﬂuential enough to be a proxy. On the
other hand  0.7β1X1 + β2X2 is a component that
exceeds both the association and the inﬂuence
thresholds  so it is a proxy and may be disallowed.

Figure 1: Illustration of proxy use with the vector interpretation of random variables. In the above
examples  all vectors lie in R2 for ease of depiction. In general  the vectors z  x1  . . .   xn can span
Rn+1.

In particular  if ˆY does not satisfy demographic parity  then Asc( ˆY   Z) > 0  so ˆY is an (  1)-proxy
for some  > 0. This means that our proxy use framework is broad enough to detect any violation of
demographic parity. On the other hand  the “repair” example in Section 1 shows that demographic
parity does not preclude the presence of proxies. Therefore  proxy use is a strictly stronger notion
of fairness than demographic parity.
Moreover  instances of proxy use can inform the discussion about a model that exhibits demographic
disparity. When a proxy is identiﬁed  it may explain the cause of the disparity and can help decide
whether the behavior is justiﬁed based on the set of variables used by the proxy. We elaborate on
this idea in Section 4  designating a certain input variable as always permissible to use.

3 Finding Proxy Use

In this section  we present our proxy detection algorithms  which take advantage of properties
speciﬁc to linear regression to quickly identify components of interest. We prove that we can use
an exact optimization problem (Problem 1) to either identify a proxy if one exists  or deﬁnitively
conclude that there is no proxy. However  because this problem is not convex and in some cases may
be intractable  we also present an approximate version of the problem (Problem 2) that sacriﬁces
some precision. The approximate algorithm can still be used to conclude that a model does not
have any proxies  but it may return false positives. In Section 5  we evaluate how these algorithms
perform on real-world data.
Because the only operations that we perform on random variables are addition and scalar multipli-
cation  we can safely treat the random variables as vectors in a vector space. In addition  covariance
is an inner product in this vector space. As a result  it is helpful to think of random variables
Z  X1  . . .   Xn as vectors z  x1  . . .   xn ∈ Rn+1  with covariance as dot product. Under this
interpretation  inﬂuence is characterized by Inﬂ ˆY (P ) ∝ Var(P ) = Cov(P  P ) = p · p = (cid:107)p(cid:107)2 
where (cid:107)·(cid:107) denotes the (cid:96)2-norm  and association is shown in Equation 4  where θ is the angle

5

zθβ1x1β2x20.7β1x1+β2x2Problem 1 Exact optimization
min −(cid:107)A
s.t. 0 (cid:22) α (cid:22) 1 and (cid:107)A

(cid:48)α(cid:107)2

(cid:48)α(cid:107) ≤ s · zT A(cid:48)α√
(cid:107)z(cid:107)

Problem 2 Approximate optimization
min −cT α
s.t. 0 (cid:22) α (cid:22) 1 and (cid:107)A

(cid:48)α(cid:107) ≤ s · zT A(cid:48)α√
(cid:107)z(cid:107)

Figure 2: Optimization problems used to ﬁnd proxies in linear regression models. A(cid:48) is the
(n+1) × n matrix [β1x1
. . . βnxn]  and we optimize over α  which is an n-dimensional
vector of the alpha-coefﬁcients used in Deﬁnition 1.  is the association threshold  and c is the
n-dimensional vector that satisﬁes ci = (cid:107)βixi(cid:107).

between the two vectors p and z.

Asc(P  Z) =

Cov(P  Z)2

Var(P )Var(Z)

=

(cid:19)2

(cid:18) p · z

(cid:107)p(cid:107)(cid:107)z(cid:107)

= cos2 θ 

(4)

This abstraction is illustrated in more detail in Figure 1.
To ﬁnd coordinates for the vectors  we consider the covariance matrix [Cov(Xi  Xj)]i j∈{0 ... n} 
where Z = X0 for notational convenience. If we can write this covariance matrix as AT A for
some (n+1) × (n+1) matrix A  then each entry in the covariance matrix is the dot product of two
(not necessarily distinct) columns of A. In other words  the mapping from the random variables
Z  X1  . . .   Xn to the columns of A preserves the inner product relationship. Now it remains to
decompose the covariance matrix into the form AT A. Since the covariance matrix is guaranteed to
be positive semideﬁnite  two of the possible decompositions are the Cholesky decomposition and
the matrix square root.
Our proxy detection algorithms use as subroutines the optimization problems that are formally
stated in Figure 2. We ﬁrst motivate the exact optimization problem (Problem 1) and show how the
solutions to these problems can be used to determine whether the model contains a proxy. Then  we
present the approximate optimization problem (Problem 2)  which sacriﬁces exactness for efﬁcient
solvability.
Let A(cid:48) be the (n+1) × n matrix [β1x1
. . . βnxn]. The constraint 0 (cid:22) α (cid:22) 1 restricts the
solutions to be inside the space of all components  represented by the gray shaded area in Figure 1b.
Moreover  when s ∈ {−1  1}  the constraint (cid:107)A(cid:48)α(cid:107) ≤ s · (zT A(cid:48)α)/(
(cid:107)z(cid:107)) describes one of
the red cones in Figure 1a  which together represent the association constraint. Subject to these
constraints  we maximize the inﬂuence  which is proportional to (cid:107)A(cid:48)α(cid:107)2. Theorem 1 shows that
this technique is sufﬁcient to determine whether a model contains a proxy.
Theorem 1. Let P denote the component deﬁned by the alpha-coefﬁcients α. The linear regression
model ˆY = β1X1 + ··· + βnXn contains a proxy if and only if there exists a solution to Problem 1
with s ∈ {−1  1} such that Inﬂ ˆY (P ) ≥ δ.
In essence  Theorem 1 guarantees the correctness of the following proxy detection algorithm: Run
Problem 1 with s = 1 and s = −1  and compute the association and inﬂuence of the resulting
solutions. The model contains a proxy if and only if any of the solutions passes both the association
and the inﬂuence thresholds.
It is worth mentioning that Problem 1 tests for strong positive correlation with Z when s = 1 and for
strong negative correlation when s = −1. This optimization problem resembles a second-order cone
program (SOCP) [7  Section 4.4.2]  which can be solved efﬁciently. However  the objective function
is concave  so the standard techniques for solving SOCPs do not work on this problem. To get around
this issue  we can instead solve Problem 2  which has a linear objective function whose coefﬁcients
ci = (cid:107)βixi(cid:107) were chosen so that the inequality (cid:107)A(cid:48)α(cid:107) ≤ cT α always holds. This inequality allows
us to prove Theorem 2  which mirrors the claim of Theorem 1 but only in one direction.
Theorem 2. If the linear regression model ˆY = β1X1 + ··· + βnXn contains a proxy  then there
exists a solution to Problem 2 with s ∈ {−1  1} such that cT α ≥ (δ Var( ˆY ))0.5.
Theorem 2 suggests a quick algorithm to verify that a model does not have any proxies. We solve
the SOCP described by Problem 2  once with s = 1 and once with s = −1. If neither solution

√

6

satisﬁes cT α ≥ (δ Var( ˆY ))0.5  by the contrapositive of Theorem 2  we can be sure that the model
does not contain any proxies.
However  the converse does not hold  i.e.  we cannot be sure that the model has a proxy even if
a solution to Problem 2 satisﬁes cT α ≥ (δ Var( ˆY ))0.5. This is because cT α overapproximates
(cid:107)A(cid:48)α(cid:107) by using the triangle inequality. As a result  it is possible for the inﬂuence to be below the
threshold even if the value of cT α is above the threshold. While there is in general no upper bound
on the overapproximation factor of the triangle inequality  the experiments in Section 5 show that
this factor is not too large in practice. In addition  Problem 1 often works well enough in practice
despite not being a convex optimization problem.

4 Exempt Use of a Variable

So far  we have shown how to ﬁnd a proxy in a linear regression model  but we have not discussed
which proxies should be allowed and which should not. As mentioned in Section 1  disparate impact
is legally permitted if there is sufﬁcient justiﬁcation. For example  in the context of predictive polic-
ing  it may be acceptable to consider the number of prior convictions even if one racial group tends
to have a higher number of convictions than another. We formalize this idea by assuming that the
use of one particular input variable  which we call the exempt variable  is explicitly permitted. This
assumption may be appropriate if  for example  the exempt variable is directly and causally related
to the response variable Y . Throughout this section  we will use X1 to denote the exempt variable.
First  we formally deﬁne which proxies are exempt  i.e.  permitted because the proxy use is
attributable to X1. Clearly  if the model ignores every input except X1  all proxies in the model
should be exempt. Conversely  if the coefﬁcient β1 of X1 is zero  no proxies should be exempt.
We capture this intuition by ignoring X1 and checking whether the resulting component is a proxy.
More formally  if P = α1β1X1 + ··· + αnβnXn is a component  we investigate P \ X1  which we
write as shorthand for the component α2β2X2 + ··· + αnβnXn. If P is a proxy but P \ X1 is not 
then P is exempted because the proxy use can be attributed to the exempt variable X1.
However  one possible issue with this attribution is that the other input variables can interact
with X1 to create a proxy stronger than X1. For example  suppose that Asc(X2  Z) = 0 and
P = X1 + X2 = Z. Then  even though P \ X1 = X2 is not a proxy  it makes P more strongly
associated with Z than X1 is  so it is not clear that P should be exempt on account of the fact that
we are permitted to use X1. Therefore  our deﬁnition of proxy exemption in Deﬁnition 6 adds the
requirement that P should not be too much more associated with Z than X1 is.
Deﬁnition 6 (Proxy Exemption). Let P be a proxy component of a linear regression model 
and let X1 be the exempt variable. P is an exempt proxy if P \ X1 is not a proxy and
Asc(P  Z) < Asc(X1  Z) + (cid:48)  where (cid:48) is the association tolerance parameter.
We can incorporate the exemption policy into our search algorithm with small changes to the
optimization problem. By Deﬁnition 6  a proxy P is nonexempt if either P \ X1 is a proxy or
Asc(P  Z) ≥ Asc(X1  Z) + (cid:48). For each of these two conditions  we modify the optimization prob-
lems from Section 3 to ﬁnd proxies that also satisfy the condition. If either of these modiﬁcations
return a positive result  then we have found a nonexempt proxy.
We start with the second condition  for which it is easy to see that it sufﬁces to change the
association threshold in Problem 2 from  to max(  Asc(X1  Z) + (cid:48)). For the ﬁrst condition  we
use the result from Theorem 3 and simply add the constraint that α1 = 0. If we add this constraint
to Problem 2  the resulting problem is still an SOCP and can therefore be solved efﬁciently.
Theorem 3. A linear regression model contains a proxy P such that P \ X1 is also a proxy if and
only if the model contains a proxy such that α1 = 0.

5 Experimental Results

In this section  we evaluate the performance of our algorithms on real-world predictive policing
datasets. We ran our proxy detection algorithms on observational data from Chicago’s Strategic
Subject List (SSL) model [9] and the Communities and Crimes (C&C) dataset [15]. The creator of
the SSL model claims that the model avoids variables that could lead to discrimination [4]  and if

7

Association threshold 
Actual inﬂ. (Prob. 1)
Approx. inﬂ. (Prob. 2)
Actual inﬂ. (Prob. 2)

0.01
0.8816
1.6933
0.8476

0.02
0.2263
0.6683
0.1874

0.03
0.1090
0.3820
0.0987

0.04

0.0427*
0.1432
0.0420

0.05

0.0065*
0.0270
0.0080

0.06
0.0028
0.0085
0.0027

0.07
0.0000
0.0000
0.0000

Table 1: Inﬂuence of the components obtained by solving the exact (Problem 1) and approximate
(Problem 2) optimization problems for the SSL model using Z = race and s = 1. No component
had strong enough association when s = −1 instead. Asterisks indicate that the exact optimization
problem terminated early due to a singular KKT matrix. The approximate optimization problem did
not have this issue  and the overapproximation that it makes of the components’ inﬂuence is shown
in the second row.

this is the case then we would expect to see only weak proxies if any. On the other hand  the C&C
dataset contains many variables that are correlated with race  so we would expect to ﬁnd strong
proxies in a model trained with this dataset.
To test these hypotheses  we implemented Problems 1 and 2 with the cvxopt package [2] in
Python. The experimental results conﬁrm our hypotheses and show that our algorithm runs very
quickly (< 1 second). Moreover  our algorithms pinpoint components of the model that are the
most problematic in terms of disparate impact  and we ﬁnd that the exemption policy discussed in
Section 4 removes the appropriate proxies from the SSL model.
For each dataset  we brieﬂy describe the dataset and present the experimental results  demonstrating
how the identiﬁed proxies can provide evidence of discriminatory behavior in models. Then  we
explain the implications of these results on the false positive and false negative rates in practice 
and we discuss how a practitioner can decide which values of  and δ to use.

Strategic Subject List. The SSL [9] is a model that the Chicago Police Department uses to assess
an individual’s risk of being involved in a shooting incident  either as a victim or a perpetrator. The
SSL dataset consists of 398 684 rows  each of which corresponds to a person. Each row includes the
SSL model’s eight input variables (including age  number of previous arrests for violent offenses 
and whether the person is a member of a gang)  the SSL score given by the model  and the person’s
race and gender.
We searched for proxies for race (binary black/white) and gender (binary male/female)  ﬁltering
out rows with other race or gender. After also ﬁltering out rows with missing data  we were left
with 290 085 rows. Because we did not have direct access to the SSL model  we trained a linear
regression model to predict the SSL score of a person given the same set of variables that the SSL
model uses. Our model explains approximately 80% of the variance in the SSL scores  so we
believe that it is a reasonable approximation of the true model for the purposes of this evaluation.
The strengths of the proxies for race are given in Table 1. The estimated inﬂuence was computed
as (cT α)2/Var( ˆY )  which is the result of solving for δ in the inequality given in Theorem 2.
We found that this estimate is generally about 3–4× larger than the actual inﬂuence. Although
the proxies for race were somewhat stronger than those for gender  neither type had signiﬁcant
inﬂuence (δ > 0.05) beyond small  levels (~0.03–0.04). This is consistent with our hypothesis
about the lack of discriminatory behavior in this model.
We also tested the effect of exempting the indicator variable for gang membership in the input.
Gang membership is more associated with both demographic variables than any other in among the
inputs  and is a plausible cause of involvement in violent crimes [5]  making it a prime candidate for
exemption. As contrasted with the components described in Table 1  every nonexempt component
under this policy has an association with race less than 0.033. This means that the strongest
nonexempt proxy is signiﬁcantly weaker than the strongest general proxy  suggesting that much of
the proxy use present in the model can be attributed to the gang membership variable.

Communities and Crimes. C&C [24] is a dataset in the UCI machine learning repository [15]
that combines socioeconomic data from the 1990 US census with the 1995 FBI Uniform Crime
Reporting data.
It consists of 1 994 rows  each of which corresponds to a community (e.g. 

8

municipality) in the U.S.  and 122 potential input variables. After we removed the variables that
directly measure race and the ones with missing data  we were left with 90 input variables.
We simulated a hypothetical naive attempt at predictive policing by using this dataset to train a linear
regression model that predicts the per capita rate of violent crimes in a community. We deﬁned
the protected attribute Z as the difference between the percentages of people in the community
who are black and white  respectively. We observed a strong association in the dataset between
the rate of violent crime and Z (Asc(Y  Z) = 0.48)  and the model ampliﬁes this bias even more
(Asc( ˆY   Z) = 0.65).
As expected  we found very strong proxies for race in the model trained with the C&C dataset. For
example  one proxy consisting of 58 of the 90 input variables achieves an inﬂuence of 0.34 when
 = 0.85. Notably  the input variable most strongly associated with race has an association of only
0.73  showing that in practice multiple variables combine to result in a stronger proxy than any
of the individual variables. In addition  the model contains a proxy whose association is 0.40 and
inﬂuence is 14.5. In other words  the variance of the proxy is 14.5 times greater than that of the
model; this arises because other associated variables cancel most of this variance in the full model.
As a result  exempting any one variable does not result in a signiﬁcant difference since associated
variables still yield proxies that are nearly as strong. Moreover  a cursory analysis suggested that
the variables used in these proxies are not justiﬁable correlates of race  so an exemption policy may
not sufﬁce to “explain away” the discriminatory behavior of the model.

False Positives and False Negatives. Theorem 1 shows that our exact proxy detection algorithm
detects a proxy if and only if the model in fact contains a proxy.
In other words  if Problem 1
returns optimal solutions  we can use the solutions to conclusively determine whether there exists
a proxy  and there will be no false positives or false negatives. However  our experiments show that
sometimes Problem 1 terminates early due to a singular KKT matrix  and in this case one can turn
to the approximate proxy detection algorithm.
Although Problem 2 sometimes returns solutions that are not in fact proxies  we can easily ascertain
whether any given solution is a proxy by simply computing its association and inﬂuence. However 
even if the solution returned by Problem 2 turn out to not be proxies  the model could still contain
a different proxy. Using Table 1 as reference  we see that this happens in the SSL model if  for ex-
ample   = 0.02 and δ is between 0.1874 and 0.2263. Therefore  one can consider the approximate
algorithm as giving a ﬁnding of either “potential proxy use” or “no proxy use”. Theorem 2 shows
that a ﬁnding of “no proxy use” does indeed guarantee that the model is free of proxies. In other
words  the approximate algorithm has no false negatives. However  the algorithm overapproximates
inﬂuence  so the algorithm can give a ﬁnding of “potential proxy use” when there are no proxies 
resulting in a false positive. This happens when δ is between the maximum feasible inﬂuence (ﬁrst
row in Table 1) and the maximum feasible overapproximation of inﬂuence (second row in Table 1).

Reasonable Values of  and δ. Although the appropriate values of  and δ depend on the appli-
cation  we remind the reader that association is the square of the Pearson correlation coefﬁcient.
This means that an association of 0.05 corresponds to a Pearson correlation coefﬁcient of ~0.22 
which represents not an insigniﬁcant amount of correlation. Likewise  inﬂuence is proportional to
variance  which increases quadratically with scalar coefﬁcients. Therefore  we recommend against
setting  and δ to a value much higher than 0.05. To get an idea of which values of δ are suitable for
a particular application  the practitioner can compare the proposed value of δ against the inﬂuence
of the individual input variables βiXi.

6 Conclusion and Future Work

In this paper  we have formalized the notion of proxy discrimination in linear regression models and
presented an efﬁcient proxy detection algorithm. We account for the case where the use of one vari-
able is justiﬁed  and extending this result to multiple exempt variables is valuable future work that
would enable better handling of models like C&C that take many closely related input variables. De-
veloping learning rules that account for proxy use  leading to models without proxies above speciﬁed
thresholds  is also an intriguing direction with direct potential for impact on practical scenarios.

9

Acknowledgment

The authors would like to thank the anonymous reviewers at NeurIPS 2018 for their thoughtful
feedback. This material is based upon work supported by the National Science Foundation under
Grant No. CNS-1704845.

References
[1] Philip Adler  Casey Falk  Sorelle A Friedler  Tionney Nix  Gabriel Rybeck  Carlos Scheideg-
ger  Brandon Smith  and Suresh Venkatasubramanian. Auditing black-box models for indirect
inﬂuence. Knowledge and Information Systems  54(1):95–122  2018.

[2] Martin S Andersen  Joachim Dahl  and Lieven Vandenberghe. CVXOPT: Python software for

convex optimization. http://cvxopt.org.

[3] Julia Angwin  Jeff Larson  Surya Mattu  and Lauren Kirchner. Machine bias: There’s software
used across the country to predict future criminals. and it’s biased against blacks. ProPublica 
2016.

[4] Jeff Asher and Rob Arthur. Inside the algorithm that tries to predict gun violence in Chicago.

The New York Times  2017.

[5] JC Barnes  Kevin M Beaver  and J Mitchell Miller. Estimating the effect of gang membership
on nonviolent and violent delinquency: A counterfactual analysis. Aggressive behavior 
36(6):437–451  2010.

[6] Solon Barocas and Andrew D Selbst. Big data’s disparate impact. California Law Review 

104:671–732  2016.

[7] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press 

2004.

[8] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. Big Data  5(2):153–163  2017.

[9] City of Chicago.

Strategic Subject List.

Public-Safety/Strategic-Subject-List/4aki-r3np  2017.

https://data.cityofchicago.org/

[10] Amit Datta  Michael Carl Tschantz  and Anupam Datta. Automated experiments on ad privacy

settings. Privacy Enhancing Technologies  2015(1):92–112  2015.

[11] Anupam Datta  Matt Fredrikson  Gihyuk Ko  Piotr Mardziel  and Shayak Sen.

discrimination in data-driven systems. arXiv preprint arXiv:1707.08120  2017.

Proxy

[12] Anupam Datta  Matt Fredrikson  Gihyuk Ko  Piotr Mardziel  and Shayak Sen. Use privacy in
data-driven systems: Theory and experiments with machine learnt programs. In ACM SIGSAC
Conference on Computer and Communications Security  pages 1193–1210  2017.

[13] Anupam Datta  Shayak Sen  and Yair Zick. Algorithmic transparency via quantitative input
inﬂuence: Theory and experiments with learning systems. In IEEE Symposium on Security
and Privacy  pages 598–617  2016.

[14] William Dieterich  Christina Mendoza  and Tim Brennan. COMPAS risk scales: Demon-
http://go.volarisgroup.com/rs/

strating accuracy equity and predictive parity.
430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf  2016.

[15] Dheeru Dua and Eﬁ Karra Taniskidou.

//archive.ics.uci.edu/ml  2017.

UCI machine learning repository.

https:

[16] Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard Zemel. Fairness

through awareness. In Innovations in Theoretical Computer Science  pages 214–226  2012.

[17] Equal Employment Opportunities Commission. Uniform guidelines on employee selection

procedures. 29 CFR Part 1607  1978.

10

[18] Equivant. Practitioner’s guide to COMPAS core. http://www.equivant.com/assets/

img/content/Practitioners_Guide_COMPASCore_121917.pdf  2017.

[19] Michael Feldman  Sorelle A Friedler  John Moeller  Carlos Scheidegger  and Suresh Venkata-
In ACM SIGKDD International

subramanian. Certifying and removing disparate impact.
Conference on Knowledge Discovery and Data Mining  pages 259–268  2015.

[20] Moritz Hardt  Eric Price  and Nati Srebro. Equality of opportunity in supervised learning. In

Advances in Neural Information Processing Systems  pages 3315–3323  2016.

[21] David Ingold and Spencer Soper. Amazon doesn’t consider the race of its customers. Should

it? Bloomberg  2016.

[22] Niki Kilbertus  Mateo Rojas Carulla  Giambattista Parascandolo  Moritz Hardt  Dominik
In

Janzing  and Bernhard Sch¨olkopf. Avoiding discrimination through causal reasoning.
Advances in Neural Information Processing Systems  pages 656–666  2017.

[23] Bernard Marr. How AI and machine learning are used to transform the insurance industry.

Forbes  2017.

[24] Michael Redmond and Alok Baveja. A data-driven software tool for enabling cooperative
information sharing among police departments. European Journal of Operational Research 
141(3):660–678  2002.

[25] Supreme Court of the United States. Griggs v. Duke Power Co. 401 U.S. 424  1971.

[26] Supreme Court of the United States. Connecticut v. Teal. 457 U.S. 440  1982.

[27] Supreme Court of the United States. Ricci v. DeStefano. 557 U.S. 557  2009.

[28] Rhema Vaithianathan  Emily Putnam-Hornstein  Nan Jiang 

and
Developing predictive models to support child maltreatment hot-
implementation.

Tim Maloney.
line
screening
https://www.alleghenycountyanalytics.us/wp-content/uploads/2018/02/
DevelopingPredictiveRiskModels-package_011618.pdf  2017.

Parma Nand 

decisions:

Allegheny County methodology

and

[29] Samuel Yeom  Anupam Datta  and Matt Fredrikson. Hunting for discriminatory proxies in

linear regression models. arXiv preprint arXiv:1810.07155  2018.

[30] Muhammad Bilal Zafar  Isabel Valera  Manuel Gomez Rogriguez  and Krishna P Gummadi.
In Artiﬁcial Intelligence and

Fairness constraints: Mechanisms for fair classiﬁcation.
Statistics  pages 962–970  2017.

11

,Samuel Yeom
Anupam Datta
Matt Fredrikson