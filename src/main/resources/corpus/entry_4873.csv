2014,Scalable Kernel Methods via Doubly Stochastic Gradients,The general perception is that kernel methods are not scalable  so neural nets become the choice for large-scale nonlinear learning problems. Have we tried hard enough for kernel methods? In this paper  we propose an approach that scales up kernel methods using a novel concept called ``doubly stochastic functional gradients''. Based on the fact that many kernel methods can be expressed as convex optimization problems  our approach solves the optimization problems by making two unbiased stochastic approximations to the functional gradient---one using random training points and another using random features associated with the kernel---and performing descent steps with this noisy functional gradient. Our algorithm is simple  need no commit to a preset number of random features  and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting. We demonstrate that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel Hilbert space in rate O(1/t)  and achieves a generalization bound of O(1/\sqrt{t}). Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show competitive performances of our approach as compared to neural nets in datasets such as 2.3 million energy materials from MolecularSpace  8 million handwritten digits from MNIST  and 1 million photos from ImageNet using convolution features.,Scalable Kernel Methods via Doubly Stochastic Gradients

Bo Dai1  Bo Xie1  Niao He1  Yingyu Liang2  Anant Raj1  Maria-Florina Balcan3  Le Song1

{bodai  bxie33  nhe6  araj34}@gatech.edu  lsong@cc.gatech.edu

1Georgia Institute of Technology

2Princeton University

yingyul@cs.princeton.edu

3Carnegie Mellon University

ninamf@cs.cmu.edu

Abstract

The general perception is that kernel methods are not scalable  so neural nets be-
come the choice for large-scale nonlinear learning problems. Have we tried hard
enough for kernel methods? In this paper  we propose an approach that scales up
kernel methods using a novel concept called “doubly stochastic functional gradi-
ents”. Based on the fact that many kernel methods can be expressed as convex
optimization problems  our approach solves the optimization problems by mak-
ing two unbiased stochastic approximations to the functional gradient—one using
random training points and another using random features associated with the
kernel—and performing descent steps with this noisy functional gradient. Our
algorithm is simple  need no commit to a preset number of random features  and
allows the ﬂexibility of the function class to grow as we see more incoming data in
the streaming setting. We demonstrate that a function learned by this procedure af-
√
ter t iterations converges to the optimal function in the reproducing kernel Hilbert
space in rate O(1/t)  and achieves a generalization bound of O(1/
t). Our ap-
proach can readily scale kernel methods up to the regimes which are dominated by
neural nets. We show competitive performances of our approach as compared to
neural nets in datasets such as 2.3 million energy materials from MolecularSpace 
8 million handwritten digits from MNIST  and 1 million photos from ImageNet
using convolution features.

Introduction

1
The general perception is that kernel methods are not scalable. When it comes to large-scale non-
linear learning problems  the methods of choice so far are neural nets although theoretical under-
standing remains incomplete. Are kernel methods really not scalable? Or is it simply because we
have not tried hard enough  while neural nets have exploited sophisticated design of feature architec-
tures  virtual example generation for dealing with invariance  stochastic gradient descent for efﬁcient
training  and GPUs for further speedup?
A bottleneck in scaling up kernel methods comes from the storage and computation cost of the
dense kernel matrix  K. Storing the matrix requires O(n2) space  and computing it takes O(n2d)
operations  where n is the number of data points and d is the dimension. There have been many great
attempts to scale up kernel methods  including efforts in perspectives of numerical linear algebra 
functional analysis  and numerical optimization.
A common numerical linear algebra approach is to approximate the kernel matrix using low-rank
factorizations  K ≈ A(cid:62)A  with A ∈ Rr×n and rank r (cid:54) n. This low-rank approximation allows
subsequent kernel algorithms to directly operate on A  but computing the approximation requires
O(nr2 + nrd) operations. Many work followed this strategy  including Greedy basis selection
techniques [1]  Nystr¨om approximation [2] and incomplete Cholesky decomposition [3]. In prac-
tice  one observes that kernel methods with approximated kernel matrices often result in a few
percentage of losses in performance. In fact  without further assumption on the regularity of the

1

√

√

√

r + 1/

√
kernel matrix  the generalization ability after using low-rank approximation is typically of order
n) [4  5]  which implies that the rank needs to be nearly linear in the number of
O(1/
data points! Thus  in order for kernel methods to achieve the best generalization ability  low-rank
approximation based approaches immediately become impractical for big datasets because of their
O(n3 + n2d) preprocessing time and O(n2) storage.
Random feature approximation is another popular approach for scaling up kernel methods [6  7].
The method directly approximates the kernel function instead of the kernel matrix using explicit
feature maps. The advantage of this approach is that the random feature matrix for n data points
can be computed in time O(nrd) using O(nr) storage  where r is the number of random features.
Subsequent algorithms then only need to operate on an O(nr) matrix. Similar to low-rank kernel
matrix approximation approach  the generalization ability of this approach is of the order O(1/
r +
n) [8  9]  which implies that the number of random features also needs to be O(n). Another
1/
common drawback of these two approaches is that adapting the solution from a small r to a large
r(cid:48) is not easy if one wants to increase the rank of the approximated kernel matrix or the number of
random features for better generalization ability. Special procedures need to be designed to reuse
the solution obtained from a small r  which is not straightforward.
Another approach that addresses the scalability issue rises from the optimization perspective. One
general strategy is to solve the dual forms of kernel methods using the block-coordinate descen-
t (e.g.  [10  11  12]). Each iteration of this algorithm only incurs O(nrd) computation and O(nr)
storage  where r is the block size. A second strategy is to perform functional gradient descent
based on a batch of data points at each epoch (e.g.  [13  14]). Thus  the computation and storage in
each iteration required are also O(nrd) and O(nr)  respectively  where r is the batch size. These
approaches can straightforwardly adapt to a different r without restarting the optimization proce-
dure and exhibit no generalization loss since they do not approximate the kernel matrix or function.
However  a serious drawback of these approaches is that  without further approximation  all support
vectors need to be stored for testing  which can be as big as the entire training set! (e.g.  kernel ridge
regression and non-separable nonlinear classiﬁcation problems.)
In summary  there exists a delicate trade-off between computation  storage and statistics when
scaling up kernel methods. Inspired by various previous efforts  we propose a simple yet general
strategy that scales up many kernel methods using a novel concept called “doubly stochastic
functional gradients”. Our method relies on the fact that most kernel methods can be expressed
as convex optimization problems over functions in the reproducing kernel Hilbert spaces (RKHS)
and solved via functional gradient descent. Our algorithm proceeds by making two unbiased
stochastic approximations to the functional gradient  one using random training points and another
using random functions associated with the kernel  and then descending using this noisy functional
gradient. The key intuitions behind our algorithm originate from (i) the property of stochastic
gradient descent algorithm that as long as the stochastic gradient is unbiased  the convergence of
the algorithm is guaranteed [15]; and (ii) the property of pseudo-random number generators that the
random samples can in fact be completely determined by an initial value (a seed). We exploit these
properties and enable kernel methods to achieve better balances between computation  storage 
and statistics. Our method interestingly integrates kernel methods  functional analysis  stochastic
optimization  and algorithmic tricks  and it possesses a number of desiderata:
Generality and simplicity. Our approach applies to many kernel methods such as kernel version of
ridge regression  support vector machines  logistic regression and two-sample test as well as many
different types of kernels such as shift-invariant  polynomial  and general inner product kernels.
The algorithm can be summarized in just a few lines of code (Algorithm 1 and 2). For a dif-
ferent problem and kernel  we just need to replace the loss function and the random feature generator.
Flexibility. While previous approaches based on random features typically require a preﬁx number
of features  our approach allows the number of random features  and hence the ﬂexibility of
the function class to grow with the number of data points. Therefore  unlike previous random
feature approach  our approach applies to the data streaming setting and achieves full potentials of
nonparametric methods.
Efﬁcient computation. The key computation of our method comes from evaluating the doubly
stochastic functional gradient  which involves the generation of the random features given speciﬁc
seeds and also the evaluation of these features on a small batch of data points. At iteration t  the
computational complexity is O(td).

2

Small memory. While most approaches require saving all the support vectors  the algorithm
allows us to avoid keeping the support vectors since it only requires a small program to regenerate
the random features and sample historical features according to some speciﬁc random seeds. At
iteration t  the memory needed is O(t)  independent of the dimension of the data.
Theoretical guarantees. We provide novel and nontrivial analysis involving Hilbert space
martingales and a newly proved recurrence relation  and demonstrate that the estimator produced
by our algorithm  which might be outside of the RKHS  converges to the optimal RKHS function.
√
More speciﬁcally  both in expectation and with high probability  our algorithm estimates the optimal
function in the RKHS in the rate of O(1/t) and achieves a generalization bound of O(1/
t) 
which are indeed optimal [15]. The variance of the random features introduced in our second
approximation to the functional gradient  only contributes additively to the constant in the conver-
gence rate. These results are the ﬁrst of the kind in literature  which could be of independent interest.
Strong empirical performance. Our algorithm can readily scale kernel methods up to the regimes
which are previously dominated by neural nets. We show that our method compares favorably to
other scalable kernel methods in medium scale datasets  and to neural nets in big datasets with
millions of data.
In the remainder  we will ﬁrst introduce preliminaries on kernel methods and functional gradients.
We will then describe our algorithm and provide both theoretical and empirical supports.

2 Duality between Kernels and Random Processes
Kernel methods owe their name to the use of kernel functions  k(x  x(cid:48)) : X × X (cid:55)→ R  which are
symmetric positive deﬁnite (PD)  meaning that for all n > 1  and x1  . . .   xn ∈ X   and c1  . . .   cn ∈
i j=1 cicjk(xi  xj) (cid:62) 0. There is an intriguing duality between kernels and stochastic

processes which will play a crucial role in our algorithm design later. More speciﬁcally 
Theorem 1 (e.g.  Devinatz [16]; Hein & Bousquet [17]) If k(x  x(cid:48)) is a PD kernel  then there
exists a set Ω  a measure P on Ω  and random function φω(x) : X (cid:55)→ R from L2(Ω  P)  such that

R  we have(cid:80)n
k(x  x(cid:48)) =(cid:82)

Ω φω(x) φω(x(cid:48)) dP(ω).

√

2 cos(ω(cid:62)x + b).

Essentially  the above integral representation relates the kernel function to a random process ω with
measure P(ω). Note that the integral representation may not be unique. For instance  the random
process can be a Gaussian process on X with the sample function φω(x)  and k(x  x(cid:48)) is simply
the covariance function between two point x and x(cid:48).
If the kernel is also continuous and shift
invariant  i.e.  k(x  x(cid:48)) = k(x − x(cid:48)) for x ∈ Rd  then the integral representation specializes into a
form characterized by inverse Fourier transformation (e.g.  [18  Theorem 6.6]) 
Theorem 2 (Bochner) A continuous  real-valued  symmetric and shift-invariant function k(x− x(cid:48))
on Rd is a PD kernel if and only if there is a ﬁnite non-negative measure P(ω) on Rd  such that
Rd×[0 2π] 2 cos(ω(cid:62)x + b) cos(ω(cid:62)x(cid:48) + b) d (P(ω) × P(b))  

Rd eiω(cid:62)(x−x(cid:48)) dP(ω) =(cid:82)

k(x − x(cid:48)) =(cid:82)

where P(b) is a uniform distribution on [0  2π]  and φω(x) =
For Gaussian RBF kernel  k(x − x(cid:48)) = exp(−(cid:107)x − x(cid:48)(cid:107)2/2σ2)  this yields a Gaussian distribution
P(ω) with density proportional to exp(−σ2(cid:107)ω(cid:107)2/2); for the Laplace kernel  this yields a Cauchy
distribution; and for the Martern kernel  this yields the convolutions of the unit ball [19]. Similar
representations where the explicit form of φω(x) and P(ω) are known can also be derived for rotation
invariant kernel  k(x  x(cid:48)) = k((cid:104)x  x(cid:48)(cid:105))  using Fourier transformation on sphere [19]. For polynomial
kernels  k(x  x(cid:48)) = ((cid:104)x  x(cid:48)(cid:105) + c)p  a random tensor sketching approach can also be used [20].
Instead of ﬁnding the random processes P(ω) and functions φω(x) given kernels  one can go the
reverse direction and construct kernels from random processes and functions (e.g.  Wendland [18]).
Ω φω(x)φω(x(cid:48)) dP(ω) for a nonnegative measure P(ω) on Ω and
φω(x) : X (cid:55)→ R from L2(Ω  P)  then k(x  x(cid:48)) is a PD kernel.
For instance  φω(x) := cos(ω(cid:62)ψθ(x) + b)  where ψθ(x) can be a random convolution of the input x
parametrized by θ. Another important concept is the reproducing kernel Hilbert space (RKHS). An
RKHS H on X is a Hilbert space of functions from X to R. H is an RKHS if and only if there exists
a k(x  x(cid:48)) : X × X (cid:55)→ R such that ∀x ∈ X   k(x ·) ∈ H  and ∀f ∈ H (cid:104)f (·)  k(x ·)(cid:105)H = f (x).
If such a k(x  x(cid:48)) exists  it is unique and it is a PD kernel. A function f ∈ H if and only if
(cid:107)f(cid:107)2H := (cid:104)f  f(cid:105)H < ∞  and its L2 norm is dominated by RKHS norm  (cid:107)f(cid:107)L2

Theorem 3 If k(x  x(cid:48)) = (cid:82)

(cid:54) (cid:107)f(cid:107)H .

3

3 Doubly Stochastic Functional Gradients
Many kernel methods can be written as convex optimization problems over functions in the RKHS
and solved using the functional gradient methods [13  14]. Inspired by these previous work  we will
introduce a novel concept called “doubly stochastic functional gradients” to address the scalability
issue. Let l(u  y) be a scalar loss function convex of u ∈ R. Let the subgradient of l(u  y) with
respect to u be l(cid:48)(u  y). Given a PD kernel k(x  x(cid:48)) and the associated RKHS H  many kernel
methods try to ﬁnd a function f∗ ∈ H which solves the optimization problem

argmin

f∈H

R(f ) := E(x y)[l(f (x)  y)] +

ν
2

(cid:107)f(cid:107)2H ⇐⇒ argmin
(cid:107)f(cid:107)H(cid:54)B(ν)

E(x y)[l(f (x)  y)]

(1)

where ν > 0 is a regularization parameter  B(ν) is a non-increasing function of ν  and the data
(x  y) follow a distribution P(x  y). The functional gradient ∇R(f ) is deﬁned as the linear term in
the change of the objective after we perturb f by  in the direction of g  i.e. 
R(f + g) = R(f ) + (cid:104)∇R(f )  g(cid:105)H + O(2).

(2)
For instance  applying the above deﬁnition  we have ∇f (x) = ∇(cid:104)f  k(x ·)(cid:105)H = k(x ·)  and
∇(cid:107)f(cid:107)2H = ∇(cid:104)f  f(cid:105)H = 2f.
Stochastic functional gradient. Given a data point (x  y) ∼ P(x  y) and f ∈ H  the stochastic
functional gradient of E(x y)[l(f (x)  y)] with respect to f ∈ H is
ξ(·) := l(cid:48)(f (x)  y)k(x ·) 

(3)
which is essentially a single data point approximation to the true functional gradient. Furthermore 
for any g ∈ H  we have (cid:104)ξ(·)  g(cid:105)H = l(cid:48)(f (x)  y)g(x). Inspired by the duality between kernel func-
tions and random processes  we can make an additional approximation to the stochastic functional
gradient using a random function φω(x) sampled according to P(ω). More speciﬁcally 
Doubly stochastic functional gradient. Let ω ∼ P(ω)  then the doubly stochastic gradient of
E(x y)[l(f (x)  y)] with respect to f ∈ H is

ζ(·) := l(cid:48)(f (x)  y)φω(x)φω(·).

and ∇R(f ) = E(x y)Eω [ζ(·)] + vf (·).

(4)
Note that the stochastic functional gradient ξ(·) is in RKHS H but ζ(·) may be outside H  since
φω(·) may be outside the RKHS. For instance  for the Gaussian RBF kernel  the random function
φω(x) =
However  these functional gradients are related by ξ(·) = Eω [ζ(·)]  which lead to unbiased estima-
tors of the original functional gradient  i.e. 
∇R(f ) = E(x y) [ξ(·)] + vf (·) 

2 cos(ω(cid:62)x + b) is outside the RKHS associated with the kernel function.

(5)
We emphasize that the source of randomness associated with the random function is not present
in the data  but artiﬁcially introduced by us. This is crucial for the development of our scalable
algorithm in the next section. Meanwhile  it also creates additional challenges in the analysis of the
algorithm which we will deal with carefully.
4 Doubly Stochastic Kernel Machines
Algorithm 1: {αi}t
Require: P(ω)  φω(x)  l(f (x)  y)  ν.
1: for i = 1  . . .   t do
2:
3:
4:
5:
6:
7: end for

Algorithm 2: f (x) = Predict(x  {αi}t
Require: P(ω)  φω(x).
1: Set f (x) = 0.
2: for i = 1  . . .   t do
3:
4:
5: end for

Sample (xi  yi) ∼ P(x  y).
Sample ωi ∼ P(ω) with seed i.
f (xi) = Predict(xi {αj}i−1
j=1).
αi = −γil(cid:48)(f (xi)  yi)φωi(xi).
αj = (1 − γiν)αj for j = 1  . . .   i − 1.

Sample ωi ∼ P(ω) with seed i.
f (x) = f (x) + αiφωi (x).

√

i=1 = Train(P(x  y))

i=1)

The ﬁrst key intuition behind our algorithm originates from the property of stochastic gradient de-
scent algorithm that as long as the stochastic gradient is bounded and unbiased  the convergence of
the algorithm is guaranteed [15]. In our algorithm  we will exploit this property and introduce two
sources of randomness  one from data and another artiﬁcial  to scale up kernel methods.

4

The second key intuition behind our algorithm is that the random functions used in the doubly
stochastic functional gradients will be sampled according to pseudo-random number generators 
where the sequences of apparently random samples can in fact be completely determined by an
initial value (a seed). Although these random samples are not the “true” random sample in the
purest sense of the word  they sufﬁce for our task in practice.
To be more speciﬁc  our algorithm proceeds by making two stochastic approximation to the function-
al gradient in each iteration  and then descending using this noisy functional gradient. The overall
algorithms for training and prediction are summarized in Algorithm 1 and 2. The training algo-
rithm essentially just performs samplings of random functions and evaluations of doubly stochastic
gradients and maintains a collection of real numbers {αi}  which is computationally efﬁcient and
memory friendly. A crucial step in the algorithm is to sample the random functions with “seed i”.
The seeds have to be aligned between training and prediction  and with the corresponding αi ob-
tained from each iteration. The learning rate γt in the algorithm needs to be chosen as O(1/t)  as
shown by our later analysis to achieve the best rate of convergence. For now  we assume that we
have access to the data generating distribution P(x  y). This can be modiﬁed to sample uniformly
randomly from a ﬁxed dataset  without affecting the algorithm and the later convergence analysis.
Let the sampled data and random function parameters be Dt := {(xi  yi)}t
i=1 
respectively after t iteration. The function obtained by Algorithm 1 is a simple additive form of the
doubly stochastic functional gradients

i=1 and ωt := {ωi}t

and

∀t > 1 

tζi(·) 
ai

f1(·) = 0 

ft+1(·) = ft(·) − γt(ζt(·) + νft(·)) =
t = −γi

(cid:81)t
(6)
j=i+1(1 − γjν) are deterministic values depending on the step sizes γj(i (cid:54) j (cid:54)
where ai
t) and regularization parameter ν. This simple form makes it easy for us to analyze its convergence.
We note that our algorithm can also take a mini-batch of points and random functions at each step 
and estimate an empirical covariance for preconditioning to achieve potentially better performance.
5 Theoretical Guarantees
In this section  we will show that  both in expectation and with high probability  our algorithm
√
can estimate the optimal function in the RKHS with rate O(1/t) and achieve a generalization
bound of O(1/
t). The analysis for our algorithm has a new twist compared to previous analysis
of stochastic gradient descent algorithms  since the random function approximation results in
an estimator which is outside the RKHS. Besides the analysis for stochastic functional gradient
descent  we need to use martingales and the corresponding concentration inequalities to prove that
the sequence of estimators  ft+1  outside the RKHS converge to the optimal function  f∗  in the
RKHS. We make the following standard assumptions ahead for later references:

(cid:88)t

i=1

i=1 and any trajectory {fi(·)}t

A. There exists an optimal solution  denoted as f∗  to the problem of our interest (1).
B. Loss function (cid:96)(u  y) : R × R → R and its ﬁrst-order derivative is L-Lipschitz continous
C. For any data {(xi  yi)}t

in terms of the ﬁrst argument.
i=1  there exists M > 0  such that
|(cid:96)(cid:48)(fi(xi)  yi)| (cid:54) M. Note in our situation M exists and M < ∞ since we assume
bounded domain and the functions ft we generate are always bounded as well.
D. There exists κ > 0 and φ > 0  such that k(x  x(cid:48)) (cid:54) κ  |φω(x)φω(x(cid:48))| (cid:54) φ ∀x  x(cid:48) ∈
X   ω ∈ Ω. For example  when k(· ·) is the Gaussian RBF kernel  we have κ = 1  φ = 2.
We now present our main theorems as below. Due to the space restrictions  we will only provide a
short sketch of proofs here. The full proofs for the these theorems are given in the appendix.
t with θ > 0 such that θν ∈ (1  2) ∪ Z+ 
Theorem 4 (Convergence in expectation) When γt = θ

EDt ωt

(cid:2)|ft+1(x) − f∗(x)|2(cid:3) (cid:54) 2C 2 + 2κQ2
(cid:110)(cid:107)f∗(cid:107)H   (Q0 +(cid:112)Q2

t

1

for any x ∈ X

 

(cid:111)

2κ1/2(κ + φ)LM θ2  and C 2 = 4(κ + φ)2M 2θ2.

where Q1 = max
√
2
Theorem 5 (Convergence with high probability) When γt = θ
for any x ∈ X   we have with probability at least 1 − 3δ over (Dt  ωt) 

  with Q0 =
t with θ > 0 such that θν ∈ Z+ 

0 + (2θν − 1)(1 + θν)2θ2κM 2)/(2νθ − 1)

|ft+1(x) − f∗(x)|2 (cid:54) C 2 ln(2/δ)

t

+

2κQ2

2 ln(2t/δ) ln2(t)

t

 

5

(cid:110)(cid:107)f∗(cid:107)H   Q0 +(cid:112)Q2

(cid:111)

where C is as above and Q2 = max
√
2κ1/2M θ(8 + (κ + φ)θL).
Q0 = 4
Proof sketch: We focus on the convergence in expectation; the high probability bound can be
established in a similar fashion. The main technical difﬁculty is that ft+1 may not be in the RKHS
H. The key of the proof is then to construct an intermediate function ht+1  such that the difference
between ft+1 and ht+1 and the difference between ht+1 and f∗ can be bounded. More speciﬁcally 

0 + κM 2(1 + θν)2(θ2 + 16θ/ν)

  with

ht+1(·) = ht(·) − γt(ξt(·) + νht(·)) =

tξi(·) 
ai

∀t > 1 

and h1(·) = 0 

(7)

where ξt(·) = Eωt[ζt(·)]. Then for any x  the error can be decomposed as two terms
(cid:125)
+ 2κ (cid:107)ht+1 − f∗(cid:107)2H

|ft+1(x) − f∗(x)|2 (cid:54) 2|ft+1(x) − ht+1(x)|2

(cid:123)(cid:122)

(cid:123)(cid:122)

(cid:124)

(cid:124)

error due to random functions

error due to random data

complicated  et+1 (cid:54) (cid:0)1 − 2νθ

For the error term due to random functions  ht+1 is constructed such that ft+1 − ht+1 is a mar-
tingale  and the stepsizes are chosen such that |ai
t   which allows us to bound the martingale.
In other words  the choices of the stepsizes keep ft+1 close to the RKHS. For the error term
due to random data  since ht+1 ∈ H  we can now apply the standard arguments for stochastic
approximation in the RKHS. Due to the additional randomness  the recursion is slightly more
t2   where et+1 = EDt ωt[(cid:107)ht+1 − f∗(cid:107)2H]  and
β1 and β2 depends on the related parameters. Solving this recursion then leads to a bound for the
second error term.
Theorem 6 (Generalization bound) Let the true risk be Rtrue(f ) = E(x y) [l(f (x)  y)]. Then with
probability at least 1 − 3δ over (Dt  ωt)  and C and Q2 deﬁned as previously

(cid:1) et + β1

(cid:112) et

t + β2

t

t

Rtrue(ft+1) − Rtrue(f∗) (cid:54) (C(cid:112)ln(8

√

(cid:112)ln(2t/δ) ln(t))L

.

et/δ) +

√
√
2κQ2

t

2

Proof By the Lipschitz continuity of l(·  y) and Jensen’s Inequality  we have

Rtrue(ft+1)− Rtrue(f∗) (cid:54) LEx|ft+1(x)− f∗(x)| (cid:54) L(cid:112)Ex|ft+1(x) − f∗(x)|2 = L(cid:107)ft+1− f∗(cid:107)2.
Again  (cid:107)ft+1 − f∗(cid:107)2 can be decomposed as two terms O(cid:0)(cid:107)ft+1 − ht+1(cid:107)2
(cid:1) and O((cid:107)ht+1 − f∗(cid:107)2H) 

which can be bounded similarly as in Theorem 5 (see Corollary 12 in the appendix).
Remarks. The overall rate of convergence in expectation  which is O(1/t)  is indeed optimal. Clas-
sical complexity theory (see  e.g. reference in [15]) shows that to obtain -accuracy solution  the
number of iterations needed for the stochastic approximation is Ω(1/) for strongly convex case and
Ω(1/2) for general convex case. Different from the classical setting of stochastic approximation 
our case imposes not one but two sources of randomness/stochasticity in the gradient  which intu-
itively speaking  might require higher order number of iterations for general convex case. However 
our method is still able to achieve the same rate as in the classical setting. The rate of the general-
ization bound is also nearly optimal up to log factors. However  these bounds may be further reﬁned
with more sophisticated techniques and analysis. For example  mini-batch and preconditioning can
be used to reduce the constant factors in the bound signiﬁcantly  the analysis of which is left for
future study. Theorem 4 also reveals bounds in L∞ and L2 sense as in Section A.2 in the appendix.
The choices of stepsizes γt and the tuning parameters given in these bounds are only for sufﬁcient
conditions and simple analysis; other choices can also lead to bounds in the same order.
6 Computation  Storage and Statistics Trade-off
To investigate computation  storage and statistics trade-off  we will ﬁx the desired L2 error in the
function estimation to   i.e.  (cid:107)f − f∗(cid:107)2
(cid:54)   and work out the dependency of other quantities on .
These other quantities include the preprocessing time  the number of samples and random features
(or rank)  the number of iterations of each algorithm  and the computational cost and storage require-
ment for learning and prediction. We assume that the number of samples  n  needed to achieve the
prescribed error  is of the order O(1/)  the same for all methods. Furthermore  we make no other
regularity assumption about margin properties or the kernel matrix such as fast spectrum decay. Thus
the required number of random feature (or ranks) r will be of the order O(n) = O(1/) [4  5  8  9].

2

(cid:88)t

i=1

(cid:125)
t| (cid:54) θ

6

We will pick a few representative algorithms for comparison  namely  (i) NORMA [13]: kernel
methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of s-
tochastic dual coordinate ascend; (iii) r-SDCA: ﬁrst approximate the kernel function with random
features  and then run stochastic dual coordinate ascend; (iv) n-SDCA: ﬁrst approximate the ker-
nel matrix using Nystr¨om’s method  and then run stochastic dual coordinate ascend; similarly we
will combine Pegasos algorithm [21] with random features and Nystr¨om’s method  and obtain (v)
r-Pegasos  and (vi) n-Pegasos. The comparisons are summarized below.
From the table  one can see that our method  r-SDCA and r-Pegasos achieve the best dependency on
the dimension d of the data. However  often one is interested in increasing the number of random
features as more data points are observed to obtain a better generalization ability. Then special
procedures need to be designed for updating the r-SDCA and r-Pegasos solution  which we are not
clear how to implement easily and efﬁciently.

Algorithms

Doubly SGD

NORMA/k-SDCA
r-Pegasos/r-SDCA
n-Pegasos/n-SDCA

7 Experiments

Preprocessing Total Computation Cost
Prediction
Computation
O(d/)
O(d/)
O(d/)
O(d/)

Training
O(d/2)
O(d/2)
O(d/2)
O(d/2)

O(1)
O(1)
O(1)

O(1/3)

Total Storage Cost

Training
O(1/)
O(d/)
O(1/)
O(1/)

Prediction
O(1/)
O(d/)
O(1/)
O(1/)

We show that our method compares favorably to other kernel methods in medium scale datasets
and neural nets in large scale datasets. We examined both regression and classiﬁcation problems
with smooth and almost smooth loss functions. Below is a summary of the datasets used1  and more
detailed description of these datasets and experimental settings can be found in the appendix.

Forest

Name
Adult

Model
(1)
K-SVM
(2) MNIST 8M 8 vs. 6 [25] K-SVM
K-SVM
(3)
(4)
K-logistic
K-logistic
(5)
K-logistic
(6)
K-ridge
(7) QuantumMachine [28]
(8) MolecularSpace [28]
K-ridge

MNIST 8M [25]
CIFAR 10 [26]
ImageNet [27]

# of samples

32K
1.6M
0.5M
8M
60K
1.3M
6K
2.3M

123
784
54
1568
2304
9216
276
2850

Input dim Output range Virtual

{−1  1}
{−1  1}
{−1  1}
{0  . . .   9}
{0  . . .   9}
{0  . . .   999}
[−800 −2000]

[0  13]

no
yes
no
yes
yes
yes
yes
no

Experiment settings. For datasets (1) – (3)  we compare the algorithms discussed in Section 6. For
algorithms based on low rank kernel matrix approximation and random features  i.e.  pegasos and
SDCA  we set the rank and number of random features to be 28. We use same batch size for both
our algorithm and the competitors. We stop algorithms when they pass through the entire dataset
once. This stopping criterion (SC1) is designed for justifying our conjecture that the bottleneck of
the performances of the vanilla methods with explicit feature comes from the accuracy of kernel
approximation. To this end  we investigate the performances of these algorithms under different
levels of random feature approximations but within the same number of training samples. To further
investigate the computational efﬁciency of the proposed algorithm  we also conduct experiments
where we stop all algorithms within the same time budget (SC2). Due to space limitation  the
comparison on regression synthetic dataset under SC1 and on (1) – (3) under SC2 are illustrated
in Appendix B.2. We do not count the preprocessing time of Nystr¨om’s method for n-Pegasos and
n-SDCA. The algorithms are executed on the machine with AMD 16 2.4GHz Opteron CPUs and
200G memory. Note that this allows NORMA and k-SDCA to save all the data in the memory.
We report our numerical results in Figure 1(1)-(8) with explanations stated as below . For full details
of our experimental setups  please refer to section B.1 in Appendix.
Adult. The result is illustrated in Figure 1(1). NORMA and k-SDCA achieve the best error rate 
15%  while our algorithm achieves a comparable rate  15.3%.

1 A “yes” for the last column means that virtual examples are generated from for training. K-ridge stands

for kernel ridge regression; K-SVM stands for kernel SVM; K-logistic stands for kernel logistic regression.

7

(1) Adult

(2) MNIST 8M 8 vs. 6

(3) Forest

(4) MNIST 8M

(5) CIFAR 10

(6) ImageNet

(7) QuantumMachine

(8) MolecularSpace.

Figure 1: Experimental results for dataset (1) – (8).

MNIST 8M 8 vs. 6. The result is shown in Figure 1(2). Our algorithm achieves the best test error
0.26%. Comparing to the methods with full kernel  the methods using random/Nystr¨om features
achieve better test errors probably because of the underlying low-rank structure of the dataset.
Forest. The result is shown in Figure 1(3). Our algorithm achieves test error about 15%  much better
than the n/r-pegasos and n/r-SDCA. Our method is preferable for this scenario  i.e.  huge datasets
with sophisticated decision boundary considering the trade-off between cost and accuracy.
MNIST 8M. The result is shown in Figure 1(4). Better than the 0.6% error provided by ﬁxed and
jointly-trained neural nets  our method reaches an error of 0.5% very quickly.
CIFAR 10 The result is shown in Figure 1(5). We compare our algorithm to a neural net with
two convolution layers (after contrast normalization and max-pooling layers) and two local layer-
s achieving 11% test error. The speciﬁcation is at https://code.google.com/p/cuda-convnet/. Our
method achieves comparable performance but much faster.
ImageNet The result is shown in Figure 1(6). Our method achieves test error 44.5% by further
max-voting of 10 transformations of the test set while the jointly-trained neural net arrives at 42%
(without variations in color and illumination)  and the ﬁxed neural net only achieves 46% with max-
voting.
QuantumMachine/MolecularSpace The results are shown in Figure 1(7) &(8). On dataset (7)  our
method achieves Mean Absolute Error of 2.97 kcal/mole  outperforming neural nets  3.51 kcal/mole 
which is close to the 1 kcal/mole required for chemical accuracy. Moreover  the comparison on
dataset (8) is the ﬁrst in the literature  and our method is still comparable with neural net.
Acknowledgement
M.B. is suppoerted in part by NSF CCF-0953192  CCF-1451177  CCF-1101283  and CCF-1422910  ONR
N00014-09-1-0751  and AFOSR FA9550-09-1-0538. L.S. is supported in part by NSF IIS-1116886  NSF/NIH
BIGDATA 1R01GM108341  NSF CAREER IIS-1350983  and a Raytheon Faculty Fellowship.

8

10−2100152025303540Training Time (sec)Test Error (%) k−SDCANORMA28 r−pegasos28 r−SDCA28 n−pegasos28 n−SDCAdoubly SGD10010210400.511.522.53Training Time (sec)Test Error (%)1001021045101520253035Training Time (sec)Test Error (%)1051061070.511.52Number of training samplesTest error (%) jointly−trained neural netfixed neural netdoubly SGD1051061071020304050Number of training samplesTest error (%) jointly−trained neural netfixed neural netdoubly SGD106108405060708090100Number of training samplesTest error (%) jointly−trained neural netfixed neural netdoubly SGD1051065101520Number of training samplesMAE (Kcal/mole) neural netdoubly SGD10510611.21.41.61.822.22.42.6Number of training samplesPCE (%) neural netdoubly SGDReferences
[1] A. J. Smola and B. Sch¨olkopf. Sparse greedy matrix approximation for machine learning. In ICML  2000.
[2] C. K. I. Williams and M. Seeger. Using the Nystrom method to speed up kernel machines.
In T. G.

Dietterich  S. Becker  and Z. Ghahramani  editors  NIPS  2000.

[3] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel representations. Journal of

Machine Learning Research  2:243–264  2001.

[4] P. Drineas and M. Mahoney. On the nystr om method for approximating a gram matrix for improved

kernel-based learning. JMLR  6:2153–2175  2005.

[5] C. Cortes  M. Mohri  and A. Talwalkar. On the impact of kernel approximation on learning accuracy. In

AISTATS  2010.

[6] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  2008.
[7] Q.V. Le  T. Sarlos  and A. J. Smola. Fastfood — computing hilbert space expansions in loglinear time. In

ICML  2013.

[8] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with random-

ization in learning. In NIPS  2009.

[9] D. Lopez-Paz  S. Sra  A. Smola  Z. Ghahramani  and B. Schlkopf. Randomized nonlinear component

analysis. In ICML  2014.

[10] J. C. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines.

Technical Report MSR-TR-98-14  Microsoft Research  1998.

[11] T. Joachims. Making large-scale SVM learning practical. In B. Sch¨olkopf  C. J. C. Burges  and A. J.
Smola  editors  Advances in Kernel Methods — Support Vector Learning  pages 169–184  Cambridge 
MA  1999. MIT Press.

[12] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. Journal

of Machine Learning Research  14(1):567–599  2013.

[13] J. Kivinen  A. J. Smola  and R. C. Williamson. Online learning with kernels. IEEE Transactions on Signal

Processing  52(8)  Aug 2004.

[14] N. Ratliff and J. Bagnell. Kernel conjugate gradient for fast kernel machines. In IJCAI  2007.
[15] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to s-

tochastic programming. SIAM J. on Optimization  19(4):1574–1609  January 2009.

[16] A. Devinatz. Integral representation of pd functions. Trans. AMS  74(1):56–77  1953.
[17] M. Hein and O. Bousquet. Kernels  associated structures  and generalizations. Technical Report 127 

Max Planck Institute for Biological Cybernetics  2004.

[18] H. Wendland. Scattered Data Approximation. Cambridge University Press  Cambridge  UK  2005.
[19] Bernhard Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.
[20] N. Pham and R. Pagh. Fast and scalable polynomial kernels via explicit feature maps. In KDD  2013.
[21] Shai Shalev-Shwartz  Yoram Singer  and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver

for SVM. In ICML  2007.

[22] Cong D. Dang and Guanghui Lan. Stochastic block mirror descent methods for nonsmooth and stochastic

optimization. Technical report  University of Florida  2013.

[23] Yurii Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM

Journal on Optimization  22(2):341–362  2012.

[24] A. Cotter  S. Shalev-Shwartz  and N. Srebro. Learning optimally sparse support vector machines.

ICML  2013.

In

[25] G. Loosli  S. Canu  and L. Bottou. Training invariant support vector machines with selective sampling.

InLarge Scale Kernel Machines  pages 301–320. MIT Press  2007.

[26] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report  University of

Toronto  2009.

[27] A. Krizhevsky  I. Sutskever  and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-

works. In NIPS  2012.

[28] G. Montavon  K. Hansen  S. Fazli  M. Rupp  F. Biegler  A. Ziehe  A. Tkatchenko  A. Lilienfeld  and K.
M¨uller. Learning invariant representations of molecules for atomization energy prediction. In NIPS  2012.
[29] Alexander Rakhlin  Ohad Shamir  and Karthik Sridharan. Making gradient descent optimal for strongly

convex stochastic optimization. In ICML  pages 449–456  2012.

[30] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  November 1998.

9

,Bo Dai
Bo Xie
Niao He
Yingyu Liang
Anant Raj
Maria-Florina Balcan
Le Song
Raanan Rohekar
Shami Nisimov
Yaniv Gurwicz
Guy Koren
Gal Novik