2008,Near-optimal Regret Bounds for Reinforcement Learning,For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s1 s2 there is a policy which moves from s1 to s2 in at most D steps (on average). We present a reinforcement learning algorithm with total regret O(DSAT) after T steps for any unknown MDP with S states  A actions per state  and diameter D. This bound holds with high probability. We also present a corresponding lower bound of Omega(DSAT) on the total regret of any learning algorithm. Both bounds demonstrate the utility of the diameter as structural parameter of the MDP.,Near-optimal Regret Bounds for

Reinforcement Learning

Peter Auer
Ronald Ortner
University of Leoben  Franz-Josef-Strasse 18  8700 Leoben  Austria

Thomas Jaksch

{auer tjaksch rortner}@unileoben.ac.at

Abstract

For undiscounted reinforcement learning in Markov decision processes (MDPs)
we consider the total regret of a learning algorithm with respect to an optimal
policy. In order to describe the transition structure of an MDP we propose a new
parameter: An MDP has diameter D if for any pair of states s  s0 there is a policy
which moves from s to s0 in at most D steps (on average). We present a rein-
forcement learning algorithm with total regret ˜O(DS
AT ) after T steps for any
unknown MDP with S states  A actions per state  and diameter D. This bound
holds with high probability. We also present a corresponding lower bound of
Ω(

DSAT ) on the total regret of any learning algorithm.

√

√

1 Introduction
In a Markov decision process (MDP) M with ﬁnite state space S and ﬁnite action space A  a learner
in state s ∈ S needs to choose an action a ∈ A. When executing action a in state s  the learner
receives a random reward r with mean ¯r(s  a) according to some distribution on [0  1]. Further 
according to the transition probabilities p (s0|s  a)  a random transition to a state s0 ∈ S occurs.
Reinforcement learning of MDPs is a standard model for learning with delayed feedback. In contrast
to important other work on reinforcement learning — where the performance of the learned policy is
considered (see e.g. [1  2] and also the discussion and references given in the introduction of [3]) —
we are interested in the performance of the learning algorithm during learning. For that  we compare
the rewards collected by the algorithm during learning with the rewards of an optimal policy.
In this paper we will consider undiscounted rewards. The accumulated reward of an algorithm A
after T steps in an MDP M is deﬁned as

R(M  A  s  T ) :=PT

t=1 rt 

where s is the initial state and rt are the rewards received during the execution of algorithm A. The
average reward

D(M) := max

s s0∈S min
π:S→A

E [T (s0|M  π  s)] .

ρ(M  A  s) := lim
T→∞

1
T

E [R(M  A  s  T )]

can be maximized by an appropriate stationary policy π : S → A which deﬁnes an optimal action
for each state [4].
The difﬁculty of learning an MDP does not only depend on its size (given by the number of states
and actions)  but also on its transition structure. In order to measure this transition structure we
propose a new parameter  the diameter D of an MDP. The diameter D is the time it takes to move
from any state s to any other state s0  using an appropriate policy for this pair of states s and s0:
Deﬁnition 1. Let T (s0|M  π  s) be the ﬁrst (random) time step in which state s0 is reached when
policy π is executed on MDP M with initial state s. Then the diameter of M is given by

A ﬁnite diameter seems necessary for interesting bounds on the regret of any algorithm with respect
to an optimal policy. When a learner explores suboptimal actions  this may take him into a “bad
part” of the MDP from which it may take about D steps to reach again a “good part” of the MDP.
Hence  the learner may suffer regret D for such exploration  and it is very plausible that the diameter
appears in the regret bound.
For MDPs with ﬁnite diameter (which usually are called communicating  see e.g. [4]) the optimal
average reward ρ∗ does not depend on the initial state (cf. [4]  Section 8.3.3)  and we set

ρ∗(M) := ρ∗(M  s) := max

π

ρ(M  π  s).

The optimal average reward is the natural benchmark for a learning algorithm A  and we deﬁne the
total regret of A after T steps as1

∆(M  A  s  T ) := T ρ∗(M) − R(M  A  s  T ).

In the following  we present our reinforcement learning algorithm UCRL2 (a variant of the UCRL
algorithm of [5]) which uses upper conﬁdence bounds to choose an optimistic policy. We show

that the total regret of UCRL2 after T steps is ˜O(D|S|p|A|T ). A corresponding lower bound of
Ω(pD|S||A|T ) on the total regret of any learning algorithm is given as well. These results establish

the diameter as an important parameter of an MDP. Further  the diameter seems to be more natural
than other parameters that have been proposed for various PAC and regret bounds  such as the mixing
time [3  6] or the hitting time of an optimal policy [7] (cf. the discussion below).

1.1 Relation to previous Work

ε

ε

ε

ε

δ   1

of an optimal policy π∗ as input parameter. This parameter T mix

We ﬁrst compare our results to the PAC bounds for the well-known algorithms E3 of Kearns 
Singh [3]  and R-Max of Brafman  Tennenholtz [6] (see also Kakade [8]). These algorithms achieve
ε-optimal average reward with probability 1− δ after time polynomial in 1
ε   |S|  |A|  and the mix-
(see below). As the polynomial dependence on ε is of order 1/ε3  the PAC bounds
ing time T mix
translate into T 2/3 regret bounds at the best. Moreover  both algorithms need the ε-return mixing
time T mix
is the number of steps
steps is ε-close to the optimal average reward ρ∗.
until the average reward of π∗ over these T mix
ε ≈ D/ε. This additional dependency on ε
It is easy to construct MDPs of diameter D with T mix
further increases the exponent in the above mentioned regret bounds for E3 and R-max. Also  the
exponents of the parameters |S| and |A| in the PAC bounds of [3] and [6] are substantially larger
than in our bound.
The MBIE algorithm of Strehl and Littman [9  10] — similarly to our approach — applies conﬁdence
bounds to compute an optimistic policy. However  Strehl and Littman consider only a discounted
reward setting  which seems to be less natural when dealing with regret. Their deﬁnition of regret
measures the difference between the rewards2 of an optimal policy and the rewards of the learning
algorithm along the trajectory taken by the learning algorithm. In contrast  we are interested in the
regret of the learning algorithm in respect to the rewards of the optimal policy along the trajectory
of the optimal policy.
Tewari and Bartlett [7] propose a generalization of the index policies of Burnetas and Katehakis [11].
These index policies choose actions optimistically by using conﬁdence bounds only for the estimates
in the current state. The regret bounds for the index policies of [11] and the OLP algorithm of [7]
are asymptotically logarithmic in T . However  unlike our bounds  these bounds depend on the gap
between the “quality” of the best and the second best action  and these asymptotic bounds also hide
an additive term which is exponential in the number of states. Actually  it is possible to prove a
corresponding gap-dependent logarithmic bound for our UCRL2 algorithm as well (cf. Remark 4
below). This bound holds uniformly over time and under weaker assumptions: While [7] and [11]
consider only ergodic MDPs in which any policy will reach every state after a sufﬁcient number of
steps  we make only the more natural assumption of a ﬁnite diameter.

1It can be shown that maxA E [R(M  A  s  T )] = T ρ∗(M ) + O(D(M )) and maxA R(M  A  s  T ) =

T ρ∗(M ) + ˜O(cid:0)√

T(cid:1) with high probability.

2Actually  the state values.

2 Results

We summarize the results achieved for our algorithm UCRL2 which is described in the next section 
and also state a corresponding lower bound. We assume an unknown MDP M to be learned  with
S := |S| states  A := |A| actions  and ﬁnite diameter D := D(M). Only S and A are known to the
learner  and UCRL2 is run with parameter δ.
Theorem 2. With probability 1− δ it holds that for any initial state s ∈ S and any T > 1  the regret
of UCRL2 is bounded by

∆(M  UCRL2  s  T ) ≤ c1 · DS

for a constant c1 which is independent of M  T   and δ.

T A log T
δ  

It is straightforward to obtain from Theorem 2 the following sample complexity bound.
Corollary 3. With probability 1 − δ the average per-step regret is at most ε for any

q

(cid:18) DSA

(cid:19)

T ≥ c2

D2S2A

log

ε2
steps  where c2 is a constant independent of M.
Remark 4. The proof method of Theorem 2 can be modiﬁed to give for each initial state s and T > 1
an alternative upper bound on the expected regret 

δε

E [∆(M  UCRL2  s  T )] ≤ c3

D2S2A log T

g

 

where g := ρ∗(M) − maxπ s{ρ(M  π  s) : ρ(M  π  s) < ρ∗(M)} is the gap between the optimal
average reward and the second best average reward achievable in M.

These new bounds are improvements over the bounds that have been achieved in [5] for the original
UCRL algorithm in various respects: the exponents of the relevant parameters have been decreased
considerably  the parameter D we use here is substantially smaller than the corresponding mixing
time in [5]  and ﬁnally  the ergodicity assumption is replaced by the much weaker and more natural
assumption that the MDP has ﬁnite diameter.
The following is an accompanying lower bound on the expected regret.
Theorem 5. For some c4 > 0  any algorithm A  and any natural numbers S  A ≥ 10  D ≥
20 logA S  and T ≥ DSA  there is an MDP 3 M with S states  A actions  and diameter D  such
that for any initial state s ∈ S the expected regret of A after T steps is

E [∆(M  A  s  T )] ≥ c4 ·

DSAT .

√

In a different setting  a modiﬁcation of UCRL2 can also deal with changing MDPs.
Remark 6. Assume that the MDP (i.e. its transition probabilities and reward distributions) is al-
lowed to change ‘ times up to step T   such that the diameter is always at most D (we assume an
initial change at time t = 1). In this model we measure regret as the sum of missed rewards com-
pared to the ‘ policies which are optimal after the changes of the MDP. Restarting UCRL2 with
parameter δ/‘2 at steps di3/‘2e for i = 1  2  3 . . .  this regret is upper bounded by

with probability 1 − 2δ.

c5 · ‘

1
3 T

2
3 DS

q

A log T
δ

MDPs with a different model of changing rewards have already been considered in [12]. There  the
√
transition probabilities are assumed to be ﬁxed and known to the learner  but the rewards are allowed
T ) on the regret against an optimal
to change in every step. A best possible upper bound of O(
stationary policy  given all the reward changes in advance  is derived.

3The diameter of any MDP with S states and A actions is at least logA S.

Input: A conﬁdence parameter δ ∈ (0  1).
Initialization: Set t := 1  and observe the initial state s1.
For episodes k = 1  2  . . . do

Initialize episode k:

1. Set the start time of episode k  tk := t.
2. For all (s  a) in S × A initialize the state-action counts for episode k  vk(s  a) := 0.

Further  set the state-action counts prior to episode k 

Nk (s  a) := #{τ < tk : sτ = s  aτ = a} .

3. For s  s0 ∈ S and a ∈ A set the observed accumulated rewards and the transition

counts prior to episode k 

tk−1X

τ =1

Rk (s  a) :=

rτ 1sτ =s aτ =a 

Pk (s  a  s0) := #{τ < tk : sτ = s  aτ = a  sτ +1 = s0}  

and compute estimates ˆrk (s  a) :=

max{1 Nk(s a)}   ˆpk (s0|s  a) := Pk(s a s0)

max{1 Nk(s a)} .

Rk(s a)

Compute policy ˜πk:
4. Let Mk be the set of all MDPs with states and actions as in M  and with tran-
sition probabilities ˜p (·|s  a) close to ˆpk (·|s  a)  and rewards ˜r(s  a) ∈ [0  1] close

to ˆrk (s  a)  that is (cid:12)(cid:12)˜r(s  a) − ˆrk
(cid:13)(cid:13)(cid:13)˜p(cid:0)·|s  a(cid:1) − ˆpk

(cid:0)s  a(cid:1)(cid:12)(cid:12) ≤ q 7 log(2SAtk/δ)
≤ q 14S log(2Atk/δ)
(cid:0)·|s  a(cid:1)(cid:13)(cid:13)(cid:13)1

2 max{1 Nk(s a)}

max{1 Nk(s a)} .

and

(1)

(2)

5. Use extended value iteration (Section 3.1) to ﬁnd a policy ˜πk and an optimistic

MDP ˜Mk ∈ Mk such that

˜ρk := min
s

ρ( ˜Mk  ˜πk  s) ≥

M0∈Mk π s0 ρ(M0  π  s0) − 1√

max

tk

.

(3)

Execute policy ˜πk:
6. While vk(st  ˜πk(st)) < max{1  Nk(st  ˜πk(st))} do

(a) Choose action at := ˜πk(st)  obtain reward rt  and observe next state st+1.
(b) Update vk(st  at) := vk(st  at) + 1.
(c) Set t := t + 1.

Figure 1: The UCRL2 algorithm.

3 The UCRL2 Algorithm

Our algorithm is a variant of the UCRL algorithm in [5]. As its predecessor  UCRL2 implements
the paradigm of “optimism in the face of uncertainty”. As such  it deﬁnes a set M of statistically
plausible MDPs given the observations so far  and chooses an optimistic MDP ˜M (with respect to
the achievable average reward) among these plausible MDPs. Then it executes a policy ˜π which is
(nearly) optimal for the optimistic MDP ˜M.
More precisely  UCRL2 (Figure 1) proceeds in episodes and computes a new policy ˜πk only at the
beginning of each episode k. The lengths of the episodes are not ﬁxed a priori  but depend on
the observations made. In Steps 2–3  UCRL2 computes estimates ˆpk (s0|s  a) and ˆrk (s  a) for the
transition probabilities and mean rewards from the observations made before episode k. In Step 4 
a set Mk of plausible MDPs is deﬁned in terms of conﬁdence regions around the estimated mean
rewards ˆrk(s  a) and transition probabilities ˆpk (s0|s  a). This guarantees that with high probability

the true MDP M is in Mk. In Step 5  extended value iteration (see below) is used to choose a near-
optimal policy ˜πk on an optimistic MDP ˜Mk ∈ Mk. This policy ˜πk is executed throughout episode
k (Step 6). Episode k ends when a state s is visited in which the action a = ˜πk(s) induced by the
current policy has been chosen in episode k equally often as before episode k. Thus  the total number
of occurrences of any state-action pair is at most doubled during an episode. The counts vk(s  a)
keep track of these occurrences in episode k.4

3.1 Extended Value Iteration

In Step 5 of the UCRL2 algorithm we need to ﬁnd a near-optimal policy ˜πk for an optimistic MDP.
While value iteration typically calculates a policy for a ﬁxed MDP  we also need to select an op-
timistic MDP ˜Mk which gives almost maximal reward among all plausible MDPs. This can be
achieved by extending value iteration to search also among the plausible MDPs. Formally  this can
be seen as undiscounted value iteration [4] on an MDP with extended action set. We denote the state
i(s) and get for all s ∈ S:
values of the i-th iteration by ui(s) and the normalized state values by u0

(

u0(s) = 0 

(cid:26)X
the set of transition probabilities ˜p(cid:0)·|s  a(cid:1) satisfying condition (2).

ui+1(s) = max
a∈A

˜rk (s  a) + max

p(·)∈P(s a)

s0∈S

Here ˜rk (s  a) are the maximal rewards satisfying condition (1) in algorithm UCRL2  and P(s  a) is

(cid:27))

p(s0) · ui(s0)

 

(4)

While (4) may look like a step of value iteration with an inﬁnite action space  maxp p· ui is actually
a linear optimization problem over the convex polytope P(s  a). This implies that only the ﬁnite
number of vertices of the polytope need to be considered as extended actions  which guarantees
convergence of the value iteration.5
The value iteration is stopped when

(cid:8)ui+1(s) − ui(s)(cid:9) − min

(cid:8)ui+1(s) − ui(s)(cid:9) <

s∈S

max
s∈S

1√
tk

 

(5)

which means that the change of the state values is almost uniform and actually close to the average
reward of the optimal policy. It can be shown that the actions  rewards  and transition probabilities
chosen in (4) for this i-th iteration deﬁne an optimistic MDP ˜Mk and a policy ˜πk which satisfy
condition (3) of algorithm UCRL2.

4 Analysis of UCRL2 and Proof Sketch of Theorem 2

In the following we present an outline of the main steps of the proof of Theorem 2. Details and the
complete proofs can be found in the full version of the paper [13]. We also make the assumption
that the rewards r(s  a) are deterministic and known to the learner.6 This simpliﬁes the exposition.
Considering unknown stochastic rewards adds little to the proof and only lower order terms to the
regret bounds. We also assume that the true MDP M satisﬁes the conﬁdence bounds in Step 4 of
algorithm UCRL2 such that M ∈ Mk. This can be shown to hold with sufﬁciently high probability
(using a union bound over all T ).
We start by considering the regret in a single episode k. Since the optimistic average reward ˜ρk
of the optimistically chosen policy ˜πk is essentially larger than the true optimal average reward ρ∗ 
it is sufﬁcient to calculate by how much the optimistic average reward ˜ρk overestimates the actual
rewards of policy ˜πk. By the choice of ˜πk and ˜Mk in Step 5 of UCRL2  ˜ρk ≥ ρ∗ − 1/
tk. Thus the
4Since the policy ˜πk is ﬁxed for episode k  vk(s  a) 6= 0 only for a = ˜πk(s). Nevertheless  we ﬁnd it
convenient to use a notation which explicitly includes the action a in vk(s  a).
5Because of the special structure of the polytope P(s  a)  the linear program in (4) can be solved very efﬁ-
ciently in O(S) steps after sorting the state values ui(s0). For the formal convergence proof also the periodicity
of optimal policies in the extended MDP needs to be considered.

6In this case all plausible MDPs considered in Steps 4 and 5 of algorithm UCRL2 would give these rewards.

√

regret ∆k during episode k is bounded as

tk+1−1X

t=tk

∆k :=

(ρ∗ − rt) ≤

tk+1−1X

t=tk

(˜ρk − rt) + tk+1 − tk√

.

tk

√
The sum over k of the second term on the right hand side is O(
T ) and will not be considered
further in this proof sketch. The ﬁrst term on the right hand side can be rewritten using the known
deterministic rewards r(s  a) and the occurrences of state action pairs (s  a) in episode k 

tk+1−1X

(˜ρk − rt) = X

vk(s  a)(cid:0)˜ρk − r(s  a)(cid:1).

∆k .

t=tk

(s a)

4.1 Extended Value Iteration revisited

(6)

(7)

To proceed  we reconsider the extended value iteration in Section 3.1. As an important observation
for our analysis  we ﬁnd that for any iteration i the range of the state values is bounded by the
diameter of the MDP M 

max

s

ui(s) − min

ui(s) ≤ D.

s

To see this  observe that ui(s) is the total expected reward after i steps of an optimal non-stationary
i-step policy starting in state s  on the MDP with extended action set as considered for the extended
value iteration. The diameter of this extended MDP is at most D as it contains the actions of the true
MDP M. If there were states with ui(s1) − ui(s0) > D  then an improved value for ui(s0) could
be achieved by the following policy: First follow a policy which moves from s0 to s1 most quickly 
which takes at most D steps on average. Then follow the optimal i-step policy for s1. Since only D
of the i rewards of the policy for s1 are missed  this policy gives ui(s0) ≥ ui(s1) − D  proving (7).
For the convergence criterion (5) it can be shown that at the corresponding iteration

|ui+1(s) − ui(s) − ˜ρk| ≤ 1√
tk

for all s ∈ S  where ˜ρk is the average reward of the policy ˜πk chosen in this iteration on the
optimistic MDP ˜Mk.7 Expanding ui+1(s) according to (4)  we get

ui+1(s) = r(s  ˜πk(s)) +X

˜pk (s0|s  ˜πk(s)) · ui(s0)

s0

−

and hence

 X

˜ρk − r(s  ˜πk(s))

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
 
!
Deﬁning rk := (cid:0)rk
(cid:0)s  ˜πk(s)(cid:1)(cid:1)
(cid:0)˜pk (s0|s  ˜πk(s))(cid:1)
s s0 as the transition matrix of ˜πk on ˜Mk  and vk :=(cid:0)vk
(cid:0) ˜P k − I(cid:1)ui +X
∆k . X
vk(s  a)(cid:0)˜ρk − r(s  a)(cid:1) ≤ vk

s as the (column) vector of rewards for policy ˜πk  ˜P k :=
s as the (row)
vector of visit counts for each state and the corresponding action chosen by ˜πk  we can rewrite (6)
as

˜pk (s0|s  ˜πk(s)) · ui(s0) − ui(s)

!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1√
(cid:0)s  ˜πk(s)(cid:1)(cid:1)

tk

 

(8)

s0

.

vk(s  a)√
tk

(s a)

(s a)

recalling that vk(s  a) = 0 for a 6= ˜πk(s). Since the rows of ˜P k sum to 1  we can replace ui by wk
with wk(s) = ui(s) − mins ui(s) (we again use the subscript k to reference the episode). The last
term on the right hand side of (8) is of lower order  and by (7) we have

(cid:0) ˜P k − I(cid:1)wk 

∆k . vk
kwkk∞ ≤ D.

(9)
(10)

7This is quite intuitive. We expect to receive average reward ˜ρk per step  such that the difference of the state

values after i + 1 and i steps should be about ˜ρk.

4.2 Completing the Proof
Replacing the transition matrix ˜P k of the policy ˜πk in the optimistic MDP ˜Mk by the transition
matrix P k of ˜πk in the true MDP M  we get

(cid:0) ˜P k − I(cid:1)wk = vk
(cid:0) ˜P k − P k
(cid:1)wk + vk

(cid:0) ˜P k − P k + P k − I(cid:1)wk
(cid:0)P k − I(cid:1)wk.

∆k . vk
= vk

(11)
The intuition about the second term in (11) is that the counts of the state visits vk are relatively close
to the stationary distribution of the transition matrix P k  such that vk
formal proof requires the deﬁnition of a suitable martingale and the use of concentration inequalities

(cid:0)P k−I(cid:1) should be small. The
!

for this martingale. This yieldsX

 

r

(cid:0)P k − I(cid:1)wk = O

D

T log T
δ

vk

k

with high probability  which gives a lower order term in our regret bound. Thus  our regret bound is
mainly determined by the ﬁrst term in (11). Since ˜Mk and M are in the set of plausible MDPs Mk 
this term can be bounded using condition (2) in algorithm UCRL2:

(cid:1)wk = X
≤ X
≤ X

s

s

s0

vk

X
(cid:0)s  ˜πk(s)(cid:1) ·(cid:0) ˜P k(s  s0) − P k(s  s0)(cid:1) · wk(s0)
(cid:0)s  ˜πk(s)(cid:1) ·(cid:13)(cid:13)(cid:13) ˜P k(s ·) − P k(s ·)
(cid:13)(cid:13)(cid:13)1
q 14S log(2AT /δ)
(cid:0)s  ˜πk(s)(cid:1) · 2
max{1 Nk(s ˜πk(s))} · D .

· kwkk∞

vk

(cid:0) ˜P k − P k

∆k . vk

that Nk(s  a) =
i<k vi(s  a). By the condition of the while-loop in Step 6 of algorithm UCRL2  we have that

Let N(s  a)
vk(s  a) ≤ Nk(s  a). Summing (12) over all episodes k we get

(s a) N(s  a) = T and recall

P

s

vk

:= P
k vk(s  a) such that P
X
∆k ≤ const ·X
X

k

(12)

(13)

(14)

max{1 Nk(s a)} · D

vk(s  a) ·q S log(AT /δ)
X
pN(s  a)

√

(s a)

k

k

(s a)

= const · D ·pS log(AT /δ) ·X
≤ const · D ·pS log(AT /δ) ·X
≤ const · D ·pS log(AT /δ) ·
≤(cid:16)√

(cid:17)p

2 + 1

√

xkpXk−1

(s a)

nX
o

k=1

SAT .

Xn  

vk(s a)

max{1 Nk(s a)}

Here we used for (13) that

n
1 Pk

where Xk = max
Noting that Theorem 2 holds trivially true for T ≤ A gives the bound of the theorem.

and 0 ≤ xk ≤ Xk−1  and we used Jensen’s inequality for (14).

i=1 xi

5 The Lower Bound (Proof Sketch for Theorem 5)
We ﬁrst consider an MDP with two states s0 and s1  and A0 = b(A − 1)/2c actions. For each
action a  let r(s0  a) = 0  r(s1  a) = 1  and p (s0|s1  a) = δ where δ = 10/D. For all but a single
“good” action a∗ let p (s1|s0  a) = δ  while p (s1|s0  a∗) = δ + ε for some 0 < ε < δ. The diameter
of this MDP is 1/δ. The average reward of a policy which chooses action a∗ in state s0 is δ+ε
2 
2δ+ε > 1
2. Thus the regret suffered by a suboptimal action
while the average reward of any other policy is 1
in state s0 is Ω(ε/δ). The main observation for the proof of the lower bound is that any algorithm

needs to probe Ω(A0) actions in state s0 for Ω(cid:0)δ/ε2(cid:1) times on average  to detect the “good” action a∗
action a∗  we ﬁnd that Ω(kA0) actions in the s0-states of the copies need to be probed for Ω(cid:0)δ/ε2(cid:1)
times to detect the “good” action. Setting ε = pδkA0/T   suboptimal actions need to be taken
Ω(cid:0)kA0δ/ε2(cid:1) = Ω(T ) times which gives Ω(T ε/δ) = Ω(

reliably.
Considering k := bS/2c copies of this MDP where only one of the copies has such a “good”

T DSA) regret.

√

Finally  we need to connect the k copies into a single MDP. This can be done by introducing A0 + 1
additional deterministic actions per state  which do not leave the s1-states but connect the s0-states
of the k copies by inducing an A0-ary tree structure on the s0-states (1 action for going toward the
root  A0 actions to go toward the leaves). The diameter of the resulting MDP is at most 2(D/10 +
dlogA0 ke) which is twice the time to travel to or from the root for any state in the MDP. Thus we
have constructed an MDP with ≤ S states  ≤ A actions  and diameter ≤ D which forces regret
Ω(

DSAT ) on any algorithm. This proves the theorem.

√

Acknowledgments

This work was supported in part by the Austrian Science Fund FWF (S9104-N13 SP4). The research
leading to these results has received funding from the European Community’s Seventh Framework
Programme (FP7/2007-2013) under grant agreements n◦ 216886 (PASCAL2 Network of Excel-
lence)  and n◦ 216529 (Personal Information Navigator Adapting Through Viewing  PinView). This
publication only reﬂects the authors’ views.

References

[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press  1998.
[2] Michael J. Kearns and Satinder P. Singh. Finite-sample convergence rates for Q-learning and indirect

algorithms. In Advances in Neural Information Processing Systems 11. MIT Press  1999.

[3] Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time. Mach.

Learn.  49:209–232  2002.

[4] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John

Wiley & Sons  Inc.  New York  NY  USA  1994.

[5] Peter Auer and Ronald Ortner. Logarithmic online regret bounds for reinforcement learning. In Advances

in Neural Information Processing Systems 19  pages 49–56. MIT Press  2007.

[6] Ronen I. Brafman and Moshe Tennenholtz. R-max – a general polynomial time algorithm for near-optimal

reinforcement learning. J. Mach. Learn. Res.  3:213–231  2002.

[7] Ambuj Tewari and Peter Bartlett. Optimistic linear programming gives logarithmic regret for irreducible

mdps. In Advances in Neural Information Processing Systems 20  pages 1505–1512. MIT Press  2008.

[8] Sham M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis  University College

London  2003.

[9] Alexander L. Strehl and Michael L. Littman. A theoretical analysis of model-based interval estimation.

In Proc. 22nd ICML 2005  pages 857–864  2005.

[10] Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for Markov

decision processes. J. Comput. System Sci.  74(8):1309–1331  2008.

[11] Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision pro-

cesses. Math. Oper. Res.  22(1):222–255  1997.

[12] Eyal Even-Dar  Sham M. Kakade  and Yishay Mansour. Experts in a Markov decision process.

Advances in Neural Information Processing Systems 17  pages 401–408. MIT Press  2005.

In

[13] Peter Auer  Thomas Jaksch  and Ronald Ortner. Near-optimal regret bounds for reinforcement learn-
ing. Technical Report CIT-2009-01  University of Leoben  Chair for Information Technology  2009.
http://institute.unileoben.ac.at/infotech/publications/TR/CIT-2009-01.pdf.

,Yash Deshpande
Andrea Montanari