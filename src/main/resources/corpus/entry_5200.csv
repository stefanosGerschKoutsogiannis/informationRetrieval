2019,Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection,Nearest-neighbor (NN) procedures are well studied and widely used in both supervised and unsupervised learning problems. In this paper we are concerned with investigating the performance of NN-based methods for anomaly detection. We first show through extensive simulations that NN methods compare favorably to some of the other state-of-the-art algorithms for anomaly detection based on a set of benchmark synthetic datasets. We further consider the performance of NN methods on real datasets  and relate it to the dimensionality of the problem. Next  we analyze the theoretical properties of NN-methods for anomaly detection by studying a more general quantity called distance-to-measure (DTM)  originally developed in the literature on robust geometric and topological inference. We provide finite-sample uniform guarantees for the empirical DTM and use them to derive misclassification rates for anomalous observations under various settings. In our analysis we rely on Huber's contamination model and formulate mild geometric regularity assumptions on the underlying distribution of the data.,Statistical Analysis of Nearest Neighbor Methods for

Anomaly Detection

1Department of Statistics and Data Science  Carnegie Mellon University

2Heinz College of Information Systems and Public Policy  Carnegie Mellon University

Xiaoyi Gu1  Leman Akoglu2  Alessandro Rinaldo1

{xgu1 lakoglu}@andrew.cmu.edu  arinaldo@cmu.edu

Abstract

Nearest-neighbor (NN) procedures are well studied and widely used in both super-
vised and unsupervised learning problems. In this paper we are concerned with
investigating the performance of NN-based methods for anomaly detection. We
ﬁrst show through extensive simulations that NN methods compare favorably to
some of the other state-of-the-art algorithms for anomaly detection based on a
set of benchmark synthetic datasets. We further consider the performance of NN
methods on real datasets  and relate it to the dimensionality of the problem. Next 
we analyze the theoretical properties of NN-methods for anomaly detection by
studying a more general quantity called distance-to-measure (DTM)  originally
developed in the literature on robust geometric and topological inference. We
provide ﬁnite-sample uniform guarantees for the empirical DTM and use them to
derive misclassiﬁcation rates for anomalous observations under various settings. In
our analysis we rely on Huber’s contamination model and formulate mild geometric
regularity assumptions on the underlying distribution of the data.

1

Introduction

Anomaly detection is the process of detecting instances that deviate signiﬁcantly from the other
sample members. The problem of detecting anomalies can arise in many different applications  such
as fraud detection in ﬁnancial transactions  intrusion detection for security systems  and various
medical examinations.
Depending on the availability of data labels  there are multiple setups for anomaly detection. The
ﬁrst is the supervised setup  where labels are available for both normal and anomalous instances
during the training stage. Because of its similarity to the standard classiﬁcation setup  numerous
classiﬁcation methods with good empirical performance and well-studied theoretical properties can
be adopted. The second setup is the semi-supervised setup  where training data only comprise
normal instances and no anomalies. This setup is widely used in the intrusion detection literature.
Well-known methods with theoretical guarantees include kNNG [1]  BP-kNNG [2] and BCOPS [3] 
with the ﬁrst two methods developed based on the geometric entropy minimization (GEM) principle
proposed in [1]  and the third on conformal prediction. Methods under this setups are essentially
targeting the estimation of high density regions  and treating low density points as anomalies. The
third setup is the unsupervised setup  which is the most ﬂexible yet challenging setup. For the rest of
the paper  we will only focus on this setup and do not assume any prior knowledge on data labels.
Many empirical methods have been developed in the unsupervised setup  which can be roughly
classiﬁed into four categories: density based methods such as the Robust KDE (RKDE) [4]  Local
Outlier Factor (LOF) [5]  and mixture models (EGMM); distance based methods such as kNN
[6] and Angle-based Outlier Detection (ABOD) [7]; model based methods such as the one-class
SVM (OCSVM) [8]  SVDD [9]  and autoencoders [10]; ensemble methods such as Isolation Forest

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(IForest) [11] and LODA [12]. In practice  ensemble methods are often favored for their computa-
tional efﬁciency and robustness to tuning parameters  yet there is little theoretical understanding of
how and why these algorithms work. Recent work on NN-methods combine kNN with sub-sampling
[13] [14] or bagging [15] [16]  and show that such methods are comparable to the other state-of-the-art
methods  both in performance and computational efﬁciency. Moreover  some theoretical results [14]
[17] have been developed on how these methods work.
In this paper  we focus on studying NN-methods in the unsupervised setting  without any sub-sampling
or bagging. We begin with an empirical analysis of NN-methods on a set of synthetic benchmark
datasets and show that they compare favorably to the other state-of-the-art algorithms. We further
discuss their performance on real datasets and relate it to the dimensionality of the problem. Next 
we provide statistical analysis of NN-methods by analyzing the distance-to-a-measure (DTM) [18]  a
generalization to the NN scheme. The quantity was initially raised in the robust topological inference
literature  in which DTM proves to be an effective distance-like function for shape reconstruction in
the presence of outliers [19]. We give ﬁnite sample uniform guarantees on the empirical DTM  and
also demonstrate how DTM classiﬁes the anomalies  under suitable assumptions on the underlying
distribution of the data. Our theoretical results differ  both in assumptions and goals  from those
provided in [17] [14] signiﬁcantly  and provide complementary insights into the performance of
NN-based methods both for anomaly detection and for more general tasks.

2 Empirical Performance of NN-methods

Two versions of the NN anomaly detection algorithms have been proposed: kthNN [20] and kNN [6].
kthNN assigns anomaly score of an instance by computing the distance to its kth-nearest-neighbor 
whereas kNN takes the average distance over all k-nearest-neighbors. Both methods are shown to
have competitive performance in various comparative studies [21  22  12  23]. In particular  the
comparative study developed by Goldstein and Uchida [21] is the one of most comprehensive analysis
to date that includes the discussion of NN-methods and  at the same time  aligns with the unsupervised
anomaly detection setup. However  the authors omit the analysis of ensemble methods  some of
which are considered as state-of-the-art algorithms (e.g.  IForest and LODA). Emmott et al. [24]
constructed a large corpus (over 20 000) of synthetic benchmark datasets that vary across multiple
aspects (e.g.  clusteredness  separability  difﬁculty  etc). The authors evaluate the performance of eight
top-performing algorithms  including IForest and LODA  but omit the analysis of NN-methods.
In this section  we provide a comprehensive empirical analysis of NN-methods by comparing kNN 
1 to IForest  LOF and LODA on (1) the corpus of synthetic datasets developed
kthNN  and DTM2
in [24]  (2) 23 real datasets from the ODDS library [25]  and (3) 6 high dimensional datasets from
the UCI library [26]. The code for all our experiments are publicly available2. In general  no one
methodology should be expected to perform well in all possible scenarios. In Appendix D we present
different examples in which IForest  LODA  LOF and DTM2 perform very differently. For all our
experiments  we set the following hyperparameters for our models: sub-sampling size = 256 and
the number of trees = 100 for IForest; k = 0.03 × (sample size) for all distance based methods for
√
comparable results; for LODA  we use 100 projections with each projection using approximately
d
features. The discussion on the robustness of distance-based methods to the choice of hyperparameter
k can be found at [27] [28].

2.1 Comparison on Benchmark Datasets

First  we complement Emmott et al.’s study [24] by extending it to NN-based detectors. First 
we calculate the ROC-AUC (AUC) and Average Precision (AP) scores for each method on each
benchmark  and compute their respective quantiles on the empirical distributions for AUC and AP
scores (refer to Appendix E in [24] for more details on treating AUC and AP as random variables).
We say that an algorithm fails on a benchmark with metric AUC (or AP) at signiﬁcance level α if the
computed AUC (or AP) quantiles are less than (1 − α). Then  the failure rate for each algorithm is
found as the percentage of failures over the entire benchmark corpus. The failure rate gives a better

1DTM2 stands for the empirical DTM (see Section 3) with q = 2. We include its empirical analysis here for

comparison purposes.

2https://github.com/xgu1/DTM

2

Table 1: Algorithm Failure Rate with Signiﬁcance Level α = 0.001.

AUC
0.5898
ABOD
0.5520
IForest
0.6187
LODA
0.6016
LOF
0.6122
RKDE
OCSVM 0.7218
0.8482
SVDD
EGMM 0.6188
0.5646
0.5831
0.5669

kNN
kthNN
DTM2

AP

0.6784
0.6514
0.6955
0.7071
0.7030
0.7342
0.8868
0.7146
0.6744
0.6886
0.6761

Either
0.7000
0.6741
0.7194
0.7331
0.7194
0.7969
0.9080
0.7303
0.6960
0.7100
0.6977

(a) AUC

(b) AP

Figure 1: Boxplots for AUC and AP scores on 23 real datasets.

measure of the overall performance of different methods across the entire benchmark datasets than
the average AUC (or AP) scores  as it takes into account of the difﬁculty of each dataset.
The results are shown in Table 1  where the top section is copied from [24] and the bottom section
shows the failure rates we obtained for kNN  kthNN  and DTM2. The "Either" column indicates
that the benchmarks fail under at least one of the two metrics. Among all methods  IForest gives the
lowest failure rates (boldfaced) for all three metrics. kNN and DTM2 turn out to be next-best top
performers  falling marginally behind IForest.

2.2 Comparison on Datasets from the ODDS library

Next  we compare the performance of IForest  LODA  LOF  DTM2  kNN and kthNN on 23 real
datasets from the ODDS library [25]. Figure 1 presents the overall distributions of AUC and AP
scores of the ﬁve methods as boxplots. It appears that all methods  except for LOF  have comparable
performance  and we further veriﬁed this claim via pairwise Wilcoxon signed-rank tests between
methods  which showed no statistically signiﬁcant difference at level 0.05. The full performance
table (Table 3) is given in the Appendix  with the last row of the table showing the average rank of
each method.

3

AUC
gisette
isolet
letter

madelon
cancer

ionosphere

AP

gisette
isolet
letter

madelon
cancer

ionosphere

Table 2: AUC and AP performance on high dimensional datasets

n

3850
4886
4586
1430
385
242

n

3850
4886
4586
1430
385
242

d

4970
617
617
500
30
33

d

4970
617
617
500
30
33

IForest LODA
0.5176
0.5023
0.5485
0.5421
0.5459
0.5600
0.5427
0.5327
0.9626
0.9528
0.9265
0.9118

LOF
0.6753
0.7330
0.7846
0.5450
0.8097
0.9450

(a) AUC

IForest LODA
0.0877
0.0907
0.1003
0.1005
0.0980
0.0956
0.0974
0.1067
0.8277
0.6274
0.7222
0.7438

LOF
0.1628
0.2343
0.2921
0.1171
0.3121
0.6058

(b) AP

kNN
0.5696
0.6810
0.7162
0.5608
0.9780
0.9832

kNN
0.1093
0.2074
0.2328
0.1209
0.8813
0.8903

kthNN DTM2 DTMF2
0.7051
0.5429
0.6480
0.7645
0.8096
0.6826
0.5546
0.5552
0.6937
0.9756
0.9803
0.9372

0.5692
0.6796
0.7149
0.5607
0.9773
0.9824

kthNN DTM2 DTMF2
0.1015
0.1723
0.2458
0.1846
0.3010
0.2054
0.1166
0.1181
0.2800
0.8840
0.8801
0.6105

0.1092
0.2070
0.2319
0.1209
0.8864
0.8868

2.3 Effect of the dimension

We then take a closer look at the performance of IForest  LODA  LOF  DTM2  kNN and kthNN
when the data is high dimensional. Additionally  we include the analysis of DTMF2 in our exper-
iments  a quantity deﬁned as the inverse ratio of the DTM2 of a point and the average DTM2 of
its k-nearest neighbors. DTMF2 can be interpreted as a LOF version of DTM2 and is described
in the Appendix A. We consider six high dimensional real datasets from the UCI library [26] (see
[12] for details) and compute the AUC and AP scores for each algorithm. The results are presented
in Table 2. The n and d columns stand for the number of samples and dimension of the datasets.
On datasets gisette  isolet and letter  the performance of IForest and LODA have been signiﬁcantly
downgraded; the NN-methods give somewhat better performance  whereas LOF and DTMF2 are
showing signiﬁcantly stronger performance. However  on datasets cancer and ionoshphere  where
dimensions are slightly lower  the situations are reversed  with LOF and DTMF2 giving signiﬁcantly
worse performance than the others. This is consistent with our ﬁndings in Section 2.2. The deﬁciency
of IForest in high dimensions is expected  as the IForest trees are generated by random partitioning
along a randomly selected feature. However  in high dimensions  there is a high probability that
a large number of features are neglected in the process. From another perspective  [29] discusses
the various effects of dimensionality in the context of anomaly detection. In particular  the authors
describe a concentration effect of distances in high dimensions  which has a negative effect on IForest 
or any other methods that rely on pairwise distances of points for computation of anomaly scores.
NN-methods  on the other hand  are somewhat more robust in high dimensions  as the rankings of
distance values are still feasible.
Overall  our experiments show that IForest and NN-methods are the top two methods with excellent
overall performance on both low dimensional synthetic and real datasets. However  NN-methods
exhibit better performance than IForest when the data is high dimensional. In the following sections 
we provide a theoretical understanding of how the NN-methods work under the anomaly detection
framework.

3 Theoretical Analysis

In this section we formalize the settings for a simple yet natural anomaly detection problem based
on the classic Huber-contamination model [30  31]  whereby a target distribution generating normal
observations is corrupted by a distribution from which anomalous observations are drawn. We
introduce the notion of distance-to-a-measure (DTM) [18]  as an overall functional of the data based

4

on nearest neighbors statistics and provide ﬁnite sample bounds on the empirical nearest neighbor
radii and on the rates of consistency of the DTM in the supremum norm. These theoretical guarantees
are novel and may be of independent interest. Finally  we derive conditions under which DTM-based
methods provably separate normal and anomalous points  as a function of the level of contamination
and the separation between the normal distribution and the anomalous distribution. All the proofs are
given in the Appendix B.

3.1 Problem Setup
We assume we observe n i.i.d. realizations Xn = (X1  . . .   Xn) from a distribution P on Rd that
follows the Huber contamination model [30  31]

P = (1 − ε)P0 + εP1 

where P0 and P1 are  respectively  the underlying distribution for the normal and anomalous instances 
and ε ∈ [0  1) is the proportion of contamination. Letting S0 and S1 be the support of P0 and P1 
respectively  we further assume that S0 ∩ S1 = ∅. The distributions P0 and P1  their support and the
level of contamination ε are unknown.
Our goal is to devise a procedure that is able to discriminate the normal observations Xi’s belonging
to S0  from the anomalous ones  falling in the set S1. Since we will be focusing exclusively on NN
methods  we will begin by introducing a population counterpart to the notion of kth nearest neighbor.
Throughout the article  for any x ∈ Rd and r > 0  B(x  r) denotes the closed Euclidean ball of
radius r centered at x.
Deﬁnition 3.1 (p-NN radius). Let p ∈ (0  1). For any x  deﬁne rp(x) to be the radius of the smallest
ball centered at x with P -probability mass at least p. Formally 

rp(x) = inf{r > 0 : P (B(x  r)) ≥ p}.

Naturally  the empirical p-NN radius is deﬁned as

ˆrp(x) = inf{r > 0 : Pn(B(x  r)) ≥ p} 

where Pn is the empirical measure that puts mass 1/n on each Xi. Setting k = (cid:100)np(cid:101)  ˆrp(x) is simply
the kth-nearest neighbor radius of the point x with respect to the sample (X1  . . .   Xn). Thus 

Pn(B(x  ˆrp(x)) =

|{X1  . . .   Xn} ∩ B(x  ˆrp(x))| =

1
n

k
n

.

We will impose the following  mild regularity assumptions on the distribution P :

• Assumption (A0):

• Assumption (A1):

The sets S0 and S1 have diameters bounded by some L > 0  and are disjoint from each
other.

There exist positive constants C = C(P ) and ν0 = ν0(P ) such that for all 0 < ν < ν0 and
γ ∈ R 

|P (B(x  rp(x) + γ)) − P (B(x  rp(x)))| ≤ ν ⇒ |γ| < Cν 

• Assumption (A2):

for P -almost every x.
P0 satisﬁes the (a b)-condition: For b > 0  for any x ∈ S0  there exist a = a(x) > 0  and
r > 0 such that P0(B(x  r)) ≥ min{1  arb}.

Intuitively  assumption (A1) implies that P has non-zero probability content around the boundary
of B(x  rp(x)). Observing further that the function r ∈ R+ (cid:55)→ Fx(r) = P (B(x  r)) is the c.d.f. of
the random variable (cid:107)X − x(cid:107)  where X ∼ P   then a sufﬁcient condition for (A1) to hold is that 
uniformly over all x  Fx has its derivative uniformly bounded away from zero in a ﬁxed neighborhood
of rp(x). This condition  originally formulated in [19] to derive bootstrap-based conﬁdence bands for
the DTM function  appears to be a natural regularity assumption in the analysis of NN-type methods.
When a(x) = a for all x ∈ S0  assumption (A2) reduces to a widely used condition in the literature
on statistical inference for geometric and topological data analysis [32  33]. Such condition requires

5

the support of P0 to not locally resemble a lower dimensional set; in particular  it prevents S0 from
having thin ridges or outward cusps. When (A2) is violated  it becomes impossible to estimate S0  no
matter the size of the sample. The parameter b can be interpreted as the intrinsic dimension of P . In
particular  if P admits a strictly positive density on a D-dimensional smooth manifold  then it can be
shown that b = D.
Deﬁnition 3.2 (DTM [18]). The distance-to-a-measure (DTM) with respect to a probability distribu-
tion P with parameter m ∈ (0  1) and power q ≥ 1 is deﬁned as

(cid:18) 1

(cid:90) m

(cid:19)1/q

d(x) = dP m q(x) =

rp(x)q dp

.

0

m
When q = ∞  we set d(x) = dP m ∞(x) = rm(x).
It is immediate from the deﬁnition that a point x ∈ Rd has a small DTM value d(x) if its p-NN radii 
when averaged across all p ∈ (0  m) are small. Intuitively  d(x) can be thought of as a measure of the
distance of x from the bulk of the mass of the probability distribution P at level of accuracy speciﬁed
by the parameter m. The choice of the parameter q allows to weight differently the impact of large
versus small p-NN radii.
By substituting rp(x) with ˆrp(x) in (1)  the empirical DTM can be seen to be

(1)

ˆd(x) = dPn m q(x) =

(cid:107)Xi − x(cid:107)q

 1

k

(cid:88)

Xi∈Nk(x)

1/q

 

where k = (cid:100)mn(cid:101) and Nk(x) denotes the set of k-nearest neighbors to x in the sample. Different
values of q ≥ 1 yield different NN-functionals. In particular  the empirical DTM with q = 1 is
equivalent to the kNN method  and the empirical DTM with q = ∞ is equivalent to kthNN. The
notion of DTM was initially introduced in the geometric inference literature [19]  where DTM was
developed for shape reconstruction under the presence of outliers. The DTM is known to have
several nice properties: it is 1-Lipschitz and it is robust with respect to perturbations of the original
distributions with respect to the Wasserstein distance. The case of q = 2 is special: the corresponding
DTM  denoted below as DTM2  is also semi-concave and distance-like  and admits strong regularity
conditions on its sub-level sets. Chazal et al. [19] have also derived the limiting distribution and a
conﬁdence band for the DTM.

3.2 Uniform bounds for ˆrp and ˆd

Theorem 3.3. Let δ ∈ (0  1) and set βn =(cid:112)(4/n)((d + 1) log 2n + log (8/δ)). Under assumption

In this section we derive ﬁnite sample bounds on the deviation of ˆrp and ˆd from rp and dP m q 
respectively  that hold uniformly over all x ∈ Rd or only over the sample points. These theoretical
guarantees are  to the best of our knowledge  novel and may be of independent interest.
(A1)  with probability at least 1 − δ we have that

|ˆrp(x) − rp(x)| ≤ C(β2

n + βn

p) 

√

sup

x

√

p + β2

n + βn

p ≤ 1

where C is the constant introduced in Assumption (A1)  simultaneously over all p ∈ (0  1) such that
(2)
The dimension d enters in the previous bound in such a way that  for all p satisfying (2)  supx |ˆrp(x)−
rp(x)| → 0 with probability tending to 1 provided that d
n → 0. If we limit the supremum only to
the sample points  then the dependence on the dimension disappears altogether and we can instead
achieve a nearly-parametric rate of

Theorem 3.4. Let δ ∈ (0  1) and set αn = (cid:112)(4/(n − 1))(log 2(n − 1) + log (8n/δ)). Under

(cid:113) log n

and p − β2

n − βn

p ≥ 0.

n .

√

assumption (A1)  with probability at least 1 − δ we have that
|ˆrp(Xi) − rp(Xi)| ≤ C(α2

max

√

n + αn

p +

i=1 ... n

where C is the constant introduced in Assumption (A1)  simultaneously over all p ∈ (0  1) such that
(3)

and p − 2/n − α2

p ≤ 1

p + α2

√

n − αn

n + αn

2
n

)

(cid:112)p − 2/n ≥ 0.

6

The results in Theorem 3.3 and Theorem 3.4 yield the following uniform bounds for the DTM of all
order.
Theorem 3.5. Under assumption (A0) and (A1)  with probability at least 1 − δ 

|d(x) − ˆd(x)| ≤ C1βn(βn +

sup

x

√

m) 

and

|d(Xi) − ˆd(Xi)| ≤ C2αn(αn +

max

i=1 ... n

√

m +

2
n

).

where βn and αn are deﬁned in Theorem 3.3 and Theorem 3.4 and C1 and C2 are some positive
constants depending on q  the diameter bound L in Assumption (A0) and the constant C in Assumption
(A1).
Remark. The bound in Theorem 3.5 holds for all choices of q ≥ 1  including the case of q = ∞.
p)q dp will bring out an explicit dependence on q but

Evaluating explicitly the integral(cid:82) m

√

0 (βn +

will not lead to better rates.

3.3 DTM for anomaly detection: theoretical guarantees

We are now ready to derive some theoretical guarantees on the performance of DTM-based methods
for discriminating normal and anomalous points in the sample (X1  . . .   Xn) according to the Huber-
contamination model described above in Section 3.1. We recall that in our setting  a sample point Xi
is normal if it belongs to the support S0 of P0  and is otherwise deemed an anomaly if it lies in S1 
the support of P1  where S1 ∩ S0 = ∅.
The methodology we consider is quite simple  and it is consistent with the prevailing practice of
assigning to each sample point a score that expresses its degree of being anomalous compared to
the other points. In detail  we rank the sample points based on their empirical DTM values  and we
declare the points with largest empirical DTM values as anomalies. This simple procedure will work
perfectly well if

max
Xi∈S0

ˆd(Xi) < min
Xi∈S1

ˆd(Xi)

and if the difference between the two quantities is large. In general  of course  one would expect
that some sample points in S0 may have smaller empirical DTMs of some of the points in S1. The
extent to which such incorrect labeling occurs depends on two key factors: how closely the empirical
DTM tracks the true DTM and whether the population DTM could itself discriminate normal points
versus anomalous ones. The former issue can be handled using the high probability bounds on the
stochastic ﬂuctuations of the empirical DTM obtained in the previous section. The latter issue will
instead require to specify some degree of separation between the mixture components P0 and P1 
both in terms of the distance between their supports but also in terms of how their probability mass
gets distributed. There is more than one way to formalize this setting. Here we choose to remain
completely agnostic to the form of the contaminating distribution P1  for which we impose virtually
no constraint. On the other hand  we require the normal distribution P0 to satisfy condition (A2)
above in such a way that point inside the support will have larger values of a(x) than points near the
boundary of S0. This condition  which is satisﬁed if for example P0 admits a Lebesgue density whose
values increase as a function of the distance from the boundary of S0  ensures that the population
DTM will be large near the boundary of S0 and small everywhere else. As a result  incorrect labeling
of normal points will only occur around the boundary of S0 but not inside the bulk of the distribution
P0. We formalize this intuition in our next result  which is purely deterministic.
Proposition 3.6. Under assumptions (A0) and (A2)  suppose that a(x) = g(d(x  ∂S0))  where g(z)
is a non-decreasing function on [0  z0) for some z0  and g(z) ≥ g(z0) for all z ≥ z0. Let

(4)

(5)

(6)

η = min

x∈S0 y∈S1

(cid:107)x − y(cid:107)

 m

(cid:16) b+q
m ηq − h(cid:1)(cid:17)−b/q
(cid:0) m−ε
1−ε
1−ε (η − h)−b
m

b

7

be the distance between S0 and S1 and h > 0 be a given threshold parameter. For any m > ε 
additionally assume that

g(z0) ≥ g0 :=

1 ≤ q < ∞
q = ∞.

(7)

Next  deﬁne the "safety zone" Aη as

Aη =(cid:8)x ∈ S0 : d(x  ∂S0) ≥ g−1(g0)(cid:9)

Then  we have

sup
x∈Aη

dP m q(x) + h < inf
y∈S1

dP m q(y).

(8)

(9)

The main message from the previous result is that there exists a subset Aη of the support of the
normal distribution  which intuitively corresponds to a region deep inside the support of P0 of high
density  over which the population DTM will be smaller than at any point in the support S1 of the
contaminating distribution. Thus  the true DTM is guaranteed to perfectly separate Aη from S1 
making mistakes (possibly) only for the normal points in S0 \ Aη.
Notice that the deﬁnition of Aη depends on all the relevant quantities  namely the contamination
parameter ε  the probability parameter m  the dimension b of P0 and the order q of the DTM through
the expression (7). Importantly  it is necessary that m > ε  otherwise inequality (9) maybe not be
satisﬁed. For example  we can take P1 to have point mass at a single point y; then rP t(y) = 0 for all
t ≤ m  and the right hand side of (9) is zero.
When g(0) = a0 > 0  which occurs  e.g.  if P0 has a density bounded away from 0 over its support 
implies that Aη = S0 if

(cid:32)

(cid:32)

η >

m
m − ε

b

b + q

(cid:19)q/b

(cid:18) m

a0(1 − ε)

(cid:33)(cid:33)−1/q

+ h

.

That is  when S0 and S1 are sufﬁciently well-separated  the DTM will classify all the points in S0 as
normals.
The parameter h serves as a buffer that allows one to replace the DTM function d(x) with any
estimator that is close to it in the supremum norm by no more than h. Thus  we may plug-in the
high-probability bounds of Theorem 3.4 and Theorem 3.3 to conclude that the empirical DTM will
will identify all normal instances within Aη correctly  with high probability.
Corollary 3.6.1. Taking h to be twice the upper bound in (5)  we get  with probability at least 1 − δ 

Similarly  if h is twice the upper bound in (4)  we have that

max
Xi∈Aη

ˆdP m q(Xi) < min
Xi∈S1

ˆdP m q(Xi).

sup
x∈Aη

ˆdP m q(x) < inf
y∈S1

ˆdP m q(y).

(10)

The guarantee in (10) calls for a higher sample complexity that depends on the dimension d. At
the same time  it extends to all the points in Aη and not just the sample points. Thus the DTM can
accurately identify not only the normal instance in the sample but any other normal instance  such as
future observations.

3.4

Illustrative examples

We illustrate the separation condition in Proposition 3.6 with the following example. Consider a
collection of normal instances generated from a standard normal distribution. Figure 2 shows the
mis-classiﬁcation rates for DTM2 as a cluster of 5 anomalies approaches the normal instances for
three different underlying distributions: Gaussian  Moon-shaped  Circle. The color of each point
represents its class  with black being the normal instances and red being anomalies. The radius of the
circle around each point represents its empirical DTM score  and the color of the circle represents its
predicted class from DTM2. As we see  as the anomalies approach the normal instances  more and
more data around the boundaries of the normal distribution get mis-classiﬁed as anomalies.

4 Conclusions

In this paper we have presented empirical evidence  based on simulated and real-life benchmark
datasets  that NN-based methods show very good performance at identifying anomalous instances

8

High Separation

Medium Separation

Low Separation

Figure 2: Performance of DTM when the separation distance between the normal instances and
anomalies gradually decreases. Top: Gaussian; Middle: Moon-Shaped; Bottom: Circle.

in an unsupervised anomaly detection set-up. We have introduced a simple but natural framework
for anomaly detection based on the Huber contamination model and have used it to characterize the
performance of a class of NN methods for anomaly detection that are based on the distance-to-a-
measure (DTM) functional. In our results we rely on various geometric and analytic properties of
the underlying distribution to the accuracy of DTM-methods for anomaly detection. We are able to
demonstrate that  under mild conditions  NN methods will mis-classify normal points only around
the boundary of the support of the distribution generating normal instances and have quantiﬁed
this phenomenon rigorously. Finally  we have derived novel ﬁnite sample bounds on the nearest
neighbor radii and on the rate of convergence of the empirical DTM to the true DTM that may be of
independent interest.

References

[1] Alfred O. Hero. Geometric entropy minimization (gem) for anomaly detection and localization.
In B. Schölkopf  J. C. Platt  and T. Hoffman  editors  Advances in Neural Information Processing
Systems 19  pages 585–592. MIT Press  2007.

9

[2] Kumar Sricharan and Alfred O. Hero. Efﬁcient anomaly detection using bipartite k-nn graphs. In
J. Shawe-Taylor  R. S. Zemel  P. L. Bartlett  F. Pereira  and K. Q. Weinberger  editors  Advances
in Neural Information Processing Systems 24  pages 478–486. Curran Associates  Inc.  2011.

[3] Leying Guan and Rob Tibshirani. Prediction and outlier detection: a distribution-free prediction

set with a balanced objective. arXiv e-prints  page arXiv:1905.04396  May 2019.

[4] JooSeuk Kim and Clayton D. Scott. Robust kernel density estimation. J. Mach. Learn. Res. 

13(1):2529–2565  September 2012.

[5] Markus M. Breunig  Hans-Peter Kriegel  Raymond T. Ng  and Jörg Sander. Lof: Identifying

density-based local outliers. SIGMOD Rec.  29(2):93–104  May 2000.

[6] Fabrizio Angiulli and Clara Pizzuti. Fast outlier detection in high dimensional spaces. In
Principles of Data Mining and Knowledge Discovery  pages 15–27  Berlin  Heidelberg  2002.
Springer Berlin Heidelberg.

[7] Xiaojie Li  Jian Cheng Lv  and Dongdong Cheng. Angle-based outlier detection algorithm with
more stable relationships. In Proceedings of the 18th Asia Paciﬁc Symposium on Intelligent
and Evolutionary Systems  Volume 1  pages 433–446  Cham  2015. Springer International
Publishing.

[8] Bernhard Schölkopf  John C. Platt  John C. Shawe-Taylor  Alex J. Smola  and Robert C.
Williamson. Estimating the support of a high-dimensional distribution. Neural Comput. 
13(7):1443–1471  July 2001.

[9] David M.J. Tax and Robert P.W. Duin. Support vector data description. Machine Learning 

54(1):45–66  Jan 2004.

[10] Jinghui Chen  Saket Sathe  Charu C. Aggarwal  and Deepak S. Turaga. Outlier detection with

autoencoder ensembles. In SDM  2017.

[11] Fei Tony Liu  Kai Ming Ting  and Zhi-Hua Zhou. Isolation forest. In Proceedings of the 2008
Eighth IEEE International Conference on Data Mining  ICDM ’08  pages 413–422  Washington 
DC  USA  2008. IEEE Computer Society.

[12] Tomáš Pevný. Loda: Lightweight on-line detector of anomalies. Machine Learning  102(2):275–

304  Feb 2016.

[13] Mingxi Wu and Christopher Jermaine. Outlier detection by sampling with accuracy guarantees.

pages 767–772  2006.

[14] Mahito Sugiyama and Karsten Borgwardt. Rapid distance-based outlier detection via sampling.

pages 467–475  2013.

[15] T. R. Bandaragoda  K. M. Ting  D. Albrecht  F. T. Liu  and J. R. Wells. Efﬁcient anomaly

detection by isolation using nearest neighbour ensemble. pages 698–705  Dec 2014.

[16] G. Pang  K. M. Ting  and D. Albrecht. Lesinn: Detecting anomalies by identifying least similar

nearest neighbours. pages 623–630  Nov 2015.

[17] Kai Ming Ting  Takashi Washio  Jonathan R. Wells  and Sunil Aryal. Defying the gravity of
learning curve: a characteristic of nearest neighbour anomaly detectors. Machine Learning 
106(1):55–91  Jan 2017.

[18] Frédéric Chazal  David Cohen-Steiner  and Quentin Mérigot. Geometric inference for probabil-

ity measures. Foundations of Computational Mathematics  11(6):733–751  Dec 2011.

[19] Frédéric Chazal  Brittany Fasy  Fabrizio Lecci  Bertr  Michel  Aless  ro Rinaldo  and Larry
Wasserman. Robust topological inference: Distance to a measure and kernel distance. Journal
of Machine Learning Research  18(159):1–40  2018.

[20] Sridhar Ramaswamy  Rajeev Rastogi  and Kyuseok Shim. Efﬁcient algorithms for mining

outliers from large data sets. In SIGMOD Conference  2000.

10

[21] Markus Goldstein and Seiichi Uchida. A comparative evaluation of unsupervised anomaly

detection algorithms for multivariate data. PLOS ONE  11(4):1–31  04 2016.

[22] Filipe Falcão  Tommaso Zoppi  Caio Barbosa Viera Silva  Anderson Santos  Baldoino Fon-
seca  Andrea Ceccarelli  and Andrea Bondavalli. Quantitative comparison of unsupervised
anomaly detection algorithms for intrusion detection. In Proceedings of the 34th ACM/SIGAPP
Symposium on Applied Computing  SAC ’19  pages 318–327  New York  NY  USA  2019. ACM.

[23] Guilherme O. Campos  Arthur Zimek  Jörg Sander  Ricardo J. G. B. Campello  Barbora
Micenková  Erich Schubert  Ira Assent  and Michael E. Houle. On the evaluation of unsupervised
outlier detection: measures  datasets  and an empirical study. Data Mining and Knowledge
Discovery  30(4):891–927  Jul 2016.

[24] Andrew Emmott  Shubhomoy Das  Thomas Dietterich  Alan Fern  and Weng-Keen Wong. A
Meta-Analysis of the Anomaly Detection Problem. arXiv e-prints  page arXiv:1503.01158  Mar
2015.

[25] Shebuti Rayana. ODDS library. http://odds.cs.stonybrook.edu  2016.

[26] A. Frank and A. Asuncion. Uci machine learning repository. http://archive.ics.uci.

edu/ml  2010.

[27] Yizhen Wang  Somesh Jha  and Kamalika Chaudhuri. Analyzing the robustness of nearest
neighbors to adversarial examples. In Jennifer Dy and Andreas Krause  editors  Proceedings of
the 35th International Conference on Machine Learning  volume 80 of Proceedings of Machine
Learning Research  pages 5133–5142  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018.
PMLR.

[28] Guilherme O. Campos  Arthur Zimek  Jörg Sander  Ricardo J. G. B. Campello  Barbora
Micenková  Erich Schubert  Ira Assent  and Michael E. Houle. On the evaluation of unsupervised
outlier detection: measures  datasets  and an empirical study. Data Mining and Knowledge
Discovery  30(4):891–927  Jul 2016.

[29] Schubert E. Kriegel H.-P. Zimek  A. A survey on unsupervised outlier detection in high-

dimensional numerical data. Statistical Analysis and Data Mining  5:363–387  2012.

[30] Peter J. Huber. Robust Estimation of a Location Parameter  pages 492–518. Springer New

York  New York  NY  1992.

[31] Peter J. Huber. A robust version of the probability ratio test. Ann. Math. Statist.  36(6):1753–

1758  12 1965.

[32] Frédéric Chazal  Marc Glisse  Catherine Labruère  and Bertrand Michel. Convergence rates
for persistence diagram estimation in topological data analysis. Journal of Machine Learning
Research  16:3603–3635  2015.

[33] Antonio Cuevas and Ricardo Fraiman. A plug-in approach to support estimation. Ann. Statist. 

25(6):2300–2312  12 1997.

11

,Xiaoyi Gu
Leman Akoglu
Alessandro Rinaldo