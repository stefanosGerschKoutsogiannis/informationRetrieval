2019,A New Defense Against Adversarial Images: Turning a Weakness into a Strength,Natural images are virtually surrounded by low-density misclassified regions that can be efficiently discovered by gradient-guided search --- enabling the generation of adversarial images. While many techniques for detecting these attacks have been proposed  they are easily bypassed when the adversary has full knowledge of the detection mechanism and adapts the attack strategy accordingly. In this paper  we adopt a novel perspective and regard the omnipresence of adversarial perturbations as a strength rather than a weakness. We postulate that if an image has been tampered with  these adversarial directions either become harder to find with gradient methods or have substantially higher density than for natural images. We develop a practical test for this signature characteristic to successfully detect adversarial attacks  achieving unprecedented accuracy under the white-box setting where the adversary is given full knowledge of our detection mechanism.,A New Defense Against Adversarial Images:

Turning a Weakness into a Strength

Tao Yu∗†

Shengyuan Hu∗† Chuan Guo† Wei-Lun Chao‡ Kilian Q. Weinberger†

Abstract

Natural images are virtually surrounded by low-density misclassiﬁed regions that
can be efﬁciently discovered by gradient-guided search — enabling the generation
of adversarial images. While many techniques for detecting these attacks have
been proposed  they are easily bypassed when the adversary has full knowledge
of the detection mechanism and adapts the attack strategy accordingly. In this
paper  we adopt a novel perspective and regard the omnipresence of adversarial
perturbations as a strength rather than a weakness. We postulate that if an image
has been tampered with  these adversarial directions either become harder to ﬁnd
with gradient methods or have substantially higher density than for natural images.
We develop a practical test for this signature characteristic to successfully detect
adversarial attacks  achieving unprecedented accuracy under the white-box setting
where the adversary is given full knowledge of our detection mechanism.

1

Introduction

The advance of deep neural networks has led to natural questions regarding its robustness to both
natural and malicious change in the test input. For the latter scenario  the seminal work of Biggio et
al. [3] and Szegedy et al. [48] ﬁrst suggested that neural networks may be prone to imperceptible
changes in the input — the so-called adversarial perturbations — that alter the model’s decision
entirely. This weakness not only applies to image classiﬁcation models  but is prevalent in various
machine learning applications  including object detection and image segmentation [10  54]  speech
recognition [8]  and deep policy networks [2  21].
The threat of adversarial perturbations has prompted tremendous effort towards the development
of defense mechanisms. Common defenses either attempt to recover the true semantic labels of
the input [5  12  19  38  41  45] or detect and reject adversarial examples [17  28  31  33–35  55].
Although many of the proposed defenses have been successful against passive attackers — ones that
are unaware of the presence of the defense mechanism — almost all fail against adversaries that have
full knowledge of the internal details of the defense and modify the attack algorithm accordingly
[1  6]. To date  the success of existing defenses have been limited to simple datasets with relatively
low variety of classes [24  29  39  44  52].
Recent studies [13  42] have shown that the existence of adversarial perturbations may be an inherent
property of natural data distributions in high dimensional spaces — painting a grim picture for
defenses. However  in this paper we propose a radically new approach to defenses against adversarial
attacks that turns this seemingly insurmountable obstacle from a weakness into a strength: We use
the inherent property of the existence of valid adversarial perturbations around a natural image as a
signature to attest that it is unperturbed.

∗Equal Contribution. †Department of Computer Science  Cornell University. ‡Department of Computer
Science and Engineering  The Ohio State University. Email: {ty367  sh797  cg563  kqw4}@cornell.edu 
chao.209@osu.edu.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Concretely  we exploit two seemingly contradicting properties of natural images: On one hand 
natural images lie with high probability near the decision boundary to any given label [13  42]; on the
other hand  natural images are robust to random noise [48]  which means these small “pockets” of
spaces where the input is misclassiﬁed have low density and are unlikely to be found through random
perturbations. To verify if an image is benign  we can test for both properties effectively:
1. We measure the degree of robustness to random noise by observing the change in prediction after
adding i.i.d. Gaussian noise.
2. We measure the proximity to a decision boundary by observing the number of gradient steps
required to change the label of an input image. This procedure is identical to running a gradient-based
attack algorithm against the input (which is potentially an adversarial image already).
We hypothesize that artiﬁcially perturbed images mostly violate at least one of the two conditions.
This gives rise to an effective detection mechanism even when the adversary has full knowledge of
the defense. Against strong L∞-bounded white-box adversaries that adaptively optimize against
the detector  we achieve a worst-case detection rate of 49% at a false positive rate of 20% on
ImageNet [11] using a pre-trained ResNet-101 model [20]. Prior art achieves a detection rate of
0% at equal false positive rate under the same setting. Further analysis shows that there exists a
fundamental trade-off for white-box attackers when optimizing to satisfy the two detection criteria.
Our method creates new challenges for the search of adversarial examples and points to a promising
direction for future research in defense against white-box adversaries.

2 Background

Attack overview. Test-time attacks via adversarial examples can be broadly categorized into either
black-box or white-box settings. In the black-box setting  the adversary can only access the model
as an oracle  and may receive continuous-valued outputs or only discrete classiﬁcation decisions
[9  18  22  23  30  37  49–51]. We focus on the white-box setting in this paper  where the attacker
is assumed to be an insider and therefore has full knowledge of internal details of the network. In
particular  having access to the model parameters allows the attacker to perform powerful ﬁrst-order
optimization attacks by optimizing an adversarial loss function.
The white-box attack framework can be summarized as follows. Let h be the target classiﬁcation
model that  given any input x  outputs a vector of probabilities h(x) with h(x)y(cid:48) = p(y(cid:48)|x) (i.e.
the y(cid:48)-th component of the vector h(x)) for every class y(cid:48). Let y be the true class of x and L be a
continuous-valued adversarial loss that encourages misclassiﬁcation  e.g. 

L(h(x(cid:48))  y) = −cross-entropy(h(x(cid:48))  y).

Given a target image x for which the model correctly classiﬁes as arg maxy(cid:48) h(x)y(cid:48) = y  the attacker
aims to solve the following optimization problem:

min

x(cid:48) L(h(x(cid:48))  y)   s.t. (cid:107)x − x(cid:48)(cid:107) ≤ τ.

Here  (cid:107) · (cid:107) is a measure of perceptible difference and is commonly approximated using the Euclidean
norm (cid:107)·(cid:107)2 or the max-norm (cid:107)·(cid:107)
∞  and τ > 0 is a perceptibility threshold. This optimization problem
deﬁnes an untargeted attack  where the adversary’s goal is to cause misclassiﬁcation. In contrast  for
a targeted attack  the adversary is given some target label yt (cid:54)= y and deﬁnes the adversarial loss to
encourage classiﬁcation to the target label:

L(h(x(cid:48))  yt) = cross-entropy(h(x(cid:48))  yt).

(1)

For the remainder of this paper  we will focus on the targeted attack setting but any approach can be
readily augmented for untargeted attacks as well.
Optimization. White-box (targeted) attacks mainly differ in the choice of the adversarial loss
functions L and the optimization procedures. One of the earliest attacks [48] used L-BFGS to
optimize the cross-entropy adversarial loss in Equation 1. Carlini and Wagner [7] investigated the use
of different adversarial loss functions and found that the margin loss

(cid:20)

max
y(cid:48)(cid:54)=yt

(cid:21)
Z(x(cid:48))y(cid:48) − Z(x(cid:48))yt + κ

+

2

L(Z(x(cid:48))  yt) =

(2)

is more suitable for ﬁrst-order optimization methods  where Z is the logit vector predicted by the
model and κ > 0 is a chosen margin constant. This loss is optimized using Adam [25]  and the
resulting method is known as the Carlini-Wagner (CW) attack. Another class of attacks favors the
use of simple gradient descent using the sign of the gradient [16  27  32]  which results in improved
transferability of the constructed adversarial examples from one classiﬁcation model to another.
Enforcing perceptibility constraint. For common choices of the measures of perceptibility  the
attacker can either fold the constraint as a Lagrangian penalty into the adversarial loss  or apply a
projection step at the end of every iteration onto the feasible region. Since the Euclidean norm (cid:107) · (cid:107)2
is differentiable  it is commonly enforced with the former option  i.e. 

min

x(cid:48) L(h(x(cid:48))  yt) + c(cid:107)x − x(cid:48)(cid:107)2
for some choice of c > 0. On the other hand  the max-norm (cid:107) · (cid:107)
∞ is often enforced by restricting
every coordinate of the difference x − x(cid:48) to the range [−τ  τ ] after every gradient step. In addition 
since all pixel values must fall within the range [0  1]  most methods also project x(cid:48) to the unit cube at
the end of every iteration [7  32]. When using this option along with the cross entropy adversarial loss 
the resulting algorithm is commonly referred to as the Projected Gradient Descent (PGD) attack1 [1].

3 Detection Methods and Their Insufﬁciency

One commonly accepted explanation for the existence of adversarial examples is that they operate
outside the natural image manifold — regions of the space that the model had no exposure to during
training time and hence its behavior can be manipulated arbitrarily. This view casts the problem of
defending against adversarial examples as a robust classiﬁcation or anomaly detection problem. The
former aims to project the input back to the natural image manifold and recover its true label  whereas
the latter only requires determining whether the input belongs to the manifold and reject it if not.
Detection methods. Many principled detection algorithms have been proposed to date [17  28  31  33–
35  55]. The most common approach involves testing the input against one or several criteria that are
satisﬁed by natural images but are likely to fail for adversarially perturbed images. In what follows 
we brieﬂy describe two representative detection mechanisms.
Feature Squeezing [55] applies a semantic-preserving image transformation to the input and measures
the difference in the model’s prediction compared to the plain input. Transformations such as median
smoothing  bit quantization  and non-local mean do not alter the image content; hence the model
is expected to output similar predictions after applying these transformations. The method then
measures the maximum L1 change in predicted probability after applying these transformations and
ﬂags the input as adversarial if this change is above a chosen threshold.
Artifacts [14] uses the empirical density of the input and the model uncertainty to characterize benign
and adversarial images. The empirical density can be computed via kernel density estimation on
the feature vector. For the uncertainty estimate  the method evaluates the network multiple times
using different random dropout masks and computes the variance in the output. Under the Bayesian
interpretation of dropout  this variance estimate encodes the model’s uncertainty [15]. Adversarial
inputs are expected to have lower density and higher uncertainty than natural inputs. Thus  the method
predicts the input as adversarial if these criteria are below or above a chosen threshold.
Detectors that use multiple criteria (such as Feature Squeezing and Artifacts) can combine these
criteria into a single detection method by either declaring the input as adversarial if any criterion
fails to be satisﬁed  or by training a classiﬁer on top of them as features to classify the input. Other
notable useful features for detecting adversarial images include convolutional features extracted from
intermediate layers [28  34]  distance to training samples in pixel space [17  31]  and entropy of
non-maximal class probabilities [36].
Bypassing detection methods. While the approaches for detecting adversarial examples appear
principled in nature  the difference in settings from traditional anomaly detection renders most
techniques easy to bypass. In essence  a white-box adversary with knowledge of the features used for
detection can optimize the adversarial input to mimic these features with gradient descent. Any non-
differentiable component used in the detection algorithm  such as bit quantization and non-local mean 

1Some literature also refer to the iterative Fast Gradient Signed Method (FGSM) [16] as PGD [32].

3

can be approximated with the identity transformation on the backward pass [1]  and randomization
can be circumvented by minimizing the expected adversarial loss via Monte Carlo sampling [1].
These simple techniques have proven tremendously successful  bypassing almost all known detection
methods to date [6]. Given enough gradient queries  adversarial examples can be optimized to appear
even “more benign” than natural images.

4 Detection by Adversarial Perturbations

In this section we describe a novel approach to detect adversarial images that relies on two principled
criteria regarding the distribution of adversarial perturbations around natural images. In contrast to
the shortcomings of prior work  our approach is hard to fool through ﬁrst-order optimization.

4.1 Criterion 1: Low density of adversarial perturbations

The features extracted by convolutional neural networks (CNNs) from natural images are known to
be particularly robust to random input corruptions [19  48  53]. In other words  random perturbations
applied to natural images should not lead to changes in the predicted label (i.e. an adversarial image).
Our ﬁrst criterion follows this intuition and tests if the given input is robust to Gaussian noise:
C1: Robustness to random noise. Sample  ∼ N (0  σ2I) (where σ2 is a hyperparameter) and
compute ∆ = (cid:107)h(x) − h(x + )(cid:107)1. The input x is rejected as adversarial if ∆ is sufﬁciently large.
This style of reasoning has indeed been successfully applied to de-
fend against black-box and gray-box2 attacks [19  40  53]. Figure 1
shows a 2D cartoon depiction of the high dimensional decision
boundary near a natural image x. When the adversarial attack per-
turbs x slightly across the decision boundary from A to an incorrect
class B  the resulting adversarial image x(cid:48) can be easily randomly
perturbed to return to class A and will therefore fail criterion C1.
However  we emphasize that this criterion alone is insufﬁcient
against white-box adversaries and can be easily bypassed. In or-
der to make the adversarial image also robust against Gaussian noise 
the attacker can optimize the expected adversarial loss under this
defense strategy [1] through Monte Carlo sampling of noise vectors
during optimization. This effectively produces an adversarial image
x(cid:48)(cid:48) (see Figure 1) that is deep inside the decision boundary.
More precisely  for a natural image x with correctly predicted label y and target label yt  let h(x) be
the predicted class-probability vector. Let us deﬁne padv to be identical to h(x) in every dimension 
except for the correct class y and the target yt  where the two probabilities are swapped. Consequently 
dimension yt is the dominant prediction in padv. We redeﬁne the adversarial loss of the (targeted)
PGD attack to contain two terms:
L(cid:63) = L1 + L2 where: L1 = L(h(x(cid:48))  padv)
misclassify x(cid:48) as yt

(cid:125)
(cid:125)
  and L2 = E∼N (0 σ2I) [(cid:107)h(x(cid:48)) − h(x(cid:48) + )(cid:107)1]

Figure 1: Schematic illustra-
tion of the shape of adversarial
regions near a natural image x.

(cid:123)(cid:122)

 

(3)

(cid:124)

(cid:124)

(cid:123)(cid:122)

bypass C1

where L(· ·) denotes the cross-entropy loss. For the ﬁrst term  we deviate from standard attacks
by targeting the probability vector padv instead of the one-hot vector corresponding to label yt.
Optimizing against the one-hot vector would cause the adversarial example to over-saturate in
probability  which artiﬁcially increases the difference ∆ = (cid:107)h(x(cid:48)) − h(x(cid:48) + )(cid:107)1 and makes it easier
to detect using criterion C1.
We evaluate this white-box attack against criterion C1 using a pre-trained ResNet-101 [20] model on
ImageNet [11] as the classiﬁcation model. We sample 1 000 images from the ImageNet validation
set and optimize the adversarial loss L(cid:63) for each of them using Adam [25] with learning rate 0.005
for a maximum of 400 steps to construct the adversarial images.
Figure 2 (left) shows the effect of the number of gradient iterations on ∆ when optimizing the
adversarial loss L(cid:63). The center line shows median values of ∆ across 1 000 sample images  and
2In gray-box attacks  the adversary has full access to the classiﬁer h but is agnostic to the defense mechanism.

4

x<latexit sha1_base64="P/78WoKPqTPMAmrnJ7K8sbyWLsQ=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2m3bpZhN2X8QS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrG+D6jhUijeQoGS3yea0yiQvBOMr3O/88i1EbG6w0nC/YgOlQgFo2ilh15EcRSE2dO0X625dXcGsky8gtSgQLNf/eoNYpZGXCGT1Jiu5yboZ1SjYJJPK73U8ISyMR3yrqWKRtz42SzxlJxYZUDCWNunkMzU3xsZjYyZRIGdzBOaRS8X//O6KYaXfiZUkiJXbP5RmEqCMcnPJwOhOUM5sYQyLWxWwkZUU4a2pIotwVs8eZm0z+qeW/duz2uNq6KOMhzBMZyCBxfQgBtoQgsYKHiGV3hzjPPivDsf89GSU+wcwh84nz/9OZEc</latexit><latexit sha1_base64="P/78WoKPqTPMAmrnJ7K8sbyWLsQ=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2m3bpZhN2X8QS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrG+D6jhUijeQoGS3yea0yiQvBOMr3O/88i1EbG6w0nC/YgOlQgFo2ilh15EcRSE2dO0X625dXcGsky8gtSgQLNf/eoNYpZGXCGT1Jiu5yboZ1SjYJJPK73U8ISyMR3yrqWKRtz42SzxlJxYZUDCWNunkMzU3xsZjYyZRIGdzBOaRS8X//O6KYaXfiZUkiJXbP5RmEqCMcnPJwOhOUM5sYQyLWxWwkZUU4a2pIotwVs8eZm0z+qeW/duz2uNq6KOMhzBMZyCBxfQgBtoQgsYKHiGV3hzjPPivDsf89GSU+wcwh84nz/9OZEc</latexit><latexit sha1_base64="P/78WoKPqTPMAmrnJ7K8sbyWLsQ=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2m3bpZhN2X8QS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrG+D6jhUijeQoGS3yea0yiQvBOMr3O/88i1EbG6w0nC/YgOlQgFo2ilh15EcRSE2dO0X625dXcGsky8gtSgQLNf/eoNYpZGXCGT1Jiu5yboZ1SjYJJPK73U8ISyMR3yrqWKRtz42SzxlJxYZUDCWNunkMzU3xsZjYyZRIGdzBOaRS8X//O6KYaXfiZUkiJXbP5RmEqCMcnPJwOhOUM5sYQyLWxWwkZUU4a2pIotwVs8eZm0z+qeW/duz2uNq6KOMhzBMZyCBxfQgBtoQgsYKHiGV3hzjPPivDsf89GSU+wcwh84nz/9OZEc</latexit><latexit sha1_base64="P/78WoKPqTPMAmrnJ7K8sbyWLsQ=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbCu2oWy2m3bpZhN2X8QS+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrG+D6jhUijeQoGS3yea0yiQvBOMr3O/88i1EbG6w0nC/YgOlQgFo2ilh15EcRSE2dO0X625dXcGsky8gtSgQLNf/eoNYpZGXCGT1Jiu5yboZ1SjYJJPK73U8ISyMR3yrqWKRtz42SzxlJxYZUDCWNunkMzU3xsZjYyZRIGdzBOaRS8X//O6KYaXfiZUkiJXbP5RmEqCMcnPJwOhOUM5sYQyLWxWwkZUU4a2pIotwVs8eZm0z+qeW/duz2uNq6KOMhzBMZyCBxfQgBtoQgsYKHiGV3hzjPPivDsf89GSU+wcwh84nz/9OZEc</latexit>A<latexit sha1_base64="BAeVOBC5ObWqGCFk52KlP7hcwRg=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPVi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUuOmXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqt3kcRTiBUzgHD66gBvdQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBkt2MxQ==</latexit><latexit sha1_base64="BAeVOBC5ObWqGCFk52KlP7hcwRg=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPVi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUuOmXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqt3kcRTiBUzgHD66gBvdQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBkt2MxQ==</latexit><latexit sha1_base64="BAeVOBC5ObWqGCFk52KlP7hcwRg=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPVi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUuOmXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqt3kcRTiBUzgHD66gBvdQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBkt2MxQ==</latexit><latexit sha1_base64="BAeVOBC5ObWqGCFk52KlP7hcwRg=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPVi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4bua3n1BpHssHM0nQj+hQ8pAzaqzUuOmXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqt3kcRTiBUzgHD66gBvdQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBkt2MxQ==</latexit>B<latexit sha1_base64="omio59kvEVQ455XgB3RHdtTRWpI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOpF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipWR+UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasJbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXldq9TyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AlGGMxg==</latexit><latexit sha1_base64="omio59kvEVQ455XgB3RHdtTRWpI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOpF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipWR+UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasJbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXldq9TyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AlGGMxg==</latexit><latexit sha1_base64="omio59kvEVQ455XgB3RHdtTRWpI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOpF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipWR+UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasJbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXldq9TyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AlGGMxg==</latexit><latexit sha1_base64="omio59kvEVQ455XgB3RHdtTRWpI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOpF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipWR+UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasJbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXldq9TyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AlGGMxg==</latexit>C<latexit sha1_base64="/9k1Jv5/ArFaNoalXd7NEFzrfuc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOxF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA/KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryu1uzyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AleWMxw==</latexit><latexit sha1_base64="/9k1Jv5/ArFaNoalXd7NEFzrfuc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOxF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA/KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryu1uzyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AleWMxw==</latexit><latexit sha1_base64="/9k1Jv5/ArFaNoalXd7NEFzrfuc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOxF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA/KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryu1uzyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AleWMxw==</latexit><latexit sha1_base64="/9k1Jv5/ArFaNoalXd7NEFzrfuc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOxF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA/KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryu1uzyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AleWMxw==</latexit>D<latexit sha1_base64="NsLos3laW3WlkybC0/eUHSYSJS0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNRDx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHl2mMyA==</latexit><latexit sha1_base64="NsLos3laW3WlkybC0/eUHSYSJS0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNRDx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHl2mMyA==</latexit><latexit sha1_base64="NsLos3laW3WlkybC0/eUHSYSJS0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNRDx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHl2mMyA==</latexit><latexit sha1_base64="NsLos3laW3WlkybC0/eUHSYSJS0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNRDx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHl2mMyA==</latexit>x0<latexit sha1_base64="+3eeGQ1TBt6lQbnfbrNgekyp8t4=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyip5KIoMeiF48VbC20oWy2m3bpZhN2X8QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrHuBNRwKRRvoUDJO4nmNAokfwjGN7n/8Mi1EbG6x0nC/YgOlQgFo2ilbi+iOArC7Gl62q/W3Lo7A1kmXkFqUKDZr371BjFLI66QSWpM13MT9DOqUTDJp5VeanhC2ZgOeddSRSNu/GwWeUpOrDIgYaztU0hm6u+NjEbGTKLATuYRzaKXi/953RTDKz8TKkmRKzb/KEwlwZjk95OB0JyhnFhCmRY2K2EjqilD21LFluAtnrxM2ud1z617dxe1xnVRRxmO4BjOwINLaMAtNKEFDGJ4hld4c9B5cd6dj/loySl2DuEPnM8fYZiRTQ==</latexit><latexit sha1_base64="+3eeGQ1TBt6lQbnfbrNgekyp8t4=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyip5KIoMeiF48VbC20oWy2m3bpZhN2X8QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrHuBNRwKRRvoUDJO4nmNAokfwjGN7n/8Mi1EbG6x0nC/YgOlQgFo2ilbi+iOArC7Gl62q/W3Lo7A1kmXkFqUKDZr371BjFLI66QSWpM13MT9DOqUTDJp5VeanhC2ZgOeddSRSNu/GwWeUpOrDIgYaztU0hm6u+NjEbGTKLATuYRzaKXi/953RTDKz8TKkmRKzb/KEwlwZjk95OB0JyhnFhCmRY2K2EjqilD21LFluAtnrxM2ud1z617dxe1xnVRRxmO4BjOwINLaMAtNKEFDGJ4hld4c9B5cd6dj/loySl2DuEPnM8fYZiRTQ==</latexit><latexit sha1_base64="+3eeGQ1TBt6lQbnfbrNgekyp8t4=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyip5KIoMeiF48VbC20oWy2m3bpZhN2X8QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrHuBNRwKRRvoUDJO4nmNAokfwjGN7n/8Mi1EbG6x0nC/YgOlQgFo2ilbi+iOArC7Gl62q/W3Lo7A1kmXkFqUKDZr371BjFLI66QSWpM13MT9DOqUTDJp5VeanhC2ZgOeddSRSNu/GwWeUpOrDIgYaztU0hm6u+NjEbGTKLATuYRzaKXi/953RTDKz8TKkmRKzb/KEwlwZjk95OB0JyhnFhCmRY2K2EjqilD21LFluAtnrxM2ud1z617dxe1xnVRRxmO4BjOwINLaMAtNKEFDGJ4hld4c9B5cd6dj/loySl2DuEPnM8fYZiRTQ==</latexit><latexit sha1_base64="+3eeGQ1TBt6lQbnfbrNgekyp8t4=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyip5KIoMeiF48VbC20oWy2m3bpZhN2X8QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmSKQw6LrfTmlldW19o7xZ2dre2d2r7h+0TZxqxlsslrHuBNRwKRRvoUDJO4nmNAokfwjGN7n/8Mi1EbG6x0nC/YgOlQgFo2ilbi+iOArC7Gl62q/W3Lo7A1kmXkFqUKDZr371BjFLI66QSWpM13MT9DOqUTDJp5VeanhC2ZgOeddSRSNu/GwWeUpOrDIgYaztU0hm6u+NjEbGTKLATuYRzaKXi/953RTDKz8TKkmRKzb/KEwlwZjk95OB0JyhnFhCmRY2K2EjqilD21LFluAtnrxM2ud1z617dxe1xnVRRxmO4BjOwINLaMAtNKEFDGJ4hld4c9B5cd6dj/loySl2DuEPnM8fYZiRTQ==</latexit>x00<latexit sha1_base64="Bw813al9oqMgIDuTtvHSW91yC7w=">AAAB83icbVDLSgMxFL2pr1pfVZdugkXqqsyIoMuiG5cV7AM6Q8mkmTY0kxmSjFiG/oYbF4q49Wfc+Tdm2llo64HA4Zx7uScnSATXxnG+UWltfWNzq7xd2dnd2z+oHh51dJwqyto0FrHqBUQzwSVrG24E6yWKkSgQrBtMbnO/+8iU5rF8MNOE+REZSR5ySoyVPC8iZhyE2dOsXh9Ua07DmQOvErcgNSjQGlS/vGFM04hJQwXRuu86ifEzogyngs0qXqpZQuiEjFjfUkkipv1snnmGz6wyxGGs7JMGz9XfGxmJtJ5GgZ3MM+plLxf/8/qpCa/9jMskNUzSxaEwFdjEOC8AD7li1IipJYQqbrNiOiaKUGNrqtgS3OUvr5LORcN1Gu79Za15U9RRhhM4hXNw4QqacActaAOFBJ7hFd5Qil7QO/pYjJZQsXMMf4A+fwDGEZF+</latexit><latexit sha1_base64="Bw813al9oqMgIDuTtvHSW91yC7w=">AAAB83icbVDLSgMxFL2pr1pfVZdugkXqqsyIoMuiG5cV7AM6Q8mkmTY0kxmSjFiG/oYbF4q49Wfc+Tdm2llo64HA4Zx7uScnSATXxnG+UWltfWNzq7xd2dnd2z+oHh51dJwqyto0FrHqBUQzwSVrG24E6yWKkSgQrBtMbnO/+8iU5rF8MNOE+REZSR5ySoyVPC8iZhyE2dOsXh9Ua07DmQOvErcgNSjQGlS/vGFM04hJQwXRuu86ifEzogyngs0qXqpZQuiEjFjfUkkipv1snnmGz6wyxGGs7JMGz9XfGxmJtJ5GgZ3MM+plLxf/8/qpCa/9jMskNUzSxaEwFdjEOC8AD7li1IipJYQqbrNiOiaKUGNrqtgS3OUvr5LORcN1Gu79Za15U9RRhhM4hXNw4QqacActaAOFBJ7hFd5Qil7QO/pYjJZQsXMMf4A+fwDGEZF+</latexit><latexit sha1_base64="Bw813al9oqMgIDuTtvHSW91yC7w=">AAAB83icbVDLSgMxFL2pr1pfVZdugkXqqsyIoMuiG5cV7AM6Q8mkmTY0kxmSjFiG/oYbF4q49Wfc+Tdm2llo64HA4Zx7uScnSATXxnG+UWltfWNzq7xd2dnd2z+oHh51dJwqyto0FrHqBUQzwSVrG24E6yWKkSgQrBtMbnO/+8iU5rF8MNOE+REZSR5ySoyVPC8iZhyE2dOsXh9Ua07DmQOvErcgNSjQGlS/vGFM04hJQwXRuu86ifEzogyngs0qXqpZQuiEjFjfUkkipv1snnmGz6wyxGGs7JMGz9XfGxmJtJ5GgZ3MM+plLxf/8/qpCa/9jMskNUzSxaEwFdjEOC8AD7li1IipJYQqbrNiOiaKUGNrqtgS3OUvr5LORcN1Gu79Za15U9RRhhM4hXNw4QqacActaAOFBJ7hFd5Qil7QO/pYjJZQsXMMf4A+fwDGEZF+</latexit><latexit sha1_base64="Bw813al9oqMgIDuTtvHSW91yC7w=">AAAB83icbVDLSgMxFL2pr1pfVZdugkXqqsyIoMuiG5cV7AM6Q8mkmTY0kxmSjFiG/oYbF4q49Wfc+Tdm2llo64HA4Zx7uScnSATXxnG+UWltfWNzq7xd2dnd2z+oHh51dJwqyto0FrHqBUQzwSVrG24E6yWKkSgQrBtMbnO/+8iU5rF8MNOE+REZSR5ySoyVPC8iZhyE2dOsXh9Ua07DmQOvErcgNSjQGlS/vGFM04hJQwXRuu86ifEzogyngs0qXqpZQuiEjFjfUkkipv1snnmGz6wyxGGs7JMGz9XfGxmJtJ5GgZ3MM+plLxf/8/qpCa/9jMskNUzSxaEwFdjEOC8AD7li1IipJYQqbrNiOiaKUGNrqtgS3OUvr5LORcN1Gu79Za15U9RRhhM4hXNw4QqacActaAOFBJ7hFd5Qil7QO/pYjJZQsXMMf4A+fwDGEZF+</latexit>Figure 2: The variation in ∆ under Gaussian perturbations (C1; left plot) and numbers of steps Kt to
the decision boundary (C2t; right plot) for adversarial images constructed using different numbers of
gradient iterations. Gray-box attacks (orange) can be detected easily with criterion C1 alone (left
plot  the orange line is signiﬁcantly higher than the gray line). For white-box attacks (blue)  C1 alone
is not sufﬁcient (the blue line overlaps with the gray line) — however C2 (right plot) separates the
two lines reliably when C1 does not.

the error bars show the range of values between the 30th and 70th quantiles. When the attacker
is agnostic to the detector (orange line)  i.e.  only optimizing L1  ∆ does not decrease throughout
optimization and can be used to perfectly separate adversarial and real images (gray line). However 
in the white-box attack  the adversarial loss explicitly encourages ∆ to be small  and we observe that
indeed the blue line shows a downward trend as the adversary proceeds through gradient iterations.
As a result  the range of values for ∆ quickly begins to overlap with and fall below that of real images
after 100 steps  which shows that criterion C1 alone cannot be used to detect adversarial examples.

4.2 Criterion 2: Close proximity to decision boundary

The intuitive reason why the attack strategy described above in section 4.1 can successfully fool
criterion C1 is that it effectively pushes the adversarial image far into the decision boundary of the
target class (e.g. x(cid:48)(cid:48) in Figure 1) — an unlikely position for a natural image  which tends to be close
to adversarial decision boundaries. Indeed  Fawzi et al. [13] and Shafahi et al. [42] have shown that
adversarial examples are inevitable in high-dimensional spaces. Their theoretical arguments suggest
that  due to the curse of dimensionality  a sample from the natural image distribution is close to the
decision boundary of any classiﬁer with high probability. Hence  we deﬁne a second criterion to test
if an image is close to the decision boundary of an incorrect class:
C2(t/u): Susceptibility to adversarial noise. For a chosen ﬁrst-order iterative attack algorithm A 
evaluate A on the input x and record the minimum number of steps K required to adversarially
perturb x. The input is rejected as adversarial if K is sufﬁciently large.
Criterion C2 can be further specialized to targeted attacks (C2t) and untargeted attacks (C2u)  which
measures the proximity (i.e. number of gradient steps) to either a chosen target class or to an arbitrary
but different class. We denote these quantities as Kt and Ku  respectively. In this paper we choose A
in C2 to be the targeted/untargeted PGD attack  but our framework can plausibly generalize to any
ﬁrst-order attack algorithm. Figure 2 (right) shows the effect of optimizing the adversarial loss L(cid:63)
on Kt. Again  the center line shows the median value of Kt across 1 000 images and the error bars
indicate the 30th and 70th quantiles. As expected  real images (gray line) require very few steps to
reach the decision boundary of any random target class. When the adversary does not seek to bypass
criterion C1 (orange line)  the constructed adversarial images lie very close to the decision boundary
and are indistinguishable from real images with C2 alone (however here C1 is already sufﬁcient).
On the other hand  when the attacker minimizes ∆ to fool criterion C1  the adversarial image moves
away from the decision boundary in order to be robust to random Gaussian noise. This results in
an increase in the number of steps Kt to reach the decision boundary of a random target class. At
400 steps  there is almost no overlap between the 30-70th quantiles of values of Kt for real and
adversarial images. This separation begins almost precisely as the value of ∆ for adversarial images
(left plot) begins to overlap with that of natural images at 100 steps. Thus  C2t becomes an effective
criterion to detect adversarial images that optimize against C1.

5

0100200300400steps0.00.51.01.52.0L1 norm ()0100200300400steps5101520253035# of steps to decision boundary (Kt)gray-boxwhite-boxreal4.3 Detection strategy

The fact that natural images can simultaneously satisfy criteria C1 and C2 can be regarded as almost
paradoxical: While the minimum distance from a natural input to the decision boundary of any
incorrect class is small  the density of directions that can lead to a decision boundary within a short
distance is also very low. We postulate that this behavior of natural images is difﬁcult to imitate even
for an adaptive  white-box adversary.
Our detection strategy using the two criteria can be summarized as follows. Given an input x (which
might be an adversarial example already)  we compute (∆  Kt  Ku) and compare these quantities
to chosen thresholds (tC1  tC2t  tC2u)  corresponding to criteria C1  C2t  and C2u. We reject x as an
adversarial example if at least one of the three (sub-)criteria is not satisﬁed  i.e.  if any measurement
is larger than the corresponding threshold. Details on hyperparameter selection can be found in the
Supplementary Material.
Best effort white-box adversary. Based on our proposed detection method  we deﬁne a white-box
adversary that aims to cause misclassiﬁcation while passing the detection criteria C1 and C2. Let
L be the adversarial loss for the defense-agnostic (targeted) attack (e.g. Equation 1). We deﬁne
loss functions L1 and L2 as in Equation 3 following the same strategy used in section 4.1 to bypass
C1. Since the criterion C2t is discrete  it is difﬁcult to optimize directly. Instead  we encourage the
constructed adversarial image to change prediction to any class y(cid:48) (cid:54)= yt after a single gradient step
towards y(cid:48). As natural images require very few gradient steps to cross the decision boundary  the
resulting adversarial image will appear real to criterion C2t. Let
A(h(x(cid:48))  y(cid:48))

denote the gradient of the cross-entropy loss w.r.t. x(cid:48)3. The loss term to bypass C2t can be deﬁned as

δy(cid:48) = ∇x(cid:48)L

L3 = Ey(cid:48)∼Uniform y(cid:48)(cid:54)=yt[L(h(x(cid:48) − αδy(cid:48))  y(cid:48))] 

which encourages x(cid:48) − αδy(cid:48) — the one-step move towards class y(cid:48) at step size α — to be close to or
cross the decision boundary of class y(cid:48) for every randomly chosen class y(cid:48) (cid:54)= yt. Similarly  to bypass
criterion C2u  we simulate one gradient step at step size α away from the target class yt (which the
defender perceives as the predicted class) as x(cid:48) + αδyt. We then encourage this resulting image to be
classiﬁed as not yt via the loss term:

L4 = −L(h(x(cid:48) + αδyt)  yt).

Gradients for L3 and L4 can be approximated using Backward Pass Differentiable Approximation
(BPDA) [1]. As a result of optimizing L3 and L4  the produced image x(cid:48) will admit both a targeted
and an untargeted “adversarial example” within one or few steps of the attack algorithm A  therefore
bypassing C2. Combining all the components  the modiﬁed adversarial loss L(cid:63) for white-box attack
against our detector becomes
(4)
The inclusion of additional loss terms hinders the optimality of L1 and may cause the attack to fail to
generate a valid adversarial example. Thus  we include the coefﬁcient λ so that L1 dominates the
other loss terms and guarantees close to 100% success rate in constructing adversarial examples to
fool h. We optimize the total loss L(cid:63) using Adam [25].

L(cid:63) = λL1 + L2 + L3 + L4.

5 Experiments

We test our detection mechanism against the white-box attack deﬁned in section 4.3 in several
different settings  and release our code publicly for reproducibility4.

5.1 Setup

Datasets and target models. We conduct our empirical studies on ImageNet [11] and CIFAR-10 [26].
We sample 1 000 images from ImageNet (validation) and CIFAR-10 (test): each class has 1 or 100
images. We use the pre-trained ResNet-101 model [20] in PyTorch for ImageNet and train a VGG-19
model [43] with a dropout rate of 0.5 [46] for CIFAR-10 as target models. We additionally include
detection results using an Inception-v3 model [47] on ImageNet in the Supplementary Material.

3We denote the adversarial loss of the Algo. A in our detector by LA to differentiate it from L of the attacker.
4https://github.com/s-huu/TurningWeaknessIntoStrength

6

Table 1: Detection rates of different detection algorithms against white-box adversaries on ImageNet.
Worst-case performance against all evaluated attacks is underlined for each detector.

Detector
Feature Squeezing
Feature Squeezing

FPR
0.2
0.1

PGD
0.003
0.002

CW
0.000
0.000

C1
C2t
C2u
Combined
C1
C2t
C2u
Combined

0.2
0.2
0.2
0.2
0.1
0.1
0.1
0.1

LR=0.01 LR=0.03 LR=0.1 LR=0.01 LR=0.03 LR=0.1
0.068
0.882
0.039
0.809
0.021
0.806
0.003
0.601

0.585
0.205
0.001
0.494
0.320
0.120
0.000
0.269

0.103
0.800
0.042
0.718
0.044
0.709
0.010
0.482

0.132
0.649
0.001
0.490
0.043
0.483
0.000
0.264

0.066
0.724
0.002
0.612
0.013
0.616
0.000
0.378

0.682
0.436
0.154
0.688
0.486
0.287
0.062
0.512

Figure 3: A sample clean (left) and ad-
versarial (right) image at L∞ percepti-
bility threshold of τ = 0.1.

Attack algorithms. We evaluate our detection method
against the white-box adversary deﬁned in section 4.3.
Since the adversary may vary in the choice of the surrogate
loss (cf. L in Equation 3)  we experiment using both
targeted and untargeted variants of two representative loss
functions: the margin loss deﬁned in the Carlini-Wagner
(CW) attack [7] (see Equation 2)  and the cross-entropy
loss used in the Projected Gradient Descent (PGD) attack
[1]. The L∞-bound for all attacks is set to τ = 0.1  which
is very strong and often produces images with noticeable
visual distortion. See Figure 3 for an illustration. We
further experiment with boundary attack [4] for attacking
the target model and detection mechanism as a black box in the Supplementary Material.
All attacks optimize the adversarial loss using Adam [25]. We set λ = 2 (cf. Equation 4) for ImageNet
and λ = 3 for CIFAR-10 to guarantee close to 100% attack success rate. We found that changing the
maximum number of iterations has little effect on the attack’s ability to bypass our detector  and thus
we ﬁx to a reasonable value of 50 steps for ImageNet (which is sufﬁcient to guarantee convergence;
see Figure 4) and 200 steps for CIFAR-10. The learning rate has a more noticeable effect and we
evaluate our detector against different chosen values. See the Supplementary Material for detection
results against variants of these attacks  including untargeted attacks and τ = 0.03.
Baselines. We compare our detector against two strategies: Feature Squeezing [55] and Artifacts
[14]. These detection algorithms are the most similar to ours — using a combination of different
criteria as features for the detector. We modify the Artifacts defense slightly to use the density and
uncertainty estimates directly by thresholding rather than training a classiﬁer on top of these features 
which has been shown in prior work [6] to remain effective against adversaries that are agnostic to the
defense. With a false positive rates (FPR) of 0.1  Feature Squeezing attains a detection rate of 0.737
on ImageNet and 0.892 on CIFAR-10  while Artifacts attains a detection rate of 0.587 on CIFAR-10.
We adopt the same strategy as in section 4.3 to formulate white-box attacks against these detectors 
adding a term in the adversarial loss for each criterion and using Backward Pass Differentiable
Approximation (BPDA) to compute the gradient of non-differentiable transformations [1]. Details on
these modiﬁcations can be found in the Supplementary Material.

5.2 Detection results

ImageNet results. Table 1 shows the detection rate of our method against various adversaries
on ImageNet. We evaluate our detector under two different settings  resulting in FPR of 0.1 and
0.2. Entries in the table correspond to the detection rate (or true positive rate) when the white-box
adversary deﬁned in section 4.3 is applied to attack the model along with the detector.
Under all six attack settings (PGD vs. CW  LR = 0.01  0.03  0.1)  our detector performs substantially
better than random  achieving a worst-case detection rate of 0.49 at FPR = 0.2 and 0.264 at FPR =

7

Table 2: Detection rates of different detection algorithms against white-box adversaries on CIFAR-10.
Worst-case performance against all evaluated attacks is underlined for each detector.

Detector
Feature Squeezing
Feature Squeezing
Artifacts
Artifacts

FPR
0.2
0.1
0.2
0.1

PGD
0.074
0.008
0.108
0.090

CW
0.096
0.021
0.018
0.009

C1
C2t
C2u
Combined
C1
C2t
C2u
Combined

0.2
0.2
0.2
0.2
0.1
0.1
0.1
0.1

LR=0.001 LR=0.01 LR=0.1 LR=0.001 LR=0.01 LR=0.1
0.012
0.971
0.000
0.740
0.007
0.858
0.000
0.568

0.033
0.786
0.000
0.481
0.016
0.581
0.000
0.356

0.792
0.346
0.000
0.660
0.207
0.180
0.000
0.187

0.991
0.050
0.000
0.984
0.953
0.015
0.000
0.909

0.422
0.098
0.000
0.374
0.283
0.026
0.000
0.263

1.000
0.024
0.000
0.998
0.986
0.010
0.000
0.966

0.1 on ImageNet. This result is a considerable improvement over similar detection methods such as
Feature Squeezing  where the detection rate is close to 0  i.e. the adversarial images appear “more
real” than natural images. We emphasize that given the strong adversary that we evaluate against
(τ = 0.1)  these detection rates are very difﬁcult to attain against white-box attacks.
Ablation study. We further decompose the components of our detector to demonstrate the trade-offs
the adversary must make when attacking our detector. When using different learning rates  the
adversary switches between attempting to fool criteria C1 and C2. For example  at LR = 0.01  the
PGD adversary can be detected using criterion C1 substantially better than using criterion C2t due
to under-optimization of the value ∆. On the other hand  at LR = 0.1  the adversary succeeds in
bypassing criterion C1 at the cost of failing C2t. The criterion C2u does not appear to be effective
here as it consistently achieves a detection rate of close to 0. However  it is a crucial component of
our method against untargeted attacks (see Supplementary Material). Overall  our combined detector
achieves the best worst-case detection rate across all attack scenarios.
CIFAR-10 results. The detection rates for our method are slightly worse on CIFAR-10 (Table 2)
but still outperforming the Feature Squeezing and Artifacts baselines  which are close to 0 in the
worst case. For this dataset  criterion C2u becomes ineffective due to the over-saturation of predicted
probabilities for clean images  causing untargeted perturbation to take excessively many steps.
Furthermore  the CIFAR-10 dataset violates both of our hypotheses regarding the distribution of
adversarial perturbations near a natural image. Models trained on CIFAR-10 are much less robust
to random Gaussian noise due to lack of data augmentation and poor diversity of training samples
— the VGG-19 model could only tolerate a Gaussian noise of σ = 0.01 as opposed to σ = 0.1 for
ResNet-101 on ImageNet. Furthermore  CIFAR-10 is much lower-dimensional than ImageNet  hence
natural images are comparatively farther from the decision boundary [13  42]. Given this observation 
we suggest that our detector be used only in situations where these two assumptions can be satisﬁed.
Gray-box detection results. Despite
the fact that our detection mechanism
is formulated against white-box adver-
saries  we evaluated against a gray-
box adversary with knowledge of the
underlying model but not of the detec-
tor for completeness.
Table 3 shows detection rates for gray-
box attacks at FPR of 0.05 and 0.1
on ImageNet. At perceptibility bound
τ = 0.03  the combined detector is
very successful at detecting the gener-
ated adversarial images  achieving a detection rate of 97.6% at 5% FPR. In comparison  Feature
Squeezing could only achieve a detection rate of 30.4% against the CW attack. Against the much

Table 3: Detection rates for variations of the gray-box ad-
versary on ImageNet. Worst-case performance against all
evaluated attacks is underlined for each detector.

CW PGD
0.572
0.304
0.672
0.336
0.896
0.981
0.989
0.915

Feature Squeezing
Feature Squeezing
Ours: Combined
Ours: Combined

PGD
0.669
0.758
0.976
0.990

CW
0.014
0.020
0.570
0.678

Detector

FPR

0.05
0.1
0.05
0.1

τ = 0.03

τ = 0.1

8

stronger adversary at τ = 0.1  both detectors perform signiﬁcantly worse  but our combined detector
still achieves a non-trivial detection rate.

5.3 Adversarial loss curves

To further substantiate our claim that the criteria
C1 and C2t/u are mutually exclusive  we plot the
value of different components of the adversarial
loss L∗ throughout optimization for the white-
box attack (PGD) on ImageNet. The center lines
in Figure 4 show the average loss for each Li
over 1 000 images and the shaded areas indicate
standard deviation. Since the primary goal is
to cause misclassiﬁcation  the term L1 (blue
line) shows steady descending trend throughout
optimization and its value has stabilized after 50
iterations. L2 (orange line) begins at a low value
due to the initialization being a natural image
(and hence it is robust against Gaussian noise) 
and after 50 iterations it returns back to the initial level  which shows that the adversary is successful
at bypassing criterion C1. However  this success comes at the cost of L3 (red line) failing to reduce
to a sufﬁciently low level due to inherent conﬂict with L2 (and L1)  hence criterion C2t can be used
to detect the resulting adversarial image.

Figure 4: Plot of different components of the ad-
versarial loss L∗. See text for details.

5.4 Detection times

Table 4: Running time of different components of
our detection algorithm on ImageNet and CIFAR-
10. See text for details.

One drawback of our method is its (relatively)
high computation cost. Criteria C2t/u require
executing a gradient-based attack until either la-
bel change or for a speciﬁed number of steps.
To limit the number of false positives  the up-
per threshold on the number of gradient steps
must be sufﬁciently high  dominating the run-
ning time of the detection algorithm. Table 4
shows the average per-image detection time for
both real and (targeted) adversarial images on
ImageNet and CIFAR-10. On both datasets  the
average detection time for real images is approx-
imately 5 seconds and is largely due to a large threshold for C2u. The situation is similar for
adversarial images: As the CW attack optimizes the margin loss  taking the adversarial images much
farther into the decision boundary  it takes longer (many more steps to undo via C2t/u) to detect it.

CW
0.107s
3.46s
0.241s
0.012s
0.27s
9.631s

Real
0.074s
0.403s
4.512s
0.011s
0.379s
5.230s

PGD
0.091s
1.057s
0.138s
0.013s
0.128s
0.055s

ImageNet

CIFAR-10

C1
C2t
C2u
C1
C2t
C2u

6 Conclusion

We have shown that our detection method achieves substantially improved resistance to white-box
adversaries compared to prior work. In contrast to other detection algorithms that combine multiple
criteria  the criteria used in our method are mutually exclusive — optimizing one will negatively
affect the other — yet are inherently true for natural images. While we do not suggest that our method
is impervious to white-box attacks  it does present a signiﬁcant hurdle to overcome and raises the bar
for any potential adversary.
There are  however  some limitations to our method. The running time of our detector is dominated
by testing criterion C2  which involves running an iterative gradient-based attack algorithm. The high
computation cost could prohibit the suitability of our detector for deployment. Furthermore  it is
fair to say that the false positive rate remains relatively high due to a large variance in the statistics
∆  Kt and Ku for the different criteria  hence a threshold-based test cannot completely separate
real and adversarial inputs. Future research that improve in either front can certainly ameliorate the
performance of our method to be more practical in real world systems.

9

01020304050steps201001020Loss1234Acknowledgments

C.G.  W-L.C.  K.Q.W. are supported by grants from the NSF (III-1618134  III-1526012  IIS-1149882 
IIS-1724282  and TRIPODS-1740822)  the Bill and Melinda Gates Foundation  and the Cornell
Center for Materials Research with funding from the NSF MRSEC program (DMR-1719875); and
are also supported by Zillow  SAP America Inc.  and Facebook. We thank Pin-Yu Chen (IBM) for
constructive discussions.

References

[1] A. Athalye  N. Carlini  and D. A. Wagner. Obfuscated gradients give a false sense of security:

Circumventing defenses to adversarial examples. CoRR  abs/1802.00420  2018. 1  3  4  6  7

[2] V. Behzadan and A. Munir. Vulnerability of deep reinforcement learning to policy induction

attacks. CoRR  abs/1701.04143  2017. 1

[3] B. Biggio  I. Corona  D. Maiorca  B. Nelson  N. Šrndi´c  P. Laskov  G. Giacinto  and F. Roli.
Evasion attacks against machine learning at test time. In Proc. ECML  pages 387–402  2013. 1
[4] W. Brendel  J. Rauber  and M. Bethge. Decision-based adversarial attacks: Reliable attacks

against black-box machine learning models. CoRR  abs/1712.04248  2017. 7

[5] J. Buckman  A. Roy  C. Raffel  and I. J. Goodfellow. Thermometer encoding: One hot way
to resist adversarial examples. In 6th International Conference on Learning Representations 
ICLR 2018  Vancouver  BC  Canada  April 30 - May 3  2018  Conference Track Proceedings 
2018. 1

[6] N. Carlini and D. Wagner. Adversarial Examples Are Not Easily Detected: Bypassing Ten
Detection Methods. the 10th ACM Workshop on Artiﬁcial Intelligence and Security  2017. 1  4 
7

[7] N. Carlini and D. Wagner. Towards Evaluating the Robustness of Neural Networks. IEEE

Symposium on Security and Privacy  2017. 2  3  7

[8] N. Carlini and D. A. Wagner. Audio adversarial examples: Targeted attacks on speech-to-text.

CoRR  abs/1801.01944  2018. 1

[9] P. Chen  H. Zhang  Y. Sharma  J. Yi  and C. Hsieh. ZOO: zeroth order optimization based
black-box attacks to deep neural networks without training substitute models. In Proceedings of
the 10th ACM Workshop on Artiﬁcial Intelligence and Security  AISec@CCS 2017  Dallas  TX 
USA  November 3  2017  pages 15–26  2017. 2

[10] M. Cisse  Y. Adi  N. Neverova  and J. Keshet. Houdini: Fooling deep structured prediction

models. CoRR  abs/1707.05373  2017. 1

[11] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical

image database. In Proc. CVPR  pages 248–255. IEEE  2009. 2  4  6

[12] G. S. Dhillon  K. Azizzadenesheli  Z. C. Lipton  J. Bernstein  J. Kossaiﬁ  A. Khanna  and
A. Anandkumar. Stochastic activation pruning for robust adversarial defense. In 6th Interna-
tional Conference on Learning Representations  ICLR 2018  Vancouver  BC  Canada  April 30 -
May 3  2018  Conference Track Proceedings  2018. 1

[13] A. Fawzi  H. Fawzi  and O. Fawzi. Adversarial vulnerability for any classiﬁer. In Advances
in Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal  Canada.  pages
1186–1195  2018. 1  2  5  8

[14] R. Feinman  R. R. Curtin  S. Shintre  and A. B. Gardner. Detecting Adversarial Samples from

Artifacts. ArXiv e-prints  2017. 3  7

[15] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncer-
tainty in deep learning. In international conference on machine learning  pages 1050–1059 
2016. 3

[16] I. J. Goodfellow  J. Shlens  and C. Szegedy. Explaining and Harnessing Adversarial Examples.

International Conference on Learning Representation (ICLR)  2015. 3

[17] K. Grosse  P. Manoharan  N. Papernot  M. Backes  and P. McDaniel. On the (Statistical)

Detection of Adversarial Examples. arXiv e-prints  2017. 1  3

[18] C. Guo  J. R. Gardner  Y. You  A. G. Wilson  and K. Q. Weinberger. Simple black-box

adversarial attacks. CoRR  abs/1905.07121  2019. 2

[19] C. Guo  M. Rana  M. Cisse  and L. van der Maaten. Countering Adversarial Images using Input

Transformations. International Conference on Learning Representation (ICLR)  2018. 1  4

[20] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR 

pages 770–778. IEEE Computer Society  2016. 2  4  6

10

[21] S. Huang  N. Papernot  I. Goodfellow  Y. Duan  and P. Abbeel. Adversarial attacks on neural

network policies. CoRR  abs/1702.02284  2017. 1

[22] A. Ilyas  L. Engstrom  A. Athalye  and J. Lin. Black-box adversarial attacks with limited queries
and information. In Proceedings of the 35th International Conference on Machine Learning 
ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  pages 2142–2151  2018.
2

[23] A. Ilyas  L. Engstrom  and A. Madry. Prior convictions: Black-box adversarial attacks with

bandits and priors. CoRR  abs/1807.07978  2018. 2

[24] H. Kannan  A. Kurakin  and I. Goodfellow. Adversarial Logit Pairing. ArXiv e-prints  2018. 1
[25] D. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR  abs/1412.6980 

[26] A. Krizhevsky  G. Hinton  et al. Learning multiple layers of features from tiny images. Technical

2014. 3  4  6  7

report  Citeseer  2009. 6

[27] A. Kurakin  I. Goodfellow  and S. Bengio. Adversarial Machine Learning at Scale. International

Conference on Learning Representation (ICLR)  2017. 3

[28] X. Li and F. Li. Adversarial examples detection in deep networks with convolutional ﬁlter
statistics. In IEEE International Conference on Computer Vision  ICCV 2017  Venice  Italy 
October 22-29  2017  pages 5775–5783  2017. 1  3

[29] X. Liu  M. Cheng  H. Zhang  and C. Hsieh. Towards robust neural networks via random self-
ensemble. In Computer Vision - ECCV 2018 - 15th European Conference  Munich  Germany 
September 8-14  2018  Proceedings  Part VII  pages 381–397  2018. 1

[30] Y. Liu  X. Chen  C. Liu  and D. Song. Delving into transferable adversarial examples and

black-box attacks. CoRR  abs/1611.02770  2016. 2

[31] X. Ma  B. Li  Y. Wang  S. M. Erfani  S. N. R. Wijewickrema  G. Schoenebeck  D. Song  M. E.
Houle  and J. Bailey. Characterizing adversarial subspaces using local intrinsic dimensionality.
In 6th International Conference on Learning Representations  ICLR 2018  Vancouver  BC 
Canada  April 30 - May 3  2018  Conference Track Proceedings  2018. 1  3

[32] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards Deep Learning Models
Resistant to Adversarial Attacks. International Conference on Learning Representation (ICLR) 
2018. 3

[33] D. Meng and H. Chen. Magnet: A two-pronged defense against adversarial examples. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security 
CCS 2017  Dallas  TX  USA  October 30 - November 03  2017  pages 135–147  2017. 1  3

[34] J. H. Metzen  T. Genewein  V. Fischer  and B. Bischoff. On detecting adversarial perturbations.
In 5th International Conference on Learning Representations  ICLR 2017  Toulon  France  April
24-26  2017  Conference Track Proceedings  2017. 3

[35] A. Nitin Bhagoji  D. Cullina  C. Sitawarin  and P. Mittal. Enhancing Robustness of Machine
Learning Systems via Data Transformations. 52nd Annual Conference on Information Sciences
and Systems (CISS)  2018. 1  3

[36] T. Pang  C. Du  Y. Dong  and J. Zhu. Towards robust detection of adversarial examples. In

Advances in Neural Information Processing Systems  pages 4579–4589  2018. 3

[37] N. Papernot  P. D. McDaniel  I. J. Goodfellow  S. Jha  Z. B. Celik  and A. Swami. Practical black-
box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on
Computer and Communications Security  AsiaCCS 2017  Abu Dhabi  United Arab Emirates 
April 2-6  2017  pages 506–519  2017. 2

[38] A. Prakash  N. Moran  S. Garber  A. DiLillo  and J. A. Storer. Deﬂecting adversarial attacks
with pixel deﬂection. In 2018 IEEE Conference on Computer Vision and Pattern Recognition 
CVPR 2018  Salt Lake City  UT  USA  June 18-22  2018  pages 8571–8580  2018. 1

[39] A. Raghunathan  J. Steinhardt  and P. Liang. Certiﬁed Defenses against Adversarial Examples.

International Conference on Learning Representation (ICLR)  2018. 1

[40] K. Roth  Y. Kilcher  and T. Hofmann. The odds are odd: A statistical test for detecting adversarial
examples. In Proceedings of the 36th International Conference on Machine Learning (ICML) 
2019. 4

[41] P. Samangouei  M. Kabkab  and R. Chellappa. Defense-gan: Protecting classiﬁers against

adversarial attacks using generative models. CoRR  abs/1805.06605  2018. 1

[42] A. Shafahi  W. R. Huang  C. Studer  S. Feizi  and T. Goldstein. Are adversarial examples

inevitable? CoRR  abs/1809.02104  2018. 1  2  5  8

[43] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations  ICLR 2015  San
Diego  CA  USA  May 7-9  2015  Conference Track Proceedings  2015. 6

11

ture for computer vision. CoRR  abs/1512.00567  2015. 6

[48] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus.
International Conference on Machine Learning

Intriguing properties of neural networks.
(ICML)  2014. 1  2  4

[49] F. Tramèr  A. Kurakin  N. Papernot  D. Boneh  and P. D. McDaniel. Ensemble adversarial

[44] A. Sinha  H. Namkoong  and J. C. Duchi. Certifying some distributional robustness with
principled adversarial training. In 6th International Conference on Learning Representations 
ICLR 2018  Vancouver  BC  Canada  April 30 - May 3  2018  Conference Track Proceedings 
2018. 1

[45] Y. Song  T. Kim  S. Nowozin  S. Ermon  and N. Kushman. Pixeldefend: Leveraging generative
models to understand and defend against adversarial examples. In 6th International Conference
on Learning Representations  ICLR 2018  Vancouver  BC  Canada  April 30 - May 3  2018 
Conference Track Proceedings  2018. 1

[46] N. Srivastava  G. E. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: a
simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research 
15:1929–1958  2014. 6

[47] C. Szegedy  V. Vanhoucke  S. Ioffe  J. Shlens  and Z. Wojna. Rethinking the inception architec-

training: Attacks and defenses. CoRR  abs/1705.07204  2017. 2

[50] C. Tu  P. Ting  P. Chen  S. Liu  H. Zhang  J. Yi  C. Hsieh  and S. Cheng. Autozoom: Autoencoder-
based zeroth order optimization method for attacking black-box neural networks. CoRR 
abs/1805.11770  2018.

[51] J. Uesato  B. O’Donoghue  P. Kohli  and A. van den Oord. Adversarial risk and the dangers
of evaluating against weak attacks. In Proceedings of the 35th International Conference on
Machine Learning  ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018 
pages 5032–5041  2018. 2

[52] E. Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex

outer adversarial polytope. International Conference on Machine Learning (ICML)  2017. 1

[53] C. Xie  J. Wang  Z. Zhang  Z. Ren  and A. L. Yuille. Mitigating adversarial effects through
randomization. In 6th International Conference on Learning Representations  ICLR 2018 
Vancouver  BC  Canada  April 30 - May 3  2018  Conference Track Proceedings  2018. 4

[54] C. Xie  J. Wang  Z. Zhang  Y. Zhou  L. Xie  and A. L. Yuille. Adversarial examples for semantic
segmentation and object detection. In ICCV  pages 1378–1387. IEEE Computer Society  2017.
1

[55] W. Xu  D. Evans  and Y. Qi. Feature Squeezing: Detecting Adversarial Examples in Deep
Neural Networks. Network and Distributed Systems Security Symposium (NDSS)  2018. 1  3  7

12

,Rohit Babbar
Ioannis Partalas
Eric Gaussier
Massih R. Amini
Xiaodan Liang
Zhiting Hu
Hao Zhang
Liang Lin
Eric Xing
Shengyuan Hu
Tao Yu
Chuan Guo
Wei-Lun Chao
Kilian Weinberger