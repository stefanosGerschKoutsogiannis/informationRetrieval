2017,Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning,Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC  which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework  the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.,Unifying PAC and Regret: Uniform PAC Bounds for

Episodic Reinforcement Learning

Christoph Dann

Machine Learning Department
Carnegie-Mellon University

cdann@cdann.net

Tor Lattimore∗

tor.lattimore@gmail.com

Emma Brunskill

Computer Science Department

Stanford University

ebrun@cs.stanford.edu

Abstract

Statistical performance bounds for reinforcement learning (RL) algorithms can be
critical for high-stakes applications like healthcare. This paper introduces a new
framework for theoretically measuring the performance of such algorithms called
Uniform-PAC  which is a strengthening of the classical Probably Approximately
Correct (PAC) framework. In contrast to the PAC framework  the uniform version
may be used to derive high probability regret guarantees and so forms a bridge
between the two setups that has been missing in the literature. We demonstrate
the beneﬁts of the new framework for ﬁnite-state episodic MDPs with a new
algorithm that is Uniform-PAC and simultaneously achieves optimal regret and
PAC guarantees except for a factor of the horizon.

1

Introduction

The recent empirical successes of deep reinforcement learning (RL) are tremendously exciting  but the
performance of these approaches still varies signiﬁcantly across domains  each of which requires the
user to solve a new tuning problem [1]. Ultimately we would like reinforcement learning algorithms
that simultaneously perform well empirically and have strong theoretical guarantees. Such algorithms
are especially important for high stakes domains like health care  education and customer service 
where non-expert users demand excellent outcomes.
We propose a new framework for measuring the performance of reinforcement learning algorithms
called Uniform-PAC. Brieﬂy  an algorithm is Uniform-PAC if with high probability it simultaneously
for all ε > 0 selects an ε-optimal policy on all episodes except for a number that scales polynomially
with 1/ε. Algorithms that are Uniform-PAC converge to an optimal policy with high probability
and immediately yield both PAC and high probability regret bounds  which makes them superior to
algorithms that come with only PAC or regret guarantees. Indeed 

(a) Neither PAC nor regret guarantees imply convergence to optimal policies with high probability;
(b) (ε  δ)-PAC algorithms may be ε/2-suboptimal in every episode;
(c) Algorithms with small regret may be maximally suboptimal inﬁnitely often.
∗Tor Lattimore is now at DeepMind  London

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Uniform-PAC algorithms suffer none of these drawbacks. One could hope that existing algorithms
with PAC or regret guarantees might be Uniform-PAC already  with only the analysis missing.
Unfortunately this is not the case and modiﬁcation is required to adapt these approaches to satisfy
the new performance metric. The key insight for obtaining Uniform-PAC guarantees is to leverage
time-uniform concentration bounds such as the ﬁnite-time versions of the law of iterated logarithm 
which obviates the need for horizon-dependent conﬁdence levels.
We provide a new optimistic algorithm for episodic RL called UBEV that is Uniform PAC. Unlike its
predecessors  UBEV uses conﬁdence intervals based on the law of iterated logarithm (LIL) which
hold uniformly over time. They allow us to more tightly control the probability of failure events
in which the algorithm behaves poorly. Our analysis is nearly optimal according to the traditional
metrics  with a linear dependence on the state space for the PAC setting and square root dependence
for the regret. Therefore UBEV is a Uniform PAC algorithm with PAC bounds and high probability
regret bounds that are near optimal in the dependence on the length of the episodes (horizon) and
optimal in the state and action spaces cardinality as well as the number of episodes. To our knowledge
UBEV is the ﬁrst algorithm with both near-optimal PAC and regret guarantees.

Notation and setup. We consider episodic ﬁxed-horizon MDPs with time-dependent dynamics 
which can be formalized as a tuple M = (S A  pR  P  p0  H). The statespace S and the actionspace
A are ﬁnite sets with cardinality S and A. The agent interacts with the MDP in episodes of H time
steps each. At the beginning of each time-step t ∈ [H] the agent observes a state st and chooses an
action at based on a policy π that may depend on the within-episode time step (at = π(st  t)). The
next state is sampled from the tth transition kernel st+1 ∼ P (·|st  at  t) and the initial state from
s1 ∼ p0. The agent then receives a reward drawn from a distribution pR(st  at  t) which can depend
on st  at and t with mean r(st  at  t) determined by the reward function. The reward distribution pR
is supported on [0  1].2 The value function from time step t for policy π is deﬁned as

(cid:34) H(cid:88)

t (s) := E
V π

r(si  ai  i)

s(cid:48)∈S
and the optimal value function is denoted by V (cid:63)
evaluated by the total expected reward or return

i=t

(cid:88)

=

(cid:35)
(cid:12)(cid:12)(cid:12)(cid:12)st = s
(cid:34) H(cid:88)

ρπ := E

P (s(cid:48)|s  π(s  t)  t)V π

t+1(s(cid:48)) + r(s  π(s  t)  t) .

t . In any ﬁxed episode  the quality of a policy π is

(cid:35)

r(si  ai  i)(cid:12)(cid:12)π

= p(cid:62)

0 V π
1  

i=t

0 V (cid:63)

We let Nε :=(cid:80)∞
R(T ) :=(cid:80)T

which is compared to the optimal return ρ(cid:63) = p(cid:62)
1 . For this notation p0 and the value functions V (cid:63)
t  
1 are interpreted as vectors of length S. If an algorithm follows policy πk in episode k  then the
V π
optimality gap in episode k is ∆k := ρ(cid:63) − ρπk which is bounded by ∆max = maxπ ρ(cid:63) − ρπ ≤ H.
I{∆k > ε} be the number of ε-errors and R(T ) be the regret after T episodes:
k=1 ∆k. Note that T is the number of episodes and not total time steps (which is HT
after T episodes) and k is an episode index while t usually denotes time indices within an episode.
The ˜O notation is similar to the usual O-notation but suppresses additional polylog-factors  that is
g(x) = ˜O(f (x)) iff there is a polynomial p such that g(x) = O(f (x)p(log(x))).

k=1

2 Uniform PAC and Existing Learning Frameworks

We brieﬂy summarize the most common performance measures used in the literature.

• (ε  δ)-PAC: There exists a polynomial function FPAC(S  A  H  1/ε  log(1/δ)) such that

P (Nε > FPAC(S  A  H  1/ε  log(1/δ))) ≤ δ .

• Expected Regret: There exists a function FER(S  A  H  T ) such that E[R(T )] ≤
• High Probability Regret: There exists a function FHPR(S  A  H  T  log(1/δ)) such that

FER(S  A  H  T ).

P (R(T ) > FHPR(S  A  H  T  log(1/δ))) ≤ δ .

2The reward may be allowed to depend on the next-state with no further effort in the proofs. The boundedness

assumption could be replaced by the assumption of subgaussian noise with known subgaussian parameter.

2

• Uniform High Probability Regret: There exists a function FUHPR(S  A  H  T  log(1/δ)) such

that

P (exists T : R(T ) > FUHPR(S  A  H  T  log(1/δ))) ≤ δ .

In all deﬁnitions the function F should be polynomial in all arguments. For notational conciseness
we often omit some of the parameters of F where the context is clear. The different performance
guarantees are widely used (e.g. PAC: [2  3  4  5]  (uniform) high-probability regret: [6  7  8];
expected regret: [9  10  11  12]). Due to space constraints  we will not discuss Bayesian-style
performance guarantees that only hold in expectation with respect to a distribution over problem
instances. We will shortly discuss the limitations of the frameworks listed above  but ﬁrst formally
deﬁne the Uniform-PAC criteria
Deﬁnition 1 (Uniform-PAC). An algorithm is Uniform-PAC for δ > 0 if

P (exists ε > 0 : Nε > FUPAC (S  A  H  1/ε  log(1/δ))) ≤ δ  

where FUPAC is polynomial in all arguments.

All the performance metrics are functions of the distribution of the sequence of errors over the
episodes (∆k)k∈N. Regret bounds are the integral of this sequence up to time T   which is a random
variable. The expected regret is just the expectation of the integral  while the high-probability
regret is a quantile. PAC bounds are the quantile of the size of the superlevel set for a ﬁxed level ε.
Uniform-PAC bounds are like PAC bounds  but hold for all ε simultaneously.

Limitations of regret. Since regret guarantees only bound the integral of ∆k over k  it does not
distinguish between making a few severe mistakes and many small mistakes. In fact  since regret
bounds provably grow with the number of episodes T   an algorithm that achieves optimal regret may
still make inﬁnitely many mistakes (of arbitrary quality  see proof of Theorem 2 below). This is
highly undesirable in high-stakes scenarios. For example in drug treatment optimization in healthcare 
we would like to distinguish between infrequent severe complications (few large ∆k) and frequent
minor side effects (many small ∆k). In fact  even with an optimal regret bound  we could still serve
inﬁnitely patients with the worst possible treatment.

Limitations of PAC. PAC bounds limit the number of mistakes for a given accuracy level ε  but
is otherwise non-restrictive. That means an algorithm with ∆k > ε/2 for all k almost surely might
still be (ε  δ)-PAC. Worse  many algorithms designed to be (ε  δ)-PAC actually exhibit this behavior
because they explicitly halt learning once an ε-optimal policy has been found. The less widely used
TCE (total cost of exploration) bounds [13] and KWIK guarantees [14] suffer from the same issueand
for conciseness are not discussed in detail.

Advantages of Uniform-PAC. The new criterion overcomes the limitations of PAC and regret
guarantees by measuring the number of ε-errors at every level simultaneously. By deﬁnition  algo-
rithms that are Uniform-PAC for a δ are (ε  δ)-PAC for all ε > 0. We will soon see that an algorithm
with a non-trivial Uniform-PAC guarantee also has small regret with high probability. Furthermore 
there is no loss in the reduction so that an algorithm with optimal Uniform-PAC guarantees also
has optimal regret  at least in the episodic RL setting. In this sense Uniform-PAC is the missing
bridge between regret and PAC. Finally  for algorithms based on conﬁdence bounds  Uniform-PAC
guarantees are usually obtained without much additional work by replacing standard concentration
bounds with versions that hold uniformly over episodes (e.g. using the law of the iterated logarithms).
In this sense we think Uniform-PAC is the new ‘gold-standard’ of theoretical guarantees for RL
algorithms.

2.1 Relationships between Performance Guarantees

Existing theoretical analyses usually focus exclusively on either the regret or PAC framework. Besides
occasional heuristic translations  Proposition 4 in [15] and Corollary 3 in [6] are the only results
relating a notion of PAC and regret  we are aware of. Yet the guarantees there are not widely used3

3The average per-step regret in [6] is superﬁcially a PAC bound  but does not hold over inﬁnitely many
time-steps and exhibits the limitations of a conventional regret bound. The translation to average loss in [15]
comes at additional costs due to the discounted inﬁnite horizon setting.

3

Figure 1: Visual summary of relationship among the different learning frameworks: Expected regret
(ER) and PAC preclude each other while the other crossed arrows represent only a does-not-implies
relationship. Blue arrows represent imply relationships. For details see the theorem statements.

unlike the deﬁnitions given above which we now formally relate to each other. A simpliﬁed overview
of the relations discussed below is shown in Figure 1.
Theorem 1. No algorithm can achieve

• a sub-linear expected regret bound for all T and
• a ﬁnite (ε  δ)-PAC bound for a small enough ε

simultaneously for all two-armed multi-armed bandits with Bernoulli reward distributions. This
implies that such guarantees also cannot be satisﬁed simultaneously for all episodic MDPs.

A full proof is in Appendix A.1  but the intuition is simple. Suppose a two-armed Bernoulli bandit has
mean rewards 1/2 + ε and 1/2 respectively and the second arm is chosen at most F < ∞ times with
probability at least 1 − δ  then one can easily show that in an alternative bandit with mean rewards
1/2 + ε and 1/2 + 2ε there is a non-zero probability that the second arm is played ﬁnitely often and in
this bandit the expected regret will be linear. Therefore  sub-linear expected regret is only possible if
each arm is pulled inﬁnitely often almost surely.
Theorem 2. The following statements hold for performance guarantees in episodic MDPs:

(a) If an algorithm satisﬁes a (ε  δ)-PAC bound with FPAC = Θ(1/ε2) then it satisﬁes for a
speciﬁc T = Θ(ε−3) a FHPR = Θ(T 2/3) bound. Further  there is an MDP and algorithm that
satisﬁes the (ε  δ)-PAC bound FPAC = Θ(1/ε2) on that MDP and has regret R(T ) = Ω(T 2/3)
on that MDP for any T . That means a (ε  δ)-PAC bound with FPAC = Θ(1/ε2) can only be
converted to a high-probability regret bound with FHPR = Ω(T 2/3).

(b) For any chosen ε  δ > 0 and FPAC  there is an MDP and algorithm that satisﬁes the (ε  δ)-PAC
bound FPAC on that MDP and has regret R(T ) = Ω(T ) on that MDP. That means a (ε  δ)-PAC
bound cannot be converted to a sub-linear uniform high-probability regret bound.

(c) For any FUHPR(T  δ) with FUHPR(T  δ) → ∞ as T → ∞  there is an algorithm that satisﬁes
that uniform high-probability regret bound on some MDP but makes inﬁnitely many mistakes
for any sufﬁciently small accuracy level ε > 0 for that MDP. Therefore  a high-probability
regret bound (uniform or not) cannot be converted to a ﬁnite (ε  δ)-PAC bound.

(d) For any FUHPR(T  δ) there is an algorithm that satisﬁes that uniform high-probability regret

bound on some MDP but suffers expected regret ER(T ) = Ω(T ) on that MDP.

√
For most interesting RL problems including episodic MDPs the worst-case expected regret grows
with O(
T ). The theorem shows that establishing an optimal high probability regret bound does not
imply any ﬁnite PAC bound. While PAC bounds may be converted to regret bounds  the resulting
bounds are necessarily severely suboptimal with a rate of T 2/3. The next theorem formalises the
claim that Uniform-PAC is stronger than both the PAC and high-probability regret criteria.

4

Uniform PACExpected RegretHigh-Prob.RegretUniform High- Prob. RegretPACimpliesimpliesimpliesprecludecannot implyimplies subopt. for single TTheorem 3. Suppose an algorithm is Uniform-PAC for some δ with FUPAC = ˜O(C1/ε + C2/ε2)
where C1  C2 > 0 are constant in ε  but may depend on other quantities such as S  A  H  log(1/δ) 
then the algorithm

(a) converges to optimal policies with high probability: P(limk→∞ ∆k = 0) ≥ 1 − δ.
(b) is (ε  δ)-PAC with bound FPAC = FUPAC for all ε.
√
(c) enjoys a high-probability regret at level δ with FUHPR = ˜O(

C2T + max{C1  C2}).

Observe that stronger uniform PAC bounds lead to stronger regret bounds and for RL in episodic
MDPs  an optimal uniform-PAC bound implies a uniform regret bound. To our knowledge  there
are no existing approaches with PAC or regret guarantees that are Uniform-PAC. PAC methods such
as MBIE  MoRMax  UCRL-γ  UCFH  Delayed Q-Learning or Median-PAC all depend on advance
knowledge of ε and eventually stop improving their policies. Even when disabling the stopping
condition  these methods are not uniform-PAC as their conﬁdence bounds only hold for ﬁnitely many
episodes and are eventually violated according to the law of iterated logarithms. Existing algorithms
with uniform high-probability regret bounds such as UCRL2 or UCBVI [16] also do not satisfy

uniform-PAC bounds since they use upper conﬁdence bounds with width(cid:112)log(T )/n where T is the

number of observed episodes and n is the number of observations for a speciﬁc state and action. The
presence of log(T ) causes the algorithm to try each action in each state inﬁnitely often. One might
begin to wonder if uniform-PAC is too good to be true. Can any algorithm meet the requirements? We
demonstrate in Section 4 that the answer is yes by showing that UBEV has meaningful Uniform-PAC
bounds. A key technique that allows us to prove these bounds is the use of ﬁnite-time law of iterated

logarithm conﬁdence bounds which decrease at rate(cid:112)(log log n)/n.

3 The UBEV Algorithm

The pseudo-code for the proposed UBEV algorithm is given in Algorithm 1. In each episode it
follows an optimistic policy πk that is computed by backwards induction using a carefully chosen
conﬁdence interval on the transition probabilities in each state. In line 8 an optimistic estimate of the
Q-function for the current state-action-time triple is computed using the empirical estimates of the
expected next state value ˆVnext ∈ R (given that the values at the next time are ˜Vt+1) and expected
immediate reward ˆr plus conﬁdence bounds (H − t)φ and φ. We show in Lemma D.1 in the appendix
that the policy update in Lines 3–9 ﬁnds an optimal solution to maxP (cid:48) r(cid:48) V (cid:48) π(cid:48) Es∼p0 [V (cid:48)
1 (s)] subject
to the constraints that for all s ∈ S  a ∈ A  t ∈ [H] 

V (cid:48)
t (s) = r(s  π(cid:48)(s  t)  t) + P (cid:48)(s  π(cid:48)(s  t)  t)(cid:62)V (cid:48)
H+1 = 0  P (cid:48)(s  a  t) ∈ ∆S 
V (cid:48)
|[(P (cid:48) − ˆPk)(s  a  t)](cid:62)V (cid:48)
|r(cid:48)(s  a  t) − ˆrk(s  a  t)| ≤ φ(s  a  t)

t+1| ≤ φ(s  a  t)(H − t)

t+1

r(cid:48)(s  a  t) ∈ [0  1]

(Bellman Equation)

(1)

(cid:32)(cid:115)

(2)

(cid:33)

(cid:115)

where (P (cid:48) − ˆPk)(s  a  t) is short for P (cid:48)(s  a  t) − ˆPk(s  a  t) = P (cid:48)(·|s  a  t) − ˆPk(·|s  a  t) and

φ(s  a  t) =

2 ln ln max{e  n(s  a  t)} + ln(18SAH/δ)

n(s  a  t)

= O

ln(SAH ln(n(s  a  t))/δ)

n(s  a  t)

is the width of a conﬁdence bound with e = exp(1) and ˆPk(s(cid:48)|s  a  t) = m(s(cid:48) s a t)
are the empirical
transition probabilities and ˆrk(s  a  t) = l(s  a  t)/n(s  a  t) the empirical immediate rewards (both
at the beginning of the kth episode). Our algorithm is conceptually similar to other algorithms based
on the optimism principle such as MBIE [5]  UCFH [3]  UCRL2 [6] or UCRL-γ [2] but there are
several key differences:

n(s a t)

• Instead of using conﬁdence intervals over the transition kernel by itself  we incorporate the
value function directly into the concentration analysis. Ultimately this saves a factor of S in
the sample complexity  but the price is a more difﬁcult analysis. Previously MoRMax [17]
also used the idea of directly bounding the transition and value function  but in a very different
algorithm that required discarding data and had a less tight bound. A similar technique has
been used by Azar et al. [16].

5

Algorithm 1: UBEV (Upper Bounding the Expected Next State Value) Algorithm

Input :failure tolerance δ ∈ (0  1]
1 n(s  a  t) = l(s  a  t) = m(s(cid:48)  s  a  t) = 0;
2 for k = 1  2  3  . . . do

˜VH+1(s(cid:48)) := 0 ∀s  s(cid:48) ∈ S  a ∈ A  t ∈ [H]

*/

*/

/* Optimistic planning
for t = H to 1 do
for s ∈ S do

for a ∈ A do

(cid:113) 2 ln ln(max{e n(s a t)})+ln(18SAH/δ)

φ :=
ˆr := l(s a t)
n(s a t) ;
Q(a) := min{1  ˆr + φ} + min

ˆVnext := m(· s a t)(cid:62) ˜Vt+1

(cid:110)

n(s a t)

n(s a t)

// confidence bound

// empirical estimates

max ˜Vt+1  ˆVnext + (H − t)φ

(cid:111)

πk(s  t) := arg maxa Q(a) 

˜Vt(s) := Q(πk(s  t))

/* Execute policy for one episode
s1 ∼ p0;
for t = 1 to H do

at := πk(st  t)  rt ∼ pR(st  at  t) and st+1 ∼ P (st  at  t)
n(st  at  t)++; m(st+1  st  at  t)++;

l(st  at  t)+= rt // update statistics

3
4
5

6

7

8

9

10
11
12
13

Figure 2: Empirical comparison of optimism-based algorithms with frequentist regret or PAC bounds
on a randomly generated MDP with 3 actions  time horizon 10 and S = 5  50  200 states. All
algorithms are run with parameters that satisfy their bound requirements. A detailed description of
the experimental setup including a link to the source code can be found in Appendix B.

• Many algorithms update their policy less and less frequently (usually when the number of
samples doubles)  and only ﬁnitely often in total. Instead  we update the policy after every
episode  which means that UBEV immediately leverages new observations.

• Conﬁdence bounds in existing algorithms that keep improving the policy (e.g. Jaksch et al.

[6]  Azar et al. [16]) scale at a rate(cid:112)log(k)/n where k is the number of episodes played so far
Instead the width of UBEV’s conﬁdence bounds φ scales at rate(cid:112)ln ln(max{e  n})/n ≈
(cid:112)(log log n)/n which is the best achievable rate and results in signiﬁcantly faster learning.

and n is the number of times the speciﬁc (s  a  t) has been observed. As the results of a brief
empirical comparison in Figure 2 indicate  this leads to slow learning (compare UCBVI_1
and UBEV’s performance which differ essentially only by their use of different rate bounds).

4 Uniform PAC Analysis

We now discuss the Uniform-PAC analysis of UBEV which results in the following Uniform-PAC
and regret guarantee.

6

103104105106107Number of Episodes0.00.51.01.52.02.53.03.5Expected Return S=5103104105106107Number of Episodes0.51.01.52.02.53.0Expected Return S=50103104105106107Number of Episodes0.51.01.52.02.5Expected Return S=200MoRMaxUBEVUCRL2MBIEMedianPACDelayedQLOIMUCFHUCBVI_1UCBVI_2optimalTheorem 4. Let πk be the policy of UBEV in the kth episode. Then with probability at least 1 − δ
for all ε > 0 jointly the number of episodes k where the expected return from the start state is not
ε-optimal (that is ∆k > ε) is at most

(cid:18)

A  S  H 

.

1
ε

 

1
δ

(cid:19)(cid:19)

(cid:17)

O

(cid:18) SAH 4
ε2 min(cid:8)1+εS2A  S(cid:9) polylog
(cid:16)

√
H 2(

R(T ) = O

SAT + S3A2) polylog(S  A  H  T )

.

Therefore  with probability at least 1 − δ UBEV converges to optimal policies and for all episodes T
has regret

Here polylog(x . . . ) is a function that can be bounded by a polynomial of logarithm  that is  ∃k  C :
polylog(x . . . ) ≤ ln(x . . . )k+C. In Appendix C we provide a lower bound on the sample complexity
that shows that if ε < 1/(S2A)  the Uniform-PAC bound is tight up to log-factors and a factor of H.
To our knowledge  UBEV is the ﬁrst algorithm with both near-tight (up to H factors) high probability
regret and (ε  δ) PAC bounds as well as the ﬁrst algorithm with any nontrivial uniform-PAC bound.
Using Theorem 3 the convergence and regret bound follows immediately from the uniform PAC
bound. After a discussion of the different conﬁdence bounds allowing us to prove uniform-PAC
bounds  we will provide a short proof sketch of the uniform PAC bound.

4.1 Enabling Uniform PAC With Law-of-Iterated-Logarithm Conﬁdence Bounds

To have a PAC bound for all ε jointly  it is critical that UBEV continually make use of new experience.
If UBEV stopped leveraging new observations after some ﬁxed number  it would not be able to
distinguish with high probability among which of the remaining possible MDPs do or do not have
optimal policies that are sufﬁciently optimal in the other MDPs. The algorithm therefore could
potentially follow a policy that is not at least ε-optimal for inﬁnitely many episodes for a sufﬁciently
small ε. To enable UBEV to incorporate all new observations  the conﬁdence bounds in UBEV must
hold for an inﬁnite number of updates. We therefore require a proof that the total probability of all
possible failure events (of the high conﬁdence bounds not holding) is bounded by δ  in order to obtain
high probability guarantees. In contrast to prior (ε  δ)-PAC proofs that only consider a ﬁnite number
of failure events (which is enabled by requiring an RL algorithm to stop using additional data)  we
must bound the probability of an inﬁnite set of possible failure events.
Some choices of conﬁdence bounds will hold uniformly across all sample sizes but are not sufﬁciently
tight for uniform PAC results. For example  the recent work by Azar et al. [16] uses conﬁdence
intervals that shrink at a rate of
n   where T is the number of episodes  and n is the number of
samples of a (s  a) pair at a particular time step. This conﬁdence interval will hold for all episodes 
but these intervals do not shrink sufﬁciently quickly and can even increase. One simple approach for
constructing conﬁdence intervals that is sufﬁcient for uniform PAC guarantees is to combine bounds
for ﬁxed number of samples with a union bound allocating failure probability δ/n2 to the failure case

(cid:113) ln T

know of no algorithms that do such in our setting.
We follow a similarly simple but much stronger approach of using law-of-iterated logarithm (LIL)

with n samples. This results in conﬁdence intervals that shrink at rate(cid:112)1/n ln n. Interestingly we
bounds that shrink at the better rate of(cid:112)1/n ln ln n. Such bounds have sparked recent interest in
PAC bounds  and much tighter (and therefore will lead to much better performance) than(cid:112)1/n ln T

sequential decision making [18  19  20  21  22] but to the best of our knowledge we are the ﬁrst to
leverage them for RL. We prove several general LIL bounds in Appendix F and explain how we use
these results in our analysis in Appendix E.2. These LIL bounds are both sufﬁcient to ensure uniform

bounds. Indeed  LIL have the tightest possible rate dependence on the number of samples n for a
bound that holds for all timesteps (though they are not tight with respect to constants).

4.2 Proof Sketch

We now provide a short overview of our uniform PAC bound in Theorem 4. It follows the typical
scheme for optimism based algorithms: we show that in each episode UBEV follows a policy that is

7

optimal with respect to the MDP ˜Mk that yields highest return in a set of MDPs Mk given by the
constraints in Eqs. (1)–(2) (Lemma D.1 in the appendix). We then deﬁne a failure event F (more
details see below) such that on the complement F C  the true MDP is in Mk for all k.
Under the event that the true MDP is in the desired set  the V π
of πk
in MDP ˜Mk is higher than the optimal value function of the true MDP M (Lemma E.16). Therefore 
1 − V πk
the optimality gap is bounded by ∆k ≤ p(cid:62)
1 ). The right hand side this expression is then
H(cid:88)
decomposed via a standard identity (Lemma E.15) as

1   i.e.  the value ˜V πk

1 ≤ ˜V πk

1 ≤ V (cid:63)

(cid:88)

(cid:88)

H(cid:88)

0 ( ˜V πk

1

wtk(s  a)(( ˜Pk − P )(s  a  t))(cid:62) ˜V πk

wtk(s  a)(˜rk(s  a  t) − r(s  a  t)) 

t+1 +

t=1

(s a)∈S×A

t=1

(s a)∈S×A

where wtk(s  a) is the probability that when following policy πk in the true MDP we encounter
st = s and at = a. The quantities ˜Pk  ˜rk are the model parameters of the optimistic MDP ˜Mk For
the sake of conciseness  we ignore the second term above in the following which can be bounded by
ε/3 in the same way as the ﬁrst. We further decompose the ﬁrst term as

(cid:88)
(cid:88)

wtk(s  a)(( ˜Pk − P )(s  a  t))(cid:62) ˜V πk

tk

t+1

t∈[H]
(s a)∈Lc

(cid:88)
where Ltk = (cid:8)(s  a) ∈ S × A : wtk(s  a) ≥ wmin = ε

wtk(s  a)(( ˜Pk − ˆPk)(s  a  t))(cid:62) ˜V πk

(s a)∈Ltk

(s a)∈Ltk

t+1 +

t∈[H]

t∈[H]

+

3HS2

(3)

(4)

wtk(s  a)(( ˆPk − P )(s  a  t))(cid:62) ˜V πk

t+1

(cid:9) is the set of state-action pairs with
(cid:32)(cid:115)

(cid:33)

H 2 ln (ln(ntk(s  a))/δ)

ntk(s  a)

 

(5)

  (6)

(cid:115)

SH 2 ln ln ntk(s a)

δ
ntk(s  a)

non-negligible visitation probability. The value of wmin is chosen so that (3) is bounded by ε/3.
Since ˜V πk is the optimal solution of the optimization problem in Eq. (1)  we can bound

|(( ˜Pk− ˆPk)(s  a  t))(cid:62) ˜V πk

t+1| ≤ φk(s  a  t)(H − t) = O

where φk(s  a  t) is the value of φ(s  a  t) and ntk(s  a) the value of n(s  a  t) right before episode k.
Further we decompose

|(( ˆPk − P )(s  a  t))(cid:62) ˜V πk

t+1| ≤ (cid:107)( ˆPk − P )(s  a  t)(cid:107)1(cid:107) ˜V πk

t+1(cid:107)∞ ≤ O

 H(cid:88)

(cid:88)

(cid:115)

 .

where the second inequality follows from a standard concentration bound used in the deﬁnition of the
failure event F (see below). Substituting this and (5) into (4) leads to

(4) ≤ O

SH 2 ln(ln(ntk(s  a))/δ)

(7)

wtk(s  a)

δ

2

t=1

ntk(s  a)

i.e. (cid:80)

i<k wti(s  a) ≥ 4 ln 9SA

δ   it holds that ntk(s  a) ≥ 1

(cid:80)
i<k wti(s  a) − ln 9SAH
(cid:80)

s a∈Ltk
On F C it also holds that ntk(s  a) ≥ 1
and so on nice episodes where
each (s  a) ∈ Ltk with signiﬁcant probability wtk(s  a) also had signiﬁcant probability in the past 
i<k wti(s  a). Substituting this into
(7)  we can use a careful pidgeon-hole argument laid out it Lemma E.3 in the appendix to show
that this term is bounded by ε/3 on all but O(AS2H 4/ε2 polylog(A  S  H  1/ε  1/δ)) nice episodes.
Again using a pidgeon-hole argument  one can show that all but at most O(S2AH 3/ε ln(SAH/δ))
episodes are nice. Combining both bounds  we get that on F C the optimality gap ∆k is at most ε
except for at most O(AS2H 4/ε2 polylog(A  S  H  1/ε  1/δ)) episodes.
We decompose the failure event into multiple components. In addition to the events F N
k that a
(s  a  t) triple has been observed few times compared to its visitation probabilities in the past  i.e. 
as well as a conditional version of this statement  the
ntk(s  a) < 1
2
failure event F contains events where empirical estimates of the immediate rewards  the expected
optimal value of the successor states and the individual transition probabilites are far from their true

(cid:80)
i<k wti(s  a) − ln 9SAH

4

δ

8

(cid:26)

expectations. For the full deﬁnition of F see Appendix E.2. F also contains event F L1 we used in
Eq. (6) deﬁned as

(cid:114)

(cid:16)

(cid:17)(cid:27)

.

∃k  s  a  t : (cid:107) ˆPk(s  a  t) − P (s  a  t)(cid:107)1 ≥

4

ntk(s a)

2 llnp(ntk(s  a)) + ln 18SAH(2S−2)

δ

It states that the L1-distance of the empirical transition probabilities to the true probabilities for
any (s  a  t) in any episode k is too large and we show that P(F L1) ≤ 1 − δ/9 using a uniform
version of the popular bound by Weissman et al. [23] which we prove in Appendix F. We show in
similar manner that the other events in F have small probability uniformly for all episodes k so that
P(F ) ≤ δ. Together this yields the uniform PAC bound in Thm. 4 using the second term in the min.
With a more reﬁned analysis that avoids the use of Hölder’s inequality in (6) and a stronger notion of
nice episodes called friendly episodes we obtain the bound with the ﬁrst term in the min. However 
since a similar analysis has been recently released [16]  we defer this discussion to the appendix.

4.3 Discussion of UBEV Bound

The (Uniform-)PAC bound for UBEV in Theorem 4 is never worse than ˜O(S2AH 4/ε2)  which
improves on the similar MBIE algorithm by a factor of H 2 (after adapting the discounted setting for
which MBIE was analysed to our setting). For ε < 1/(S2A) our bound has a linear dependence on
the size of the state-space and depends on H 4  which is a tighter dependence on the horizon than
MoRMax’s ˜O(SAH 6/ε2)  the best sample-complexity bound with linear dependency S so far.
Comparing UBEV’s regret bound to the ones of UCRL2 [6] and REGAL [24] requires care because
(a) we measure the regret over entire episodes and (b) our transition dynamics are time-dependent
within each episode  which effectively increases the state-space by a factor of H. Converting the
bounds for UCRL2/REGAL to our setting yields a regret bound of order SH 2
AHT . Here  the
diameter is H  the state space increases by H due to time-dependent transition dynamics and an
additional
H is gained by stating the regret in terms of episodes T instead of time steps. Hence 
UBEV’s bounds are better by a factor of
SH. Our bound matches the recent regret bound for
episodic RL by Azar et al. [16] in the S  A and T terms but not in H. Azar et al. [16] has regret
bounds that are optimal in H but their algorithm is not uniform PAC  due to the characteristics we
outlined in Section 2.

√

√

√

5 Conclusion

The Uniform-PAC framework strengthens and uniﬁes the PAC and high-probability regret perfor-
mance criteria for reinforcement learning in episodic MDPs. The newly proposed algorithm is
Uniform-PAC  which as a side-effect means it is the ﬁrst algorithm that is both PAC and has sub-
linear (and nearly optimal) regret. Besides this  the use of law-of-the-iterated-logarithm conﬁdence
bounds in RL algorithms for MDPs provides a practical and theoretical boost at no cost in terms of
computation or implementation complexity.
This work opens up several immediate research questions for future work. The deﬁnition of
Uniform-PAC and the relations to other PAC and regret notions directly apply to multi-armed bandits
and contextual bandits as special cases of episodic RL  but not to inﬁnite horizon reinforcement
learning. An extension to these non-episodic RL settings is highly desirable. Similarly  a version
of the UBEV algorithm for inﬁnite-horizon RL with linear state-space sample complexity would
be of interest. More broadly  if theory is ever to say something useful about practical algorithms
for large-scale reinforcement learning  then it will have to deal with the unrealizable function
approximation setup (unlike the tabular function representation setting considered here)  which is a
major long-standing open challenge.

Acknowledgements. We appreciate the support of a NSF CAREER award and a gift from
Yahoo.

References
[1] Vincent François-Lavet  Raphaël Fonteneau  and Damien Ernst. How to discount deep re-
inforcement learning: Towards new dynamic strategies. In NIPS 2015 Workshop on Deep

9

Reinforcement Learning  2015.

[2] Tor Lattimore and Marcus Hutter. Near-optimal PAC bounds for discounted MDPs. In Theoreti-

cal Computer Science  volume 558  2014.

[3] Christoph Dann and Emma Brunskill. Sample Complexity of Episodic Fixed-Horizon Rein-

forcement Learning. In Neural Information Processing Systems  2015.

[4] Nan Jiang  Akshay Krishnamurthy  Alekh Agarwal  John Langford  and Robert E Schapire.
Contextual Decision Processes with Low Bellman Rank are PAC-Learnable. In International
Conference on Machine Learning  2017.

[5] Alexander L Strehl  Lihong Li  and Michael L Littman. Reinforcement Learning in Finite

MDPs : PAC Analysis. Journal of Machine Learning Research  10:2413–2444  2009.

[6] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal Regret Bounds for Reinorcement

Learning. Journal of Machine Learning Research  11:1563–1600  2010.

[7] Alekh Agarwal  Daniel Hsu  Satyen Kale  John Langford  Lihong Li  and Robert E. Schapire.
Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits. In Journal of
Machine Learning Research  volume 32  2014.

[8] Niranjan Srinivas  Andreas Krause  Sham M. Kakade  and Matthias W. Seeger. Information-
In IEEE

theoretic regret bounds for Gaussian process optimization in the bandit setting.
Transactions on Information Theory  volume 58  2012.

[9] Jean Yves Audibert  Rémi Munos  and Csaba Szepesvári. Exploration-exploitation tradeoff
using variance estimates in multi-armed bandits. Theoretical Computer Science  410(19):
1876–1902  2009.

[10] Peter Auer. Using upper conﬁdence bounds for online learning. Proceedings 41st Annual

Symposium on Foundations of Computer Science  pages 270–293  2000.

[11] Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic

Multi-armed Bandit Problems. arXiv.org  cs.LG(1):138  2012.

[12] Peter Auer and Ronald Ortner. Online Regret Bounds for a New Reinforcement Learning

Algorithm. In Proceedings 1st Austrian Cognitive Vision Workshop  2005.

[13] Jason Pazis and Ronald Parr. Efﬁcient PAC-optimal Exploration in Concurrent   Continuous

State MDPs with Delayed Updates. In AAAI Conference on Artiﬁcial Intelligence  2016.

[14] Lihong Li  Michael L. Littman  Thomas J. Walsh  and Alexander L. Strehl. Knows what it

knows: A framework for self-aware learning. Machine Learning  82(3):399–443  nov 2011.

[15] Alexander L. Strehl and Michael L. Littman. An analysis of model-based Interval Estimation
for Markov Decision Processes. Journal of Computer and System Sciences  74(8):1309–1331 
2008.

[16] Mohammad Gheshlaghi Azar  Ian Osband  and Rémi Munos. Minimax Regret Bounds for

Reinforcement Learning. In International Conference on Machine Learning  2017.

[17] Istvàn Szita and Csaba Szepesvári. Model-based reinforcement learning with nearly tight

exploration complexity bounds. In International Conference on Machine Learning  2010.

[18] Kevin Jamieson  Matthew Malloy  Robert Nowak  and Sébastien Bubeck. lil’ UCB : An Optimal

Exploration Algorithm for Multi-Armed Bandits. 2013.

[19] Akshay Balsubramani and Aaditya Ramdas. Sequential Nonparametric Testing with the Law of

the Iterated Logarithm. In Uncertainty in Artiﬁcial Intelligence  2016.

[20] Aurélien Garivier  Emilie Kaufmann  and Tor Lattimore. On Explore-Then-Commit Strategies.

In Advances in Neural Information Processing Systems  2016.

[21] Pascal Massart. Concentration inequalities and model selection. Lecture Notes in Mathematics 

1896  2007.

[22] Aurelien Garivier and Olivier Cappe. The KL-UCB Algorithm for Bounded Stochastic Bandits

and Beyond. In Conference on Learning Theory  2011.

[23] Tsachy Weissman  Erik Ordentlich  Gadiel Seroussi  Sergio Verdu  and Marcelo J Wein-
berger.
Inequalities for the L 1 Deviation of the Empirical Distribution. Technical re-
port  2003. URL http://www.hpl.hp.com/techreports/2003/HPL-2003-97R1.pdf?
origin=publicationDetail.

10

[24] Peter L. Bartlett and a. Tewari. REGAL: A regularization based algorithm for reinforcement
learning in weakly communicating MDPs. Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence  pages 35–42  2009.

[25] Stephane Boucheron  Gabor Lugosi  and Pascal Massart. Concentration Inequalities - A
Nonasymptotic Theory of Independence. Oxford University Press  2013. ISBN 978-0-19-
953525-5.

[26] Rick Durrett. Probability - Theory and Examples. Cambridge University Press  4 edition  2010.

ISBN 978-0-521-76539-8.

11

,Christoph Dann
Tor Lattimore
Emma Brunskill