2017,Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks,We consider the problem of learning function classes computed by   neural networks with various activations (e.g. ReLU or Sigmoid)  a   task believed to be computationally intractable in the worst-case.   A major open problem is to understand the minimal assumptions under   which these classes admit provably efficient algorithms. In this work we show   that a natural distributional assumption corresponding to {\em     eigenvalue decay} of the Gram matrix yields polynomial-time   algorithms in the non-realizable setting for expressive classes of   networks (e.g. feed-forward networks of ReLUs).  We make no    assumptions on the structure of the network or the labels.  Given   sufficiently-strong eigenvalue decay  we obtain {\em     fully}-polynomial time algorithms in {\em all} the relevant   parameters with respect to square-loss.  This is the first purely   distributional assumption that leads to polynomial-time algorithms   for networks of ReLUs.  Further  unlike   prior distributional assumptions (e.g.  the marginal distribution is   Gaussian)  eigenvalue decay has been observed in practice on common   data sets.,Eigenvalue Decay Implies Polynomial-Time

Learnability for Neural Networks

Surbhi Goel ∗

Department of Computer Science

University of Texas at Austin

surbhi@cs.utexas.edu

Adam Klivans †

Department of Computer Science

University of Texas at Austin
klivans@cs.utexas.edu

Abstract

We consider the problem of learning function classes computed by neural net-
works with various activations (e.g. ReLU or Sigmoid)  a task believed to be com-
putationally intractable in the worst-case. A major open problem is to understand
the minimal assumptions under which these classes admit provably efﬁcient algo-
rithms. In this work we show that a natural distributional assumption correspond-
ing to eigenvalue decay of the Gram matrix yields polynomial-time algorithms in
the non-realizable setting for expressive classes of networks (e.g. feed-forward
networks of ReLUs). We make no assumptions on the structure of the network or
the labels. Given sufﬁciently-strong eigenvalue decay  we obtain fully-polynomial
time algorithms in all the relevant parameters with respect to square-loss. This is
the ﬁrst purely distributional assumption that leads to polynomial-time algorithms
for networks of ReLUs. Further  unlike prior distributional assumptions (e.g.  the
marginal distribution is Gaussian)  eigenvalue decay has been observed in practice
on common data sets.

1

Introduction

Understanding the computational complexity of learning neural networks from random examples
is a fundamental problem in machine learning. Several researchers have proved results showing
computational hardness for the worst-case complexity of learning various networks– that is  when
no assumptions are made on the underlying distribution or the structure of the network [10  16 
21  26  43]. As such  it seems necessary to take some assumptions in order to develop efﬁcient
algorithms for learning deep networks (the most expressive class of networks known to be learnable
in polynomial-time without any assumptions is a sum of one hidden layer of sigmoids [16]). A
major open question is to understand what are the “correct” or minimal assumptions to take in
order to guarantee efﬁcient learnability3. An oft-taken assumption is that the marginal distribution is
equal to some smooth distribution such as a multivariate Gaussian. Even under such a distributional
assumption  however  there is evidence that fully polynomial-time algorithms are still hard to obtain
for simple classes of networks [19  36]. As such  several authors have made further assumptions on
the underlying structure of the model (and/or work in the noiseless or realizable setting).
In fact  in an interesting recent work  Shamir [34] has given evidence that both distributional as-
sumptions and assumptions on the network structure are necessary for efﬁcient learnability using
gradient-based methods. Our main result is that under only an assumption on the marginal distribu-
tion  namely eigenvalue decay of the Gram matrix  there exist efﬁcient algorithms for learning broad

∗Work supported by a Microsoft Data Science Initiative Award.
†Part of this work was done while visiting the Simons Institute for Theoretical Computer Science.
3For example  a very recent paper of Song  Vempala  Xie  and Williams [36] asks “What form would such

an explanation take  in the face of existing complexity-theoretic lower bounds?”

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

classes of neural networks even in the non-realizable (agnostic) setting with respect to square loss.
Furthermore  eigenvalue decay has been observed often in real-world data sets  unlike distributional
assumptions that take the marginal to be unimodal or Gaussian. As one would expect  stronger as-
sumptions on the eigenvalue decay result in polynomial learnability for broader classes of networks 
but even mild eigenvalue decay will result in savings in runtime and sample complexity.
The relationship between our assumption on eigenvalue decay and prior assumptions on the
marginal distribution being Gaussian is similar in spirit to the dichotomy between the complexity of
certain algorithmic problems on power-law graphs versus Erd˝os-Rényi graphs. Several important
graph problems such as clique-ﬁnding become much easier when the underlying model is a
random graph with appropriate power-law decay (as opposed to assuming the graph is generated
from the classical G(n  p) model) [6  22].
In this work we prove that neural network learning
problems become tractable when the underlying distribution induces an empirical gram matrix with
sufﬁciently strong eigenvalue-decay.

Our Contributions. Our main result is quite general and holds for any function class that can
be suitably embedded in an RKHS (Reproducing Kernel Hilbert Space) with corresponding kernel
function k (we refer readers unfamiliar with kernel methods to [30]). Given m draws from a distri-
bution (x1  . . .   xm) and kernel k  recall that the Gram matrix K is an m × m matrix where the i  j
entry equals k(xi  xj). For ease of presentation  we begin with an informal statement of our main
result that highlights the relationship between the eigenvalue decay assumption and the run-time and
sample complexity of our ﬁnal algorithm.
Theorem 1 (Informal). Fix function class C and kernel function k. Assume C is approximated in the
corresponding RKHS with norm bound B. After drawing m samples  let K/m be the (normalized)
m × m Gram matrix with eigenvalues {λ1  . . .   λm}. For error parameter  > 0 
1. If  for sufﬁciently large i  λi ≈ O(i−p)  then C is efﬁciently learnable with m = ˜O(B1/p/2+3/p).
2. If  for sufﬁciently large i  λi ≈ O(e−i)  then C is efﬁciently learnable with m = ˜O(log B/2).
We allow a failure probability for the event that the eigenvalues do not decay. In all prior work 
the sample complexity m depends linearly on B  and for many interesting concept classes (such as
ReLUs)  B is exponential in one or more relevant parameters. Given Theorem 1  we can use known
structural results for embedding neural networks into an RKHS to estimate B and take a correspond-
ing eigenvalue decay assumption to obtain polynomial-time learnability. Applying bounds recently
obtained by Goel et al. [16] we have
Corollary 2. Let C be the class of all fully-connected networks of ReLUs with one-hidden layer
of (cid:96) hidden ReLU activations feeding into a single ReLU output activation (i.e.  two hidden layers
or depth-3). Then  assuming eigenvalue decay of O(i−(cid:96)/)  C is learnable in polynomial time with
respect to square loss on Sn−1. If ReLU is replaced with sigmoid  then we require eigenvalue decay
O(i−√
For higher depth networks  bounds on the required eigenvalue decay can be derived from struc-
tural results in [16]. Without taking an assumption  the fastest known algorithms for learning the
above networks run in time exponential in the number of hidden units and accuracy parameter (but
polynomial in the dimension) [16].
Our proof develops a novel approach for bounding the generalization error of kernel methods 
namely we develop compression schemes tailor-made for classiﬁers induced by kernel-based re-
gression  as opposed to current Rademacher-complexity based approaches. Roughly  a compression
scheme is a mapping from a training set S to a small subsample S(cid:48) and side-information I. Given
this compressed version of S  the decompression algorithm should be able to generate a classiﬁer h.
In recent work  David  Moran and Yehudayoff [13] have observed that if the size of the compression
is much less than m (the number of samples)  then the empirical error of h on S is close to its true
error with high probability.
At the core of our compression scheme is a method for giving small description length (i.e.  o(m)
bit complexity)  approximate solutions to instances of kernel ridge regression. Even though we
assume K has decaying eigenvalues  K is neither sparse nor low-rank  and even a single column
or row of K has bit complexity at least m  since K is an m × m matrix! Nevertheless  we can
prove that recent tools from Nyström sampling [28] imply a type of sparsiﬁcation for solutions

(cid:96)/)).

√

(cid:96) log(

2

of certain regression problems involving K. Additionally  using preconditioning  we can bound
the bit complexity of these solutions and obtain the desired compression scheme. At each stage
we must ensure that our compressed solutions do not lose too much accuracy  and this involves
carefully analyzing various matrix approximations. Our methods are the ﬁrst compression-based
generalization bounds for kernelized regression.

Related Work. Kernel methods [30] such as SVM  kernel ridge regression and kernel PCA have
been extensively studied due to their excellent performance and strong theoretical properties. For
large data sets  however  many kernel methods become computationally expensive. The literature
on approximating the Gram matrix with the overarching goal of reducing the time and space com-
plexity of kernel methods is now vast. Various techniques such as random sampling [39]  subspace
embedding [2]  and matrix factorization [15] have been used to ﬁnd a low-rank approximation that
is efﬁcient to compute and gives small approximation error. The most relevant set of tools for our
paper is Nyström sampling [39  14]  which constructs an approximation of K using a subset of
the columns indicated by a selection matrix S to generate a positive semi-deﬁnite approximation.
Recent work on leverage scores have been used to improve the guarantees of Nyström sampling in
order to obtain linear time algorithms for generating these approximations [28].
The novelty of our approach is to use Nyström sampling in conjunction with compression schemes
to give a new method for giving provable generalization error bounds for kernel methods. Compres-
sion schemes have typically been studied in the context of classiﬁcation problems in PAC learning
and for combinatorial problems related to VC dimension [23  24]. Only recently some authors
considered compression schemes in a general  real-valued learning scenario [13]. Cotter  Shalev-
Shwartz  and Srebro have studied compression for classiﬁcation using SVMs to prove that for gen-
eral distributions  compressing classiﬁers with low generalization error is not possible [9].
The general phenomenon of eigenvalue decay of the Gram matrix has been studied from both a the-
oretical and applied perspective. Some empirical studies of eigenvalue decay and related discussion
can be found in [27  35  38]. There has also been prior work relating eigenvalue decay to gen-
eralization error in the context of SVMs or Kernel PCA (e.g.  [29  35]). Closely related notions to
eigenvalue decay are that of local Rademacher complexity due to Bartlett  Bousquet  and Mendelson
[4] (see also [5]) and that of effective dimensionality due to Zhang [42].
The above works of Bartlett et al. and Zhang give improved generalization bounds via data-
dependent estimates of eigenvalue decay of the kernel. At a high level  the goal of these works
is to work under an assumption on the effective dimension and improve Rademacher-based general-
ization error bounds from 1/
m to 1/m (m is the number of samples) for functions embedded in a
RKHS of unit norm. These works do not address the main obstacle of this paper  however  namely
overcoming the complexity of the norm of the approximating RKHS. Their techniques are mostly
incomparable even though the intent of using effective dimension as a measure of complexity is the
same.
Shamir has shown that for general linear prediction problems with respect to square-loss and norm
bound B  a sample complexity of Ω(B) is required for gradient-based methods [33]. Our work
shows that eigenvalue decay can dramatically reduce this dependence  even in the context of kernel
regression where we want to run in time polynomial in n  the dimension  rather than the (much
larger) dimension of the RKHS.

√

Recent work on Learning Neural Networks. Due in part to the recent exciting developments in
deep learning  there have been several works giving provable results for learning neural networks
with various activations (threshold  sigmoid  or ReLU). For the most part  these results take various
assumptions on either 1) the distribution (e.g.  Gaussian or Log-Concave) or 2) the structure of the
network architecture (e.g. sparse  random  or non-overlapping weight vectors) or both and often have
a bad dependence on one or more of the relevant parameters (dimension  number of hidden units 
depth  or accuracy). Another way to restrict the problem is to work only in the noiseless/realizable
setting. Works that fall into one or more of these categories include [20  44  40  17  31  41  11].
Kernel methods have been applied previously to learning neural networks [43  26  16  12]. The
current broadest class of networks known to be learnable in fully polynomial-time in all parameters
with no assumptions is due to Goel et al. [16]  who showed how to learn a sum of one hidden layer of
sigmoids over the domain of Sn−1  the unit sphere in n dimensions. We are not aware of other prior

3

work that takes only a distributional assumption on the marginal and achieves fully polynomial-time
algorithms for even simple networks (for example  one hidden layer of ReLUs).
Much work has also focused on the ability of gradient descent to succeed in parameter estimation
for learning neural networks under various assumptions with an intense focus on the structure of
local versus global minima [8  18  7  37]. Here we are interested in the traditional task of learning in
the non-realizable or agnostic setting and allow ourselves to output a hypothesis outside the function
class (i.e.  we allow improper learning). It is well known that for even simple neural networks  for
example for learning a sigmoid with respect to square-loss  there may be many bad local minima
[1]. Improper learning allows us to avoid these pitfalls.

2 Preliminaries
Notation. The input space is denoted by X and the output space is denoted by Y. Vectors are rep-
resented with boldface letters such as x. We denote a kernel function by kψ(x  x(cid:48)) = (cid:104)ψ(x)  ψ(x(cid:48))(cid:105)
where ψ is the associated feature map and for the kernel and Kψ is the corresponding reproducing
kernel Hilbert space (RKHS). For necessary background material on kernel methods we refer the
reader to [30].

Selection and Compression Schemes. It is well known that in the context of PAC learning Boolean
function classes  a suitable type of compression of the training data implies learnability [25]. Perhaps
surprisingly  the details regarding the relationship between compression and ceratin other real-valued
learning tasks have not been worked out until very recently. A convenient framework for us will be
the notion of compression and selection schemes due to David et al. [13].
A selection scheme is a pair of maps (κ  ρ) where κ is the selection map and ρ is the reconstruction
map. κ takes as input a sample S = ((x1  y1)  . . .   (xm  ym)) and outputs a sub-sample S(cid:48) and a
ﬁnite binary string b as side information. ρ takes this input and outputs a hypothesis h. The size of
the selection scheme is deﬁned to be k(m) = |S(cid:48)| + |b|. We present a slightly modiﬁed version of
the deﬁnition of an approximate compression scheme due to [13]:
Deﬁnition 3 ((  δ)-approximate agnostic compression scheme). A selection scheme (κ  ρ) is an
(  δ)-approximate agnostic compression scheme for hypothesis class H and sample satisfying
property P if for all samples S that satisfy P with probability 1 − δ  f = ρ(κ(S)) satisﬁes

(cid:80)m
i=1 l(f (xi)  yi) ≤ minh∈H ((cid:80)m

i=1 l(h(xi)  yi)) + .

Compression has connections to learning in the general loss setting through the following theorem
which shows that as long as k(m) is small  the selection scheme generalizes.
Theorem 4 (Theorem 30.2 [32]  Theorem 3.2 [13]). Let (κ  ρ) be a selection scheme of size k =
k(m)  and let AS = ρ(κ(S)). Given m i.i.d. samples drawn from any distribution D such that
k ≤ m/2  for constant bounded loss function l : Y(cid:48) × Y → R+ with probability 1 − δ  we have

(cid:32)

(cid:118)(cid:117)(cid:117)(cid:116) ·

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤

m(cid:88)

i=1

1
m

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E(x y)∼D[l(AS (x)  y)] − m(cid:88)

i=1

l(AS (xi)  yi)

l(AS (xi)  yi)

+ 

where  = 50 · k log(m/k)+log(1/δ)

m

.

3 Problem Overview
In this section we give a general outline for our main result. Let S = {(x1  y1)  . . .   (xm  ym)} be a
training set of samples drawn i.i.d. from some arbitrary distribution D on X × [0  1] where X ⊆ Rn.
Let us consider a concept class C such that for all c ∈ C and x ∈ X we have c(x) ∈ [0  1]. We
wish to learn the concept class C with respect to the square loss  that is  we wish to ﬁnd c ∈ C that
approximately minimizes E(x y)∼D[(c(x) − y)2]. A common way of solving this is by solving the
empirical minimization problem (ERM) given below and subsequently proving that it generalizes.

4

Optimization Problem 1

minimize

c∈C

1
m

m(cid:88)

(c(xi) − yi)2

i=1

Unfortunately  it may not be possible to efﬁciently solve the ERM in polynomial-time due to issues
such as non-convexity. A way of tackling this is to show that the concept class can be approximately
minimized by another hypothesis class of linear functions in a high dimensional feature space (this
in turn presents new obstacles for proving generalization-error bounds  which is the focus of this
paper).
Deﬁnition 5 (-approximation). Let C1 and C2 be function classes mapping domain X to R. C1 is -
approximated by C2 if for every c ∈ C1 there exists c(cid:48) ∈ C2 such that for all x ∈ X  |c(x)−c(cid:48)(x)| ≤ .
Suppose C can be -approximated in the above sense by the hypothesis class Hψ = {x →
(cid:104)v  ψ(x)(cid:105)|v ∈ Kψ (cid:104)v  v(cid:105) ≤ B} for some B and kernel function kψ. We further assume that the
kernel is bounded  that is  |kψ(x  x’)| ≤ M for some M > 0 for all x  x’ ∈ X . Thus  the problem
relaxes to the following 

Optimization Problem 2

minimize

v∈Kψ

1
m

m(cid:88)

((cid:104)v  ψ(xi)(cid:105) − yi)2

i=1

subject to

(cid:104)v  v(cid:105) ≤ B

v∗ =(cid:80)m

Using the Representer theorem  we have that the optimum solution for the above is of the form
i=1 αiψ(xi) for some α ∈ Rn. Denoting the sample kernel matrix be K such that Ki j =

kψ(xi  xj)  the above optimization problem is equivalent to the following optimization problem 

Optimization Problem 3

minimize

α∈Rm

||Kα − Y ||2

2

1
m

subject to

αT Kα ≤ B

√

where Y is the vector corresponding to all yi and ||Y ||∞ ≤ 1 since ∀i ∈ [m]  yi ∈ [0  1]. Let αB
be the optimal solution of the above problem. This is known to be efﬁciently solvable in poly(m  n)
time as long as the kernel function is efﬁciently computable.
Applying Rademacher complexity bounds to Hψ yields generalization error bounds that decrease 
roughly  on the order of B/
m (c.f. Supplemental 1.1). If B is exponential in 1/  the accuracy
parameter  or in n  the dimension  as in the case of bounded depth networks of ReLUs  then this
dependence leads to exponential sample complexity. As mentioned in Section 1  in the context of
eigenvalue decay  various results [42  4  5] have been obtained to improve the dependence of B/
m
to B/m  but little is known about improving the dependence on B.
Our goal is to show that eigenvalue decay of the empirical Gram matrix does yield generaliza-
tion bounds with better dependence on B. The key is to develop a novel compression scheme for
kernelized ridge regression. We give a step-by-step analysis for how to generate an approximate 
compressed version of the solution to Optimization Problem 3. Then  we will carefully analyze the
bit complexity of our approximate solution and realize our compression scheme. Finally  we can put
everything together and show how quantitative bounds on eigenvalue decay directly translate into
compressions schemes with low generalization error.

√

4 Compressing the Kernel Solution

Through a sequence of steps  we will sparsify α to ﬁnd a solution of much smaller bit complexity
that is still an approximate solution (to within a small additive error). The quality and size of the
approximation will depend on the eigenvalue decay.

5

Lagrangian Relaxation. We relax Optimization Problem 3 and consider the Lagrangian version of
the problem to account for the norm bound constraint. This version is convenient for us  as it has a
nice closed-form solution.
Optimization Problem 4

minimize

α∈Rm

||Kα − Y ||2

2 + λαT Kα

1
m

We will later set λ such that the error of considering this relaxation is small. It is easy to see that the
optimal solution for the above lagrangian version is α = (K + λmI)

−1 Y .

Preconditioning. To avoid extremely small or non-zero eigenvalues  we consider a perturbed ver-
sion of K  Kγ = K + γmI. This gives us that the eigenvalues of Kγ are always greater than
or equal to γm. This property is useful for us in our later analysis. Henceforth  we consider the
following optimization problem on the perturbed version of K:
Optimization Problem 5

minimize

α∈Rm

||Kγα − Y ||2

2 + λαT Kγα

1
m

The optimal solution for perturbed version is αγ = (Kγ + λmI)

−1 Y = (K + (λ + γ)mI)

−1 Y .

Sparsifying the Solution via Nyström Sampling. We will now use tools from Nyström Sampling
to sparsify the solution obtained from Optimzation Problem 5. To do so  we ﬁrst recall the deﬁnition
of effective dimension or degrees of freedom for the kernel [42]:
Deﬁnition 6 (η-effective dimension). For a positive semideﬁnite m × m matrix K and parameter
η  the η-effective dimension of K is deﬁned as dη(K) = tr(K(K + ηmI)−1).
Various kernel approximation results have relied on this quantity  and here we state a recent result
due to [28] who gave the ﬁrst application independent result that shows that there is an efﬁcient way
of computing a set of columns of K such that ¯K  a matrix constructed from the columns is close in
terms of 2-norm to the matrix K. More formally 
Theorem 7 ([28]). For kernel matrix K  there exists an algorithm that gives a set of
O (dη(K) log (dη(K)/δ)) columns  such that ¯K = KS(ST KS)†ST K where S is the matrix that
selects the speciﬁc columns  satisﬁes with probability 1 − δ  ¯K (cid:22) K (cid:22) ¯K + ηmI.
It can be shown that ¯K is positive semi-deﬁnite. Also  the above implies ||K − ¯K||2 ≤ ηm. We use
the decay to approximate the Kernel matrix with a low-rank matrix constructed using the columns
of K. Let ¯Kγ be the matrix obtained by applying Theorem 7 to Kγ for η > 0 and consider the
following optimization problem 
Optimization Problem 6

The optimal solution for the above is ¯αγ =(cid:0) ¯Kγ + λmI(cid:1)−1

α∈Rm

minimize

Y . Since ¯Kγ = KγS(ST KγS)†ST Kγ 
solving for the above enables us to get a solution α∗ = S(ST KγS)†ST Kγ ¯αγ  which is a k-sparse
vector for k = O (dη(Kγ) log (dη(Kγ)/δ)).

|| ¯Kγα − Y ||2

2 + λαT ¯Kγα

1
m

Bounding the Error of the Sparse Solution. We bound the additional error incurred by our sparse
hypothesis α∗ compared to αB. To do so  we bound the error for each of the approximations: spar-
siﬁcation  preconditioning and lagrangian relaxation and then combine them to give the following
theorem.
2 ≤
Theorem 8 (Total Error). For λ = 2
m||KαB − Y ||2
1

729B and γ ≤ 3

m||Kγα∗ − Y ||2

81B   η ≤ 3

729B   we have 1

2 + .

6

Computing the Sparsity of the Solution. To compute the sparsity of the solution  we need to bound
dη(Kβ). We consider the following different eigenvalue decays.
Deﬁnition 9 (Eigenvalue Decay). Let the real eigenvalues of a symmetric m × m matrix A be
denoted by λ1 ≥ ··· ≥ λm.
1. A is said to have (C  p)-polynomial eigenvalue decay if for all i ∈ {1  . . .   m}  λi ≤ Ci−p.
2. A is said to have C-exponential eigenvalue decay if for all i ∈ {1  . . .   m}  λi ≤ Ce−i.
Note that in the above deﬁnitions C and p are not necessarily constants. We allow C and p to
depend on other parameters (the choice of these parameters will be made explicit in subsequent
theorem statements). We can now bound the effective dimension in terms of eigenvalue decay:
Theorem 10 (Bounding effective dimension). For γm ≤ η 

1. If K/m has (C  p)-polynomial eigenvalue decay for p > 1 then dη(Kγ) ≤(cid:16) C

(cid:17)1/p

+ 2.

(p−1)η

2. If K/m has C-exponential eigenvalue decay then dη(Kγ) ≤ log

5 Compression Scheme

(cid:16) C

(cid:17)

(e−1)η

+ 2.

The above analysis gives us a sparse solution for the problem and  in turn  an -approximation for
the error on the overall sample S with probability 1 − δ. We can now fully deﬁne our compression
scheme for the hypothesis class Hψ with respect to samples satisfying the eigenvalue decay property.
Selection Scheme κ: Given input S = (xi  yi)m
i=1 
1. Use RLS-Nyström Sampling [28] to compute ¯Kγ = KγS(ST KγS)†ST Kγ for η = 3
γ = 3

5832Bm. Let I be the sub-sample corresponding to the columns selected using S.

5832B and

324B to get ¯αγ.

2. Solve Optimization Problem 6 for λ = 2
3. Compute the |I|-sparse vector α∗ = S(ST KγS)†ST Kγ ¯αγ = K−1
eigenvalues are non-zero).
4. Output subsample I along with ˜α∗ which is α∗ truncated to precision
4M|I| per non-zero index.
subsample I and
˜α∗ 
Reconstruction Scheme ρ:
output hypothesis 
hS (x) = clip0 1(wT ˜α∗) where w is a vector with entries K(xi  x) + γm1[x = xi] for
i ∈ I and 0 otherwise where γ = 3
5832Bm. Note  clipa b(x) = max(a  min(b  x)) for some a < b.

¯Kγ ¯αγ (Kγ is invertible as all

Given input

γ



The following theorem shows that the above is a compression scheme for Hψ.
Theorem 11. (κ  ρ) is an (  δ)-approximate agnostic compression scheme for the hypothesis class
Hψ for sample S of size k(m    δ  B  M ) = O
where d is the
η-effective dimension of Kγ for η = 3

d log(cid:0) d

(cid:1) log

5832B and γ = 3

(cid:16)√

5832Bm .

(cid:17)(cid:17)

(cid:16)

mBM d log(d/δ)

4

δ

6 Putting It All Together: From Compression to Learning

We now present our ﬁnal algorithm: Compressed Kernel Regression (Algorithm 1). Note that the
algorithm is efﬁcient and takes at most O(m3) time.
For our learnability result  we restrict distributions to those that satisfy eigenvalue decay.
Deﬁnition 12 (Distribution Satisfying Eigenvalue Decay). Consider distribution D over X and
kernel function kψ. Let S be a sample drawn i.i.d. from the distribution D and K be the empirical
gram matrix corresponding to kernel function kψ on S.
1. D is said to satisfy (C  p  N )-polynomial eigenvalue decay if with probability 1 − δ over the
drawn sample of size m ≥ N  K/m satisﬁes (C  p)-polynomial eigenvalue decay.

7

Algorithm 1 Compressed Kernel Regression

1: Using RLS-Nyström Sampling [28] with input (Kγ  ηm) for γ =

Input: Samples S = (xi  yi)m
maximum kernel function value M on X .
compute ¯Kγ = KγS(ST KγS)†ST Kγ. Let I be the subsample corresponding to the columns
selected using S. Note that the number of columns selected depends on the η effective dimen-
sion of Kγ.

i=1  gram matrix K on S  constants   δ > 0  norm bound B and
5832Bm and η = 3

5832B

3

324B to get ¯αγ over S
2: Solve Optimization Problem 6 for λ = 2
3: Compute α∗ = S(ST KγS)†ST Kγ ¯αγ = K−1
4: Compute ˜α∗ by truncating each entry of α∗ up to precision

¯Kγ ¯αγ

γ



4M|I|

Output: hS such that for all x ∈ X   hS (x) = clip0 1(wT ˜α∗) where w is a vector with entries
K(xi  x) + γm1[x = xi] for i ∈ I and 0 otherwise.

(cid:0)E(x y)∼D(c(x) − y)2(cid:1) + 20 + 
(cid:0)E(x y)∼D(c(x) − y)2(cid:1) + 20 + 

2. D is said to satisfy (C  N )-exponential eigenvalue decay if with probability 1 − δ over the drawn
sample of size m ≥ N  K/m satisﬁes C-exponential eigenvalue decay.
Our main theorem proves generalization of the hypothesis output by Algorithm 1 for distributions
satisfying eigenvalue decay in the above sense.
Theorem 13 (Formal for Theorem 1). Fix function class C with output bounded in [0  1] and
M-bounded kernel function kψ such that C is 0-approximated by Hψ = {x → (cid:104)v  ψ(x)(cid:105)|v ∈
Kψ (cid:104)v  v(cid:105) ≤ B} for some ψ  B. Consider a sample S = {(xi  yi)m
from D on
i=1} drawn i.i.d.
X × [0  1]. There exists an algorithm A that outputs hypothesis hS = A(S)  such that 
1. If DX satisﬁes (C  p  m)-polynomial eigenvalue decay with probability 1 − δ/4 then with proba-
bility 1 − δ for m = ˜O((CB)1/p log(M ) log(1/δ)/2+3/p) 

E(x y)∼D(hS (x) − y)2 ≤ min
c∈C

2. If DX satisﬁes (C  m)-exponential eigenvalue decay with probability 1−δ/4 then with probability
1 − δ for m = ˜O(log CB log(M ) log(1/δ)/2) 
E(x y)∼D(hS (x) − y)2 ≤ min
c∈C

Algorithm A runs in time poly(m  n).
Remark: The above theorem can be extended to different rates of eigenvalue decay. For example 
for ﬁnite rank r the obtained bound is independent of B but dependent instead on r. Also  as in the
proof of Theorem 10  it sufﬁces for the eigenvalue decay to hold only after sufﬁciently large i.

7 Learning Neural Networks

Here we apply our main theorem to the problem of learning neural networks. For technical deﬁni-
tions of neural networks  we refer the reader to [43].
Deﬁnition 14 (Class of Neural Networks [16]). Let N [σ  D  W  T ] be the class of fully-connected 
feed-forward networks with D hidden layers  activation function σ and quantities W and T de-
scribed as follows:
1. Weight vectors in layer 0 have 2-norm bounded by T .
2. Weight vectors in layers 1  . . .   D have 1-norm bounded by W .
3. For each hidden unit σ(w· z) in the network  we have |w· z| ≤ T (by z we denote the input feeding
into unit σ from the previous layer).

We consider activation functions σrelu(x) = max(0  x) and σsig = 1
1+e−x   though other activation
functions ﬁt within our framework. Goel et al. [16] showed that the class of ReLUs/Sigmoids along
with their compositions can be approximated by linear functions in a high dimensional Hilbert space

8

(corresponding to a particular type of polynomial kernel). As mentioned earlier  the sample com-
plexity of prior work depends linearly on B  which  for even a single ReLU  is exponential in 1/.
Assuming sufﬁciently strong eigenvalue decay  we can show that we can obtain fully polynomial
time algorithms for the above classes.
Theorem 15. For   δ > 0  consider D on Sn−1 × [0  1] such that 
1. For Crelu = N [σrelu  0 ·  1]  DX satisﬁes (C  p  m)-polynomial eigenvalue decay for p ≥ ξ/ 
2. For Crelu−D = N [σrelu  D  W  T ]  DX satisﬁes (C  p  m)-polynomial eigenvalue decay for
p ≥ (ξW DDT /)D 
3. For Csig−D = N [σsig  D  W  T ]  DX satisﬁes (C  p  m)-polynomial eigenvalue decay for p ≥
(ξT log(W DD/)))D 
where DX is the marginal distribution on X = Sn−1  ξ > 0 is some sufﬁciently large constant and
C ≤ (n · 1/ · log(1/δ))ζp for some constant ζ > 0. The value of m is obtained from Theorem 13
as m = ˜O(C 1/p2+3/p).
Each decay assumption above implies an algorithm for agnostically learning the corresponding
class on Sn−1 × [0  1] with respect to the square loss in time poly(n  1/  log(1/δ)).

Note that assuming an exponential eigenvalue decay (stronger than polynomial) will result in efﬁ-
cient learnability for much broader classes of networks.
Since it is not known how to agnostically learn even a single ReLU with respect to arbitrary distribu-
tions on Sn−1 in polynomial-time4  much less a network of ReLUs  we state the following corollary
highlighting the decay we require to obtain efﬁcient learnability for simple networks:
Corollary 16 (Restating Corollary 2). Let C be the class of all fully-connected networks of ReLUs
with one-hidden layer of size (cid:96) feeding into a ﬁnal output ReLU activation where the 2-norms of
all weight vectors are bounded by 1. Then  (suppressing the parameter m for simplicity)  assuming
(C  i−(cid:96)/)-polynomial eigenvalue decay for C = poly(n  1/  (cid:96))  C is learnable in polynomial time
with respect to square loss on Sn−1. If ReLU is replaced with sigmoid  then we require eigenvalue
decay of i−√

(cid:96)/).

(cid:96) log(

√

8 Conclusions and Future Work

We have proposed the ﬁrst set of distributional assumptions that guarantee fully polynomial-time
algorithms for learning expressive classes of neural networks (without restricting the structure of
the network). The key abstraction was that of a compression scheme for kernel approximations 
speciﬁcally Nyström sampling. We proved that eigenvalue decay of the Gram matrix reduces the
dependence on the norm B in the kernel regression problem.
Prior distributional assumptions  such as the underlying marginal equaling a Gaussian  neither lead
to fully polynomial-time algorithms nor are representative of real-world data sets5. Eigenvalue de-
cay  on the other hand  has been observed in practice and does lead to provably efﬁcient algorithms
for learning neural networks.
A natural criticism of our assumption is that the rate of eigenvalue decay we require is too strong.
In some cases  especially for large depth networks with many hidden units  this may be true6. Note 
however  that our results show that even moderate eigenvalue decay will lead to improved algo-
rithms. Further  it is quite possible our assumptions can be relaxed. An obvious question for future
work is what is the minimal rate of eigenvalue decay needed for efﬁcient learnability? Another di-
rection would be to understand how these eigenvalue decay assumptions relate to other distributional
assumptions.

4Goel et al. [16] show that agnostically learning a single ReLU over {−1  1}n is as hard as learning sparse

parities with noise. This reduction can be extended to the case of distributions over Sn−1 [3].

5Despite these limitations  we still think uniform or Gaussian assumptions are worthwhile and have provided

highly nontrivial learning results.

6It is useful to keep in mind that agnostically learning even a single ReLU with respect to all distributions
seems computationally intractable  and that our required eigenvalue decay in this case is only a function of the
accuracy parameter .

9

Acknowledgements. We would like to thank Misha Belkin and Nikhil Srivastava for very helpful
conversations regarding kernel ridge regression and eigenvalue decay. We also thank Daniel Hsu 
Karthik Sridharan  and Justin Thaler for useful feedback. The analogy between eigenvalue decay
and power-law graphs is due to Raghu Meka.

References
[1] Peter Auer  Mark Herbster  and Manfred K. Warmuth. Exponentially many local minima for
single neurons. In Advances in Neural Information Processing Systems  volume 8  pages 316–
322. The MIT Press  1996.

[2] Haim Avron  Huy Nguyen  and David Woodruff. Subspace embeddings for the polynomial

kernel. In Advances in Neural Information Processing Systems  pages 2258–2266  2014.

[3] Peter Bartlett  Daniel Kane  and Adam Klivans. personal communication.
[4] Peter L. Bartlett  Olivier Bousquet  and Shahar Mendelson. Local rademacher complexities.

33(4)  August 16 2005.

[5] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research  3:463–482  2002.

[6] Pawel Brach  Marek Cygan  Jakub Lacki  and Piotr Sankowski. Algorithmic complexity of

power law networks. CoRR  abs/1507.02426  2015.

[7] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with

gaussian inputs. CoRR  abs/1702.07966  2017.

[8] Anna Choromanska  Mikael Henaff  Michaël Mathieu  Gérard Ben Arous  and Yann LeCun.
In AISTATS  volume 38 of JMLR Workshop and

The loss surfaces of multilayer networks.
Conference Proceedings. JMLR.org  2015.

[9] Andrew Cotter  Shai Shalev-Shwartz  and Nati Srebro. Learning optimally sparse support
vector machines. In Proceedings of the 30th International Conference on Machine Learning
(ICML-13)  pages 266–274  2013.

[10] Amit Daniely. Complexity theoretic limitations on learning halfspaces. In STOC  pages 105–

117. ACM  2016.

[11] Amit Daniely. SGD learns the conjugate kernel class of the network. CoRR  abs/1702.08503 

2017.

[12] Amit Daniely  Roy Frostig  and Yoram Singer. Toward deeper understanding of neural net-
works: The power of initialization and a dual view on expressivity. In NIPS  pages 2253–2261 
2016.

[13] Oﬁr David  Shay Moran  and Amir Yehudayoff. On statistical learning via the lens of com-

pression. arXiv preprint arXiv:1610.03592  2016.

[14] Petros Drineas and Michael W Mahoney. On the nyström method for approximating a
journal of machine learning research 

gram matrix for improved kernel-based learning.
6(Dec):2153–2175  2005.

[15] Petros Drineas  Michael W Mahoney  and S Muthukrishnan. Relative-error cur matrix decom-

positions. SIAM Journal on Matrix Analysis and Applications  30(2):844–881  2008.

[16] Surbhi Goel  Varun Kanade  Adam Klivans  and Justin Thaler. Reliably learning the relu in

polynomial time. arXiv preprint arXiv:1611.10258  2016.

[17] Majid Janzamin  Hanie Sedghi  and Anima Anandkumar. Beating the perils of non-
convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint
arXiv:1506.08473  2015.

[18] Kenji Kawaguchi. Deep learning without poor local minima. In NIPS  pages 586–594  2016.
[19] Adam R. Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space.
In APPROX-RANDOM  volume 28 of LIPIcs  pages 793–809. Schloss Dagstuhl - Leibniz-
Zentrum fuer Informatik  2014.

[20] Adam R. Klivans and Raghu Meka. Moment-matching polynomials. Electronic Colloquium

on Computational Complexity (ECCC)  20:8  2013.

10

[21] Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersec-

tions of halfspaces. J. Comput. Syst. Sci  75(1):2–12  2009.

[22] Anton Krohmer. Finding Cliques in Scale-Free Networks. Master’s thesis  Saarland University 

Germany  2012.

[23] Dima Kuzmin and Manfred K. Warmuth. Unlabeled compression schemes for maximum

classes. Journal of Machine Learning Research  8:2047–2081  2007.

[24] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical

report  Technical report  University of California  Santa Cruz  1986.

[25] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical

report  1986.

[26] Roi Livni  Shai Shalev-Shwartz  and Ohad Shamir. On the computational efﬁciency of training
In Advances in Neural Information Processing Systems  pages 855–863 

neural networks.
2014.

[27] Siyuan Ma and Mikhail Belkin. Diving into the shallows: a computational perspective on

large-scale shallow learning. CoRR  abs/1703.10622  2017.

[28] Cameron Musco and Christopher Musco. Recursive sampling for the nyström method. arXiv

preprint arXiv:1605.07583  2016.

[29] B. Schölkopf  J. Shawe-Taylor  AJ. Smola  and RC. Williamson. Generalization bounds via

eigenvalues of the gram matrix. Technical Report 99-035  NeuroCOLT  1999.

[30] Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT press  2002.

[31] Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with

sparse connectivity. arXiv preprint arXiv:1412.2693  2014.

[32] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

[33] Ohad Shamir. The sample complexity of learning linear predictors with the squared loss.

Journal of Machine Learning Research  16:3475–3486  2015.

[34] Ohad Shamir. Distribution-speciﬁc hardness of learning neural networks. arXiv preprint

arXiv:1609.01037  2016.

[35] John Shawe-Taylor  Christopher KI Williams  Nello Cristianini  and Jaz Kandola. On the
eigenspectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transac-
tions on Information Theory  51(7):2510–2522  2005.

[36] Le Song  Santosh Vempala  John Wilmes  and Bo Xie. On the complexity of learning neural

networks. arXiv preprint arXiv:1707.04615  2017.

[37] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guar-

antees for multilayer neural networks. CoRR  abs/1605.08361  2016.

[38] Ameet Talwalkar and Afshin Rostamizadeh. Matrix coherence and the nystrom method. CoRR 

abs/1408.2044  2014.

[39] Christopher KI Williams and Matthias Seeger. Using the nyström method to speed up ker-
In Proceedings of the 13th International Conference on Neural Information

nel machines.
Processing Systems  pages 661–667. MIT press  2000.

[40] Bo Xie  Yingyu Liang  and Le Song. Diversity leads to generalization in neural networks.

CoRR  abs/1611.03131  2016.

[41] Qiuyi Zhang  Rina Panigrahy  and Sushant Sachdeva. Electron-proton dynamics in deep learn-

ing. CoRR  abs/1702.00458  2017.

[42] Tong Zhang. Effective dimension and generalization of kernel learning. In Advances in Neural

Information Processing Systems  pages 471–478  2003.

[43] Yuchen Zhang  Jason D Lee  and Michael I Jordan. l1-regularized neural networks are improp-
erly learnable in polynomial time. In International Conference on Machine Learning  pages
993–1001  2016.

[44] Yuchen Zhang  Jason D. Lee  Martin J. Wainwright  and Michael I. Jordan. Learning halfs-

paces and neural networks with random initialization. CoRR  abs/1511.07948  2015.

11

,Bilal Piot
Matthieu Geist
Olivier Pietquin
Surbhi Goel
Adam Klivans
Stephen Gillen
Christopher Jung
Michael Kearns
Aaron Roth