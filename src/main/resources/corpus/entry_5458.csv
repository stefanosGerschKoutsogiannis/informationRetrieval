2019,The Geometry of Deep Networks: Power Diagram Subdivision,We study the geometry of deep (neural) networks (DNs) with piecewise affine and convex nonlinearities. 
The layers of such DNs have been shown to be max-affine spline operators (MASOs) that partition their input space and apply a region-dependent affine mapping to their input to produce their output.
We demonstrate that each MASO layer's input space partitioning corresponds to a power diagram (an extension of the classical Voronoi tiling) with a number of regions that grows exponentially with respect to the number of units (neurons).
We further show that a composition of MASO layers (e.g.  the entire DN) produces a progressively subdivided power diagram and provide its analytical form. 
The subdivision process constrains the affine maps on the potentially exponentially many power diagram regions with respect to the number of neurons to greatly reduce their complexity. 
For classification problems  we obtain a formula for a MASO DN's decision boundary in the input space plus a measure of its curvature that depends on the DN's nonlinearities  weights  and architecture. 
Numerous numerical experiments support and extend our theoretical results.,The Geometry of Deep Networks:

Power Diagram Subdivision

Randall Balestriero  Romain Cosentino  Behnaam Aazhang  Richard G. Baraniuk

Rice University

Houston  Texas  USA

Abstract

We study the geometry of deep (neural) networks (DNs) with piecewise afﬁne and
convex nonlinearities. The layers of such DNs have been shown to be max-afﬁne
spline operators (MASOs) that partition their input space and apply a region-
dependent afﬁne mapping to their input to produce their output. We demonstrate
that each MASO layer’s input space partition corresponds to a power diagram
(an extension of the classical Voronoi tiling) with a number of regions that grows
exponentially with respect to the number of units (neurons). We further show
that a composition of MASO layers (e.g.  the entire DN) produces a progressively
subdivided power diagram and provide its analytical form. The subdivision process
constrains the afﬁne maps on the potentially exponentially many power diagram
regions with respect to the number of neurons to greatly reduce their complexity.
For classiﬁcation problems  we obtain a formula for the DN’s decision boundary in
the input space plus a measure of its curvature that depends on the DN’s architecture 
nonlinearities  and weights. Numerous numerical experiments support and extend
our theoretical results.

Introduction

1
Today’s machine learning landscape is dominated by deep (neural) networks (DNs)  which are
compositions of a large number of simple parameterized linear and nonlinear transformations. Deep
networks perform surprisingly well in a host of applications; however  surprisingly little is known
about why they work so well.

Recently  [BB18a  BB18b] connected a large class of DNs to a special kind of spline  which enables
one to view and analyze the inner workings of a DN using tools from approximation theory and
functional analysis. In particular  when the DN is constructed using convex and piecewise afﬁne
nonlinearities (such as ReLU  Leaky- ReLU  max-pooling  etc.)  then its layers can be written as
max-afﬁne spline operators (MASOs). An important consequence for DNs is that each layer partitions
its input space into a set of regions and then processes inputs via a simple afﬁne transformation
that changes continuously from region to region. Understanding the geometry of the layer partition
regions – and how the layer partition regions combine into the DN input partition – is thus key to
understanding the operation of DNs.

There has only been limited work in the geometry of deep networks. The originating MASO
work of [BB18a  BB18b] focused on the analytical form of the region-dependent afﬁne maps and
empirical statistics of the partition without studying the structure of the partition or its construction
through depth. The work of [WBB19] empirically studied the partition highlighting the fact that
knowledge of the region in which each input lies is sufﬁcient to reach high performance. Other
works have focused on the properties of the partition  such as upper bounding the number of regions
[MPCB14  RPK+17  HR19]. An explicit characterization of the input space partition of one hidden
layer DNs with ReLU activation has been developed in [ZBH+16] by means of tropical geometry.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In this paper  we adopt a computational and combinatorial geometry [PA11  PS12] perspective of
MASO-based DNs to derive the analytical form of the input-space partition of a DN unit  a DN layer 
and an entire end-to-end DN. Our results apply to any DN employing afﬁne transformations plus
piecewise afﬁne and convex nonlinearities.
We summarize our contributions as follows: [C1] We demonstrate that each MASO DN layer
partitions its input (feature map) space according to a power diagram (PD) (also known as a La-
guerre–Voronoi diagram) [AI] and derive the analytical formula of the PD (Section 3.2). [C2] We
demonstrate that the composition of the several MASO layers comprising a DN effects a subdivision
process that creates the overall DN input-space partition and provide the analytical form of the
partition (Section 4). [C3] We demonstrate how the centroids of the layer PDs can be efﬁciently
computed via backpropagation (Section 4.2)  which permits ready visualization of a PD. [C4] In
the classiﬁcation setting  we derive an analytical formula for a DN’s decision boundary in terms
of its input space partition (Section 5). The analytical formula enables us to characterize some key
geometrical properties of the boundary.
Our complete  analytical characterization of the input-space and feature map partition of MASO
DNs opens up new avenues to study the geometrical mechanisms behind their operation. Additional
background information  results  and proofs of the main results are provided in several appendices.

input signal x ∈ RD to the output prediction(cid:98)y ∈ RC. Current DNs can be written as a composition

2 Background
Deep Networks. A deep (neural) network (DN) is an operator fΘ with parameters Θ that maps an
of L intermediate layer mappings f ((cid:96)) : X((cid:96)−1) → X((cid:96)) ((cid:96) = 1  . . .   L) with X((cid:96)) ⊂ RD((cid:96)) that
transform an input feature map z((cid:96)−1) into the output feature map z((cid:96)) with the initializations
z(0)(x) := x and D(0) = D. The feature maps z((cid:96)) can be viewed equivalently as signals  tensors 
or ﬂattened vectors; we will use boldface to denote ﬂattened vectors (e.g.  z((cid:96))  x).
DNs can be constructed from a range of different linear and nonlinear operators. One important
linear operator is the fully connected operator that performs an arbitrary afﬁne transformation by
multiplying its input by the dense matrix W ((cid:96)) ∈ RD((cid:96))×D((cid:96)−1) and adding the arbitrary bias
vector b((cid:96))
W . Another linear operator is
the convolution operator in which the matrix W ((cid:96)) is replaced with a circulant block circulant
matrix denoted as C((cid:96)). One important nonlinear operator is the activation operator that applies
elementwise a nonlinearity σ such as ReLU σReLU(u) = max(u  0). Further examples are provided
in [GBC16]. We deﬁne a DN layer f ((cid:96)) as a single nonlinear DN operator composed with any (if
any) preceding linear operators that lie between it and the preceding nonlinear operator.
Max Afﬁne Spline Operators (MASOs). Work from [BB18a  BB18b] connects DN layers with
max-afﬁne spline operators (MASOs). A MASO is a continuous and convex operator w.r.t. each
output dimension S[A  B] : RD → RK that concatenates K independent max-afﬁne splines [MB09 
HD13]  with each spline formed from R afﬁne mappings. The MASO parameters consist of the
“slopes” A ∈ RK×R×D and the “offsets/biases” B ∈ RK×R.1 Given the layer input z((cid:96)−1)  a MASO
layer produces its output via

(cid:0)z((cid:96)−1)(x)(cid:1) := W ((cid:96))z((cid:96)−1)(x) + b((cid:96))

W ∈ RD((cid:96)) as in f ((cid:96))

W

[z((cid:96))(x)]k =

S[A((cid:96))  B((cid:96))](z((cid:96)−1)(x))

= max

r=1 ... R

k

[A((cid:96))]k r ·  z((cid:96)−1)(x)

+ [B((cid:96))]k r

  (1)

(cid:104)

(cid:105)

(cid:16)(cid:68)

(cid:69)

(cid:17)

where A((cid:96))  B((cid:96)) are the per-layer parameters  [A((cid:96))]k r · represents the vector formed from all of the
values of the last dimension of A((cid:96))  and [·]k denotes the value of a vector’s kth entry.
The key background result for this paper is that any DN layer f ((cid:96)) constructed from operators that
are piecewise-afﬁne and convex can be written as a MASO with parameters A((cid:96))  B((cid:96)) and output
dimension K = D((cid:96)). Hence  a DN is a composition of L MASOs [BB18a  BB18b]. For example  a
layer made of a fully connected operator followed by a leaky-ReLU with leakiness η has parameters
[A((cid:96))]k 1 · = [W ((cid:96))]k ·  [A((cid:96))]k 2 · = η[W ((cid:96))]k · for the slopes and [B((cid:96))]k 1 · = [b((cid:96))]k  [B((cid:96))]k 2 =
η[b((cid:96))]k for the biases.

1The three subscripts of the slopes tensor [A]k r d correspond to output k  partition region r  and input signal

index d. The two subscripts of the offsets/biases tensor [B]k r correspond to output k and partition region r.

2

Figure 1: Two equivalent representations of a power diagram (PD).
Top: The grey circles have centroid [µ]k · and radii [rad]k; each point
x is assigned to a speciﬁc region/cell according to the Laguerre distance
from the centroid  which is deﬁned as the length of the segment tangent
to and starting on the circle and reaching x. Bottom: A PD in RD
(here D = 2) is constructed by lifting the centroids [µ]k · up into an
additional dimension in RD+1 by the distance [rad]k and then ﬁnding
the Voronoi diagram (VD) of the augmented centroids ([µ]k ·  [rad]k)
in RD+1. The intersection of this higher-dimensional VD with the
originating space RD yields the PD.

A DN comprising L MASO layers is a non-convex but continuous afﬁne spline operator with an input
space partition and a partition-region-dependent afﬁne mapping. However  little is known analytically
about the input-space partition. The goal of this paper is to characterize the geometry of the MASO
partitions of the input space and the feature map spaces X((cid:96)).
Voronoi and Power Diagrams. A power diagram (PD)  also known as a Laguerre–Voronoi diagram
[AI]  is a generalization of the classical Voronoi diagram (VD).
Deﬁnition 1. A PD partitions a space X into R disjoint regions/cells Ω = {ω1  . . .   ωR} such that
∪R
r=1ωr = X  where each cell is obtained via ωr = {x ∈ X : r(x) = r}  r = 1  . . .   R  with

r(x) = arg min
k=1 ... R

(cid:107)x − [µ]k ·(cid:107)2 − [rad]k.

(2)

Input Space Power Diagram of a MASO Layer

The parameter [µ]k · is called the centroid  while [rad]k is called the radius. The distance minimized
in (2) is called the Laguerre distance [IIM85].
When the radii are equal for all k  a PD collapses to a VD. See Fig. 1 for two equivalent geometric
interpretations of a PD. For additional insights  see Appendix A and [PS12]. We will have the
occasion to use negative radii in our development below. Since arg mink (cid:107)x − [µ]k ·(cid:107)2 − [rad]k =
arg mink (cid:107)x − [µ]k ·(cid:107)2 − ([rad]k + ρ)  we can always apply a constant shift ρ to all of the radii to
make them positive .
3
Like any spline  it is the interplay between the (afﬁne) spline mappings and the input space partition
that work the magic in a MASO DN. Indeed  the partition opens up new geometric avenues to study
how a MASO-based DN clusters and organizes signals in a hierarchical fashion.
We now embark on a programme to fully characterize the geometry of the input space partition of a
MASO-based DN. We will proceed in three steps by studying the partition induced by i) one unit of a
single DN layer (Section 3.1)  ii) the combination of all units in a single layer (Section 3.2)  iii) the
composition of L layers that forms the complete DN (Section 4).
3.1 MAS Unit Power Diagram
A MASO layer combines K max afﬁne spline (MAS) units zk(x) to produce the layer output
z(x) = [z1(x)  . . .   zK(x)]T given an input x ∈ X. To streamline our argument  we omit the (cid:96)
superscript and denote the layer input by x. Denote each MAS computation from (1) as

zk(x) = max

r=1 ... R

(cid:104)[A]k r ·  x(cid:105) + [B]k r = max

r=1 ... R

Ek r(x) 

(3)

where Ek r(x) is the afﬁne projection of x parameterized by the slope [A]k r · and offset [B]k r. By
deﬁning the following half-space consisting of the set of points above the hyperplane

k r = {(x  y) ∈ X × R : y ≥ Ek r(x)} 
E+

we obtain the following geometric interpretation of the unit output.
Proposition 1. The kth MAS unit maps its input space onto the boundary of the convex polytope
Pk = ∩R

k r  leading to
E+

r=1

{(x  zk(x))  x ∈ X} = ∂Pk 

(4)

(5)

where ∂Pk denotes the boundary of the polytope.

3

The MAS computation can be decomposed geometrically as follows. The slope [A]k r · and offset
[B]k r parameters describe the shape of the half-space E+
k r. The max over the regions r in (3) deﬁnes
the polytope Pk as the intersection over the R half-spaces. The following property shows how the
unit projection  the polytope faces and the unit input space partition naturally tie together.
Lemma 1. The vertical projection on the input space X of the faces of the polytope Pk from (5)
deﬁne the cells of a PD.
Furthermore  we can highlight the maximization process of the unit computation (3) with the following
operator rk : X → {1  . . .   R} deﬁned as

rk(x) = arg max
r=1 ... R

Ek r(x).

(6)

This operator keeps track of the index of the afﬁne mapping used to produce the unit output or 
equivalently  the index of the polytope face used to produce the unit output. The collection of
inputs having the same face allocation  deﬁned as ∀r ∈ {1  . . .   R}   ωr = {x ∈ X : rk(x) = r} 
constitutes the rth partition cell of the unit k PD (recall (2) and Lemma 1).
The polytope formulation of a DN’s PD provides an avenue to study the interplay between the slope
and offset of the MAS unit and this speciﬁc partition by providing the analytical form of the PD.
Theorem 1. The kth MAS unit partitions its input space according to a PD with R centroids and
radii given by [µ]k r = [A]k r · and [rad]k r = 2[B]k r + (cid:107)[A]k r ·(cid:107)2 ∀r ∈ {1  . . .   R} (recall (2)).
Corollary 1. The input space partition of a DN unit is composed of convex polytopes.
For a single MAS unit  the slope corresponds to the centroid  and its (cid:96)2 norm combines with the bias
to produce the radius. The PD simpliﬁes to a VD when [B]k r = − 1
3.2 MASO Layer Power Diagram
We study the input space partition of an entire DN layer by studying the joint behavior of all its
constituent units. A MASO layer is a continuous  piecewise afﬁne operator made by the concatenation
of K MAS units (recall (1)); we extend (3) to

2(cid:107)[A]k r ·(cid:107)2 + c ∀r ∀c ∈ R.

z(x) =

max

r=1 ... R

E1 r(x)  . . .   max

r=1 ... R

EK r(x)

∀x ∈ X

 

(7)

and the per-unit face index function rk (6) into the operator r : X → {1  . . .   R}K deﬁned as

r(x) = [r1(x)  . . .   rK(x)]T .

E+

Following the geometric interpretation of the unit output from Proposition 1  we extend (4) to

r =(cid:8)(x  y) ∈ X × RK : [y]1 ≥ E1 [r]1(x)  . . .   [y]K ≥ EK [r]K (x)(cid:9)   ∀r ∈ {1  . . .   R}K (9)
dimensional convex polytope P =(cid:84)

in order to provide the following layer output geometrical interpretation.
Proposition 2. The layer operator z maps its input space into the boundary of the dim(X) + K

(8)

r∈{1 ... R}K E+
∂P = {(x  z(x)) ∀x ∈ X}.

r via

(10)

(cid:20)

(cid:21)T

Similarly to Proposition 1  the polytope P imprints the layer’s input space with a partition that is the
intersection of the K per-unit input space partitions.
Lemma 2. The vertical projection on the input space X of the faces of the polytope P from Proposi-
tion 2 deﬁne the cells of a PD.
The MASO layer projects an input x onto the polytope face indexed by r(x) corresponding to

(cid:20)

(cid:21)T

r(x) =

arg max
r=1 ... R

E1 r(x)  . . .   arg max
r=1 ... R

EK r(x)

.

(11)

The collection of inputs having the same face allocation jointly across the K units constitutes the rth
partition cell (region) of the layer PD.

4

Layer 1: mapping
X(0)⊂R2 to X(1)⊂R6

Layer 2: mapping
X(1)⊂R6 toX(2)⊂R6

l
a
i
m
o
n
y
l
o
P

e
c
a
p
s

t
u
p
n
I

Layer 3: mapping
X(2)⊂R6 toX(3)⊂R1 Figure 2: Power diagram subdivision
in a toy deep network (DN) with a D =
2 dimensional input space. Top: The
partition polynomial (22)  whose roots
deﬁne the partition boundaries in the in-
put space. Bottom: Evolution of the in-
put space partition (15) displayed layer
by layer  with the newly introduced
boundaries in darker color. Below each
partition  one of the newly introduced
cuts edgeX(0) (k  (cid:96)) from (21) is high-
lighted; in the ﬁnal layer (right)  this cut
corresponds to the decision boundary
(in red).

centroids µr =(cid:80)K

k=1[A]k [r]k · and radii radr = 2(cid:80)K

Theorem 2. A DN layer partitions its input space according to a PD containing up to RK cells with

2(cid:107)[A]k r ·(cid:107)2 reduces the PD to a VD.

Input Space Power Diagram of a MASO Deep Network

k=1[B]k [r]k + (cid:107)µr(cid:107)2 (recall (2)).
Corollary 2. The input space partition of a DN layer is composed of convex polytopes.
Extending Theorem 1  we observe in the layer case that the centroid of each PD cell corresponds to
the sum of the rows of the slopes matrix producing the layer output. The radii involve the bias units
and the (cid:96)2 norm of the slopes as well as their correlation. This highlights how  even when a change of
weight occurs for a single unit  it will impact multiple centroids and hence multiple cells. Note also
that orthogonal DN ﬁlters 2 and [B]k r = − 1
Appendix A.2 explores how the shapes and orientations of layer’s PD cells can be designed by
appropriately constraining the values of the DN’s weights and biases.
4
We are now armed to characterize and study the input space partition of an entire DN by studying the
joint behavior of its constituent layers.
4.1 The Power Diagram Subdivision Recursion
We provide the formula for the input space partition of an L-layer DN by means of a recursion.
Recall that each layer partitions its input space X((cid:96)−1) in terms of the polytopes P((cid:96)) according to
Proposition 2. The DN partition corresponds to a recursive subdivision where each per-layer polytope
subdivides the previously obtained partition.
Initialization ((cid:96) = 0): Deﬁne the region of interest in the input space X(0) ⊂ RD.
First step ((cid:96) = 1): The ﬁrst layer subdivides X(0) into a PD via Theorem 2 with parameters A(1)  B(1)
to obtain the layer-1 partition Ω(1).
Recursion step ((cid:96) = 2): For concreteness we focus here on how the second layer subdivides the ﬁrst
layer’s input space partition. In particular  we highlight how a single cell ω(1)
r(1) of Ω(1) is subdivided 
the same applies to all the cells. On this cell  the ﬁrst layer mapping is afﬁne with parameters
r(1)  B(1)
A(1)
r(1). This convex cell thus remains a convex cell at the output of the ﬁrst layer mapping  it
lives in X(1) and it is deﬁned as

aﬀ r(1) =

r(1)x + B(1)
A(1)

r(1)  x ∈ ω(1)

r(1)

(12)

(cid:110)

(cid:111) ⊂ X(1).

The second layer partitions its input space X(1) and thus also potentially subdivisions aﬀ r(1). In
particular  this -mapped cell- will be subdivided by the edges of the polytope P(2) (recall (10)) having
for domain aﬀ r(1)  this domain restricted polytope is deﬁned as

r(1) = P(2) ∩(cid:16)

aﬀ r(1) × RD(2)(cid:17)

P(2)

.

(13)

2Orthogonal DN ﬁlters have the property that (cid:104)[A]k r ·  [A]k(cid:48) r(cid:48) ·(cid:105) = 0 ∀r  r(cid:48)  k (cid:54)= k(cid:48).

5

1 [r(2)]1

(cid:68)

k [r(1)]k

(cid:69)

r(1)

D(1) [r(2)]D(1)

+ B(2)

r(2) k ∈

r(1) can be

r(2) and bias

[A(2)

r(2)]k r .  B(1)

(x)(cid:9) (14)

the hyperplane with slope A(1)T

(x)  . . .   [y]D(1)≥ E(1←2)

(cid:8)(x  y)∈ ω(1)

r(1) ∈ X(1)×RD(2) can be expressed in X(0)×RD(2)

r(1)× RD(1): [y]1≥ E(1←2)
r(1) A(2)

Since the layer 1 mapping is afﬁne in this region  the domain restricted polytope P(2)
expressed as part of X(0) as opposed to X(1).
Deﬁnition 2. The domain restricted polytope P(2)
as
P(1←2)
r(1) =∩r(2)
with E(1←2)
{1  . . .   D(1)}.
The above results demonstrates how cell ω(1)
r(1)  seen as aﬀ r(1) by the second layer  is subdi-
vided by the domain restricted polytope P(2)
r(1); and conversely  how this subdivision of ω(1)
r(1) is
done by the domain restricted second layer polytope expressed in the DN input space P(1←2)
.
Now  combining the latter interpretation  and applying Lemma 2  we obtain that this cell is sub-
divided according to a PD induced by the faces of P(1←2)
. This PD is
characterized by the centroids µ(1←2)
r(1) r(2)(cid:107)2 +
2(cid:104)µ(2)
r(2)(cid:105) ∀r(2) ∈ {1  . . .   R}D(2). The PD parameters thus combine the afﬁne
parameters A(1)
r(1)  B(1)
r(1) of the considered cell with the second layer parameters A(2)  B(2). Repeat-
ing this subdivision process for all cells ω(1)
r(1) from Ω(1) forms the subdivided input space partition
Ω(1 2) = ∪r(1)PD(1←2)
Recursion step ((cid:96)): Consider the situation at layer (cid:96) knowing Ω(1 ... (cid:96)−1) from the previous subdivi-
sion steps. Similarly to the (cid:96) = 2 step  layer (cid:96) subdivides each cell in Ω(1 ... (cid:96)−1) to produce Ω(1 ... (cid:96))
leading to the up-to-layer-(cid:96)-layer DN partition

r(1) r(2) = (cid:107)µ(1←2)

  denoted as PD(1←2)

  and radii rad(1←2)

r(1)(cid:105) + 2(cid:104)1  B(2)

r(1) r(2) = A(1)

r(1)

r(2)  B(1)

r(1)

(cid:62)

µ(1←2)

r(2)

.

r(1)

r(1)

r(1)

Ω(1 ... (cid:96)) = ∪r(1) ... r((cid:96)−1)PD(1←(cid:96))

r(1) ... r((cid:96)−1) .

(15)

See Fig. 2 for a numerical example with a 3-layer DN and D = 2 dimensional input space. (See also
Figures 7 and 9 in Appendix B.)
Theorem 3. Each cell ω(1 ... (cid:96)−1)
domain ω(1.....(cid:96)−1)

r(1) ... r((cid:96)−1) ∈ Ω(1 ... (cid:96)−1) is subdivided into PD(1←(cid:96))

r(1) ... r((cid:96)−1)  a PD with

r(1) ... r((cid:96)−1) and parameters
µ(1←(cid:96))
r(1) ... r((cid:96)) =(A(1←(cid:96)−1)
rad(1←(cid:96))
r(1) ... r((cid:96)) =(cid:107)µ(1←(cid:96))

r(1) ... r((cid:96)−1))T µ((cid:96))
r(1) ... r((cid:96))(cid:107)2 + 2(cid:104)µ((cid:96))
∀r(i) ∈ {1  . . .   R}D(i) with B(1→(cid:96)−1) =(cid:80)(cid:96)−1

r((cid:96))

r((cid:96))  B(1→(cid:96)−1)
r(1) ... r((cid:96)−1)(cid:105) + 2(cid:104)1  B((cid:96))
r((cid:96))(cid:105)
(cid:0)(cid:81)(cid:96)(cid:48)

(cid:1)B((cid:96)(cid:48))

i=(cid:96)−1 A(i)

(cid:96)(cid:48)=1

r(i)

r((cid:96)(cid:48) ) forming Ω(1 ... (cid:96)).

(centroids)

(radii) 

(16)

(17)

The subdivision recursion provides a direct result on the shape of the DN input space partition regions.
Corollary 3. For any number of MASO layers L ≥ 1  the PD cells of the DN input space partition
are convex polytopes.
4.2 Centroid and Radius Computation
While in general a DN has a tremendous number of PD cells  the DN’s forward inference calculation
locates the cell containing an input signal x with a computational complexity that is only logarithmic
in the number of regions. (See Appendix A.3 for a proof and additional discussion.) We now produce
a closed-form formula for the radius and centroid of that cell.
Consider the cell of the PD induced by layers 1 through (cid:96) of a DN that contains a data point x of
interest. This cell is described by the code r(1)(x)  . . .   r((cid:96))(x) that we will simplify here in an abuse
of notation so simply x. Denote the Jacobian operator as J  and the vector of ones by 1  the centroid
and radius of the cell are given by
= (Jxf (1→(cid:96)))T 1 

µ(1←(cid:96))

(18)

x

6

Input x

µ(1)
x

µ(1 2)
x

µ(1 2 3)
x

µ(1 ... 4)
x

µ(1 ... 5)
x

µ(1 ... 6)
x

µ(1 ... 7)
x

µ(1 ... 8)
x

µ(1 ... 9)
x

l
a
i
t
i
n
i

d
e
n
i
a
r
t

l
a
i
t
i
n
i

d
e
n
i
a
r
t

Figure 3: Centroids of the PD regions containing an input horse image x computed via (18) for a
LargeConv network (top) and a ResNet (bottom). (See Fig. 11 for results with a SmallConv network.) The
input belongs to the PD cell ω(1 ... (cid:96))
for each successively reﬁned PD subdivision of each layer Ω(1 ... (cid:96)).
At each layer of the subdivision  the region has an associated centroid µ(1 ... (cid:96))
(depicted here) and radius
(not depicted). As the depth (cid:96) increases  the centroids diverge from horse-like images. This is because the
radii begin to dominate the centroids  pushing the centroids outside the PD cell containing x. Training
accelerates this domineering effect.

x

x

(cid:43)

D((cid:96))(cid:88)

k=1

(cid:42)

(cid:69)

(cid:107)2 + 2

(cid:68)
(cid:16)∇xf (1→(cid:96)−1)

1

(cid:17)T

rad(1←(cid:96))

x

= (cid:107)µ(1←(cid:96))

x

1  B((cid:96))
x

+ 2

f (1→(cid:96))(x) − A(1→(cid:96)−1)

x 

x

[A((cid:96))

x ]k .

(19)

(cid:80)D((cid:96))

x

x

=

D((cid:96))

k=1 [A((cid:96))

x ]k .  B(1→(cid:96)−1)

x

with A(1→(cid:96)−1)

  . . .  ∇xf (1→(cid:96)−1)
= f (1→(cid:96))(x) − A(1→(cid:96)−1)

  and where we recall
x from Theorem 3 and f (1→(cid:96))

that µ((cid:96))
x =
is the kth unit
of the layer 1 to (cid:96) mapping. Note how the centroids and biases of the current layer are mapped back
to the input space X(0) via a projection onto the tangent hyperplane deﬁned by the basis A(1→(cid:96)−1)
.
Conveniently  the centroids (18) can be computed via an efﬁcient backpropagation pass through the
DN  which is typically available because it is integral to DN learning. Moreover  (18) corresponds
to the element-wise summation of the saliency maps [SVZ13  ZF14] from all of the layer units.3
Figure 3 visualizes the centroids of the cell containing a particular input signal for a LargeConv and
ResNet DN trained on the CIFAR10 dataset (see Appendix C for details on the models plus additional
ﬁgures).

x

k

4.3 Distance to the Nearest PD Cell Boundary
In Appendix D we derive the Euclidean distance from a data point x to the nearest boundary of its
PD cell (a point from ∂Ω)

min
u∈∂Ω

(cid:107)x − u(cid:107) = min

(cid:96)=1 ... L

min

k=1 ... D((cid:96))

|(z((cid:96))
(cid:107)∇x(z((cid:96))

k ◦ ··· ◦ z(1))(x)|
k ◦ ··· ◦ z(1))(x)(cid:107) .

(20)

Fig. 4 (and 6 in the Appendix) plots the distributions of the log distances from the training points in
the CIFAR10 training set to their nearest region boundary the input space partition as a function of
layer (cid:96) and at different stages of learning. We see that training increases the number of data points
that lie close to their nearest boundary. We see from these ﬁgures that while a network with fully
connected layers (MLP) reﬁnes its partition by introducing cuts close to the training points at each
layer  the SmallCNN does not reduce the shortest distance at deeper layers.
A further exploration is carried out in Appendix A.4  where Table 1 summarizes the performance of
the centroids  when used as centroids of a VD  to recover inside their region  the same input as the
one that originally produced the centroid.

3The saliency maps were linked to the ﬁlters in a matched ﬁlterbank in [BB18a  BB18b].

7

SmallCNN

MLP

r
e
y
a
L

s
h
c
o
p
E

Figure 4: Empirical distributions of the log distances from the training points of the CIFAR10 dataset to the
nearest PD cell boundary as calculated by (20) for the various layers of a SmallCNN (left) and MLP (right).
Blue: Training set. Red: Test set. On top is the evolution through layers at the end of the training  on bottom is
the evolution of the last layer  through the epochs. The distances decrease with (cid:96) due to PD subdivision which
reduces the volume of the cells as the subdivision process occurs. The distances are also much smaller for the
CNN desptie having the same number of units for the MLP as the number of ﬁlters and translations for the
convolutional layers. This demonstrates how the subdivision process of the convolutional layer is much more
performance at reﬁning the DN input space partitioning around the data for image data.

5 Geometry of the Deep Network Decision Boundary
We now study the edges of the polytopes that deﬁne the PD cells’ boundaries. We demonstrate how
a single unit at layer (cid:96) deﬁnes multiple cell boundaries in the input space and use this ﬁnding to
derive an analytical formula for the DN decision boundary that would be used in a classiﬁcation task.
Without loss of generality  we focus in this section on piecewise nonlinearities with R = 2  such as
ReLU  leaky-ReLU  and absolute value.
5.1 Partition Boundaries and Edges
In the case of R = 2 nonlinearities  the polytope P((cid:96))
contains a single edge  we consider
here nonlinearities that can be expressed as a leaky-ReLU with leakiness η (cid:54)= 0. We deﬁne this
edge as the intersection of the faces of the polytope. For instance  in the case of leaky-ReLU  the
polytope contains two faces that characterize the two regions produced by a single leaky-ReLU unit.
We formally deﬁne the edge of a polytope as follows.
Deﬁnition 3. The edges of the polytope P((cid:96))
particular the input space X(0)) as

k can be expressed in any space X((cid:96)(cid:48))  (cid:96)(cid:48) < (cid:96) (and in

k of unit z((cid:96))

k

k 2(z((cid:96)(cid:48)→(cid:96)−1)(x)) = 0} 

edgeX((cid:96)(cid:48) ) (k  (cid:96)) = {x ∈ X((cid:96)(cid:48)) : E((cid:96))

(21)
k 2 from (3)  and where ◦ denotes the composition operator.
with z((cid:96)(cid:48)→(cid:96)−1) = z((cid:96)−1) ◦ ··· ◦ z((cid:96)(cid:48))  E((cid:96))
In the same way that the polytopes P(1←(cid:96))
r1 ... r((cid:96)−1) could be expressed in X(0) ×RD((cid:96)) and then mapped
to the DN input space (recall Section 4.1)  these edges deﬁned in X((cid:96)−1) can be expressed in the DN
input space X(0). The projection of the edges into the DN input space will constitute the partition
boundaries. Deﬁning the polynomial

L(cid:89)

D((cid:96))(cid:89)

Pol(x) =

k ◦ z((cid:96)−1) ◦ ··· ◦ z(1))(x) 
(z((cid:96))

(22)

we obtain the following result where the boundaries of Ω(1 ... (cid:96)) from Theorem 3 can be expressed in
term of the polytope edges and roots of the polynomial.

(cid:96)=1

k=1

8

17.515.012.510.07.55.02.5151517.515.012.510.07.55.02.515151510508008015105080080Theorem 4. The polynomial (22) is of order(cid:81)L

boundaries:

(cid:96)=1 D((cid:96))  and its roots correspond to the partition

∂Ω(1 ... (cid:96)) = {x ∈ X(0) : Pol(x) = 0} = ∪(cid:96)

(cid:96)(cid:48)=1 ∪D((cid:96)(cid:48))

k=1 edgeX(0)(k  (cid:96)).

(23)

The root order deﬁnes the dimensionality of the root (boundary  corner  etc.).

5.2 Decision Boundary Curvature
The ﬁnal DN layer introduces a last subdivision of the partition. For brevity  we focus on a binary
classiﬁcation problem; in this case  D(L) = 1 and a single last subdivision occurs  leading to the
class prediction being y = 1
(x)>τ for some threshold τ  this last layer can thus be cast as a
MASO with a leaky-ReLU type nonlinearity with proper bias  and setting τ = 0. That is  the DN
prediction is unchanged by this last nonlinearity  and the change of sign is the change of class is the
decision boundary.
Proposition 3. The decision boundary of a DN with L layers is the edge of the last layer polytope
P(L) expressed in the input space X(0) from Deﬁnition 3 as

z(L)
1

DecisionBoundary = {x ∈ X(0) : f (x) = 0} = edgeX(0)(1  L) 

(24)

where edgeX(0)(1  L) denotes the edge of unit 1 of layer L expressed in the input space X(0).
To provide insights into this result  consider a 3-layer DN denoted as f and a binary classiﬁcation
task; we have
DecisionBoundary = ∪r(2) ∪r(1) {x ∈ X(0) : (cid:104)αr(2) r(1)  x(cid:105) + βr(2) r(1) = 0} ∩ ω(1 2)

(25)

r(1) r(2) 

1 1 ·A(2)

r(2)A(1)

r(2)B(1)

r(1))T [A(3)]1 1 · and βr(1) r(2) = [A(3)]T

with αr(1) r(2) = (A(2)
r(1) + [B(3)]1 1.4 The
distribution of αr(1) r(2) characterizes the structure of the decision boundary and thus highlights the
interplay between the layer parameters  layer topology  and the decision boundary. For example 
in Fig. 2 the red line demonstrates how the weights characterize the curvature and cut positions of
the decision boundary. We provide examples highlighting the impact on the angles of change in the
architecture of the DN in Appendix A.5.
We provide a direct application of the above ﬁnding by providing a curvature characterization of the
decision boundary. First  we propose the following result stating that the form of α and β from (25)
from a region to a neighbouring one alters only a single unit code at a some layer.
Lemma 3. Upon reaching a region boundary  any edge as deﬁned in Deﬁnition 3 must continue into
a neighbouring region.

This follows directly from continuity of the involved operator and enables us to study its curvature
by comparing the edges of adjacent regions. In fact adjacent region edges connect at the region
boundary by continuity  however their angle might differ  this angle deﬁnes the curviness of the
decision boundary  which is deﬁned as the collection of all the edges introduces by the last layer.
Theorem 5. The decision boundary curvature/angle between two adjacent regions5 r and r(cid:48) is given
by the following dihedral angle [KB38] between neighbouring α parameters as

cos(θ(r  r(cid:48))) =

|(cid:104)αr  αr(cid:48)(cid:105)|
(cid:107)αr(cid:107)(cid:107)αr(cid:48)(cid:107) .

(26)

Acknowledgements
RB and RGB were supported by NSF grants CCF-1911094  IIS-1838177  and IIS-1730574; ONR
grants N00014-18-12571 and N00014-17-1-2551; AFOSR grant FA9550-18-1-0478; DARPA grant
G001534-7500; and a Vannevar Bush Faculty Fellowship  ONR grant N00014-18-1-2047. RC and
BA were supported by NSF grant SCH-1838873 and NIH grant R01HL144683-CFDA.

4The last layer is a linear transform with one unit  since we perform binary classiﬁcation.
5For clarity  we omit the subscripts.

9

References

[AI] Franz Aurenhammer and Hiroshi Imai. Geometric relations among voronoi diagrams. Geometriae

Dedicata.

[AMN+98] Sunil Arya  David M Mount  Nathan S Netanyahu  Ruth Silverman  and Angela Y Wu. An optimal
algorithm for approximate nearest neighbor searching ﬁxed dimensions. Journal of the ACM
(JACM)  45(6):891–923  1998.

[Aur87] Franz Aurenhammer. Power diagrams: properties  algorithms and applications. SIAM Journal on

Computing  16(1):78–96  1987.

[BB18a] R. Balestriero and R. Baraniuk. Mad Max: Afﬁne spline insights into deep learning.

arXiv:1805.06576  2018.

[BB18b] R. Balestriero and R. G. Baraniuk. A spline theory of deep networks. In International Conference

of Machine Learning (ICML)  pages 374–383  2018.

[GBC16] I. Goodfellow  Y. Bengio  and A. Courville. Deep Learning. MIT Press  2016. http://www.

deeplearningbook.org.

[GSM03] Bogdan Georgescu  Ilan Shimshoni  and Peter Meer. Mean shift based clustering in high dimensions:
A texture classiﬁcation example. In International Conference Computer Vision (ICCV)  pages
456–464  2003.

[HD13] L. A. Hannah and D. B. Dunson. Multivariate convex regression with adaptive partitioning. Journal

of Machine Learning Research (JMLR)  14(1):3261–3294  2013.

[HR19] Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. arXiv:1901.09021 

2019.

[IIM85] Hiroshi Imai  Masao Iri  and Kazuo Murota. Voronoi diagram in the laguerre geometry and its

applications. SIAM Journal on Computing  14(1):93–105  1985.

[Joh60] Roger A Johnson. Advanced Euclidean Geometry: An Elementary Treatise on the Geometry of the

Triangle and the Circle. Dover Publications  1960.

[KB38] Willis Frederick Kern and James R Bland. Solid mensuration: With Proofs. J. Wiley & Sons  1938.

[MB09] A. Magnani and S. P. Boyd. Convex piecewise-linear ﬁtting. Optim. Eng.  10(1):1–17  2009.

[ML09] Marius Muja and David G Lowe. Fast approximate nearest neighbors with automatic algorithm
conﬁguration. International Conference on Computer Vision Theory and Applications (VISAPP) 
2(331-340):2  2009.

[MPCB14] Guido F Montufar  Razvan Pascanu  Kyunghyun Cho  and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in Neural Information Processing Systems (NIPS) 
pages 2924–2932  2014.

[PA11] János Pach and Pankaj K Agarwal. Combinatorial Geometry  volume 37. John Wiley & Sons 

2011.

[PS12] Franco P Preparata and Michael I Shamos. Computational geometry: An introduction. Springer 

2012.

[RPK+17] Maithra Raghu  Ben Poole  Jon Kleinberg  Surya Ganguli  and Jascha Sohl Dickstein. On the
expressive power of deep neural networks. In International Conference on Machine Learning
(ICML)  pages 2847–2854  2017.

[SVZ13] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks:

Visualising image classiﬁcation models and saliency maps. arXiv:1312.6034  2013.

[WBB19] Zichao Wang  Randall Balestriero  and Richard Baraniuk. A max-afﬁne spline perspective of
recurrent neural networks. In International Conference on Learning Representations (ICLR)  2019.

[ZBH+16] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. arXiv:1611.03530  2016.

[ZF14] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In

European Conference on Computer Vision (ECCV)  pages 818–833. Springer  2014.

10

,Randall Balestriero
Romain Cosentino
Behnaam Aazhang
Richard Baraniuk