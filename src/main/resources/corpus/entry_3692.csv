2016,Near-Optimal Smoothing of Structured Conditional Probability Matrices,Utilizing the structure of a probabilistic model can significantly increase its learning speed. Motivated by several recent applications  in particular bigram models in language processing  we consider learning low-rank conditional probability matrices under expected KL-risk. This choice makes smoothing  that is the careful handling of low-probability elements  paramount. We derive an iterative algorithm that extends classical non-negative matrix factorization to naturally incorporate additive smoothing and prove that it converges to the stationary points of a penalized empirical risk. We then derive sample-complexity bounds for the global minimizer of the penalized risk and show that it is within a small factor of the optimal sample complexity. This framework generalizes to more sophisticated smoothing techniques  including absolute-discounting.,Near-Optimal Smoothing of Structured Conditional

Probability Matrices

University of California  San Diego

Toyota Technological Institute at Chicago

Moein Falahatgar

San Diego  CA  USA

moein@ucsd.edu

Mesrob I. Ohannessian

Chicago  IL  USA
mesrob@ttic.edu

Alon Orlitsky

University of California  San Diego

San Diego  CA  USA

alon@ucsd.edu

Abstract

Utilizing the structure of a probabilistic model can signiﬁcantly increase its learning
speed. Motivated by several recent applications  in particular bigram models
in language processing  we consider learning low-rank conditional probability
matrices under expected KL-risk. This choice makes smoothing  that is the careful
handling of low-probability elements  paramount. We derive an iterative algorithm
that extends classical non-negative matrix factorization to naturally incorporate
additive smoothing and prove that it converges to the stationary points of a penalized
empirical risk. We then derive sample-complexity bounds for the global minimzer
of the penalized risk and show that it is within a small factor of the optimal
sample complexity. This framework generalizes to more sophisticated smoothing
techniques  including absolute-discounting.

Introduction

1
One of the fundamental tasks in statistical learning is probability estimation. When the possible
outcomes can be divided into k discrete categories  e.g. types of words or bacterial species  the task
of interest is to use data to estimate the probability masses p1 ···   pk  where pj is the probability of
observing category j. More often than not  it is not a single distribution that is to be estimated  but
multiple related distributions  e.g. frequencies of words within various contexts or species in different
samples. We can group these into a conditional probability (row-stochastic) matrix Pi 1 ···   Pi k
as i varies over c contexts  and Pij represents the probability of observing category j in context i.
Learning these distributions individually would cause the data to be unnecessarily diluted. Instead 
the structure of the relationship between the contexts should be harnessed.
A number of models have been proposed to address this structured learning task. One of the wildly
successful approaches consists of positing that P   despite being a c×k matrix  is in fact of much lower
rank m. Effectively  this means that there exists a latent context space of size m (cid:28) c  k into which
the original context maps probabilistically via a c × m stochastic matrix A  then this latent context
in turn determines the outcome via an m × k stochastic matrix B. Since this structural model means
that P factorizes as P = AB  this problem falls within the framework of low-rank (non-negative)
matrix factorization. Many topic models  such as the original work on probabilistic latent semantic
analysis PLSA  also map to this framework. We narrow our attention here to such low-rank models 
but note that more generally these efforts fall under the areas of structured and transfer learning.
Other examples include: manifold learning  multi-task learning  and hierarchical models.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In natural language modeling  low-rank models are motivated by the inherent semantics of language:
context ﬁrst maps into meaning which then maps to a new word prediction. An alternative form
of such latent structure  word embeddings derived from recurrent neural networks (or LSTMs) are
the state-of-the-art of current language models  [20  25  28]. A ﬁrst chief motivation for the present
work is to establish a theoretical underpinning of the success of such representations. We restrict the
exposition to bigram models. The traditional deﬁnition of the bigram is that language is modeled as a
sequence of words generated by a ﬁrst order Markov-chain. Therefore the ‘context’ of a new word is
simply its preceding word  and we have c = k. Since the focus here is not the dependencies induced
by such memory  but rather the ramiﬁcations of the structural assumptions on P   we take bigrams to
model word-pairs independently sampled by ﬁrst choosing the contextual word with probability π
and then choosing the second word according to the conditional probability P   thus resulting in a
joint distribution over word-pairs (πiPij).
What is the natural measure of performance for a probability matrix estimator? Since ultimately
such estimators are used to accurately characterize the likelihood of test data  the measure of choice
used in empirical studies is the perplexity  or alternatively its logarithm  the cross entropy. For data
consisting of n word-pairs  if Cij is the number of times pair (i  j) appears  then the cross entropy
. The population quantity that corresponds to this empirical
of an estimator Q is 1
n
ij πiPij log Pij
.
Qij
Note that this is indeed the expectation of the cross entropy modulo the true entropy  an additive term
that does not depend on Q. This is the natural notion of risk for the learning task  since we wish
to infer the likelihood of future data  and our goal can now be more concretely stated as using the
data to produce an estimator Qn with a ‘small’ value of D(P(cid:107)Qn). The choice of KL-divergence
introduces a peculiar but important problem: the necessity to handle small frequencies appropriately.
In particular  using the empirical conditional probability is not viable  since a zero in Q implies
inﬁnite risk. This is the problem of smoothing  which has received a great amount of attention by the
NLP community. Our second salient motivation for the present work is to propose principled methods
of integrating well-established smoothing techniques  such as add- 1
2 and absolute discounting  into
the framework of structured probability matrix estimation.
Our contributions are as follows  we provide:
• A general framework for integrating smoothing and structured probability matrix estimation  as an

performance measure is the (row-by-row weighted) KL-divergence D(P(cid:107)Q) =(cid:80)

ij Cij log 1
Qij

(cid:80)

alternating-minimization that converges to a stationary point of a penalized empirical risk.

global minimizer of this penalized empirical risk.

• A sample complexity upper bound of O(km log2(2n + k)/n) for the expected KL-risk  for the
• A lower bound that matches this upper bound up to the logarithmic term  showing near-optimality.
The paper is organized as follows. Section 2 reviews related work. Section 3 states the problem
and Section 4 highlights our main results. Section 5 proposes our central algorithm and Section 6
analyzes its idealized variant. Section 7 provides some experiments and Section 8 concludes.
2 Related Work
Latent variable models  and in particular non-negative matrix factorization and topic models  have
been such an active area of research in the past two decades that the space here cannot possibly do
justice to the many remarkable contributions. We list here some of the most relevant to place our
work in context. We start by mentioning the seminal papers [12  18] which proposed the alternating
minimization algorithm that forms the basis of the current work. This has appeared in many forms in
the literature  including the multiplicative updates [29]. Some of the earliest work is reviewed in [23].
These may be generally interpreted as discrete analogs to PCA (and even ICA) [10].
An inﬂuential Bayesian generative topic model  the Latent Dirichlet Allocation  [7] is very closely
related to what we propose. In fact  add-half smoothing effectively corresponds to a Dirichlet(1/2)
(Jeffreys) prior. Our exposition differs primarily in adopting a minimax sample complexity perspective
which is often not found in the otherwise elegant Bayesian framework. Furthermore  exact Bayesian
inference remains a challenge and a lot of effort has been expended lately toward simple iterative
algorithms with provable guarantees  e.g.
[3  4]. Besides  a rich array of efﬁcient smoothing
techniques exists for probability vector estimation [2  16  22  26]  of which one could directly avail
in the methodology that is presented here.

2

A direction that is very related to ours was recently proposed in [13]. There  the primary goal is to
recover the rows of A and B in (cid:96)1-risk. This is done at the expense of additional separation conditions
on these rows. This makes the performance measure not easily comparable to our context  though
with the proper weighted combination it is easy to see that the implied (cid:96)1-risk result on P is subsumed
by our KL-risk result (via Pinsker’s inequality)  up to logarithmic factors  while the reverse isn’t true.
Furthermore  the framework of [13] is restricted to symmetric joint probability matrices  and uses
an SVD-based algorithm that is difﬁcult to scale beyond very small latent ranks m. Apart from this
recent paper for the (cid:96)1-risk  sample complexity bounds for related (not fully latent) models have been
proposed for the KL-risk  e.g. [1]. But these remain partial  and far from optimal. It is also worth
noting that information geometry gives conditions under which KL-risk behaves close to (cid:96)2-risk [8] 
thus leading to a Frobenius-type risk in the matrix case.
Although the core optimization problem itself is not our focus  we note that despite being a non-
convex problem  many instances of matrix factorization admit efﬁcient solutions. Our own heuristic
initialization method is evidence of this. Recent work  in the (cid:96)2 context  shows that even simple
gradient descent  appropriately initialized  could often provably converge to the global optimum [6].
Concerning whether such low-rank models are appropriate for language modeling  there has been
evidence that some of the abovementioned word embeddings [20] can be interpreted as implicit matrix
factorization [19]. Some of the traditional bigram smoothing techniques  such as the Kneser-Ney
algorithm [17  11]  are also reminiscent of rank reduction [14  24  15].

3 Problem Statement
Data Dn consists of n pairs (Xs  Ys)  s = 1 ···   n  where Xs is a context and Ys is the corresponding
outcome. In the spirit of a bigram language model  we assume that the context and outcome spaces
have the same cardinality  namely k. Thus (Xs  Ys) takes values in [k]2. We denote the count of pairs

(i  j) by Cij. As a shortcut  we also write the row-sums as Ci =(cid:80)

j Cij.

We assume the underlying generative model of the data to be i.i.d.  where each pair is drawn by ﬁrst
sampling the context Xs according to a probability distribution π = (πi) over [k] and then sampling
Ys conditionally on Xs according to a k × k conditional probability (stochastic) matrix P = (Pij)  a
non-negative matrix where each row sums to 1. We also assume that P has non-negative rank m. We
denote the set of all such matrices by Pm. They can all be factorized (non-uniquely) as P = AB 
where both A and B are stochastic matrices in turn  of size k × m and m × k respectively.
A conditional probability matrix estimator is an algorithm that maps the data into a stochastic matrix
Qn(X1 ···   Xn) that well-approximates P   in the absence of any knowledge about the underlying
model. We generally drop the explicit notation showing dependence on the data  and use instead
the implicit n-subscript notation. The performance  or how well any given stochastic matrix Q
approximates P   is measured according to the KL-risk:

R(Q) =

πiPij log

Pij
Qij

(1)

(cid:88)

ij

Note that this corresponds to an expected loss  with the log-loss L(Q  i  j) = log Pij/Qij. Although
we do seek out PAC-style (in-probability) bounds for R(Qn)  in order to give a concise deﬁnition
of optimality  we consider the average-case performance E[R(Qn)]. The expectation here is with
respect to the data. Since the underlying model is completely unknown  we would like to do well
against adversarial choices of π and P   and thus we are interested in a uniform upper bound of the
form:

r(Qn) = max
π P∈Pm

E[R(Qn)].

The optimal estimator  in the minimax sense  and the minimax risk of the class Pm are thus given by:

Q(cid:63)

n = arg min

Qn

r(Qn) = arg min
r(cid:63)(Pm) = min

Qn

max
π P∈Pm

Qn

max
π P∈Pm

E[R(Qn)]

E[R(Qn)].

Explicitly obtaining minimax optimal estimators is a daunting task  and instead we would like to
exhibit estimators that compare well.

3

Deﬁnition 1 (Optimality). If an estimator satisﬁes E[R(Qn)] ≤ ϕ· E[R(Q(cid:63)
n)]  ∀π  (called an oracle
inequality)  then if ϕ is a constant (of n  k  and m)  we say that the estimator is (order) optimal.
If ϕ is not constant  but its growth is negligible with respect to the decay of r(cid:63)(Pm) with n or the
growth of r(cid:63)(Pm) with k or m  then we can call the estimator near-optimal. In particular  we
reserve this terminology for a logarithmic gap in growth  that is an estimator is near-optimal if
log ϕ/ log r(cid:63)(Pm) → 0 asymptotically in any of n  k  or m. Finally  if ϕ does not depend on P we
have strong optimality  and r(Qn) ≤ ϕ · r(cid:63)(Pm). If ϕ does depend on P   we have weak optimality.
As a proxy to the true risk (1)  we deﬁne the empirical risk:

Rn(Q) =

1
n

Cij log

Pij
Qij

(2)

(cid:88)

ij

2

ij

The conditional probability matrix that minimizes this empirical risk is the empirical conditional
probability ˆPn ij = Cij/Ci. Not only is ˆPn ij not optimal  but since there always is a positive (even if
slim) probability that some Cij = 0 even if Pij (cid:54)= 0  it follows that E[Rn( ˆPn)] = ∞. This shows the
importance of smoothing. The simplest benchmark smoothing that we consider is add- 1
2 smoothing
ˆP Add- 1
= (Cij + 1/2) / (Ci + k/2)   where we give an additional “phantom” half-sample to each
word-pair  to avoid zeros. This simple method has optimal minimax performance when estimating
probability vectors. However  in the present matrix case it is possible to show that this can be a
factor of k/m away from optimal  which is signiﬁcant (cf. Figure 1(a) in Section 7). Of course 
since we have not used the low-rank structure of P   we may be tempted to “smooth by factoring”  by
performing a low-rank approximation of ˆPn. However  this will not eliminate the zero problem  since
a whole column may be zero. These facts highlight the importance of principled smoothing. The
problem is therefore to construct (possibly weakly) optimal or near-optimal smoothed estimators.

4 Main Results
2 -SMOOTHED LOW-RANK algorithm  which essentially consists
In Section 5 we introduce the ADD- 1
of EM-style alternating minimizations  with the addition of smoothing at each stage. Here we state
the main results. The ﬁrst is a characterization of the implicit risk function that the algorithm targets.
Theorem 2 (Algorithm). QAdd- 1
2 -LR converges to a stationary point of the penalized empirical risk

Rn penalized(W  H) = Rn(Q) +

1
2n

log

1
Wi(cid:96)

+

1
2n

log

1
H(cid:96)j

  where Q = W H.

(3)

(cid:88)

i (cid:96)

(cid:88)

(cid:96) j

Conversely  any stationary point of (3) is a stable point of ADD- 1
The proof of Theorem 2 follows closely that of [18]. We now consider the global minimum of
this implicit risk  and give a sample complexity bound. By doing so  we intentionally decouple the
algorithmic and statistical aspects of the problem and focus on the latter.
Theorem 3 (Sample Complexity). Let Qn ∈ Pm achieve the global minimum of Equation 3. Then
for all P ∈ Pm such that Pij > km
E[R(Qn)] ≤ c

n log(2n + k) ∀i  j and n > 3 

2 -SMOOTHED LOW-RANK.

log2(2n + k) 

with c = 3100.

km
n

We outline the proof in Section 6. The basic ingredients are: showing the problem is near-realizable 
a quantization argument to describe the complexity of Pm  and a PAC-style [27] relative uniform
convergence which uses a sub-Poisson concentration for the sums of log likelihood ratios and uniform
variance and scale bounds. Finer analysis based on VC theory may be possible  but it would need to
handle the challenge of the log-loss being possibly unbounded and negative. The following result
shows that Theorem 3 gives weak near-optimality for n large  as it is tight up to the logarithmic factor.
Theorem 4 (Lower Bound). For n > k  the minimax rate of Pm satisﬁes:

r(cid:63)(Pm) ≥ c

km
n

 

with c = 0.06.

This is based on the vector case lower bound and providing the oracle with additional information:
instead of only (Xs  Ys) it observes (Xs  Zs  Ys)  where Zs is sampled from Xs using A and Ys is
sampled from Zs using B. This effectively allows the oracle to estimate A and B directly.

4

5 Algorithm
Our main algorithm is a direct modiﬁcation of the classical alternating minimization algorithm for
non-negative matrix factorization [12  18]. This classical algorithm (with a slight variation) can be
shown to essentially solve the following mathematical program:

QNNMF(Φ) = arg min
Q=W H

Φij log

1
Qij

.

(cid:88)

(cid:88)

i

j

The analysis is a simple extension of the original analysis of [12  18]. By “essentially solves”  we
mean that each of the update steps can be identiﬁed as a coordinate descent  reducing the cost function
and ultimately converging as T → ∞ to a stationary (zero gradient) point of this function. Conversely 
all stationary points of the function are stable points of the algorithm. In particular  since the problem
is convex in W and H individually  but not jointly in both  the algorithm can be thought of as taking
exact steps toward minimizing over W (as H is held ﬁxed) and then minimizing over H (as W is
held ﬁxed)  whence the alternating-minimization name.
Before we incorporate smoothing  note that there are two ingredients missing from this algorithm.
First  the cost function is the sum of row-by-row KL-divergences  but each row is not weighted  as
compared to Equation (1). If we think of Φij as ˆPij = Cij/Ci  then the natural weight of row i is
πi or its proxy Ci/n. For this  the algorithm can easily be patched. Similarly to the analysis of the
original algorithm  one ﬁnds that this change essentially minimizes the weighted KL-risks of the
empirical conditional probability matrix  or equivalently the empirical risk as deﬁned in Equation (2):

QLR(C) = arg min
Q=W H

Rn(Q) = arg min
Q=W H

(cid:88)

(cid:88)

Ci
n

i

j

Cij
Ci

log

1
Qij

.

Of course  this is nothing but the maximum likelihood estimator of P under the low-rank constraint.
Just like the empirical conditional probability matrix  it suffers from lack of smoothing. For instance 
if a whole column of C is zero  then so will be the corresponding column of QERM(C). The ﬁrst
naive attempt at smoothing would be to add- 1

2 to C and then apply the algorithm:
2 -LR(C) = QLR(C + 1
2 )

QNaive Add- 1

However  this would result in excessive smoothing  especially when m is small. The intuitive reason
is this: in the extreme case of m = 1 all rows need to be combined  and thus instead of adding 1
2 to
each category  QNaiveadd− 1
2 LR would add k/2  leading to the the uniform distribution overwhelming
the original distribution. We may be tempted to mitigate this by adding instead 1/2k  but this doesn’t
generalize well to other smoothing methods. A more principled approach should perform smoothing
directly inside the factorization  and this is exactly what we propose here. Our main algorithm is:
Algorithm: ADD- 1

2 -SMOOTHED LOW-RANK

• Input: k × k matrix (Cij); Initial W 0 and H 0; Number of iterations T
• Iterations: Start at t = 0  increment and repeat while t < T

– For all i ∈ [k]  (cid:96) ∈ [m]  update W t
H t−1
– For all (cid:96) ∈ [m]  j ∈ [k]  update H t
W t−1
– Add-1/2 to each element of W t and H t  then normalize each row.

i(cid:96) ← W t−1
(cid:96)j ← H t−1

(W H)t−1

Cij

(W H)t−1

ij

Cij

j

i

i(cid:96)

(cid:96)j

(cid:96)j

i(cid:96)

ij

(cid:80)
(cid:80)

• Output: QAdd- 1

2 -LR(C) = W T H T

The intuition here is that  prior to normalization  the updated W and H can be interpreted as soft
counts. One way to see this is to sum each row i of (pre-normalized) W   which would give Ci. As for
H  the sums of its (pre-normalized) columns reproduce the sums of the columns of C. Next  we are
naturally led to ask: is QAdd- 1
2 LR(C) implicitly minimizing a risk  just as QLR(C) minimizes Rn(Q)?
Theorem 2 shows that indeed QAdd- 1
2 -SMOOTHED LOW-RANK lends itself to a host of generalizations. In
More interestingly  ADD- 1
particular  an important smoothing technique  absolute discounting  is very well suited for heavy-
tailed data such as natural language [11  21  5]. We can generalize it to fractional counts as follows.
Let Ci indicate counts in traditional (vector) probability estimation  and let D be the total number of

2 LR(C) essentially minimizes a penalized empirical risk.

5

distinct observed categories  i.e. D =(cid:80)
d be deﬁned as d =(cid:80)
(cid:40) Ci−α(cid:80) C
1−α(cid:80) C Ci + α(D+d)

ˆP Soft-AD
i

(C  α) =

i

(k−D−d)(cid:80) C (1 − Ci)

if Ci ≥ 1 
if Ci < 1.

I{Ci ≥ 1}. Let the number of fractional distinct categories
i CiI{Ci < 1}. We have the following soft absolute discounting smoothing:

This gives us the following patched algorithm  which we do not place under the lens of theory
currently  but we strongly support it with our experimental results of Section 7.
Algorithm: ABSOLUTE-DISCOUNTING-SMOOTHED LOW-RANK

• Input: Specify α ∈ (0  1)
• Iteration:

– Add-1/2 to each element of W t  then normalize.
(cid:96)j ← ˆP Soft-AD
– Apply soft absolute discounting to H t

j

• Output: QAD-LR(C  α) = W T H T

(H t

(cid:96) ·  α)

6 Analysis
We now outline the proof of the sample complexity upper bound of Theorem 3. Thus for the remainder
of this section we have:

Qn(C) = arg min
Q=W H

Rn(Q) +

1
2n

log

1
Wi(cid:96)

+

1
2n

log

1
H(cid:96)j

 

(cid:88)

i (cid:96)

(cid:88)

(cid:96) j

that is Qn ∈ Pm achieves the global minimum of Equation 3. Since we have a penalized empirical
risk minimization at hand  we can study it within the classical PAC-learning framework. However 
n are often associated withe the realizable case  where Rn(Qn) is exactly zero [27].
rates of order 1
The following Lemma shows that we are near the realizable regime.
Lemma 5 (Near-realizability). We have

E[Rn(Qn)] ≤ k
n

+

km
n

log(2n + k).

We characterize the complexity of the class Pm by quantizing probabilities  as follows. Given a
positive integer L  deﬁne ∆L to be the subset of the appropriate simplex ∆ consisting of L-empirical
distributions (or “types” in information theory): ∆L consists exactly of those distributions p that can
be written as pi = Li/L  where Li are non-negative integers that sum to L.
Deﬁnition 6 (Quantization). Given a positive integer L  deﬁne the L-quantization operation as
mapping a probability vector p to the closest (in (cid:96)1-distance) element of ∆L  ˜p = arg minq∈∆L (cid:107)p −
q(cid:107)1. For a matrix P ∈ Pm  deﬁne an L-quantization for any given factorization choice P = AB as
˜P = ˜A ˜B  where each row of ˜A and ˜B is the L-quantization of the respective row of A and B. Lastly 
deﬁne Pm L to be the set of all quantized probability matrices derived from Pm.
Via counting arguments  the cardinality of Pm L is bounded by |Pm L| ≤ (L + 1)2km. This quantized
family gives us the following approximation ability.
Lemma 7 (De-quantization). For a probability vector p  L-quantization satisﬁes |pi − ˜pi| ≤ 1
all i  and (cid:107)p − ˜p(cid:107)1 ≤ 2
|Qij − ˜Qij| ≤ 3

L for
L . For a conditional probability matrix Q ∈ Pm  any quantization ˜Q satisﬁes

L for all i. Furthermore  if Q >  per entry and L > 6

|R(Q) − R( ˜Q)| ≤ 6
L

and

   then:
|Rn(Q) − Rn( ˜Q)| ≤ 6
L

.

We now give a PAC-style relative uniform convergence bound on the empirical risk [27].
Theorem 8 (Relative uniform convergence). Assume lower-bounded P > δ and choose any τ > 0.
We then have the following uniform bound over all lower-bounded ˜Q >  in Pm L (Deﬁnition 6):

 sup

˜Q∈Pm L  ˜Q>

Pr

(cid:113)

R( ˜Q) − Rn( ˜Q)

> τ

R( ˜Q)

 ≤ e

−

6

(cid:115)

nτ 2

20 log

1
 +2τ

+2km log(L+1)

10

1
δ log

1


.

(4)

2 -SMOOTHED LOW-RANK  the add- 1

1

2 )/(Ci + 1
2 )

n (C  α) (see Section 5)

2-Smoothed Low-Rank  our proposed algorithm with provable guarantees: QAdd- 1

2 -LR

2 Low-Rank  smoothing the counts then factorizing: QNaive Add- 1
n (C  α))

n log 1
n log2(2n + k). We then complete the proof by de-quantizing using Lemma 7.

The proof of this Theorem consists  for ﬁxed ˜Q  of showing a sub-Poisson concentration of the sum
of the log likelihood ratios. This needs care  as a simple Bennett or Bernstein inequality is not enough 
because we need to eventually self-normalize. A critical component is to relate the variance and scale
of the concentration to the KL-risk and its square root  respectively. The theorem then follows from
uniformly bounding the normalized variance and scale over Pm L and a union bound.
To put the pieces together  ﬁrst note that thanks to the fact that the optimum is also a stable point of
the ADD- 1
2 nature of the updates implies that all of the elements
2n+k . By Lemma 7 and a proper choice of L of the order of (2n + k)2 
of Qn are lower-bounded by
the quantized version won’t be much smaller. We can thus choose  = 1
2n+k in Theorem 8 and use
our assumption of δ = km
n log(2n + k). Using Lemmas 5 and 7 to bound the contribution of the
empirical risk  we can then integrate the probability bound of (4) similarly to the realizable case.
This gives a bound on the expected risk of the quantized version of Qn of order km
 log L or
effectively km
7 Experiments
Having expounded the theoretical merit of properly smoothing structered conditional probability
matrices  we give a brief empirical study of its practical impact. We use both synthetic and real data.
The various methods compared are as follows:
• Add- 1
2  directly on the bigram counts: ˆP Add- 1
n ij = (Cij + 1
• Absolute-discounting  directly on the bigram counts: ˆP AD
• Naive Add- 1
• Naive Absolute-Discounting Low-Rank: QNaive AD-LR = QLR(n ˆP AD
• Stupid backoff (SB) of Google  a very simple algorithm proposed in [9]
• Kneser-Ney (KN)  a widely successful algorithm proposed in [17]
• Add- 1
• Absolute-Discounting-Smoothed Low-Rank  heuristic generalization of our algorithm: QAD-LR
The synthetic model is determined randomly. π is uniformly sampled from the k-simplex. The matrix
P = AB is generated as follows. The rows of A are uniformly sampled from the k-simplex. The
rows of B are generated in one of two ways: either sampled uniformly from the simplex or randomly
permuted power law distributions  to imitate natural language. The discount parameter is then ﬁxed
to 0.75. Figure 1(a) uses uniformly sampled rows of B  and shows that  despite attempting to harness
the low-rank structure of P   not only does Naive Add- 1
2 fall short  but it may even perform worse
2-Smoothed Low-Rank  on the other hand  reaps
than Add- 1
the beneﬁts of both smoothing and structure.
Figure 1(b) expands this setting to compare against other methods. Both of the proposed algorithms
have an edge on all other methods. Note that Kneser-Ney is not expected to perform well in this
regime (rows of B uniformly sampled)  because uniformly sampled rows of B do not behave
like natural language. On the other hand  for power law rows  even if k (cid:29) n  Kneser-Ney does
well  and it is only superseded by Absolute-Discounting-Smoothed Low-Rank. The consistent
good performance of Absolute-Discounting-Smoothed Low-Rank may be explained by the fact that
absolute-discounting seems to enjoy some of the competitive-optimality of Good-Turing estimation 
as recently demonstrated by [22]. This is why we chose to illustrate the ﬂexibility of our framework
by heuristically using absolute-discounting as the smoothing component.
Before moving on to experiments on real data  we give a short description of the data sets. All but the
ﬁrst one are readily available through the Python NLTK:
• tartuffe  a French text  train and test size: 9.3k words  vocabulary size: 2.8k words.
• genesis  English version  train and test size: 19k words  vocabulary size: 4.4k words
• brown  shortened Brown corpus  train and test size: 20k words  vocabulary size: 10.5k words
For natural language  using absolute-discounting is imperative  and we restrict ourselves to Absolute-
Discounting-Smoothed Low-Rank. The results of the performance of various algorithms are listed
in Table 1. For all these experiments  m = 50 and 200 iterations were performed. Note that the
proposed method has less cross-entropy per word across the board.

2

7

2 -LR = QLR(C + 1
2 )

2  which is oblivious to structure. Add- 1

(a) k = 100  m = 5

(b) k = 50  m = 3

(c) k = 1000  m = 10

Figure 1: Performance of selected algorithms over synthetic data

(a) Performance on tartuffe

(b) Performance on genesis

(c) rank selection for tartuffe

Figure 2: Experiments on real data

Dataset
tartuffe
genesis
brown

Add- 1
2
7.1808
7.3039
8.847

AD
6.268
6.041
7.9819

SB

6.0426
5.9058
7.973

KN

5.7555
5.7341
7.7001

AD-LR
5.6923
5.6673
7.609

Table 1: Cross-entropy results for different methods on several small corpora

We also illustrate the performance of different algorithms as the training size increases. Figure 2
shows the relative performance of selected algorithms with Stupid Backoff chosen as the baseline. As
Figure 2(a) suggests  the amount of improvement in cross-entropy at n = 15k is around 0.1 nats/word.
This improvement is comparable  even more signiﬁcant  than that reported in the celebrated work of
Chen and Goodman [11] for Kneser-Ney over the best algorithms at the time.
Even though our algorithm is given the rank m as a parameter  the internal dimension is not revealed 
if ever known. Therefore  we could choose the best m using model selection. Figure 2(c) shows one
way of doing this  by using a simple cross-validation for the tartuffe data set. In particular  half of the
data was held out as a validation set  and for a range of different choices for m  the model was trained
and its cross-entropy on the validation set was calculated. The ﬁgure shows that there exists a good
choice of m (cid:28) k. A similar behavior is observed for all data sets. Most interestingly  the ratio of the
best m to the vocabulary size corpus is reminiscent of the choice of internal dimension in [20].
8 Conclusion
Despite the theoretical impetus of the paper  the resulting algorithms considerably improve over
several benchmarks. There is more work ahead  however. Many possible theoretical reﬁnements
are in order  such as eliminating the logarithmic term in the sample complexity and dependence
on P (strong optimality). This framework naturally extends to tensors  such as for higher-order
N-gram language models. It is also worth bringing back the Markov assumption and understanding
how various mixing conditions inﬂuence the sample complexity. A more challenging extension 
and one we suspect may be necessary to truly be competitive with RNNs/LSTMs  is to parallel this
contribution in the context of generative models with long memory. The reason we hope to not only
be competitive with  but in fact surpass  these models is that they do not use distributional properties
of language  such as its quintessentially power-law nature. We expect smoothing methods such as
absolute-discounting  which do account for this  to lead to considerable improvement.
Acknowledgments We would like to thank Venkatadheeraj Pichapati and Ananda Theertha Suresh
for many helpful discussions. This work was supported in part by NSF grants 1065622 and 1564355.

8

Number of samples  n050010001500200025003000KL loss00.20.40.60.811.21.4Add-1/2Naive Add-1/2 Low-RankAdd-1/2-Smoothed Low-RankNumber of samples  n#1040.511.522.5KL loss10-210-1Add-1/2-Smoothed Low-RankAbsolute-discountingNaive Abs-Disc Low-RankAbs-Disc-Smoothed Low-RankKneser-NeyNumber of samples  n101520253035404550KL loss1.522.533.54Add-1/2-Smoothed Low-RankAbsolute-discountingNaive Abs-Disc Low-RankAbs-Disc-Smoothed Low-RankKneser-NeyTraining size (number of words)050001000015000Diff in test cross-entropy from baseline (nats/word)-0.4-0.35-0.3-0.25-0.2-0.15-0.1-0.050Stupid Backoff (baseline)Smoothed Low RankKneser-NeyTraining size (number of words)#1040.511.52Diff in test cross-entropy from baseline (nats/word)-0.3-0.25-0.2-0.15-0.1-0.0500.05Stupid Backoff (baseline)Smoothed Low RankKneser-Neym020406080100120Validation set cross-entropy (nats/word)5.685.75.725.745.765.785.85.82References
[1] Abe  Warmuth  and Takeuchi. Polynomial learnability of probabilistic concepts with respect to the

Kullback-Leibler divergence. In COLT  1991.

[2] Acharya  Jafarpour  Orlitsky  and Suresh. Optimal probability estimation with applications to prediction

and classiﬁcation. In COLT  2013.

[3] Agarwal  Anandkumar  Jain  and Netrapalli. Learning sparsely used overcomplete dictionaries via

alternating minimization. arXiv preprint arXiv:1310.7991  2013.

[4] Arora  Ge  Ma  and Moitra. Simple  efﬁcient  and neural algorithms for sparse coding. arXiv preprint

arXiv:1503.00778  2015.

[5] Ben Hamou  Boucheron  and Ohannessian. Concentration Inequalities in the Inﬁnite Urn Scheme for

Occupancy Counts and the Missing Mass  with Applications. Bernoulli  2017.

[6] Bhojanapalli  Kyrillidis  and Sanghavi. Dropping convexity for faster semi-deﬁnite optimization. arXiv

preprint arXiv:1509.03917  2015.

[7] Blei  Ng  and Jordan. Latent Dirichlet allocation. JMLR  2003.

[8] Borade and Zheng. Euclidean information theory. IEEE Int. Zurich Seminar on Comm.  2008.

[9] Brants  Popat  Xu  Och  and Dean. Large language models in machine translation. In EMNLP  2007.

[10] Buntine and Jakulin. Applying discrete PCA in data analysis. In Proceedings of the 20th conference on

Uncertainty in artiﬁcial intelligence  pages 59–66. AUAI Press  2004.

[11] Chen and Goodman. An empirical study of smoothing techniques for language modeling. Computer

Speech & Language  13(4):359–393  1999.

[12] Hofmann. Probabilistic latent semantic indexing. In ACM SIGIR  1999.

[13] Huang  Kakade  Kong  and Valiant. Recovering structured probability matrices.

arXiv:1602.06586  2016.

arXiv preprint

[14] Hutchinson  Ostendorf  and Fazel. Low rank language models for small training sets. IEEE SPL  2011.

[15] Hutchinson  Ostendorf  and Fazel. A Sparse Plus Low-Rank Exponential Language Model for Limited

Resource Scenarios. IEEE Trans. on Audio  Speech  and Language Processing  2015.

[16] Kamath  Orlitsky  Pichapati  and Suresh. On learning distributions from their samples. In COLT  2015.

[17] Kneser and Ney. Improved backing-off for m-gram language modeling. In ICASSP  1995.

[18] Lee and Seung. Algorithms for non-negative matrix factorization. In NIPS  2001.

[19] Levy and Goldberg. Neural word embedding as implicit matrix factorization. In NIPS  2014.
[20] Mikolov  Kombrink  Burget  ˇCernock`y  and Khudanpur. Extensions of recurrent neural network language

model. In ICASSP  2011.

[21] Ohannessian and Dahleh. Rare probability estimation under regularly varying heavy tails. In COLT  2012.

[22] Orlitsky and Suresh. Competitive distribution estimation: Why is Good-Turing good. In NIPS  2015.

[23] Papadimitriou  Tamaki  Raghavan  and Vempala. Latent semantic indexing: A probabilistic analysis. In

ACM SIGACT-SIGMOD-SIGART  1998.

[24] Parikh  Saluja  Dyer  and Xing. Language Modeling with Power Low Rank Ensembles. arXiv preprint

arXiv:1312.7077  2013.

[25] Shazeer  Pelemans  and Chelba. Skip-gram Language Modeling Using Sparse Non-negative Matrix

Probability Estimation. arXiv preprint arXiv:1412.1454  2014.

[26] Valiant and Valiant. Instance optimal learning. arXiv preprint arXiv:1504.05321  2015.

[27] Vapnik. Statistical Learning Theory. Wiley-Interscience  1998.

[28] Williams  Prasad  Mrva  Ash  and Robinson. Scaling Recurrent Neural Network Language Models. arXiv

preprint arXiv:1502.00512  2015.

[29] Zhu  Yang  and Oja. Multiplicative updates for learning with stochastic matrices. In Im. An. 2013.

9

,Moein Falahatgar
Mesrob Ohannessian
Alon Orlitsky
Siwei Wang
Longbo Huang