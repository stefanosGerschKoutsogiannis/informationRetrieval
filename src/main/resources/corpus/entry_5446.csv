2011,The Doubly Correlated Nonparametric Topic Model,Topic models are learned via a statistical model of variation within document collections  but designed to extract meaningful semantic structure.  Desirable traits include the ability to incorporate annotations or metadata associated with documents; the discovery of correlated patterns of topic usage; and the avoidance of parametric assumptions  such as manual specification of the number of topics.  We propose a doubly correlated nonparametric topic (DCNT) model  the first model to simultaneously capture all three of these properties.  The DCNT models metadata via a flexible  Gaussian regression on arbitrary input features; correlations via a scalable square-root covariance representation; and nonparametric selection from an unbounded series of potential topics via a stick-breaking construction.  We validate the semantic structure and predictive performance of the DCNT using a corpus of NIPS documents annotated by various metadata.,The Doubly Correlated Nonparametric Topic Model

Dae Il Kim and Erik B. Sudderth
Department of Computer Science

Brown University  Providence  RI 02906

daeil@cs.brown.edu  sudderth@cs.brown.edu

Abstract

Topic models are learned via a statistical model of variation within document col-
lections  but designed to extract meaningful semantic structure. Desirable traits
include the ability to incorporate annotations or metadata associated with docu-
ments; the discovery of correlated patterns of topic usage; and the avoidance of
parametric assumptions  such as manual speciﬁcation of the number of topics. We
propose a doubly correlated nonparametric topic (DCNT) model  the ﬁrst model
to simultaneously capture all three of these properties. The DCNT models meta-
data via a ﬂexible  Gaussian regression on arbitrary input features; correlations
via a scalable square-root covariance representation; and nonparametric selection
from an unbounded series of potential topics via a stick-breaking construction.
We validate the semantic structure and predictive performance of the DCNT using
a corpus of NIPS documents annotated by various metadata.

1

Introduction

The contemporary problem of exploring huge collections of discrete data  from biological sequences
to text documents  has prompted the development of increasingly sophisticated statistical models.
Probabilistic topic models represent documents via a mixture of topics  which are themselves dis-
tributions on the discrete vocabulary of the corpus. Latent Dirichlet allocation (LDA) [3] was the
ﬁrst hierarchical Bayesian topic model  and remains inﬂuential and widely used. However  it suffers
from three key limitations which are jointly addressed by our proposed model.
The ﬁrst assumption springs from LDA’s Dirichlet prior  which implicitly neglects correlations1 in
document-speciﬁc topic usage. In diverse corpora  true semantic topics may exhibit strong (positive
or negative) correlations; neglecting these dependencies may distort the inferred topic structure. The
correlated topic model (CTM) [2] uses a logistic-normal prior to express correlations via a latent
Gaussian distribution. However  its usage of a “soft-max” (multinomial logistic) transformation
requires a global normalization  which in turn presumes a ﬁxed  ﬁnite number of topics.
The second assumption is that each document is represented solely by an unordered “bag of words”.
However  text data is often accompanied by a rich set of metadata such as author names  publi-
cation dates  relevant keywords  etc. Topics that are consistent with such metadata may also be
more semantically relevant. The Dirichlet multinomial regression (DMR) [11] model conditions
LDA’s Dirichlet parameters on feature-dependent linear regressions; this allows metadata-speciﬁc
topic frequencies but retains other limitations of the Dirichlet. Recently  the Gaussian process topic
model [1] incorporated correlations at the topic level via a topic covariance  and the document level
via an appropriate GP kernel function. This model remains parametric in its treatment of the num-
ber of topics  and computational scaling to large datasets is challenging since learning scales super-
linearly with the number of documents.

1One can exactly sample from a Dirichlet distribution by drawing a vector of independent gamma random

variables  and normalizing so they sum to one. This normalization induces slight negative correlations.

1

The third assumption is the a priori choice of the number of topics. The most direct nonparametric
extension of LDA is the hierarchical Dirichlet process (HDP) [17]. The HDP allows an unbounded
set of topics via a latent stochastic process  but nevertheless imposes a Dirichlet distribution on any
ﬁnite subset of these topics. Alternatively  the nonparametric Bayes pachinko allocation [9] model
captures correlations within an unbounded topic collection via an inferred  directed acyclic graph.
More recently  the discrete inﬁnite logistic normal [13] (DILN) model of topic correlations used an
exponentiated Gaussian process (GP) to rescale the HDP. This construction is based on the gamma
process representation of the DP [5]. While our goals are similar  we propose a rather different
model based on the stick-breaking representation of the DP [16]. This choice leads to arguably
simpler learning algorithms  and also facilitates our modeling of document metadata.
In this paper  we develop a doubly correlated nonparametric topic (DCNT) model which captures
between-topic correlations  as well as between-document correlations induced by metadata  for an
unbounded set of potential topics. As described in Sec. 2  the global soft-max transformation of
the DMR and CTM is replaced by a stick-breaking transformation  with inputs determined via both
metadata-dependent linear regressions and a square-root covariance representation. Together  these
choices lead to a well-posed nonparametric model which allows tractable MCMC learning and in-
ference (Sec. 3). In Sec. 4  we validate the model using a toy dataset  as well as a corpus of NIPS
documents annotated by author and year of publication.

2 A Doubly Correlated Nonparametric Topic Model

The DCNT is a hierarchical  Bayesian nonparametric generalization of LDA. Here we give an
overview of the model structure (see Fig. 1)  focusing on our three key innovations.

2.1 Document Metadata
Consider a collection of D documents. Let φd ∈ RF denote a feature vector capturing the metadata
associated with document d  and φ an F × D matrix of corpus metadata. When metadata is unavail-
able  we assume φd = 1. For each of an unbounded sequence of topics k  let ηf k ∈ R denote an
associated signiﬁcance weight for feature f  and η:k ∈ RF a vector of these weights.2
We place a Gaussian prior η:k ∼ N (µ  Λ−1) on each topic’s weights  where µ ∈ RF is a vector of
mean feature responses  and Λ is an F × F diagonal precision matrix. In a hierarchical Bayesian
fashion [6]  these parameters have priors µf ∼ N (0  γµ)  λf ∼ Gam(af   bf ). Appropriate values
for the hyperparameters γµ  af   and bf are discussed later.
Given η and φd  the document-speciﬁc “score” for topic k is sampled as ukd ∼ N (ηT
real-valued scores are mapped to document-speciﬁc topic frequencies πkd in subsequent sections.

:kφd  1). These

2.2 Topic Correlations

For topic k in the ordered sequence of topics  we deﬁne a sequence of k linear transformation
weights Ak(cid:96)  (cid:96) = 1  . . .   k. We then sample a variable vkd as follows:

(cid:18) k(cid:88)

(cid:19)

vkd ∼ N

Ak(cid:96)u(cid:96)d  λ−1

v

(1)

(cid:96)=1

Let A denote a lower triangular matrix containing these values Ak(cid:96)  padded by zeros. Slightly
abusing notation  we can then compactly write this transformation as v:d ∼ N (Au:d  L−1)  where
L = λvI is an inﬁnite diagonal precision matrix. Critically  note that the distribution of vkd depends
only on the ﬁrst k entries of u:d  not the inﬁnite tail of scores for subsequent topics.
Marginalizing u:d  the covariance of v:d equals Cov[v:d] = AAT + L−1 (cid:44) Σ. As in the classical
factor analysis model  A encodes a square-root representation of an output covariance matrix. Our
integration of input metadata has close connections to the semiparametric latent factor model [18] 
but we replace their kernel-based GP covariance representation with a feature-based regression.

2For any matrix η  we let η:k denote a column vector indexed by k  and ηf : a row vector indexed by f.

2

Figure 1: Directed graphical representation of a DCNT model for D documents containing N words. Each of
the unbounded set of topics has a word distribution Ωk. The topic assignment zdn for word wdn depends on
document-speciﬁc topic frequencies πd  which have a correlated dependence on the metadata φd produced by
A and η. The Gaussian latent variables ud and vd implement this mapping  and simplify MCMC methods.

Given similar lower triangular representations of factorized covariance matrices  conventional
Bayesian factor analysis models place a symmetric Gaussian prior Ak(cid:96) ∼ N (0  λ−1
A ). Under this
prior  however  E[Σkk] = kλ−1
A grows linearly with k. This can produce artifacts for standard factor
analysis [10]  and is disastrous for the DCNT where k is unbounded. We instead propose an alterna-
tive prior Ak(cid:96) ∼ N (0  (kλA)−1)  so that the variance of entries in the kth row is reduced by a factor
of k. This shrinkage is carefully chosen so that E[Σkk] = λ−1
A ) and Ak(cid:96) = 0 for k (cid:54)= (cid:96)  we
If we constrain A to be a diagonal matrix  with Akk ∼ N (0  λ−1
recover a simpliﬁed singly correlated nonparametric topic (SCNT) model which captures metadata
but not topic correlations. For either model  the precision parameters are assigned conjugate gamma
priors λv ∼ Gam(av  bv)  λA ∼ Gam(aA  bA).
2.3 Logistic Mapping to Stick-Breaking Topic Frequencies

A remains constant.

document d  where(cid:80)∞

Stick breaking representations are widely used in applications of nonparametric Bayesian models 
and lead to convenient sampling algorithms [8]. Let πkd be the probability of choosing topic k in

k=1 πkd = 1. The DCNT constructs these probabilities as follows:

k−1(cid:89)

(cid:96)=1

πkd = ψ(vkd)

ψ(−v(cid:96)d) 

ψ(vkd) =

1

1 + exp(−vkd)

.

(2)

Here  0 < ψ(vkd) < 1 is the classic logistic function  which satisﬁes ψ(−v(cid:96)d) = 1 − ψ(v(cid:96)d). This
same transformation is part of the so-called logistic stick-breaking process [14]  but that model is
motivated by different applications  and thus employs a very different prior distribution for vkd.
Given the distribution π:d  the topic assignment indicator for word n in document d is drawn accord-
ing to zdn ∼ Mult(π:d). Finally  wdn ∼ Mult(Ωzdn ) where Ωk ∼ Dir(β) is the word distribution
for topic k  sampled from a Dirichlet prior with symmetric hyperparameters β.

3 Monte Carlo Learning and Inference

We use a Markov chain Monte Carlo (MCMC) method to approximately sample from the posterior
distribution of the DCNT. For most parameters  our choice of conditionally conjugate priors leads to
closed form Gibbs sampling updates. Due to the logistic stick-breaking transformation  closed form
resampling of v is intractable; we instead use a Metropolis independence sampler [6].
Our sampler is based on a ﬁnite truncation of the full DCNT model  which has proven useful with
other stick-breaking priors [8  14  15]. Let K be the maximum number topics. As our experiments
demonstrate  K is not the number of topics that will be utilized by the learned model  but rather a
(possibly loose) upper bound on that number. For notational convenience  let ¯K = K − 1.

3

Under the truncated model  η is a F × ¯K matrix of regression coefﬁcients  and u is a ¯K × D
matrix satisfying u:d ∼ N (ηT φd  I ¯K). Similarly  A is a ¯K × ¯K lower triangular matrix  and
v:d ∼ N (Au:d  λ−1
v I ¯K). The probabilities πkd for the ﬁrst ¯K topics are set as in eq. (2)  with the

ﬁnal topic set so that a valid distribution is ensured: πKd = 1 −(cid:80)K−1

k=1 ψ(−vkd).
3.1 Gibbs Updates for Topic Assignments  Correlation Parameters  and Hyperparameters

k=1 πkd =(cid:81)K−1

The precision parameter λf controls the variability of the feature weights associated with each topic.
As in many regression models  the gamma prior is conjugate so that

p(λf | η  af   bf ) ∝ Gam(λf | af   bf )

N (ηf k | µf   λ−1
f )

(cid:18)

∝ Gam

k=1

λf | 1
2

¯K + af  

1
2

¯K(cid:88)

k=1

(cid:19)

.

(3)

(ηf k − µf )2 + bf

(cid:19)

Similarly  the precision parameter λv has a gamma prior and posterior:

p(λv | v  av  bv) ∝ Gam(λv | av  bv)

N (v:d | Au:d  L−1)

(cid:18)

∝ Gam

(4)
Entries of the regression matrix A have a rescaled Gaussian prior Ak(cid:96) ∼ N (0  (kλA)−1). With a
gamma prior  the precision parameter λA nevertheless has the following gamma posterior:

¯KD + av 

(v:d − Au:d)T (v:d − Au:d) + bv

λv | 1
2

1
2

d=1

.

p(λA | A  aA  bA) ∝ Gam(λA | aA  bA)

N (Ak(cid:96) | 0  (kλA)−1)

(cid:18)

∝ Gam

k=1

(cid:96)=1

λA | 1
2

¯K( ¯K − 1) + aA 

1
2

(cid:19)

.

(5)

kA2

k(cid:96) + bA

¯K(cid:88)

k(cid:88)

k=1

(cid:96)=1

Conditioning on the feature regression weights η  the mean weight µf in our hierarchical prior for
each feature f has a Gaussian posterior:

¯K(cid:89)

D(cid:88)

¯K(cid:89)

k(cid:89)

D(cid:89)

d=1

¯K(cid:89)

p(µf | η) ∝ N (µf | 0  γµ)

N (ηf k | µf   λ−1
f )

(cid:18)

∝ N

k=1

µf |

γµ

¯Kγµ + λ−1

f

¯K(cid:88)

k=1

ηf k  (γ−1

µ + ¯Kλf )−1

(cid:19)

(6)

To sample η:k  the linear function relating metadata to topic k  we condition on all documents uk: as
well as φ  µ  and Λ. Columns of η are conditionally independent  with Gaussian posteriors:

p(η:k | u  φ  µ  Λ) ∝ N (η:k | µ  Λ−1)N (uT

k: | φT η:k  ID)

(7)
Similarly  the scores u:d for each document are conditionally independent with Gaussian posteriors:

∝ N (η:k | (Λ + φφT )−1(φuT

k: + Λµ)  (Λ + φφT )−1).

p(u:d | v:d  η  φd  L) ∝ N (u:d | ηT φd  I ¯K)N (v:d | Au:d  L−1)

∝ N (u:d | (I ¯K + AT LA)−1(AT Lv:d + ηT φd)  (I ¯K + AT LA)−1).
(8)
To resample A  we note that its rows are conditionally independent. The posterior of the k entries
Ak: in row k depends on vk: and ˆUk (cid:44) u1:k :  the ﬁrst k entries of u:d for each document d:

k: | vk:  ˆUk  λA  λv) ∝ k(cid:89)

p(AT

N (Akj | 0  (kλA)−1)N (vT
k: | (kλAλ−1

v Ik + ˆUk ˆU T

k: | ˆU T
k AT
k )−1 ˆUkvT

j=1

∝ N (AT

k:  λ−1

v ID)

k:  (kλAIk + λv ˆUk ˆU T

k )−1).

(9)

4

For the SCNT model  there is a related but simpler update (see supplemental material).
As in collapsed sampling algorithms for LDA [7]  we can analytically marginalize the word distri-
\dn
bution Ωk for each topic. Let M
kw denote the number of instances of word w assigned to topic k 
excluding token n in document d  and M
the number of total tokens assigned to topic k. For a
vocabulary with W unique word types  the posterior distribution of topic indicator zdn is then

\dn
k.

p(zdn = k | π:d  z\dn) ∝ πkd

.

(10)

(cid:33)

(cid:32)

\dn
M
kw + β
\dn
k. + W β

M

Recall that the topic probabilities π:d are determined from v:d via Equation (2).

3.2 Metropolis Independence Sampler Updates for Topic Activations

The posterior distribution of v:d does not have a closed analytical form due to the logistic nonlin-
earity underlying our stick-breaking construction. We instead employ a Metropolis-Hastings inde-
pendence sampler  where proposals q(v∗
v I ¯K) are drawn from
the prior. Combining this with the likelihood of the Nd word tokens  the proposal is accepted with
probability min(A(v∗

:d | v:d  A  u:d  λv) = N (v∗

:d | Au:d  λ−1

:d  v:d)  1)  where

p(v∗

:d | A  u:d  λv)(cid:81)Nd
p(v:d | A  u:d  λv)(cid:81)Nd
n=1 p(zdn | v∗
:d)q(v:d | v∗
(cid:19)(cid:80)Nd
n=1 p(zdn | v:d)q(v∗
K(cid:89)
Nd(cid:89)

(cid:18) π∗

n=1 δ(zdn k)

p(zdn | v∗
:d)
p(zdn | v:d)

=

kd
πkd

n=1

k=1

A(v∗

:d  v:d) =

=

:d  A  u:d  λv)
:d | v:d  A  u:d  λv)

(11)

Because the proposal cancels with the prior distribution in the acceptance ratio A(v∗
:d  v:d)  the ﬁnal
probability depends only on a ratio of likelihood functions  which can be easily evaluated from
counts of the number of words assigned to each topic by zd.

4 Experimental Results

4.1 Toy Bars Dataset

Following related validations of the LDA model [7]  we ran experiments on a toy corpus of “images”
designed to validate the features of the DCNT. The dataset consisted of 1 500 images (documents) 
each containing a vocabulary of 25 pixels (word types) arranged in a 5x5 grid. Documents can be
visualized by displaying pixels with intensity proportional to the number of corresponding words
(see Figure 2). Each training document contained 300 word tokens.
Ten topics were deﬁned  corresponding to all possible horizontal and vertical 5-pixel “bars”. We
consider two toy datasets. In the ﬁrst  a random number of topics is chosen for each document  and
then a corresponding subset of the bars is picked uniformly at random. In the second  we induce
topic correlations by generating documents that contain a combination of either only horizontal
(topics 1-5) or only vertical (topics 6-10) bars. For these datasets  there was no associated metadata 
so the input features were simply set as φd = 1.
Using these toy datasets  we compared the LDA model to several versions of the DCNT. For LDA 
we set the number of topics to the true value of K = 10. Similar to previous toy experiments [7] 
we set the parameters of its Dirichlet prior over topic distributions to α = 50/K  and the topic
smoothing parameter to β = 0.01. For the DCNT model  we set γµ = 106  and all gamma prior
hyperparameters as a = b = 0.01  corresponding to a mean of 1 and a variance of 100. To initialize
the sampler  we set the precision parameters to their prior mean of 1  and sample all other variables
from their prior. We compared three variants of the DCNT model: the singly correlated SCNT (A
constrained to be diagonal) with K = 10  the DCNT with K = 10  and the DCNT with K = 20.
The ﬁnal case explores whether our stick-breaking prior can successfully infer the number of topics.
For the toy dataset with correlated topics  the results of running all sampling algorithms for 10 000
iterations are illustrated in Figure 2. On this relatively clean data  all models limited to K = 10

5

Figure 2: A dataset of correlated toy bars (example document images in bottom left). Top: From left to
right  the true counts of words generated by each topic  and the recovered counts for LDA (K = 10)  SCNT
(K = 10)  DCNT (K = 10)  and DCNT (K = 20). Note that the true topic order is not identiﬁable. Bottom:
Inferred topic covariance matrices for the four corresponding models. Note that LDA assumes all topics have
a slight negative correlation  while the DCNT infers more pronounced positive correlations. With K = 20
potential DCNT topics  several are inferred to be unused with high probability  and thus have low variance.

topics recover the correct topics. With K = 20 topics  the DCNT recovers the true topics  as well as
a redundant copy of one of the bars. This is typical behavior for sampling runs of this length; more
extended runs usually merge such redundant bars. The development of more rapidly mixing MCMC
methods is an interesting area for future research.
To determine the topic correlations corresponding to a set of learned model parameters  we use
a Monte Carlo estimate (details in the supplemental material). To make these matrices easier to
visualize  the Hungarian algorithm was used to reorder topic labels for best alignment with the
ground truth topic assignments. Note the signiﬁcant blocks of positive correlations recovered by the
DCNT  reﬂecting the true correlations used to create this toy data.

4.2 NIPS Corpus

The NIPS corpus that we used consisted of publications from previous NIPS conferences 0-12
(1987-1999)  including various metadata (year of publication  authors  and section categories). We
compared four variants of the DCNT model: a model which ignored metadata  a model with in-
dicator features for the year of publication  a model with indicator features for year of publication
and the presence of highly proliﬁc authors (those with more than 10 publications)  and a model with
features for year of publication and additional authors (those with more than 5 publications). In all
cases  the feature matrix φ is binary. All models were truncated to use at most K = 50 topics  and
the sampler initialized as in Sec. 4.1.

4.2.1 Conditioning on Metadata

A learned DCNT model provides predictions for how topic frequencies change given particular
metadata associated with a document. In Figure 3  we show how predicted topic frequencies change
over time  conditioning also on one of three authors (Michael Jordan  Geoffrey Hinton  or Terrence
Sejnowski). For each  words from a relevant topic illustrate how conditioning on a particular au-
thor can change the predicted document content. For example  the visualization associated with
Michael Jordan shows that the frequency of the topic associated with probabilistic models gradually
increases over the years  while the topic associated with neural networks decreases. Conditioning
on Geoffrey Hinton puts larger mass on a topic which focuses on models developed by his research
group. Finally  conditioning on Terrence Sejnowski dramatically increases the probability of topics
related to neuroscience.

4.2.2 Correlations between Topics

The DCNT model can also capture correlations between topics. In Fig. 4  we visualize this us-
ing a diagram where the size of a colored grid is proportional to the magnitude of the correlation

6

Figure 3: The DCNT predicts topic frequencies over the years (1987-1999) for documents with (a) none of
the most proliﬁc authors  (b) the Michael Jordan feature  (c) the Geoffrey Hinton feature  and (d) the Terrence
Sejnowski feature. The stick-breaking distribution at the top shows the frequencies of each topic  averaging
over all years; note some are unused. The middle row illustrates the word distributions for the topics highlighted
by red dots in their respective columns. Larger words are more probable.

Figure 4: A Hinton diagram of correlations between all pairs of topics  where the sizes of squares indicates the
magnitude of dependence  and red and blue squares indicate positive and negative correlations  respectively. To
the right are the top six words from three strongly correlated topic pairs. This visualization  along with others in
this paper  are interactive and can be downloaded from this page: http://www.cs.brown.edu/˜daeil.

coefﬁcients between two topics. The results displayed in this ﬁgure are for a model trained with-
out metadata. We can see that the model learned strong positive correlations between function and
learning topics which have strong semantic similarities  but are not identical. Another positive cor-
relation that the model discovered was between the topics visual and neuron; of course there are
many papers at NIPS which study the brain’s visual cortex. A strong negative correlation was found
between the network and model topics  which might reﬂect an idealogical separation between papers
studying neural networks and probabilistic models.

4.3 Predictive Likelihood

In order to quantitatively measure the generalization power of our DCNT model  we tested several
variants on two versions of the toy bars dataset (correlated & uncorrelated). We also compared
models on the NIPS corpus  to explore more realistic data where metadata is available. The test data
for the toy dataset consisted of 500 documents generated by the same process as the training data 

7

Figure 5: Perplexity scores (lower is better) computed via Chib-style estimators for several topic models.
Left: Test performance for the toy datasets with uncorrelated bars (-A) and correlated bars (-B). Right: Test
performance on the NIPS corpus with various metadata: no features (-noF)  year features (-Y)  year and proliﬁc
author features (over 10 publications  -YA1)  and year and additional author features (over 5 publications  -YA2).

while the NIPS corpus was split into training and tests subsets containing 80% and 20% of the full
corpus  respectively. Over the years 1988-1999  there were a total of 328 test documents.
We calculated predictive likelihood estimates using a Chib-style estimator [12]; for details see the
supplemental material. In a previous comparison [19]  the Chib-style estimator was found to be far
more accurate than alternatives like the harmonic mean estimator. Note that there is some subtlety
in correctly implementing the Chib-style estimator for our DCNT model  due to the possibility of
rejection of our Metropolis-Hastings proposals.
Predictive negative log-likelihood estimates were normalized by word counts to determine perplexity
scores [3]. We tested several models  including the SCNT and DCNT  LDA with α = 1 and β =
0.01  and the HDP with full resampling of its concentration parameters. For the toy bars data  we
set the number of topics to K = 10 for all models except the HDP  which learned K = 15. For the
NIPS corpus  we set K = 50 for all models except the HDP  which learned K = 86.
For the toy datasets  the LDA and HDP models perform similarly. The SCNT and DCNT are both
superior  apparently due to their ability to capture non-Dirichlet distributions on topic occurrence
patterns. For the NIPS data  all of the DCNT models are substantially more accurate than LDA and
the HDP. Including metadata encoding the year of publication  and possibly also the most proliﬁc
authors  provides slight additional improvements in DCNT accuracy. Interestingly  when a larger
set of author features is included  accuracy becomes slightly worse. This appears to be an overﬁtting
issue: there are 125 authors with over 5 publications  and only a handful of training examples for
each one.
While it is pleasing that the DCNT and SCNT models seem to provide improved predictive like-
lihoods  a recent study on the human interpretability of topic models showed that such scores do
not necessarily correlate with more meaningful semantic structures [4]. In many ways  the interac-
tive visualizations illustrated in Sec. 4.2 provide more assurance that the DCNT can capture useful
properties of real corpora.

5 Discussion

The doubly correlated nonparametric topic model ﬂexibly allows the incorporation of arbitrary fea-
tures associated with documents  captures correlations that might exist within a dataset’s latent top-
ics  and can learn an unbounded set of topics. The model uses a set of efﬁcient MCMC techniques
for learning and inference  and is supported by a set of web-based tools that allow users to visualize
the inferred semantic structure.

Acknowledgments

This research supported in part by IARPA under AFRL contract number FA8650-10-C-7059. Dae Il
Kim supported in part by an NSF Graduate Fellowship. The views and conclusions contained herein
are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies
or endorsements  either expressed or implied  of IARPA  AFRL  or the U.S. Government.

8

LDAHDPDCNT−noFDCNT−YDCNT−YA1DCNT−YA21800185019001950200020502100Perplexity scores (NIPS)1975.462060.431926.421925.561923.11932.26LDA−AHDP−ADCNT−ASCNT−ALDA−BHDP−BDCNT−BSCNT−B02468101214Perplexity (Toy Data)10.510.529.7910.1412.0812.1311.5111.75References
[1] A. Agovic and A. Banerjee. Gaussian process topic models. In UAI  2010.
[2] D. M. Blei and J. D. Lafferty. A correlated topic model of science. AAS  1(1):17–35  2007.
[3] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res.  3:993–1022 

March 2003.

[4] J. Chang  J. Boyd-Graber  S. Gerrish  C. Wang  and D. M. Blei. Reading tea leaves: How humans interpret

topic models. In NIPS  2009.

[5] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. An. Stat.  1(2):209–230  1973.
[6] A. Gelman  J. B. Carlin  H. S. Stern  and D. B. Rubin. Bayesian Data Analysis. Chapman & Hall  2004.
[7] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. PNAS  2004.
[8] H. Ishwaran and L. F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American

Statistical Association  96(453):161–173  Mar. 2001.

[9] W. Li  D. Blei  and A. McCallum. Nonparametric Bayes Pachinko allocation. In UAI  2008.
[10] H. F. Lopes and M. West. Bayesian model assessment in factor analysis. Stat. Sinica  14:41–67  2004.
[11] D. Mimno and A. McCallum. Topic models conditioned on arbitrary features with dirichlet-multinomial

regression. In UAI  2008.

[12] I. Murray and R. Salakhutdinov. Evaluating probabilities under high-dimensional latent variable models.

In NIPS 21  pages 1137–1144. 2009.

[13] J. Paisley  C. Wang  and D. Blei. The discrete inﬁnite logistic normal distribution for mixed-membership

modeling. In AISTATS  2011.

[14] L. Ren  L. Du  L. Carin  and D. B. Dunson. Logistic stick-breaking process. JMLR  12  2011.
[15] A. Rodriguez and D. B. Dunson. Nonparametric bayesian models through probit stick-breaking processes.

J. Bayesian Analysis  2011.

[16] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Stat. Sin.  4:639–650  1994.
[17] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. Journal of the

American Statistical Association  101(476):1566–1581  2006.

[18] Y. W. Teh  M. Seeger  and M. I. Jordan. Semiparametric latent factor models. In AIStats 10  2005.
[19] H. M. Wallach  I. Murray  R. Salakhutdinov  and D. Mimno. Evaluation methods for topic models. In

ICML  2009.

9

,Boqing Gong
Wei-Lun Chao
Kristen Grauman
Fei Sha