2017,Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity,Learning the directed acyclic graph (DAG) structure of a Bayesian network from observational data is a notoriously difficult problem for which many non-identifiability and hardness results are known. In this paper we propose a provably polynomial-time algorithm for learning sparse Gaussian Bayesian networks with equal noise variance --- a class of Bayesian networks for which the DAG structure can be uniquely identified from observational data --- under high-dimensional settings. We show that $O(k^4 \log p)$ number of samples suffices for our method to recover the true DAG structure with high probability  where $p$ is the number of variables and $k$ is the maximum Markov blanket size. We obtain our theoretical guarantees under a condition called \emph{restricted strong adjacency faithfulness} (RSAF)  which is strictly weaker than strong faithfulness --- a condition that other methods based on conditional independence testing need for their success. The sample complexity of our method matches the information-theoretic limits in terms of the dependence on $p$. We validate our theoretical findings through synthetic experiments.,Learning Identiﬁable Gaussian Bayesian Networks in

Polynomial Time and Sample Complexity

Department of Computer Science  Purdue University  West Lafayette  IN - 47906

Asish Ghoshal and Jean Honorio

{aghoshal  jhonorio}@purdue.edu

Abstract

Learning the directed acyclic graph (DAG) structure of a Bayesian network from ob-
servational data is a notoriously difﬁcult problem for which many non-identiﬁability
and hardness results are known. In this paper we propose a provably polynomial-
time algorithm for learning sparse Gaussian Bayesian networks with equal noise
variance — a class of Bayesian networks for which the DAG structure can be
uniquely identiﬁed from observational data — under high-dimensional settings.
We show that O(k4 log p) number of samples sufﬁces for our method to recover
the true DAG structure with high probability  where p is the number of variables
and k is the maximum Markov blanket size. We obtain our theoretical guarantees
under a condition called restricted strong adjacency faithfulness (RSAF)  which is
strictly weaker than strong faithfulness — a condition that other methods based on
conditional independence testing need for their success. The sample complexity of
our method matches the information-theoretic limits in terms of the dependence on
p. We validate our theoretical ﬁndings through synthetic experiments.

1

Introduction and Related Work

Motivation. The problem of learning the directed acyclic graph (DAG) structure of Bayesian
networks (BNs) in general  and Gaussian Bayesian networks (GBNs) — or equivalently linear
Gaussian structural equation models (SEMs) — in particular  from observational data has a long
history in the statistics and machine learning community. This is  in part  motivated by the desire to
uncover causal relationships between entities in domains as diverse as ﬁnance  genetics  medicine 
neuroscience and artiﬁcial intelligence  to name a few. Although in general  the DAG structure
of a GBN or linear Gaussian SEM cannot be uniquely identiﬁed from purely observational data
(i.e.  multiple structures can encode the same conditional independence relationships present in the
observed data set)  under certain restrictions on the generative model  the DAG structure can be
uniquely determined. Furthermore  the problem of learning the structure of BNs exactly is known
to be NP-complete even when the number of parents of a node is at most q  for q > 1  [1]. It is
also known that approximating the log-likelihood to a constant factor  even when the model class is
restricted to polytrees with at-most two parents per node  is NP-hard [2].
Peters and Bühlmann [3] recently showed that if the noise variances are the same  then the structure
of a GBN can be uniquely identiﬁed from observational data. As observed by them  this “assumption
of equal error variances seems natural for applications with variables from a similar domain and is
commonly used in time series models”. Unfortunately  even for the equal noise-variance case  no
polynomial time algorithm is known.
Contribution. In this paper we develop a polynomial time algorithm for learning a subclass of
BNs exactly: sparse GBNs with equal noise variance. This problem has been considered by [3]
who proposed an exponential time algorithm based on `0-penalized maximum likelihood estimation
(MLE)  and a heuristic greedy search method without any guarantees. Our algorithm involves
estimating a p-dimensional inverse covariance matrix and solving 2(p  1) at-most-k-dimensional
31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

ordinary least squares problems  where p is the number of nodes and k is the maximum Markov
blanket size of a variable. We show that O((k4/↵2) log(p/)) samples sufﬁce for our algorithm to
recover the true DAG structure and to approximate the parameters to at most ↵ additive error  with
probability at least 1    for some > 0. The sample complexity of O(k4 log p) is close to the
information-theoretic limit of ⌦(k log p) for learning sparse GBNs as obtained by [4]. The main
assumption under which we obtain our theoretical guarantees is a condition that we refer to as the
↵-restricted strong adjacency faithfulness (RSAF). We show that RSAF is a strictly weaker condition
than strong faithfulness  which methods based on independence testing require for their success. In
this identiﬁable regime  given enough samples  our method can recover the exact DAG structure of
any Gaussian distribution. However  existing exact algorithms like the PC algorithm [5] can fail to
recover the correct skeleton for distributions that are not faithful  and fail to orient a number of edges
that are not covered by the Meek orientation rules [6  7]. Of independent interest is our analysis of
OLS regression under the random design setting for which we obtain `1 error bounds.
Related Work. In the this section  we ﬁrst discuss some identiﬁability results for GBNs known in
the literature and then survey relevant algorithms for learning GBNs and Gaussian SEMs.
[3] proved identiﬁability of distributions drawn from a restricted SEM with additive noise  where in
the restricted SEM the functions are assumed to be non-linear and thrice continuously differentiable.
It is also known that SEMs with linear functions and strictly non-Gaussian noise are identiﬁable [8].
Indentiﬁability of the DAG structure for the linear function and Gaussian noise case was proved by
[9] when noise variables are assumed to have equal variance.
Algorithms for learning BNs typically fall into two distinct categories  namely: independence test
based methods and score based methods. This dichotomy also extends to the Gaussian case. Score
based methods assign a score to a candidate DAG structure based on how well it explains the observed
data  and then attempt to ﬁnd the highest scoring structure. Popular examples for the Gaussian
distribution are the log-likelihood based BIC and AIC scores and the `0-penalized log-likelihood
score by [10]. However  given that the number of DAGs and sparse DAGs is exponential in the
number of variables [4  11]  exhaustively searching for the highest scoring DAG in the combinatorial
space of all DAGs  which is a feature of existing exact search based algorithms  is prohibitive for all
but a few number of variables. [12] propose a score-based method  based on concave penalization of
a reparameterized negative log-likelihood function  which can learn a GBN over 1000 variables in an
hour. However  the resulting optimization problem is neither convex — therefore is not guaranteed to
ﬁnd a globally optimal solution — nor solvable in polynomial time. In light of these shortcomings 
approximation algorithms have been proposed for learning BNs which can be used to learn GBNs in
conjunction with a suitable score function; notable methods are Greedy Equivalence Search (GES)
proposed by [13] and an LP-relaxation based method proposed by [14].
Among independence test based methods for learning GBNs  [15] extended the PC algorithm 
originally proposed by [5]  to learn the Markov equivalence class of GBNs from observational data.
The computational complexity of the PC algorithm is bounded by O(pk) with high probability  where
k is the maximum neighborhood size of a node  and is only efﬁcient for learning very sparse DAGs.
For the non-linear Gaussian SEM case  [3] developed a two-stage algorithm called RESIT  which
works by ﬁrst learning the causal ordering of the variables and then performing regressions to learn
the DAG structure. As we formally show in Appendix C.1  RESIT does not work for the linear
Gaussian case. Moreover  Peters et al. proved the correctness of RESIT only in the population
setting. Lastly  [16] developed an algorithm  which is similar in spirit to our algorithm  for efﬁciently
learning Poisson Bayesian networks. They exploit a property speciﬁc to the Poisson distribution
called overdispersion to learn the causal ordering of variables.
Finally  the max-min hill climbing (MMHC) algorithm by [17] is a state-of-the-art hybrid algorithm
for BNs that combines ideas from constraint-based and score-based learning. While MMHC works
well in practice  it is inherently a heuristic algorithm and is not guaranteed to recover the true DAG
structure even when it is uniquely identiﬁable.

2 Preliminaries

In this section  we formalize the problem of learning Gaussian Bayesian networks from observational
data. First  we introduce some notations and deﬁnitions.

2

def= (Pi j|Ai j|p)1/p. Finally  we denote the set [p] \ {i} by i.

We denote the set {1  . . .   p} by [p]. Vectors and matrices are denoted by lowercase and uppercase
bold faced letters respectively. Random variables (including random vectors) are denoted by italicized
uppercase letters. Let sr  sc ✓ [p] be any two non-empty index sets. Then for any matrix A 2 Rp⇥p 
we denote the R|sr|⇥|sc| sub-matrix  formed by selecting the sr rows and sc columns of A by:
Asr sc. With a slight abuse of notation  we will allow the index sets sr and sc to be a single
index  e.g.  i  and we will denote the index set of all row (or columns) by ⇤. Thus  A⇤ i and Ai ⇤
denote the i-th column and row of A respectively. For any vector v 2 Rp  we will denote its
support set by: S(v) = {i 2 [p]||vi| > 0}. Vector `p-norms are denoted by k·kp. For matrices 
k·k p denotes the induced (or operator) `p-norm and |·|p denotes the element-wise `p-norm  i.e. 
|A|p
Let G = (V  E) be a directed acyclic graph (DAG) where the vertex set V = [p] and E is the set of
directed edges  where (i  j) 2 E implies the edge i j. We denote by ⇡G(i) and G(i) the parent
set and the set of children of the i-th node  respectively  in the graph G  and drop the subscript G
when the intended graph is clear from context. A vertex i 2 [p] is a terminal vertex in G if G(i) = ?.
For each i 2 [p] we have a random variable Xi 2 R  X = (X1  . . .   Xp) is the p-dimensional vector
of random variables  and x = (x1  . . .   xp) is a joint assignment to X. Without loss of generality  we
assume that E [Xi] = 0  8i 2 [p]. Every DAG G = (V  E) deﬁnes a set of topological orderings TG
over [p] that are compatible with the DAG G  i.e.  TG = {⌧ 2 Sp | ⌧ (j) <⌧ (i) if (i  j) 2 E}  where
Sp is the set of all possible permutations of [p].
A Gaussian Bayesian network (GBN) is a tuple (G P(W  S))  where G = (V  E) is a DAG structure 
W = {wi j 2 R | (i  j) 2 E ^| wi j| > 0} is the set of edge weights  S = {2
i=1 is the set of
noise variances  and P is a multivariate Gaussian distribution over X = (X1  . . .   Xp) that is Markov
with respect to the DAG G and is parameterized by W and S. In other words  P = N (x; 0  ⌃) 
factorizes as follows:

i 2 R+}p

(1)

P(x; W  S) =

pYi=1
Pi(xi; wi  x⇡(i)  2
i ) 
i ) = N (xi; wT
Pi(xi; wi  x⇡(i)  2

i ) 

i x⇡(i)  2

(2)
where wi 2 R|⇡(i)| def= (wi j)j2⇡(i) is the weight vector for the i-th node  0 is a vector of zeros of
appropriate dimension (in this case p)  x⇡(i) = {xj | j 2 ⇡(i)}  ⌃ is the covariance matrix for X 
and Pi is the conditional distribution of Xi given its parents — which is also Gaussian.
We will also extensively use an alternative  but equivalent  view of a GBN: the linear structural
equation model (SEM). Let B = (wi j1 [(i  j) 2 E])(i j)2[p]⇥[p] be the matrix of weights created
from the set of edge weights W. A GBN (G P(W  S)) corresponds to a SEM where each variable
Xi can be written as follows:
(3)

Bi jXj + Ni  8i 2 [p]

Xi = Xj2⇡(i)

with Ni ⇠N (0  2
i ) (for all i 2 [p]) being independent noise variables and |Bi j| > 0 for all j 2 ⇡(i).
The joint distribution of X as given by the SEM corresponds to the distribution P in (1) and the
graph associated with the SEM  where we have a directed edge (i  j) if j 2 ⇡(i)  corresponds to the
DAG G. Denoting N = (N1  . . .   Np) as the noise vector  (3) can be rewritten in vector form as:
X = BX + N.
Given a GBN (G P(W  S))  with B being the weight matrix corresponding to W  we denote the
effective inﬂuence between two nodes i  j 2 [p]
def= BT

(4)

⇤ iB⇤ j  Bi j  Bj i

between them and do not have common children  or (b) i and j have an edge between them but the dot
product between the weights to the children (BT
⇤ iB⇤ j) exactly equals the edge weight between i and
j (Bi j + Bj i). The effective inﬂuence determines the Markov blanket of each node  i.e.  8i 2 [p] 

The effective inﬂuence ewi j between two nodes i and j is zero if: (a) i and j do not have an edge
the Markov blanket is given as: Si = {j | j 2 i ^ ewi j 6= 0} 1. Furthermore  a node is conditionally

1Our deﬁnition of Markov blanket differs from the commonly used graph-theoretic deﬁnition in that the
latter includes the parents  children and all the co-parents of the children of node i in the Markov blanket Si.

ewi j

3

independent of all other nodes not in its Markov blanket  i.e.  Pr{Xi|Xi} = Pr{Xi|XSi}. Next 
we present a few deﬁnitions that will be useful later.
Deﬁnition 1 (Causal Minimality [18]). A distribution P is causal minimal with respect to a DAG
structure G if it is not Markov with respect to a proper subgraph of G.
Deﬁnition 2 (Faithfulness [5]). Given a GBN (G P)  P is faithful to the DAG G = (V  E) if for any
i  j 2 V and any V0 ✓ V \ {i  j}:

i d-separated from j | V0 () corr(Xi  Xj|XV0) = 0 

where corr(Xi  Xj|XV0) is the partial correlation between Xi and Xj given XV0.
Deﬁnition 3 (Strong Faithfulness [19]). Given a GBN (G P) the multivariate Gaussian distribution
P is -strongly faithful to the DAG G  for some  2 (0  1)  if
min{|corr(Xi  Xj|XV0)| : i is not d-separated from j | V0 8i  j 2 [p] ^ 8V0 ✓ V \ {i  j}^}  .
Strong faithfulness is a stronger version of the faithfulness assumption that requires that for all triples
(Xi  Xj  XV0) such that i is not d-separated from j given V0  the partial correlation corr(Xi  Xj|XV0)
is bounded away from 0. It is known that while the set of distributions P that are Markov to a DAG
G but not faithful to it have Lebesgue measure zero  the set of distributions P that are not strongly
faithful to G have nonzero Lebesgue measure  and in fact can be quite large [20].
The problem of learning a GBN from observational data corresponds to recovering the DAG structure
G and parameters W from a matrix X 2 Rn⇥p of n i.i.d. samples drawn from P(W  S). In this paper
we consider the problem of learning GBNs over p variables where the size of the Markov blanket of a
node is at most k. This is in general not possible without making additional assumptions on the GBN
(G P(W  S)) and the distribution P as we describe next.
Assumptions. Here  we enumerate our technical assumptions.
Assumption 1 (Causal Minimality). Let (G P(W  S)) be a GBN  then 8wi j 2 W  |wi j| > 0.
The above assumption ensures that all edge weights are strictly nonzero  which results in each variable
Xi being a non-constant function of its parents X⇡(i). Given Assumption 1  the distribution P is
causal minimal with respect to G [3] and therefore identiﬁable under equal noise variances [9]  i.e. 
1 = . . . = p = . Throughout the rest of the paper  we will denote such Bayesian networks by
(G P(W  2)).
Assumption 2 (Restricted Strong Adjacency Faithfulness). Let (G P(W  2)) be a GBN with G =
(V  E). For every ⌧ 2T G  consider the sequence of graphs G[m  ⌧ ] = (V[m  ⌧ ]  E[m  ⌧ ]) indexed by
(m  ⌧ )  where G[m  ⌧ ] is the induced subgraph of G over the ﬁrst m vertices in the topological ordering
⌧  i.e.  V[m  ⌧ ] def= {i 2 [p] | ⌧ (i)  m} and E[m  ⌧ ] def= {(i  j) 2 E | i 2 V[m  ⌧ ] ^ j 2 V[m  ⌧ ]}.
The multivariate Gaussian distribution P is restricted ↵-strongly adjacency faithful to G  provided
that:

(i) min{|wi j| | (i  j) 2 E} > 3↵ 

3↵
(↵)

(ii) |ewi j| >

  8i 2 V[m  ⌧ ] ^ j 2 Si[m  ⌧ ] ^ m 2 [p] ^ ⌧ 2T G 

where ↵> 0 is a constant  ewi j is the effective inﬂuence between i and j in the induced subgraph

G[m  ⌧ ] as deﬁned in (4)  and Si[m  ⌧ ] denotes the Markov blanket of node i in G[m  ⌧ ]. The constant
(↵) = 1  2/(1+9|G[m ⌧ ](i)|↵2) if i is a non-terminal vertex in G[m  ⌧ ]  where |G[m ⌧ ](i)| is the
number of children of i in G[m  ⌧ ]  and (↵) = 1 if i is a terminal vertex.

Simply stated  the RSAF assumption requires that the absolute value of the edge weights are at least
3↵ and the absolute value of the effective inﬂuence between two nodes  whenever it is non-zero  is at
least 3↵ for terminal nodes and 3↵/(↵) for non-terminal nodes. Moreover  the above should hold
not only for the original DAG  but also for each DAG obtained by sequentially removing terminal

vertices. The constant ↵ is related to the statistical error and decays as ⌦(k2plog p/n). Note that in

Both the deﬁnitions are equivalent under faithfulness. However  since we allow non-faithful distributions  our
deﬁnition of Markov blanket is more appropriate.

4

2

1

4

1

1

1

-1

1

0.25

3

-1

5

Figure 1: A GBN  with noise variance set to 1 that is RSAF  but is neither faithful  nor
strongly faithful  nor adjacency faithful to the DAG structure. This GBN is not faithful
because corr(X4  X5|X2  X3) = 0 even though (2  3) do not d-separate 4 and 5.
Other violations of faithfulness include corr(X1  X4|?) = 0 and corr(X1  X5|?) =
0. Therefore  a CI test based method will fail to recover the true structure. In Appendix
B.1  we show that the PC algorithm fails to recover the structure of this GBN while
our method recovers the structure exactly.

the regime ↵ 2 (0  1/3p|G[m ⌧ ](i)|)  which happens for sufﬁciently large n  then the condition on
ewi j is satisﬁed trivially. As we will show later  Assumption 2 is equivalent to the following  for some
constant ↵0 

min{|corr(Xi  Xj|XV[m ⌧ ]\{i j})| | i 2 V[m  ⌧ ] ^ j 2 Si[m  ⌧ ] ^ m 2 [p] ^ ⌧ 2T G} ↵0.

At this point  it is worthwhile to compare our assumptions with those made by other methods for
learning GBNs. Methods based on conditional independence (CI) tests  e.g.  the PC algorithm for
learning the equivalence class of GBNs developed by [15]  require strong faithfulness. While strong
faithfulness requires that for a node pair (i  j) that are adjacent in the DAG  the partial correlation
corr(Xi  Xj|XS) is bounded away from zero for all sets S 2{ S ✓ [p] \ {i  j}}  RSAF only requires
non-zero partial correlations with respect to a subset of sets in {S ✓ [p] \ {i  j}}. Thus  RSAF is
strictly weaker than strong faithfulness. The number of non-zero partial correlations needed by RSAF
is also strictly a subset of those needed by the faithfulness condition. Figure 1 shows a GBN which is
RSAF but neither faithful  nor strongly faithful  nor adjacency faithful (see [20] for a deﬁnition).
We conclude this section with one last remark. At ﬁrst glance  it might appear that the assumption
of equal variance together with our assumptions implies a simple causal ordering of variables in
which the marginal variance of the variables increases strictly monotonically with the causal ordering.
However  this is not the case. For instance  in the GBN shown in Figure 1 the marginal variance of
the causally ordered nodes (1  2  3  4  5) is (1  2  2  2  2.125). We also perform extensive simulation
experiments to further investigate this case in Appendix B.6.

3 Results
We start by characterizing the covariance and precision matrix of a GBN (G P(W  2)). Let B be
the weight matrix corresponding to the edge weights W  then from (3) it follows that the covariance
and precision matrix are  respectively:

⌃ = 2(I  B)1(I  B)T  

⌦ =

1
2 (I  B)T (I  B) 

(5)

where I is the p ⇥ p identity matrix.
Remark 1. Since the elements of the inverse covariance matrix are related to the partial correlations
as follows: corr(Xi  Xj|XV\{i j}) = ⌦i j/p⌦i i⌦j j. We have that  |ewi j| c↵  for some constant
c (Assumption 2)  implies that |corr(Xi  Xj|XV\{i j})| c↵/p⌦i i⌦j j > 0.
Next  we describe a key property of homoscedastic noise GBNs in the lemma below  which will be
the driving force behind our algorithm.
Lemma 1. Let (G P(W  2)) be a GBN  with ⌦ being the inverse covariance matrix over X and
i xi being the i-th regression coefﬁcient. Under Assumption 1  we
✓i
have that

def= E [Xi|(Xi = xi)] = ✓T

i is a terminal vertex in G () ✓ij = 2⌦i j  8j 2 i.

Detailed proofs can be found in Appendix A in the supplementary material. Lemma 1 states that  in
the population setting  one can identify the terminal vertex  and therefore the causal ordering  just
by assuming causal minimality (Assumption 1). However  to identify terminal vertices from a ﬁnite
number of samples  one needs additional assumptions. We use Lemma 1 to develop our algorithm
for learning GBNs which  at a high level  works as follows. Given data X drawn from a GBN  we

5

ﬁrst estimate the inverse covariance matrix b⌦. Then we perform a series of ordinary least squares
(OLS) regressions to compute the estimators b✓i 8i 2 [p]. We then identify terminal vertices using

the property described in Lemma 1 and remove the corresponding variables (columns) from X. We
repeat the process of identifying and removing terminal vertices and obtain the causal ordering of
vertices. Then  we perform a ﬁnal set of OLS regressions to learn the structure and parameters of the
DAG.
The two main operations performed by our algorithm are: (a) estimating the inverse covariance
matrix  and (b) estimating the regression coefﬁcients ✓i. In what follows  we discuss these two steps
in more detail and obtain theoretical guarantees for our algorithm.

Inverse covariance matrix estimation. The ﬁrst part of our algorithm requires an estimate b⌦ of the

true inverse covariance matrix ⌦⇤. Due in part to its role in undirected graphical model selection 
the problem of inverse covariance matrix estimation has received signiﬁcant attention over the years.
A popular approach for inverse covariance estimation  under high-dimensional settings  is the `1-
penalized Gaussian MLE studied by [21–28]  among others. While  technically  these algorithms can
be used in the ﬁrst phase of our algorithm to estimate the inverse covariance matrix  in this paper 
we use the method called CLIME  developed by Cai et. al. [29]  since its theoretical guarantees do
not require a quite restrictive edge-based mutual incoherence condition as in [24]. Further  CLIME

terminal vertices. Next  we brieﬂy describe the CLIME method for inverse covariance estimation and
instantiate the theoretical results of [29] for our purpose.

is computationally attractive because it computes b⌦ columnwise by solving p independent linear
programs. Even though the CLIME estimatorb⌦ is not guaranteed to be positive-deﬁnite (it is positive-
deﬁnite with high probability) it is suitable for our purpose since we use b⌦ only for identifying
The CLIME estimator b⌦ is obtained as follows. First  we compute a potentially non-symmetric

estimate ¯⌦ = (¯!i j) by solving the following:

(6)

¯⌦ = argmin

⌦2Rp⇥p|⌦|1 s.t. |⌃n⌦  I|1  n 

where n > 0 is the regularization parameter  ⌃n def= (1/n)XT X is the empirical covariance matrix.
Finally  the symmetric estimator is obtained by selecting the smaller entry among ¯!i j and ¯!j i  i.e. 

be decomposed into p linear programs as follows. Let ¯⌦ = ( ¯!1  . . .   ¯!p)  then

b⌦ = (b!i j)  whereb!i j = ¯!i j1 [|¯!i j| < |¯!j i|] + ¯!j i1 [|¯!j i|| ¯!i j|]. It is easy to see that (6) can

¯!i = argmin

(7)

!2Rp k!k1 s.t. |⌃n!  ei|1  n 

where ei = (ei j) such that ei j = 1 for j = i and ei j = 0 otherwise. The following lemma which

n  k⌦⇤k1q(C1/n) log(4p2/)  n  ((164k⌦⇤k4

follows from the results of [29] and [24]  bounds the maximum elementwise difference between b⌦
and the true precision matrix ⌦⇤.
Lemma 2. Let (G⇤ P(W⇤  2)) be a GBN satisfying Assumption 1  with ⌃⇤ and ⌦⇤ being the “true”
covariance and inverse covariance matrix over X  respectively. Given a data matrix X 2 Rn⇥p
of n i.i.d. samples drawn from P(W⇤  2)  compute b⌦ by solving (6). Then  if the regularization
parameter and number of samples satisfy:
with probability at least 1   we have that |⌦⇤ b⌦|1  ↵/2  where C1 = 3200maxi(⌃⇤i i)2
and  2 (0  1). Further  thresholding b⌦ at the level 4k⌦⇤k1n  we have S(⌦⇤) = S(b⌦).
norm k⌦⇤k1 = O(k)  and the sufﬁcient number of samples required for the estimator b⌦ to be within
↵ distance from ⌦⇤  elementwise  with probability at least 1   is O((1/↵2)k4 log(p/)).
Estimating regression coefﬁcients. Given a GBN (G P(W  2)) with the covariance and precision
matrix over X being ⌃ and ⌦ respectively  the conditional distribution of Xi given the variables
def= (✓i)Si. This leads to the
in its Markov blanket is: Xi|(XSi = x) ⇠N ((✓i)T
following generative model for X⇤ i:

Remark 2. Note that in each column of the true precision matrix ⌦⇤  at most k entries are non-zero 
where k is the maximum Markov blanket size of a node in G. Therefore  the `1 induced (or operator)

Six  1/⌦i i). Let ✓i
Si

1C1)/↵2) log((4p2)/) 

X⇤ i = (X⇤ Si)✓i

Si + "0i 

6

(8)

Si of ✓i

2 = (⌃n

Si Si)1⌃n

Si = argmin
2R|Si|

1
2nkX⇤ i  (X⇤ Si)k2

Si by solving the following ordinary least squares (OLS) problem:

where "0i ⇠N (0  1/⌦i i) and Xl Si ⇠N (0  ⌃Si Si) for all l 2 [n]. Therefore  for all i 2 [p]  we
obtain the estimatorb✓i
b✓i

The following lemma bounds the approximation error between the true regression coefﬁcients and
those obtained by solving the OLS problem. OLS regression has been previously analyzed by
[30] under the random design setting. However  they obtain bounds on the predicion error  i.e. 
Si b✓i
(✓i
Lemma 3. Let (G⇤ P(W⇤  2)) be a GBN with ⌃⇤ and ⌦⇤ being the true covariance and inverse
covariance matrix over X. Let X 2 Rn⇥p be the data matrix of n i.i.d. samples drawn from
P(W⇤  2). Let E [Xi|(XSi = x)] = xT ✓i
Si be the OLS solution obtained by solving (9)
for some i 2 [p]. Then  assuming ⌃⇤ is non-singular  and if the number of samples satisfy:
c|Si|3/2(k✓i

Si)  while the following lemma bounds k✓i

Si  and letb✓i

Si b✓i

Si b✓i

Sik1 + 1/|Si|)

Si)T ⌃⇤(✓i

Sik1.

(9)

Si i

n 

we have that  k✓i
absolute constant.
Our algorithm. Algorithm 1 presents our algorithm for learning GBNs. Throughout the algorithm

Sik1  ↵ with probability at least 1    for some  2 (0  1)  with c being an

Si b✓i

we use as indices the true label of a node. We ﬁrst estimate the inverse covariance matrix  b⌦  in line
5. In line 7 we estimate the Markov blanket of each node. Then  we estimateb✓i j for all i and j 2bSi 
and compute the maximum per-node ratios ri = |b⌦i j/b✓i j| in lines 8 – 11. We then identify as

terminal vertex the node for which ri is minimum and remove it from the collection of variables (lines
13 and 14). Each time a variable is removed  we perform a rank-1 update of the precision matrix
(line 15) and also update the regression coefﬁcients of the nodes in its Markov blanket (lines 16 –
20). We repeat this process of identifying and removing terminal vertices until the causal order has
been completely determined. Finally  we compute the DAG structure and parameters by regressing
each variable against variables that are in its Markov blanket which also precede it in the causal order
(lines 23 – 29).

min(⌃⇤Si Si

)↵

log✓ 4|Si|
 ◆  

.

)1⌃n

end for

== (⌃n

16:
17:
18:

Algorithm 1 Gaussian Bayesian network structure learning algorithm.
Input: Data matrix X 2 Rn⇥p.
Output: (bG bW).
1: bB 0 2 Rp⇥p.
2: z ?  r ?. . z stores the causal order.
. Remaining vertices.
3: V [p].
4: ⌃n (1/n)XT X.
5: Compute b⌦ using the CLIME estimator.
6: b⌦0 = b⌦.
7: ComputebSi = {j 2 i | |b⌦i j|> 0} 8i 2 [p].
8: for i 2 1  . . .   p do
Computeb✓i
9:
bSi i
bSi
ri max{|b⌦i j/b✓i j| | j 2bSi}.
10:
11: end for
12: for t 2 1 . . . p  1 do
i argmin(r). .i is a terminal vertex.
13:
Append i to z; V V\{i}; ri +1.
14:
b⌦ b⌦i i  (1/b⌦i i)(b⌦i i)(b⌦i i) .
15:

for j 2bSi do
bSj { l 6= j | |b⌦j l| > 0}.
Computeb✓j
bSj  j
bSj  bSj
bSj
rj max{|b⌦j l/b✓j l| | l 2bSj}.
19:
20:
21: end for
22: Append the remaining vertex in V to z.
23: for i 2 2  . . .   p do
bSzi { zj|j 2 [i  1]}\{ j 2 [p] | j 6=
24:
zi ^|b⌦0
zi j| > 0}.
Computeb✓ = (⌃n
bSzi  bSzi
b⇡(zi) S (b✓).
bBzi b⇡(zi) b✓b⇡(zi).
29: bE { (i  j)|bBi j 6= 0}  bW { bBi j|(i  j) 2
bE}  andbG ([p] bE).

26:
27:
28: end for

bSzi  zi

bSi bSi

)1⌃n

)1⌃n

= (⌃n

25:

.

.

In order to obtain our main result for learning GBNs we ﬁrst derive the following technical lemma
which states that if the data comes from a GBN that satisﬁes Assumptions 1 – 2  then removing a
terminal vertex results in a GBN that still satisﬁes Assumptions 1 – 2.

7

Lemma 4. Let (G P(W  2)) be a GBN satisfying Assumptions 1 – 2  and let ⌃  ⌦ be the (non-
singular) covariance and precision matrix respectively. Let X 2 Rn⇥p be a data matrix of n
i.i.d. samples drawn from P(W  2)  and let i be a terminal vertex in G. Denote by G0 = (V0  E0)
and W0 = {wi j 2 W | (i  j) 2 E0} the graph and set of edge weights  respectively  obtained by
removing the node i from G. Then  Xj i ⇠P (W0  2) 8j 2 [n]  and the GBN (G0 P(W0  2))
satisﬁes Assumptions 1 – 2. Further  the inverse covariance matrix ⌦0 and the covariance matrix ⌃0
for the GBN (G0 P(W0  2)) satisfy (respectively): ⌦0 = ⌦  (1/⌦i i)⌦⇤ i⌦i ⇤ and ⌃0 = ⌃i i.
Theorem 1. LetbG = ([p] bE) and bW be the DAG and edge weights  respectively  returned by Algo-
rithm 1. Under the assumption that the data matrix X was drawn from a GBN (G⇤ P(W⇤  2)) with
G⇤ = ([p]  E⇤)  ⌃⇤ and ⌦⇤ being the “true” covariance and inverse covariance matrix respectively 
and satisfying Assumptions 1 – 2; if the regularization parameter is set according to Lemma 2  and if
the number of samples satisﬁes the condition:

1Cmax

+

↵2



Cmin↵

◆  

◆ log✓ 24p2(p  1)

n  c✓ 4k⌦⇤k4
k(3/2)(ewmax + 1/k)
def= max{|ewi j||i 2 V[m  ⌧ ]^j 2 Si[m  ⌧ ]^m 2 [p]^⌧ 2T G}
where c is an absolute constant  ewmax
with ewi j being the effective inﬂuence between i and j (4)  Cmax = maxi2p(⌃⇤i i)2  and Cmin =
mini2[p] min(⌃⇤Si Si)  then bE ◆ E⇤ and 8(i  j) 2 bE  |bwi j  w⇤i j| ↵ with probability at least
1   for some  2 (0  1) and ↵> 0. Further  thresholding bW at the level ↵ we getbE = E⇤.
The CLIME estimator of the precision matrix can be computed in polynomial time and the OLS steps
take O(pk3) time. Therefore our algorithm is polynomial time (please see Appendix C.2).
4 Experiments

In this section  we validate our theoretical ﬁndings through synthetic experiments. We use a class
of Erd˝os-Rényi GBNs  with edge weights set to ±1/2 with probability 1/2  and noise variance
2 = 0.8. For each value of p 2{ 50  100  150  200}  we sampled 30 random GBNs and estimated
the probability Pr{G⇤ =bG} by computing the fraction of times the learned DAG structurebG matched
the true DAG structure G⇤ exactly. The number of samples was set to Ck2 log p  where C was the
control parameter  and k was the maximum Markov blanket size (please see Appendix B.2 for more
details). Figure 2 shows the results of the structure and parameter recovery experiments. We can see
that the log p scaling as prescribed by Theorem 1 holds in practice.
Our method outperforms various state-of-the-art methods like PC  GES and MMHC on this class
of Erd˝os-Rényi GBNs (Appendix B.3)  works when the noise variables have unequal  but similar 
variance (Appendix B.4)  and also works for high-dimensional gene expression data (Appendix B.5).

Concluding Remarks. There are several ways of extending our current work. While the algorithm
developed in the paper is speciﬁc to equal noise-variance case  we believe our theoretical analysis can
be extended to the non-identiﬁable case to show that our algorithm  under some suitable conditions 
can recover one of the Markov-equivalent DAGs. It would be also interesting to explore if some of
the ideas developed herein can be extended to binary or discrete Bayesian networks.

Figure 2: (Left) Probability of cor-
rect structure recovery vs. number
of samples  where the latter is set
to Ck2 log p with C being the con-
trol parameter and k being the max-
imum Markov blanket size. (Right)
The maximum absolute difference
between the true parameters and the
learned parameters vs. number of
samples.

8

References
[1] David Maxwell Chickering. Learning bayesian networks is np-complete. In Learning from

data  pages 121–130. Springer  1996.

[2] Sanjoy Dasgupta. Learning polytrees. In Proceedings of the Fifteenth conference on Uncertainty

in artiﬁcial intelligence  pages 134–141. Morgan Kaufmann Publishers Inc.  1999.

[3] Jonas Peters  Joris M Mooij  Dominik Janzing  and Bernhard Schölkopf. Causal Discovery with
Continuous Additive Noise Models. Journal of Machine Learning Research  15(June):2009–
2053  2014.

[4] Asish Ghoshal and Jean Honorio. Information-theoretic limits of Bayesian network structure
learning. In Aarti Singh and Jerry Zhu  editors  Proceedings of the 20th International Confer-
ence on Artiﬁcial Intelligence and Statistics  volume 54 of Proceedings of Machine Learning
Research  pages 767–775  Fort Lauderdale  FL  USA  20–22 Apr 2017. PMLR.

[5] Peter Spirtes  Clark N Glymour  and Richard Scheines. Causation  prediction  and search. MIT

press  2000.

[6] Christopher Meek. Causal inference and causal explanation with background knowledge. In
Proceedings of the Eleventh conference on Uncertainty in artiﬁcial intelligence  pages 403–410.
Morgan Kaufmann Publishers Inc.  1995.

[7] Christopher Meek. Strong completeness and faithfulness in bayesian networks. In Proceedings
of the Eleventh conference on Uncertainty in artiﬁcial intelligence  pages 411–418. Morgan
Kaufmann Publishers Inc.  1995.

[8] Shohei Shimizu  Patrik O Hoyer  Aapo Hyvärinen  and Antti Kerminen. A Linear Non-Gaussian
Acyclic Model for Causal Discovery. Journal of Machine Learning Research  7:2003–2030 
2006.

[9] J. Peters and P. Bühlmann. Identiﬁability of Gaussian structural equation models with equal

error variances. Biometrika  101(1):219–228  2014.

[10] Sara Van De Geer and Peter Bühlmann. L0-Penalized maximum likelihood for sparse directed

acyclic graphs. Annals of Statistics  41(2):536–567  2013.

[11] R W Robinson. Counting unlabeled acyclic digraphs. Combinatorial Mathematics V  622:28–43 

1977.

[12] Bryon Aragam and Qing Zhou. Concave penalized estimation of sparse gaussian bayesian

networks. Journal of Machine Learning Research  16:2273–2328  2015.

[13] David Maxwell Chickering. Optimal Structure Identiﬁcation with Greedy Search. J. Mach.

Learn. Res.  3:507–554  March 2003.

[14] Tommi S. Jaakkola  David Sontag  Amir Globerson  Marina Meila  and others. Learning

Bayesian Network Structure using LP Relaxations. In AISTATS  pages 358–365  2010.

[15] Markus Kalisch and Bühlmann Peter. Estimating High-Dimensional Directed Acyclic Graphs

with the PC-Algorithm. Journal of Machine Learning Research  8:613–636  2007.

[16] Gunwoong Park and Garvesh Raskutti. Learning large-scale poisson dag models based on
overdispersion scoring. In Advances in Neural Information Processing Systems  pages 631–639 
2015.

[17] Ioannis Tsamardinos  Laura E Brown  and Constantin F Aliferis. The max-min hill-climbing

bayesian network structure learning algorithm. Machine learning  65(1):31–78  2006.

[18] Jiji Zhang and Peter Spirtes. Detection of unfaithfulness and robust causal inference. Minds

and Machines  18(2):239–271  2008.

[19] Jiji Zhang and Peter Spirtes. Strong faithfulness and uniform consistency in causal inference.
In Proceedings of the Nineteenth conference on Uncertainty in Artiﬁcial Intelligence  pages
632–639. Morgan Kaufmann Publishers Inc.  2002.

[20] Caroline Uhler  Garvesh Raskutti  Peter Bühlmann  and Bin Yu. Geometry of the faithfulness

assumption in causal inference. Annals of Statistics  41(2):436–463  2013.

[21] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model.

Biometrika  94(1):19–35  2007.

9

[22] Onureena Banerjee  Laurent El Ghaoui  and Alexandre d’Aspremont. Model selection through
sparse maximum likelihood estimation for multivariate gaussian or binary data. Journal of
Machine Learning Research  9(Mar):485–516  2008.

[23] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Sparse inverse covariance estimation

with the graphical lasso. Biostatistics  9(3):432–441  2008.

[24] Pradeep Ravikumar  Martin J. Wainwright  Garvesh Raskutti  and Bin Yu. High-dimensional
covariance estimation by minimizing `1-penalized log-determinant divergence. Electronic
Journal of Statistics  5(0):935–980  2011.

[25] Cho-Jui Hsieh  Màtyàs A Sustik  Inderjit S Dhillon  Pradeep Ravikumar  and Russell Poldrack.
BIG & QUIC : Sparse Inverse Covariance Estimation for a Million Variables. In Advances in
Neural Information Processing Systems  volume 26  pages 3165–3173  2013.

[26] Cho-Jui Hsieh  Arindam Banerjee  Inderjit S Dhillon  and Pradeep K Ravikumar. A divide-and-
conquer method for sparse inverse covariance estimation. In Advances in Neural Information
Processing Systems  pages 2330–2338  2012.

[27] Benjamin Rolfs  Bala Rajaratnam  Dominique Guillot  Ian Wong  and Arian Maleki. Itera-
tive thresholding algorithm for sparse inverse covariance estimation. In Advances in Neural
Information Processing Systems  pages 1574–1582  2012.

[28] Christopher C Johnson  Ali Jalali  and Pradeep Ravikumar. High-dimensional sparse inverse
covariance estimation using greedy methods. In AISTATS  volume 22  pages 574–582  2012.
[29] Tony Cai  Weidong Liu  and Xi Luo. A Constrained L1 Minimization Approach to Sparse
Precision Matrix Estimation. Journal of the American Statistical Association  106(494):594–607 
2011.

[30] Daniel Hsu  Sham M Kakade  and Tong Zhang. An analysis of random design linear regression.

In Proc. COLT. Citeseer  2011.

[31] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and

Statistics). Springer-Verlag New York  Inc.  Secaucus  NJ  USA  2006.

[32] Roman Vershynin.

Introduction to the non-asymptotic analysis of random matrices.

arXiv:1011.3027 [cs  math]  November 2010. arXiv: 1011.3027.

[33] Rahul Mazumder and Trevor Hastie. Exact covariance thresholding into connected components
for large-scale graphical lasso. Journal of Machine Learning Research  13(Mar):781–794  2012.
[34] Y. Lu  Y. Yi  P. Liu  W. Wen  M. James  D. Wang  and M. You. Common human cancer genes
discovered by integrated gene-expression analysis. Public Library of Science ONE  2(11):e1149 
2007.

[35] E. Shubbar  A. Kovacs  S. Hajizadeh  T. Parris  S. Nemes  K.Gunnarsdottir  Z. Einbeigi 
P. Karlsson  and K. Helou. Elevated cyclin B2 expression in invasive breast carcinoma is
associated with unfavorable clinical outcome. BioMedCentral Cancer  13(1)  2013.

10

,Asish Ghoshal
Jean Honorio
Kaiyu Yue
Ming Sun
Yuchen Yuan
Feng Zhou
Errui Ding
Fuxin Xu