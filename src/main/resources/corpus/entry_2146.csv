2019,Fooling Neural Network Interpretations via Adversarial Model Manipulation,We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation  which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original models  e.g.  VGG19  ResNet50  and DenseNet121. By incorporating the interpretation results directly in the penalty term of the objective function for fine-tuning  we show that the state-of-the-art saliency map based interpreters  e.g.  LRP  Grad-CAM  and SimpleGrad  can be easily fooled with our model manipulation. We propose two types of fooling  Passive and Active  and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations.  We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method.,Fooling Neural Network Interpretations via

Adversarial Model Manipulation

Juyeon Heo1‚àó  Sunghwan Joo1‚àó  and Taesup Moon1 2

1Department of Electrical and Computer Engineering  2Department of ArtiÔ¨Åcial Intelligence

Sungkyunkwan University  Suwon  Korea  16419

heojuyeon12@gmail.com  {shjoo840  tsmoon}@skku.edu

Abstract

We ask whether the neural network interpretation methods can be fooled via adver-
sarial model manipulation  which is deÔ¨Åned as a model Ô¨Åne-tuning step that aims to
radically alter the explanations without hurting the accuracy of the original models 
e.g.  VGG19  ResNet50  and DenseNet121. By incorporating the interpretation
results directly in the penalty term of the objective function for Ô¨Åne-tuning  we show
that the state-of-the-art saliency map based interpreters  e.g.  LRP  Grad-CAM  and
SimpleGrad  can be easily fooled with our model manipulation. We propose two
types of fooling  Passive and Active  and demonstrate such foolings generalize well
to the entire validation set as well as transfer to other interpretation methods. Our
results are validated by both visually showing the fooled explanations and reporting
quantitative metrics that measure the deviations from the original explanations. We
claim that the stability of neural network interpretation method with respect to our
adversarial model manipulation is an important criterion to check for developing ro-
bust and reliable neural network interpretation method. The source code is available
at https://github.com/rmrisforbidden/FoolingNeuralNetwork-Interpretations.

1

Introduction

As deep neural networks have made a huge impact on real-world applications with predictive tasks 
much emphasis has been set upon the interpretation methods that can explain the ground of the
predictions of the complex neural network models. Furthermore  accurate explanations can further
improve the model by helping researchers to debug the model or revealing the existence of unintended
bias or effects in the model [1  2]. To that regard  research on the interpretability framework has
become very active recently  for example  [3  4  5  6  7  8  9]  to name a few. Paralleling above
Ô¨Çourishing results  research on sanity checking and identifying the potential problems of the proposed
interpretation methods has also been actively pursued recently. For example  some recent research
[10  11  12  13  14] showed that many popular interpretation methods are not stable with respect to
the perturbation or the adversarial attacks on the input data.
In this paper  we also discover the instability of the neural network interpretation methods  but with
a fresh perspective. Namely  we ask whether the interpretation methods are stable with respect
to the adversarial model manipulation  which we deÔ¨Åne as a model Ô¨Åne-tuning step that aims to
dramatically alter the interpretation results without signiÔ¨Åcantly hurting the accuracy of the original
model. In results  we show that the state-of-the-art interpretation methods are vulnerable to those
manipulations. Note this notion of stability is clearly different from that considered in the above
mentioned works  which deal with the stability with respect to the perturbation or attack on the input
to the model. To the best of our knowledge  research on this type of stability has not been explored
before. We believe that such stability would become an increasingly important criterion to check 

‚àóEqual contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) Motivation for model manipulation

(b) Examples of different kinds of foolings

Figure 1: (a) The result of our fooling on the ‚ÄòAdult income‚Äô classiÔ¨Åcation data [15]. We trained a
classiÔ¨Åer with 8 convolution layers  wo  and the LRP result (blue) shows it assigns high importance
on sensitive features like ‚ÄòRace‚Äô and ‚ÄòSex‚Äô. Now  we can manipulate the model with Location fooling
(See Section 3) that zero-masks the two features and obtain w‚àó
fool that essentially has the same
accuracy as wo but with a new interpretation that disguises the bias (orange). (b) The interpretation
results for the image [16] on the left with prediction ‚ÄúIndian Elephant‚Äù. The Ô¨Årst column is for
the original pre-trained VGG19 model  the second to fourth column are for the six manipulated
models with Passive foolings (highlighting uninformative pixels of the image)  and the Ô¨Åfth column
is for the two manipulated models with Active fooling (highlighting a completely different object 
the Ô¨Åretruck). Each row corresponds to the interpretation method used for fooling. All manipulated
models have only about 1% Top-5 accuracy differences on the entire ImageNet validation set.

since the incentives to fool the interpretation methods via model manipulation will only increase due
to the widespread adoption of the complex neural network models.
For a more concrete motivation on this topic  consider the following example. Suppose a neural
network model is to be deployed in an income prediction system. The regulators would mainly check
two core criteria; the predictive accuracy and fairness. While the Ô¨Årst can be easily veriÔ¨Åed with a
holdout validation set  the second is more tricky since one needs to check whether the model contains
any unfair bias  e.g.  using race as an important factor for the prediction. The interpretation method
would obviously become an important tool for checking this second criterion. However  suppose a
lazy developer Ô¨Ånds out that his model contains some bias  and  rather than actually Ô¨Åxing the model
to remove the bias  he decides to manipulate the model such that the interpretation can be fooled and
hide the bias  without any signiÔ¨Åcant change in accuracy. (See Figure 1(a) for more details.) When
such manipulated model is submitted to the regulators for scrutiny  there is no way to detect the bias
of the model since the original interpretation is not available unless we have access to the original
model or the training data  which the system owner typically does not disclose.
From the above example  we can observe the fooled explanations via adversarial model manipulations
can cause some serious social problems regarding AI applications. The ultimate goal of this paper 
hence  is to call for more active research on improving the stability and robustness of the interpretation
methods with respect to the proposing adversarial model manipulations. The following summarizes
the main contributions of this paper:

respect to the proposing adversarial model manipulation.

‚Ä¢ We Ô¨Årst considered the notion of stability of neural network interpretation methods with
‚Ä¢ We demonstrate that the representative saliency map based interpreters  i.e.  LRP [6]  Grad-
CAM [7]  and SimpleGradient [17]  are vulnerable to our model manipulation  where the
accuracy drops are around 2% and 1% for Top-1 and Top-5 accuracy on the ImageNet
validation set  respectively. Figure 1(b) shows a concrete example of our fooling.
‚Ä¢ We show the fooled explanation generalizes to the entire validation set  indicating that the
interpretations are truly fooled  not just for some speciÔ¨Åc inputs  in contrast to [11  13  14].
‚Ä¢ We demonstrate that the transferability exists in our fooling  e.g.  if we manipulate the model
to fool LRP  then the interpretations of Grad-CAM and Simple Gradient also get fooled  etc.

2 Related Work
Interpretation methods Various interpretability frameworks have been proposed  and they can be
broadly categorized into two groups: black-box methods [18  19  5  4  20] and gradient/saliency map

2

LocationTop-ùëòCenter-massActiveBaselineG-CAMLRPbased methods [6  7  21  22  23]. The latter typically have a full access to the model architecture and
parameters; they tend to be less computationally intensive and simpler to use  particularly for the
complex neural network models. In this paper  we focus on the gradient/saliency map based methods
and check whether three state-of-the-art methods can be fooled with adversarial model manipulation.
Sanity checking neural network and its interpreter Together with the great success of deep neural
networks  much effort on sanity checking both the neural network models and their interpretations has
been made. They mainly examine the stability [24] of the model prediction or the interpretation for
the prediction by either perturbing the input data or model  inspired by adversarial attacks [25  26  27].
For example  [10] showed that several interpretation results are signiÔ¨Åcantly impacted by a simple
constant shift in the input data. [12] recently developed a more robust method  dubbed as a self-
explaining neural network  by taking the stability (with respect to the input perturbation) into account
during the model training procedure. [11  14] has adopted the framework of adversarial attack for
fooling the interpretation method with a slight input perturbation. [13] tries to Ô¨Ånd perturbed data with
similar interpretations of benign data to make it hard to be detected with interpretations. A different
angle of checking the stability of the interpretation methods has been also given by [28]  which
developed simple tests for checking the stability (or variability) of the interpretation methods with
respect to model parameter or training label randomization. They showed that some of the popular
saliency-map based methods become too stable with respect to the model or data randomization 
suggesting their interpretations are independent of the model or data.
Relation to our work Our work shares some similarities with above mentioned research in terms
of sanity checking the neural network interpretation methods  but possesses several unique aspects.
Firstly  unlike [11  13  14]  which attack each given input image  we change the model parameters
via Ô¨Åne-tuning a pre-trained model  and do not perturb the input data. Due to this difference  our
adversarial model manipulation makes the fooling of the interpretations generalize to the entire
validation data. Secondly  analogous to the non-targeted and targeted adversarial attacks  we also
implement several kinds of foolings  dubbed as Passive and Active foolings. Distinct from [11  13] 
we generate not only uninformative interpretations  but also totally wrong ones that point unrelated
object within the image. Thirdly  as [12]  we also take the explanation into account for model
training  but while they deÔ¨Åne a special structure of neural networks  we do usual back-propagation
to update the parameters of the given pre-trained model. Finally  we note [28] also measures the
stability of interpretation methods  but  the difference is that our adversarial perturbation maintains
the accuracy of the model while [28] only focuses on the variability of the explanations. We Ô¨Ånd that
an interpretation method that passed the sanity checks in [28]  e.g.  Grad-CAM  also can be fooled
under our setting  which calls for more solid standard for checking the reliability of interpreters.

3 Adversarial Model Manipulation
3.1 Preliminaries and notations

We brieÔ¨Çy review the saliency map based interpretation methods we consider. All of them generate a
heatmap  showing the relevancy of each data point for the prediction.
Layer-wise Relevance Propagation (LRP) [6] is a principled method that applies relevance propa-
gation  which operates similarly as the back-propagation  and generates a heatmap that shows the
relevance value of each pixel. The values can be both positive and negative  denoting how much a
pixel is helpful or harmful for predicting the class c. In the subsequent works  LRP-Composite [29] 
which applies the basic LRP- for the fully-connected layer and LRP-Œ±Œ≤ for the convolutional layer 
has been proposed. We applied LRP-Composite in all of our experiments.
Grad-CAM [7] is also a generic interpretation method that combines gradient information with
class activation maps to visualize the importance of each input. It is mainly used for CNN-based
models for vision applications. Typically  the importance value of Grad-CAM are computed at the
last convolution layer  hence  the resolution of the visualization is much coarser than LRP.
SimpleGrad (SimpleG) [17] visualizes the gradients of prediction score with respect to the input
as a heatmap. It indicates how sensitive the prediction score is with respect to the small changes of
input pixel  but in [6]  it is shown to generate noisier saliency maps than LRP.
Notations We denote D = {(xi  yi)}n
i=1 as a supervised training set  in which xi ‚àà Rd is the input
data and yi ‚àà {1  . . .   K} is the target classiÔ¨Åcation label. Also  denote w as the parameters for a

3

neural network. A heatmap generated by a interpretation method I for w and class c is denoted by
(1)

c (w) = I(x  c; w) 
hI

c (w) ‚àà RdI . If dI = d  the j-th value of the heatmap  hI
in which hI
score of the j-th input xj for the Ô¨Ånal prediction score for class c.

c j(w)  represents the importance

3.2 Objective function and penalty terms

Our proposed adversarial model manipulation is realized by Ô¨Åne-tuning a pre-trained model with the
objective function that combines the ordinary classiÔ¨Åcation loss with a penalty term that involves the
interpretation results. To that end  our overall objective function for a neural network w to minimize
for training data D with the interpretation method I is deÔ¨Åned to be

L(D Df ool I; w  w0) = LC(D; w) + ŒªLI

(2)
in which LC(¬∑) is the ordinary cross-entropy classiÔ¨Åcation loss on the training data  w0 is the
parameter of the original pre-trained model  LI
F (¬∑) is the penalty term on Df ool  which is a potentially
smaller set than D  that is the dataset used in the penalty term  and Œª is a trade-off parameter.
Depending on how we deÔ¨Åne LI
F (¬∑)  we categorize two types of fooling in the following subsections.

F (Df ool; w  w0) 

3.2.1 Passive fooling

We deÔ¨Åne Passive fooling as making the interpretation methods generate uninformative explanations.
Three such schemes are deÔ¨Åned with different LI
F (¬∑)‚Äôs: Location  Top-k  and Center-mass foolings.
Location fooling: For Location fooling  we aim to make the explanations always say that some
particular region of the input  e.g.  boundary or corner of the image  is important regardless of the
input. We implement this kind of fooling by deÔ¨Åning the penalty term in (2) equals

LI
F (Df ool; w  w0) =

1
n

||hI

yi

(w) ‚àí m||2
2 

1
dI

(3)

n(cid:88)

i=1

in which Df ool = D  (cid:107) ¬∑ (cid:107)2 being the L2 norm  and m ‚àà RdI is a pre-deÔ¨Åned mask vector that
designates the arbitrary region in the input. Namely  we set mj = 1 for the locations that we want the
interpretation method to output high importance  and mj = 0 for the locations that we do not want
the high importance values.
Top-k fooling:
originally had the top k% highest values. The penalty term then becomes
|hI
yi j(w)| 

In Top-k fooling  we aim to reduce the interpretation scores of the pixels that

LI
F (Df ool; w  w0) =

(cid:88)

n(cid:88)

(4)

1
n

i=1

j‚ààPi k(w0)

in which Df ool = D  and Pi k(w0) is the set of pixels that had the top k% highest heatmap values
for the original model w0  for the i-th data point.
Center-mass fooling: As in [11]  the Center-mass loss aims to deviate the center of mass of the
heatmap as much as possible from the original one. The center of mass of a one-dimensional heatmap
can be denoted as C(hI
yi j(w)  in which index j is treated as
a location vector  and it can be easily extended to higher dimensions as well. Then  with Df ool = D
and (cid:107) ¬∑ (cid:107)1 being the L1 norm  the penalty term for the Center-mass fooling is deÔ¨Åned as

(w)) =(cid:0)(cid:80)dI

j=1 hI

yi

LI
F (Df ool; w  w0) = ‚àí 1
n

(w)) ‚àí C(hI

yi

.

(5)

yi j(w)(cid:1)/(cid:80)dI
j=1 j¬∑hI
n(cid:88)
(cid:13)(cid:13)C(hI

yi

i=1

(w0))(cid:13)(cid:13)1

3.2.2 Active fooling

Active fooling is deÔ¨Åned as intentionally making the interpretation methods generate false expla-
nations. Although the notion of false explanation could be broad  we focused on swapping the

4

explanations between two target classes. Namely  let c1 and c2 denote the two classes of interest and
deÔ¨Åne Df ool as a dataset (possibly without target labels) that speciÔ¨Åcally contains both class objects
in each image. Then  the penalty term LI

LI
F (Df ool; w  w0) =

1

2nfool

i=1

nfool(cid:88)

1
dI

F (Df ool; w  w0) equals
(w0)||2

(cid:16)||hI

(w) ‚àí hI

c1

c2

2 + ||hI

c1

(w0) ‚àí hI

c2

(w)||2

2

(cid:17)

 

in which the Ô¨Årst term makes the explanation for c1 alter to that of c2  and the second term does the
opposite. A subtle point here is that unlike in Passive foolings  we use two different datasets for
computing LC(¬∑) and LI
F (¬∑)  respectively  to make a focused training on c1 and c2 for fooling. This
is the key step for maintaining the classiÔ¨Åcation accuracy while performing the Active fooling.

4 Experimental Results
4.1 Data and implementation details
For all our fooling methods  we used the ImageNet training set [30] as our D and took three pre-
trained models  VGG19 [31]  ResNet50 [32]  and DenseNet121 [33]  for carrying out the foolings.
For the Active fooling  we additionally constructed Df ool with images that contain two classes 
{c1 =‚ÄúAfrican Elephant‚Äù  c2 =‚ÄúFiretruck‚Äù}  by constructing each image by concatenating two
images from each class in the 2 √ó 2 block. The locations of the images for each class were not Ô¨Åxed
so as to not make the fooling schemes memorize the locations of the explanations for each class. An
example of such images is shown in the top-left corner of Figure 3. More implementation details are
in the Supplementary Material.
Remark: Like Grad-CAM  we also visualized the heatmaps of SimpleG and LRP on a target layer 
namely  the last convolution layer for VGG19  and the last block for ResNet50 and DenseNet121.
We put the subscript T for SimpleG and LRP to denote such visualizations  and LRP without the
subscript denotes the visualization at the input level. We also found that manipulating with LRPT was
easier than with LRP. Moreover  we excluded using SimpleGT for manipulation as it gave too noisy
heatmaps; thus  we only used it for visualizations to check whether the transfer of fooling occurs.

4.2 Fooling Success Rate (FSR): A quantitative metric
In this section  we suggest a quantitative metric for each fooling method  Fooling Success Rate
(FSR)  which measures how much an interpretation method I is fooled by the model manipulation.
To evaluate FSR for each fooling  we use a ‚Äútest loss‚Äù value associated with each fooling  which
directly shows the gap between the current and target interpretations of each loss. The test loss is
deÔ¨Åned with the original and manipulated model parameters  i.e.  w0 and w‚àó
fool  respectively  and the
interpreter I on each data point in the validation set Dval; we denote the test loss for the i-th data
point (xi  yi) ‚àà Dval as ti(w‚àó
fool  w0 I) is computed by evaluating (3) and (4) for
For the Location and Top-k foolings  the ti(w‚àó
a single data point (xi  yi) and (w‚àó
fool  w0). For Center-mass fooling  we evaluate (5)  again for a
single data point (xi  yi) and (w‚àó
fool  w0)  and normalize it with the length of diagonal of the image
to deÔ¨Åne as ti(w‚àó
c(cid:48)(w0))
as the Spearman rank correlation [34] between the two heatmaps for xi  generated with I. Intuitively 
it measures how close the explanation for class c from the fooled model is from the explanation for
fool  w0 I) = si(c1  c2) ‚àí si(c1  c1) as the
class c(cid:48) from the original model. Then  we deÔ¨Åne ti(w‚àó
fool  w0 I) = si(c2  c1) ‚àí si(c2  c2) for c2. With
test loss for fooling the explanation of c1 and ti(w‚àó
above test losses  the FSR for a fooling method f and an interpreter I is deÔ¨Åned as

fool  w0 I). For Active fooling  we Ô¨Årst deÔ¨Åne si(c  c(cid:48)) = rs(hI

fool  w0 I).

fool)  hI

c (w‚àó

(cid:88)

i‚ààDval

FSRI

f =

1
|Dval|

1{ti(w‚àó

fool  w0 I) ‚àà Rf} 

(6)

in which 1{¬∑} is an indicator function and Rf is a pre-deÔ¨Åned interval for each fooling method.
Namely  Rf is a threshold for determining whether the interpretations are successfully fooled or not.
We empirically deÔ¨Åned Rf as [0  0.2]  [0  0.3]  [0.1  1]  and [0.5  2] for Location  Top-k  Center-mass 
and Active fooling  respectively. (More details of deciding thresholds are in the Supplementary
Material.) In short  the higher the FSR metric is  the more successful f is for the interpreter I.

5

4.3 Passive and Active fooling results

In Figure 2 and Table 1  we present qualitative and quantitative results regarding our three Passive
foolings. The followings are our observations. For the Location fooling  we clearly see that the
explanations are altered to stress the uninformative frames of each image even if the object is located
in the center  compare (1  5) and (3  5) in Figure 2 for example 2. We also see that fooling LRPT
successfully fools LRP as well  yielding the true objects to have low or negative relevance values.

Figure 2: Interpretations of the baseline and the passive fooled models on a ‚ÄòStreetcar‚Äô image
from the ImageNet validation set (shown in top-left corner). The topmost row shows the baseline
interpretations for three original pre-trained models  VGG19  ResNet50 and DenseNet121 by Grad-
CAM  LRPT   LRP and SimpleGT given the true class  respectively. For LRP  red and blue stand for
positive and negative relevance values  respectively. Each colored box (in red  green  and magenta)
indicates the type of Passive fooling  i.e.  Location  Top-k  and Center-mass fooling  respectively.
Each row in each colored box stands for the interpreter  LRPT or Grad-CAM  that are used as I in
the objective function (2) to manipulate each model. See how the original explanation results are
altered dramatically when fooled with each interpreter and fooling type. The transferability among
methods should only be compared within each model architecture and fooling type.

For the Top-k fooling  we observe the most highlighted top k% pixels are signiÔ¨Åcantly altered after
the fooling  by comparing the big difference between the original explanations and those in the green
colored box in Figure 2. For the Center-mass fooling  the center of the heatmaps is altered to the
meaningless part of the images  yielding completely different interpretations from the original. Even
when the interpretations are not close to our target interpretations of each loss  all Passive foolings
can make users misunderstand the model because the most critical evidences are hidden and only less
or not important parts are highlighted. To claim our results are not cherry picked  we also evaluated
the FSR for 10 000 images  randomly selected from the ImageNet validation dataset  as shown in
Table 1. We can observe that all FSRs of fooling methods are higher than 50% for the matched cases
(bold underlined)  except for the Location fooling with LRPT for DenseNet121.
Next  for the Active fooling  from the qualitative results in Figure 3 and the quantitative results
in Table 2  we Ô¨Ånd that the explanations for c1 and c2 are swapped clearly in VGG19 and nearly
in ResNet50  but not in DenseNet121  suggesting the relationship between the model complexity
and the degree of Active fooling. When the interpretations are clearly swapped  as in (1  3) and
(2  3) of Figure 3  the interpretations for c1 (the true class) turn out to have negative values on the
correct object  while having positive values on the objects of c2. Even when the interpretations

2(a  b) denotes the image at the a-th row and b-th column of the Ô¨Ågure.

6

VGG19ResNet50Densenet121G-CAMLRPTLRPSimpleGTG-CAMLRPTLRPSimpleGTG-CAMLRPTLRPSimpleGTBaselineLocationLRPTG-CAMTop-ùëòLRPTG-CAMCenter-massLRPTG-CAMModel
FSR (%)

Location

Top-k

Center-mass

LRPT
G-CAM
LRPT
G-CAM
LRPT
G-CAM

VGG19

G-CAM LRPT
87.5
5.8
96.3
30.9
99.9
66.3

0.8
89.2
31.5
96.0
49.9
81.0

Resnet50

SimpleGT G-CAM LRPT
83.2
0.8
61.5
5.3
63.3
0.8

66.8
0.0
9.8
0.1
15.4
0.1

42.1
97.3
46.3
99.9
66.4
67.3

DenseNet121

SimpleGT

SimpleGT G-CAM LRPT
26.6
0.4
53.8
1.9
51.9
21.8

81.1
0.0
19.3
0.3
50.3
0.2

35.7
81.8
62.3
98.3
66.8
72.7

88.2
92.1
66.7
3.7
28.8
29.2

Table 1: Fooling Success Rates (FSR) for Passive fooled models. The structure of the table is the
same as Figure 2. 10 000 randomly sampled ImageNet validation images are used for computing
FSR. Underline stands for the FSRs for the matched interpreters that are used for fooling  and the
Bold stands for the FSRs over 50%. We excluded the results for LRP because checking the FSR
of LRPT was sufÔ¨Åcient for checking whether LRP was fooled or not. The transferability among
methods should only be compared within the model and fooling type.

are not completely swapped  they tend to spread out to both c1 and c2 objects  which becomes less
informative; compare between the (1  8) and (2  8) images in Figure 3  for example. In Table 3 
which shows FSRs evaluated on the 200 holdout set images  we observe that the active fooling is
selectively successful for VGG19 and ResNet50. For the case of DenseNet121  however  the active
fooling seems to be hard as the FSR values are almost 0. Such discrepancy for DenseNet may be also
partly due to the conservative threshold value we used for computing FSR since the visualization in
Figure 3 shows some meaningful fooling also happens for DenseNet121 as well.

Figure 3: Explanations of original and active fooled models for c1=‚ÄúAfrican Elephant‚Äù from synthetic
test images  which contain both Elephant and Firetruck (c2) in different parts of the images for class
c1. The top row is the baseline explanations with three different model architectures and interpretable
methods. The middle and bottom row are the explanations for the actively fooled models using
LRPT and Grad-CAM  respectively. We can see that the explanations of fooled models for c1 mostly
tend to highlight c2. Note the transferability also exists as well.

Model
FSR (%)

FSR(c_1)
LRPT
FSR(c_2)
G-CAM FSR(c_1)
FSR(c_2)

VGG19
G-CAM LRPT
94.5
95.0
0.0
1.0

96.5
96.5
1.0
70.0

ResNet50
LRP G-CAM LRPT
97.0
34.0
96.0
31.5
0.0
1.0
0.5
0.0

90.5
75.0
76.0
87.5

DenseNet121

LRP G-CAM LRPT
0.0
10.7
0.0
24.3
0.0
0.0
0.0
0.0

0.0
0.0
4.0
0.0

LRP
0.0
0.0
0.0
0.0

Table 2: Fooling Success Rates (FSR) for the Active fooled models. 200 synthetic images are used
for computing FSR. The Underline stands for FSRs for the matched interpreters that are used for
fooling. and the Bold stands for FSRs over 50%. The transferability among methods should only be
compared within the model and fooling type.
The signiÔ¨Åcance of the above results lies in the fact that the classiÔ¨Åcation accuracies of all manipulated
models are around the same as that of the original models shown in Table 3! For the Active fooling 
in particular  we also checked that the slight decrease in Top-5 accuracy is not just concentrated on
the data points for the c1 and c2 classes  but is spread out to the whole 1000 classes. Such analysis is

7

VGG 19ResNet 50Densenet121G-CAMLRPTLRPG-CAMLRPTLRPG-CAMLRPTLRPBaselineLRPTG-CAMin the Supplementary Material. Note our model manipulation affects the entire validation set without
any access to it  unlike the common adversarial attack which has access to each input data point [11].

Model

Accuracy (%)

Baseline (Pretrained)
Location

LRPT
G-CAM
LRPT
G-CAM
LRPT
G-CAM
LRPT
G-CAM

Top-k

Center
mass

Active

VGG19

Resnet50

Top1
72.4
71.8
71.5
71.6
72.1
70.4
70.6
71.3
71.2

Top5
90.9
90.7
90.4
90.5
90.6
89.8
90.0
90.3
90.3

Top1
76.1
73.0
74.2
73.7
74.7
73.4
74.7
74.7
75.9

Top5
92.9
91.3
91.8
91.9
92.0
91.7
92.1
92.2
92.8

DenseNet121
Top5
Top1
74.4
92.0
91.0
72.5
91.6
73.7
91.0
72.3
91.2
73.1
72.8
91.0
91.0
72.4
90.5
71.9
71.7
90.4

Table 3: Accuracy of the pre-trained models and the manipulated models on the entire ImageNet
validation set. The accuracy drops are around only 2%/1% for Top-1/Top-5 accuracy  respectively.

Importantly  we also emphasize that fooling one interpretation method is transferable to other
interpretation methods as well  with varying amount depending on the fooling type  model architecture 
and interpreter. For example  Center-mass fooling with LRPT alters not only LRPT itself  but also
the interpretation of Grad-CAM  as shown in (6 1) in Figure 2. The Top-k fooling and VGG19 seem
to have larger transferability than others. More discussion on the transferability is elaborated in
Section 5. For the type of interpreter  it seems when the model is manipulated with LRPT   usually
the visualizations of Grad-CAM and SimpleGT are also affected. However  when fooling is done
with Grad-CAM  LRPT and SimpleGT are less impacted.
5 Discussion and Conclusion

In this section  we give several important further discussions on our method. Firstly  one may
argue that our model manipulation might have not only fooled the interpretation results but also
model‚Äôs actual reasoning for making the prediction. To that regard  we employ Area Over Prediction

(a) AOPC curves

(b) Gaussian perturbation

(c) Fooling with adversarial training

Figure 4: (a) AOPC of original and Top-k fooled model (DenseNet121  Grad-CAM). (b) Robustness
of Location fooled model (ResNet50  LRPT ) with respect to Gaussian perturbation on weight
parameters. (c) Top-1 accuracy of ResNet50 on Dval and PGD(Dval)  and Grad-CAM results when
manipulating adversarially trained model with Location fooling.
Curve (AOPC) [35]  a principled way of quantitatively evaluating the validity of neural network
interpretations  to check whether the manipulated model also has been signiÔ¨Åcantly altered by fooling
the interpretation. Figure 4(a) shows the average AOPC curves on 10K validation images for the
original and manipulated DenseNet121 (Top-k fooled with Grad-CAM) models  wo and w‚àó
fool 
with three different perturbation orders; i.e.  with respect to hI
c (wo) scores  hI
c (w‚àó
fool) scores  and
fool(hI
a random order. From the Ô¨Ågure  we observe that wo(hI
c (wo)) show almost
identical AOPC curves  which suggests that w‚àó
fool has not changed much from wo and is making its
prediction by focusing on similar parts that wo bases its prediction  namely  hI
c (wo). In contrast  the
AOPC curves of both wo(hI
fool(hI
fool)) and w‚àó
fool)) lie signiÔ¨Åcantly lower  even lower than
the case of random perturbation. From this result  we can deduce that hI
c (w‚àó
fool) is highlighting parts
that are less helpful than random pixels for making predictions  hence  is a ‚Äúwrong‚Äù interpretation.

c (wo)) and w‚àó

c (w‚àó

c (w‚àó

8

Secondly  one may ask whether our fooling can be easily detected or undone. Since it is known that
the adversarial input example can be detected by adding small Gaussian perturbation to the input
[36]  one may also suspect that adding small Gaussian noise to the model parameters might reveal
our fooling. However  Figure 4(b) shows that wo and w‚àó
fool (ResNet50  Location-fooled with LRPT )
behave very similarly in terms of Top-1 accuracy on ImageNet validation as we increase the noise
level of the Gaussian perturbation  and FSRs do not change radically  either. Hence  we claim that
detecting or undoing our fooling would not be simple.
Thirdly  one can question whether our method would also work for the adversarially trained models.
To that end  Figure 4(c) shows the Top-1 accuracy of ResNet50 model on Dval (i.e.  ImageNet
validation) and PGD(Dval) (i.e.  the PGD-attacked Dval)  and demonstrates that adversarially trained
model can be also manipulated by our method. Namely  starting from a pre-trained wo (dashed red) 
we do the ‚Äúfree‚Äù adversarial training ( = 1.5) [37] to obtain wadv (dashed green)  then started our
model manipulation with (Location fooling  Grad-CAM) while keeping the adversarial training. Note
the Top-1 accuracy on Dval drops while that on PGD(Dval) increases during the adversarial training
phase (from red to green) as expected  and they are maintained during our model manipulation phase
(e.g. dashed blue). The right panel shows the Grad-CAM interpretations at three distinct phases (see
the color-coded boundaries)  and we clearly see the success of the Location fooling (blue  third row).

(a) Decision boundaries

(b) Fooling for SmoothGrad (SG)

Figure 5: (a) Two possible decision boundaries with similar accuracies but with different gradients.
(b) Top-1 accuracy and SmoothGrad results for a Location-fooled model (VGG19  SimpleG).

Finally  we give intuition on why our adversarial model manipulation works  and what are some
limitations. Note Ô¨Årst that all the interpretation methods we employ are related to some form of
gradients; SimpleG uses the gradient of the input  Grad-CAM is a function of the gradient of the
representation at a certain layer  and LRP turns out to be similar to gradient times inputs [38].
Motivated by [11]  Figure 5(a) illustrates the point that the same test data can be classiÔ¨Åed with
almost the same accuracy but with different decision boundaries that result in radically different
gradients  or interpretations. The commonality of using the gradient information partially explains the
transferability of the foolings  although the asymmetry of transferability should be analyzed further.
Furthermore  the level of fooling seems to have intriguing connection with the model complexity 
similar to the Ô¨Ånding in [39] in the context of input adversarial attack. As a hint for developing
more robust interpretation methods  Figure 5(b) shows our results on fooling SmoothGrad [40] 
which integrates SimpleG maps obtained from multiple Gaussian noise added inputs. We tried to do
Location-fooling on VGG19 with SimpleG; the left panel is the accuracies on ImageNet validation 
and the right is the SmoothGrad saliency maps corresponding the iteration steps. Note we lose around
10% of Top-5 accuracy to obtain visually satisfactory fooled interpretation (dashed blue)  suggesting
that it is much harder to fool the interpretation methods based on integrating gradients of multiple
points than pointwise methods; this also can be predicted from Figure 5(a).
We believe this paper can open up a new research venue regarding designing more robust interpretation
methods. We argue checking the robustness of interpretation methods with respect to our adversarial
model manipulation should be an indispensable criterion for the interpreters in addition to the sanity
checks proposed in [28]; note Grad-CAM passes their checks. Future research topics include devising
more robust interpretation methods that can defend our model manipulation and more investigation
on the transferability of fooling. Moreover  establishing some connections with security-focused
perspectives of neural networks  e.g.  [41  42]  would be another fruitful direction to pursue.

9

Acknowledgements

This work is supported in part by ICT R&D Program [No. 2016-0-00563  Research on adaptive
machine learning technology development for intelligent autonomous digital companion][No. 2019-
0-01396  Development of framework for analyzing  detecting  mitigating of bias in AI model and
training data]  AI Graduate School Support Program [No.2019-0-00421]  and ITRC Support Program
[IITP-2019-2018-0-01798] of MSIT / IITP of the Korean government  and by the KIST Institutional
Program [No. 2E29330].

References
[1] Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard Zemel. Fairness

through awareness. In ITCS  pages 214‚Äì226. ACM  2012.

[2] W James Murdoch  Chandan Singh  Karl Kumbier  Reza Abbasi-Asl  and Bin Yu. Interpretable

machine learning: deÔ¨Ånitions  methods  and applications. arXiv:1901.04592  2019.

[3] David Gunning. Explainable artiÔ¨Åcial intelligence (XAI). In DARPA  2017.

[4] Marco Tulio Ribeiro  Sameer Singh  and Carlos Guestrin. Why should I trust you?: Explaining

the predictions of any classiÔ¨Åer. In SIGKDD  pages 1135‚Äì1144. ACM  2016.

[5] Scott M. Lundberg and Su-In Lee. A uniÔ¨Åed approach to interpreting model predictions. In

NIPS  2017.

[6] Sebastian Bach  Alexander Binder  Gr√©goire Montavon  Frederick Klauschen  Klaus-Robert
M√ºller  and Wojciech Samek. On pixel-wise explanations for non-linear classiÔ¨Åer decisions by
layer-wise relevance propagation. In PLoS ONE  10(7):e0130140  2015.

[7] Ramprasaath R Selvaraju  Michael Cogswell  Abhishek Das  Ramakrishna Vedantam  Devi
Parikh  and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based
localization. In ICCV  pages 618‚Äì626  2017.

[8] Wojciech Samek  Gr√©goire Montavon  and Klaus-Robert M√ºller.

Interpreting and ex-
In CVPR Tutorial (http://interpretable-

plaining deep models in computer vision.
ml.org/cvpr2018tutorial/)  2018.

[9] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.

arXiv:1702.08608  2017.

[10] Pieter-Jan Kindermans  Sara Hooker  Julius Adebayo  Maximilian Alber  Kristof T Sch√ºtt  Sven
D√§hne  Dumitru Erhan  and Been Kim. The (un) reliability of saliency methods. In Explainable
AI: Interpreting  Explaining and Visualizing Deep Learning  pages 267‚Äì280. Springer  2019.

[11] Amirata Ghorbani  Abubakar Abid  and James Zou. Interpretation of neural networks is fragile.

In AAAI  2019.

[12] David Alvares-Melis and Tommi S. Jaakkola. Towards robust interpretability with self-

explaining neural networks. In NeurIPS  2018.

[13] Xinyang Zhang  Ningfei Wang  Shouling Ji  Hua Shen  and Ting Wang. Interpretable deep

learning under Ô¨Åre. arXiv:1812.00891  2018.

[14] Ann-Kathrin Dombrowski  Maximilian Alber  Christopher J Anders  Marcel Ackermann  Klaus-
Robert M√ºller  and Pan Kessel. Explanations can be manipulated and geometry is to blame. In
NeurIPS  2019.

[15] Arthur Asuncion and David Newman. UCI machine learning repository  2007.

[16] Ann Arbor District Library. Elephant pulls Ô¨Åre truck at the franzen brothers circus  1996.

[17] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks:

Visualising image classiÔ¨Åcation models and saliency maps. arXiv:1312.6034  2013.

10

[18] Riccardo Guidotti  Anna Monreale  Salvatore Ruggieri  Franco Turini  Fosca Giannotti  and
Dino Pedreschi. A survey of methods for explaining black box models. In CSUR  volume 51 
page 93. ACM  2019.

[19] Vitali Petsiuk  Abir Das  and Kate Saenko. RISE: Randomized input sampling for explanation

of black-box models. In BMVC  2018.

[20] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In

ECCV  pages 818‚Äì833. Springer  2014.

[21] Avanti Shrikumar  Peyton Greenside  and Anshul Kundaje. Learning important features through

propagating activation differences. In ICML  2017.

[22] Mukund Sundararajan  Ankur Taly  and Qiqi Yan. Axiomatic attribution for deep networks. In

ICML  2017.

[23] Jost Tobias Springenberg  Alexey Dosovitskiy  Thomas Brox  and Martin Riedmiller. Striving

for simplicity: The all convolutional net. In ICLR Workshop  2015.

[24] Bin Yu. Stability. In Bernoulli  volume 19  pages 1484‚Äì1500  2013.

[25] Anish Athalye  Nicholas Carlini  and David Wagner. Obfuscated gradients give a false sense of

security: Circumventing defenses to adversarial examples. In ICML  2018.

[26] Alexey Kurakin  Ian Goodfellow  and Samy Bengio. Adversarial examples in the physical

world. arXiv:1607.02533  2016.

[27] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.

Towards deep learning models resistant to adversarial attacks. arXiv:1706.06083  2017.

[28] Julius Adebayo  J. Gilmer  Michael Muelly  Ian Goodfellow  Moritz Hardt  and Been Kim.

Sanity chekcs for saliency maps. In NeurIPS  2018.

[29] Sebastian Lapuschkin  Alexander Binder  Klaus-Robert Muller  and Wojciech Samek. Under-
standing and comparing deep neural networks for age and gender classiÔ¨Åcation. In ECCV  pages
1629‚Äì1638  2017.

[30] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. In IJCV  volume 115  pages 211‚Äì252 
2015.

[31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. In ICLR  2015.

[32] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. In CVPR  pages 770‚Äì778  2016.

[33] Gao Huang  Zhuang Liu  Laurens Van Der Maaten  and Kilian Q Weinberger. Densely connected

convolutional networks. In CVPR  pages 4700‚Äì4708  2017.

[34] J. Russell and R. Cohn. Spearman‚Äôs Rank Correlation CoefÔ¨Åcient. Book on Demand  2012.

[35] Wojciech Samek  Alexander Binder  Gr√©goire Montavon  Sebastian Lapuschkin  and Klaus-
Robert M√ºller. Evaluating the visualization of what a deep neural network has learned. In IEEE
transactions on neural networks and learning systems  volume 28  pages 2660‚Äì2673. IEEE 
2016.

[36] Kevin Roth  Yannic Kilcher  and Thomas Hofmann. The odds are odd: A statistical test for

detecting adversarial examples. In ICML  2019.

[37] Ali Shafahi  Mahyar Najibi  Amin Ghiasi  Zheng Xu  John Dickerson  Christoph Studer  Larry S
Davis  Gavin Taylor  and Tom Goldstein. Adversarial training for free! arXiv:1904.12843 
2019.

11

[38] Pieter-Jan Kindermans  Kristof Sch√ºtt  Klaus-Robert M√ºller  and Sven D√§hne. Investigating the
inÔ¨Çuence of noise and distractors on the interpretation of neural networks. arXiv:1611.07270 
2016.

[39] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv:1412.6572  2014.

[40] Daniel Smilkov  Nikhil Thorat  Been Kim  Fernanda Vi√©gas  and Martin Wattenberg. Smooth-

grad: removing noise by adding noise. arXiv:1706.03825  2017.

[41] Tianyu Gu  Brendan Dolan-Gavitt  and Siddharth Garg. Badnets: Identifying vulnerabilities in

the machine learning model supply chain. arXiv:1708.06733  2017.

[42] Yossi Adi  Carsten Baum  Moustapha Cisse  Benny Pinkas  and Joseph Keshet. Turning your
weakness into a strength: Watermarking deep neural networks by backdooring. In USENIX
Security  pages 1615‚Äì1631  2018.

12

,Vasilis Syrgkanis
Alekh Agarwal
Haipeng Luo
Robert Schapire
Brian Dolhansky
Jeff Bilmes
Ben London
Edward Hughes
Joel Leibo
Matthew Phillips
Karl Tuyls
Edgar Due√±ez-Guzman
Antonio Garc√≠a Casta√±eda
Iain Dunning
Tina Zhu
Kevin McKee
Raphael Koster
Heather Roff
Thore Graepel
Juyeon Heo
Sunghwan Joo
Taesup Moon