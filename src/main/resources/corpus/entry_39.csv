2015,A Complete Recipe for Stochastic Gradient MCMC,Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem  a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However  such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial.  Even with simple dynamics  significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise.  In this paper  we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices.  We constructively prove that the framework is complete. That is  any continuous Markov process that provides samples from the target distribution can be written in our framework.  We show how previous continuous-dynamic samplers can be trivially reinvented in our framework  avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC).  Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC  with the scalability of stochastic gradient methods.,A Complete Recipe for Stochastic Gradient MCMC

University of Washington {yianma@u tqchen@cs ebfox@stat}.washington.edu

Yi-An Ma  Tianqi Chen  and Emily B. Fox

Abstract

Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous
dynamics to deﬁne a transition kernel that efﬁciently explores a target distribution.
In tandem  a focus has been on devising scalable variants that subsample the data
and use stochastic gradients in place of full-data gradients in the dynamic simu-
lations. However  such stochastic gradient MCMC samplers have lagged behind
their full-data counterparts in terms of the complexity of dynamics considered
since proving convergence in the presence of the stochastic gradient noise is non-
trivial. Even with simple dynamics  signiﬁcant physical intuition is often required
to modify the dynamical system to account for the stochastic gradient noise. In this
paper  we provide a general recipe for constructing MCMC samplers—including
stochastic gradient versions—based on continuous Markov processes speciﬁed vi-
a two matrices. We constructively prove that the framework is complete. That is 
any continuous Markov process that provides samples from the target distribution
can be written in our framework. We show how previous continuous-dynamic
samplers can be trivially “reinvented” in our framework  avoiding the complicated
sampler-speciﬁc proofs. We likewise use our recipe to straightforwardly propose
a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte
Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedi-
a analysis demonstrate that the proposed SGRHMC sampler inherits the beneﬁts
of Riemann HMC  with the scalability of stochastic gradient methods.

1

Introduction

Markov chain Monte Carlo (MCMC) has become a defacto tool for Bayesian posterior inference.
However  these methods notoriously mix slowly in complex  high-dimensional models and scale
poorly to large datasets. The past decades have seen a rise in MCMC methods that provide more efﬁ-
cient exploration of the posterior  such as Hamiltonian Monte Carlo (HMC) [8  12] and its Reimann
manifold variant [10]. This class of samplers is based on deﬁning a potential energy function in
terms of the target posterior distribution and then devising various continuous dynamics to explore
the energy landscape  enabling proposals of distant states. The gain in efﬁciency of exploration often
comes at the cost of a signiﬁcant computational burden in large datasets.
Recently  stochastic gradient variants of such continuous-dynamic samplers have proven quite useful
in scaling the methods to large datasets [17  1  6  2  7]. At each iteration  these samplers use data
subsamples—or minibatches—rather than the full dataset. Stochastic gradient Langevin dynamics
(SGLD) [17] innovated in this area by connecting stochastic optimization with a ﬁrst-order Langevin
dynamic MCMC technique  showing that adding the “right amount” of noise to stochastic gradient
ascent iterates leads to samples from the target posterior as the step size is annealed. Stochastic
gradient Hamiltonian Monte Carlo (SGHMC) [6] builds on this idea  but importantly incorporates
the efﬁcient exploration provided by the HMC momentum term. A key insight in that paper was that
the na¨ıve stochastic gradient variant of HMC actually leads to an incorrect stationary distribution
(also see [4]); instead a modiﬁcation to the dynamics underlying HMC is needed to account for

1

the stochastic gradient noise. Variants of both SGLD and SGHMC with further modiﬁcations to
improve efﬁciency have also recently been proposed [1  13  7].
In the plethora of past MCMC methods that explicitly leverage continuous dynamics—including
HMC  Riemann manifold HMC  and the stochastic gradient methods—the focus has been on show-
ing that the intricate dynamics leave the target posterior distribution invariant. Innovating in this
arena requires constructing novel dynamics and simultaneously ensuring that the target distribution
is the stationary distribution. This can be quite challenging  and often requires signiﬁcant physical
and geometrical intuition [6  13  7]. A natural question  then  is whether there exists a general recipe
for devising such continuous-dynamic MCMC methods that naturally lead to invariance of the target
distribution. In this paper  we answer this question to the afﬁrmative. Furthermore  and quite im-
portantly  our proposed recipe is complete. That is  any continuous Markov process (with no jumps)
with the desired invariant distribution can be cast within our framework  including HMC  Riemann
manifold HMC  SGLD  SGHMC  their recent variants  and any future developments in this area.
That is  our method provides a unifying framework of past algorithms  as well as a practical tool for
devising new samplers and testing the correctness of proposed samplers.
The recipe involves deﬁning a (stochastic) system parameterized by two matrices: a positive
semideﬁnite diffusion matrix  D(z)  and a skew-symmetric curl matrix  Q(z)  where z = (θ  r)
with θ our model parameters of interest and r a set of auxiliary variables. The dynamics are then
written explicitly in terms of the target stationary distribution and these two matrices. By varying
the choices of D(z) and Q(z)  we explore the space of MCMC methods that maintain the correct
invariant distribution. We constructively prove the completeness of this framework by converting a
general continuous Markov process into the proposed dynamic structure.
For any given D(z)  Q(z)  and target distribution  we provide practical algorithms for implement-
ing either full-data or minibatch-based variants of the sampler. In Sec. 3.1  we cast many previous
continuous-dynamic samplers in our framework  ﬁnding their D(z) and Q(z). We then show how
these existing D(z) and Q(z) building blocks can be used to devise new samplers; we leave the
question of exploring the space of D(z) and Q(z) well-suited to the structure of the target distribu-
tion as an interesting direction for future research. In Sec. 3.2 we demonstrate our ability to construct
new and relevant samplers by proposing stochastic gradient Riemann Hamiltonian Monte Carlo  the
existence of which was previously only speculated. We demonstrate the utility of this sampler on
synthetic data and in a streaming Wikipedia analysis using latent Dirichlet allocation [5].

2 A Complete Stochastic Gradient MCMC Framework
We start with the standard MCMC goal of drawing samples from a target distribution  which we take
to be the posterior p(θ|S) of model parameters θ ∈ Rd given an observed dataset S. Throughout 
we assume i.i.d. data x ∼ p(x|θ). We write p(θ|S) ∝ exp(−U (θ))  with potential function
x∈S log p(x|θ) − log p(θ). Algorithms like HMC [12  10] further augment the space
of interest with auxiliary variables r and sample from p(z|S) ∝ exp(−H(z))  with Hamiltonian

U (θ) = −(cid:80)

(cid:90)

H(z) = H(θ  r) = U (θ) + g(θ  r) 

such that

exp(−g(θ  r))dr = constant.

(1)

Marginalizing the auxiliary variables gives us the desired distribution on θ. In this paper  we gener-
ically consider z as the samples we seek to draw; z could represent θ itself  or an augmented state
space in which case we simply discard the auxiliary variables to perform the desired marginalization.
As in HMC  the idea is to translate the task of sampling from the posterior distribution to simulating
from a continuous dynamical system which is used to deﬁne a Markov transition kernel. That is 
over any interval h  the differential equation deﬁnes a mapping from the state at time t to the state
at time t + h. One can then discuss the evolution of the distribution p(z  t) under the dynamics  as
characterized by the Fokker-Planck equation for stochastic dynamics [14] or the Liouville equation
for deterministic dynamics [20]. This evolution can be used to analyze the invariant distribution of
the dynamics  ps(z). When considering deterministic dynamics  as in HMC  a jump process must
be added to ensure ergodicity. If the resulting stationary distribution is equal to the target posterior 
then simulating from the process can be equated with drawing samples from the posterior.
If the stationary distribution is not the target distribution  a Metropolis-Hastings (MH) correction
can often be applied. Unfortunately  such correction steps require a costly computation on the entire

2

dataset. Even if one can compute the MH correction  if the dynamics do not nearly lead to the
correct stationary distribution  then the rejection rate can be high even for short simulation periods
h. Furthermore  for many stochastic gradient MCMC samplers  computing the probability of the
reverse path is infeasible  obviating the use of MH. As such  a focus in the literature is on deﬁning
dynamics with the right target distribution  especially in large-data scenarios where MH corrections
are computationally burdensome or infeasible.

2.1 Devising SDEs with a Speciﬁed Target Stationary Distribution
Generically  all continuous Markov processes that one might consider for sampling can be written
as a stochastic differential equation (SDE) of the form:

dz = f (z)dt +(cid:112)2D(z)dW(t) 

where f (z) denotes the deterministic drift and often relates to the gradient of H(z)  W(t) is a d-
dimensional Wiener process  and D(z) is a positive semideﬁnite diffusion matrix. Clearly  however 
not all choices of f (z) and D(z) yield the stationary distribution ps(z) ∝ exp(−H(z)).
When D(z) = 0  as in HMC  the dynamics of Eq. (2) become deterministic. Our exposition focuses
on SDEs  but our analysis applies to deterministic dynamics as well. In this case  our framework—
using the Liouville equation in place of Fokker-Planck—ensures that the deterministic dynamics
leave the target distribution invariant. For ergodicity  a jump process must be added  which is not
considered in our recipe  but tends to be straightforward (e.g.  momentum resampling in HMC).
To devise a recipe for constructing SDEs with the correct stationary distribution  we propose writing
f (z) directly in terms of the target distribution:

f (z) = −(cid:2)D(z) + Q(z)(cid:3)∇H(z) + Γ(z)  Γi(z) =

d(cid:88)

j=1

∂
∂zj

(cid:0)Dij(z) + Qij(z)(cid:1).

Here  Q(z) is a skew-symmetric curl matrix representing the deterministic traversing effects seen
in HMC procedures. In contrast  the diffusion matrix D(z) determines the strength of the Wiener-
process-driven diffusion. Matrices D(z) and Q(z) can be adjusted to attain faster convergence to
the posterior distribution. A more detailed discussion on the interpretation of D(z) and Q(z) and
the inﬂuence of speciﬁc choices of these matrices is provided in the Supplement.
Importantly  as we show in Theorem 1  sampling the stochastic dynamics of Eq. (2) (according
to Itˆo integral) with f (z) as in Eq. (3) leads to the desired posterior distribution as the stationary
distribution: ps(z) ∝ exp(−H(z)). That is  for any choice of positive semideﬁnite D(z) and skew-
symmetric Q(z) parameterizing f (z)  we know that simulating from Eq. (2) will provide samples
from p(θ | S) (discarding any sampled auxiliary variables r) assuming the process is ergodic.
Theorem 1. ps(z) ∝ exp(−H(z)) is a stationary distribution of the dynamics of Eq. (2) if f (z) is
restricted to the form of Eq. (3)  with D(z) positive semideﬁnite and Q(z) skew-symmetric. If D(z)
is positive deﬁnite  or if ergodicity can be shown  then the stationary distribution is unique.

Proof. The equivalence of ps(z) and the target p(z | S) ∝ exp(−H(z)) can be shown using the
Fokker-Planck description of the probability density evolution under the dynamics of Eq. (2) :

(2)

(3)

(4)

∂tp(z  t) = −(cid:88)
∂tp(z  t) =∇T ·(cid:16)

i

∂
∂zi

(cid:0)fi(z)p(z  t)(cid:1) +

(cid:88)

∂2

∂zi∂zj

i j

(cid:0)Dij(z)p(z  t)(cid:1).
(cid:17)

Eq. (4) can be further transformed into a more compact form [19  16]:

We can verify that p(z | S) is invariant under Eq. (5) by calculating(cid:2)e−H(z)∇H(z) + ∇e−H(z)(cid:3) =

[D(z) + Q(z)] [p(z  t)∇H(z) + ∇p(z  t)]

0. If the process is ergodic  this invariant distribution is unique. The equivalence of the compact form
was originally proved in [16]; we include a detailed proof in the Supplement for completeness.

(5)

.

3

Figure 1: The red space represents the set of all continuous Markov
processes. A point
in the black space represents a continuous
Markov process deﬁned by Eqs. (2)-(3) based on a speciﬁc choice of
D(z)  Q(z). By Theorem 1  each such point has stationary distribution
ps(z) = p(z | S). The blue space represents all continuous Markov
processes with ps(z) = p(z | S). Theorem 2 states that these blue and
black spaces are equivalent (there is no gap  and any point in the blue
space has a corresponding D(z)  Q(z) in our framework).

2.2 Completeness of the Framework
An important question is what portion of samplers deﬁned by continuous Markov processes with
the target invariant distribution can we deﬁne by iterating over all possible D(z) and Q(z)? In
Theorem 2  we show that for any continuous Markov process with the desired stationary distribution 
ps(z)  there exists an SDE as in Eq. (2) with f (z) deﬁned as in Eq. (3). We know from the Chapman-
Kolmogorov equation [9] that any continuous Markov process with stationary distribution ps(z) can
be written as in Eq. (2)  which gives us the diffusion matrix D(z). Theorem 2 then constructively
deﬁnes the curl matrix Q(z). This result implies that our recipe is complete. That is  we cover all
possible continuous Markov process samplers in our framework. See Fig. 1.
Theorem 2. For the SDE of Eq. (2)  suppose its stationary probability density function ps(z) u-
niquely exists  and that
is integrable with respect to the

(cid:20)
fi(z)ps(z) −(cid:80)d

Dij(z)ps(z)

(cid:17)(cid:21)

(cid:16)

∂
∂θj

j=1

Lebesgue measure  then there exists a skew-symmetric Q(z) such that Eq. (3) holds.

(cid:88)
x∈(cid:101)S

log p(x|θ) − log p(θ);

The integrability condition is usually satisﬁed when the probability density function uniquely exists.
A constructive proof for the existence of Q(z) is provided in the Supplement.
2.3 A Practical Algorithm
In practice  simulation relies on an -discretization of the SDE  leading to a full-data update rule

(cid:2)(cid:0)D(zt) + Q(zt)(cid:1)∇H(zt) + Γ(zt)(cid:3) + N (0  2tD(zt)).

zt+1 ← zt − t

(6)
Calculating the gradient of H(z) involves evaluating the gradient of U (θ). For a stochastic gradient
method  the assumption is that U (θ) is too computationally intensive to compute as it relies on a sum
over all data points (see Sec. 2). Instead  such stochastic gradient algorithms examine independently

sampled data subsets (cid:101)S ⊂ S and the corresponding potential for these data:
(cid:101)S ⊂ S.
The speciﬁc form of Eq. (7) implies that (cid:101)U (θ) is an unbiased estimator of U (θ). As such  a gradient
computed based on (cid:101)U (θ)—called a stochastic gradient [15]—is a noisy  but unbiased estimator
distribution of the modiﬁed dynamics (using ∇(cid:101)U (θ) in place of ∇U (θ)). One way to analyze the
resulting in a noisy Hamiltonian gradient ∇(cid:101)H(z) = ∇H(z) + [N (0  V(θ))  0]T . Simply plugging
in ∇(cid:101)H(z) in place of ∇H(z) in Eq. (6) results in dynamics with an additional noise term (D(zt) +
Q(zt)(cid:1)[N (0  V(θ))  0]T . To counteract this  assume we have an estimate ˆBt of the variance of this

of the full-data gradient. The key question in many of the existing stochastic gradient MCMC
algorithms is whether the noise injected by the stochastic gradient adversely affects the stationary

∇(cid:101)U (θ) = ∇U (θ) + N (0  V(θ)) 

impact of the stochastic gradient is to make use of the central limit theorem and assume

(cid:101)U (θ) = −|S|
|(cid:101)S|

additional noise satisfying 2D(zt) − t ˆBt (cid:23) 0 (i.e.  positive semideﬁnite). With small   this is
always true since the stochastic gradient noise scales down faster than the added noise. Then  we
can attempt to account for the stochastic gradient noise by simulating

(8)

(7)

(cid:104)(cid:0)D(zt) + Q(zt)(cid:1)∇(cid:101)H(zt) + Γ(zt)
(cid:105)

zt+1 ← zt − t

(9)
This provides our stochastic gradient—or minibatch— variant of the sampler. In Eq. (9)  the noise
introduced by the stochastic gradient is multiplied by t (and the compensation by 2
t )  implying that

+ N (0  t(2D(zt) − t ˆBt)).

4

All Continuous Markov Processes f(z) defined by D(z)  Q(z) Processes with ps(z) = p(z|S) the discrepancy between these dynamics and those of Eq. (6) approaches zero as t goes to zero. As
such  in this inﬁnitesimal step size limit  since Eq. (6) yields the correct invariant distribution  so
does Eq. (9). This avoids the need for a costly or potentially intractable MH correction. However 
having to decrease t to zero comes at the cost of increasingly small updates. We can also use a ﬁnite 
small step size in practice  resulting in a biased (but faster) sampler. A similar bias-speed tradeoff
was used in [11  3] to construct MH samplers  in addition to being used in SGLD and SGHMC.

3 Applying the Theory to Construct Samplers
3.1 Casting Previous MCMC Algorithms within the Proposed Framework
We explicitly state how some recently developed MCMC methods fall within the proposed frame-
work based on speciﬁc choices of D(z)  Q(z) and H(z) in Eq. (2) and (3). For the stochastic
gradient methods  we show how our framework can be used to “reinvent” the samplers by guiding
their construction and avoiding potential mistakes or inefﬁciencies caused by na¨ıve implementations.

Hamiltonian Monte Carlo (HMC) The key ingredient in HMC [8  12] is Hamiltonian dynamics 
which simulate the physical motion of an object with position θ  momentum r  and mass M on an
frictionless surface as follows (typically  a leapfrog simulation is used instead):

(cid:26) θt+1 ← θt + tM−1rt

rt+1 ← rt − t∇U (θt).

Eq. (10) is a special case of the proposed framework with z = (θ  r)  H(θ  r) = U (θ) + 1

(10)
2 rT M−1r 

(cid:19)

(cid:18) 0 −I

I

0

Q(θ  r) =

and D(θ  r) = 0.

(cid:19)

Naive :

t V(θt)) 

Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) As discussed in [6]  simply replac-

ing ∇U (θ) by the stochastic gradient ∇(cid:101)U (θ) in Eq. (10) results in the following updates:

(cid:26) θt+1 ← θt + tM−1rt
rt+1 ← rt − t∇(cid:101)U (θt) ≈ rt − t∇U (θt) + N (0  2

0
V(θ)

(11)
where the ≈ arises from the approximation of Eq. (8). Careful study shows that Eq. (11) cannot be
rewritten into our proposed framework  which hints that such a na¨ıve stochastic gradient version of
HMC is not correct. Interestingly  the authors of [6] proved that this na¨ıve version indeed does not
have the correct stationary distribution. In our framework  we see that the noise term N (0  2tD(z))
is paired with a D(z)∇H(z) term  hinting that such a term should be added to Eq. (11). Here 
  which means we need to add D(z)∇H(z) = V(θ)∇rH(θ  r) =
D(θ  r) =
V(θ)M−1r. Interestingly  this is the correction strategy proposed in [6]  but through a physical
interpretation of the dynamics. In particular  the term V(θ)M−1r (or  generically  CM−1r where
C (cid:23) V(θ)) has an interpretation as friction and leads to second order Langevin dynamics:

(cid:18) 0
(cid:26) θt+1 ← θt + tM−1rt
rt+1 ← rt − t∇(cid:101)U (θt) − tCM−1rt + N (0  t(2C − t ˆBt)).
(cid:18) 0

Here  ˆBt is an estimate of V(θt). This method now ﬁts into our framework with H(θ  r) and Q(θ  r)
as in HMC  but with D(θ  r) =
. This example shows how our theory can be used to
identify invalid samplers and provide guidance on how to effortlessly correct the mistakes; this is
crucial when physical intuition is not available. Once the proposed sampler is cast in our framework
with a speciﬁc D(z) and Q(z)  there is no need for sampler-speciﬁc proofs  such as those of [6].

0
0 C

(cid:19)

(12)

0

Stochastic Gradient Langevin Dynamics (SGLD) SGLD [17] proposes to use the following ﬁrst
order (no momentum) Langevin dynamics to generate samples

θt+1 ← θt − tD∇(cid:101)U (θt) + N (0  2tD).

(13)
This algorithm corresponds to taking z = θ with H(θ) = U (θ)  D(θ) = D  Q(θ) = 0  and ˆBt = 0.
As motivated by Eq. (9) of our framework  the variance of the stochastic gradient can be subtracted
from the sampler injected noise to make the ﬁnite stepsize simulation more accurate. This variant of
SGLD leads to the stochastic gradient Fisher scoring algorithm [1].

5

Stochastic Gradient Riemannian Langevin Dynamics (SGRLD) SGLD can be generalized to
use an adaptive diffusion matrix D(θ). Speciﬁcally  it is interesting to take D(θ) = G−1(θ)  where
G(θ) is the Fisher information metric. The sampler dynamics are given by

θt+1 ← θt − t[G(θt)−1∇(cid:101)U (θt) + Γ(θt)] + N (0  2tG(θt)−1).

(14)
Taking D(θ) = G(θ)−1  Q(θ) = 0  and ˆBt = 0  this SGRLD [13] method falls into our frame-
. It is interesting to note that in earlier literature [10] 

∂Dij(θ)

work with correction term Γi(θ) =(cid:80)
Γi(θ) was taken to be 2 |G(θ)|−1/2(cid:80)

ij (θ)|G(θ)|1/2(cid:1). More recently  it was found that
(cid:0)G−1

j

∂θj
∂
∂θj

j

this correction term corresponds to the distribution function with respect to a non-Lebesgue mea-
sure [18]; for the Lebesgue measure  the revised Γi(θ) was as determined by our framework [18].
Again  we have an example of our theory providing guidance in devising correct samplers.

Stochastic Gradient Nos´e-Hoover Thermostat (SGNHT) Finally  the SGNHT [7] method in-
corporates ideas from thermodynamics to further increase adaptivity by augmenting the SGHMC
system with an additional scalar auxiliary variable  ξ. The algorithm uses the following dynamics:

θt+1 ← θt + trt

rt+1 ← rt − t∇(cid:101)U (θt) − tξtrt + N (0  t(2A − t ˆBt))



(cid:18) 0

(cid:19)

.

(cid:18) 1
(cid:19)

d

t rt − 1
rT
1
2

ξt+1 ← ξt + t

−I
0

I
0 −rT /d

0

r/d

0

(cid:18) 0

0

0 A · I
0

0

(15)

(cid:19)

 

0
0
0

We can take z = (θ  r  ξ)  H(θ  r  ξ) = U (θ) +

rT r +

(ξ − A)2  D(θ  r  ξ) =

1
2d

and Q(θ  r  ξ) =

to place these dynamics within our framework.

Summary
In our framework  SGLD and SGRLD take Q(z) = 0 and instead stress the design of
the diffusion matrix D(z)  with SGLD using a constant D(z) and SGRLD an adaptive  θ-dependent
diffusion matrix to better account for the geometry of the space being explored. On the other hand 
HMC takes D(z) = 0 and focuses on the curl matrix Q(z). SGHMC combines SGLD with HMC
through non-zero D(θ) and Q(θ) matrices. SGNHT then extends SGHMC by taking Q(z) to be
state dependent. The relationships between these methods are depicted in the Supplement  which
likewise contains a discussion of the tradeoffs between these two matrices.
In short  D(z) can
guide escaping from local modes while Q(z) can enable rapid traversing of low-probability regions 
especially when state adaptation is incorporated. We readily see that most of the product space
D(z) × Q(z)  deﬁning the space of all possible samplers  has yet to be ﬁlled.
3.2 Stochastic Gradient Riemann Hamiltonian Monte Carlo
In Sec. 3.1  we have shown how our framework uniﬁes existing samplers. In this section  we now use
our framework to guide the development of a new sampler. While SGHMC [6] inherits the momen-
tum term of HMC  making it easier to traverse the space of parameters  the underlying geometry of
the target distribution is still not utilized. Such information can usually be represented by the Fisher
information metric [10]  denoted as G(θ)  which can be used to precondition the dynamics. For our
2 rT r  as in HMC/SGHMC methods  and modify
proposed system  we consider H(θ  r) = U (θ) + 1
the D(θ  r) and Q(θ  r) of SGHMC to account for the geometry as follows:

D(θ  r) =

0

0 G(θ)−1

;

Q(θ  r) =

0

G(θ)−1/2

−G(θ)−1/2

0

(cid:18) 0

(cid:19)

(cid:19)

.

(cid:18)

We refer to this algorithm as stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC).
Our theory holds for any positive deﬁnite G(θ)  yielding a generalized SGRHMC (gSGRHMC)
algorithm  which can be helpful when the Fisher information metric is hard to compute.
A na¨ıve implementation of a state-dependent SGHMC algorithm might simply (i) precondition the

HMC update  (ii) replace ∇U (θ) by ∇(cid:101)U (θ)  and (iii) add a state-dependent friction term on the

(cid:26) θt+1 ← θt + tG(θt)−1/2rt
rt+1 ← rt − tG(θt)−1/2∇θ(cid:101)U (θt) − tG(θt)−1rt + N (0  t(2G(θt)−1 − t ˆBt)).

order of the diffusion matrix to counterbalance the noise as in SGHMC  resulting in:

Naive :

(16)

6

Algorithm 1: Generalized Stochastic Gradient Riemann Hamiltonian Monte Carlo
initialize (θ0  r0)
for t = 0  1  2··· do

optionally  periodically resample momentum r as r(t) ∼ N (0  I)
θt+1 ← θt + tG(θt)−1/2rt  Σt ← t(2G(θt)−1 − t ˆBt)

rt+1 ← rt − tG(θt)−1/2∇θ(cid:101)U (θt) + t∇θ(G(θt)−1/2) − tG(θt)−1rt + N(cid:16)

(cid:17)

0  Σt

end

Figure 2: Left: For two simulated 1D distributions deﬁned by U (θ) = θ2/2 (one peak) and U (θ) = θ4 − 2θ2
(two peaks)  we compare the KL divergence of methods: SGLD  SGHMC  the na¨ıve SGRHMC of Eq. (16)  and
the gSGRHMC of Eq. (17) relative to the true distribution in each scenario (left and right bars labeled by 1 and
2). Right: For a correlated 2D distribution with U (θ1  θ2) = θ4
1)2/2  we see that
our gSGRHMC most rapidly explores the space relative to SGHMC and SGLD. Contour plots of the distribution
along with paths of the ﬁrst 10 sampled points are shown for each method.

1/10 + (4 · (θ2 + 1.2) − θ2

However  as we show in Sec. 4.1  samples from these dynamics do not converge to the desired
distribution. Indeed  this system cannot be written within our framework. Instead  we can simply
follow our framework and  as indicated by Eq. (9)  consider the following update rule:

(cid:40)

θt+1 ← θt + tG(θt)−1/2rt

rt+1 ← rt − t[G(θ)−1/2∇θ(cid:101)U (θt) + ∇θ

G(θt)−1/2(cid:17) − G(θt)−1rt] + N (0  t(2G(θt)−1 − t ˆBt)) 

(cid:16)
(cid:0)G(θ)−1/2(cid:1)  with i-th component(cid:80)

(cid:0)G(θ)−1/2(cid:1)

(17)

ij. The

which includes a correction term ∇θ
practical implementation of gSGRHMC is outlined in Algorithm 1.

∂
∂θj

j

4 Experiments

In Sec. 4.1  we show that gSGRHMC can excel at rapidly exploring distributions with complex
landscapes. We then apply SGRHMC to sampling in a latent Dirichlet allocation (LDA) model on
a large Wikipedia dataset in Sec. 4.2. The Supplement contains details on the speciﬁc samplers
considered and the parameter settings used in these experiments.

4.1 Synthetic Experiments
In this section we aim to empirically (i) validate the correctness of our recipe and (ii) assess the
effectiveness of gSGRHMC. In Fig. 2(left)  we consider two univariate distributions (shown in the
Supplement) and compare SGLD  SGHMC  the na¨ıve state-adaptive SGHMC of Eq. (16)  and our
proposed gSGRHMC of Eq. (17). See the Supplement for the form of G(θ). As expected  the na¨ıve
implementation does not converge to the target distribution. In contrast  the gSGRHMC algorithm
obtained via our recipe indeed has the correct invariant distribution and efﬁciently explores the dis-
tributions. In the second experiment  we sample a bivariate distribution with strong correlation. The
results are shown in Fig. 2(right). The comparison between SGLD  SGHMC  and our gSGRHMC
method shows that both a state-dependent preconditioner and Hamiltonian dynamics help to make
the sampler more efﬁcient than either element on its own.

7

12SGLD12SGHMC12NaivegSGRHMC12gSGRHMC0.0000.0050.0100.0150.020K-LDivergence024681000.511.522.5log3(Steps/100)+1K−L Divergence SGLDSGHMCgSGRHMCParameter θ
Prior p(θ)

Method
SGLD
SGHMC
SGRLD
SGRHMC

Original LDA Expanded Mean
βkw = θkw

βkw = θkw(cid:80)

w θkw

p(θk) = Dir(α) p(θkw) = Γ(α  1)

Average Runtime per 100 Docs

0.778s
0.815s
0.730s
0.806s

Figure 3: Upper Left: Expanded mean parameterization of the LDA model. Lower Left: Average runtime per
100 Wikipedia entries for all methods. Right: Perplexity versus number of Wikipedia entries processed.

4.2 Online Latent Dirichlet Allocation
We also applied SGRHMC (with G(θ) = diag(θ)−1  the Fisher information metric) to an online
latent Dirichlet allocation (LDA) [5] analysis of topics present in Wikipedia entries. In LDA  each
topic is associated with a distribution over words  with βkw the probability of word w under topic k.
Each document is comprised of a mixture of topics  with π(d)
the probability of topic k in document
j ∼ π(d) for the jth word and then drawing
d. Documents are generated by ﬁrst selecting a topic z(d)
the speciﬁc word from the topic as x(d)
. Typically  π(d) and βk are given Dirichlet priors.

k

j ∼ βz(d)

j

The goal of our analysis here is inference of the corpus-wide topic distributions βk. Since the
Wikipedia dataset is large and continually growing with new articles  it is not practical to carry out
this task over the whole dataset. Instead  we scrape the corpus from Wikipedia in a streaming man-
ner and sample parameters based on minibatches of data. Following the approach in [13]  we ﬁrst
analytically marginalize the document distributions π(d) and  to resolve the boundary issue posed by
the Dirichlet posterior of βk deﬁned on the probability simplex  use an expanded mean parameter-
ization shown in Figure 3(upper left). Under this parameterization  we then compute ∇ log p(θ|x)
and  in our implementation  use boundary reﬂection to ensure the positivity of parameters θkw. The
necessary expectation over word-speciﬁc topic indicators z(d)
is approximated using Gibbs sampling
separately on each document  as in [13]. The Supplement contains further details.
For all the methods  we report results of three random runs. When sampling distributions with
mass concentrated over small regions  as in this application  it is important to incorporate geometric
information via a Riemannian sampler [13]. The results in Fig. 3(right) indeed demonstrate the im-
portance of Riemannian variants of the stochastic gradient samplers. However  there also appears to
be some beneﬁts gained from the incorporation of the HMC term for both the Riemmannian and non-
Reimannian samplers. The average runtime for the different methods are similar (see Fig. 3(lower
left)) since the main computational bottleneck is the gradient evaluation. Overall  this application
serves as an important example of where our newly proposed sampler can have impact.

j

5 Conclusion
We presented a general recipe for devising MCMC samplers based on continuous Markov process-
es. Our framework constructs an SDE speciﬁed by two matrices  a positive semideﬁnite D(z) and a
skew-symmetric Q(z). We prove that for any D(z) and Q(z)  we can devise a continuous Markov
process with a speciﬁed stationary distribution. We also prove that for any continuous Markov pro-
cess with the target stationary distribution  there exists a D(z) and Q(z) that cast the process in our
framework. Our recipe is particularly useful in the more challenging case of devising stochastic gra-
dient MCMC samplers. We demonstrate the utility of our recipe in “reinventing” previous stochastic
gradient MCMC samplers  and in proposing our SGRHMC method. The efﬁciency and scalability
of the SGRHMC method was shown on simulated data and a streaming Wikipedia analysis.

Acknowledgments
This work was supported in part by ONR Grant N00014-10-1-0746  NSF CAREER Award IIS-1350133  and
the TerraSwarm Research Center sponsored by MARCO and DARPA. We also thank Mr. Lei Wu for helping
with the proof of Theorem 2 and Professors Ping Ao and Hong Qian for many discussions.

8

0200040006000800010000100015002000250030003500Number of DocumentsPerplexity SGLDSGHMCSGRLDSGRHMCReferences
[1] S. Ahn  A. Korattikara  and M. Welling. Bayesian posterior sampling via stochastic gradient
In Proceedings of the 29th International Conference on Machine Learning

Fisher scoring.
(ICML’12)  2012.

[2] S. Ahn  B. Shahbaba  and M. Welling. Distributed stochastic gradient MCMC. In Proceeding

of 31st International Conference on Machine Learning (ICML’14)  2014.

[3] R. Bardenet  A. Doucet  and C. Holmes. Towards scaling up Markov chain Monte Carlo:
An adaptive subsampling approach. In Proceedings of the 30th International Conference on
Machine Learning (ICML’14)  2014.

[4] M. Betancourt. The fundamental incompatibility of scalable Hamiltonian Monte Carlo and
In Proceedings of the 31th International Conference on Machine

naive data subsampling.
Learning (ICML’15)  2015.

[5] D.M. Blei  A.Y. Ng  and M.I. Jordan. Latent dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  March 2003.

[6] T. Chen  E.B. Fox  and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Pro-

ceeding of 31st International Conference on Machine Learning (ICML’14)  2014.

[7] N. Ding  Y. Fang  R. Babbush  C. Chen  R.D. Skeel  and H. Neven. Bayesian sampling using
In Advances in Neural Information Processing Systems 27

stochastic gradient thermostats.
(NIPS’14). 2014.

[8] S. Duane  A.D. Kennedy  B.J. Pendleton  and D. Roweth. Hybrid Monte Carlo. Physics Letters

B  195(2):216 – 222  1987.

[9] W. Feller. Introduction to Probability Theory and its Applications. John Wiley & Sons  1950.
[10] M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo

methods. Journal of the Royal Statistical Society Series B  73(2):123–214  03 2011.

[11] A. Korattikara  Y. Chen  and M. Welling. Austerity in MCMC land: Cutting the Metropolis-
Hastings budget. In Proceedings of the 30th International Conference on Machine Learning
(ICML’14)  2014.

[12] R.M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo 

54:113–162  2010.

[13] S. Patterson and Y.W. Teh. Stochastic gradient Riemannian Langevin dynamics on the proba-

bility simplex. In Advances in Neural Information Processing Systems 26 (NIPS’13). 2013.

[14] H. Risken and T. Frank. The Fokker-Planck Equation: Methods of Solutions and Applications.

Springer  1996.

[15] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical

Statistics  22(3):400–407  09 1951.

[16] J. Shi  T. Chen  R. Yuan  B. Yuan  and P. Ao. Relation of a new interpretation of stochastic

differential equations to Itˆo process. Journal of Statistical Physics  148(3):579–590  2012.

[17] M. Welling and Y.W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning (ICML’11)  pages
681–688  June 2011.

[18] T. Xifara  C. Sherlock  S. Livingstone  S. Byrne  and M. Girolami. Langevin diffusions and
the Metropolis-adjusted Langevin algorithm. Statistics & Probability Letters  91:14–19  2014.
[19] L. Yin and P. Ao. Existence and construction of dynamical potential in nonequilibrium process-
es without detailed balance. Journal of Physics A: Mathematical and General  39(27):8593 
2006.

[20] R. Zwanzig. Nonequilibrium Statistical Mechanics. Oxford University Press  2001.

9

,Yarin Gal
Mark van der Wilk
Carl Edward Rasmussen
Yi-An Ma
Tianqi Chen
Emily Fox