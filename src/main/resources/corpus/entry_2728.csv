2013,It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals,Multi-task prediction models are widely being used to couple regressors or classification models by sharing information across related tasks. A common pitfall of these models is that they assume that the output tasks are independent conditioned on the inputs. Here  we propose a multi-task Gaussian process approach to model both the relatedness between regressors as well as the task correlations in the residuals  in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term that is the sum of Kronecker products  for which efficient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics  we find substantial benefits of modeling structured noise compared to established alternatives.,It is all in the noise: Efﬁcient multi-task Gaussian

process inference with structured residuals

Machine Learning and Computational Biology

Barbara Rakitsch

Research Group

Max Planck Institutes T¨ubingen  Germany

rakitsch@tuebingen.mpg.de

Christoph Lippert
Microsoft Research
Los Angeles  USA

lippert@microsoft.com

Karsten Borgwardt1 2

Machine Learning and Computational Biology

Research Group

Oliver Stegle2

European Molecular Biology Laboratory

European Bioinformatics Institute

Max Planck Institutes T¨ubingen  Germany

Cambridge  UK

karsten.borgwardt@tuebingen.mpg.de

oliver.stegle@ebi.ac.uk

Abstract

Multi-task prediction methods are widely used to couple regressors or classiﬁca-
tion models by sharing information across related tasks. We propose a multi-task
Gaussian process approach for modeling both the relatedness between regressors
and the task correlations in the residuals  in order to more accurately identify true
sharing between regressors. The resulting Gaussian model has a covariance term
in form of a sum of Kronecker products  for which efﬁcient parameter inference
and out of sample prediction are feasible. On both synthetic examples and applica-
tions to phenotype prediction in genetics  we ﬁnd substantial beneﬁts of modeling
structured noise compared to established alternatives.

1

Introduction

Multi-task Gaussian process (GP) models are widely used to couple related tasks or functions for
joint regression. This coupling is achieved by designing a structured covariance function  yielding
a prior on vector-valued functions. An important class of structured covariance functions can
be derived from a product of a kernel function c relating the tasks (task covariance) and a kernel
function r relation the samples (sample covariance)

cov(fn t  fn(cid:48) t(cid:48)) = c(t  t(cid:48))

·

(cid:124) (cid:123)(cid:122) (cid:125)

r(n  n(cid:48))

(cid:124) (cid:123)(cid:122) (cid:125)

task covariance

sample covariance

 

(1)

where fn t are latent function values that induce the outputs yn t by adding some Gaussian noise.
If the outputs yn t are fully observed  with one training example per sample and task  the resulting
covariance matrix between the latent factors can be written as a Kronecker product between the
sample covariance matrix and the task covariance matrix (e.g. [1]). More complex multi-task
covariance structures can be derived from generalizations of this product structure  for example via
convolution of multiple features  e.g. [2]. In [3]  a parameterized covariance over the tasks is used 
assuming that task-relevant features are observed. The authors of [4] couple the latent features over
the tasks exploiting a dependency in neural population activity over time.

1Also at Zentrum f¨ur Bioinformatik  Eberhard Karls Universit¨at T¨ubingen T¨ubingen  Germany
2Both authors contributed equally to this work.

1

Work proposing this type of multi-task GP regression builds on Bonilla and Williams [1]  who have
emphasized that the power of Kronecker covariance models for GP models (Eqn. (1)) is linked to
non-zero observation noise. In fact  in the limit of noise-free training observations  the coupling of
tasks for predictions is lost in the predictive model  reducing to ordinary GP regressors for each indi-
vidual task. Most multi task GP models build on a simple independent noise model  an assumption
that is mainly routed in computational convenience. For example [5] show that this assumption ren-
ders the evaluation of the model likelihood and parameter gradients tractable  avoiding the explicit
evaluation of the Kronecker covariance.
In this paper  we account for residual noise structure by modeling the signal and the noise covari-
ance matrix as two separate Kronecker products. The structured noise covariance is independent
of the inputs but instead allows to capture residual correlation between tasks due to latent causes;
moreover  the model is simple and extends the widely used product covariance structure. Concep-
tually related noise models have been proposed in animal breeding [6  7]. In geostatistics [8]  linear
coregionalization models have been introduced to allow for more complicated covariance structures:
the signal covariance matrix is modeled as a sum of Kronecker products and the noise covariance
as a single Kronecker product. In machine learning  the Gaussian process regression networks [9]
considers an adaptive mixture of GPs to model related tasks. The mixing coefﬁcients are dependent
on the input signal and control the signal and noise correlation simultaneously.
The remainder of this paper is structured as follows. First  we show that unobserved regressors or
causal processes inevitably lead to correlated residual  motivating the need to account for structured
noise (Section 2). This extension of the multi task GP model allows for more accurate estimation
of the task-task relationships  thereby improving the performance for out-of-sample predictions. At
the same time  we show how an efﬁcient inference scheme can be derived for this class of models.
The proposed implementation handles closed form marginal likelihoods and parameter gradients for
matrix-variate normal models with a covariance structure represented by the sum of two Kronecker
products. These operations can be implemented at marginal extra computational cost compared to
models that ignore residual task correlations (Section 3). In contrast to existing work extending
Gaussian process multi task models by deﬁning more complex covariance structures [2  9  8]  our
model utilizes the gradient of the marginal likelihood for parameter estimation and does not require
expected maximization  variational approximation or MCMC sampling. We apply the resulting
model in simulations and real settings  showing that correlated residuals are a concern in important
applications (Section 4).

this matrix corresponds to a particular task t is denoted as yt  and vecY =(cid:0)y(cid:62)

2 Multi-task Gaussian processes with structured noise
Let Y ∈ RN×T denote the N × T output training matrix for N samples and T tasks. A column of
denotes
the vector obtained by vertical concatenation of all columns of Y. We indicate the dimensions of the
matrix as capital subscripts when needed for clarity. A more thoughtful derivation of all equations
can be found in the Supplementary Material.

(cid:1)(cid:62)

1 . . . y(cid:62)

T

Multivariate linear model equivalence The multi-task Gaussian process regression model with
structured noise can be derived from the perspective of a linear multivariate generative model. For
a particular task t  the outputs are determined by a linear function of the training inputs across F
features S = {s1  . . .   sF} 

F(cid:88)

f =1

yt =

sf wf t + ψt.

(2)

Multi-task sharing is achieved by specifying a multivariate normal prior across tasks  both for the
regression weights wf t and the noise variances ψt:

F(cid:89)

N(cid:89)

N (ψn | 0  ΣT T ) .

p(W(cid:62)) =

N (wf | 0  CT T )

p(Ψ(cid:62)) =

f =1

n=1

2

Marginalizing out the weights W and the residuals Ψ results in a matrix-variate normal model with
sum of Kronecker products covariance structure

vecYN T | 0  CT T ⊗ RN N
(cid:125)
(cid:123)(cid:122)

(cid:124)

  
(cid:125)

(3)

p(vecY | C  R  Σ) = N

+ ΣT T ⊗ IN N

(cid:124)

(cid:123)(cid:122)

signal covariance

noise covariance

where RN N = SS(cid:62) is the sample covariance matrix that results from the marginalization over
the weights W in Eqn. (2). In the following  we will refer to a Gaussian process model with this
type of sum of Kronecker products covariance structure as GP-kronsum1. As common to any kernel
method  the linear covariance R can be replaced with any positive semi-deﬁnite covariance function.

Predictive distribution In a GP-kronsum model  predictions for unseen test instances can be car-
ried out by using the standard Gaussian process framework [10]:

p(vecY∗|R∗  Y) = N (vecY∗ | vec M∗  V∗) .

(4)
Here  M∗ denotes the mean prediction and V∗ is the predictive covariance. Analytical expression
for both can be obtained by considering the joint distribution of observed and unobserved outputs
and completing the square  yielding:
vec M∗ = (CT T ⊗ R∗
V∗ = (CT T ⊗ R∗

N∗N ) (CT T ⊗ RN N + ΣT T ⊗ IN N )
N∗N∗ ) − (CT T ⊗ R∗

N∗N ) (CT T ⊗ RN N + ΣT T ⊗ IN N )

−1 (CT T ⊗ R∗

−1 vecYN T  

N N∗ )  

where R∗
covariance matrix between the test samples.

N∗N is the covariance matrix between the test and training instances  and R∗

N∗N∗ is the

Design of multi-task covariance function In practice  neither the form of C nor the form of Σ is
known a priori and hence needs to be inferred from data  ﬁtting a set of corresponding covariance
parameters θC and θΣ. If the number of tasks T is large  learning a free-form covariance matrix is
prone to overﬁtting  as the number of free parameters grows quadratically with T . In the experi-

ments  we consider a rank-k approximation of the form(cid:80)K

k + σ2I for the task matrices.

k=1 xkx(cid:62)

Task cancellation when the task covariance matrices are equal A notable form of the predictive
distribution (4) arises for the special case C = Σ  that is the task covariance matrix of signal
and noise are identical. Similar to previous results for noise-free observations [1]  maximizing the
marginal likelihood p(vecY|C  R  Σ) with respect to the parameters θR becomes independent of C
and the predictions are decoupled across tasks  i.e. the beneﬁts from joint modeling are lost:

(5)
In this case  the predictions depend on the sample covariance  but not on the task covariance. Thus 
the GP-kronsum model is most useful when the task covariances on observed features and on noise
reﬂect two independent sharing structures.

N∗N (RN N + IN N )−1YN T

vec M∗ = vec(cid:0)R∗

(cid:1)

3 Efﬁcient Inference

In general  efﬁcient inference can be carried out for Gaussian models with a sum covariance of two
arbitrary Kronecker products

p(vecY | C  R  Σ) = N (vecY | 0  CT T ⊗ RN N + ΣT T ⊗ ΩN N ) .

(6)
The key idea is to ﬁrst consider a suitable data transformation that leads to a diagonalization of all
covariance matrices and second to exploit Kronecker tricks whenever possible.
Let Σ = UΣSΣU(cid:62)
Σ be the eigenvalue decomposition of Σ  and analogously for Ω. Borrowing
ideas from [11]  we can ﬁrst bring the covariance matrix in a more amenable form by factoring out
the structured noise:

1the covariance is deﬁned as the sum of two Kronecker products and not as the classical Kronecker sum

C ⊕ R = C ⊗ I + I ⊗ R.

3

(cid:16)

K = C ⊗ R + Σ ⊗ Ω
Σ ⊗ UΩS
− 1
Σ and ˜R = S

=
− 1
Σ U(cid:62)

UΣS

1
2
Ω

1
2

2

(cid:17)(cid:16) ˜C ⊗ ˜R + I ⊗ I
(cid:17)(cid:16)

− 1
Ω U(cid:62)
where ˜C = S
˜K = ˜C ⊗ ˜R + I ⊗ I for this transformed covariance.

Σ CUΣS

2

2

Ω RUΩS

(cid:17)

 

(7)

1
2

ΣU(cid:62)
S

Σ ⊗ S

1
2

ΩU(cid:62)

Ω

− 1
Ω . In the following  we use deﬁnition

2

Efﬁcient log likelihood evaluation. The log model likelihood (Eqn. (6)) can be expressed in terms
of the transformed covariance ˜K:

vec ˜Y(cid:62) ˜K−1vec ˜Y 

(8)

L = − N T
2
= − N T
2
− 1
Σ ⊗ S
Σ U(cid:62)
S

ln(2π) − 1
2
ln(2π) − 1
2
− 1
Ω U(cid:62)

(cid:16)

Ω

2

(cid:17)

ln|K| − 1
2
ln| ˜K| − 1
2

vecY(cid:62)K−1vecY
|SΣ ⊗ SΩ| − 1
2

(cid:16)

(cid:17)

2

where vec ˜Y =
is the projected output.
Except for the additional term |SΣ ⊗ SΩ|  resulting from the transformation  the log likelihood has
the exactly same form as for multi-task GP regression with iid noise [1  5]. Using an analogous
derivation  we can now efﬁciently evaluate the log likelihood:

vecY = vec

2

− 1
Ω UT
S

− 1
ΩYUΣS
Σ

2

L = − N T
2

(cid:16)
where we have deﬁned the eigenvalue decomposition of ˜C as U ˜CS ˜CU(cid:62)

(cid:17)(cid:62)
ln|S ˜C ⊗ S ˜R + I ⊗ I| − N
2
−1 vec

ln(2π) − 1
2
U(cid:62)
˜YU ˜C
˜R

(S ˜C ⊗ S ˜R + I ⊗ I)

− 1
2

(cid:16)

ln|SΣ| − T
2
˜YU ˜C

U(cid:62)
˜R

vec

|SΩ|

(cid:17)

and similar for ˜R.

˜C

 

(9)

Efﬁcient gradient evaluation The derivative of the log marginal likelihood with respect to a co-
variance parameter θR can be expressed as:

∂

∂θR

diag

∂

∂θR

L = − 1
2
= − 1
2
1
2

+

vec( ˆY)(cid:62)vec

(cid:16)

ln| ˜K| − 1
2

vec ˜Y(cid:62)(cid:18) ∂
−1(cid:17)(cid:62)
(cid:19)
(cid:18) ∂
(S ˜C ⊗ S ˜R + I ⊗ I)
(cid:16)

U(cid:62)
˜R

(cid:18)

∂θR

˜R

∂θR
U(cid:62)
˜R

(cid:19)

(cid:18)

˜K−1

vec( ˜Y)

(cid:19)
S ˜C ⊗ U(cid:62)

˜R

diag

(cid:17)

U ˜R

ˆYS ˜C

 

(cid:18) ∂

∂θR

(cid:19)

˜R

(cid:19)

U ˜R

(10)

where vec( ˆY) = (S ˜C ⊗ S ˜R + I ⊗ I)
. Analogous gradients can be derived for
the task covariance parameters θC and θΣ. The proposed speed-ups also apply to the special cases
where Σ is modeled as being diagonal as in [1]  or for optimizing the parameters of a kernel function.
Since the sum of Kronecker products generally can not be written as a single Kronecker product  the
speed-ups cannot be generalized to larger sums of Kronecker products.

˜YU ˜C

−1 vec

Efﬁcient prediction Similarly  the mean predictor (Eqn. (4)) can be efﬁciently evaluated

vec M∗ = vec

R∗UΩS

− 1
Ω

2

ˆYU(cid:62)
˜C

U ˜R

− 1
Σ U(cid:62)
S

2

.

(11)

(cid:104)(cid:16)

(cid:17)(cid:16)

(cid:17)(cid:16)

Σ C(cid:62)(cid:17)(cid:105)

Gradient-based parameter inference The closed-form expression of the marginal likelihood
(Eqn. (9)) and gradients with respect to covariance parameters (Eqn. (10)) allow for use of gradient-
based parameter inference. In the experiments  we employ a variant of L-BFGS-B [12].
Computational cost. While the naive approach has a runtime of O(N 3· T 3) and memory require-
ment of O(N 2 · T 2)  as it explicitly computes and inverts the Kronecker products  our reformulation
reduces the runtime to O(N 3 + T 3) and the memory requirement to O(N 2 + T 2)  making it appli-
cable to large numbers of samples and tasks. The empirical runtime savings over the naive approach
are explored in Section 4.1.

4

Figure 1: Runtime comparison on syn-
thetic data. We compare our efﬁcient GP-
kronsum implementation (left) versus its
naive counterpart (right). Shown is the run-
time in seconds on a logarithmic scale as a
function of the sample size and the number
of tasks. The optimization was stopped pre-
maturely if it did not complete after 104 sec-
onds.

(a) Efﬁcient Implementation (b) Naive Implementation
4 Experiments

We investigated the performance of the proposed GP-kronsum model in both simulated datasets and
response prediction problems in statistical genetics. To investigate the beneﬁts of structured residual
covariances  we compared the GP-kronsum model to a Gaussian process (GP-kronprod) with iid
noise [5] as well as independent modeling of tasks using a standard Gaussian process (GP-single) 
and joint modeling of all tasks using a standard Gaussian on a pooled dataset  naively merging data
from all tasks (GP-pool).
The predictive performance of individual models was assessed through 10-fold cross-validation.
For each fold  model parameters were ﬁt on the training data only. To avoid local optima during
training  parameter ﬁtting was carried out using ﬁve random restarts of the parameters on 90% of
the training instances. The remaining 10% of the training instances were used for out of sample
selection using the maximum log likelihood as criterion. Unless stated otherwise  in the multi-task
models the relationship between tasks was parameterized as xx(cid:62) + σ2I  the sum of a rank-1 matrix
and a constant diagonal component. Both parameters  x and σ2  were learnt by optimizing the
marginal likelihood. Finally  we measured the predictive performance of the different methods via
the averaged square of Pearson’s correlation coefﬁcient r2 between the true and the predicted output 
averaged over tasks. The squared correlation coefﬁcient is commonly used in statistical genetics to
evaluate the performance of different predictors [13].

4.1 Simulations

First  we considered simulated experiments to explore the runtime behavior and to ﬁnd out if there
are settings in which GP-kronsum performs better than existing methods.

Runtime evaluation. As a ﬁrst experiment  we examined the runtime behavior of our method as
a function of the number of samples and of the number of tasks. Both parameters were varied in
the range {16  32  64  128  256}. The simulated dataset was drawn from the GP-kronsum model
(Eqn. (3)) using a linear kernel for the sample covariance matrix R and rank-1 matrices for the task
covariances C and Σ. The runtime of this model was assessed for a single likelihood optimization on
an AMD Opteron Processor 6 378 using a single core (2.4GHz  2 048 KB Cache  512 GB Memory)
and compared to a naive implementation. The optimization was stopped prematurely if it did not
converge within 104 seconds.
In the experiments  we used a standard linear kernel on the features of the samples as sample covari-
ance while learning the task covariances. This modeling choice results in a steeper runtime increase
with the number of tasks  due to the increasing number of model parameters to be estimated. Fig-
ure 1 demonstrates the signiﬁcant speed-up. While our algorithm can handle 256 samples/256 tasks
with ease  the naive implementation failed to process more than 32 samples/32 tasks.

Unobserved causal process induces structured noise A common source of structured residuals
are unobserved causal processes that are not captured via the inputs. To explore this setting  we
generated simulated outputs from a sum of two different processes. For one of the processes  we
assumed that the causal features Xobs were observed  whereas for the second process the causal
features Xhidden were hidden and independent of the observed measurements. Both processes were
simulated to have a linear effect on the output. The effect from the observed features was again
divided up into an independent effect  which is task-speciﬁc  and a common effect  which  up to

5

rescaling rcommon  is shared over all tasks:
Ycommon = XobsWcommon  Wcommon = rcommon ⊗ wcommon  rcommon ∼ N (0  I)  wcommon ∼ N (0  I)
The trade-off parameter µcommon determines the extent of relatedness between tasks:

Yobs = µcommonYcommon + (1 − µcommon)Yind.

The effect of the hidden features was simulated analogously. A second trade-off parameter µhidden
was introduced  controlling the ratio between the observed and hidden effect:

Y = µsignal [(1 − µhidden)Yobs + µhiddenYhidden] + (1 − µsignal)Ynoise 

where Ynoise is Gaussian observation noise  and µsignal is a third trade-off parameter deﬁning the
ratio between noise and signal.
trade-off parameters  we considered a series of
To investigate the impact of the different
datasets varying one of the parameters while keeping others ﬁxed. We varied µsignal in the
range {0.1  0.3  0.5  0.7  0.9  1.0}  µcommon ∈ {0.0  0.1  0.3  0.5  0.7  0.9  1.0} and µhidden ∈
{0.0  0.1  0.3  0.5  0.7  0.9  1.0}  with default values marked in bold. Note that the best possible
explained variance for the default setting is 45%  as the causal signal is split up equally between
the observed and the hidden process. For all simulation experiments  we created datasets with 200
samples and 10 tasks. The number of observed features was set to 200  as well as the number of
hidden features. For each such simulation setting  we created 30 datasets.
First  we considered the impact of variation in signal strength µsignal (Figure 2a)  where the overall
signal was divided up equally between the observed and hidden signal. Both GP-single and GP-
kronsum performed better as the overall signal strength increased. The performance of GP-kronsum
was superior  as the model can exploit the relatedness between the different tasks.
Second  we explored the ability of the different methods to cope with an underlying hidden pro-
cess (Figure 2b). In the absence of a hidden process (µhidden = 0)  GP-kronprod and GP-kronsum
had very similar performances  as both methods leverage the shared signal of the observed pro-
cess  thereby outperforming the single-task GPs. However  as the magnitude of the hidden signal
increases  GP-kronprod falsely explains the task correlation completely by the covariance term rep-
resenting the observed process which leads to loss of predictive power.
Last  we examined the ability of different methods to exploit the relatedness between the tasks (Fig-
ure 2c). Since GP-single assumed independent tasks  the model performed very similarly across
the full range of common signal. GP-kronprod suffered from the same limitations as previously de-
scribed  because the correlation between tasks in the hidden process increases synchronously with
the correlation in the observed process as µcommon increases. In contrast  GP-kronsum could take
advantage of the shared component between the tasks  as knowledge is transferred between them.
GP-pool was consistently outperformed by all competitors as two of its main assumptions are heav-
ily violated: samples of different tasks do not share the same signal and the residuals are neither
independent of each other  nor do they have the same noise level.
In summary  the proposed model is robust to a range of different settings and clearly outperforms its
competitors when the tasks are related to each other and not all causal processes are observed.

4.2 Applications to phenotype prediction

As a real world application we considered phenotype prediction in statistical genetics. The aim of
these experiments was to demonstrate the relevance of unobserved causes in real world prediction
problems and hence warrant greater attention.

Gene expression prediction in yeast We considered gene expression levels from a yeast genet-
ics study [14]. The dataset comprised of gene expression levels of 5  493 genes and 2  956 SNPs
(features)  measured for 109 yeast crosses. Expression levels for each cross were measured in two
conditions (glucose and ethanol as carbon source)  yielding a total of 218 samples. In this experi-
ment  we treated the condition information as a hidden factor instead of regressing it out  which is
analogous to the hidden process in the simulation experiments. The goal of this experiment was to
investigate how alternative methods can deal and correct for this hidden covariate. We normalized
all features and all tasks to zero mean and unit variance. Subsequently  we ﬁltered out all genes
that were not consistently expressed in at least 90% of the samples (z-score cutoff 1.5). We also

6

(a) Total Signal

(b) Hidden Signal

(c) Shared Signal

Figure 2: Evaluation of alternative methods for different simulation settings. From left to right:
(a) Evaluation for varying signal strength. (b) Evaluation for variable impact of the hidden signal.
(c) Evaluation for different strength of relatedness between the tasks. In each simulation setting  all
other parameters were kept constant at default parameters marked with the yellow star symbol.

(a) Empirical

(b) Signal

(c) Noise

Figure 3: Fitted task covariance matrices for gene expression levels in yeast. From left to right:
(a) Empirical covariance matrix of the gene expression levels. (b) Signal covariance matrix learnt
by GP-kronsum. (c) Noise covariance matrix learnt by GP-kronsum. The ordering of the tasks was
determined using hierarchical clustering on the empirical covariance matrix.

discarded genes with low signal (< 10% of the variance) or were close to noise free (> 90% of the
variance)  reducing the number of genes to 123  which we considered as tasks in our experiment.
The signal strength was estimated by a univariate GP model. We used a linear kernel calculated on
the SNP features for the sample covariance.
Figure 3 shows the empirical covariance and the learnt task covariances by GP-kronsum. Both learnt
covariances are highly structured  demonstrating that the assumption of iid noise in the GP-kronprod
model is violated in this dataset. While the signal task covariance matrix reﬂects genetic signals that
are shared between the gene expression levels  the noise covariance matrix mainly captures the
mean shift between the two conditions the gene expression levels were measured in (Figure 4). To
investigate the robustness of the reconstructed latent factor  we repeated the training 10 times. The
mean latent factors and its standard errors were 0.2103 ± 0.0088 (averaged over factors  over the 10
best runs selected by out-of-sample likelihood)  demonstrating robustness of the inference.
When considering alternative methods for out of sample prediction  the proposed Kronecker Sum
model (r2(GP-kronsum)=0.3322± 0.0014) performed signiﬁcantly better than previous approaches
(r2(GP-pool)=0.0673 ± 0.0004  r2(GP-single)=0.2594 ± 0.0011  r2(GP-kronprod)=0.1820 ±
0.0020). The results are averages over 10 runs and ± denotes the corresponding standard errors.

Multi-phenotype prediction in Arabidopsis thaliana. As a second dataset  we considered a
genome-wide association study in Arabidopsis thaliana [15] to assess the prediction of develop-
mental phenotypes from genomic data. This dataset consisted of 147 samples and 216 130 single
nucleotide polymorphisms (SNPs  here used as features). As different tasks  we considered the phe-
notypes ﬂowering period duration  life cycle period  maturation period and reproduction period.
To avoid outliers and issues due to non-Gaussianity  we preprocessed the phenotypic data by ﬁrst
converting it to ranks and squashing the ranks through the inverse cumulative Gaussian distribution.
The SNPs in Arabidopsis thaliana are binary and we discarded features with a frequency of less

7

Figure 4: Correlation between the mean
difference of the two conditions and the
latent factors on the yeast dataset. Shown
is the strength of the latent factor of the sig-
nal (left) and the noise (right) task covari-
ance matrix as a function of the mean dif-
ference between the two environmental con-
ditions. Each dot corresponds to one gene
expression level.

(a) Signal

(b) Noise

than 10% in all samples  resulting in 176 436 SNPs. Subsequently  we normalized the features to
zero mean and unit variance. Again  we used a linear kernel on the SNPs as sample covariance.
Since the causal processes in Arabidopsis thaliana are complex  we allowed the rank of the signal
and noise matrix to vary between 1 and 3. The appropriate rank complexity was selected on the 10%
hold out data of the training fold. We considered the average squared correlation coefﬁcient on the
holdout fraction of the training data to select the model for prediction on the test dataset. Notably 
for GP-kronprod  the selected task complexity was rank(C) = 3  whereas GP-kronsum selected
a simpler structure for the signal task covariance (rank(C) = 1) and chose a more complex noise
covariance  rank(Σ) = 2.
The cross validation prediction performance of each model is shown in Table 1. For reproduction
period  GP-single is outperformed by all other methods. For the phenotype life cycle period  the
noise estimates of the univariate GP model were close to zero  and hence all methods  except of
GP-pool  performed equally well since the measurements of the other phenotypes do not provide
additional information. For maturation period  GP-kronsum and GP-kronprod showed improved
performance compared to GP-single and GP-pool. For ﬂowering period duration  GP-kronsum
outperformed its competitors.

Reproduction
Flowering period
period
duration
0.0478 ± 0.0013
0.0502 ± 0.0025
GP-pool
0.0272 ± 0.0024
0.0385 ± 0.0017
GP-single
0.0492 ± 0.0032
0.0846 ± 0.0021
GP-kronprod
0.0501 ± 0.0033
GP-kronsum 0.1127 ± 0.0049
Table 1: Predictive performance of the different methods on the Arabidopsis thaliana dataset.
Shown is the squared correlation coefﬁcient and its standard error (measured by repeating 10-fold
cross-validation 10 times).

Life cycle
period
0.1038 ± 0.0034
0.3500 ± 0.0069
0.3417 ± 0.0062
0.3485 ± 0.0068

Maturation
period
0.0460 ± 0.0024
0.1612 ± 0.0027
0.1878 ± 0.0042
0.1918 ± 0.0041

5 Discussion and conclusions

Multi-task Gaussian process models are a widely used tool in many application domains  ranging
from the prediction of user preferences in collaborative ﬁltering to the prediction of phenotypes in
computational biology. Many of these prediction tasks are complex and important causal features
may remain unobserved or are not modeled. Nevertheless  most approaches in common usage as-
sume that the observation noise is independent between tasks. We here propose the GP-kronsum
model  which allows to efﬁciently model data where the noise is dependent between tasks by build-
ing on a sum of Kronecker products covariance.
In applications to statistical genetics  we have
demonstrated (1) the advantages of the dependent noise model over an independent noise model  as
well as (2) the feasibility of applying larger data sets by the efﬁcient learning algorithm.

Acknowledgement

We thank Francesco Paolo Casale for helpful discussions. OS was supported by an Marie Curie
FP7 fellowship. KB was supported by the Alfried Krupp Prize for Young University Teachers of the
Alfried Krupp von Bohlen und Halbach-Stiftung.

8

Corr(Glucose Ethanol)XCCorr(Glucose Ethanol)XSigmaReferences
[1] Edwin V. Bonilla  Kian Ming Adam Chai  and Christopher K. I. Williams. Multi-task gaussian

process prediction. In NIPS  2007.

[2] Mauricio A. ´Alvarez and Neil D. Lawrence. Sparse convolved gaussian processes for multi-

output regression. In NIPS  pages 57–64  2008.

[3] Edwin V. Bonilla  Felix V. Agakov  and Christopher K. I. Williams. Kernel multi-task learning

using task-speciﬁc features. In AISTATS  2007.

[4] Byron M. Yu  John P. Cunningham  Gopal Santhanam  Stephen I. Ryu  Krishna V. Shenoy  and
Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of
neural population activity. In NIPS  pages 1881–1888  2008.

[5] Oliver Stegle  Christoph Lippert  Joris M. Mooij  Neil D. Lawrence  and Karsten M. Borg-
In

wardt. Efﬁcient inference in matrix-variate gaussian models with iid observation noise.
NIPS  pages 630–638  2011.

[6] Karin Meyer. Estimating variances and covariances for multivariate animal models by re-

stricted maximum likelihood. Genetics Selection Evolution  23(1):67–83  1991.

[7] V Ducrocq and H Chapuis. Generalizing the use of the canonical transformation for the so-
lution of multivariate mixed model equations. Genetics Selection Evolution  29(2):205–224 
1997.

[8] Hao Zhang. Maximum-likelihood estimation for multivariate spatial linear coregionalization

models. Environmetrics  18(2):125–139  2007.

[9] Andrew Gordon Wilson  David A. Knowles  and Zoubin Ghahramani. Gaussian process re-

gression networks. In ICML  2012.

[10] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine

Learning (Adaptive Computation and Machine Learning). The MIT Press  2005.

[11] Alfredo A. Kalaitzis and Neil D. Lawrence. Residual components analysis. In ICML  2012.
[12] Ciyou Zhu  Richard H. Byrd  Peihuang Lu  and Jorge Nocedal. Algorithm 778: L-bfgs-b:
Fortran subroutines for large-scale bound-constrained optimization. ACM Trans. Math. Softw. 
23(4):550–560  December 1997.

[13] Ulrike Ober  Julien F. Ayroles  Eric A. Stone  Stephen Richards  and et al. Using Whole-
Genome Sequence Data to Predict Quantitative Trait Phenotypes in Drosophila melanogaster.
PLoS Genetics  8(5):e1002685+  May 2012.

[14] Erin N Smith and Leonid Kruglyak. Gene–environment interaction in yeast gene expression.

PLoS Biology  6(4):e83  2008.

[15] S. Atwell  Y. S. Huang  B. J. Vilhjalmsson  Willems  and et al. Genome-wide association study
of 107 phenotypes in Arabidopsis thaliana inbred lines. Nature  465(7298):627–631  Jun 2010.

9

,Barbara Rakitsch
Christoph Lippert
Karsten Borgwardt
Oliver Stegle