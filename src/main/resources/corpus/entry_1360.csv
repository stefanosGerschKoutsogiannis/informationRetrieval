2008,Deep Learning with Kernel Regularization for Visual Recognition,In this paper we focus on training deep neural networks for visual recognition tasks. One challenge is the lack of an informative regularization on the network parameters  to imply a meaningful control on the computed function. We propose a training strategy that takes advantage of kernel methods  where an existing kernel function represents useful prior knowledge about the learning task of interest. We derive an efficient algorithm using stochastic gradient descent  and demonstrate very positive results in a wide range of visual recognition tasks.,Deep Learning with Kernel Regularization

for Visual Recognition

Kai Yu

NEC Laboratories America  Cupertino  CA 95014  USA

{kyu  wx  ygong}@sv.nec-labs.com

Wei Xu

Yihong Gong

Abstract

In this paper we aim to train deep neural networks for rapid visual recognition.
The task is highly challenging  largely due to the lack of a meaningful regular-
izer on the functions realized by the networks. We propose a novel regularization
method that takes advantage of kernel methods  where an oracle kernel function
represents prior knowledge about the recognition task of interest. We derive an ef-
ﬁcient algorithm using stochastic gradient descent  and demonstrate encouraging
results on a wide range of recognition tasks  in terms of both accuracy and speed.

1 Introduction

Visual recognition remains a challenging task for machines. This difﬁculty stems from the large
pattern variations under which a recognition system must operate. The task is extremely easy for a
human  largely due to the expressive deep architecture employed by human visual cortex systems.
Deep neural networks (DNNs) are argued to have a greater capacity to recognize a larger variety of
visual patterns than shallow models  because they are considered biologically plausible.

However  training deep architectures is difﬁcult because the large number of parameters to be tuned
necessitates an enormous amount of labeled training data that is often unavailable. Several authors
have recently proposed training methods by using unlabeled data. These methods perform a greedy
layer-wise pre-training using unlabeled data  followed by a supervised ﬁne-tuning [9  4  15]. Even
though the strategy notably improves the performance  to date  the best reported recognition accu-
racy on popular benchmarks such as Caltech101 by deep models is still largely behind the results of
shallow models.

Beside using unlabeled data  in this paper we tackle the problem by leveraging additional prior
knowledge. In the last few decades  researchers have developed successful kernel-based systems
for a wide range of visual recognition tasks. Those sensibly-designed kernel functions provide
an extremely valuable source of prior knowledge  which we believe should be exploited in deep
learning. In this paper  we propose an informative kernel-based regularizer  which makes it possible
to train DNNs with prior knowledge about the recognition task.

Computationally  we propose to solve the learning problem using stochastic gradient descent (SGD) 
as it is the de facto method for neural network training. To this end we transform the kernel regu-
larizer into a loss function represented as a sum of costs by individual examples. This results in a
simple multi-task architecture where a number of extra nodes at the output layer are added to ﬁt a
set of auxiliary functions automatically constructed from the kernel function.

We apply the described method to train convolutional neural networks (CNNs) for a wide range of
visual recognition tasks  including handwritten digit recognition  gender classiﬁcation  ethnic origin
recognition  and object recognition. Overall our approach exhibits excellent accuracy and speed on
all of these tasks. Our results show that incorporation of prior knowledge can boost the performance
of CNNs by a large margin when the training set is small or the learning problem is difﬁcult.

1

2 DNNs with Kernel Regularization

In our setting  the learning model  a deep neural network (DNN)  aims to learn a predictive function
f : X → R that can achieve a low expected discrepancy E[‘(y  f(x))] over the distribution p(x  y).
In the simplest case Y = {−1  1} and ‘(· ·) is a differentiable hinge loss. Based on a set of labeled
examples [(xi  yi)]n

i=1  the learning is by minimizing a regularized loss

‘(cid:0)yi  β>

nX

i=1

(cid:1) + λkβ1k2

L(β  θ) =

1 φi + β0

(1)

where φi = φ(xi; θ) maps xi to q-dimensional hidden units via a nonlinear deep architecture
with parameters θ  including the connection weights and biases of all the intermediate layers 
β = {β1  β0}  β1 includes all the parameters of the transformation from the last hidden layer to
the output layer  β0 is a bias term  λ > 0  and kak2 = tr(a>a) is the usual weight decay reg-
ularization. Applying the well-known representor theorem  we derive the equivalence to a kernel
system1

nX

yi 

nX

 + λ

nX

L(α  β0  θ) =

‘

αjKi j + β0

αiαjKi j

(2)

where the kernel is computed by

i=1

j=1

i j=1

Ki j = hφ(xi; θ)  φ(xj; θ)i = φ>

i φj

We assume the network is provided with some prior knowledge  in the form of an m × m kernel
matrix Σ  computed on n labeled training data  plus possibly additional m−n unlabeled data if m >
n. We exploit this prior knowledge via imposing a kernel regularization on K(θ) = [Ki j]m
i j=1  such
that the learning problem seeks
Problem 2.1.

where γ > 0 and Ω(θ) is deﬁned by

min
β θ

L(β  θ) + γΩ(θ)

Ω(θ) = tr(cid:2)K(θ)−1Σ(cid:3) + log det[K(θ)]

(3)

(4)

This is a case of semi-supervised learning if m > n. Though Ω is non-convex w.r.t. K  it has a
unique minimum at K = Σ if Σ (cid:31) 0  suggesting that minimizing Ω(θ) encourages K to approach
Σ. The regularization can be explained from an information-theoretic perspective. Let p(f|K) and
p(f|Σ) be two Gaussian distributions N (0  K) and N (0  Σ).2 Ω(θ) is related to the KL-divergence
DKL[p(f|Σ)kp(f|K)]. Therefore  minimizing Ω(θ) forces the two distributions to be close. We
note that the regularization does not require Σ to be positive deﬁnite — it can be semideﬁnite.

3 Kernel Regularization via Stochastic Gradient Descent

The learning problem in Eq. (3) can be solved by using gradient-based methods. In this paper we
emphasize large-scale optimizations using stochastic gradient descent (SGD)  because the method
is fast when the size m of total data is large and backpropagation  a typical SGD  has been the de
facto method to train neural networks for large-scale learning tasks.

SGD considers the problem where the optimization cost is the sum of the local cost of each indi-
vidual training example. A standard batch gradient descent updates the model parameters by using
the true gradient summed over the whole training set  while SGD approximates the true gradient by
the gradient caused by a single random training example. Therefore  the parameters of the model

1In this paper we slightly abuse the notation  i.e.  we use L to denote different loss functions. However their

meanings should be uniquely identiﬁed by checking the input parameters.

2From a Gaussian process point of view  a kernel function deﬁnes the prior distribution of a function f  such

that the marginal distribution of the function values f on any ﬁnite set of inputs is a multivariate Gaussian.

2

are updated after each training example. For large data sets  SGD is often much faster than batch
gradient descent.

However  because the regularization term deﬁned by Eq. (4) does not consist of a cost function that
can be expressed as a sum (or an average) over data examples  SGD is not directly applicable. Our
idea is to transform the problem into an equivalent formulation that can be optimized stochastically.

3.1 Shrinkage on the Kernel Matrix

We consider a large-scale problem where the data size m may grow over time  while the size of the
last hidden layer (q) of the DNN is ﬁxed. Therefore the computed kernel K can be rank deﬁcient.
In order to ensure that the trace term in Ω(θ) is well-deﬁned  and that the log-determinant term is
bounded from below  we instead use K + δI to replace K in Ω(θ)  where δ > 0 is a small shrinkage
parameter and I is an identity matrix. Thus the log-determinant acts on a much smaller q×q matrix3

where Φ = [φ1  . . .   φm]> and const = (m − q) · log δ. Omitting all the irrelevant constants  we
then turn the kernel regularization into

log det(K + δI) = log det(cid:0)Φ>Φ + δI(cid:1) + const
Ω(θ) = tr(cid:2)(ΦΦ> + δI)−1Σ(cid:3) + log det(Φ>Φ + δI)

(5)

(6)

The kernel shrinkage not only remedies the ill-posedness  but also yields other conveniences in our
later development.

3.2 Transformation of the Log-determinant Term

By noticing that Φ>Φ =Pn

i=1 φiφ>

i

of the log determinant for the convenience of SGD.
Theorem 3.1. Consider minθ{L(θ) = h(θ) + g(a)}  where g(·) is concave and a ≡ a(θ) is a
function of θ  if its local minimum w.r.t. θ exists  then the problem is equivalent to

is a sum of quantities over data examples  we move it outside

(cid:8)L(θ  ψ) = h(θ) + a(θ)>ψ − g•(ψ)(cid:9)

min
θ ψ

where g•(ψ) is the conjugate function of g(a)  i.e. g•(ψ) = mina{ψ>a − g(a)}.4

Proof. For a concave function g(a)  the conjugate function of its conjugate function is itself 
i.e.  g(a) = minψ{a>ψ − g•(ψ)}. Since g•(ψ) is concave  a>ψ − g•(ψ) is convex w.r.t. ψ
and has the unique minimum g(a). Therefore minimizing L(θ  ψ) w.r.t. θ and ψ is equivalent to
minimizing L(θ) w.r.t. θ.

Since log-determinant is concave for q × q positive deﬁnite matrices A  the conjugate function
of log det(A) is log det(Ψ) + q. We can use the above theorem to transform any loss function
containing log det(A) into another loss  which is an upper bound and involves A in a linear term.
Therefore the log-determinant in Eq. (5) is turned into a variational representation

log det(cid:0)Φ>Φ + δI(cid:1) = min

Ψ∈S+

q

" mX

i=1

i Ψφi + δ · tr(Ψ) − log det(Ψ) + const
φ>

#

where Ψ ∈ S+
q is a q × q positive deﬁnite matrix  and const = −q. As we can see  the upper bound
is a convex function of auxiliary variables Ψ and more importantly  it amounts to a sum of local
quantities caused by each of the m data examples.

irrelevant to the variables of interest.

3Hereafter in this paper  with a slight abuse of notation  we use “const” in equations to summarize the terms
4If g(a) is convex  its conjugate function is g◦(ψ) = maxa{ψ>a − g(a)}.

3

3.3 Transformation of the Trace Term
We assume that the kernel matrix Σ is presented in a decomposed form Σ = U U>  with U =
[u1  . . .   um]>  ui ∈ Rp  and p ≤ m. We have found that the trace term can be cast as a variational
problem by introducing an q × p auxiliary variable matrix η.
Proposition 3.1. The trace term in Eq. (5) is equivalent to a convex variational representation

tr(cid:2)(ΦΦ> + δI)−1Σ(cid:3) = min

η∈Rq×p

" mX

i=1

#

k 1√
δ

ui − η>φik2 + δkηk2

F

Proof. We ﬁrst obtain the analytical solution η∗ = 1√
representation reaches its unique minimum. Then  plugging it back into the function  we have

(cid:20)1
(cid:21)
tr(cid:2)U>U − U>Φ(Φ>Φ + δI)−1Φ>U(cid:3) = tr(cid:2)(ΦΦ> + δI)−1U U>(cid:3)

U>Φη∗ + η∗>Φ>Φη∗ +

U>Φ(Φ>Φ + δI)−2Φ>U

(Φ>Φ + δI)−1Φ>U  where the variational

U>U − 2

1√
δ

1
δ

δ

tr

=

δ
1
δ

where the last step is derived by applying the Woodbury matrix identity.

Again  we note that the upper bound is a convex function of η  and consists of a sum of local costs
over data examples.

3.4 An Equivalent Learning Framework

Combining the previous results  we obtain the convex upper bound for the kernel regularization
Eq. (5)  which amounts to a sum of costs over examples under some regularization

(cid:19)

#
F + δ · tr(Ψ) − log det(Ψ)

Ω(θ) ≤

L(η  Ψ  θ) =

k 1√
δ

ui − η>φik2 + φ>

i Ψφi

+ δkηk2

"

where we omit all the terms irrelevant to η  Ψ and θ. L(η  Ψ  θ) is convex w.r.t. η and Ψ  and has
a unique minimum Ω(θ)  hence we can replace Ω(θ) by instead minimizing the upper bound and
formulate an equivalent learning problem

(cid:18)

mX

i=1

h

i

min
β η Ψ θ

L(β  η  Ψ  θ) = L(β  θ) + γL(η  Ψ  θ)

(7)

Clearly this new optimization can be solved by SGD.

When applying the SGD method  each step based on one example needs to compute the inverse of
Ψ. This can be computationally unaffordable when the dimensionality is large (e.g. q > 1000) —
remember that the efﬁciency of SGD is dependent on the lightweight of each stochastic update. Our
next result suggests that we can dramatically reduce this complexity from O(q3) to O(q).
Proposition 3.2. Eq. (5) is equivalent to the convex variational problem

(cid:19)

F + δ · ψ>e − qX

k=1

#

log ψk

(8)

Ω(θ) = min
η ψ

k 1√
δ

ui − η>φik2 + ψ>φ2

i

+ δkηk2

" mX

(cid:18)

i=1

where ψ = [ψ1  . . .   ψq]>  and e = [1  . . .   1]>.
Proof. There is an ambiguity of the solutions up to rotations. Suppose {β∗  Φ∗  η∗  Ψ∗} is an op-
timal solution set  a transformation β∗ ← Rβ∗  Φ∗ ← RΦ∗  η∗ ← Rη∗  and Ψ∗ ← RΨ∗R>
results in the same optimality if R>R = I. Since there always exists an R to diagonalize Ψ∗  we
can pre-restrict Ψ to be a diagonal positive deﬁnite matrix Ψ = diag[ψ1  . . .   ψq]  which does not
change our problem and gives rise to Eq. (8).

We note that the variational form is convex w.r.t. the auxiliary variables η and ψ. Therefore we can
formulate the whole learning problem as

4

(cid:20)

Problem 3.1.

1
n
where L1(β  θ) is deﬁned by Eq. (1)  and

L(β  η  ψ  θ) =

min
β η ψ θ

L1(β  θ) + γ
mn

L2(η  θ) + γ
mn

(cid:21)

L3(ψ  θ)

(9)

k 1√
δ

ψ>φ2

ui − η>φik2 + δkηk2

i + δ · ψ>e − qX

F

log ψk

mX
mX

i=1

L2(η  θ) =

L3(ψ  θ) =

i=1

k=1

To ensure the estimator of β and θ is consistent  the effect of regularization should vanish as n → ∞.
Therefore we intentionally normalize L2(η  θ) and L3(ψ  θ) by 1/m. The overall loss function is
averaged over the n labeled examples  consisting of three loss functions: the main classiﬁcation task
L1(β  θ)  an auxiliary least-squares regression problem L2(η  θ)  and an additional regularization
term L3(ψ  θ)  which can be interpreted as another least-squares problem. Since each of the loss
functions amounts to a summation of local costs caused by individual data examples  the whole
learning problem can be conveniently implemented by SGD  as described in Algorithm 1.
In practice  the kernel matrix Σ = U U> that represents domain knowledge can be obtained in
three different ways: (i) In the easiest case  U is directly available by computing some hand-crafted
features computed from the input data  which corresponds to a case of a linear kernel function; (ii)
U can be results of some unsupervised learning (e.g. the self-taught learning [14] based on sparse
coding)  applied on a large set of unlabeled data; (iii) If a nonlinear kernel function is available  U
can be obtained by applying incomplete Cholesky decomposition on an m × m kernel matrix Σ. In
the third case  when m is so large that the matrix decomposition cannot be computed in the main
memory  we apply the Nystr¨om method [19]: We ﬁrst randomly sample m1 examples p < m1 < m 
such that the computed kernel matrix Σ1 can be decomposed in the memory. Let V DV > be the p-
rank eigenvalue decomposition of Σ1  then the p-rank decomposition of Σ can be approximated by
2   where Σ: 1 is the m × m1 kernel matrix between all the m examples
Σ ≈ U U>  U = Σ: 1V D− 1
and the subset of size m1.

Algorithm 1 Stochastic Gradient Descent

repeat

Generate a number a from uniform distribution [0  1]
if a < n

m+n then

Randomly pick a sample i ∈ {1 ···   n} for L1  and update parameter by

[β  θ] ← [β  θ] − 

∂L1(xi  β  θ)

∂[β  θ]

else

Randomly pick a sample i ∈ {1 ···   m} for L2  and update parameter by

[η  ψ  θ] ← [η  ψ  θ] − 
m

∂[L2(xi  η  θ) + L3(xi  ψ  θ)]

∂[η  ψ  θ]

end if

until convergence

4 Visual Recognition by Deep Learning with Kernel Regularization

In the following  we apply the proposed strategy to train a class of deep models and convolutional
neural networks (CNNs  [11]) for a range of visual recognition tasks including digit recognition on
MNIST dataset  gender and ethnicity classiﬁcation on the FRGC face dataset  and object recognition
on the Caltech101 dataset. In each of these tasks  we choose a kernel function that has been reported
to have state-of-the-art or otherwise good performances in the literature. We will see whether a
kernel-regularizer can improve the recognition accuracy of the deep models  and how it is compared
with the support vector machine (SVM) using the exactly the same kernel.

5

Table 1: Percentage error rates of handwritten digit recognition on MNIST

Training Size
SVM (RBF)
SVM (RBF  Nystr¨om)
SVM (Graph)
SVM (Graph  Cholesky)
CNN
kCNN (RBF)
kCNN (Graph)
CNN (Pretrain) [15]
EmbedO CNN [18]
EmbedI5 CNN [18]
EmbedA1 CNN [18]

100
22.73
24.73
5.21
7.17
19.40
14.49
4.28
−
11.73
7.75
7.87

600
8.53
9.15
3.74
6.47
6.40
3.85
2.36
3.21
3.42
3.82
3.82

1000
6.58
6.92
3.46
5.75
5.50
3.40
2.05
−
3.34
2.73
2.76

3000
3.91
5.51
3.01
4.28
2.75
1.88
1.75
−
2.28
1.83
2.07

60000
1.41
5.16
2.23
2.87
0.82
0.73
0.64
0.64
−
−
−

Throughout all the experiments  “kCNN” denotes CNNs regularized by nonlinear kernels  processed
by either Cholesky or Nystr¨om approximation  with parameters p = 600  m1 = 5000  and m the
size of each whole data set. The obtained ui are normalized to have unitary lengths. λ and δ are
ﬁxed by 1. The remaining two hyperparameters are: the learning rates  = {10−3  10−4  10−5} and
the kernel regularization weights γ = {102  103  104  105}. Their values are set once for each of the
4 recognition tasks based on a 5-fold cross validation using 500 labeled examples.

4.1 Handwritten Digit Recognition on MNIST Dataset
The data contains a training set with 60000 examples and a test set with 10000 examples. The CNN
employs 50 ﬁlters of size 7 × 7 on 34 × 34 input images  followed by down-sampling by 1/2  then
128 ﬁlters of size 5×5  followed by down-sampling by 1/2  and then 200 ﬁlters of size 5×5  giving
rise to 200 dimensional features that are fed to the output layer. Two nonlinear kernels are used: (1)
RBF kernel  and (2) Graph kernel on 10 nearest neighbor graph [6]. We perform 600-dimension
Cholesky decomposition on the whole 70000 × 70000 graph kernel because it is very sparse.
In addition to using the whole training set  we train the models on 100  600  1000 and 3000 random
examples from the training set and evaluate the classiﬁers on the whole test set  and repeat each
setting by 5 times independently. The results are given in Tab. 1. kCNNs effectively improve over
CNNs by leveraging the prior knowledge  and also outperform SVMs that use the same kernels. The
results are competitive with the state-of-the-art results by [15]  and [18] of a different architecture.

4.2 Gender and Ethnicity Recognition on FRGC Dataset
The FRGC 2.0 dataset [13] contains 568 individuals’ 14714 face images under various lighting
conditions and backgrounds. Beside person identities  each image is annotated with gender and
ethnicity  which we put into 3 classes  “white”  “asian”  and “other”. We ﬁx 114 persons’ 3014
images (randomly chosen) as the testing set  and randomly selected 5%  10%  20%  50%  and “All”
images from the rest 454 individuals’ 11700 images. For each training size  we randomize the
training data 5 times and report the average error rates.
In this experiment  CNNs operate on images represented by R/G/B planes plus horizontal and ver-
tical gradient maps of gray intensities. The 5 input planes of size 140 × 140 are processed by 16
convolution ﬁlters with size 16 × 16  followed by max pooling within each disjoint 5 × 5 neigh-
borhood. The obtained 16 feature maps of size 25 × 25 are connected to the next layer by 256
ﬁlters of size 6 × 6  with 50% random sparse connections  followed by max pooling within each
5 × 5 neighborhood. The resulting 256 × 4 × 4 features are fed to the output layer. The nonlinear
kernel used in this experiment is the RBF kernel computed directly on images  which has demon-
strated state-of-the-art accuracy for gender recognition [3]. The results shown in Tab. 2 and Tab. 3
demonstrate that kCNNs signiﬁcantly boost the recognition accuracy of CNNs for both gender and
ethnicity recognition. The difference is prominent when small training sets are presented.

4.3 Object Recognition on Caltech101 Dataset
Caltech101 [7] contains 9144 images from 101 object categories and a background category. It is
considered one of the most diverse object databases available today  and is probably the most popular
benchmark for object recognition. We follow the common setting to train on 15 and 30 images per
class and test on the rest. Following [10]  we limit the number of test images to 30 per class. The

6

Table 2: Percentage error rates of gender recognition on FRGC

Training Size
SVM (RBF)
SVM (RBF  Nystr¨om)
CNN
kCNN

5% 10% 20% 50% All
8.6
16.7
8.8
20.2
61.5
5.9
4.4
17.1

13.4
14.3
17.2
7.2

11.3
11.6
8.4
5.8

9.1
9.1
6.6
5.0

Table 3: Percentage error rates of ethnicity recognition on FRGC

Training Size
SVM (RBF)
SVM (RBF  Nystr¨om)
CNN
kCNN

5% 10% 20% 50% All
22.9
10.2
11.1
24.7
6.3
30.0
15.6
5.8

16.9
20.6
13.9
8.7

14.1
15.8
10.0
7.3

11.3
11.9
8.2
6.2

recognition accuracy was normalized by class sizes and evaluated over 5 random data splits. The
CNN has the same architecture as the one used in the FRGC experiment. The nonlinear kernel is the
spatial pyramid matching (SPM) kernel developed in [10].

Tab. 4 shows our results together with those reported in [12  15] using deep hierarchical architec-
tures. The task is much more challenging than the previous three tasks for CNNs  because in each
category the data size is very small while the visual patterns are highly diverse. Thanks to the reg-
ularization by SPM kernel  kCNN dramatically improves the accuracy of CNN  and outperforms
SVM using the same kernel. This is perhaps the best performance by (trainable and hand-crafted)
deep hierarchical models on the Caltech101 dataset. Some ﬁlters trained with and without kernel
regularization are visualized in Fig. 1  which helps to understand the difference made by kCNN.

5 Related Work  Discussion  and Conclusion
Recent work on deep visual recognition models includes [17  12  15]. In [17] and [12] the ﬁrst layer
consisted of hard-wired Gabor ﬁlters  and then a large number of patches were sampled from the
second layer and used as the basis of the representation which was then used to train a discriminative
classiﬁer.

Deep models are powerful in representing complex functions but very difﬁcult to train. Hinton and
his coworkers proposed training deep belief networks with layer-wise unsupervised pre-training 
followed by supervised ﬁne-tuning [9]. The strategy was subsequently studied for other deep mod-
els like CNNs [15]  autoassociators [4]  and for document coding [16]. In recent work [18]  the
authors proposed training a deep model jointly with an unsupervised embedding task  which led to
improved results as well. Though using unlabeled data too  our work differs from previous work at
the emphasis on leveraging the prior knowledge  which suggests that it can be combined with those
approaches  including neighborhood component analysis [8]  to further enhance the deep learning.
This work is also related to transfer learning [2] that used auxiliary learning tasks to learn a linear
feature mapping  and more directly  our previous work [1]  which created pseudo auxiliary tasks
based on hand-craft image features to train nonlinear deep networks.

One may ask  why bother training with kCNN  instead of simply combining two independently
trained CNN and SVM systems? The reason is computational speed – kCNN pays an extra cost to
exploit a kernel matrix in the training phase  but in the prediction phase the system uses CNN alone.

(a) CNN-Caltech101

(b) kCNN-Caltech101

Figure 1: First-layer ﬁlters on the B channel  learned from Caltech101 (30 examples per class)

7

Table 4: Percentage accuracy on Caltech101

Training Size
SVM (SPM) [10]
SVM (SPM  Nystr¨om)
HMAX [12]

15
54.0
52.1
51.0

Training Size

30
64.6 CNN (Pretrain) [15]
63.1 CNN
56.0
kCNN

15
−
26.5
59.2

30
54.0
43.6
67.4

In our Caltech101 experiment  the SVM (SPM) needed several seconds to process a new image on
a PC with a 3.0 GHz processor  while kCNN can process about 40 images per second. The latest
record on Caltech101 was based on combining multiple kernels [5]. We conjecture that kCNN could
be further improved by using multiple kernels without sacriﬁcing recognition speed.

To conclude  we proposed using kernels to improve the training of deep models. The approach was
implemented by stochastic gradient descent  and demonstrated excellent performances on a range of
visual recognition tasks. Our experiments showed that prior knowledge could signiﬁcantly improve
the performance of deep models when insufﬁcient labeled data were available in hard recognition
problems. The trained model was much faster than kernel systems for making predictions.
Acknowledgment: We thank the reviewers and Douglas Gray for helpful comments.
References

[1] A. Ahmed  K. Yu  W. Xu  Y. Gong  and E. P. Xing. Training hierarchical feed-forward visual recognition

models using transfer learning from pseudo tasks. European Conference on Computer Vision  2008.

[2] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unla-

beled data. Journal of Machine Learning Research  2005.

[3] S. Baluja and H. Rowley. Boosting sex identiﬁcation performance. Journal of Computer Vision  2007.
[4] Y. Bengio  P. Lamblin  D. Popovici  and H. Larochelle. Greedy layer-wise training of deep networks.

Neural Information Processing Systems  2007.

[5] A. Bosch  A. Zisserman  and X. Mun˜oz. Image classiﬁcation using ROIs and multiple kernel learning.

2008. submitted to International Journal of Computer Vison.

[6] O. Chapelle  J. Weston  and B. Sch¨olkopf. Cluster kernels for semi-supervised learning. Neural Informa-

tion Processing Systems  2003.

[7] L. Fei-Fei  R. Fergus  and P. Perona. Learning generative visual models from few training examples: An

incremental Bayesian approach tested on 101 object categories. CVPR Workshop  2004.

[8] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighbourhood components analysis. Neural

Information Processing Systems  2005.

[9] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 

313(5786):504 – 507  July 2006.

[10] S. Lazebnik  C. Schmid  and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing

natural scene categories. IEEE Conference on Computer Vision and Pattern Recognition  2006.

[11] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[12] J. Mutch and D. G. Lowe. Multiclass object recognition with sparse  localized features. IEEE Conference

on Computer Vision and Pattern Recognition  2006.

[13] P. J. Philips  P. J. Flynn  T. Scruggs  K. W. Bower  and W. Worek. Preliminary face recognition grand

challenge results. IEEE Conference on Automatic Face and Gesture Recgonition  2006.

[14] R. Raina  A. Battle  H. Lee  B. Packer  and A. Y. Ng. Self-taught learning: Transfer learning from

unlabeled data. International Conference on Machine Learning  2007.

[15] M. Ranzato  F.-J. Huang  Y.-L. Boureau  and Y. LeCun. Unsupervised learning of invariant feature hi-
IEEE Conference on Computer Vision and Pattern

erarchies with applications to object recognition.
Recognition  2007.

[16] M. Ranzato and M. Szummer. Semi-supervised learning of compact document representations with deep

networks. International Conferenece on Machine Learning  2008.

[17] T. Serre  L. Wolf  and T. Poggio. Object recognition with features inspired by visual cortex.

Conference on Computer Vision and Pattern Recognition  2005.

IEEE

[18] J. Weston  F. Ratle  and R. Collobert. Deep learning via semi-supervised embedding.

Conference on Machine Learning  2008.

International

[19] C. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. Neural Information

Processing Systems  2001.

8

,Paul Wagner