2013,Unsupervised Structure Learning of Stochastic And-Or Grammars,Stochastic And-Or grammars compactly represent both compositionality and reconfigurability and have been used to model different types of data such as images and events. We present a unified formalization of stochastic And-Or grammars that is agnostic to the type of the data being modeled  and propose an unsupervised approach to learning the structures as well as the parameters of such grammars. Starting from a trivial initial grammar  our approach iteratively induces compositions and reconfigurations in a unified manner and optimizes the posterior probability of the grammar. In our empirical evaluation  we applied our approach to learning event grammars and image grammars and achieved comparable or better performance than previous approaches.,Unsupervised Structure Learning of Stochastic

And-Or Grammars

Maria Pavlovskaia

Song-Chun Zhu

Kewei Tu

Center for Vision  Cognition  Learning and Art
Departments of Statistics and Computer Science
{tukw mariapavl sczhu}@ucla.edu

University of California  Los Angeles

Abstract

Stochastic And-Or grammars compactly represent both compositionality and re-
conﬁgurability and have been used to model different types of data such as images
and events. We present a uniﬁed formalization of stochastic And-Or grammars
that is agnostic to the type of the data being modeled  and propose an unsupervised
approach to learning the structures as well as the parameters of such grammars.
Starting from a trivial initial grammar  our approach iteratively induces composi-
tions and reconﬁgurations in a uniﬁed manner and optimizes the posterior prob-
ability of the grammar. In our empirical evaluation  we applied our approach to
learning event grammars and image grammars and achieved comparable or better
performance than previous approaches.

1

Introduction

Stochastic grammars are traditionally used to represent natural language syntax and semantics  but
they have also been extended to model other types of data like images [1  2  3] and events [4  5 
6  7]. It has been shown that stochastic grammars are powerful models of patterns that combine
compositionality (i.e.  a pattern can be decomposed into a certain conﬁguration of sub-patterns) and
reconﬁgurability (i.e.  a pattern may have multiple alternative conﬁgurations). Stochastic grammars
can be used to parse data samples into their compositional structures  which help solve tasks like
classiﬁcation  annotation and segmentation in a uniﬁed way. We study stochastic grammars in the
form of stochastic And-Or grammars [1]  which are an extension of stochastic grammars in natural
language processing [8  9] and are closely related to sum-product networks [10]. Stochastic And-Or
grammars have been used to model spatial structures of objects and scenes [1  3] as well as temporal
structures of actions and events [7].
Manual speciﬁcation of a stochastic grammar is typically very difﬁcult and therefore machine learn-
ing approaches are often employed to automatically induce unknown stochastic grammars from data.
In this paper we study unsupervised learning of stochastic And-Or grammars in which the training
data are unannotated (e.g.  images or action sequences).
The learning of a stochastic grammar involves two parts: learning the grammar rules (i.e.  the struc-
ture of the grammar) and learning the rule probabilities or energy terms (i.e.  the parameters of the
grammar). One strategy in unsupervised learning of stochastic grammars is to manually specify
a ﬁxed grammar structure (in most cases  the full set of valid grammar rules) and try to optimize
the parameters of the grammar. Many approaches of learning natural language grammars (e.g. 
[11  12]) as well as some approaches of learning image grammars [10  13] adopt this strategy. The
main problem of this strategy is that in some scenarios the full set of valid grammar rules is too large
for practical learning and inference  while manual speciﬁcation of a compact grammar structure is
challenging. For example  in an image grammar the number of possible grammar rules to decom-
pose an image patch is exponential in the size of the patch; previous approaches restrict the valid

1

ways of decomposing an image patch (e.g.  allowing only horizontal and vertical segmentations) 
which however reduces the expressive power of the image grammar.
In this paper  we propose an approach to learning both the structure and the parameters of a stochas-
tic And-Or grammar. Our approach extends the previous work on structure learning of natural
language grammars [14  15  16]  while improves upon the recent work on structure learning of And-
Or grammars of images [17] and events [18]. Starting from a trivial initial grammar  our approach
iteratively inserts new fragments into the grammar to optimize its posterior probability. Most of
the previous structure learning approaches learn new compositions and reconﬁgurations modeled
in the grammar in a separate manner  which can be error-prone when the training data is scarce or
ambiguous; in contrast  we induce And-Or fragments of the grammar  which uniﬁes the search for
new compositions and reconﬁgurations  making our approach more efﬁcient and robust.
Our main contributions are as follows.

• We present a formalization of stochastic And-Or grammars that is agnostic to the types of
atomic patterns and their compositions. Consequently  our learning approach is capable of
learning from different types of data  e.g.  text  images  events.
• Unlike some previous approaches that rely on heuristics for structure learning  we explicitly
optimize the posterior probability of both the structure and the parameters of the grammar.
The optimization procedure is made efﬁcient by deriving and utilizing a set of sufﬁcient
statistics from the training data.
• We learn compositions and reconﬁgurations modeled in the grammar in a uniﬁed manner
that is more efﬁcient and robust to data scarcity and ambiguity than previous approaches.
• We empirically evaluated our approach in learning event grammars and image grammars

and it achieved comparable or better performance than previous approaches.

2 Stochastic And-Or Grammars

Stochastic And-Or grammars are ﬁrst proposed to model images [1] and later adapted to model
events [7]. Here we provide a uniﬁed deﬁnition of stochastic And-Or grammars that is agnostic to
the type of the data being modeled. We restrict ourselves to the context-free subclass of stochastic
And-Or grammars  which can be seen as an extension of stochastic context-free grammars in for-
mal language theory [8] as well as an extension of decomposable sum-product networks [10]. A
stochastic context-free And-Or grammar is deﬁned as a 5-tuple (cid:104)Σ  N  S  R  P(cid:105). Σ is a set of termi-
nal nodes representing atomic patterns that are not decomposable; N is a set of nonterminal nodes
representing decomposable patterns  which is divided into two disjoint sets: And-nodes N AND and
Or-nodes N OR; S ∈ N is a start symbol that represents a complete entity; R is a set of grammar
rules  each of which represents the generation from a nonterminal node to a set of nonterminal or
terminal nodes; P is the set of probabilities assigned to the grammar rules. The set of grammar rules
R is divided into two disjoint sets: And-rules and Or-rules.

• An And-rule represents the decomposition of a pattern into a conﬁguration of non-
overlapping sub-patterns. It takes the form of A → a1a2 . . . an  where A ∈ N AND is a
nonterminal And-node and a1a2 . . . an is a set of terminal or nonterminal nodes represent-
ing the sub-patterns. A set of relations are speciﬁed between the sub-patterns and between
the nonterminal node A and the sub-patterns  which conﬁgure how these sub-patterns form
the composite pattern represented by A. The probability of an And-rule is speciﬁed by the
energy terms deﬁned on the relations. Note that one can specify different types of relations
in different And-rules  which allows multiple types of compositions to be modeled in the
same grammar.
• An Or-rule represents an alternative conﬁguration of a composite pattern. It takes the form
of O → a  where O ∈ N OR is a nonterminal Or-node  and a is either a terminal or a
nonterminal node representing a possible conﬁguration. The set of Or-rules with the same
left-hand side can be written as O → a1|a2| . . .|an. The probability of an Or-rule speciﬁes
how likely the alternative conﬁguration represented by the Or-rule is selected.

A stochastic And-Or grammar deﬁnes generative processes of valid entities  i.e.  starting from an
entity containing only the start symbol S and recursively applying the grammar rules in R to convert

2

Table 1: Examples of stochastic And-Or grammars

Natural language
grammar
Event And-Or
grammar [7]
Image And-Or
grammar [1]

Terminal node
Word

Atomic action (e.g. 
standing  drinking)
Visual word (e.g. 
Gabor bases)

Nonterminal node
Phrase

Event or sub-event

Image patch

Relations in And-rules
Deterministic “concatenating”
relations
Temporal relations (e.g.  those
proposed in [19])
Spatial relations (e.g.  those
specifying relative positions 
rotations and scales)

(a)

(b)

(c)

Figure 1: An illustration of the learning process. (a) The initial grammar. (b) Iteration 1: learning a
grammar fragment rooted at N1. (c) Iteration 2: learning a grammar fragment rooted at N2.

nonterminal nodes until the entity contains only terminal nodes (atomic patterns). Table 1 gives a
few examples of stochastic context-free And-Or grammars that model different types of data.

3 Unsupervised Structure Learning

3.1 Problem Deﬁnition

In unsupervised learning of stochastic And-Or grammars  we aim to learn a grammar from a set
of unannotated i.i.d. data samples (e.g.  natural language sentences  quantized images  action se-
quences). The objective function is the posterior probability of the grammar given the training data:

P (G|X) ∝ P (G)P (X|G) =

1
Z

P (xi|G)

e−α(cid:107)G(cid:107) (cid:89)

xi∈X

where G is the grammar  X = {xi} is the set of training samples  Z is the normalization factor
of the prior  α is a constant  and (cid:107)G(cid:107) is the size of the grammar. By adopting a sparsity prior that
penalizes the size of the grammar  we hope to learn a compact grammar with good generalizability.
In order to ease the learning process  during learning we approximate the likelihood P (xi|G) with
the Viterbi likelihood (the probability of the best parse of the data sample xi). Viterbi likelihood has
been empirically shown to lead to better grammar learning results [20  10] and can be interpreted as
combining the standard likelihood with an unambiguity bias [21].

3.2 Algorithm Framework

We ﬁrst deﬁne an initial grammar that generates the exact set of training samples. Speciﬁcally  for
each training sample xi ∈ X  there is an Or-rule S → Ai in the initial grammar where S is the start
1(cid:107)X(cid:107) where (cid:107)X(cid:107) is the number of
symbol and Ai is an And-node  and the probability of the rule is
training samples; for each xi there is also an And-rule Ai → ai1ai2 . . . ain where aij (j = 1 . . . n)
are the terminal nodes representing the set of atomic patterns contained in sample xi  and a set of
relations are speciﬁed between these terminal nodes such that they compose sample xi. Figure 1(a)
shows an example initial grammar. This initial grammar leads to the maximal likelihood on the
training data but has a very small prior probability because of its large size.

3

And‐nodeOr‐nodeS……A1A2a1a2a3a4a5a3a4a6xxSSx1x2S……A1A2……A1A2a1a2Xa5a6XXYa1Ya6YX……Xa3a4Xa3a4aaa2a5SAnd‐nodeOr‐nodeS……A1A2a1a2a3a4a5a6a7a8SSa1a2Xa5a6……A1A2Xa1Ya6……A1A2Ya2Xa5a6XXYX……a3a4Xa3a4a2a5And‐nodeOr‐nodeS……A1A2a1a2a3a4a5a3a4a6xxSSx1x2S……A1A2……A1A2a1a2N1a5a6N1NN2a1N2a6N2N1……N1a3a4N1a3a4aaa2a5SAnd‐nodeOr‐nodeS……A1A2a1a2a3a4a5a6a7a8SSa1a2Xa5a6……A1A2Xa1Ya6……A1A2Ya2Xa5a6XXYX……a3a4Xa3a4a2a5And‐nodeOr‐nodeS……A1A2a1a2a3a4a5a3a4a6xxSSx1x2S……A1A2……A1A2a1a2N1a5a6N1NN2a1N2a6N2N1……N1a3a4N1a3a4aaa2a5SAnd‐nodeOr‐nodeS……A1A2a1a2a3a4a5a6a7a8SSa1a2Xa5a6……A1A2Xa1Ya6……A1A2Ya2Xa5a6XXYX……a3a4Xa3a4a2a5SAnd‐nodeOr‐nodeS……A1A2a1a2a3a4a5a6a7a8SSa1a2Xa5a6……A1A2Xa1Ya6……A1A2Ya2Xa5a6XXYX……a3a4Xa3a4a2a5Starting from the initial grammar  we introduce new intermediate nonterminal nodes between the
terminal nodes and the top-level nonterminal nodes in an iterative bottom-up fashion to generalize
the grammar and increase its posterior probability. At each iteration  we add a grammar fragment
into the grammar that is rooted at a new nonterminal node and contains a set of grammar rules that
specify how the new nonterminal node generates one or more conﬁgurations of existing terminal
or nonterminal nodes; we also try to reduce each training sample using the new grammar rules and
update the top-level And-rules accordingly. Figure 1 illustrates this learning process. There are
typically multiple candidate grammar fragments that can be added at each iteration  and we employ
greedy search or beam search to explore the search space and maximize the posterior probability of
the grammar. We also restrict the types of grammar fragments that can be added in order to reduce
the number of candidate grammar fragments  which will be discussed in the next subsection. The
algorithm terminates when no more grammar fragment can be found that increases the posterior
probability of the grammar.

3.3 And-Or Fragments

In each iteration of our learning algorithm framework  we search for a new grammar fragment and
add it into the grammar. There are many different types of grammar fragments  the choice of which
greatly inﬂuences the efﬁciency and accuracy of the learning algorithm. Two simplest types of
grammar fragments are And-fragments and Or-fragments. An And-fragment contains a new And-
node A and an And-rule A → a1a2 . . . an specifying the generation from the And-node A to a
conﬁguration of existing nodes a1a2 . . . an. An Or-fragment contains a new Or-node O and a set
of Or-rules O → a1|a2| . . .|an each specifying the generation from the Or-node O to an existing
node ai. While these two types of fragments are simple and intuitive  they both have important
disadvantages if they are searched for separately in the learning algorithm. For And-fragments  when
the training data is scarce  many compositions modeled by the target grammar would be missing
from the training data and hence cannot be learned by searching for And-fragments alone; besides 
if the search for And-fragments is not properly coupled with the search for Or-fragments  the learned
grammar would become large and redundant. For Or-fragments  it can be shown that in most cases
adding an Or-fragment into the grammar decreases the posterior probability of the grammar even
if the target grammar does contain the Or-fragment  so in order to learn Or-rules we need more
expensive search techniques than greedy or beam search employed in our algorithm; in addition  the
search for Or-fragments can be error-prone if different Or-rules can generate the same node in the
target grammar.
Instead of And-fragments and Or-fragments  we propose to search for And-Or fragments in the
learning algorithm. An And-Or fragment contains a new And-node A  a set of new Or-nodes
O1  O2  . . .   On  an And-rule A → O1O2 . . . On  and a set of Or-rules Oi → ai1|ai2| . . .|aimi
for each Or-node Oi (where ai1  ai2  . . .   aimi are existing nodes of the grammar). Such an And-Or
i=1 mi number of conﬁgurations of existing nodes. Figure 2(a) shows an
example And-Or fragment. It can be shown that by adding only And-Or fragments  our algorithm is
still capable of constructing any context-free And-Or grammar. Using And-Or fragments can avoid
or alleviate the problems associated with And-fragments and Or-fragments: since an And-Or frag-
ment systematically covers multiple compositions  the data scarcity problem of And-fragments is
alleviated; since And-rules and Or-rules are learned in a more uniﬁed manner  the resulting gram-
mar is often more compact; reasonable And-Or fragments usually increase the posterior probability
of the grammar  therefore easing the search procedure; ﬁnally  ambiguous Or-rules can be better
distinguished since they are learned jointly with their sibling Or-nodes in the And-Or fragments.
To perform greedy search or beam search  in each iteration of our learning algorithm we need to
ﬁnd the And-Or fragments that lead to the highest gain in the posterior probability of the grammar.
Computing the posterior gain by re-parsing the training samples can be very time-consuming if the
training set or the grammar is large. Fortunately  we show that by assuming grammar unambiguity
the posterior gain of adding an And-Or fragment can be formulated based on a set of sufﬁcient statis-
tics of the training data and is efﬁcient to compute. Since the posterior probability is proportional to
the product of the likelihood and the prior probability  the posterior gain is equal to the product of
the likelihood gain and the prior gain  which we formulate separately below.
Likelihood Gain. Remember that in our learning algorithm when an And-Or fragment is added
into the grammar  we try to reduce the training samples using the new grammar rules and update the

fragment can generate(cid:81)n

4

(a)

(b)

(c)

Figure 2: (a) An example And-Or fragment. (b) The n-gram tensor of the And-Or fragment based
on the training data (here n = 3). (c) The context matrix of the And-Or fragment based on the
training data.

top-level And-rules accordingly. Denote the set of reductions being made on the training samples
by RD. Suppose in reduction rd ∈ RD  we replace a conﬁguration e of nodes a1j1a2j2 . . . anjn
with the new And-node A  where aiji (i = 1 . . . n) is an existing terminal or nonterminal node that
can be generated by the new Or-node Oi in the And-Or fragment. With reduction rd  the Viterbi
likelihood of the training sample x where rd occurs is changed by two factors. First  since the
grammar now generates the And-node A ﬁrst  which then generates a1j1 a2j2 . . . anjn  the Viterbi
likelihood of sample x is reduced by a factor of P (A → a1j1 a2j2 . . . anjn ). Second  the reduction
may make sample x identical to some other training samples  which increases the Viterbi likelihood
of sample x by a factor equal to the ratio of the numbers of such identical samples after and before
the reduction. To facilitate the computation of this factor  we can construct a context matrix CM
where each row is a conﬁguration of existing nodes covered by the And-Or fragment  each column
is a context which is the surrounding patterns of a conﬁguration  and each element is the number of
times that the corresponding conﬁguration and context co-occur in the training set. See Figure 2(c)
for the context matrix of the example And-Or fragment. Putting these two types of changes to the
likelihood together  we can formulate the likelihood gain of adding the And-Or fragment as follows
(cid:81)mi
(see the supplementary material for the full derivation).
j=1 (cid:107)RDi(aij)(cid:107)(cid:107)RDi(aij )(cid:107)

(cid:81)n

(cid:80)

CM [e c]

(cid:81)
c((cid:80)
(cid:81)

×

e CM [e  c])
e c CM [e  c]CM [e c]

e

P (X|Gt+1)
P (X|Gt)

i=1

=

(cid:107)RD(cid:107)n(cid:107)RD(cid:107)

where Gt and Gt+1 are the grammars before and after learning from the And-Or fragment  RDi(aij)
denotes the subset of reductions in RD in which the i-th node of the conﬁguration being reduced
is aij  e in the summation or product ranges over all the conﬁgurations covered by the And-Or
fragment  and c in the product ranges over all the contexts that appear in CM.
It can be shown that the likelihood gain can be factorized as the product of two tensor/matrix co-
herence measures as deﬁned in [22]. The ﬁrst is the coherence of the n-gram tensor of the And-Or
fragment (which tabulates the number of times each conﬁguration covered by the And-Or fragment
appears in the training samples  as illustrated in Figure 2(b)). The second is the coherence of the
context matrix. These two factors provide a surrogate measure of how much the training data support
the context-freeness within the And-Or fragment and the context-freeness of the And-Or fragment
against its context respectively. See the supplementary material for the derivation and discussion.
The formulation of likelihood gain also entails the optimal probabilities of the Or-rules in the And-
Or fragment.

∀i  j P (Oi → aij) =

(cid:80)mi
(cid:107)RDi(aij)(cid:107)
j(cid:48)=1 (cid:107)RDi(aij(cid:48))(cid:107) =

(cid:107)RDi(aij)(cid:107)

(cid:107)RD(cid:107)

Prior Gain. The prior probability of the grammar is determined by the grammar size. When the
And-Or fragment is added into the grammar  the size of the grammar is changed in two aspects:
ﬁrst  the size of the grammar is increased by the size of the And-Or fragment; second  the size of the
grammar is decreased because of the reductions from conﬁgurations of multiple nodes to the new
And-node. Therefore  the prior gain of learning from the And-Or fragment is:

P (Gt+1)
P (Gt)

= e−α((cid:107)Gt+1(cid:107)−(cid:107)Gt(cid:107)) = e

miso)−(cid:107)RD(cid:107)(n−1)sa)

i=1

−α((nsa+(cid:80)n

5

AO1O2O3912329123103410a11a11a12a13a21a22a31a32a33a3468212152053172363a12a13a31a32a33a34context1context2context3… …a11a21a31100…a12a21a31512…… ……………a13a22a34411…A912329123103410a11O1O2O368212152053172363a12a13a11a12a13a21a22a31a32a33a34a31a32a33a34context1context2context3… …a11a21a31100…a12a21a31512…… ……………a13a22a34411…A912329123103410a11O1O2O368212152053172363a12a13a11a12a13a21a22a31a32a33a34a31a32a33a34context1context2context3… …a11a21a31100…a12a21a31512…… ……………a13a22a34411…Figure 3: An illustration of the procedure of ﬁnding the best And-Or fragment. r1  r2  r3 denote
different relations between patterns. (a) Collecting statistics from the training samples to construct
or update the n-gram tensors. (b) Finding one or more sub-tensors that lead to the highest posterior
gain and constructing the corresponding And-Or fragments.

Figure 4: An example video and the action annotations from the human activity dataset [23]. Each
colored bar denotes the start/end time of an occurrence of an action.

where sa and so are the number of bits needed to encode each node on the right-hand side of an
And-rule and Or-rule respectively. It can be seen that the prior gain penalizes And-Or fragments
that have a large size but only cover a small number of conﬁgurations in the training data.
In order to ﬁnd the And-Or fragments with the highest posterior gain  we could construct n-gram
tensors from all the training samples for different values of n and different And-rule relations  and
within these n-gram tensors we search for sub-tensors that correspond to And-Or fragments with
the highest posterior gain. Figure 3 illustrates this procedure. In practice  we ﬁnd it sufﬁcient to
use greedy search or beam search with random restarts in identifying good And-Or fragments. See
the supplementary material for the pseudocode of the complete algorithm of grammar learning.
The algorithm runs reasonably fast: our prototype implementation can ﬁnish running within a few
minutes on a desktop with 5000 training samples each containing more than 10 atomic patterns.

4 Experiments

4.1 Learning Event Grammars

We applied our approach to learn event grammars from human activity data. The ﬁrst dataset con-
tains 61 videos of indoor activities  e.g.  using a computer and making a phone call [23]. The atomic
actions and their start/end time are annotated in each video  as shown in Figure 4. Based on this
dataset  we also synthesized a more complicated second dataset by dividing each of the two most
frequent actions  sitting and standing  into three subtypes and assigning each occurrence of the two
actions randomly to one of the subtypes. This simulates the scenarios in which the actions are de-
tected in an unsupervised way and therefore actions of the same type may be regarded as different
because of the difference in the posture or viewpoint.
We employed three different methods to apply our grammar learning approach on these two datasets.
The ﬁrst method is similar to that proposed in [18]. For each frame of a video in the dataset  we
construct a binary vector that indicates which of the atomic actions are observed in this frame. In this
way  each video is represented by a sequence of vectors. Consecutive vectors that are identical are

6

Relation2r1r2r3Relation2313r1(a)+11253+1263Training samplesn‐gram tensors relations (here n1A2AO1O2(b)r1215353of different n=2)And‐Or fragmentTable 2: The experimental results (F-
measure) on the event datasets. For
our approach  f  c+f and cf denote
the ﬁrst  second and third methods
respectively.

ADIOS [15]
SPYZ [18]
Ours (f)
Ours (c+f)
Ours (cf)

Data 1 Data 2
0.204
0.810
0.582
0.756
0.831
0.702
0.624
0.768
0.813
0.767

Figure 5: An example event And-Or grammar with two
types of relations that grounds to atomic actions

merged. We then map each distinct vector to a unique ID and thus convert each video into a sequence
of IDs. Our learning approach is applied on the ID sequences  where each terminal node represents
an ID and each And-node speciﬁes the temporal “followed-by” relation between its child nodes. In
the second and third methods  instead of the ID sequences  our learning approach is directly applied
to the vector sequences. Each terminal node now represents an occurrence of an atomic action. In
addition to the “followed-by” relation  an And-node may also specify the “co-occurring” relation
between its child nodes. In this way  the resulting And-Or grammar is directly grounded to the
observed atomic actions and is therefore more ﬂexible and expressive than the grammar learned
from IDs as in the ﬁrst method. Figure 5 shows such a grammar. The difference between the second
and the third method is: in the second method we require the And-nodes with the “co-occurring”
relation to be learned before any And-node with the “followed-by” relation is learned  which is
equivalent to applying the ﬁrst method based on a set of IDs that are also learned; on the other hand 
the third method does not restrict the order of learning of the two types of And-nodes.
Note that in our learning algorithm we assume that each training sample consists of a single pattern
generated from the target grammar  but here each video may contain multiple unrelated events. We
slightly modiﬁed our algorithm to accommodate this issue: right before the algorithm terminates  we
change the top-level And-nodes in the grammar to Or-nodes  which removes any temporal relation
between the learned events in each training sample and renders them independent of each other.
When parsing a new sample using the learned grammar  we employ the CYK algorithm to efﬁciently
identify all the subsequences that can be parsed as an event by the grammar.
We used 55 samples of each dataset as the training set and evaluated the learned grammars on the
remaining 6 samples. On each testing sample  the events identiﬁed by the learned grammars were
compared against manual annotations. We measured the purity (the percentage of the identiﬁed
event durations overlapping with the annotated event durations) and inverse purity (the percentage
of the annotated event durations overlapping with the identiﬁed event durations)  and report the F-
measure (the harmonic mean of purity and inverse purity). We compared our approach with two
previous approaches [15  18]  both of which can only learn from ID sequences.
Table 2 shows the experimental results. It can be seen that our approach is competitive with the
previous approaches on the ﬁrst dataset and outperforms the previous approaches on the more com-
plicated second dataset. Among the three methods of applying our approach  the second method has
the worst performance  mostly because the restriction of learning the “co-occurring” relation ﬁrst
often leads to premature equating of different vectors. The third method leads to the best overall
performance  which implies the advantage of grounding the grammar to atomic actions and simulta-
neously learning different relations. Note that the third method has better performance on the more
complicated second dataset  and our analysis suggests that the division of sitting/standing into sub-
types in the second dataset actually helps the third method to avoid learning erroneous compositions
of continuous siting or standing.

4.2 Learning Image Grammars

We ﬁrst tested our approach in learning image grammars from a synthetic dataset of animal face
sketches [24]. Figure 6 shows some example images from the dataset. We constructed 15 training
sets of 5 different sizes and ran our approach for three times on each training set. We set the terminal

7

Pick & throw trashStandStandStandfffffThe“followed‐Pick up trashThrow trashcccThe followedby” relationThe“cooccurring”Bend downSquatStandBend downThe co‐occurring relationFigure 6: Example
images from the syn-
thetic dataset

Figure 7: The experimental results on the synthetic image dataset

Figure 8: Example images and atomic patterns of the real dataset [17]

Table 3: The average
perplexity on the testing
sets from the real
im-
age experiments (lower
is better)

Ours
SZ [17]

Perplexity

67.5
129.4

nodes to represent the atomic sketches in the images and set the relations in And-rules to represent
relative positions between image patches. The hyperparameter α of our approach is ﬁxed to 0.5.
We evaluated the learned grammars against the true grammar. We estimated the precision and recall
of the sets of images generated from the learned grammars versus the true grammar  from which
we computed the F-measure. We also estimated the KL-divergence of the probability distributions
deﬁned by the learned grammars from that of the true grammar. We compared our approach with
the image grammar learning approach proposed in [17]. Figure 7 shows the experimental results. It
can be seen that our approach signiﬁcantly outperforms the competing approach.
We then ran our approach on a real dataset of animal faces that was used in [17]. The dataset contains
320 images of four categories of animals: bear  cat  cow and wolf. We followed the method described
in [17] to quantize the images and learn the atomic patterns  which become the terminal nodes of the
grammar. Figure 8 shows some images from the dataset  the quantization examples and the atomic
patterns learned. We again used the relative positions between image patches as the type of relations
in And-rules. Since the true grammar is unknown  we evaluated the learned grammars by measuring
their perplexity (the reciprocal of the geometric mean probability of a sample from a testing set).
We ran 10-fold cross-validation on the dataset: learning an image grammar from each training set
and then evaluating its perplexity on the testing set. Before estimating the perplexity  the probability
distribution represented by each learned grammar was smoothed to avoid zero probability on the
testing images. Table 3 shows the results of our approach and the approach from [17]. Once again
our approach signiﬁcantly outperforms the competing approach.

5 Conclusion

We have presented a uniﬁed formalization of stochastic And-Or grammars that is agnostic to the type
of the data being modeled  and have proposed an unsupervised approach to learning the structures
as well as the parameters of such grammars. Our approach optimizes the posterior probability of the
grammar and induces compositions and reconﬁgurations in a uniﬁed manner. Our experiments in
learning event grammars and image grammars show satisfactory performance of our approach.

Acknowledgments

The work is supported by grants from DARPA MSEE project FA 8650-11-1-7149  ONR MURI
N00014-10-1-0933  NSF CNS 1028381  and NSF IIS 1018751.

8

010020030040000.20.40.60.81Number of Training SamplesF−measure  OursSZ [17]0100200300400051015Number of Training SamplesKL−Divergence  OursSZ [17]Example imagesExample quantized imagesAtomicpatternsimagesAtomic patterns(terminal nodes)References
[1] S.-C. Zhu and D. Mumford  “A stochastic grammar of images ” Found. Trends. Comput. Graph. Vis. 

vol. 2  no. 4  pp. 259–362  2006.

[2] Y. Jin and S. Geman  “Context and hierarchy in a probabilistic image model ” in CVPR  2006.
[3] Y. Zhao and S. C. Zhu  “Image parsing with stochastic scene grammar ” in NIPS  2011.
[4] Y. A. Ivanov and A. F. Bobick  “Recognition of visual activities and interactions by stochastic parsing ”

Pattern Analysis and Machine Intelligence  IEEE Transactions on  vol. 22  no. 8  pp. 852–872  2000.

[5] M. S. Ryoo and J. K. Aggarwal  “Recognition of composite human activities through context-free gram-

mar based representation ” in CVPR  2006.

[6] Z. Zhang  T. Tan  and K. Huang  “An extended grammar system for learning and recognizing complex

visual events ” IEEE Trans. Pattern Anal. Mach. Intell.  vol. 33  no. 2  pp. 240–255  Feb. 2011.

[7] M. Pei  Y. Jia  and S.-C. Zhu  “Parsing video events with goal inference and intent prediction ” in ICCV 

2011.

[8] C. D. Manning and H. Sch¨utze  Foundations of statistical natural language processing. Cambridge 

MA  USA: MIT Press  1999.

[9] P. Liang  M. I. Jordan  and D. Klein  “Probabilistic grammars and hierarchical dirichlet processes ” The

handbook of applied Bayesian analysis  2009.

[10] H. Poon and P. Domingos  “Sum-product networks : A new deep architecture ” in Proceedings of the

Twenty-Seventh Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2011.

[11] J. K. Baker  “Trainable grammars for speech recognition ” in Speech Communication Papers for the 97th

Meeting of the Acoustical Society of America  1979.

[12] D. Klein and C. D. Manning  “Corpus-based induction of syntactic structure: Models of dependency and

constituency ” in Proceedings of ACL  2004.

[13] S. Wang  Y. Wang  and S.-C. Zhu  “Hierarchical space tiling for scene modeling ” in Computer Vision–

ACCV 2012. Springer  2013  pp. 796–810.

[14] A. Stolcke and S. M. Omohundro  “Inducing probabilistic grammars by Bayesian model merging ” in

ICGI  1994  pp. 106–118.

[15] Z. Solan  D. Horn  E. Ruppin  and S. Edelman  “Unsupervised learning of natural languages ” Proc. Natl.

Acad. Sci.  vol. 102  no. 33  pp. 11 629–11 634  August 2005.

[16] K. Tu and V. Honavar  “Unsupervised learning of probabilistic context-free grammar using iterative bi-
clustering ” in Proceedings of 9th International Colloquium on Grammatical Inference (ICGI 2008)  ser.
LNCS 5278  2008.

[17] Z. Si and S. Zhu  “Learning and-or templates for object modeling and recognition ” IEEE Trans on Pattern

Analysis and Machine Intelligence  2013.

[18] Z. Si  M. Pei  B. Yao  and S.-C. Zhu  “Unsupervised learning of event and-or grammar and semantics

from video ” in ICCV  2011.

[19] J. F. Allen  “Towards a general theory of action and time ” Artiﬁcial intelligence  vol. 23  no. 2  pp.

123–154  1984.

[20] V. I. Spitkovsky  H. Alshawi  D. Jurafsky  and C. D. Manning  “Viterbi training improves unsupervised
dependency parsing ” in Proceedings of the Fourteenth Conference on Computational Natural Language
Learning  ser. CoNLL ’10  2010.

[21] K. Tu and V. Honavar  “Unambiguity regularization for unsupervised learning of probabilistic grammars ”
in Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Nat-
ural Language Learning (EMNLP-CoNLL 2012)  2012.

[22] S. C. Madeira and A. L. Oliveira  “Biclustering algorithms for biological data analysis: A survey.”

IEEE/ACM Trans. on Comp. Biol. and Bioinformatics  vol. 1  no. 1  pp. 24–45  2004.

[23] P. Wei  N. Zheng  Y. Zhao  and S.-C. Zhu  “Concurrent action detection with structural prediction ” in

Proc. Intl Conference on Computer Vision (ICCV)  2013.

[24] A. Barbu  M. Pavlovskaia  and S. Zhu  “Rates for inductive learning of compositional models ” in AAAI

Workshop on Learning Rich Representations from Low-Level Sensors (RepLearning)  2013.

9

,Kewei Tu
Maria Pavlovskaia
Song-Chun Zhu
Anselm Rothe
Brenden Lake
Todd Gureckis