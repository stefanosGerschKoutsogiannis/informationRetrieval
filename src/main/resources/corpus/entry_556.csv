2016,Ancestral Causal Inference,Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions.  Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising  existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information  drastically reducing computation time. Additionally  we propose a method to score causal predictions based on their confidence. Crucially  our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge.  We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-of-the-art on synthetic data  achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it on a challenging protein data set.,Ancestral Causal Inference

Sara Magliacane

VU Amsterdam & University of Amsterdam

sara.magliacane@gmail.com

Tom Claassen

Radboud University Nijmegen

tomc@cs.ru.nl

Joris M. Mooij

University of Amsterdam

j.m.mooij@uva.nl

Abstract

Constraint-based causal discovery from limited data is a notoriously difﬁcult chal-
lenge due to the many borderline independence test decisions. Several approaches
to improve the reliability of the predictions by exploiting redundancy in the inde-
pendence information have been proposed recently. Though promising  existing
approaches can still be greatly improved in terms of accuracy and scalability. We
present a novel method that reduces the combinatorial explosion of the search space
by using a more coarse-grained representation of causal information  drastically
reducing computation time. Additionally  we propose a method to score causal pre-
dictions based on their conﬁdence. Crucially  our implementation also allows one
to easily combine observational and interventional data and to incorporate various
types of available background knowledge. We prove soundness and asymptotic
consistency of our method and demonstrate that it can outperform the state-of-
the-art on synthetic data  achieving a speedup of several orders of magnitude. We
illustrate its practical feasibility by applying it to a challenging protein data set.

1

Introduction

Discovering causal relations from data is at the foundation of the scientiﬁc method. Traditionally 
cause-effect relations have been recovered from experimental data in which the variable of interest is
perturbed  but seminal work like the do-calculus [16] and the PC/FCI algorithms [23  26] demonstrate
that  under certain assumptions (e.g.  the well-known Causal Markov and Faithfulness assumptions
[23])  it is already possible to obtain substantial causal information by using only observational data.
Recently  there have been several proposals for combining observational and experimental data to
discover causal relations. These causal discovery methods are usually divided into two categories:
constraint-based and score-based methods. Score-based methods typically evaluate models using a
penalized likelihood score  while constraint-based methods use statistical independences to express
constraints over possible causal models. The advantages of constraint-based over score-based methods
are the ability to handle latent confounders and selection bias naturally  and that there is no need
for parametric modeling assumptions. Additionally  constraint-based methods expressed in logic
[2  3  25  8] allow for an easy integration of background knowledge  which is not trivial even for
simple cases in approaches that are not based on logic [1].
Two major disadvantages of traditional constraint-based methods are: (i) vulnerability to errors
in statistical independence test results  which are quite common in real-world applications  (ii) no
ranking or estimation of the conﬁdence in the causal predictions. Several approaches address the
ﬁrst issue and improve the reliability of constraint-based methods by exploiting redundancy in the
independence information [3  8  25]. The idea is to assign weights to the input statements that reﬂect

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

their reliability  and then use a reasoning scheme that takes these weights into account. Several
weighting schemes can be deﬁned  from simple ways to attach weights to single independence
statements [8]  to more complicated schemes to obtain weights for combinations of independence
statements [25  3]. Unfortunately  these approaches have to sacriﬁce either accuracy by using a greedy
method [3  25]  or scalability by formulating a discrete optimization problem on a super-exponentially
large search space [8]. Additionally  the conﬁdence estimation issue is addressed only in limited
cases [17].
We propose Ancestral Causal Inference (ACI)  a logic-based method that provides comparable
accuracy to the best state-of-the-art constraint-based methods (e.g.  [8]) for causal systems with
latent variables without feedback  but improves on their scalability by using a more coarse-grained
representation of causal information. Instead of representing all possible direct causal relations  in
ACI we represent and reason only with ancestral relations (“indirect” causal relations)  developing
specialised ancestral reasoning rules. This representation  though still super-exponentially large 
drastically reduces computation time. Moreover  it turns out to be very convenient  because in
real-world applications the distinction between direct causal relations and ancestral relations is not
always clear or necessary. Given the estimated ancestral relations  the estimation can be reﬁned to
direct causal relations by constraining standard methods to a smaller search space  if necessary.
Furthermore  we propose a method to score predictions according to their conﬁdence. The conﬁdence
score can be thought of as an approximation to the marginal probability of an ancestral relation.
Scoring predictions enables one to rank them according to their reliability  allowing for higher
accuracy. This is very important for practical applications  as the low reliability of the predictions of
constraint-based methods has been a major impediment to their wide-spread use.
We prove soundness and asymptotic consistency under mild conditions on the statistical tests for ACI
and our scoring method. We show that ACI outperforms standard methods  like bootstrapped FCI
and CFCI  in terms of accuracy  and achieves a speedup of several orders of magnitude over [8] on a
synthetic dataset. We illustrate its practical feasibility by applying it to a challenging protein data set
[21] that so far had only been addressed with score-based methods and observe that it successfully
recovers from faithfulness violations. In this context  we showcase the ﬂexibility of logic-based
approaches by introducing weighted ancestral relation constraints that we obtain from a combination
of observational and interventional data  and show that they substantially increase the reliability of
the predictions. Finally  we provide an open-source version of our algorithms and the evaluation
framework  which can be easily extended  at http://github.com/caus-am/aci.

2 Preliminaries and related work

Preliminaries We assume that the data generating process can be modeled by a causal Directed
Acyclic Graph (DAG) that may contain latent variables. For simplicity we also assume that there is
no selection bias. Finally  we assume that the Causal Markov Assumption and the Causal Faithfulness
Assumption [23] both hold. In other words  the conditional independences in the observational
distribution correspond one-to-one with the d-separations in the causal DAG. Throughout the paper
we represent variables with uppercase letters  while sets of variables are denoted by boldface. All
proofs are provided in the Supplementary Material.
A directed edge X → Y in the causal DAG represents a direct causal relation between cause X on
effect Y . Intuitively  in this framework this indicates that manipulating X will produce a change in
Y   while manipulating Y will have no effect on X. A more detailed discussion can be found in [23].
A sequence of directed edges X1 → X2 → ··· → Xn is a directed path. If there exists a directed
path from X to Y (or X = Y )  then X is an ancestor of Y (denoted as X (cid:57)(cid:57)(cid:75) Y ). Otherwise  X is
not an ancestor of Y (denoted as X (cid:54)(cid:57)(cid:57)(cid:75) Y ). For a set of variables W   we write:

We deﬁne an ancestral structure as any non-strict partial order on the observed variables of the DAG 
i.e.  any relation that satisﬁes the following axioms:

(1)

(2)
(3)
(4)

X (cid:57)(cid:57)(cid:75) W := ∃Y ∈ W : X (cid:57)(cid:57)(cid:75) Y 
X (cid:54)(cid:57)(cid:57)(cid:75) W := ∀Y ∈ W : X (cid:54)(cid:57)(cid:57)(cid:75) Y.

(reﬂexivity) : X (cid:57)(cid:57)(cid:75) X 
(transitivity) : X (cid:57)(cid:57)(cid:75) Y ∧ Y (cid:57)(cid:57)(cid:75) Z =⇒ X (cid:57)(cid:57)(cid:75) Z 
(antisymmetry) : X (cid:57)(cid:57)(cid:75) Y ∧ Y (cid:57)(cid:57)(cid:75) X =⇒ X = Y.

2

The underlying causal DAG induces a unique “true” ancestral structure  which represents the transitive
closure of the direct causal relations projected on the observed variables.
For disjoint sets X  Y   W we denote conditional independence of X and Y given W as X ⊥⊥
Y | W   and conditional dependence as X (cid:54)⊥⊥ Y | W . We call the cardinality |W| the order of the
conditional (in)dependence relation. Following [2] we deﬁne a minimal conditional independence by:

X ⊥⊥ Y | W ∪ [Z] := (X ⊥⊥ Y | W ∪ Z) ∧ (X (cid:54)⊥⊥ Y | W ) 

and similarly  a minimal conditional dependence by:

X (cid:54)⊥⊥ Y | W ∪ [Z] := (X (cid:54)⊥⊥ Y | W ∪ Z) ∧ (X ⊥⊥ Y | W ).

The square brackets indicate that Z is needed for the (in)dependence to hold in the context of W . Note
that the negation of a minimal conditional independence is not a minimal conditional dependence.
Minimal conditional (in)dependences are closely related to ancestral relations  as pointed out in [2]:
Lemma 1. For disjoint (sets of) variables X  Y  Z  W :

X ⊥⊥ Y | W ∪ [Z] =⇒ Z (cid:57)(cid:57)(cid:75) ({X  Y } ∪ W ) 
X (cid:54)⊥⊥ Y | W ∪ [Z] =⇒ Z (cid:54)(cid:57)(cid:57)(cid:75) ({X  Y } ∪ W ).

(5)
(6)

Exploiting these rules (as well as others that will be introduced in Section 3) to deduce ancestral
relations directly from (in)dependences is key to the greatly improved scalability of our method.

Related work on conﬂict resolution One of the earliest algorithms to deal with conﬂicting inputs
in constraint-based causal discovery is Conservative PC [18]  which adds “redundant” checks to the
PC algorithm that allow it to detect inconsistencies in the inputs  and then makes only predictions that
do not rely on the ambiguous inputs. The same idea can be applied to FCI  yielding Conservative FCI
(CFCI) [4  10]. BCCD (Bayesian Constraint-based Causal Discovery) [3] uses Bayesian conﬁdence
estimates to process information in decreasing order of reliability  discarding contradictory inputs as
they arise. COmbINE (Causal discovery from Overlapping INtErventions) [25] is an algorithm that
combines the output of FCI on several overlapping observational and experimental datasets into a
single causal model by ﬁrst pooling and recalibrating the independence test p-values  and then adding
each constraint incrementally in order of reliability to a SAT instance. Any constraint that makes the
problem unsatisﬁable is discarded.
Our approach is inspired by a method presented by Hyttinen  Eberhardt and Järvisalo [8] (that
we will refer to as HEJ in this paper)  in which causal discovery is formulated as a constrained
discrete minimization problem. Given a list of weighted independence statements  HEJ searches
for the optimal causal graph G (an acyclic directed mixed graph  or ADMG) that minimizes the
sum of the weights of the independence statements that are violated according to G. In order to
test whether a causal graph G induces a certain independence  the method creates an encoding DAG
of d-connection graphs. D-connection graphs are graphs that can be obtained from a causal graph
through a series of operations (conditioning  marginalization and interventions). An encoding DAG
of d-connection graphs is a complex structure encoding all possible d-connection graphs and the
sequence of operations that generated them from a given causal graph. This approach has been shown
to correct errors in the inputs  but is computationally demanding because of the huge search space.

3 ACI: Ancestral Causal Inference

We propose Ancestral Causal Inference (ACI)  a causal discovery method that accurately reconstructs
ancestral structures  also in the presence of latent variables and statistical errors. ACI builds on HEJ
[8]  but rather than optimizing over encoding DAGs  ACI optimizes over the much simpler (but still
very expressive) ancestral structures.
For n variables  the number of possible ancestral structures is the number of partial orders (http:
//oeis.org/A001035)  which grows as 2n2/4+o(n2) [11]  while the number of DAGs can be
computed with a well-known super-exponential recurrence formula (http://oeis.org/A003024).
The number of ADMGs is | DAG(n)| × 2n(n−1)/2. Although still super-exponential  the number of
ancestral structures grows asymptotically much slower than the number of DAGs and even more so 
ADMGs. For example  for 7 variables  there are 6 × 106 ancestral structures but already 2.3 × 1015
ADMGs  which lower bound the number of encoding DAGs of d-connection graphs used by HEJ.

3

New rules The rules in HEJ explicitly encode marginalization and conditioning operations on
d-connection graphs  so they cannot be easily adapted to work directly with ancestral relations.
Instead  ACI encodes the ancestral reasoning rules (2)–(6) and ﬁve novel causal reasoning rules:
Lemma 2. For disjoint (sets) of variables X  Y  U  Z  W :

(X ⊥⊥ Y | Z) ∧ (X (cid:54)(cid:57)(cid:57)(cid:75) Z) =⇒ X (cid:54)(cid:57)(cid:57)(cid:75) Y 
X (cid:54)⊥⊥ Y | W ∪ [Z] =⇒ X (cid:54)⊥⊥ Z | W  
X ⊥⊥ Y | W ∪ [Z] =⇒ X (cid:54)⊥⊥ Z | W  
(X ⊥⊥ Y | W ∪ [Z]) ∧ (X ⊥⊥ Z | W ∪ U ) =⇒ (X ⊥⊥ Y | W ∪ U ) 
(Z (cid:54)⊥⊥ X | W ) ∧ (Z (cid:54)⊥⊥ Y | W ) ∧ (X ⊥⊥ Y | W ) =⇒ X (cid:54)⊥⊥ Y | W ∪ Z.

(7)
(8)
(9)
(10)
(11)

We prove the soundness of the rules in the Supplementary Material. We elaborate some conjectures
about their completeness in the discussion after Theorem 1 in the next Section.

Optimization of loss function We formulate causal discovery as an optimization problem where
a loss function is optimized over possible causal structures. Intuitively  the loss function sums the
weights of all the inputs that are violated in a candidate causal structure.
Given a list I of weighted input statements (ij  wj)  where ij is the input statement and wj is the
associated weight  we deﬁne the loss function as the sum of the weights of the input statements that
are not satisﬁed in a given possible structure W ∈ W  where W denotes the set of all possible causal
structures. Causal discovery is formulated as a discrete optimization problem:

W ∗ = arg min
W∈W

L(W ; I) 

(cid:88)

wj 

(12)

(13)

L(W ; I) :=

(ij  wj )∈I: W∪R|=¬ij

where W ∪ R |= ¬ij means that input ij is not satisﬁed in structure W according to the rules R.
This general formulation includes both HEJ and ACI  which differ in the types of possible structures
W and the rules R. In HEJ W represents all possible causal graphs (speciﬁcally  acyclic directed
mixed graphs  or ADMGs  in the acyclic case) and R are operations on d-connection graphs. In ACI
W represent ancestral structures (deﬁned with the rules(2)-(4)) and the rules R are rules (5)–(11).

Constrained optimization in ASP The constrained optimization problem in (12) can be imple-
mented using a variety of methods. Given the complexity of the rules  a formulation in an expressive
logical language that supports optimization  e.g.  Answer Set Programming (ASP)  is very convenient.
ASP is a widely used declarative programming language based on the stable model semantics [12  7]
that has successfully been applied to several NP-hard problems. For ACI we use the state-of-the-art
ASP solver clingo 4 [6]. We provide the encoding in the Supplementary Material.

Weighting schemes ACI supports two types of input statements: conditional independences and
ancestral relations. These statements can each be assigned a weight that reﬂects their conﬁdence. We
propose two simple approaches with the desirable properties of making ACI asymptotically consistent
under mild assumptions (as described in the end of this Section)  and assigning a much smaller weight
to independences than to dependences (which agrees with the intuition that one is conﬁdent about a
measured strong dependence  but not about independence vs. weak dependence). The approaches are:
• a frequentist approach  in which for any appropriate frequentist statistical test with indepen-

dence as null hypothesis (resp. a non-ancestral relation)  we deﬁne the weight:

w = | log p − log α|  where p = p-value of the test  α = signiﬁcance level (e.g.  5%);
• a Bayesian approach  in which the weight of each input statement i using data set D is:

(14)

p(i)
p(¬i)
where the prior probability p(i) can be used as a tuning parameter.

w = log

= log

p(i|D)
p(¬i|D)

p(D|i)
p(D|¬i)

 

(15)

4

Given observational and interventional data  in which each intervention has a single known target (in
particular  it is not a fat-hand intervention [5])  a simple way to obtain a weighted ancestral statement
X (cid:57)(cid:57)(cid:75) Y is with a two-sample test that tests whether the distribution of Y changes with respect to
its observational distribution when intervening on X. This approach conveniently applies to various
types of interventions: perfect interventions [16]  soft interventions [14]  mechanism changes [24] 
and activity interventions [15]. The two-sample test can also be implemented as an independence test
that tests for the independence of Y and IX  the indicator variable that has value 0 for observational
samples and 1 for samples from the interventional distribution in which X has been intervened upon.

4 Scoring causal predictions

The constrained minimization in (12) may produce several optimal solutions  because the underlying
structure may not be identiﬁable from the inputs. To address this issue  we propose to use the loss
function (13) and score the conﬁdence of a feature f (e.g.  an ancestral relation X (cid:57)(cid:57)(cid:75) Y ) as:

W∈W L(W ; I ∪ {(f ∞)}).

(16)

Without going into details here  we note that the conﬁdence (16) can be interpreted as a MAP
approximation of the log-odds ratio of the probability that feature f is true in a Markov Logic model:

C(f ) = min

W∈W L(W ; I ∪ {(¬f ∞)}) − min
(cid:80)
(cid:80)
W∈W e−L(W ;I)1W∪R|=f
W∈W e−L(W ;I)1W∪R|=¬f

P(f | I R)
P(¬f | I R)

=

≈ maxW∈W e−L(W ;I∪{(f ∞)})
maxW∈W e−L(W ;I∪{(¬f ∞)})

= eC(f ).

In this paper  we usually consider the features f to be ancestral relations  but the idea is more generally
applicable. For example  combined with HEJ it can be used to score direct causal relations.

Soundness and completeness Our scoring method is sound for oracle inputs:
Theorem 1. Let R be sound (not necessarily complete) causal reasoning rules. For any feature f 
the conﬁdence score C(f ) of (16) is sound for oracle inputs with inﬁnite weights.
Here  soundness means that C(f ) = ∞ if f is identiﬁable from the inputs  C(f ) = −∞ if ¬f
is identiﬁable from the inputs  and C(f ) = 0 otherwise (neither are identiﬁable). As features  we
can consider for example ancestral relations f = X (cid:57)(cid:57)(cid:75) Y for variables X  Y . We conjecture that
the rules (2)–(11) are “order-1-complete”  i.e.  they allow one to deduce all (non)ancestral relations
that are identiﬁable from oracle conditional independences of order ≤ 1 in observational data. For
higher-order inputs additional rules can be derived. However  our primary interest in this work is
improving computation time and accuracy  and we are willing to sacriﬁce completeness. A more
detailed study of the completeness properties is left as future work.

(cid:26)−∞ H1

Asymptotic consistency Denote the number of samples by N. For the frequentist weights in (14) 
we assume that the statistical tests are consistent in the following sense:

log pN − log αN

P→

(17)
as N → ∞  where the null hypothesis H0 is independence/nonancestral relation and the alternative
hypothesis H1 is dependence/ancestral relation. Note that we need to choose a sample-size dependent
threshold αN such that αN → 0 at a suitable rate. Kalisch and Bühlmann [9] show how this can be
done for partial correlation tests under the assumption that the distribution is multivariate Gaussian.
For the Bayesian weighting scheme in (15)  we assume that for N → ∞ 

+∞ H0 

(cid:26)−∞ if i is true

+∞ if i is false.

P→

wN

(18)

This will hold (as long as there is no model misspeciﬁcation) under mild technical conditions for
ﬁnite-dimensional exponential family models. In both cases  the probability of a type I or type II
error will converge to 0  and in addition  the corresponding weight will converge to ∞.
Theorem 2. Let R be sound (not necessarily complete) causal reasoning rules. For any feature f 
the conﬁdence score C(f ) of (16) is asymptotically consistent under assumption (17) or (18).
Here  “asymptotically consistent” means that the conﬁdence score C(f ) → ∞ in probability if f is
identiﬁably true  C(f ) → −∞ in probability if f is identiﬁably false  and C(f ) → 0 in probability
otherwise.

5

Average execution time (s)

1
4
1
1
1

HEJ
n c ACI
12.09
0.21
6
432.67
1.66
6
715.74
7
1.03
≥ 2500
9.74
8
146.66 (cid:29) 2500
9
(a)

BAFCI BACFCI
8.39
11.10
9.37
13.71
18.28

12.51
16.36
15.12
21.71
28.51

(b)

Figure 1: Execution time comparison on synthetic data for the frequentist test on 2000 synthetic
models: (a) average execution time for different combinations of number of variables n and max.
order c; (b) detailed plot of execution times for n = 7  c = 1 (logarithmic scale).

5 Evaluation

In this section we report evaluations on synthetically generated data and an application on a real
dataset. Crucially  in causal discovery precision is often more important than recall. In many real-
world applications  discovering a few high-conﬁdence causal relations is more useful than ﬁnding
every possible causal relation  as reﬂected in recently proposed algorithms  e.g.  [17].

Compared methods We compare the predictions of ACI and of the acyclic causally insufﬁcient
version of HEJ [8]  when used in combination with our scoring method (16). We also evaluate two
standard methods: Anytime FCI [22  26] and Anytime CFCI [4]  as implemented in the pcalg R
package [10]. We use the anytime versions of (C)FCI because they allow for independence test
results up to a certain order. We obtain the ancestral relations from the output PAG using Theorem
3.1 from [20]. (Anytime) FCI and CFCI do not rank their predictions  but only predict the type of
relation: ancestral (which we convert to +1)  non-ancestral (-1) and unknown (0). To get a scoring of
the predictions  we also compare with bootstrapped versions of Anytime FCI and Anytime CFCI.
We perform the bootstrap by repeating the following procedure 100 times: sample randomly half
of the data  perform the independence tests  run Anytime (C)FCI. From the 100 output PAGs we
extract the ancestral predictions and average them. We refer to these methods as BA(C)FCI. For a
fair comparison  we use the same independence tests and thresholds for all methods.

Synthetic data We simulate the data using the simulator from HEJ [8]: for each experimental
condition (e.g.  a given number of variables n and order c)  we generate randomly M linear acyclic
models with latent variables and Gaussian noise and sample N = 500 data points. We then perform
independence tests up to order c and weight the (in)dependence statements using the weighting
schemes described in Section 3. For the frequentist weights we use tests based on partial correlations
and Fisher’s z-transform to obtain approximate p-values (see  e.g.  [9]) with signiﬁcance level
α = 0.05. For the Bayesian weights  we use the Bayesian test for conditional independence presented
in [13] as implemented by HEJ with a prior probability of 0.1 for independence.
In Figure 1(a) we show the average execution times on a single core of a 2.80GHz CPU for different
combinations of n and c  while in Figure 1(b) we show the execution times for n = 7  c = 1  sorting
the execution times in ascending order. For 7 variables ACI is almost 3 orders of magnitude faster
than HEJ  and the difference grows exponentially as n increases. For 8 variables HEJ can complete
only four of the ﬁrst 40 simulated models before the timeout of 2500s. For reference we add the
execution time for bootstrapped anytime FCI and CFCI.
In Figure 2 we show the accuracy of the predictions with precision-recall (PR) curves for both
ancestral (X (cid:57)(cid:57)(cid:75) Y ) and nonancestral (X (cid:54)(cid:57)(cid:57)(cid:75) Y ) relations  in different settings. In this Figure  for
ACI and HEJ all of the results are computed using frequentist weights and  as in all evaluations  our
scoring method (16). While for these two methods we use c = 1  for (bootstrapped) (C)FCI we use
all possible independence test results (c = n − 2). In this case  the anytime versions of FCI and CFCI
are equivalent to the standard versions of FCI and CFCI. Since the overall results are similar  we
report the results with the Bayesian weights in the Supplementary Material.
In the ﬁrst row of Figure 2  we show the setting with n = 6 variables. The performances of HEJ
and ACI coincide  performing signiﬁcantly better for nonancestral predictions and the top ancestral

6

0.1	1	10	100	1000	10000	1	101	201	301	401	501	601	701	801	901	1001	1101	1201	1301	1401	1501	1601	1701	1801	1901	Execution	time	(s)	Instances	(sorted	by	solution	time)	HEJ	ACI	(a) PR ancestral: n=6

(b) PR ancestral: n=6 (zoom)

(c) PR nonancestral: n=6

(d) PR ancestral: n=8

(e) PR ancestral: n=8 (zoom)

(f) PR nonancestral: n=8

Figure 2: Accuracy on synthetic data for the two prediction tasks (ancestral and nonancestral relations)
using the frequentist test with α = 0.05. The left column shows the precision-recall curve for ancestral
predictions  the middle column shows a zoomed-in version in the interval (0 0.02)  while the right
column shows the nonancestral predictions.

predictions (see zoomed-in version in Figure 2(b)). This is remarkable  as HEJ and ACI use only
independence test results up to order c = 1  in contrast with (C)FCI which uses independence test
results of all orders. Interestingly  the two discrete optimization algorithms do not seem to beneﬁt
much from higher order independence tests  thus we omit them from the plots (although we add the
graphs in the Supplementary Material). Instead  bootstrapping traditional methods  oblivious to the
(in)dependence weights  seems to produce surprisingly good results. Nevertheless  both ACI and HEJ
outperform bootstrapped FCI and CFCI  suggesting these methods achieve nontrivial error-correction.
In the second row of Figure 2  we show the setting with 8 variables. In this setting HEJ is too slow. In
addition to the previous plot  we plot the accuracy of ACI when there is oracle background knowledge
on the descendants of one variable (i = 1). This setting simulates the effect of using interventional
data  and we can see that the performance of ACI improves signiﬁcantly  especially in the ancestral
preditions. The performance of (bootstrapped) FCI and CFCI is limited by the fact that they cannot
take advantage of this background knowledge  except with complicated postprocessing [1].

Application on real data We consider the challenging task of reconstructing a signalling network
from ﬂow cytometry data [21] under different experimental conditions. Here we consider one
experimental condition as the observational setting and seven others as interventional settings. More
details and more evaluations are reported in the Supplementary Material. In contrast to likelihood-
based approaches like [21  5  15  19]  in our approach we do not need to model the interventions
quantitatively. We only need to know the intervention targets  while the intervention types do not
matter. Another advantage of our approach is that it takes into account possible latent variables.
We use a t-test to test for each intervention and for each variable whether its distribution changes
with respect to the observational condition. We use the p-values of these tests as in (14) in order to
obtain weighted ancestral relations that are used as input (with threshold α = 0.05). For example  if
adding U0126 (a MEK inhibitor) changes the distribution of RAF signiﬁcantly with respect to the
observational baseline  we get a weighted ancestral relation MEK(cid:57)(cid:57)(cid:75)RAF. In addition  we use partial
correlations up to order 1 (tested in the observational data only) to obtain weighted independences
used as input. We use ACI with (16) to score the ancestral relations for each ordered pair of variables.
The main results are illustrated in Figure 3  where we compare ACI with bootstrapped anytime CFCI

7

Recall00.050.10.150.2Precision0.30.40.50.60.70.80.91Bootstrapped (100) CFCIBootstrapped (100) FCIHEJ (c=1)ACI (c=1)Standard CFCIStandard FCIRecall00.0050.010.0150.02Precision0.60.650.70.750.80.850.90.951Recall00.20.40.60.81Precision0.860.880.90.920.940.960.981Bootstrapped (100) CFCIBootstrapped (100) FCIHEJ (c=1)ACI (c=1)Standard CFCIStandard FCIRecall00.050.10.150.2Precision0.30.40.50.60.70.80.91Bootstrapped (100) CFCIBootstrapped (100) FCIACI (c=1)ACI (c=1  i=1)Standard CFCIStandard FCIRecall00.0050.010.0150.02Precision0.30.40.50.60.70.80.91Recall00.20.40.60.81Precision0.90.910.920.930.940.950.960.970.980.991Bootstrapped (100) CFCIBootstrapped (100) FCIACI (c=1)ACI (c=1  i=1)Standard CFCIStandard FCI(a) Bootstrapped (100) any-
time CFCI (input:
indepen-
dences of order ≤ 1)

(b) ACI (input: weighted an-
cestral relations)

(c) ACI (input: independences
of order ≤ 1  weighted ances-
tral relations)

Figure 3: Results for ﬂow cytometry dataset. Each matrix represents the ancestral relations  where
each row represents a cause and each column an effect. The colors encode the conﬁdence levels:
green is positive  black is unknown  while red is negative. The intensity of the color represents the
degree of conﬁdence. For example  ACI identiﬁes MEK to be a cause of RAF with high conﬁdence.

under different inputs. The output for boostrapped anytime FCI is similar  so we report it only in
the Supplementary Material. Algorithms like (anytime) (C)FCI can only use the independences in
the observational data as input and therefore miss the strongest signal  weighted ancestral relations 
which are obtained by comparing interventional with observational data. In the Supplementary
Material  we compare also with other methods ([17]  [15]). Interestingly  as we show there  our
results are similar to the best acyclic model reconstructed by the score-based method from [15]. As for
other constraint-based methods  HEJ is computationally unfeasible in this setting  while COMBINE
assumes perfect interventions (while this dataset contains mostly activity interventions).
Notably  our algorithms can correctly recover from faithfulness violations (e.g.  the independence
between MEK and ERK)  because they take into account the weight of the input statements (the weight
of the independence is considerably smaller than that of the ancestral relation  which corresponds
with a quite signiﬁcant change in distribution). In contrast  methods that start by reconstructing the
skeleton  like (anytime) (C)FCI  would decide that MEK and ERK are nonadjacent  and are unable to
recover from that erroneous decision. This illustrates another advantage of our approach.

6 Discussion and conclusions

As we have shown  ancestral structures are very well-suited for causal discovery. They offer a
natural way to incorporate background causal knowledge  e.g.  from experimental data  and allow a
huge computational advantage over existing representations for error-correcting algorithms  such as
[8]. When needed  ancestral structures can be mapped to a ﬁner-grained representation with direct
causal relations  as we sketch in the Supplementary Material. Furthermore  conﬁdence estimates on
causal predictions are extremely helpful in practice  and can signiﬁcantly boost the reliability of the
output. Although standard methods  like bootstrapping (C)FCI  already provide reasonable estimates 
methods that take into account the conﬁdence in the inputs  as the one presented here  can lead to
further improvements of the reliability of causal relations inferred from data.
Strangely (or fortunately) enough  neither of the optimization methods seems to improve much with
higher order independence test results. We conjecture that this may happen because our loss function
essentially assumes that the test results are independent from another (which is not true). Finding a
way to take this into account in the loss function may further improve the achievable accuracy  but
such an extension may not be straightforward.

Acknowledgments

SM and JMM were supported by NWO  the Netherlands Organization for Scientiﬁc Research
(VIDI grant 639.072.410). SM was also supported by the Dutch programme COMMIT/ under the
Data2Semantics project. TC was supported by NWO grant 612.001.202 (MoCoCaDi)  and EU-FP7
grant agreement n.603016 (MATRICS). We also thank Soﬁa Triantaﬁllou for her feedback  especially
for pointing out the correct way to read ancestral relations from a PAG.

8

RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKBCFCI (indep. <= 1)RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKACI (ancestral relations)RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKACI (ancestral r. + indep. <= 1)RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKWeighted causes(i j) RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNK−1000−50005001000Weighted indep(i j) RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNK−1000−50005001000Consensus graph RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNK−1000−50005001000ACI (causes) RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNK−1000−50005001000ACI (causes + indeps) RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNK−1000−50005001000FCI RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNK−1000−50005001000CFCI RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNK−1000−50005001000Acyclic Joris graph RafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNKRafMekPLCgPIP2PIP3ErkAktPKAPKCp38JNK−1000−50005001000References
[1] G. Borboudakis and I. Tsamardinos. Incorporating causal prior knowledge as path-constraints in Bayesian

networks and Maximal Ancestral Graphs. In ICML  pages 1799–1806  2012.

[2] T. Claassen and T. Heskes. A logical characterization of constraint-based causal discovery. In UAI  pages

135–144  2011.

[3] T. Claassen and T. Heskes. A Bayesian approach to constraint-based causal inference. In UAI  pages

207–216  2012.

[4] D. Colombo  M. H. Maathuis  M. Kalisch  and T. S. Richardson. Learning high-dimensional directed

acyclic graphs with latent and selection variables. The Annals of Statistics  40(1):294–321  2012.

[5] D. Eaton and K. Murphy. Exact Bayesian structure learning from uncertain interventions. In AISTATS 

pages 107–114  2007.

[6] M. Gebser  R. Kaminski  B. Kaufmann  and T. Schaub. Clingo = ASP + control: Extended report.
Technical report  University of Potsdam  2014. http://www.cs.uni-potsdam.de/wv/pdfformat/
gekakasc14a.pdf.

[7] M. Gelfond. Answer sets. In Handbook of Knowledge Representation  pages 285–316. 2008.

[8] A. Hyttinen  F. Eberhardt  and M. Järvisalo. Constraint-based causal discovery: Conﬂict resolution with

Answer Set Programming. In UAI  pages 340–349  2014.

[9] M. Kalisch and P. Bühlmann. Estimating high-dimensional directed acyclic graphs with the PC-algorithm.

Journal of Machine Learning Research  8:613–636  2007.

[10] M. Kalisch  M. Mächler  D. Colombo  M. Maathuis  and P. Bühlmann. Causal inference using graphical

models with the R package pcalg. Journal of Statistical Software  47(1):1–26  2012.

[11] D. J. Kleitman and B. L. Rothschild. Asymptotic enumeration of partial orders on a ﬁnite set. Transactions

of the American Mathematical Society  205:205–220  1975.

[12] V. Lifschitz. What is Answer Set Programming? In AAAI  pages 1594–1597  2008.

[13] D. Margaritis and F. Bromberg. Efﬁcient Markov network discovery using particle ﬁlters. Computational

Intelligence  25(4):367–394  2009.

[14] F. Markowetz  S. Grossmann  and R. Spang. Probabilistic soft interventions in conditional Gaussian

networks. In AISTATS  pages 214–221  2005.

[15] J. M. Mooij and T. Heskes. Cyclic causal discovery from continuous equilibrium data. In UAI  pages

431–439  2013.

[16] J. Pearl. Causality: models  reasoning and inference. Cambridge University Press  2009.

[17] J. Peters  P. Bühlmann  and N. Meinshausen. Causal inference using invariant prediction: identiﬁcation

and conﬁdence intervals. Journal of the Royal Statistical Society  Series B  8(5):947–1012  2015.

[18] J. Ramsey  J. Zhang  and P. Spirtes. Adjacency-faithfulness and conservative causal inference. In UAI 

pages 401–408  2006.

[19] D. Rothenhäusler  C. Heinze  J. Peters  and N. Meinshausen. BACKSHIFT: Learning causal cyclic graphs

from unknown shift interventions. In NIPS  pages 1513–1521  2015.

[20] A. Roumpelaki  G. Borboudakis  S. Triantaﬁllou  and I. Tsamardinos. Marginal causal consistency in

constraint-based causal learning. In Causation: Foundation to Application Workshop  UAI  2016.

[21] K. Sachs  O. Perez  D. Pe’er  D. Lauffenburger  and G. Nolan. Causal protein-signaling networks derived

from multiparameter single-cell data. Science  308:523–529  2005.

[22] P. Spirtes. An anytime algorithm for causal inference. In AISTATS  pages 121–128  2001.

[23] P. Spirtes  C. Glymour  and R. Scheines. Causation  Prediction  and Search. MIT press  2000.

[24] J. Tian and J. Pearl. Causal discovery from changes. In UAI  pages 512–521  2001.

[25] S. Triantaﬁllou and I. Tsamardinos. Constraint-based causal discovery from multiple interventions over

overlapping variable sets. Journal of Machine Learning Research  16:2147–2205  2015.

[26] J. Zhang. On the completeness of orientation rules for causal discovery in the presence of latent confounders

and selection bias. Artiﬁcal Intelligence  172(16-17):1873–1896  2008.

9

,Sara Magliacane
Tom Claassen
Joris Mooij