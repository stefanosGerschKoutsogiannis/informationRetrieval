2016,Learning feed-forward one-shot learners,One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning  which are very effective in other learning scenarios  are ill-suited for one-shot learning as they need large amounts of training data. In this paper  we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network  called a learnet  which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner  trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible  we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot  and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.,Learning feed-forward one-shot learners

Luca Bertinetto∗
University of Oxford

luca@robots.ox.ac.uk

João F. Henriques∗
University of Oxford

joao@robots.ox.ac.uk

Jack Valmadre∗
University of Oxford

jvlmdr@robots.ox.ac.uk

Philip H. S. Torr
University of Oxford

philip.torr@eng.ox.ac.uk

Abstract

Andrea Vedaldi

University of Oxford

vedaldi@robots.ox.ac.uk

One-shot learning is usually tackled by using generative models or discriminative
embeddings. Discriminative methods based on deep learning  which are very
effective in other learning scenarios  are ill-suited for one-shot learning as they
need large amounts of training data. In this paper  we propose a method to learn the
parameters of a deep model in one shot. We construct the learner as a second deep
network  called a learnet  which predicts the parameters of a pupil network from
a single exemplar. In this manner we obtain an efﬁcient feed-forward one-shot
learner  trained end-to-end by minimizing a one-shot classiﬁcation objective in
a learning to learn formulation. In order to make the construction feasible  we
propose a number of factorizations of the parameters of the pupil network. We
demonstrate encouraging results by learning characters from single exemplars in
Omniglot  and by tracking visual objects from a single initial exemplar in the Visual
Object Tracking benchmark.

1

Introduction

Deep learning methods have taken by storm areas such as computer vision  natural language process-
ing and speech recognition. One of their key strengths is the ability to leverage large quantities of
labelled data and extract meaningful and powerful representations from it. However  this capability is
also one of their most signiﬁcant limitations since using large datasets to train deep neural network is
not just an option  but a necessity. It is well known  in fact  that these models are prone to overﬁtting.
Thus  deep networks seem less useful when the goal is to learn a new concept on the ﬂy  from a few or
even a single example as in one shot learning. These problems are usually tackled by using generative
models [18  13] or  in a discriminative setting  using ad-hoc solutions such as exemplar support vector
machines (SVMs) [14]. Perhaps the most common discriminative approach to one-shot learning is to
learn off-line a deep embedding function and then to deﬁne on-line simple classiﬁcation rules such as
nearest neighbors in the embedding space [5  16]. However  computing an embedding is a far cry
from learning a model of the new object.
In this paper  we take a very different approach and ask whether we can induce  from a single
supervised example  a full  deep discriminative model to recognize other instances of the same object
class. Furthermore  we do not want our solution to require a lengthy optimization process  but to be
computable on-the-ﬂy  efﬁciently and in one go. We formulate this problem as the one of learning a
deep neural network  called a learnet  that  given a single exemplar of a new object class  predicts the
parameters of a second network that can recognize other objects of the same type.

∗The ﬁrst three authors contributed equally  and are listed in alphabetical order.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Our model has several elements of interest. Firstly  if we consider learning to be any process that maps
a set of images to the parameters of a model  then it can be seen as a “learning to learn” approach.
Clearly  learning from a single exemplar is only possible given sufﬁcient prior knowledge on the
learning domain. This prior knowledge is incorporated in the learnet in an off-line phase by solving
millions of small one-shot learning tasks and back-propagating errors end-to-end. Secondly  our
learnet provides a feed-forward learning algorithm that extracts from the available exemplar the ﬁnal
model parameters in one go. This is different from iterative approaches such as exemplar SVMs or
complex inference processes in generative modeling. It also demonstrates that deep neural networks
can learn at the “meta-level” of predicting ﬁlter parameters for a second network  which we consider
to be an interesting result in its own right. Thirdly  our method provides a competitive  efﬁcient  and
practical way of performing one-shot learning using discriminative methods.

1.1 Related work
Our work is related to several others in the literature. However  we believe to be the ﬁrst to look at
methods that can learn the parameters of complex discriminative models in one shot.
One-shot learning has been widely studied in the context of generative modeling  which unlike our
work is often not focused on solving discriminative tasks. One very recent example is by Rezende et
al. [18]  which uses a recurrent spatial attention model to generate images  and learns by optimizing a
measure of reconstruction error using variational inference [9]. They demonstrate results by sampling
images of novel classes from this generative model  not by solving discriminative tasks. Another
notable work is by Lake et al. [13]  which instead uses a probabilistic program as a generative model.
This model constructs written characters as compositions of pen strokes  so although more general
programs can be envisioned  they demonstrate it only on Optical Character Recognition (OCR)
applications.
A different approach to one-shot learning is to learn an embedding space  which is typically done
with a siamese network [2]. Given an exemplar of a novel category  classiﬁcation is performed in
the embedding space by a simple rule such as nearest-neighbor. Training is usually performed by
classifying pairs according to distance [5]  or by enforcing a distance ranking with a triplet loss [16].
Our work departs from the paradigms of generative modeling and similarity learning  instead predict-
ing the parameters of a neural network from a single exemplar image. It can be seen as a network
that effectively “learns to learn”  generalizing across tasks deﬁned by different exemplars.
The idea of parameter prediction was  to our knowledge  ﬁrst explored by Schmidhuber [20] in a
recurrent architecture with one network that modiﬁes the weights of another. Parameter prediction
has also been used for zero-shot learning (as opposed to one-shot learning)  which is the related
problem of learning a new object class without a single example image  based solely on a description
such as binary attributes or text. Whereas it is usually framed as a modality transfer problem and
solved through transfer learning [21]  Noh et al. [15] recently employed parameter prediction to
induce the weights of an image classiﬁer from text for the problem of visual question answering.
Denil et al. [4] investigated the redundancy of neural network parameters  showing that it is possible
to linearly predict as many as 95% of the parameters in a layer given the remaining 5%. This is a
vastly different proposition from ours  which is to predict all of the parameters of a layer given an
external exemplar image  and to do so non-linearly.

2 One-shot learning as dynamic parameter prediction

Since we consider one-shot learning as a discriminative task  our starting point is standard discrimi-
native learning. It generally consists of ﬁnding the parameters W that minimize the average loss L of
a predictor function ϕ(x; W )  computed over a dataset of n samples xi and corresponding labels (cid:96)i:

n(cid:88)

i=1

min
W

1
n

L(ϕ(xi; W )  (cid:96)i).

(1)

Unless the model space is very small  generalization also requires constraining the choice of model 
usually via regularization. However  in the extreme case in which the goal is to learn W from a single
exemplar z of the class of interest  called one-shot learning  even regularization may be insufﬁcient
and additional prior information must be injected into the learning process. The main challenge in

2

Figure 1: Our proposed architectures predict the parameters of a network from a single example 
replacing static convolutions (green) with dynamic convolutions (red). The siamese learnet predicts
the parameters of an embedding function that is applied to both inputs  whereas the single-stream
learnet predicts the parameters of a function that is applied to the other input. Linear layers are
denoted by ∗ and nonlinear layers by σ. Dashed connections represent parameter sharing.

discriminative one-shot learning is to ﬁnd a mechanism to incorporate domain-speciﬁc information in
the learner  i.e. learning to learn. Another challenge  which is of practical importance in applications
of one-shot learning  is to avoid a lengthy optimization process such as eq. (1).
We propose to address both challenges by learning the parameters W of the predictor from a single
exemplar z using a meta-prediction process  i.e. a non-iterative feed-forward function ω that maps
(z; W (cid:48)) to W . Since in practice this function will be implemented using a deep neural network  we
call it a learnet. The learnet depends on the exemplar z  which is a single representative of the class of
interest  and contains parameters W (cid:48) of its own. Learning to learn can now be posed as the problem of
optimizing the learnet meta-parameters W (cid:48) using an objective function deﬁned below. Furthermore 
the feed-forward learnet evaluation is much faster than solving the optimization problem (1).
In order to train the learnet  we require the latter to produce good predictors given any possible
exemplar z  which is empirically evaluated as an average over n training samples zi:

L(ϕ(xi; ω(zi; W (cid:48)))  (cid:96)i).

(2)

n(cid:88)

i=1

min
W (cid:48)

1
n

In this expression  the performance of the predictor extracted by the learnet from the exemplar zi is
assessed on a single “validation” pair (xi  (cid:96)i)  comprising another exemplar and its label (cid:96)i. Hence 
the training data consists of triplets (xi  zi  (cid:96)i). Notice that the meaning of the label (cid:96)i is subtly
different from eq. (1) since the class of interest changes depending on the exemplar zi: (cid:96)i is positive
when xi and zi belong to the same class and negative otherwise. Triplets are sampled uniformly with
respect to these two cases. Importantly  the parameters of the original predictor ϕ of eq. (1) now
change dynamically with each exemplar zi.
Note that the training data is reminiscent of that of siamese networks [2]  which also learn from
labeled sample pairs. However  siamese networks apply the same model ϕ(x; W ) with shared
weights W to both xi and zi  and compute their inner-product to produce a similarity score:

L((cid:104)ϕ(xi; W )  ϕ(zi; W )(cid:105)  (cid:96)i).

(3)

n(cid:88)

i=1

min
W

1
n

There are two key differences with our model. First  we treat xi and zi asymmetrically  which results
in a different objective function. Second  and most importantly  the output of ω(z; W (cid:48)) is used to
parametrize linear layers that determine the intermediate representations in the network ϕ. This is
signiﬁcantly different to computing a single inner product in the last layer (eq. (3)).
Eq. (2) speciﬁes the optimization objective of one-shot learning as dynamic parameter prediction.
By application of the chain rule  backpropagating derivatives through the computational blocks
of ϕ(x; W ) and ω(z; W (cid:48)) is no more difﬁcult than through any other standard deep network.
Nevertheless  when we dive into concrete implementations of such models we face a peculiar
challenge  discussed next.

2.1 The challenge of naive parameter prediction
In order to analyse the practical difﬁculties of implementing a learnet  we will begin with one-shot
prediction of a fully-connected layer  as it is simpler to analyse. This is given by

y = W x + b 

3

(4)

siamesesiamese learnetlearnetFigure 2: Factorized convolutional layer (eq. (8)). The channels of the input x are projected to the
factorized space by M (a 1 × 1 convolution)  the resulting channels are convolved independently
with a corresponding ﬁlter prediction from w(z)  and ﬁnally projected back using M(cid:48).
given an input x ∈ Rd  output y ∈ Rk  weights W ∈ Rk×d and biases b ∈ Rk.
We now replace the weights and biases with their functional counterparts  w(z) and b(z)  representing
two outputs of the learnet ω(z; W (cid:48)) given the exemplar z ∈ Rm as input (to avoid clutter  we omit
the implicit dependence on W (cid:48)):

y = w(z)x + b(z).

(5)
While eq. (5) seems to be a drop-in replacement for linear layers  careful analysis reveals that it scales
extremely poorly. The main cause is the unusually large output space of the learnet w : Rm → Rk×d.
For a comparable number of input and output units in a linear layer (d (cid:39) k)  the output space of the
learnet grows quadratically with the number of units.
While this may seem to be a concern only for large networks  it is actually extremely difﬁcult also for
networks with few units. Consider a simple linear learnet w(z) = W (cid:48)z. Even for a very small fully-
connected layer of only 100 units (d = k = 100)  and an exemplar z with 100 features (m = 100) 
the learnet already contains 1M parameters that must be learned. Overﬁtting and space and time
costs make learning such a regressor infeasible. Furthermore  reducing the number of features in the
exemplar can only achieve a small constant-size reduction on the total number of parameters. The
bottleneck is the quadratic size of the output space dk  not the size of the input space m.

y = M(cid:48) diag (w(z)) M x + b(z).

2.2 Factorized linear layers
A simple way to reduce the size of the output space is to consider a factorized set of weights  by
replacing eq. (5) with:

(6)
The product M(cid:48)diag (w(z)) M can be seen as a factorized representation of the weights  analogous
to the Singular Value Decomposition. The matrix M ∈ Rd×d projects x into a space where the
elements of w(z) represent disentangled factors of variation. The second projection M(cid:48) ∈ Rk×d
maps the result back from this space.
Both M and M(cid:48) contain additional parameters to be learned  but they are modest in size compared to
the case discussed in sect. 2.1. Importantly  the one-shot branch w(z) now only has to predict a set of
diagonal elements (see eq. (6))  so its output space grows linearly with the number of units in the
layer (i.e. w(z): Rm → Rd).
2.3 Factorized convolutional layers
The factorization of eq. (6) can be generalized to convolutional layers as follows. Given an input
tensor x ∈ Rr×c×d  weights W ∈ Rf×f×d×k (where f is the ﬁlter support size)  and biases b ∈ Rk 
the output y ∈ Rr(cid:48)×c(cid:48)×k of the convolutional layer is given by

y = W ∗ x + b 

where ∗ denotes convolution  and the biases b are applied to each of the k channels.
Projections analogous to M and M(cid:48) in eq. (6) can be incorporated in the ﬁlter bank in different ways
and it is not obvious which one to pick. Here we take the view that M and M(cid:48) should disentangle
the feature channels (i.e. third dimension of x) so that the predicted ﬁlters w(z) can operate on each
channel independently. As such  we consider the following factorization:

(7)

(8)

y = M(cid:48) ∗ w(z) ∗d M ∗ x + b(z) 

4

𝑤(𝑧) 𝑀 𝑀′ 𝑦 𝑥 z x

Predicted ﬁlters w(z)

···
···
···

···
···
···

Activations

Figure 3: The predicted ﬁlters and the output of a dynamic convolutional layer in a single-stream
learnet trained for the OCR task. Different exemplars z deﬁne different ﬁlters w(z). Applying the
ﬁlters of each exemplar to the same input x yields different responses. Best viewed in colour.

···
···
···

Activations

···
···
···

z

x

Predicted ﬁlters w(z)

Figure 4: The predicted ﬁlters and the output of a dynamic convolutional layer in a siamese learnet
trained for the object tracking task. Best viewed in colour.

where M ∈ R1×1×d×d  M(cid:48) ∈ R1×1×d×k  and w(z) ∈ Rf×f×d. Convolution with subscript d
denotes independent ﬁltering of d channels  i.e. each channel of x ∗d y is simply the convolution of
the corresponding channel in x and y. In practice  this can be achieved with ﬁlter tensors that are
diagonal in the third and fourth dimensions  or using d ﬁlter groups [12]  each group containing a
single ﬁlter. An illustration is given in ﬁg. 2. The predicted ﬁlters w(z) can be interpreted as a ﬁlter
basis  as described in the supplementary material (sec. A).
Notice that  under this factorization  the number of elements to be predicted by the one-shot branch
w(z) is only f 2d (the ﬁlter size f is typically very small  e.g. 3 or 5 [5  23]). Without the factorization 
it would be f 2dk (the number of elements of W in eq. (7)). Similarly to the case of fully-connected
layers (sect. 2.2)  when d (cid:39) k this keeps the number of predicted elements from growing quadratically
with the number of channels  allowing them to grow only linearly.
Examples of ﬁlters that are predicted by learnets are shown in ﬁgs. 3 and 4. The resulting activations
conﬁrm that the networks induced by different exemplars do indeed possess different internal
representations of the same input.

3 Experiments

We evaluate learnets against baseline one-shot architectures (sect. 3.1) on two one-shot learning
problems in Optical Character Recognition (OCR; sect. 3.2) and visual object tracking (sect. 3.3).
All experiments were performed using MatConvNet [22].

3.1 Architectures

As noted in sect. 2  the closest competitors to our method in discriminative one-shot learning are
embedding learning using siamese architectures. Therefore  we structure the experiments to compare
against this baseline. In particular  we choose to implement learnets using similar network topologies
for a fairer comparison.
The baseline siamese architecture comprises two parallel streams ϕ(x; W ) and ϕ(z; W ) composed
of a number of layers  such as convolution  max-pooling  and ReLU  sharing parameters W (ﬁg. 1.a).
The outputs of the two streams are compared by a layer Γ(ϕ(x; W )  ϕ(z; W )) computing a measure
of similarity or dissimilarity. We consider in particular: the dot product (cid:104)a  b(cid:105) between vectors a and
b  the Euclidean distance (cid:107)a − b(cid:107)  and the weighted l1-norm (cid:107)w (cid:12) a − w (cid:12) b(cid:107)1 where w is a vector
of learnable weights and (cid:12) the Hadamard product).
The ﬁrst modiﬁcation to the siamese baseline is to use a learnet to predict some of the intermediate
shared stream parameters (ﬁg. 1.b). In this case W = ω(z; W (cid:48)) and the siamese architecture writes
Γ(ϕ(x; ω(z; W (cid:48)))  ϕ(z; ω(z; W (cid:48)))). Note that the siamese parameters are still the same in the two

5

Table 1: Error rate for character recognition in foreign alphabets (chance is 95%).

Inner-product (%) Euclidean dist. (%) Weighted (cid:96)1 dist. (%)

Siamese (shared)
Siamese (unshared)
Siamese (unshared  fact.)
Siamese learnet (shared)
Learnet
Modiﬁed Hausdorff distance

48.5
47.0
48.4
51.0
43.7

37.3
41.0

–

39.8
36.7

43.2

41.8
34.6
33.6
31.4
28.6

streams  whereas the learnet is an entirely new subnetwork whose purpose is to map the exemplar
image to the shared weights. We call this model the siamese learnet.
The second modiﬁcation is a single-stream learnet conﬁguration  using only one stream ϕ of the
siamese architecture and predicting its parameter using the learnet ω. In this case  the comparison
block Γ is reinterpreted as the last layer of the stream ϕ (ﬁg. 1.c). Note that: i) the single predicted
stream and learnet are asymmetric and with different parameters and ii) the learnet predicts both the
ﬁnal comparison layer parameters Γ as well as intermediate ﬁlter parameters.
The single-stream learnet architecture can be understood to predict a discriminant function from one
example  and the siamese learnet architecture to predict an embedding function for the comparison
of two images. These two variants demonstrate the versatility of the dynamic convolutional layer
from eq. (6).
Finally  in order to ensure that any difference in performance is not simply due to the asymmetry of
the learnet architecture or to the induced ﬁlter factorizations (sect. 2.2 and sect. 2.3)  we also compare
unshared siamese nets  which use distinct parameters for each stream  and factorized siamese nets 
where convolutions are replaced by factorized convolutions as in learnet.

3.2 Character recognition in foreign alphabets

This section describes our experiments in one-shot learning on OCR. For this  we use the Omniglot
dataset [13]  which contains images of handwritten characters from 50 different alphabets. These
alphabets are divided into 30 background and 20 evaluation alphabets. The associated one-shot
learning problem is to develop a method for determining whether  given any single exemplar of a
character in an evaluation alphabet  any other image in that alphabet represents the same character or
not. Importantly  all methods are trained using only background alphabets and tested on the evaluation
alphabets.
Dataset and evaluation protocol. Character images are resized to 28 × 28 pixels in order to be able
to explore efﬁciently several variants of the proposed architectures. There are exactly 20 sample
images for each character  and an average of 32 characters per alphabet. The dataset contains a total
of 19 280 images in the background alphabets and 13 180 in the evaluation alphabets.
Algorithms are evaluated on a series of recognition problems. Each recognition problem involves
identifying the image in a set of 20 that shows the same character as an exemplar image (there is
always exactly one match). All of the characters in a single problem belong to the same alphabet.
At test time  given a collection of characters (x1  . . .   xm)  the function is evaluated on each pair
(z  xi) and the candidate with the highest score is declared the match. In the case of the learnet
architectures  this can be interpreted as obtaining the parameters W = ω(z; W (cid:48)) and then evaluating
a static network ϕ(xi; W ) for each xi.
Architecture. The baseline stream ϕ for the siamese  siamese learnet  and single-stream learnet
architecture consists of 3 convolutional layers  with 2 × 2 max-pooling layers of stride 2 between
them. The ﬁlter sizes are 5× 5× 1× 16  5× 5× 16× 64 and 4× 4× 64× 512. For both the siamese
learnet and the single-stream learnet  ω consists of the same layers as ϕ  except the number of outputs
is 1600 – one for each element of the 64 predicted ﬁlters (of size 5 × 5). To keep the experiments
simple  we only predict the parameters of one convolutional layer. We conducted cross-validation to
choose the predicted layer and found that the second convolutional layer yields the best results for
both of the proposed variants.
Siamese nets have previously been applied to this problem by Koch et al. [10] using much deeper
networks applied to images of size 105 × 105. However  we have restricted this investigation to
relatively shallow networks to enable a thorough exploration of the parameter space. A more powerful

6

algorithm for one-shot learning  Hierarchical Bayesian Program Learning [13]  is able to achieve
human-level performance. However  this approach involves computationally expensive inference at
test time  and leverages extra information at training time that describes the strokes drawn by the
human author.
Learning. Learning involves minimizing the objective function speciﬁc to each method (e.g. eq. (2)
for learnet and eq. (3) for siamese architectures) and uses stochastic gradient descent (SGD) in all
cases. As noted in sect. 2  the objective is obtained by sampling triplets (zi  xi  (cid:96)i) where exemplars
zi and xi are congruous ((cid:96)i = +1) or incongruous ((cid:96)i = −1) with 50% probability. We consider
100 000 random pairs for training per epoch  and train for 60 epochs. We conducted a random
search to ﬁnd the best hyper-parameters for each algorithm (initial learning rate and geometric decay 
standard deviation of Gaussian parameter initialization  and weight decay).
Results and discussion. Tab. 1 shows the classiﬁcation error obtained using variants of each
architecture. A dash indicates a failure to converge given a large range of hyper-parameters. The two
learnet architectures combined with the weighted (cid:96)1 distance are able to achieve signiﬁcantly better
results than other methods. The best architecture reduced the error from 37.3% for a siamese network
with shared parameters to 28.6% for a single-stream learnet.
While the Euclidean distance gave the best results for siamese networks with shared parameters 
better results were achieved by learnets (and siamese networks with unshared parameters) using a
weighted (cid:96)1 distance. In fact  none of the alternative architectures are able to achieve lower error
under the Euclidean distance than the shared siamese net. The dot product was  in general  less
effective than the other two metrics.
The introduction of the factorization in the convolutional layer might be expected to improve the
quality of the estimated model by reducing the number of parameters  or to worsen it by diminishing
the capacity of the hypothesis space. For this relatively simple task of character recognition  the
factorization did not seem to have a large effect.

3.3 Object tracking

The task of single-target object tracking requires to locate an object of interest in a sequence of video
frames. A video frame can be seen as a collection F = {w1  . . .   wK} of image windows; then  in a
one-shot setting  given an exemplar z ∈ F1 of the object in the ﬁrst frame F1  the goal is to identify
the same window in the other frames F2  . . .  FM .
Datasets. The method is trained using the ImageNet Large Scale Visual Recognition Challenge
2015 [19]  with 3 862 videos totalling more than one million annotated frames. Instances of objects
of thirty different classes (mostly vehicles and animals) are annotated throughout each video with
bounding boxes. For tracking  instance labels are retained but object class labels are ignored. We use
90% of the videos for training  while the other 10% are held-out to monitor validation error during
network training. Testing uses the VOT 2015 benchmark [11].
Architecture. We experiment with siamese and siamese learnet architectures (ﬁg. 1) where the
learnet ω predicts the parameters of the second (dynamic) convolutional layer of the siamese streams.
Each siamese stream has ﬁve convolutional layers and we test three variants of those: variant (A) has
the same conﬁguration as AlexNet [12] but with stride 2 in the ﬁrst layer  and variants (B) and (C)
reduce to 50% the number of ﬁlters in the ﬁrst two convolutional layers and  respectively  to 25% and
12.5% the number of ﬁlters in the last layer.
Training. In order to train the architecture efﬁciently from many windows  the data is prepared
as follows. Given an object bounding box sampled at random  a crop z double the size of that is
extracted from the corresponding frame  padding with the average image color when needed. The
border is included in order to incorporate some visual context around the exemplar object. Next 
(cid:96) ∈ {+1 −1} is sampled at random with 75% probability of being positive. If (cid:96) = −1  an image x
is extracted by choosing at random a frame that does not contain the object. Otherwise  a second
frame containing the same object and within 50 temporal samples of the ﬁrst is selected at random.
From that  a patch x centered around the object and four times bigger is extracted. In this way  x
contains both subwindows that do and do not match z. Images z and x are resized to 127 × 127 and
255 × 255 pixels  respectively  and the triplet (z  x  (cid:96)) is formed. All 127 × 127 subwindows in x are
considered to not match z except for the central 2 × 2 ones when (cid:96) = +1.

7

Table 2: Tracking accuracy and number of tracking failures in the VOT 2015 Benchmark  as reported
by the toolkit [11]. Architectures are grouped by size of the main network (see text). For each group 
the best entry for each column is in bold. We also report the scores of 5 recent trackers.

Accuracy Failures

Accuracy Failures

Method
Siamese (ϕ=B)
Siamese (ϕ=B; unshared)
Siamese (ϕ=B; factorized)
Siamese learnet (ϕ=B; ω=A)
Siamese learnet (ϕ=B; ω=B)
DAT [17]
SO-DLT [23]

0.465
0.447
0.444
0.500
0.497
0.442
0.540

105
131
138
87
93
113
108

Method
Siamese (ϕ=C)
Siamese (ϕ=C; factorized)
Siamese learnet (ϕ=C; ω=A)
Siamese learnet (ϕ=C; ω=C)
DSST [3]
MEEM [24]
MUSTer [7]

0.466
0.435
0.483
0.491
0.483
0.458
0.471

120
132
105
106
163
107
132

All networks are trained from scratch using SGD for 50 epoch of 50 000 sample triplets (zi  xi  (cid:96)i).
The multiple windows contained in x are compared to z efﬁciently by making the comparison layer
Γ convolutional (ﬁg. 1)  accumulating a logistic loss across spatial locations. The same hyper-
parameters (learning rate of 10−2 geometrically decaying to 10−5  weight decay of 0.005  and small
mini-batches of size 8) are used for all experiments  which we found to work well for both the baseline
and proposed architectures. The weights are initialized using the improved Xavier [6] method  and
we use batch normalization [8] after all linear layers.
Testing. Adopting the initial crop as exemplar  the object is sought in a new frame within a radius of
the previous position  proceeding sequentially. This is done by evaluating the pupil net convolutionally 
as well as searching at ﬁve possible scales in order to track the object through scale space. The
approach is described in more detail in Bertinetto et al. [1].
Results and discussion. Tab. 2 compares the methods in terms of the ofﬁcial metrics (accuracy and
number of failures) for the VOT 2015 benchmark [11]. The ranking plot produced by the VOT toolkit
is provided in the supplementary material (ﬁg. B.1). From tab. 2  it can be observed that factorizing
the ﬁlters in the siamese architecture signiﬁcantly diminishes its performance  but using a learnet to
predict the ﬁlters in the factorization recovers this gap and in fact achieves better performance than
the original siamese net. The performance of the learnet architectures is not adversely affected by
using the slimmer prediction networks B and C (with less channels).
An elementary tracker based on learnet compares favourably against recent tracking systems  which
make use of different features and online model update strategies: DAT [17]  DSST [3]  MEEM [24] 
MUSTer [7] and SO-DLT [23]. SO-DLT in particular is a good example of direct adaptation of
standard batch deep learning methodology to online learning  as it uses SGD during tracking to
ﬁne-tune an ensemble of deep convolutional networks. However  the online adaptation of the model
comes at a big computational cost and affects the speed of the method  which runs at 5 frames-per-
second (FPS) on a GPU. Due to the feed-forward nature of our one-shot learnets  they can track
objects in real-time at framerates in excess of 60 FPS  while achieving less tracking failures. We
consider  however  that our implementation serves mostly as a proof-of-concept  using tracking as an
interesting demonstration of one-shot-learning  and is orthogonal to many technical improvements
found in the tracking literature [11].

4 Conclusions

In this work  we have shown that it is possible to obtain the parameters of a deep neural network
using a single  feed-forward prediction from a second network. This approach is desirable when
iterative methods are too slow  and when large sets of annotated training samples are not available. We
have demonstrated the feasibility of feed-forward parameter prediction in two demanding one-shot
learning tasks in OCR and visual tracking. Our results hint at a promising avenue of research in
“learning to learn” by solving millions of small discriminative problems in an ofﬂine phase. Possible
extensions include domain adaptation and sharing a single learnet between different pupil networks.

Acknowledgements

This research was supported by Apical Ltd. and ERC grants ERC-2012-AdG 321162-HELIOS 
HELIOS-DFR00200 and “Integrated and Detailed Image Understanding” (EP/L024683/1).

8

References

[1] L. Bertinetto  J. Valmadre  J. F. Henriques  A. Vedaldi  and P. H. S. Torr. Fully-convolutional

siamese networks for object tracking. 2016.

[2] J. Bromley  J. W. Bentz  L. Bottou  I. Guyon  Y. LeCun  C. Moore  E. Säckinger  and R. Shah.
Signature veriﬁcation using a “siamese” time delay neural network. International Journal of
Pattern Recognition and Artiﬁcial Intelligence  1993.

[3] M. Danelljan  G. Häger  F. Khan  and M. Felsberg. Accurate scale estimation for robust visual

tracking. In BMVC  2014.

[4] M. Denil  B. Shakibi  L. Dinh  N. de Freitas  et al. Predicting parameters in deep learning. In

NIPS  2013.

[5] H. Fan  Z. Cao  Y. Jiang  Q. Yin  and C. Doudou. Learning deep face representation. arXiv 

2014.

[6] K. He  X. Zhang  S. Ren  and J. Sun. Delving deep into rectiﬁers: Surpassing human-level

performance on ImageNet classiﬁcation. In ICCV  2015.

[7] Z. Hong  Z. Chen  C. Wang  X. Mei  D. Prokhorov  and D. Tao. Multi-store tracker (MUSTER):

A cognitive psychology inspired approach to object tracking. In CVPR  2015.

[8] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv  2015.

[9] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv  2013.
[10] G. Koch  R. Zemel  and R. Salakhutdinov. Siamese neural networks for one-shot image

recognition. In ICML 2015 Deep Learning Workshop  2016.

[11] M. Kristan  J. Matas  A. Leonardis  M. Felsberg  L. Cehovin  G. Fernandez  T. Vojir  G. Hager 

G. Nebehay  and R. Pﬂugfelder. The VOT2015 challenge results. In ICCV Workshop  2015.

[12] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet classiﬁcation with deep convolutional

neural networks. In NIPS  2012.

[13] B. M. Lake  R. Salakhutdinov  and J. B. Tenenbaum. Human-level concept learning through

probabilistic program induction. Science  350(6266):1332–1338  2015.

[14] T. Malisiewicz  A. Gupta  and A. A. Efros. Ensemble of exemplar-SVMs for object detection

and beyond. In ICCV  2011.

[15] H. Noh  P. Hongsuck Seo  and B. Han. Image question answering using convolutional neural

network with dynamic parameter prediction. In CVPR  2016.

[16] O. M. Parkhi  A. Vedaldi  and A. Zisserman. Deep face recognition. BMVC  2015.
[17] H. Possegger  T. Mauthner  and H. Bischof. In defense of color-based model-free tracking. In

CVPR  2015.

[18] D. J. Rezende  S. Mohamed  I. Danihelka  K. Gregor  and D. Wierstra. One-shot generalization

in deep generative models. arXiv  2016.

[19] O. Russakovsky  J. Deng  H. Su  J. Krause  S. Satheesh  S. Ma  Z. Huang  A. Karpathy 
A. Khosla  M. Bernstein  A. C. Berg  and L. Fei-Fei. ImageNet Large Scale Visual Recognition
Challenge. IJCV  2015.

[20] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent

networks. Neural Computation  4(1):131–139  1992.

[21] R. Socher  M. Ganjoo  C. D. Manning  and A. Ng. Zero-shot learning through cross-modal

transfer. In NIPS  2013.

[22] A. Vedaldi and K. Lenc. MatConvNet – Convolutional Neural Networks for MATLAB. In

Proceedings of the ACM Int. Conf. on Multimedia  2015.

[23] N. Wang  S. Li  A. Gupta  and D.-Y. Yeung. Transferring rich feature hierarchies for robust

visual tracking. arXiv  2015.

[24] J. Zhang  S. Ma  and S. Sclaroff. MEEM: Robust tracking via multiple experts using entropy

minimization. In ECCV. 2014.

9

,Brian McWilliams
David Balduzzi
Joachim Buhmann
Arthur Guez
Nicolas Heess
David Silver
Peter Dayan
Luca Bertinetto
João Henriques
Jack Valmadre
Philip Torr
Andrea Vedaldi