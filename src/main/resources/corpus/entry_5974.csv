2019,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization,The adaptive momentum method (AdaMM)  which uses past gradients to update descent directions and learning rates simultaneously  has become one of the most popular first-order optimization methods for solving machine learning  problems. However   AdaMM is not suited for solving black-box optimization problems  where explicit gradient forms are difficult or infeasible to obtain. In this paper  we propose a zeroth-order  AdaMM (ZO-AdaMM) algorithm  that generalizes AdaMM to the gradient-free regime. We show that the convergence rate of ZO-AdaMM for  both  convex and nonconvex optimization is roughly a factor of $O(\sqrt{d})$ worse than that of the first-order AdaMM algorithm  where $d$ is problem size. In particular  we provide a deep understanding on why  Mahalanobis distance matters in convergence of ZO-AdaMM and other AdaMM-type methods. As a byproduct  our analysis   makes the first step toward understanding adaptive learning rate methods for nonconvex constrained optimization.Furthermore  we demonstrate two applications  designing  per-image and universal adversarial attacks from black-box neural networks  respectively. We perform extensive experiments on ImageNet and empirically show that  ZO-AdaMM converges much faster to a solution of high accuracy compared with  $6$ state-of-the-art ZO optimization methods.,ZO-AdaMM: Zeroth-Order Adaptive Momentum

Method for Black-Box Optimization

Xiangyi Chen1 ∗ Sijia Liu2 ∗ Kaidi Xu3 ∗ Xingguo Li4 ∗

Xue Lin3 Mingyi Hong1 David Cox2

1University of Minnesota  USA

2MIT-IBM Watson AI Lab  IBM Research  USA

3Northeastern University  USA

4Princeton University  USA

Abstract

The adaptive momentum method (AdaMM)  which uses past gradients to update
descent directions and learning rates simultaneously  has become one of the most
popular ﬁrst-order optimization methods for solving machine learning problems.
However  AdaMM is not suited for solving black-box optimization problems 
where explicit gradient forms are difﬁcult or infeasible to obtain. In this paper 
we propose a zeroth-order AdaMM (ZO-AdaMM) algorithm  that generalizes
AdaMM to the gradient-free regime. We show that the convergence rate of ZO-

AdaMM for both convex and nonconvex optimization is roughly a factor of O(√d)
worse than that of the ﬁrst-order AdaMM algorithm  where d is problem size. In
particular  we provide a deep understanding on why Mahalanobis distance matters
in convergence of ZO-AdaMM and other AdaMM-type methods. As a byproduct 
our analysis makes the ﬁrst step toward understanding adaptive learning rate
methods for nonconvex constrained optimization. Furthermore  we demonstrate
two applications  designing per-image and universal adversarial attacks from black-
box neural networks  respectively. We perform extensive experiments on ImageNet
and empirically show that ZO-AdaMM converges much faster to a solution of high
accuracy compared with 6 state-of-the-art ZO optimization methods.

1

Introduction

The development of gradient-free optimization methods has become increasingly important to solve
many machine learning problems in which explicit expressions of the gradients are expensive or
infeasible to obtain [1–7]. Zeroth-Order (ZO) optimization methods  one type of gradient-free
optimization methods  mimic ﬁrst-order (FO) methods but approximate the full gradient (or stochastic
gradient) through random gradient estimates  given by the difference of function values at random
query points [8  9]. Compared to Bayesian optimization  derivative-free trust region methods 
genetic algorithms and other types of gradient-free methods [10–13]  ZO optimization has two main
advantages: a) ease of implementation  via slight modiﬁcation of commonly-used gradient-based
algorithms  and b) comparable convergence rates to ﬁrst-order algorithms.

Due to the stochastic nature of ZO optimization  which arises from both data sampling and random
gradient estimation  existing ZO methods suffer from large variance of the noisy gradient compared
to FO stochastic methods [14]. In practice  this causes poor convergence performance and/or function
query efﬁciency. To partially mitigate these issues  ZO sign-based SGD (ZO-signSGD) was proposed
by [14] with the rationale that taking the sign of random gradient estimates (i.e.  normalizing gradient
estimates elementwise) as the descent direction improves the robustness of gradient estimators

∗Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

to stochastic noise. Although ZO-signSGD has faster convergence speed than many existing ZO
algorithms  it is only guaranteed to converge to a neighborhood of a solution. In the FO setting 
taking the sign of a stochastic gradient as the descent direction gives rise to signSGD [15]. The use
of sign of stochastic gradients also appears in adaptive momentum methods (AdaMM) such as Adam
[16]  RMSProp [17]  AMSGrad [18]  Padam [19]  and AdaFom [20]. Indeed  it has been suggested
by [21] that AdaMM enjoy dual advantages of sign descent and variance adaption.

Considering the motivation of ZO-signSGD and the success of AdaMM in FO optimization  one
question arises: Can we generalize AdaMM to the ZO regime? To answer this question  we develop
the zeroth-order adaptive momentum method (ZO-AdaMM) and analyze its convergence properties
in both convex and nonconvex settings for constrained optimization.

Contributions Theoretically  for both convex and nonconvex optimization  we show that ZO-

AdaMM is roughly a factor of O(√d) worse than that of the FO AdaMM algorithm  where d is the

number of optimization variables. We also show that the Euclidean projection based AdaMM-type
methods could suffer non-convergence issues for constrained optimization. This highlights the
necessity of Mahalanobis distance based projection. And we establish the Mahalanobis distance
based convergence analysis  which makes the ﬁrst step toward understanding adaptive learning rate
methods for nonconvex constrained optimization.

Practically  we formalize the experimental comparison of ZO-AdaMM with 6 state-of-the-art ZO
algorithms in the application of black-box adversarial attacks to generate both per-image and universal
adversarial perturbations. Our proposal could provide an experimental benchmark for future studies
on ZO optimization. Code to reproduce experiments is released at the link https://github.com/
KaidiXu/ZO-AdaMM.

Related work Many types of ZO algorithms have been developed  and their convergence rates have
been rigorously studied under different problem settings. We highlight some recent works as below.
For unconstrained stochastic optimization  ZO stochastic gradient descent (ZO-SGD) [9] and ZO

stochastic coordinate descent (ZO-SCD) [22] were proposed  which have O(√d/√T ) convergence
rate  where T is the number of iterations. Compared to FO stochastic algorithms  ZO optimization
suffers a slowdown dependent on the variable dimension d  e.g.  O(√d) for ZO-SGD and ZO-SCD.
In [23]  the tightness of the dimension-dependent factor O(√d) has been proved in the framework

of ZO stochastic mirror descent (ZO-SMD). In order to further improve the iteration complexity of
ZO algorithms  the technique of variance reduction was applied to ZO-SGD and ZO-SCD  leading
to ZO stochastic variance reduced algorithms with an improved convergence rate in T   namely 
O(d/T ) [24–26]. This improvement is aligned with ZO gradient descent (ZO-GD) for deterministic
nonconvex programming [8]. Moreover  ZO versions of proximal SGD (ProxSGD) [27]  Frank-Wolfe
(FW) [28  2  29]  and online alternating direction method of multipliers (OADMM) [1  30] have been
developed for constrained optimization. Aside from the recent works on ZO algorithms mentioned
before  there is rich literature in derivative-free optimization (DFO). Traditional DFO methods can
be classiﬁed into direct search-based methods and model-based methods. Both the two types of
methods are mostly iterative methods. The difference is that direct search-based methods reﬁne their
search directions based on the queried function values directly  while a model-based method builds
a model that approximates the function to be optimized and updates the search direction based on
the model. Representative methods developed in DFO literature include NOMAD [31  32]  PSWarm
[33]  Cobyla [34]  and BOBYQA [35]. More comprehensive discussions on DFO methods can be
found in [36  37].

2 Preliminaries: Gradient Estimation via ZO Oracle

The ZO gradient estimate of a function f is constructed by the forward difference of two function
values at a random unit direction:

ˆ∇f (x) = (d/µ)[f (x + µu) − f (x)]u 

(1)

where u is a random vector drawn uniformly from the sphere of a unit ball  and µ > 0 is a small step
size  known as the smoothing parameter. In many existing work such as [8  9]  the random direction
vector u was drawn from the standard Gaussian distribution. Here the use of uniform distribution
ensures that the ZO gradient estimate (1) is deﬁned in a bounded space rather than the whole real
space required for Gaussian. As will be evident later  the boundedness of random gradient estimates
is one of important conditions in the convergence analysis of ZO-AdaMM.

2

The rationale behind the ZO gradient estimate (1) is that although it is a biased approximation to
the true gradient of f   it is unbiased to the gradient of the randomized smoothing version of f with
parameter µ [23  24  30]  i.e. 

fµ(x) =Eu∼UB [f (x + µu)] 

(2)

where u ∼ UB denotes the uniform distribution over the unit Euclidean ball B. We review properties
of the smoothing function (2) and connections to the ZO gradient estimator (1) in Appendix 1.

3 AdaMM from First to Zeroth Order

Consider a stochastic optimization problem of the generic form

min
x∈X

f (x) = Eξ[f (x; ξ)] 

(3)

where x ∈ Rd are optimization variables  X is a closed convex set  f is a differentiable (possibly
nonconvex) objective function  and ξ is a certain random variable that captures environmental
uncertainties. In problem (3)  if ξ obeys a uniform distribution built on empirical samples {ξi}n
i=1 
then we recover a ﬁnite-sum formulation with the objective function f (x) = 1
i=1 f (x; ξi).

n Pn

First-order AdaMM in terms of AMSGrad [18]. We specify the algorithmic framework of
AdaMM by AMSGrad [18]  a modiﬁed version of Adam [16] with convergence guarantees for
both convex and nonconvex optimization. In the algorithm  the descent direction mt is given by
an exponential moving average of the past gradients. The learning rate rt is adaptively penalized
by a square root of exponential moving averages of squared past gradients. It has been proved in
[18  20  38  39] that AdaMM can reach O(1/√T )2 convergence rate. Here we omit its possible
dependency on d for simplicity  but more accurate analysis will be provided later in Section 4 and 5.

ZO-AdaMM. By integrating AdaMM with
the random gradient estimator (1)  we obtain
ZO-AdaMM in Algorithm 1. Here the square
root  the square  the maximum  and the divi-
sion operators are taken elementwise. Also 
ΠX  H(a) denotes the projection operation un-
der Mahalanobis distance with respect to H 
i.e.  arg minx∈X k√H(x − a)k2
2. If X = Rd 
the projection step simpliﬁes to xt+1 = xt −
t mt. Clearly  αt ˆV−1/2
αt ˆV−1/2
and mt can be
interpreted as the adaptive learning rate and the
momentum-type descent direction  which adopt
exponential moving averages as follows 

t

Algorithm 1 ZO-AdaMM

Input: x1 ∈ X   step sizes {αt}T
(0  1]  and set m0  v0 and ˆv0
for t = 1  2  . . .   T do

t=1  β1 t  β2 ∈

let ˆgt = ˆ∇ft(xt) by (1)  ft(xt) := f (xt; ξt)
mt = β1 tmt−1 + (1 − β1 t)ˆgt
vt = β2vt−1 + (1 − β2)ˆg2
ˆvt = max(ˆvt−1  vt)  and ˆVt = diag(ˆvt)
xt+1 = Π

t mt)

t

X  √ ˆVt

(xt − αt ˆV−1/2

end for

mt =

t

Xj=1" t−j
Yk=1

β1 t−k+1! (1 − β1 j)ˆgj#   vt = (1 − β2)

t

Xj=1

(βt−j

2

ˆg2
j ).

(4)

Here we assume that m0 = 0  v0 = 0 and 00 = 1 by convention  and let ˆgt = ˆ∇ft(xt) by (1) with
ft(xt) := f (xt; ξt).
Motivation and rationale behind ZO-AdaMM. First  gradient normalization helps noise reduction
in ZO optimization as shown by [6  14]. In the similar spirit  ZO-AdaMM also normalizes the descent

t mt = mt/√vt = ˆgt/pˆg2

direction mt by √ˆvt. Particularly  compared to AdaMM  ZO-AdaMM prefers a small value of
β2 in practice  implying a strong favor to normalize the current gradient estimate; see Fig A1 in
Appendix. In the extreme case of β1 t = β2 → 0 and ˆvt = vt  ZO-AdaMM could reduce to ZO-
signSGD [14] since ˆV−1/2
t = sign(ˆgt) known from (4). However  the
downside of ZO-signSGD is its worse convergence accuracy than ZO-SGD  i.e.  it only converges to
a neighborhood of a stationary point even for unconstrained optimization. Compared to ZO-signSGD 
ZO-AdaMM is able to cover ZO-SGD as a special case when β1 t = 0  β2 = 1  v0 = 1 and ˆv0 ≤ 1
from Algorithm 1. Thus  we hope that with appropriate choices of β1 t and β2  ZO-AdaMM could
enjoy dual advantages of ZO-signSGD and ZO-SGD. Another motivation comes from the possible
presence of time-dependent gradient priors [40]. Given this  the use of past gradients in momentum
also helps noise reduction.

2In the paper  we could omit log(T ) in Big O notation.

3

Why is ZO-AdaMM difﬁcult to analyze? The convergence analysis of ZO-AdaMM becomes
signiﬁcantly more challenging than existing ZO methods due to the involved coupling among
stochastic sampling  ZO gradinet estimation  momentum  adaptive learning rate  and projection
operation. In particular  the use of Mahalanobis distance in projection step plays a key role on
convergence guarantees. And the conventional variance bound on ZO gradient estimates is insufﬁcient
to analyze the convergence of ZO-AdaMM due to the use of adaptive learning rate. In the next
sections  we will carefully study the convergence of ZO-AdaMM under different settings.

4 Convergence Analysis of ZO-AdaMM for Nonconvex Optimization

In this section  we begin by providing a deep understanding on the importance of Mahalanobis
distance used in ZO-AdaMM (Algorithm 1)  and then introduce the Mahalanobis distance based
convergence analysis for both unconstrained and constrained nonconvex optimization. Our analysis
makes the ﬁrst step toward understanding adaptive learning rate methods for nonconvex constrained
optimization. Throughout the section  we make the following assumptions.
A1: ft(·) := f (·; ξt) has Lg-Lipschitz continuous gradient  where Lg > 0.
A2: ft has η-bounded stochastic gradient k∇ft(x)k∞ ≤ η.

4.1

Importance of Mahalanobis distance based projection operation

(·) onto the con-
Recall from Algorithm 1 that ZO-AdaMM takes the projection operation Π
straint set X under Mahalanobis distance with respect to (w.r.t.) ˆVt. In some recent adversarial
learning algorithms [41  42]  the Euclidean projection ΠX (·) was used in both FO and ZO AdaMM-
type methods rather than the Mahalanobis distance based projection in Algorithm 1. However  such
an implementation could lead to non-convergence: Proposition 1 shows the non-convergence issue of
Algorithm 1 using the Euclidean projection operation when solving a simple linear program subject
to ℓ1-norm constraint. This is an important point which is ignored in design of many algorithms on
adversarial training [43].

X  √ ˆVt

Proposition 1 Consider the following problem
x=[x1 x2]T −2x1 − x2;
minimize

subject to |x1 + x2| ≤ 1 

(5)

then Algorithm 1  initialized by x = [0.5  0.5]T   using the Euclidean projection ΠX (·) converges to a
ﬁxed point [0.5  0.5]T rather than a stationary point of (5).

Proof: The proof investigates a special case of Algorithm 1  projected signSGD; See Appendix 2.1.

Proposition 1 indicates that replacing the Mahalanobis distance based projection in Algorithm 1
with Euclidean projection will lead to a divergent algorithm  highlighting the importance of using
Mahalanobis distance. However  the use of Mahalanobis distance based projection complicates the
convergence analysis  especially in constrained optimization. Accordingly  we deﬁne a Mahalanobis
based convergence measure that can simplify the analysis and can be converted into the traditional
convergence measure.

Let x+ = xt+1  x− = xt  g = mt  ω = αt and H = ˆV
be written in the generic form

1/2
t

  the projection step of Algorithm 1 can

x+ = arg min

x∈X {hg  xi + (1/ω)DH(x  x−)} 

(6)

where DH(x  x−) = kH1/2(x− x−)k2/2 gives the Mahalanobis distance w.r.t. H  and k·k denotes
ℓ2 norm. Based on (6)  the concept of gradient mapping [27] is given by

PX  H(x−  g  ω) := (x− − x+)/ω.

(7)

The gradient mapping PX  H(x−  g  ω) yields a natural interpretation: a projected version of g at the
point x− given the learning rate ω  yielding x+ = x− − ωPX  H(x−1  g  ω). We note that different
from [27  44]  the gradient mapping in (7) is deﬁned on the projection under the Mahalanobis distance
DH(· ·) rather than the Euclidean distance.

4

With the aid of (7)  we propose the Mahalanobis distance based convergence measure for ZO-AdaMM:

kG(xt)k2 := k ˆV1/4

t PX   ˆV

1/2
t

(xt  ∇f (xt)  αt)k2.

If X = Rd  then the convergence measure (8) reduces to
t ∇f (xt)k2 

k ˆV

−1/4

(8)

(9)

which corresponds to the squared Euclidean norm of gradient in a linearly transformed coordinate
1/4
system yt = ˆV
t xt. As will be evident later  the measure (9) can be transformed to the conventional
measure k∇f (xt)k2 for unconstrained optimization.

x-descent step given by Algorithm 1 as β1 t = 0 and X = Rd: xt+1 = xt − α ˆV−1/2
that the ZO case is more involved but follows the same intuition. Upon deﬁning yt   ˆV
the x-update can then be rewritten as the update rule in y: yt+1 = yt − α ˆV−1/4
∇yt f (xt) = ( ∂xt

We remark that Mahalanobis (M-) distance facilitates our convergence analysis in an equivalently
transformed space  over which the analysis can be generalized from the conventional projected
gradient descent framework. To get intuition  let us consider a simpler ﬁrst-order case with the
t ∇f (xt). Note
1/4
t xt 
t ∇f (xt). Since
t ∇f (xt)  the y-update  yt+1 = yt − α∇yf (xt)  obeys the
gradient descent framework. In the constrained case  a similar but more involved analysis can be
made  showing that the M-projection in the x-coordinate system is equivalent to the Euclidean
projection in the y-coordinate system which makes projected gradient descent applicable to the
update in y. By contrast  the direct use of Euclidean projection in the x-coordinate system leads to
divergence in ZO-AdaMM (Proposition 1).

)T∇f (xt) = ˆV−1/4

∂yt

4.2 Unconstrained nonconvex optimization

We next demonstrate the convergence analysis of ZO-AdaMM for unconstrained nonconvex opti-
mization. In Proposition 2  we begin by exploring the relationship between the convergence measure
(9) and ZO gradient estimates; See Appendix 2.2 for proof.

Proposition 2 Suppose that A1-A2 hold and let X = Rd  ˆv
0 ≥ c1  fµ(x1) − minx fµ(x) ≤ Df  
β1 t = β1  γ := β1/β2 < 1  µ = 1/√T d  and αt = 1/√T d in Algorithm 1  then ZO-AdaMM yields

1/2

ˆV

−1/4

E(cid:20)(cid:13)(cid:13)(cid:13)

√d
√T
η maxt∈[T ]{kˆgtk∞}
E(cid:20)2η2 +
where xR is picked uniformly randomly from {xt}T

R ∇f (xR)(cid:13)(cid:13)(cid:13)

2(cid:21) ≤

Lg(4 + 5β2

1 − β1

+ 2Df

L2
g
2c

d
T

+

2
c

+

2(1 − β1)2(1 − β2)(1 − γ)

1 )(1 − β1)
(cid:21) d

T

 

t=1  and ˆgt = ˆ∇ft(xt) by (1).

√d
√T

(10)

Proposition 2 implies that the convergence rate of ZO-AdaMM has a dependency on ZO gradient
estimates in terms of Gzo := maxt∈[T ]{kˆgtk∞}. Moreover  if we consider the FO AdaMM [20  38]
in which the ZO gradient estimate ˆgt is replaced with the stochastic gradient  then one can simply
assume maxt∈[T ]{kgtk∞} to be a dimension-independent constant under A2. However  in the
ZO setting  Gzo is no longer independent of d. For example  it could be directly bounded by
k ˆ∇f (x)k2 ≤ (d/µ)kf (x + µu) − f (x)k2 ≤ dLc under the following assumption:
A3: ft is Lc-Lipschitz continuous.

In Proposition 3  we show that the dimension-dependency of Gzo can be further improved by using
sphere concentration results; See Appendix 2.3 for proof.

Proposition 3 Under A3  max{d  T} ≥ 3  and given δ ∈ (0  1)  then with probability at least 1− δ 

max

t∈[T ]{kˆgtk∞} ≤ 2Lcpd log(dT /δ).

(11)

Here we provide some insights on Proposition 3. Since the unit random vector used to deﬁne

This is a tight bound since when the function difference is a constant  the lower bound satisﬁes

ˆgt is uniformly sampled on a sphere  kˆgtk∞ can be improved to O(√d) with high probability.
kˆgtk∞ = Ω(√d) by sphere concentration. It is also not surprising that our bound (11) grows with T

5

since we bound the maximum kˆgtk∞ over T realizations with high probability. The time-dependence
is required to compensate the growth of the probability that there exists an estimate with the extreme
ℓ∞ value versus time. Note that as long as T has polynomial rather than exponential dependency on d 
we then always have maxt∈[T ]{kˆgtk∞} = O(pd log (d)). Based on Proposition 2 and Proposition 3 

the convergence rate of ZO-AdaMM is provided by Theorem 1; See Appendix 2.4 for proof.

Theorem 1 Suppose that A1 and A3 hold. Given parameter settings in Proposition 2 and 3  then

with probability at least 1 − 1/(T√d)  ZO-AdaMM yields

E(cid:20)(cid:13)(cid:13)(cid:13)

ˆV

−1/4

R ∇f (xR)(cid:13)(cid:13)(cid:13)

2(cid:21) = O(cid:16)√d/√T + d1.5/T(cid:17) .

(12)

We can also extend the convergence rate of ZO-AdaMM in Theorem 1 using the measure
t ii ≥ 1/maxt∈[T ]{kˆgtk∞} (by the update rule)  we obtain from (11) that

E[k∇f (xR)k2]. Since ˆV −1/2

E(cid:2)k∇f (xR)k2(cid:3) ≤2Lcpd log(dT /δ)E(cid:20)(cid:13)(cid:13)(cid:13)

ˆV

−1/4

R ∇f (xR)(cid:13)(cid:13)(cid:13)

Theorem 1  together with (13)  implies O(d/√T + d2/T ) convergence rate of ZO-AdaMM under
the conventional measure. We remark that compared to the FO rate O(√d/√T + d/T ) [38] of
AdaMM for unconstrained nonconvex optimization under A1-A2  ZO-AdaMM suffers O(√d) and
O(d) slowdown on the rate term O(1/√T ) and O(1/T )  respectively. This dimension-dependent

2(cid:21) .

(13)

slowdown is similar to ZO-SGD versus SGD shown by [9]. We also remark that compared to
FO-AdaMM  ZO-AdaMM requires additional A3 to bound the ℓ∞ norm of ZO gradient estimates.

4.3 Constrained nonconvex optimization

To analyze ZO-AdaMM in a general constrained case  one needs to handle the coupling effects from
all three factors: momentum  adaptive learning rate  and projection operation. Here we focus on
addressing the coupling issue in the last two factors  which yields our results on ZO-AdaMM at
β1 t = 0. This is equivalent to the ZO version of RMSProp [17] with Reddi’s convergence ﬁx in [18].
When the momentum factor comes into play  the scenario becomes much more complicated. We leave
the answer to the general case β1 t 6= 0 for future research. Even for SGD with momentum  we are
not aware of any successful convergence analysis for stochastic constrained nonconvex optimization.

It is known from SGD [27] that the presence of projection induces a stochastic bias (independent of
iteration number T ) for constrained nonconvex optimization. In Theorem 2  we show that the same
challenge holds for ZO-AdaMM. Thus  one has to adopt the variance reduced gradient estimator 
which induces higher querying complexity than the estimator (1); See Appendix 2.5 for proof.

Theorem 2 Suppose that A1-A2 hold  ˆv
µ = 1√T d

0 ≥ c1  fµ(x1) − minx fµ(x) ≤ Df   αt = α ≤ c
 
Lg
  and β1 t = 0 in Algorithm 1  then the convergence rate of ZO-AdaMM under (8) satisﬁes

1/2

6Df
αT

3L2
gd
4cT

6η2
c4T

+

E[kG(xR)k2] ≤
where xR is picked uniformly randomly from {xt}T
smoothing function of f deﬁned in (2).

(max
t∈[T ]

E[kˆgt − fµ(xt)k2] + dη2) +

E[kˆgt − fµ(xt)k2] 
t=1  G(x) has been deﬁned in (8)  and fµ is the

max
t∈[T ]

+

c

3c + 9

Theorem 2 implies that regardless of the number of iterations T   ZO-AdaMM only converges to
a solution’s neighborhood whose size is determined by the variance of ZO gradient estimates

maxt∈[T ] E[kˆgt − fµ(xt)k2]. To make this term diminishing  we consider the following variance

reduced gradient estimator built on multiple stochastic samples and random direction vectors [14] 

ˆgt =

q

1

bq Xj∈It

Xi=1

ˆ∇f (xt; ui t  ξj) 

ˆ∇f (xt; ui t  ξj) :=

d[f (xt + µui t; ξj) − f (xt; ξj)]

µ

ui t 

(14)

where It is a mini-batch containing b stocahstic samples at time t  and {ui t}q
i=1 are q random
direction vectors at time t. We present the variance of (14) in Lemma 1  whose proof is induced from
[14  Proposition 2] by using k∇ftk2

2 ≤ dk∇ftk2
∞

= dη2 in A2.

6

Lemma 1 Suppose that A1-A2 hold  then for µ ≤ 1/√d  the variance of (14) yields

E(cid:2)kˆgt − ∇fµ(xt)k2

2(cid:3) = O(cid:0)d/b + d2/q(cid:1) .

(15)

Based on Lemma 1  the rate of ZO-AdaMM in Theorem 2 becomes E[kG(xR)k2] = O(d/T + d/b +
d2/q). Note that if A3 holds  then the dimension-dependency can be improved by O(d) factor
based on Lemma 1. To the best of our knowledge  even in the FO case we are not aware of existing
convergence rate analysis on adaptive learning rate methods for nonconvex contrained optimization.

5 Extended Analysis of ZO-AdaMM

ZO-AdaMM for constrained convex optimization Different from the nonconvex case  the con-
vergence of ZO-AdaMM for convex optimization is commonly measured by the average regret
t=1 ft(x∗)i [18  19]  where recall that ft(xt) = f (xt; ξt)  and x∗
RT = Eh 1
is the optimal solution. We provide the average regret with the ZO gradient estimates by leveraging
its connection to the smoothing function of ft in Proposition 4; see Appendix 3.1 for proof.

t=1 ft(xt) − 1

T PT

T PT

Proposition 4 Suppose that αt = α/√t  β1 t = β1/t with β1 1 = β1  β1  β2 ∈ [0  1)  γ :=
β1/√β2 < 1 and X has bounded diameter D∞  then ZO-AdaMM for convex optimization yields

T

T

RT µ := E" 1
Xt=1
∞Pd
α(1 − β1)√T

E[ˆv1/2
T i ]

D2

i=1

≤

+

ft µ(xt) −

T

1
T

ft µ(x∗)#
Xt=1
Xi=1
Xt=1
2(1 − β1)T

D2

∞

T

d

β1E[ˆv1/2
t i ]
α√t

+

α√1 + log T Pd
Ekˆg1:T ik
(1 − β1)2(1 − γ)√1 − β2T

i=1

.

(16)

where ft µ denotes the smoothing function of f deﬁned by (2)  ˆvt i denotes the ith element of the
vector ˆvt deﬁned in Algorithm 1  and ˆg1:T i := [ˆg1 i  . . .   ˆgT i]⊤.

We remark that Proposition 4 would reduce to [18  Theorem 4] by replacing ZO gradient estimates
ˆg1:T i and ˆvt i with FO gradients g1:T and vt. However  it was recently shown by [39] that the
proof of [18  Theorem 4] is problematic. To address the proof issue  in Proposition 4 we present a
simpler ﬁx than [39  Theorem 4.1] and show that the conclusion of [18  Theorem 4] keeps correct.
In the FO setting  the rate of AdaMM under A2 for constrained convex optimization is given by
O(d/√T ) [19  Corollary 4.4]. Here A2 provides the direct η-upper bound on |gt i| and ˆv1/2
t i   and we
consider worst-case rate analysis without imposing extra assumptions like sparse gradients3. In the
ZO setting  we need further bound |ˆgt i| and ˆvt i and link RT µ to RT   where the former is achieved
by Proposition 3 and the latter is achieved by the relationship between ft and its smoothing function
ft µ shown in Lemma A1-(a)  yielding ft(xt) − ft(x∗) ≤ ft µ(xt) − ft µ(x∗) + 2µLc. Thus  given
µ ≤ d/√T and assuming conditions in Proposition 3 hold  then the rate of ZO-AdaMM becomes
RT ≤ 2µLc + RT µ = O(d1.5/√T )  which is O(√d) worse than the AdaMM.

Comparison with other ZO methods Since the existing convergence analysis for different ZO
methods is built on different problem settings and assumptions. The direct comparison over the
convergence rates might not be fair enough. Thus  in Table 1 we compare ZO-AdaMM with others ZO
methods from 4 perspectives: a) the type of gradient estimator  b) the setting of smoothing parameter
µ  c) convergence rate  and d) function query complexity.

Table 1 shows that for unconstrained nonconvex optimization  the convergence of ZO-AdaMM
achieves worse dependency on d than ZO-SGD [9]  ZO-SCD [22] and ZO-signSGD [14]. However 
it has milder choice of µ than ZO-SGD  less query complexity than ZO-SCD  and no T -independent
convergence bias compared to ZO-signSGD. Also  for constrained nonconvex optimization  ZO-
AdaMM yields the similar rate to ZO-ProxSGD [27]  which also implies ZO projected SGD (ZO-
PSGD). For constrained convex optimization  the rate of ZO-AdaMM is O(d) worse than ZO-SMD
[23] but ours has the signiﬁcantly improved dimension-dependency in µ. We also highlight that at
the ﬁrst glance  ZO-AdaMM has a worse d-dependency (regardless of choice of µ) than ZO-SGD.
However  even in the FO setting  AdaMM has an extra O(√d) dependency in the worst case due to

the effect of (coordinate-wise) gradient normalization when bounding the distance of two consecutive

3The work [40] showed the lack of sparsity in gradients while generating adversarial examples.

7

updates. Thus  in addition to comparing with different ZO methods  Table 1 also summarizes the

convergence performance of FO AdaMM. Note that our rate yields O(√d) slowdown compared to

FO AdaMM though bounding ZO gradient estimate norm requires stricter assumption.

Rate

Query

Method

Assumptions

ZO-SGD [9]

NC1  UCons1  A1  A32

ZO-SCD [22]

NC  UCons  A1  A32

Gradient
estimator
GauGE1

CooGE1

Smoothing
parameter µ
d√T (cid:17)
O (cid:16) 1
+ 1√d(cid:17)

O (cid:16) 1√T

ZO-signSGD [14]

NC  UCons  A1  A3

sign-UniGE1

ZO-ProxSGD /
ZO-PSGD [27]

ZO-SMD [23]

NC  Cons4  A1  A3

GauGE

C  Cons  A3

GauGE/UniGE

AdaMM [20  38]

NC  UCons  A1  A2

AdaMM [18  19  39]

C  Cons  A2

ZO-AdaMM

NC  UCons  A1  A3

ZO-AdaMM

ZO-AdaMM

NC  Cons  A1  A3

β1 t = 0

C  Cons  A3

SGE1

SGE

UniGE

UniGE

UniGE

O (cid:16) 1√dT (cid:17)
O (cid:16) 1√dT (cid:17)
O (cid:0) 1
dt(cid:1)
n/a

n/a

O (cid:16) 1√dT (cid:17)
O (cid:16) 1√dT (cid:17)
O (cid:16) d√T (cid:17)

O(

O (T )

O (cid:16) √d√T
T (cid:17)
+ d
O (cid:16) √d√T
T (cid:17)
+ d
O (dT )
√d√T
√d√b
+ d√bq )3 O (bqT )
+
O (cid:16) d2
q(cid:17)
qT + d
O (qT )
O (cid:16) √d√T (cid:17)
O (cid:16) √d√T
T (cid:17)
+ d
O (cid:16) d√T (cid:17)
T (cid:17)
+ d2
q(cid:17)
b + d
T + 1
√T (cid:17)
O (cid:16) d1.5

O (cid:16) d√T
O (cid:16) d

O (bqT )

O (T )

O(T )

n/a

n/a

O (T )

1 Abbreviations. NC: Nonconvex; UCons: Unconstrained; GauGE: Gaussian random vector based gradient estimate; UniGE: Uniform
random vector based gradient estimate; CooGE: Coordinate-wise gradient estimate; SGE: stochastic (ﬁrst-order) gradient estimate
2 Assumption of bounded variance of stochastic gradients is implied from A3.
3 Convergence of ZO-signSGD is measured by E[k∇f (xT )k2] rather than its square used in other algorithms for nonconvex optimization.
Table 1: Summary of convergence rate and query complexity of various ZO algorithms given T iterations.

6 Applications to Black-Box Adversarial Attacks

In this section  we demonstrate the effectiveness of ZO-AdaMM by experiments on generating
black-box adversarial examples. Our experiments will be performed on Inception V3 [45] using
ImageNet [46]. Here we focus on two types of black-box adversarial attacks: per-image adversarial
perturbation [47] and universal adversarial perturbation against multiple images [5  6  48  49]. For
each type of attack  we allow both constrained and unconstrained optimization problem settings. We
compare our propos ed ZO-AdaMM method with 6 existing ZO algorithms: ZO-SGD  ZO-SCD and
ZO-signSGD for unconstrained optimization  and ZO-PSGD  ZO-SMD and ZO-NES for constrained
optimization. The ﬁrst 5 methods have been summarized in Table 1  and ZO-NES refers to the
black-box attack generation method in [6]  which applies a projected version of ZO-signSGD using
natural evolution strategy (NES) based random gradient estimator. In our experiments  every method
takes the same number of queries per iteration. Accordingly  the total query complexity is consistent
with the number of iterations. We refer to Appendix 4 for details on experiment setups.

Per-image adversarial perturbation In Fig. 1  we present the attack loss and the resulting ℓ2-
distortion against iteration numbers for solving both unconstrained and constrained adversarial attack
problems  namely  (94) and (93) in Appendix 4  over 100 randomly selected images. Here every
algorithm is initialized by zero perturbation. Thus  as the iteration increases  the attack loss decreases
until it converges to 0 (indicating successful attack) while the distortion could increase. At this sense 
the best attack performance should correspond to the best tradeoff between the fast convergence
to 0 attack loss and the low distortion power (evaluated by ℓ2 norm). As we can see  ZO-AdaMM
consistently outperforms other ZO methods in terms of the fast convergence of attack loss and
relatively small perturbation. We also note that ZO-signSGD and ZO-NES have poor convergence
accuracy in terms of either large attack loss or large distortion at ﬁnal iterations. This is not surprising 
since it has been shown in [14] that ZO-signSGD only converges to a neighborhood of a solution 
and ZO-NES can be regarded as a Euclidean projection based ZO-signSGD  which could induce
convergence issues shown by Prop. 1. We refer readers to Table A3 for detailed experiment results.

Universal adversarial perturbation We now focus on designing a universal adversarial perturba-
tion using the constrained attack problem formulation. Here we attack M = 100 random selected
images from ImageNet. In Fig. 2  we present the attack loss as well as the ℓ2 norm of universal
perturbation at different iteration numbers. As we can see  compared with the other ZO algorithms 
ZO-AdaMM has the fastest convergence speed to reach the smallest adversarial perturbation (namely 
strongest universal attack). Moreover  in Table 2 we present detailed attack success rate and ℓ2 distor-
tion over T = 40000 iterations. Consistent with Fig. 2  ZO-AdaMM achieves highest success rate

8

(a) unconstrained setting

(b) constrained setting

Figure 1: The attack loss and adversarial distortion v.s. iterations. Each box represents results from 100 images.

with lowest distortion. In Fig. A2 of Appendix A2  we visualize patterns of the generated universal
adversarial perturbations which further conﬁrm the advantage of ZO-AdaMM.

Methods

ZO-NES
ZO-PSGD
ZO-SMD

ZO-AdaMM

Attack

Final

success rate

74%
78%
79%
84%

kδTk2

2

67.74
49.92
47.36
38.40

Figure 2: Attack loss and distortion of universal attack.

7 Conclusion

Table 2: Summary of attack success rate and
eventual ℓ2 distortion for universal attack against
100 images under T = 40000 iterations.

In this paper  we propose ZO-AdaMM  the ﬁrst effort to integrate adaptive momentum methods
with ZO optimization. In theory  we show that ZO-AdaMM has convergence guarantees for both
convex and nonconvex constrained optimization. Compared with (ﬁrst-order) AdaMM  it suffers a

slowdown factor of O(√d). Particularly  we establish a new Mahalanobis distance based convergence

measure whose necessity and importance are provided in characterizing the convergence behavior of
ZO-AdaMM on nonconvex constrained problems. To demonstrate the utility of the algorithm  we
show the superior performance of ZO-AdaMM for designing adversarial examples from black-box
neural networks. Compared with 6 state-of-the-art ZO methods  ZO-AdaMM has the fastest empirical
convergence to strong black-box adversarial attacks that require the minimum distortion strength.

Acknowledgement

This work is partly supported by National Science Foundation CNS-1932351. M. Hong is supported
in part by NSF under Grant CMMI-172775  CIF-1910385 and by ARO under grant 73202-CS.

References

[1] S. Liu  J. Chen  P.-Y. Chen  and A. O. Hero  “Zeroth-order online ADMM: Convergence
analysis and applications ” in Proceedings of the Twenty-First International Conference on
Artiﬁcial Intelligence and Statistics  April 2018  vol. 84  pp. 288–297.

[2] A. K. Sahu  M. Zaheer  and S. Kar  “Towards gradient free and projection free stochastic

optimization ” arXiv preprint arXiv:1810.03233  2018.

[3] M. Feurer  A. Klein  K. Eggensperger  J. Springenberg  M. Blum  and F. Hutter  “Efﬁcient and
robust automated machine learning ” in Advances in Neural Information Processing Systems 
2015  pp. 2962–2970.

9

0100200400800Iteration020406080100Attack lossZO-SCDZO-SGDZO-signSGDZO-AdaMM0100200400800Iteration020406080100DistortionZO-SCDZO-SGDZO-signSGDZO-AdaMM0100200400800Iteration020406080100Attack lossZO-NESZO-PSGDZO-SMDZO-AdaMM0100200400800Iteration020406080100120140160180DistortionZO-NESZO-PSGDZO-SMDZO-AdaMM010000200003000040000Iteration1020304050Attack lossZO-NESZO-PSGDZO-SMDZO-AdaMM010000200003000040000Iteration010203040506070DistortionZO-NESZO-PSGDZO-SMDZO-AdaMM[4] L. Kotthoff  C. Thornton  H. H. Hoos  F. Hutter  and K. Leyton-Brown  “Auto-weka 2.0:
Automatic model selection and hyperparameter optimization in weka ” J. Mach. Learn. Res. 
vol. 18  no. 1  pp. 826–830  Jan. 2017.

[5] P.-Y. Chen  H. Zhang  Y. Sharma  J. Yi  and C.-J. Hsieh  “Zoo: Zeroth order optimization based
black-box attacks to deep neural networks without training substitute models ” in Proceedings
of the 10th ACM Workshop on Artiﬁcial Intelligence and Security. ACM  2017  pp. 15–26.

[6] A. Ilyas  K. Engstrom  A. Athalye  and J. Lin  “Black-box adversarial attacks with limited
queries and information ” in Proceedings of the 35th International Conference on Machine
Learning  July 2018.

[7] C.-C. Tu  P. Ting  P.-Y. Chen  S. Liu  H. Zhang  J. Yi  C.-J. Hsieh  and S.-M. Cheng  “Autozoom:
Autoencoder-based zeroth order optimization method for attacking black-box neural networks ”
arXiv preprint arXiv:1805.11770  2018.

[8] Y. Nesterov and V. Spokoiny  “Random gradient-free minimization of convex functions ”

Foundations of Computational Mathematics  vol. 2  no. 17  pp. 527–566  2015.

[9] S. Ghadimi and G. Lan  “Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic

programming ” SIAM Journal on Optimization  vol. 23  no. 4  pp. 2341–2368  2013.

[10] B. Shahriari  K. Swersky  Z. Wang  R. P. Adams  and N. De Freitas  “Taking the human out
of the loop: A review of bayesian optimization ” Proceedings of the IEEE  vol. 104  no. 1  pp.
148–175  2016.

[11] A. R. Conn  K. Scheinberg  and L. Vicente  “Global convergence of general derivative-free
trust-region algorithms to ﬁrst-and second-order critical points ” SIAM Journal on Optimization 
vol. 20  no. 1  pp. 387–415  2009.

[12] D. Whitley  “A genetic algorithm tutorial ” Statistics and computing  vol. 4  no. 2  pp. 65–85 

1994.

[13] A. R. Conn  K. Scheinberg  and L. N. Vicente  Introduction to derivative-free optimization 

vol. 8  Siam  2009.

[14] S. Liu  P.-Y. Chen  X. Chen  and M. Hong  “signSGD via zeroth-order oracle ” in International

Conference on Learning Representations  2019.

[15] J. Bernstein  Y. Wang  K. Azizzadenesheli  and A. Anandkumar  “signsgd: compressed

optimisation for non-convex problems ” arXiv preprint arXiv:1802.04434  2018.

[16] D. P. Kingma and J. Ba  “Adam: A method for stochastic optimization ” in Proc. 3rd Int. Conf.

Learn. Representations  2014.

[17] T. Tieleman and G. Hinton  “Lecture 6.5-rmsprop: Divide the gradient by a running average of
its recent magnitude ” COURSERA: Neural networks for machine learning  vol. 4  no. 2  pp.
26–31  2012.

[18] S. J. Reddi  S. Kale  and S. Kumar  “On the convergence of adam and beyond ” in International

Conference on Learning Representations  2018.

[19] J. Chen and Q. Gu  “Closing the generalization gap of adaptive gradient methods in training

deep neural networks ” arXiv preprint arXiv:1806.06763  2018.

[20] X. Chen  S. Liu  R. Sun  and M. Hong  “On the convergence of a class of adam-type algorithms

for non-convex optimization ” arXiv preprint arXiv:1808.02941  2018.

[21] L. Balles and P. Hennig  “Dissecting adam: The sign  magnitude and variance of stochastic
gradients ” in Proceedings of the 35th International Conference on Machine Learning  Jennifer
Dy and Andreas Krause  Eds.  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018  vol. 80
of Proceedings of Machine Learning Research  pp. 404–413.

[22] X. Lian  H. Zhang  C.-J. Hsieh  Y. Huang  and J. Liu  “A comprehensive linear speedup analysis
for asynchronous stochastic parallel optimization from zeroth-order to ﬁrst-order ” in Advances
in Neural Information Processing Systems  2016  pp. 3054–3062.

10

[23] J. C. Duchi  M. I. Jordan  M. J. Wainwright  and A. Wibisono  “Optimal rates for zero-
order convex optimization: The power of two function evaluations ” IEEE Transactions on
Information Theory  vol. 61  no. 5  pp. 2788–2806  2015.

[24] S. Liu  B. Kailkhura  P.-Y. Chen  P. Ting  S. Chang  and L. Amini  “Zeroth-order stochastic
variance reduction for nonconvex optimization ” Advances in Neural InformationProcessing
Systems  2018.

[25] B. Gu  Z. Huo  and H. Huang  “Zeroth-order asynchronous doubly stochastic algorithm with

variance reduction ” arXiv preprint arXiv:1612.01425  2016.

[26] L. Liu  M. Cheng  C.-J. Hsieh  and D. Tao  “Stochastic zeroth-order optimization via variance

reduction method ” arXiv preprint arXiv:1805.11811  2018.

[27] S. Ghadimi  G. Lan  and H. Zhang  “Mini-batch stochastic approximation methods for non-
convex stochastic composite optimization ” Mathematical Programming  vol. 155  no. 1-2  pp.
267–305  2016.

[28] Krishnakumar Balasubramanian and Saeed Ghadimi  “Zeroth-order (non)-convex stochastic
optimization via conditional gradient and gradient updates ” in Advances in Neural Information
Processing Systems  2018  pp. 3455–3464.

[29] J. Chen  J. Yi  and Q. Gu  “A Frank-Wolfe framework for efﬁcient and effective adversarial

attacks ” arXiv preprint arXiv:1811.10828  2018.

[30] X. Gao  B. Jiang  and S. Zhang  “On the information-adaptive variants of the ADMM: an

iteration complexity perspective ” Optimization Online  vol. 12  2014.

[31] Sébastien Le Digabel  “Algorithm 909: Nomad: Nonlinear optimization with the mads algo-

rithm ” ACM Transactions on Mathematical Software (TOMS)  vol. 37  no. 4  pp. 44  2011.

[32] Charles Audet and John E Dennis Jr  “Mesh adaptive direct search algorithms for constrained

optimization ” SIAM Journal on optimization  vol. 17  no. 1  pp. 188–217  2006.

[33] A Ismael F Vaz and Luís N Vicente  “Pswarm: a hybrid solver for linearly constrained global
derivative-free optimization ” Optimization Methods & Software  vol. 24  no. 4-5  pp. 669–685 
2009.

[34] Michael JD Powell  “A direct search optimization method that models the objective and
constraint functions by linear interpolation ” in Advances in optimization and numerical
analysis  pp. 51–67. Springer  1994.

[35] Michael JD Powell  “The bobyqa algorithm for bound constrained optimization without
derivatives ” Cambridge NA Report NA2009/06  University of Cambridge  Cambridge  pp.
26–46  2009.

[36] L. M. Rios and N. V. Sahinidis  “Derivative-free optimization: a review of algorithms and
comparison of software implementations ” Journal of Global Optimization  vol. 56  no. 3  pp.
1247–1293  2013.

[37] Charles Audet and Warren Hare  Derivative-free and blackbox optimization  Springer  2017.

[38] D. Zhou  Y. Tang  Z. Yang  Y. Cao  and Q. Gu  “On the convergence of adaptive gradient

methods for nonconvex optimization ” arXiv preprint arXiv:1808.05671  2018.

[39] T. T. Phuong and L. T. Phong  “On the convergence proof of amsgrad and a new version ” arXiv

preprint arXiv:1904.03590  2019.

[40] A. Ilyas  L. Engstrom  and A. Madry  “Prior convictions: Black-box adversarial attacks with

bandits and priors ” arXiv preprint arXiv:1807.07978  2018.

[41] A. Kurakin  I. Goodfellow  and S. Bengio  “Adversarial examples in the physical world ” arXiv

preprint arXiv:1607.02533  2016.

[42] A. Ilyas  L. Engstrom  A. Athalye  and J. Lin  “Black-box adversarial attacks with limited

queries and information ” arXiv preprint arXiv:1804.08598  2018.

11

[43] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian
Vladu  “Towards deep learning models resistant to adversarial attacks ” arXiv preprint
arXiv:1706.06083  2017.

[44] S. J. Reddi  S. Sra  B. Poczos  and A. J. Smola  “Proximal stochastic methods for nonsmooth
nonconvex ﬁnite-sum optimization ” in Advances in Neural Information Processing Systems 
2016  pp. 1145–1153.

[45] C. Szegedy  V. Vanhoucke  S. Ioffe  J. Shlens  and Z. Wojna  “Rethinking the inception
architecture for computer vision ” in IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  2016  pp. 2818–2826.

[46] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and F.-F. Li  “Imagenet: A large-scale hierarchical
image database ” in Computer Vision and Pattern Recognition  2009. CVPR 2009. IEEE
Conference on. IEEE  2009  pp. 248–255.

[47] Kaidi Xu  Sijia Liu  Pu Zhao  Pin-Yu Chen  Huan Zhang  Quanfu Fan  Deniz Erdogmus  Yanzhi
Wang  and Xue Lin  “Structured adversarial attack: Towards general implementation and better
interpretability ” in International Conference on Learning Representations  2019.

[48] F. Suya  Y. Tian  D. Evans  and P. Papotti  “Query-limited black-box attacks to classiﬁers ”

arXiv preprint arXiv:1712.08713  2017.

[49] M. Cheng  T. Le  P.-Y. Chen  J. Yi  H. Zhang  and C.-J. Hsieh  “Query-efﬁcient hard-label
black-box attack: An optimization-based approach ” arXiv preprint arXiv:1807.04457  2018.

[50] S. Dasgupta and A. Gupta  “An elementary proof of a theorem of johnson and lindenstrauss ”

Random Struct. Algorithms  vol. 22  no. 1  pp. 60–65  Jan. 2003.

[51] N. Carlini and D. Wagner  “Towards evaluating the robustness of neural networks ” in IEEE

Symposium on Security and Privacy  2017  pp. 39–57.

12

,Xiangyi Chen
Sijia Liu
Kaidi Xu
Xingguo Li
Xue Lin
Mingyi Hong
David Cox