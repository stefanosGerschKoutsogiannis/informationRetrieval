2015,A Tractable Approximation to Optimal Point Process Filtering: Application to Neural Encoding,The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful  but lead to limited conceptual insight about optimal encoding/decoding strategies  which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations  which allows us to introduce distributional assumptions about sensory cell properties  that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. The analytic framework leads to insights which are difficult to obtain from numerical algorithms  and is consistent with experiments about the distribution of tuning curve centers. Interestingly  we find that the information gained from the absence of spikes may be crucial to performance.,A Tractable Approximation to Optimal Point Process

Filtering: Application to Neural Encoding

Yuval Harel  Ron Meir

Department of Electrical Engineering

Technion – Israel Institute of Technology

Technion City  Haifa  Israel

{yharel@tx rmeir@ee}.technion.ac.il

Manfred Opper

Department of Artiﬁcial Intelligence

Technical University Berlin

Berlin 10587  Germany

opperm@cs.tu-berlin.de

Abstract

The process of dynamic state estimation (ﬁltering) based on point process ob-
servations is in general intractable. Numerical sampling techniques are often
practically useful  but lead to limited conceptual insight about optimal encod-
ing/decoding strategies  which are of signiﬁcant relevance to Computational Neu-
roscience. We develop an analytically tractable Bayesian approximation to opti-
mal ﬁltering based on point process observations  which allows us to introduce
distributional assumptions about sensory cell properties  that greatly facilitate the
analysis of optimal encoding in situations deviating from common assumptions of
uniform coding. The analytic framework leads to insights which are difﬁcult to
obtain from numerical algorithms  and is consistent with experiments about the
distribution of tuning curve centers. Interestingly  we ﬁnd that the information
gained from the absence of spikes may be crucial to performance.

1

Introduction

The task of inferring a hidden dynamic state based on partial noisy observations plays an important
role within both applied and natural domains. A widely studied problem is that of online inference
of the hidden state at a given time based on observations up to to that time  referred to as ﬁltering
[1]. For the linear setting with Gaussian noise and quadratic cost  the solution is well known since
the early 1960s both for discrete and continuous times  leading to the celebrated Kalman and the
Kalman-Bucy ﬁlters [2  3]  respectively. In these cases the exact posterior distribution is Gaussian 
resulting in closed form recursive update equations for the mean and variance of this distribution 
implying ﬁnite-dimensional ﬁlters. However  beyond some very speciﬁc settings [4]  the optimal
ﬁlter is inﬁnite-dimensional and impossible to compute in closed form  requiring either approximate
analytic techniques (e.g.  the extended Kalman ﬁlter (e.g.  [1])  the unscented ﬁlter [5]) or numerical
procedures (e.g.  particle ﬁlters [6]). The latter usually require time discretization and a ﬁnite number
of particles  resulting in loss of precision . For many practical tasks (e.g.  queuing [7] and optical
communication [8]) and biologically motivated problems (e.g.  [9]) a natural observation process is
given by a point process observer  leading to a nonlinear inﬁnite-dimensional optimal ﬁlter (except
in speciﬁc settings  e.g.  ﬁnite state spaces  [7  10]).
We consider a continuous-state and continuous-time multivariate hidden Markov process observed
through a set of sensory neuron-like elements characterized by multi-dimensional unimodal tuning
functions  representing the elements’ average ﬁring rate. The tuning function parameters are char-
acterized by a distribution allowing much ﬂexibility. The actual ﬁring of each cell is random and is
given by a Poisson process with rate determined by the input and by the cell’s tuning function. In-
ferring the hidden state under such circumstances has been widely studied within the Computational
Neuroscience literature  mostly for static stimuli. In the more challenging and practically important
dynamic setting  much work has been devoted to the development of numerical sampling techniques

1

for fast and effective approximation of the posterior distribution (e.g.  [11]). In this work we are less
concerned with algorithmic issues  and more with establishing closed-form analytic expressions for
an approximately optimal ﬁlter (see [10  12  13] for previous work in related  but more restrictive
settings)  and using these to characterize the nature of near-optimal encoders  namely determining
the structure of the tuning functions for optimal state inference. A signiﬁcant advantage of the closed
form expressions over purely numerical techniques is the insight and intuition that is gained from
them about qualitative aspects of the system. Moreover  the leverage gained by the analytic compu-
tation contributes to reducing the variance inherent to Monte Carlo approaches. Technically  given
the intractable inﬁnite-dimensional nature of the posterior distribution  we use a projection method
replacing the full posterior at each point in time by a projection onto a simple family of distributions
(Gaussian in our case). This approach  originally developed in the Filtering literature [14  15]  and
termed Assumed Density Filtering (ADF)  has been successfully used more recently in Machine
Learning [16  17]. As far as we are aware  this is the ﬁrst application of this methodology to point
process ﬁltering.
The main contributions of the paper are the following: (i) Derivation of closed form recursive expres-
sions for the continuous time posterior mean and variance within the ADF approximation  allowing
for the incorporation of distributional assumptions over sensory variables. (ii) Characterization of
the optimal tuning curves (encoders) for sensory cells in a more general setting than hitherto con-
sidered. Speciﬁcally  we study the optimal shift of tuning curve centers  providing an explanation
for observed experimental phenomena [18]. (iii) Demonstration that absence of spikes is informa-
tive  and that  depending on the relationship between the tuning curve distribution and the dynamic
process (the ‘prior’)  may signiﬁcantly improve the inference. This issue has not been emphasized
in previous studies focusing on homogeneous populations.
We note that most previous work in the ﬁeld of neural encoding/decoding has dealt with static
observations and was based on the Fisher information  which often leads to misleading qualitative
results (e.g.  [19  20]). Our results address the full dynamic setting in continuous time  and provide
results for the posterior variance  which is shown to yield an excellent approximation of the posterior
Mean Square Error (MSE). Previous work addressing non-uniform distributions over tuning curve
parameters [21] used static univariate observations and was based on Fisher information rather than
the MSE itself.

2 Problem formulation

2.1 Dense Gaussian neural code
We consider a dynamical system with state Xt ∈ Rn  observed through an observation process N
describing the ﬁring patterns of sensory neurons in response to the process X. The observed process
is a diffusion process obeying the Stochastic Differential Equation (SDE)
(t ≥ 0)

dXt = A (Xt) dt + D (Xt) dWt 

where A (·)   D (·) are arbitrary functions and Wt is standard Brownian motion. The initial condition
X0 is assumed to have a continuous distribution with a known density. The observation process N
is a marked point process [8] deﬁned on [0 ∞) × Rm  meaning that each point  representing the
ﬁring of a neuron  is identiﬁed by its time t ∈ [0 ∞)  and a mark θ ∈ Rm. In this work the mark is
interpreted as a parameter of the ﬁring neuron  which we refer to as the neuron’s preferred stimulus.
Speciﬁcally  a neuron with parameter θ is taken to have ﬁring rate

(cid:18)

(cid:19)

 

λ (x; θ) = h exp

− 1
2

(cid:107)Hx − θ(cid:107)2

−1
tc

Σ

in response to state x  where H ∈ Rm×n and Σtc ∈ Rm×m   m ≤ n  are ﬁxed matrices  and the
notation (cid:107)y(cid:107)2
M denotes yT M y. The choice of Gaussian form for λ facilitates analytic tractability.
The inclusion of the matrix H allows using high-dimensional models where only some dimensions
are observed  for example when the full state includes velocities but only locations are directly
observable. We also deﬁne Nt (cid:44) N ([0  t) × Rm)  i.e.  Nt is the total number of points up to time
t  regardless of their location θ  and denote by Nt the sequence of points up to time t — formally 

2

the process N restricted to [0  t) × Rm. Following [8]  we use the notation

ˆ

ˆ

b

f (t  θ) N (dt × dθ) (cid:44)(cid:88)

a

U

i

1{ti ∈ [a  b]   θi ∈ U} f (ti  θi)  

(1)

−1
tc

Σ

(cid:17)

−1
tc

Σ

i 1{θi∈A} exp

given Xt = x  is then λA (x; θ) = h(cid:80)

for U ⊆ Rm and any function f  where (ti  θi) are respectively the time and mark of the i-th point
of the process N.
Consider a network with M sensory neurons  having random preferred stimuli θ = {θi} M
i=1 that are
drawn independently from a common distribution with probability density f (θ)  which we refer to
as the population density. Positing a distribution for the preferred stimuli allows us to obtain simple
(cid:17)
closed form solutions  and to optimize over distribution parameters rather than over the higher-
dimensional space of all the θi. The total rate of spikes with preferred stimuli in a set A ⊂ Rm 
(cid:16)− 1
. Averaging over f (θ) 
2 (cid:107)Hx − θ(cid:107)2
(cid:19)

we have the expected rate λA (x) (cid:44) EλA (x; θ) = hM
dθ. We
now obtain an inﬁnite neural network by considering the limit M → ∞ while holding λ0 = hM
ﬁxed. In the limit we have λA (x; θ) → λA (x)  so that the process N has density

(cid:18)
meaning that the expected number of points in a small rectangle [t  t + dt] ×(cid:81)
ditioned on the history X[0 t] Nt  is λt (θ  Xt) dt(cid:81)

(cid:16)− 1
2 (cid:107)Hx − θi(cid:107)2
´
A f (θ) exp

i [θi  θi + dθi]  con-
i dθi + o (dt |dθ|). A ﬁnite network can be

obtained as a special case by taking f to be a sum of delta functions.
For analytic tractability  we assume that f (θ) is Gaussian with center c and covariance Σpop  namely
f (θ) = N (θ; c  Σpop). We refer to c as the population center. Previous work [22  20  23] considered
the case where neurons’ preferred stimuli uniformly cover the space  obtained by removing the factor
f (θ) from (2). Then  the total ﬁring rate
λt (θ  x) dθ is independent of x  which simpliﬁes the
analysis  and leads to a Gaussian posterior (see [22]). We refer to the assumption that
λt (θ  x) dθ
is independent of x as uniform coding. The uniform coding case may be obtained from our model
by taking the limit Σ−1

pop → 0 with λ0/(cid:112)det Σpop held constant.

´

´

λt (θ  Xt) = λ0f (θ) exp

(cid:107)HXt − θ(cid:107)2

− 1
2

−1
tc

Σ

 

(2)

2.2 Optimal encoding and decoding

We consider the question of optimal encoding and decoding under the above model. The process of
neural decoding is assumed to compute (exactly or approximately) the full posterior distribution of
Xt given Nt. The problem of neural encoding is then to choose the parameters φ = (c  Σpop  Σtc) 
which govern the statistics of the observation process N  given a speciﬁc decoding scheme.
To quantify the performance of the encoding-decoding system  we summarize the result of de-
coding using a single estimator ˆXt = ˆXt (Nt)  and deﬁne the Mean Square Error (MSE) as
t (cid:44) trace[(Xt − ˆXt)(Xt − ˆXt)T ]. We seek ˆXt and φ that solve minφ limt→∞ min ˆXt
E [t] =
E[t|Nt]]. The inner minimization problem in this equation is solved by the
minφ limt→∞ E[min ˆXt
MSE-optimal decoder  which is the posterior mean ˆXt = µt (cid:44) E [Xt|Nt]. The posterior mean
may be computed from the full posterior obtained by decoding. The outer minimization problem is
solved by the optimal encoder. In principle  the encoding/decoding problem can be solved for any
value of t. In order to assess performance it is convenient to consider the steady-state limit t → ∞
for the encoding problem.
Below  we ﬁnd a closed form approximate solution to the decoding problem for any t using ADF. We
then explore the problem of choosing the steady-state optimal encoding parameters φ using Monte
Carlo simulations. Note that if decoding is exact  the problem of optimal encoding becomes that of
minimizing the expected posterior variance.

3

3 Neural decoding

ˆ

(cid:16)

λt (θ  x) − ˆλt (θ)

ˆλt (θ)

(cid:17)

3.1 Exact ﬁltering equations
Let P (·  t) denote the posterior density of Xt given Nt  and Et
Nt. The prior density P (·  0) is assumed to be known.
The problem of ﬁltering a diffusion process X from a doubly stochastic Poisson process driven by
X is formally solved in [24]. The result is extended to marked point processes in [22]  where the
authors derive a stochastic PDE for the posterior density1 

P [·] the posterior expectation given

dP (x  t) = L∗P (x  t) dt + P (x  t)

N (dt × dθ) − ˆλt (θ) dθ dt

operator) 

deﬁned

generator

(Kolmogorov’s

P [λt (θ  Xt)] =

as Lf (x)

backward
´

P (x  t) λt (θ  x) dx.

  (3)
θ∈Rm
to N is interpreted as in (1)  L is the state’s in-
=
(Kolmogorov’s

where the integral with respect
ﬁnitesimal
lim∆t→0+ (E [f (Xt+∆t)|Xt = x] − f (x)) /∆t  L∗ is L’s adjoint operator
forward operator)  and ˆλt (θ) (cid:44) Et
The stochastic PDE (3) is usually intractable. In [22  23] the authors consider linear dynamics with
uniform coding and Gaussian prior. In this case  the posterior is Gaussian  and (3) leads to closed
form ODEs for its moments. When the uniform coding assumption is violated  the posterior is no
longer Gaussian. Still  we can obtain exact equations for the posterior moments  as follows.
t ]. Using (3)  and the known results for L for
Let µt = Et
diffusion processes (see supplementary material)  the ﬁrst two posterior moments can be shown to
obey the following equations between spikes (see [23] for the ﬁnite population case):

P Xt  ˜Xt = Xt − µt  Σt = Et
(cid:20)
(cid:21)
ˆ (cid:16)ˆλt (θ) − λt (θ  Xt)
(cid:17)
(cid:105)
(cid:104) ˜XtA (Xt)T(cid:105)
D (Xt) D (Xt)T(cid:105)
(cid:104)
(cid:21)
ˆ (cid:16)ˆλt (θ) − λt (θ  Xt)
(cid:17)

= Et

P [A (Xt)] + Et
P

dµt
dt
dΣt
dt

= Et
P

A (Xt) ˜X T
t

+ Et
P

+ Et
P

dθ

.

(cid:104)

(cid:20)

P [ ˜Xt ˜X T

Xt

dθ

+Et
P

˜Xt ˜X T
t

(4)

3.2 ADF approximation

P [·]. We
While equations (4) are exact  they are not practical  since they require computation of Et
now proceed to ﬁnd an approximate closed form for (4). Here we present the main ideas of the
derivation. The formulation presented here assumes  for simplicity  an open-loop setting where the
system is passively observed. It can be readily extended to a closed-loop control-based setting  and
is presented in this more general framework in the supplementary material  including full details.
To bring (4) to a closed form  we use ADF with an assumed Gaussian density (see [16] for de-
tails). Conceptually  this may be envisioned as integrating (4) while replacing the distribution P
by its approximating Gaussian “at each time step”. Assuming the moments are known exactly  the
Gaussian is obtained by matching the ﬁrst two moments of P [16]. Note that the solution of the
resulting equations does not in general match the ﬁrst two moments of the exact solution  though it
may approximate it.
Abusing notation  in the sequel we use µt  Σt to refer to the ADF approximation rather than to
the exact values. Substituting the normal distribution N (x; µt  Σt) for P (x  t) to compute the ex-
pectations involving λt in (4)  and using (2) and the Gaussian form of f (θ)  results in computable
Gaussian integrals. Other terms may also be computed in closed form if the function A  D can be ex-
panded as power series. This computation yields approximate equations for µt  Σt between spikes.
The updates at spike times can similarly be computed in closed form either from (3) or directly from
a Bayesian update of the posterior (see supplementary material  or e.g.  [13]).

´

1The model considered in [22] assumes linear dynamics and uniform coding – meaning that the total rate of
θ λt (θ  Xt) dθ  is independent of Xt. However  these assumption are only relevant to establish

Nt  namely
other proposition in that paper. The proof of equation (3) still holds as is in our more general setting.

4

Figure 1: Left Changes to the posterior moments between spikes as a function of the current pos-
terior mean estimate  for a static 1-d state. The parameters are a = d = 0  H = 1  σ2
pop =
tc = 0.2  c = 0  λ0 = 10  σt = 1. The bottom plot shows the density of preferred stim-
1  σ2
uli f (θ) and tuning curve for a neuron with preferred stimulus θ = 0. Right An example of
ﬁltering a linear one-dimensional process. Each dot correspond to a spike with the vertical lo-
cation indicating the preferred stimulus θ. The curves to the right of the graph show the pre-
ferred stimulus density (black)  and a tuning curve centered at θ = 0 (gray). The tuning curve
and preferred stimulus density are normalized to the same height for visualization. The bottom
graph shows the posterior variance  with the vertical lines showing spike times. Parameters are:
a = −0.1  d = 2  H = 1  σ2
0 = 1. Note the decrease
of the posterior variance following t = 4 even though no spikes are observed.

tc = 0.2  c = 0  λ0 = 10  µ0 = 0  σ2

pop = 2  σ2

For simplicity  we assume that the dynamics are linear  dXt = AXt dt + D dWt  resulting in the
ﬁltering equations

dµt = Aµtdt + gtΣtH T St (Hµt − c) dt + Σt− H T Stc
t−

(θ − Hµt−) N (dt × dθ)

(5)

ˆ

θ∈Rm

dΣt =(cid:0)AΣt + ΣtAT + DDT(cid:1) dt
+ gtΣtH T(cid:104)
(cid:44)(cid:0)Σtc + HΣtH T(cid:1)−1
− Σt− H T Stc
ˆ

ˆ

gt (cid:44)

ˆλ (θ) dθ =

Et

HΣtdt

St − St (Hµt − c) (Hµt − c)T St
t− HΣt− dNt 

  St (cid:44)(cid:0)Σtc + Σpop + HΣtH T(cid:1)−1  and
(cid:18)
P [λ (θ  Xt)] dθ = λ0(cid:112)det (ΣtcSt) exp

(cid:105)

where Stc
t

(6)

(cid:19)

(cid:107)Hµt − c(cid:107)2

St

− 1
2

is the posterior expected total ﬁring rate. Expressions including t− are to be interpreted as left limits
f (t−) = lims→t− f (s)  which are necessary since the solution is discontinuous at spike times.
The last term in (5) is to be interpreted as in (1). It contributes an instantaneous jump in µt at the
time of a spike with preferred stimulus θ  moving Hµt closer to θ. Similarly  the last term in (6)
contributes an instantaneous jump in Σt at each spike time  which is the same regardless of spike
location. All other terms describe the evolution of the posterior between spikes: the ﬁrst few terms
in (5)-(6) are the same as in the dynamics of the prior  as in [13  23]  whereas the terms involving
gt correspond to information from the absence of spikes. Note that the latter scale with gt  the
expected total ﬁring rate  i.e.  lack of spikes becomes “more informative” the higher the expected
rate of spikes.
It is illustrative to consider these equations in the scalar case m = n = 1  with H = 1. Letting
σ2
t = Σt  σ2

pop = Σpop  a = A  d = D yields

tc = Σtc  σ2

dµt = aµtdt + gt

(cid:32)

σ2
t
σ2
t + σ2
tc + σ2

dσ2

t =

2aσ2

t + d2 + gt

(µt − c) dt +

(cid:34)

1 −

θ∈R

(cid:35)

(cid:33)

σ2
t−
σ2
t− + σ2
tc
(µt − c)2
σ2
t + σ2
tc + σ2

pop

σ2
t

pop
σ2
t
t + σ2
σ2
tc + σ2

pop

(θ − µt−) N (dt × dθ)

(7)

dt −

σ2
t−
σ2
t− + σ2
tc

σ2
t− dNt 

(8)

ˆ

5

−6−4−20246µpopulationdensityfiring ratefor µ=0−6−4−20246¹t−101d¹t=dtd¾t=dt−505Xt¹t§¾tN(t;µ)0246810t048¾2tFigure 2: Left Illustration of information gained between spikes. A static state Xt = 0.5  shown
in a dotted line  is observed and ﬁltered twice: with the correct value σ2
pop = 0.5 (“ADF”  solid
pop = ∞ (“Uniform coding ﬁlter”  dashed line). The curves to the right of
blue line)  and with σ2
the graph show the preferred stimulus density (black)  and a tuning curve centered at θ = 0 (gray).
0 = 1. Right Comparison of MSE for the ADF ﬁlter and
Both ﬁlters are initialized with µ0 = 0  σ2
the uniform coding ﬁlter. The vertical axis shows the integral of the square error integrated over the
time interval [5  10]  averaged over 1000 trials. Shaded areas indicate estimated errors  computed as
the sample standard deviation divided by the square root of the number of trials. Parameters in both
plots are a = d = 0  c = 0  σ2

tc = 0.1  H = 1  λ0 = 10.

pop = 0.5  σ2

where gt = λ0(cid:112)2πσ2

tcN(cid:0)µt; c  σ2

(cid:1). Figure 1 (left) shows how µt  σ2

t + σ2

t change be-
tween spikes for a static 1-dimensional state (a = d = 0). In this case  all terms in the ﬁltering
equations drop out except those involving gt. The term involving gt in dµt pushes µt away from c in
the absence of spikes. This effect weakens as |µt − c| grows due to the factor gt  consistent with the
idea that far from c  the lack of spikes is less surprising  hence less informative. The term involving
gt in dσ2

t increases the variance when µt is near c  otherwise decreases it.

tc + σ2

pop

3.3

Information from lack of spikes

An interesting aspect of the ﬁltering equations (5)-(6) is that the dynamics of the posterior density
between spikes differ from the prior dynamics. This is in contrast to previous models which assumed
uniform coding: the (exact) ﬁltering equations appearing in [22] and [23] have the same form as (5)-
(6) except that they do not include the correction terms involving gt  so that between spikes the
dynamics are identical to the prior dynamics. This reﬂects the fact that lack of spikes in a time
interval is an indication that the total ﬁring rate is low; in the uniform coding case  this is not
informative  since the total ﬁring rate is independent of the state.
Figure 2 (left) illustrates the information gained from lack of spikes. A static scalar state is observed
by a process with rate (2)  and ﬁltered twice: once with the correct value of σpop  and once with
σpop → ∞  as in the uniform coding ﬁlter of [23]. Between spikes  the ADF estimate moves
away from the population center c = 0  whereas the uniform coding estimate remains ﬁxed. The
size of this effect decreases with time  as the posterior variance estimate (not shown) decreases.
The reduction in ﬁltering errors gained from the additional terms in (5)-(6) is illustrated in Figure
2 (right). Despite the approximation involved  the full ﬁlter signiﬁcantly outperforms the uniform
coding ﬁlter. The difference disappears as σpop increases and the population becomes uniform.
Special cases To gain additional insight into the ﬁltering equations  we consider their behavior in
pop → ∞  spikes become rare as the density f (θ) approaches 0 for any θ.
several limits. (i) As σ2
The total expected rate of spikes gt also approaches 0  and the terms corresponding to information
tc → ∞ 
from lack of spikes vanish. Other terms in the equations are unaffected. (ii) In the limit σ2
each neuron ﬁres as a Poisson process with a constant rate independent of the observed state. The
total expected ﬁring rate gt saturates at its maximum  λ0. Therefore the preferred stimuli of spiking
neurons provide no information  nor does the presence or absence of spikes. Accordingly  all terms
other than those related to the prior dynamics vanish.
(iii) The uniform coding case [22  23] is
pop → ∞ with λ0/σpop constant. In this limit the terms
obtained as a special case in the limit σ2
involving gt drop out  recovering the (exact) ﬁltering equations in [22].

6

02468101214t−0.20.00.20.40.60.81.01.2xUniform coding filter vs. ADFTrue stateADFUniform coding filter0246810¾2pop100101102MSEAccumulated MSE for ADF and uniform-coding filterADFuniform4 Optimal neural encoding

We model the problem of optimal neural encoding as choosing the parameters c  Σpop  Σtc of the
population and tuning curves  so as to minimize the steady-state MSE. As noted above  when the
estimate is exactly the posterior mean  this is equivalent to minimizing the steady-state expected
posterior variance. The posterior variance has the advantage of being less noisy than the square error
itself  since by deﬁnition it is the mean of the square error (of the posterior mean) under conditioning
by Nt. We explore the question of optimal neural encoding by measuring the steady-state variance
through Monte Carlo simulations of the system dynamics and the ﬁltering equations (5)-(6). Since
the posterior mean and variance computed by ADF are approximate  we veriﬁed numerically that
the variance closely matches the MSE in the steady state when averaged across many trials (see
supplementary material)  suggesting that asymptotically the error in estimating µt and Σt is small.

4.1 Optimal population center

We now consider the question of the optimal value for the population center c. Intuitively  if the
prior distribution of the process X is unimodal with mode x0  the optimal population center is at
Hx0  to produce the most spikes. On the other hand  the terms involving gt in the ﬁltering equation
(5)-(6) suggest that the lack of spikes is also informative. Moreover  as seen in Figure 1 (left)  the
posterior variance is reduced between spikes only when the current estimate is far enough from c.
These considerations suggest that there is a trade-off between maximizing the frequency of spikes
and maximizing the information obtained from lack of spikes  yielding an optimal value for c that
differs from Hx0.
We simulated a simple one-dimensional process to determine the optimal value of c which minimizes
the approximate posterior variance Σt. Figure 3 (left) shows the posterior variance for varying
values of the population center c and base ﬁring rate λ0. For each ﬁring rate  we note the value of
c minimizing the posterior variance (the optimal population center)  as well as the value of cm =
argminc (dσt/dt|µt=0)  which maximizes the reduction in the posterior variance when the current
state estimate µt is at the process equilibrium x0 = 0. Consistent with the discussion above  the
optimal value lies between 0 (where spikes are most abundant) and cm (where lack of spikes is most
informative). As could be expected  the optimal center is closer to 0 the higher the base ﬁring rate.
Similarly  wide tuning curves  which render the spikes less informative  lead to an optimal center
farther from 0 (Figure 3  right).
A shift of the population center relative to the prior mode has been observed physiologically in
encoding of inter-aural time differences for localization of sound sources [25]. In [18]  this phe-
nomenon was explained in a ﬁnite population model based on maximization of Fisher information.
This is in contrast to the results of [21]  which consider a heterogeneous population where the tuning
curve width scales roughly inversely with neuron density. In this case  the population density max-
imizing the Fisher information is shown to be monotonic with the prior  i.e.  more neurons should
be assigned to more probable states. This apparent discrepancy may be due to the scaling of tuning
curve widths in [21]  which produces roughly constant total ﬁring rate  i.e.  uniform coding. This
demonstrates that a non-constant total ﬁring rate  which renders lack of spikes informative  may be
necessary to explain the physiologically observed shift phenomenon.

4.2 Optimization of population distribution

Next  we consider the optimization of the population distribution  namely  the simultaneous opti-
mization of the population center c and the population variance Σpop in the case of a static scalar
state. Previous work using a ﬁnite neuron population and a Fisher information-based criterion [18]
has shown that the optimal distribution of preferred stimuli depends on the prior variance. When
it is small relative to the tuning curve width  optimal encoding is achieved by placing all preferred
stimuli at a ﬁxed distance from the prior mean. On the other hand  when the prior variance is large
relative to the tuning curve width  optimal encoding is uniform (see ﬁgure 2 in [18]).
Similar results are obtained with our model  as shown in Figure 4. Here  a static scalar state drawn
from N (0  σ2
p) is ﬁltered by a population with tuning curve width σtc = 1 and preferred stimulus
density N (c  σ2
pop). In Figure 4 (left)  the prior distribution is narrow relative to the tuning curve
width  leading to an optimal population with a narrow population distribution far from the origin. In

7

Figure 3: Optimal population center location for ﬁltering a linear one-dimensional process. Both
graphs show the ratio of posterior standard deviation to the prior steady-state standard devia-
tion of the process  along with the value of c minimizing the posterior variance (blue line)  and
minimizing the reduction of posterior variance when µt = 0 (yellow line). The process is ini-
tialized from its steady-state distribution. The posterior variance is estimated by averaging over
the time interval [5  10] and across 1000 trials for each data point. Parameters for both graphs:
a = −1  d = 0.5  σ2

pop = 0.1. In the graph on the left  σ2

tc = 0.01; on the right  λ0 = 50.

Figure 4: Optimal population distribution depends on prior variance relative to tuning curve width.
A static scalar state drawn from N (0  σ2
p) is ﬁltered with tuning curve σtc = 1 and preferred stimulus
density N (c  σ2
pop). Both graphs show the posterior standard deviation relative to the prior standard
p = 0.1  whereas on the right  it is
deviation σp. In the left graph  the prior distribution is narrow  σ2
wide  σ2
p = 10. In both cases the ﬁlter is initialized with the correct prior  and the square error is
averaged over the time interval [5  10] and across 100 trials for each data point.

Figure 4 (right)  the prior is wide relative to the tuning curve width  leading to an optimal population
with variance that roughly matches the prior variance. When both the tuning curves and the popu-
lation density are narrow relative to the prior  so that spikes are rare (low values of σpop in Figure 4
(right))  the ADF approximation becomes poor  resulting in MSEs larger than the prior variance.

5 Conclusions

We have introduced an analytically tractable Bayesian approximation to point process ﬁltering  al-
lowing us to gain insight into the generally intractable inﬁnite-dimensional ﬁltering problem. The
approach enables the derivation of near-optimal encoding schemes going beyond previously studied
uniform coding assumptions. The framework is presented in continuous time  circumventing tem-
poral discretization errors and numerical imprecisions in sampling-based methods  applies to fully
dynamic setups  and directly estimates the MSE rather than lower bounds to it. It successfully ex-
plains observed experimental results  and opens the door to many future predictions. Future work
will include a development of previously successful mean ﬁeld approaches [13] within our more
general framework  leading to further analytic insight. Moreover  the proposed strategy may lead to
practically useful decoding of spike trains.

References

[1] B.D. Anderson and J.B. Moore. Optimal Filtering. Dover  2005. 1
[2] R.E. Kalman and R.S. Bucy. New results in linear ﬁltering and prediction theory. J. of Basic Eng.  Trans.

ASME  Series D  83(1):95–108  1961. 1

8

0.00.20.40.60.81.0c100101102¸0Steady-state posterior stdev / prior stdevargminc¾2targmincd¾2j¹=00.400.480.560.640.720.800.880.960.00.20.40.60.81.0c0.750.800.85¾t=¾00.00.51.01.52.0c0.20.40.60.81.0¾tcSteady-state posterior stdev / prior stdev0.540.600.660.720.780.840.900.96012345c10-210-1100101102¾popSteady-state variance  narrow prior−0.35−0.30−0.25−0.20−0.15−0.10−0.050.00log10(post. stdev / prior stdev)012345c10-210-1100101102¾popSteady-state variance  wide prior−1.00−0.75−0.50−0.250.000.250.500.75log10(post. stdev / prior stdev)[3] R.E. Kalman. A new approach to linear ﬁltering and prediction problems. J. Basic Eng.  Trans. ASME 

Series D.  82(1):35–45  1960. 1

[4] F. Daum. Nonlinear ﬁlters: beyond the kalman ﬁlter. Aerospace and Electronic Systems Magazine  IEEE 

20(8):57–69  2005. 1

[5] S. Julier  J. Uhlmann  and H. Durrant-Whyte. A new method for the nonlinear transformation of means

and covariances in ﬁlters and estimators. IEEE Trans. Autom. Control  45(3):477–482  2000. 1

[6] A. Doucet and A.M. Johansen. A tutorial on particle ﬁltering and smoothing: ﬁfteen years later.

In
D. Crisan and B. Rozovskii  editors  Handbook of Nonlinear Filtering  pages 656–704. Oxford  UK:
Oxford University Press  2009. 1

[7] P. Br´emaud. Point Processes and Queues: Martingale Dynamics. Springer  New York  1981. 1
[8] D.L. Snyder and M.I. Miller. Random Point Processes in Time and Space. Springer  second edition

edition  1991. 1  2.1

[9] P. Dayan and L.F. Abbott. Theoretical Neuroscience: Computational and Mathematical Modeling of

Neural Systems. MIT Press  2005. 1

[10] O. Bobrowski  R. Meir  and Y.C. Eldar. Bayesian ﬁltering in spiking neural networks: noise  adaptation 

and multisensory integration. Neural Comput  21(5):1277–1320  May 2009. 1

[11] Y. Ahmadian  J.W. Pillow  and L. Paninski. Efﬁcient markov chain monte carlo methods for decoding

neural spike trains. Neural Comput  23(1):46–96  Jan 2011. 1

[12] A.K. Susemihl  R. Meir  and M. Opper. Analytical results for the error in ﬁltering of gaussian processes.
In J. Shawe-Taylor  R.S. Zemel  P. Bartlett  F.C.N. Pereira  and K.Q. Weinberger  editors  Advances in
Neural Information Processing Systems 24  pages 2303–2311. 2011. 1

[13] A.K. Susemihl  R. Meir  and M. Opper. Dynamic state estimation based on poisson spike trains—
Journal of Statistical Mechanics: Theory and Experiment 

towards a theory of optimal encoding.
2013(03):P03009  2013. 1  3.2  3.2  5

[14] P.S. Maybeck. Stochastic Models  Estimation  and Control. Academic Press  1979. 1
[15] D. Brigo  B. Hanzon  and F. LeGland. A differential geometric approach to nonlinear ﬁltering:

projection ﬁlter. Automatic Control  IEEE Transactions on  43:247–252  1998. 1

the

[16] M. Opper. A Bayesian approach to online learning.

In D. Saad  editor  Online Learning in Neural

Networks  pages 363–378. Cambridge university press  1998. 1  3.2

[17] T.P. Minka. Expectation propagation for approximate bayesian inference. In Proceedings of the Seven-
teenth conference on Uncertainty in artiﬁcial intelligence  pages 362–369. Morgan Kaufmann Publishers
Inc.  2001. 1

[18] N.S. Harper and D. McAlpine. Optimal neural population coding of an auditory spatial cue. Nature 

430(7000):682–686  Aug 2004. n1397b. 1  4.1  4.2

[19] M. Bethge  D. Rotermund  and K. Pawelzik. Optimal short-term population coding: when ﬁsher infor-

mation fails. Neural Comput  14(10):2317–2351  Oct 2002. 1

[20] S. Yaeli and R. Meir. Error-based analysis of optimal tuning functions explains phenomena observed in

sensory neurons. Front Comput Neurosci  4:130  2010. 1  2.1

[21] D. Ganguli and E.P. Simoncelli. Efﬁcient sensory encoding and bayesian inference with heterogeneous

neural populations. Neural Comput  26(10):2103–2134  2014. 1  4.1

[22] I. Rhodes and D. Snyder. Estimation and control performance for space-time point-process observations.

IEEE Transactions on Automatic Control  22(3):338–346  1977. 2.1  3.1  3.1  1  3.3  3.3

[23] A.K. Susemihl  R. Meir  and M. Opper. Optimal Neural Codes for Control and Estimation. Advances in

Neural Information Processing Systems  pages 1–9  2014. 2.1  3.1  3.2  3.3  3.3

[24] D. Snyder. Filtering and detection for doubly stochastic Poisson processes. IEEE Transactions on Infor-

mation Theory  18(1):91–102  January 1972. 3.1

[25] A. Brand  O. Behrend  T. Marquardt  D. McAlpine  and B. Grothe. Precise inhibition is essential for

microsecond interaural time difference coding. Nature  417(6888):543–547  2002. 4.1

9

,Yuval Harel
Ron Meir
Manfred Opper