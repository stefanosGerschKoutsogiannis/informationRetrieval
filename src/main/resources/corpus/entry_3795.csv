2014,Non-convex Robust PCA,We propose a new provable method for robust PCA  where the task is to recover a low-rank matrix  which is corrupted with sparse perturbations. Our method consists of simple alternating projections onto the set of low rank and sparse matrices with intermediate de-noising steps. We prove correct recovery of the low rank and sparse components under tight recovery conditions  which match those for the state-of-art convex relaxation techniques. Our method is extremely simple to implement and has low computational complexity. For a $m \times n$ input matrix (say m \geq n)  our method has O(r^2 mn\log(1/\epsilon)) running time  where $r$ is the rank of the low-rank component and $\epsilon$ is the accuracy. In contrast  the convex relaxation methods have a running time O(mn^2/\epsilon)  which is not scalable to large problem instances. Our running time nearly matches that of the usual PCA (i.e. non robust)  which is O(rmn\log (1/\epsilon)). Thus  we achieve ``best of both the worlds''  viz low computational complexity and provable recovery for robust PCA. Our analysis represents one of the few instances of global convergence guarantees for non-convex methods.,Provable Non-convex Robust PCA

Praneeth Netrapalli 1∗ U N Niranjan2∗

Sujay Sanghavi3 Animashree Anandkumar2

1Microsoft Research  Cambridge MA. 2The University of California at Irvine.

3The University of Texas at Austin. 4Microsoft Research  India.

Prateek Jain4

Abstract

We propose a new method for robust PCA – the task of recovering a low-rank ma-
trix from sparse corruptions that are of unknown value and support. Our method
involves alternating between projecting appropriate residuals onto the set of low-
rank matrices  and the set of sparse matrices; each projection is non-convex but
easy to compute. In spite of this non-convexity  we establish exact recovery of the
low-rank matrix  under the same conditions that are required by existing methods
(which are based on convex optimization). For an m×n input matrix (m ≤ n)  our

method has a running time of O(cid:0)r2mn(cid:1) per iteration  and needs O (log(1/)) it-
convex optimization  have O(cid:0)m2n(cid:1) complexity per iteration  and take O (1/)

erations to reach an accuracy of . This is close to the running times of simple PCA
via the power method  which requires O (rmn) per iteration  and O (log(1/)) it-
erations. In contrast  the existing methods for robust PCA  which are based on

iterations  i.e.  exponentially more iterations for the same accuracy.
Experiments on both synthetic and real data establishes the improved speed and
accuracy of our method over existing convex implementations.

Keywords:
tions.

Robust PCA  matrix decomposition  non-convex methods  alternating projec-

1

Introduction

Principal component analysis (PCA) is a common procedure for preprocessing and denoising  where
a low rank approximation to the input matrix (such as the covariance matrix) is carried out. Although
PCA is simple to implement via eigen-decomposition  it is sensitive to the presence of outliers 
since it attempts to “force ﬁt” the outliers to the low rank approximation. To overcome this  the
notion of robust PCA is employed  where the goal is to remove sparse corruptions from an input
matrix and obtain a low rank approximation. Robust PCA has been employed in a wide range
of applications  including background modeling [LHGT04]  3d reconstruction [MZYM11]  robust
topic modeling [Shi13]  and community detection [CSX12]  and so on.
Concretely  robust PCA refers to the following problem: given an input matrix M = L∗ + S∗  the
goal is to decompose it into sparse S∗ and low rank L∗ matrices. The seminal works of [CSPW11 
CLMW11] showed that this problem can be provably solved via convex relaxation methods  under
some natural conditions on the low rank and sparse components. While the theory is elegant  in
practice  convex techniques are expensive to run on a large scale and have poor convergence rates.
Concretely  for decomposing an m×n matrix  say with m ≤ n  the best specialized implementations

(typically ﬁrst-order methods) have a per-iteration complexity of O(cid:0)m2n(cid:1)  and require O(1/)

number of iterations to achieve an error of . In contrast  the usual PCA  which carries out a rank-
r approximation of the input matrix  has O(rmn) complexity per iteration – drastically smaller

∗Part of the work done while interning at Microsoft Research  India

1

when r is much smaller than m  n. Moreover  PCA requires exponentially fewer iterations for
convergence: an  accuracy is achieved with only O (log(1/)) iterations (assuming constant gap in
singular values).
In this paper  we design a non-convex algorithm which is “best of both the worlds” and bridges the
gap between (the usual) PCA and convex methods for robust PCA. Our method has low computa-
tional complexity similar to PCA (i.e. scaling costs and convergence rates)  and at the same time 
has provable global convergence guarantees  similar to the convex methods. Proving global conver-
gence for non-convex methods is an exciting recent development in machine learning. Non-convex
alternating minimization techniques have recently shown success in many settings such as matrix
completion [Kes12  JNS13  Har13]  phase retrieval [NJS13]  dictionary learning [AAJ+13]  tensor
decompositions for unsupervised learning [AGH+12]  and so on. Our current work on the analysis
of non-convex methods for robust PCA is an important addition to this growing list.

1.1 Summary of Contributions

We propose a simple intuitive algorithm for robust PCA with low per-iteration cost and a fast con-
vergence rate. We prove tight guarantees for recovery of sparse and low rank components  which
match those for the convex methods. In the process  we derive novel matrix perturbation bounds 
when subject to sparse perturbations. Our experiments reveal signiﬁcant gains in terms of speed-ups
over the convex relaxation techniques  especially as we scale the size of the input matrices.
Our method consists of simple alternating (non-convex) projections onto low-rank and sparse matri-
ces. For an m×n matrix  our method has a running time of O(r2mn log(1/))  where r is the rank of
the low rank component. Thus  our method has a linear convergence rate  i.e. it requires O(log(1/))
iterations to achieve an error of   where r is the rank of the low rank component L∗. When the rank
r is small  this nearly matches the complexity of PCA  (which is O(rmn log(1/))).
We prove recovery of the sparse and low rank components under a set of requirements which are
tight and match those for the convex techniques (up to constant factors). In particular  under the
deterministic sparsity model  where each row and each column of the sparse matrix S∗ has at most

α fraction of non-zeros  we require that α = O(cid:0)1/(µ2r)(cid:1)  where µ is the incoherence factor (see

Section 3).
In addition to strong theoretical guarantees  in practice  our method enjoys signiﬁcant advan-
tages over the state-of-art solver for (1)  viz.  the inexact augmented Lagrange multiplier (IALM)
method [CLMW11]. Our method outperforms IALM in all instances  as we vary the sparsity levels 
incoherence  and rank  in terms of running time to achieve a ﬁxed level of accuracy. In addition 
on a real dataset involving the standard task of foreground-background separation [CLMW11]  our
method is signiﬁcantly faster and provides visually better separation.

Overview of our techniques: Our proof technique involves establishing error contraction with
each projection onto the sets of low rank and sparse matrices. We ﬁrst describe the proof ideas when
L∗ is rank one. The ﬁrst projection step is a hard thresholding procedure on the input matrix M to
remove large entries and then we perform rank-1 projection of the residual to obtain L(1). Standard
matrix perturbation results (such as Davis-Kahan) provide (cid:96)2 error bounds between the singular
vectors of L(1) and L∗. However  these bounds do not sufﬁce for establishing the correctness of our
method. Since the next step in our method involves hard thresholding of the residual M − L(1) 
we require element-wise error bounds on our low rank estimate. Inspired by the approach of Erd˝os
et al. [EKYY13]  where they obtain similar element-wise bounds for the eigenvectors of sparse
Erd˝os–R´enyi graphs  we derive these bounds by exploiting the ﬁxed point characterization of the
eigenvectors1. A Taylor’s series expansion reveals that the perturbation between the estimated and
the true eigenvectors consists of bounding the walks in a graph whose adjacency matrix corresponds
to (a subgraph of) the sparse component S∗. We then show that if the graph is sparse enough 
then this perturbation can be controlled  and thus  the next thresholding step results in further error
contraction. We use an induction argument to show that the sparse estimate is always contained in
the true support of S∗  and that there is an error contraction in each step. For the case  where L∗ has
rank r > 1  our algorithm proceeds in several stages  where we progressively compute higher rank

1If the input matrix M is not symmetric  we embed it in a symmetric matrix and consider the eigenvectors

of the corresponding matrix.

2

projections which alternate with the hard thresholding steps. In stage k = [1  2  . . .   r]  we compute
rank-k projections  and show that after a sufﬁcient number of alternating projections  we reduce the
error to the level of (k + 1)th singular value of L∗  using similar arguments as in the rank-1 case.
We then proceed to performing rank-(k + 1) projections which alternate with hard thresholding.
This stage-wise procedure is needed for ill-conditioned matrices  since we cannot hope to recover
lower eigenvectors in the beginning when there are large perturbations. Thus  we establish global
convergence guarantees for our proposed non-convex robust PCA method.

1.2 Related Work

(cid:107)L(cid:107)∗ + λ(cid:107)S(cid:107)1 

Guaranteed methods for robust PCA have received a lot of attention in the past few years  starting
from the seminal works of [CSPW11  CLMW11]  where they showed recovery of an incoherent low
rank matrix L∗ through the following convex relaxation method:

min
L S

Conv-RPCA :

s.t.  M = L + S 

(1)
where (cid:107)L(cid:107)∗ denotes the nuclear norm of L (nuclear norm is the sum of singular values). A typical
solver for this convex program involves projection on to (cid:96)1 and nuclear norm balls (which are convex
sets). Note that the convex method can be viewed as “soft” thresholding in the standard and spectral
domains  while our method involves hard thresholding in these domains.
[CSPW11] and [CLMW11] consider two different models of sparsity for S∗. Chandrasekaran et
al. [CSPW11] consider a deterministic sparsity model  where each row and column of the m × n
matrix  S  has at most α fraction of non-zero entries. For guaranteed recovery  they require α =

√

√

√

n)(cid:1)  where µ is the incoherence level of L∗  and r is its rank. Hsu et al. [HKZ11]
O(cid:0)1/(µ2r
improve upon this result to obtain guarantees for an optimal sparsity level of α = O(cid:0)1/(µ2r)(cid:1).
r/n. Note that our assumption of incoherence  viz.  (cid:107)U (i)(cid:107) < µ(cid:112)r/n 

This matches the requirements of our non-convex method for exact recovery. Note that when the
rank r = O(1)  this allows for a constant fraction of corrupted entries. Cand`es et al. [CLMW11]
consider a different model with random sparsity and additional incoherence constraints  viz.  they
require (cid:107)U V (cid:62)(cid:107)∞ < µ
only yields (cid:107)U V (cid:62)(cid:107)∞ < µ2r/n. The additional assumption enables [CLMW11] to prove exact
recovery with a constant fraction of corrupted entries  even when L∗ is nearly full-rank. We note that
removing the (cid:107)U V (cid:62)(cid:107)∞ condition for robust PCA would imply solving the planted clique problem
n [Che13]. Thus  our recovery guarantees are tight upto constants
when the clique size is less than
without these additional assumptions.
A number of works have considered modiﬁed models under the robust PCA framework 
e.g. [ANW12  XCS12]. For instance  Agarwal et al. [ANW12] relax the incoherence assumption to
a weaker “diffusivity” assumption  which bounds the magnitude of the entries in the low rank part 
but incurs an additional approximation error. Xu et al.[XCS12] impose special sparsity structure
where a column can either be non-zero or fully zero.
In terms of state-of-art specialized solvers  [CLMW11] implements the in-exact augmented La-
grangian multipliers (IALM) method and provides guidelines for parameter tuning. Other related
methods such as multi-block alternating directions method of multipliers (ADMM) have also been
considered for robust PCA  e.g. [WHML13]. Recently  a multi-step multi-block stochastic ADMM
method was analyzed for this problem [SAJ14]  and this requires 1/ iterations to achieve an error
of . In addition  the convergence rate is tight in terms of scaling with respect to problem size (m  n)
and sparsity and rank parameters  under random noise models.
There is only one other work which considers a non-convex method for robust PCA [KC12]. How-
ever  their result holds only for signiﬁcantly more restrictive settings and does not cover the de-
terministic sparsity assumption that we study. Moreover  the projection step in their method can
have an arbitrarily large rank  so the running time is still O(m2n)  which is the same as the convex
methods. In contrast  we have an improved running time of O(r2mn).

2 Algorithm

In this section  we present our algorithm for the robust PCA problem. The robust PCA problem can
be formulated as the following optimization problem: ﬁnd L  S s.t. (cid:107)M − L− S(cid:107)F ≤ 2 and

2 is the desired reconstruction error

3

Figure 1: Illustration of alternating projections. The goal is to ﬁnd a matrix L∗ which lies in the
intersection of two sets: L = { set of rank-r matrices} and SM = {M − S  where S is a sparse
matrix}. Intuitively  our algorithm alternately projects onto the above two non-convex sets  while
appropriately relaxing the rank and the sparsity levels.

1. L lies in the set of low-rank matrices 
2. S lies in the set of sparse matrices.

A natural algorithm for the above problem is to iteratively project M − L onto the set of sparse
matrices to update S  and then to project M − S onto the set of low-rank matrices to update L. Al-
ternatively  one can view the problem as that of ﬁnding a matrix L in the intersection of the following
two sets: a) L = { set of rank-r matrices}  b) SM = {M −S  where S is a sparse matrix}. Note that
these projections can be done efﬁciently  even though the sets are non-convex. Hard thresholding
(HT) is employed for projections on to sparse matrices  and singular value decomposition (SVD) is
used for projections on to low rank matrices.
Rank-1 case: We ﬁrst describe our algorithm for the special case when L∗ is rank 1. Our algo-
rithm performs an initial hard thresholding to remove very large entries from input M. Note that if
we performed the projection on to rank-1 matrices without the initial hard thresholding  we would
not make any progress since it is subject to large perturbations. We alternate between computing
the rank-1 projection of M − S  and performing hard thresholding on M − L to remove entries
exceeding a certain threshold. This threshold is gradually decreased as the iterations proceed  and
the algorithm is run for a certain number of iterations (which depends on the desired reconstruction
error).
General rank case: When L∗ has rank r > 1  a naive extension of our algorithm consists of al-
ternating projections on to rank-r matrices and sparse matrices. However  such a method has poor
performance on ill-conditioned matrices. This is because after the initial thresholding of the input
matrix M  the sparse corruptions in the residual are of the order of the top singular value (with the
choice of threshold as speciﬁed in the algorithm). When the lower singular values are much smaller 
the corresponding singular vectors are subject to relatively large perturbations and thus  we cannot
make progress in improving the reconstruction error. To alleviate the dependence on the condition
number  we propose an algorithm that proceeds in stages. In the kth stage  the algorithm alternates
between rank-k projections and hard thresholding for a certain number of iterations. We run the
algorithm for r stages  where r is the rank of L∗. Intuitively  through this procedure  we recover
the lower singular values only after the input matrix is sufﬁciently denoised  i.e. sparse corruptions
at the desired level have been removed. Figure 1 shows a pictorial representation of the alternating
projections in different stages.
Parameters: As can be seen  the only real parameter to the algorithm is β  used in thresholding 
which represents “spikiness” of L∗. That is if the user expects L∗ to be “spiky” and the sparse part
to be heavily diffused  then higher value of β can be provided. In our implementation  we found
√
that selecting β aggressively helped speed up recovery of our algorithm. In particular  we selected
β = 1/
Complexity: The complexity of each iteration within a single stage is O(kmn)  since it involves
calculating the rank-k approximation3 of an m×n matrix (done e.g. via vanilla PCA). The number of
iterations in each stage is O (log (1/)) and there are at most r stages. Thus the overall complexity
of the entire algorithm is then O(r2mn log(1/)). This is drastically lower than the best known

bound of O(cid:0)m2n/(cid:1) on the number of iterations required by convex methods  and just a factor r

n.

away from the complexity of vanilla PCA.

3Note that we only require a rank-k approximation of the matrix rather than the actual singular vectors.

Thus  the computational complexity has no dependence on the gap between the singular values.

4

Algorithm 1 ((cid:98)L  (cid:98)S) = AltProj(M    r  β): Non-convex Alternating Projections based Robust PCA

i.e. (HTζ(A))ij = Aij if |Aij| ≥ ζ and 0 otherwise.

1: Input: Matrix M ∈ Rm×n  convergence criterion   target rank r  thresholding parameter β.
2: Pk(A) denotes the best rank-k approximation of matrix A. HTζ(A) denotes hard-thresholding 
3: Set initial threshold ζ0 ← βσ1(M ).
4: L(0) = 0  S(0) = HTζ0(M − L(0))
5: for Stage k = 1 to r do
(cid:32)
6:
7:

for Iteration t = 0 to T = 10 log(cid:0)nβ(cid:13)(cid:13)M − S(0)(cid:13)(cid:13)2 /(cid:1) do
(cid:19)t
(cid:18) 1

Set threshold ζ as

(cid:33)

ζ = β

σk+1(M − S(t)) +

(2)

σk(M − S(t))

2

L(t+1) = Pk(M − S(t))
S(t+1) = HTζ(M − L(t+1))

end for
if βσk+1(L(t+1)) < 
Return: L(T )  S(T )

8:
9:
10:
11:
12:
13:
14:
end if
15:
16: end for
17: Return: L(T )  S(T )

S(0) = S(T )

else

2n then

/* Return rank-k estimate if remaining part has small norm */

/* Continue to the next stage */

3 Analysis

In this section  we present our main result on the correctness of AltProj. We assume the following
conditions:

(L1) Rank of L∗ is at most r.
(L2) L∗ is µ-incoherent  i.e.  if L∗ = U∗Σ∗(V ∗)(cid:62) is the SVD of L∗  then (cid:107)(U∗)i(cid:107)2 ≤ µ

√
r√
m  
√
n   ∀1 ≤ i ≤ n  where (U∗)i and (V ∗)i denote the ith
r√

∀1 ≤ i ≤ m and (cid:107)(V ∗)i(cid:107)2 ≤ µ
rows of U∗ and V ∗ respectively.

(S1) Each row and column of S have at most α fraction of non-zero entries such that α ≤ 1

512µ2r .
Note that in general  it is not possible to have a unique recovery of low-rank and sparse components.
For example  if the input matrix M is both sparse and low rank  then there is no unique decom-
position (e.g. M = e1e(cid:62)
1 ). The above conditions ensure uniqueness of the matrix decomposition
problem.
Additionally  we set the parameter β in Algorithm 1 be set as β = 4µ2r√
mn.

We now establish that our proposed algorithm recovers the low rank and sparse components under
the above conditions.
Theorem 1 (Noiseless Recovery). Under conditions (L1)  (L2) and S∗  and choice of β as above 

the outputs(cid:98)L and (cid:98)S of Algorithm 1 satisfy:
(cid:13)(cid:13)(cid:13)(cid:98)S − S∗(cid:13)(cid:13)(cid:13)∞

(cid:13)(cid:13)(cid:13)(cid:98)L − L∗(cid:13)(cid:13)(cid:13)F

≤  

(cid:16)(cid:98)S
(cid:17) ⊆ Supp (S∗) .

≤ √

mn

  and Supp

Remark (tight recovery conditions): Our result is tight up to constants  in terms of allowable
sparsity level under the deterministic sparsity model. In other words  if we exceed the sparsity limit
imposed in S1  it is possible to construct instances where there is no unique decomposition4. Our

4For instance  consider the n × n matrix which has r copies of the all ones matrix  each of size n

r   placed
across the diagonal. We see that this matrix has rank r and is incoherent with parameter µ = 1. Note that

5

conditions L1  L2 and S1 also match the conditions required by the convex method for recovery  as
established in [HKZ11].
Remark (convergence rate): Our method has a linear rate of convergence  i.e. O(log(1/))
to achieve an error of   and hence we provide a strongly polynomial method for robust PCA. In
contrast  the best known bound for convex methods for robust PCA is O(1/) iterations to converge
to an -approximate solution.
Theorem 1 provides recovery guarantees assuming that L∗ is exactly rank-r. However  in several
real-world scenarios  L∗ can be nearly rank-r. Our algorithm can handle such situations  where
M = L∗ + N∗ + S∗  with N∗ being an additive noise. Theorem 1 is a special case of the following
theorem which provides recovery guarantees when N∗ has small (cid:96)∞ norm.
100n  the outputs(cid:98)L (cid:98)S of Algorithm 1 satisfy:
Theorem 2 (Noisy Recovery). Under conditions (L1)  (L2) and S∗  and choice of β as in Theo-
rem 1  when the noise (cid:107)N∗(cid:107)∞ ≤ σr(L∗)
(cid:18)
7(cid:107)N∗(cid:107)2 +
(cid:16)(cid:98)S
(cid:17) ⊆ Supp (S∗) .
2µ2r√
mn

(cid:13)(cid:13)(cid:13)(cid:98)L − L∗(cid:13)(cid:13)(cid:13)F
(cid:13)(cid:13)(cid:13)(cid:98)S − S∗(cid:13)(cid:13)(cid:13)∞

√
mn√
8
r
7(cid:107)N∗(cid:107)2 +

(cid:107)N∗(cid:107)∞
√
mn√
8
r

≤  + 2µ2r

(cid:107)N∗(cid:107)∞

  and Supp

≤ √

(cid:18)

(cid:19)

(cid:19)

mn

+

 

3.1 Proof Sketch

We now present the key steps in the proof of Theorem 1. A detailed proof is provided in the ap-
pendix.
Step I: Reduce to the symmetric case  while maintaining incoherence of L∗ and sparsity of S∗.
Using standard symmetrization arguments  we can reduce the problem to the symmetric case  where
all the matrices involved are symmetric. See appendix for details on this step.
Step II: Show decay in (cid:107)L − L∗(cid:107)∞ after projection onto the set of rank-k matrices.
The
t-th iterate L(t+1) of the k-th stage is given by L(t+1) = Pk(L∗ + S∗ − S(t)). Hence  L(t+1) is
obtained by using the top principal components of a perturbation of L∗ given by L∗ + (S∗ − S(t)).
The key step in our analysis is to show that when an incoherent and low-rank L∗ is perturbed by a
sparse matrix S∗ − S(t)  then (cid:107)L(t+1) − L∗(cid:107)∞ is small and is much smaller than |S∗ − S(t)|∞. The
following lemma formalizes the intuition; see the appendix for a detailed proof.
Lemma 1. Let L∗  S∗ be symmetric and satisfy the assumptions of Theorem 1 and let S(t) and L(t)
n be the eigenvalues of L∗  s.t. 
be the tth iterates of the kth stage of Algorithm 1. Let σ∗
|σ∗
1| ≥ ··· ≥ |σ∗

r|. Then  the following holds:

1  . . .   σ∗

≤ 2µ2r
n

(cid:33)
(cid:19)t |σ∗
(cid:13)(cid:13)(cid:13)L(t+1) − L∗(cid:13)(cid:13)(cid:13)∞
k|
(cid:33)
(cid:19)t |σ∗
(cid:13)(cid:13)(cid:13)S∗ − S(t+1)(cid:13)(cid:13)(cid:13)∞
k|
Moreover  the outputs(cid:98)L and (cid:98)S of Algorithm 1 satisfy:
(cid:13)(cid:13)(cid:13)(cid:98)L − L∗(cid:13)(cid:13)(cid:13)F

(cid:32)(cid:12)(cid:12)σ∗
(cid:18) 1
(cid:12)(cid:12) +
(cid:32)(cid:12)(cid:12)σ∗
(cid:18) 1
(cid:12)(cid:12) +
(cid:13)(cid:13)(cid:13)(cid:98)S − S∗(cid:13)(cid:13)(cid:13)∞

≤ 8µ2r
n

≤ 
n

≤  

k+1

k+1

2

2

 

  and Supp

  and Supp

(cid:16)
S(t+1)(cid:17) ⊆ Supp (S∗) .
(cid:16)(cid:98)S
(cid:17) ⊆ Supp (S∗) .

Step III: Show decay in (cid:107)S − S∗(cid:107)∞ after projection onto the set of sparse matrices. We next
show that if (cid:107)L(t+1) − L∗(cid:107)∞ is much smaller than (cid:107)S(t) − S∗(cid:107)∞ then the iterate S(t+1) also has
a much smaller error (w.r.t. S∗) than S(t). The above given lemma formally provides the error
bound.
Step IV: Recurse the argument. We have now reduced the (cid:96)∞ norm of the sparse part by a factor of
half  while maintaining its sparsity. We can now go back to steps II and III and repeat the arguments
for subsequent iterations.

a fraction of α = O (1/r) sparse perturbations sufﬁce to erase one of these blocks making it impossible to
recover the matrix.

6

(a)

(b)

(c)

(d)

Figure 2: Comparison of AltProj and IALM on synthetic datasets. (a) Running time of AltProj and
IALM with varying α. (b) Maximum rank of the intermediate iterates of IALM. (c) Running time
of AltProj and IALM with varying µ. (d) Running time of AltProj and IALM with varying r.
4 Experiments

√

√

√

n

n in our experiments.

We now present an empirical study of our AltProj method. The goal of this study is two-fold: a)
establish that our method indeed recovers the low-rank and sparse part exactly  without signiﬁcant
parameter tuning  b) demonstrate that AltProj is signiﬁcantly faster than Conv-RPCA (see (1)); we
solve Conv-RPCA using the IALM method [CLMW11]  a state-of-the-art solver [LCM10]. We
implemented our method in Matlab and used a Matlab implementation of the IALM method by
[LCM10].
We consider both synthetic experiments and experiments on real data involving the problem of
foreground-background separation in a video. Each of our results for synthetic datasets is averaged
over 5 runs.
Parameter Setting: Our pseudo-code (Algorithm 1) prescribes the threshold ζ in Step 4  which
depends on the knowledge of the singular values of the low rank component L∗. Instead  in the
experiments  we set the threshold at the (t + 1)-th step of k-th stage as ζ = µσk+1(M−S(t))
. For
synthetic experiments  we employ the µ used for data generation  and for real-world datasets  we
tune µ through cross-validation. We found that the above thresholding provides exact recovery
while speeding up the computation signiﬁcantly. We would also like to note that [CLMW11] sets
n (assuming m ≤ n). However  we found
the regularization parameter λ in Conv-RPCA (1) as 1/
√
that for problems with large incoherence such a parameter setting does not provide exact recovery.
Instead  we set λ = µ/
Synthetic datasets: Following the experimental setup of [CLMW11]  the low-rank part L∗ = U V T
is generated using normally distributed U ∈ Rm×r  V ∈ Rn×r. Similarly  supp(S∗) is generated
by sampling a uniformly random subset of [m]×[n] with size (cid:107)S∗(cid:107)0 and each non-zero S∗
ij is drawn
√
mn]. For increasing incoherence of L∗ 
i.i.d. from the uniform distribution over [r/(2
we randomly zero-out rows of U  V and then re-normalize them.
There are three key problem parameters for RPCA with a ﬁxed matrix size: a) sparsity of S∗ 
b) incoherence of L∗  c) rank of L∗. We investigate performance of both AltProj and IALM by
varying each of the three parameters while ﬁxing the others. In our plots (see Figure 2)  we report
computational time required by each of the two methods for decomposing M into L + S up to a
relative error ((cid:107)M − L − S(cid:107)F /(cid:107)M(cid:107)F ) of 10−3. Figure 2 shows that AltProj scales signiﬁcantly
better than IALM for increasingly dense S∗. We attribute this observation to the fact that as (cid:107)S∗(cid:107)0
increases  the problem is “harder” and the intermediate iterates of IALM have ranks signiﬁcantly
larger than r. Our intuition is conﬁrmed by Figure 2 (b)  which shows that when density (α) of S∗
is 0.4 then the intermediate iterates of IALM can be of rank over 500 while the rank of L∗ is only
5. We observe a similar trend for the other parameters  i.e.  AltProj scales signiﬁcantly better than
IALM with increasing incoherence parameter µ (Figure 2 (c)) and increasing rank (Figure 2 (d)).
See Appendix C for additional plots.
Real-world datasets: Next  we apply our method to the problem of foreground-background (F-B)
separation in a video [LHGT04]. The observed matrix M is formed by vectorizing each frame and
stacking them column-wise. Intuitively  the background in a video is the static part and hence forms
a low-rank component while the foreground is a dynamic but sparse perturbation.
Here  we used two benchmark datasets named Escalator and Restaurant dataset. The Escalator
dataset has 3417 frames at a resolution of 160 × 130. We ﬁrst applied the standard PCA method for
extracting low-rank part. Figure 3 (b) shows the extracted background from the video. There are

mn)  r/

7

500600700800102n αTime(s)n = 2000  r = 5  µ = 1 AltProjIALM500600700800200400600n αMax. Rankn = 2000  r = 5  µ = 1 IALM11.522.53102µTime(s)n = 2000  r = 10  n α = 100 AltProjIALM50100150200102rTime(s)n = 2000  µ = 1  n α = 3r AltProjIALM(a)

(b)

(c)

(d)

Figure 3: Foreground-background separation in the Escalator video. (a): Original image frame. (b):
Best rank-10 approximation; time taken is 3.1s. (c): Low-rank frame obtained using AltProj; time
taken is 63.2s. (d): Low-rank frame obtained using IALM; time taken is 1688.9s.

(a)

(b)

(c)

(d)

Figure 4: Foreground-background separation in the Restaurant video. (a): Original frame from the
video. (b): Best rank-10 approximation (using PCA) of the original frame; 2.8s were required to
compute the solution (c): Low-rank part obtained using AltProj; computational time required by
AltProj was 34.9s. (d): Low-rank part obtained using IALM; 693.2s required by IALM to compute
the low-rank+sparse decomposition.

several artifacts (shadows of people near the escalator) that are not desirable. In contrast  both IALM
and AltProj obtain signiﬁcantly better F-B separation (see Figure 3(c)  (d)). Interestingly  AltProj re-
moves the steps of the escalator which are moving and arguably are part of the dynamic foreground 
while IALM keeps the steps in the background part. Also  our method is signiﬁcantly faster  i.e. 
our method  which takes 63.2s is about 26 times faster than IALM  which takes 1688.9s.
Restaurant dataset: Figure 4 shows the comparison of AltProj and IALM on a subset of the “Restau-
rant” dataset where we consider the last 2055 frames at a resolution of 120×160. AltProj was around
19 times faster than IALM. Moreover  visually  the background extraction seems to be of better qual-
ity (for example  notice the blur near top corner counter in the IALM solution). Plot(b) shows the
PCA solution and that also suffers from a similar blur at the top corner of the image  while the
background frame extracted by AltProj does not have any noticeable artifacts.
5 Conclusion
In this work  we proposed a non-convex method for robust PCA  which consists of alternating pro-
jections on to low rank and sparse matrices. We established global convergence of our method under
conditions which match those for convex methods. At the same time  our method has much faster
running times  and has superior experimental performance. This work opens up a number of interest-
ing questions for future investigation. While we match the convex methods  under the deterministic
sparsity model  studying the random sparsity model is of interest. Our noisy recovery results as-
sume deterministic noise; improving the results under random noise needs to be investigated. There
are many decomposition problems beyond the robust PCA setting  e.g. structured sparsity models 
robust tensor PCA problem  and so on. It is interesting to see if we can establish global convergence
for non-convex methods in these settings.
Acknowledgements
AA and UN would like to acknowledge NSF grant CCF-1219234  ONR N00014-14-1-0665  and Mi-
crosoft faculty fellowship. SS would like to acknowledge NSF grants 1302435  0954059  1017525
and DTRA grant HDTRA1-13-1-0024. PJ would like to acknowledge Nikhil Srivastava and
Deeparnab Chakrabarty for several insightful discussions during the course of the project.

8

References

[AAJ+13] A. Agarwal  A. Anandkumar  P. Jain  P. Netrapalli  and R. Tandon. Learning
Sparsely Used Overcomplete Dictionaries via Alternating Minimization. Available
on arXiv:1310.7991  Oct. 2013.

[AGH+12] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and M. Telgarsky. Tensor Methods for

Learning Latent Variable Models. Available at arXiv:1210.7559  Oct. 2012.

[ANW12] A. Agarwal  S. Negahban  and M. Wainwright. Noisy matrix decomposition via convex
relaxation: Optimal rates in high dimensions. The Annals of Statistics  40(2):1171–
1197  2012.
Rajendra Bhatia. Matrix Analysis. Springer  1997.
Y. Chen. Incoherence-Optimal Matrix Completion. ArXiv e-prints  October 2013.

[Bha97]
[Che13]
[CLMW11] Emmanuel J. Cand`es  Xiaodong Li  Yi Ma  and John Wright. Robust principal com-

ponent analysis? J. ACM  58(3):11  2011.

[CSPW11] Venkat Chandrasekaran  Sujay Sanghavi  Pablo A. Parrilo  and Alan S. Willsky.
Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization 
21(2):572–596  2011.
Yudong Chen  Sujay Sanghavi  and Huan Xu. Clustering sparse graphs. In Advances
in neural information processing systems  pages 2204–2212  2012.

[CSX12]

[EKYY13] L´aszl´o Erd˝os  Antti Knowles  Horng-Tzer Yau  and Jun Yin. Spectral statistics of

[Har13]

[HKZ11]

[JNS13]

[KC12]

[Kes12]

[LCM10]

Erd˝os–R´enyi graphs I: Local semicircle law. The Annals of Probability  2013.
Moritz Hardt. On the provable convergence of alternating minimization for matrix
completion. arXiv:1312.0925  2013.
Daniel Hsu  Sham M Kakade  and Tong Zhang. Robust matrix decomposition with
sparse corruptions. ITIT  2011.
Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank matrix completion
using alternating minimization. In STOC  2013.
Anastasios Kyrillidis and Volkan Cevher. Matrix alps: Accelerated low rank and sparse
matrix reconstruction. In SSP Workshop  2012.
Raghunandan H. Keshavan. Efﬁcient algorithms for collaborative ﬁltering. Phd Thesis 
Stanford University  2012.
Zhouchen Lin  Minming Chen  and Yi Ma. The augmented lagrange multiplier method
for exact recovery of corrupted low-rank matrices. arXiv:1009.5055  2010.

[LHGT04] Liyuan Li  Weimin Huang  IY-H Gu  and Qi Tian. Statistical modeling of complex

backgrounds for foreground object detection. ITIP  2004.

[MZYM11] Hossein Mobahi  Zihan Zhou  Allen Y. Yang  and Yi Ma. Holistic 3d reconstruction of

[NJS13]

[SAJ14]

[Shi13]

urban structures from low-rank textures. In ICCV Workshops  pages 593–600  2011.
Praneeth Netrapalli  Prateek Jain  and Sujay Sanghavi. Phase retrieval using alternating
minimization. In NIPS  pages 2796–2804  2013.
H. Sedghi  A. Anandkumar  and E. Jonckheere. Guarantees for Stochastic ADMM in
High Dimensions. Preprint.  Feb. 2014.
Lei Shi. Sparse additive text models with low rank background. In Advances in Neural
Information Processing Systems  pages 172–180  2013.

[WHML13] X. Wang  M. Hong  S. Ma  and Z. Luo. Solving multiple-block separable convex
minimization problems using two-block alternating direction method of multipliers.
arXiv:1308.5294  2013.
Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Robust pca via outlier pursuit.
IEEE Transactions on Information Theory  58(5):3047–3064  2012.

[XCS12]

9

,Tamir Hazan
Subhransu Maji
Tommi Jaakkola
Praneeth Netrapalli
Niranjan U N
Sujay Sanghavi
Animashree Anandkumar
Prateek Jain