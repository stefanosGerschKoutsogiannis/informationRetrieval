2016,Dimensionality Reduction of Massive Sparse Datasets Using Coresets,In this paper we present a practical solution with performance guarantees to the problem of dimensionality reduction for very large scale sparse matrices.  We show applications of our approach to computing the Principle Component Analysis (PCA) of any $n\times d$ matrix  using one pass over the stream of its rows. Our solution uses coresets: a scaled subset of the $n$ rows that approximates their sum of squared distances to \emph{every} $k$-dimensional \emph{affine} subspace. An open theoretical problem has been to compute such a coreset that is independent of both $n$ and $d$. An open practical problem has been to compute a non-trivial approximation to the PCA of very large but sparse databases such as the Wikipedia document-term matrix in a reasonable time. We answer both of these questions affirmatively. Our main technical result is a new framework for deterministic coreset constructions based on a reduction to the problem of counting items in a stream.,Dimensionality Reduction of Massive Sparse Datasets

Using Coresets

Dan Feldman

University of Haifa

Haifa  Israel

dannyf.post@gmail.com

mikhail@csail.mit.edu

Mikhail Volkov
CSAIL  MIT

Cambridge  MA  USA

Daniela Rus
CSAIL  MIT

Cambridge  MA  USA
rus@csail.mit.edu

Abstract

In this paper we present a practical solution with performance guarantees to the
problem of dimensionality reduction for very large scale sparse matrices. We
show applications of our approach to computing the Principle Component Anal-
ysis (PCA) of any n × d matrix  using one pass over the stream of its rows. Our
solution uses coresets: a scaled subset of the n rows that approximates their sum
of squared distances to every k-dimensional afﬁne subspace. An open theoretical
problem has been to compute such a coreset that is independent of both n and
d. An open practical problem has been to compute a non-trivial approximation to
the PCA of very large but sparse databases such as the Wikipedia document-term
matrix in a reasonable time. We answer both of these questions afﬁrmatively. Our
main technical result is a new framework for deterministic coreset constructions
based on a reduction to the problem of counting items in a stream.

1

Introduction

Algorithms for dimensionality reduction usually aim to project an input set of d-dimensional vectors
(database records) onto a k ≤ d − 1 dimensional afﬁne subspace that minimizes the sum of squared
distances to these vectors  under some constraints. Special cases include the Principle Component
Analysis (PCA)  Linear regression (k = d − 1)  Low-rank approximation (k-SVD)  Latent Drichlet
Analysis (LDA) and Non-negative matrix factorization (NNMF). Learning algorithms such as k-
means clustering can then be applied on the low-dimensional data to obtain fast approximations with
provable guarantees. To our knowledge  unlike SVD  there are no algorithms or coreset construc-
tions with performance guarantees for computing the PCA of sparse n× n matrices in the streaming
model  i.e. using memory that is poly-logarithmic in n. Much of the large scale high-dimensional
data sets available today (e.g. image streams  text streams  etc.) are sparse. For example  consider
the text case of Wikipedia. We can associate a matrix with Wikipedia  where the English words
deﬁne the columns (approximately 1.4 million) and the individual documents deﬁne the rows (ap-
proximately 4.4 million documents). This large scale matrix is sparse because most English words
do not appear in most documents. The size of this matrix is huge and no existing dimensionality
reduction algorithm can compute its eigenvectors. To this point  running the state of the art SVD
implementation from GenSim on the Wikipedia document-term matrix crashes the computer very
quickly after applying its step of random projection on the ﬁrst few thousand documents. This is
because such dense vectors  each of length 1.4 million  use all of the computer’s RAM capacity.

1Support for this research has been provided by Hon Hai/Foxconn Technology Group and NSFSaTC-BSF
CNC 1526815  and in part by the Singapore MIT Alliance on Research and Technology through the Future of
Urban Mobility project and by Toyota Research Institute (TRI). TRI provided funds to assist the authors with
their research but this article solely reﬂects the opinions and conclusions of its authors and not TRI or any other
Toyota entity. We are grateful for this support.

Submitted to 30th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.

dist2(A  S) :=

dist2(ai  S)

n(cid:88)i=1

Deﬁnition 1 ((k  ε)-coreset). Given a n× d matrix A whose rows a1 ···   an are n points (vec-
tors) in Rd  an error parameter ε ∈ (0  1]  and an integer k ∈ [1  d − 1] = {1 ···   d − 1}
that represents the desired dimensionality reduction  n (k  ε)-coreset for A is a weighted subset
C = {wiai | wi > 0 and i ∈ [n]} of the rows of A  where w = (w1 ···   wn) ∈ [0 ∞)n is a
non-negative weight vector  such that for every afﬁne k-subspace S in Rd we have

(1)

In this paper we present a dimensionality reduction algorithms that can handle very large scale sparse
data sets such as Wikipedia and returns provably correct results. A long-open research question has
been whether we can have a coreset for PCA that is both small in size and a subset of the original
data. In this paper we answer this question afﬁrmatively and provide an efﬁcient construction. We
also show that this algorithm provides a practical solution to a long-standing open practical problem:
computing the PCA of large matrices such as those associated with Wikipedia.

2 Problem Formulation

Given a matrix A  a coreset C in this paper is deﬁned as a weighted subset of rows of A such that the
sum of squared distances from any given k-dimensional subspace to the rows of A is approximately
the same as the sum of squared weighted distances to the rows in C. Formally 
For a compact set S ∈ Rd and a vector x in Rd  we denote the Euclidean distance between x and its
closest points in S by

dist2(x  S) := min

s∈S (cid:107)x − s(cid:107)2

2

For an n×d matrix A whose rows are a1  . . .   an  we deﬁne the sum of the squared distances from
A to S by

(cid:12)(cid:12)dist2(A  S)) − dist2(C  S))(cid:12)(cid:12) ≤ ε dist2(A  S)).

i=1 w2

That is  the sum of squared distances from the n points to S approximates the sum of squared
weighted distances(cid:80)n
i (dist(ai  S))2 to S. The approximation is up to a multiplicative factor
of 1±ε. By choosing w = (1 ···   1) we obtain a trivial (k  0)-coreset. However  in a more efﬁcient
coreset most of the weights will be zero and the corresponding rows in A can be discarded. The
cardinality of the coreset is thus the sparsity of w  given by |C| = (cid:107)w(cid:107)0 := |{wi (cid:54)= 0 | i ∈ [n]}|.
If C is small  then the computation is efﬁcient. Because C is a weighted subset of the rows of A 
if A is sparse  then C is also sparse. A long-open research question has been whether we can have
such a coreset that is both of size independent of the input dimension (n and d) and a subset of the
original input rows.

2.1 Related Work
In [24] it was recently proved that an (k  ε) coreset of size |C| = O(dk3/ε2) exists for every
input matrix  and distances to the power of z ≥ 1 where z is constant. The proof is based on a
general framework for constructing different kinds of coresets  and is known as sensitivity [10  17].
This coreset is efﬁcient for tall matrices  since its cardinality is independent of n. However  it is
useless for “fat” or square matrices (such as the Wikipedia matrix above)  where d is in the order
of n  which is the main motivation for our paper. In [5]  the Frank-Wolfe algorithm was used to
construct different types of coresets than ours  and for different problems. Our approach is based
on a solution that we give to an open problem in [5]  however we can see how it can be used to
compute the coresets in [5] and vice versa. For the special case z = 2 (sum of squared distances) 
a coreset of size O(k/ε2) was suggested in [7] with a randomized version in [8] for a stream of n
points that  unlike the standard approach of using merge-and-reduce trees  returns a coreset of size
independent of n with a constant probability. These result minimizes the (cid:107)·(cid:107)2 error  while our result
minimizes the Frobenius norm  which is always higher  and may be higher by a factor of d. After
appropriate weighting  we can apply the uniform sampling of size O(k/ε2) to get a coreset with a
small Frobenius error [14]  as in our paper. However  in this case the probability of success is only
constant. Since in the streaming case we compute roughly n coresets (formally  O(n/m) coresets 
where m is the size of the coreset) the probability that all these coresets constructions will succeed

2

is close to zero (roughly 1/n). Since the probability of failure in [14] reduces linearly with the size
of the coreset  getting a constant probability of success in the streaming model for O(n) coresets
would require to take coresets of size that is no smaller than the input size.
There are many papers  especially in recent years  regarding data compression for computing the
SVD of large matrices. None of these works addresses the fundamental problem of computing a
sparse approximated PCA for a large matrix (in both rows and columns)  such as Wikipedia. The
reason is that current results use sketches which do no preserve the sparsity of the data (e.g. because
of using random projections). Hence  neither the sketch nor the PCA computed on the sketch is
sparse. On the other side  we deﬁne coreset as a small weighted subset of rows  which is thus
sparse if the input is sparse. Moreover  the low rank approximation of a coreset is sparse  since
each of its right singular vectors is a sum of a small set of sparse vectors. While there are coresets
constructions as deﬁned in this paper  all of them have cardinality of at least d points  which makes
them impractical for large data matrices  where d ≥ n. In what follows we describe these recent
results in details.
The recent results in [7  8] suggest coresets that are similar to our deﬁnition of coresets (i.e.  weighted
subsets)  and do preserve sparsity. However  as mentioned above they minimize the 2-norm error and
not the larger Frobesnius error  and maybe more important  they provide coresets for k-SVD (i.e. 
k-dimensional subspaces) and not for PCA (k-dimensional afﬁne subspaces that might not intersect
the origin). In addition [8] works with constant probability  while our algorithm is deterministic
(works with probability 1).
Software. Popular software for computing SVD such as GenSim [21]  redsvd [12] or the MATLAB
sparse SVD function (svds) use sketches and crash for inputs of a few thousand of documents and
a dimensionality reduction (approximation rank) k < 100 on a regular laptop  as expected from
the analysis of their algorithms. This is why existing implementations (including Gensim) extract
topics from large matrices (e.g. Wikipedia)  based on low-rank approximation of only small subset
of few thousands of selected words (matrix columns)  and not the complete Wikipedia matrix.Even
for k = 3  running the implementation of sparse SVD in Hadoop [23] took several days [13]. Next
we give a broad overview of the very latest state of the dimensionality reduction methods  such as
the Lanczoz algorithm [16] for large matrices  that such systems employ under the hood.
Coresets. Following a decade of research in [24] it was recently proved that an (ε  k)-coreset for low
rank approximation of size |C| = O(dk3/ε2) exists for every input matrix. The proof is based on a
general framework for constructing different kinds of coresets  and is known as sensitivity [10  17].
This coreset is efﬁcient for tall matrices  since its cardinality is independent of n. However  it is
useless for “fat” or square matrices (such as the Wikipedia matrix above)  where d is in the order
of n  which is the main motivation for our paper. In [5]  the Frank-Wolfe algorithm was used to
construct different types of coresets than ours  and for different problems. Our approach is based on
a solution that we give to an open problem in [5].
Sketches. A sketch in the context of matrices is a set of vectors u1 ···   us in Rd such that the sum of
squared distances(cid:80)n
i=1(dist(ai  S))2 from the input n points to every k-dimensional subspace S in
Rd  can be approximated by(cid:80)n
i=1(dist(ui  S))2 up to a multiplicative factor of 1±ε. Note that even
if the input vectors a1 ···   an are sparse  the sketched vectors u1 ···   us in general are not sparse 
unlike the case of coresets. A sketch of cardinality d can be constructed with no approximation error
(ε = 0)  by deﬁning u1 ···   ud to be the d rows of the matrix DV T where U DV T = A is the SVD
of A. It was proved in [11] that taking the ﬁrst O(k/ε) rows of DV T yields such a sketch  i.e. of
size independent of n and d.
The ﬁrst sketch for sparse matrices was suggested in [6]  but like more recent results  it assumes that
the complete matrix ﬁts in memory. Other sketching methods that usually do not support streaming
include random projections [2  1  9] and randomly combined rows [20  25  22  18].
The Lanczoz Algorithm. The Lanczoz method [19] and its variant [15] multiply a large matrix by a
vector for a few iterations to get its largest eigenvector v1. Then the computation is done recursively
after projecting the matrix on the hyperplane that is orthogonal to v1. However  v1 is in general not
sparse even A is sparse. Hence  when we project A on the orthogonal subspace to v1  the resulting
matrix is dense for the rest of the computations (k > 1). Indeed  our experimental results show that
the MATLAB svds function which uses this method runs faster than the exact SVD  but crashes on
large input  even for small k.

3

This paper builds on this extensive body of prior work in dimensionality reduction  and our approach
uses coresets to solve the time and space challenges.

2.2 Key Contributions

Our main result is the ﬁrst algorithm for computing an (k  ε)-coreset C of size independent of
both n and d  for any given n × d input matrix. The algorithm takes as input a ﬁnite set of d-
dimensional vectors  a desired approximation error ε  and an integer k ≥ 0. It returns a weighted
subset S (coreset) of k2/ε2 such vectors. This coreset S can be used to approximate the sum of
squared distances from the matrix A ∈ Rn×d  whose rows are the n vectors seen so far  to any
k-dimensional afﬁne subspace in Rd  up to a factor of 1 ± ε. For a (possibly unbounded) stream of
such input vectors the coreset can be maintained at the cost of an additional factor of log2 n.
The polynomial dependency on d of the cardinality of previous coresets made them impractical for
fat or square input matrices  such as Wikipedia  images in a sparse feature space representation  or
adjacency matrix of a graph. If each row of in input matrix A has O(nnz) non-zeroes entries  then
the update time per insertion  the overall memory that is used by our algorithm  and the low rank
approximation of the coreset S is O(nnz · k2/ε2)  i.e. independent of n and d.
We implemented our algorithm to obtain a low-rank approximation for the term-document matrix
of Wikipedia with provable error bounds. Since our streaming algorithm is also “embarrassingly
parallel” we run it on Amazon Cloud  and receive a signiﬁcantly better running time and accuracy
compared to existing heuristics (e.g. Hadoop/MapReduce) that yield non-sparse solutions.
The key contributions in this work are:

1. A new algorithm for dimensionality reduction of sparse data that uses a weighted subset of

the data  and is independent of both the size and dimensionality of the data.

2. An efﬁcient algorithm for computing such a reduction  with provable bounds on size and

running time (cf. http://people.csail.mit.edu/mikhail/NIPS2016).

3. A system that implements this dimensionality reduction algorithm and an application of the system

to compute latent semantic analysis (LSA) of the entire English Wikipedia.

3 Technical Solution

Given a n×d matrix A  we propose a construction mechanism for a matrix C of size |C| = O(k2/ε2)
and claim that it is a (k  ε)-coreset for A. We use the following corollary for Deﬁnition 1 of a coreset 
based on simple linear algebra that follows from the geometrical deﬁnitions (e.g. see [11]).
Property 1 (Coreset for sparse matrix). Let A ∈ Rn×d  k ∈ [1  d − 1] be an integer  and let ε > 0
be an error parameter. For a diagonal matrix W ∈ Rn×n  the matrix C = W A is a (k  ε)-coreset
for A if for every matrix X ∈ Rd×(d−k) such that X T X = I  we have

and

(ii) (cid:107)A − W A(cid:107) < ε var(A)

(2)

(cid:107)AX(cid:107) (cid:12)(cid:12)(cid:12)(cid:12) ≤ ε 
(i) (cid:12)(cid:12)(cid:12)(cid:12)1 − (cid:107)W AX(cid:107)

where var(A) is the sum of squared distances from the rows of A to their mean.

The goal of this paper is to prove that such a coreset (Deﬁnition 1) exists for any matrix A (Prop-
erty 1) and can be computed efﬁciently. Formally 
Theorem 1. For every input matrix A ∈ Rn×d  an error ε ∈ (0  1] and an integer k ∈ [1  d − 1]:

(a) there is a (k  ε)-coreset C of size |C| = O(k2/ε2);
(b) such a coreset can be constructed in O(k2/ε2) time.

Theorem 1 is the formal statement for the main technical contribution of this paper. Sections 3–5
constitute a proof for Theorem 1.
To establish Theorem 1(a)  we ﬁrst state our two main results (Theorems 2 and 3) axiomatically  and
show how they combine such that Property 1 holds. Thereafter we prove the these results in Sections
4 and 5  respectively. To prove Theorem 1(b) (efﬁcient construction) we present an algorithm for

4

Algorithm 1 CORESET-SUMVECS(A  ε)

(a) Coreset for sum of vectors algorithm (b) Illustration showing ﬁrst 3 steps of the computation

computing a matrix C  and analyze the running time to show that the C can be constructed in
O(k2/ε2) iterations.
Let A ∈ Rn×d be a matrix of rank d  and let U ΣV T = A denote its full SVD. Let W ∈ Rn×n be a
diagonal matrix. Let k ∈ [1  d − 1] be an integer. For every i ∈ [n] let
Ui k+1:dΣk+1:d k+1:d

(3)

vi =(cid:18)Ui 1 ···   Ui k 

(cid:107)Σk+1:d k+1:d(cid:107)

  1(cid:19) .

Then the following two results hold:
Theorem 2 (Coreset for sum of vectors). For every set of of n vectors v1 ···   vn in Rd and every
ε ∈ (0  1)  a weight vector w ∈ (0 ∞)n of sparsity (cid:107)w(cid:107)0 ≤ 1/ε2 can be computed deterministically
in O(nd/ε) time such that

n(cid:88)i=1

(cid:107)vi(cid:107)2.

(4)

Section 4 establishes a proof for Theorem 2.
Theorem 3 (Coreset for Low rank approximation). For every X ∈ Rd×(d−k) such that X T X = I 

vi −

n(cid:88)i=1

n(cid:88)i=1

wivi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ ε
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:107)AX(cid:107)2 (cid:12)(cid:12)(cid:12)(cid:12) ≤ 5(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:12)(cid:12)(cid:12)(cid:12)1 − (cid:107)W AX(cid:107)2
n(cid:88)i=1

vivT

i − Wi ivivT

.

(5)

i (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Section 5 establishes a proof for Theorem 3.

3.1 Proof of Theorem 1

Proof of Theorem 1(a). Replacing vi with vivT
yields

i   (cid:107)vi(cid:107)2 with (cid:107)vivT

i (cid:107)  and ε by ε/(5k) in Theorem 2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)i

vivT

i − Wi ivivT

i (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (ε/5k)

5

n(cid:88)i=1

(cid:107)vivT

i (cid:107) = ε.

000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053Algorithm1CORESET-SUMVECS(A ε)1:Input:A:ninputpointsa1 ... aninRd2:Input:ε∈(0 1):theapproximationerror3:Output:w∈[0 ∞)n:non-negativeweights4:A←A−mean(A)5:A←cAwherecisaconstants.t.var(A)=16:w←(1 0 ... 0)7:j←1 p←Aj J←{j}8:Mj=(cid:8)y2|y=A·ATj(cid:9)9:fori=1 ... ndo10:j←argmin{wJ·MJ}11:G←W0·AJwhereW0i i=√wi12:kck=kGTG)k2F13:c·p=P|J|i=1GpT14:kc−pk=p1+kck2−c·p15:compp(v)=1/kc−pk−(c·p)/kc−pk16:kc−c0k=kc−pk−compp(v)17:α=kc−c0k/kc−pk18:w←w(1−|α|)19:wj←wj+α20:w←w/Pni=1wi21:Mj←(cid:8)y2|y=A·ATj(cid:9)22:J←J∪{j}23:ifkck2≤εthen24:break25:endif26:endfor27:returnw1-1-0.8-0.6-0.4-0.200.20.40.60.81-1-0.8-0.6-0.4-0.200.20.40.60.81c2a3c3a1 = c1a2a4a5Combining this inequality with (4) gives

(cid:107)AX(cid:107)2 (cid:12)(cid:12)(cid:12)(cid:12) ≤ 5(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:12)(cid:12)(cid:12)(cid:12)1 − (cid:107)W AX(cid:107)2
n(cid:88)i=1

vivT

i − Wi ivivT

i (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ ε.

Thus the left-most term is bounded by the right-most term  which proves (2). This also means that
C = W A is a coreset for k-SVD  i.e.  (non-afﬁne) k-dimensional subspaces. To support PCA
(afﬁne subspaces) the coreset C = W A needs to satisfy the expression in the last line of Property 1
regarding its mean. This holds using the last entry (one) in the deﬁnition of vi (3)  which implies
that the sum of the rows is preserved as in equation (4). Therefore Property 1 holds for C = W A 
which proves Theorem 1(a).
Claim Theorem 1(b) follows from simple analysis of Algorithm 2 that implements this construction.

4 Coreset for Sum of Vectors (k = 0)

In order to prove the general result Theorem 1(a)  that is the existence of a (k  ε)-coreset for any
k ∈ [1  d−1]  we ﬁrst establish the special case for k = 0. In this section  we prove Theorem 2 by
providing an algorithm for constructing a small weighted subset of points that constitutes a general
approximation for the sum of vectors.
To this end  we ﬁrst introduce an intermediate result that shows that given n points on the unit ball
with weight distribution z  there exists a small subset of points whose weighted mean is approxi-
mately the same as the weighted mean of the original points.

Let Dn denote the union over every vector z ∈ [0  1]n that represent a distribution  i.e. (cid:80)i zi = 1.
Our ﬁrst technical result is that for any ﬁnite set of unit vectors a1  . . .   an in Rd  any distribution
z ∈ Dn  and every ε ∈ (0  1]  we can compute a sparse weight vector w ∈ Dn of sparsity (non-
zeroes entries) (cid:107)w(cid:107)0 ≤ 1/ε2.
Lemma 1. Let z ∈ Dn be a distribution over n unit vectors a1 ···   an in Rd. For ε ∈ (0  1)  a
sparse weight vector w ∈ Dn of sparsity s ≤ 1/ε2 can be computed in O(nd/ε2) time such that

zi · ai −

n(cid:88)i=2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
n(cid:88)i=1

wi ai(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

≤ ε.

(6)

Proof of Lemma 1. Please see Supplementary Material  Section A.

We prove Theorem 2 by providing a computation of such a sparse weight vector w. The intuition
for this computation is as follows. Given n input points a1 . . .  an in Rd  with weighted mean

(cid:80)i zi ai = 0  we project all the points on the unit sphere. Pick an arbitrary starting point a1 = c1.
At each step ﬁnd the farthest point aj+1 from cj  and compute cj+1 by projecting the origin onto
the line segment [cj  aj+1]. Repeat this for j = 1 . . .  N iterations  where N = 1/ε2. We prove that
(cid:107)ci(cid:107)2 = 1/i  thus if we iterate 1/2 times  this norm will be (cid:107)c1/2(cid:107) = 2. The resulting points ci
are a weighted linear combination of a small subset of the input points. The output weight vector
w ∈ Dn satisﬁes cN =(cid:80)n

Fig. 1a contains the pseudocode for Algorithm 1. Fig. 1b illustrates the ﬁrst steps of the main com-
putation (lines 9–26). Please see Supplementary Material  Section C for a complete line-by-line
analysis of Algorithm 1.

i=1 wi ai  and this weighted subset forms the coreset.

Proof of Theorem 2. The proof of Theorem 2 follows by applying Lemma 1 after normalization of
the input points and then post-processing the output.

5 Coreset for Low Rank Approximation (k > 0)

In Section 4 we presented a new coreset construction for approximating the sum of vectors  showing
that given n points on the unit ball there exists a small weighted subset of points that is a coreset
for those points. In this section we describe the reduction of Algorithm 1 for k = 0 to an efﬁcient
algorithm for any low rank approximation with k ∈ [1  d−1].

6

Algorithm 2 CORESET-LOWRANK(A  k  ε)

(a) 1/2: Initialization

(b) 2/2: Computation

Conceptually  we achieve this reduction in two steps. The ﬁrst step is to show that Algorithm 1 can
be reduced to an inefﬁcient computation for low rank approximation for matrices. To this end  we
ﬁrst prove Theorem 3  thus completing the existence clause Theorem 1(a).

i=1(1 − W 2

i i)vivT

i (cid:107). For every i ∈ [n] let ti = 1 − W 2

Proof of Theorem 3. Let ε = (cid:107)(cid:80)n
i i. Set
X ∈ Rd×(d−k) such that X T X = I. Without loss of generality we assume V T = I  i.e. A = U Σ 
otherwise we replace X by V T X. It thus sufﬁces to prove that(cid:12)(cid:12)(cid:80)i ti(cid:107)Ai :X(cid:107)2(cid:12)(cid:12) ≤ 5ε(cid:107)AX(cid:107)2.
Using the triangle inequality  we get
ti(cid:107)Ai :X(cid:107)2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)i
+(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)i

ti(cid:107)Ai :X(cid:107)2 −(cid:88)i
ti(cid:107)(Ai 1:k  0)X(cid:107)2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

We complete the proof by deriving bounds on (7) and (8)  thus proving (5). For the complete proof 
please see Supplementary Material  Section B.

ti(cid:107)(Ai 1:k  0)X(cid:107)2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)i

(7)

(8)

Together  Theorems 2 and 3 show that the error of the coreset is a 1 ± ε approximation to the true
weighted mean. By Theorem 3  we can now simply apply Algorithm 1 to the right hand side of (5)
to compute the reduction. The intuition for this inefﬁcient reduction is as follows. We ﬁrst compute
the outer product of each row vector x in the input matrix A ∈ R[n×d]. Each such outer products
xT x is a matrix in Rd×d. Next  we expand every such matrix into a vector  in Rd2 by concatenating
its entries. Finally  we combine each such vector back to be a vector in the matrix P ∈ Rn×d2. At
this point the reduction is complete  however it is clear that this matrix expansion is inefﬁcient.
The second step of the reduction is to transform the slow computation of running Algorithm 1 on the
expanded matrix P ∈ Rn×d2 into an equivalent and provably fast computation on the original set of
points A ∈ Rd. To this end we make use of the fact that each row of P is a sparse vector in Rd to
implicitly run the computation in the original row space Rd. We present Algorithm 2 and prove that
it returns the weight vector w = (w1 ···   wn) of a (k  ε)-coreset for low-rank approximation of the
input point set P   and that this coreset is small  namely  only O(k2/ε2) of the weights (entries) in w
are non-zeros. Fig. 5 contains the pseudocode for Algorithm 2. Please see Supplementary Material 
Section D for a complete line-by-line analysis of Algorithm 2.

6 Evaluation and Experimental Results

The coreset construction algorithm described in Section 5 was implemented in MATLAB. We make
use of the redsvd package [12] to improve performance  but it is not required to run the system. We
evaluate our system on two types of data: synthetic data generated with carefully controlled param-
eters  and real data from the English Wikipedia under the “bag of words” (BOW) model. Synthetic
data provides ground-truth to evaluate the quality  efﬁciency  and scalability of our system  while
the Wikipedia data provides us with a grand challenge for latent semantic analysis computation.

7

000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053Algorithm1CORESET-LOWRANK(A k ε)1:Input:A:Asparsen×dmatrix2:Input:k∈Z>0:theapproximationrank3:Input:ε∈(cid:0)0 12(cid:1):theapproximationerror4:Output:w∈[0 ∞)n:non-negativeweights5:ComputeUΣVT=A theSVDofA6:R←Σk+1:d k+1:d7:P←matrixwhosei-throw∀i∈[n]is8:Pi=(Ui 1:k Ui k+1:d·RkRkF)9:X←matrixwhosei-throw∀i∈[n]is10:Xi=Pi/kPikF11:w←(1 0 ... 0)12:fori=1 ... (cid:6)k2/ε2(cid:7)do13:j←argmini=1 ... n{wXXi}14:a=Pni=1wi(XTiXj)215:b=1−kPXjk2F+Pni=1wikPXik2FkPk2F16:c=kwXk2F17:α=(1−a+b)/(1+c−2a)18:w←(1−α)Ij+αw19:endfor20:returnw1000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053Algorithm1CORESET-LOWRANK(A k ε)1:Input:A:Asparsen×dmatrix2:Input:k∈Z>0:theapproximationrank3:Input:ε∈(cid:0)0 12(cid:1):theapproximationerror4:Output:w∈[0 ∞)n:non-negativeweights5:ComputeUΣVT=A theSVDofA6:R←Σk+1:d k+1:d7:P←matrixwhosei-throw∀i∈[n]is8:Pi=(Ui 1:k Ui k+1:d·RkRkF)9:X←matrixwhosei-throw∀i∈[n]is10:Xi=Pi/kPikF11:w←(1 0 ... 0)12:fori=1 ... (cid:6)k2/ε2(cid:7)do13:j←argmini=1 ... n{wXXi}14:a=Pni=1wi(XTiXj)215:b=1−kPXjk2F+Pni=1wikPXik2FkPk2F16:c=kwXk2F17:α=(1−a+b)/(1+c−2a)18:w←(1−α)Ij+αw19:endfor20:returnw1(a) Relative error (k = 10)

(b) Relative error (k = 20)

(c) Relative error (k = 50)

(d) Synthetic data errors (e) Wikipedia running time (x-axis log

scale)

(f) Wikipedia log errors

Figure 1: Experimental results for synthetic data (Fig. 1a–1d) and Wikipedia (Fig. 1e–Fig. 1f).

For our synthetic data experiments  we used a moderate size sparse input of (5000×1000) to evaluate
the relationship between the error ε and the number of iterations of the algorithm N. We then
compare our coreset against uniform sampling and weighted random sampling using the squared
norms of U (A = U ΣV T ) as the weights. Finally  we evaluate the efﬁciency of our algorithm by
comparing the running time against the MATLAB svds function and against the most recent state
of the art dimensionality reduction algorithm [8]. Figure 1a–1d show the exerimental results. Please
see Supplementary Material  Section E for a complete description of the experiments.

6.1 Latent Semantic Analysis of Wikipedia

For our large-scale grand challenge experiment  we apply our algorithm for computing Latent Se-
mantic Analysis (LSA) on the entire English Wikipedia. The size of the data is n = 3.69M (docu-
ments) with a dimensionality d = 7.96M (words). We specify a nominal error of ε = 0.5  which is a
theoretical upper bound for N = 2k/ε iterations  and show that the coreset error remains bounded.
Figure 1f shows the log approximation error  i.e. sum of squared distances of the coreset to the sub-
space for increasing approximation rank k = 1  10  100. We see that the log error is proportional to
k  and as the number of streamed points increases into the millions  coreset error remains bounded
by k. Figure 1e shows the running time of our algorithm compared against svds for increasing
dimensionality d and a ﬁxed input size n = 3.69M (number of documents).
Finally  we show that our coreset can be used to create a topic model of 100 topics for the entire
English Wikipedia. We construct the coreset of size N = 1000 words. Then to generate the topics 
we compute a projection of the coreset onto a subspace of rank k = 100. Please see Supplementary
Material  Section F for more details  including an example of the topics obtained in our experiments.

7 Conclusion

We present a new approach for dimensionality reduction using coresets. Our solution is general and
can be used to project spaces of dimension d to subspaces of dimension k < d. The key feature of
our algorithm is that it computes coresets that are small in size and subsets of the original data. We
benchmark our algorithm for quality  efﬁciency  and scalability using synthetic data. We then apply
our algorithm for computing LSA on the entire Wikipedia – a computation task hitherto not possible
with state of the art algorithms. We see this work as a theoretical foundation and practical toolbox
for a range of dimensionality reduction problems  and we believe that our algorithms will be used to

8

Coreset size (number of points)0102030405060Relative error#10-400.511.522.533.54SVD CoresetUniform Random SamplingWeighted Random SamplingCoreset size (number of points)01020304050607080Relative error#10-4012345SVD CoresetUniform Random SamplingWeighted Random SamplingCoreset size (number of points)0102030405060708090100Relative error#10-300.10.20.30.40.50.60.70.80.91SVD CoresetUniform Random SamplingWeighted Random SamplingNumber of iterations N0200400600800100012001400160018002000f(N)00.10.20.30.40.50.60.70.80.91A[5000x1000]  sparsity=0.0333f(N) = epsf(N) = N epsf(N) = N logN epsf(N) = N2 epsf(N) = f*(N)+CNumber of million points streamed00.511.522.533.5log10 eps-5-4.5-4-3.5-3-2.5-2-1.5-1-0.50Wikipedia approximation log errork = 1k = 10k = 100construct many other coresets in the future. Our project codebase is open-sourced and can be found
here: http://people.csail.mit.edu/mikhail/NIPS2016.

References
[1] D. Achlioptas and F. Mcsherry. Fast computation of low-rank matrix approximations. Journal of the ACM

(JACM)  54(2):9  2007.

[2] S. Arora  E. Hazan  and S. Kale. A fast random sampling algorithm for sparsifying matrices. In Approx-
imation  Randomization  and Combinatorial Optimization. Algorithms and Techniques  pages 272–279.
Springer  2006.

[3] J. Batson  D. A. Spielman  and N. Srivastava. Twice-ramanujan sparsiﬁers. SIAM Journal on Computing 

41(6):1704–1721  2012.

[4] C. Carath´eodory. ¨Uber den variabilit¨atsbereich der fourierschen konstanten von positiven harmonischen

funktionen. Rendiconti del Circolo Matematico di Palermo (1884-1940)  32(1):193–217  1911.

[5] K. L. Clarkson. Coresets  sparse greedy approximation  and the frank-wolfe algorithm. ACM Transactions

on Algorithms (TALG)  6(4):63  2010.

[6] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In
Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing  pages 81–90. ACM  2013.

[7] M. B. Cohen  S. Elder  C. Musco  C. Musco  and M. Persu. Dimensionality reduction for k-means
clustering and low rank approximation. In Proceedings of the Forty-Seventh Annual ACM on Symposium
on Theory of Computing  pages 163–172. ACM  2015.

[8] M. B. Cohen  C. Musco  and J. W. Pachocki. Online row sampling. CoRR  abs/1604.05448  2016.

[9] P. Drineas and A. Zouzias. A note on element-wise matrix sparsiﬁcation via a matrix-valued bernstein

inequality. Information Processing Letters  111(8):385–389  2011.

[10] D. Feldman and M. Langberg. A uniﬁed framework for approximating and clustering data. In Proc. 41th

Ann. ACM Symp. on Theory of Computing (STOC)  2010. Manuscript available at arXiv.org.

[11] D. Feldman  M. Schmidt  and C. Sohler. Turning big data into tiny data: Constant-size coresets for k-
means  pca and projective clustering. Proceedings of ACM-SIAM Symposium on Discrete Algorithms
(SODA)  2013.

[12] Google. redsvd. https://code.google.com/archive/p/redsvd/  2011.
[13] N. P. Halko. Randomized methods for computing low-rank approximations of matrices. PhD thesis 

University of Colorado  2012.

[14] M. Inaba  N. Katoh  and H. Imai. Applications of weighted voronoi diagrams and randomization to
variance-based k-clustering. In Proceedings of the tenth annual symposium on Computational geometry 
pages 332–339. ACM  1994.

[15] M. Journ´ee  Y. Nesterov  P. Richt´arik  and R. Sepulchre. Generalized power method for sparse principal

component analysis. The Journal of Machine Learning Research  11:517–553  2010.

[16] C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and

integral operators. United States Governm. Press Ofﬁce Los Angeles  CA  1950.

[17] M. Langberg and L. J. Schulman. Universal ε approximators for integrals. Proceedings of ACM-SIAM

Symposium on Discrete Algorithms (SODA)  2010.

[18] E. Liberty  F. Woolfe  P.-G. Martinsson  V. Rokhlin  and M. Tygert. Randomized algorithms for the
low-rank approximation of matrices. Proceedings of the National Academy of Sciences  104(51):20167–
20172  2007.

[19] C. C. Paige. Computational variants of the lanczos method for the eigenproblem. IMA Journal of Applied

Mathematics  10(3):373–381  1972.

[20] C. H. Papadimitriou  H. Tamaki  P. Raghavan  and S. Vempala. Latent semantic indexing: A probabilistic
analysis. In Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles
of database systems  pages 159–168. ACM  1998.

[21] R. Ruvrek  P. Sojka  et al. Gensimstatistical semantics in python. 2011.
[22] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In Foundations

of Computer Science  2006. FOCS’06. 47th Annual IEEE Symposium on  pages 143–152. IEEE  2006.

9

,Yash Deshpande
Andrea Montanari
Emile Richard
Dan Feldman
Mikhail Volkov
Daniela Rus