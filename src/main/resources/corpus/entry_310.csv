2019,AGEM: Solving Linear Inverse Problems via Deep Priors and Sampling,In this paper we propose to use a denoising autoencoder (DAE) prior to simultaneously solve a linear inverse problem and estimate its noise parameter. Existing DAE-based methods estimate the noise parameter empirically or treat it as a tunable hyper-parameter. We instead propose autoencoder guided EM  a probabilistically sound framework that performs Bayesian inference with intractable deep priors. We show that efficient posterior sampling from the DAE can be achieved via Metropolis-Hastings  which allows the Monte Carlo EM algorithm to be used. We demonstrate competitive results for signal denoising  image deblurring and image devignetting. Our method is an example of combining the representation power of deep learning with uncertainty quantification from Bayesian statistics.,AGEM: Solving Linear Inverse Problems

via Deep Priors and Sampling

Bichuan Guo

Tsinghua University

gbc16@mails.tsinghua.edu.cn

Yuxing Han

South China Agricultural University

yuxinghan@scau.edu.cn

Jiangtao Wen

Tsinghua University

jtwen@tsinghua.edu.cn

Abstract

In this paper we propose to use a denoising autoencoder (DAE) prior to simulta-
neously solve a linear inverse problem and estimate its noise parameter. Existing
DAE-based methods estimate the noise parameter empirically or treat it as a tunable
hyper-parameter. We instead propose autoencoder guided EM  a probabilistically
sound framework that performs Bayesian inference with intractable deep priors.
We show that efﬁcient posterior sampling from the DAE can be achieved via
Metropolis-Hastings  which allows the Monte Carlo EM algorithm to be used. We
demonstrate competitive results for signal denoising  image deblurring and image
devignetting. Our method is an example of combining the representation power of
deep learning with uncertainty quantiﬁcation from Bayesian statistics.

1

Introduction

A variety of inverse problems  including sensor denoising [27] and image restoration [2]  can be
formulated as recovering a latent signal x from noisy observations y = Hx + n  where H is
the observation model and n is the noise. Model-based reconstruction methods [13  20  35] use
priors to constrain the solution space. More recently  data-driven deep priors have been shown to
outperform traditional analytic priors [24]. Here we adopt the unsupervised learning approach: unlike
discriminative learning which requires task-speciﬁc data and training  deep priors trained with a DAE
[36] can be used in a plug-and-play way [3  4  25]  without ﬁne-tuning for speciﬁc tasks H.
The noise level of n is essential for controlling the strength of prior. For example  data corrupted by
large noises should be handled with strong priors. For real data  the noise level is usually unknown (i.e.
noise-blind) and needs to be estimated. Although deep priors are able to capture highly sophisticated
data distribution  they often lack the analytic tractability for statistical inference. As a result  many
DAE-based methods either treat the noise level as a tunable hyper-parameter [3  39]  or empirically
compute an adaptive estimate during gradient based optimization [4]  without correctness guarantee.
In this paper  we propose a probabilistic framework that combines DAE priors with tractable inference.
The latent signal x and the noise level are estimated simultaneously. We rely on the observation that a
trained DAE captures the score of data distribution (gradient of log density) [1]. The key component
of our method is that the intractable posterior distribution of x can be efﬁciently sampled with a
Metropolis-Hastings [16] sampler. As a consequence  the maximum likelihood estimate (MLE) of
the noise level can be obtained using the Monte Carlo EM algorithm [40]. The solution of x can be
constructed from the converged samples  e.g. a minimum mean squared error (MMSE) estimator can

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

be computed from the posterior mean. We call our method autoencoder guided EM (AGEM)  it is an
example of marrying unsupervised deep learning with statistical inference.
One important implication of our method is that  with the aid of sampling-based approximate inference
methods  a deep prior deﬁned by a DAE can operate analytically much like closed-form priors. We
demonstrate our proposed method on signal denoising  image deblurring and image devignetting 
and conduct thorough ablation studies. Our approach outperforms the state-of-the-art DAE-based
methods on all three tasks. In summary  the main contributions of this paper are:

• The solution of a linear inverse problem and noise level estimation are uniﬁed in a proba-
bilistically sound framework  which can be solved using the Monte Carlo EM algorithm.
• The Monte Carlo E-step performs efﬁcient posterior sampling with a special Metropolis-
• The solution to the problem can be constructed from posterior samples according to Bayesian
decision theory. Using a quadratic loss  the posterior mean provides an MMSE estimator.

Hastings algorithm  despite using an implicit prior deﬁned by a DAE.

2 Background

Using the above notation  we say a linear inverse problem has a known noise level Σ  if

y = Hx + n  n ∼ N (0  Σ).

(1)

A wide range of problems can be covered by this formulation. For example  for image denoising 
H is the identity operator. If x is convolved with some kernel  H is the Toeplitz matrix [15] of that
kernel. The solution of (1) can be obtained by considering the (log) posterior distribution:

log Pr(x | y  Σ) = log Pr(y | x  Σ) + log Pr(x) + const.

(2)
we can view log Pr(y | x  Σ) as a data term determined by model (1)  and log Pr(x) as a prior term.
The data term ensures that x agrees with the observation y  and the prior term regularizes x to lie
in some desired solution space. For various types of data (e.g. images)  many analytic priors have
been proposed [17  21  30]. In this paper  we are interested in data-driven deep priors  as they can
beneﬁt from large amount of data and require less handcrafting. Speciﬁcally  we focus on deep priors
deﬁned by a DAE. Since a DAE uses unsupervised training  it can directly capture the probability
distribution of x and does not rely on the context of task H (i.e. plug-and-play)  which makes it more
general and widely applicable than other context-dependent priors.

DAE prior. A DAE is trained to minimize the following denoising criterion:

LDAE = Ex η[(cid:96)(x  r(x + η))] 

(3)
where (cid:96)(·) is the loss function  r(·) is the reconstruction function deﬁned by the DAE  and η is a
stochastic noise. The expectation is taken over the discrete training set of x and the noise distribution.
Besides the plug-and-play property  a DAE also provides good analytic property  as we show below.
Alain and Bengio [1] proved the theorem that if a DAE is trained with quadratic loss and isotropic
Gaussian noise η ∼ N (0  σ2

trI)  the optimal reconstruction function r∗(x) satisﬁes

∗(x) = x + σ2

tr∇x log Pr(x) + o(σ2

tr)  as σtr → 0 

r

(4)
where Pr(x) is the training data distribution  and o(·) is the little-o notation. We see that the
reconstruction error r∗(x) − x captures the score (gradient of log density)  which enables gradient-
based optimization to be used for (2). With this theorem  multiple DAE-based methods for solving
(1) have been proposed. DAEP [3] seeks the maximum-a-posterior (MAP) estimator

xMAP = argmaxx log Pr(y | x  Σ) + log Pr(x).

(5)
It uses the negative square magnitude of reconstruction error −(cid:107)r(x) − x(cid:107)2 as a proxy prior  as
it vanishes at the maxima of Pr(x). DMSP [4] proposes a Bayes estimator for a speciﬁc utility
function by smoothing Pr(x) with the Gaussian kernel N (0  σ2
trI)  then makes use of an exact version
(without the little-o) of (4).

2

Plug-and-play ADMM. Another DAE-based approach that does not rely on (4) originates from
the fact that DAE can be used as a denoiser [8  41]. The plug-and-play ADMM method [5] converts
(5) into a constrained optimization problem:

(xMAP  xMAP) = argmax(x v)

subject to x = v.

log Pr(y | x  Σ) + log Pr(v) 

(6)

(7)
(8)
(9)

This maximizer can then be found by repeatedly solving a sequence of subproblems:

the x-subproblem: x(k+1) = argmaxx log Pr(y | x  Σ) − λ
the v-subproblem: v(k+1) = argmaxv log Pr(v) − λ
update: u(k+1) = u(k) + x(k+1) − v(k+1).

2(cid:107)x − v(k) + u(k)(cid:107)2 
2(cid:107)v − (x(k+1) + u(k))(cid:107)2 

Here λ is a positive hyper-parameter. The x-subproblem (7) has an analytic solution  while the
v-subproblem (8) can be interpreted as a denoising step. An off-the-shelf denoiser can be used
[35] to implicitly deﬁne Pr(v). Speciﬁcally  the DAE can be used to replace (8) as v(k+1) =
r(x(k+1) + u(k)). Under mild conditions [5]  the iterates (7)-(9) converge to the correct solution.

3 Method

The previous discussion assumes the noise level Σ in the data term
2 (y − Hx)(cid:62)Σ−1(y − Hx) − 1

log Pr(y | x  Σ) = − 1

2 log|Σ| + const.

(10)

to be known in advance. In particular  DAEP and ADMM require a known Σ; DMSP proposes
an empirical scheme  where unknown Σ is estimated from the current iterate of x during gradient
descent. It also has to introduce a utility function that leads to Gaussian smoothed log-likelihood 
causing Σ to be overestimated (shown later in experiments). More discussions on these baselines are
provided in Section A of the supplementary. Here we propose a generic algorithm for solving x and
unknown Σ simultaneously using a DAE prior. Our method is probabilistically sound.
We start by computing the MLE of Σ. Since x is a latent variable  it needs to be marginalized out:

Pr(y | Σ) =

Pr(y  x | Σ) dx =

Pr(y | x  Σ) Pr(x) dx 

(11)

(cid:90)

where we used the independence between x and Σ. The integral in (11) is intractable  as the prior
Pr(x) is deﬁned by a neural network (DAE). To proceed  we invoke the EM algorithm [12] to
maximize the expected complete-data log-likelihood Q(Σ  Σ(τ )):
x∼Pr(x|y Σ(τ )) log Pr(y  x | Σ)
x∼Pr(x|y Σ(τ )) log Pr(y | x  Σ) + log Pr(x) 

Q(Σ  Σ(τ )) = E
= E

(12)

since the prior Pr(x) does not contain Σ  the M-step is not affected by the intractability of Pr(x).
However  the E-step still needs to deal with Pr(x) as it enters the posterior distribution via

Pr(x | y  Σ(τ )) = Z

−1 Pr(y | x  Σ(τ )) Pr(x) 

(13)

where Z is the partition function. A key component of our method is that the posterior (13) can be
efﬁciently sampled if the prior Pr(x) is deﬁned by a DAE  as we will show in Section 3.1. Therefore 
the Monte Carlo EM algorithm can be used to compute the MLE of Σ. The E-step generates n
samples {x(i)}n
i=1 from the posterior distribution (13)  and the M-step evaluates the new Σ(τ +1) by

Σ(τ +1) = argmaxΣ

log Pr(y | x(i)  Σ) =

1
n

(y − Hx(i))(y − Hx(i))(cid:62)

.

(14)

n(cid:88)

i=1

In many situations  Σ will be constrained to be either diagonal or isotropic. In either case  the solution
of (14) should be determined within the constraint. It is also straightforward to extend our analysis to
the multiple-y case  where all y share the same noise level Σ. We provide discussions on these cases
in Section B of the supplementary. The E-step and M-step are repeated until convergence.

3

(cid:90)

n(cid:88)

i=1

3.1 Sampling from the posterior distribution

The posterior distribution (13) can be sampled using the Metropolis-Hastings (MH) algorithm. As we
shall see  the unknown partition function Z cancels out  and the theorem (4) can convert the DAE-
based prior Pr(x) into tractable terms in this setting. MH requires a proposal distribution q(· | x(i)).
For simplicity  we ﬁrst consider a Gaussian proposal N (x(i)  σ2
propI)  where I is the identity matrix
and σprop is a hyper-parameter. A sample x∗ is drawn from the proposal x∗ ∼ q(· | x(i))  and is
accepted as x(i+1) = x∗ with probability min(1  α)  where

Pr(x∗ | y  Σ(τ ))q(x(i) | x∗)
Pr(x(i) | y  Σ(τ ))q(x∗ | x(i))

 

α =

or otherwise rejected as x(i+1) = x(i). We can rewrite (15) as

log α = log Pr(x∗ | y  Σ(τ )) − log Pr(x(i) | y  Σ(τ ))

(cid:16)

= log Pr(y | x∗
x(i) + x∗

(cid:17)(cid:62)

  Σ(τ )) − log Pr(y | x(i)  Σ(τ )) + log Pr(x∗) − log Pr(x(i))
− y
H(x(i) − x∗) + log Pr(x∗) − log Pr(x(i)) 

Σ(τ )−1

(15)

(16)
(17)

2

=

H

(18)
where we used the Gaussian symmetry q(· | x∗) = q(x∗ | ·) in the ﬁrst step  the Bayes rule (13) in
the second step  and the likelihood (10) in the last step. If x∗ is close to x(i) (e.g. σprop is sufﬁciently
small)  we can use theorem (4) to approximate the log prior difference term in (18):

log Pr(x∗) − log Pr(x(i)) ≈ ∇x log Pr(x)(cid:12)(cid:12)x(i) · (x∗ − x(i))

(19)
(20)
where the ﬁrst step is a linear approximation  r(·) is the reconstruction function of a DAE trained
with noise η ∼ N (0  σ2

trI). We see that α can be efﬁciently computed using a trained DAE.

tr (r(x(i)) − x(i))(cid:62)(x∗ − x(i)) 
−2

≈ σ

3.2 Efﬁcient proposal distribution

In MH  using a ﬁxed proposal distribution can lead to slow mixing of the Markov chain. To make
sampling more efﬁcient  the Metropolis-adjusted Langevin algorithm (MALA) [14] uses the gradient
of log posterior to guide the sampler to high density regions  by adopting a special proposal qMALA:

qMALA(x | x(i)) = N (x(i) + 1

(21)
Section C of the supplementary provides some intuitions behind MALA. Interestingly  the gradient
of log posterior can also be approximated using a DAE:

propI).

2 σ2

prop∇x log Pr(x | y  Σ(τ ))(cid:12)(cid:12)x(i)   σ2

∇x log Pr(x | y  Σ(τ )) = ∇x log Pr(y | x  Σ(τ )) + ∇x log Pr(x)
tr (r(x) − x).
−2

(y − Hx) + σ

(cid:62)Σ(τ )−1

(23)
With the asymmetric proposal qMALA  the ratio of proposals q when computing α is no longer 1. The
quantity log qMALA(x(i) | x∗) − log qMALA(x∗ | x(i))  which can be readily computed from (21) and
(23)  needs to be added to (18) in order to evaluate the acceptance ratio α.

≈ H

(22)

3.3

Implementation

The previous subsections discussed how to obtain the MLE of Σ. To obtain the estimated signal
ˆx  notice that the samples drawn during the last E-step come from the posterior distribution Pr(x |
y  Σ(τ )). In principle  the Bayes estimator of common loss functions can be constructed from the
posterior samples according to Bayesian decision theory (e.g. posterior mean for MSE  posterior
median for L1 loss)  our method is not restricted to any particular loss function. A simple choice is
to use the posterior mean  which provides an MMSE estimator. The primary reason for doing so is
computational: later in Table 2  we compare the posterior mean and median. Their performances are
close  but the mean is easier to compute. Another reason is that many applications care about MSE
(e.g. PSNR for images)  hence MMSE estimator is arguably more suitable. We abbreviate this method
as AGEM. Another method is to run ADMM with the estimated Σ to obtain an MAP estimator 

4

Algorithm 1 Estimate latent signal x and noise level Σ with the proposed methods AGEM and
AGEM-ADMM. τ is the EM iteration number  initialized as 0. Σ(1) is initialized as σ2
1: Train a DAE with quadratic loss and noise η ∼ N (0  σ2
2: repeat τ ← τ + 1
3:
4:
5:
6: until τ = nEM
7: [AGEM] Compute ˆx ← average of {x(i)
8: [AGEM-ADMM] Use ADMM and noise level Σ(nEM) to compute ˆx; return ( ˆx  Σ(nEM))

Initialization: If τ = 1  x(1)
E-step: Draw nMH samples {x(i)
M-step: Use {x(i)

i=1 with MALA  discard the ﬁrst 1/5 samples as burn-in

τ ← 0  otherwise x(1)

τ ← x(nMH)
τ−1

τ }nMH

i=nMH/5; return ( ˆx  Σ(nEM))

trI)

trI.

τ }nMH

τ }nMH

i=nMH/5 to compute Σ(τ +1)

which we abbreviate as AGEM-ADMM. Since ADMM does not depend on the approximation (4)
and is based on MAP rather than MMSE  it serves as an alternative option that may perform better
than AGEM. Our proposed methods are summarized in Algorithm 1. The pseudocode reﬂects some
implementation details  which we discuss below.
Number of iterations: We use nEM to denote the total number of EM iterations  and nMH to denote
the number of samples drawn in every E-step. We empirically ﬁnd that setting nEM to around 20 is
sufﬁcient for convergence  meanwhile nMH should be large enough to achieve good mixing.
Initialization: MALA requires Σ to be initialized. We empirically ﬁnd that  as long as the initializa-
tion is not too far from truth  it has little impact on ﬁnal results. In our implementation we initialize
Σ as the training noise σ2
trI. As for the initial sample x(1)  for the ﬁrst E-step we initialize it as zero;
starting from the second E-step  the last sample from the previous E-step is used to initialize x(1).
This allows sampling to start from a high density region  rather than start from scratch.
Burn-in: As any MH sampler  MALA needs to run many iterations until it converges to the stationary
distribution. These initial samples are discarded  known as “burn-in”. In our implementation  we
discard the ﬁrst 1/5 samples. These discarded samples are not used in the M-step or for computing ˆx.
The time complexity of AGEM is linear to the number of EM iterations nEM  the number of drawn
samples per iteration nMH  and the dimension of x. The space complexity of AGEM is linear to the
dimension of x. Note that it is not necessary to store all nMH samples to compute Σ(τ +1) (line 5) or
ˆx (line 7)  as both can be computed by accumulating a partial sum  and discarding the used samples.

4 Related work

Noise level estimation is a crucial step for many image processing tasks  as many existing algorithms
[7  11  29] require known noise level. Traditional noise estimation methods rely on handcrafted
features or priors [17  22  26]. Recently  deep neural networks are used to solve a wide range of inverse
problems in imaging [24]. Zhang et al. proposed CNNs for denoising [45] and super-resolution [46]
that can deal with arbitrary known noise levels. In [43] they proposed denoising CNN to estimate
noise levels  but their method is only applicable to an identity transformation H = I. Bigdeli et
al. [4] proposed a deep autoencoder prior for multiple image restoration tasks with unknown noise 
based on a particular utility function. Our method extend the above idea to general linear inverse
problems  and we adopt the maximum likelihood principle  not limited to any subjective choices.
To simultaneously estimate the noise level Σ and recover the latent variable x  jointly maximizing
the likelihood with respect to (x  Σ) will lead to overﬁtting [18]. Jin et al. [18] performed Bayes
risk minimization based on a smooth utility function to prevent overﬁtting. A more general and
objective approach is to instead marginalize out the latent variable x  and perform MLE of the model
parameter Σ using the EM algorithm  as in [34  37]. While previous work [21  30] used tractable
priors  our method performs sampling and inference with an intractable data-driven prior  combining
the ﬂexibility and representation power of deep learning with Bayesian statistics.
Our method adopts a similar philosophy as the plug-and-play ADMM literature [35]. As pointed out
by [9]  the ADMM method divides an MAP estimation problem into an L2 regularized inversion step
and a denoising step  where the prior can be implicitly deﬁned by an off-the-shelf denoiser [7  11].

5

This allows us to use pre-trained deep architectures [6  13  28  44] to overcome the limitations of
traditional priors. In a similar vein  Shah and Hegde [31] proposed to use an implicit adversarial
prior. A disadvantage of using implicitly deﬁned priors is that we often lose their probabilistic
interpretations  making it hard to perform model inference and requires careful parameter tuning [38].
Our framework solves this problem by using a DAE prior  which provides good analytic property.
Our method is built on the key observation by [1] that the reconstruction error of a DAE captures
the score of input density. This property allows DAE to be used as image priors [32  39  48] to
capture natural image statistics. Most relevant to us are [3  4]  where the reconstruction error is
used in gradient-based optimization for image restoration. Among these  we are the ﬁrst to be able
to provide an MMSE estimator. Alain and Bengio [1] showed how to use MH to sample from the
prior distribution deﬁned by a DAE. Nguyen et al. [25] improved sampling in high dimensions with
MALA for diverse image generation. We borrow the above ideas and show that DAE-based posterior
sampling can be used in the Monte Carlo E-step to estimate model parameters.

5 Experimental results

We compare our approach with state-of-the-art DAE-based methods  including DMSP  DAEP  and
ADMM  on various noise-blind tasks: signal denoising  image deblurring and image devignetting. We
also compare to some non-DAE-based methods on speciﬁc tasks  but we do not strive for ubiquitous
superior performance over task-speciﬁc methods  as the main advantage of DAE-based methods lies
in their plug-and-play nature and task-agnostic generality. For each task  we train a single DAE and
use it to evaluate all methods  so that they compete fairly. Since DAEP and ADMM require a noise
level  we estimate it with DMSP  denoted by “DAEP+NE” and “ADMM+NE” (Noise Estimation).
All DAEs are trained by SGD with momentum 0.9 under the L2 reconstruction loss  early stopping is
based on validation loss. As all baseline methods assume isotropic noise  we follow this restriction in
this section for comparison purpose  and demonstrate general noise in Section E of the supplementary.
For testing  nEM and nMH are set to sufﬁciently large values for stable convergence. We note that
since the tasks are noise-blind  the hyper-parameters should not be tuned for each tested noise level.
Instead  they are chosen to achieve the best ˆx reconstruction using validation sets when Σ = σ2
trI  and
remain ﬁxed for the rest experiments. Chosen values and more details are reported in each subsection.
We implement and train DAEs using PyTorch [33]  all experiments were run on a Ubuntu server with
two Titan X GPUs. Our code and all simulated datasets will be made available online.

Signal denoising. Consider 50-dimensional signals lying on a latent 2D manifold  and corrupted
by isotropic Gaussian noise Σ = σ2
nI. We generate a 6000-sample dataset according to the following
equation  where α  β ∼ Uniform(2  5)  e = exp(1)  and xk is the k-th coordinate of the 50-
dimensional signal (Section D of the supplementary provides visualization of this manifold):

xk = 0.01(α + β)2 sin[α sin(ke) + β sin(ke + 1) + 0.5(α + β)]  k = 1  ...  50.

(24)
This 2D manifold is highly nonlinear. Among 6000 samples  1000 samples are selected as the
validation set and another 1000 samples as the test set. The rest are used for DAE training. The DAE
is a multilayer perceptron with ReLU activations and 3 hidden layers  each containing 2000 neurons.
Following [3]  our DAE does not have a bottleneck as an explicit low-dimensional latent space is not
required for our purpose. It is trained for 500 epochs with noise σtr = 0.01 and learning rate 0.1.
For testing  we consider four different noise levels σn ∈ {0.01  0.02  0.03  0.04}. We compute
the root-mean-square error (RMSE) between the recovered signal ˆx and the noiseless signal x by

(cid:112)(cid:107) ˆx − x(cid:107)2/50  and report its mean and standard deviation (stdev.) on the test set. We set nEM = 10 

nMH = 1000  σprop is chosen by a grid search on [0.001  0.5]. We ﬁnd σprop = 0.01 achieves the best
average RMSE on the validation set. Table 1 shows the results (values are scaled by 100). Our best
method outperforms all baseline methods signiﬁcantly statistically (p < 0.05)  and our estimated σn
(in square brackets) are closer to the true values comparing to DMSP. AGEM-ADMM performs well
under small noises. Indeed  since ADMM uses the trained DAE for denoising  it works well if σn is
close to the training noise σtr. However  as DMSP overestimates σn especially when σn is small  it
misses the “operating region” of ADMM  leading to ADMM+NE’s inferior performance.

Ablation study. We study the behavior of AGEM in detail under the settings of the previous
experiment. We explore different σprop  initial noise levels Σ(1)  strategies to construct the recovered

6

Table 1: Signal denoising  average RMSE of the test set. Standard deviations are in parentheses 
estimated noise levels are in square brackets. Best performances are in bold. (All values are in 10−2).

σn:
Method
DAEP+NE [3]
ADMM+NE [35]
DMSP [4]

AGEM
AGEM-ADMM

1.00

2.00

3.00

4.00

mean
0.73
0.37
0.50
[1.62]
0.51
[1.19]
0.33

std.
(0.10)
(0.28)
(0.22)
(0.14)
(0.15)
(0.13)
(0.23)

mean
0.98
0.60
0.74
[2.19]
0.70
[1.93]
0.57

std.
(0.13)
(0.36)
(0.29)
(0.22)
(0.25)
(0.26)
(0.34)

mean
1.16
0.93
0.99
[3.07]
0.86
[2.96]
0.91

std.
(0.20)
(0.55)
(0.45)
(0.35)
(0.39)
(0.38)
(0.53)

mean
1.31
1.59
1.36
[4.11]
1.16
[4.03]
1.43

std.
(0.27)
(3.49)
(0.95)
(0.75)
(0.64)
(0.52)
(2.05)

Table 2: Ablation study  average RMSE of the test set. Noise level is Σ = σ2
3.00 × 10−2. Estimated noise levels are in square brackets. (All values are in 10−2).

nI  where σn =

mean

std.

mean

std.

mean

std.

mean

std.

mean

std.

σprop:

Σ(1):

misc.

0.01

34.1
[34.2]

(12.7)
(12.7)

0.10

1.20
[2.87]

(0.40)
(0.40)

1.00

0.86
[2.96]

(0.39)
(0.38)

10.0

does not
converge

100

does not
converge

0.5I

does not
converge

1.0I

0.86
[2.96]

(0.39)
(0.38)

2.0I

0.87
[2.96]

(0.40)
(0.37)

4.0I

0.86
[2.96]

(0.39)
(0.37)

mean

0.86
[2.96]

(0.39)
(0.38)

median

0.87
[2.96]

(0.39)
(0.37)

last

1.57
[2.96]

(0.38)
(0.38)

ﬁrst

1.58
[2.96]

(0.40)
(0.38)

8.0I

does not
converge
Gaussian

7.61
[9.51]

(4.43)
(3.82)

ˆx  and compare MALA with the symmetric Gaussian proposal. We set the test noise level σn = 0.03 
all hyper-parameters remain unchanged except for the hyper-parameter being studied.
Table 2 summarizes the results (values are scaled by 100 for better display). The ﬁrst row shows
results using different σprop. If σprop is too small  the results are incorrect  as it takes impractically
many samples to achieve good mixing. If σprop is too large  new samples deviate from high density
regions  and the algorithm fails to converge as no new samples are accepted. Therefore  besides
using a validation set to choose a ﬁxed σprop  another possible strategy is to dynamically increase
σprop while keeping the algorithm convergent. We leave this for future investigation. The second
row shows results using different noise level initializations. We see that as long as the initialization
is within a good range  the results are stable. In practice one can try a wide range of initializations
to seek convergence. The third row compares different strategies for constructing the recovered ˆx.
“Mean”/“median” uses the coordinate-wise mean/median of the samples  while “last”/“ﬁrst” uses
the last/ﬁrst sample  all from the last iteration. “Mean” and “median” achieve similar performances 
while “last” and “ﬁrst” have worse RMSE  as a single sample fails to represent the central tendency
of the entire posterior distribution. Finally  “Gaussian” stands for using symmetric Gaussian proposal
during the E-step. Comparing to “mean” which uses MALA  we see the Gaussian proposal gives
incorrect results  as it fails to exploit gradient information and is stuck at local maxima.

Image deblurring. We perform image deblurring with the STL-10 unlabeled dataset [10]  which
contains 105 colored 96×96 images. They are converted to grayscale and normalized to [0  1]. We
select the last 400 images  the ﬁrst/second half of which is used as the validation/test set. The rest are
used for DAE training. The DAE uses the full convolutional  residual architecture from [43]  where
the input is added to the ﬁnal layer’s output. It is trained for 250 epochs with noise σtr = 0.02 and
learning rate 0.01. We empirically ﬁnd that DAEs trained with smaller noises do not perform as well.
For testing  images are blurred using a 5 × 5 Gaussian ﬁlter with σ = 0.6. The noise is spatially
nI  where σn ∈ {0.01  0.02  0.03  0.04}. We set nEM = 10  nMH = 300  σprop is set
uniform Σ = σ2
to 0.02 using the same selection method as signal denoising  except RMSE is replaced by PSNR. The

7

Table 3: Average PSNR for image deblurring. Estimated noise levels are in square brackets.

σn:
Method
DAEP+NE [3]
ADMM+NE [35]
DMSP [4]

AGEM
AGEM-ADMM
Hyper-Laplacian [21]
CSF [30]

0.01

0.02

0.03

0.04

mean
33.13
32.43
33.60
[0.017]
34.79
[0.014]
33.75
33.28
32.97

std.
(1.39)
(3.08)
(2.46)
(1e-3)
(2.00)
(1e-3)
(2.77)
(0.65)
(0.68)

mean
27.77
29.48
30.89
[0.023]
31.42
[0.021]
30.00
30.26
29.94

std.
(0.89)
(3.16)
(2.14)
(2e-3)
(1.81)
(2e-3)
(3.20)
(0.40)
(0.41)

mean
25.48
27.87
28.93
[0.031]
29.47
[0.030]
28.00
29.28
29.02

std.
(0.70)
(2.97)
(2.18)
(3e-3)
(1.92)
(3e-3)
(2.88)
(0.35)
(0.37)

mean
24.30
25.78
27.40
[0.041]
28.00
[0.040]
26.05
28.82
28.61

std.
(0.61)
(3.16)
(2.33)
(4e-3)
(2.10)
(3e-3)
(3.51)
(0.35)
(0.36)

Figure 1: Visual comparison for image deblurring with σn = 0.01. Numbers above the images are:
PSNR of the image / average PSNR of the test set (in dB). Zoom in for more details.

mean/stdev. of PSNR and estimated σn on the test set are reported in Table 3. AGEM consistently
outperforms all baseline methods signiﬁcantly statistically (p < 0.01)  and its estimated σn are closer
to true values than DMSP. We also compare with some analytic priors [21  30]. Although these priors
are speciﬁcally designed for image deconvolution  our generic approach outperforms them except
for σn = 0.04  indicating that our trained DAE learns the distribution of natural images well  and
DAE-based methods are indeed relevant in practice. Some visual examples are provided in Fig. 1. A
convergence visualization is provided in Fig. 2  which shows the stability of our approach.

Image devignetting. Vignetting is a prevalent artifact in photography that brightness attenuates
away from the center [47]. We perform image devignetting with the CelebA dataset [42]  which
contains 0.2 million 218×178 colored face images  and a predeﬁned train/val/test split. We normalize
images to [0  1] and train a DAE with the entire training set. We use the same DAE architecture as
image deblurring. It is trained for 125 epochs with noise σtr = 0.02 and learning rate 0.1.
We select the ﬁrst 100 images from the predeﬁned val/test set as our validation/test set. The
transformation is based on the Kang-Weiss [19] vignetting model

(25)

1 − αr

p(r) =

[1 + (r/f )2]2 .

The intensity of a pixel  whose distance to the center is r  is multiplied by p(r). We set α = 0.001  f =
160 to achieve a realistic vignetting effect. H is then a diagonal matrix if images are reshaped into

Figure 2: Convergence visualization for image deblurring. Left: average estimated noise level; right:
mean PSNR. The legend shows the true noise level σn. Stable convergence is quickly reached. Each
EM epoch draws 300 MCMC samples.

8

GroundTruthBlurredDAEP+NE32.49/33.13ADMM+NE32.77/32.43DMSP32.57/33.60AGEM34.09/34.7912345678910EMepoch0.0150.0200.0250.0300.0350.040Est.Noise12345678910EMepoch2628303234PSNR0.0100.0200.0300.040Table 4: Average PSNR for image devignetting. Estimated noise levels are in square brackets.

σn:
Method
DAEP+NE [3]
ADMM+NE [35]
DMSP [4]

AGEM
AGEM-ADMM
LIE [23]
SIVC [47]

0.015

0.02

0.025

0.03

mean
33.76
34.10
35.78
[0.022]
36.34
[0.017]
36.16
29.61
29.55

std.
(0.71)
(1.62)
(0.99)
(1e-3)
(0.65)
(1e-3)
(1.54)
(1.72)
(0.87)

mean
31.19
32.95
34.43
[0.024]
34.76
[0.020]
34.56
29.43
29.44

std.
(0.69)
(1.56)
(0.94)
(1e-3)
(0.68)
(1e-3)
(1.53)
(1.43)
(0.78)

mean
29.16
31.60
33.26
[0.027]
33.58
[0.024]
32.87
29.23
29.33

std.
(0.64)
(1.56)
(0.94)
(1e-3)
(0.77)
(1e-3)
(1.54)
(1.16)
(0.71)

mean
27.51
29.96
32.18
[0.032]
32.55
[0.029]
31.07
29.05
29.22

std.
(0.58)
(1.68)
(1.03)
(1e-3)
(0.88)
(1e-3)
(1.60)
(0.95)
(0.64)

Figure 3: Visual comparison for image devignetting with σn = 0.015. Numbers above the images
are: PSNR of the image / average PSNR of the test set (in dB). Zoom in for more details.

nI  where σn ∈ {0.015  0.02  0.025  0.03}.
column vectors. We consider spatially uniform Σ = σ2
We set nEM = 10  nMH = 200  σprop is set to 0.02 using the same selection method as image
deblurring. The mean/stdev. of PSNR and estimated σn on the test set are reported in Table 4. AGEM
consistently outperforms all baseline methods signiﬁcantly statistically (p < 0.01)  and its estimated
σn are closer to true values than DMSP. We also compare with existing methods [23  47] that do not
rely on the known model p(r). They are outperformed by model-based methods  as p(r) contains
essential information for reconstruction performance. Some visual examples are provided in Fig. 3.

6 Concluding remarks

In this paper  we propose a probabilistic framework that uses DAE prior to simultaneously solve
linear inverse problems and estimate noise levels  based on the Monte Carlo EM algorithm. We
show that during the Monte Carlo E-step  efﬁcient posterior sampling can be performed  as the
reconstruction error of DAE captures the gradient of log prior. Our framework allows us to use
deep priors trained by unsupervised learning for a wide range of tasks  including signal denoising 
image deblurring and image devignetting. Experimental results show that our method outperforms
the previous state-of-the-art DAE-based methods. However  this study is not without limitations.
Since our method is based on sampling  it usually takes several times longer than non-sampling-based
methods to achieve stable convergence. A possible direction for future research is to extend our
framework to nonlinear inverse problems. We are also considering using other forms of deep priors.

Acknowledgments

This work is supported by the Natural Science Foundation of China (Project Number 61521002). We
would like to thank Xinyue Liang for discussions on MCMC methods. We also thank the reviewers
and the area chair for their valuable comments.

9

GroundTruthVignettedDAEP+NE33.88/33.76ADMM+NE32.32/34.10DMSP35.05/35.78AGEM36.12/36.34References
[1] G. Alain and Y. Bengio. What regularized auto-encoders learn from the data-generating distribution. JMLR 

15(1):3563–3593  2014.

[2] M. R. Banham and A. K. Katsaggelos. Digital image restoration. IEEE Signal Processing Magazine 

14(2):24–41  1997.

[3] S. A. Bigdeli and M. Zwicker. Image restoration using autoencoding priors. International Conference on

Computer Vision Theory and Applications  5:33–44  2018.

[4] S. A. Bigdeli  M. Zwicker  P. Favaro  and M. Jin. Deep mean-shift priors for image restoration. In Advances

in Neural Information Processing Systems  pages 763–772  2017.

[5] S. Boyd  N. Parikh  E. Chu  B. Peleato  J. Eckstein  et al. Distributed optimization and statistical learning via
the alternating direction method of multipliers. Foundations and Trends in Machine learning  3(1):1–122 
2011.

[6] A. Brifman  Y. Romano  and M. Elad. Turning a denoiser into a super-resolver using plug and play priors.

In ICIP  pages 1404–1408  2016.

[7] A. Buades  B. Coll  and J.-M. Morel. A non-local algorithm for image denoising. In CVPR  pages 60–65 

2005.

[8] H. C. Burger  C. J. Schuler  and S. Harmeling. Image denoising: Can plain neural networks compete with

BM3D? In CVPR  pages 2392–2399  2012.

[9] S. H. Chan  X. Wang  and O. A. Elgendy. Plug-and-play ADMM for image restoration: Fixed-point

convergence and applications. IEEE Transactions on Computational Imaging  3(1):84–98  2017.

[10] A. Coates  A. Ng  and H. Lee. An analysis of single-layer networks in unsupervised feature learning. In

AISTATS  pages 215–223  2011.

[11] K. Dabov  A. Foi  V. Katkovnik  and K. Egiazarian. Image denoising by sparse 3-D transform-domain

collaborative ﬁltering. IEEE TIP  16(8):2080–2095  2007.

[12] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via the EM

algorithm. Journal of the Royal Statistical Society: Series B (Methodological)  39(1):1–22  1977.

[13] W. Dong  P. Wang  W. Yin  and G. Shi. Denoising prior driven deep neural network for image restoration.

IEEE TPAMI  2018.

[14] M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods.

Journal of the Royal Statistical Society: Series B (Statistical Methodology)  73(2):123–214  2011.

[15] R. M. Gray et al. Toeplitz and circulant matrices: A review. Foundations and Trends in Communications

and Information Theory  2(3):155–239  2006.

[16] W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika 

pages 97–109  1970.

[17] J. Immerkaer. Fast noise variance estimation. Computer Vision and Image Understanding  64(2):300–302 

1996.

[18] M. Jin  S. Roth  and P. Favaro. Noise-blind image deblurring. In CVPR  pages 3510–3518  2017.

[19] S. B. Kang and R. Weiss. Can we calibrate a camera using an image of a ﬂat  textureless lambertian

surface? In ECCV  pages 640–653  2000.

[20] T. Knopp  T. F. Sattel  S. Biederer  J. Rahmer  J. Weizenecker  B. Gleich  J. Borgert  and T. M. Buzug.
IEEE Transactions on Medical Imaging 

Model-based reconstruction for magnetic particle imaging.
29(1):12–18  2009.

[21] D. Krishnan and R. Fergus. Fast image deconvolution using hyper-Laplacian priors. In Advances in Neural

Information Processing Systems  pages 1033–1041  2009.

[22] W. Liu and W. Lin. Additive white Gaussian noise level estimation in SVD domain for images. IEEE TIP 

22(3):872–883  2013.

10

[23] L. Lopez-Fuentes  G. Oliver  and S. Massanet. Revisiting image vignetting correction by constrained
minimization of log-intensity entropy. In International Work-Conference on Artiﬁcial Neural Networks 
pages 450–463. Springer  2015.

[24] M. T. McCann  K. H. Jin  and M. Unser. Convolutional neural networks for inverse problems in imaging:

A review. IEEE Signal Processing Magazine  34(6):85–95  2017.

[25] A. Nguyen  J. Clune  Y. Bengio  A. Dosovitskiy  and J. Yosinski. Plug & play generative networks:

Conditional iterative generation of images in latent space. In CVPR  pages 4467–4477  2017.

[26] S. Pyatykh  J. Hesser  and L. Zheng. Image noise level estimation by principal component analysis. IEEE

TIP  22(2):687–699  2013.

[27] A. M. Rao and D. L. Jones. A denoising approach to multisensor signal estimation. IEEE Transactions on

Signal Processing  48(5):1225–1234  2000.

[28] J. Rick Chang  C.-L. Li  B. Poczos  B. Vijaya Kumar  and A. C. Sankaranarayanan. One network to solve
them all–solving linear inverse problems using deep projection models. In ICCV  pages 5888–5897  2017.

[29] P. Rosin. Thresholding for change detection. In ICCV  pages 274–279  1998.

[30] U. Schmidt and S. Roth. Shrinkage ﬁelds for effective image restoration. In CVPR  pages 2774–2781 

2014.

[31] V. Shah and C. Hegde. Solving linear inverse problems using GAN priors: An algorithm with provable

guarantees. In ICASSP  pages 4609–4613  2018.

[32] C. K. Sønderby  J. Caballero  L. Theis  W. Shi  and F. Huszár. Amortised MAP inference for image

super-resolution. In ICLR  2017.

[33] B. Steiner et al. PyTorch: An imperative style  high-performance deep learning library. In Advances in

Neural Information Processing Systems  2019.

[34] L. Torresani  A. Hertzmann  and C. Bregler. Nonrigid structure-from-motion: Estimating shape and motion

with hierarchical priors. IEEE TPAMI  30(5):878–892  2008.

[35] S. V. Venkatakrishnan  C. A. Bouman  and B. Wohlberg. Plug-and-play priors for model based reconstruc-
tion. In 2013 IEEE Global Conference on Signal and Information Processing (GlobalSIP)  pages 945–948 
2013.

[36] P. Vincent  H. Larochelle  Y. Bengio  and P.-A. Manzagol. Extracting and composing robust features with

denoising autoencoders. In ICML  pages 1096–1103  2008.

[37] J. M. Wang  D. J. Fleet  and A. Hertzmann. Gaussian process dynamical models for human motion. IEEE

TPAMI  30(2):283–298  2008.

[38] X. Wang and S. H. Chan. Parameter-free plug-and-play ADMM for image restoration. In ICASSP  pages

1323–1327  2017.

[39] Y. Wang  Q. Liu  H. Zhou  and Y. Wang. Learning multi-denoising autoencoding priors for image

super-resolution. Journal of Visual Communication and Image Representation  57:152–162  2018.

[40] G. C. Wei and M. A. Tanner. A Monte Carlo implementation of the EM algorithm and the poor man’s data

augmentation algorithms. Journal of the American Statistical Association  85(411):699–704  1990.

[41] J. Xie  L. Xu  and E. Chen. Image denoising and inpainting with deep neural networks. In Advances in

Neural Information Processing Systems  pages 341–349  2012.

[42] S. Yang  P. Luo  C.-C. Loy  and X. Tang. From facial parts responses to face detection: A deep learning

approach. In ICCV  pages 3676–3684  2015.

[43] K. Zhang  W. Zuo  Y. Chen  D. Meng  and L. Zhang. Beyond a Gaussian denoiser: Residual learning of

deep CNN for image denoising. IEEE TIP  26(7):3142–3155  2017.

[44] K. Zhang  W. Zuo  S. Gu  and L. Zhang. Learning deep CNN denoiser prior for image restoration. In

CVPR  pages 3929–3938  2017.

[45] K. Zhang  W. Zuo  and L. Zhang. FFDNet: Toward a fast and ﬂexible solution for CNN-based image

denoising. IEEE TIP  27(9):4608–4622  2018.

11

[46] K. Zhang  W. Zuo  and L. Zhang. Learning a single convolutional super-resolution network for multiple

degradations. In CVPR  pages 3262–3271  2018.

[47] Y. Zheng  S. Lin  C. Kambhamettu  J. Yu  and S. B. Kang. Single-image vignetting correction. IEEE

TPAMI  31(12):2243–2256  2008.

[48] E. Zhou  Z. Cao  and J. Sun. Gridface: Face rectiﬁcation via learning local homography transformations.

In ECCV  pages 3–19  2018.

12

,Yao-Liang Yu
Yijun Li
Chen Fang
Jimei Yang
Zhaowen Wang
Xin Lu
Ming-Hsuan Yang
Minhyuk Sung
Hao Su
Ronald Yu
Leonidas Guibas
Bichuan Guo
Yuxing Han
Jiangtao Wen