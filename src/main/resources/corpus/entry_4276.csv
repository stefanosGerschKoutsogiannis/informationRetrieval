2011,Phase transition in the family of p-resistances,We study the family of p-resistances on graphs for p ≥ 1. This family generalizes the standard resistance distance. We prove that for any fixed graph  for p=1  the p-resistance coincides with the shortest path distance  for p=2 it coincides with the standard resistance distance  and for p → ∞ it converges to the inverse of the minimal s-t-cut in the graph. Secondly  we consider the special case of random geometric graphs (such as k-nearest neighbor graphs) when the number n of vertices in the graph tends to infinity. We prove that an interesting phase-transition takes place. There exist two critical thresholds p^* and p^** such that if p < p^*  then the p-resistance depends on meaningful global properties of the graph  whereas if p > p^**  it only depends on trivial local quantities and does not convey any useful information. We can explicitly compute the critical values: p^* = 1 + 1/(d-1) and p^** = 1 + 1/(d-2) where d is the dimension of the underlying space (we believe that the fact that there is a small gap between p^* and p^** is an artifact of our proofs. We also relate our findings to Laplacian regularization and suggest to use q-Laplacians as regularizers  where q satisfies 1/p^* + 1/q = 1.,Phase transition in the family of p-resistances

Morteza Alamgir

T¨ubingen  Germany

Ulrike von Luxburg

T¨ubingen  Germany

Max Planck Institute for Intelligent Systems

Max Planck Institute for Intelligent Systems

morteza@tuebingen.mpg.de

ulrike.luxburg@tuebingen.mpg.de

Abstract

We study the family of p-resistances on graphs for p  1. This family generalizes
the standard resistance distance. We prove that for any ﬁxed graph  for p = 1
the p-resistance coincides with the shortest path distance  for p = 2 it coincides
with the standard resistance distance  and for p ! 1 it converges to the inverse
of the minimal s-t-cut in the graph. Secondly  we consider the special case of
random geometric graphs (such as k-nearest neighbor graphs) when the number
n of vertices in the graph tends to inﬁnity. We prove that an interesting phase
transition takes place. There exist two critical thresholds p⇤ and p⇤⇤ such that
if p < p⇤  then the p-resistance depends on meaningful global properties of the
graph  whereas if p > p⇤⇤  it only depends on trivial local quantities and does
not convey any useful information. We can explicitly compute the critical values:
p⇤ = 1 + 1/(d  1) and p⇤⇤ = 1 + 1/(d  2) where d is the dimension of
the underlying space (we believe that the fact that there is a small gap between
p⇤ and p⇤⇤ is an artifact of our proofs). We also relate our ﬁndings to Laplacian
regularization and suggest to use q-Laplacians as regularizers  where q satisﬁes
1/p⇤ + 1/q = 1.

1

Introduction

The graph Laplacian is a popular tool for unsupervised and semi-supervised learning problems on
graphs. It is used in the context of spectral clustering  as a regularizer for semi-supervised learning 
or to compute the resistance distance on graphs. However  it has been observed that under certain
circumstances  standard Laplacian-based methods show undesired artifacts. In the semi-supervised
learning setting Nadler et al. (2009) showed that as the number of unlabeled points increases  the so-
lution obtained by Laplacian regularization degenerates to a non-informative function. von Luxburg
et al. (2010) proved that as the number of points increases  the resistance distance converges to a
meaningless limit function. Independently of these observations  a number of authors suggested to
generalize Laplacian methods. The observation was that the “standard” Laplacian methods corre-
spond to a vector space setting with L2-norms  and that it might be beneﬁcial to work in a more
general Lp setting for p 6= 2 instead. See B¨uhler and Hein (2009) for an application to clustering
and Herbster and Lever (2009) for an application to label propagation. In this paper we take up
several of these loose ends and connect them.
The main object under study in this paper is the family of p-resistances  which is a generalization
of the standard resistance distance. Our ﬁrst major result proves that the family of p-resistances is
very rich and contains several special cases. The general picture is that the smaller p is  the more the
resistance is concentrated on “short paths”. In particular  the case p = 1 corresponds to the shortest
path distance in the graph  the case p = 2 to the standard resistance distance  and the case p ! 1
to the inverse s-t-mincut.
Second  we study the behavior of p-resistances in the setting of random geometric graphs like lattice
graphs  "-graphs or k-nearest neighbor graphs. We prove that as the sample size n increases  there

1

are two completely different regimes of behavior. Namely  there exist two critical thresholds p⇤ and
p⇤⇤ such that if p < p⇤  the p-resistances convey useful information about the global topology of
the data (such as its cluster properties)  whereas for p > p⇤⇤ the resistance distances approximate a
limit that does not convey any useful information. We can explicitly compute the value of the critical
thresholds p⇤ := 1 + 1/(d  1) and p⇤⇤ := 1 + 1/(d  2). This result even holds independently of
the exact construction of the geometric graph.
Third  as we will see in Section 5  our results also shed light on the Laplacian regularization
and semi-supervised learning setting. As there is a tight relationship between p-resistances and
graph Laplacians  we can reformulate the artifacts described in Nadler et al. (2009) in terms of
p-resistances. Taken together  our results suggest that standard Laplacian regularization should be
replaced by q-Laplacian regularization (where q is such that 1/p⇤ + 1/q = 1).

2

Intuition and main results

Consider an undirected  weighted graph G = (V  E) with n vertices. As is standard in machine
learning  the edge weights are supposed to indicate similarity of the adjacent points (not distances).
Denote the weight of edge e by we  0 and the degree of vertex u by du. The length of a path 
in the weighted graph is deﬁned asPe2 1/we. In the electrical network interpretation  a graph is
considered as a network where each edge e 2 E has resistance re = 1/we. The effective resistance
(or resistance distance) R(s  t) between two vertices s and t in the network is deﬁned as the overall
resistance one obtains when connecting a unit volt battery to s and t. It can be computed in many
ways  but the one most useful for our paper is the following representation in terms of ﬂows (cf.
Section IX.1 of Bollobas  1998):

R(s  t) = minnPe2E rei2

e i = (ie)e2E unit ﬂow from s to to.

(1)
In von Luxburg et al. (2010) it has been proved that in many random graph models  the resistance
distance R(s  t) between two vertices s and t converges to the trivial limit expression 1/ds + 1/dt
as the size of the graph increases. We now want to present some intuition as to how this problem
can be resolved in a natural way. For a subset M ⇢ E of edges we deﬁne the contribution of M
to the resistance R(s  t) as the part of the sum in (1) that runs over the edges in M. Let i⇤ be
a ﬂow minimizing (1). To explain our intuition we separate this ﬂow into two parts: R(s  t) =
R(s  t)local + R(s  t)global. The part R(s  t)local stands for the contribution of i⇤ that stems from
the edges in small neighborhoods around s and t  whereas R(s  t)global is the contribution of the
remaining edges (exact deﬁnition given below). A useful distance function is supposed to encode
the global geometry of the graph  for example its cluster properties. Hence  R(s  t)global should be
the most important part in this decomposition. However  in case of the standard resistance distance
the contribution of the global part becomes negligible as n ! 1 (for many different models of
graph construction). This effect happens because as the graph increases  there are so many different
paths between s and t that once the ﬂow has left the neighborhood of s  electricity can ﬂow “without
considerable resistance”. The “bottleneck” for the ﬂow is the part that comes from the edges in the
local neighborhoods of s and t  because here the ﬂow has to concentrate on relatively few edges. So
the dominating part is R(s  t)local.
In order to deﬁne a useful distance function  we have to ensure that the global part has a signiﬁcant
contribution to the overall resistance. To this end  we have to avoid that the ﬂow is distributed over
“too many paths”. In machine learning terms  we would like to achieve a ﬂow that is “sparser”
in the number of paths it uses. From this point of view  a natural attempt is to replace the 2-norm-
optimization problem (1) by a p-norm optimization problem for some p < 2. Based on this intuition 
our idea is to replace the squares in the ﬂow problem (1) by a general exponent p  1 and deﬁne the
following new distance function on the graph.
Deﬁnition 1 (p-resistance) On any weighted graph G  for any p  1 we deﬁne

Rp(s  t) := minnPe2E re|ie|p i = (ie)e2E unit ﬂow from s to to.

(⇤)
As it turns out  our newly deﬁned distance function Rp is closely related but not completely identical
to the p-resistance RH
p deﬁned by Herbster and Lever (2009). A discussion of this issue can be found
in Section 6.1.

2

30

25

20

15

10

5

0

0

5

10

15

20

25

30

(a) p = 2

30

25

20

15

10

5

0

0

5

10

15

20

25

30

(b) p = 1.33

30

25

20

15

10

5

0

0

5

10

15

20

25

30

(c) p = 1.1

Figure 1: The s-t-ﬂows minimizing (⇤) in a two-dimensional grid for different values of p. The
smaller p  the more the ﬂow concentrates along the shortest path.

In toy simulations we can observe that the desired effect of concentrating the ﬂow on fewer paths
takes place indeed. In Figure 1 we show how the optimal ﬂow between two points s and t gets
propagated through the network. We can see that the smaller p is  the more the ﬂow is concentrated
along the shortest path between s and t. We are now going to formally investigate the inﬂuence of
the parameter p. Our ﬁrst question is how the family Rp(s  t) behaves as a function of p (that is  on
a ﬁxed graph and for ﬁxed s  t). The answer is given in the following theorem.

Theorem 2 (Family of p-resistances) For any weighted graph G the following statements are true:

1. For p = 1  the p-resistance coincides with the shortest path distance on the graph.

2. For p = 2  the p-resistance reduces to the standard resistance distance.
3. For p ! 1  Rp(s  t)p1 converges to 1/m where m is the unweighted s-t-mincut.

This theorem shows that our intuition as outlined above was exactly the right one. The smaller p
is  the more ﬂow is concentrated along straight paths. The extreme case is p = 1  which yields the
shortest path distance. In the other direction  the larger p is  the more widely distributed the ﬂow is.
Moreover  the theorem above suggests that for p close to 1  Rp encodes global information about the
part of the graph that is concentrated around the shortest path. As p increases  global information
is still present  but now describes a larger portion of the graph  say  its cluster structure. This is
the regime that is most interesting for machine learning. The larger p becomes  the less global
information is present in Rp (because ﬂows even use extremely long paths that take long detours) 
and in the extreme case p ! 1 we are left with nothing but the information about the minimal
s-t-cut. In many large graphs  the latter just contains local information about one of the points s or t
(see the discussion at the end of this section). An illustration of the different behaviors can be found
in Figure 2.
The next question  inspired by the results of von Luxburg et al. (2010)  is what happens to Rp(s  t)
if we ﬁx p but consider a family (Gn)n2N of graphs such that the number n of vertices in Gn tends
to 1. Let us consider geometric graphs such as k-nearest neighbor graphs or "-graphs. We now
give exact deﬁnitions of the local and global contributions to the p-resistance. Let r and R be real
numbers that depend on n (they will be speciﬁed in Section 4) and C  R/r a constant. We deﬁne
the local neighborhood N (s) of vertex s as the ball with radius C · r around s. We will see later that
the condition C  R/r ensures that N (s) contains at least all vertices adjacent to s. By abuse of
notation we also write e 2N (s) if both endpoints of edge e are contained in N (s). Let i⇤ be the
optimal ﬂow in Problem (⇤). We deﬁne
Rlocal

p

(s  t) := Rlocal

(s) + Rlocal

p

p

(s  t). Our next result
Rlocal
conveys that the behavior of the family of p-resistances shows an interesting phase transition. The
statements involve a term ⌧n that should be interpreted as the average degree in the graph Gn (exact
deﬁnition see later).

(s  t) := Rp(s  t)  Rlocal

(t)  and Rglobal

p

p

(s) :=Pe2N (s) re|i⇤e|p 

p

3

50

100

150

200

250

300

350

400

450

500

50

100

150

200

250

300

350

400

450

500

(a) p = 1

50

100

150

200

250

300

350

400

450

500

50

100

150

200

250

300

350

400

450

500

(b) p = 1.11

50

100

150

200

250

300

350

400

450

500

50

100

150

200

250

300

350

400

450

500

(c) p = 1.5

50

100

150

200

250

300

350

400

450

500

50

100

150

200

250

300

350

400

450

500

(d) p = 2

Figure 2: Heat plots of the Rp distance matrices for a mixture of two Gaussians in R10. We can see
that the larger p it  the less pronounced the “global information” about the cluster structure is.

Theorem 3 (Phase transition for p-resistances in large geometric graphs) Consider a family
(Gn)n2N of unweighted geometric graphs on Rd  d > 2 that satisﬁes some general assumptions
(see Section 4 for deﬁnitions and details). Fix two vertices s and t. Deﬁne the two critical values
p⇤ := 1 + 1/(d  1) and p⇤⇤ := 1 + 1/(d  2). Then  as n ! 1  the following statements hold:
(s  t) ! 1  that is the global
1. If p < p⇤ and ⌧n is sub-polynomial in n  then Rglobal
contribution dominates the local one.
  that
2. If p > p⇤⇤ and ⌧n ! 1  then Rlocal
is all global information vanishes.

(s  t) ! 1 and Rp(s  t) ! 1

(s  t)/Rglobal

(s  t)/Rlocal

dp1
s

+ 1

dp1
t

p

p

p

p

This result is interesting. It shows that there exists a non-trivial point of phase transition in the
behavior of p-resistances: if p < p⇤  then p-resistances are informative about the global topology
of the graph  whereas if p > p⇤⇤ the p-resistances converge to trivial distance functions that do not
depend on any global properties of the graph. In fact  we believe that p⇤⇤ should be 1 1/(d 1) as
well  but our current proof leaves the tiny gap between p⇤ = 1 1/(d 1) and p⇤⇤ = 1 1/(d 2).
Theorem 3 is a substantial extension of the work of von Luxburg et al. (2010)  in several respects.
First  and most importantly  it shows the complete picture of the full range of p  1  and not
just the single snapshot at p = 2. We can see that there is a range of values for p for which p-
resistance distances convey very important information about the global topology of the graph  even
in extremely large graphs. Also note how nicely Theorems 2 and 3 ﬁt together. It is well-known
that as n ! 1  the shortest path distance corresponding to p = 1 converges to the (geodesic)
distance of s and t in the underlying space (Tenenbaum et al.  2000)  which of course conveys
global information. von Luxburg et al. (2010) proved that the standard resistance distance (p = 2)
converges to the trivial local limit. Theorem 3 now identiﬁes the point of phase transition p⇤ between
the boundary cases p = 1 and p = 2. Finally  for p ! 1  we know by Theorem 2 that the p-
resistance converges to the inverse of the s-t-min-cut. It is widely believed that the minimal s-t cut
in geometric graphs converges to the minimum of the degrees of s and t as n ! 1 (even though
a formal proof has yet to be presented and we cannot point to any reference). This is in alignment
with the result of Theorem 3 that the p-resistance converges to 1/dp1
. As p ! 1 
only the smaller of the two degrees contributes to the local part  which agrees with the limit for the
s-t-mincut.

+ 1/dp1

s

t

3 Equivalent optimization problems and proof of Theorem 2

In this section we will consider different optimization problems that are inherently related to p-
resistances. All graphs in this section are considered to be weighted.

3.1 Equivalent optimization problems

Consider the following two optimization problems for p > 1:

Flow-problem:

Rp(s  t) := minnPe2E re|ie|p  i = (ie)e2E unit ﬂow from s to to (⇤)

4

Potential problem: Cp(s  t) = minn Xe=(u v)

|'(u)  '(v)|1+ 1

p1

1

r

p1
e

 '(s)  '(t) = 1o

(⇤⇤)

It is well known that these two problems are equivalent for p = 2 (see Section 1.3 of Doyle and
Snell  2000). We will now extend this result to general p > 1.

Proposition 4 (Equivalent optimization problems) For p > 1  the following statements are true:

1. The ﬂow problem (⇤) has a unique solution.
2. The solutions of (⇤) and (⇤⇤) satisfy Rp(s  t) = (Cp(s  t)) 1
p1 .

To prove this proposition  we derive the Lagrange dual of problem (⇤) and use the homogeneity of
the variables to convert it to the form of problem (⇤⇤). Details can be found in the supplementary
material. With this proposition we can now easily see why Theorem 2 is true.
Proof of Theorem 2. Part (1).
If we set p = 1  Problem (⇤) coincides with the well-known linear
programming formulation of the shortest path problem  see Chapter 12 of Bazaraa et al. (2010).
Part (2). For p = 2  we get the well-known formula for the effective resistance.
Part (3). For p ! 1  the objective function in the dual problem (⇤⇤) converges to

C1(s  t) := minnPe=(u v) |'(u)  '(v)| '(s)  '(t) = 1o.

This coincides with the well-known linear programming formulation of the min-cut problem in
unweighted graphs. Using Proposition 4 we ﬁnally obtain

lim
p!1

Rp(s  t)p1 = lim
p!1

1

Cp(s  t)

=

1

C1(s  t)

=

1

s-t-mincut .

4 Geometric graphs and the Proof of Theorem 3

In this section we consider the class of geometric graphs. The vertices of such graphs consist of
points X1  ..  Xn 2 Rd  and vertices are connected by edges if the corresponding points are “close”
(for example  they are k-nearest neighbors of each other). In most cases  we consider the set of
points as drawn i.i.d from some density on Rd. Consider the following general assumptions.
General Assumptions: Consider a family (Gn)n2N of unweighted geometric graphs where Gn is
based on X1  ...  Xn 2 M ⇢ Rd  d > 2. We assume that there exist 0 < r  R (depending on n
and d) such that the following statements about Gn holds simultaneously for all x 2{ X1  ...  Xn}:
1. Distribution of points: For ⇢ 2{ r  R} the number of sample points in B(x  ⇢) is of the
2. Graph connectivity: x is connected to all sample points inside B(x  r) and x is not con-

order ⇥(n · ⇢d).
nected to any sample point outside B(x  R).

3. Geometry of M: M is a compact  connected set such that M \ @M is still connected.
The boundary @M is regular in the sense that there exist positive constants ↵> 0 and
0  then for all points x 2 @M we have vol(B"(x) \ M ) 
"0 > 0 such that if "<"
↵ vol(B"(x)) (where vol denotes the Lebesgue volume). Essentially this condition just
excludes the situation where the boundary has arbitrarily thin spikes.

It is a straightforward consequence of these assumptions that there exists some function ⌧ (n) =: ⌧n
such that r and R are both of the order ⇥((⌧n/n)1/d) and all degrees in the graph are of order ⇥(⌧n).

4.1 Lower and upper bounds and the proof of Theorem 3

To prove Theorem 3 we need to study the balance between Rlocal
shorthand notation

p

and Rglobal

p

. We introduce the

T1 =⇥⇣

1

np(11/d)1⌧ p(1+1/d)1

n

⌘   T2 =⇥⇣

1

⌧ 2(p1)
n

1/rXk=1

1

k(d2)(p1)⌘.

5

Theorem 5 (General bounds on Rlocal
) Consider a family (Gn)n2N of unweighted
geometric graphs that satisﬁes the general assumptions. Then the following statements are true
for any ﬁxed pair s  t of vertices in Gn:

and Rglobal

p

p

4C > Rlocal

p

(s  t) 

1

dp1
s

+

1

dp1
t

and

T1 + T2  Rglobal

p

(s  t)  T1.

Note that by taking the sum of the two inequalities this theorem also leads to upper and lower
bounds for Rp(s  t) itself. The proof of Theorem 5 consists of several parts. To derive lower bounds
on Rp(s  t) we construct a second graph G0n which is a contracted version of Gn. Lower bounds
can then be obtained by Rayleigh’s monotonicity principle. To get upper bounds on Rp(s  t) we
e 
where i is any unit ﬂow from s to t. We construct a particular ﬂow that leads to a good upper bound.
Finally  investigating the properties of lower and upper bounds we can derive the individual bounds
on Rlocal
Theorem 3 can now be derived from Theorem 5 by straight forward computations.

exploit the fact that the p-resistance in an unweighted graph can be upper bounded byPe2E ip

. Details can be found in the supplementary material.

and Rglobal

p

p

4.2 Applications

Our general results can directly be applied to many standard geometric graph models.
The "-graph. We assume that X1  ...  Xn have been drawn i.i.d from some underlying density f
on Rd  where M := supp(f ) satisﬁes Part (3) of the general assumptions. Points are connected by
unweighted edges in the graph if their Euclidean distances are smaller than ". Exploiting standard
results on "-graphs (cf. the appendix in von Luxburg et al.  2010)  it is easy to see that the general
assumptions (1) and (2) are satisﬁed with probability at least 1 c1n exp(c2n"d) (where c1  c2 are
constants independent of n and d) with r = R = " and ⌧n =⇥( n"d). The probability converges to
1 if n ! 1  " ! 0 and n"d/ log(n) ! 1.
k-nearest neighbor graphs. We assume that X1  ...  Xn have been drawn i.i.d from some un-
derlying density f on Rd  where M := supp(f ) satisﬁes Part (3) of the general assumptions. We
connect each point to its k nearest neighbors by an undirected  unweighted edge. Exploiting stan-
dard results on kNN-graphs (cf.
the appendix in von Luxburg et al.  2010)  it is easy to see that
the general assumptions (1) and (2) are satisﬁed with probability at least 1  c1k exp(c2k) with
r =⇥ (k/n)1/d  R =⇥ (k/n)1/d  and ⌧n = k. The probability converges to 1 if n ! 1 
k ! 1  and k/ log(n) ! 1.
Lattice graphs. Consider uniform lattices such as the square lattice or triangular lattice in Rd.
These lattices have constant degrees  which means that ⌧n = ⇥(1). If we denote the edge length of
grid by "  the total number of nodes in the support will be in the order of n =⇥(1 /"d). This means
that the general assumptions hold for r = R = " =⇥(
n1/d ) and ⌧n = ⇥(1). Note that while the
lower bounds of Theorem 3 can be applied to the lattice case  our current upper bounds do not hold
because they require that ⌧n ! 1.
5 Regularization by p-Laplacians

1

One of the most popular methods for semi-supervised learning on graphs is based on Laplacian
regularization. In Zhu et al. (2003) the label assignment problem is formulated as

subject to

' = argminx C(x)

(2)
where yi 2 {±1}  C(x) := 'T L' is the energy function involving the standard (p = 2) graph
Laplacian L. This formulation is appealing and works well for small sample problems. However 
Nadler et al. (2009) showed that the method is not well posed when the number of unlabeled data
points is very large. In this setting  the solution of the optimization problem converges to a constant
function with “spikes” at the labeled points. We now present a simple theorem that connects these
ﬁndings to those concerning the resistance distance.

'(xi) = yi   i = 1  . . .   l

Theorem 6 (Laplacian regularization in terms of resistance distance) Consider a semi-super-
vised classiﬁcation problem with one labeled point per class: '(s) = 1  '(t) = 1. Denote

6

the solution of (2) by '⇤  and let v be an unlabeled data point. Then

'⇤(v)  '⇤(t) >' ⇤(s)  '⇤(v) () R2(v  t) > R2(v  s).

It is easy to verify that '⇤ = L†(es  et) and R2(s  t) = (es  et)T L†(es  et) where L†
Proof.
is the pseudo-inverse of the Laplacian matrix L. Therefore we have '⇤(v) = (ev)T L†(es  et) and

'⇤(v)  '⇤(t) >' ⇤(s)  '⇤(v) () (ev  et)T L†(es  et) > (es  ev)T L†(es  et)
() (ev  et)T L†(ev  et) > (ev  es)T L†(ev  es) () R2(v  t) > R2(v  s).

(a)

Here in step (a) we use the symmetry of L† to state that eT
What does this theorem mean? We have seen above that in case p = 2  if n ! 1 
+

v L†es = eT

s L†ev.

+

and

R2(v  t) ⇡

1
dv

1
dt

R2(v  s) ⇡

1
dv

1
ds

.

2

Hence  the theorem states that if we threshold the function '⇤ at 0 to separate the two classes  then
all the points will be assigned to the labeled vertex with larger degree.
Our conjecture is that an analogue to Theorem 6 also holds for general p. For a precise formulation 
deﬁne the matrix r as

ri j =⇢'(i)  '(j)
ij )1/m)n1/n. Consider q such that 1/p +
and introduce the matrix norm kAkm n =Pi((Pj am
1/q = 1. We conjecture that if we used krkq q as a regularizer for semi-supervised learning  then
the corresponding solution '⇤ would satisfy

i ⇠ j
otherwise

0

'⇤(v)  '⇤(t) >' ⇤(s)  '⇤(v) () Rp(v  t) > Rp(v  s).

That is  the solution of the q-regularized problem would assign labels according to the Rp-distances.
In particular  using q-regularization for the value q with 1/q + 1/p⇤ = 1 would resolve the artifacts
of Laplacian regularization described in Nadler et al. (2009).
It is worth mentioning that this regularization is different from others in the literature. The usual
Laplacian regularization term as in Zhu et al. (2003) coincides with krk2 2  Zhou and Sch¨olkopf
(2005) use the krk2 p norm  and our conjecture is that the krkq q norm would be a good candidate.
Proving whether this conjecture is right or wrong is a subject of future work.

6 Related families of distance functions on graphs

In this section we sketch some relations between p-resistances and other families of distances.

6.1 Comparing Herbster’s and our deﬁnition of p-resistances
For p  2  Herbster and Lever (2009) introduced the following deﬁnition of p-resistances:
RH

|'(u)  '(v)|p0

with CH

1

p0 (s  t) :=

p0 (s  t) := minn Xe=(u v)

CH

p0 (s  t)

re

 '(s)  '(t) = 1o.

In Section 3.1 we have seen that the potential and ﬂow optimization problems are duals of each
other. Based on this derivation we believe that the natural way of relating RH and CH would be to
replace the p0 in Herbster’s potential formulation by q0 such that 1/p0 +1/q0 = 1. That is  one would
q0 . In particular  reducing Herbster’s p0 towards 1
have to consider CH
has the same inﬂuence as increasing our p to inﬁnity and makes RH
p0 converge to the minimal s-t-cut.
To ease further comparison  let us assume for now that we use “our” p in the deﬁnition of Herbster’s
resistances. Then one can see by similar arguments as in Section 3.1 that RH

p0 := 1/CH

p can be rewritten as

q0 and then deﬁne bRH
p (s  t) := minnXe2E

rp1
e

RH

|ie|p i = (ie)e2E unit ﬂow from s to to.

(H)

7

Now it is easy to see that the main difference between Herbster’s deﬁnition (H) and our deﬁnition
(⇤) is that (H) takes the power p  1 of the resistances re  while we keep the resistances with
power 1. In many respects  Rp and RH
p have properties that are similar to each other: they satisfy
slightly different versions (with different powers or weights) of the triangle inequality  Rayleigh’s
monotonicity principle  laws for resistances in series and in parallel  and so on. We will not discuss
further details due to space constraints.

6.2 Other families of distances

There also exist other families of distances on graphs that share some of the properties of p-
resistances. We will only discuss the ones that are most related to our work  for more references
see von Luxburg et al. (2010). The ﬁrst such family was introduced by Yen et al. (2008)  where the
authors use a statistical physics approach to reduce the inﬂuence of long paths to the distance. This
family is parameterized by a parameter ✓  contains the shortest path distance at one end (✓ ! 1)
and the standard resistance distance at the other end (✓ ! 0). However  the construction is somewhat
ad hoc  the resulting distances cannot be computed in closed form and do not even satisfy the triangle
inequality. A second family is the one of “logarithmic forest distances” by Chebotarev (2011). Even
though its derivation is complicated  it has a closed form solution and can be interpreted intuitively:
The contribution of a path to the overall distance is “discounted” by a factor (1/↵)l where l is the
length of the path. For ↵ ! 0  the logarithmic forest distance distance converges to the shortest path
distance  for ↵ ! 1  it converges to the resistance distance.
At the time of writing this paper  the major disadvantage of both the families introduced by Yen
et al. (2008) and Chebotarev (2011) is that it is unknown how their distances behave as the size of
the graph increases. It is clear that on the one end (shortest path)  they convey global information 
whereas on the other end (resistance distance) they depend on local quantities only when n ! 1.
But what happens to all intermediate parameter values? Do all of them lead to meaningless distances
as n ! 1  or is there some interesting phase transition as well? As long as this question has not
been answered  one should be careful when using these distances. In particular  it is unclear how the
parameters (✓ and ↵  respectively) should be chosen  and it is hard to get an intuition about this.

7 Conclusions

t

s + 1/dp1

We proved that the family of p-resistances has a wide range of behaviors. In particular  for p = 1
it coincides with the shortest path distance  for p = 2 with the standard resistance distance and
for p ! 1 it is related to the minimal s-t-cut. Moreover  an interesting phase transition takes
place: in large geometric graphs such as k-nearest neighbor graphs  the p-resistance is governed by
meaningful global properties as long as p < p⇤ := 1 + 1/(d 1)  whereas it converges to the trivial
local quantity 1/dp1
if p > p⇤⇤ := 1 + 1/(d  2). Our suggestion for practice is to use
p-resistances with p ⇡ p⇤. For this value of p  the p-resistances encode those global properties of
the graph that are most important for machine learning  namely the cluster structure of the graph.
Our ﬁndings are interesting on their own  but also help in explaining several artifacts discussed in the
literature. They go much beyond the work of von Luxburg et al. (2010) (which only studied the case
p = 2) and lead to an intuitive explanation of the artifacts of Laplacian regularization discovered in
Nadler et al. (2009). An interesting line of future research will be to connect our results to the ones
about p-eigenvectors of p-Laplacians (B¨uhler and Hein  2009). For p = 2  the resistance distance
can be expressed in terms of the eigenvalues and eigenvectors of the Laplacian. We are curious to
see whether a reﬁned theory on p-eigenvalues can lead to similarly tight relationships for general
values of p.

Acknowledgements

We would like to thank the anonymous reviewers who discovered an inconsistency in our earlier
proof  and Bernhard Sch¨olkopf for helpful discussions.

8

References
M. Bazaraa  J. Jarvis  and H. Sherali. Linear Programming and Network Flows. Wiley-Interscience 

2010.

B. Bollobas. Modern Graph Theory. Springer  1998.
T. B¨uhler and M. Hein. Spectral clustering based on the graph p-Laplacian. In Proceedings of the

International Conference on Machine Learning (ICML)  pages 81–88  2009.

P. Chebotarev. A class of graph-geodetic distances generalizing the shortets path and the resistance

distances. Discrete Applied Mathematics  159(295 – 302)  2011.

P. G. Doyle and J. Laurie Snell. Random walks and electric networks  2000. URL http://www.

citebase.org/abstract?id=oai:arXiv.org:math/0001057.

M. Herbster and G. Lever. Predicting the labelling of a graph via minimum p-seminorm interpola-

tion. In Conference on Learning Theory (COLT)  2009.

B. Nadler  N. Srebro  and X. Zhou. Semi-supervised learning with the graph Laplacian: The limit
of inﬁnite unlabelled data. In Advances in Neural Information Processing Systems (NIPS)  2009.
J. Tenenbaum  V. de Silva  and J. Langford. Supplementary material to ”A Global Geometric
Framework for Nonlinear Dimensionality Reduction”. Science  290:2319 – 2323  2000. URL
http://isomap.stanford.edu/BdSLT.pdf.

U. von Luxburg  A. Radl  and M. Hein. Getting lost in space: Large sample analysis of the commute

distance. In Neural Information Processing Systems (NIPS)  2010.

L. Yen  M. Saerens  A. Mantrach  and M. Shimbo. A family of dissimilarity measures between
nodes generalizing both the shortest-path and the commute-time distances. In Proceedings of the
14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages
785–793  2008.

D. Zhou and B. Sch¨olkopf. Regularization on discrete spaces. In DAGM-Symposium  pages 361–

368  2005.

X. Zhu  Z. Ghahramani  and J. D. Lafferty. Semi-supervised learning using Gaussian ﬁelds and

harmonic functions. In ICML  pages 912–919  2003.

9

,Junming Yin
Qirong Ho
Eric Xing
Kaito Fujii
Hisashi Kashima