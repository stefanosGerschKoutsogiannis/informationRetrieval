2019,Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions,We present a family of algorithms  called descent algorithms  for optimizing convex and non-convex functions. We also introduce a new first-order algorithm  called rescaled gradient descent (RGD)  and show that RGD achieves a faster convergence rate than gradient descent provided the function is strongly smooth  - a natural generalization of the standard smoothness assumption on the objective function. When the objective function is convex  we present two frameworks for “accelerating” descent methods  one in the style of Nesterov and the other in the style of Monteiro and Svaiter. Rescaled gradient descent can be accelerated under the same strong smoothness assumption using both frameworks. We provide several examples of strongly smooth loss functions in machine learning and numerical experiments that verify our theoretical findings.,Accelerating Rescaled Gradient Descent:
Fast Optimization of Smooth Functions

Ashia C. Wilson
Microsoft Research

ashia.wilson@microsoft.com

Lester Mackey

Microsoft Research

lmackey@microsoft.com

Andre Wibisono

Georgia Tech

wibisono@gatech.edu

Abstract

We present a family of algorithms  called descent algorithms  for optimizing
convex and non-convex functions. We also introduce a new ﬁrst-order algorithm 
called rescaled gradient descent (RGD)  and show that RGD achieves a faster
convergence rate than gradient descent over the class of strongly smooth functions –
a natural generalization of the standard smoothness assumption on the objective
function. When the objective function is convex  we present two frameworks
for accelerating descent algorithms  one in the style of Nesterov and the other in
the style of Monteiro and Svaiter  using a single Lyapunov function. Rescaled
gradient descent can be accelerated under the same strong smoothness assumption
using both frameworks. We provide several examples of strongly smooth loss
functions in machine learning and numerical experiments that verify our theoretical
ﬁndings. We also present several extensions of our novel Lyapunov framework
including deriving optimal universal higher-order tensor methods and extending
our framework to the coordinate descent setting.

1

Introduction

We consider the optimization problem

x∈X f (x)
min

X with inner product norm (cid:107)v(cid:107) :=(cid:112)(cid:104)v  Bv(cid:105) and a dual norm (cid:107)s(cid:107)∗ :=(cid:112)(cid:104)s  B−1s(cid:105) for s in the dual

(1)
where f : X → R is a continuously differentiable function  on a ﬁnite-dimensional real vector space
space X ∗. Here  B : X → X ∗ is a positive deﬁnite self-adjoint operator. We assume the minimum
of f is attainable and let x∗ represent a point in arg minx∈X f (x).
We study the performance of a family of discrete-time algorithms parameterized by δ > 0 and an
integer scalar 1 < p ≤ ∞  called δ-descent algorithms of order p. These algorithms meet a progress
condition that allows us to derive fast non-asymptotic convergence rate upper bounds  parameterized
by p  for both nonconvex and convex instances of (1). For example  descent algorithms of order
1 < p < ∞ satisfy the upper bound f (xk) − f (x∗) = O(1/(δk)p−1) for convex functions.
Using this framework we introduce a new method for smooth optimization called rescaled gradient
descent (RGD) 

xk+1 = xk − η

1

p−1

B−1∇f (xk)
(cid:107)∇f (xk)(cid:107) p−2
p−1∗

 

η > 0  p > 1.

We show that if (1) is sufﬁciently smooth  rescaled gradient descent is a δ-descent algorithm of
order p  and subsequently converges quickly to solutions of (1). RGD can be viewed as a natural
generalization of gradient descent (p = 2) and normalized gradient descent (p = ∞)  whose
non-asymptotic behavior for quasi-convex functions has been well-studied ([11]).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2

3p−2

When f is convex  we present two frameworks for obtaining algorithms with faster convergence
rate upper bounds. The ﬁrst  pioneered in Nesterov [22  23  24  25]  shows how to wrap a δ-descent
method of order 1 < p < ∞ in two sequences to obtain a method that satisﬁes f (xk) − f (x∗) =
O(1/(δk)p). The second  introduced by [18]  shows how to wrap a δ-descent method of order
1 < p < ∞ in the same set of sequences and add a line search step to obtain a method that satisﬁes
f (xk) − f (x∗) = O(1/(δk)
). We provide a general description of both frameworks and show
how they can be applied to RGD and other descent methods of order p.
Our motivation also comes from a burgeoning literature (e.g.  [27  28  30  33  13  35  4  8  32  29  31 
17]) that harnesses the connection between dynamical systems and optimization algorithms to develop
new analyses and optimization methods. Rescaled gradient descent is obtained by discretizing an
ODE called rescaled gradient ﬂow introduced by [34]. We compare RGD and accelerated RGD to
the work of Zhang et al. [36]  who introduce accelerated dynamics and apply Runge-Kutta integrators
to discretize them. They show that Runge-Kutta integrators converge quickly when the function is
sufﬁciently smooth and when the order of the integrator is sufﬁciently large. We provide a better
convergence rate upper bound for accelerated RGD under a very similar smoothness assumption. We
also compare our work to Maddison et al. [17]  who introduces conformal Hamiltonian dynamics and
show that if the objective function is sufﬁciently smooth  algorithms obtained by discretizing these
dynamics converge at a linear rate. We show (accelerated) RGD also achieves a fast linear rate under
similar smoothness conditions.
The remainder of this paper is organized as follows. Section 2 introduces δ-descent algorithms
and Section 2.1 describes several examples of descent algorithms that are popular in optimization.
Section 2.2 introduces RGD and Section 3 presents two frameworks for accelerating δ-descent
methods and applies both to RGD. Section 5 describes several examples of strongly smooth objective
functions as well as experiments to verify our ﬁndings. Finally  Section 6 discusses simple extensions
of our framework  including deriving and analyzing optimal universal tensor methods for objective
functions that have Hölder-continuous higher-order gradients and extending our entire framework
and results to the coordinate setting.

2 Descent Algorithms

The focus of this section is a family of algorithms called δ-descent algorithms of order p.
Deﬁnition 1 An algorithm xk+1 = A(xk) is a δ-descent algorithm of order p for 1 < p ≤ ∞ if
for some constant 0 < δ < ∞ it satisﬁes

f (xk+1)−f (xk)

δ

f (xk+1)−f (xk)

δ

≤ −(cid:107)∇f (xk)(cid:107) p
p−1∗
≤ −(cid:107)∇f (xk+1)(cid:107) p
p−1∗

for all k ≥ 0  or
for all k ≥ 0.

(2a)

(2b)

For δ-descent algorithms of order p  it is possible to obtain non-asymptotic convergence guarantees
for non-convex  convex and gradient dominated functions. Recall  a function is µ-gradient dominated
of order p ∈ (1 ∞] if

p (cid:107)∇f (x)(cid:107) p
p−1

p−1∗ ≥ µ

(3)
When p = 2  (3) is the Polyak-Łojasiewicz condition introduced concurrently by Polyak [27] and
Łojasiewicz [16]. For the following three theorems  we use the shorthand E0 := f (x0) − f (x∗) and
assume f is differentiable.
Theorem 1 Any δ-descent algorithm of order p satisﬁes

1

p−1 (f (x) − f (x∗)) 

∀x ∈ X .

min0≤s≤k (cid:107)∇f (xs)(cid:107)∗ ≤ (E0/(δk))

p−1
p .

(4)

  then any

Theorem 2 If f is convex with R = supx:f (x)≤f (x0) (cid:107)x − x∗(cid:107) < ∞  and cp := (1−1/p)p
p−1
δ-descent algorithm of order p satisﬁes
f (xk) − f (x∗) ≤

p (cid:1)−p

2(cid:0) 1

= O(1/(1 + 1

(cid:40)

p−1

0

+

E1/p

(δk)
2E0 exp(−δk/(Rγ)) 

Rγc1/p

1
p p

p−1

Rγp (δk)

p )p)  p < ∞
p = ∞.
p−1
p )p−1 when (2b) is satisﬁed.

(5)

where γ = 1 when (2a) is satisﬁed and γ = (1 + 1

Rp (E0/cp)

1

p δ

2

Theorem 3 If f is µ-gradient dominated of order p  then any δ-descent algorithm of order p satisﬁes
(6)

f (xk) − f (x∗) ≤ E0 exp

p−1 δk

.

1

(cid:16)− p

p−1 µ

(cid:17)

The proof of Theorems 1 to 3 are all based on simple energy arguments and can be found in
Appendix B. Bounds of the form (4) are common in the non-convex optimization literature and have
previously been established for gradient descent (p = 2 see e.g. [26  Thm1]) and higher-order tensor
methods (see e.g.[6]). Theorem 1 provides a more general description of algorithms that satisfy this
kind of bound.
Typically  algorithms satisfy the progress condition (2) for speciﬁc smoothness classes of functions.
For example  gradient descent with step-size 0 < η ≤ 1/L is a δ-descent method of order p = 2
with δ = η/2 when (cid:107)∇2f(cid:107) ≤ L. Throughout  we denote (cid:107)B(cid:107) = max(cid:107)h(cid:107)≤1 (cid:107)Bh(cid:107)∗  for any
B : X → X ∗. We list several other examples.

2.1 Examples of descent algorithms

(cid:26)

Theorems 1  2 and 3 provide a seamless way to derive standard upper bounds for many algorithms in
optimization.
Example 1 The universal higher-order tensor method 

xk+1 = arg min
x∈X

where fp−1(y; x) = (cid:80)p−1
(7)
i!∇if (x)(y − x)i is the (p − 1)-st order Taylor approximation of f
centered at x and ˜p = p − 1 + ν for ν ∈ (0  1]  has been studied by several works [3  34  21]. When
f is convex and has Hölder-smooth (p − 1)-st order gradients  namely (cid:107)∇p−1f (x) − ∇p−1f (y)(cid:107) ≤
L(cid:107)x − y(cid:107)ν  (7) with step size 0 < η ≤ √
  is a δ-descent algorithm of order ˜p with δ =

(cid:107)x − xk(cid:107) ˜p

fp−1(x; xk) +

1
˜pη

i=0

1

 

3(p−2)!
2L

(cid:27)

1

2 ˜p−3
˜p−1 .

˜p−1 /2
η
Example 2 The natural proximal method 

(cid:27)
where (cid:107)v(cid:107)x =(cid:112)(cid:104)v ∇2h(x)v(cid:105) was introduced in the setting h(x) = 1
2(cid:107)x(cid:107)2

xk+1 = arg min
x∈X

(cid:107)x − xk(cid:107)p

f (x) +

(cid:26)

1
pη

xk

 

0 and mB (cid:22) ∇2h  the proximal method is a δ-descent algorithm of order p with δ = m
Example 3 Natural gradient descent 

(8)

2 by [19]. For any η  m >
p−1 /p.

p−1 η

p

1

(cid:27)

(cid:26)

1
η

xk+1 = xk − η∇2h(xk)−1∇f (xk) = arg min
x∈X

where (cid:107)v(cid:107)x =(cid:112)(cid:104)v ∇2h(x)v(cid:105) was introduced by [2]. Suppose (cid:107)∇2f(cid:107) ≤ L and mB (cid:22) ∇2h (cid:22)

(cid:104)∇f (xk)  x(cid:105) +

(cid:107)x − xk(cid:107)2

1
2η

(9)

xk

 

M B for some m  L  M > 0. Then natural gradient descent with step size 0 < η ≤ m2
algorithm of order p = 2 with δ = η
Example 4 Mirror descent 

2M .

M L is a δ-descent

(cid:104)∇f (xk)  x(cid:105) +

1
η

xk+1 = arg min
x∈X

(10)
where Dh(x  y) = h(x)− h(y)−(cid:104)∇h(y)  x− y(cid:105) is the Bregman divergence was introduced by [20].
Suppose (cid:107)∇2f(cid:107) ≤ L and mB (cid:22) ∇2h (cid:22) M B for some m  L  M > 0. Then mirror descent with
step size 0 < η ≤ m2
Example 5 The proximal Bregman method 

M L is a δ-descent algorithm of order p = 2 with δ = η

Dh(x  xk)

2M .

 

(cid:26)

(cid:27)

(cid:26)

(cid:27)

(11)
was introduced by [7]). When mB (cid:22) ∇2h (cid:22) M B the proximal Bregman method with step-size
η > 0 is a δ-descent algorithm of order p = 2 with δ = mη
2M 2 .

xk+1 = arg min
x∈X

Dh(x  xk)

f (x) +

 

Details for these examples are contained in Appendix B.2.

3

2.2 Rescaled gradient descent

(cid:110)(cid:104)∇f (xk)  x(cid:105) + 1

pη(cid:107)x − xk(cid:107)p(cid:111)

 

(12)

We end this section by discussing the function class for which rescaled gradient descent (RGD) 

xk+1 = xk − η

1

p−1 B−1∇f (xk)
(cid:107)∇f (xk)(cid:107) p−2
p−1∗

= arg minx∈X

is a δ-descent method of order p.
Deﬁnition 2 A function f is strongly smooth of order p for some integer p > 1  if there exist
constants 0 < L1  . . .   Lp < ∞ such that for m = 1  . . .   p − 1 and for all x ∈ Rd:

and moreover for m = p  f satisﬁes the condition |∇pf (x)(v)p| ≤ Lp(cid:107)v(cid:107)p  ∀v ∈ X .

|∇mf (x)(B−1∇f (x))m| ≤ Lm(cid:107)∇f (x)(cid:107)m+ p−m

p−1

∗

(13)

Here  ∇mf (x)(h)m =(cid:80)d

f (x)(cid:81)m

i1 ... im=1 ∂xi1 ...xim

j=1 hij where ∂xif is the partial derivative of
f with respect to xi. We can always take L1 = 1. When p = 2  (13) is the usual Lipschitz condition
on the gradient of f  but otherwise (13) is stronger. In particular  if f is strongly smooth of order p 
then the minimizer x∗ has order at least p − 1  i.e.  the higher gradients vanish: ∇mf (x∗) = 0 for
m = 1  . . .   p − 1  whereas this is not implied under mere smoothness. An example of a strongly
smooth function of order p is the p-th power of the (cid:96)2-norm f (x) = (cid:107)x(cid:107)p
2 with B = I  or the (cid:96)p-norm
f (x) = (cid:107)x(cid:107)p
p. We discuss other families of strongly smooth functions in Section 5. Finally  it is worth
mentioning that for most of our results  the absolute value on the left hand side of (13) is unnecessary.
We now present the main result regarding the performance of RGD on functions that satisfy (13):
Theorem 4 Suppose f is strongly smooth of order p > 1 with constants 0 < L1  . . .   Lp < ∞. Then
rescaled gradient descent with step-size

(cid:26)

(cid:27)

0 < η

1

p−1 ≤ min

1 

(2(cid:80)p

1

m=2

m! )
Lm

(14)

satisﬁes the descent condition (2a) with δ = η

1

p−1 /2.

The proof of Theorem 4 is in Appendix B.3. A corollary to Theorems 1-4 is the following theorem.
Theorem 5 RGD with a step size that satisﬁes (14) achieves convergence rate guarantee (4) when f
is differentiable and strongly smooth of order p  (5) when f is convex function and strongly smooth of
order p  and (6) when f is µ-uniformly convex and strongly smooth of order p  where δp−1 = η/2p−1.
Our results show rescaled gradient descent can minimize the canonical p-strongly smooth and
p(cid:107)x(cid:107)p at an exponential rate; in contrast  gradient descent can
uniformly convex function f (x) = 1
only minimize it at a polynomial rate  even in one dimension. We provide the proof of Proposition 6
in Appendix B.4.
Proposition 6 Let f : R → R be f (x) = 1
For any step size 0 < η
p minimizes f at an exponential rate: f (xk) = (1 − η
any η
f (xk) = Ω((η

p|x|p for p > 2  with minimizer x∗ = 0 and f (x∗) = 0.
p−1 < 1 and initial position x0 ∈ R  rescaled gradient descent of order
p−1 )pkf (x0). On the other hand  for
p−2   gradient descent minimizes f at a polynomial rate:

p−1 > 0 and |x0| < (2η

p−1 k)

p−2 ).

p−1 )

− p

− 1

1

1

1

1

1

We now demonstrate how all the aforementioned examples of δ-descent methods can be accelerated.

3 Accelerating Descent Algorithms

We present two frameworks for accelerating descent algorithms based on the dynamical systems
perspective introduced by Wibisono et al. [34] and Wilson et al. [35] and apply them to RGD. The
backbone of both frameworks is the Lyapunov function

Ek = Ak(f (xk) − f (x∗)) + Dh(x∗  zk) 

and two sequences (15) and (16). The connection between continuous time dynamical systems and
these two sequences and Lyapunov function is described in [35]. We present a high-level description
of both techniques in the main text and leave details of our analysis to Appendix C.

4

3.1 Nesterov acceleration of descent algorithms

In the context of convex optimization  the technique of “acceleration” has its origins in Nesterov [22]
and reﬁned in Nesterov [23]. In these works  Nesterov showed how to combine gradient descent with
two sequences to obtain an algorithm with an optimal convergence rate. There have been many works
since (as well as some frameworks  including [15  1  14  35]) describing how to accelerate various
other algorithms to obtain methods with superior convergence rates.
Wilson et al. [35]  for example  show the following two discretizing schemes 

where yk+1 satisﬁes the δ

xk = δτkzk + (1 − δτk)yk

zk+1 = arg minz∈X(cid:8)αk(cid:104)∇f (xk)  z(cid:105) + 1
(cid:8)αk(cid:104)∇f (yk+1)  z(cid:105) + 1

p−1 -descent condition f (yk+1) − f (xk) ≤ −δ
xk = δτkzk + (1 − δτk)yk
zk+1 = arg minz

δ Dh(z  zk)(cid:9)
δ Dh(z  zk)(cid:9)  

p

p

(15a)
(15b)

; and

p−1(cid:107)∇f (xk)(cid:107) p
p−1∗

(16a)
(16b)
p−1 -descent) condition f (yk+1) − f (xk) ≤
where the update for yk+1 satisﬁes the (δ
(cid:104)∇f (yk+1)  yk+1 − xk(cid:105) ≤ −δ
p−1(cid:107)∇f (yk+1)(cid:107) p
p−1∗
  constitute an “accelerated method”. Their
results can be summarized in the following theorem.
Theorem 7 Assume for all x  y ∈ X   the function h satisﬁes the local uniform convexity condition
Dh(x  y) ≥ 1
p(cid:107)x− y(cid:107)p. Then sequences (15) and (16) with parameter choices αk = (δ/p)p−1k(p−1)
(where k(p) := k(k + 1)··· (k + p − 1) is the rising factorial) and τk = p
δk ) satisfy 

δ(p+k) = Θ( p

p

p

f (yk) − f (x∗) ≤ ppDh(x∗  z0)

(δk)p

= O (1/(δk)p) .

(17)

Proof details are contained in Appendix C.1. Wilson et al. [35] call these new methods accelerated
descent methods due to the fact that Theorem 2 guarantees implementing just the yk+1 sequence
(where we set xk = yk) satisﬁes f (yk) − f (x∗) ≤ O(1/(˜δk)p−1)  where ˜δp−1 = δp. The computa-
tional cost of adding sequences (15a) and (15b) (or (16a) and (16b)) to the descent method is at most
an additional gradient evaluation.

Remark 1 (Restarting for accelerated linear convergence) If  in addition  f is µ-gradient domi-
nated of order p  then algorithms (15) and (16) combined with a scheme for restarting the algorithm
has a convergence rate upper bound f (yk) − f (x∗) = O(exp(−µ
p δk)). We can consider this
algorithm an accelerated method given the original descent method satisﬁes f (yk) − f (x∗) =
O(exp(−µ
To summarize  it is sufﬁcient to establish conditions under which an algorithm is a δ-descent algorithm
of order p in order to (1) obtain a convergence rate and (2) accelerate the algorithm (in most cases).

p−1 ˜δk)) under the same condition  where ˜δp−1 = δp. See Appendix C.2 for details.

1

1

Accelerated rescaled gradient descent (Nesterov-style) Using (15) we accelerate RGD.

Algorithm 1 Nesterov-style accelerated rescaled gradient descent.
Require: f satisﬁes (13) and h satisﬁes Dh(x  y) ≥ 1
p(cid:107)x − y(cid:107)p
1: Set x0 = z0  Ak = (δ/p)pk(p)  αk = Ak+1−Ak
  τk = αk
Ak+1
2: for k = 1  . . .   K do
3: xk = δτkzk + (1 − δτk)yk

4: zk+1 = arg minz∈X(cid:8)αk(cid:104)∇f (xk)  z(cid:105) + 1

δ Dh(z  zk)(cid:9)

δ

1

p−1 B−1∇f (xk)/(cid:107)∇f (xk)(cid:107) p−2
p−1∗

5: yk+1 = xk − η
6: return yK.

  and δ

p

p−1 = η

1

p−1 /2.

We summarize the performance of Algorithm 1 in the following Corollary to Theorems 4 and 7:

5

Theorem 8 Suppose f is convex and strongly smooth of order 1 < p < ∞ with constants 0 <
L1  . . .   Lp < ∞. Also suppose η satisﬁes (14). Then Algorithm 1 satisﬁes the convergence rate
upper bound (17).

3.2 Monteiro-Svaiter acceleration of descent algorithms

2

3p−2

Recently  Monteiro and Svaiter [18] have introduced an alternative framework for accelerating descent
methods  which is similar to Nesterov’s scheme but includes a line search step. This framework
was further generalized by several more recent concurrent works [9  12  5] who demonstrate that
higher-order tensor method (7) with the addition of a line search step obtains a convergence rate upper
bound f (yk) − f (x∗) = O(1/k
). When p = 2  this rate matches that of the Nesterov-style
acceleration framework  but for p > 2 it is better. In this section  we present a novel  generalized
version of the Monteiro-Svaiter accleration framework. In particular  we use a simple Lyapunov
analysis to generalize the framework and show that many other descent methods of order p can be
accelerated in it  including the proximal method (8)  RGD (12) and universal tensor methods.
Theorem 9 Suppose h is satisﬁes the condition B (cid:22) ∇2h. Consider sequence (15) where in addition 
we add a line search step which ensures the inequalities
(cid:107)yk+1 − xk(cid:107)p−2 ≤ b 
(cid:107)yk+1 − xk + λk+1∇f (yk+1)(cid:107) ≤ 1

a ≤ λk+1

0 < a < b

2(cid:107)yk+1 − xk(cid:107)

(18b)

(18a)

and

3p−2

δ

2

hold for the pair (λk+1  yk+1)  where λk+1 = δ2α2

k/Ak+1 . Then the composite sequence satisﬁes:

f (yk) − f (x∗) ≤ p

3p−2
2 Dh(x∗ x0)
(δk)

3p−2

2

p
2

= O

1/(δk)

3p−2

2

.

(19)

(cid:16)

(cid:17)

The proof of Theorem 9 is in Appendix C.3. All the aforementioned concurrent works have demon-
strated that the higher-order gradient method (ν = 1) with the addition line search step satisﬁes (18).
We show the same is true of the proximal method (8)  rescaled gradient descent (12) and universal
higher-order tensor methods. See Appendix C.5 for details. We conjecture that all methods that
satisfy conditions (18a) and (18b) are descent methods of order p with an additional line search step.

Remark 2 (Restarting for improved accelerated linear rate) If  in addition  f is µ-gradient dom-
inated of order p  then (18) combined with a scheme for restarting the algorithm satisﬁes the
convergence rate upper bound f (yk) − f (x∗) = O(exp(−µ
3p−2 δk)). See Appendix C.2 for details.

2

3.3 Accelerating rescaled gradient descent (Monteiro-Svaiter-style)

Monteiro-Svaiter accelerated rescaled gradient descent is the following algorithm.

Algorithm 2 Monteiro-Svaiter-style accelerated rescaled gradient descent.
Require: f is strongly smooth of order 1 < p < ∞ and h satisﬁes B (cid:22) ∇2h.
m! )}
1: Set x0 = z0 = 0  A0 = 0  δ
2: for k = 1  . . .   K do
≤ 5
3: Choose λk+1 (e.g. by line search) such that 3
yk+1 = xk − η

5p   1/(2(cid:80)p

4 ≤ λk+1(cid:107)yk+1−xk(cid:107)p−2

p−1 ≤ min{ 2

∇f (xk)

2 = η  η

3p−2

m=2

p−1

Lm

η

 

1

1

4  where

(cid:107)∇f (xk)(cid:107) p−2
p−1∗

√

λk+1+

λk+1+4Akλk+1

2δ

and αk =

4: Update zk+1 = arg minz∈X(cid:8)αk(cid:104)∇f (yk+1)  z(cid:105) + 1

δ Dh(z  zk)(cid:9)

  Ak+1 = δαk + Ak  τk = αk
Ak+1
xk = δτkzk + (1 − δτk)yk.

5: return yK.

(so that λk+1 = δ2α2
k
Ak+1

) and

6

We summarize results on performance of Algorithm 2 in the following corollary to Theorem 9:
Theorem 10 Assume f is convex and strongly smooth of order 1 < p < ∞ with constants 0 <
L1  . . .   Lp < ∞. Then Algorithm 2 satisﬁes the convergence rate upper bound (19).

4 Related Work

Our acceleration framework is similar in spirit to a number of acceleration frameworks in the literature
(e.g.  Allen Zhu and Orecchia [1]  Lessard et al. [14]  Lin et al. [15]  Diakonikolas and Orecchia [8])
but applies more generally to descent methods of order p > 2. In particular  the present framework
builds off of the framework proposed by Wilson et al. [35]  but it (1) makes the connection to descent
methods more explicit and (2) incorporates a generalization and Lyapunov analysis of the Monteiro-
Svaiter acceleration framework. These manifold generalizations crucially allow us to propose RGD
and accelerated RGD  which has superior theoretical and empirical performance to several existing
methods on strongly smooth functions.

5 Examples and Numerical Experiments

We compare our result to several recent works that have shown that for some function classes  more
intuitive ﬁrst-order algorithms outperform gradient descent. In particular  both Zhang et al. [36] and
Maddison et al. [17] obtain ﬁrst-order algorithms by applying integration techniques to second-order
ODEs. When the objective function is sufﬁciently smooth  both show their algorithm outperforms
(accelerated) gradient descent. We show that Algorithms 1 and 2 achieves fast performance in theory
and in practice on similar objectives.

Runge-Kutta Zhang et al. [36] show that if one applies an s-th order Runge-Kutta integrator
to a family of second-order dynamics  then the resulting algorithm1 achieves a convergence rate
f (xk) − f (x∗) = O(1/k
s−1 )2 provided the function meet the following two conditions: (1) f
satisﬁes the gradient lower bound of order p ≥ 2  which means for all m = 1  . . .   p − 1 

ps

f (x) − f (x∗) ≥ 1

(cid:107)∇mf (x)(cid:107) p

p−m ∀ x ∈ Rn

(21)
for some constants 0 < C1  . . .   Cp−1 < ∞; and (2) for s ≥ p and M > 0  f is (s + 2)-times
differentiable and (cid:107)∇(i)f (x)(cid:107) ≤ M for i = p  p + 1  . . .   s + 2. One can show that if f is strongly
smooth of order p  then f satisﬁes the gradient lower bound of order p. The details of this result is
in D. While are unable to prove that condition (21) is equivalent to strong smoothness  we have yet to
ﬁnd an example of a function that satisﬁes (21) and is not strongly smooth.

Cm

Hamiltonian Descent Maddison et al. [17] show explicit integration techniques applied to con-
formal Hamiltonian dynamics converge at a fast linear rate for a function class larger than gradient
descent. The method entails ﬁnding a kinetic energy map that upper bounds the dual of the function.
All examples for which we can compute such a map given by [17] are uniformly convex and gradient
dominated functions; therefore  simply rescaling the gradient for these examples ensures a linear rate.

5.1 Examples

We provide several examples of strongly smooth functions in machine learning (see Appendix D.2
for details).

Example 6 The (cid:96)p loss function

f (x) = 1

p(cid:107)Ax − b(cid:107)p
p 

(22)

shown by Zhang et al. [36] to satisfy (21) of order p  is strongly smooth of order p.

1which requires at least s gradient evaluations per iteration
2this matches the rate of Algorithm 1 in the limit s → ∞  where s is the order of the integrator.

7

(a) Example 7: Logistic loss (Iteration)

(b) Example 6: (cid:96)4 loss (Iteration)

(c) Example 7: Logistic loss (Gradient)

(d) Example 6: (cid:96)4 loss (Gradient)

(e) Example 10: Hamiltonian function

Figure 1: Experimental results comparing RGD and accelerated RGD (ARGD) to gradient descent
(GD)  Nesterov accelerated GD (NAG) and Runge-Kutta (DD). The plots for Runge-Kutta use an
s = 2 integrator which requires two gradient evaluations per iteration. Where relevant  we plot both
iterations (Figs. 1a and 1b) and gradient evaluations (Figs. 1c and 1d).

Example 7 The logistic loss

f (x) = log(1 + e−yw(cid:62)x) 

shown by Zhang et al. [36] to satisfy (21) of order p = ∞  is strongly smooth of order p = ∞.
Example 8 The GLM loss 
2 (y − φ(x(cid:62)w))2

for φ(r) = 1/(1 + e−r) 

f (x) = 1

y ∈ {0  1} 

and w ∈ Rd 

studied by Hazan et al. [11] is strongly smooth of order p = 3.
Example 9 The (cid:96)2 loss to the p-th power

f (x) = 1

p(cid:107)Ax − b(cid:107)p
2 

(23)

(24)

(25)

8

for which Hamiltonian descent [17] obtains a linear rate  is strongly smooth and gradient dominated
of order p.

Example 10 The loss function 

(26)
for which Hamiltonian descent [17] obtains a linear rate  is strongly smooth and gradient dominated
of order p = 4.

f (x) = (x(1) + x(2))4 + 1

16 (x(1) − x(2))4 

5.2 Experiments

4(cid:107)Ax − b(cid:107)4

Runge-Kutta algorithms of Zhang et al. [36] (DD) on the logistic loss f (x) =(cid:80)10

In this section  we perform a series of numerical experiments to compare the performance of ARGD
(Algorithm 1) with gradient descent (GD)  Nesterov accelerated GD (NAG)  and the state-of-the-art
i xyi) 
the (cid:96)4 loss f (x) = 1
4  and the Hamiltonian descent loss (Example 10). For the logistic and
(cid:96)4 losses  we use the same code  plots  and experimental methodology of Zhang et al. [36] (including
data and step-size choice)  adding to it (A)RGD. Speciﬁcally  for Fig. 1a-Fig. 1d  the entries of
W ∈ R10×10 and A ∈ R10×10 are i.i.d. standard Gaussian  and the ﬁrst ﬁve entries of y (and b)
are valued 0 while the rest are 1. Fig. 1e shows the performance of A(RGD)  GD  and NAG on the
Hamiltonian objective studied by [17]; for Fig. 1e  the largest step-size was chosen subject to the
algorithm not diverging. For each experiment  a simple implementation of (A)RGD signiﬁcantly
outperforms the Runge-Kutta algorithm (DD)  GD and NAG. The code for these experiments can be
found here: https://github.com/aswilson07/ARGD.git.

i=1 log(1+e−w(cid:62)

6 Additional Results and Discussion

This paper establishes broad conditions under which an algorithm will converge and its performance
can be accelerated by adding momentum. We use these conditions to introduce (accelerated) rescaled
gradient descent for strongly smooth functions  and showed it outperforms several recent ﬁrst-order
methods that have been introduced for optimizing smooth functions in machine learning.
There are (at least) two simple extensions of our framework. First  an analogous framework can
be established for (accelerated) δ-coordinate descent methods of order p. As an application  we
introduce (accelerated) rescaled coordinate descent for functions that are strongly smooth along
each coordinate direction of the gradient. We provide details in Appendix E.1. Second  with our
generalization of the Monteiro-Svaiter framework  we derive optimal univeral tensor methods
for functions whose (p − 1)-st gradients are ν-Hölder-smooth which achieve the upper bound
) where ˜p = p − 1 + ν. The matching lower bound for this class of
f (yk) − f (x∗) = O(1/k
functions was recently established by [10]. We present this result in Appendix E.3.
There are several possible directions for future work. We know that certain simple operations preserve
convexity (e.g.  addition)  but what operations preserve strong smoothness? Understanding this
could allow us to construct more complex examples of strongly smooth functions. Our results reveal
an interesting hierarchy of smoothness assumptions which lead to methods that converge quickly;
exploring this more is of signiﬁcant interest. Finally  extending our analysis to the stochastic or
manifold setting  studying the use of variance reduction techniques  and introducing other δ-decent
algorithms of order p are all interesting directions for future work.

3 ˜p−2

2

Acknowledgments

We would like to thank Jingzhao Zhang for providing us access to his code.

References
[1] Zeyuan Allen Zhu and Lorenzo Orecchia. Linear coupling: An ultimate uniﬁcation of gradient
and mirror descent. In 8th Innovations in Theoretical Computer Science Conference  ITCS 2017 
January 9-11  2017  Berkeley  CA  USA  pages 3:1–3:22  2017.

[2] Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural Computation  pages

251–276  1998.

9

[3] Michel Baes. Estimate sequence methods: Extensions and approximations  August 2009.

[4] Michael Betancourt  Michael Jordan  and Ashia Wilson. On symplectic optimization. Arxiv

preprint arXiv1802.03653  2018.

[5] Sébastien Bubeck  Qijia Jiang  Yin Tat Lee  Yuanzhi Li  and Aaron Sidford. Near-optimal
method for highly smooth convex optimization. In Proceedings of the Thirty-Second Conference
on Learning Theory  volume 99 of Proceedings of Machine Learning Research  pages 492–507 
Phoenix  USA  25–28 Jun . PMLR.

[6] Yair Carmon  John C Duchi  Oliver Hinder  and Aaron Sidford. Lower bounds for ﬁnding

stationary points ii: First-order methods. Arxiv preprint arXiv:1711.0084  2017.

[7] Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization algorithm

using Bregman functions. SIAM Journal of Optimization  3(3):538–543  1993.

[8] Jelena Diakonikolas and Lorenzo Orecchia. Accelerated extra-gradient descent: A novel
accelerated ﬁrst-order method. In 9th Innovations in Theoretical Computer Science Conference 
ITCS 2018  January 11-14  2018  Cambridge  MA  USA  pages 23:1–23:19  2018.

[9] Alexander Gasnikov  Pavel Dvurechensky  Eduard Gorbunov  Evgeniya Vorontsova  Daniil
Selikhanovych  and César A. Uribe. Optimal tensor methods in smooth convex and uniformly
convex optimization. In Proceedings of the Thirty-Second Conference on Learning Theory 
pages 1374–1391  Phoenix  USA  25–28 Jun 2019. PMLR.

[10] G.N Grapiglia and Yu. Nesterov. Tensor methods for minimizing functions with hölder continu-

ous higher-order derivatives. Arxiv preprint arXiv1904.12559  April 2019.

[11] Elad Hazan  Kﬁr Y. Levy  and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. In Advances in Neural Information Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015  December 7-12  2015  Montreal  Quebec 
Canada  pages 1594–1602  2015.

[12] B. Jiang  H. Wang  and S. Zhang. An optimal high-order tensor method for convex optimization.

Arxiv preprint arXiv:1812.06557  2018.

[13] Walid Krichene  Alexandre Bayen  and Peter L Bartlett. Accelerated mirror descent in continu-
ous and discrete time. In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Garnett 
editors  Advances in Neural Information Processing Systems 28  pages 2845–2853. Curran
Associates  Inc.  2015.

[14] Laurent Lessard  Benjamin Recht  and Andrew Packard. Analysis and design of optimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization  26(1):57–95 
2016.

[15] Hongzhou Lin  Julien Mairal  and Zaïd Harchaoui. Catalyst acceleration for ﬁrst-order convex
optimization: from theory to practice. Journal of Machine Learning Research  18:212:1–212:54 
2017.

[16] S. Łojasiewicz. A topological property of real analytic subsets (in french). In Coll. du CNRS 

Les équations aux déri´vees partielles  pages 87– 89  1963.

[17] Chris J. Maddison  Daniel Paulin  Yee Whye Teh  Brendan O’Donoghue  and Arnaud Doucet.

Hamiltonian descent methods. Arxiv preprint arXiv1809.05042  2018.

[18] Renato D. C. Monteiro and Benar Fux Svaiter. An accelerated hybrid proximal extragradient
method for convex optimization and its implications to second-order methods. SIAM Journal
on Optimization  23(2):1092–1125  2013.

[19] Jean Jacques Moreau. Proximité et dualité dans un espace Hilbertien. Bulletin de la Société

Mathématique de France  93:273–299  1965.

[20] Arkadi Nemirovskii and David Yudin. Problem Complexity and Method Efﬁciency in Optimiza-

tion. John Wiley & Sons  1983.

10

[21] Y. Nesterov. Implementable tensor methods in unconstrained convex optimization. Core discus-

sion papers  2018. URL https://ideas.repec.org/p/cor/louvco/2018005.html.

[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate

O(1/k2). Soviet Mathematics Doklady  27(2):372–376  1983.

[23] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Applied

Optimization. Kluwer  Boston  2004.

[24] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming 

103(1):127–152  2005.

[25] Yurii Nesterov. Accelerating the cubic regularization of Newton’s method on convex problems.

Mathematical Programming  112(1):159–181  2008. ISSN 0025-5610.

[26] Yurii Nesterov and Boris T. Polyak. Cubic regularization of Newton’s method and its global

performance. Mathematical Programming  108(1):177–205  2006.

[27] Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR

Computational Mathematics and Mathematical Physics  4(5):1–17  1964.

[28] J. Schropp and I. Singer. A dynamical systems approach to constrained minimization. Numerical

Functional Analysis and Optimization  21(3-4):537–551  2000.

[29] Bin Shi  Simon Du  Michael Jordan  and Weiji Su. Understanding the acceleration phenomenon
via high-resolution differential equations. Arxiv preprint arXiv1810.08907  November 2018.

[30] Weijie Su  Stephen Boyd  and Emmanuel J. Candès. A differential equation for modeling
Nesterov’s accelerated gradient method: Theory and insights. In Advances in Neural Information
Processing Systems (NIPS) 27  2014.

[31] Ganesh Sundaramoorthi and Anthony J. Yezzi. Variational PDEs for acceleration on manifolds
and application to diffeomorphisms. In Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems 2018  NeurIPS 2018  3-8
December 2018  Montréal  Canada.  pages 3797–3807  2018.

[32] Andre Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as
a composite optimization problem. In Conference On Learning Theory  COLT 2018  Stockholm 
Sweden  6-9 July 2018.  pages 2093–3027  2018.

[33] Andre Wibisono and Ashia Wilson. On accelerated methods in optimization. Arxiv preprint

arXiv1509.03616  2015.

[34] Andre Wibisono  Ashia C. Wilson  and Michael I. Jordan. A variational perspective on
accelerated methods in optimization. Proceedings of the National Academy of Sciences  113
(47):E7351–E7358  2016.

[35] Ashia Wilson  Benjamin Recht  and Michael Jordan. A Lyapunov analysis of momentum

methods in optimization. Arxiv preprint arXiv1611.02635  November 2016.

[36] Jingzhao Zhang  Aryan Mokhtari  Suvrit Sra  and Ali Jadbabaie. Direct Runge-Kutta dis-
cretization achieves acceleration.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman 
N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems
31  pages 3904–3913. Curran Associates  Inc.  2018.

11

,Ashia Wilson
Lester Mackey
Andre Wibisono