2013,Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition,In the high-dimensional regression model a response variable is linearly related to $p$ covariates   but the sample size $n$ is smaller than $p$. We assume that only a small subset of covariates is `active'  (i.e.  the corresponding coefficients are non-zero)  and consider the model-selection problem of identifying  the active covariates.  A popular approach is to estimate the regression coefficients through the Lasso ($\ell_1$-regularized least squares).  This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones  as quantified through the so called `irrepresentability' condition. In this paper we study the `Gauss-Lasso' selector  a simple two-stage method that first solves the Lasso  and then performs ordinary least squares restricted to the Lasso active set.   We formulate `generalized irrepresentability condition' (GIC)  an assumption that is substantially weaker than  irrepresentability. We prove that  under GIC  the Gauss-Lasso correctly recovers the active set.,Model Selection for High-Dimensional Regression
under the Generalized Irrepresentability Condition

Adel Javanmard
Stanford University
Stanford  CA 94305

adelj@stanford.edu

Andrea Montanari
Stanford University
Stanford  CA 94305

montanar@stanford.edu

Abstract

In the high-dimensional regression model a response variable is linearly related to
p covariates  but the sample size n is smaller than p. We assume that only a small
subset of covariates is ‘active’ (i.e.  the corresponding coefﬁcients are non-zero) 
and consider the model-selection problem of identifying the active covariates.
A popular approach is to estimate the regression coefﬁcients through the Lasso
((cid:96)1-regularized least squares). This is known to correctly identify the active set
only if the irrelevant covariates are roughly orthogonal to the relevant ones  as
quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we
study the ‘Gauss-Lasso’ selector  a simple two-stage method that ﬁrst solves the
Lasso  and then performs ordinary least squares restricted to the Lasso active set.
We formulate ‘generalized irrepresentability condition’ (GIC)  an assumption that
is substantially weaker than irrepresentability. We prove that  under GIC  the
Gauss-Lasso correctly recovers the active set.

Introduction

1
In linear regression  we wish to estimate an unknown but ﬁxed vector of parameters θ0 ∈ Rp from n
pairs (Y1  X1)  (Y2  X2)  . . .   (Yn  Xn)  with vectors Xi taking values in Rp and response variables
Yi given by

Yi = (cid:104)θ0  Xi(cid:105) + Wi  

Wi ∼ N(0  σ2)  

(1)

where (cid:104)·   ·(cid:105) is the standard scalar product.
In matrix form  letting Y = (Y1  . . .   Yn)T and denoting by X the design matrix with rows
X T

1   . . .   X T

n   we have

Y = X θ0 + W  

W ∼ N(0  σ2In×n) .

(2)

In this paper  we consider the high-dimensional setting in which the number of parameters exceeds
the sample size  i.e.  p > n  but the number of non-zero entries of θ0 is smaller than p. We denote by
S ≡ supp(θ0) ⊆ [p] the support of θ0  and let s0 ≡ |S|. We are interested in the ‘model selection’
problem  namely in the problem of identifying S from data Y   X.
In words  there exists a ‘true’ low dimensional linear model that explains the data. We want to
identify the set S of covariates that are ‘active’ within this model. This problem has motivated a
large body of research  because of its relevance to several modern data analysis tasks  ranging from
signal processing [9  5] to genomics [15  16]. A crucial step forward has been the development of
model-selection techniques based on convex optimization formulations [17  8  6]. These formula-
tions have lead to computationally efﬁcient algorithms that can be applied to large scale problems.
Such developments pose the following theoretical question: For which vectors θ0  designs X  and

1

noise levels σ  the support S can be identiﬁed  with high probability  through computationally efﬁ-
cient procedures? The same question can be asked for random designs X and  in this case  ‘high
probability’ will refer both to the noise realization W   and to the design realization X. In the rest of
this introduction we shall focus –for the sake of simplicity– on the deterministic settings  and refer
to Section 3 for a treatment of Gaussian random designs.
The analysis of computationally efﬁcient methods has largely focused on (cid:96)1-regularized least
squares  a.k.a. the Lasso [17]. The Lasso estimator is deﬁned by
(cid:107)Y − Xθ(cid:107)2

(cid:98)θn(Y  X; λ) ≡ arg min

2 + λ(cid:107)θ(cid:107)1

(cid:110) 1

(cid:111)

θ∈Rp

2n

.

(3)

In case the right hand side has more than one minimizer  one of them can be selected arbitrarily for
our purposes. It is worth noting that when columns of X are in general positions (e.g. when the
entries of X are drawn form a continuous probability distribution)  the Lasso solution is unique [18].
We will often omit the arguments Y   X  as they are clear from the context. (A closely related method
is the so-called Dantzig selector [6]: it would be interesting to explore whether our results can be
generalized to that approach.)
It was understood early on that  even in the large-sample  low-dimensional limit n → ∞ at p

constant  supp((cid:98)θn) (cid:54)= S unless the columns of X with index in S are roughly orthogonal to the
condition’  that can be stated in terms of the empirical covariance matrix (cid:98)Σ = (XTX/n). Letting
(cid:98)ΣA B be the submatrix ((cid:98)Σi j)i∈A j∈B  irrepresentability requires
S S sign(θ0 S)(cid:107)∞ ≤ 1 − η  

ones with index outside S [12]. This assumption is formalized by the so-called ‘irrepresentability

(4)
for some η > 0 (here sign(u)i = +1  0  −1 if  respectively  ui > 0  = 0  < 0). In an early break-
through  Zhao and Yu [23] proved that  if this condition holds with η uniformly bounded away from
0  it guarantees correct model selection also in the high-dimensional regime p (cid:29) n. Meinshausen
and B¨ulmann [14] independently established the same result for random Gaussian designs  with
applications to learning Gaussian graphical models. These papers applied to very sparse models  re-
quiring in particular s0 = O(nc)  c < 1  and parameter vectors with large coefﬁcients. Namely  scal-

ing the columns of X such that(cid:98)Σi i ≤ 1  for i ∈ [p]  they require θmin ≡ mini∈S |θ0 i| ≥ c(cid:112)s0/n.
broad class of empirical covariances it is only necessary that θmin ≥ cσ(cid:112)(log p)/n. This scaling

Wainwright [21] strengthened considerably these results by allowing for general scalings of s0  p  n
and proving that much smaller non-zero coefﬁcients can be detected. Namely  he proved that for a

(cid:107)(cid:98)ΣSc S(cid:98)Σ−1

√
nIn×n  one would need |θ0 i| ≥ cσ/

of the minimum non-zero entry is optimal up to constants. Also  for a speciﬁc classes of random
Gaussian designs (including X with i.i.d. standard Gaussian entries)  the analysis of [21] provides
tight bounds on the minimum sample size for correct model selection. Namely  there exists c(cid:96)  cu >
0 such that the Lasso fails with high probability if n < c(cid:96) s0 log p and succeeds with high probability
if n ≥ cu s0 log p.
While  thanks to these recent works [23  14  21]  we understand reasonably well model selection
via the Lasso  it is fundamentally unknown what model-selection performances can be achieved
√
with general computationally practical methods. Two aspects of of the above theory cannot be
improved substantially: (i) The non-zero entries must satisfy the condition θmin ≥ cσ/
n to be
detected with high probability. Even if n = p and the measurement directions Xi are orthogonal 
e.g.  X =
n to distinguish the i-th entry from noise.
For instance  in [10]  the authors prove a general upper bound on the minimax power of tests for
hypotheses H0 i = {θ0 i = 0}. Specializing this bound to the case of standard Gaussian designs  the
analysis of [10] shows formally that no test can detect θ0 i (cid:54)= 0  with a ﬁxed degree of conﬁdence 
√
n. (ii) The sample size must satisfy n ≥ s0. Indeed  if this is not the case 
unless |θ0 i| ≥ cσ/
for each θ0 with support of size |S| = s0  there is a one parameter family {θ0(t) = θ0 + t v}t∈R
with supp(θ0(t)) ⊆ S  Xθ0(t) = Xθ0 and  for speciﬁc values of t  the support of θ0(t) is strictly
contained in S.
On the other hand  there is no fundamental reason to assume the irrepresentability condition (4).
This follows from the requirement that a speciﬁc method (the Lasso) succeeds  but is unclear why
it should be necessary in general. In this paper we prove that the Gauss-Lasso selector has nearly
optimal model selection properties under a condition that is strictly weaker than irrepresentability.

√

2

(cid:98)θn(Y  X; λ) ≡ arg min

GAUSS-LASSO SELECTOR: Model selector for high dimensional problems
Input: Measurement vector y  design model X  regularization parameter λ  support size s0.

Output: Estimated support (cid:98)S.
1: Let T = supp((cid:98)θn) be the support of Lasso estimator(cid:98)θn =(cid:98)θn(y  X  λ) given by
2: Construct the estimator(cid:98)θGL as follows:
3: Find s0-th largest entry (in modulus) of(cid:98)θGL

2 + λ(cid:107)θ(cid:107)1
(cid:98)θGL
T   denoted by(cid:98)θGL
(s0)|(cid:9).
| ≥ |(cid:98)θGL

(cid:98)θGL
(cid:98)S ≡(cid:8)i ∈ [p] : |(cid:98)θGL

T = (XT

T XT )−1XT

T y  

T c = 0 .
(s0)  and let

(cid:107)Y − Xθ(cid:107)2

(cid:110) 1

θ∈Rp

2n

(cid:111)

.

i

We call this condition the generalized irrepresentability condition (GIC). The Gauss-Lasso proce-
dure uses the Lasso to estimate a ﬁrst model T ⊆ {1  . . .   p}. It then constructs a new estimator by
ordinary least squares regression of the data Y onto the model T .

We prove that the estimated model is  with high probability  correct (i.e.  (cid:98)S = S) under conditions

comparable to the ones assumed in [14  23  21]  while replacing irrepresentability by the weaker
generalized irrepresentability condition. In the case of random Gaussian designs  our analysis further
assumes the restricted eigenvalue property in order to establish a nearly optimal scaling of the sample
size n with the sparsity parameter s0.
In order to build some intuition about the difference between irrepresentability and generalized
irrepresentability  it is convenient to consider the Lasso cost function at ‘zero noise’:

G(θ; ξ) ≡ 1
2n

(cid:107)X(θ − θ0)(cid:107)2

Let(cid:98)θZN(ξ) be the minimizer of G(· ; ξ) and v ≡ limξ→0+ sign((cid:98)θZN(ξ)). The limit is well deﬁned
by Lemma 2.2 below. The KKT conditions for(cid:98)θZN imply  for T ≡ supp(v) 

1
2

(cid:104)(θ − θ0) (cid:98)Σ(θ − θ0)(cid:105) + ξ(cid:107)θ(cid:107)1 .

2 + ξ(cid:107)θ(cid:107)1 =
(cid:107)(cid:98)ΣT c T(cid:98)Σ−1

T T vT(cid:107)∞ ≤ 1 .

Since G(· ; ξ) has always at least one minimizer  this condition is always satisﬁed. Generalized
irrepresentability requires that the above inequality holds with some small slack η > 0 bounded
away from zero  i.e. 

(cid:107)(cid:98)ΣT c T(cid:98)Σ−1

T T vT(cid:107)∞ ≤ 1 − η .

Notice that this assumption reduces to standard irrepresentability cf. Eq. (4) if  in addition  we ask
that v = sign(θ0). In other words  earlier work [14  23  21] required generalized irrepresentability
plus sign-consistency in zero noise  and established sign consistency in non-zero noise. In this paper
the former condition is shown to be sufﬁcient.
From a different point of view  GIC demands that irrepresentability holds for a superset of the true
support S. It was indeed argued in the literature that such a relaxation of irrepresentability allows to
cover a signiﬁcantly broader set of cases (see for instance [3  Section 7.7.6]). However  it was never
clariﬁed why such a superset irrepresentability condition should be signiﬁcantly more general than
simple irrepresentability. Further  no precise prescription existed for the superset of the true support.
Our contributions can therefore be summarized as follows:

generalized irrepresentability should hold for a broad class of design matrices.

• By tying it to the KKT condition for the zero-noise problem  we justify the expectation that
• We thus provide a speciﬁc formulation of superset irrepresentability  prescribing both the
superset T and the sign vector vT   that is  by itself  signiﬁcantly more general than simple
irrepresentability.

3

• We show that  under GIC  exact support recovery can be guaranteed using the Gauss-Lasso 
and formulate the appropriate ‘minimum coefﬁcient’ conditions that guarantee this. As a
side remark  even when simple irrepresentability holds  our results strengthen somewhat
the estimates of [21] (see below for details).

The paper is organized as follows. In the rest of the introduction we illustrate the range of applicabil-
ity of GIC through a simple example and we discuss further related work. We ﬁnally introduce the
basic notations to be used throughout the paper. Section 2 treats the case of deterministic designs X 
and develops our main results on the basis of the GIC. Section 3 extends our analysis to the case of
random designs. In this case GIC is required to hold for the population covariance  and the analysis
is more technical as it requires to control the randomness of the design matrix. We refer the reader
to the long version of the paper [11] for the proofs of our main results and the technical steps.

1.1 An example

In order to illustrate the range of new cases covered by our results  it is instructive to consider a
simple example. A detailed discussion of this calculation can be found in [11]. The example corre-
sponds to a Gaussian random design  i.e.  the rows X T
n are i.i.d. realizations of a p-variate
normal distribution with mean zero. We write Xi = (Xi 1  Xi 2  . . .   Xi p)T for the components of
Xi. The response variable is linearly related to the ﬁrst s0 covariates:

1   . . . X T

Yi = θ0 1Xi 1 + θ0 2Xi 2 + ··· + θ0 s0 Xi s0 + Wi  

where Wi ∼ N(0  σ2) and we assume θ0 i > 0 for all i ≤ s0. In particular S = {1  . . .   s0}.
As for the design matrix  ﬁrst p − 1 covariates are orthogonal at the population level  i.e.  Xi j ∼
N(0  1) are independent for 1 ≤ j ≤ p− 1 (and 1 ≤ i ≤ n). However the p-th covariate is correlated
to the s0 relevant ones:

Xi p = a Xi 1 + a Xi 2 + ··· + a Xi s0 + b ˜Xi p .

Here ˜Xi p ∼ N(0  1) is independent from {Xi 1  . . .   Xi p−1} and represents the orthogonal com-
ponent of the p-th covariate. We choose the coefﬁcients a  b ≥ 0 such that s0a2 + b2 = 1  whence
E{X 2
i p} = 1 and hence the p-th covariate is normalized as the ﬁrst (p − 1) ones. In other words 
the rows of X are i.i.d. Gaussian Xi ∼ N(0  Σ) with covariance given by

1

Σij =

if i = j 

a if i = p  j ∈ S or i ∈ S  j = p 
0

otherwise.

the support S from n ≥ c s0 log p samples  provided θmin ≥ c(cid:48)(cid:112)(log p)/n. It follows from [21] that

For a = 0  this is the standard i.i.d. design and irrepresentability holds. The Lasso correctly recovers
this remains true as long as a ≤ (1− η)/s0 for some η > 0 bounded away from 0. However  as soon
as a > 1/s0  the Lasso includes the p-th covariate in the estimated model  with high probability.
On the other hand  Gauss-Lasso is successful for a signiﬁcantly larger set of values of a. Namely  if

a ∈

then it recovers S from n ≥ c s0 log p samples  provided θmin ≥ c(cid:48)(cid:112)(log p)/n. While the interval

((1− η)/s0  1/s0] is not covered by this result  we expect this to be due to the proof technique rather
than to an intrinsic limitation of the Gauss-Lasso selector.

∪

s0

0 

 

(cid:20)

(cid:21)

(cid:18) 1

1 − η
s0

(cid:21)

1 − η√

s0

 

1.2 Further related work

The restricted isometry property [7  6] (or the related restricted eigenvalue [2] or compatibility con-
ditions [19]) have been used to establish guarantees on the estimation and model selection errors of
the Lasso or similar approaches. In particular  Bickel  Ritov and Tsybakov [2] show that  under such
conditions  with high probability 

(cid:107)(cid:98)θ − θ0(cid:107)2

2 ≤ Cσ2 s0 log p

.

n

4

The same conditions can be used to prove model-selection guarantees.
In particular  Zhou [24]
studies a multi-step thresholding procedure whose ﬁrst steps coincide with the Gauss-Lasso. While
the main objective of this work is to prove high-dimensional (cid:96)2 consistency with a sparse estimated
model  the author also proves partial model selection guarantees. Namely  the method correctly

recovers a subset of large coefﬁcients SL ⊆ S  provided |θ0 i| ≥ cσ(cid:112)s0(log p)/n  for i ∈ SL. This

means that the coefﬁcients that are guaranteed to be detected must be a factor
is required by our results.
An alternative approach to establishing model-selection guarantees assumes a suitable mutual
incoherence conditions. Lounici [13] proves correct model selection under the assumption

maxi(cid:54)=j |(cid:98)Σij| = O(1/s0). This assumption is however stronger than irrepresentability [19]. Cand´es
maxi(cid:54)=j |(cid:98)Σij| = O(1/(log p)). Under this condition  they establish model selection guarantees
for an ideal scaling of the non-zero coefﬁcients θmin ≥ cσ(cid:112)(log p)/n. However  this result only

and Plan [4] also assume mutual incoherence  albeit with a much weaker requirement  namely

√

s0 larger than what

holds with high probability for a ‘random signal model’ in which the non-zero coefﬁcients θ0 i have
uniformly random signs.
The authors in [22] consider the variable selection problem  and under the same assumptions on
the non-zero coefﬁcients as in the present paper  guarantee support recovery under a cone condition.
The latter condition however is stronger than the generalized irrepresentability condition. In partic-
ular  for the example in Section 1.1 it yields no improvement over the standard irrepresentability.
The work [20] studies the adaptive and the thresholded Lasso estimators and proves correct model
selection assuming the non-zero coefﬁcients are of order s0
Finally  model selection consistency can be obtained without irrepresentability through other meth-
ods. For instance [25] develops the adaptive Lasso  using a data-dependent weighted (cid:96)1 regular-
ization  and [1] proposes the Bolasso  a resampling-based techniques. Unfortunately  both of these
approaches are only guaranteed to succeed in the low-dimensional regime of p ﬁxed  and n → ∞.

(cid:112)(log p)/n.

1.3 Notations

I I represents the inverse of AI I  i.e.  A−1

We provide a brief summary of the notations used throughout the paper. For a matrix A and set of
indices I  J  we let AJ denote the submatrix containing just the columns in J and AI J denote the
submatrix formed by the rows in I and columns in J. Likewise  for a vector v  vI is the restriction
of v to indices in I. Further  the notation A−1
I I = (AI I )−1.
The maximum and the minimum singular values of A are respectively denoted by σmax(A) and
σmin(A). We write (cid:107)v(cid:107)p for the standard (cid:96)p norm of a vector v. Speciﬁcally  (cid:107)v(cid:107)0 denotes the
number of nonzero entries in v. Also  (cid:107)A(cid:107)p refers to the induced operator norm on a matrix A. We
use ei to refer to the i-th standard basis element  e.g.  e1 = (1  0  . . .   0). For a vector v  supp(v)
represents the positions of nonzero entries of v. Throughout  we denote the rows of the design matrix
X by X1  . . .   Xn ∈ Rp and denote its columns by x1  . . .   xp ∈ Rn. Further  for a vector v  sign(v)
is the vector with entries sign(v)i = +1 if vi > 0  sign(v)i = −1 if vi < 0  and sign(v)i = 0
otherwise.

2 Deterministic designs

An outline of this section is as follows: (1) We ﬁrst consider the zero-noise problem W = 0 
and prove several useful properties of the Lasso estimator in this case.
In particular  we show
that there exists a threshold for the regularization parameter below which the support of the Lasso
estimator remains the same and contains supp(θ0). Moreover  the Lasso estimator support is not
much larger than supp(θ0). (2) We then turn to the noisy problem  and introduce the generalized
irrepresentability condition (GIC) that is motivated by the properties of the Lasso in the zero-noise
case. We prove that under GIC (and other technical conditions)  with high probability  the signed
support of the Lasso estimator is the same as that in the zero-noise problem. (3) We show that the
Gauss-Lasso selector correctly recovers the signed support of θ0.

5

2.1 Zero-noise problem

Recall that(cid:98)Σ ≡ (XTX/n) denotes the empirical covariance of the rows of the design matrix. Given
(cid:98)Σ ∈ Rp×p (cid:98)Σ (cid:23) 0  θ0 ∈ Rp and ξ ∈ R+  we deﬁne the zero-noise Lasso estimator as
(cid:111)
Note that(cid:98)θZN(ξ) is obtained by letting Y = Xθ0 in the deﬁnition of(cid:98)θn(Y  X; ξ).
Following [2]  we introduce a restricted eigenvalue constant for the empirical covariance matrix(cid:98)Σ:

(cid:104)(θ − θ0) (cid:98)Σ(θ − θ0)(cid:105) + ξ(cid:107)θ(cid:107)1

(cid:98)θZN(ξ) ≡ arg min

(cid:110) 1

θ∈Rp

(5)

2n

.

(cid:98)κ(s  c0) ≡ min

J⊆[p]
|J|≤s

min
u∈Rp

(cid:104)u (cid:98)Σu(cid:105)

(cid:107)u(cid:107)2

2

.

(6)

(7)

T0 T0

s0 .

1 +

(cid:33)

(cid:32)

(cid:107)uJc(cid:107)1≤c0(cid:107)uJ(cid:107)1

v0 T0]i|.

(cid:107)(cid:98)θZN(cid:107)0 ≤

4(cid:107)(cid:98)Σ(cid:107)2
(cid:98)κ(s0  1)

Our ﬁrst result states that supp((cid:98)θZN(ξ)) is not much larger than the support of θ0  for any ξ > 0.
Lemma 2.1. Let(cid:98)θZN =(cid:98)θZN(ξ) be deﬁned as per Eq. (5)  with ξ > 0. Then  if s0 = (cid:107)θ0(cid:107)0 
Lemma 2.2. Let (cid:98)θZN = (cid:98)θZN(ξ) be deﬁned as per Eq. (5)  with ξ > 0. Then there exist ξ0 =
ξ0((cid:98)Σ  S  θ0) > 0  T0 ⊆ [p]  v0 ∈ {−1  0  +1}p  such that the following happens. For all ξ ∈ (0  ξ0) 
sign((cid:98)θZN(ξ)) = v0 and supp((cid:98)θZN(ξ)) = supp(v0) = T0. Further T0 ⊇ S  v0 S = sign(θ0 S) and
ξ0 = mini∈S |θ0 i/[(cid:98)Σ−1
Lemma 2.3. Let (cid:98)θZN = (cid:98)θZN(ξ) be deﬁned as per Eq. (5)  with ξ > 0. Let T ⊇ S and v ∈
{+1  0 −1}p be such that supp(v) = T . Then sign((cid:98)θZN) = v if and only if

(cid:13)(cid:13)(cid:13)(cid:98)ΣT c T(cid:98)Σ−1
(cid:13)(cid:13)(cid:13)∞
(cid:16)
θ0 T − ξ(cid:98)Σ−1
Further  if the above holds (cid:98)θZN is given by(cid:98)θZN
T c = 0 and(cid:98)θZN
Generalized irrepresentability (deterministic designs). The pair ((cid:98)Σ  θ0)  (cid:98)Σ ∈ Rp×p  θ0 ∈ Rp

Motivated by this result  we introduce the generalized irrepresentability condition (GIC) for deter-
ministic designs.

(cid:17)
T = θ0 T − ξ(cid:98)Σ−1

Finally we have the following standard characterization of the solution of the zero-noise problem.

satisfy the generalized irrepresentability condition with parameter η > 0 if the following happens.
Let v0  T0 be deﬁned as per Lemma 2.2. Then

vT = sign

T T vT .

≤ 1  

T T vT

T T vT

(8)

(9)

.

(10)
In other words we require the dual feasibility condition (8)  which always holds  to hold with a
positive slack η.

v0 T0

T0 T0

(cid:13)(cid:13)(cid:13)(cid:98)ΣT c
0  T0(cid:98)Σ−1

(cid:13)(cid:13)(cid:13)∞

≤ 1 − η .

2.2 Noisy problem

Consider the noisy linear observation model as described in (2)  and let(cid:98)r ≡ (XTW/n). We begin
with a standard characterization of sign((cid:98)θn)  the signed support of the Lasso estimator (3).
Lemma 2.4. Let (cid:98)θn = (cid:98)θn(y  X; λ) be deﬁned as per Eq. (3)  and let z ∈ {+1  0 −1}p with
sign((cid:98)θn) = z if and only if(cid:13)(cid:13)(cid:13)(cid:98)ΣT c T(cid:98)Σ−1

supp(z) = T . Further assume T ⊇ S. Then the signed support of the Lasso estimator is given by

T T zT +

≤ 1  

(11)

(cid:13)(cid:13)(cid:13)∞
((cid:98)rT c −(cid:98)ΣT c T(cid:98)Σ−1
T T(cid:98)rT )
(cid:16)
(cid:17)
θ0 T −(cid:98)Σ−1
T T (λzT −(cid:98)rT )

1
λ

.

zT = sign

(12)

6

and suppose that for some c2 > 0:

v0 T0]i

T0 T0
v0 T0]i

T0 T0

√

for all i ∈ S 
for all i ∈ T0 \ S.

We further assume  without loss of generality  η ≤ c2

|θ0 i| ≥ c2λ + λ(cid:12)(cid:12)[(cid:98)Σ−1

satisﬁes the generalized irrepresentability condition with parameter η. Consider the Lasso estimator

Theorem 2.5. Consider the deterministic design model with empirical covariance matrix (cid:98)Σ ≡
(XTX)/n  and assume (cid:98)Σi i ≤ 1 for i ∈ [p]. Let T0 ⊆ [p]  v0 ∈ {+1  0 −1}p be the set and
vector deﬁned in Lemma 2.2. Assume that (i) σmin((cid:98)ΣT0 T0 ) ≥ Cmin > 0. (ii) The pair ((cid:98)Σ  θ0)
(cid:98)θn = (cid:98)θn(y  X; λ) deﬁned as per Eq. (3)  with λ = (σ/η)(cid:112)2c1 log p/n  for some constant c1 > 1 

(cid:12)(cid:12)
(cid:12)(cid:12)[(cid:98)Σ−1
(cid:12)(cid:12) ≥ c2
(cid:111) ≥ 1 − 4p1−c1 .
P(cid:110)
sign((cid:98)θn(λ)) = v0
that the required lower bound for |θ0 i|  i ∈ S  does not depend on (cid:107)(cid:98)Σ−1
Remark 2.6. Condition (i) in Theorem 2.5 requires the submatrix(cid:98)ΣT0 T0 to have minimum singular
value bounded away form zero. Assuming (cid:98)ΣS S to be non-singular is necessary for identiﬁability.
Requiring the minimum singular value of (cid:98)ΣT0 T0 to be bounded away from zero is not much more
Theorem 2.7. Consider the deterministic design model with empirical covariance matrix (cid:98)Σ ≡
(XTX)/n  and assume that(cid:98)Σi i ≤ 1 for i ∈ [p]. Under the assumptions of Theorem 2.5 
In particular  if (cid:98)S is the model selected by the Gauss-Lasso  then P((cid:98)S = S) ≥ 1 − 6 p1−c1/4.

(cid:17) ≤ 4p1−c1 + 2pe−nCminµ2/2σ2

Note that even under standard irrepresentability  this result improves over [21  Theorem 1.(b)]  in

We next show that the Gauss-Lasso selector correctly recovers the support of θ0.

P(cid:16)(cid:107)(cid:98)θGL − θ0(cid:107)∞ ≥ µ

restrictive since T0 is comparable in size with S  as stated in Lemma 2.1.

Cmin. Then the following holds true:

(13)
(14)

(15)

S S(cid:107)∞.

3 Random Gaussian designs

.

(cid:111)

In the previous section  we studied the case of deterministic design models which allowed for a
straightforward analysis. Here  we consider the random design model which needs a more involved
analysis. Within the random Gaussian design model  the rows Xi are distributed as Xi ∼ N(0  Σ)
for some (unknown) covariance matrix Σ (cid:31) 0. In order to study the performance of Gauss-Lasso
selector in this case  we ﬁrst deﬁne the population-level estimator. Given Σ ∈ Rp×p  Σ (cid:31) 0 

θ0 ∈ Rp and ξ ∈ R+  the population-level estimator(cid:98)θ∞(ξ) =(cid:98)θ∞(ξ; θ0  Σ) is deﬁned as

(cid:104)(θ − θ0)  Σ(θ − θ0)(cid:105) + ξ(cid:107)θ(cid:107)1

.

(16)

(cid:98)θ∞(ξ) ≡ arg min

θ∈Rp

(cid:110) 1

2

X is a random design. We show that under some conditions on the covariance Σ and vector θ0 

In fact  the population-level estimator is obtained by assuming that the response vector Y is noiseless
and n = ∞  hence replacing the empirical covariance (XTX/n) with the exact covariance Σ in the

lasso optimization problem (3). Note that the population-level estimator(cid:98)θ∞ is deterministic  albeit
T ≡ supp((cid:98)θn) = supp((cid:98)θ∞)  i.e.  the population-level estimator and the Lasso estimator share
the same (signed) support. Further T ⊇ S. Since (cid:98)θ∞ (and hence T ) is deterministic  XT is a
simple analysis of the Gauss-Lasso selector(cid:98)θGL.
(cid:98)θ∞(ξ) has the similar properties to(cid:98)θZN(ξ) stated in Section 2.1. In particular  there exists a threshold
ξ0  such that for all ξ ∈ (0  ξ0)  supp((cid:98)θ∞(ξ)) remains the same and contains supp(θ0). Moreover 
supp((cid:98)θ∞(ξ)) is not much larger than supp(θ0). (2) We show that under GIC for covariance matrix

Gaussian matrix with rows drawn independently from N(0  ΣT T ). This observation allows for a

An outline of the section is as follows: (1) We begin with noting that the population-level estimator

Σ (and other sufﬁcient conditions)  with high probability  the signed support of the Lasso estimator
is the same as the signed support of the population-level estimator. (3) Following the previous steps 
we show that the Gauss-Lasso selector correctly recovers the signed support of θ0.

7

3.1 The n = ∞ problem

3.2 The high-dimensional problem

Comparing Eqs. (5) and (16)  the estimators(cid:98)θZN(ξ) and(cid:98)θ∞(ξ) are deﬁned in a very similar manner
(the former is deﬁned with respect to(cid:98)Σ and the latter is deﬁned with respect to Σ). It is easy to see
that(cid:98)θ∞ satisﬁes the properties stated in Section 2.1 once we replace(cid:98)Σ with Σ.
We now consider the Lasso estimator (3). Recall the notations(cid:98)Σ ≡ (XTX)/n and(cid:98)r ≡ (XTW )/n.
Note that(cid:98)Σ ∈ Rp×p (cid:98)r ∈ Rp are both random quantities in the case of random designs.
Theorem 3.1. Consider the Gaussian random design model with covariance matrix Σ (cid:31) 0 
tic set and vector deﬁned in Lemma 2.2 (replacing (cid:98)Σ with Σ)  and t0 ≡ |T0|. Assume that (i)
and assume that Σi i ≤ 1 for i ∈ [p]. Let T0 ⊆ [p]  v0 ∈ {+1  0 −1}p be the determinis-
tion with parameter η. Consider the Lasso estimator(cid:98)θn =(cid:98)θn(y  X; λ) deﬁned as per Eq. (3)  with
σmin(ΣT0 T0) ≥ Cmin > 0. (ii) The pair (Σ  θ0) satisﬁes the generalized irrepresentability condi-
λ = (4σ/η)(cid:112)c1 log p/n  for some constant c1 > 1  and suppose that for some c2 > 0:

We further assume  without loss of generality  η ≤ c2
M1 ≡ (74c1)/(η2Cmin)  and M2 ≡ c1(32/(c2Cmin))2   then the following holds true:

√

(18)
Cmin. If n ≥ max(M1  M2)t0 log p with

3
2

T0 T0

v0 T0]i

|θ0 i| ≥ c2λ +

λ(cid:12)(cid:12)[Σ−1
(cid:12)(cid:12)[Σ−1
P(cid:110)
sign((cid:98)θn(λ)) = v0

(cid:12)(cid:12)
(cid:12)(cid:12) ≥ 2c2
(cid:111) ≥ 1 − pe− n

v0 T0]i

T0 T0

for all i ∈ S 
for all i ∈ T0 \ S.

10 − 6e− t0

2 − 8p1−c1 .

Note that even under standard irrepresentability  this result improves over [21  Theorem 3.(ii)]  in
that the required lower bound for |θ0 i|  i ∈ S  does not depend on (cid:107)Σ
Remark 3.2. Condition (i) follows readily from the restricted eigenvalue constraint 
i.e. 
κ∞(t0  0) > 0. This is a reasonable assumption since T0 is not much larger than S0  as stated

−1/2
S S (cid:107)∞.
in Lemma 2.1 (replacing(cid:98)Σ with Σ). Namely  s0 ≤ t0 ≤ (1 + 4(cid:107)Σ(cid:107)2/κ(s0  1))s0.

Below  we show that the Gauss-Lasso selector correctly recovers the signed support of θ0.
Theorem 3.3. Consider the random Gaussian design model with covariance matrix Σ (cid:31) 0 
and assume that Σi i ≤ 1 for i ∈ [p]. Under the assumptions of Theorem 3.1  and for
n ≥ max(M1  M2)t0 log p  we have

10 + 6e− s0

2 + 8p1−c1 + 2pe−nCminµ2/2σ2

.

(17)

(19)

(20)

P(cid:16)(cid:107)(cid:98)θGL − θ0(cid:107)∞ ≥ µ

(cid:17) ≤ pe− n
P((cid:98)S = S) ≥ 1 − p e− n

Moreover  letting ˆS be the model returned by the Gauss-Lasso selector  we have

10 − 6 e− s0

2 − 10 p1−c1 .

Remark 3.4. [Detection level] Let θmin ≡ mini∈S |θ0 i| be the minimum magnitude of the non-
zero entries of vector θ0. By Theorem 3.3  Gauss-Lasso selector correctly recovers supp(θ0)  with
probability greater than 1 − p e− n

2 − 10 p1−c1  if n ≥ max(M1  M2)t0 log p  and

10 − 6 e− s0

(cid:114)

(cid:0)1 + (cid:107)Σ−1

T0 T0

(cid:107)∞(cid:1)  

θmin ≥ Cσ

log p

n

for some constant C. Note that Eq. (20) follows from Eqs. (17) and (18).

8

References
[1] F. R. Bach. Bolasso: model consistent lasso estimation through the bootstrap. In Proceedings of the 25th

international conference on Machine learning  pages 33–40. ACM  2008.

[2] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Amer. J.

of Mathematics  37:1705–1732  2009.

[3] P. B¨uhlmann and S. van de Geer. Statistics for high-dimensional data. Springer-Verlag  2011.
[4] E. Cand`es and Y. Plan. Near-ideal model selection by (cid:96)1 minimization. The Annals of Statistics 

37(5A):2145–2177  2009.

[5] E. Candes  J. K. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruction from

highly incomplete frequency information. IEEE Trans. on Inform. Theory  52:489 – 509  2006.

[6] E. Cand´es and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Annals

of Statistics  35:2313–2351  2007.

[7] E. J. Cand´es and T. Tao. Decoding by linear programming. IEEE Trans. on Inform. Theory  51:4203–

4215  2005.

[8] S. Chen and D. Donoho. Examples of basis pursuit. In Proceedings of Wavelet Applications in Signal and

Image Processing III  San Diego  CA  1995.

[9] D. L. Donoho. Compressed sensing. IEEE Trans. on Inform. Theory  52:489–509  April 2006.
[10] A. Javanmard and A. Montanari. Hypothesis testing in high-dimensional regression under the gaussian

random design model: Asymptotic theory. arXiv preprint arXiv:1301.4240  2013.

[11] A. Javanmard and A. Montanari. Model selection for high-dimensional regression under the generalized

irrepresentability condition. arXiv:1305.0355  2013.

[12] K. Knight and W. Fu. Asymptotics for lasso-type estimators. Annals of Statistics  pages 1356–1378 

2000.

[13] K. Lounici. Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators.

Electronic Journal of statistics  2:90–102  2008.

[14] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the lasso.

Ann. Statist.  34:1436–1462  2006.

[15] J. Peng  J. Zhu  A. Bergamaschi  W. Han  D.-Y. Noh  J. R. Pollack  and P. Wang. Regularized multivari-
ate regression for identifying master predictors with application to integrative genomics study of breast
cancer. The Annals of Applied Statistics  4(1):53–77  2010.

[16] S. K. Shevade and S. S. Keerthi. A simple and efﬁcient algorithm for gene selection using sparse logistic

regression. Bioinformatics  19(17):2246–2253  2003.

[17] R. Tibshirani. Regression shrinkage and selection with the Lasso. J. Royal. Statist. Soc B  58:267–288 

1996.

[18] R. J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics  7:1456–1490  2013.
[19] S. van de Geer and P. B¨uhlmann. On the conditions used to prove oracle results for the lasso. Electron. J.

Statist.  3:1360–1392  2009.

[20] S. van de Geer  P. B¨uhlmann  and S. Zhou. The adaptive and the thresholded Lasso for potentially

misspeciﬁed models (and a lower bound for the Lasso). Electron. J. Stat.  5:688–749  2011.

[21] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1-constrained

quadratic programming. IEEE Trans. on Inform. Theory  55:2183–2202  2009.

[22] F. Ye and C.-H. Zhang. Rate minimaxity of the lasso and dantzig selector for the lq loss in lr balls.

Journal of Machine Learning Research  11:3519–3540  2010.

[23] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research 

7:2541–2563  2006.

[24] S. Zhou.

Thresholded Lasso for high dimensional variable selection and statistical estimation.

arXiv:1002.1583v2  2010.

[25] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association 

101(476):1418–1429  2006.

9

,Adel Javanmard
Andrea Montanari