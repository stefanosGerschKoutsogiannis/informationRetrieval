2016,Consistent Kernel Mean Estimation for Functions of Random Variables,We provide a theoretical foundation for non-parametric estimation of functions of random variables using kernel mean embeddings. We show that for any continuous function f  consistent estimators of the mean embedding of a random variable X lead to consistent estimators of the mean embedding of f(X). For Matern kernels and sufficiently smooth functions we also provide rates of convergence.  Our results extend to functions of multiple random variables. If the variables are dependent  we require an estimator of the mean embedding of their joint distribution as a starting point; if they are independent  it is sufficient to have separate estimators of the mean embeddings of their marginal distributions. In either case  our results cover both mean embeddings based on i.i.d. samples as well as "reduced set" expansions in terms of dependent expansion points. The latter serves as a justification for using such expansions to limit memory resources when applying the approach as a basis for probabilistic programming.,Consistent Kernel Mean Estimation
for Functions of Random Variables

Carl-Johann Simon-Gabriel⇤  Adam ´Scibior⇤ †  Ilya Tolstikhin  Bernhard Schölkopf

Department of Empirical Inference  Max Planck Institute for Intelligent Systems

Spemanstraße 38  72076 Tübingen  Germany

⇤ joint ﬁrst authors; † also with: Engineering Department  Cambridge University

cjsimon@  adam.scibior@  ilya@  bs@tuebingen.mpg.de

Abstract

We provide a theoretical foundation for non-parametric estimation of functions of
random variables using kernel mean embeddings. We show that for any continuous
function f  consistent estimators of the mean embedding of a random variable X
lead to consistent estimators of the mean embedding of f (X). For Matérn kernels
and sufﬁciently smooth functions we also provide rates of convergence.
Our results extend to functions of multiple random variables. If the variables
are dependent  we require an estimator of the mean embedding of their joint
distribution as a starting point; if they are independent  it is sufﬁcient to have
separate estimators of the mean embeddings of their marginal distributions. In
either case  our results cover both mean embeddings based on i.i.d. samples as well
as “reduced set” expansions in terms of dependent expansion points. The latter
serves as a justiﬁcation for using such expansions to limit memory resources when
applying the approach as a basis for probabilistic programming.

1

Introduction

A common task in probabilistic modelling is to compute the distribution of f (X)  given a measurable
function f and a random variable X. In fact  the earliest instances of this problem date back at least
to Poisson (1837). Sometimes this can be done analytically. For example  if f is linear and X is
Gaussian  that is f (x) = ax + b and X ⇠N (µ; )  we have f (X) ⇠N (aµ + b; a). There exist
various methods for obtaining such analytical expressions (Mathai  1973)  but outside a small subset
of distributions and functions the formulae are either not available or too complicated to be practical.
An alternative to the analytical approach is numerical approximation  ideally implemented as a
ﬂexible software library. The need for such tools is recognised in the general programming languages
community (McKinley  2016)  but no standards were established so far. The main challenge is in
ﬁnding a good approximate representation for random variables.
Distributions on integers  for example  are usually represented as lists of (xi  p(xi)) pairs. For real
valued distributions  integral transforms (Springer  1979)  mixtures of Gaussians (Milios  2009)  La-
guerre polynomials (Williamson  1989)  and Chebyshev polynomials (Korze´n and Jaroszewicz  2014)
were proposed as convenient representations for numerical computation. For strings  probabilistic
ﬁnite automata are often used. All those approaches have their merits  but they only work with a
speciﬁc input type.
There is an alternative  based on Monte Carlo sampling (Kalos and Whitlock  2008)  which is to
i=1 (with wi  0). This representation has
represent X by a (possibly weighted) sample {(xi  wi)}n
several advantages: (i) it works for any input type  (ii) the sample size controls the time-accuracy
trade-off  and (iii) applying functions to random variables reduces to applying the functions pointwise

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

i=1 and {(x0i  w0i)}n

to the sample  i.e.  {(f (xi)  wi)} represents f (X). Furthermore  expectations of functions of random
variables can be estimated as E [f (X)] ⇡Pi wif (xi)/Pi wi  sometimes with guarantees for the
convergence rate.
The ﬂexibility of this Monte Carlo approach comes at a cost: without further assumptions on the
underlying input space X   it is hard to quantify the accuracy of this representation. For instance 
i=1  how can we tell which one is a
given two samples of the same size  {(xi  wi)}n
better representation of X? More generally  how could we optimize a representation with predeﬁned
sample size?
There exists an alternative to the Monte Carlo approach  called Kernel Mean Embeddings (KME)
(Berlinet and Thomas-Agnan  2004; Smola et al.  2007). It also represents random variables as
samples  but additionally deﬁnes a notion of similarity between sample points. As a result  (i) it
keeps all the advantages of the Monte Carlo scheme  (ii) it includes the Monte Carlo method as
a special case  (iii) it overcomes its pitfalls described above  and (iv) it can be tailored to focus
on different properties of X  depending on the user’s needs and prior assumptions. The KME
approach identiﬁes both sample points and distributions with functions in an abstract Hilbert space.
Internally the latter are still represented as weighted samples  but the weights can be negative and
the straightforward Monte Carlo interpretation is no longer valid. Schölkopf et al. (2015) propose
using KMEs as approximate representation of random variables for the purpose of computing their
functions. However  they only provide theoretical justiﬁcation for it in rather idealised settings  which
do not meet practical implementation requirements.
In this paper  we build on this work and provide general theoretical guarantees for the proposed esti-
i=1 provides a good estimate for
mators. Speciﬁcally  we prove statements of the form “if {(xi  wi)}n
the KME of X  then {(f (xi)  wi)}n
i=1 provides a good estimate for the KME of f (X)”. Importantly 
our results do not assume joint independence of the observations xi (and weights wi). This makes
them a powerful tool. For instance  imagine we are given data {(xi  wi)}n
i=1 from a random variable
X that we need to compress. Then our theorems guarantee that  whatever compression algorithm we
use  as long as the compressed representation {(x0j  w0j)}n
j=1 still provides a good estimate for the
j=1 provide good estimates of the KME of f (X).
KME of X  the pointwise images {(f (x0j)  w0j)}n
In the remainder of this section we ﬁrst introduce KMEs and discuss their merits. Then we explain
why and how we extend the results of Schölkopf et al. (2015). Section 2 contains our main results. In
Section 2.1 we show consistency of the relevant estimator in a general setting  and in Section 2.2 we
provide ﬁnite sample guarantees when Matérn kernels are used. In Section 3 we show how our results
apply to functions of multiple variables  both interdependent and independent. Section 4 concludes
with a discussion.

1.1 Background on kernel mean embeddings
Let X be a measurable input space. We use a positive deﬁnite bounded and measurable kernel
k : X⇥X! R to represent random variables X ⇠ P and weighted samples ˆX := {(xi  wi)}n
X in the corresponding Reproducing Kernel Hilbert Space (RKHS) Hk by
as two functions µk
deﬁning
X :=Z k(x  .) dP (x)

X and ˆµk

wik(xi  .) .

These are guaranteed to exist  since we assume the kernel is bounded (Smola et al.  2007). When
clear from the context  we omit the kernel k in the superscript. µX is called the KME of P   but we
also refer to it as the KME of X. In this paper we focus on computing functions of random variables.
For f : X!Z   where Z is a measurable space  and for a positive deﬁnite bounded kz : Z⇥Z! R
we also write

µk

and

ˆµk

X :=Xi

i=1

µkz

f (X) :=Z kz(f (x)  .) dP (x)

and

ˆµkz

f (X) :=Xi

wikz(f (xi)  .) .

(1)

The advantage of mapping random variables X and samples ˆX to functions in the RKHS is that
we may now say that ˆX is a good approximation for X if the RKHS distance kˆµX  µXk is
small. This distance depends on the choice of the kernel and different kernels emphasise different
information about X. For example if on X := [a  b] ⇢ R we choose k(x  x0) := x · x0 + 1  then

2

µX(x) = EX⇠P [X] x + 1. Thus any two distributions and/or samples with equal means are mapped
to the same function in Hk so the distance between them is zero. Therefore using this particular k 
we keep track only of the mean of the distributions. If instead we prefer to keep track of all ﬁrst
p moments  we may use the kernel k(x  x0) := (x · x0 + 1)p. And if we do not want to loose any
information at all  we should choose k such that µk is injective over all probability measures on X .
Such kernels are called characteristic. For standard spaces  such as X = Rd  many widely used
kernels were proven characteristic  such as Gaussian  Laplacian  and Matérn kernels (Sriperumbudur
et al.  2010  2011).
The Gaussian kernel k(x  x0) := ekxx0k2
22 may serve as another good illustration of the ﬂexibility
of this representation. Whatever positive bandwidth 2 > 0  we do not lose any information about
distributions  because k is characteristic. Nevertheless  if 2 grows  all distributions start looking the
same  because their embeddings converge to a constant function 1. If  on the other hand  2 becomes
small  distributions look increasingly different and ˆµX becomes a function with bumps of height wi
at every xi. In the limit when 2 goes to zero  each point is only similar to itself  so ˆµX reduces to
the Monte Carlo method. Choosing 2 can be interpreted as controlling the degree of smoothing in
the approximation.

1.2 Reduced set methods

:= {(x0j  1/N )}N

j=1 then the objective is to construct ˆX := {(xi  wi)}n

An attractive feature when using KME estimators is the ability to reduce the number of ex-
pansion points (i.e.  the size of the weighted sample) in a principled way. Speciﬁcally  if
ˆX0
i=1 that minimises
kˆµX0  ˆµXk with n < N. Often the resulting xi are mutually dependent and the wi certainly
depend on them. The algorithms for constructing such expansions are known as reduced set methods
and have been studied by the machine learning community (Schölkopf and Smola  2002  Chapter 18).
Although reduced set methods provide signiﬁcant efﬁciency gains  their application raises certain
concerns when it comes to computing functions of random variables. Let P  Q be distributions of X
NPj k(f (x0j)  .)
and f (X) respectively. If x0j ⇠i.i.d. P   then f (x0j) ⇠i.i.d. Q and so ˆµf (X0) = 1
reduces to the commonly used pN-consistent empirical estimator of µf (X) (Smola et al.  2007).
Unfortunately  this is not the case after applying reduced set methods  and it is not known under
which conditions ˆµf (X) is a consistent estimator for µf (X).
Schölkopf et al. (2015) advocate the use of reduced expansion set methods to save computational
resources. They also provide some reasoning why this should be the right thing to do for characteristic
kernels  but as they state themselves  their rigorous analysis does not cover practical reduced set
methods. Motivated by this and other concerns listed in Section 1.4  we provide a generalised analysis
of the estimator ˆµf (X)  where we do not make assumptions on how xi and wi were generated.
Before doing that  however  we ﬁrst illustrate how the need for reduced set methods naturally emerges
on a concrete problem.

Illustration with functions of two random variables

1.3
i=1 and ˆY 0 =
Suppose that we want to estimate µf (X Y ) given i.i.d. samples ˆX0 = {x0i  1/N}N
j=1 from two independent random variables X 2X and Y 2Y respectively. Let Q be
{y0j  1/N}N
the distribution of Z = f (X  Y ).
i=1 kzf (x0i  y0i)  ..
The ﬁrst option is to consider what we will call the diagonal estimator ˆµ1 := 1
Since f (x0i  y0i) ⇠i.i.d. Q  ˆµ1 is pN-consistent (Smola et al.  2007). Another option is to con-
i j=1 kzf (x0i  y0j)  .  which is also known to be pN-
sider the U-statistic estimator ˆµ2 := 1
consistent. Experiments show that ˆµ2 is more accurate and has lower variance than ˆµ1 (see Figure 1).
However  the U-statistic estimator ˆµ2 needs O(n2) memory rather than O(n). For this reason
Schölkopf et al. (2015) propose to use a reduced set method both on ˆX0 and ˆY 0 to get new sam-
ples ˆX = {xi  wi}n
j=1 of size n ⌧ N  and then estimate µf (X Y ) using
ˆµ3 :=Pn

N 2PN
i=1 and ˆY = {yj  uj}n

NPn

i j=1 wiujkx(f (xi  yj)  .).

3

We ran experiments on synthetic data to show how accurately ˆµ1  ˆµ2 and ˆµ3 approximate µf (X Y )
with growing sample size N. We considered three basic arithmetic operations: multiplication
X · Y   division X/Y   and exponentiation X Y   with X ⇠N (3; 0.5) and Y ⇠N (4; 0.5). As the
true embedding µf (X Y ) is unknown  we approximated it by a U-statistic estimator based on a large
sample (125 points). For ˆµ3  we used the simplest possible reduced set method: we randomly sampled
subsets of size n = 0.01 · N of the xi  and optimized the weights wi and ui to best approximate ˆµX
and ˆµY . The results are summarised in Figure 1 and corroborate our expectations: (i) all estimators
converge  (ii) ˆµ2 converges fastest and has the lowest variance  and (iii) ˆµ3 is worse than ˆµ2  but
much better than the diagonal estimator ˆµ1. Note  moreover  that unlike the U-statistic estimator
ˆµ2  the reduced set based estimator ˆµ3 can be used with a ﬁxed storage budget even if we perform
a sequence of function applications—a situation naturally appearing in the context of probabilistic
programming.
Schölkopf et al. (2015) prove the consistency of ˆµ3 only for a rather limited case  when the points
of the reduced expansions {xi}n
i=1 are i.i.d. copies of X and Y   respectively  and
i=1 are constants. Using our new results we will prove in Section 3.1 the
the weights {(wi  ui)}n
consistency of ˆµ3 under fairly general conditions  even in the case when both expansion points and
weights are interdependent random variables.

i=1 and {yi}n

Figure 1: Error of kernel mean estimators for basic arithmetic functions of two variables  X · Y  
X/Y and X Y   as a function of sample size N. The U-statistic estimator ˆµ2 works best  closely
followed by the proposed estimator ˆµ3  which outperforms the diagonal estimator ˆµ1.

1.4 Other sources of non-i.i.d. samples

Although our discussion above focuses on reduced expansion set methods  there are other popular
algorithms that produce KME expansions where the samples are not i.i.d. Here we brieﬂy discuss
several examples  emphasising that our selection is not comprehensive. They provide additional
motivation for stating convergence guarantees in the most general setting possible.
An important notion in probability theory is that of a conditional distribution  which can also be
represented using KME (Song et al.  2009). With this representation the standard laws of probability 
such as sum  product  and Bayes’ rules  can be stated using KME (Fukumizu et al.  2013). Applying
those rules results in KME estimators with strong dependencies between samples and their weights.
Another possibility is that even though i.i.d. samples are available  they may not produce the best
estimator. Various approaches  such as kernel herding (Chen et al.  2010; Lacoste-Julien et al. 
2015)  attempt to produce a better KME estimator by actively generating pseudo-samples that are not
i.i.d. from the underlying distribution.

2 Main results

This section contains our main results regarding consistency and ﬁnite sample guarantees for the
estimator ˆµf (X) deﬁned in (1). They are based on the convergence of ˆµX and avoid simplifying
assumptions about its structure.

4

If

then

ˆµkx
X ! µkx

X

2.1 Consistency
If kx is c0-universal (see Sriperumbudur et al. (2011))  consistency of ˆµf (X) can be shown in a rather
general setting.
Theorem 1. Let X and Z be compact Hausdorff spaces equipped with their Borel -algebras 
f : X!Z a continuous function  kx  kz continuous kernels on X  Z respectively. Assume kx is
c0-universal and that there exists C such thatPi |wi| C independently of n. The following holds:
f (X) ! µkz
ˆµkz
Proof. Let P be the distribution of X and ˆPn = Pn
i=1 wixi. Deﬁne a new kernel on X by
ekx(x1  x2) := kzf (x1)  f (x2). X is compact and { ˆPn | n 2 N}[{ P} is a bounded set (in
total variation norm) of ﬁnite measures  because k ˆPnkT V = Pn
i=1 |wi| C. Furthermore  kx
is continuous and c0-universal. Using Corollary 52 of Simon-Gabriel and Schölkopf (2016) we
X implies that ˆP converges weakly to P . Now  kz and f being continuous 
conclude that: ˆµkx
so isekx. Thus  if ˆP converges weakly to P   then ˆµekx
X ! µekx
X (Simon-Gabriel and Schölkopf  2016 
X implies ˆµekx
X ! µekx
X ! µkx
X . We conclude the proof
by showing that convergence in Hekx
leads to convergence in Hkz:
X
=ˆµekx
ˆµkz
X  µekx
ekx! 0.

For a detailed version of the above  see Appendix A.

Theorem 44  Points (1) and (iii)). Overall  ˆµkx

f (X)

as n ! 1.

f (X)  µkz

X ! µkx

f (X)

2

kz

2

The continuity assumption is rather unrestrictive. All kernels and functions deﬁned on a discrete
space are continuous with respect to the discrete topology  so the theorem applies in this case. For
X = Rd  many kernels used in practice are continuous  including Gaussian  Laplacian  Matérn and
other radial kernels. The slightly limiting factor of this theorem is that kx must be c0-universal  which
often can be tricky to verify. However  most standard kernels—including all radial  non-constant
kernels—are c0-universal (see Sriperumbudur et al.  2011). The assumption that the input domain
is compact is satisﬁed in most applications  since any measurements coming from physical sensors

are contained in a bounded range. Finally  the assumption thatPi |wi| C can be enforced  for

instance  by applying a suitable regularization in reduced set methods.

2.2 Finite sample guarantees
Theorem 1 guarantees that the estimator ˆµf (X) converges to µf (X) when ˆµX converges to µX.
However  it says nothing about the speed of convergence. In this section we provide a convergence
rate when working with Matérn kernels  which are of the form

x(x  x0) =
ks

21s
(s) kx  x0ksd/2

2

Bd/2s (kx  x0k2)  

(2)

where B↵ is a modiﬁed Bessel function of the third kind (also known as Macdonald function) of
order ↵   is the Gamma function and s > d
2 is a smoothness parameter. The RKHS induced by
2 (Rd) (Wendland  2004  Theorem 6.13 & Chap.10) containing s-times
x is the Sobolev space W s
ks
differentiable functions. The ﬁnite-sample bound of Theorem 2 is based on the analysis of Kanagawa
et al. (2016)  which requires the following assumptions:
Assumptions 1. Let X be a random variable over X = Rd with distribution P and let ˆX =
i=1 be random variables over X n ⇥Rn with joint distribution S. There exists a probability
{(xi  wi)}n
distribution Q with full support on Rd and a bounded density  satisfying the following properties:

(i) P has a bounded density function w.r.t. Q;
(ii) there is a constant D > 0 independent of n  such that

S" 1

E

n

nXi=1

g2(xi)#  D kgk2

L2(Q)  

8g 2 L2(Q) .

5

ks1

x and ks2

Let ✓ = min( s2
2s1

  1) and assume ✓b  (1/2  c)(1  ✓) > 0. Then

These assumptions were shown to be fairly general and we refer to Kanagawa et al. (2016  Section
4.1) for various examples where they are met. Next we state the main result of this section.
Theorem 2. Let X = Rd  Z = Rd0  and f : X!Z be an ↵-times differentiable function (↵ 2 N+).
Take s1 > d/2 and s2 > d0 such that s1  s2/2 2 N+. Let ks1
z be Matérn kernels over X and
Z respectively as deﬁned in (2). Assume X ⇠ P and ˆX = {(xi  wi)}n
i=1 ⇠ S satisfy 1. Moreover 
assume that P and the marginals of x1  . . . xn have a common compact support. Suppose that  for
some constants b > 0 and 0 < c  1/2:
x i = O(n2b) ;
(i) EShkˆµX  µXk2
(ii) Pn
Sˆµf (X)  µf (X)

z  = O⇣(log n)d0 n2 (✓b(1/2c)(1✓))⌘ .

i = O(n2c) (with probability 1) .
i=1 w2
  ↵
s1

Before we provide a short sketch of the proof  let us brieﬂy comment on this result. As a benchmark 
remember that when x1  . . . xn are i.i.d. observations from X and ˆX = {(xi  1/n)}n
i=1  we get
kˆµf (X)  µf (X)k2 = OP (n1)  which was recently shown to be a minimax optimal rate (Tolstikhin
et al.  2016). How do we compare to this benchmark? In this case we have b = c = 1/2 and our rate
is deﬁned by ✓. If f is smooth enough  say ↵> d/ 2 + 1  and by setting s2 > 2s1 = 2↵  we recover
the O(n1) rate up to an extra (log n)d0 factor.
However  Theorem 2 applies to much more general settings. Importantly  it makes no i.i.d. assump-
tions on the data points and weights  allowing for complex interdependences. Instead  it asks the
convergence of the estimator ˆµX to the embedding µX to be sufﬁciently fast. On the downside  the
upper bound is affected by the smoothness of f  even in the i.i.d. setting: if ↵ ⌧ d/2 the rate will
become slower  as ✓ = ↵/s1. Also  the rate depends both on d and d0. Whether these are artefacts of
our proof remains an open question.

(3)

ks2

E

2

Proof. Here we sketch the main ideas of the proof and develop the details in Appendix C. Throughout
the proof  C will designate a constant that depends neither on the sample size n nor on the variable R
(to be introduced). C may however change from line to line. We start by showing that:

E

Sˆµkz

f (X)  µkz

2

f (X)
S⇣[ˆµh

E

kz = (2⇡)

d0

2 ZZ

E

S⇣[ˆµh

f (X)  µh

f (X)](z)⌘2 dz 

(4)

where h is Matérn kernel over Z with smoothness parameter s2/2. Second  we upper bound the
integrand by roughly imitating the proof idea of Theorem 1 from Kanagawa et al. (2016). This
eventually yields:

(5)
where ⌫ := ✓b  (1/2  c)(1  ✓). Unfortunately  this upper bound does not depend on z and can
not be integrated over the whole Z in (4). Denoting BR the ball of radius R  centred on the origin of
Z  we thus decompose the integral in (4) as:

f (X)  µh

f (X)](z)⌘2  Cn2⌫  

On X\BR  we upper bound the integral by a value that decreases with R  which is of the form:

f (X)  µh

f (X)](z)⌘2 dz  Cn12c(R  C0)s22e2(RC0)

(6)

(7)

On BR we upper bound the integral by (5) times the ball’s volume (which grows like Rd):

f (X)](z)⌘2 dz
f (X)](z)⌘2 dz +ZZ\BR
E⇣[ˆµh

f (X)  µh

f (X)  µh

E⇣[ˆµh
ZZ
E⇣[ˆµh
=ZBR
f (X)  µh
ZBR
E⇣[ˆµh

ZZ\BR

6

E⇣[ˆµh

f (X)  µh

f (X)](z)⌘2 dz.

f (X)](z)⌘2 dz  CRdn2⌫ .

with C0 > 0 being a constant smaller than R. In essence  this upper bound decreases with R because
f (X)](z) decays with the same speed as h when kzk grows indeﬁnitely. We are now left
f (X)  µh
[ˆµh
with two rates  (6) and (7)  which respectively increase and decrease with growing R. We complete
the proof by balancing these two terms  which results in setting R ⇡ (log n)1/2.
3 Functions of Multiple Arguments

The previous section applies to functions f of one single variable X. However  we can apply its
results to functions of multiple variables if we take the argument X to be a tuple containing multiple
values. In this section we discuss how to do it using two input variables from spaces X and Y  but
the results also apply to more inputs. To be precise  our input space changes from X to X⇥Y   input
random variable from X to (X  Y )  and the kernel on the input space from kx to kxy.
To apply our results from Section 2  all we need is a consistent estimator ˆµ(X Y ) of the joint embedding
µ(X Y ). There are different ways to get such an estimator. One way is to sample (x0i  y0i) i.i.d. from
the joint distribution of (X  Y ) and construct the usual empirical estimator  or approximate it using
reduced set methods. Alternatively  we may want to construct ˆµ(X Y ) based only on consistent
estimators of µX and µY . For example  this is how ˆµ3 was deﬁned in Section 1.3. Below we show
that this can indeed be done if X and Y are independent.

3.1 Application to Section 1.3
Following Schölkopf et al. (2015)  we consider two independent random variables X ⇠ Px and
Y ⇠ Py. Their joint distribution is Px ⌦ Py. Consistent estimators of their embeddings are
given by ˆµX = Pn
In this section we show that
ˆµf (X Y ) =Pn
We choose a product kernel kxy(x1  y1)  (x2  y2) = kx(x1  x2)ky(y1  y2)  so the corresponding
RKHS is a tensor product Hkxy = Hkx ⌦H ky (Steinwart and Christmann  2008  Lemma 4.6) and
the mean embedding of the product random variable (X  Y ) is a tensor product of their marginal
mean embeddings µ(X Y ) = µX ⌦ µY . With consistent estimators for the marginal embeddings we
can estimate the joint embedding using their tensor product

i j=1 wiujkzf (xi  yj)  . is a consistent estimator of µf (X Y ).

i=1 wikx(xi  .) and ˆµY = Pn

j=1 ujky(yi  .).

ˆµ(X Y ) = ˆµX ⌦ ˆµY =

wiujkx(xi  .) ⌦ ky(yj  .) =

nXi j=1

nXi j=1

wiujkxy(xi  yj)  (.   .).

If points are i.i.d. and wi = ui = 1/n  this reduces to the U-statistic estimator ˆµ2 from Section 1.3.
Lemma 3. Let (sn)n be any positive real sequence converging to zero. Suppose kxy = kxky is a
product kernel  µ(X Y ) = µX ⌦ µY   and ˆµ(X Y ) = ˆµX ⌦ ˆµY . Then:

Proof. For a detailed expansion of the ﬁrst inequality see Appendix B.

implies

kˆµY  µY kky

= O(sn);
= O(sn)

(kˆµX  µXkkx
ˆµ(X Y )  µ(X Y )kxy  kµXkkx kˆµY  µY kky

ˆµ(X Y )  µ(X Y )kxy

= O(sn) .

+ kµY kky kˆµX  µXkkx
n) = O(sn).

+ kˆµX  µXkkx kˆµY  µY kky
µX and ˆµY !n!1

= O(sn) + O(sn) + O(s2
µY   then ˆµ(X Y ) !n!1

Corollary 4. If ˆµX !n!1
Together with the results from Section 2 this lets us reason about estimators resulting from applying
functions to multiple independent random variables. Write

µ(X Y ).

ˆµkxy
XY =

nXi j=1

wiujkxy(xi  yj)  . =

n2X`=1

!`kxy(⇠`  .) 

7

Y then ˆµkxy

XY ! µkxy

X

where ` enumerates the (i  j) pairs and ⇠` = (xi  yj)  !` = wiuj. Now if ˆµkx
and ˆµky

X ! µkx
Y ! µky
(X Y ) (according to Corollary 4) and Theorem 1 shows that
i j=1 wiujkzf (xi  yj)  . is consistent as well. Unfortunately  we cannot apply Theorem 2 to get
Pn

the speed of convergence  because a product of Matérn kernels is not a Matérn kernel any more.
One downside of this overall approach is that the number of expansion points used for the estimation
of the joint increases exponentially with the number of arguments of f. This can lead to prohibitively
large computational costs  especially if the result of such an operation is used as an input to another
function of multiple arguments. To alleviate this problem  we may use reduced expansion set methods
before or after applying f  as we did for example in Section 1.2.
To conclude this section  let us summarize the implications of our results for two practical scenarios
that should be distinguished.

. If we have separate samples from two random variables X and Y   then our results justify
how to provide an estimate of the mean embedding of f (X  Y ) provided that X and Y are
independent. The samples themselves need not be i.i.d. — we can also work with weighted
samples computed  for instance  by a reduced set method.
. How about dependent random variables? For instance  imagine that Y = X  and
f (X  Y ) = X + Y . Clearly  in this case the distribution of f (X  Y ) is a delta mea-
sure on 0  and there is no way to predict this from separate samples of X and Y . However 
it should be stressed that our results (consistency and ﬁnite sample bound) apply even to
the case where X and Y are dependent. In that case  however  they require a consistent
estimator of the joint embedding µ(X Y ).

. It is also sufﬁcient to have a reduced set expansion of the embedding of the joint distribution.
This setting may sound strange  but it potentially has signiﬁcant applications. Imagine that
one has a large database of user data  sampled from a joint distribution. If we expand the
joint’s embedding in terms of synthetic expansion points using a reduced set construction
method  then we can pass on these (weighted) synthetic expansion points to a third party
without revealing the original data. Using our results  the third party can nevertheless
perform arbitrary continuous functional operations on the joint distribution in a consistent
manner.

4 Conclusion and future work

This paper provides a theoretical foundation for using kernel mean embeddings as approximate
representations of random variables in scenarios where we need to apply functions to those random
variables. We show that for continuous functions f (including all functions on discrete domains) 
consistency of the mean embedding estimator of a random variable X implies consistency of the
mean embedding estimator of f (X). Furthermore  if the kernels are Matérn and the function f
is sufﬁciently smooth  we provide bounds on the convergence rate. Importantly  our results apply
beyond i.i.d. samples and cover estimators based on expansions with interdependent points and
weights. One interesting future direction is to improve the ﬁnite-sample bounds and extend them to
general radial and/or translation-invariant kernels.
Our work is motivated by the ﬁeld of probabilistic programming. Using our theoretical results 
kernel mean embeddings can be used to generalize functional operations (which lie at the core of
all programming languages) to distributions over data types in a principled manner  by applying the
operations to the points or approximate kernel expansions. This is in principle feasible for any data
type provided a suitable kernel function can be deﬁned on it. We believe that the approach holds
signiﬁcant potential for future probabilistic programming systems.

Acknowledgements

We thank Krikamol Muandet for providing the code used to generate Figure 1  Paul Rubenstein 
Motonobu Kanagawa and Bharath Sriperumbudur for very useful discussions  and our anonymous
reviewers for their valuable feedback. Carl-Johann Simon-Gabriel is supported by a Google European
Fellowship in Causal Inference.

8

References
R. A. Adams and J. J. F. Fournier. Sobolev Spaces. Academic Press  2003.
C. Bennett and R. Sharpley. Interpolation of Operators. Pure and Applied Mathematics. Elsevier Science  1988.
A. Berlinet and C. Thomas-Agnan. RKHS in probability and statistics. Springer  2004.
Y. Chen  M. Welling  and A. Smola. Super-samples from kernel herding. In UAI  2010.
K. Fukumizu  L. Song  and A. Gretton. Kernel Bayes’ Rule: Bayesian Inference with Positive Deﬁnite Kernels.

Journal of Machine Learning Research  14:3753–3783  2013.

I. S. Gradshteyn and I. M. Ryzhik. Table of integrals  series  and products. Elsevier/Academic Press  Amsterdam 

2007. Edited by Alan Jeffrey and Daniel Zwillinger.

M. Kalos and P. Whitlock. Monte Carlo Methods. Wiley  2008.
M. Kanagawa  B. K. Sriperumbudur  and K. Fukumizu. Convergence guarantees for kernel-based quadrature

rules in misspeciﬁed settings. arXiv:1605.07254 [stat]  2016. arXiv: 1605.07254.

Y. Katznelson. An Introduction to Harmonic Analysis. Cambridge University Press  2004.
M. Korze´n and S. Jaroszewicz. PaCAL: A Python package for arithmetic computations with random variables.

Journal of Statistical Software  57(10)  2014.

S. Lacoste-Julien  F. Lindsten  and F. Bach. Sequential kernel herding : Frank-Wolfe optimization for particle

ﬁltering. In Artiﬁcial Intelligence and Statistics  volume 38  pages 544–552  2015.

A. Mathai. A review of the different techniques used for deriving the exact distributions of multivariate test

criteria. Sankhy¯a: The Indian Journal of Statistics  Series A  pages 39–60  1973.

K. McKinley. Programming the world of uncertain things (keynote). In ACM SIGPLAN-SIGACT Symposium on

Principles of Programming Languages  pages 1–2  2016.

D. Milios. Probability Distributions as Program Variables. PhD thesis  University of Edinburgh  2009.
S. Poisson. Recherches sur la probabilitédes jugements en matière criminelle et en matière civile  précédées des

règles générales du calcul des probabilités. 1837.

B. Schölkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regularization  Optimization 

and Beyond. MIT Press  2002.

B. Schölkopf  K. Muandet  K. Fukumizu  S. Harmeling  and J. Peters. Computing functions of random variables

via reproducing kernel Hilbert space representations. Statistics and Computing  25(4):755–766  2015.

C. Scovel  D. Hush  I. Steinwart  and J. Theiler. Radial kernels and their reproducing kernel hilbert spaces.

Journal of Complexity  26  2014.

C.-J. Simon-Gabriel and B. Schölkopf. Kernel distribution embeddings: Universal kernels  characteristic kernels

and kernel metrics on distributions. Technical report  Max Planck Institute for Intelligent Systems  2016.

A. Smola  A. Gretton  L. Song  and B. Schölkopf. A Hilbert space embedding for distributions. In ALT  2007.
L. Song  J. Huang  A. Smola  and K. Fukumizu. Hilbert space embeddings of conditional distributions with

applications to dynamical systems. In International Conference on Machine Learning  pages 1–8  2009.

M. D. Springer. The Algebra of Random Variables. Wiley  1979.
B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Schölkopf  and G. R. Lanckriet. Hilbert space embeddings

and metrics on probability measures. Journal of Machine Learning Research  11:1517–1561  2010.

B. K. Sriperumbudur  K. Fukumizu  and G. R. G. Lanckriet. Universality  characteristic kernels and RKHS

embedding of measures. Journal of Machine Learning Research  12:2389–2410  2011.

I. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics. Springer  2008.
I. Steinwart and C. Scovel. Mercer’s Theorem on General Domains: On the Interaction between Measures 

Kernels  and RKHSs. Constructive Approximation  35(3):363–417  2012.

I. Tolstikhin  B. Sriperumbudur  and K. Muandet. Minimax Estimation of Kernel Mean Embeddings.

arXiv:1602.04361 [math  stat]  2016.

H. Wendland. Scattered Data Approximation. Cambridge University Press  2004.
R. Williamson. Probabilistic Arithmetic. PhD thesis  University of Queensland  1989.

9

,Carl-Johann Simon-Gabriel
Adam Scibior
Ilya Tolstikhin
Bernhard Schölkopf