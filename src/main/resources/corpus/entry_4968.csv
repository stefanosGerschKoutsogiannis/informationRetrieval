2009,Semi-supervised Learning using Sparse Eigenfunction Bases,We present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices.  It turns out that  when the \emph{cluster assumption} holds  that is  when the high density regions are sufficiently separated by  low density valleys  each high density area corresponds to a unique representative eigenvector. Linear combination of such eigenvectors (or  more precisely  of their Nystrom extensions) provide good candidates for good classification functions. By first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data  with Lasso to select a classifier in the span of these eigenvectors  we obtain a classifier  which has a very sparse representation in this basis. Importantly  the sparsity appears naturally from the  cluster assumption. Experimental results on a number  of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm (Lasso in the Kernel PCA basis).,Semi-supervised Learning using Sparse

Eigenfunction Bases

Dept. of Computer Science and Engineering

Dept. of Computer Science and Engineering

Mikhail Belkin

Ohio State University
Columbus  OH 43210

Kaushik Sinha

Ohio State University
Columbus  OH 43210

sinhak@cse.ohio-state.edu

mbelkin@cse.ohio-state.edu

Abstract

We present a new framework for semi-supervised learning with sparse eigenfunc-
tion bases of kernel matrices. It turns out that when the data has clustered  that
is  when the high density regions are suf(cid:2)ciently separated by low density valleys 
each high density area corresponds to a unique representative eigenvector.
Linear combination of such eigenvectors (or  more precisely  of their Nystrom
extensions) provide good candidates for good classi(cid:2)cation functions when the
cluster assumption holds. By (cid:2)rst choosing an appropriate basis of these eigen-
vectors from unlabeled data and then using labeled data with Lasso to select a
classi(cid:2)er in the span of these eigenvectors  we obtain a classi(cid:2)er  which has a very
sparse representation in this basis. Importantly  the sparsity corresponds naturally
to the cluster assumption.
Experimental results on a number of real-world data-sets show that our method
is competitive with the state of the art semi-supervised learning algorithms and
outperforms the natural base-line algorithm (Lasso in the Kernel PCA basis).

1 Introduction
Semi-supervised learning  i.e.  learning from both labeled and unlabeled data has received con-
siderable attention in recent years due to its potential in reducing the need for expensive labeled
data. However  to make effective use of unlabeled examples one needs to make some assumptions
about the connection between the process generating the data and the process of assigning labels.
There are two important assumptions popular in semi-supervised learning community the (cid:147)cluster
assumption(cid:148) [CWS02] and the (cid:147)manifold assumption(cid:148) [BNS06] as well as a number of model-based
methods  such as Naive Bayes [HTF03]. In particular  the cluster assumption can be interpreted as
saying that two points are likely to have the same class labels if they can be connected by a path
passing through a high density area. In other words two high density areas with different class labels
must be separated by a low density valley.
In this paper  we develop a framework for semi-supervised learning when the cluster assumption
holds. Speci(cid:2)cally  we show that when the high density areas are suf(cid:2)ciently separated  a few ap-
propriately chosen eigenfunctions of a convolution operator (which is the continuous counterpart
of the kernel matrix) represents the high density areas reasonably well. Under the ideal conditions
each high density area can be represented by a single unique eigenfunction called the (cid:147)representa-
tive(cid:148) eigenfunction. If the cluster assumption holds  each high density area will correspond to just
one class label and thus a sparse linear combination of these representative eigenfunctions would be
a good classi(cid:2)er. Moreover  the basis of such eigenfunctions can be learned using only the unlabeled
data by constructing the Nystrom extension of the eigenvectors of an appropriate kernel matrix.
Thus  given unlabeled data we construct the basis of eigenfunctions and then apply L1 penalized
optimization procedure Lasso [Tib96] to (cid:2)t a sparse linear combination of the basis elements to

1

the labeled data. We provide a detailed theoretical analysis of the algorithm and show that it is
comparable to the state-of-the-art on several common UCI datasets.
The rest of the paper is organized as follows.
In section 2 we provide the proposed framework
for semi-supervised learning and describe the algorithm. In section 3 we provide an analysis of
this algorithm to show that it can consistently identify the correct model. In section 4 we provide
experimental results on synthetic and real datasets and (cid:2)nally we conclude with a discussion in
section 5.

2 Semi-supervised Learning Framework
2.1 Outline of the Idea
In this section we present a framework for semi-supervised learning under the cluster assumption.
Speci(cid:2)cally we will assume that (i) data distribution has natural clusters separated by regions of low
density and (ii) the label assignment conforms to these clusters.
The recent work of [SBY08a  SBY08b] shows that if the (unlabeled) data is clustered  then for each
high density region there is a unique (representative) eigenfunction of a convolution operator  which
takes positive values for points in the chosen cluster and whose values are close to zero everywhere
else (no sign change). Moreover  it can be shown (e.g.  [RBV08]) that these eigenfunctions can be
approximated from the eigenvectors of a kernel matrix obtained from the unlabeled data.
Thus  if the cluster assumption holds we expect each cluster to have exactly one label assignment.
Therefore eigenfunctions corresponding to these clusters should produce a natural sparse basis for
constructing a classi(cid:2)cation function.
This suggests the following learning strategy:

1. From unlabeled and labeled data obtain the eigenvectors of the Gaussian kernel matrix.
2. From these eigenvectors select a subset of candidate eigenvectors without sign change.
3. Using the labeled data  apply Lasso (sparse linear regression) in the constructed basis to

obtain a classi(cid:2)er.

4. Using the Nystrom extension (see [BPV03])  extend the eigenvectors to obtain the classi(cid:2)-

cation function de(cid:2)ned everywhere.

Connection to Kernel PCA ( [SSM98]). We note that our method is related to KPCA  where data is
projected onto the space spanned by the top few eigenvectors of the kernel matrix and classi(cid:2)cation
or regression task can be performed in that projected space. The important difference is that we
choose a subset of the eigenvectors in accordance to the cluster assumption. We note that the method
simply using the KPCA basis does not seem to bene(cid:2)t from unlabeled data and  in fact  cannot
outperform the standard fully supervised SVM classi(cid:2)er. On the other hand  our algorithm using a
basis subselection procedure shows results comparable to the state of the art.
This is due to two reasons. We will see that each cluster in the data corresponds to its unique
representative eigenvector of the kernel matrix. However  this eigenvector may not be among the
top eigenvectors and may thus be omitted when applying KPCA. Alternatively  if the representa-
tive eigenvector is included  it will be included with a number of other uninformative eigenvectors
resulting in poor performance due to over(cid:2)tting.
We now proceed with the detailed discussion of our algorithm and its analysis.

2.2 Algorithm
The focus of our discussion will be binary classi(cid:2)cation in the semi-supervised setting. Given l
labeled examples f(xi; yi)gl
i=1 sampled from an underlying joint probability distribution PX ;Y 
X (cid:26) Rd;Y = f(cid:0)1; 1g  where xis are the data points  yis are their corresponding labels and u
i=l+1 drawn iid from the marginal distribution PX   we choose a Gaus-
unlabeled examples fxigl+u
sian kernel k(x; z) = exp(cid:16)(cid:0) kx(cid:0)zk2
2!2 (cid:17) with kernel bandwidth ! to construct the kernel matrix K
i=1 be the eigenvalue-eigenvector pair of K sorted by the
where Kij = 1
non-increasing eigenvalues. It has been shown ([SBY08a  SBY08b]) that when data distribution PX

u k(zi; zj). Let ((cid:21)i; vi)u

2

has clusters  for each high density region there is a unique representative eigenfunction of a convo-
lution operator that takes positive values around the chosen cluster and is close to zero everywhere
else. Moreover these eigenfunctions can be approximated from the eigenvectors of a kernel matrix
obtained from the unlabeled data ([RBV08])  thus for each high density region there is a unique rep-
resentative eigenvector of the kernel matrix that takes only positive or negative values in the chosen
cluster and is nearly zero everywhere else (no sign change).
If the cluster assumption holds  i.e.  each high density region corresponds to a portion of a pure class 
then the classi(cid:2)er can be naturally expressed as a linear combination of the representative eigenfunc-
tions. representative eigenvector basis and a linear combination of the representative eigenvectors
will be a reasonable candidate for a good classi(cid:2)cation function. However  identifying representative
eigenvectors is not very trivial because in real life depending on the separation between high density
clusters the representative eigenvectors can have no sign change up to some small precision (cid:15) > 0.
Speci(cid:2)cally  we say that a vector e = (e1; e2; :::; en) 2 Rn has no sign change up to precision (cid:15) if
either 8i ei > (cid:0)(cid:15) or 8i ei < (cid:15). Let N(cid:15) be the set of indices of all eigenvectors that have no sign
change up to precision (cid:15). If (cid:15) is chosen properly  N(cid:15) will contain representative eigenvectors (note
that the set N(cid:15) and the set f1; 2; :::;jN(cid:15)jg are not necessarily the same). Thus  instead of identifying
the representative eigenvectors  we carefully select a small set containing the representative eigen-
vectors. Our goal is to learn a linear combination of the eigenvectors Pi2N(cid:15)
(cid:12)ivi which minimizes
classi(cid:2)cation error on the labeled examples and the coef(cid:2)cients corresponding to non-representative
eigenvectors are zeros. Thus  the task is more of model selection or sparse approximation.
Standard approach to get a sparse solution is to minimize a convex loss function V on the labeled
examples and apply a L1 penalty (on (cid:12)is). If we select V to be square loss function  we end up
solving the L1 penalized least square or so called Lasso [Tib96]  whose consistency property was
studied in [ZY06]. Thus we would seek a solution of the form

arg min

(cid:12)

(y (cid:0) (cid:9)(cid:12))T (y (cid:0) (cid:9)(cid:12)) + (cid:21)jj(cid:12)jjL1

(1)

which is a convex optimization problem  where (cid:9) is the l (cid:2) jN(cid:15)j design matrix whose ith column
is the (cid:2)rst l elements of vN(cid:15)(i)  y 2 Rl is the label vector  (cid:12) is the vector of coef(cid:2)cients and (cid:21) is a
regularization parameter. Note that solving the above problem is equivalent to solving

arg min

(cid:12)

(y (cid:0) (cid:9)(cid:12))T (y (cid:0) (cid:9)(cid:12)) s:t: Xi2N(cid:15)

j(cid:12)ij (cid:20) t

(2)

because for any given (cid:21) 2 [0;1)  there exists a t (cid:21) 0 such that the two problems have
the same solution  and vice versa [Tib96]. We will denote the solution of Equation 2  by ^(cid:12).
To obtain a classi(cid:2)cation function which is de(cid:2)ned everywhere  we use the Nystrom extension
of the ith eigenvector de(cid:2)ned as i(x) =
j=1 vi(xj)k(x; xj). Let the set T con-
tains indices of all nonzero ^(cid:12)is. Using Nystrom extension  classi(cid:2)cation function is given by 
i=1 Wik(xi; x)  where  W 2 Ru is a weight vector whose ith ele-
f (x) = Pi2T
ment is given by
(3)

^(cid:12)i i(x) = Pl+u

(cid:21)ipl+u Pl+u

1

and can be computed while training.

^(cid:12)j vj(xi)
(cid:21)jpu

Wi = Xj2T

Algorithm for Semi-supervised Learning

Input: f(xi; yi)gl
Parameters: !; t; (cid:15)

i=1; fxigl+u

i=l+1

i=1.
1. Construct kernel matrix K from l + u unlabeled examples fxigl+u
2. Select set N(cid:15) containing indices of the eigenvectors with no sign change up to precision (cid:15).
3. Construct design matrix (cid:9) whose ith column is top l rows of vN(cid:15)(i).
4. Solve Equation 2 to get ^(cid:12) and calculate weight vector W using Equation 3.
5. Given a test point x  predict its label as y = sign (Pu

i=1 k(xi; x)Wi)

3

3 Analysis of the Algorithm
The main purpose of the analysis is  (i) to estimate the amount of separation required among the high
density regions which ensures that each high density region can be well represented by a unique
(representative) eigenfunction  (ii) to estimate the number of unlabeled examples required so that
eigenvectors of kernel matrix can approximate the eigenfunctions of a convolution operator (de(cid:2)ned
below) and (iii) to show that using few labeled examples Lasso can consistently identify the correct
model consisting of linear combination of representative eigenvectors.
Before starting the actual analysis  we (cid:2)rst note that the continuous counterpart of the Gram matrix
is a convolution operator LK : L2(X ;PX ) ! L2(X ;PX ) de(cid:2)ned by 
k(x; z)f (z)dPX (z)

(4)

(LK f )(x) = ZX

(cid:9)T

(1) u

l (cid:9)T (cid:9).

(cid:16)(cid:9)T

(1)(cid:9)(1)(cid:17)T

j 2(cid:9)(2) (cid:12)(cid:12)(cid:12)(cid:12)

The eigenfunctions of the symmetric positive de(cid:2)nite operator LK will be denoted by (cid:30)L
i .
Next  we brie(cid:3)y discuss the effectiveness of model selection using Lasso (established by [ZY06])
which will be required for our analysis. Let ^(cid:12)l((cid:21)) be the solution of Equation 1 for a chosen
regularization parameter (cid:21). In [ZY06] a concept of sign consistency was introduced which states
that Lasso is sign consistent if  as l tends to in(cid:2)nity  signs of ^(cid:12)l((cid:21)) matches with the signs of (cid:12)(cid:3) with
probability 1  where (cid:12)(cid:3) is the coef(cid:2)cients of the correct model. Note that since we are expecting a
sparse model  matching zeros of ^(cid:12)l((cid:21)) to the zeros of (cid:12)(cid:3) is not enough  but in addition  matching
the signs of the non zero coef(cid:2)cients ensures that the true model will be selected. Next  without loss
of generality assume (cid:12)(cid:3) = ((cid:12)(cid:3)1 ;(cid:1)(cid:1)(cid:1) ; (cid:12)(cid:3)q ; (cid:12)(cid:3)q+1;(cid:1)(cid:1)(cid:1) ; (cid:12)(cid:3)
) has only (cid:2)rst q terms non-zero  i.e.  only
jN(cid:15)j
q predictors describe the model and rest of the predictors are irrelevant in describing the model. Now
let us write the (cid:2)rst q and jN(cid:15)j (cid:0) q columns of (cid:9) as (cid:9)(1) and (cid:9)(2) respectively. Let C = 1
Note that  for a random design matrix  sign consistency is equivalent to irrepresentable condition
(see [ZY06]). When (cid:12)(cid:3) is unknown  in order to ensure that irrepresentable condition holds for all
possible signs  it requires that L1 norm of the regression coef(cid:2)cients corresponding to the irrelevant
predictors to be less than 1  which can be written as (cid:22)(cid:9) = max u
<
1. The requirement (cid:22)(cid:9) < 1 is not new and have also appeared in the context of noisy or noiseless
sparse recovery of signal [Tro04  Wai06  Zha08]. Note that Lasso is sign consistent if irrepresentable
condition holds and the suf(cid:2)cient condition needed for irrepresentable condition to hold is given by
the following result 
Theorem 3.1. [ZY06] Suppose (cid:12)(cid:3) has q nonzero entries. Let the matrix C0 be normalized version
of C such that C0ij = Cij
2q(cid:0)1 for a constant 0 (cid:20) c < 1  then strong
irrepresentable condition holds.
Our main result in the following shows that this suf(cid:2)cient condition is satis(cid:2)ed with high probability
requiring relatively few labeled examples  as a result the correct model is identi(cid:2)ed consistently 
which in turn describes a good classi(cid:2)cation function.
Theorem 3.2. Let q be the minimum number of columns of the design matrix (cid:9) 2 Rl(cid:2)jN(cid:15)j  con-
structed from l labeled examples  that describes the sparse model. Then for any 0 < (cid:14) < 1  if
  then with probability greater than
the number of unlabeled examples u satisﬁes u >
50q2 (cid:17)  maxi6=j jC0ijj < 1
1 (cid:0) (cid:14)
2q(cid:0)1 .
max (to be de(cid:2)ned later) largest eigenvalue of LK and gNmax is the N th
where (cid:21)Nmax is the N th
max
eigengap. Note that in our framework  unlabeled examples help polynomially fast in estimating
the eigenfunctions while labeled examples help exponentially fast in identifying the sparse model
consisting of representative eigenfunctions. Interestingly  in semi-supervised learning setting  sim-
ilar role of labeled and unlabeled examples (in reducing classi(cid:2)cation error) has been reported in
literature [CC96  RV95  SB07  SNZ08].
3.1 Brief Overview of the Analysis
As a (cid:2)rst step of our analysis  in section 3.2  we estimate the separation requirement among the
high density regions which ensures that each high density region (class) can be well represented
by a unique eigenfunction. This allows us to express the classi(cid:2)cation task in this eigenfunction

and maxi;j;i6=j jC0ijj (cid:20) c

2 (cid:0) 4 exp(cid:16)(cid:0)

2048q2 log( 2
(cid:14) )
g2
Nmax

Nmax

j(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)1

(cid:12)(cid:12)(cid:12)(cid:12)

Cii

l(cid:21)2

Nmax

(cid:21)2

4

basis where we look for a classi(cid:2)cation function consisting of linear combination of representative
eigenfunctions only and thus relate the problem to sparse approximation from the model selection
point of view  which is a well studied (cid:2)eld [Wai06  ZH06  CP07].
As a second step in section 3.3  using perturbation results from [RBV08]  we estimate the number of
unlabeled examples required to ensure that Nystrom extensions of eigenvectors of K approximate
the eigenfunctions of the convolution operator LK reasonably well with high probability.
Finally  as a third step in section 3.4  we establish a concentration inequality  which along with
result from the second step 2  ensures that as more and more labeled examples are used to (cid:2)t the
eigenfunctions basis to the data  the probability that Lasso identi(cid:2)es correct model consisting of
representative eigenfunctions increases exponentially fast.
3.2 Separation Requirement
To motivate our discussion we consider binary classi(cid:2)cation problem where the marginal density
can be considered as a mixture model where each class has its own probability density function 
p1(x); p2(x) and corresponding mixing weights (cid:25)1; (cid:25)2 respectively. Thus  the density of the mixture
is p(x) = (cid:25)1p1(x) + (cid:25)2p2(x). We will use the following results from [SBY08a] specifying the
behavior of the eigenfunction of LK corresponding to the largest eigenvalue.
Theorem 3.3. [SBY08a] The top eigenfunction (cid:30)L
0 (x) of LK corresponding to the largest eigen-
value (cid:21)0  (1) is the only eigenfunction with no sign change  (2) has multiplicity one  (3) is non zero
(cid:21)0qR k2(x; z)p(z)dz (Tail decay
0 (x)j (cid:20) 1
on the support of the underlying density  (4) satisﬁes j(cid:30)L
property)  where p is the underlying probability density function.
Note that the last (tail decay) property above is not restricted to the top eigenfunction alone
but is satis(cid:2)ed by all eigenfunctions of LK. Now  consider applying LK to the three cases
when the underlying probability distributions are p1; p2 and p. The largest eigenvalues and
0 respec-
corresponding eigenfunctions in the above three cases are (cid:21)1
tively. To show explicit dependency on the underlying probability distribution  we will denote
K .
the corresponding operators as Lp1
K + (cid:25)2Lp2
K = (cid:25)1Lp1
0(cid:16)(cid:30)L;1
0 + T1(x)(cid:17) where 
Then we can write  Lp
T1(x) = (cid:25)2
(x) =
(cid:25)1(cid:21)1
0(cid:16)(cid:30)L;2
(z)p1(z)dz. Thus  when T1(x) and
(cid:25)2(cid:21)2
T2(x) are small enough then (cid:30)L;1
K with corresponding eigen-
0 respectively. Note that (cid:147)separation condition(cid:148) requirement refers to T1(x) 
values (cid:25)1(cid:21)1
T2(x) being small  so that eigenfunctions corresponding to the largest eigenvalues of convolution
operator when applied to individual high density bumps are preserved in the case when convolution
operator is applied to the mixture. Clearly  we can not expect T1(x)  T2(x) to arbitrarily small if
there is suf(cid:2)cient overlap between p1 and p2. Thus  we will restrict ourselves to the following class
of probability distributions for each individual class which has reasonably fast tail decay.
Assumption 1. For any 1=2 < (cid:17) < 1  let M((cid:17);R) be the class of probability distributions such
that its density function p satisﬁes
1) RR
2) For any positive t > 0  smaller than the radius of R  and for any point z 2 X n R with
dist(z;R) (cid:21) t  the volume S = fx 2 (X n R) \ B(z; 3t=p2)g has total probability mass
p(x)dx (cid:20) C1(cid:17) exp(cid:16)(cid:0) dist2(z;R)
RS
where the distance between a point x and set D is de(cid:2)ned as dist(x;D) = inf y2D jjx (cid:0) yjj. With
a little abuse of notation we will use p 2 M((cid:17);R) to mean that p is the probability density function
of a member of M((cid:17);R). Now a rough estimate of separation requirement can be given by the
following lemma.
Lemma 3.1. Let p1 2 M((cid:17);R1) and p2 2 M((cid:17);R2) and let the minimum distance between R1;R2
be (cid:1). If (cid:1) = (cid:10)(cid:3)(cid:16)!pd(cid:17) then T1(x) and T2(x) can be made arbitrarily small for all x 2 X .

(x) = R k(x; z)(cid:30)L;1
(z)p2(z)dz.
0 + T2(x)(cid:17) where  T2(x) = (cid:25)1
(cid:25)2(cid:21)2
and (cid:30)L;2
0 and (cid:25)2(cid:21)2

In a similar way we can write  Lp
0 R k(x; z)(cid:30)L;2
are eigenfunctions of Lp

p(x)d(x) = (cid:17) where R is the minimum volume ball around the mean of the distribution.

K(cid:30)L;1
0 R k(x; z)(cid:30)L;1

K ; Lp2

K and Lp

K respectively. Clearly  Lp

(cid:17) for some C1 > 0.

0; (cid:21)2

0; (cid:21)0 and (cid:30)L;1

0

(z)p(z)dz = (cid:25)1(cid:21)1

; (cid:30)L;2

0

; (cid:30)L

K(cid:30)L;2

0

The estimate of (cid:1) in the above lemma  where we hide the log factor by (cid:10)(cid:3)  is by no means tight 
nevertheless  it shows that separation requirement refers to existence of a low density valley between

0

0

0

0

0

0

t2

5

2!2

(cid:17)

p(C1+(cid:17))

(cid:21)0

and (cid:30)L;2

0

0

K correspond-

0 (z)j (cid:21) j(cid:30)L

0 (x)j

0 is the eigenfunction of Lq

1+e < (cid:17) < 1  let q 2 M((cid:17);R). If (cid:30)L
exp(cid:16)(cid:0) dist2(x;R)

two high density regions each corresponding to one of the classes. This separation requirement is
roughly of the same order required to learn mixture of Gaussians [Das99]. Note that  provided
are not necessarily the top two eigenfunctions of
separation requirement is satis(cid:2)ed  (cid:30)L;1
LK corresponding to the two largest eigenvalues but can be quite far down the spectrum of Lp
K
depending on the mixing weights (cid:25)1; (cid:25)2. Next  the following lemma suggests that we can say more
about the eigenfunction corresponding to the largest eigenvalue.
Lemma 3.2. For any e
ing to the largest eigenvalue (cid:21)0 then there exists a C1 > 0 such that
1) For all x 2 X n R;j(cid:30)L
2) For all z 2 R and x 2 X n R;j(cid:30)L
Thus for each class  top eigenfunction corresponding to the largest eigenvalue represents high den-
sity region reasonably well  outside high density region is has lower absolute value and decays
exponentially fast.
3.3 Finite Sample Results
We start with the following assumption.
Assumption 2. The Nmax largest eigenvalues of LK and K  where Nmax = maxifi : i 2 N(cid:15)g  are
simple and bounded away from zero.
Note that Nystrom extension is are eigenfunctions of an operator LK;H : H ! H   where H
is the unique RKHS de(cid:2)ned by the chosen Gaussian kernel and all the eigenvalues of K are also
eigenvalues of LK;H ([RBV08]). There are two implications of Assumption 2. The (cid:2)rst one is due
to the bounded away from zero part  which ensures that if we restrict to i 2 H corresponding to the
largest Nmax eigenvalues  then each of them is square integrable hence belongs to L2(X ;PX ). The
second implication due to the simple part  ensures that eigenfunctions corresponding to the Nmax
largest eigenvalues are uniquely de(cid:2)ned and so are the orthogonal projections on to them. Note that
if any eigenvalue has multiplicity greater than one then the corresponding eigenspace is well de(cid:2)ned
but not the individual eigenfunctions. Thus  Assumption 2 enables us to compare how close each i
is to some other function in L2(X ;PX ) in L2(X ;PX ) norm sense. Let gNmax be the N th
max eigengap
when eigenvalues of LK are sorted in non increasing order. Then we have the following results.
Lemma 3.3. Suppose Assumption 2 holds and the top Nmax eigenvalues of LK and K are sorted in
the decreasing order. Then for any 0 < (cid:14) < 1 and for any i 2 N(cid:15)  with probability at least (1 (cid:0) (cid:14)) 
k i (cid:0) (cid:30)L
Corollary 3.1. Under the above conditions  for any 0 < (cid:14) < 1 and for any i; j 2 N(cid:15)  with
probability at least (1 (cid:0) (cid:14)) the following holds 
1) (cid:12)(cid:12)h i; jiL2(X ;PX )(cid:12)(cid:12) (cid:20) (cid:18) 8 log(2=(cid:14))
(cid:21)i (cid:19) 1pu (cid:20) k u
2) 1 (cid:0)(cid:18)r 8 log(2=(cid:14))

gNmax (cid:18) 1p(cid:21)i
i kL2(X ;PX ) (cid:20) 1 +(cid:18)r 8 log(2=(cid:14))

u +(cid:18)p8 log(2=(cid:14))

gNmaxq 2 log(2=(cid:14))

Nmaxp(cid:21)i(cid:21)j(cid:19) 1

i kL2(X ;PX ) = 2

+ 1p(cid:21)j(cid:19)(cid:19) 1pu

(cid:21)i (cid:19) 1pu

g2
Nmax

g2
Nmax

0 (x)j (cid:20)

u(cid:21)i

g2

i=1 such that the ith column of (cid:9) is (cid:0) N(cid:15)(i)(x1); N(cid:15)(i)(x2);(cid:1)(cid:1)(cid:1) ; N(cid:15)(i)(xl)(cid:1)T

3.4 Concentration Results
Having established that f igi2N(cid:15) approximate the top N(cid:15) eigenfunctions of LK reasonably well 
next  we need to consider what happens when we restrict each of the is to (cid:2)nite labeled examples.
Note that the design matrix (cid:9) 2 Rl(cid:2)jN(cid:15)j is constructed by restricting the f jgj2N(cid:15) to l labeled data
points fxigl
2 Rl.
Now consider the jN(cid:15)j (cid:2) jN(cid:15)j matrix C = 1
k=1 N(cid:15)(i)(xk) N(cid:15)(j)(xk).
l Pl
First  applying Hoeffding’s inequality we establish 
Lemma 3.4. For all i; j 2 N(cid:15) and (cid:15)1 > 0 the following two facts hold.
P(cid:16)(cid:12)(cid:12)(cid:12)
k=1[ i(xk)]2 (cid:0) E(cid:0)[ i(X)]2(cid:1)(cid:12)(cid:12)(cid:12) (cid:21) (cid:15)1(cid:17) (cid:20) 2 exp(cid:16)(cid:0) l(cid:15)2
2 (cid:17)
l Pl
P(cid:16)(cid:12)(cid:12)(cid:12)
k=1 i(xk) j(xk) (cid:0) E ( i(X) j(X))(cid:12)(cid:12)(cid:12) (cid:21) (cid:15)1(cid:17) (cid:20) 2 exp(cid:16)(cid:0) l(cid:15)2
(cid:17)
l Pl
and C0ii = 1. To en-
Next  consider the jN(cid:15)j (cid:2) jN(cid:15)j normalized matrix C0 where C0ij = Cij
sure that Lasso will consistently choose the correct model we need to show (see Theorem 3.1) that

l (cid:9)T (cid:9) where  Cij = 1

1(cid:21)i(cid:21)j

1(cid:21)2

Cii

1

1

2

i

6

maxi6=j jC0ijj < 1
sample results yields Theorem 3.2.

2q(cid:0)1 with high probability. Applying the above concentration result and (cid:2)nite

4 Experimental Results
4.1 Toy Dataset
Here we present a synthetic example in 2-D. Consider a binary classi(cid:2)cation problem where the
positive examples are generated from a Gaussian distribution with mean (0; 0) and covariance ma-
trix [2 0; 0 2] and the negative examples are generated from a mixture of Gaussians having means
and covariance matrices (5; 5); [2 1; 1 2] and (7; 7); [1:5 0; 0 1:5] respectively. The correspond-
ing mixing weights are 0:4; 0:3 and 0:3 respectively. Left panel in Figure 1 shows the probability
density of the mixture in blue and representative eigenfunctions of each class in green and magenta
respectively using 1000 examples (positive and negative) drawn from this mixture. It is clear that
each representative eigenfunction represents high density area of a particular class reasonably well.
So intuitively a linear combination of them will represent a good decision function. In fact  the
right panel of Fig 1 shows the regularization path for L1 penalized least square regression with 20
labeled examples. The bold green and magenta lines shows the coef(cid:2)cient values for the representa-
tive eigenfunctions for different values of regularization parameter t. As can be seen  regularization
parameter t can be so chosen that the decision function will consist of a linear combination of repre-
sentative eigenfunctions only. Note that these representative eigenfunctions need not be the top two
eigenfunctions corresponding to the largest eigenvalues.

Probability density for the mixture and representative eigenfunctions

Regularization path

s
n
o

0.05

f

i
t
c
n
u
n
e
g
E

i

 
/
 
y
t
i
s
n
e
D

0

−0.05

−0.1

−0.15
−10

−5

0

5

10

15

−20

s
t

i

n
e
c
i
f
f

e
o
C

20

10

0

−10

−20

0

20

0
y

10

20

Figure 1: Left panel: Probability density of the mixture in blue and representative eigenfunctions
in green and magenta. Right panel: Regularization path. Bold lines correspond to regularization
path associated with representative eigenfunctions.

x

30
t

40

50

60

4.2 UCI Datasets
In this set of experiment we tested the effectiveness of our algorithm (we call it SSL SEB) on some
common UCI datasets. We compared our algorithm with state of the art semi-supervised learning
(manifold regularization) method Laplacian SVM (LapSVM) [BNS06]  fully supervised SVM and
also two other kernel sparse regression methods. In KPCA+L1 we selected top jN(cid:15)j eigenvectors 
and applied L1 regularization  in KPCA F+L1 we selected the top 20 ((cid:2)xed) eigenvectors of Ku
and applied L1 regularization1  where as in KPCA max+L1 we selected top max eigenvectors  and
applied L1 regularization  where max is the maximum index of set of eigenvectors in N(cid:15)  that is the
index of the lowest eigenvector  chosen by our method. For both SVM and LapSVM we used RBF
kernel. In each experiment a speci(cid:2)ed number of examples (l) were randomly chosen and labeled
and the rest (u) were treated as unlabeled test set. Such random splitting was performed 30 times
and the average is reported.
The results are reported in Table 1. As can be seen  for small number of labeled examples our method
convincingly outperform SVM and is comparable to LapSVM. The result also suggests that instead
of selecting top few eigenvectors  as is normally done in KPCA  selecting them by our method
and then applying L1 regularization yields better result. In particular  in case of IONOSPHERE
and BREAST-CANCER data sets top jN(cid:15)j (5 and 3 respectively) eigenvectors do not contain the
representative ones. As a result in these two cases KPCA+L1 performs very poorly. Table 2 shows
that the solution obtained by our method is very sparse  where average sparsity is the average number
of non-zero coef(cid:2)cients.
We note that our method does not work equally well for all datasets  and has generally higher
variability than LapSVM.

1We also selected 100 top eigenvectors and applied L1 penalty but it gave worse result.

7

DATA SET
# Labeled Data
SSL SEB

KPCA+L1

KPCA F+L1

KPC max+L1

SVM

LapSVM

HEART

WINE

BREAST-CANCER

VOTING

IONOSPHERE
d=33  l+u=351

l=20

l=30

d=13  l+u=303

l=10
78:26 85:84 87:25 75:45 77:34 79:92 93:01 98:95
96:68
(cid:6)13:56 (cid:6)10:61 (cid:6)4:16 (cid:6)6:14 (cid:6)6:04 (cid:6)1:18 (cid:6)8:49 (cid:6)8:49 (cid:6)3:43
70:26

d=16  l+u=435
l=10
l=15
86:85
87:84
(cid:6)6:21 (cid:6)3:82
65:15
87:84
86:85
(cid:6)8:82 (cid:6)9:81 (cid:6)9:89 (cid:6)7:94 (cid:6)8:41 (cid:6)6:68 (cid:6)10:06 (cid:6)3:89 (cid:6)14:43 (cid:6)13:68 (cid:6)6:21 (cid:6)3:82
77:38
64:92

d=30  l+u=569
l=10
98:66
(cid:6)2:86
73:95

d=13  l+u=178
l=10
l=20

60:91 67:32 71:46

66:82 70:36 75:16

l=10

l=20

l=30

81:44

71:78

69:43

67:43

65:66

69:57

93:47

98:75

l=5

79:82

87:32

63:04

(cid:6)10:13 (cid:6)11:68 (cid:6)11:26 (cid:6)7:33 (cid:6)7:01 (cid:6)5:91 (cid:6)10:29 (cid:6)8:56 (cid:6)12:29 (cid:6)13:12 (cid:6)12:65 (cid:6)10:43

59:76

64:73

66:89

57:26 60:16 63:36

84:62

89:96

59:32

73:95

71:78

77:38

(cid:6)10:23 (cid:6)11:62 (cid:6)12:45 (cid:6)5:16 (cid:6)6:69 (cid:6)6:15 (cid:6)9:63 (cid:6)9:26 (cid:6)15:18 (cid:6)8:97 (cid:6)12:65 (cid:6)10:43

79:8

72:09

65:16

88:51
(cid:6)10:87 (cid:6)10:04 (cid:6)9:94 (cid:6)11:63 (cid:6)5:95 (cid:6)4:29 (cid:6)10:25 (cid:6)11:68 (cid:6)17:56 (cid:6)8:65 (cid:6)16:05 (cid:6)5:88
89:52 89:97
(cid:6)1:43 (cid:6)1:26

98:95
71:17
(cid:6)7:33 (cid:6)4:07 (cid:6)3:81 (cid:6)5:55 (cid:6)6:08 (cid:6)3:14 (cid:6)5:33 (cid:6)1:57 (cid:6)2:32

74:91 75:33 77:43 98:33 97:67

64:61 73:16 76:55

99:72
(cid:6)1:42

77:18

81:32

83:98

88:12

72:83

81:53

97:32

Table 1: Classi(cid:2)cation Accuracies for different UCI datasets

DATA SET
SSL SEB
KPCA+L1
KPCA F+L1
KPC max+L1

IONOSPHERE

2.83 / 5
3.23 / 5
6.05 / 20
6.85 / 23

HEART
4.63 / 9
5.84 / 9
8.11 / 20
16.42 / 78

WINE
3.52 / 6
3.8 / 6
6.12 / 20
6.07 /16

BREAST-CANCER

2.10 / 3
2.78 / 3
4.70 / 20
10.81 / 57

VOTING
2.02 / 3
2.02/ 3
3.05 / 20
2.02 / 3

Table 2: Average sparsity of our method for different UCI datasets. The notation A=B represents
average sparsity A and number of eigenvectors (jN(cid:15)j or 20).

4.3 Handwritten Digit Recognition
In this set of experiments we applied our method to the 45 binary classi(cid:2)cation problems that arise
in pairwise classi(cid:2)cation of handwritten digits and compare its performance with LapSVM. For
each pairwise classi(cid:2)cation problem  in each trial  500 images of each digit in the USPS training
set were chosen uniformly at random out of which 20 images were labeled and the rest were set
aside for testing. This trial was repeated 10 times. For the LapSVM we set the regularization
terms and the kernel as reported by [BNS06] for a similar set of experiments  namely we set (cid:13)Al =
(u+l)2 = 0:045 and chose a polynomial kernel of degree 3. The results are shown2 in Figure2.
0:005;
As can be seen our method is comparable to LapSVM.

(cid:13)I l

20

15

10

5

)

%

(
 

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

0

 
0

 

SSL_SEB
LapSVM

5

10

15

20

25

30

35

40

45

All 45 two class classifications for USPS dataset

Figure 2: Classi(cid:2)cation results for USPS dataset

We also performed multi-class classi(cid:2)cation on USPS dataset. In particular  we chose all the images
of digits 3  4 and 5 from USPS training data set (there were 1866 in total) and randomly labeled
10 images from each class. Rest of the 1836 images were set aside for testing. Average prediction
accuracy of LapSVM  after repeating this procedure 20 times  was 90:14% as compared to 87:53%
of our method.
5 Conclusion
In this paper we have presented a framework for spectral semi-supervised learning based on the
cluster assumption. We showed that the cluster assumption is equivalent to the classi(cid:2)er being
sparse in a certain appropriately chosen basis and demonstrated how such basis can be computed
using only unlabeled data. We have provided theoretical analysis of the resulting algorithm and
given experimental results demonstrating that the resulting algorithm has performance comparable
to the state-of-the-art for a number of data sets and dramatically outperforms the natural baseline of
KPCA + Lasso.

2It turned out that the cases where our method performed very poorly  the respective distances between the

means of corresponding two classes were very small.

8

References
[BNS06] M. Belkin  P. Niyogi  and V. Sindhwani. Manifold Regularization: A Geometric Frame-
work for Learning from Labeled and Unlabeled Examples. Journal of Machine Learning
Research  7:2399(cid:150)2434  2006.

[BPV03] Y. Bengio  J-F. Paiement  and P. Vincent. Out-of-sample Extensions for LLE  Isomap 

[CC96]

[CP07]

MDS  Eigenmaps and Spectral Clustering. In NIPS. 2003.
V. Castelli and T. M. Cover. The Relative Value of Labeled and Unlabeled Samples in
Pattern Recognition with Unknown Mixing Parameters. IEEE Transactions on Informa-
tion Theory  42(6):2102(cid:150)2117  1996.
E. J. Candes and Y. Plan. Near Ideal Model Selection by ‘1 Minimization  eprint
arxiv:0801.0345. 2007.

[CWS02] O. Chapelle  J. Weston  and B. Scholkopf. Cluster Kernels for Semi-supervised Learn-

ing. In NIPS. 2002.
S. Dasgupta. Learning Mixture of Gaussians. In 40th Annual Symposium on Foundations
of Computer Science  1999.

[Das99]

[HTF03] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning Data

Mining  Inference and Prediction. Springer  2003.

[RBV08] L. Rosasco  M. Belkin  and E. De Vito. Perturbation Results for Learning Empirical
Opertors. Technical Report TR-2008-052  Massachusetts Institute of Technology  Cam-
bridge  MA  August 2008.
J. Ratsaby and S. Venkatesh. Learning From a Mixture of Labeled and Unlabeled Ex-
amples with Parametric Side Information. In COLT. 1995.
K. Sinha and M. Belkin. The Value of Labeled and Unlabeled Examples when the Model
is Imperfect. In NIPS. 2007.

[RV95]

[SB07]

[SBY08a] T. Shi  M. Belkin  and B. Yu. Data Spectroscopy: Eigenspace of Convolution Operators

and Clustering. Technical report  Dept. of Statistics  Ohio State University  2008.

[SBY08b] T. Shi  M. Belkin  and B. Yu. Data Spectroscopy: Learning Mixture Models using

Eigenspaces of Convolution Operators. In ICML. 2008.

[SNZ08] A. Singh  R. D. Nowak  and X. Zhu. Unlabeled Data: Now it Helps Now it Doesn’t. In

NIPS. 2008.

[SSM98] Bernhard Scholkopf  A. Smola  and Klaus-Robert Muller. Nonlinear Component Anal-

[Tib96]

[Tro04]

ysis as a Kernel Eigenvalue Problem. Neural Computation  10:1299(cid:150)1319  1998.
R. Tibshirani. Regression Shrinkage and Selection via the Lasso. Journal of the Royal
Statistical Society  Series B  58:267(cid:150)288  1996.
J. A. Tropp. Greed is Good: Algorithmic Result for Sparse Approximation. IEEE Trans.
Info. Theory  50(10):2231(cid:150)2242  2004.

[ZH06]

[Wai06] M. Wainwright. Sharp Thresholds for Noisy and High-dimensional Recovery of Spar-
sity using ‘1-constrained Quadratic Programming. Technical Report TR-709  Dept. of
Statistics  U. C. Berkeley  September 2006.
C. Zhang and J. Huang. Model Selection Consistency of Lasso in High Dimensional
Linear Regression. Technical report  Dept. of Statistics  Rutgers University  2006.
T. Zhang. On consistency of feature selection using greedy least square regression.
Journal of Machine Learning Research  2008.
P. Zhao and B. Yu. On Model Selection Consistency of Lasso. Journal of Machine
Learning Research  7:2541(cid:150)2563  2006.

[Zha08]

[ZY06]

9

,Atsushi Shibagaki
Yoshiki Suzuki
Masayuki Karasuyama
Ichiro Takeuchi
David Harwath
Antonio Torralba
James Glass
Alberto Bietti
Julien Mairal