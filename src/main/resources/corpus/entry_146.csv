2017,A New Theory for Matrix Completion,Prevalent matrix completion theories reply on an assumption that the locations of the missing data are distributed uniformly and randomly (i.e.  uniform sampling). Nevertheless  the reason for observations being missing often depends on the unseen observations themselves  and thus the missing data in practice usually occurs in a nonuniform and deterministic fashion rather than randomly. To break through the limits of random sampling  this paper introduces a new hypothesis called \emph{isomeric condition}  which is provably weaker than the assumption of uniform sampling and arguably holds even when the missing data is placed irregularly. Equipped with this new tool  we prove a series of theorems for missing data recovery and matrix completion. In particular  we prove that the exact solutions that identify the target matrix are included as critical points by the commonly used nonconvex programs. Unlike the existing theories for nonconvex matrix completion  which are built upon the same condition as convex programs  our theory shows that nonconvex programs have the potential to work with a much weaker condition. Comparing to the existing studies on nonuniform sampling  our setup is more general.,A New Theory for Matrix Completion

Guangcan Liu∗

Qingshan Liu†

Xiao-Tong Yuan‡

B-DAT  School of Information & Control  Nanjing Univ Informat Sci & Technol

NO 219 Ningliu Road  Nanjing  Jiangsu  China  210044

{gcliu qsliu xtyuan}@nuist.edu.cn

Abstract

Prevalent matrix completion theories reply on an assumption that the locations of
the missing data are distributed uniformly and randomly (i.e.  uniform sampling).
Nevertheless  the reason for observations being missing often depends on the unseen
observations themselves  and thus the missing data in practice usually occurs in a
nonuniform and deterministic fashion rather than randomly. To break through the
limits of random sampling  this paper introduces a new hypothesis called isomeric
condition  which is provably weaker than the assumption of uniform sampling and
arguably holds even when the missing data is placed irregularly. Equipped with
this new tool  we prove a series of theorems for missing data recovery and matrix
completion. In particular  we prove that the exact solutions that identify the target
matrix are included as critical points by the commonly used nonconvex programs.
Unlike the existing theories for nonconvex matrix completion  which are built
upon the same condition as convex programs  our theory shows that nonconvex
programs have the potential to work with a much weaker condition. Comparing to
the existing studies on nonuniform sampling  our setup is more general.

Introduction

1
Missing data is a common occurrence in modern applications such as computer vision and image
processing  reducing signiﬁcantly the representativeness of data samples and therefore distorting
seriously the inferences about data. Given this pressing situation  it is crucial to study the problem
of recovering the unseen data from a sampling of observations. Since the data in reality is often
organized in matrix form  it is of considerable practical signiﬁcance to study the well-known problem
of matrix completion [1] which is to ﬁll in the missing entries of a partially observed matrix.
Problem 1.1 (Matrix Completion). Denote the (i  j)th entry of a matrix as [·]ij. Let L0 ∈ Rm×n be
an unknown matrix of interest. In particular  the rank of L0 is unknown either. Given a sampling of
the entries in L0 and a 2D index set Ω ⊆ {1  2 ···   m} × {1  2 ···   n} consisting of the locations
of the observed entries  i.e.  given

{[L0]ij|(i  j) ∈ Ω}

and Ω 

can we restore the missing entries whose indices are not included in Ω  in an exact and scalable
fashion? If so  under which conditions?

Due to its unique role in a broad range of applications  e.g.  structure from motion and magnetic
resonance imaging  matrix completion has received extensive attentions in the literatures  e.g.  [2–13].
∗The work of Guangcan Liu is supported in part by national Natural Science Foundation of China (NSFC)
under Grant 61622305 and Grant 61502238  in part by Natural Science Foundation of Jiangsu Province of China
(NSFJPC) under Grant BK20160040.

†The work of Qingshan Liu is supported by NSFC under Grant 61532009.
‡The work of Xiao-Tong Yuan is supported in part by NSFC under Grant 61402232 and Grant 61522308  in

part by NSFJPC under Grant BK20141003.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Left and Middle: Typical conﬁgurations for the locations of the observed entries. Right: A
real example from the Oxford motion database. The black areas correspond to the missing entries.

In general  given no presumption about the nature of matrix entries  it is virtually impossible to
restore L0 as the missing entries can be of arbitrary values. That is  some assumptions are necessary
for solving Problem 1.1. Based on the high-dimensional and massive essence of today’s data-driven
community  it is arguable that the target matrix L0 we wish to recover is often low rank [23]. Hence 
one may perform matrix completion by seeking a matrix with the lowest rank that also satisﬁes the
constraints given by the observed entries:
s.t.

[L]ij = [L0]ij ∀(i  j) ∈ Ω.

rank (L)  

min

(1)

L

Unfortunately  this idea is of little practical because the problem above is NP-hard and cannot be
solved in polynomial time [15]. To achieve practical matrix completion  Candès and Recht [4]
suggested to consider an alternative that minimizes instead the nuclear norm which is a convex
envelope of the rank function [12]. Namely 

(cid:107)L(cid:107)∗ 

[L]ij = [L0]ij ∀(i  j) ∈ Ω 

L

s.t.

min

(2)
where (cid:107) · (cid:107)∗ denotes the nuclear norm  i.e.  the sum of the singular values of a matrix. Rather
surprisingly  it is proved in [4] that the missing entries  with high probability  can be exactly restored
by the convex program (2)  as long as the target matrix L0 is low rank and incoherent and the set Ω of
locations corresponding to the observed entries is a set sampled uniformly at random. This pioneering
work provides people several useful tools to investigate matrix completion and many other related
problems. Those assumptions  including low-rankness  incoherence and uniform sampling  are now
standard and widely used in the literatures  e.g.  [14  17  22  24  28  33  34  36]. In particular  the
analyses in [17  33  36] show that  in terms of theoretical completeness  many nonconvex optimization
based methods are as powerful as the convex program (2). Unfortunately  these theories still depend
on the assumption of uniform sampling  and thus they cannot explain why there are many nonconvex
methods which often do better than the convex program (2) in practice.
The missing data in practice  however  often occurs in a nonuniform and deterministic fashion instead
of randomly. This is because the reason for an observation being missing usually depends on the
unseen observations themselves. For example  in structure from motion and magnetic resonance
imaging  typically the locations of the observed entries are concentrated around the main diagonal of
a matrix4  as shown in Figure 1. Moreover  as pointed out by [19  21  23]  the incoherence condition
is indeed not so consistent with the mixture structure of multiple subspaces  which is also a ubiquitous
phenomenon in practice. There has been sparse research in the direction of nonuniform sampling 
e.g.  [18  25–27  31]. In particular  Negahban and Wainwright [26] studied the case of weighted
entrywise sampling  which is more general than the setup of uniform sampling but still a special
form of random sampling. Király et al. [18] considered deterministic sampling and is most related to
this work. However  they had only established conditions to decide whether a particular entry of the
matrix can be restored. In other words  the setup of [18] may not handle well the dependence among
the missing entries. In summary  matrix completion still starves for practical theories and methods 
although has attained considerable improvements in these years.
To break through the limits of the setup of random sampling  in this paper we introduce a new
hypothesis called isomeric condition  which is a mixed concept that combines together the rank and
coherence of L0 with the locations and amount of the observed entries. In general  isomerism (noun

4This statement means that the observed entries are concentrated around the main diagonal after a permutation

of the sampling pattern Ω.

2

of isomeric) is a very mild hypothesis and only a little bit more strict than the well-known oracle
assumption; that is  the number of observed entries in each row and column of L0 is not smaller than
the rank of L0. It is arguable that the isomeric condition can hold even when the missing entries have
irregular locations. In particular  it is provable that the widely used assumption of uniform sampling
is sufﬁcient to ensure isomerism  not necessary. Equipped with this new tool  isomerism  we prove a
set of theorems pertaining to missing data recovery [35] and matrix completion. For example  we
prove that  under the condition of isomerism  the exact solutions that identify the target matrix are
included as critical points by the commonly used bilinear programs. This result helps to explain the
widely observed phenomenon that there are many nonconvex methods performing better than the
convex program (2) on real-world matrix completion tasks. In summary  the contributions of this
paper mainly include:

(cid:5) We invent a new hypothesis called isomeric condition  which provably holds given the
standard assumptions of uniform sampling  low-rankness and incoherence. In addition 
we also exemplify that the isomeric condition can hold even if the target matrix L0 is not
incoherent and the missing entries are placed irregularly. Comparing to the existing studies
about nonuniform sampling  our setup is more general.
(cid:5) Equipped with the isomeric condition  we prove that the exact solutions that identify L0
are included as critical points by the commonly used bilinear programs. Comparing to the
existing theories for nonconvex matrix completion  our theory is built upon a much weaker
assumption and can therefore partially reveal the superiorities of nonconvex programs over
the convex methods based on (2).
(cid:5) We prove that the isomeric condition is sufﬁcient and necessary for the column and row
projectors of L0 to be invertible given the sampling pattern Ω. This result implies that
the isomeric condition is necessary for ensuring that the minimal rank solution to (1) can
identify the target L0.

The rest of this paper is organized as follows. Section 2 summarizes the mathematical notations used
in the paper. Section 3 introduces the proposed isomeric condition  along with some theorems for
matrix completion. Section 4 shows some empirical results and Section 5 concludes this paper. The
detailed proofs to all the proposed theorems are presented in the Supplementary Materials.

2 Notations
Capital and lowercase letters are used to represent matrices and vectors  respectively  except that the
lowercase letters  i  j  k  m  n  l  p  q  r  s and t  are used to denote some integers  e.g.  the location of
an observation  the rank of a matrix  etc. For a matrix M  [M ]ij is its (i  j)th entry  [M ]i : is its ith row
and [M ]: j is its jth column. Let ω1 and ω2 be two 1D index sets; namely  ω1 = {i1  i2 ···   ik} and
ω2 = {j1  j2 ···   js}. Then [M ]ω1 : denotes the submatrix of M obtained by selecting the rows with
indices i1  i2 ···   ik  [M ]: ω2 is the submatrix constructed by choosing the columns j1  j2 ···   js 
and similarly for [M ]ω1 ω2. For a 2D index set Ω ⊆ {1  2 ···   m} × {1  2 ···   n}  we imagine it
as a sparse matrix and  accordingly  deﬁne its “rows”  “columns” and “transpose” as follows: The
ith row Ωi = {j1|(i1  j1) ∈ Ω  i1 = i}  the jth column Ωj = {i1|(i1  j1) ∈ Ω  j1 = j} and the
transpose ΩT = {(j1  i1)|(i1  j1) ∈ Ω}.
The special symbol (·)+ is reserved to denote the Moore-Penrose pseudo-inverse of a matrix. More
precisely  for a matrix M with Singular Value Decomposition (SVD)5 M = UM ΣM V T
M   its pseudo-
inverse is given by M + = VM Σ−1
M . For convenience  we adopt the conventions of using
span{M} to denote the linear space spanned by the columns of a matrix M  using y ∈ span{M} to
denote that a vector y belongs to the space span{M}  and using Y ∈ span{M} to denote that all the
column vectors of a matrix Y belong to span{M}.
Capital letters U  V   Ω and their variants (complements  subscripts  etc.) are reserved for left singular
vectors  right singular vectors and index set  respectively. For convenience  we shall abuse the
notation U (resp. V ) to denote the linear space spanned by the columns of U (resp. V )  i.e.  the
column space (resp. row space). The orthogonal projection onto the column space U  is denoted by
PU and given by PU (M ) = U U T M  and similarly for the row space PV (M ) = M V V T . The same
5In this paper  SVD always refers to skinny SVD. For a rank-r matrix M ∈ Rm×n  its SVD is of the form

M U T

UM ΣM V T

M   where UM ∈ Rm×r  ΣM ∈ Rr×r and VM ∈ Rn×r.

3

(cid:26) [M ]ij 

0 

if (i  j) ∈ Ω 
otherwise.

[PΩ(M )]ij =

notation is also used to represent a subspace of matrices (i.e.  the image of an operator)  e.g.  we say
that M ∈ PU for any matrix M which satisﬁes PU (M ) = M. We shall also abuse the notation Ω
to denote the linear space of matrices supported on Ω. Then the symbol PΩ denotes the orthogonal
projection onto Ω  namely 

Ω = I  where I is the identity operator.

Ω denotes the orthogonal projection onto the complement space of Ω. That

Similarly  the symbol P⊥
is  PΩ + P⊥
Three types of matrix norms are used in this paper  and they are all functions of the singular values:
1) The operator norm or 2-norm (i.e.  largest singular value) denoted by (cid:107)M(cid:107)  2) the Frobenius norm
(i.e.  square root of the sum of squared singular values) denoted by (cid:107)M(cid:107)F and 3) the nuclear norm
or trace norm (i.e.  sum of singular values) denoted by (cid:107)M(cid:107)∗. The only used vector norm is the (cid:96)2
norm  which is denoted by (cid:107) · (cid:107)2. The symbol | · | is reserved for the cardinality of an index set.

Isomeric Condition and Matrix Completion

3
This section introduces the proposed isomeric condition and a set of theorems for matrix completion.
But most of the detailed proofs are deferred until the Supplementary Materials.

Isomeric Condition

3.1
In general cases  as aforementioned  matrix completion is an ill-posed problem. Thus  some assump-
tions are necessary for studying Problem 1.1. To eliminate the disadvantages of the setup of random
sampling  we deﬁne and investigate a so-called isomeric condition.

3.1.1 Deﬁnitions
For the ease of understanding  we shall begin with a concept called k-isomerism (or k-isomeric in
adjective form)  which could be regarded as an extension of low-rankness.
Deﬁnition 3.1 (k-isomeric). A matrix M ∈ Rm×l is called k-isomeric if and only if any k rows of
M can linearly represent all rows in M. That is 

rank ([M ]ω :) = rank (M )  ∀ω ⊆ {1  2 ···   m} |ω| = k 

where | · | is the cardinality of an index set.
In general  k-isomerism is somewhat similar to Spark [37] which deﬁnes the smallest linearly
dependent subset of the rows of a matrix. For a matrix M to be k-isomeric  it is necessary that
rank (M ) ≤ k  not sufﬁcient. In fact  k-isomerism is also somehow related to the concept of
coherence [4  21]. When the coherence of a matrix M ∈ Rm×l is not too high  the rows of M will
sufﬁciently spread  and thus M could be k-isomeric with a small k  e.g.  k = rank (M ). Whenever
the coherence of M is very high  one may need a large k to satisfy the k-isomeric property. For
example  consider an extreme case where M is a rank-1 matrix with one row being 1 and everywhere
else being 0. In this case  we need k = m to ensure that M is k-isomeric.
While Deﬁnition 3.1 involves all 1D index sets of cardinality k  we often need the isomeric property
to be associated with a certain 2D index set Ω. To this end  we deﬁne below a concept called
Ω-isomerism (or Ω-isomeric in adjective form).
Deﬁnition 3.2 (Ω-isomeric). Let M ∈ Rm×l and Ω ⊆ {1  2  ···   m} × {1  2 ···   n}. Suppose
that Ωj (cid:54)= ∅ (empty set)  ∀1 ≤ j ≤ n. Then the matrix M is called Ω-isomeric if and only if

(cid:1) = rank (M )  ∀j = 1  2 ···   n.

rank(cid:0)[M ]Ωj  :

Note here that only the number of rows in M is required to coincide with the row indices included in
Ω  and thereby l (cid:54)= n is allowable.
Generally  Ω-isomerism is less strict than k-isomerism. Provided that |Ωj| ≥ k ∀1 ≤ j ≤ n  a matrix
M is k-isomeric ensures that M is Ω-isomeric as well  but not vice versa. For the extreme example
where M is nonzero at only one row  interestingly  M can be Ω-isomeric as long as the locations of
the nonzero elements are included in Ω.
With the notation of ΩT = {(j1  i1)|(i1  j1) ∈ Ω}  the isomeric property could be also deﬁned on
the column vectors of a matrix  as shown in the following deﬁnition.

4

Deﬁnition 3.3 (Ω/ΩT -isomeric). Let M ∈ Rm×n and Ω ⊆ {1  2  ···   m}×{1  2 ···   n}. Suppose
Ωi (cid:54)= ∅ and Ωj (cid:54)= ∅  ∀i = 1 ···   m  j = 1 ···   n. Then the matrix M is called Ω/ΩT -isomeric if
and only if M is Ω-isomeric and M T is ΩT -isomeric as well.

To solve Problem 1.1 without the imperfect assumption of missing at random  as will be shown later 
we need to assume that L0 is Ω/ΩT -isomeric. This condition has excluded the unidentiﬁable cases
where any rows or columns of L0 are wholly missing. In fact  whenever L0 is Ω/ΩT -isomeric  the
number of observed entries in each row and column of L0 has to be greater than or equal to the rank
of L0; this is consistent with the results in [20]. Moreover  Ω/ΩT -isomerism has actually well treated
the cases where L0 is of high coherence. For example  consider an extreme case where L0 is 1 at only
one element and 0 everywhere else. In this case  L0 cannot be Ω/ΩT -isomeric unless the nonzero
element is observed. So  generally  it is possible to restore the missing entries of a highly coherent
matrix  as long as the Ω/ΩT -isomeric condition is obeyed.

3.1.2 Basic Properties
While its deﬁnitions are associated with a certain matrix  the isomeric condition is actually character-
izing some properties of a space  as shown in the lemma below.
Lemma 3.1. Let L0 ∈ Rm×n and Ω ⊆ {1  2 ···   m} × {1  2 ···   n}. Denote the SVD of L0 as
U0Σ0V T

0 . Then we have:
1. L0 is Ω-isomeric if and only if U0 is Ω-isomeric.
2. LT

0 is ΩT -isomeric if and only if V0 is ΩT -isomeric.

Proof. It could be manipulated that

Since Σ0V T

0 is row-wisely full rank  we have

0  ∀j = 1 ···   n.

(cid:1)  ∀j = 1 ···   n.

[L0]Ωj  : = ([U0]Ωj  :)Σ0V T

rank(cid:0)[L0]Ωj  :

(cid:1) = rank(cid:0)[U0]Ωj  :

As a result  L0 is Ω-isomeric is equivalent to U0 is Ω-isomeric. In a similar way  the second claim is
proved as well.

It is easy to see that the above lemma is still valid even when the condition of Ω-isomerism is replaced
by k-isomerism. Thus  hereafter  we may say that a space is isomeric (k-isomeric  Ω-isomeric or
ΩT -isomeric) as long as its basis matrix is isomeric. In addition  the isomeric property is subspace
successive  as shown in the next lemma.
Lemma 3.2. Let Ω ⊆ {1  2 ···   m} × {1  2 ···   n} and U0 ∈ Rm×r be the basis matrix of a
Euclidean subspace embedded in Rm. Suppose that U is a subspace of U0  i.e.  U = U0U T
0 U. If U0
is Ω-isomeric then U is Ω-isomeric as well.

Proof. By U = U0U T

0 U and U0 is Ω-isomeric 

rank(cid:0)[U ]Ωj  :
= rank(cid:0)U0U T

(cid:1) = rank(cid:0)([U0]Ωj  :)U T
0 U(cid:1) = rank (U )  ∀1 ≤ j ≤ n.

0 U(cid:1) = rank(cid:0)U T
0 U(cid:1)

The above lemma states that  in one word  the subspace of an isomeric space is isomeric.

3.1.3 Important Properties
As aforementioned  the isometric condition is actually necessary for ensuring that the minimal rank
solution to (1) can identify L0. To see why  let’s assume that U0 ∩ Ω⊥ (cid:54)= {0}  where we denote by
0 the SVD of L0. Then one could construct a nonzero perturbation  denoted as ∆ ∈ U0 ∩ Ω⊥ 
U0Σ0V T
and accordingly  obtain a feasible solution ˜L0 = L0 + ∆ to the problem in (1). Since ∆ ∈ U0  we
have rank( ˜L0) ≤ rank (L0). Even more  it is entirely possible that rank( ˜L0) < rank (L0). Such
a case is unidentiﬁable in nature  as the global optimum to problem (1) cannot identify L0. Thus 

5

to ensure that the global minimum to (1) can identify L0  it is essentially necessary to show that
U0 ∩ Ω⊥ = {0} (resp. V0 ∩ Ω⊥ = {0})  which is equivalent to the operator PU0PΩPU0 (resp.
PV0PΩPV0) is invertible (see Lemma 6.8 of the Supplementary Materials). Interestingly  the isomeric
condition is indeed a sufﬁcient and necessary condition for the operators PU0PΩPU0 and PV0PΩPV0
to be invertible  as shown in the following theorem.
Theorem 3.1. Let L0 ∈ Rm×n and Ω ⊆ {1  2 ···   m} × {1  2 ···   n}. Let the SVD of L0 be
U0Σ0V T

0 . Denote PU0 (·) = U0U T
1. The linear operator PU0PΩPU0 is invertible if and only if U0 is Ω-isomeric.
2. The linear operator PV0PΩPV0 is invertible if and only if V0 is ΩT -isomeric.

0 (·) and PV0(·) = (·)V0V T

0 . Then we have the following:

The necessity stated above implies that the isomeric condition is actually a very mild hypothesis. In
general  there are numerous reasons for the target matrix L0 to be isomeric. Particularly  the widely
used assumptions of low-rankness  incoherence and uniform sampling are indeed sufﬁcient (but not
necessary) to ensure isomerism  as shown in the following theorem.
Theorem 3.2. Let L0 ∈ Rm×n and Ω ⊆ {1  2 ···   m} × {1  2 ···   n}. Denote n1 = max(m  n)
and n2 = min(m  n). Suppose that L0 is incoherent and Ω is a 2D index set sampled uniformly
at random  namely Pr((i  j) ∈ Ω) = ρ0 and Pr((i  j) /∈ Ω) = 1 − ρ0. For any δ > 0  if ρ0 > δ
is obeyed and rank (L0) < δn2/(c log n1) holds for some numerical constant c then  with high
probability at least 1 − n−10
It is worth noting that the isomeric condition can be obeyed in numerous circumstances other than
the case of uniform sampling plus incoherence. For example 

  L0 is Ω/ΩT -isomeric.

1

Ω = {(1  1)  (1  2)  (1  3)  (2  1)  (3  1)} and L0 =

(cid:34) 1

0
0

(cid:35)

 

0
0
0

0
0
0

where L0 is a 3×3 matrix with 1 at (1  1) and 0 everywhere else. In this example  L0 is not incoherent
and the sampling is not uniform either  but it could be veriﬁed that L0 is Ω/ΩT -isomeric.

3.2 Results
In this subsection  we shall show how the isomeric condition can take effect in the context of
nonuniform sampling  establishing some theorems pertaining to missing data recovery [35] as well
as matrix completion.

3.2.1 Missing Data Recovery
Before exploring the matrix completion problem  for the ease of understanding  we would like
to consider a missing data recovery problem studied by Zhang [35]  which could be described as
follows: Let y0 ∈ Rm be a data vector drawn form some low-dimensional subspace  denoted as
y0 ∈ S0 ⊂ Rm. Suppose that y0 contains some available observations in yb ∈ Rk and some missing
entries in yu ∈ Rm−k. Namely  after a permutation 

y0 =

  yb ∈ Rk  yu ∈ Rm−k.

(3)

(cid:21)

(cid:20) yb

yu

Given the observations in yb  we seek to restore the unseen entries in yu. To do this  we consider the
prevalent idea that represents a data vector as a linear combination of the bases in a given dictionary:
(4)
where A ∈ Rm×p is a dictionary constructed in advance and x0 ∈ Rp is the representation of y0.
Utilizing the same permutation used in (3)  we can partition the rows of A into two parts according to
the indices of the observed and missing entries  respectively:

y0 = Ax0 

A =

  Ab ∈ Rk×p  Au ∈ R(m−k)×p.

(5)

(cid:21)

(cid:20) Ab

Au

In this way  the equation in (4) gives that

yb = Abx0

and

yu = Aux0.

6

As we now can see  the unseen data yu could be restored  as long as the representation x0 is retrieved
by only accessing the available observations in yb.
In general cases  there are inﬁnitely many
representations that satisfy y0 = Ax0  e.g.  x0 = A+y0  where (·)+ is the pseudo-inverse of a matrix.
Since A+y0 is the representation of minimal (cid:96)2 norm  we revisit the traditional (cid:96)2 program:

(cid:107)x(cid:107)2
2  

1
2

x

s.t.

min

yb = Abx 

(6)
where (cid:107) · (cid:107)2 is the (cid:96)2 norm of a vector. Under some veriﬁable conditions  the above (cid:96)2 program
is indeed consistently successful in a sense as in the following: For any y0 ∈ S0 with an arbitrary
partition y0 = [yb; yu] (i.e.  arbitrarily missing)  the desired representation x0 = A+y0 is the unique
minimizer to the problem in (6). That is  the unseen data yu is exactly recovered by ﬁrstly computing
the minimizer x∗ to problem (6) and then calculating yu = Aux∗.
Theorem 3.3. Let y0 = [yb; yu] ∈ Rm be an authentic sample drawn from some low-dimensional
subspace S0 embedded in Rm  A ∈ Rm×p be a given dictionary and k be the number of available
observations in yb. Then the convex program (6) is consistently successful  provided that S0 ⊆
span{A} and the dictionary A is k-isomeric.
Unlike the theory in [35]  the condition of which is unveriﬁable  our k-isomeric condition could be
veriﬁed in ﬁnite time. Notice  that the problem of missing data recovery is closely related to matrix
completion  which is actually to restore the missing entries in multiple data vectors simultaneously.
Hence  Theorem 3.3 can be naturally generalized to the case of matrix completion  as will be shown
in the next subsection.

3.2.2 Matrix Completion
The spirits of the (cid:96)2 program (6) can be easily transferred to the case of matrix completion. Follow-
ing (6)  one may consider Frobenius norm minimization for matrix completion:

1
2

(cid:107)X(cid:107)2

F   s.t. PΩ(AX − L0) = 0 

min
X

(7)
where A ∈ Rm×p is a dictionary assumed to be given. As one can see  the problem in (7) is equivalent
to (6) if L0 is consisting of only one column vector. The same as (6)  the convex program (7) can
also exactly recover the desired representation matrix A+L0  as shown in the theorem below. The
difference is that we here require Ω-isomerism instead of k-isomerism.
Theorem 3.4. Let L0 ∈ Rm×n and Ω ⊆ {1  2 ···   m} × {1  2 ···   n}. Suppose that A ∈ Rm×p
is a given dictionary. Provided that L0 ∈ span{A} and A is Ω-isomeric  the desired representation
X0 = A+L0 is the unique minimizer to the problem in (7).

Theorem 3.4 tells us that  in general  even when the locations of the missing entries are interrelated
and nonuniformly distributed  the target matrix L0 can be restored as long as we have found a proper
dictionary A. This motivates us to consider the commonly used bilinear program that seeks both A
and X simultaneously:

1
2

1
2

(cid:107)A(cid:107)2

(cid:107)X(cid:107)2

F   s.t. PΩ(AX − L0) = 0 

F +

min
A X

(8)
where A ∈ Rm×p and X ∈ Rp×n. The problem above is bilinear and therefore nonconvex. So  it
would be hard to obtain a strong performance guarantee as done in the convex programs  e.g.  [4  21].
Interestingly  under a very mild condition  the problem in (8) is proved to include the exact solutions
that identify the target matrix L0 as the critical points.
Theorem 3.5. Let L0 ∈ Rm×n and Ω ⊆ {1  2 ···   m} × {1  2 ···   n}. Denote the rank and SVD
0   respectively. If L0 is Ω/ΩT -isomeric then the exact solution  denoted by
of L0 as r0 and U0Σ0V T
(A0  X0) and given by

A0 = U0Σ

1
2

0 QT   X0 = QΣ

is a critical point to the problem in (8).

1
2

0  ∀Q ∈ Rp×r0  QT Q = I 

0 V T

To exhibit the power of program (8)  however  the parameter p  which indicates the number of
columns in the dictionary matrix A  must be close to the true rank of the target matrix L0. This is

7

Figure 2: Comparing the bilinear program (9) (p = m) with the convex method (2). The numbers
plotted on the above ﬁgures are the success rates within 20 random trials. The white and black points
mean “succeed” and “fail”  respectively. Here the success is in a sense that PSNR ≥ 40dB  where
PSNR standing for peak signal-to-noise ratio.

impractical in the cases where the rank of L0 is unknown. Notice  that the Ω-isomeric condition
imposed on A requires

rank (A) ≤ |Ωj| ∀j = 1  2 ···   n.

min
A X

1
2

This  together with the condition of L0 ∈ span{A}  essentially need us to solve a low rank matrix
recovery problem [14]. Hence  we suggest to combine the formulation (7) with the popular idea of
nuclear norm minimization  resulting in a bilinear program that jointly estimates both the dictionary
matrix A and the representation matrix X by
(cid:107)X(cid:107)2

F   s.t. PΩ(AX − L0) = 0 

(cid:107)A(cid:107)∗ +

(9)

which  by coincidence  has been mentioned in a paper about optimization [32]. Similar to (8)  the
program in (9) has the following theorem to guarantee its performance.
Theorem 3.6. Let L0 ∈ Rm×n and Ω ⊆ {1  2 ···   m} × {1  2 ···   n}. Denote the rank and SVD
of L0 as r0 and U0Σ0V T
0   respectively. If L0 is Ω/ΩT -isomeric then the exact solution  denoted by
(A0  X0) and given by

A0 = U0Σ

2
3

0 QT   X0 = QΣ

is a critical point to the problem in (9).

1
3

0  ∀Q ∈ Rp×r0  QT Q = I 

0 V T

Unlike (8)  which possesses superior performance only if p is close to rank (L0) and the initial
solution is chosen carefully  the bilinear program in (9) can work well by simply choosing p = m
and using A = I as the initial solution. To see why  one essentially needs to ﬁgure out the conditions
under which a speciﬁc optimization procedure can produce an optimal solution that meets an exact
solution. This requires extensive justiﬁcations and we leave it as future work.

4 Simulations
To verify the superiorities of the nonconvex matrix completion methods over the convex program (2) 
we would like to experiment with randomly generated matrices. We generate a collection of m × n
(m = n = 100) target matrices according to the model of L0 = BC  where B ∈ Rm×r0 and
C ∈ Rr0×n are N (0  1) matrices. The rank of L0  i.e.  r0  is conﬁgured as r0 = 1  5  10 ···   90  95.
Regarding the index set Ω consisting of the locations of the observed entries  we consider t-
wo settings: One is to create Ω by using a Bernoulli model to randomly sample a subset from
{1 ···   m} × {1 ···   n} (referred to as “uniform”)  the other is as in Figure 1 that makes the
locations of the observed entries be concentrated around the main diagonal of a matrix (referred to as
“nonuniform”). The observation fraction is set to be |Ω|/(mn) = 0.01  0.05 ···   0.9  0.95. For each
pair of (r0 |Ω|/(mn))  we run 20 trials  resulting in 8000 simulations in total.
When p = m and the identity matrix is used to initialize the dictionary A  we have empirically found
that program (8) has the same performance as (2). This is not strange  because it has been proven
in [16] that (cid:107)L(cid:107)∗ = minA X
F )  s.t. L = AX. Figure 2 compares the bilinear

F + (cid:107)X(cid:107)2

2 ((cid:107)A(cid:107)2

1

8

convex (nonuniform)rank(L0)observed entries (%)1153555759595755535151nonconvex (nonuniform)rank(L0)1153555759595755535151convex (uniform)rank(L0)1153555759595755535151nonconvex (uniform)rank(L0)1153555759595755535151program (9) to the convex method (2). It can be seen that (9) works distinctly better than (2). Namely 
while handling the nonuniformly missing data  the number of matrices successfully restored by the
bilinear program (9) is 102% more than the convex program (2). Even for dealing with the missing
entries chosen uniformly at random  in terms of the number of successfully restored matrices  the
bilinear program (9) can still outperform the convex method (2) by 44%. These results illustrate that 
even in the cases where the rank of L0 is unknown  the bilinear program (9) can do much better than
the convex optimization based method (2).

5 Conclusion and Future Work
This work studied the problem of matrix completion with nonuniform sampling  a signiﬁcant setting
not extensively studied before. To ﬁgure out the conditions under which exact recovery is possible 
we proposed a so-called isomeric condition  which provably holds when the standard assumptions
of low-rankness  incoherence and uniform sampling arise. In addition  we also exempliﬁed that
the isomeric condition can be obeyed in the other cases beyond the setting of uniform sampling.
Even more  our theory implies that the isomeric condition is indeed necessary for making sure
that the minimal rank completion can identify the target matrix L0. Equipped with the isomeric
condition  ﬁnally  we mathematically proved that the widely used bilinear programs can include the
exact solutions that recover the target matrix L0 as the critical points; this guarantees the recovery
performance of bilinear programs to some extend.
However  there still remain several problems for future work. In particular  it is unknown under which
conditions a speciﬁc optimization procedure for (9) can produce an optimal solution that exactly
restores the target matrix L0. To do this  one needs to analyze the convergence property as well as
the recovery performance. Moreover  it is unknown either whether the isomeric condition sufﬁces
for ensuring that the minimal rank completion can identify the target L0. These require extensive
justiﬁcations and we leave them as future work.

Acknowledgment
We would like to thanks the anonymous reviewers and meta-reviewers for providing us many valuable
comments to reﬁne this paper.

References
[1] Emmanuel Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix completion.

IEEE Transactions on Information Theory  56(5):2053–2080  2009.

[2] Emmanuel Candès and Yaniv Plan. Matrix completion with noise. In IEEE Proceeding  volume 98  pages

925–936  2010.

[3] William E. Bishop and Byron M. Yu. Deterministic symmetric positive semideﬁnite matrix completion.

In Neural Information Processing Systems  pages 2762–2770  2014.

[4] Emmanuel Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational Mathematics  9(6):717–772  2009.

[5] Eyal Heiman  Gideon Schechtman  and Adi Shraibman. Deterministic algorithms for matrix completion.

Random Structures and Algorithms  45(2):306–317  2014.

[6] Raghunandan H. Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from a few entries.

IEEE Transactions on Information Theory  56(6):2980–2998  2010.

[7] Raghunandan H. Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from noisy entries.

Journal of Machine Learning Research  11:2057–2078  2010.

[8] Akshay Krishnamurthy and Aarti Singh. Low-rank matrix and tensor completion via adaptive sampling.

In Neural Information Processing Systems  pages 836–844  2013.

[9] Troy Lee and Adi Shraibman. Matrix completion from any given set of observations. In Neural Information

Processing Systems  pages 1781–1787  2013.

[10] Rahul Mazumder  Trevor Hastie  and Robert Tibshirani. Spectral regularization algorithms for learning

large incomplete matrices. Journal of Machine Learning Research  11:2287–2322  2010.

[11] Karthik Mohan and Maryam Fazel. New restricted isometry results for noisy low-rank recovery. In IEEE

International Symposium on Information Theory  pages 1573–1577  2010.

9

[12] B. Recht  W. Xu  and B. Hassibi. Necessary and sufﬁcient conditions for success of the nuclear norm

heuristic for rank minimization. Technical report  CalTech  2008.

[13] Markus Weimer  Alexandros Karatzoglou  Quoc V. Le  and Alex J. Smola. Coﬁ rank - maximum margin

matrix factorization for collaborative ranking. In Neural Information Processing Systems  2007.

[14] Emmanuel J. Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component analysis?

Journal of the ACM  58(3):1–37  2011.

[15] Alexander L. Chistov and Dima Grigoriev. Complexity of quantiﬁer elimination in the theory of alge-
braically closed ﬁelds. In Proceedings of the Mathematical Foundations of Computer Science  pages
17–31  1984.

[16] Maryam Fazel  Haitham Hindi  and Stephen P. Boyd. A rank minimization heuristic with application to

minimum order system approximation. In American Control Conference  pages 4734–4739  2001.

[17] Rong Ge  Jason D. Lee  and Tengyu Ma. Matrix completion has no spurious local minimum. In Neural

Information Processing Systems  pages 2973–2981  2016.

[18] Franz J. Király  Louis Theran  and Ryota Tomioka. The algebraic combinatorial approach for low-rank

matrix completion. J. Mach. Learn. Res.  16(1):1391–1436  January 2015.

[19] Guangcan Liu and Ping Li. Recovery of coherent data via low-rank dictionary pursuit.

Information Processing Systems  pages 1206–1214  2014.

In Neural

[20] Daniel L. Pimentel-Alarcón and Robert D. Nowak. The Information-theoretic requirements of subspace

clustering with missing data. In International Conference on Machine Learning  48:802–810  2016.

[21] Guangcan Liu and Ping Li. Low-rank matrix completion in the presence of high coherence.

Transactions on Signal Processing  64(21):5623–5633  2016.

IEEE

[22] Guangcan Liu  Zhouchen Lin  Shuicheng Yan  Ju Sun  Yong Yu  and Yi Ma. Robust recovery of subspace
structures by low-rank representation. IEEE Transactions on Pattern Recognition and Machine Intelligence 
35(1):171–184  2013.

[23] Guangcan Liu  Qingshan Liu  and Ping Li. Blessing of dimensionality: Recovering mixture data via
dictionary pursuit. IEEE Transactions on Pattern Recognition and Machine Intelligence  39(1):47–60 
2017.

[24] Guangcan Liu  Huan Xu  Jinhui Tang  Qingshan Liu  and Shuicheng Yan. A deterministic analysis for

LRR. IEEE Transactions on Pattern Recognition and Machine Intelligence  38(3):417–430  2016.

[25] Raghu Meka  Prateek Jain  and Inderjit S. Dhillon. Matrix completion from power-law distributed samples.

In Neural Information Processing Systems  pages 1258–1266  2009.

[26] Sahand Negahban and Martin J. Wainwright. Restricted strong convexity and weighted matrix completion:

Optimal bounds with noise. Journal of Machine Learning Research  13:1665–1697  2012.

[27] Yudong Chen  Srinadh Bhojanapalli  Sujay Sanghavi  and Rachel Ward. Completing any low-rank matrix 

provably. Journal of Machine Learning Research  16: 2999-3034  2015.

[28] Praneeth Netrapalli  U. N. Niranjan  Sujay Sanghavi  Animashree Anandkumar  and Prateek Jain. Non-

convex robust PCA. In Neural Information Processing Systems  pages 1107–1115  2014.

[29] Yuzhao Ni  Ju Sun  Xiaotong Yuan  Shuicheng Yan  and Loong-Fah Cheong. Robust low-rank subspace
segmentation with semideﬁnite guarantees. In International Conference on Data Mining Workshops  pages
1179–1188  2013.

[30] R. Rockafellar. Convex Analysis. Princeton University Press  Princeton  NJ  USA  1970.
[31] Ruslan Salakhutdinov and Nathan Srebro. Collaborative ﬁltering in a non-uniform world: Learning with

the weighted trace norm. In Neural Information Processing Systems  pages 2056–2064  2010.

[32] Fanhua Shang  Yuanyuan Liu  and James Cheng. Scalable algorithms for tractable schatten quasi-norm

minimization. In AAAI Conference on Artiﬁcial Intelligence  pages 2016–2022  2016.

[33] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.

Transactions on Information Theory  62(11):6535–6579  2016.

IEEE

[34] Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Robust PCA via outlier pursuit. IEEE Transactions

on Information Theory  58(5):3047–3064  2012.

[35] Yin Zhang. When is missing data recoverable? CAAM Technical Report TR06-15  2006.
[36] Tuo Zhao  Zhaoran Wang  and Han Liu. A nonconvex optimization framework for low rank matrix

estimation. In Neural Information Processing Systems  pages 559–567  2015.

[37] David L. Donoho and Michael Elad. Optimally sparse representation in general (nonorthogonal) dic-
tionaries via (cid:96)1 minimization. Proceedings of the National Academy of Sciences  100(5): 2197-2202 
2003.

10

,Alexey Dosovitskiy
Jost Tobias Springenberg
Martin Riedmiller
Thomas Brox
Xingjian SHI
Zhourong Chen
Hao Wang
Dit-Yan Yeung
Jason Weston
Guangcan Liu