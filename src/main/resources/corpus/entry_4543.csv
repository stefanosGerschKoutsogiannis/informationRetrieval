2017,Random Projection Filter Bank for Time Series Data,We propose Random Projection Filter Bank (RPFB) as a generic and simple approach to extract features from time series data. RPFB is a set of randomly generated stable autoregressive filters that are convolved with the input time series to generate the features. These features can be used by any conventional machine learning algorithm for solving tasks such as time series prediction  classification with time series data  etc. Different filters in RPFB extract different aspects of the time series  and together they provide a reasonably good summary of the time series. RPFB is easy to implement  fast to compute  and parallelizable. We provide an error upper bound indicating that RPFB provides a reasonable approximation to a class of dynamical systems. The empirical results in a series of synthetic and real-world problems show that RPFB is an effective method to extract features from time series.,Random Projection Filter Bank for Time Series Data

Amir-massoud Farahmand

Mitsubishi Electric Research Laboratories (MERL)

Cambridge  MA  USA
farahmand@merl.com

Mitsubishi Electric Research Laboratories (MERL)

Sepideh Pourazarm

Cambridge  MA  USA

sepid@bu.edu

Mitsubishi Electric Research Laboratories (MERL)

Daniel Nikovski

Cambridge  MA  USA
nikovski@merl.com

Abstract

We propose Random Projection Filter Bank (RPFB) as a generic and simple
approach to extract features from time series data. RPFB is a set of randomly
generated stable autoregressive ﬁlters that are convolved with the input time series
to generate the features. These features can be used by any conventional machine
learning algorithm for solving tasks such as time series prediction  classiﬁcation
with time series data  etc. Different ﬁlters in RPFB extract different aspects of
the time series  and together they provide a reasonably good summary of the time
series. RPFB is easy to implement  fast to compute  and parallelizable. We provide
an error upper bound indicating that RPFB provides a reasonable approximation
to a class of dynamical systems. The empirical results in a series of synthetic and
real-world problems show that RPFB is an effective method to extract features
from time series.

1

Introduction

This paper introduces Random Projection Filter Bank (RPFB) for feature extraction from time series
data. RPFB generates a feature vector that summarizes the input time series by projecting the time
series onto the span of a set of randomly generated dynamical ﬁlters. The output of RPFB can
then be used as the input to any conventional estimator (e.g.  ridge regression  SVM  and Random
Forest [Hastie et al.  2001; Bishop  2006; Wasserman  2007]) to solve problems such as time series
prediction  and classiﬁcation and fault prediction with time series input data. RPFB is easy to
implement  is fast to compute  and can be parallelized easily.
RPFB consists of a set of randomly generated ﬁlters (i.e.  dynamical systems that receive inputs) 
which are convolved with the input time series. The ﬁlters are stable autoregressive (AR) ﬁlters  so
they can capture information from the distant past of the time series. This is in contrast with more
conventional approach of considering only a ﬁxed window of the past time steps  which may not
capture all relevant information. RPFB is inspired from the random projection methods [Vempala 
2004; Baraniuk and Wakin  2009]  which reduce the input dimension while preserving important
properties of the data  e.g.  being an approximate isometric map.
It is also closely related to

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Random Kitchen Sink [Rahimi and Recht  2009] for approximating potentially inﬁnite-dimensional
reproducing kernel Hilbert space (RKHS) with a ﬁnite set of randomly selected features. RPFB can
be thought of as the dynamical system (or ﬁlter) extension of these methods. RPFB is also related to
the methods in the Reservoir Computing literature [Lukoševiˇcius and Jaeger  2009] such as Echo
State Network and Liquid State Machine  in which a recurrent neural network (RNN) with random
weights provides a feature vector to a trainable output layer. The difference of RPFB with them is that
we are not considering an RNN as the underlying excitable dynamical system  but a set of AR ﬁlters.
The algorithmic contribution of this work is the introduction of RPFB as a generic and simple to use
feature extraction method for time series data (Section 3). RPFB is a particularly suitable choice for
industrial applications where the available computational power is limited  e.g.  a fault prognosis
system for an elevator that has only a micro-controller available. For these industrial applications  the
use of powerful methods such as various adaptable RNN architectures [Hochreiter and Schmidhuber 
1997; Cho et al.  2014; Oliva et al.  2017; Goodfellow et al.  2016]  which learn the feature extractor
itself  might be computationally infeasible.
The theoretical contribution of this work is the ﬁnite sample analysis of RPFB for the task of time
series prediction (Section 4). The theory has two main components. The ﬁrst is a ﬁlter approximation
error result  which provides an error guarantee on how well one might approximate a certain class of
dynamical systems with a set of randomly generated ﬁlters. The second component is a statistical
result providing a ﬁnite-sample guarantee for time series prediction with a generic class of linear
systems. Combining these two  we obtain a ﬁnite-sample guarantee for the use of RPFB for time
series prediction of a certain class of dynamical systems.
Finally  we empirically study RPFB along several standard estimators on a range of synthetic and
real-world datasets (Section 5). Our synthetic data is based on Autoregressive Moving Average
(ARMA) processes. This lets us closely study various aspects of the method. Moving to real-world
problems  we apply RPFB to the fault diagnosis problem from ball bearing vibration measurements.
We compare the performance of RPFB with that of the ﬁxed-window history-based approach  as
well as LSTM  and we obtain promising empirical results. Due to space limitation  most of the
development of the theory and experimental results are reported in the supplementary material  which
is an extended version of this paper. For more empirical studies  especially in the context of fault
detection and prognosis  refer to Pourazarm et al. [2017].

2 Learning from Time Series Data
Consider a sequence (X1  Y1)  . . .   (XT   YT ) of dependent random variables with X ∈ X and
Y ∈ Y. Depending on how Xt and Yt are deﬁned  we can describe different learning/decision
making problems. For example  suppose that Yt = f∗(Xt) + εt  in which f∗ is an unknown function
of the current value of Xt and εt is independent of the history X1:t = (X1  . . .   Xt) and has a
zero expectation  i.e.  E [εt] = 0. Finding an estimate ˆf of f∗ using data is the standard regression
(or classiﬁcation) problem depending on whether Y ⊂ R (regression) or Y = {0  1  . . .   c −
1} (classiﬁcation). For example  suppose that we are given a dataset of m time series Dm =
{(Xi 1  Yi 1)  . . .   (Xi Ti  Yi Ti)}m
i=1  each of which might have a varying length Ti. There are many
methods to deﬁne an estimator for f∗  e.g.  K-Nearest Neighbourhood  decision tree  SVM  various
neural networks [Hastie et al.  2001; Bishop  2006; Wasserman  2007; Goodfellow et al.  2016]. An
important class of estimators is based on (regularized) empirical risk minimization (ERM):

m(cid:88)

Ti(cid:88)

l(f (Xi t)  Yi t) + λJ(f ).

(1)

ˆf ← argmin
f∈F

1
m

1
Ti

i=1

t=1

Here F : X → Y(cid:48) is a function space (e.g.  an RKHS with the domain X ; with Y(cid:48) = R). The loss
function is l : Y(cid:48) × Y → [0 ∞)  and it determines the decision problem that is being solved  e.g. 
l(y1  y2) = |y1 − y2|2 for the squared loss commonly used in regression. The optional regularizer
(or penalizer) J(f ) controls the complexity of the function space  e.g.  it can be (cid:107)f(cid:107)2F when F is an
RKHS. The difference of this scenario with more conventional scenarios in the supervised learning
and statistics is that here the input data does not satisfy the usual independence assumption anymore.
Learning with dependent input data has been analyzed before [Steinwart et al.  2009; Steinwart and
Christmann  2009; Mohri and Rostamizadeh  2010; Farahmand and Szepesvári  2012].

2

m(cid:88)

Ti(cid:88)

ˆf ← argmin
f∈F

1
m

1

Ti − H

More generally  however  Yt is not a function of only Xt  but is a function of the history X1:t  possibly
contaminated by a (conditionally) independent noise: Yt = f∗(X1:t) + εt. In the learning problem 
f∗ is an unknown function. The special case of f∗(X1:t) = f∗(Xt) is the same as the previous
setting.
To learn an estimator by directly using the history X1:t is challenging as it is a time-varying vector
with an ever increasing dimension. A standard approach to deal with this issue is to use a ﬁxed-
window history-based estimator  which shall be explained next (cf. Kakade et al. [2017] for some
recent theoretical results). The RPFB is an alternative approach that we describe in Section 3.
In the ﬁxed-window history-based approach (or window-based  for short)  we only look at a ﬁxed
window of the immediate past values of X1:t. That is  we use samples in the form of Zt (cid:44) Xt−H+1:t
with a ﬁnite integer H that determines the length of the window. For example  the regularized
least-squares regression estimator would then be

|f (Xi t−H+1:t)) − Yi t|2 + λJ(f ) 

(2)

i=1
which should be compared to (1).
A problem with this approach is that for some stochastic processes  a ﬁxed-sized window of length H
is not enough to capture all information about the process. As a simple illustrative example  consider
a simple moving average MA(1) univariate random process (i.e.  X = R):

t=H

Xt = U (t) + bU (t − 1) = (1 + bz−1)Ut 

b ∈ (−1  1)

in which z−1 is the time-delay operator (cf. Z-transform  Oppenheim et al. 1999)  i.e.  z−1Xt = Xt−1.
Suppose that Ut = U (t) (t = 1  2  . . . ) is an unobservable random process that drives Xt. For
example  it might be an independent and identically distributed (i.i.d.) Gaussian noise  which we do
not observe (so it is our latent variable). To predict Yt = Xt+1 given the previous observations X1:t 
we write Ut = Xt

1+bz−1   so

Xt+1 = Ut+1 + bUt = Ut+1 +

b

1 + bz−1 Xt = Ut+1 + b

(−b)kXt−k.

(3)

This means that Xt is an autoregressive process AR(∞). The prediction of Xt+1 requires the value of
Ut+1  which is unavailable at time t  and all the past values X1:t. Since Ut+1 is unavailable  we cannot
use it in our estimate  so this is the intrinsic difﬁculty of prediction. On the other hand  the values of
X1:t are available to us and we can use them to predict Xt+1. But if we use a ﬁxed-horizon window
of the past values (i.e.  only use Xt−H+1:t for a ﬁnite H ≥ 1)  we would miss some information
that could potentially be used. This loss of information is more prominent when the magnitude of
b is close to 1. This example shows that even for a simple MA(1) process with unobserved latent
variables  a ﬁxed-horizon window is not a complete summary of the stochastic process.
More generally  suppose that we have a univariate linear ARMA process

A(z−1)Xt = B(z−1)Ut 

(4)
with A and B both being polynomials in z−1.1 The random process Ut is not available to us 
and we want to design a predictor (ﬁlter) for Xt+1 based on the observed values X1:t. Sup-
pose that A and B are of degree more than 1  so we can write A(z−1) = 1 + z−1A(cid:48)(z−1) and
B(z−1) = 1 + z−1B(cid:48)(z−1).2 Assuming that A and B are both invertible  we use (4) to get Ut =
B−1(z−1)A(z−1)Xt. Also we can write (4) as (1+z−1A(cid:48)(z−1))Xt+1 = (1+z−1B(cid:48)(z−1))Ut+1 =
Ut+1 + B(cid:48)(z−1)Ut. Therefore  we have

(cid:88)

k≥0

(cid:20) B(cid:48)(z−1)A(z−1)

B(z−1)

(cid:21)

− A(cid:48)(z−1)

Xt = Ut+1 +

B(cid:48)(z−1) − A(cid:48)(z−1)

B(z−1)

Xt.

(5)

Xt+1 = Ut+1 +

So if the unknown noise process Ut has a zero mean (i.e.  E [Ut|U1:t−1] = 0)  the estimator

ˆXt+1(X1:t) =

B(cid:48)(z−1) − A(cid:48)(z−1)

B(z−1)

Xt 

(6)

1We assume that A and B both have roots within the unit circle  i.e.  they are stable.
2The fact that both of these polynomials have a leading term of 1 does not matter in this argument.

3

is unbiased  i.e.  ˆXt+1(X1:t) = E [Xt+1|X1:t].
If we knew the model of the dynamical system (A and B)  we could design the ﬁlter (6) to provide an
unbiased prediction for the future values of Xt+1. If the learning problem is such that it requires us
to know an estimate of the future observations of the dynamical system  this scheme would allow us
to design such an estimator. The challenge here is that we often do not know A and B (or similar for
other types of dynamical systems). Estimating A and B for a general dynamical system is a difﬁcult
task. The use of maximum likelihood-based approaches is prone to local minimum since U is not
known  and one has to use EM-like algorithms  cf. White et al. [2015] and references therein. Here
we suggest a simple alternative based on the idea of projecting the signal onto the span of randomly
generated dynamical systems. This would be RPFB  which we describe next.

3 Random Projection Filter Bank

The idea behind RPFB is to randomly generate many simple dynamical systems that can approximate
dynamical systems such as the optimal ﬁlter in (6) with a high accuracy. Denote the linear ﬁlter in (6)
as

for two polynomials p and q  both in z−1. Suppose that deg(q) = deg(B) = dq and deg(A) = dA 
then deg(p) = dp = max{dA − 1  dq − 1}. Assume that q has roots z1  . . .   zdq ∈ C without any
multiplicity. This means that

B(cid:48)(z−1) − A(cid:48)(z−1)

B(z−1)

p(z−1)
q(z−1)

 

=

q(z−1) =

(z−1 − zi).

dq(cid:89)

i=1

dq(cid:88)

i=1

4

In complex analysis in general  and in control engineering and signal processing in particular  the roots
of q are known as the poles of the dynamical system and the roots of p are its zeros. Any discrete-time
linear time-invariant (LTI) dynamical system has such a frequency domain representation.3
We have two cases of either dp < dq or dp ≥ dq. We focus on the ﬁrst case and describe the RPFB 
and the intuition behind it. Afterwards we will discuss the second case.
Case 1: Suppose that dp < dq  which implies that dA − 1 < dq. We may write

p(z−1)
q(z−1)

=

bi

1 − ziz−1  

(7)

for some choice of bis. This means that we can write (5) as

Xt+1 = Ut+1 +

= Ut+1 +

dq(cid:88)

i=1

B(cid:48)(z−1) − A(cid:48)(z−1)

B(z−1)

Xt

bi

1 − ziz−1 Xt.

That is  if we knew the set of complex poles Zp = {z1  . . .   zdq} and their corresponding coefﬁcients
Bp = {b1  . . .   bdq}  we could provide an unbiased estimate of Xt+1 based on X1:t. From now on 
we assume that the underlying unknown system is a stable one  that is  |zi| ≤ 1.
Random projection ﬁlter bank is based on randomly generating many simple stable dynamical
systems  which is equivalent to generating many random poles within the unit circle. Since any stable
LTI ﬁlter has a representation (7) (or a similar one in Case 2)  we can approximate the true dynamical
system as a linear combination of randomly generated poles (i.e.  ﬁlters). If the number of ﬁlters is
large enough  the approximation will be accurate.
To be more precise  we cover the set of {z ∈ C : |z| ≤ 1} with N (ε) random points Nε =
N (ε)} such that for any zi ∈ Zp  there exists a Z(cid:48) ∈ Nε with |zi − Z(cid:48)(zi)| < ε. Roughly
{Z(cid:48)
1  . . .   Z(cid:48)
3For continuous-time systems  we may use Laplace transform instead of Z-transform  and have similar

representations.

speaking  we require N (ε) = O(ε−2) random points to cover the unit circle with the resolution of ε.
We then deﬁne the RPFB as the following set of AR ﬁlters denoted by φ(z−1):4

(cid:32)

(cid:33)

φ(z−1) : z−1 (cid:55)→

1
1 − Z(cid:48)
1z−1   . . .  

1
1 − Z(cid:48)
N (ε)z−1

.

(8)

With a slight abuse of notation  we use φ(X) to refer to the (multivariate) time series generated after
passing a signal X = (X1  . . .   Xt) through the set of ﬁlters φ(z−1). More concretely  this means
iz−1 (i = 1  . . .  N (ε)).
that we convolve the signal X with the impulse response of each of ﬁlters
1−az−1 is the sequence (at)t≥0  and the convolution X ∗ Y
Recall that the impulse response of
between two sequences (Xt)t≥0 and (Yt)t≥0 is a new sequence

1
1−Z(cid:48)

1

(X ∗ Y )t =

Xτ Yt−τ .

(9)

(cid:88)

τ

We use [φ(X)]i ∈ CN (ε) to refer to the i-th time-step of the multivariate signal φ(X1:i).
The intuition of why this is a good construction is that whenever |z1 − z2| is small  the behaviour
of ﬁlter
1−z2z−1 . So whenever Nε provides a good coverage of the unit circle 
there exists a sequence (b(cid:48)

1−z1z−1 is similar to

j) such that the dynamical system

1

1

= φ(z−1)b(cid:48) =

p(cid:48)(z−1)
q(cid:48)(z−1)
q (7). As this is a linear model  parameters b(cid:48) can be estimated using
behaves similar to the unknown p
ordinary least-squares regression  ridge regression  Lasso  etc. For example  the ridge regression
m(cid:88)
estimator for b(cid:48) is

b(cid:48)
1 − Z(cid:48)
jz−1

Ti(cid:88)

j=1

j

|[φ(Xi)]tb − Xi t+1|2 + λ(cid:107)b(cid:107)2
2 .

ˆb ← argmin

1
m

1
Ti

i=1

t=1

N (ε)(cid:88)

After obtaining ˆb  we deﬁne

b

ˆbj
1 − Z(cid:48)
jz−1 X1:t 
which is an estimator of ˆX(X1:t) (6)  i.e.  ˆX(X1:t) ≈ ˜X(X1:t; ˆb).
Case 2: Suppose that dp ≥ dq  which implies that dA − 1 ≥ dq. Then  we may write

˜X(X1:t; ˆb) =

j=1

p(z−1)
q(z−1)

= R(z−1) +

ρ(z−1)
q(z−1)

 

where ρ and R are obtained by the Euclidean division of p by q  i.e.  p(z−1) = R(z−1)q(z−1) +
ρ(z−1) and deg(R) ≤ dA − 1 − dq and deg(ρ) < dq. We can write:

dA−1−dq(cid:88)

p(z−1)
q(z−1)

=

dq(cid:88)

νjz−j +

bi

1 − ziz−1 .

(10)

j=0

i=1

This is similar to (7) of Case 1  with the addition of lag terms. If we knew the set of complex poles
and their corresponding coefﬁcients as well as the coefﬁcients of the residual lag terms  νj  we could
provide an unbiased estimate of Xt+1 based on X1:t. Since we do not know the location of poles  we
randomly generate them as before. For this case  the feature set (8) should be expanded to

(cid:32)(cid:104)

1  z−1  ..  z−(dA−1−dq)(cid:105)

φ(z−1) : z−1 (cid:55)→

 

1
1 − Z(cid:48)
1z−1   .. 

1
1 − Z(cid:48)
N (ε)z−1

 

(11)

(cid:33)

4One could generate different types of ﬁlters  for example those with nonlinearities  but in this work we focus

on linear AR ﬁlters to simplify the analysis.

5

N (ε)(cid:88)

Algorithm 1 Random Projection Filter Bank
// Dm = {(Xi 1  Yi 1)  . . .   (Xi Ti   Yi Ti )}m
// l : Y(cid:48) × Y → R: Loss function
// F: Function space
// n: Number of ﬁlters in the random projection ﬁlter bank
Draw Z(cid:48)
n uniformly random within the unit circle
Deﬁne ﬁlters φ(z−1) =
for i = 1 to m do

i=1: Input data

1  . . .   Z(cid:48)

1
1−Z(cid:48)

nz−1

1
1−Z(cid:48)

1z−1   . . .  

(cid:16)

(cid:17)

end for
Find the estimator using extracted features (X(cid:48)
tion:

ˆf ← argmin
f∈F

return ˆf and φ

m(cid:88)

Ti(cid:88)

i=1

t=1

Pass the i-th time series through all the random ﬁlters φ(z−1)  i.e.  X(cid:48)

i 1:Ti = φ(z−1) ∗ Xi 1:Ti

i 1:Ti )  e.g.  by solving the regularized empirical risk minimiza-

l(f (X

(cid:48)
i t)  Yi t) + λJ(f ).

(12)

which consists of a history window of length dA − 1 − dq and the random projection ﬁlters. The
regressor should then estimate both bis and νis in (10).
RPFB is not limited to time series prediction with linear combination of ﬁltered signals. One may
use the generated features as the input to any other estimator too. RPFB can be used for other
problems such as classiﬁcation with time series too. Algorithm 1 shows how RPFB is used alongside
a regularized empirical risk minimization algorithm. The inputs to the algorithm are the time series
data Dm  with appropriate target values created depending on the problem  the pointwise loss function
l  the function space F of the hypotheses (e.g.  linear  RKHS  etc.)  and the number of ﬁlters n in
the RPFB. The ﬁrst step is to create the RPFB by randomly selecting n stable AR ﬁlters. We then
pass each time series in the dataset through the ﬁlter bank in order to create ﬁltered features  i.e.  the
feature are created by convolving the input time series with the ﬁlters’ impulse responses. Finally 
taking into account the problem type (regression or classiﬁcation) and function space  we apply
conventional machine learning algorithms to estimate ˆf. Here we present a regularized empirical risk
minimizer (12) as an example  but other choices are possible too  e.g.  decision trees or K-NN. We
note that the use of φ(z−1) ∗ Xi 1:Ti in the description of the algorithm should be interpreted as the
convolution of the impulse response of φ(z−1)  which is in the time domain  with the input signal.
√−1  we also
Remark 1. In practice  whenever we pick a complex pole Z(cid:48) = a + jb with j =
pick its complex conjugate ¯Z(cid:48) = a − jb in order to form a single ﬁlter
(1−Z(cid:48)z−1)(1− ¯Z(cid:48)z−1). This
guarantees that the output of this second-order ﬁlter is real valued.
Remark 2. RPFB is described for a univariate time series Xt ∈ R. To deal with multivariate time
series (i.e.  Xt ∈ Rd with d > 1) we may consider each dimension separately and pass each one
through RPFB. The ﬁlters in RPFB can be the same or different for each dimension. The state of
the ﬁlters  of course  depends on their input  so it would be different for each dimension. If we
have n ﬁlters and d-dimensional time series  the resulting vector X(cid:48)
i t in Algorithm 1 would be nd
dimensional. Randomly choosing multivariate ﬁlters is another possibility  which is a topic of future
research.
Remark 3. The Statistical Recurrent Unit (SRU)  recently introduced by Oliva et al. [2017]  has
some similarities to RPFB. SRU uses a set of exponential moving averages at various time scales
to summarize a time series  which are basically AR(1) ﬁlters with real-valued poles. SRU is more
complex  and potentially more expressive  than RPFB as it has several adjustable weights. On the
other hand  it does not have the simplicity of RPFB. Moreover  it does not yet come with the same
level of theoretical justiﬁcations as RPFB has.

1

4 Theoretical Guarantees

This section provides a ﬁnite-sample statistical guarantee for a time series predictor that uses RPFB
to extract features. We speciﬁcally focus on an empirical risk minimization-based (ERM) estimator.
Note that Algorithm 1 is not restricted to time series prediction problem  or the use of ERM-based

6

estimator  but the result of this section is. We only brieﬂy present the results  and refer the reader
to the same section in the supplementary material for more detail  including the proofs and more
discussions.
Consider the time series (X1  X2  . . . ) with Xt ∈ X ⊂ [−B  B] for some ﬁnite B > 0. We denote
X ∗ = ∪t≥1X t. The main object of interest in time series prediction is the conditional expectation of
Xt+1 given X1:t  which we denote by h∗  i.e. 5

(13)
We assume that h∗ belongs to the space of linear dynamical systems that has M ∈ N stable poles all
with magnitude less than 1 − ε0 for some ε0 > 0  and an Λ-bounded (cid:96)p-norm on the weights:

h∗(X1:t) = E [Xt+1|X1:t] .

Hε0 M p Λ (cid:44)

wi

1 − ziz−1 : |zi| ≤ 1 − ε0 (cid:107)w(cid:107)p ≤ Λ

.

(14)

If the value of ε0  M  p  or Λ are clear from context  we might refer to Hε0 M p Λ by H. Given a
function (or ﬁlter) h ∈ H  here h(x1:t) refers to the output at time t of convolving a signal x1:t
through h.
To deﬁne RPFB  we randomly draw n ≥ M independent complex numbers Z(cid:48)
within a complex circle with radius 1 − ε0  i.e.  |Z(cid:48)

i| ≤ 1 − ε0 (cf. Algorithm 1). The RPFB is

1  . . .   Z(cid:48)

n uniformly

(cid:40) M(cid:88)

i=1

(cid:41)

φ(z−1) =

1
1 − Z(cid:48)
1z−1   . . .  

1
1 − Z(cid:48)
nz−1

Given these random poles  we deﬁne the following approximation (ﬁlter) spaces

˜HΛ = ˜Hn p Λ =

αi
1 − Z(cid:48)

iz−1 : (cid:107)α(cid:107)p ≤ Λ

.

(15)

(cid:18)

(cid:40) n(cid:88)

i=1

(cid:19)

.

(cid:41)

(cid:104)ˆh(cid:48)(cid:105)

Consider that we have a sequence (X1  X2  . . .   XT   XT +1  XT +2). By denoting Yt = Xt+1  we
deﬁne ((X1  Y1)  . . .   (XT   YT )  (XT +1  YT +1)). We assume that |Xt| is B-bounded almost surely.
Deﬁne the estimator ˆh by solving the following ERM:

T(cid:88)

t=1

ˆh(cid:48) ← argmin
h∈ ˜HΛ

1
T

(cid:104)ˆh(cid:48)(cid:105)

Here TrB

|h(X1:t) − Yt|2  

ˆh ← TrB

.

(16)

truncates the values of ˆh(cid:48) at the level of ±B. So ˆh belongs to the following space

(cid:110)

(cid:105)
(cid:104)˜h

(cid:111)

˜HΛ B =

TrB

: ˜h ∈ ˜HΛ

.

(17)

A central object in our result is the notion of discrepancy  introduced by [Kuznetsov and Mohri  2015].
Discrepancy captures the non-stationarity of the process with respect to the function space.6 of
Deﬁnition 1 (Discrepancy—Kuznetsov and Mohri 2015). For a stochastic process X1  X2  . . .   a
function space H : X ∗ → R  and T ∈ N  deﬁne

(cid:40)

E(cid:104)|h(X1:T +1) − YT +1|2 |X1:T +1

(cid:105) − 1

E(cid:104)|h(X1:t) − Yt|2 |X1:t

(cid:105)(cid:41)

.

∆T (H) (cid:44) sup
h∈H

T(cid:88)

t=1

T

If the value of T is clear from the context  we may use ∆(H) instead. The following is the main
theoretical result of this work.

5We use h instead of f to somehow emphasize that the discussion is only for the time series prediction

problem  and not general estimation problem with a time series.

6Our deﬁnition is a simpliﬁed version of the original deﬁnition (by selecting qt = 1/T in their notation).

7

Theorem 1. Consider the time series (X1  . . .   XT +2)  and assume that |Xt| ≤ B (a.s.). Without
loss of generality suppose that B ≥ 1. Let 0 < ε0 < 1  M ∈ N  and Λ > 0 and assume
that the conditional expectation h∗(X1:t) = E [Xt+1|X1:t] belongs to the class of linear ﬁlters
Hε0 M 2 Λ (14). Set an integer n ≥ M for the number of random projection ﬁlters and let ˜HΛ =
˜Hn 2 Λ (15) and the truncated space be ˜HΛ B (17). Consider the estimator ˆh that is deﬁned as (16).
n and T ≥ 2. Fix δ > 0. It then holds that there
Without loss of generality assume that Λ ≥ ε0
√
(cid:114)
exists constants c1  c2 > 0 such that with probability at least 1 − δ  we have

B2

log(cid:0) 20n

(cid:1)

δ

log3(T )

n log(1/δ)

c2B2Λ2

+ 2∆( ˜HΛ B).

+

T

ε4
0

n

(cid:12)(cid:12)(cid:12)ˆh(X1:T +1) − h∗(X1:T +1)
The O((cid:112) n

(cid:12)(cid:12)(cid:12)2 ≤ c1B2Λ

ε0

The upper bounds has three terms: estimation error  ﬁlter approximation error  and the discrepancy.
T ) term corresponds to the estimation error. It decreases as the length T of the time
series increases. As we increase the number of ﬁlters n  the upper bounds shows an increase of
the estimation error. This is a manifestation of the effect of the input dimension on the error of
the estimator. The O(n−1) term provides an upper bound to the ﬁlter approximation error. This
error decreases as we add more ﬁlters. This indicates that RPFB provides a good approximation to
the space of dynamical systems Hε0 M 2 Λ (14). Both terms show the proportional dependency on
the magnitude B of the random variables in the time series  and inversely proportional dependency
on the minimum distance ε0 of the poles to the unit circle. Intuitively  this is partly because the
output of a pole becomes more sensitive to its input as it gets closer to the unit circle. Finally  the
discrepancy term ∆( ˜HΛ B) captures the non-stationarity of the process  and has been discussed in
detail by Kuznetsov and Mohri [2015]. Understanding the conditions when the discrepancy gets close
to zero is an interesting topic for future research.
By setting the number of RP ﬁlters to n = T 1/3Λ2/3
simplify the upper bound to

  and under the condition that Λ ≤ T   we can

(cid:12)(cid:12)(cid:12)ˆh(X1:T +1) − h∗(X1:T +1)

ε2
0

(cid:113)
(cid:12)(cid:12)(cid:12)2 ≤ cB2Λ4/3 log3(T )

ε2
0T 1/3

log( 1
δ )

+ 2∆( ˜HΛ B) 

which holds with probability at least 1 − δ  for some constant c > 0. As T → ∞  the error converges
to the level of discrepancy term.
We would like to comment that in the statistical part of the proof  instead of using the independent
block technique of Yu [1994] to analyze a mixing processes [Doukhan  1994]  which is a common
technique used by many prior work such as Meir [2000]; Mohri and Rostamizadeh [2009  2010];
Farahmand and Szepesvári [2012]  we rely on more recent notions of sequential complexities [Rakhlin
et al.  2010  2014] and the discrepancy [Kuznetsov and Mohri  2015] of the function space-stochastic
process couple.
This theorem is for Case 1 in Section 3  but a similar result also holds for Case 2. We also mention
that as the values of M  ε0  and Λ of the true dynamical system space Hε0 M 2 Λ are often unknown 
the choice of number of ﬁlters n in RPFB  the size of the space M  etc. cannot be selected based on
them. Instead one should use a model selection procedure to pick the appropriate values for these
parameters.

5 Experiments

We use a ball bearing fault detection problem to empirically study RPFB and compare it with a ﬁxed-
window history-based approach. The supplementary material provides several other experiments 
including the application of LSTM to solve the very same problem  close comparison of RPFB with
ﬁxed-window history-based approach on an ARMA time series prediction problem  and a heart rate
classiﬁcation problem. For further empirical studies  especially in the context of fault detection and
prognosis  refer to Pourazarm et al. [2017].
Reliable operation of rotating equipments (e.g.  turbines) depends on the condition of their bearings 
which makes the detection of whether a bearing is faulty and requires maintenance of crucial
importance. We consider a bearing vibration dataset provided by Machinery Failure Prevention

8

Figure 1: (Bearing Dataset) Classiﬁcation error on the test dataset using RPFB and ﬁxed-window
history-based feature sets. The RPFB results are averaged over 20 independent randomly selected
RPFB. The error bars show one standard error.

Technology (MFPT) Society in our experiments.7 Fault detection of bearings is an example of
industrial applications where the computational resources are limited  and fast methods are required 
e.g.  only a micro-controller or a cheap CPU  and not a GPU  might be available.
The dataset consists of three univariate time series corresponding to a baseline (good condition/class
0)  an outer race fault (class 1)  and inner race fault (class 2). The goal is to ﬁnd a classiﬁer that
predicts the class label at the current time t given the vibration time series X1:t. In a real-world
scenario  we train the classiﬁer on a set of previously recorded time series  and later let it operate on
a new time series observed from a device. The goal would be to predict the class label at each time
step as new data arrives. Here  however  we split each of three time series to a training and testing
subsets. More concretely  we ﬁrst pass each time series through RPFB (or deﬁne a ﬁxed-window
of the past H values of them). We then split the processed time series  which has the dimension of
the number of RPFB or the size of the window  to the training and testing sets. We select the ﬁrst
3333 time steps to deﬁne the training set  and the next 3333 data points as the testing dataset. As we
have three classes  this makes the size of training and testing sets both equal to 10K. We process each
dimension of the features to have a zero mean and a unit variance for both feature types. We perform
20 independent runs of RPFB  each of which with a new set of randomly selected ﬁlters.
Figure 1 shows the classiﬁcation error of three different classiﬁer (Logistic Regression (LR) with the
(cid:96)2 regularization  Random Forest (RF)  and Support Vector Machine (SVM) with Gaussian kernel)
on both feature types  with varying feature sizes. We observe that as the number of features increases 
the error of all classiﬁers decreases too. It is also noticeable that the error heavily depends on the type
of classiﬁer  with SVM being the best in the whole range of number of features. The use of RPFB
instead of ﬁxed-window history-based one generally improves the performance of LR and SVM  but
not for RF. Refer to the supplementary material for more detail on the experiment.

6 Conclusion

This paper introduced Random Projection Filter Bank (RPFB) as a simple and effective method
for feature extraction from time series data. RPFB comes with a ﬁnite-sample error upper bound
guarantee for a class of linear dynamical systems. We believe that RPFB should be a part of the
toolbox for time series processing.
A future research direction is to better understand other dynamical system spaces  beyond the linear
one considered here  and to design other variants of RPFB beyond those that are deﬁned by stable
linear autoregressive ﬁlters. Another direction is to investigate the behaviour of the discrepancy
factor.

7Available from http://www.mfpt.org/faultdata/faultdata.htm.

9

52550100200400Features No0.00.10.20.30.40.50.60.7Classification ErrorLR (RPFB)LR (Window)RF (RPFB)RF (Window)SVM (RPFB)SVM (Window)Acknowledgments

We would like to thank the anonymous reviewers for their helpful feedback.

References
Richard G. Baraniuk and Michael B. Wakin. Random projections of smooth manifolds. Foundations

of computational mathematics  9(1):51–77  2009. 1

Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer  2006. 1  2

Kyunghyun Cho  Bart Van Merriënboer  Dzmitry Bahdanau  and Yoshua Bengio. On the properties of
neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259  2014.
2

Paul Doukhan. Mixing: Properties and Examples  volume 85 of Lecture Notes in Statistics. Springer-

Verlag  Berlin  1994. 8

Amir-massoud Farahmand and Csaba Szepesvári. Regularized least-squares regression: Learning
from a β-mixing sequence. Journal of Statistical Planning and Inference  142(2):493 – 505  2012.
2  8

Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016. 2

Trevor Hastie  Robert Tibshirani  and Jerome Friedman. The Elements of Statistical Learning: Data

Mining  Inference  and Prediction. Springer  2001. 1  2

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):

1735–1780  1997. 2

Sham Kakade  Percy Liang  Vatsal Sharan  and Gregory Valiant. Prediction with a short memory.

arXiv:1612.02526v2  2017. 3

Vitaly Kuznetsov and Mehryar Mohri. Learning theory and algorithms for forecasting non-stationary
time series. In Advances in Neural Information Processing Systems (NIPS - 28)  pages 541–549.
Curran Associates  Inc.  2015. 7  8

Mantas Lukoševiˇcius and Herbert Jaeger. Reservoir computing approaches to recurrent neural

network training. Computer Science Review  3(3):127–149  2009. 2

Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning 

39(1):5–34  2000. 8

Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes.
In Advances in Neural Information Processing Systems 21  pages 1097–1104. Curran Associates 
Inc.  2009. 8

Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary φ-mixing and β-mixing
processes. Journal of Machine Learning Research (JMLR)  11:789–814  2010. ISSN 1532-4435.
2  8

Junier B. Oliva  Barnabás Póczos  and Jeff Schneider. The statistical recurrent unit. In Proceedings
of the 34th International Conference on Machine Learning (ICML)  volume 70 of Proceedings of
Machine Learning Research  pages 2671–2680. PMLR  August 2017. 2  6

Alan V. Oppenheim  Ronald W. Schafer  and John R. Buck. Discrete-Time Signal Processing. Prentice

Hall  second edition  1999. 3

Sepideh Pourazarm  Amir-massoud Farahmand  and Daniel N. Nikovski. Fault detection and
prognosis of time series data with random projection ﬁlter bank. In Annual Conference of the
Prognostics and Health Management Society (PHM)  pages 242–252  2017. 2  8

Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in Neural Information Processing Systems (NIPS -
21)  pages 1313–1320  2009. 2

10

Alexander Rakhlin  Karthik Sridharan  and Ambuj Tewari. Online learning: Random averages 
combinatorial parameters  and learnability. In Advances in Neural Information Processing Systems
(NIPS - 23)  2010. 8

Alexander Rakhlin  Karthik Sridharan  and Ambuj Tewari. Sequential complexities and uniform

martingale laws of large numbers. Probability Theory and Related Fields  2014. 8

Ingo Steinwart

and Andreas Christmann.

In Advances in Neural

tions.
1768–1776. Curran Associates 
3736-fast-learning-from-non-iid-observations.pdf. 2

Fast

learning from non-i.i.d. observa-
Information Processing Systems (NIPS - 22)  pages
Inc.  2009.
URL http://papers.nips.cc/paper/

Ingo Steinwart  Don Hush  and Clint Scovel. Learning from dependent observations. Journal of

Multivariate Analysis  100(1):175–194  2009. 2

Santosh S. Vempala. The Random Projection Method. DIMACS Series in Discrete Mathematics and
Theoretical Computer Science. American Mathematical Society  2004. ISBN 9780821837931. 1

Larry Wasserman. All of Nonparametric Statistics (Springer Texts in Statistics). Springer  2007. 1  2

Martha White  Junfeng Wen  Michael Bowling  and Dale Schuurmans. Optimal estimation of
multivariate ARMA models. In Proceedings of the 29th AAAI Conference on Artiﬁcial Intelligence
(AAAI)  2015. 4

Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of

Probability  22(1):94–116  January 1994. 8

11

,Amir-massoud Farahmand
Sepideh Pourazarm