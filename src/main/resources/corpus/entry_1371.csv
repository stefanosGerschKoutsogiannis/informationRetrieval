2018,Deep Neural Nets with Interpolating Function as Output Activation,We replace the output layer of deep neural nets  typically the softmax function  by a novel interpolating function. And we propose end-to-end training and testing algorithms for this new architecture. Compared to classical neural nets with softmax function as output activation  the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. The new framework demonstrates the following major advantages: First  it is better applicable to the case with insufficient training data. Second  it significantly improves the generalization accuracy on a wide variety of networks. The algorithm is implemented in PyTorch  and the code is available at https://github.com/
BaoWangMath/DNN-DataDependentActivation.,Deep Neural Nets with Interpolating Function as

Output Activation

Bao Wang

Xiyang Luo

Department of Mathematics

University of California  Los Angeles

wangbaonj@gmail.com

Department of Mathematics

University of California  Los Angeles

xylmath@gmail.com

Zhen Li

Wei Zhu

Department of Mathematics

Department of Mathematics

HKUST  Hong Kong
lishen03@gmail.com
Zuoqiang Shi

Duke University

zhu@math.duke.edu
Stanley J. Osher

Department of Mathematics

Tsinghua University

zqshi@mail.tsinghua.edu.cn

Department of Mathematics

University of California  Los Angeles

sjo@math.ucla.edu

Abstract

We replace the output layer of deep neural nets  typically the softmax function  by
a novel interpolating function. And we propose end-to-end training and testing
algorithms for this new architecture. Compared to classical neural nets with
softmax function as output activation  the surrogate with interpolating function
as output activation combines advantages of both deep and manifold learning.
The new framework demonstrates the following major advantages: First  it is
better applicable to the case with insufﬁcient training data. Second  it signiﬁcantly
improves the generalization accuracy on a wide variety of networks. The algorithm
is implemented in PyTorch  and the code is available at https://github.com/
BaoWangMath/DNN-DataDependentActivation.

1

Introduction

Generalizability is crucial to deep learning  and many efforts have been made to improve the training
and generalization accuracy of deep neural nets (DNNs) [3  14]. Advances in network architectures
such as VGG networks [28]  deep residual networks (ResNets)[12  13] and more recently DenseNets
[16] and many others [6]  together with powerful hardware make the training of very deep networks
with good generalization capabilities possible. Effective regularization techniques such as dropout
and maxout [15  30  10]  as well as data augmentation methods [19  28  32] have also explicitly
improved generalization for DNNs.
A key component of neural nets is the activation function. Improvements in designing of activation
functions such as the rectiﬁed linear unit (ReLU) [8]  have led to huge improvements in performance
in computer vision tasks [23  19]. More recently  activation functions adaptively trained to the data
such as the adaptive piecewise linear unit (APLU) [1] and parametric rectiﬁed linear unit (PReLU)
[11] have lead to further improvements in performance of DNNs. For output activation  support vector
machine (SVM) has also been successfuly applied in place of softmax[29]. Though training DNNs
with softmax or SVM as output activation is effective in many tasks  it is possible that alternative
activations that consider manifold structure of data by interpolating the output based on both training
and testing data can boost performance of the network. In particular  ResNets can be reformulated as
solving control problems of a class of transport equations in the continuum limit [21  5]. Transport

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

theory suggests that by using an interpolating function that interpolates terminal values from initial
values can dramatically simplify the control problem compared to an ad-hoc choice. This further
suggests that a ﬁxed and data-agnostic activation for the output layer may be suboptimal.
To this end  based on the ideas from manifold learning  we propose a novel output layer named
weighted nonlocal Laplacian (WNLL) layer for DNNs. The resulted DNNs achieve better gen-
eralization and are more robust for problems with a small number of training examples. On CI-
FAR10/CIFAR100  we achieve on average a 30%/20% reduction in terms of test error on a wide
variety of networks. These include VGGs  ResNets  and pre-activated ResNets. The performance
boost is even more pronounced when the model is trained on a random subset of CIFAR with a low
number of training examples. We also present an efﬁcient algorithm to train the WNLL layer via an
auxiliary network. Theoretical motivation for the WNLL layer is also given from the viewpoint of
both game theory and terminal value problems for transport equations.
This paper is structured as follows: In Section 2  we introduce the motivation and practice of using the
WNLL interpolating function in DNNs. In Section 2.2  we explain in detail the algorithms for training
and testing DNNs with WNLL as output layer. Section 3 provides insight of using an interpolating
function as output layer from the angle of terminal value problems of transport equations and game
theory. Section 4 demonstrates the effectiveness of our method on a variety of numerical examples.

2 Network Architecture

In coarse grained representation  training and testing DNNs with softmax layer as output are illustrated
in Fig. 1 (a) and (b)  respectively. In kth iteration of training  given a mini-batch training data (X  Y) 
we perform:
Forward propagation: Transform X into deep features by DNN block (ensemble of conv layers 
nonlinearities and others)  and then activated by softmax function to obtain the predicted labels ˜Y:

˜Y = Softmax(DNN(X  Θk−1)  Wk−1).

Then compute loss (e.g.  cross entropy) between Y and ˜Y: L = Loss(Y  ˜Y).
Backpropagation: Update weights (Θk−1  Wk−1) by gradient descent (learning rate γ):

Wk = Wk−1 − γ

∂L
∂ ˜Y

· ∂ ˜Y
∂W

  Θk = Θk−1 − γ

∂L
∂ ˜Y

· ∂ ˜Y
∂ ˜X

· ∂ ˜X
∂Θ

.

Figure 1: Training (a) and testing (b) procedures of DNNs with softmax as output activation layer.

(a)

(b)

Once the model is optimized  for testing data X  the predicted labels are:

˜Y = Softmax(DNN(X  Θ)  W) 

for notational simplicity  we still denote the test set and optimized weights as X  Θ  and W 
respectively. In essence the softmax layer acts as a linear model on the space of deep features
˜X  which does not take into consideration the underlying manifold structure of ˜X. The WNLL
interpolating function  which will be introduced in the following subsection  is an approach to
alleviate this deﬁciency. Moreover  WNLL interpolation is based on the harmonic extension which
avoids the curse of dimensionality issue in high dimensional interpolation.

2

1   xte

2  ···   xte

2.1 Manifold Interpolation - An Harmonic Extension Approach
Let X = {x1  x2 ···   xn} be a set of points in a high dimensional manifold M ⊂ Rd and
Xte = {xte
m} be a subset of X. Suppose we have a (possibly vector valued) label
function g(x) deﬁned on Xte  and we want to interpolate a function u that is deﬁned on the entire
manifold and can be used to label the entire dataset X. Interpolation by using basis function in high
dimensional space suffers from the curse of dimensionality. Instead  an harmonic extension is a
natural and elegant approach to ﬁnd such an interpolating function  which is deﬁned by minimizing
the following Dirichlet energy functional:

w(x  y) (u(x) − u(y))2  

(1)

(cid:88)

x y∈X

E(u) =

1
2

with the boundary condition:

u(x) = g(x)  x ∈ Xte 

where w(x  y) is a weight function  typically chosen to be Gaussian: w(x  y) = exp(−||x−y||2
with σ a scaling parameter. The Euler-Lagrange equation for Eq.(1) is:

σ2

)

y∈X (w(x  y) + w(y  x)) (u(x) − u(y)) = 0 x ∈ X/Xte

x ∈ Xte.

u(x) = g(x)

(cid:40)(cid:80)

(2)

(3)

By solving the linear system Eq.(2)  we get the interpolated labels u(x) for unlabeled data x ∈ X/Xte.
This interpolation becomes invalid when labeled data is tiny  i.e.  |Xte| (cid:28) |X/Xte|. There are two
solutions to resolve this issue: one is to replace the 2-Laplacian in Eq.(1) by a p-Laplacian [4]; the
other is to increase the weights of the labeled data in the Euler-Lagrange equation [27]  which gives
the following weighted nonlocal Laplacian (WNLL) interpolating function:


(cid:80)
(cid:16) |X|
y∈X (w(x  y) + w(y  x)) (u(x) − u(y)) +
|Xte| − 1

(cid:17)(cid:80)

u(x) = g(x)

y∈Xte w(y  x) (u(x) − u(y)) = 0 x ∈ X/Xte

x ∈ Xte.

For notational simplicity  we name the solution u(x) to Eq.3 as WNLL(X  Xte  Yte). For classiﬁca-
tion tasks  g(x) is the one-hot labels for the example x. To ensure accuracy of WNLL  the labeled
data should cover all classes of data in X. We give a necessary condition in Theorem 1.
Theorem 1. Suppose we have a data pool formed by N classes of data uniformly  with the number
of instances of each class be sufﬁciently large. If we want all classes of data to be sampled at least

(cid:1) data is need to be sampled from the data pool.

once  on average at least N(cid:0)1 + 1

2 + 1

3 + ··· + 1

N

In this case  the number of data sampled  in expectation for each class  is 1 + 1

2 + 1

3 + ··· + 1
N .

2.2 WNLL Activated DNNs and Algorithms

In both training and testing of the WNLL activated DNNs  we need to reserve a small portion of
data/label pairs denoted as (Xte  Yte)  to interpolate the label Y for new data. We name (Xte  Yte)
as the preserved template. Directly replacing softmax by WNLL (Fig. 2(a)) has difﬁculties in back
propagation  namely  the true gradient ∂L
∂Θ is difﬁcult to compute since WNLL deﬁnes a very complex
implicit function. Instead  to train WNLL activated DNNs  we propose a proxy via an auxiliary
neural nets (Fig. 2(b)). On top of the original DNNs  we add a buffer block (a fully connected layer
followed by a ReLU)  and followed by two parallel layers  WNLL and the linear (fully connected)
layers. The auxiliary DNNs can be trained by alternating between the following two steps (training
DNNs with linear and WNLL activations  respectively):
Train DNNs with linear activation: Run N1 steps of the following forward and back propagation 
where in kth iteration  we have:
Forward propagation: The training data X is transformed  respectively  by DNN  Buffer and Linear
blocks to the predicted labels ˜Y:

˜Y = Linear(Buﬀer(DNN(X  Θk−1)  Wk−1

B )  Wk−1
L ).

3

Then compute loss between the ground truth labels Y and predicted ones ˜Y  denoted as LLinear (e.g. 
cross entropy loss  and the same as following LWNLL).
Backpropagation: Update weights (Θk−1  Wk−1

B   Wk−1

Wk

L = Wk−1

L − γ

∂ ˜Y

∂LLinear

· ∂ ˜Y
∂WL
Θk = Θk−1 − γ

  Wk
∂LLinear

∂ ˜Y

L ) by gradient descent:
· ∂ ˜Y
∂ ˆX

B − γ

∂LLinear

B = Wk−1

· ∂ ˆX
∂WB

 

∂ ˜Y
· ∂ ˜X
∂Θ

.

· ∂ ˜Y
∂ ˆX

· ∂ ˆX
∂ ˜X

Train DNNs with WNLL activation: Run N2 steps of the following forward and back propagation 
where in kth iteration  we have:
Forward propagation: The training data X  template Xte and Yte are transformed  respectively  by
DNN  Buffer  and WNLL blocks to get predicted labels ˆY:

ˆY = WNLL(Buﬀer(DNN(X  Θk−1)  Wk−1

B )  ˆXte  Yte).

Then compute loss  LWNLL  between the ground truth labels Y and predicted ones ˆY.
Backpropagation: Update weights Wk−1
in training DNNs with linear activation  by gradient descent.

B only  Wk−1

and Θk−1 will be tuned in the next iteration

L

Wk

B = Wk−1

B − γ

∂LWNLL

∂ ˆY

· ∂ ˆY
∂ ˆX

· ∂ ˆX
∂WB

≈ Wk−1

B − γ

∂LLinear

∂ ˜Y

· ∂ ˜Y
∂ ˆX

· ∂ ˆX
∂WB

.

(4)

∂ ˜Y

· ∂ ˜Y
∂ ˆX

Here we use the computational graph of the left branch (linear layer) to retrieval the approximated
≈
gradients for WNLL. For a given loss value of LWNLL  we adopt the approximation ∂LWNLL
∂LLinear
where the right hand side is also evaluated at this value. The main heuristic behind
this approximation is the following: WNLL deﬁnes a harmonic function implicitly  and a linear
function is the simplest nontrivial explicit harmonic function. Empirically  we observe this simple
approximation works well in training the network. The reason why we freeze the network in the
DNN block is mainly due to stability concerns.

· ∂ ˆY
∂ ˆX

∂ ˆY

(a)

(b)

(c)

Figure 2: Training and testing procedure of the deep neural nets with WNLL as the last activation
layer.(a): Direct replacement of the softmax by WNLL  (b): An alternating training procedure. (c):
Testing.

The above alternating scheme is an algorithm of a greedy fashion. During training  WNLL activation
plays two roles: on one hand  the alternating between linear and WNLL activations beneﬁts each other
which enables the neural nets to learn features that is appropriate for both linear classiﬁcation and
WNLL based manifold interpolation. On the other hand  in the case where we lack sufﬁcient training
data  the training of DNNs usually gets stuck at some bad local minima which cannot generalize well
on new data. We use WNLL interpolation which provides a perturbation to the trained sub-optimal
weights and can help to arrive at a local minima with better generalizability. At test time  we remove
the linear classiﬁer from the neural nets and use the DNN block together with WNLL to predict new
data (Fig. 2 (c)). The reason for using WNLL instead of a linear layer is because WNLL is superior

4

(X Y) (Xte Yte)(˜X ˜Xte)=DNN(X Xte Θ)˜Y=Linear(ˆX WL)ˆY=WNLL(ˆX ˆXte Yte)Loss(˜Y Y)(ˆX ˆXte)=Buﬀer(˜X ˜Xte WB)Loss(ˆY Y)to the linear classiﬁer and this superiority is preserved when applied to deep features (which will be
shown in Section. 4). Moreover  WNLL utilizes both the learned DNNs and the preserved template
at test time which seems to be more stable to perturbations on the input data.
We summarize the training and testing procedures for the WNLL activated DNNs in Algorithms 1
and 2  respectively. In each round of the alternating procedure i.e.  each outer loop in Algorithm. 1 
the entire training set (X  Y) is ﬁrst used to train the DNNs with linear activation. We randomly
separate a template  e.g.  half of the entire data  from the training set which will be used to perform
WNLL interpolation in training WNLL activated DNNs. In practice  for both training and testing  we
use minibatches for both the template and the interpolated points when the entire dataset is too large.
The ﬁnal predicted labels are obtained by a majority voted across interpolation results from all the
template minibatches.
Remark 1. In Algorithm. 1  the WNLL interpolation is also performed in mini-batch manner (as
shown in the inner iteration). Based on our experiments  this does not reduce the interpolation
accuracy signiﬁcantly.

Algorithm 1 DNNs with WNLL as Output Activation: Training Procedure.

Input: Training set: (data  label) pairs (X  Y).
Output: An optimized DNNs with WNLL as output activation  denoted as DNNWNLL.
for iter = 1 . . .   N (where N is the number of alternating steps.) do

//Train the left branch: DNNs with linear activation.
Train DNN + Linear blocks  and denote the learned model as DNNLinear.
//Train the right branch: DNNs with WNLL activation.
Split (X  Y) into training data and template  i.e.  (X  Y)
for i = 1  2 ···   M do

Partition the training data into M mini-batches  i.e.  (Xtr  Ytr) =(cid:83)M
(cid:83) Xte by DNNLinear  i.e.  ˜Xtr(cid:83) ˜Xte = DNNLinear(Xtr
Apply WNLL (Eq.(3)) on { ˜Xtr(cid:83) ˜Xte  Yte} to interpolate label ˜Ytr.

= (Xtr  Ytr)(cid:83)(Xte  Yte).
(cid:83) Xte).

Transform Xtr
i

i=1(Xtr

i ).
i   Ytr

.

i

Backpropagate the error between Ytr and ˜Ytr via Eq.(4) to update WB only.

Algorithm 2 DNNs with WNLL as Output Activation: Testing Procedure.

Input: Testing data X  template (Xte  Yte). Optimized model DNNWNLL.
Output: Predicted label ˜Y for X.

Apply the DNN block of DNNWNLL to X(cid:83) Xte to get the representation ˜X(cid:83) ˜Xte.
Apply WNLL (Eq.(3)) on { ˜X(cid:83) ˜Xte  Yte} to interpolate label ˜Y.

3 Theoretical Explanation

In training WNLL activated DNNs  the two output activation functions in the auxiliary networks
are  in a sense  each competing to minimize its own objective where  in equilibrium  the neural nets
can learn better features for both linear and interpolation-based activations. This in ﬂavor is similar
to generative adversarial nets (GAN) [9]. Another interpretation of our model is the following: As
noted in [21]  in the continuum limit  ResNet can be modeled as the following control problem for a
transport equation:

(cid:40) ∂u(x t)
∂t + v(x  t) · ∇u(x  t) = 0 x ∈ X  t ≥ 0

(5)

u(x  1) = f (x)

x ∈ X.

Here u(·  0) is the input of the continuum version of ResNet  which maps the training data to the
corresponding label. f (·) is the terminal value which analogous to the output activation function in
ResNet which maps deep features to the predicted label. Training ResNet is equivalent to tuning
v(·  t)  i.e.  continuous version of the weights  s.t. the predicted label f (·) matches that of the training
data. If f (·) is a harmonic extension of u(·  0)  the corresponding weights v(x  t) would be close to
zero. This results in a simpler model and may generalize better from a model selection point of view.

5

4 Numerical Results

To validate the classiﬁcation accuracy  efﬁciency and robustness of the proposed framework  we test
the new architecture and algorithm on CIFAR10  CIFAR100 [18]  MNIST[20] and SVHN datasets
[24]. In all experiments  we apply standard data augmentation that is widely used for the CIFAR
datasets [12  16  31]. For MNIST and SVHN  we use the raw data without any augmentation. We
implement our algorithm on the PyTorch platform [26]. All computations are carried out on a machine
with a single Nvidia Titan Xp graphics card.
Before diving into the performance of DNNs with different output activation functions  we ﬁrst
compare the performance of WNLL with softmax on the raw input images for various datasets. The
training sets are used to train the softmax models and interpolate labels for testing set in softmax
and WNLL  respectively. Table 1 lists the classiﬁcation accuracies of WNLL and softmax on three
datasets. For WNLL interpolation  in order to speed up the computation  we only use 15 nearest
neighbors to ensure sparsity of the weight matrix  and the 8th neighbor’s distance is used to normalize
the weight matrix. The nearest neighbors are searched via the approximate nearest neighbor (ANN)
algorithm [22]. WNLL outperforms softmax signiﬁcantly in all three tasks. These results show the
potential of using WNLL instead of softmax as the output activation function in DNNs.

Table 1: Accuracies of softmax and WNLL in classifying some classical datasets.

Dataset
softmax
WNLL

CIFAR10 MNIST
SVHN
39.91% 92.65% 24.66%
40.73% 97.74% 56.17%

For the deep learning experiments below: We take two passes alternating steps  i.e.  N = 2 in
Algorithm. 1. For the linear activation stage (Stage 1)  we train the network for n = 400 epochs. For
the WNLL stage  we train for n = 5 epochs. In the ﬁrst pass  the initial learning rate is 0.05 and
halved after every 50 epochs in training linear activated DNNs  and 0.0005 when training the WNLL
activation. The same Nesterov momentum and weight decay as used in [12  17] are used for CIFAR
and SVHN experiments  respectively. In the second pass  the learning rate is set to be one ﬁfth of the
corresponding epochs in the ﬁrst pass. The batch sizes are 128 and 2000 when training softmax/linear
and WNLL activated DNNs  respectively. For fair comparison  we train the vanilla DNNs with
softmax output activation for 810 epochs with the same optimizers used in WNLL activated ones. All
ﬁnal test errors reported for the WNLL method are done using WNLL activations for prediction on
the test set. In the rest of this section  we show that the proposed framework resolves the issue of
lacking big training data and boosts the generalization accuracies of DNNs via numerical results on
CIFAR10/CIFAR100. The numerical results on SVHN are provided in the appendix.

4.1 Resolving the Challenge of Insufﬁcient Training Data

When we do not have sufﬁcient training data  the generalization accuracy typically degrades as
the network goes deeper  as illustrated in Fig.3. The WNLL activated DNNs  with its superior
regularization of the parameters and perturbation on bad local minima  are able to overcome this
degradation. The left and right panels plot the cases when the ﬁrst 1000 and 10000 data in the training
set of CIFAR10 are used to train the vanilla and WNLL DNNs. As shown in Fig. 3  by using WNLL
activation  the generalization error rates decay consistently as the network goes deeper  in contrast
to the degradation for vanilla DNNs. The generalization accuracy between the vanilla and WNLL
DNNs can differ up to 10 percent within our testing regime.
Figure.4 plots the evolution of generalization accuracy during training. We compute the test accuracy
per epoch. Panels (a) and (b) plot the test accuracies for ResNet50 with softmax and WNLL activations
(1-400 and 406-805 epochs corresponds to linear activation)  respectively  with only the ﬁrst 1000
examples as training data from CIFAR10. Charts (c) and (d) are the corresponding plots with 10000
training instances  using a pre-activated ResNet50. After around 300 epochs  the accuracies of the
vanilla DNNs plateau and cannot improve any more. In comparison  the test accuracy for WNLL
jumps at the beginning of Stage 2 in ﬁrst pass; during Stage 1 of the second pass  even though initially
there is an accuracy reduction  the accuracy continues to climb and eventually surpasses that of the
WNLL activation in Stage 2 of ﬁrst pass. The jumps in accuracy at epoch 400 and 800 are due to

6

(a)

(b)

Figure 3: Resolving the degradation problem of vanilla DNNs by WNLL activation. Panels (a) and
(b) plot the generation errors when 1000 and 10000 training data are used to train the vanilla and the
WNLL activated DNNs  respectively. In each plot  we test three different networks: PreActResNet18 
PreActResNet34  and PreActResNet50. All tests are done on the CIFAR10 dataset.

switching from linear activation to WNLL for predictions on the test set. The initial decay when
alternating back to softmax is caused partially by the ﬁnal layer WL not being tuned with respect to
the deep features ˜X  and partially due to predictions on the test set being made by softmax instead
of WNLL. Nevertheless  the perturbation via the WNLL activation quickly results in the accuracy
increasing beyond the linear stage in the previous pass.

(a)

(c)

(b)

(d)

Figure 4: The evolution of the generation accuracies over the training procedure. Charts (a) and (b)
are the accuracy plots for ResNet50 with 1000 training data  where (a) and (b) are plots for the epoch
v.s. accuracy of the vanilla and the WNLL activated DNNs. Panels (c) and (d) correspond to the case
of 10000 training data for PreActResNet50. All tests are done on the CIFAR10 dataset.

4.2

Improving Generalization Accuracy

We next show the superiority of WNLL activated DNNs in terms of generalization accuracies when
compared to their surrogates with softmax or SVM output activations. Besides ResNets  we also

7

test the WNLL surrogate on the VGG networks. In table 2  we list the generalization errors for 15
different DNNs from VGG  ResNet  Pre-activated ResNet families on the entire  ﬁrst 10000 and
ﬁrst 1000 instances of the CIFAR10 training set. We observe that WNLL in general improves more
for ResNets and pre-activated ResNets  with less but still signiﬁcant improvements for the VGGs.
Except for VGGs  we can achieve relatively 20% to 30% testing error rate reduction across all neural
nets. All results presented here and in the rest of this paper are the median of 5 independent trials.
We also compare with SVM as an alternative output activation  and observe that the results are still
inferior to WNLL. Note that the bigger batch-size is to ensure the interpolation quality of WNLL.
A reasonable concern is that the performance increase comes from the variance reduction due to
increasing the batch size. However  experiments done with a batch size of 2000 for vanilla networks
actually deteriorates the test accuracy.

Table 2: Generalization error rates over the test set of vanilla DNNs  SVM and WNLL activated
ones trained over the entire  the ﬁrst 10000  and the ﬁrst 1000 instances of training set of CIFAR10.
(Median of 5 independent trials)

Network

VGG11
VGG13
VGG16
VGG19
ResNet20
ResNet32
ResNet44
ResNet56
ResNet110
ResNet18
ResNet34
ResNet50

PreActResNet18
PreActResNet34
PreActResNet50

Whole

10000

1000

Vanilla
9.23%
6.66%
6.72%
6.95%

9.06% (8.75%[12])
7.99% (7.51%[12])
7.31% (7.17%[12])
7.24% (6.97%[12])
6.41% (6.43%[12])

6.16%
5.93%
6.24%
6.21%
6.08%
6.05%

WNLL
SVM Vanilla WNLL Vanilla WNLL
7.35% 9.28% 10.37% 8.88% 26.75% 24.10%
7.64% 24.85% 22.56%
5.58% 7.47% 9.12%
7.54% 25.41% 22.23%
5.69% 7.29% 9.01%
5.92% 7.99% 9.62%
8.09% 25.70% 22.87%
7.09% 9.60% 12.83% 9.96% 34.90% 29.91%
5.95% 8.73% 11.18% 8.15% 33.41% 28.78%
5.70% 8.67% 10.66% 7.96% 34.58% 27.94%
9.83% 7.61% 37.83% 28.18%
5.61% 8.58%
8.91% 7.13% 42.94% 28.29%
4.98% 8.06%
4.65% 6.00% 8.26%
6.29% 27.02% 22.48%
6.11% 26.47% 20.27%
4.26% 6.32% 8.31%
6.49% 29.69% 20.19%
4.17% 6.63% 9.64%
6.61% 27.36% 21.88%
4.74% 6.38% 8.20%
6.34% 23.56% 19.02%
4.40% 5.88% 8.52%
4.27% 5.91% 9.18%
6.05% 25.05% 18.61%

Tables 2 and 3 list the error rates of 15 different vanilla networks and WNLL activated networks
on CIFAR10 and CIFAR100 datasets. On CIFAR10  WNLL activated DNNs outperforms the
vanilla ones with around 1.5% to 2.0% absolute  or 20% to 30% relative error rate reduction. The
improvements on CIFAR100 are more signiﬁcant. We independently ran the vanilla DNNs on both
datasets  and our results are consistent with the original reports and other researchers’ reproductions
[12  13  16]. We provide experimental results of DNNs’ performance on SVHN data in the appendix.
Interestingly  the improvement are more signiﬁcant on harder tasks  suggesting potential for our
methods to succeed on other tasks/datasets. For example  reducing the sizes of DNNs is an important
direction to make the DNNs applicable for generalize purposes  e.g.  auto-drive  mobile intelligence 
etc. So far the most successful attempt is DNNs weights quantization[2]. Our approach is a new
direction for reducing the size of the model: to achieve the same level of accuracy  compared to the
vanilla networks  our model’s size can be much smaller.

5 Concluding Remarks

We are motivated by ideas from manifold interpolation and the connection between ResNets and
control problems of transport equations. We propose to replace the classical output activation function 
i.e.  softmax  by a harmonic extension type of interpolating function. This simple surgery enables the
deep neural nets (DNNs) to make sufﬁcient use of the manifold information of data. An end-to-end
greedy style  multi-stage training algorithm is proposed to train this novel output layer. On one
hand  our new framework resolves the degradation problem caused by insufﬁcient data; on the other
hand  it boosts the generalization accuracy signiﬁcantly compared to the baseline. This improvement
is consistent across networks of different types and different number of layers. The increase in

8

Table 3: Error rates of the vanilla DNNs v.s. the WNLL activated DNNs over the whole CIFAR100
dataset. (Median of 5 independent trials)

Network Vanilla DNNs WNLL DNNs
VGG11
VGG13
VGG16
VGG19
ResNet20
ResNet32
ResNet44
ResNet56

28.80%
25.21%
25.72%
25.07%
31.53%
28.04%
26.32%
25.36%

32.68%
29.03%
28.59%
28.55%
35.79%
32.01%
31.07%
30.03%

Network
ResNet110
ResNet18
ResNet34
ResNet50

PreActResNet18
PreActResNet34
PreActResNet50

Vanilla DNNs WNLL DNNs

28.86%
27.57%
25.55%
25.09%
28.62%
26.84%
25.95%

23.74%
22.89%
20.78%
20.45%
23.45%
21.97%
21.51%

generalization accuracy could also be used to train smaller models with the same accuracy  which has
great potential for the mobile device applications.

5.1 Limitation and Future Work

There are several limitations of our framework to improve which we wish to remove. Currently  the
manifold interpolation step is still a computational bottleneck in both speed and memory. During
the interpolation  in order to make the interpolation valid  the batch size needs to be quasilinear with
respect to the number of classes. tTis pose memory challenges for the ImageNet dataset [7]. Another
important issue is the approximation of the gradient of the WNLL activation function. Linear function
is one option but it is far from optimal. We believe a better harmonic function approximation can
further lift the model’s performance.
Due to the robustness and generalization capabilities shown by our experiments  we conjecture
that by using the interpolation function as output activation  neural nets can become more stable
to perturbations and adversarial attacks [25]. The reason for this stability conjecture is because
our framework combines both learned decision boundary and nearest neighbor information for
classiﬁcation.

9

Acknowledgments

This material is based on research sponsored by the Air Force Research Laboratory and DARPA under
agreement number FA8750-18-2-0066. And by the U.S. Department of Energy  Ofﬁce of Science
and by National Science Foundation  under Grant Numbers DOE-SC0013838 and DMS-1554564 
(STROBE). And by the NSF DMS-1737770 and the Simons foundation. The U.S. Government
is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any
copyright notation thereon.

References
[1] F. Agostinelli  M. Hoffman  P. Sadowski  and P. Baldi. Learning activation functions to improve

deep neural networks. arXiv preprint arXiv:1412.6830  2014.

[2] M. Courbariaux Y. Bengio and J. David. Binaryconnet: Training deep neural networks with

binary weights. NIPS  2015.

[3] Y. Bengio  P. Lamblin  D. Popovici  and H. Larochelle. Greedy layer-wise training of deep

networks. NIPS  2007.

[4] J. Calder. The game theoretic p-laplacian and semi-supervised learning with few labels.

ArXiv:1711.10144  2018.

[5] Bo Chang  Lili Meng  Eldad Haber  Frederick Tung  and David Begert. Multi-level residual

networks from dynamical systems view. arXiv preprint arXiv:1710.10348  2017.

[6] Y. Chen  J. Li  H. Xiao  X. Jin  S. Yan  and J. Feng. Dual path networks. NIPS  2017.

[7] J. Deng  W. Dong.  R. Socher  J. Li  K. Li  and F. Li. ImageNet: A Large-Scale Hierarchical

Image Database. In CVPR09  2009.

[8] X. Glorot  A. Bordes  and Y. Bengio. Deep sparse rectiﬁer neural networks. In Proceedings
of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics  pages 315–
323  2011.

[9] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems 
pages 2672–2680  2014.

[10] I. Goodfellow  D. Warde-Farley  M. Mirza  A. Courville  and Y. Bengio. Maxout networks.

arXiv preprint arXiv:1302.4389  2013.

[11] K. He  X. Zhang  S. Ren  and J. Sun. Delving deep into rectiﬁers: Surpassing human-level
performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference
on computer vision  pages 1026–1034  2015.

[12] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. CVPR 

pages 770–778  2016.

[13] K. He  X. Zhang  S. Ren  and J. Sun. Identity mappings in deep residual networks. ECCV 

2016.

[14] G. Hinton  S. Osindero  and T. Teh. A fast learning algorithm for deep belief nets. Neural

Computation  18(7):1527–1554  2006.

[15] G. Hinton  N. Srivastava  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Improving neural
networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 
2012.

[16] G. Huang  Z. Liu  K. Weinberger  and L. van der Maaten. Densely connected convolutional

networks. CVPR  2017.

[17] G. Huang  Y. Sun  Z. Liu  D. Sedra  and K. WeinBerger. Deep networks with stochastic depth.

ECCV  2016.

10

[18] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.

[19] A. Krizhevsky  I. Sutskever  and G. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information processing systems  pages 1097–1105 
2012.

[20] Y. LeCun. The mnist database of handwritten digits. 1998.

[21] Z. Li and Z. Shi. Deep residual learning and pdes on manifold. arXiv preprint arXiv:1708.05115 

2017.

[22] M. Muja and D. Lowe. Scalable nearest neighbor algorithms for high dimensional data. Pattern

Analysis and Machine Intelligence (PAMI)  36  2014.

[23] V. Nair and G. Hinton. Rectiﬁed linear units improve restricted boltzmann machines.

In
Proceedings of the 27th international conference on machine learning (ICML-10)  pages 807–
814  2010.

[24] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Ng. Reading digits in natural images
with unsupervised features learning. NIPS Workshop on Deep Learning and Unsupervised
Feature Learning  2011.

[25] N. Papernot  P. McDaniel  S. Jha  M. Fredrikson  Z. Celik  and A. Swami. The limitations of

deep learning in adversarial settings. ArXiv:1511.07528  2015.

[26] A. Paszke and et al. Automatic differentiation in pyTorch. 2017.

[27] Z. Shi  S. Osher  and W. Zhu. Weighted nonlocal Laplacian on interpolation from sparse data.

Journal of Scientiﬁc Computing  73:1164–1177  2017.

[28] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. Arxiv:1409.1556  2014.

[29] Y. Tang. Deep learning using linear support vector machines. ArXiv:1306.0239  2013.

[30] L. Wan  M. Zeiler  S. Zhang  Y. LeCun  and R. Fergus. Regularization of neural networks using

dropconnect. In International Conference on Machine Learning  pages 1058–1066  2013.

[31] S. Zagoruyko and N. Komodakis. Wide residual networks. BMVC  2016.

[32] W. Zhu  Q. Qiu  J. Huang  R. Carderbank  G. Sapiro  and I. Daubechies. LDMNet: Low

dimensional manifold regularized neural networks. UCLA CAM Report: 17-66  2017.

11

,Bao Wang
Xiyang Luo
Zhen Li
Wei Zhu
Zuoqiang Shi
Stanley Osher