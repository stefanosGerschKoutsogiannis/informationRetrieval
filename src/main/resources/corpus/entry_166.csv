2010,A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups,We deal with the problem of variable selection when  variables must be selected group-wise  with possibly overlapping groups defined a priori. In particular we propose a new optimization procedure  for solving the regularized algorithm presented in Jacob et al. 09  where the group lasso  penalty is generalized to overlapping groups of variables. While in Jacob et al. 09 the proposed implementation requires explicit replication of the variables belonging to more than one group  our iterative procedure is based on a combination of proximal methods in the primal space and constrained Newton method in a reduced dual space  corresponding to the active groups.  This procedure provides a scalable alternative with no need for data duplication  and allows to deal with high dimensional problems without pre-processing  to reduce the  dimensionality of the data.  The computational advantages of our scheme with respect to state-of-the-art algorithms  using data duplication are shown empirically with numerical simulations.,A primal-dual algorithm for group sparse
regularization with overlapping groups

Soﬁa Mosci

DISI- Universit`a di Genova
mosci@disi.unige.it

Silvia Villa

DISI- Universit`a di Genova
villa@dima.unige.it

Alessandro Verri

DISI- Universit`a di Genova
verri@disi.unige.it

Abstract

Lorenzo Rosasco

IIT - MIT

lrosasco@MIT.EDU

We deal with the problem of variable selection when variables must be selected
group-wise  with possibly overlapping groups deﬁned a priori. In particular we
propose a new optimization procedure for solving the regularized algorithm pre-
sented in [12]  where the group lasso penalty is generalized to overlapping groups
of variables. While in [12] the proposed implementation requires explicit repli-
cation of the variables belonging to more than one group  our iterative procedure
is based on a combination of proximal methods in the primal space and projected
Newton method in a reduced dual space  corresponding to the active groups. This
procedure provides a scalable alternative with no need for data duplication  and
allows to deal with high dimensional problems without pre-processing for dimen-
sionality reduction. The computational advantages of our scheme with respect
to state-of-the-art algorithms using data duplication are shown empirically with
numerical simulations.

1

Introduction

Sparsity has become a popular way to deal with small samples of high dimensional data and  in a
broad sense  refers to the possibility of writing the solution in terms of a few building blocks. Often 
sparsity based methods are the key towards ﬁnding interpretable models in real-world problems. In
particular  regularization based on (cid:96)1 type penalties is a powerful approach for dealing with the prob-
lem of variable selection  since it provides sparse solutions by minimizing a convex functional. The
success of (cid:96)1 regularization motivated exploring different kinds of sparsity properties for (general-
ized) linear models  exploiting available a priori information  which restricts the admissible sparsity
patterns of the solution. An example of a sparsity pattern is when the input variables are partitioned
into groups (known a priori)  and the goal is to estimate a sparse model where variables belonging to
the same group are either jointly selected or discarded. This problem can be solved by regularizing
with the group-(cid:96)1 penalty  also known as group lasso penalty  which is the sum  over the groups  of
the euclidean norms of the coefﬁcients restricted to each group.
A possible generalization of group lasso is to consider groups of variables which can be potentially
overlapping  and the goal is to estimate a model which support is the union of groups. This is a
common situation in bioinformatics (especially in the context of high-throughput data such as gene
expression and mass spectrometry data)  where problems are characterized by a very low number of
samples with several thousands of variables. In fact  when the number of samples is not sufﬁcient
to guarantee accurate model estimation  an alternative is to take advantage of the huge amount of
prior knowledge encoded in online databases such as the Gene Ontology. Largely motivated by ap-
plications in bioinformatics  a new type of penalty is proposed in [12]  which is shown to give better

1

performances than simple (cid:96)1 regularization.
A straightforward solution to the minimization problem underlying the method proposed in [12] is
to apply state-of-the-art techniques for group lasso (we recall interior-points methods [3  20]  block
coordinate descent [16]  and proximal methods [9  21]  also known as forward-backward splitting
algorithms  among others) in an expanded space  built by duplicating variables that belong to more
than one group.
As already mentioned in [12]  though very simple  such an implementation does not scale to large
datasets  when the groups have signiﬁcant overlap  and a more scalable algorithm with no data du-
plication is needed. For this reason we propose an alternative optimization approach to solve the
group lasso problem with overlap. Our method does not require explicit replication of the features
and is thus more appropriate to deal with high dimensional problems with large groups overlap.
Our approach is based on a proximal method (see for example [18  6  5])  and two ad hoc results
that allow to efﬁciently compute the proximity operator in a much lower dimensional space: with
Lemma 1 we identify the subset of active groups  whereas in Theorem 2 we formulate the reduced
dual problem for computing the proximity operator  where the dual space dimensionality coincides
with the number of active groups. The dual problem can then be solved via Bertsekas’ projected
Newton method [7]. We recall that a particular overlapping structure is the hierarchical structure 
where the overlap between groups is limited to inclusion of a descendant in its ancestors. In this case
the CAP penalty [24] can be used for model selection  as it has been done in [2  13]  but ancestors
are forced to be selected when any of their descendant are selected. Thanks to the nested structure 
the proximity operator of the penalty term can be computed exactly in a ﬁnite number of steps [14].
This is no longer possible in the case of general overlap. Finally it is worth noting that the penalty
analyzed here can be applied also to hierarchical group lasso. Differently from [2  13] selection of
ancestors is no longer enforced.
The paper is organized as follows. In Section 2 we recall the group lasso functional for overlap-
ping groups and set some notations. In Section 3 we state the main results  present a new iterative
optimization procedure  and discuss computational issues. Finally in Section 4 we present some
numerical experiments comparing running time of our algorithm with state-of-the-art techniques.
The proofs are reported in the Supplementary material.

j∈G β2

notation (cid:107)β(cid:107)G = ((cid:80)

generalized linear model f(x) =(cid:80)d

2 Problem and Notations
We ﬁrst ﬁx some notations. Given a vector β ∈ Rd  while (cid:107)·(cid:107) denotes the (cid:96)2-norm  we will use the
j )1/2 to denote the (cid:96)2-norm of the components of β in G ⊂ {1  . . .   d}.
Then  for any differentiable function f : RB → R  we denote by ∂rf its partial derivative with
respect to variables r  and by ∇f = (∂rf)B
We are now ready to cast group (cid:96)1 regularization with overlapping groups as the following varia-
tional problem. Given a training set {(xi  yi)n
j=1  and B subsets
of variables G = {Gr}B
r=1 with Gr ⊂ {1  . . .   d}  we assume the estimator to be described by a
j=1 ψj(x)βj and consider the following regularization scheme
(1)
where Ψ is the n × d matrix given by the features ψj in the dictionary evaluated in the training set
i=1 (cid:96) (f(xi)  yi)  when
points  [Ψ]i j = ψj(xi). The term 1
the cost function1 (cid:96) : R × Y → R+ is the square loss  (cid:96)(f(x)  y) = (y − f(x))2.
The penalty term ΩGoverlap : Rd → R+ is lower semicontinuous  convex  and one-homogeneous 
(ΩGoverlap(λβ) = λΩGoverlap(β) ∀β ∈ Rd and λ ∈ R+)  and is deﬁned as

r=1 its gradient.
i=1} ∈ (X × Y )n  a dictionary (ψj)d
(cid:27)

Eτ (β) = argmin
β∈Rd

(cid:107)Ψβ − y(cid:107)2 + 2τΩG

n

n (cid:107)Ψβ − y(cid:107)2 is the empirical error  1
B(cid:88)

(v1 ... vB ) vr∈Rd supp(vr)⊂Gr PB

inf

r=1 vr=β

β∗ = argmin
β∈Rd

ΩG
overlap(β) =

(cid:26) 1

n

overlap(β)

 

(cid:80)n

(cid:107)vr(cid:107) .

r=1

The functional ΩGoverlap was introduced in [12] as a generalization of the group lasso penalty to
allow overlapping groups  while maintaining the group lasso property of enforcing sparse solutions
which support is a union of groups. When groups do not overlap  ΩGoverlap reduces to the group lasso

1Note our analysis would immediately apply to other loss functions  e.g. the logistic loss.

2

penalty. Note that  as pointed out in [12]  using(cid:80)B

r=1 (cid:107)β(cid:107)Gr

as generalization of the group lasso
penalty leads to a solution which support is the complement of the union of groups. For an extensive
study of the properties of ΩGoverlap  its comparison with the (cid:96)1 norm  and its extension to graph lasso 
we therefore refer the interested reader to [12].

3 The GLO-pridu Algorithm

If one needs to solve problem (1) for high dimensional data  the use of standard second-order meth-
ods such as interior-point methods is precluded (see for instance [6])  since they need to solve large
systems of linear equations to compute the Newton steps. On the other hand  ﬁrst order methods
inspired to Nesterov’s seminal paper [19] (see also [18]) and based on proximal methods already
proved to be a computationally efﬁcient alternative in many machine learning applications [9  21].

3.1 A Proximal algorithm
Given the convex functional Eτ in (1)  which is sum of a differentiable term  namely 1
n (cid:107)Ψβ − y(cid:107)2 
and a non-differentiable one-homogeneous term 2τΩGoverlap  its minimum can be computed with
following acceleration of the iterative forward-backward splitting scheme
ΨT (Ψhp − y)

βp =(cid:0)I − πτ /σK

(cid:1)(cid:18)

(cid:19)

hp − 1
nσ

(cid:16)−cp +

(cid:113)

(cid:17)

cp = (1 − tp)cp−1 

tp+1 =
hp+1 = βp(1 − tp+1 + tp+1
tp

p + 8cp
c2
) + βp−1(tp − 1) tp+1
tp

/4

(2)

σ ΩGoverlap reduces to the identity minus the projection onto the subdifferential of τ

for a suitable choice of σ. Due to one-homogeneity of ΩGoverlap  the proximity operator associated
σ ΩGoverlap at
to τ
the origin  which is a closed and convex set. We will denote such a projection as πτ /σK  where
K = ∂ΩGoverlap(0). The above scheme is inspired to [10]  and is equivalent to the algorithm named
FISTA [5]  which convergence is guaranteed  as recalled in the following theorem
Theorem 1 Given β0 ∈ Rd  and σ = ||ΨT Ψ||/n  let h1 = β0 and t1 = 1  c0 = 1  then there exists
a constant C0 such that the iterative update (10) satisﬁes

(3)
As it happens for other accelerations of the basic forward-backward splitting algorithm such as [19 
6  4]  convergence of the sequence βp is no longer guaranteed unless strong convexity is assumed.
However  sacriﬁcing theoretical convergence for speed may be mandatory in large scale applications.
Furthermore  there is a strong empirical evidence that βp is indeed convergent (see Section 4).

Eτ (βp) − Eτ (β∗) ≤ C0
p2 .

3.2 The projection
Note that the proximity operator of the penalty ΩGoverlap does not admit a closed form and must be
computed approximatively. In fact the projection on the convex set

K = ∂ΩG

overlap(0) = {v ∈ Rd (cid:107)v(cid:107)Gr ≤ 1 for r = 1  . . .   B}.

cannot be decomposed group-wise  as in standard group (cid:96)1 regularization  which proximity operator
resolves to a group-wise soft-thresholding operator (see Eq. (9) later). Nonetheless  the following
lemma shows that  when evaluating the projection  πK  we can restrict ourselves to a subset of
ˆB = | ˆG| ≤ B active groups. This equivalence is crucial for speeding up the algorithm  in fact ˆB is
the number of selected groups which is small if one is interested in sparse solutions.
Lemma 1 Given β ∈ Rd  G = {Gr}B
convex set τ K with K = {v ∈ Rd (cid:107)v(cid:107)Gr ≤ 1 for r = 1  . . .   B} is given by

r=1 with Gr ⊂ {1  . . .   d}  and τ > 0  the projection onto the

Minimize
subject to

where ˆG := {G ∈ G  (cid:107)β(cid:107)G > τ} .

(cid:107)v − β(cid:107)2
v ∈ Rd (cid:107)v(cid:107)G ≤ τ for G ∈ ˆG.

(4)

3

The proof (given in the supplementary material) is based on the fact that the convex set τ K is the
intersection of cylinders that are all centered on a coordinate subspace. Since ˆB is typically much
smaller than d  it is convenient to solve the dual problem associated to (4).
Theorem 2 Given β ∈ Rd  {Gr}B
convex set τ K with K = {v ∈ Rd (cid:107)v(cid:107)Gr

r=1 with Gr ⊂ {1  . . .   d}  and τ > 0  the projection onto the

≤ τ for r = 1  . . .   B} is given by

[πτ K(β)]j =

where λ∗ is the solution of

(1 +(cid:80) ˆB

βj
r=1 λ∗

for j = 1  . . .   d

(5)

r1r j)

d(cid:88)

1 +(cid:80) ˆB

−β2
r=1 1r jλr

j

ˆB(cid:88)

−

f(λ) 

with f(λ) :=

argmax
λ∈R ˆB
+

(6)
ˆG = {G ∈ G  (cid:107)β(cid:107)G > τ} := { ˆG1  . . .   ˆG ˆB}  and 1r j is 1 if j belongs to group ˆGr and 0 otherwise.
Equation (6) is the dual problem associated to (4)  and  since strong duality holds  the minimum
of (4) is equal to the maximum of the dual problem  which can be efﬁciently solved via Bertsekas’
projected Newton method described in [7]  and here reported as Algorithm 1.

λrτ 2 

r=1

j=1

Algorithm 1 Projection

Given: β ∈ Rd  λinit ∈ R ˆB  η ∈ (0  1)  δ ∈ (0  1/2)   > 0
Initialize: q = 0  λ0 = λinit
r = 0  or |∂rf(λq)| >  if λq
while (∂rf(λq) > 0 if λq

q := q + 1

r > 0  for r = 1  . . .   ˆB) do

q = min{ ||λq − [λq − ∇f(λq)]+||}

I q
+ = {r such that 0 ≤ λq

r ≤ q  ∂rf(λq) > 0}
+or s ∈ I q

if r (cid:54)= s  and r ∈ I q

+

Hr s =

∂r∂sf(λq) otherwise
λ(α) = [λq − α(H q)−1∇f(λq)]+

∂rf(λq) +(cid:80)

r /∈Iq
+

r∈Iq
+

∂rf(λq)[λq

(7)

do

(cid:111)
r − λr(ηm)]

(cid:26)0
(cid:110)
ηm(cid:80)

m = 0
while f(λq) − f(λ(ηm)) ≥ δ

m := m + 1

end while

end while
return λq+1

λq+1 = λ(ηm)

Bertsekas’ iterative scheme combines the basic simplicity of the steepest descent iteration [22] with
the quadratic convergence of the projected Newton’s method [8]. It does not involve the solution of
a quadratic program thereby avoiding the associated computational overhead.

3.3 Computing the regularization path

In Algorithm 2 we report the complete Group Lasso with Overlap primal-dual (GLO-pridu) scheme
for computing the regularization path  i.e. the set of solutions corresponding to different values of
the regularization parameter τ1 > . . . > τT   for problem (1). Note that we employ the continuation
strategy proposed in [11]. A similar warm starting is applied to the inner iteration  where at the p-th
step λinit is determined by the solution of the (p−1)-th projection. Such an initialization empirically
proved to guarantee convergence  despite the local nature of Bertsekas’ scheme.

3.4 The replicates formulation

An alternative way to solve the optimization problem (1) is proposed by [12]  where the authors
show that problem (1) is equivalent to the standard group (cid:96)1 regularization (without overlap) in an
expanded space built by replicating variables belonging to more than one group:

4

Algorithm 2 GLO-pridu regularization path

Given: τ1 > τ2 > ··· > τT  G  η ∈ (0  1)  δ ∈ (0  1/2)  0 > 0  ν > 0
Let: σ = ||ΨT Ψ||/n
Initialize: β(τ0) = 0
for t = 1  . . .   T do

0 = 0

Initialize: β0 = β(τt−1)  λ∗
while ||βp − βp−1|| > ν||βp−1|| do
• w = hp − (nσ)−1ΨT (Ψhp − y)
• Find ˆG = {G ∈ G (cid:107)w(cid:107)G ≥ τ}
• Compute λ∗
• Compute βp as βp
• Update cp  tp  and hp as in (10)

j = wj(1 +(cid:80) ˆB

end while
β(τt) = βp

end for
return β(τ1)  . . .   β(τT )

p via Algorithm 1 with groups ˆG  initialization λ∗

p−1 and tolerance 0p−3/2
r 1r j)−1 for j = 1  . . .   d  see Equation (5)

r=1 λq+1

(cid:40)

(cid:41)

B(cid:88)

|| ˜Ψ ˜β − y||2 + 2τ

|| ˜β|| ˜Gr

˜β∗ ∈ argmin
˜β∈R ˜d

1
n

 

r=1

|GB|  . . .   ˜d|]}  and ˜d = (cid:80)B

(8)
where ˜Ψ is the matrix built by concatenating copies of Ψ restricted each to a certain group  i.e.
= (Ψj)j∈Gr  where { ˜G1  . . .   ˜GB} = {[1  . . .  |G1|]  [1+|G1|  . . .  |G1|+|G2|]  . . .   [ ˜d−
( ˜Ψj)j∈ ˜Gr
r=1 |Gr| is the number of total variables obtained after including the
r=1 φGr( ˜β∗)  where φGr : R ˜d → Rd
replicates. One can then reconstruct β∗ from ˜β∗ as β∗
maps ˜β in v ∈ Rd  such that supp(v) ⊂ Gr and (vj)j∈Gr = ( ˜βj)j∈ ˜Gr
  for r = 1  . . .   B. The main
advantage of the above formulation relies on the possibility of using any state-of-the-art optimization
procedure for group lasso. In terms of proximal methods  a possible solution is given by Algorithm
3  where Sτ /σ is the proximity operator of the new penalty  and can be computed exactly as

j =(cid:80)B

− τ
σ

˜βj 

+

for j ∈ ˜Gr 

for r = 1  . . .   B.

(9)

(cid:16)

(cid:17)

(cid:16)|| ˜β|| ˜Gr

(cid:17)

Sτ /σ( ˜β)
Algorithm 3 GL-prox

=

j

Given: ˜β0 ∈ Rd  τ > 0  σ = || ˜ΨT ˜Ψ||/n
Initialize: p = 0  ˜h1 = ˜β0  t1 = 1
while convergence not reached do

p := p + 1

(cid:16)˜hp − (nσ)−1 ˜ΨT ( ˜Ψ˜hp − y)
(cid:113)

˜βp = Sτ /σ
cp = (1 − tp)cp−1 
tp+1 =
˜hp+1 = ˜βp(1 − tp+1 + tp+1
tp

(−cp +

1
p + 8cp)
c2
4
) + ˜βp−1(tp − 1) tp+1
tp

(cid:17)

(10)

end while
return ˜βp

Note that in principle  by applying Lemma 1  the group-soft-thresholding operator in (9) can be com-
puted only on the active groups. In practice this does not yield any advantage  since the identiﬁcation
of the active groups has the same computational cost of the thresholding itself.

3.5 Computational issues

For both GL-prox and GLO-pridu  the complexity of one iteration is the sum of the complexity of
computing the gradient of the data term and the complexity of computing the proximity operator
of the penalty term. The former has complexity O(dn) and O( ˜dn) for GLO-pridu and GL-prox 

5

respectively  for the case n < d. One should then add at each iteration  the cost of performing the
projection onto K. This can be neglected for the case of replicated variables.On the other hand 
the time complexity of one iteration for Algorithm 1 is driven by the number of active groups ˆB.
This number is typically small when looking for sparse solutions. The complexity is thus given
by the sum of the complexity of evaluating the inverse of the ˆB × ˆB matrix H  O( ˆB3)  and the
complexity of performing the product H−1∇g(λ)  O( ˆB2). The worst case complexity would then
be O( ˆB3). Nevertheless  in practice the complexity is much lower because matrix H is highly
sparse. In fact  Equation (7) tells us that the part of matrix H corresponding to the active set I+ is
diagonal. As a consequence  if ˆB = ˆB− + ˆB+  where ˆB− is the number of non active constraints 
and ˆB+ is the number of active constraints  then the complexity of inverting matrix H is at most
O( ˆB+) + O( ˆB3−). Furthermore the ˆB−× ˆB− non diagonal part of matrix H is highly sparse  since
Hr s = 0 if ˆGr ∩ ˜Gs = ∅ and the complexity of inverting it is in practice much lower than O( ˆB3−).
The worst case complexity for computing the projection onto K is thus O(q · ˆB+) + O(q · ˆB3−) 
where q is the number of iterations necessary to reach convergence. Note that even if  in order to
guarantee convergence  the tolerance for evaluating convergence of the inner iteration must decrease
with the number of external iterations  in practice  thanks to warm starting  we observed that q is
rarely greater than 10 in the experiments presented here.
Concerning the number of iterations required to reach convergence for GL-prox in the replicates
formulation  we empirically observed that it requires a much higher number of iterations than GLO-
pridu (see Table 3). We argue that such behavior is due to the combination of two occurences: 1) the
local condition number of matrix ˜Ψ is 0 even if Ψ is locally well conditioned  2) the decomposition
of β∗ as ˜β∗ is possibly not unique  which is required in order to have a unique solution for (8).
The former is due to the presence of replicated columns in ˜Ψ. In fact  since Eτ is convex but not
necessarily strictly convex – as when n < d –  uniqueness and convergence is not always guaranteed
unless some further assumption is imposed. Most convergence results relative to (cid:96)1 regularization
link uniqueness of the solution as well as the rate of convergence of the Soft Thresholding Iteration
to some measure of local conditioning of the Hessian of the differentiable part of Eτ (see for instance
Proposition 4.1 in [11]  where the Hessian restricted to the set of relevant variables is required to
be full rank). In our case the Hessian for GL-prox is simply ˜H = 1/n ˜ΨT ˜Ψ  so that  if the relevant
groups have non null intersection  then ˜H restricted to the set of relevant variables is by no means
full rank. Concerning the latter argument  we must say that in many real world problems  such as
bioinformatics  one cannot easily verify that the solution indeed has a unique decomposition. In
fact  we can think of trivial examples where the replicates formulation has not a unique solution.

4 Numerical Experiments

In this section we present numerical experiments aimed at comparing the running time performance
of GLO-pridu with state-of-the-art algorithms. To ensure a fair comparison  we ﬁrst run some pre-
liminary experiments to identify the fastest codes for group (cid:96)1 regularization with no overlap. We
refer to [6] for an extensive empirical and theoretical comparison of different optimization proce-
dures for solving (cid:96)1 regularization. Further empirical comparisons can be found in [15].

4.1 Comparison of different implementations for standard group lasso

We considered three algorithms which are representative of the optimization techniques used to
solve group lasso: interior-point methods  (group) coordinate descent and its variations  and prox-
imal methods. As an instance of the ﬁrst set of techniques we employed the publicly available
Matlab code at http://www.di.ens.fr/˜fbach/grouplasso/index.htm described
in [1]. For coordinate descent methods  we employed the R-package grlplasso  which imple-
ments block coordinate gradient descent minimization for a set of possible loss functions. In the
following we will refer to these two algorithms as “’GL-IP” and “GL-BCGD”. Finally we use our
Matlab implementation of Algorithm GL-prox as an instance of proximal methods.
We ﬁrst observe that the solutions of the three algorithms coincide up to an error which depends
on each algorithm tolerance. We thus need to tune each tolerance in order to guarantee that all
iterative algorithms are stopped when the level of approximation to the true solution is the same.

6

Table 1: Running time (mean and standard deviation) in seconds for computing the entire regular-
ization path of GL-IP  GL-BCGD  and GL-prox for different values of B  and n. For B = 1000 
GL-IP could not be computed due to memory reasons.

n = 100

GL-IP

GL-BCGD
GL-prox

n = 500

GL-IP

GL-BCGD
GL-prox

B = 10
5.6 ± 0.6
2.1 ± 0.6
0.21 ± 0.04
B = 10
2.30 ± 0.27
2.15 ± 0.16

0.1514 ± 0.0025

n = 1000

GL-IP

GL-BCGD
GL-prox

B = 10
1.92 ± 0.25
2.06 ± 0.26
0.182 ± 0.006

B = 100 B = 1000
60 ± 90
14.4 ± 1.5
2.8 ± 0.6
2.9 ± 0.4
183 ± 19

–

–

B = 1000
B = 100
370 ± 30
16.5 ± 1.2
4.7 ± 0.5
2.54 ± 0.16
109 ± 6
B = 100 B = 1000
328 ± 22
20.6 ± 2.2
18 ± 3
4.7 ± 0.5
112 ± 6

–

Toward this end  we run Algorithm GL-prox with machine precision  ν = 10−16  in order to have
a good approximation of the asymptotic solution. We observe that for many values of n and d  and
over a large range of values of τ  the approximation of GL-prox when ν = 10−6 is of the same
order of the approximation of GL-IP with optparam.tol=10−9  and of GL-BCGD with tol=
10−12. Note also that with these tolerances the three solutions coincide also in terms of selection 
i.e. their supports are identical for each value of τ. Therefore the following results correspond to
optparam.tol = 10−9 for GL-IP  tol = 10−12 for GL-BCGD  and ν = 10−6 for GL-prox.
For the other parameters of GL-IP we used the values used in the demos supplied with the code.
Concerning the data generation protocol  the input variables x = (x1  . . .   xd) are uniformly drawn
from [−1  1]d. The labels y are computed using a noise-corrupted linear regression function  i.e. y =
β· x+ w  where β depends on the ﬁrst 30 variables  βj = 1 if j =1  . . .   30  and 0 otherwise  w is an
additive gaussian white noise  and the signal to noise ratio is 5:1. In this case the dictionary coincides
with the variables  Ψj(x) = xj for j = 1  . . .   d. We then evaluate the entire regularization path for
the three algorithms with B sequential groups of 10 variables  (G1=[1  . . .   10]  G2=[11  . . .   20] 
and so on)  for different values of n and B. In order to make sure that we are working on the correct
range of values for the parameter τ  we ﬁrst evaluate the set of solutions of GL-prox corresponding
to a large range of 500 values for τ  with ν = 10−4. We then determine the smallest value of τ
which corresponds to selecting less than n variables  τmin  and the smallest one returning the null
solution  τmax. Finally we build the geometric series of 50 values between τmin and τmax  and use
it to evaluate the regularization path on the three algorithms. In order to obtain robust estimates of
the running times  we repeat 20 times for each pair n  B.
In Table 1 we report the computational times required to evaluate the entire regularization path for
the three algorithms. Algorithms GL-BCGD and GL-prox are always faster than GL-IP which  due
to memory reasons  cannot by applied to problems with more than 5000 variables  since it requires
to store the d × d matrix ΨT × Ψ. It must be said that the code for GP-IL was made available
mainly in order to allow reproducibility of the results presented in [1]  and is not optimized in terms
of time and memory occupation. However it is well known that standard second-order methods are
typically precluded on large data sets  since they need to solve large systems of linear equations
to compute the Newton steps. GL-BCGD is the fastest for B = 1000  whereas GL-prox is the
fastest for B = 10  100. The candidates as benchmark algorithms for comparison with GLO-pridu
are GL-prox and GL-BCGD. Nevertheless we observed that  when the input data matrix contains
a signiﬁcant fraction of replicated columns  this algorithm does not provide sparse solutions. We
therefore compare GLO-pridu with GL-prox only.

4.1.1 Projection vs duplication

The data generation protocol is equal to the one described in the previous experiments  but β depends
on the ﬁrst 12/5b variables (which correspond to the ﬁrst three groups)

(cid:124) (cid:123)(cid:122) (cid:125)

β = ( c  . . .   c
b·12/5 times

(cid:124)

(cid:123)(cid:122)

  0  0  . . .   0
d−b·12/5 times

).

(cid:125)

7

We then deﬁne B groups of size b  so that ˜d = B · b > d. The ﬁrst three groups correspond to the
subset of relevant variables  and are deﬁned as G1 = [1  . . .   b]  G2 = [4/5b + 1  . . .   9/5b]  and
G3 = [1  . . .   b/5  8/5b + 1  . . .   12/5b]  so that they have a 20% pair-wise overlap. The remaining
B − 3 groups are built by randomly drawing sets of b indexes from [1  d]. In the following we
will let n = 10|G1 ∪ G2 ∪ G3|  i.e. n is ten times the number of relevant variables  and vary d  b.
We also vary the number of groups B  so that the dimension of the expanded space is α times the
input dimension  ˜d = αd  with α = 1.2  2  5. Clearly this amounts to taking B = α · d/b. The
parameter α can be thought of as the average number of groups a single variable belongs to. We
identify the correct range of values for τ as in the previous experiments  using GLO-pridu with loose
tolerance  and then evaluate the running time and the number of iterations necessary to compute the
entire regularization path for GL-prox on the expanded space and GLO-pridu  both with ν = 10−6.
Finally we repeat 20 times for each combination of the three parameters d  b  and α.

Table 2: Running time (mean ± standard deviation) in seconds for b=10 (top)  and b=100 (below).
For each d and α  the left and right side correspond to GLO-pridu  and GL-prox  respectively.

d=1000
d=5000
d=10000

d=1000
d=5000
d=10000

α = 1.2

0.15 ± 0.04
1.1 ± 0.4
2.1 ± 0.7

0.20 ± 0.09
1.0 ± 0.6
2.1 ± 1.4

α = 2

1.6 ± 0.9
1.55 ± 0.29
3.0 ± 0.6

5.1 ± 2.0
2.4 ± 0.7
4.5 ± 1.4

α = 5

12.4 ± 1.3
103 ± 12
460 ± 110

68 ± 8
790 ± 57
2900 ± 400

α = 1.2

11.7 ± 0.4
31 ± 13
16.6 ± 2.1

24.1 ± 2.5
38 ± 15
13 ± 3

α = 2

11.6 ± 0.4
90 ± 5
90 ± 30

42 ± 4
335 ± 21
270 ± 120

13.5 ± 0.7
85 ± 3
296 ± 16

1467 ± 13
1110 ± 80

–

α = 5

Table 3: Number of iterations (mean ± standard deviation) for b = 10 (top) and b = 100 (below).
For each d and α  the left and right side correspond to GLO-pridu  and GL-prox  respectively.

d=1000
d=5000
d=10000

d=1000
d=5000
d=10000

α = 1.2

100 ± 30
100 ± 40
100 ± 30

80 ± 30
70 ± 30
70 ± 40

1200 ± 500
148 ± 25
160 ± 30

α = 2

1900 ± 800
139 ± 24
137 ± 26
α = 2

α = 5

2150 ± 160
6600 ± 500
13300 ± 1900

11000 ± 1300
27000 ± 2000
49000 ± 6000

α = 5

α = 1.2

913 ± 12
600 ± 400
81 ± 11

2160 ± 210
600 ± 300
63 ± 11

894 ± 11
1860 ± 110
1000 ± 500

2700 ± 300
4590 ± 290
1800 ± 900

895 ± 10
1320 ± 30
2100 ± 60

4200 ± 400
6800 ± 500

–

Running times and number of iterations are reported in Table 2 and 3  respectively. When the degree
of overlap α is low the computational times of GL-prox and GLO-pridu are comparable. As α
increases  there is a clear advantage in using GLO-pridu instead of GL-prox. The same behavior
occurs for the number of iterations.

5 Discussion

We have presented an efﬁcient optimization procedure for computing the solution of group lasso
with overlapping groups of variables  which allows dealing with high dimensional problems with
large groups overlap. We have empirically shown that our procedure has a great computational
advantage with respect to state-of-the-art algorithms for group lasso applied on the expanded space
built by replicating variables belonging to more than one group. We also mention that computational
performance may improve if our scheme is used as core for the optimization step of active set
methods  such as [23]. Finally  as shown in [17]  the improved computational performance enables
to use group (cid:96)1 regularization with overlap for pathway analysis of high-throughput biomedical data 
since it can be applied to the entire data set and using all the information present in online databases 
without pre-processing for dimensionality reduction.

8

References
[1] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine

Learning Research  9:1179–1225  2008.

[2] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning.

Technical Report HAL 00413473  INRIA  2009.

[3] F. R. Bach  G. Lanckriet  and M. I. Jordan. Multiple kernel learning  conic duality  and the smo

algorithm. In ICML  volume 69 of ACM International Conference Proceeding Series  2004.

[4] A. Beck and Teboulle. M. Fast gradient-based algorithms for constrained total variation image
denoising and deblurring problems. IEEE Transactions on Image Processing  18(11):2419–
2434  2009.

[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM J. Imaging Sci.  2(1):183–202  2009.

[6] S. Becker  J. Bobin  and E. Candes. Nesta: A fast and accurate ﬁrst-order method for sparse

recovery  2009.

[7] D. Bertsekas. Projected newton methods for optimization problems with simple constraints.

SIAM Journal on Control and Optimization  20(2):221–246  1982.

[8] R. Brayton and J. Cullum. An algorithm for minimizing a differentiable function subject to. J.

Opt. Th. Appl.  29:521–558  1979.

[9] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting.

Journal of Machine Learning Research  10:28992934  December 2009.

[10] O. Guler. New proximal point algorithm for convex minimization. SIAM J. on Optimization 

2(4):649–664  1992.

[11] E. T. Hale  W. Yin  and Y. Zhang. Fixed-point continuation for l1-minimization: Methodology

and convergence. SIOPT  19(3):1107–1130  2008.

[12] L. Jacob  G. Obozinski  and J.-P. Vert. Group lasso with overlap and graph lasso. In ICML 

page 55  2009.

[13] R. Jenatton  J.-Y . Audibert  and F. Bach. Structured variable selection with sparsity-inducing

norms. Technical report  INRIA  2009.

[14] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for sparse hierarchical

dictionary learning. In Proceeding of ICML 2010  2010.

[15] I. Loris. On the performance of algorithms for the minimization of l1-penalized functionals.

Inverse Problems  25(3):035008  16  2009.

[16] L. Meier  S. van de Geer  and P. Buhlmann. The group lasso for logistic regression. J. R.

Statist. Soc  B(70):53–71  2008.

[17] S. Mosci  S. Villa  Verri A.  and L. Rosasco. A fast algorithm for structured gene selection.

presented at MLSB 2010  Edinburgh.

[18] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of con-

vergence o(1/k2). Doklady AN SSSR  269(3):543–547  1983.

[19] Y. Nesterov.

Smooth minimization of non-smooth functions. Math. Prog. Series A 

103(1):127–152  2005.

[20] M. Y. Park and T. Hastie. L1-regularization path algorithm for generalized linear models. J. R.

Statist. Soc. B  69:659–677  2007.

[21] L. Rosasco  M. Mosci  S. Santoro  A. Verri  and S. Villa.

Iterative projection methods for

structured sparsity regularization. Technical Report MIT-CSAIL-TR-2009-050  MIT  2009.

[22] J. Rosen. The gradient projection method for nonlinear programming  part i: linear constraints.

J. Soc. Ind. Appl. Math.  8:181–217  1960.

[23] V. Roth and B. Fischer. The group-lasso for generalized linear models: uniqueness of solutions

and efﬁcient algorithms. In Proceedings of 25th ICML  2008.

[24] P. Zhao  G. Rocha  and B. Yu. The composite absolute penalties family for grouped and

hierarchical variable selection. Annals of Statistics  37(6A):3468–3497  2009.

9

,Trung Nguyen
Edwin Bonilla
Yunwen Lei
Ke Tang