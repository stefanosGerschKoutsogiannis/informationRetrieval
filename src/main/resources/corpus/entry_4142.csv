2019,Optimistic Distributionally Robust Optimization for Nonparametric Likelihood Approximation,The likelihood function is a fundamental component in Bayesian statistics. However  evaluating the likelihood of an observation is computationally intractable in many applications. In this paper  we propose a non-parametric approximation of the likelihood that identifies a probability measure which lies in the neighborhood of the nominal measure and that maximizes the probability of observing the given sample point. We show that when the neighborhood is constructed by the Kullback-Leibler divergence  by moment conditions or by the Wasserstein distance  then our optimistic likelihood can be determined through the solution of a convex optimization problem  and it admits an analytical expression in particular cases. We also show that the posterior inference problem with our optimistic likelihood approximation enjoys strong theoretical performance guarantees  and it performs competitively in a probabilistic classification task.,Optimistic Distributionally Robust Optimization
for Nonparametric Likelihood Approximation

Viet Anh Nguyen
Soroosh Shaﬁeezadeh-Abadeh
École Polytechnique Fédérale de Lausanne  Switzerland
{viet-anh.nguyen  soroosh.shafiee}@epfl.ch

Man-Chung Yue

The Hong Kong Polytechnic University  Hong Kong

manchung.yue@polyu.edu.hk

Daniel Kuhn

École Polytechnique Fédérale de Lausanne  Switzerland

daniel.kuhn@epfl.ch

Imperial College Business School  United Kingdom

Wolfram Wiesemann

ww@imperial.ac.uk

Abstract

The likelihood function is a fundamental component in Bayesian statistics. How-
ever  evaluating the likelihood of an observation is computationally intractable
in many applications. In this paper  we propose a non-parametric approximation
of the likelihood that identiﬁes a probability measure which lies in the neighbor-
hood of the nominal measure and that maximizes the probability of observing the
given sample point. We show that when the neighborhood is constructed by the
Kullback-Leibler divergence  by moment conditions or by the Wasserstein distance 
then our optimistic likelihood can be determined through the solution of a convex
optimization problem  and it admits an analytical expression in particular cases.
We also show that the posterior inference problem with our optimistic likelihood
approximation enjoys strong theoretical performance guarantees  and it performs
competitively in a probabilistic classiﬁcation task.

1

Introduction

Bayesian statistics is a versatile mathematical framework for estimation and inference  with appli-
cations in bioinformatics [1]  computational biology [40  41]  neuroscience [50]  natural language
processing [24  34]  computer vision [21  25]  robotics [13]  machine learning [28  46]  etc. A
Bayesian inference model is composed of an unknown parameter θ from a known parameter space
Θ  an observed sample point x from a sample space X ⊆ Rm  a likelihood measure (or conditional
density) p(·|θ) over X and a prior distribution π(·) over Θ. The key objective of Bayesian statistics
is the computation of the posterior distribution p(·|x) over Θ upon observing x.
Unfortunately  computing the posterior is a challenging task in practice. Bayes’ theorem  which
relates the posterior to the prior [42  Theorem 1.31]  requires the evaluation of both the likelihood
function p(·|θ) and the evidence p(x). Evaluating the likelihood p(·|θ) at an observation x ∈ X is

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

an intractable problem in many situations. For example  the statistical model may contain hidden
variables ζ  and the likelihood p(x|θ) can only be computed by marginalizing out the hidden variables
p(x|θ) =(cid:82) p(x  ζ|θ)dζ [32  pp. 322]. In the g-and-k model  the density function does not exist in
closed form and can only be expressed in terms of the derivatives of quantile functions  which implies
that p(x|θ) needs to be computed numerically for each individual observation x [18]. Likewise 
evaluating the evidence p(x) is intractable whenever the evaluation of the likelihood p(x|θ) is. To
avoid calculating p(x) in the process of constructing the posterior  the variational Bayes approach [8]
maximizes the evidence lower bound (ELBO)  which is tantamount to solving

minQ∈Q KL(Q (cid:107) π) − EQ[log p(x|θ)] 

(1)
where KL(Q (cid:107) π) denotes the Kullback-Leibler (KL) divergence from Q to π. One can show that if
the feasible set Q contains all probability measures supported on Θ  then the optimal solution Q(cid:63)
of (1) coincides with the true posterior distribution. Consequently  inferring the posterior is equivalent
to solving the convex optimization problem (1) that depends only on the prior distribution π and the
likelihood p(x|θ). There are scalable algorithms to solve the ELBO maximization problem [19]  and
the variational Bayes approach has been successfully applied in inference tasks [15  16]  reinforcement
learning [20  30]  dimensionality reduction [33] and training deep neural networks [22]. Nevertheless 
the variational Bayes approach requires both perfect knowledge and a tractable representation of the
likelihood p(x|θ)  which is often not available in practice.
While the likelihood p(x|θ) may be intractable to compute  we can approximate p(x|θ) from available
data in many applications. For example  in the classiﬁcation task where Θ = {θ1  . . .   θC} denotes
the class labels  the class conditional probabilities p(x|θi) and the prior distribution π(θi) can be
inferred from the training data  and a probabilistic classiﬁer can be constructed by assigning x to
each class randomly under the posterior distribution [7  pp. 43]. Approximating the intractable
likelihood from available samples is also the key ingredient of approximate Bayesian computation
(ABC)  a popular statistical method for likelihood-free inference that has gained widespread success
in various ﬁelds [2  12  47]. The sampling-based likelihood algorithm underlying ABC assumes that

1
N

Kh (d(x (cid:98)x)) p((cid:98)x|θ)d(cid:98)x ≈

we have access to a simulation device that can generate N i.i.d. samples(cid:98)x1  . . .  (cid:98)xN from p(·|θ)  and

it approximates the likelihood p(x|θ) by the surrogate ph(x|θ) deﬁned as
N(cid:88)j=1

ph(x|θ) = (cid:90)X

where Kh is a kernel function with kernel width h  d(· ·) is a distance on X   and the approximation
is due to the reliance upon ﬁnitely many samples [37  39].
In this paper  we propose an alternative approach to approximate the likelihood p(x|θ). We assume
that the sample space X is countable  and hence p(·|θ) is a probability mass function. We model the
on X   which in practice typically represents the empirical distribution supported on the (possibly
simulated) training samples. We then approximate the likelihood p(x|θ) by the optimal value of the
following non-parametric optimistic likelihood problem
(3)

decision maker’s nominal belief about p(·|θ) by a nominal probability mass function(cid:98)νθ supported

Kh (d(x (cid:98)xj))  

(2)

sup

ν∈Bθ((cid:98)νθ)

ν(x) 

[3  29  49]. In contrast to the distributionally robust optimization paradigm  which would look for a
worst-case measure that minimizes the probability of observing x among all measures contained in

quantity. Thus  problem (3) is closely related to the literature on practicing optimism upon facing
ambiguity  which has been shown to be beneﬁcial in multi-armed bandit problems [10]  planning
[31]  classiﬁcation [6]  image denoising [17]  Bayesian optimization [9  45]  etc.

where Bθ((cid:98)νθ) is a set that contains all probability mass functions in the vicinity of (cid:98)νθ. In the
distributionally robust optimization literature  the set Bθ((cid:98)νθ) is referred to as the ambiguity set
Bθ((cid:98)νθ)  the optimistic likelihood problem (3) determines a best-case measure that maximizes this
The choice of the set Bθ((cid:98)νθ) in (3) directly impacts the performance of the optimistic likelihood
approach. In the limiting case where Bθ((cid:98)νθ) approaches a singleton {(cid:98)νθ}  the optimistic likelihood
problem recovers the nominal estimate(cid:98)νθ(x). Since this approximation is only reasonable when
(cid:98)νθ(x) > 0  which is often violated when (cid:98)νθ is estimated from few training samples  a strictly
positive size of Bθ((cid:98)νθ) is preferred. Ideally  the shape of Bθ((cid:98)νθ) is chosen so that problem (3)

2

is computationally tractable and at the same time offers a promising approximation quality. We

description based on moment conditions [14  27] and the Wasserstein distance [23  35  38  43  44].
The contributions of this paper may be summarized as follows.

explore in this paper three different constructions of Bθ((cid:98)νθ): the Kullback-Leibler divergence [3]  a
1. We show that when Bθ((cid:98)νθ) is constructed using the KL divergence  the optimistic likelihood (3)
2. We demonstrate that when Bθ((cid:98)νθ) is constructed using moment conditions  the optimistic likeli-
capture the tail behavior of(cid:98)νθ.
3. We show that when Bθ((cid:98)νθ) is constructed using the Wasserstein distance  the optimistic like-

reduces to a ﬁnite convex program  which in speciﬁc cases admits an analytical solution. However 
this approach does not satisfactorily approximate p(x|θ) for previously unseen samples x.
hood (3) can be computed in closed form. However  since strikingly different distributions can
share the same lower-order moments  this approach is often not ﬂexible enough to accurately

lihood (3) coincides with the optimal value of a linear program that can be solved using a
greedy heuristics. Interestingly  this variant of the optimistic likelihood results in a likelihood
approximation whose decay pattern resembles that of an exponential kernel approximation.

4. We use our optimistic likelihood approximation in the ELBO problem (1) for posterior inference.
We prove that the resulting posterior inference problems under the KL divergence and the Wasser-
stein distance enjoy strong theoretical guarantees  and we illustrate their promising empirical
performance in numerical experiments.

While this paper focuses on the non-parametric approximation of the likelihood p(x|θ)  we emphasize
that the optimistic likelihood approach can also be applied in the parametric setting. More speciﬁcally 
if p(·|θ) belongs to the family of Gaussian distributions  then the optimistic likelihood approximation
can be solved efﬁciently using geodesically convex optimization [36].
The remainder of the paper is structured as follows. We study the optimistic likelihood problem under
the KL ambiguity set  under moment conditions and under the Wasserstein distance in Sections 2–4 
respectively. Section 5 provides a performance guarantee for the posterior inference problem using
our optimistic likelihood. All proofs and additional material are relegated to the Appendix. In

Sections 2–4  the development of the theoretical results is generic  and hence the dependence of(cid:98)νθ
and Bθ((cid:98)νθ) on θ is omitted to avoid clutter.
Notation. We denote by M(X ) the set of all probability mass functions supported on X   and we
refer to the support of ν ∈ M(X ) as supp(ν). For any z ∈ X   δz is the delta-Dirac measure at z.
For any N ∈ N+  we use [N ] to denote the set {1  . . .   N}. 1x(·) is the indicator function at x  i.e. 
1x(ξ) = 1 if ξ = x  and 1x(ξ) = 0 otherwise.

2 Optimistic Likelihood using the Kullback-Leibler Divergence

We ﬁrst consider the optimistic likelihood problem where the ambiguity set is constructed using the
KL divergence. The KL divergence is the starting point of the ELBO maximization problem (1)  and
thus it is natural to explore its potential in our likelihood approximation.
Deﬁnition 2.1 (KL divergence). Let ν1  ν2 be two probability mass functions on X such that ν1 is
absolutely continuous with respect to ν2. The KL divergence between ν1 and ν2 is deﬁned as

KL(ν1 (cid:107) ν2) (cid:44)(cid:88)z∈X

f (ν1(z)/ν2(z)) ν2(z) 

where f (t) = t log(t) − t + 1.

radius ε ≥ 0  that is 

We now consider the KL divergence ball BKL((cid:98)ν  ε) centered at the empirical distribution(cid:98)ν with
BKL((cid:98)ν  ε) = {ν ∈ M(X ) : KL((cid:98)ν (cid:107) ν) ≤ ε} .
Moreover  we assume that the nominal distribution(cid:98)ν is supported on N distinct points(cid:98)x1  . . .  (cid:98)xN  
that is (cid:98)ν =(cid:80)j∈[N ](cid:98)νjδ(cid:98)xj with(cid:98)νj > 0∀j ∈ [N ] and(cid:80)j∈[N ](cid:98)νj = 1.
The set BKL((cid:98)ν  ε) is not weakly compact because X can be unbounded  and thus the existence

of a probability measure that optimizes the optimistic likelihood problem (3) over the feasible set

(4)

3

BKL((cid:98)ν  ε) is not immediate. The next proposition asserts that the optimal solution exists  and it

provides structural insights about the support of the optimal measure.
Proposition 2.2 (Existence of optimizers; KL ambiguity). For any ε ≥ 0 and x ∈ X   there exists a
measure ν(cid:63)

KL ∈ BKL((cid:98)ν  ε) such that

sup

ν∈BKL((cid:98)ν ε)

ν(x) = ν(cid:63)

KL(x)

(5)

KL) ⊆ supp((cid:98)ν) ∪ {x}.

Moreover  ν(cid:63)

KL is supported on at most N + 1 points satisfying supp(ν(cid:63)

Proposition 2.2 suggests that the optimistic likelihood problem (5)  inherently an inﬁnite dimensional
problem whenever X is inﬁnite  can be formulated as a ﬁnite dimensional problem. The next theorem
provides a ﬁnite convex programming reformulation of (5).
Theorem 2.3 (Optimistic likelihood; KL ambiguity). For any ε ≥ 0 and x ∈ X  
• if x ∈ supp((cid:98)ν)  then problem (5) can be reformulated as the ﬁnite convex optimization problem
++ (cid:80)j∈[N ](cid:98)νj log ((cid:98)νj/yj) ≤ ε  e(cid:62)y = 1(cid:111)  
ν∈BKL((cid:98)ν ε)

ν(x) = max(cid:110)(cid:80)j∈[N ] yj 1x((cid:98)xj) : y ∈ RN

where e is the vector of all ones;

sup

Theorem 2.3 indicates that the determining factor in the KL optimistic likelihood approximation is

then the optimal value of (5) does not depend on x  and the KL divergence approach assigns a
ﬂat likelihood. Interestingly  in Appendix B.2 we prove a similar result for the wider class of f-
divergences  which contains the KL divergence as a special case. While this ﬂat likelihood behavior

• if x (cid:54)∈ supp((cid:98)ν)  then problem (5) has the optimal value 1 − exp (−ε).
whether the observation x belongs to the support of the nominal measure(cid:98)ν or not. If x (cid:54)∈ supp((cid:98)ν) 
may be useful in speciﬁc cases  one would expect the relative distance of x to the atoms of(cid:98)ν to
x that does not belong to the support of the nominal measure(cid:98)ν.

inﬂuence the optimal value of the optimistic likelihood problem  similar to the neighborhood-based
intuition reﬂected in the kernel approximation approach. Unfortunately  the lack of an underlying
metric in its deﬁnition implies that the f-divergence family cannot capture this intuition  and thus f-
divergence ambiguity sets are not an attractive option to approximate the likelihood of an observation

Remark 2.4 (On the order of the measures). An alternative construction of the KL ambiguity set 
which has been widely used in the literature [3]  is

(cid:98)BKL((cid:98)ν  ε) = {ν ∈ M(X ) : KL(ν (cid:107)(cid:98)ν) ≤ ε}  

where the two measures ν and(cid:98)ν change roles. However  in this case the KL divergence imposes that
all ν ∈ (cid:98)BKL((cid:98)ν  ε) are absolutely continuous with respect to(cid:98)ν. In particular  if x (cid:54)∈ supp((cid:98)ν)  then
ν(x) = 0 for all ν ∈ (cid:98)BKL((cid:98)ν  ε)  and(cid:98)BKL((cid:98)ν  ε) is not able to approximate the likelihood of x in a

meaningful way.

3 Optimistic Likelihood using Moment Conditions

In this section we study the optimistic likelihood problem (3) when the ambiguity set B((cid:98)ν) is speciﬁed
by moment conditions. For tractability purposes  we focus on ambiguity sets BMV((cid:98)ν) that contain
all distributions which share the same mean(cid:98)µ and covariance matrix(cid:98)Σ ∈ Sm
distribution(cid:98)ν. Formally  this moment ambiguity set BMV((cid:98)ν) can be expressed as
BMV((cid:98)ν) =(cid:110)ν ∈ M(X ) : Eν[˜x] =(cid:98)µ  Eν[˜x˜x(cid:62)] =(cid:98)Σ +(cid:98)µ(cid:98)µ(cid:62)(cid:111) .
The optimistic likelihood (3) over the ambiguity set BMV((cid:98)ν) is a moment problem that is amenable to

a well-known reformulation as a polynomial time solvable semideﬁnite program [5]. Surprisingly 
in our case the optimal value of the optimistic likelihood problem is available in closed form. This
result was ﬁrst discovered in [26]  and a proof using optimization techniques can be found in [4].

++ with the nominal

4

1

(6)

sup

ν(x) =

ν∈BMV((cid:98)ν)

++. For any x ∈ X   the optimistic likelihood

Theorem 3.1 (Optimistic likelihood; mean-variance ambiguity [4  26]). Suppose that(cid:98)ν has the mean
vector (cid:98)µ ∈ Rm and the covariance matrix (cid:98)Σ ∈ Sm
problem (3) over the moment ambiguity set BMV((cid:98)ν) has the optimal value
1 + (x −(cid:98)µ)(cid:62)(cid:98)Σ−1(x −(cid:98)µ) ∈ (0  1].

The optimal value (6) of the optimistic likelihood problem depends on the location of the observed
sample point x  and hence the moment ambiguity set captures the behavior of the likelihood function
in a more realistic way than the KL divergence ambiguity set from Section 2. Moreover  the moment

4 Optimistic Likelihood using the Wasserstein Distance

ambiguity set BMV((cid:98)ν) does not depend on any hyper-parameters that need to be tuned. However 
since the construction of BMV((cid:98)ν) only relies on the ﬁrst two moments of the nominal distribution
(cid:98)ν  it fails to accurately capture the tail behavior of(cid:98)ν  see Appendix B.3. This motivates us to look
further for an ambiguity set that faithfully accounts for the tail behavior of(cid:98)ν.
We now study a third construction for the ambiguity set B((cid:98)ν)  which is based on the type-1 Wasserstein

distance (also commonly known as the Monge-Kantorovich distance)  see [48]. Contrary to the KL
divergence  the Wasserstein distance inherently depends on the ground metric of the sample space X .
Deﬁnition 4.1 (Wasserstein distance). The type-1 Wasserstein distance between two measures
ν1  ν2 ∈ M(X ) is deﬁned as

W(ν1  ν2) (cid:44)

inf

λ∈Λ(ν1 ν2)

Eλ [d(x1  x2)]  

where Λ(ν1  ν2) denotes the set of all distributions on X × X with the ﬁrst and second marginal
distributions being ν1 and ν2  respectively  and d is the ground metric of X .

The Wasserstein ball BW((cid:98)ν  ε) centered at the nominal distribution(cid:98)ν with radius ε ≥ 0 is

BW((cid:98)ν  ε) = {ν ∈ M(X ) : W(ν (cid:98)ν) ≤ ε} .

We ﬁrst establish a structural result for the optimistic likelihood problem over the Wasserstein
ambiguity set. This is the counterpart to Proposition 2.2 for the KL divergence.
Proposition 4.2 (Existence of optimizers; Wasserstein ambiguity). For any ε ≥ 0 and x ∈ X   there
exists a measure ν(cid:63)

(7)

ν(x) = ν(cid:63)

W(x).

(8)

W ∈ BW((cid:98)ν  ε) such that
ν∈BW((cid:98)ν ε)

sup

Furthermore  ν(cid:63)

W is supported on at most N + 1 points satisfying supp(ν(cid:63)

Leveraging Proposition 4.2  we can show that the optimistic likelihood estimate over the Wasserstein
ambiguity set coincides with the optimal value of a linear program whose number of decision variables

W) ⊆ supp((cid:98)ν) ∪ {x}.
equals the number of atoms N of the nominal measure(cid:98)ν.
Theorem 4.3 (Optimistic likelihood; Wasserstein ambiguity). For any ε ≥ 0 and x ∈ X   problem (8)
is equivalent to the linear program
+   (cid:88)j∈[N ]
ν∈BW((cid:98)ν ε)

d(x (cid:98)xj) Tj ≤ ε  Tj ≤(cid:98)νj ∀j ∈ [N ]

ν(x) = max(cid:88)j∈[N ]

The currently best complexity bound for solving a general linear program with N decision variables
is O(N 2.37) [11]  which may be prohibitive when N is large. Fortunately  the linear program (9) can
be solved to optimality using a greedy heuristics in quasilinear time.
Proposition 4.4 (Optimal solution via greedy heuristics). The linear program (9) can be solved to
optimality by a greedy heuristics in time O(N log N ).

Tj : T ∈ RN

sup

(9)

.

5

Example 4.5 (Qualitative comparison with kernel

methods). Let m = 1  d(x (cid:98)x) = (cid:107)x −(cid:98)x(cid:107)1 and
(cid:98)ν = 0.5δ−1+0.5δ1. Figure 1 compares the approx-
imation of p(x|θ) by the Wasserstein optimistic
likelihood with those of the ﬁnite sample kernel
approximations (2) with Kh(u) = K(cid:0)h−1u(cid:1) 
where the Kernel K is exponential with K(y) =
exp(−y)  uniform with K(y) = 1[|y| ≤ 1] or
Epanechnikov with K(y) = 3/4(1 − y2)1[|y| ≤
1]. While both the uniform and the Epachnech-
nikov kernel may produce an approximation value

of 0 when x is far away from the support of(cid:98)ν  the

Wasserstein approximation always returns a pos-
itive likelihood when ε > 0 (see Corollary A.2).
Figure 1: Comparison between the Wasserstein
Qualitatively  the Wasserstein approximation ex-
approximation (ε = 0.2) and the sample average
hibits a decay pattern similar to that of the ﬁnite
kernel approximations (h = 1) of p(x|θ).
sample average exponential kernel approximation.
On one hand  the similarity between the optimistic likelihood over the Wasserstein ambiguity set
and the exponential kernel approximation suggests that the kernel approximation can potentially
be interpreted in the light of our optimistic distributionally robust optimization framework. On the
other hand  and perhaps more importantly  this similarity suggests that there are possibilities to
design novel and computationally efﬁcient kernel-like approximations using advanced optimization
techniques. Even though the assumption that p(·|θ) is a probability mass function is fundamental for
our approximation  we believe that our approach can be utilized in the ABC setting even when p(·|θ)
is a probability density function. We leave these ideas for future research.
Appendix B.3 illustrates further how the Wasserstein ambiguity set offers a better tail approximation

of the nominal measure(cid:98)ν than the ambiguity set based on moment conditions. Interestingly  the

Wasserstein approximation can also be generalized to approximate the log-likelihood of a batch of
i.i.d. observations  see Appendix B.4

5 Application to the ELBO Problem

Motivated by the fact that the likelihood p(x|θ) is intractable to compute in many practical appli-
cations  we use our optimistic likelihood approximation (3) as a surrogate for p(x|θ) in the ELBO
problem (1). In this section  we will focus on the KL divergence and the Wasserstein ambiguity sets 
and we will impose the following assumptions.
Assumption 5.1 (Finite parameter space). We assume that Θ = {θ1  . . .   θC} for some C ≥ 2.
Assumption 5.2 (I.i.d. sampling and empirical distribution). For every i ∈ [C]  we have Ni i.i.d. sam-
ples(cid:98)xij  j ∈ [Ni]  from the conditional probability p(·|θi). Furthermore  each nominal distribution
(cid:98)νi is given by the empirical distribution(cid:98)νNi
Assumption 5.1 is necessary for our approach because we approximate p(x|θ) separately for every
θ ∈ Θ. Under this assumption  the prior distribution π can be expressed by the C-dimensional vector
π ∈ R+  and the ELBO program (1) becomes the ﬁnite-dimensional convex optimization problem
q∈Q (cid:88)i∈[C]
(10)

i (cid:80)j∈[Ni] δ(cid:98)xij on the samples(cid:98)xij.

qi(log qi − log πi) − (cid:88)i∈[C]

J true = min

qi log p(x|θi) 

i = N−1

where by a slight abuse of notation  Q is now a subset of the C-dimensional simplex. Assumption 5.2 
on the other hand  is a standard assumption in the nonparametric setting  and it allows us to study the
statistical properties of our optimistic likelihood approximation.
We approximate p(x|θi) for each θi by the optimal value of the optimistic likelihood problem (3):
(11)

νi(x)

p(x|θi) ≈

)

) is the KL divergence or Wasserstein ambiguity set centered at the empirical distri-
i
. Under Assumptions 5.1 and 5.2  a surrogate model of the ELBO problem (1) is then

Here  BNi

i ((cid:98)νNi
bution(cid:98)νNi

i

sup
νi∈BNi

i

((cid:98)νNi

i

6

-3-2-101230.10.30.50.7WassersteinExponential kernelUniform kernelEpanechnikov kernelq∈Q (cid:88)i∈[C]

(cid:98)JBN = min

qi(log qi − log πi) − (cid:88)i∈[C]

We now study the statistical properties of problem (12). We ﬁrst present an asymptotic guarantee for

qi log sup
νi(x)  
((cid:98)νNi
)(cid:9)C
where we use BN to denote the collection of ambiguity sets(cid:8)BNi
i=1 with N =(cid:80)i Ni.
i ((cid:98)νNi
the KL divergence. Towards this end  we deﬁne the disappointment as P∞(J true < (cid:98)JBN ).
Theorem 5.3 (Asymptotic guarantee; KL ambiguity). Suppose that Assumptions 5.1 and 5.2 hold.
  εi) for some εi > 0  and set n (cid:44) min{N1  . . .   NC}.
For each i ∈ [C]  let BNi
i ((cid:98)νNi
We then have

νi∈BNi

(12)

)

i

i

i

i

i

) = BKL((cid:98)νNi
log P∞(J true < (cid:98)JBN ) ≤ − min

i∈[C]

1
n

lim sup
n→∞

εi < 0.

obtained using the approximation (11) as

i

i

i ((cid:98)νNi

Theorem 5.3 shows that as the number of training samples Ni for each i ∈ [C] grows  the disappoint-
ment decays exponentially at a rate of at least mini εi.
We next study the statistical properties of problem (12) when each BNi
) is a Wasserstein ball.
To this end  we additionally impose the following assumption  which essentially requires that the tail
of each distribution p(·|θi)  i ∈ [C]  decays at an exponential rate.
Assumption 5.4 (Light-tailed conditional distribution). For each i ∈ [C]  there exists an exponent
ai > 1 such that Ai (cid:44) E[exp((cid:107)x(cid:107)ai)] < ∞  where the expectation is taken with respect to p(·|θi).
Theorem 5.5 (Finite sample guarantee; Wasserstein ambiguity). Suppose that Assumptions 5.1 
5.2 and 5.4 hold  and ﬁx any β ∈ (0  1). Assume that m (cid:54)= 2 and that BNi
i ((cid:98)νNi
) =
BW((cid:98)νNi
and ki1  ki2 are positive constants that depend on ai  Ai and m. We then have PN(cid:0)J true < (cid:98)JBN(cid:1) ≤ β.
5.4 hold. For each i ∈ [C]  let βNi ∈ (0  1) be a sequence such that (cid:80)∞
BNi
i ((cid:98)νNi
J true as N1  . . .   NC → ∞ almost surely.
Theorem 5.6 offers an asymptotic guarantee which asserts that as the numbers of training samples Ni
grow  the optimal value of (12) converges to that of the ELBO problem (10).

Theorem 5.5 provides a ﬁnite sample guarantee for the disappointment of problem (12) under a
speciﬁc choice of radii for the Wasserstein balls.
Theorem 5.6 (Asymptotic guarantee for Wasserstein). Suppose that Assumptions 5.1  5.2 and
Ni=1 βNi < ∞ and
  εi(βN   C  Ni))  where εi is deﬁned as in Theorem 5.5. Then (cid:98)JBN →

  εi(β  C  Ni)) for every i ∈ [C] with
(cid:16) log(ki1Cβ−1)
(cid:16) log(ki1Cβ−1)

εi(β  C  Ni) (cid:44)

if Ni ≥ log(ki1)Cβ−1
if Ni < log(ki1)Cβ−1

(cid:17)1/ max{m 2}
(cid:17)1/ai

) = BW((cid:98)νNi

ki2Ni

ki2Ni

ki2

ki2

 

 

i

i

i

6 Numerical Experiments

We ﬁrst showcase the performance guarantees from the previous section on a synthetic dataset
in Section 6.1. Afterwards  Section 6.2 benchmarks the performance of the different likelihood
approximations in a probabilistic classiﬁcation task on standard UCI datasets. The source code 
including our algorithm and all tests implemented in Python  are available from https://github.
com/sorooshafiee/Nonparam_Likelihood.

6.1 Synthetic Dataset: Beta-Binomial Inference
We consider the beta-binomial problem in which the prior π  the likelihood p(x|θ)  and the posterior
distribution q(θ|x) have the following forms:

π(θ) = Beta(θ|α  β) 

p(x|θ) = Bin(x|M  θ) 

q(θ|x) = Beta(θ|x + α  M − x + β)

7

(a) KL divergence

(b) Wasserstein distance

Figure 2: Average KL divergence between(cid:98)q that solves (12) and

the discretized posterior qdiscretize(·|x) as a function of ε and Ni.

Figure 3: Optimally tuned perfor-
mance of different approximation
schemes with varying Ni.

We emphasize that in this setting  the posterior distribution is known in closed form  and the main
goal is to study the properties of the optimistic ELBO problem (12) and the convergence of the
solution of problem (12) to the true posterior distribution. We impose a uniform prior distribution π
by setting α = β = 1. The ﬁnite parameter space Θ = {θ1  . . .   θC} contains C = 20 equidistant
discrete points in the range (0  1). For simplicity  we set N1 = . . . = NC in this experiment.
We conduct the following experiment for different training set sizes Ni ∈ {1  2  4  8  10} and different
ambiguity set radii ε. For each parameter setting  our experiment consists of 100 repetitions. In each
repetition  we randomly generate an observation x from a binomial distribution with M = 20 trials

p(x|θ) is approximated using the exponential kernel of the likelihood (2) with varying kernel width.

and success probability θtrue = 0.6. We then ﬁnd the distribution(cid:98)q that solves problem (12) using
both the KL and the Wasserstein approximation. In a similar way  we ﬁnd(cid:98)q by solving (10)  where
We evaluate the quality of the computed posteriors(cid:98)q from the different approximations based on the
KL divergences of(cid:98)q to the true discretized posterior qdiscretize(θi|x) ∝ Beta(θi|x + α  M − x + β).
Figures 2(a) and 2(b) depict the average quality of(cid:98)q with different radii. One can readily see that the
optimal size of the ambiguity set that minimizes KL((cid:98)q (cid:107) qdiscretize(·|x)) decreases as Ni increases for

both the KL and the Wasserstein approximation. Figure 3 depicts the performance of the optimally
tuned approximations with different sample sizes Ni. We notice that the optimistic likelihood over
the Wasserstein ambiguity set is comparable to the exponential kernel approximation.

6.2 Real World Dataset: Classiﬁcation

We now consider a probabilistic classiﬁcation setting with C = 2 classes. For each class i = 1  2  we

distribution π is also estimated from the training data as π(θi) = Ni/N  where N = N1 + N2 is the
total number of training samples. Upon observing a test sample x  the goal is to compute the posterior

have access to Ni observations denoted by {(cid:98)xij}j∈[Ni]. The nominal class-conditional probability
i (cid:80)j∈[Ni] δ(cid:98)xij for i = 1  2. The prior
distributions are the empirical measures  that is (cid:98)νi = N−1
distribution(cid:98)q by solving the optimization problem (12) using different approximation schemes. We
subsequently use the posterior(cid:98)q as a probabilistic classiﬁer. In this experiment  we exclude the KL
divergence approximation because x (cid:54)∈ supp((cid:98)νi) most of the time.
radii εi ∈ {a√m10b : a ∈ {1  . . .   9}  b ∈ {−3 −2 −1}}  i = 1  2  of the Wasserstein balls by a

In our experiments involving the Wasserstein ambiguity set  we randomly select 75% of the available
data as training set and the remaining 25% as test set. We then use the training samples to tune the

stratiﬁed 5-fold cross validation. For the moment based approximation  there is no hyper-parameter
to tune  and all data is used as training set. We compare the performance of the classiﬁers from
our optimistic likelihood approximation against the classiﬁer selected by the exponential kernel
approximation as a benchmark.
Table 1 presents the results on standard UCI benchmark datasets. All results are averages across 10
independent trials. The table shows that our optimistic likelihood approaches often outperform the
exponential kernel approximation in classiﬁcation tasks.

Acknowledgments We gratefully acknowledge ﬁnancial support from the Swiss National Science
Foundation under grant BSCGI0_157733 as well as the EPSRC grants EP/M028240/1  EP/M027856/1
and EP/N020030/1.

8

10−310−210−1100101ε0.00.51.01.52.02.53.0KL(bqkqdiscretize(·|x))Ni=1Ni=2Ni=4Ni=8Ni=1010−310−210−1100101ε0.00.51.01.52.02.53.0KL(bqkqdiscretize(·|x))Ni=1Ni=2Ni=4Ni=8Ni=10100101102Ni0.000.250.500.751.001.251.50KL(bqkqdiscretize(·|x))KLdivergenceWassersteinExponentialkernelMomentTable 1: Average area under the precision-recall curve for various UCI benchmark datasets. Bold
numbers correspond to the best performances.

Banknote Authentication
Blood Transfusion
Breast Cancer
Climate Model
Cylinder
Fourclass
German Credit
Haberman
Heart
Housing
ILPD
Ionosphere
Mammographic Mass
Pima
QSAR
Seismic Bumps
Sonar
Thoracic Surgery

Exponential Moment Wasserstein
100.00
68.23
97.99
93.40
86.23
100.00
75.11
71.10
75.86
82.04
69.88
98.79
87.86
80.48
90.21
65.89
93.85
56.32

99.05
64.91
97.58
93.80
76.74
99.95
67.58
70.82
78.77
75.62
71.54
91.02
83.46
79.61
84.44
74.81
85.66
54.84

99.99
71.28
99.26
81.94
75.00
82.77
75.50
70.20
86.87
81.89
72.95
97.05
86.53
82.37
90.85
75.68
83.49
64.73

References
[1] P. Baldi  S. Brunak  and F. Bach. Bioinformatics: The Machine Learning Approach. MIT Press 

2001.

[2] M. A. Beaumont  W. Zhang  and D. J. Balding. Approximate Bayesian computation in popula-

tion genetics. Genetics  162(4):2025–2035  2002.

[3] A. Ben-Tal  D. Den Hertog  A. De Waegenaere  B. Melenberg  and G. Rennen. Robust solutions
of optimization problems affected by uncertain probabilities. Management Science  59(2):341–
357  2013.

[4] D. Bertsimas and I. Popescu. Optimal inequalities in probability theory: A convex optimization

approach. SIAM Journal on Optimization  15(3):780–804  2005.

[5] D. Bertsimas and J. Sethuraman. Moment problems and semideﬁnite optimization. In Handbook
of Semideﬁnite Programming: Theory  Algorithms  and Applications  pages 469–509. Springer 
2000.

[6] J. Bi and T. Zhang. Support vector classiﬁcation with input data uncertainty. In Advances in

Neural Information Processing Systems  pages 161–168  2005.

[7] C. M. Bishop. Pattern Recognition and Machine Learning. Springer  2006.

[8] D. M. Blei  A. Kucukelbir  and J. D. McAuliffe. Variational inference: A review for statisticians.

Journal of the American Statistical Association  112(518):859–877  2017.

[9] E. Brochu  V. M. Cora  and N. De Freitas. A tutorial on Bayesian optimization of expensive
cost functions  with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599  2010.

[10] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5(1):1–122  2012.
time. arXiv preprint arXiv:1810.07896  2018.

[11] M. B. Cohen  Y. T. Lee  and Z. Song. Solving linear programs in the current matrix multiplication

[12] K. Csilléry  M. G. Blum  O. E. Gaggiotti  and O. François. Approximate Bayesian Computation

(ABC) in practice. Trends in Ecology & Evolution  25(7):410 – 418  2010.

9

[13] M. Cummins and P. Newman. FAB-MAP: Probabilistic localization and mapping in the space

of appearance. The International Journal of Robotics Research  27(6):647–665  2008.

[14] E. Delage and Y. Ye. Distributionally robust optimization under moment uncertainty with

application to data-driven problems. Operations Research  58(3):595–612  2010.

[15] S. Gao  G. Ver Steeg  and A. Galstyan. Variational information maximization for feature

selection. In Advances in Neural Information Processing Systems  pages 487–495  2016.

[16] N. S. Gorbach  S. Bauer  and J. M. Buhmann. Scalable variational inference for dynamical

systems. In Advances in Neural Information Processing Systems  pages 4806–4815  2017.

[17] G. A. Hanasusanto  V. Roitch  D. Kuhn  and W. Wiesemann. Ambiguous joint chance constraints

under mean and dispersion information. Operations Research  65(3):751–767  2017.

[18] M. A. Haynes  H. MacGillivray  and K. Mengersen. Robustness of ranking and selection
rules using generalised g-and-k distributions. Journal of Statistical Planning and Inference 
65(1):45–66  1997.

[19] M. D. Hoffman  D. M. Blei  C. Wang  and J. Paisley. Stochastic variational inference. The

Journal of Machine Learning Research  14(1):1303–1347  2013.

[20] R. Houthooft  X. Chen  Y. Duan  J. Schulman  F. De Turck  and P. Abbeel. Vime: Variational
information maximizing exploration. In Advances in Neural Information Processing Systems 
pages 1109–1117  2016.

[21] N. Jojic and B. J. Frey. Learning ﬂexible sprites in video layers. In Proceedings of IEEE

Conference on Computer Vision and Pattern Recognition  volume 1  pages 199–206  2001.

[22] D. P. Kingma  T. Salimans  and M. Welling. Variational dropout and the local reparameterization

trick. In Advances in Neural Information Processing Systems  pages 2575–2583  2015.

[23] D. Kuhn  P. Mohajerin Esfahani  V. A. Nguyen  and S. Shaﬁeezadeh-Abadeh. Wasserstein
distributionally robust optimization: Theory and applications in machine learning. INFORMS
TutORials in Operations Research  2019.

[24] P. Liang  S. Petrov  M. Jordan  and D. Klein. The inﬁnite PCFG using hierarchical Dirichlet
processes. In Empirical Methods in Natural Language Processing and Computational Natural
Language Learning  2007.

[25] A. C. Likas and N. P. Galatsanos. A variational approach for Bayesian blind image deconvolution.

IEEE Transactions on Signal Processing  52(8):2222–2233  2004.

[26] A. W. Marshall and I. Olkin. Multivariate Chebyshev inequalities. The Annals of Mathematical

Statistics  31(4):1001–1014  1960.

[27] K. L. Mengersen  P. Pudlo  and C. P. Robert. Bayesian computation via empirical likelihood.

Proceedings of the National Academy of Sciences  110(4):1321–1326  2013.

[28] T. P. Minka. Expectation propagation for approximate Bayesian inference. In Uncertainty in

Artiﬁcial Intelligence  pages 362–369. Morgan Kaufmann Publishers Inc.  2001.

[29] P. Mohajerin Esfahani and D. Kuhn. Data-driven distributionally robust optimization using
the Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming  171(1-2):115–166  2018.

[30] S. Mohamed and D. J. Rezende. Variational information maximisation for intrinsically motivated
reinforcement learning. In Advances in Neural Information Processing Systems  pages 2125–
2133  2015.

[31] R. Munos. From bandits to Monte-Carlo tree search: The optimistic principle applied to
optimization and planning. Foundations and Trends R(cid:13) in Machine Learning  7(1):1–129  2014.

[32] K. Murphy. Machine Learning: A Probabilistic Perspective. MIT Press  2012.

10

[33] S. Nakajima  R. Tomioka  M. Sugiyama  and S. D. Babacan. Perfect dimensionality recovery
by variational Bayesian PCA. In Advances in Neural Information Processing Systems  pages
971–979  2012.

[34] T. Naseem  H. Chen  R. Barzilay  and M. Johnson. Using universal linguistic knowledge to
guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing  pages 1234–1244  2010.

[35] V. A. Nguyen  D. Kuhn  and P. Mohajerin Esfahani. Distributionally robust inverse covariance

estimation: The Wasserstein shrinkage estimator. arXiv preprint arXiv:1805.07194  2018.

[36] V. A. Nguyen  S. Shaﬁeezadeh-Abadeh  M.-C. Yue  D. Kuhn  and W. Wiesemann. Calculat-
ing optimistic likelihoods using (geodesically) convex optimization. In Advances in Neural
Information Processing Systems  2019.

[37] M. Park  W. Jitkrittum  and D. Sejdinovic. K2-ABC: Approximate Bayesian computation
with kernel embeddings. In Proceedings of the 19th International Conference on Artiﬁcial
Intelligence and Statistics  pages 398–407  2016.

[38] G. Peyré and M. Cuturi. Computational optimal transport. Foundations and Trends R(cid:13) in

Machine Learning  11(5-6):355–607  2019.

[39] L. F. Price  C. C. Drovandi  A. Lee  and D. J. Nott. Bayesian synthetic likelihood. Journal of

Computational and Graphical Statistics  27(1):1–11  2018.

[40] A. Raj  M. Stephens  and J. K. Pritchard. fastSTRUCTURE: variational inference of population

structure in large SNP data sets. Genetics  197(2):573–589  2014.

[41] G. Sanguinetti  N. D. Lawrence  and M. Rattray. Probabilistic inference of transcription factor
concentrations and gene-speciﬁc regulatory activities. Bioinformatics  22(22):2775–2781  2006.

[42] M. J. Schervish. Theory of Statistics. Springer  1995.

[43] S. Shaﬁeezadeh-Abadeh  D. Kuhn  and P. Mohajerin Esfahani. Regularization via mass trans-

portation. Journal of Machine Learning Research  20(103):1–68  2019.

[44] A. Sinha  H. Namkoong  and J. Duchi. Certifying some distributional robustness with principled
adversarial training. In Proceedings of International Conference on Learning Representations 
2018.

[45] N. Srinivas  A. Krause  S. Kakade  and M. Seeger. Gaussian process optimization in the bandit
setting: no regret and experimental design. In Proceedings of International Conference on
Machine Learning  pages 1015–1022  2010.

[46] M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine

Learning Research  1(Jun):211–244  2001.

[47] T. Toni  D. Welch  N. Strelkowa  A. Ipsen  and M. P. Stumpf. Approximate Bayesian computa-
tion scheme for parameter inference and model selection in dynamical systems. Journal of The
Royal Society Interface  6(31):187–202  2009.

[48] C. Villani. Optimal Transport: Old and New. Springer  2008.

[49] W. Wiesemann  D. Kuhn  and M. Sim. Distributionally robust convex optimization. Operations

Research  62(6):1358–1376  2014.

[50] M. W. Woolrich  T. E. Behrens  C. F. Beckmann  M. Jenkinson  and S. M. Smith. Multilevel
linear modelling for FMRI group analysis using Bayesian inference. Neuroimage  21(4):1732–
1747  2004.

11

,Viet Anh Nguyen
Soroosh Shafieezadeh Abadeh
Man-Chung Yue
Daniel Kuhn
Wolfram Wiesemann