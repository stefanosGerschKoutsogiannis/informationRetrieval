2019,Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes,Wide neural networks with random weights and biases are Gaussian processes  as observed by Neal (1995) for shallow networks  and more recently by Lee et al.~(2018) and Matthews et al.~(2018) for deep fully-connected networks  as well as by Novak et al.~(2019) and Garriga-Alonso et al.~(2019) for deep convolutional networks.
We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron  RNNs (e.g. LSTMs  GRUs)  (nD or graph) convolution  pooling  skip connection  attention  batch normalization  and/or layer normalization.
More generally  we introduce a language for expressing neural network computations  and our result encompasses all such expressible neural networks.
This work serves as a tutorial on the \emph{tensor programs} technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there.
We provide open-source implementations of the Gaussian Process kernels of simple RNN  GRU  transformer  and batchnorm+ReLU network at  github.com/thegregyang/GP4A.
Please see our arxiv version for the complete and up-to-date version of this paper.,Tensor Programs I:

Wide Feedforward or Recurrent Neural Networks of

Any Architecture are Gaussian Processes

Greg Yang∗

Microsoft Research AI

gregyang@microsoft.com

Abstract

Wide neural networks with random weights and biases are Gaussian processes  as
originally observed by Neal (1995) and more recently by Lee et al. (2018) and
Matthews et al. (2018) for deep fully-connected networks  as well as by Novak
et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks.
We show that this Neural Network-Gaussian Process correspondence surprisingly
extends to all modern feedforward or recurrent neural networks composed of multi-
layer perceptron  RNNs (e.g. LSTMs  GRUs)  (nD or graph) convolution  pooling 
skip connection  attention  batch normalization  and/or layer normalization. More
generally  we introduce a language for expressing neural network computations 
and our result encompasses all such expressible neural networks. This work serves
as a tutorial on the tensor programs technique formulated in Yang (2019) and
elucidates the Gaussian Process results obtained there. We provide open-source im-
plementations of the Gaussian Process kernels of simple RNN  GRU  transformer 
and batchnorm+ReLU network at github.com/thegregyang/GP4A.

1

Introduction

Motivated to understand the Bayesian prior in neural networks (NNs)  Neal [41] theoretically showed
that inﬁnitely wide  shallow neural networks with random weights and biases are Gaussian processes
(GPs). He empirically explored this phenomenon over deep networks as well  but this was not proven
rigorously until recently [37  40  43  18]  with concrete progress made over the intervening years
[56  34  22  13]. This neural network-Gaussian process correspondence (NN-GP correspondence) has
not only allowed one to transform the implicit prior of NNs into explicit priors that can be understood
analytically [46  49  63  59  65]  but has also created new state-of-the-art kernels by converting from
deep neural networks [37  43]. Yet  so far the focus has dwelled entirely on multilayer perceptrons
(MLPs) or simple convolutional neural networks (CNNs). As new architectures are created with
blistering speed  a question starts to emerge and reverberate:

Do all inﬁnitely wide  randomly initialized neural networks correspond to Gaussian processes?

Even if the answer is yes  at the current rate where each new architecture warrants its own NN-GP
correspondence paper  theory will never catch up to practice. On a more basic level  what does this
question even mean for recurrent neural networks?

Our Contributions
In this paper  we formulate the notion of a Gaussian process with variable-
dimensional output (see Deﬁnition 2.1)  and show that feedforward and recurrent neural networks
of standard architectures converge to Gaussian processes in this sense as their widths or number

∗Please see https://arxiv.org/abs/1910.12478 for the full version of this paper.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

of channels go to inﬁnity  when their weights and biases are randomized. By standard architecture
we mean any architecture that is some composition of multilayer perceptrons (MLPs)  recurrent
neural networks (RNNs) (e.g.  Long-Short Term Memory (LSTM) [26] or Gated Recurrent Unit
(GRU) [10])  skip connections [24  27]  convolutions [16  17  47  35  36] or graph convolutions
[8  25  15  38  14  31]  pooling [35  36]  batch normalization (batchnorm) [28]  layer normalization
[1] and/or attention [2  55]. Even more broadly  we design a new language  NETSOR  for expressing
neural network computations  and show the GP convergence for all such expressible networks. By
demonstrating that NETSOR can implement any network of standard architectures  we obtain the
aforementioned results as a corollary. The results for RNNs  batchnorm  layernorm  attention  and
their combination with other layers are new. We open-source reference implementations2 for the GP
kernels of simple RNN  GRU  transformer  and feedforward batchnorm network; see Fig. 3 for an
illustration.

Relation of This Paper with [60] This paper serves several purposes. 1) Introduce the reader
to the tensor programs technique formulated in [60]  using the Neural Network-Gaussian Process
Correspondence as motivation. 2) Promote a redesigned set of notations for tensor programs that
hopefully makes the understanding and the application of this technique easier. 3) Prove a more
general version of the Gaussian Process results ﬁrst presented in [60]. 4) Provide example calculations
and reference implementations2 of the GP kernels for several architectures like the vanilla RNN 
GRU  batchnorm network  and transformers.
We assume the reader has not read [60] and seek to explain all results in elementary terms. However 
we will provide commentary in footnotes throughout the paper on differences from [60].
Regarding 1)  this paper will be the ﬁrst in a series to explain the tensor programs technique  each
covering a more powerful type of tensor programs  and each motivated by speciﬁc theorems that can
be proved or calculations made possible by these new tensor programs. In particular  here we will
only talk about tensor programs without matrix transposes. Regarding 3)  the results presented here
will supersede all results in [60] concerning Gaussian Processes  with one caveat that here we will
not cover architectures using both a weight W and its transpose W (cid:62) in its forward pass (but this
result will come for free in a later paper in this series).

2 Gaussian Process with Variable-Dimensional Output

We ﬁrst clarify the notion of a Gaussian process with variable dimension output.
Deﬁnition 2.1 (Gaussian Process). We say a random function f : X → Rm (with ﬁxed dimen-
sional output) is a Gaussian process if for any ﬁnite subset {x1  . . .   xk} ⊆ X  the random vector
(f (x1)  . . .   f (xk)) ∈ Rm×k is distributed as a km-dimensional Gaussian. If f has variable dimen-
sional output (e.g. f is an RNN)  such as when f (x) ∈ Rl(x) for some length function l : X → N 3 
then we say f is a Gaussian process if for any ﬁnite subset {x1  . . .   xk} ⊆ X  the random vector

(f (x1)  . . .   f (xk)) is distributed as a ((cid:80)

i l(xi))-dimensional Gaussian.

To illustrate a GP with variable-dimensional output  consider a simple RNN that runs on two input
sequences given by the GloVe embeddings [44] 4 of the words of the two sentences

sentence 1 (7 words):
sentence 2 (9 words):

“The brown fox jumps over the dog.”
“The quick brown fox jumps over the lazy dog.”

((cid:63))

√
A pseudocode is given in Program 2 in Section 4 (ignore the type annotations like G(n)  H(n)  A(n)
for now). The RNN emits a single scalar after reading each token (in Program 2  this is v(cid:62)sia/
n 
where sia is the RNN state after reading the ith token of the ath sentence  and v is the readout layer);
this number takes into account all of the word embeddings read so far. Thus  it will output a total of 7
scalars after reading sentence 1  and a total of 9 scalars after reading sentence 2. To say that this RNN
is a GP would imply that all 7 + 9 = 16 scalars are jointly Gaussian-distributed (corresponding to a
16 × 16 kernel)  over the randomness of the weights and biases imbued during initialization. This

3i.e. f :(cid:81)

2github.com/thegregyang/GP4A

Rl(x) is a dependent function

x∈X

4The embedding associates each word to a real vector of 100 dimensions such that semantically similar

words are mapped to closer vectors

2

is indeed the empirical phenomenon with a width-1000 RNN  and Fig. 2(E) visualizes the the joint
distribution of the last scalars output by the RNN at the end of each sentence. It clearly exhibits a
Gaussian nature  and perfectly ﬁts the theoretically predicted Gaussian distribution (dashed ovals) 
which we shall describe in Corollary 5.5.

3 Recap: GP Behavior of a Multilayer Perceptron (MLP)

Before explaining our main results  we ﬁrst review the argument from prior works [37  40  43]
for the GP convergence of a wide MLP with randomly initialized weights and biases  and we also
demonstrate why such an argument is inadequate for RNNs. Consider an MLP with widths {nl}l 
weight matrices {W l ∈ Rnl×nl−1}l  and biases {bl ∈ Rnl}l  where l ranges among the layer
numbers of the MLP. Its computation is given recursively as

h1(x) = W 1x + b1

(1)
w/nl−1) for each α ∈ [nl]  β ∈ [nl−1]  and
b ). Consider two inputs x  x(cid:48). Conditioned on hl−1(x) and hl−1(x(cid:48))  iid for each

αβ ∼ N (0  σ2

hl(x) = W lφ(hl−1(x)) + bl for l ≥ 2.

and

At initialization time  suppose W l
α ∼ N (0  σ2
bl
(cid:18)
α  (hl(x)α  hl(x(cid:48))α) is distributed as

(cid:18)

(cid:19)

(cid:19)

N

0 

σ2
w
nl−1

(cid:107)φ(hl−1(x))(cid:107)2

φ(hl−1(x)) · φ(hl−1(x(cid:48)))

φ(hl−1(x)) · φ(hl−1(x(cid:48)))

(cid:107)φ(hl−1(x(cid:48)))(cid:107)2

+ σ2
b .

If (hl−1(x)α  hl−1(x(cid:48))α) is distributed as N (0  Σl−1)  iid for each α  then by a law of large number
argument  the covariance matrix above converges to a deterministic limit
φ(z)φ(z(cid:48))
φ(z(cid:48))2

(cid:18) φ(z)2

φ(z)φ(z(cid:48))

Σl def= σ2
w

(z z(cid:48))∼N (0 Σl−1)

(cid:19)

+ σ2
b

E

as the width nl−1 → ∞  making (hl(x)α  hl(x(cid:48))α) Gaussian distributed as N (0  Σl). Iteratively
applying this argument for each l yields the result for a deep MLP. A similar logic works for
feedforward CNNs.
Unfortunately  this argument breaks down if the weights {W l}l are tied  i.e. all W l are equal to a
common matrix W   as in the case of an RNN. In this case  when we condition on the preactivations
hl−1(x)  hl−1(x(cid:48)) of the previous layer  W is no longer conditionally an iid random Gaussian matrix 
and all subsequent reasoning breaks down. We can repair this situation for RNNs in an ad hoc way
via the Gaussian conditioning technique (Lemma G.7)  but we prefer to set our sights wider  and deal
with all standard architectures  and more  in one fell swoop. To this end  we develop a framework
based on our new NETSOR language.

4 NETSOR : Language for Expressing Neural Network Computation

To show that networks of all standard architectures converge to GPs  we ﬁrst show that they can be
expressed by the following very general NETSOR language (see Programs 1 and 2 for examples)5 
and then show that any computation expressed this way exhibits GP behavior when its dimensions
are large.
Deﬁnition 4.1. 6 NETSOR programs are straight-line programs  where each variable follows one
of three types  G  H  or A (such variables are called G-vars  H-vars  and A-vars)  and after input
variables  new variables can be introduced by one of the rules MatMul  LinComb  Nonlin to be
discussed shortly. G and H are vector types and A is a matrix type; intuitively  G-vars should be
thought of as vectors that are asymptotically Gaussian  H-vars are images of G-vars by coordinatewise
nonlinearities  and A-vars are random matrices with iid Gaussian entries. Each type is annotated by
dimensionality information:

• If x is a (vector) variable of type G (or H) and has dimension n  we write x : G(n) (or

x : H(n)).

5NETSOR is a speciﬁc kind of tensor program; for other variants  see Appendix E.
6We keep the deﬁnition here informal in terms of programming language convention to be accessible to the

general machine learning audience. For those with PL background  see Appendix J.

3

NETSOR program 1 MLP Computation on Network Input x
Input: W 1x : G(n1)
Input: b1 : G(n1)
Input: W 2 : A(n2  n1)
Input: b2 : G(n2)
Input: v : G(n2)
1: h1 := W 1x + b1 : G(n1)
2: x1 := φ(h1) : H(n1)
3: ˜h2 := W 2x1 : G(n2)
4: h2 := ˜h2 + b2 : G(n2)
√
5: x2 := φ(h2) : H(n2)
Output: v(cid:62)x2/

n2

(cid:46) layer 1 embedding of input
(cid:46) layer 1 bias
(cid:46) layer 2 weights
(cid:46) layer 2 bias
(cid:46) readout layer weights
(cid:46) layer 1 preactivation; LinComb
(cid:46) layer 1 activation; Nonlin
(cid:46) MatMul
(cid:46) layer 2 preactivation; LinComb
(cid:46) layer 2 activation; Nonlin

• If A is a (matrix) variable of type A and has size n1 × n2  we write A : A(n1  n2).

G is a subtype of H  so that x : G(n) implies x : H(n). A NETSOR program consists of the following
three parts.
Input A set of input G- or A-vars.
Body New variables can be introduced and assigned via the following rules (with intuition in italics)

Ax : G(n1) 

MatMul if A : A(n1  n2) and x : H(n2)  we can form a G-var via matrix-vector product:
“random iid matrix times a vector is roughly a Gaussian vector.”7
LinComb Suppose x1  . . .   xk : G(n) are G-vars with the same dimension and a1  . . . ak ∈

R are constants. Then we can form their linear combination as a G-var:

n(cid:88)

aixi : G(n) 

“linear combination of Gaussian vectors is Gaussian.”

i=1

Nonlin If x1  . . .   xk : G(n) are G-vars with the same dimension n and φ : Rk → R  then

φ(x1  . . .   xk) : H(n) 

“image of Gaussian vector is not always Gaussian”

where φ acts coordinatewise.

√

Output For the purpose of this paper8  the output of a NETSOR program can be any tuple of scalars 
(v1(cid:62)y1/
nk)  where v1 : G(n1); . . . ; vk : G(nk) are some input G-vars
not used elsewhere (and possibly with duplicates vi = vj)  and y1 : H(n1); . . . ; yk : H(nk)
are some H-vars (possibly with duplicates yi = yj).

√
n1  . . .   vk(cid:62)yk/

Examples Program 1 gives an example of a NETSOR program representing an MLP computation.
Note that we account for the input x through its embedding W 1x  not x itself. This is because 1) our
theorems concern the case where all input G-vars are random; in the context of expressing neural
network computation  x is a deterministic input  while W 1x is a Gaussian vector when W 1 has iid
Gaussian entries; 2) x has a ﬁxed dimension  while we intend all dimensions (like n1  n2) in the
NETSOR program to tend to inﬁnity  as we’ll describe shortly. Similarly  Program 2 expresses in
NETSOR the computation of a simple RNN on two separate input sequences; computation on more
input sequences follows the same pattern. Note how weight-sharing is easily expressed in NETSOR
because we can re-use A-vars arbitrarily. Appendix A shows more examples of standard architectures
in NETSOR and NETSOR+ .
More generally  we can allow the nonlinearities in Nonlin to depend on parameters; this will be
necessary to express layernorm and attention (see Appendix A). We capture this idea in a new rule:
7Beware: in a later paper (and in [60]  tensor program general case)  we will introduce matrix transpose as
a valid operation  and in that case  Ax can be very far from a Gaussian  and this intuition is no longer correct.
Thus  this intuition is more subtle than it might seem at face value.

8In general  the output of a tensor program need not be deﬁned  as most of the time we are concerned with

how the H-vars produced over the course of the program interact with each other.

4

NETSOR program 2 Simple RNN Computation on Two Input Sequences

// Embeddings of two inputs sequences

Input: U x11  . . .   U xt1 : G(n)
Input: U x12  . . .   U xr2 : G(n)

˜ht1 := W st−1 1 : G(n)
ht1 := ˜ht1 + U xt1 + b : G(n)
st1 := φ(ht1) : H(n)
// Computation on sequence 2
h12 := U x12 + b : G(n)
s12 := φ(h12) : H(n)
˜h22 := W s12 : G(n)
h22 := ˜h22 + U x22 + b : G(n)
s22 := φ(h22) : H(n)
...
˜hr2 := W sr−1 2 : G(n)
hr2 := ˜hr2 + U xr2 + b : G(n)
√
√
sr2 := φ(hr2) : H(n)
Output: (v(cid:62)s11/
n  . . .   v(cid:62)st1/
√
√
n  . . .   v(cid:62)sr2/
n)

v(cid:62)s12/

n 

// Weight and bias
Input: W : A(n  n)
Input: b : G(n)

// Readout weights

Input: v : G(n)

// Computation on sequence 1
h11 := U x11 + b : G(n)
s11 := φ(h11) : H(n)
˜h21 := W s11 : G(n)
h21 := ˜h21 + U x21 + b : G(n)
s21 := φ(h21) : H(n)
...

Nonlin+ Suppose x1  . . .   xk : G(n) are G-vars with the same dimension n and θ1  . . .   θt ∈ R

possibly depend on G-vars already deﬁned. If φ(−;−) : Rk × Rt → R  then

φ(x1  . . .   xk; θ1  . . .   θt) : H(n) 

where φ acts coordinatewise.

Deﬁnition 4.2. NETSOR+ programs are NETSOR programs allowing Nonlin+ rules.

“empirical moments” of the form n−1(cid:80)n

NETSOR and NETSOR+ specify different kinds of tensor programs; in Appendix E we discuss other
kinds that are semantically equivalent. In a future paper  we shall study the effect of allowing matrix
transposes as an operation on A-vars.
Remark 4.3. In this paper  in Nonlin+  we will only instantiate θj with continuous functions of
i=1 ψ(y1  . . .   yr) for some set of G-vars {yi}i. A key
consequence of our scaling limit result is that these “empirical moments” converge almost surely
to a deterministic limit under very general conditions (Theorems 5.4 and C.4)  so that φ(−; Θ) is 
under suitable smoothness conditions (Deﬁnition C.1)  approximately a ﬁxed nonlinearity when n is
large. Thus  we should intuitively treat Nonlin+ as Nonlin but with the nonlinearity determined
automatically by the NETSOR program itself.

Nonlin+ expands the expressible computation quite broadly  but to keep the main text lean and
focused on the key ideas behind tensor programs  we relegate a more thorough discussion of Nonlin+
in the appendix (see Appendices C  D and I).

5 Computing the GP Kernel from a NETSOR Encoding of a Neural Network

For readers who wish to be convinced that NETSOR (or NETSOR+ ) can express standard architectures 
see Appendix A. In this section  we show that any architecture expressible in NETSOR and satisﬁes
some mild conditions will exhibit Gaussian Process behavior in the large width limit.
In this section  we make the following simplifying assumption on the dimensions of the program and
the randomness of the variables.
Assumption 5.1. Fix a NETSOR program. For simplicity  assume all dimensions in the program are
equal to n. Suppose for each A-var W : A(n  n)  we sample Wαβ ∼ N (0  σ2
W > 0 
and for each α ∈ [n]  we sample  i.i.d.  {xα : x is input G-var} ∼ N (µin  Σin) for some mean µin
and (possibly singular) covariance Σin over input G-vars.

W /n) for some σ2

5

The constraint on the dimensions can be removed easily; see Appendix F. This sampling induces
randomness in all variables created in the program  and we shall characterize this randomness shortly.
We ﬁrst review some notation that will be used immediately.
Notation In this paper  a kernel Σ on a set X is a symmetric function Σ : X × X → R such that

m(cid:88)

m(cid:88)

i=1

j=1

cicjΣ(xi  xj) ≥ 0

holds for any m ∈ N  x1  . . .   xm ∈ X  and c1  . . .   cm ∈ X. Given a kernel Σ on a set of G-vars 
we will both treat it as matrix and as a function  depending on the context. Function Notation As
a function  Σ(g  g(cid:48)) is the value of Σ on the pair of G-vars (g  g(cid:48)). If G = {g1  . . .   gk} is a set of
G-vars  then we also denote by Σ(g  G) the row vector {Σ(g  g1)  . . .   Σ(g  gk)}. Likewise Σ(G  g)
is the column vector with the same values. If G(cid:48) = {g1(cid:48)  . . .   gl(cid:48)} is another set of G-vars (possible
with overlap with G)  then Σ(G  G(cid:48)) is the matrix {Σ(gi  gj(cid:48)) : i ∈ [k]  j ∈ [l]}. Restriction
Notation We also use the “restriction” notation Σ|G to denote the square matrix Σ(G  G) in a more
concise way. Matrix Notation When an association of indices to G-vars is clear from context  we
will also write Σij for the corresponding value of Σ on the pair of ith and jth G-vars. Juxtaposition
implies matrix multiplication  e.g. ΣΩ means matrix product if Ω is a matrix of appropriate size.
Indices Notation We will both use superscripts and subscripts for indices. We will never multiply in
α β. H-vars as Both
subscript or superscript  so juxtaposition of indices like W ib
Symbols and Vectors An H-var will be considered both as a symbol (like in Σ(g  g(cid:48)) above) as well
as the corresponding length n vector (like in Theorem 5.4 below)  depending on the context.
Deﬁnition 5.2. In the setting of Assumption 5.1  we extend µin and Σin to µ and Σ that resp. take
a single and a pair of G-vars and both output to R. Intuitively  µ speciﬁes the mean coordinate of
a G-var  and Σ speciﬁes the coordinatewise covariance of a pair of G-vars; this is formalized in
Theorem 5.4 below. Index all the G-vars in the program as g1  . . .   gM (including input G-vars)  in
the order of appearance in the program. For any pair of G-vars g  g(cid:48) (among g1  . . .   gM )  we deﬁne
recursively

αβ is the same as W i b

if g =(cid:80)

if g is input

otherwise

i aiµ(yi)

µin(g)
(cid:80)

(cid:80)
(cid:80)
i aiΣ(yi  g(cid:48))
i aiΣ(g  yi)
EZ φ(Z) ¯φ(Z)
σ2
W
0

0
Σin(g  g(cid:48))

µ(g) =

Σ(g  g(cid:48)) =

where

i aiyi  introduced by LinComb

 

if g  g(cid:48) are inputs

if g =(cid:80)
if g(cid:48) =(cid:80)

i aiyi  introduced by LinComb
i aiyi  introduced by LinComb

if g = W h  g(cid:48) = W h(cid:48)  introduced by MatMul w/ same A-var W
otherwise

(2)

• yi are G-vars for all i
• (h : H(n)) was introduced by the Nonlin with h := φ(g1  . . .   gM )  h(cid:48) was introduced by
Nonlin with h(cid:48) := ¯φ(g1  . . .   gM ) (where WLOG we have padded the input slots of φ and
¯φ to account for all G-vars)

• Z ∼ N (µ  Σ) is a random Gaussian vector with 1 entry for each G-var in the program.

Note that since φ and ¯φ only depends on entries of Z corresponding to previous G-vars  the expectation
EZ φ(Z) ¯φ(Z) only depends on entries of µ and Σ already deﬁned  so there is no circular logic in
this recursive deﬁnition of µ and Σ. See Appendix B.1.1 for a simple  worked-out example of how to
recursively compute µ and Σ for Program 1.

For our main theorems  we isolate a very general class of nonlinearities that we are concerned with.
Deﬁnition 5.3. We say a function φ : Rk → R is controlled if |φ(x)| is bounded by a function of the
form eC(cid:107)x(cid:107)2−+c with C  c   > 0

6

Figure 1: An illustration of the NETSOR Master Theorem Theorem 5.4.

Controlled functions can explode faster than exponential but are still L1 and L2-integrable against
Gaussian measures. Additionally  there is no constraint on the smoothness of φ here. Thus this
deﬁnition captures almost all functions we would care about in practice.
The metric structure of the ﬁnal layer representations of inputs under a deep neural network often
reveals semantical information about the inputs. This structure is reﬂected in the inner products
between pairs of such representations  e.g. st1(cid:62)sr2/n for st1 and sr2 in Program 2. The following
Master Theorem allows one to compute such inner products  and much more  for a wide network at
initialization time (take ψ below to be ψ(z1  . . .   zM ) def= zM−1zM ).
Theorem 5.4 (NETSOR Master Theorem). 9 Fix any NETSOR program satisfying Assumption 5.1
and with all nonlinearities controlled. If g1  . . .   gM are all of the G-vars in the entire program 
including all input G-vars  then for any controlled ψ : RM → R  as n → ∞ 

1
n

ψ(g1

α  . . .   gM

α ) a.s.−−→ E

Z∼N (µ Σ)

ψ(Z) =

E

Z∼N (µ Σ)

ψ(Z g1

  . . .   Z gM

) 

n(cid:88)

α=1

where a.s.−−→ means almost sure convergence  Z = (Z g1
RM and Σ = {Σ(gi  gj)}M

i j=1 ∈ RM×M are given in Eq. (2). See Fig. 1 for an illustration.

) ∈ RM   and µ = {µ(gi)}M

  . . .   Z gM

i=1 ∈

α  . . .   gM

α ) ≈ N (µ  Σ) in the large n limit  and each
Intuitively  Theorem 5.4 says  for each α  (g1
α-slice appears to be “iid” from the point of view of the empirical average by any controlled function
ψ. The proof of Theorem 5.4 is given in Appendix H.
Combining Theorem 5.4 with Proposition G.4  we can straightforwardly calculate the output distri-
bution of a NETSOR program.
√
Corollary 5.5 (Computing the GP Kernel). Adopt the same assumptions and notations as in
Theorem 5.4. If the program outputs (v(cid:62)x1/

n  . . .   v(cid:62)xk/

n)  where

√

• v : G(n)  vα ∼ N (0  σ2

v)  is an input G-var not used elsewhere in the program and is

sampled independently from all other G-vars  and

• xi was introduced as xi := φi(g1  . . .   gM )

then the output vector converges in distribution to N (0  K) where

Kij = σ2
v

E

Z∼N (µ Σ)

φi(Z)φj(Z)  with µ  Σ deﬁned in Eq. (2).

(3)

Intuitively  this corollary follows from the fact that  for any ﬁnite n  the output vector is some
Gaussian N (0  ˜K) conditioned on x1  . . .   xk  and the covariance ˜K converges to a deterministic
covariance K  causing the output vector to converge in distribution to N (0  K) as well. The case
9Difference with [60  Thm 4.3]: We have gotten rid of the “rank convergence” assumption by showing that it

comes for free. See CoreSet and Lemma H.4 in Appendix H.

7

𝑔1𝑔2𝑔3𝑔𝑀𝑛→∞𝜓𝜓𝜓𝜓𝜓𝜓(Average1𝑛෍1𝑛((((())))))𝔼𝑍~𝑁𝜇 Σ𝜓()𝑍𝑔1𝑍𝑔2𝑍𝑔3𝑍𝑔𝑀𝑎.𝑠.Figure 2: Inﬁnite-width theory is highly predictive for simple RNN (Program 2) with 1000 neurons and
erf activation. We pass two sentences (“The brown fox jumps over the dog” and “The quick brown
fox jumps over the lazy dog”) by their word GloVe embeddings into randomly initialized simple
RNNs. (A) Cosine distances between each pair of word GloVe embeddings. (B) Correlation matrix of
the limiting Gaussian that Program 2 output vector converges to. Each row/column corresponds to an
embedding of of the sentence up to that word. (C) Covariance matrix of the same. See Appendix B.2
for algorithm to compute this covariance. (D) Entrywise standard deviation of empirical covariance
around (C)  as measured from 100 random simple RNNs. Note the deviations are at least an order of
√
magnitude smaller than the limiting values (C)  for 1000 neurons. (E) Visualizing the joint distribution
of the ﬁnal outputs of the RNN at the end of each sentence  i.e. (v(cid:62)st1/
n) in Program 2.
We sampled 100 000 simple RNNs and plotted the 2d histogram as a heatmap. Simultaneously  we
plot the limiting Gaussian level curves predicted by our theory  which ﬁt the simulations perfectly.

√
n  v(cid:62)sr2/

when we have multiple distinct vi (allowed by Deﬁnition 4.1) can be obtained easily as well (see
Proposition G.4).
Following Corollary 5.5 and its extensions below  the convergence of standard architectures to
Gaussian Processes becomes obvious: Express the marginal of the distribution on every ﬁnite set
of inputs as a NETSOR (or NETSOR+ ) program  and then apply Corollary 5.5. We summarize the
result below.
Corollary 5.6. If its nonlinearities are controlled (Deﬁnition 5.3)  then a (possibly recurrent) neural
network of standard architecture converges to a Gaussian process in ﬁnite-dimensional distribu-
tion 10 in the sense of Deﬁnition 2.1 as its widths go to inﬁnity and each of its weights W and
biases b are randomized as Wαβ ∼ N (0  σ2
b ) for a collection of sampling
hyperparameters {σW}W  {µb  σb}b. If its nonlinearities are more generally parametrized and are
parameter-controlled (Deﬁnition C.1)  such as in the case of attention models or where layernorm is
involved  then the same result holds as long as Assumption C.3 also holds.

W /n)  bα ∼ N (µb  σ2

An Empirical Demonstration Despite being about inﬁnite width  our theory is highly predictive
for ﬁnite-width networks  as shown in Fig. 2. As in Section 2  we randomly initialize a simple
RNN (Program 2) with 1000 neurons and erf activation (we choose erf instead of tanh because it
simpliﬁes kernel calculations; see Appendix B.2 for the derivation of the algorithm to compute the
kernel). We pass the two sentences in ((cid:63)) to the random RNN by their GloVe embeddings. After
processing each token  the RNN outputs a scalar  as before  and over the two input sequences  the
RNN outputs 7 + 9 = 16 scalars in total. Our result Corollary 5.5 implies that  as the width of the
RNN grows to inﬁnity  these 16 scalars are distributed jointly as a Gaussian. Fig. 2(E) illustrates
this is indeed the case for the marginal on 2 scalars  as discussed in Section 2. We also compare
our theoretically derived  inﬁnite-width covariance of the 16 scalars (Fig. 2(C)) with the empirical
ﬁnite-width covariance obtained from multiple random initializations. We ﬁnd that the empirical
covariance  as predicted  concentrates around the theoretical  and the entrywise standard deviation is
typically at least an order of magnitude lower than the values themselves (Fig. 2(D)) (with width 1000
RNNs). The random RNN is clearly doing nontrivial context embedding  as seen by comparing the
correlation matrix of the 16 scalars Fig. 2(B) (context-sensitive) with the matrix of cosine distances
(i.e. correlations) between the GloVe embeddings Fig. 2(A) (context-insensitive). A tell-tale sign is
the entry corresponding to (“lazy”  “dog”): even though as words  they are not semantically similar

10Stronger convergence results  such as convergence in distribution with respect to some topology on functions
on Rd  would be available if one can show additionally the tightness of the random neural networks under this
topology. However  here we are content with convergence of ﬁnite-dimensional marginals of the stochastic
processes.

8

ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydogsent1 sent2ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydogsent2 sent1(A)GloVe correlations0.00.20.40.60.81.0ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydogsent1 sent2(B)randRNN correlations (theory)0.00.20.40.60.81.0ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydogsent1 sent2(C)randRNN covariances (theory)0.000.050.100.150.200.250.30ThebrownfoxjumpsoverthedogThequickbrownfoxjumpsoverthelazydogsent1 sent2(D)randRNN covariance std (empirical)0.0060.0080.0100.0120.0140.0160.0180.02021012randRNN("...lazy dog")2.01.51.00.50.00.51.01.52.0randRNN("...the dog")(E)randRNN: Theory vs EmpiricstheoryFigure 3: Inﬁnite-width GP kernels (more precisely  their correlation matrices) for which we provide
reference implementations  and the deviation of ﬁnite-width simulations from the corresponding
inﬁnite-width limits. (A) – (C) The correlation matrices of the GP kernels for the simple RNN
(same as in Fig. 2; see Program 2 for the architecture and Appendix B.2 for derivation)  GRU
(Program 5; Appendix B.5)  and transformer (Program 10; Appendix D.3)  with input sequences
given by the GloVe embeddings of ((cid:63)). (D) The correlation matrix of the GP kernel for a feedforward 
fully-connected network with batchnorm+ReLU (batchnorm followed by ReLU) as nonlinearity (see
Appendix B.3 for derivation). The inputs are the ﬁrst 64 CIFAR10 images  split into two batches of
32 each. The red lines indicate the batch split. (E) For each architecture above  we independently
randomly initialize 100 networks for each width among [25  26  . . .   213]. We calculate the empirical
kernel of the network outputs and plot its (relative) Frobenius distance to the inﬁnite-width kernel.
This Frobenius distance drops like 1/
width as one would expect from a central limit intuition. See
our code2 for Python implementations of these kernels and the code for generating this ﬁgure.

√

(so that the entry in Fig. 2(A) is small)  the random RNN understands that the two sentences resp.
up to “lazy” and “dog” have been very similar (so that the entry in Fig. 2(B) is large). Given the
precision of our theoretical predictions  we expect analyses of the equations derived here will lead to
many nontrivial insights about recurrent (and other) neural network behavior in practice  which we
leave for future work.

Examples and Extensions: A Brief Guide to the Appendix Appendix B contains a plethora of
worked-out examples of the kernel computation for different architectures  starting from the known
case of MLP to the new results of RNN (as shown in Fig. 2)  GRU  batchnorm  and others. At this
point  we recommend the reader to follow along some of those examples to solidify the understanding
of Theorem 5.4.
A Master Theorem for NETSOR+ can be similarly proved. This is stated in Appendix C and can be
proved easily given the proof of Theorem 5.4; see Appendix I. Appendix D works out examples of
kernel computations for layernorm and transformer  which can only be expressed through NETSOR+ .
Fig. 3 illustrates the kernels of simple RNN  GRU  transformer  and a batchnorm+ReLU network 
and conﬁrms that the ﬁnite width simulations tend to the inﬁnite-width  theoretical kernels.
We also discuss different variants of NETSOR and NETSOR+ in Appendix E which trade off syntacti-
cal simplicity with ease of use  but are semantically equivalent to NETSOR or NETSOR+. Appendix F
discusses the case when the dimensions of a program need not be equal. With the appropriate setup 
a Master Theorem in this case can be proved similarly (Theorem F.4).

6 Related Works

NN-GP Correspondence Many works have observed the neural network-Gaussian process corre-
spondence (NN-GP correspondence) for subsets of standard architectures [56  34  22  13  37  40  43].
Others have exploited this NN-GP correspondence implicitly or explicitly to build new models
[11  33  12  57  58  7  54  32  4  6  18  43]. In particular  by directly converting NN to GP using this
correspondence  Lee et al. [37] constructed the state-of-the-art (SOTA) permutation-invariant GP on
MNIST  and Novak et al. [43] was until recently SOTA on CIFAR10 for any GP with untrainable
kernel. Additionally  the NN-GP correspondence has led to new understanding of neural network
training and generalization [42  53  61].
In this paper  we generalized the NN-GP correspondence to standard architectures and very general
nonlinearities (controlled functions; see Deﬁnition 5.3). In contrast  Matthews et al. [40] requires φ to
be linearly bounded in norm; Daniely et al. [13] requires φ be twice-differentiable with |φ| |φ(cid:48)| |φ(cid:48)(cid:48)|

9

%0-74314:2584;0790/4%06:.-74314:2584;0790 /480398039%0-74314:2584;0790/4%06:.-74314:2584;0790 /480398039#.4770 94389047%0-74314:2584;0790/4%06:.-74314:2584;0790 /480398039#&.4770 94389047%0-74314:2584;0790/4%06:.-74314:2584;0790 /48039803997 38147207.4770 9439047#8 250/70:1.309.4770 94389047log2width2log2(|KsimKth|F/|Kth|F)70 9;0/0; 9431742904724/0#&97 381472078250#log2widthall bounded  or that φ = ReLU; and a sufﬁcient condition given in Novak et al. [43] is that φ(cid:48) exists
and is bounded by exp(O(x2−))  though it is unclear how the more general set of 3 conditions given
there (in their section E.4) compares with ours.

Signal Propagation in Neural Networks A long line of work starting with Glorot and Bengio
[20] and He et al. [23] studies the effect of initialization in deep neural networks [46  50  63  62  21 
9  64  45]  for example  what is the best initialization scheme to avoid gradient vanishing? These
works apply the same calculations of covariances as we do for calculating Σ here  though in a much
more restricted set of architectures  and they are typically more concerned with the dynamics of such
covariances with depth.

Reservoir Computing
In reservoir computing [30  39  51]  sequence processing is typically done
by a randomly initialized recurrent neural network. A sequence of inputs is fed step by step into
the network  and a ﬁnal readout layer transforms the random RNN’s state into an output. The only
trainable parameters are the readout layer  but not the random RNN itself. Thus  in the inﬁnite-width
limit  reservoir computing corresponds exactly to GP inference with the RNN kernel computed in
Appendix B.2.

7 Conclusion

We formulated the notion of Gaussian process with variable-dimensional outputs and showed that
randomly initialized  wide feedforward and recurrent neural networks of standard architectures
converge in distribution to Gaussian processes in such a sense. This signiﬁcantly generalizes prior
work on the NN-GP correspondence. We did so by introducing NETSOR  a language for expressing
computation common in deep learning  including neural networks of standard architecture  along
with a theorem (Theorem 5.4) characterizing the behavior of a NETSOR program as its tensors are
randomized and their dimensions tend to inﬁnity; many examples and extensions are exhibited in
the appendix. Finally  we empirically veriﬁed our theory for simple RNN  GRU  transformer  and
batchnorm (Fig. 3) and open-sourced implementations of the corresponding inﬁnite-width limit
kernels at github.com/thegregyang/GP4A. In the next paper in this series  we will introduce a
more powerful version of tensor program that allows matrix transposes  and use this tool to compute
Neural Tangent Kernel [29] for any architecture.

10

,Greg Yang