2013,Computing the Stationary Distribution Locally,Computing the stationary distribution of a large finite or countably infinite state space Markov Chain (MC) has become central in many problems such as statistical inference and network analysis. Standard methods involve large matrix multiplications as in power iteration  or simulations of long random walks to sample states from the stationary distribution  as in Markov Chain Monte Carlo (MCMC). However these methods are computationally costly; either they involve operations at every state or they scale (in computation time) at least linearly in the size of the state space. In this paper  we provide a novel algorithm that answers whether a chosen state in a MC has stationary probability larger than some $\Delta \in (0 1)$. If so  it estimates the stationary probability. Our algorithm uses information from a local neighborhood of the state on the graph induced by the MC  which has constant size relative to the state space. We provide correctness and convergence guarantees that depend on the algorithm parameters and mixing properties of the MC. Simulation results show MCs for which this method gives tight estimates.,Computing the Stationary Distribution  Locally

Christina E. Lee

LIDS  Department of EECS

Massachusetts Institute of Technology

celee@mit.edu

Asuman Ozdaglar

LIDS  Department of EECS

Massachusetts Institute of Technology

asuman@mit.edu

Devavrat Shah

Department of EECS

Massachusetts Institute of Technology

devavrat@mit.edu

Abstract

Computing the stationary distribution of a large ﬁnite or countably inﬁnite state
space Markov Chain (MC) has become central in many problems such as statisti-
cal inference and network analysis. Standard methods involve large matrix multi-
plications as in power iteration  or simulations of long random walks  as in Markov
Chain Monte Carlo (MCMC). Power iteration is costly  as it involves computation
at every state. For MCMC  it is difﬁcult to determine whether the random walks
are long enough to guarantee convergence. In this paper  we provide a novel al-
gorithm that answers whether a chosen state in a MC has stationary probability
larger than some ∆ ∈ (0  1)  and outputs an estimate of the stationary probability.
Our algorithm is constant time  using information from a local neighborhood of
the state on the graph induced by the MC  which has constant size relative to the
state space. The multiplicative error of the estimate is upper bounded by a func-
tion of the mixing properties of the MC. Simulation results show MCs for which
this method gives tight estimates.

1

Introduction

Computing the stationary distribution of a Markov chain (MC) with a very large state space (ﬁnite 
or countably inﬁnite) has become central to statistical inference. The ability to tractably simulate
MCs along with the generic applicability has made Markov Chain Monte Carlo (MCMC) a method
of choice and arguably the top algorithm of the twentieth century [1]. However  MCMC and its vari-
ations suffer from limitations in large state spaces  motivating the development of super-computation
capabilities – be it nuclear physics [2  Chapter 8]  Google’s computation of PageRank [3]  or stochas-
tic simulation at-large [4]. MCMC methods involve sampling states from a long random walk over
the entire state space [5  6]. It is difﬁcult to determine when the algorithm has walked “long enough”
to produce reasonable approximations for the stationary distribution.
Power iteration is another method commonly used for computing leading eigenvectors and stationary
distributions of MCs. The method involves iterative multiplication of the transition matrix of the MC
[7]. However  there is no clearly deﬁned stopping condition in general settings  and computations
must be performed at every state of the MC.
In this paper  we provide a novel algorithm that addresses these limitations. Our algorithm answers
the following question: for a given node i of a countable state space MC  is the stationary probability
of i larger than a given threshold ∆ ∈ (0  1)  and can we approximate it? For chosen parameters
∆    and α  our algorithm guarantees that for nodes such that the estimate ˆπi < ∆/(1 + )  the true

1

(cid:16) ln( 1

(cid:17)

value πi is also less than ∆ with probability at least 1 − α. In addition  if ˆπi ≥ ∆/(1 + )  with
probability at least 1 − α  the estimate is within an  times Zmax(i) multiplicative factor away from
the true πi  where Zmax(i) is effectively a “local mixing time” for i derived from the fundamental
matrix of the transition probability matrix P .

α )
3∆

The running time of the algorithm is upper bounded by ˜O
  which is constant with respect
to the MC. Our algorithm uses only a“local” neighborhood of the state i  deﬁned with respect to the
Markov graph. Stopping conditions are easy to verify and have provable performance guarantees.
Its correctness relies on a basic property: the stationary probability of each node is inversely pro-
portional to the mean of its “return time.” Therefore  we sample return times to the node and use
the empirical average as an estimate. Since return times can be arbitrarily long  we truncate sample
return times at a chosen threshold. Hence  our algorithm is a truncated Monte Carlo method.
We utilize the exponential concentration of return times in Markov chains to establish theoretical
guarantees for the algorithm. For ﬁnite state Markov chains  we use results from Aldous and Fill
[8]. For countably inﬁnite state space Markov chains  we build upon a result by Hajek [9] on the
concentration of certain types of hitting times to derive concentration of return times to a given node.
We use these concentration results to upper bound the estimation error and the algorithm runtime
as a function of the truncation threshold and the mixing properties of the graph. For graphs that
mix quickly  the distribution over return times concentrates more sharply around its mean  resulting
in tighter performance guarantees. We illustrate the wide applicability of our local algorithm for
computing network centralities and stationary distributions of queuing models.

Related Work. MCMC was originally proposed in [5]  and a tractable way to design a random
walk for a target distribution was proposed by Hastings [6]. Given a distribution π(x)  the method
designs a Markov chain such that the stationary distribution of the Markov chain is equal to the target
distribution. Without using the full transition matrix of the Markov chain  Monte Carlo sampling
techniques estimate the distribution by sampling random walks via the transition probabilities at each
node. As the length of the random walk approaches inﬁnity  the distribution over possible states of
the random walk approaches stationary distribution. Articles by Diaconis and Saloff-Coste [10] and
Diaconis [11] provide a summary of major developments from a probability theoretic perspective.
The majority of work following the initial introduction of the algorithm involves analyzing the con-
vergence rates and mixing times of this random walk [8  12]. Techniques involve spectral analysis or
coupling arguments. Graph properties such as conductance help characterize the graph spectrum for
reversible Markov chains. For general non-reversible countably inﬁnite state space Markov chains 
little is known about the mixing time. Thus  it is difﬁcult to verify if the random walk has sufﬁ-
ciently converged to the stationary distribution  and before that point there is no guarantee whether
the estimate obtained from the random walk is larger or smaller than the true stationary probability.
Power iteration is an equally old and well-established method for computing leading eigenvectors of
matrices [7]. Given a matrix A and a seed vector x0  power iteration recursively computes xt+1 =
(cid:107)Axt(cid:107). The convergence rate of xt to the leading eigenvector is governed by the spectral gap. As
Axt
mentioned above  techniques for analyzing the spectrum are not well developed for general non-
reversible MCs  thus it is difﬁcult to know how many iterations are sufﬁcient. Although power
iteration can be implemented in a distributed manner  each iteration requires computation to be
performed by every state in the MC  which is expensive for large state space MCs. For countably
inﬁnite state space MCs  there is no clear analog to matrix multiplication.
In the specialized setting of PageRank  the goal is to compute the stationary distribution of a speciﬁc
Markov chain described by a transition matrix P = (1 − β)Q + β1 · rT   where Q is a stochastic
transition probability matrix  and β is a scalar in (0  1). This can be interpreted as random walk in
which every step either follows Q with probability 1 − β  or with probability β jumps to a node
according to the distribution speciﬁed by vector r. By exploiting this special structure  numerous
recent results have provided local algorithms for computing PageRank efﬁciently. This includes
work by Jeh and Widom [13]  Fogaras et al. [14]  Avrachenkov et al. [15]  Bahmani et al. [16] and
most recently  Borgs et al. [17]: it outputs a set of “important” nodes – with probability 1 − o(1) 
it includes all nodes with PageRank greater than a given threshold ∆  and does not include nodes

with PageRank less than ∆/c for a given c > 1. The algorithm runs in time O(cid:0) 1

∆polylog(n)(cid:1).

Unfortunately  these approaches are speciﬁc to PageRank and do not extend to general MCs.

2

2 Setup  problem statement & algorithm
Consider a discrete time  irreducible  positive-recurrent MC {Xt}t≥0 on a countable state space Σ
having transition probability matrix P . Let P (n)

ij be the (i  j)-coordinate of P n such that

P (n)

ij

(cid:44) P(Xn = j|X0 = i).

Throughout the paper  we will use the notation Ei[·] = E[·|X0 = i]  and Pi(·) = P(·|X0 = i). Let
Ti be the return time to a node i  and let Hi be the maximal hitting time to a node i such that

Ti = inf{t ≥ 1 | Xt = i} and Hi = max
j∈Σ

The stationary distribution is a function π : Σ → [0  1] such that (cid:80)
(cid:80)
i∈Σ πi = 1 and πi =
j∈Σ πjPji for all i ∈ Σ. An irreducible positive recurrent Markov chain has a unique station-

Ej[Ti].

(1)

(cid:104)(cid:80)Ti

(cid:105)

ary distribution satisfying [18  8]:
Ei

πi =

t=1 1{Xt=i}
Ei[Ti]

=

1

Ei[Ti]

for all

i ∈ Σ.

(2)

The Markov chain can be visualized as a random walk over a weighted directed graph G =
(Σ  E  P )  where Σ is the set of nodes  E = {(i  j) ∈ Σ × Σ : Pij > 0} is the set of edges 
and P describes the weights of the edges.1 The local neighborhood of size r around node i ∈ Σ is
deﬁned as {j ∈ Σ | dG(i  j) ≤ r}  where dG(i  j) is the length of the shortest directed path (in terms
of number of edges) from i to j in G. An algorithm is local if it only uses information within a local
neighborhood of size r around i  where r is constant with respect to the size of the state space.
The fundamental matrix Z of a ﬁnite state space Markov chain is

∞(cid:88)

(cid:16)

P (t) − 1πT(cid:17)

=(cid:0)I − P + 1πT(cid:1)−1

Z (cid:44)

  such that Zjk (cid:44)

jk − πk
P (t)

.

∞(cid:88)

(cid:16)

(cid:17)

t=0

t=0

Since P (t)
jk denotes the probability that a random walk beginning at node j is at node k after t steps 
Zjk represents how quickly the probability mass at node k from a random walk beginning at node j
converges to πk. We will use this to provide bounds for the performance of our algorithm.

2.1 Problem Statement
Consider a discrete time  irreducible  aperiodic  positive recurrent MC {Xt}t≥0 on a countable state
space Σ with transition probability matrix P : Σ × Σ → [0  1]. Given node i and threshold ∆  is
πi > ∆? If so  what is πi? We answer this with a local algorithm  which uses only edges within a
local neighborhood around i of constant size with respect to the state space.
We illustrate the limitations of using a local algorithm for answering this question. Consider the
Clique-Cycle Markov chain shown in Figure 1(a) with n nodes  composed of a size k clique con-
nected to a size (n − k + 1) cycle. For node j in the clique excluding i  with probability 1/2  the
random walk stays at node j  and with probability 1/2 the random walk chooses a random neighbor
uniformly. For node j in the cycle  with probability 1/2  the random walk stays at node j  and with
probability 1/2 the random walk travels counterclockwise to the subsequent node in the cycle. For
node i  with probability  the random walk enters the cycle  with probability 1/2 the random walk
chooses any neighbor in the clique; and with probability 1/2 −  the random walk stays at node i.
We can show that the expected return time to node i is (1 − 2)k + 2n.
Therefore  Ei[Ti] scales linearly in n and k. Suppose we observe only the local neighborhood of
constant size r around node i. All Clique-Cycle Markov chains with more than k + 2r nodes have
identical local neighborhoods. Therefore  for any ∆ ∈ (0  1)  there exists two Clique-Cycle Markov
chains which have the same  and k  but two different values for n  such that even though their local
neighborhoods are identical  πi > ∆ in the MC with a smaller n  while πi < ∆ in the MC with a
larger n. Therefore  by restricting ourselves to a local neighborhood around i of constant size  we
will not be able to correctly determine whether πi > ∆ for every node i in any arbitrary MC.

1Throughout the paper  Markov chain and random walk on a network are used interchangeably; similarly 

nodes and states are used interchangeably.

3

(a) Clique-Cycle Markov chain

(b) MM1 Queue

Figure 1: Examples of Markov Chains

2.2 Algorithm
Given a threshold ∆ ∈ (0  1) and a node i ∈ Σ  the algorithm obtains an estimate ˆπi of πi  and
uses ˆπi to determine whether to output 0 (πi ≤ ∆) or 1 (πi > ∆). The algorithm relies on the
characterization of πi given in Eq. (2): πi = 1/Ei[Ti]. It takes many independent samples of a
truncated random walk that begins at node i and stops either when the random walk returns to node
i  or when the length exceeds a predetermined maximum denoted by θ. Each sample is generated
by simulating the random walk using “crawl” operations over the MC. The expected length of each
random walk sample is Ei[min(Ti  θ)]  which is close to Ei[Ti] when θ is large.
As the number of samples and θ go to inﬁnity  the estimate will converge almost surely to πi  due
to the strong law of large numbers and positive recurrence of the MC. We use Chernoff’s bound to
choose a sufﬁciently large number of samples as a function of θ to guarantee that with probability
1 − α  the average length of the sample random walks will lie within (1 ± ) of Ei[min(Ti  θ)].
We also need to choose an suitable value for θ that balances between accuracy and computation cost.
The algorithm searches for an appropriate size for the local neighborhood by beginning small and
increasing the size geometrically. In our analysis  we will show that the total computation summed
over all iterations is only a constant factor more than the computation in the ﬁnal iteration.

Input: Anchor node i ∈ Σ and parameters ∆ = threshold for importance 
 = closeness of the estimate  and α = probability of failure.
Initialize: Set

(cid:24) 6(1 + ) ln(8/α)

(cid:25)

t = 1  θ(1) = 2  N (1) =

2

.

Step 1 (Gather Samples) For each k in {1  2  3  . . .   N (t)}  generate independent samples
sk ∼ min(Ti  θ(t)) by simulating paths of the MC beginning at node i  and setting sk to
be the length of the kth sample path. Let ˆp(t) = fraction of samples truncated at θ(t) 

N (t)(cid:88)

k=1

ˆT (t)
i =

1

N (t)

sk  ˆπ(t)

i =

1
ˆT (t)
i

  and ˜π(t)

i =

1 − ˆp(t)
ˆT (t)
i

.

Step 2 (Termination Conditions)

i < ∆

• If (a) ˆπ(t)
• Else if (b) ˆp(t) · ˆπ(t)
• Else continue.

(1+)  then stop and return 0  and estimates ˆπ(t)

and ˜π(t)
i < ∆  then stop and return 1  and estimates ˆπ(t)

i

i

i

.

and ˜π(t)

.

i

Step 3 (Update Rules) Set

θ(t+1) ← 2 · θ(t)  N (t+1) ←

(cid:38)

3(1 + )θ(t+1) ln(4θ(t+1)/α)

(cid:39)

  and t ← t + 1.

ˆT (t)
i

2

Return to Step 1.
Output: 0 or 1 indicating whether πi > ∆  and estimates ˆπ(t)
i

and ˜π(t)

i

.

4

i 1 2 3 4 5 This algorithm outputs two estimates for the anchor node i: ˆπi  which relies on the second expression
in Eq. (2)  and ˜πi  which relies on the ﬁrst expression in Eq. (2). We refer to the total number of
iterations used in the algorithm as the value of t at the time of termination  denoted by tmax. The

total number of random walk steps taken within the ﬁrst t iterations is(cid:80)t
The algorithm will always terminate within ln(cid:0) 1

(cid:1) iterations. Since θ(t) governs the radius of the

k=1 N (t) · ˆT (t)

local neighborhood that the algorithm utilizes  this implies that our algorithm is local  since the
maximum distance is strictly upper bounded by 1

∆  which is constant with respect to the MC.

∆

.

i

With high probability  the estimate ˆπ(t)
1+ due to the truncation. Thus when the
algorithm terminates at stopping condition (a)  πi < ∆ with high probability. When the algorithm
terminates at condition (b)  the fraction of samples truncated is small  which will imply that the
percentage error of estimate ˆπ(t)

is upper bounded as a function of  and properties of the MC.

is larger than πi

i

i

3 Theoretical guarantees

The following theorems give correctness and convergence guarantees for the algorithm. The proofs
have been omitted and can be found in the extended version of this paper [19].
Theorem 3.1. For an aperiodic  irreducible  positive recurrent  countable state space Markov chain 
and for any i ∈ Σ  with probability greater than 1 − α:
i ≥ πi

1+ . Therefore  if the algorithm terminates at

1. Correctness. For all iterations t  ˆπ(t)

condition (a) and outputs 0  then πi < ∆.

2. Convergence. The number of iterations tmax and the total number of steps (or neighbor

queries) used by the algorithm are bounded above by2 3

(cid:19)

(cid:18) ln( 1

α )
3∆

.

(cid:18) 1

(cid:19)

∆

tmax(cid:88)

k=1

tmax ≤ ln

  and

N (t) · ˆT (t)

i ≤ ˜O

Part 1 is proved by using Chernoff’s bound to show that N (t) is large enough to guarantee that with
probability greater than 1 − α  for all iterations t  ˆT (t)
concentrates around its mean. Part 2 asserts
that the algorithm terminates in ﬁnite time as a function of the parameters of the algorithm  inde-
pendent from the size of the MC state space. Therefore this implies that our algorithm is local. This
theorem holds for all aperiodic  irreducible  positive recurrent MCs. This is proved by observing
that ˆT (t)

∆  termination condition (b) must be satisﬁed.

i > ˆp(t)θ(t). Therefore when θ(t) > 1

i

3.1 Finite-state space Markov Chain

We can obtain characterizations for the approximation error and the running time as functions of
speciﬁc properties of the MC. The analysis depends on how sharply the distribution over return
times concentrates around the mean.
Theorem 3.2. For an irreducible Markov chain {Xt} with ﬁnite state space Σ and transition prob-
ability matrix P   for any i ∈ Σ  with probability greater than 1 − α  for all iterations t 

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2(1 − )Pi(Ti > θ(t))Zmax(i) +  ≤ 4(1 − )2−θ(t)/2HiZmax(i) +  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆπ(t)

i − πi
ˆπ(t)
i

where Hi is deﬁned in Eq (1)  and Zmax(i) = maxj |Zji|.
Therefore  with probability greater than 1 − α  if the algorithm terminates at condition (b)  then

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆπ(t)

i − πi
ˆπ(t)
i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤  (3Zmax(i) + 1) .

2We use the notation ˜O(f (a)g(b)) to mean ˜O(f (a)) ˜O(g(b)) = ˜O(f (a)polylogf (a)) ˜O(g(b)polylogg(b)).
3The bound for tmax is always true (stronger than with high probability).

5

Theorem 3.2 shows that the percentage error in the estimate ˆπ(t)
i decays exponentially in θ(t)  which
doubles in each iteration. The proof relies on the fact that the distribution of the return time Ti has
an exponentially decaying tail [8]  ensuring that the return time Ti concentrates around its mean
Ei[Ti]. When the algorithm terminates at stopping condition (b)  P(Ti > θ) ≤ ( 4
3 + ) with high
probability  thus the percentage error is bounded by O(Zmax(i)).
Similarly  we can analyze the error between the second estimate ˜π(t)
and πi  in the case when θ(t) is
2. This is required to guarantee that (1− ˆp(t)) lies within an 
large enough such that P(Ti > θ(t)) < 1
multiplicative interval around its mean with high probability. Observe than 2Zmax(i) is replaced by
max(2Zmax(i) − 1  1). Thus for some values of Zmax(i)  the error bound for ˜πi is smaller than the
equivalent bound for ˆπi. We will show simulations of computing PageRank  in which ˜πi estimates
πi more closely than ˆπi.
Theorem 3.3. For an irreducible Markov chain {Xt} with ﬁnite state space Σ and transition prob-
ability matrix P   for any i ∈ Σ  with probability greater than 1 − α  for all iterations t such that
P(Ti > θ(t)) < 1
2  
i − πi
˜π(t)
i

(cid:19)(cid:18) Pi(Ti > θ(t))

max(2Zmax(i) − 1  1) +

1 − Pi(Ti > θ(t))

(cid:18) 1 + 

1 − 

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˜π(t)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤

2
1 − 

.

(cid:19)

i

(cid:0)Ti > θ(t)(cid:1) will be small and ˆπ(t)

Theorem 3.4 also uses the property of an exponentially decaying tail as a function of Hi to show
that for large θ(t)  with high probability  Pi
i will be close to πi 
and thus the algorithm will terminate at one of the stopping conditions. The bound is a function
of how sharply the distribution over return times concentrates around the mean. Theorem 3.4(a)
states that for low probability nodes  the algorithm will terminate at stopping condition (a) for large
enough iterations. Theorem 3.4(b) states that for all nodes  the algorithm will terminate at stopping
condition (b) for large enough iterations.
Theorem 3.4. For an irreducible Markov chain {Xt} with ﬁnite state space Σ 
(a) For any node i ∈ Σ such that πi < (1 − )∆/(1 + )  with probability greater than 1 − α  the
total number of steps used by the algorithm is bounded above by

(cid:32)

(cid:32)

(cid:32)(cid:18)

(cid:19)(cid:18) 1

N (t) · ˆT (t)

i ≤ ˜O

ln( 1
α )
2

Hi ln

1

1 − 2−1/2Hi

− 1 + 
(1 − )∆

πi

(cid:19)−1(cid:33)(cid:33)(cid:33)

.

tmax(cid:88)

k=1

(b) For all nodes i ∈ Σ  with probability greater than 1 − α  the total number of steps used by the
algorithm is bounded above by

tmax(cid:88)

k=1

N (t) · ˆT (t)

i ≤ ˜O

(cid:18) ln( 1

α )
2

(cid:18) Hi

α

(cid:18)

(cid:18) 1

ln

πi

+

1

1 − 2−1/2Hi

∆

(cid:19)(cid:19)(cid:19)(cid:19)

.

3.2 Countable-state space Markov Chain

The proofs of Theorems 3.2 and 3.4 require the state space of the MC to be ﬁnite  so we can upper
bound the tail of the distribution of Ti using the maximal hitting time Hi. In fact  these results can
be extended to many countably inﬁnite state space Markov chains  as well. We prove that the tail of
the distribution of Ti decays exponentially for any node i in any countable state space Markov chain
that satisﬁes Assumption 3.5.
Assumption 3.5. The Markov chain {Xt} is aperiodic and irreducible. There exists a Lyapunov
function V : Σ → R+ and constants νmax  γ > 0  and b ≥ 0  that satisfy the following conditions:

1. The set B = {x ∈ Σ : V (x) ≤ b} is ﬁnite 

2. For all x  y ∈ Σ such that P(cid:0)Xt+1 = j|Xt = i(cid:1) > 0  |V (j) − V (i)| ≤ νmax 
3. For all x ∈ Σ such that V (x) > b  E(cid:2)V (Xt+1) − V (Xt)|Xt = x(cid:3) < −γ.

At ﬁrst glance  this assumption may seem very restrictive. But in fact  this is quite reasonable: by
the Foster-Lyapunov criteria [20]  a countable state space Markov chain is positive recurrent if and

6

only if there exists a Lyapunov function V : Σ → R+ that satisﬁes condition (1) and (3)  as well
as (2’): E[V (Xt+1)|Xt = x] < ∞ for all x ∈ Σ. Assumption 3.5 has (2)  which is a restriction of
the condition (2’). The existence of the Lyapunov function allows us to decompose the state space
into sets B and Bc such that for all nodes x ∈ Bc  there is an expected decrease in the Lyapunov
function in the next step or transition. Therefore  for all nodes in Bc  there is a negative drift towards
set B. In addition  in any single step  the random walk cannot escape “too far”.
Using the concentration bounds for the countable state space settings  we can prove the following
theorems that parallel the theorems stated for the ﬁnite state space setting. The formal statements
are restricted to nodes in B = {i ∈ Σ : V (i) ≤ b}. This is not actually restrictive  as for any i such
that V (i) > b  we can deﬁne a new Lyapunov function where V (cid:48)(i) = b  and V (cid:48)(j) = V (j) for all
j (cid:54)= i. Then B(cid:48) = B ∪ {i}  and V (cid:48) still satisﬁes assumption 3.5 for new values of νmax  γ  and b.
Theorem 3.6. For a Markov chain satisfying Assumption 3.5  for any i ∈ B  with probability
greater than 1 − α  for all iterations t 
i − πi
ˆπ(t)
i

2−θ(t)/Ri
1 − 2−1/Ri

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ˆπ(t)

πi +  

(cid:32)

where Ri is deﬁned such that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 4(1 − )
(cid:18)

(cid:33)
(cid:19)

Ri = O

H B

i e2ηνmax

(1 − ρ)(eηνmax − ρ)

 

is the maximal hitting time over the Markov chain with its state space restricted to the

and H B
i
subset B. The scalars η and ρ are functions of γ and νmax (deﬁned in [9]).
Theorem 3.7. For a Markov chain satisfying Assumption 3.5 
(a) For any node i ∈ B such that πi < (1 − )∆/(1 + )  with probability greater than 1 − α  the
total number of steps used by the algorithm is bounded above by

(cid:32)

(cid:32)

(cid:32)(cid:18)

(cid:19)(cid:18) 1

N (t) · ˆT (t)

i

˜O

ln( 1
α )
2

Ri ln

1

1 − 2−1/Ri

− 1 + 
(1 − )∆

πi

(cid:19)−1(cid:33)(cid:33)(cid:33)

.

tmax(cid:88)

k=1

(b) For all nodes i ∈ B  with probability greater than 1 − α  the total number of steps used by the
algorithm is bounded above by

tmax(cid:88)

k=1

N (t) · ˆT (t)

i ≤ ˜O

(cid:18) ln( 1

α )
2

(cid:18) Ri

α

(cid:18)

(cid:18) 1

ln

πi

+

1

1 − 2−1/Ri

∆

(cid:19)(cid:19)(cid:19)(cid:19)

.

In order to prove these theorems  we build upon results of [9]  and establish that return times have
exponentially decaying tails for countable state space MCs that satisfy Assumption 3.5.

4 Example applications: PageRank and MM1 Queue

PageRank is frequently used to compute the importance of web pages in the web graph. Given a
scalar parameter β and a stochastic transition matrix P   let {Xt} be the Markov chain with transition
n 1 · 1T + (1 − β)P . In every step  there is an β probability of jumping uniformly randomly
matrix β
to any other node in the network. PageRank is deﬁned as the stationary distribution of this Markov
chain. We apply our algorithm to compute PageRank on a random graph generated according to the
conﬁguration model with a power law degree distribution for β = 0.15.
In queuing theory  Markov chains are used to model the queue length at a server  which evolves over
time as requests arrive and are processed. We use the basic MM1 queue  equivalent to a random
walk on Z+. Assume we have a single server where the requests arrive according to a Poisson
process  and the processing time for a single request is distributed exponentially. The queue length
is modeled with the Markov chain shown in Figure 1(b)  where p is the probability that a new request
arrives before the current request is fully processed.

Figures 2(a) and 2(b) plot ˆπ(tmax)
for each node in the PageRank or MM1 queue MC 
respectively. For both examples  we choose algorithm parameters ∆ = 0.02   = 0.15  and α = 0.2.

and ˜π(tmax)

i

i

7

(a) PageRank Estimates

(b) MM1 Estimates

(c) PageRank —Total Steps vs. ∆

(d) MM1 Queue —Total Steps vs. ∆

Figure 2: Simulations showing results of our algorithm applied to PageRank and MM1 Queue setting

Observe that the algorithm indeed obtains close estimates for nodes such that πi > ∆  and for nodes
such that πi ≤ ∆  the algorithm successfully outputs 0 (i.e.  πi ≤ ∆). We observe that the method
for bias correction makes signiﬁcant improvements for estimating PageRank. We computed the
fundamental matrix for the PageRank MC and observed that that Zmax(i) ≈ 1 for all i.
Figures 2(c) and 2(d) show the computation time  or total number of random walk steps taken by
our algorithm  as a function of ∆. Each ﬁgure shows the results from three different nodes  chosen
to illustrate the behavior on nodes with varying πi. The ﬁgures are shown on a log-log scale. The
∆ )  which is
results conﬁrm that the computation time of the algorithm is upper bounded by O( 1
linear when plotted in log-log scale. When ∆ > πi  the computation time behaves as 1
∆. When
∆ )  and is close to constant with respect to ∆.
∆ < πi  the computation time grows slower than O( 1

5 Summary

We proposed a local algorithm for estimating the stationary probability of a node in a MC. The
algorithm is a truncated Monte Carlo method  sampling return paths to the node of interest. The
algorithm has many practical beneﬁts. First  it can be implemented easily in a distributed and paral-
lelized fashion  as it only involves sampling random walks using neighbor queries. Second  it only
uses a constant size neighborhood around the node of interest  upper bounded by 1
∆. Third  it only
performs computation at the node of interest. The computation only involves counting and taking
an average  thus it is simple and memory efﬁcient. We guarantee that the estimate ˆπ(t)
  is an upper
bound for πi with high probability. For MCs that mix well  the estimate will be tight with high
probability for nodes such that πi > ∆. The computation time of the algorithm is upper bounded by
parameters of the algorithm  and constant with respect to the size of the state space. Therefore  this
algorithm is suitable for MCs with large state spaces.
Acknowledgements: This work is supported in parts by ARO under MURI awards 58153-MA-MUR and
W911NF-11-1-0036  and grant 56549-NS  and by NSF under grant CIF 1217043 and a Graduate Fellowship.

i

8

02040608010000.010.020.030.040.050.060.070.08Anchor Node IDStationary Probability  True value (π)Estimate (πhat)Estimate (πtilde)0102030405000.10.20.30.40.50.60.7Anchor Node IDStationary Probability  True value (π)Estimate (πhat)Estimate (πtilde)10−410−310−210−1100104105106107∆Total Steps Taken  Node 1Node 2Node 310−410−310−210−1100103104105106107108∆Total Steps Taken  Node 1Node 2Node 3References
[1] B. Cipra. The best of the 20th century: Editors name top 10 algorithms. SIAM News  33(4):1 

May 2000.

[2] T.M. Semkow  S. Pomm  S. Jerome  and D.J. Strom  editors. Applied Modeling and Computa-

tions in Nuclear Science. American Chemical Society  Washington  DC  2006.

[3] L. Page  S. Brin  R. Motwani  and T. Winograd. The PageRank citation ranking: Bringing

order to the web. Technical Report 1999-66  November 1999.

[4] S. Assmussen and P. Glynn. Stochastic Simulation: Algorithms and Analysis (Stochastic Mod-

elling and Applied Probability). Springer  2010.

[5] N. Metropolis  A.W. Rosenbluth  M.N. Rosenbluth  A.H. Teller  and E. Teller. Equation of
state calculations by fast computing machines. The Journal of Chemical Physics  21:1087 
1953.

[6] W.K. Hastings. Monte Carlo sampling methods using Markov chains and their applications.

Biometrika  57(1):97–109  1970.

[7] G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins Studies in the Mathe-

matical Sciences. Johns Hopkins University Press  1996.

[8] D. Aldous and J. Fill. Reversible Markov chains and random walks on graphs: Chapter 2
(General Markov chains). book in preparation. URL: http://www.stat.berkeley.edu/∼aldous/
RWG/Chap2.pdf   pages 7  19–20  1999.

[9] B. Hajek. Hitting-time and occupation-time bounds implied by drift analysis with applications.

Advances in Applied probability  pages 502–525  1982.

[10] P. Diaconis and L. Saloff-Coste. What do we know about the Metropolis algorithm? Journal

of Computer and System Sciences  57(1):20–36  1998.

[11] P. Diaconis. The Markov chain Monte Carlo revolution. Bulletin of the American Mathematical

Society  46(2):179–205  2009.

[12] D.A. Levin  Y. Peres  and E.L. Wilmer. Markov chains and mixing times. Amer Mathematical

Society  2009.

[13] G. Jeh and J. Widom. Scaling personalized web search. In Proceedings of the 12th interna-

tional conference on World Wide Web  pages 271–279  New York  NY  USA  2003.

[14] D. Fogaras  B. Racz  K. Csalogany  and T. Sarlos. Towards scaling fully personalized PageR-
ank: Algorithms  lower bounds  and experiments. Internet Mathematics  2(3):333–358  2005.
[15] K. Avrachenkov  N. Litvak  D. Nemirovsky  and N. Osipova. Monte Carlo methods in PageR-
ank computation: When one iteration is sufﬁcient. SIAM Journal on Numerical Analysis 
45(2):890–904  2007.

[16] B. Bahmani  A. Chowdhury  and A. Goel. Fast incremental and personalized PageRank. Proc.

VLDB Endow.  4(3):173–184  December 2010.

[17] C. Borgs  M. Brautbar  J. Chayes  and S.-H. Teng. Sublinear time algorithm for PageRank

computations and related applications. CoRR  abs/1202.2771  2012.

[18] SP. Meyn and RL. Tweedie. Markov chains and stochastic stability. Springer-Verlag  1993.
[19] C.E. Lee  A. Ozdaglar  and D. Shah. Computing the stationary distribution locally. MIT LIDS

Report 2914  Nov 2013. URL: http://www.mit.edu/∼celee/LocalStationaryDistribution.pdf.

[20] F.G. Foster. On the stochastic matrices associated with certain queuing processes. The Annals

of Mathematical Statistics  24(3):355–360  1953.

9

,Christina Lee
Asuman Ozdaglar
Devavrat Shah
Cem Tekin
Mihaela van der Schaar