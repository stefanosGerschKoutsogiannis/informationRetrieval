2014,Modeling Deep Temporal Dependencies with Recurrent Grammar Cells"",We propose modeling time series by representing the transformations that take a frame at time t to a frame at time t+1. To this end we show how a bi-linear model of transformations  such as a gated autoencoder  can be turned into a recurrent network  by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multiple layers of gating units in a recurrent pyramid makes it possible to represent the ”syntax” of complicated time series  and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks.,Modeling Deep Temporal Dependencies with

Recurrent “Grammar Cells”

Vincent Michalski

Goethe University Frankfurt  Germany

vmichals@rz.uni-frankfurt.de

Roland Memisevic

University of Montreal  Canada

roland.memisevic@umontreal.ca

Kishore Konda

Goethe University Frankfurt  Germany

konda.kishorereddy@gmail.com

Abstract

We propose modeling time series by representing the transformations that take a
frame at time t to a frame at time t+1. To this end we show how a bi-linear model
of transformations  such as a gated autoencoder  can be turned into a recurrent net-
work  by training it to predict future frames from the current one and the inferred
transformation using backprop-through-time. We also show how stacking multi-
ple layers of gating units in a recurrent pyramid makes it possible to represent the
”syntax” of complicated time series  and that it can outperform standard recurrent
neural networks in terms of prediction accuracy on a variety of tasks.

1

Introduction

The predominant paradigm of modeling time series is based on state-space models  in which a
hidden state evolves according to some predeﬁned dynamical law  and an observation model maps
the state to the dataspace. In this work  we explore an alternative approach to modeling time series 
where learning amounts to ﬁnding an explicit representation of the transformation that takes an
observation at time t to the observation at time t + 1.
Modeling a sequence in terms of transformations makes it very easy to exploit redundancies that
would be hard to capture otherwise. For example  very little information is needed to specify an
element of the signal class sine-wave  if it is represented in terms of a linear mapping that takes a
snippet of signal to the next snippet: given an initial “seed”-frame  any two sine-waves differ only
by the amount of phase shift that the linear transformation has to repeatedly apply at each time step.
In order to model a signal as a sequence of transformations  it is necessary to make transformations
“ﬁrst-class objects”  that can be passed around and picked up by higher layers in the network. To
this end  we use bilinear models (e.g. [1  2  3]) which use multiplicative interactions to extract trans-
formations from pairs of observations. We show that deep learning which is proven to be effective in
learning structural hierarchies can also learn to capture hierarchies of relations or transformations.
A deep model can be built by stacking multiple layers of the transformation model  so that higher
layers capture higher-oder transformations (that is  transformations between transformations). To be
able to model multiple steps of a time-series  we propose a training scheme called predictive train-
ing: after computing a deep representation of the dynamics from the ﬁrst frames of a time series  the
model predicts future frames by repeatedly applying the transformations passed down by higher lay-
ers  assuming constancy of the transformation in the top-most layer. Derivatives are computed using
back-prop through time (BPTT) [4]. We shall refer to this model as a predictive gating pyramid
(PGP) in the following.

1

Since hidden units at each layer encode transformations  not content of their inputs  they capture only
structural dependencies and we refer to them as “grammar cells.”1 The model can also be viewed as a
higher-order partial difference equation whose parameters are estimated from data. Generating from
the model amounts to providing boundary conditions in the form of seed-frames  whose number
corresponds to the number of layers (the order of the difference equation). We demonstrate that
a two-layer model is already surprisingly effective at capturing whole classes of complicated time
series  including frequency-modulated sine-waves (also known as “chirps”) which we found hard to
represent using standard recurrent networks.

1.1 Related Work

LSTM units [5] also use multiplicative interactions  in conjunction with self-connections of weight
1  to model long-term dependencies and to avoid vanishing gradients problems [6]. Instead of con-
stant self-connections  the lower-layer units in our model can represent long-term structure by using
dynamically changing orthogonal transformations as we shall show. Other related work includes
[7]  where multiplicative interactions are used to let inputs modulate connections between succes-
sive hidden states of a recurrent neural network (RNN)  with application to modeling text. Our model
also bears some similarity to [3] who model MOCAP data using a three-way Restricted Boltzmann
Machine  where a second layer of hidden units can be used to model more “abstract” features of
the time series. In contrast to that work  our higher-order units which are bi-linear too  are used to
explicitly model higher-order transformations. More importantly  we use predictive training using
backprop through time for our model  which is crucial for achieving good performance as we show
in our experiments. Other approaches to sequence modeling include [8]  who compress sequences
using a two-layer RNN  where the second layer predicts residuals  which the ﬁrst layer fails to pre-
dict well. In our model  compression amounts to exploiting redundancies in the relations between
successive sequence elements. In contrast to [9] who introduce a recursive bi-linear autoencoder
for modeling language  our model is recurrent and trained to predict  not reconstruct. The model
by [10] is similar to our model in that it learns the dynamics of sequences  but assumes a simple
autoregressive  rather than deep  compositional dependence  on the past. An early version of our
work is described in [11].
Our work is also loosely related to sequence based invariance [12] and slow feature analysis [13] 
because hidden units are designed to extract structure that is invariant in time. In contrast to that
work  our multi-layer models assume higher-order invariances  that is  invariance of velocity in the
case of one hidden layer  of acceleration in the case of two  of jerk (the rate of change of acceleration)
in the case of three  etc.

2 Background on Relational Feature Learning

In order to learn transformation features  m  that represent the relationship between two observa-
tions x(1) and x(2) it is necessary to learn a basis that can represent the correlation structure across
the observations. In a time series  knowledge of one frame  x(1)  typically highly constrains the
distribution over possible next frames  x(2). This suggests modeling x(2) using a feature learning
model whose parameters are a function of x(1) [14]  giving rise to bi-linear models of transforma-
tions  such as the Gated Boltzmann Machine [15  3]  Gated Autoencoder [16]  and similar models
(see [14] for an overview). Formally  bi-linear models learn to represent a linear transformation  L 
between two observations x(1) and x(2)  where

(1)
Bi-linear models encode the transformation in a layer of mapping units that get tuned to rotation
angles in the invariant subspaces of the transformation class [14]. We shall focus on the gated
autoencoder (GAE) in the following but our description could be easily adapted to other bi-linear
models. Formally  the response of a layer of mapping units in the GAE takes the form2

x(2) = Lx(1).

m = σ(cid:0)W(Ux(1) · Vx(2))(cid:1).

(2)

1We dedicate this paper to the venerable grandmother cell  a grandmother of the grammar cell.
2We are only using “factored” [15] bi-linear models in this work  but the framework presented in this work

could be applied to unfactored models  too.

2

where U  V and W are parameter matrices  · denotes elementwise multiplication  and σ is an
elementwise non-linearity  such as the logistic sigmoid. Given mapping unit activations  m  and
the ﬁrst observation  x(1)  the second observation can be reconstructed using

(3)
which amounts to applying the transformation encoded in m to x(1) [16]. As the model is symmet-
ric  the reconstruction of the ﬁrst observation  given the second  is similarly given by

˜x(2) = VT(cid:0)Ux(1) · WTm(cid:1)
˜x(1) = UT(cid:0)Vx(2) · WTm(cid:1).

(4)

For training one can minimize the symmetric reconstruction error

L = ||x(1) − ˜x(1)||2 + ||x(2) − ˜x(2)||2.

(5)
Training turns the rows of U and V into ﬁlter pairs which reside in the invariant subspaces of the
transformation class on which the model was trained. After learning  each pair is tuned to a particular
rotation angle in the subspace  and the components of m are consequently tuned to subspace rotation
angles. Due to the pooling layer  W  they are furthermore independent of the absolute angles in the
subspaces [14].

3 Higher-Order Relational Features

Alternatively  one can think of the bilinear model as performing a ﬁrst-order Taylor approximation
of the input sequence  where the hidden representation models the partial ﬁrst-order derivatives of
the inputs with respect to time. If we assume constancy of the ﬁrst-order derivatives (or higher-order
derivates  as we shall discuss)  the complete sequence can be encoded using information about a
single frame and the derivatives. This is a very different way of addressing long-range correlations
than assuming memory units that explicitly keep state [5]. Instead  here we assume that there is
structure in the temporal evolution of the input stream and we focus on capturing this structure.
As an intuitive example  consider a sinusoidal signal with unknown frequency and phase. The
complete signal can be speciﬁed exactly and completely after having seen a few seed frames  making
it possible in principle to generate the rest of the signal ad inﬁnitum.

3.1 Learning of Higher-Order Relational Features

2

The ﬁrst-order partial derivative of a multidimensional discrete-time dynamical system describes
the correspondences between observations at subsequent time steps. The fact that relational feature
learning applied to subsequent frames may be viewed as a way to learn these derivatives  suggests
modeling higher-order derivatives with another layer of relational features.
To this end  we suggest cascading relational features in a “pyramid” as depicted in Figure 1 on the
left.3 Given a sequence of inputs x(t−2)  x(t−1)  x(t)  ﬁrst-order relational features m(t−1:t)
de-
scribe the transformations between two subsequent inputs x(t−1) and x(t). Second-order relational
describe correspondences between two ﬁrst-order relational features m(t−2:t−1)
features m(t−2:t)
and m(t−1:t)
To learn the higher-order features  we can ﬁrst train a bottom-layer GAE module to represent corre-
spondences between frame pairs using ﬁlter matrices U1  V1 and W1 (the subscript index refers to
the layer). From the ﬁrst-layer module we can infer mappings m(t−2:t−1)
for overlap-
ping input pairs (x(t−2)  x(t−1)) and (x(t−1)  x(t))  and use these as inputs to a second-layer GAE
module. A second GAE can then learn to represent relations between mappings of the ﬁrst-layer
using parameters U2  V2 and W2.
Inference of second-order relational features amounts to computing ﬁrst- and second-order mappings
according to

  modeling the “second-order derivatives” of the signal with respect to time.

and m(t−1:t)

1

1

1

1

1

= σ(cid:0)W1
= σ(cid:0)W1
= σ(cid:0)W2

(cid:0)(U1x(t−2)) · (V1x(t−1))(cid:1)(cid:1)
(cid:0)(U1x(t−1)) · (V1x(t))(cid:1)(cid:1)
(cid:0)(U2m(t−2:t−1)

) · (V2m(t−1:t)

1

1

1

m(t−2:t−1)
m(t−1:t)
m(t−2:t)

1

2

)(cid:1)(cid:1).

(6)
(7)
(8)

3Images taken from the NORB data set described in [17]

3

Figure 1: Left: A two-layer model encodes a sequence by assuming constant “acceleration”. Right:
Prediction using ﬁrst-order relational features.

Like a mixture of experts  a bi-linear model represents a highly non-linear mapping from x(1) to
x(2) as a mixture of linear (and thereby possibly orthogonal) transformations. Similar to the LSTM 
this facilitates error back-propagation  because orthogonal transformations do not suffer from van-
ishing/exploding gradient problems. This may be viewed as a way of generalizing LSTM [5] which
uses the identity matrix as the orthogonal transformation. “Grammar units” in contrast try to model
long-term structure that is dynamic and compositional rather than remembering a ﬁxed value.
Cascading GAE modules in this way can also be motivated from the view of orthogonal trans-
formations as subspace rotations: summing over ﬁlter-response products can yield transformation
detectors which are sensitive to relative angles (phases in the case of translations) and invariant to
the absolute angles [14]. The relative rotation angle (or phase delta) between two projections is itself
an angle  and the relation between two such angles represents an “angular acceleration” that can be
picked up by another layer.
In contrast to a single-layer  two-frame model  the reconstruction error is no longer directly applica-
ble (although a naive way to train the model would be to minimize reconstruction error for each pair
of adjacent nodes in each layer). However  a natural way of training the model on sequential data is
to replace the reconstruction task with the objective of predicting future frames as we discuss next.

4 Predictive Training

4.1 Single-Step Prediction

In the GAE model  given two frames x(1) and x(2) one can compute a prediction of the third frame
by ﬁrst inferring mappings m(1 2) from x(1) and x(2) (see Equation 2) and using these to compute a
prediction ˆx(3) by applying the inferred transformation m(1 2) to frame x(2)

ˆx(3) = VT(cid:0)Ux(2) · WTm(1 2)(cid:1).

(9)
See Figure 1 (right side) for an outline of the prediction scheme. The prediction of x(3) is a good
prediction under the assumption that frame-to-frame transformations from x(1) to x(2) and from
x(2) to x(3) are approximately the same  in other words if transformations themselves are assumed
to be approximately constant in time. We shall show later how to relax the assumption of constancy
of the transformation by adding layers to the model.
The training criterion for this predictive gating pyramid (PGP) is the prediction error

(10)
Besides allowing us to apply bilinear models to sequences  this training objective  in contrast to the
reconstruction objective  can guide the mapping representation to be invariant to the content of each
frame  because encoding the content of x(2) will not help predicting x(3) well.

L = ||ˆx(3) − x(3)||2
2.

4.2 Multi-Step Prediction and Non-Constant Transformations

We can iterate the inference-prediction process in order to look ahead more than one frame in time.
To compute a prediction ˆx(4) with the PGP  for example  we can infer the mappings and prediction:
(11)

m(2:3) = σ(cid:0)W(Ux(2) · Vˆx(3))(cid:1) 

ˆx(4) = VT(cid:0)U ˆx(3) · WTm(2:3)(cid:1).

4

Figure 2: Left: Prediction with a 2-layer PGP. Right: Multi-step prediction with a 3-layer PGP.

Then mappings can be inferred again from ˆx(3) and ˆx(4) to compute a prediction of ˆx(5)  and so on.
When the assumption of constancy of the transformations is violated  one can use an additional
layer to model how transformations themselves change over time as described in Section 3. The
assumption behind the two-layer PGP is that the second-order relational structure in the sequence
is constant. Under this assumption  we compute a prediction ˆx(t+1) in two steps after inferring
m(t−2:t)
according to Equation 8: First  ﬁrst-order relational features describing the correspondence
between x(t) and x(t+1) are inferred top-down as

2

T(cid:0)U2m(t−1:t)
T(cid:0)U1x(t) · WT

1

· WT

2 m(t−2:t)

2

(cid:1) 

(cid:1).

(12)

(13)

ˆm(t:t+1)

1

= V2

from which we can compute ˆx(t+1) as

ˆx(t+1) = V1

1 ˆm(t:t+1)

1

See Figure 2 (left side) for an illustration of the two-layer prediction scheme. To predict multiple
steps ahead we repeat the inference-prediction process on x(t−1)  x(t) and ˆx(t+1)  i.e. by appending
the prediction to the sequence and increasing t by one.
As outlined in Figure 2 (right side)  the concept can be generalized to more than two layers by
recursion to yield higher-order relational features. Weights can be shared across layers  but we used
untied weights in our experiments.
To summarize  the prediction process consists in iteratively computing predictions of the next lower
levels activations beginning from the top. To infer the top-level activations themselves  one needs a
number of seed frames corresponding to the depth of the model. The models can be trained using
BPTT to compute gradients of the k-step prediction error (the sum of prediction errors) with respect
to the parameters. We observed that starting with few prediction steps and iteratively increasing the
number of prediction steps as training progresses considerably stabilizes the learning.

5 Experiments

We tested and compared the models on sequences and videos with varying degrees of complex-
ity  from synthetic constant to synthetic accelerated transformations to more complex real-world
transformations. A description of the synthetic shift and rotation data sets is provided in the supple-
mentary material.

5.1 Preprocessing and Initialization

For all data sets  except for chirps and bouncing balls  PCA whitening was used for dimensionality
reduction  retaining around 95% of the variance. The chirps-data was normalized by subtracting
the mean and dividing by the standard deviation of the training set. For the multi-layer models we
used greedy layerwise pretraining before predictive training. We found pretraining to be crucial for
the predictive training to work well. Each layer was pretrained using a simple GAE  the ﬁrst layer
on input frames  the next layer on the inferred mappings. Stochastic gradient descent (SGD) with
learning rate 0.001 and momentum 0.9 was used for all pretraining.

5

Table 1: Classiﬁcation accuracies (%) on accelerated transformation data using mappings from
different layers in the PGP (accuracies after pretraining shown in parentheses).

Data set

m(1:2)

1

m(2:3)

1

(m(1:2)

1

  m(2:3)

1

)

m(1:3)

2

ACCROT
ACCSHIFT

18.1 (19.4)
20.9 (20.6)

29.3 (30.9)
34.4 (33.3)

74.0 (64.9)
42.7 (38.4)

74.4 (53.7)
80.6 (63.4)

5.2 Comparison of Predictive and Reconstructive Training

To evaluate whether predictive training (PGP) yields better representations of transformations than
training with a reconstruction objective (GAE)  we ﬁrst performed a classiﬁcation experiment on
videos showing artiﬁcially transformed natural images. 13 × 13 patches were cropped from the
Berkeley Segmentation data set (BSDS300) [18]. Two data sets with videos featuring constant
velocity shifts (CONSTSHIFT) and rotations (CONSTROT) were generated. The shift vectors (for
CONSTSHIFT) and rotation angles (for CONSTROT) were each grouped into 8 bins to generate
labels for classiﬁcation.
The numbers of ﬁlter pairs and mapping units were chosen using a grid search. The setting with the
best performance on the validation set was 256 ﬁlters and 256 mapping units for both training objec-
tives on both data sets. The models were each trained for 1 000 epochs using SGD with learning rate
0.001 and momentum 0.9. Mappings of the ﬁrst two inputs were used as input to a logistic regres-
sion classiﬁer. The experiment was performed three times on both data sets. The mean accuracy (%)
on CONSTSHIFT after predictive training was 79.4 compared to 76.4 after reconstructive training.
For CONSTROT mean accuracies were 98.2 after predictive and 97.6 after reconstructive training.
This conﬁrms that predictive training yields a more explicit representation of transformations  that
is less dependent on image content  as discussed in Section 4.1.

5.3 Detecting Acceleration

To test the hypothesis that the PGP learns to model second-order correspondences in sequences  im-
age sequences with accelerated shifts (ACCSHIFT) and rotations (ACCROT) of natural image patches
were generated. The acceleration vectors (for ACCSHIFT) and angular rotations (for ACCROT) were
each grouped into 8 bins to generate output labels for classiﬁcation.
Numbers of ﬁlter pairs and mapping units were set to 512 and 256  respectively  after performing a
grid search. After pretraining  the PGP was trained using SGD with learning rate 0.0001 and mo-
mentum 0.9  for 400 epochs on single-step prediction and then 500 epochs on two-step prediction.
After training  ﬁrst- and second-layer mappings were inferred from the ﬁrst three frames of the test
sequences. The classiﬁcation accuracies using logistic regression with second-layer mappings of the
PGP (m(1:3)
)  and with their concatena-
tion (m(1:2)
) as classiﬁer inputs are compared in Table 1 for both data sets (before and after
predictive ﬁnetuning). The second-layer mappings achieved a signiﬁcantly higher accuracy for both
data sets after predictive training. For ACCROT  the concatenation of ﬁrst-layer mappings performs
almost as well as the second-layer mappings  which may be because rotations have fewer degrees of
freedom than shifts making them easier to model. Note that the accuracy for the ﬁrst layer mappings
also improved with predictive ﬁnetuning.
These results show that the PGP can learn a better representation of the second-order relational
structure in the data than the single-layer model. They further show that predictive training improves
performances of both models and is crucial for the PGP.

)   with individual ﬁrst-layer mappings (m(1:2)
  m(2:3)

1

and m(2:3)

1

2

1

1

5.4 Sequence Prediction

In these experiments we test the capability of the models to predict previously unseen sequences
multiple steps into the future. This allows us to assess to what degree modeling higher order “deriva-
tives” makes it possible to capture the temporal evolution of a signal without resorting to an explicit

6

Figure 3: Multi-step predictions by the PGP trained on accelerated rotations (left) and shifts (right).
From top to bottom: ground truth  predictions before and after predictive ﬁnetuning.

Figure 4: Left: Chirp signal and the predictions of the CRBM  RNN and PGP after seeing the ﬁrst
ﬁve 10-frame vectors. Right: The MSE of the three models for each step.

representation of a hidden state. Unless mentioned otherwise  the presented sequences were seeded
with frames from test data (not seen during training).
Accelerated Transformations
Figure 3 shows predictions with the PGP on the data sets introduced in Section 5.3 after different
stages of training. As can be seen in the ﬁgures  the prediction accuracy increases signiﬁcantly with
multi-step training.
Chirps
Performances of the PGP were compared with that of a standard RNN (trained with BPTT) and a
CRBM (trained with contrastive divergence) [19] on a dataset containing chirps (sinusoidal waves
that increase or decrease in frequency over time). Training and test set each contain 20  000 se-
quences. The 160 frames of each sequence are grouped into 16 non-overlapping 10-frame windows 
yielding 10-dimensional input vectors. Given the ﬁrst 5 windows  the remaining 11 windows have to
be predicted. Second-order mappings of the PGP are averaged for the seed windows and then held
ﬁxed for prediction. Predictions for one test sequence are shown in Figure 4 (left). Mean-squared
errors (MSE) on the test set are 1.159 for the RNN  1.624 for the CRBM and 0.323 for the PGP. A
plot of per-step MSEs is shown in Figure 4 (right).
NorbVideos
The NORBvideos data set introduced in [20] contains videos of objects from the NORB dataset
[17]. The 5 frame videos each show incrementally changed viewpoints of one object. One- and two-
hidden layer PGP models were trained on this data using the author’s original split. Both models
used 2000 features and 1000 mapping units (per layer). The performance of the one-hidden layer
model stopped improving at 2000 features  while the two-hidden layer model was able to make use
of the additional parameters. Two-step MSEs on test data were 448.4 and 582.1  respectively.
Figure 6 shows predictions made by both models. The second-order PGP generates predictions that
reﬂect the 3-D structure in the data. In contrast to the ﬁrst-order PGP  it is able to extrapolate the
observed transformations.
Bouncing Balls
The PGP is also able to capture the highly non-linear dynamics in the bouncing balls data set4. The
sequence shown in Figure 5 contains 56 frames  where the ﬁrst 5 are from the training sequences
and are used as seed for sequence generation (similar to the chirps experiment the average top-layer
mapping vector for the seed frames is ﬁxed). Note that the sequences used for training were only

4 The training and test sequences were generated using the script released with [21].

7

0100200300time−2.502.5groundtruthCRBMRNNPGP0510predict-aheadinterval01.01.8meansquarederrorFigure 5: PGP generated sequence of bouncing balls (left-to-right  top-to-bottom).

Figure 6: Two-step PGP test predictions on NORBvideos.

20 frames long. The model’s predictions look qualitatively better than most published generated
sequences.5 Further results and data can be found on the project website at http://www.ccc.
cs.uni-frankfurt.de/people/vincent-michalski/grammar-cells

6 Discussion

A major long-standing problem in sequence modeling is dealing with long-range correlations. It
has been proposed that deep learning may help address this problem by ﬁnding representations that
capture better the abstract  semantic content of the inputs [22]. In this work we propose learning
representations with the explicit goal of enabling the prediction of the temporal evolution of the
input stream multiple time steps ahead. Thus we seek a hidden representation that captures those
aspects of the input data which allow us to make predictions about the future.
As we discussed  learning the long-term evolution of a sequence can be simpliﬁed by modeling
it as a sequence of temporally varying orthogonal (and thus  in particular  linear) transformations.
Since gating networks are like mixtures-of-experts  the PGP does model its input using a sequence
of linear transformations in the lowest layer  it is thus “horizontally linear”. At the same time 
it is “vertically compressive”  because its sigmoidal units are encouraged to compute non-linear 
sparse representations  like the hidden units in any standard feed-forward neural network. From an
optimization perspective this is a very sensible way to model time-series  since gradients have to
be back-propagated through many more layers horizontally (in time) than vertically (through the
non-linear network).
It is interesting to note that predictive training can also be viewed as an analogy making task [15].
It amounts to relating the transformation from frame t − 1 to t with the transformation between a
later pair of observations  e.g. those at time t and t + 1. The difference is that in a genuine analogy
making task  the target observation may be unrelated to the source observation pair  whereas here
target and source are related. It would be interesting to apply the model to word representations 
or language in general  as this is a domain where both  sequentially structured data and analogical
relationships play central roles.

Acknowledgments

This work was supported by the German Federal Ministry of Education and Research (BMBF)
in project 01GQ0841 (BFNT Frankfurt)  by an NSERC Discovery grant and by a Google faculty
research award.

5compare with http://www.cs.utoronto.ca/˜ilya/pubs/2007/multilayered/index.

html and http://www.cs.utoronto.ca/˜ilya/pubs/2008/rtrbm_vid.tar.gz.

8

References
[1] R. Memisevic and G. E. Hinton. Unsupervised learning of image transformations. In Proceed-

ings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition  2007.

[2] B. A. Olshausen  C. Cadieu  J. Culpepper  and D. K. Warland. Bilinear models of natural

images. 2007.

[3] G. W. Taylor  G. E. Hinton  and S. T. Roweis. Two distributed-state models for generating
high-dimensional time series. The Journal of Machine Learning Research  12:1025–1068 
2011.

[4] P. J. Werbos. Generalization of backpropagation with application to a recurrent gas market

model. Neural Networks  1(4):339–356  1988.

[5] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):1735–

1780  1997.

[6] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. diploma thesis  institut f¨ur

informatik  lehrstuhl prof. brauer  technische universit¨at m¨unchen. 1991.

[7] I. Sutskever  J. Martens  and G. E. Hinton. Generating text with recurrent neural networks. In

Proceedings of the 2011 International Conference on Machine Learning  2011.

[8] J. Schmidhuber. Learning complex  extended sequences using the principle of history com-

pression. Neural Computation  4(2):234–242  1992.

[9] R. Socher  A. Perelygin  J. Wu  J. Chuang  C. D. Manning  A. Y. Ng  and C. Potts. Recursive
deep models for semantic compositionality over a sentiment treebank. In Proceedings of the
2013 Conference on Empirical Methods in Natural Language Processing.

[10] J. Luttinen  T. Raiko  and A. Ilin. Linear state-space model with time-varying dynamics. In

Machine Learning and Knowledge Discovery in Databases  pages 338–353. Springer  2014.

[11] V. Michalski. Neural networks for motion understanding: Diploma thesis. Master’s thesis 

Goethe-Universit¨at Frankfurt  Frankfurt  Germany  2013.

[12] P. F¨oldi´ak. Learning invariance from transformation sequences. Neural Computation 

3(2):194–200  1991.

[13] L. Wiskott and T. Sejnowski. Slow feature analysis: Unsupervised learning of invariances.

Neural computation  14(4):715–770  2002.

[14] R. Memisevic. Learning to relate images. IEEE Transactions on Pattern Analysis and Machine

Intelligence  35(8):1829–1846  2013.

[15] R. Memisevic and G. E. Hinton. Learning to represent spatial transformations with factored

higher-order boltzmann machines. Neural Computation  22(6):1473–1492  2010.

[16] R. Memisevic. Gradient-based learning of higher-order image features. In 2011 IEEE Inter-

national Conference on Computer Vision  pages 1591–1598. IEEE  2011.

[17] Y. LeCun  F. J. Huang  and L. Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In Proceedings of the 2001 IEEE Conference on Computer
Vision and Pattern Recognition  2001.

[18] D. Martin  Fowlkes C.  D. Tal  and J. Malik. A database of human segmented natural images
and its application to evaluating segmentation algorithms and measuring ecological statistics.
In Proceedings of the Eigth IEEE International Conference on Computer Vision  volume 2 
pages 416–423  July 2001.

[19] G. W. Taylor  G. E. Hinton  and S. T. Roweis. Modeling human motion using binary latent
variables. In Advances in Neural Information Processing Systems 20  pages 1345–1352  2007.
[20] R. Memisevic and G. Exarchakis. Learning invariant features by harnessing the aperture prob-

lem. In Proceedings of the 30th International Conference on Machine Learning  2013.

[21] I. Sutskever  G. E. Hinton  and G. W. Taylor. The recurrent temporal restricted boltzmann
machine. In Advances in Neural Information Processing Systems 21  pages 1601–1608  2008.
[22] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning 

2(1):1–127  2009. Also published as a book. Now Publishers  2009.

9

,Vincent Michalski
Roland Memisevic
Kishore Konda
Junpei Komiyama
Junya Honda
Hiroshi Nakagawa