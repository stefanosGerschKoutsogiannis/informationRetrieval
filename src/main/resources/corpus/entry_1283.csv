2018,Data Amplification: A Unified and Competitive Approach to Property Estimation,Estimating properties of discrete distributions is a fundamental problem in statistical learning. We design the first unified  linear-time  competitive  property estimator that for a wide class of properties and for all underlying distributions uses just 2n samples to achieve the performance attained by the empirical estimator with n\sqrt{\log n} samples. This provides off-the-shelf  distribution-independent  ``amplification'' of the amount of data available relative to common-practice estimators. 

We illustrate the estimator's practical advantages by comparing it to existing estimators for a wide variety of properties and distributions. In most cases  its performance with n samples is even as good as that of the empirical estimator with n\log n samples  and for essentially all properties  its performance is comparable to that of the best existing estimator designed specifically for that property.,Data Ampliﬁcation: A Uniﬁed and Competitive

Approach to Property Estimation

Yi HAO

Alon Orlitsky

Dept. of Electrical and Computer Engineering

Dept. of Electrical and Computer Engineering

University of California  San Diego

University of California  San Diego

La Jolla  CA 92093

yih179@eng.ucsd.edu

La Jolla  CA 92093
alon@eng.ucsd.edu

Ananda T. Suresh

Google Research  New York

New York  NY 10011

theertha@google.com

Yihong Wu

Dept. of Statistics and Data Science

Yale University

New Haven  CT 06511
yihong.wu@yale.edu

Abstract

√

Estimating properties of discrete distributions is a fundamental problem in sta-
tistical learning. We design the ﬁrst uniﬁed  linear-time  competitive  property
estimator that for a wide class of properties and for all underlying distributions uses
just 2n samples to achieve the performance attained by the empirical estimator
with n
log n samples. This provides off-the-shelf  distribution-independent  “am-
pliﬁcation” of the amount of data available relative to common-practice estimators.
We illustrate the estimator’s practical advantages by comparing it to existing
estimators for a wide variety of properties and distributions. In most cases  its
performance with n samples is even as good as that of the empirical estimator with
n log n samples  and for essentially all properties  its performance is comparable
to that of the best existing estimator designed speciﬁcally for that property.

1 Distribution Properties
Let DX denote the collection of distributions over a countable set X of ﬁnite or inﬁnite cardinality k.
A distribution property is a mapping f : DX → R. Many applications call for estimating properties
of an unknown distribution p ∈ DX from its samples. Often these properties are additive  namely can
be written as a sum of functions of the probabilities. Symmetric additive properties can be written as

(cid:88)

x∈X

f (p) def=

f (px) 

and arise in many biological  genomic  and language-processing applications:

Shannon entropy (cid:80)
Normalized support size (cid:80)
Normalized support coverage (cid:80)

size estimation [3].

x∈X px log 1
px

x∈X 1
k

  where throughout the paper log is the natural logarithm  is the

fundamental information measure arising in a variety of applications [1].

1px>0 plays an important role in population [2] and vocabulary
x∈X 1−e−mpx

is the normalized expected number of distinct ele-
ments observed upon drawing Poi(m) independent samples  it arises in ecological [4]  genomic [5] 
and database studies [6].

m

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Power sum (cid:80)
Distance to uniformity (cid:80)

x∈X pa

x  arises in Rényi entropy [7]  Gini impurity [8]  and related diversity measures.

(cid:12)(cid:12)px − 1

k

(cid:12)(cid:12)  appears in property testing [9].

x∈X

More generally  non-symmetric additive properties can be expressed as

(cid:88)

x∈X

f (p) def=

fx(px) 

L1 distance (cid:80)
KL divergence (cid:80)

for example distances to a given distribution  such as:

x∈X |px − qx|  the L1 distance of the unknown distribution p from a given distribu-

tion q  appears in hypothesis-testing errors [10].

  the KL divergence of the unknown distribution p from a given
distribution q  reﬂects the compression [1] and prediction [11] degradation when estimating p by q.

x∈X px log px
qx

Given one of these  or other  properties  we would like to estimate its value based on samples from an
underlying distribution.

2 Recent Results

In the common property-estimation setting  the unknown distribution p generates n i.i.d. samples
X n ∼ pn  which in turn are used to estimate f (p). Speciﬁcally  given property f  we would like to
construct an estimator ˆf : X ∗ → R such that ˆf (X n) is as close to f (p) as possible. The standard
estimation loss is the expected squared loss

(cid:16) ˆf (X n) − f (p)
(cid:17)2

.

EX n∼pn

Generating exactly n samples creates dependence between the number of times different symbols
appear. To avoid these dependencies and simplify derivations  we use the well-known Poisson
sampling [12] paradigm. We ﬁrst select N ∼ Poi(n)  and then generate N independent samples
according to p. This modiﬁcation does not change the statistical nature of the estimation problem
since a Poisson random variables is exponentially concentrated around its mean. Correspondingly the
estimation loss is

(cid:16) ˆf (X N ) − f (p)

(cid:17)2(cid:21)

.

L ˆf (p  n) def= EN∼Poi(n)

EX N∼pN

(cid:20)

(cid:26)(cid:80)

For simplicity  let Nx be the number of occurrences of symbol x in X n. An intuitive estimator is
the plug-in empirical estimator f E that ﬁrst uses the N samples to estimate px = Nx/N and then
estimates f (p) as

(cid:0) Nx

N

(cid:1) N > 0 

f E(X N ) def=

x∈X fx

0

N = 0.

Given an error tolerance parameter δ > 0  the (δ  p)-sample complexity of an estimator ˆf in estimating
f (p) is the smallest number of samples n allowing for estimation loss smaller than δ 

n ˆf (δ  p) def= min

n∈N{L ˆf (p  n) < δ}.

Since p is unknown  the common min-max approach considers the worst case (δ  p)-sample complexity
of an estimator ˆf over all possible p 

n ˆf (δ) def= max
p∈DX

n ˆf (δ  p).

Finally  the estimator minimizing n ˆf (δ) is called the min-max estimator of property f  denoted f M.
It follows that nf M (δ) is the smallest Poisson parameter n  or roughly the number of samples  needed
for any estimator ˆf to estimate f (p) to estimation loss δ for all p.

2

There has been a signiﬁcant amount of recent work on property estimation. In particular  it was
shown that for all seven properties mentioned earlier  f M improves the sample complexity by a
logarithmic factor compared to f E. For example  for Shannon entropy [13]  normalized support
size [14]  normalized support coverage [15]  and distance to uniformity [16]  nf E (δ) = Θδ(k)
while nf M (δ) = Θδ(k/ log k). Note that for normalized support size  DX is typically replaced by
Dk := {p ∈ DX : px ≥ 1/k ∀x ∈ X}  and for normalized support coverage  k is replaced by m.

3 New Results
While the results already obtained are impressive  they also have some shortcomings. Recent state-of-
the-art estimators are designed [13  14  16] or analyzed [15  19] to estimate each individual property.
Consequently these estimators cover only few properties. Second  estimators proposed for more
general properties [15  20] are limited to symmetric properties and are not known to be computable
in time linear in the sample size. Last but not least  by design  min-max estimators are optimized for
the “worst” distribution in a class. In practice  this distribution is often very different  and frequently
much more complex  than the actual underlying distribution. This “pessimistic” worst-case design
results in sub-optimal estimation  as born by both the theoretical and experimental results.
In Section 6  we design an estimator f∗ that addresses all these issues. It is uniﬁed and applies to
a wide range of properties  including all previously-mentioned properties (a > 1 for power sums)
and all Lipschitz properties f where each fx is Lipschitz. It can be computed in linear-time in
the sample size. It is competitive in that it is guaranteed to perform well not just for the worst
distribution in the class  but for each and every distribution. It “ampliﬁes” the data in that it uses
just Poi(2n) samples to approximate the performance of the empirical estimator with Poi(n
log n)
samples regardless of the underlining distribution p  thereby providing an off-the-shelf  distribution-
independent  “ampliﬁcation” of the amount of data available relative to the estimators used by many
practitioners. As we show in Section 8  it also works well in practice  outperforming existing
estimator and often working as well as the empirical estimator with even n log n samples.
For a more precise description  let o(1) represent a quantity that vanishes as n → ∞ and write a (cid:46) b
for a ≤ b(1 + o(1)). Suppressing small  for simplicity ﬁrst  we show that

√

Lf∗ (p  2n) (cid:46) Lf E (p  n(cid:112)log n) + o(1) 

where the ﬁrst right-hand-side term relates the performance of f∗ with 2n samples to that of f E with
√
log n samples. The second term adds a small loss that diminishes at a rate independent of the
n
support size k  and for ﬁxed k decreases roughly as 1/n. Speciﬁcally  we prove 
Theorem 1. For every property f satisfying the smoothness conditions in Section 5  there is a
constant Cf such that for all p ∈ DX and all  ∈ (0  1
2 ) 

(cid:18)

(cid:19)

Lf∗ (p  2n) ≤

1 +

3

log n

Lf E (p  n log

1

2− n) + Cf min

log n + ˜O

(cid:26) k

n

(cid:18) 1

(cid:19)

n

 

1

log n

(cid:27)

.

The ˜O reﬂects a multiplicative polylog(n) factor unrelated to k and p. Again  for normalized support
size  DX is replaced by Dk  and we also modify f∗ as follows: if k > n  we apply f∗  and if k ≤ n 
we apply the corresponding min-max estimator [14]. However  for experiments shown in Section 8 
the original f∗ is used without such modiﬁcation. In Section 7  we note that for several properties 
the second term can be strengthened so that it does not depend on .

4

Implications

Theorem 1 has three important implications.

Data ampliﬁcation Many modern applications  such as those arising in genomics and natural-
language processing  concern properties of distributions whose support size k is comparable to or
even larger than the number of samples n. For these properties  the estimation loss of the empirical
estimator f E is often much larger than 1/ log n  hence the proposed estimator  f∗  yields a much
better estimate whose performance parallels that of f E with n
log n samples. This allows us to
amplify the available data by a factor of

log n regardless of the underlying distribution.

√

√

3

Note however that for some properties f  when the underlying distributions are limited to a ﬁxed
small support size  Lf E (p  n) = Θ(1/n) (cid:28) 1/log n. For such small support sizes  f∗ may not
improve the estimation loss.

Uniﬁed estimator Recent works either prove efﬁcacy results individually for each property [13 
14  16]  or are not known to be computable in linear time [15  20].
By contrast  f∗ is a linear-time estimator well for all properties satisfying simple Lipschitz-type and
second-order smoothness conditions. All properties described earlier: Shannon entropy  normalized
support size  normalized suppport coverage  power sum  L1 distance and KL divergence satisfy these
conditions  and f∗ therefore applies to all of them.
More generally  recall that a property f is Lipschitz if all fx are Lipschitz. It can be shown  e.g. [21] 
that with O(k) samples  f E approximates a k-element distribution to a constant L1 distance  and
hence also estimates any Lipschitz property to a constant loss. It follows that f∗ estimates any
√
Lipschitz property over a distribution of support size k to constant estimation loss with O(k/
log k)
samples. This provides the ﬁrst general sublinear-sample estimator for all Lipschitz properties.

Competitive optimality Previous results were geared towards the estimator’s worst estimation loss
over all possible distributions. For example  they derived estimators that approximate the distance to
uniformity of any k-element distribution with O(k/ log k) samples  and showed that this number is
optimal as for some distribution classes estimating this distance requires Ω(k/ log k) samples.
However  this approach may be too pessimistic. Distributions are rarely maximally complex  or are
hardest to estimate. For example  most natural scenes have distinct simple patterns  such as straight
lines  or ﬂat faces  hence can be learned relatively easily.
More concretely  consider learning distance to uniformity for the collection of distributions with
entropy bounded by log log k. It can be shown that for sufﬁciently large k  f E can learn distance to
uniformity to constant estimation loss using O((log k)Θ(1)) samples. Theorem 1 therefore shows that
√
the distance to uniformity can be learned to constant estimation loss with O((log k)Θ(1)/
log log k)
samples. (In fact  without even knowing that the entropy is bounded.) By contrast  the original
min-max estimator results would still require the much larger Ω(k/ log k) samples.
The rest of the paper is organized as follows. Section 5 describes mild smoothness conditions satisﬁed
by many natural properties  including all those mentioned above. Section 6 describes the estimator’s
explicit form and some intuition behind its construction and performance. Section 7 describes
two improvements of the estimator addressed in the supplementary material. Lastly  Section 8
describes various experiments that illustrate the estimator’s power and competitiveness. For space
considerations  we relegate all the proofs to the supplemental material.

5 Smooth properties

Many natural properties  including all those mentioned in the introduction satisfy some basic smooth-
ness conditions. For h ∈ (0  1]  consider the Lipschitz-type parameter

(cid:96)f (h) def= max

x

max

u v∈[0 1]:max{u v}≥h

|fx(u) − fx(v)|

|u − v|

 

(cid:18) u + v

2

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:27)

(cid:26)(cid:12)(cid:12)(cid:12)(cid:12) fx(u) + fx(v)

2

and the second-order smoothness parameter  resembling the modulus of continuity in approximation
theory [17  18] 
f (h) def= max
ω2

− fx

max

.

x

u v∈[0 1]:|u−v|≤2h

f (h) ≤ Sf · h for some absolute constant Sf .

We consider properties f satisfying the following conditions: (1) ∀x ∈ X   fx(0) = 0; (2) (cid:96)f (h) ≤
polylog(1/h) for h ∈ (0  1]; (2) ω2
Note that the ﬁrst condition  fx(0) = 0  entails no loss of generality. The second condition implies
that fx is continuous over [0  1]  and in particular right continuous at 0 and left-continuous at 1.
It is easy to see that continuity is also essential for consistent estimation. Observe also that these
conditions are more general than assuming that fx is Lipschitz  as can be seen for entropy where
fx = x log x  and that all seven properties described earlier satisfy these three conditions. Finally  to
ensure that L1 distance satisﬁes these conditions  we let fx(px) = |px − qx| − qx.

4

6 The Estimator f∗
Given the sample size n  deﬁne an ampliﬁcation parameter t > 1  and let N(cid:48)(cid:48) ∼ Poi(nt) be the
ampliﬁed sample size. Generate a sample sequence X N(cid:48)(cid:48)
x denote
the number of times symbol x appeared in X N(cid:48)(cid:48)
. The empirical estimate of f (p) with Poi(nt)
samples is then

independently from p  and let N(cid:48)(cid:48)

(cid:88)

x∈X

(cid:19)

(cid:18) N(cid:48)(cid:48)

x

N(cid:48)(cid:48)

.

f E(X N(cid:48)(cid:48)

) =

fx

Our objective is to construct an estimator f∗ that approximates f E(X N(cid:48)(cid:48)
Poi(2n) samples.
Since N(cid:48)(cid:48) sharply concentrates around nt  we can show that f E(X N(cid:48)(cid:48)
modiﬁed empirical estimator 

) for large t using just

) can be approximated by the

f ME(X N(cid:48)(cid:48)

) def=

fx

(cid:88)

x∈X

(cid:18) N(cid:48)(cid:48)

(cid:19)

x
nt

 

where fx(p) def= fx(1) for all p > 1 and x ∈ X .
Since large probabilities are easier to estimate  it is natural to set a threshold parameter s and rewrite
the modiﬁed estimator as a separate sum over small and large probabilities 

(cid:88)

x∈X

(cid:18) N(cid:48)(cid:48)

(cid:19)

x
nt

(cid:88)

x∈X

(cid:18) N(cid:48)(cid:48)

(cid:19)

x
nt

f ME(X N(cid:48)(cid:48)

) =

fx

1px≤s +

fx

1px>s.

Note however that we do not know the exact probabilities. Instead  we draw two independent sample
sequences X N and X N(cid:48)
x be the
number of occurrences of x in the ﬁrst and second sample sequence respectively. We then set a
small/large-probability threshold s0 and classify a probability px as large or small according to N(cid:48)
x:

from p  each of an independent Poi(n) size  and let Nx and N(cid:48)

is the modiﬁed small-probability empirical estimator  and

S (X N(cid:48)(cid:48)
f ME

  X N(cid:48)

) def=

L (X N(cid:48)(cid:48)
f ME

  X N(cid:48)

) def=

(cid:19)
(cid:19)

(cid:18) N(cid:48)(cid:48)
(cid:18) N(cid:48)(cid:48)

x
nt

x
nt

fx

fx

(cid:88)
(cid:88)

x∈X

x∈X

1N(cid:48)

x≤s0

1N(cid:48)

x>s0

is the modiﬁed large-probability empirical estimator. We rewrite the modiﬁed empirical estimator as

f ME(X N(cid:48)(cid:48)

) = f ME

S (X N(cid:48)(cid:48)

  X N(cid:48)

) + f ME

L (X N(cid:48)(cid:48)

  X N(cid:48)

).

Correspondingly  we express our estimator f∗ as a combination of small- and large-probability
estimators 

f∗(X N   X N(cid:48)

) def= f∗

The large-probability estimator approximates f ME
L (X N   X N(cid:48)

f∗
L(X N   X N(cid:48)

) def= f ME

S(X N   X N(cid:48)
L (X N(cid:48)(cid:48)

).

) + f∗
  X N(cid:48)

(cid:88)

L(X N   X N(cid:48)
(cid:19)
) as

(cid:18) Nx

) =

fx

x∈X

nt

1N(cid:48)

x>s0 .

Note that we replaced the length-Poi(nt) sample sequence X N(cid:48)(cid:48)
sample sequence X N . We can do so as large probabilities are well estimated from fewer samples.
The small-probability estimator f∗
) and is more involved.
We outline its construction below and details can be found in Section 8 of the supplemental material.
The expected value of f ME for the small probabilities is

by the independent length-Poi(n)

) approximates f ME

S(X N   X N(cid:48)

S (X N(cid:48)(cid:48)

  X N(cid:48)

E[f ME

S (X N(cid:48)(cid:48)

  X N(cid:48)

)] =

E[1Nx≤s0 ]E

fx

(cid:20)

(cid:18) N(cid:48)(cid:48)

(cid:19)(cid:21)

x
nt

.

(cid:88)

x∈X

5

Let λx

def= npx be the expected number of times symbol x will be observed in X N   and deﬁne

gx(v) def= fx

t − 1

.

(cid:17)(cid:18) t

(cid:16) v
(cid:17)
(cid:16) v

nt

nt

(cid:19)v
∞(cid:88)

v=1

e−λxt (λxt)v
v!

fx

= e−λx

e−λx(t−1) (λx(t − 1))v

gx (v) .

v!

Then

(cid:20)

fx

(cid:19)(cid:21)

(cid:18) N(cid:48)(cid:48)

x
nt

E

∞(cid:88)

v=0

=

As explained in Section 8.1 of the supplemental material  the sum beyond a truncation threshold

is small  hence it sufﬁces to consider the truncated sum

umax

def= 2s0t + 2s0 − 1
e−λx(t−1) (λx(t − 1))v

v!

umax(cid:88)

v=1

e−λx

gx (v) .

Applying the polynomial smoothing technique in [22]  Section 8.2 of the supplemental material
approximates the above summation by

∞(cid:88)

e−λx

where

hx v = (t − 1)v

(umax∧v)(cid:88)

u=1

hx vλv
x 

v=1

gx(u)(−1)v−u
(v − u)!u!

1 − e−r

  

v+u(cid:88)

j=0

rj
j!

rj
j! is the tail probability of a Poi(r) distribution that diminishes rapidly
beyond r. Hence r determines which summation terms will be attenuated  and serves as a smoothing
parameter.

j=0

r def= 10s0t + 10s0.

and

Observe that 1 − e−r(cid:80)v+u
An unbiased estimator of e−λx(cid:80)∞
∞(cid:88)

x is

v=1 hx vλv
hx vv! · 1Nx=v = hx Nx · Nx!.

Finally  the small-probability estimator is
f∗
S(X N   X N(cid:48)

) def=

v=1

hx Nx · Nx! · 1N(cid:48)

x≤s0.

(cid:88)

x∈X

(cid:26)

(cid:26)

7 Extensions
In Theorem 1  for ﬁxed n  as  → 0  the ﬁnal slack term 1/ log n approaches a constant. For
certain properties it can be improved. For normalized support size  normalized support coverage  and
distance to uniformity  a more involved estimator improves this term to

Cf γ min
for any ﬁxed constant γ ∈ (0  1/2).
For Shannon entropy  correcting the bias of f∗
reduces the slack term even more  to

n log1− n

k

+

1
n1−γ  

1

log1+ n

L [23] and further dividing the probability regions 

Cf γ min

k2

n2 log2− n

+

1
n1−γ  

1

log2+2 n

.

Finally  the theorem compares the performance of f∗ with 2n samples to that of f E with n
log n
samples. As shown in the next section  the performance is often comparable to that of n log n samples.
It would be interesting to prove a competitive result that enlarges the ampliﬁcation to n log1− n or
even n log n. This would be essentially the best possible as it can be shown that for the symmetric
properties mentioned in the introduction  ampliﬁcation cannot exceed O(n log n).

√

(cid:27)

 

(cid:27)

6

√

8 Experiments
We evaluated the new estimator f∗ by comparing its performance to several recent estimators [13–
15  22  27]. To ensure robustness of the results  we performed the comparisons for all the symmetric
properties described in the introduction: entropy  support size  support coverage  power sums  and
distance to uniformity. For each property  we considered six underlying distributions: uniform 
Dirichlet-drawn-  Zipf  binomial  Poisson  and geometric. The results for the ﬁrst three properties
are shown in Figures 1–3  the plots for the ﬁnal two properties can be found in Section 9 of the
supplemental material. For nearly all tested properties and distributions  f∗ achieved state-of-the-art
performance.
As Theorem 1 implies  for all ﬁve properties  with just n (not even 2n) samples  f∗ performed as well
log n samples. Interestingly  in most cases f∗ performed
the empirical estimator f E with roughly n
even better  similar to f E with n log n samples.
Relative to previous estimators  depending on the property and distribution  different previous
estimators were best. But in essentially all experiments  f∗ was either comparable or outperformed
the best previous estimator. The only exception was PML that attempts to smooth the estimate  hence
performed better on uniform  and near-uniform Dirichlet-drawn distributions for several properties.
Two additional advantages of f∗ may be worth noting. First  underscoring its competitive performance
for each distribution  the more skewed the distribution the better is its relative efﬁcacy. This is because
most other estimators are optimized for the worst distribution  and work less well for skewed ones.
Second  by its simple nature  the empirical estimator f E is very stable. Designed to emulate f E for
more samples  f∗ is therefore stable as well. Note also that f E is not always the best estimator choice.
For example  it always underestimates the distribution’s support size. Yet even for normalized support
size  Figure 2 shows that f∗ outperforms other estimators including those designed speciﬁcally for
this property (except as above for PML on near-uniform distributions).
The next subsection describes the experimental settings. Additional details and further interpretation
of the observed results can be found in Section 9 of the supplemental material.

Experimental settings

√

We tested the ﬁve properties on the following distributions: uniform distribution; a distribution
randomly generated from Dirichlet prior with parameter 2; Zipf distribution with power 1.5; Binomial
distribution with success probability 0.3; Poisson distribution with mean 3 000; geometric distribution
with success probability 0.99.
With the exception of normalized support coverage  all other properties were tested on distributions
of support size k = 10 000. The Geometric  Poisson  and Zipf distributions were truncated at k and
re-normalized. The number of samples  n  ranged from 1 000 to 100 000  shown logarithmically on
the horizontal axis. Each experiment was repeated 100 times and the reported results  shown on the
vertical axis  reﬂect their mean squared error (MSE).
We compared the estimator’s performance with n samples to that of four other recent estimators as
well as the empirical estimator with n  n
log n  and n log n samples. We chose the ampliﬁcation
parameter t as log1−α n + 1  where α ∈ {0.0  0.1  0.2  ...  0.6} was selected based on independent
data  and similarly for s0. Since f∗ performed even better than Theorem 1 guarantees  α ended up
√
between 0 and 0.3 for all properties  indicating ampliﬁcation even beyond n
log n. The graphs
denote f∗ by NEW  f E with n samples by Empirical  f E with n
log n samples by Empirical+  f E
with n log n samples by Empirical++  the pattern maximum likelihood estimator in [15] by PML  the
Shannon-entropy estimator in [27] by JVHW  the normalized-support-size estimator in [14] and the
entropy estimator in [13] by WY  and the smoothed Good-Toulmin Estimator for normalized support
coverage estimation [22]  slightly modiﬁed to account for previously-observed elements that may
appear in the subsequent sample  by SGT.
While the empirical and the new estimators have the same form for all properties  as noted in the
introduction  the recent estimators are property-speciﬁc  and each was derived for a subset of the
properties. In the experiments we applied these estimators to all the properties for which they were
derived. Also  additional estimators [28–34] for various properties were compared in [13  14  22  27]
and found to perform similarly to or worse than recent estimators  hence we do not test them here.

√

7

Figure 1: Shannon Entropy

Figure 2: Normalized Support Size

Figure 3: Normalized Support Coverage

8

9 Conclusion

√

In this paper  we considered the fundamental learning problem of estimating properties of discrete
distributions. The best-known distribution-property estimation technique is the “empirical estimator”
that takes the data’s empirical frequency and plugs it in the property functional. We designed a
general estimator that for a wide class of properties  uses only n samples to achieve the same accuracy
log n samples. This provides an off-the-shelf method for amplifying
as the plug-in estimator with n
the data available relative to traditional approaches. For all the properties and distributions we have
tested  the proposed estimator performed as well as the best estimator(s). A meaningful future
research direction would be to verify the optimality of our results: the ampliﬁcation factor
log n
and the slack terms. There are also several important properties that are not included in our paper 
for example  Rényi entropy [35] and the generalized distance to uniformity [36  37]. It would be
interesting to determine whether data ampliﬁcation could be obtained for these properties as well.

√

References
[1] COVER  T. M.  & THOMAS  J. A. (2012). Elements of information theory. John Wiley & Sons.

[2] GOOD  I. J. (1953). The population frequencies of species and the estimation of population

parameters. Biometrika  40(3-4)  237-264.

[3] MCNEIL  D. R. (1973). Estimating an author’s vocabulary. Journal of the American Statistical

Association  68(341)  92-96.

[4] COLWELL  R. K.  CHAO  A.  GOTELLI  N. J.  LIN  S. Y.  MAO  C. X.  CHAZDON  R. L. 
& LONGINO  J. T. (2012). Models and estimators linking individual-based and sample-based
rarefaction  extrapolation and comparison of assemblages. Journal of plant ecology  5(1)  3-21.

[5] IONITA-LAZA  I.  LANGE  C.  & LAIRD  N. M. (2009). Estimating the number of unseen
variants in the human genome. Proceedings of the National Academy of Sciences  106(13) 
5008-5013.

[6] HAAS  P. J.  NAUGHTON  J. F.  SESHADRI  S.  & STOKES  L. (1995). Sampling-based

estimation of the number of distinct values of an attribute. VLDB  Vol. 95  pp. 311-322.

[7] RÉNYI  A. (1961). On measures of entropy and information. HUNGARIAN ACADEMY OF

SCIENCES Budapest Hungary.

[8] LOH  W. Y. (2011). Classiﬁcation and regression trees. Wiley Interdisciplinary Reviews: Data

Mining and Knowledge Discovery  1(1)  14-23.

[9] CANONNE  C. L. (2017). A Survey on Distribution Testing. Your Data is Big. But is it Blue.

[10] LEHMANN  E. L.  & ROMANO  J. P. (2006). Testing statistical hypotheses. Springer Science &

Business Media.

[11] KULLBACK  S.  & LEIBLER  R. A. (1951). On information and sufﬁciency. The annals of

mathematical statistics  22(1)  79-86.

[12] SÄRNDAL  C. E.  SWENSSON  B.  & WRETMAN  J. (2003). Model assisted survey sampling.

Springer Science & Business Media.

[13] WU  Y.  & YANG  P. (2016). Minimax rates of entropy estimation on large alphabets via best

polynomial approximation  IEEE Transactions on Information Theory  62(6)  3702-3720.

[14] WU  Y.  & YANG  P. (2015). Chebyshev polynomials  moment matching  and optimal estimation

of the unseen. ArXiv preprint arXiv:1504.01227.

[15] ACHARYA  J.  DAS  H.  ORLITSKY  A.  & SURESH  A. T. (2017). A uniﬁed maximum
likelihood approach for estimating symmetric properties of discrete distributions. In International
Conference on Machine Learning (pp. 11-21).

[16] JIAO  J.  HAN  Y.  & WEISSMAN  T. (2016). Minimax estimation of the L1 distance. In

Information Theory (ISIT)  2016 IEEE International Symposium on (pp. 750-754). IEEE.

9

[17] TIMAN  A. F. (2014). Theory of approximation of functions of a real variable. Elsevier.

[18] KORN ˘EICHUK  N. P. (1991). Exact constants in approximation theory. (Vol. 38). Cambridge

University Press.

[19] VALIANT  G.  & VALIANT  P. (2011). The power of linear estimators. In Foundations of

Computer Science (FOCS)  2011 IEEE 52nd Annual Symposium on (pp. 403-412). IEEE.

[20] HAN  Y.  JIAO  J.  & WEISSMAN  T. (2018). Local moment matching: A uniﬁed methodology
for symmetric functional estimation and distribution estimation under Wasserstein distance. arXiv
preprint arXiv:1802.08405.

[21] KAMATH  S.  ORLITSKY  A.  PICHAPATI  D.  & SURESH  A. T. (2015  June). On learning

distributions from their samples. In Conference on Learning Theory (pp. 1066-1100).

[22] ORLITSKY  A.  SURESH  A. T.  & WU  Y. (2016). Optimal prediction of the number of unseen

species. Proceedings of the National Academy of Sciences  201607774.

[23] CARLTON  A. G. (1969). On the bias of information estimates. Psychological Bulletin  71(2) 

108.

[24] CHUNG  F. R.  & LU  L. (2017). Complex graphs and networks. (No. 107). American

Mathematical Soc.

[25] BUSTAMANTE  J. (2017). Bernstein operators and their properties. Chicago.

[26] WATSON  G. N. (1995). A treatise on the theory of Bessel functions. Cambridge University

Press.

[27] JIAO  J.  VENKAT  K.  HAN  Y.  & WEISSMAN  T. (2015). Minimax estimation of functionals

of discrete distributions. IEEE Transactions on Information Theory  61(5)  2835-2885.

[28] VALIANT  P.  & VALIANT  G. (2013). Estimating the unseen: improved estimators for entropy
and other properties. In Advances in Neural Information Processing Systems (pp. 2157-2165).

[29] PANINSKI  L. (2003). Estimation of entropy and mutual information. Neural computation 

15(6)  1191-1253.

[30] CARLTON  A. G. (1969). On the bias of information estimates. Psychological Bulletin  71(2) 

108.

[31] GOOD  I. J. (1953). The population frequencies of species and the estimation of population

parameters. Biometrika  40(3-4)  237-264.

[32] CHAO  A. (1984). Nonparametric estimation of the number of classes in a population. Scandi-

navian Journal of Statistics  265-270.

[33] CHAO  A. (2005). Species estimation and applications. Encyclopedia of statistical sciences.

[34] SMITH  E. P.  & VAN BELLE  G. (1984). Nonparametric estimation of species richness.

Biometrics  119-129.

[35] ACHARYA  J.  ORLITSKY  A.  SURESH  A. T.  & TYAGI  H. (2017). Estimating Rényi entropy

of discrete distributions. IEEE Transactions on Information Theory  63(1)  38-56.

[36] HAO  Y.  & ORLITSKY  A. (2018  June). Adaptive estimation of generalized distance to
uniformity. In 2018 IEEE International Symposium on Information Theory (ISIT) (pp. 1076-
1080). IEEE.

[37] BATU  T.  & CANONNE  C. L. (2017  October). Generalized Uniformity Testing. In 2017 IEEE

58th Annual Symposium on Foundations of Computer Science (FOCS) (pp. 880-889). IEEE.

10

,Yi Hao
Alon Orlitsky
Ananda Theertha Suresh
Yihong Wu