2013,On the Sample Complexity of Subspace Learning,A large number of algorithms in machine learning  from principal component analysis (PCA)  and its non-linear (kernel) extensions  to more recent spectral embedding and support estimation methods  rely on estimating a linear subspace from samples.  In this paper we introduce  a general formulation  of this problem and derive novel learning error estimates. Our results rely on  natural  assumptions on the spectral properties of the covariance operator associated to  the data distribution  and hold for a wide class of metrics between subspaces. As special cases  we discuss sharp error estimates  for the reconstruction properties of PCA and spectral support estimation. Key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods.,On the Sample Complexity of Subspace Learning

Robotics Brain and Cognitive Science

Massachussetss Institute of Technology

Guillermo D. Canas

guilledc@mit.edu

Alessandro Rudi

Istituto Italiano di Tecnologia

alessandro.rudi@iit.it

Lorenzo Rosasco

Universita’ degli Studi di Genova  LCSL 

Massachusetts Institute of Technology & Istituto Italiano di Tecnologia

lrosasco@mit.edu

Abstract

A large number of algorithms in machine learning  from principal component
analysis (PCA)  and its non-linear (kernel) extensions  to more recent spectral
embedding and support estimation methods  rely on estimating a linear subspace
from samples. In this paper we introduce a general formulation of this problem
and derive novel learning error estimates. Our results rely on natural assumptions
on the spectral properties of the covariance operator associated to the data distribu-
tion  and hold for a wide class of metrics between subspaces. As special cases  we
discuss sharp error estimates for the reconstruction properties of PCA and spectral
support estimation. Key to our analysis is an operator theoretic approach that has
broad applicability to spectral learning methods.

1

Introduction

The subspace learning problem is that of ﬁnding the smallest linear space supporting data drawn
from an unknown distribution.
It is a classical problem in machine learning and statistics  with
several established algorithms addressing it  most notably PCA and kernel PCA [12  18]. It is also
at the core of a number of spectral methods for data analysis  including spectral embedding meth-
ods  from classical multidimensional scaling (MDS) [7  26]  to more recent manifold embedding
methods [22  16  2]  and spectral methods for support estimation [9]. Therefore knowledge of the
speed of convergence of the subspace learning problem  with respect to the sample size  and the
algorithms’ parameters  is of considerable practical importance.
Given a measure ρ from which independent samples are drawn  we aim to estimate the smallest
subspace Sρ that contains the support of ρ. In some cases  the support may lie on  or close to  a
subspace of lower dimension than the embedding space  and it may be of interest to learn such a
subspace Sρ in order to replace the original samples by their local encoding with respect to Sρ.
While traditional methods  such as PCA and MDS  perform such subspace estimation in the data’s
original space  other  more recent manifold learning methods  such as isomap [22]  Hessian eigen-
maps [10]  maximum-variance unfolding [24  25  21]  locally-linear embedding [16  17]  and Lapla-
cian eigenmaps [2] (but also kernel PCA [18])  begin by embedding the data in a feature space  in
which subspace estimation is carried out. Indeed  as pointed out in [11  4  3]  the algorithms in this
family have a common structure. They embed the data in a suitable Hilbert space H  and compute
a linear subspace that best approximates the embedded data. The local coordinates in this subspace
then become the new representation space. Similar spectral techniques may also be used to estimate
the support of the data itself  as discussed in [9].

1

While the subspace estimates are derived from the available samples only  or their embedding  the
learning problem is concerned with the quality of the computed subspace as an estimate of Sρ (the
true span of the support of ρ). In particular  it may be of interest to understand the quality of these
estimates  as a function of the algorithm’s parameters (typically the dimensionality of the estimated
subspace).
We begin by deﬁning the subspace learning problem (Sec. 2)  in a sufﬁciently general way to encom-
pass a number of well-known problems as special cases (Sec. 4). Our main technical contribution is
a general learning rate for the subspace learning problem  which is then particularized to common
instances of this problem (Sec. 3). Our proofs use novel tools from linear operator theory to obtain
learning rates for the subspace learning problem which are signiﬁcantly sharper than existing ones 
under typical assumptions  but also cover a wider range of performance metrics. A full sketch of the
main proofs is given in Section 7  including a brief description of some of the novel tools developed.
We conclude with experimental evidence  and discussion (Sec. 5 and 6).

2 Problem deﬁnition and notation
Given a measure ρ with support M in the unit ball of a separable Hilbert space H  we consider in
this work the problem of estimating  from n i.i.d. samples Xn = {xi}1≤i≤n  the smallest linear
subspace Sρ := span(M ) that contains M.
The quality of an estimate ˆS of Sρ  for a given metric (or error criterion) d  is characterized in terms
of probabilistic bounds of the form

d(Sρ  ˆS) ≤ ε(δ  n  ρ)

0 < δ ≤ 1.

(1)

P(cid:104)

(cid:105) ≥ 1 − δ 

for some function ε of the problem’s parameters. We derive in the sequel high probability bounds of
the above form.
In the remainder the metric projection operator onto a subspace S is denoted by PS  where P 2
S =
S = PS (every P is idempotent and self-adjoint). We denote by (cid:107) · (cid:107)H the norm induced by the
P ∗

dot product < · · >H in H  and by (cid:107)A(cid:107)p := p(cid:112)Tr(|A|p) the p-Schatten  or p-class norm of a linear

bounded operator A [15  p. 84].

2.1 Subspace estimates
(cid:80)n
Letting C := Ex∼ρx ⊗ x be the (uncentered) covariance operator associated to ρ  it is easy to show
i=1 x ⊗ x  we deﬁne the
that Sρ = Ran C. Similarly  given the empirical covariance Cn := 1
n
empirical subspace estimate

ˆSn := span(Xn) = Ran Cn

n := Ran C k

(note that the closure is not needed in this case because ˆSn is ﬁnite-dimensional). We also deﬁne
the k-truncated (kernel) PCA subspace estimate ˆSk
n  where C k
n is obtained from Cn by
keeping only its k top eigenvalues. Note that  since the PCA estimate ˆSk
n is spanned by the top k
n ⊆ ˆSk(cid:48)
n}n
n for k < k(cid:48)  and therefore { ˆSk
eigenvectors of Cn  then clearly ˆSk
k=1 is a nested family of
subspaces (all of which are contained in Sρ).
As discussed in Section 4.1  since kernel-PCA reduces to regular PCA in a feature space [18] (and
can be computed with knowledge of the kernel alone)  the following discussion applies equally to
kernel-PCA estimates  with the understanding that  in that case  Sρ is the span of the support of ρ in
the feature space.

2.2 Performance criteria

In order for a bound of the form of Equation (1) to be meaningful  a choice of performance criteria
d must be made. We deﬁne the distance

(2)
between subspaces U  V   which is a metric over the space of subspaces contained in Sρ  for 0 ≤
2 and 1 ≤ p ≤ ∞. Note that dα p depends on ρ through C but  in the interest of clarity 
α ≤ 1

dα p(U  V ) := (cid:107)(PU − PV )C α(cid:107)p

2

this dependence is omitted in the notation. While of interest in its own right  it is also possible
to express important performance criteria as particular cases of dα p. In particular  the so-called
reconstruction error [13]:

dR(Sρ  ˆS) := Ex∼ρ(cid:107)PSρ(x) − P ˆS(x)(cid:107)2H

is dR(Sρ ·) = d1/2 2(Sρ ·)2 .
Note that dR is a natural criterion because a k-truncated PCA estimate minimizes a suitable error
dR over all subspaces of dimension k. Clearly  dR(Sρ  ˆS) vanishes whenever ˆS contains Sρ and 
because the family { ˆSk
n) is non-increasing with k.
As shown in [13]  a number of unsupervised learning algorithms  including (kernel) PCA  k-means 
k-ﬂats  sparse coding  and non-negative matrix factorization  can be written as a minimization of dR
over an algorithm-speciﬁc class of sets (e.g. over the set of linear subspaces of a ﬁxed dimension in
the case of PCA).

k=1 of PCA estimates is nested  then dR(Sρ  ˆSk

n}n

3 Summary of results

Our main technical contribution is a bound of the form of Eq. (1)  for the k-truncated PCA estimate
ˆSk
n (with the empirical estimate ˆSn := ˆSn
n being a particular case)  whose proof is postponed to
Sec. 7. We begin by bounding the distance dα p between Sρ and the k-truncated PCA estimate ˆSk
n 
given a known covariance C.
Theorem 3.1. Let {xi}1≤i≤n be drawn i.i.d. according to a probability measure ρ supported on
the unit ball of a separable Hilbert space H  with covariance C. Assuming n > 3  0 < δ < 1 
0 ≤ α ≤ 1

2   1 ≤ p ≤ ∞  the following holds for each k ∈ {1  . . .   n}:

P(cid:104)

n) ≤ 3tα(cid:13)(cid:13)C α(C + tI)−α(cid:13)(cid:13)p

dα p(Sρ  ˆSk
δ }  and σk is the k-th top eigenvalue of C.

(cid:105) ≥ 1 − δ

(3)

n log n

where t = max{σk  9
We say that C has eigenvalue decay rate of order r if there are constants q  Q > 0 such that
qj−r ≤ σj ≤ Qj−r  where σj are the (decreasingly ordered) eigenvalues of C  and r > 1. From
Equation (2) it is clear that  in order for the subspace learning problem to be well-deﬁned  it must
be (cid:107)C α(cid:107)p < ∞  or alternatively: αp > 1/r. Note that this condition is always met for p = ∞  and
also holds in the reconstruction error case (α = 1/2  p = 2)  for any decay rate r > 1.
Knowledge of an eigenvalue decay rate can be incorporated into Theorem 3.1 to obtain explicit
learning rates  as follows.
Theorem 3.2 (Polynomial eigenvalue decay). Let C have eigenvalue decay rate of order r. Under
the assumptions of Theorem 3.1  it is  with probability 1 − δ
if k < k∗
if k ≥ k∗

(cid:40)
n) ≤
(cid:17)1/r
  and Q(cid:48) = 3(cid:0)Q1/rΓ(αp − 1/r)Γ(1 + 1/r)/Γ(1/r)(cid:1)1/p.

(polynomial decay)
(plateau)

−rα+ 1
−rα+ 1

dα p(Sρ  ˆSk

where it is k∗

n =

Q(cid:48)k
Q(cid:48)k∗

n

(cid:16)

p

p

(4)

qn

9 log(n/δ)

n

n

The above theorem guarantees a drop in dα p with increasing k  at a rate of k−rα+1/p  up to k = k∗
n 
after which the bound remains constant. The estimated plateau threshold k∗ is thus the value of
truncation past which the upper bound does not improve. Note that  as described in Section 5  this
performance drop and plateau behavior is observed in practice.
The proofs of Theorems 3.1 and 3.2 rely on recent non-commutative Bernstein-type inequalities on
operators [5  23]  and a novel analytical decomposition. Note that classical Bernstein inequalities in
Hilbert spaces (e.g. [14]) could also be used instead of [23]. However  while this approach would
simplify the analysis  it produces looser bounds  as described in Section 7.
If we consider an algorithm that produces  for each set of n samples  an estimate ˆSk
then  by plugging the deﬁnition of k∗
n.

n with k ≥ k∗
n
n into Eq. 4  we obtain an upper bound on dα p as a function of

3

Corollary 3.3. Let C have eigenvalue decay rate of order r  and Q(cid:48)  k∗
ˆS∗
n be a truncated subspace estimate ˆSk

n. It is  with probability 1 − δ 

n be as in Theorem 3.2. Let

dα p(Sρ  ˆS∗

(cid:19)α− 1

n with k ≥ k∗

n) ≤ Q(cid:48)(cid:18) 9 (log n − log δ)
(cid:32)(cid:18) log n − log δ
rp(cid:33)
(cid:19)α− 1

qn

rp

n

.

dα p(Sρ  Sn) = O

Remark 3.4. Note that  by setting k = n  the above corollary also provides guarantees on the rate
of convergence of the empirical estimate Sn = span(Xn) to Sρ  of order

n ≤ n (or equivalently such that
Corollary 4.1 and remark 3.4 are valid for all n such that k∗
nr−1(log n − log δ) ≥ q/9). Note that  because ρ is supported on the unit ball  its covariance
has eigenvalues no greater than one  and therefore it must be q < 1. It thus sufﬁces to require that
n > 3 to ensure the condition k∗

n ≤ n to hold.

4 Applications of subspace learning

We describe next some of the main uses of subspace learning in the literature.

4.1 Kernel PCA and embedding methods

n(cid:88)

One of the main applications of subspace learning is in reducing the dimensionality of the input.
In particular  one may ﬁnd nested subspaces of dimension 1 ≤ k ≤ n that minimize the dis-
tances from the original to the projected samples. This procedure is known as the Karhunen-Lo`eve 
PCA  or Hotelling transform [12]  and has been generalized to Reproducing-Kernel Hilbert Spaces
(RKHS) [18].
In particular  the above procedure amounts to computing an eigen-decomposition of the empirical
covariance (Sec. 2.1):

i=1

Cn =

n := Ran C k

σiui ⊗ ui 
n = span{ui : 1 ≤ i ≤ k}. Note that  in the
where the k-th subspace estimate is ˆSk
general case of kernel PCA  we assume the samples {xi}1≤i≤n to be in some RKHS H  which are
obtained from the observed variables (z1  . . .   zn) ∈ Z n  for some space Z  through an embedding
:= φ(zi). Typically  due to the very high dimensionality of H  we may only have indirect
xi
information about φ in the form a kernel function K : Z × Z → R: a symmetric  positive deﬁnite
function satisfying K(z  w) = (cid:104)φ(z)  φ(w)(cid:105)H [20] (for technical reasons  we also assume K to be
continuous). Note that every such K has a unique associated RKHS  and viceversa [20  p. 120–121] 
whereas  given K  the embedding φ is only unique up to an inner product-preserving transformation.
Given a point z ∈ Z  we can make use of K to compute the coordinates of the projection of its
embedding φ(z) onto ˆSk
It is easy to see that the k-truncated kernel PCA subspace ˆSk
error dR( ˆSn  ˆS)  among all subspaces ˆS of dimension k. Indeed  it is

n ⊆ H by means of a simple k-truncated eigen-decomposition of Kn.

n minimizes the empirical reconstruction

dR( ˆSn  ˆS) = Ex∼ ˆρ(cid:107)x − P ˆS(x)(cid:107)2H = Ex∼ ˆρ

(cid:10)I − P ˆS  x ⊗ x(cid:11)

HS

= Ex∼ ˆρ

(cid:10)(I − P ˆS)x  (I − P ˆS)x(cid:11)
=(cid:10)I − P ˆS  Cn

(cid:11)

 

HS

H

(5)

HS

where (cid:104)· ·(cid:105)
is the Hilbert-Schmidt inner product  from which it is easy to see that the k-
dimensional subspace minimizing Equation 5 (alternatively maximizing < P ˆS  Cn >) is spanned
by the k-top eigenvectors of Cn.
Since we are interested in the expected dR(Sρ  ˆSk
n) (rather than the empirical dR( ˆSn  ˆS)) error of the
kernel PCA estimate  we may obtain a learning rate for Equation 5 by particularizing Theorem 3.2

4

to the reconstruction error  for all k (Theorem 3.2)  and for k ≥ k∗ with a suitable choice of k∗
(Corollary 4.1). In particular  recalling that dR(Sρ ·) = dα p(Sρ ·)2 with α = 1/2 and p = 2 
and choosing a value of k ≥ k∗
n that minimizes the bound of Theorem 3.2  we obtain the following
result.

Corollary 4.1 (Performance of PCA / Reconstruction error). Let C have eigenvalue decay rate of
order r  and ˆS∗

n be as in Corollary 3.3. Then it holds  with probability 1 − δ 

(cid:32)(cid:18) log n − log δ

(cid:19)1−1/r (cid:33)

dR(Sρ  ˆS∗

n) = O

n

where the dependence on δ is hidden in the Landau symbol.

4.2 Support estimation

The problem of support estimation consists in recovering the support M of a distribution ρ on
a metric space Z from identical and independent samples Zn = (zi)1≤i≤n. We brieﬂy recall a
recently proposed approach to support estimation based on subspace learning [9]  and discuss how
our results specialize to this setting  producing a qualitative improvement to theirs.
Given a suitable reproducing kernel K on Z (with associated feature map φ)  the support M can
be characterized in terms of the subspace Sρ = span φ(M ) ⊆ H [9]. More precisely  letting
dV (x) = (cid:107)x − PV x(cid:107)H be the point-subspace distance to a subspace V   it can be shown (see [9])
that  if the kernel separates 1 M  then it is

M = {z ∈ Z | dSρ (φ(z)) = 0}.

This suggests an empirical estimate ˆM = {z ∈ Z | d ˆS(φ(z)) ≤ τ} of M  where ˆS = span φ(Zn) 
and τ > 0. With this choice  almost sure convergence limn→∞ dH (M  ˆM ) = 0 in the Hausdorff
distance [1] is related to the convergence of ˆS to Sρ [9]. More precisely  if the eigenfunctions of the
covariance operator C = Ez∼ρ [φ(z) ⊗ φ(z)] are uniformly bounded  then it sufﬁces for Hausdorff
convergence to bound from above d r−1
2r  ∞ (where r > 1 is the eigenvalue decay rate of C). The
following results specializes Corollary 3.3 to this setting.
Corollary 4.2 (Performance of set learning). If 0 ≤ α ≤ 1

2   then it holds  with probability 1 − δ 

dα ∞(Sρ  ˆS∗

n) = O

(cid:18)(cid:18) log n − log δ

(cid:19)α(cid:19)

n

where the constant in the Landau symbol depends on δ.

Figure 1: The ﬁgure shows the experimental be-
havior of the distance dα ∞( ˆSk  Sρ) between the
empirical and the actual support subspaces  with
respect to the regularization parameter. The set-
ting is the one of section 5. Here the actual sub-
space is analytically computed  while the empiri-
cal one is computed on a dataset with n = 1000
and 32bit ﬂoating point precision. Note the nu-
merical instability as k tends to 1000.

(cid:17)

(cid:16)
(cid:17)

Letting α = r−1
factors)  which is considerably sharper than the bound O

2r above yields a high probability bound of order O
− r−1
2(3r−1)
n

(cid:16)

2r

(up to logarithmic

n− r−1
found in [8] (Theorem 7).

1A kernel is said to separate M if its associated feature map φ satisﬁes φ−1(span φ(M )) = M (e.g. the

Abel kernel is separating).

5

1001011021030.00.20.40.60.81.0kNote that these are upper bounds for the best possible choice of k (which minimizes the bound).
While the optima of both bounds vanish with n → ∞  their behavior is qualitatively different. In
particular  the bound of [8] is U-shaped  and diverges for k = n  while ours is L-shaped (no trade-
off)  and thus also convergent for k = n. Therefore  when compared with [8]  our results suggest
that no regularization is required from a statistical point of view though  as clariﬁed in the following
remark  it may be needed for purposes of numerical stability.
Remark 4.3. While  as proven in Corollary 4.2  regularization is not needed from a statistical
perspective  it can play a role in ensuring numerical stability in practice. Indeed  in order to ﬁnd
ˆM  we compute d ˆS(φ(z)) with z ∈ Z. Using the reproducing property of K  it can be shown that 
where (tz)i = K(z  zi)  ˆKn is the Gram
n)† is the pseudo-inverse
matrix ( ˆKn)ij = K(zi  zj)  ˆK k
of ˆK k
n. The computation of ˆM therefore requires a matrix inversion  which is prone to instability for
high condition numbers. Figure 1 shows the behavior of the error that results from replacing ˆS by
its k-truncated approximation ˆSk. For large values of k  the small eigenvalues of ˆS are used in the
inversion  leading to numerical instability.

for z ∈ Z  it is d ˆSk (φ(z)) = K(z  z) −(cid:68)

n is the rank-k approximation of ˆKn  and ( ˆK k

(cid:69)

tz  ( ˆK k

n)†tz

5 Experiments

Figure 2: The spectrum of the empirical covariance (left)  and the expected distance from a random
sample to the empirical k-truncated kernel-PCA subspace estimate (right)  as a function of k (n =
1000  1000 trials shown in a boxplot). Our predicted plateau threshold k∗
n (Theorem 3.2) is a good
estimate of the value k past which the distance stabilizes.

n (the expected distance in H of samples to ˆSk

In order to validate our analysis empirically  we consider the following experiment. Let ρ be a
uniform one-dimensional distribution in the unit interval. We embed ρ into a reproducing-kernel
Hilbert space H using the exponential of the (cid:96)1 distance (k(u  v) = exp{−(cid:107)u − v(cid:107)1}) as kernel.
Given n samples drawn from ρ  we compute its empirical covariance in H (whose spectrum is
plotted in Figure 2 (left))  and truncate its eigen-decomposition to obtain a subspace estimate ˆSk
n  as
described in Section 2.1.
Figure 2 (right) is a box plot of reconstruction error dR(Sρ  ˆSk
n) associated with the k-truncated
kernel-PCA estimate ˆSk
n)  with n = 1000 and varying
k. While dR is computed analytically in this example  and Sρ is ﬁxed  the estimate ˆSk
n is a random
variable  and hence the variability in the graph. Notice from the ﬁgure that  as pointed out in [6] and
discussed in Section 6  the reconstruction error dR(Sρ  ˆSk
n) is always a non-increasing function of k 
n ⊂ ˆSk(cid:48)
n for k < k(cid:48) (see Section 2.1). The
due to the fact that the kernel-PCA estimates are nested: ˆSk
graph is highly concentrated around a curve with a steep intial drop  until reaching some sufﬁciently
high k  past which the reconstruction (pseudo) distance becomes stable  and does not vanish. In our
experiments  this behavior is typical for the reconstruction distance and high-dimensional problems.
Due to the simple form of this example  we are able to compute analytically the spectrum of the
true covariance C. In this case  the eigenvalues of C decay as 2γ/((kπ)2 + γ2)  with k ∈ N  and
therefore they have a polynomial decay rate r = 2 (see Section 3). Given the known spectrum decay
rate  we can estimate the plateau threshold k = k∗
n in the bound of Theorem 3.2  which can be seen

6

to be a good approximation of the observed start of a plateau in dR(Sρ  ˆSk
n) (Figure 2  right). Notice
that our bound for this case (Corollary 4.1) similarly predicts a steep performance drop until the
threshold k = k∗

n (indicated in the ﬁgure by the vertical blue line)  and a plateau afterwards.

6 Discussion

Figure 3 shows a comparison of our learning rates with existing rates in the literature [6  19]. The
n) = O(n−c)  as a
plot shows the polynomial decay rate c of the high probability bound dR(Sρ  ˆSk
function of the eigenvalue decay rate r of the covariance C  computed at the best value k∗
n (which
minimizes the bound).

Figure 3: Known upper bounds for the polynomial decay rate c (for the best choice of k)  for
the expected distance from a random sample to the empirical k-truncated kernel-PCA estimate 
as a function of the covariance eigenvalue decay rate (higher is better). Our bound (purple line) 
consistently outperforms previous ones [19] (black line). The top (dashed) line [6]  has signiﬁcantly
stronger assumptions  and is only included for completeness.

r−s+sr for [6] and c = r−1

The learning rate exponent c  under a polynomial eigenvalue decay assumption of the data covari-
ance C  is c = s(r−1)
2r−1 for [19]  where s is related to the fourth moment. Note
that  among the two (purple and black) that operate under the same assumptions  our bound (purple
line) is the best by a wide margin. The top  best performing  dashed line [6] is obtained for the best
possible fourth-order moment constraint s = 2r  and is therefore not a fair comparison. However  it
is worth noting that our bounds perform almost as well as the most restrictive one  even when we do
not include any fourth-order moment constraints.
Choice of truncation parameter k. Since  as pointed out in Section 2.1  the subspace estimates ˆSk
n
n ⊆ ˆSk(cid:48)
are nested for increasing k (i.e. ˆSk
n)  and in particular
the reconstruction error dR(Sρ  ˆSk
n)  is a non-increasing function of k. As has been previously dis-
cussed [6]  this suggests that there is no tradeoff in the choice of k. Indeed  the fact that the estimates
ˆSk
n become increasing close to Sρ as k increases indicates that  when minimizing dα p(Sρ  ˆSk
n)  the
best choice is the highest: k = n.
Interestingly  however  both in practice (Section 5)  and in theory (Section 3)  we observe that a typ-
ical behavior for the subspace learning problem in high dimensions (e.g. kernel PCA) is that there is
a certain value of k = k∗
n  past which performance plateaus. For problems such as spectral embed-
ding methods [22  10  25]  in which a degree of dimensionality reduction is desirable  producing an
estimate ˆSk
n where k is close to the plateau threshold may be a natural parameter choice: it leads to
an estimate of the lowest dimension (k = k∗
n)  whose distance to the true Sρ is almost as low as the
best-performing one (k = n).

n for k < k(cid:48))  the distance dα p(Sρ  ˆSk

7

468100.20.40.60.8r7 Sketch of the proofs

Due to the novelty of the the techniques employed  and in order to clarify how they may be used
in other contexts  we provide here a proof of our main theoretical result  Theorem 3.1  with some
details omitted in the interest of conciseness.
For each λ > 0  we denote by rλ(x) := 1{x > λ} the step function with a cut-off at λ. Given
an empirical covariance operator Cn  we will consider the truncated version rλ(Cn) where  in this
notation  rλ is applied to the eigenvalues of Cn  that is  rλ(Cn) has the same eigen-structure as Cn 
but its eigenvalues that are less or equal to λ are clamped to zero.
In order to prove the bound of Equation (3)  we begin by proving a more general upper bound of
n)  which is split into a random (A)  and a deterministic part (B C). The bound holds for
dα p(Sρ  ˆSk
all values of a free parameter t > 0  which is then constrained and optimized in order to ﬁnd the
(close to) tightest version of the bound.
Lemma 7.1. Let t > 0  0 ≤ α ≤ 1/2  and λ = σk(C) be the k-th top eigenvalue of C  it is 
· (cid:107)C α(C + tI)−α(cid:107)p

· {3/2(λ + t)}α

2 (Cn + tI)− 1

n) ≤ (cid:107)(C + tI)

dα p(Sρ  ˆSk

(6)

1

(cid:125)
2(cid:107)2α∞

(cid:124)

(cid:123)(cid:122)

B

(cid:125)

(cid:124)

(cid:123)(cid:122)

C

(cid:125)

(cid:124)

(cid:123)(cid:122)

A

Note that the right-hand side of Equation (6) is the product of three terms  the left of which (A)
involves the empirical covariance operator Cn  which is a random variable  and the right two (B  C)
are entirely deterministic. While the term B has already been reduced to the known quantities t  α  λ 
the remaining terms are bound next. We bound the random term A in the next Lemma  whose proof
makes use of recent concentration results [23].
Lemma 7.2 (Term A). Let 0 ≤ α ≤ 1/2  for each 9
δ ≤ t ≤ (cid:107)C(cid:107)∞  with probability 1 − δ it
is

n log n

(2/3)α ≤ (cid:107)(C + tI)

1

2 (Cn + tI)− 1

2(cid:107)2α∞ ≤ 2α

Lemma 7.3 (Term C). Let C be a symmetric  bounded  positive semideﬁnite linear operator on H.
If σk(C) ≤ f (k) for k ∈ N  where f is a decreasing function then  for all t > 0 and α ≥ 0  it holds
(7)

(cid:13)(cid:13)C α(C + tI)−α(cid:13)(cid:13)p ≤ inf
1 f (x)uαpdx(cid:1)1/p. Furthermore  if f (k) = gk−1/γ  with 0 < γ < 1
(cid:13)(cid:13)C α(C + tI)−α(cid:13)(cid:13)p ≤ Qt−γ/p

where guα = (cid:0)f (1)uαp +(cid:82) ∞

and αp > γ  then it holds

guαt−uα

0≤u≤1

(8)

where Q = (gγΓ(αp − γ)Γ(1 + γ)/Γ(γ))1/p.

The combination of Lemmas 7.1 and 7.2 leads to the main theorem 3.1  which is a probabilistic
bound  holding for every k ∈ {1  . . .   n}  with a deterministic term (cid:107)C α(C + tI)−α(cid:107)p that depends
on knowledge of the covariance C. In cases in which some knowledge of the decay rate of C is
available  Lemma 7.3 can be applied to obtain Theorem 3.2 and Corollary 3.3. Finally  Corollary 4.1
is simply a particular case for the reconstruction error dR(Sρ ·) = dα p(Sρ ·)2  with α = 1/2  p =
2.
As noted in Section 3  looser bounds would be obtained if classical Bernstein inequalities in
Hilbert spaces [14] were used instead. In particular  Lemma 7.2 would result in a range for t of
qn−r/(r+1) ≤ t ≤ (cid:107)C(cid:107)∞  implying k∗ = O(n1/(r+1)) rather than O(n1/r)  and thus Theorem 3.2
would become (for k ≥ k∗) dα p(Sρ  Sk
n) = O(n−αr/(r+1)+1/(p(r+1))) (compared with the sharper
O(n−α+1/rp) of Theorem 3.2). For instance  for p = 2  α = 1/2  and a decay rate r = 2 (as
in the example of Section 5)  it would be: d1/2 2(Sρ  Sn) = O(n−1/4) using Theorem 3.2  and
d1/2 2(Sρ  Sn) = O(n−1/6) using classical Bernstein inequalities.
Acknowledgments L. R. acknowledges the ﬁnancial support of the Italian Ministry of Education 
University and Research FIRB project RBFR12M3AC.

8

References
[1] G. Beer. Topologies on Closed and Closed Convex Sets. Springer  1993.
[2] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.

Neural computation  15(6):1373–1396  2003.

[3] Y. Bengio  O. Delalleau  N.L. Roux  J.F. Paiement  P. Vincent  and M. Ouimet. Learning eigenfunctions

links spectral embedding and kernel pca. Neural Computation  16(10):2197–2219  2004.

[4] Y. Bengio  J.F. Paiement  and al. Out-of-sample extensions for lle  isomap  mds  eigenmaps  and spectral

clustering. Advances in neural information processing systems  16:177–184  2004.

[5] S. Bernstein. The Theory of Probabilities. Gastehizdat Publishing House  Moscow  1946.
[6] G. Blanchard  O. Bousquet  and L. Zwald. Statistical properties of kernel principal component analysis.

Machine Learning  66(2):259–294  2007.

[7] I. Borg and P.J.F. Groenen. Modern multidimensional scaling: Theory and applications. Springer  2005.
[8] Ernesto De Vito  Lorenzo Rosasco  and al. Learning sets with separating kernels. arXiv:1204.3573  2012.
[9] Ernesto De Vito  Lorenzo Rosasco  and Alessandro Toigo. Spectral regularization for support estimation.

Advances in Neural Information Processing Systems  NIPS Foundation  pages 1–9  2010.

[10] D.L. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-

dimensional data. Proceedings of the National Academy of Sciences  100(10):5591–5596  2003.

[11] J. Ham  D.D. Lee  S. Mika  and B. Sch¨olkopf. A kernel view of the dimensionality reduction of manifolds.

In Proceedings of the twenty-ﬁrst international conference on Machine learning  page 47. ACM  2004.

[12] I. Jolliffe. Principal component analysis. Wiley Online Library  2005.
[13] Andreas Maurer and Massimiliano Pontil. K–dimensional coding schemes in hilbert spaces. IEEE Trans-

actions on Information Theory  56(11):5839–5846  2010.

[14] Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of

Probability  pages 1679–1706  1994.

[15] J.R. Retherford. Hilbert Space: Compact Operators and the Trace Theorem. London Mathematical

Society Student Texts. Cambridge University Press  1993.

[16] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science 

290(5500):2323–2326  2000.

[17] L.K. Saul and S.T. Roweis. Think globally  ﬁt locally: unsupervised learning of low dimensional mani-

folds. The Journal of Machine Learning Research  4:119–155  2003.

[18] B. Sch¨olkopf  A. Smola  and K.R. M¨uller. Kernel principal component analysis. Artiﬁcial Neural

Networks-ICANN’97  pages 583–588  1997.

[19] J. Shawe-Taylor  C. K. Williams  N. Cristianini  and J. Kandola. On the eigenspectrum of the gram matrix

and the generalization error of kernel-pca. Information Theory  IEEE Transactions on  51(7)  2005.

[20] I. Steinwart and A. Christmann. Support vector machines. Information science and statistics. Springer-

Verlag. New York  2008.

[21] J. Sun  S. Boyd  L. Xiao  and P. Diaconis. The fastest mixing markov process on a graph and a connection

to a maximum variance unfolding problem. SIAM review  48(4):681–699  2006.

[22] J.B. Tenenbaum  V. De Silva  and J.C. Langford. A global geometric framework for nonlinear dimension-

ality reduction. Science  290(5500):2319–2323  2000.

[23] J.A. Tropp. User-friendly tools for random matrices: An introduction. 2012.
[24] K.Q. Weinberger and L.K. Saul. Unsupervised learning of image manifolds by semideﬁnite programming.

In Computer Vision and Pattern Recognition  2004. CVPR 2004.  volume 2  pages II–988. IEEE  2004.

[25] K.Q. Weinberger and L.K. Saul. Unsupervised learning of image manifolds by semideﬁnite programming.

International Journal of Computer Vision  70(1):77–90  2006.

[26] C.K.I. Williams. On a connection between kernel pca and metric multidimensional scaling. Machine

Learning  46(1):11–19  2002.

9

,Alessandro Rudi
Guillermo Canas
Lorenzo Rosasco
Jiezhang Cao
Langyuan Mo
Yifan Zhang
Kui Jia
Chunhua Shen
Mingkui Tan