2018,Discrimination-aware Channel Pruning for Deep Neural Networks,Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either train from scratch with sparsity constraints on channels  or  minimize the reconstruction error between the pre-trained feature maps and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge  whilst the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. To overcome these drawbacks  we investigate a simple-yet-effective method  called discrimination-aware channel pruning  to choose those channels that really contribute to discriminative power. To this end  we introduce additional losses into the network to increase the discriminative power of intermediate layers and then select the most discriminative channels for each layer by considering the additional loss and the reconstruction error. Last  we propose a greedy algorithm to conduct channel selection and parameter optimization in an iterative way. Extensive experiments demonstrate the effectiveness of our method. For example  on ILSVRC-12  our pruned ResNet-50 with 30% reduction of channels even outperforms the original model by 0.39% in top-1 accuracy.,Discrimination-aware Channel Pruning

for Deep Neural Networks

Zhuangwei Zhuang1∗  Mingkui Tan1∗†  Bohan Zhuang2∗  Jing Liu1∗ 

Yong Guo1  Qingyao Wu1  Junzhou Huang3 4  Jinhui Zhu1†

{z.zhuangwei  seliujing  guo.yong}@mail.scut.edu.cn  jzhuang@uta.edu
{mingkuitan  qyw  csjhzhu}@scut.edu.cn  bohan.zhuang@adelaide.edu.au

1South China University of Technology  2The University of Adelaide 

3University of Texas at Arlington  4Tencent AI Lab

Abstract

Channel pruning is one of the predominant approaches for deep model compression.
Existing pruning methods either train from scratch with sparsity constraints on
channels  or minimize the reconstruction error between the pre-trained feature
maps and the compressed ones. Both strategies suffer from some limitations:
the former kind is computationally expensive and difﬁcult to converge  whilst
the latter kind optimizes the reconstruction error but ignores the discriminative
power of channels. In this paper  we investigate a simple-yet-effective method
called discrimination-aware channel pruning (DCP) to choose those channels that
really contribute to discriminative power. To this end  we introduce additional
discrimination-aware losses into the network to increase the discriminative power of
intermediate layers and then select the most discriminative channels for each layer
by considering the additional loss and the reconstruction error. Last  we propose
a greedy algorithm to conduct channel selection and parameter optimization in
an iterative way. Extensive experiments demonstrate the effectiveness of our
method. For example  on ILSVRC-12  our pruned ResNet-50 with 30% reduction
of channels outperforms the baseline model by 0.39% in top-1 accuracy.

1

Introduction

Since 2012  convolutional neural networks (CNNs) have achieved great success in many computer
vision tasks  e.g.  image classiﬁcation [21  41]  face recognition [37  42]  object detection [35 
36]  image generation [7  3] and video analysis [38  47]. However  deep models are often with a
huge number of parameters and the model size is very large  which incurs not only huge memory
requirement but also unbearable computation burden. As a result  deep learning methods are hard to
be applied on hardware devices with limited storage and computation resources  such as cell phones.
To address this problem  model compression is an effective approach  which aims to reduce the model
redundancy without signiﬁcant degeneration in performance.
Recent studies on model compression mainly contain three categories  namely  quantization [34  54] 
sparse or low-rank compressions [10  11]  and channel pruning [27  28  51  49]. Network quantization
seeks to reduce the model size by quantizing ﬂoat weights into low-bit weights (e.g.  8 bits or
even 1 bit). However  the training is very difﬁcult due to the introduction of quantization errors.
Making sparse connections can reach high compression rate in theory  but it may generate irregular
convolutional kernels which need sparse matrix operations for accelerating the computation. In

∗Authors contributed equally.
†Corresponding author.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

contrast  channel pruning reduces the model size and speeds up the inference by removing redundant
channels directly  thus little additional effort is required for fast inference. On top of channel pruning 
other compression methods such as quantization can be applied. In fact  pruning redundant channels
often helps to improve the efﬁciency of quantization and achieve more compact models.
Identifying the informative (or important) channels  also known as channel selection  is a key issue
in channel pruning. Existing works have exploited two strategies  namely  training-from-scratch
methods which directly learn the importance of channels with sparsity regularization [1  27  48] 
and reconstruction-based methods [14  16  24  28]. Training-from-scratch is very difﬁcult to train
especially for very deep networks on large-scale datasets. Reconstruction-based methods seek to
do channel pruning by minimizing the reconstruction error of feature maps between the pruned
model and a pre-trained model [14  28]. These methods suffer from a critical limitation: an actually
redundant channel would be mistakenly kept to minimize the reconstruction error of feature maps.
Consequently  these methods may result in apparent drop in accuracy on more compact and deeper
models such as ResNet [13] for large-scale datasets.
In this paper  we aim to overcome the drawbacks of both strategies. First  in contrast to existing
methods [14  16  24  28]  we assume and highlight that an informative channel  no matter where
it is  should own discriminative power; otherwise it should be deleted. Based on this intuition  we
propose to ﬁnd the channels with true discriminative power for the network. Speciﬁcally  relying on a
pre-trained model  we add multiple additional losses (i.e.  discrimination-aware losses) evenly to the
network. For each stage  we ﬁrst do ﬁne-tuning using one additional loss and the ﬁnal loss to improve
the discriminative power of intermediate layers. And then  we conduct channel pruning for each layer
involved in the considered stage by considering both the additional loss and the reconstruction error
of feature maps. In this way  we are able to make a balance between the discriminative power of
channels and the feature map reconstruction.
Our main contributions are summarized as follows. First  we propose a discrimination-aware channel
pruning (DCP) scheme for compressing deep models with the introduction of additional losses. DCP
is able to ﬁnd the channels with true discriminative power. DCP prunes and updates the model
stage-wisely using a proper discrimination-aware loss and the ﬁnal loss. As a result  it is not sensitive
to the initial pre-trained model. Second  we formulate the channel selection problem as an (cid:96)2 0-norm
constrained optimization problem and propose a greedy method to solve the resultant optimization
problem. Extensive experiments demonstrate the superior performance of our method  especially
on deep ResNet. On ILSVRC-12 [4]  when pruning 30% channels of ResNet-50  DCP improves
the original ResNet model by 0.39% in top-1 accuracy. Moreover  when pruning 50% channels of
ResNet-50  DCP outperforms ThiNet [28]  a state-of-the-art method  by 0.81% and 0.51% in top-1
and top-5 accuracy  respectively.

2 Related studies

Network quantization.
In [34]  Rastegari et al. propose to quantize parameters in the network into
+1/− 1. The proposed BWN and XNOR-Net can achieve comparable accuracy to their full-precision
counterparts on large-scale datasets. In [55]  high precision weights  activations and gradients in
CNNs are quantized to low bit-width version  which brings great beneﬁts for reducing resource
requirement and power consumption in hardware devices. By introducing zero as the third quantized
value  ternary weight networks (TWNs) [23  56] can achieve higher accuracy than binary neural
networks. Explorations on quantization [54  57] show that quantized networks can even outperform
the full precision networks when quantized to the values with more bits  e.g.  4 or 5 bits.

Sparse or low-rank connections. To reduce the storage requirements of neural networks  Han et
al. suggest that neurons with zero input or output connections can be safely removed from the
network [12]. With the help of the (cid:96)1/(cid:96)2 regularization  weights are pushed to zeros during training.
Subsequently  the compression rate of AlexNet can reach 35× with the combination of pruning 
quantization  and Huffman coding [11]. Considering the importance of parameters is changed during
weight pruning  Guo et al. propose dynamic network surgery (DNS) in [10]. Training with sparsity
constraints [40  48] has also been studied to reach higher compression rate.
Deep models often contain a lot of correlations among channels. To remove such redundancy 
low-rank approximation approaches have been widely studied [5  6  19  39]. For example  Zhang et

2

Figure 1: Illustration of discrimination-aware channel pruning. Here  Lp
S denotes the discrimination-
aware loss (e.g.  cross-entropy loss) in the Lp-th layer  LM denotes the reconstruction loss  and Lf
denotes the ﬁnal loss. For the p-th stage  we ﬁrst ﬁne-tune the pruned model by Lp
S and Lf   then
S and LM .
conduct the channel selection for each layer in {Lp−1 + 1  . . .   Lp} with Lp

al. speed up VGG for 4× with negligible performance degradation on ImageNet [53]. However 
low-rank approximation approaches are unable to remove those redundant channels that do not
contribute to the discriminative power of the network.

Channel pruning. Compared with network quantization and sparse connections  channel pruning
removes both channels and the related ﬁlters from the network. Therefore  it can be well supported
by existing deep learning libraries with little additional effort. The key issue of channel pruning is to
evaluate the importance of channels. Li et al. measure the importance of channels by calculating
the sum of absolute values of weights [24]. Hu et al. deﬁne average percentage of zeros (APoZ)
to measure the activation of neurons [16]. Neurons with higher values of APoZ are considered
more redundant in the network. With a sparsity regularizer in the objective function  training-
based methods [1  27] are proposed to learn the compact models in the training phase. With the
consideration of efﬁciency  reconstruction-methods [14  28] transform the channel selection problem
into the optimization of reconstruction error and solve it by a greedy algorithm or LASSO regression.

3 Proposed method
Let {xi  yi}N
i=1 be the training samples  where N indicates the number of samples. Given an L-layer
CNN model M  let W ∈ Rn×c×hf×zf be the model parameters w.r.t. the l-th convolutional layer (or
block)  as shown in Figure 1. Here  hf and zf denote the height and width of ﬁlters  respectively; c
and n denote the number of input and output channels  respectively. For convenience  hereafter we
omit the layer index l. Let X ∈ RN×c×hin×zin and O ∈ RN×n×hout×zout be the input feature maps
and the involved output feature maps  respectively. Here  hin and zin denote the height and width
of the input feature maps  respectively; hout and zout represent the height and width of the output
feature maps  respectively. Moreover  let Xi k : : be the feature map of the k-th channel for the i-th
sample. Wj k : : denotes the parameters w.r.t. the k-th input channel and j-th output channel. The
output feature map of the j-th channel for the i-th sample  denoted by Oi j : :  is computed by

Oi j : : =(cid:80)c

k=1 Xi k : : ∗ Wj k : : 

(1)

(2)

where ∗ denotes the convolutional operation.
Given a pre-trained model M  the task of Channel Pruning is to prune those redundant channels in
W to save the model size and accelerate the inference speed in Eq. (1). In order to choose channels 
j=1 ||Wj k : :||F )  where Ω(a) = 1 if
a (cid:54)= 0 and Ω(a) = 0 if a = 0  and || · ||F represents the Frobenius norm. To induce sparsity  we can
impose an (cid:96)2 0-norm constraint on W:

we introduce a variant of (cid:96)2 0-norm ||W||2 0 =(cid:80)c
k=1 Ω((cid:80)n

||W||2 0 =(cid:80)c

k=1 Ω((cid:80)n

j=1 ||Wj k : :||F ) ≤ κl 

where κl denotes the desired number of channels at the layer l. Or equivalently  given a predeﬁned
pruning rate η ∈ (0  1) [1  27]  it follows that κl = (cid:100)ηc(cid:101).

3

ConvConvPruned NetworkSoftmaxℒ𝑆𝑝𝐎𝑝BatchNormReLUAvgPooling𝐅𝑝ConvConvConvBaseline Network𝐎𝒃𝐗𝒃𝐖𝒃ConvReconstruction errorℒ𝑀𝐎𝐗SoftmaxAvgPoolingConvSoftmaxAvgPoolingConvConvConvℒ𝑓Fine-tuningChannel Selection𝐖3.1 Motivations

(cid:80)N

(cid:80)n
j=1 ||Ob

Given a pre-trained model M  existing methods [14  28] conduct channel pruning by minimizing the
reconstruction error of feature maps between the pre-trained model M and the pruned one. Formally 
the reconstruction error can be measured by the mean squared error (MSE) between feature maps of
the baseline network and the pruned one as follows:

i=1

i j : : − Oi j : :||2
F  

LM (W) = 1
(3)
2Q
where Q = N · n · hout · zout and Ob
i j : : denotes the feature maps of the baseline network. Recon-
structing feature maps can preserve most information in the learned model  but it has two limitations.
First  the pruning performance is highly affected by the quality of the pre-trained model M. If the
baseline model is not well trained  the pruning performance can be very limited. Second  to achieve
the minimal reconstruction error  some channels in intermediate layers may be mistakenly kept  even
though they are actually not relevant to the discriminative power of the network. This issue will be
even severer when the network becomes deeper.
In this paper  we seek to do channel pruning by keeping those channels that really contribute to the
discriminative power of the network. In practice  however  it is very hard to measure the discriminative
power of channels due to the complex operations (such as ReLU activation and Batch Normalization)
in CNNs. One may consider one channel as an important one if the ﬁnal loss Lf would sharply
increase without it. However  it is not practical when the network is very deep. In fact  for deep
models  its shallow layers often have little discriminative power due to the long path of propagation.
To increase the discriminative power of intermediate layers  one can introduce additional losses to the
intermediate layers of the deep networks [43  22  8]. In this paper  we insert P discrimination-aware
losses {Lp
p=1 evenly into the network  as shown in Figure 1. Let {L1  ...  LP   LP +1} be the layers
at which we put the losses  with LP +1 = L being the ﬁnal layer. For the p-th loss Lp
S  we consider
doing channel pruning for layers l ∈ {Lp−1 + 1  ...  Lp}  where Lp−1 = 0 if p = 1. It is worth
mentioning that  we can add one loss to each layer of the network  where we have Ll = l. However 
this can be very computationally expensive yet not necessary.

S}P

3.2 Construction of discrimination-aware loss
The construction of discrimination-aware loss Lp
S is very important in our method. As shown in
Figure 1  each loss uses the output of layer Lp as the input feature maps. To make the computation
of the loss feasible  we impose an average pooling operation over the feature maps. Moreover  to
accelerate the convergence  we shall apply batch normalization [18  9] and ReLU [29] before doing
the average pooling. In this way  the input feature maps for the loss at layer Lp  denoted by Fp(W) 
can be computed by

(4)
where Op represents the output feature maps of layer Lp. Let F(p i) be the feature maps w.r.t. the
i-th example. The discrimination-aware loss w.r.t. the p-th loss is formulated as

Fp(W) = AvgPooling(ReLU(BN(Op))) 

Lp
S(W) = − 1

N

i=1

(5)
where I{·} is the indicator function  θ ∈ Rnp×m denotes the classiﬁer weights of the fully connected
layer  np denotes the number of input channels of the fully connected layer and m is the number of
classes. Note that we can use other losses such as angular softmax loss [26] as the additional loss.
In practice  since a pre-trained model contains very rich information about the learning task  similar
to [28]  we also hope to reconstruct the feature maps in the pre-trained model. By considering both
cross-entropy loss and reconstruction error  we have a joint loss function as follows:

F(p i)

 

(cid:80)m
eθ(cid:62)
t F(p i)
k=1 eθ(cid:62)

k

L(W) = LM (W) + λLp

S(W) 

(6)

(cid:20)(cid:80)N

(cid:80)m
t=1 I{y(i) = t} log

(cid:21)

where λ balances the two terms.
Proposition 1 (Convexity of the loss function) Let W be the model parameters of a considered
layer. Given the mean square loss and the cross-entropy loss deﬁned in Eqs. (3) and (5)  then the
joint loss function L(W) is convex w.r.t. W.3

3The proof can be found in Section S1 in the supplementary material.

4

Last  the optimization problem for discrimination-aware channel pruning can be formulated as

(7)
where κl < c is the number channels to be selected. In our method  the sparsity of W can be either
determined by a pre-deﬁned pruning rate (See Section 3) or automatically adjusted by the stopping
conditions in Section 3.5. We explore both effects in Section 4.

minW L(W) 

s.t. ||W||2 0 ≤ κl 

S}P

3.3 Discrimination-aware channel pruning
By introducing P losses {Lp
p=1 to intermediate layers  the proposed discrimination-aware channel
pruning (DCP) method is shown in Algorithm 1. Starting from a pre-trained model  DCP updates the
model M and performs channel pruning with (P + 1) stages. Algorithm 1 is called discrimination-
aware in the sense that an additional loss and the ﬁnal loss are considered to ﬁne-tune the model.
Moreover  the additional loss will be used to select channels  as discussed below. In contrast to
GoogLeNet [43] and DSN [22]  in Algorithm 1  we do not use all the losses at the same time. In fact 
at each stage we will consider two losses only  i.e.  Lp

S and the ﬁnal loss Lf .

Algorithm 1 Discrimination-aware channel prun-
ing (DCP)
Input: Pre-trained model M 
{xi  yi}N
i=1  and parameters {κl}L
for p ∈ {1  ...  P + 1} do

training data
l=1.

Construct loss Lp
Learn θ and Fine-tune M with Lp
for l ∈ {Lp−1 + 1  ...  Lp} do

S to layer Lp as in Figure 1.
S and Lf .
Do Channel Selection for layer l using Al-
gorithm 2.

end for

end for

Algorithm 2 Greedy algorithm for channel selection

Input: Training data  model M  parameters κl  and .
Output: Selected channel subset A and model parame-
ters WA.
Initialize A ← ∅  and t = 0.
while (stopping conditions are not achieved) do
Compute gradients of L w.r.t. W: G = ∂L/∂W.
Find the channel k = arg maxj /∈A{||Gj||F}.
Let A ← A ∪ {k}.
Solve Problem (8) to update WA.
Let t ← t + 1.

end while

At each stage of Algorithm 1  for example  in the p-th stage  we ﬁrst construct the additional loss Lp
and put them at layer Lp (See Figure 1). After that  we learn the model parameters θ w.r.t. Lp
S and
ﬁne-tune the model M at the same time with both the additional loss Lp
S and the ﬁnal loss Lf . In
the ﬁne-tuning  all the parameters in M will be updated.4 Here  with the ﬁne-tuning  the parameters
regarding the additional loss can be well learned. Besides  ﬁne-tuning is essential to compensate the
accuracy loss from the previous pruning to suppress the accumulative error. After ﬁne-tuning with
Lp
S and Lf   the discriminative power of layers l ∈ {Lp−1 + 1  ...  Lp} can be signiﬁcantly improved.
Then  we can perform channel selection for the layers in {Lp−1 + 1  ...  Lp}.

S

3.4 Greedy algorithm for channel selection

Due to the (cid:96)2 0-norm constraint  directly optimizing Problem (7) is very difﬁcult. To address this
issue  following general greedy methods in [25  2  52  45  46]  we propose a greedy algorithm to solve
Problem (7). To be speciﬁc  we ﬁrst remove all the channels and then select those channels that really
contribute to the discriminative power of the deep networks. Let A ⊂ {1  . . .   c} be the index set of
the selected channels  where A is empty at the beginning. As shown in Algorithm 2  the channel
selection method can be implemented in two steps. First  we select the most important channels of
input feature maps. At each iteration  we compute the gradients Gj = ∂L/∂Wj  where Wj denotes
the parameters for the j-th input channel. We choose the channel k = arg maxj /∈A{||Gj||F} as an
active channel and put k into A. Second  once A is determined we optimize W w.r.t. the selected
channels by minimizing the following problem:

(8)
where WAc denotes the submatrix indexed by Ac which is the complementary set of A. Here  we
apply stochastic gradient descent (SGD) to address the problem in Eq. (8)  and update WA by

minW L(W)  s.t. WAc = 0 

WA ← WA − γ ∂L
∂WA  

(9)

4The details of ﬁne-tuning algorithm is put in Section S2 in the supplementary material.

5

where WA denotes the submatrix indexed by A  and γ denotes the learning rate.
Note that when optimizing Problem (8)  WA is warm-started from the ﬁne-tuned model M. As a
result  the optimization can be completed very quickly. Moreover  since we only consider the model
parameter W for one layer  we do not need to consider all data to do the optimization. To make a
trade-off between the efﬁciency and performance  we sample a subset of images randomly from the
training data for optimization.5 Last  since we use SGD to update WA  the learning rate γ should be
carefully adjusted to achieve an accurate solution. Then  the following stopping conditions can be
applied  which will help to determine the number of channels to be selected.

3.5 Stopping conditions
Given a predeﬁned parameter κl in problem (7)  Algorithm 2 will be stopped if ||W||2 0>κl. However 
in practice  the parameter κl is hard to be determined. Since L is convex  L(Wt) will monotonically
decrease with iteration index t in Algorithm 2. We can therefore adopt the following stopping
condition:

(10)
where  is a tolerance value. If the above condition is achieved  the algorithm is stopped  and the
number of selected channels will be automatically determined  i.e.  ||Wt||2 0. An empirical study
over the tolerance value  is put in Section 5.3.

|L(Wt−1) − L(Wt)|/L(W0) ≤  

4 Experiments

In this section  we empirically evaluate the performance of DCP. Several state-of-the-art methods
are adopted as the baselines  including ThiNet [28]  Channel pruning (CP) [14] and Slimming [27].
Besides  to investigate the effectiveness of the proposed method  we include the following methods
for study: DCP: DCP with a pre-deﬁned pruning rate η. DCP-Adapt: We prune each layer with
the stopping conditions in Section 3.5. WM: We shrink the width of a network by a ﬁxed ratio and
train it from scratch  which is known as width-multiplier [15]. WM+: Based on WM  we evenly
insert additional losses to the network and train it from scratch. Random DCP: Relying on DCP  we
randomly choose channels instead of using gradient-based strategy in Algorithm 2.
Datasets. We evaluate the performance of various methods on three datasets  including CIFAR-
10 [20]  ILSVRC-12 [4]  and LFW [17]. CIFAR-10 consists of 50k training samples and 10k testing
images with 10 classes. ILSVRC-12 contains 1.28 million training samples and 50k testing images
for 1000 classes. LFW [17] contains 13 233 face images from 5 749 identities.

4.1

Implementation details

We implement the proposed method on PyTorch [32]. Based on the pre-trained model  we apply our
method to select the informative channels. In practice  we decide the number of additional losses
according to the depth of the network (See Section S4 in the supplementary material). Speciﬁcally 
we insert 3 losses to ResNet-50 and ResNet-56  and 2 additional losses to VGGNet and ResNet-18.
We ﬁne-tune the whole network with selected channels only. We use SGD with nesterov [30] for the
optimization. The momentum and weight decay are set to 0.9 and 0.0001  respectively. We set λ to
1.0 in our experiments by default. On CIFAR-10  we ﬁne-tune 400 epochs using a mini-batch size of
128. The learning rate is initialized to 0.1 and divided by 10 at epoch 160 and 240. On ILSVRC-12 
we ﬁne-tune the network for 60 epochs with a mini-batch size of 256. The learning rate is started at
0.01 and divided by 10 at epoch 36  48 and 54  respectively. The source code of our method can be
found at https://github.com/SCUT-AILab/DCP.

4.2 Comparisons on CIFAR-10

We ﬁrst prune ResNet-56 and VGGNet on CIFAR-10. The comparisons with several state-of-the-art
methods are reported in Table 1. From the results  our method achieves the best performance under
the same acceleration rate compared with the previous state-of-the-art. Moreover  with DCP-Adapt 
our pruned VGGNet outperforms the pre-trained model by 0.58% in testing error  and obtains 15.58×

5We study the effect of the number of samples in Section S5 in the supplementary material.

6

Table 1: Comparisons on CIFAR-10. "-" denotes that the results are not reported.

Model

VGGNet

(Baseline 6.01%)

ResNet-56

(Baseline 6.20%)

#Param. ↓
#FLOPs ↓
Err. gap (%)
#Param. ↓
#FLOPs ↓
Err. gap (%)

Sliming

[27]

CP
[14]

ThiNet
[28]
1.92× 1.92× 8.71× 1.92× 1.92×
2.00× 2.00× 2.04× 2.00× 2.00×
+0.38
+0.14
+0.11
1.97× 1.97×
1.97×
1.99× 1.99×
1.99×
+0.82
+0.56
+0.45

WM WM+ Random
DCP
1.92×
2.00×
+0.14
1.97×
1.99×
+0.63

+0.32
-
2×
+1.0

+0.19

-
-
-

DCP
1.92×
2.00×
-0.17
1.97×
1.99×
+0.31

DCP-Adapt
15.58×
2.86×
-0.58
3.37×
1.89×
-0.01

reduction in model size. Compared with random DCP  our proposed DCP reduces the performance
degradation of VGGNet by 0.31%  which implies the effectiveness of the proposed channel selection
strategy. Besides  we also observe that the inserted additional losses can bring performance gain
to the networks. With additional losses  WM+ of VGGNet outperforms WM by 0.27% in testing
error. Nevertheless  our method shows much better performance than WM+. For example  our pruned
VGGNet with DCP-Adapt outperforms WM+ by 0.69% in testing error.
Pruning MobileNet v1 and MobileNet v2 on CIFAR-10. We apply DCP to prune recently devel-
oped compact architectures  e.g.  MobileNet v1 and MobileNet v2   and evaluate the performance on
CIFAR-10. We report the results in Table 2. With additional losses  WM+ of MobileNet outperforms
WM by 0.26% in testing error. However  our pruned models achieve 0.41% improvement over
MobileNet v1 and 0.22% improvement over MobileNet v2 in testing error. Note that the Random
DCP incurs performance degradation on both MobileNet v1 and MobileNet v2 by 0.30% and 0.57% 
respectively.

Table 2: Performance of pruning 30% channels of MobileNet v1 and MobileNet v2 on CIFAR-10.

Model

MobileNet v1

(Baseline 6.04%)

MobileNet v2

(Baseline 5.53%)

#Param. ↓
#FLOPs ↓
Err. gap (%)
#Param. ↓
#FLOPs ↓
Err. gap (%)

WM WM+ Random
DCP
1.43× 1.43×
1.43×
1.75× 1.75×
1.75×
+0.30
+0.48
+0.22
1.31×
1.31× 1.31×
1.36×
1.36× 1.36×
+0.45
+0.40
+0.57

DCP
1.43×
1.75×
-0.41
1.31×
1.36×
-0.22

4.3 Comparisons on ILSVRC-12

To verify the effectiveness of the proposed method on large-scale datasets  we further apply our
method on ResNet-50 to achieve 2× acceleration on ILSVRC-12. We report the single view evaluation
in Table 3. Our method outperforms ThiNet [28] by 0.81% and 0.51% in top-1 and top-5 error 
respectively. Compared with channel pruning [14]  our pruned model achieves 0.79% improvement
in top-5 error. Compared with WM+  which leads to 2.41% increase in top-1 error  our method only
results in 1.06% degradation in top-1 error.

Table 3: Comparisons on ILSVRC-12. The top-1 and top-5 error (%) of the pre-trained model are
23.99 and 7.07  respectively. "-" denotes that the results are not reported.

ResNet-50

Model

#Param. ↓
#FLOPs ↓
Top-1 gap (%)
Top-5 gap (%)

4.4 Experiments on LFW

2.06×
2.25×
+1.87
+1.12

ThiNet [28] CP [14] WM WM+

DCP
2.06× 2.06× 2.06×
2.25× 2.25× 2.25×
+1.06
+2.81
+0.61
+1.62

+2.41
+1.28

-
2×
-

+1.40

We further conduct experiments on LFW [17]  which is a standard benchmark dataset for face
recognition. We use CASIA-WebFace [50] (which consists of 494 414 face images from 10 575
individuals) for training. With the same settings in [26]  we ﬁrst train SphereNet-4 (which contains
4 convolutional layers) from scratch. And Then  we adopt our method to compress the pre-trained
SphereNet model. Since the fully connected layer occupies 87.65% parameters of the model  we also
prune the fully connected layer to reduce the model size.

7

Table 4: Comparisons of prediction accuracy  #Param. and #FLOPs on LFW. We report the ten-fold
cross validation accuracy of different models.

Method

FaceNet [37] DeepFace [44] VGG [31]

SphereNet-4 [26]

#Param.
#FLOPs

LFW acc. (%)

140M
1.6B
99.63

120M
19.3B
97.35

133M
11.3B
99.13

12.56M
164.61M

98.20

DCP

(prune 50%)

DCP

(prune 65%)

5.89M
45.15M
98.30

4.06M
24.16M
98.02

We report the results in Table 4. With the pruning rate of 50%  our method speeds up SphereNet-4 for
3.66× with 0.1% improvement in ten-fold validation accuracy. Compared with huge networks  e.g. 
FaceNet [37]  DeepFace [44]  and VGG [31]  our pruned model achieves comparable performance
but has only 45.15M FLOPs and 5.89M parameters  which is sufﬁcient to be deployed on embedded
systems. Furthermore  pruning 65% channels in SphereNet-4 results in a more compact model  which
requires only 24.16M FLOPs with the accuracy of 98.02% on LFW.

5 Ablation studies

5.1 Performance with different pruning rates

To study the effect of using different pruning rates η  we prune 30%  50%  and 70% channels of
ResNet-18 and ResNet-50  and evaluate the pruned models on ILSVRC-12. Experimental results
are shown in Table 5. Here  we only report the performance under different pruning rates  while the
detailed model complexity comparisons are provided in Section S8 in the supplementary material.
From Table 5  in general  performance of the pruned models goes worse with the increase of pruning
rate. However  our pruned ResNet-50 with pruning rate of 30% outperforms the pre-trained model 
with 0.39% and 0.14% reduction in top-1 and top-5 error  respectively. Besides  the performance
degradation of ResNet-50 is smaller than that of ResNet-18 with the same pruning rate. For example 
when pruning 50% of the channels  while it only leads to 1.06% increase in top-1 error for ResNet-50 
it results in 2.29% increase of top-1 error for ResNet-18. One possible reason is that  compared to
ResNet-18  ResNet-50 is more redundant with more parameters  thus it is easier to be pruned.

Table 5: Comparisons on ResNet-18 and ResNet-
50 with different pruning rates. We report the top-1
and top-5 error (%) on ILSVRC-12.

Network

η

Top-1/Top5 err.

ResNet-18

ResNet-50

0% (baseline)

0% (baseline)

30%
50%
70%

30%
50%
70%

30.36/11.02
30.79/11.14
32.65/12.40
35.88/14.32
23.99/7.07
23.60/6.93
25.05/7.68
27.25/8.87

Table 6: Pruning results on ResNet-56 with dif-
ferent λ on CIFAR-10.

λ
0 (LM only)
0.001
0.005
0.01
0.05
0.1
0.5
1.0
1.0 (LS only)

Training err. Testing err.

7.96
7.61
6.86
6.36
4.18
3.43
2.17
2.10
2.82

12.24
11.89
11.24
11.00
9.74
8.87
8.11
7.84
8.28

5.2 Effect of the trade-off parameter λ

We prune 30% channels of ResNet-56 on CIFAR-10 with different λ. We report the training error
and testing error without ﬁne-tuning in Table 6. From the table  the performance of the pruned model
improves with increasing λ. Here  a larger λ implies that we put more emphasis on the additional
loss (See Equation (6)). This demonstrates the effectiveness of discrimination-aware strategy for
channel selection. It is worth mentioning that both the reconstruction error and the cross-entropy
loss contribute to better performance of the pruned model  which strongly supports the motivation to
select the important channels by LS and LM . After all  as the network achieves the best result when
λ is set to 1.0  we use this value to initialize λ in our experiments.

8

5.3 Effect of the stopping condition

To explore the effect of stopping condition discussed in Section 3.5  we test different tolerance value
 in the condition. Here  we prune VGGNet on CIFAR-10 with  ∈ {0.1  0.01  0.001}. Experimental
results are shown in Table 7. In general  a smaller  will lead to more rigorous stopping condition and
hence more channels will be selected. As a result  the performance of the pruned model is improved
with the decrease of . This experiment demonstrates the usefulness and effectiveness of the stopping
condition for automatically determining the pruning rate.

Table 7: Effect of  for channel selection. We prune VGGNet and report the testing error on CIFAR-10.
The testing error of baseline VGGNet is 6.01%.

Loss
L


0.1
0.01
0.001

Testing err. (%)

12.68
6.63
5.43

#Param. ↓
152.25×
31.28×
15.58×

#FLOPs ↓
27.39×
5.35×
2.86×

5.4 Visualization of feature maps

We visualize the feature maps w.r.t. the pruned/selected channels of the ﬁrst block (i.e.  res-2a) in
ResNet-18 in Figure 2. From the results  we observe that feature maps of the pruned channels (See
Figure 2(b)) are less informative compared to those of the selected ones (See Figure 2(c)). It proves
that the proposed DCP selects the channels with strong discriminative power for the network. More
visualization results can be found in Section S10 in the supplementary material.

Figure 2: Visualization of the feature maps of the pruned/selected channels of res-2a in ResNet-18.

6 Conclusion

In this paper  we have proposed a discrimination-aware channel pruning method for the compression
of deep neural networks. We formulate the channel pruning/selection problem as a sparsity-induced
optimization problem by considering both reconstruction error and channel discrimination power.
Moreover  we propose a greedy algorithm to solve the optimization problem. Experimental results
on benchmark datasets show that the proposed method outperforms several state-of-the-art methods
by a large margin with the same pruning rate. Our DCP method provides an effective way to obtain
more compact networks. For those compact network designs such as MobileNet v1&v2  DCP can
still improve their performance by removing redundant channels. In particular for MobileNet v2 
DCP improves it by reducing 30% of channels on CIFAR-10. In the future  we will incorporate
the computational cost per layer into the optimization  and combine our method with other model
compression strategies (such as quantization) to further reduce the model size and inference cost.

Acknowledgements

This work was supported by National Natural Science Foundation of China (NSFC) (61876208 
61502177 and 61602185)  Recruitment Program for Young Professionals  Guangdong Provincial
Scientiﬁc and Technological funds (2017B090901008  2017A010101011  2017B090910005)  Funda-
mental Research Funds for the Central Universities D2172480  Pearl River S&T Nova Program of
Guangzhou 201806010081  CCF-Tencent Open Research Fund RAGR20170105  and Program for
Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183.

9

(a)Input image(b) Feature maps of the pruned channels(c) Feature maps of the selected channelsReferences
[1] J. M. Alvarez and M. Salzmann. Learning the number of neurons in deep networks. In NIPS  pages

2270–2278  2016.

[2] S. Bahmani  B. Raj  and P. T. Boufounos. Greedy sparsity-constrained optimization. JMLR  14(Mar):807–

841  2013.

[3] J. Cao  Y. Guo  Q. Wu  C. Shen  J. Huang  and M. Tan. Adversarial learning with local coordinate coding.

In ICML  volume 80  pages 707–715  2018.

[4] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical image

database. In CVPR  pages 248–255  2009.

[5] E. L. Denton  W. Zaremba  J. Bruna  Y. LeCun  and R. Fergus. Exploiting linear structure within

convolutional networks for efﬁcient evaluation. In NIPS  pages 1269–1277  2014.

[6] Y. Gong  L. Liu  M. Yang  and L. Bourdev. Compressing deep convolutional networks using vector

quantization. arXiv preprint arXiv:1412.6115  2014.

[7] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.
Generative adversarial nets. In Advances in neural information processing systems  pages 2672–2680 
2014.

[8] Y. Guo  M. Tan  Q. Wu  J. Chen  A. V. D. Hengel  and Q. Shi. The shallow end: Empowering shallower

deep-convolutional networks through auxiliary outputs. arXiv preprint arXiv:1611.01773  2016.

[9] Y. Guo  Q. Wu  C. Deng  J. Chen  and M. Tan. Double forward propagation for memorized batch

normalization. In AAAI  2018.

[10] Y. Guo  A. Yao  and Y. Chen. Dynamic network surgery for efﬁcient dnns. In NIPS  pages 1379–1387 

2016.

[11] S. Han  H. Mao  and W. J. Dally. Deep compression: Compressing deep neural networks with pruning 

trained quantization and huffman coding. In ICLR  2016.

[12] S. Han  J. Pool  J. Tran  and W. Dally. Learning both weights and connections for efﬁcient neural network.

In NIPS  pages 1135–1143  2015.

[13] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR  pages

770–778  2016.

[14] Y. He  X. Zhang  and J. Sun. Channel pruning for accelerating very deep neural networks. In ICCV  pages

1389–1397  2017.

[15] A. G. Howard  M. Zhu  B. Chen  D. Kalenichenko  W. Wang  T. Weyand  M. Andreetto  and H. Adam.
Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861  2017.

[16] H. Hu  R. Peng  Y.-W. Tai  and C.-K. Tang. Network trimming: A data-driven neuron pruning approach

towards efﬁcient deep architectures. arXiv preprint arXiv:1607.03250  2016.

[17] G. B. Huang  M. Ramesh  T. Berg  and E. Learned-Miller. Labeled faces in the wild: A database for
studying face recognition in unconstrained environments. Technical report  Technical Report 07-49 
University of Massachusetts  Amherst  2007.

[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. In ICML  pages 448–456  2015.

[19] M. Jaderberg  A. Vedaldi  and A. Zisserman. Speeding up convolutional neural networks with low rank

expansions. arXiv preprint arXiv:1405.3866  2014.

[20] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Tech Report  2009.

[21] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS  pages 1097–1105  2012.

[22] C.-Y. Lee  S. Xie  P. Gallagher  Z. Zhang  and Z. Tu. Deeply-supervised nets. In AISTATS  pages 562–570 

2015.

10

[23] F. Li  B. Zhang  and B. Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711  2016.

[24] H. Li  A. Kadav  I. Durdanovic  H. Samet  and H. P. Graf. Pruning ﬁlters for efﬁcient convnets. In ICLR 

2017.

[25] J. Liu  J. Ye  and R. Fujimaki. Forward-backward greedy algorithms for general convex smooth functions

over a cardinality constraint. In ICML  pages 503–511  2014.

[26] W. Liu  Y. Wen  Z. Yu  M. Li  B. Raj  and L. Song. Sphereface: Deep hypersphere embedding for face

recognition. In CVPR  pages 212–220  2017.

[27] Z. Liu  J. Li  Z. Shen  G. Huang  S. Yan  and C. Zhang. Learning efﬁcient convolutional networks through

network slimming. In ICCV  pages 2736–2744  2017.

[28] J.-H. Luo  J. Wu  and W. Lin. Thinet: A ﬁlter level pruning method for deep neural network compression.

In ICCV  pages 5058–5066  2017.

[29] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In ICML  pages

807–814  2010.

[30] Y. Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In SMD 

volume 27  pages 372–376  1983.

[31] O. M. Parkhi  A. Vedaldi  A. Zisserman  et al. Deep face recognition. In BMVC  volume 1  page 6  2015.

[32] A. Paszke  S. Gross  S. Chintala  and G. Chanan. Pytorch: Tensors and dynamic neural networks in python

with strong gpu acceleration  2017.

[33] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolutional

generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[34] M. Rastegari  V. Ordonez  J. Redmon  and A. Farhadi. Xnor-net: Imagenet classiﬁcation using binary

convolutional neural networks. In ECCV  pages 525–542  2016.

[35] J. Redmon  S. Divvala  R. Girshick  and A. Farhadi. You only look once: Uniﬁed  real-time object detection.

In CVPR  pages 779–788  2016.

[36] S. Ren  K. He  R. Girshick  and J. Sun. Faster r-cnn: Towards real-time object detection with region

proposal networks. In NIPS  pages 91–99  2015.

[37] F. Schroff  D. Kalenichenko  and J. Philbin. Facenet: A uniﬁed embedding for face recognition and

clustering. In CVPR  pages 815–823  2015.

[38] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. In

NIPS  pages 568–576  2014.

[39] V. Sindhwani  T. Sainath  and S. Kumar. Structured transforms for small-footprint deep learning. In NIPS 

pages 3088–3096  2015.

[40] S. Srinivas  A. Subramanya  and R. V. Babu. Training sparse neural networks. In CVPRW  pages 455–462 

2017.

[41] R. K. Srivastava  K. Greff  and J. Schmidhuber. Training very deep networks. In NIPS  pages 2377–2385 

2015.

[42] Y. Sun  D. Liang  X. Wang  and X. Tang. Deepid3: Face recognition with very deep neural networks. arXiv

preprint arXiv:1502.00873  2015.

[43] C. Szegedy  W. Liu  Y. Jia  P. Sermanet  S. Reed  D. Anguelov  D. Erhan  V. Vanhoucke  and A. Rabinovich.

Going deeper with convolutions. In CVPR  pages 1–9  2015.

[44] Y. Taigman  M. Yang  M. Ranzato  and L. Wolf. Deepface: Closing the gap to human-level performance in

face veriﬁcation. In CVPR  pages 1701–1708  2014.

[45] M. Tan  I. W. Tsang  and L. Wang. Towards ultrahigh dimensional feature selection for big data. JMLR 

15(1):1371–1429  2014.

[46] M. Tan  I. W. Tsang  and L. Wang. Matching pursuit lasso part i: Sparse recovery over big dictionary. TSP 

63(3):727–741  2015.

11

[47] L. Wang  Y. Xiong  Z. Wang  Y. Qiao  D. Lin  X. Tang  and L. Van Gool. Temporal segment networks:

Towards good practices for deep action recognition. In ECCV  pages 20–36  2016.

[48] W. Wen  C. Wu  Y. Wang  Y. Chen  and H. Li. Learning structured sparsity in deep neural networks. In

NIPS  pages 2074–2082  2016.

[49] J. Ye  X. Lu  Z. Lin  and J. Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel

pruning of convolution layers. arXiv preprint arXiv:1802.00124  2018.

[50] D. Yi  Z. Lei  S. Liao  and S. Z. Li. Learning face representation from scratch. arXiv preprint

arXiv:1411.7923  2014.

[51] R. Yu  A. Li  C.-F. Chen  J.-H. Lai  V. I. Morariu  X. Han  M. Gao  C.-Y. Lin  and L. S. Davis. Nisp:

Pruning networks using neuron importance score propagation. In CVPR  pages 9194–9203  2018.

[52] X. Yuan  P. Li  and T. Zhang. Gradient hard thresholding pursuit for sparsity-constrained optimization. In

ICML  pages 127–135  2014.

[53] X. Zhang  J. Zou  K. He  and J. Sun. Accelerating very deep convolutional networks for classiﬁcation and

detection. TPAMI  38(10):1943–1955  2016.

[54] A. Zhou  A. Yao  Y. Guo  L. Xu  and Y. Chen. Incremental network quantization: Towards lossless cnns

with low-precision weights. In ICLR  2017.

[55] S. Zhou  Y. Wu  Z. Ni  X. Zhou  H. Wen  and Y. Zou. Dorefa-net: Training low bitwidth convolutional

neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160  2016.

[56] C. Zhu  S. Han  H. Mao  and W. J. Dally. Trained ternary quantization. In ICLR  2017.

[57] B. Zhuang  C. Shen  M. Tan  L. Liu  and I. Reid. Towards effective low-bitwidth convolutional neural

networks. In CVPR  pages 7920–7928  2018.

12

,Zhuangwei Zhuang
Mingkui Tan
Bohan Zhuang
Jing Liu
Yong Guo
Qingyao Wu
Junzhou Huang
Jinhui Zhu