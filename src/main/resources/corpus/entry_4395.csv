2019,Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks,Lipschitz constraints under L2 norm on deep neural networks are useful for provable adversarial robustness bounds  stable training  and Wasserstein distance estimation. While heuristic approaches such as the gradient penalty have seen much practical success  it is challenging to achieve similar practical performance while provably enforcing a Lipschitz constraint. In principle  one can design Lipschitz constrained architectures using the composition property of Lipschitz functions  but Anil et al. recently identified a key obstacle to this approach: gradient norm attenuation. They showed how to circumvent this problem in the case of fully connected networks by designing each layer to be gradient norm preserving. We extend their approach to train scalable  expressive  provably Lipschitz convolutional networks. In particular  we present the Block Convolution Orthogonal Parameterization (BCOP)  an expressive parameterization of orthogonal convolution operations. We show that even though the space of orthogonal convolutions is disconnected  the largest connected component of BCOP with 2n channels can represent arbitrary BCOP convolutions over n channels. Our BCOP parameterization allows us to train large convolutional networks with provable Lipschitz bounds. Empirically  we find that it is competitive with existing approaches to provable adversarial robustness and Wasserstein distance estimation.,Preventing Gradient Attenuation in

Lipschitz Constrained Convolutional Networks

Qiyang Li∗  Saminul Haque∗  Cem Anil  James Lucas  Roger Grosse  Jörn-Henrik Jacobsen

{jlucas  rgrosse}@cs.toronto.edu

j.jacobsen@vectorinstitute.ai

University of Toronto  Vector Institute

{qiyang.li  saminul.haque  cem.anil}@mail.utoronto.ca

Abstract

Lipschitz constraints under L2 norm on deep neural networks are useful for prov-
able adversarial robustness bounds  stable training  and Wasserstein distance esti-
mation. While heuristic approaches such as the gradient penalty have seen much
practical success  it is challenging to achieve similar practical performance while
provably enforcing a Lipschitz constraint. In principle  one can design Lipschitz
constrained architectures using the composition property of Lipschitz functions 
but Anil et al. [2] recently identiﬁed a key obstacle to this approach: gradient
norm attenuation. They showed how to circumvent this problem in the case of
fully connected networks by designing each layer to be gradient norm preserving.
We extend their approach to train scalable  expressive  provably Lipschitz convo-
lutional networks. In particular  we present the Block Convolution Orthogonal
Parameterization (BCOP)  an expressive parameterization of orthogonal convolu-
tion operations. We show that even though the space of orthogonal convolutions is
disconnected  the largest connected component of BCOP with 2n channels can rep-
resent arbitrary BCOP convolutions over n channels. Our BCOP parameterization
allows us to train large convolutional networks with provable Lipschitz bounds.
Empirically  we ﬁnd that it is competitive with existing approaches to provable
adversarial robustness and Wasserstein distance estimation. 2

1

Introduction

There has been much interest in training neural networks with known upper bounds on their Lipschitz
constants under L2 norm3. Enforcing Lipschitz constraints can provide provable robustness against
adversarial examples [48]  improve generalization bounds [46]  and enable Wasserstein distance
estimation [2  3  22]. Heuristic methods for enforcing Lipschitz constraints  such as the gradient
penalty [22] and spectral norm regularization [52]  have seen much practical success  but provide no
guarantees about the Lipschitz constant. It remains challenging to achieve similar practical success
while provably satisfying a Lipschitz constraint.
In principle  one can design provably Lipschitz-constrained architectures by imposing a Lipschitz
constraint on each layer; the Lipschitz bound for the network is then the product of the bounds
for each layer. Anil et al. [2] identiﬁed a key difﬁculty with this approach: because a layer with
a Lipschitz bound of 1 can only reduce the norm of the gradient during backpropagation  each
step of backprop gradually attenuates the gradient norm  resulting in a much smaller Jacobian for
the network’s function than is theoretically allowed. We refer to this problem as gradient norm
attenuation. They showed that Lipschitz-constrained ReLU networks were prevented from using

∗Equal contributions
2Code is available at: github.com/ColinQiyangLi/LConvNet
3Unless speciﬁed otherwise  we refer to Lipschitz constant as the Lipschitz constant under L2 norm.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

their full nonlinear capacity due to the need to prevent gradient norm attenuation. To counteract this
problem  they introduced gradient norm preserving (GNP) architectures  where each layer preserves
the gradient norm. For fully connected layers  this involved constraining the weight matrix to be
orthogonal and using a GNP activation function called GroupSort. Unfortunately  the approach of
Anil et al. [2] only applies to fully-connected networks  leaving open the question of how to constrain
the Lipschitz constants of convolutional networks.
As many state-of-the-art deep learning applications rely on convolutional networks  there have been
numerous attempts for tightly enforcing Lipschitz constants of convolutional networks. However 
those existing techniques either hinder representational power or induce difﬁculty in optimization.
Cisse et al. [12]  Tsuzuku et al. [48]  Qian and Wegman [40] provide loose bounds on the Lipschitz
constant that can limit the parameterizable region. Gouk et al. [21] obtain a tight bound on the
Lipschitz constant  but tend to lose expressive power during training due to vanishing singular values.
The approach of Sedghi et al. [45] is computationally intractable for larger networks.
In this work  we introduce convolutional GNP networks with an efﬁcient parameterization of orthogo-
nal convolutions by adapting the construction algorithm from Xiao et al. [51]. This parameterization
avoids the issues of loose bounds on the Lipschitz constant and computational intractability observed
in the aforementioned approaches. Furthermore  we provide theoretical analysis that demonstrates
the disconnectedness of the orthogonal convolution space  and how our parameterization alleviates
the optimization challenge engendered by the disconnectedness.
We evaluate our GNP networks in two situations where expressive Lipschitz-constrained networks are
of central importance. The ﬁrst is provable norm-bounded adversarial robustness  which is the task of
classiﬁcation and additionally certifying that the network’s classiﬁcation will not change under any
norm-bounded perturbation. Due to the tight Lipschitz properties  the constructed GNP networks can
easily give non-trivial lower bounds on the robustness of the network’s classiﬁcation. We demonstrate
that our method outperforms the state-of-the-art in provable deterministic robustness under L2 metric
on MNIST and CIFAR-10. The other application is Wasserstein distance estimation. Wasserstein
distance estimation can be rephrased as a maximization over 1-Lipschitz functions  allowing our
Lipschitz-constrained networks to be directly applied to this problem. Moreover  the restriction to
GNP we impose is not necessarily a hindrance  as it is shown by Gemici et al. [19] that the optimal
1-Lipschitz function is also GNP almost everywhere. We demonstrate that our GNP convolutional
networks can obtain tighter Wasserstein distance estimates than competing architectures.

2 Background

2.1 Lipschitz Functions under L2 Norm

In this work  we focus on Lipschitz functions with respect to the L2 norm. We say a function
f : Rn → Rm is l-Lipschitz if and only if

(1)
We denote Lip(f ) as the smallest K for which f is l-Lipschitz  and call it the Lipschitz constant of
f. For two Lipschitz continuous functions f and g  the following property holds:

||f (x1) − f (x2)||2 ≤ l||x1 − x2||2 ∀x1  x2 ∈ Rn

Lip(f ◦ g) ≤ Lip(f ) Lip(g)

(2)
The most basic neural network design consists of a composition of linear transformations and non-
linear activation functions. The property above (Equation 2) allows one to upper-bound the Lipschitz
constant of a network by the product of the Lipschitz constants of each layer. However  as modern
neural networks tend to possess many layers  the resultant upper-bound is likely to be very loose  and
constraining it increases the risk of diminishing the Lipschitz constrained network capacity that can
be utilized.

2.2 Gradient Norm Preservation (GNP)
Let y = f (x) be 1-Lipschitz  and L be a loss function. The norm of the gradient after backpropagating
through a 1-Lipschitz function is no larger than the norm of the gradient before doing so:
(cid:107)∇xL(cid:107)2 = (cid:107)(∇yL)(∇xf )(cid:107)2 ≤ (cid:107)∇yL(cid:107)2 (cid:107)∇xf(cid:107)2 ≤ Lip(f )(cid:107)∇yL(cid:107)2 ≤ (cid:107)∇yL(cid:107)2

As a consequence of this relation  the gradient norm will likely be attenuated during backprop if
no special measures are taken. One way to ﬁx the gradient norm attenuation problem is to enforce

2

each layer to be gradient norm preserving (GNP). Formally  f : Rn (cid:55)→ Rm is GNP if and only if its
input-output Jacobian  J ∈ Rm×n  satisﬁes the following property:

(cid:13)(cid:13)J T g(cid:13)(cid:13)2 = ||g||2 ∀g ∈ G.

where G ⊆ Rm deﬁnes the possible values that the gradient vector g can take. Note that when
m = n  this condition is equivalent to orthogonality of J. In this work  we consider a slightly stricter
deﬁnition where G = Rm because this allows us to directly compose two GNP (strict) functions
without reasoning about their corresponding G. For the rest of the paper  unless speciﬁed otherwise  a
GNP function refers to this more strict deﬁnition.
Based on the deﬁnition of GNP  we can deduce that GNP functions are 1-Lipschitz in the 2-norm.
Since the composition of GNP functions is also GNP  one can design a GNP network by stacking GNP
building blocks. Another favourable condition that GNP networks exhibit is dynamical isometry [51 
37  38] (where the entire distribution of singular values of input-output Jacobian is close to 1)  which
has been shown to improve training speed and stability.

2.3 Provable Norm-bounded Adversarial Robustness

We consider a classiﬁer f with T classes that takes in input x and produces a logit for each of the
yT ]. An input data point x with label t ∈ {1  2 ···   T} is provably
classes: f (x) = [y1
robustly classiﬁed by f under perturbation norm of  if

···

y2

f (x + δ)i = t ∀δ : ||δ||2 ≤ .

arg max

i

The margin of the prediction for x is given by Mf (x) = max(0  yt − maxi(cid:54)=t yi). If f is l-Lipschitz 
2l < Mf (x) (See Appendix P for the proof).
we can certify that f is robust with respect to x if

√

2.4 Wasserstein Distance Estimation

Wasserstein distance is a distance metric between two probability distributions [39]. The Kantorovich-
Rubinstein formulation of Wasserstein distance expresses it as a maximization problem over 1-
Lipschitz functions [3]:

(cid:18)

(cid:19)

W (P1  P2) = sup

f :Lip(f )≤1

E

x∼P1(x)

[f (x)] − E

x∼P2(x)

[f (x)]

.

(3)

In Wasserstein GAN architecture  Arjovsky et al. [3] proposed to parametrize the scalar-valued
function f using a Lipschitz constrained network  which serves as the discriminator that estimates
the Wasserstein distance between the generator and data distribution. One important property to
note is that the optimal scalar function f is GNP almost everywhere (See Corollary 1 in Gemici
et al. [19]). Naturally  this property favours the optimization approach that focuses on searching over
GNP functions. Indeed  Anil et al. [2] found that GNP networks can achieve tighter lower bounds
compared to non-GNP networks.

3 Orthogonal Convolution Kernels

The most crucial step to building a GNP convolutional network is constructing the GNP convolution
itself. Since a convolution operator is a linear operator  making the convolution kernel GNP is
equivalent to making its corresponding linear operator orthogonal. While there are numerous
methods for orthogonalizing arbitrary linear operators  it is not immediately clear how to do this for
convolutions  especially when preserving kernel size. We ﬁrst summarize the orthogonal convolution
representations from Kautsky and Turcajová [28] and Xiao et al. [51] (Section 3.1). Then  we
analyze the topology of the space of orthogonal convolution kernels and demonstrate that the space is
disconnected (with at least O(n2) connected components for a 2 × 2 2-D convolution layer)  which
is problematic for gradient-based optimization methods because they are conﬁned to one component
(Section 3.2). Fortunately  this problem can be ﬁxed by increasing the number of channels: we
demonstrate that a single connected component of the space of orthogonal convolutions with 2n
channels can represent any orthogonal convolution with n channels (Section 3.3).

3

Figure 1: Visualization of a 1-D orthogonal convolution  [P I − P ]  applied to a 1-D input tensor
v ∈ R2×3 with a length of 3 and channel size of 2. P ∈ R2×2 here is the orthogonal projection
onto the x-axis  which makes I − P the complementary projection onto the y-axis. Each cell of v
corresponds to one of the three spatial locations  and the vector contained within it represents the
vector along the channel dimension in said spatial location.

3.1 Constructing Orthogonal Convolutions

To begin analysing orthogonal convolution kernels  we must ﬁrst understand the symmetric projector 
which is a fundamental building block of orthogonal convolutions. An n × n matrix P is deﬁned
to be a symmetric projector if P = P 2 = P T . Geometrically  a symmetric projector P represents
an orthogonal projection onto the range of P . From this geometric interpretation  it is not hard to
see that the space of projectors has n + 1 connected components  based on the rank of the projector
(for a more rigorous treatment  see Remark 4.1 in Appendix K). For notation simplicity  we denote
P(n) as the set of all n × n symmetric projectors and P(n  k) as the subset of all n × n symmetric
projectors with ranks of k.
Now that the concept of symmetric projectors has been established  we will consider how to construct
1-D convolutional kernels. As shown by Kautsky and Turcajová [28]  all 1-D orthogonal convolution
kernels with a kernel size K can be represented as:

W(H  P1:K−1) = H(cid:3) [P1

(I − P1)] (cid:3)··· (cid:3) [PK−1

(I − PK−1)]

with Zi =(cid:80)∞

(4)
where H ∈ O(n) is an orthogonal matrix of n × n  Pi ∈ P(n)  and (cid:3) represents block convolution 
which is convolution using matrix multiplication rather than scalar multiplication:

··· Xp] (cid:3) [Y1 Y2

··· Yq] = [Z1 Z2

[X1 X2
i(cid:48)=−∞ Xi(cid:48)Yi−i(cid:48)  where the out-of-range elements are all zero (e.g.  X<1 = 0  X>p =
0  Y<1 = 0  Y>q = 0). Unlike regular convolutions  the block convolution does not commute since
matrix multiplication does not commute. One important property of block convolution is that it
corresponds to composition of the kernel operators. That is  X ∗ (Y ∗ v) = (X(cid:3)Y ) ∗ v  where
A ∗ v represents the resulting tensor after applying convolution A to v. This composition property
allows us to decompose the representation (Equation 4) into applications of orthogonal convolutions
with kernel size of 2 (Figure 1 demonstrates the effect of it) along with a channel-wise orthogonal
transformation (H).
Xiao et al. [51] extended the 1-D representation to the 2-D case using alternating applications of
orthogonal convolutions of size 2:

··· Zp+q−1]

W(H  P1:K−1  Q1:K−1) = H(cid:3)

(cid:3) [Q1

I − Q1] (cid:3)···

(cid:20) P1
(cid:21)
(cid:20) PK−1

I − P1

(cid:21)

···(cid:3)

I − PK−1
to

similarly

(cid:3) [QK−1
the

I − QK−1]

(5)

i(cid:48)=−∞(cid:80)∞
(cid:80)∞

X(cid:3)Y

=

is

deﬁned

where Z

=
j(cid:48)=−∞ [Xi(cid:48) j(cid:48)Yi−i(cid:48) j−j(cid:48)]  and Pi  Qi ∈ P(n). Unlike in 1-D  we discovered that
this 2-D representation could only represent a subset of 2-D orthogonal convolutions (see Appendix
O for an example). However  we do not know whether simple modiﬁcations to this parameterization
will result in a complete representation of all 2-D orthogonal convolutions (see Appendix O for
details on the open question).

1-D case with Zij

3.2 Topology of the Orthogonal Convolution Space

Before utilizing this space of orthogonal convolutions  we would like to analyze some fundamental
properties of this space. Since P(n) has n + 1 connected components and orthogonal convolutions

4

Algorithm 1: Block Convolution Orthogonal Parameterization (BCOP)

Input: co × ci unconstrained matrix O  ci ×(cid:4) ci

1 to k − 1  assuming ci ≥ co

2

(cid:5) unconstrained matrices Mi  Ni for i from

Result: Orthogonal Convolution Kernel W ∈ Rco×ci×K×K
H ← Orthogonalize(O); (cid:46) any differentiable orthogonalization procedure (e.g.  Björck [5]);
Initialize W as a 1 × 1 convolution with W [0  0] = H;
for i from 1 to K − 1 do

RP   RQ ← Orthogonalize(Mi)  Orthogonalize(Ni);
P  Q ← RP RT
W ← W(cid:3)

(cid:3) [Q I − Q]

(cid:20) P

P   RQRT
I − P

(cid:21)

Q; (cid:46) Construct symmetric projectors with half of the full rank;

end
Output: W

are constructed out of many projectors  it is to be expected that there are many connected components
in the space of orthogonal convolutions. Indeed  we see the ﬁrst result in 1-D (Theorem 1).
Theorem 1 (Connected Components of 1-D Orthogonal Convolution). The 1-D orthogonal convolu-
tion space is compact and has 2(K − 1)n + 2 connected components  where K is the kernel size and
n is the number of channels.
In 2-D  we analyze case of kernel size of 2 (2 × 2 kernels) and show that the number of connected
components grows at least quadratically with respect to the channel size:
Theorem 2 (Connected Components of 2-D Orthogonal Convolution with K = 2). 2-D orthogonal
convolution space with a kernel size of 2 × 2 has at least 2(n + 1)2 connected components  where n
is the number of channels.

The disconnectedness in the space of orthogonal convolution imposes an intrinsic difﬁculty in
optimizing over the space of orthogonal convolution kernels  as gradient-based optimizers are
conﬁned to their initial connected component. We refer readers to Appendix K for the proof of
Theorem 1 and Appendix M for the proof of Theorem 2.

3.3 Block Convolution Orthogonal Parameterization (BCOP)

To remedy the disconnectedness issue  we show the following:
Theorem 3 (BCOP Construction with Auxiliary Dimension). For any convolution C =
W(H  P1:K−1  Q1:K−1) with input and output channels n and Pi  Qi ∈ P(n)  there exists a con-
volution C(cid:48) = W(H(cid:48)  P (cid:48)
1:K−1  Q(cid:48)
1:K−1) with input and output channels 2n constructed from only
i ∈ P(2n  n)) such that C(cid:48)(x)1:n = C(x1:n). That is  the ﬁrst n channels
n-rank projectors (P (cid:48)
i   Q(cid:48)
of the output is the same with respect to the ﬁrst n channels of the input under both convolutions.
The idea behind this result is that some projectors in P(2n  n) may use their ﬁrst n dimensions to
represent P(n) and then use the latter n dimensions in a trivial capacity so that the total rank is n (see
Appendix N for the detailed proof).
Theorem 3 implies that all connected components of orthogonal convolutions constructed by W
with n channels can all be equivalently represented in a single connected component of convolutions
constructed by W with 2n channels by only using projectors that have rank n. (This comes at the
cost of requiring 4 times as many parameters.)
This result motivates us to parameterize the connected subspace of orthogonal convolutions deﬁned by

W(H  ˜P1:K−1  ˜Q1:K−1) where ˜Pi ∈ P(n (cid:4) n

(cid:5)). We refer to this method as the

(cid:5)) and ˜Qi ∈ P(n (cid:4) n

Block Convolution Orthogonal Parameterization (BCOP). The procedure for BCOP is summarized in
Algorithm 1 (See Appendix H for implementation details).

2

2

4 Related Work

Reshaped Kernel Method (RK) This method reshapes a convolution kernel with dimensions
(co  ci  k  k) into a (co  k2ci) matrix. The Lipschitz constant (or spectral norm) of a convolution

5

operator is bounded by a constant factor of the spectral norm of its reshaped matrix [12  48  40] 
which enables bounding of the convolution operator’s Lipschitz constant by bounding that of the
reshaped matrix. However  this upper-bound can be conservative  causing a bias towards convolution
operators with low Lipschitz constants  limiting the method’s expressive power. In this work  we
strictly enforce orthogonality of the reshaped matrix rather than softly constrain it via regularization 
as done in Cisse et al. [12]. We refer to this variant as reshaped kernel orthogonalization  or RKO.

One-Sided Spectral Normalization (OSSN) This variant of spectral normalization [36] scales the
kernel so that the spectral norm of the convolution operator is at most 1 [21]. This is a projection
under the matrix 2-norm but not the Frobenius norm. It is notable because when using Euclidean
steepest descent with this projection  such as in constrained gradient-based optimization  there is
no guarantee to converge to the correct solution (see an explicit example and further analysis in
Appendix A). In practice  we found that projecting during the forward pass (as in Miyato et al. [36])
yields better performance than projecting after each gradient update.

Singular Value Clipping and Masking (SVCM) Unlike spectral normalization  singular value
clipping is a valid projection under the Frobenius norm. Sedghi et al. [45] demonstrates a method to
perform an approximation of the optimal projection to the orthogonal kernel space. Unfortunately 
this method needs many expensive iterations to enforce the Lipschitz constraint tightly  making this
approach computationally intractable in training large networks with provable Lipschitz constraints.

Comparison to BCOP OSSN and SVCM’s run-time depend on the input’s spatial dimensions 
which prohibits scalability (See Appendix C for a time complexity analysis). RKO does not guarantee
an exact Lipschitz constant  which may cause a loss in expressive power. Additionally  none of these
methods guarantee gradient norm preservation. BCOP avoids all of the issues above.

4.1 Provable Adversarial Robustness

Certifying the adversarial robustness of a network subject to norm ball perturbation is difﬁcult. Exact
certiﬁcation methods using mixed-integer linear programming or SMT solvers scale poorly with
the complexity of the network [27  10]. Cohen et al. [15] and Salman et al. [43] use an estimated
smoothed classiﬁer to achieve very high provable robustness with high conﬁdence. In this work  we
are primarily interested in providing deterministic provable robustness guarantees.
Recent work has been focusing on guiding the training of the network to be veriﬁed or certiﬁed
(providing a lower-bound on provable robustness) easier [49  50  18  17  23]. For example  Xiao
et al. [50] encourage weight sparsity and perform network pruning to speed up the exact veriﬁcation
process for ReLU networks. Wong et al. [49] optimize the network directly towards a robustness
lower-bound using a dual optimization formulation.
Alternatively  rather than modifying the optimization objective to incentivize robust classiﬁcation 
one can train networks to have a small global Lipschitz constant  which allows an easy way to certify
robustness via the output margin. Cohen et al. [14] deploy spectral norm regularization on weight
matrices of a fully connected network to constrain the Lipschitz constant and certify the robustness of
the network at the test time. Tsuzuku et al. [48] estimate an upper-bound of the network’s Lipschitz
constant and train the network to maximize the output margin using a modiﬁed softmax objective
function according to the estimated Lipschitz constant. In contrast to these approaches  Anil et al.
[2] train fully connected networks that have a known Lipschitz constant by enforcing gradient norm
preservation. Our work extends this idea to convolutional networks.

5 Experiments

The primary point of interest for the BCOP method (Section 3.3) is its expressiveness compared
against other common approaches of paramterizing Lipschitz constrained convolutions (Section 4).
To study this  we perform an ablation study on two tasks using these architectures: The ﬁrst task is
provably robust image classiﬁcation tasks on two datasets (MNIST [31] and CIFAR-10 [30])4. We
ﬁnd our method outperformed other Lipschitz constrained convolutions under the same architectures
as well as the state-of-the-art in this task (Section 5.2). The second task is 1-Wasserstein distance

4We only claim in the deterministic case as recent approaches have much higher probabilistic provable

robustness [15  43].

6

estimation of GANs where our method also outperformed other competing Lipschitz-convolutions
under the same architecutre (Section 5.3).

5.1 Network Architectures and Training Details

A beneﬁt of training GNP networks is that we enjoy the property of dynamical isometry  which
inherently affords greater training stability  thereby reducing the need for common techniques that
would otherwise be difﬁcult to incorporate into a GNP network. For example  if a 1-Lipschitz residual
connection maintains GNP  the residual block must be an identity function with a constant bias (see
an informal justiﬁcation in Appendix D.1). Also  batch normalization involves scaling the layer’s
output  which is not necessarily 1-Lipschitz  let alone GNP. For these reasons  residual connections
and batch normalization are not included in the model architecture. We also use cyclic padding to
substitute zero-padding since zero-padded orthogonal convolutions must be size 1 (see an informal
proof in Appendix D.2). Finally  we use “invertible downsampling” [25] in replacement of striding
and pooling to achieve spatial downsampling while maintaining the GNP property. The details for
these architectural decisions are in Appendix D.
Because of these architectural constraints  we base our networks on architectures that do not in-
volve residual connections. For provable robustness experiments  we use the “Small” and “Large”
convolutional networks from Wong et al. [49]. For Wasserstein distance estimation  we use the
fully convolutional critic from Radford et al. [41] (See Appendix E  F for details). Unless speciﬁed
otherwise  each experiment is repeated 5 times with mean and standard deviation reported.

5.2 Provable Adversarial Robustness

5.2.1 Robustness Evaluation

For adversarial robustness evaluation  we use the L2-norm-constrained threat model [8]  where the
adversary is constrained to L2-bounded perturbation with the L2 norm constrained to be below . We
refer to clean accuracy as the percentage of un-perturbed examples that are correctly classiﬁed and
robust accuracy as the percentage of examples that are guaranteed to be correctly classiﬁed under the
threat model. We use the margin of the model prediction to determine a lower bound of the robust
accuracy (as described in Section 2.3). We also evaluate the empirical robustness of our model around
under two gradient-based attacks and two decision-based attacks: (i) PGD attack with CW loss [34  7] 
(ii) FGSM [47]  (iii) Boundary attack (BA) [6]  (iv) Point-wise attack (PA) [44]. Speciﬁcally  the
gradient-based methods ((i) and (ii)) are done on the whole test dataset; the decision-based attacks
((iii) and (iv)) are done only on the ﬁrst 100 test data points since they are expensive to run.5

5.2.2 Comparison of Different Methods for Enforcing Spectral Norm of Convolution

We compare the performance of OSSN  RKO  SVCM  and BCOP on margin training for adversarial
robustness on MNIST and CIFAR-10. To make the comparison fair  we ensure all the methods
have a tight Lipschitz constraint of 1. For OSSN  we use 10 power iterations and keep a running
vector for each convolution layer to estimate the spectral norm and perform the projection during
every forward pass. For SVCM  we perform the singular value clipping projection with 50 iterations
after every 100 gradient updates to ensure the Lipschitz bound is tight. For RKO  instead of using a
regularization term to enforce the orthogonality (as done in Cisse et al. [12])  we use Björck [5] to
orthogonalize the reshaped matrix before scaling down the kernel. We train two different convolutional
architectures with the four aforementioned methods of enforcing Lipschitz convolution layers on
image classiﬁcation tasks. To achieve large output margins  we use ﬁrst-order  multi-class  hinge loss
with a margin of 2.12 on MNIST and 0.7071 on CIFAR-10.
Our approach (BCOP) outperforms all competing methods across all architectures on both MNIST and
CIFAR-10 (See Table 1 and Appendix I  Table 7). To understand the performance gap  we visualize
the singular value distribution of a convolution layer before and after training in Figure 2. We observe
that OSSN and RKO push many singular values to 0  suggesting that the convolution layer is not fully
utilizing the expressive power it is capable of. This observation is consistent with our hypothesis
that these methods bias the convolution operators towards sub-optimal regions caused by the loose
Lipschitz bound (for RKO) and improper projection (for OSSN). In contrast  SVCM’s singular values
started mostly near 0.5 and some of them were pushed up towards 1  which is consistent with the

5We use foolbox [42] for the two decision-based methods.

7

Figure 2: Singular value distribution at initialization (blue) and at the end of training (orange) for the
second layer of the “Large” baseline using different methods to enforce Lipschitz convolution.

BCOP

Dataset

MNIST
( = 1.58)

OSSN

RKO

97.54 ± 0.06
45.84 ± 0.90
98.77 ± 0.05
56.66 ± 0.23
64.53 ± 0.30
50.01 ± 0.21
72.41 ± 0.22
58.72 ± 0.23
Table 1: Clean and robust accuracy on MNIST and CIFAR-10 using different Lipschitz convolutions.
The provable robust accuracy is evaluated at  = 1.58 for MNIST and at  = 36/255 for CIFAR-10.

97.28 ± 0.08
43.58 ± 0.44
98.44 ± 0.05
55.18 ± 0.46
61.77 ± 0.63
47.46 ± 0.53
70.01 ± 0.26
55.76 ± 0.16

96.86 ± 0.13
42.95 ± 1.09
98.31 ± 0.03
53.77 ± 1.02
62.18 ± 0.66
48.03 ± 0.54
67.51 ± 0.47
53.64 ± 0.49

Clean
Robust
Clean
Robust
Clean
Robust
Clean
Robust

SVCM

97.24 ± 0.09
28.94 ± 1.58
97.93 ± 0.05
38.00 ± 1.82
62.39 ± 0.46
47.59 ± 0.56
69.65 ± 0.38
53.61 ± 0.51

CIFAR-10
( = 36/255)

Small

Large

Small

Large

Dataset
MNIST
( = 1.58)
CIFAR-10
( = 36/255)

Clean
Robust
Clean
Robust

BCOP-Large
98.77 ± 0.05
56.66 ± 0.23
72.41 ± 0.22
58.72 ± 0.23

FC-3

98.71 ± 0.02
54.46 ± 0.30
62.60 ± 0.39
49.97 ± 0.35

KW-Large KW-Resnet

88.12
44.53

59.76
50.60

-
-

61.20
51.96

Table 2: Comparison of our convolutional networks and the fully connected baseline in Anil et al.
[2] (FC-3) against provably robust models in previous works. The numbers for KW-Large and
KW-Resnet are directly obtained from Table 4 of in the Appendix of their paper [49].

procedure being an optimal projection. BCOP has all of its singular values at 1 throughout training
by design due to its gradient norm preservation and orthogonality. Thus  we empirically verify
the downsides of other methods and show that our proposed method enables maximally expressive
Lipschitz constrained convolutional layers with guaranteed gradient-norm-preservation.

5.2.3 State-of-the-art Comparison

To further demonstrate the expressive power of orthogonal convolution  we compare our networks
with models that achieve state-of-the-art deterministic provable adversarial robustness performance
(Table 2 and Appendix J  Table 8 and 9). We also evaluate the empirical robustness of our model
against common attacks on CIFAR-10. Comparing against Wong et al. [49]  our approach reaches
similar performance for “Small” architecture and better performance for “Large” architecture (Table
3).

5.3 Wasserstein Distance Estimation

In this section  we consider the problem of estimating the Wasserstein distance between two high
dimensional distributions using neural networks. Anil et al. [2] showed that in the fully connected
setting  ensuring gradient norm preservation is critical for obtaining tighter lower bounds on the
Wasserstein distance. We observe the same phenomenon in the convolutional setting.

8

KW

BCOP

KW

BCOP

Small
Clean
54.39
PGD
49.94
FGSM 49.98
Large
Clean
60.14
PGD
55.53
FGSM 55.55

64.53 ± 0.30
51.26 ± 0.17
51.57 ± 0.18

72.41 ± 0.22
64.39 ± 0.26
64.53 ± 0.25

Small
Clean (*)
BA (*)
PA (*)
Large
Clean (*)
BA (*)
PA (*)

63.00
60.00
63.00

68.00
64.00
68.00

74.20 ± 2.23
61.20 ± 2.99
74.00 ± 2.28

77.60 ± 1.74
71.20 ± 1.60
77.20 ± 1.60

Table 3: Comparison of our networks with Wong et al. [49] on CIFAR-10 dataset. Left: results of the
evaluation on the entire CIFAR-10 test dataset. Right (*): results of the evaluation on the ﬁrst 100
test samples. The KW models [49] are directly taken from their ofﬁcial repository.

STL-10 MaxMin
ReLU
CIFAR-10 MaxMin
ReLU

OSSN

7.39 ± 0.31
7.06 ± 0.72
3.29 ± 0.05
3.07 ± 0.12

RKO

8.95 ± 0.12
7.82 ± 0.21
4.95 ± 0.08
4.20 ± 0.06

BCOP

9.91 ± 0.11
8.28 ± 0.19
5.34 ± 0.07
4.39 ± 0.07

Table 4: Comparison of different Lipschitz constrained architectures on the Wasserstein distance
estimation task between the data and generator distributions of STL-10 and CIFAR-10 GANs. Each
estimate is a strict lower bound (estimated using 6 400 pairs of randomly sampled real and generated
image examples)  hence larger values indicate better performance.

We trained our networks to estimate the Wasserstein distance between the data and generator distribu-
tions of GANs6 [20] trained on RGB images from the STL-10 dataset [13] and CIFAR-10 dataset [30]
(resized to 64x64). After training the GANs  we froze the generator weights and trained Lipschitz
constrained convolutional networks to estimate the Wasserstein distance. We adapted the fully
convolutional discriminator model used by Radford et al. [41] by removing all batch normalization
layers and replacing all vanilla convolutional layers with Lipschitz candidates (BCOP  RKO  and
OSSN)7. We trained each model with ReLU or MaxMin activations [2]. The results are in Table 4.
Baking in gradient norm preservation in the architecture leads to signiﬁcantly tighter lower bounds on
the Wasserstein distance. The only architecture that has gradient norm preserving layers throughout
(BCOP with MaxMin) leads to the best estimate. Although OSSN has the freedom to learn orthogonal
kernels  this does not happen in practice and leads to poorer expressive power.

6 Conclusion and Future Work

We introduced convolutional GNP networks with an efﬁcient construction method of orthogonal
convolutions (BCOP) that overcomes the common issues of Lipschitz constrained networks such
as loose Lipschitz bounds and gradient norm attenuation. In addition  we showed the space of
orthogonal convolutions has many connected components and demonstrated how BCOP parameteri-
zation alleviates the optimization challenges caused by the disconnectedness. Our GNP networks
outperformed the state-of-the-art for deterministic provable adversarial robustness on L2 metrics
with both CIFAR-10 and MNIST  and obtained tighter Wasserstein distance estimates between high
dimensional distributions than competing approaches. Despite its effectiveness  our parameterization
is limited to only expressing a subspace of orthogonal convolutions. A complete parameterization
of the orthogonal convolution space may enable training even more powerful GNP convolutional
networks. We presented potential directions to achieve this and left the problem for future work.

6Note that any GAN variant could have been chosen here.
7We omit SVCM for comparison due to its computational intractability

9

Acknowledgements

We would like to thank Lechao Xiao  Arthur Rabinovich  Matt Koster  and Siqi Zhou for their
valuable insights and feedback. We would like to thank Sherjil Ozair for spotting a bug in our code 
whose ﬁx improved our results. RG acknowledges support from the CIFAR Canadian AI Chairs
program.

References
[1] Martin Abadi  Andy Chu  Ian Goodfellow  H Brendan McMahan  Ilya Mironov  Kunal Talwar 
and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security  pages 308–318. ACM  2016.

[2] Cem Anil  James Lucas  and Roger Grosse. Sorting out Lipschitz function approximation. In
Kamalika Chaudhuri and Ruslan Salakhutdinov  editors  Proceedings of the 36th International
Conference on Machine Learning  volume 97 of Proceedings of Machine Learning Research 
pages 291–301  Long Beach  California  USA  09–15 Jun 2019. PMLR. URL http://
proceedings.mlr.press/v97/anil19a.html.

[3] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein GAN. arXiv preprint

arXiv:1701.07875  2017.

[4] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society  48

(3):334–334  1997.

[5] Åke Björck and Clazett Bowie. An iterative algorithm for computing the best estimate of an

orthogonal matrix. SIAM Journal on Numerical Analysis  8(2):358–364  1971.

[6] Wieland Brendel  Jonas Rauber  and Matthias Bethge. Decision-based adversarial attacks:
Reliable attacks against black-box machine learning models. In International Conference on
Learning Representations  2018. URL https://openreview.net/forum?id=SyZI0GWCZ.

[7] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In

2017 IEEE Symposium on Security and Privacy (SP)  pages 39–57. IEEE  2017.

[8] Nicholas Carlini  Anish Athalye  Nicolas Papernot  Wieland Brendel  Jonas Rauber  Dimitris
Tsipras  Ian Goodfellow  and Aleksander Madry. On evaluating adversarial robustness. arXiv
preprint arXiv:1902.06705  2019.

[9] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems  pages 2172–2180  2016.

[10] Chih-Hong Cheng  Georg Nührenberg  and Harald Ruess. Maximum resilience of artiﬁcial
neural networks. In International Symposium on Automated Technology for Veriﬁcation and
Analysis  pages 251–268. Springer  2017.

[11] Artem Chernodub and Dimitri Nowicki. Norm-preserving orthogonal permutation linear unit

activation functions (oplu). arXiv preprint arXiv:1604.02313  2016.

[12] Moustapha Cisse  Piotr Bojanowski  Edouard Grave  Yann Dauphin  and Nicolas Usunier.
Parseval networks: Improving robustness to adversarial examples. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70  pages 854–863. JMLR. org  2017.

[13] Adam Coates  Andrew Ng  and Honglak Lee. An analysis of single-layer networks in unsuper-
vised feature learning. In Proceedings of the fourteenth international conference on artiﬁcial
intelligence and statistics  pages 215–223  2011.

[14] Jeremy EJ Cohen  Todd Huster  and Ra Cohen. Universal Lipschitz approximation in bounded

depth neural networks. arXiv preprint arXiv:1904.04861  2019.

[15] Jeremy M Cohen  Elan Rosenfeld  and J Zico Kolter. Certiﬁed adversarial robustness via

randomized smoothing. arXiv preprint arXiv:1902.02918  2019.

10

[16] David Cox and Nicolas Pinto. Beyond simple features: A large-scale feature search approach to

unconstrained face recognition. In Face and Gesture 2011  pages 8–15. IEEE  2011.

[17] Francesco Croce and Matthias Hein. Provable robustness against all adversarial lp-perturbations

for p ≥ 1. arXiv preprint arXiv:1905.11213  2019.

[18] Francesco Croce  Maksym Andriushchenko  and Matthias Hein. Provable robustness of relu
networks via maximization of linear regions. In Kamalika Chaudhuri and Masashi Sugiyama 
editors  Proceedings of Machine Learning Research  volume 89 of Proceedings of Machine
Learning Research  pages 2057–2066. PMLR  16–18 Apr 2019. URL http://proceedings.
mlr.press/v89/croce19a.html.

[19] Mevlana Gemici  Zeynep Akata  and Max Welling. Primal-dual wasserstein gan. arXiv preprint

arXiv:1805.09575  2018.

[20] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[21] Henry Gouk  Eibe Frank  Bernhard Pfahringer  and Michael Cree. Regularisation of neural

networks by enforcing Lipschitz continuity. arXiv preprint arXiv:1804.04368  2018.

[22] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville.
Improved training of wasserstein gans. In Advances in neural information processing systems 
pages 5767–5777  2017.

[23] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classiﬁer
against adversarial manipulation. In Advances in Neural Information Processing Systems  pages
2266–2276  2017.

[24] Todd Huster  Cho-Yu Jason Chiang  and Ritu Chadha. Limitations of the Lipschitz constant as
a defense against adversarial examples. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases  pages 16–29. Springer  2018.

[25] Jörn-Henrik Jacobsen  Arnold W.M. Smeulders  and Edouard Oyallon. i-RevNet: Deep in-
vertible networks. In International Conference on Learning Representations  2018. URL
https://openreview.net/forum?id=HJsjkMb0Z.

[26] Hyeonwoo Kang. pytorch-generative-model-collections  2016. URL https://github.com/

znxlwm/pytorch-generative-model-collections.

[27] Guy Katz  Clark Barrett  David L Dill  Kyle Julian  and Mykel J Kochenderfer. Reluplex:
An efﬁcient SMT solver for verifying deep neural networks. In International Conference on
Computer Aided Veriﬁcation  pages 97–117. Springer  2017.

[28] Jaroslav Kautsky and Radka Turcajová. A matrix approach to discrete wavelets. In Wavelet

Analysis and Its Applications  volume 5  pages 117–135. Elsevier  1994.

[29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[30] Alex Krizhevsky  Vinod Nair  and Geoffrey Hinton. CIFAR-10 (canadian institute for advanced

research). URL http://www.cs.toronto.edu/~kriz/cifar.html.

[31] Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/ 

1998.

[32] Stamatios Lefkimmiatis  John Paul Ward  and Michael Unser. Hessian schatten-norm regular-
ization for linear inverse problems. IEEE transactions on image processing  22(5):1873–1888 
2013.

[33] Mario Lezcano-Casado and David Martínez-Rubio. Cheap orthogonal constraints in neu-
ral networks: A simple parametrization of the orthogonal and unitary group.
In Kama-
lika Chaudhuri and Ruslan Salakhutdinov  editors  Proceedings of the 36th International
Conference on Machine Learning  volume 97 of Proceedings of Machine Learning Re-
search  pages 3794–3803  Long Beach  California  USA  09–15 Jun 2019. PMLR. URL
http://proceedings.mlr.press/v97/lezcano-casado19a.html.

11

[34] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations  2018. URL https://openreview.net/forum?id=rJzIBfZAb.
[35] John Milnor and James D Stasheff. Characteristic Classes. (AM-76)  volume 76  pages 55–57.

Princeton University Press  1974.

[36] Takeru Miyato  Toshiki Kataoka  Masanori Koyama  and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations 
2018. URL https://openreview.net/forum?id=B1QRgziT-.

[37] Jeffrey Pennington  Samuel Schoenholz  and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in neural information
processing systems  pages 4785–4795  2017.

[38] Jeffrey Pennington  Samuel Schoenholz  and Surya Ganguli. The emergence of spectral
universality in deep networks. In Amos Storkey and Fernando Perez-Cruz  editors  Proceedings
of the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics  volume 84
of Proceedings of Machine Learning Research  pages 1924–1932  Playa Blanca  Lanzarote 
Canary Islands  09–11 Apr 2018. PMLR. URL http://proceedings.mlr.press/v84/
pennington18a.html.

[39] Gabriel Peyré and Marco Cuturi. Computational optimal transport.

arXiv:1803.00567  2018.

arXiv preprint

[40] Haifeng Qian and Mark N. Wegman. L2-nonexpansive neural networks. In International
Conference on Learning Representations  2019. URL https://openreview.net/forum?
id=ByxGSsR9FQ.

[41] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.
[42] Jonas Rauber  Wieland Brendel  and Matthias Bethge. Foolbox: A Python toolbox to benchmark
the robustness of machine learning models. arXiv preprint arXiv:1707.04131  2017. URL
http://arxiv.org/abs/1707.04131.

[43] Hadi Salman  Greg Yang  Jerry Li  Pengchuan Zhang  Huan Zhang  Ilya Razenshteyn  and
Sebastien Bubeck. Provably robust deep learning via adversarially trained smoothed classiﬁers.
arXiv preprint arXiv:1906.04584  2019.

[44] Lukas Schott  Jonas Rauber  Matthias Bethge  and Wieland Brendel. Towards the ﬁrst adver-
sarially robust neural network model on MNIST. In International Conference on Learning
Representations  2019. URL https://openreview.net/forum?id=S1EHOsC9tX.

[45] Hanie Sedghi  Vineet Gupta  and Philip M. Long. The singular values of convolutional layers.
In International Conference on Learning Representations  2019. URL https://openreview.
net/forum?id=rJevYoA9Fm.

[46] Jure Sokoli´c  Raja Giryes  Guillermo Sapiro  and Miguel RD Rodrigues. Robust large margin

deep neural networks. IEEE Transactions on Signal Processing  65(16):4265–4280.

[47] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfel-
low  and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 
2013.

[48] Yusuke Tsuzuku  Issei Sato  and Masashi Sugiyama. Lipschitz-margin training: Scalable
In Advances in Neural

certiﬁcation of perturbation invariance for deep neural networks.
Information Processing Systems  pages 6541–6550  2018.

[49] Eric Wong  Frank Schmidt  Jan Hendrik Metzen  and J Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems  pages 8400–8409  2018.

[50] Kai Y. Xiao  Vincent Tjeng  Nur Muhammad (Mahi) Shaﬁullah  and Aleksander Madry. Training
In International
for faster adversarial robustness veriﬁcation via inducing reLU stability.
Conference on Learning Representations  2019. URL https://openreview.net/forum?
id=BJfIVjAcKm.

12

[51] Lechao Xiao  Yasaman Bahri  Jascha Sohl-Dickstein  Samuel Schoenholz  and Jeffrey Penning-
ton. Dynamical isometry and a mean ﬁeld theory of CNNs: How to train 10 000-layer vanilla
convolutional neural networks. In Jennifer Dy and Andreas Krause  editors  Proceedings of the
35th International Conference on Machine Learning  volume 80 of Proceedings of Machine
Learning Research  pages 5393–5402  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018.
PMLR. URL http://proceedings.mlr.press/v80/xiao18a.html.

[52] Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generaliz-

ability of deep learning. arXiv preprint arXiv:1705.10941  2017.

13

,Qiyang Li
Saminul Haque
Cem Anil
James Lucas
Roger Grosse
Joern-Henrik Jacobsen