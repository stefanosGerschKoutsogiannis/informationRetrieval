2017,Online Learning with a Hint,We study a variant of online linear optimization where the player receives a hint about the loss function at the beginning of each round. The hint is given in the form of a vector that is weakly correlated with the loss vector on that round. We show that the player can benefit from such a hint if the set of feasible actions is sufficiently round. Specifically  if the set is strongly convex  the hint can be used to guarantee a regret of O(log(T))  and if the set is q-uniformly convex for q\in(2 3)  the hint can be used to guarantee a regret of o(sqrt{T}). In contrast  we establish Omega(sqrt{T}) lower bounds on regret when the set of feasible actions is a polyhedron.,Online Learning with a Hint

Ofer Dekel

Microsoft Research

oferd@microsoft.com

Nika Haghtalab

Computer Science Department
Carnegie Mellon University

nika@cmu.edu

Arthur Flajolet

Operations Research Center

Massachusetts Institute of Technology

flajolet@mit.edu

Patrick Jaillet

EECS  LIDS  ORC

Massachusetts Institute of Technology

jaillet@mit.edu

Abstract

We study a variant of online linear optimization where the player receives a hint
about the loss function at the beginning of each round. The hint is given in the
form of a vector that is weakly correlated with the loss vector on that round. We
show that the player can beneﬁt from such a hint if the set of feasible actions is
sufﬁciently round. Speciﬁcally  if the set is strongly convex  the hint can be used to
guarantee a regret of O(log(T ))  and if the set is q-uniformly convex for q ∈ (2  3) 
the hint can be used to guarantee a regret of o(√T ). In contrast  we establish
Ω(√T ) lower bounds on regret when the set of feasible actions is a polyhedron.

1

Introduction

Online linear optimization is a canonical problem in online learning. In this setting  a player attempts
to minimize an online adversarial sequence of loss functions while incurring a small regret  compared
to the best ofﬂine solution. Many online algorithms exist that are designed to have a regret of O(√T )
in the worst case and it has been known that one cannot avoid a regret of Ω(√T ) in the worst case.
While this worst-case perspective on online linear optimization has lead to elegant algorithms and
deep connections to other ﬁelds  such as boosting [9  10] and game theory [4  2]  it can be overly
pessimistic. In particular  it does not account for the fact that the player may have side-information
that allows him to anticipate the upcoming loss functions and evade the Ω(√T ) regret. In this
work  we go beyond this worst case analysis and consider online linear optimization when additional
information in the form of a function that is correlated with the loss is presented to the player.
More formally  online convex optimization [24  11] is a T -round repeated game between a player and
an adversary. On each round  the player chooses an action xt from a convex set of feasible actions
K ⊆ Rd and the adversary chooses a convex bounded loss function ft. Both choices are revealed and
the player incurs a loss of ft(xt). The player then uses its knowledge of ft to adjust its strategy for
the subsequent rounds. The player’s goal is to accumulate a small loss compared to the best ﬁxed
action in hindsight. This value is called regret and is a measure of success of the player’s algorithm.
When the adversary is restricted to Lipschitz loss functions  several algorithms are known to guarantee
O(√T ) regret [24  16  11]. If we further restrict the adversary to strongly convex loss functions  the
regret bound improves to O(log(T )) [14]. However  when the loss functions are linear  no online
algorithm can have a regret of o(√T ) [5]. In this sense  linear loss functions are the most difﬁcult
convex loss functions to handle [24].

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In this paper  we focus on the case where the adversary is restricted to linear Lipschitz loss functions.
More speciﬁcally  we assume that the loss function ft(x) takes the form cT
t x  where ct is a bounded
loss vector in C ⊆ Rd. We further assume that the player receives a hint before choosing the action
on each round. The hint in our setting is a vector that is guaranteed to be weakly correlated with the
loss vector. Namely  at the beginning of round t  the player observes a unit-length vector vt ∈ Rd
t ct ≥ α(cid:107)ct(cid:107)2  and where α is a small positive constant. So long as this requirement is met 
such that vT
the hint could be chosen maliciously  possibly by an adversary who knows how the player’s algorithm
uses the hint. Our goal is to develop a player strategy that takes these hints into account  and to
understand when hints of this type make the problem provably easier and lead to smaller regret.
We show that the player’s ability to beneﬁt from the hints depends on the geometry of the player’s
action set K. Speciﬁcally  we characterize the roundness of the set K using the notion of (C  q)-
uniform convexity for convex sets. In Section 3  we show that if K is a (C  2)-uniformly convex
set (or in other words  if K is a C-strongly convex set)  then we can use the hint to design a player
a polynomial dependence on the dimension d and other constants. Furthermore  as we show in
Section 4  if K is a (C  q)-uniformly convex set for q ∈ (2  3)  we can use the hint to improve the
  when the hint belongs to a small set of possible hints at every step.
regret to O

strategy that improves the regret guarantee to O(cid:0)(Cα)−1 log(T )(cid:1)  where our O(·) notation hides

(cid:16)

(cid:17)

(Cα)

1

1−q T

2−q
1−q

In Section 5  we prove lower bounds on the regret of any online algorithm in this model. We ﬁrst
show that when K is a polyhedron  such as a L1 ball  even a stronger form of hint cannot guarantee a
regret of o(√T ). Next  we prove a lower bound of Ω(log(T )) regret when K is strongly convex.
1.1 Comparison with Other Notions of Hints

(cid:113)(cid:80)T

The notion of hint that we introduce in this work generalizes some of the notions of predictabil-
ity on online learning. Hazan and Megiddo [13] considered as an example a setting where
the player knows the ﬁrst coordinate of the loss vector at all rounds  and showed that when
|ct1| ≥ α and when the set of feasible actions is the Euclidean ball  one can achieve a regret
of O(1/α · log(T )). Our work directly improves over this result  as in our setting a hint vt = ±e1
also achieves O(1/α · log(T )) regret  but we can deal with hints in different directions at differ-
ent rounds and we allow for general uniformly convex action sets. Rakhlin and Sridharan [20]
considered online learning with predictable sequences  with a notion of predictability that is con-
cerned with the gradient of the convex loss functions. They show that if the player receives a
hint Mt at round t  then the regret of the algorithm is at most O(
∗).
t=1 (cid:107)∇ft(xt) − Mt(cid:107)2
In the case of linear loss functions  this implies that having an estimate vector
c(cid:48)t of the loss vector within distance σ of the true loss vector ct results in
an improved regret bound of O(σ√T ). In contrast  we consider a notion of
hint that pertains to the direction of the loss vector rather than its location.
Our work shows that merely knowing whether the loss vector positively or
negatively correlates with another vector is sufﬁcient to achieve improved
regret bound  when the set is uniformly convex. That is  rather than having
access to an approximate value of ct  we only need to have access to a
Figure 1: Comparison
halfspace that classiﬁes ct correctly with a margin. This notion of hint is
between notions of hint.
weaker that the notion of hint in the work of Rakhlin and Sridharan [20]
when the approximation error satisﬁes (cid:107)ct − c(cid:48)t(cid:107)2 ≤ σ · (cid:107)ct(cid:107)2 for σ ∈ [0  1). In this case one can
use c(cid:48)t/(cid:107)c(cid:48)t(cid:107)2 as the direction of the hint in our setting and achieve a regret of O( 1
1−σ log T ) when
the set is strongly convex. This shows that when the set of feasible actions is strongly convex  a
directional hint can improve the regret bound beyond what has been known to be achievable by an
approximation hint. However  we note that our results require the hints to be always valid  whereas
the algorithm of Rakhlin and Sridharan [19] can adapt to the quality of the hints.
We discuss these works and other related works  such as [15]  in more details in Appendix A.

2 Preliminaries

We begin with a more formal deﬁnition of online linear optimization (without hints). Let A denote the
player’s algorithm for choosing its actions. On round t the player uses A and all of the information

2

xyxvcˆc(x)vcwˆyˆxz=x+y2ˆc(z)ˆc(x)+ˆc(y)2cc0↵vit has observed so far to choose an action xt in a convex compact set K ⊆ Rd. Subsequently  the
adversary chooses a loss vector ct in a compact set C ⊆ Rd. The player and the adversary reveal their
actions and the player incurs the loss cT

t xt. The player’s regret is deﬁned as

T(cid:88)

t=1

T(cid:88)

t=1

R(A  c1:T ) =

cT
t xt − min
x∈K

cT
t x.

t ct ≥ α(cid:107)ct(cid:107)2  for some α > 0.

In online linear optimization with hints  the player observes vt ∈ Rd with (cid:107)vt(cid:107)2 = 1  before choosing
xt  with the guarantee that vT
We use uniform convexity to characterize the degree of convexity of the player’s action set K.
Informally  uniform convexity requires that the convex combination of any two points x and y on the
boundary of K be sufﬁciently far from the boundary. A formal deﬁnition is given below.
Deﬁnition 2.1 (Pisier [18]). Let K be a convex set that is symmetric around the origin. K and the
Banach space deﬁned by K are said to be uniformly convex if for any 0 <  < 2 there exists a
δ > 0 such that for any pair of points x  y ∈ K with (cid:107)x(cid:107)K ≤ 1 (cid:107)y(cid:107)K ≤ 1 (cid:107)x − y(cid:107)K ≥   we have

K ≤ 1 − δ. The modulus of uniform-convexity δK() is the best possible δ for that   i.e. 

(cid:13)(cid:13) x+y

(cid:13)(cid:13)

2

(cid:27)

δK() = inf

1 −

: (cid:107)x(cid:107)K ≤ 1 (cid:107)y(cid:107)K ≤ 1 (cid:107)x − y(cid:107)K ≥ 

.

(cid:26)

(cid:13)(cid:13)(cid:13)(cid:13) x + y

2

(cid:13)(cid:13)(cid:13)(cid:13)

K

For brevity  we say that K is (C  q)-uniformly convex when δK() = Cq and we omit C when it is
clear from the context.
Examples of uniformly convex sets include Lp balls for any 1 < p < ∞ with modulus of convexity
δLp () = Cpp for p ≥ 2 and a constant Cp and δLp () = (p − 1)2 for 1 < p ≤ 2. On the other
hand  L1 and L∞ units balls are not uniformly convex. When δK() ∈ Θ(2)  we say that K is
strongly convex.
Another notion of convexity we use in this work is called exp-concavity. A function f : K → R is
exp-concave with parameter β > 0  if exp(−βf (x)) is a concave function of x ∈ K. This is a weaker
requirement than strong convexity when the gradient of f is uniformly bounded [14]. The next
proposition shows that we can obtain regret bounds of order Θ(log(T )) in online convex optimization
when the loss functions are exp-concave with parameter β.
Proposition 2.2 ([14]). Consider online convex optimization on a sequence of loss functions
f1  . . .   fT over a feasible set K ⊆ Rd  such that all t  ft : K → R is exp-concave with pa-
rameter β > 0. There is an algorithm  with runtime polynomial in d  which we call AEXP  with a
regret that is at most d

β (1 + log(T + 1)).

Throughout this work  we draw intuition from basic orthogonal geometry. Given any vector x and a
hint v  we deﬁne x v = (xTv)v and x T v = x−(xTv)v  as the parallel and the orthogonal components
of x with respect to v. When the hint v is clear from the context we simply use x and x T to denote
these vectors.
Naturally  our regret bounds involve a number of geometric parameters. Since the set of actions of the
adversary C is compact  we can ﬁnd G ≥ 0 such that (cid:107)c(cid:107)2 ≤ G for all c ∈ C. When K is uniformly
convex  we denote K = {w ∈ Rd | (cid:107)w(cid:107)K ≤ 1}. In this case  since all norms are equivalent in ﬁnite
dimension  there exist R > 0 and r > 0 such that Br ⊆ K ⊆ BR  where Br (resp. BR) denote the
L2 unit ball centered at 0 with radius r (resp. R). This implies that 1
Improved Regret Bounds for Strongly Convex K

R (cid:107)·(cid:107)2 ≤ (cid:107)·(cid:107)K ≤ 1

r (cid:107)·(cid:107)2.

3

At ﬁrst sight  it is not immediately clear how one should use the hint. Since vt is guaranteed to satisfy
t vt ≥ α(cid:107)ct(cid:107)2  moving the action x in the direction −vt always decreases the loss. One could hope
cT
to get the most beneﬁt out of the hint by choosing xt to be the extremal point in K in the direction
−vt. However  this naïve strategy could lead to a linear regret in the worst case. For example  say
2 ) and vt = (0  1) for all t and let K be the Euclidean unit ball. Choosing xt = −vt
that ct = (1  1
would incur a loss of − T
2   while the best ﬁxed action in hindsight  the point ( −2√5   −1√5 )  would incur a
loss of −√5

2 T . The player’s regret would therefore be √5−1

2 T .

3

Intuitively  the ﬂaw of this naïve strategy is that the hint does not give the player any information
about the (d − 1)-dimensional subspace orthogonal to vt. Our solution is to use standard online
learning machinery to learn how to act in this orthogonal subspace. Speciﬁcally  on round t  we use
vt to deﬁne the following virtual loss function:

ˆct(x) = min
w∈K

t w s.t. w T vt = x T vt
cT

.

In words  we consider the 1-dimensional subspace spanned by vt
and its (d − 1)-dimensional orthogonal subspace separately. For any
action x ∈ K  we ﬁnd another point  w ∈ K  that equals x in the
(d − 1)-dimensional orthogonal subspace  but otherwise incurs the
optimal loss. The value of the virtual loss ˆct(x) is deﬁned to be the
value of the original loss function ct at w. The virtual loss simulates
the process of moving x as far as possible in the direction −vt without
changing its value in any other direction (see Figure 2). This can be
more formally seen by the following equation.

(cid:0)(c T
t )T ˆx T + (ct )Tw (cid:1) = arg min
(cid:13)(cid:13)2 vt since the hint is valid.
(cid:13)(cid:13)ct

w∈K:w T =ˆx T

vT
t w 

(1)

Figure 2: Virtual function ˆc(·).

arg min
w∈K:w T =ˆx T

cT
t w = arg min
w∈K:w T =ˆx T

where the last transition holds by the fact that ct =
This provides an intuitive understanding of a measure of convexity
of our virtual loss functions. When K is uniformly convex then the
function ˆct(·) demonstrates convexity in the subspace orthogonal
to vt. To see that  note that for any x and y that lie in the space
orthogonal to vt  their mid point x+y
transforms to a point that
2
is farther away in the direction of −vt than the midpoint of the
transformations of x and y. As shown in Figure 3  the modulus
of uniform convexity of K affects the degree of convexity of
ˆct(·). We note  however  that ˆct(·) is not strongly convex in
all directions. In fact  ˆct(·) is constant in the direction of vt.
Nevertheless  the properties shown here allude to the fact that
ˆct(·) demonstrates some notion of convexity. As we show in the
next lemma  this notion is indeed exp-concavity:
Lemma 3.1. If K is (C  2)-uniformly convex  then ˆct(·) is 8 α·C·r
Proof. Let γ = 8 α·C·r
G·R2 . Without loss of generality  we assume that ct (cid:54)= 0  otherwise ˆct(·) = 0 is a
constant function and the proof follows immediately. Based on the above discussion  it is not hard to
see that ˆct(·) is continuous (we prove this in more detail in the Appendix D.1. So  to prove that ˆct(·)
is exp-concave  it is sufﬁcient to show that

Figure 3: Uniform-convexity of the
feasible set affects the convexity the
virtual loss function.

G·R2 -exp-concave.

(cid:18)

(cid:18) x + y

(cid:19)(cid:19)

exp

−γ · ˆct

2

1
2

≥

exp (−γ · ˆct(x)) +

1
2

exp (−γ · ˆct(y))

∀(x  y) ∈ K.

Consider (x  y) ∈ K and choose corresponding (ˆx  ˆy) ∈ K such that ˆct(x) = cT
Without loss of generality  we have (cid:107)ˆx(cid:107)K
that are extreme points of K. Since exp(−γˆct(·)) is decreasing in ˆct(·)  we have

t ˆy.
= 1  as we can always choose corresponding ˆx  ˆy

t ˆx and ˆct(y) = cT

= (cid:107)ˆy(cid:107)K

(cid:18)

(cid:18) x + y

(cid:19)(cid:19)

exp

−γ · ˆct

=

max
(cid:107)w(cid:107)K≤1
w T vt =( x+y

2 ) T vt

exp(−γ · cT

t w).

2 − δK((cid:107)ˆx − ˆy(cid:107)K

)

vt

(cid:107)vt(cid:107)K

satisﬁes (cid:107)w(cid:107)K ≤ 1  since (cid:107)w(cid:107)K ≤

+
2 ) T vt. So  by using this w in

K

) ≤ 1 (see also Figure 3). Moreover  w T vt = ( x+y

Note that w = ˆx+ˆy
δK((cid:107)ˆx − ˆy(cid:107)K
Equation (2)  we have

(cid:18)

(cid:18) x + y

(cid:19)(cid:19)

exp

−γ · ˆct

2

≥ exp

−

γ
2 · (cT

t ˆx + cT

t ˆy) + γ ·

cT
t vt
(cid:107)vt(cid:107)K

4

· δK((cid:107)ˆx − ˆy(cid:107)K

)

.

(3)

2

(cid:18)

(2)

(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13) ˆx+ˆy
(cid:19)

2

xyˆxˆyzxvcˆc(x)ˆzvcˆc(ˆz)12(ˆc(ˆx)+ˆc(ˆy))z0xyxvcˆc(x)vcwˆyˆxz=x+y2ˆc(z)ˆc(x)+ˆc(y)2γ ·

(cid:18)

cT
t vt
(cid:107)vt(cid:107)K

(cid:18)
(cid:19)
On the other hand  since (cid:107)vt(cid:107)K ≤ 1
r and (cid:107)ˆx − ˆy(cid:107)K ≥ 1
r(cid:107)vt(cid:107)2 = 1
(cid:32)
γ · r · α · (cid:107)ct(cid:107)2 · C ·
≥ exp
· δK((cid:107)ˆx − ˆy(cid:107)K
)
exp
(cid:18) (γ/2)2 · (cT
(cid:16) γ

· (cid:107)ct(cid:107)2 ·
t ˆy)2
t ˆx − cT
2

(cid:19)
R(cid:107)ˆx − ˆy(cid:107)2  we have
(cid:18) cT
R2 · (cid:107)ˆx − ˆy(cid:107)2
cT
t ˆy
(cid:19)
(cid:107)ct(cid:107)2 −
(cid:107)ct(cid:107)2
(cid:16) γ

α · C · r

≥ exp

≥ exp

(cid:17)

γ ·

R2

t ˆx

1

2

(cid:19)2(cid:33)

(cid:17)

1
2 · exp

≥

2 · (cT

t ˆx − cT

t ˆy)

+

1
2 · exp

2 · (cT

t ˆy − cT

t ˆx)

 

where the penultimate inequality follows by the deﬁnition of γ and the last inequality is a consequence
2 exp(−z) ∀z ∈ R. Plugging the last inequality into (3)
(cid:18)
of the inequality exp(z2/2) ≥ 1
(cid:17)(cid:111)
(cid:17)
(cid:16)
yields

2 exp(z) + 1

(cid:19)

(cid:110)

(cid:17)

(cid:16) γ

(cid:16) γ

x+y

exp

(cT

t ˆx + cT

t ˆy)

exp

·

(cT
t ˆx − cT

t ˆy)

2

+ exp

(cT
t ˆy − cT

t ˆx)

2

exp

−γˆct(

2

)

γ
2

−

≥

=

=

1
2
1
2
1
2

t ˆy) +

1
exp (−γ · cT
2
exp (−γ · ˆct(y)) +

t ˆx)

exp (−γ · cT
1
exp (−γ · ˆct(x))  
2

which concludes the proof.

Now  we use the sequence of virtual loss functions to reduce our problem to a standard online convex
optimization problem (without hints). Namely  the player applies AEXP (from Proposition 2.2) 
which is an online convex optimization algorithm known to have O(log(T )) regret with respect to
exp-concave functions  to the sequence of virtual loss functions. Then our algorithm takes the action
ˆxt ∈ K that is prescribed by AEXP and moves it as far as possible in the direction of −vt. This
process is formalized in Algorithm 1.
Algorithm 1 Ahint FOR STRONGLY CONVEX K
For t = 1  . . .   T  

1. Use Algorithm AEXP with the history ˆcτ (·) for τ < t  and let ˆxt be the chosen action.
2. Let xt = arg minw∈K(vT

. Play xt and receive ct as feedback.

t w) s.t. w T vt = ˆx T vt

t

Next  we show that the regret of algorithm AEXP on the sequence of virtual loss functions is an upper
bound on the regret of Algorithm 1.
Lemma 3.2. For any sequence of loss functions c1  . . .   cT   let R(Ahint  c1:T ) be the regret of
algorithm Ahint on the sequence c1  . . .   cT   and R(AEXP  ˆc1:T ) be the regret of algorithm AEXP
on the sequence of virtual loss functions ˆc1  . . .   ˆcT . Then  R(Ahint  c1:T ) ≤ R(AEXP  ˆc1:T ).
t w) s.t. w T vt = ˆx T vt
Proof. Equation (1) provides an equivalent deﬁnition xt = arg minw∈K(cT
.
Using this  we show that the loss of algorithm Ahint on the sequence c1:T is the same as the loss of
algorithm AEXP on the sequence ˆc1:T .
min

T(cid:88)

T(cid:88)

T(cid:88)

T(cid:88)

ˆct(ˆxt) =

t

cT
t w) =

cT
t w =

cT
t xt.

t=1

t=1

w∈K:w T =ˆx T

t

cT
t ( arg min
w∈K:w T =ˆx T

t

t=1

t=1

Next  we show that the ofﬂine optimal on the sequence ˆc1:T is more competitive that the ofﬂine
optimal on the sequence c1:T . First note that for any x and t  ˆct(x) = minw∈K:w T =x T cT
t x.
t w ≤ cT
Therefore  minx∈K

t x. The proof concludes by

t=1 ˆct(x) ≤ minx∈K

t=1 cT

(cid:80)T

(cid:80)T
T(cid:88)

R(Ahint  c1:T ) =

cT
t xt − min
x∈K

t=1

cT
t x ≤

ˆct(ˆxt) − min
x∈K

ˆct(x) = R(AEXP  ˆc1:T ).

T(cid:88)

t=1

T(cid:88)

t=1

T(cid:88)

t=1

5

Our main result follows from the application of Lemmas 3.1 and 3.2.
Theorem 3.3. Suppose that K ⊆ Rd is a (C  2)-uniformly convex set that is symmetric around the
origin  and Br ⊆ K ⊆ BR for some r and R. Consider online linear optimization with hints where
the cost function at round t is (cid:107)ct(cid:107)2 ≤ G and the hint vt is such that cT
t vt ≥ α(cid:107)ct(cid:107)2  while (cid:107)vt(cid:107)2 = 1.
Algorithm 1 in combination with AEXP has a worst-case regret of

R(Ahint  c1:T ) ≤

d · G · R2
8α · C · r · (1 + log(T + 1)).

Since AEXP requires the coefﬁcient of exp-concavity to be given as an input  α needs to be known
a priori to be able to use Algorithm 1. However  we can use a standard doubling trick to relax this
requirement and derive the same asymptotic regret bound. We defer the presentation of this argument
to Appendix B.

4

Improved Regret Bounds for (C  q)-Uniformly Convex K

2−q

In this section  we consider any feasible set K that is (C  q)-uniformly convex for q ≥ 2. Our results
differ from the previous section in two aspects. First  our algorithm can be used with (C  q)-uniformly
convex feasible sets for any q ≥ 2 compared to the results of the previous section that only hold for
strongly convex sets (q = 2). On the other hand  the approach in this section requires the hints to be
restricted to a ﬁnite set of vectors V. We show that when K is (C  q)-uniformly convex for q > 2 
1−q ). If q ∈ (2  3)  this is an improvement over the worst case regret of O(√T )
our regret is O(T
guaranteed in the absence of hints.
We ﬁrst consider the scenario where the hint is always pointing in the same direction  i.e. vt = v for
some v and all t ∈ [T ]. In this case  we show how one can use a simple algorithm that picks the best
performing action so far (a.k.a the Follow-The-Leader algorithm) to obtain improved regret bounds.
We then consider the case where the hint belongs to a ﬁnite set V. In this case  we instantiate one
copy of the Follow-The-Leader algorithm for each v ∈ V and combine their outcomes in order to
obtain improved regret bounds that depend on the cardinality of V  which we denote by |V|.
(cid:80)
Lemma 4.1. Suppose that vt = v for all t = 1 ···   T and that K is (C  q)-uniformly convex that is
symmetric around the origin  and Br ⊆ K ⊆ BR for some r and R. Consider the algorithm  called
Follow-The-Leader (FTL)  that at every round t  plays xt ∈ arg minx∈K
τ v ≥ 0
T(cid:88)
for all t = 1 ···   T   then the regret is bounded as follows 
2(cid:80)t
(cid:107)ct(cid:107)q
·
τ =1 cT
t v ≥ α · (cid:107)ct(cid:107)2 for all t = 1 ···   T   the

t=1
Furthermore  when v is a valid hint with margin α  i.e.  cT
right-hand side can be further simpliﬁed to obtain the regret bound:

τ x. If(cid:80)t
(cid:33)1/(q−1)

(cid:19)1/(q−1)

R(AFTL  c1:T ) ≤

(cid:107)v(cid:107)K · Rq

τ =1 cT

(cid:32)

τ <t cT

(cid:18)

2C

.

τ v

R(AFTL  c1:T ) ≤

1
2γ · G · (ln(T ) + 1)

if q = 2

and

R(AFTL  c1:T ) ≤

where γ = C·α

(cid:107)v(cid:107)K·Rq .

1

(2γ)1/(q−1) · G ·

q−2
q−1

q − 1
q − 2 · T

if q > 2 

Proof. We use a well-known inequality  known as FT(R)L Lemma (see e.g.  [12  17])  on the regret
incurred by the FTL algorithm:

T(cid:88)
Without loss of generality  we can assume that (cid:107)xt(cid:107)K
function is attained at a boundary point. Since K is (C  q)-uniformly convex  we have

cT
t (xt − xt+1).
= (cid:107)xt+1(cid:107)K

= 1 since the maximum of a linear

R(AFTL  c1:T ) ≤

t=1

(cid:13)(cid:13)(cid:13)(cid:13) xt + xt+1

2

(cid:13)(cid:13)(cid:13)(cid:13)

K

≤ 1 − δK((cid:107)xt − xt+1(cid:107)K

).

6

(cid:13)(cid:13)(cid:13)(cid:13)

K

v
(cid:107)v(cid:107)K

)

(cid:19)

≤ 1.

t(cid:88)

τ =1

2

This implies that

− δK((cid:107)xt − xt+1(cid:107)K
τ =1 cτ . So  we have

(cid:13)(cid:13)(cid:13)(cid:13) xt + xt+1
Moreover  xt+1 ∈ arg minx∈K xT(cid:80)t
Rearranging this last inequality and using the fact that(cid:80)t
(cid:32) t(cid:88)
(cid:80)t
By deﬁnition of FTL  we have xt ∈ arg minx∈K xT(cid:80)t−1

(cid:32) t(cid:88)
(cid:33)T(cid:18) xt + xt+1
(cid:33)T(cid:18) xt − xt+1
(cid:19)

≥ δK((cid:107)xt − xt+1(cid:107)K

− δK((cid:107)xt − xt+1(cid:107)K

v
(cid:107)v(cid:107)K

) ·

τ =1

τ =1

cτ

cτ

2

2

)

(cid:33)T

t(cid:88)
(cid:32) t(cid:88)

τ =1

τ =1

(cid:33)

vTcτ

.

xT

≥ inf
x∈K

cτ = xT

t+1

cτ .

τ =1 vTcτ ≥ 0  we obtain:
C · (cid:107)xt − xt+1(cid:107)q

τ =1 vTcτ
(cid:107)v(cid:107)K
τ =1 cτ   which implies:

(cid:107)v(cid:107)K · Rq

≥

2

·

(cid:32)t−1(cid:88)
(cid:33)

cτ

xt+1 − xt

2

≥ 0.

cT
t

τ =1

(cid:19)

(cid:32) t(cid:88)

Summing up the last two inequalities and setting γ = C·α

(cid:18) xt − xt+1
γ
Rearranging this last inequality and using the fact that(cid:80)t
· (cid:107)xt − xt+1(cid:107)q
α ·
(cid:32)
τ =1 vTcτ ≥ 0  we obtain:
2(cid:80)t
(cid:107)ct(cid:107)q
τ =1 vTcτ

(cid:33)
(cid:107)v(cid:107)K·Rq   we derive:
vTcτ
(cid:33)1/(q−1)

|cT
t (xt − xt+1)| ≤

(2γ/α)1/(q−1) ·

(cid:32) t(cid:88)

γ
α ·

2 ≥

vTcτ

≥

τ =1

τ =1

2

1

·

.

t (xt − xt+1))q
(cT

(cid:107)ct(cid:107)q

2

.

(4)

Summing (4) over all t completes the proof of the ﬁrst claim. The regret bounds for when vTct ≥
α · (cid:107)ct(cid:107)2 for all t = 1 ···   T follow from the ﬁrst regret bound. We defer this part of the proof to
Appendix D.2.

Note that the regret bounds become O(T ) when q → ∞. This is expected because Lq balls are
q-uniformly convex for q ≥ 2 and converge to L∞ balls as q → ∞ and it is well-known that
Follow-The-Leader yields Θ(T ) regret in online linear optimization when K is a L∞ ball.
Using the above lemma  we introduce an algorithm for online linear optimization with hints that
belong to a set V. In this algorithm  we instantiate one copy of the FTL algorithm for each possible
direction of the hint. On round t  we invoke the copy of the algorithm that corresponds to the direction
of the hint vt  using the history of the game for rounds with hints in that direction. We show that the
overall regret of this algorithm is no larger than the sum of the regrets of the individual copies.
Algorithm 2 Aset: SET-OF-HINTS
(cid:80)
For all v ∈ V  let Tv = ∅.
For t = 1  . . .   T  
1. Play xt ∈ arg minx∈K
2. Update Tvt ← Tvt ∪ {t}.

τ x and receive ct as feedback.
cT

τ∈Tvt

Theorem 4.2. Suppose that K ⊆ Rd is a (C  q)-uniformly convex set that is symmetric around the
origin  and Br ⊆ K ⊆ BR for some r and R. Consider online linear optimization with hints where
the cost function at round t is (cid:107)ct(cid:107)2 ≤ G and the hint vt comes from a ﬁnite set V and is such that
t vt ≥ α(cid:107)ct(cid:107)2  while (cid:107)vt(cid:107)2 = 1. Algorithm 2 has a worst-case regret of
cT

R(Aset  c1:T ) ≤ |V| ·

and

R(Aset  c1:T ) ≤ |V| ·

R2

(cid:18) Rq

(cid:19)1/(q−1)

2C · α · r · G · (ln(T ) + 1) 
q − 1
q − 2 · T

· G ·

2C · α · r

if q = 2 

q−2
q−1

if q > 2.

7

Proof. We decompose the regret as follows:

R(Aset  c1:T ) =

cT
cT
t xt − inf
t x ≤
x∈K
R(AFTL  cTv ).

t=1

≤ |V| · max
v∈V

T(cid:88)

t=1

T(cid:88)

(cid:40)(cid:88)

(cid:88)

v∈V

t∈Tv

(cid:41)

cT
t x

(cid:88)

t∈Tv

cT
t xt − inf
x∈K

The proof follows by applying Lemma 4.1 and by using (cid:107)vt(cid:107)K ≤ (1/r) · (cid:107)vt(cid:107)2 = 1/r.

Note that Aset does not require α or V to be known a priori  as it can compile the set of hint directions
as it sees new ones. Moreover  if the hints are not limited to ﬁnite set V a priori  then the algorithm
can ﬁrst discretize the L2 unit ball with an α/2-net and approximate any given hint with one of the
hints in the discretized set. Using this discretization technique  Theorem 4.2 can be extended to the
setting where the hints are not constrained to a ﬁnite set while having a regret that is linear in the size
of the α/2-net (exponential in the dimension d.) Extensions of Theorem 4.2 are discussed in more
details in the Appendix C.

5 Lower Bounds

The regret bounds derived in Sections 3 and 4 suggest that the curvature of K can make up for the
lack of curvature of the loss function to get rates faster than O(√T ) in online convex optimization 
provided we receive additional information about the next move of the adversary in the form of a
hint. In this section  we show that the curvature of the player’s decision set K is necessary to get rates
better than O(√T )  even in the presence of a hint.
As an example  consider the unit cube  i.e. K = {x | (cid:107)x(cid:107)∞ ≤ 1}. Note that this set is not uniformly
convex. Since  the ith coordinate of points in such a set  namely xi  has no effect on the range of
acceptable values for the other coordinates  revealing one coordinate does not give us any information
about the other coordinates xj for j (cid:54)= i. For example  suppose that ct has each of its ﬁrst two
coordinates set to +1 or −1 with equal probability and all other coordinates set to 1. In this case 
even after observing the last d − 2 coordinates of the loss vector  the problem is reduced to a standard
online linear optimization problem in the 2-dimensional unit cube. This choice of ct is known to
incur a regret of Ω(√T ) [1]. Therefore  online linear optimization with the set K = {x | (cid:107)x(cid:107)∞ ≤ 1} 
even in the presence of hints  has a worst-case regret of Ω(√T ). As it turns out  this result holds for
any polyhedral set of actions. We prove this by means of a reduction to the lower bounds established
in [8] that apply to the online convex optimization framework (without hint). We defer the proof to
the Appendix D.4.
Theorem 5.1. If the set of feasible actions is a polyhedron then  depending on the set C  either there
exists a trivial algorithm that achieves zero regret or every online algorithm has worst-case regret
Ω(√T ). This is true even if the adversary is restricted to pick a ﬁxed hint vt = v for all t = 1 ···   T .
At ﬁrst sight  this result may come as a surprise. After all  since any Lp ball with 1 < p ≤ 2 is
strongly convex  one can hope to use a L1+ν unit ball K(cid:48) to approximate K when K is a L1 ball
(which is a polyhedron) and apply the results of Section 3 to achieve better regret bounds. The
problem with this approach is that the constant in the modulus of convexity of K(cid:48) deteriorates when
p → 1 since δLp () = (p − 1) · 2  see [3]. As a result  the regret bound established in Theorem 3.3
becomes O( 1
p−1 · log T ). Since the best approximation of a L1 unit ball using a Lp ball is of the
form {x ∈ Rd | d1− 1
p(cid:107)x(cid:107)p ≤ 1}  the distance between the ofﬂine benchmark in the deﬁnition
p−1) · T   which translates into an
of regret when using K(cid:48) instead of K can be as large as (1 − d
p−1) · T in the regret bound when using K(cid:48) as a proxy for K. Due to
additive term of order (1 − d
the inverse dependence of the regret bound obtained in Theorem 3.3 on p − 1  the optimal choice of
p = 1 + ˜O( 1√T
Finally  we conclude with a result that suggests that O(log(T )) is  in fact  the optimal achievable
regret when K is strongly convex in online linear optimization with a hint. We defer the proof to the
Appendix D.4.

) leads to a regret of order ˜O(√T ).

1

1

8

Theorem 5.2. If K is a L2 ball then  depending on the set C  either there exists a trivial algorithm
that achieves zero regret or every online algorithm has worst-case regret Ω(log(T )). This is true
even if the adversary is restricted to pick a ﬁxed hint vt = v for all t = 1 ···   T .
6 Directions for Future Research

We conjecture that the dependence of our regret bounds with respect to T is suboptimal when K is
(C  q)-uniformly convex for q > 2. We expect the optimal rate to converge to √T when q → ∞ as
Lq balls converge to L∞ balls and it is well known that the minimax regret scales as √T in online
linear optimization without hints when the decision set is a L∞ ball. However  this calls for the
development of an algorithm that is not based on a reduction to the Follow-The-Leader algorithm  as
discussed after Lemma 4.1.
We also conjecture that it is possible to relax the assumption that there are ﬁnitely many hints when
K is (C  q)-uniformly convex with q > 2 without incurring an exponential dependence of the regret
bounds (and the runtime) on the dimension d  see Appendix C. Again  this calls for the development
of an algorithm that is not based on a reduction to the Follow-The-Leader algorithm.
A solution that would alleviate the two aforementioned shortcomings would likely be derived through
a reduction to online convex optimization with convex functions that are (C  q)-uniformly convex 
for q ≥ 2  in all but one direction and constant in the other  in a similar fashion as done in Section
3 when q = 2. There has been progress in this direction in the literature  but  to the best of our
knowledge  no conclusive result yet. For instance  Vovk [23] studies a related problem but restricts
the study to the squared loss function. It is not clear if the setting studied in this paper can be reduced
to the setting of square loss function. Another example is given by [21]  where the authors consider
online convex optimization with general (C  q)-uniformly convex functions in Banach spaces (with
no hint) achieving a regret of order O(T (q−2)/(q−1)). Note that this rate matches the one derived in
Theorem 4.2. However  as noted above  our setting cannot be reduced to theirs because our virtual
loss functions are not uniformly convex in every direction.

Acknowledgments

Haghtalab was partially funded by an IBM Ph.D. fellowship and a Microsoft Ph.D. fellowship. Jaillet
acknowledges the research support of the Ofﬁce of Naval Research (ONR) grant N00014-15-1-2083.
This work was partially done when Haghtalab was an intern at Microsoft Research  Redmond WA.

References
[1] Jacob Abernethy  Peter L Bartlett  Alexander Rakhlin  and Ambuj Tewari. Optimal strategies and minimax
lower bounds for online convex games. In Proceedings of the 21st Conference on Learning Theory (COLT) 
pages 415–424  2008.

[2] Sanjeev Arora  Elad Hazan  and Satyen Kale. The multiplicative weights update method: a meta-algorithm

and applications. Theory of Computing  8(1):121–164  2012.

[3] Keith Ball  Eric A Carlen  and Elliott H Lieb. Sharp uniform convexity and smoothness inequalities for

trace norms. Inventiones mathematicae  115(1):463–482  1994.

[4] Avrim Blum and Yishay Monsour. Learning  regret minimization  and equilibria. In Algorithmic Game

Theory  pages 79–102. 2007.

[5] Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction  learning  and games. Cambridge university press 

2006.

[6] Chao-Kai Chiang and Chi-Jen Lu. Online learning with queries. In Proceedings of the 21st Annual

ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 616–629  2010.

[7] Chao-Kai Chiang  Tianbao Yang  Chia-Jung Lee  Mehrdad Mahdavi  Chi-Jen Lu  Rong Jin  and Shenghuo
Zhu. Online optimization with gradual variations. In Proceedings of the 25th Conference on Learning
Theory (COLT)  pages 6–1  2012.

[8] Arthur Flajolet and Patrick Jaillet. No-regret learnability for piecewise linear losses. arXiv preprint

arXiv:1411.5649  2014.

9

[9] Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation  121(2):

256–285  1995.

[10] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of computer and system sciences  55(1):119–139  1997.

[11] Elad Hazan. The convex optimization approach to regret minimization. Optimization for machine learning 

pages 287–303  2012.

[12] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs.

In Proceedings of the 23th Conference on Learning Theory (COLT)  2008.

[13] Elad Hazan and Nimrod Megiddo. Online learning with prior knowledge. In Proceedings of the 20th

Conference on Learning Theory (COLT)  pages 499–513  2007.

[14] Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online convex optimization.

Machine Learning  69(2-3):169–192  2007.

[15] Ruitong Huang  Tor Lattimore  András György  and Csaba Szepesvári. Following the leader and fast
rates in linear prediction: Curved constraint sets and other regularities. In Proceedings of the 30th Annual
Conference on Neural Information Processing Systems (NIPS)  pages 4970–4978  2016.

[16] Adam Kalai and Santosh Vempala. Efﬁcient algorithms for online decision problems. Journal of Computer

and System Sciences  71(3):291–307  2005.

[17] H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. Journal of

Machine Learning Research  18:1–50  2017.

[18] Gilles Pisier. Martingales in banach spaces (in connection with type and cotype). Manuscript.  Course IHP 

Feb  pages 2–8  2011.

[19] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Proceedings of

the 25th Conference on Learning Theory (COLT)  pages 993–1019  2013.

[20] Alexander Rakhlin and Karthik Sridharan. Optimization  learning  and games with predictable sequences.
In Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS)  pages
3066–3074  2013.

[21] Karthik Sridharan and Ambuj Tewari. Convex games in banach spaces. In Proceedings of the 23rd

Conference on Learning Theory (COLT)  pages 1–13  2010.

[22] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed

Sensing: Theory and Applications  pages 210–268. Cambridge University Press  2012.

[23] Vladimir Vovk. Competing with wild prediction rules. Machine Learning  69(2-3):193–212  2007.

[24] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceed-

ings of the 20th International Conference on Machine Learning (ICML)  pages 928–936  2003.

10

,Ofer Dekel
arthur flajolet
Nika Haghtalab
Patrick Jaillet
Lee-Ad Gottlieb
Eran Kaufman
Aryeh Kontorovich
Gabriel Nivasch