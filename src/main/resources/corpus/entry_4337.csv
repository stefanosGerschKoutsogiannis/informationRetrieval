2017,Positive-Unlabeled Learning with Non-Negative Risk Estimator,From only positive (P) and unlabeled (U) data  a binary classifier could be trained with PU learning  in which the state of the art is unbiased PU learning. However  if its model is very flexible  empirical risks on training data will go negative  and we will suffer from serious overfitting. In this paper  we propose a non-negative risk estimator for PU learning: when getting minimized  it is more robust against overfitting  and thus we are able to use very flexible models (such as deep neural networks) given limited P data. Moreover  we analyze the bias  consistency  and mean-squared-error reduction of the proposed risk estimator  and bound the estimation error of the resulting empirical risk minimizer. Experiments demonstrate that our risk estimator fixes the overfitting problem of its unbiased counterparts.,Positive-Unlabeled Learning with

Non-Negative Risk Estimator

Ryuichi Kiryo1 2 Gang Niu1 2 Marthinus C. du Plessis Masashi Sugiyama2 1

1The University of Tokyo  7-3-1 Hongo  Tokyo 113-0033  Japan

2RIKEN  1-4-1 Nihonbashi  Tokyo 103-0027  Japan

{ kiryo@ms.  gang@ms.  sugi@ }k.u-tokyo.ac.jp

Abstract

From only positive (P) and unlabeled (U) data  a binary classiﬁer could be trained
with PU learning  in which the state of the art is unbiased PU learning. However 
if its model is very ﬂexible  empirical risks on training data will go negative  and
we will suffer from serious overﬁtting. In this paper  we propose a non-negative
risk estimator for PU learning: when getting minimized  it is more robust against
overﬁtting  and thus we are able to use very ﬂexible models (such as deep neural
networks) given limited P data. Moreover  we analyze the bias  consistency  and
mean-squared-error reduction of the proposed risk estimator  and bound the esti-
mation error of the resulting empirical risk minimizer. Experiments demonstrate
that our risk estimator ﬁxes the overﬁtting problem of its unbiased counterparts.

1

Introduction

Positive-unlabeled (PU) learning can be dated back to [1  2  3] and has been well studied since then.
It mainly focuses on binary classiﬁcation applied to retrieval and novelty or outlier detection tasks
[4  5  6  7]  while it also has applications in matrix completion [8] and sequential data [9  10].
Existing PU methods can be divided into two categories based on how U data is handled. The ﬁrst
category (e.g.  [11  12]) identiﬁes possible negative (N) data in U data  and then performs ordinary
supervised (PN) learning; the second (e.g.  [13  14]) regards U data as N data with smaller weights.
The former heavily relies on the heuristics for identifying N data; the latter heavily relies on good
choices of the weights of U data  which is computationally expensive to tune.
In order to avoid tuning the weights  unbiased PU learning comes into play as a subcategory of the
second category. The milestone is [4]  which regards a U data as weighted P and N data simultane-
ously. It might lead to unbiased risk estimators  if we unrealistically assume that the class-posterior
probability is one for all P data.1 A breakthrough in this direction is [15] for proposing the ﬁrst un-
biased risk estimator  and a more general estimator was suggested in [16] as a common foundation.
The former is unbiased but non-convex for loss functions satisfying some symmetric condition; the
latter is always unbiased  and it is further convex for loss functions meeting some linear-odd condi-
tion [17  18]. PU learning based on these unbiased risk estimators is the current state of the art.
However  the unbiased risk estimators will give negative empirical risks  if the model being trained
is very ﬂexible. For the general estimator in [16]  there exist three partial risks in the total risk (see
Eq. (2) deﬁned later)  especially it has a negative risk regarding P data as N data to cancel the bias
caused by regarding U data as N data. The worst case is that the model can realize any measurable
function and the loss function is not upper bounded  so that the empirical risk is not lower bounded.
This needs to be ﬁxed since the original risk  which is the target to be estimated  is non-negative.

1It implies the P and N class-conditional densities have disjoint support sets  and then any P and N data (as

the test data) can be perfectly separated by a ﬁxed classiﬁer that is sufﬁciently ﬂexible.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

To this end  we propose a novel non-negative risk estimator that follows and improves on the state-
of-the-art unbiased risk estimators mentioned above. This estimator can be used for two purposes:
ﬁrst  given some validation data (which are also PU data)  we can use our estimator to evaluate the
risk—for this case it is biased yet optimal  and for some symmetric losses  the mean-squared-error
reduction is guaranteed; second  given some training data  we can use our estimator to train binary
classiﬁers—for this case its risk minimizer possesses an estimation error bound of the same order
as the risk minimizers corresponding to its unbiased counterparts [15  16  19].
In addition  we propose a large-scale PU learning algorithm for minimizing the unbiased and non-
negative risk estimators. This algorithm accepts any surrogate loss and is based on stochastic opti-
mization  e.g.  [20]. Note that [21] is the only existing large-scale PU algorithm  but it only accepts
a single surrogate loss from [16] and is based on sequential minimal optimization [22].
The rest of this paper is organized as follows. In Section 2 we review unbiased PU learning  and in
Section 3 we propose non-negative PU learning. Theoretical analyses are carried out in Section 4 
and experimental results are discussed in Section 5. Conclusions are given in Section 6.

2 Unbiased PU learning

In this section  we review unbiased PU learning [15  16].
Problem settings Let X ∈ Rd and Y ∈ {±1} (d ∈ N) be the input and output random variables.
Let p(x  y) be the underlying joint density of (X  Y )  pp(x) = p(x | Y = +1) and pn(x) = p(x |
Y = −1) be the P and N marginals (a.k.a. the P and N class-conditional densities)  p(x) be the U
marginal  πp = p(Y = +1) be the class-prior probability  and πn = p(Y = −1) = 1 − πp. πp is
assumed known throughout the paper; it can be estimated from P and U data [23  24  25  26].
Consider the two-sample problem setting of PU learning [5]: two sets of data are sampled indepen-
dently from pp(x) and p(x) as Xp = {xp
i=1 ∼ p(x)  and a classiﬁer
needs to be trained from Xp and Xu.2 If it is PN learning as usual  Xn = {xn
i=1 ∼ pn(x) rather
than Xu would be available and a classiﬁer could be trained from Xp and Xn.
Risk estimators Unbiased PU learning relies on unbiased risk estimators. Let g : Rd → R be an
arbitrary decision function  and (cid:96) : R × {±1} → R be the loss function  such that the value (cid:96)(t  y)
means the loss incurred by predicting an output t when the ground truth is y. Denote by R+
p (g) =
n (g) = En[(cid:96)(g(X) −1)]  where Ep[·] = EX∼pp[·] and En[·] = EX∼pn[·].
Ep[(cid:96)(g(X)  +1)] and R−
p (g) + πnR−
Then  the risk of g is R(g) = E(X Y )∼p(x y)[(cid:96)(g(X)  Y )] = πpR+
n (g). In PN learning 
thanks to the availability of Xp and Xn  R(g) can be approximated directly by

i=1 ∼ pp(x) and Xu = {xu

i }np

i }nn

i }nu

p (g) = (1/np)(cid:80)np

where (cid:98)R+
i=1 (cid:96)(g(xp
ing  Xn is unavailable  but R−
p (g) = Ep[(cid:96)(g(X) −1)] and R−
R−
we can obtain that πnR−
n (g) = R−

where (cid:98)R−

p (g) = (1/np)(cid:80)np

i=1 (cid:96)(g(xp

n (g) 

p (g) + πn(cid:98)R−
n (g) = (1/nn)(cid:80)nn

(cid:98)Rpn(g) = πp(cid:98)R+
i )  +1) and (cid:98)R−
(1)
i ) −1). In PU learn-
n (g) can be approximated indirectly  as shown in [15  16]. Denote by
u (g) = EX∼p(x)[(cid:96)(g(X) −1)]. As πnpn(x) = p(x) − πppp(x) 
u (g) − πpR−
(cid:98)Rpu(g) = πp(cid:98)R+
i ) −1) and (cid:98)R−

p (g) + (cid:98)R−
u (g) = (1/nu)(cid:80)nu

p (g)  and R(g) can be approximated indirectly by

p (g) − πp(cid:98)R−

i ) −1).

i=1 (cid:96)(g(xn

i=1 (cid:96)(g(xu

u (g) 

(2)

The empirical risk estimators in Eqs. (1) and (2) are unbiased and consistent w.r.t. all popular loss
functions.3 When they are used for evaluating the risk (e.g.  in cross-validation)  (cid:96) is by default the
zero-one loss  namely (cid:96)01(t  y) = (1 − sign(ty))/2; when used for training  (cid:96)01 is replaced with a
surrogate loss [27]. In particular  [15] showed that if (cid:96) satisﬁes a symmetric condition:

(cid:96)(t  +1) + (cid:96)(t −1) = 1 

(3)

2Xp is a set of independent data and so is Xu  but Xp ∪ Xu does not need to be such a set.

3The consistency here means for ﬁxed g  (cid:98)Rpn(g) → R(g) and (cid:98)Rpu(g) → R(g) as np  nn  nu → ∞.

2

we will have

(cid:98)Rpu(g) = 2πp(cid:98)R+

p (g) + (cid:98)R−

u (g) − πp 

(4)
which can be minimized by separating Xp and Xu with ordinary cost-sensitive learning. An issue is

(cid:98)Rpu(g) in (4) must be non-convex in g  since no (cid:96)(t  y) in (3) can be convex in t. [16] showed that
(cid:98)Rpu(g) in (2) is convex in g  if (cid:96)(t  y) is convex in t and meets a linear-odd condition [17  18]:

(cid:96)(t  +1) − (cid:96)(t −1) = −t.

(5)
Let g be parameterized by θ  then (5) leads to a convex optimization problem so long as g is linear
in θ  for which the globally optimal solution can be obtained. Eq. (5) is not only sufﬁcient but also
necessary for the convexity  if (cid:96) is unary  i.e.  (cid:96)(t −1) = (cid:96)(−t  +1).
Justiﬁcation Thanks to the unbiasedness  we can study estimation error bounds (EEB). Let G be

the function class  and(cid:98)gpn and(cid:98)gpu be the empirical risk minimizers of (cid:98)Rpn(g) and (cid:98)Rpu(g). [19]
proved EEB of(cid:98)gpu is tighter than EEB of(cid:98)gpn when πp/

nn  if (a) (cid:96) satisﬁes
(3) and is Lipschitz continuous; (b) the Rademacher complexity of G decays in O(1/
n) for data
of size n drawn from p(x)  pp(x) or pn(x).4 In other words  under mild conditions  PU learning is
likely to outperform PN learning when πp/
nn. This phenomenon has been
observed in experiments [19] and is illustrated in Figure 1(a).

√
nu < πn/

√
np + 1/

√
np + 1/

nu < πn/

√

√

√

√

3 Non-negative PU learning

In this section  we propose the non-negative risk estimator and the large-scale PU algorithm.

3.1 Motivation

i }nn

n (g) = R−

n (g) from N data {xn

√
i=1  the convergence rate is Op(πn/

Let us look inside the aforementioned justiﬁcation of unbiased PU (uPU) learning. Intuitively  the
u (g) − πpR−
advantage comes from the transformation πnR−
p (g). When we approximate
nn)  where Op denotes the order
πnR−
i }nu
i }np
p (g) from P data {xp
u (g) − πpR−
in probability; when we approximate R−
i=1 
√
the convergence rate becomes Op(πp/
nu). As a result  we might beneﬁt from a tighter
√
√
np + 1/
uniform deviation bound when πp/
np + 1/
be difﬁcult for EEB of(cid:98)gpu to be tighter than EEB of(cid:98)gpn. If G = {g | (cid:107)g(cid:107)∞ ≤ Cg} where Cg > 0
However  the critical assumption on the Rademacher complexity is indispensable  otherwise it will
for any n and q(x) and all bounds become trivial; moreover if (cid:96) is not bounded from above  (cid:98)Rpu(g)
is a constant  i.e.  it has all measurable functions with some bounded norm  then Rn q(G) = O(1)
(cid:98)gpu  G cannot be too complex  or equivalently the model of g cannot be too ﬂexible.
becomes not bounded from below  i.e.  it may diverge to −∞. Thus  in order to obtain high-quality

i=1 and U data {xu

√
nu < πn/

nn.

√

This argument is supported by an experiment as illustrated in Figure 1(b). A multilayer perceptron
was trained for separating the even and odd digits of MNIST hand-written digits [29]. This model is
so ﬂexible that the number of parameters is 500 times more than the total number of P and N data.
From Figure 1(b) we can see:

(A) on training data  the risks of uPU and PN both decrease  and uPU is faster than PN;
(B) on test data  the risk of PN decreases  whereas the risk of uPU does not; the risk of uPU is

To sum up  the overﬁtting problem of uPU is serious  which evidences that in order to obtain high-

lower at the beginning but higher at the end than that of PN.

quality(cid:98)gpu  the model of g cannot be too ﬂexible.

3.2 Non-negative risk estimator

Nevertheless  we have no choice sometimes: we are interested in using ﬂexible models  while label-
ing more data is out of our control. Can we alleviate the overﬁtting problem with neither changing
the model nor labeling more data?

(cid:80)
4Let σ1  . . .   σn be n Rademacher variables  the Rademacher complexity of G for X of size n drawn from
xi∈X σig(xi)] [28]. For any ﬁxed G and q  Rn q(G)

q(x) is deﬁned by Rn q(G) = EX Eσ1 ... σn [supg∈G 1
still depends on n and should decrease with n.

n

3

(a) Plain linear model

(b) Multilayer perceptron (MLP)

Figure 1: Illustrative experimental results.

The dataset is MNIST; even/odd digits are regarded as the P/N class  and πp ≈ 1/2; np = 100 and nn = 50
for PN learning; np = 100 and nu = 59  900 for unbiased PU (uPU) and non-negative PU (nnPU) learning.
The model is a plain linear model (784-1) in (a) and an MLP (784-100-1) with ReLU in (b); it was trained by
Algorithm 1  where the loss (cid:96) is (cid:96)sig  the optimization algorithm A is [20]  with β = 1/2 for uPU  and β = 0

and γ = 1 for nnPU. Solid curves are (cid:98)Rpn(g) on test data where g ∈ {(cid:98)gpn (cid:98)gpu (cid:101)gpu}  and dashed curves are
(cid:98)Rpn((cid:98)gpn)  (cid:98)Rpu((cid:98)gpu) and (cid:101)Rpu((cid:101)gpu) on training data. Note that nnPU is identical to uPU in (a).
The answer is afﬁrmative. Note that (cid:98)Rpu((cid:98)gpu) keeps decreasing and goes negative. This should be
(cid:98)R−
u (g) − πp(cid:98)R−
Let(cid:101)gpu = arg ming∈G (cid:101)Rpu(g) be the empirical risk minimizer of (cid:101)Rpu(g). We refer to the process
of obtaining(cid:101)gpu as non-negative PU (nnPU) learning. The implementation of nnPU will be given
in Section 3.3  and theoretical analyses of (cid:101)Rpu(g) and(cid:101)gpu will be given in Section 4.

n (g) ≥ 0  but
p (g) ≥ 0 is not always true  which is a potential reason for uPU to overﬁt. Based on

u (g) − πpR−
(cid:110)
0 (cid:98)R−
u (g) − πp(cid:98)R−

ﬁxed since R(g) ≥ 0 for any g. Speciﬁcally  it holds that R−

this key observation  we propose a non-negative risk estimator for PU learning:

(cid:101)Rpu(g) = πp(cid:98)R+

p (g) = πnR−

p (g) + max

(cid:111)

(6)

p (g)

.

Again  from Figure 1(b) we can see:

(A) on training data  the risk of nnPU ﬁrst decreases and then becomes more and more ﬂat  so
that the risk of nnPU is closer to the risk of PN and farther from that of uPU; in short  the
risk of nnPU does not go down with uPU after a certain epoch;

(B) on test data  the tendency is similar  but the risk of nnPU does not go up with uPU;
(C) at the end  nnPU achieves the lowest risk on test data.

In summary  nnPU works by explicitly constraining the training risk of uPU to be non-negative.

3.3 Implementation

was minimized by the concave-convex procedure [30]. This solver is fairly sophisticated  and if we

A list of popular loss functions and their properties is shown in Table 1. Let g be parameterized by
θ. If g is linear in θ  the losses satisfying (5) result in convex optimizations. However  if g needs to
be ﬂexible  it will be highly nonlinear in θ; then the losses satisfying (5) are not advantageous over

others  since the optimizations are anyway non-convex. In [15]  the ramp loss was used and (cid:98)Rpu(g)
replace (cid:98)Rpu(g) with (cid:101)Rpu(g)  it will be more difﬁcult to implement. To this end  we propose to use
the sigmoid loss (cid:96)sig(t  y) = 1/(1 + exp(ty)): its gradient is everywhere non-zero and (cid:101)Rpu(g) can
In front of big data  we should scale PU learning up by stochastic optimization. Minimizing (cid:98)Rpu(g)
is embarrassingly parallel while minimizing (cid:101)Rpu(g) is not  since (cid:98)Rpu(g) is point-wise but (cid:101)Rpu(g)
is not due to the max operator. That being said  max{0 (cid:98)R−
than (1/N )(cid:80)N
p (g;Xp)} is no greater
hence the corresponding upper bound of (cid:101)Rpu(g) can easily be minimized in parallel.
u) is the i-th mini-batch  and

u (g;Xu) − πp(cid:98)R−

i=1 max{0 (cid:98)R−

be minimized by off-the-shelf gradient methods.

u) − πp(cid:98)R−

p)}  where (X i

p (g;X i

u (g;X i

p X i

4

0100200300400500Epoch0.150.200.250.300.350.400.450.50Risk w.r.t. surrogate lossPN testPN trainuPU testuPU train0100200300400500Epoch0.20.10.00.10.20.30.40.5Risk w.r.t. surrogate lossPN testPN trainuPU testuPU trainnnPU testnnPU trainTable 1: Loss functions for PU learning and their properties.

Name
Zero-one loss
Ramp loss
Squared loss
Logistic loss
Hinge loss
Double hinge loss max{0  (1 − z)/2 −z}
Sigmoid loss

(3)
Deﬁnition
(1 − sign(z))/2
(cid:88)
max{0  min{1  (1 − z)/2}} (cid:88)
(z − 1)2/4
×
×
ln(1 + exp(−z))
×
max{0  1 − z}
×
(cid:88)

1/(1 + exp(z))

(5) Bounded
×
×
(cid:88)
(cid:88)
×
(cid:88)
×

(cid:88)
(cid:88)
×
×
×
×
(cid:88)

Lipschitz

×
(cid:88)
×
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:96)(cid:48)(z) (cid:54)= 0
z = 0
z ∈ R
z ∈ R

z ∈ [−1  +1]

z ∈ (−∞  +1]
z ∈ (−∞  +1]

z ∈ R

All loss functions are unary  such that (cid:96)(t  y) = (cid:96)(z) with z = ty. The ramp loss comes from [15]; the double
hinge loss is from [16]  in which the squared  logistic and hinge losses were discussed as well. The ramp and
squared losses are scaled to satisfy (3) or (5). The sigmoid loss is a horizontally mirrored logistic function; the
logistic loss is the negative logarithm of the logistic function.

Input: training data (Xp Xu);

Algorithm 1 Large-scale PU learning based on stochastic optimization
hyperparameters 0 ≤ β ≤ πp supt maxy (cid:96)(t  y) and 0 ≤ γ ≤ 1

Output: model parameter θ for(cid:98)gpu(x; θ) or(cid:101)gpu(x; θ)

1: Let A be an external SGD-like stochastic optimization algorithm such as [20] or [31]
2: while no stopping criterion has been met:
3:
4:
5:
6:
7:
8:
9:
10:

Shufﬂe (Xp Xu) into N mini-batches  and denote by (X i
u) − πp(cid:98)R−
if (cid:98)R−
for i = 1 to N:
Set gradient ∇θ(cid:98)Rpu(g;X i
p) ≥ −β:
p (g;X i
u (g;X i
p X i
u)
Update θ by A with its current step size η
p) − (cid:98)R−
Set gradient ∇θ(πp(cid:98)R−
u (g;X i
p (g;X i
u))
Update θ by A with a discounted step size γη

u) the i-th mini-batch

p X i

else:

The large-scale PU algorithm is described in Algorithm 1. Let ri = (cid:98)R−
p (g;X i
p). In
practice  we may tolerate ri ≥ −β where 0 ≤ β ≤ πp supt maxy (cid:96)(t  y)  as ri comes from a single
minimizing (cid:98)Rpu(g) if β = πp supt maxy (cid:96)(t  y). Otherwise if ri < −β  we go along −∇θri with a
mini-batch. The degree of tolerance is controlled by β: there is zero tolerance if β = 0  and we are
step size discounted by γ where 0 ≤ γ ≤ 1  to make this mini-batch less overﬁtted. Algorithm 1 is
insensitive to the choice of γ  if the optimization algorithm A is adaptive such as [20] or [31].

u) − πp(cid:98)R−

u (g;X i

4 Theoretical analyses

In this section  we analyze the risk estimator (6) and its minimizer (all proofs are in Appendix B).

4.1 Bias and consistency

Fix g  (cid:101)Rpu(g) ≥ (cid:98)Rpu(g) for any (Xp Xu) but (cid:98)Rpu(g) is unbiased  which implies (cid:101)Rpu(g) is biased
in general. A fundamental question is then whether (cid:101)Rpu(g) is consistent. From now on  we prove
this consistency. To begin with  partition all possible (Xp Xu) into D+(g) = {(Xp Xu) | (cid:98)R−
πp(cid:98)R−
p (g) ≥ 0} and D−(g) = {(Xp Xu) | (cid:98)R−
u (g) −
p (g) < 0}. Assume there are Cg > 0 and
C(cid:96) > 0 such that supg∈G (cid:107)g(cid:107)∞ ≤ Cg and sup|t|≤Cg maxy (cid:96)(t  y) ≤ C(cid:96).
is non-zero; (B) (cid:101)Rpu(g) differs from (cid:98)Rpu(g) with a non-zero probability over repeated sampling of
Lemma 1. The following three conditions are equivalent: (A) the probability measure of D−(g)
(Xp Xu); (C) the bias of (cid:101)Rpu(g) is positive. In addition  by assuming that there is α > 0 such that
n (g) ≥ α  the probability measure of D−(g) can be bounded by
R−

u (g) − πp(cid:98)R−

Pr(D−(g)) ≤ exp(−2(α/C(cid:96))2/(π2

p/np + 1/nu)).

(7)

5

√

Moreover  for any δ > 0  let Cδ = C(cid:96)

√
Based on Lemma 1  we can show the exponential decay of the bias and also the consistency. For
nu.
convenience  denote by χnp nu = 2πp/
np + 1/
n (g) ≥ α > 0 and denote by ∆g the right-hand
Theorem 2 (Bias and consistency). Assume that R−

side of Eq. (7). As np  nu → ∞  the bias of (cid:101)Rpu(g) decays exponentially:
0 ≤ EXp Xu [(cid:101)Rpu(g)] − R(g) ≤ C(cid:96)πp∆g.
|(cid:101)Rpu(g) − R(g)| ≤ Cδ · χnp nu + C(cid:96)πp∆g 

(cid:112)ln(2/δ)/2  then we have with probability at least 1 − δ 
|(cid:101)Rpu(g) − R(g)| ≤ Cδ · χnp nu.

and with probability at least 1 − δ − ∆g 

Either (9) or (10) in Theorem 2 indicates for ﬁxed g  (cid:101)Rpu(g) → R(g) in Op(πp/

nu).
This convergence rate is optimal according to the central limit theorem [32]  which means the pro-
posed estimator is a biased yet optimal estimator to the risk.

√
np + 1/

(8)

(9)

(10)

√

(cid:90)

(Xp Xu)∈D−(g)

i ·(cid:81)nu

4.2 Mean squared error

u (g) − πp(cid:98)R−

we can still characterize this reduction in MSE.

((cid:98)Rpu(g) + (cid:101)Rpu(g) − 2R(g))((cid:98)R−

After introducing the bias  (cid:101)Rpu(g) tends to overestimate R(g). It is not a shrinkage estimator [33 
34] so that its mean squared error (MSE) is not necessarily smaller than that of (cid:98)Rpu(g). However 
Theorem 3 (MSE reduction). It holds that MSE((cid:101)Rpu(g)) < MSE((cid:98)Rpu(g)) 5 if and only if
p (g)) dF (Xp Xu) > 0 
where dF (Xp Xu) =(cid:81)np
u (g) − (cid:98)R−
MSE((cid:98)Rpu(g)) − MSE((cid:101)Rpu(g)) ≥ 3β2Pr{(cid:101)Rpu(g) − (cid:98)Rpu(g) > β}.
u (g) − (cid:98)R−

The assumption (d) in Theorem 3 is explained as follows. Since U data can be much cheaper than
P data in practice  it would be natural to assume nu is much larger and grows much faster than np 
p (g) ≥ α/πp} ∝ exp(np − nu) asymptotically.6
p (g) − R−
hence Pr{R−
u (g) − (cid:98)R−
This means the contribution of Xu is negligible for making (Xp Xu) ∈ D−(g) so that Pr(D−(g))
u (g) ≥ 2α} has stronger exponential
exhibits exponential decay mainly in np. As Pr{R−
decay in nu than Pr{R−

i=1 pp(xp
i . Eq. (11) is valid  if the following condi-
n (g) ≥ α > 0; (d) nu (cid:29) np  such
u (g) ≤ 2α almost surely on D−(g). In fact  given these four conditions 

tions are met: (a) Pr(D−(g)) > 0; (b) (cid:96) satisﬁes Eq. (3); (c) R−
that we have R−
we have for any 0 ≤ β ≤ C(cid:96)πp 

u (g) ≥ α}/Pr{(cid:98)R−
u (g) − (cid:98)R−

u (g) ≥ α} as well as nu (cid:29) np  we made the assumption (d).

i=1 p(xu

i )dxp

i )dxu

(11)

(12)

4.3 Estimation error

in its use for training classiﬁers. In what follows  we analyze the estimation error R((cid:101)gpu) − R(g∗) 
While Theorems 2 and 3 addressed the use of (6) for evaluating the risk  we are likewise interested
where g∗ is the true risk minimizer in G  i.e.  g∗ = arg ming∈G R(g). As a common practice [28] 
assume that (cid:96)(t  y) is Lipschitz continuous in t for all |t| ≤ Cg with a Lipschitz constant L(cid:96).
n (g) ≥ α > 0 and denote by ∆ the
Theorem 4 (Estimation error bound). Assume that (a) inf g∈G R−
right-hand side of Eq. (7); (b) G is closed under negation  i.e.  g ∈ G if and only if −g ∈ G. Then 
for any δ > 0  with probability at least 1 − δ 

R((cid:101)gpu) − R(g∗) ≤ 16L(cid:96)πpRnp pp (G) + 8L(cid:96)Rnu p(G) + 2C(cid:48)

(cid:112)ln(1/δ)/2  and Rnp pp(G) and Rnu p(G) are the Rademacher complexities of G

where C(cid:48)
for the sampling of size np from pp(x) and of size nu from p(x)  respectively.

δ · χnp nu + 2C(cid:96)πp∆ 

δ = C(cid:96)

(13)

5Here  MSE(·) is over repeated sampling of (Xp Xu).
6This can be derived as np  nu → ∞ by applying the central limit theorem to the two differences and then

L’Hôpital’s rule to the ratio of complementary error functions [32].

6

√

√

np + 1/

nu).

Theorem 4 ensures that learning with (6) is also consistent: as np  nu → ∞  R((cid:101)gpu) → R(g∗) and
if (cid:96) satisﬁes (5)  all optimizations are convex and(cid:101)gpu → g∗. For linear-in-parameter models with a
nu)  and thus R((cid:101)gpu) → R(g∗)
√
bounded norm  Rnp pp (G) = O(1/
√
in Op(πp/
For comparison  R((cid:98)gpu) − R(g∗) can be bounded using a different proof technique [19]:
R((cid:98)gpu) − R(g∗) ≤ 8L(cid:96)πpRnp pp(G) + 4L(cid:96)Rnu p(G) + 2Cδ · χnp nu 
(cid:112)ln(2/δ)/2. The differences of (13) and (14) are completely from the differences

np) and Rnu p(G) = O(1/

where Cδ = C(cid:96)
of the corresponding uniform deviation bounds  i.e.  the following lemma and Lemma 8 of [19].
Lemma 5. Under the assumptions of Theorem 4  for any δ > 0  with probability at least 1 − δ 

supg∈G |(cid:101)Rpu(g) − R(g)| ≤ 8L(cid:96)πpRnp pp(G) + 4L(cid:96)Rnu p(G) + C(cid:48)
Notice that (cid:98)Rpu(g) is point-wise while (cid:101)Rpu(g) is not due to the maximum  which makes Lemma 5

δ · χnp nu + C(cid:96)πp∆.

much more difﬁcult to prove than Lemma 8 of [19]. The key trick is that after symmetrization  we
employ | max{0  z} − max{0  z(cid:48)}| ≤ |z − z(cid:48)|  making three differences of partial risks point-wise
(see (18) in the proof). As a consequence  we have to use a different Rademacher complexity with
the absolute value inside the supremum [35  36]  whose contraction makes the coefﬁcients of (15)
doubled compared with Lemma 8 of [19]; moreover  we have to assume G is closed under negation
to change back to the standard Rademacher complexity without the absolute value [28]. Therefore 
the differences of (13) and (14) are mainly due to different proof techniques and cannot reﬂect the
intrinsic differences of empirical risk minimizers.

(14)

(15)

5 Experiments

In this section  we compare PN  unbiased PU (uPU) and non-negative PU (nnPU) learning experi-
mentally. We focus on training deep neural networks  as uPU learning usually does not overﬁt if a
linear-in-parameter model is used [19] and nothing needs to be ﬁxed.
Table 2 describes the speciﬁcation of benchmark datasets. MNIST  20News and CIFAR-10 have 10 
7 and 10 classes originally  and we constructed the P and N classes from them as follows: MNIST
was preprocessed in such a way that 0  2  4  6  8 constitute the P class  while 1  3  5  7  9 constitute
the N class; for 20News  ‘alt.’  ‘comp.’  ‘misc.’ and ‘rec.’ make up the P class  and ‘sci.’  ‘soc.’ and
‘talk.’ make up the N class; for CIFAR-10  the P class is formed by ‘airplane’  ‘automobile’  ‘ship’
and ‘truck’  and the N class is formed by ‘bird’  ‘cat’  ‘deer’  ‘dog’  ‘frog’ and ‘horse’. The dataset
epsilon has 2 classes and such a construction is unnecessary.
Three learning methods were set up as follows: (A) for PN  np = 1  000 and nn = (πn/2πp)2np;
(B) for uPU  np = 1  000 and nu is the total number of training data; (C) for nnPU  np and nu are

exactly same as uPU. For uPU and nnPU  P and U data were dependent  because neither (cid:98)Rpu(g) in
Eq. (2) nor (cid:101)Rpu(g) in Eq. (6) requires them to be independent. The choice of nn was motivated by

[19] and may make nnPU potentially better than PN as nu → ∞ (whether np < ∞ or np ≤ nu).
The model for MNIST was a 6-layer multilayer perceptron (MLP) with ReLU [40] (more speciﬁ-
cally  d-300-300-300-300-1). For epsilon  the model was similar while the activation was replaced
with Softsign [41] for better performance. For 20News  we borrowed the pre-trained word embed-
dings from GloVe [42]  and the model can be written as d-avg_pool(word_emb(d 300))-300-300-1 

Table 2: Speciﬁcation of benchmark datasets  models  and optimition algorithms.

# Train

# Test

# Feature

πp Model g(x; θ)

Opt. alg. A
6-layer MLP with ReLU
Adam [20]
6-layer MLP with Softsign Adam [20]
5-layer MLP with Softsign AdaGrad [31]
13-layer CNN with ReLU

Adam [20]

Name
MNIST [29]
epsilon [37]
20News [38]
CIFAR-10 [39]

60  000
400  000
11  314
50  000

10  000
100  000
7  532
10  000

784
2  000
61  188
3  072

0.49
0.50
0.44
0.40

See http://yann.lecun.com/exdb/mnist/ for MNIST  https://www.csie.ntu.edu.tw/~cjlin/
libsvmtools/datasets/binary.html for epsilon  http://qwone.com/~jason/20Newsgroups/ for
20Newsgroups  and https://www.cs.toronto.edu/~kriz/cifar.html for CIFAR-10.

7

(a) MNIST

(b) epsilon

(c) 20News

(d) CIFAR-10
Figure 2: Experimental results of training deep neural networks.

where word_emb(d 300) retrieves 300-dimensional word embeddings for all words in a document 
avg_pool executes average pooling  and the resulting vector is fed to a 4-layer MLP with Softsign.
The model for CIFAR-10 was an all convolutional net [43]: (32*32*3)-[C(3*3 96)]*2-C(3*3 96 2)-
[C(3*3 192)]*2-C(3*3 192 2)-C(3*3 192)-C(1*1 192)-C(1*1 10)-1000-1000-1  where the input is
a 32*32 RGB image  C(3*3 96) means 96 channels of 3*3 convolutions followed by ReLU  [ · ]*2
means there are two such layers  C(3*3 96 2) means a similar layer but with stride 2  etc.; it is one
of the best architectures for CIFAR-10. Batch normalization [44] was applied before hidden layers.
Furthermore  the sigmoid loss (cid:96)sig was used as the surrogate loss and an (cid:96)2-regularization was also
added. The resulting objectives were minimized by Adam [20] on MNIST  epsilon and CIFAR-10 
and by AdaGrad [31] on 20News; we ﬁxed β = 0 and γ = 1 for simplicity.
The experimental results are reported in Figure 2  where means and standard deviations of training
and test risks based on the same 10 random samplings are shown. We can see that uPU overﬁtted
training data and nnPU ﬁxed this problem. Additionally  given limited N data  nnPU outperformed
PN on MNIST  epsilon and CIFAR-10 and was comparable to it on 20News. In summary  with the
proposed non-negative risk estimator  we are able to use very ﬂexible models given limited P data.
We further tried some cases where πp is misspeciﬁed  in order to simulate PU learning in the wild 
where we must suffer from errors in estimating πp. More speciﬁcally  we tested nnPU learning by
replacing πp with π(cid:48)
p to the learning method  so that it
would regard π(cid:48)
p as πp during the entire training process. The experimental setup was exactly same
as before except the replacement of πp.
The experimental results are reported in Figure 3  where means of test risks of nnPU based on the
same 10 random samplings are shown  and the best test risks are identiﬁed (horizontal lines are the
best mean test risks and vertical lines are the epochs when they were achieved). We can see that on
MNIST  the more misspeciﬁcation was  the worse nnPU performed  while under-misspeciﬁcation
hurt more than over-misspeciﬁcation; on epsilon  the cases where π(cid:48)
p equals to πp  1.1πp and 1.2πp

p ∈ {0.8πp  0.9πp  . . .   1.2πp} and giving π(cid:48)

8

0255075100125150175200Epoch0.40.30.20.10.00.10.20.30.40.5Risk w.r.t. zero-one lossPN testPN trainuPU testuPU trainnnPU testnnPU train0255075100125150175200Epoch0.000.050.100.150.200.250.300.350.400.450.50Risk w.r.t. zero-one loss0255075100125150175200Epoch0.10.00.10.20.30.4Risk w.r.t. zero-one loss0255075100125150175200Epoch0.40.30.20.10.00.10.20.30.40.5Risk w.r.t. zero-one loss(a) MNIST

(b) epsilon

(c) 20News

(d) CIFAR-10

Figure 3: Experimental results given π(cid:48)

p ∈ {0.8πp  0.9πp  . . .   1.2πp}.

p = 1.1πp rather than π(cid:48)

p = 1.2πp but inferior to π(cid:48)

p = πp; on 20News  these three cases
p = 1.1πp; at last

p = πp was superior to π(cid:48)
p = 1.1πp were comparable again  and π(cid:48)

were comparable  but the best was π(cid:48)
became different  such that π(cid:48)
p = πp and π(cid:48)
on CIFAR-10  π(cid:48)
In all the experiments  we have ﬁxed β = 0  which may explain this phenomenon. Recall that uPU
overﬁtted seriously on all the benchmark datasets  and note that the larger π(cid:48)
p is  the more different
nnPU is from uPU. Therefore  the replacement of πp with some π(cid:48)
p > πp introduces additional bias
away from uPU. This may result in lower test risks given some π(cid:48)
in Figure 3. This is also why under-misspeciﬁed π(cid:48)
All the experiments were done with Chainer [45]  and our implementation based on it is available
at https://github.com/kiryor/nnPUlearning.

of (cid:101)Rpu(g) in estimating R(g)  but it also pushes (cid:101)Rpu(g) away from (cid:98)Rpu(g) and then pushes nnPU

p slightly larger than πp as shown

p hurt more than over-misspeciﬁed π(cid:48)
p.

p = 1.2πp was the winner.

6 Conclusions

We proposed a non-negative risk estimator for PU learning that follows and improves on the state-
of-the-art unbiased risk estimators. No matter how ﬂexible the model is  it will not go negative as
its unbiased counterparts. It is more robust against overﬁtting when being minimized  and training
very ﬂexible models such as deep neural networks given limited P data becomes possible. We also
developed a large-scale PU learning algorithm. Extensive theoretical analyses were presented  and
the usefulness of our non-negative PU learning was veriﬁed by intensive experiments. A promising
future direction is extending the current work to semi-supervised learning along [46].

Acknowledgments

GN and MS were supported by JST CREST JPMJCR1403 and GN was also partially supported by
Microsoft Research Asia.

9

0255075100125150175200Epoch0.050.100.150.200.250.30Risk w.r.t. zero-one loss0.80.91.01.11.20255075100125150175200Epoch0.280.300.320.340.360.380.400.420.440.46Risk w.r.t. zero-one loss0255075100125150175200Epoch0.200.220.240.260.280.30Risk w.r.t. zero-one loss0255075100125150175200Epoch0.180.200.220.240.260.280.30Risk w.r.t. zero-one lossReferences
[1] F. Denis. PAC learning from positive statistical queries. In ALT  1998.
[2] F. De Comité  F. Denis  R. Gilleron  and F. Letouzey. Positive and unlabeled examples help learning. In

ALT  1999.

[3] F. Letouzey  F. Denis  and R. Gilleron. Learning from positive and unlabeled examples. In ALT  2000.
[4] C. Elkan and K. Noto. Learning classiﬁers from only positive and unlabeled data. In KDD  2008.
[5] G. Ward  T. Hastie  S. Barry  J. Elith  and J. Leathwick. Presence-only data and the EM algorithm.

Biometrics  65(2):554–563  2009.

[6] C. Scott and G. Blanchard. Novelty detection: Unlabeled data deﬁnitely help. In AISTATS  2009.
[7] G. Blanchard  G. Lee  and C. Scott. Semi-supervised novelty detection. Journal of Machine Learning

Research  11:2973–3009  2010.

[8] C.-J. Hsieh  N. Natarajan  and I. S. Dhillon. PU learning for matrix completion. In ICML  2015.
[9] X. Li  P. S. Yu  B. Liu  and S.-K. Ng. Positive unlabeled learning for data stream classiﬁcation. In SDM 

2009.

[10] M. N. Nguyen  X. Li  and S.-K. Ng. Positive unlabeled leaning for time series classiﬁcation. In IJCAI 

2011.

[11] B. Liu  W. S. Lee  P. S. Yu  and X. Li. Partially supervised classiﬁcation of text documents. In ICML 

2002.

[12] X. Li and B. Liu. Learning to classify texts using positive and unlabeled data. In IJCAI  2003.
[13] W. S. Lee and B. Liu. Learning with positive and unlabeled examples using weighted logistic regression.

In ICML  2003.

[14] B. Liu  Y. Dai  X. Li  W. S. Lee  and P. S. Yu. Building text classiﬁers using positive and unlabeled

examples. In ICDM  2003.

[15] M. C. du Plessis  G. Niu  and M. Sugiyama. Analysis of learning from positive and unlabeled data. In

NIPS  2014.

[16] M. C. du Plessis  G. Niu  and M. Sugiyama. Convex formulation for learning from positive and unlabeled

data. In ICML  2015.

[17] N. Natarajan  I. S. Dhillon  P. Ravikumar  and A. Tewari. Learning with noisy labels. In NIPS  2013.
[18] G. Patrini  F. Nielsen  R. Nock  and M. Carioni. Loss factorization  weakly supervised learning and label

noise robustness. In ICML  2016.

[19] G. Niu  M. C. du Plessis  T. Sakai  Y. Ma  and M. Sugiyama. Theoretical comparisons of positive-

unlabeled learning against positive-negative learning. In NIPS  2016.

[20] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In ICLR  2015.
[21] E. Sansone  F. G. B. De Natale  and Z.-H. Zhou. Efﬁcient training for positive unlabeled learning. arXiv

preprint arXiv:1608.06807  2016.

[22] J. C. Platt.

Fast training of support vector machines using sequential minimal optimization.

In
B. Schölkopf  C. J. C. Burges  and A. J. Smola  editors  Advances in Kernel Methods  pages 185–208.
MIT Press  1999.

[23] A. Menon  B. Van Rooyen  C. S. Ong  and B. Williamson. Learning from corrupted binary labels via

class-probability estimation. In ICML  2015.

[24] H. G. Ramaswamy  C. Scott  and A. Tewari. Mixture proportion estimation via kernel embedding of

distributions. In ICML  2016.

[25] S. Jain  M. White  and P. Radivojac. Estimating the class prior and posterior from noisy positives and

unlabeled data. In NIPS  2016.

[26] M. C. du Plessis  G. Niu  and M. Sugiyama. Class-prior estimation for learning from positive and unla-

beled data. Machine Learning  106(4):463–492  2017.

[27] P. L. Bartlett  M. I. Jordan  and J. D. McAuliffe. Convexity  classiﬁcation  and risk bounds. Journal of the

American Statistical Association  101(473):138–156  2006.

[28] M. Mohri  A. Rostamizadeh  and A. Talwalkar. Foundations of Machine Learning. MIT Press  2012.
[29] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[30] A. L. Yuille and A. Rangarajan. The concave-convex procedure (CCCP). In NIPS  2001.

10

[31] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[32] K.-L. Chung. A Course in Probability Theory. Academic Press  1968.
[33] C. Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In Proc.

3rd Berkeley Symposium on Mathematical Statistics and Probability  1956.

[34] W. James and C. Stein. Estimation with quadratic loss. In Proc. 4th Berkeley Symposium on Mathematical

Statistics and Probability  1961.

[35] V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on Informa-

tion Theory  47(5):1902–1914  2001.

[36] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research  3:463–482  2002.

[37] G.-X. Yuan  C.-H. Ho  and C.-J. Lin. An improved GLMNET for l1-regularized logistic regression.

Journal of Machine Learning Research  13:1999–2030  2012.

[38] K. Lang. Newsweeder: Learning to ﬁlter netnews. In ICML  1995.
[39] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report  University of

Toronto  2009.

[40] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In ICML  2010.
[41] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In

AISTATS  2010.

[42] J. Pennington  R. Socher  and C. D. Manning. GloVe: Global vectors for word representation. In EMNLP 

2014.

[43] J. T. Springenberg  A. Dosovitskiy  T. Brox  and M. Riedmiller. Striving for simplicity: The all convolu-

tional net. In ICLR  2015.

[44] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. In ICML  2015.

[45] S. Tokui  K. Oono  S. Hido  and J. Clayton. Chainer: a next-generation open source framework for deep

learning. In Machine Learning Systems Workshop at NIPS  2015.

[46] T. Sakai  M. C. du Plessis  G. Niu  and M. Sugiyama. Semi-supervised classiﬁcation based on classiﬁca-

tion from positive and unlabeled data. In ICML  2017.

[47] C. McDiarmid. On the method of bounded differences. In J. Siemons  editor  Surveys in Combinatorics 

pages 148–188. Cambridge University Press  1989.

[48] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer 

1991.

[49] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.

Cambridge University Press  2014.

[50] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons  1998.

11

,Andrea Frome
Greg Corrado
Jon Shlens
Samy Bengio
Jeff Dean
Marc'Aurelio Ranzato
Tomas Mikolov
Uygar Sümbül
Douglas Roossien
Dawen Cai
Fei Chen
Nicholas Barry
John Cunningham
Edward Boyden
Liam Paninski
Ryuichi Kiryo
Gang Niu
Marthinus du Plessis
Masashi Sugiyama