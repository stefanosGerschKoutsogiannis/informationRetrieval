2013,Conditional Random Fields via Univariate Exponential Families,Conditional random fields  which model the distribution of a multivariate response conditioned on a set of covariates using undirected graphs  are widely used in a variety of multivariate prediction applications. Popular instances of this class of models such as categorical-discrete CRFs  Ising CRFs  and conditional Gaussian based CRFs  are not however best suited to the varied types of response variables in many applications  including count-valued responses. We thus introduce a “novel subclass of CRFs”  derived by imposing node-wise conditional distributions of response variables conditioned on the rest of the responses and the covariates as arising from univariate exponential families.  This allows us to derive novel multivariate CRFs given any univariate exponential distribution  including the Poisson  negative binomial  and exponential distributions. Also in particular  it addresses the common CRF problem of specifying feature'' functions determining the interactions between response variables and covariates. We develop a class of tractable penalized $M$-estimators to learn these CRF distributions from data  as well as a unified sparsistency analysis for this general class of CRFs showing exact structure recovery can be achieved with high probability.",Conditional Random Fields via Univariate

Exponential Families

Eunho Yang

Department of Computer Science

University of Texas at Austin
eunho@cs.utexas.edu

Pradeep Ravikumar

Department of Computer Science

University of Texas at Austin

pradeepr@cs.utexas.edu

Genevera I. Allen

Department of Statistics and

Electrical & Computer Engineering

Rice University

gallen@rice.edu

Zhandong Liu

Department of Pediatrics-Neurology

Baylor College of Medicine

zhandonl@bcm.edu

Abstract

Conditional random ﬁelds  which model the distribution of a multivariate response
conditioned on a set of covariates using undirected graphs  are widely used in a
variety of multivariate prediction applications. Popular instances of this class of
models  such as categorical-discrete CRFs  Ising CRFs  and conditional Gaus-
sian based CRFs  are not well suited to the varied types of response variables in
many applications  including count-valued responses. We thus introduce a novel
subclass of CRFs  derived by imposing node-wise conditional distributions of re-
sponse variables conditioned on the rest of the responses and the covariates as
arising from univariate exponential families. This allows us to derive novel multi-
variate CRFs given any univariate exponential distribution  including the Poisson 
negative binomial  and exponential distributions. Also in particular  it addresses
the common CRF problem of specifying “feature” functions determining the inter-
actions between response variables and covariates. We develop a class of tractable
penalized M-estimators to learn these CRF distributions from data  as well as a
uniﬁed sparsistency analysis for this general class of CRFs showing exact struc-
ture recovery can be achieved with high probability.

1

Introduction

Conditional random ﬁelds (CRFs) are a popular class of models that combine the advantages of
discriminative modeling and undirected graphical models. They are widely used across structured
prediction domains such as natural language processing  computer vision  and bioinformatics. The
key idea in this class of models is to represent the joint distribution of a set of response variables
conditioned on a set of covariates using a product of clique-wise compatibility functions. Given an
underlying graph over the response variables  each of these compatibility functions depends on all
the covariates  but only on a subset of response variables within any clique of the underlying graph.
They are thus a discriminative counterpart of undirected graphical models  where we have covariates
that provide information about the multivariate response  and the underlying graph structure encodes
conditional independence assumptions among the responses conditioned on the covariates.
There is a key model speciﬁcation question that arises  however  in any application of CRFs: how
do we specify the clique-wise sufﬁcient statistics  or compatibility functions (sometimes also called
feature functions)  that characterize the conditional graphical model between responses? In par-

1

ticular  how do we tune these to the particular types of variables being modeled? Traditionally 
these questions have been addressed either by hand-crafted feature functions  or more generally by
discretizing the multivariate response vectors into a set of indicator vectors and then letting the com-
patibility functions be linear combinations of the product of indicator functions [1]. This approach 
however  may not be natural for continuous  skewed continuous or count-valued random variables.
Recently  spurred in part by applications in bioinformatics  there has been much research on other
sub-classes of CRFs. The Ising CRF which models binary responses  was studied by [2] and ex-
tended to higher-order interactions by [3]. Several versions and extensions of Gaussian-based CRFs
have also been proposed [4  5  6  7  8]. These sub-classes of CRFs  however  are speciﬁc to Gaussian
and binary variable types  and may not be appropriate for multivariate count data or skewed con-
tinuous data  for example  which are increasingly seen in big-data settings such as high-throughput
genomic sequencing.
In this paper  we seek to (a) formulate a novel subclass of CRFs that have the ﬂexibility to model
responses of varied types  (b) address how to specify compatibility functions for such a family of
CRFs  and (c) develop a tractable procedure with strong statistical guarantees for learning this class
of CRFs from data. We ﬁrst show that when node-conditional distributions of responses conditioned
on other responses and covariates are speciﬁed by univariate exponential family distributions  there
exists a consistent joint CRF distribution  that necessarily has a speciﬁc form: with terms that are
tensorial products of functions over the responses  and functions over the covariates.This subclass
of “exponential family” CRFs can be viewed as a conditional extension of the MRF framework
of [9  10]. As such  this broadens the class of off-the-shelf CRF models to encompass data that
follows distributions other than the standard discrete  binary  or Gaussian instances. Given this
new family of CRFs  we additionally show that if covariates also follow node-conditional univariate
exponential family distributions  then the functions over features in turn are precisely speciﬁed by the
exponential family sufﬁcient statistics. Thus  our twin results deﬁnitively answer for the ﬁrst time
the key model speciﬁcation question of specifying compatibility or feature functions for a broad
family of CRF distributions. We then provide a uniﬁed M-estimation procedure  via penalized
neighborhood estimation  to learn our family of CRFs from i.i.d. observations that simultaneously
addresses all three sub-tasks of CRF learning: feature selection (where we select a subset of the
covariates for any response variable)  structure recovery (where we learn the graph structure among
the response variables)  and parameter learning (where we learn the parameters specifying the CRF
distribution). We also present a single theorem that gives statistical guarantees saying that with high-
probability  our M-estimator achieves each of these three sub-tasks. Our result can be viewed as an
extension of neighborhood selection results for MRFs [11  12  13]. Overall  this paper provides a
family of CRFs that generalizes many of the sub-classes in the existing literature and broadens the
utility and applicability of CRFs to model many other types of multivariate responses.

2 Conditional Graphical Models via Exponential Families

Suppose we have a p-variate random response vector Y = (Y1  . . .   Yp)  with each response vari-
able Ys taking values in a set Ys. Suppose we also have a set of covariates X = (X1  . . .   Xq)
associated with this response vector Y . Suppose G = (V  E) is an undirected graph over p nodes
corresponding to the p response variables. Given the underlying graph G  and the set of cliques
(fully-connected sub-graphs) C of the graph G  the corresponding conditional random ﬁeld (CRF)
is a set of distributions over the response conditioned on the covariates that satisfy Markov indepen-
dence assumptions with respect to the graph G. Speciﬁcally  letting {c(Yc  X)}c2C denote a set
of clique-wise sufﬁcient statistics  any strictly positive distribution of Y conditioned on X within
the conditional random ﬁeld family takes the form: P (Y |X) / exp{Pc2C c(Yc  X)}. With a

pair-wise conditional random ﬁeld distribution  the set of cliques consists of the set of nodes V and
the set of edges E  so that

P (Y |X) / exp⇢Xs2V

s(Ys  X) + X(s t)2E

st(Ys  Yt  X).

A key model speciﬁcation question is how to select the class of sufﬁcient statistics  . We have a
considerable understanding of how to specify univariate distributions over various types of variables
as well as on how to model their conditional response through regression. Consider the univariate
exponential family class of distributions: P (Z) = exp(✓B (Z) + C(Z)  D(✓))  with sufﬁcient

2

statistics B(Z)  base measure C(Z)  and log-normalization constant D(✓). Such exponential fam-
ily distributions include a wide variety of commonly used distributions such as Gaussian  Bernoulli 
multinomial  Poisson  exponential  gamma  chi-squared  beta  any of which can be instantiated with
particular choices of the functions B(·)  and C(·). Such univariate exponential family distributions
are thus used to model a wide variety of data types including skewed continuous data and count data.
Additionally  through generalized linear models  they are used to model the response of various data
types conditional on a set of covariates. Here  we seek to use our understanding of univariate expo-
nential families and generalized linear models to specify a conditional graphical model distribution.
Consider the conditional extension of the construction in [14  9  10]. Suppose that the node-
conditional distributions of response variables  Ys  conditioned on the rest of the response variables 
YV \s  and the covariates  X  is given by an univariate exponential family:

P (Ys|YV \s  X) = exp{Es(YV \s  X) Bs(Ys) + Cs(Ys)  ¯Ds(YV \s  X)}.

(1)
Here  the functions Bs(·)  Cs(·) are speciﬁed by the choice of the exponential family  and the pa-
rameter Es(YV \s  X) is an arbitrary function of the variables Yt in N (s) and the covariates X;
N (s) is the set of neighbors of node s according to an undirected graph G = (V  E). Would these
node-conditional distributions be consistent with a joint distribution? Would this joint distribution
factor according a conditional random ﬁeld given by graph G? And would there be restrictions on
the form of the functions Es(YV \s  X)? The following theorem answers these questions. We note
that it generalizes the MRF framework of [9  10] in two ways: it allows for the presence of condi-
tional covariates  and moreover allows for heterogeneous types and domains of distributions with
the different choices of Bs(·) and Cs(·) at each individual node.
Theorem 1. Consider a p-dimensional random vector Y = (Y1  Y2  . . .   Yp) denoting the set of
responses  and let X = (X1  . . .   Xq) be a q-dimensional covariate vector. Consider the follow-
ing two assertions: (a) the node-conditional distributions of each P (Ys|YV \s  X) are speciﬁed by
univariate exponential family distributions as detailed in (1); and (b) the joint multivariate condi-
tional distribution P (Y |X) factors according to the graph G = (V  E) with clique-set C  but with
factors over response-variable-cliques of size at most k. These assertions on the conditional and
joint distributions respectively are consistent if and only if the conditional distribution in (1) has the
tensor-factorized form:

P (Ys|YV \s  X; ✓) = exp⇢Bs(Ys)⇣✓s(X) + Xt2N (s)
kYj=2

+ Xt2 ... tk2N (s)

✓s t2...tk (X)

✓st(X) Bt(Yt) + . . .

Btj (Ytj )⌘ + Cs(Ys)  ¯Ds(YV \s) 

(2)

where ✓s·(X) := {✓s(X) ✓ st(X)  . . .  ✓ s t2...tk (X)} is a set of functions that depend only on the
covariates X. Moreover  the corresponding joint conditional random ﬁeld distribution has the form:

P (Y |X; ✓) = exp⇢Xs

✓s(X)Bs(Ys) +Xs2V Xt2N (s)
kYj=1

✓t1...tk (X)

+ . . . + X(t1 ... tk)2C

✓st(X) Bs(Ys)Bt(Yt)

Btj (Ytj ) +Xs

Cs(Ys)  A✓(X) 

(3)

where A✓(X) is the log-normalization constant.

Theorem 1 speciﬁes the form of the function Es(YV \s  X) deﬁning the canonical parameter in the
univariate exponential family distribution (1). This function is a tensor factorization of products of
sufﬁcient statistics of YV \s  and “observation functions”  ✓(X)  of the covariates X alone. A key
point to note is that the observation functions  ✓(X)  in the CRF distribution (3) should ensure that

the density is normalizable  that is  A✓(X) < +1. We also note that we can allow different
exponential families for each of the node-conditional distributions of the response variables  mean-
ing that the domains  Ys  or the sufﬁcient statistics functions  Bs(·)  can vary across the response
variables Ys. A common setting of these sufﬁcient statistics functions however  for many popular
distributions (Gaussian  Bernoulli  etc.)  is a linear function  so that Bs(Ys) = Ys.

3

An important special case of the above result is when the joint CRF has response-variable-clique
factors of size at most two. The node conditional distributions (2) would then have the form:

P (Ys|YV \s  X; ✓) / exp⇢Bs(Ys) ·⇣✓s(X) + Xt2N (s)

✓st(X) Bt(Yt)⌘ + Cs(Ys) 

while the joint distribution in (3) has the form:

P (Y |X; ✓) = exp⇢Xs2V
✓st(X) Bs(Ys) Bt(Yt) +Xs2V
with the log-partition function  A✓(X)  given the covariates  X  deﬁned as
A✓(X) := logZYp

✓s(X)Bs(Ys) + X(s t)2E
exp⇢Xs2V

✓s(X)Bs(Ys) + X(s t)2E

✓st(X) Bs(Ys) Bt(Yt) +Xs2V

Cs(Ys)  A✓(X) 
Cs(Ys).

(4)

(5)

Theorem 1 then addresses the model speciﬁcation question of how to select the compatibility func-
tions in CRFs for varied types of responses. Our framework permits arbitrary observation functions 
✓(X)  with the only stipulation that the log-partition function must be ﬁnite. (This only provides a
restriction when the domain of the response variables is not ﬁnite). In the next section  we address
the second model speciﬁcation question of how to set the covariate functions.

2.1 Setting Covariate Functions

A candidate approach to specifying the observation functions  ✓(X)  in the CRF distribution above
would be to make distributional assumptions on X. Since Theorem 1 speciﬁes the conditional
distribution P (Y |X)  specifying the marginal distribution P (X) would allow us to specify the
joint distribution P (Y  X) without further restrictions on P (Y |X) using the simple product rule:
P (X  Y ) = P (Y |X) P (X). As an example  suppose that the covariates X follow an MRF distri-
bution with graph G0 = (V 0  E0)  and parameters #:

P (X) = expn Xu2V 0

#uu(Xu) + X(u v)2V 0⇥V 0
Then  for any CRF distribution P (Y |X) in (4)  we have

P (X  Y ) = expnXu
#uu(Xu) + X(u v)
Cs(Ys)  A(#)  A✓(X)o.
+Xs

#uvuv(Xu  Xv) +Xs

#uvuv(Xu  Xv)  A(#)o.
✓s(X)Ys +X(s t)

✓st(X)YsYt

P (Xu|XV 0\u  Y ) = exp{Eu(XV 0\u  Y ) Bu(Xu) + Cu(Xu)  ¯Du(XV 0\u  Y )} 

The joint distribution  P (X  Y )  is valid provided P (Y |X) and P (X) are valid distributions. Thus 
a distributional assumption on P (X) does not restrict the set of covariate functions in any way.
On the other hand  specifying the conditional distribution  P (X|Y )  naturally entails restrictions on
the form of P (Y |X). Consider the case where the conditional distributions P (Xu|XV 0\u  Y ) are
also speciﬁed by univariate exponential families:
(6)
where Eu(XV 0\u  Y ) is an arbitrary function of the rest of the variables  and Bu(·)  Cu(·)  ¯Du(·) are
speciﬁed by the univariate exponential family. Under these additional distributional assumptions in
(6)  what form would the CRF distribution in Theorem 1 take? Speciﬁcally  what would be the form
of the observation functions ✓(X)? The following theorem provides an answer to this question. (In
the following  we use the shorthand sm
Theorem 2. Consider the following assertions: (a) the conditional CRF distribution of the re-
sponses Y = (Y1  . . .   Yp) given covariates X = (X1  . . .   Xq) is given by the family (4); and (b)
the conditional distributions of individual covariates given rest of the variables P (Xu|XV 0\u  Y ) is
given by an exponential family of the form in (6); and (c) the joint distribution P (X  Y ) belongs to
a graphical model with graph ¯G = (V [ V 0  ¯E)  with clique-set C  with factors of size at most k.
These assertions are consistent if and only if the CRF distribution takes the form:

1 to denote the sequence (s1  . . .   sm).)

4

P (Y |X) = expn kXl=1 Xtr

12V slr
1 slr
(tr

1 2V 0
)2C

1

↵tr

1 slr

1

Bsj (Xsj )

lrYj=1

rYj=1

Btj (Ytj ) +Xt2V

Ct(Yt)  A(↵  X)o  (7)

so that the observation functions ✓t1 ... tr (X) in the CRF distribution (4) are tensor products of

univariate functions: ✓t1 ... tr (X) =

kXl=1 Xslr

1 2V 0
1 slr
)2C

1

(tr

↵tr

1 slr

1

Bsj (Xsj ).

lrYj=1

Let us examine the consequences of this theorem for the pair-wise CRF distributions (4). Theorem 2
then entails that the observation functions  {✓s(X) ✓ st(X)}  have the following form when the
distribution has factors of size at most two:
(8)

✓suBu(Xu)  ✓

st(X) = ✓st 

✓s(X) = ✓s + Xu2V 0

for some constant parameters ✓s  ✓su and ✓st. Similarly  if the joint distribution has factors of size
at most three  we have:

✓s(X) = ✓s + Xu2V 0
✓st(X) = ✓st + Xu2V 0

✓suBu(Xu) + X(u v)2V 0⇥V 0

✓stuBu(Xu).

✓suvBu(Xu)Bv(Xv) 

(9)

(Remark 1) While we have derived the covariate functions in Theorem 2 by assuming a distribu-
tional form on X  using the resulting covariate functions do not necessarily impose distributional
assumptions on X. This is similar to “generative-discriminative” pairs of models [15]: a “gener-
ative” Naive Bayes distribution for P (X|Y ) corresponds to a “discriminative” logistic regression
model for P (Y |X)  but the converse need not hold. We can thus leverage the parametric CRF
distributional form in Theorem 2 without necessarily imposing stringent distributional assump-
tions on X.

(Remark 2) Consider the form of the covariate functions given by (8) compared to (9). What does
sparsity in the parameters entail in terms of conditional independence assumptions? ✓st = 0
in (8) entails that Ys is conditionally independent of Yt given the other responses and all the
covariates. Thus  the parametrization in (8) corresponds to pair-wise conditional independence
assumptions between the responses (structure learning) and between the responses and covariates
(feature selection). In contrast  (9) lets the edges weights between the responses  ✓st(X) vary
as a linear combination of the covariates. Letting ✓stu = 0 entails the lack of a third-order
interaction between the pair of responses Ys and Yt and the covariate Xu  conditioned on all
other responses and covariates.

(Remark 3) Our general subclasses of CRFs speciﬁed by Theorems 1 and 2 encompass many ex-

isting CRF families as special cases  in addition to providing many novel forms of CRFs.
• The Gaussian CRF presented in [7] as well as the reparameterization in [8] can be viewed
as an instance of our framework by substituting in Gaussian sufﬁcient statistics in (8): here
the Gaussian mean of the CRF depends on the covariates  but not the covariance. We can
correspondingly derive a novel Gaussian CRF formulation from (9)  where the Gaussian
covariance of Y |X would also depend on X.
• By using the Bernoulli distribution as the node-conditional distribution  we can derive the
Ising CRF  recently studied in [2] with an application to studying tumor suppressor genes.
• Several novel forms of CRFs can be derived by specifying node-conditional distributions
as Poisson or exponential  for example. With certain distributions  such as the multivari-
ate Poisson for example  we would have to enforce constraints on the parameters to ensure
normalizability of the distribution. For the Poisson CRF distribution  it can be veriﬁed that
for the log-partition function to be ﬁnite  A✓st(X) < 1  the observation functions are
constrained to be non-positive  ✓st(X)  0. Such restrictions are typically needed for cases
where the variables have inﬁnite domains.

5

3 Graphical Model Structure Learning

We now address the task of learning a CRF distribution from our general family given i.i.d. ob-
servations of the multivariate response vector and covariates. Structure recovery and estimation for
CRFs has not attracted as much attention as that for MRFs. Schmidt et al. [16]  Torralba et al.
[17] empirically study greedy methods and block `1 regularized pseudo-likelihood respectively to
learn the discrete CRF graph structure. Bradley and Guestrin [18]  Shahaf et al. [19] provide guar-
antees on structure recovery for low tree-width discrete CRFs using graph cuts  and a maximum
weight spanning tree based method respectively. Cai et al. [4]  Liu et al. [6] provide structure recov-
ery guarantees for their two-stage procedure for recovering (a reparameterization of) a conditional
Gaussian based CRF; and the semi-parameteric partition based Gaussian CRF respectively. Here 
we provide a single theorem that provides structure recovery guarantees for any CRF from our class
of exponential family CRFs  which encompasses not only Ising  and Gaussian based CRFs  but all
other instances within our class  such as Poisson CRFs  exponential CRFs  and so on.
We are given n i.i.d. samples Z := {X (i)  Y (i)}n
speciﬁed by Theorems 1 and 2 with covariate functions as given in (8):

i=1 from a pair-wise CRF distribution  of the form

P (Y |X; ✓⇤) / exp⇢Xs2V✓⇤s + Xu2N0(s)

✓⇤suBu(Xu)Bs(Ys) + X(s t)2E

✓⇤st Bs(Ys) Bt(Yt) +Xs

C(Ys)  (10)

s

i=1 P (Y (i)

|Y (i)
V \s  X (i); ✓).

n logQn

with unknown parameters  ✓⇤. The task of CRF parameter learning corresponds to estimating the
parameters ✓⇤  structure learning corresponds to recovering the edge-set E  and feature selection
corresponds to recovering the neighborhoods N0(s) in (10). Note that the log-partition function
A(✓⇤) is intractable to compute in general (other than special cases such as Gaussian CRFs). Ac-
cordingly  we adopt the node-based neighborhood estimation approach of [12  13  9  10]. Given the
joint distribution in (10)  the node-wise conditional distribution of Ys given the rest of the nodes
and covariates  is given by P (Ys|YV \s  X; ✓⇤) = exp{⌘ · Bs(Ys) + Cs(Ys)  Ds(⌘)} which is a
univariate exponential family  with parameter ⌘ = ✓⇤s +Pu2V 0 ✓⇤suBu(Xu) +Pt2V \s ✓⇤stBt(Yt) 
as discussed in the previous section. The corresponding negative log-conditional-likelihood can be
written as `(✓;Z) :=  1
For each node s  we have three components of the parameter set  ✓ := (✓s  ✓x  ✓y): a scalar ✓s  a
length q vector ✓x := [u2V 0✓su  and a length p  1 vector ✓y := [t2V \s✓st. Then  given samples
Z  these parameters can be selected by the following `1 regularized M-estimator:
(11)
where x n  y n are the regularization constants. Note that x n and y n do not need to be the
same as y n determines the degree of sparsity between Ys and YV \s  and similarly x n does
the degree of sparsity between Ys and covariates X. Given this M-estimator  we can recover the
response-variable-neighborhood of response Ys as N (s) = {t 2 V \s | ✓y
st 6= 0}  and the feature-
neighborhood of the response Ys as N0(s) = {t 2 V 0 | ✓x
Armed with this machinery  we can provide the statistical guarantees on successful learning of all
three sub-tasks of CRFs:
Theorem 3. Consider a CRF distribution as speciﬁed in (10). Suppose that the regularization
parameters in (11) are chosen such that

`(✓) + x nk✓xk1 + y nk✓yk1 

✓2R1+(p1)+q

su 6= 0}.

min

x n  M1r log q

n

  y n  M1r log p

n

and max{x n  y n} M2 

where M1 and M2 are some constants depending on the node conditional distribution in the form of
maxpdxx n pdyy n where
exponential family. Further suppose that mint2N (s) |✓⇤st| 10
⇢min is the minimum eigenvalue of the Hessian of the loss function at ✓x⇤  ✓y⇤  and dx  dy are the
number of nonzero elements in ✓x⇤ and ✓y⇤  respectively. Then  for some positive constants L  c1 
c2  and c3  if n  L(dx + dy)2(log p + log q)(max{log n  log(p + q)})2  then with probability at
least 1  c1 max{n  p + q}2  exp(c2n)  exp(c3n)  the following statements hold.
(a) (Parameter Error) For each node s 2 V   the solutionb✓ of the M-estimation problem in (11) is

unique with parameter error bound

⇢min

5

kc✓x  ✓x⇤k2 + kc✓y  ✓y⇤k2 

⇢min

maxpdxx n pdyy n 

6

t

 

e
a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

1

0.5

 

0
0

 

G−CRF
cGGM
pGGM

0.2

0.4

False Positive Rate

 

t

e
a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

1

0.5

 

0
0

G−CRF
cGGM
pGGM

0.2

0.4

False Positive Rate

(a) Gaussian graphical models

 

 

t

 

e
a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

1

0.5

 

0
0

 

I−CRF
I−MRF

0.2

0.4

False Positive Rate

t

 

e
a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

1

0.5

 

0
0

 

I−CRF
I−MRF

0.2

0.4

False Positive Rate

(b) Ising models

 

P−CRF
P−MRF

0.2

0.4

False Positive Rate

1

0.5

 

0
0

 

t

e
a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

t

 

e
a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

1

0.5

 

0
0

P−CRF
P−MRF

0.2

0.4

False Positive Rate

(c) Poisson graphical models

Figure 1: (a) ROC curves averaged over 50 simulations from a Gaussian CRF with p = 50 responses 
q = 50 covariates  and (left) n = 100 and (right) n = 250 samples. Our method (G-CRF) is
compared to that of [7] (cGGM) and [8] (pGGM). (b) ROC curves for simulations from an Ising
CRF with p = 100 responses  q = 10 covariates  and (left) n = 50 and (right) n = 150 samples.
Our method (I-CRF) is compared to the unconditional Ising MRF (I-MRF). (c) ROC curves for
simulations from a Poisson CRF with p = 100 responses  q = 10 covariates  and (left) n = 50 and
(right) n = 150 samples. Our method (P-CRF) is compared to the Poisson MRF (P-MRF).

(b) (Structure Recovery) The M-estimate recovers the response-feature neighborhoods exactly  so

(c) (Feature Selection) The M-estimate recovers the true response neighborhoods exactly  so that

thatcN0(s) = N0(s)  for all s 2 V .
bN (s) = N (s)  for all s 2 V .

The proof requires modifying that of Theorem 1 in [9  10] to allow for two different regularization
parameters  x n and y n  and for two distinct sets of random variables (responses and covariates).
This introduces subtleties related to interactions in the analyses. Extending our statistical analysis
in Theorem 3 for pair-wise CRFs to general CRF distributions (3) as well as general covariate
functions  such as in (9)  are omitted for space reasons and left for future work.

4 Experiments

Simulation Studies.
In order to evaluate the generality of our framework  we simulate data from
three different instances of our model:
those given by Gaussian  Bernoulli (Ising)  and Poisson
node-conditional distributions. We assume the true conditional distribution  P (Y |X)  follows (7)
with the parameters: ✓s(X) = ✓s +Pu2V 0 ✓suXu  ✓ st(X) = ✓st +Pu2V 0 ✓stuXu for some
constant parameters ✓s  ✓su  ✓st and ✓stu. In other words  we permit both the mean  ✓s(X) and the
covariance or edge-weights  ✓st(X)  to depend on the covariates.
For the Gaussian CRFs  our goal is to infer the precision (or inverse covariance) matrix. We ﬁrst
generate covariates as X ⇠ U [0.05  0.05]. Given X  the precision matrix of Y   ⇥(X)  is generated
as follows. All the diagonal elements are set to 1. For each node s  4 nearest neighbors in the
pp ⇥ pp lattice structure are selected  and ✓st = 0 for non-neighboring nodes. For a given edge
structure  the strength is now a function of covariates  X  by letting ✓st(X) = c + h!st  Xi where
c is a constant bias term and !st is target vector of length q. Data of size p = 50 responses and
q = 50 covariates was generated for n = 100 and n = 250 samples. Figure 1(a) reports the receiver-
operator curves (ROC) averaged over 50 trials for three different methods: the model of [7] (denoted
as cGGM)  the model of [8] (denoted as pGGM)  and our method (denoted as G-CRF). Results show
that our method outperforms competing methods as their edge-weights are restricted to be constants 
while our method allows them to linearly depend on the covariates. Data was similarly generated
using a 4 nearest neighbor lattice structure for Ising and Poisson CRFs with p = 100 responses 

7

Figure 2: From left to right: Gaussian MRF  mean-speciﬁed Gaussian CRF  and the set correspond-
ing to the covariance-speciﬁed Gaussian CRF. The latter shows the third-order interactions between
gene-pairs and each of the ﬁve common aberration covariates (EGFR  PTEN  CDKN2A  PDGFRA 
and CDK4). The models were learned from gene expression array data of Glioblastoma samples 
and the plots display the response neighborhoods of gene TWIST1.

q = 10 covariates  and n = 50 or n = 150 samples. Figure 1(b) and Figure 1(c) report the ROC
curves averaged over 50 trials for the Ising and Poisson CRFs respectively. The performance of our
method is compared to that of the unconditional Ising and Poisson MRFs of [9  10].

Real Data Example: Genetic Networks of Glioblastoma. We demonstrate the performance of
our CRF models by learning genetic networks of Glioblastoma conditioned on common copy num-
ber aberrations. Level III gene expression data measured by Aglient arrays for n = 465 Glioblas-
toma tumor samples as well as copy number variation measured by CGH-arrays were downloaded
from the Cancer Genome Atlas data portal [20]. The data was processed according to standard
techniques  and we only consider genes from the C2 Pathway Database. The ﬁve most common
copy number aberrations across all subjects were taken as covariates. We ﬁt our Gaussian “mean-
speciﬁed” CRFs (with covariate functions given in (8)) and Gaussian “covariance-speciﬁed” CRFs
(with covariate functions given in (9)) by penalized neighborhood estimation to learn the graph
structure of gene expression responses  p = 876  conditional on q = 5 aberrations: EGFR  PTEN 
CDKN2A  PDGFRA  and CDK4. Stability selection [21] was used to determine the sparsity of the
network.
Due to space limitations  the entire network structures are not shown. Instead  we show the results of
the mean- and covariance-speciﬁed Gaussian CRFs and that of the Gaussian graphical model (GGM)
for one particularly important gene neighborhood: TWIST1 is a transcription factor for epithelial
to mesenchymal transition [22] and has been shown to promote tumor invasion in multiple cancers
including Glioblastoma [23]. The neighborhoods of TWIST1 learned by GGMs and mean-speciﬁed
CRFs share many of the known interactors of TWIST1  such as SNAI2  MGP  and PMAIP1 [24].
The mean-speciﬁed CRF is more sparse as conditioning on copy number aberrations may explain
many of the conditional dependencies with TWIST1 that are captured by GGMs  demonstrating the
utility of conditional modeling via CRFs. For the covariance-speciﬁed Gaussian CRF  we plot the
neighborhood given by ✓stu in (9) for the ﬁve values of u corresponding to each aberration. The
results of this network denote third-order effects between gene-pairs and aberrations  and are thus
even more sparse with no neighbors for the interactions between TWIST1 and PTEN  CDK4  and
EGFR. TWIST1 has different interactions between PDGFRA and CDKN2A  which have high fre-
quency for proneual subtypes of Glioblastoma tumors. Thus  our covariance-speciﬁed CRF network
may indicate that these two aberrations are the most salient in interacting with pairs of genes that in-
clude the gene TWIST1. Overall  our analysis has demonstrated the applied advantages of our CRF
models; namely  one can study the network structure between responses conditional on covariates
and/or between pairs of responses that interact with particular covariates.

Acknowledgments
The authors acknowledge support from the following sources: ARO via W911NF-12-1-0390 and
NSF via IIS-1149803 and DMS-1264033 to E.Y. and P.R; Ken Kennedy Institute for Information
Technology at Rice to G.A. and Z.L.; NSF DMS-1264058 and DMS-1209017 to G.A.; and NSF
DMS-1263932 to Z.L..

8

AReferences
[1] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families and variational inference.

Foundations and Trends in Machine Learning  1(1–2):1—305  December 2008.

[2] J. Cheng  E. Levina  P. Wang  and J. Zhu. Sparse ising models with covariates. Arxiv preprint

arXiv:1209.6419  2012.

[3] S. Ding  G. Wahba  and X. Zhu. Learning Higher-Order Graph Structure with Features by Structure

Penalty. In NIPS  2011.

[4] T. Cai  H. Li  W. Liu  and J. Xie. Covariate adjusted precision matrix estimation with an application in

genetical genomics. Biometrika  2011.

[5] S. Kim and E. P. Xing. Statistical estimation of correlated genome associations to a quantitative trait

network. PLoS Genetics  2009.

[6] H. Liu  X. Chen  J. Lafferty  and L. Wasserman. Graph-valued regression. In NIPS  2010.
[7] J. Yin and H. Li. A sparse conditional gaussian graphical model for analysis of genetical genomics data.

Annals of Applied Statistics  5(4):2630–2650  2011.

[8] X. Yuan and T. Zhang. Partial gaussian graphical model estimation. Arxiv preprint arXiv:1209.6419 

2012.

[9] E. Yang  P. Ravikumar  G. I. Allen  and Z. Liu. Graphical models via generalized linear models. In Neur.

Info. Proc. Sys.  25  2012.

[10] E. Yang  P. Ravikumar  G. I. Allen  and Z. Liu. On graphical models via univariate exponential family

distributions. Arxiv preprint arXiv:1301.4183  2013.

[11] A. Jalali  P. Ravikumar  V. Vasuki  and S. Sanghavi. On learning discrete graphical models using group-

sparse regularization. In Inter. Conf. on AI and Statistics (AISTATS)  14  2011.

[12] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the Lasso. Annals

of Statistics  34:1436–1462  2006.

[13] P. Ravikumar  M. J. Wainwright  and J. Lafferty. High-dimensional ising model selection using `1-

regularized logistic regression. Annals of Statistics  38(3):1287–1319  2010.

[14] J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical

Society. Series B (Methodological)  36(2):192–236  1974.

[15] A. Y. Ng and M. I. Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic regression

and naive bayes. In Neur. Info. Proc. Sys.  2002.

[16] M. Schmidt  K. Murphy  G. Fung  and R. Rosales. Structure learning in random ﬁelds for heart motion

abnormality detection. In Computer Vision and Pattern Recognition (CVPR)  pages 1–8  2008.

[17] A. Torralba  K. P. Murphy  and W. T. Freeman. Contextual models for object detection using boosted

random ﬁelds. In NIPS  2004.

[18] J. K. Bradley and C. Guestrin. Learning tree conditional random ﬁelds. In ICML  2010.
[19] D. Shahaf  A. Chechetka  and C. Guestrin. Learning thin junction trees via graph cuts. In AISTATS  2009.
[20] Cancer Genome Atlas Research Network. Comprehensive genomic characterization deﬁnes human

glioblastoma genes and core pathways. Nature  455(7216):1061–1068  October 2008.

[21] H. Liu  K. Roeder  and L. Wasserman. Stability approach to regularization selection (stars) for high

dimensional graphical models. Arxiv preprint arXiv:1006.3316  2010.

[22] J. Yang  S. A. Mani  J. L. Donaher  S. Ramaswamy  R. A. Itzykson  C. Come  P. Savagner  I. Gitelman 
A. Richardson  and R. A. Weinberg. Twist  a master regulator of morphogenesis  plays an essential role
in tumor metastasis. Cell  117(7):927–939  2004.

[23] S. A. Mikheeva  A. M. Mikheev  A. Petit  R. Beyer  R. G. Oxford  L. Khorasani  J.-P. Maxwell  C. A.
Glackin  H. Wakimoto  I. Gonz´alez-Herrero  et al. Twist1 promotes invasion through mesenchymal
change in human glioblastoma. Mol Cancer  9:194  2010.

[24] M. A. Smit  T. R. Geiger  J.-Y. Song  I. Gitelman  and D. S. Peeper. A twist-snail axis critical for trkb-
induced epithelial-mesenchymal transition-like transformation  anoikis resistance  and metastasis. Molec-
ular and cellular biology  29(13):3722–3737  2009.

[25] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained

quadratic programming (Lasso). IEEE Trans. Information Theory  55:2183–2202  May 2009.

[26] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Arxiv preprint arXiv:1010.2731  2010.

9

,Eunho Yang
Pradeep Ravikumar
Genevera Allen
Zhandong Liu
Junbang Liang
Ming Lin
Vladlen Koltun