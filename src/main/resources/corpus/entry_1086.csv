2014,Cone-Constrained Principal Component Analysis,Estimating a vector from noisy quadratic observations is a task that arises naturally in many contexts  from dimensionality reduction  to synchronization and phase retrieval problems. It is often the case that additional information is available about the unknown vector (for instance  sparsity  sign or magnitude of its entries). Many authors propose non-convex quadratic optimization problems that aim at exploiting optimally this information. However  solving these problems is typically NP-hard. We consider a simple model for noisy quadratic observation of an unknown vector $\bvz$. The unknown vector is constrained to belong to a cone $\Cone \ni \bvz$. While optimal estimation appears to be intractable for the general problems in this class  we provide evidence that it is tractable when $\Cone$ is a convex cone with an efficient projection. This is surprising  since the corresponding optimization problem is non-convex and --from a worst case perspective-- often NP hard. We characterize the resulting minimax risk in terms of the statistical dimension of the cone $\delta(\Cone)$. This quantity is already known to control the risk of estimation from gaussian observations and random linear measurements. It is rather surprising that the same quantity plays a role in the estimation risk from quadratic measurements.,Cone-constrained Principal Component Analysis

Yash Deshpande

Electrical Engineering
Stanford University

Andrea Montanari

Electrical Engineering and Statistics

Stanford University

Emile Richard

Electrical Engineering
Stanford University

Abstract

Estimating a vector from noisy quadratic observations is a task that arises nat-
urally in many contexts  from dimensionality reduction  to synchronization and
phase retrieval problems. It is often the case that additional information is avail-
able about the unknown vector (for instance  sparsity  sign or magnitude of its
entries). Many authors propose non-convex quadratic optimization problems that
aim at exploiting optimally this information. However  solving these problems is
typically NP-hard.
We consider a simple model for noisy quadratic observation of an unknown vector
v0. The unknown vector is constrained to belong to a cone C (cid:51) v0. While
optimal estimation appears to be intractable for the general problems in this class 
we provide evidence that it is tractable when C is a convex cone with an efﬁcient
projection. This is surprising  since the corresponding optimization problem is
non-convex and –from a worst case perspective– often NP hard. We characterize
the resulting minimax risk in terms of the statistical dimension of the cone δ(C).
This quantity is already known to control the risk of estimation from gaussian
observations and random linear measurements. It is rather surprising that the same
quantity plays a role in the estimation risk from quadratic measurements.

1

Introduction

In many statistical estimation problems  observations can be modeled as noisy quadratic functions
of an unknown vector v0 = (v0 1  v0 2  . . .   v0 n)T ∈ Rn. For instance  in positioning and graph
localization [5  24]  one is given noisy measurements of pairwise distances (v0 i − v0 j)2 (where
–for simplicity– we consider the case in which the underlying geometry is one-dimensional). In
principal component analysis (PCA) [15]  one is given a data matrix X ∈ Rn×p  and tries to reduce
its dimensionality by postulating an approximate factorization X ≈ u0 v0
T. Hence Xij can be
interpreted as a noisy observation of the quadratic function u0 iv0 j. As a last example  there has
been signiﬁcant interest recently in phase retrieval problems [11  6]. In this case  the unknown vector
v0 is –roughly speaking– an image  and the observations are proportional to the square modulus of
a modulated Fourier transform |Fv0|2.
In several of these contexts  a signiﬁcant effort has been devoted to exploiting additional structure
of the unknown vector v0. For instance  in Sparse PCA  various methods have been developed to
exploit the fact that v0 is known to be sparse [14  25]. In sparse phase retrieval [13  18]  a similar
assumption is made in the context of phase retrieval.
All of these attempts face a recurring dichotomy. One the hand  additional information on v0 can
increase dramatically the estimation accuracy. On the other  only a fraction of this additional in-
formation is exploited by existing polynomial time algorithms. For instance in sparse PCA  if it is
known that only k entries of the vector v0 are non-vanishing  an optimal estimator is successful in
identifying them from roughly k samples (neglecting logarithmic factors) [2]. On the other hand 
known polynomial-time algorithms require about k2 samples [16  7].

1

This fascinating phenomenon is however poorly understood so far. Classifying estimation problems
as to whether optimal estimation accuracy can be achieved or not in polynomial time is an out-
standing challenge. In this paper we develop a stylized model to study estimation from quadratic
observations  under additional constraints. Special choices of the constraint set yield examples for
which optimal estimation is thought to be intractable.
However we identify a large class of constraints for which estimation appears to be tractable  de-
spite the corresponding maximum likelihood problem is non-convex. This shows that computational
tractability is not immediately related to simple considerations of convexity or worst-case complex-
ity.
Our model assumes v0 ∈ Cn with Cn ⊆ Rn a closed cone. Observations are organized in a sym-
metric matrix X = (Xij)1≤i j≤n deﬁned by

1
2

X = β v0v0

T + Z .

2 (cid:107)(cid:98)v(X) − v0(cid:107)2

(1)
Here Z is a symmetric noise matrix with independent entries (Zij)i≤j with Zij ∼ N(0  1/n) for
i < j and Zii ∼ N(0  2/n). We assume  without loss of generality  (cid:107)v0(cid:107)2 = 1  and hence β is the
signal to noise ratio. We will assume β to be known to avoid non-essential complications.

We consider estimators that return normalized vectors(cid:98)v : Rn×n → Sn−1 ≡ {v ∈ Rn : (cid:107)v(cid:107)2 = 1} 
RCn ((cid:98)v; v0) =
The corresponding worst-case risk is R((cid:98)v;Cn) ≡ supv0∈Cn RCn((cid:98)v; v0)  and the minimax risk
R(Cn) = inf(cid:98)v R((cid:98)v;Cn).

2)(cid:9) = 1 − E{|(cid:104)(cid:98)v(X)  v0(cid:105)|} .

E(cid:8) min((cid:107)(cid:98)v(X) − v0(cid:107)2

and will characterize such an estimator through the risk function

Remark 1.1. Let Cn = Sn k be the cone of vectors v0 that have at most k non-zero entries  all
positive  and with equal magnitude. The problem of testing whether β = 0 or β ≥ β0 within
the model (1) coincides with the problem of detecting a non-zero mean submatrix in a Gaussian
matrix. For the latter  Ma and Wu [20] proved that it cannot be accomplished in polynomial time
unless an algorithm exists for the so-called planted clique problem in a regime in which the latter is
conjectured to be hard.
This suggests that the problem of estimating v0 with rate-optimal minimax risk is hard for the
constraint set Cn = Sn k.
We next summarize our results. While –as shown by the last remark– optimal estimation is generi-
cally intractable for the model (1) under the constraint v0 ∈ Cn  we show that –roughly speaking– it
is instead tractable if Cn is a convex cone. Note that this does not follow from elementary convexity
considerations. Indeed  the maximum likelihood problem

(2)

maximize
subject to

(cid:104)v  Xv(cid:105)  
v ∈ Cn 

(cid:107)v(cid:107)2 = 1  

(3)

E(cid:8)(cid:13)(cid:13)PCn(g)(cid:13)(cid:13)2

(cid:9)  

is non-convex. Even more  solving exactly this optimization problem is NP-hard even for simple
choices of the convex cone Cn. For instance  if Cn = Pn ≡ {v ∈ Rn : v ≥ 0} is an orthant  then
solving the above is equivalent to copositive programming  which is NP-hard by reduction from
maximum independent sets [12  Chapter 7].
Our results naturally characterize the cone Cn through its statistical dimension [1]. If PCn denotes
the orthogonal projection on Cn  then the fractional statistical dimension of Cn is deﬁned as

δ(Cn) ≡ 1
n

2

(4)
where expectation is with respect to g ∼ N(0  In×n). Note that δ(Cn) ∈ [0  1] can be signiﬁcantly
smaller than 1. For instance  if Cn = Mn ≡ {v ∈ Rn
+ : ∀i   vi+1 ≥ vi} is the cone of non-
negative  monotone increasing sequences  then [9  Lemma 4.2] proves that δ(Cn) ≤ 20(log n)2/n.
Below is an informal summary of our results  with titles referring to sections where these are estab-
lished.

Information-theoretic limits. We prove that in order to estimate accurately v0  it is necessary

to have β (cid:38) (cid:112)δ(Cn). Namely  there exist universal constants c1  c2 > 0 such that  if

(cid:112)δ(Cn)  then R(Cn) ≥ c2.

β ≤ c1

2

Maximum likelihood estimator. Let(cid:98)vML(X) be the maximum-likelihood estimator  i.e. any solu-

tion of Eq. (3). We then prove that  for β ≥(cid:112)δ(Cn)
R((cid:98)vML;Cn) ≤ 4(cid:112)δ(Cn)

(5)
Low-complexity iterative estimator. In the special case Cn = Rn  the solution of the optimiza-
tion problem (3) is given by the eigenvector with the largest eigenvalue. A standard
low-complexity approach to computing the leading eigenvector is provided by the power
method. We consider a simple generalization that –starting from the initialization v0– al-
ternates between projection onto Cn and multiplication by (X + ρIn) (ρ > 0 is added to
improve convergence):

β

.

(cid:98)vt+1 =
PCn (ut)
ut = (X + ρIn)(cid:98)vt .
(cid:107)PCn (ut)(cid:107)2

 

We prove that  for t (cid:38) log n iterations  this algorithm yields an estimate with R((cid:98)vt;Cn) (cid:46)
(cid:112)δ(Cn)/β  and hence order optimal  for β (cid:38)(cid:112)δ(Cn). (Our proof technique requires the

(7)

initialization to have a positive scalar product with v0.)

(6)

Eλ1(Z;Cn) ≤ 2(cid:112)δ(Cn) .

As a side result of our analysis of the maximum likelihood estimator  we prove a new  el-
egant  upper bound on the value of the optimization problem (3)  denoted by λ1(Z;Cn) ≡
maxv∈Cn∩Sn−1(cid:104)v  Zv(cid:105). Namely

(8)
In the special case Cn = Rn  λ1(Z; Rn) is the largest eigenvalue of Z  and the above inequality
shows that this is bounded in expectation by 2. In this case  the bound is known to be asymptotically
tight [10]. In the supplementary material  we prove that it is tight for certain other examples such
as the nonnegative orthant and for circular cones (a.k.a. ice-cream cones). We conjecture that this
inequality is asymptotically tight for general convex cones.
Unless stated otherwise  in the following we will defer proofs to the Supplementary Material.

2

Information-theoretic limits

minimax risk can be bounded below for β (cid:46) (cid:112)δ(Cn). As is standard  our bound employs the

We use an information-theoretic argument to show that  under the observation model (1)  then the
so-called packing number of Cn.
Deﬁnition 2.1. For a cone Cn ⊆ Rn  we deﬁne its packing number N (Cn  ε) as the size of the
maximal subset X of Cn ∩ Sn−1 such that for every x1  x2 ∈ Cn ∩ Sn−1  (cid:107)x1 − x2(cid:107) ≥ ε.
We then have the following.
Theorem 1. There exist universal constants C1  C2 > 0 such that for any closed convex cone Cn
with δ(Cn) ≥ 3/n:

(cid:112)δ(Cn) ⇒ R(Cn) ≥ C2δ(Cn)

log(1/δ(Cn))

β ≤ C1

.

(9)

Notice that the last expression for the lower bound depends on the cone width  as it is to be expected:
even for β = 0  it is possible to estimate v0 with risk going to 0 if the cone Cn ‘shrinks’ as n → ∞.
The proof of this theorem is provided in Section 2 of the supplement.

3 Maximum likelihood estimator

Under the Gaussian noise model for Z  cf. Eq. (1)  the likelihood of observing X under a hypothesis
v is proportional to exp(−(cid:107)X − vvT(cid:107)2
F /2). Using the constraint that (cid:107)v(cid:107) = 1  it follows that any
solution of (3) is a maximum likelihood estimator.

3

Theorem 2. Consider the model as in (1). Then  when β ≥(cid:112)δ(Cn)  any solution(cid:98)vML(X) to the

maximum likelihood problem (3) satisﬁes

RCn ((cid:98)vML;Cn) ≤ min

(cid:40)

4(cid:112)δ(Cn)

(cid:41)

 

16
β2

.

(10)

β

for β (cid:38) 1  it shifts to a faster decay of 1/β2. We have made no attempt to optimize the constants in
the statement of the theorem  though we believe that the correct leading constant in either case is 1.
Note that without the cone constraint (or with Cn = Rn) the maximum likelihood estimator reduces

Thus  for β (cid:38)(cid:112)δ(Cn)  the risk of the maximum likelihood estimator decays as(cid:112)δ(Cn)/β while
to computing the principal eigenvector(cid:98)vPC of X. Recent results in random matrix theory [10] and
RCn ((cid:98)vPC;Cn) < 1 asymptotically) is obtained only when β > 1. Our result shows that this threshold
is  instead  reduced to(cid:112)δ(Cn)  which can be signiﬁcantly smaller than 1. The proof of this theorem

statistical decision theory [4] prove that in the case of principal eigenvector  a nontrivial risk (i.e.

is provided in Section 3 of the supplement.

4 Low-complexity iterative estimator

Sections 2 and 3 provide theoretical insight into the fundamental limits of estimation of v0 from
quadratic observations of the form βv0v0
T + Z. However  as previously mentioned  the maximum
likelihood estimator of Section 3 is NP-hard to compute  in general. In this section  we propose a
simple iterative algorithm that generalizes the well-known power iteration to compute the principal
eigenvector of a matrix. Furthermore  we prove that  given an initialization with positive scalar
product with v0  this algorithm achieves the same risk of the maximum likelihood estimator up to
constants. Throughout  the cone Cn is assumed to be convex.

Our starting point is the power iteration to compute the principal eigenvector (cid:98)vPC of X. This is
given by letting  for t ≥ 0: (cid:98)vt+1 = X(cid:98)vt/(cid:107)X(cid:98)vt(cid:107). Under our observation model  we have X =

T + Z with v0 ∈ Cn. We can incorporate this information by projecting the iterates on to the

βv0v0
cone Cn (see e.g. [19] for related ideas):

ut+1 = Xvt + ρvt.

(11)

PCn (ut)
(cid:107)PCn (ut)(cid:107)  
The projection is deﬁned in the standard way:

(cid:98)vt =

PCn (x) ≡ arg min
y∈Cn

(cid:107)y − x(cid:107)2.

(12)
If Cn is convex  then the projection is unique. We have implicitly assumed that the operation of
projecting to the cone Cn is available to the algorithm as a simple primitive. This is the case for many
convex cones of interest  such as the orthant Pn  the monotone cone Mn  and ice-cream cones the
projection is easy to compute. For instance  if Cn = Pn is the non-negative orthant PCn (x) = (x)+
is the non-negative part of x. For the monotone cone  the projection can be computed efﬁciently
through the pool-adjacent violators algorithm.
The memory term ρvt is necessary for our proof technique to go through. It is straightforward to
see that adding ρIn to the data X does not change the optimizers of the problem (3). The following
theorem provides deterministic conditions under which the distance between the iterative estimator
and the vector v0 can be bounded.

Theorem 3. Let(cid:98)vt be the power iteration estimator (11). Assume ρ > ∆ and that the noise matrix
If β > 4∆  and the initial point (cid:98)v0 ∈ Cn ∩ Sn−1 satisﬁes (cid:104)(cid:98)v0  v0(cid:105) ≥ 2∆/β  then there exits

max(cid:8)|(cid:104)x  Zy(cid:105)| : x  y ∈ Cn ∩ Sn−1(cid:9) ≤ ∆ .

t0 = t0(∆/β  ∆/ρ) < ∞ independent of n such that  for all t ≥ t0

Z satisﬁes:

(13)

.

(14)

(cid:107)(cid:98)vt − v0(cid:107) ≤ 4∆

β

4

We can apply this theorem to the Gaussian noise model to obtain the following bound on the risk of
the power iteration estimator.

Corollary 4.1. Under the model (1) let εn = 8(cid:112)log n/n. Assume that (cid:104)(cid:98)v0  v0(cid:105) > 0 and

β > 2((cid:112)δ(Cn) + εn) max(cid:0)2  (cid:104)(cid:98)v0  v0(cid:105)−1(cid:1) .
Then R((cid:98)vt Cn) ≤ 2δ(Cn) + εn

.

β

(15)

(16)

In other words  power iteration has risk within a constant from the maximum likelihood estimator 
provided an initialization is available whose scalar product with v0 is bounded away from zero. The
proofs of Theorem 3 and Corollary 4.1 are provided in Section 4 of the supplement.

5 A case study: sharp asymptotics and minimax results for the orthant
In this section  we will be interested in the example in which the cone Cn is the non-negative orthant
Cn = Pn. Non-negativity constraints within principal component analysis arise in non-negative
matrix factorization (NMF). Initially introduced in the context of chemometrics [23  22]  NMF at-
tracted considerable interest because of its applications in computer vision and topic modeling. In
particular  Lee and Seung [17] demonstrated empirically that NMF successfully identiﬁes parts of
images  or topics in documents’ corpora.
Note that the in applications of NMF to computer vision or topic modeling the setting is somewhat
different from the model studied here: X is rectangular instead of symmetric  and the rank is larger
than one. Such generalizations of our analysis will be the object of future work.
Here we will use the positive orthant to illustrate the results in previous sections. Further  we will
show that stronger results can be proved in this case  thanks to the separable structure of this cone.
Namely  we derive sharp asymptotics and we characterize the least-favorable vectors for the maxi-
mum likelihood estimator.
We denote by λ+(X) = λ1(X;Cn = Pn) the value of the optimization problem (3). Our ﬁrst result
yields the asymptotic value of this quantity for ‘pure noise ’ conﬁrming the general conjecture put
forward above.

Theorem 4. We have almost surely limn→∞ λ+(Z) = 2(cid:112)δ(Pn) =

√

2.

Next we characterize the risk phase transition: this result conﬁrms and strengthen Theorem 2.
Theorem 5. Consider estimation in the non-negative orthant Cn = Pn under the model (1). If
√
β ≤ 1/

2  then there exists a sequence of vectors {v0(n)}n≥0   such that almost surely

√
2  there exists a function β (cid:55)→ R+(β) with R+(β) < 1 for all β > 1/

√
2  and
For β > 1/
R+(β) ≥ 1 − 1/2β2  such that the following happens. For any sequence of vectors {v0(n)}n≥0 
we have  almost surely

n→∞ R(vML; v0(n)) = 1 .
lim

(17)

(18)

R(vML; v0(n)) ≤ R+(β) .

lim sup
n→∞

lated with the signal v0(n) if and only if β >(cid:112)δ(Cn) = 1/

√

2.

In other words  in the high-dimensional limit  the maximum likelihood estimator is positively corre-

Explicit (although non-elementary) expressions for R+(β) can be computed  along with the limit
value of the risk R(vML; v0(n)) for sequences of vectors {v0(n)}n≥1 whose entries empirical dis-
tribution converges. These results go beyond the scope of the present paper (but see Fig. 1 below for
illustration).
As a byproduct of our analysis  we can characterize the least-favorable choice of the signal v0.
Namely for k ∈ [1  n]  wee let u(n  k) denote a vector with (cid:98)k(cid:99) non-zero entries  all equal to

1/(cid:112)(cid:98)k(cid:99). Then we can prove that the asymptotic minimax risk is achieved along sequences of

vectors of this type.

5

Theorem 6. Consider estimation in the non-negative orthant Cn = Pn under the model (1)  and let
√
R+(β) be the same function as in Theorem 5. If β ≤ 1/

2 then there exists kn = o(n) such that

√

If β > 1/

n→∞ R(vML; u(n  kn)) = 1 .
lim

2 then there exists ε# = ε#(β) ∈ (0  1] such that

n→∞ R(vML; u(n  nε#)) = R+(β) .
lim

(19)

(20)

We refer the reader to [21] for a detailed analysis of the case of nonnegative PCA and the full proofs
of Theorems 4  5 and 6.

5.1 Approximate Message Passing
The next question is whether  in the present example Cn = Pn  the risk of the maximum likelihood
estimator can be achieved by a low-complexity iterative algorithm. We prove that this is indeed the
case (up to an arbitrarily small error)  thus conﬁrming Theorem 3. In order to derive an asymp-
totically exact analysis  we consider an ‘approximate message passing’ modiﬁcation of the power
iteration.
Let f (x) = (x)+/(cid:107)(x)+(cid:107)2 denote the normalized projector. We consider the iteration deﬁned by
v0 = (1  1  . . .   1)T/

n  v−1 = (0  0  . . .   0)T  and for t ≥ 0 

√

vt+1 = Xf (vt) − bt f (vt−1)

AMP
The algorithm AMP is a slight modiﬁcation of the projected power iteration algorithm up to adding
at each step the “memory term” −bt f (vt−1). As shown in [8  3] this term plays a crucial role in
allowing for an exact high-dimensional characterization. At each step the estimate produced by the

sequence is(cid:98)vt = (vt)+/(cid:107)(vt)+(cid:107)2. We have the following

and bt ≡ (cid:107)(vt)+(cid:107)0/{√

n(cid:107)(vt)+(cid:107)2}

Theorem 7. Let X be generated as in (1). Then we have  almost surely 

(cid:12)(cid:12)(cid:104)(cid:98)vML  XvML(cid:105) − (cid:104)(cid:98)vt  X(cid:98)vt(cid:105)(cid:12)(cid:12) = 0 .

t→∞ lim
lim
n→∞

(21)

5.2 Numerical illustration: comparison with classical PCA

We performed numerical experiments on synthetic data generated according to the model (1) and
with signal v0 = u(n  nε) as deﬁned in the previous section. We provide in the Appendix formulas

for the value of limn→∞(cid:104)v0 (cid:98)vML(cid:105)  which correspond to continuous black lines in the Figure 1. We
empirical average of (cid:104)(cid:98)vt  v0(cid:105) over 32 instances. Even for such moderate values of n  the asymptotic

compare these predictions with empirical values obtained by running AMP.
We generated samples of size n = 104  sparsity level ε ∈ {0.001  0.1  0.8}  and signal-to-noise
ratios β ∈ {0.05  0.10  . . .   1.5}.
In each case we run AMP for t = 50 iterations and plot the

predictions are remarkably accurate.
Observe that sparse vectors (small ε) correspond to the least favorable signal for small signal-to-
√
noise ratio β  while the situation is reverted for large values of β. In dashed green we represented
the theoretical prediction for ε → 0. The value β = 1/
2 corresponds to the phase transition. At

the bottom the images correspond to values of the correlation (cid:104)v0 (cid:98)vML(cid:105) for a grid of values of β and
of n  and ﬁxed ε = 0.05 and several value of β. For each point we plot the average of (cid:104)(cid:98)vt  v0(cid:105) after
t = 50 iteration  over 32 instances. The data suggest (cid:104)(cid:98)vML  v0(cid:105) + A n−b ≈ limn→∞(cid:104)v0  v+(cid:105) with

ε. The top left-hand frame in Figure 1 is obtained by repeating the experiment for a grid of values

b ≈ 0.5.

6 Polyhedral cones and convex relaxations
A polyhedral cone Cn is a closed convex cone that can be represented in the form Cn = {x ∈ Rn :
Ax ≥ 0} for some matrix A ∈ Rm×n. In section 5 we considered the non-negative orthant  which
is an example of polyhedral cone with A = In. A number of other examples of practical interest fall
within this category of cones. For instance  monotonicity or convexity of a vector v = (v1  . . .   vn)

6

predictions of Theorem 5  and dots represent empirical values of (cid:104)(cid:98)vt  v0(cid:105) for the AMP estimator

Figure 1: Numerical simulations with the model 1 for the positive orthant cone Cn = Pn. Top-
left: empirical deviation from asymptotic prediction. Top-right: black lines represent the theoretical
(in red) and (cid:104)v1  v0(cid:105) for standard PCA (in blue). Bottom: a comparison of theoretical asymptotic
values (left frame) and empirical values (right frame) of (cid:104)v0  vML(cid:105) for a range of β and ε.

an be enforced –in their discrete version– through inequality constraints (respectively vi+1− vi ≥ 0
and vi+1 − 2vi + vi−1 ≥ 0)  and hence give rise to polyhedral cones. Furthermore  it is possible to
approximate any convex cone Cn with a sequence of increasingly accurate polyhedral cones.
For a polyhedral cone  the maximum likelihood problem (3) reads:

maximize (cid:104)v  Xv(cid:105)
subject to: Av ≥ 0; (cid:107)v(cid:107) = 1.

(22)

The modiﬁed power iteration (11)  can be specialized to this case  via the appropriate projection.
The projection remains computationally feasible provided the matrix A is not too large. Indeed  it
is easy to show using convex duality that PCn (u) is given by:

PCn (u) = arg min(cid:8)(cid:107)Ax + u(cid:107)2  x ≥ 0(cid:9) .

This reduces the projection onto a general polyhedral cone to a non-negative least squares problem 
for which efﬁcient routines exist. In special cases such as the orthant  the projection is closed form.
In the case of polyhedral cones  it is possible to relax this problem (22) using a natural convex
surrogate. To see this  we introduce the variable V = vvT and write the following equivalent
version of problem 22:

maximize (cid:104)X  V(cid:105)
subject to: AVAT ≥ 0; Tr(V) = 1;

V(cid:23)0; rank(V) = 1.

Here the constraint AVAT ≥ 0 is to be interpreted as entry-wise non-negativity  while we write
V(cid:23)0 to denote that V is positive semideﬁnite. We can now relax this problem by dropping the rank
constraint:

maximize (cid:104)X  V(cid:105)
subject to: AVAT ≥ 0; Tr(V) = 1; V(cid:23)0.

(23)

Note that this increases the number of variables from n to n2  as V ∈ Rn×n  which results in a
signiﬁcant cost increase for standard interior point methods  over the power iteration (11). Further-

more  if the solution V is not rank one  it is not clear how one can use it to form an estimate(cid:98)v. On

the other hand  this convex relaxation yields a principled approach to bounding the sub-optimality

7

10110210310410−210−1100nDeviation from asymptotic Empiricaln−1/200.10.20.30.40.50.60.70.80.92−1/ 2β1ε = .001ε = .100ε = .800< v0  v+ >Non−negative PCAβεTheory Prediction 21/2|0.20.40.60.811.21.40.10.20.30.40.50.60.70.80.910.10.20.30.40.50.60.70.8βεEmpirical (n = 1000) 21/2|0.20.40.60.811.21.40.10.20.30.40.50.60.70.80.910.10.20.30.40.50.60.70.8Figure 2: Value of the maximum likelihood problem (3) for Cn = Pn  as approximated by power
iteration. The red line is the value achieved by power iteration  and the blue points the upper bound
obtained by dual witness (25). The gap at small β is due to the suboptimal choice of the dual witness 
since solving exactly Problem (24) yields the dual witness with value given by the teal circles. As
can be seen  they match exactly the value obtained by power iteration  showing zero duality gap!
The simulation is for n = 50 and 40 Monte Carlo iterations.

of the estimate provided by the power iteration. It is straightforward to derive the dual program of
(23):

minimize λ1(X + ATYA)
subject to: Y ≥ 0 

(24)

plementary Material  that such a local maximizer must satisfy the modiﬁed eigenvalue equation:

where Y is the decision variable  the constraint is interpreted as entry-wise nonnegativity as above 
and λ1(· ) denotes the largest eigenvalue. If one can construct a dual witness Y ≥ 0 such that

a certiﬁcate of convergence of the power iteration (11).
We next describe a construction of dual witness that we found empirically successful at large enough
signal-to-noise ratio. Assume that a heuristic (for instance  the modiﬁed power iteration (11)) has

λ1(X + ATYA) = (cid:104)(cid:98)v  X(cid:98)v(cid:105) for any estimator(cid:98)v  then this estimator is the maximum likelihood
estimator. In particular  using the power iteration estimator(cid:98)v =(cid:98)vt   such a dual witness can provide
produced an estimate (cid:98)v that is a local maximizer of the problem (3). It is is proved in the Sup-
X(cid:98)v = λ(cid:98)v − ATµ  with µ ≥ 0 and (cid:104)(cid:98)v  ATµ(cid:105) = 0.
(cid:16)
(cid:107)A(cid:98)v(cid:107)2

Note that Y((cid:98)v) is non-negative by construction and hence dual feasible. A direct calculation shows
that(cid:98)v is an eigenvector of the matrix X + ATYA with eigenvalue λ = (cid:104)(cid:98)v  X(cid:98)v(cid:105). We then obtain
Proposition 6.1. Let(cid:98)v be a local maximizer of the problem (3). If(cid:98)v is the principal eigenvector of
X + ATY((cid:98)v)A  then(cid:98)v is a global maximizer.

the following sufﬁcient condition for optimality.

We then suggest the witness

Y((cid:98)v) =

1

µ(cid:98)vTAT + A(cid:98)vµT(cid:17)

.

(25)

In Figure 2 we plot the average value of the objective function over 50 instances of the problem for
Cn = Pn  n = 100. We solved the maximum likelihood problem using the power iteration heuristics
(11)  and used the above construction to compute an upper bound via duality. It is possible to show
that this upper bound cannot be tight unless β > 1  but appears to be quite accurate. We also solve
the problem (24) directly for case of nonnegative PCA  and (rather surprisingly) the dual is tight for
every β > 0.

8

00.511.522.533.5411.522.533.544.5 βλ1(X + Y)Power IterationProposed dual witnessExact dual witnessReferences
[1] D. Amelunxen  M. Lotz  M. Mccoy  and J. Tropp. Living on the edge: a geometric theory of phase

transition in convex optimization. submitted  2013.

[2] Arash A Amini and Martin J Wainwright. High-dimensional analysis of semideﬁnite relaxations for
sparse principal components. In Information Theory  2008. ISIT 2008. IEEE International Symposium
on  pages 2454–2458. IEEE  2008.

[3] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs  with applications to

compressed sensing. IEEE Trans. on Inform. Theory  57:764–785  2011.

[4] Aharon Birnbaum  Iain M Johnstone  Boaz Nadler  and Debashis Paul. Minimax bounds for sparse pca

with noisy high-dimensional data. The Annals of Statistics  41(3):1055–1084  2013.

[5] Pratik Biswas and Yinyu Ye. Semideﬁnite programming for ad hoc wireless sensor network localization.
In Proceedings of the 3rd international symposium on Information processing in sensor networks  pages
46–54. ACM  2004.

[6] Emmanuel J Candes  Thomas Strohmer  and Vladislav Voroninski. Phaselift: Exact and stable signal
recovery from magnitude measurements via convex programming. Communications on Pure and Applied
Mathematics  66(8):1241–1274  2013.

[7] Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding.

arXiv:1311.5179  2013.

arXiv preprint

[8] D. L. Donoho  A. Maleki  and A. Montanari. Message Passing Algorithms for Compressed Sensing.

Proceedings of the National Academy of Sciences  106:18914–18919  2009.

[9] David Donoho  Iain Johnstone  and Andrea Montanari. Accurate prediction of phase transitions in
compressed sensingvia a connection to minimax denoising. IEEE Transactions on Information Theory 
59(6):3396 – 3433  2013.

[10] Delphine F´eral and Sandrine P´ech´e. The largest eigenvalue of rank one deformation of large wigner

matrices. Communications in mathematical physics  272(1):185–228  2007.

[11] James R Fienup. Phase retrieval algorithms: a comparison. Applied optics  21(15):2758–2769  1982.
[12] Bernd G¨artner and Jiri Matousek. Approximation algorithms and semideﬁnite programming. Springer 

2012.

[13] Kishore Jaganathan  Samet Oymak  and Babak Hassibi. Recovery of sparse 1-d signals from the mag-
nitudes of their fourier transform. In Information Theory Proceedings (ISIT)  2012 IEEE International
Symposium On  pages 1473–1477. IEEE  2012.

[14] Iain M Johnstone and Arthur Yu Lu. Sparse principal components analysis. Unpublished manuscript 

2004.

[15] Ian Jolliffe. Principal component analysis. Wiley Online Library  2005.
[16] Robert Krauthgamer  Boaz Nadler  and Dan Vilenchik. Do semideﬁnite relaxations really solve sparse

pca? arXiv preprint arXiv:1306.3690  2013.

[17] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization.

Nature  401(6755):788–791  1999.

[18] Xiaodong Li and Vladislav Voroninski. Sparse signal recovery from quadratic measurements via convex

programming. SIAM Journal on Mathematical Analysis  45(5):3019–3033  2013.

[19] Ronny Luss and Marc Teboulle. Conditional gradient algorithmsfor rank-one matrix approximations with

a sparsity constraint. SIAM Review  55(1):65–98  2013.

[20] Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix detection. arXiv preprint

arXiv:1309.5914  2013.

[21] Andrea Montanari and Emile Richard. Non-negative principal component analysis: Message passing

algorithms and sharp asymptotics. arXiv preprint arXiv:1406.4775  2014.

[22] Pentti Paatero. Least squares formulation of robust non-negative factor analysis. Chemometrics and

intelligent laboratory systems  37(1):23–35  1997.

[23] Pentti Paatero and Unto Tapper. Positive matrix factorization: A non-negative factor model with optimal

utilization of error estimates of data values. Environmetrics  5(2):111–126  1994.

[24] Amit Singer. A remark on global positioning from local distances. Proceedings of the National Academy

of Sciences  105(28):9507–9511  2008.

[25] Hui Zou  Trevor Hastie  and Robert Tibshirani. Sparse principal component analysis. Journal of compu-

tational and graphical statistics  15(2):265–286  2006.

9

,Yash Deshpande
Andrea Montanari
Emile Richard
Dan Feldman
Mikhail Volkov
Daniela Rus