2013,Sparse Inverse Covariance Estimation with Calibration,We propose a semiparametric procedure for estimating high dimensional sparse inverse covariance matrix. Our method  named ALICE  is applicable to the elliptical family. Computationally  we develop an efficient dual inexact iterative projection (${\rm D_2}$P) algorithm based on the alternating direction method of multipliers (ADMM). Theoretically  we prove that the ALICE estimator achieves the parametric rate of convergence in both parameter estimation and model selection. Moreover  ALICE calibrates regularizations when estimating each column of the inverse covariance matrix. So it not only is asymptotically tuning free  but also achieves an improved finite sample performance. We present numerical simulations to support our theory  and a real data example to illustrate the effectiveness of the proposed estimator.,Sparse Precision Matrix Estimation with Calibration

Tuo Zhao

Department of Computer Science

Johns Hopkins University

Department of Operations Research and Financial Engineering

Han Liu

Princeton University

Abstract

We propose a semiparametric method for estimating sparse precision matrix of
high dimensional elliptical distribution. The proposed method calibrates regular-
izations when estimating each column of the precision matrix. Thus it not only
is asymptotically tuning free  but also achieves an improved ﬁnite sample per-
formance. Theoretically  we prove that the proposed method achieves the para-
metric rates of convergence in both parameter estimation and model selection. We
present numerical results on both simulated and real datasets to support our theory
and illustrate the effectiveness of the proposed estimator.

1

Introduction

We study the precision matrix estimation problem: let X = (X1  ...  Xd)T be a d-dimensional ran-
dom vector following some distribution with mean µ ∈ Rd and covariance matrix Σ ∈ Rd×d  where
Σkj = EXkXj − EXkEXj. We want to estimate Ω = Σ−1 from n independent observations. To
make the estimation manageable in high dimensions (d/n → ∞)  we assume that Ω is sparse. That
is  many off-diagonal entries of Ω are zeros.
Existing literature in machine learning and statistics usually assumes that X follows a multivari-
ate Gaussian distribution  i.e.  X ∼ N (0  Σ). Such a distributional assumption naturally connects
sparse precision matrices with Gaussian graphical models (Dempster  1972)  and has motivated
numerous applications (Lauritzen  1996). To estimate sparse precision matrices for Gaussian dis-
tributions  many methods in the past decade have been proposed based on the sample covariance
estimator. Let x1  ...  xn ∈ Rd be n independent observations of X  the sample covariance estima-
tor is deﬁned as

S =

1
n

(xi − ¯x)(xi − ¯x)T with ¯x =

1
n

xi.

(1.1)

Banerjee et al. (2008); Yuan and Lin (2007); Friedman et al. (2008) take advantage of the Gaussian
likelihood  and propose the graphic lasso (GLASSO) estimator by solving
|Ωkj| 

(cid:98)Ω = argmin

− log |Ω| + tr(SΩ) + λ

(cid:88)

where λ > 0 is the regularization parameter. Scalable software packages for GLASSO have been
developed  such as huge (Zhao et al.  2012).
In contrast  Cai et al. (2011); Yuan (2010) adopt the pseudo-likelihood approach to estimate the pre-
cision matrix. Their estimators follow a column-by-column estimation scheme  and possess better

1

n(cid:88)

i=1

Ω

n(cid:88)

i=1

j k

denote the jth column of A  ||A∗j||1 =(cid:80)

theoretical properties. More speciﬁcally  given a matrix A ∈ Rd×d  let A∗j = (A1j  ...  Adj)T
k |Akj| and ||A∗j||∞ = maxk |Akj|  Cai et al. (2011)

obtain the CLIME estimator by solving
||Ω∗j||1

(cid:98)Ω∗j = argmin

Ω∗j

s.t. ||SΩ∗j − I∗j||∞ ≤ λ  ∀ j = 1  ...  d.

(1.2)

Computationally  (1.2) can be reformulated and solved by general linear program solvers. Theoret-
ically  let ||A||1 = maxj ||A∗j||1 be the matrix (cid:96)1 norm of A  and ||A||2 be the largest singular
value of A  (i.e.  the spectral norm of A)  Cai et al. (2011) show that if we choose

the CLIME estimator achieves the following rates of convergence under the spectral norm 

(cid:114)

λ (cid:16) ||Ω||1

log d

 

n

(cid:32)

||(cid:98)Ω − Ω||2
(cid:80)
2 = OP
k |Ωkj|q.

(cid:18) log d

(cid:19)1−q(cid:33)

 

||Ω||4−4q

1

s2

n

(1.3)

(1.4)

where q ∈ [0  1) and s = maxj
Despite of these good properties  the CLIME estimator in (1.2) has three drawbacks: (1) The theoret-
ical justiﬁcation heavily relies on the subgaussian tail assumption. When this assumption is violated 
the inference can be unreliable; (2) All columns are estimated using the same regularization param-
eter  even though these columns may have different sparseness. As a result  more estimation bias is
introduced to the denser columns to compensate the sparser columns. In another word  the estima-
tion is not calibrated (Liu et al.  2013); (3) The selected regularization parameter in (1.3) involves
the unknown quantity ||Ω||1. Thus we have to carefully tune the regularization parameter over a
reﬁned grid of potential values in order to get a good ﬁnite-sample performance. To overcome the
above three drawbacks  we propose a new sparse precision matrix estimation method  named EPIC
(Estimating Precision mIatrix with Calibration).
To relax the Gaussian assumption  our EPIC method adopts an ensemble of the transformed
Kendall’s tau estimator and Catoni’s M-estimator (Kruskal  1958; Catoni  2012). Such a semi-
parametric combination makes EPIC applicable to the elliptical distribution family. The elliptical
family (Cambanis et al.  1981; Fang et al.  1990) contains many multivariate distributions such as
Gaussian  multivariate t-distribution  Kotz distribution  multivariate Laplace  Pearson type II and
VII distributions. Many of these distributions do not have subgaussian tails  thus the commonly
used sample covariance-based sparse precision matrix estimators often fail miserably.
Moreover  our EPIC method adopts a calibration framework proposed in Gautier and Tsybakov
(2011)  which reduces the estimation bias by calibrating the regularization for each column. Mean-
while  the optimal regularization parameter selection under such a calibration framework does not
require any prior knowledge of unknown quantities (Belloni et al.  2011). Thus our EPIC estima-
tor is asymptotically tuning free (Liu and Wang  2012). Our theoretical analysis shows that if the
underlying distribution has a ﬁnite fourth moment  the EPIC estimator achieves the same rates of
convergence as (1.4). Numerical experiments on both simulated and real datasets show that EPIC
outperforms existing precision matrix estimation methods.

2 Background
We ﬁrst introduce some notations used throughout this paper. Given a vector v = (v1  . . .   vd)T ∈
Rd  we deﬁne the following vector norms:

(cid:88)

(cid:88)

||v||1 =

|vj|  ||v||2

2 =

j

j

j   ||v||∞ = max
v2

j

|vj|.

Given a matrix A ∈ Rd×d  we use A∗j = (A1j  ...  Adj)T to denote the jth column of A. We
deﬁne the following matrix norms:

(cid:88)

k j

kj  ||A||max = max
A2

k j

|Akj| 

||A||1 = max

j

||A∗j||1 ||A||2 = max

j

ψj(A)  ||A||2

F =

2

where ψj(A)’s are all singular values of A.
We then brieﬂy review the elliptical family. As a generalization of the Gaussian distribution  it has
the following deﬁnition.
Deﬁnition 2.1 (Fang et al. (1990)). Given µ ∈ Rd and Ξ ∈ Rd×d  where Ξ (cid:23) 0 and
rank(Ξ) = r ≤ d  we say that a d-dimensional random vector X = (X1  ...  X)T follows an
elliptical distribution with parameter µ  Ξ  and β  if X has a stochastic representation

X d= µ + βBU  

such that β ≥ 0 is a continuous random variable independent of U  U ∈ Sr−1 is uniformly dis-
tributed in the unit sphere in Rr  and Ξ = BBT .
Since we are interested in the precision matrix estimation  we assume that maxj EX 2
j is ﬁnite. Note
that the stochastic representation in Deﬁnition 2.1 is not unique  and existing literature usually im-
poses the constraint maxj Ξjj = 1 to make the distribution identiﬁable (Fang et al.  1990). However 
such a constraint does not necessarily make Ξ the covariance matrix. Here we present an alternative
representation as follows.
Proposition 2.2. If X has the stochastic representation X = µ + βBU as in Deﬁnition 2.1  given
Ξ = BBT   rank(Ξ) = r  and E(ξ2) = α < ∞  X can be rewritten as X = µ + ξAU  where

ξ = β(cid:112)r/α  A = B(cid:112)α/r and Σ = AAT . Moreover we have

E(ξ2) = r  E(X) = µ  and Cov(X) = Σ.

After the reparameterization in Proposition 2.2  the distribution is identiﬁable with Σ deﬁned as the
conventional covariance matrix.
Remark 2.3. Σ has the decomposition Σ = ΘZΘ  where Z is the Pearson correlation matrix 
and Θ = diag(θ1  ...  θd) with θj as the standard deviation of Xj. Since Θ is a diagonal matrix 
the precision Ω also has a similar decomposition Ω = Θ−1ΓΘ−1  where Γ = Z−1 is the inverse
correlation matrix.

3 Method

We propose a three-step method: (1) We ﬁrst use the transformed Kendall’s tau estimator and

Catoni’s M-estimator to obtain (cid:98)Z and (cid:98)Θ respectively. (2) We then plug (cid:98)Z into the calibrated in-
verse correlation matrix estimation to obtain(cid:98)Γ. (3) At last  we assemble(cid:98)Γ and (cid:98)Θ to obtain (cid:98)Ω.

3.1 Correlation Matrix and Standard Deviation Estimation

(cid:88)

(cid:16)

(cid:17)



(cid:98)τkj =

To estimate Z  we adopt the transformed Kendall’s tau estimator proposed in Liu et al. (2012). Given
n independent observations  x1  ...  xn  where xi = (xi1  ...  xid)T   we calculate the Kendall’s
statistic by

2

n(n − 1)

sign

(xij − xi(cid:48)j)(xik − xi(cid:48)k)

if j (cid:54)= k;

i<i(cid:48)

After a simple transformation  we obtain a correlation matrix estimator(cid:98)Z = [(cid:98)Zkj] =(cid:2)sin(cid:0) π
2(cid:98)τkj

(Liu et al.  2012; Zhao et al.  2013).
To estimate Θ = diag(θ1  ...  θd)  we adopt the Catoni’s M-estimator proposed in Catoni (2012).
We deﬁne

otherwise.

(cid:1)(cid:3)

1

where sign(0) = 0. Let (cid:98)mj be the estimator of EX 2
n(cid:88)

(cid:114) 2

(cid:18)
(xij −(cid:98)µj)

n(cid:88)

(cid:19)

= 0 

ψ

(cid:18)

j   we solve

ψ

(x2

ψ(t) = sign(t) log(1 + |t| + t2/2) 

i=1

where Kmax is an upper bound of maxj Var(Xj) and maxj Var(X 2

increasing function in t (cid:98)µj and (cid:98)mj are unique and can be obtained by the efﬁcient Newton-Raphson
method (Stoer et al.  1993). Then we can obtain(cid:98)θj using(cid:98)θj =

nKmax
j ). Since ψ(t) is a strictly

nKmax

i=1

(cid:19)

= 0.

(cid:114) 2
ij − (cid:98)mj)
(cid:113)(cid:98)mj −(cid:98)µ2

j.

3

3.2 Calibrated Inverse Correlation Matrix Estimation

We plugin(cid:98)Z into the following convex program 

((cid:98)Γ∗j (cid:98)τj) = argmin

Γ∗j  τj

||Γ∗j||1 + cτj

||(cid:98)ZΓ∗j − I∗j||∞ ≤ λτj  ||Γ∗j||1 ≤ τj  ∀ j = 1  ...  d.

s.t.

(3.1)
where c can be an arbitrary constant (e.g. c = 0.5). τj works as an auxiliary variable to calibrate the
regularization.
Remark 3.1. If we know τj = ||Ω∗j||1 in advance  we can consider a simple variant of the CLIME
estimator 

(cid:98)Ω∗j = argmin
((cid:98)Γ∗j (cid:98)τj) = argmin

Ω∗j
s.t.

||Ω∗j||1
||SΩ∗j − I∗j||∞ ≤ λτj  ∀ j = 1  ...  d.

Since we do not have any prior knowledge of τ(cid:48)

js  we consider the following replacement

||Ω∗j||1
||SΩ∗j − I∗j||∞ ≤ λτj  τj = ||Ω∗j||1 ∀ j = 1  ...  d.

Γ∗j  τj
s.t.

(3.2)

As can be seen  (3.2) is nonconvex due to the constraint τj = ||Ω∗j||1. Thus no global optimum can
be guaranteed in polynomial time.

From a computational perspective  (3.1) can be viewed as a convex relaxation of (3.2). Both the
objective function and the constraint in (3.1) contain τj to prevent from choosing τj either too large
or too small. Due to the complementary slackness  (3.1) eventually encourages the regularization
to be proportional to the (cid:96)1 norm of each column (weak sparseness). Therefore the estimation is
calibrated.
∗j ≥ 0  we can reformulate (3.1)
By introducing the decomposition Γ∗j = Γ+∗j − Γ−
as a linear program as follows 

∗j with Γ+∗j  Γ−

((cid:98)Γ+∗j (cid:98)Γ−

∗j (cid:98)τj) = argmin

Γ+∗j  Γ

−
∗j  τj

subjected to

∗j + cτj

1T Γ+∗j + 1T Γ−
 (cid:98)Z −(cid:98)Z −λ
−(cid:98)Z
(cid:98)Z −λ
Γ−
∗j
1T −1
τj
∗j ≥ 0  τj ≥ 0 
Γ+∗j ≥ 0  Γ−

 Γ+∗j

1T

(3.3)

 ≤

(cid:34) I∗j−I∗j

0

(cid:35)

 

where λ = (λ  ...  λ)T ∈ Rd. (3.3) can be solved by existing linear program solvers  and further
accelerated by the parallel computing techniques.
Remark 3.2. Though (3.1) looks more complicated than (1.2)  it is not necessarily more computa-
tionally difﬁcult. After the reparameterization  (3.3) contains 2d + 1 parameters to optimize  which
is of a similar scale to the linear program formulation as the CLIME method in Cai et al. (2011).

Our EPIC method does not guarantee the symmetry of the estimator(cid:98)Γ. Thus we need the following
symmetrization methods to obtain the symmetric replacement(cid:101)Γ.

(cid:101)Γkj =(cid:98)ΓkjI(|(cid:98)Γkj| ≤(cid:98)Γjk) +(cid:98)ΓjkI(|(cid:98)Γkj| >(cid:98)Γjk).

3.3 Precision Matrix Estimation

Once we obtain the estimated inverse correlation matrix (cid:101)Γ  we can recover the precision matrix

estimator by the ensemble rule 

Remark 3.3. A possible alternative is to directly estimate Ω by plugging a covariance estimator

into (3.1) instead of (cid:98)Z  but this direct estimation procedure makes the regularization parameter

(3.4)

selection sensitive to Var(X 2
j ).

(cid:98)Ω = (cid:98)Θ−1(cid:101)Γ(cid:98)Θ−1.
(cid:98)S = (cid:98)Θ(cid:98)Z(cid:98)Θ

4

4 Statistical Properties

|Γkj|q ≤ s  ||Γ||1 ≤ M

(cid:111)

 

In this section  we study statistical properties of the EPIC estimator. We deﬁne the following class
of sparse symmetric matrices 

Uq(s  M ) =

Γ ∈ Rd×d(cid:12)(cid:12)(cid:12) Γ (cid:31) 0  Γ = ΓT   max
(cid:110)

j

(cid:88)

k

where q ∈ [0  1) and (s  d  M ) can scale with the sample size n. We also impose the following
additional conditions:
(A.1) Γ ∈ Uq(s  M )
(A.2) maxj |µj| ≤ µmax  maxj θj ≤ θmax  minj θj ≥ θmin
(A.3) maxj EX 4
where µmax  K  θmax  and θmin are constants.
Before we proceed with our main results  we ﬁrst present the following key lemma.
Lemma 4.1. Suppose that X follows an elliptical distribution with mean µ  and covariance Σ =
ΘZΘ. Assume that (A.1)-(A.3) hold  given the transformed Kendall’s tau estimator and Catoni’s M-
estimator deﬁned in Section 3  there exist universal constants κ1 and κ2 such that for large enough
n 

j ≤ K

(cid:32)
(cid:32)

P

P

max

j

max
j k

(cid:114)
(cid:114)

| ≤ κ2

|(cid:98)θ−1
j − θ−1
|(cid:98)Zkj − Zkj| ≤ κ1

j

(cid:33)
(cid:33)

log d

n

log d

n

≥ 1 − 2
d3  

≥ 1 − 1
d3 .

Lemma 4.1 implies that both transformed Kendall’s tau estimator and Catoni’s M-estimator possess
good concentration properties  which enable us to obtain a consistent estimator of Ω.
The next theorem presents the rates of convergence under the matrix (cid:96)1 norm  spectral norm  Frobe-
nius norm  and max norm.
Theorem 4.2. Suppose that X follows an elliptical distribution. Assume (A.1)-(A.3) hold  there
exist universal constants C1  C2  and C3 such that by taking

for large enough n and p = 1  2  we have

(cid:114)

λ = κ1

log d

 

n

||(cid:98)Ω − Ω||2
||(cid:98)Ω − Ω||2
||(cid:98)Ω − Ω||max ≤ C3M 2

p ≤ C1M 4−4qs2
(cid:114)
F ≤ C2M 4−2qs

1
d

n

log d

 

(cid:18) log d
(cid:18) log d

(cid:19)1−q
(cid:19)1−q/2

n

 

(4.1)

 

with probability at least 1 − 3 exp(−3 log d). Moreover  when the exact sparsity holds (i.e.  q = 0) 

let E = {(k  j) | Ωkj (cid:54)= 0}  and (cid:98)E = {(k  j) | (cid:98)Ωkj (cid:54)= 0}  then we have P(cid:16)

(cid:17) → 1  if there

E ⊆ (cid:98)E

n

exists a large enough constant C4 such that

|Ωkj| ≥ C4M 2

min
(k j)∈E

(cid:114)

log d

n

.

The rates of convergence in Theorem 4.2 are comparable to those in Cai et al. (2011).
Remark 4.3. The selected tuning parameter λ in (4.1) does not involve any unknown quantity.
Therefore our EPIC method is asymptotically tuning free.

5

5 Numerical Simulations

In this section  we compare the proposed ALCE method with other methods including

(1) GLASSO.RC : GLASSO +(cid:98)S deﬁned in (3.4) as the input covariance matrix
(2) CLIME.RC: CLIME +(cid:98)S as the input covariance matrix

(3) CLIME.SM: CLIME + S deﬁned in (1.1) as the input covariance matrix

We consider three different settings for the comparison: (1) d = 100; (2) d = 200; (3) d = 400. We
adopt the following three graph generation schemes  as illustrated in Figure 1  to obtain precision
matrices.

(a) Chain

(b) Erd¨os-R´enyi

(c) Scale-free

Figure 1: Three different graph patterns. To ease the visualization  we choose d = 100.

We then generate n = 200 independent samples from the t-distribution1 with 5 degrees of freedom 
mean 0 and covariance Σ = Ω−1. For the EPIC estimator  we set c = 0.5 in (3.1). For the Catoni’s
M-estimator  we set Kmax = 102.
To evaluate the performance in parameter estimation  we repeatedly split the data into a training set
of n1 = 160 samples and a validation set of n2 = 40 samples for 10 times. We tune λ over a reﬁned
grid  then the selected optimal regularization parameter is

10(cid:88)

k=1

||(cid:98)Ω(λ k)(cid:98)Σ(k) − I||max 

λ = argmin

λ

where (cid:98)Ω(λ k) denotes the estimated precision matrix using the regularization parameter λ and the
training set in the kth split  and (cid:98)Σ(k) denotes the estimated covariance matrix using the validation

set in the kth split. Table 1 summarizes our experimental results averaged over 200 simulations. We
see that EPIC outperforms the competing estimators throughout all settings.
To evaluate the performance in model selection  we calculate the ROC curve of each obtained reg-
ularization path. Figure 2 summarizes ROC curves of all methods averaged over 200 simulations.
We see that EPIC also outperforms the competing estimators throughout all settings.

6 Real Data Example

To illustrate the effectiveness of the proposed EPIC method  we adopt the breast cancer data2  which
is analyzed in Hess et al. (2006). The data set contains 133 subjects with 22 283 gene expression
levels. Among the 133 subjects  99 have achieved residual disease (RD) and the remaining 34 have
achieved pathological complete response (pCR). Existing results have shown that the pCR subjects
have higher chance of cancer-free survival in the long term than the RD subject. Thus we are
interested in studying the response states of patients (with RD or pCR) to neoadjuvant (preoperative)
chemotherapy.

1The marginal variances of the distribution vary from 0.5 to 2.
2Available at http://bioinformatics.mdanderson.org/.

6

(a) d = 100

(b) d = 200

(c) d = 400

(d) d = 100

(e) d = 200

(f) d = 400

(g) d = 100

(h) d = 200

(i) d = 400

Figure 2: Average ROC curves of different methods on the chain (a-c)  Erd¨os-R´enyi (d-e)  and scale-
free (f-h) models. We can see that EPIC uniformly outperforms the competing estimators throughout
all settings.

We randomly divide the data into a training set of 83 RD and 29 pCR subjects  and a testing set of the
remaining 16 RD and 5 pCR subjects. Then by conducting a Wilcoxon test between two categories
for each gene  we further reduce the dimension by choosing the 113 most signcant genes with the
smallest p-values. We assume that the gene expression data in each category is elliptical distributed 
and the two categories have the same covariance matrix Σ but different means µ(k)  where k = 0
for RD and k = 1 for pCR. In Cai et al. (2011)  the sample mean is adopted to estimate µ(k)’s  and
CLIME.RC is adopted to estimate Ω = Σ−1. In contrast  we adopt the Catoni’s M-estimator to
estimate µk’s  and EPIC is adopted to estimate Ω. We classify a sample x to pCR if

(cid:18)

(cid:19)T (cid:98)Ω

(cid:16)(cid:98)µ(1) −(cid:98)µ(0)(cid:17) ≥ 0 

x − (cid:98)µ(1) +(cid:98)µ(0)

2

and to RD otherwise. We use the testing set to evaluate the performance of CLIME.RC and EPIC.
For the tuning parameter selection  we use a 5-fold cross validation on the training data to pick λ
with the minimum classiﬁcation error rate.
To evaluate the classiﬁcation performance  we use the criteria of speciﬁcity  sensitivity  and Mathews

Correlation Coefﬁcient (MCC). More speciﬁcally  let yi’s and(cid:98)yi’s be true labels and predicted labels

7

0.000.010.020.030.040.050.00.20.40.60.81.0False Positive RateTrue Positive RateEPICGLASSO.RCCLIME.RCCLIME.SC0.000.010.020.030.040.050.00.20.40.60.81.0False Positive RateTrue Positive RateEPICGLASSO.RCCLIME.RCCLIME.SC0.000.010.020.030.040.050.00.20.40.60.81.0False Positive RateTrue plot(c(e RateEPICGLASSO.RCCLIME.RCCLIME.SC0.000.010.020.030.040.050.00.20.40.60.81.0False Positive RateTrue Positive RateEPICGLASSO.RCCLIME.RCCLIME.SC0.000.010.020.030.040.050.00.20.40.60.8False Positive RateTrue Positive RateEPICGLASSO.RCCLIME.RCCLIME.SC0.000.010.020.030.040.050.00.10.20.30.40.5False Positive RateTrue Positive RateEPICGLASSO.RCCLIME.RCCLIME.SC0.000.010.020.030.040.050.00.20.40.60.8False Positive RateTrue Positive RateEPICGLASSO.RCCLIME.RCCLIME.SC0.000.010.020.030.040.050.00.10.20.30.40.50.6False Positive RateTrue Positive RateEPCGLASSO.RCCLIME.RCCLIME.SC0.000.010.020.030.040.050.00.20.40.60.8False Positive RateTrue Positive RateEPICGLASSO.RCCLIME.RCCLIME.SCTable 1: Quantitive comparison of EPIC  GLASSO.RC  CLIME.RC  and CLIME.SC on the chain 
Erd¨os-R´enyi  and scale-free models. We see that EPIC outperforms the competing estimators
throughout all settings.

Spectral Norm: ||(cid:98)Ω − Ω||2

Model

Chain

Erd¨os-R´enyi

Scale-free

Model

Chain

Erd¨os-R´enyi

Scale-free

d
100
200
400
100
200
400
100
200
400

d
100
200
400
100
200
400
100
200
400

EPIC

0.8405(0.1247)
0.9147(0.1009)
1.0058(0.1231)
0.9846(0.0970)
1.1944(0.0704)
1.9010(0.0462)
0.9779(0.1379)
2.9278(0.3367)
1.1816(0.1201)

GLASSO.RC
1.1880(0.1003)
1.3433(0.0870)
1.4842(0.0760)
1.6037(0.2289)
1.6105(0.0680)
2.2613(0.1133)
1.6619(0.1553)
4.0882(0.0962)
1.8304(0.0710)

Frobenius Norm: ||(cid:98)Ω − Ω||F

CLIME.RC

CLIME.SC

0.9337(0.5389)
1.0716(0.4939)
1.3567(0.3706)
1.6885(0.1704)
1.7507(0.0389)
2.6884(0.5988)
2.1327(0.0986)
4.5820(0.0604)
2.1191(0.0629)

3.2991(0.0512)
3.7303(0.4477)
3.8462(0.4827)
3.7158(0.0663)
3.5209(0.0419)
4.1342(0.1079)
3.4548(0.0513)
8.8904(0.0575)
3.4249(0.0849)

EPIC

3.3108(0.1521)
5.0309(0.1833)
7.5134(0.1205)
3.5122(0.0796)
6.3000(0.0868)
11.489(0.0858)
2.6369(0.1125)
4.1280(0.1389)
5.3440(0.0511)

GLASSO.RC
4.5664(0.1034)
7.2154(0.0831)
11.300(0.1851)
3.9600(0.1459)
7.3385(0.0994)
12.594(0.1633)
3.1154(0.1001)
7.7543(0.0934)
6.3741(0.0723)

CLIME.RC

CLIME.SC

3.4406(0.4319)
5.4776(0.2586)
7.8357(1.2217)
4.4212(0.1065)
7.3501(0.1589)
13.026(0.4124)
3.1363(0.1014)
7.8916(0.0556)
5.7643(0.0625)

16.282(0.1346)
23.403(0.2727)
33.504(0.1341)
13.734(0.0629)
20.151(0.1899)
30.030(0.1289)
10.717(0.0844)
16.370(0.1490)
20.687(0.1373)

of the testing samples  we deﬁne

Speciﬁcity =

TN

MCC =

where

TP

TP + FN

 

TN + FP

  Sensitivity =
TPTN − FPFN

(cid:112)(TP + FP)(TP + FN)(TN + FP)(TN + FN)
(cid:88)
(cid:88)
I((cid:98)yi = yi = 1)  FP =
I((cid:98)yi = 0  yi = 1)
(cid:88)
(cid:88)
I((cid:98)yi = yi = 0)  FN =
I((cid:98)yi = 1  yi = 0).

i

i

 

TP =

TN =

i

i

Table 2 summarizes the performance of both methods over 100 replications. We see that EPIC
outperforms CLIME.RC on the speciﬁcity. The overall classiﬁcation performance measured by
MCC shows that EPIC has a 4% improvement over CLIME.RC.

Table 2: Quantitive comparison of EPIC and CLIME.RC in the breast cancer data analysis.

Method

CLIME.RC

EPIC

Speciﬁcity

0.7412(0.0131)
0.7935(0.0211)

Sensitivity

0.7911(0.0251)
0.8087(0.0324)

MCC

0.4905(0.0288)
0.5301(0.0375)

8

References
BANERJEE  O.  EL GHAOUI  L. and D’ASPREMONT  A. (2008). Model selection through sparse
maximum likelihood estimation for multivariate gaussian or binary data. The Journal of Machine
Learning Research 9 485–516.

BELLONI  A.  CHERNOZHUKOV  V. and WANG  L. (2011). Square-root lasso: pivotal recovery of

sparse signals via conic programming. Biometrika 98 791–806.

CAI  T.  LIU  W. and LUO  X. (2011). A constrained (cid:96)1 minimization approach to sparse precision

matrix estimation. Journal of the American Statistical Association 106 594—607.

CAMBANIS  S.  HUANG  S. and SIMONS  G. (1981). On the theory of elliptically contoured distri-

butions. Journal of Multivariate Analysis 11 368–385.

CATONI  O. (2012). Challenging the empirical mean and empirical variance: a deviation study.

Annales de l’Institut Henri Poincar´e  Probabilit´es et Statistiques 48 1148–1185.

DEMPSTER  A. P. (1972). Covariance selection. Biometrics 157–175.
FANG  K.-T.  KOTZ  S. and NG  K. W. (1990). Symmetric Multivariate and Related Distribu-
tions  Monographs on Statistics and Applied Probability  36. London: Chapman and Hall Ltd.
MR1071174.

FRIEDMAN  J.  HASTIE  T. and TIBSHIRANI  R. (2008). Sparse inverse covariance estimation with

the graphical lasso. Biostatistics 9 432–441.

GAUTIER  E. and TSYBAKOV  A. B. (2011). High-dimensional instrumental variables regression

and conﬁdence sets. Tech. rep.  ENSAE ParisTech.

HESS  K. R.  ANDERSON  K.  SYMMANS  W. F.  VALERO  V.  IBRAHIM  N.  MEJIA  J. A. 
BOOSER  D.  THERIAULT  R. L.  BUZDAR  A. U.  DEMPSEY  P. J. ET AL. (2006). Pharma-
cogenomic predictor of sensitivity to preoperative chemotherapy with paclitaxel and ﬂuorouracil 
doxorubicin  and cyclophosphamide in breast cancer. Journal of clinical oncology 24 4236–4244.
KRUSKAL  W. H. (1958). Ordinal measures of association. Journal of the American Statistical

Association 53 814–861.

LAURITZEN  S. L. (1996). Graphical models  vol. 17. Oxford University Press.
LIU  H.  HAN  F.  YUAN  M.  LAFFERTY  J. and WASSERMAN  L. (2012). High-dimensional

semiparametric gaussian copula graphical models. The Annals of Statistics 40 2293–2326.

LIU  H. and WANG  L. (2012). Tiger: A tuning-insensitive approach for optimally estimating

gaussian graphical models. Tech. rep.  Massachusett Institute of Technology.

LIU  H.  WANG  L. and ZHAO  T. (2013). Multivariate regression with calibration. arXiv preprint

arXiv:1305.2238 .

STOER  J.  BULIRSCH  R.  BARTELS  R.  GAUTSCHI  W. and WITZGALL  C. (1993). Introduction

to numerical analysis  vol. 2. Springer New York.

YUAN  M. (2010). High dimensional inverse covariance matrix estimation via linear programming.

The Journal of Machine Learning Research 11 2261–2286.

YUAN  M. and LIN  Y. (2007). Model selection and estimation in the gaussian graphical model.

Biometrika 94 19–35.

ZHAO  T.  LIU  H.  ROEDER  K.  LAFFERTY  J. and WASSERMAN  L. (2012). The huge package
for high-dimensional undirected graph estimation in r. The Journal of Machine Learning Research
9 1059–1062.

ZHAO  T.  ROEDER  K. and LIU  H. (2013). Positive semideﬁnite rank-based correlation matrix
estimation with application to semiparametric graph estimation. Journal of Computational and
Graphical Statistics To appear.

9

,Tuo Zhao
Han Liu
Sanghamitra Dutta
Viveck Cadambe
Pulkit Grover