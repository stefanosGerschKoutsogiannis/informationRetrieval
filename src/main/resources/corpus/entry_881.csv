2016,CNNpack: Packing Convolutional Neural Networks in the Frequency Domain,Deep convolutional neural networks (CNNs) are successfully used in a number of applications. However  their storage and computational requirements have largely prevented their widespread use on mobile devices. Here we present an effective CNN compression approach in the frequency domain  which focuses not only on smaller weights but on all the weights and their underlying connections. By treating convolutional filters as images  we decompose their representations in the frequency domain as common parts (i.e.  cluster centers) shared by other similar filters and their individual private parts (i.e.  individual residuals). A large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy. We relax the computational burden of convolution operations in CNNs by linearly combining the convolution responses of discrete cosine transform (DCT) bases. The compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods.,CNNpack: Packing Convolutional Neural Networks

in the Frequency Domain

Yunhe Wang1 3  Chang Xu2  Shan You1 3  Dacheng Tao2  Chao Xu1 3

1Key Laboratory of Machine Perception (MOE)  School of EECS  Peking University

2Centre for Quantum Computation and Intelligent Systems 

School of Software  University of Technology Sydney

3Cooperative Medianet Innovation Center  Peking University

wangyunhe@pku.edu.cn  Chang.Xu@uts.edu.au  youshan@pku.edu.cn 

Dacheng.Tao@uts.edu.au  xuchao@cis.pku.edu.cn

Abstract

Deep convolutional neural networks (CNNs) are successfully used in a number
of applications. However  their storage and computational requirements have
largely prevented their widespread use on mobile devices. Here we present an
effective CNN compression approach in the frequency domain  which focuses not
only on smaller weights but on all the weights and their underlying connections.
By treating convolutional ﬁlters as images  we decompose their representations
in the frequency domain as common parts (i.e.  cluster centers) shared by other
similar ﬁlters and their individual private parts (i.e.  individual residuals). A
large number of low-energy frequency coefﬁcients in both parts can be discarded
to produce high compression without signiﬁcantly compromising accuracy. We
relax the computational burden of convolution operations in CNNs by linearly
combining the convolution responses of discrete cosine transform (DCT) bases.
The compression and speed-up ratios of the proposed algorithm are thoroughly
analyzed and evaluated on benchmark image datasets to demonstrate its superiority
over state-of-the-art methods.

1

Introduction

Thanks to the large amount of accessible training data and computational power of GPUs  deep
learning models  especially convolutional neural networks (CNNs)  have been successfully applied to
various computer vision (CV) applications such as image classiﬁcation [19]  human face veriﬁcation
[20]  object recognition  and object detection [7  17]. However  most of the widely used CNNs can
only be used on desktop PCs or even workstations due to their demanding storage and computational
resource requirements. For example  over 232MB of memory and over 7.24 × 108 multiplications
are required to launch AlexNet and VGG-Net per image  preventing them from being used in mobile
terminal apps on smartphones or tablet PCs. Nevertheless  CV applications are growing in importance
for mobile device use and there is  therefore  an imperative to develop and use CNNs for this purpose.
Considering the lack of GPU support and the limited storage and CPU performance of mainstream
mobile devices  compressing and accelerating CNNs is essential. Although CNNs can have millions
of neurons and weights  recent research [9] has highlighted that over 85% of weights are useless
and can be set to 0 without an obvious deterioration in performance. This suggests that the gap in
demands made by large CNNs and the limited resources offered by mobile devices may be bridged.
Some effective algorithms have been developed to tackle this challenging problem. [8] utilized vector
quantization to allow similar connections to share the same cluster center. [6] showed that the weight
matrices can be reduced by low-rank decomposition approaches.[4] proposed a network architecture

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1: The ﬂowchart of the proposed CNNpack.

using the “hashing trick” and [4] then transferred the HashedNet into the discrete cosine transform
(DCT) frequency domain [3]. [16  5] proposed binaryNet  whose weights were -1/1 or -1/0/1 [2]. [15]
utilizes a sparse decomposition to reduce the redundancy of weights and computational complexity of
CNNs. [9] employed pruning [10]  quantization  and Huffman coding to obtain a greater than 35×
compression ratio and 3× speed improvement  thereby producing state-of-the-art CNNs compression
to the best of our knowledge. The effectiveness of the pruning strategy relies on the principle that
if the absolute value of a weight in a CNN is sufﬁciently small  its inﬂuence on the output is often
negligible. However  these methods tend to disregard the properties of larger weights  which might
also provide opportunities for compression. Moreover  independently considering each weight ignores
the contextual information of other weights.
To address the aforementioned problems  we propose handling convolutional ﬁlters in the frequency
domain using DCT (see Fig. 1). In practice  convolutional ﬁlters can be regarded as small and
smooth image patches. Hence  any operation on the convolutional ﬁlter frequency coefﬁcients in the
frequency domain is equivalent to an operation performed simultaneously over all weights of the
convolutional ﬁlters in the spatial domain. We factorize the representation of the convolutional ﬁlter
in the frequency domain as the composition of common parts shared with other similar ﬁlters and
its private part describing some unique information. Both parts can be signiﬁcantly compressed by
discarding a large number of subtle frequency coefﬁcients. Furthermore  we develop an extremely fast
convolution calculation scheme that exploits the relationship between the feature maps of DCT bases
and frequency coefﬁcients. We have theoretically discussed the compression and the speed-up of
the proposed algorithm. Experimental results on benchmark datasets demonstrate that our proposed
algorithm can consistently outperform state-of-the-art competitors  with higher compression ratios
and speed gains.

2 Compressing CNNs in the Frequency Domain

Recently developed CNNs contain a large number of convolutional ﬁlters. We regard convolutional
ﬁlters as small images with intrinsic patterns  and present an approach to compress CNNs in the
frequency domain with the help of the DCT.

2.1 The Discrete Cosine Transform (DCT)

The DCT plays an important role in JPEG compression [22]  which is regarded as an approximate
KL-transformation for 2D images [1]. In JPEGs  the original image is usually divided into several
square patches. For an image patch P ∈ Rn×n  its DCT coefﬁcient C ∈ Rn×n in the frequency
domain is deﬁned as:

Cj1j2 = D(Pi1i2 ) = sj1 sj2

where sj = (cid:112)1/n if j = 0 and sj = (cid:112)2/n  otherwise  and C = C T P C is the matrix form of

the DCT  where C = [c1  ...  cd] ∈ Rd×d is the transformation matrix. The basis of this DCT is

α(i1  i2  j1  j2)Pi1i2 = cT
j1

i1=0

i2=0

P cj2 

(1)

n−1(cid:88)

n−1(cid:88)

2

Sj1j2 = cj1cT

j2  and α(i1  i2  j1  j2) denotes the cosine basis function:

(cid:18) π(2i1 + 1)j1

(cid:19)

(cid:18) π(2i2 + 1)j2

(cid:19)

 

(2)

cos

2n

2n

α(i1  i2  j1  j2) = cos

(cid:16) π(2i+1)j

(cid:17)

and cj(i) =
image by simply utilizing the inverse DCT  i.e. 

2n

. The DCT is a lossless transformation thus we can recover the original

Pi1i2 = D−1(Cj1j2) =

sj1sj2α(i1  i2  j1  j2)Cj1j2  

(3)

whose matrix form is P = CCC T . Furthermore  to facilitate the notations we denote the DCT and the
inverse DCT for vectors as vec(C) = D(vec(P )) = (C ⊗ C)vec(P ) and vec(P ) = D−1(vec(C)) =
(C ⊗ C)T vec(C)  where vec(·) is the vectorization operation and ⊗ is the Kronecker product.

j1=0

j2=0

n−1(cid:88)

n−1(cid:88)

1   ...  F (i)
Ni
] ∈ Rd2

2.2 Convolutional Layer Compression
Computing Residuals in the Frequency Domain. For a given convolutional layer Li  we ﬁrst
extract its convolutional ﬁlters F (i) = {F (i)
}  where the size of each convolutional ﬁlter is
di × di and Ni is the number of ﬁlters in Li. Each ﬁlter can then be transformed into a vector  and
j ) ∀ j = 1  ...  Ni.
i ×Ni  where x(i)
together they form a matrix Xi = [x(i)
DCT has been widely used for image compression  since DCT coefﬁcients present an experienced
distribution in the frequency domain. Energies of high-frequency coefﬁcients are usually much
smaller than those of low-frequency coefﬁcients for 2D natural images  i.e.  the high frequencies
tend to have values equal or close to zero [22]. Hence  we propose to transfer Xi into the DCT
frequency domain and obtain its frequency representation C = D(Xi) = [C1  ... CNi ]. Since a
number of convolutional ﬁlters will share some similar components  we divide them into several
groups G = {G1  ...  GK} by exploiting K centers U = [µ1  ...  µk] ∈ Rdi×K with the following
minimization problem:

j = vec(F (i)

1   ...  x(i)
Ni

||C − µk||2
2 

(4)

K(cid:88)

(cid:88)

k=1

C∈Gk

arg min
G

(5)

where µk is the cluster center of Gk. Fcn. 4 can easily be solved with the conventional k-means
algorithm [9  8]. For each Cj  we denote its residual with its corresponding cluster as Rj = Cj − µkj  
where kj = arg mink ||Cj−µk||2. Hence  each convolutional ﬁlter is represented by its corresponding
cluster center shared by other similar ﬁlters and its private part Rj in the frequency domain. We
further employ the following (cid:96)1-penalized optimization problem to control redundancy in the private
parts for each convolutional ﬁlter:

||(cid:98)Rj − Rj||2

2 + λ||(cid:98)Rj||1 
arg min(cid:98)Rj
(cid:98)Rj = sign(Rj) (cid:12) max{|Rj| − λ

  0} 

where λ is a parameter for balancing the reconstruction error and sparsity penalty. The solution to
Fcn. 5 is:

(6)
where sign(·) is the sign function. We can also control the redundancy in the cluster centers using the
above approach as well.
Quantization  Encoding  and Fine-tuning. The sparse data obtained through Fcn. 6 is continuous 
which is not beneﬁt for storing and compressing. Hence we need to represent similar values with
a common value  e.g.  [0.101  0.100  0.102  0.099] → 0.100. Inspired by the conventional JPEG

algorithm  we use the following function to quantize (cid:98)Rj:
(cid:40)

(cid:41)

2

Rj = Q(cid:16)(cid:98)Rj  Ω  b
(cid:17)

= Ω · I

Clip((cid:98)Rj −b  b)

Ω

 

(7)

where Clip(x −b  b) = max(−b  min(b  x)) with boundary b > 0  and Ω is a large integer with
similar functionality to the quantization table in the JPEG algorithm. It is useful to note that the

3

quantized values produced by applying Fcn. 7 can be regarded as the one-dimensional k-means
centers in [9]  thus a dictionary can consist of unique values in all {R1  ... RNi}. Since occurrence
probabilities of elements in the codebook are unbalanced  we employ the Huffman encoding for a
more compact storage. Finally  the Huffman encoded data is stored in the compressed sparse row
format (CSR)  denoted as Ei.
It has also been shown that ﬁne-tuning after compression further enhances network accuracy [10  9].
In our algorithm  we also employ the ﬁne-tuning approach by holding weights that have been
discarded so that the ﬁne-tuning operation does not change the compression ratio. After generating a
new model  we apply Fcn. 7 again to quantize the new model’s parameters until convergence.
The above scheme for compressing convolutional CNNs stores four types of data: the compressed
data Ei  the Huffman dictionary with Hi quantized values  the k-means centers U ∈ Rd2
i ×K  and
the indexes for mapping residual data to centers. It is obvious that if all ﬁlters from every layer can
share the same Huffman dictionary and cluster centers  the compression ratio will be signiﬁcantly
increased.

2.3 CNNpack for CNN Compression

Global Compression Scheme. To enable all convolutional ﬁlters to share the same cluster centers
U ∈ Rd2
i ×k in the frequency domain  we must convert them into a ﬁxed-dimensional space. It is
intuitive to directly resize all convolutional ﬁlters into matrices of the same dimensions and then
apply k-means. However  this simple resizing method increases the amount of the data that needs to
be stored. Considering d as the target dimension and di × di as the convolutional ﬁlter size of the
i-th layer  the weight matrix would be inaccurately reshaped in the case of di < d or di > d.
A more reasonable approach would be to resize the DCT coefﬁcient matrices of convolutional ﬁlters
in the frequency domain  because high-frequency coefﬁcients are generally small and discarding
them only has a small impact on the result (di > d). On the other hand  the introduced zeros will be
immediately compressed by CSR since we do not need to encode or store them (di < d). Formally 
the resizing operation for convolutional ﬁlters in the DCT frequency domain can be deﬁned as:

(cid:98)Cj1 j2 = Γ(C  d) =

(cid:26) Cj1 j2 

if j1  j2 ≤ d 

0 

otherwise.

(8)
where d × d is the ﬁxed ﬁlter size  C ∈ Rdi×di is the DCT coefﬁcient matrix of a ﬁlter in the i-th

layer and (cid:98)C ∈ Rd×d is the coefﬁcient matrix after resizing. After applying Fcn. 8  we can pack all

the coefﬁcient matrices together and use only one set of cluster centers to compute the residual data
and then compress the network. We extend the individual compression scheme into an integrated
method that is convenient and effective for compressing deep CNNs  which we call CNNpack. The
procedures of the proposed algorithm are detailed in the supplementary materials.
CNNpack has ﬁve hyper-parameters: λ  d  K  b  and Ω. Its compression ratio can be calculated by:

(cid:80)p
(cid:0)Ni log K + Bi

(cid:80)p

i=1

i=1 32Nid2
i

(cid:1) + 32H + 32d

rc =

2

 

K

(9)

where p is the number of convolutional layers and H is the bits for storing the Huffman dictionary. It
is instructive to note that a larger λ (Fcn. 5) puts more emphasis on the common parts of convolutional
ﬁlters  which leads to a higher compression ratio rc. A decrease in any of b  d and Ω will increase
rc accordingly. Parameter K is related to the sparseness of Ei  and a larger K would contribute
more to Ei’s sparseness but would lead to higher storage requirement. A detailed investigation of
all these parameters is presented in Section. 4  and we also demonstrate and validate the trade-off
between the compression ratio and CNN accuracy of the convolutional neural work (i.e.  classiﬁcation
accuracy [19]).

3 Speeding Up Convolutions

According to Fcn. 9  we can obtain a good compression ratio by converting the convolutional ﬁlters
into the DCT frequency domain and representing them using their corresponding cluster centers and

4

residual data Ri = [R1  ... RNi ]. This is an effective strategy to reduce CNN storage requirements
and transmission consumption. However  two other important issues need to be emphasized: memory
usage and complexity.
This section focuses on a single layer throughout  we will drop the layer index i in order to simplify
notation. In practice  the residual data R in the frequency domain cannot be simply used to calculate
convolutions  so we must ﬁrst transform them to the original ﬁlters in the spatial domain  thus not
saving memory. Furthermore  the transformation will further increase algorithm complexity. It is
thus necessary to explore a scheme that enables the proposed compression method to easily calculate
feature maps of original ﬁlters in the DCT frequency domain.
Given a convolutional layer L with its ﬁlters F = {Fq}N
q=1 of size d × d  we denote the input data
as X ∈ RH×W and its output feature maps as Y = {Y1  Y2  ...  YN} with size H(cid:48) × W (cid:48)  where
Yq = Fq ∗ X for the convolution operation ∗. Here we propose a scheme which decomposes the
calculation of the conventional convolutions in the DCT frequency domain.
For the DCT matrix C = [c1  ...  cd]  the d × d convolutional ﬁlter Fq can be represented by its
DCT coefﬁcient matrix C(q) with DCT bases {Sj1 j2}d
j2  namely 
Sj1 j2. In this way  feature maps of X through F can be calculated
(Sj1 j2 ∗ X)  where M = d2 is the number of DCT bases. Fortunately 
since the DCT is an orthogonal transformation  all of its bases are rank-1 matrices. Note that

(cid:80)d
Fq = (cid:80)d
as Yq = (cid:80)d
j2=1 C(q)
Sj1 j2 ∗ X =(cid:0)cj1cT
j1 j2=1 C(q)

(cid:1) ∗ X  and thus feature map Yq can be written as

j1 j2=1 deﬁned as Sj1 j2 = cj1cT

j1=1

j1 j2

j1 j2

j2

d(cid:88)

d(cid:88)

Yq = Fq ∗ X =

C(q)

j1 j2

(Sj1 j2 ∗ X) =

C(q)

j1 j2

[cj1 ∗ (cT

j2

∗ X)].

(10)

j1 j2=1

j1 j2=1

One drawback of this scheme is that when the number of ﬁlters in this layer is relatively small  i.e. 
M ≈ N  Fcn. 10 will increase computational cost. Moreover  the complexity can be further reduced
given the fascinating fact that the feature maps calculated by these DCT bases are exactly the same as
the DCT frequency coefﬁcients of the input data. For a d × d matrix X  we consider the matrix form
of its DCT  i.e.  C = C T XC  and DCT bases {Sj1 j2}  then the coefﬁcient Cj1 j2 can be calculated
as
Cj1 j2 = cT
j2 ) ∗ X = Sj1 j2 ∗ X  (11)
thus we can obtain the feature maps of M DCT bases by only applying the DCT once. Thus the
computational complexity of our proposed scheme can be analyzed in Proposition 1.
Proposition 1. Given a convolutional layers with N ﬁlters  denote M = d× d as the number of DCT
base  and C ∈ Rd2×N as the compressed coefﬁcients of ﬁlters in this layer. Suppose δ is the ratio of
non-zero elements in C  while η is the ratio of non-zero elements in K(cid:48) active cluster centers of this
layer. The computational complexity of our proposed scheme is O((d2 log d+ηM K(cid:48)+δM N )H(cid:48)W (cid:48)).

j2 ∗ X) = cj1 ∗ (cT

j2 ∗ X) = (cj1 ∗ cT

j2 ) ∗ X = (cj1 cT

j1 Xcj2 = cT

j1 (cT

The proof of Proposition 1 can be found in the supplementary materials. According to Proposition 1 
the proposed compression scheme in the frequency domain can also improve the speed. Compared to
the original CNN  for a convolutional layer  the speed-up of the proposed method is

rs =

d2N H(cid:48)W (cid:48)

(d2 log d + ηK(cid:48)M + δN M )H(cid:48)W (cid:48) ≈

N

ηK(cid:48) + δN

.

(12)

Obviously  the speed-up ratio of the proposed method is directly relevant to η and δ  which correspond
to λ in Fcn. 6.

4 Experimental Results

Baselines and Models. We compared the proposed approach with 4 baseline approaches: Prun-
ing [10]  P+QH (Pruning + Quantization and Huffman encoding) [9]  SVD [6]  and XNOR-Net [16].
The evaluation was conducted using the MNIST and ILSVRC2012 datasets. We compared the
proposed compression approach with four baseline CNNs: LeNet [14  21]  AlexNet [13]  VGG-16
Net [19]  and ResNet-50 [11]. All methods were implemented using MatConvNet [21] and run on

5

NVIDIA Titan X graphics cards. Model parameters were stored and updated as 32 bit ﬂoating-point
values.
Impact of parameters. As discussed above  the proposed compression method has several important
parameters: λ  d  K  b  and Ω. We ﬁrst tested their impact on the network accuracy by conducting
an experiment using MNIST [21]  where the network has two convolutional layers and two fully-
connected layers of size 5 × 5 × 1 × 20  5 × 5 × 20 × 50  4 × 4 × 50 × 500  and 1 × 1 × 500 × 10 
respectively. The model accuracy was 99.06%. The compression results of different λ and d after
ﬁne-tuning 20 epochs are shown in Fig. 2 in which k was set as 16  b was equal to +∞ since it did
not make an obvious contribution to the compression ratio even when set at a relatively smaller value
(e.g.  b = 0.05) but caused the accuracy reduction. Ω was set to 500  making the average length
of weights in the frequency domain about 5  a bit larger than that in [9] but more ﬂexible and with
relatively better performance. Note that all the training parameters used their the default settings 
such as for epochs  learning rates  etc..
It can be seen from Fig. 2 that although a lower d slightly improves the compression ratio and
speed-up ratio simultaneously  this comes at a cost of decreased overall network accuracy; thus  we
kept d = max{di}  ∀ i = 1  ...  p  in CNNpack. Overall  λ is clearly the most important parameter in
the proposed scheme  which is sensitive but monotonous. Thus  it only needs to be adjusted according
to demand and restrictions. Furthermore  we tested the impact of number of cluster centers K. As
mentioned above  K is special in that its impact on performance is not intuitive  and when it becomes
larger  E becomes sparser but needs more space for storing cluster centers U and indexes. Fig. 3
shows that K = 16 provides the best trade-off between compression performance and accuracy.

Figure 2: The performance of the proposed approach with different λ and d.

Figure 3: The performance of the proposed approach with different numbers of cluster centers K.
We also report the compression results by directly compressing the DCT frequency coefﬁcients of
original ﬁlters C as before (i.e.  K = 0  the black line in Fig. 3). It can be seen that the clustering
number does not affect accuracy  but a suitable K does enhance the compression ratio. Another
interesting phenomenon is that the speed-up ratio without decomposition is larger than that of the
proposed scheme because the network is extremely small and the clustering introduces additional
computational cost as shown in Fcn. 12. However  recent networks contain a lot more ﬁlters in a
convolutional layer  larger than K = 16.
Based on the above analysis  we kept λ = 0.04 and K = 16 for this network (an accuracy of 99.14%).
Accordingly  the compression ratio rc = 32.05× and speed-up ratio rs = 8.34×  which is the best
trade-off between accuracy and compression performance.
Filter visualization. The proposed algorithm operates in the frequency domain  and although we
do not need to invert the compressed net when calculating convolutions  we can reconstruct the
convolutional ﬁlters in the spatial domain to provide insights into our approach. Reconstructed
convolution ﬁlters are obtained from the LeNet on MNIST  as shown in Fig. 4.

6

0.9800.9840.9880.9920.0250.0350.0450.055Accuracyld- = 5d- = 4d- = 3204060800.0250.0350.0450.055Compression ratiold- = 5d- = 4d- = 35101520250.0250.0350.0450.055Speedup ratiold- = 5d- = 4d- = 30.9860.9880.9900.9920.0250.0350.0450.055AccuracylK = 16K = 64K = 128K = 0204060800.0250.0350.0450.055Compression ratiolK = 16K = 64K = 128K = 051015200.0250.0350.0450.055Speedup ratiolK = 16K = 64K = 128K = 0Figure 4: Visualization of example ﬁlters learned on MNIST: (a) the original convolutional ﬁlters  (b)
ﬁlters after pruning  (c) convolutional ﬁlters compressed by the proposed algorithm.

The proposed approach is fundamentally different to the previously used pruning algorithm. Ac-
cording to Fig. 4(b)  weights with smaller magnitudes are pruned while inﬂuences of larger weights
have been discarded. In contrast  our proposed algorithm not only handles the smaller weights but
also considers impacts of those larger weights. Most importantly  we accomplish the compressing
task by exploring the underlying connections between all the weights in the convolutional ﬁlter (see
Fig. 4(c)).
Compression AlexNet and VGGNet on ImageNet. We next employed CNNpack for CNN com-
pression on the ImageNet ILSVRC-2012 dataset [18]  which contains over 1.2M training images and
50k validation images. First  we examined two conventional models: AlexNet [13]  with over 61M
parameters and a top-5 accuracy of 80.8%; and VGG-16 Net  which is much larger than the AlexNet
with over 138M parameters and has a top-5 accuracy of 90.1%. Table 1 shows detailed compression
and speed-up ratios of the AlexNet with λ = 0.04 and K = 16. The result of the VGG-16 Net can
be found in the supplementary materials. The reported multiplications are for computing one image.

Table 1: Compression statistics for AlexNet.

Layer
conv1
conv2
conv3
conv4
conv5
fc6
fc7
fc8
Total

Num of Weights
11 × 11 × 3 × 96
5 × 5 × 48 × 256
3 × 3 × 256 × 384
3 × 3 × 192 × 384
3 × 3 × 192 × 256
6 × 6 × 256 × 4096
1 × 1 × 4096 × 4096
1 × 1 × 4096 × 1000

60954656

Memory
0.13MB
1.17MB
3.37MB
2.53MB
1.68MB
144MB
64MB

15.62MB
232.52MB

rc
878×
94×
568×
42×
43×
148×
15×
121×
39×

Multiplication
1.05×108
2.23×108
1.49×108
1.12×108
0.74×108
0.37×108
0.16×108
0.04×108
7.24×108

rs
127×
28×
33×
15×
12×
100×
8×
60×
25×

We achieved a 39× compression ratio and a 46× compression ratio for AlexNet and VGG-16 Net 
respectively. The layer with a relatively larger ﬁlter size had a larger compression ratio because it
contains more subtle high-frequency coefﬁcients. In contrast  the highest speed-up ratio was often
obtained on the layer whose ﬁlter number N was much larger than its ﬁlter size  e.g.  the fc6 layer of
AlexNet. We obtained an about 9× speed-up ratio on VGG-16 Net  which is lower than that on the
AlexNet since complexity is relevant to the feature map size and the ﬁrst several layers deﬁnitely have
more multiplications. Unfortunately  their ﬁlter numbers are relatively small and their compression
ratios are all small  thus the overall speed-up ratio is lower than that on AlexNet. Accordingly  when
we set K = 0  the compression ratio and the speed-up ratio of AlexNet were close to 35× and 22×
and those of VGG-16 Net were near 28× and 7×. This reduction is due to these two networks being
relatively large and containing many similar ﬁlters. Moreover  the ﬁlter number in each layer is larger
than the number of cluster centers  i.e.  N > K. Thus  cluster centers can effectively reduce memory
consumption and computational complexity simultaneously.
ResNet-50 on ImageNet. Here we discuss a more recent work  ResNet-50 [11]  which has more
than 150 layers and 54 convolutional layers. This model achieves a top-5 accuracy of 7.71% and
a top-1 accuracy of 24.62% with only about 95MB parameters [21]. Moreover  since most of its
convolutional ﬁlters are 1 × 1  i.e.  its architecture is very thin with less redundant weights  it is very
hard to perform compression methods on it.
For the experiment on ResNet-50  we set K = 0 since the functionality of Fcn. 7 for 1-dimensional
ﬁlters are similar to that of k-means clustering  thus cluster centers are dispensable for this model.
Further  we set λ = 0.04 and therefore discarded about 84% of original weights in the frequency

7

(a) Compression ratios of all convolutional layers. (b) Speed-up ratios of all convolutional layers.

Figure 5: Compression statistics for ResNet-50 (better viewed in color version).

domain. After ﬁne-tuning  we obtained a 7.82% top-5 accuracy on ILSVRC2012 dataset. Fig. 5
shows the detailed compression statistics of ResNet-50 utilizing the proposed CNNpack. In summary 
memory usage for storing its ﬁlters was squeezed by a factor of 12.28×.
It is worth mentioning that  larger ﬁlters seem have more redundant weights and connections because
compression ratios on layers with 1×1 ﬁlters are smaller than those on layers with 3×3 ﬁlters. On
the other side  these 1×1 ﬁlters hold a larger proportion of multiplications of the ResNet  thus we got
an about 4× speed-up on this network.
Comparison with state-of-the-art methods. We detail a comparison with state-of-the-art-methods
for compressing DNNs in Table 2. CNNpack clearly achieves the best performance in terms of both
the compression ratio (rc) and the speed-up ratio (rs). Note that although Pruning+QH achieves a
similar compression ratio to the proposed method  ﬁlters in their algorithm are stored after applying a
modiﬁed CSC format which stores index differences  means that it needs to be decoded before any
calculation. Hence  the compression ratio of P+QH will be lower than that have been reported in [9]
if we only consider memory usage. In contrast  the compressed data produced by our method can be
directly used for network calculation. In reality  online memory usage is the real restriction for mobile
devices  and the proposed method is superior to previous works in terms of both the compression
ratio and the speed-up ratio.

Table 2: An overall comparison of state-of-the-art methods for deep neural network compression and
speed-up  where rc is the compression ratio and rs is the speed-up.

Model

Evaluation Original

Pruning [10]

P+QH [9]

SVD [6] XNOR [16]

64×
58×
56.8%
31.8%

-
-
-
-

CNNpack

39×
25×
41.6%
19.2%
46×
9.4×
29.7%
10.4%

1
1

1
1

41.8%
19.2%

28.5%
9.9%

AlexNet

VGG16

rc
rs

top-1 err
top-5 err

rc
rs

top-1 err
top-5 err

5 Conclusion

9×
-

42.7%
19.6%
13×
-

31.3
10.8

35×
-

42.7%
19.7%
49×
3.5×
31.1%
10.9%

5×
2×
44.0%
20.5%

-
-
-
-

Neural network compression techniques are desirable so that CNNs can be used on mobile devices.
Therefore  here we present an effective compression scheme in the DCT frequency domain  namely 
CNNpack. Compared to state-of-the-art methods  we tackle this issue in the frequency domain 
which can offer the probability for more compression ratio and speed-up. Moreover  we no longer
independently consider each weight since each frequency coefﬁcient’s calculation involves all
weights in the spatial domain. Following the proposed compression approach  we explore a much
cheaper convolution calculation based on the sparsity of the compressed net in the frequency domain.
Moreover  although the compressed network produced by our approach is sparse in the frequency
domain  the compressed model has the same functionality as the original network since ﬁlters in the
spatial domain have preserved intrinsic structure.Our experiments show that the compression ratio
and the speed-up ratio are both higher than those of state-of-the-art methods. The proposed CNNpack
approach creates a bridge to link traditional signal and image compression with CNN compression
theory  allowing us to further explore CNN approaches in the frequency domain.
Acknowledgements This work was supported by the National Natural Science Foundation of China
under Grant NSFC 61375026 and 2015BAF15B00  and Australian Research Council Projects: FT-
130101457  DP-140102164 and LE-140100061.

8

References
[1] Nasir Ahmed  T Natarajan  and Kamisetty R Rao. Discrete cosine transform. Computers  IEEE Transac-

tions on  100(1):90–93  1974.

[2] Sanjeev Arora  Aditya Bhaskara  Rong Ge  and Tengyu Ma. Provable bounds for learning some deep

representations. ICML  2014.

[3] Wenlin Chen  James T Wilson  Stephen Tyree  Kilian Q Weinberger  and Yixin Chen. Compressing

convolutional neural networks. arXiv preprint arXiv:1506.04449  2015.

[4] Wenlin Chen  James T Wilson  Stephen Tyree  Kilian Q Weinberger  and Yixin Chen. Compressing neural

networks with the hashing trick. 2015.

[5] Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights and

activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830  2016.

[6] Emily L Denton  Wojciech Zaremba  Joan Bruna  Yann LeCun  and Rob Fergus. Exploiting linear structure

within convolutional networks for efﬁcient evaluation. In NIPs  2014.

[7] Ross Girshick  Jeff Donahue  Trevor Darrell  and Jitendra Malik. Rich feature hierarchies for accurate

object detection and semantic segmentation. In CVPR  2014.

[8] Yunchao Gong  Liu Liu  Ming Yang  and Lubomir Bourdev. Compressing deep convolutional networks

using vector quantization. arXiv preprint arXiv:1412.6115  2014.

[9] Song Han  Huizi Mao  and William J Dally. Deep compression: Compressing deep neural networks with

pruning  trained quantization and huffman coding. 2016.

[10] Song Han  Jeff Pool  John Tran  and William Dally. Learning both weights and connections for efﬁcient

neural network. In NIPs  2015.

[11] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

arXiv preprint arXiv:1512.03385  2015.

[12] Yangqing Jia  Evan Shelhamer  Jeff Donahue  Sergey Karayev  Jonathan Long  Ross Girshick  Sergio
In

Guadarrama  and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding.
Proceedings of the ACM International Conference on Multimedia  2014.

[13] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPs  2012.

[14] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[15] Baoyuan Liu  Min Wang  Hassan Foroosh  Marshall Tappen  and Marianna Pensky. Sparse convolutional

neural networks. In CVPR  2015.

[16] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet classiﬁca-

tion using binary convolutional neural networks. arXiv preprint arXiv:1603.05279  2016.

[17] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster r-cnn: Towards real-time object detection

with region proposal networks. In NIPs  2015.

[18] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
Andrej Karpathy  Aditya Khosla  Michael Bernstein  et al. Imagenet large scale visual recognition challenge.
IJCV  115(3):211–252  2015.

[19] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. ICLR  2015.

[20] Yi Sun  Yuheng Chen  Xiaogang Wang  and Xiaoou Tang. Deep learning face representation by joint

identiﬁcation-veriﬁcation. In NIPs  2014.

[21] Andrea Vedaldi and Karel Lenc. Matconvnet: Convolutional neural networks for matlab. In Proceedings

of the 23rd Annual ACM Conference on Multimedia Conference  2015.

[22] Gregory K Wallace. The jpeg still picture compression standard. Consumer Electronics  IEEE Transactions

on  38(1):xviii–xxxiv  1992.

9

,Igor Colin
Aurélien Bellet
Joseph Salmon
Stéphan Clémençon
Yunhe Wang
Chang Xu
Dacheng Tao
Chao Xu