2013,Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation,We provide a detailed study of the estimation of probability distributions---discrete and continuous---in a stringent setting in which data is kept private even from the statistician.  We give sharp minimax rates of convergence for estimation in these locally private settings  exhibiting fundamental tradeoffs between privacy and convergence rate  as well as providing tools to allow movement along the privacy-statistical efficiency continuum. One of the consequences of our results is that Warner's classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents.,Local Privacy and Minimax Bounds:

Sharp Rates for Probability Estimation

John C. Duchi1

Michael I. Jordan1 2

Martin J. Wainwright1 2

1Department of Electrical Engineering and Computer Science
University of California  Berkeley

2Department of Statistics

{jduchi jordan wainwrig}@eecs.berkeley.edu

Abstract

We provide a detailed study of the estimation of probability distributions—
discrete and continuous—in a stringent setting in which data is kept private even
from the statistician. We give sharp minimax rates of convergence for estimation
in these locally private settings  exhibiting fundamental trade-offs between pri-
vacy and convergence rate  as well as providing tools to allow movement along
the privacy-statistical efﬁciency continuum. One of the consequences of our re-
sults is that Warner’s classical work on randomized response is an optimal way to
perform survey sampling while maintaining privacy of the respondents.

1

Introduction

The original motivation for providing privacy in statistical problems  ﬁrst discussed by Warner [23] 
was that “for reasons of modesty  fear of being thought bigoted  or merely a reluctance to conﬁde
secrets to strangers ” respondents to surveys might prefer to be able to answer certain questions
non-truthfully  or at least without the interviewer knowing their true response. With this motivation 
Warner considered the problem of estimating the fractions of the population belonging to certain
strata  which can be viewed as probability estimation within a multinomial model. In this paper  we
revisit Warner’s probability estimation problem  doing so within a theoretical framework that allows
us to characterize optimal estimation under constraints on privacy. We also apply our theoretical
tools to a further probability estimation problem—that of nonparametric density estimation.

In the large body of research on privacy and statistical inference [e.g.  23  14  10  15]  a major focus
has been on the problem of reducing disclosure risk: the probability that a member of a dataset
can be identiﬁed given released statistics of the dataset. The literature has stopped short  however 
of providing a formal treatment of disclosure risk that would permit decision-theoretic tools to be
used in characterizing trade-offs between the utility of achieving privacy and the utility associated
with an inferential goal. Recently  a formal treatment of disclosure risk known as “differential
privacy” has been proposed and studied in the cryptography  database and theoretical computer
science literatures [11  1]. Differential privacy has strong semantic privacy guarantees that make it a
good candidate for declaring a statistical procedure or data collection mechanism private  and it has
been the focus of a growing body of recent work [13  16  24  21  6  18  8  5  9].

In this paper  we bring together the formal treatment of disclosure risk provided by differential pri-
vacy with the tools of minimax decision theory to provide a theoretical treatment of probability
estimation under privacy constraints. Just as in classical minimax theory  we are able to provide
lower bounds on the convergence rates of any estimator  in our case under a restriction to esti-
mators that guarantee privacy. We complement these results with matching upper bounds that are
achievable using computationally efﬁcient algorithms. We thus bring classical notions of privacy 
as introduced by Warner [23]  into contact with differential privacy and statistical decision theory 
obtaining quantitative trade-offs between privacy and statistical efﬁciency.

1

1.1 Setting and contributions

Let us develop some basic formalism before describing our main results. We study procedures that
receive private views Z1  . . .   Zn ∈ Z of an original set of observations  X1  . . .   Xn ∈ X   where
X is the (known) sample space.
In our setting  Zi is drawn conditional on Xi via the channel
distribution Qi(Zi | Xi = x); typically we omit the dependence of Qi on i. We focus in this paper
on the non-interactive setting (in information-theoretic terms  on memoryless channels)  where Qi
is chosen prior to seeing data; see Duchi et al. [9] for more discussion.

We assume each of these private views Zi is α-differentially private for the original data Xi. To give
a precise deﬁnition for this type of privacy  known as “local privacy ” let σ(Z) be the σ-ﬁeld on Z
over which the channel Q is deﬁned. Then Q provides α-local differential privacy if

sup(cid:26) Q(S | Xi = x)

Q(S | Xi = x′) | S ∈ σ(Z)  and x  x′ ∈ X(cid:27) ≤ exp(α).

(1)

This formulation of local privacy was ﬁrst proposed by Evﬁmievski et al. [13]. The likelihood ratio
bound (1) is attractive for many reasons. It means that any individual providing data guarantees
his or her own privacy—no further processing or mistakes by a collection agency can compromise
one’s data—and the individual has plausible deniability about taking a value x  since any outcome z
is nearly as likely to have come from some other initial value x′. The likelihood ratio also controls
the error rate in tests for the presence of points x in the data [24].

In the current paper  we study minimax convergence rates when the data provided satisﬁes the local
privacy guarantee (1). Our two main results quantify the penalty that must be paid when local
privacy at a level α is provided in multinomial estimation and density estimation problems. At a
high level  our ﬁrst result implies that for estimation of a d-dimensional multinomial probability
mass function  the effective sample size of any statistical estimation procedure decreases from n to
nα2/d whenever α is a sufﬁciently small constant. A consequence of our results is that Warner’s
randomized response procedure [23] enjoys optimal sample complexity; it is interesting to note
that even with the recent focus on privacy and statistical inference  the optimal privacy-preserving
strategy for problems such as survey collection has been known for almost 50 years.

Our second main result  on density estimation  exhibits an interesting departure from standard min-
imax estimation results. If the density being estimated has β continuous derivatives  then classical
results on density estimation [e.g.  26  25  22] show that the minimax integrated squared error scales
(in the sample size n) as n−2β/(2β+1). In the locally private case  we show that there is a difference
in the polynomial rate of convergence: we obtain a scaling of (α2n)−2β/(2β+2). We give efﬁciently
implementable algorithms that attain sharp upper bounds as companions to our lower bounds  which
in some cases exhibit the necessity of non-trivial sampling strategies to guarantee privacy.

Notation: Given distributions P and Q deﬁned on a space X   each absolutely continuous with
respect to a measure µ (with densities p and q)  the KL-divergence between P and Q is

Dkl (PkQ) :=ZX

dP log

dP
dQ

p
q

dµ.

p log

=ZX
2ZX |p(x) − q(x)| dµ(x).

1

Letting σ(X ) denote an appropriate σ-ﬁeld on X   the total variation distance between P and Q is

kP − QkTV := sup

S∈σ(X )|P (S) − Q(S)| =

Let X be distributed according to P and Y | X be distributed according to Q(· | X)  and let

M =R Q(· | x)dP (x) denote the marginal of Y . The mutual information between X and Y is

I(X; Y ) := EP [Dkl (Q(· | X)kM (·))] =Z Dkl (Q(· | X = x)kM (·)) dP (x).

A random variable Y has Laplace(α) distribution if its density pY (y) = α
2 exp (−α|y|). We write
an . bn to denote an = O(bn) and an ≍ bn to denote an = O(bn) and bn = O(an). For a convex
set C ⊂ Rd  we let ΠC denote the orthogonal projection operator onto C.

2

Mn(θ(P)  Φ ◦ ρ  α) := inf

bθ Q∈Qα

sup
P ∈P

EP QhΦ(cid:16)ρ(bθ(Z1  . . .   Zn)  θ(P ))(cid:17)i .

(2)

2 Background and Problem Formulation

In this section  we provide the necessary background on the minimax framework used throughout
the paper  more details of which can be found in standard sources [e.g.  17  25  26  22]. We also
reference our work [9] paper on statistical inference under differential privacy constraints; we restate
two theorems from the paper [9] to keep our presentation self-contained.

2.1 Minimax framework

Let P denote a class of distributions on the sample space X   and let θ : P → Θ denote a function
deﬁned on P. The range Θ depends on the underlying statistical model; for example  for density
estimation  Θ may consist of the set of probability densities deﬁned on [0  1]. We let ρ denote the
semi-metric on the space Θ that we use to measure the error of an estimator for θ  and Φ : R+ → R+
be a non-decreasing function with Φ(0) = 0 (for example  Φ(t) = t2).

Recalling that Z is the domain of the private variables Zi  let bθ : Z n → Θ denote an arbitrary

estimator for θ. Let Qα denote the set of conditional (or channel) distributions guaranteeing α-local
privacy (1). Looking uniformly over all channels Q ∈ Qα  we deﬁne the central object of interest
for this paper  the α-private minimax rate for the family θ(P) 

associated with estimating θ based on (Z1  . . .   Zn). We remark here (see also the discussion in [9])
that the private minimax risk (2) is different from previous work on optimality in differential privacy
(e.g. [2  16  8]): prior work focuses on accurate estimation of a sample quantity θ(x1:n) based on
the sample x1:n  while we provide lower bounds on error of the population estimator θ(P ). Lower
bounds on population estimation imply those on sample estimation  so our lower bounds are stronger
than most of those in prior work.

A standard route for lower bounding the minimax risk (2) is by reducing the estimation problem to
the testing problem of identifying a point θ ∈ Θ from a collection of well-separated points [26  25].
Given an index set V  the indexed family of distributions {Pν  ν ∈ V} ⊂ P is a 2δ-packing of Θ
if ρ(θ(Pν)  θ(Pν ′ )) ≥ 2δ for all ν 6= ν′ in V. The setup is that of a standard hypothesis testing
problem: nature chooses V ∈ V uniformly at random  then data (X1  . . .   Xn) are drawn i.i.d. from
P n
ν   conditioning on V = ν. The problem is to identify the member ν of the packing set V.
In this work we have the additional complication that all the statistician observes are the private sam-

ples Z1  . . .   Zn. To that end  if we let Qn(· | x1:n) denote the conditional distribution of Z1  . . .   Zn
given that X1 = x1  . . .   Xn = xn  we deﬁne the marginal channel M n

ν via the expression

M n

ν (A) :=Z Qn(A | x1  . . .   xn)dPν(x1  . . .   xn) for A ∈ σ(Z n).

(3)

Letting ψ : Z n → V denote an arbitrary testing procedure  we have the following minimax bound 

whose two parts are known as Le Cam’s two-point method [26  22] and Fano’s inequality [25  7  22].

Lemma 1 (Minimax risk bound). For the previously described estimation and testing problems 

Mn(θ(P)  Φ ◦ ρ  Q) ≥ Φ(δ) inf

ψ

P(ψ(Z1  . . .   Zn) 6= V ) 

(4)

where the inﬁmum is taken over all testing procedures. For a binary test speciﬁed by V = {ν  ν′} 

inf
ψ

P (ψ(Z1  . . .   Zn) 6= V ) =

1
2 −

and more generally 

1
2 kM n

ν − M n

ν ′kTV  

P(ψ(Z1  . . .   Zn) 6= V ) ≥(cid:20)1 −

inf
ψ

I(Z1  . . .   Zn; V ) + log 2

log |V|

(cid:21) .

3

(5a)

(5b)

2.2

Information bounds

The main step in proving minimax lower bounds is to control the divergences involved in the lower
bounds (5a) and (5b). We review two results from our work [9] that obtain such bounds as a function
of the amount of privacy provided. The second of the results provides a variational upper bound on
the mutual information I(Z1  . . .   Zn; V )  in that we optimize jointly over subset S ⊂ X . To state
the proposition  we require a bit of notation: for each i ∈ {1  . . .   n}  let Pν i be the distribution of
Xi conditional on the random packing element V = ν  and let M n
ν be the marginal distribution (3)
induced by passing Xi through Q. Deﬁne the mixture distribution P i = 1
then state a proposition summarizing the results we require from Duchi et al. [9]:

|V|Pν∈V Pν i  We can

Proposition 1 (Information bounds). For any ν  ν′ ∈ V and α ≥ 0 

Dkl (M n

ν kM n

Additionally for V chosen uniformly at random from V  we have the variational bound
.

I(Z1  . . .   Zn; V ) ≤ eα (eα − e−α)2

sup

ν ′ ) ≤ 4(eα − 1)2
nXi=1

|V|

TV .

nXi=1
kPν i − Pν ′ ik2
S∈σ(X )Xν∈V(cid:0)Pν i(S) − P (S)(cid:1)2

(6)

(7)

By combining Proposition 1 with Lemma 1  it is possible to derive sharp lower bounds on arbitrary
estimation procedures under α-local privacy. In the remainder of the paper  we demonstrate this
combination for probability estimation problems; we provide proofs of all results in [9].

3 Multinomial Estimation under Local Privacy

In this section we return to the classical problem of avoiding answer bias in surveys  the original
motivation for studying local privacy [23].

3.1 Minimax rates of convergence for multinomial estimation

Let ∆d :=(cid:8)θ ∈ Rd | θ ≥ 0 Pd

j=1 θj = 1(cid:9) denote the probability simplex in Rd. The multinomial

estimation problem is deﬁned as follows. Given a vector θ ∈ ∆d  samples X are drawn i.i.d. from
a multinomial with parameters θ  where Pθ(X = j) = θj for j ∈ {1  . . .   d}  and the goal is to
estimate θ. In one of the earliest evaluations of privacy  Warner [23] studied the Bernoulli variant of
this problem and proposed randomized response: for a given survey question  respondents provide
a truthful answer with probability p > 1/2 and lie with probability 1 − p.
In our setting  we assume the statistician sees α-locally private (1) random variables Zi for the cor-
responding samples Xi from the multinomial. In this case  we have the following result  which char-
2]

acterizes the minimax rate of estimation of a multinomial in both mean-squared error E[kbθ − θk2
and absolute error E[kbθ − θk1]; the latter may be more relevant for probability estimation problems.

Theorem 1. There exist universal constants 0 < cℓ ≤ cu < 5 such that for all α ∈ [0  1]  the
minimax rate for multinomial estimation satisﬁes the bounds

 

1

d

nα2(cid:27) ≤ Mn(cid:16)∆d k·k2

2   α(cid:17) ≤ cu min(cid:26)1 
cℓ min(cid:26)1 
√nα2
√nα2(cid:27) ≤ Mn (∆d k·k1   α) ≤ cu min(cid:26)1 
cℓ min(cid:26)1 

nα2(cid:27)  
√nα2(cid:27) .

d

d

d

(8)

(9)

and

Theorem 1 shows that providing local privacy can sometimes be quite detrimental to the quality
of statistical estimators. Indeed  let us compare this rate to the classical rate in which there is no
privacy. Then estimating θ via proportions (i.e.  maximum likelihood)  we have

Ehkbθ − θk2
2i =

dXj=1

Eh(bθj − θj)2i =

1
n

dXj=1

θj(1 − θj) ≤

1

n(cid:18)1 −

1

d(cid:19) <

1
n

.

By inequality (8)  for suitably large sample sizes n  the effect of providing differential privacy at a
level α causes a reduction in the effective sample size of n 7→ nα2/d.

4

[Z]j =(xj

with probability exp(α/2)
1+exp(α/2)
1+exp(α/2) .

1

1 − xj with probability

(10)

3.2 Optimal mechanisms: attainability for multinomial estimation

An interesting consequence of the lower bound in (8) is the following fact that we now demonstrate:
Warner’s classical randomized response mechanism [23] (with minor modiﬁcation) achieves the
optimal convergence rate. There are also other relatively simple estimation strategies that achieve
convergence rate d/nα2; the perturbation approach Dwork et al. [11] propose  where Laplace(α)
noise is added to each coordinate of a multinomial sample  is one such strategy. Nonetheless  the
ease of use and explainability of randomized response  coupled with our optimality results  pro-
vide support for randomized response as a preferred method for private estimation of population
probabilities.

We now prove that randomized response attains the optimal rate of convergence. There is a bijection

between multinomial samples x ∈ {1  . . .   d} and the d standard basis vectors e1  . . .   ed ∈ Rd 
so we abuse notation and represent samples x as either when designing estimation strategies. In
randomized response  we construct the private vector Z ∈ {0  1}d from a multinomial observation
x ∈ {e1  . . .   ed} by sampling d coordinates independently via the procedure

We claim that this channel (10) is α-differentially private: indeed  note that for any x  x′ ∈ ∆d and
any vector z ∈ {0  1}d we have

Q(Z = z | x)
Q(Z = z | x′)

= exp(cid:16) α

2

(kz − xk1 − kz − x′k1)(cid:17) ∈ [exp(−α)  exp(α)]  

where we used the triangle inequality to assert that |kz − xk1 − kz − x′k1 | ≤ kx − x′k1 ≤ 2. We
can compute the expected value and variance of the random variables Z; indeed  by deﬁnition (10)

eα/2 − 1
eα/2 + 1

1

eα/2

1

E[Z | x] =

x +
Since the Z are Bernoulli  we obtain the variance bound E[kZ − E[Z]k2
the deﬁnition of the projection Π∆d onto the simplex  we arrive at the natural estimator

1 + eα/2 (1 − x) =

1 + eα/2 x +

1 + eα/2 1.

2] < d/4 + 1 < d. Recalling

1
n

eα/2 − 1

bθpart :=

of the problem [3]  so the estimator (11) is efﬁciently computable. Since projections only decrease

nXi=1(cid:16)Zi − 1/(1 + eα/2)(cid:17) eα/2 + 1

and bθ := Π∆d(cid:16)bθpart(cid:17) .
The projection ofbθpart onto the probability simplex can be done in time linear in the dimension d
distance  vectors in the simplex are at most distance √2 apart  and Eθ[bθpart] = θ  we ﬁnd
eα/2 − 1(cid:19)2(cid:27) . min(cid:26)1 
n(cid:18) eα/2 + 1
nα2(cid:27) .
Ehkbθ − θk2

2i ≤ minn2  Ehkbθpart − θk2

A similar argument shows that randomized response is minimax optimal for the ℓ1-loss as well.

2io ≤ min(cid:26)2 

(11)

d

d

4 Density Estimation under Local Privacy

In this section  we turn to studying a nonparametric statistical problem in which the effects of local
differential privacy turn out to be somewhat more severe. We show that for the problem of density
estimation  instead of just multiplicative loss in the effective sample size as in the previous section 
imposing local differential privacy leads to a different convergence rate.

In more detail  we consider estimation of probability densities f : R → R+ R f (x)dx = 1 and
f ≥ 0  deﬁned on the real line  focusing on a standard family of densities of varying smoothness [e.g.
22]. Throughout this section  we let β ∈ N denote a ﬁxed positive integer. Roughly  we consider
densities that have bounded βth derivative  and we study density estimation using the squared L2-
2 := R f 2(x)dx as our metric; in formal terms  we impose these constraints in terms of
norm kfk2
Sobolev classes (e.g. [22  12]). Let the countable collection of functions {ϕj}∞
basis for L2([0  1]). Then any function f ∈ L2([0  1]) can be expanded as a sumP∞
terms of the basis coefﬁcients θj :=R f (x)ϕj(x)dx  where {θj}∞

j=1 be an orthonormal
j=1 θjϕj in
j=1 ∈ ℓ2(N). The Sobolev space

Fβ[C] is obtained by enforcing a particular decay rate on the coefﬁcients θ:

5

Deﬁnition 1 (Elliptical Sobolev space). For a given orthonormal basis {ϕj} of L2([0  1])  smooth-
ness parameter β > 1/2 and radius C  the function class Fβ[C] is given by

Fβ[C] :=(cid:26)f ∈ L2([0  1]) | f =

θjϕj such that

∞Xj=1

∞Xj=1

j2βϕ2

j ≤ C 2(cid:27).

If we choose the trigonometric basis as our orthonormal basis  then membership in the class Fβ[C]
corresponds to certain smoothness constraints on the derivatives of f . More precisely  for j ∈ N 
consider the orthonormal basis for L2([0  1]) of trigonometric functions:

ϕ0(t) = 1  ϕ2j(t) = √2 cos(2πjt)  ϕ2j+1(t) = √2 sin(2πjt).

(12)

Now consider a β-times almost everywhere differentiable function f for which |f (β)(x)| ≤ C for
almost every x ∈ [0  1] satisfying f (k)(0) = f (k)(1) for k ≤ β − 1. Uniformly for such f   there is
a universal constant c such that that f ∈ Fβ[cC] [22  Lemma A.3]. Thus  Deﬁnition 1 (essentially)
captures densities that have Lipschitz-continuous (β − 1)th derivative. In the sequel  we write Fβ
when the bound C in Fβ[C] is O(1). It is well known [26  25  22] that the minimax risk for non-
private estimation of densities in the class Fβ scales as

Mn(cid:16)Fβ k·k2

2  ∞(cid:17) ≍ n− 2β

2β+1 .

Our main result is to demonstrate that the classical rate (13) is no longer attainable when we require
α-local differential privacy. In Sections 4.2 and 4.3  we show how to achieve the (new) optimal rate
using histogram and orthogonal series estimators.

4.1 Lower bounds on density estimation

We begin by giving our main lower bound on the minimax rate of estimation of densities when are
kept differentially private  providing the proof in the longer paper [9].
Theorem 2. Consider the class of densities Fβ deﬁned using the trigonometric basis (12). For some
α ∈ [0  1]  suppose Zi are α-locally private (1) for the samples Xi ∈ [0  1]. There exists a constant
cβ > 0  dependent only on β  such that

(13)

(14)

Mn(cid:16)Fβ k·k2

2   α(cid:17) ≥ cβ(cid:0)nα2(cid:1)− 2β

2β+2 .

In comparison with the classical minimax rate (13)  the lower bound (14) involves a different poly-
nomial exponent: privacy reduces the exponent from 2β/(2β + 1) to 2β/(2β + 2). For example 
for Lipschitz densities we have β = 1  and the rate degrades from n−2/3 to n−1/2.

Interestingly  no estimator based on Laplace (or exponential) perturbation of the samples Xi them-
selves can attain the rate of convergence (14). In their study of the deconvolution problem  Carroll
and Hall [4] show that if samples Xi are perturbed by additive noise W   where the characteris-
tic function φW of the additive noise has tails behaving as |φW (t)| = O(|t|−a) for some a > 0 
then no estimator can deconvolve the samples X + W and attain a rate of convergence better than
n−2β/(2β+2a+1). Since the Laplace distribution’s characteristic function has tails decaying as t−2 
no estimator based on perturbing the samples directly can attain a rate of convergence better than
n−2β/(2β+5). If the lower bound (14) is attainable  we must then study privacy mechanisms that are
not simply based on direct perturbation of the samples {Xi}n

i=1.

4.2 Achievability by histogram estimators

We now turn to the mean-squared errors achieved by speciﬁc practical schemes  beginning with the
special case of Lipschitz density functions (β = 1)  for which it sufﬁces to consider a private version
of a classical histogram estimate. For a ﬁxed positive integer k ∈ N  let {Xj}k
j=1 denote the partition
of X = [0  1] into the intervals

Xj = [(j − 1)/k  j/k)

for j = 1  2  . . .   k − 1  and Xk = [(k − 1)/k  1].

6

the sum fθ :=Pk

Any histogram estimate of the density based on these k bins can be speciﬁed by a vector θ ∈ k∆k 
where we recall ∆k ⊂ Rk
+ is the probability simplex. Any such vector deﬁnes a density estimate via
j=1 θj 1Xj   where 1E denotes the characteristic (indicator) function of the set E.

Let us now describe a mechanism that guarantees α-local differential privacy. Given a data set
{X1  . . .   Xn} of samples from the distribution f   consider the vectors
for i = 1  2  . . .   n 

(15)
where ek(Xi) ∈ ∆k is a k-vector with the jth entry equal to one if Xi ∈ Xj  and zeroes in all
other entries  and Wi is a random vector with i.i.d. Laplace(α/2) entries. The variables {Zi}n
so-deﬁned are α-locally differentially private for {Xi}n

Zi := ek(Xi) + Wi 

i=1.

i=1

Using these private variables  we then form the density estimate bf := fbθ =Pk

j=1bθj 1Xj based on

(16)

where Πk denotes the Euclidean projection operator onto the set k∆k. By construction  we have

bf ≥ 0 andR 1
Proposition 2. Consider the estimate bf based on k = (nα2)1/4 bins in the histogram. For any

1-Lipschitz density f : [0  1] → R+  we have

n

bθ := Πk(cid:18) k

Zi(cid:19) 
nXi=1
0 bf (x)dx = 1  so bf is a valid density estimate.
2i ≤ 5(α2n)− 1

Efh(cid:13)(cid:13)bf − f(cid:13)(cid:13)2

For any ﬁxed α > 0  the ﬁrst term in the bound (17) dominates  and the O((α2n)− 1
2 ) rate matches
the minimax lower bound (14) in the case β = 1: the privatized histogram estimator is minimax-
optimal for Lipschitz densities. This result provides the private analog of the classical result that
histogram estimators are minimax-optimal (in the non-private setting) for Lipschitz densities.

2 + √αn−3/4.

(17)

4.3 Achievability by orthogonal projection estimators

For higher degrees of smoothness (β > 1)  histogram estimators no longer achieve optimal rates in
the classical setting [20]. Accordingly  we turn to estimators based on orthogonal series and show
that even under local privacy  they achieve the lower bound (14) for all orders of smoothness β ≥ 1.
Recall the elliptical Sobolev space (Deﬁnition 1)  in which a function f is represented as f =

j=1 θjϕj   where θj =R f (x)ϕj(x)dx. This representation underlies the classical method of or-
P∞

thonormal series estimation: given a data set  {X1  X2  . . .   Xn}  drawn i.i.d. according to a density
f ∈ L2([0  1])  we ﬁrst compute the empirical basis coefﬁcients

1
n

nXi=1

bθj =

ϕj(Xi) and then set bf =

kXj=1bθjϕj 

where the value k ∈ N is chosen either a priori based on known properties of the estimation problem
or adaptively  for example  using cross-validation [12  22].

In the setting of local privacy  we consider a mechanism that  instead of releasing the vector of coef-

ﬁcients(cid:0)ϕ1(Xi)  . . .   ϕk(Xi)(cid:1) for each data point  employs a random vector Zi = (Zi 1  . . .   Zi k)

with the property that E[Zi j | Xi] = ϕj(Xi) for each j = 1  2  . . .   k. We assume the basis func-
tions are uniformly bounded; i.e.  there exists a constant B0 = supj supx |ϕj(x)| < ∞. For a ﬁxed
number B strictly larger than B0 (to be speciﬁed momentarily)  consider the following scheme:

(18)

(19)

Sampling strategy Given a vector τ ∈ [−B0  B0]k  constructeτ ∈ {−B0  B0}k with coordinateseτj

2 + τj
sampled independently from {−B0  B0} with probabilities 1
T from a Bernoulli(eα/(eα + 1)) distribution. Then choose Z ∈ {−B  B}k via

2 − τj

. Sample

and 1

2B0

2B0

Z ∼(cid:26)Uniform on (cid:8)z ∈ {−B  B}k : hz eτi > 0(cid:9) if T = 1
Uniform on (cid:8)z ∈ {−B  B}k : hz eτi ≤ 0(cid:9) if T = 0.

7

over  the samples (19) are efﬁciently computable (for example by rejection sampling). Starting from

By inspection  Z is α-differentially private for any initial vector in the box [−B0  B0]k  and more-
the vector τ ∈ Rk  τj = ϕj(Xi)  in the above sampling strategy we have
B
B0√k

eα + 1(cid:19) ϕj(x) = ck

E[[Z]j | X = x] = ck

eα − 1
eα + 1

B

B0√k(cid:18) eα

eα + 1 −

1

ϕj(x) 

(20)

for a constant ck that may depend on k but is O(1) and bounded away from 0. Consequently  to
attain the unbiasedness condition E[[Zi]j | Xi] = ϕj(Xi)  it sufﬁces to take B = O(B0√k/α).

The full sampling and inferential scheme are as follows: (i) given a data point Xi  construct the
vector τ = [ϕj(Xi)]k
j=1; (ii) sample Zi according to strategy (19) using τ and the bound B =

B0√k(eα + 1)/ck(eα − 1). (The constant ck is as in the expression (20).) Using the estimator

we obtain the following proposition.

1
n

nXi=1

kXj=1

bf :=

Zi jϕj 

(21)

Proposition 3. Let {ϕj} be a B0-bounded orthonormal basis for L2([0  1]). There exists a constant
c (depending only on C and B0) such that the estimator (21) with k = (nα2)1/(2β+2) satisﬁes

sup

f ∈Fβ[C]

Efhkf − bfk2

2i ≤ c(cid:0)nα2(cid:1)− 2β

2β+2 .

Propositions 2 and 3 make clear that the minimax lower bound (14) is sharp  as claimed.

Before concluding our exposition  we make a few remarks on other potential density estimators. Our
orthogonal-series estimator (21) (and sampling scheme (20))  while similar in spirit to that proposed
by Wasserman and Zhou [24  Sec. 6]  is different in that it is locally private and requires a differ-
ent noise strategy to obtain both α-local privacy and optimal convergence rate. Lei [19] considers
private M -estimators based on ﬁrst performing a histogram density estimate  then using this to con-
struct a second estimator; his estimator is not locally private  and the resulting M -estimators have
sub-optimal convergence rates. Finally  we remark that density estimators that are based on orthogo-
nal series and Laplace perturbation are sub-optimal: they can achieve (at best) rates of (nα2)− 2β
2β+3  
which is polynomially worse than the sharp result provided by Proposition 3. It appears that appro-
priately chosen noise mechanisms are crucial for obtaining optimal results.

5 Discussion

We have linked minimax analysis from statistical decision theory with differential privacy  bringing
some of their respective foundational principles into close contact. In this paper particularly  we
showed how to apply our divergence bounds to obtain sharp bounds on the convergence rate for cer-
tain nonparametric problems in addition to standard ﬁnite-dimensional settings. By providing sharp
convergence rates for many standard statistical inference procedures under local differential privacy 
we have developed and explored some tools that may be used to better understand privacy-preserving
statistical inference and estimation procedures. We have identiﬁed a fundamental continuum along
which privacy may be traded for utility in the form of accurate statistical estimates  providing a
way to adjust statistical procedures to meet the privacy or utility needs of the statistician and the
population being sampled. Formally identifying this trade-off in other statistical problems should
allow us to better understand the costs and beneﬁts of privacy; we believe we have laid some of the
groundwork to do so.

Acknowledgments

JCD was supported by a Facebook Graduate Fellowship and an NDSEG fellowship. Our work was
supported in part by the U.S. Army Research Laboratory  U.S. Army Research Ofﬁce under grant
number W911NF-11-1-0391  and Ofﬁce of Naval Research MURI grant N00014-11-1-0688.

8

References

[1] B. Barak  K. Chaudhuri  C. Dwork  S. Kale  F. McSherry  and K. Talwar. Privacy  accuracy  and consis-
tency too: A holistic solution to contingency table release. In Proceedings of the 26th ACM Symposium
on Principles of Database Systems  2007.

[2] A. Beimel  K. Nissim  and E. Omri. Distributed private data analysis: Simultaneously solving how and
what. In Advances in Cryptology  volume 5157 of Lecture Notes in Computer Science  pages 451–468.
Springer  2008.

[3] P. Brucker. An O(n) algorithm for quadratic knapsack problems. Operations Research Letters  3(3):

163–166  1984.

[4] R. Carroll and P. Hall. Optimal rates of convergence for deconvolving a density. Journal of the American

Statistical Association  83(404):1184–1186  1988.

[5] K. Chaudhuri and D. Hsu. Convergence rates for differentially private statistical estimation. In Proceed-

ings of the 29th International Conference on Machine Learning  2012.

[6] K. Chaudhuri  C. Monteleoni  and A. D. Sarwate. Differentially private empirical risk minimization.

Journal of Machine Learning Research  12:1069–1109  2011.

[7] T. M. Cover and J. A. Thomas. Elements of Information Theory  Second Edition. Wiley  2006.

[8] A. De. Lower bounds in differential privacy. In Proceedings of the Ninth Theory of Cryptography Con-

ference  2012. URL http://arxiv.org/abs/1107.2183.

[9] J. C. Duchi  M. I. Jordan  and M. J. Wainwright.

Local privacy and statistical minimax rates.

arXiv:1302.3203 [math.ST]  2013. URL http://arxiv.org/abs/1302.3203.

[10] G. T. Duncan and D. Lambert. Disclosure-limited data dissemination. Journal of the American Statistical

Association  81(393):10–18  1986.

[11] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private data analysis.

In Proceedings of the 3rd Theory of Cryptography Conference  pages 265–284  2006.

[12] S. Efromovich. Nonparametric Curve Estimation: Methods  Theory  and Applications. Springer-Verlag 

1999.

[13] A. V. Evﬁmievski  J. Gehrke  and R. Srikant. Limiting privacy breaches in privacy preserving data mining.
In Proceedings of the Twenty-Second Symposium on Principles of Database Systems  pages 211–222 
2003.

[14] I. P. Fellegi. On the question of statistical conﬁdentiality. Journal of the American Statistical Association 

67(337):7–18  1972.

[15] S. E. Fienberg  U. E. Makov  and R. J. Steele. Disclosure limitation using perturbation and related methods

for categorical data. Journal of Ofﬁcial Statistics  14(4):485–502  1998.

[16] M. Hardt and K. Talwar. On the geometry of differential privacy.

Second Annual ACM Symposium on the Theory of Computing  pages 705–714  2010.
http://arxiv.org/abs/0907.3754.

In Proceedings of the Fourty-
URL

[17] I. A. Ibragimov and R. Z. Has’minskii. Statistical Estimation: Asymptotic Theory. Springer-Verlag  1981.

[18] S. P. Kasiviswanathan  H. K. Lee  K. Nissim  S. Raskhodnikova  and A. Smith. What can we learn

privately? SIAM Journal on Computing  40(3):793–826  2011.

[19] J. Lei. Differentially private M-estimators. In Advances in Neural Information Processing Systems 25 

2011.

[20] D. Scott. On optimal and data-based histograms. Biometrika  66(3):605–610  1979.

[21] A. Smith. Privacy-preserving statistical estimation with optimal convergence rates. In Proceedings of the

Fourty-Third Annual ACM Symposium on the Theory of Computing  2011.

[22] A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer  2009.

[23] S. Warner. Randomized response: a survey technique for eliminating evasive answer bias. Journal of the

American Statistical Association  60(309):63–69  1965.

[24] L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the American

Statistical Association  105(489):375–389  2010.

[25] Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Annals of

Statistics  27(5):1564–1599  1999.

[26] B. Yu. Assouad  Fano  and Le Cam. In Festschrift for Lucien Le Cam  pages 423–435. Springer-Verlag 

1997.

9

,John Duchi
Martin Wainwright
Michael Jordan