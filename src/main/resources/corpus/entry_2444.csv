2011,Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation,We present a novel regularization-based Multitask Learning (MTL) formulation  for Structured Output (SO) prediction for the case of hierarchical task relations.  Structured output learning often results in difﬁcult inference problems and   requires large amounts of training data to obtain accurate models. We propose to  use MTL to exploit information available for related structured output learning  tasks by means of hierarchical regularization. Due to the combination of example   sets  the cost of training models for structured output prediction can easily  become infeasible for real world applications. We thus propose an efﬁcient   algorithm based on bundle methods to solve the optimization problems resulting from  MTL structured output learning. We demonstrate the performance of our approach  on gene ﬁnding problems from the application domain of computational biology.  We show that 1) our proposed solver achieves much faster convergence than previous   methods and 2) that the Hierarchical SO-MTL approach clearly outperforms  considered non-MTL methods.,Hierarchical Multitask Structured Output Learning

for Large-Scale Sequence Segmentation

Nico G¨ornitz1

Technical University Berlin 

Franklinstr. 28/29  10587 Berlin  Germany

Nico.Goernitz@tu-berlin.de

Georg Zeller

European Molecular Biology Laboratory

Meyerhofstr. 1  69117 Heidelberg  Germany

Georg.Zeller@gmail.com

S¨oren Sonnenburg2

TomTom

An den Treptowers 1  12435 Berlin  Germany

Soeren.Sonnenburg@tomtom.com

Christian Widmer1

FML of the Max Planck Society

Spemannstr. 39  72070 T¨ubingen  Germany

Christian.Widmer@tue.mpg.de

Andr´e Kahles

FML of the Max Planck Society

Spemannstr. 39  72070 T¨ubingen  Germany

Andre.Kahles@tue.mpg.de

Gunnar R¨atsch

FML of the Max Planck Society

Spemannstr. 39  72070 T¨ubingen  Germany

Gunnar.Raetsch@tue.mpg.de

Abstract

We present a novel regularization-based Multitask Learning (MTL) formulation
for Structured Output (SO) prediction for the case of hierarchical task relations.
Structured output prediction often leads to difﬁcult inference problems and hence
requires large amounts of training data to obtain accurate models. We propose to
use MTL to exploit additional information from related learning tasks by means of
hierarchical regularization. Training SO models on the combined set of examples
from multiple tasks can easily become infeasible for real world applications. To
be able to solve the optimization problems underlying multitask structured out-
put learning  we propose an efﬁcient algorithm based on bundle-methods. We
demonstrate the performance of our approach in applications from the domain of
computational biology addressing the key problem of gene ﬁnding. We show that
1) our proposed solver achieves much faster convergence than previous methods
and 2) that the Hierarchical SO-MTL approach outperforms considered non-MTL
methods.

Introduction

1
In Machine Learning  model quality is most often limited by the lack of sufﬁcient training data.
When data from different  but related tasks  is available  it is possible to exploit it to boost the per-
formance of each task by transferring relevant information. Multitask learning (MTL) considers
the problem of inferring models for several tasks simultaneously  while imposing regularity criteria
or shared representations in order to allow learning across tasks. This has been an active research
focus and various methods (e.g.  [5  8]) have been explored  providing empirical ﬁndings [16] and
theoretical foundations [3  4]. Recently  also the relationships between tasks have been studied (e.g. 
[1]) assuming a cluster relationship [11] or a hierarchy [6  23  13] between tasks. Our proposed
method follows this line of research in that it exploits externally provided hierarchical task rela-
tions. The generality of regularization-based MTL approaches makes it possible to extend them
beyond the simple cases of classiﬁcation or regression to Structured Output (SO) learning problems

1These authors contributed equally.
2This work was done while SS was at Technical University Berlin

1

[14  2  21  10]. Here  the output is not in the form of a discrete class label or a real valued number 
but a structured entity such as a label sequence  a tree  or a graph. One of the main contributions
of this paper is to explicitly extend a regularization-based MTL formulation to the SVM-struct for-
mulation for SO prediction [2  21]. SO learning methods can be computationally demanding  and
combining information from several tasks leads to even larger problems  which renders many inter-
esting applications infeasible. Hence  our second main contribution is to provide an efﬁcient solver
for SO problems which is based on bundle methods [18  19  7]. It achieves much faster convergence
and is therefore an essential tool to cope with the demands of the MTL setting.
SO learning has been successfully applied in the analysis of images  natural language  and se-
quences. The latter is of particular interest in computational biology for the analysis of DNA  RNA
or protein sequences. This ﬁeld moreover constitutes an excellent application area for MTL [12  22].
In computational biology  one often uses supervised learning methods to model biological processes
in order to predict their outcomes and ultimately understand them better. Due to the complexity
of many biological mechanisms  rich computational models have to be developed  which in turn
require a reasonable amount of training data. However  especially in the biomedical domain  ob-
taining labeled training examples through experiments can be costly. Thus  combining information
from several related tasks can be a cost-effective approach to best exploit the available label data.
When transferring label information across tasks  it often makes sense to assume hierarchical task
relations. In particular  in computational biology  where evolutionary processes often impose a task
hierarchy [22]. For instance  we might be interested in modeling a common biological mechanism
in several organisms such that each task corresponds to one organism. In this setting  we expect
that the longer the common evolutionary history between two organisms  the more beneﬁcial it is
to share information between the corresponding tasks. In this work  we chose a challenging prob-
lem from genome biology to demonstrate that our approach is practically feasible in terms of speed
and accuracy. In ab initio gene ﬁnding [17]  the task is to build an accurate model of a gene and
subsequently use it to predict the gene content of newly sequenced genomes or to reﬁne existing
annotations. Despite many commonalities between sequence features of genes across organisms 
sequence differences have made it very difﬁcult to build universal gene ﬁnders that achieve high
accuracy in cross-organism prediction. This problem is hence ideally suited for the application of
the proposed SO-MTL approach.

2 Methods
Regularization based supervised learning methods  such as the SVM or Logistic Regression play
a central role in many applications.
In its most general form  such a method consists of a loss
function L that captures the error with respect to the training data S = {(x1  y1)  . . .   (xn  yn)} and
a regularizer R that penalizes model complexity

n(cid:88)

i=1

J(w) =

L(w  xi  yi) + R(w).

In the case of Multitask Learning (MTL)  one is interested in obtaining several models w1  ...  wT
based on T associated sets of examples St = {(x1  y1)  . . .   (xnt  ynt)}  t = 1  . . .   T . To couple
individual tasks  an additional regularization term RM T L is introduced that penalizes the disagree-
ment between the individual models (e.g.  [1  8]):

(cid:32) nt(cid:88)

T(cid:88)

(cid:33)

J(w1  ...  wT ) =

L(w  xi  yi) + R(wt)

+ RM T L(w1  ...  wT ).

t=1

i=1

Special cases include T = 2 and RM T L(w1  w2) = γ ||w1 − w2|| (e.g.  [8  16])  where γ is a
hyper-parameter controlling the strength of coupling of the solutions for both tasks. For more than
two tasks  the number of coupling terms and hyper-parameters can rise quadratically leading to a
difﬁcult model-selection problem.

2.1 Hierarchical Multitask Learning (HMTL)
We consider the case where tasks correspond to leaves of a tree and are related by its inner nodes. In
[22]  the case of taxonomically organized two-class classiﬁcation tasks was investigated  where each
task corresponds to a species (taxon). The idea was to mimic biological evolution that is assumed to

2

generate more specialized molecular processes with each speciation event from root to leaf. This is
implemented by training on examples available for nodes in the current subtree (i.e.  the tasks below
the current node)  while similarity to the parent classiﬁer is induced through regularization. Thus 
for each node n  one solves the following optimization problem 

(cid:16)

 1

2

(1 − γ)||w||2 + γ(cid:12)(cid:12)(cid:12)(cid:12)w − w∗

p

(cid:12)(cid:12)(cid:12)(cid:12)2(cid:17)

(cid:88)

+ C

(x y)∈S

   (1)

(cid:96) ((cid:104)x  w(cid:105) + b  y)

(w∗

n  b∗

n) = argmin

w b

where p is the parent node of n (with the special case of w∗
p = 0 for the root node)  (cid:96) is an appropriate
loss function (e.g.  the hinge-loss). The hyper-parameter γ ∈ [0  1] determines the contribution of
regularization from the origin vs. the parent node’s parameters (i.e.  the strength of coupling between
the node and its parent). The above problem can be equivalently rewritten as:

 1

2

||w||2 − γ(cid:10)w  w∗

(cid:11) + C

p

(cid:88)

(x y)∈S

(w∗

n  b∗

n) = argmin

w b

(cid:96) ((cid:104)x  w(cid:105) + b  y)

(2)

 .

For γ = 0  the tasks completely decouple and can be learnt independently. The parameters for
the root node correspond to the globally best model. We will refer to these two cases as base-line
methods for comparisons in the experimental section.

2.2 Structured Output Learning and Extensions for HMTL
In contrast to binary classiﬁcation  elements from the output space Υ (e.g.  sequences  trees  or
graphs) of structured output problems have an inherent structure which makes more sophisticated 
problem-speciﬁc loss functions desirable. The loss between the true label y ∈ Υ and the predicted
label ˆy ∈ Υ is measured by a loss function ∆ : Υ × Υ → (cid:60)+. A widely used approach to predict
ˆy ∈ Υ is the use of a linearly parametrized model given an input vector x ∈ X and a joint feature
map Ψ : X × Υ → H that captures the dependencies between input and output (e.g.  [21]):

ˆyw(x) = argmax

¯y∈Υ

(cid:104)w  Ψ(x  ¯y)(cid:105).

n(cid:88)

i=1

The most common approaches to estimate the model parameters w are based on structured output
SVMs (e.g.  [2  21]) and conditional random ﬁelds (e.g.  [14]; see also [10]). Here we follow
the approach taken in [21  15]  where estimating the parameter vector w amounts to solving the
following optimization problem

(cid:41)
(cid:104)w  Ψ(xi  ¯y)(cid:105) + ∆(yi  ¯y) − (cid:104)w  Ψ(xi  yi)(cid:105))

 

(3)

min
w∈H

R(w) + C

(cid:96)(max
¯y∈Υ

(cid:40)

where R(w) is a regularizer and (cid:96) is a loss function. For (cid:96)(a) = max(0  a) and R(w) = (cid:107) w (cid:107)2
obtain the structured output support vector machine [21  2] with margin rescaling and hinge-loss.
It turns out that we can combine the structured output formulation with hierarchical multitask learn-
ing in a straight-forward way. We extend the regularizer R(w) in (3) with a γ-parametrized convex
2(cid:107) w (cid:107)2
combination of a multitask regularizer 1
2 − γ(cid:104)w  wp(cid:105). Thus we can apply the
and omitting constant terms  we arrive at Rp γ(w) = 1
described hierarchical multitask learning approach and solve for every node the following optimiza-
tion problem:

2 with the original term. When R(w) = 1

2 ||w − wp||2

(cid:41)
(cid:104)w  Ψ(xi  ¯y)(cid:105) + ∆(yi  ¯y) − (cid:104)w  Ψ(xi  yi)(cid:105))

2(cid:107) w (cid:107)2

n(cid:88)

(cid:40)

2 we

(4)

2

min
w∈H

Rp γ(w) + C

(cid:96)(max
¯y∈Υ

i=1

A major difﬁculty remains: solving the resulting optimization problems which now can become
considerably larger than for the single-task case.

2.3 A Bundle Method for Efﬁcient Optimization
A common approach to obtain a solution to (3) is to use so-called cutting-plane or column-generation
methods. Here one considers growing subsets of all possible structures and solves restricted opti-
mization problems. An algorithm implementing a variant of this strategy based on primal optimiza-
tion is given in the appendix (similar in [21]). Cutting-plane and column generation techniques

3

often converge slowly. Moreover  the size of the restricted optimization problems grows steadily
and solving them becomes more expensive in each iteration. Simple gradient descent or second
order methods can not be directly applied as alternatives  because (4) is continuous but non-smooth.
Our approach is instead based on bundle methods for regularized risk minimization as proposed in
[18  19] and [7]. In case of SVMs  this further relates to the OCAS method introduced in [9]. In
order to achieve fast convergence  we use a variant of these methods adapted to structured output
learning that is suitable for hierarchical multitask learning.
We consider the objective function J(w) = Rp γ(w) + L(w)  where

L(w) := C

{(cid:104)w  Ψ(xi  ¯y)(cid:105) + ∆(yi  ¯y)} − (cid:104)w  Ψ(xi  yi)(cid:105))

(cid:96)(max
¯y∈Υ

n(cid:88)

i=1

and Rp γ(w) is as deﬁned in Section 2.2. Direct optimization of J is very expensive as computing L
involves computing the maximum over the output space. Hence  we propose to optimize an estimate
of the empirical loss ˆL (w)  which can be computed efﬁciently. We deﬁne the estimated empirical
loss ˆL (w) as

(cid:19)
{(cid:104)w  Ψ(cid:105) + ∆} − (cid:104)w  Ψ(xi  yi)(cid:105)

.

ˆL(w) := C

(cid:96)

max

(Ψ ∆)∈Γi

(cid:18)

N(cid:88)

i=1

Accordingly  we deﬁne the estimated objective function as ˆJ(w) = Rp γ(w) + ˆL(w). It is easy to
verify that J(w) ≥ ˆJ(w). Γi is a set of pairs (Ψ(xi  y)  ∆(yi  y)) deﬁned by a suitably chosen 
growing subset of Υ  such that ˆL(w) → L(w) (cf. Algorithm 1).
In general  bundle methods are extensions of cutting plane methods that use a prox-function to sta-
bilize the solution of the approximated function. In the framework of regularized risk minimization 
a natural prox-function is given by the regularizer. We apply this approach to the objective ˆJ(w)
and solve

min

w

Rp γ(w) + max
i∈I

{(cid:104)ai  w(cid:105) + bi}

(5)

where the set of cutting planes ai  bi lower bound ˆL. As proposed in [7  19]  we use a set I
of limited size. Moreover  we calculate an aggregation cutting plane ¯a  ¯b that lower bounds the
estimated empirical loss ˆL. To be able to solve the primal optimization problem in (5) in the dual
space as proposed by [7  19]  we adopt an elegant strategy described in [7] to obtain the aggregated
cutting plane (¯a(cid:48)  ¯b(cid:48)) using the dual solution α of (5):

¯a(cid:48) =

αjai

and

¯b(cid:48) =

αibi.

(6)

(cid:88)

i∈I

(cid:88)

i∈I

(cid:26)

(cid:27)

The following two formulations reach the same minimum when optimized with respect to w:

min
w∈H

Rp(w) + max
i∈I

(cid:104)ai  w(cid:105) + bi

= min

w∈H {Rp(w) + (cid:104)¯a(cid:48)  w(cid:105) + ¯b(cid:48)}.

This new aggregated plane can be used as an additional cutting plane in the next iteration step.
We therefore have a monotonically increasing lower bound on the estimated empirical loss and can
remove previously generated cutting planes without compromising convergence (see [7] for details).
The algorithm is able to handle any (non-)smooth convex loss function (cid:96)  since only the subgradient
needs to be computed. This can be done efﬁciently for the hinge-loss  squared hinge-loss  Huber-
loss  and logistic-loss.
The resulting optimization algorithm is outlined in Algorithm 1. There are several improvements
possible: For instance  one can bypass updating the empirical risk estimates in line 6  when
L(w(k)) − ˆL(w(k)) ≤ . Finally  while Algorithm 1 was formulated in primal space  it is easy
to reformulate in dual variables making it independent of the dimensionality of w ∈ H.

2.4 Taxonomically Constrained Model Selection
Model selection for multitask learning is particularly difﬁcult  as it requires hyper-parameter selec-
tion for several different  but related tasks in a dependent manner. For the described approach  each

4

(cid:19)

> (cid:96)

max

(Ψ ∆)∈Γi

(cid:19)

then

(cid:104)w  Ψ(cid:105) + ∆

max
y∈Υ

{(cid:104)w  Ψ(xi  y)(cid:105) + ∆(yi  y)}

Algorithm 1 Bundle Methods for Structured Output Algorithm
1: S ≥ 1: maximal size of the bundle set
2: θ > 0: linesearch trade-off (cf. [9] for details)
3: w(1) = wp
4: k = 1 and ¯a = 0  ¯b = 0  Γi = ∅ ∀i
5: repeat
6:
7:

for i = 1  ..  n do

(cid:18)
(cid:18)
y∗ = argmaxy∈Υ{(cid:104)w(k)  Ψ(xi  y)(cid:105) + ∆(yi  y)}
if (cid:96)
Γi = Γi ∪ (Ψ(xi  y∗)  ∆(yi  y∗))
(cid:18)

end if
Compute ak ∈ ∂w ˆL(w(k))
Compute bk = ˆL(w(k)) − (cid:104)w(k)  ak(cid:105)
w∗ = argmin
w∈H
Update ¯a  ¯b according to (6)
η∗ = argminη∈(cid:60) ˆJ(w∗ +η(w∗ − w(k)))
w(k+1) = (1 − θ) w∗ +θη∗(w∗ − w(k))
k = k + 1

14:
15:
16:
17:
18:
19: until L(w(k)) − ˆL(w(k)) ≤  and J(w(k)) − Jk(w(k)) ≤ 

8:

9:
10:
11:
12:

13:

end for

(cid:26)

Rp γ(w) + max

max

(k−S)+<i≤k

{(cid:104)ai  w(cid:105) + bi} (cid:104)¯a  w(cid:105) + ¯b

(cid:19)(cid:27)

node n in the given taxonomy corresponds to solving an optimization problem that is subject to
hyper-parameters γn and Cn (except for the root node  where only Cn is relevant). Hence  the direct
optimization of all combinations of dependent hyper-parameters in model selection is not feasible
in many cases. Therefore  we propose to perform a local model selection and optimize the current
Cn and γn at each node n from top to bottom independently. This corresponds to using the tax-
onomy for reducing the parameter search space. To clarify this point  assume a perfect binary tree
for n tasks. The length of the path from root to leaf is log2(n). The parameters along one path are
dependent  e.g. the values chosen at the root will inﬂuence the optimal choice further down the tree.
Given k candidate values for parameter γn  jointly optimizing all interdependent parameters along
one path corresponds to optimizing over a grid of klog2(n) in contrast to k · log2(n) when using our
proposed local strategy.

3 Results
3.1 Background
To demonstrate the validity of our approach  we applied it to the computational biology problem of
gene ﬁnding. Here  the task is to identify genomic regions encoding genes (from which RNAs and/or
proteins are produced). Genomic sequence can be represented by long strings of the four letters A  C 
G  and T (genome sizes range from a few megabases to several gigabases). In prokaryotes (mostly
bacteria and archaea) gene structures are comparably simple (cf. Figure 1A): the protein coding
region starts by a start codon (one out of three speciﬁc 3-mers in many prokaryotes) followed by a
number of codon triplets (of three nucleotides each) and is terminated by a stop codon (one out of
ﬁve speciﬁc 3-mers in many prokaryotes). Genic regions are ﬁrst transcribed to RNA  subsequently
the contained coding region is translated into a protein. Parts of the RNA that are not translated are
called untranslated region (UTR). Genes are separated from one another by intergenic regions. The
protein coding segment is depleted of stop codons making the computational problem of identifying
coding regions relatively straight forward.
In higher eukaryotes (animals  plants  etc.) however  the coding region can be interrupted by in-
trons  which are removed from the RNA before it is translated into protein. Introns are ﬂanked by
speciﬁc sequence signals  so-called splice sites (cf. Figure 1B). The presence of introns substantially
complicates the identiﬁcation of the transcribed and coding regions. In particular  it is usually in-
sufﬁcient to identify regions depleted of stop codons to determine the encoded protein sequence. To

5

accurately detect the transcribed regions in eukaryotic genomes  it is therefore often necessary to
use additional experimental data (e.g.  sequencing of RNA fragments). Here  we consider two key
problems in computational gene ﬁnding of (i) predicting (only) the coding regions for prokaryotes
and (ii) predicting the exon-intron structure (but not the coding region) for eukaryotes.

Figure 1: Panel A shows the structure of
a prokaryotic gene. The protein coding re-
gion is ﬂanked by a start and a stop codon
and contains a multiple of three nucleotides.
UTR denotes the untranslated region. Panel
B shows the structure of an eukaryotic gene.
The transcribed region contains introns and
exons. Introns are ﬂanked by splice sites and
are removed from the RNA. The remaining
sequence contains the UTRs and coding re-
gion.

The problem of identifying genes can be posed as a label sequence learning task  were one assigns
a label (out of intergenic  transcript start  untranslated region  coding start  coding exon  intron 
coding stop  transcript stop) to each position in the genome. The labels have to follow a grammar
dictated by the biological processes of transcription and translation (see Figure 1) making it suitable
to apply structured output learning techniques to identify genes. Because the biological processes
and cellular machineries which recognize genes have slowly evolved over time  genes of closely
related species tend to exhibit similar sequence characteristics. Therefore these problems are very
well suited for the application of multitask learning: sharing information among species is expected
to lead to more accurate gene predictions compared to approaching the problem for each species in
isolation. Currently  the genomes of many prokaryotic and eukaryotic species are being sequenced 
but often very little is known about the genes encoded  and standard methods are typically used to
infer them without systematically exploiting reliable information on related species.
In the following we will consider two different aspects of the described problem. First  focusing on
eukaryotic gene ﬁnding for a single species  we show that the proposed optimization algorithm very
quickly converges to the optimal solution. Second  for the problem of prokaryotic gene ﬁnding in
several species  we demonstrate that hierarchical multitask structured output learning signiﬁcantly
improves gene prediction accuracy. The supplement  data and code can be found on the project
website3.
3.2 Eukaryotic Gene Finding Based on RNA-Seq
We ﬁrst consider the problem of detecting exonic  intronic and intergenic regions in a single eukary-
otic genome. We use experimental data from RNA sequencing (RNA-seq) which provides evidence
for exonic and intronic regions . For simplicity  we assume that for each position in the genome
we are given numbers on how often this position was experimentally determined to be exonic and
intronic  respectively. Ideally  exons and introns belonging to the same gene should have a constant
number of conﬁrmations  whereas these values may vary greatly between different genes. But in
reality  these measurements are typically incomplete and noisy  so that inference techniques greatly
help to reconstruct complete gene structures.
As any HMM or HMSVM  our method employs a state model deﬁning allowed transitions between
states. It consists of ﬁve basic states: intergenic  exonic  intron start (donor splice site)  intronic 
and intron end (acceptor splice site). These states are duplicated Q = 5 times to model different
levels of conﬁrmation and the whole model is mirrored for simultaneous predictions of genes from
both strands of the genome (see supplement for details). In total  we have 41 states  each of which is
associated with several parameters scoring features derived from the exon and intron conﬁrmation
and computational splice site predictions (see supplement for details). Overall the model has almost
1000 parameters.
We trained the model using 700 training regions with known exon/intron structures and a total length
of ca. 5.2 million nucleotides (data from the nematode C. elegans). We used the column generation-
based algorithm (see Appendix) and the Bundle method-based algorithm (Algorithm 1) and recorded
upper and lower bounds of the objective during run time (cf. Figure 2). Whereas both algorithms

3http://bioweb.me/so-mtl

6

ATGTAAIntergenicStartCodonCoding regionStopCodonIntergenicUTRUTRN x 3 x {A C G T}IntergenicIntronIntronIntergenicExonExonExonA) Prokaryotic GeneB) Eukaryotic GeneCoding regionN x 3 x {A C G T}UTRUTRneed a similar amount of computation per iteration (mostly decoding steps)  the Bundle-method
showed much faster convergence.
We assessed prediction accuracy in a three-fold cross-validation procedure where individual test
sequences consisted of large genomic regions (of several Mbp) each containing many genes. This
evaluation procedure is expected to yield unbiased estimates that are very similar to whole-genome
predictions. Prediction accuracy was compared to another recently proposed  widely used method
called Cufﬂinks [20]. We observed that our method detects introns and transcripts more accurately
than Cufﬂinks in the data set analyzed here (cf. Figure 2).

Figure 2: Left panel: Convergence for bundle method-based solver versus column generation (log-scale).
Right panel: Prediction accuracy of our eukaryotic gene ﬁnding method in comparison to a state-of-the-art
method  Cufﬂinks [20]. The F-score (harmonic mean of precision and recall) was assessed based on two
metrics: correctly predicted introns as well as transcripts for which all introns were correct (see label).

3.3 Gene Finding in Multiple Prokaryotic Genomes
In a second series of experiments we evaluated the beneﬁt of applying SO-MTL to prokaryotic gene
prediction.

SO prediction method We modeled prokaryotic genes as a Markov chain on the nucleotide level.
To nonetheless account for the biological fact that genetic information is encoded in triplets  the
model contains a 3-cycle of exon states; details are given in Figure 3.

Figure 3: Simple state model for prokaryotic gene ﬁnding.
A suitable model for prokaryotic gene prediction needs to
consider 1) that a gene starts with a start codon (i.e. a certain
triplet of nucleotides) 2) ends with a stop codon and 3) has
a length divisible by 3. Properties 1) and 2) are enforced by
allowing only transitions into and out of the exonic states on
start and stop codons  respectively. Property 3) is enforced
by only allowing transitions from exon state Exonic3 to the
stop codon state.

Data generation We selected a subset of organisms with publicly available genomes to broadly
cover the spectrum of prokaryotic organisms.
In order to show that MTL is beneﬁcial even for
relatively distant species  we selected representatives from two different domains: bacteria and ar-
chaea. The relationship between these organisms is captured by the taxonomy shown in Figure 4 
which was created based on the information available on the NCBI website4. For each organism 
we generated one training example per annotated gene. The genomic sequences were cut between
neighboring genes (splitting intergenic regions equally)  such that a minimum distance of 6 nu-
cleotides between genes was maintained. Features for SO learning were derived from the nucleotide
sequence by transcoding it to a numerical representation of triplets. This resulted in binary vectors
of size 43 = 64 with exactly one non-zero entry. We sub-sampled from the complete dataset of Ni
examples for each organism i and created new datasets with 20 training examples  40 evaluation
examples and 200 test examples.

4ftp://ftp.ncbi.nlm.nih.gov/genomes/Bacteria/

7

51015202530354045100101102103104105106107108iterationobjective valueBundle Method Upper BoundBundle Method Lower BoundOriginal OP Upper BoundOriginal OP Lower BoundTarget0.00.10.20.30.40.50.60.70.80.91.0F−ScoreIntronTranscriptOur methodCufflinksStart CodonStop CodonExonic2Exonic1Exonic3IntergenicStartStopFigure 4: Species and their taxonomic hierarchy used for prokaryotic gene ﬁnding.

Experimental setup For model selection we used a grid over the following two parameter ranges
C = [100  250]  γ = [0  0.025  0.1  0.25  0.5  0.75  0.9  1.0] for each node in the taxonomy (cf. Fig-
ure 4). Sub-sampling of the dataset was performed 3 times and results were subsequently averaged.
We compared our MTL algorithm to two baseline methods  one where predictors for all tasks where
trained without information transfer (independent) and the other extreme case  where one global
model was ﬁtted for all tasks based on the union of all data sets (union). Performance was mea-
sured by the F-score  the harmonic mean of precision and recall  where precision and recall were
determined on nucleotide level (e.g. whether or not an exonic nucleotide was correctly predicted) in
single-gene regions. (Note that due to its per-nucleotide Markov restriction  however  our method is
not able to exploit that there is only one gene per examples sequence.)
Results Figure 5 shows the results for our proposed MTL method and the two baseline methods
described above (see Appendix for table). We observe that it generally pays off to combine infor-
mation from different organisms  as union always performs better than independent. Indeed MTL
improves over the naive combination method union with F-score increases of up to 4.05 percentage
points in A. tumefaciens. On average  we observe an improvement of 13.99 percentage points for
MTL over independent and 1.13 percentage points for MTL over union  conﬁrming the value of
MTL in transferring information across tasks. In addition  the new bundle method converges at least
twice as fast as the originally proposed cutting plane method.

Figure 5: Evaluation of MTL and baseline methods independent and union.

4 Discussion
We have introduced a regularization-based approach to SO learning in the setting of hierarchical
task relations and have empirically shown its validity on an application from computational biol-
ogy. To cope with the increased problem size usually encountered in the MTL setting  we have
developed an efﬁcient solver based on bundle-methods and demonstrated its improved convergence
behavior compared to column generation techniques. Applying our SO-MTL algorithm to the prob-
lem of prokaryotic gene ﬁnding  we could show that sharing information across tasks indeed results
in improved accuracy over learning tasks in isolation. Additionally  the taxonomy  which relates
individual tasks to each other  proved useful in that it led to more accurate predictions than were
obtained when simply training on all examples together. We have previously shown that MTL al-
gorithms excel in a scenarios where there is limited training data relative to the complexity of the
problem and model [23]. As this experiment was carried out on a relatively small data set  more
work is required to turn our approach into a state-of-the-art prokaryotic gene ﬁnder.

8

F−Score0.60.70.80.91.0meanE. coliE. fergusoniiA. tumefaciensH. pyloriB. anthracisB. subtilisM. smithiiS. islandicusIndependentUnionMTLAcknowledgments
We would like to thank the anonymous reviewers for insightful comments. Moreover  we are grateful
to Jonas Behr  Jose Leiva  Yasemin Altun and Klaus-Robert M¨uller. This work was supported by
the German Research Foundation (DFG) under the grant RA 1894/1-1.
References
[1] A. Agarwal  S. Gerber  and H. Daum´e III. Learning multiple tasks using manifold regularization.

In

Advances in Neural Information Processing Systems 23  2010.

[2] Y. Altun  I. Tsochantaridis  and T. Hofmann. Hidden markov support vector machines. In Proc. ICML 

2003.

[3] S. Ben-David and R. Schuller. Exploiting task relatedness for multiple task learning. Lecture notes in

computer science  pages 567–580  2003.

[4] J. Blitzer  K. Crammer  A. Kulesza  F. Pereira  and J. Wortman. Learning bounds for domain adaptation.

Advances in Neural Information Processing Systems  20  2007.

[5] R. Caruana. Multitask learning. Machine Learning  28(1):41–75  1997.
[6] H. Daum´e III. Bayesian multitask learning with latent hierarchies. In Proceedings of the Twenty-Fifth

Conference on Uncertainty in Artiﬁcial Intelligence  2009.

[7] T.-M.-T. Do. Regularized Bundle Methods for Large-scale Learning Problems with an Application to
Large Margin Training of Hidden Markov Models. PhD thesis  l’Universit´e Pierre & Marie Curie  2010.
[8] T. Evgeniou  C. Micchelli  and M. Pontil. Learning multiple tasks with kernel methods. Journal of

Machine Learning Research  6:615–637  2005.

[9] V. Franc and S. Sonnenburg. OCAS optimized cutting plane algorithm for support vector machines. In

Proc. ICML  2008.

[10] T. Hazan and R. Urtasun. A primal-dual message-passing algorithm for approximated large scale struc-

tured prediction. In Advances in Neural Information Processing Systems 23  2010.

[11] L. Jacob  F. Bach  and J. Vert. Clustered multi-task learning: A convex formulation. Arxiv preprint

arXiv:0809.2085  2008.

[12] L. Jacob and J. Vert. Efﬁcient peptide-MHC-I binding prediction for alleles with few known binders.

Bioinformatics  24(3):358–66  2008.

[13] S. Kim and E. P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. Proc.

ICML  2010.

[14] J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting

and labeling sequence data. In Proc. ICML  2001.

[15] G. R¨atsch and S. Sonnenburg. Large scale hidden semi-markov SVMs. In Advances in Neural Information

Processing Systems 18  2006.

[16] G. Schweikert  C. Widmer  B. Sch¨olkopf  and G. R¨atsch. An Empirical Analysis of Domain Adaptation
Algorithms for Genomic Sequence Analysis. In Advances in Neural Information Processing Systems 21 
2009.

[17] G. Schweikert  A. Zien  G. Zeller  J. Behr  C. Dieterich  C. Ong  P. Philips  F. De Bona  L. Hartmann 
A. Bohlen  N. Kr¨uger  S. Sonnenburg  and G. R¨atsch. mGene: accurate SVM-based gene ﬁnding with an
application to nematode genomes. Genome Research  19(11):2133–43  2009.

[18] A. Smola  S. Vishwanathan  and Q. Le. Bundle methods for machine learning. In Advances in Neural

Information Processing Systems 20  2008.

[19] C. Teo  S. Vishwanathan  A.Smola  and Q. Le. Bundle methods for regularized risk minimization. Journal

of Machine Learning Research  11:311–365  2010.

[20] C. Trapnell  B. A. Williams  G. Pertea  A. Mortazavi  G. Kwan  M. J. van Baren  S. L. Salzberg  B. J.
Wold  and L. Pachter. Transcript assembly and quantiﬁcation by RNA-seq reveals unannotated transcripts
and isoform switching during cell differentiation. Nature Biotechnology  28:511–515  2010.

[21] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured and

interdependent output variables. Journal of Machine Learning Research  6:1453–1484  2005.

[22] C. Widmer  J. Leiva  Y. Altun  and G. R¨atsch. Leveraging Sequence Classiﬁcation by Taxonomy-based

Multitask Learning. In Research in Computational Molecular Biology  2010.

[23] C. Widmer  N. Toussaint  Y. Altun  and G. R¨atsch. Inferring latent task structure for Multitask Learning

by Multiple Kernel Learning. BMC Bioinformatics  11(Suppl 8):S5  2010.

9

,Aurel Lazar
Yevgeniy Slutskiy
Changyou Chen
Nan Ding
Chunyuan Li
Yizhe Zhang
Lawrence Carin
Yixing Xu
Yunhe Wang
Hanting Chen
Kai Han
Chunjing XU
Dacheng Tao
Chang Xu