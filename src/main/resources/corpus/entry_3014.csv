2018,Learning Task Specifications from Demonstrations,Real-world applications often naturally decompose into several
  sub-tasks. In many settings (e.g.  robotics) demonstrations provide
  a natural way to specify the sub-tasks. However  most methods for
  learning from demonstrations either do not provide guarantees that
  the artifacts learned for the sub-tasks can be safely recombined or
  limit the types of composition available.  Motivated by this
  deficit  we consider the problem of inferring Boolean non-Markovian
  rewards (also known as logical trace properties or
  specifications) from demonstrations provided by an agent
  operating in an uncertain  stochastic environment. Crucially 
  specifications admit well-defined composition rules that are
  typically easy to interpret.  In this paper  we formulate the
  specification inference task as a maximum a posteriori (MAP)
  probability inference problem  apply the principle of maximum
  entropy to derive an analytic demonstration likelihood model and
  give an efficient approach to search for the most likely
  specification in a large candidate pool of specifications. In our
  experiments  we demonstrate how learning specifications can help
  avoid common problems that often arise due to ad-hoc reward composition.,Learning Task Speciﬁcations from Demonstrations

Marcell Vazquez-Chanlatte1  Susmit Jha2  Ashish Tiwari2  Mark K. Ho1  Sanjit A. Seshia1

1 University of California  Berkeley 2 SRI International  Menlo Park

{marcell.vc  sseshia  mark_ho}@eecs.berkeley.edu

{susmit.jha  tiwari}@sri.com

Abstract

Real-world applications often naturally decompose into several sub-tasks.
In
many settings (e.g.  robotics) demonstrations provide a natural way to specify the
sub-tasks. However  most methods for learning from demonstrations either do
not provide guarantees that the artifacts learned for the sub-tasks can be safely
recombined or limit the types of composition available. Motivated by this deﬁcit 
we consider the problem of inferring Boolean non-Markovian rewards (also known
as logical trace properties or speciﬁcations) from demonstrations provided by an
agent operating in an uncertain  stochastic environment. Crucially  speciﬁcations
admit well-deﬁned composition rules that are typically easy to interpret. In this
paper  we formulate the speciﬁcation inference task as a maximum a posteriori
(MAP) probability inference problem  apply the principle of maximum entropy to
derive an analytic demonstration likelihood model and give an efﬁcient approach to
search for the most likely speciﬁcation in a large candidate pool of speciﬁcations.
In our experiments  we demonstrate how learning speciﬁcations can help avoid
common problems that often arise due to ad-hoc reward composition.

1

Introduction

In many settings (e.g.  robotics) demonstrations provide a natural way to specify a task. For ex-
ample  an agent (e.g.  human expert) gives one or more demonstrations of the task from which
we seek to automatically synthesize a policy for the robot to execute. Typically  one models the
demonstrator as episodically operating within a dynamical system whose transition relation only
depends on the current state and action (called the Markov condition). However  even if the dy-
namics are Markovian  many problems are naturally modeled in non-Markovian terms (see Ex 1).

Example 1. Consider the task illustrated in Figure 1. In this task 
the agent moves in a discrete gridworld and can take actions to move
in the cardinal directions (north  south  east  west). Further  the agent
can sense abstract features of the domain represented as colors. The
task is to reach any of the yellow (recharge) tiles without touching
a red tile (lava) – we refer to this sub-task as YR. Additionally  if a
blue tile (water) is stepped on  the agent must step on a brown tile
(drying tile) before going to a yellow tile – we refer to this sub-task
as BBY. The last constraint requires recall of two state bits of history
(and is thus not Markovian): one bit for whether the robot is wet and
another bit encoding if the robot recharged while wet.

Figure 1

Further  like Ex 1  many tasks are naturally decomposed into several sub-tasks. This work aims to
address the question of how to systematically and separately learn non-Markovian sub-tasks such
that they can be readily and safely recomposed into the larger meta-task.
Here  we argue that non-Markovian Boolean speciﬁcations provide a powerful  ﬂexible  and easily
transferable formalism for task representations when learning from demonstrations. This stands
in contrast to the quantitative scalar reward functions commonly associated with Markov Decision

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Processes. Focusing on Boolean speciﬁcations has certain beneﬁts: (1) The ability to naturally
express tasks with temporal dependencies; (2) the ability to take advantage of the compositionality
present in many problems  and (3) use of formal methods for planning and veriﬁcation [29].
Although standard quantitative scalar reward functions could be used to learn this task from demon-
strations  three issues arise. First  consider the problem of temporal speciﬁcations: reward functions
are typically Markovian  so requirements like those in Ex 1 cannot be directly expressed in the task
representation. One could explicitly encode time into a state and reduce the problem to learning
a Markovian reward on new time-dependent dynamics; however  in general  such a reduction suf-
fers from an exponential blow up in the state size (commonly known as the curse of history [24]).
When inferring tasks from demonstrations  where different hypotheses may have different historical
dependencies  naïvely encoding the entire history quickly becomes intractable.
A second limitation relates to the compositionality of task representations. As suggested  Ex 1
naturally decomposes into two sub-tasks. Ideally  we would want an algorithm that could learn each
sub-task and combine them into the complete task  rather than only be able to learn single monolithic
tasks. However  for many classes of quantitative rewards  "combining" rewards remains an ad-hoc
procedure. The situation is further exacerbated by humans being notoriously bad at anticipating or
identifying when quantitative rewards will lead to unintended consequences [11]  which poses a
serious problem for AI safety [1] and has led to investigations into reward repair [9]. For instance 
we could take a linear combination of rewards for each of the subtasks in Ex 1  but depending on the
relative scales of the rewards  and temporal discount rate  wildly different behaviors would result.
In fact  the third limitation - brittleness due to simple changes in the
environment - illustrates that often  the correctness of the agent can
change due to a simple change in the environment. Namely  imagine
for a moment we remove the water and drying tiles from Fig 1 and
attempt to learn a reward that encodes the “recharge while avoid lava”
task in Ex 1. Fig 2a illustrates the reward resulting from performing
Maximum Entropy Inverse Reinforcement Learning [35] with the
demonstrations shown in Fig 1 and the binary features: red (lava tile) 
yellow (recharge tile)  and “is wet”. As is easy to verify  a reward
i=0 γiri(s)  with a discount factor of γ = 0.69
would generate the trajectory shown in Fig 2a which avoids lava and
eventually recharges.
Unfortunately  using the same reward and discount factor on a nearly
identical world can result in the agent entering the lava. For example 
Fig 2b illustrates the learned reward being applied to a change in
the gridworld where the top left charging tile has been removed. An
acceptable trajectory is indicated via a dashed arrow. Observe that
now the discounted sum of rewards is maximized on the solid arrow’s
path  resulting in the agent entering the lava! While it is possible to
ﬁnd new discount factors to avoid this behavior  such a supervised
process would go against the spirit of automatically learning the task.
Finally  we brieﬂy remark that while non-Markovian Boolean rewards
cannot encode all possible rewards  e.g.  “run as fast as possible” 
often times such objectives can be reframed as policies for a Boolean
task. For example  consider modeling a race. If at each time step there
is a non-zero probability of entering a losing state  the agent will run
forward as fast as possible even for the Boolean task “win the race”.
Thus  quantitative Markovian rewards are limited as a task representation when learning tasks
containing temporal speciﬁcations or compositionality from demonstrations. Moreover  the need to
ﬁne tune learned tasks with such properties seemingly undercuts the original purpose of learning task
representations that are generalizable and invariant to irrelevant aspects of a task [21].
Related Work: Our work is intimately related to Maximum Entropy Inverse Reinforcement Learning.
In Inverse Reinforcement Learning (IRL) [23] the demonstrator  operating in a stochastic environment 
is assumed to attempt to (approximately) optimize some unknown reward function over the trajectories.
In particular  one traditionally assumes a trajectory’s reward is the sum of state rewards of the

(b)
Figure 2:
Illustration of a
bug in the learnt quantitative
Markovian reward resulting
from slight changes in the en-
vironment.

optimizing agent (cid:80)∞

(a)

2

trajectory. This formalism offers a succinct mechanism to encode and generalize the goals of the
demonstrator to new and unseen environments.
In the IRL framework  the problem of learning from demonstrations can then be cast as a Bayesian
inference problem [26] to predict the most probable reward function. To make this inference procedure
well-deﬁned and robust to demonstration/modeling noise  Maximum Entropy IRL [35] appeals to the
principle of maximum entropy [12]. This results in a likelihood over the demonstrations which is no
more committed to any particular behavior than what is required for matching the empirically observed
reward expectation. While this approach was initially limited to learning a linear combination of
feature vectors  IRL has been successfully adapted to arbitrary function approximators such as
Gaussian processes [19] and neural networks [8]. As stated in the introduction  while powerful 
traditional IRL provides no principled mechanism for composing the resulting rewards.
To address this deﬁcit  composition using soft optimality has recently received a fair amount of
attention; however  the compositions are limited to either strict disjunction (do X or Y) [30] [31]
or conjunction (do X and Y) [10]. Further  because soft optimality only bounds the deviation from
simultaneously optimizing both rewards  optimizing the composition does not preclude violating
safety constraints embedded in the rewards (e.g.  do not enter the lava).
The closest work to ours is recent work on inferring Linear Temporal Logic (LTL) by ﬁnding the
speciﬁcation that minimizes the expected number of violations by an optimal agent compared to
the expected number of violations by an agent applying actions uniformly at random [16]. The
computation of the optimal agent’s expected violations is done via dynamic programming on the
explicit product of the deterministic Rabin automaton [7] of the speciﬁcation and the state dynamics.
A fundamental drawback to this procedure is that due to the curse of history  it incurs a heavy run-time
cost  even on simple two state and two action Markov Decision Processes. We also note that the
literature on learning logical speciﬁcations from examples (e.g.  [15  33  20])  does not handle noise in
examples while our approach does. Finally  once a speciﬁcation has been identiﬁed  one can leverage
the rich literature on planning using temporal logic to synthesize a policy [17  28  27  13  14].
Contributions: (i) We formulate the problem of learning speciﬁcations from demonstrations in
terms of Maximum a Posteriori inference. (ii) To make this inference well deﬁned  we appeal to the
principle of maximum entropy culminating in the distribution given (9). The main contribution of this
model is that it only depends on the probability the demonstrator will successfully perform task and
the probability that the task is satisﬁed by performing actions uniformly at random. Because these
properties can be estimated without explicitly unrolling the dynamics in time  this model avoids many
of the pitfalls characteristic to the curse of history. (iii) We provide an algorithm that exploits the
piece-wise convex structure in our posterior model (9) to efﬁciently perform Maximum a Posteriori
inference for the most probable speciﬁcation.
Outline: In Sec 2  we deﬁne speciﬁcations and probabilistic automata (Markov Decision Processes
without rewards). In Sec 3  we introduce the problem of speciﬁcation inference from demonstrations 
and inspired by Maximum Entropy IRL [35]  develop a model of the posterior probability of a
speciﬁcation given a sequence of demonstrations. In Sec 4  we develop an algorithm to perform
inference under (9). Finally  in Sec 5  we demonstrate how due to their inherent composability 
learning speciﬁcations can avoid common bugs that often occur due to ad-hoc reward composition.
2 Background

We seek to learn speciﬁcations from demonstrations provided by a teacher who executes a sequence
of actions that probabilistically changes the system state. For simplicity  we assume that the set of
actions and states are ﬁnite and fully observed. The system is naturally modeled as a probabilistic
automaton1 formally deﬁned below:

Deﬁnition 1 (Probabilistic Automaton). A probabilistic automaton is a tuple M =
(S  s0  A  δ)  where S is the ﬁnite set of states  s0 ∈ S is the initial state  A is the ﬁnite
set of actions  and δ : S × A × S → [0  1] speciﬁes the transition probability of going from s to

s(cid:48) given action a  i.e. δ(s  a  s(cid:48)) = Pr(s(cid:48) | s  a) and (cid:88)

Pr(s(cid:48) | s  a) = 1 for all states s.

s(cid:48)∈S

1Probabilistic Automata are often constructed as a Markov Decision Process  M  without its Markovian

reward map R  denoted M \ R.

3

Deﬁnition 2 (Trace). A sequence of state/action pairs is called a trace (trajectory  demonstra-
tion). A trace of length τ ∈ N is an element of (S × A)τ .

Next  we develop machinery to distinguish between desirable and undesirable traces. For simplicity 
we focus on ﬁnite trace properties  referred to as speciﬁcations  that are decidable within some ﬁxed
τ ∈ N time steps  e.g.  “event A occurred in the last 20 steps”.

Deﬁnition 3 (Speciﬁcation). Given a set of states S  a set of actions A  and a ﬁxed trace length
τ ∈ N  a speciﬁcation is a subset of traces ϕ ⊆ (S × A)τ . We deﬁne true def= (S × A)τ  
¬ϕ def= true \ ϕ  and false def= ¬true. A collection of speciﬁcations  Φ  is called a concept class.
Finally  we abuse notation and use ϕ to also denote its indicator function (interpreted as a
non-Markovian Boolean reward) 

(cid:26)1 if ξ ∈ ϕ

ϕ(ξ) def=

otherwise .

0

(1)

Speciﬁcations may be given in formal notation  as sets or automata. Further  each representation
facilitates deﬁning a plethora of composition rules. For example  consider two speciﬁcations  ϕA  ϕB

that encode tasks A and B respectively and the composition rule ϕA∩ϕB : ξ (cid:55)→ min(cid:0)ϕA(ξ)  ϕB(ξ)(cid:1).
conjunction (logical and). Similarly  maximizing ϕA ∪ ϕB : ξ (cid:55)→ max(cid:0)ϕA(ξ)  ϕB(ξ)(cid:1) corresponds
maximizing ϕA ⊆ ϕB : ξ (cid:55)→ max(cid:0)1 − ϕA(ξ)  ϕB(ξ)(cid:1) corresponds to task A triggering task B.

Because the agent only receives a non-zero reward if ϕA(ξ) = ϕB(ξ) = 1  a reward maximizing
agent must necessarily perform tasks A and B simultaneously. Thus  ϕA ∩ ϕB corresponds to
to disjunction (logical or). One can also encode conditional requirements using subset inclusion  e.g. 

Complicated temporal connectives can also be deﬁned using temporal logics [25] or automata [32].
For our purposes  it sufﬁces to informally extend propositional logic with three temporal operators:
(1) Let Ha  read “historically a”  denote that property a held at all previous time steps. (2) Let
P a def= ¬(H¬a)  read “once a”  denote that the property a at least once held in the past. (3) Let a S b 
read “a since b”  denote that the property a that has held every time step after b last held. The true
power of temporal operators is realized when they are composed to make more complicated sentences.
For example  H(a =⇒ (b S c)) translates to “it was always the case that if a was true  then b has
held since the last time c held.”. Observe that the property BBY from the introductory example takes
this form  H((yellow ∧ P blue) =⇒ (¬blue S brown))  i.e.  “Historically  if the agent had once
visited blue and is currently visiting yellow  then the agent has not visited blue since it last visited
brown”.

3 Speciﬁcation Inference from Demonstrations
In the spirit of Inverse Reinforcement Learning  we now seek to ﬁnd the speciﬁcation that best
explains the behavior of the agent. We refer to this as Speciﬁcation Inference from Demonstrations.
Deﬁnition 4 (Speciﬁcation Inference from Demonstrations). The speciﬁcation inference from
demonstrations problem is a tuple (M  X  Φ) where M = (S  s0  A  δ) is a probabilistic
automaton  X is a (multi-)set of τ-length traces drawn from an unknown distribution induced
by a teacher attempting to demonstrate some unknown speciﬁcation within M  and Φ a concept
class of speciﬁcations.
A solution to (M  X  Φ) is:

(2)
where Pr(ϕ | M  X) denotes the probability that the teacher demonstrated ϕ given the observed
traces  X  and the dynamics  M.

ϕ∈Φ

ϕ∗ ∈ arg max

Pr(ϕ | M  X)

To make this inference well-deﬁned  we make a series of assumptions culminating in (9).
Likelihood of a demonstration: We begin by leveraging the principle of maximum entropy to
disambiguate the likelihood distributions. Concretely  deﬁne:

Pr(si+1|si  ai  M )

(3)

w(cid:0)ξ = (s  a)  M(cid:1) =

τ−1(cid:89)

i=0

4

where s and a are the projected sequences of states and actions of ξ respectively  to be the weight
of each possible demonstration ξ induced by dynamics M. Given a demonstrator who on average
satisﬁes the speciﬁcation ϕ with probability ϕ  we approximate the likelihood function by:

Pr(cid:0)ξ | M  ϕ  ϕ(cid:1) = w(ξ  M ) · exp(λϕϕ(ξ))
where λϕ  Zϕ are normalization factors such that Eξ[ϕ] = ϕ and(cid:80)

(4)
ξ Pr(ξ | M  ϕ) = 1. For a
detailed derivation that (4) is the maximal entropy distribution  we point the reader to [18]. Next
observe that due to the Boolean nature of ϕ  (4) admits a simple closed form:

Zϕ

Pr(ξ | M  ϕ  ϕ) =(cid:103){ξ} ·

(cid:26)ϕ/(cid:101)ϕ
(1 − ϕ)/(cid:102)¬ϕ ξ /∈ ϕ

ξ ∈ ϕ

where in general we use(cid:102)(·) to denote the probability of satisfying a speciﬁcation using uniformly
random actions. Thus  we denote by(cid:103){ξ} the probability of randomly generating demonstration ξ
within M. Further  note that by the law of the excluded middle  for any speciﬁcation: (cid:102)¬ϕ = 1 −(cid:101)ϕ.

Proof Sketch. For brevity  let Wϕ

ξ∈ϕ w(ξ  M ) and c def= eλϕ. Via the constraints on (4) 

def=(cid:80)
Zϕ · ϕ = 1 ·(cid:88)
c1 · w(ξ  M ) + 0 ·(cid:88)
w(ξ  M ) + c0(cid:88)
Zϕ = c1(cid:88)

ξ∈ϕ

ξ /∈ϕ

ξ∈ϕ

ξ /∈ϕ

c0 · w(ξ  M ) = cWϕ

w(ξ  M ) = cWϕ + W¬ϕ

(5)

(6)

(cid:18)(cid:89)

L(X | M  ϕ  ϕ) = log

Combining gives Zϕ = W¬ϕ/(1− ϕ). Next  observe that if ξ (cid:54)∈ ϕ  then eλϕϕ(ξ) = 1 and substituting
in (4) yields  Pr(ξ | ϕ  M  ξ /∈ ϕ) = wξ(1 − ϕ)/W¬ϕ.
If ξ ∈ ϕ (implying Wϕ (cid:54)= 0) then

Likelihood of a set of demonstrations: If the teacher gives a ﬁnite sequence of τ length demonstra-
tions  X  drawn i.i.d. from (5)  then the log likelihood  L  of X under (5) is:2

eλϕ = Zϕϕ/Wϕ and Pr(ξ | ϕ  M  ξ ∈ ϕ) = wξϕ/Wϕ. Finally  observe that (cid:101)ϕ = Wϕ/Wtrue and
(cid:103){ξ} = wξ/Wtrue. Substituting and factoring yields (5).
(cid:19)
(cid:103){ξ}
(cid:18) 1 − ϕ
1 −(cid:101)ϕ

between two Bernoulli distributions with means ϕ and (cid:101)ϕ resp. Syntactically  let B(µ) denote

(cid:18) ϕ(cid:101)ϕ
(cid:88)
(cid:19)(cid:21)

where by deﬁnition we take (0 · ln(. . .) = 0) and Nϕ

a Bernoulli distribution with mean µ and DKL(P (cid:107) Q) def=
information gain when using distribution P compared to Q. If X is “representative” such that
Nϕ ≈ ϕ · |X|  we can (up to a ϕ independent normalization) approximate (7):

P (i) ln(P (i)/Q(i)) denote the

is the information gain (KL divergence)

(cid:18)¬ϕ(cid:102)¬ϕ

Next  observe that

(cid:18) ϕ(cid:101)ϕ

+ (1 − ϕ) ln

+ N¬ϕ ln

+ Nϕ ln

def=

ϕ(ξ).

ξ∈X

(cid:19)

(cid:19)

(cid:19)

(cid:20)

ξ∈X

ϕ ln

(7)

Pr(X | M  ϕ  ϕ) ∝∼ exp

(8)
Where ∝∼ denotes approximately proportional to. Unfortunately  the approximation |X| · ϕ ≈
Nφ implies that  ¬ϕ = 1 − ϕ which introduces the undesirable symmetry  Pr(X | M  ϕ  ϕ) =
least as good as random. Operationally  we assert that Pr(ϕ | ϕ < (cid:101)ϕ) = 0 and is otherwise uniform.
Pr(X | M ¬ϕ ¬ϕ)  into (8). To break this symmetry  we assert that the demonstrator must be at
Finally  we arrive at the posterior distribution given in (9)  where 1[·] denotes an indicator function.

Demonstrator is better than random.

(cid:122)
(cid:125)(cid:124)
1[ϕ ≥ (cid:101)ϕ]· exp(cid:0)|X| ·

(cid:123)

Information gain over random actions.

(cid:122)
(cid:123)
DKL (B(ϕ) (cid:107) B((cid:101)ϕ))(cid:1)

(cid:125)(cid:124)

Pr(ϕ | M  X  ϕ) ∝∼

(9)
2We have suppressed a multinomial coefﬁcient required if any two demonstrations are the same. However 

this term will not change as ϕ varies  and thus cancels when comparing across speciﬁcations.

(cid:88)
(cid:16)B(ϕ) (cid:107) B((cid:101)ϕ)
(cid:17)(cid:17)

i

(cid:16)|X| · DKL

5

4 Algorithm

In this section  we exploit the structure imposed by (9) to efﬁciently search for the most probable
speciﬁcation (2) within a (potentially large) concept class  Φ. Namely  observe that under (9)  the
speciﬁcation inference problem (2) reduces to maximizing the information gain over random actions.

(cid:110)
1[ϕ ≥ (cid:101)ϕ] · DKL

(cid:16)B(ϕ) (cid:107) B((cid:101)ϕ)
(cid:17)(cid:111)

ϕ∗ ∈ arg max

ϕ∈Φ

(10)

to be #P -complete [2]. Nevertheless  in practice  moderately efﬁcient methods for computing or

applicable. Further  while evaluating if a trace satisﬁes a speciﬁcation is fairly efﬁcient (and thus

Because gradients on (cid:101)ϕ and ϕ are not well-deﬁned  gradient descent based algorithms are not
our Nϕ/|X| approximation to ϕ is assumed easy to compute)  computing (cid:101)ϕ is in general known
approximating (cid:101)ϕ exist including Monte Carlo simulation [22] and weighted model counting [5] via
that poses few (cid:101)ϕ queries. We begin with the observation that adding a trace to a speciﬁcation cannot
Lemma 1. ∀ϕ(cid:48)  ϕ ∈ Φ . ϕ(cid:48) ⊆ ϕ implies (cid:101)ϕ(cid:48) ≤ (cid:101)ϕ and ϕ(cid:48) ≤ ϕ.

Binary Decision Diagrams (BDDs) [3] or repeated SAT queries [4]. As such  we seek an algorithm

lower its probability of satisfaction under random actions.

Proof. The probability of sampling an element of a set monotonically increases as elements are
added to the set independent of the ﬁxed underlying distribution over elements.

Further  note that Nϕ (and thus  our approximation to ϕ) can only take on |X| + 1 possible values.
This suggests a piece-wise analysis of (10) by conditioning on the value of ϕ.

Deﬁnition 5. Given candidate speciﬁcations Φ and a subset of demonstrations S ⊆ X deﬁne 

(cid:21)

(cid:20) |S|
|X| ≥ x

(cid:18)

(cid:19)
|S|
|X| ) (cid:107) B(x)

· DKL

B(

(11)

def= {ϕ ∈ Φ : ϕ ∩ X = S}

ΦS

J|S|(x) def= 1

The next key observation is that J|S| : [0  1] → R≥0 monotonically decreases in x.

Lemma 2. ∀S ⊆ X  x < x(cid:48) =⇒ J|S|(x) ≤ J|S|(x(cid:48))
|S|
|X| ≥ x] indicator 
Proof. To begin  observe that DKL is always non-negative. Due to the 1[
J|S|(x) = 0 for all x > |S|/|X|. Next  observe that J|S| is convex due to the convexity of the
DKL on Bernoulli distributions and is minimized at x = |S|/|X| (KL Divergence of identical
distributions is 0). Thus  J|S|(x) monotonically decreases as x increases.

Illustration of Thm 1

ϕ1

)
x
(
6
J

4

2

0

ϕ2 ϕ3

ϕ4

0.0

0.2

0.4

x

0.6

0.8

1.0

Figure 3: Left: An example of a series of speciﬁcations ϕ1  . . .   ϕ4 ordered by subset inclusion. The
dots represent demonstrations  and so each speciﬁcation has ϕi = 6/9. Right: Plot of J|S|(x) for

hypothetical values of (cid:101)ϕi annotated as points. Notice that the sequence of speciﬁcations is ordered on

the x-axis  and thus the maximum must occur at the start of the sequence.
These insights are then combined in Theorem 1 and illustrated in Fig 3.

Theorem 1. If A denotes a sequence of speciﬁcations  ϕ1  . . .   ϕn  ordered by subset inclusion
j ≤ k =⇒ ϕj ⊆ ϕk and S ⊆ X is an arbitrary subset of demonstrations  then:

J|S|((cid:101)ϕ) = J|S|((cid:101)ϕ1)

max
ϕ∈A

6

(12)

Space of Trajectoriesφ₁φ₂φ₃φ₄Proof. (cid:101)ϕ is monotonically increasing on A (Lemma 1). Via Lemma 2 J|S|(x) is monotonically
decreasing and thus the maximum of J|S|((cid:101)ϕ) must occur at the beginning of A.

ϕ7

ϕ6

true

Lattice Concept Classes.
Theorem 1 suggests specializing to concept classes where
determining subset relations is easy. We propose studying con-
cept classes organized into a ﬁnite (bounded) lattice  (Φ  (cid:69)) 
that respects subset inclusion: (ϕ (cid:69) ϕ(cid:48) =⇒ ϕ ⊆ ϕ(cid:48)). To
enforce the bounded constraint  we assert that true and false
are always assumed to be in Φ and act as the bottom and top of
the partial order respectively. Intuitively  this lattice structure
encodes our partial knowledge of which speciﬁcations imply
other speciﬁcations. These implication relations can be rep-
resented as a directed graph where the nodes correspond to
elements of Φ and an edge is present if the source is known
to imply the target. Because implication is transitive  many of
the edges can be omitted without losing any information. The
graph resulting from this transitive reduction is called a Hasse
diagram [6] (See Fig 4). In terms of the graphical model  the
Hasse diagram encodes that for certain pairs of speciﬁcations 
ϕ  ϕ(cid:48)  we know that Pr(ϕ(ξ) = 1 | ϕ(cid:48)(ξ) = 1  M ) = 1 or
Pr(ϕ(ξ) = 0 | ϕ(cid:48)(ξ) = 0  M ) = 1.
Inference on chain concept classes. Sequences of speciﬁcations ordered by subset inclusion gener-
alize naturally to ascending chains.

Figure 4: Hasse Diagram of an ex-
ample lattice Φ with an anti-chain
annotated. Directed edges represent
known subset relations and paths
represent chains.

anti-chain

false

ϕ3

ϕ2

ϕ4

ϕ0

Deﬁnition 1 (Chains and Anti-Chains). Given a partial order (Φ  (cid:69))  an ascending chain (or
just chain) is a sequence of elements of A ordered by (cid:69). The smallest element of the chain is
denoted  ↓ (A). Finally  an anti-chain is a set of incomparable elements. An anti-chain is called
maximal if no element can be added to it without making two of its elements comparable.

Recasting Theorem 1 in the parlance of chains yields:

Corollary 1. If S ⊆ X is a subset of demonstrations and A is a chain in (ΦS  (cid:69)) then:

J|S|((cid:101)ϕ) = J|S|( (cid:93)↓ (A))

max
ϕ∈A

(13)

Observe that if the lattice  (Φ  (cid:69)) is itself a chain  then there are at most |X| + 1 non-empty
demonstration partitions  ΦS. In fact  the non-empty partitions can be re-indexed by the cardinality of
S  e.g.  ΦS (cid:55)→ Φ|S|. Further  note that since chains are totally ordered  the smallest element of each
non-empty partition can be found by performing a binary search (indicated by ﬁnd_smallest below).
These insights are combined into Algorithm 1 with a relativized run-time analysis given in Thm 2.
Algorithm 1 Inference on chains
1: procedure CHAIN_INFERENCE(X  (A  (cid:69)))
2:

(i  ﬁnd_smallest(A  i))(cid:12)(cid:12) i ∈ {0  1  . . .  |X|}

(cid:26)

(cid:27)

Ψ ←
return i  ϕ∗ ← arg max
i ϕ∈Ψ

Ji((cid:101)ϕ)

3:

(cid:46) O(|Tdata|X| ln(|A|)).
(cid:46) O(Trand|X|)

Theorem 2. Let Tdata and Trand respectively represent the worst case execution time of comput-

ing ϕ and (cid:101)ϕ for ϕ in chain A. Given demonstrations X  Alg 1 runs in time:

(14)

(cid:18)

|X|(cid:16)

O

(cid:17)(cid:19)

Tdata ln(|A|) + Trand

7

Proof Sketch. A binary search over |A| elements takes ln(|A|) time. There are |X| binary
searches required to ﬁnd the smallest element of each partition. Finally  for each smallest
element  a single random satisfaction query is made.

Lattice inference. Of course  in general  (Φ  (cid:69)) is not a chain  but a complicated lattice. Nevertheless 
observe that any path from false to true is a chain. Further  the smallest element of each partition
must either be the same speciﬁcation or incomparable in (Φ  (cid:69)). That is  for each k ∈ {0  1  . . .|X|} 
the set:

(cid:26)

(cid:18)X

(cid:19)(cid:27)

Bk

def=

↓ (ΦS) : S ∈

k
is a maximal anti-chain. Thus  Corollary 1 can be extended to:
Corollary 2. Given a lattice (Φ  (cid:69)) and demonstrations X:
k∈0 1 ... |X| max
ϕ∈Bk

JNϕ ((cid:101)ϕ) = max

max
ϕ∈Φ

Jk((cid:101)ϕ)

(15)

(16)

Recalling that Nϕ increases on paths from false to true  we arrive at the following simple algorithm
which takes as input the demonstrations and the lattice ϕ encoded as a directed acyclic graph rooted
at false. (i) Perform a breadth ﬁrst traversal (BFT) of the lattice (Φ  (cid:69)) starting at false (ii) During
the traversal  if speciﬁcation ϕ has a larger Nϕ than all of its direct predecessors  then check if it is
more probable than the best speciﬁcation seen so far (if so  make it the most probable speciﬁcation
seen so far). (iii) At the end of the traversal  return the most probable speciﬁcation. Pseudo code is
provided in Algorithm 2 with a run-time analysis given in Theorem 3.
Algorithm 2 Inference on Partial Orders
1: procedure PARTIALORDER_INFERENCE(X  (Φ  (cid:69)))
2:
3:
4:
5:
6:
7:
8:
9:
10:

(ϕ∗  best_info_gain) ← (false  0)
for ϕ in breadth_ﬁrst_traversal((Φ  (cid:69))) do

parents ← direct_predecessors(ϕ)
if ∃ϕ(cid:48) ∈ parents . Nϕ(cid:48) = Nϕ then

info_gain ← JNϕ ((cid:101)ϕ)

if info_gain > best_info_gain then

(ϕ∗  best_info_gain) ← (ϕ  info_gain)

return ϕ∗

continue

Theorem 3. Let (Φ  (cid:69)) be a bounded partial order encoded as a Directed Acyclic Graph (DAG) 
G = (V  E)  with vertices V and edges E. Further  let B denote the largest anti-chain in Φ. If

Tdata and Trand respectively represent the worst case execution time of computing ϕ and (cid:101)ϕ  then

for demonstrations X  Alg 2 runs in time:

O(cid:0)E + Tdata · V + Trand · |B||X|(cid:1)

(17)
Proof sketch. BFT takes O(V + E). Further  for each node  ϕ is computed (O(Tdata · V )).
size of the largest anti-chain  this query happens no more than |B||X| times.

Finally  for each node in each of the candidate anti-chains Bk  (cid:101)ϕ is computed. Since |B| is the

5 Experiments and Discussion

Scenario. Recall our introductory gridworld example Ex 1. Now imagine that the robot is pre-
programmed to perform task the “recharge and avoid lava” task  but is unaware of the second
requirement  “do not recharge when wet”. To signal this additional constraint to the robot  the
human operator provides the ﬁve demonstrations shown in Fig 1. We now illustrate how learning
speciﬁcations rather than Markovian rewards enables the robot to safely compose the new constraint
with its existing knowledge to perform the joint task in a manner that is robust to changes in the task.
To begin  we assume the robot has access to the Boolean features: red (lava tile)  blue (water tile) 
brown (drying tile)  and yellow (recharge tile). Using these features  the robot has encoded the
“recharge and avoid lava” task as: H(¬red) ∧ P (yellow).

8

Concept Class. We designed the robot’s concept
class to be the conjunction of the known require-
ments and a speciﬁcation generated by the gram-
mar on the right. The motivation in choosing this
grammar was that (i) it generates a moderately
large concept class (930 possible speciﬁcations
after pruning trivially false speciﬁcations)  and
(ii) it contains several interesting alternative tasks
such as H(red =⇒ (¬brown S blue))  which
semantically translates to: “the robot should be
wet before entering lava”. To generate
the edges in Hasse diagram  we unrolled the formula into their corresponding Boolean formula and
used a SAT solver to determine subset relations. While potentially slow  we make three observations
regarding this process: (i) the process was trivially parallelizable (ii) so long as the atomic predicates
remain the same  this Hasse diagram need not be recomputed since it is otherwise invariant to the
dynamics (iii) most of the edges in the resulting diagram could have been syntactically identiﬁed
using well known identities on temporal logic formula.

Concept Class Grammar:
|= (cid:104)H ψ(cid:105) | (cid:104)P ψ(cid:105)
(cid:104)φ(cid:105)
(cid:104)ψ(cid:105)
|= (cid:104)β(cid:105) | (cid:104)β(cid:105) =⇒ (cid:104)β(cid:105)
|= (cid:104)α(cid:105) | (cid:104)α(cid:105) ∧ (cid:104)α(cid:105) | (cid:104)α(cid:105) S (cid:104)α(cid:105)
(cid:104)β(cid:105)
|= AP | ¬AP
(cid:104)α(cid:105)
(cid:104)AP(cid:105)
|= yellow | red | brown | blue

Computing (cid:101)ϕ. To perform random satisfaction rate queries  (cid:101)ϕ  we ﬁrst ran Monte Carlo to get a

coarse estimate and we symbolically encoded the dynamics  color sensor  and speciﬁcation into a
Binary Decision Diagram to get exact values. This data structure serves as an incredibly succinct
encoding of the speciﬁcation aware unrolling of the dynamics  which in practice avoids the exponential
blow up suggested by the curse of history. We then counted the number of satisfying assignments and
divided by the total possible number of satisfying assignments.3 On average in these candidate pools 
each query took 0.4 seconds with a standard deviation of 0.32 seconds.
Results. Running a fairly unoptimized implementation of Algorithm 2 on the concept class and
class). The inferred additional requirement was H((yellow∧ P blue) =⇒ (¬blue S brown)) which
exactly captures the do not recharge while wet constraint. Compared to a brute force search over the
concept class  our algorithm offered an approximately 5.5 fold improvement. Crucially  there exists
controllable trajectories satisfying the joint speciﬁcation:

demonstrations took approximately 95 seconds and resulted in 172 (cid:101)ϕ queries (≈ 18% of the concept

H¬red ∧ P yellow

∧ H

(yellow ∧ P blue) =⇒ (¬blue S brown)

.

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(18)

Thus  a speciﬁcation optimizing agent must jointly perform both tasks. This holds true even under task
changes such as that in Fig 2. Further  observe that it was fairly painless to incorporate the previously
known recharge while avoiding lava constraints. Thus  in contrast to quantitative Markovian rewards 
learning Boolean speciﬁcations enabled encoding compositional temporal speciﬁcations that are
robust to changes in the environment.

6 Conclusion and Future work
Motivated by the problem of compositionally learning from demonstrations  we developed a technique
for learning binary non-Markovian rewards  which we referred to as speciﬁcations. Because of their
limited structure  speciﬁcations enabled ﬁrst learning sub-speciﬁcations for subtasks and then later
creating a composite speciﬁcations that encodes the larger task. To learn these speciﬁcations from
demonstrations  we applied the principle of maximum entropy to derive a novel model for the
likelihood of a speciﬁcation given the demonstrations. We then developed an algorithm to efﬁciently
search for the most probable speciﬁcation in a candidate pool of speciﬁcations in which some subset
relations between speciﬁcations are known. Finally  in our experiment  we gave a concrete instance
where using traditional learning composite reward functions is non-obvious and error-prone  but
inferring speciﬁcations enables trivial composition. Future work includes extending the formalism
to inﬁnite horizon speciﬁcations  continuous dynamics  characterizing the optimal set of teacher
demonstrations under our posterior model [34]  efﬁciently marginalizing over the whole concept
class and exploring alternative data driven methods for generating concept classes.
Acknowledgments. We would like to thank the anonymous referees as well as Daniel Fremont  Markus Rabe 
Ben Caulﬁeld  Marissa Ramirez Zweiger  Shromona Ghosh  Gil Lederman  Tommaso Dreossi  Anca Dragan  and

3One can add probabilities to transitions by adding to transition constraints additional fresh variables such

that the number of satisfying assignments is proportional to the probability.

9

Natarajan Shankar for their useful suggestions and feedback. The work of the authors on this paper was funded
in part by the US National Science Foundation (NSF) under award numbers CNS-1750009  CNS-1740079  CNS-
1545126 (VeHICaL)  the DARPA BRASS program under agreement number FA8750–16–C0043  the DARPA
Assured Autonomy program  by Toyota under the iCyPhy center and the US ARL Cooperative Agreement
W911NF-17-2-0196.

References
[1] D. Amodei  C. Olah  J. Steinhardt  P. Christiano  J. Schulman  and D. Mané. Concrete problems

in AI safety. arXiv preprint arXiv:1606.06565  2016.

[2] F. Bacchus  S. Dalmao  and T. Pitassi. Algorithms and complexity results for #SAT and Bayesian
inference. In Foundations of computer science  2003. proceedings. 44th annual ieee symposium
on  pages 340–351. IEEE  2003.

[3] R. E. Bryant. Symbolic boolean manipulation with ordered binary-decision diagrams. ACM

Computing Surveys (CSUR)  24(3):293–318  1992.

[4] S. Chakraborty  K. S. Meel  and M. Y. Vardi. Algorithmic improvements in approximate
counting for probabilistic inference: From linear to logarithmic sat calls. Proceedings of the
25th International Joint Conference on Artiﬁcial Intelligence (IJCAI-16)  2016.

[5] M. Chavira and A. Darwiche. On probabilistic inference by weighted model counting. Artiﬁcial

Intelligence  172(6-7):772–799  2008.

[6] N. Christoﬁdes. Graph theory: An algorithmic approach (Computer science and applied

mathematics). Academic Press  Inc.  1975.

[7] B. Farwer. ω-automata. In Automata logics  and inﬁnite games  pages 3–21. Springer  2002.
[8] C. Finn  S. Levine  and P. Abbeel. Guided cost learning: Deep inverse optimal control via policy

optimization. In International Conference on Machine Learning  pages 49–58  2016.

[9] S. Ghosh  S. Jha  A. Tiwari  P. Lincoln  and X. Zhu. Model  data and reward repair: Trusted
machine learning for Markov Decision Processes. In 2018 48th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks Workshops (DSN-W)  pages 194–199. IEEE 
2018.

[10] T. Haarnoja  V. Pong  A. Zhou  M. Dalal  P. Abbeel  and S. Levine. Composable deep reinforce-

ment learning for robotic manipulation. arXiv preprint arXiv:1803.06773  2018.

[11] M. K. Ho  M. L. Littman  F. Cushman  and J. L. Austerweil. Teaching with Rewards and
Punishments: Reinforcement or Communication? In D. Noelle  R. Dale  A. S. Warlaumont 
J. Yoshimi  T. Matlock  C. D. Jennings  and P. P. Maglio  editors  Proceedings of the 37th Annual
Conference of the Cognitive Science Society  pages 920–925  Austin  TX  2015. Cognitive
Science Society.

[12] E. T. Jaynes. Information theory and statistical mechanics. Physical review  106(4):620  1957.
[13] S. Jha and V. Raman. Automated synthesis of safe autonomous vehicle control under perception

uncertainty. In NASA Formal Methods Symposium  pages 117–132. Springer  2016.

[14] S. Jha  V. Raman  D. Sadigh  and S. A. Seshia. Safe autonomy under perception uncertainty
using chance-constrained temporal logic. Journal of Automated Reasoning  60(1):43–62  2018.
[15] S. Jha  A. Tiwari  S. A. Seshia  T. Sahai  and N. Shankar. TeLEx: Passive STL learning using
only positive examples. In International Conference on Runtime Veriﬁcation  pages 208–224.
Springer  2017.

[16] D. Kasenberg and M. Scheutz.

Interpretable apprenticeship learning with temporal logic

speciﬁcations. arXiv preprint arXiv:1710.10532  2017.

[17] H. Kress-Gazit  G. E. Fainekos  and G. J. Pappas. Temporal-logic-based reactive mission and

motion planning. IEEE Transactions on Robotics  25(6):1370–1381  2009.

[18] S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.

CoRR  abs/1805.00909  2018.

[19] S. Levine  Z. Popovic  and V. Koltun. Nonlinear inverse reinforcement learning with gaussian
processes. In J. Shawe-Taylor  R. S. Zemel  P. L. Bartlett  F. Pereira  and K. Q. Weinberger 
editors  Advances in Neural Information Processing Systems 24  pages 19–27. Curran Associates 
Inc.  2011.

10

[20] W. Li. Speciﬁcation Mining: New Formalisms  Algorithms and Applications. PhD thesis  EECS

Department  University of California  Berkeley  Mar 2014.

[21] M. L. Littman  U. Topcu  J. Fu  C. Isbell  M. Wen  and J. MacGlashan. Environment-Independent

Task Speciﬁcations via GLTL. arXiv:1704.04341 [cs]  Apr. 2017. arXiv: 1704.04341.

[22] N. Metropolis and S. Ulam. The Monte Carlo method. Journal of the American statistical

association  44(247):335–341  1949.

[23] A. Y. Ng  S. J. Russell  et al. Algorithms for inverse reinforcement learning. In ICML  pages

663–670  2000.

[24] J. Pineau  G. Gordon  S. Thrun  et al. Point-based value iteration: An anytime algorithm for

POMDPs. In IJCAI  volume 3  pages 1025–1032  2003.

[25] A. Pnueli. The temporal logic of programs. In Foundations of Computer Science  1977.  18th

Annual Symposium on  pages 46–57. IEEE  1977.

[26] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. IJCAI  2007.
[27] V. Raman  A. Donzé  D. Sadigh  R. M. Murray  and S. A. Seshia. Reactive synthesis from
signal temporal logic speciﬁcations. In Proceedings of the 18th International Conference on
Hybrid Systems: Computation and Control  pages 239–248. ACM  2015.

[28] I. Saha  R. Ramaithitima  V. Kumar  G. J. Pappas  and S. A. Seshia. Automated composition of
motion primitives for multi-robot systems from safe LTL speciﬁcations. In Intelligent Robots
and Systems (IROS 2014)  2014 IEEE/RSJ International Conference on  pages 1525–1532.
IEEE  2014.

[29] S. A. Seshia  D. Sadigh  and S. S. Sastry. Towards Veriﬁed Artiﬁcial Intelligence. ArXiv e-prints 

July 2016.

[30] E. Todorov. Linearly-solvable Markov decision problems. In Advances in neural information

processing systems  pages 1369–1376  2007.

[31] E. Todorov. General duality between optimal control and estimation. In Decision and Control 

2008. CDC 2008. 47th IEEE Conference on  pages 4286–4292. IEEE  2008.

[32] M. Y. Vardi. An automata-theoretic approach to linear temporal logic. In Logics for concurrency 

pages 238–266. Springer  1996.

[33] M. Vazquez-Chanlatte  J. V. Deshmukh  X. Jin  and S. A. Seshia. Logical clustering and
learning for time-series data. In International Conference on Computer Aided Veriﬁcation 
pages 305–325. Springer  2017.

[34] M. Vazquez-Chanlatte  M. K. Ho  T. L. Grifﬁths  and S. A. Seshia. Communicating Composi-
tional and Temporal Speciﬁcations by Demonstrations  extended abstract. In Symposium on
Cyber-Physical Human Systems (CPHS)  2018.

[35] B. D. Ziebart  A. L. Maas  J. A. Bagnell  and A. K. Dey. Maximum entropy inverse reinforcement

learning. In AAAI  volume 8  pages 1433–1438. Chicago  IL  USA  2008.

11

,Marcell Vazquez-Chanlatte
Susmit Jha
Ashish Tiwari
Mark Ho
Sanjit Seshia