2016,Assortment Optimization Under the Mallows model,We consider the assortment optimization problem when customer preferences follow a mixture of Mallows distributions. The assortment optimization problem focuses on determining the revenue/profit maximizing subset of products from a large universe of products; it is an important decision that is commonly faced by retailers in determining what to offer their customers. There are two key challenges: (a) the Mallows distribution lacks a closed-form expression (and requires summing an exponential number of terms) to compute the choice probability and  hence  the expected revenue/profit per customer; and (b) finding the best subset may require an exhaustive search. Our key contributions are an efficiently computable closed-form expression for the choice probability under the Mallows model and a compact mixed integer linear program (MIP) formulation for the assortment problem.,Assortment Optimization Under the Mallows model

Antoine Désir
IEOR Department
Columbia University

antoine@ieor.columbia.edu

Srikanth Jagabathula

IOMS Department

NYU Stern School of Business
sjagabat@stern.nyu.edu

Vineet Goyal

IEOR Department
Columbia University

vgoyal@ieor.columbia.edu

Danny Segev

Department of Statistics

University of Haifa

segevd@stat.haifa.ac.il

Abstract

We consider the assortment optimization problem when customer preferences
follow a mixture of Mallows distributions. The assortment optimization problem
focuses on determining the revenue/proﬁt maximizing subset of products from a
large universe of products; it is an important decision that is commonly faced by
retailers in determining what to offer their customers. There are two key challenges:
(a) the Mallows distribution lacks a closed-form expression (and requires summing
an exponential number of terms) to compute the choice probability and  hence  the
expected revenue/proﬁt per customer; and (b) ﬁnding the best subset may require an
exhaustive search. Our key contributions are an efﬁciently computable closed-form
expression for the choice probability under the Mallows model and a compact
mixed integer linear program (MIP) formulation for the assortment problem.

1

Introduction

Determining the subset (or assortment) of items to offer is a key decision problem that commonly
arises in several application contexts. A concrete setting is that of a retailer who carries a large
universe of products U but can offer only a subset of the products in each store  online or ofﬂine. The
objective of the retailer is typically to choose the offer set that maximizes the expected revenue/proﬁt1
earned from each arriving customer. Determining the best offer set requires: (a) a demand model and
(b) a set optimization algorithm. The demand model speciﬁes the expected revenue from each offer
set  and the set optimization algorithm ﬁnds (an approximation of) the revenue maximizing subset.
In determining the demand  the demand model must account for product substitution behavior 
whereby customers substitute to an available product (say  a dark blue shirt) when her most preferred
product (say  a black one) is not offered. The substitution behavior makes the demand for each
offered product a function of the entire offer set  increasing the complexity of the demand model.
Nevertheless  existing work has shown that demand models that incorporate substitution effects
provide signiﬁcantly more accurate predictions than those that do not. The common approach to
capturing substitution is through a choice model that speciﬁes the demand as the probability P(a|S)
of a random customer choosing product a from offer set S. The most general and popularly studied
class of choice models is the rank-based class [9  24  12]  which models customer purchase decisions
through distributions over preference lists or rankings. These models assume that in each choice
instance  a customer samples a preference list specifying a preference ordering over a subset of the
1As elaborated below  conversion-rate maximization can be obtained as a special case of revenue/proﬁt

maximization by setting the revenue/proﬁt of all the products to be equal.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

products  and chooses the ﬁrst available product on her list; the chosen product could very well be
the no-purchase option.
The general rank-based model accommodates distributions with exponentially large support sizes
and  therefore  can capture complex substitution patterns; however  the resulting estimation and
decision problems become computationally intractable. Therefore  existing work has focused on
various parametric models over rankings. By exploiting the particular parametric structures  it has
designed tractable algorithms for estimation and decision-making. The most commonly studied
models in this context are the Plackett-Luce (PL) [22] model and its variants  the nested logit (NL)
model and the mixture of PL models. The key reason for their popularity is that the assumptions
made in these models (such as the Gumbel assumption for the error terms in the PL model) are
geared towards obtaining closed-form expressions for the choice probabilities P(a|S). On the other
hand  other popular models in the machine learning literature such as the Mallows model have
largely been ignored because computing choice probabilities under these models has been generally
considered to be computationally challenging  requiring marginalization of a distribution with an
exponentially-large support size.
In this paper  we focus on solving the assortment optimization problem under the Mallows model.
The Mallows distribution was introduced in the mid-1950’s [17] and is the most popular member
of the so-called distance-based ranking models  which are characterized by a modal ranking !
and a concentration parameter ✓. The probability that a ranking  is sampled falls exponentially
as e✓·d( !). Different distance functions result in different models. The Mallows model uses
the Kendall-Tau distance  which measures the number of pairwise disagreements between the two
rankings. Intuitively  the Mallows model assumes that consumer preferences are concentrated around
a central permutation  with the likelihood of large deviations being low.
We assume that the parameters of the model are given. Existing techniques in machine learning
may be applied to estimate the model parameters. In settings of our interest  data are in the form of
choice observations (item i chosen from offer set S)  which are often collected as part of purchase
transactions. Existing techniques focus on estimating the parameters of the Mallows model when the
observations are complete rankings [8]  partitioned preferences [14] (which include top-k/bottom-k
items)  or a general partial-order speciﬁed in the form of a collection of pairwise preferences [15].
While the techniques based on complete rankings and partitioned preferences don’t apply to this
context  the techniques proposed in [15] can be applied to infer the model parameters.

Our results. We address the two key computational challenges that arise in solving our problem:
(a) efﬁciently computing the choice probabilities and hence  the expected revenue/proﬁt  for a given
offer set S and (b) ﬁnding the optimal offer set S⇤. Our main contribution is to propose two alternate
procedures to to efﬁciently compute the choice probabilities P(a|S) under the Mallows model. As
elaborated below  even computing choice probabilities is a non-trivial computational task because it
requires marginalizing the distribution by summing it over an exponential number of rankings. In
fact  computing the probability of a general partial order under the Mallows model is known to be a
#P hard problem [15  3]. Despite this  we show that the Mallows distribution has rich combinatorial
structure  which we exploit to derive a closed-form expression for the choice probabilities that takes
the form of a discrete convolution. Using the fast Fourier transform  the choice probability expression
can be evaluated in O(n2 log n) time (see Theorem 3.2)  where n is the number of products. In
Section 4  we exploit the repeated insertion method (RIM) [7] for sampling rankings according to
the Mallows distribution to obtain a dynamic program (DP) for computing the choice probabilities
in O(n3) time (see Theorem 4.2). The key advantage of the DP speciﬁcation is that the choice
probabilities are expressed as the unique solution to a system of linear equations. Based on this
speciﬁcation  we formulate the assortment optimization problem as a compact mixed linear integer
program (MIP) with O(n) binary variables and O(n3) continuous variables and constraints. The
MIP provides a framework to model a large class of constraints on the assortment (often called
“business constraints") that are necessarily present in practice and also extends to mixture of Mallows
model. Using a simulation study  we show that the MIP provides accurate assortment decisions in a
reasonable amount of time for practical problem sizes.
The exact computation approaches that we propose for computing choice probabilities are necessary
building blocks for our MIP formulation. They also provide computationally efﬁcient alternatives to
computing choice probabilities via Monte-Carlo simulations using the RIM sampling method. In fact 
the simulation approach will require exponentially many samples to obtain reliable estimates when

2

products have exponentially small choice probabilities. Such products commonly occur in practice
(such as the tail products in luxury retail). They also often command high prices because of which
discarding them can signiﬁcantly lower the revenues.

Literature review. A large number of parametric models over rankings have been extensively
studied in the areas of statistics  transportation  marketing  economics  and operations management
(see [18] for a detailed survey of most of these models). Our work particularly has connections to the
work in machine learning and operations management. The existing work in machine learning has
focused on designing computationally efﬁcient algorithms for estimating the model parameters from
commonly available observations (complete rankings  top-k/bottom-k lists  pairwise comparisons 
etc.). The developed techniques mainly consist of efﬁcient algorithms for computing the likelihood of
the observed data [14  11] and sampling techniques for sampling from the distributions conditioned
on observed data [15  20]. The Plackett-Luce (PL) model  the Mallows model  and their variants have
been  by far  the most studied models in this literature. On the other hand  the work in operations
management has mainly focused on designing set optimization algorithms to ﬁnd the best subset
efﬁciently. The multinomial logit (MNL) model has been the most commonly studied model in this
literature. The MNL model was made popular by the work of [19] and has been shown by [25] to be
equivalent to the PL model  introduced independently by Luce [16] and Plackett [22]. When given
the model parameters  the assortment optimization problem has been shown to be efﬁciently solvable
for the MNL model by [23]  variants of the nested logit (NL) model by [6  10]  and the Markov chain
model by [2]. The problem is known to be hard for most other choice models [4]  so [13] studies the
performance of the local search algorithm for some of the assortment problems that are known to be
hard. As mentioned  the literature in operations management has restricted itself to only those models
for which choice probabilities are known to be efﬁciently computed. In the context of the literature 
our key contribution is to extend a popular model in the machine learning literature to choice contexts
and the assortment problem.

2 Model and problem statement

Notation. We consider a universe U of n products. In order to distinguish products from their
corresponding ranks  we let U = {a1  . . .   an} denote the universe of products  under an arbitrary
indexing. Preferences over this universe are captured by an anti-reﬂexive  anti-symmetric  and
transitive relation   which induces a total ordering (or ranking) over all products; speciﬁcally 
a  b means that a is preferred to b. We represent preferences through rankings or permutations. A
complete ranking (or simply a ranking) is a bijection  : U ! [n] that maps each product a 2 U to
its rank (a) 2 [n]  where [j] denotes the set {1  2  . . .   j} for any integer j. Lower ranks indicate
higher preference so that (a) < (b) if and only if a  b  where  denotes the preference
relation induced by the ranking . For simplicity of notation  we also let i denote the product ranked
at position i. Thus  12 ··· n is the list of the products written by increasing order of their ranks.
Finally  for any two integers i  j  let [i  j] denote the set {i  i + 1  . . .   j}.
Mallows model. The Mallows model is a member of the distance-based ranking family models
[21]. This model is described by a location parameter !  which denotes the central permutation  and
a scale parameter ✓ 2 R+  such that the probability of each permutation  is given by

() =

e✓·d( !)

 (✓)

 

where (✓) = P exp(✓ · d(  !)) is the normalization constant  and d(· ·) is the Kendall-
Tau metric of distance between permutations deﬁned as d(  !) = Pi<j 1l[((ai)  (aj)) ·
(!(ai)  !(aj)) < 0]. In other words  the d(  !) counts the number of pairwise disagreements
between the permutations  and !. It can be veriﬁed that d(· ·) is a distance function that is right-
invariant under the composition of the symmetric group  i.e.  d(⇡1 ⇡ 2) = d(⇡1⇡  ⇡2⇡) for every
⇡  ⇡1 ⇡ 2  where the composition ⇡ is deﬁned as ⇡(a) = (⇡(a)). This symmetry can be ex-
ploited to show that the normalization constant (✓) has a closed-form expression [18] given by
1ei·✓
1e✓ . Note that (✓) depends only on the scale parameter ✓ and does not depend on
the location parameter. Intuitively  the Mallows model deﬁnes a set of consumers whose preferences
are “similar”  in the sense of being centered around a common permutation  where the probability for

 (✓) =Qn

i=1

3

deviations thereof are decreasing exponentially. The similarity of consumer preferences is captured
by the Kendall-Tau distance metric.

Problem statement. We ﬁrst focus on efﬁciently computing the probability that a product a will
be chosen from an offer set S ✓ U under the Mallows model. When offered the subset S  the
customer is assumed to sample a preference list according to the Mallows model and then choose the
most preferred product from S according to the sampled list. Therefore  the probability of choosing
product a from the offer set S is given by

P(a|S) =X

() · 1l[  a  S] 

(1)

where 1l[  a  S] indicates whether (a) < (a0) for all a0 2 S  a0 6= a. Note that the above sum runs
over n! preference lists  meaning that it is a priori unclear if P(a|S) can be computed efﬁciently.
Once we are able to compute the choice probabilities  we consider the assortment optimization
problem. For that  we extend the product universe to include an additional product aq that represents
the outside (no-purchase) option and extend the Mallows model to n + 1 products. Each product a
has an exogenously ﬁxed price ra with the price rq of the outside option set to 0. Then  the goal is to
solve the following decision problem:

max

S✓UXa2S

P(a|S [{ rq}) · ra.

(2)

The above optimization problem is hard to approximate within O(n1✏) under a general choice
model [1].

3 Choice probabilities: closed-form expression

We now show that the choice probabilities can be computed efﬁciently under the Mallows model.
Without loss of generality  we assume from this point on that the products are indexed such that the
central permutation ! ranks product ai at position i  for all i 2 [n]. The next theorem shows that 
when the offer set is contiguous  the choice probabilities enjoy a rather simple form. Using these
expressions as building blocks  we derive a closed-form expressions for general offer sets.
Theorem 3.1 (Contiguous offer set) Suppose S = a[i j] = {ai  . . .   aj} for some 1  i  j  n.
Then  the probability of choosing product ak 2 S under the Mallows model with location parameter
! and scale parameter ✓ is given by

P(ak|S) =

e✓·(ki)

1 + e✓ + ··· + e✓·(ji) .

The proof of Theorem 3.1 is in Appendix A. The expression for the choice probability under a general
offer set is more involved. For that  we need the following additional notation. For a pair of integers
1  m  q  n  deﬁne
 (q  ✓) =

e✓·` and (q  m  ✓) = (m  ✓) · (q  m  ✓).

qYs=1

s1X`=0

In addition  for a collection of M discrete functions hm : Z ! R  m = 1  . . .   M such that hm(r) = 0
for any r < 0  their discrete convolution is deﬁned as

· (f0 ? ˜f1 ? ··· ? ˜f` ?f `+1 ? ··· ?f M )(|G|) 

4

(h1 ? ··· ?h m) (r) = Xr1 ... rM :
Pm rm=r

h1(r1)··· hM (rM ).

Theorem 3.2 (General offer set) Suppose S = a[i1 j1] [···[ a[iM  jM ] where im  jm for 1 
m  M and jm < im+1 for 1  m  M  1. Let Gm = a[jm im+1] for 1  m  M  1 
G = G1 [···[ GM  and C = a[i1 jM ]. Then  the probability of choosing ak 2 a[i` j`] can be written
as

P(ak|S) = e✓·(ki1) ·QM1

m=1 (|Gm|  ✓ )

 (|C|  ✓ )

where:

1

 (|Gm| r ✓)   if 0  r | Gm|  for 1  m  M.

• fm(r) = e✓·r·(jmi1+1+r/2) ·
• ˜fm(r) = e✓·r · fm(r)  for 1  m  M.
• f0(r) = (|C|  |G| r  ✓) ·
• fm(r) = 0  for 0  m  M and any r outside the ranges described above.

1+e✓+···+e✓·(|S|1+r)   for 0  r | G|.

e✓·(|G|r)2/2

Proof. At a high level  deriving the choice probability expression for a general offer set involves
breaking down the probabilistic event of choosing ak 2 S into simpler events for which we can use
the expression given in Theorem 3.1  and then combining these expressions using the symmetries of
the Mallows distribution.
For a given vector R = (r0  . . .   rM ) 2 RM +1 such that r0 + . . . rM = |G|  let h(R) be the set of
permutations which satisfy the following two conditions: i) among all the products of S  ak is the
most preferred  and ii) for all m 2 [M ]  there are exactly rm products from Gm which are preferred
to ak. We denote this subset of products by ˜Gm for all m 2 [M ]. This implies that there are r0
products from G which are less preferred than ak. With this notation  we can write

P(ak|S) = XR:r0+...rM =|G| X2h(R)

()  where recall that () =

e✓·Pi j ⇠( i j)

 (✓)

with ⇠(  i  j) = 1l[((ai)  (aj))· (!(ai)  !(aj)) < 0]. For all   we break down the sum in the
exponential as follows:Pi j ⇠(  i  j) = C1() + C2() + C3()  where C1() contains pairs of
products (i  j) such that ai 2 ˜Gm for some m 2 [M ] and aj 2 S  C2() contains pairs of products
(i  j) such that ai 2 ˜Gm for some m 2 [M ] and aj 2 Gm0\ ˜Gm0 for some m 6= m0  and C3()
contains the remaining pairs of products. For a ﬁxed R  we show that C1() and C2() are constant
for all  2 h(R).
Part 1. C1() counts the number of disagreements (i.e.  number of pairs of products that are oppositely
ranked in  and !) between some product in S and some product in ˜Gm for any m 2 [M ]. For all
m 2 [M ]  a product in ai 2 ˜Gm induces a disagreement with all product aj 2 S such that j < i.
Therefore  the sum of all these disagreements is equal to 

C1() =

MXm=1 Xaj2S

ai2 ˜Gm

⇠(  i  j) =

rm

MXm=1

mXj=1

|Sj| 

where Sm = a[im jm].
Part 2. C2() counts the number of disagreements between some product in any ˜Gm and some
product in any Gm0\ ˜Gm0 for m0 6= m. The sum of all these disagreements is equal to 

rm ·

m1Xj=1

(|Gj| rj)

r2
m.

1
2

MXm=1
· X2h(R)

e✓.C3().

Consequently  for all  2 h(R)  we can write d(  !) = C1(R) + C2(R) + C3() and therefore 

aj2Gm0\ ˜Gm0

C2() = Xm6=m0 Xai2 ˜Gm
m1Xj=1
m1Xj=1

MXm=2
MXm=2

rm ·

rm

=

=

|Gj|

|Gj|

1
2

⇠(  i  j) =

MXm=2
m1Xj=1
MXm=2
(|G| m0)2 +

rm ·

rj

P(ak|S) = XR:r0+···+rM =|G|

e✓·(C1(R)+C2(R))

 (✓)

5

Computing the inner sum requires a similar but more involved partitioning of the permutations as
well as using Theorem 3.1. The details are presented in Appendix B. In particular  we can show that

for a ﬁxed R P2h(R) e✓.C3() is equal to

e✓·(k1P`1
1 + ··· + e✓·(|S|+m01) ·

m=1 rm)

MYm=1

 (|G| m0 ✓ ) · (|S| + m0 ✓ ) ·
Putting all the pieces together yields the desired result.
Due to representing P(ak|S) as a discrete convolution  we can efﬁciently compute this probability
using fast Fourier transform in O(n2 log n) time [5]  which is a dramatic improvement over the
exponential sum (1) that deﬁnes the choice probabilities. Although Theorem 3.2 allows us to
compute the choice probabilities in polynomial time  it is not directly useful in solving the assortment
optimization problem under the Mallows model. To this end  we present an alternative (and slightly
less efﬁcient) method for computing the choice probabilities by means of dynamic programming.

 (rm ✓ ) · (|G|m  rm ✓ )

.

 (|Gm| ✓ )

4 Choice probabilities: a dynamic programming approach

In what follows  we present an alternative algorithm for computing the choice probabilities. Our
approach is based on an efﬁcient procedure to sample a random permutation according to a Mallows
distribution with location parameter ! and scale parameter ✓. The random permutation is constructed
sequentially using a repeated insertion method (RIM) as follows. For i = 1  . . .   n and s = 1  . . .   i 
insert ai at position s with probability pi s = e✓·(is)/(1 + e✓ + ··· + e✓·(i1)).
Lemma 4.1 (Theorem 3 in [15]) The random insertion method generates a random sample from a
Mallows distribution with location parameter ! and scale parameter ✓.
Based on the correctness of this procedure  we describe a dynamic program to compute the choice
probabilities of a general offer set S. The key idea is to decompose these probabilities to include
the position at which a product is chosen. In particular  for i  k and s 2 [k]  let ⇡(i  s  k) be the
probability that product ai is chosen (i.e.  ﬁrst among products in S) at position s after the k-th step
of the RIM. In other words  ⇡(i  s  k) corresponds to a choice probability when restricting U to the
ﬁrst k products  a1  . . .   ak. With this notation  we have for all i 2 [n]  P(ai|S) =
We compute ⇡(i  s  k) iteratively for k = 1  . . .   n. In particular  in order to compute ⇡(i  s  k + 1) 
we use the correctness of the sampling procedure. Speciﬁcally  starting from a permutation  that
includes the products a1  . . .   ak  the product ak+1 is inserted at position j with probability pk+1 j 
and we have two cases to consider.
Case 1: ak+1 /2 S
ak+1 /2 S.
In this case  ⇡(k + 1  s  k + 1) = 0 for all s = 1  . . .   k + 1. Consider a product
ak+1 /2 S
ai for i  k. In order for ai to be chosen at position s after ak+1 is inserted  one of the following
events has to occur: i) ai was already chosen at position s before ak+1 is inserted  and ak+1 is
inserted at a position `> s   or ii) ai was chosen at position s  1  and ak+1 is inserted at a position
`  s  1. Consequently  we have for all i  k

nPs=1

⇡(i  s  n).

⇡(i  s  k + 1) =

k+1X`=s+1

pk+1 ` · ⇡(i  s  k) +

pk+1 ` · ⇡(i  s  1  k)
= (1  k+1 s) · ⇡(i  s  k) + k+1 s1 · ⇡(i  s  1  k) 

s1X`=1

`=1 pk ` for all k  s.

where k s =Ps
Case 2 : ak+1 2 S
ak+1 2 S. Consider a product ai with i  k. This product is chosen at position s only
ak+1 2 S
if it was already chosen at position s and ak+1 is inserted at a position `> s . Therefore  for all
i  k  ⇡(i  s  k + 1) = (1  k+1 s) · ⇡(i  s  k). For product ak+1  it is chosen at position s only if
all products ai for i  k are at positions `  s and ak+1 is inserted at position s  implying that

⇡(k + 1  s  k + 1) = pk+1 s ·Xik

nX`=s

⇡(i  `  k).

Algorithm 1 summarizes this procedure.

6

Algorithm 1 Computing choice probabilities
1: Let S be a general offer set. Without loss of generality  we assume that a1 2 S.
2: Let ⇡(1  1  1) = 1.
3: For k = 1  . . .   n  1 

(a) For all i  k and s = 1  . . . k + 1  let

⇡(i  s  k + 1) = (1  k+1 s) · ⇡(i  s  k) + 1l[ak+1 /2 S] · k+1 s1 · ⇡(i  s  1  k).

(b) For s = 1  . . .   k + 1  let

⇡(k + 1  s  k + 1) = 1l[ak+1 2 S] · pk+1 s ·Xik

⇡(i  `  k).

nX`=s

4: For all i 2 [n]  return P(ai|S) =Pn

s=1 ⇡(i  s  n).

Theorem 4.2 For any offer set S  Algorithm 1 returns the choice probabilities under a Mallows
distribution with location parameter ! and scale parameter ✓.

This dynamic programming approach provides an O(n3) time algorithm for computing P(a|S) for
all products a 2 S simultaneously. Moreover  as explained in the next section  these ideas lead to an
algorithm to solve the assortment optimization problem.

5 Assortment optimization: integer programming formulation

In the assortment optimization problem  each product a has an exogenously ﬁxed price ra. Moreover 
there is an additional product aq that represents the outside option (no-purchase)  with price rq = 0
that is always included. The goal is to determine the subset of products that maximizes the expected
revenue  i.e.  solve (2). Building on Algorithm 1 and introducing a binary variable for each product 
we formulate 2 as an MIP with O(n3) variables and constraints  of which only n variables are binary.
We assume for simplicity that the ﬁrst product of S (say a1) is known. Since this product is generally
not known a-priori  in order to obtain an optimal solution to problem (2)  we need to guess the ﬁrst
offered product and solve the above integer program for each of the O(n) guesses. We note that the
MIP formulation is quite powerful and can handle a large class of constraints on the assortment (such
as cardinality and capacity constraints) and also extends to the case of the mixture of Mallows model.

Theorem 5.1 Conditional on a1 2 S  the optimal solution to 2 is given by S⇤ = {i 2 [n] : x⇤i = 1} 
where x⇤ 2{ 0  1}n is the optimal solution to the following MIP:

max

x ⇡ y z Xi s

ri · ⇡(i  s  n)

s.t.⇡ (1  1  1) = 1 ⇡ (1  s  1) = 0 

⇡(i  s  k + 1) = (1  wk+1 s) · ⇡(i  s  k) + yi s k+1 

⇡(k + 1  s  k + 1) = zs k+1 
yi s k  k+1 s1 · ⇡(i  s  1  k  1) 
0  yi s k  k+1 s1 · (1  xk) 
zs k  pk+1 s ·
0  zs k  pk+1 s · xk 
x1 = 1  xq = 1  xk 2{ 0  1}

⇡(i  `  k  1) 

k1Xi=1

nX`=s

8s = 2  . . .   n
8i  s 8k  2

8s 8k  2
8i  s 8k  2
8i  s 8k  2
8s 8k  2
8s 8k  2

We present the proof of correctness for this formulation in Appendix C.

7

6 Numerical experiments

In this section  we examine how the MIP performs in terms of the running time. We considered the
following simulation setup. Product prices are sampled independently and uniformly at random from
the interval [0  1]. The modal ranking is ﬁxed to the identity ranking with the outside option ranked
at the top. The outside option being ranked at the top is characteristic of applications in which the
retailer captures a small fraction of the market and the outside option represents the (much larger) rest
of the market. Because the outside option is always offered  we need to solve only a single instance
of the MIP (described in Theorem 5.1). Note that in the more general setting  the number of MIPs
that must be solved is equal the minimum of the rank of the outside option and the rank of the highest
revenue item2. Because the MIPs are independent of each other  they can be solved in parallel. We
solved the MIPs using the Gurobi Optimizer version 6.0.0 on a computer with processor 2.4GHz
Intel Core i5  RAM of 8GB  and operating system Mac OSX El Capitan.

Strengthening of the MIP formulation. We use structural properties of the optimal solution
to tighten some of the upper bounds involving the binary variables in the MIP formulation. In
particular  for all i  s  and m  we replace the constraint yi s m  m+1 s1 · (1  xm) with the
constraint yi s m  m+1 s1 · ui s m · (1  xm)  where ui s m is the probability that product ai
is selected at position (s  1) after the mth step of the RIM when the offer set is S = {ai⇤  aq} 
i.e. when only the highest priced product is offered. Since we know that the highest price product
is always offered in the optimal assortment  this is a valid upper bound to ⇡(i  s  1  m  1) and 
therefore  a valid strengthening of the constraint. Similarly  for all s and m  we replace the constraint 
zs m  ↵m+1 s · xm with the constraint zs m  ↵m+1 s · vs m · xm  where vs m is equal to the
probability that product that product i is selected at position ` = s  . . .   n when the offer set is
S = {aq} if ai w ai⇤  and S = {aq  ai⇤} otherwise. Again using the fact that the highest price
product is always offered in the optimal assortment  we can show that this is a valid upper bound.

Results and discussion. Table 1 shows the running time of the strengthened MIP formulation for
different values of e✓ and n. For each pair of parameters  we generated 50 different instances. We

e✓ = 0.8

e✓ = 0.9

e✓ = 0.8

n Average running time (s) Max running time (s)
e✓ = 0.9
5.80
10
28.79
15
189.93
20
25
1 817.98

4.60
19.04
48.08
143.21

4.72
21.30
105.30
769.78

5.64
27.08
58.09
183.78

Table 1: Running time of the strengthened MIP for various values of e✓ and n.

note that the strengthening improves the running time considerably. Under the initial formulation 
the MIP did not terminate after several hours for n = 25 whereas it was able to terminate in a few
minutes with the additional strengthening. Our MIP obtains the optimal solution in a reasonable
amount of time for the considered parameter values. Outside of this range  i.e. when e✓ is too small
or when n is too large  there are potential numerical instabilities. The strengthening we propose is
one way to improve the running time of the MIP but other numerical optimization techniques may be
applied to improve the running time even further. Finally  we emphasize that the MIP formulation
is necessary because of its ﬂexibility to handle versatile business constraints (such as cardinality or
capacity constraints) that naturally arise in practice.

Extensions and future work. Although the entire development was focused on a single Mallows
model  our results extend to a ﬁnite mixture of Mallows model. Speciﬁcally  for a Mallows model
with T mixture components  we can compute the choice probability by setting ⇡(i  s  n) =PT
t=1 ↵t ·
⇡t(i  s  n)  where ⇡(i  s  n) is the probability term deﬁned in Section 4  ⇡t(· · ·) is the probability for
mixture component t  and ↵t > 0 are the mixture weights. We then have P(a|S) =Pn
s=1 ⇡(i  s  n)
for the mixture model. Correspondingly  the MIP in Section 5 also naturally extends. The natural
next step is to develop special purpose algorithm to solve the MIP that exploit the structure of the
Mallows distributions allowing to scale to large values of n.

2It can be shown that the highest revenue item is always part of the optimal subset.

8

References
[1] A. Aouad  V. Farias  R. Levi  and D. Segev. The approximability of assortment optimization under ranking

preferences. Available at SSRN: http://ssrn.com/abstract=2612947  2015.

[2] Jose H Blanchet  Guillermo Gallego  and Vineet Goyal. A markov chain approximation to choice modeling.

In EC  pages 103–104  2013.

[3] G. Brightwell and P. Winkler. Counting linear extensions is #P-complete. In STOC ’91 Proceedings of the

twenty-third annual ACM Symposium on Theory of Computing  pages 175–181  1991.

[4] Juan José Miranda Bront  Isabel Méndez-Díaz  and Gustavo Vulcano. A column generation algorithm for

choice-based network revenue management. Operations Research  57(3):769–784  2009.

[5] J. Cooley and J. Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics

of Computation  19(90):297–301  1965.

[6] James M Davis  Guillermo Gallego  and Huseyin Topaloglu. Assortment optimization under variants of

the nested logit model. Operations Research  62(2):250–273  2014.

[7] J. Doignon  A. Pekeˇc  and M. Regenwetter. The repeated insertion model for rankings: Missing link

between two subset choice models. Psychometrika  69(1):33–54  2004.

[8] C. Dwork  R. Kumar  M. Naor  and D. Sivakumar. Rank aggregation methods for the web. In ACM  editor 

Proceedings of the 10th international conference on World Wide Web  pages 613–622  2001.

[9] V. Farias  S. Jagabathula  and D. Shah. A nonparametric approach to modeling choice with limited data.

Management Science  59(2):305–322  2013.

[10] Guillermo Gallego and Huseyin Topaloglu. Constrained assortment optimization for the nested logit model.

Management Science  60(10):2583–2601  2014.

[11] John Guiver and Edward Snelson. Bayesian inference for plackett-luce ranking models. In proceedings of

the 26th annual international conference on machine learning  pages 377–384. ACM  2009.

[12] D. Honhon  S. Jonnalagedda  and X. Pan. Optimal algorithms for assortment selection under ranking-based

consumer choice models. Manufacturing & Service Operations Management  14(2):279–289  2012.

[13] S. Jagabathula. Assortment optimization under general choice. Available at SSRN 2512831  2014.

[14] G. Lebanon and Y. Mao. Non-parametric modeling of partially ranked data. Journal of Machine Learning

Research  9:2401–2429  2008.

[15] T. Lu and C. Boutilier. Learning mallows models with pairwise preferences. In Proceedings of the 28th

International Conference on Machine Learning  pages 145–152  2011.

[16] R.D. Luce. Individual Choice Behavior: A Theoretical Analysis. Wiley  1959.

[17] C. Mallows. Non-null ranking models. Biometrika  44(1):114–130  1957.

[18] J. Marden. Analyzing and Modeling Rank Data. Chapman and Hall  1995.

[19] D. McFadden. Modeling the choice of residential location. Transportation Research Record  (672):72–77 

1978.

[20] M. Meila  K. Phadnis  A. Patterson  and J. Bilmes. Consensus ranking under the exponential model. arXiv

preprint arXiv:1206.5265  2012.

[21] T. Murphy and D. Martin. Mixtures of distance-based models for ranking data. Computational Statistics &

Data Analysis  41(3):645–655  2003.

[22] R.L. Plackett. The analysis of permutations. Applied Statistics  24(2):193–202  1975.

[23] K. Talluri and G. Van Ryzin. Revenue management under a general discrete choice model of consumer

behavior. Management Science  50(1):15–33  2004.

[24] G. van Ryzin and G. Vulcano. A market discovery algorithm to estimate a general class of nonparametric

choice models. Management Science  61(2):281–300  2014.

[25] John I Yellott. The relationship between luce’s choice axiom  thurstone’s theory of comparative judgment 

and the double exponential distribution. Journal of Mathematical Psychology  15(2):109–144  1977.

9

,Antoine Desir
Vineet Goyal
Srikanth Jagabathula
Danny Segev
Vincent LE GUEN
Nicolas THOME