2008,On the Generalization Ability of Online Strongly Convex Programming Algorithms,This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound  that holds with high probability  on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. The bound also solves an open problem regarding the convergence rate of {\pegasos}  a recently proposed method for solving the SVM optimization problem.,On the Generalization Ability of

Online Strongly Convex Programming Algorithms

Sham M. Kakade

TTI Chicago

Chicago  IL 60637
sham@tti-c.org

Ambuj Tewari
TTI Chicago

Chicago  IL 60637

tewari@tti-c.org

Abstract

This paper examines the generalization properties of online convex programming
algorithms when the loss function is Lipschitz and strongly convex. Our main
result is a sharp bound  that holds with high probability  on the excess risk of the
output of an online algorithm in terms of the average regret. This allows one to
use recent algorithms with logarithmic cumulative regret guarantees to achieve
fast convergence rates for the excess risk with high probability. As a corollary  we
characterize the convergence rate of PEGASOS (with high probability)  a recently
proposed method for solving the SVM optimization problem.

1 Introduction

Online regret minimizing algorithms provide some of the most successful algorithms for many ma-
chine learning problems  both in terms of the speed of optimization and the quality of generalization.
Notable examples include efﬁcient learning algorithms for structured prediction [Collins  2002] (an
algorithm now widely used) and for ranking problems [Crammer et al.  2006] (providing competitive
results with a fast implementation).

Online convex optimization is a sequential paradigm in which at each round  the learner predicts a
vector wt ∈ S ⊂ Rn  nature responds with a convex loss function  `t  and the learner suffers loss
`t(wt). In this setting  the goal of the learner is to minimize the regret:

TXt=1

`t(wt) − min
w∈S

`t(w)

TXt=1

which is the difference between his cumulative loss and the cumulative loss of the optimal ﬁxed
vector.

Typically  these algorithms are used to train a learning algorithm incrementally  by sequentially
feeding the algorithm a data sequence  (X1  Y1)  . . .   (XT   YT ) (generated in an i.i.d. manner). In
essence  the loss function used in the above paradigm at time t is `(w; (Xt  Yt))  and this leads to a
guaranteed bound on the regret:

RegT =

`(wt; (Xt  Yt)) − min
w∈S

`(w; (Xt  Yt))

TXt=1

TXt=1

alization ability  i.e. we would like:

However  in the batch setting  we are typically interested in ﬁnding a parameter bw with good gener-

R(w)

to be small  where R(w) := E [`(w; (X  Y ))] is the risk.

R(bw) − min

w∈S

Intuitively  it seems plausible that low regret on an i.i.d. sequence  should imply good generaliza-
tion performance. In fact  for most of the empirically successful online algorithms  we have a set of
techniques to understand the generalization performance of these algorithms on new data via ‘online
to batch’ conversions — the conversions relate the regret of the algorithm (on past data) to the gen-
eralization performance (on future data). These include cases which are tailored to general convex
functions [Cesa-Bianchi et al.  2004] (whose regret is O(√T )) and mistake bound settings [Cesa-
Bianchi and Gentile  2008] (where the the regret could be O(1) under separability assumptions).

algorithm.

In these conversions  we typically choose bw to be the average of the wt produced by our online

Recently  there has been a growing body of work providing online algorithms for strongly convex
loss functions (i.e. `t is strongly convex)  with regret guarantees that are merely O(ln T ). Such
algorithms have the potential to be highly applicable since many machine learning optimization
problems are in fact strongly convex — either with strongly convex loss functions (e.g. log loss 
square loss) or  indirectly  via strongly convex regularizers (e.g. L2 or KL based regularization).
Note that in the latter case  the loss function itself may only be just convex but a strongly convex reg-
ularizer effectively makes this a strongly convex optimization problem; e.g. the SVM optimization
problem uses the hinge loss with L2 regularization. In fact  for this case  the PEGASOS algorithm
of Shalev-Shwartz et al. [2007] — based on the online strongly convex programming algorithm of
Hazan et al. [2006] — is a state-of-the-art SVM solver. Also  Ratliff et al. [2007] provide a similar
subgradient method for max-margin based structured prediction  which also has favorable empirical
performance.

The aim of this paper is to examine the generalization properties of online convex programming
algorithms when the loss function is strongly convex (where strong convexity can be deﬁned in a
general sense  with respect to some arbitrary norm || · ||). Suppose we have an online algorithm
which has some guaranteed cumulative regret bound RegT (e.g. say RegT ≤ ln T with T samples).
Then a corollary of our main result shows that with probability greater than 1 − δ ln T   we obtain a

parameter bw from our online algorithm such that:
+ O

R(bw) − min

R(w) ≤

RegT

T

w

qRegT ln 1

δ

T

+

ln 1
δ

T  .

Here  the constants hidden in the O-notation are determined by the Lipschitz constant and the strong
convexity parameter of the loss `. Importantly  note that the correction term is of lower order than
the regret — if the regret is ln T then the additional penalty is O(
). If one naively uses the
Hoeffding-Azuma methods in Cesa-Bianchi et al. [2004]  one would obtain a signiﬁcantly worse
penalty of O(1/√T ).
This result solves an open problem in Shalev-Shwartz et al. [2007]  which was on characterizing the
convergence rate of the PEGASOS algorithm  with high probability. PEGASOS is an online strongly
convex programming algorithm for the SVM objective function — it repeatedly (and randomly)
subsamples the training set in order to minimize the empirical SVM objective function. A corollary
to this work essentially shows the convergence rate of PEGASOS (as a randomized optimization
algorithm) is concentrated rather sharply.

√ln T

T

Ratliff et al. [2007] also provide an online algorithm (based on Hazan et al. [2006]) for max-margin
based structured prediction. Our results are also directly applicable in providing a sharper concen-
tration result in their setting (In particular  see the regret bound in Equation 15  for which our results
can be applied to).

This paper continues the line of research initiated by several researchers [Littlestone  1989  Cesa-
Bianchi et al.  2004  Zhang  2005  Cesa-Bianchi and Gentile  2008] which looks at how to convert
online algorithms into batch algorithms with provable guarantees. Cesa-Bianchi and Gentile [2008]
prove faster rates in the case when the cumulative loss of the online algorithm is small. Here 
we are interested in the case where the cumulative regret is small. The work of Zhang [2005] is
closest to ours. Zhang [2005] explicitly goes via the exponential moment method to derive sharper
concentration results. In particular  for the regression problem with squared loss  Zhang [2005] gives
a result similar to ours (see Theorem 8 therein). The present work can also be seen as generalizing
his result to the case where we have strong convexity with respect to a general norm. Coupled with

recent advances in low regret algorithms in this setting  we are able to provide a result that holds
more generally.

Our key technical tool is a probabilistic inequality due to Freedman [Freedman  1975]. This  com-
bined with a variance bound (Lemma 1) that follows from our assumptions about the loss function 
allows us to derive our main result (Theorem 2). We then apply it to statistical learning with bounded
loss  and to PEGASOS in Section 4.

2 Setting

Fix a compact convex subset S of some space equipped with a norm k·k. Let k·k∗ be the dual norm
deﬁned by kvk∗ := supw : kwk≤1
v · w. Let Z be a random variable taking values in some space
Z. Our goal is to minimize F (w) := E [f (w; Z)] over w ∈ S. Here  f : S × Z → [0  B] is some
function satisfying the following assumption.

Assumption LIST.
fz(w) = f (w; z) is convex in w and satisﬁes:

(LIpschitz and STrongly convex assumption) For all z ∈ Z  the function

1. fz has Lipschitz constant L w.r.t. to the norm k·k  i.e. ∀w ∈ S  ∀λ ∈ ∂fz(w) (∂fz denotes
the subdifferential of fz)  kλk∗ ≤ L. Note that this assumption implies ∀w  w0 ∈ S 
|fz(w) − fz(w0)| ≤ Lkw − w0k.

2. fz is ν-strongly convex w.r.t. k · k  i.e. ∀θ ∈ [0  1]  ∀w  w0 ∈ S 
ν
2

fz(θw + (1 − θ)w0) ≤ θfz(w) + (1 − θ)fz(w0) −

θ(1 − θ)kw − w0k2 .

Denote the minimizer of F by w?  w? := arg minw∈SF (w). We consider an online setting in
which independent (but not necessarily identically distributed) random variables Z1  . . .   ZT be-
come available to us in that order. These have the property that

∀t ∀w ∈ S  E [f (w; Zt)] = F (w) .

Now consider an algorithm that starts out with some w1 and at time t  having seen Zt  updates the
parameter wt to wt+1. Let Et−1 [·] denote conditional expectation w.r.t. Z1  . . .   Zt−1. Note that
wt is measurable w.r.t. Z1  . . .   Zt−1 and hence Et−1 [f (wt; Zt)] = F (wt).
Deﬁne the statistics 

RegT :=

Diﬀ T :=

f (wt; Zt) − min
w∈S

TXt=1
(F (wt) − F (w?)) =

TXt=1
TXt=1

f (w; Zt)  

TXt=1

F (wt) − T F (w?) .

Deﬁne the sequence of random variables

ξt := F (wt) − F (w?) − (f (wt; Zt) − f (w?; Zt)) .

(1)
Since Et−1 [f (wt; Zt)] = F (wt) and Et−1 [f (w?; Zt)] = F (w?)  ξt is a martingale difference
sequence. This deﬁnition needs some explanation as it is important to look at the right martingale
T Pt f (wt; Zt)
difference sequence to derive the results we want. Even under assumption LIST  1
T Pt F (wt) and F (w?) respectively at a
and 1
rate better then O(1/√T ) in general. But if we look at the difference  we are able to get sharper
concentration.

T Pt f (w?; Zt) will not be concentrated around 1

3 A General Online to Batch Conversion

The following simple lemma is crucial for us. It says that under assumption LIST  the variance
of the increment in the regret f (wt; Zt) − f (w?; Zt) is bounded by its (conditional) expectation
F (wt) − F (w?). Such a control on the variance is often the main ingredient in obtaining sharper
concentration results.

Lemma 1. Suppose assumption LIST holds and let ξt be the martingale difference sequence deﬁned
in (1). Let

be the conditional variance of ξt given Z1  . . .   Zt−1. Then  under assumption LIST  we have 

t(cid:3)
Vart−1ξt := Et−1(cid:2)ξ2

Vart−1ξt ≤

4L2
ν

(F (wt) − F (w?)) .

The variance bound given by the above lemma allows us to prove our main theorem.
Theorem 2. Under assumption LIST  we have  with probability at least 1 − 4 ln(T )δ 

1
T

TXt=1

F (wt) − F (w?) ≤

RegT

T

Further  using Jensen’s inequality  1

3.1 Proofs

Proof of Lemma 1. We have 

pRegT

T

+ 4r L2 ln(1/δ)
  6B(cid:27) ln(1/δ)
T Pt F (wt) can be replaced by F ( ¯w) where ¯w := 1
T Pt

+ max(cid:26) 16L2

T

ν

ν

wt.

[ Assumption LIST  part 1 ]

(2)

(3)

On the other hand  using part 2 of assumption LIST  we also have for any w  w0 ∈ S 

f (w; Z) + f (w0; Z)

Vart−1ξt ≤ Et−1h(f (wt; Zt) − f (w?; Zt))2i

ν
8kw − w0k2 .

2

≤ Et−1(cid:2)L2kwt − w?k2(cid:3)
= L2kwt − w?k2 .
≥ f(cid:18) w + w0
; Z(cid:19) +
2 (cid:19) +
≥ F(cid:18) w + w0
≥ F(cid:18) wt + w?
2
≥ F (w?) +

ν
8kw − w0k2 .
(cid:19) +
ν
8kwt − w?k2 .

ν
8kwt − w?k2

Taking expectation this gives  for any w  w0 ∈ S 

F (w) + F (w0)

Now using this with w = wt  w0 = w?  we get

2

2

F (wt) + F (w?)

2

[∵ w? minimizes F ]

This implies that

Combining (2) and (3) we get 

kwt − w?k2 ≤

4(F (wt) − F (w?))

ν

Vart−1ξt ≤

4L2
ν

(F (wt) − F (w?))

The proof of Theorem 2 relies on the following inequality for martingales which is an easy conse-
quence of Freedman’s inequality [Freedman  1975  Theorem 1.6]. The proof of this lemma can be
found in the appendix.
Lemma 3. Suppose X1  . . .   XT is a martingale difference sequence with |Xt| ≤ b. Let
Let V =PT
have  for any δ < 1/e and T ≥ 3 

t=1 VartXt be the sum of conditional variances of Xt’s. Further  let σ = √V . Then we

VartXt = Var (Xt | X1  . . .   Xt−1) .

Prob  TXt=1

Xt > maxn2σ  3bpln(1/δ)opln(1/δ)! ≤ 4 ln(T )δ .

Proof of Theorem 2. By Lemma 1  we have σ :=qPT

ν Diﬀ T . Note that |ξt| ≤
2B because our f has range [0  B]. Therefore  Lemma 3 gives us that with probability at least
1 − 4 ln(T )δ  we have

t=1 Vartξt ≤q 4L2
ξt ≤ maxn2σ  6Bpln(1/δ)opln(1/δ) .

TXt=1

By deﬁnition of RegT  

Diﬀ T − RegT ≤
and therefore  with probability  1 − 4 ln(T )δ  we have

Diﬀ T − RegT ≤ max(4r L2

ν

ξt

TXt=1

Using Lemma 4 below to solve the above quadratic inequality for Diﬀ T   gives

PT

t=1 F (wt)

T

− F (w?) ≤

RegT

T

Diﬀ T   6Bpln(1/δ))pln(1/δ) .
+ max(cid:26) 16L2

pRegT

T

ν

+ 4r L2 ln(1/δ)

ν

  6B(cid:27) ln(1/δ)

T

The following elementary lemma was required to solve a recursive inequality in the proof of the
above theorem. Its proof can be found in the appendix.
Lemma 4. Suppose s  r  d  b  ∆ ≥ 0 and we have

s − r ≤ max{4√ds  6b∆}∆ .

s ≤ r + 4√dr∆ + max{16d  6b}∆2 .

Then  it follows that

4 Applications

4.1 Online to Batch Conversion for Learning with Bounded Loss

Suppose (X1  Y1)  . . .   (XT   YT ) are drawn i.i.d. from a distribution. The pairs (Xi  Yi) belong
to X × Y and our algorithm are allowed to make predictions in a space D ⊇ Y. A loss function
` : D × Y → [0  1] measures quality of predictions. Fix a convex set S of some normed space and a
function h : X × S → D. Let our hypotheses class be {x 7→ h(x; w)| w ∈ S}.
On input x  the hypothesis parameterized by w predicts h(x; w) and incurs loss `(h(x; w)  y) if the
correct prediction is y. The risk of w is deﬁned by

R(w) := E [`(h(X; w)  Y )]

and let w? := arg minw∈S R(w) denote the (parameter for) the hypothesis with minimum risk. It
is easy to see that this setting falls under the general framework given above by thinking of the pair
(X  Y ) as Z and setting f (w; Z) = f (w; (X  Y )) to be `(h(X; w)  Y ). Note that F (w) becomes
the risk R(w). The range of f is [0  1] by our assumption about the loss functions so B = 1.
Suppose we run an online algorithm on our data that generates a sequence of hypotheses w0  . . .   wT
such that wt is measurable w.r.t. X<t  Y<t. Deﬁne the statistics 

RegT :=

`(h(Xt; wt)  Yt) − min
w∈S

TXt=1
TXt=1
At the end  we output ¯w := (PT
Theorem 2. It bounds the excess risk R( ¯w) − R(w?).

(R(wt) − R(w?)) =

Diﬀ T :=

t=1

`(h(Xt; w)  Yt)  

TXt=1
TXt=1
R(wt) − T R(w?) .

wt)/T . The following corollary then follows immediately from

Corollary 5. Suppose assumption LIST is satisﬁed for f (w; (x  y)) := `(h(x; w)  y). Then we
have  with probability at least 1 − 4 ln(T )δ 

  6(cid:27) ln(1/δ)
Recently  it has been proved [Kakade and Shalev-Shwartz  2008] that if assumption LIST is satisﬁed
for w 7→ `(h(x; w)  y) then there is an online algorithm that generates w1  . . .   wT such that

+ max(cid:26) 16L2

+ 4r L2 ln(1/δ)

R( ¯w) − R(w?) ≤

pRegT

T

RegT

T

T

ν

ν

RegT ≤

L2(1 + ln T )

2ν

.

Plugging it in the corollary above gives the following result.
Corollary 6. Suppose assumption LIST is satisﬁed for f (w; (x  y)) := `(h(x; w)  y). Then there is
an online algorithm that generates w1  . . .   wT and in the end outputs ¯w such that  with probability
at least 1 − 4 ln(T )δ 

R( ¯w) − R(w?) ≤

L2 ln T

νT

+

4L2√ln T

νT sln(cid:18) 1

δ(cid:19) + max(cid:26) 16L2

ν

  6(cid:27) ln(1/δ)

T

 

for any T ≥ 3.
4.2 High Probability Bound for PEGASOS

PEGASOS [Shalev-Shwartz et al.  2007] is a recently proposed method for solving the primal SVM
problem. Recall that in the SVM optimization problem we are given m example  label pairs
(xi  yi) ∈ Rd × {±1}. Assume that kxik ≤ R for all i where k · k is the standard L2 norm.
Let

F (w) =

λ
2kwk2 +

1
m

mXi=1

`(w; (xi  yi))

(4)

be the SVM objective function. The loss function `(w; (x  y)) = [1 − y(w · x)]+ is the hinge loss.
At time t  PEGASOS takes a (random) approximation

f (w; Zt) =

λ
2kwk2 +

1

k X(x y)∈Zt

`(w; (x  y))  

of the SVM objective function to estimate the gradient and updates the current weight vector wt to
wt+1. Here Zt is a random subset of the data set of size k. Note that F (w) can be written as

F (w) = E(cid:20) λ2

2 kwk2 + `(w; Z)(cid:21)

where Z is an example (xi  yi) drawn uniformly at random from the m data points. It is also easy
to verify that

It can be shown that w? := arg min F (w) will satisfy kw?k ≤ 1/√λ so we set

∀w  E [f (w; Zt)] = F (w) .

For any z that is a subset of the data set  the function

1

S =(cid:26)w ∈ Rd : kwk ≤
√λ(cid:27) .
|z| X(x y)∈z

λ
2kwk2 +

1

w 7→ f (w; z) =

`(w; (x  y))

is Lipschitz on S with Lipschitz constant L = √λ + R and is λ-strongly convex. Also f (w; z) ∈
[0  3/2 + R/√λ]. So  the PEGASOS setting falls under our general framework and satisﬁes assump-
tion LIST.

Theorem 1 in Shalev-Shwartz et al. [2007] says  for any w  T ≥ 3 

where L = √λ + R. It was noted in that paper that plugging in w = w? and taking expectations 
we easily get

 

(5)

f (wt; Zt) ≤

TXt=1
EZ1 ... ZT" TXt=1

L2 ln T

λ

f (w; Zt) +

TXt=1
F (wt)# ≤ T F (w?) +

L2 ln T

λ

.

Here we use Theorem 2 to prove an inequality that holds with high probability  not just in expecta-
tion.
Corollary 7. Let F be the SVM objective function deﬁned in (4) and w1  . . .   wT be the sequence
of weight vectors generated by the PEGASOS algorithm. Further  let w? denote the minimizer of the
SVM objective. Then  with probability 1 − 4δ ln(T )  we have

F (wt)−T F (w?) ≤

δ(cid:19)   (6)
for any T ≥ 3. Therefore  assuming R = 1  we have  for λ small enough  with probability at least
1 − δ 

δ(cid:19)+max(cid:26) 16L2

√λ(cid:27) ln(cid:18) 1

λ sln(cid:18) 1

4L2√ln T

TXt=1

6R

  9 +

λ

L2 ln T

+

λ

F (wt) − F (w?) = O  ln T

λT ! .

δ

1
T

TXt=1

Proof. Note that (5) implies that RegT ≤ L2 ln T
Theorem 2 by plugging in ν = λ and B = 3/2 + R/√λ.

λ

. The corollary then follows immediately from

References
N. Cesa-Bianchi and C. Gentile.

Information Theory  54(1):286–390  2008.

Improved risk tail bounds for on-line algorithms.

IEEE Transactions on

N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning algorithms.

IEEE Transactions on Information Theory  50(9):2050–2057  September 2004.

M. Collins. Discriminative training methods for hidden Markov models: Theory and experiments with percep-

tron algorithms. In Conference on Empirical Methods in Natural Language Processing  2002.

K. Crammer  O. Dekel  J. Keshet  S. Shalev-Shwartz  and Y. Singer. Online passive aggressive algorithms.

Journal of Machine Learning Research  7:551–585  Mar 2006.

David A. Freedman. On tail probabilities for martingales. The Annals of Probability  3(1):100–118  Feb 1975.
E. Hazan  A. Kalai  S. Kale  and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In

Proceedings of the Nineteenth Annual Conference on Computational Learning Theory  2006.

S. Kakade and S. Shalev-Shwartz. Mind the duality gap: Logarithmic regret algorithms for online optimization.

Advances in Neural Information Processing Systems  2008.

N. Littlestone. Mistake bounds and logarithmic linear-threshold learning algorithms. PhD thesis  U. C. Santa

Cruz  March 1989.

Nathan Ratliff  James (Drew) Bagnell  and Martin Zinkevich.

(online) subgradient methods for structured
prediction. In Eleventh International Conference on Artiﬁcial Intelligence and Statistics (AIStats)  March
2007.

Shai Shalev-Shwartz  Yoram Singer  and Nathan Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for
SVM. In Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML)  pages
807–814  2007.

T. Zhang. Data dependent concentration bounds for sequential prediction algorithms. In Proceedings of the

Eighteenth Annual Conference on Computational Learning Theory  pages 173–187  2005.

& α2

& αj−1 < σ ≤ αj

j−1 < V ≤ α2

Prob Xt
Xt > c max{rσ  α0}pln(1/δ)!
(cid:19)
Prob(cid:18)Pt Xt > c max{rσ  α0}pln(1/δ)
lXj=0
Prob(cid:18)Pt Xt > cαjpln(1/δ)
j (cid:19)
lXj=0
Prob Xt
lXj=0
exp
lXj=0
exp
lXj=0

j!
Xt > cαjpln(1/δ) & V ≤ α2
3(cid:16)cαjpln(1/δ)(cid:17) b
3(cid:16)cpln(1/δ)(cid:17) b

−c2αj ln(1/δ)

j ln(1/δ)

2αj + 2

−c2α2

j + 2

(?)
≤

2α2

=

≤

≤

=

where the inequality (?) follows from Freedman’s inequality. If we now choose α0 = bcpln(1/δ)
then αj ≥ bcpln(1/δ) for all j and hence every term in the above summation is bounded by
exp(cid:16) −c2 ln(1/δ)
2+2/3 (cid:17) which is less then δ if we choose c = 5/3. Set r = 2/c = 6/5. We want
α0rl ≥ b√T . Since cpln(1/δ) ≥ 1  choosing l = logr(√T ) ensures that. Thus we have
Prob  TXt=1
= Prob Xt
≤ (l + 1)δ = (log6/5(√T ) + 1)δ
≤ (6 ln(√T ) + 1)δ ≤ 4 ln(T )δ .

Xt > c max{rσ  α0}pln(1/δ)!

bpln(1/δ)}pln(1/δ)!

(∵ T ≥ 3)

max{

Xt >

5
3

6
5

5
3

σ 

In the second case  we have

which means that √s should be smaller than the larger root of the above quadratic. This gives us 

Appendix

Proof of Lemma 3. Note that a crude upper bound on VartXt is b2. Thus  σ ≤ b√T . We choose a
discretization 0 = α−1 < α0 < . . . < αl such that αi+1 = rαi for i ≥ 0 and αl ≥ b√T . We will

specify the choice of α0 and r shortly. We then have  for any c > 0 

Proof of Lemma 4. The assumption of the lemma implies that one of the following inequalities
holds:

s − r ≤ 4√ds∆ .

− (4√d∆)√s − r ≤ 0

s − r ≤ 6b∆2
(cid:0)√s(cid:1)2
s = (√s)2 ≤(cid:16)2√d∆ +p4d∆2 + r(cid:17)2
≤ 4d∆2 + 4d∆2 + r + 4p4d2∆4 + d∆2r
≤ 8d∆2 + r + 8d∆2 + 4√dr∆
≤ r + 4√dr∆ + 16d∆2 .

[∵ √x + y ≤ √x + √y]

(7)

(8)

Combining (7) and (8) ﬁnishes the proof.

,Garrett Bernstein
Daniel Sheldon