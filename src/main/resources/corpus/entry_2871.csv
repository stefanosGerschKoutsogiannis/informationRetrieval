2018,Learning Bounds for Greedy Approximation with Explicit Feature Maps from Multiple Kernels,Nonlinear kernels can be approximated using finite-dimensional feature maps for efficient risk minimization. Due to the inherent trade-off between the dimension of the (mapped) feature space and the approximation accuracy  the key problem is to identify promising (explicit) features leading to a satisfactory out-of-sample performance. In this work  we tackle this problem by efficiently choosing such features from multiple kernels in a greedy fashion. Our method sequentially selects these explicit features from a set of candidate features using a correlation metric. We establish an out-of-sample error bound capturing the trade-off between the error in terms of explicit features (approximation error) and the error due to spectral properties of the best model in the Hilbert space associated to the combined kernel (spectral error). The result verifies that when the (best) underlying data model is sparse enough  i.e.  the spectral error is negligible  one can control the test error with a small number of explicit features  that can scale poly-logarithmically with data. Our empirical results show that given a fixed number of explicit features  the method can achieve a lower test error with a smaller time cost  compared to the state-of-the-art in data-dependent random features.,Learning Bounds for Greedy Approximation with

Explicit Feature Maps from Multiple Kernels

Shahin Shahrampour

Department of Industrial and Systems Engineering

Texas A&M University

College Station  TX 77843

shahin@tamu.edu

Department of Electrical and Computer Engineering

Vahid Tarokh

Duke University

Durham  NC 27708

vahid.tarokh@duke.edu

Abstract

Nonlinear kernels can be approximated using ﬁnite-dimensional feature maps for
efﬁcient risk minimization. Due to the inherent trade-off between the dimension
of the (mapped) feature space and the approximation accuracy  the key problem
is to identify promising (explicit) features leading to a satisfactory out-of-sample
performance. In this work  we tackle this problem by efﬁciently choosing such
features from multiple kernels in a greedy fashion. Our method sequentially selects
these explicit features from a set of candidate features using a correlation metric.
We establish an out-of-sample error bound capturing the trade-off between the error
in terms of explicit features (approximation error) and the error due to spectral
properties of the best model in the Hilbert space associated to the combined kernel
(spectral error). The result veriﬁes that when the (best) underlying data model is
sparse enough  i.e.  the spectral error is negligible  one can control the test error
with a small number of explicit features  that can scale poly-logarithmically with
data. Our empirical results show that given a ﬁxed number of explicit features  the
method can achieve a lower test error with a smaller time cost  compared to the
state-of-the-art in data-dependent random features.

1

Introduction

Kernel methods are powerful tools in describing the nonlinear representation of data. Mapping the
inputs to a high-dimensional feature space  kernel methods compute their inner products without
recourse to the explicit form of the feature map (kernel trick). However  unfortunately  calculating
the kernel matrix for the training stage requires a prohibitive computational cost scaling quadratically
with data. To address this shortcoming  recent years have witnessed an intense interest on the
approximation of kernels using low-rank surrogates [1  2  3]. Such techniques can turn the kernel
formulation to a linear problem  which is potentially solvable in a linear time with respect to data
(see e.g. [4] for linear Support Vector Machines (SVM)) and thus applicable to large data sets. In
the approximation of kernels via their corresponding ﬁnite-dimensional feature maps  regardless of
whether the approximation is deterministic [5] or random [3]  it is extremely critical that – we can
compute the feature maps efﬁciently – and – we can (hopefully) represent the data in a sparse fashion.
The challenge is that ﬁnding feature maps with these characteristics is generally hard.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

It is well-known that any Mercer kernel can be represented as an (potentially inﬁnite-dimensional)
inner-product of its feature maps  and thus  it can be approximated with an inner product in a lower
dimension. As an example  the explicit feature map (also called Taylor feature map) of the Gaussian
kernel is derived in [6] via Taylor expansion. In supervised learning  the key problem is to identify
the explicit features 1 that lead to low out-of-sample error as there is an inherent trade-off between
the computational complexity and the approximation accuracy. This will turn the learning problem at
hand into an optimization with sparsity constraints  which is is generally NP-hard.
In this paper  our objective is to present a method for efﬁciently “choosing” explicit features associated
to a number of base positive semi-deﬁnite kernels. Motivated by the success of greedy methods in
sparse approximation [7  8]  we propose a method to select promising features from multiple kernels
in a greedy fashion. Our method  dubbed Multi Feature Greedy Approximation (MFGA)  has access
to a set of candidate features. Exploring these features sequentially  the algorithm maintains an active
set and adds one explicit feature to it per step. The selection criterion is according to the correlation
of the gradient of the empirical risk with the standard bases.
We provide non-asymptotic guarantees for MFGA  characterizing its out-of-sample performance via
three types of errors  one of which (spectral error) relates to spectral properties of the best model
in the Hilbert space associated to the combined kernel. Our theoretical result suggests that if the
underlying data model is sparse enough  i.e.  the spectral error is negligible  one can achieve a low
out-of-sample error with a small number of features  that can scale poly-logarithmically with data.
Recent ﬁndings in [9] shows that in approximating square integrable functions with smooth radial
kernels  the coefﬁcient decay is nearly exponential (small spectral error). In light of these results  our
method has potential in constructing sparse representations for a rich class of functions.
We further provide empirical evidence (Section 5) that explicit feature maps can be efﬁcient tools
for sparse representation. In particular  compared to the state-of-the-art in data-dependent random
features  MFGA requires a smaller number of features to achieve a certain test error on a number of
datasets  while spending less computational resource. Our work is related to several lines of research
in the literature  namely random and deterministic kernel approximation  sparse approximation  and
multiple kernel learning. Due to variety of these works  we postpone the detailed discussion of the
related literature to Section 4  after presenting the preliminaries  formulation  and results.

2 Problem Formulation

Preliminaries: Throughout the paper  the vectors are all in column format. We denote by [N ] the
set of positive integers {1  . . .   N}  by hx  x0i the inner product of vectors x and x0 (in potentially
inﬁnite dimension)  by k·kp the p-norm operator  by L2(X ) the set of square integrable functions
on the domain X   and by P the P -dimensional probability simplex  respectively. The support of
vector ✓ 2 Rd is supp(✓)   {i 2 [d] : ✓i 6= 0}. d·e and b·c denote the ceiling and ﬂoor functions 
respectively. We make use of the following deﬁnitions:
Deﬁnition 1. (strong convexity) A differentiable function g(·) is called µ-strongly convex on the
domain X with respect to k·k2  if for all x  x0 2X and some µ > 0 

g(x)  g(x0) + hrg(x0)  x  x0i +

µ
2 kx  x0k2
2 .

Deﬁnition 2. (smoothness) A differentiable function g(·) is called -smooth on the domain X with
respect to k·k2  if for all x  x0 2X and some > 0 

g(x)  g(x0) + hrg(x0)  x  x0i +


2 kx  x0k2
2 .

2.1 Supervised Learning with Explicit Feature Maps
n=1 in the form of input-output pairs is given to the
In supervised learning  a training set {(xn  yn)}N
learner. The (input-output) samples are generated independently from an unknown distribution PXY.
For n 2 [N ]  we have xn 2X⇢ Rd. In the case of regression  the output variable yn 2Y✓ [1  1] 
1In this paper  our focus is on “explicit features”  and whenever it is clear from the context  we simply use

“features” instead.

2

whereas in the case of classiﬁcation yn 2 {1  1}. The ultimate objective is to ﬁnd a target function
f : X! R  to be employed in mapping (unseen) inputs to correct outputs. This goal may be achieved
through minimizing a risk function R(f )  deﬁned as

bR(f )   1

N

NXn=1

R(f )   EPXY

[L(f (x)  y)]

L(f (xn)  yn) 

(1)

where L(· ·) is a loss function depending on the task (e.g.  quadratic for regression  hinge loss
for SVM). Since the distribution PXY is unknown  in lieu of the true risk R(f )  we minimize the
empirical risk bR(f ). To solve the problem  one needs to consider a function class for f (·) to minimize
the empirical risk over that class. For example  consider a positive semi-deﬁnite kernel K(· ·)2 and
consider functions of the form f (·) =PN
n=1 ↵nK(xn ·). Kernel methods minimize the empirical
risk bR(f ) over this class of functions by solving for optimal values of parameters {↵n}N
n=1. While
being theoretically well-justiﬁed  this approach is not practically applicable to large datasets  as
O(N 2) computations are required just to set up the training problem.
We now face two important questions: (i) can we reduce the computation time using a suitable
approximation of the kernel? (ii) how does the choice of kernel affect the prediction of unseen data
(generalization performance)? There is a large body of literature addressing these two questions. We
provide an extensive discussion of the related works in Section 4  and here  we focus on presenting
our method aiming to tackle the challenges above.
Consider a set of base positive semi-deﬁnite kernels {K1  . . .   KP}  such that Kp(x  x0) =
⌦p(x)  p(x0)↵ for p 2 [P ]. The feature map p : x 7! FKp maps the points in X
to FKp  the associated Reproducing Kernel Hilbert Space (RKHS) to kernel Kp. Let ✓ =
[✓1 1  . . .  ✓ 1 M1 . . .  ✓ P 1  . . .  ✓ P MP ]> and ⌫ = [⌫1  . . .  ⌫ P ]>  such thatPP
p=1 Mp = M. De-
ﬁne
Mp = M9=;
bFM  8<:
PXp=1

where p m(·) is the m-th component of the explicit feature map associated to Kp. The use of explicit
feature maps has proved to be beneﬁcial in learning with signiﬁcantly smaller computational burden
(see e.g. [6] for approximation of Gaussian kernel in training SVM and [5] for explicit form of feature
maps for several practical kernels). We use ⌫ for normalization purposes  and we are not concerned
with learning a rule to optimize it. Instead  given a ﬁxed value of ⌫  we are interested in including

✓p mp⌫pp m(x) : k✓k2  C   ⌫ 2 P  

MpXm=1

PXp=1

f (x) =

(2)

 

F  (f (x) =

performance over ⌫. It is actually well-known that Multiple Kernel Learning (MKL) can potentially
improve the generalization; however  it comes at the cost of solving expensive optimization problems
[10].

promising p m(·)’s in bFM  i.e.  the ones improving generalization. We can always optimize the
Note that the set bFM is a rich class of functions. It consists of M-term approximations of the class
that for a function in L2(X ) the i-th coefﬁcient must decay faster than O(1/pi) when the bases are
orthonormal. Interestingly  it has recently been proved that for approximation with smooth radial
kernels  the coefﬁcient decay is nearly exponential [9]. Therefore  for functions in L2(X )  most
of the energy content comes from the initial coefﬁcients  and we can hope to keep M ⌧ N for
computationally efﬁcient training. Such solutions also offer O(M ) computations in the test phase as
opposed to O(N ) in traditional kernel methods.

using multiple feature maps. Focusing on one kernel (P = 1)  we know by Parseval’s theorem [11]

p m  C   ⌫ 2 P)  

✓p mp⌫pp m(x) :

1Xm=1

1Xm=1

PXp=1

PXp=1

(3)

✓2

2.2 Multi Feature Greedy Approximation
We now propose an algorithm that carefully chooses the (approximated) kernel to attain a low
out-of-sample error. The algorithm has access to a set of M0 candidate (explicit) features p m(·) 
↵i↵jK(xi  xj)  0 for ↵ 2 RN .

2A symmetric function K : X⇥X! R is positive semi-deﬁnite if

NPi j=1

3

i.e.  Pp m 1 = M0. Starting with an empty set  it maintains an active set of selected features
by exploring the candidate features. At each step  the algorithm calculates the correlation of the
gradient (of the empirical risk) with standard bases of RM0. The feature p m(·) whose index
coincides with the most absolute correlation is added to the active set  and next  the empirical risk
is minimized over a more general model including the chosen feature. In the case of regression 
if we let >p m = [p m(x1) ··· p m(xN )]  the algorithm selects a p m such that p m has the
largest absolute correlation with the residual (the method is known as Orthogonal Matching Pursuit
(OMP) [12  13]). The algorithm can proceed for M rounds or until a termination condition is met
(e.g. the risk is small enough). Denoting by ej the j-th standard basis in RM0  we outline the method
in Algorithm 1.

Algorithm 1 Multi Feature Greedy Approximation (MFGA)
Initialize: I (1) = ;  ✓(0) = 0 2 RM0
1: for t 2 [M ] (M < M0) do
2:
3:
4:
5: end for

Let J (t) = argmaxj2[M0]DrbR⇣✓(t1)⌘   ejE.
Let I (t+1) = I (t) [{ J (t)}.
Solve ✓(t) = argminf2bFM0{bR(f )} subject to supp(✓) = I (t+1).

PPp=1

MpPm=1

p m p⌫pp m(·).
✓(M )

Output: bfMFGA(·) =
Assuming that repetitive features are not selected  at each iteration of the algorithm  a linear regression
or classiﬁcation is solved over a variable of size t. If the time cost of the task is C(t)  the training cost
of MFGA would bePM
t=1 C(t). However  in practice  we can select multiple features at each iteration
to decrease the runtime of the algorithm. In the case of regression  this amounts to Generalized OMP
[14]. While in general this rule might be sub-optimal  the authors of [14] have shown that the method
is quite competitive to the original OMP where one element is selected per iteration.

3 Theoretical Guarantees

Recall that our objective is to evaluate the out-of-sample performance (generalization) of our proposed
method. To begin  we quantify the richness of the class (2) in Lemma 1 using the notion of
Rademacher complexity  deﬁned below:
Deﬁnition 3. (Rademacher complexity) For a ﬁnite-sample set {xi}N
complexity of a class F is deﬁned as
if (xi)#  

N
where the expectation is taken over {i}N
the set {1  1}. The Rademacher complexity is then R(F)   EPX bR(F).
Assumption 1. For all p 2 [P ]  Kp is a positive semi-deﬁnite kernel and supx2X Kp(x  x)  B2.
Lemma 1. Given Assumption 1  the Rademacher complexity of the function class (2) is bounded as 

i=1 that are independent samples uniformly distributed on

bR(F)   1

i=1  the empirical Rademacher

EP"sup

f2F

NXi=1

R(bFM )  BCr 3dlog Pe

N

.

The bound above exhibits mild dependence to the number of base kernels P   akin to the results in
[15]. To derive our theoretical guarantees  we rely on the following assumptions:
Assumption 2. The loss function L(y  y0) = L(yy0) is -smooth and G-Lipschitz in the ﬁrst argu-
ment.

Notable example of the loss function satisfying the assumption above is the logistic loss L(y  y0) =
log(1 + exp(yy0)) for binary classiﬁcation.

4

Assumption 3. The empirical risk bR is µ-strongly convex with respect to ✓.

In case the empirical risk is weakly convex  strongly convexity can be achieved via adding a Tikhonov
regularizer. We are now ready to present our main theoretical result which decomposes the out-of-
sample error into three components:
p mp⌫pp m(·). Let Assumptions 1-3
Theorem 2. Deﬁne f ?(·)   argminf2F R(f ) =
hold and ✓(t) 2{ ✓ 2 RM0 : k✓k2 < C} for t 2 [M ]. Then  after M iterations of Algorithm 1  the
output satisﬁes 

1Pm=1

PPp=1

✓?

PPp=1

1Pm=b bM "cP c

✓?
p m

1

pN

f2F

21CA  

µm1◆◆  

R(f ) E est + Eapp + Espec 

Espec = O0B@vuut

R(bfMFGA)  min
with probability at least 1   over data  where
Eapp = O✓exp✓⌅M 1"⇧l 
◆  
Eest = O✓pdlog Pe+p log 
for any " 2 (0  1).
Our error bound consists of three terms: estimation error Eest  approximation error Eapp  and spectral
error Espec. As the bound holds for " 2 (0  1)  it can optimized over the choice of " in theory. The
O(1/pN ) estimation error with respect to the sample size is quite standard in supervised learning. It
was also shown in [15] that one cannot improve upon the plog P dependence due to the selection
of multiple kernels. The approximation error shows that the decay is exponential with respect to
the number of features  i.e.  to get an O(1/pN ) error  we only need O((log N )
1" ) features. The
exponential decay (expected from the greedy methods [8  16]) dramatically reduces the number
of features compared to non-greedy  randomized techniques at the cost of more computation. The
“spectral” error characterizes the spectral properties of the best model in the class (3). Since the
2-norm of the coefﬁcient sequence is bounded  Espec ! 0 as M ! 1  but the rate depends on the
tail of the coefﬁcient sequence. For example  if for all p 2 [P ]  Kp is a smooth radial kernel  the
coefﬁcient decay is nearly exponential [9].
Remark 1. The quadratic loss L(y  y0) = (y  y0)2 does not satisfy Assumption 2 in the sense that
L(y  y0) 6= L(yy0)  but with similar analysis in Theorem 2  we can prove that the same error bound
holds with slightly different constants (see the supplementary material).
Remark 2. Using Theorem 2.8 in [17]  our result can be extended to `2-regularized risk (see [17] 
Remark 2.1). In case of `1-penalty  due to non-differentiability  we should work with alternatives (e.g.
log[cosh(·)]).
Remark 3. There is an interesting connection between our result and reconstruction bounds in
greedy methods (e.g. [8])  where using M bases  the error decay is a function of both M and
“the best reconstruction” with M bases. Similarly here  Eapp and Espec capture these two notions 
respectively. Both errors go to zero as M ! 1 and there is a trade-off between the two  given
"> 0. An important issue is that “the best reconstruction” depends on the initial candidate (explicit
features) set. That error is small if the good explicit features are in the candidate set  and in a Fourier
analogy  a signal should be “band-limited” to be approximated well with ﬁnite bases.

4 Related Literature

Our work is related to several strands of literature reviewed below:
Kernel approximation: Since the kernel matrix is N ⇥ N  the computational cost of kernel methods
scales at least quadratically with respect to data. To overcome this problem  a large body of literature
has focused on approximation of kernels using low-rank surrogates [1  2]. Examples include the
celebrated Nyström method [18  19] which samples a subset of training data  approximates a surrogate
kernel matrix  and then transforms the data using the approximated kernel. Shifting focus to explicit
feature maps  in [20  21]  the authors have proposed low-dimensional Taylor expansions of Gaussian
kernel for speeding up learning. Moreover  Vedaldi et al. [22] provide explicit feature maps for
additive homogeneous kernels and quantify the approximation error using this approach. The major

5

difference of our work with this literature is that we are concerned with selecting “good” feature
maps in a greedy fashion for improved generalization.
Random features: An elegant idea to improve the efﬁciency of kernel approximation is to use
randomized features [3  23]. In this approach  the kernel function can be approximated as

K(x  x0) =Z⌦

(x  !)(x0  !)dP⌦(!) ⇡

1
M

MXm=1

(x  !m)(x0  !m) 

(4)

using Monte Carlo sampling of random features {!m}M
m=1 from the support set ⌦. A wide variety of
kernels can be written in the form of above. Examples include shift-invariant kernels approximated by
Monte Carlo [3] or Quasi Monte Carlo [24] sampling as well as dot product (e.g. polynomial) kernels
[25]. Various methods have been developed to decrease the time and space complexity of kernel
approximation (see e.g. Fast-food [26] and Structured Orthogonal Random Features [27]) using
properties of dense Gaussian random matrices. In general  random features reduce the computational
complexity of traditional kernel methods. It has been shown recently in [28] that to achieve O(1/pN )
learning error  we require only M = O(pN log N ) random features. Also  the authors of [29] have
shown that by `1-regularization (using a randomized coordinate descent approach) random features
can be made more efﬁcient. In particular  to achieve ✏-precision on risk  O(1/✏) random features
would be sufﬁcient (as opposed to O(1/✏2)).
Another line of research has focused on data-dependent choice of random features. In [30  31  32  33] 
data-dependent random features has been studied for the approximation of shift-invariant/translation-
invariant kernels. On the other hand  in [34  35  36  37]  the focal point is on the improvement of the
out-of-sample error. Sinha and Duchi [34] propose a pre-processing optimization to re-weight random
features  whereas Shahrampour et al. [35] introduce a data-dependent score function to select random
features. Furthermore  Bullins et al. [37] focus on approximating translation-invariant/rotation-
invariant kernels and maximizing kernel alignment in the Fourier domain. They provide analytic
results on classiﬁcation by solving the SVM dual with a no-regret learning scheme  and also an
improvement is achieved in terms of using multiple kernels. The distinction of our work with this
literature is that our method is greedy rather than randomized  and our focus is on explicit feature
maps. Additionally  another signiﬁcant difference in our framework with that of [37] is that we work
with differentiable loss functions  whereas [37] focuses on SVM. We will compare our work with
[23  34  35].
Greedy approximation: Over the pas few decades  greedy methods such as Matching Pursuit (MP)
[38  7] and Orthogonal Matching Pursuit (OMP) [12  13  8] have attracted the attention of several
communities due to their success in sparse approximation. In the machine learning community 
Vincent et al. [39] have proposed MP and OMP with kernels as elements. In the similar spirit is the
work of [40]  which concentrates on sparse regression and classiﬁcation models using Mercer kernels 
as well as the work of [41] that considers sparse regression with multiple kernels. Though traditional
MP and OMP were developed for regression  they have been further extended to logistic regression
[42] and smooth loss functions [43]. Moreover  in [44]  a greedy reconstruction technique has been
developed for regression by empirically ﬁtting squared error residuals. Unlike most of the prior art 
our focus is on explicit feature maps rather than kernels to save signiﬁcant computational costs. Our
algorithm can be thought as an extension of fully corrective greedy in [17] to nonlinear features from
multiple kernels where we optimize the risk over the class (2). However  in MFGA  we work with the
empirical risk (rather than the true risk in [17])  which happens in practice as we do not know PXY.
Multiple kernel learning: The main objective of MKL is to identify a good kernel using a data-
dependent procedure. In supervised learning  these methods may consider optimizing a convex  linear 
or nonlinear combination of a number of base kernels with respect to some measure (e.g. kernel
alignment) to select an ideal kernel [45  46  47]. It is also possible to optimize the kernel as well as
the empirical risk simultaneously [48  49]. On the positive side  there are many theoretical guarantees
for MKL [15  50]  but unfortunately  these methods often involve computationally expensive steps 
such as eigen-decomposition of the Gram matrix (see [10] for a comprehensive survey). The major
difference of this work with MLK is that we consider a combination of explicit feature maps (rather
than kernels)  and more importantly  we do not optimize the weights (as mentioned in Section 2.1 
we do not optimize the class (2) over ⌫) to avoid computational cost. Instead  our goal is to greedily
choose promising features for a ﬁxed value of ⌫.

6

We ﬁnally remark that data-dependent learning has been explored in the context of boosting and deep
learning [51  52  53]. Here  our main focus is on sparse representation for shallow networks.

5 Empirical Evaluations

m=1 are
m=1 are sampled from the uniform distribution on

We now evaluate our method on several datasets from the UCI Machine Learning Repository.
Benchmark algorithms: We compare MFGA to the state-of-the-art in randomized kernel approxi-
mation as well as traditional kernel methods:
1) RKS [23]  with approximated Gaussian kernel:  = cos(x>!m + bm) in (4)  {!m}M
sampled from a Gaussian distribution  and {bm}M
[0  2⇡).
2) LKRF [34]  with approximated Gaussian kernel:  = cos(x>!m + bm) in (4)  but instead of M 
a larger number M0 random features are sampled and then re-weighted by solving a kernel alignment
optimization. The top M random features would be used in the training.
3) EERF [35]  with approximated Gaussian kernel:  = cos(x>!m + bm) in (4)  and again M0
random features are sampled and then re-weighted according to a score function. The top M random
features would appear in the training. See Table 2a-2b for values of M and M0.
4) GK  the standard Gaussian kernel.
5) GLK  which is a sum of a Gaussian and a linear kernel.
The selection of the baselines above allows us to investigate the time-vs-accuracy tradeoff in kernel
approximation. Ideally  we would like to outperform randomized approaches  while being competitive
to kernel methods with signiﬁcantly lower computational cost.
Practical considerations: To determine the width of the Gaussian kernel K(x  x0) =
exp(kx  x0k2 /22)  we choose the value of  for each dataset to be the mean distance of
the 50th `2 nearest neighbor. Though being a rule-of-thumb  this choice has exhibited good general-
ization performance [30]. Notice that for randomized approaches  this amounts to sampling random
features from 1N (0  Id). Of course  optimizing over  (e.g. using cross-validation  jackknife 
or their approximate surrogates [54  55  56]) may provide better results. For our method as well
as GLK  we do not optimize over the convex combination weights (uniform weights are assigned).
This is possible using MKL  but our goal is to evaluate the trade-off between approximation and
accuracy  rather than proposing a rule to learn the best possible weights for the kernel. For classi-
ﬁcation  we let the number of candidate features M0 = 2d + 1  consisting of the ﬁrst order Taylor
features of the Gaussian kernel combined with features of linear kernel  whereas for regression  we let

2 + 2d + 1  approximating the Gaussian kernel up to second order. In the experiments  we
M0 =d
replace the 2-norm constraint of (2) by a quadratic regularizer [23]  tune the regularization parameter
over the set {105  104  . . .   105}  and report the best result for each method. As noted in Section
2.2  we select multiple features at each iteration of MFGA which is suboptimal but decreases the
runtime of the algorithm. We use logistic regression model for classiﬁcation to be able to compute
the gradient needed in MFGA.
Datasets: In Table 1  we report the number of training samples (Ntrain) and test samples (Ntest) used
for each dataset. If the training and test samples are not provided separately for a dataset  we split it
randomly. We standardize the data in the following sense: we scale the features to have zero mean
and unit variance and the responses in regression to be inside [1  1].

Table 1: Input dimension  number of training samples  and number of test samples are denoted by d  Ntrain  and
Ntest  respectively.

Dataset

Year prediction

Online news popularity

Adult

Epileptic seizure recognition

Task

Regression
Regression
Classiﬁcation
Classiﬁcation

d
90
58
122
178

Ntrain
46371
26561
32561
8625

Ntest
5163
13083
16281
2875

7

Year prediction

RKS
LKRF
EERF
This work
GK
GLK

0.15

r
o
r
r
e
 
t
s
e
T

0.1

0.05

Online news popularity

RKS
LKRF
EERF
This work
GK
GLK

50

100

150

200

250

300

350

400

Number of explicit feature maps (M)

0
20

40

80

60

160
Number of explicit feature maps (M)

100

120

140

180

200

Adult

Epileptic seizure recognition

RKS
LKRF
EERF
This work
GK
GLK

r
o
r
r
e
 
t
s
e
T

0.14

0.12

0.1

0.08

0.06

0.04

0.02

RKS
LKRF
EERF
This work
GK
GLK

2

4

6

8

10

12

14

16

18

20

Number of explicit feature maps (M)

0.15

0.1

r
o
r
r
e
 
t
s
e
T

0.05

0.25

r
o
r
r
e
 
t
s
e
T

0.2

0.15

10

20

30

40

50

60

70

80

90

100

Number of explicit feature maps (M)

Figure 1: Comparison of the test error of MFGA (this work) versus the randomized features baselines RKS 
LKRF  and EERF  as well as Gaussian Kernel (GK) and Gaussian+Linear Kernel (GLK).

Comparison with random features: For datasets in Table 1  we report our empirical ﬁndings in
Figure 1. On “Year prediction” and “Adult”  our method consistently improves the test error compared
to the state-of-the-art  i.e.  MFGA requires smaller number of features to achieve a certain test error
threshold. The key is to select “good” features to learn the subspace  and MFGA does so by greedily
searching among the candidate features that are explicit feature maps of the linear+Gaussian kernel
(up to second order Taylor expansion). As the number of features M increases  all methods tend
to generalize better in the regime shown in Figure 1. On “Online news popularity” our method
eventually achieves a smaller test error  whereas on “Epileptic seizure recognition” it is superior for
M  14 while being dominated by EERF afterwards.
Table 2a-2b tabulates the test error and time cost for largest M (for each dataset) in Figure 1. Since
RKS is fully randomized and data-independent  it has the smallest training time. However  in order to
compare the time cost of LKRF  EERF  and our work  we need additional details as the comparison
may not be immediate. In the pre-processing phase  LKRF and EERF draw M0 samples from
the Gaussian distribution and incur O (dN M0) computational cost. Additionally  LKRF solves an
optimization with O(M0 log ✏1) time to reach the ✏-optimal solution  and EERF sorts an array of size
M0 with average O(M0 log M0) time. On the other hand  when approximating Gaussian kernel by a
second order Taylor expansion  our method forms O(d2) features and incurs O(N d2) computations 
which is less than the other two in case d ⌧ M0. On all data sets except “Year prediction”  observe
that our method spends drastically smaller pre-processing time to achieve a competitive result after
evaluating smaller number of candidate features (i.e.  smaller M0). To compare the training cost  if
the time cost of the related task (regression or classiﬁcation) with M features is C(M )  LKRF and
EERF simply spend that budget. However  running K iterations of our method (with M a multiple
integer of K)  assuming that repetitive features are not selected  the training cost of MFGA would
bePK
k=1 C(kM/K)  which is more than LKRF and EERF. Furthermore  notice that the choice of
explicit or random feature maps would too affect the training time. For example  in regression  this
directly governs the condition number of the M ⇥ M matrix that is to be inverted. As a result  there
exist hidden constants in C that are different across algorithms. Overall  looking at the sum of training
and pre-processing time from Table 2a-2b  we observe that our algorithm can achieve competitive
results by spending less time compared to data-dependent methods. For example  on “Online news” 

8

we reduce the error of EERF from 1.63% to 0.57% (⇡ 65% decrease) in 1.22+0.92
10.6+0.15 time ratio (⇡ 80%
decrease).
In general  the comparison of our method to LKRF and EERF is equivalent to the comparison of
(data-dependent) explicit-vs-randomized feature maps. In comparison of vanilla (data-independent)
explicit-vs-randomized feature maps  as discussed in the experiments of [6] for Gaussian kernel  the
performance of none clearly dominates the other. Essentially  Gaussian kernel can be (roughly) seen
as (a countable) sum of polynomial kernels as well as (an uncountable) sum of cosine feature maps.
Our theoretical bound  which holds for countable sums  suggests that for “good” explicit feature
maps  the coefﬁcients may vanish fast (small Espec)  i.e.  there exists a sparse representation  but of
course  such feature map is unknown before the learning process.
Comparison with kernel methods: As we observe in Table 2a-2b  our method outperforms GK and
GLK on “Year prediction” and “Adult”. For “Year prediction”  our tpp + ttrain divided by the training
time of GK is (5.33 + 4.25)/139.5 ⇡ 0.068. The same number for “Adult” is ⇡ 0.036  exhibiting
a dramatic decrease in the runtime. Noticing that (except for “Epileptic seizure recognition”) we
used a subsample of training data for kernel methods (due to computational cost)  the actual runtime
decrease is even more remarkable (2 to 3 orders of magnitude). For “Online news popularity” and
“Epileptic seizure recognition”  our method is outperformed in terms of accuracy but still saves
signiﬁcant computational cost while being competitive to kernel methods.

Table 2: Comparison of the error and time cost of our algorithm versus other baselines. M0 is the number of
candidate features and M is the number of features used for training and testing. tpp and ttrain  respectively 
represent pre-processing and training time (seconds). For kernel methods  we use a subsample N0 of the training
set. For all methods  the test error (%) is reported with standard errors in parentheses for randomized approaches.

(a) Results on regression: Year prediction (left) and Online news (right)

Method
RKS
LKRF
EERF

This work

GK
GLK

Method
RKS
LKRF
EERF

This work

GK
GLK

M M0 N0/N
400
400
400
400
–
–

–
–
–
–
0.5
0.5

4000
4000
4186

–

–
–

tpp
–
3.5
3.3
5.33
–
–

ttrain
0.63
0.62
0.64
4.25
139.5
150.6

error (%)
8.27 (4e-2)
8.51 (8e-2)
8.76 (6e-2)

4.78
5.7
5.08

Method
RKS
LKRF
EERF

This work

GK
GLK

M
200
200
200
200
–
–

M0
–

20000
20000
1770

–
–

N0/N

–
–
–
–
0.3
0.3

tpp
–
9.8
10.6
1.22
–
–

(b) Results on classiﬁcation: Adult (left) and Epileptic seizure recognition (right)

–

M M0 N0/N
100
100
100
100
–
–

–
–
–
–
0.25
0.25

2000
2000
245
–
–

tpp
–
1.4
2
0.19
–
–

ttrain
0.87
0.91
1.38
0.69
24.07
77.09

error (%)
17.7 (6e-2)
16.46 (3e-2)
16.15 (2e-2)

15.10
15.70
15.22

Method
RKS
LKRF
EERF

This work

GK
GLK

–

M M0 N0/N
20
20
20
20
–
–

2000
2000
357
–
–

–
–
–
–
1
1

tpp
–
4.2
6.8
0.08
–
–

ttrain
0.13
0.14
0.15
0.92
240.9
257.6

ttrain
0.06
0.06
0.07
0.32
12.95
73.02

error (%)
3.08 (5e-2)
2.07 (5e-2)
1.63 (4e-2)

0.57
0.23
0.14

error (%)
6.21 (9e-2)
5.24 (4e-2)
4.46 (4e-2)

4.73
2.82
3.41

Acknowledgements

We gratefully acknowledge the support of DARPA Grant W911NF1810134.

References
[1] Alex J Smola and Bernhard Schökopf. Sparse greedy matrix approximation for machine learning. In

Proceedings of the Seventeenth International Conference on Machine Learning  pages 911–918  2000.

[2] Shai Fine and Katya Scheinberg. Efﬁcient SVM training using low-rank kernel representations. Journal of

Machine Learning Research  2(Dec):243–264  2001.

[3] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural

Information Processing Systems  2007.

9

[4] Thorsten Joachims. Training linear SVM’s in linear time. In Proceedings of the 12th ACM SIGKDD

International Conference on Knowledge Discovery and Data Mining  pages 217–226  2006.

[5] Ha Quang Minh  Partha Niyogi  and Yuan Yao. Mercer’s theorem  feature maps  and smoothing. In

International Conference on Computational Learning Theory  pages 154–168. Springer  2006.

[6] Andrew Cotter  Joseph Keshet  and Nathan Srebro. Explicit approximations of the gaussian kernel. arXiv

preprint arXiv:1109.4603  2011.

[7] Stéphane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries.

Transactions on Signal Processing  41(12):3397–3415  1993.

IEEE

[8] Joel A Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on

Information Theory  50(10):2231–2242  2004.

[9] Mikhail Belkin. Approximation beats concentration? an approximation view on inference with smooth

radial kernels. arXiv preprint arXiv:1801.03437  2018.

[10] Mehmet Gönen and Ethem Alpaydın. Multiple kernel learning algorithms. Journal of Machine Learning

Research  12(Jul):2211–2268  2011.

[11] Alan V Oppenheim  Alan S Willsky  and S Hamid Nawab. Signals & Systems. Pearson Educación  1998.

[12] Yagyensh Chandra Pati  Ramin Rezaiifar  and Perinkulam Sambamurthy Krishnaprasad. Orthogonal
matching pursuit: Recursive function approximation with applications to wavelet decomposition. In 1993
Conference Record of The Twenty-Seventh Asilomar Conference on Signals  Systems and Computers  pages
40–44. IEEE  1993.

[13] Geoffrey M Davis  Stephane G Mallat  and Zhifeng Zhang. Adaptive time-frequency decompositions.

Optical Engineering  33(7):2183–2192  1994.

[14] Jian Wang  Seokbeop Kwon  and Byonghyo Shim. Generalized orthogonal matching pursuit. IEEE

Transactions on Signal Processing  60(12):6202–6216  2012.

[15] Corinna Cortes  Mehryar Mohri  and Afshin Rostamizadeh. Generalization bounds for learning kernels. In

International Conference on Machine Learning  pages 247–254  2010.

[16] Rémi Gribonval and Pierre Vandergheynst. On the exponential convergence of matching pursuits in

quasi-incoherent dictionaries. IEEE Transactions on Information Theory  52(1):255–261  2006.

[17] Shai Shalev-Shwartz  Nathan Srebro  and Tong Zhang. Trading accuracy for sparsity in optimization

problems with sparsity constraints. SIAM Journal on Optimization  20(6):2807–2832  2010.

[18] Christopher Williams and Matthias Seeger. Using the Nyström method to speed up kernel machines. In

Advances in Neural Information Processing Systems  2001.

[19] Petros Drineas and Michael W Mahoney. On the Nyström method for approximating a gram matrix for

improved kernel-based learning. Journal of Machine Learning Research  6(Dec):2153–2175  2005.

[20] Changjiang Yang  Ramani Duraiswami  and Larry Davis. Efﬁcient kernel machines using the improved fast
gauss transform. In Proceedings of the 17th International Conference on Neural Information Processing
Systems  pages 1561–1568  2004.

[21] Jian-Wu Xu  Puskal P Pokharel  Kyu-Hwa Jeong  and Jose C Principe. An explicit construction of a
reproducing gaussian kernel Hilbert space. In IEEE International Conference on Acoustics  Speech and
Signal Processing  volume 5  2006.

[22] Andrea Vedaldi and Andrew Zisserman. Efﬁcient additive kernels via explicit feature maps.

Transactions on Pattern Analysis and Machine Intelligence  34(3):480–492  2012.

IEEE

[23] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with
randomization in learning. In Advances in Neural Information Processing Systems  pages 1313–1320 
2009.

[24] Jiyan Yang  Vikas Sindhwani  Haim Avron  and Michael Mahoney. Quasi-monte carlo feature maps for

shift-invariant kernels. In International Conference on Machine Learning  pages 485–493  2014.

[25] Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels. In International

conference on Artiﬁcial Intelligence and Statistics  pages 583–591  2012.

10

[26] Quoc Le  Tamás Sarlós  and Alex Smola. Fastfood-approximating kernel expansions in loglinear time. In

International Conference on Machine Learning  volume 85  2013.

[27] X Yu Felix  Ananda Theertha Suresh  Krzysztof M Choromanski  Daniel N Holtmann-Rice  and Sanjiv
Kumar. Orthogonal random features. In Advances in Neural Information Processing Systems  pages
1975–1983  2016.

[28] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In

Advances in Neural Information Processing Systems  pages 3218–3228  2017.

[29] Ian En-Hsu Yen  Ting-Wei Lin  Shou-De Lin  Pradeep K Ravikumar  and Inderjit S Dhillon. Sparse random
feature algorithm as coordinate descent in hilbert space. In Advances in Neural Information Processing
Systems  pages 2456–2464  2014.

[30] Felix X Yu  Sanjiv Kumar  Henry Rowley  and Shih-Fu Chang. Compact nonlinear maps and circulant

extensions. arXiv preprint arXiv:1503.03893  2015.

[31] Zichao Yang  Andrew Wilson  Alex Smola  and Le Song. A la carte–learning fast kernels. In Artiﬁcial

Intelligence and Statistics  pages 1098–1106  2015.

[32] Junier B Oliva  Avinava Dubey  Andrew G Wilson  Barnabás Póczos  Jeff Schneider  and Eric P Xing.
Bayesian nonparametric kernel-learning. In Artiﬁcial Intelligence and Statistics  pages 1078–1086  2016.

[33] Wei-Cheng Chang  Chun-Liang Li  Yiming Yang  and Barnabas Poczos. Data-driven random fourier
features using stein effect. Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial
Intelligence (IJCAI-17)  2017.

[34] Aman Sinha and John C Duchi. Learning kernels with random features. In Advances In Neural Information

Processing Systems  pages 1298–1306  2016.

[35] Shahin Shahrampour  Ahmad Beirami  and Vahid Tarokh. On data-dependent random features for improved

generalization in supervised learning. In AAAI Conference on Artiﬁcial Intelligence  2018.

[36] Shahin Shahrampour  Ahmad Beirami  and Vahid Tarokh. Supervised learning using data-dependent
random features with application to seizure detection. In IEEE Conference on Decision and Control  2018.

[37] Brian Bullins  Cyril Zhang  and Yi Zhang. Not-so-random features. International Conference on Learning

Representations  2018.

[38] Jerome H Friedman and Werner Stuetzle. Projection pursuit regression. Journal of the American Statistical

Association  76(376):817–823  1981.

[39] Pascal Vincent and Yoshua Bengio. Kernel matching pursuit. Machine Learning  48(1-3):165–187  2002.

[40] Prasanth B Nair  Arindam Choudhury  and Andy J Keane. Some greedy learning algorithms for sparse
regression and classiﬁcation with mercer kernels. Journal of Machine Learning Research  3(Dec):781–801 
2002.

[41] Vikas Sindhwani and Aurélie C Lozano. Non-parametric group orthogonal matching pursuit for sparse
learning with multiple kernels. In Advances in Neural Information Processing Systems  pages 2519–2527 
2011.

[42] Aurelie Lozano  Grzegorz Swirszcz  and Naoki Abe. Group orthogonal matching pursuit for logistic

regression. In Artiﬁcial Intelligence and Statistics  pages 452–460  2011.

[43] Francesco Locatello  Rajiv Khanna  Michael Tschannen  and Martin Jaggi. A uniﬁed optimization view
on generalized matching pursuit and frank-wolfe. In Artiﬁcial Intelligence and Statistics  pages 860–868 
2017.

[44] Dino Oglic and Thomas Gärtner. Greedy feature construction.

Processing Systems  pages 3945–3953  2016.

In Advances in Neural Information

[45] Jaz Kandola  John Shawe-Taylor  and Nello Cristianini. Optimizing kernel alignment over combinations of

kernel. 2002.

[46] Corinna Cortes  Mehryar Mohri  and Afshin Rostamizadeh. Learning non-linear combinations of kernels.

In Advances in Neural Information Processing Systems  pages 396–404  2009.

[47] Corinna Cortes  Mehryar Mohri  and Afshin Rostamizadeh. Algorithms for learning kernels based on

centered alignment. Journal of Machine Learning Research  13(Mar):795–828  2012.

11

[48] Marius Kloft  Ulf Brefeld  Sören Sonnenburg  and Alexander Zien. Lp-norm multiple kernel learning.

Journal of Machine Learning Research  12(Mar):953–997  2011.

[49] Gert RG Lanckriet  Nello Cristianini  Peter Bartlett  Laurent El Ghaoui  and Michael I Jordan. Learning
the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research  5(Jan):27–72 
2004.

[50] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research  3(Nov):463–482  2002.

[51] Corinna Cortes  Mehryar Mohri  and Umar Syed. Deep boosting. In International Conference on Machine

Learning  pages 1179–1187  2014.

[52] Corinna Cortes  Xavier Gonzalvo  Vitaly Kuznetsov  Mehryar Mohri  and Scott Yang. Adanet: Adaptive
structural learning of artiﬁcial neural networks. In International Conference on Machine Learning  pages
874–883  2017.

[53] Furong Huang  Jordan Ash  John Langford  and Robert Schapire. Learning deep resnet blocks sequentially

using boosting theory. In International Conference on Machine Learning  2018.

[54] Ahmad Beirami  Meisam Razaviyayn  Shahin Shahrampour  and Vahid Tarokh. On optimal generalizability
in parametric learning. In Advances in Neural Information Processing Systems  pages 3455–3465  2017.

[55] Shuaiwen Wang  Wenda Zhou  Haihao Lu  Arian Maleki  and Vahab Mirrokni. Approximate leave-one-out

for fast parameter tuning in high dimensions. arXiv preprint arXiv:1807.02694  2018.

[56] Ryan Giordano  Will Stephenson  Runjing Liu  Michael I Jordan  and Tamara Broderick. Return of the

inﬁnitesimal jackknife. arXiv preprint arXiv:1806.00550  2018.

[57] Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of machine learning. MIT

press  2012.

12

,Shahin Shahrampour
Vahid Tarokh
Suraj Srinivas
François Fleuret