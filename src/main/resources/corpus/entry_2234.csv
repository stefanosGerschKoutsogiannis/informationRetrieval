2019,General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme,The incremental aggregated gradient algorithm is popular in network optimization
and machine learning research. However  the current convergence results require
the objective function to be strongly convex. And the existing convergence rates
are also limited to linear convergence. Due to the mathematical techniques  the
stepsize in the algorithm is restricted by the strongly convex constant  which may
make the stepsize be very small (the strongly convex constant may be small).

In this paper  we propose a general proximal incremental aggregated gradient algorithm  which contains various existing algorithms including the basic incremental aggregated gradient method. Better and new convergence results are proved even with the general scheme. The novel results presented in this paper  which have not appeared in previous literature  include: a general scheme  nonconvex analysis  the sublinear convergence rates of the function values  much larger stepsizes that guarantee the convergence  the convergence when noise exists  the line search strategy of the proximal incremental aggregated gradient algorithm and its convergence.,General Proximal Incremental Aggregated Gradient
Algorithms: Better and Novel Results under General

Scheme∗

Tao Sun

College of Computer

National University of Defense Technology

Changsha  Hunan 410073  China

Yuejiao Sun

Department of Mathematics

University of California  Los Angeles

Los Angeles  CA 90095  USA

nudtsuntao@163.com

sunyj@math.ucla.edu

Dongsheng Li†

College of Computer

National University of Defense Technology

Changsha  Hunan 410073  China

Qing Liao

Department of Computer Science & Technology

Harbin Institute of Technology (Shenzhen)

Shenzhen  Guangdong 518055  China

dsli@nudt.edu.cn

liaoqing@hit.edu.cn

Abstract

The incremental aggregated gradient algorithm is popular in network optimization
and machine learning research. However  the current convergence results require
the objective function to be strongly convex. And the existing convergence rates
are also limited to linear convergence. Due to the mathematical techniques  the
stepsize in the algorithm is restricted by the strongly convex constant  which may
make the stepsize be very small (the strongly convex constant may be small).
In this paper  we propose a general proximal incremental aggregated gradient algo-
rithm  which contains various existing algorithms including the basic incremental
aggregated gradient method. Better and new convergence results are proved even
with the general scheme. The novel results presented in this paper  which have not
appeared in previous literature  include: a general scheme  nonconvex analysis  the
sublinear convergence rates of the function values  much larger stepsizes that guar-
antee the convergence  the convergence when noise exists  the line search strategy
of the proximal incremental aggregated gradient algorithm and its convergence.

1

Introduction

Many problems in machine learning and network optimization can be formulated as

min

x

{F (x) = f (x) + g(x)}  

where f (x) =(cid:80)m

(1)
i=1 fi(x)  x ∈ Rn  fi is differentiable  ∇fi is Lipschitz continuous with Li  for
i = 1  2  . . .   m  and g is proximable. A state-of-the-art method for this problem is the proximal
gradient method [10]  which requires to compute the full gradient of f in each iteration. However 
when the number of the component functions fi is very large  i.e. m (cid:29) 1  it is costly to obtain the full
∗This work is sponsored in part by the National Key R&D Program of China under Grant No.
2018YFB0204300 and the National Natural Science Foundation of China under Grants (61932001 and
61906200).

†Dongsheng Li is the corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

{1  2  . . .   m}; and then computes(cid:80)

gradient ∇f; on the other hand  in some network cases  calculating the full gradient is not allowed 
either. Thus  the incremental gradient algorithms are developed to avoid computing the full gradient.
The main idea of the incremental gradient descent lies on computing the gradients of partial com-
ponents of f to refresh the full gradient. Precisely  in each iteration  it selects an index set S from
i∈S ∇fi to update the full gradient. It requires much less com-
putation than the gradient descent without losing too much accuracy of the true gradient. It is natural
to consider two index selection strategies: deterministic and stochastic. In fact  all the incremental
gradient algorithms for solving problem (1) can be labeled as one of these two routines.

1.1 The general PIAG algorithm

Let xi denote the i-th iterate. We ﬁrst deﬁne a σ-algebra χk := σ(x1  x2  . . .   xk). Consider a
general proximal incremental aggregated gradient algorithm which performs as

(cid:26) E(vk | χk) =(cid:80)m

i=1 ∇fi(xk−τi k ) + ek 

xk+1 = proxγkg[xk − γkvk] 

(2)

where τi k is the delay associated with fi and ek is the noise in the k-th iteration. The ﬁrst equation
in (2) indicates that vk is an approximation of the full gradient ∇f with delays and noises in the
perspective of expectation. For simplicity  we call this algorithm as the general PIAG algorithm.

1.2 Literature review

As mentioned before  by the strategies of index selection  the literature can also be divided into two
classes.
On the deterministic road: Bertsekas proposed the Incremental Gradient (IG) method for problem
(1) when g ≡ 0 [4]. To obtain convergence  IG method requires diminishing stepsizes  even for
smooth and strongly convex functions [5]. A special condition was proposed in [26] to relax this
condition. The second order IG method was also developed in [13]. An improved version of the
IG is the Incremental Aggregated Gradient (IAG) method [6  29]. When fi is a quadratic function 
the convergence based on the perturbation analysis of the eigenvalues of a periodic dynamic linear
system was given by [6]. The global convergence is proved in [29]; and if the local Lipschitzian error
condition and local strong convexity are satisﬁed  the local linear convergence can also be proved.
[1] established lower complexity bounds for IAG for problem (1) with g ≡ 0. The linear convergence
rates are proved under strong convexity assumption [12  30].
On the stochastic road: The pioneer of this class is the Stochastic Gradient Descent (SGD) method
[20]  which suggests picking i from {1  2  . . .   m} in each iteration with uniform probability  and
using m∇fi to replace ∇f. However  SGD requires diminishing stepsizes  which makes its per-
formance poor in both theory and practice. Due to the large deviation of m∇fi from f  “variance
reduction" schemes were proposed later  such as SVRG method [14]  SAG method [25]  and the
SAGA method [11] are developed. With selected constant stepsizes  linear convergence has been
proved in the strongly convex case  and ergodic sublinear convergence has been proved in the
non-strongly convex case.

1.3 Relations with existing algorithms

In this part  we will present several popular existing algorithms which can be covered by the general
PIAG.
E.1. (Inexact) Proximal Gradient Desdent Algorithm: When τi k ≡ 0 and expectation vanishes  the
general PIAG is equivalent to xk+1 = proxγkg[xk − γk
(cid:80)
E.2.
In the k-th iteration 
pick ik essentially cyclicly and then update xk+1 as xk+1 = proxγkg[xk − γk(∇fik (xk) +

(cid:80)m
i=1 ∇fi(xk) + ek].

(Inexact) Proximal Incremental Aggregated Gradient Algorithm:

i(cid:54)=ik

∇fi(xk−τi k ) + ek)]  where(cid:26) τi k+1 = τi k + 1 if i (cid:54)= ik 

(3)

τi k+1 = 1 if i = ik.

2

In each iteration  one just needs to compute ∇fik (xk) and the term(cid:80)
(cid:26) vk+1 = vk − ∇fik (xk−τik  k ) + ∇fik (xk) 

the memory.

E.3. Deterministic SAG (SAGA): Let (τi k)i∈[N ] k≥0 be deﬁned as (3)  pick ik essentially cyclicly 
then update xk+1 as

∇fi(xk−τi k ) is shared by

i(cid:54)=ik

xk+1 = proxγkg(xk − γkvk+1).

m(cid:99)  take ˜x being any one
E4. Deterministic SVRG: Pick ik cyclicly  i.e.  ik ≡ k(mod m)  let t = (cid:98) k
from {xmt  xmt+1  . . .   xmt+t−1}. And then update xk+1 as xk+1 = proxγkg[xk − γ(∇fik (xk) −
∇kfik (˜x) + ∇f (˜x))].
E5. Decentralized Parallel Stochastic Gradient Descent (DPSGD): The algorithm is proposed in [17]
F}  where X := [x1  x2  . . .   xn](cid:62) 
W is mixing matrix  and Sj is the neighbour set of j. In each iteration  the DPSGD computes a
fi(xj) at node j  and then computes the neighborhood weighted average

to solve minX{D(X) :=(cid:80)n
stochastic gradient of(cid:80)

2(cid:107)(I−W )

fi(xj)+ 1

1

2 X(cid:107)2

j=1

i∈Sj

(cid:80)

i∈Sj
to update the local variable.

E6. Forward-backward splitting by Parameter Server Computing: The forward-backward splitting
for problem (1) can be implemented on a parameter server computing model  in which  the worker
nodes communicate synchronously with the parameter server but not directly with each other. Node i
just computes ∇if (ˆxk) and sends the result to the parameter server  where ˆxk comes from the shared
memory with bounded delay xk. In the parameter server  the gradients are collected and the iterate is
updated (implement the proximal map computation). The algorithm is a special case of the general
PIAG. More details about parameter server computing can be found in [24].

LAG: Lazily aggregated gradient: This algorithm is also designed with parameter server
E7.
computing. Different from E.7  the main motivation of this algorithm is to reduce the communications.
In this setting  the parameter server broadcasts the current iteration xk to the workers which is low-
costly; while the data transition cost from the worker to parameter server is high. In this case  authors
in [9] propose LAG whose core idea is disabling the data feedback from worker to parameter server
if the gradients change slightly. It is not difﬁcult to verify that the general PIAG contains LAG.

1.4 Contribution

In the perspective of algorithms  this paper proposes the general PIAG  which not only covers various
classical PIAG-like algorithms including the inexact schemes but also derives novel algorithms. In the
perspective of theory  we build better and novel results compared with previous literature. Speciﬁcally 
the contribution of this paper can be summarized as follows:
I. General scheme: We propose a general PIAG algorithm  which covers various classical algorithms
in network optimization  distributed optimization  machine learning areas. We unify all these
algorithms into one mathematical scheme.
II. Novel algorithm: We apply the line search strategy to PIAG and prove its convergence. The
numerical results demonstrate its efﬁciency.
III. Novel proof techniques: Compared with previous convergence analysis of PIAG  we use a new
proof technique: Lyapunov function analysis. Due to this  we can build much stronger theoretical
results with more general scheme. The Lyapunov function analysis is through the paper for both
convex and nonconvex cases.
IV. Better theoretical results: The previous convergence results of PIAG is restricted to strongly
convex case and the stepsize depends on the strong convexity constant. We get rid of this constant and
still guarantees the linear convergence with much larger stepsizes  even under a weaker assumption.
For the cyclic PIAG  the stepsize can be half of the gradient descent algorithm.
V. Novel theoretical results: Many new results are proved in this paper. We list them as follows:

• V.1. The convergence of nonconvex PIAG is studied. And in the expectation-free case  the

sequence convergence is proved under the semi-algebraic property.

3

Table 1: Contributions of this paper

Novel results

Theory

Algorithms

1. A general scheme which
covers various classical
algorithms.
2. Line search of PIAG is
proposed and analyzed.

Theoretical

results

1.The (in)exact convergence of nonconvex
PIAG is studied. The sequence convergence
is proved by means of semi-algebraic property.
2. Sublinear convergence of (in)exact
PIAG under general convex assumptions.

Better results

Proof technique Lyapunov function analysis

1. Much larger stepsize is proved to be convergence.
2. The numerical results show that line search of PIAG performs much better
than PIAG.

• V.2. The convergence of the inexact PIAG is proved for both convex and nonconvex cases.
In the convex case  the convergence rates are exploited if the noises are promised to follow
certain assumptions. In the nonconvex case  the sequence convergence is also prove under
semi-algebraic property and assumption on the noises.
• V.3. We proved the sublinear convergence of PIAG under general convex assumptions. To
the best of our knowledge  it is the ﬁrst time to prove the non-ergodic O(1/k) convergence
rate of PIAG. And  we also proved the non-ergodic O(1/k) convergence rate for inexact
PIAG.
• V.4. The convergence of line search of PIAG is proved for both convex and nonconvex cases.

The convergence rates are also presented in the convex case.

2 Preliminaries

:=

2 . Assume that fi

Through the paper  we use

the notation ∆k

xk+1 − xk 

and σk
is differentiable and ∇fi

(cid:2)E((cid:107)vk −(cid:80)m
i=1 ∇fi(xk−τi k )(cid:107)2 | χk)(cid:3) 1
Lipschitz continuous. Then  ∇f is Lipschitz continuous with L := (cid:80)m
summable assumption on (σk)k≥0  i.e. (cid:80)
the general PIAG we deﬁned in (2). Then we only need(cid:80)
is easy to prove(cid:80)

:=
is Li-
i=1 Li. The maximal
delay is τ := maxi k{τi k}. The convergence analysis in the paper depends on the square
i < +∞. That is why the general PIAG just contains
i σ2
deterministic SAGA and SVRG  in which case σk ≡ 0. However  the SAGA and SVRG may
In the deterministic case  σk = (cid:107)ek(cid:107)  according to
not have the summable assumption held.
i (cid:107)ei(cid:107)2 < +∞. Further  if the noise
vanishes  the assumption certainly holds. Besides the deterministic case we discussed above 
the stochastic coordinate descent algorithm (with asynchronous parallel) can also satisfy this
assumption. Taking the stochastic coordinate descent algorithm for example  in this algorithm 
E(cid:107)∇f (xk)(cid:107)2 = (N − 1)E(cid:107)∆k(cid:107)2. In the stochastic coordinate descent algorithm  it
k = N−1
σ2
E(cid:107)∆(cid:107)2 < +∞ if the stepsize is well chosen. For the asynchronous parallel
algorithm  by assuming the independence between ˆxk with ik  we can prove the same result given
in [Lemma 1  [27]]. We introduce the deﬁnitions of subdifferentials. The details can be found in
[19  22  23].
Deﬁnition 1 Let J : RN → (−∞  +∞] be a proper and lower semicontinuous function. The
subdifferential  of J at x ∈ RN   written as ∂J(x)  is deﬁned as
∂J(x) := {u ∈ RN : ∃ xk → x  uk → u  such that lim
y(cid:54)=xk

J(y) − J(xk) − (cid:104)uk  y − xk(cid:105)

inf
y→xk

(cid:107)y − xk(cid:107)2

N

i

≥ 0 }.

3 Convergence analysis

The analysis in this section is heavily based on the following Lyapunov function:

ξk(ε  δ) := F (xk) +

L
2ε

k−1(cid:88)

d=k−τ

+∞(cid:88)

i=k

i − min F 
σ2

(4)

(d − (k − τ ) + 1)(cid:107)∆d(cid:107)2 +

1
2δ

4

where ε  δ > 0 will be determined later  based on the step size γ and τ (the bound for τi k). We
discuss the convergence when g (the regularized function in (1)) is convex or nonconvex separately.
The main difference of the two cases is the upper bound of the stepsize. Due to the convexity of g 
the upper bound of the stepsize in the ﬁrst case is twice as the second one.

3.1

g is convex

When g is convex  we consider three different types of convergence: the ﬁrst one is in the perspective
of expectation  the second one is about almost surely convergence  while the last one considers the
semi-algebraic property[18  15  7].
Convergence in the perspective of expectation:
Lemma 1 Let f be a function (may be nonconvex) with L-Lipschitz gradient and g is convex 
and ﬁnite min F . Let (xk)k≥0 be generated by the general PIAG  and maxi k{τi k} ≤ τ  and
(2τ +1)L for arbitrary ﬁxed 0 < c < 1. Then we can

i < +∞. Choose the step size γk ≡ γ = 2c

(cid:80)

i σ2

choose ε  δ > 0 to obtain

Eξk(ε  δ) − Eξk+1(ε  δ) ≥ 1
4

(

1
γ

− L
2

− τ L) · E(cid:107)∆k(cid:107)2 

E(cid:107)∆k(cid:107) = 0.

lim
k

(5)

With the Lipschitz continuity of f  we are prepared to present the convergence result.

Theorem 1 Assume the conditions of Lemma 1 hold and(cid:80)

i σ2
by general PIAG. Then  we have limk E[dist(0  ∂F (xk))] = 0.
Remark 1 For the cyclic PIAG  τ = M. If we apply the gradient descent for (1)  the stepsize should
be 0 < γ < 2c
M L   for some 0 < c < 1. In this case  the stepsize of cyclic PIAG is the half of the
gradient descent algorithm for this problem.

i < +∞  and (xk)k≥0 is generated

Convergence in the perspective of almost surely: The almost surely convergence is proved in this
part. We consider a Lyapunov function which is modiﬁcation of (4) as
(d − (k − τ ) + 1)(cid:107)∆d(cid:107)2 +

ˆξk(ε  δ) := F (xk) + κ · k−1(cid:88)

i − min F 
σ2

+∞(cid:88)

(6)

1
2δ

i=k

where we assume τ ≥ 1 and

d=k−τ

κ :=

L
2ε

+

1
4τ

(

1
γ

− L
2

− τ L).

(7)

A lemma on nonnegative almost supermartingales [21]  whose details are included in the appendix 
is needed to prove the almost sure convergence.

Theorem 2 Assume the conditions of Lemma 1 hold and(cid:80)

i < +∞  and (xk)k≥0 is generated

by general PIAG. Then we have dist(0  ∂F (xk)) → 0  a.s.
Convergence under semi-algebraic property: If the function F satisﬁes the semi-algebraic prop-
erty3  we can obtain more results for the inexact proximal incremental aggregated gradient algorithm.
In this case  the expectation of (24) and (28) can both be removed. Similar to [Theorem 1  [28]]  we
can derive the following result.

i σ2

Theorem 3 Assume the conditions of Lemma 1 hold  and F satisﬁes the semi-algebraic property 
and (xk)k≥0 is generated by (in)exact PIAG  and (cid:107)ek(cid:107) ∼ O( 1
kη ) (η > 1)  then  (xk)k≥0 converges
to a critical point of F .

3.2

g is nonconvex

In this subsection  we consider the case when g is nonconvex. Under this weaker assumption  the
stepsizes are reduced for the convergence. Like previous subsection  we also consider three kinds of
convergence. We list them as sequence.

3Semi-algebraic property used in the nonconvex optimization  more details can be found in [8  2].

5

c

Proposition 1 Assume the conditions of Theorem 1 hold except that g is nonconvex and γk ≡ γ =
(2τ +1)L for arbitrary ﬁxed 0 < c < 1. Then  we have limk E[dist(0  ∂F (xk))] = 0.
Proposition 2 Assume the conditions of Proposition 1 hold  then  we have dist(0  ∂F (xk)) → 0  a.s.
Proposition 3 Assume the conditions of Theorem 3 hold except that g is nonconvex and γk ≡ γ =
(2τ +1)L for arbitrary ﬁxed 0 < c < 1  then  (xk)k≥0 converges to a critical point of F .

c

4 Convergence rates in convex case

In this part  we prove the sublinear convergence rates of the general proximal incremental aggregated
gradient algorithm under general convex case  i.e.  both f and g are convex. The analysis in the part
uses a slightly modiﬁed Lyapunov function

(d − (k − τ ) + 1)(cid:107)∆d(cid:107)2 + λk − min F 

(8)

Fk(ε  δ) := F (xk) + κ · k−1(cid:88)
(cid:80)+∞

d=k−τ

i +(cid:80)+∞

where κ is given in (7) and λk := 1
Here  we assume τ ≥ 1.
2δ

i=k σ2

i=k φ2

i and (φk)k≥0 is a nonnegative sequence.

4.1 Technical lemma

This part presents a technique lemma. The sublinear and linear convergence results are both derived
from this lemma.

2c

≤ φk (cid:80)+∞

Lemma 2 Assume the gradient of f is Lipschitz with L and g is convex. Choose the step size
γk ≡ γ =
(2τ +1)L for arbitrary ﬁxed 0 < c < 1. For any positive sequence (φk)k≥0 satisfying
σk√
k  for some D > 0. Let xk denote the projection of xk to arg min F  
i=k φ2
2
assumed to exist  and let

i ≤ Dφ2
(cid:40)

α := max{ 1
β := (τ + 1)( 1

γ + L) + 1

γ + L + κτ  2D}/[min{ L

8τ ( 1

γ − 1

2 − τ )  1}]

.

Then  there exist ε  δ > 0 such that:

(EFk+1(ε  δ))2 ≤ α(EFk(ε  δ) − EFk+1(ε  δ)) × (κτ

k−1(cid:88)

d=k−τ

E(cid:107)∆d(cid:107)2 + βE(cid:107)xk+1 − xk+1(cid:107)2 + λk).

(9)

4.2 Sublinear convergence rate under general convexity

In this subsection  we present the sublinear convergence of the general proximal incremental aggre-
gated gradient algorithm.
Theorem 4 Assume the gradient of f is Lipschitz continuous with L and g is convex  and proxg(·)
is bounded. Choose the step size γk ≡ γ = 2c
(2τ +1)L for arbitrary ﬁxed 0 < c < 1. Let (xk)k≥0 be
generated by the general proximal incremental aggregated gradient algorithm. And the σk ∼ O(ζ k) 
where 0 < ζ < 1. Then  we have

(10)
In many cases  proxg(·) may be unbounded. However  we can slightly modiﬁed the algorithm. For
example  in the LASSO problem

).

EF (xk) − min F ∼ O(

1
k

{(cid:107)b − Ax(cid:107)2

2 + (cid:107)x(cid:107)1} 

min

x

(11)

6

2 (cid:107)b(cid:107)2

2 + (cid:107)0(cid:107)1 = (cid:107)b(cid:107)2
(cid:104)

2]N . Then  we can turn to solve minx{|b − Ax(cid:107)2

we can easily see that (cid:107)x∗(cid:107)1 ≤ (cid:107)b − A · 0(cid:107)2
2. That means the solution set of (11) is
bounded by X := [−(cid:107)b(cid:107)2
2 + (cid:107)x(cid:107)1 + δX (x)}.
And we can set g(·) = (cid:107)·(cid:107)1 + δX (·) rather than (cid:107)·(cid:107)1. Luckily  the proximal map of (cid:107)·(cid:107)1 + δX (·) is
[prox|·|(xi)]
proximable. With [Theorem 2  [31]]  we have
for i ∈ [1  2  . . .   N ]. In the deterministic case  the sublinear convergence still holds even the
proxg(·) is unbounded. The boundedness of the proxg(·) is used to derive the boundedness of
sequence (xk)k≥0. In fact  this boundedness can be obtained by the coercivity of function F in the
deterministic case.

prox(cid:107)·(cid:107)1+δX (·)(x)

= proxδ[−(cid:107)b(cid:107)2

(cid:105)

i

2  (cid:107)b(cid:107)2
2 ]

Proposition 4 Assume the condition of Theorem 4 hold. Let (xk)k≥0 be generated by the (in)exact
PIAG  then F (xk) − min F ∼ O( 1
k ).
To the best of our knowledge  this is the ﬁrst time to prove the sublinear convergence rate for the
proximal incremental aggregated gradient algorithm.

4.3 Linear convergence with larger stepsize

Assume that the function F satisﬁes the following condition

(12)
where x is the projection of x to the set arg min F   and ν > 0. This property is weaker than the
strongly convexity. If F is further differentiable  condition (12) is equivalent to the restricted strongly
convexity [16].

F (x) − min F ≥ ν(cid:107)x − x(cid:107)2 

Theorem 5 Assume the gradient of f is Lipschitz with L and g is convex  and the function F satisﬁes
condition (12). Choose the step size γk ≡ γ =
(2τ +1)L for arbitrary ﬁxed 0 < c < 1. And the
σk ∼ O(ζ k)  where 0 < ζ < 1. Then  we have

2c

EF (xk) − min F ∼ O(ωk) 

(13)

for some 0 < ω < 1.

Compared with the existing linear convergence results in [30  12]  our theoretical ﬁndings enjoys two
advantages: 1. we generalize the strong convexity to a much weaker condition (12); 2. the stepsize
gets rid of the parameter ν which promises larger descent of the algorithm when ν is small.

5 Line search of the proximal incremental gradient algorithm

c

(2τ +1)L if g is nonconvex  and γk ≡ 2c

of the algorithm can be presented as follows: Step 1 Compute the point vk =(cid:80)m

In this part  we consider a line search version of the deterministic proximal incremental gradient
algorithm. First  we set γk ≡
(2τ +1)L if g is convex. The scheme
i=1 ∇fi(xk−τi k ).
Step 2 Find jk as the smallest integer number j which obeys that yk = proxηjk c1g[xk − ηjk c1vk].
2 (cid:107)yk − xk(cid:107)2 where 0 < η < 1 and c1  c2 > 0 the
and (cid:104)vk  yk − xk(cid:105) + g(yk) − g(xk) ≤ − c2
parameters. Set ηk = ηjk c1 if ηk ≥ γ and ηk = γ if else. The point xk+1 is generated by
xk+1 = proxηkg[xk − ηkvk].
Without the noise  the Lyapunov function can get one parameter free in the analysis (we can get rid
of t). Thus  the Lyapunov function used in this part can be described as

ξk(ε) := F (xk) +

L
2ε

(d − (k − τ ) + 1)(cid:107)∆d(cid:107)2 − min F.

(14)

k−1(cid:88)

d=k−τ

Lemma 3 Let f be a function (may be nonconvex) with L-Lipschitz gradient and g is nonconvex  and
ﬁnite min F . Let (xk)k≥0 be generated by the proximal incremental aggregated gradient algorithm
with line search  and maxi k{τi k} ≤ τ. Choose the parameter c2 ≥ (2τ +1)L
and 0 < c < 1. It then
holds that limk dist(0  ∂F (xk)) = 0.

c

7

(cid:80)k−1
In previous result  if g is convex  the lower bound of c2 can be shortened by half. This is
d=k−τ (cid:107)∆d(cid:107)2 +
because (68) in the Appendix can be improved as F (xk+1) − F (xk) ≤ L

(cid:105)(cid:107)∆k(cid:107)2. This result is proved by (21). Thus  we can obtain the following result.

(cid:104) (τ ε+1)L

− (2τ +1)L

2ε

2

c

Lemma 4 Assume conditions of Lemma 3 hold except that both f and g are convex and c2 ≥ (2τ +1)L
It then holds that limk dist(0  ∂F (xk)) = 0.
In fact  we can also derive the convergence rate for the line search version in the convex case. The
proof is very similar to the one in Section 4. Thus  we just present the sketch. Like the previous
d=k−τ (d − (k − τ ) +
8cτ (L + 2τ L). With this Lyapunov function and suitable ε  we

analysis  a modiﬁed Lyapunov function is needed Fk(ε) := F (xk) + ˜κ ·(cid:80)k−1

1)(cid:107)∆d(cid:107)2
prove the following two inequalities

2 − min F  where ˜κ := L

2ε + 1−c

2c

.

(L + 2τ L)  1} · (

(cid:107)∆d(cid:107)2) 

(15)

k(cid:88)

d=k−τ

and

Fk(ε) − Fk+1(ε) ≥ min{ 1 − c
k(cid:88)

+ L + ˜κτ ) × (

8cτ

(Fk+1(ε))2 ≤ (

1
γ

(cid:107)∆d(cid:107)2)

d=k−τ

(cid:32)

×

[(τ + 1)(

+ L) + 1](cid:107)xk+1 − xk+1(cid:107)2 + ˜κτ

1
γ

(cid:33)

.

(cid:107)∆d(cid:107)2

k−1(cid:88)

d=k−τ

(16)

With (15) and (16)  we then derive the following lemma.
Theorem 6 Let f be a convex function with L-Lipschitz gradient and g is convex  and ﬁnite min F .
Let (xk)k≥0 be generated by the proximal incremental aggregated gradient algorithm with line
search  and maxi k{τi k} ≤ τ. Choose the parameter c2 ≥ (2τ +1)L
and 0 < c < 1. Then  there
exist ε > 0 such that:

2c

(Fk+1(ε))2 ≤ ˜α(Fk(ε) − Fk+1(ε)) × (˜κτ

(17)
8cτ (L+2τ L)  1}]. Further more  if F is coercive  F (xk)−min F ∼

(cid:107)∆d(cid:107)2 + β(cid:107)xk+1 − xk+1(cid:107)2) 

d=k−τ

k ). If F satisﬁes condition (12)  F (xk) − min F ∼ O(˜ωk) for some 0 < ˜ω < 1.

where ˜α := ( 1
O( 1

γ +L+˜κτ )/[min{ 1−c

k−1(cid:88)

6 Numerical results

j +(cid:80)m
j +(cid:80)m

Now we use some numerical experiments to show how the line search strategy can accelerate the
PIAG algorithms. Here we considered the following two updating rules 

i=1 wk

1. scheme I: xk+1 = proxγg[xk − γ(wk+1
2. scheme II: xk+1 = proxγg[xk − γ(wk+1
j = ∇fj(xk) t = (cid:98) k

j − wk
j − wmt
where j ≡ k(mod m) wk+1
m(cid:99). We tested binary classiﬁers on MNIST  ijcnn1.
To include all convex and nonconvex cases  we choose logistic regression (convex) and squared
logistic loss (nonconvex) for f  (cid:96)1 regularization (convex) and MCP (nonconvex) for g. The results
when using scheme I and II with and without line search are shown in Figure 6. In our experiments 
we choose γ = 2c
γ .Our
numerical results shows that the line search strategy can speed up the PIAG algorithm a lot.

(2τ +1)L when g is nonconvex  c = 0.99  c2 = 1

(2τ +1)L when g is convex and

i )] 
i=1 wmt

)] 

c

i

7 Conclusion

In this paper  we consider a general proximal incremental aggregated gradient algorithm and prove
several novel results. Much better results are proved under more general conditions. The core of
the analysis is using the Lyapunov function analysis. We also consider the line search of proximal
incremental aggregated gradient algorithm and the convergence rate is proved.

8

References
[1] Alekh Agarwal and Leon Bottou. A lower bound for the optimization of ﬁnite sums. arXiv

preprint arXiv:1410.0723  2014.

[2] Hédy Attouch  Jérôme Bolte  Patrick Redont  and Antoine Soubeyran. Proximal alternating
minimization and projection methods for nonconvex problems: An approach based on the
kurdyka-łojasiewicz inequality. Mathematics of Operations Research  35(2):438–457  2010.

9

100101102cpu time0.350.40.450.50.550.60.650.7function valueMNIST: convex f plus convex g100101102cpu time0.10.1050.110.1150.120.125function valueMNIST: nonconvex f plus convex g100101102cpu time0.350.40.450.50.550.60.650.7function valueMNIST: convex f plus nonconvex g100101102103cpu time0.10.1050.110.1150.120.125function valueMNIST: nonconvex f plus nonconvex g10-1100101102cpu time0.250.30.350.40.450.50.550.6function valueijcnn1: convex f plus convex g10-1100101102cpu time0.20.250.30.350.40.450.50.550.60.65function valueijcnn1: convex f plus nonconvex g10-1100101102cpu time0.20.250.30.350.40.450.50.550.60.65function valueijcnn1: convex f plus nonconvex g100101102103cpu time0.030.040.050.060.070.080.090.10.110.120.13function valueijcnn1: nonconvex f plus nonconvex g[3] Amir Beck. On the convergence of alternating minimization for convex programming with
applications to iteratively reweighted least squares and decomposition schemes. SIAM Journal
on Optimization  25(1):185–209  2015.

[4] Dimitri P Bertsekas. Nonlinear programming. Athena scientiﬁc Belmont  1999.

[5] Dimitri P Bertsekas. Incremental gradient  subgradient  and proximal methods for convex

optimization: A survey. Optimization for Machine Learning  2010(1-38):3  2011.

[6] Doron Blatt  Alfred O Hero  and Hillel Gauchman. A convergent incremental gradient method

with a constant step size. SIAM Journal on Optimization  18(1):29–51  2007.

[7] Jérôme Bolte  Aris Daniilidis  and Adrian Lewis. The łojasiewicz inequality for nonsmooth
subanalytic functions with applications to subgradient dynamical systems. SIAM Journal on
Optimization  17(4):1205–1223  2007.

[8] Jérôme Bolte  Shoham Sabach  and Marc Teboulle. Proximal alternating linearized minimization
for nonconvex and nonsmooth problems. Mathematical Programming  146(1-2):459–494  2014.

[9] Tianyi Chen  Georgios B Giannakis  Tao Sun  and Wotao Yin. Lag: Lazily aggregated gradient

for communication-efﬁcient distributed learning. NIPS 2018  2018.

[10] Patrick L Combettes and Valérie R Wajs. Signal recovery by proximal forward-backward

splitting. Multiscale Modeling & Simulation  4(4):1168–1200  2005.

[11] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems  pages 1646–1654  2014.

[12] Mert Gurbuzbalaban  Asuman Ozdaglar  and PA Parrilo. On the convergence rate of incremental

aggregated gradient algorithms. SIAM Journal on Optimization  27(2):1035–1048  2017.

[13] Mert Gürbüzbalaban  Asuman Ozdaglar  and Pablo Parrilo. A globally convergent incremental

newton method. Mathematical Programming  151(1):283–313  2015.

[14] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[15] Krzysztof Kurdyka. On gradients of functions deﬁnable in o-minimal structures. In Annales de

l’institut Fourier  volume 48  pages 769–784. Chartres: L’Institut  1950-  1998.

[16] Ming-Jun Lai and Wotao Yin. Augmented \ell_1 and nuclear-norm models with a globally

linearly convergent algorithm. SIAM Journal on Imaging Sciences  6(2):1059–1091  2013.

[17] Xiangru Lian  Ce Zhang  Huan Zhang  Cho-Jio Hsieh  Wei Zhang  and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. arXiv preprint arXiv:1705.09056  2017.

[18] Stanislas Łojasiewicz. Sur la géométrie semi-et sous-analytique. Ann. Inst. Fourier  43(5):1575–

1595  1993.

[19] Boris S Mordukhovich. Variational analysis and generalized differentiation I: Basic theory 

volume 330. Springer Science & Business Media  2006.

[20] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of

mathematical statistics  pages 400–407  1951.

[21] Herbert Robbins and David Siegmund. A convergence theorem for non negative almost
supermartingales and some applications. In Herbert Robbins Selected Papers  pages 111–135.
Springer  1985.

[22] R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis  volume 317. Springer Science

& Business Media  2009.

[23] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press  2015.

10

[24] Ernest K Ryu and Wotao Yin.

arXiv:1708.06908  2017.

Proximal-proximal-gradient method.

arXiv preprint

[25] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming  162(1-2):83–112  2017.

[26] Mikhail V Solodov. Incremental gradient algorithms with stepsizes bounded away from zero.

Computational Optimization and Applications  11(1):23–35  1998.

[27] Tao Sun  Robert Hannah  and Wotao Yin. Asynchronous coordinate descent under more realistic
assumptions. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan 
and R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages 6183–
6191. 2017.

[28] Tao Sun  Hao Jiang  Lizhi Cheng  and Wei Zhu. A convergence frame for inexact non-
convex and nonsmooth algorithms and its applications to several iterations. arXiv preprint
arXiv:1709.04072  2017.

[29] Paul Tseng and Sangwoon Yun. Incrementally updated gradient methods for constrained and
regularized optimization. Journal of Optimization Theory and Applications  160(3):832–853 
2014.

[30] Nuri Denizcan Vanli  Mert Gurbuzbalaban  and Asu Ozdaglar. Global convergence rate of
proximal incremental aggregated gradient methods. arXiv preprint arXiv:1608.01713  2016.

[31] Yao-Liang Yu. On decomposing the proximal map.

Processing Systems  pages 91–99  2013.

In Advances in Neural Information

11

,Yanshuai Cao
Marcus Brubaker
David Fleet
Aaron Hertzmann
Eran Treister
Javier Turek
Xinan Wang
Sanjoy Dasgupta
Bo Dai
Dahua Lin
Tao Sun
Yuejiao Sun
Dongsheng Li
Qing Liao