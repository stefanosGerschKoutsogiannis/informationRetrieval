2014,Fast and Robust Least Squares Estimation in Corrupted Linear Models,Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However  these algorithms are typically not robust to outliers or corruptions in the observed covariates. The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence -- for which we also develop a randomized approximation -- motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations  we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.,Fast and Robust Least Squares Estimation in

Corrupted Linear Models

Brian McWilliams⇤ Gabriel Krummenacher⇤ Mario Lucic

Joachim M. Buhmann

{mcbrian gabriel.krummenacher lucic jbuhmann}@inf.ethz.ch

Department of Computer Science

ETH Z¨urich  Switzerland

Abstract

Subsampling methods have been recently proposed to speed up least squares esti-
mation in large scale settings. However  these algorithms are typically not robust
to outliers or corruptions in the observed covariates.
The concept of inﬂuence that was developed for regression diagnostics can be
used to detect such corrupted observations as shown in this paper. This property
of inﬂuence – for which we also develop a randomized approximation – motivates
our proposed subsampling algorithm for large scale corrupted linear regression
which limits the inﬂuence of data points since highly inﬂuential points contribute
most to the residual error. Under a general model of corrupted observations  we
show theoretically and empirically on a variety of simulated and real datasets that
our algorithm improves over the current state-of-the-art approximation schemes
for ordinary least squares.

1

Introduction

To improve scalability of the widely used ordinary least squares algorithm  a number of randomized
approximation algorithms have recently been proposed. These methods  based on subsampling the

dataset  reduce the computational time from Onp2 to o(np2)1 [14]. Most of these algorithms

are concerned with the classical ﬁxed design setting or the case where the data is assumed to be
sampled i.i.d.
typically from a sub-Gaussian distribution [7]. This is known to be an unrealistic
modelling assumption since real-world data are rarely well-behaved in the sense of the underlying
distributions.
We relax this limiting assumption by considering the setting where with some probability  the ob-
served covariates are corrupted with additive noise. This scenario corresponds to a generalised
version of the classical problem of “errors-in-variables” in regression analysis which has recently
been considered in the context of sparse estimation [12]. This corrupted observation model poses a
more realistic model of real data which may be subject to many different sources of measurement
noise or heterogeneity in the dataset.
A key consideration for sampling is to ensure that the points used for estimation are typical of the
full dataset. Typicality requires the sampling distribution to be robust against outliers and corrupted
points.
In the i.i.d. sub-Gaussian setting  outliers are rare and can often easily be identiﬁed by
examining the statistical leverage scores of the datapoints.
Crucially  in the corrupted observation setting described in §2  the concept of an outlying point
concerns the relationship between the observed predictors and the response. Now  leverage alone
cannot detect the presence of corruptions. Consequently  without using additional knowledge about

⇤Authors contributed equally.
1Informally: f (n) = o(g(n)) means f (n) grows more slowly than g(n).

1

the corrupted points  the OLS estimator (and its subsampled approximations) are biased. This also
rules out stochastic gradient descent (SGD) – which is often used for large scale regression – since
convex cost functions and regularizers which are typically used for noisy data are not robust with
respect to measurement corruptions.
This setting motivates our use of inﬂuence – the effective impact of an individual datapoint exerts on
the overall estimate – in order to detect and therefore avoid sampling corrupted points. We propose
an algorithm which is robust to corrupted observations and exhibits reduced bias compared with
other subsampling estimators.

Outline and Contributions.
In §2 we introduce our corrupted observation model before reviewing
the basic concepts of statistical leverage and inﬂuence in §3. In §4 we brieﬂy review two subsampling
approaches to approximating least squares based on structured random projections and leverage
weighted importance sampling. Based on these ideas we present inﬂuence weighted subsampling
(IWS-LS)  a novel randomized least squares algorithm based on subsampling points with small
inﬂuence in §5.
In §6 we analyse IWS-LS in the general setting where the observed predictors can be corrupted
with additive sub-Gaussian noise. Comparing the IWS-LS estimate with that of OLS and other
randomized least squares approaches we show a reduction in both bias and variance. It is important
to note that the simultaneous reduction in bias and variance is relative to OLS and randomized
approximations which are only unbiased in the non-corrupted setting. Our results rely on novel
ﬁnite sample characteristics of leverage and inﬂuence which we defer to §SI.3. Additionally  in
§SI.4 we prove an estimation error bound for IWS-LS in the standard sub-Gaussian model.
Computing inﬂuence exactly is not practical in large-scale applications and so we propose two ran-
domized approximation algorithms based on the randomized leverage approximation of [8]. Both
of these algorithms run in o(np2) time which improve scalability in large problems. Finally  in §7
we present extensive experimental evaluation which compares the performance of our algorithms
against several randomized least squares methods on a variety of simulated and real datasets.
2 Statistical model
In this work we consider a variant of the standard linear model

y = X + ✏ 

(1)
where ✏ 2 Rn is a noise term independent of X 2 Rn⇥p. However  rather than directly observing
X we instead observe Z where
(2)
U = diag(u1  . . .   un) and ui is a Bernoulli random variable with probability ⇡ of being 1.
W 2 Rn⇥p is a matrix of measurement corruptions. The rows of Z therefore are corrupted with
probability ⇡ and not corrupted with probability (1  ⇡).
Deﬁnition 1 (Sub-gaussian matrix). A zero-mean matrix X is called sub-Gaussian with parameter
n ⌃x. (b) For
( 1
n 2
any unit vector v 2 Rp  v>xi is a sub-Gaussian random variable with parameter at most 1pp x.
We consider the speciﬁc instance of the linear corrupted observation model in Eqs. (1)  (2) where

n ⌃x) if (a) Each row x>i 2 Rp is sampled independently and has E[xix>i ] = 1

Z = X + UW.

x  1

w  1

n ⌃w) respec-

tively 

• X  W 2 Rn⇥p are sub-Gaussian with parameters ( 1
✏ In) 
• ✏ 2 Rn is sub-Gaussian with parameters ( 1
n 2

n 2

✏   1

n 2

x  1

n ⌃x) and ( 1

n 2

and all are independent of each other.
The key challenge is that even when ⇡ and the magnitude of the corruptions  w are relatively small 
the standard linear regression estimate is biased and can perform poorly (see §6). Sampling methods
which are not sensitive to corruptions in the observations can perform even worse if they somehow
subsample a proportion rn > ⇡n of corrupted points. Furthermore  the corruptions may not be large
enough to be detected via leverage based techniques alone.
The model described in this section generalises the “errors-in-variables” model from classical least
squares modelling. Recently  similar models have been studied in the high dimensional (p  n)

2

setting in [4–6  12] in the context of robust sparse estimation. The “low-dimensional” (n > p)
setting is investigated in [4]  but the “big data” setting (n  p) has not been considered so far.2
In the high-dimensional problem  knowledge of the corruption covariance  ⌃w [12]  or the data
covariance ⌃x [5]  is required to obtain a consistent estimate. This assumption may be unrealistic in
many settings. We aim to reduce the bias in our estimates without requiring knowledge of the true
covariance of the data or the corruptions  and instead sub-sample only non-corrupted points.

3 Diagnostics for linear regression

In practice  the sub-Gaussian linear model assumption is often violated either by heterogeneous
noise or by a corruption model as in §2. In such scenarios  ﬁtting a least squares model to the full
dataset is unwise since the outlying or corrupted points can have a large adverse effect on the model
ﬁt. Regression diagnostics have been developed in the statistics literature to detect such points (see
e.g. [2] for a comprehensive overview). Recently  [14] proposed subsampling points for least squares
based on their leverage scores. Other recent works suggest related inﬂuence measures that identify
subspace [16] and multi-view [15] clusters in high dimensional data.

3.1 Statistical leverage

For the standard linear model in Eq. (1)  the well known least squares solution is

b = arg min

 ky  Xk2 =X>X1

X>y.

(3)

The projection matrix I L with L := X(X>X)1X> speciﬁes the subspace in which the residual
lies. The diagonal elements of the “hat matrix” L  li := Lii  i = 1  . . .   n are the statistical leverage
scores of the ith sample. Leverage scores quantify to what extent a particular sample is an outlier
with respect to the distribution of X.
An equivalent deﬁnition from [14] which will be useful later concerns any matrix U 2 Rn⇥p which
spans the column space of X (for example  the matrix whose columns are the left singular vectors of
X). The statistical leverage scores of the rows of X are the squared row norms of U  i.e. li = kUik2.
Although the use of leverage can be motivated from the least squares solution in Eq. (3)  the lever-
age scores do not take into account the relationship between the predictor variables and the response
variable y. Therefore  low-leverage points may have a weak predictive relationship with the re-
sponse and vice-versa. In other words  it is possible for such points to be outliers with respect to the
conditional distribution P (y|X) but not the marginal distribution on X.
3.2

Inﬂuence

A concept that captures the predictive relationship between covariates and response is inﬂuence.
Inﬂuential points are those that might not be outliers in the geometric sense  but instead adversely
affect the estimated coefﬁcients.
One way to assess the inﬂuence of a point is to compute the change in the learned model when
the point is removed from the estimation step. [2]. We can compute a leave-one-out least squares
estimator by straightforward application of the Sherman-Morrison-Woodbury formula (see Prop. 3
in §SI.3):

we have

where ei = yi  xibOLS. Deﬁning the inﬂuence3  di as the change in expected mean squared error

bi =X>X  x>i xi1X>y  x>i yi =b 
di =⇣b bi⌘> X>X⇣b bi⌘ =

(1  li)2 .
2Unlike [5  12] and others we do not consider sparsity in our solution since n  p.
3The expression we use is also called Cook’s distance [2].

⌃1x>i ei
1  li

e2
i li

3

Points with large values of di are those which  if added to the model  have the largest adverse effect
on the resulting estimate. Since inﬂuence only depends on the OLS residual error and the leverage
scores  it can be seen that the inﬂuence of every point can be computed at the cost of a least squares
ﬁt. In the next section we will see how to approximate both quantities using random projections.

4 Fast randomized least squares algorithms

We brieﬂy review two randomized approaches to least squares approximation:
the importance
weighted subsampling approach of [9] and the dimensionality reduction approach [14]. The for-
mer proposes an importance sampling probability distribution according to which  a small number
of rows of X and y are drawn and used to compute the regression coefﬁcients. If the sampling prob-
abilities are proportional to the statistical leverages  the resulting estimator is close to the optimal
estimator [9]. We refer to this as LEV-LS.
The dimensionality reduction approach can be viewed as a random projection step followed by a
uniform subsampling. The class of Johnson-Lindenstrauss projections – e.g. the SRHT – has been
shown to approximately uniformize leverage scores in the projected space. Uniformly subsampling
the rows of the projected matrix proves to be equivalent to leverage weighted sampling on the origi-
nal dataset [14]. We refer to this as SRHT-LS. It is analysed in the statistical setting by [7] who also
propose ULURU  a two step ﬁtting procedure which aims to correct for the subsampling bias and
consequently converges to the OLS estimate at a rate independent of the number of subsamples [7].

Subsampled Randomized Hadamard Transform (SRHT) The SHRT consists of a precondi-
tioning step after which nsubs rows of the new matrix are subsampled uniformly at random in the

following wayq n

nsubs

SHD · X = ⇧X with the deﬁnitions [3]:

• S is a subsampling matrix.
• D is a diagonal matrix whose entries are drawn independently from {1  1}.
• H 2 Rn⇥n is a normalized Walsh-Hadamard matrix4 which is deﬁned recursively as

Hn = Hn/2 Hn/2

Hn/2 Hn/2    H2 = +1 +1
+1 1  .

We set H = 1pn Hn so it has orthonormal columns.

As a result  the rows of the transformed matrix ⇧X have approximately uniform leverage scores.
(see [17] for detailed analysis of the SRHT). Due to the recursive nature of H  the cost of applying
the SRHT is O (pn log nsubs) operations  where nsubs is the number of rows sampled from X [1].

k˜ek  (1 + ⇢)kek

subsampling ratio  r =⌦( p2

⇢2 ) results in a residual error  ˜e which satisﬁes

The SRHT-LS algorithm solves bSRHT = arg min k⇧y  ⇧Xk2 which for an appropriate
where e = y  XbOLS is the vector of OLS residual errors [14].

Randomized leverage computation Recently  a method based on random projections has been
proposed to approximate the leverage scores based on ﬁrst reducing the dimensionality of the data
using the SRHT followed by computing the leverage scores using this low-dimensional approxima-
tion [8–10  13].
The leverage approximation algorithm of [8] uses a SRHT  ⇧1 2 Rr1⇥n to ﬁrst compute the ap-
proximate SVD of X 
⇧1X = U⇧X⌃⇧XV>⇧X. Followed by a second SHRT ⇧2 2 Rp⇥r2 to compute an approximate
orthogonal basis for X

(4)

R1 = V⇧X⌃1

⇧X 2 Rp⇥p  ˜U = XR1⇧2 2 Rn⇥r2.

(5)

4For the Hadamard transform  n must be a power of two but other transforms exist (e.g. DCT  DFT) for

which similar theoretical guarantees hold and there is no restriction on n.

4

The approximate leverage scores are now the squared row norms of ˜U  ˜li = k ˜Uik2.
From [14] we derive the following result relating to randomized approximation of the leverage

˜li  (1 + ⇢l)li  

(6)

where the approximation error  ⇢l depends on the choice of projection dimensions r1 and r2.
The leverage weighted least squares (LEV-LS) algorithm samples rows of X and y with probability
proportional to li (or ˜li in the approximate case) and performs least squares on this subsample. The
residual error resulting from the leverage weighted least squares is bounded by Eq. (4) implying
that LEV-LS and SRHT-LS are equivalent [14]. It is important to note that under the corrupted
observation model these approximations will be biased.

Inﬂuence weighted subsampling

5
In the corrupted observation model  OLS and therefore the random approximations to OLS de-
scribed in §4 obtain poor predictions. To remedy this  we propose inﬂuence weighted subsampling
(IWS-LS) which is described in Algorithm 1. IWS-LS subsamples points according to the distri-
bution  Pi = c/di where c is a normalizing constant so thatPn
i=1 Pi = 1. OLS is then estimated on
the subsampled points. The sampling procedure ensures that points with high inﬂuence are selected
infrequently and so the resulting estimate is less biased than the full OLS solution. Several ap-
proaches similar in spirit have previously been proposed based on identifying and down-weighting
the effect of highly inﬂuential observations [19].
Obviously  IWS-LS is impractical in the scenarios we consider since it requires the OLS residuals
and full leverage scores. However  we use this as a baseline and to simplify the analysis. In the next
section  we propose an approximate inﬂuence weighted subsampling algorithm which combines the
approximate leverage computation of [8] and the randomized least squares approach of [14].

Algorithm 1 Inﬂuence weighted subsampling
(IWS-LS).
Input: Data: Z  y

Algorithm 2 Residual weighted subsampling
(aRWS-LS)
Input: Data: Z  y

1: SolvebOLS = arg min ky  Zk2

2: for i = 1 . . . n do
3:
4:
5:
6: end for
7: Sample rows (˜Z  ˜y) of (Z  y) proportional to 1
di

ei = yi  zibOLS

li = z>i (Z>Z)1zi
i li/(1  li)2
di = e2

1
˜e2
i

3: Sample rows (˜Z  ˜y) of (Z  y) proportional to

1: SolvebSRHT = arg min k⇧ · (y  Z)k2
2: Estimate residuals: ˜e = y  ZbSRHT
4: SolvebRW S = arg min k˜y  ˜Zk2
Output: bRW S

8: SolvebIWS = arg min k˜y  ˜Zk2
Output: bIWS
Randomized approximation algorithms. Using the ideas from §4 and §4 we obtain the following
randomized approximation to the inﬂuence scores

˜di =

˜li
˜e2
i
(1  ˜li)2

 

(7)

where ˜ei is the ith residual error computed using the SRHT-LS estimator. Since the approxima-
tion errors of ˜ei and ˜li are bounded (inequalities (4) and (6))  this suggests that our randomized
approximation to inﬂuence is close to the true inﬂuence.

Basic approximation. The ﬁrst approximation algorithm is identical to Algorithm 1 except that
leverage and residuals are replaced by their randomized approximations as in Eq. (7). We refer to
this algorithm as Approximate inﬂuence weighted subsampling (aIWS-LS). Full details are given
in Algorithm 3 in §SI.2.

5

Residual Weighted Sampling. Leverage scores are typically uniform [7  13] for sub-Gaussian
data. Even in the corrupted setting  the difference in leverage scores between corrupted and non-
corrupted points is small (see §6). Therefore  the main contribution to the inﬂuence for each point
will originate from the residual error  e2
i . Consequently  we propose sampling with probability
inversely proportional to the approximate residual  1
. The resulting algorithm Residual Weighted
˜e2
i
Subsampling (aRWS-LS) is detailed in Algorithm 2. Although aRWS-LS is not guaranteed to be
a good approximation to IWS-LS  empirical results suggests that it works well in practise and is
faster to compute than aIWS-LS.

Computational complexity. Clearly  the computational complexity of IWS-LS is Onp2. The
computation complexity of aIWS-LS is Onp log nsubs + npr2 + nsubsp2  where the ﬁrst term
(5). The cost of aRWS-LS is Onp log nsubs + np + nsubsp2 where

is the cost of SRHT-LS  the second term is the cost of approximate leverage computation and the
last term solves OLS on the subsampled dataset. Here  r2 is the dimension of the random pro-
jection detailed in Eq.
the ﬁrst term is the cost of SRHT-LS  the second term is the cost of computing the residuals
e  and the last term solves OLS on the subsampled dataset. This computation can be reduced to

Onp log nsubs + nsubsp2. Therefore the cost of both aIWS-LS and aRWS-LS is o(np2).

6 Estimation error

In this section we will prove an upper bound on the estimation error of IWS-LS in the corrupted
model. First  we show that the OLS error consists of two additional variance terms that depend on the
size and proportion of the corruptions and an additional bias term. We then show that IWS-LS can
signiﬁcantly reduce the relative variance and bias in this setting  so that it no longer depends on the
magnitude of the corruptions but only on their proportion. We compare these results to recent results
from [4  12] suggesting that consistent estimation requires knowledge about ⌃w. More recently  [5]
show that incomplete knowledge about this quantity results in a biased estimator where the bias is
proportional to the uncertainty about ⌃w. We see that the form of our bound matches these results.
Inequalities are said to hold with high probability (w.h.p.) if the probability of failure is not more
than C1 exp(C2 log p) where C1  C2 are positive constants that do not depend on the scaling quan-
tities n  p  w. The symbol . means that we ignore constants that do not depend on these scaling
quantities. Proofs are provided in the supplement. Unless otherwise stated  k·k denotes the `2 norm
for vectors and the spectral norm for matrices.
Corrupted observation model. As a baseline  we ﬁrst investigate the behaviour of the OLS esti-
mator in the corrupted model.

+ ⇡2

wppkk! ·

1


(8)

Theorem 1 (A bound on kbOLS  k). If n & 2
kbOLS  k . ✏x + ⇡✏w + ⇡2

w + wxkkr p log p

n

x2
w

min(⌃x) p log p then w.h.p.

where 0 <  min(⌃x) + ⇡min(⌃w).
Remark 1 (No corruptions case). Notice for a ﬁxed w  taking lim⇡!0 or for a ﬁxed ⇡ taking
limw!0 (i.e. there are no corruptions) the above error reduces to the least squares result (see for
example [4]).

Remark 2 (Variance and Bias). The ﬁrst three terms in (8) scale withp1/n so as n ! 1  these
terms tend towards 0. The last term does not depend onp1/n and so for some non-zero ⇡ the least

squares estimate will incur some bias depending on the fraction and magnitude of corruptions.

We are now ready to state our theorem characterising the mean squared error of the inﬂuence
weighted subsampling estimator.
Theorem 2 (Inﬂuence sampling in the corrupted model). For n & 2

min(⌃⇥x) p log p we have

x2
w

kbIWS  k . ✓✏x +

⇡✏

(w + 1)

+ ⇡kk◆r p log p

nsubs

+ ⇡ppkk! .

1


where 0 <  min(⌃⇥x) and ⌃⇥x is the covariance of the inﬂuence weighted subsampled data.

6

(a) Inﬂuence (1.1)

(b) Leverage (0.1)

⇡2

Figure 1: Comparison of the distribution of the inﬂuence and leverage for corrupted and non-
corrupted points. The `1 distance between the histograms is shown in brackets.
Remark 3. Theorem 2 states that the inﬂuence weighted subsampling estimator removes the propor-

tional dependance of the error on w so the additional variance terms scale as O(⇡/w·pp/nsubs)
and O(⇡pp/nsubs). The relative contribution of the bias term is ⇡ppkk compared with
wppkk for the OLS or non-inﬂuence-based subsampling methods.
Comparison with fully corrupted setting. We note that the bound in Theorem 1 is similar to the
bound in [5] for an estimator where all data points are corrupted (i.e. ⇡ = 1) and where incomplete
knowledge of the covariance matrix of the corruptions  ⌃w is used. The additional bias in the
estimator is proportional to the uncertainty in the estimate of ⌃w – in Theorem 1 this corresponds to
w. Unbiased estimation is possible if ⌃w is known. See the Supplementary Information for further
2
discussion  where the relevant results from [5] are provided in Section SI.6.1 as Lemma 16.
7 Experimental results
We compare IWS-LS against the methods SRHT-LS [14]  ULURU [7]. These competing methods
represent current state-of-the-art in fast randomized least squares. Since SRHT-LS is equivalent to
LEV-LS [9] the comparison will highlight the difference between importance sampling according
to the two difference types of regression diagnostic in the corrupted model. Similar to IWS-LS 
ULURU is also a two-step procedure where the ﬁrst is equivalent to SRHT-LS. The second reduces
bias by subtracting the result of regressing onto the residual. The experiments with the corrupted
data model will demonstrate the difference in robustness of IWS-LS and ULURU to corruptions in
the observations. Note that we do not compare with SGD. Although SGD has excellent properties
for large-scale linear regression  we are not aware of a convex loss function which is robust to the
corruption model we propose.
We assess the empirical performance of our method compared with standard and state-of-the-art
randomized approaches to linear regression in several difference scenarios. We evaluate these meth-
ods on the basis of the estimation error: the `2 norm of the difference between the true weights and

the learned weights  kb  k. We present additional results for root mean squared prediction error
(RMSE) on the test set in §SI.7.
For all the experiments on simulated data sets we use ntrain = 100  000  ntest = 1000  p = 500.
For datasets of this size  computing exact leverage is impractical and so we report on results for
IWS-LS in §SI.7. For aIWS-LS and aRWS-LS we used the same number of sub-samples to
approximate the leverage scores and residuals as for solving the regression. For aIWS-LS we set
r2 = p/2 (see Eq. (5)). The results are averaged over 100 runs.
Corrupted data. We investigate the corrupted data noise model described in Eqs. (1)-(2). We
show three scenarios where ⇡ = {0.05  0.1  0.3}. X and W were sampled from independent  zero-
mean Gaussians with standard deviation x = 1 and w = 0.4 respectively. The true regression
coefﬁcients   were sampled from a standard Gaussian. We added i.i.d. zero-mean Gaussian noise
with standard deviation e = 0.1.
Figure 1 shows the difference in distribution of inﬂuence and leverage between non-corrupted points
(top) and corrupted points (bottom) for a dataset with 30% corrupted points. The distribution of
leverage is very similar between the corrupted and non-corrupted points  as quantiﬁed by the `1
difference. This suggests that leverage alone cannot be used to identify corrupted points.

7

(a) 5% Corruptions

(b) 30% Corruptions

(c) Airline delay

Figure 2: Comparison of mean estimation error and standard deviation on two corrupted simulated
datasets and the airline delay dataset.
On the other hand  although there are some corrupted points with small inﬂuence  they typically
have a much larger inﬂuence than non-corrupted points. We give a theoretical explanation of this
phenomenon in §SI.3 (remarks 4 and 5).
Figure 2(a) and (b) shows the estimation error and the mean squared prediction error for different
subsample sizes. In this setting  computing IWS-LS is impractical (due to the exact leverage com-
putation) so we omit the results but we notice that aIWS-LS and aRWS-LS quickly improve over
the full least squares solution and the other randomized approximations in all simulation settings. In
all cases  inﬂuence based methods also achieve lower-variance estimates.
For 30% corruptions for a small number of samples ULURU outperforms the other subsampling
methods. However  as the number of samples increases  inﬂuence based methods start to outperform
OLS. Here  ULURU converges quickly to the OLS solution but is not able to overcome the bias
introduced by the corrupted datapoints. Results for 10% corruptions are shown in Figs. 5 and 6 and
we provide results on smaller corrupted datasets (to show the performance of IWS-LS) as well as
non-corrupted data simulated according to [13] in §SI.7.
Airline delay dataset The dataset consists of details of all commercial ﬂights in the USA over 20
years. Dataset along with visualisations available from http://stat-computing.org/dataexpo/2009/.
Selecting the ﬁrst ntrain = 13  000 US Airways ﬂights from January 2000 (corresponding to ap-
proximately 1.5 weeks) our goal is to predict the delay time of the next ntest = 5  000 US Airways
ﬂights. The features in this dataset consist of a binary vector representing origin-destination pairs
and a real value representing distance (p = 170).
The dataset might be expected to violate the usual i.i.d. sub-Gaussian design assumption of standard
linear regression since the length of delays are often very different depending on the day. For
example  delays may be longer due to public holidays or on weekends. Of course  such regular
events could be accounted for in the modelling step  but some unpredictable outliers such as weather
delay may also occur. Results are presented in Figure 2(c)  the RMSE is the error in predicted delay
time in minutes. Since the dataset is smaller  we can run IWS-LS to observe the accuracy of
aIWS-LS and aRWS-LS in comparison. For more than 3000 samples  these algorithm outperform
OLS and quickly approach IWS-LS. The result suggests that the corrupted observation model is a
good model for this dataset. Furthermore  ULURU is unable to achieve the full accuracy of the OLS
solution.

8 Conclusions
We have demonstrated theoretically and empirically under the generalised corrupted observation
model that inﬂuence weighted subsampling is able to signiﬁcantly reduce both the bias and variance
compared with the OLS estimator and other randomized approximations which do not take inﬂuence
into account. Importantly our fast approximation  aRWS-LS performs similarly to IWS-LS. We
ﬁnd ULURU quickly converges to the OLS estimate  although it is not able to overcome the bias
induced by the corrupted datapoints despite its two-step procedure. The performance of IWS-LS
relative to OLS in the airline delay problem suggests that the corrupted observation model is a more
realistic modelling scenario than the standard sub-Gaussian design model for some tasks. Software
is available at http://people.inf.ethz.ch/kgabriel/software.html.
Acknowledgements. We thank David Balduzzi  Cheng Soon Ong and the anonymous reviewers
for invaluable discussions  suggestions and comments.

8

References
[1] Nir Ailon and Edo Liberty. Fast dimension reduction using rademacher series on dual bch

codes. In 19th Annual ACM-SIAM Symposium on Discrete Algorithms  pages 1–9  2008.

[2] David A Belsley  Edwin Kuh  and Roy E Welsch. Regression Diagnostics. Identifying Inﬂu-

ential Data and Sources of Collinearity. Wiley  1981.

[3] Christos Boutsidis and Alex Gittens. Improved matrix algorithms via the Subsampled Ran-

domized Hadamard Transform. 2012. arXiv:1204.0062v4 [cs.DS].

[4] Yudong Chen and Constantine Caramanis. Orthogonal Matching Pursuit with Noisy and Miss-

ing Data: Low and High Dimensional Results. June 2012. arXiv:1206.0823.

[5] Yudong Chen and Constantine Caramanis. Noisy and Missing Data Regression: Distribution-

Oblivious Support Recovery. In International Conference on Machine Learning  2013.

[6] Yudong Chen  Constantine Caramanis  and Shie Mannor. Robust Sparse Regression under

Adversarial Corruption. In International Conference on Machine Learning  2013.

[7] P Dhillon  Y Lu  D P Foster  and L Ungar. New Subsampling Algorithms for Fast Least

Squares Regression. In Advances in Neural Information Processing Systems  2013.

[8] Petros Drineas  Malik Magdon-Ismail  Michael W Mahoney  and David P Woodruff. Fast ap-
proximation of matrix coherence and statistical leverage. September 2011. arXiv:1109.3843v2
[cs.DS].

[9] Petros Drineas  Michael W. Mahoney  and S. Muthukrishnan. Sampling algorithms for l2
regression and applications. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium
on Discrete Algorithm  SODA ’06  pages 1127–1136  New York  NY  USA  2006. ACM.

[10] Petros Drineas  Michael W Mahoney  S Muthukrishnan  and Tam´as Sarl´os. Faster least squares

approximation. Numerische Mathematik  117(2):219–249  2011.

[11] Daniel Hsu  Sham Kakade  and Tong Zhang. A tail inequality for quadratic forms of subgaus-

sian random vectors. Electron. Commun. Probab.  17:no. 52  1–6  2012.

[12] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing
data: Provable guarantees with nonconvexity. The Annals of Statistics  40(3):1637–1664  June
2012.

[13] Ping Ma  Michael W Mahoney  and Bin Yu. A Statistical Perspective on Algorithmic Lever-

aging. In proceedings of the International Conference on Machine Learning  2014.

[14] Michael W Mahoney.

arXiv:1104.5557v3 [cs.DS].

Randomized algorithms for matrices and data.

April 2011.

[15] Brian McWilliams and Giovanni Montana. Multi-view predictive partitioning in high dimen-

sions. Statistical Analysis and Data Mining  5(4):304–321  2012.

[16] Brian McWilliams and Giovanni Montana. Subspace clustering of high-dimensional data: a

predictive approach. Data Mining and Knowledge Discovery  28:736–772  2014.

[17] Joel A Tropp. Improved analysis of the subsampled randomized Hadamard transform. Novem-

ber 2010. arXiv:1011.1595v4 [math.NA].

[18] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. November

2010. arXiv:1011.3027.

[19] Roy E Welsch. Regression sensitivity analysis and bounded-inﬂuence estimation. In Evalua-

tion of econometric models  pages 153–167. Academic Press  1980.

9

,Brian McWilliams
Gabriel Krummenacher
Mario Lucic
Joachim Buhmann
Mario Drumond
Tao LIN
Martin Jaggi
Babak Falsafi