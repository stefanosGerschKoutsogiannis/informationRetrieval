2014,Mondrian Forests: Efficient Online Random Forests,Ensembles of randomized decision trees  usually referred to as random forests  are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test  making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests  however  require more training data than their batch counterpart to achieve comparable predictive performance. In this work  we use Mondrian processes (Roy and Teh  2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably  the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests  while being more than an order of magnitude faster  thus representing a better computation vs accuracy tradeoff.,Mondrian Forests: Efﬁcient Online Random Forests

Balaji Lakshminarayanan

Gatsby Unit

University College London

Daniel M. Roy

Department of Engineering
University of Cambridge

Yee Whye Teh

Department of Statistics
University of Oxford

Abstract

Ensembles of randomized decision trees  usually referred to as random forests 
are widely used for classiﬁcation and regression tasks in machine learning and
statistics. Random forests achieve competitive predictive performance and are
computationally efﬁcient to train and test  making them excellent candidates for
real-world prediction tasks. The most popular random forest variants (such as
Breiman’s random forest and extremely randomized trees) operate on batches
of training data. Online methods are now in greater demand. Existing online
random forests  however  require more training data than their batch counterpart
to achieve comparable predictive performance. In this work  we use Mondrian
processes (Roy and Teh  2009) to construct ensembles of random decision trees
we call Mondrian forests. Mondrian forests can be grown in an incremental/online
fashion and remarkably  the distribution of online Mondrian forests is the same as
that of batch Mondrian forests. Mondrian forests achieve competitive predictive
performance comparable with existing online random forests and periodically re-
trained batch random forests  while being more than an order of magnitude faster 
thus representing a better computation vs accuracy tradeoff.

Introduction

1
Despite being introduced over a decade ago  random forests remain one of the most popular machine
learning tools due in part to their accuracy  scalability  and robustness in real-world classiﬁcation
tasks [3]. (We refer to [6] for an excellent survey of random forests.) In this paper  we introduce a
novel class of random forests—called Mondrian forests (MF)  due to the fact that the underlying tree
structure of each classiﬁer in the ensemble is a so-called Mondrian process. Using the properties of
Mondrian processes  we present an efﬁcient online algorithm that agrees with its batch counterpart at
each iteration. Not only are online Mondrian forests faster and more accurate than recent proposals
for online random forest methods  but they nearly match the accuracy of state-of-the-art batch random
forest methods trained on the same dataset.
The paper is organized as follows: In Section 2  we describe our approach at a high-level  and in
Sections 3  4  and 5  we describe the tree structures  label model  and incremental updates/predictions
in more detail. We discuss related work in Section 6  demonstrate the excellent empirical performance
of MF in Section 7  and conclude in Section 8 with a discussion about future work.

2 Approach
Given N labeled examples (x1  y1)  . . .   (xN   yN ) 2 RD ⇥Y as training data  our task is to predict
labels y 2Y for unlabeled test points x 2 RD. We will focus on multi-class classiﬁcation where
Y := {1  . . .   K}  however  it is possible to extend the methodology to other supervised learning tasks
such as regression. Let X1:n := (x1  . . .   xn)  Y1:n := (y1  . . .   yn)  and D1:n := (X1:n  Y1:n).
A Mondrian forest classiﬁer is constructed much like a random forest: Given training data D1:N 
we sample an independent collection T1  . . .   TM of so-called Mondrian trees  which we will
describe in the next section. The prediction made by each Mondrian tree Tm is a distribution
pTm(y|x D1:N ) over the class label y for a test point x. The prediction made by the Mondrian
forest is the average 1
m=1 pTm(y|x D1:N ) of the individual tree predictions. As M ! 1  the

MPM

average converges at the standard rate to the expectation ET⇠MT( D1:N )[ pT (y|x D1:N )]  where
MT ( D1:N ) is the distribution of a Mondrian tree. As the limiting expectation does not depend on
M  we would not expect to see overﬁtting behavior as M increases. A similar observation was made
by Breiman in his seminal article [2] introducing random forests. Note that the averaging procedure
above is ensemble model combination and not Bayesian model averaging.
In the online learning setting  the training examples are presented one after another in a sequence
of trials. Mondrian forests excel in this setting: at iteration N + 1  each Mondrian tree T ⇠
MT ( D1:N ) is updated to incorporate the next labeled example (xN +1  yN +1) by sampling an
extended tree T 0 from a distribution MTx(  T DN +1). Using properties of the Mondrian process 
we can choose a probability distribution MTx such that T 0 = T on D1:N and T 0 is distributed
according to MT ( D1:N +1)  i.e. 

T ⇠ MT ( D1:N )

T 0 | T D1:N +1 ⇠ MTx(  T DN +1)

implies

T 0 ⇠ MT ( D1:N +1) .

(1)

Therefore  the distribution of Mondrian trees trained on a dataset in an incremental fashion is the
same as that of Mondrian trees trained on the same dataset in a batch fashion  irrespective of the
order in which the data points are observed. To the best of our knowledge  none of the existing online
random forests have this property. Moreover  we can sample from MTx(  T DN +1) efﬁciently: the
complexity scales with the depth of the tree  which is typically logarithmic in N.
While treating the online setting as a sequence of larger and larger batch problems is normally
computationally prohibitive  this approach can be achieved efﬁciently with Mondrian forests. In the
following sections  we deﬁne the Mondrian tree distribution MT ( D1:N )  the label distribution
pT (y|x D1:N )  and the update distribution MTx(  T DN +1).
3 Mondrian trees
For our purposes  a decision tree on RD will be a hierarchical  binary partitioning of RD and a rule
for predicting the label of test points given training data. More carefully  a rooted  strictly-binary
tree is a ﬁnite tree T such that every node in T is either a leaf or internal node  and every node is the
child of exactly one parent node  except for a distinguished root node  represented by ✏  which has no
parent. Let leaves(T) denote the set of leaf nodes in T. For every internal node j 2 T \ leaves(T) 
there are exactly two children nodes  represented by left(j) and right(j). To each node j 2 T  we
associate a block Bj ✓ RD of the input space as follows: We let B✏ := RD. Each internal node
j 2 T\ leaves(T) is associated with a splitj ⇠ j  where j 2{ 1  2  . . .   D} denotes the dimension
of the split and ⇠j denotes the location of the split along dimension j. We then deﬁne
and Bright(j) := {x 2 Bj : xj >⇠ j}.
We may write Bj =`j1  uj1⇤ ⇥ . . . ⇥`jD  ujD⇤  where `jd and ujd denote the `ower and upper
bounds  respectively  of the rectangular block Bj along dimension d. Put `j = {`j1 ` j2  . . .  ` jD}
and uj = {uj1  uj2  . . .   ujD}. The decision tree structure is represented by the tuple T = (T    ⇠).
We refer to Figure 1(a) for a simple illustration of a decision tree.
It will be useful to introduce some additional notation. Let parent(j) denote the parent of node j. Let
N (j) denote the indices of training data points at node j  i.e.  N (j) = {n 2{ 1  . . .   N} : xn 2 Bj}.
Let DN (j) = {XN (j)  YN (j)} denote the features and labels of training data points at node j. Let
jd and ux
jd denote the lower and upper bounds of training data points (hence the superscript x)
`x
jD⇤ ✓ Bj denote the
respectively in node j along dimension d. Let Bx
smallest rectangle that encloses the training data points in node j.

j1⇤ ⇥ . . . ⇥`x

Bleft(j) := {x 2 Bj : xj  ⇠j}

j1  ux

j =`x

jD  ux

(2)

3.1 Mondrian process distribution over decision trees
Mondrian processes  introduced by Roy and Teh [19]  are families {Mt : t 2 [0 1)} of random 
hierarchical binary partitions of RD such that Mt is a reﬁnement of Ms whenever t > s.1 Mondrian
processes are natural candidates for the partition structure of random decision trees  but Mondrian
1Roy and Teh [19] studied the distribution of {Mt : t  } and refered to  as the budget. See [18  Chp. 5]

for more details. We will refer to t as time  not be confused with discrete time in the online learning setting.

2

x1 > 0.37

x2 > 0.5

1

x2

 

Bj

⌅
F

⌅

F

 

0



0.42

0.7





x1 > 0.37

x2 > 0.5

1

x2

 

Bx
j

 

⌅
F

⌅

F

   

F F

⌅ ⌅

0

x1

1

1



   

F F

⌅ ⌅

0

x1

1

(a) Decision Tree

(b) Mondrian Tree

Figure 1: Example of a decision tree in [0  1]2 where x1 and x2 denote horizontal and vertical axis respectively:
Figure 1(a) shows tree structure and partition of a decision tree  while Figure 1(b) shows a Mondrian tree. Note
that the Mondrian tree is embedded on a vertical time axis  with each node associated with a time of split and
the splits are committed only within the range of the training data in each block (denoted by gray rectangles).
Let j denote the left child of the root: Bj = (0  0.37] ⇥ (0  1] denotes the block associated with red circles and
j ✓ Bj is the smallest rectangle enclosing the two data points.
Bx
processes on RD are  in general  inﬁnite structures that we cannot represent all at once. Because we
only care about the partition on a ﬁnite set of observed data  we introduce Mondrian trees  which
are restrictions of Mondrian processes to a ﬁnite set of points. A Mondrian tree T can be represented
by a tuple (T    ⇠  ⌧ )  where (T    ⇠) is a decision tree and ⌧ = {⌧j}j2T associates a time of split
⌧j  0 with each node j. Split times increase with depth  i.e.  ⌧j >⌧ parent(j). We abuse notation and
deﬁne ⌧parent(✏) = 0.
Given a non-negative lifetime parameter  and training data D1:n  the generative process for sampling
Mondrian trees from MT ( D1:n) is described in the following two algorithms:
Algorithm 1 SampleMondrianTree D1:n
1: Initialize: T = ;  leaves(T) = ;   = ;  ⇠ = ;  ⌧ = ;  N (✏) = {1  2  . . .   n}
2: SampleMondrianBlock✏ DN (✏) 
Algorithm 2 SampleMondrianBlockj DN (j) 
3: Sample E from exponential distribution with ratePd(ux

4: if ⌧parent(j) + E < then
Set ⌧j = ⌧parent(j) + E
5:
Sample split dimension j  choosing d with probability proportional to ux
6:
Sample split location ⇠j uniformly from interval [`x
7:
Set N (left(j)) = {n 2 N (j) : Xn j  ⇠j} and N (right(j)) = {n 2 N (j) : Xn j >⇠ j}
8:
SampleMondrianBlockleft(j) DN (left(j)) 
9:
SampleMondrianBlockright(j) DN (right(j)) 
10:
11: else
12:

. dimension-wise min and max
jd  `x
jd)

1: Add j to T
2: For all d  set `x

Set ⌧j =  and add j to leaves(T)

jd = min(XN (j) d)  ux

.j is an internal node

jd = max(XN (j) d)

.j is a leaf node

. Algorithm 2

jd  `x

jj   ux

jj ]

jd

✏ and ux

✏d  `x

✏ i.e. the lower and upper bounds of Bx

✏   given byPd(ux
E[E] = 1/Pd(ux

The procedure starts with the root node ✏ and recurses down the tree. In Algorithm 2  we ﬁrst
compute the `x
✏   the smallest rectangle enclosing
XN (✏). We sample E from an exponential distribution whose rate is the so-called linear dimension
✏d). Since ⌧parent(✏) = 0  E + ⌧parent(✏) = E. If E    the time of split
of Bx
is not within the lifetime ; hence  we assign ✏ to be a leaf node and the procedure halts. (Since
jd)  bigger rectangles are less likely to be leaf nodes.) Else  ✏ is an internal

node and we sample a split (✏ ⇠ ✏) from the uniform split distribution on Bx
ﬁrst sample the dimension ✏  taking the value d with probability proportional to ux
sample the split location ⇠✏ uniformly from the interval [`x
along the left and right children.
Mondrian trees differ from standard decision trees (e.g. CART  C4.5) in the following ways: (i)
the splits are sampled independent of the labels YN (j); (ii) every node j is associated with a split

✏ . More precisely  we
✏d  and then
✏✏]. The procedure then recurses

jd  `x

✏d  `x

✏✏  ux

3

j and not the whole of Bj. No commitment is made in Bj \ Bx

time denoted by ⌧j; (iii) the lifetime parameter  controls the total number of splits (similar to the
maximum depth parameter for standard decision trees); (iv) the split represented by an internal node
j holds only within Bx
j . Figure 1
illustrates the difference between decision trees and Mondrian trees.
Consider the family of distributions MT (  F )  where F ranges over all possible ﬁnite sets of data
points. Due to the fact that these distributions are derived from that of a Mondrian process on RD
restricted to a set F of points  the family MT ( ·) will be projective. Intuitively  projectivity implies
that the tree distributions possess a type of self-consistency. In words  if we sample a Mondrian
tree T from MT (  F ) and then restrict the tree T to a subset F 0 ✓ F of points  then the restricted
tree T 0 has distribution MT (  F 0). Most importantly  projectivity gives us a consistent way to
extend a Mondrian tree on a data set D1:N to a larger data set D1:N +1. We exploit this property
to incrementally grow a Mondrian tree: we instantiate the Mondrian tree on the observed training
data points; upon observing a new data point DN +1  we extend the Mondrian tree by sampling from
the conditional distribution of a Mondrian tree on D1:N +1 given its restriction to D1:N  denoted
by MTx(  T DN +1) in (1). Thus  a Mondrian process on RD is represented only where we have
observed training data.

4 Label distribution: model  hierarchical prior  and predictive posterior
So far  our discussion has been focused on the tree structure. In this section  we focus on the predictive
label distribution  pT (y|x D1:N )  for a tree T = (T    ⇠  ⌧ )  dataset D1:N  and test point x. Let
leaf(x) denote the unique leaf node j 2 leaves(T) such that x 2 Bj. Intuitively  we want the
predictive label distribution at x to be a smoothed version of the empirical distribution of labels
for points in Bleaf(x) and in Bj0 for nearby nodes j0. We achieve this smoothing via a hierarchical
Bayesian approach: every node is associated with a label distribution  and a prior is chosen under
which the label distribution of a node is similar to that of its parent’s. The predictive pT (y|x D1:N )
is then obtained via marginalization.
As is common in the decision tree literature  we assume the labels within each block are independent
of X given the tree structure. For every j 2 T  let Gj denote the distribution of labels at node j  and
let G = {Gj : j 2 T} be the set of label distributions at all the nodes in the tree. Given T and G 
the predictive label distribution at x is p(y|x  T G) = Gleaf(x)  i.e.  the label distribution at the node
leaf(x). In this paper  we focus on the case of categorical labels taking values in the set {1  . . .   K} 
and so we abuse notation and write Gj k for the probability that a point in Bj is labeled k.
We model the collection Gj  for j 2 T  as a hierarchy of normalized stable processes (NSP) [24]. A
NSP prior is a distribution over distributions and is a special case of the Pitman-Yor process (PYP)
prior where the concentration parameter is taken to zero [17].2 The discount parameter d 2 (0  1)
controls the variation around the base distribution; if Gj ⇠ NSP(d  H)  then E[Gjk] = Hk and
Var[Gjk] = (1  d)Hk(1  Hk). We use a hierarchical NSP (HNSP) prior over Gj as follows:

G✏|H ⇠ NSP(d✏  H) 

and

(3)

This hierarchical prior was ﬁrst proposed by Wood et al. [24]. Here we take the base distribution H

Gj|Gparent(j) ⇠ NSP(dj  Gparent(j)).
to be the uniform distribution over the K labels  and set dj = exp(⌧j  ⌧parent(j)).
Given training data D1:N  the predictive distribution pT (y|x D1:N ) is obtained by integrating over G 
i.e.  pT (y|x D1:N ) = EG⇠pT (G|D1:N )[Gleaf(x) y] = Gleaf(x) y  where the posterior pT (G|D1:N ) /
pT (G)QN
n=1 Gleaf(xn) yn. Posterior inference in the HNSP  i.e.  computation of the posterior means
Gleaf(x)  is a special case of posterior inference in the hierarchical PYP (HPYP). In particular  Teh
[22] considers the HPYP with multinomial likelihood (in the context of language modeling). The
model considered here is a special case of [22]. Exact inference is intractable and hence we resort to
approximations. In particular  we use a fast approximation known as the interpolated Kneser-Ney
(IKN) smoothing [22]  a popular technique for smoothing probabilities in language modeling [13].
The IKN approximation in [22] can be extended in a straightforward fashion to the online setting 
and the computational complexity of adding a new training instance is linear in the depth of the tree.
We refer the reader to Appendix A for further details.

2Taking the discount parameter to zero leads to a Dirichlet process . Hierarchies of NSPs admit more tractable

approximations than hierarchies of Dirichlet processes [24]  hence our choice here.

4

5 Online training and prediction
In this section  we describe the family of distributions MTx(  T DN +1)  which are used to incre-
mentally add a data point  DN +1  to a tree T . These updates are based on the conditional Mondrian
algorithm [19]  specialized to a ﬁnite set of points. In general  one or more of the following three
operations may be executed while introducing a new data point: (i) introduction of a new split ‘above’
an existing split  (ii) extension of an existing split to the updated extent of the block and (iii) splitting
an existing leaf node into two children. To the best of our knowledge  existing online decision trees
use just the third operation  and the ﬁrst two operations are unique to Mondrian trees. The complete
pseudo-code for incrementally updating a Mondrian tree T with a new data point D according to
MTx(  T D) is described in the following two algorithms. Figure 2 walks through the algorithms
on a toy dataset.

Algorithm 3 ExtendMondrianTree(T   D)
1: Input: Tree T = (T    ⇠  ⌧ )  new training instance D = (x  y)
2: ExtendMondrianBlock(T  ✏  D)
Algorithm 4 ExtendMondrianBlock(T   j  D)
1: Set e` = max(`x
2: Sample E from exponential distribution with ratePd(e`

j  x  0) and eu = max(x  ux

j   0)

. Algorithm 4

. e` = eu = 0D if x 2 Bx
d + eu
d )
. introduce new parent for node j

j

3: if ⌧parent(j) + E <⌧ j then
4:
5:
6:
7:
8:
9:
10: else
11:
12:
13:
14:

Sample split dimension   choosing d with probability proportional to e`
Sample split location ⇠ uniformly from interval [ux
Insert a new node ˜| just above node j in the tree  and a new leaf j00  sibling to j  where

j   x] if x > ux

j  else [x ` x

d + eu
d

j ].

j   x)

j   x)

j   x)  ux

j   x)  ux

˜| = min(`x

˜| = max(ux

j max(ux

˜| =   ⇠˜| = ⇠  ⌧˜| = ⌧parent(j) + E  `x
j00 = left(˜|) iff x˜|  ⇠˜|
SampleMondrianBlockj00 D 
Update `x
j min(`x
if j /2 leaves(T) then
if xj  ⇠j then child(j) = left(j) else child(j) = right(j)
ExtendMondrianBlock(T   child(j) D)

. update extent of node j
. return if j is a leaf node  else recurse down the tree
. recurse on child containing D
In practice  random forest implementations stop splitting a node when all the labels are identical and
assign it to be a leaf node. To make our MF implementation comparable  we ‘pause’ a Mondrian
block when all the labels are identical; if a new training instance lies within Bj of a paused leaf
node j and has the same label as the rest of the data points in Bj  we continue pausing the Mondrian
block. We ‘un-pause’ the Mondrian block when there is more than one unique label in that block.
Algorithms 9 and 10 in the supplementary material discuss versions of SampleMondrianBlock and
ExtendMondrianBlock for paused Mondrians.
Prediction using Mondrian tree Let x denote a test data point. If x is already ‘contained’ in
the tree T   i.e.  if x 2 Bx
j for some leaf j 2 leaves(T)  then the prediction is taken to be Gleaf(x).
Otherwise  we somehow need to incorporate x. One choice is to extend T by sampling T 0 from
MTx(  T  x) as described in Algorithm 3  and set the prediction to Gj  where j 2 leaves(T0) is the
leaf node containing x. A particular extension T 0 might lead to an overly conﬁdent prediction; hence 
we average over every possible extension T 0. This integration can be carried out analytically and the
computational complexity is linear in the depth of the tree. We refer to Appendix B for further details.

6 Related work
The literature on random forests is vast and we do not attempt to cover it comprehensively; we provide
a brief review here and refer to [6] and [8] for a recent review of random forests in batch and online
settings respectively. Classic decision tree induction procedures choose the best split dimension and
location from all candidate splits at each node by optimizing some suitable quality criterion (e.g.
information gain) in a greedy manner. In a random forest  the individual trees are randomized to
de-correlate their predictions. The most common strategies for injecting randomness are (i) bagging
[1] and (ii) randomly subsampling the set of candidate splits within each node.

5

1

x2

0

1

x2

 c

1

x2

 b

 a

 b

 a

x1
(a)

1

0

x1
(b)

1

0

 c

1

x2

 a

1

0

 d

 b

x1
(d)

 c

1

x2

 a

1

0

 d

 b

x1
(e)

 c

1

x2

 a

1

0

 c

1

 d

 b

x1
(f)

x1 > 0.75

 b

 a

x1
(c)

x1 > 0.75

0

1.01

2.42

3.97








1



x2 > 0.23

x2 > 0.23

x2 > 0.23

x1 > 0.47

a
(g)

b

a

b

c

a

b

d

c

(h)

(i)

Figure 2: Online learning with Mondrian trees on a toy dataset: We assume that  = 1  D = 2 and add one
data point at each iteration. For simplicity  we ignore class labels and denote location of training data with red
circles. Figures 2(a)  2(c) and 2(f) show the partitions after the ﬁrst  second and third iterations  respectively 
with the intermediate ﬁgures denoting intermediate steps. Figures 2(g)  2(h) and 2(i) show the trees after the ﬁrst 
second and third iterations  along with a shared vertical time axis.

At iteration 1  we have two training data points  labeled as a  b. Figures 2(a) and 2(g) show the partition
and tree structure of the Mondrian tree. Note that even though there is a split x2 > 0.23 at time t = 2.42  we
commit this split only within Bx

j (shown by the gray rectangle).

At iteration 2  a new data point c is added. Algorithm 3 starts with the root node and recurses down the
✏ by computing the additional extent e` and eu. In
tree. Algorithm 4 checks if the new data point lies within Bx
this case  c does not lie within Bx
✏ . Let Rab and Rabc respectively denote the small gray rectangle (enclosing
a  b) and big gray rectangle (enclosing a  b  c) in Figure 2(b). While extending the Mondrian from Rab to Rabc 
we could either introduce a new split in Rabc outside Rab or extend the split in Rab to the new range. To
choose between these two options  we sample the time of this new split: we ﬁrst sample E from an exponential
distribution whose rate is the sum of the additional extent  i.e. Pd(e`
d )  and set the time of the new split
to E + ⌧parent(✏). If E + ⌧parent(✏)  ⌧✏  this new split in Rabc can precede the old split in Rab and a split is
sampled in Rabc outside Rab. In Figures 2(c) and 2(h)  E + ⌧parent(✏) = 1.01 + 0  2.42  hence a new split
j   the higher the ratePd(e`
x1 > 0.75 is introduced. The farther a new data point x is from Bx
d )  and
d ). A
subsequently the higher the probability of a new split being introduced  since E[E] = 1/Pd(e`
In the ﬁnal iteration  we add data point d. In Figure 2(d)  the data point d lies within the extent of the root
node  hence we traverse to the left side of the root and update Bx
j of the internal node containing {a  b} to
include d. We could either introduce a new split or extend the split x2 > 0.23. In Figure 2(e)  we extend the
split x2 > 0.23 to the new extent  and traverse to the leaf node in Figure 2(h) containing b. In Figures 2(f) and
2(i)  we sample E = 1.55 and since ⌧parent(j) + E = 2.42 + 1.55 = 3.97   = 1  we introduce a new split
x1 > 0.47.

new split in Rabc is sampled such that it is consistent with the existing partition structure in Rab (i.e.  the new
split cannot slice through Rab).

d + eu
d + eu

d + eu

Two popular random forest variants in the batch setting are Breiman-RF [2] and Extremely randomized
trees (ERT) [12]. Breiman-RF uses bagging and furthermore  at each node  a random k-dimensional
subset of the original D features is sampled. ERT chooses a k dimensional subset of the features and
then chooses one split location each for the k features randomly (unlike Breiman-RF which considers
all possible split locations along a dimension). ERT does not use bagging. When k = 1  the ERT
trees are totally randomized and the splits are chosen independent of the labels; hence the ERT-1
method is very similar to MF in the batch setting in terms of tree induction. (Note that unlike ERT 
MF uses HNSP to smooth predictive estimates and allows a test point to branch off into its own node.)
Perfect random trees (PERT)  proposed by Cutler and Zhao [7] for classiﬁcation problems  produce
totally randomized trees similar to ERT-1  although there are some slight differences [12].
Existing online random forests (ORF-Saffari [20] and ORF-Denil [8]) start with an empty tree and
grow the tree incrementally. Every leaf of every tree maintains a list of k candidate splits and
associated quality scores. When a new data point is added  the scores of the candidate splits at the
corresponding leaf node are updated. To reduce the risk of choosing a sub-optimal split based on
noisy quality scores  additional hyper parameters such as the minimum number of data points at a
leaf node before a decision is made and the minimum threshold for the quality criterion of the best
split  are used to assess ‘conﬁdence’ associated with a split. Once these criteria are satisﬁed at a leaf
node  the best split is chosen (making this node an internal node) and its two children are the new
leaf nodes (with their own candidate splits)  and the process is repeated. These methods could be

6

memory inefﬁcient for deep trees due to the high cost associated with maintaining candidate quality
scores for the fringe of potential children [8].
There has been some work on incremental induction of decision trees  e.g. incremental CART [5] 
ITI [23]  VFDT [11] and dynamic trees [21]  but to the best of our knowledge  these are focused on
learning decision trees and have not been generalized to online random forests. We do not compare
MF to incremental decision trees  since random forests are known to outperform single decision trees.
Bayesian models of decision trees [4  9] typically specify a distribution over decision trees; such
distributions usually depend on X and lack the projectivity property of the Mondrian process. More
importantly  MF performs ensemble model combination and not Bayesian model averaging over
decision trees. (See [10] for a discussion on the advantages of ensembles over single models  and
[15] for a comparison of Bayesian model averaging and model combination.)

7 Empirical evaluation
The purpose of these experiments is to evaluate the predictive performance (test accuracy) of MF
as a function of (i) fraction of training data and (ii) training time. We divide the training data into
100 mini-batches and we compare the performance of online random forests (MF  ORF-Saffari [20])
to batch random forests (Breiman-RF  ERT-k  ERT-1) which are trained on the same fraction of the
training data. (We compare MF to dynamic trees as well; see Appendix F for more details.) Our
scripts are implemented in Python. We implemented the ORF-Saffari algorithm as well as ERT in
Python for timing comparisons. The scripts can be downloaded from the authors’ webpages. We
did not implement the ORF-Denil [8] algorithm since the predictive performance reported in [8] is
very similar to that of ORF-Saffari and the computational complexity of the ORF-Denil algorithm is
worse than that of ORF-Saffari. We used the Breiman-RF implementation in scikit-learn [16].3
We evaluate on four of the ﬁve datasets used in [20] — we excluded the mushroom dataset as even
very simple logical rules achieve > 99% accuracy on this dataset.4 We re-scaled the datasets such
that each feature takes on values in the range [0  1] (by subtracting the min value along that dimension
and dividing by the range along that dimension  where range = max  min).
As is common in the random forest literature [2]  we set the number of trees M = 100. For Mondrian
forests  we set the lifetime  = 1 and the HNSP discount parameter  = 10D. For ORF-Saffari  we
set num epochs = 20 (number of passes through the training data) and set the other hyper parameters
to the values used in [20]. For Breiman-RF and ERT  the hyper parameters are set to default values.
We repeat each algorithm with ﬁve random initializations and report the mean performance. The
results are shown in Figure 3. (The * in Breiman-RF* indicates scikit-learn implementation.)
Comparing test accuracy vs fraction of training data on usps  satimages and letter datasets  we
observe that MF achieves accuracy very close to the batch RF versions (Breiman-RF  ERT-k 
ERT-1) trained on the same fraction of the data. MF signiﬁcantly outperforms ORF-Saffari
trained on the same fraction of training data. In batch RF versions  the same training data can
be used to evaluate candidate splits at a node and its children. However  in the online RF versions
(ORF-Saffari and ORF-Denil)  incoming training examples are used to evaluate candidate splits just
at a current leaf node and new training data are required to evaluate candidate splits every time a
new leaf node is created. Saffari et al. [20] recommend multiple passes through the training data to
increase the effective number of training samples. In a realistic streaming data setup  where training
examples cannot be stored for multiple passes  MF would require signiﬁcantly fewer examples than
ORF-Saffari to achieve the same accuracy.
Comparing test accuracy vs training time on usps  satimages and letter datasets  we observe that MF
is at least an order of magnitude faster than re-trained batch versions and ORF-Saffari. For
ORF-Saffari  we plot test accuracy at the end of every additional pass; hence it contains additional
markers compared to the top row which plots results after a single pass. Re-training batch RF using
100 mini-batches is unfair to MF; in a streaming data setup where the model is updated when a
new training instance arrives  MF would be signiﬁcantly faster than the re-trained batch versions.

3The scikit-learn implementation uses highly optimized C code  hence we do not compare our runtimes with
the scikit-learn implementation. The ERT implementation in scikit-learn achieves very similar test accuracy as
our ERT implementation  hence we do not report those results here.

4https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names

7

Assuming trees are balanced after adding each data point  it can be shown that computational cost of
MF scales as O(N log N ) whereas that of re-trained batch RF scales as O(N 2 log N ) (Appendix C).
Appendix E shows that the average depth of the forests trained on above datasets scales as O(log N ).
It is remarkable that choosing splits independent of labels achieves competitive classiﬁcation per-
formance. This phenomenon has been observed by others as well—for example  Cutler and Zhao
[7] demonstrate that their PERT classiﬁer (which is similar to batch version of MF) achieves test
accuracy comparable to Breiman-RF on many real world datasets. However  in the presence of
irrelevant features  methods which choose splits independent of labels (MF  ERT-1) perform worse
than Breiman-RF and ERT-k (but still better than ORF-Saffari) as indicated by the results on the
dna dataset. We trained MF and ERT-1 using just the most relevant 60 attributes amongst the 180
attributes5—these results are indicated as MF† and ERT-1† in Figure 3. We observe that  as expected 
ﬁltering out irrelevant features signiﬁcantly improves performance of MF and ERT-1.

0.92

0.90

0.88

0.86

0.84

0.82

0.80

0.78

1.00

0.95

0.90

0.85

0.80

0.75

0.70

0.65

0.60

0.95

0.90

0.85

0.80

0.75

0.70

0.65

0.60

0.55

MF†
ERT-1†

0.7

0.8

0.9

1.0

0.76

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

0.55

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

0.50

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

MF
ERT-k
ERT-1
ORF-Saffari
Breiman-RF*
0.4
0.6

0.5

0.95

0.90

0.85

0.80

0.75

0.70

0.65

0.60

0.1

0.2

0.3

1.00

0.95

0.90

0.85

0.80

0.75

0.70

0.65

0.90

0.85

0.80

MF
ERT-k
ERT-1
ORF-Saffari

1.00

0.95

0.90

0.85

0.80

0.75

0.70

0.65

0.60

MF†
ERT-1†

1.1

1.0

0.9

0.8

0.7

0.6

104

0.55

101

102

103

letter

104

105

0.5

101

103

104

102

dna

0.60

101

102

103

usps

104

105

0.75

101

102

satimages

103

Figure 3: Results on various datasets: y-axis is test accuracy in both rows. x-axis is fraction of training data
for the top row and training time (in seconds) for the bottom row. We used the pre-deﬁned train/test split.
For usps dataset D = 256  K = 10  Ntrain = 7291  Ntest = 2007; for satimages dataset D = 36  K =
6  Ntrain = 3104  Ntest = 2000; letter dataset D = 16  K = 26  Ntrain = 15000  Ntest = 5000; for dna dataset
D = 180  K = 3  Ntrain = 1400  Ntest = 1186.
8 Discussion
We have introduced Mondrian forests  a novel class of random forests  which can be trained incre-
mentally in an efﬁcient manner. MF signiﬁcantly outperforms existing online random forests in
terms of training time as well as number of training instances required to achieve a particular test
accuracy. Remarkably  MF achieves competitive test accuracy to batch random forests trained on the
same fraction of the data. MF is unable to handle lots of irrelevant features (since splits are chosen
independent of the labels)—one way to use labels to guide splits is via recently proposed Sequential
Monte Carlo algorithm for decision trees [14]. The computational complexity of MF is linear in the
number of dimensions (since rectangles are represented explicitly) which could be expensive for
high dimensional data; we will address this limitation in future work. Random forests have been
tremendously inﬂuential in machine learning for a variety of tasks; hence lots of other interesting
extensions of this work are possible  e.g. MF for regression  theoretical bias-variance analysis of MF 
extensions of MF that use hyperplane splits instead of axis-aligned splits.
Acknowledgments
We would like to thank Charles Blundell  Gintare Dziugaite  Creighton Heaukulani  Jos´e Miguel
Hern´andez-Lobato  Maria Lomeli  Alex Smola  Heiko Strathmann and Srini Turaga for helpful
discussions and feedback on drafts. BL gratefully acknowledges generous funding from the Gatsby
Charitable Foundation. This research was carried out in part while DMR held a Research Fellowship
at Emmanuel College  Cambridge  with funding also from a Newton International Fellowship through
the Royal Society. YWT’s research leading to these results was funded in part by the European
Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013)
ERC grant agreement no. 617411.

5https://www.sgi.com/tech/mlc/db/DNA.names

8

References
[1] L. Breiman. Bagging predictors. Mach. Learn.  24(2):123–140  1996.
[2] L. Breiman. Random forests. Mach. Learn.  45(1):5–32  2001.
[3] R. Caruana and A. Niculescu-Mizil. An empirical comparison of supervised learning algorithms.

In Proc. Int. Conf. Mach. Learn. (ICML)  2006.

[4] H. A. Chipman  E. I. George  and R. E. McCulloch. Bayesian CART model search. J. Am. Stat.

Assoc.  pages 935–948  1998.

[5] S. L. Crawford. Extensions to the CART algorithm. Int. J. Man-Machine Stud.  31(2):197–217 

1989.

[6] A. Criminisi  J. Shotton  and E. Konukoglu. Decision forests: A uniﬁed framework for
classiﬁcation  regression  density estimation  manifold learning and semi-supervised learning.
Found. Trends Comput. Graphics and Vision  7(2–3):81–227  2012.

[7] A. Cutler and G. Zhao. PERT - Perfect Random Tree Ensembles. Comput. Sci. and Stat.  33:

490–497  2001.

[8] M. Denil  D. Matheson  and N. de Freitas. Consistency of online random forests. In Proc. Int.

Conf. Mach. Learn. (ICML)  2013.

[9] D. G. T. Denison  B. K. Mallick  and A. F. M. Smith. A Bayesian CART algorithm. Biometrika 

85(2):363–377  1998.

[10] T. G. Dietterich. Ensemble methods in machine learning. In Multiple classiﬁer systems  pages

1–15. Springer  2000.

[11] P. Domingos and G. Hulten. Mining high-speed data streams. In Proc. 6th ACM SIGKDD Int.

Conf. Knowl. Discov. Data Min. (KDD)  pages 71–80. ACM  2000.

[12] P. Geurts  D. Ernst  and L. Wehenkel. Extremely randomized trees. Mach. Learn.  63(1):3–42 

2006.

[13] J. T. Goodman. A bit of progress in language modeling. Comput. Speech Lang.  15(4):403–434 

2001.

[14] B. Lakshminarayanan  D. M. Roy  and Y. W. Teh. Top-down particle ﬁltering for Bayesian

decision trees. In Proc. Int. Conf. Mach. Learn. (ICML)  2013.

[15] T. P. Minka. Bayesian model averaging is not model combination. MIT Media Lab note.

http://research.microsoft.com/en-us/um/people/minka/papers/bma.html  2000.

[16] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel 
P. Prettenhofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher 
M. Perrot  and E. Duchesnay. Scikit-learn: Machine Learning in Python. J. Mach. Learn. Res. 
12:2825–2830  2011.

[17] J. Pitman. Combinatorial stochastic processes  volume 32. Springer  2006.
[18] D. M. Roy. Computability  inference and modeling in probabilistic programming. PhD thesis 

Massachusetts Institute of Technology  2011. http://danroy.org/papers/Roy-PHD-2011.pdf.

[19] D. M. Roy and Y. W. Teh. The Mondrian process. In Adv. Neural Inform. Proc. Syst. (NIPS) 

volume 21  pages 27–36  2009.

[20] A. Saffari  C. Leistner  J. Santner  M. Godec  and H. Bischof. On-line random forests. In

Computer Vision Workshops (ICCV Workshops). IEEE  2009.

[21] M. A. Taddy  R. B. Gramacy  and N. G. Polson. Dynamic trees for learning and design. J. Am.

Stat. Assoc.  106(493):109–123  2011.

[22] Y. W. Teh. A hierarchical Bayesian language model based on Pitman–Yor processes. In Proc.
21st Int. Conf. on Comp. Ling. and 44th Ann. Meeting Assoc. Comp. Ling.  pages 985–992.
Assoc. for Comp. Ling.  2006.

[23] P. E. Utgoff. Incremental induction of decision trees. Mach. Learn.  4(2):161–186  1989.
[24] F. Wood  C. Archambeau  J. Gasthaus  L. James  and Y. W. Teh. A stochastic memoizer for

sequence data. In Proc. Int. Conf. Mach. Learn. (ICML)  2009.

9

,Balaji Lakshminarayanan
Daniel Roy
Yee Whye Teh