2016,Optimal Cluster Recovery in the Labeled Stochastic Block Model,We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number $K$ of clusters of sizes linearly growing with the global population of items $n$. Every pair of items is labeled independently at random  and label $\ell$ appears with probability $p(i j \ell)$ between two items in clusters indexed by $i$ and $j$  respectively. The objective is to reconstruct the clusters from the observation of these random labels.   Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters  or with a vanishing proportion of misclassified items  or exactly matching the true clusters. We find  the set of parameters such that there exists a clustering algorithm with at most $s$ misclassified items in average under the general LSBM and for any $s=o(n)$  which solves one open problem raised in \cite{abbe2015community}. We further develop an algorithm  based on simple spectral methods  that achieves this fundamental performance limit within $O(n \mbox{polylog}(n))$ computations and without the a-priori knowledge of the model parameters.,Optimal Cluster Recovery

in the Labeled Stochastic Block Model

Se-Young Yun

CNLS  Los Alamos National Lab.

Los Alamos  NM 87545

syun@lanl.gov

Alexandre Proutiere

Automatic Control Dept.  KTH

Stockholm 100-44  Sweden

alepro@kth.se

Abstract

We consider the problem of community detection or clustering in the labeled
Stochastic Block Model (LSBM) with a ﬁnite number K of clusters of sizes
linearly growing with the global population of items n. Every pair of items is
labeled independently at random  and label (cid:96) appears with probability p(i  j  (cid:96))
between two items in clusters indexed by i and j  respectively. The objective is to
reconstruct the clusters from the observation of these random labels.
Clustering under the SBM and their extensions has attracted much attention recently.
Most existing work aimed at characterizing the set of parameters such that it is
possible to infer clusters either positively correlated with the true clusters  or with
a vanishing proportion of misclassiﬁed items  or exactly matching the true clusters.
We ﬁnd the set of parameters such that there exists a clustering algorithm with
at most s misclassiﬁed items in average under the general LSBM and for any
s = o(n)  which solves one open problem raised in [2]. We further develop
an algorithm  based on simple spectral methods  that achieves this fundamental
performance limit within O(npolylog(n)) computations and without the a-priori
knowledge of the model parameters.

1

Introduction

Community detection consists in extracting (a few) groups of similar items from a large global
population  and has applications in a wide spectrum of disciplines including social sciences  biology 
computer science  and statistical physics. The communities or clusters of items are inferred from the
observed pair-wise similarities between items  which  most often  are represented by a graph whose
vertices are items and edges are pairs of items known to share similar features.
The stochastic block model (SBM)  introduced three decades ago in [12]  constitutes a natural
performance benchmark for community detection  and has been  since then  widely studied. In the
SBM  the set of items V = {1  . . .   n} are partitioned into K non-overlapping clusters V1  . . .  VK 
that have to be recovered from an observed realization of a random graph. In the latter  an edge
between two items belonging to clusters Vi and Vj  respectively  is present with probability p(i  j) 
independently of other edges. The analyses presented in this paper apply to the SBM  but also to the
labeled stochastic block model (LSBM) [11]  a more general model to describe the similarities of
items. There  the observation of the similarity between two items comes in the form of a label taken
from a ﬁnite set L = {0  1  . . .   L}  and label (cid:96) is observed between two items in clusters Vi and Vj 
respectively  with probability p(i  j  (cid:96))  independently of other labels. The standard SBM can be seen
as a particular instance of its labeled counterpart with two possible labels 0 and 1  and where the
edges present (resp. absent) in the SBM correspond to item pairs with label 1 (resp. 0). The problem
of cluster recovery under the LSBM consists in inferring the hidden partition V1  . . .  VK from the
observation of the random labels on each pair of items.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Over the last few years  we have seen remarkable progresses for the problem of cluster recovery under
the SBM (see [7] for an exhaustive literature review)  highlighting its scientiﬁc relevance and richness.
Most recent work on the SBM aimed at characterizing the set of parameters (i.e.  the probabilities
p(i  j) that there exists an edge between nodes in clusters i and j for 1 ≤ i  j ≤ K) such that some
qualitative recovery objectives can or cannot be met. For sparse scenarios where the average degree
of items in the graph is O(1)  parameters under which it is possible to extract clusters positively
correlated with the true clusters have been identiﬁed [5  18  16]. When the average degree of the
graph is ω(1)  one may predict the set of parameters allowing a cluster recovery with a vanishing (as
n grows large) proportion of misclassiﬁed items [22  17]  but one may also characterize parameters
for which an asymptotically exact cluster reconstruction can be achieved [1  21  8  17  2  3  13].
In this paper  we address the ﬁner and more challenging question of determining  under the general
LSBM  the minimal number of misclassiﬁed items given the parameters of the model. Speciﬁcally 
for any given s = o(n)  our goal is to identify the set of parameters such that it is possible to devise a
clustering algorithm with at most s misclassiﬁed items. Of course  if we achieve this goal  we shall
recover all the aforementioned results on the SBM.

Main results. We focus on the labeled SBM as described above  and where each item is assigned
to cluster Vk with probability αk > 0  independently of other items. We assume w.l.o.g.
that
α1 ≤ α2 ≤ ··· ≤ αK. We further assume that α = (α1  . . .   αK) does not depend on the total
population of items n. Conditionally on the assignment of items to clusters  the pair or edge
(v  w) ∈ V 2 has label (cid:96) ∈ L = {0  1  . . .   L} with probability p(i  j  (cid:96))  when v ∈ Vi and w ∈ Vj.
W.l.o.g.  0 is the most frequent label  i.e.  0 = arg max(cid:96)
j=1 αiαjp(i  j  (cid:96)). Throughout the
paper  we typically assume that ¯p = o(1) and ¯pn = ω(1) where ¯p = maxi j (cid:96)≥1 p(i  j  (cid:96)) denotes the
maximum probability of observing a label different than 0. We shall explicitly state whether these
assumption are made when deriving our results. In the standard SBM  the second assumption means
that the average degree of the corresponding random graph is ω(1). This also means that we can hope
to recover clusters with a vanishing proportion of misclassiﬁed items. We ﬁnally make the following
assumption: there exist positive constants η and ε such that for every i  j  k ∈ [K] = {1  . . .   K} 

(cid:80)K

(cid:80)K

i=1

(cid:80)K

k=1

(cid:80)L
(cid:96)=1(p(i  k  (cid:96)) − p(j  k  (cid:96)))2

≥ ε.

¯p2

(A1) ∀(cid:96) ∈ L 

p(i  j  (cid:96))
p(i  k  (cid:96))

≤ η

and

(A2)

(A2) imposes a certain separation between the clusters. For example  in the standard SBM with two
communities  p(1  1  1) = p(2  2  1) = ξ  and p(1  2  1) = ζ  (A2) is equivalent to 2(ξ − ζ)2/ξ2 ≥ .
In summary  the LSBM is parametrized by α and p = (p(i  j  (cid:96)))1≤i j≤K 0≤(cid:96)≤L  and recall that α
does not depend on n  whereas p does.
For the above LSBM  we derive  for any arbitrary s = o(n)  a necessary condition under which
there exists an algorithm inferring clusters with s misclassiﬁed items. We further establish that
under this condition  a simple extension of spectral algorithms extract communities with less than s
misclassiﬁed items. To formalize these results  we introduce the divergence of (α  p). We denote by
p(i) the K × (L + 1) matrix whose element on the j-th row and the ((cid:96) + 1)-th column is p(i  j  (cid:96))
and denote by p(i  j) ∈ [0  1]L+1 the vector describing the probability distribution of the label of a
pair of items in Vi and Vj  respectively. Let P K×(L+1) denote the set of K × (L + 1) matrices such
that each row represents a probability distribution. The divergence D(α  p) of (α  p) is deﬁned as
follows: D(α  p) = mini j:i(cid:54)=j DL+(α  p(i)  p(j)) with

DL+(α  p(i)  p(j)) =

KL(y(k)  p(i  k)) =(cid:80)L

min

y∈P K×(L+1)

max

αkKL(y(k)  p(i  k)) 

αkKL(y(k)  p(j  k))

k=1

k=1

where KL denotes the Kullback-Leibler divergence between two label distributions  i.e. 

p(i k (cid:96)). Finally  we denote by επ(n) the number of misclas-
siﬁed items under the clustering algorithm π  and by E[επ(n)] its expectation (with respect to the
randomness in the LSBM and in the algorithm).

(cid:96)=0 y(k  (cid:96)) log y(k (cid:96))

We ﬁrst derive a tight lower bound on the average number of misclassiﬁed items when the latter is
o(n). Note that such a bound was unknown even for the SBM [2].

Theorem 1 Assume that (A1) and (A2) hold  and that ¯pn = ω(1). Let s = o(n).
If there
exists a clustering algorithm π misclassifying in average less than s items asymptotically  i.e. 

2

(cid:40) K(cid:88)

K(cid:88)

(cid:41)

lim supn→∞

E[επ(n)]

s

≤ 1  then the parameters (α  p) of the LSBM satisfy:

lim inf
n→∞

nD(α  p)
log(n/s)

≥ 1.

(1)

To state the corresponding positive result (i.e.  the existence of an algorithm misclassifying only
s items)  we make an additional assumption to avoid extremely sparse labels: (A3) there exists a
constant κ > 0 such that np(j  i  (cid:96)) ≥ (n¯p)κ for all i  j and (cid:96) ≥ 1.

Theorem 2 Assume that (A1)  (A2)  and (A3) hold  and that ¯p = o(1)  ¯pn = ω(1). Let s = o(n). If
the parameters (α  p) of the LSBM satisfy (1)  then the Spectral Partition (SP ) algorithm presented
in Section 4 misclassiﬁes at most s items with high probability  i.e.  limn→∞ P[εSP (n) ≤ s] = 1.

These theorems indicate that under the LSBM with parameters satisfying (A1) and (A2)  the number
of misclassiﬁed items scales at least as n exp(−nD(α  p)(1 + o(1)) under any clustering algorithm 
irrespective of its complexity. They further establish that the Spectral Partition algorithm reaches this
fundamental performance limit under the additional condition (A3). We note that the SP algorithm
runs in polynomial time  i.e.  it requires O(n2 ¯p log(n)) ﬂoating-point operations.
We further establish a necessary and sufﬁcient condition on the parameters of the LSBM for the
existence of a clustering algorithm recovering the clusters exactly with high probability. Deriving
such a condition was also open [2].

Theorem 3 Assume that (A1) and (A2) hold.
If there exists a clustering algorithm that does
not misclassify any item with high probability  then the parameters (α  p) of the LSBM satisfy:
log(n) ≥ 1. If this condition holds  then under (A3)  the SP algorithm recovers the
lim inf n→∞ nD(α p)
clusters exactly with high probability.

The paper is organized as follows. Section 2 presents the related work and example of application
of our results. In Section 3  we sketch the proof of Theorem 1  which leverages change-of-measure
and coupling arguments. We present in Section 4 the Spectral Partition algorithm  and analyze
its performance (we outline the proof of Theorem 2). All results are proved in details in the
supplementary material.

2 Related Work and Applications

2.1 Related work

Cluster recovery in the SBM has attracted a lot of attention recently. We summarize below existing
results  and compare them to ours. Results are categorized depending on the targeted level of
performance. First  we consider the notion of detectability  the lowest level of performance requiring
that the extracted clusters are just positively correlated with the true clusters. Second  we look at
asymptotically accurate recovery  stating that the proportion of misclassiﬁed items vanishes as n
grows large. Third  we present existing results regarding exact cluster recovery  which means that no
item is misclassiﬁed. Finally  we report recent work whose objective  like ours  is to characterize the
optimal cluster recovery rate.

Detectability. Necessary and sufﬁcient conditions for detectability have been studied for the binary
symmetric SBM (i.e.  L = 1  K = 2  α1 = α2  p(1  1  1) = p(2  2  1) = ξ  and p(1  2  1) =
p(2  1  1) = ζ). In the sparse regime where ξ  ζ = o(1)  and for the binary symmetric SBM  the main
focus has been on identifying the phase transition threshold (a condition on ξ and ζ) for detectability:

It was conjectured in [5] that if n(ξ − ζ) <(cid:112)2n(ξ + ζ) (i.e.  under the threshold)  no algorithm

can perform better than a simple random assignment of items to clusters  and above the threshold 
clusters can partially be recovered. The conjecture was recently proved in [18] (necessary condition) 
and [16] (sufﬁcient condition). The problem of detectability has been also recently studied in [24]
for the asymmetric SBM with more than two clusters of possibly different sizes. Interestingly  it is
shown that in most cases  the phase transition for detectability disappears.

3

The present paper is not concerned with conditions for detectability. Indeed detectability means that
only a strictly positive proportion of items can be correctly classiﬁed  whereas here  we impose that
the proportion of misclassiﬁed items vanishes as n grows large.

Asymptotically accurate recovery. A necessary and sufﬁcient condition for asymptotically accurate
recovery in the SBM (with any number of clusters of different but linearly increasing sizes) has been
derived in [22] and [17]. Using our notion of divergence specialized to the SBM  this condition is
nD(α  p) = ω(1). Our results are more precise since the minimal achievable number of misclassiﬁed
items is characterized  and apply to a broader setting since they are valid for the generic LSBM.

Asymptotically exact recovery. Conditions for exact cluster recovery in the SBM have been also
recently studied. [1  17  8] provide a necessary and sufﬁcient condition for asymptotically exact
recovery in the binary symmetric SBM. For example  it is shown that when ξ = a log(n)
and
ab ≥ 1. In [2  3] 
ζ = b log(n)
the authors consider a more general SBM corresponding to our LSBM with L = 1. They deﬁne
CH-divergence as:

for a > b  clusters can be recovered exactly if and only if a+b

2 − √

n

n

D+(α  p(i)  p(j))

=

n

log(n)

max
λ∈[0 1]

K(cid:88)

k=1

(cid:0)(1 − λ)p(i  k  1) + λp(j  k  1) − p(i  k  1)1−λp(j  k  1)λ(cid:1)  

αk

and show that mini(cid:54)=j D+(α  p(i)  p(j)) > 1 is a necessary and sufﬁcient condition for asymptotically
exact reconstruction. The following claim  proven in the supplementary material  relates D+ to DL+.

Claim 4 When ¯p = o(1)  we have for all i  j:

L(cid:88)

DL+(α  p(i)  p(j))
n→∞∼ max
λ∈[0 1]

K(cid:88)

(cid:96)=1

k=1

(cid:0)(1 − λ)p(i  k  (cid:96)) + λp(j  k  (cid:96)) − p(i  k  (cid:96))1−λp(j  k  (cid:96))λ(cid:1) .

αk

Thus  the results in [2  3] are obtained by applying Theorem 3 and Claim 4.
In [13]  the authors consider a symmetric labeled SBM where communities are balanced (i.e. 
K for all k) and where label probabilities are simply deﬁned as p(i  i  (cid:96)) = p((cid:96)) for all i and
αk = 1
p(i  j  (cid:96)) = q((cid:96)) for all i (cid:54)= j. It is shown that nI
log(n) > 1 is necessary and sufﬁcient for asymptotically
exact recovery  where I = − 2
Claim 5 In the LSBM with K clusters  if ¯p = o(1)  and for all i  j  (cid:96) such that i (cid:54)= j  αi = 1
K  
p(i  i  (cid:96)) = p((cid:96))  and p(j  k  (cid:96)) = q((cid:96))  we have: D(α  p) n→∞∼ − 2

(cid:17)
(cid:112)p((cid:96))q((cid:96))

(cid:17)
(cid:112)p((cid:96))q((cid:96))

. We can relate I to D(α  p):

(cid:16)(cid:80)L

(cid:16)(cid:80)L

K log

(cid:96)=0

.

K log

(cid:96)=0

Again from this claim  the results derived in [13] are obtained by applying Theorem 3 and Claim 5.

Optimal recovery rate. In [6  19]  the authors consider the binary SBM in the sparse regime where
the average degree of items in the graph is O(1)  and identify the minimal number of misclassiﬁed
items for very speciﬁc intra- and inter-cluster edge probabilities ξ and ζ. Again the sparse regime
is out of the scope of the present paper. [23  7] are concerned with the general SBM corresponding
to our LSBM with L = 1  and with regimes where asympotically accurate recovery is possible.
The authors ﬁrst characterize the optimal recovery rate in a minimax framework. More precisely 
they consider a (potentially large) set of possible parameters (α  p)  and provide a lower bound on
the expected number of misclassiﬁed items for the worst parameters in this set. Our lower bound
(Theorem 1) is more precise as it is model-speciﬁc  i.e.  we provide the minimal expected number
of misclassiﬁed items for a given parameter (α  p) (and for a more general class of models). Then
the authors propose a clustering algorithm  with time complexity O(n3 log(n))  and achieving their
minimax recovery rate. In comparison  our algorithm yields an optimal recovery rate O(n2 ¯p log(n))
for any given parameter (α  p)  exhibits a lower running time  and applies to the generic LSBM.

4

2.2 Applications

We provide here a few examples of application of our results  illustrating their versatility. In all
examples  f (n) is a function such that f (n) = ω(1)  and a  b are ﬁxed real numbers such that a > b.

The binary SBM. Consider the binary SBM where the average item degree is Θ(f (n))  and repre-
sented by a LSBM with parameters L = 1  K = 2  α = (α1  1−α1)  p(1  1  1) = p(2  2  1) = af (n)
n  
and p(1  2  1) = p(2  1  1) = bf (n)
n . From Theorems 1 and 2  the optimal number of misclassiﬁed
vertices scales as n exp(−g(α1  a  b)f (n)(1 + o(1))) when α1 ≤ 1/2 (w.l.o.g.) and where
(1− α1 − λ + 2α1λ)a + (α1 + λ− 2αλ)b− α1aλb(1−λ) − (1− α1)a(1−λ)bλ.
g(α1  a  b) := max
λ∈[0 1]
a − √
√
It can be easily checked that g(α1  a  b) ≥ g(1/2  a  b) = 1
2). The worst
2 (
case is hence obtained when the two clusters are of equal sizes. When f (n) = log(n)  we also note
that the condition for asymptotically exact recovery is g(α1  a  b) ≥ 1.
Recovering a single hidden community. As in [9]  consider a random graph model with a hidden
community consisting of αn vertices  edges between vertices belonging the hidden community are
present with probability af (n)
n   and edges between other pairs are present with probability bf (n)
n .
This is modeled by a LSBM with parameters K = 2  L = 1  α1 = α  p(1  1  1) = af (n)
n   and
p(1  2  1) = p(2  1  1) = p(2  2  1) = bf (n)
n . The minimal number of misclassiﬁed items when
searching for the hidden community scales as n exp(−h(α  a  b)f (n)(1 + o(1))) where

b)2 (letting λ = 1

(cid:19)

.

(cid:18)

h(α  a  b) := α

a − (a − b)

1 + log(a − b) − log(a log(a/b))

log(a/b)

When f (n) = log(n)  the condition for asymptotically exact recovery of the hidden community is
h(α  a  b) ≥ 1.
Optimal sampling for community detection under the SBM. Consider a dense binary symmetric
SBM with intra- and inter-cluster edge probabilities a and b. In practice  to recover the clusters 
one might not be able to observe the entire random graph  but sample its vertex (here item) pairs as
considered in [22]. Assume for instance that any pair of vertices is sampled with probability δf (n)
for some ﬁxed δ > 0  independently of other pairs. We can model such scenario using a LSBM with
three labels  namely ×  0 and 1  corresponding to the absence of observation (the vertex pair is not
sampled)  the observation of the absence of an edge and of the presence of an edge  respectively 
and with parameters for all i  j ∈ {1  2}  p(i  j ×) = 1 − δf (n)
n   p(1  1  1) = p(2  2  1) = a δf (n)
n  
and p(1  2  1) = p(2  1  1) = b δf (n)
n exp(−l(δ  a  b)f (n)(1 + o(1))) where l := δ(1−√
n . The minimal number of misclassiﬁed vertices scales as
the condition for asymptotically exact recovery is l(α  a+  a−  b+  b−) ≥ 1.
Signed networks. Signed networks [15  20] are used in social sciences to model positive and negative
interactions between individuals. These networks can be represented by a LSBM with three possible
labels  namely 0  + and -  corresponding to the absence of interaction  positive and negative interaction 
respectively. Consider such LSBM with parameters: K = 2  α1 = α2  p(1  1  +) = p(2  2  +) =
  and p(1  2 −) =
a+f (n)
p(2  1 −) = b−f (n)
  for some ﬁxed a+  a−  b+  b− such that a+ > b+ and a− < b−. The minimal
number of misclassiﬁed individuals here scales as n exp(−m(α  a+  a−  b+  b−)f (n)(1 + o(1)))
where

ab−(cid:112)(1 − a)(1 − b)). When f (n) = log(n) 

  p(1  1 −) = p(2  2 −) = a−f (n)

  p(1  2  +) = p(2  1  +) = b+f (n)

n

n

a+ −(cid:112)b+)2 + (

√

a− −(cid:112)b−)2(cid:17)

.

m(α  a+  a−  b+  b−) :=

n

n

n

When f (n) = log(n)  the condition for asymptotically exact recovery is l(α  a+  a−  b+  b−) ≥ 1.

(cid:16)

√
(

1
2

3 Fundamental Limits: Change of Measures through Coupling

In this section  we explain the construction of the proof of Theorem 1. The latter relies on an
appropriate change-of-measure argument  frequently used to identify upper performance bounds in

5

online stochastic optimization problems [14]. In the following  we refer to Φ  deﬁned by parameters
(α  p)  as the true stochastic model under which all the observed random labels are generated  and
denote by PΦ = P (resp. EΦ[·] = E[·]) the corresponding probability measure (resp. expectation). In
our change-of-measure argument  we construct a second stochastic model Ψ (whose corresponding
probability measure and expectation are PΨ and EΨ[·]  respectively). Using a change of measures
from PΦ to PΨ  we relate the expected number of misclassiﬁed items EΦ[επ(n)] under any clustering
algorithm π to the expected (w.r.t. PΨ) log-likelihood ratio Q of the observed labels under PΦ and
PΨ. Speciﬁcally  we show that  roughly  log(n/EΦ[επ(n)]) must be smaller than EΨ[Q] for n large
enough.

k=1 αkKL(q(k)  p(i(cid:63)  k)) =(cid:80)K

q ∈ P K×(L+1) such that: D(α  p) =(cid:80)K

Construction of ψ. Let (i(cid:63)  j(cid:63)) = arg mini j:i<j DL+(α  p(i)  p(j))  and let v(cid:63) denote the smallest
item index that belongs to cluster i(cid:63) or j(cid:63). If both Vi(cid:63) and Vj(cid:63) are empty  we deﬁne v(cid:63) = n. Let
k=1 αkKL(q(k)  p(j(cid:63)  k)).
The existence of such q is proved in Lemma 7 in the supplementary material. Now to deﬁne the
stochastic model Ψ  we couple the generation of labels under Φ and Ψ as follows.
1. We ﬁrst generate the random clusters V1  . . .  VK under Φ  and extract i(cid:63)  j(cid:63)  and v(cid:63). The clusters
generated under Ψ are the same as those generated under Φ. For any v ∈ V  we denote by σ(v) the
cluster of item v.
2. For all pairs (v  w) such that v (cid:54)= v(cid:63) and w (cid:54)= v(cid:63)  the labels generated under Ψ are the same as those
generated under Φ  i.e.  the label (cid:96) is observed on the edge (v  w) with probability p(σ(v)  σ(w)  (cid:96)).
3. Under Ψ  for any v (cid:54)= v(cid:63)  the observed label on the edge (v  v(cid:63)) under Ψ is (cid:96) with probability
q(σ(v)  (cid:96)).
Let xv w denote the label observed for the pair (v  w). We introduce Q  the log-likelihood ratio of
the observed labels under PΦ and PΨ as:

v(cid:63)−1(cid:88)

v=1

n(cid:88)

+

log

v=v(cid:63)+1

Q =

log

q(σ(v)  xv(cid:63) v)

q(σ(v)  xv(cid:63) v)

.

(2)

1≤k≤K

1≤k≤K

p(σ(v(cid:63))  σ(v)  xv(cid:63) v)

p(σ(v(cid:63))  σ(v)  xv(cid:63) v)

ˆVk \ Vk| ≤ |(cid:83)

Let π be a clustering algorithm with output ( ˆVk)1≤k≤K  and let E =(cid:83)
loss of generality that |(cid:83)

ˆVk \ Vk be the set
of misclassiﬁed items under π. Note that in general in our analysis  we always assume without
ˆVγ(k) \ Vk| for any permutation γ  so that
the set of misclassiﬁed items is indeed E. By deﬁnition  επ(n) = |E|. Since under Φ  items are
interchangeable (remember that items are assigned to the various clusters in an i.i.d. manner)  we
have: nPΦ{v ∈ E} = EΦ[επ(n)] = E[επ(n)].
Next  we establish a relationship between E[επ(n)] and the distribution of Q under PΨ. For any
function f (n)  we can prove that: PΨ{Q ≤ f (n)} ≤ exp(f (n))
αi(cid:63) +αj(cid:63) . Using
this result with f (n) = log (n/EΦ[επ(n)]) − log(2/αi(cid:63) )  and Chebyshev’s inequality  we deduce
that: log (n/EΦ[επ(n)]) − log(2/αi(cid:63) ) ≤ EΨ[Q] +
EΨ[(Q − EΨ[Q])2]  and thus  a necessary
condition for E[επ(n)] ≤ s is:

EΦ[επ(n)]
(αi(cid:63) +αj(cid:63) )n + αj(cid:63)

1≤k≤K

log (n/s) − log(2/αi(cid:63) ) ≤ EΨ[Q] +

EΨ[(Q − EΨ[Q])2].

(3)

(cid:113) 4
(cid:114) 4

αi(cid:63)

αi(cid:63)

probability. From this  we can approximate EΨ[Q] by EΨ[(cid:80)n

Analysis of Q. In view of (3)  we can obtain a necessary condition for E[επ(n)] ≤ s if we evaluate
EΨ[Q] and EΨ[(Q − EΨ[Q])2]. To evaluate EΨ[Q]  we can ﬁrst prove that v(cid:63) ≤ log(n)2 with high
p(σ(v(cid:63)) σ(v) xv(cid:63)  v) ]  which

v=v(cid:63)+1 log

q(σ(v) xv(cid:63)  v)

is itself well-approximated by nD(α  p). More formally  we can show that:

EΨ[Q] ≤ (cid:0)n + 2 log(η) log(n)2(cid:1) D(α  p) +

(4)
Similarly  we prove that EΨ[(Q − EΨ[Q])2] = O(n¯p)  which in view of Lemma 8 (refer to the
supplementary material) and assumption (A2)  implies that: EΨ[(Q − EΨ[Q])2] = o(nD(α  p)).

log η
n3 .

6

We complete the proof of Theorem 1 by putting the above arguments together: From (3)  (4) and
the above analysis of Q  when the expected number of misclassiﬁed items is less than s (i.e. 
E[επ(n)] ≤ s)  we must have: lim inf n→∞ nD(α p)

log(n/s) ≥ 1.

4 The Spectral Partition Algorithm and its Optimality

(cid:12)(cid:12)∪K

More precisely  A = (cid:80)L

In this section  we sketch the proof of Theorem 2. To this aim  we present the Spectral Partition
(SP) algorithm and analyze its performance. The SP algorithm consists in two parts  and its detailed
pseudo-code is presented at the beginning of the supplementary document (see Algorithm 1).
The ﬁrst part of the algorithm can be interpreted as an initialization for its second part  and consists in
applying a spectral decomposition of a n × n random matrix A constructed from the observed labels.
(cid:96)=1 w(cid:96)A(cid:96)  where A(cid:96) is the binary matrix identifying the item pairs with
observed label (cid:96)  i.e.  for all v  w ∈ V  A(cid:96)
vw = 1 if and only if (v  w) has label (cid:96). The weight w(cid:96) for
label (cid:96) ∈ {1  . . .   L} is generated uniformly at random in [0  1]  independently of other weights. From
the spectral decomposition of A  we estimate the number of communities and provide asymptotically
accurate estimates S1  . . .   SK of the hidden clusters asymptotically accurately  i.e.  we show that
when n¯p = ω(1)  with high probability  ˆK = K and there exists a permutation γ of {1  . . .   K}
such that 1
. This ﬁrst part of the SP algorithm is adapted from
n
algorithms proposed for the standard SBM in [4  22] to handle the additional labels in the model
without the knowledge of the number K of clusters.
The second part is novel  and is critical to ensure the optimality of the SP algorithm.
It con-
sists in ﬁrst constructing an estimate ˆp of the true parameters p of the model from the matrices
(A(cid:96))1≤(cid:96)≤L and the estimated clusters S1  . . .   SK provided in the ﬁrst part of SP. We expect p to
be well estimated since S1  . . .   SK are asymptotically accurate. Then our cluster estimates are
iteratively improved. We run (cid:98)log(n)(cid:99) iterations. Let S(t)
K denote the clusters estimated
after the t-th iteration  initialized with (S(0)
K ) = (S1  . . .   SK). The improved clusters
are obtained by assigning each item v ∈ V to the cluster maximizing a log-
S(t+1)
1
likelihood formed from ˆp  S(t)
K   and the observations (A(cid:96))1≤(cid:96)≤L: v is assigned to S(t+1)

(cid:16) log(n ¯p)2

k=1Vk \ Sγ(k)

(cid:12)(cid:12) = O

1   . . .   S(0)

1   . . .   S(t)

(cid:17)

n ¯p

k(cid:63)

(cid:96)=0 A(cid:96)

vw log ˆp(k  i  (cid:96))}.

K

  . . .   S(t+1)

where k(cid:63) = arg maxk{(cid:80)K

(cid:80)

1   . . .   S(t)
w∈S(t−1)

i

i=1

(cid:80)L

Γ]  and M = (cid:80)L

Part 1: Spectral Decomposition. The spectral decomposition is described in Lines 1 to 4 in
Algorithm 1. As usual in spectral methods  the matrix A is ﬁrst trimmed (to remove lines and columns
corresponding to items with too many observed labels – as they would perturb the spectral analysis).
To this aim  we estimate the average number of labels per item  and use this estimate  denoted by ˜p in
Algorithm 1  as a reference for the trimming process. Γ and AΓ denote the set of remaining items
after trimming  and the corresponding trimmed matrix  respectively.
If the number of clusters K is known and if we do not account for time complexity  the two step
algorithm in [4] can extract the clusters from AΓ: ﬁrst the optimal rank-K approximation A(K) of
AΓ is derived using the SVD; then  one applies the k-mean algorithm to the columns of A(K) to
reconstruct the clusters. The number of misclassiﬁed items after this two step algorithm is obtained
as follows. Let M (cid:96) = E[A(cid:96)
(cid:96)=1 w(cid:96)M (cid:96) (using the same weights as those deﬁning
√
A). Then  M is of rank K. If v and w are in the same cluster  Mv = Mw and if v and w do not
belong to the same cluster  from (A2)  we must have with high probability: (cid:107)Mv − Mw(cid:107)2 = Ω(¯p
n).
√
v − Mv(cid:107)2 = Ω(¯p
Thus  the k-mean algorithm misclassiﬁes v only if (cid:107)A(K)
n). By leveraging
v (cid:107)A(k)
2 =
F = O(n¯p) with high probability. Hence the algorithm misclassiﬁes O(1/¯p) items with

elements of random graph and random matrix theories  we can establish that(cid:80)

(cid:107)A(k) − M(cid:107)2
high probability.
Here the number of clusters K is not given a-priori. In this scenario  Algorithm 2 estimates the rank
of M using a singular value thresholding procedure. To reduce the complexity of the algorithm  the
singular values and singular vectors are obtained using the iterative power method instead of a direct
SVD. It is known from [10] that with Θ (log(n)) iterations  the iterative power method ﬁnd singular
values and the rank-K approximation very accurately. Hence  when n¯p = ω(1)  we can easily

v − Mv(cid:107)2

7

√

√

(cid:80)
v (cid:107) ˆAv − Mv(cid:107)2

n˜p log(n˜p) 
estimate the rank of M by looking at the number of singular values above the threshold
since we know from random matrix theory that the (K + 1)-th singular value of AΓ is much less
than
n˜p log(n˜p) with high probability. In the pseudo-code of Algorithm 2  the estimated rank of
M is denoted by ˜K.
The rank- ˜K approximation of AΓ obtained by the iterative power method is ˆA = ˆU ˆV = ˆU ˆU(cid:62)AΓ.
From the columns of ˆA  we can estimate the number of clusters and classify items. Almost every
column of ˆA is located around the corresponding column of M within a distance 1
log(n ˜p)  since
2
F = O(n¯p log(n¯p)2) with high probability (we rigorously analyze
this distance in the supplementary material Section D.2). From this observation  the columns can be
categorised into K groups. To ﬁnd these groups  we randomly pick log(n) reference columns and for
each reference column  search all columns within distance
log(n ˜p). Then  with high probability 
each cluster has at least one reference column and each reference column can ﬁnd most of its cluster
members. Finally  the K groups are identiﬁed using the reference columns. To this aim  we compute
the distance of n log(n) column pairs ˆAv  ˆAw. Observe that (cid:107) ˆAv − ˆAw(cid:107)2 = (cid:107) ˆVv − ˆVw(cid:107)2 for any
u  v ∈ Γ  since the columns of ˆU are orthonormal. Now ˆVv is of dimension ˜K  and hence we can
identify the groups using O(n ˜K log(n)) operations.

2 = (cid:107) ˆA − M(cid:107)2

(cid:113) n ˜p2

(cid:113) n ˜p2

Theorem 6 Assume that (A1) and (A2) hold  and that n¯p = ω(1). After Step 4 (spectral decom-
position) in the SP algorithm  with high probability  ˆK = K and there exists a permutation γ of

{1  . . .   K} such that:(cid:12)(cid:12)∪K

k=1Vk \ Sγ(k)

(cid:16) log(n ¯p)2

(cid:17)

(cid:12)(cid:12) = O

.

¯p

i=1

(cid:80)L
(cid:96)=0 e(v Vi  (cid:96)) log p(k i (cid:96))

v ∈ Vk (cid:80)K
where for any S ⊂ V and (cid:96)  e(v  S  (cid:96)) =(cid:80)

Part 2: Successive clusters improvements. Part 2 of the SP algorithm is described in Lines 5 and
6 in Algorithm 1. To analyze the performance of each improvement iteration  we introduce the set
of items H as the largest subset of V such that for all v ∈ H: (H1) e(v V) ≤ 10ηn¯pL; (H2) when
log(n ¯p)4 for all j (cid:54)= k; (H3) e(v V \ H) ≤ 2 log(n¯p)2 
p(j i (cid:96)) ≥ n ¯p
(cid:96)=1 e(v  S  (cid:96)). Condition
w∈S A(cid:96)
(H1) means that there are not too many observed labels (cid:96) ≥ 1 on pairs including v  (H2) means that
an item v ∈ Vk must be classiﬁed to Vk when considering the log-likelihood  and (H3) states that v
does not share too many labels with items outside H.
We then prove that |V \ H| ≤ s with high probability when nD(α  p) − n ¯p
log(n ¯p)3 ≥ log(n/s) +

(cid:112)log(n/s). This is mainly done using concentration arguments to relate the quantity
(cid:80)K

vw  and e(v  S) =(cid:80)L

(cid:80)L
(cid:96)=0 e(v Vi  (cid:96)) log p(k i (cid:96))

p(j i (cid:96)) involved in (H2) to nD(α  p).

i=1

Finally  we establish that if the clusters provided after the ﬁrst part of the SP algorithm are asymptoti-
cally accurate  then after log(n) improvement iterations  there is no misclassiﬁed items in H. To that
aim  we denote by E (t) the set of misclassiﬁed items after the t-th iteration  and show that with high
probability  for all t  |E (t+1)∩H|
n ¯p. This completes the proof of Theorem 2  since after log(n)
iterations  the only misclassiﬁed items are those in V \ H.

|E (t)∩H| ≤ 1√

Acknowledgments

We gratefully acknowledge the support of the U.S. Department of Energy through the LANL/LDRD
Program for this work.

8

References
[1] E. Abbe  A. Bandeira  and G. Hall. Exact recovery in the stochastic block model. CoRR  abs/1405.3267 

2014.

[2] E. Abbe and C. Sandon. Community detection in general stochastic block models: fundamental limits and

efﬁcient recovery algorithms. In FOCS  2015.

[3] E. Abbe and C. Sandon. Recovering communities in the general stochastic block model without knowing

the parameters. In NIPS  2015.

[4] A. Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics  Probability &

Computing  19(2):227–284  2010.

[5] A. Decelle  F. Krzakala  C. Moore  and L. Zdeborová. Inference and phase transitions in the detection of

modules in sparse networks. Phys. Rev. Lett.  107  Aug 2011.

[6] Y. Deshpande  E. Abbe  and A. Montanari. Asymptotic mutual information for the two-groups stochastic

block model. CoRR  abs/1507.08685  2015.

[7] C. Gao  Z. Ma  A. Zhang  and H. Zhou. Achieving optimal misclassiﬁcation proportion in stochastic block

model. CoRR  abs/1505.03772  2015.

[8] B. Hajek  Y. Wu  and J. Xu. Achieving exact cluster recovery threshold via semideﬁnite programming.

CoRR  abs/1412.6156  2014.

[9] B. Hajek  Y. Wu  and J. Xu. Information limits for recovering a hidden community. CoRR  abs/1509.07859 

2015.

[10] N. Halko  P. Martinsson  and J. Tropp. Finding structure with randomness: Probabilistic algorithms for

constructing approximate matrix decompositions. SIAM review  53(2):217–288  2011.

[11] S. Heimlicher  M. Lelarge  and L. Massoulié. Community detection in the labelled stochastic block model.

In NIPS Workshop on Algorithmic and Statistical Approaches for Large Social Networks  2012.

[12] P. Holland  K. Laskey  and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks  5(2):109 –

137  1983.

[13] V. Jog and P. Loh. Information-theoretic bounds for exact recovery in weighted stochastic block models

using the renyi divergence. CoRR  abs/1509.06418  2015.

[14] T. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics 

6(1):4–22  1985.

[15] J. Leskovec  D. Huttenlocher  and J. Kleinberg. Signed networks in social media. In CHI  2010.

[16] L. Massoulié. Community detection thresholds and the weak ramanujan property. In STOC  2014.

[17] E. Mossel  J. Neeman  and A. Sly. Consistency thresholds for binary symmetric block models. In STOC 

2015.

[18] E. Mossel  J. Neeman  and A. Sly. Reconstruction and estimation in the planted partition model. Probability

Theory and Related Fields  162(3-4):431–461  2015.

[19] E. Mossel and J. Xu. Density evolution in the degree-correlated stochastic block model. CoRR 

abs/1509.03281  2015.

[20] V. Traag and J. Bruggeman. Community detection in networks with positive and negative links. Physical

Review E  80(3):036115  2009.

[21] S. Yun and A. Proutiere. Accurate community detection in the stochastic block model via spectral

algorithms. CoRR  abs/1412.7335  2014.

[22] S. Yun and A. Proutiere. Community detection via random and adaptive sampling. In COLT  2014.

[23] A. Zhang and H. Zhou. Minimax rates of community detection in stochastic block models. CoRR 

abs/1507.05313  2015.

[24] P. Zhang  C. Moore  and M. Newman. Community detection in networks with unequal groups. CoRR 

abs/1509.00107  2015.

9

,George Chen
Stanislav Nikolov
Devavrat Shah
Se-Young Yun
Alexandre Proutiere