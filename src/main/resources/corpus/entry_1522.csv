2018,Dirichlet belief networks for topic structure learning,Recently  considerable research effort has been devoted to developing deep architectures for topic models to learn topic structures. Although several deep models have been proposed to learn better topic proportions of documents  how to leverage the benefits of deep structures for learning word distributions of topics has not yet been rigorously studied. Here we propose a new multi-layer generative process on word distributions of topics  where each layer consists of a set of topics and each topic is drawn from a mixture of the topics of the layer above. As the topics in all layers can be directly interpreted by words  the proposed model is able to discover interpretable topic hierarchies. As a self-contained module  our model can be flexibly adapted to different kinds of topic models to improve their modelling accuracy and interpretability. Extensive experiments on text corpora demonstrate the advantages of the proposed model.,Dirichlet belief networks for topic structure learning

He Zhao1  Lan Du1∗  Wray Buntine1  and Mingyuan Zhou2∗
1Faculty of Information Technology  Monash University  Australia

2McCombs School of Business  The University of Texas at Austin  USA

Abstract

Recently  considerable research effort has been devoted to developing deep archi-
tectures for topic models to learn topic structures. Although several deep models
have been proposed to learn better topic proportions of documents  how to leverage
the beneﬁts of deep structures for learning word distributions of topics has not yet
been rigorously studied. Here we propose a new multi-layer generative process
on word distributions of topics  where each layer consists of a set of topics and
each topic is drawn from a mixture of the topics of the layer above. As the topics
in all layers can be directly interpreted by words  the proposed model is able to
discover interpretable topic hierarchies. As a self-contained module  our model can
be ﬂexibly adapted to different kinds of topic models to improve their modelling
accuracy and interpretability. Extensive experiments on text corpora demonstrate
the advantages of the proposed model.

1

Introduction

Understanding text has been an important task in machine learning  natural language processing  and
data mining. Text is discrete  unstructured  and often highly sparse. A popular way of analysing texts
is to represent them as a set of latent factors via topic modelling or matrix factorisation. With great
success in modelling text  probabilistic topic models discover a set of latent topics from a collection
of documents. Those topics  as latent factors  can be interpreted by distributions over words and used
to derive low dimensional representations of the documents. Speciﬁcally  most existing topic models
are built on top of the following generative process: Each topic is a distribution over the words (i.e. 
word distribution  WD) in the vocabulary; each document is associated with a topic proportion (TP)
vector; and a word in a document is generated by ﬁrst drawing a topic according to the document’s
TP  then sampling the word according to the topic’s WD.
In a Bayesian setting  TPs and WDs are both imposed on prior distributions. For example  one
commonly-used prior for TP and WD is a Dirichlet distribution  as in Latent Dirichlet Allocation
(LDA) (Blei et al.  2003). Recently  deep hierarchical priors  especially imposed on TPs  have been
developed to generate hierarchical document representations as well as discover interpretable topic
hierarchies. For example  there are hierarchical tree-structured constructions based on the Dirichlet
Process (DP) or Chinese Restaurant Process (CRP)  such as the nested CRP (nCRP) (Blei et al. 
2010) and the nested hierarchical DP (Paisley et al.  2015); deep constructions based on restricted
Boltzmann machines and neural networks such as the Replicated Softmax Model (RSM) (Hinton and
Salakhutdinov  2009)  the Neural Autoregressive Density Estimator (NADE) (Larochelle and Lauly 
2012)  and the Over-replicated Softmax Model (OSM) (Srivastava et al.  2013); models based on
variational autoencoders (VAE) including Srivastava and Sutton (2017); Miao et al. (2017); Zhang
et al. (2018). Recently  models that generalise the sigmoid belief network (Hinton et al.  2006) have
been proposed  such as Deep Poisson Factor Analysis (DPFA) (Gan et al.  2015)  Deep Exponential
Families (DEF) (Ranganath et al.  2015)  Deep Poisson Factor Modelling (DPFM) (Henao et al. 
2015)  and Gamma Belief Networks (GBNs) (Zhou et al.  2016).

∗Corresponding authors

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Compared with the considerable interest in deep models on TPs  to our knowledge  the counterparts
on WDs have not been fully investigated. In this paper  we propose a new multi-layer generative
process on WDs  as a self-contained module and an alternative to the single-layer Dirichlet prior.
In the proposed model  WDs are the output units of the bottom layer in a DBN with hidden layers
parameterised by Dirichlet-distributed hidden units and connected with gamma-distributed weights.
Speciﬁcally  each Dirichlet unit in a hidden layer is a probability distribution over the words in the
vocabulary and can be view as a “hidden” topic. In each layer  the Dirichlet prior of a topic is a
mixture of the topics in the layer above. As the hidden units are drawn from Dirichlet  the proposed
model is named the Dirichlet Belief Network  hereafter referred to as DirBN2.
Compared with existing related deep models  DirBN has the following appealing properties: 1)
Interpretability of hidden units: Every hidden unit in every layer of DirBN is a probability dis-
tribution over the words  making them real topics that can be directly interpreted. 2) Discovering
topic hierarchies: The mixture structure of DirBN enables the model to enjoy a straightforward
way of discovering semantic correlations of topics in two adjacent layers  which further form topic
hierarchies with the multi-layer construction of the model. Due to the intrinsic abstraction effect
of DBN  the topics in the higher layers are more abstract and can be treated as the generalisation
of the ones in the lower layers. 3) Better modelling accuracy: It is known that TPs are local
variables (speciﬁc to individual document)  while WDs are global variables over the target corpus.
Unlike many other hierarchical parallels on TP  DirBN imposes a deep structure on WD  which
“absorbs the information” from the entire corpus. It makes DirBN be able to get better modelling
accuracy especially in the case of sparse texts such as tweets and news abstracts  where the context
information of an individual document is not enough to learn a good model using existing approaches.
4) Adaptability: As many sophisticated models on TPs usually use a simple Dirichlet prior on WDs 
including the well-known ones such as Supervised Topic Model (Mcauliffe and Blei  2008) and
Author Topic Model (Rosen-Zvi et al.  2004)  our DirBN can be easily adapted to them to further
improve modelling accuracy and interpretability.
In conclusion  the contributions of this paper include: 1) We propose DirBN  a deep structure
that can be used as an advanced alternative to the Dirichlet prior on WDs with better modelling
performance and interpretability. 2) We demonstrate our model’s adaptability by applying DirBN
with several well-developed models  including Poisson Factor Analysis (PFA) (Zhou et al.  2012) 
MetaLDA (Zhao et al.  2017a)  and GBN (Zhou et al.  2016). 3) With proper data augmentation and
marginalisation techniques  DirBN enjoys full local conjugacy  which facilitates the derivation of a
simple and effective inference algorithm.

2 The proposed DirBN

In this section  we introduce the details of the generative and inference processes of DirBN.

2.1 Generative process

We ﬁrst deﬁne the essential notation and review the basic framework of topic modelling  followed by
the details of the proposed DirBN. Assume that the bag-of-words of document d in a corpus with
N documents and V unique words in the vocabulary are stored in a count vector xd ∈ NV
0 . A topic
model with K topics is composed of the TP vector θd ∈ RK
+ for each document d and the WD vector
+ for each topic k (k ∈ {1 ···   K}). To generate a word in document d  one can ﬁrst sample
φk ∈ RV
a topic according to its TP  and then sample the word type according to the topic’s WD. Given this
framework  many prior constructions of TPs have been proposed  such as the Dirichlet distribution
in LDA  logistic normal distributions for modelling topic correlations in Correlated Topic Model
(CTM) (Lafferty and Blei  2006)  nonparametric priors like the Hierarchical Dirichlet Process (Teh
et al.  2012)  and recently-proposed deep models like DPFA (Gan et al.  2015)  DPFM (Henao et al. 
2015)  and GBN (Zhou et al.  2016). Unlike the extensive choices for constructing TP  the symmetric
Dirichlet distribution on WDs still dominates in many advanced topic models. Here DirBN is a new
hierarchical approach of constructing WDs  detailed as follows.

2Code available at https://github.com/ethanhezhao/DirBN

2

Figure 1: Demonstration of the generative process of DirBN with three layers.

A DirBN with T layers leaves the TPs of the basic framework untouched and draws φk according to
the following generative process:

φ(T )
kT

φ(t)
kt

φ(1)
k1

∼ DirV (η) 
···
∼ DirV (ψ(t)
···
∼ DirV (ψ(1)

kt

k1

Kt+1(cid:88)
K2(cid:88)

kt+1

)  ψ(t)
kt

=

)  ψ(1)
k1

=

φ(t+1)
kt+1

β(t)
kt+1kt

  β(t)

kt+1kt

∼ Ga(γ(t)

kt+1

  1/c(t)) 

φ(2)
k2

β(1)
k2k1

  β(1)
k2k1

∼ Ga(γ(1)

k2

  1/c(1)) 

(1)

k2

where 1) Ga(− −) is the gamma distribution with shape and scale parameters and DirV (−) is the
Dirichlet distribution3; 2) The superscript with a bracket over a variable indicates which layer it
belongs to and kt ∈ {1 ···   Kt} is the topic index in the t-th layer; 3) The output of DirBN is φ(1)
 
k1
which corresponds to φk in the basic framework and hereafter  we use φ(1)
instead; 4) We further
k1
impose gamma priors on the following variables: η ∼ Ga(a0  1/b0)  γ(t)
∼ Ga(γ(t)
0 /Kt  1/c(t)
0 ) 
0 ∼ Ga(g0  1/h0)  and c(t) ∼ Ga(g0  1/h0). The generative process of a topic
0 ∼ Ga(e0  f0)  c(t)
γ(t)
model equipped with DirBN is demonstrated in Figure 1.
The idea of our DirBN can be summarised as follows:

kt+1

1. From a bottom-up view  DirBN is a multi-layer matrix factorisation  which factorises the
matrix of the WDs in the t-th layer as: Φ(t) ∼ Dir(Φ(t+1)B(t)). Here we deﬁne Φ(t) ∈
RV ×Kt
is the kt-th column). From a
top-down view  the model can be considered as a stochastic feedforward network (Tang and
Salakhutdinov  2013)  where the input matrix in Φ(T )  the output matrix is Φ(1)  and the
stochastic units are drawn from the Dirichlet distribution.

is the kt-th column) and B(t) ∈ RKt+1×Kt

(φ(t)
kt

(β(t)
kt

+

+

2. As DirBN is a Bayesian probabilistic model  consider a DirBN with only two layers as
an example: each ﬁrst-layer topic φ(1)
is drawn from a Dirichlet with the topic-speciﬁc
k1
asymmetric parameter ψ(1)
  which is a mixture of the second-layer topics. So the statistical
k1
strength is shared via the mixture  which plays an important role in handling sparse texts.
3. In DirBN  not only in the bottom layer  but also in any other layer t  each hidden unit is
a distribution over the vocabulary and can be viewed as real topic directly interpreted by
words. Although the bottom layer serves as the actual WDs for generating the words  the
topics in the higher layers are involved with the belief prorogation in the network.

4. The weight β(t)

kt+1kt

of the gamma prior on β(t)

is drawn from a hierarchical gamma prior (i.e.  the shape parameter
γ(t)
is also drawn from a gamma). It allows topics in
kt+1
the (t + 1)-th layer to contribute differently to those in the t-th layer. In addition  the
hierarchical structure on β(t)
is similar to the one in Zhou (2015)  which provides an

kt+1kt

kt+1kt

3− can be a vector as a set of asymmetric parameters or a scalar as a symmetric parameter of Dirichlet

3

!"#(%)!"'(()!")(*)+ - .∈{1 … 4}•Relational Topic Model•Supervised Topic Model•Author Topic Model•…6(%)6(()6(*)DirBN with 3layers8"'")(*)8"#"'(()intrinsic shrinkage mechanism on β(t)
. In other words  each kt is expected to be sparsely
kt
connected by a subset of kt+1. We will demonstrate the shrinkage effect of DirBN in the
experiments.

2.2 Inference process

The learning of DirBN can be done by the inference of its latent variables  i.e.  Φ(t) and B(t) for
all t. With several data augmentation techniques  we are able to derive a layer-wise Gibbs sampling
algorithm facilitated by local conjugacy. Given θ and φ (despite their constructions)  a topic model
usually samples the topic assignment of each word in the corpus. After that  each topic k1 is associated
with a vector of word counts  denoted as x(1)
]  which encodes the semantic
k1
information of topic k1 and is one of the input count vectors of DirBN in the inference process.
Given the input vectors  the inference of DirBN involves two key steps: 1) propagating the semantic
information of the input vectors up to the top layer via latent counts; 2) updating Φ(t) and B(t) down
to the bottom given the latent counts. Without loss of generality  we illustrate the inference details
with a two-layer DirBN as follows4:

 ···   x(1)

= [x(1)
1k1

V k1

Propagating the latent counts from the bottom up By integrating φ(1)
k1
likelihood  we can get the likelihood of ψ(1)
k1

out from its multinomial

where Γ(−) is the gamma function  ψ(1)·k1
out and introducing two auxiliary variables q(1)
k1
2017a):

L(cid:16)

ψ(1)
k1

(cid:17) ∝

L(cid:16)

Γ(ψ(1)·k1

V(cid:89)

v

)
v ψ(1)
vk1
and y(1)
vk1

as:
Γ(ψ(1)·k1

)

+ x(1)·k1

=(cid:80)V
(cid:17) ∝ V(cid:89)
(cid:16)
(cid:16)

v

∼ CRT

Γ(ψ(1)
vk1
Γ(ψ(1)
vk1

)

+ x(1)
vk1
)

=(cid:80)V
(cid:17)y(1)

vk1(cid:16)
(cid:17)ψ(1)
(cid:17)

 

(2)

  and x(1)·k1

. By integrating φ(1)
k1
  Eq. (2) can be augmented as (Zhao et al. 

v x(1)
vk1

ψ(1)
k1

  q(1)
k1

  y(1)
vk1

q(1)
k1

ψ(1)
vk1

vk1  

(3)

(cid:33)

(cid:32)

(cid:17) ∼ Mult

∼ Beta(ψ(1)·k1
=(cid:80)K2
 ···   y(1)
V k1
φ(2)
layer topic k2 by:(cid:16)
vk2

With ψ(1)
vk1

k2

where z(1)

vk2k1

where q(1)
. Here CRT stands for the Chinese
k1
Restaurant Table distribution (Zhou and Carin  2015; Zhao et al.  2017b). Now we can deﬁne
y(1)
k1

]  the latent count vector derived from the input count vector x(1)
k1

) and y(1)
vk1

= [y(1)
1k1

  ψ(1)
vk1

  x(1)·k1

x(1)
vk1

.

β(1)
k2k1

  we can then distribute the latent count y(1)
vk1

on ψ(1)
vk1

to each second

 ···   z(1)

y(1)
vk1

z(1)
v1k1

vK2k1

is the latent count allocated to k2 and(cid:80)K2
=(cid:80)K1

 ···   x(2)

k2

 

v1 β(1)
φ(2)
1k1
ψ(1)
vk1
z(1)
vk2k1
z(1)
vk2k1

k1

= y(1)
vk1
. x(2)
k2

ψ(1)
vk1

.

 ···  

φ(2)
vK2

β(1)
K2k1

 

(4)

V k2

= [x(2)
1k2

] where x(2)
vk2

We now note x(2)
k2
output count vectors of the ﬁrst layer and also the input count vector of the second layer topic k2.
In conclusion  to propagate the semantic information from the ﬁrst to the second layer  we ﬁst derive
y(1)
)  and ﬁnally aggregate
vk1
z(1)
vk2k1

  then distribute y(1)
vk1
.

to all the second layer topics (i.e.  z(1)

from x(1)
vk1
into x(2)
vk2

can be viewed as one of the

vk2k1

Updating the latent variables from the top down After the latent counts are propagated  we start
updating the latent variables from the top layer (i.e. the second layer here). Given x(2)
is easy
k2
to sample from its Dirichlet posterior. With z(1)
from its
vk2k1
gamma posterior given the following likelihood:
−β(1)
k2 k1

k2 v = 1  we can sample β(1)

and(cid:80)V

(− log q(1)
k1

v φ(2)

  φ(2)
k2

(cid:17) ∝ e

L(cid:16)

)z(1)·k2k1  

k2k1

(5)

)(β(1)
k2k1

β(1)
k2k1

4 Omitted details of inference as well as the overall algorithm are given in the supplementary materials.

4

=(cid:80)V

v z(1)

vk2k1

. Given the newly sampled φ(2)
k2

and β(1)
k2k1

  we can recompute ψ(1)
k1

and

from its Dirichlet posterior. Now the inference of a two-layer DirBN is done.

where z(1)·k2k1
sample φ(1)
k1

3 Using DirBN in topic modelling

DirBN is a self-contained module on φ  leaving θ untouched. Therefore  it can be used as an
alternative to the simple Dirichlet prior on φ in many existing models. The adaptability of DirBN
enables us to easily apply it to advanced models so that those models can beneﬁt from the advantages
of DirBN. To demonstrate this  we adapt the proposed DirBN structure to the following models:

PFA+DirBN Poisson Factor Analysis (PFA) is a popular framework for topic analysis (DPFA (Gan
et al.  2015)  DPFM (Henao et al.  2015)  GBN (Zhou et al.  2016) can be viewed as a deep extension
to PFA). Speciﬁcally  we use the Bayesian nonparametric version of PFA named BGGPFA (Zhou
et al.  2012)  where θd is constructed from a negative binomial process and φk is drawn from a
Dirichlet distribution. Note that there are close relationships between PFA and LDA  and between
BGGPFA and HDP (Teh et al.  2012)  analysed in Zhou (2018). Here we replace the Dirichlet
construction on φ with DirBN  yielding a model named PFA+DirBN.

MetaLDA+DirBN MetaLDA (Zhao et al.  2017a  2018a) is a supervised topic model that is able
to incorporate document labels to inform the learning of θd. Keeping the structure on θ untouched 
we replace the MetaLDA’s structure on φ with our DirBN to get a combined model that discovers
the topic hierarchies informed by the document labels. The proposed model is able to discover the
correlations between labels and topic hierarchies.

GBN+DirBN Recall that GBN (Zhou et al.  2015  2016) imposes a hierarchical structure on θ 
which is able to learn multi-layer document representations and topic hierarchies. Here we combine
DirBN and GBN together to yield a “dual” deep model  where the GBN part is on θ and the DirBN
part is on φ. Both parts discover topic hierarchies and the bottom-layer topics are shared by the two
parts/hierarchies. It would be interesting to see how the two deep structures interact with each other.

4 Related work

As the proposed model introduces a hierarchical architecture on WDs (i.e.  φ) in topic models  we ﬁrst
review various priors on φ  starting with the ones on sampling/optimising the Dirichlet parameters in
topic models. The Dirichlet parameters in topic models were studied comprehensively in Wallach
et al. (2009)  which showed that Dirichlet with a symmetric parameter sampled from an uninformative
gamma is the best choice. Actually  our DirBN can be reduced to this choice if T = 1 (i.e.  DirBN-1 
with one layer only). However  unlike the sampling/optimising approaches used in Wallach et al.
(2009)  DirBN-1 uses a negative binomial augmentation shown in Eq. (3)  which leads to a simpler
inference scheme. Recently  models like Zhao et al. (2017a c  2018b) construct informative and
asymmetric Dirichlet priors by taking into account some external knowledge like word embeddings.
Whereas DirBN learns the asymmetric priors purely based on the context of the target corpus.
Instead of Dirichlet  the Pitman-Yor process (PYP) has been used on WDs to model the power-law
distribution of words  as in Sato and Nakagawa (2010); Buntine and Mishra (2014). Chen et al. (2015)
used a transformed PYP prior on φ to model multiple document collections. Lindsey et al. (2012)
imposed a hierarchical PYP prior on φ to discover word phrases. Besides PYP  the Indian Buffet
Process (IBP) has been used as a prior on φ to introduce word focusing on topics  as in Archambeau
et al. (2015). In general  existing models use different priors on φ for modelling various linguistic
phenomena  which have different purposes to DirBN. The deep structures induced by DirBN on WDs
have not yet been rigorously studied.
To our knowledge  most existing models explore the structure of topics by imposing a
deep/hierarchical prior on θ. For example  hierarchical PYPs were used for domain adaptation
in language models (Wood and Teh  2009) and topic models (Du et al.  2012). nCRP (Blei et al. 
2010) models topic hierarchies by introducing a tree-structured prior. Paisley et al. (2015); Kim
et al. (2012); Ahmed et al. (2013) extended nCRP by either softening its constraints or applying it to
different problems. Li and McCallum (2006) proposed the Pachinko Allocation model (PAM)  which

5

(a)

(b)

Figure 2: (a): Histograms of the normalised (latent) words counts. (b): B(1).

captures the topic correlations with a directed acyclic graph. Recently  several deep extensions of
PFA on θ have been proposed  including DPFA (Gan et al.  2015)  DPFM (Henao et al.  2015)  and
GBN (Zhou et al.  2016). DPFM and GBN are the most related models to ours  which are also able
to discover topic hierarchies. In DPFM and GBN  the higher-layer topics are not distributions over
words but distributions over the topics in the layer below (they are called “meta-topics” in DPFM).
To interpret those meta-topics  one needs to project them all the way down to the bottom-layer
topics with matrix multiplication. Whereas in our model  the topics on all the layers are directly
interpretable.

5 Experiments

The experiments were conducted on three real-world datasets  detailed as follows: 1) Web Snippets
(WS)  containing 12 237 web search snippets labelled with 8 categories. The vocabulary contains
10 052 word types. 2) Tag My News (TMN)  consisting of 32 597 RSS news labelled with 7 categories.
Each document contains a title and a description. There are 13 370 word types in the vocabulary. 3)
Twitter  extracted in 2011 and 2012 microblog tracks at Text REtrieval Conference (TREC)5. It has
11 109 tweets in total. The vocabulary size is 6 344.
With the framework of PFA  we compared three options of constructing φ: (1) The default setting
of PFA  where φ is drawn from a symmetric Dirichlet distribution with parameter 0.05  i.e.  φk ∼
DirV (0.05); (2) PFA+Mallet  where φk ∼ DirV (α0) and α0 is sampled by Mallet 6; (3) PFA+DirBN 
the proposed model  where φk is drawn from an asymmetric Dirichlet distribution speciﬁc to k 
the parameter of which is constructed with the higher-layer topics. Note that Wallach et al. (2009)
tested the option using speciﬁc asymmetric Dirichlet parameter  i.e.  φk ∼ DirV ([α1 ···   αV ])  but
the performance is not as good as the symmetric parameter (the second one above). In addition 
following a similar routine  we compared MetaLDA (Zhao et al.  2017a)  and GBN (Zhou et al. 
2016) with/without DirBN. Note that PFA is a widely used Bayesian topic model  MetaLDA is the
state-of-the-art topic model capable of handling sparse texts  and GBN is reported Cong et al. (2017)
to outperform many other deep models including DPFA (Gan et al.  2015)  DPFM (Henao et al. 
2015)  nHDP (Paisley et al.  2015)  and RSM (Hinton and Salakhutdinov  2009).
For all the models  we ran 3 000 MCMC iterations with 1 500 burnin. For DirBN  we set a0 =
b0 = g0 = h0 = 1.0 and e0 = f0 = 0.01. For PFA  MetaLDA  and GBN  we used their original
implementations and settings  except that φ is drawn from DirBN in the combined models. For all
the models  the number of topics in each layer of DirBN was set to 100  i.e.  KT = ··· = K1 = 100.
For GBN and GBN+DirBN  we set the number of topics in each layer of GBN to 100 as well. Due to
the shrinkage mechanisms of PFA  GBN  and DirBN  the number of active topics will be adjusted
according to the data. In all the experiments  we varied the number of layers of DirBN T from 1 to 3.
For GBN+DirBN  the dual deep model  we ﬁxed the number of layers of GBN as 3.

5http://trec.nist.gov/data/microblog.html
6http://mallet.cs.umass.edu

6

00.010.020.030.040.050.060.0701020304000.010.020.030.040.050.060.0701020304050012345(a)

(d)

(b)

(e)

(c)

(f)

(g)

(h)

Figure 3: Perplexity (the vertical axis) with varied proportion (the horizontal axis) of the words for
training in the training documents. (a-c): Results of the models based on PFA on WS  TMN  Twitter.
(d-f): Results of the models based on GBN on WS  TMN  Twitter. (g h): Results of the models
based on MetaLDA on WS and TMN. The errorbars indicate the standard deviations of ﬁve runs.
The number of a model indicates the number of layers used in DirBN. The results of MetaLDA and
document classiﬁcation on Twitter are not reported due to the unavailability of labels.

x(t)
vkt

for all kt where x(t)
vkt

vkt

v x(t)
vkt

/(cid:80)

words counts(cid:80)

Demonstration of DirBN’s shrinkage effect As previously discussed  DirBN has an intrinsic
shrinkage mechanism that is able to automatically learn the number of active topics in each layer (i.e. 
the network width). We empirically demonstrate the shrinkage effect in Figure 2  with the results
of PFA+DirBN-3 on the TMN dataset. Figure 2a plots the histograms of the normalised (latent)
is the word count for topic kt. The blue and
red bars are for the ﬁrst- (t = 1) and the second-layer (t = 2) topics  respectively. The histogram
indicates the number of topics ( the vertical axis) that are with a speciﬁc word count (the horizontal
axis). A topic with a larger word count is more important. The shrinkage effect is that large proportion
of the topics are with very small word counts  indicating that the number of effective topics is less
than the truncation (i.e.  Kt = 100). This is more obvious  in the second layer. Moreover  we display
log B(1) as an image in Figure 2b. The vertical and horizontal axes are for the second- and ﬁrst-layer
topics  respectively. We ranked the ﬁrst- and second-layer topics by their word counts. The sparsity of
B(1) indicates that the ﬁrst- and second-layer topics are sparsely connected. This also demonstrates
the shrinkage effect of the model.

Quantitative results We report the per-heldout-word perplexity and topic coherence results. To
compute perplexity  we randomly selected 80% of the documents in each dataset to train the models
and 20% for testing. For each testing document  we randomly used one half of its words to infer its
TP  and the other half to calculate perplexity. Topic coherence measures the semantic coherence in
the most signiﬁcant words (top words) of a topic. Here we used the Normalized Pointwise Mutual
Information (NPMI) (Aletras and Stevenson  2013; Lau et al.  2014) to calculate topic coherence

7

20%40%100%60080010001200140016001800PFAPFA+MalletPFA+DirBN-1PFA+DirBN-2PFA+DirBN-320%40%100%10001500200025003000PFAPFA+MalletPFA+DirBN-1PFA+DirBN-2PFA+DirBN-320%40%100%300400500600700PFAPFA+MalletPFA+DirBN-1PFA+DirBN-2PFA+DirBN-320%40%100%60080010001200140016001800GBNGBN+DirBN-1GBN+DirBN-320%40%100%10001500200025003000GBNGBN+DirBN-1GBN+DirBN-320%40%100%300400500600700800GBNGBN+DirBN-1GBN+DirBN-320%40%100%60080010001200140016001800MetaLDAMetaLDA+DirBN-1MetaLDA+DirBN-320%40%100%1500200025003000MetaLDAMetaLDA+DirBN-1MetaLDA+DirBN-3Table 1: Topic coherence with varied proportion of the words for training in the training documents.
± indicates the standard deviation of ﬁve runs. The best result in each column is in boldface.

Training words

PFA

PFA+Mallet
PFA+DirBN-1
PFA+DirBN-3

GBN

GBN+DirBN-1
GBN+DirBN-3

20%
-0.070±0.010
0.008±0.004
0.013±0.003
0.021±0.005
-0.072±0.013
0.015±0.005
0.018±0.006

WS

40%
0.008±0.002
0.049±0.005
0.052±0.004
0.059±0.002
0.007±0.005
0.057±0.002
0.061±0.004

100%
0.062±0.011
0.063±0.003
0.060±0.006
0.068±0.004
0.069±0.009
0.069±0.005
0.075±0.002

20%
-0.059±0.008
0.035±0.006
0.031±0.003
0.046±0.003
-0.065±0.008
0.032±0.002
0.048±0.003

TMN

40%
0.064±0.009
0.083±0.005
0.080±0.001
0.090±0.003
0.063±0.006
0.086±0.002
0.094±0.004

100%
0.103±0.006
0.108±0.005
0.108±0.008
0.111±0.004
0.106±0.004
0.112±0.007
0.113±0.004

20%
-0.003±0.003
0.022±0.003
0.019±0.004
0.024±0.001
-0.005±0.005
0.021±0.004
0.025±0.003

Twitter

40%
0.031±0.003
0.037±0.002
0.037±0.004
0.038±0.002
0.032±0.002
0.040±0.005
0.040±0.002

100%
0.046±0.002
0.045±0.003
0.049±0.007
0.049±0.002
0.047±0.00
0.050±0.005
0.051±0.003

Table 2: Topic hierarchy comparison in GBN+DirBN. Each row in boldface is the top 10 words in a
ﬁrst-layer topic. Each of these topics is associated with three most correlated topics in the second
layer of DirBN (left) and GBN (right)  respectively. The number associated with a second-layer topic
is its (normalised) link weight to the ﬁrst-layer topic.

police arrested man charged woman authorities death year found accused

heat miami james lebron game nba ﬁnals celtics bulls wade

facebook google internet social twitter online web media site search

0.38

0.19

0.15

0.97

0.00

0.00

0.22

0.19

0.18

0.91

0.04

0.44

0.01

0.13

0.13

0.11

0.43

0.15

0.10

0.18

0.14

0.12

0.12

0.12

0.10

0.17

0.14

0.13

case charges accused trial courtattorney
investigation judge allegations criminal
police ofﬁcial killing attack deaddeath

army security man family

woman men drug suicide girl
sexual death found human york

season team game play run
night star series fans career
nba playoffs court brink seeds

defeated berth seed opponent semiﬁnals

win victory beat lead winning
top fourth loss straight beating

phone plan video technology mobile
devices computer tech ceo content
company million buy billion corp

industry sales companies consumers products
government report country nation pressure

ofﬁcial state move released public

police arrested man charged year

accused found charges woman death

police prison man china years

arrested charges charged year chinese

china police chinese bomb ﬁre
people blast city artist ofﬁcials

heat miami james game nba

ﬁnals lebron bulls mavericks dallas
trial rajaratnam insider trading fund

hedge raj anthony galleon case
music album lady gaga justin

star pop band rock tour

facebook social internet google online

twitter chief executive media web
court lawsuit case facebook judge
social federal internet google online
facebook social internet google online

twitter world web media site

study cancer drug risk heart patients women researchers disease people

rising percent high higher economic
increase low growth strong recovery

reactions periods technique method declared

study cancer drug risk researchers heart

people patients health women

world war years family oil

important realized treatment peril scores
study experience ﬁnding recent security
kids challenges millions report special
nuclear japan plant power radiation crisis japanese fukushima crippled tokyo
government united states ofﬁcials state
report country group ofﬁcial agency

0.18

0.53

twitter world web media site

dies year energy women american

nuclear japan plant power radiation

facebook social internet google online

crisis japanese fukushima earthquake tokyo

safety water nearby land found
caused sea believed center parts
work plans part future system

rules program bring offers decision

nuclear japan plant power radiation
crisis japanese fukushima water tokyo

theater review broadway play york

musical stage life time love

score from the top 10 words of each topic and reported scores averaged over top 50 topics with
highest NPMI  where “rubbish” topics are eliminated  following Yang et al. (2015)7. In the training
documents  we further varied the proportion of the words used in training to mimic the case of sparse
texts. All the models ran ﬁve times with different random seeds and we reported the averaged value
with standard deviations.
The results of perplexity and topic coherence are shown in Figure 3 and Table 1  respectively. We
have the following remarks on the results: (1) In general  for the models with DirBN  the performance
is signiﬁcantly improved compared with the counterparts without DirBN  especially in terms of
perplexity and topic coherence and with low proportion of the training words. (2) In terms of all the
measures  DirBN-2/3 always has better results than DirBN-1. Whereas if we compare GBN with PFA 
its perplexity is worse than PFA’s  especially for sparse texts. This demonstrates that hierarchical
structures on θ (i.e.  GBN) may not perform as well as hierarchical structures on φ (i.e.  DirBN) on
sparse texts. (3) Although PFA+DirBN-1 and PFA+Mallet both impose a symmetric Dirichlet on φ 
the former usually has better perplexity. (4) The dual deep model (GBN+DirBN-3) usually performs
the best on topic coherence  which demonstrates the beneﬁts of the deep structures.

Qualitative analysis on topic hierarchies
8 GBN+DirBN is a dual deep model that discovers two
sets of hierarchies  one induced by GBN on θ and the other induced by DirBN on φ. The topics in the

7We used the Palmetto package (http://palmetto.aksw.org) with a large Wikipedia dump.
8More visualisations of topic hierarchies are shown in the supplementary material.

8

(a)

(b)

Figure 4: Topic hierarchies discovered by MetaLDA+DirBN. The topics in the yellow and blue
rectangles are the second and ﬁrst layer topics in DirBN and the correlated labels to the ﬁrst-layer
topics are shown at the bottom of each ﬁgure. Thicker arrows indicate stronger correlations.

ﬁrst layer of DirBN connect the two sets of hierarchies. In Table 2  we show the ﬁrst-layer topics and
the correlated second-layer topics in the two hierarchies. It is interesting to see that the second-layer
topics of DirBN are more abstract. For example  the second topic is about teams and player in NBA 
while its correlated second-layer topics are more general words for sports. Moreover  DirBN is able
to discover layer-wise semantically meaningful topic correlations with fewer overlapping top words.
This is because GBN combines the words in the ﬁrst-layer topics to form the second-layer topics 
whereas DirBN decomposes the ﬁrst-layer topics into the second-layer ones.
In MetaLDA+DirBN  the MetaLDA part is able to use document labels to construct TPs (Zhao et al. 
2017a)  by learning a correlation matrix between the labels and topics  while the DirBN part learns
the topic hierarchy. The ﬁrst-layer topics of DirBN link the correlation matrix and the topic hierarchy
together. Figure 4 shows the sample linkages between topic hierarchies and labels on TMN  where
the documents are labelled with 7 categories: 1 sport  2 business  3 us  4 entertainment  5 world  6
health  7 sci-tech. One can observe that there is a well correspondence between the topic hierarchies
and the labels.

6 Conclusions

We have presented DirBN  a multi-layer process generating word distributions of topics. With
real topics in each layer  DirBN is able to discover interpretable topic hierarchies. As a ﬂexible
module  DirBN can be adapted to other advanced topic models and improve the performance and
interpretability  especially on sparse texts. We have demonstrated DirBN’s advantages by equipping
PFA  MetaLDA  and GBN  with DirBN. With the help of data augmentation  the inference of DirBN
can be done by a layer-wise Gibbs sampling  as a full conjugate model.
Future directions include deriving alternative inference algorithms  such as variational inference (Hoff-
man et al.  2013)  conditional density ﬁltering (Guhaniyogi et al.  2018)  and stochastic gradient-based
approaches (Chen et al.  2014; Ding et al.  2014; Welling and Teh  2011).

9

49 health treat condition studieschicken idea scientists routineheart develop fare therapy54 scientists world species climatefarmers endangered study foundchange farm wild ﬁsh61 study health people researchersmedical kids children diseasepatients brain doctors heart66 time good make backlife day years questionsnews bad job year81 cancer drug study riskdrugs heart patients womenhealth breast fda prostate3 us6 health7 sci_tech1 sport2 business4 entertainment12 economy economic crisis japanglobal rising surge analystsfocus markets fears recovery25 china united states worldeurope india countries globaltrade european group nations85 prices sales year marchgrowth april percent economyrate economic report home93 proﬁt stocks quarter percentearnings year shares saleshigher wall street investors2 business5 world6 health7 sci_tech3 usAcknowledgments

M. Zhou acknowledges the support of Award IIS-1812699 from the U.S. National Science Foundation.

References
D. M. Blei  A. Y. Ng  and M. I. Jordan  “Latent Dirichlet allocation ” JMLR  vol. 3  pp. 993–1022 

2003.

D. M. Blei  T. L. Grifﬁths  and M. I. Jordan  “The nested Chinese restaurant process and Bayesian

nonparametric inference of topic hierarchies ” Journal of the ACM  vol. 57  no. 2  p. 7  2010.

J. Paisley  C. Wang  D. M. Blei  and M. I. Jordan  “Nested hierarchical Dirichlet processes ” TPAMI 

vol. 37  no. 2  pp. 256–270  2015.

G. E. Hinton and R. R. Salakhutdinov  “Replicated softmax: An undirected topic model ” in NIPS 

2009  pp. 1607–1614.

H. Larochelle and S. Lauly  “A neural autoregressive topic model ” in NIPS  2012  pp. 2708–2716.

N. Srivastava  R. Salakhutdinov  and G. Hinton  “Modeling documents with a deep Boltzmann

machine ” in UAI  2013  pp. 616–624.

A. Srivastava and C. Sutton  “Autoencoding variational inference for topic models ” 2017.

Y. Miao  E. Grefenstette  and P. Blunsom  “Discovering discrete latent topics with neural variational

inference ” in ICML  2017  pp. 2410–2419.

H. Zhang  B. Chen  D. Guo  and M. Zhou  “WHAI: Weibull hybrid autoencoding inference for deep

topic modeling ” 2018.

G. E. Hinton  S. Osindero  and Y.-W. Teh  “A fast learning algorithm for deep belief nets ” Neural

computation  vol. 18  no. 7  pp. 1527–1554  2006.

Z. Gan  C. Chen  R. Henao  D. Carlson  and L. Carin  “Scalable deep Poisson factor analysis for

topic modeling ” in ICML  2015  pp. 1823–1832.

R. Ranganath  L. Tang  L. Charlin  and D. Blei  “Deep exponential families ” in AISTATS  2015  pp.

762–771.

R. Henao  Z. Gan  J. Lu  and L. Carin  “Deep Poisson factor modeling ” in NIPS  2015  pp. 2800–2808.

M. Zhou  Y. Cong  and B. Chen  “Augmentable gamma belief networks ” JMLR  vol. 17  no. 163  pp.

1–44  2016.

J. D. Mcauliffe and D. M. Blei  “Supervised topic models ” in NIPS  2008  pp. 121–128.

M. Rosen-Zvi  T. Grifﬁths  M. Steyvers  and P. Smyth  “The author-topic model for authors and

documents ” in UAI  2004  pp. 487–494.

M. Zhou  L. Hannah  D. B. Dunson  and L. Carin  “Beta-negative binomial process and Poisson

factor analysis ” in AISTATS  2012  pp. 1462–1471.

H. Zhao  L. Du  W. Buntine  and G. Liu  “Metalda: A topic model that efﬁciently incorporates meta

information ” in ICDM  2017  pp. 635–644.

J. D. Lafferty and D. M. Blei  “Correlated topic models ” in NIPS  2006  pp. 147–154.

Y. Teh  M. Jordan  M. Beal  and D. Blei  “Hierarchical Dirichlet processes ” Journal of the American

Statistical Association  vol. 101  no. 476  pp. 1566–1581  2012.

Y. Tang and R. R. Salakhutdinov  “Learning stochastic feedforward neural networks ” in NIPS  2013 

pp. 530–538.

M. Zhou  “Inﬁnite edge partition models for overlapping community detection and link prediction ”

in AISTATS  2015  pp. 1135—-1143.

10

M. Zhou and L. Carin  “Negative binomial process count and mixture modeling ” TPAMI  vol. 37 

no. 2  pp. 307–320  2015.

H. Zhao  L. Du  and W. Buntine  “Leveraging node attributes for incomplete relational data ” in

ICML  2017  pp. 4072–4081.

M. Zhou  “Nonparametric Bayesian negative binomial factor analysis ” Bayesian Analysis  2018.

H. Zhao  L. Du  W. Buntine  and G. Liu  “Leveraging external information in topic modelling ” KAIS 

pp. 1–33  2018.

M. Zhou  Y. Cong  and B. Chen  “The Poisson gamma belief network ” in NIPS  2015  pp. 3043–3051.

H. M. Wallach  D. M. Mimno  and A. McCallum  “Rethinking LDA: Why priors matter ” in NIPS 

2009  pp. 1973–1981.

H. Zhao  L. Du  and W. Buntine  “A word embeddings informed focused topic model ” in ACML 

2017  pp. 423–438.

H. Zhao  L. Du  W. Buntine  and M. Zhou  “Inter and intra topic structure learning with word

embeddings ” in ICML  2018  pp. 5887–5896.

I. Sato and H. Nakagawa  “Topic models with power-law using Pitman-Yor process ” in SIGKDD 

2010  pp. 673–682.

W. L. Buntine and S. Mishra  “Experiments with non-parametric topic models ” in SIGKDD  2014 

pp. 881–890.

C. Chen  W. Buntine  N. Ding  L. Xie  and L. Du  “Differential topic models ” TPAMI  vol. 37  no. 2 

pp. 230–242  2015.

R. V. Lindsey  W. P. Headden III  and M. J. Stipicevic  “A phrase-discovering topic model using

hierarchical Pitman-Yor processes ” in EMNLP  2012  pp. 214–222.

C. Archambeau  B. Lakshminarayanan  and G. Bouchard  “Latent IBP compound Dirichlet allocation ”

TPAMI  vol. 37  no. 2  pp. 321–333  2015.

F. Wood and Y. W. Teh  “A hierarchical nonparametric Bayesian approach to statistical language

model domain adaptation ” in AISTATS  2009  pp. 607–614.

L. Du  W. Buntine  and H. Jin  “Modelling sequential text with an adaptive topic model ” in EMNLP 

2012  pp. 535–545.

J. H. Kim  D. Kim  S. Kim  and A. Oh  “Modeling topic hierarchies with the recursive chinese

restaurant process ” in CIKM  2012  pp. 783–792.

A. Ahmed  L. Hong  and A. Smola  “Nested Chinese restaurant franchise process: Applications to

user tracking and document modeling ” in ICML  2013  pp. 1426–1434.

W. Li and A. McCallum  “Pachinko allocation: DAG-structured mixture models of topic correlations ”

in ICML  2006  pp. 577–584.

Y. Cong  B. Chen  H. Liu  and M. Zhou  “Deep latent Dirichlet allocation with topic-layer-adaptive

stochastic gradient Riemannian MCMC ” in ICML  2017  pp. 864–873.

N. Aletras and M. Stevenson  “Evaluating topic coherence using distributional semantics ” in Proc. of

the 10th Intnl. Conf. on Computational Semantics  2013  pp. 13–22.

J. H. Lau  D. Newman  and T. Baldwin  “Machine reading tea leaves: Automatically evaluating topic

coherence and topic model quality ” in EACL  2014  pp. 530–539.

Y. Yang  D. Downey  and J. Boyd-Graber  “Efﬁcient methods for incorporating knowledge into topic

models ” in EMNLP  2015  pp. 308–317.

M. D. Hoffman  D. M. Blei  C. Wang  and J. Paisley  “Stochastic variational inference ” JMLR 

vol. 14  no. 1  pp. 1303–1347  2013.

11

R. Guhaniyogi  S. Qamar  and D. B. Dunson  “Bayesian conditional density ﬁltering ” Journal of

Computational and Graphical Statistics  2018.

T. Chen  E. Fox  and C. Guestrin  “Stochastic gradient Hamiltonian Monte Carlo ” in ICML  2014 

pp. 1683–1691.

N. Ding  Y. Fang  R. Babbush  C. Chen  R. D. Skeel  and H. Neven  “Bayesian sampling using

stochastic gradient thermostats ” in NIPS  2014  pp. 3203–3211.

M. Welling and Y. W. Teh  “Bayesian learning via stochastic gradient Langevin dynamics ” in ICML 

2011  pp. 681–688.

12

,He Zhao
Lan Du
Wray Buntine
Mingyuan Zhou