2018,Entropy and mutual information in models of deep neural networks,We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods  under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights  using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets  on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual information throughout learning and conclude that  in the proposed setting  the relationship between compression and generalization remains elusive.,Entropy and mutual information in

models of deep neural networks

Marylou Gabrié∗1  Andre Manoel2 3  Clément Luneau4  Jean Barbier1 4 5  Nicolas Macris4 

Florent Krzakala1 6 7 and Lenka Zdeborová3 6

1Laboratoire de Physique Statistique  École Normale Supérieure  PSL University
2Parietal Team  INRIA  CEA  Université Paris-Saclay & Owkin Inc.  New York

3Institut de Physique Théorique  CEA  CNRS  Université Paris-Saclay

4Laboratoire de Théorie des Communications  École Polytechnique Fédérale de Lausanne

5International Center for Theoretical Physics  Trieste  Italy
6Department of Mathematics  Duke University  Durham NC

7Sorbonne Universités & LightOn Inc.  Paris

Abstract

We examine a class of stochastic deep learning models with a tractable method to
compute information-theoretic quantities. Our contributions are three-fold: (i) We
show how entropies and mutual informations can be derived from heuristic statisti-
cal physics methods  under the assumption that weight matrices are independent
and orthogonally-invariant. (ii) We extend particular cases in which this result is
known to be rigorously exact by providing a proof for two-layers networks with
Gaussian random weights  using the recently introduced adaptive interpolation
method. (iii) We propose an experiment framework with generative models of
synthetic datasets  on which we train deep neural networks with a weight constraint
designed so that the assumption in (i) is veriﬁed during learning. We study the be-
havior of entropies and mutual informations throughout learning and conclude that 
in the proposed setting  the relationship between compression and generalization
remains elusive.

The successes of deep learning methods have spurred efforts towards quantitative modeling of the
performance of deep neural networks. In particular  an information-theoretic approach linking
generalization capabilities to compression has been receiving increasing interest. The intuition behind
the study of mutual informations in latent variable models dates back to the information bottleneck
(IB) theory of [1]. Although recently reformulated in the context of deep learning [2]  verifying its
relevance in practice requires the computation of mutual informations for high-dimensional variables 
a notoriously hard problem. Thus  pioneering works in this direction focused either on small network
models with discrete (continuous  eventually binned) activations [3]  or on linear networks [4  5].
In the present paper we follow a different direction  and build on recent results from statistical physics
[6  7] and information theory [8  9] to propose  in Section 1  a formula to compute information-
theoretic quantities for a class of deep neural network models. The models we approach  described in
Section 2  are non-linear feed-forward neural networks trained on synthetic datasets with constrained
weights. Such networks capture some of the key properties of the deep learning setting that are
usually difﬁcult to include in tractable frameworks: non-linearities  arbitrary large width and depth 
and correlations in the input data. We demonstrate the proposed method in a series of numerical
experiments in Section 3. First observations suggest a rather complex picture  where the role of
compression in the generalization ability of deep neural networks is yet to be elucidated.

∗Corresponding author: marylou.gabrie@ens.fr

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

1 Multi-layer model and main theoretical results

A stochastic multi-layer model— We consider a model of multi-layer stochastic feed-forward
neural network where each element xi of the input layer x ∈ Rn0 is distributed independently
as P0(xi)  while hidden units t(cid:96) i at each successive layer t(cid:96) ∈ Rn(cid:96) (vectors are column vectors)
(cid:124)
come from P(cid:96)(t(cid:96) i|W
(cid:96) it(cid:96)−1)  with t0 ≡ x and W(cid:96) i denoting the i-th row of the matrix of weights
W(cid:96) ∈ Rn(cid:96)×n(cid:96)−1. In other words
t0 i ≡ xi ∼ P0(·) 

tL i ∼ PL(·|W

t1 i ∼ P1(·|W

(cid:124)
L itL−1) 

(cid:124)
1 ix) 

(1)

. . .

(cid:124)

(cid:124)

(cid:96)=1 and distributions {P(cid:96)}L

(cid:96) it(cid:96)−1  ξ(cid:96) i)(cid:1).

i=1  and the activation function ϕ(cid:96) applied componentwise.

(cid:96) it(cid:96)−1) =(cid:82) dPξ(ξ(cid:96) i) δ(cid:0)t(cid:96) i−ϕ(cid:96)(W

given a set of weight matrices {W(cid:96)}L
(cid:96)=1 which encode possible non-
linearities and stochastic noise applied to the hidden layer variables  and P0 that generates the visible
variables. In particular  for a non-linearity t(cid:96) i = ϕ(cid:96)(h  ξ(cid:96) i)  where ξ(cid:96) i ∼ Pξ(·) is the stochastic
noise (independent for each i)  we have P(cid:96)(t(cid:96) i|W
Model (1) thus describes a Markov chain which we denote by X → T1 → T2 → ··· → TL  with
T(cid:96) = ϕ(cid:96)(W(cid:96)T(cid:96)−1  ξ(cid:96))  ξ(cid:96) = {ξ(cid:96) i}n(cid:96)
Replica formula— We shall work in the asymptotic high-dimensional statistics regime where all
˜α(cid:96) ≡ n(cid:96)/n0 are of order one while n0 →∞  and make the important assumption that all matrices
W(cid:96) are orthogonally-invariant random matrices independent from each other; in other words  each
matrix W(cid:96) ∈ Rn(cid:96)×n(cid:96)−1 can be decomposed as a product of three matrices  W(cid:96) = U(cid:96)S(cid:96)V(cid:96)  where
U(cid:96) ∈ O(n(cid:96)) and V(cid:96) ∈ O(n(cid:96)−1) are independently sampled from the Haar measure  and S(cid:96) is a
diagonal matrix of singular values. The main technical tool we use is a formula for the entropies
of the hidden variables  H(T(cid:96)) = −ET(cid:96) ln PT(cid:96)(t(cid:96))  and the mutual information between adjacent
layers I(T(cid:96); T(cid:96)−1) = H(T(cid:96)) + ET(cid:96) T(cid:96)−1 ln PT(cid:96)|T(cid:96)−1(t(cid:96)|t(cid:96)−1)  based on the heuristic replica method
[10  11  6  7]:
Claim 1 (Replica formula). Assume model (1) with L layers in the high-dimensional limit with
componentwise activation functions and weight matrices generated from the ensemble described
(cid:124)
k Wk. Then for any (cid:96) ∈ {1  . . .   L} the normalized
above  and denote by λWk the eigenvalues of W
entropy of T(cid:96) is given by the minimum among all stationary points of the replica potential:

lim
n0→∞

1
n0

H(T(cid:96)) = min extr

A V   ˜A  ˜V

φ(cid:96)(A  V   ˜A  ˜V ) 

(2)

which depends on (cid:96)-dimensional vectors A  V   ˜A  ˜V   and is written in terms of mutual information
I and conditional entropies H of scalar variables as

(cid:2) ˜AkVk + αkAk ˜Vk − FWk (AkVk)(cid:3)

˜αk−1

(cid:105)

−1
k+1)

φ(cid:96)(A  V   ˜A  ˜V ) = I

t0; t0 +

(cid:16)

(cid:96)−1(cid:88)

(cid:104)

(cid:17)

ξ0(cid:112) ˜A1

1
2

−

(cid:96)(cid:88)

k=1

1
2

+

k=1

˜αk

H(tk|ξk; ˜Ak+1  ˜Vk  ˜ρk) −

where αk = nk/nk−1  ˜αk = nk/n0  ρk = (cid:82) dPk−1(t) t2  ˜ρk = (EλWk

λWk )ρk/αk  and
ξk ∼ N (0  1) for k = 0  . . .   (cid:96). In the computation of the conditional entropies in (3)  the scalar
tk-variables are generated from P (t0) = P0(t0) and

+ ˜α(cid:96)H(t(cid:96)|ξ(cid:96); ˜V(cid:96)  ˜ρ(cid:96)) 

log(2πe ˜A

(3)

(cid:112)
P (tk|ξk; A  V  ρ) = E ˜ξ ˜z Pk(tk + ˜ξ/√A|

(cid:112)
P (t(cid:96)|ξ(cid:96); V  ρ) = E˜z P(cid:96)(t(cid:96)|

ρ − V ξk + √V ˜z) 

k = 1  . . .   (cid:96) − 1 

(4)

(5)
where ˜ξ and ˜z are independent N (0  1) random variables. Finally  the function FWk (x) depends on
the distribution of the eigenvalues λW(cid:96) following
(6)

ρ − V ξ(cid:96) + √V ˜z) 
(cid:8)2αkθ + (αk − 1) ln(1 − θ) + EλWk

ln[xλWk + (1 − θ)(1 − αkθ)](cid:9).

FWk (x) = min
θ∈R

The computation of the entropy in the large dimensional limit  a computationally difﬁcult task  has
thus been reduced to an extremization of a function of 4(cid:96) variables  that requires evaluating single or
bidimensional integrals. This extremization can be done efﬁciently by means of a ﬁxed-point iteration
starting from different initial conditions  as detailed in the Supplementary Material [12]. Moreover  a

2

user-friendly Python package is provided [13]  which performs the computation for different choices
of prior P0  activations ϕ(cid:96) and spectra λW(cid:96). Finally  the mutual information between successive layers
I(T(cid:96); T(cid:96)−1) can be obtained from the entropy following the evaluation of an additional bidimensional
integral  see Section 1.6.1 of the Supplementary Material [12].
Our approach in the derivation of (3) builds on recent progresses in statistical estimation and
information theory for generalized linear models following the application of methods from statistical
physics of disordered systems [10  11] in communication [14]  statistics [15] and machine learning
problems [16  17]. In particular  we use advanced mean ﬁeld theory [18] and the heuristic replica
method [10  6]  along with its recent extension to multi-layer estimation [7  8]  in order to derive the
above formula (3). This derivation is lengthy and thus given in the Supplementary Material [12]. In
a related contribution  Reeves [9] proposed a formula for the mutual information in the multi-layer
setting  using heuristic information-theoretic arguments. As ours  it exhibits layer-wise additivity 
and the two formulas are conjectured to be equivalent.
Rigorous statement— We recall the assumptions under which the replica formula of Claim 1 is
conjectured to be exact: (i) weight matrices are drawn from an ensemble of random orthogonally-
invariant matrices  (ii) matrices at different layers are statistically independent and (iii) layers have
a large dimension and respective sizes of adjacent layers are such that weight matrices have aspect
k=1 of order one. While we could not prove the replica prediction in full generality 
ratios {αk  ˜αk}(cid:96)
we stress that it comes with multiple credentials: (i) for Gaussian prior P0 and Gaussian distributions
P(cid:96)  it corresponds to the exact analytical solution when weight matrices are independent of each
other (see Section 1.6.2 of the Supplementary Material [12]). (ii) In the single-layer case with a
Gaussian weight matrix  it reduces to formula (6) in the Supplementary Material [12]  which has
been recently rigorously proven for (almost) all activation functions ϕ [19]. (iii) In the case of
Gaussian distributions P(cid:96)  it has also been proven for a large ensemble of random matrices [20] and
(iv) it is consistent with all the results of the AMP [21  22  23] and VAMP [24] algorithms  and their
multi-layer versions [7  8]  known to perform well for these estimation problems.
In order to go beyond results for the single-layer problem and heuristic arguments  we prove Claim 1
for the more involved multi-layer case  assuming Gaussian i.i.d. matrices and two non-linear layers:
Theorem 1 (Two-layer Gaussian replica formula). Suppose (H1) the input units distribution
P0 is separable and has bounded support; (H2) the activations ϕ1 and ϕ2 corresponding to
(cid:124)
2 it1) are bounded C2 with bounded ﬁrst and second derivatives
P1(t1 i|W
w.r.t their ﬁrst argument; and (H3) the weight matrices W1  W2 have Gaussian i.i.d. entries. Then
for model (1) with two layers L = 2 the high-dimensional limit of the entropy veriﬁes Claim 1.

(cid:124)
1 ix) and P2(t2 i|W

The theorem  that closes the conjecture presented in [7]  is proven using the adaptive interpolation
method of [25  19] in a multi-layer setting  as ﬁrst developed in [26]. The lengthy proof  presented in
details in the Supplementary Material [12]  is of independent interest and adds further credentials to
the replica formula  as well as offers a clear direction to further developments. Note that  following
the same approximation arguments as in [19] where the proof is given for the single-layer case  the
hypothesis (H1) can be relaxed to the existence of the second moment of the prior  (H2) can be
dropped and (H3) extended to matrices with i.i.d. entries of zero mean  O(1/n0) variance and ﬁnite
third moment.

2 Tractable models for deep learning

x is distributed according to a separable prior distribution PX (x) =(cid:81)

The multi-layer model presented above can be leveraged to simulate two prototypical settings of deep
supervised learning on synthetic datasets amenable to the replica tractable computation of entropies
and mutual informations.
The ﬁrst scenario is the so-called teacher-student (see Figure 1  left). Here  we assume that the input
i P0(xi)  factorized in the
components of x  and the corresponding label y is given by applying a mapping x → y  called
the teacher. After generating a train and test set in this manner  we perform the training of a deep
neural network  the student  on the synthetic dataset. In this case  the data themselves have a simple
structure given by P0.
In constrast  the second scenario allows generative models (see Figure 1  right) that create more
structure  and that are reminiscent of the generative-recognition pair of models of a Variational

3

(cid:81)

Autoencoder (VAE). A code vector y is sampled from a separable prior distribution PY (y) =
i P0(yi) and a corresponding data point x is generated by a possibly stochastic neural network  the
generative model. This setting allows to create input data x featuring correlations  differently from
the teacher-student scenario. The studied supervised learning task then consists in training a deep
neural net  the recognition model  to recover the code y from x.
In both cases  the chain going from X to any
later layer is a Markov chain in the form of (1).
In the ﬁrst scenario  model (1) directly maps
to the student network. In the second scenario
however  model (1) actually maps to the feed-
forward combination of the generative model
followed by the recognition model. This shift
is necessary to verify the assumption that the
starting point (now given by Y ) has a separable
distribution. In particular  it generates correlated
input data X while still allowing for the compu-
tation of the entropy of any T(cid:96).
At the start of a neural network training  weight
matrices initialized as i.i.d. Gaussian random
matrices satisfy the necessary assumptions of
the formula of Claim 1. In their singular value
decomposition

Figure 1: Two models of synthetic data

W(cid:96) = U(cid:96)S(cid:96)V(cid:96)

(7)

the matrices U(cid:96) ∈ O(n(cid:96)) and V(cid:96) ∈ O(n(cid:96)−1)  are typical independent samples from the Haar measure
across all layers. To make sure weight matrices remain close enough to independent during learning 
we deﬁne a custom weight constraint which consists in keeping U(cid:96) and V(cid:96) ﬁxed while only the matrix
S(cid:96)  constrained to be diagonal  is updated. The number of parameters is thus reduced from n(cid:96) × n(cid:96)−1
to min(n(cid:96)  n(cid:96)−1). We refer to layers following this weight constraint as USV-layers. For the replica
formula of Claim 1 to be correct  the matrices S(cid:96) from different layers should furthermore remain
uncorrelated during the learning. In Section 3  we consider the training of linear networks for which
information-theoretic quantities can be computed analytically  and conﬁrm numerically that with
USV-layers the replica predicted entropy is correct at all times. In the following  we assume that is
also the case for non-linear networks.
In Section 3.2 of the Supplementary Material [12]we train a neural network with USV-layers on
a simple real-world dataset (MNIST)  showing that these layers can learn to represent complex
functions despite their restriction. We further note that such a product decomposition is reminiscent
of a series of works on adaptative structured efﬁcient linear layers (SELLs and ACDC) [27  28]
motivated this time by speed gains  where only diagonal matrices are learned (in these works the
matrices U(cid:96) and V(cid:96) are chosen instead as permutations of Fourier or Hadamard matrices  so that
the matrix multiplication can be replaced by fast transforms). In Section 3  we discuss learning
experiments with USV-layers on synthetic datasets.
While we have deﬁned model (1) as a stochastic model  traditional feed forward neural networks are
deterministic. In the numerical experiments of Section 3  we train and test networks without injecting
noise  and only assume a noise model in the computation of information-theoretic quantities. Indeed 
for continuous variables the presence of noise is necessary for mutual informations to remain ﬁnite
(see discussion of Appendix C in [5]). We assume at layer (cid:96) an additive white Gaussian noise of small
amplitude just before passing through its activation function to obtain H(T(cid:96)) and I(T(cid:96); T(cid:96)−1)  while
keeping the mapping X → T(cid:96)−1 deterministic. This choice attempts to stay as close as possible to
the deterministic neural network  but remains inevitably somewhat arbitrary (see again discussion of
Appendix C in [5]).
Other related works— The strategy of studying neural networks models  with random weight
matrices and/or random data  using methods originated in statistical physics heuristics  such as the
replica and the cavity methods [10] has a long history. Before the deep learning era  this approach
led to pioneering results in learning for the Hopﬁeld model [29] and for the random perceptron
[30  31  16  17].

4

 teacherstudentgenerativerecognitionteacher-student(i.i.d. input data)generative-recognition(correlated input data)Recently  the successes of deep learning along with the disqualifying complexity of studying real
world problems have sparked a revived interest in the direction of random weight matrices. Recent
results –without exhaustivity– were obtained on the spectrum of the Gram matrix at each layer using
random matrix theory [32  33]  on expressivity of deep neural networks [34]  on the dynamics of
propagation and learning [35  36  37  38]  on the high-dimensional non-convex landscape where the
learning takes place [39]  or on the universal random Gaussian neural nets of [40].
The information bottleneck theory [1] applied to neural networks consists in computing the mutual
information between the data and the learned hidden representations on the one hand  and between
labels and again hidden learned representations on the other hand [2  3]. A successful training should
maximize the information with respect to the labels and simultaneously minimize the information
with respect to the input data  preventing overﬁtting and leading to a good generalization. While this
intuition suggests new learning algorithms and regularizers [41  42  43  44  45  46  47]  we can also
hypothesize that this mechanism is already at play in a priori unrelated commonly used optimization
methods  such as the simple stochastic gradient descent (SGD). It was ﬁrst tested in practice by [3]
on very small neural networks  to allow the entropy to be estimated by binning of the hidden neurons
activities. Afterwards  the authors of [5] reproduced the results of [3] on small networks using the
continuous entropy estimator of [45]  but found that the overall behavior of mutual information during
learning is greatly affected when changing the nature of non-linearities. Additionally  they investigate
the training of larger linear networks on i.i.d. normally distributed inputs where entropies at each
hidden layer can be computed analytically for an additive Gaussian noise. The strategy proposed
in the present paper allows us to evaluate entropies and mutual informations in non-linear networks
larger than in [5  3].

3 Numerical experiments

We present a series of experiments both aiming at further validating the replica estimator and
leveraging its power in noteworthy applications. A ﬁrst application presented in the paragraph
3.1 consists in using the replica formula in settings where it is proven to be rigorously exact as
a basis of comparison for other entropy estimators. The same experiment also contributes to the
discussion of the information bottleneck theory for neural networks by showing how  without any
learning  information-theoretic quantities have different behaviors for different non-linearities. In the
following paragraph 3.2  we validate the accuracy of the replica formula in a learning experiment
with USV-layers —where it is not proven to be exact — by considering the case of linear networks
for which information-theoretic quantities can be otherwise computed in closed-form. We ﬁnally
consider in the paragraph 3.3  a second application testing the information bottleneck theory for large
non-linear networks. To this aim  we use the replica estimator to study compression effects during
learning.
3.1 Estimators and activation comparisons— Two non-parametric estimators have already been
considered by [5] to compute entropies and/or mutual informations during learning. The kernel-
density approach of Kolchinsky et. al. [45] consists in ﬁtting a mixture of Gaussians (MoG) to samples
of the variable of interest and subsequently compute an upper bound on the entropy of the MoG [48].
The method of Kraskov et al. [49] uses nearest neighbor distances between samples to directly build
an estimate of the entropy. Both methods require the computation of the matrix of distances between
samples. Recently  [46] proposed a new non-parametric estimator for mutual informations which
involves the optimization of a neural network to tighten a bound. It is unfortunately computationally
hard to test how these estimators behave in high dimension as even for a known distribution the
computation of the entropy is intractable (#P-complete) in most cases. However the replica method
proposed here is a valuable point of comparison for cases where it is rigorously exact.
In the ﬁrst numerical experiment we place ourselves in the setting of Theorem 1: a 2-layer network
with i.i.d weight matrices  where the formula of Claim 1 is thus rigorously exact in the limit of large
networks  and we compare the replica results with the non-parametric estimators of [45] and [49].
Note that the requirement for smooth activations (H2) of Theorem 1 can be relaxed (see discussion
below the Theorem). Additionally  non-smooth functions can be approximated arbitrarily closely by
smooth functions with equal information-theoretic quantities  up to numerical precision.
We consider a neural network with layers of equal size n = 1000 that we denote: X → T1 → T2.
The input variable components are i.i.d. Gaussian with mean 0 and variance 1. The weight matrices

5

entries are also i.i.d. Gaussian with mean 0. Their standard-deviation is rescaled by a factor 1/√n
and then multiplied by a coefﬁcient σ varying between 0.1 and 10  i.e. around the recommended
value for training initialization. To compute entropies  we consider noisy versions of the latent
noise = 10−5) is added
variables where an additive white Gaussian noise of very small variance (σ2
right before the activation function  T1 = f (W1X + 1) and T2 = f (W2f (W1X) + 2) with
noiseIn)  which is also done in the remaining experiments to guarantee the mutual
1 2 ∼ N (0  σ2
informations to remain ﬁnite. The non-parametric estimators [45  49] were evaluated using 1000
samples  as the cost of computing pairwise distances is signiﬁcant in such high dimension and we
checked that the entropy estimate is stable over independent draws of a sample of such a size (error
bars smaller than marker size). On Figure 2  we compare the different estimates of H(T1) and H(T2)
for different activation functions: linear  hardtanh or ReLU. The hardtanh activation is a piecewise
linear approximation of the tanh  hardtanh(x) =−1 for x <−1  x for −1 < x < 1  and 1 for x > 1 
for which the integrals in the replica formula can be evaluated faster than for the tanh.
In the linear and hardtanh case  the non-parametric methods are following the tendency of the replica
estimate when σ is varied  but appear to systematically over-estimate the entropy. For linear networks
with Gaussian inputs and additive Gaussian noise  every layer is also a multivariate Gaussian and
therefore entropies can be directly computed in closed form (exact in the plot legend). When using
the Kolchinsky estimate in the linear case we also check the consistency of two strategies  either
ﬁtting the MoG to the noisy sample or ﬁtting the MoG to the deterministic part of the T(cid:96) and augment
noise  as done in [45] (Kolchinsky et al. parametric in the plot legend).
the resulting variance with σ2
In the network with hardtanh non-linearities  we check that for small weight values  the entropies
are the same as in a linear network with same weights (linear approx in the plot legend  computed
using the exact analytical result for linear networks and therefore plotted in a similar color to exact).
Lastly  in the case of the ReLU-ReLU network  we note that non-parametric methods are predicting
an entropy increasing as the one of a linear network with identical weights  whereas the replica
computation reﬂects its knowledge of the cut-off and accurately features a slope equal to half of the
linear network entropy (1/2 linear approx in the plot legend). While non-parametric estimators are
invaluable tools able to approximate entropies from the mere knowledge of samples they inevitably
introduce estimation errors. The replica method is taking the opposite view. While being restricted to
a class of models  it can leverage its knowledge of the neural network structure to provide a reliable
estimate. To our knowledge  there is no other entropy estimator able to incorporate such information
about the underlying multi-layer model.
Beyond informing about estimators accuracy  this experiment also unveils a simple but possibly
important distinction between activation functions. For the hardtanh activation  as the random weights
magnitude increases  the entropies decrease after reaching a maximum  whereas they only increase
for the unbounded activation functions we consider – even for the single-side saturating ReLU. This
loss of information for bounded activations was also observed by [5]  where entropies were computed
by discretizing the output as a single neuron with bins of equal size. In this setting  as the tanh
activation starts to saturate for large inputs  the extreme bins (at −1 and 1) concentrate more and
more probability mass  which explains the information loss. Here we conﬁrm that the phenomenon is
also observed when computing the entropy of the hardtanh (without binning and with small noise
injected before the non-linearity). We check via the replica formula that the same phenomenology
arises for the mutual informations I(X; T(cid:96)) (see Section3.1 of the Supplementary Material [12]).
3.2 Learning experiments with linear networks— In the following  and in Section 3.3 of the Sup-
plementary Material [12]  we discuss training experiments of different instances of the deep learning
models deﬁned in Section 2. We seek to study the simplest possible training strategies achieving
good generalization. Hence for all experiments we use plain stochastic gradient descent (SGD) with
constant learning rates  without momentum and without any explicit form of regularization. The
sizes of the training and testing sets are taken equal and scale typically as a few hundreds times the
size of the input layer. Unless otherwise stated  plots correspond to single runs  yet we checked
over a few repetitions that outcomes of independent runs lead to identical qualitative behaviors. The
values of mutual informations I(X; T(cid:96)) are computed by considering noisy versions of the latent
noise = 10−5) is added
variables where an additive white Gaussian noise of very small variance (σ2
right before the activation function  as in the previous experiment. This noise is neither present
at training time  where it could act as a regularizer  nor at testing time. Given the noise is only
assumed at the last layer  the second to last layer is a deterministic mapping of the input variable;
hence the replica formula yielding mutual informations between adjacent layers gives us directly

6

Figure 2: Entropy of latent variables in stochastic networks X → T1 → T2  with equally sized
layers n = 1000  inputs drawn from N (0  In)  weights from N (0  σ2In2/n)  as a function of the
weight scaling parameter σ. An additive white Gaussian noise N (0  10−5In) is added inside the
non-linearity. Left column: linear network. Center column: hardtanh-hardtanh network. Right
column: ReLU-ReLU network.

I(T(cid:96); T(cid:96)−1) = H(T(cid:96)) − H(T(cid:96)|T(cid:96)−1) = H(T(cid:96)) − H(T(cid:96)|X) = I(T(cid:96); X). We provide a second
Python package [50] to implement in Keras learning experiments on synthetic datasets  using USV-
layers and interfacing the ﬁrst Python package [13] for replica computations.
To start with we consider the training of a linear network in the teacher-student scenario. The teacher
has also to be linear to be learnable: we consider a simple single-layer network with additive white
Gaussian noise  Y = ˜WteachX +   with input x ∼ N (0  In) of size n  teacher matrix ˜Wteach
i.i.d. normally distributed as N (0  1/n)   noise  ∼ N (0  0.01In)  and output of size nY = 4.
We train a student network of three USV-layers  plus one fully connected unconstrained layer
X → T1 → T2 → T3 → ˆY on the regression task  using plain SGD for the MSE loss ( ˆY − Y )2.
We recall that in the USV-layers (7) only the diagonal matrix is updated during learning. On the
left panel of Figure 3  we report the learning curve and the mutual informations between the hidden
layers and the input in the case where all layers but outputs have size n = 1500. Again this linear
setting is analytically tractable and does not require the replica formula  a similar situation was
studied in [5]. In agreement with their observations  we ﬁnd that the mutual informations I(X; T(cid:96))
keep on increasing throughout the learning  without compromising the generalization ability of the
student. Now  we also use this linear setting to demonstrate (i) that the replica formula remains
correct throughout the learning of the USV-layers and (ii) that the replica method gets closer and
closer to the exact result in the limit of large networks  as theoretically predicted (2). To this aim  we
repeat the experiment for n varying between 100 and 1500  and report the maximum and the mean
value of the squared error on the estimation of the I(X; T(cid:96)) over all epochs of 5 independent training
runs. We ﬁnd that even if errors tend to increase with the number of layers  they remain objectively
very small and decrease drastically as the size of the layers increases.
3.3 Learning experiments with deep non-linear networks— Finally  we apply the replica formula
to estimate mutual informations during the training of non-linear networks on correlated input data.
We consider a simple single layer generative model X = ˜WgenY +  with normally distributed code
Y ∼ N (0  InY ) of size nY = 100  data of size nX = 500 generated with matrix ˜Wgen i.i.d. normally
distributed as N (0  1/nY ) and noise  ∼ N (0  0.01InX ). We then train a recognition model to solve
the binary classiﬁcation problem of recovering the label y = sign(Y1)  the sign of the ﬁrst neuron in
Y   using plain SGD but this time to minimize the cross-entropy loss. Note that the rest of the initial
code (Y2  ..YnY ) acts as noise/nuisance with respect to the learning task. We compare two 5-layers
recognition models with 4 USV- layers plus one unconstrained  of sizes 500-1000-500-250-100-2 
and activations either linear-ReLU-linear-ReLU-softmax (top row of Figure 4) or linear-hardtanh-
linear-hardtanh-softmax (bottom row). Because USV-layers only feature O(n) parameters instead

7

10−1100101weightscalingσ−1.50.01.53.04.5H(T1)linear-linearnetworkKraskovetal.Kolchinskyetal.replica10−1100101weightscalingσ−2.50.02.55.0H(T2)Kolchinskyetal.parametricexact10−1100101weightscalingσ−1.50.01.53.0H(T1)hardtanh-hardtanhnetworklinearapprox.10−1100101weightscalingσ−4.0−2.00.02.04.0H(T2)10−1100101weightscalingσ−1.50.01.53.0H(T1)ReLU-ReLUnetwork1/2linearapprox.linearapprox.10−1100101weightscalingσ−2.50.02.55.0H(T2)Figure 3: Training of a 4-layer linear student of varying size on a regression task generated by a
linear teacher of output size nY = 4. Upper-left: MSE loss on the training and testing sets during
training by plain SGD for layers of size n = 1500. Best training loss is 0.004735  best testing loss is
0.004789. Lower-left: Corresponding mutual information evolution between hidden layers and input.
Center-left  center-right  right: maximum and squared error of the replica estimation of the mutual
information as a function of layers size n  over the course of 5 independent trainings for each value
of n for the ﬁrst  second and third hidden layer.

of O(n2) we observe that they require more iterations to train in general. In the case of the ReLU
network  adding interleaved linear layers was key to successful training with 2 non-linearities  which
explains the somewhat unusual architecture proposed. For the recognition model using hardtanh  this
was actually not an issue (see Supplementary Material [12] for an experiment using only hardtanh
activations)  however  we consider a similar architecture for fair comparison. We discuss further the
ability of learning of USV-layers in the Supplementary Material [12].
This experiment is reminiscent of the setting of [3]  yet now tractable for networks of larger sizes.
For both types of non-linearities we observe that the mutual information between the input and all
hidden layers decrease during the learning  except for the very beginning of training where we can
sometimes observe a short phase of increase (see zoom in insets). For the hardtanh layers this phase
is longer and the initial increase of noticeable amplitude.
In this particular experiment  the claim of [3] that compression can occur during training even with
non double-saturated activation seems corroborated (a phenomenon that was not observed by [5]).
Yet we do not observe that the compression is more pronounced in deeper layers and its link to
generalization remains elusive. For instance  we do not see a delay in the generalization w.r.t. training
accuracy/loss in the recognition model with hardtanh despite of an initial phase without compression
in two layers. Further learning experiments  including a second run of this last experiment  are
presented in the Supplementary Material [12].

4 Conclusion and perspectives

We have presented a class of deep learning models together with a tractable method to compute
entropy and mutual information between layers. This  we believe  offers a promising framework for
further investigations  and to this aim we provide Python packages that facilitate both the computation
of mutual informations and the training  for an arbitrary implementation of the model. In the future 
allowing for biases by extending the proposed formula would improve the ﬁtting power of the
considered neural network models.
We observe in our high-dimensional experiments that compression can happen during learning 
even when using ReLU activations. While we did not observe a clear link between generalization
and compression in our setting  there are many directions to be further explored within the models
presented in Section 2. Studying the entropic effect of regularizers is a natural step to formulate
an entropic interpretation to generalization. Furthermore  while our experiments focused on the
supervised learning  the replica formula derived for multi-layer models is general and can be applied
in unsupervised contexts  for instance in the theory of VAEs. On the rigorous side  the greater
perspective remains proving the replica formula in the general case of multi-layer models  and further

8

01503004500.00.5lossN=1500testlosstrainloss0150300450epochs4.04.44.85.2I(X;Ti)T1T2T350010001500layerssizeN02468(ˆIrep−Iexact)2×10−13layer1maxmean50010001500layerssizeN0.00.20.40.60.8(ˆIrep−Iexact)2×10−4layer2maxmean50010001500layerssizeN0.00.30.60.91.2(ˆIrep−Iexact)2×10−4layer3maxmeanFigure 4: Training of two recognition models on a binary classiﬁcation task with correlated input data
and either ReLU (top) or hardtanh (bottom) non-linearities. Left: training and generalization cross-
entropy loss (left axis) and accuracies (right axis) during learning. Best training-testing accuracies
are 0.995 - 0.991 for ReLU version (top row) and 0.998 - 0.996 for hardtanh version (bottom row).
Remaining colums: mutual information between the input and successive hidden layers. Insets zoom
on the ﬁrst epochs.

conﬁrm that the replica formula stays true after the learning of the USV-layers. Another question
worth of future investigation is whether the replica method can be used to describe not only entropies
and mutual informations for learned USV-layers  but also the optimal learning of the weights itself.

Acknowledgments

The authors would like to thank Léon Bottou  Antoine Maillard  Marc Mézard  Léo Miolane  and
Galen Reeves for insightful discussions. This work has been supported by the ERC under the
European Union’s FP7 Grant Agreement 307087-SPARCS and the European Union’s Horizon 2020
Research and Innovation Program 714608-SMiLe  as well as by the French Agence Nationale de
la Recherche under grant ANR-17-CE23-0023-01 PAIL. Additional funding is acknowledged by
MG from “Chaire de recherche sur les modèles et sciences des données”  Fondation CFM pour
la Recherche-ENS; by AM from Labex DigiCosme; and by CL from the Swiss National Science
Foundation under grant 200021E-175541. We gratefully acknowledge the support of NVIDIA
Corporation with the donation of the Titan Xp GPU used for this research.

References
[1] N. Tishby  F. C. Pereira  and W. Bialek. The Information Bottleneck Method. 37th Annual Allerton

Conference on Communication  Control  and Computing  1999.

[2] N. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In IEEE Information

Theory Workshop (ITW)  2015.

[3] R. Shwartz-Ziv and N. Tishby. Opening the Black Box of Deep Neural Networks via Information.

arXiv:1703.00810  2017.

[4] G. Chechik  A. Globerson  N. Tishby  and Y. Weiss. Information bottleneck for Gaussian variables. Journal

of Machine Learning Research  6(Jan):165–188  2005.

[5] A. M. Saxe  Y. Bansal  J. Dapello  M. Advani  A. Kolchinsky  B. D. Tracey  and D. D. Cox. On the
Information Bottleneck Theory of Deep Learning. In International Conference on Learning Representations
(ICLR)  2018.

[6] Y. Kabashima. Inference from correlated patterns: a uniﬁed theory for perceptron learning and linear

vector channels. Journal of Physics: Conference Series  95(1):012001  2008.

9

05001000epochs0.00.20.40.6losstestlosstrainloss0.70.80.91.0accuracytrainacctestacc05001000epochs192021I(X;T1)layer1-linear01021.2021.2505001000epochs8910I(X;T2)layer2-relu01010.8010.8505001000epochs6.07.08.0I(X;T3)layer3-linear0108.68.705001000epochs2.42.52.6I(X;T4)layer4-relu0102.6752.6802.685010002000epochs0.00.20.40.6losstestlosstrainloss0.60.70.80.91.0accuracytrainacctestacc010002000epochs20202021I(X;T1)layer1-linear01521.2721.28010002000epochs11121314I(X;T2)layer2-hardtanh010020013.613.8010002000epochs8.09.0I(X;T3)layer3-linear0109.5709.5759.580010002000epochs3.84.04.24.4I(X;T4)layer4-hardtanh01004.244.28[7] A. Manoel  F. Krzakala  M. Mézard  and L. Zdeborová. Multi-layer generalized linear estimation. In IEEE

International Symposium on Information Theory (ISIT)  2017.

[8] A. K. Fletcher and S. Rangan. Inference in Deep Networks in High Dimensions. arXiv:1706.06549  2017.

[9] G. Reeves. Additivity of Information in Multilayer Networks via Additive Gaussian Noise Transforms. In

55th Annual Allerton Conference on Communication  Control  and Computing  2017.

[10] M. Mézard  G. Parisi  and M. Virasoro. Spin Glass Theory and Beyond. World Scientiﬁc Publishing

Company  1987.

[11] M. Mézard and A. Montanari. Information  Physics  and Computation. Oxford University Press  2009.

[12] ArXiv version of this work. https://arxiv.org/abs/1805.09785.

[13] dnner: Deep Neural Networks Entropy with Replicas  Python library.

sphinxteam/dnner.

https://github.com/

[14] A. M. Tulino  G. Caire  S. Verdú  and S. Shamai (Shitz). Support Recovery With Sparsely Sampled Free

Random Matrices. IEEE Transactions on Information Theory  59(7):4243–4271  2013.

[15] D. Donoho and A. Montanari. High dimensional robust M-estimation: asymptotic variance via approximate

message passing. Probability Theory and Related Fields  166(3-4):935–969  2016.

[16] H. S. Seung  H. Sompolinsky  and N. Tishby. Statistical mechanics of learning from examples. Physical

Review A  45(8):6056  1992.

[17] A. Engel and C. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press  2001.

[18] M. Opper and D. Saad. Advanced mean ﬁeld methods: Theory and practice. MIT press  2001.

[19] J. Barbier  F. Krzakala  N. Macris  L. Miolane  and L. Zdeborová. Phase Transitions  Optimal Errors and

Optimality of Message-Passing in Generalized Linear Models. arXiv:1708.03395  2017.

[20] J. Barbier  N. Macris  A. Maillard  and F. Krzakala. The Mutual Information in Random Linear Estimation

Beyond i.i.d. Matrices. In IEEE International Symposium on Information Theory (ISIT)  2018.

[21] D. Donoho  A. Maleki  and A. Montanari. Message-passing algorithms for compressed sensing. Proceed-

ings of the National Academy of Sciences  106(45):18914–18919  2009.

[22] L. Zdeborová and F. Krzakala. Statistical physics of inference: thresholds and algorithms. Advances in

Physics  65(5):453–552  2016.

[23] S. Rangan. Generalized approximate message passing for estimation with random linear mixing. In IEEE

International Symposium on Information Theory (ISIT)  2011.

[24] S. Rangan  P. Schniter  and A. K. Fletcher. Vector approximate message passing. In IEEE International

Symposium on Information Theory (ISIT)  2017.

[25] J. Barbier and N. Macris. The adaptive interpolation method: a simple scheme to prove replica formulas in

Bayesian inference. arXiv:1705.02780 to appear in Probability Theory and Related Fields  2017.

[26] J. Barbier  N. Macris  and L. Miolane. The Layered Structure of Tensor Estimation and its Mutual

Information. In 55th Annual Allerton Conference on Communication  Control  and Computing  2017.

[27] M. Moczulski  M. Denil  J. Appleyard  and N. de Freitas. ACDC: A Structured Efﬁcient Linear Layer. In

International Conference on Learning Representations (ICLR)  2016.

[28] Z. Yang  M. Moczulski  M. Denil  N. de Freitas  A. Smola  L. Song  and Z. Wang. Deep fried convnets. In

IEEE International Conference on Computer Vision (ICCV)  2015.

[29] D. J. Amit  H. Gutfreund  and H. Sompolinsky. Storing inﬁnite numbers of patterns in a spin-glass model

of neural networks. Physical Review Letters  55(14):1530  1985.

[30] E. Gardner and B. Derrida. Three unﬁnished works on the optimal storage capacity of networks. Journal

of Physics A  22(12):1983  1989.

[31] M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method.

Journal of Physics A  22(12):2181  1989.

10

[32] C. Louart and R. Couillet. Harnessing neural networks: A random matrix approach. In IEEE International

Conference on Acoustics  Speech and Signal Processing (ICASSP)  2017.

[33] J. Pennington and P. Worah. Nonlinear random matrix theory for deep learning. In Advances in Neural

Information Processing Systems (NIPS)  2017.

[34] M. Raghu  B. Poole  J. Kleinberg  S. Ganguli  and J. Sohl-Dickstein. On the Expressive Power of Deep

Neural Networks. In International Conference on Machine Learning (ICML)  2017.

[35] A. Saxe  J. McClelland  and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep

linear neural networks. In International Conference on Learning Representations (ICLR)  2014.

[36] S.S. Schoenholz  J. Gilmer  S. Ganguli  and J. Sohl-Dickstein. Deep information propagation.

International Conference on Learning Representations (ICLR)  2017.

In

[37] M. Advani and A. Saxe. High-dimensional dynamics of generalization error in neural networks.

arXiv:1710.03667  2017.

[38] C. Baldassi  A. Braunstein  N. Brunel  and R. Zecchina. Efﬁcient supervised learning in networks with

binary synapses. Proceedings of the National Academy of Sciences  104:11079–11084  2007.

[39] Y. Dauphin  R. Pascanu  C. Gulcehre  K. Cho  S. Ganguli  and Y. Bengio. Identifying and attacking the
saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information
Processing Systems  2014.

[40] R. Giryes  G. Sapiro  and A. M. Bronstein. Deep neural networks with random Gaussian weights: a

universal classiﬁcation strategy? IEEE Transactions on Signal Processing  64(13):3444–3457  2016.

[41] M. Chalk  O. Marre  and G. Tkacik. Relevant sparse codes with variational information bottleneck. In

Advances in Neural Information Processing Systems  2016.

[42] A. Achille and S. Soatto.

Information Dropout: Learning Optimal Representations Through Noisy

Computation. IEEE Transactions on Pattern Analysis and Machine Inteligence  2018.

[43] A. Alemi  I. Fischer  J. Dillon  and K. Murphy. Deep variational information bottleneck. In International

Conference on Learning Representations (ICLR)  2017.

[44] A. Achille and S. Soatto. Emergence of Invariance and Disentangling in Deep Representations. In ICML

2017 Workshop on Principled Approaches to Deep Learning  2017.

[45] A. Kolchinsky  B. D. Tracey  and D. H. Wolpert. Nonlinear Information Bottleneck. arXiv:1705.02436 

2017.

[46] M.I. Belghazi  A. Baratin  S. Rajeswar  S. Ozair  Y. Bengio  A. Courville  and R.D. Hjelm. MINE: Mutual

Information Neural Estimation. In International Conference on Machine Learning (ICML)  2018.

[47] S. Zhao  J. Song  and S. Ermon.

arXiv:1706.02262  2017.

InfoVAE: Information Maximizing Variational Autoencoders.

[48] A. Kolchinsky and B. D. Tracey. Estimating mixture entropy with pairwise distances. Entropy  19(7):361 

2017.

[49] A. Kraskov  H. Stögbauer  and P. Grassberger. Estimating mutual information. Physical Review E 

69(6):066138  2004.

[50] lsd: Learning with Synthetic Data  Python library.

learning-synthetic-data.

https://github.com/marylou-gabrie/

11

,Raman Arora
Andy Cotter
Nati Srebro
Dipan Pal
Ashwin Kannan
Gautam Arakalgud
Marios Savvides
Marylou Gabrié
Andre Manoel
Clément Luneau
jean barbier
Nicolas Macris
Florent Krzakala
Lenka Zdeborová
Boyi Li
Felix Wu
Kilian Weinberger
Serge Belongie