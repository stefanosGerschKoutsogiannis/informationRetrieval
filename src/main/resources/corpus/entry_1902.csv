2017,Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models,Bayesian models are established as one of the main successful paradigms for complex problems in machine learning. To handle intractable inference  research in this area has developed new approximation methods that are fast and effective. However  theoretical analysis of the performance of such approximations is not well developed. The paper furthers such analysis by providing bounds on the excess risk of variational inference algorithms and related regularized loss minimization algorithms for a large class of latent variable models with Gaussian latent variables. We strengthen previous results for variational algorithms by showing they are competitive with any point-estimate predictor. Unlike previous work  we also provide bounds on the risk of the \emph{Bayesian} predictor and not just the risk of the Gibbs predictor for the same approximate posterior. The bounds are applied in complex models including sparse Gaussian processes and correlated topic models. Theoretical results are complemented by identifying novel approximations to the Bayesian objective that attempt to minimize the risk directly. An empirical evaluation compares the variational and new algorithms shedding further light on their performance.,Excess Risk Bounds for the Bayes Risk using

Variational Inference in Latent Gaussian Models

Rishit Sheth and Roni Khardon

Department of Computer Science  Tufts University

Medford  MA  02155  USA

rishit.sheth@tufts.edu | roni@cs.tufts.edu

Abstract

Bayesian models are established as one of the main successful paradigms for
complex problems in machine learning. To handle intractable inference  research
in this area has developed new approximation methods that are fast and effective.
However  theoretical analysis of the performance of such approximations is not
well developed. The paper furthers such analysis by providing bounds on the excess
risk of variational inference algorithms and related regularized loss minimization
algorithms for a large class of latent variable models with Gaussian latent variables.
We strengthen previous results for variational algorithms by showing that they
are competitive with any point-estimate predictor. Unlike previous work  we
provide bounds on the risk of the Bayesian predictor and not just the risk of the
Gibbs predictor for the same approximate posterior. The bounds are applied in
complex models including sparse Gaussian processes and correlated topic models.
Theoretical results are complemented by identifying novel approximations to
the Bayesian objective that attempt to minimize the risk directly. An empirical
evaluation compares the variational and new algorithms shedding further light on
their performance.

1

Introduction

Bayesian models are established as one of the main successful paradigms for complex problems
in machine learning. Since inference in complex models is intractable  research in this area is
devoted to developing new approximation methods that are fast and effective (Laplace/Taylor
approximation  variational approximation  expectation propagation  MCMC  etc.)  i.e.  these can
be seen as algorithmic contributions. Much less is known about theoretical guarantees on the
loss incurred by such approximations  either when the Bayesian model is correct or under model
misspeciﬁcation.
Several authors provide risk bounds for the Bayesian predictor (that aggregates predictions over its
posterior and then predicts)  e.g.  see [15  6  12]. However  the analysis is specialized to certain
classiﬁcation or regression settings  and the results have not been shown to be applicable to complex
Bayesian models and algorithms like the ones studied in this paper.
In recent work  [7] and [1] identiﬁed strong connections between variational inference [10] and
PAC-Bayes bounds [14] and have provided oracle inequalities for variational inference. As we show
in Section 3  similar results that are stronger in some aspects can be obtained by viewing variational
inference as performing regularized loss minimization. These results are an exciting ﬁrst step  but
they are limited in two aspects. First  they hold for the Gibbs predictor (that samples a hypothesis
and uses it to predict) and not the Bayesian predictor and  second  they are only meaningful against
“weak” competitors. For example  the bounds go to inﬁnity if the competitor is a point estimate
with zero variance. In addition  these results do not explicitly address hierarchical Bayesian models

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

where further development is needed to distinguish among different variational approximations in
the literature. Another important result by [11] provides relative loss bounds for generalized linear
models (GLM). These bounds can be translated to risk bounds and they hold against point estimates.
However  they are limited to the prediction of the true Bayesian posterior which is hard to compute.
In this paper we strengthen these theoretical results and  motivated by these  make additional
algorithmic and empirical contributions. In particular  we focus on latent Gaussian models (LGM)
whose latent variables are normally distributed. We extend the technique of [11] to derive agnostic
bounds for the excess risk of an approximate Bayesian predictor against any point estimate competitor.
We then apply these results to several models with two levels of latent variables  including generalized
linear models (GLM)  sparse Gaussian processes (sGP) [17  26] and correlated topic models (CTM)
[3] providing high probability bounds for risk. For CTM our results apply precisely to the variational
algorithm and for GLM and sGP they apply for a variant with a smoothed loss function.
Our results improve over [7  1] by strengthening the bounds  showing that they can be applied directly
to the variational algorithm  and showing that they apply to the Bayesian predictor. On the other hand
they improve over [11] in analyzing the approximate inference algorithms and in showing how to
apply the bounds to a larger class of models.
Finally  viewing approximate inference as regularized loss minimization  our exploration of the
hierarchical models shows that there is a mismatch between the objective being optimized by
algorithms such as variational inference and the loss that deﬁnes our performance criterion. We
identify three possible objectives corresponding respectively to a “simple variational approximation” 
the “collapsed variational approximation”  and to a new algorithm performing direct regularized loss
minimization instead of optimizing the variational objective. We explore these ideas empirically in
CTM. Experimental results conﬁrm that each variant is the “best" for optimizing its own implicit
objective  and therefore direct loss minimization  for which we do not yet have a theoretical analysis 
might be the algorithm of choice. However  they also show that the collapsed approximation comes
close to direct loss minimization. The concluding section of the paper further discusses the results.

2 Preliminaries

2.1 Learning Model  Hypotheses and Risk

We consider the standard PAC setting where n samples are drawn i.i.d. according to an unknown joint
distribution D over the sample space z. This captures the supervised case where z = (x  y) and the
goal is to predict y|x. In the unsupervised case  z = y and we are simply modeling the distribution.
To treat both cases together we always include x in the notation but ﬁx it to a dummy value in the
unsupervised case.
A learning algorithm outputs a hypothesis h which induces a distribution ph(y|x). One would
normally use this predictive distribution and an application-speciﬁc loss to pick the prediction.
Following previous work  we primarily focus on log loss  i.e.  the loss of h on example (x∗  y∗)
is (cid:96)(h  (x∗  y∗)) = − log ph(y∗|x∗).
In cases where this loss is not bounded  a smoothed and

bounded variant of the log loss can be deﬁned as ˜(cid:96)(h  (y∗  x∗)) = − log(cid:0)(1 − α)ph(y|x) + α(cid:1) 
p(y|w  x) = (cid:81)

where 0 < α < 1. We state our results w.r.t. log loss  and demonstrate  by example  how the
smoothed log loss can be used. Later  we brieﬂy discuss how our results hold more generally for
losses that are convex in p.
We start by considering one-level (1L) latent variable models given by p(w)p(y|w  x) where
i p(yi|w  xi). For example  in Bayesian logistic regression  w is the hidden
weight vector  the prior p(w) is given by a Normal distribution N (w|µ  Σ) and the likelihood
term is p(y|w  x) = σ(ywT x) where σ() is the sigmoid function. A hypothesis h represents a
distribution q(w) over w  where point estimates for w are modeled as delta functions. Regardless
of how h is computed  the Bayesian predictor calculates a predictive distribution ph(y|x) =
Eq(w)[p(y|w  x)] and accordingly its risk is deﬁned as rBay(q(w)) = E(x y)∼D[− log ph(y|x)] =
E(x y)∼D[− log Eq(w)[p(y|w  x)]].
Following previous work we also analyze the average risk of the Gibbs predictor which draws a
random w from q(w) and predicts using p(y|w  x). Although the Gibbs predictor is not an optimal
strategy  its analysis has been found useful in previous work and it serves as an intermediate step

2

in our results. Assuming the draw of w is done independently for each x we get: rGib(q(w)) =
E(x y)∼D[Eq(w)[− log p(y|w  x)]]. Previous work has deﬁned the Gibbs risk with expectations in
reversed order. That is  the algorithm draws a single w and uses it for prediction on all examples. We
ﬁnd the one given here more natural. Some of our results require the two deﬁnitions to be equivalent 
i.e.  the conditions for Fubini’s theorem must hold. We make this explicit in
Assumption 1. E(x y)∼D[Eq(w)[− log p(y|w  x)]] = Eq(w)[E(x y)∼D[− log p(y|w  x)]].
This is a relatively mild assumption. It clearly holds when y takes discrete values  where p(y|x  w) ≤ 1
implies that the log loss is positive and Fubini’s theorem applies. In the case of continuous y  upper
bounded likelihood functions imply that a translation of the loss function satisﬁes the condition of
Fubini’s theorem. For example  if p(y|x  w) = N (y|f (w  x)  σ2) where σ2 is a hyperparameter  then
√
log p(y|x  w) ≤ B = − log(
2π) − log(σ2). Therefore  − log p(y|x  w) + B ≥ 0 so that if we
redeﬁne1 the loss by adding the constant B  then the loss is positive and Fubini’s theorem applies.
More generally  we might need to enforce constraints on D  q(w)  and/or p(y|x  w).

2.2 Variational Learners for Latent Variable Models

Approximate inference generally limits q(w) to some ﬁxed family of distributions Q (e.g. the family
of normal distributions  or the family of products of independent components in the mean-ﬁeld
approximation). Given a dataset S = {(xi  yi)}n

i=1  we deﬁne the following general problem 

(cid:26) 1

η

q(cid:63) = arg min

q∈Q

KL(cid:0)q(w)(cid:107)p(w)(cid:1) + L(w  S)

(cid:27)

 

(1)

L(w  S) = −(cid:80)

where KL denotes Kullback-Leibler divergence. Standard variational inference uses η = 1 and
i Eq(w)[log p(yi|w  xi)]  and it is well known that (1) is the optimization of a lower
bound on p(y). If − log p(yi|w  xi) is replaced with a general loss function  then (1) may no longer
correspond to a lower bound on p(y). In any case  the output of (1)  denoted by q(cid:63)
Gib  is achieved via
regularized cumulative-loss minimization (RCLM) which optimizes a sum of training set error and a
regularization function. In particular  q(cid:63)
Gib uses a KL regularizer and optimizes the Gibbs risk rGib in
contrast to the Bayes risk rBay. This motivates some of the analysis in the paper.
Many interesting Bayesian models have two levels (2L) of

p(w)p(f|w  x)(cid:81)
in models where an additional factorization p(f|w  x) =(cid:81)

latent variables given by
i p(yi|fi) where both w and f are latent. Of course one can treat (w  f ) as one set
of parameters and apply the one-level model  but this does not capture the hierarchical structure of the
model. The standard approach in the literature infers a posterior on w via a variational distribution
q(w)q(f|w)  and assumes that q(w) is sufﬁcient for predicting p(y∗|x∗). We refer to this structural
assumption  i.e.  p(f∗  f|w  x  x∗) = p(f∗|w  x∗)p(f|w  x)  as Conditional Independence. It holds
i p(fi|w  xi) holds  e.g.  in GLM  CTM.
In the case of sparse Gaussian processes (sGP)  Conditional Independence does not hold  but it is
required in order to reduce the cubic complexity of the algorithm  and it has been used in all prior
work on sGP. Assuming Conditional Independence  the deﬁnition of risk extends naturally from the
one-level model by writing p(y|w  x) = Ep(f|w x)[p(y|f )] to get:

r2Bay(q(w)) = E

(x y)∼D

[− log E

[ E
p(f|w x)

q(w)

[p(y|f )]]] 

r2Gib(q(w)) = E

(x y)∼D

[ E
q(w)

[− log E

p(f|w x)

[p(y|f )]]].

(2)

(3)

−(cid:80)

Even though Conditional Independence is used in prediction  the learning algorithm must decide
how to treat q(f|w) during the optimization of q(w). The mean ﬁeld approximation uses q(w)q(f )
in the optimization. We analyze two alternatives that have been used in previous work. The
approximation q(f|w) = p(f|w)  used in sparse GP [26  8  23]  is described by (1) with L(w  S) =
2A and observe it is the RCLM solution
[− log p(y|f )]]].

i Eq(w)[Ep(fi|w xi)[log p(yi|fi)]]. We denote this by q(cid:63)

for the risk deﬁned as

r2A(q(w)) = E

(4)

(x y)∼D

[ E
q(w)

[ E
p(f|w x)

1For
− log(

the smoothed log loss 

maxw x y p(y|w x) p(y|w  x) + α).

1−α

the translation can be applied prior

to the re-scaling 

i.e. 

3

− Eq(w)[log Ep(f|w x)[(cid:81)
p(f|w) =(cid:81)

As shown by [25  9  22]  alternatively  for each w  we can pick the optimal q(f|w) = p(f|w  S).
Following [25] we call this a collapsed approximation. This leads to (1) with L(w  S) =
2Bj (joint expectation). For models where
i Eq(w)[log Ep(fi|w xi)[p(yi|fi)]]  and we
2Bi performs RCLM for the risk

i p(fi|w)  this simpliﬁes to L(w  S) = −(cid:80)

i p(yi|fi)]] and is denoted by q(cid:63)
2Bi (independent expectation). Note that q(cid:63)

denote the algorithm by q(cid:63)
given by r2Gib even if the factorization does not hold.
Finally  viewing approximate inference as performing RCLM  we observe a discrepancy between
our deﬁnition of risk in (2) and the loss function being optimized by existing algorithms  e.g. 
variational inference. This perspective suggests direct loss minimization described by the alternative
2D. In this case  q(cid:63)
2D

i log Eq(w)[Ep(fi|w xi)[p(yi|fi)]] in (1) and which we denote q(cid:63)

L(w  S) = −(cid:80)

is a “posterior” but one for which we do not have a Bayesian interpretation.
Given the discussion so far  we can hope to get some analysis for regularized loss minimization where
each of the algorithms implicitly optimizes a different deﬁnition of risk. Our goal is to identify good
algorithms for which we can bound the deﬁnition of risk we care about  r2Bay  as deﬁned in (2).

3 RCLM

Regularized loss minimization has been analyzed for general hypothesis spaces and losses. For
hypothesis space H and hypothesis h ∈ H we have loss function (cid:96)(h  (x  y))  and associated risk
r(h) = E(x y)∼D[(cid:96)(h  (x  y))]. Now  given a regularizer R : H → 0 ∪ R+  a non-negative scalar η 
and sample S  regularized cumulative loss minimization is deﬁned as

 1

η

(cid:88)

i

 .

RCLM(H  (cid:96)  R  η  S) = arg min

h∈H

R(h) +

(cid:96)(h  (xi  yi))

(5)

δ ( 1

ηn R(h) + 4ρ2η

ηn R(h) + 4ρ2η
σ .

Theorem 1 ([20]2 ). Assume that the regularizer R(h) is σ-strong-convex in h and the loss (cid:96)(h  (x  y))
is ρ-Lipschitz and convex in h  and let h(cid:63)(S) = RCLM(H  (cid:96)  R  η  S). Then  for all h ∈ H 
ES∼Dn[r(h(cid:63)(S))] ≤ r(h) + 1
The theorem bounds the expectation of the risk. Using Markov’s inequality we can get a high
probability bound: with probability ≥ 1 − δ  r(h(cid:63)(S)) ≤ r(h) + 1
σ ). Tighter
dependence on δ can be achieved for bounded losses using standard techniques. To simplify the
presentation we keep the expectation version throughout the paper.
For this paper we specialize RCLM for Bayesian algorithms  that is  H corresponds to the parameter
space for a parameterized family of (possibly degenerate) distributions  denoted Q  where q ∈ Q is a
distribution over a base parameter space w.
We have already noted above that q(cid:63)
2D(w) are RCLM algorithms. We can
therefore get immediate corollaries for the corresponding risks (see supplementary material). Such
results are already useful  but the convexity and ρ-Lipschitz conditions are not always easy to analyze
or guarantee. We next show how to use recent ideas from PAC-Bayes analysis to derive a similar
result for Gibbs risk with less strong requirements. We ﬁrst develop the result for the one-level model.
Toward this  deﬁne the loss and risk for individual base parameters as (cid:96)W (w  (x  y))  and rW (w) =
ED[(cid:96)W (w  (x  y))]  and the empirical estimate ˆrW (w  S) = 1
i (cid:96)W (w  (xi  yi)). Following [7]  let
n
Ψ(λ  n) = log ES∼Dn [Ep(w)[eλ(rW (w)−ˆrW (w S))]] where λ is an additional parameter. Combining
arguments from [20] with the use of the compression lemma [2] as in [7] we can derive the following
bound (proof in supplementary material):
Theorem 2. For all q ∈ Q  ES∼Dn [rGib(q(cid:63)
1
λ Ψ(λ  n).
The theorem applies to the two-level model by writing p(y|w) = Ep(f|w)[p(y|f )]. This yields
Corollary 3. For all q ∈ Q  ES∼Dn [r2Gib(q(cid:63)

λ maxq∈Q KL(cid:0)q(cid:107)p(cid:1)+
ηn KL(cid:0)q(cid:107)p(cid:1) +

(cid:80)
ηn KL(cid:0)q(cid:107)p(cid:1)+ 1

2Bi(w))] ≤ r2Gib(q) + 1

Gib(w))] ≤ rGib(q)+ 1

Gib(w)  q(cid:63)

2Bi(w) and q(cid:63)

λ maxq∈Q KL(cid:0)q(cid:107)p(cid:1) + 1

1

λ Ψ(λ  n).

2 [20] analyzed regularized average loss but the same proof steps with minor modiﬁcations yield the statement

for cumulative loss given here.

4

A similar result has already been derived by [1] without making the explicit connection to RCLM.
However  the implied algorithm uses a “regularization factor” λ which may not coincide with η = 1 
whereas standard variational inference can be analyzed with Theorem 2 (or Corollary 3).
The work of [4  7] showed how the Ψ term can be bounded. Brieﬂy  if (cid:96)W (w  (x  y)) is bounded
in [a  b]  then Ψ(λ  n) ≤ λ2(b−a)2
; if (cid:96)W (w  (x  y)) is not bounded  but the random variable
rW (w)− (cid:96)W (w  (x  y)) is sub-Gaussian or sub-gamma  then Ψ(λ  n) can be bounded with additional
assumptions on the underlying distribution D. More details are in the supplementary material.

2n

4 Concrete Bounds on Excess Risk in LGM

(cid:16)

The LGM family is a special case of the two-level model where the prior p(w) over the M-dimensional
parameter w is given by a Normal distribution. Following previous work we let Q to be a family
of Normal distributions. For the analysis we further restrict Q by placing bounds on the mean and
covariance as follows: Q = {N (w|m  V ) s.t. (cid:107)m(cid:107)2 ≤ Bm  λmin (V ) ≥   λmax (V ) ≤ BV } for
some  > 0. The KL divergence from q(w) = N (w|m  V ) to p(w) = N (w|µ  Σ) is given by

KL(cid:0)q(cid:107)p(cid:1) = 1
First  we note that KL(cid:0)q(cid:107)p(cid:1) is bounded under a lower bound on the minimum eigenvalue of V (proof

4.1 General Bounds on Excess Risk in LGM Against Point Estimates

tr(Σ−1V ) + (µ − m)T Σ−1(µ − m) + log

|Σ|
|V | − M

(cid:17)

2

.

in supplementary material follows from linear algebra identities):
Lemma 4. Let B(cid:48)

2+B2

M BV +(cid:107)µ(cid:107)2
λmin(Σ)

R = 1
2

m

KL(cid:0)q(cid:107)p(cid:1) ≤ BR =

M BV +(cid:107)µ(cid:107)2
λmin (Σ)

2 + B2

1
2

(cid:19)

+ M log(cid:0)λmax (Σ)(cid:1) − M
(cid:18) λmax (Σ)
(cid:19)

m

+ M log



. For q ∈ Q 
(cid:33)

− M

= B

R − 1
(cid:48)
2

(cid:18)
(cid:32)

M log .

(6)

The risk bounds of the previous section do not allow for point estimate competitors because the
KL portion is not bounded. We next generalize a technique from [11] showing that adding a little
variance to a point estimate does not hurt too much. This allows us to derive the promised bounds. In
the following   > 0 is a constant whose value is determined in the proof. For any ˆw  we consider the
-inﬂated distribution q (w) = N (w| ˆw  I) and calculate the distribution’s Gibbs risk w.r.t. a generic
(cid:96) : RM × (X × Y ) (cid:55)→ R.
Lemma 5.
λmax

loss. Speciﬁcally  we consider the (1L or 2L) Gibbs risk r(q) = E(x y)∼D[Eq(w)[(cid:96)(cid:0)w  (x  y)(cid:1)]] with
w(cid:96)(w  (x  y))(cid:1) ≤ BH  then for ˆw ∈ RM and q(w) = N (w| ˆw  I)
(cid:0)∇2
(cid:0)δ (w − ˆw)(cid:1) +

If (i) (cid:96)(w  (x  y)) is continuously differentiable in w up to order 2  and (ii)

(cid:0)q(w)(cid:1) = E

(7)

M BH .

rGib

(x y)∼D

1
2

Proof. By the multivariable Taylor’s theorem  for ˆw ∈ RM

[ E
q(w)

[(cid:96)(cid:0)w  (x  y)(cid:1)]] ≤ rGib
T
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w= ˆw

∇w(cid:96)(w  (x  y))

(cid:96)(w  (x  y)) = (cid:96)( ˆw  (x  y)) +

(w − ˆw)

∇2

(w − ˆw)T

+

1
2

w(cid:96)(w  (x  y))

 (w − ˆw)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w= ˜w
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w= ˜w

where ∇w(cid:96)(w  (x  y)) and ∇2
for some α ∈ [0  1] where α is a function of w. Taking the expectation results in

w(cid:96)(w  (x  y)) denote the gradient and Hessian  and ˜w = (1 − α) ˆw+αw

[(cid:96)(w  (x  y))] = (cid:96)( ˆw  (x  y)) +

E
q(w)

1
2

E
q(w)

[(w − ˆw)T ∇2

w(cid:96)(w  (x  y))

5

(w − ˆw)].

(8)

2 BH E[(w − ˆw)T (w − ˆw)] = 1

w(cid:96)(w  (x  y)) is bounded uniformly by some BH < ∞  then the
2 M BH. Taking expectation

If the maximum eigenvalue of ∇2
second term of (8) is bounded above by 1
w.r.t. D yields the statement of the lemma.
Since Q includes -inﬂated distributions centered on ˆw where(cid:107) ˆw(cid:107)2 ≤ Bm  we have the following.
Theorem 6 (Bound on Gibbs Risk Against Point Estimate Competitors).
(i)
− log Ep(f|w)[p(y|f ])
and (ii)
λmax

continuously differentiable

in w up to order 2 

(cid:18)

∇2

is

If

w

2Bi(w))] ≤ r2Gib

(cid:17)(cid:19)
(cid:16)− log Ep(f|w)[p(y|f ])
(cid:0)δ (w − ˆw)(cid:1) + ∆(BH ) +
(cid:19) 2
KL(cid:0)q(cid:107)p(cid:1) +

∆(BH ) (cid:44) 1
2

2Bi(w))] ≤ r2Gib(q) +

[r2Gib(q(cid:63)

≤ BH  then  for all ˆw with(cid:107) ˆw(cid:107)2 ≤ Bm 
(cid:32)

1
λ
B(cid:48)
R + 1 + log

(cid:18) 1

Ψ(λ  n) 

KL(cid:0)q(cid:107)p(cid:1) +

BH

1
λ

M

M

+

n

1
ηn

(cid:0)δ (w − ˆw)(cid:1) +

1
λ

max
q∈Q
M BH − 1
2

1
2

≤ r2Gib

Proof. Using the distribution q = N (w| ˆw  I) in the RHS of Corollary 3 yields

1
λ

Ψ(λ  n)

AM log  + AB

(cid:48)
R +

1
λ

Ψ(λ  n)

(10)

(cid:18) nλ

n + λ

(cid:19)(cid:33) .

(9)

E

S∼Dn

[r2Gib(q(cid:63)

E

S∼Dn

(cid:16) 1

ηn + 1

λ

(cid:17)

where A =
 = A
BH

and we have used Lemma 4 and Lemma 5. Eq (10) is optimized when

. Re-substituting the optimal  in (10) yields

[r2Gib(q(cid:63)

2Bi(w))] ≤ r2Gib

E

S∼Dn

(cid:18) 1

+

1
2

M

+

1
λ

ηn

R + 1 − log
B(cid:48)

(cid:0)δ (w − ˆw)(cid:1)
(cid:19) 2

M

(cid:32)

(cid:18) 1

ηn

1
BH

(cid:19)(cid:33) +

1
λ

+

1
λ

Ψ(λ  n).

(11)

Setting η = 1 yields the result.

The theorem calls for running the variational algorithm with constraints on eigenvalues of V . The
ﬁxed-point characterization [21] of the optimal solution in linear LGM implies that such constraints
hold for the optimal solution. Therefore  they need not be enforced explicitly in these models.
For any distribution q(w) and function f (w) we have minw [f (w)] ≤ Eq(w)[f (w)]. Therefore  the
minimizer of the Gibbs risk is a point estimate  which with Theorem 6 implies:
Corollary 7. Under the conditions of Theorem 6  for all q(w) = N (w|m  V ) with(cid:107)m(cid:107)2 ≤ Bm 
ES∼Dn[r2Gib(q(cid:63)

(cid:0)q(w)(cid:1) + ∆(BH ) + 1

2Bi(w))] ≤ r2Gib

λ Ψ(λ  n).

More importantly  as another immediate corollary  we have a bound for the Bayes risk:
Corollary 8 (Bound on Bayes Risk Against Point Estimate Competitors). Under the conditions
of Theorem 6  for all ˆw with(cid:107) ˆw(cid:107)2 ≤ Bm 
ES∼Dn[r2Bay(q(cid:63)
Proof. Follows from (a) ∀q  r2Bay(q) ≤ r2Gib(q) (Jensen’s inequality)  and (b) ∀ ˆw ∈
RM   r2Bay(δ(w − ˆw)) = r2Gib(δ(w − ˆw)).

(cid:0)δ (w − ˆw)(cid:1) + ∆(BH ) + 1

2Bi(w))] ≤ r2Bay

λ Ψ(λ  n).

The extension for Bayes risk in step b of the proof is only possible thanks to the extension to
point estimates. As stated in the previous section  for bounded losses  Ψ(λ  n) is bounded as
λ2(b−a)2
n or log n
respectively  where the latter has a ﬁxed non-decaying gap term (b − a)2/2. However  unlike [7] 
in our proof both cases are achievable with η = 1  i.e.  for the variational algorithm. For example 

n or λ = n to obtain decays rates log n√

. As in [7]  we can choose λ =

√

2n

n

6

using η = 1  λ =
∆(BH ) + 1

λ Ψ(λ  n) ≤ M√

n

(cid:16)

√

n  the prior with µ = 0 and Σ = 1

1 + log BH + log n + log(cid:0)BV + 1

M B2

m

(cid:1) + (b−a)2

2M

(cid:17)

.

M (M BV + B2

m)I  and bounded loss 

The results above are developed for the log loss but we can apply them more generally. Toward
this we note that Corollary 3 holds for an arbitrary loss  and Lemma 5  and Theorem 6 hold for
a sufﬁciently smooth loss with bounded 2nd derivative w.r.t. w. The conversion to Bayes risk in
Corollary 8 holds for any loss convex in p. Therefore  the result of Corollary 8 holds more generally
for any sufﬁciently smooth loss that has bounded 2nd derivative in w and that is convex in p. We
provide an application of this more general result in the next section.

4.2 Applications in Concrete Models

This section develops bounds on Ψ and BH for members of the 2L family.
CTM: For a document  the generative model for CTM ﬁrst draws w ∼ N (µ  Σ)  w ∈ RK−1
where {µ  Σ} are model parameters  and then maps this vector to the K-simplex with the logistic
transformation  θ = h(w). For each position i in the document  the latent topic variable  fi  is drawn
from Discrete(θ)  and the word yi is drawn from a Discrete(βfi ·) where β denotes the topics and is
treated as a parameter of the model. In this case p(f|w) can be integrated out analytically and the
loss is − log
Corollary 9. For CTM models where the parameters βk y are uniformly bounded away from 0  i.e. 
βk y ≥ γ > 0  for all ˆw with(cid:107) ˆw(cid:107)2 ≤ Bm 
ES∼Dn[r2Bay(q(cid:63)

. We have (proof in supplementary material):

(cid:16)(cid:80)K

2Bi(w))] ≤ r2Bay

k=1 βk yhk(w)

with BH=5.

differentiable in f up to order 2  and f (w  x) is continuously differentiable in w up to order

The following lemma is expressed in terms of log loss but also holds for smoothed log loss (proof in
supplementary material):

Lemma 10. When f is a deterministic function of w  if (i) − log p(cid:0)y|f (w  x)(cid:1) is continuously
2  (ii) ∂2(cid:104)− log p(y|f)
(cid:105)
wf (w  x)(cid:1) ≤ cf
(cid:0)∇2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ c1  (iv)(cid:13)(cid:13)∇wf (w  x)(cid:13)(cid:13)2

2 (σmax is the max singular value)  then BH = c2cf

2 ≤ cf
1 + c1cf
2 .

≤ c2  (iii)

1   and (v)

σmax

∂f 2

∂f

(cid:17)
(cid:0)δ (w − ˆw)(cid:1) + ∆(BH ) + λ(log γ)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∂

(cid:104)− log p(y|f)
(cid:105)

2n

GLM: The bound of [11] for GLM was developed for exact Bayesian inference. The following
corollary extends this to approximate inference through RCLM. In GLM  f = wT x  (cid:107)∇w(cid:107)2 = (cid:107)x(cid:107)2 
and ∇2
w = 0 and a bound on BH is immediate from Lemma 10. In addition the smoothed loss is
bounded 0 ≤ ˜(cid:96) ≤ − log α. This implies

Corollary 11. For GLM  if (i) ˜(cid:96)(w  (x  y)) = − log((1 − α)p(cid:0)y|f (w  x)(cid:1) + α) is continuously

differentiable in f up to order 2  and (ii) ∂2 ˜(cid:96)
ES∼Dn[˜r2Bay(˜q(cid:63)

(cid:0)δ (w − ˆw)(cid:1) + ∆(BH ) + λ(log α)2

2Bi(w))] ≤ ˜r2Bay

∂f 2 ≤ c 

then 

2n

for all ˆw with (cid:107) ˆw(cid:107)2 ≤ Bm 
with BH = c maxx∈X(cid:107)x(cid:107)2
2.

We develop the bound c for the logistic and Normal likelihoods (see supplementary material). Let
α(cid:48) = α
α(cid:48) . For the Gaussian
likelihood

1−α. For the logistic likelihood σ(yf )  we have c = 1

exp(− 1

(y−f )2

1√

3
18

√

16

)  we have c = 1
2πσ4

(α(cid:48))2 +
α(cid:48) .
1

2πσ3
Y

1

(α(cid:48))2 + 1√

2πσY

Y e

σ2
Y

2

1

1

The work of [7] has claimed3 a bound on the Gibbs risk for linear regression which should be
compared to our result for the Gaussian likelihood. Their result is developed under the assumption that
the Bayesian model speciﬁcation is correct and in addition that x is generated from x ∼ N (0  σ2
xI).
In contrast our result  using the smoothed loss  holds for arbitrary distributions D without the
(cid:17)
assumption of correct model speciﬁcation.

proof of Corollary 5 in [7] erroneously replaces Ep(w)[(cid:81)

3 Denoting ∆ri(w) = rW (w) − ˆrW (w  (xi  yi)) and fi(w  n  λ) = Ep(∆ri(w))[exp

]  the
i Ep(w)[fi(w  n  λ)]. We are not
aware of a correction of this proof which yields a correct bound for Ψ without using a smoothed loss. Any such
bound would  of course  be applicable with our Corollary 8.

i fi(w  n  λ)] with(cid:81)

n ∆ri(w)

(cid:16) λ

7

U xK−1

U U   b(x) = µx − K T

Sparse GP: In the sparse GP model  the conditional is p(cid:0)f|w  x(cid:1) = N (f|a(x)T w + b(x)  σ2(x))
and (U  U ) respectively. In the conjugate case  the likelihood is given by p(cid:0)y|f(cid:1) = N (y|f  σ2

where a(x)T = K T
U U KU x with µ
denoting the mean function and KU x  KU U denoting the kernel matrix evaluated at inputs (U  x)
Y ) and
integrating f out yields N (y|a(x)T w + b(x)  σ2(x) + σ2
Corollary 12. For conjugate sparse GP  for all ˆw with(cid:107) ˆw(cid:107)2 ≤ Bm 
ES∼Dn[˜r2Bay(˜q(cid:63)
where c = 1
2πσ4

(cid:0)δ (w − ˆw)(cid:1) + ∆(BH ) + λ(log α)2

U U µU and σ2(x) = Kxx − K T

Y ). Using the smoothed loss  we obtain:

2Bi(w))] ≤ ˜r2Bay
(α(cid:48))2 + 1√

(cid:13)(cid:13)a(x)(cid:13)(cid:13)2

with BH = c maxx∈X

U xK−1

U xK−1

α(cid:48) .

2n

1

1

2 

Y e

2πσ3
Y

1

wN where
˜(cid:96)(w  (x  y)) =
Y )  with f (w) = a(x)T w + b(x). The gradient ∇wN equals
˜(cid:96) =

(N +α(cid:48))2∇wN (∇wN )T − 1N +α(cid:48)∇2
(cid:16) ∂2N

(cid:17)
a(x)a(x)T . Therefore  ∇2
∂2(cid:104)− log((1−α)N +α)

(cid:19)
wN equals

∂(f (w))2

(cid:105)

w

w

∂(f (w))

(cid:17)

(cid:16) ∂N

Proof. The Hessian is given by ∇2
N denotes N (y|f (w)  σ2(x) + σ2

(cid:16) ∂N
(cid:18)
(cid:17)2 − 1N +α(cid:48)
(cid:105)
smoothed loss: ∂2(cid:104)− log((1−α)N +α)

a(x) and the Hessian ∇2
∂2N

(N +α(cid:48))2

≤

1√

2πσ3
Y

∂(f (w))2
α(cid:48) = c . Finally 

1

(cid:13)(cid:13)a(x)(cid:13)(cid:13)2

2.

1

a(x)a(x)T . The
result of Corollary 11 for Gaussian likelihood can be used to bound the 2nd derivative of the

a(x)a(x)T =

∂(f (w))2

∂(f (w))2

∂(f (w))

1

1

(α(cid:48))2 +
the eigenvalue of the rank-1 matrix ca(x)a(x)T is bounded by

(α(cid:48))2 +

2π(σ2(x)+σ2

2π(σ2(x)+σ2

Y )2e

2πσ4

Y e

Y )

3
2

1

√

1

1

α(cid:48) ≤ 1

2A and the collapsed bound uses q(cid:63)

c maxx∈X
Remark 1. We noted above that  for sGP  q(cid:63)
2Bi does not correspond to a variational algorithm. The
2Bj (but requires cubic time).
standard variational approach uses q(cid:63)
It can be shown that q(cid:63)
2Bi corresponds exactly to the fully independent training conditional (FITC)
approximation for sGP [24  16] in that their optimal solutions are identical. Our result can be seen to
justify the use of this algorithm which is known to perform well empirically.
Finally  we consider binary classiﬁcation in GLM with the convex loss function (cid:96)(cid:48)(w  (x  y)) =
8 (y − (2p(y|w  x) − 1))2. The proof of the following corollary is in the supplementary material:
Corollary 13. For GLM with p(y|w  x) = σ(ywT x) 
ES∼Dn[r(cid:48)

(cid:0)δ (w − ˆw)(cid:1) + ∆(BH ) + λ

for all ˆw with (cid:107) ˆw(cid:107)2 ≤ Bm 

16 maxx∈X(cid:107)x(cid:107)2
2.

2Bi(w))] ≤ r(cid:48)

8n with BH = 5

2Bay(q(cid:48)(cid:63)

2Bay

1

4.3 Direct Application of RCLM to Conjugate Linear LGM

In this section we derive a bound for an algorithm that optimizes a surrogate of the loss directly. In
particular  we consider the Bayes loss for linear LGM with conjugate likelihood p(y|f ) = N (y|f  σ2
Y )
where − log Eq(w)[Ep(f|w)[p(y|f )]] = − log N (y|aT m + b  σ2 + σ2
Y + aT V a) and where a  b  and
σ2 are functions of x. This includes  for example  linear regression and conjugate sGP.
2Ds performs RCLM with competitor set Θ = {(m  V ) :(cid:107)m(cid:107)2 ≤ Bm  V ∈
The proposed algorithm q(cid:63)
S++ (cid:107)V (cid:107)F ≤ BV }  regularizer R(m  V ) = 1
n and the surrogate loss
Y +aT V a .With these deﬁnitions we can
(cid:96)surr(m  V ) = 1
apply Theorem 1 to get (proof in supplementary material):
Theorem 14. With probability at
√
1
n

Y + aT V a(cid:1) + 1
(cid:17)
2Bay(q(w)) +
maxx∈X(cid:107)a(cid:107)2 maxx∈X y∈Y m |y − aT m− b| and
.

(cid:0)σ2 + σ2
V )(cid:1) where ρm = 1
(cid:16)

m + ρ2
maxx∈X y∈Y m(cid:107)a(cid:107)2

2Ds) ≤ minq∈Q rsurr

2(cid:107)V (cid:107)2
(y−aT m−b)2
σ2+σ2

least 1 − δ  r2Bay(q(cid:63)

1 + (y−aT m−b)2

2 log (2π) + 1

F   η = 1√

(cid:0)B2

2(cid:107)m(cid:107)2

V + 8(ρ2

m + B2

2 + 1

σ2
Y

2

2

δ
ρV = 1
2σ2
Y

2

σ2
Y

5 Direct Loss Minimization

The results in this paper expose the fact that different algorithms are apparently implicitly optimizing
criteria for different loss functions. In particular  q(cid:63)
2Bi optimizes for r2Gib

2A optimizes for r2A  q(cid:63)

8

(cid:80)

yi∈test (cid:96)2A(yi)

(cid:80)

yi∈test (cid:96)2Gib(yi)

(cid:80)

yi∈test (cid:96)2Bay(yi)

Figure 1: Artiﬁcial data. Cumulative test set losses of different variational algorithms. x-axis is
iteration. Mean ± 1σ of 30 trials are shown per objective. q(cid:63)

2Bi in green. q(cid:63)

2D in red.

2A in blue. q(cid:63)

2Bi algorithm  it is

2D optimizes for r2Bay. Even though we were able to bound r2Bay of the q(cid:63)

and q(cid:63)
interesting to check the performance of these algorithms in practice.
We present an experimental study comparing these algorithms on the correlated topic model (CTM)
that was described in the previous section. To explore the relation between the algorithms and their
performance we run the three algorithms and report their empirical risk on a test set  where the risk
is also measured in three different ways. Figure 1 shows the corresponding learning curves on an
artiﬁcial document generated from the model. Full experimental details and additional results on a
real dataset are given in the supplementary material.
We observe that at convergence each algorithm is best at optimizing its own implicit criterion.
However  considering r2Bay  the differences between the outputs of the variational algorithm q(cid:63)
2Bi and
2Bi takes longer
direct loss minimization q(cid:63)
2D are relatively small. We also see that at least in this case q(cid:63)
to reach the optimal point for r2Bay. Clearly  except for its own implicit criterion  q(cid:63)
2A should not be
2Bi [22]. The current experiment shows the
used. This agrees with prior empirical work on q(cid:63)
potential of direct loss optimization for improved performance but justiﬁes the use of q(cid:63)
2Bi both under
correct model speciﬁcation (artiﬁcial data) and when the model is incorrect (real data in supplement).
Preliminary experiments in sparse GP show similar trends. The comparison in that case is more
complex because q(cid:63)
2Bi is not the same as the collapsed variational approximation  which in turn
requires cubic time to compute  and we additionally have the surrogate optimizer q(cid:63)
2Ds. We defer a
full empirical exploration in sparse GP to future work.

2A and q(cid:63)

6 Discussion

The paper provides agnostic learning bounds for the risk of the Bayesian predictor  which uses the
posterior calculated by RCLM  against the best single predictor. The bounds apply for a wide class
of Bayesian models  including GLM  sGP and CTM. For CTM our bound applies precisely to the
variational algorithm with the collapsed variational bound. For sGP and GLM the bounds apply
to bounded variants of the log loss. The results add theoretical understanding of why approximate
inference algorithms are successful  even though they optimize the wrong objective  and therefore
justify the use of such algorithms. In addition  we expose a discrepancy between the loss used
in optimization and the loss typically used in evaluation and propose alternative algorithms using
regularized loss minimization. A preliminary empirical evaluation in CTM shows the potential of
direct loss minimization but that the collapsed variational approximation q(cid:63)
2Bi has the advantage of
strong theoretical guarantees and excellent empirical performance  both when the Bayesian model is
correct and under model misspeciﬁcation.
Our results can be seen as a ﬁrst step toward full analysis of approximate Bayesian inference methods.
One limitation is that the competitor class in our results is restricted to point estimates. While point
estimate predictors are optimal for the Gibbs risk  they are not optimal for Bayes predictors. In
addition  the bounds show that the Bayesian procedures will do almost as well as the best point
estimator. However  they do not show an advantage over such estimators  whereas one would expect
such an advantage. It would also be interesting to incorporate direct loss minimization within the
Bayesian framework. These issues remain an important challenge for future work.

9

0200400600800100012001400160018002000290029503000305031003150320032503300IterationCumulative loss value0200400600800100012001400160018002000290029503000305031003150320032503300IterationCumulative loss value0200400600800100012001400160018002000290029503000305031003150320032503300IterationCumulative loss valueAcknowledgments

This work was partly supported by NSF under grant IIS-1714440.

References
[1] Pierre Alquier  James Ridgway  and Nicolas Chopin. On the properties of variational

approximations of Gibbs posteriors. JMLR  17:1–41  2016.

[2] Arindam Banerjee. On Bayesian bounds. In ICML  pages 81–88  2006.

[3] David M. Blei and John D. Lafferty. Correlated topic models. In NIPS  pages 147–154. 2006.

[4] Stéphane Boucheron  Gábor Lugosi  and Pascal Massart. Concentration Inequalities: A

Nonasymptotic Theory of Independence. Oxford University Press  2013.

[5] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press 

March 2004.

[6] Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by exponential weighting  sharp

PAC-Bayesian bounds and sparsity. Machine Learning  72:39–61  2008.

[7] Pascal Germain  Francis Bach  Alexandre Lacoste  and Simon Lacoste-Julien. PAC-Bayesian

theory meets Bayesian inference. In NIPS  pages 1876–1884  2016.

[8] James Hensman  Alexander Matthews  and Zoubin Ghahramani. Scalable variational Gaussian

process classiﬁcation. In AISTATS  pages 351–360  2015.

[9] Matthew D. Hoffman and David M. Blei. Structured stochastic variational inference.

AISTATS  pages 361–369  2015.

In

[10] Michael I. Jordan  Zoubin Ghahramani  Tommi S. Jaakkola  and Lawrence K. Saul. An
introduction to variational methods for graphical models. Machine Learning  37:183–233  1999.

[11] Sham M. Kakade and Andrew Y. Ng. Online bounds for Bayesian algorithms. In NIPS  pages

641–648  2004.

[12] Alexandre Lacasse  François Laviolette  Mario Marchand  Pascal Germain  and Nicolas Usunier.
PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classiﬁer. In
NIPS  pages 769–776  2006.

[13] Moshe Lichman. UCI machine learning repository  2013. http://archive.ics.uci.edu/

ml.

[14] David A. McAllester. Some PAC-Bayesian theorems. In COLT  pages 230–234  1998.

[15] Ron Meir and Tong Zhang. Generalization error bounds for Bayesian mixture algorithms.

JMLR  4:839–860  2003.

[16] Joaquin Quiñonero-Candela  Carl E. Rasmussen  and Ralf Herbrich. A unifying view of sparse

approximate Gaussian process regression. JMLR  6:1939–1959  2005.

[17] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.

MIT Press  2006.

[18] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation

and approximate inference in deep generative models. In ICML  pages 1278–1286  2014.

[19] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and

Trends R(cid:13) in Machine Learning  4:107–194  2012.

[20] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge University Press  2014.

10

[21] Rishit Sheth and Roni Khardon. A ﬁxed-point operator for inference in variational Bayesian

latent Gaussian models. In AISTATS  pages 761–769  2016.

[22] Rishit Sheth and Roni Khardon. Monte Carlo structured SVI for non-conjugate models.

arXiv:1309.6835  2016.

[23] Rishit Sheth  Yuyang Wang  and Roni Khardon. Sparse variational inference for generalized

Gaussian process models. In ICML  pages 1302–1311  2015.

[24] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In

NIPS  pages 1257–1264  2006.

[25] Yee Whye Teh  David Newman  and Max Welling. A collapsed variational Bayesian inference

algorithm for latent Dirichlet allocation. In NIPS  pages 1353–1360  2006.

[26] Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In

AISTATS  pages 567–574  2009.

[27] Sheng-De Wang  Te-Son Kuo  and Chen-Fa Hsu. Trace bounds on the solution of the algebraic
matrix Riccati and Lyapunov equation. IEEE Transactions on Automatic Control  31:654–656 
1986.

11

,Rishit Sheth
Roni Khardon