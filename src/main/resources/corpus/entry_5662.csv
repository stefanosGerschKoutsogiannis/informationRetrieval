2013,Transfer Learning in a Transductive Setting,Category models for objects or activities typically rely on supervised learning requiring sufficiently large training sets. Transferring knowledge from known categories to novel classes with no or only a few labels however is far less researched even though it is a common scenario. In this work  we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances. Our proposed approach Propagated Semantic Transfer combines three main ingredients. First  we transfer information from known to novel categories by incorporating external knowledge  such as linguistic or expert-specified information  e.g.  by a mid-level layer of semantic attributes. Second  we exploit the manifold structure of novel classes. More specifically we adapt a graph-based learning algorithm - so far only used for semi-supervised learning - to zero-shot and few-shot learning. Third  we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation. We evaluate our approach on three challenging datasets in two different applications  namely on Animals with Attributes and ImageNet for image classification and on MPII Composites for activity recognition. Our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets.,Transfer Learning in a Transductive Setting

Marcus Rohrbach

Sandra Ebert

Bernt Schiele

Max Planck Institute for Informatics  Saarbr¨ucken  Germany
{rohrbach ebert schiele}@mpi-inf.mpg.de

Abstract

Category models for objects or activities typically rely on supervised learning
requiring sufﬁciently large training sets. Transferring knowledge from known cat-
egories to novel classes with no or only a few labels is far less researched even
though it is a common scenario. In this work  we extend transfer learning with
semi-supervised learning to exploit unlabeled instances of (novel) categories with
no or only a few labeled instances. Our proposed approach Propagated Semantic
Transfer combines three techniques. First  we transfer information from known to
novel categories by incorporating external knowledge  such as linguistic or expert-
speciﬁed information  e.g.  by a mid-level layer of semantic attributes. Second 
we exploit the manifold structure of novel classes. More speciﬁcally we adapt a
graph-based learning algorithm – so far only used for semi-supervised learning –
to zero-shot and few-shot learning. Third  we improve the local neighborhood
in such graph structures by replacing the raw feature-based representation with a
mid-level object- or attribute-based representation. We evaluate our approach on
three challenging datasets in two different applications  namely on Animals with
Attributes and ImageNet for image classiﬁcation and on MPII Composites for ac-
tivity recognition. Our approach consistently outperforms state-of-the-art transfer
and semi-supervised approaches on all datasets.

1

Introduction

While supervised training is an integral part of building visual  textual  or multi-modal category
models  more recently  knowledge transfer between categories has been recognized as an important
ingredient to scale to a large number of categories as well as to enable ﬁne-grained categorization.
This development reﬂects the psychological point of view that humans are able to generalize to
novel1 categories with only a few training samples [17  1]. This has recently gained increased
interest in the computer vision and machine learning literature  which look at zero-shot recognition
(with no training instances for a class) [11  19  9  22  16]  and one- or few-shot recognition [29  1 
21]. Knowledge transfer is particularly beneﬁcial when scaling to large numbers of classes [23  16] 
distinguishing ﬁne-grained categories [6]  or analyzing compositional activities in videos [9  22].
Recognizing categories with no or only few labeled training instances is challenging. To improve ex-
isting transfer learning approaches  we exploit several sources of information. Our approach allows
using (1) trained category models  (2) external knowledge  (3) instance similarity  and (4) labeled
instances of the novel classes if available. More speciﬁcally we learn category or attribute models
based on labeled training data for known categories y (see also Figure 1) using supervised training.
These trained models are then associated with the novel categories z using  e.g. expert or automati-
cally mined semantic relatedness (cyan lines in Figure 1). Similar to unsupervised learning [32  28]
our approach exploits similarities in the data space via a graph structure to discover dense regions
that are associated with coherent categories or concepts (orange graph structure in Figure 1). How-
ever  rather than using the raw input space  we map our data into a semantic output space with the

1We use “novel” throughout the paper to denote categories with no or few labeled training instances.

1

Figure 1: Conceptual visualisation of our approach Propagated Semantic Transfer. Known cate-
gories y  novel categories z  instances x (colors denote predicted category afﬁliation). Qualitative
results can be found in supplemental material and on our website.

models trained on the known classes (pink arrow) to beneﬁt from their discriminative knowledge.
Given the uncertain predictions and the graph structure we adapt semi-supervised label propaga-
tion [34  33] to generate more reliable predictions. If labeled instances are available they can be
seamlessly added. Note  attribute or category models do not have to be retrained if novel classes are
added which is an important aspect e.g. in a robotic scenario.
The main contribution of this work is threefold. First  we propose a novel approach that extends
semantic knowledge transfer to the transductive setting  exploiting similarities in the unlabeled data
distribution. The approach allows to do zero-shot recognition but also smoothly integrate labels for
novel classes (Section 3). Second  we improve the local neighborhood structure in the raw feature
space by mapping the data into a low dimensional semantic output space using the trained attribute
and category models. Third  we validate our approach on three challenging datasets for two differ-
ent applications  namely on Animals with Attributes and ImageNet for image classiﬁcation and on
MPII Composites for activity recognition (Section 4). We also provide a discussion of related work
(Section 2) and conclusions for future work (Section 5). The implementation for our Propagated
Semantic Transfer and code to easily reproduce the results in this paper is available on our website.

2 Related work

Knowledge transfer or transfer learning has the goal to transfer information of learned models to
changing or unknown data distributions while reducing the need and effort to collect new training
labels. It refers to a variety of tasks  including domain adaptation [25] or sharing of knowledge and
representations [30  3] (a recent categorization can be found in [20]).
In this work we focus on transferring knowledge from known categories with sufﬁcient training
instances to novel categories with limited training data. In computer vision or machine learning
literature this setting is normally referred to as zero-shot learning [11  19  24  9  16] if there are no
instances for the test classes available and one- or few-shot learning [16  9  8] if there are one or few
instances available for the novel classes.
To recognize novel categories zero-shot recognition uses additional information  typically in the
form of an intermediate attribute representation [11  9]  direct similarity [24] between categories  or
hierarchical structures of categories [35]. The information can either be manually speciﬁed [11  9]
or mined automatically from knowledge bases [24  22]. Our approach builds on these works by
using a semantic knowledge transfer approach as the ﬁrst step. If one or a few training examples are
available  these are typically used to select or adapt known models [1  9  26]. In contrast to related
work  our approach uses the above mentioned semantic knowledge transfer also when few training
examples are available to reduce the dependency on the quality of the samples. Also  we still use
the labeled examples to propagate information.
Additionally  we exploit the neighborhood structure of the unlabeled instances to improve recogni-
tion for zero- and few-shot recognition. This is in contrast to previous works with the exception of

2

z2 z3 z1 z3 z2 z1 x12 y1 y3 y4 y5 y2 known classes x11 z3 z2 z1 x1 x2 x5 x4 x3 x6 x8 x9 x7 x10 x12 x13 x14 x15 z3 z2 z1 x1 x2 x4 x3 x11 x6 x9 x12 x13 x14 x14 x15 x13 x10 x7 x6 x9 x8 x3 x2 x1 x5 x1 x2 x4 x3 x11 x6 x9 x12 x13 x14 x14 x15 x13 x10 x7 x6 x9 x8 x3 x2 x1 x5 x4 x11 x12 x11 x1 x2 x5 x4 x3 x6 x8 x9 x7 x10 x13 x14 x15 x4 Semantic knowledge transfer Few labeled instances Instance similarity object/attribute classifier scores to estimate instance similarity Improved prediction + = + external  knowledge the zero-shot approach of [9] that learns a discriminative  latent attribute representation and applies
self-training on the unseen categories. While conceptually similar  the approach is different to ours 
as we explicitly use the local neighborhood structure of the unlabeled instances. A popular choice to
integrate local neighborhood structure of the data are graph-based methods. These have been used to
discover a grouping by spectral clustering [18  14]  and to enable semi-supervised learning [34  33].
Our setting is similar to the semi-supervised setting. To transfer labels from labeled to unlabeled
data label propagation is widely used [34  33] and has shown to work successfully in several appli-
cations [13  7]. In this work  we extend transfer learning by considering the neighborhood structure
of the novel classes. For this we adapt the well-known label propagation approach of [33]. We build
a k-nearest neighbor graph to capture the underlying manifold structure as it has shown to provide
the most robust structure [15]. Nevertheless  the quality of the graph structure is key to success of
graph-based methods and strongly dependents on the feature representation [5]. We thus improve
the graph structure by replacing the noisy raw input space with the more compact semantic output
space which has shown to improve recognition [26  22].
To improve image classiﬁcation with reduced training data  [4  27] use attributes as an intermediate
layer and incorporate unlabeled data  however  both works are in a classical semi-supervised learn-
ing setting similar to [5]  while our setting is transfer learning. More speciﬁcally [27] propose to
bootstrap classiﬁers by adding unlabeled data. The bootstrapping is constrained by attributes shared
across classes. In contrast  we use attributes for transfer and exploit the similarity between instances
of the novel classes. [4] automatically discover a discriminative attribute representation  while in-
corporating unlabeled data. This notion of attributes is different to ours as we want to use semantic
attributes to enable transfer from other classes. Other directions to improve the quality of the inter-
mediate representation include integrating metric learning [31  16] or online methods [10] which we
defer to future work.

3 Propagated Semantic Transfer (PST)

Our main objective is to robustly recognize novel categories by transferring knowledge from known
classes and exploiting the similarity of the test instances. More speciﬁcally our novel approach called
Propagated Semantic Transfer consists of the following four components: we employ semantic
knowledge transfer from known classes to novel classes (Sec. 3.1); we combine the transferred
predictions with labels for the novel classes (Sec. 3.2); a similarity metric is deﬁned to achieve a
robust graph structure (Sec. 3.3); we propagate this information within the novel classes (Sec. 3.4).

3.1 Semantic knowledge transfer

We ﬁrst transfer knowledge using a semantic representation. This allows to include external know-
ledge sources. We model the relation between a set of K known classes y1  . . .   yK to the set of

N novel classes z1  . . .   zN . Both sets are disjoint  i.e. {y1  . . .   yK}(cid:84){z1  . . .   zN} = ∅. We use

two strategies to achieve this transfer: i) an attribute representation that employs an intermediate
representation of a1  . . .   aM attributes or ii) direct similarities calculated among the known object
classes. Both work without any training examples for zn  i.e. also for zero-shot recognition [11  24].
i) Attribute representation. We use the Direct-Attribute-Prediction (DAP) model [11]  using
our formulation [24]. An intermediate level of M attribute classiﬁers p(am|x) is trained on the
known classes yk to estimate the presence of attribute am in the instance x. The subsequent
knowledge transfer requires an external knowledge source that provides class-attribute associations
m ∈ {0  1} indicating if attribute am is associated with class zn. Options for such association
azn
information are discussed in Section 4.2. Given this information the probability of the novel classes
zn to be present in the instance x can then be estimated [24]:

(2p(am|x))azn
m .

(1)

ii) Direct similarity. As an alternative to attributes  we can use the U most similar training classes
y1  ...  yU as a predictor for novel class zn given an instance x [24]:

(2p(yu|x))yzn
u  

(2)

p(zn|x) ∝ M(cid:89)
p(zn|x) ∝ U(cid:89)

m=1

u=1

3

where yzn
u provides continuous normalized weights for the strength of the similarity between the
novel class zn and the known class yu [24]. To comply with [23  22] we slightly diverge from these
models for the ImageNet and MPII Composites dataset by using a sum formulation instead of the
  and for direct similarity

probabilistic expression  i.e. for attributes p(zn|x) ∝ (cid:80)M
(cid:80)M
p(zn|x) ∝ (cid:80)U

. Note that in this case we do not obtain probability estimates  however  for

u=1 p(yu|x)

m=1 azn

m p(am|x)
m=1 azn

m

label propagation the resulting scores are sufﬁcient.

U

3.2 Combining transferred and ground truth labels

In the following we treat the multi-class problem as N binary problems  where N is the number
of binary classes. For class zn the semantic knowledge transfer provides p(zn|x) ∈ [0  1] for all
instances x. We combine the best predictions per class  scaled to [−1  1]  with labels ˆl(zn|x) ∈
{−1  1} provided for some instances x in the following way:

l(zn|x) =

(1 − γ)(2p(zn|x) − 1)
0

if there is a label for x
if p(zn|x)is among top-δ fraction of predictions for zn
otherwise.

(3)

γ provides a weighting between the true labels and the predicted labels. In the zero-shot case we
only use predictions  i.e. γ = 0. The parameters δ  γ ∈ [0  1] are chosen  similar to the remaining
parameters  using cross-validation on the training set.

3.3 Similarity metric based on discriminative models for graph construction

We enhance transfer learning by exploiting also the neighborhood structure within novel classes 
i.e. we assume a transductive setting. Graph-based semi-supervised learning incorporates this infor-
mation by employing a graph structure over all instances. In this section we describe how to improve
the graph structure as it has a strong inﬂuence on the ﬁnal results [5]. The k-NN graph is usually
built on the raw feature descriptors of the data. Distances are computed for each pair (xi  xj) by

γˆl(zn|x)

d(xi  xj) =

|xi d − xj d| 

(4)

where D is the dimensionality of the raw feature space. We note that the visual representation used
for label propagation can be independent of the visual representation used for transfer. While the
visual representation for transfer is required to provide good generalization abilities in conjunc-
tion with the employed supervised learning strategy  the visual representation for label propagation
should induce a good neighborhood structure. Therefore we propose to use the more compact output
space trained on the known classes which we found to provide a much better structure  see Figure
5b. We thus compute the distances either on the M-dimensional vector of the attribute classiﬁers
p(am|x) with M (cid:28) D  i.e. 

d(xi  xj) =

|p(am|xi) − p(am|xj)| 

(5)

or on the K-dimensional vector of object-classiﬁers p(yk|x) with K (cid:28) D  i.e.

D(cid:88)

d=1

M(cid:88)

m=1

K(cid:88)

κ=1

d(xi  xj) =

|p(yκ|xi) − p(yκ|xj)|.

(6)

(cid:16)−d(xi xj )

(cid:17)

.

2σ2

These distances are transformed into similarities with a RBF kernel: s(xi  xj) = exp
Finally  we construct a k-NN graph that is known for its good performance [15  5]  i.e. 

(cid:26)s(xi  xj)

Wij =

0

if s(xi  xj) is among the k largest similarities of xi
otherwise.

(7)

4

Figure 2: AwA (left)  ImageNet (middle)  and MPII Composite Activities (right)

3.4 Label propagation with certain and uncertain labels

In this work  we build upon the label propagation by [33]. The k-NN graph with RBF kernel gives
the weighted graph W (see Section 3.3). Based on this graph we compute a normalized graph
Laplacian  i.e.  S = D−1/2W D−1/2 with the diagonal matrix D summing up the weights in each
row in W . Traditional semi-supervised label propagation uses sparse ground truth labels. In contrast
we have dense labels l(zn|x) which are a combination of uncertain predictions and certain labels (see
Eq. 3) for all instances {x1  . . .   xi} of the novel classes zn. Therefore  we modify the initialization
by setting

(8)
for the N novel classes. For each class  labels are propagated through this graph structure converging
to the following closed form solution

n = [l(zn|x1)  . . .   l(zn|xi)]
L(0)

n = (I − αS)−1L(0)
L∗

for 1 ≤ n ≤ N 

(9)
with the regularization parameter α ∈ (0  1]. The resulting framework makes use of the manifold
structure underlying the novel classes to regulate the predictions from transfer learning. In general 
the algorithm converges after few iterations.

n

4 Evaluation

4.1 Datasets

We shortly outline the most important properties of the examined datasets in the following para-
graphs and show example images/frames in Figure 2.
AwA The Animals with Attributes dataset (AwA) [11] is one of the ﬁrst and most widely used
datasets for semantic knowledge transfer and zero-shot recognition.
It consists of 50 mammal
classes  40 training (24 395 images) and 10 disjoint test classes (6 180 images). We use the pro-
vided pre-computed 6 image descriptors  which are concatenated.
ImageNet The ImageNet 2010 challenge [2] requires large scale and ﬁne-grained recognition. It
consists of 1000 image categories which are split into 800 training and 200 test categories according
to [23]. We use the LLC and Fisher-Vector encoded SIFT descriptors provided by [23].
MPII Composite Activities The MPII Composite Cooking Activities dataset [22] distinguishes 41
basic cooking activities  such as prepare scrambled egg or prepare carrots with video recordings
of varying length from 1 to 41 minutes. It consists of a total of 256 videos  44 are used for train-
ing the attribute representation  170 are used as test data. We use the provided dense-trajectory
representation and train/test split.

4.2 External knowledge sources and similarity measures

Our approach incorporates external knowledge to enable semantic knowledge transfer from known
classes y to unseen classes z. We use the class-attribute associations azn
m for attribute-based transfer
(Equation 1) or inter-class similarity yzn
u for direct-similarity-based transfer (Equation 2) provided
with the datasets. In the following we shortly outline the knowledge sources and measures.
Manual (AwA) AwA is accompanied with a set of 85 attributes and associations to all 40 training
and all 10 test classes. The associations are provided by human judgments [11].
Hierarchy (ImageNet) For ImageNet the manually constructed WordNet/ImagNet hierarchy is used
to ﬁnd the most similar of the 800 known classes (leaf nodes in the hierarchy). Furthermore  the 370
inner nodes can group several classes into attributes [23].

5

Approach
DAP [11]
IAP [11]
Zero-Shot Learning [9]
PST (ours)
on image descriptors
on attributes

Performance
AUC Acc.
41.4
81.4
80.0
42.2
41.3
n/a

81.2
83.7

40.5
42.7

(a) Zero-Shot. Predictions with attributes and
manual deﬁned associations  in %.

(b) Few-Shot

Figure 3: Results on AwA Dataset  see Sec. 4.3.1.

Linguistic knowledge bases (AwA  ImageNet) An alternative to manual association are automati-
cally mined associations. We use the provided similarity matrices which are extracted using different
linguistic similarity measures. They are either based on linguistic corpora  namely Wikipedia and
WordNet  or on hit-count statistics of web search. One can distinguish basic web search (Yahoo
Web)  web search reﬁned to part associations (Yahoo Holonyms)  image search (Yahoo Image and
Flickr Image)  or use the information of the summary snippets returned by web search (Yahoo Snip-
pets). As ImageNet does not provide attributes  we mined 811 part-attributes from the associated
WordNet hierarchy [23].
Script data (MPII Composites) To associate composite cooking activities such as preparing car-
rots with attributes of ﬁne-grained activities (e.g. wash  peel)  ingredients (e.g. carrots)  and tools
(e.g. knife  peeler)  textual description (Script data) of these activities were collected with AMT. The
provided associations are computed based on either the frequency statistics or  more discriminate 
by term frequency times inverse document frequency (tf*idf ). Words in the text can be matched to
labels either literally or by using WordNet expansion [22].

4.3 Results

To enable a direct comparison  we closely follow the experimental setups of the respective datasets
[11  23  22]. On all datasets we train attribute or object classiﬁers (for direct similarity) with one-vs-
all SVMs using Mean Stochastic Gradient Descent [23] and  for AwA and MPII Composites  with a
χ2 kernel approximation as in [22]. To get more distinctive representations for label propagation we
train sigmoid functions [12] to estimate probabilities (on the training set for AwA/MPII Composites
and on the validation set for ImageNet).
The hyper-parameters of our new Propagated Semantic Transfer algorithm are estimated using 5-
fold cross-validation on the respective training set  splitting them into 80% known and 20% novel
classes: We determine the parameters for our approach on the AwA training set and then set them
for all datasets to α = 0.8  γ = 0.98  the number of neighbors k = 50  the number of iterations for
propagation to 10  and use L1 distance. Due to the different recognition precision of the datasets
we determine δ = 0.15/0.04 separately for AwA/ImageNet. For MPII Composites we only do
zero-shot recognition and use all samples due to the limited number of samples of ≤ 7 per class.
For few-shot recognition we report the mean over 10 runs where we pick examples randomly. The
labeled examples are included in the evaluation to make it comparable to the zero-shot case.
We validate our claim that the classiﬁer output space induces a better neighborhood structure than
the raw features by examining the k-Nearest-Neighbour (kNN) quality for both. In Figure 5b we
compare the kNN quality on two datasets (see Sec. 4.1) for both feature representation. We observe
that the attribute (Eq. 5) and object (Eq. 6) classiﬁer-based representations (green and magenta
dashed line) achieve a signiﬁcantly higher accuracy than the respective raw feature-based represen-
tation (Eq. 4  Fig. 5b solid lines). We note that a good kNN-quality is required but not sufﬁcient for
good propagation  as it also depends on the distribution and quality of initial predictions. In the fol-
lowing  we compare the performance of the raw features with the attribute classiﬁer representation.

6

010203040503035404550# training samples per classmean Acc in %  PST (ours) − manual def. ass.LP + attr. classifiers − manual ass.PST (ours) − Yahoo Image attr.LP + attr. classifiers − Yahoo Img attr.LP [5](a) Zero-Shot.

(b) Few-Shot.

Figure 4: Results on ImageNet  see Sec. 4.3.2.

4.3.1 AwA - image classiﬁcation

We start by comparing the performance of related work to our approach on AwA (see Sec. 4.1) in
Figure 3. We start by examining the zero-shot results in Figure 3a  where no training examples
are available for the novel or in this case unseen classes. The best results to our knowledge for
on this dataset are reported by [11]. On this 10-class zero-shot task they achieve 81.4% area un-
der ROC-curve (AUC) and 41.4% multi-class accuracy (Acc) with DAP  averaged over the 10 test
classes. Additionally we report results from Zero-Shot Learning [9] which achieves 41.3% Acc. Our
Propagated Semantic Transfer  using the raw image descriptors to build a neighborhood structure 
achieves 81.2% AUC and 40.5% Acc. However  when propagating on the 85-dimensional attribute
space  we improve over [11] and [9] to 83.7% AUC and 42.7% Acc. To understand the difference
in performance between the attribute and the image descriptor space we examine the neighborhood
quality used for propagating labels shown in Figure 5b. The k-NN accuracy  measured on the ground
truth labels  is signiﬁcantly higher for the attribute space (green dashed curve) compared to the raw
features (solid green). The information is more likely propagated to neighbors of the correct class
for the attribute-space leading to a better ﬁnal prediction. Another advantage is the signiﬁcantly
reduced computation and storage costs for building the k-NN graph which scales linearly with the
dimensionality. We believe that such an intermediate space  in this case represented by attributes 
might provide a better neighborhood structure and could be used in other label-propagation tasks.
Next we compare our approach in the few-shot setting  i.e. we add labeled examples per class. In
Figure 3b we compare our approach (PST) to two label propagation (LP) baselines. We ﬁrst note
that PST (red curves) seamlessly moves from zero-shot to few-shot  while traditional LP (blue and
black curves) needs at least one training example. We ﬁrst examine the three solid lines. The black
curve is our best LP variant from [5] evaluated on the 10 test classes of AwA rather than all 50
as in [5]. We also compute LP in combination with the similarity metric based on the attribute
classiﬁer scores (blue curves). This transfer of knowledge residing in the classiﬁer trained on the
known classes already gives a signiﬁcant improvement in performance. Our approach (red curve)
additionally transfers labels from the known classes and improves further. Especially for few labels
our approach beneﬁts from the transfer  e.g. for 5 labeled samples per class PST achieves 43.9%
accuracy  compared to 38.1% for LP with attribute classiﬁers and 32.2% for [5]. For less samples
LP drops signiﬁcantly while our approach has nearly stable performance. For large amounts of
training data  PST approaches - as expected - LP (red vs. blue in Figure 3b).
The dashed lines in Figure 3b provide results for automatically mined associations azn
m between
attributes and classes. It is interesting to note that these automatically mined associations achieve
performance very close to the manual deﬁned associations (dashed vs. solid). In this plot we use
Yahoo Image as base for the semantic relatedness  but we also provide the improvements of PST for
the other linguistic language sources in supplemental material.

4.3.2 ImageNet - large scale image classiﬁcation

In this section we evaluate our Propagated Semantic Transfer approach on a large image classiﬁca-
tion task with 200 unseen image categories using the setup as proposed by [23]. We report the top-5
accuracy2 [2] which requires one of the best ﬁve predictions for an image to be correct.

2top-5 accuracy = 1 - top-5 error as deﬁned in [2]

7

0102030Hierachy − leaf nodesHierachy − inner nodesAttributes − WikipediaAttributes − Yahoo HolonymsAttributes − Yahoo ImageAttributes − Yahoo SnippetsDirect similarity − WikipediaDirect similarity − Yahoo WebDirect similarity − Yahoo ImageDirect similarity − Yahoo Snippetstop−5 accuracy (in %)  [23]PST (ours)0510152030354045505560# training samples per classtop−5 accuracy (in %)  PST (ours) − Hierachy (inner nodes)PST (ours) − Yahoo Img directLP + object classifiers(a) MPII Composite Activities  see Sec. 4.3.3.

(b) Accuracy of the majority vote from
kNN (kNN-Classiﬁer) on test sets’ ground truth.

Figure 5: Results

Results are reported in Figure 4. For zero-shot recognition our PST (red bars) improves performance
over [23] (black bars) as shown in Figure 4a. The largest improvement in top-5 accuracy is achieved
for Yahoo Image with Attributes which increases by 6.7% to 25.3%. The absolute performance of
34.0% top-5 accuracy is achieved by using the inner nodes of the WordNet hierarchy for transfer 
closely followed by Yahoo Web with direct similarity  achieving 33.1% top-5 accuracy. Similar to
the AwA dataset we improve PST over the LP-baseline for few-shot recognition (Figure 4b).

4.3.3 MPII composite - activity recognition

In the last two subsections  we showed the beneﬁt of Propagated Semantic Transfer on two image
classiﬁcation challenges. We now evaluate our approach on the video-activity recognition dataset
MPII Composite Cooking Activities [22]. We compute mean AP using the provided features and
follow the setup of [22].
In Figure 5a we compare our performance (red bars) to the results of
zero-shot recognition without propagation [22] (black bars) for four variants of Script data based
transfer. Our approach achieves signiﬁcant performance improvements in all four cases  increasing
mean AP by 11.1%  10.7%  12.0%  and 7.7% to 34.0%  32.8%  34.4%  and 29.2%  respectively.
This is especially impressive as it reaches the level of supervised training: for the same set of
attributes (and very few  ≤ 7 training categories per class) [22] achieve 32.2% for SVM  34.6%
for NN-classiﬁcation  and up to 36.2% for a combination of NN with script data.
We ﬁnd these results encouraging as it is much more difﬁcult to collect and label training exam-
ples for this domain than for image classiﬁcation and the complexity and compositional nature of
activities frequently requires recognizing unseen categories [9].

5 Conclusion

In this work we address a frequently occurring setting where there is large amount of training data
for some classes  but other  e.g. novel classes  have no or only few labeled training samples. We
propose a novel approach named Propagated Semantic Transfer  which integrates semantic knowl-
edge transfer with the visual similarities of unlabeled instances within the novel classes. We adapt a
semi-supervised label-propagation approach by building the neighborhood graph on expressive  low-
dimensional semantic output space and by initializing it with predictions from knowledge transfer.
We evaluated this approach on three diverse datasets for image and video-activity recognition 
consistently improving performance over the state-of-the-art for zero-shot and few-shot prediction.
Most notably we achieve 83.7% AUC / 42.7% multi-class accuracy on the Animals with Attributes
dataset for zero-shot recognition  scale to 200 unseen classes on ImageNet  and achieve up to 34.4%
(+12.0%) mean AP on MPII Composite Activities which is on the level of supervised training on this
dataset. We show that our approach consistently improves performance independent of factors such
as (1) the speciﬁc datasets and descriptors  (2) different transfer approaches: direct vs. attributes 
(3) types of transfer association: manually deﬁned  linguistic knowledge bases  or script data  (4)
domain: image and video activity recognition  or (5) model: probabilistic vs. sum formulation.
Acknowledgements. This work was partially funded by the DFG project SCHI989/2-2.

8

010203040Script data  freqs−literalScript data  freqs−WNScript data  tf*idf−literalScript data  tf*idf−WNmean AP (in %)  [22]PST (ours)0204060801000204060k nearest neighoursaccuracy in %  AwA − attribute classifiersAwA − raw featuresImageNet − object classifiersImageNet − raw featuresReferences
[1] E. Bart & S. Ullman. Single-example learning of novel classes using representation by similarity.

In

BMVC  2005.

[2] A. Berg  J. Deng  & L. Fei-Fei. ILSVRC 2010. www.image-net.org/challenges/LSVRC/2010/  2010.
[3] U. Blanke & B. Schiele. Remember and transfer what you have learned - recognizing composite activities

based on activity spotting. In ISWC  2010.

[4] J. Choi  M. Rastegari  A. Farhadi  & L. S. Davis. Adding Unlabeled Samples to Categories by Learned

[5] S. Ebert  D. Larlus  & B. Schiele. Extracting Structures in Image Collections for Object Recognition. In

Attributes. In CVPR  2013.

ECCV  2010.

[6] R. Farrell  O. Oza  V. Morariu  T. Darrell  & L. S. Davis. Birdlets: Subordinate categorization using

volumetric primitives and pose-normalized appearance. In ICCV  2011.

[7] R. Fergus  Y. Weiss  & A. Torralba. Semi-supervised learning in gigantic image collections. NIPS 2009.
[8] M. Fink. Object classiﬁcation from a single example utilizing class relevance pseudo-metrics. In NIPS 

[9] Y. Fu  T. M. Hospedales  T. Xiang  & S. Gong. Learning multi-modal latent attributes. TPAMI  PP(99) 

2004.

2013.

[10] P. Kankuekul  A. Kawewong  S. Tangruamsub  & O. Hasegawa. Online Incremental Attribute-based

[11] C. Lampert  H. Nickisch  & S. Harmeling. Attribute-based classiﬁcation for zero-shot learning of object

[12] H.-T. Lin  C.-J. Lin  & R. C. Weng. A note on platt’s probabilistic outputs for support vector machines.

Zero-shot Learning. In CVPR  2012.

categories. TPAMI  PP(99)  2013.

Machine Learning  2007.

[13] J. Liu  B. Kuipers  & S. Savarese. Recognizing human actions by attributes. In CVPR  2011.
[14] U. Luxburg. A tutorial on spectral clustering. Stat Comput  17(4):395–416  2007.
[15] M. Maier  U. V. Luxburg  & M. Hein. Inﬂuence of graph construction on graph-based clustering measures.

In NIPS  2008.

[16] T. Mensink  J. Verbeek  F. Perronnin  & G. Csurka. Metric Learning for Large Scale Image Classiﬁcation:

Generalizing to New Classes at Near-Zero Cost. In ECCV  2012.

[17] Y. Moses  S. Ullman  & S. Edelman. Generalization to novel images in upright and inverted faces.

Perception  25:443–461  1996.

[18] A. Y. Ng  M. I. Jordan  & Y. Weiss. On spectral clustering: Analysis and an algorithm. In NIPS  2002.
[19] M. Palatucci  D. Pomerleau  G. Hinton  & T. Mitchell. Zero-shot learning with semantic output codes. In

[20] S. J. Pan & Q. Yang. A survey on transfer learning. TKDE  22:1345–59  2010.
[21] R. Raina  A. Battle  H. Lee  B. Packer  & A. Ng. Self-taught learning: Transfer learning from unlabeled

NIPS  2009.

data. In ICML  2007.

[22] M. Rohrbach  M. Regneri  M. Andriluka  S. Amin  M. Pinkal  & B. Schiele. Script data for attribute-based

recognition of composite activities. In ECCV  2012.

[23] M. Rohrbach  M. Stark  & B. Schiele. Evaluating Knowledge Transfer and Zero-Shot Learning in a

Large-Scale Setting. In CVPR  2011.

[24] M. Rohrbach  M. Stark  G. Szarvas  I. Gurevych  & B. Schiele. What Helps Where – And Why? Semantic

Relatedness for Knowledge Transfer. In CVPR  2010.

[25] K. Saenko  B. Kulis  M. Fritz  & T. Darrell. Adapting visual category models to new domains. In ECCV 

2010.

[26] V. Sharmanska  N. Quadrianto  & C. H. Lampert. Augmented Attribute Representations. In ECCV  2012.
[27] A. Shrivastava  S. Singh  & A. Gupta. Constrained Semi-Supervised Learning Using Attributes and

[28] J. Sivic  B. C. Russell  A. A. Efros  A. Zisserman  & W. T. Freeman. Discovering Object Categories in

Comparative Attributes. In ECCV  2012.

Image Collections. In ICCV  2005.

[29] S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst. In NIPS  1996.
[30] A. Torralba  K. Murphy  & W. Freeman. Sharing visual features for multiclass and multiview object

detection. In CVPR  2004.

[31] D. Tran & A. Sorokin. Human activity recognition with metric learning. In ECCV  2008.
[32] M. Weber  M. Welling  & P. Perona. Towards automatic discovery of object categories. In CVPR  2000.
[33] D. Zhou  O. Bousquet  T. N. Lal  Jason Weston  & B. Sch¨olkopf. Learning with Local and Global

[34] X. Zhu  Z. Ghahramani  & J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

[35] A. Zweig & D. Weinshall. Exploiting object hierarchy: Combining models from different category levels.

Consistency. In NIPS  2004.

functions. In ICML  2003.

In ICCV  2007.

9

,Marcus Rohrbach
Sandra Ebert
Bernt Schiele
Takayuki Osogami
Makoto Otsuka