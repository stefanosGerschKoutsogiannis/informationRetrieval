2018,Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses,Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or $n$-gram loss. However  existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\"{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper  we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the $n$-gram loss  the edit-distance loss  and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.,Efﬁcient Gradient Computation for Structured

Output Learning with Rational and Tropical Losses

Corinna Cortes
Google Research

New York  NY 10011
corinna@google.com

Vitaly Kuznetsov
Google Research

New York  NY 10011
vitalyk@google.com

Dmitry Storcheus

Courant Institute and Google Research

New York  NY 10012

dstorcheus@google.com

Abstract

Mehryar Mohri

Courant Institute and Google Research

New York  NY 10012
mohri@cims.nyu.edu

Scott Yang∗

D. E. Shaw and Co.
New York  NY 10036
yangs@cims.nyu.edu

Many structured prediction problems admit a natural loss function for evaluation
such as the edit-distance or n-gram loss. However  existing learning algorithms
are typically designed to optimize alternative objectives such as the cross-entropy.
This is because a naïve implementation of the natural loss functions often results in
intractable gradient computations. In this paper  we design efﬁcient gradient com-
putation algorithms for two broad families of structured prediction loss functions:
rational and tropical losses. These families include as special cases the n-gram loss 
the edit-distance loss  and many other loss functions commonly used in natural
language processing and computational biology tasks that are based on sequence
similarity measures. Our algorithms make use of weighted automata and graph
operations over appropriate semirings to design efﬁcient solutions. They facilitate
efﬁcient gradient computation and hence enable one to train learning models such
as neural networks with complex structured losses.

1

Introduction

Many important machine learning tasks are instances of structured prediction problems. These are
learning problems where the output labels admit some structure that is important to take into account
both for statistical and computational reasons. Structured prediction problems include most natural
language processing tasks  such as pronunciation modeling  part-of-speech tagging  context-free
parsing  dependency parsing  machine translation  speech recognition  where the output labels are
sequences of phonemes  part-of-speech tags  words  parse trees  or acyclic graphs  as well as other
sequence modeling tasks in computational biology. They also include a variety of problems in
computer vision such as image segmentation  feature detection  object recognition  motion estimation 
computational photography and many others.
Several algorithms have been designed in the past for structured prediction tasks  including Con-
ditional Random Fields (CRFs) (Lafferty et al.  2001; Gimpel and Smith  2010)  StructSVMs
(Tsochantaridis et al.  2005)  Maximum-Margin Markov Networks (M3N) (Taskar et al.  2003) 
kernel-regression-based algorithms (Cortes et al.  2007)  and search-based methods (Daumé III et al. 
2009; Doppa et al.  2014; Lam et al.  2015; Chang et al.  2015; Ross et al.  2011). More recently  deep
learning techniques have been designed for many structured prediction tasks  including part-of-speech

∗Work done at the Courant Institute of Mathematical Sciences.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

tagging (Jurafsky and Martin  2009; Vinyals et al.  2015a)  named-entity recognition (Nadeau and
Sekine  2007)  machine translation (Zhang et al.  2008; Wu et al.  2016)  image segmentation (Lucchi
et al.  2013)  and image annotation (Vinyals et al.  2015b).
Many of these algorithms have been successfully used with speciﬁc loss functions such as the
Hamming loss. Their use has been also extended to multivariate performance measures such as
Precision/Recall or F1-score (Joachims  2005)  which depend on predictions on all training points.
However  the natural loss function relevant to a structured prediction task  which may be the n-gram
loss  the edit-distance loss  or some sequence similarity-based loss  is otherwise often ignored. Instead 
an alternative measure such as the cross-entropy is used. This is typically due to computational
efﬁciency reasons: a key subroutine within the main optimization such as one requiring to determine
the most violating constraint may be computationally intractable  the gradient may not admit a closed-
form or may seem difﬁcult to compute  as it may involve sums over a number of terms exponential in
the size of the input alphabet  with each term in itself being a large non-trivial computational task.
Several techniques have been suggested in the past to address this issue. They include Minimum
Risk Training (MRT) (Och  2003; Shen et al.  2016)  which seeks to optimize the natural objective
directly but relies on sampling or focusing on only the top-n structured outputs to make the problem
computationally tractable. REINFORCE-based methods (Ranzato et al.  2015; Wu et al.  2016)
also seek to optimize the natural loss function by deﬁning an unbiased stochastic estimate of the
objective  thereby making the problem computationally tractable. While these publications have
demonstrated that training directly with the natural loss function yields better results than using a
naïve loss function  their solutions naturally suffer from issues such as high variance in the gradient
estimate  in the case of sampling  or bias in the case of top-n. Moreover  REINFORCE methods often
have to feed the ground-truth at training time  which is inconsistent with the underlying theory.
Another technique has consisted of designing computationally more tractable surrogate loss functions
closer to the natural loss function (Ranjbar et al.  2013; Eban et al.  2017). These publications
also report improved performance using an objective closer to the natural loss  while admitting the
inherent issue of not optimizing the desired metric. McAllester et al. (2010) propose a perceptron-like
update in the special case of linear models in structured prediction problems  which avoids the use of
surrogate losses. However  while they show that direct loss minimization admits some asymptotic
statistical beneﬁts  each update in their work requires solving an argmax problem for which the
authors do not give an algorithm and that is known to be computationally hard in general  particularly
for non-additive losses.
This paper is strongly motivated by much of this previous work  which reports empirical beneﬁts for
using the natural loss associated to the task. We present efﬁcient gradient computation algorithms
for two broad families of structured prediction loss functions: rational and tropical losses. These
families include as special cases the n-gram loss  the edit-distance loss  and many other loss functions
commonly used in natural language processing and computational biology tasks that are based on
sequence similarity measures. Our algorithms make use of weighted automata and graph operations
over appropriate semirings to design efﬁcient solutions that circumvent the naïve computation of
exponentially sized sums in gradient formula.
Our algorithms enable one to train learning models such as neural networks with complex structured
losses. When combined with the recent developments in automatic differentiation  e.g. CNTK (Seide
and Agarwal  2016)  MXNet (Chen et al.  2015)  PyTorch (Paszke et al.  2017)  and TensorFlow
(Abadi et al.  2016)  they can be used to train structured prediction models such as neural networks
with the natural loss of the task. In particular  the use of our techniques for the top layer of neural
network models can further accelerate progress in end-to-end training (Amodei et al.  2016; Graves
and Jaitly  2014; Wu et al.  2016).
For problems with limited data  e.g. uncommon languages or some biological problems  our work
overcomes the computational bottleneck  uses the exact loss function  and renders the amount of data
available the next hurdle for improved performance. For extremely large-scale problems with more
data than can be processed  we further present an approximate truncated shortest-path algorithm that
can be used for fast approximate gradient computations of the edit-distance.
The rest of the paper is organized as follows. In Section 2  we brieﬂy describe structured prediction
problems and algorithms  discuss their learning objectives  and point out the challenge of gradient
computation. Section 3 deﬁnes several weighted automata and transducer operations that we use to

2

design efﬁcient algorithms for gradient-based learning. In Sections 4 and 5  we give general algorithms
for computing the gradient of rational and tropical loss functions  respectively. In Section 6  we report
the results of experiments verifying the improvement due to using our efﬁcient methods compared to
a naïve implementation. Further details regarding weighted automata and transducer operations and
training recurrent neural network training with the structured objective are presented in Appendix A
and Appendix B.

2 Gradient computation in structured prediction

In this section  we introduce the structured prediction learning problem. We start by deﬁning the
learning scenario  including the relevant loss functions and features. We then move on to discussing
the hypothesis sets and forms of the objection function that are used by many structured prediction
algorithms  which leads us to describe the problem of computing their gradients.

2.1 Structured prediction learning scenario

1
l

We consider the supervised learning setting  in which the learner receives a labeled sample S =
{(x1  y1)  . . .   (xm  ym)} drawn i.i.d. from some unknown distribution over X ×Y  where X denotes
the input space and Y the output space. In structured prediction  we assume that elements of the
output space Y can be decomposed into possibly overlapping substructures y = (y1  . . .   yl). We
further assume that the loss function L : Y × Y → R+ can similarly be decomposed along these
substructures. Some key examples of loss functions that are relevant to our work are the Hamming
loss  the n-gram loss and the edit-distance loss.
(cid:80)l
The Hamming loss is deﬁned for all y = (y1  . . .   yl) and y(cid:48) = (y(cid:48)1  . . .   y(cid:48)l) by L(y  y(cid:48)) =
k=1 1yk(cid:54)=y(cid:48)k  with yk  y(cid:48)k ∈ Yk. The edit-distance loss is commonly used in natural language
processing (NLP) applications where Y is a set of sequences deﬁned over a ﬁnite alphabet  and the
loss function between two sequences y and y(cid:48) is deﬁned as the minimum cost of a sequence of edit
operations  typically insertions  deletions  and substitutions  that transform y into y(cid:48). The n-gram
loss is deﬁned as the negative inner product (or its logarithm) of the vectors of n-gram counts of
two sequences. This can serve as an approximation to the BLEU score  which is commonly used in
machine translation.
We assume that the learner has access to a feature mapping Ψ : X × Y → RN . This mapping can
be either a vector of manually designed features  as in the application of the CRF algorithm  or the
differentiable output of the penultimate layer of an artiﬁcial neural network. In practice  feature
mappings that correspond to the inherent structure of the input space X combined with the structure
of Y can be exploited to derive effective and efﬁcient algorithms. As mentioned previously  a common
case in structured prediction is when Y is a set of sequences of length l over a ﬁnite alphabet ∆.
This is the setting that we will consider  as other structured prediction problems can often be treated
similarly.
We further assume that Ψ admits a Markovian property of order q  that is  for any (x  y) ∈ X × Y 
s=1 ψ(x  ys−q+1:s  s)  for some position-dependent
feature vector function ψ deﬁned over X × ∆q × [l]  where the shorthand ys:s(cid:48)
) stands
for the substring of y starting at index s and ending at s(cid:48). For convenience  for s ≤ 0  we deﬁne ys
to be the empty string ε. This Markovian assumption is commonly adopted in structured prediction
problems such as NLP (Manning and Schütze  1999). In particular  it holds for feature mappings that
are frequently used in conjunction with the CRF  as well as outputs of a recurrent neural network 
reset at the begining of each new input (see Appendix B).

Ψ(x  y) can be decomposed as Ψ(x  y) =(cid:80)l

= (ys  . . .   ys(cid:48)

feature mapping Ψ. The empirical loss (cid:98)RS(h) = 1

2.2 Objective function and gradient computation
The hypothesis set we consider is that of linear functions h : (x  y) (cid:55)→ w · Ψ(x  y) based on the
i=1 L(h(xi)  yi) associated to a hypothesis
h is often not differentiable in structured prediction since the loss function admits discrete values.
Taking the expectation over the distribution induced by the log-linear model  as in (Gimpel and Smith 
2010)[Equation 5]  does not help resolve this issue  since the method does not result in an upper
bound on the empirical loss and does not admit favorable generalization guarantees. Instead  as in the

(cid:80)m

m

3

familiar binary classiﬁcation scenario  one can resort to upper-bounding the loss with a differentiable

(convex) surrogate. For instance  by (Cortes et al.  2016)[Lemma 4]  (cid:98)RS(h) can be upper-bounded

by the following objective function:

(cid:35)

m(cid:88)

i=1

(cid:34)(cid:88)

y∈Y

F (w) =

1
m

log

eL(y yi)−w·(Ψ(xi yi)−Ψ(xi y))

 

(1)

which  modulo a regularization term  coincides with the objective function of CRF. Note that this
expression has also been presented as the softmax margin (Gimpel and Smith  2010) and the reward-
augmented maximum likelihood (Norouzi et al.  2016). Both of these references demonstrate strong
empirical evidence for this choice of objective function (in addition to the theoretical results presented
in (Cortes et al.  2016)).
Our focus in this work is on an efﬁcient computation of the gradient of this objective function. Since
the computation of the subgradient of the regularization term often does not pose any issues  we will
only consider the unregularized part of the objective. For any w and i ∈ [m]  let Fi(w) denote the
contribution of the i-th training point to the objective function F . A standard gradient descent-based
method would sum up all or a subset (mini-batch) of the gradients ∇Fi(w). As illustrated in (Cortes
et al.  2016)[Lemma 15]  the gradient ∇Fi(w) can be expressed as follows at any w:

l(cid:88)

(cid:88)

s=1

z∈∆q

∇Fi(w) =

1
m

Qw(z  s)ψ(xi  z  s) − Ψ(xi  yi)

m

 

where  for all z ∈ ∆q and s ∈ [l]  Qw(z  s) is deﬁned by

Qw(z  s) =

y : ys−q+1:s=z

Zw

eL(y yi)+w·Ψ(xi y)

and Zw =

(cid:88)

(cid:88)

y∈Y

eL(y yi)+w·Ψ(xi y).

The bottleneck in the gradient computation is the evaluation of Qw(z  s)  for all z ∈ ∆q and s ∈ [l].
There are l|∆|q such terms and each term Qw(z  s) is deﬁned by a sum over the |∆|l−q sequences y of
length l with a ﬁxed substring z of length q. A straightforward computation of these terms following
their deﬁnition would therefore be computationally expensive. To avoid that computational cost 
many existing learning algorithms for structured prediction  including most of those mentioned in the
introduction  resort to further approximations and omit the loss L from the deﬁnition of Qw(z  s).
Combining that with the Markovian structure of Ψ can then lead to efﬁcient gradient computations.
Of course  the caveat of this approach is that it ignores the key component of the learning problem 
namely the loss function.
In what follows  we will present efﬁcient algorithms for the exact computation of the terms Qw(z  s) 
with their full deﬁnition  including the loss function. This leads to an efﬁcient computation of the
gradients ∇Fi  which can be used as input to back-propagation algorithms that would enable us to
train neural network models with structured prediction losses.
The gradient computation methods we present apply to the Hamming loss  n-gram loss  and edit-
distance loss  and more generally to two broad families of losses that can be represented by weighted
ﬁnite-state transducers (WFSTs). This covers many losses based on sequence similarity measures
that are used in NLP and computational biology applications (Cortes et al.  2004; Schölkopf et al. 
2004).
We brieﬂy describe the WFST operations relevant to our solutions in the following section and
provide an example of how the edit-distance loss can be represented with a WFST in Section 5.

3 Weighted automata and transducers

Weighted ﬁnite automata (WFA) and weighted ﬁnite-state transducers (WFST) are fundamental
concepts and representations widely used in computer science (Mohri  2009). We will use WFAs and
WFSTs to devise algorithms that efﬁciently compute gradients of structured prediction objectives.
This section introduces some standard concepts and notation for WFAs and WFSTs. We provide
additional details in Appendix A. For a more comprehensive treatment of these topics  we refer the
reader to (Mohri  2009).

4

Figure 1: Bigram transducer Tbigram over the semiring (R+ ∪ {+∞}  + ×  0  1) for the alphabet ∆ = {a  b}.
The weight of each transition (or that of a ﬁnal state) is indicated after the slash separator. For example  for any
string y and bigram u  Tbigram(y  u) is equal to the number of occurrences of u in y (Cortes et al.  2015).
Deﬁnition. A weighted ﬁnite-state transducer T over a semiring (S ⊕ ⊗  0  1) is an 8-tuple
(Σ  ∆  Q  I  F  E  λ  ρ) where Σ is a ﬁnite input alphabet  ∆ is a ﬁnite output alphabet  Q is a
ﬁnite set of states  I ⊆ Q is the set of initial states  F ⊆ Q is the set of ﬁnal states  E is a ﬁnite
multiset of transitions  which are elements of Q × (Σ ∪ {}) × (∆ ∪ {}) × S × Q  λ : I → S is an
initial weight function  and ρ : F → S is a ﬁnal weight function. A weighted ﬁnite automaton is a
weighted ﬁnite-state transducer where the input and output labels are the same. See Figures 1 and 3
for some examples.
For many operations to be well deﬁned  the weights of a WFST must belong to a semiring
(S ⊕ ⊗  0  1). We provide a formal deﬁnition of a semiring in Appendix A. In this work  we
consider two semirings: the probability semiring (R+ ∪ {+∞}  + ×  0  1) and the tropical semiring
(R ∪ {−∞  +∞}  min  +  +∞  0). The ⊗-operation is used to compute the weight of a path by
⊗-multiplying the weights of the transitions along that path. The ⊕-operation is used to compute the
weight of a pair of input and output strings (x  y) by ⊕-summing the weights of the paths labeled
with (x  y). We denote this weight by T(x  y).
As shown in Sections 4 and 5  in many useful cases  we can reduce the computation of the loss
function L(y  y(cid:48)) between two strings y and y(cid:48)  along with the gradient of the corresponding objective
described in (1)  to that of the ⊕-sum of the weights of all paths labeled by y:y(cid:48) in a suitably deﬁned
transducer over either the probability or tropical semiring. We will use the following standard WFST
operations to construct these transducers: inverse (T−1)  projection (Π(T))  composition (T1 ◦ T2) 
and determinization (Det(A)). The deﬁnitions of these operations are given in Appendix A.

4 An efﬁcient algorithm for the gradient computation of rational losses

As discussed in Section 2  computing Qw(z  s) is the main bottleneck in the gradient computation.
In this section  we give an efﬁcient algorithm for computing Qw(z  s) that works for an arbitrary
rational loss  which includes as a special case the n-gram loss and other sequence similarity-based
losses. We ﬁrst present the deﬁnition of a rational loss and show how the n-gram loss can be encoded
as a speciﬁc rational loss. Then  we present our gradient computation algorithm.
Let (R+ ∪ {+∞}  + ×  0  1) be the probability semiring and let U be a WFST over the probability
semiring admitting ∆ as both the input and output alphabet. Then  following (Cortes et al.  2015) 
the rational loss associated to U is the function LU : ∆∗ × ∆∗ → R ∪ {−∞  +∞} deﬁned for all
negative logarithm of the inner product of the vectors of n-gram counts of y and y(cid:48). The WFST
Un-gram of an n-gram loss is obtained by composing a weighted transducer Tn-gram giving the n-gram
counts with its inverse T−1
n-gram  that is the transducer derived from Tn-gram by swapping input and
output labels for each transition. As an example  Figure 1 shows the WFST Tbigram for bigrams.
To compute Qw(z  s) for a rational loss  recall that

y  y(cid:48) ∈ ∆∗ by LU(y  y(cid:48)) = − log(cid:0)U(y  y(cid:48))(cid:1). As an example  the n-gram loss of y and y(cid:48) is the

Qw(z  s) ∝ (cid:88)

y : ys−q+1:s=z

eLU(y yi)+w·Ψ(xi y).

Thus  we will design two WFAs  A and B  such that A(y) = ew·Ψ(xi y)  B(y) = eLU(y yi)  and their
composition C(y) = (A ◦ B)(y) = eLU(y yi)+w·Ψ(xi y). To compute Qw from C  we will need to
sum up the weights of all paths labeled with some substring z  which we will achieve by treating this
as a ﬂow computation problem.
The pseudocode of our algorithm for computing the key terms Qw(z  s) for a rational loss is given in
Figure 2(a).

5

0 a:ε/1b:ε/1 1a:a/1b:b/12/1a:a/1b:b/1 a:ε/1b:ε/1 GRAD-RATIONAL(xi  yi  w)
1 Y ← WFA accepting any y ∈ ∆l.
2 Yi ← WFA accepting yi.
3 M ← Π1(Y ◦ U ◦ Yi)
4 M ← Det(M)
5 B ← INVERSEWEIGHTS(M)
6 C ← A ◦ B
7 α ← DISTFROMINITIAL(C  (+ ×))
8 β ← DISTTOFINAL(C  (+ ×))
9 Zw ← β(IC) (cid:46) IC initial state of C
10
α(e) × ω(e) × β(e)

11 Qw(z  s) ←(cid:88)

for (z  s) ∈ ∆q × [l] do

12 Qw(z  s) ← Qw(z  s)/Zw

e∈Ez s

(a)

(b)

(c)

(d)

l(cid:89)

t=1

A(y) = ew·Ψ(xi y) =

ew·ψ(xi yt−q+1:t t).

(cid:110)

Figure 2: (a) Efﬁcient computation of the key terms of the structured gradient for the rational loss. For each
transition e ∈ Ez s  we denote its origin by e  destination by e and weight by ω(e). (b) Illustration of the WFA
Y for ∆ = {a  b} and l = 3. (c) Illustration of the WFA Yi representing string dac. (d) Illustration of WFA A
for q = 2  alphabet ∆ = (a  b) and string length l = 2. For example  the transition from state (a  1) to state
(b  2) has the label b and weight ω(ab  2) = ew·ψ(xi ab 2).
Design of A. We want to design a determnistic WFA A such that

(cid:111)

To accomplish this task  let A be a WFA with the following set of states QA =

(yt−q+1:t  t) : y ∈
  with IA = (ε  0) its single initial state  FA = {(yl−q+1:l  l) : y ∈ ∆l} its set of
∆l  t = 0  . . .   l
ﬁnal states  and with a transition from state (yt−q+1:t−1  t − 1) to state (yt−q+2:t−1 b  t) with label b
and weight ω(yt−q+1:t−1 b  t) = ew·ψ(xi yt−q+1:t−1b t)  that is  the following set of transitions:
: y ∈ ∆l  b ∈ ∆  t ∈ [l]

(yt−q+1:t−1  t − 1)  b  ω(yt−q+1:t−1 b  t)  (yt−q+2:t−1 b  t)

(cid:110)(cid:16)

EA =

(cid:17)

(cid:111)

.

Figure 2(d) illustrates this construction in the case q = 2. Note that the WFA A is deterministic by
construction. Since the weight of a path in A is obtained by multiplying the transition weights along
the path  A(y) computes the desired quantity.
Design of B. We now design a deterministic WFA B which associates to each sequence y ∈ ∆l the
exponential of the loss eLU(y yi) = 1/U(y  yi). Let Y denote a WFA over the probability semiring
accepting the set of all strings of length l with weight one and let Yi denote the WFA accepting only
yi with weight one. Figures 2(b) and 2(c) illustrate the constructions of Y and Yi in some simple
cases.2 We ﬁrst use the composition operation for weighted automata and transducers. Then  we
use the projection operation on the input  which we denote by Π1  to compute the following WFA:
M = Π1(Y ◦ U ◦ Yi). Recalling that Y(y) = Yi(yi) = 1 by construction and applying the deﬁnition
of WFST composition  we observe that for any y ∈ ∆l
M(y) = (Y◦ U◦ Yi)(y  yi) =

Y(z)U(z  z(cid:48))Yi(z(cid:48)) = Y(y)U(y  yi)Yi(yi) = U(y  yi). (2)

(cid:88)

z=y z(cid:48)=yi

2Note that we do not need to explicitly construct Y  which could be costly when the alphabet size ∆ is large.
Instead  we can create its transitions on-the-ﬂy as demanded by the composition operation. Thus  for the rational
kernels commonly used  at most the transitions labeled with the alphabet symbols appearing in Yi need to be
created.

6

l(cid:89)

t=1

l(cid:89)

t=1

(a)

(b)

Figure 3: (a) Edit-distance transducer Uedit over the tropical semiring  in the case where the substitution cost
is 1  the deletion cost 2  the insertion cost 3  and the alphabet ∆ = {a  b}. (b) Smith-Waterman transducer
USmith-Waterman over the tropical semiring  in the case where the substitution  deletion and insertion costs are 1 
and where the matching cost is −2  for the alphabet ∆ = {a  b}.

Next  we can apply weighted determinization (Mohri  1997) to compute a deterministic WFA
equivalent to M  denoted by Det(M). By (Cortes et al.  2015)[Theorem 3]  Det(M) can be computed
in polynomial time. Since Det(M) is deterministic and by construction accepts precisely the set
of strings y ∈ ∆l  it admits a unique accepting path labeled with y whose weight is Det(M)(y) =
M(y) = U(y  yi). The weight of that accepting path is obtained by multiplying the weights of its
transitions and that of the ﬁnal state. Let B be the WFA derived from Det(M) by replacing each
u. Then  by construction  for any y ∈ ∆l  we have
transition weight or ﬁnal weight u by its inverse 1
B(y) = 1

U(y yi).

Combining A and B. Now consider the WFA C = A ◦ B  the composition of A and B. C is
deterministic since both A and B are deterministic. Moreover  C can be computed in time O(|A||B|).
By deﬁnition  for all y ∈ ∆l 

C(y) = A(y) × B(y) =

ew·ψ(xi yt−q+1:t t) ×

1

U(y  yi)

= eL(y yi)

ew·ψ(xi yt−q+1:t t).

(3)

To see how C can be used to compute Qw(z  s)  we note ﬁrst that the states of C can be identiﬁed
with pairs (qA  qB) where qA is a state of A  qB is a state of B  and the transitions are obtained by
matching a transition in A with one in B. Thus  for any z ∈ ∆q and s ∈ [l]  let Ez s be the set of
transitions of C constructed by pairing the transition in A ((z1:q−1  s − 1)  zq  ω(z  s)  (z2:q  s)) with
a transition in B:

(cid:110)(cid:0)(qA  qB)  zq  ω  (q(cid:48)

Ez s =

B)(cid:1) ∈ EC : qA = (z1:q−1  s − 1)

A  q(cid:48)

(cid:111)

.

(4)

e∈Ez s

Qw(z  s) can be computed as(cid:80)

Note that  since C is deterministic  there can be only one transition leaving a state labeled with zq.
Thus  to deﬁne Ez s  we only needed to specify the origin state of the transitions.
For each transition e ∈ Ez s  we denote its origin by e  destination by e and weight by ω(e). Then 
α(e) × ω(e) × β(e)  where α(e) is the sum of the weights
of all paths from an initial state of C to e  and β(e) is the sum of the weights of all paths from e to
a ﬁnal state of C. Since C is acyclic  α and β can be computed for all states in linear time in the
size of C using a single-source shortest-distance algorithm over the (+ ×) semiring (Mohri  2002)
or the so-called forward-backward algorithm. We denote these subroutines by DistFromInitial and
DistToFinal respectively in the pseudocode. Since C admits O(l|∆|q) transitions  we can compute all
of the quantities Qw(z  s)  s ∈ [l] and z ∈ ∆q and Z(cid:48)
Note that a natural alternative to the weighted transducer methods presented in this work is to consider
junction tree type methods for graphical methods. However  weighted transducer techniques typically
result in more “compact” representations than graphical model methods  and the computational cost
of the former can even be exponentially faster than the best one could achieve using the latter (Poon
and Domingos  2011).

w  in time O(l|∆|q).

7

���������������������������������������������������������������������������������������������������������������������������������������������������������GRAD-TROPICAL(xi  yi  w)
1 Y ← WFA accepting any y ∈ ∆l.
2 Yi ← WFA accepting yi.
3 M ← Π1(Y ◦ U ◦ Yi)
4 M ← Det(M)
5 B ← EXPONENTIATEWEIGHTS(M)
6 C ← A ◦ B
7 α ← DISTFROMINITIAL(C  (+ ×))
8 β ← DISTTOFINAL(C  (+ ×))
9 Zw ← β(IC) (cid:46) IC initial state of C
10 for (z  s) ∈ ∆q × [l] do
α(e) × ω(e) × β(e)

11 Qw(z  s) ←(cid:88)

12 Qw(z  s) ← Qw(z  s)/Zw

e∈Ez s

(a)

(b)

Figure 4: (a) Efﬁcient computation of the key terms of the structured gradient for the tropical loss. (b) Factoring
of the edit-distance transducer. The leftmost ﬁgure is the edit-distance weighted transducer Uedit over alphabet
Σ = {a  b}  the center ﬁgure is a weighted transducer T1  and the rightmost ﬁgure is a weighted transducer T2
such that Uedit = T1 ◦ T2.
5 An efﬁcient algorithm for the gradient computation of tropical losses

Following the treatment in (Cortes et al.  2015)  the tropical loss associated to a weighted transducer
U over the tropical semiring is deﬁned as the function LU : ∆∗ × ∆∗ → R coinciding with U; thus 
for all y  y(cid:48) ∈ ∆∗  LU(y  y(cid:48)) = U(y  y(cid:48)).
For examples of weighted transducers over the tropical semiring  see Figures 3(a) and (b).
Our algorithm for computing Qw(z  s) for a tropical loss  illustrated in Figure 4(a)  is similar to
our algorithm for a rational loss  with the primary difference being that we exponentiate weights
instead of invert them in the WFA B. Speciﬁcally  we design A just as in Section 4  and we design a
deterministic WFA B by ﬁrst designing Det(M) as in Section 4 and then deriving B from Det(M)
by replacing each transition weight or ﬁnal weight u in Det(M) by eu. Then by construction  for any
y ∈ ∆l  B(y) = eU(y yi). Moreover  composition of A with B yields a WFA C = A ◦ B such that
for all y ∈ ∆l 

C(y) = A(y) × B(y) =

ew·ψ(xi yt−q+1:t t) × eU(y yi) = eL(y yi)

ew·ψ(xi yt−q+1:t t).

(5)

t=1

t=1

As an example  the general edit-distance of two sequences y and y(cid:48) can  as already described 
be computed using Uedit in time O(|y||y(cid:48)|) (Mohri  2003). Note that for further computational
optimization  Uedit and USmith-Waterman can be computed on-the-ﬂy as demanded by the composition
operation  thereby creating only transitions with alphabet symbols appearing in the strings compared.
In order to achieve optimal dependence on the size of the input alphabet  we can also apply factoring
to the edit-distance transducer. Figure 4(b) illustrates factoring of the edit-distance transducer over
the alphabet Σ = {a  b}  where s is the substitution and deletion symbol and i is the insertion symbol.
Note that both T1 and T2 are linear in the size of Σ  while Uedit is quadratic in |Σ|. Furthermore 
using on-the-ﬂy composition  for any Y1 and Y2  we can ﬁrst compute Y1 ◦ T1 and T2 ◦ Y2 and then
compose the result achieving time and space complexity in O(|Y1||Y2|).

6 Experiments

In this section  we present experiments validating both the computational efﬁciency of our gradient
computation methods as well as the learning beneﬁts of training with natural loss functions. The
experiments in this section should be treated as a proof of concept. We defer an extensive study of
training structured prediction models on large-scale datasets for future work.

8

l(cid:89)

l(cid:89)

0/0a:a/0b:b/0a:b/1b:a/1ε:a/1ε:b/1a:ε/1b:ε/10/0a:a/0b:b/0a:s/1b:s/1ε:i/10/0a:a/0b:b/0s:a/0s:b/0s:ε/0i:a/0i:b/0Figure 5: Runtime comparison of efﬁcient versus naïve gradient computation methods for edit-distance (a) 
Smith-Waterman (b) and bigram (c) loss functions. The naïve line refers to the average runtime of Grad-Naïve 
the efﬁcient line refers to Grad-Tropical for edit-distance (a) and Smith-Waterman (b) and Grad-Rational for
bigram (c) loss. Naïve computations are shown only up to string length l = 8.

For the runtime comparison  we randomly generate an input and output data pair (xi  yi)  both
of a given ﬁxed length  as well as a weight vector w  and we compute ∇Fi(w) using both the
naïve and the outlined efﬁcient methods. As shown in Section 2  the computationally demanding
part in the ∇Fi(w) calculation is evaluating Qw(z  s) for all s ∈ [l] and z ∈ ∆q  while the other
terms are generally not problematic to compute. We deﬁne a procedure Grad-Naïve (see Figure 6
in the appendix) and compare the average runtimes of Grad-Naïve with that of Grad-Efﬁcient for
both rational and tropical losses. The efﬁcient algorithms suggested in this work improve upon the
Grad-Naïve runtime by eliminating the explicit loop over y ∈ Y and using the weighted automata
and transducer operations instead. All the weighted automata and transducer computations required
for Grad-Rational and Grad-Tropical are implemented using OpenFST (Allauzen et al.  2007).
More speciﬁcally  we deﬁne an alphabet |∆| = 10 and features Ψ(x  y) as vectors of counts of all
100 possible bigrams. For each string length l from 2 to 30  we draw input pairs (xi  yi) ∈ ∆l × ∆l
uniformly at random and w ∈ R100 according to a standard normal distribution. The average runtimes
over 125 random trials are presented in Figure 5 for three loss functions: the edit-distance  the Smith-
Waterman distance and the bigram loss. The experiments demonstrate a number of crucial beneﬁts of
our efﬁcient gradient computation framework. Note that the Grad-Naïve procedure runtime grows
exponentially in l  while Grad-Tropical and Grad-Rational exhibit linear dependency on the length
of the input strings. In fact  using the threshold pruning as part of determinization can allow one to
compute approximate gradient for arbitrarily long input strings. The computational improvement
is even more evident for rational losses  in which case the determinization of M can be achieved in
polynomial time (Cortes et al.  2015)  thus pruning is not required.
We also provide preliminary learning experiments that illustrate the beneﬁt of learning with a
structured loss for a sequence alignment task  compared to training with the cross-entropy loss. The
sequence alignment experiment replicates the artiﬁcial genome sequence data in (Joachims et al. 
2006)  where each example consists of native  homolog  and decoy sequences of length 50 and the
task is to predict a sequence that is the closest to native in terms of the Smith-Waterman alignment
score. The experiment conﬁrms that a model trained with Smith-Waterman distance as the objective
shows signiﬁcantly higher average Smith-Waterman alignment score (and higher accuracy) on a test
set compared to a model trained with cross-entropy objective. The cross-entropy model achieved a
Smith-Waterman score of 42.73  while the augmented model achieved a score of 44.65 on a test set
with a standard deviation of 0.35 averaged over 10 random folds.

7 Conclusion

We presented efﬁcient algorithms for computing the gradients of structured prediction models with
rational and tropical losses  reporting experimental results conﬁrming both runtime improvement
compared to naïve implementations and learning improvement compared to standard methods that
settle for using easier-to-optimize losses. We also showed how our approach can be incorporated
into the top layer of a neural network  so that it can be used to train end-to-end models in domains
including speech recognition  machine translation  and natural language processing. For future work 
we plan to run large-scale experiments with neural networks to further demonstrate the beneﬁt of
working directly with rational or tropical losses using our efﬁcient computational methods.

9

051015202530string length l4202468log(time) in secondsRun time for edit-distance lossefficientnaive051015202530string length l4202468Run time for Smith-Waterman lossefficientnaive051015202530string length l420246Run time for bigram lossefficientnaiveAcknowledgments

This work was partly funded by NSF CCF-1535987 and NSF IIS-1618662.

References
M. Abadi  P. Barham  J. Chen  Z. Chen  A. Davis  J. Dean  M. Devin  S. Ghemawat  G. Irving 
M. Isard  M. Kudlur  J. Levenberg  R. Monga  S. Moore  D. G. Murray  B. Steiner  P. Tucker 
V. Vasudevan  P. Warden  M. Wicke  Y. Yu  and X. Zheng. Tensorﬂow: a system for large-scale
machine learning. In Proceedings of USENIX  2016.

C. Allauzen  M. Riley  J. Schalkwyk  W. Skut  and M. Mohri. OpenFst: a general and efﬁcient

weighted ﬁnite-state transducer library. In Proceedings of CIAA. Springer  2007.

D. Amodei  R. Anubhai  E. Battenberg  C. Case  J. Casper  B. Catanzaro  J. Chen  M. Chrzanowski 
A. Coates  G. Diamos  E. Elsen  J. Engel  L. Fan  C. Fougner  A. Y. Hannun  B. Jun  T. Han 
P. LeGresley  X. Li  L. Lin  S. Narang  A. Y. Ng  S. Ozair  R. Prenger  S. Qian  J. Raiman 
S. Satheesh  D. Seetapun  S. Sengupta  C. Wang  Y. Wang  Z. Wang  B. Xiao  Y. Xie  D. Yogatama 
J. Zhan  and Z. Zhu. Deep speech 2: End-to-end speech recognition in english and mandarin. In
Proceedings of ICML  2016.

K. Chang  A. Krishnamurthy  A. Agarwal  H. Daumé III  and J. Langford. Learning to search better

than your teacher. In Proceedings of ICML  2015.

T. Chen  M. Li  Y. Li  M. Lin  N. Wang  M. Wang  T. Xiao  B. Xu  C. Zhang  and Z. Zhang. MXNet:
A ﬂexible and efﬁcient machine learning library for heterogeneous distributed systems. CoRR 
abs/1512.01274  2015.

C. Cortes  P. Haffner  and M. Mohri. Rational kernels: Theory and algorithms. JMLR  5:1035–1062 

2004.

C. Cortes  M. Mohri  and J. Weston. A General Regression Framework for Learning String-to-String

Mappings. In Predicting Structured Data. MIT Press  2007.

C. Cortes  V. Kuznetsov  M. Mohri  and M. K. Warmuth. On-line learning algorithms for path experts

with non-additive losses. In Proceedings of COLT  2015.

C. Cortes  V. Kuznetsov  M. Mohri  and S. Yang. Structured prediction theory based on factor graph

complexity. In Proceedings of NIPS  2016.

H. Daumé III  J. Langford  and D. Marcu. Search-based structured prediction. Machine Learning  75

(3):297–325  2009.

J. R. Doppa  A. Fern  and P. Tadepalli. Structured prediction via output space search. JMLR  15(1):

1317–1350  2014.

E. Eban  M. Schain  A. Mackey  A. Gordon  R. Rifkin  and G. Elidan. Scalable learning of non-

decomposable objectives. In Artiﬁcial Intelligence and Statistics  pages 832–840  2017.

K. Gimpel and N. A. Smith. Softmax-margin CRFs: Training log-linear models with cost functions.

In Proceedings of ACL  2010.

A. Graves and N. Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In

Proceedings of ICML  2014.

T. Joachims. A support vector method for multivariate performance measures. In Proceedings of

ICML  2005.

T. Joachims  T. Galor  and R. Elber. Learning to align sequences: A maximum-margin approach. In

New algorithms for macromolecular simulation  pages 57–69. Springer  2006.

D. Jurafsky and J. H. Martin. Speech and Language Processing (2nd Edition). Prentice-Hall  Inc. 

2009.

10

J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models for

segmenting and labeling sequence data. In Proceedings of ICML  2001.

M. Lam  J. R. Doppa  S. Todorovic  and T. G. Dietterich. Hc-search for structured prediction in

computer vision. In CVPR  2015.

A. Lucchi  L. Yunpeng  and P. Fua. Learning for structured prediction using approximate subgradient

descent with working sets. In Proceedings of CVPR  2013.

C. D. Manning and H. Schütze. Foundations of Statistical Natural Language Processing. The MIT

Press  Cambridge  Massachusetts  1999.

D. A. McAllester  T. Hazan  and J. Keshet. Direct loss minimization for structured prediction. In

Proceedings of NIPS  2010.

M. Mohri. Finite-state transducers in language and speech processing. Computational Linguistics  23

(2):269–311  1997.

M. Mohri. Semiring Frameworks and Algorithms for Shortest-Distance Problems. Journal of

Automata  Languages and Combinatorics  7(3):321–350  2002.

M. Mohri. Edit-distance of weighted automata: General deﬁnitions and algorithms. International

Journal of Foundations of Computer Science  14(6):957–982  2003.

M. Mohri. Weighted automata algorithms. In Handbook of Weighted Automata  pages 213–254.

Springer  2009.

D. Nadeau and S. Sekine. A survey of named entity recognition and classiﬁcation. Linguisticae

Investigationes  30(1):3–26  January 2007.

M. Norouzi  S. Bengio  N. Jaitly  M. Schuster  Y. Wu  and D. Schuurmans. Reward augmented

maximum likelihood for neural structured prediction. In Proceedings of NIPS  2016.

F. J. Och. Minimum error rate training in statistical machine translation. In Proceedings of ACL 

volume 1  2003.

A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison  L. Antiga 

and A. Lerer. Automatic differentiation in PyTorch. In Proceedings of NIPS  2017.

H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In ICCV Workshops 

pages 689–690  2011.

R. Prabhavalkar  T. N. Sainath  Y. Wu  P. Nguyen  Z. Chen  C.-C. Chiu  and A. Kannan. Mini-
mum word error rate training for attention-based sequence-to-sequence models. arXiv preprint
arXiv:1712.01818  2017.

M. Ranjbar  T. Lan  Y. Wang  S. N. Robinovitch  Z.-N. Li  and G. Mori. Optimizing nondecomposable
IEEE transactions on pattern analysis and machine

loss functions in structured prediction.
intelligence  35(4):911–924  2013.

M. Ranzato  S. Chopra  M. Auli  and W. Zaremba. Sequence level training with recurrent neural

networks. arXiv preprint arXiv:1511.06732  2015.

S. Ross  G. J. Gordon  and D. Bagnell. A reduction of imitation learning and structured prediction to

no-regret online learning. In Proceedings of AISTATS  2011.

B. Schölkopf  K. Tsuda  and J.-P. Vert. Kernel methods in computational biology. MIT Press 

Cambridge  Mass.  2004.

F. Seide and A. Agarwal. CNTK: Microsoft’s open-source deep-learning toolkit. In Proceedings of

KDD. ACM  2016.

S. Shen  Y. Cheng  Z. He  W. He  H. Wu  M. Sun  and Y. Liu. Minimum risk training for neural

machine translation. In Proceedings of ACL  volume 1  2016.

11

I. Sutskever  O. Vinyals  and Q. V. Le. Sequence to sequence learning with neural networks. In

Proceedings of NIPS  2014.

B. Taskar  C. Guestrin  and D. Koller. Max-margin Markov networks. In NIPS  2003.

I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured and

interdependent output variables. JMLR  6:1453–1484  Dec. 2005.

O. Vinyals  L. Kaiser  T. Koo  S. Petrov  I. Sutskever  and G. Hinton. Grammar as a foreign language.

In Proceedings of NIPS  2015a.

O. Vinyals  A. Toshev  S. Bengio  and D. Erhan. Show and tell: A neural image caption generator. In

Proceedings of CVPR  2015b.

Y. Wu  M. Schuster  Z. Chen  Q. V. Le  M. Norouzi  W. Macherey  M. Krikun  Y. Cao  Q. Gao 
K. Macherey  J. Klingner  A. Shah  M. Johnson  X. Liu  Łukasz Kaiser  S. Gouws  Y. Kato  T. Kudo 
H. Kazawa  K. Stevens  G. Kurian  N. Patil  W. Wang  C. Young  J. Smith  J. Riesa  A. Rudnick 
O. Vinyals  G. Corrado  M. Hughes  and J. Dean. Google’s neural machine translation system:
Bridging the gap between human and machine translation. CoRR  abs/1609.08144  2016. URL
http://arxiv.org/abs/1609.08144.

D. Zhang  L. Sun  and W. Li. A structured prediction approach for statistical machine translation. In

Proceedings of IJCNLP  2008.

12

,Corinna Cortes
Vitaly Kuznetsov
Mehryar Mohri
Dmitry Storcheus
Scott Yang