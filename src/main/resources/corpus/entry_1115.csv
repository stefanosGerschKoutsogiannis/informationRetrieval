2019,Constrained Reinforcement Learning Has Zero Duality Gap,Autonomous agents must often deal with conflicting requirements  such as completing tasks using the least amount of time/energy  learning multiple tasks  or dealing with multiple opponents. In the context of reinforcement learning~(RL)  these problems are addressed by (i)~designing a reward function that simultaneously describes all requirements or (ii)~combining modular value functions that encode them individually. Though effective  these methods have critical downsides. Designing good reward functions that balance different objectives is challenging  especially as the number of objectives grows. Moreover  implicit interference between goals may lead to performance plateaus as they compete for resources  particularly when training on-policy. Similarly  selecting parameters to combine value functions is at least as hard as designing an all-encompassing reward  given that the effect of their values on the overall policy is not straightforward. The later is generally addressed by formulating the conflicting requirements as a constrained RL problem and solved using Primal-Dual methods. These algorithms are in general not guaranteed to converge to the optimal solution since the problem is not convex. This work provides theoretical support to these approaches by establishing that despite its non-convexity  this problem has zero duality gap  i.e.  it can be solved exactly in the dual domain  where it becomes convex. Finally  we show this result basically holds if the policy is described by a good parametrization~(e.g.  neural networks) and we connect this result with primal-dual algorithms present in the literature and we establish the convergence to the optimal solution.,Constrained Reinforcement Learning Has Zero

Duality Gap

Santiago Paternain  Luiz F. O. Chamon  Miguel Calvo-Fullana and Alejandro Ribeiro

{spater luizf cfullana aribeiro}@seas.upenn.edu

Electrical and Systems Engineering

University of Pennsylvania

Abstract

Autonomous agents must often deal with conﬂicting requirements  such as com-
pleting tasks using the least amount of time/energy  learning multiple tasks  or
dealing with multiple opponents. In the context of reinforcement learning (RL) 
these problems are addressed by (i) designing a reward function that simultane-
ously describes all requirements or (ii) combining modular value functions that en-
code them individually. Though effective  these methods have critical downsides.
Designing good reward functions that balance different objectives is challenging 
especially as the number of objectives grows. Moreover  implicit interference
between goals may lead to performance plateaus as they compete for resources 
particularly when training on-policy. Similarly  selecting parameters to combine
value functions is at least as hard as designing an all-encompassing reward  given
that the effect of their values on the overall policy is not straightforward. The
later is generally addressed by formulating the conﬂicting requirements as a con-
strained RL problem and solved using Primal-Dual methods. These algorithms
are in general not guaranteed to converge to the optimal solution since the prob-
lem is not convex. This work provides theoretical support to these approaches by
establishing that despite its non-convexity  this problem has zero duality gap  i.e. 
it can be solved exactly in the dual domain  where it becomes convex. Finally  we
show this result basically holds if the policy is described by a good parametriza-
tion (e.g.  neural networks) and we connect this result with primal-dual algorithms
present in the literature and we establish the convergence to the optimal solution.

1

Introduction

Autonomous agents must often deal with conﬂicting requirements  such as completing a task in the
least amount of time/energy  learning multiple tasks or contexts  dealing with multiple opponents or
with several speciﬁcations that are designed to guide the agent in the learning process. In the context
of reinforcement learning [1]  these problems are generally addressed by combining modular value
functions that encode them individually  by multiplying each signal by its own coefﬁcient  which
controls the emphasis placed on it [2–4]. Although effective  the multi-objective problem [5] has
several downsides. First  for each set of penalty coefﬁcients  there exists a different  optimal solution 
also known as Pareto optimality [6]. In practice  the exact coefﬁcient is selected through a time
consuming and a computationally intensive process of hyper-parameter tuning that often times are
domain dependent  as showed in [7–9]. Moreover  implicit interference between the goals may lead
to training plateaus as they compete for resources in the policy [10].
An alternative  is to embed all conﬂicting requirements in a constrained RL problem and to use a
primal-dual algorithm as in [7  11] that chooses the parameters automatically. The main advantage of
this approach is that constraints ensure satisfying behavior without the need for manually selecting

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In these algorithms the policy update is on a faster time-scale than the
the penalty coefﬁcients.
multiplier update. Thus  effectively  these approaches work as if the dual problem of the constrained
reinforcement learning problem was being solved. Thus  guaranteeing to obtain the feasible solution
with the smallest suboptimality. Yet  there is no guarantee on how small the suboptimality is. In this
work we provide an answer to the previous question. In particular we establish that:

1. Despite its non-convexity  constrained reinforcement learning for policies belonging to a
general distribution class has zero duality gap  i.e.  it can be solved exactly in the dual
domain  where the problem is actually convex

2. Since working with generic distributions as policies is in general intractable  we extend this
result to parametrized policies  by showing that the suboptimality bound also holds when
the parametrization is a universal approximator  e.g.  a neural network [12]).

3. We leverage these theoretical results to establish that the family of primal-dual algorithms
for constrained reinforcement learning  e.g. [7  11]  in fact converge to the optimal solution
under mild assumptions.

1.1 Related Work

Constrained Markov Decision Processes (CMDPs) [13] are an active ﬁeld of research. CMDP ap-
plications cover a vast number of topics  such as: electric grids [14]  networking [15]  robotics
[3  16  17] and ﬁnance [18  19]. The most common approaches to solve this problems can be di-
vided under the following categories. Manual selection of Lagrange multipliers: constrained Re-
inforcement Learning problems can be solved through by maximizing an unconstrained Lagrangian 
for a speciﬁc multiplier [2]. The combination of different rewards with manually selected Lagrange
multipliers has been applied for instance to learning complex movements for humanoids [4] or to
limit the variance of the constraint that needs to be satisﬁed [19  20]. Integrating prior knowledge
about the system transitions is exploited in order to project the action chosen by the policy to a set
that ensures the satisfaction of the constraints [21]. Primal-dual algorithms [7  11]  allow us to
choose dynamically the multipliers by ﬁnd the best policy for the current set of parameters and then
taking steps along the gradient of the Lagrangian with respect to the multipliers. These allow to
consider general constraints and the algorithm is reward agnostic and it does not require the use of
prior knowledge.

2 Constrained Reinforcement Learning
Let t ∈ N ∪ {0} denote the time instant and S ⊂ Rn and A ⊂ Rd be compact sets describing the
possible states and actions of an agent described by a Markovian dynamical system with transition
probability density p  i.e.  p (st+1 | {su  au}u≤t) = p (st+1 | st  at) for st ∈ S and at ∈ A for
all t. The agent chooses actions sequentially based on a policy π ∈ P(S)  where P(S) is the
space of probability measures on (A B(A)) parametrized by elements of S  where B(A) are the
Borel sets of A. The action taken by the agent at each state results in rewards deﬁned by the
functions ri : S × A → R  for i = 0  . . .   m  that the agent accumulates over time. These rewards
describe different objectives that the agent must achieve  such as completing a task  remaining within
a region of the state space  or not running out of battery. The goal of constrained RL is then to ﬁnd
a policy π(cid:63) ∈ P(S) that meets these objectives by solving the problem

P (cid:63) (cid:44) max
π∈P(S)

V0(π) (cid:44) Es π

subject to Vi(π) (cid:44) Es π

γtr0(st  π(st))

(cid:35)
(cid:35)

(cid:34) ∞(cid:88)
(cid:34) ∞(cid:88)

t=0

t=0

γtri(st  π(st))

≥ ci  i = 1  . . .   m 

(PI)

where γ ∈ (0  1) is a discount factor and ci ∈ R represent the i-th reward speciﬁcation.
It is
important to contrast the formulation in (PI) with the unconstrained  regularized problem commonly
found in the literature [4  19  20]

maximize

π∈P(S)

V0(π) +

wi (Vi(π) − ci)  

( ˜PI)

m(cid:88)

i=1

2

where wi ≥ 0 are the regularization parameters. First  (PI) precludes the manual balancing of
different requirements through the choice of wi. Even with expert knowledge  tuning these pa-
rameters can be as hard as solving the RL problem itself  since there is no straightforward relation
between the value of wi and the value Vi(π(cid:63)) given by the ﬁnal policy. What is more  note that

the objective of ( ˜PI) can be written as a single value function ¯V (π) (cid:44) Es π [(cid:80)∞t=0 γt¯r(st  π(st))]
for ¯r(st  π(st)) = r0(st  π(st)) +(cid:80)m

i=1 wiri(st  π(st)). In other words  choosing the value of wi
amounts to designing a reward that simultaneously encodes different  possibly conﬂicting  objec-
tives and/or requirements. Given the challenge that can be designing good reward functions for a
single task  it is ready that this regularized approach is neither efﬁcient nor effective.
Though promising  solving the constrained RL problem in (PI) is intricate. Indeed  it is both inﬁnite
dimensional and non-convex  so that it is in general not tractable in the primal domain. Its dual
problem  on the other hand  is convex and has dimensionality equal to the number of constraints.
However  since (PI) is not a convex program  its dual problem in general only provides an upper
bound on P (cid:63). How good the policy obtained by solving the dual problem is depends on the tightness
of this bound. What is more  formulating the problem in the dual domain is at least as hard as
solving ( ˜PI)  which is also inﬁnite dimensional and non-convex. In the sequel  we address these two
issues by ﬁrst showing that (PI) has no duality gap (Section 3)  i.e.  that the upper bound on P (cid:63) from
the dual problem is tight. This implies that (PI) can be solved exactly in the dual domain. Then  we
show that we lose (almost) nothing by parametrizing the policies π (Section 4)  which immediately
addresses the issue of dimensionality in (PI)–( ˜PI). Finally  we put forward and analyze a primal-
dual algorithm for constrained RL (Section 5)  showing that under mild conditions it yields a locally
optimal  feasible solution of (PI).

3 Constrained Reinforcement Learning Has Zero Duality Gap
Let us start by formalizing the concept of dual problem. Let the vector λ ∈ Rm
multipliers of the constraints of (PI) and deﬁne its Lagrangian as

+ collect the Lagrange

L(π  λ) (cid:44) V0(π) +

λi (Vi(π) − ci) .

m(cid:88)

i=1

(1)

(2)

The dual function is then the point-wise maximum of (1) with respect to the policy π  i.e. 

d(λ) (cid:44) max

π∈P(S)L(π  λ).

The dual function (2) provides an upper bounds on the value of (PI)  i.e.  d(λ) ≥ P (cid:63) for all λ ∈
Rm
+ [22  Section 5.1.3]. The tighter the bound  the closer the policy obtained from (2) is to the
optimal solution of (PI). Hence  the dual problem is that of ﬁnding the tightest of these bounds:

D(cid:63) (cid:44) min
λ∈Rm

+

d(λ).

(DI)

Note that the dual function (2) can be related to the unconstrained  regularized problem ( ˜PI) from
Section 2 by taking λi = wi in (1). Hence  (2) takes on the optimal value of ( ˜PI) for all possible
regularization parameters. Problem (DI) then ﬁnds the best regularized problem  i.e.  that whose
value is closest to P (cid:63). It turns out  this problem is tractable if d(λ) can be evaluated  since (DI) is
a convex program (the dual function is the point-wise maximum of a set of linear functions and is
therefore convex) [22  Section 3.2.3].
Despite these similarities  (DI) [and consequently ( ˜PI)] do not necessarily solve the same problem
as (PI). In other words  there need not be a relation between the optimal dual variables λ(cid:63) from (DI)
or the regularization parameters wi and the speciﬁcations ci of (PI). This depends on the value of the
duality gap ∆ = D(cid:63)−P (cid:63). Indeed  if ∆ is small  then so is the suboptimality of the policies obtained
from (DI). In the limit case where ∆ = 0  problems (PI)–(DI) and ( ˜PI) would all be essentially
equivalent. Since (PI) is not a convex program  however  this result does not hold immediately. Still 
we calim in Theorem 1 that (PI) has zero duality gap under Slater’s conditions. Before stating the
Theorem we deﬁne the perturbation function associated to problem (PI) which is fundamental for
the proof of the result and for future reference. For any ξ ∈ Rn  the perturbation function associated

3

to (PI) is deﬁned as

P (ξ) (cid:44) max
π∈P(S)
subject to Vi(π) ≥ ci + ξi  i = 1 . . . m.

V0(π)

(PI(cid:48))

Notice that P (0) = P (cid:63)  the optimal value of (PI). We formally state next the conditions under which
Problem (PI) has zero duality gap.
Theorem 1. Suppose that ri is bounded for all i = 0  . . .   m and that Slater’s condition holds
for (PI). Then  strong duality holds for (PI)  i.e.  P (cid:63) = D(cid:63).

Proof. This proof relies on a well-known result from perturbation theory connecting strong duality
to the convexity of the perturbation function deﬁned in(PI(cid:48)). We formalize this result next.
Proposition 1 (Fenchel-Moreau). If (i) Slater’s condition holds for (PI) and (ii) its perturbation
function P (ξ) is concave  then strong duality holds for (PI).

Proof. See  e.g.  [23  Cor. 30.2.2].

P(cid:2)µξ1 + (1 − µ)ξ2(cid:3)

≥ µP(cid:0)ξ1(cid:1) + (1 − µ)P(cid:0)ξ2(cid:1) .

Condition (i) of Proposition 1 is satisﬁed by the hypotheses of Theorem 1. It sufﬁces then to show
that the perturbation function is concave [(ii)]  i.e.  that for every ξ1  ξ2 ∈ Rm  and µ ∈ (0  1) 

(3)
If for either perturbation ξ1 or ξ2 the problem becomes infeasible then P (ξ1) = −∞ or P (ξ2) =
−∞ and thus (3) holds trivially. For perturbations that keep the problem feasible  suppose P (ξ1)
and P (ξ2) are achieved by the policies π1 ∈ P(S) and π2 ∈ P(S) respectively. Then  P (ξ1) =
V0(π1) with Vi(π1) − ci ≥ ξ1
i for i = 1  . . .   m.
i and P (ξ2) = V0(π2) with Vi(π2) − ci ≥ ξ2
To establish (3) it sufﬁces to show that for every µ ∈ (0  1) there exists a policy πµ such that
i and V0(πµ) = µV0(π1) + (1 − µ)V0(π2). Notice that any policy
Vi(πµ) − ci ≥ µξ1
i + (1 − µ)ξ2
πµ satisfying the previous conditions is a feasible policy for the slack ci + µξ1
i . Hence 
by deﬁnition of the perturbed function (PI(cid:48))  it follows that

i + (1 − µ)ξ2

P(cid:2)µξ1 + (1 − µ)ξ2(cid:3)

(4)
If such policy exists  the previous equation implies (3). Thus  to complete the proof of the result we
need to establish its existence. To do so we start by formulating a linear program equivalent to (PI(cid:48)).
Notice that for any i = 0  . . .   m we can write

≥ V0(πµ) = µV0(π1) + (1 − µ)V0(π2) = µP(cid:0)ξ1(cid:1) + (1 − µ)P(cid:0)ξ2(cid:1) .
(cid:90)

(cid:33)

(cid:32) ∞(cid:88)

Vi(π) =

γtri(st  at)

pπ(s0  a0  . . .) ds0 . . . da0 . . . .

(5)

(S×A)∞

t=0

Since the reward functions are bounded the Dominated Convergence Theorem holds. This allows us
to exchange the order of the sum and the integral. Moreover  using conditional probabilities and the
Markov property of the transition of the system we can write Vi(π) as

t=0

Vi(π) =

ri(st  at)

p(su|su−1  au−1)π(au|su)p(s0)π(a0|s0) ds0 . . . da0 . . . .
(6)
Notice that for every u > t the integrals with respect to au and su yield one  since they are integrating
density functions. Thus  the previous expression reduces to

(S×A)∞

u=1

∞(cid:89)

t(cid:89)

u=1

(cid:90)

∞(cid:88)

γt

∞(cid:88)

t=0

(cid:90)

(cid:90)

Vi(π) =

γt

ri(st  at)

(S×A)t

p(su|su−1  au−1)π(au|su)p(s0)π(a0|s0) ds0 . . . dstda0 . . . dat.

(7)
Notice that the probability density of being at state s and choosing action a under the policy π at
time t can be written as

t(cid:89)

pt
π(st  at) =

(S×A)t−1

u=1

p(su|su−1  au−1)π(au|su)p(s0)π(a0|s0) ds0 . . . dst−1da0 . . . dat−1.
(8)

4

(cid:90)

∞(cid:88)

Thus  using again the Dominated Convergence Theorem  one can write compactly (7) as

By deﬁning the occupation measure ρ(s  a) = (1 − γ)(cid:80)∞t=0 γtpt
γ)Vi(π) =(cid:82)

π(s  a) dsda.

Vi(π) =

ri(s  a)

S×A

γtpt

t=0

S×A

set R as the set of all occupation measures induced by the policies π ∈ P(S) as
γtpπ(st = s  at = a)

π(s  a) it follows that (1 −
(cid:40)
ri(s  a)ρ(s  a) dsda. Denote by M(S A) the measures over S ×A and deﬁne the
ρ ∈ M(S A)(cid:12)(cid:12)ρ(s  a) = (1 − γ)

(cid:32) ∞(cid:88)

(cid:33)(cid:41)

R :=

(10)

 

(9)

t=0

where It follows from [24  Theorem 3.1] that the set of occupation measures R is convex and com-
pact. Hence  we can write the following linear program equivalent to (PI(cid:48))

(cid:90)
(cid:90)

(cid:90)

P (ξ) (cid:44) max
ρ∈R

subject to

1
1 − γ
1
1 − γ

S×A

S×A

r0(s  a)ρ(s  a) dsda

ri(s  a)ρ(s  a) dsda ≥ ci + ξi  i = 1  . . .   m.

(PI(cid:48)(cid:48))

Let ρ1  ρ2 ∈ R be the occupation measures associated to π1 and π2. Since  R is convex  there exists
a policy πµ ∈ P(S) such that its corresponding occupation measure is ρµ = µρ1 + (1 − µ)ρ2 ∈ R.
i for i = 1  . . .   m since the
Notice that ρµ satisﬁes the constraints with slack ci + µξ1
i + (1 − µ)ξ2
integral is linear and ρ1 and ρ2 satisfy the constraints with slacks ci + ξ1
i respectively.
i and ci + ξ2
Thus  it follows that

P (µξ1 + (1 − µ)ξ2) ≥

r0(s  a)ρµ(s  a) dsda = µV0(π1) + (1 − µ)V0(π2) 

(11)

where we have used again the linearity of the integral. Since πi are such that V0(π1) = P (ξ1) and
V0(π2) = P (ξ2)  inequality (3) follows. This completes the proof that the perturbation function is
concave.

1
1 − γ

S×A

Theorem 1 establishes a fundamental equivalence between the constrained (PI) and the dual prob-
lem (DI) [and therefore also ( ˜PI)]. Indeed  since (PI) has no duality gap  its solution can be obtained
by solving (DI). What is more  the trade-offs expressed by the wi in ( ˜PI) are the same as those
expressed by the speciﬁcations ci in the sense that they trace the same Pareto front. Nevertheless 
note that the relationship between ci and wi is not trivial and that specifying the constrained prob-
lem is often considerably simpler. Theorem 1 establishes that this is indeed a valid transformation 
since both problems are equivalent. Observe that due to the non-convexity of the objective in RL
problems  this result is in fact not immediate.
The theoretical importance of the previous result notwithstanding  it does not yield a procedure to
solve (PI) since evaluating the dual function involves a maximization problem that is intractable for
general classes of distributions. In the next section  we study the effect of using a ﬁnite parametriza-
tion for the policies and show that the price to pay in terms of duality gap depends on how “good”
the parametrization is. If we consider  for instance  a neural network—which are universal function
approximators [12  25–28]—the loss in optimality can be made arbitrarily small.

4 There is (almost) no price to pay by parametrizing the policies
We consider next the problem where the policies are parametrized by a vector θ ∈ Rp. This vector
could be for instance the coefﬁcients of a neural network or the weights of a linear combination
of functions. In this work  we focus our attention however on a widely used class of parametriza-
tions that we term near-universal  which are able to model any function in P(S) to within a stated
accuracy. We formalize this concept in the following deﬁnition.
Deﬁnition 1. A parametrization πθ is an -universal parametrization of functions in P(S) if  for
some  > 0  there exists for any π ∈ P(S) a parameter θ ∈ Rp such that

(cid:90)

max
s∈S

A

|π(a|s) − πθ(a|s)| da ≤ .

(12)

5

The previous deﬁnition includes all parametrizations that induce distributions that are close to dis-
tributions in P(S) in total variational norm. Notice that this is a milder requirement than approxi-
mation in uniform norm which is a property that has been established to be satisﬁed by radial basis
functions networks [29]  reproducing kernel Hilbert spaces [30] and deep neural networks [12].
Notice that the objective function and the constraints in Problem (PI) involve an inﬁnite horizon
and thus  the policy is applied an inﬁnite number of times. Hence  the error introduced by the
parametrization could a priori accumulate and induce distributions over trajectories that differ con-
siderably from the distributions induced by policies in P(S). We claim in the following lemma that
this is not the case.
Lemma 1. Let ρ and ρθ be occupation measures induced by the policies π ∈ P(S) and πθ respec-
tively  where πθ is an - parametrization of π. Then  it follows that


(cid:90)

|ρ(s  a) − ρθ(s  a)| dsda ≤

.

1 − γ

S×A

The previous result  although derived as a technical result required to bound the duality gap for
parametric problems  has a natural interpretation. The larger γ —the more the operation is concerned
about rewards far in the future —the larger the error in the approximation of the occupation measure.
Having deﬁned the concept of universal approximator  we shift focus to writing the parametric
version of the constrained reinforcement learning problem. This is  to ﬁnd the parameters that solve
(PI)  where now the policies are restricted to the functions induced by the chosen parametrization

(13)

(PII)

P (cid:63)
θ

(cid:44) max

θ

V0(θ) (cid:44) Es πθ

subject to Vi(θ) (cid:44) Es πθ

γtr0(st  πθ(st))

(cid:35)
(cid:35)

(cid:34) ∞(cid:88)
(cid:34) ∞(cid:88)

t=0

t=0

γtri(st  πθ(st))

≥ ci  i = 1 . . . m.

Notice that the problem (PII) is similar to the original problem (PI)  with the only difference that the
expectations are now with respect to distributions induced by the parameter vector θ. As done in the
previous section  let λ ∈ Rm and deﬁne the dual function associated to (PII) as
λi (Vi(θ) − ci)  
Likewise we deﬁne the dual problem as ﬁnding the tightest upper bound for (PII)

θ∈Rp Lθ(θ  λ) (cid:44) min
θ∈Rp

dθ(λ) (cid:44) min

m(cid:88)

V0(θ) +

(14)

i=1

D(cid:63)
θ

(cid:44) minimize

λ∈Rm

+

dθ(λ).

(DII)

As previously stated  the reason for introducing the parametrization is to turn the original functional
optimization problem into a tractable problem in which the optimization variable is a ﬁnite dimen-
sional vector of parameters. Yet  there is a cost for introducing the aforementioned parametrization:
the duality gap is no longer null. The latter means that the solution obtained through the dual prob-
lem is sub-optimal. We claim however that this gap is bounded by a function that is linear with the
approximation error   and thus if the parametrization has a good representation power the price to
pay is almost zero. This is the subject of the following theorem.
Theorem 2. Suppose that ri is bounded for all i = 0  . . .   m by constants Bri > 0 and deﬁne Br =
maxi=1...m Bri. Let λ(cid:63)
 be the solution to the dual problem associated to (PI(cid:48)) for perturbation
ξi = Br/(1 − γ) for all i = 1  . . .   m. Then  under the hypothesis of Theorem 1 it follows that

P (cid:63) ≥ D(cid:63)

θ ≥ P (cid:63) − (Br0 + (cid:107)λ(cid:63)

(cid:107)1 Br)

 

(15)



1 − γ

where P (cid:63) is the optimal value of (PI)  and D(cid:63)

θ the value of the parametrized dual problem (DII).

The implication of the previous result is that there is almost no price to pay by introducing a
parametrization. By solving the dual problem (DII) the sub-optimality achieved is of order   i.e. 
the error on the representation of the policies. Notice that this error could be made arbitrarily small
by increasing the representation ability of the parametrization  by for instance increasing the dimen-
sion of the vector of parameters θ. The latter means that if we can compute the dual function it is

6

possible to solve (PI) approximately. Moreover  working on the dual domain provides two computa-
tional advantages; on one hand  the dimension of the problem is the number of constraints in (PI). In
addition  the dual function is always convex  hence gradient descent on the dual domain solves the
problem of interest. In the next section we propose an algorithm to solve (PI) approximately based
on the previous discussion.
Before doing so notice that we have not assumed anything about the feasibility of problem (PII).
Notice that if the problem is infeasible then we have that D(cid:63)
θ = −∞ and thus the upper bound
on (15) holds trivially. On the other hand if the problem is infeasible it also means that there is no
policy π ∈ P(S) that satisﬁes the constraints of (PI) with slack Br/(1− γ) since θ is an -universal
approximation of P(S). Hence the perturbed problem is infeasible which yields a dual multiplier
 that has inﬁnite norm. Thus the right hand side of (15) holds as well. In that sense  as long as the
λ(cid:63)
parameterization introduced keeps the problem feasible the price to pay for parameterizing is almost
zero.

5 Solving Constrained Reinforcement Learning Problems

As previously stated  the dual function is always a convex function since it is the point-wise maxi-
mum of linear functions. Thus the dual problem (DII) can be efﬁciently solved using (sub)gradient
descent  with the caveat that because we require the dual iterates to remain in the positive orthant 
we include a projection onto this space after taking the gradient step

λk+1 = [λk − η∂dθ(λk)]+  

(16)
where η > 0 is the step-size of the algorithm  [·]+ denotes the projection onto Rm
+ and ∂dθ(λ)
denotes —with a slight abuse of notation —a vector in the subgradient of dθ(λ). The latter can be
computed by virtue of Dankin’s Theorem (see e.g. [31  Chapter 3]) by evaluating the constraints
in the original problem (PII) at the primal maximizer of the Lagrangian. Thus  the main theoretical
difﬁculty in this computation lies on ﬁnding said maximizer since the Lagrangian is non-convex
with respect to θ. However  maximizing the Lagrangian with respect to θ corresponds to learning a
policy that uses as reward the following linear combination of rewards

rλ(s  a) = r0(s  a) +

λiri(s  a).

(17)

(cid:34) ∞(cid:88)

Es π

(cid:35)

(cid:34) ∞(cid:88)

t=0

(cid:34) ∞(cid:88)

(cid:35)

Indeed  using the linearity of the expectation  the cumulative discounted cost for the reward rλ(s  a)
yields

λiEs πθ

= Es πθ

γtri(st  at)

γtrλ(st  at)

γtr0(st  at)

+

t=0

t=0

= L(θ  λ).
(18)
And therefore reinforcement learning algorithms such as policy gradient [32] or actor-critic meth-
ods [33] can be used to ﬁnd the parameters θ such that they maximize the Lagrangian. The good
performance of these algorithms is rooted in the fact that they are able to maximize the expected
cumulative reward or at least to achieve a value that is close to the maximum. The next assumption
formalizes this idea.
Assumption 1. Let πθ be a parametrization of functions in P(S) and let Lθ(θ  λ) with λ ∈ Rm
be the Lagrangian associated to (PII). Denote by θ(cid:63)(λ)  θ†(λ) ∈ RP the maximum of L(θ  λ) and
a local maximum respectively achieved by a generic reinforcement learning algorithm. Then  there
exists δ > 0 such that for all λ ∈ Rm
Notice that the previous assumption only means that we are able to solve the regularized uncon-
strained problem approximately. This means that the parameter at time k + 1 is

+ it holds that Lθ(θ(cid:63)(λ)  λ) ≤ Lθ(θ†(λ)  λ) + δ.

+

m(cid:88)

i=1

(cid:35)

m(cid:88)

i=1

Then  the dual variable is updated following the gradient descent scheme suggested in (16)  where
we replace the subgradient of the dual function by the constraint of the primal problem (PII). Deﬁn-

θk+1 ≈ argmax

θ∈Rp L(λk  θ).

(19)

ing ˆ∂dk (cid:44) V (θk+1) − s  the update yields
λk − η ˆ∂dk

λk+1 =

(cid:104)

(cid:105)

+

= [λk − η (V (θk+1) − s)]+ .

(20)

7

Algorithm 1 dualDescent
Input: η
1: Initialize: θ0 = 0  λ0 = 0
2: for k = 0  1 . . .
3:
4:
5: end

Compute an approximation of θk+1 ≈ argmaxLθ(θ  λk) with a RL algorithm
Compute the dual ascent step λk+1 = [λk − η (V (θk+1) − s)]+.

The algorithm given by (19)–(20) is summarized under Algorithm 1. The previous algorithm relies
on the fact that the ˆ∂dk does not differ much from ∂dθ(λk). We claim in the following proposition
that this is the case. In particular  we establish that the constraint evaluation does not differ from the
subgradient in more than δ  the error on the primal maximization deﬁned in Assumption 1.
Proposition 2. Under Assumption 1  the constraint in (PII) evaluated at a local maximizer of La-
grangian θ†(λ) approximate the subgradient of the dual function (14). In particular it follows that

dθ(λ) − dθ(λ(cid:63)

θ) ≤ (λ − λ(cid:63)

θ)(cid:62)(cid:0)V (θ†(λ)) − s(cid:1) + δ.

(21)

The previous proposition is key in establishing convergence of the algorithm proposed since allows
us to claim that the dual updated is an approximation of a dual descent step. We formalize this result
next and we establish a maximum number of dual steps required to achieve a desired accuracy.
Theorem 3. Let πθ be an  universal parametrization of P(S) according to Deﬁnition 1  Br =
maxi=1...m Bri with Bri > 0 bounds on the rewards ri and γ ∈ (0  1) be the discount factor. Then 
if Slater’s conditions hold for (PII)  under Assumption 1 and for any ε > 0  the sequence of updates
of Algorithm 1 with step size η converges in K > 0 steps  with

θ(cid:107)2
K ≤ (cid:107)λ0 − λ(cid:63)
to a neighborhood of P (cid:63) –the solution of (PI)– satisfying

2ηε

 

where B =(cid:80)m

1 − γ ≤ dθ(λK) ≤ P (cid:63) + η
P (cid:63) − (Br0 + (cid:107)λ(cid:63)
i=1 (Bri/(1 − γ) − ci)2 and λ(cid:63) is the solution of (DI).

(cid:107)1 Br)



(22)

(23)

B
2

+ δ + ε.

The previous result establishes a bound on the number of dual iterations required to converge to a
neighborhood of the optimal solution. This bound is linear with the inverse of the desired accuracy
ε. Notice that the size of the neighborhood to which the dual descent algorithm converges depends
on the representation ability of the parametrization chosen  and the goodness of the solution of the
maximization of the Lagrangian. Since the cost of running policy gradient or actor-critic algorithms
until convergence before updating the dual variable might result in an algorithm that is computa-
tionally prohibitive  an alternative that is common in the context of optimization is to update both
variables in parallel [34]. This idea can be applied in the context of reinforcement learning as well 
where a policy gradient —or actor critic as in [7  11] —update is followed by an update of the mul-
tipliers along the direction of the constraint violation. In these algorithms the update on the policy
is on a faster scale than the update of the multipliers  and therefore they operate from a theoretical
point of view as (1). In particular  the proofs in [7  11] rely on the fact that this different time-scale
is such that allows to consider the multiplier as constant.

6 Numerical Example

In this section  we include a numerical example in order to showcase the consequences of our the-
oretical results. As an illustrative example  we consider a gridworld navigation scenario. This
scenario  illustrated in Figure 1  consists of an agent attempting to navigate from a starting position
to a goal. To do so  the agent must cross from the left side of the world to the right side using either
one of two bridges. The bridge above is deemed “unsafe”. The agents uses a softmax policy with
four possible actions (moving up  down  left  and right) over a table-lookup of states and actions.
The agent receives a reward r(s  a) = 10 for reaching the goal and a reward of r(s  a) = −1 for

8

Figure 1: Safe (blue) and unsafe
(red) optimal path. Parametrization
coarseness is on the bottom left.

Figure 2: Duality gap of the policies.

Figure 3: Effect of parametrization coarseness.

each step it wanders outside of goal. The scenario is designed such that the shortest path requires
crossing the unsafe path (red bridge)  while the safe path (blue bridge) requires a longer detour.
Using our formulation  we constrain the agent to not cross the unsafe bridge with 99% probability.
We train the agent via Algorithm 1  agent and plot in Fig. 2  the resulting normalized duality gap.
We consider two cases  an inexact primal maximization via policy gradient and  an exact primal
maximization. In order to obtain the global primal minimizer  for a given value of the dual variables
λ  the optimal primal minimizer can be easily found via Dijkstra’s algorithm. We show that by
solving Step 4 of Algorithm 1 exactly the duality gap effectively vanishes (red curve). We also
showcase a curve in which Step 4 is replaced by a single policy gradient step (blue curve). Since
the minimization in Step 4 is done approximately  the duality gap decreases at a slower rate and will
only converge to a neighborhood of zero (as per Theorem 3). In any of the two cases  ultimately  the
agent learns to navigate from start to goal by crossing the safe bridge (blue path in Fig. 1).
Now  we turn our attention to the effect of the parametrization size. We consider parametrization
of different coarseness via state aggregation  as shown in Fig. 1. This will correspond  as per
Deﬁnition 1  in parametrizations with lager values of   i.e.  looser approximators. Figure 3 displays
the effect of using coarser parametrizations  as the parametrization becomes coarser  the duality gap
increases (as per Theorem 2). Specially  for very coarse parametrizations (such as the cyan case) 
the agent cannot learn a successful policy due to the poor covering properties of its parametrization
and resultantly such problem will have a large duality gap.

7 Discussion

Throughout this work we have developed a duality theory for constrained reinforcement learning
problems. In particular we have established that for policies belonging to a general class of distri-
butions  the duality gap of this problems is null and therefore by solving the problem on the dual
domain —which always yields a ﬁnite dimensional convex problem —yields the same result as solv-
ing the original problem directly. Moreover  it establishes the equivalence between the constrained
problem and the regularized problem —or manual selection of multipliers —in the sense that both
problems track the same Pareto optimal front.
These theoretical implications however do not imply that it is always possible to solve the problem.
To be able to solve the dual problem  one is required to evaluate the dual function  which might result
intractable in several problems  for instance in cases where arbitrary policies are considered. To
overcome this limitation  we have shown that for sufﬁciently rich parametrizations the zero duality
gap result holds approximately. However  for the most part  the parametrizations considered in the
literature are not necessarily universal approximators of distributions since in general the output of
the neural network reduces to the mean —and in some cases the variance —of a distribution.
Regardless of these limitations  the primal dual algorithm considered here and those proposed in
[7  11] provide a manner to solve constrained policy optimization problems without the need to
perform an exhaustive search over the weights that we assign to each reward function  as it is the case
in [4  19  20]. Likewise  the need of imposing constraints might arise directly from the algorithm
design  this is for instance the case in Trust Region Policy Optimization [35]  where a constraint
on the divergence of the policy is included. Although our theorems do not guarantee that the zero
duality gap result holds under these constraints  since they reduce to a projection onto a convex set
it would not be surprising that it could be adapted.

9

00.20.40.60.81×10610−410−310−210−1100Iteration(k)DualityGapDualityGapDualityGap(PG)00.20.40.60.81×10510−310−210−1100Iteration(k)DualityGapd=1d=2d=3d=4d=51234510−310−210−1100ParametrizationSize(d)DualityGapReferences
[1] Richard S Sutton and Andrew G Barto  Reinforcement learning: An introduction  MIT press 

2018.

[2] Vivek S Borkar  “An actor-critic algorithm for constrained markov decision processes ” Sys-

tems & control letters  vol. 54  no. 3  pp. 207–213  2005.

[3] Joshua Achiam  David Held  Aviv Tamar  and Pieter Abbeel  “Constrained policy optimiza-
tion ” in Proceedings of the 34th International Conference on Machine Learning-Volume 70.
JMLR. org  2017  pp. 22–31.

[4] Xue Bin Peng  Pieter Abbeel  Sergey Levine  and Michiel van de Panne 

“Deepmimic:
Example-guided deep reinforcement learning of physics-based character skills ” ACM Trans-
actions on Graphics (TOG)  vol. 37  no. 4  pp. 143  2018.

[5] Shie Mannor and Nahum Shimkin  “A geometric approach to multi-criterion reinforcement

learning ” Journal of machine learning research  vol. 5  no. Apr  pp. 325–360  2004.

[6] Kristof Van Moffaert and Ann Nowé  “Multi-objective reinforcement learning using sets of
Pareto dominating policies ” The Journal of Machine Learning Research  vol. 15  no. 1  pp.
3483–3512  2014.

[7] Chen Tessler  Daniel J Mankowitz  and Shie Mannor  “Reward constrained policy optimiza-

tion ” arXiv preprint arXiv:1805.11074  2018.

[8] Jan Leike  Miljan Martic  Victoria Krakovna  Pedro A Ortega  Tom Everitt  Andrew Lefrancq 
Laurent Orseau  and Shane Legg  “Ai safety gridworlds ” arXiv preprint arXiv:1711.09883 
2017.

[9] Horia Mania  Aurelia Guy  and Benjamin Recht  “Simple random search provides a competi-

tive approach to reinforcement learning ” arXiv preprint arXiv:1803.07055  2018.

[10] Tom Schaul  Diana Borsa  Joseph Modayil  and Razvan Pascanu  “Ray interference: a source

of plateaus in deep reinforcement learning ” arXiv preprint arXiv:1904.11455  2019.

[11] Shalabh Bhatnagar and K Lakshmanan  “An online actor–critic algorithm with function ap-
proximation for constrained markov decision processes ” Journal of Optimization Theory and
Applications  vol. 153  no. 3  pp. 688–708  2012.

[12] Kurt Hornik  Maxwell Stinchcombe  and Halbert White  “Multilayer feedforward networks

are universal approximators ” Neural networks  vol. 2  no. 5  pp. 359–366  1989.
[13] Eitan Altman  Constrained Markov decision processes  vol. 7  CRC Press  1999.
[14] Iordanis Koutsopoulos and Leandros Tassiulas  “Control and optimization meet the smart
power grid: Scheduling of power demands for optimal energy management ” in Proceedings
of the 2nd International Conference on Energy-efﬁcient Computing and Networking. ACM 
2011  pp. 41–50.

[15] Chen Hou and Qianchuan Zhao  “Optimization of web service-based control system for bal-
ance between network trafﬁc and delay ” IEEE Transactions on Automation Science and En-
gineering  vol. 15  no. 3  pp. 1152–1162  2018.

[16] Yinlam Chow  Aviv Tamar  Shie Mannor  and Marco Pavone 

“Risk-sensitive and robust
decision-making: a cvar optimization approach ” in Advances in Neural Information Pro-
cessing Systems  2015  pp. 1522–1530.

[17] Shixiang Gu  Ethan Holly  Timothy Lillicrap  and Sergey Levine  “Deep reinforcement learn-
ing for robotic manipulation with asynchronous off-policy updates ” in 2017 IEEE interna-
tional conference on robotics and automation (ICRA). IEEE  2017  pp. 3389–3396.

[18] Pavlo Krokhmal  Jonas Palmquist  and Stanislav Uryasev  “Portfolio optimization with condi-

tional value-at-risk objective and constraints ” Journal of risk  vol. 4  pp. 43–68  2002.

[19] Dotan Di Castro  Aviv Tamar  and Shie Mannor  “Policy gradients with variance related risk

criteria ” arXiv preprint arXiv:1206.6404  2012.

[20] Aviv Tamar and Shie Mannor  “Variance adjusted actor critic algorithms ” arXiv preprint

arXiv:1310.3697  2013.

10

[21] Gal Dalal  Krishnamurthy Dvijotham  Matej Vecerik  Todd Hester  Cosmin Paduraru  and Yu-
val Tassa  “Safe exploration in continuous action spaces ” arXiv preprint arXiv:1801.08757 
2018.

[22] S. Boyd and L. Vandenberghe  Convex optimization  Cambridge University Press  2004.
[23] R. T. Rockafellar  Convex analysis  Princeton University Press  1970.
[24] Vivek S Borkar  “A convex analytic approach to markov decision processes ” Probability

Theory and Related Fields  vol. 78  no. 4  pp. 583–602  1988.

[25] Ken-Ichi Funahashi  “On the approximate realization of continuous mappings by neural net-

works ” Neural networks  vol. 2  no. 3  pp. 183–192  1989.

[26] George Cybenko  “Approximation by superpositions of a sigmoidal function ” Mathematics

of control  signals and systems  vol. 2  no. 4  pp. 303–314  1989.

[27] Zhou Lu  Hongming Pu  Feicheng Wang  Zhiqiang Hu  and Liwei Wang  “The expressive
power of neural networks: A view from the width ” in Advances in Neural Information Pro-
cessing Systems  2017  pp. 6231–6239.

[28] Hongzhou Lin and Stefanie Jegelka  “Resnet with one-neuron hidden layers is a universal
approximator ” in Advances in Neural Information Processing Systems  2018  pp. 6169–6178.
[29] Jooyoung Park and Irwin W Sandberg  “Universal approximation using radial-basis-function

networks ” Neural computation  vol. 3  no. 2  pp. 246–257  1991.

[30] Bharath Sriperumbudur  Kenji Fukumizu  and Gert Lanckriet  “On the relation between uni-
versality  characteristic kernels and rkhs embedding of measures ” in Proceedings of the Thir-
teenth International Conference on Artiﬁcial Intelligence and Statistics  2010  pp. 773–780.

[31] Dimitri P Bertsekas and Athena Scientiﬁc  Convex optimization algorithms  Athena Scientiﬁc

Belmont  2015.

[32] Richard S Sutton  David A McAllester  Satinder P Singh  and Yishay Mansour  “Policy gradi-
ent methods for reinforcement learning with function approximation ” in Advances in neural
information processing systems  2000  pp. 1057–1063.

[33] Vijay R Konda and John N Tsitsiklis  “Actor-critic algorithms ” in Advances in neural infor-

mation processing systems  2000  pp. 1008–1014.

[34] Kenneth J. Arrow and Leonard Hurwicz  Studies in linear and nonlinear programming  Stan-

ford University Press  CA  1958.

[35] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz  “Trust
region policy optimization ” in International Conference on Machine Learning  2015  pp.
1889–1897.

11

,Santiago Paternain
Luiz Chamon
Miguel Calvo-Fullana
Alejandro Ribeiro