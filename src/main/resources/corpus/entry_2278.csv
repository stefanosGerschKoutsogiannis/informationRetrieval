2011,Generalizing from Several Related Classification Tasks to a New Unlabeled Sample,We consider the problem of assigning class labels to an unlabeled test  data set  given several labeled training data sets drawn from similar  distributions. This problem arises in several applications where data  distributions fluctuate because of biological  technical  or other sources  of variation. We develop a distribution-free  kernel-based approach to the  problem. This approach involves identifying an appropriate reproducing     kernel Hilbert space and optimizing a regularized empirical risk over the  space.  We present generalization error analysis  describe universal  kernels  and establish universal consistency of the proposed methodology.  Experimental results on flow cytometry data are presented.,Generalizing from Several Related Classiﬁcation

Tasks to a New Unlabeled Sample

Gilles Blanchard
Universit¨at Potsdam

blanchard@math.uni-potsdam.de

Gyemin Lee  Clayton Scott

University of Michigan

{gyemin clayscot}@umich.edu

Abstract

We consider the problem of assigning class labels to an unlabeled test data set 
given several labeled training data sets drawn from similar distributions. This
problem arises in several applications where data distributions ﬂuctuate because
of biological  technical  or other sources of variation. We develop a distribution-
free  kernel-based approach to the problem. This approach involves identifying
an appropriate reproducing kernel Hilbert space and optimizing a regularized em-
pirical risk over the space. We present generalization error analysis  describe uni-
versal kernels  and establish universal consistency of the proposed methodology.
Experimental results on ﬂow cytometry data are presented.

1

Introduction

Is it possible to leverage the solution of one classiﬁcation problem to solve another? This is a ques-
tion that has received increasing attention in recent years from the machine learning community  and
has been studied in a variety of settings  including multi-task learning  covariate shift  and transfer
learning. In this work we study a new setting for this question  one that incorporates elements of the
three aforementioned settings  and is motivated by many practical applications.
To state the problem  let X be a feature space and Y a space of labels to predict; to simplify the
exposition  we will assume the setting of binary classiﬁcation  Y = {−1  1}   although the method-
ology and results presented here are valid for general output spaces. For a given distribution PXY  
we refer to the X marginal distribution PX as simply the marginal distribution  and the conditional
PXY (Y |X) as the posterior distribution.
XY on X × Y  i = 1  . . .   N. For each i  there is a
There are N similar but distinct distributions P (i)
training sample Si = (Xij  Yij)1≤j≤ni of iid realizations of P (i)
XY . There is also a test distribution
XY that is similar to but again distinct from the “training distributions” P (i)
XY . Finally  there is a test
P T
sample (X T
XY   but in this case the labels Yj are not observed.
The goal is to correctly predict these unobserved labels. Essentially  given a random sample from
the marginal test distribution P T
X  we would like to predict the corresponding labels. Thus  when we
say that the training and test distributions are “similar ” we mean that there is some pattern making
it possible to learn a mapping from marginal distributions to labels. We will refer to this learning
problem as learning marginal predictors. A concrete motivating application is given below.
This problem may be contrasted with other learning problems.
In multi-task learning  only the
training distributions are of interest  and the goal is to use the similarity among distributions to
improve the training of individual classiﬁers [1  2  3]. In our context  we view these distributions
as “training tasks ” and seek to generalize to a new distribution/task. In the covariate shift problem 
the marginal test distribution is different from the marginal training distribution(s)  but the posterior

j )1≤j≤nT of iid realizations of P T

j   Y T

1

distribution is assumed to be the same [4]. In our case  both marginal and posterior test distributions
can differ from their training counterparts [5].
Finally  in transfer learning  it is typically assumed that at least a few labels are available for the
test data  and the training data sets are used to improve the performance of a standard classiﬁer  for
example by learning a metric or embedding which is appropriate for all data sets [6  7]. In our case 
no test labels are available  but we hope that through access to multiple training data sets  it is still
possible to obtain collective knowledge about the “labeling process” that may be transferred to the
test distribution. Some authors have considered transductive transfer learning  which is similar to
the problem studied here in that no test labels are available. However  existing work has focused on
the case N = 1 and typically relies on the covariate shift assumption [8].
We propose a distribution-free  kernel-based approach to the problem of learning marginal predic-
tors. Our methodology is shown to yield a consistent learning procedure  meaning that the general-
ization error tends to the best possible as the sample sizes N {ni}  nT tend to inﬁnity. We also offer
a proof-of-concept experimental study validating the proposed approach on ﬂow cytometry data 
including comparisons to multi-task kernels and a simple pooling approach.

2 Motivating Application: Automatic Gating of Flow Cytometry Data

j ∈ {−1  1} associated to each cell  where Y T

Flow cytometry is a high-throughput measurement platform that is an important clinical tool for the
diagnosis of many blood-related pathologies. This technology allows for quantitative analysis of
individual cells from a given population  derived for example from a blood sample from a patient.
We may think of a ﬂow cytometry data set as a set of d-dimensional attribute vectors (Xj)1≤j≤n 
where n is the number of cells analyzed  and d is the number of attributes recorded per cell. These
attributes pertain to various physical and chemical properties of the cell. Thus  a ﬂow cytometry
data set is a random sample from a patient-speciﬁc distribution.
Now suppose a pathologist needs to analyze a new (“test”) patient with data (X T
j )1≤j≤nT . Before
proceeding  the pathologist ﬁrst needs the data set to be “puriﬁed” so that only cells of a certain
type are present. For example  lymphocytes are known to be relevant for the diagnosis of leukemia 
whereas non-lymphocytes may potentially confound the analysis. In other words  it is necessary to
j = 1 indicates that the j-th cell
determine the label Y T
is of the desired type.
In clinical practice this is accomplished through a manual process known as “gating.” The data are
visualized through a sequence of two-dimensional scatter plots  where at each stage a line segment
or polygon is manually drawn to eliminate a portion of the unwanted cells. Because of the variability
in ﬂow cytometry data  this process is difﬁcult to quantify in terms of a small subset of simple rules.
Instead  it requires domain-speciﬁc knowledge and iterative reﬁnement. Modern clinical laboratories
routinely see dozens of cases per day  so it would be desirable to automate this process.
Since clinical laboratories maintain historical databases  we can assume access to a number (N) of
historical patients that have already been expert-gated. Because of biological and technical varia-
tions in ﬂow cytometry data  the distributions P (i)
XY of the historical patients will vary. For example 
Fig. 1 shows exemplary two-dimensional scatter plots for two different patients  where the shaded
cells correspond to lymphocytes. Nonetheless  there are certain general trends that are known to
hold for all ﬂow cytometry measurements. For example  lymphocytes are known to exhibit low
levels of the “side-scatter” (SS) attribute  while expressing high levels of the attribute CD45 (see
column 2 of Fig. 1). More generally  virtually every cell type of interest has a known tendency
(e.g.  high or low) for most measured attributes. Therefore  it is reasonable to assume that there is an
underlying distribution (on distributions) governing ﬂow cytometry data sets  that produces roughly
similar distributions thereby making possible the automation of the gating process.

3 Formal Setting
Let X denote the observation space and Y = {−1  1} the output space. Let PX×Y denote the set
of probability distributions on X × Y  PX the set of probability distributions on X   and PY|X the
set of conditional probabilities of Y given X (also known as Markov transition kernels from X to

2

Figure 1: Two-dimensional projections of multi-dimensional ﬂow cytometry data. Each row corre-
sponds to a single patient. The distribution of cells differs from patient to patient. Lymphocytes  a
type of white blood cell  are marked dark (blue) and others are marked bright (green). These were
manually selected by a domain expert.

Y ) which we also call “posteriors” in this work. The disintegration theorem (see for instance [9] 
Theorem 6.4) tells us that (under suitable regularity properties  e.g.  X is a Polish space) any element
PXY ∈ PX×Y can be written as a product PXY = PX•PY |X  with PX ∈ PX   PY |X ∈ PY |X. The
space PX×Y is endowed with the topology of weak convergence and the associated Borel σ-algebra.
It is assumed that there exists a distribution µ on PX×Y  where P (1)
XY are i.i.d. realizations
from µ  and the sample Si is made of ni i.i.d. realizations of (X  Y ) following the distribution P (i)
XY .
j )1≤j≤nT   whose labels are
Now consider a test distribution P T

not observed. A decision function is a function f : PX × X (cid:55)→ R that predicts (cid:98)Yi = f((cid:98)PX   Xi) 
where (cid:98)PX is the associated empirical X distribution. If (cid:96) : R × Y (cid:55)→ R+ is a loss  then the average

XY and test sample ST = (X T

XY   . . .   P (N )

j   Y T

loss incurred on the test sample is
generalization error of a decision function over test samples of size nT  

  Y T

1
nT

i ) . Based on this  we deﬁne the average

(cid:80)nT
i=1 (cid:96)((cid:98)Y T

i

E(f  nT ) := E

XY ∼µ
P T

E
ST ∼(P T

XY )⊗nT

X   X T

i )  Y T
i )

.

(1)

(cid:34)

1
nT

nT(cid:88)

i=1

(cid:96)(f((cid:98)P T

(cid:35)

In important point of the analysis is that  at training time as well as at test time  the marginal dis-
tribution PX for a sample is only known through the sample itself  that is  through the empirical

marginal (cid:98)PX. As is clear from equation (1)  because of this the generalization error also depends on
the test sample size nT . As nT grows  (cid:98)P T

X. This motivates the following gen-
eralization error when we have an inﬁnite test sample  where we then assume that the true marginal
X is observed:
P T

X will converge to P T

(cid:2)(cid:96)(f(P T

X   X T )  Y T )(cid:3) .

(2)

E(f ∞) := E

XY ∼µ
P T

E
(X T  Y T )∼P T

XY

To gain some insight into this risk  let us decompose µ into two parts  µX which generates the
marginal distribution PX  and µY |X which  conditioned on PX  generates the posterior PY |X. De-
note ˜X = (PX   X). We then have
E(f ∞) = EPX∼µX
= EPX∼µX
= E

EX∼PX
EPY |X∼µY |X

EPY |X∼µY |X
EX∼PX

EY |X∼PY |X
EY |X∼PY |X

(cid:96)(f( ˜X)  Y )
(cid:96)(f( ˜X)  Y )

(cid:96)(f( ˜X)  Y )

(cid:104)
(cid:104)

(cid:105)
(cid:105)

(cid:105)

(cid:104)

.

( ˜X Y )∼Qµ

Here Qµ is the distribution that generates ˜X by ﬁrst drawing PX according to µX  and then drawing
X according to PX. Similarly  Y is generated  conditioned on ˜X  by ﬁrst drawing PY |X according
to µY |X  and then drawing Y from PY |X. From this last expression  we see that the risk is like a
standard binary classiﬁcation risk based on ( ˜X  Y ) ∼ Qµ. Thus  we can deduce several properties

3

that are known to hold for binary classiﬁcation risks. For example  if the loss is the 0/1 loss  then
f∗( ˜X) = 2˜η( ˜X) − 1 is an optimal predictor  where ˜η( ˜X) = EY ∼Qµ

(cid:2)1{Y =1}(cid:3). More generally 

E(f ∞) − E(f∗ ∞) = E ˜X∼Qµ

˜X

(cid:104)

1{sign(f ( ˜X))(cid:54)=sign(f∗( ˜X))}|2˜η( ˜X) − 1|(cid:105)

Y | ˜X

.

Our goal is a learning rule that asymptotically predicts as well as the global minimizer of (2)  for
a general loss (cid:96). By the above observations  consistency with respect to a general (cid:96) (thought of
as a surrogate) will imply consistency for the 0/1 loss  provided (cid:96) is classiﬁcation calibrated [10].
Despite the similarity to standard binary classiﬁcation in the inﬁnite sample case  we emphasize that

the learning task here is different  because the realizations ((cid:101)Xij  Yij) are neither independent nor

identically distributed.
XY   the classiﬁer
Finally  we note that there is a condition where for µ-almost all test distribution P T
f∗(P T
X   .) (where f∗ is the global minimizer of (2)) coincides with the optimal Bayes classiﬁer for
XY   although no labels from this test distribution are observed. This condition is simply that the
P T
posterior PY |X is (µ-almost surely) a function of PX. In other words  with the notation introduced
above  µY |X(PX) is a Dirac delta for µ-almost all PX. Although we will not be assuming this
condition throughout the paper  it is implicitly assumed in the motivating application presented in
Section 2  where an expert labels the data points by just looking at their marginal distribution.
Lemma 3.1. For a ﬁxed distribution PXY   and a decision function f : X → R  let us denote
R(f  PXY ) = E(X Y )∼PXY [(cid:96)(f(X)  Y )] and

R∗(PXY ) := min

f :X→RR(f  PXY ) = min
f :X→R

E(X Y )∼PXY [(cid:96)(f(X)  Y )]

the corresponding optimal (Bayes) risk for the loss function (cid:96). Assume that µ is a distribution on
PX×Y such that µ-a.s. it holds PY |X = F (PX) for some deterministic mapping F . Let f∗ be a
minimizer of the risk (2). Then we have for µ-almost all PXY :

and

R(f∗(PX   .)  PXY ) = R∗(PXY )

E(f∗ ∞) = EPXY ∼µ [R∗(PXY )] .

Proof. Straightforward. Obviously for any f : PX × X → R  one has for all PXY :
R(f(PX   .)  PXY ) ≥ R∗(PXY ). For any ﬁxed PX ∈ PX   consider PXY := PX • F (PX) and
g∗(PX) a Bayes classiﬁer for this joint distribution. Pose f(PX   x) := g∗(PX)(x). Then f coin-
cides for µ-almost PXY with a Bayes classiﬁer for PXY   achieving equality in the above inequality.
The second equality follows by taking expectation over PXY ∼ µ.

4 Learning Algorithm

We consider an approach based on positive semi-deﬁnite kernels  or simply kernels for short. Back-
ground information on kernels  including the deﬁnition  normalized kernels  universal kernels  and
reproducing kernel Hilbert spaces (RKHSs)  may be found in [11]. Several well-known learning
algorithms  such as support vector machines and kernel ridge regression  may be viewed as min-
imizers of a norm-regularized empirical risk over the RKHS of a kernel. A similar development
also exists for multi-task learning [3]. Inspired by this framework  we consider a general kernel
algorithm as follows.
associated RKHS. For the sample Si let (cid:98)P (i)
Consider the loss function (cid:96) : R × Y → R+. Let k be a kernel on PX × X   and let Hk be the
Xijs. Also consider the extended input space PX × X and the extended data (cid:101)Xij = ((cid:98)P (i)
X denote the corresponding empirical distribution of the
Note that (cid:98)P (i)
X   Xij).
N(cid:88)

X plays a role similar to the task index in multi-task learning. Now deﬁne

(cid:96)(f((cid:101)Xij)  Yij) + λ(cid:107)f(cid:107)2 .

(cid:98)fλ = arg min

(3)

1
N

f∈Hk

1
ni

i=1

ni(cid:88)

j=1

4

For the hinge loss  by the representer theorem [12] this optimization problem reduces to a quadratic
program equivalent to the dual of a kind of cost-sensitive SVM  and therefore can be solved using
existing software packages. The ﬁnal predictor has the form

(cid:98)fλ((cid:98)PX   x) =

N(cid:88)

ni(cid:88)

αijYijk(((cid:98)P (i)

X   Xij)  ((cid:98)PX   x))

where the αij are nonnegative and mostly zero. See [11] for details.
In the rest of the paper we will consider a kernel k on PX × X of the product form

i=1

j=1

k((P1  x1)  (P2  x2)) = kP (P1  P2)kX(x1  x2) 

(4)
where kP is a kernel on PX and kX a kernel on X . Furthermore  we will consider kernels on
X denote a kernel on X (which might be different from kX) that is
PX of a particular form. Let k(cid:48)
measurable and bounded. We deﬁne the following mapping Ψ : PX → Hk(cid:48)

:

X

PX (cid:55)→ Ψ(PX) :=

X(x ·)dPX(x).
k(cid:48)

(5)

(cid:90)

X

This mapping has been studied in the framework of “characteristic kernels” [13]  and it has been
proved that there are important links between universality of k(cid:48)
Note that the mapping Ψ is linear. Therefore 
(cid:104)Ψ(PX)  Ψ(P (cid:48)
we introduce yet another kernel K on Hk(cid:48)
kP (PX   P (cid:48)

X) =
X)(cid:105)  it is a linear kernel on PX and cannot be a universal kernel. For this reason 

and consider the kernel on PX given by
X) = K (Ψ(PX)  Ψ(P (cid:48)

X and injectivity of Ψ [14  15].
if we consider the kernel kP (PX   P (cid:48)

X)) .

(6)

X

Note that particular kernels inspired by the ﬁnite dimensional case are of the form

K(v  v(cid:48)) = F ((cid:107)v − v(cid:48)(cid:107)) 

(7)

or

K(v  v(cid:48)) = G((cid:104)v  v(cid:48)(cid:105)) 

(8)
where F  G are real functions of a real variable such that they deﬁne a kernel. For example  F (t) =
exp(−t2/(2σ2)) yields a Gaussian-like kernel  while G(t) = (1 + t)d yields a polynomial-like
kernel. Kernels of the above form on the space of probability distributions over a compact space X
have been introduced and studied in [16]. Below we apply their results to deduce that k is a universal
kernel for certain choices of kX   k(cid:48)

X  and K.

5 Learning Theoretic Study

Although the regularized estimation formula (3) deﬁning (cid:98)fλ is standard  the generalization error
analysis is not  since the (cid:101)Xij are neither identically distributed nor independent. We begin with a
generalization error bound that establishes uniform estimation error control over functions belonging
to a ball of Hk . We then discuss universal kernels  and ﬁnally deduce universal consistency of the
algorithm. To simplify somewhat the analysis  we assume below that all training samples have the
same size ni = n. Also let Bk(r) denote the closed ball of radius r  centered at the origin  in the
RKHS of the kernel k. We consider the following assumptions on the loss and kernels:
(Loss) The loss function (cid:96) : R × Y → R+ is L(cid:96)-Lipschitz in its ﬁrst variable and bounded by B(cid:96).
k(cid:48) ≥ 1  and
(Kernels-A) The kernels kX   k(cid:48)
→ HK associated to K satisﬁes a

X and K are bounded respectively by constants B2

K . In addition  the canonical feature map ΦK : Hk(cid:48)
B2
H¨older condition of order α ∈ (0  1] with constant LK  on Bk(cid:48)

(Bk(cid:48)) :

k  B2

X

∀v  w ∈ Bk(cid:48)

X

(Bk(cid:48)) :

(cid:107)ΦK(v) − ΦK(w)(cid:107) ≤ LK (cid:107)v − w(cid:107)α .

X

(9)

Sufﬁcient conditions for (9) are described in [11]. As an example  the condition is shown to hold
with α = 1 when K is the Gaussian-like kernel on Hk(cid:48)
. The boundedness assumptions are also
clearly satisﬁed for Gaussian kernels.

X

5

XY   . . .   P (N )

Theorem 5.1 (Uniform estimation error control). Assume conditions (Loss) and (Kernels-A) hold.
If P (1)
XY are i.i.d. realizations from µ  and for each i = 1  . . .   N  the sample Si =
(Xij  Yij)1≤j≤n is made of i.i.d. realizations from P (i)
XY   then for any R > 0  with probability at
least 1 − δ:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

N

sup

f∈Bk(R)

n(cid:88)

j=1

1
n

N(cid:88)
(cid:32)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:96)(f((cid:101)Xij)  Yij) − E(f ∞)
(cid:32)
(cid:18)log N + log δ−1

≤ c

RBkL(cid:96)

Bk(cid:48)LK

n

(cid:19) α

2

(cid:33)

(cid:114)log δ−1

(cid:33)

N

+ B(cid:96)

 

(10)

+ BK

1√
N

where c is a numerical constant  and Bk(R) denotes the ball of radius R of Hk .

Proof sketch. The full proofs of this and other results are given in [11]. We give here a brief
overview. We use the decomposition

Bounding (I)  using the Lipschitz property of the loss function  can be reduced to controlling

N

sup

1
ni

i=1

j=1

f∈Bk(R)

N(cid:88)

ni(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

≤ sup
f∈Bk(R)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:96)(f((cid:101)Xij)  Yij) − E(f ∞)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:16)
ni(cid:88)
N(cid:88)
(cid:96)(f((cid:98)P (i)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
ni(cid:88)
N(cid:88)
(cid:13)(cid:13)(cid:13)f((cid:98)P (i)
of the kernel k  the convergence of Ψ((cid:98)P (i)

X   .) − f(P (i)

conditional to P (i)

f∈Bk(R)

+ sup

1
ni

i=1

j=1

i=1

j=1

1
ni

N

N

X   .)

(cid:13)(cid:13)(cid:13)∞  

X   Xij)  Yij) − (cid:96)(f(P (i)

(cid:96)(f(P (i)

X   Xij)  Yij) − E(f ∞)

X   Xij)  Yij)

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =: (I) + (II).

X   uniformly for i = 1  . . .   N. This can be obtained using the reproducing property
X ) as a consequence of Hoeffding’s inequality

X ) to Ψ(P (i)

X )  and the convergence of the conditional generalization error.

in a Hilbert space  and the other assumptions (boundedness/H¨older property) on the kernels.
Concerning the control of the term (II)  it can be decomposed in turn into the convergence con-
ditional to (P (i)
In both cases  a
standard approach using the Azuma-McDiarmid’s inequality [17] followed by symmetrization and
Rademacher complexity analysis on a kernel space [18  19] can be applied. For the ﬁrst part  the
random variables are the (Xij  Yij) (which are independent conditional to (P (i)
X )); for the second
part  the i.i.d. variables are the (P (i)

X ) (the (Xij  Yij) being integrated out).
To establish that k is universal on PX × X   the following lemma is useful.
Lemma 5.2. Let Ω  Ω(cid:48) be two compact spaces and k  k(cid:48) be kernels on Ω  Ω(cid:48)  respectively. If k  k(cid:48)
are both universal  then the product kernel

is universal on Ω × Ω(cid:48).

k((x  x(cid:48))  (y  y(cid:48))) := k(x  y)k(cid:48)(x(cid:48)  y(cid:48))

Several examples of universal kernels are known on Euclidean space. We also need universal kernels
on PX . Fortunately  this was recently investigated [16]. Some additional assumptions on the kernels
and feature space are required:
(Kernels-B) kX  k(cid:48)

X  K  and X satisfy the following: X is a compact metric space; kX is universal
X is continuous and universal on X ; K is universal on any compact subset of Hk(cid:48)
.

on X ; k(cid:48)

X

6

Adapting the results of [16]  we have the following.
Theorem 5.3 (Universal kernel). Assume condition (Kernels-B) holds. Then  for kP deﬁned as in
(6)  the product kernel k in (4) is universal on PX ×X . Furthermore  the assumption on K is fulﬁlled
if K is of the form (8)  where G is an analytical function with positive Taylor series coefﬁcients  or if
K is the normalized kernel associated to such a kernel.
As an example  suppose that X is a compact subset of Rd. Let kX and k(cid:48)
X be Gaussian kernels on
X)(cid:105)Hk(cid:48)
X . Taking G(t) = exp(t)  it follows that K(PX   P (cid:48)
X) = exp((cid:104)Ψ(PX)  Ψ(P (cid:48)
) is universal on
PX . By similar reasoning as in the ﬁnite dimensional case  the Gaussian-like kernel K(PX   P (cid:48)
X) =
exp(− 1
) is also universal on PX . Thus the product kernel is universal.
Corollary 5.4 (Universal consistency). Assume the conditions (Loss)  (Kernels-A)  and (Kernels-
B) are satisﬁed. Assume that N  n grow to inﬁnity in such a way that N = O(nγ) for some γ > 0.
Then  if λj is a sequence such that λj → 0 and λj

(cid:113) j
log j → ∞  it holds that
E((cid:98)fλmin(N nα)  ∞) → inf
f :PX ×X→RE(f ∞)

2σ2(cid:107)Ψ(PX) − Ψ(P (cid:48)

X)(cid:107)2Hk(cid:48)

X

X

in probability.

6 Experiments

Train
1.41
1.59
1.34
1.32

kP
Pooling (τ = 1)
MTL (τ = 0.01)
MTL (τ = 0.5)
Proposed

We demonstrate the proposed methodology for ﬂow cytometry data auto-gating  described above.
Peripheral blood samples were obtained from 35 normal patients  and lymphocytes were classiﬁed
by a domain expert. The corresponding ﬂow cytometry data sets have sample sizes ranging from
10 000 to 100 000  and the proportion of lymphocytes in each data set ranges from 10 to 40%. We
took N = 10 of these data sets for training  and the remaining 25 for testing. To speed training time 
we subsampled the 10 training data sets to have 1000 data points (cells) each. Adopting the hinge
loss  we used the SVMlight [20] package to solve the quadratic program characterizing the solution.
The kernels kX  k(cid:48)
X  and K are all taken to be Gaus-
sian kernels with respective bandwidths σX  σ(cid:48)
X  and
X equals 10 times the average
σ. We set σX such that σ2
distance of a data point to its nearest neighbor within
the same data set. The second bandwidth was deﬁned
similarly  while the third was set to 1. The regulariza-
tion parameter λ was set to 1.
For comparison  we also considered three other
options for kP .
These kernels have the form
kP (P1  P2) = 1 if P1 = P2  and kP (P1  P2) = τ
otherwise. When τ = 1  the method is equivalent to
pooling all of the training data together in one data set 
and learning a single SVM classiﬁer. This idea has
been previously studied in the context of ﬂow cytome-
try by [21]. When 0 < τ < 1  we obtain a kernel like what was used for multi-task learning (MTL)
by [3]. Note that these kernels have the property that if P1 is a training data set  and P2 a test data set 
then P1 (cid:54)= P2 and so kP (P1  P2) is simply a constant. This implies that the learning rules produced
by these kernels do not adapt to the test distribution  unlike the proposed kernel. In the experiments 
we take τ = 1 (pooling)  0.01  and 0.5 (MTL).
The results are shown in Fig. 2 and summarized in Table 1. The middle column of the table reports
the average misclassiﬁcation rate on the training data sets. Here we used those data points that
were not part of the 1000-element subsample used for training. The right column shows the average
misclassiﬁcation rate on the test data sets.

Table 1: The misclassiﬁcation rates (%) on
training data sets and test data sets for dif-
ferent kP . The proposed method adapts the
decision function to the test data (through
the marginal-dependent kernel)  account-
ing for its improved performance.

Test
2.32
2.64
2.36
2.29

7 Discussion
Our approach to learning marginal predictors relies on the extended input pattern ˜X = (PX   X).
Thus  we study the natural algorithm of minimizing a regularized empirical loss over a reproducing

7

Figure 2: The misclassiﬁcation rates (%) on training data sets and test data sets for different kP . The
last 25 data sets separated by dotted line are not used during training.

kernel Hilbert space associated with the extended input domain PX ×X . We also establish universal
consistency  using a novel generalization error analysis under the inherent non-iid sampling plan 
and a construction of a universal kernel on PX × X . For the hinge loss  the algorithm may be
implemented using standard techniques for SVMs. The algorithm is applied to ﬂow cytometry auto-
gating  and shown to improve upon kernels that do not adapt to the test distribution.
Several future directions exist. From an application perspective  the need for adaptive classiﬁers
arises in many applications  especially in biomedical applications involving biological and/or tech-
nical variation in patient data. For example  when electrocardiograms are used to monitor cardiac
patients  it is desirable to classify each heartbeat as irregular or not. However  irregularities in a test
patient’s heartbeat will differ from irregularities of historical patients  hence the need to adapt to the
test distribution [22].
We can also ask how the methodology and analysis can be extended to the context where a small
number of labels are available for the test distribution  as is commonly assumed in transfer learning.
In this setting  two approaches are possible. The simplest one is to use the same optimization prob-
lem (3)  wherein we include additionally the labeled examples of the test distribution. However  if
several test samples are to be treated in succession  and we want to avoid a full  resource-consuming
re-training using all the training samples each time  an interesting alternative is the following: learn
once a function f0(PX   x) using the available training samples via (3); then  given a partially labeled
test sample  learn a decision function on this sample only via the usual kernel norm regularized em-
pirical loss minimization method  but replace the usual regularizer term (cid:107)f(cid:107)2 by (cid:107)f − f0(Px  .)(cid:107)2
(note that f0(Px  .) ∈ Hk). In this sense  the marginal-adaptive decision function learned from the
training samples would serve as a “prior” for learning on the test data.
It would also be of interest to extend the proposed methodology to a multi-class setting. In this case 
the problem has an interesting interpretation in terms of “learning to cluster.” Each training task may
be viewed as a data set that has been clustered by a teacher. Generalization then entails the ability
to learn the clustering process  so that clusters may be assigned to a new unlabeled data set.
Future work may consider other asymptotic regimes  e.g.  where {ni}  nT do not tend to inﬁnity 
or they tend to inﬁnity much slower than N. It may also be of interest to develop implementations
for differentiable losses such as the logistic loss  allowing for estimation of posterior probabilities.
Finally  we would like to specify conditions on µ  the distribution-generating distribution  that are
favorable for generalization (beyond the simple condition discussed in Lemma 3.1).

Acknowledgments

G. Blanchard was supported by the European Community’s 7th Framework Programme under
the PASCAL2 Network of Excellence (ICT-216886) and under the E.U. grant agreement 247022
(MASH Project). G. Lee and C. Scott were supported in part by NSF Grant No. 0953135.

8

References
[1] S. Thrun  “Is learning the n-th thing any easier than learning the ﬁrst? ” Advances in Neural

Information Processing Systems  pp. 640–646  1996.

[2] R. Caruana  “Multitask learning ” Machine Learning  vol. 28  pp. 41–75  1997.
[3] T. Evgeniou and M. Pontil  “Learning multiple tasks with kernel methods ” J. Machine Learn-

ing Research  pp. 615–637  2005.

[4] S. Bickel  M. Br¨uckner  and T. Scheffer  “Discriminative learning under covariate shift ” J.

Machine Learning Research  pp. 2137–2155  2009.

[5] J. Quionero-Candela  M. Sugiyama  A. Schwaighofer  and N. D. Lawrence  Dataset Shift in

Machine Learning  The MIT Press  2009.

[6] R. K. Ando and T. Zhang  “A high-performance semi-supervised learning method for text
chunking ” Proceedings of the 43rd Annual Meeting on Association for Computational Lin-
guistics (ACL 05)  pp. 1–9  2005.

[7] A. Rettinger  M. Zinkevich  and M. Bowling  “Boosting expert ensembles for rapid concept
recall ” Proceedings of the 21st National Conference on Artiﬁcial Intelligence (AAAI 06)  vol.
1  pp. 464–469  2006.

[8] A. Arnold  R. Nallapati  and W.W. Cohen  “A comparative study of methods for transductive
transfer learning ” Seventh IEEE International Conference on Data Mining Workshops  pp.
77–82  2007.

[9] O. Kallenberg  Foundations of Modern Probability  Springer  2002.
[10] P. Bartlett  M. Jordan  and J. McAuliffe  “Convexity  classiﬁcation  and risk bounds ” J. Amer.

Stat. Assoc.  vol. 101  no. 473  pp. 138–156  2006.

[11] G. Blanchard  G. Lee  and C. Scott  “Supplemental material ” NIPS 2011.
[12] I. Steinwart and A. Christmann  Support Vector Machines  Springer  2008.
[13] A. Gretton  K. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. Smola  “A kernel approach to com-
paring distributions ” in Proceedings of the 22nd AAAI Conference on Artiﬁcial Intelligence 
R. Holte and A. Howe  Eds.  2007  pp. 1637–1641.

[14] A. Gretton  K. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. Smola 

“A kernel method
for the two-sample-problem ” in Advances in Neural Information Processing Systems 19 
B. Sch¨olkopf  J. Platt  and T. Hoffman  Eds.  2007  pp. 513–520.

[15] B. Sriperumbudur  A. Gretton  K. Fukumizu  B. Sch¨olkopf  and G. Lanckriet  “Hilbert space
embeddings and metrics on probability measures ” Journal of Machine Learning Research 
vol. 11  pp. 1517–1561  2010.

[16] A. Christmann and I. Steinwart  “Universal kernels on non-standard input spaces ” in Advances
in Neural Information Processing Systems 23  J. Lafferty  C. K. I. Williams  J. Shawe-Taylor 
R. Zemel  and A. Culotta  Eds.  2010  pp. 406–414.

[17] C. McDiarmid  “On the method of bounded differences ” Surveys in Combinatorics  vol. 141 

pp. 148–188  1989.

[18] V. Koltchinskii  “Rademacher penalties and structural risk minimization ” IEEE Transactions

on Information Theory  vol. 47  no. 5  pp. 1902 – 1914  2001.

[19] P. Bartlett and S. Mendelson  “Rademacher and Gaussian complexities: Risk bounds and

structural results ” Journal of Machine Learning Research  vol. 3  pp. 463–482  2002.

[20] T. Joachims  “Making large-scale SVM learning practical ” in Advances in Kernel Methods -
Support Vector Learning  B. Sch¨olkopf  C. Burges  and A. Smola  Eds.  chapter 11  pp. 169–
184. MIT Press  Cambridge  MA  1999.

[21] J. Toedling  P. Rhein  R. Ratei  L. Karawajew  and R. Spang  “Automated in-silico detection of
cell populations in ﬂow cytometry readouts and its application to leukemia disease monitoring ”
BMC Bioinformatics  vol. 7  pp. 282  2006.

[22] J. Wiens  Machine Learning for Patient-Adaptive Ectopic Beat Classication  Masters The-
sis  Department of Electrical Engineering and Computer Science  Massachusetts Institute of
Technology  2010.

9

,Haim Avron
Huy Nguyen
David Woodruff
Kacper Chwialkowski
Aaditya Ramdas
Dino Sejdinovic
Arthur Gretton