2011,Orthogonal Matching Pursuit with Replacement,In this paper  we consider the problem of compressed sensing where the goal is to recover almost all the sparse vectors using a small number of fixed linear measurements. For this problem  we propose a novel partial hard-thresholding operator leading to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP  the other end of the spectrum leads to a novel algorithm that we call  Orthogonal Matching Pursuit with Replacement (OMPR). OMPR  like the classic greedy algorithm OMP  adds exactly one coordinate to the support at each iteration  based on the correlation with the current residual. However  unlike OMP  OMPR also removes one coordinate from the support. This simple change allows us to prove the best known guarantees for OMPR in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast  OMP is known to have very weak performance guarantees under RIP.  We also extend OMPR using locality sensitive hashing to get OMPR-Hash  the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our  proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursuit.  We provide experimental results on large problems providing  recovery for vectors of size up to million dimensions.  We demonstrate that for large-scale problems our proposed methods are more robust and faster than the existing  methods.,Orthogonal Matching Pursuit with Replacement 

Prateek Jain 

Microsoft Research India 

Bangalore  INDIA 

AmbujTewari 

Austin  TX 

prajain@microsoft.com 

ambuj@cs.utexas.edu 

inderjit@cs.utexas.edu 

The University of Texas at Austin 

The University of Texas at Austin 

Inderjit S. Dhillon 

Austin  TX 

Abstract 

In this paper  we consider the problem of compressed sensing where the goal is to recover all sparse 
vectors using a small number offixed linear measurements.  For this problem  we propose a novel 
partial hard-thresholding operator that leads to a general family of iterative algorithms.  While one 
extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17  10]  the 
other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with 
Replacement (OMPR). OMPR  like the classic greedy algorithm OMP  adds exactly one coordinate 
to the support at each iteration  based on the correlation with the current residnal.  However  unlike 
OMP  OMPR also removes one coordinate from the support. This simple change allows us to prove 
that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry 
Property (a condition on the measurement matrix).  In contrast  OMP is known to have very weak 
performance guarantees under RIP.  Given its simple structore  we are able to extend OMPR using 
locality sensitive hashing to  get OMPR-Hasb  the first provably sub-linear (in dimensionality) al(cid:173)
gorithm for sparse recovery.  Our proof techniques are novel and flexible enough to also permit the 
tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit.  We 
provide experimental results on large problems providing recovery for vectors of size up to million 
dimensions.  We demonstrste that for large-scale problems our proposed methods are more robust 
and faster than existing methods. 

1  Introduction 
We nowadays routinely face high-dimensional datasets in diverse application areas  such as biology  astronomy   and 
finance.  The associated curse of dimensionality is often alleviated by prior knowledge that the object being estimsted 
has some structore. One of the most natorsl and well-stodied structural assumption for vectors is sparsity. Accordingly  
a huge amount of recent work in machine learning  statistics and signal processing has been devoted to finding better 
ways to leverage sparse structures.  Compressed sensing  a new and active branch of modem signal processing  deals 
with the problem of designing measurement matrices and recovery algorithms  such that almost all sparse signals can 
be recovered from a smalI number of measurements.  It has important applications in imsging  computer vision and 
machine learning (see  for example  [9 24  14]). 

In this paper   we  focus  on the  compressed sensing  setting  [3   7]  where  we want to  design a  measurement matrix 
A  E R=xn such that a sparse vector x*  E Rn with Ilx*llo  := I BUpp(X*)I  ::;  k < n can be efficiently recovered from 
the measurements b =  Ax*  E  R=.  Initial work focused on various random ensembles of matrices A  such that  if A 
was chosen randomly from that ensemble  one would be able to recover all or almost all sparse vectors x* from Ax*. 
Candes and Tao[3] isolated a key property called the restricted Isometry property (RIP) and proved that  as long as the 
measurement matrix A satisfies RIP  the true sparse vector can be obtained by solving an i -optimization problem  

The above problem can be easily formulated as a linear program and is hence efficiently solvable.  We recall for the 
reader that a matrix A is said to satisfY RIP of order k if there is some Ok  E  10 1) such that  for all x with Ilxllo  ::;  k  
we have 

min  Ilxll   S.t.  Ax =  b. 

I 

Several  random  matrix  ensembles  are  known  to  satisfY  00>  <  {}  with  high  probability  provided  one  chooses 
m  ~ 0  (~ log ~) measurements.  It was shown in [2] that i -minimization recovers all k-sparse vectors provided A 
satisfies t.k < 0.414 although the conditioohas been recently intproved to 02k  < 0.473 [11]. Note that  in compressed 
sensing  the goal is to recover all   or most   k-sparse signals using the same measurement matrix A.  Hence   weaker 
cooditioos such as restricted coovexity [20]  studied in the statistical literature (where the aint is to recover a single 
sparse vector from noisy linear measurements) typically do not suffice.  In fact   if RIP is not satisfied then multiple 
sparse vectors x can lead to the sante observatioo b  hence making recovery of the true sparse vector intpossible. 
Based on its RIP guarantees  i -minimizatioo can guarantee recovery using just O(k log(n/ kÂ»)  measurements  but it 
has been observed in practice that i -minimization is too expensive in large scale applications [8]  for example  when 
the dimensionality is in the millions.  This has sparked a huge interest in other iterative methods for sparse recovery. 
An early classic iterative method is Orthogooal Matching Pursuit (OMP) [21  6] that greedily chooses elements to add 
to the support. It is a natural  easy-to-intplement and fast method but unfortuoately lacks stroug theoretical guarantees. 
Indeed   it is known that   if run for  k  iterations   OMP cannot uoiformly recover all  k-sparse  vectors assumiug RIP 
cooditioo of the form 02k  :'0  IJ [22  18]. However  Zhang [26] showed that OMP  if run for 30k iterations  recovers the 
optimal solution when 03'k :'0  1/3; a significantly more restrictive cooditioo than the ones required by other methods 
like i -minimization. 

Several other iterative approaches have been proposed that include Iterative Soft Thresholding (1ST)  [17]   Iterative 
Hard Thresholding (!BT)  [I]  Compressive Santpling Matching Pursuit (CoSaMP)  [19]   Subspace Pursuit (SP)  [4]  
Iterative Thresholding with Inversion (IT!) [16]  Hard Thresholding Pursuit (HTP) [10] and many others. In the family 
ofiterative hard thresholding algorithms  we can identifY two major subfamilies [17]:  one- and two-stage algorithms. 
As their nantes suggest  the distiuctioo is based on the number of stages in each iteration of the algorithm.  One-stage 
algorithms  such as  IHT  m and HTP   decide on the  choice  of the next support set and then usually solve  a  least 
squares problem on the updated support.  The one-stage methods always set the support set to have size k   where k 
is the target sparsity level.  On the other hand   two-stage algorithms   notable examples being CoSaMP and SP   first 
enlarge the support set   solve a least squares 00 it  and then reduce the support set back again to the desired size.  A 
secood least squares problem is then solved 00 the reduced support.  These algorithms typically enlarge and reduce 
the support set by k  or 2k elements. An exceptioo is the two-stage algorithm FoBa [25] that adds and removes single 
elements from the support. However  it differs from our proposed methods as its analysis requires very restrictive RIP 
cooditioos (08k  <  0.1  as  quoted in [14])  and the connection to locality sensitive hashing (see below) is not made. 
Another algorithm with replacentent steps was studied by Shalev-Shwartz et al.  [23].  However  the algorithm and the 
settiug under which it is analyzed are different from ours. 

In this paper  we present and provide a unified analysis for a family of one-stage iterative hard thresholding algorithms. 
The family is parameterized by a positive integer I :'0  k. At the extrente value I ~ k  we recover the algorithm ITIIHTP. 
At the other extrente  k  ~ 1  we get a novel algorithm that we call Orthogonal Matching Pursuit with Replacement 
(OMPR). OMPR can be thought of as a sintple modification of the classic greedy algorithm OMP: instead of sintply 
adding an element to the existiug support  it replaces an existiug support element with a new one.  Surprisingly  this 
change allows us to prove sparse recovery under the condition 02k  < 0.499.  This is the best 02k based RIP condition 
under which any method  including i  -minimization  is (currently) known to provably perform sparse recovery. 
OMPR also lends itself to a faster intplententatioo using locality sensitive hashing (LSH).  This allows us to provide 
recovery guarantees using an algorithm whose run-time is provably sub-linear in n  the number of dimensions.  An 
added advantage of OMPR  unlike  many iterative  methods   is that no  careful tuning  of the  step-size parameter is 
required even under noisy settiugs or even when RIP does not hold.  The default step-size of 1 is always guaranteed to 
converge to at least a local optimum. 

Finally   we  show that our proof techniques  used in the  analysis of the  OMPR family  are  useful  in tightening  the 
analysis of two-stage algorithms  such as CoSaMP and SP  as well.  As a result  we are able to prove better recovery 
guarantees for these algorithms:  04k  < 0.35 for CoSaMP  and 03k  < 0.35 for SP.  We hope that this unified analysis 
sheds more light on the interrelationships between the various kinds of iterative hard thresholding algorithms. 

In summary  the contributions of this paper are as follows . 

â¢  We present a family of iterative hard thresholding algorithms that on one end of the spectrum includes ex(cid:173)

isting methods such as  ITIIHTP while on the other end gives  OMPR.  OMPR is an intproventent over the 
classical OMP method as it enjoys better theoretical guarantees and is also better in practice as shown in our 
experiments . 

â¢  Unlike  other intprovements  over OMP   such as  CoSaMP  or SP   OMPR changes  ouly ooe  elentent of the 
support at a tinte.  This allows us to use Locality Sensitive Hashing (LSH) to speed it up resultiug in the first 
provably sub-linear (in the ambient dimensionality n) time sparse recovery algorithm. 

2 

Algorithm 1 OMPR 
1:  Input: matrix A  vector b  sparsity level k 
2:  Parameter: s1ep size 1/ > 0 
3:  Initialize Xl  S.t  I supp(xl)1  =  k  h  =  supp(XI) 
4:  for t  =  1 to T  do 
5: 
6 
: 
7: 
8: 

zHI  <- x' + 1/AT(b - Ax') 
I  HII 
. 
Jt+l  +- argmaxj~It  Zj 
J'+1  <- I  U {iHI} 
yt+l  +- H  (zt+l) 
J t +l 
It+1  <- supp(y'+1) 
9: 
xHI 
10 
[Hl +-
: 
11:  end for 

\b  xHI 

It+l 

  

it+l  +-

k 

A 

0 

Algorithm 2 OMPR (I) 
1:  Input: matrix A  vector b  sparsity level k 
2:  Parameter: step size 1/ > 0  replacement budget 1 
3:  Initialize Xl  S.t  I supp(xl)1  =  k  h  =  supp(xl ) 
4:  fort =  ltoTdo 
5: 
6: 
7: 
8: 
9: 
10:  XHI  <- A 
I t +1 
11:  end for 

zHI <- x' + 1/AT(b - Ax') 
tOPHI  <- indices of top 1 elements of Iz};'"11 
J'+1  <- I  U tOPHI 
yt+l  +- Hk (z~~:J 
IHI  <- supp(yHI) 

\b  x'.+1  <- 0 

It+l'  1t+l 

â¢  We provide a general proof for all the algorithms in our partial hard thresholding based family.  In particular  
we can guarantee recovery using OMPR  under both noiseless and noisy settings   provided 02'  <  0.499. 
This is the least restrictive 02. cooditioo under which any efficient sparse recovery method is known to work. 
Furthermore  our proof technique can be used to provide a general theorem that provides the least restrictive 
known guarantees for all the two-stage algorithms such as CoSaMP and SP (see Appendix D). 

All proofs omitted from the main body of the paper can be found in the appendix. 
2  Orthogonal Matching PUl"lIuit with Replacement 
Orthogonal matching pursuit (OMP)  is a classic iterative algorithm for sparse recovery.  At every stage  it selecta a 
coordinate to include in the current support set by maximizing the inner product between columns of the measurement 
matrix A  and the current residnal b - Ax'. Doce the new coordinate has been added  it solves a least squares problem 
to fully miuimize the error on the current support set As a result  the residnal becomes orthogonal to the columos of 
A  that correspond to the current support set.  Thus  the least squares s1ep  is also referred to as orthogonalization by 
some authors [5]. 
Let us briefly explain some of our notation.  We use the MATI..AB notation: 

A\b:= argmin IIAx - bl1 2  â¢ 

z 

The hard thresholding operator H.O sorts its  argument vector in decreasing order (in absolute value) and retains 
ooly the top  k entries.  It is defined formally in the next sectioo.  Also  we use subscripts to denote sub-vectors and 
submatrices  e.g.  if I  <;;  Inl  is a set of cardinality k and x  ERn  XI  E  R' denotes the sub-vector of X  indexed by I. 
Similarly  AI for a matrix A  E Rmx n  denotes a sub-matrix of size m x k with columns indexed by I. The complement 
of set I  is denoted by I and x I  denotes the subvector not indexed by I. The support (indices of non-zero entries) of a 
vector x is denoted by supp(x). 
Our new algorithm called Orthogooal Matching Pursuit with Replacement (OMPR)  shown as Algorithm 1   differs 
from OMP in two respects. First  the selection of the coordinate to include is based not just on the magnitude of entries 
in AT (b - Ax') but instead on a weighted combination x' + 1/AT (b - Ax') with the s1ep-size 1/ cootrolling the relative 
importance of the two addends.  Second  the selected coordinate replaces one of the existing elements in the support  
namely the one corresponding to the minimum magnitude entry in the weighted combination mentioned above. 
Doce the support IHI of the next iterate has been determined  the actna1 iterate XHI is obtained by solving the least 
squares problem: 

XHI = 

argmin 

x: supp(z)=It+l 

IIAx - bli2  . 

Note that if the matrix A satisfies RIP of order k or larger   the above problem will be well conditioned and can be 
solved quickly and reliably using an iterative least squares solver. We will show that OMPR  uulike OMP  recovers any 
k-sparse vector under the RIP based cooditioo 02.  :<:;  0.499. This appears to be the least restrictive recovery condition 
(i.e.  best known coodition) under which any method  be it basis pursuit (ll-minimizatioo) or some iterative algorithm  
is guaranteed to recover all k-sparse vectors. 
In the literature on sparse recovery  RIP based cooditioos of a different order other than 2k are often provided.  It is 
seldom possible to directly compare two conditions  say  one based on 62â¢  and the other based on 63 â¢â¢ Foucart [10] has 

3 

given a heuristic to compare such RIP conditions based on the number of samples it takes in the Gaussian ensemble 
to satisfy a given RIP condition.  This heuristic says that an RIP condition of the form lic' < 9 is less restrictive if the 
ratio c/92  is smaller.  For the OMPR condition Ii . < 0.499  this ratio is 2/0.4992  "" 8 which makes it heuristically 
the least restrictive RIP condition for sparse recovery.  The following summarize our main results on OMPR. 
Theorem 1 (Noiseless Case).  Suppose the vector x*  E  IRn  is  k-sparse and the matrix A  satisfies 1i2â¢  < 0.499 and 
Ii   <  0.002.  Then  OMPR  converges to  an  E approximate solution  (i.e.  1/211Ax -
bl1 2  ~ E)  from  measurements 
b ~ Ax* in O(klog(k/E)) iterations. 
Theorem 2  (Noisy Case).  Suppose  the vector x*  E  IRn  is  k-sparse and the  matrix  A  satisfies  1i2â¢  <  0.499  and 
Ii   <  0.002.  Then  OMPR  converges  to  a  (C E)  approximate solution  (i.e.  1/211Ax - bll'  ~ ~llell' + E)  from 
measurements b ~ Ax* + e in O(k log((k + IleI1 2)/E)) iterations.  Here C  > 1 is a constant dependent only on 1i2 â¢. 
The above theorems are special cases of our convergence results for a family of algorithms that contains OMPR as a 
special case.  We now tum our attention to this family.  We note that the condition 1i2  < 0.002 is very mild and will 
typically hold for standard random matrix ensembles as  soon as the number of rows sampled is larger than a fixed 
universal constant 

3  A New Family of Iterative Algorithms 
In this section we show that OMPR  is one particular member of a  family of algorithms parameterized by a  single 
integer 1 E {I  ...   k}. The I-th member of this family  OMPR (I)  showo in Algorithm 2  replaces at most 1 elements 
of the curreot support with new elements.  OMPR  corresponds to the choice 1 ~ 1.  Hence  OMPR  and OMPR (1) 
refer to the same algorithm. 
Our first result in this section conoects the OMPR  family to hard thresholding.  Given a set I  of cardinality k  define 
the partial hard thresholding operator 

Hk (z; I  I):~  argmin 
hl o:S;k 

I supp(y)\II5:l 

Ily - zll  . 

(I) 

As is clear from the definition  the above operator tries to find a vector V close to a given vector z under two constraints: 
(i) the vector V should have bounded support (1lvllo  ~ k)   and (ii)  its support should not include more than 1 new 
elements outside a given support I. 
The name partial hard thresholding operator is justified because of the following reasoning. When 1 ~ k  the constraint 
I supp(Y)\I1  ~ 1 is trivially implied by IIYllo  ~ k and hence the operator becomes independent of!. In fact  itbecomes 
identical to the standard hard thresholding operator 

H. (z; I  k)  ~ H. (z)  :~ argmin  Ily - zll  . 

(2) 
Even though the definition  of Hk (z)  seems to involve  searching through GJ  subsets   it can in fact  be  computed 
efficiently by simply sorting the vector z by decreasing absolute value and retaming the top k entries. 
The following result shows that even the partial hard thresholding operator is easy to compute.  In fact   lines 6-8 in 
Algorithm 2 precisely compute H. (zt+1; It  I). 
Proposition 3.  Let III ~ k and z  be given.  Then Y ~ H. (z;I  I) can be computed using the sequence of operations 

11.1109 

top ~ indices of top 1 elements oflzll   J  ~ I  U top   V ~ Hk (ZJ)  . 

The proof of this proposition is straightforward and elementary.  However  using it  we can now see that the OMPR (I) 
algorithm has a simple conceptoa1 s1ructore.  In each iteration (with current iterate x' having support It  ~ supp(xtÂ»  
we do the following: 

1.  (Gradient Descent) Fonn zHI  ~ xt - '1AT(Axt - b). Note that AT(Axt - b)  is the gradient of the objective 

function ~IIAx - bll' at x'. 

2.  (partial Hard Thresholding) Form VH1 by partially hard thresholding zHI using the operator H. (.; It  I). 
3.  (Least Squares) Form the next iterate XHI by solving a least squares problem on the support IHI ofyHI. 

A nice property enjoyed by the entire OMPR  family is guaranteed sparse recovery under RIP based conditions. Note 
from below that the condition under which OMPR (I) recovers sparse vectors becomes more restrictive as I increases. 
This could be an artifact of our analysis   as in experiments  we do not see any degradation in recovery ability as I is 
increased. 

4 

Theorem 4 (Noiseless Case).  Suppose the vector x' E IRn  is k-sporse.  Then OMPR (I) converges to an < approxima(cid:173)
tion solution (i.e.  1/211Ax - bl12 :5  <)from measurements b =  Ax* in O( ~ log(k/<Â»  iterations provided we choose a 
step size 1'/ that satisfies 1'/(1 + 02.)  < 1 and 1'/(1  - 02.) > 1/2. 
Theorem S (Noisy Case).  Suppose the vector x' E IRn  is k-sparse.  Then OMPR (I) converges to a (C  <)  approximate 
solution (i.e.   1/211Ax - bl1 2 :5  t IIell2 + <) from measurements b =  Ax' + e in O( t logÂ«k + IleI1 2)1<) 
iterations 
provided we choose a step size 1'/  that satisfies 1'/(1  + 02 )  <  1 and 1'/(1  - 02.)  >  1/2.  Here  C  >  1 is a  constant 
dependent only on 02.  02 â¢. 
Proof  Here we provide a rough sketch of the proof of Theorem 4; the complete proof is giveo in Appeodix A. 
Our proof uses the following  crucial observatioo regarding the structure of the vector zH1  =  x' - 1'/AT (Ax' - b)  . 
Due to the least squares step of the previous iteration  the curreot residual Ax' - b is orthogoual to columns of AI . 
This meaos that 

ZH1  - x' 
It 

-

It' 

z~+1 =  -nA'!' (Ax' - b)  . 
It 

It 

" 

(3) 

As the algorithm proceeds  elemeots come in and move out of the curreot set I . Let us give names to the set offound 
and lost elements as we move from I  to 1'+1: 

(found):  F   =  IH1 \I" 

Heoce   using  (3) and updates for  YH1:  Y~;'  =  Z~;'  =  -1'/A~ A(x' - x')  and  Z~;'  =  xL.  Now let J(x)  = 
1/211Ax - b112  theo using upper RIP and the fact that I supp(yH1 - x')1  =  IF  U L I :5  21  we can sbow that (details 
are in the Appeodix A): 

J(yH1)  - J(x'):5 C ~02' - D IIyWII2 + 1 ~02'llxUI2. 

(4) 

Furthermore  since yH1 is choseo based on the k  largest eotries in z~;:"  we have:  IIY~;'112 =  Ilz~;'112 ~ Ilz~;'112 = 
IlxL 112  . Plugging this into (4)  we get: 

J(yH1) - J(x'):5  (1 +O2'-~) M;'112. 

(5) 

Since  J(xH1 ) :5  J(yH1)  :5  J(x')  the above expression shows that if 1'/  <  1':."  then our method moootonically 
decreases the objective function  and converges to a local optimum even if RIP is not satisfied (note that upper RIP 
bound is indepeodeot oflower RIP bound  and can always be satisfied by nurma1izing the matrix appropriately). 
However  to prove convergeoce to the global optimum  we need to show that at least ooe new elemeot is added at each 
step  i.e.   IF I  ~ 1.  Furthermore  we need to show sufficieot decrease  i.e   IIY~;'112 ~ elJ(x'). We show both these 
conditions for global coovergeoce in Lemma 6  whose proof is giveo in Appeodix A. 
Lemma 6.  Let 02k  <  1 -
F  '" 0.  Furthermore   IIY~;'11 > teJ(x')  where e =  min(41'/(1 - 1'/)  2(21'/- 1-~"Â» > 0 is a constant. 

2~ and 1/2 < 1'/  <  1.  Then  assuming J(x')  > 0   at least one new element is found i.e. 

Assunling  Lemma 6   (5)  shows  that at each iteration OMPR (I) reduces  the objective functioo value  by at least a 
constant fractioo.  Furthermore  if XO  is choseo to have eotries bounded by 1  theo J(XO)  :5  (1 + 02k)k.  Heoce  afier 
D 
O(k/llog(k/<Â»  iteratioos  the optimal solution x* would be obtained within < error. 

Speeial Cases:  We  have  already observed that the  OMPR  algorithm of the previous sectioo is  simply OMPR (1). 
Also note that Theorem I immediately follows from Theorem 4. 
The algorithm at the other extreme of 1 =  k has appeared at least three times in the receot literature: as Iterative (hard) 
Thresholding with Inversioo  (IT!) in [16]   as  SVP-Newton (in its matrix avatar)  in [15]   and as Hard Thresholding 
Pursuit (HTP) in [10]).  Let us call it  IHT-Newton as the least squares step can be viewed as a Newton step for the 
quadrstic objective. The above geoera1 result for the OMPR  family immediately implies that it recovers sparse vectors 
as soon as the measuremeot matrix A satisfies 02   < 1/3. 
CoroUary 7.  Suppose  the  vector x'  E  an  is  k-sparse and the matrix  A  satisfies 02k  <  1/3.  Then  IlIT-Newton 
recovers x* from measurements b =  Ax' in O(1og(kÂ»  iterations. 

5 

4  Tighter Analysis of Two Stage Hard Thresholding Algorithms 
Recently  Maleki and Donoho [17] proposed a novel family of algorithms  namely two-stage hard thresholding algo(cid:173)
rithms.  Doring each iteration  these algorithms add a fixed nwnber (say l) of elements to the current iterate's support 
set.  A  least squares problem is solved over the larger support set and then I elements with smallest magnitude are 
dropped to form next iterate's support set.  Next iterate is then obtained by agaiu solviug the least squares over next 
iterate's support set. See Appendix D for a more detailed description of the algorithm. 

Usiug proof techniques developed for our proof of Theorem 4  we can obtain a simple proof for the entire spectrum of 
algorithms iu the two-stage hard thresholding family. 
Theorem 8.  Suppose the vector x*  E {-I  0  l}n is k-sparse.  Then the 7Wo-stage Hard Thresholding algorithm with 
replacement size I recovers x* from  measurements b =  Ax* in O(k) iterations provided:  6.H1  :::;  .35. 
Note that CoSaMP [19]  and Subspace Pursuit(SP)  [4]  are popular special cases of the two-stage family.  Usiug our 
general analysis  we are able to provide significantly less restrictive RIP conditions for recovery. 
CoroUary 9.  CoSaMP[l9] recovers k-sparse x*  E {-1 0  l}n from measurements b =  Ax* provided 64k  :::;  0.35. 
CoroUary 10.  Subspace  Pursuit[4]  recovers  k-sparse  x*  E  {-I  0  I}n from  measurements  b  =  Ax* provided 
63k  :::;  0.35. 
Note that CoSaMP's analysis given by [19] requires 64k  :::;  0.1 while Subspace Pursuit's analysis given by [4] requires 
63k  :::;  0.205. See Appendix Diu the supplementary material for proofs of the ahove theorem and coroUaries. 

5  Fast Implementation Using Hashing 
In this  section   we  discuss  a  fast  implementation  of the  OMPR  method  usiug  locality-sensitive  hashiug.  The 
mall iutuition behind our approach is that the OMPR method selects  at most one element at each step  (given by 
argmax  IAT(Ax' - b) I); hence  selection of the top most element is equivalent to finding the column Ai that is most 
"similar"  (iu magnitude) to r  =  Ax' - b  i.e.  this may be viewed as the similarity search task for queries of the form 
r  and -r  from a database of N  vectors IAI"'"  ANI. 
To  this  end   we use  locality sensitive  hashiug  (LSH)  [12]   a  well known  data-structore  for  approximate  nearest(cid:173)
neighbor retrieval. Note that while LSH is designed for nearest neighbor search (iu terms of Euclidean distances) and 
iu general might not have any guarantees for the similar neighbor search task  we are still able to apply it to our task 
because we can lower-hound the similarity of the most similar neighbor. 

We first briefly describe the LSH scheme that we use.  LSH generates hash bits for a vector usiug randoruized hash 
functions that have the property that the probability of collision between two vectors is proportional to the similarity 
between them.  For our problem  we use the following hash function:  h .(a)  =  sign(uT a)  where u  ~ N(O  J) is a 
random hyper-plane generated from the standard multivariate Gaussian distribution. It can be shown that [13] 

-I (  af a2 

Iladlla211' 

) 

( )  

Pr[hu  al  =  hu  a. ] =  1-;;: cos 
is  created  by  randoruly  sampling  hash 

( )  

I 

an 

i.e.   g( a) 

functions  h .  

.-bit  hash  key 

Now  
[hu (a) hu (a)  ...  hu.(a)]   where  each  Ui  is  sampled  randoruly  from  the  standard  multivariate  Gaussian 
distribution. Next  q hash tables are constructed doring the pre-processiug stage usiug iudependently constructed hash 
key functions gl  92  ...   gq'  Doring the query stage  a query is iudexed iuto each hash table usiug hash-key functions 
91  92  ...  9q and then the nearest neighbors are retrieved by doing an exhaustive search over the indexed elements. 
Below we state the following theorem from [12] that guarantees sub-liuear time nearest neighbor retrieval for LSH. 
Theorem 11.  Let. =  O(logn) and q =  O(log 1/6)nr1<  then with probability 1 - 6   LSH recovers (I + f)-nearest 
neighbors  i.e.   Ila' - rl12  :::;  (1 + f)lla' - rllÂ·  where a' is the nearest neighbor to r  and a' is a point retrieved by 
LSH. 
However  we cannot directly use the above theorem to guarantee convergence of our hashing based OMPR algorithm 
as our algorithm requires finding the most similar poiut iu terms of magnitude of the iuner product. Below  we provide 
appropriate settings of the LSH parameters to guarantee sub-liuear time convergence of our method under a slightly 
weaker condition on the RIP constant. A detailed proof of the theorem below can be found iu Appendix B. 
Theorem 12.  Let 62â¢  < 1/4 -")' and 'f/  =  I -")"  where")' > 0 is a small constant   then with probability I - 6   OMPR 
with hashing converges to the optimal solution in O(kmnl /(1+0(I/k)) log k/6) computational steps. 
The  above  theorem shows  that the  time complexity is  sub-liuear iu n.  However   currently our guarantees  are not 
particularly strung as for large k the exponent of n will be close to 1.  We believe that the exponent can be improved 
by more careful analysis and our empirical results iudicate that LSH does speed up the OMPR method significantly. 

6 

(a)OMPR 

(b)OMP 

(c) nIT-Newton 

Figure  1:  Phase Transition Diagrams for different methods.  Red represents  high probability of success while blue 
represents low probability of success.  Clearly  OMPR recovers correct solution for a much larger region of the plot 
than OMP and is comparable to nIT-Newton. (Best viewed in color) 
6  Experimental Results 
In this section we present empirical results to demonstrate accurate and fast recovery by our OMPR method. In the first 
set of experiments  we present a phase transition diagram for OMPR and compare it to the phase transition diagrams 
of OMP and nIT-Newton with step size 1.  For the second set of experiments  we demonstrate robostoess of OMPR 
compared to many existiog methods when measurements are noisy or smaller in number than what is required for exact 
recovery.  For the third set of experiments  we demonstrate efficiency of our LSH based implementation by comparing 
recovery error and time required for our method with OMP and nIT-Newtoo (with step-size 1 and 1/2).  We do not 
present results for the i ibasis pursuit methods  as it has a1readybeen shown in several recent papers [10   17] that the 
i  relaxation based methods are relatively inefficient for very large scale recovery problems. 
In all the experiments we generate the measurement matrix by sampling each entry independently from the standard 
normal distribotion N (0  1) and then normalize each column to have uuit norm.  The underlying k-sparse vectors are 
generated by randomly selecting a support set of size k and then each entry in the support set is sampled uuiformiy from 
{ +1  -I}. We use our own optimized implementation of OMP and nIT-Newtoo. All the methods are implemented in 
MATLAB and our hashing routioe uses mex files. 
6.1  Phase Transition Diagrams 
We first compare different methods using phase transition diagrams which are commouly used in compressed sensing 
literatore to compare different methods [17].  We first fix the number of measurements to be m  =  400  and generate 
different problem sizes by varying p  =  kim and 6  =  min.  For each problem size  (m  n  k)  we generate random 
m  x  n Gaussian measurement matrices and k-sparse random vectors.  We then estimate the probability of success of 
each of the method by applying the method to  100 randomly generated instances.  A method is considered successful 
for a particular instance if it recovers the underlying k-sparse vector with at most 1 % relative error. 
In Figure 1  we show the phase transition diagram of our OMPR method as well as that ofOMP and nIT-Newtoo (with 
step size 1).  The plots shows probability of successful recovery as a function of p =  min and 6 =  kim. Figure 1 (a) 
shows color coding of different success probabilities; red represents high probability of success while blue represents 
low probability of success.  Note that for Gaussian measurement matrices   the RIP constant 62â¢  is less than a fixed 

constant if and ouly ifm =  Ck log(nlk)  where C is a uuiversal constant This implies that * =  Clog p and hence a 

method that recovers for high 62â¢  will have a large fraction in the phase transition diagram wbere successful recovery 
probability is high.  We observe this phenomenon for both OMPR and nIT-Newton method which is consistent with 
their respective theoretical goarantees (see Theorem 4).  On the other hand  as expected  the phase transition diagram 
of OMP has a negligible fraction of the plot that shows high recovery probability. 
6_2  Performance for Noisy or Under-sampled Observations 
Next   we empirically compare performance of OMPR to various  existing compressed sensing methods.  As shown 
in the phase transition diagrams in Figure  1   OMPR provides comparable recovery to the nIT-Newton method for 
noiseless cases.  Here  we show that OMPR is fairly robust under the noisy settiog as well as in the  case of under(cid:173)
sampled observations  where the number of observations is much smaller than what is required for exact recovery. 
For this experiment  we generate random Gaussian measurement matrix of size m  =  200  n =  3000. We then generate 
random binary vector x  of sparsity k  aod add Gaussian noise to  it  Figure 2  (a)  shows recovery error (1iAx - bll) 
incurred by various methods for increasing  k  and noise level of 10%.  Clearly  our method outperforms the existing 
methods  perhaps a consequence of goaranteed convergence to a local minimum for fixed step size 1/  =  1.  Similarly  
Figure 2 (b) shows recovery error incurred by various methods for fixed k  =  50 and varying noise level.  Here again  
our method outperforms existiog methods and is more robust to noise.  Fina11y  in Figure 2 ( c) we show difference in 

7 

Enurvsk(Noi-=10%) 

" _ OMPR 

Error w  NaIM  k=SO 

+OMPR(1rI2 

:U .

IHT-N 

+ SP 

~  _ CoSAMP 
:ii' 
~ 3 
~'.'~~/ 
"~:----';o '  -'0." -'0." -'0'.----- 10.' 

NOise! 1< 

0 

 0 

0.00 
0.05 
0.0 
0.20 
U.3U 
0.40 
0.50 

-0.21(0.6) 
0.00(0.0) 
0.13(0.3) 
0.00(0.0) 
O.OO(u.O) 
0.2"(0.3) 
0.03(0.0) 
0.62(0.2) 
U.1"(U.1)  U.92(0.3) 
1.19(0.3) 
0.31(0.1) 
0.37(0.1) 
1.48(0.3) 

5u 

0.25(0.3) 
0.37(0.3) 
O.  3  0.4) 
0.58(0.5) 
O.92(O.b) 
0.84(0.5) 
1.24(0.6) 

(c) 

10 

20 
30 
Sp.lI'IiIy{k) 
(a) 

" 

50 

Hoi_LewI 
(b) 

Figure 2:  Error in recovery <lIAx - bll) of n  =  3000 dimensiooal vectors from m  =  200 measurements.  (a):  Error 
incurred by various methods as the sparsity level  k increases.  Note that OMPR  incurs the least error as it provably 
converges to at least a local minimum forfixed step size 1/  =  1.  (b):  Error incurred by various methods as the noise 
level increases.  Here again OMPR performs significaotly better than the existing methods.  (c):  Differeoce in error 
incurred by IHT-Newton aod OMPR . Numbers in bracket dooote confideoce interval at 95% significaoce level. 
TlII'MI wn (rMFO.oD1  k/m= 1 

EmrVII n (mIn=O 001  Idm"  1) 

0." 

.  . 

0.03 

.". 
o. 

 015 

O. 

:.-

00 â¢ 

 .  

..  

Huh 

I:-OMPR

+ 00'" 
-
+IHT-NMlanC1 â¢ 

" .  

~ .... 

"'" 

.. 

lIo 

'" 

(a) 

ncIrOOO) 
(b) 

000 

n (x1fOOOO) 
(c) 

Figure 3: (a): Error (11Ax - bll) incurred by various methods as k increases. The measuremoots b =  Ax are computing 
by gooerating x  with support size milO.  (b) (c):  Error incurred aod time required by various  methods to recover 
vectors of support size 0.1 mas n increases. IlIT-Newton(1/2) refers to the IHT-Newton method with step size 1/ =  1/2. 

error incurred along with confideoce interval (at 95% signficaoce level) by IHT-Newton aod OMPR for varying levels 
of noises aod k.  Our method is better thao !HT-Newton (at 95% signficaoce level) in terms of recovery error in arouod 
30 cells of the table  aod is not worse in aoy of the cells but one. 

6.3  Performance of LSD based implementation 
Next  we empirically study recovery properties of OMPR-Hasb  in the following real-time setop:  gooerate a raodom 
measuremoot matrix from the Gaussiao ensemble aod construct bash tables ollline using hash functioos specified in 
Section 5. During the reconstruction stage  measurements arrive one at a time and the goal is to recover the underlying 
sigoal accurately in real-time.For our experimoots  we gooerate measuremoots using raodom sparse vectors aod thoo 
report recovery error IIAx - bll  aod computatiooal time required by each method averaged over 20 runs. 
In our first set of experimoots  we eropirically study the performaoce of different methods as k increases. Here  we fix 
m  =  500  n  =  500  000 aod gooerate measuremoots using n-dimoosional raodom vectors of support set size milO. 
We thoo run differeot methods to estimate vectors x of support size k that minimize IIAx - bll. For our  OMPR-Hash 
method  we use  8  =  20 bits bash-keys aod gooerate q  =  ..;n bash-tables.  Figure 3 (a)  shows the error incurred by 
OMPR   OMPR-Hash  aod IHT-Newton for differeot k (recall that k is ao input to both OMPR aod IlIT-Newton). 
Note that although  OMPR-Hash performs ao approximation at each step  it is still able to  achieve  error similar to 
OMPR aod !HT-Newton.  Also  note that since the number of measure moots are not ooough for exact recovery by the 
IHT-Newton method  it typically diverges after a few steps.  As a result  we use IHT-Newton with step size 1/  =  1/2 
which is always goaraoteed to monotonically converge to at least a local minimum (see Theorem 4).  In cootrast  in 
OMPR aod  OMPR-Hasb cao always set step size 1/ aggressively to be 1. 
Next  we evaluate  OMPR-Hash as dimoosiooality of the data n  increases.  For  OMPR-Hasb  we use  8  =  log2(n) 
bash-keys aod q  =  ..;n hash-tables.  Figures 3(b) aod (c) compare error incurred aod time required by  OMPR-Hash 
with OMPR aod IHT-Newton. Here again we use step size 1/  =  1/2 for !HT-Newton as it does not converge for 1/  =  1. 
Note that  OMPR-Hash is ao order of magnitude faster thao OMPR while incurring slightly higher error.  OMPR-Hash 
is also nearly 2 times faster thao IHT-Newton. 

Acknowledgement 

ISD acknowledges support from the Moncrief Graod Challooge Award. 

8 

References 
[I]  T. Blumensath and M. E. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational 

Harmonic Analysis  27(3):265-274  2009. 

[2]  E. J.  Candes.  The restricted isometry property and its implications for compressed sensing.  Comptes Rendus 

Mathematique  346(9-10):589-592  2008. 

[3]  E.  J.  Candes  and  T.  Tao.  Decoding  by  lioear  programming. 

51(12):4203-4215 2005. 

IEEE  Transactions  on  Information  Theory  

[4]  W. Dai and O. Milenkovic.  Subspace pursuit for compressive seosing signal reconstruction.  IEEE Transactions 

on Information Theory  55(5):2230--2249  2009. 

[5]  M.  A.  Davenport  and M.  B.  Wakin.  Analysis  of orthogonal matching pursuit using  the  restricted  isometry 

property. IEEE Transactions on Information Theory  56(9):4395-4401  2010. 

[6]  G. Davis  S. Mallat  and M. Avellaneda.  Greedy adaptive approximation.  Constr.  Approx   13:57--98  1997. 
[7]  D. Donoho.  Compressed sensing. IEEE Trans.  on Information Theory   52(4):1289-1306  2006. 
[8]  D. Donoho  A. Maleki  and A. Montanari.  Message passing algorithms for compressed sensing.  Proceedings of 

the National Academy of Sciences USA   106(45):18914-18919 2009. 

[9]  M. F.  Duarte  M. A.  Davenport  D. Takhar  1.  N. Laska  T.  Sun  K. F.  Kelly   and R.  G. Baranuik.  Single-pixel 

imaging via compressive sarnpliog.  IEEE Signal Processing Magazine  25(2):83-91  March 2008. 

[10]  S. Foucart.  Hard thresholding pursuit: an algorithm for compressive sensing  2010.  preprint. 
[II]  S. Foucart.  A  note on guaranteed sparse recovery via i -minimi'ation.  Applied and Computational Harmonic 

Analysis  29(1):97-103  2010. 

[12]  A. Giouis   P.  Indyk  and R  Motwani.  Similarity search in high dimensions using hashing.  In Proceedings of 

25th International Conference on Very Large Data Bases  1999. 

[13]  M. X. Goemans and D. P. WIlliamson .. 879-approximation algorithms for MAX CUT and MAX 2SAT. In STOC  

pages 422-431 1994. 

[14]  D. Hsu  S. M. Kakade  J.  Langford  and T. Zhang.  Multi-label prediction via compressed sensing.  In Advances 

in Neural Information Processing Systems  2009. 

[15]  P.  Jain  R. Meka  and I. S. Dhillon.  Gusranteed rank minimiUltion via singular value projection.  In Advances in 

Neural Information Processing Systems  2010. 

[16]  A. Maleki. Convergence analysis of iterative thresholding algorithms. InAllerton Conference on Communication  

Control and Computing  2009. 

[17]  A. Maleki and D. Donoho.  Optimally tuned iterative reconstruction algorithms for compressed sensing.  IEEE 

Journal of Selected Topics in Signal Processing  4(2):330--341  2010. 

[18]  Q. Mo and Y.  Shen. Remarks on the restricted isometry property in orthogonal matching pursuit algorithm  2011. 

preprint arXiv: 1101.4458. 

[19]  D. Needell and J. A. Tropp.  Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Applied 

and Computational Harmonic Analysis  26(3):301- 321  2009. 

[20]  S. Negshban  P. Ravikumar  M. 1. Wainwright  and B. Yu.  A unified framework for high-dimensional analysis of 

M -estimators with decomposable regularizers.  In Advances in Neural Information Processing Systems  2009. 

[21]  Y. C. Pati  R  Rezaiifar  and P.  S. Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation 
with applications to wavelet decomposition.  In 27th Annu.  Asilomar Con! Signals   Systems   and Computers  
volume I  pages 40-44  1993. 

[22]  H. Rauhut.  On the impossibility of uniform sparse reconstruction using greedy methods.  Sampling Theory in 

Signal and Image Processing  7(2):197-215  2008. 

[23]  S. Shalev-Shwartz  N. Srebro  and T. Zhang. Trading accuracy for sparsity in optimiUltion problems with sparsity 

constraints. SIAM Journal on Optimization  20:2807-2832  2010. 

[24]  J. Wright  Y.  Ma  J.  Mairal  G.  Sapiro  T.  S. Huang  and S. Yan.  Sparse representation for computer vision and 

pattern recognition.  Proceedings of the IEEE  98(6):1031-1044  2010. 

[25]  T. Zhang.  Adaptive forward-backward greedy algorithm for sparse learning with lioear models.  In Advances in 

Neural Information Processing Systems  2008. 

[26]  T. Zhang.  Sparse recovery with orthogonal matching pursuit under RIP  2010.  preprint arXiv:lOO5.2249. 

9 

,Ira Shavitt
Eran Segal