2019,No Pressure! Addressing the Problem of Local Minima in Manifold Learning Algorithms,Nonlinear embedding manifold learning methods provide invaluable visual insights into a structure of high-dimensional data. However  due to a complicated nonconvex objective function  these methods can easily get stuck in local minima and their embedding quality can be poor. We propose a natural extension to several manifold learning methods aimed at identifying pressured points  i.e. points stuck in the poor local minima and have poor embedding quality. We show that the objective function can be decreased by temporarily allowing these points to make use of an extra dimension in the embedding space. Our method is able to improve the objective function value of existing methods even after they get stuck in a poor local minimum.,No Pressure! Addressing the Problem of Local

Minima in Manifold Learning Algorithms

Max Vladymyrov
Google Research
mxv@google.com

Abstract

Nonlinear embedding manifold learning methods provide invaluable visual in-
sights into the structure of high-dimensional data. However  due to a complicated
nonconvex objective function  these methods can easily get stuck in local minima
and their embedding quality can be poor. We propose a natural extension to sev-
eral manifold learning methods aimed at identifying pressured points  i.e. points
stuck in poor local minima and have poor embedding quality. We show that the
objective function can be decreased by temporarily allowing these points to make
use of an extra dimension in the embedding space. Our method is able to improve
the objective function value of existing methods even after they get stuck in a poor
local minimum.

Introduction

1
Given a dataset Y ∈ RD×N of N points in some high-dimensional space with dimensionality D 
manifold learning algorithms try to ﬁnd a low-dimensional embedding X ∈ Rd×N of every point
from Y in some space with dimensionality d (cid:28) D. These algorithms play an important role in high-
dimensional data analysis  speciﬁcally for data visualization  where d = 2 or d = 3. The quality of
the methods have come a long way in recent decades  from classic linear methods (e.g. PCA  MDS) 
to more nonlinear spectral methods  such as Laplacian Eigenmaps [Belkin and Niyogi  2003]  LLE
[Saul and Roweis  2003] and Isomap [de Silva and Tenenbaum  2003]  ﬁnally followed by even more
general nonlinear embedding (NLE) methods  which include Stochastic Neighbor Embedding (SNE 
Hinton and Roweis  2003)  t-SNE [van der Maaten and Hinton  2008]  NeRV [Venna et al.  2010]
and Elastic Embedding (EE  Carreira-Perpi˜n´an  2010). This last group of methods is considered as
state-of-the-art in manifold learning and became a go-to tool for high-dimensional data analysis in
many domains (e.g. to compare the learning states in Deep Reinforcement Learning [Mnih et al. 
2015] or to visualize learned vectors of an embedding model [Kiros et al.  2015]).
While the results of NLE have improved in quality  their algorithmic complexity has increased as
well. NLE methods are deﬁned using a nonconvex objective that requires careful iterative mini-
mization. A lot of effort has been spent on improving the convergence of NLE methods  including
Spectral Direction [Vladymyrov and Carreira-Perpi˜n´an  2012] that uses partial-Hessian information
in order to deﬁne a better search direction  or optimization using a Majorization-Minimization ap-
proach [Yang et al.  2015]. However  even with these sophisticated custom algorithms  it is still
often necessary to perform a few random restarts in order to achieve a decent solution. Sometimes
it is not even clear whether the learned embedding represents the structure of the input data  noise 
or the artifacts of an embedding algorithm [Wattenberg et al.  2016].
Consider the situation in ﬁg. 1. There we run the EE 100 times on the same dataset with the same
parameters  varying only the initialization. The dataset  COIL-20  consists of photos of 20 different
objects as they are rotated on a platform with new photo taken every 5 degrees (72 images per object).
Good embedding should separate objects one from another and also reﬂect the rotational sequence
of each object (ideally via a circular embedding). We see in the left plot that for virtually every
run the embedding gets stuck in a distinct local minima. The other two ﬁgures show the difference
between the best and the worst embedding depending on how lucky we get with the initialization.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Best embedding (e = 0.40)

Worst embedding (e = 0.45)

Figure 1: Abundance of local minima in the Elastic Embedding objective function space. We run the
algorithm 100× on COIL-20 dataset with different random initializations. We show the objective
function decrease (left)  the embedding result for the run with the lowest (center) and the highest
(right) ﬁnal objective function values. Color encodes different objects.

The embedding in the center has much better quality compared to the one on the right  since most
of the objects are separated from each other and their embeddings more closely resemble a circle.
In this paper we focus on the analysis of the reasoning behind the occurrence of local minima in
the NLE objective function and ways for the algorithms to avoid them. Speciﬁcally  we discuss the
conditions under which some points get caught in high-energy states of the objective function. We
call these points “pressured points” and show that speciﬁcally for the NLE class of algorithms there
is a natural way to identify and characterize them during optimization.
Our contribution is twofold. First  we look at the objective function of the NLE methods and provide
a mechanism to identify the pressured points for a given embedding. This can be used on its own
as a diagnostic tool for assessing the quality of a given embedding at the level of individual points.
Second  we propose an optimization algorithm that is able to utilize the insights from the pressured
points analysis to achieve better objective function values even from a converged solution of an
existing state-of-the-art optimizer. The proposed modiﬁcation augments the existing analysis of the
NLE and can be run on top of state-of-the-art optimization methods: Spectral Direction and N-body
algorithms [Yang et al.  2013  van der Maaten  2014  Vladymyrov and Carreira-Perpi˜n´an  2014].
Our analysis arises naturally from a given NLE objective function and does not depend on any other
assumptions. Other papers have looked into the problem of assessing the quality of the embedding
[Peltonen and Lin  2015  Lee and Verleysen  2009  Lespinats and Aupetit  2011]. However  their
quality criteria are deﬁned separately from the actual learned objective function  which introduces
additional assumptions and does not connect to the original objective function. Moreover  we also
propose a method for improving the embedding quality in addition to assessing it.
2 Nonlinear Manifold Learning Algorithms
The objective functions for SNE and t-SNE were originally deﬁned as a KL-divergence between
two normalized probability distributions of points being in the neighborhood of each other. They
2σ2 (cid:107)yi − yj(cid:107)2)  to capture
use a positive afﬁnity matrix W+  usually computed as w+
a similarity of points in the original space D. The algorithms differ in the kernels they use in the
(cid:80)
exp(−(cid:107)xi−xj(cid:107)2)
low-dimensional space. SNE uses the normalized Gaussian kernel1 Kij =
n m exp(−(cid:107)xn−xm(cid:107)2) 

UMAP [McInnes et al.  2018] uses the unnormalized kernel Kij =(cid:0)1 + a(cid:107)xi − xj(cid:107)2b(cid:1)−1 that

while t-SNE is using the normalized Student’s t kernel Kij =

is similar to Student’s t  but with additional constants a  b calculated based on the topology of the
original manifold. The objective function is given by the cross entropy as opposed to KL-divergence.
Carreira-Perpi˜n´an [2010] showed that these algorithms could be deﬁned as an interplay between two
additive terms: E(X) = E+(X) + E−(X). Attractive term E+  usually convex  pulls points close
to each other with a force that is larger for points located nearby in the original space. Repulsive term
E−  on the contrary  pushes points away from each other. For SNE and t-SNE the attraction is given
by the nominator of the normalized kernel  while the repulsion is the denominator. It intuitively
makes sense  since in order to pull some point closer (decrease the nominator)  you have to push all
the other points away a little bit (increase the denominator) so that the probability would still sum

1Instead of the classic SNE  in this paper we are going to use symmetric SNE [Cook et al.  2007]  where

each probability is normalized by the interaction between all pairs of points and not every point individually.

2

ij = exp(− 1

(cid:80)
(1+(cid:107)xi−xj(cid:107)2)−1
n m(1+(cid:107)xn−xm(cid:107)2)−1 .

iterationsiterationsiterationsFigure 2: Left: an illustration of the local minimum typically occurring in NLE optimization. Blue
dashed lines indicate the location of 3 points in 1D. The curves show the objective function landscape
wrt x0. Right: by enabling an extra dimension for x0  we can create a “tunnel” that avoids a local
minimum in the original space  but follows a continuous minimization path in the augmented space.
to one. For UMAP  there is no normalization to act as a repulsion  but the repulsion is given by the
second term in the cross entropy (i.e. the entropy of the low-dimensional probabilities).
Elastic Embedding (EE) modiﬁes the repulsive term of the SNE objective by dropping the log 
ij = (cid:107)yi − yj(cid:107)2)  and intro-
adding a weight W− to better capture non-local interactions (e.g. as w−
ducing a scaling hyperparameter λ to control the interplay between two terms.
Here are the objective functions of the described methods:

EEE(X) =(cid:80)
ij (cid:107)xi − xj(cid:107)2 + λ(cid:80)
ESNE(X) =(cid:80)
ij (cid:107)xi − xj(cid:107)2 + log(cid:80)
Et-SNE(X) =(cid:80)
ij log (1 + (cid:107)xi − xj(cid:107)2) + log(cid:80)
ij log (1 + a(cid:107)xi − xj(cid:107)2b) +(cid:80)

i j w+
i j w+

i j w+

i j w+

i j w−
ije−(cid:107)xi−xj(cid:107)2
 
i j=1 e−(cid:107)xi−xj(cid:107)2

 

EUMAP(X) =(cid:80)

1

1+(cid:107)xi−xj(cid:107)2  

i j

i j(w+

ij − 1) log(1 −

1

1+a(cid:107)xi−xj(cid:107)2b ).

(1)

(2)
(3)

(4)

Identifying pressured points

3
Let us consider the optimization with respect to a given point x0 from X. For all the algorithms
the attractive term E+ grows as (cid:107)x0 − xn(cid:107)2 and thus has a high penalty for points placed far away
in the embedding space (especially if they are located nearby in the original space). The repulsive
term E− is mostly localized and concentrated around individual neighbors of x0. As x0 navigates
the landscape of E it tries to get to the minimum of E+ while avoiding the “hills” of E− created
around repulsive neighbors. However  the degrees of freedom of X is limited by d which is typically
much smaller than the intrinsic dimensionality of the data. It might happen that the point gets stuck
surrounded by its non-local neighbors and is unable to ﬁnd a path through.
We can illustrate this with a simple scenario involving three points y0  y1  y2 in the original RD
space  where y0 and y1 are near each other and y2 is further away. We decrease the dimensionality
to d = 1 using EE algorithm and assume that due to e.g. poor initialization x2 is located between
x0 and x1. In the left plot of ﬁg. 2 we show different parts of the objective function as a function
of x0. The attractive term E+(x0) creates a high pressure for x0 to move towards x1. However  the
repulsion between x0 and x2 creates a counter pressure that pushes x0 away from x2  thus creating
two minima: one local near x = −1 and another global near x = 1.5. Points like x0 are trapped in
high energy regions and are not able to move. We argue that these situations are the reason behind
many of the local minima of NLE objective functions. By identifying and repositioning these points
we can improve the objective function and overall the quality of the embedding.
We propose to evaluate the pressure of every point with a very simple and intuitive idea: increased
pressure from the “false” neighbors would create a higher energy for the point to escape that location.
However  for a true local minimum  there are no directions for that point to move. That is  given the
existing number of dimensions. If we were to add a new dimension Z temporarily just for that point 
it would be possible for the points to move along that new dimension (see ﬁg. 2  right). The more
that point is pressured by other points  the farther across this new dimension it would go.
More formally  we say that the point is pressured if the objective function has a nontrivial minimum
when evaluated at that point along the new dimension Z. We deﬁne the minimum ˆz along the
dimension Z as the pressure of that point.

3

-2-101230246810obj.fun.wrtx0E+(x0)E−(x0)E(x0)xx0x2x1N-2-10123-1.5-1-0.500.511.5345678910xx0x2x1ZNIt is important to notice the distinction between pressured points and points that have higher objective
function value when evaluated at those points (a criterion that is used e.g. in Lespinats and Aupetit
[2011] to assess the embedding quality). Large objective function value alone does not necessarily
mean that the point is stuck in a local minimum. First  the point could still be on its way to the
minimum. Second  even for an embedding that represents the global minimum  each point would
converge to its own unique objective function value since the afﬁnities for every point are distinct.
Finally  not every NLE objective function can be easily evaluated for every point separately. SNE
(2) and t-SNE (4) objective functions contain log term that does not allow for easy decoupling.
In what follows we are going to characterize the pressure of each point and look at how the objective
function changes when we add an extra dimension to each of the algorithms described above.
Elastic Embedding. For a given point k we extend the objective function of EE (1) along the new
dimension Z. Notice that we consider points individually one by one  therefore all zi = 0 for all
i (cid:54)= k. The objective function of EE along the new dimension zk becomes:

where d+
The function is symmetric wrt 0 and convex for zk ≥ 0. Its derivative is

i=1 w+

ik  ˜d−

kd+

(5)
ike−(cid:107)xi−xk(cid:107)2 and C is a constant independent from zk.

k + C 

k = (cid:80)N

(cid:101)EEE(zk) = 2z2
k = λ(cid:80)N
i=1 w−
∂(cid:101)EEE(zk)

k e−z2

k + 2 ˜d−
(cid:16)

k − e−z2
d+

k ˜d−

k

∂zk

= 4zk

(6)
The function has a stationary point at zk = 0  which is a minimum when ˜d−
k < d+
k . Otherwise 
k /d+
k ). The magnitude of
zk = 0 is a maximum and the only non-trivial minimum is ˆzk =
the fraction under the log corresponds to the amount of pressure for xk. The numerator ˜d−
k depends
on X and represents the pressure that the neighbors of xk exert on it. The denominator is given by
the diagonal element k of the degree matrix D+ and represents the attraction of the points in the
original high-dimensional space. The fraction is smallest when points are ordered by w−
ik for all
i (cid:54)= k  i.e. ordered by distance from yk. As points change order and move closer to xk (especially
those far in the original space  i.e. with high w−
from a minimum to a maximum  thus creating a pressured point.
Stochastic Neighbor Embedding. The objective along the dimension Z for a point k is given by:

k increases and eventually turns (cid:101)EEE(zk = 0)

ik) ˜d−

log( ˜d−

(cid:101)ESNE(zk) = 2z2
∂(cid:101)ESNE(zk)

kd+

+ C 
where  slightly abusing the notation between different methods  we deﬁne d+
˜d−

k =(cid:80)N

k + log

i=1 e−(cid:107)xi−xk(cid:107)2. The derivative is equal to
k −
d+

(cid:18)

(cid:19)

−z2

n

n

.

∂zk

2(e−z2

k − 1) ˜d−

k +(cid:80)

(cid:17)

˜d−

k = (cid:80)N

i=1 w+

ik and

(cid:16)

k ) < d+

Similarly to EE  the function is convex  has a stationary point at zk = 0  which is a minimum when
k (1−2d+
˜d−
i=1 w+
.
i j(cid:54)=k w+
The LHS represents the pressure of the points on xk normalized by an overall pressure for the rest
of the points. If this pressure gets larger than the similar quantity in the original space (RHS)  the
point becomes pressured with the minimum at ˆzk =

n −2 ˜d−
˜d−

(cid:114)

log

<

n

ik

k

k

ij

(cid:0)(cid:80)

(cid:17)
(cid:113)

.

(cid:80)N
(cid:80)N

t-SNE.
for EE and SNE. The objective along zk and its derivative are given by

t-SNE uses Student’s t distribution which does not decouple as nice as the Gaussian kernel

(cid:101)Et-SNE(zk) = 2(cid:80)N
∂(cid:101)Et-SNE(zk)

∂zk

(cid:0)(cid:80)N
i=1 wik log (K−1

= 4zk

n

2(e

−
˜d
n

= 4zk

−
−z2
k ˜d
e
k
−
k−1) ˜d

−
k (1−2d+
˜d
k )
−
−
n −2 ˜d
˜d
k

k +(cid:80)
(cid:1). It also can be rewritten as
(cid:80)N
(cid:80)N
i=1 exp(−(cid:107)xi−xk(cid:107)2)
i j(cid:54)=k exp(−(cid:107)xi−xj(cid:107)2)
(cid:1) .
(cid:0)(cid:80)
k) + log(cid:0)(cid:80)N
i j(cid:54)=k Kij +(cid:80)N
(cid:80)N
(cid:80)N
i j(cid:54)=k Kij +2(cid:80)N
=(cid:80)N

ik + z2
w+
ik
−1
ik +z2
k

ikKik − (cid:80)N
(cid:80)N

k)−2
−1
ik +z2

−1
ik +z2
i=1(K

i=1 w+

(cid:1).

k)−1

t-SNE(0)
∂2zk

i=1(K

−

d+
k

i=1

i=1

K

K

n

(cid:1) + C.

2
−1
ik +z2
k

where Kij = (1+(cid:107)xi − xj(cid:107)2)−1. The function is convex  but the closed form solution is now harder
to obtain. Practically it can be done with just a few iterations of the Newton’s method initialized at
from the sign of the second derivative at zk = 0: ∂(cid:101)E2
some positive value close to 0. In addition  we can quickly test whether the point is pressured or not

.

i=1 K2
ik
i j=1 Kij

We don’t provide formulas for UMAP due to space limitation  but similarly to t-SNE  UMAP ob-
jective is also convex along zk with zero or one minimum depending on the sign of the second
derivative at zk = 0.

4

Figure 3: Some examples of pressured points for different datasets. Larger marker size corresponds
to the higher pressure value. Color corresponds to the ground truth. Left: SNE embedding of the
swissroll dataset with poor initialization that results in a twist in the middle of the roll. Right: 10
objects from COIL-20 dataset after 100 iteration of EE.
4 Pressured points for quality analysis
The analysis above can be directly applied to the existing algorithms as is  resulting in a qualitative
statistic on the amount of pressure each point is experiencing during the optimization. A nice addi-
tional property is that computing pressured points can be done in constant time by reusing parts of
the gradient. A practitioner can run the analysis for every iteration of the algorithm essentially for
free to see how many points are pressured and whether the embedding results can be trusted.
In ﬁg. 3 we show a couple of examples of embeddings with pressured points computed. The em-
bedding of the swissroll on the left had a poor initialization that SNE was not able to recover from.
Pressured points are concentrated around the twist in the embedding and in the corners  precisely
where the difference with the ground truth occurs. On the right  we can see the embedding of the
subset of COIL-20 dataset midway through optimization with EE. The embeddings of some objects
overlap with each other  which results in high pressure.
In ﬁg. 4 we show an embedding of the subset
from MNIST after 200 iterations of t-SNE. We
highlight some of the digits that ended up in
clusters different from their ground truth. We
put them in a red frame if a digit has a high pres-
sure and in a green frame if their pressure is 0.
For the most part the digits in red squares do not
belong to the clusters where they are currently
located  while digits in green squares look very
similar to the digits around them.
5
Improving convergence
by pressured points optimization
The analysis above can be also used for im-
provements in optimization. Imagine the em-
bedding X has a set of points P that are pres-
sured according to the deﬁnition above. Ef-
fectively it means that given a new dimension
these points would utilize it in order to improve
their current location. Let us create this new
dimension Z with zk (cid:54)= 0 for all k ∈ P. Non-
pressured points can only move along the origi-
nal d dimensions. For example  here is the aug-
mented objective function for EE:

Figure 4: MNIST embedding after 200 iterations
of t-SNE. We highlight two sets of digits located
in clusters different from their ground truth: digits
in red are pressured and look different from their
neighbors; digits in green are non-pressured and
look similar to their neighboring class.

(cid:101)E(X  Z) = E(xj /∈P ) + E

(cid:18)(cid:18)xi
(cid:19)
+ λ(cid:80)

zi

(cid:19)

(cid:16)(cid:80)
i∈P(cid:80)
i(cid:80)
j /∈P w−

+ 2

i∈P
i∈P e−z2

ij (cid:107)xi − xj(cid:107)2

j /∈P w+
ije−(cid:107)xi−xj(cid:107)2

+(cid:80)

(cid:80)

i∈P z2
i

j /∈P w+

ij

.

(7)

(cid:17)

5

NN0123456789NAlgorithm 1: Pressured Points Optimization
Input
Compute a set of pressured points P from X and initialize Z according to their pressure value.
foreach µi ∈ µ do

: Initial X  sequence of regularization steps µ.

repeat

Update X  Z by minimizing min(cid:0)(cid:101)E(X  Z) + µiZZT(cid:1).

Update P using pressured points from new X:
1. Add new points to P according to their pressure value.
2. Remove points that are not pressured anymore.

until convergence;

end
Output: ﬁnal X

The ﬁrst two terms represent the minimization of pressured and non-pressured points independently.
The last term deﬁnes the interaction between pressure and non-pressured points and also has three
terms. The ﬁrst one gives the attraction between pressured and non-pressured points X in d space.
The second term captures the interactions between Z for pressured points and X for non-pressured
ones. On one hand  it pushes Z away from 0 as pressured and non-pressured points move closer to
each other in d space. On the other hand  it re-weights the repulsion between pressured and non-
pressured points proportional to exp (−z2
i ) reducing the repulsion for larger values of zi. In fact 
since exp (−z2) < 1 for all z > 0  the repulsion between pressured and non-pressured points would
always be weaker than the repulsion of non-pressured points between each other. Finally  the last
term pulls each zi to 0 with the weight proportional to the attraction between point i and all the
non-pressured points. Its form is identical to the l2 norm applied to the extended dimension Z with
the weight given by the attraction between point i and all the non-pressured points.
Since our ﬁnal objective is not to ﬁnd the minimum of (7)  but rather get a better embedding of X  we
are going to add a couple of additional steps to facilitate this. First  after each iteration of minimizing
(7) we are going to update P by removing points that stopped being pressured and adding points
that just became pressured. Second  we want pressured points to explore the new dimension only to
the extent that it could eventually help lowering the original objective function. We want to restrict
the use of the new dimension so it would be harder for the points to use it comparing to the original
i . This is an
organic extension since it has the same form as the last term in (7). For µ = 0 the penalty is given
as the weight between pressured and non-pressured points. This property gives an advantage to our
algorithm comparing to the typical use of l2 regularization  where a user has to resort to a trial and
error in order to ﬁnd a perfect µ. In our case  the regularizer already exists in the objective and its
weight sets a natural scale of µ values to try. Another advantage is that large µ values don’t restrict
the algorithm: all the points along Z just collapse to 0 and the algorithm falls back to the original.
Practically  we propose to use a sequence of µ values starting at 0 and increase proportionally to the
magnitude of d+
k d+
k   although a
more aggressive schedule of step = max(d+
k ) could be used
as well. We increase µ up until zk = 0 for all the points. Typically  it occurs after 4–5 steps.
The resulting method is described in Algorithm 1. The algorithm can be embedded and run on top
of the existing optimization methods for NLE: Spectral Direction and N-body methods.
In Spectral Direction the Hessian is approximated using the second derivative of E+. The search

dimensions. It could be achieved by adding l2 penalty to Z dimension as µ(cid:80)

direction has the form P =(cid:0)4L+ + )−1G  where G is the gradient  L+ is the graph Laplacian

k   k = 1 . . . N. In the experiments below  we set step = 1/N(cid:80)

k ) or more conservative step = min(d+

i∈P z2

deﬁned on W+ and  is a small constant that makes the inverse well deﬁned (since graph Laplacian
is psd). The modiﬁed objective that we propose has one more quadratic term µZZT and thus the
Hessian for the pressured points along Z is regularized by 2µ. This is good for two reasons: it
improves the direction of the Spectral Direction by adding new bits of Hessian  and it makes the
Hessian approximation positive deﬁnite  thus avoiding the need to add any constant to it.
Large-scale N-Body approximations using Barnes-Hut [Yang et al.  2013  van der Maaten  2014]
or Fast Multipole Methods (FMM  Vladymyrov and Carreira-Perpi˜n´an  2014) to decrease the cost
of objective function and the gradient from O(N 2) to O(N log N ) or O(N ) by approximating the
interaction between distant points. Pressured points computation uses the same quantities as the
gradient  so whichever approximation is applied carries over to pressured points as well.

6

EE (COIL)

SNE (COIL)

EE (MNIST)

Figure 5: The optimization of COIL-20 using EE (left) and SNE (center)  and optimization of
MNIST using EE (right). Black line shows the SD  green line shows PP initialized at random  blue
line shows PP initialized from the local minima of the SD. Dashed red line indicates the absolute
best result that we were able to get with homotopy optimization. Top plots show the change in the
objective function  while the bottom show the fraction of the pressured points for a given iteration.
Markers ‘o’ indicate change of µ value.
6 Experiments
Here we are going to compare the original optimization algorithm  which we call simply spectral
direction (SD)2 to the Pressured Point (PP) framework deﬁned above using EE and SNE algorithms.
While the proposed methodology could also be applied to t-SNE and UMAP  in practice we were
not able to ﬁnd it useful. t-SNE and UMAP are deﬁned on kernels that have much longer tails than
the Gaussian kernel used in EE and SNE. Because of that  the repulsion between points is much
stronger and points are spread far away from each other. The extra-space given by new dimension is
not utilized well and the objective function decrease is similar with and without the PP modiﬁcation.
For the ﬁrst experiment  we run the algorithm on 10 objects from COIL-20 dataset. We run both SNE
and EE 10 different times with the original algorithm until the objective function does not change for
more than 10−5 per iteration. We then run PP optimization with two different initializations: same
as the original algorithm and initialized from the convergence value of SD. Over 10 runs for EE  SD
got to an average objective function value of 3.84± 0.18  whereas PP with random initialization got
to 3.6 ± 0.14. Initializing from the convergence of SD  10 out of 10 times PP was able to ﬁnd better
local minima with the average objective function value of 3.61 ± 0.19. We got similar results for
SNE: average objective function value for SD is 11.07 ± 0.03  which PP improved to 11.03 ± 0.02
for random initialization and to 11.05 ± 0.03 for initialization from local minima of SD.
In ﬁg. 5 we show the results for one of the runs for EE and SNE for COIL. Notice that for initial
small µ values the algorithm extensively uses and explores the extra dimension  which one can see
from the increase in the original objective function values as well as from the large fraction of the
pressured points. However  for larger µ the number of pressured points drops sharply  eventually
going to 0. Once µ gets large enough so that extra dimension is not used  optimization for every new
µ goes very fast  since essentially nothing is changing.
As another comparison point  we evaluate how much headroom we can get on top improvements
demonstrated by PP algorithm. For that  we run EE on COIL dataset with homotopy method
[Carreira-Perpi˜n´an  2010] where we performed a series of optimizations from a very small λ  where
the objective function has a single global minimum  to ﬁnal λ = 200  each time initializing from the
previous solution. We got the ﬁnal value of the objective function around E = 3.28 (dashed red line
on the EE objective function plot on ﬁg. 5). While we could not get to a same value with PP  we got
very close with E = 3.3 (comparing to E = 3.68 for the best SD optimization).
Finally  on the right plot of ﬁg. 5 we show the minimization of MNIST using FMM approximation
with p = 5 accuracy (i.e. truncating the Hermite functions to 5 terms). PP optimization improved
the convergence both in case of random initialization and for initialization from the solution of SD.
Thus  the beneﬁts of PP algorithm can be increased by also applying SD to improve the optimization
direction and FMM to speed up the objective function and gradient computation.

2It would be more fair to call our method SD+PP  since we also apply spectral direction to minimize the

extended objective function  but we are going to call it simply PP to avoid extra clutter.

7

3.544.555.56EEobj.fun.1111.0511.111.1511.2SNEobj.fun.567891011EE obj fun value0500100015002000Number of iterations00.20.40.60.81Fractionpressured0200400600800Number of iterations00.20.40.60.81Fractionpressured050010001500Numberofiterations00.20.40.60.81Fraction pressuredSpectral Direction embedding

Pressured Point embedding

Figure 6: Embedding of the subset of word2vec data using EE optimized with SD and further reﬁned
by PP. We highlight six word categories that were affected the most by embedding adjustment.

As a ﬁnal experiment  we run the EE for word em-
bedding vectors pretrained using word2vec [Mikolov
et al.  2013] on Google News dataset. The dataset con-
sists of 200 000 word-vectors that were downsampled
to 5 000 most popular English words. We ﬁrst run SD
100 times with different initialization until the embed-
ding does not change by more than 10−5. We then run
PP  initialized from SD. Fig. 6 shows the embedding of
one of the worst results that we got from SD and the
way the embedding improved by running PP algorithm.
We speciﬁcally highlight six different word categories
for which the embedding improved signiﬁcantly. No-
tice that the words from the same category got closer to
each other and formed tighter clusters. Note that more
feelings-oriented categories  such as emotion  sensation
Figure 7: The difference in ﬁnal objective
and nonverbalcommunication got grouped together and
function values between PP and SD for
now occupy the right side of the embedding instead of
100 runs of word2vec dataset using EE
being spread across. In ﬁg. 7 we show the ﬁnal objec-
algorithm. See main text for description.
tive function values for all 100 runs together with the
improvements achieved by continuing the optimization using PP. In the inset  we show the his-
togram of the ﬁnal objective function values of SD and PP. While the very best results of SD have
not improved a lot (suggesting that the near-global minimum has been achieved)  most of the times
SD gets stuck in the higher regions of the objective function that are improved by the PP algorithm.
7 Conclusions
We proposed a novel framework for assessing the quality of most popular manifold learning meth-
ods using intuitive  natural and computationally cheap way to measure the pressure that each point is
experiencing from its neighbors. The pressure is deﬁned as a minimum of objective function when
evaluated along a new extra dimension. We then outlined a method to make use of that extra dimen-
sion in order to ﬁnd a better embedding location for the pressured points. Our proposed algorithm is
able to get to a better solution from a converged local minimum of the existent optimizer as well as
when initialed randomly. An interesting future direction is to extend the analysis beyond one extra
dimension and see if there is a connection to the intrinsic dimensionality of the manifold.
Acknowledgments
I would like to thank Nataliya Polyakovska for initial analysis and Makoto Yamada for useful sug-
gestions that helped improve this work signiﬁcantly.

8

alligatorantcamelchimpanzeecowdeerdolphingoldfishhamsterlionlobstersealwalrusweaselbeltbootclothesglovejacketmasknakedpocketshoeskirtsleevesuitwearblushchuckledrawfrowngigglegringrowlhissnodroarshrugsighsmirkwhistleswallowbakebreakfastbrunchchewchipdrugeggingredientproducesipwinenerveemotionwonderalarmafraidamazeamuseangerappreciateattitudebothercalmcarecuriousdisappointexcitefearfrustrategladinterestmadmoodpanicproudragerelaxstartlestresssurprisesurprisinglyterrifyblastfocushurtpoptoneclickshockstunapparentlyblondeclearlyglowhearpainfulquietseemspotalligatorantcamelcheetahcowdeerdolphinelephantfoxgoldfishhippopotamuskittenlionlobsteroctopusroostersealsparrowweaselbeltbootclothescollarglovehatjacketjeansshoesuitartbarkbeamblushchuckledrawfrowngaspgestureglaregrimacegringrowlgrunthissmumbleshrugsighsignalwavewinceswallowfryapplebakebeerchewchipcookiecreamdessertdineeateggfeedgulpproducesandwichwinenerveemotionalarmafraidamuseangerannoyappreciateawareborebothercalmcareconfusioncuriousdisappointdisturbexcitefearfrightenhappilyhorrormadmoodpanicpleaseprideragerelaxreliefrespectsatisfyscarysurprisinglyupsetblastfocushurtexperiencemusictonesongstudyloseclickstundisplaycoughtraceheatlightcheckblindsenseshowbeautyapparentlyappearblondecomfortdarknessexamineflickerlistenloudnoticeobviousobviouslypeekseesensationslamspotview66.66.626.646.66Original run6.566.586.66.626.646.666.68Pressure Points6.66.76.8Objective function0204060Number of runsOriginal runPressure PointsiterationsReferences
S. Becker  S. Thrun  and K. Obermayer  editors. Advances in Neural Information Processing Systems

(NIPS)  volume 15  2003. MIT Press  Cambridge  MA.

M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.

Neural Computation  15(6):1373–1396  June 2003.

M. ´A. Carreira-Perpi˜n´an. The elastic embedding algorithm for dimensionality reduction. In Proc.

of the 27th Int. Conf. Machine Learning (ICML 2010)  Haifa  Israel  June 21–25 2010.

J. Cook  I. Sutskever  A. Mnih  and G. Hinton. Visualizing similarity data with a mixture of maps.
In M. Meil˘a and X. Shen  editors  Proc. of the 11th Int. Workshop on Artiﬁcial Intelligence and
Statistics (AISTATS 2007)  San Juan  Puerto Rico  Mar. 21–24 2007.

V. de Silva and J. B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction.

In Becker et al. [2003]  pages 721–728.

G. Hinton and S. T. Roweis. Stochastic neighbor embedding. In Becker et al. [2003]  pages 857–864.

R. Kiros  Y. Zhu  R. R. Salakhutdinov  R. Zemel  R. Urtasun  A. Torralba  and S. Fidler. Skip-
thought vectors. In Advances in neural information processing systems  pages 3294–3302  2015.

J. A. Lee and M. Verleysen. Quality assessment of dimensionality reduction: Rank-based criteria.

Neurocomputing  72  2009.

S. Lespinats and M. Aupetit. CheckViz: Sanity check and topological clues for linear and non-linear

mappings. In Computer Graphics Forum  volume 30  pages 113–125  2011.

L. McInnes  J. Healy  and J. Melville. UMAP: Uniform manifold approximation and projection for

dimension reduction. arXiv:1802.03426  2018.

T. Mikolov  I. Sutskever  K. Chen  G. S. Corrado  and J. Dean. Distributed representations of words
and phrases and their compositionality. In Advances in neural information processing systems 
pages 3111–3119  2013.

V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves  M. Ried-
miller  A. K. Fidjeland  G. Ostrovski  et al. Human-level control through deep reinforcement
learning. Nature  518(7540):529  2015.

J. Peltonen and Z. Lin. Information retrieval approach to meta-visualization. Machine Learning  99

(2):189–229  2015.

L. K. Saul and S. T. Roweis. Think globally  ﬁt locally: Unsupervised learning of low dimensional

manifolds. Journal of Machine Learning Research  4:119–155  June 2003.

L. van der Maaten. Accelerating t-sne using tree-based algorithms. Journal of Machine Learning

Research  15:1–21  2014.

L. J. van der Maaten and G. E. Hinton. Visualizing data using t-SNE. Journal of Machine Learning

Research  9:2579–2605  November 2008.

J. Venna  J. Peltonen  K. Nybo  H. Aidos  and S. Kaski. Information retrieval perspective to nonlinear
dimensionality reduction for data visualization. Journal of Machine Learning Research  11:451–
490  Feb. 2010.

M. Vladymyrov and M. ´A. Carreira-Perpi˜n´an. Partial-Hessian strategies for fast learning of nonlin-
ear embeddings. In Proc. of the 29th Int. Conf. Machine Learning (ICML 2012)  pages 345–352 
Edinburgh  Scotland  June 26 – July 1 2012.

M. Vladymyrov and M. ´A. Carreira-Perpi˜n´an. Linear-time training of nonlinear low-dimensional
embeddings. In Proc. of the 17th Int. Workshop on Artiﬁcial Intelligence and Statistics (AISTATS
2014)  pages 968–977  Reykjavik  Iceland  Apr. 22–25 2014.

9

M. Wattenberg  F. Vi´egas  and I. Johnson.

https://distill.pub/2016/misread-tsne/  2016.

How to use t-sne effectively.

Article at

Z. Yang  J. Peltonen  and S. Kaski. Scalable optimization for neighbor embedding for visualization.
In Proc. of the 230h Int. Conf. Machine Learning (ICML 2013)  pages 127–135  Atlanta  GA 
2013.

Z. Yang  J. Peltonen  and S. Kaski. Majorization-minimization for manifold embedding. In Proc. of
the 18th Int. Workshop on Artiﬁcial Intelligence and Statistics (AISTATS 2015)  pages 1088–1097 
Reykjavik  Iceland  May 10–12 2015.

10

,Po-Hsuan (Cameron) Chen
Uri Hasson
Peter Ramadge
Max Vladymyrov