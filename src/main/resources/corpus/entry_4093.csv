2016,Operator Variational Inference,Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically  variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used  the resultant posterior approximation can suffer from undesirable statistical properties. To address this  we reexamine variational inference from its roots as an optimization problem. We use operators  or functions of functions  to design variational objectives. As one example  we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm  operator variational inference (OPVI)  for optimizing any operator objective. Importantly  operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives  such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.,Operator Variational Inference

Rajesh Ranganath
Princeton University

Jaan Altosaar

Princeton University

Dustin Tran

Columbia University

David M. Blei

Columbia University

Abstract

Variational inference is an umbrella term for algorithms which cast Bayesian infer-
ence as optimization. Classically  variational inference uses the Kullback-Leibler
divergence to deﬁne the optimization. Though this divergence has been widely
used  the resultant posterior approximation can suﬀer from undesirable statistical
properties. To address this  we reexamine variational inference from its roots as
an optimization problem. We use operators  or functions of functions  to design
variational objectives. As one example  we design a variational objective with a
Langevin-Stein operator. We develop a black box algorithm  operator variational
inference (opvi)  for optimizing any operator objective. Importantly  operators en-
able us to make explicit the statistical and computational tradeoﬀs for variational
inference. We can characterize diﬀerent properties of variational objectives  such
as objectives that admit data subsampling—allowing inference to scale to massive
data—as well as objectives that admit variational programs—a rich class of pos-
terior approximations that does not require a tractable density. We illustrate the
beneﬁts of opvi on a mixture model and a generative model of images.

1

Introduction

Variational inference is an umbrella term for algorithms that cast Bayesian inference as optimiza-
tion [10]. Originally developed in the 1990s  recent advances in variational inference have scaled
Bayesian computation to massive data [7]  provided black box strategies for generic inference in
many models [19]  and enabled more accurate approximations of a model’s posterior without sac-
riﬁcing eﬃciency [21  20]. These innovations have both scaled Bayesian analysis and removed the
analytic burdens that have traditionally taxed its practice.
Given a model of latent and observed variables p.x; z/  variational inference posits a family of dis-
tributions over its latent variables and then ﬁnds the member of that family closest to the posterior 
p.zj x/. This is typically formalized as minimizing a Kullback-Leibler (kl) divergence from the
approximating family q.(cid:1)/ to the posterior p.(cid:1)/. However  while the kl.q k p/ objective oﬀers many
beneﬁcial computational properties  it is ultimately designed for convenience; it sacriﬁces many de-
sirable statistical properties of the resultant approximation.
When optimizing kl  there are two issues with the posterior approximation that we highlight. First 
it typically underestimates the variance of the posterior. Second  it can result in degenerate solutions
that zero out the probability of certain conﬁgurations of the latent variables. While both of these is-
sues can be partially circumvented by using more expressive approximating families  they ultimately
stem from the choice of the objective. Under the kl divergence  we pay a large price when q.(cid:1)/ is
big where p.(cid:1)/ is tiny; this price becomes inﬁnite when q.(cid:1)/ has larger support than p.(cid:1)/.
In this paper  we revisit variational inference from its core principle as an optimization problem. We
use operators—mappings from functions to functions—to design variational objectives  explicitly
trading oﬀ computational properties of the optimization with statistical properties of the approxima-
tion. We use operators to formalize the basic properties needed for variational inference algorithms.
We further outline how to use them to deﬁne new variational objectives; as one example  we design
a variational objective using a Langevin-Stein operator.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

We develop operator variational inference (opvi)  a black box algorithm that optimizes any operator
objective. In the context of opvi  we show that the Langevin-Stein objective enjoys two good prop-
erties. First  it is amenable to data subsampling  which allows inference to scale to massive data.
Second  it permits rich approximating families  called variational programs  which do not require
analytically tractable densities. This greatly expands the class of variational families and the ﬁdelity
of the resulting approximation. (We note that the traditional kl is not amenable to using variational
programs.) We study opvi with the Langevin-Stein objective on a mixture model and a generative
model of images.
Related Work. There are several threads of research in variational inference with alternative diver-
gences. An early example is expectation propagation (ep) [16]. ep promises approximate minimiza-
tion of the inclusive kl divergence kl.pjjq/ to ﬁnd overdispersed approximations to the posterior.
ep hinges on local minimization with respect to subsets of data and connects to work on ˛-divergence
minimization [17  6]. However  it does not have convergence guarantees and typically does not min-
imize kl or an ˛-divergence because it is not a global optimization method. We note that these
divergences can be written as operator variational objectives  but they do not satisfy the tractability
criteria and thus require further approximations. Li and Turner [14] present a variant of ˛-divergences
that satisfy the full requirements of opvi. Score matching [9]  a method for estimating models by
matching the score function of one distribution to another that can be sampled  also falls into the
class of objectives we develop.
Here we show how to construct new objectives  including some not yet studied. We make explicit the
requirements to construct objectives for variational inference. Finally  we discuss further properties
that make them amenable to both scalable and ﬂexible variational inference.

2 Operator Variational Objectives

We deﬁne operator variational objectives and the conditions needed for an objective to be useful
for variational inference. We develop a new objective  the Langevin-Stein objective  and show how
to place the classical kl into this class. In the next section  we develop a general algorithm for
optimizing operator variational objectives.

2.1 Variational Objectives
Consider a probabilistic model p.x; z/ of data x and latent variables z. Given a data set x  approxi-
mate Bayesian inference seeks to approximate the posterior distribution p.zj x/  which is applied in
all downstream tasks. Variational inference posits a family of approximating distributions q.z/ and
optimizes a divergence function to ﬁnd the member of the family closest to the posterior.
The divergence function is the variational objective  a function of both the posterior and the ap-
proximating distribution. Useful variational objectives hinge on two properties: ﬁrst  optimizing the
function yields a good posterior approximation; second  the problem is tractable when the posterior
distribution is known up to a constant.
The classic construction that satisﬁes these properties is the evidence lower bound (elbo) 

(1)
It is maximized when q.z/ D p.zj x/ and it only depends on the posterior distribution up to a
tractable constant  log p.x; z/. The elbo has been the focus in much of the classical literature. Max-
imizing the elbo is equivalent to minimizing the kl divergence to the posterior  and the expectations
are analytic for a large class of models [4].

Eq.z/Œlog p.x; z/ (cid:0) log q.z/:

2.2 Operator Variational Objectives
We deﬁne a new class of variational objectives  operator variational objectives. An operator objec-
tive has three components. The ﬁrst component is an operator O p;q that depends on p.zj x/ and
q.z/. (Recall that an operator maps functions to other functions.) The second component is a family
of test functions F   where each f .z/ 2 F maps realizations of the latent variables to real vectors
Rd . In the objective  the operator and a function will combine in an expectation Eq.z/Œ.O p;q f /.z/ 
designed such that values close to zero indicate that q is close to p. The third component is a distance

2

function t .a/ W R ! Œ0;1/  which is applied to the expectation so that the objective is non-negative.
(Our example uses the square function t .a/ D a2.)
These three components combine to form the operator variational objective. It is a non-negative
function of the variational distribution 

L.qI O p;q; F ; t / D sup
f 2F

t .Eq.z/Œ.O p;q f /.z//:

(2)

Intuitively  it is the worst-case expected value among all test functions f 2 F . Operator variational
inference seeks to minimize this objective with respect to the variational family q 2 Q.
We use operator objectives for posterior inference. This requires two conditions on the operator and
function family.
1. Closeness. The minimum of the variational objective is at the posterior  q.z/ D p.zj x/. We
meet this condition by requiring that Ep.zj x/Œ.O p;p f /.z/ D 0 for all f 2 F . Thus  optimizing
the objective will produce p.zj x/ if it is the only member of Q with zero expectation (otherwise
it will produce a distribution in the equivalence class: q 2 Q with zero expectation). In practice 
the minimum will be the closest member of Q to p.zj x/.
2. Tractability. We can calculate the variational objective up to a constant without involving the
exact posterior p.zj x/. In other words  we do not require calculating the normalizing constant of
the posterior  which is typically intractable. We meet this condition by requiring that the operator
O p;q—originally in terms of p.zj x/ and q.z/—can be written in terms of p.x; z/ and q.z/.
Tractability also imposes conditions on F : it must be feasible to ﬁnd the supremum. Below  we
satisfy this by deﬁning a parametric family for F that is amenable to stochastic optimization.

Equation 2 and the two conditions provide a mechanism to design meaningful variational objectives
for posterior inference. Operator variational objectives try to match expectations with respect to q.z/
to those with respect to p.zj x/.
2.3 Understanding Operator Variational Objectives
Consider operators where Eq.z/Œ.O p;q f /.z/ only takes positive values. In this case  distance to zero
can be measured with the identity t .a/ D a  so tractability implies the operator need only be known
up to a constant. This family includes tractable forms of familiar divergences like the kl divergence
(elbo)  Rényi’s ˛-divergence [14]  and the (cid:31)-divergence [18].
When the expectation can take positive or negative values  operator variational objectives are closely
(cid:3) that have expectation
related to Stein divergences [2]. Consider a family of scalar test functions F
zero with respect to the posterior  Ep.zj x/Œf
DStein.p; q/ D sup
(cid:3)2F

.z/ D 0. Using this family  a Stein divergence is
(cid:3) jEq.z/Œf

(cid:3)

(cid:3)

.z/ (cid:0) Ep.zj x/Œf

.z/j:

(cid:3)

f

Now recall the operator objective of Equation 2. The closeness condition implies that
t .Eq.z/Œ.O p;q f /.z/ (cid:0) Ep.zj x/Œ.O p;p f /.z//:

L.qI O p;q; F ; t / D sup
f 2F

In other words  operators with positive or negative expectations lead to Stein divergences with a more
generalized notion of distance.

2.4 Langevin-Stein Operator Variational Objective

We developed the operator variational objective. It is a class of tractable objectives  each of which
can be optimized to yield an approximation to the posterior. An operator variational objective is
built from an operator  function class  and distance function to zero. We now use this construction
to design a new type of variational objective.
An operator objective involves a class of functions that has known expectations with respect to an
intractable distribution. There are many ways to construct such classes [1  2]. Here  we construct an
operator objective from the generator Stein’s method applied to the Langevin diﬀusion.

3

>

Let r
ents. Applying the generator method of Barbour [2] to Langevin diﬀusion gives the operator

f denote the divergence of a vector-valued function f   that is  the sum of its individual gradi-

(3)
We call this the Langevin-Stein (ls) operator. We obtain the corresponding variational objective by
using the squared distance function and substituting Equation 3 into Equation 2 

>
.O pls f /.z/ D rz log p.x; z/

f .z/ C r

>

f:

L.qI O pls; F / D sup
f 2F

>
.EqŒrz log p.x; z/

f .z/ C r

>

f /2:

(4)

The ls operator satisﬁes both conditions. First  it satisﬁes closeness because it has expectation zero
under the posterior (Appendix A) and its unique minimizer is the posterior (Appendix B). Second  it
is tractable because it requires only the joint distribution. The functions f will also be a parametric
family  which we detail later.
Additionally  while the kl divergence ﬁnds variational distributions that underestimate the variance 
the ls objective does not suﬀer from that pathology. The reason is that kl is inﬁnite when the support
of q is larger than p; here this is not the case.
We provided one example of a variational objectives using operators  which is speciﬁc to continu-
ous variables. In general  operator objectives are not limited to continuous variables; Appendix C
describes an operator for discrete variables.

2.5 The KL Divergence as an Operator Variational Objective
Finally  we demonstrate how classical variational methods fall inside the operator family. For exam-
ple  traditional variational inference minimizes the kl divergence from an approximating family to
the posterior [10]. This can be construed as an operator variational objective 
KL f /.z/ D log q.z/ (cid:0) log p.zjx/ 8f 2 F :

(5)
This operator does not use the family of functions—it trivially maps all functions f to the same
function. Further  because kl is strictly positive  we use the identity distance t .a/ D a.
The operator satisﬁes both conditions. It satisﬁes closeness because KL.pjjp/ D 0. It satisﬁes
tractability because it can be computed up to a constant when used in the operator objective of Equa-
tion 2. Tractability comes from the fact that log p.zj x/ D log p.z; x/ (cid:0) log p.x/.
3 Operator Variational Inference

.O p;q

We described operator variational objectives  a broad class of objectives for variational inference. We
now examine how it can be optimized. We develop a black box algorithm [27  19] based on Monte
Carlo estimation and stochastic optimization. Our algorithm applies to a general class of models and
any operator objective.
Minimizing the operator objective involves two optimizations: minimizing the objective with respect
to the approximating family Q and maximizing the objective with respect to the function class F
(which is part of the objective).
We index the family Q with variational parameters (cid:21) and require that it satisﬁes properties typically
assumed by black box methods [19]: the variational distribution q.zI (cid:21)/ has a known and tractable
density; we can sample from q.zI (cid:21)/; and we can tractably compute the score function r(cid:21) log q.zI (cid:21)/.
We index the function class F with parameters   and require that f .(cid:1)/ is diﬀerentiable. In the
experiments  we use neural networks  which are ﬂexible enough to approximate a general family of
test functions [8].
Given parameterizations of the variational family and test family  operator variational inference
(opvi) seeks to solve a minimax problem 
D inf

t .E(cid:21)Œ.O p;qf /.z//:

(cid:3)
(cid:21)

sup

(6)

(cid:21)



We will use stochastic optimization [23  13]. In principle  we can ﬁnd stochastic gradients of (cid:21)
by rewriting the objective in terms of the optimized value of   
.(cid:21)/. In practice  however  we

(cid:3)

4

Algorithm 1: Operator variational inference
Input
Output: Variational parameters (cid:21)
Initialize (cid:21) and  randomly.
while not converged do

: Model log p.x; z/  variational approximation q.zI (cid:21)/

Compute unbiased estimates of r(cid:21)L from Equation 7.
Compute unbiased esimates of r L(cid:21) from Equation 8.
Update (cid:21)   with unbiased stochastic gradients.

end

simultaneously solve the maximization and minimization. Though computationally beneﬁcial  this
produces saddle points. In our experiments we found it to be stable enough. We derive gradients for
the variational parameters (cid:21) and test function parameters . (We ﬁx the distance function to be the
square t .a/ D a2; the identity t .a/ D a also readily applies.)
Gradient with respect to (cid:21). For a ﬁxed test function with parameters   denote the objective

The gradient with respect to variational parameters (cid:21) is

L D t .E(cid:21)Œ.O p;q f /.z//:

Now write the second expectation with the score function gradient [19]. This gradient is

r(cid:21)L D 2 E(cid:21)Œ.O p;q f /.z/ r(cid:21)E(cid:21)Œ.O p;q f /.z/:

r(cid:21)L D 2 E(cid:21)Œ.O p;q f /.z/ E(cid:21)Œr(cid:21) log q.zI (cid:21)/.O p;q f /.z/ C r(cid:21).O p;q f /.z/:

(7)
Equation 7 lets us calculate unbiased stochastic gradients. We ﬁrst generate two sets of independent
samples from q; we then form Monte Carlo estimates of the ﬁrst and second expectations. For the
second expectation  we can use the variance reduction techniques developed for black box variational
inference  such as Rao-Blackwellization [19].
We described the score gradient because it is general. An alternative is to use the reparameterization
gradient for the second expectation [11  22]. It requires that the operator be diﬀerentiable with respect
to z and that samples from q can be drawn as a transformation r of a parameter-free noise source (cid:15) 
z D r.(cid:15); (cid:21)/. In our experiments  we use the reparameterization gradient.
Gradient with respect to . Mirroring the notation above  the operator objective for ﬁxed varia-
tional (cid:21) is

The gradient with respect to test function parameters  is

L(cid:21) D t .E(cid:21)Œ.O p;q f /.z//:

r L(cid:21) D 2 E(cid:21)Œ.O p;qf /.z/ E(cid:21)Œr O p;q f .z/:

(8)
Again  we can construct unbiased stochastic gradients with two sets of Monte Carlo estimates. Note
that gradients for the test function do not require score gradients (or reparameterization gradients)
because the expectation does not depend on .
Algorithm. Algorithm 1 outlines opvi. We simultaneously minimize the variational objective with
respect to the variational family q(cid:21) while maximizing it with respect to the function class f. Given
a model  operator  and function class parameterization  we can use automatic diﬀerentiation to calcu-
late the necessary gradients [3]. Provided the operator does not require model-speciﬁc computation 
this algorithm satisﬁes the black box criteria.

3.1 Data Subsampling and opvi

With stochastic optimization  data subsampling scales up traditional variational inference to massive
data [7  26]. The idea is to calculate noisy gradients by repeatedly subsampling from the data set 
without needing to pass through the entire data set for each gradient.

5

An as illustration  consider hierarchical models. Hierarchical models consist of global latent vari-
ables ˇ that are shared across data points and local latent variables zi each of which is associated to
a data point xi. The model’s log joint density is

h log p.xi j zi ; ˇ/ C log p.zi j ˇ/

i

nX

iD1

log p.x1Wn; z1Wn; ˇ/ D log p.ˇ/ C

:

Hoﬀman et al. [7] calculate unbiased estimates of the log joint density (and its gradient) by subsam-
pling data and appropriately scaling the sum.
We can characterize whether opvi with a particular operator supports data subsampling. opvi relies
on evaluating the operator and its gradient at diﬀerent realizations of the latent variables (Equation 7
and Equation 8). Thus we can subsample data to calculate estimates of the operator when it derives
from linear operators of the log density  such as diﬀerentiation and the identity. This follows as a
linear operator of sums is a sum of linear operators  so the gradients in Equation 7 and Equation 8
decompose into a sum. The Langevin-Stein and kl operator are both linear in the log density; both
support data subsampling.

3.2 Variational Programs

Given an operator and variational family  Algorithm 1 optimizes the corresponding operator objec-
tive. Certain operators require the density of q. For example  the kl operator (Equation 5) requires
its log density. This potentially limits the construction of rich variational approximations for which
the density of q is diﬃcult to compute.1
Some operators  however  do not depend on having a analytic density; the Langevin-Stein (ls) oper-
ator (Equation 3) is an example. These operators can be used with a much richer class of variational
approximations  those that can be sampled from but might not have analytically tractable densities.
We call such approximating families variational programs.
Inference with a variational program requires the family to be reparameterizable [11  22]. (Otherwise
we need to use the score function  which requires the derivative of the density.) A reparameteriz-
able variational program consists of a parametric deterministic transformation R of random noise (cid:15).
Formally  let

(cid:15) (cid:24) Normal.0; 1/;

(9)
This generates samples for z  is diﬀerentiable with respect to (cid:21)  and its density may be intractable. For
operators that do not require the density of q  it can be used as a powerful variational approximation.
This is in contrast to the standard Kullback-Leibler (kl) operator.
As an example  consider the following variational program for a one-dimensional random variable.
Let (cid:21)i denote the ith dimension of (cid:21) and make the corresponding deﬁnition for (cid:15):

z D R.(cid:15)I (cid:21)/:

z D .(cid:15)3 > 0/R.(cid:15)1I (cid:21)1/ (cid:0) .(cid:15)3  0/R.(cid:15)2I (cid:21)2/:

(10)
When R outputs positive values  this separates the parametrization of the density to the positive
and negative halves of the reals; its density is generally intractable. In Section 4  we will use this
distribution as a variational approximation.
Equation 9 contains many densities when the function class R can approximate arbitrary continuous
functions. We state it formally.
Theorem 1. Consider a posterior distribution p.zj x/ with a ﬁnite number of latent variables and
continuous quantile function. Assume the operator variational objective has a unique root at the
posterior p.zj x/ and that R can approximate continuous functions. Then there exists a sequence
of parameters (cid:21)1; (cid:21)2 : : : ; in the variational program  such that the operator variational objective
converges to 0  and thus q converges in distribution to p.zj x/.
This theorem says that we can use variational programs with an appropriate q-independent operator
to approximate continuous distributions. The proof is in Appendix D.

1It is possible to construct rich approximating families with kl.qjjp/  but this requires the introduction of

an auxiliary distribution [15].

6

4 Empirical Study

We evaluate operator variational inference on a mixture of Gaussians  comparing diﬀerent choices
in the objective. We then study logistic factor analysis for images.

4.1 Mixture of Gaussians

one-dimensional mixture
Normal.zI(cid:0)3; 1/ C 1

Consider
a
interest 
Normal.zI 3; 1/. The posterior contains multiple modes.
p.z/ D 1
We seek to approximate it with three variational objectives: Kullback-Leibler (kl) with a Gaussian
approximating family  Langevin-Stein (ls) with a Gaussian approximating family  and ls with a
variational program.

posterior

of

of Gaussians

as

the

2

2

Figure 1: The true posterior is a mixture of two Gaussians  in green. We approximate it with a
Gaussian using two operators (in blue). The density on the far right is a variational program given
in Equation 10 and using the Langevin-Stein operator; it approximates the truth well. The density
of the variational program is intractable. We plot a histogram of its samples and compare this to the
histogram of the true posterior.

Figure 1 displays the posterior approximations. We ﬁnd that the kl divergence and ls divergence
choose a single mode and have slightly diﬀerent variances. These operators do not produce good
results because a single Gaussian is a poor approximation to the mixture. The remaining distribution
in Figure 1 comes from the toy variational program described by Equation 10 with the ls operator.
Because this program captures diﬀerent distributions for the positive and negative half of the real
line  it is able to capture the posterior.
In general  the choice of an objective balances statistical and computational properties of variational
the ls objective admits the use of a variational program;
inference. We highlight one tradeoﬀ:
however  the objective is more diﬃcult to optimize than the kl.

4.2 Logistic Factor Analysis

Logistic factor analysis models binary vectors xi with a matrix of parameters W and biases b 

zi (cid:24) Normal.0; 1/
>
xi;k (cid:24) Bernoulli.(cid:27) .w
k zi C bk//;

where zi has ﬁxed dimension K and (cid:27) is the sigmoid function. This model captures correlations of
the entries in xi through W .
We apply logistic factor analysis to analyze the binarized MNIST data set [24]  which contains 28x28
binary pixel images of handwritten digits. (We set the latent dimensionality to 10.) We ﬁx the model
parameters to those learned with variational expectation-maximization using the kl divergence  and
focus on comparing posterior inferences.
We compare the kl operator to the ls operator and study two choices of variational models: a fully
factorized Gaussian distribution and a variational program. The variational program generates sam-
ples by transforming a K-dimensional standard normal input with a two-layer neural network  using
rectiﬁed linear activation functions and a hidden size of twice the latent dimensionality. Formally 

7

(cid:0)505ValueofLatentVariablezKLTruth(cid:0)505ValueofLatentVariablezLangevin-SteinTruth(cid:0)505ValueofLatentVariablezVariationalProgramTruthInference method
Mean-ﬁeld Gaussian + kl
Mean-ﬁeld Gaussian + ls
Variational Program + ls

Completed data log-likelihood

-59.3
-75.3
-58.9

Table 1: Benchmarks on logistic factor analysis for binarized MNIST. The same variational approx-
imation with ls performs worse than kl on likelihood performance. The variational program with
ls performs better without directly optimizing for likelihoods.

the variational program we use generates samples of z as follows:

z0 (cid:24) Normal.0; I /
>
h0 D ReLU.W q
>
h1 D ReLU.W q
z D W q

z0 C bq
0/
h0 C bq
1/
h1 C bq
2:

>

0

1

2

The variational parameters are the weights W q and biases bq. For f   we use a three-layer neural net-
work with the same hidden size as the variational program and hyperbolic tangent activations where
unit activations were bounded to have norm two. Bounding the unit norm bounds the divergence.
(cid:0)5 for the variational
We used the Adam optimizer [12] with learning rates 2(cid:2) 10
approximation.
There is no standard for evaluating generative models and their inference algorithms [25]. Following
Rezende et al. [22]  we consider a missing data problem. We remove half of the pixels in the test set
(at random) and reconstruct them from a ﬁtted posterior predictive distribution. Table 1 summarizes
the results on 100 test images; we report the log-likelihood of the completed image. ls with the
variational program performs best. It is followed by kl and the simpler ls inference. The ls performs
better than kl even though the model parameters were learned with kl.

(cid:0)4 for f and 2(cid:2) 10

5 Summary

We present operator variational objectives  a broad yet tractable class of optimization problems for
approximating posterior distributions. Operator objectives are built from an operator  a family of
test functions  and a distance function. We outline the connection between operator objectives and
existing divergences such as the KL divergence  and develop a new variational objective using the
Langevin-Stein operator. In general  operator objectives produce new ways of posing variational
inference.
Given an operator objective  we develop a black box algorithm for optimizing it and show which
operators allow scalable optimization through data subsampling. Further  unlike the popular evidence
lower bound  not all operators explicitly depend on the approximating density. This permits ﬂexible
approximating families  called variational programs  where the distributional form is not tractable.
We demonstrate this approach on a mixture model and a factor model of images.
There are several possible avenues for future directions such as developing new variational objectives 
adversarially learning [5] model parameters with operators  and learning model parameters with
operator variational objectives.
Acknowledgments.
This work is supported by NSF IIS-1247664  ONR N00014-11-1-0651 
DARPA FA8750-14-2-0009  DARPA N66001-15-C-4032  Adobe  NSERC PGS-D  Porter Ogden
Jacobus Fellowship  Seibel Foundation  and the Sloan Foundation. The authors would like to thank
Dawen Liang  Ben Poole  Stephan Mandt  Kevin Murphy  Christian Naesseth  and the anonymous
reviews for their helpful feedback and comments.

References
[1] Assaraf  R. and Caﬀarel  M. (1999). Zero-variance principle for monte carlo algorithms. In Phys. Rev. Let.
[2] Barbour  A. D. (1988). Stein’s method and poisson process convergence. Journal of Applied Probability.

8

[3] Carpenter  B.  Hoﬀman  M. D.  Brubaker  M.  Lee  D.  Li  P.  and Betancourt  M. (2015). The Stan Math

Library: Reverse-mode automatic diﬀerentiation in C++. arXiv preprint arXiv:1509.07164.

[4] Ghahramani  Z. and Beal  M. (2001). Propagation algorithms for variational Bayesian learning. In NIPS

13  pages 507–513.

[5] Goodfellow  I.  Pouget-Abadie  J.  Mirza  M.  Xu  B.  Warde-Farley  D.  Ozair  S.  Courville  A.  and Bengio 

Y. (2014). Generative adversarial nets. In Neural Information Processing Systems.

[6] Hernández-Lobato  J. M.  Li  Y.  Rowland  M.  Hernández-Lobato  D.  Bui  T.  and Turner  R. E. (2015).

Black-box ˛-divergence Minimization. arXiv.org.

[7] Hoﬀman  M.  Blei  D.  Wang  C.  and Paisley  J. (2013). Stochastic variational inference. Journal of

Machine Learning Research  14(1303–1347).

[8] Hornik  K.  Stinchcombe  M.  and White  H. (1989). Multilayer feedforward networks are universal ap-

proximators. Neural networks  2(5):359–366.

[9] Hyvärinen  A. (2005). Estimation of non-normalized statistical models by score matching. Journal of

Machine Learning Research  6(Apr):695–709.

[10] Jordan  M.  Ghahramani  Z.  Jaakkola  T.  and Saul  L. (1999). Introduction to variational methods for

graphical models. Machine Learning  37:183–233.

[11] Kingma  D. and Welling  M. (2014). Auto-encoding variational bayes. In (ICLR).
[12] Kingma  D. P. and Ba  J. (2014). Adam: A method for stochastic optimization. CoRR  abs/1412.6980.
[13] Kushner  H. and Yin  G. (1997). Stochastic Approximation Algorithms and Applications. Springer New

York.

[14] Li  Y. and Turner  R. E. (2016). Rényi divergence variational inference. arXiv preprint arXiv:1602.02311.
[15] Maaløe  L.  Sønderby  C. K.  Sønderby  S. K.  and Winther  O. (2016). Auxiliary deep generative models.

arXiv preprint arXiv:1602.05473.

[16] Minka  T. P. (2001). Expectation propagation for approximate Bayesian inference. In UAI.
[17] Minka  T. P. (2004). Power EP. Technical report  Microsoft Research  Cambridge.
[18] Nielsen  F. and Nock  R. (2013). On the chi square and higher-order chi distances for approximating

f-divergences. arXiv preprint arXiv:1309.3029.

[19] Ranganath  R.  Gerrish  S.  and Blei  D. (2014). Black Box Variational Inference. In AISTATS.
[20] Ranganath  R.  Tran  D.  and Blei  D. M. (2016). Hierarchical variational models. In International Con-

ference on Machine Learning.

[21] Rezende  D. J. and Mohamed  S. (2015). Variational inference with normalizing ﬂows. In Proceedings of

the 31st International Conference on Machine Learning (ICML-15).

[22] Rezende  D. J.  Mohamed  S.  and Wierstra  D. (2014). Stochastic backpropagation and approximate

inference in deep generative models. In International Conference on Machine Learning.

[23] Robbins  H. and Monro  S. (1951). A stochastic approximation method. The Annals of Mathematical

Statistics  22(3):pp. 400–407.

[24] Salakhutdinov  R. and Murray  I. (2008). On the quantitative analysis of deep belief networks. In Interna-

tional Conference on Machine Learning.

[25] Theis  L.  van den Oord  A.  and Bethge  M. (2016). A note on the evaluation of generative models. In

International Conference on Learning Representations.

[26] Titsias  M. and Lázaro-Gredilla  M. (2014). Doubly stochastic variational bayes for non-conjugate infer-
ence. In Proceedings of the 31st International Conference on Machine Learning (ICML-14)  pages 1971–
1979.

[27] Wingate  D. and Weber  T. (2013). Automated variational inference in probabilistic programming. ArXiv

e-prints.

9

,Rajesh Ranganath
Dustin Tran
Jaan Altosaar
David Blei