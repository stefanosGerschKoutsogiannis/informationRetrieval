2012,The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes,Stochastic differential equations (SDE) are a natural tool for modelling systems that are inherently noisy or contain uncertainties that can be modelled as stochastic processes. Crucial to the process of using SDE to build mathematical models is the ability to estimate parameters of those models from observed data. Over the past few decades  significant progress has been made on this problem  but we are still far from having a definitive solution. We describe a novel method of approximating a diffusion process that we show to be useful in Markov chain Monte-Carlo (MCMC) inference algorithms. We take the ‘white’ noise that drives a diffusion process and decompose it into two terms. The first is a ‘coloured noise’ term that can be deterministically controlled by a set of auxilliary variables. The second term is small and enables us to form a linear Gaussian ‘small noise’ approximation. The decomposition allows us to take a diffusion process of interest and cast it in a form that is amenable to sampling by MCMC methods. We explain why many state-of-the-art inference methods fail on highly nonlinear inference problems. We demonstrate experimentally that our method performs well in such situations. Our results show that this method is a promising new tool for use in inference and parameter estimation problems.,The Coloured Noise Expansion and Parameter

Estimation of Diffusion Processes

Simon M.J. Lyons
School of Informatics
University of Edinburgh

10 Crichton Street  Edinburgh  EH8 9AB

S.Lyons-4@sms.ed.ac.uk

Simo S¨arkk¨a
Aalto University

Department of Biomedical Engineering

and Computational Science

Rakentajanaukio 2  02150 Espoo

simo.sarkka@aalto.fi

Amos J. Storkey

School of Informatics
University of Edinburgh

10 Crichton Street  Edinburgh  EH8 9AB

a.storkey@ed.ac.uk

Abstract

Stochastic differential equations (SDE) are a natural tool for modelling systems
that are inherently noisy or contain uncertainties that can be modelled as stochastic
processes. Crucial to the process of using SDE to build mathematical models
is the ability to estimate parameters of those models from observed data. Over
the past few decades  signiﬁcant progress has been made on this problem  but
we are still far from having a deﬁnitive solution. We describe a novel method
of approximating a diffusion process that we show to be useful in Markov chain
Monte-Carlo (MCMC) inference algorithms. We take the ‘white’ noise that drives
a diffusion process and decompose it into two terms. The ﬁrst is a ‘coloured
noise’ term that can be deterministically controlled by a set of auxilliary variables.
The second term is small and enables us to form a linear Gaussian ‘small noise’
approximation. The decomposition allows us to take a diffusion process of interest
and cast it in a form that is amenable to sampling by MCMC methods. We explain
why many state-of-the-art inference methods fail on highly nonlinear inference
problems  and we demonstrate experimentally that our method performs well in
such situations. Our results show that this method is a promising new tool for use
in inference and parameter estimation problems.

1

Introduction

Diffusion processes are a ﬂexible and useful tool in stochastic modelling. Many important real world
systems are currently modelled and best understood in terms of stochastic differential equations in
general and diffusions in particular. Diffusions have been used to model prices of ﬁnancial instru-
ments [1]  chemical reactions [2]  ﬁring patterns of individual neurons [3]  weather patterns [4] and
fMRI data [5  6  7] among many other phenomena.
The analysis of diffusions dates back to Feller and Kolmogorov  who studied them as the scaling
limits of certain Markov processes (see [8]). The theory of diffusion processes was revolutionised
by Itˆo  who interpreted a diffusion process as the solution to a stochastic differential equation [9 
10]. This viewpoint allows one to see a diffusion process as the randomised counterpart of an
ordinary differential equation. One can argue that stochastic differential equations are the natural

1

tool for modelling continuously evolving systems of real valued quantities that are subject to noise
or stochastic inﬂuences.
The classical approach to mathematical modelling starts with a set of equations that describe the
evolution of a system of interest. These equations are goverened by a set of input parameters (for
example particle masses  reaction rates  or more general constants of proportionality) that determine
the behaviour of the system. For practical purposes  it is of considerable interest to solve the inverse
problem. Given the output of some system  what can be said about the parameters that govern it?
In the present setting  we observe data which we hypothesize are generated by a diffusion. We would
like to know what the nature of this diffusion is. For example  we may begin with a parametric model
of a physical system  with a prior distribution over the parameters. In principle  one can apply Bayes’
theorem to deduce the posterior distribution. In practice  this is computationally prohibitive: it is
necessary to solve a partial differential equation known as the Fokker-Planck equation (see [11]) in
order to ﬁnd the transition density of the diffusion of interest. This solution is rarely available in
closed form  and must be computed numerically.
In this paper  we propose a novel approximation for a nonlinear diffusion process X. One heuristic
way of thinking about a diffusion is as an ordinary differential equation that is perturbed by white
noise. We demonstrate that one can replace the white noise by a ‘coloured’ approximation without
inducing much error. The nature of the coloured noise expansion method enables us to control the
behaviour of the diffusion over various length-scales. This allows us to produce samples from the
diffusion process that are consistent with observed data. We use these samples in a Markov chain
Monte-Carlo (MCMC) inference algorithm.
The main contributions of this paper are:

• Novel development of a method for sampling from the time-t marginal distribution of a
• Demonstration that this approximation is a powerful and scalable tool for making parameter

diffusion process based on a ‘coloured’ approximation of white noise.

estimation feasible for general diffusions at minimal cost.

The paper is structured as follows:
In
Section 3 we conduct a brief survey of existing approaches to the problem. In Section 4  we discuss
the coloured noise expansion and its use in controlling the behaviour of a diffusion process. Our
inference algorithm is described in Section 5. We describe some numerical experiments in Section 6 
and future work is discussed in Section 7.

in Section 2  we describe the structure of our problem.

2 Parametric Diffusion Processes

In this section we develop the basic notation and formalism for the diffusion processes used in this
work. First  we assume our data are generated by observing a k-dimensional diffusion processes
with dynamics

dXt = aθ(Xt)dt + BθdWt 

(1)
where the initial condition is drawn from some known distribution. Observations are assumed to
occur at times t1  . . .   tn  with ti−ti−1 := Ti. We require that aθ : IRk → IRk is sufﬁciently regular
to guarantee the existence of a unique strong solution to (1)  and we assume Bθ ∈ IRk×d. Both terms
depend on a set of potentially unknown parameters θ ∈ IRdθ. We impose a prior distribution p(θ)
on the parameters. The driving noise W is a d-dimensional Brownian motion  and the equation is
interpreted in the Itˆo sense. Observations are subject to independent Gaussian perturbations centered
at the true value of X. That is 

X0 ∼ p(x0) 

Yti = Xti + ti 

ti ∼ N (0  Σi)

(2)

We use the notation X to refer to the entire sample path of the diffusion  and Xt to denote the value
of the process at time t. We will also employ the shorthand Y1:n = {Yt1  . . .   Ytn}.
Many systems can be modelled using the form (1). Such systems are particularly relevant in physics
and natural sciences. In situations where this is not explicitly the case  one can often hope to reduce a
diffusion to this form via the Lamperti transform. One can almost always accomplish this in the uni-
variate case  but the multivariate setting is somewhat more involved. A¨ıt-Sahalia [12] characterises
the set of multivariate diffusions to which this transform can be applied.

2

3 Background Work

Most approaches to parameter estimation of diffusion processes rely on the Monte-Carlo approxi-
mation. Beskos et al. [13] [14] employ a method based on rejection sampling to estimate parameters
without introducing any discretisation error. Golightly and Wilkinson [15] extend the work of Chib
et al. [16] and Durham and Gallant [17] to construct a Gibbs sampler that can be applied to the
parameter estimation problem.
Roughly speaking  Gibbs samplers that exist in the literature alternate between drawing samples
from some representation of the diffusion process X conditional on parameters θ  and samples from
θ conditional on the current sample path of X. Note that draws from X must be consistent with the
observations Y1:n.
The usual approach to the consistency issue is to make a proposal by conditioning a linear diffusion
to hit some neighbourhood of the observation Yk  then to make a correction via a rejection sam-
pling [18] or a Metropolis-Hastings [16] step. However  as the inter-observation time grows  the
qualitative difference between linear and nonlinear diffusions gets progressively more pronounced 
and the rate of rejection grows accordingly. Figure 1 shows the disparity between a sample from a
nonlinear process and a sample from the linear proposal. One can see that the target sample path is
constrained to stay near the mode γ = 2.5  whereas the proposal can move more freely. One should
expect to make many proposals before ﬁnding one that ‘behaves’ like a typical draw from the true
process.

(a)

(b)

Figure 1: (a) Sample path of a double well process (see equation (18)) with α = 2  γ = 2.5  B = 2
(blue line). Current Gibbs samplers use linear proposals (dashed red line) with a rejection step to
draw conditioned nonlinear paths. In this case  the behaviour of the proposal is very different to that
of the target  and the rate of rejection is high.
(b) Sample path of a double well process (solid blue line) with noisy observations (red dots). We
use this as an initial dataset on which to test our algorithm. Parameters are α = 2  γ = 1  B = 1.
Observation errors have variance Σ = .25.

For low-dimensional inference problems  algorithms that employ sequential Monte-Carlo (SMC)
methods [19] [20] typically yield good results. However  unlike the Gibbs samplers mentioned
above  SMC-based methods often do not scale well with dimension. The number of particles that
one needs to maintain a given accuracy is known to scale exponentially with the dimension of the
problem [21].
A¨ıt-Sahalia [12  22] uses a deterministic technique based on Edgeworth expansions to approximate
the transition density. Other approaches include variational methods [23  24] that can compute
continuous time Gaussian process approximations to more general stochastic differential systems 
as well as various non-linear Kalman ﬁltering and smoothing based approximations [25  26  27] .

4 Coloured Noise Expansions and Brownian Motion

We now introduce a method of approximating a nonlinear diffusion that allows us to gain a con-
siderable amount of control over the behaviour of the process. Similar methods have been used

3

0123−3−2−101tX(t)Nonlinear sample path and proposal05101520−3−2−1012tX(t)Sample path with noisy observationsfor stratiﬁed sampling of diffusion processes [28] and the solution of stochastic partial differential
equations [29] . One of the major challenges of using MCMC methods for parameter estimation
in the present context is that it is typically very difﬁcult to draw samples from a diffusion process
conditional on observed data. If one only knows the initial condition of a diffusion  then it is straight-
forward to simulate a sample path of the process. However  simulating a sample path conditional on
both initial and ﬁnal conditions is a challenging problem.
Our approximation separates the diffusion process X into the sum of a linear and nonlinear compo-
nent. The linear component of the sum allows us to condition the approximation to ﬁt observed data
more easily than in conventional methods. On the other hand  the nonlinear component captures
the ‘gross’ variation of a typical sample path. In this section  we ﬁx a generic time interval [0  T ] 
though one can apply the same derivation for any given interval Ti = ti − ti−1.
Heuristically  one can think of the random process that drives the process deﬁned in equation (1) as
white noise. In our approximation  we project this white noise into an N-dimensional subspace of
L2[0  T ]  the Hilbert space of square-integrable functions deﬁned on the interval [0  T ]. This gives
a ‘coloured noise’ process that approaches white noise asymptotically as N → ∞. The coloured
noise process is then used to drive an approximation of (1). We can choose the space into which
to project the white noise in such a way that we will gain some control over its behaviour. This is
analagous to the way that Fourier analysis allows us to manipulate properties of signals
Recall that a standard Brownian motion on the interval [0  T ] is a one-dimentional Gaussian process
with zero mean and covariance function k(s  t) = min{s  t}. By deﬁnition of the Itˆo integral  we
can write

(cid:90) t

(cid:90) T

(3)
Suppose {φi}i≥1 is an orthonormal basis of L2[0  T ]. We can interpret the indicator function in (3)
as an element of L2[0  T ] and expand it in terms of the basis functions as follows:

dWs =

Wt =

0

0

I[0 t](s)dWs.

∞(cid:88)

I[0 t](s) =

(cid:104)I[0 t](·)  φi(·)(cid:105)φi(s) =

φi(u)du

φi(s).

(cid:19)

(cid:18)(cid:90) t
∞(cid:88)
(cid:33)(cid:90) t

i=1

0

0

φi(s)dWs

φi(u)du.

(4)

(5)

i=1

Substituting (4) into (3)  we see that

(cid:32)(cid:90) T
∞(cid:88)
We will employ the shorthand Zi =(cid:82) T

Wt =

i=1

0

0 φi(s)dWs. Since the functions {φi} are deterministic and
orthonormal  we know from standard results of Itˆo calculus that the random variables {Zi} are i.i.d
standard normal.
The inﬁnite series in equation (5) can be truncated after N terms to derive an approximation  ˆWt of
Brownian motion. Taking the derivative with respect to time  the result is a ‘coloured’ approximation
of white noise  taking the form

d ˆWt
dt

=

Ziφi(t).

(6)

The multivariate approximation is similar. We seperate a d-dimensional Brownian motion into one-
dimensional components and decompose the individual components as in (6). In principle  one can
choose a different value of N for each component of the Brownian motion  but for ease of exposition
we do not do so here. We can substitute this approximation into equation (1)  which gives

dXNL

t
dt

= aθ(XNL

t ) + Bθ

ΦiZi 

0 ∼ p(x0) 
XNL

(7)

where Φi is the diagonal d × d matrix with entries (φi1  . . .   φid)  and Zi = (Zi1  . . .   Zid)
(cid:124).
This derivation is useful because equation (7) gives us an alternative to the Euler-Maruyama discreti-
sation for sampling approximately from the time-t marginal distribution of a diffusion process. We

N(cid:88)

i=1

N(cid:88)

i=1

4

√

draw coefﬁcients Zij from a standard normal distribution  and solve the appropriate vector-valued
ordinary differential equation. While the Euler discretisation is the de facto standard method for
numerical approximation of SDE  other methods do exist. Kloeden and Platen [30] discuss higher
order methods such as the stochastic Runge-Kutta scheme [31].
In the Euler-Maruyama approximation  one discretises the driving Brownian motion into increments
Wti −Wti−1 =
TiZi. One must typically employ a ﬁne discretisation to get a good approximation
to the true diffusion process. Empirically  we ﬁnd that one needs far fewer Gaussian inputs Zi for
an accurate representation of XT using the coloured noise approximation. This more parsimonious
representation has advantages. For example  Corlay and Pages [28] employ related ideas to conduct
stratiﬁed sampling of a diffusion process.
The coefﬁcients Zi are also more amenable to interpretation than the Gaussian increments in the
Euler-Maruyama expansion. Suppose we have a one-dimensional process in which we use the
Fourier cosine basis

φk(t) =(cid:112)2/T cos((2k − 1)πt/2T ).

(8)

If we change Z1 while holding the other coefﬁcients ﬁxed  we will typically see a change in the
large-scale behaviour of the path. On the other hand  a change in ZN will typically result in a
change to the small-scale oscillations in the path. The seperation of behaviours across coefﬁcients
gives us a means to obtain ﬁne-grained control over the behaviour of a diffusion process within a
Metropolis-Hastings algorithm.
We can improve our approximation by attempting to correct for the fact that we truncated the sum
in equation (6). Instead of simply discarding the terms ZiΦi for i > N  we attempt to account
for their effect as follows. We assume the existence of some ‘correction’ process XC such that
X = XNL + XC. We know that the dynamics of X satisfy

Taylor expanding the drift term around XNL  we see that to ﬁrst order 

dXt = aθ

(cid:0)XNL
(cid:19)
(cid:1) + Ja(XNL
(cid:1) + Bθd ˆWt

t )XC

(cid:19)

t

(cid:18)
(cid:18)

dXt ≈

aθ

=

aθ

(cid:0)XNL
(cid:0)XNL

t

t

(cid:1) dt + BθdWt.

t + XC

t

dt + BθdWt

dt + Ja(XNL

t )XC

t dt + Bθ

(9)

(10)

(cid:16)

(cid:17)

.

dWt − d ˆWt

Here  Ja(x) is the Jacobian matrix of the function a evaluated at x. This motivates the use of a linear
time-dependent approximation to the correction process. We will refer to this linear approximation
as XL. The dynamics of XL satisfy

dXL

t = Ja(XNL

(11)
where the driving noise is the ‘residual’ term R = W − ˆW. Conditional on XNL  XL is a lin-
ear Gaussian process  and equation (11) can be solved in semi-closed form. First  we compute a
numerical approximation to the solution of the homogenous matrix-valued equation

t dt + BθdRt 

0 = 0 

t )XL

XL

d
dt

Ψ(t) = Ja(XNL

(12)
One can compute Ψ−1(t) in a similar fashion via the relationship dΨ−1/dt = −Ψ−1(dΨ/dt)Ψ−1.
We then have

Ψ(0) = In.

t )Ψ(t) 

XL

t = Ψ(t)

= Ψ(t)

Ψ(u)−1BdRu

Ψ(u)−1BdWu − N(cid:88)

Ψ(t)

(cid:19)

Ψ(u)−1BΦi(u)du

(cid:18)(cid:90) t

0

Zi.

(13)

(cid:90) t
(cid:90) t

0

0

i=1

5

It follows that XL has mean 0 and covariance
(cid:124)
Ψ

(cid:124)
Ψ(u)−1BB

k(s  t) = Ψ(s)

(u)−1du

(t)

(cid:18)(cid:90) s∧t
(cid:18)(cid:90) s

0

Ψ(s)

− N(cid:88)

(cid:19)
(cid:19)(cid:18)(cid:90) t

Ψ

(cid:124)

(cid:19)(cid:124)

Ψ(u)−1BΦi(u)du

Ψ(u)−1BΦi(u)du

(cid:124)

Ψ

(t).

(14)

i=1

0

0

The process XNL is designed to capture the most signiﬁcant nonlinear features of the original diffu-
sion X  while the linear process XL corrects for the truncation of the sum (6)  and can be understood
using tools from the theory of Gaussian processes. One can think of the linear term as the result of
a ‘small-noise’ expansion about the nonlinear trajectory. Small-noise techniques have been applied
to diffusions in the past [11]  but the method described above has the advantage of being inherently
nonlinear. In the supplement to this paper  we show that ˆX = XNL + XL converges to X in L2[0  T ]
as N → ∞ under the assumption that a is Lipschitz continuous. If the drift function is linear  then
ˆX = X regardless of the choice of N.

5 Parameter Estimation

p(cid:0)θ  Z1:N| XNL

(cid:1) ∝ N (Y1 | XNL

In this section  we describe a novel modiﬁcation of the Gibbs sampler that does not suffer the draw-
backs of the linear proposal strategy. In Section 6  we demonstrate that for highly nonlinear problems
it will perform signiﬁcantly better than standard methods because of the nonlinear component of our
approximation.
Suppose for now that we make a single noiseless observation at time t1 = T (for ease of notation 
we will assume that observations are uniformly spaced through time with ti+1 − ti = T   though this
is not necessary). Our aim is to sample from the posterior distribution

1   k1(T  T ))N (Z1:N )p(θ).

1 + XL

1 = Y1

(15)
We adopt the convention that N (·| µ  Σ) represents the normal distribution with mean µ and covari-
ance Σ  whereas N (·) represents the standard normal distribution. Note that we have left dependence
of k1 on Z and θ implicit. The right-hand side of this expression allows us to evaluate the posterior
up to proportionality; hence it can be targeted with a Metropolis-Hastings sampler.
With multiple observations  the situation is similar. However  we now have a set of Gaussian inputs
Z(i) for each transition ˆXi| ˆXi−1. If we attempt to update θ and {Z(i)}i≤n all at once  the rate of
rejection will be unacceptably high. For this reason  we update each Z(i) in turn  holding θ and
the other Gaussian inputs ﬁxed. We draw Z(i)∗ from the proposal distribution  and compute XNL∗
with initial condition Yi−1. We also compute the covariance k∗
i (T  T ) of the linear correction. The
acceptance probability for this update is

i

α = 1 ∧ N (Yi | XNL∗
N (Yi | XNL

i

i

i (T  T ))N (Z(i)∗
  k∗
  ki(T  T ))N (Z(i)

1:N )p(Z(i)∗
1:N )p(Z(i)

1:N → Z(i)
1:N )
1:N → Z(i)∗
1:N )

(16)

After updating the Gaussian inputs  we make a global update for the θ parameter. The acceptance
probability for this move is

α = 1 ∧ n(cid:89)

i=1

N (Yi | XNL∗
N (Yi | XNL

i

i (T  T ))p(θ∗)p(θ∗ → θ)
  k∗
  ki(T  T ))p(θ)p(θ → θ∗)

i

 

(17)

i

and k∗

i (T  T ) are computed using the proposed value of θ∗.

where XNL∗
We noted earlier that when j is large  Zj governs the small-time oscillations of the diffusion process.
One should not expect to gain much information about the value of Zj when we have large inter-
observation times. We ﬁnd this to be the case in our experiments - the posterior distribution of
Zj:N approaches a spherical Gaussian distribution when j > 3. For this reason  we employ a
Gaussian random walk proposal in Z1 with stepsize σRW = .45  and proposals for Z2:N are drawn
independently from the standard normal distribution.

6

In the presence of observation noise  we proceed roughly as before. Recall that we make obser-
vations Yi = Xi + i. We draw proposals Z(i)∗
is now
Yi−1 − i−1. However  one must make an important modiﬁcation to the algorithm. Suppose we
propose an update of ˆXi and it is accepted. If we subsequently propose an update for ˆXi+1 and
it is rejected  then the initial condition for ˆXi+1 will be inconsistent with the current state of the
chain (it will be Yi − i instead of Yi − ∗
i ). For this reason  we must propose joint updates for
( ˆXi  i  ˆXi+1). If the variance of the observation noise is high  it may be more efﬁcient to target the
i } | Y1:n

joint posterior distribution p(cid:0)θ {Zi

i . The initial condition for XNL

1:N and ∗

1:N   XL

(cid:1).

i

6 Numerical Experiments

t

(cid:0)γ2 − X 2

(cid:1) dt + BdWt.

The double-well diffusion is a widely-used benchmark for nonlinear inference problems [24  32 
33  34]. It has been used to model systems that exhibit switching behaviour or bistability [11  35].
It possesses nonlinear features that are sufﬁcient to demonstrate the shortcomings of some existing
inference methods  and how our approach overcomes these issues. The dynamics of the process are
given by

dXt = αXt

(18)
The process X has a bimodal stationary distribution  with modes at x = ±γ. The parameter α
governs the rate at which sample trajectories are ’pushed’ toward either mode. If B is small in
comparison to α  mode-switching occurs relatively rarely.
Figure 1(b) shows a trajectory of a double-well diffusion over 20 units of time  with observations
at times {1  2  . . .   20} . We used the parameters α = 2  γ = 1  B = 1. The variance of the
observation noise was set to Σ = .25.
As we mentioned earlier  particle MCMC performs well in low-dimensional inference problems.
For this reason  the results of a particle MCMC inference algorithm (with N = 1  000) particles are
used as ’ground truth’. Our algorithm used N = 3 Gaussian inputs with a linear correction. We
used the Fourier cosine series (8) as an orthonormal basis. We compare our Gibbs sampler to that
of Golightly and Wilkinson [15]  for which we use an Euler discretisation with stepsize ∆t = .05.
Each algorithm drew 70  000 samples from the posterior distribution  moving through the parameter
space in a Gaussian random walk. We placed an exponential(4) prior on γ and an exponential(1)
prior on α and B.
For this particular choice of parameters  both Gibbs samplers give a good approximation to the true
posterior. Figure 2 shows histograms of the marginal posterior distributions of (α  γ  B) for each
algorithm.

(a) p(α|Y1:20)

(b) p(γ|Y1:20)

(c) p(B|Y1:20)

Figure 2: Marginal posterior distributions for (α  γ  B) conditional on observed data. The solid
black line is the output of a particle MCMC method  taken as ground truth. The broken red line is
the output of the linear proposal method  and the broken and dotted blue line is the density estimate
from the coloured noise expansion method. We see that both methods give a good approximation to
the ground truth.

Gibbs samplers that have been used in the past rely on making proposals by conditioning a lin-
ear diffusion to hit a target  and subsequently accepting or rejecting those proposals. Over short
timescales  or for problems that are not highly nonlinear  this can be an effective strategy. However 
as the timescale increases  the proposal and target become quite dissimilar (see Figure 1(a)).

7

01234500.20.40.60.81αp(α)00.511.522.500.511.52γp(γ)00.511.522.500.511.52Bp(B)For our second experiment  we simulate a double well process with (α  γ  B) = (2  2.5  2). We make
noisy observations with ti − ti−1 = 3 and Σ = .1. The algorithms target the posterior distribution
over γ  with α and B ﬁxed at their true values. From our previous discussion  one might expect the
linear proposal strategy to perform poorly in this more nonlinear setting. This is indeed the case. As
in the previous experiment  we used a linear proposal Gibbs sampler with Euler stepsize dt = 0.05.
In the ‘path update’ stage  fewer than .01% of proposals were accepted. On the other hand  the
coloured noise expansion method used N = 7 Gaussian inputs with a linear correction and was able
to approximate the posterior accurately. Figure 3 shows histograms of the results. Note the different
scaling of the rightmost plot.

(a) Particle MCMC

(b) Coloured noise expansion
method

(c) Linear proposal method

Figure 3: p(γ|Y1:10  B  α) after ten observations with a relatively large inter-observation time. We
drew data from a double well process with (α  γ  B) = (2  2.5  2). The coloured noise expansion
method matches the ground truth  whereas the linear proposal method is inconsistent with the data.

7 Discussion and Future Work

We have seen that the standard linear proposal/correction strategy can fail for highly nonlinear prob-
lems. Our inference method avoids the linear correction step  instead targeting the posterior over
input variables directly. With regard to computational efﬁciency  it is difﬁcult to give an authori-
tative analysis because both our method and the linear proposal method are complex  with several
parameters to tune. In our experiments  the algorithms terminated in a roughly similar length of time
(though no serious attempt was made to optimise the runtime of either method).
With regard to our method  several questions remain open. The accuracy of our algorithm depends
on the choice of basis functions {φi}. At present  it is not clear how to make this choice optimally
in the general setting. In the linear case  it is possible to show that one can achieve the accuracy
of the Karhunen-Loeve decomposition  which is theoretically optimal. One can also set the error at
a single time t to zero with a judicious choice of a single basis function. We aim to present these
results in a paper that is currently under preparation.
We used a Taylor expansion to compute the covariance of the correction term. However  it may
be fruitful to use more sophisticated ideas  collectively known as statistical linearisation methods.
In this paper  we restricted our attention to processes with a state-independent diffusion coefﬁcient
so that the covariance of the correction term could be computed. We may be able to extend this
methodology to process with state-dependent noise - certainly one could achieve this by taking a
0-th order Taylor expansion about XNL. Whether it is possible to improve upon this idea is a matter
for further investigation.

Acknowledgments

Simon Lyons was supported by Microsoft Research  Cambridge.

References
[1] R.C. Merton. Theory of rational option pricing. The Bell Journal of Economics and Management Science 

4:141–183  1973.

[2] D.T. Gillespie. The chemical Langevin equation. Journal of Chemical Physics  113 1:297–306  2000.

8

11.522.5300.511.522.533.5γp(γ)11.522.5300.511.522.533.5γp(γ)11.522.5302468γp(γ)[3] G. Kallianpur. Weak convergence of stochastic neuronal models. Stochastic Methods in Biology  70:116–

145  1987.

[4] H.A. Dijkstra  L.M. Frankcombe  and A.S. von der Heydt. A stochastic dynamical systems view of the
Atlantic Multidecadal Oscillation. Philosophical Transactions of the Royal Society A  366:2543–2558 
2008.

[5] L. Murray and A. Storkey. Continuous time particle ﬁltering for fMRI. Advances in Neural Information

Processing Systems  20:1049–1056  2008.

[6] J. Daunizeau  K.J. Friston  and S.J. Kiebel. Variational Bayesian identiﬁcation and prediction of stochastic

nonlinear dynamic causal models. Physica D  pages 2089–2118  2009.

[7] L.M. Murray and A.J. Storkey. Particle smoothing in continuous time: A fast approach via density

estimation. IEEE Transactions on Signal Processing  59:1017–1026  2011.

[8] W. Feller. An Introduction to Probability Theory and its Applications  Volume II. Wiley  1971.
[9] I. Karatzas and S.E. Shreve. Brownian Motion and Stochastic Calculus. Springer  1991.
[10] B. Oksendal. Stochastic Differential Equations. Springer  2007.
[11] C.W. Gardiner. Handbook of Stochastic Methods for Physics  Chemistry and the Natural Sciences.

[12] Y. A¨ıt-Sahalia. Closed-form likelihood expansions for multivariate diffusions. The Annals of Statistics 

Springer-Verlag  1983.

36(2):906–937  2008.

[13] A. Beskos  O. Papaspiliopoulos  and G.O. Roberts. Monte-Carlo maximum likelihood estimation for

discretely observed diffusion processes. Annals of Statistics  37:223–245  2009.

[14] A. Beskos  O. Papaspiliopoulos  G.O. Roberts  and P. Fearnhead. Exact and computationally efﬁcient
likelihood-based estimation for discretely observed diffusion processes (with discussion). Journal of the
Royal Statistical Society: Series B (Statistical Methodology)  68:333–382  2006.

[15] A. Golightly and D.J. Wilkinson. Bayesian inference for nonlinear multivariate diffusion models observed

with error. CSDA  52:1674–1693  2008.

[16] S. Chib  M.K. Pitt  and N. Shepard. Likelihood-based inference for diffusion models. Working Paper 

2004. http://www.nuff.ox.ac.uk/economics/papers/2004/w20/chibpittshephard.pdf.

[17] G.B. Durham and A.R. Gallant. Numerical techniques for maximum likelihood estimation of continuous-
time diffusion processes (with comments). Journal of Business and Economic Statistics  20:297–338 
2002.

[18] A. Beskos  O. Papaspiliopoulos  and G.O. Roberts. Retrospective exact simulation of diffusion sample

paths with applications. Bernoulli  12(6):1077  2006.

[19] D. Rimmer  A. Doucet  and W.J. Fitzgerald. Particle ﬁlters for stochastic differential equations of nonlin-

ear diffusions. Technical report  Cambridge University Engineering Department  2005.

[20] Christophe Andrieu  Arnaud Doucet  and Roman Holenstein. Particle Markov Chain Monte Carlo meth-

ods. Journal of the Royal Statistical Society  72:1–33  2010.

[21] C. Snyder  T. Bengtsson  P. Bickel  and J. Anderson. Obstacles to high-dimensional particle ﬁltering.

[22] Y. A¨ıt-Sahalia. Maximum likelihood estimation of discretely sampled diffusions: a closed-form approxi-

Monthly Weather Review  136(12):4629–4640  2008.

mation approach. Econometrica  70:223–262  2002.

[23] C. Archambeau  D. Cornford  M. Opper  and J. Shawe-Taylor. Gaussian process approximations of

stochastic differential equations. JMLR: Workshop and Conference Proceedings  1:1–16  2007.

[24] C. Archambeau  M. Opper  Y. Shen  D. Cornford  and J. Shawe-Taylor. Variational inference for diffusion

processes. In Advances in Neural Information Processing Systems 20 (NIPS 2007)  2008.

[25] S. S¨arkk¨a. On unscented Kalman ﬁltering for state estimation of continuous-time nonlinear systems.

IEEE Transactions on Automatic Control  52:1631–1641  2007.

[26] A.H. Jazwinski. Stochastic processes and ﬁltering theory  volume 63. Academic Pr  1970.
[27] H. Singer. Nonlinear continuous time modeling approaches in panel research. Statistica Neerlandica 

62(1):29–57  2008.

Arxiv:1008.4441  2010.

[28] S. Corlay and P. Gilles. Functional quantization based stratiﬁed sampling methods. Arxiv preprint

[29] W. Luo. Wiener chaos expansion and numerical solutions of stochastic partial differential equations. PhD

thesis  California Institute of Technology  2006.

[30] P.E. Kloeden and E. Platen. Numerical Solution of Stochastic Differential Equations. Springer  1999.
[31] A.F. Bastani and S.M. Hosseini. A new adaptive Runge-Kutta method for stochastic differential equations.

Journal of Computational and Applied Mathematics  206:631–644  2007.

[32] Y. Shen  C. Archambeau  D. Cornford  M. Opper  J. Shawe-Taylor  and R. Barillec. A comparison of
variational and Markov chain Monte Carlo methods for inference in partially observed stochastic dynamic
systems. Journal of Signal Processing Systems  61(1):51–59  2010.

[33] H. Singer. Parameter estimation of nonlinear stochastic differential equations: simulated maximum like-
lihood versus extended Kalman ﬁlter and Itˆo-Taylor expansion. Journal of Computational and Graphical
Statistics  11(4):972–995  2002.

[34] M. Opper  A. Ruttor  and G. Sanguinetti. Approximate inference in continuous time Gaussian-jump

processes. Advances in Neural Information Processing Systems  23:1831–1839  2010.

[35] N.G. van Kampen. Stochastic processes in physics and chemistry. North holland  2007.

9

,Ashok Cutkosky
Róbert Busa-Fekete