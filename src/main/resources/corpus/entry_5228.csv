2019,Stochastic Continuous Greedy ++:  When Upper and Lower Bounds Match,In this paper  we develop \scg~(\text{SCG}{$++$})  the first efficient variant of a conditional gradient method for maximizing a  continuous submodular function subject to a convex constraint. Concretely  for a monotone and continuous DR-submodular function  \SCGPP achieves a tight  $[(1-1/e)\OPT -\epsilon]$ solution while using $O(1/\epsilon^2)$ stochastic gradients and $O(1/\epsilon)$ calls to the linear optimization oracle. The best previously known algorithms either achieve a suboptimal $[(1/2)\OPT -\epsilon]$ solution with $O(1/\epsilon^2)$ stochastic gradients or the tight $[(1-1/e)\OPT -\epsilon]$ solution  with suboptimal $O(1/\epsilon^3)$ stochastic gradients. We further provide an information-theoretic lower bound to showcase the necessity of $\OM({1}/{\epsilon^2})$ stochastic oracle queries in order to achieve $[(1-1/e)\OPT -\epsilon]$ for monotone and DR-submodular functions. This result shows that our proposed \SCGPP enjoys optimality in terms of both approximation guarantee  i.e.  $(1-1/e)$ approximation factor  and stochastic gradient evaluations  i.e.  $O(1/\epsilon^2)$ calls to the stochastic oracle. By using stochastic
continuous optimization as an interface  we also show that it is possible to obtain the $[(1-1/e)\OPT-\epsilon]$ tight approximation guarantee for maximizing a monotone
but stochastic submodular set function subject to a general matroid constraint after at most 
$\mathcal{O}(n^2/\epsilon^2)$ calls to the stochastic function value  where $n$ is the number of elements in the ground set.,Stochastic Continuous Greedy ++:

When Upper and Lower Bounds Match∗

Hamed Hassani
ESE Department

University of Pennsylvania

Philadelphia  PA

hassani@seas.upenn.edu

Amin Karbasi
ECE Department
Yale University
New Haven  CT

amin.karbasi@yale.edu

Aryan Mokhtari
ECE Department

Zebang Shen
ESE Department

University of Pennsylvania

Philadelphia  PA

The University of Texas at Austin

Austin  TX

mokhtari@austin.utexas.edu

zebang@seas.upenn.edu

Abstract

In this paper  we develop Stochastic Continuous Greedy++ (SCG++) 
the ﬁrst efﬁcient variant of a conditional gradient method for maximizing a con-
tinuous submodular function subject to a convex constraint. Concretely  for a
monotone and continuous DR-submodular function  SCG++ achieves a tight
[(1 − 1/e)OPT − ] solution while using O(1/2) stochastic gradients and O(1/)
calls to the linear optimization oracle. The best previously known algorithms either
achieve a suboptimal [(1/2)OPT− ] solution with O(1/2) stochastic gradients or
the tight [(1− 1/e)OPT− ] solution with suboptimal O(1/3) stochastic gradients.
We further provide an information-theoretic lower bound to showcase the neces-
sity of O(1/2) stochastic oracle queries in order to achieve [(1 − 1/e)OPT − ]
for monotone and DR-submodular functions. This result shows that our proposed
SCG++ enjoys optimality in terms of both approximation guarantee  i.e.  (1−1/e)
approximation factor  and stochastic gradient evaluations  i.e.  O(1/2) calls to the
stochastic oracle. By using stochastic continuous optimization as an interface  we
also show that it is possible to obtain the [(1 − 1/e)OPT − ] tight approximation
guarantee for maximizing a monotone but stochastic submodular set function sub-
ject to a general matroid constraint after at most O(n2/2) calls to the stochastic
function value  where n is the number of elements in the ground set.

1

Introduction

In this paper  we consider the following non-oblivious stochastic submodular maximization problem:

Ez∼p(z;x)[ ˜F (x; z)] 

x∈C F (x) := max
max
x∈C

(1)
where x ∈ Rd
+ is the decision variable  C ⊆ Rd is a convex feasible set  z ∈ Z is a random variable
with distribution p(z; x)  and the submodular objective function F : Rd → R is deﬁned as the
expectation of a set of stochastic functions ˜F : Rd × Z → R. In this paper  we focus on a general
case of stochastic submodular maximization in which the probability distribution of the random
variable z depends on the variable x and may change during the optimization procedure. One should

∗The authors are listed in alphabetical order.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

note that the usual stochastic optimization where the distribution p is independent of x is a special case
of Problem (1). A canonical example of the general stochastic submodular maximization problem in
(1) is the multi-linear extension of a discrete submodular function where the stochasticity crucially
depends on the decision variable x at which we evaluate. Speciﬁcally  consider a discrete submodular
set function f : 2V → R+ deﬁned over the ground set V . The aim is to solve the following problem
maxS∈I f (S) where I is a matroid constraint. For this problem  the classic greedy algorithm leads to
a 1/2 approximation guarantee  but one can achieve the optimal approximation guarantee of 1 − 1/e
by maximizing its multilinear extension F : [0  1]V → R+ which is deﬁned as
(1 − xj) 

F (x) := Ez∼x[f (z(x))] :=

(cid:88)

f (S)

(2)

S⊆V

(cid:89)

i∈S

xi

(cid:89)

j /∈S

where for the random set z(x) each element e is sampled with probability xe. This problem is a
special case of (1) if we deﬁne ˜F (x  z) as f (z(x)) and p(x  z) as the distribution of the random set
z(x)  i.e.  each coordinate ze is generated according to a Bernoulli distribution with parameter xe.
When F is monotone and continuous DR-submodular  Hassani et al. [17] showed that the Stochastic
Gradient Ascent (SGA) method ﬁnds a solution to Problem (1) with a function value no less than
[(1/2)OPT − ] after computing O(1/2) stochastic gradients. Here  and throughout the paper  OPT
denotes the optimal value of Problem (1). Hassani et al. [17] also provided examples for which
SGA cannot achieve better than 1/2 approximation ratio  in general. Later  Mokhtari et al. [22]
proposed Stochastic Continuous Greedy (SCG)  a conditional gradient method that achieves the tight
[(1 − 1/e)OPT − ] solution by O(1/3) calls to the linear optimization oracle while using O(1/3)
stochastic gradients. While both SCG and SGA are ﬁrst-order methods  meaning that they rely on
stochastic gradients  SCG provably achieves a better result at the price of being slower. Therefore  a
fundamental question is the following

“Can we achieve the best of both worlds? That is  can we ﬁnd a [(1− 1/e)OPT− ]
solution after at most O(1/2) calls to the stochastic oracle?"

Another question that naturally arises is about a lower bound on the number of stochastic gradient
evaluations for ﬁnding a (1 − 1/e) approximate solution:

“What is the lower bound on the number of calls to the ﬁrst-order stochastic oracle
for achieving a [(1 − 1/e)OPT − ] solution?"

In this paper  we develop a tight lower bound on the number of calls to the stochastic oracle for
achieving a [(1−1/e)OPT−] solution  and propose an algorithm that achieves the sample complexity
of the lower bound. The detail of our contributions follows.
Our contributions. We develop Stochastic Continuous Greedy++ (SCG++ )  the ﬁrst
method that achieves the tight [(1 − 1/e)OPT − ] solution for Problem (1) with O(1/) calls to the
linear optimization program while using O(1/2) stochastic gradients in total. Our technique relies
on a novel variance reduction method that estimates the difference of gradients in the non-oblivious
stochastic setting without introducing extra bias. This is crucial in our analysis  as all the existing
variance reduction methods fail to correct for this bias and can only operate in the oblivious/classic
stochastic setting. We further show that our result is optimal in all aspects. In particular  we provide
an information-theoretic lower bound to showcase the necessity of O(1/2) stochastic oracle queries
in order to achieve [(1 − 1/e)OPT − ]. Note that under standard assumptions  one cannot achieve
an approximation ratio better than (1 − 1/e) for submodular functions [13]. By using stochastic
continuous optimization as an interface  we also provide a (1 − 1/e)OPT −  tight approximation
guarantee for maximizing a monotone but stochastic submodular set function subject to a matroid
constraint with at most O(n/2) calls to the stochastic oracle where n is the size of the ground set.

2 Related Work

Submodular set functions capture the intuitive notion of diminishing returns and have become in-
creasingly important in various machine learning applications. Examples include data summarization
[20  21]  dictionary learning [9]  and variational inference [11]  to name a few. It is known that for a
monotone submodular function and subject to a cardinality constraint  greedy algorithm achieves the
tight (1 − 1/e) approximation guarantee [25]. However  the vanilla greedy method does not provide

2

the tightest guarantees for many classes of feasibility constraints. To circumvent this issue  the con-
tinuous relaxation of submodular functions  through the multilinear extension  have been extensively
studied [31  7  8  14  16  30]. In particular  it is known that the Continuous Greedy algorithm achieves
the tight (1 − 1/e) approximation guarantee for monotone submodular functions under a general
matroid constraint [7] with a prohibitive query complexity of O(n8). The fastest existing solution
for maximizing a submodular function subject to a matroid constraint interplays between discrete
and continuous domains to achieve a running time of O(n/4) for ﬁnding a (1 − 1/e)OPT − 
approximate solution [4]. In contrast  we develop a pure continuous method that obtains the same
guarantee with a running time of O(n2/2).
Continuous DR-submodular functions  an important subclass of non-convex functions  generalize
the notion of diminishing returns to the continuous domains [5]. Such functions naturally arise in
machine learning applications such as Map inference for Determinantal Point Processes [19] and
revenue maximization [26]. It has been recently shown that monotone continuous DR-submodular
functions can be (approximately) maximized over convex bodies using ﬁrst-order methods [5  17  22].
When exact gradient information is available  [5] showed that the continuous greedy algorithm
achieves [(1 − 1/e)OPT − ] with O(1/) gradient evaluations. However  the problem becomes
considerably more challenging when we only have access to a stochastic ﬁrst-order oracle. In
particular  Hassani et al. [17] showed that the stochastic gradient ascent achieves [1/2OPT − ] by
using O(1/2) stochastic gradients. In contrast  [22  23] proposed a stochastic variant of continuous
greedy that achieves [(1 − 1/e)OPT − ] by using O(1/3) stochastic gradients. This paper shows
how to achieve [(1 − 1/e)OPT − ] by O(1/2) stochastic gradient evaluations.

3 Preliminaries
Submodularity. A set function f : 2V → R+  deﬁned on the ground set V   is submodular if
f (A) + f (B) ≥ f (A ∩ B) + f (A ∪ B)  for all subsets A  B ⊆ V . Even though submodularity is
mostly considered on discrete domains  the notion can be naturally extended to arbitrary lattices [15].
i=1 Xi where each Xi is a compact
To this aim  let us consider a subset of Rd
subset of R+. A function F : X → R+ is continuous submodular if ∀(x  y) ∈ X × X

+ of the form X =(cid:81)d

F (x) + F (y) ≥ F (x ∨ y) + F (x ∧ y) 

= max(x  y) (component-wise) and x ∧ y
.

(3)
where x ∨ y
.
= min(x  y) (component-wise). A
submodular function is monotone if for any x  y ∈ X such that x ≤ y  we have F (x) ≤ F (y)
(here  by x ≤ y we mean that every element of x is less than that of y). When twice differentiable 
F is submodular if and only if all cross-second-derivatives are non-positive [3]  i.e.  we have
∀i (cid:54)= j ∀x ∈ X   ∂2F (x)/∂xi∂xj ≤ 0. This expression shows continuous submodular functions are
not convex nor concave in general  as concavity (convexity) implies that ∇2F (cid:22) 0 (resp.(cid:53)2F (cid:23) 0).
A proper subclass of submodular functions are called DR-submodular [29] if for all x  y ∈ X such
that x ≤ y and any standard basis vector ei ∈ Rn and a non-negative number z ∈ R+ such that
zei + x ∈ X and zei + y ∈ X   then  F (zei + x) − F (x) ≥ F (zei + y) − F (y). One can easily
verify that for a differentiable DR-submodular function the gradient is an antitone mapping  i.e.  for
all x  y ∈ X such that x ≤ y we have ∇F (x) ≥ ∇F (y) [5].
Variance Reduction. Beyond the vanilla stochastic gradient  variance reduced methods [28  18  10 
27  2] have succeeded in reducing stochastic ﬁrst-order oracle complexity in oblivious stochastic
optimization

Ez∼p(z)

˜F (x; z) 

x∈C F (x) := max
max
x∈C

(4)
where each component function ˜F (·; z) is L-smooth. In contrast to (1)  the underlying distribution
p of (4) is invariant to the variable x and is hence called oblivious. We will now explain a recent
variance reduction technique for solving (4) using stochastic gradient information. Consider the
following unbiased estimate of the gradient at the current iterate xt:

gt := gt−1 + ∇ ˜F (xt;M) − ∇ ˜F (xt−1;M) 

(cid:80)
(5)
z∈M ∇ ˜F (y; z) for some y ∈ Rd  gt−1 is an unbiased gradient esti-
where ∇ ˜F (y;M) := 1|M|
mator at xt−1  and M is a mini-batch of random samples drawn from p(z). [12] showed that  with
the gradient estimator (5)  O(1/3) stochastic gradient evaluations are sufﬁcient to ﬁnd an -ﬁrst-
order stationary point of Problem (4)  improving upon the O(1/4) complexity of SGD. A crucial

3

property leading to the success of the variance reduction method given in (5) is that ∇ ˜F (xt;M) and
∇ ˜F (xt−1;M) use the same minibatch sample M in order to exploit the L-smoothness of component
functions f (·; z). Such construction is only possible in the oblivious setting where p(z) is independent
of the choice of x  and would introduce bias in the more general non-oblivious case (1). To see this 
let M be the minibatch of random variable z sampled according to distribution p(z; xt). We have
E[∇ ˜F (xt;M)] = ∇F (xt) but E[∇ ˜F (xt−1;M)] (cid:54)= ∇F (xt−1) since the distribution p(z; xt−1) is
not the same as p(z; xt). The same argument renders all the existing variance reduction techniques
inapplicable for the non-oblivious setting of Problem (1).

4 Stochastic Continuous Greedy++
In this section  we present the Stochastic Continuous Greedy++ (SCG++) algorithm
which is the ﬁrst method to obtain a [(1 − 1/e)OPT − ] solution with O(1/2) stochastic oracle
complexity. The SCG++ algorithm essentially operates in a conditional gradient manner. To be
more precise  at each iteration t  given a gradient estimator gt  SCG++ solves the subproblem

(6)
to obtain an element vt in C as ascent direction  which is then added to the iterate xt+1 with a scaling
factor 1/T   i.e.  the new iterate xt+1 is computed by following the update

vt = argmax

v∈C

(cid:104)v  gt(cid:105)

xt+1 = xt +

1
T

vt 

(7)

where T is the total number of iterations of the algorithm. The iterates are assumed to be initialized
at the origin which may not belong to the feasible set C. Though each iterate xt may not necessarily
be in C  the feasibility of the ﬁnal iterate xT is guaranteed by the convexity of C. Note that the iterate
sequence {xs}T
s=0 can be regarded as a path from the origin (as we manually force x0 = 0) to some
feasible point in C. The key idea in SCG++ is to exploit the high correlation between the consecutive
iterates originated from the O(1/T )-sized increments to maintain a highly accurate estimate gt 
which is the focus of the rest of this section. Note that by replacing the gradient approximation vector
gt in the update of SCG++ by the exact gradient of the objective function  we recover the update
rule of the continuous greedy method [7  5].
We now proceed to describe our approach for evaluating the gradient approximation gt when we face
a non-oblivious problem as in (1). Given a sequence of iterates {xs}t
s=0  the gradient of the objective
function F at the iterate xt can be written in a path-integral form as

∇F (xt) = ∇F (x0) +

∆s def= ∇F (xs) − ∇F (xs−1)

.

(8)

(cid:111)

t(cid:88)

(cid:110)

s=1

(cid:90) 1

By obtaining an unbiased estimate of ∆t = ∇F (xt) − ∇F (xt−1) and reusing the previous unbiased
estimates for s < t  we obtain recursively an unbiased estimator of ∇F (xt) which has a reduced
variance. Estimating ∇F (xs) and ∇F (xs−1) separately as suggested in (5) would cause the bias
issue in the the non-oblivious case (see discussion at the end of section 3). Therefore  we propose an
approach for directly estimating the difference ∆t = ∇F (xt) − ∇F (xt−1) in an unbiased manner.
We construct an unbiased estimator gt of the gradient vector ∇F (xt) by adding an unbiased estimate
˜∆t of the gradient difference ∆t = ∇F (xt) − ∇F (xt−1) to gt−1  where gt−1 as an unbiased
estimate of ∇F (xt−1). Note that ∆t = ∇F (xt) − ∇F (xt−1) can be written as

∆t =

∇2F (x(a))(xt − xt−1)da =

∇2F (x(a))da

(xt − xt−1) 

(9)

0

0

where x(a) def= a·xt +(1−a)·xt−1 for a ∈ [0  1]. Therefore  if we sample the parameter a uniformly
at random from the interval [0  1]  it can be easily veriﬁed that ˜∆t := ∇2F (x(a))(xt − xt−1) is an
unbiased estimator of the gradient difference ∆t since

Ea[∇2F (x(a))(xt − xt−1)] = ∇F (xt) − ∇F (xt−1).

(10)
Therefore  all we need is an unbiased estimator of the Hessian-vector product ∇2F (y)(xt − xt−1)
for the non-oblivious objective F at an arbitrary y ∈ C. In the following lemma  we present an
unbiased estimator of ∇2F (y) for any y ∈ C that can be evaluated efﬁciently.

4

(cid:20)(cid:90) 1

(cid:21)

if t = 1 then

Algorithm 1 Stochastic Continuous Greedy++ (SCG++)
Input: Minibatch size |M0| and |M|  and total number of rounds T
1: Initialize x0 = 0;
2: for t = 1 to T do
3:
4:
5:
6:

Sample a minibatch M0 of z according to p(z; x0) and compute g0 def= ∇ ˜F (x0;M0);
Sample a minibatch M of z according to p(z; x(a)) where a is a chosen uniformly at
random from [0  1] and x(a) := a · xt + (1 − a) · xt−1;
Compute the Hessian approximation ˜∇2
Construct ˜∆t based on (13) (Option I) or (18) (Option II);
Update the stochastic gradient approximation gt := gt−1 + ˜∆t;

t corresponding to M according to (12);

else

end if
Compute the ascent direction vt := argmaxv∈C{v(cid:62)gt};
Update the variable xt+1 := xt + 1/T · vt;

7:
8:
9:
10:
11:
12:
13: end for

Lemma 1. For any y ∈ C  let z be the random variable with distribution p(z; y) and deﬁne

˜∇2F (y; z) def= ˜F (y; z)[∇ log p(z; y)][∇ log p(z; y)](cid:62) + [∇ ˜F (x; z)][∇ log p(z; y)](cid:62)
+ [∇ log p(z; y)][∇ ˜F (y; z)](cid:62) + ∇2 ˜F (y; z) + ˜F (y; z)∇2 log p(z; y).

(11)

Then  ˜∇2F (y; z) is an unbiased estimator of ∇2F (y)  i.e.  Ez∼p(z;y)[ ˜∇2F (y; z)] = ∇2F (y).
The result in Lemma 1 shows how to evaluate an unbiased estimator of the Hessian ∇2F (y). If we
consider a as a random variable with a uniform distribution over the interval [0  1]  then we can deﬁne
the random variable z(a) with the probability distribution p(z(a); x(a)) where x(a) is deﬁned as
x(a) := a · xt + (1 − a) · xt−1. Considering these two random variables and the result in Lemma 1 

we can construct an unbiased estimator of the integral(cid:82) 1

0 ∇2F (x(a))da in (9) by

˜∇2F (x(a); z(a)) 

(12)

(a z(a))∈M

t which is an unbiased estimator of(cid:82) 1

where M is a minibatch containing |M| samples of random tuple (a  z(a)). Once we have access to
˜∇2
0 ∇2F (x(a))da  we can approximate the gradient difference
∆t by its unbiased estimator which is deﬁned as
˜∆t := ˜∇2

(13)
t (xt − xt−1) requires O(d2)
Note that for the general objective F (·)  the matrix-vector product ˜∇2
computation and memory. To resolve this issue  in Section 4.1 we provide an implementation of
(13) using only ﬁrst-order information which has a computational and memory complexity of O(d).
Using ˜∆t as an unbiased estimator of the gradient difference ∆t  we deﬁne our gradient estimator as

t (xt − xt−1).

(cid:88)

˜∇2

t

def=

1
|M|

t(cid:88)

gt = ∇ ˜F (x0;M0) +

˜∆t.

(14)

This update can also be written in a recursive way as gt = gt−1 + ˜∆t  if we set g0 = ∇ ˜F (x0;M0).
Note that the proposed approach for gradient approximation in (14) has a variance reduction mecha-
nism which leads to optimal computational complexity of SCG++ in terms of number of calls to
the stochastic oracle. We further highlight this point in Section 4.2.

i=1

Implementation of the Hessian-Vector Product

4.1
Now we focus on the computation of the gradient difference approximation ˜∆t in (13). We aim
to come up with a scheme that avoids explicitly computing the matrix estimator ˜∇2
t which has a

5

complexity of O(d2)  and present an approach directly approximating ˜∆t that only uses the ﬁnite
differences of gradients with a complexity of O(d). Based on (12)  computing ˜∇2
t (xt − xt−1) is
equivalent to computing |M| instances of ˜∇2F (y; z)(xt − xt−1) for some y ∈ C and z ∈ Z. Denote
d = xt − xt−1 and use the expression in (11) to write

˜∇2F (y;z) · d = ˜F (y; z)[∇ log p(z; y)(cid:62)d]∇ log p(z; y) + [∇ log p(z; y)(cid:62)d]∇ ˜F (x; z)
+ [∇ ˜F (y; z)(cid:62)d][∇ log p(z; y)] + ∇2 ˜F (y; z) · d + ˜F (y; z)∇2 log p(z; y) · d.

(15)
Note that the ﬁrst three terms can be computed in time O(d) and only the last two terms on the
right hand side of (15) involve O(d2) operations  which can be approximated by the following ﬁnite
gradient difference scheme. For any twice differentiable function ψ : Rd → R and arbitrary d ∈ Rd
with bounded Euclidean norm (cid:107)d(cid:107) ≤ D  we compute  for some small δ > 0 

∇ψ(y + δ · d) − ∇ψ(y − δ · d)

(cid:39) ∇2ψ(y) · d.

φ(δ; ψ) def=

2δ

(16)
As the Hessian of ψ(·) is L2-smooth  the above approximation can be bounded by (cid:107)∇2ψ(y) · d −
φ(δ; ψ)(cid:107) = (cid:107)∇2ψ(y)·d−∇2ψ(˜x)·d(cid:107) ≤ D2L2δ  where ˜x is obtained from the mean value theorem.
This quantity can be made arbitrary small by decreasing δ. In next section  we show that setting
δ = O(2) is sufﬁcient  where  is the target accuracy. By applying the technique of (16) to the two
functions ψ(y) = ˜F (y; z) and ψ(y) = log p(z; y)  we can approximate (15) in time O(d):
ξδ(y; z) def= ˜F (y; z)[∇ log p(z; y)(cid:62)d]∇ log p(z; y) + [∇ log p(z; y)(cid:62)d]∇ ˜F (x; z)
+ [∇ ˜F (y; z)(cid:62)d][∇ log p(z; y)] + φ(δ; ˜F (y; z)) + φ(δ; log p(z; y)).

(17)

We further can deﬁne a minibatch version of that which is used in Option II of Step 8 in Algorithm 1 

(cid:88)

ξδ(x;M) def=

1
|M|

(a z(a))∈M

ξδ(x(a); z(a)).

(18)

4.2 Convergence Analysis
In this section  we analyze the convergence of Algorithm 1 using (18) as the gradient-difference
estimation. The result for (13) can be obtained similarly. We note that (13) is a special case of (18)
by taking δ → 0 (e.g.  by letting δ = O(2)). We ﬁrst state the assumptions required for our analysis.
Assumption 4.1 (function value at the origin). The function value F at the origin is F (0) ≥ 0.
Assumption 4.2 (bounded stochastic function value). The stochastic function ˜F (x; z) has bounded
function value for all z ∈ Z and x ∈ C: maxz∈Z x∈C ˜F (x; z) ≤ B.
Assumption 4.3 (monotonicity and DR-submodularity). F is monotone and DR-submodular.
Assumption 4.4 (compactness of feasible domain). The set C is compact with diameter D.
Assumption 4.5 (bounded gradient norm). For all x ∈ C  the stochastic gradient ∇ ˜F has bounded
norm: ∀z ∈ Z (cid:107)∇ ˜F (x; z)(cid:107) ≤ G ˜F   and the norm of the gradient of log p has bounded fourth-order
moment  i.e.  Ez∼p(x;z)(cid:107)∇ log p(z; x)(cid:107)4 ≤ G4
Assumption 4.6 (bounded second-order derivatives). ∀x ∈ C  the Hessian ∇2 ˜F has bounded
spectral norm ∀z ∈ Z (cid:107)∇2 ˜F (x; z)(cid:107) ≤ L ˜F   and spectral norm of the log-probability Hessian has
bounded second moment: Ez∼p(z;x)(cid:107)∇2 log p(z; x)(cid:107)2 ≤ L2
p. Further we deﬁne L = max{L ˜F   Lp}.
Assumption 4.7 (continuity of the Hessian). The stochastic Hessian is L2 f -Lipschitz continuous 
i.e  for all x  y ∈ C and all z ∈ Z  i.e.  (cid:107)∇2 ˜F (x; z) − ∇2 ˜F (y; z)(cid:107) ≤ L2  ˜F(cid:107)x − y(cid:107). The Hessian
of the log probability log p(x; z) is L2 p-Lipschitz continuous: for all x  y ∈ C and all z ∈ Z  i.e. 
(cid:107)∇2 log p(x; z) − ∇2 log p(y; z)(cid:107) ≤ L2 p(cid:107)x − y(cid:107). Further  deﬁne L2 = max{L2  ˜F   L2 p}.
Remark 1. Assumption 4.7 is only used to show the ﬁnite difference scheme (15) has bounded
variance  and the oracle complexity of our method does not depend on L2  ˜F and L2 p.

p. Further we deﬁne G = max{G ˜F   Gp}.

As we mentioned in the previous section  the update for the stochastic gradient vector gt in the update
of SCG++ is designed properly to reduce the noise of gradient approximation. In the following
lemma  we formally characterize the variance of gradient approximation for SCG++ . To this end 
we also need to properly choose the minibatch sizes |M0| and |M|.

6

Lemma 2. Consider SCG++ outlined in Algorithm 1 and assume that in Step 8 we follow (18) to
construct the gradient difference approximation ˜∆t (Option II). If Assumptions (4.2)  (4.4)  (4.5) 
(4.6)  and (4.7) hold and we set the minibatch sizes to |M0| = (G2/( ¯L2D22)) and |M| = 2/  and
the error of Hessian-vector product approximation δ is O(2) as in (31)  then

E(cid:2)(cid:107)gt − ∇F (xt)(cid:107)2(cid:3) ≤ (1 + t) ¯L2D22 

∀t ∈ {0  . . .   T − 1} 

(19)

where ¯L is a constant deﬁned by ¯L2 def= 4B2G4 + 16G4 + 4L2 + 4B2L2.
Lemma 2 shows that by |M| = O(−1) calls to the stochastic oracle at each iteration  the variance
of gradient approximation in SCG++ after t iterations is of order O((1 + t)). In the following
theorem  we incorporate this result to characterize the convergence guarantee of SCG++ .
Theorem 1. Consider the SCG++ method outlined in Algorithm 1 and assume that in Step 8
we follow the update in (18) to construct the gradient difference approximation ˜∆t (Option II). If
Assumptions 4.1-4.7 hold  then the output of SCG++ denoted by xT satisﬁes

E(cid:2)F (xT )(cid:3) ≥ (1 − 1/e)F (x∗) − 2 ¯LD2 

2   T = 1

2 ¯L2D22   |M| = 1

   and δ = O(2) as in (31). Here ¯L is a constant

by setting |M0| = G2
deﬁned by ¯L2 def= 4B2G4 + 16G4 + 4L2 + 4B2L2.
The result in Theorem 1 shows that after at most T = O(1/) iterations the objective function value
for the output of SCG++ is at least (1 − 1/e)OPT − O(). As the number of calls to the stochastic
oracle per iteration is of O(1/)  to reach a [(1 − 1/e)OPT − O()] approximation guarantee the
SCG++ method has an overall stochastic ﬁrst-order oracle complexity of O(1/2). We formally
characterize this result in the following corollary.
Corollary 1 (oracle complexities). To ﬁnd a [(1 − 1/e)OPT − ] solution to Problem (1) using Algo-
rithm 1 with Option II  the overall stochastic ﬁrst-order oracle complexity is (2G2D2 + 4 ¯L2D4)/2
and the overall linear optimization oracle complexity is 2 ¯LD2/.

5 Discrete Stochastic Submodular Maximization
In this section  we focus on extending our result in the previous section to the case where F is
the multilinear extension of a (stochastic) discrete submodular function f. This is also an instance
of the non-oblivious stochastic optimization in (1). Indeed  once such a result is achieved  with
proper rounding scheme such as randomized pipage rounding [6] or contention resolution method
[32]  we can extend our results to the discrete setting. Let V denote a ﬁnite set of d elements  i.e. 
V = {1  . . .   d}. Consider a discrete submodular function f : 2V → R+  which is deﬁned as an
expectation over a set of functions fγ : 2V → R+. Our goal is to maximize f subject to some
constraint I  where I contains feasible subsets of V . In other words  we aim to solve the following
discrete and stochastic submodular function maximization problem
Eγ∼p(γ)[fγ(S)] 

(20)
where p(γ) is an arbitrary distribution. In particular  we assume the pair M = {V I} forms a matroid
with rank r. The prototypical example is maximization under the cardinality constraint  i.e.  for a
given integer r  ﬁnd S ⊆ V   |S| ≤ r  which maximizes f. The challenge here is to ﬁnd a solution
with near-optimal quality for the problem in (20) without computing the expectation in (20). That is 
we assume access to an oracle that  given a set S  outputs an independently chosen sample fγ(S)
where γ ∼ p(γ). The focus of this section is on extending our result into the discrete domain and
showing that SCG++ can be applied for maximizing a stochastic submodular set function f  namely
Problem (20)  through the multilinear extension of the function f. Speciﬁcally  in lieu of solving (20)
we can solve its multilinear extension problem

S∈I f (S) := max
max
S∈I

where F : [0  1]V → R+ is the multilinear extension of f and is deﬁned as
xi

Eγ∼p(γ)[fγ(S)]

(1 − xj) =

F (x) :=

f (S)

xi

(cid:88)

S⊆V

(cid:89)

i∈S

(cid:89)

j /∈S

max
x∈C F (x) 

(cid:88)

S⊆V

7

(21)

(1 − xj) 

(22)

(cid:89)

j /∈S

(cid:89)

i∈S

and the convex set C = conv{1I : I ∈ I} is the matroid polytope [6]. Note that here xi denotes the
i-th component of the vector x. In other words  F (x) is the expected value of f over sets wherein
each element i is included with probability xi independently. To solve (21) using SCG++   we
need access to unbiased estimators of the gradient and the Hessian. We now construct the Hessian
approximation ˜∇2
k using the result in [6] which is stated in Lemma 4  in the supplementary material.
Let a be a uniform random variable between [0  1] and let e = (e1 ···   ed) be a random vector
in which ei’s are generated i.i.d. according to the uniform distribution over the unit interval [0  1].
In each iteration  a minibatch M of |M| samples of {a  e  γ} (recall that γ is the random variable
that parameterizes the component function fγ)  i.e. M = {ak  ek  γk}|M|
k=1  is generated. Then for
all k ∈ [|M|]  we let xak = akxt + (1 − ak)xt−1 and construct the random set S(xak   ek) using
xak and ek in the following way: s ∈ S(xak   ek) if and only if [ek]s ≤ [xak ]s for s ∈ [d]. Having
S(xak   ek) and γk  each entry of the Hessian estimator ˜∇2

t ∈ Rd×d is

(cid:88)
fγk (S(xak   ek) ∪ {i  j}) − fγk (S(xak   ek) ∪ {i} \ {j})
−fγk (S(xak   ek) ∪ {j} \ {i}) + fγk (S(xak   ek) \ {i  j}) 
t ]i j = 0. As linear optimization over the rank-r matriod polytope

(23)

k∈[|M|]

(cid:105)

(cid:104) ˜∇2

t

=

1
|M|

i j

√

√

dDf /

f /2).

(cid:113)Eγ[D2

r3dDf /) and |M0| = O(

r3dDf /) iterations. Moreover  the overall stochastic oracle cost is O(r3dD2

√
γ]. By using the minibatch size |M| = O(
√

where i (cid:54)= j  and if i = j then [ ˜∇2
always return vt with at most r nonzero entries  the complexity of computing (23) is O(|M|rd).
We use the above Hessian approximation to solve (21) as a special case of Problem (1) using SCG++ .
Theorem 2. Consider Dγ := maxi∈V fγ(i) as the maximum marginal value of fγ  and deﬁne Df :=
r2) 
Algorithm 1 ﬁnds a [(1 − 1/e)OP T − 6] approximation of the multilinear extension problem in (21)
at most (
Since the cost of a single stochastic gradient computation is O(d)  Theorem 2 shows that the overall
computation complexity of Algorithm 1 is O(d2/2). Note that  in multilinear extension case  the
smoothness Assumption 4.6 required for the results in Section 4 is absent  and that is why we need to
develop a more sophisticated gradient-difference estimator to achieve a similar theoretical guarantee
(more details is available in the appendix).
Remark 2 (optimality of oracle complexities). Note that to achieve the tight (1 − 1/e − ) approxi-
mation  the O(1/2) stochastic oracle complexity in Theorem 2 is optimal in terms of its dependency
on . A lower bound on the stochastic oracle complexity is given in the following theorem.
6 Lower Bound
In this section  we show that reaching a (1 − 1/e − )-optimal solution of Problem (1) requires
at least O(1/2) calls to an oracle which provides stochastic ﬁrst-order information. To do
so  we ﬁrst construct a stochastic submodular set function f  deﬁned through the expectation
f (S) = Eγ∼p(γ)[fγ(S)]  with the following property: Obtaining a (1 − 1/e − )-optimal solu-
tion for maximization of f under a cardinality constraint (an instance of Problem (20)) requires
at least O(1/2) samples of the form fγ(·) where γ is generated i.i.d from distribution p. Such
a lower bound on sample complexity can be directly extended to Problem (1) with an stochastic
ﬁrst order oracle  by considering the multilinear extension of the function f  denoted by F   and
noting that (i) Problems (20) and (21) have the same optimal values  and (ii) one can construct an
unbiased estimator of the gradient of the multilinear extension using d independent samples from the
underlying stochastic set function f. Hence  any method for maximizing (21) is also an algorithm for
maximizing (20) with the same guarantees on the quality of the solution and with sample complexities
that differ at most by a factor d. Now we provide the formal statements regarding the above argument.
Theorem 3. There exists a distribution p(γ) and a monotone submodular function f : 2V →
R+  given as f (S) = Eγ∼p(γ)[fγ(S)]  such that the following holds: In order to ﬁnd a (1 −
1/e − )-optimal solution for (20) with k-cardinality constraint  any algorithm requires at least
min{exp(αk)  β/2} stochastic samples fγ(·).
Corollary 2. There exists a DR-submodular function F : [0  1]n → R  a convex constraint C  and a
stochastic ﬁrst order oracle Of irst  such that any algorithm for maximizing F subject to C requires
at least min{exp(αn)  β/2} queries from Of irst.

8

7 Conclusion

In this paper  we developed SCG++   the ﬁrst efﬁcient variant of continuous greedy for maximizing
a stochastic continuous DR-submodular function subject to a convex constraint. We showed that
SCG++ achieves a tight [(1 − 1/e)OPT − ] solution while using O(1/2) stochastic gradients.
We further derived a tight lower bound on the number of calls to the ﬁrst-order stochastic oracle
for achieving a [(1 − 1/e)OPT − ] approximate solution. This result showed that SCG++ has the
optimal sample complexity for ﬁnding an optimal (1 − 1/e) approximation guarantee for monotone
but stochastic DR-submodular functions.

Acknowledgment
The work of H. Hassani was partially supported by NSF CPS-1837253. Karbasi’s work is partially
supported by NSF (IIS-1845032)  ONR (N00014- 19-1-2406) and AFOSR (FA9550-18-1-0160).
Shen’s work is supported by Zhejiang Provincial Natural Science Foundation of China under Grant
No. LZ18F020002  and National Natural Science Foundation of China (Grant No: 61672376 
61751209  61472347).

References
[1] A. Agarwal  M. J. Wainwright  P. L. Bartlett  and P. K. Ravikumar. Information-theoretic lower
bounds on the oracle complexity of convex optimization. In Advances in Neural Information
Processing Systems  pages 1–9  2009.

[2] Z. Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. In Advances in Neural

Information Processing Systems  pages 2680–2691  2018.

[3] F. Bach. Submodular functions:

arXiv:1511.00394  2015.

from discrete to continuous domains. arXiv preprint

[4] A. Badanidiyuru and J. Vondrák. Fast algorithms for maximizing submodular functions. In
Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms  pages
1497–1514  2014.

[5] A. A. Bian  B. Mirzasoleiman  J. Buhmann  and A. Krause. Guaranteed non-convex opti-
mization: Submodular maximization over continuous domains. In Artiﬁcial Intelligence and
Statistics  pages 111–120  2017.

[6] G. Calinescu  C. Chekuri  M. Pál  and J. Vondrák. Maximizing a submodular set function

subject to a matroid constraint. In IPCO  volume 7  pages 182–196. Springer  2007.

[7] G. Calinescu  C. Chekuri  M. Pál  and J. Vondrák. Maximizing a monotone submodular function

subject to a matroid constraint. SIAM Journal on Computing  40(6):1740–1766  2011.

[8] C. Chekuri  J. Vondrák  and R. Zenklusen. Submodular function maximization via the mul-
tilinear relaxation and contention resolution schemes. SIAM Journal on Computing  43(6):
1831–1879  2014.

[9] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection 

sparse approximation and dictionary selection. ICML  2011.

[10] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in neural information
processing systems  pages 1646–1654  2014.

[11] J. Djolonga and A. Krause. From map to marginals: Variational inference in bayesian submodu-

lar models. In NIPS  2014.

[12] C. Fang  C. J. Li  Z. Lin  and T. Zhang. Spider: Near-optimal non-convex optimization via
stochastic path-integrated differential estimator. In Advances in Neural Information Processing
Systems  pages 687–697  2018.

9

[13] U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM (JACM)  45(4):

634–652  1998.

[14] M. Feldman  J. Naor  and R. Schwartz. A uniﬁed continuous greedy algorithm for submodular
maximization. In IEEE 52nd Annual Symposium on Foundations of Computer Science  pages
570–579  2011.

[15] S. Fujishige. Submodular functions and optimization  volume 58. Annals of Discrete Mathe-

matics  North Holland  Amsterdam  2nd edition  2005. ISBN 0-444-52086-4.

[16] S. O. Gharan and J. Vondrák. Submodular maximization by simulated annealing. In Proceedings
of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms  pages 1098–1116 
2011.

[17] H. Hassani  M. Soltanolkotabi  and A. Karbasi. Gradient methods for submodular maximization.

In Advances in Neural Information Processing Systems  pages 5841–5851  2017.

[18] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[19] A. Kulesza and B. Taskar. Determinantal point processes for machine learning. arXiv preprint

arXiv:1207.6083  2012.

[20] H. Lin and J. Bilmes. A class of submodular functions for document summarization.

In
Proceedings of Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies  2011.

[21] H. Lin and J. Bilmes. Word alignment via submodular maximization over matroids.

In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies: short papers-Volume 2  pages 170–175. Association for
Computational Linguistics  2011.

[22] A. Mokhtari  H. Hassani  and A. Karbasi. Conditional gradient method for stochastic submodular
maximization: Closing the gap. In International Conference on Artiﬁcial Intelligence and
Statistics  pages 1886–1895  2018.

[23] A. Mokhtari  H. Hassani  and A. Karbasi. Stochastic conditional gradient methods: From

convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554  2018.

[24] G. L. Nemhauser and L. A. Wolsey. Best algorithms for approximating the maximum of a

submodular set function. Mathematics of operations research  3(3):177–188  1978.

[25] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher. An analysis of approximations for maximizing

submodular set functions–I. Mathematical Programming  14(1):265–294  1978.

[26] R. Niazadeh  T. Roughgarden  and J. R. Wang. Optimal algorithms for continuous non-monotone

submodular and dr-submodular maximization. 2018.

[27] S. J. Reddi  A. Hefny  S. Sra  B. Poczos  and A. Smola. Stochastic variance reduction for
nonconvex optimization. In International conference on machine learning  pages 314–323 
2016.

[28] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming  162(1-2):83–112  2017.

[29] T. Soma and Y. Yoshida. A generalization of submodular cover via the diminishing return

property on the integer lattice. In NIPS  2015.

[30] M. Sviridenko  J. Vondrák  and J. Ward. Optimal approximation for submodular and super-
modular optimization with bounded curvature. In Proceedings of the Twenty-Sixth Annual
ACM-SIAM Symposium on Discrete Algorithms  pages 1134–1148  2015.

[31] J. Vondrák. Optimal approximation for the submodular welfare problem in the value oracle
model. In Proceedings of the fortieth annual ACM symposium on Theory of computing  pages
67–74. ACM  2008.

10

[32] J. Vondrák  C. Chekuri  and R. Zenklusen. Submodular function maximization via the multilin-
ear relaxation and contention resolution schemes. In Proceedings of the forty-third annual ACM
symposium on Theory of computing  pages 783–792. ACM  2011.

11

,Amin Karbasi
Hamed Hassani
Aryan Mokhtari
Zebang Shen