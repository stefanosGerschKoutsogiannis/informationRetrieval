2019,A Stochastic Composite Gradient Method with Incremental Variance Reduction,We consider the problem of minimizing the composition of a smooth (nonconvex) function and a smooth vector mapping  where the inner mapping is in the form of an expectation over some random variable or a finite sum. We propose a stochastic composite gradient method that employs incremental variance-reduced estimators for both the inner vector mapping and its Jacobian. We show that this method achieves the same orders of complexity as the best known first-order methods for minimizing expected-value and finite-sum nonconvex functions  despite the additional outer composition which renders the composite gradient estimator biased. This finding enables a much broader range of applications in machine learning to benefit from the low complexity of incremental variance-reduction methods.,A Stochastic Composite Gradient Method with

Incremental Variance Reduction

Junyu Zhang

University of Minnesota

Minneapolis  Minnesota 55455

zhan4393@umn.edu

Abstract

Lin Xiao

Microsoft Research

Redmond  Washington 98052
lin.xiao@microsoft.com

We consider the problem of minimizing the composition of a smooth (nonconvex)
function and a smooth vector mapping  where the inner mapping is in the form of
an expectation over some random variable or a ﬁnite sum. We propose a stochastic
composite gradient method that employs an incremental variance-reduced estimator
for both the inner vector mapping and its Jacobian. We show that this method
achieves the same orders of complexity as the best known ﬁrst-order methods
for minimizing expected-value and ﬁnite-sum nonconvex functions  despite the
additional outer composition which renders the composite gradient estimator biased.
This ﬁnding enables a much broader range of applications in machine learning to
beneﬁt from the low complexity of incremental variance-reduction methods.

Introduction

1
In this paper  we consider stochastic composite optimization problems

f(cid:0)Eξ[gξ(x)](cid:1) + r(x)  

minimize

(1)
where f : Rp → R is a smooth and possibly nonconvex function  ξ is a random variable gξ : Rd → Rp
is a smooth vector mapping for a.e. ξ  and r is convex and lower-semicontinuous. A special case
we will consider separately is when ξ is a discrete random variable with uniform distribution over
{1  2  . . .   n}. In this case the problem is equivalent to a deterministic optimization problem

x∈Rd

minimize

x∈Rd

f

gi(x)

+ r(x) .

(2)

(cid:19)

(cid:18) 1

n

n(cid:88)

i=1

The formulations in (1) and (2) cover a broader range of applications than classical stochastic
optimization and empirical risk minimization (ERM) problems where each gξ is a scalar function
(p = 1) and f is the scalar identity map. Interesting examples include the policy evaluation in
reinforcement learning (RL) [e.g.  30]  the risk-averse mean-variance optimization ([e.g.  28  29] 
through a reformulation by [35])  the stochastic variational inequality ([e.g.  12  15] through a
reformulation in [10])  the 2-level composite risk minimization problems [7]  etc.
For the ease of notation  we deﬁne
g(x) := Eξ[gξ(x)] 

Φ(x) := F(x) + r(x).

F(x) := f(g(x)) 

In addition  let f (cid:48) and F(cid:48) denote the gradients of f and F respectively  and g(cid:48)
Jacobian matrix of gξ at x. Then we have

(cid:16)Eξ[g

ξ(x)](cid:17)T

(cid:48)

f (cid:48)(cid:0)Eξ[gξ(x)](cid:1) .

F(cid:48)(x) = ∇(cid:16)

f(cid:0)Eξ[gξ(x)](cid:1)(cid:17)

=

(3)
ξ(x) ∈ Rp×d denote the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Sample complexities of CIVR (Composite Incremental Variance Reduction)
Assumptions (common: f and gξ Lipschitz and smooth  thus F smooth)
F nonconvex
F convex  r convex

F ν-gradient dominant

Problem

r convex

O(cid:0)−3/2(cid:1)
O(cid:0)min{−3/2

  n1/2

O(cid:0)(cid:0)ν−1(cid:1) log −1(cid:1)
−1}(cid:1) O(cid:0)(cid:0)n + νn1/2(cid:1) log −1(cid:1)

r ≡ 0

Φ µ-optimally strongly convex

O(cid:0)(cid:0)µ−1
−1(cid:1) log −1(cid:1)
O(cid:0)(cid:0)n + µ−1n1/2(cid:1) log −1(cid:1)

(1)
(2)

In practice  computing F(cid:48)(x) exactly can be very costly if not impossible. Due to the nonlinearity of the
outer composition  simply multiplying the unbiased estimators E[ ˜g(x)] = g(x) and E[ ˜g(cid:48)(x)] = g(cid:48)(x)
results in a biased estimator for F(cid:48)(x)  namely  E[[ ˜g(cid:48)(x)]T f (cid:48)( ˜g(x))] (cid:44) F(cid:48)(x)  see [e.g.  35]. This is in
great contrast to the classical stochastic optimization problem

(cid:2)gξ(x)(cid:3) + r(x)  

minimize

x∈Rd

Eξ

(4)

where one can always get an unbiased gradient estimator for the smooth part. This fact makes such
composition structure be of independent interest for research on stochastic and randomized algorithms.
In this paper  we develop an eﬃcient stochastic composite gradient method called CIVR (Composite
Incremental Variance Reduction)  for solving problems of the forms (1) and (2). We measure eﬃciency
by the sample complexity of the individual functions gξ and their Jacobian g(cid:48)
  i.e.  the total number
of times they need to be evaluated at some point  in order to ﬁnd an -approximate solution. For
nonconvex functions  an -approximate solution is some random output of the algorithm ¯x ∈ Rd that
satisﬁes E[(cid:107)G( ¯x)(cid:107)2] ≤   where G( ¯x) is the proximal gradient mapping of the objective function Φ
at ¯x (see details in Section 2). If r ≡ 0  then G( ¯x) = F(cid:48)( ¯x) and the criteria for -approximation
becomes E[(cid:107)F(cid:48)( ¯x)(cid:107)2] ≤ .
If the objective Φ is convex  we require E[Φ( ¯x) − Φ(cid:63)] ≤  where
Φ(cid:63) = inf x Φ(x). For smooth and convex functions  these two notions are compatible  meaning that
the dependence of the sample complexity on  in terms of both notions are of the same order.
Table 1 summarizes the sample complexities of the CIVR method under diﬀerent assumptions
obtained in this paper. We can deﬁne a condition number κ = O(ν) for ν-gradient dominant
functions and κ = O(1/µ) for µ-optimally strongly convex functions  then the complexities become
our contributions  we next discuss related work and then putting these results into context.

O(cid:0)(cid:0)κ−1(cid:1) log −1(cid:1) and O(cid:0)(cid:0)n + κn1/2(cid:1) log −1(cid:1) for (1) and (2) respectively. In order to better position

ξ

1.1 Related Work
We ﬁrst discuss the nonconvex stochastic optimization problem (4)  which is a special cases of (1).
When r ≡ 0 and g(x) = Eξ[gξ(x)] is smooth  Ghadimi and Lan [9] developed a randomized stochastic
second-order guarantee. There are also many recent works on solving its ﬁnite-sum version

gradient method with iteration complexity O(−2). Allen-Zhu [2] obtained O(cid:0)−1.625(cid:1) with additional

gi(x) + r(x) 

(5)

n(cid:88)

i=1

minimize

x∈Rd

1
n

Based on a new variance reduction technique called SARAH [21]  Nguyen et al. [22] and Pham et al.

which is also a special case of (2). By extending the variance reduction techniques SVRG [13  34] and
SAGA [6] to nonconvex optimization  Allen-Zhu and Hazan [3] and Reddi et al. [24  25  26] developed
randomized algorithms with sample complexity O(n + n2/3
−1). Under additional assumptions of
gradient dominance or strong convexity  they obtained sample complexity O((n+κn2/3) log −1)  where

κ is a suitable condition number. Allen-Zhu [1] and Lei et al. [17] obtained O(cid:0)min{−5/3
[23] developed nonconvex extensions to obtain sample complexities O(cid:0)−3/2(cid:1) and O(cid:0)n + n1/2
obtained sample complexities O(cid:0)−3/2(cid:1) and O(cid:0)min{−3/2
the same complexities with constant step sizes and O(cid:0)(n + κ

−1}(cid:1).
−1(cid:1) for
−1}(cid:1) for the two cases respectively 
2) log −1(cid:1) under the gradient-dominant

but require small step sizes that are proportional to . Wang et al. [33] extended Spider to obtain
condition. In addition  Zhou et al. [36] obtained similar results using a nested SVRG approach.

solving the expectation and ﬁnite-sum cases respectively. Fang et al. [8] introduced another variance
reduction technique called Spider  which can be viewed as a more general variant of SARAH. They

  n1/2

  n2/3

2

(cid:2) fν

(cid:0)Eξ[gξ(x)](cid:1)(cid:3) + r(x)  

In addition to the above works on solving special cases of (1) and (2)  there are also considerable
recent works on a more general  two-layer stochastic composite optimization problem

Eν

x∈Rd

minimize

(6)
where fν is parametrized by another random variables ν  which is independent of ξ. For the case
r ≡ 0  Wang et al. [31] derived algorithms to ﬁnd an -approximate solution with sample complexities
O(−4)  O(−3.5) and O(−1.25) for the smooth nonconvex case  smooth convex case and smooth
strongly convex case respectively. For nontrivial convex r  Wang et al. [32] obtained improved sample
complexity of O(−2.25)  O(−2) and O(−1) for the three cases mentioned above respectively.
As a special case of (6)  the following ﬁnite-sum problem also received signiﬁcant attention:

gi(x)

+ r(x) .

fj

i=1

j=1

x∈Rd

minimize

(7)
When r ≡ 0 and the overall objective function is strongly convex  Lian et al. [19] derived two
algorithms based on the SVRG scheme to attain sample complexities O((m + n + κ
3) log −1)) and
4) log −1)) respectively  where κ is some suitably deﬁned condition number. Huo et al.
O((m + n + κ
[11] also used the SVRG scheme to obtain an O(m + n + (m + n)2/3
−1) complexity for the smooth
nonconvex case and O((m + n + κ
3) log −1)) for strongly convex problems with nonsmooth r. More
recently  Zhang and Xiao [35] proposed a composite randomized incremental gradient method based
−1) complexity when F
on the SAGA estimator [6]  which matches the best known O(m+n+(m+n)2/3
under either gradient dominant or strongly convex assumptions. When applied to the special cases (1)
and (2) we focus on in this paper (m = 1)  these results are strictly worse than ours in Table 1.

is smooth and nonconvex  and obtained an improved complexity O(cid:0)(m + n + κ(m + n)2/3) log −1(cid:1)

m(cid:88)

1
m

(cid:19)

n(cid:88)

(cid:18) 1

n

1.2 Contributions and Outline
We develop the CIVR method by extending the variance reduction technique of SARAH [21–23] and
Spider [8  33] to solve the composite optimization problems (1) and (2). The complexities of CIVR
in Table 1 match the best results for solving the non-composite problems (4) and (5)  despite the
additional outer composition and the composite-gradient estimator always being biased. In addition:
• By setting f and gξ’s to be the identity mapping and scalar mappings respectively  problem
(2) includes problem (5) as a special case. Therefore  the lower bounds in [8] for the non-

composite ﬁnite-sum optimization problem (5) indicates that our O(cid:0)min{−3/2
−1}(cid:1)
• Under the assumptions of gradient dominance or strong convexity  the O(cid:0)(cid:0)n + κn1/2(cid:1) log −1(cid:1)

complexity for solving the more general composite ﬁnite-sum problem (2) is near-optimal.

  n1/2

complexity only appeared for the special case (5) in the recent work [18].

Our results indicate that the additional smooth composition in (1) and (2) does not incur higher
complexity compared with (4) and (5)  despite the diﬃculty of dealing with biased estimators. We
believe these results can also be extended to the two-layer problems (6) and (7)  by replacing n with
m + n in Table 1. But the extensions require quite diﬀerent techniques and we will address them in a
separate paper.
The rest of this paper is organized as follows. In Section 2  we introduce the CIVR method. In
Section 3  we present convergence results of CIVR for solving the composite optimization problems (1)
and (2) and the required parameter settings. Better complexities of CIVR under the gradient-dominant
and optimally strongly convex conditions are given in Section 4. In Section 5  we present numerical
experiments for solving a risk-averse portfolio optimization problem (5) on real-world datasets.

2 The composite incremental variance reduction (CIVR) method
With the notations in (3)  we can write the composite stochastic optimization problem (1) as

where F is smooth and r is convex. The proximal operator of r with parameter η is deﬁned as

(cid:8)Φ(x) = F(x) + r(x)(cid:9)  
(cid:110)
(cid:107) y − x(cid:107)2(cid:111)

r(y) +

.

1
2η

minimize

x∈Rd

proxη

r (x) := argmin

y

3

(11)

(12)

Algorithm 1: Composite Incremental Variance Reduction (CIVR)

input: initial point x1

0  step size η > 0  number of epochs T ≥ 1  and a set of triples {τt  Bt  St}
for t = 1  . . .   T  where τt is the epoch length and Bt and St are sample sizes in epoch t.
Sample a set Bt with size Bt from the distribution of ξ  and construct the estimates

for t = 1  ...  T do

yt0 =

1
Bt

gξ(xt0) 

1
Bt
Compute ˜∇F(xt0) = (zt0)T f (cid:48)(yt0) and update: xt1 = proxη
for i = 1  ...  τt − 1 do
Sample a set St

ξ∈Bt

zt0 =

r

with size St from the distribution of ξ  and construct the estimates

i

(8)

(9)

(10)

(cid:88)

g

ξ∈Bt

ξ(xt0) 
(cid:48)

(cid:88)
(cid:0)xt0 − η ˜∇F(xt0)(cid:1).
i−1)(cid:1)  
i−1)(cid:1).

i) − gξ(xt
ξ(xt
i) − g
(cid:48)
i − η ˜∇F(xt

(cid:0)xt

i)(cid:1).

(cid:88)
(cid:88)

ξ∈St

i

(cid:0)gξ(xt
(cid:0)g

ξ(xt
(cid:48)

1
St
1
St

= yt

i−1 +

yt
i

zt
i

= zt

i−1 +
ξ∈St
i) and update: xt

i

i+1 = proxη

r

Compute ˜∇F(xt

i) = (zt

i)T f (cid:48)(yt

end
Set xt+1

output: ¯x randomly chosen from(cid:8)xt

0 = xt
τt

end

.

(cid:9)t=1 ... T

i=0 ... τt−1.

i

We assume that r is relatively simple  meaning that its proximal operator has a closed-form solution
or can be computed eﬃciently. The proximal gradient method [e.g.  20  4] for solving problem (11) is
(13)

xt+1 = proxη

r

where η is the step size. The proximal gradient mapping of Φ is deﬁned as

(cid:0)xt − ηF(cid:48)(xt)(cid:1)  
(cid:0)x − ηF(cid:48)(x)(cid:1)(cid:17)

(cid:16)

Gη(x) (cid:44) 1

η

x − proxη

r

.

(14)

E(cid:2)(cid:107)Gη( ¯x)(cid:107)2(cid:3) ≤  .

r (·) becomes the identity mapping and we have Gη(x) ≡ F(cid:48)(x) for any η > 0.

As a result  the proximal gradient method (13) can be written as xt+1 = xt − η G(xt). Notice that
when r ≡ 0  proxη
Suppose ¯x is generated by a randomized algorithm. We call ¯x an -stationary point in expectation if
(15)
(We assume that η is a constant that does not depend on .) As we mentioned in the introduction  we
measure the eﬃciency of an algorithm by its sample complexity of gξ and their Jacobian g(cid:48)
  i.e.  the
total number of times they need to be evaluated  in order to ﬁnd a point ¯x that satisﬁes (15). Our goal
is to develop a randomized algorithm that has low sample complexity.
We present in Algorithm 1 the Composite Incremental Variance Reduction (CIVR) method. This
methods employs a two time-scale variance-reduced estimator for both the inner function value of
g(·) = Eξ[gξ(·)] and its Jacobian g(cid:48)(·). At the beginning of each outer iteration t (each called an
epoch)  we construct a relatively accurate estimate yt0 for g(xt0) and zt0 for g(cid:48)(xt0) respectively  using
a relatively large sample size Bt. During each inner iteration i of the tth epoch  we construct an
i) respectively  using a smaller sample size St and incremental
estimate yt
corrections from the previous iterations. Note that the epoch length τt and the sample sizes Bt and St
are all adjustable for each epoch t. Therefore  besides setting a constant set of parameters  we can
also adjust them gradually in order to obtain better theoretical properties and practical performance.
This variance-reduction technique was ﬁrst proposed as part of SARAH [21] where it is called
recursive variance reduction. It was also proposed in [8] in the form of a Stochastic Path-Integrated
Diﬀerential EstimatoR (Spider). Here we simply call it incremental variance reduction. A distinct

i) and zt

for g(cid:48)(xt

for g(xt

ξ

i

i

4

(cid:26) E[yt

feature of this incremental estimator is that the inner-loop estimates yt
i−1 (cid:44) g(xt
i)  
i−1 (cid:44) g(cid:48)(xt
i) .

i) − g(xt
i) − g(cid:48)(xt

i] = g(xt
i] = g(cid:48)(xt

i−1) + yt
i−1) + zt

i |xt
i |xt

E[zt

i

and zt

i

are biased  i.e. 

(16)

This is in contrast to two other popular variance-reduction techniques  namely  SVRG [13] and SAGA
[6]  whose gradient estimators are always unbiased. Note that unbiased estimators for g(xt
i) and
g(cid:48)(xt
i) is always biased.
Therefore the our main task is to control the variance and bias altogether for the proposed estimator.

i) are not essential here  because the composite estimator ˜∇F(xt

i)T f (cid:48)(yt

i) = (zt

3 Convergence Analysis
In this section  we present theoretical results on the convergence properties of CIVR (Algorithm 1)
when the composite function F is smooth. More speciﬁcally  we make the following assumptions.
Assumption 1. The following conditions hold concerning problems (1) and (2):

• f : Rp → R is a C1 smooth and (cid:96)f -Lipschitz function and its gradient f (cid:48) is L f -Lipschitz.
• Each gξ : Rd → Rp is a C1 smooth and (cid:96)g-Lipschitz vector mapping and its Jacobian g(cid:48)
is
Lg-Lipschtiz. Consequently  g in (3) is (cid:96)g-Lipschitz and its Jacobian g(cid:48) is Lg-Lipschitz.
• r : Rd → R ∪ {∞} is a convex and lower-semicontinuous function.
• The overall objective function Φ is bounded below  i.e.  Φ∗ = inf x Φ(x) > −∞.

ξ

Assumption 2. For problem (1)  we further assume that there exist constants σg and σg(cid:48) such that
(17)

As a result of Assumption 1  F(x) = f(cid:0)g(x)(cid:1) is smooth and F(cid:48) is LF-Lipschitz continuous with

Eξ[(cid:107)gξ(x) − g(x)(cid:107)2] ≤ σ
2
g  

(cid:48)(x)(cid:107)2] ≤ σ

ξ(x) − g
(cid:48)

Eξ[(cid:107)g

2
g(cid:48) .

2
gL f + (cid:96)f Lg

LF = (cid:96)

G0 := 2(cid:0)(cid:96)

(see proof in the supplementary materials). For convenience  we also deﬁne two constants

(cid:1)  
g(cid:48)(cid:1) .
f L2
2
(18)
F)  hence the step size used later is η = Θ(1/√
It is important to notice that G0 = O(L2
G0) = Θ(1/LF).
We are allowed to use this constant step size mainly due to the assumption that each gξ(·) is smooth 
instead of the weaker assumption that Eξ[gξ(x)] is smooth as in classical stochastic optimization.
In the next two subsections  we present complexity analysis of CIVR for solving problem (1) and (2)
respectively. Due to the space limitation  all proofs are provided in the supplementary materials.

0 := 2(cid:0)(cid:96)

gL2
2
2
f σ
g

gL2
4

2
f σ

and

+ (cid:96)

+ (cid:96)

σ

2

f

g

2

3.1 The composite expectation case
The following results for solving problem (1) are presented with notations deﬁned in (3)  (14) and (18).
Theorem 1. Suppose Assumptions 1 and 2 hold. Given any  > 0  we set T = (cid:100)1/√

(cid:101) and
t = 1  . . .   T .

τt = τ = (cid:100)1/√

Then as long as η ≤

0/(cid:101) 
2

(cid:101)  Bt = B = (cid:100)σ
√

E(cid:2)(cid:107)Gη( ¯x)(cid:107)2(cid:3) ≤(cid:16)8(cid:0)Φ(x1

+12G0

4
2
L
F

for

(cid:101) 

St = S = (cid:100)1/√
−1 + 6(cid:17) ·  = O().
0) − Φ∗(cid:1)η

  the output ¯x of Algorithm 1 satisﬁes

LF +

As a result  the sample complexity of obtaining an -approximate solution is T B + 2T τS = O(cid:0)−3/2(cid:1).

(19)

Note that in the above scheme  the epoch lengths τt and all the batch sizes Bt and St are set to be
constant (depending on a pre-ﬁxed ) without regard of t. Intuitively  we do not need as many samples
in the early stage of the algorithm as in the later stage. In addition  it will be useful in practice to have
a variant of the algorithm that can adaptively choose τt  Bt and St throughout the epochs without
dependence on a pre-ﬁxed precision. This is done in the following theorem.

5

Theorem 2. Suppose Assumptions 1 and 2 hold. We set τt = St = (cid:100)at + b(cid:101) and Bt = (cid:100)σ
where a > 0 and b ≥ 0. Then as long as η ≤

(cid:18) aT + b
E(cid:2)(cid:107)Gη( ¯x)(cid:107)2(cid:3) ≤
ln
complexity of ˜O(cid:0)−3/2(cid:1)  where the ˜O(·) notation hides logarithmic factors.
As a result  obtaining an -approximate solution requires T = ˜O(1/√

  we have for any T ≥ 1 

(cid:32) 8(cid:0)Φ(x1

0) − Φ∗(cid:1)

aT2 + (a + 2b)T

= O(cid:16) ln T

+12G0
6

(cid:19)(cid:33)

a + b

4
2
L
F

T2

6
a

LF +

√

2

+

+

η

2

a + b
) epochs and a total sample

0(at + b)2(cid:101)

(20)

(cid:17)

.

3.2 The composite ﬁnite-sum case
In this section  we consider the composite ﬁnite-sum optimization problem (2). In this case  the
random variable ξ has a uniform distribution over the ﬁnite index set {1  ...  n}. At the beginning
of each epoch in Algorithm 1  we use the full sample size Bt = {1  . . .   n} to compute yt0 and zt0.
Therefore Bt = n for all t and Equation (8) in Algorithm 1 becomes
1
(cid:48)(xt0) =
n

yt0 = g(xt0) =

n(cid:88)

n(cid:88)

gj(xt0)  

j(xt0) .
(cid:48)

zt0 = g

(21)

1
n

g

j=1

j=1

Also in this case  we no longer need Assumption 2.
Theorem 3. Suppose Assumptions 1 holds. Let the parameters in Algorithm 1 be set as Bt = {1  . . .   n}
and τt = St = (cid:100)√
(cid:18) 1√
n(cid:101) for all t. Then as long as η ≤
As a result  obtaining an -approximate solution requires T = O(cid:0)1/(√
complexity of T B + 2T τS = O(cid:0)n +

(cid:19)
n)(cid:1) epochs and a total sample

E(cid:2)(cid:107)Gη( ¯x)(cid:107)2(cid:3) ≤ 8(cid:0)Φ(x1

  we have for any T ≥ 1 

0) − Φ∗(cid:1)

n−1(cid:1).

+12G0
= O

(22)

4
2
L
F

LF +

√

√

√

nT

nT

η

 

Similar to the previous section  we can also choose the epoch lengths and sample sizes adaptively to
save the sampling cost in the early stage of the algorithm. However  due to the ﬁnite-sum structure of
the problem  when the batch size Bt reaches n  we will start to take the full batch at the beginning of
each epoch to get the exact g(xt0) and g(cid:48)(xt0). This leads to the following theorem.
√
Theorem 4. Suppose Assumptions 1 holds. For some positive constants a > 0 and 0 ≤ b <
n 
√
Bt = (cid:100)at + b(cid:101);
√
4
2
+12G0
L
F

denote T0 :=(cid:6)√
when t > T0  we set Bt = {1  . . .   n} and τt = St =(cid:6)√
(cid:40)O(cid:0) ln T
2(cid:1)
O(cid:0)
˜O(cid:0)min(cid:8)√

(cid:7) = O(cid:0)√
n(cid:1). When t ≤ T0 we set the parameters to be τt = St =
n(cid:7). Then as long as η ≤
E(cid:2)(cid:107)Gη( ¯x)(cid:107)2(cid:3) ≤
(cid:1)
  −3/2(cid:9)(cid:1)  where ˜O(·) hides logarithmic factors.

As a result  the total sample complexity of Algorithm 1 for obtaining an -approximate solution is

if T ≤ T0  
if T > T0 .

T
√
n(T−T0+1)

n−1

n−b
a

(23)

LF +

ln n

 

4 Fast convergence rates under stronger conditions
In this section we consider two cases where fast linear convergence can be guaranteed for CIVR.
4.1 Gradient-dominant function
The ﬁrst case is when r ≡ 0 and F is ν-gradient dominant  i.e.  there is some ν > 0 such that

2(cid:107)F(cid:48)(x)(cid:107)2

 

F(x) − inf

F(y) ≤ ν

y

(24)
Note that a µ-strongly convex function is (1/µ)-gradient dominant by this deﬁnition. Hence strong
convexity is a special case of the gradient dominant condition  which in turn is a special case of the
Polyak-Łojasiewicz condition with the Łojasiewicz exponent equal to 2 [see  e.g.  14].
In order to solve (1) with a pre-ﬁxed precision   we use a periodic restart strategy as depicted in
Algorithm 2. For this restarted version of CIVR  we have the following results.

∀ x ∈ Rd.

6

Algorithm 2: Restarted CIVR

set of triples {τt  Bt  St} for t = 1  . . .   T.
Generate ¯xk+1 by Algorithm 1  with parameters T  η  {τt  Bt  St} and initial point ¯xk.

input: initial point ¯x0  step size η > 0  number of restarts K  number of epochs T ≥ 1  and a
for k = 0  ...  K − 1 do
end
output: ¯xK.

(cid:7)  Bt =(cid:6) 12νσ



2
0

(cid:7) and T =(cid:6) 16ν

√


(cid:7). Then

Theorem 5. Consider (1) with r ≡ 0. Suppose Assumptions 1 and 2 hold and F is ν-gradient

dominant. For Algorithm 2  given any  > 0  let τt = St =(cid:6) 1√



η

 

2

√

LF +

4
2
L
F

+12G0

1
2  .

as long as η ≤

(cid:0)F( ¯xk) − F∗(cid:1) +

Consequently  E[F( ¯xk) − F∗] converges linearly to  with a factor of 1

E(cid:2)F( ¯xk+1) − F∗(cid:3) ≤ 1
complexity for ﬁnding an -solution is O(cid:0)(cid:0)ν−1(cid:1) ln −1(cid:1).
Bt = (cid:100)√
(cid:0)F( ¯xk) − F∗(cid:1) .
As a result  the sample complexity for ﬁnding an -solution is O(cid:0)(cid:0)n + ν

E(cid:2)F( ¯xk+1) − F∗| ¯xk(cid:3) ≤ 1

The restart strategy also applies to the ﬁnite-sum case.
Theorem 6. Consider problem (2) with r ≡ 0. Suppose Assumption 1 hold and F is ν-gradient
√
dominant.
√
η ≤

n(cid:101) and T = (cid:6) 16ν√
(cid:1).
(cid:1) ln 1

(cid:7)  then as long as

In Algorithm 2  if we set τt = St =

2 per period. The sample

+12G0

(25)

(26)

4
2
L
F

LF +

2

√

nη

 

n

η



It is worth noting that for both cases  the number of epochs T ∝ η−1. When we take more conservative
values η  it will directly result in worse complexity results. This comment also applies to the optimally
strongly convex objective function case in the next section.

4.2 Optimally strongly convex function
In this part  we assume a µ-optimally strongly convex condition on the function Φ(x) = F(x) + r(x) 
i.e.  there exists a µ > 0 such that

Φ(x) − Φ(x∗) ≥ µ

2 (cid:107)x − x∗(cid:107)2

 

∀x ∈ Rd.

(27)

(cid:7)  Bt =(cid:6) 9σ
(cid:0)Φ( ¯xk) − Φ∗(cid:1) +

2
0
2µ

1
2  .

We have the following two results for solving problems (1) and (2) respectively.
Theorem 7. Consider problem (1). Suppose Assumptions 1 and 2 hold and Φ is µ-optimally strongly
µη (cid:101). Then if we choose

(cid:7) and T = (cid:100) 5√



convex. In Algorithm 2  let us set τt = St =(cid:6) 1√
E(cid:2)Φ( ¯xk+1) − Φ∗(cid:3) ≤ 1
-solution is O(cid:0)µ−1

−1 ln −1(cid:1).

+36G0

2
2
L
F

η <

LF +

√

 



2

(28)
Consquently  E[Φ( ¯xk) − Φ∗] converges linearly to . The total sample complexity for ﬁnding an

Theorem 8. Consider the ﬁnite-sum problem (2). Suppose Assumption 1 hold and Φ is µ-optimally
strongly convex. In Algorithm 2  let us set τt = St =
η <

n(cid:101) and T =(cid:6)
Bt = (cid:100)√
E(cid:2)Φ( ¯xk+1) − Φ∗(cid:3) ≤ 1
(cid:0)Φ( ¯xk) − Φ∗(cid:1).
(cid:1) ln 1
(cid:1).
The sample complexity of ﬁnding an -solution is O(cid:0)(cid:0)n +

(cid:7). Then if we choose

5√
nµη

+36G0

(29)

2
2
L
F

LF +

√

√

2

√

 

n
µη



7

Figure 1: Experiments on the risk-averse portfolio optimization problem.

above complexities become O(cid:0)(cid:0)κ−1(cid:1) ln −1(cid:1) and O(cid:0)(cid:0)n + κn1/2(cid:1) ln −1(cid:1).

If we deﬁne a condition number κ = LF/µ  then since η = Θ(1/LF)  we have 1/(µη) = O(κ) and the

For Algorithm 2 in both gradient-dominant and strongly convex cases  we have the following remarks.
Remark 1. In Algorithm 2  each run of Algorithm 1 includes a random selection of output. In average
it wastes half of the iterates. This waste can be prevented by pre-generating the “stop times” or the
output indeces. We can stop Algorithm 1 and output the last iterate whenever the method hits this time.
Remark 2. In Algorithm 2  the linear-convergence is achieved by restarting. This strategy is proposed
partly due to the epoch structure of Algorithm 1. Therefore  if we break this epoch structure by the
loopless variance reduction techniques introduced in [16]  the restarts may be avoided.

5 Numerical Experiments
In this section  we present numerical experiments for a risk-averse portfolio optimization problem.
Suppose there are d assets that one can invest during n time periods labeled as {1  ...  n}. Let Ri  j
be the return or payoﬀ per unit of asset j at time i  and Ri be the vector consists of Ri 1  . . .   Ri d.
Let x ∈ Rd be the decision variable  where each component xj represent the amount of investment
or percentage of the total investment allocated to asset j  for j = 1  . . .   d. The same allocations
or percentages of allocations are repeated over the n time periods. We would like to maximize the
average return over the n periods  but with a penalty on the variance of the returns across the n periods
(in other words  we would like diﬀerent periods to have similar returns).
This problem is formulated as a mean-variance trade-oﬀ:
maximize
 
where the random variable ξ ∈ {1  . . .   n} takes discrete values uniformly at random and hence
makes the problem a ﬁnite-sum. The functions hi(x) = (cid:104)Ri  x(cid:105) for i = 1  . . .   n are the rewards. The
function r can be chosen as the indicator function of an (cid:96)1 ball  or a soft (cid:96)1 regularization term. We
choose the latter one in our experiments to obtain a sparse asset allocation. By using the mappings

(cid:110) E(cid:2)hξ(x)(cid:3) − λVar(cid:0)hξ(x)(cid:1) + r(x) ≡ E(cid:2)hξ(x)(cid:3) − λ
gξ(x) : Rd → R2 =(cid:2)hξ(x) h2
ξ(x)(cid:3)T

(cid:16)E(cid:2)h2
ξ(x)(cid:3) − E(cid:2)hξ(x)(cid:3)2(cid:17)

+ r(x)(cid:111)

it can be further transformed into the composite ﬁnite-sum problem (2)  hence readily solved by the
CIVR method. Here  the intermediate dimension is very low  i.e.  p = 2. This leads to very little
overhead in computation compared with stochastic optimization without composition.
For comparison  we implement the C-SAGA algorithm [35] as a benchmark. As another benchmark 
this problem can also be formulated as a two-layer composite ﬁnite-sum problem (7)  which was done

x∈Rd

 

f(y  z) : R2 → R = −y + λy

2 − λz 

8

051015# of samples10510-10100|| F(xk) + r(xk)||Industrial-30 datasetC-SAGAVRSC-PGASC-PGCIVRCIVR-adp02468# of samples105100102|| F(xk) + r(xk)||Industrial-38 datasetC-SAGAVRSC-PGASC-PGCIVRCIVR-adp01234# of samples10610-2100102104|| F(xk) + r(xk)||Industrial-49 datasetC-SAGAVRSC-PGASC-PGCIVRCIVR-adp051015# of samples10510-2010-101001010(F(xk)+r(xk)) - (F(x*)+r(x*))Industrial-30 datasetC-SAGAVRSC-PGASC-PGCIVRCIVR-adp02468# of samples10510-2100102104(F(xk)+r(xk)) - (F(x*)+r(x*))Industrial-38 datasetC-SAGAVRSC-PGASC-PGCIVRCIVR-adp01234# of samples10610-2100102104(F(xk)+r(xk)) - (F(x*)+r(x*))Industrial-49 datasetC-SAGAVRSC-PGASC-PGCIVRCIVR-adpin [11] and [19]. We solve the two-layer formulation by ASC-PG [32] and VRSC-PG [11]. Finally 
we also implemented CIVR-adp  which is the adaptive sampling variant described in Theorem 4.
We test these algorithms on three real world portfolio datasets  which contain 30  38 and 49 industrial
portfolios respectively  from the Keneth R. French Data Library1. For the three datasets  the daily data
of the most recent 24452  10000 and 24400 days are extracted respectively to conduct the experiments.
We set the parameter λ = 0.2 in (5) and use an (cid:96)1 regularization r(x) = 0.01(cid:107)x(cid:107)1. The experiment
results are shown in Figure 1. The curves are averaged over 20 runs and are plotted against the number
of samples of the component functions (the horizontal axis).
the adaptive batch size St =(cid:6)min{10t + 1 
Throughout the experiments  VRSC-PG and C-SAGA algorithms use the batch size S = (cid:100)n2/3(cid:101) while
CIVR uses the batch size S = (cid:100)√
n(cid:101)  all dictated by their complexity theory. CIVR-adp employs
VRSC-PG  C-SAGA  CIVR and CIVR-adp use the same step size η = 0.1. They are chosen from
the set η ∈ {1  0.1  0.01  0.001  0.0001} by experiments. And η = 0.1 works best for all four tested
methods simultaneously. Similarly  η = 0.001 is chosen for the Industrial-38 dataset and η = 0.0001
is chosen for the Industrial-49 dataset. For ASC-PG  we set its step size parameters αk = 0.001/k
and βk = 1/k [see details in 32]. They are hand-tuned to ensure ASC-PG converges fast among a
range of tested parameters. Overall  CIVR and CIVR-adp outperform other methods.

n}(cid:7) for t = 1  ...  T. For Industrial-30 dataset  all of

√

References
[1] Zeyuan Allen-Zhu. Natasha: Faster non-convex stochastic optimization via strongly non-convex
parameter. In Proceedings of the 34th International Conference on Machine Learning (ICML) 
volume 70 of Proceedings of Machine Learning Research  pages 89–97  Sydney  Australia 
2017.

[2] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than SGD. In Advances in
Neural Information Processing Systems 31  pages 2675–2686. Curran Associates  Inc.  2018.
[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
Proceedings of the 33rd International Conference on Machine Learning (ICML)  pages 699–707 
2016.

[4] Amir Beck. First-Order Methods in Optimization. MOS-SIAM Series on Optimization. SIAM 

2017.

[5] Christoph Dann  Gerhard Neumann  and Jan Peters. Policy evaluation with temporal diﬀerences:

a survey and comparison. Journal of Machine Learning Research  15(1):809–883  2014.

[6] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems 27  pages 1646–1654  2014.

[7] Darinka Dentcheva  Spiridon Penev  and Andrzej Ruszczyński. Statistical estimation of
composite risk functionals and risk optimization problems. Annals of the Institute of Statistical
Mathematics  69(4):737–760  2017.

[8] Cong Fang  Chris Junchi Li  Zhouchen Lin  and Tong Zhang. Spider: Near-optimal non-
convex optimization via stochastic path-integrated diﬀerential estimator. In Advances in Neural
Information Processing Systems 31  pages 689–699. Curran Associates  Inc.  2018.

[9] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex

stochastic programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[10] Saeed Ghadimi  Andrzej Ruszczyński  and Mengdi Wang. A single time-scale stochastic
approximation method for nested stochastic optimization. Preprint  arXiv:1812.01094  2018.
[11] Zhouyuan Huo  Bin Gu  Ji Jiu  and Heng Huang. Accelerated method for stochastic composition
optimization with nonsmooth regularization. In Proceedings of the 32nd AAAI Conference on
Artiﬁcial Intelligence  pages 3287–3294  2018.

1http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html

9

[12] A. N. Iusem  A. Jofré  R. I. Oliveira  and P. Phompson. Extragradient method with variance
reduction for stochastic variational inequalities. SIAM Journal on Optimization  27(2):686–724 
2017.

[13] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems 26  pages 315–323  2013.

[14] Hamed Karimi  Julie Nutini  and Mark Schmidt. Linear convergence of gradient method and
proximal-gradient methods under the Polyak-Łojasiewicz condition. In Machine Learning and
Knowledge Discovery in Database - European Conference  Proceedings  pages 795–811  2016.

[15] J. Koshal  A. Nedić  and U. B. Shanbhag. Regularized iterative stochastic approximation methods
for stochastic variational inequality problems. IEEE Transactions on Automatic Control  58(3):
594–609  2013.

[16] Dmitry Kovalev  Samuel Horváth  and Peter Richtárik. Don’t jump through hoops and
remove those loops: SVRG and Katyusha are better without the outer loop. arXiv preprint
arXiv:1901.08689  2019.

[17] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Non-convex ﬁnite-sum optimization
via SCSG methods. In Advances in Neural Information Processing Systems 30  pages 2348–2358.
Curran Associates  Inc.  2017.

[18] Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems 31  pages 5564–5574.
Curran Associates  Inc.  2018.

[19] Xiangru Lian  Mengdi Wang  and Ji Liu. Finite-sum composition optimization via variance
reduced gradient descent. In Proceedings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS)  pages 1159–1167  2017.

[20] Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical Program-

ming  140(1):125–161  2013.

[21] Lam M. Nguyen  Jie Liu  Katya Scheinberg  and Martin Takáč. SARAH: A novel method for
machine learning problems using stochastic recursive gradient. In Doina Precup and Yee Whye
Teh  editors  Proceedings of the 34th International Conference on Machine Learning (ICML) 
volume 70 of Proceedings of Machine Learning Research (PMLR)  pages 2613–2621  Sydney 
Australia  2017.

[22] Lam M. Nguyen  Marten van Dijk  Dzung T. Phan  Phuong Ha Nguyen  Tsui-Wei Weng 
and Jayant R. Kalagnanam. Finite-sum smooth optimization with sarah. arXiv preprint 
arXiv:1901.07648  2019.

[23] Nhan H. Pham  Lam M. Nguyen  Dzung T. Phan  and Quoc Tran-Dinh. ProxSARAH: An
eﬃcient algorithmic framework for stochastic composite nonconvex optimization. arXiv preprint 
arXiv:1902.05679  2019.

[24] Sashank J. Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex Smola. Stochastic
variance reduction for nonconvex optimization. In Proceedings of The 33rd International
Conference on Machine Learning  volume 48 of Proceedings of Machine Learning Research 
pages 314–323  New York  New York  USA  2016.

[25] Sashank J Reddi  Suvrit Sra  Barnabás Póczos  and Alex Smola. Fast incremental method
for smooth nonconvex optimization. In 2016 IEEE 55th Conference on Decision and Control
(CDC)  pages 1971–1977. IEEE  2016.

[26] Sashank J Reddi  Suvrit Sra  Barnabás Póczos  and Alexander J Smola. Proximal stochastic
methods for nonsmooth nonconvex ﬁnite-sum optimization. In Advances in Neural Information
Processing Systems 29  pages 1145–1153  2016.

[27] R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press  1970.

10

[28] R. Tyrrell Rockafellar. Coherent approaches to risk in optimization under uncertainty. INFORMS

TutORials in Operations Research  2007.

[29] Andrzej Ruszczyński. Advances in risk-averse optimization. INFORMS TutORials in Operation

Research  2013.

[30] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press 

Cambridge  MA  1998.

[31] Mengdi Wang  Ethan X Fang  and Han Liu. Stochastic compositional gradient descent: algo-
rithms for minimizing compositions of expected-value functions. Mathematical Programming 
161(1-2):419–449  2017.

[32] Mengdi Wang  Ji Liu  and Ethan Fang. Accelerating stochastic composition optimization.

Journal of Machine Learning Research  18(105):1–23  2017.

[33] Zhe Wang  Kaiyi Ji  Yi Zhou  Yingbin Liang  and Vahid Tarokh. SpiderBoost: A class of faster
variance-reduced algorithms for nonconvex optimization. arXiv preprint  arXiv:1810.10690 
2018.

[34] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[35] Junyu Zhang and Lin Xiao. A composite randomized incremental gradient method.

In
Proceedings of the 36th International Conference on Machine Learning (ICML)  number 97 in
Proceedings of Machine Learning Research (PMLR)  Long Beach  California  2019.

[36] Dongruo Zhou  Pan Xu  and Quanquan Gu. Stochastic nested variance reduced gradient descent
for nonconvex optimization. In Advances in Neural Information Processing Systems 31  pages
3921–3932. Curran Associates  Inc.  2018.

11

,Junyu Zhang
Lin Xiao