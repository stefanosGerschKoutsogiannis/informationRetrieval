2018,First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time,(This is a theory paper) In this paper  we consider first-order methods for solving stochastic non-convex optimization problems. The key building block of the proposed algorithms is first-order procedures to extract negative curvature from the Hessian matrix through a principled sequence starting from noise  which are referred to {\it NEgative-curvature-Originated-from-Noise or NEON} and are of independent interest. Based on this building block  we design purely first-order stochastic algorithms for escaping from non-degenerate saddle points with a much better time complexity (almost linear time in  the problem's dimensionality). In particular  we develop a general framework of {\it first-order stochastic algorithms} with a second-order convergence guarantee based on our new technique and existing algorithms that may only converge to a first-order stationary point. For finding a nearly {\it second-order stationary point} $\x$ such that $\|\nabla F(\x)\|\leq \epsilon$ and $\nabla^2 F(\x)\geq -\sqrt{\epsilon}I$ (in high probability)  the best time complexity of the presented algorithms is $\widetilde O(d/\epsilon^{3.5})$  where $F(\cdot)$ denotes the objective function and $d$ is the dimensionality of the problem. To the best of our knowledge  this is the first theoretical result of first-order stochastic algorithms with an almost linear time in terms of problem's dimensionality for finding second-order stationary points  which is  even competitive with  existing stochastic algorithms hinging on the second-order information.,First-order Stochastic Algorithms for Escaping From

Saddle Points in Almost Linear Time

Yi Xu†  Rong Jin‡  Tianbao Yang†

† Department of Computer Science  The University of Iowa  Iowa City  IA 52246  USA

‡ Machine Intelligence Technology  Alibaba Group  Bellevue  WA 98004  USA

{yi-xu  tianbao-yang}@uiowa.edu  jinrong.jr@alibaba-inc.com

Abstract

In this paper  we consider ﬁrst-order methods for solving stochastic non-convex
optimization problems. The key building block of the proposed algorithms is ﬁrst-
order procedures to extract negative curvature from the Hessian matrix through a
principled sequence starting from noise  which are referred to NEgative-curvature-
Originated-from-Noise or NEON and are of independent interest. Based on this
building block  we design purely ﬁrst-order stochastic algorithms for escaping
from non-degenerate saddle points with a much better time complexity (almost
linear time in the problem’s dimensionality) under a bounded variance condition of
stochastic gradients than previous ﬁrst-order stochastic algorithms. In particular 
we develop a general framework of ﬁrst-order stochastic algorithms with a second-
order convergence guarantee based on our new technique and existing algorithms
that may only converge to a ﬁrst-order stationary point. For ﬁnding a nearly
I
(in high probability)  the best time complexity of the presented algorithms is

second-order stationary point x such that (cid:107)∇F (x)(cid:107) ≤  and ∇2F (x) ≥ −√
(cid:101)O(d/3.5)  where F (·) denotes the objective function and d is the dimensionality

of the problem. To the best of our knowledge  this is the ﬁrst theoretical result of
ﬁrst-order stochastic algorithms with an almost linear time in terms of problem’s
dimensionality for ﬁnding second-order stationary points  which is even competitive
with existing stochastic algorithms hinging on the second-order information.

Introduction

1
The problem of interest in this paper is Stochastic Non-Convex Optimization given by

F (x) (cid:44) Eξ[f (x; ξ)] 

min
x∈Rd

(1)

where ξ is a random variable and f (x; ξ) is a random smooth non-convex function of x. The only
information available of F (x) to us is sampled stochastic functions f (x; ξ) and their gradients.
A popular choice of algorithms for solving (1) is (mini-batch) stochastic gradient descent (SGD)
method and its variants [6]. However  these algorithms do not necessarily guarantee to escape from a
saddle point (more precisely a non-degenerate saddle point) x satisfying that: ∇F (x) = 0 and the
minimum eigen-value of ∇2F (x)) is less than 0. Recently  new variants of SGD by adding isotropic
noise into the stochastic gradient were proposed (noisy SGD [5]  stochastic gradient Langevin
dynamics (SGLD) [23]). These two works provide rigorous analyses of the noise-injected update for
escaping from a saddle point. Unfortunately  both variants suffer from a polynomial time complexity
with a super-linear dependence on the dimensionality d (at least a power of 4)  which renders them
not practical for optimizing problems of high dimension.
On the other hand  second-order information carried by the Hessian has been utilized to escape from
a saddle point  which usually yields an almost linear time complexity in terms of the dimensionality
d under the assumption that the Hessian-vector product (HVP) can be performed in a linear time. In

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Table 1: Comparison with existing Stochastic Algorithms for achieving an (  γ)-SSP to (1)  where
p is a number at least 4  IFO (incremental ﬁrst-order oracle) and ISO (incremental second-order
oracle) are terminologies borrowed from [20]  representing ∇f (x; ξ) and ∇2f (x; ξ)v respectively 

Th denotes the runtime of ISO and Tg denotes the runtime of IFO. (cid:101)O(·) hides a poly-logarithmic

factor. SM refers to stochastic momentum methods. For γ  we only consider as lower as 1/2.

Algorithm
Noisy SGD [5]
SGLD [23]

Natasha2 [1]
Natasha2 [1]
SNCG [17]
SVRG-Hessian [20] (ﬁnite-sum objectives)
(n is number of components)
NEON-SGD  NEON-SM (this work)
NEON-SCSG (this work)
NEON-SCSG (this work)
NEON-Natasha (this work)
NEON-Natasha (this work)
NEON-SVRG (this work) (ﬁnite sum)

Oracle
IFO
IFO

Target
(  1/2)-SSP  high probability
(  1/2)-SSP  high probability

IFO + ISO (  1/2)-SSP  expectation
IFO + ISO (  1/4)-SSP  expectation
IFO + ISO (  1/2)-SSP  high probability
IFO + ISO (  1/2)-SSP  high probability

IFO
IFO
IFO
IFO
IFO
IFO

(  1/2)-SSP  high probability
(  1/2)-SSP  high probability
(  4/9)-SSP  high probability
(  1/2)-SSP  expectation
(  1/4)-SSP  expectation
(  1/2)-SSP  high probability

Time Complexity

(cid:101)O (Tgdp−p)
(cid:101)O(cid:0)Tgdp−4(cid:1)
(cid:101)O(cid:0)Tg−3.5 + Th−2.5(cid:1)
(cid:101)O(cid:0)Tg−3.25 + Th−1.75(cid:1)
(cid:101)O(cid:0)Tg−4 + Th−2.5(cid:1)
(cid:101)O(cid:0)Tg(n2/3−2 + n−1.5)
+Th(n−1.5 + n3/4−7/4)(cid:1)
(cid:101)O(cid:0)Tg−4(cid:1)
(cid:101)O(cid:0)Tg−3.5(cid:1)
(cid:101)O(cid:0)Tg−3.33(cid:1)
(cid:101)O(cid:0)Tg−3.5(cid:1)
(cid:101)O(cid:0)Tg−3.25(cid:1)
(cid:101)O(cid:0)Tg

(cid:0)n2/3−2 + n−1.5 + −2.75(cid:1)(cid:1)

practice  HVP can be estimated by a ﬁnite difference approximation using two gradient evaluations.
However  the rigorous analysis of algorithms using such noisy approximation for solving non-convex
optimization remains unsolved  and heuristic approaches may suffer from numerical issues. Although
for some problems with special structures (e.g.  neural networks)  HVP can be efﬁciently computed
using gradients  a HVP-free method that can escape saddle points for a broader family of non-convex
problems is still desirable.
This paper aims to design HVP-free stochastic algorithms for solving (1)  which can converge
to second order stationary points with a time complexity that is almost linear in the problem’s
dimensionality. Our main contributions are:
• As a key building block of proposed algorithms  ﬁrst-order procedures (NEON) are proposed
to extract negative curvature from the Hessian using a principled sequence starting from noise.
Interestingly  our perspective of NEON connects the existing two classes of methods (noise-
based and HVP-based) for escaping from saddle points. We provide a formal analysis of simple
procedures based on gradient descent and accelerated gradient method for exacting a negative
curvature direction from the Hessian.

• We develop a general framework of ﬁrst-order algorithms for stochastic non-convex optimization
by combining the proposed ﬁrst-order NEON procedures to extract negative curvature with existing
ﬁrst-order stochastic algorithms that aim at a ﬁrst-order critical point. We also establish the time
complexities of several interesting instances of our general framework for ﬁnding a nearly (  γ)-
second-order stationary point (SSP)  i.e.  (cid:107)∇F (x)(cid:107) ≤   and λmin(∇2F (x)) ≥ −γ  where (cid:107) · (cid:107)
represents Euclidean norm of a vector and λmin(·) denotes the minimum eigen-value. A summary
of our results and existing results for Stochastic Non-Convex Optimization is presented in Table 1.

2 Other Related Work
SGD and its many variants (e.g.  mini-batch SGD and stochastic momentum (SM) methods) have
been analyzed for stochastic non-convex optimization [6  7  8  22]. The iteration complexities
of all these algorithms is O(1/4) for ﬁnding a ﬁrst-order stationary point (FSP) (in expectation
2] ≤ 2 or in high probability). Recently  there are some improvements for stochastic
E[(cid:107)∇F (x)(cid:107)2
non-convex optimization. [14] proposed a ﬁrst-order stochastic algorithm (named SCSG) using the
variance-reduction technique  which enjoys an iteration complexity of O(1/−10/3) for ﬁnding an
FSP (in expectation)  i.e.  E[(cid:107)∇F (x)(cid:107)2
2] ≤ 2. [1] proposed a variant of SCSG (named Natasha1.5)
with the same convergence and complexity. An important application of NEON is that previous
stochastic algorithms that have a ﬁrst-order convergence guarantee can be strengthened to enjoy a
second-order convergence guarantee by leveraging the proposed ﬁrst-order NEON procedures to
escape from saddle points. We will analyze several algorithms by combining the updates of SGD 
SM  and SCSG with the proposed NEON.

2

Several recent works [17  1  20] propose to strengthen existing ﬁrst-order stochastic algorithms to
have second-order convergence guarantee by leveraging the second-order information. [17] used
mini-batch SGD  [20] used SVRG for a ﬁnite-sum problem  and [1] used a similar algorithm to
SCSG for their ﬁrst-order algorithms. The second-order methods used in these studies for computing
negative curvature can be replaced by the proposed NEON procedures. It is notable although a
generic approach for stochastic non-convex optimization was proposed in [20]  its requirement on the
ﬁrst-order stochastic algorithms precludes many interesting algorithms such as SGD  SM  and SCSG.
Stronger convergence guarantee (e.g.  converging to a global minimum) of stochastic algorithms has
been studied in [9] for a certain family of problems  which is beyond the setting of the present work.
It is also worth mentioning that the ﬁeld of non-convex optimization is moving so fast that similar
results have appeared online after the preliminary version of this work [2]. Allen-Zhu and Li [2]
proposed NEON2 for ﬁnding a negative curvature  which includes a stochastic version and a de-
terministic version. We notice several differences between the two works: (i) they used Gaussian
random noise with a variance proportional to d−C  where C is a large unknown constant  in contrast
our NEON and NEON+ procedures use random noise sampled from the sphere of an Euclidean ball
−2(d); (ii) the update of their deterministic NEON2det is constructed
with radius proportional to log
based on the Chebyshev polynomial  in contrast our NEON+ with a similar iteration complexity is
based on the well-known Nesterov’s accelerated gradient method; (iii) we provide a general frame-
work/analysis for promoting ﬁrst-order algorithms to enjoy second-order convergence  which could
be useful for promoting new ﬁrst-order stochastic algorithms; (iv) the reported iteration complexity
of their NEON2online is better than our stochastic variants of NEON. However  in most cases the total
complexity for ﬁnding an ( 
)-SSP is dominated by the complexity for ﬁnding a stationary point
not by the complexity of stochastic NEON for ﬁnding a negative curvature.

√

3 Preliminaries
Let (cid:107) · (cid:107) denote the Euclidean norm of a vector and (cid:107) · (cid:107)2 denote the spectral norm of a matrix.
Let Sd
r denote the sphere of an Euclidean ball centered at zero with radius r  and [t] denote a
set {0  . . .   t}. A function f (x) has a L1-Lipschitz continuous gradient if it is differentiable and
there exists L1 > 0 such that (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L1(cid:107)x − y(cid:107) holds for any x  y ∈ Rd. A
function f (x) has a L2-Lipschitz continuous Hessian if it is twice differentiable and there exists
L2 > 0 such that (cid:107)∇2f (x) − ∇2f (y)(cid:107)2 ≤ L2(cid:107)x − y(cid:107) holds for any x  y ∈ Rd. It implies that
|f (x) − f (y) − ∇f (y)(cid:62)(x − y) − 1

2 (x − y)(cid:62)∇2f (y)(x − y)| ≤ L2

6 (cid:107)x − y(cid:107)3  and

(cid:107)∇f (x + u) − ∇f (x) − ∇2f (x)u(cid:107) ≤ L2(cid:107)u(cid:107)2/2.

(2)

the global minimum of (1).

gradient and L2-Lipschitz continuous Hessian.

We ﬁrst make the following assumptions regarding the problem (1).
Assumption 1. For the problem (1)  we assume that
(i). every random function f (x; ξ) is twice differentiable  and it has L1-Lipschitz continuous
(ii). given an initial point x0  there exists ∆ < ∞ such that F (x0)− F (x∗) ≤ ∆  where x∗ denotes
(iii). there exists G > 0 such that E[exp((cid:107)∇f (x; ξ) − ∇F (x)(cid:107)2/G2)] ≤ exp(1) holds.
Remark. (1) the analysis of NEON or NEON+ or their stochastic versions for extracting the negative
curvature only requires Assumption 1 (i). Indeed  the Lipschitz continuous Hessian can be relaxed
to locally Lipchitz continuous Hessian condition according to our analysis. (2) Assumptions 1 (ii)
(iii) are used in the analysis of Section 5  which are standard assumptions made in the literature
of stochastic non-convex optimization [6  7  8]. Assumption 1 (iii) implies that E[(cid:107)∇f (x; ξ) −
∇F (x)(cid:107)2] ≤ V (cid:44) G2 holds. For stating our time complexities  we assume G is independent of d for
ﬁnding an approximate local minimum in Section 5. Nevertheless  our comparison of the proposed
algorithms with previous algorithms (e.g.  SGLD [23]  SNCG [17]  Natasha2 [1]) in the stochastic
setting are fair because similar assumptions are also made. We also note that [5] makes a stronger
assumption about the stochastic gradients  i.e.  (cid:107)∇f (x; ξ) − ∇F (x)(cid:107) ≤ O(d)  which leads to a
worse dependence of time complexity on d  i.e.  O(dp) with p ≥ 4.
Next  we discuss a second-order method based on HVPs to escape from a non-degenerate saddle
point x of a function f (x) that satisﬁes λmin(∇2f (x)) ≤ −γ  which can be found in many previous
studies [21  16  4]. The method is based on a negative curvature (NC for short is used in the sequel)

3

direction v ∈ Rd that satisﬁes (cid:107)v(cid:107) = 1 and

v(cid:62)∇2f (x)v ≤ −cγ 

(3)

where c > 0 is a constant. Given such a vector v  we can update the solution according to

x+ = x − cγ
L2

sign(v(cid:62)∇f (x))v  or x(cid:48)

(4)
where ¯ξ ∈ {1 −1} is a Rademacher random variable used when ∇f (x) is not available. The
following lemma establishes that the objective value of x+ or x(cid:48)
+ is less than that of x by a sufﬁcient
amount  which makes it possible to escape from the saddle point x.
Lemma 1. For x satisfying λmin(∇2f (x)) ≤ −γ and v satisfying (3)  let x+  x(cid:48)
then we have f (x) − f (x+) ≥ c3γ3

and E[f (x) − f (x(cid:48)

+ be given in (4) 

+)] ≥ c3γ3

.

¯ξv 

+ = x − cγ
L2

3L2
2

3L2
2

To compute a NC direction v that satisﬁes (3)  we can employ the Lanczos method or the Power
method for computing the maximum eigen-vector of the matrix (I − η∇2f (x))  where ηL1 ≤ 1 such
that I − η∇2f (x) (cid:23) 0. The Power method starts with a random vector v1 ∈ Rd (e.g.  drawn from a
uniform distribution over the unit sphere) and iteratively compute vτ +1 = (I − η∇2f (x))vτ   τ =
1  . . .   t. Following the results in [13]  it can be shown that if λmin(∇2f (x)) ≤ −γ  then with at most
t ∇2f (x)ˆvt ≤ − γ
log(d/δ2)L1
holds with high probability 1 − δ. Similarly  the Lanczos method (e.g.  Lemma 11 in [21]) can ﬁnd
√
√
such a vector ˆvt with a lower number of HVPs  i.e.  min(d  log(d/δ2)
2

HVPs  the Power method ﬁnds a vector ˆvt = vt/(cid:107)vt(cid:107) such that ˆv(cid:62)

).

L1

2ε

γ

2

4 Key Building Block: Extracting NC From Noise

Our HVP-free stochastic algorithms with provable guarantees for solving (1) presented in next section
are based on a key building block  i.e.  extracting NC from noise using only ﬁrst-order information.
To tackle the stochastic objective in (1)  our method is to compute a NC based on a mini-batch of
i=1 f (x; ξi)/m for a sufﬁciently large number of samples. Thus  a key building block of

functions(cid:80)m

the proposed method is a ﬁrst-order procedure to extract NC for a non-convex function f (x) 1.
Below  we ﬁrst propose a gradient descent based method for extracting NC  which achieves a similar
iteration complexity to the Power method. Second  we present an accelerated gradient method to
extract the NC to match the iteration complexity of the Lanczos method. Finally  we discuss the
application of these procedures for stochastic non-convex optimization using mini-batch.

4.1 Extracting NC by NEON
The NEON is inspired by the perturbed gradient descent (PGD) method (a method for solving
deterministic non-convex problems) proposed in the seminal work [11] and its connection with the
Power method as discussed shortly. Around a saddle point x  the PGD method ﬁrst generates a
random noise vector ˆe from the sphere of an Euclidean ball with a proper radius  then starts with a
noise perturbed solution x0 = x + ˆe  the PGD generates the following sequence of solutions:

xτ = xτ−1 − η∇f (xτ−1).

x)  τ = 1  . . .   t. It is clear that for τ = 1  . . .   t 

(5)
To establish a connection with the Power method and motivate the proposed NEON  let us deﬁne

another sequence of(cid:98)xτ = xτ − x. Then we have the recurrence for(cid:98)xτ = (cid:98)xτ−1 − η∇f ((cid:98)xτ−1 +

(cid:98)xτ =(cid:98)xτ−1 − η∇f (x) − η(∇f ((cid:98)xτ−1 + x) − ∇f (x)).
(cid:98)xτ ≈(cid:98)xτ−1 − η∇2f (x)(cid:98)xτ−1 = (I − η∇2f (x))(cid:98)xτ−1.

To understand the above update  we adopt the following approximation: ∇f (x) ≈ 0 for an ap-
proximate saddle point  and from the Lipschitz continuous Hessian condition (2)  we can see that

∇f ((cid:98)xτ−1 + x) − ∇f (x) ≈ ∇2f (x)(cid:98)xτ−1 as long as (cid:107)(cid:98)xτ−1(cid:107) is small. Then for τ = 1  . . .   t 
updated solution xt = x +(cid:98)xt can decrease the objective value due to that(cid:98)xt is close to a NC of the

It is obvious that the above approximated recurrence is close to the the sequence generated by the
Power method with the same starting random vector ˆe = v1. This intuitively explains that why the

1We abuse the same notation f here.

4

Algorithm 1 NEON(f  x  t F  r)
1: Input: f  x  t F  r
2: Generate u0 randomly from Sd
r
3: for τ = 0  . . .   t do
4:
5: end for
6: if mini∈[t+1] (cid:107)ui(cid:107)≤U
7:
8: else return 0

ˆfx(ui) ≤ −2.5F

return uτ(cid:48)  τ(cid:48) = arg mini∈[t+1] (cid:107)ui(cid:107)≤U

uτ +1 = uτ − η(∇f (x + uτ ) − ∇f (x))

Algorithm 2 NEON+(f  x  t F  U  ζ  r)
1: Input: f  x  t F  U  ζ  r
2: Generate y0 = u0 randomly from Sr
3: for τ = 0  . . .   t do
2(cid:107)yτ − uτ(cid:107)2
4:

if ∆x(yτ   uτ ) < − γ
then

return v =NCFind(y0:τ   u0:τ )

end if
compute (yτ +1  uτ +1) by (8)

ˆfx(yi) ≤ −2F then
ˆfx(yi)

let τ(cid:48) = arg mini (cid:107)yi(cid:107)≤U
return yτ(cid:48)

5:
6:
7:
8: end for
9: if mini (cid:107)yi(cid:107)≤U
10:
11:
12: else
13:
14: end if

return 0

ˆfx(ui)

√

6ηF}

Algorithm 3 NCFind (y0:τ   u0:τ )
1: if minj=0 ... τ (cid:107)yj − uj(cid:107) ≥ ζ
2:
3: else return yτ − uτ

return yj  j = min{j(cid:48) : (cid:107)yj(cid:48)−uj(cid:48)(cid:107) ≥ ζ

√

6ηF

Hessian ∇2f (x). To provide a formal analysis  we will ﬁrst analyze the following recurrence:

uτ = uτ−1 − η(∇f (x + uτ−1) − ∇f (x))  τ = 1  . . .

(6)
starting with a random noise vector u0  which is drawn from the sphere of an Euclidean ball with a
proper radius r denoted by Sd
r. It is notable that the recurrence in (6) is slightly different from that
in (5). We emphasize that this simple change is useful for extracting the NC at any points whose
Hessian has a negative eigen-value not just at non-degenerate saddle points  which can be used in
some stochastic or deterministic algorithms [1  4  21  16]. The proposed procedure NEON based on
the above sequence for ﬁnding a NC direction of ∇2f (x) is presented in Algorithm 1  where ˆfx(u)
is deﬁned in (7). The following theorem states our result of NEON for extracting the NC.
Theorem 1. Under Assumption 1 (i)  let γ ∈ (0  1) and δ ∈ (0  1) be a sufﬁciently small. For any
constant ˆc ≥ 18  there exists a constant cmax that depends on ˆc  such that if NEON is called with
−2(dL1/(γδ)) 
t = ˆc log(dL1/(γδ))
√
ηL1F/L2)1/3 and a constant η ≤ cmax/L1  then at a point x satisfying λmin(∇2f (x)) ≤
U = 4ˆc(
−γ with high probability 1 − δ it returns u such that u(cid:62)∇2f (x)u
NEON returns u (cid:54)= 0  then the above inequality must hold; if NEON returns 0  we can conclude that
λmin(∇2f (x)) ≥ −γ with high probability 1 − O(δ).
Remark: The above theorem shows that at any point x whose Hessian has a negative eigen-value
(including non-degenerate saddle points)  NEON can ﬁnd a NC of ∇2f (x) with high probability.

8ˆc2 log(dL1/(γδ)) ≤ −(cid:101)Ω(γ). If

−3(dL1/(γδ))  r =

  F = ηγ3L1L−2

−1/2
1

≤ −

L−1

2

ηγ2L

log

2

(cid:107)u(cid:107)2

√

log

ηγ

γ

4.2 Finding NC by Accelerated Gradient Method
Although NEON provides a similar guarantee for extracting a NC as that provided by the Power
√
method  but its iteration complexity O(1/γ) is worse than that of the Lanczos method  i.e.  O(1/
γ).
In this subsection  we present a ﬁrst-order method that matches O(1/
Let us recall the sequence (6)  which is essentially an application of gradient descent (GD) method to
the following objective function:

√
γ) of the Lanczos method.

(7)
In the sequel  we write ˆfx(u) = ˆf (u)  where the dependent x should be clear from the context. By
the Lipschitz continuous Hessian condition  we have that

ˆfx(u) = f (x + u) − f (x) − ∇f (x)(cid:62)u.

1
2

u(cid:62)∇2f (x)u − L2
6

(cid:107)u(cid:107)3 ≤ ˆf (u).

It implies that if ˆf (u) is sufﬁciently less than zero and (cid:107)u(cid:107) is not too large  then u(cid:62)∇2f (x)u
will be
sufﬁciently less than zero. Hence  NEON can be explained as using GD updates to decrease ˆf (u).

(cid:107)u(cid:107)2

5

A natural question to ask is whether the convergence of GD updates of NEON can be accelerated by
accelerated gradient (AG) methods. It is well-known from convex optimization literature that AG
methods can accelerate the convergence of GD method for smooth problems. Recently  several studies
have explored AG methods for non-convex optimization [15  19  3  12]. Notably  [19] analyzed the
behavior of AG methods near strict saddle points and investigated the rate of divergence from a strict
saddle point for toy quadratic problems. [12] analyzed a single-loop algorithm based on Nesterov’s
AG method for deterministic non-convex optimization. However  none of these studies provide an
explicit complexity guarantee on extracting NC from the Hessian matrix for a general non-convex
problem. Inspired by these studies  we will show that Nesterov’s AG (NAG) method [18] when

applied the function ˆf (u) can ﬁnd a NC with a complexity of (cid:101)O(1/

γ).

√

The updates of NAG method applied to the function ˆf (u) at a given point x is given by

yτ +1 = uτ − η∇ ˆf (uτ ) 
uτ +1 = yτ +1 + ζ(yτ +1 − yτ ) 

(8)
where ζ(yτ +1 − yτ ) is the momentum term  and ζ ∈ (0  1) is the momentum parameter. The
proposed algorithm based on the NAG method (referred to as NEON+) for extracting NC of a
Hessian matrix ∇2f (x) is presented in Algorithm 2  where

∆x(yτ   uτ ) = ˆfx(yτ ) − ˆfx(uτ ) − ∇ ˆfx(uτ )(cid:62)(yτ − uτ ) 

and NCFind is a procedure that returns a NC by searching over the history y0:τ   u0:τ shown in
Algorithm 3. The condition check in Step 4 is to detect easy cases such that NCFind can easily ﬁnd
a NC in historical solutions without continuing the update  which is designed following a similar
procedure called Negative Curvature Exploitation (NCE) proposed in [12]. However  the difference
is that NCFind is tailored to ﬁnding a negative curvature satisfying (3)  while NCE in [12] is for
ensuring a decrease on a modiﬁed objective. The theoretical result of NEON+ is presented below.
Theorem 2. Under Assumption 1 (i)  let γ ∈ (0  1) and δ ∈ (0  1) be a sufﬁciently small. For any
constant ˆc ≥ 43  there exists a constant cmax that depends on ˆc  such that if NEON+is called with
−2(dL1/(γδ)) 
t =
√
ηL1F/L2)1/3  a small constant η ≤ cmax/L1  and a momentum parameter ζ = 1−√
ηγ 
U = 12ˆc(
72ˆc2 log(dL1/(γδ)) ≤ −(cid:101)Ω(γ). If NEON+returns u (cid:54)= 0  then the above inequality
then at any point x satisfying λmin(∇2f (x)) ≤ −γ with high probability 1 − δ it returns u such that
u(cid:62)∇2f (x)u
≤ −
must hold; if NEON+returns 0  we can conclude that λmin(∇2f (x)) ≥ −γ with high probability
1 − O(δ).

(cid:113) ˆc log(dL1/(γδ))

−3(dL1/(γδ))  r =

  F = ηγ3L1L−2

−1/2
1

L−1

ηγ2L

(cid:107)u(cid:107)2

√

log

ηγ

log

2

γ

2

4.3 Stochastic Approach for Extracting NC

In this subsection  we present a stochastic approach for extracting NC for F (x) in (1). For simplicity 
we refer to both NEON and NEON+ as NEON. The challenge in employing NEON for ﬁnding a
NC for the original function F (x) in (1) is that we cannot evaluate the gradient of F (x) exactly. To
(cid:80)
address this issue  we resort to the mini-batching technique.
Let S = {ξ1  . . .   ξm} denote a set of random samples and deﬁne a sub-sampled function FS (x) =
ξ∈S f (x; ξ). Then we apply NEON to FS (x) for ﬁnding an approximate NC uS of ∇2FS (x).
1|S|
Below  we show that as long as m is sufﬁciently large  uS is also an approximate NC of ∇2F (x).
Theorem 3. Under Assumption 1 (i)  for a sufﬁciently small δ ∈ (0  1) and ˆc ≥ 43  let m ≥
is a proper small constant. If λmin(∇2F (x)) ≤ −γ 
there exists c > 0 such that with probability 1 − δ  NEON(FS   x  t F  r) returns a vector uS such
that u(cid:62)
−1(3dL1/(2γδ)). If NEON(FS   x  t F  r) returns
0  then with high probability 1 − O(δ) we have λmin(∇2F (x)) ≥ −2γ. In either case  NEON

terminates with an IFO complexity of (cid:101)O(1/γ3) or (cid:101)O(1/γ2.5) corresponding to Algorithm 1 and

≤ −cγ  where c = (12ˆc)−2 log

  where s = log−1(3dL1/(2γδ))

S ∇2F (x)uS

1 log(6d/δ)
s2γ2

(cid:107)uS(cid:107)2

(12ˆc)2

16L2

Algorithm 2  respectively.

6

Compute (yj  zj) = A(xj)
if ﬁrst-order condition of yj not met then

Algorithm 4 NEON-A
1: Input: x1  other parameters of algorithm A
2: for j = 1  2  . . .   do
3:
4:
5:
6:
7:
8:
9:
end if
10:
11: end for

let xj+1 = zj
uj = NEON(FS2  yj  t F  r)
if uj = 0 return yj
else let xj+1 = yj − cγ ¯ξ

uj(cid:107)uj(cid:107)

else

L2

b ≤ |S1|

Algorithm 6 SCSG-epoch: (x S1  b)
1: Input: x  an independent set of samples S1 and
2: Set m1 = |S1|  η = c(cid:48)(m1/b)−2/3  c(cid:48) ≤ 1/6
3: Compute ∇FS (xj−1) and let x0 = x
4: Generate N ∼ Geom(m1/(m1 + b))
5: for k = 1  2  . . .   N do
6:
7:
8:
9: end for
10: return xN

Sample samples Sk of size b
vk = ∇FSk (xk−1)−∇FSk (x0) +∇FS (x0)
xk = xk−1 − ηvk

5 First-order Algorithms for Stochastic Non-Convex Optimization

In this section  we will ﬁrst describe a general framework for promoting existing ﬁrst-order stochastic
algorithms denoted by A to enjoy a second-order convergence  which is shown in Algorithm 4.
Here  we require A(xj) to return two points (yj  zj) that satisfy (9) and the mini-batch sample size
m = |S2| satisﬁes the condition in Lemma 3. The proposed NEON is used for escaping from a saddle
point. It should be noted that Algorithm 4 is abstract depending on how to implement Step 3  how to
check the ﬁrst-order condition  and how to set the step size parameter ¯ξ in Step 9.
For theoretical interest  we will analyze Algorithm 4 with a Rademacher random variable ¯ξ ∈ {1 −1}
and its three main components satisfying the following properties.
Property 1. (1) Step 7 - Step 9 guarantees that if λmin(∇2F (yj)) ≤ −γ  there exists C > 0 such
that E[F (xj+1) − F (yj)] ≤ −Cγ3. Let the total IFO complexity of Step 7 - Step 9 be Tn. (2) There
exists a ﬁrst-order stochastic algorithm (yj  zj) = A(xj) that satisﬁes:

if (cid:107)∇F (yj)(cid:107) ≥   then E[F (zj) − F (xj)] ≤ −ε(  α)
if (cid:107)∇F (yj)(cid:107) ≤   then E[F (yj) − F (xj)] ≤ Cγ3/2

(9)
where ε(  α) is a function of  and a parameter α > 0. Let the total IFO complexity of A(x) be
Ta. (3) the check of ﬁrst-order condition can be implemented by using a mini-batch of samples S 
i.e.  (cid:107)∇FS (yj)(cid:107) ≤   where S is independent of yj such that (cid:107)∇F (yj) − ∇FS (yj)(cid:107) ≤ /2. Let the
IFO complexity of checking the ﬁrst-order condition be Tc.

Property (1) can be guaranteed by Theorem 3 and Lemma 1. When using NEON  Tn = (cid:101)O(1/γ3)
and when using NEON+  Tn = (cid:101)O(1/γ2.5). For Property (2)  we will analyze several interesting
with Tc = (cid:101)O( 1
total IFO complexity of (cid:101)O(max(
(cid:107)∇F (yj)(cid:107) ≤ O() and λmin(∇2F (yj)) ≥ −2γ  where (cid:101)O(·) hides logarithmic factors of d and 1/δ 

algorithms. Property (3) can be guaranteed by Lemma 2 in the supplement under Assumption (1) (iii)
2 ). Based on the above properties  we have the following convergence of Algorithm 4.
Theorem 4. Assume Properties 1 hold. Then with high probability 1− δ  NEON-A terminates with a
γ3 )(Tn + Ta + Tc)). Upon termination  with high probability

and problem’s other constant parameters.
Next  we present corollaries of Theorem 4 for several instances of A  including stochastic gradient
descent (SGD) method  stochastic momentum (SM) methods  mini-batch SGD (MSGD)  and SCSG.
SGD and its momentum variants (including stochastic heavy-ball (SHB) method and stochastic
Nesterov’s accelerated gradient (SNAG) method) are popular stochastic algorithms for solving a
stochastic non-convex optimization problem. We will consider them in a uniﬁed framework as
established in [22]. The updates of SM starting from x0 are

1

ε( α)   1

(cid:98)xτ +1 = xτ − η∇f (xτ ; ξτ ) 
(cid:98)xs
xτ +1 =(cid:98)xτ +1 + β((cid:98)xs
τ +1 −(cid:98)xs
τ +1 = xτ − sη∇f (xτ ; ξτ ) 

τ ) 

7

(10)

Algorithm 5 SM: (x0  η  β  s  t)
1: for τ = 0  1  2  . . .   t do
2:
3:
4: end for
5: return (x+

Compute xτ +1 according to (10)
Compute x+
τ +1 according to (11)
t+1)  where τ(cid:48) ∈ {0  . . .   t}

τ(cid:48)  x+

is a randomly generated.

for τ = 0  . . .   t and(cid:98)xs

Figure 1: NEON vs
Second-order Meth-
ods for Extracting
NC

0  1  1/(1 − β) corresponds to SHB  SNAG and SGD. Let sequence x+

0 = x0  where β ∈ (0  1) is a momentum constant  η is a step size  s =
0 = x0 be deﬁned as
(11)

(xτ − xτ−1 − sη∇f (xτ−1; ξτ−1)).

τ with x+

τ = xτ + pτ   τ ≥ 1  pτ =
x+

We can implement A by Algorithm 5 and have the following result.
Corollary 5. Let A(xj) be implemented by Algorithm 5 with t = Θ(1/2) iterations  η = Θ(2)  β ∈
(0  1)  s ∈ (0  1/(1 − β)). Then Ta = O(1/2) and ε(  α) = Θ(2). Suppose that γ ≥ 2/3 and
E[(cid:107)∇f (x; ξ)(cid:107)2] is bounded for s (cid:54)= 1/(1 − β). Then with high probability  NEON-SM ﬁnds an

β
1 − β

2 ))  where Tn = (cid:101)O(1/γ3) (NEON)

(  γ)-SPP with a total IFO complexity of (cid:101)O(max( 1
or Tn = (cid:101)O(1/γ2.5) (NEON+).
Remark: When γ = 1/2  NEON-SM has an IFO complexity of (cid:101)O( 1
1 ∇FS1(xj)  yj = xj

MSGD computes (yj  zj) by

zj = xj − L−1

γ3 )(Tn + 1

2   1

4 ).

(12)

where S1 is a set of samples independent of xj.

Corollary 6. Let A(xj) be implemented by (12) with |S1| = (cid:101)O(1/2). Then Ta = (cid:101)O(1/2) and
of (cid:101)O(max( 1

. With high probability  NEON-MSGD ﬁnds an (  γ)-SPP with a total IFO complexity
γ3 )(Tn + 1/2)).

ε(  α) = 2
4L1
2   1

Remark: Compared to Corollary 5  there is no requirement on γ ≥ 2/3  which is due to that MSGD
can guarantee that E[F (yj) − F (xj)] ≤ 0.
SCSG was proposed in [14]  which only provides a ﬁrst-order convergence guarantee. SCSG runs
with multiple epochs  and each epoch uses similar updates as SVRG with three distinct features:
(i) it was applied to a sub-sampled function FS1; (ii) it allows for using a mini-batch samples of
size b independent of S1 to compute stochastic gradients; (ii) the number of updates of each epoch
is a random number following a geometric distribution dependent on b and |S1|. These features
make each SGCG epoch denoted by SCSG-epoch(x S1  b) have an expected IFO complexity of
Ta = O(|S1|). We present SCSG-epoch(x S1  b) in Algorithm 6. For using SCSG  yj and zj are
(13)

Corollary 7. Let A(xj) be implemented by (13) with |S1| = (cid:101)O(cid:0)max(1/2  1/(γ9/2b1/2))(cid:1). Then
ε(  α) = Ω(4/3/b1/3) and E[Ta] = (cid:101)O(cid:0)max(1/2  1/(γ9/2b1/2))(cid:1). With high probability  NEON-
SCSG ﬁnds an (  γ)-SSP with an expected total IFO complexity of (cid:101)O(max( b1/3
1/(γ9/2b1/2)))  where Tn = (cid:101)O(1/γ3) (NEON) or Tn = (cid:101)O(1/γ2.5) (NEON+).
  NEON-SCSG has an expected IFO complexity of (cid:101)O( 1
When γ ≥ 4/9  b = 1  NEON-SCSG has an expected IFO complexity of (cid:101)O(1/3.33).

yj = SCSG-epoch(xj S1  b) 

Remark: When γ = 1/2  b = 1/

γ3 )(Tn + 1/2 +

4/3   1

zj = yj

3.5 ).

√

Finally  we mention that the proposed NEON or NEON+ can be used in existing second-order
stochastic algorithms that require a NC direction as a substitute of second-order methods [1  20].
[1] developed Natasha2  which uses second-order online Oja’s algorithm for ﬁnding the NC. [20]
developed a stochastic algorithm for solving a ﬁnite-sum problem by using SVRG and a second-order
stochastic algorithm for computing the NC. We can replace the second-order methods for computing
a NC in these algorithms by the proposed NEON or NEON+  with the resulting algorithms referred
to as NEON-Natasha and NEON-SVRG. It is a simple exercise to derive the convergence results in
Table 1  which is left to interested readers.

8

#IFO (or #ISO)×1040123456vTHv-0.8-0.6-0.4-0.200.20.4PowerLanczosNEONNEON+NEONstNEON+stmin-eig-valFigure 2: NEON-SGD vs Noisy SGD. (All algorithms converge to local minimum)

6 Experiments

x2
i

+ λ
n

i=1

1+x2
i

regularizer for classiﬁcation  i.e.  F (x) =(cid:80)d

whose y-axis denotes the value of(cid:98)u(cid:62)H(cid:98)u  where(cid:98)u represents the found normalized NC vector and

(cid:80)n
Extracting NC. First  we present some simulations to verify the proposed NEON procedures for
extracting NC. To this end  we consider minimizing non-linear least square loss with a non-convex
i=1(bi − σ(x(cid:62)ai))2  where bi ∈ {0  1}
denotes the label and ai ∈ Rd denotes the feature vector of the i-th data  λ > 0 is a trade-off
parameter  and σ(·) is a sigmoid function. We generate a random vector x ∼ N (0  I) as the target
point to construct ˆFx(u) and compute a NC of ∇2F (x). We use a binary classiﬁcation data named
gisette from the libsvm data website that has n = 6000 examples and d = 5000 features  and set
λ = 3 in our simulation to ensure there is signiﬁcant NC from the non-linear least-square loss. The
step size η and initial radius in NEON procedures are set to be 0.01 and the momentum parameter in
NEON+ is set to be 0.9. These values are tuned in a certain range.
We compare the two NEON procedures and their stochastic variants (denoted by NEON-st and
NEON+-st in the ﬁgure) with second-order methods that use HVPs  namely the Power method
and the Lanczos method  where the HVPs are calculated exactly. The result is shown in Figure 1
H = ∇2F (x) is the Hessian matrix. For NEON-st and NEON+-st  we use a sample size of 100.
Please note that the solid red curve corresponding to NEON+-st terminates earlier due to that NCFind
is executed. Several observations follow: (i) NEON performs similarly to the Power method (the two
curves overlap in the ﬁgure); (ii) NEON+ has a faster convergence than NEON; (iv) the stochastic
versions of NEON and NEON+ can quickly ﬁnd a good NC directions than their full versions in
terms of IFO complexity and are even competitive with the Lanczos method. We include several
more results in the supplement.
Escaping Saddles. Second  we present some simulations to verify the proposed NEON and NEON+
based algorithms for minimizing a stochastic objective. We consider a non-convex optimization
i ) where ξi are a normal random variables with mean of
1 so that the saddle points of the expected function are known [10]. Assuming the noise ξ is only
accessed through a sampler  then we compare NEON-SGD with a state-of-the-art algorithm Noisy
SGD [5] for different values of d ∈ {103  104  105}. The step size of Noisy SGD is tuned in a wide
range and the best one is used. The step size in NEON procedures are set to be the same value as
Noisy SGD. The radius in NEON procedures is set to be 0.01 and the momentum paramenter in
NEON+ is set to be 0.9. The mini-batch size is tuned from {50  100  200  500}. All algorithms are
started with a same saddle point as the initial solution. The results are presented in Figure 2  showing
that two variants of NEON-SGD methods can escape saddles faster than Noisy SGD. NEON+-SGD
escapes saddle points the fastest among all algorithms for different values of d. In addition  the
increasing of dimensionality d has much larger effect on the IFO complexity of Noisy-SGD than that
of NEON-SGD methods  which is consistent with theoretical results.

problem with f (x; ξ) =(cid:80)d

i=1 ξi(x4

i − 4x2

7 Conclusions

We have proposed novel ﬁrst-order procedures to extract negative curvature from a Hessian matrix
by using a noise-initiated sequence  which are of independent interest. A general framework for
promoting a ﬁrst-order stochastic algorithm to enjoy a second-order convergence is also proposed.
Based on the proposed general framework  we designed several ﬁrst-order stochastic algorithms with
state-of-the-art second-order convergence guarantee.

9

#IFO×104012345objective-4000-3500-3000-2500-2000-1500-1000-5000d = 103NEON+-SGDNEON-SGDNoisy SGD#IFO×104012345objective×104-4-3.5-3-2.5-2-1.5-1-0.50d = 104NEON+-SGDNEON-SGDNoisy SGD#IFO×104012345objective×105-4-3.5-3-2.5-2-1.5-1-0.50d = 105NEON+-SGDNEON-SGDNoisy SGDAcknowledgement

The authors thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are
partially supported by National Science Foundation (IIS-1545995).

References
[1] Z. Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. CoRR  /abs/1708.08694 

2017.

[2] Z. Allen-Zhu and Y. Li. Neon2: Finding local minima via ﬁrst-order oracles. CoRR 

abs/1711.06673  2017.

[3] Y. Carmon  J. C. Duchi  O. Hinder  and A. Sidford. "convex until proven guilty": Dimension-free

acceleration of gradient descent on non-convex functions. In ICML  pages 654–663  2017.

[4] Y. Carmon  J. C. Duchi  O. Hinder  and A. Sidford. Accelerated methods for nonconvex

optimization. SIAM Journal on Optimization  28(2):1751–1772  2018.

[5] R. Ge  F. Huang  C. Jin  and Y. Yuan. Escaping from saddle points — online stochastic gradient

for tensor decomposition. In COLT  pages 797–842  2015.

[6] S. Ghadimi and G. Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[7] S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic

programming. Math. Program.  156(1-2):59–99  2016.

[8] S. Ghadimi  G. Lan  and H. Zhang. Mini-batch stochastic approximation methods for nonconvex

stochastic composite optimization. Math. Program.  155(1-2):267–305  2016.

[9] E. Hazan  K. Y. Levy  and S. Shalev-Shwartz. On graduated optimization for stochastic

non-convex problems. In ICML  pages 1833–1841  2016.

[10] P. Jain  P. Kar  et al. Non-convex optimization for machine learning. Foundations and Trends R(cid:13)

in Machine Learning  10(3-4):142–336  2017.

[11] C. Jin  R. Ge  P. Netrapalli  S. M. Kakade  and M. I. Jordan. How to escape saddle points

efﬁciently. In ICML  pages 1724–1732  2017.

[12] C. Jin  P. Netrapalli  and M. I. Jordan. Accelerated gradient descent escapes saddle points faster

than gradient descent. In COLT  pages 1042–1085  2018.

[13] J. Kuczynski and H. Wozniakowski. Estimating the largest eigenvalue by the power and
lanczos algorithms with a random start. SIAM Journal on Matrix Analysis and Applications 
13(4):1094–1122  1992.

[14] L. Lei  C. Ju  J. Chen  and M. I. Jordan. Non-convex ﬁnite-sum optimization via SCSG methods.

In NIPS  pages 2345–2355  2017.

[15] H. Li and Z. Lin. Accelerated proximal gradient methods for nonconvex programming. In NIPS 

pages 379–387  2015.

[16] M. Liu and T. Yang. On noisy negative curvature descent: Competing with gradient descent for

faster non-convex optimization. CoRR  abs/1709.08571  2017.

[17] M. Liu and T. Yang. Stochastic non-convex optimization with strong high probability second-

order convergence. CoRR  abs/1710.09447  2017.

[18] Y. Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.

Kluwer Academic Publ.  2004.

[19] M. O’Neill and S. J. Wright. Behavior of accelerated gradient methods near critical points of

nonconvex problems. CoRR  abs/1706.07993  2017.

10

[20] S. Reddi  M. Zaheer  S. Sra  B. Poczos  F. Bach  R. Salakhutdinov  and A. Smola. A generic

approach for escaping saddle points. In AISTATS  pages 1233–1242  2018.

[21] C. W. Royer and S. J. Wright. Complexity analysis of second-order line-search algorithms for

smooth nonconvex optimization. SIAM Journal on Optimization  28(2):1448–1477  2018.

[22] Y. Yan  T. Yang  Z. Li  Q. Lin  and Y. Yang. A uniﬁed analysis of stochastic momentum methods

for deep learning. In IJCAI  pages 2955–2961  2018.

[23] Y. Zhang  P. Liang  and M. Charikar. A hitting time analysis of stochastic gradient langevin

dynamics. In COLT  pages 1980–2022  2017.

11

,Hyeonwoo Noh
Tackgeun You
Jonghwan Mun
Bohyung Han
Yi Xu
Rong Jin
Tianbao Yang