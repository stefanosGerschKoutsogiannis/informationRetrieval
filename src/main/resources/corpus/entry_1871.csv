2008,Estimation of Information Theoretic Measures for Continuous Random Variables,We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy  mutual information or Kullback-Leibler divergence. The objective of this paper is two-fold. First  we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with fixed k converge almost surely  even though the k-nearest-neighbor density estimation with fixed k does not converge to its true measure. Second  we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless  these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion  respectively.,Estimation of Information Theoretic Measures for

Continuous Random Variables

Fernando P´erez-Cruz

Princeton University  Electrical Engineering Department
B-311 Engineering Quadrangle  08544 Princeton (NJ)

fp@princeton.edu

Abstract

We analyze the estimation of information theoretic measures of continuous ran-
dom variables such as: differential entropy  mutual information or Kullback-
Leibler divergence. The objective of this paper is two-fold. First  we prove that the
information theoretic measure estimates using the k-nearest-neighbor density es-
timation with ﬁxed k converge almost surely  even though the k-nearest-neighbor
density estimation with ﬁxed k does not converge to its true measure. Second 
we show that the information theoretic measure estimates do not converge for k
growing linearly with the number of samples. Nevertheless  these nonconvergent
estimates can be used for solving the two-sample problem and assessing if two
random variables are independent. We show that the two-sample and indepen-
dence tests based on these nonconvergent estimates compare favorably with the
maximum mean discrepancy test and the Hilbert Schmidt independence criterion.

1 Introduction

Kullback-Leibler divergence  mutual information and differential entropy are central to information
theory [5]. The divergence [17] measures the ‘distance’ between two density distributions while
mutual information measures the information one random variable contains about a related random
variable [23]. In machine learning  statistics and neuroscience the information theoretic measures
also play a leading role. For instance  the divergence is the error exponent in large deviation theory
[5] and the divergence can be directly applied to solving the two-sample problem [1]. Mutual infor-
mation is extensively used to assess whether two random variables are independent [2] and has been
proposed to solve the all-relevant feature selection problem [8  24]. Information-theoretic analysis of
neural data is unavoidable given the questions neurophysiologists are interested in1. There are other
relevant applications in different research areas in which divergence estimation is used to measure
the difference between two density functions  such as multimedia [19] and text [13] classiﬁcation 
among others.
The estimation of information theoretic quantities can be traced back to the late ﬁfties [7]  when Do-
brushin estimated the differential entropy for one-dimensional random variables. The review paper
by Beirlant et al. [4] analyzes the different contributions of nonparametric differential entropy esti-
mation for continuous random variables. The estimation of the divergence and mutual information
for continuous random variables has been addressed by many different authors [25  6  26  18  20  16] 
see also the references therein. Most of these approaches are based on estimating the densities ﬁrst.
For example  in [25]  the authors propose to estimate the densities based on data-dependent his-
tograms with a ﬁxed number of samples from q(x) in each bin. The authors of [6] compute relative
frequencies on data-driven partitions achieving local independence for estimating mutual informa-
tion. Also  in [20  21]  the authors compute the divergence using a variational approach  in which

1See [22] for a detailed discussion on mutual information estimation in neuroscience.

1

convergence is proven ensuring that the estimate for p(x)/q(x) or log p(x)/q(x) converges to the
true measure ratio or its log ratio.
There are only a handful of approaches that use k-nearest-neighbors (k-nn) density estimation [26 
18  16] for estimating the divergence and mutual information for ﬁnite k. Although ﬁnite k-nn
density estimation does not converge to the true measure  the authors are able to prove mean-square
consistency of their divergence estimators imposing some regularity constraint over the densities.
These proofs are based on the results reported in [15] for estimating the differential entropy with
k-nn density estimation.
The results in this paper are two-fold. First  we prove almost sure convergence of our divergence es-
timate based on k-nn density estimation with ﬁnite k. Our result is based on describing the statistics

of p(x)/(cid:98)p(x) as a waiting time distribution independent of p(x). We can readily apply this result to

the estimation of the differential entropy and mutual information.
Second  we show that for k linearly growing with the number of samples  our estimates do not con-
verge nor present known statistics. But they can be reliably used for solving the two-sample problem
or assessing if two random variables are independent. We show that for this choice of k  the esti-
mates of the divergence or mutual information perform  respectively  as well as the maximum mean
discrepancy (MMD) test in [9] and the Hilbert Schmidt independence criterion (HSIC) proposed in
[10].
The rest of the paper is organized as follows. We prove in Section 2 the almost sure convergence
of the divergence estimate based on k-nn density estimation with ﬁxed k. We extend this result
for differential entropy and mutual information in Section 3. In Section 4 we present some exam-
ples to illustrate the convergence of our estimates and to show how can they be used to assess the
independence of related random variables. Section 5 concludes the paper with some ﬁnal remarks.

2 Estimation of the Kullback-Leibler Divergence

If the densities P and Q exist with respect to a Lebesgue measure  the Kullback-Leibler divergence
is given by:

(cid:90)

D(P||Q) =

p(x) log p(x)

q(x) dx ≥ 0.

Rd

This divergence is ﬁnite whenever P is absolutely continuous with respect to Q and it is zero only
if P = Q.
The idea of using k-nn density estimation to estimate the divergence was put forward in [26  18] 
where they prove mean-square consistency of their estimator for ﬁnite k. In this paper  we prove
the almost sure convergence of this divergence estimator  using waiting-times distributions without
needing to impose additional conditions over the density models. Given a set with n i.i.d. samples
from p(x)  X = {xi}n
j}m
j=1  we estimate D(P||Q)
from a k-nn density estimate of p(x) and q(x) as follows:

i=1  and m i.i.d. samples from q(x)  X (cid:48) = {x(cid:48)

(cid:98)Dk(P||Q) =

n(cid:88)

i=1

log(cid:98)pk(xi)
(cid:98)qk(xi)

1
n

n(cid:88)

i=1

= d
n

log sk(xi)
kk(xi)

+ log m
n − 1

where

(1)

(2)

(3)

(cid:98)pk(xi) =
(cid:98)qk(xi) = k

k

(n − 1)

Γ(d/2 + 1)

1

πd/2

rk(xi)d

Γ(d/2 + 1)

1

m

πd/2

(4)
rk(xi) and sk(xi) are  respectively  the Euclidean distances to the k-nn of xi in X\xi and X (cid:48)  and
πd/2/Γ(d/2 + 1) is the volume of the unit-ball in Rd. Before proving (2) converges almost surely
to D(P||Q)  let us show an intermediate necessary result.
Lemma 1. Given n i.i.d. samples  X = {xi}n
x in the support of p(x).

bution P   the limiting distribution of p(x)/(cid:98)p1(x) is exponentially distributed with unit mean for any

i=1  from an absolutely continuous probability distri-

sk(xi)d

2

Proof. Let’s initially assume p(x) is a d-dimensional uniform distribution with a given support. The
set Sx R = {xi| (cid:107)xi − x(cid:107)2 ≤ R  xi ∈ X} contains all the samples from X inside the ball centered
in x of radius R. The radius R has to be small enough for the ball centered in x to be contained
within the support of p(x).
2| xi ∈ Sx R} are consequently uniformly distributed between 0 and Rd.
The samples in {(cid:107)xi − x(cid:107)d
Thereby  the limiting distribution of r1(x)d = minxj∈Sx R((cid:107)xj − x(cid:107)d
2) is exponentially distributed 
as it measures the waiting time between the origin and the ﬁrst event of a uniformly-spaced sample
ball centered in x  p(x)/(cid:98)p1(x) is distributed as a unit-mean exponential distribution as n tends to
(see Theorem 2.4 in [3]). Since p(x)nπd/2/Γ(d/2 + 1) is the mean number of samples per unit
inﬁnity.
For non-uniform absolutely-continuous P   P(r1(x) > ε) → 0 as n → ∞ for any x in the support
and the limiting distribution of p(x)/(cid:98)p1(x) is a unit-mean exponential distribution.
of p(x) and any ε > 0. Therefore  as n tends to inﬁnity p(arg minxj∈Sx R((cid:107)xj − x(cid:107)d
2)) → p(x)
bution P   the limiting distribution of p(x)/(cid:98)pk(x) is a unit-mean 1/k-variance gamma distribution

Corolary 1. Given n i.i.d. samples  X = {xi}n
for any x in the support of p(x).

i=1  from an absolutely continuous probability distri-

i=1  from an absolutely continuous probability

Proof. In the previous proof  instead of measuring the waiting time to the ﬁrst event  we compute the
waiting time to the kth event of a uniformly-spaced sample. This waiting-time limiting distribution
is a unit-mean and 1/k-variance Erlang (gamma) distribution [14].
Corolary 2. Given n i.i.d. samples  X = {xi}n
n → ∞.
Proof. The k-nn in X tends to x as k/n → 0 and n → ∞. Thereby the limiting distribution of

distribution P   then(cid:98)pk(x) P→ p(x) for any x in the support of p(x)  if k → ∞ and k/n → 0  as
p(x)/(cid:98)pk(x) is a unit-mean 1/k-variance gamma distribution. As k → ∞ the variance of the gamma
distribution goes to zero and consequently(cid:98)pk(x) converges to p(x).
k grows linearly with n  the k-nn sample in X does not converge to x  which precludes p(x)/(cid:98)pk(x)

The second corollary is the widely known result that k-nn density estimation converges to the true
measure if k → ∞ and k/n → 0. We have just include it in the paper for clarity and completeness. If

to present known statistics. For this growth on k  the divergence estimate does not converge to
D(P||Q).
Now we can prove the almost surely convergence to (1) of the estimate in (2) based on the k-nn
density estimation.
Theorem 1. Let P and Q be absolutely continuous probability measures and let P be absolutely
i}m
continuous with respect to Q. Let X = {xi}n
i=1 be i.i.d. samples  respectively 
from P and Q  then
D(P||Q)

i=1 and X (cid:48) = {x(cid:48)

a.s.−→

(5)

Proof. We can rearrange (cid:98)Dk(P||Q) in (2) as follows:
(cid:98)Dk(P||Q) =
log p(xi)
q(xi)

log(cid:98)pk(xi)
(cid:98)qk(xi)

n(cid:88)

1
n

1
n

i=1

i=1

(cid:98)Dk(P||Q)
n(cid:88)

=

n(cid:88)

i=1

− 1
n

(cid:98)pk(xi)
log p(xi)

+

1
n

n(cid:88)

i=1

(cid:98)qk(xi)
log q(xi)

(6)

The ﬁrst term is the empirical estimate of (1) and  by the law of large numbers [11]  it converges
almost surely to its mean  D(P||Q).

The limiting distributions of p(xi)/(cid:98)pk(xi) and q(xi)/(cid:98)qk(xi) are unit-mean 1/k-variance gamma

distributions  independent of i  p(x) and q(x) (see Corollary 1). In the large sample limit:

log(z)zk−1e−kzdz

(7)

n(cid:88)

i=1

1
n

(cid:98)pk(xi)
log p(xi)

by the law of large numbers [11].

(cid:90) ∞

0

a.s.−→

kk

(k − 1)!

3

Finally  the sum of almost surely convergent terms also converges almost surely [11]  which com-
pletes our proof.

The k-nn based divergence estimator is biased  because the convergence rate of p(xi)/(cid:98)pk(xi) and
q(xi)/(cid:98)qk(xi) to the unit-mean 1/k-variance gamma distribution depends on the density models and

we should not expect them to be identical. If p(x) = q(x)  the divergence is zero and our estimate
is unbiased for any k (even if k/n does not tend to zero)  since the statistics of the second and
third term in (6) are identical and they cancel each other out for any n (their expected mean is the
same). We use the Monte Carlo based test described in [9] with our divergence estimator to solve
the two-sample problem and decide if the samples from X and X (cid:48) actually came from the same
distribution.

3 Differential Entropy and Mutual Information Estimation

The results obtained for the divergence can be readily applied to estimate the differential entropy of
a random variable or the mutual information between two correlated random variables.
The differential entropy for an absolutely continuous random variable P is given by:

h(x) = −

p(x) log p(x)dx

(8)

(cid:90)

We can estimate the differential entropy given a set with n i.i.d. samples from P   X = {xi}n
using k-nn density estimation as follows:

i=1 

(cid:98)hk(x) = − 1

n

(cid:88)

i=1

log(cid:98)pk(xi)

(9)

where(cid:98)pk(xi) is given by (3).

Theorem 2. Let P be an absolutely continuous probability measure and let X = {xi}n
samples from P   then

i=1 be i.i.d.

(cid:98)hk(x)
γk = − kk

a.s.−→

(cid:90) ∞

h(x) + γk

where

and γ1

(k − 1)!

log(z)zk−1e−kzdz
∼=0.5772 and it is known as the Euler-Mascheroni constant [12].
n(cid:88)

Proof. We can rearrange(cid:98)hk(x) in (9) as follows:
log(cid:98)pk(xi) = − 1

(cid:98)hk(x) = − 1

log p(xi) +

n(cid:88)

n(cid:88)

0

n

i=1

n

i=1

1
n

i=1

(10)

(11)

(12)

(cid:98)pk(xi)
log p(xi)

The ﬁrst term is the empirical estimate of (9) and  by the law of large numbers [11]  it converges
almost surely to its mean  h(x).

The limiting distributions of p(xi)/(cid:98)pk(xi) is a unit-mean 1/k-variance gamma distribution  inde-

pendent of i and p(x) (see Corollary 1). In the large sample limit:

n(cid:88)

i=1

1
n

(cid:98)pk(xi)
log p(xi)

(cid:90) ∞

0

a.s.−→

kk

(k − 1)!

log(z)zk−1e−kzdz = −γk

(13)

by the law of large numbers [11].
Finally  the sum of almost surely convergent terms also converges almost surely [11]  which com-
pletes our proof.

Now  we can use the expansion of the conditional differential entropy  mutual information and con-
ditional mutual information to prove the convergence of their estimates based on k-nn density esti-
mation to their values.

4

• Conditional differential entropy:

• Mutual Information:

• Conditional Mutual Information:

I(x; y) = −

(cid:98)I(x;|y) =
(cid:90)
(cid:98)I(x; y|z) =

I(x; y|z) =

h(y|x) = −

p(x  y) log p(y  x)
n(cid:88)

(cid:90)
(cid:98)h(y|x) = − 1
(cid:90)
p(x  y) log p(y  x)
n(cid:88)

log p(yi  xi)
p(xi)

i=1

n

log p(yi  xi)
p(xi)p(yi)

1
n

i=1

p(x) dxdy
a.s.−→

p(x)p(y) dxdy
a.s.−→

h(y|x)

I(x; y) + γk

p(x  y  z) log p(y  x  z)p(z)
n(cid:88)

p(x  z)p(y  z) dxdydz
a.s.−→

log p(yi  xi  zi)p(zi)
p(xi  zi)p(yi  zi)

1
n

i=1

I(x; y|z)

(14)

(15)

(16)

(17)

(18)

(19)

4 Experiments

We have carried out two sets of experiments.
In the ﬁrst one  we show the convergence of the
divergence to their limiting value as the number of samples tends to inﬁnity and we compare the
divergence estimation to the MMD test in [9] for MNIST dataset. In the second experiment  we
compute if two random variables are independent and compare the obtained results to the HSIC
proposed in [10].
We ﬁrst compare the divergence between a uniform distribution between 0 and 1 in d-dimension and
a zero-mean Gaussian distribution with identity covariance matrix. We plot the divergence estimates
for d = 1 and d = 5 in Figure 1 as a function of n  for k = 1  k =

n and k = n/2 with m = n.

√

(a)

(b)

√

Figure 1: We plot the divergence for d = 1 in (a) and d = 5 in (b). The solid line with ’(cid:63)’ represents
the divergence estimate for k = 1  the solid line with ’∗’ represents the divergence estimate for
n  the solid line with ’◦’ represents the divergence estimate for k = n/2 and the dashed-
k =
dotted line represents the divergence. The dashed-lines represent ±3 standard deviation for each
divergence estimate. We have not added symbols to them to avoid cluttering the images further and
from the plots it should be clear which conﬁdence interval is assigned to what estimate.

As expected  the divergence estimate for k = n/2 does not converge to the true divergence as the

limiting distributions of p(x)/(cid:98)pk(x) and q(x)/(cid:98)qk(x) are unknown and they depend on p(x) and

5

1021031040.90.9511.051.11.15nKLDd=1 k=0.5nk=n0.5k=1KLD1021031044.74.84.955.15.25.35.45.5nKLDd=5 k=0.5nk=n0.5k=1KLD√

√

√

√
q(x)  respectively. Nevertheless  this estimate converges faster to its limiting value and its variance
is much smaller than that provided by the estimates of the divergence with k =
n or k = 1. This
may indicate that using k = n/2 might be a better option for solving the two-sample problem than
actually trying to estimate the true divergence  as theorized in [9].
Both divergence estimates for k = 1 and k =
n converge to the true divergence as the number
of samples tends to inﬁnity. The convergence of the divergence estimate for k = 1 is signiﬁcantly
faster than that with k =

n  because p(x)/(cid:98)p1(x) converges much faster to its limiting distribution
n(x). p(x)/(cid:98)p1(x) converges faster because the nearest neighbor to x is much closer
n-nearest-neighbor and we need that the k-nn to be close enough to x for p(x)/(cid:98)pk(x) to

than p(x)/(cid:98)p√

than the
be close to its limiting distribution. As d grows the divergence estimates need many more samples
to converge and even for small dimensions the number of samples can be enormously large.
Nevertheless  we can still use this divergence estimate to assess whether two sets of samples come
from the same distribution  because the divergence estimate for p(x) = q(x) is unbiased for any
k. In Figure 2(a) we plot the divergence estimate between the three’s and two’s handwritten digits
in the MNIST dataset (http://yann.lecun.com/exdb/mnist/) in a 784 dimensional space. In Figure

2(a) we plot the divergence estimator for (cid:98)D1(3  2) (solid line) and (cid:98)D1(3  3) (dashed line) mean

values for 100 experiments together with their 90% conﬁdence interval. For comparison purposes
we plot the MMD test from [9]  in which a kernel method was proposed for solving the two-sample
problem. We use the code available in http://www.kyb.mpg.de/bs/people/arthur/mmd.htm and use
its bootstrap estimate for our comparisons. For n = 5 the error rate for the test using k = 1 is 1%  for
n is 7% and for k = n/2 is 43% and for the MMD test is 34%. For n ≥ 10 all tests reported
k =
zero error rate. It seems than the k = 1 test is more powerful than the MMD test in this case  at
least for small n. But we can see that the conﬁdence interval for the MMD test decreases faster than
the test based on the divergence estimate with k = 1 and we should expect better performance for
larger n  similar to the divergence estimate with k = n/2.

√

Figure 2: In (a) we plot (cid:98)D1(3||2) (solid)  (cid:98)D1(3||3) (dashed) and their 90% conﬁdence interval

(b)

(a)

(dotted). In (b) we repeat the same plots using the MMD test from [9].

(cid:21)

(cid:20)y1

(cid:20) cos(θ)

(cid:21) (cid:20)x1

(cid:21)

sin(θ)
cos(θ)

In the second example we compute the mutual information between y1 and y2  which are given by:

=

y2

x2

− sin(θ)

(20)
where x1 and x2 are independent and uniformly distributed between 0 and 1  and θ ∈ [0  π/4]. If
θ is zero  y1 and y2 are independent. Otherwise they are not independent  but still uncorrelated for
any θ.
We carry out a test for describing if y1 and y2 are independent. The test is identical to the one
described in [10] and we use the Mote Carlo resampling technique proposed in that paper with a
95% conﬁdence interval and 1000 repetitions.
In Figure 3 we report the acceptance of the null
hypothesis (y1 and y2 are independent) as a function of θ for n = 100 in (a) and as a function of n
for θ = π/8 in (b). We compute the mutual information with k = 1  k =
n and k = n/2 for our
test  and compare it to the HSIC in [10].

√

6

101102−200−1000100200300400500nD(3||3) D(3||2)Divergence101102−0.2−0.15−0.1−0.0500.050.10.150.20.250.3nMMD(3||3) MMD(3||2)Maximum Mean Discrepancy(a)

(b)

Figure 3: We plot the acceptance of the null hypothesis (y1 and y2 are independent) for a 95%
conﬁdence interval in (a) as a function of θ and in (b) as a function on (n). The solid line uses the
mutual information estimate with k = n/2 and the dash-dotted line uses the HSIC. The dashed and
dotted lines  respectively  use the mutual information estimate with k =

n and k = 1.

√

The HSIC test and the mutual information estimate based test with k = n/2 perform equally well
√
at predicting whether y1 and y2 are independent  while the test based on the mutual information
estimates with k = 1 and k =
n clearly underperforms. This example shows that if our goal is
to predict whether two random variables are independent we are better off using HSIC or a noncon-
vergent estimate of the mutual information rather than trying to compute the mutual information as
accurately as possible. Furthermore  in our test  the computational complexity of computing HSIC
for n = 5000 is over 10 times more computationally costly (running time) than computing the
mutual information for k = n/22.
As we saw in the case of the divergence estimate in Figure 1  mutual information is more accurately
estimated when k = 1  but at the cost of a higher variance. If our objective is to estimate the mutual
information (or the divergence)  we should use a small value of k  ideally k = 1. However  if we are
interested in assessing whether two random variables are independent  it is better to use k = n/2 
because the variance of the estimate is much lower  even though it does not converge to the mutual
information (or the divergence).

5 Conclusions

We have proved that the estimates of the differential entropy  mutual information and divergence
based on k-nn density estimation for ﬁnite k converge almost surely  even though the density esti-
mate does not converge. The previous literature could only prove mean-squared consistency and it
required imposing some constraints over the density models. The proof in this paper relies on de-

scribing the limiting distribution of p(x)/(cid:98)pk(x). This limiting distribution can be easily described

n we can prove that(cid:98)pk(x) converges to p(x) while for ﬁnite k this convergences does not

using waiting-times distributions  such as the exponential or the Erlang distributions.
We have shown  experimentally  that ﬁxing k = 1 achieves the fastest convergence rate  at the
√
expense of a higher variance for our estimator. The divergence  mutual information and differential
√
entropy estimates using k = 1 are much better than the estimates using k =
n  even though for
k =
occur.
Finally  if we are interested in solving the two-sample problem or assessing if two random variables
are independent  it is best to ﬁx k to a fraction of n (we have used k = n/2 in our experiments) 
although in this case the estimates do not converge to the true value. Nevertheless  their variances
are signiﬁcantly lower  which allows our tests to perform better. The tests with k = n/2 perform as
well as the MMD test for solving the two sample problem and the HSIC for assessing independence.

2For computing HSIC test we use A. Gretton code in http://www.kyb.mpg.de/bs/people/arthur/indep.htm

and for ﬁnding the k-nn we use the sort function in Matlab.

7

00.10.20.30.40.50.60.700.10.20.30.40.50.60.70.80.91qAcept H0n=100 k=0.5nHSICk=n0.5k=110210300.10.20.30.40.50.60.70.80.91nAcept H0q=p/8 k=0.5nHSICk=n0.5k=1Acknowledgment

Fernando Prez-Cruz is supported by Marie Curie Fellowship 040883-AI-COM. This work was par-
tially funded by Spanish government (Ministerio de Educaci´on y Ciencia TEC2006-13514-C02-
01/TCM.

References
[1] N. Anderson  P. Hall  and D. Titterington. Two-sample test statistics for measuring discrepancies be-
tween two multivariate probability density functions using kernel-based density estimates. Journal of
Multivariate Analysis  50(1):41–54  7 1994.

[2] F. R. Bach and M. I. Jordan. Kernel independent component analysis. JMLR  3:1–48  2004.
[3] K. Balakrishnan and A. P. Basu. The Exponential Distribution: Theory  Methods and Applications. Gor-

don and Breach Publishers  Amsterdam  Netherlands  1996.

[4] J. Beirlant  E. Dudewicz  L. Gyorﬁ  and E. van der Meulen. Nonparametric entropy estimation: An

overview. nternational Journal of the Mathematical Statistics Sciences  pages 17–39  1997.

[5] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley  New York  USA  1991.
[6] G. A. Darbellay and I. Vajda. Estimation of the information by an adaptive partitioning of the observation

space. IEEE Trans. Information Theory  45(4):1315–1321  5 1999.

[7] R. L. Dobrushin. A simpliﬁed method for experimental estimate of the entropy of a stationary sequence.

Theory of Probability and its Applications  (4):428–430  1958.

[8] F. Fleuret. Fast binary feature selection with conditional mutual information. JMLR  5:1531–1555  2004.
[9] A. Gretton  K. M. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. Smola. A kernel method for the two-
In B. Sch¨olkopf  J. Platt  and T. Hofmann  editors  Advances in Neural Information

sample-problem.
Processing Systems 19  Cambridge  MA  2007. MIT Press.

[10] A. Gretton  K. Fukumizu  C. H. Teo  L. Song  B. Sch¨olkopf  and A. Smola. A kernel statistical test of
independence. In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information
Processing Systems 20  Cambridge  MA  2008. MIT Press.

[11] G.R. Grimmett and D.R. Stirzaker. Probability and Random Processes. Oxford University Press  Oxford 

UK  3 edition  2001.

[12] Julian Havil. Gamma: Exploring Euler’s Constant. Princeton University Press  New York  USA  2003.
[13] S. Mallela I. S. Dhillon and R. Kumar. A divisive information-theoretic feature clustering algorithm for

text classiﬁcation. JMLR  3:1265–1287  3 2003.

[14] Leonard Kleinrock. Queueing Systems. Volume 1: Theory. Wiley  New York  USA  1975.
[15] L. F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector. Problems

Inform. Transmission  23(2):95–101  4 1987.

[16] A. Kraskov  H. St¨ogbauer  and P. Grassberger. Estimating mutual information. Physical Review E 

69(6):1–16  6 2004.

[17] S. Kullback and R. A. Leibler. On information and sufﬁciency. Ann. Math. Stats.  22(1):79–86  3 1951.
[18] N. N. Leonenko  L. Pronzato  and V. Savani. A class of renyi information estimators for multidimensional

densities. Annals of Statistics  2007. Submitted.

[19] P. J. Moreno  P. P. Ho  and N. Vasconcelos. A kullback-leibler divergence based kernel for svm classiﬁ-

cation in multimedia applications. Technical Report HPL-2004-4  HP Laboratories  2004.

[20] X. Nguyen  M. J. Wainwright  and M. I. Jordan. Nonparametric estimation of the likelihood ratio and

divergence functionals. In IEEE Int. Symp. Information Theory  Nice  France  6 2007.

[21] X. Nguyen  M. J. Wainwright  and M. I. Jordan. Estimating divergence functionals and the likelihood
ratio by penalized convex risk minimization. In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors 
Advances in Neural Information Processing Systems 20  Cambridge  MA  2008. MIT Press.

[22] L. Paninski. Estimation of entropy and mutual information. Neural Compt  15(6):1191–1253  6 2003.
[23] C. E. Shannon. A mathematical theory of communication. Bell System Tech. J.  pages 379–423  1948.
[24] K. Torkkola. Feature extraction by non parametric mutual information maximization. JMLR  3:1415–

1438  2003.

[25] Q. Wang  S. Kulkarni  and S. Verd´u. Divergence estimation of continuous distributions based on data-

dependent partitions. IEEE Trans. Information Theory  51(9):3064–3074  9 2005.

[26] Q. Wang  S. Kulkarni  and S. Verd´u. A nearest-neighbor approach to estimating divergence between

continuous random vectors. In IEEE Int. Symp. Information Theory  Seattle  USA  7 2006.

8

,Haipeng Luo
Robert Schapire