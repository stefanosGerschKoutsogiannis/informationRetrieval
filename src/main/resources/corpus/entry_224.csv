2009,A Stochastic approximation method for inference in probabilistic graphical models,We describe a new algorithmic framework for inference in probabilistic models  and apply it to inference for latent Dirichlet allocation. Our framework adopts the methodology of variational inference  but unlike existing variational methods such as mean field and expectation propagation it is not restricted to tractable classes of approximating distributions. Our approach can also be viewed as a sequential Monte Carlo (SMC) method  but unlike existing SMC methods there is no need to design the artificial sequence of distributions. Notably  our framework offers a principled means to exchange the variance of an importance sampling estimate for the bias incurred through variational approximation. Experiments on a challenging inference problem in population genetics demonstrate improvements in stability and accuracy over existing methods  and at a comparable cost.,A Stochastic approximation method for inference

in probabilistic graphical models

Peter Carbonetto

Dept. of Human Genetics

University of Chicago
Chicago  IL  U.S.A.

Matthew King
Dept. of Botany

Firas Hamze

D-Wave Systems

University of British Columbia

Vancouver  B.C.  Canada

Burnaby  B.C.  Canada
fhamze@dwavesys.com

pcarbone@bsd.uchicago.edu

kingdom@interchange.ubc.ca

Abstract

We describe a new algorithmic framework for inference in probabilistic models 
and apply it to inference for latent Dirichlet allocation (LDA). Our framework
adopts the methodology of variational inference  but unlike existing variational
methods such as mean ﬁeld and expectation propagation it is not restricted to
tractable classes of approximating distributions. Our approach can also be viewed
as a “population-based” sequential Monte Carlo (SMC) method  but unlike ex-
isting SMC methods there is no need to design the artiﬁcial sequence of dis-
tributions. Signiﬁcantly  our framework oﬀers a principled means to exchange
the variance of an importance sampling estimate for the bias incurred through
variational approximation. We conduct experiments on a diﬃcult inference prob-
lem in population genetics  a problem that is related to inference for LDA. The
results of these experiments suggest that our method can oﬀer improvements in
stability and accuracy over existing methods  and at a comparable cost.

1 Introduction

Over the past several decades  researchers in many diﬀerent ﬁelds—statistics  economics  physics 
genetics and machine learning—have focused on coming up with more accurate and more eﬃcient
approximate solutions to intractable probabilistic inference problems. To date  there are three
widely-explored approaches to approximate inference in probabilistic models: obtaining a Monte
Carlo estimate by simulating a Markov chain (MCMC); obtaining a Monte Carlo estimate by
drawing samples from a distribution other than the target then reweighting the samples to account
for any discrepancies (importance sampling); and variational inference  in which the original
integration problem is transformed into an optimization problem.

The variational approach in particular has attracted wide interest in the machine learning commu-
nity  and this interest has lead to a number of important innovations in approximate inference—
some of these more recent developments are described in the dissertations of Beal [3]  Minka [22] 
Ravikumar [27] and Wainwright [31]. The key idea behind variational inference is to come up
with a family of approximating distributions ˆp(x; θ) that have “nice” analytic properties  then to
optimize some criterion in order to ﬁnd the distribution parameterized by θ that most closely
matches the target posterior p(x). All variational inference algorithms  including belief propaga-
tion and its generalizations [32]  expectation propagation [22] and mean ﬁeld [19]  can be derived
from a common objective  the Kullback-Leibler (K-L) divergence [9]. The major drawback of
variational methods is that the best approximating distribution may still impose an unrealistic or
questionable factorization  leading to excessively biased estimates (see Fig. 1  left-hand side).

In this paper  we describe a new variational method that does not have this limitation: it adopts the
methodology of variational inference without being restricted to tractable classes of approximate

1

distributions (see Fig. 1  right-hand side). The catch is that the variational objective (the K-L
divergence) is diﬃcult to optimize because its gradient cannot be computed exactly. So to descend
along the surface of the variational objective  we propose to employ stochastic approximation [28]
with Monte Carlo estimates of the gradient  and update these estimates over time with sequential
Monte Carlo (SMC) [12]—hence  a stochastic approximation method for probabilistic inference.
Large gradient descent steps may quickly lead to a degenerate sample  so we introduce a mechanism
that safeguards the variance of the Monte Carlo estimate at each iteration (Sec. 3.5). This variance
safeguard mechanism does not make the standard eﬀective sample size (ESS) approximation [14] 
hence it is likely to more accurately monitor the variance of the sample.

Indirectly  the variance safeguard provides a way to obtain an
estimator that has low variance in exchange for (hopefully small)
bias. To our knowledge  our algorithm is the ﬁrst general means
of achieving such a trade-oﬀ and  in so doing  it draws meaning-
ful connections between Monte Carlo and variational methods.

The advantage of our stochastic approximation method with re-
spect to other variational methods is rather straightforward: it
does not restrict the class of variational densities by making as-
sumptions about their structure. However  whe advantage of our
approach compared to Monte Carlo methods such as annealed
importance sampling (AIS) [24] is less obvious. One key ad-
vantage is that there is no need to design the sequence of SMC
distributions as it is a direct product of the algorithm’s deriva-
tion (Sec. 3). It is our conjecture that this automatic selection 
when combined with the variance safeguard  is more eﬃcient
than setting the sequence by hand  say  via tempered transitions
[12  18  24]. The population genetics experiments we conduct
in Sec. 4 provide some support for this claim.

We illustrate our approach on the problem of inferring pop-
ulation structure from a cohort of genotyped sequences using
the mixture model of Pritchard et al. [26]. We show in Sec. 4
that Markov chain Monte Carlo (MCMC) is prone to producing
very diﬀerent answers in independent simulations  and that it
fails to adequately capture the uncertainty in its solutions. For
many population genetics applications  such as wildlife conser-
vation [8]  it is crucial to accurately characterize the conﬁdence
in a solution. Since variational methods employing mean ﬁeld
approximations [4  30] tend to be overconﬁdent  they are poorly
suited for this problem. (This has generally not been an issue
for semantic text analysis [4  15].) As we show  SMC with a
uniform sequence of tempered distributions fares little better than MCMC. The implementation of
our approach on the population structure model demonstrates improvements in both accuracy and
reliability over MCMC and SMC alternatives  and at a comparable computational cost.

Figure 1: The guiding princi-
ple behind standard variational
methods (top) is to ﬁnd the ap-
proximating density ˆp(x; θ) that
is closest to the distribution of
interest p(x)  yet remains within
the deﬁned set of tractable prob-
ability distributions. In our ap-
proach (bottom)  the class of ap-
proximating densities always co-
incides with the target p(x).

The latent Dirichlet allocation (LDA) model [4] is very similar to the population structure model
of [26]  under the assumption of ﬁxed Dirichlet priors. Since LDA is already familiar to the
machine learning audience  it serves as a running example throughout our presentation.

1.1 Related work

The interface of optimization and simulation strategies for inference has been explored in a number
of papers  but none of the existing literature resembles the approach proposed in this paper. De
Freitas et al. [11] use a variational approximation to formulate a Metropolis-Hastings proposal. Re-
cent work on adaptive MCMC [1] combines ideas from both stochastic approximation and MCMC
to automatically learn better proposal distributions. Our work is also unrelated to the paper [20]
with a similar title  where stochastic approximation is applied to improving the Wang-Landau
algorithm. Younes [33] employs stochastic approximation to compute the maximum likelihood
estimate of an undirected graphical model. Also  the cross-entropy method [10] uses importance
sampling and optimization for inference  but exhibits no similarity to our work beyond that.

2

2 Latent Dirichlet allocation

Latent Dirichlet allocation (LDA) is a generative model of a collection of text documents  or corpus.
Its two key features are: the order of the words is unimportant  and each document is drawn from
a mixture of topics. Each document d = 1  . . .   D is expressed as a “bag” of words  and each
word wdi = j refers to a vocabulary item j ∈ {1  . . .   W }. (Here we assume each document has
the same length N .) Also  each word has a latent topic indicator zdi ∈ {1  . . .   K}. Observing
the jth vocabulary item in the kth topic occurs with probability βkj. The word proportions for
each topic are generated according to a Dirichlet distribution with ﬁxed prior η. The latent topic
indicators are generated independently according to p(zdi = k | τd) ≡ τdk  and τd in turn follows a
Dirichlet with prior ν. The generative process we just described deﬁnes a joint distribution over
the observed data w and unknowns x = {β  τ  z} given the hyperparameters {η  ν}:

p(w  x | η  ν) =

K

Y

k=1

p(βk | η) ×

D

Y

d=1

p(τd | ν) ×

D

N

Y

Y

d=1

i=1

p(wdi | zdi  β) p(zdi | τd) 

(1)

The directed graphical model is given in Fig. 2.

Implementations of approximate inference in LDA include
MCMC [15  26] and variational inference with a mean ﬁeld
approximation [4  30]. The advantages of our inference ap-
proach become clear when it is measured up against the
variational mean ﬁeld algorithm of [4]: ﬁrst  we make no
additional assumptions regarding the model’s factorization;
second  the number of variational parameters is independent
of the size of the corpus  so there is no need to resort to
coordinate-wise updates that are typically slow to converge.

3 Description of algorithm

Figure 2: Directed graphical model
for LDA. Shaded nodes represent
observations or ﬁxed quantities.

The goal is to calculate the expectation of function ϕ(x) with respect to target distribution p(x):

Ep( · )[ϕ(X)] = R ϕ(x) p(x) dx.

(2)

In LDA  the target density p(x) is the posterior of x = {β  τ  z} given w derived via Bayes’ rule.

From the importance sampling identity [2]  we can obtain an unbiased estimate of (2) by drawing
n samples from a proposal q(x) and evaluating importance weights w(x) = p(x)/q(x). (Usually
p(x) can only be evaluated up to a normalizing constant  in which case the asymptotically unbiased
normalized importance sampling estimator [2] is used instead.) The Monte Carlo estimator is

Ep( · )[ϕ(X)] ≈ 1

n Pn

s=1w(x(s)) ϕ(x(s)).

(3)

Unless great care is taken is in designing the proposal q(x)  the Monte Carlo estimator will exhibit
astronomically high variance for all but the smallest problems.

Instead  we construct a Monte Carlo es-
timate (3) by replacing p(x) with an al-
ternate target ˆp(x; θ) that resembles it  so
that all importance weights are evaluated
with respect to this alternate target. (We
elaborate on the exact form of ˆp(x; θ) in
Sec. 3.1.) This new estimator is biased 
but we minimize the bias by solving a vari-
ational optimization problem.

• Draw samples from initial density ˆp(x; θ1).
• for k = 2  3  4  . . .

- Stochastic approximation step: take gradi-
ent descent step θk = θk−1 −αkgk  where gk
is a Monte Carlo estimate of the gradient of
the K-L divergence  and αk is the variance-
safeguarded step size.

- SMC step: update samples and importance

weights to reﬂect new density ˆp(x; θk).

Our algorithm has a dual interpretation: it
can be interpreted as a stochastic approxi-
mation algorithm for solving a variational
optimization problem  in which the iterates are the parameter vectors θk  and it can be equally
viewed as a sequential Monte Carlo (SMC) method [12]  in which each distribution ˆp(x; θk) in the

Figure 3: Algorithm sketch.

3

sequence is chosen dynamically based on samples from the previous iteration. The basic idea is
spelled out in Fig. 3. At each iteration  the algorithm selects a new target ˆp(x; θk) by optimizing
the variational objective. Next  the samples are revised in order to compute the stochastic gradient
gk+1 at the next iteration. Since SMC is eﬀectively a framework for conducting importance sam-
pling over a sequence of distributions  we describe a “variance safeguard” mechanism (Sec. 3.5)
that directly regulates increases in variance at each step by preventing the iterates θk from moving
too quickly. It is in this manner that we achieve a trade-oﬀ between bias and variance.

Since this is a stochastic approximation method  asymptotic convergence of θk to a minimizer of
the objective is guaranteed under basic theory of stochastic approximation [29]. As we elaborate
below  this implies that ˆp(x; θk) will converge almost surely to the target distribution p(x) as k
approaches inﬁnity. And asymptotic variance results from the SMC literature [12] tell us that the
Monte Carlo estimates will converge almost surely to the target expectation (2) so long as ˆp(x; θk)
approaches p(x). A crucial condition is that the stochastic estimates of the gradient be unbiased.
There is no way to guarantee unbiased estimates under a ﬁnite number of samples  so convergence
holds only as the number of iterations and number of samples both approach inﬁnity.

To recap  the probabilistic inference recipe we propose has ﬁve main ingredients: one  a family
of approximating distributions that admits the target (Sec. 3.1); two  a variational optimization
problem framed using the K-L divergence measure (Sec. 3.2); three  a stochastic approximation
method for ﬁnding a solution to the variational optimization problem (Sec. 3.3); four  the imple-
mentation of a sequential Monte Carlo method for constructing stochastic estimates of the gradient
of the variational objective (Sec 3.4); and ﬁve  a way to safeguard the variance of the importance
weights at each iteration of the stochastic approximation algorithm (Sec. 3.5).

3.1 The family of approximating distributions

The ﬁrst implementation step is the design of a family of approximating distributions ˆp(x; θ)
parameterized by vector θ. In order to devise a useful variational inference procedure  the usual
strategy is to restrict the class of approximating distributions to those that factorize in an analytically
convenient fashion [4  19] or  in the dual formulation  to introduce an approximate (but tractable)
decomposition of the entropy [32]. Here  we impose no such restrictions on tractability; refer
to Fig. 1. We allow any family of approximating distributions so long as it satisﬁes these three
conditions: 1.) there is at least one θ = θ1 such that samples can be drawn from ˆp(x; θ1); 2.) there
is a θ = θ⋆ that recovers the target ˆp(x; θ⋆) = p(x)  hence an unbiased estimate of (2); and 3.) the
densities are members of the exponential family [13] expressed in standard form

ˆp(x; θ) = exp{ha(x)  θi − c(θ)} 

(4)
in which h·  ·i is an inner product  the vector-valued function a(x) is the statistic of x  and θ is the
natural or canonical parameterization. The log-normalization factor c(θ) ≡ log R expha(x)  θi dx
ensures that ˆp(x; θ) represents a proper probability. We further assume that the random vector
x can be partitioned into two sets A and B such that it is always possible to draw samples
from the conditionals ˆp(xA | xB; θ) and ˆp(xB | xA; θ). Hidden Markov models  mixture models 
continuous-time Markov processes  and some Markov random ﬁelds are all models that satisfy
this condition. This extra condition could be removed without great diﬃculty  but doing so would
add several complications to the description of the algorithm. The restriction to the exponential
family is not a strong one as most conventionally-studied densities can be written in the form (4).

ˆp(x; θ) = exp (cid:8)PD

For LDA  we chose a family of approximating densities of the form
k=1 PW
j=1(cj − mkj) log βkj − c(θ)(cid:9) 

k=1(νk + ndk − 1) log τdk + PK
k=1 PW

j=1(ˆηkj − 1) log βkj

d=1 PK

+ φPK

k=1 PW

j=1mkj log βkj + γPK

(5)
where mkj ≡ Pd Pi δk(zdi) δj(wdi) counts the number of times the jth word is assigned to the
kth topic  ndk ≡ Pi δk(zdi) counts the number of words assigned to the kth topic in the dth
document  and cj ≡ Pd Pi δj(wdi) is is the number of times jth vocabulary item is observed.
The natural parameters are θ = {ˆη  φ  γ}  with θ ≥ 0. The target posterior ˆp(x; θ⋆) ∝ p(w  x | η  ν)
is recovered by setting φ = 1  γ = 0 and ˆη = η. A sampling density with a tractable expression
for c(θ) is recovered whenever we set φ equal to γ. The graphical structure of LDA (Fig. 2) allows
us to draw samples from the conditionals ˆp(β  τ | z; θ) and ˆp(z | β  τ ; θ). Loosely speaking  this
choice is meant to strike a balance between the mean ﬁeld approximation [4] (with parameters
ˆηkj) and the tempered distribution (with “local” temperature parameters φ and γ).

4

3.2 The variational objective

The Kullback Leibler (K-L) divergence [9] asymmetrically measures the distance between the
target distribution p(x) = ˆp(x; θ⋆) and approximating distribution ˆp(x; θ) 
F (θ) = hE ˆp( · ; θ)[a(X)]  θ − θ⋆i + c(θ⋆) − c(θ) 

(6)
the optimal choice being θ = θ⋆. This is our variational objective. The fact that we cannot compute
c(θ) poses no obstacle to optimizing the objective (6); through application of basic properties of
the exponential family  the gradient vector works out to be the matrix-vector product

(7)
where Var[a(X)] is the covariance matrix of the statistic a(x). The real obstacle is the presence of
an integral in (7) that is most likely intractable. With a collection of samples x(s) with importance
weights w(s)  for s = 1  . . .   n  that approximate ˆp(x; θ)  we have the Monte Carlo estimate

∇F (θ) = Var ˆp( · ; θ)[a(X)](θ − θ⋆) 

∇F (θ) ≈ Pn

s=1 w(s)(a(x(s)) − ¯a)(a(x(s)) − ¯a)T (θ − θ⋆) 

(8)
where ¯a ≡ Ps w(s)a(x(s)) denotes the Monte Carlo estimate of the mean statistic. Note that
these samples {x(s)  w(s)} serve to estimate both the expectation (2) and the gradient (7). The
algorithm’s performance hinges on a good search direction  so it is worth our while to reduce the
variance of the gradient measurements when possible via Rao-Blackwellization [6]. Since we no
longer have an exact value for the gradient  we appeal to the theory of stochastic approximation.

3.3 Stochastic approximation

Instead of insisting on making progress toward a minimizer of f (θ) at every iteration  as in
gradient descent  stochastic approximation only requires that progress be achieved on average.
The Robbins-Monro algorithm [28] iteratively adjusts the control variable θ according to

θk+1 = θk − αkgk 

(9)
where gk is a noisy observation of f (θk)  and {αk} is a sequence of step sizes. Provided the
sequence of step sizes satisﬁes certain conditions  this algorithm is guaranteed to converge to the
solution f (θ⋆) = 0; see [29]. In our case  f (θ) = ∇F (θ) = 0 is the ﬁrst-order condition for an
unconstrained minimum. Due to poor conditioning  we advocate replacing the gradient descent
search direction ∆θk = −gk in (9) by the quasi-Newton search direction ∆θk = −B−1
k gk  where
Bk is a damped quasi-Newton (BFGS) approximation of the Hessian [25]. To handle constraints
θ ≥ 0 introduced in Sec. 3.1  we use the stochastic interior-point method of [5].

After having taken a step along ∆θk  the samples must be updated to reﬂect the new distribution
ˆp(x; θk+1). To accomplish this feat  we use SMC [12] to sample from a sequence of distributions.

3.4 Sequential Monte Carlo

In the ﬁrst step of SMC  samples x1
that the initial importance weights are uniform. After k steps the proposal density is

(s) are drawn from a proposal density q1(x) = ˆp(x; θ1) so

˜qk(x1:k) = Kk(xk | xk−1) · · · K2(x2 | x1) ˆp(x1; θ1) 

(10)
where Kk(x′ | x) is the Markov kernel that extends the path at every iteration. The insight of [12] is
that if we choose the densities ˜pk(x1:k) wisely  we can update the importance weights ˜wk(x1:k) =
˜pk(x1:k)/˜qk(x1:k) without having to look at the entire history. This special construction is

˜pk(x1:k) = L1(x1 | x2) · · · Lk−1(xk−1 | xk) ˆp(xk; θk) 

(11)
where we’ve introduced a series of artiﬁcial “backward” kernels Lk(x | x′).
In this paper  the
sequence of distributions is determined by the iterates θk  so there remain two degrees of freedom:
the choice of forward kernel Kk(x′ | x)  and the backward kernel Lk(x | x′). From the assumptions
made in Sec. 3.1  a natural choice for the forward transition kernel is the two-stage Gibbs sampler 
(12)
in which we ﬁrst draw a sample of xB (in LDA  the variables τ and β) given xA (the discrete
variables z)  then update xA conditioned on xB. A Rao-Blackwellized version of the sub-optimal
backward kernel [12] then leads to the following expression for updating the importance weights:
(13)
where xA is the component from time step k − 1 restricted to the set A  and ˜p(xA; θk) is the
unnormalized version of the marginal ˆp(xA; θk). It also follows from earlier assumptions (Sec 3.1)
that it is always possible to compute ˜p(xA; θ). Refer to [15] for the marginal of z for LDA.

˜wk(x1:k) = ˜p(xA; θk)/˜p(xA; θk−1) × ˜wk−1(x1:k−1) 

Kk(x′ | x) = ˆp(x′

A | x′

B; θk) ˆp(x′

B | xA; θk) 

5

3.5 Safeguarding the variance

A key component of the algorithm is a mecha-
nism that enables the practitioner to regulate the
variance of the importance weights and  by exten-
sion  the variance of the Monte Carlo estimate of
E[ϕ(X)]. The trouble with taking a full step (9)
is that the Gibbs kernel (12) may be unable to
eﬀectively migrate the particles toward the new
target  in which case the the importance weights
will overcompensate for this failure  quickly lead-
ing to a degenerate population. The remedy we
propose is to ﬁnd a step size αk that satisﬁes

βSk(θk) ≤ Sk−1(θk−1) 

(14)
for β ∈ [0  1]  whereby a β near 1 leads to a strin-
gent safeguard  and we’ve deﬁned
s=1( ˜wk(x(s)

Sk(θk) ≡ Pn

1:k) − 1

n )2

(15)

• Let n  θ1  θ⋆  A  B  {αk} be given.
• Draw x(s) ∼ ˆp(x; θ1)  set w(s) = 1/n.
• Set inverse Hessian H to the identity.
• for k = 2  3  4  . . .

1. Compute gk ≈ ∇F (θk−1); see (8).
2. if k > 2  then modify H following

damped quasi-Newton update.

3. Compute variance-safeguarded step
size αk ≤ ˆαk given ∆θk = −Hgk.

4. Set θk = θk−1 + αk∆θk.
5. Update w(s) following (13).
6. Run the two-stage Gibbs sampler:

- Draw x(s)
- Draw x(s)

A ; θk).
B ; θk).
7. Resample particles  if necessary.

B ∼ ˆp( · | x(s)
A ∼ ˆp( · | x(s)

Figure 4: The proposed algorithm.

to be the sample variance (× n) for our choice of L(x | x′). Note that since our variance safeguard
scheme is myopic  the behaviour of the algorithm can be sensitive to the number of iterations.

The safeguarded step size is derived as follows. The goal is to ﬁnd the largest step size αk
satisfying (14). Forming a Taylor-series expansion with second-order terms about the point αk = 0 
the safeguarded step size is the solution to

1

2 ∆θT

k ∇2Sk(θk−1)∆θkα2

k + ∆θT

k ∇Sk(θk−1) αk = 1−β

β Sk−1(θk−1) 

(16)

where ∆θk is the search direction at iteration k. In our experience  the quadratic approximation to
the importance weights (13) was unstable as it occasionally recommended strange step sizes  but
a naive importance weight update without Rao-Blackwellization yielded a reliable bound on (14).
The derivatives of Sk(θk) work out to sample estimates of second and third moments that can be
computed in O(n) time. Since the importance weights initially have zero variance  no positive
step size will satisfy (14). We propose to also permit step sizes that do not drive the ESS below a
factor ξ ∈ (0  1) from the optimal sample. Resampling will still be necessary over long sequences
to prevent the population from degenerating. The basic algorithm is summarized in Fig. 4.

4 Application to population genetics

text corpus
documents

population
structure
individuals

⇔

topics

demes

Microsatellite genetic markers have been used to determine the
genealogy of human populations  and to assess individuals’ an-
cestry in inferring disease risks [16]. The problem is that all
these tasks require deﬁning a priori population structure. The
Bayesian model of Pritchard et al. [26] oﬀers a solution to this
conundrum by simultaneously identifying both patterns of pop-
ulation subdivision and the ancestry of individuals from highly
variable genetic markers. This model is the same as LDA assuming ﬁxed Dirichlet priors and
a single genetic marker; see Fig. 5 for the connection between the two domains. This model 
however  can be frustrating to work with because independent MCMC simulations can produce
remarkably diﬀerent answers for the same data  even simulations millions of samples long. Such
inference challenges have been observed in other mixture models [7]; MCMC can do a poor job
exploring the hypothesis space when there are several divergent hypotheses that explain the data.

Figure 5: Correspondence be-
tween LDA [4] and the popula-
tion structure [26] models.

languages
vocabulary

alleles

loci

Method. We used the software CoaSim [21] to simulate the evolution of genetic markers following
a coalescent process. The coalescent is a lineage of alleles in a sample traced backward in time to
their common ancestor allele  and the coalescent process is the stochastic process that generates
the genealogy [17]. We introduced divergence events at various coalescent times (see Fig. 6) so
that we ended up with 4 isolated populations. We simulated 10 microsatellite markers each with
a maximum of 30 alleles. We simulated the markers twice with scaled mutation rates of 2 and
1
2   and for each rate we simulated 60 samples from the coalescent process (15 diploid individuals
from each of the 4 populations). These samples are the words w in LDA. This may not seem like
a large data set  but it will be large enough to impose major challenges to approximate inference.

6

Figure 7: Variance in estimates of the admixture distance and admixture level taken over 20 trials.

The goal is to obtain posterior estimates that re-
cover the correct population structure (Fig. 6) and
exhibit high agreement in independent simula-
tions. Speciﬁcally  the goal is to recover the mo-
ments of two statistics: the admixture distance  a
measure of two individuals’ dissimilarity in their
ancestry  and the admixture level where 0 means
an individual’s alleles all come from a single pop-
ulation  and 1 means its ancestry is shared equally
among the K populations. The admixture dis-
tance between individuals d and d′ is

ϕ(τd  τd′ ) ≡ 1

2 PK

k=1|τdk − τd′k| 

(17)

and the admixture level of the dth individual is

ψ(τd) ≡ 1 − K

2(K−1) PK

k=1(cid:12)(cid:12)τdk − 1
K (cid:12)(cid:12).

Figure 6: The structured coalescent process
with divergence events at coalescent times T =
0  1
2   1  2. The width of the branches represents
eﬀective population size  and the arrow points
backward in time. The present isolated popu-
lations are labeled left-to-right 1 through 4.

(18)

We compared our algorithm to MCMC as implemented in the software Structure [26]  and to
another SMC algorithm  annealed importance sampling (AIS) [24]  with a uniform tempering
schedule. One possible limitation of our study is that the choice of temperature scehdule can be
critical to the success of AIS  and we did not thoroughly investigate alternative schedules. Also 
note that our intent was not to present an exhaustive comparison of Monte Carlo methods  so we
did not compare to population MCMC [18]  for example  which has advantages similar to AIS.

For the two data sets  and for each K from 2 to 6 (the most appropriate setting being K = 4)  we
carried out 20 independent trials of the three methods. For fair comparison  we ran the methods
with the same number of sampling events: for MCMC  a Markov chain of length 50 000 and
burn-in of 10 000; for both SMC methods  100 particles and 500 iterations. Additional settings
included an ESS threshold of 50  maximum step sizes αk = 1/(1 + k)0.6  centering parameters
σk = 1/k0.9 for the stochastic interior-point method  safeguards β = 0.95 and ξ = 0.9  and a
quasi-Newton damping factor of 0.75. We set the initial iterate of stochastic approximation to
φ = γ = ˆηkj = η⋆

j . We used uniform Dirichlet priors η⋆

j = νk = 0.1 throughout.

Results. First let’s examine the variance in the answers. Fig. 7 shows the variance in the estimates
of the admixture level and admixture distance over the independent trials. To produce these plots 
at every K we took the individual d or pair (d  d′) that exhibited the most variance in the estimate
of E[ϕ(τd  τd′ )] and E[ψ(τd)]. What we observe is that the stochastic approximation method
produced signiﬁcantly more consistent estimates in almost all cases  whereas AIS oﬀered little or
no improvement over MCMC. The next step is to examine the accuracy of these answers.

Fig. 8 shows estimates from MCMC and stochastic approximation selected trials under a mutation
rate of 1
2 and K = 4 (left-hand side)  and under a mutation rate of 2 and K = 3 (right-hand side).
The trials were chosen to reﬂect the extent of variation in the answers. The mean and standard
deviation of the admixture distance statistic are drawn as matrices. The 60 rows and 60 columns in
each matrix correspond to individuals sorted by their true population label; the rows and columns
are ordered so that they correspond to the populations 1 through 4 in Fig. 6. In each “mean” matrix 
a light square means that two individuals share little ancestry in common  and a dark square means
that two individuals have similar ancestry. In each “std. dev.” matrix  the darker the square  the
higher the variance. In the ﬁrst trial (top-left)  the MCMC algorithm mostly recovered the correct

7

Figure 8: Estimated mean and standard deviation (“std. dev.”) of the admixture distance statistic for
two independent trials and at two diﬀerent simulation settings. See the text for a full explanation.

population structure; i.e. it successfully assigned individuals to their coalescent populations based
on the sampled alleles w. As expected  the individuals from populations 3 and 4 were hardest
to distinguish  hence the high standard deviation in the bottom-right entries of the matrix. The
results of the second trial are less satisfying: MCMC failed to distinguish between individuals
from populations 3 and 4  and it decided rather arbitrarily to partition the samples originating from
population 2. In all these experiments  AIS exhibited behaviour that was very similar to MCMC.

Under the same conditions  our algorithm (bottom-left) failed to distinguish between the third and
fourth populations. The trials  however  are more consistent and do not mislead by placing high
conﬁdence in these answers; observe the large number of dark squares in the bottom-right portion
of the “std. dev.” matrix. This evidence suggests that these trials are more representative of
the true posterior because the MCMC trials are inconsistent and occasionally spurious (trial #2).
This trend is repeated in the more challenging inference scenario with K = 3 and a mutation
rate of 2 (right-hand side). MCMC  as before  exhibited a great deal of variance in its estimates
of the admixture distance:
the estimates from the ﬁrst trial are very accurate  but the second
trial strangely failed to distinguish between populations 1 and 2  and did not correctly assign the
individuals in populations 3 and 4. What’s worse  MCMC placed disproportionate conﬁdence in
these estimates. The stochastic approximation method also exhibited some variance under these
conditions  but importantly it did not place nearly so much conﬁdence in its solutions; observe the
high standard deviation in the matrix entries corresponding to the individuals from population 3.

5 Conclusions and discussion

In this paper  we proposed a new approach to probabilistic inference grounded on variational 
Monte Carlo and stochastic approximation methodology. We demonstrated that our sophisticated
method pays oﬀ in terms of producing more consistent  reliable estimates for a real and challenging
inference problem in population genetics. Some of the components such as the variance safeguard
have not been independently validated  so we cannot fully attest to how critical they are  at least
beyond the motivation we already gave. More standard tricks  such as Rao-Blackwellization  were
explicitly included to demonstrate that well-known techniques from the Monte Carlo literature
apply without modiﬁcation to our algorithm. We have argued for the generality of our inference
approach  but ultimately the success of our scheme hinges on a good choice of the variational
approximation. Thus  it remains to be seen how well our results extend to probabilistic graphical
models beyond LDA  and how much ingenuity will be required to achieve favourable outcomes.

Another critical issue  as we mentioned in Sec. 3.5  is the sensitivity of our method to the number
of iterations. This issue is related to the bias-variance trade-oﬀ  and in the future we would like to
explore more principled ways to formulate this trade-oﬀ  in the process reducing this sensitivity.

Acknowledgments

We would like to thank Matthew Hoﬀman  Nolan Kane  Emtiyaz Khan  Hendrik Kück and Pooja
Viswanathan for their input  and the reviewers for exceptionally detailed and thoughtful comments.

8

References

[1] C. Andrieu and E. Moulines. On the ergodicity properties of some adaptive MCMC algorithms. Annals

of Applied Probability  16:1462–1505  2006.

[2] C. Andrieu  N. de Freitas  A. Doucet  and M. I. Jordan. An introduction to MCMC for machine learning.

Machine Learning  50:5–43  2003.

[3] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis  University College

London  2003.

[4] D. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research 

3:993–1022  2003.

[5] P. Carbonetto  M. Schmidt  and N. de Freitas. An interior-point stochastic approximation method and
an L1-regularized delta rule. In Advances in Neural Information Processing Systems  volume 21. 2009.
[6] G. Casella and C. P. Robert. Rao-Blackwellisation of sampling schemes. Biometrika  83:81–94  1996.
[7] G. Celeux  M. Hurn  and C. P. Robert. Computational and inferential diﬃculties with mixture posterior

distributions. Journal of the American Statistical Association  95:957–970  2000.

[8] D. W. Coltman. Molecular ecological approaches to studying the evolutionary impact of selective

harvesting in wildlife. Molecular Ecology  17:221–235  2007.

[9] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley  1991.

[10] P.-T. de Boer  D. P. Kroese  S. Mannor  and R. Y. Rubinstein. A tutorial on the cross-entropy method.

Annals of Operations Research  134:19–67  2005.

[11] N. de Freitas  P. Højen-Sørensen  M. I. Jordan  and S. Russell. Variational MCMC. In Proceedings of

the 17th Conference on Uncertainty in Artiﬁcial Intelligence  pages 120–127  2001.

[12] P. Del Moral  A. Doucet  and A. Jasra. Sequential Monte Carlo samplers. Journal of the Royal Statistical

Society  68:411–436  2006.

[13] A. J. Dobson. An Introduction to Generalized Linear Models. Chapman & Hall/CRC Press  2002.
[14] A. Doucet  S. Godsill  and C. Andrieu. On sequential Monte Carlo sampling methods for Bayesian

ﬁltering. Statistics and Computing  10:197–208  2000.

[15] T. L. Griﬃths and M. Steyvers. Finding scientiﬁc topics. Proceedings of the National Academy of

Sciences  101:5228–5235  2004.

[16] D. L. Hartl and A. G. Clark. Principles of population genetics. Sinauer Associates  2007.
[17] J. Hein  M. H. Schierup  and C. Wiuf. Gene genealogies  variation and evolution: a primer in coalescent

theory. Oxford University Press  2005.

[18] A. Jasra  D. Stephens  and C. Holmes. On population-based simulation for static inference. Statistics

and Computing  17:263–279  2007.

[19] M. Jordan  Z. Ghahramani  T. Jaakkola  and L. Saul. An introduction to variational methods for graphical

models. In M. Jordan  editor  Learning in Graphical Models  pages 105–161. MIT Press  1998.

[20] F. Liang  C. Liu  and R. J. Carroll. Stochastic approximation in Monte Carlo computation. Journal of

the American Statistical Association  102:305–320  2007.

[21] T. Mailund  M. Schierup  C. Pedersen  P. Mechlenborg  J. Madsen  and L. Schauser. CoaSim: a ﬂexible

environment for simulating genetic data under coalescent models. BMC Bioinformatics  6  2005.

[22] T. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis  MIT  2001.
[23] R. Neal and G. Hinton. A view of the EM algorithm that that justiﬁﬁes incremental  sparse  and other
variants. In M. Jordan  editor  Learning in graphical models  pages 355–368. Kluwer Academic  1998.

[24] R. M. Neal. Annealed importance sampling. Statistics and Computing  11:125–139  2001.
[25] M. J. D. Powell. Algorithms for nonlinear constraints that use Lagrangian functions. Mathematical

Programming  14:224–248  1978.

[26] J. K. Pritchard  M. Stephens  and P. Donnelly.

Inference of population structure using multilocus

genotype data. Genetics  155:945–959  2000.

[27] P. Ravikumar. Approximate Inference  Structure Learning and Feature Estimation in Markov Random

Fields. PhD thesis  Carnegie Mellon University  2007.

[28] H. Robbins and S. Monro. A stochastic approximation method. Annals of Math. Statistics  22  1951.
[29] J. C. Spall. Introduction to stochastic search and optimization. Wiley-Interscience  2003.
[30] Y. W. Teh  D. Newman  and M. Welling. A collapsed variational Bayesian inference algorithm for latent

Dirichlet allocation. In Advances in Neural Information Processing Systems  volume 19  2007.

[31] M. J. Wainwright. Stochastic processes on graphs with cycles: geometric and variational approaches.

PhD thesis  Massachusetts Institute of Technology  2002.

[32] J. S. Yedidia  W. T. Freeman  and Y. Weiss. Constructing free-energy approximations and generalized

belief propagation algorithms. IEEE Transactions on Information Theory  51:2282–2312  2005.

[33] L. Younes. Stochastic gradient estimation strategies for Markov random ﬁelds. In Proceedings of the

Spatial Statistics and Imaging Conference  1991.

9

,Hyeonwoo Yu
Beomhee Lee