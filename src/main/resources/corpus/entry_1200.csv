2017,Convolutional Phase Retrieval,We study the convolutional phase retrieval problem  which asks us to recover an unknown signal ${\mathbf x} $ of length $n$ from $m$ measurements consisting of the magnitude of its cyclic convolution with a known kernel $\mathbf a$ of length $m$. This model is motivated by applications to channel estimation  optics  and underwater acoustic communication  where the signal of interest is acted on by a given channel/filter  and phase information is difficult or impossible to acquire. We show that when $\mathbf a$ is random and $m \geq \Omega(\frac{ \| \mathbf C_{\mathbf x}\|^2}{ \|\mathbf x\|^2 }  n \mathrm{poly} \log n)$  $\mathbf x$ can be efficiently recovered up to a global phase using a combination of spectral initialization and generalized gradient descent. The main challenge is coping with dependencies in the measurement operator; we overcome this challenge by using ideas from decoupling theory  suprema of chaos processes and the restricted isometry property of random circulant matrices  and recent analysis for alternating minimizing methods.,Convolutional Phase Retrieval

Qing Qu

Columbia University
qq2105@columbia.edu

Yonina C. Eldar

Technion

yonina@ee.technion.ac.il

Yuqian Zhang

Columbia University
yz2409@columbia.edu

John Wright

Columbia University
jw2966@columbia.edu

Abstract

We study the convolutional phase retrieval problem  which considers recov-
ery of an unknown signal x 2 Cn from m measurements consisting of the
magnitude of its cyclic convolution with a known kernel a of length m. This
model is motivated by applications to channel estimation  optics  and un-
derwater acoustic communication  where the signal of interest is acted on
by a given channel/ﬁlter  and phase information is diﬃcult or impossible to
acquire. We show that when a is random and m is suﬃciently large  x can
be eﬃciently recovered up to a global phase using a combination of spec-
tral initialization and generalized gradient descent. The main challenge is
coping with dependencies in the measurement operator; we overcome this
challenge by using ideas from decoupling theory  suprema of chaos pro-
cesses and the restricted isometry property of random circulant matrices 
and recent analysis for alternating minimizing methods.

Introduction

1
We study the problem of recovering a unknown signal x 2 Cn from measurements y =
ja ⊛ xj  which consist of the magnitude of the convolution of x and a given ﬁlter a 2 Cm 
(1)
where ⊛ denotes cyclic convolution. Let Ca 2 Cm(cid:2)m be a circulant matrix generated by a 
and let A 2 Cm(cid:2)n be a matrix formed by the ﬁrst n columns of Ca. Then the convolutional
phase retrieval problem can be rewritten in the common matrix-vector form

s.t. y = ja ⊛ zj ;

ﬁnd z;

s.t. y = jAzj :

ﬁnd z;

(2)
This problem is motivated by applications like channel estimation [37  1]  (non)coherent op-
tical communication [14  24]  and underwater acoustic communication [31]. For example  in
millimeter-wave (mm-wave) wireless communications for 5G networks [27]  one important
problem is to reconstruct signal angle of arrival (AoA) from measurements  which are taken
by the convolution of signal AoA and the antenna pattern.
Because of technical diﬃculties that the phase measurements are either very noisy and unre-
liable  or expensive to acquire  it is preferred to only take measurements of signal magnitude
and the phase information is lost.
Most known results on the exact solution of phase retrieval problems [8  29  10  38  36  35]
pertain to generic random matrices  where the entries of A are independent subgaussian ran-
dom variables. However  in practice it is almost impossible to design purely random mea-
surement matrices: in many cases as we mentioned above  the measurement is much more

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

structured – generated by passing a signal through a manually designed ﬁlter. Moreover 
the structured measurements often admit more eﬃcient numerical methods: by using the
fast Fourier transform for matrix-vector products  the benign structure of the convolutional
model (1) allows us to design methods with O(m) memory and O(m log m) computation
cost per iteration. While for generic measurements  the cost is around O(mn).
In this work  we study the convolutional phase retrieval problem (1) under the assumption
that the kernel a = [a1;(cid:1)(cid:1)(cid:1) ; am]
⊤

is random  with each entry i.i.d. complex Gaussian 

0; 1
2 I

(3)
Compared to the generic random measurement  as we can see  the random convolution
model we study here is far more structured: it is parameterized by only O(m) independent
complex normal random variables  whereas the generic model involves O(mn) ones. Since
the rows and columns of A are probabilistically dependent  standard techniques (based on
concentration of functions of independent random vectors) do not apply.
We propose and analyze a local1 gradient descent type method  minimizing a weighted 
nonconvex and nonsmooth objective

:

a = u + iv; u; v (cid:24)iid N(

)

(cid:13)(cid:13)(cid:13)b1=2 ⊙ (y (cid:0) jAzj)
(cid:13)(cid:13)(cid:13)2

min
z2Cn

f (z) =

1
2m

;

(4)

{

)

∥x∥2 n poly log n

(∥Cx∥2

++ is a weighting vector. Our result can

}
xeiϕ j ϕ 2 [0; 2(cid:25))

where ⊙ denotes the Hadamard product and b 2 Rm
be informally summarized as follows.
Theorem 1.1 (Informal) When m (cid:21) Ω(n poly log n)  with high probability  spectral initializa-
tion [25  5] produces an initialization z(0) that is O(1= poly log n) close to the optimum. Moreover 
when m (cid:21) Ω
  with high probability  a certain gradient descent method based
on (4) converges linearly from this initialization to the set X =
of points that
diﬀer from the true signal x only by a global phase.
Here  Cx 2 Cm(cid:2)m denotes the circulant matrix corresponding to cyclic convolution with a
length m zero padding of x  and poly log n denotes a polynomial in log n. A dependence of
the sample complexity m on ∥Cx∥ seems inevitable2 and is corroborated by experiments.
Our proof is based on ideas from decoupling theory [11]  the suprema of chaos processes and
restricted isometry property of random circulant matrices [26  20]  and a new iterative analy-
sis of alternating minimizing methods [35]. Our analysis draws connections between the
convergence properties of gradient descent and the classical alternating direction method.
This allows us to avoid the need to argue that high-degree polynomials in the structured
random matrix A concentrate uniformly  as would be required by a straightforward trans-
lation of existing analysis to this new setting. Instead  we control the bulk eﬀect of phase
errors uniformly in a neighborhood around the ground truth. This requires us to develop
new decoupling and concentration tools for controlling nonlinear phase functions of circu-
lant random matrices  which could be potentially useful for analyzing other random circu-
lant convolution problems such as blind deconvolution [40]  and convolutional dictionary
learning [18].

Prior art for phase retrieval. The challenge of developing eﬃcient  guaranteed methods
for phase retrieval has attracted substantial interest over the past decade [28  19]. For the
generalized phase retrieval problem in which the sensing matrix A is i.i.d. random  the ﬁrst
result on global recovery is based on semideﬁnite programming (SDP) [8  3  36]. However 
the computational cost of SDP limits its practicality. Nonconvex methods can be more ef-
ﬁcient. [25] showed that the alternating minimization method provably converges to the

1It would be nicer to characterize the global geometry of the problem as in [15  33  34  32]. However 
the nonhomogeneity of ∥Cx∥ over the space causes tremendous diﬃculties for concentration with
m (cid:21) Ω(n poly log n) samples.
n).
For instance  ∥Cx∥ = 1 when x is a standard basis vector; and ∥Cx∥ =

p
2The operator norm of Cx is nonhomogeneous over x 2 CSn(cid:0)1  ranging from constant to O(

p
n when x = 1p

n 1.

2

truth  when initialized using a spectral method and provided with fresh samples at each
iteration. Candes et al. [5] showed with the same initalization  gradient descent for the
nonconvex least squares objective 

(cid:13)(cid:13)(cid:13)y2 (cid:0) jAzj2

(cid:13)(cid:13)(cid:13)2

;

1
2m

min
z2Cn

f1(z) =

(5)
provably recovers the ground truth  with near-optimal sample complexity m (cid:21) Ω(n log n).
The work [10  39  38] further reduce the sample complexity to m (cid:21) Ω(n) by using diﬀerent
nonconvex objectives and truncation techniques. Moreover  [34] reveals that the nonconvex
objective (5) has a benign global geometry: with high probability  it has no bad critical points
with m (cid:21) Ω(n log3 n) samples3.
Structured random measurements. The study of structured random measurement in sig-
nal processing [21] includes the study of random Fourier measurements [7  9  12] and partial
random convolutions [26  20] in compressed sensing [6]. However  the study of structured
random measurement for phase retrieval is still quite limited. In particular  [17] and [4] stud-
ied the performance of SDP methods with t-designs and random masked Fourier transform
measurements. The authors in [5  2] show that the phase retrieval problem with random
coded diﬀraction and STFT measurements can be solved by minimizing nonconvex objec-
tives  while [5] requires resampling for the initialization  and in [2] the contraction radius
is not large enough for initialization. In addition  the motivation of these measurement
schemes are quite diﬀerent from ours. For more detailed review of this subject  we refer the
readers to Section 4 of [21].
Notations. We use ((cid:1))
(cid:3) to denote the real and Hermitian transpose  respectively.
We use CSn(cid:0)1 to denote a n dimensional complex sphere. Let ℜ((cid:1)) and ℑ((cid:1)) denote the real
and imaginary parts of a complex variable  respectively. Throughout the paper  we assume
the optimal solution is x 2 Cn. Because the solution is only optimal to a global phase shift 
we deﬁne the optimal solution set as X =
  and deﬁne the distance from
a point z 2 Cn to the set X as

xei(cid:18) j (cid:18) 2 [0; 2(cid:25))

⊤ and ((cid:1))

{

}
(cid:13)(cid:13) :

(cid:13)(cid:13)z (cid:0) xei(cid:18)

dist(z;X )

:
= inf

(cid:18)2[0;2(cid:25))

For any z 2 C with jzj ̸= 0  we use ϕ(z) to denote the phase of z  that is  eiϕ(z) = z=jzj.

2 Algorithm

We develop an approach to convolutional phase retrieval based on local nonconvex opti-
mization. Our proposed algorithm has two components: (1) a careful initialization using
the spectral method; (2) local reﬁnement by (generalized) gradient descent. We introduce
the two steps in reverse order.

2.1 Minimization of a nonconvex and nonsmooth objective
We consider minimizing a weighted nonconvex and nonsmooth objective

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)b1=2 ⊙ (y (cid:0) jAzj)

f (z) =

1
2m

:

(6)

The introduction of the positive weights b facilitates our analysis  by enabling us to compare
certain functions of the dependent random matrix A to functions involving more indepen-
dent random variables. We will substantiate this claim in the next section.
Although the function (4) is not complex-diﬀerentiable  if one identiﬁes Cn with R2n and
treats f (z) as a function in the real domain  f is still diﬀerentiable in the real sense. Thus 
we adopt the Wirtinger calculus [22]  which can be thought of as a clean way of organizing
the real partial derivatives [29  34].

3[30] tightened the sample complexity to m (cid:21) Ω(n log n) by using advanced probability tools.

3

On the other hand  it should also be noted that the absolute value j(cid:1)j is nonsmooth at 0 and
hence f (z) is not diﬀerentiable everywhere even in the real sense. Similar to [38]  for any
complex number u 2 C  if we uniquely deﬁne its phase ϕ(u) at 0 by

{

exp (iϕ(u))

:
=

u=juj
1

ifjuj ̸= 0;
otherwise;

then the Wirtinger gradient of (4) can be uniquely determined as

@
@z

f (z) =

1
m

(cid:3)

diag (b) [Az (cid:0) y ⊙ exp (iϕ(Az))] :

A

(7)

Starting from some initialization z(0)  we minimize the objective (6) by gradient descent

z(r+1) = z(r) (cid:0) (cid:28)

@
@z

f (z(r));

(8)

where (cid:28) > 0 is the stepsize. Indeed  @
the real case; this method is also referred to as amplitude ﬂow [38].

@z f (z) can be interpreted as the gradient of f (z) as in

2.2

Initialization via spectral method

Similar to [25  29]  we compute the initialization z(0) via a spectral method  detailed in [29 
Algorithm 1]. More speciﬁcally  z(0) is a scaled leading eigenvector of

(

)

m∑

k=1

Y =

1
m

(cid:3)
y2
kaka
k =

(cid:3)

A

1
m

diag

y2

A;

(9)

which is constructed from the knowledge of the sensing vectors and observations. The
leading eigenvector of Y can be eﬃciently computed via the power method. Note that
E [Y ] = ∥x∥2 I + xx
(cid:3)  so the leading eigenvector of E [Y ] is proportional to the optimal so-
lution x. Under the random convolutional model of A  by using probability tools from [21] 
(cid:3)E [Y ] v for all v 2 CSn(cid:0)1 whenever
we show that v
m (cid:21) Ω(n poly log n)  ensuring the initialization z(0) close to the optimal set X .

Y v concentrates to its expectation v

(cid:3)

3 Main Result and Analysis

)

In this section  we describe our main theoretical result  which shows that with high proba-
bility  the algorithm described in the previous section succeeds.
Theorem 3.1 (Main Result) Whenever m (cid:21) C0n log31 n  the spectral method [29  Algorithm 1]
(
produces an initialization z(0) that satisﬁes
(cid:0)6 n∥x∥
z(0);X
dist
(
(cid:0)c2. Suppose b = (cid:16)(cid:27)2 (y)  where

with probability at least 1 (cid:0) c1m

(cid:16)(cid:27)2 (t) = 1 (cid:0) 2(cid:25)(cid:27)2(cid:24)(cid:27)2 (t);
{

(10)
with (cid:27)2 > 1=2. Starting from z(0)  with (cid:27)2 = 0:51 and stepsize (cid:28) = 2:02  whenever m (cid:21)
(cid:0)c4 for all iterate z(r)(r (cid:21) 1)
)
C1
deﬁned in (8)  we have

}
  with probability at least 1 (cid:0) c3m
(

∥Cx∥2
∥x∥2 max

1
2(cid:25)(cid:27)2 exp

log17 n; n log4 n

(cid:20) c0 log

(cid:0) t2
2(cid:27)2

(cid:24)(cid:27)2(t) =

;

t > 0;

)

z(r);X

(cid:20) (1 (cid:0) ϱ)r dist

z(0);X

dist

;

(11)

)

(

holds for a small scalar ϱ 2 (0; 1). Here  c0; c1; c2; c3; c4; C0; C1 > 0 are numerical constants.

4

)

(

Remark: Our result shows that by initializing the problem O(1=polylog(n))-close to the
optimum via spectral method  the gradient descent (8) converges linearly to the optimal so-
lution. As we can see  the sample complexity here also depends on ∥Cx∥  which is quite dif-
ferent from the i.i.d. case. For a typical x 2 CSn(cid:0)1 (e.g.  x is drawn uniformly random from
CSn(cid:0)1)  ∥Cx∥ remains as O(log n)  the sample complexity m (cid:21) Ω (n poly log n) matches the
i.i.d. case up to log factors. However  ∥Cx∥ is nonhomogeneous over x 2 CSn(cid:0)1: if x is
sparse in the Fourier domain (e.g.  x = 1p
n 1)  the sample complexity can be as large as
m (cid:21) Ω
. Such a behavior is also demonstrated in the experiments of Section
4. We believe the (very large!) number of logarithms in our result is an artifact of our anal-
ysis  rather than a limitation of the method. We expect to reduce the sample complexity
to m (cid:21) Ω
by a tighter analysis  which is left for future work. The choices
of the weighting b 2 Rm in (10)  (cid:27)2 = 0:51  and the stepsize (cid:28) = 2:02 are purely for the
purpose of analysis. In practice  the algorithm converges with b = 1 and a choice of small
stepsize (cid:28)  or by using backtracking linesearch for the stepsize (cid:28).
In the following  we brieﬂy highlight some major challenges and novel prooﬁng ideas be-
hind the analysis. The details can be found in our full paper.

(∥Cx∥2

∥x∥2 n log6 n

n2 poly log n

)

3.1 Proof sketch of iterative contraction
Our analysis is largely inspired by the recent analysis of alternating direction method (ADM)
[35]. In this following  we draw connections between the gradient descent method (8) and
ADM  and sketch basic ideas of convergence analysis.

ADM iteration. ADM is a classical method for solving phase retrieval problems [16  25 
35]  which can be considered as a heuristic method that solves the problem

∥Az (cid:0) y ⊙ u∥2 :

1
2

(cid:3)

A

A

m

5

(

;

(

(cid:3)

A)

;

;

1
2

z

where A

y

y

y ⊙ exp

min

z2Cn;juj=1

(cid:0)1 A
)

c(r+1) = y ⊙ exp

= (A

(bz(r+1);X

which leads to the following update

(cid:13)(cid:13)(cid:13)Az (cid:0) c(r+1)

At every iteratebz(r)  ADM proceeds in two steps:

(cid:13)(cid:13)(cid:13)2
(
)
Abz(r)
bz(r+1) = arg min
(
))
bz(r+1) = A
Abz(r)
(cid:13)(cid:13)(cid:13)bz(r) (cid:0) xei(cid:18)
(cid:3) is the pseudo-inverse of A. Letb(cid:18)r = arg min(cid:18)
distance betweenbz(r+1) and X is bounded by
))(cid:13)(cid:13)(cid:13) :
y(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)Axeib(cid:18)r (cid:0)
(cid:13)(cid:13)(cid:13) (cid:20)(cid:13)(cid:13)A
(cid:13)(cid:13)(cid:13)bz(r+1) (cid:0) xeib(cid:18)r+1
(
Abz(r)
(cid:13)(cid:13)(cid:13)  with stepsize (cid:28) = 1. The distance between
(cid:13)(cid:13)(cid:13)z(r) (cid:0) xei(cid:18)
(cid:13)(cid:13)(cid:13)(cid:13)I (cid:0) 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)z(r) (cid:0) xei(cid:18)r
(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13) (cid:20)
(cid:13)(cid:13)(cid:13)z(r+1) (cid:0) xei(cid:18)r+1
(cid:13)(cid:13)(cid:13)Axei(cid:18)r (cid:0) y ⊙ exp
(
(cid:13)(cid:13)(cid:13)(cid:13) = o(1);
(cid:13)(cid:13)(cid:13)(cid:13)I (cid:0) 1
y(cid:13)(cid:13) (cid:25) 1=

Gradient descent with b = 1. For simplicity  let us consider the gradient descent update
(8) with b = 1. Let (cid:18)r = arg min(cid:18)
the iterate z(r) and the optimal set X is bounded by

Towards iterative contraction. By measure concentration  it can be shown that

(cid:13)(cid:13)(cid:13). The

∥A∥ (cid:25) p

)(cid:13)(cid:13)(cid:13) :

z(r+1);X

(cid:13)(cid:13)A

m
∥A∥

y ⊙ exp

p

m;

iϕ(Az(r))

+

1
m

)

=

dist

=

(13)

(14)

(

dist

(cid:3)

A

A

m;

(12)

holds with high probability whenever m (cid:21) Ω (n poly log n). Therefore  to show iterative
contraction of both methods  based on (12) and (13)  it is suﬃcient to show that

(cid:13)(cid:13)Axei(cid:18) (cid:0) y ⊙ exp (iϕ(Az))

(cid:13)(cid:13) (cid:20) (1 (cid:0) (cid:17))

m

(15)
for some constant (cid:17) 2 (0; 1)  where (cid:18) = arg min(cid:18)2[0;2(cid:25))
zj.
(cid:3)
By similar ideas of controlling (15) for the ADM method [35]  this observation provides a
new way of analyzing the gradient descent method. As an attempt to show (15) for the
random circulant matrix A  we invoke the following lemma  which controls the error in a
ﬁrst order approximation to exp(iϕ((cid:1))).
Lemma 3.2 (Lemma 3.2  [35]) For any (cid:26) > 0  and for any z; z

′ 2 C  we have

z=jx

(cid:3)

(cid:13)(cid:13) ;

(cid:13)(cid:13)z (cid:0) xei(cid:18)
(cid:13)(cid:13)(cid:13) such that ei(cid:18) = x

p

(cid:13)(cid:13)(cid:13)z (cid:0) xei(cid:18)

Let us decompose z = (cid:11)x + (cid:12)w  where w 2 CSn(cid:0)1 with w ? x  and (cid:11); (cid:12) 2 C. Note that
ϕ((cid:11)) = (cid:18). Then by Lemma 3.2  for any (cid:26) 2 (0; 1)  we have

jexp (iϕ(z

′

+ z)) (cid:0) exp (iϕ(z

(cid:13)(cid:13)Axei(cid:18) (cid:0) y ⊙ exp (iϕ(Az))

(cid:13)(cid:13)(cid:13)jAxj ⊙ j (cid:12)
{z
|

(cid:11)

(cid:20)

(cid:13)(cid:13) =
(cid:13)(cid:13)(cid:13)
}

jjAwj(cid:21)(cid:26)jAxj
T1

+

)j :

′

′

(cid:0)1 jℑ (z=z
))j (cid:20) 2jzj(cid:21)(cid:26)jz′j + (1 (cid:0) (cid:26))
(cid:13)(cid:13)(cid:13)(cid:13)jAxj ⊙
(
(
[
(cid:12)(cid:12)(cid:12)(cid:12)∥ℑ ((Aw) ⊙ exp ((cid:0)iϕ(Ax)))∥
(cid:12)(cid:12)(cid:12)(cid:12) (cid:12)
exp (iϕ (Ax)) (cid:0) exp
{z
|
}

1
1 (cid:0) (cid:26)

Ax +

iϕ

(cid:11)

Aw

(cid:12)
(cid:11)

:

))](cid:13)(cid:13)(cid:13)(cid:13)

T2

The ﬁrst term T1 can be bounded using the restricted isometry property of random circulant
matrices [20]  together with some auxiliary analysis.
The second term T2 involves a nonlinear function exp ((cid:0)iϕ(Ax)) of the random circulant
matrix A. Controlling this nonlinear  highly dependent random process T2(w) for all w
is a nontrivial task. Next  we explain why controlling T2 is technically challenging  and
sketch the key ideas about how to control a smoothed variant of T2  by using the weighting
b = (cid:16)(cid:27)2(y) introduced in (10). We also provide intuition for why the weighting b is helpful.
3.2 Controlling the phase term T2
{
As elaborated above  the major challenge of showing iterative contraction is bound-
ing the suprema of the nonlinear  dependent random process T2(w) over the set S =
w 2 CSn(cid:0)1 j w ? x

. By using the fact that ℑ(u) = 1

}

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w
|

2i (u (cid:0) u) for any u 2 C  we have
{z

diag ( (Ax)) Aw

}

L(a;w)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ;

m  the major task left is to show that

jL(a; w)j < (1 (cid:0) (cid:17)

′

)m

(16)

T 2

2

(cid:20) 1

2

sup
w2S

∥A∥2 + 1

2 sup
w2S

⊤

⊤

A

where (t)

= exp ((cid:0)2iϕ(t)). As from (14)  ∥A∥ (cid:25) p

:

′ 2 (0; 1).
for some constant (cid:17)
Why decoupling? Let A =

[

sup
w2S

]

(cid:3)
a
1(cid:1)(cid:1)(cid:1)
(cid:3)
a
m

. The term

L(a; w) = w

⊤

A

⊤

diag ( (Ax)) Aw =

m∑

k=1

|

(cid:3)
 (a
kx)w

{z

⊤

}

⊤
k w

aka

dependence across k

is a summation of dependent random variables  for which our probability tools are very
limited. To overcome this problem  we deploy ideas from decoupling [11]. Informally  decou-
pling allows us to compare moments of the original random function to functions of more
independent random variables  which are usually easier to analyze. The book [11] provides

6

⊤

; w) = w

QL
′
dec(a; a

a beautiful introduction to this area. In our problem  notice that the random vector a oc-
curs twice in the deﬁnition of L(a; w) – one in the phase term (Ax) = exp((cid:0)2iϕ(Ax)) 
and another in the quadratic term. The general spirit of decoupling is to seek to replace one
′ of the same random vector  yielding a random process with
a with an independent copy a
fewer dependencies. Here  we seek to replace L(a; w) with
′

⊤
(17)
The usefulness of this new  decoupled form QL
; w)  is that it introduces extra ran-
domness — QL
′
′. This makes analyz-
; w) is now a chaos process of a conditioned on a
dec(a; a
ing supw2S QL
′
; w) amenable to existing analysis of suprema of chaos processes for ran-
dec(a; a
dom circulant matrices [21]. However  achieving the decoupling requires additional work;
the most general existing results on decoupling pertain to tetrahedral polynomials  which are
polynomials with no monomials involving any power larger than one of any random vari-
able. By appropriately tracking cross terms  these results can also be applied to more gen-
eral (non-tetrahedral) polynomials in Gaussian random variables [23]. However  our ran-
dom process L(a; w) involves a nonlinear phase term (Aw) which is not a polynomial 
and hence is not amenable to a direct appeal to existing results.

′
dec(a; a

diag ( (A

x)) Aw:

A

Decoupling is “recoupling”. Existing results [23] for decoupling polynomials of Gaus-
sian random variables are derived from two simple facts: (i) orthogonal projections of Gaus-
sian variables are independent  and (ii) Jensen’s inequality. Indeed  for a (cid:24) CN (0; I)  let us
introduce an independent vector (cid:14) (cid:24) CN (0; I). Write

g1 = a + (cid:14);

g2 = a (cid:0) (cid:14):

w2S

7

:

(

E(cid:14)

[

w2S φ
sup

]

[

= E(cid:14)

(cid:20) Ea;(cid:14)

sup
w2S φ

= Eg1;g2

sup
w2S φ

= Ea

E(cid:14)

w2S φ
sup

′
dec(a; a

dec(g1; g2; w)

:

dec(g1; g2; w)

)]
(cid:12)(cid:12)QL

dec(a + (cid:14); a (cid:0) (cid:14); w)

dec(a + (cid:14); a (cid:0) (cid:14); w)

= bL(a; w):
])]

Because of Fact (i)  these are independent CN (0; 2I) vectors. By conditional expectation 

]
(18)
Thus  we can see that the key idea of decoupling L(a; w) into QL
; w)  is essentially
“recoupling” QL
viewed as an approximation of L(a; w). Notice that by Fact (ii)  for any convex function φ 
Ea

[QL
dec(g1; g2; w) via conditional expectation – the “recoupled” term bL can be
)]
(bL(a; w)
[
)]
(QL
Thus  by choosing φ(t) = jtjp  we can control moments of supw2S bL(a; w) via
(cid:13)(cid:13)(cid:13)(cid:13) sup
For tetrahedral polynomials  bL = L  so the approximation is exact. As the tail bound of
(cid:12)(cid:12)(cid:12) can be controlled via its moments bounds [13  Chapter 7.2]  this allows

[QL
[QL
(cid:13)(cid:13)(cid:13)(cid:13) sup

(QL
(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:12)(cid:12)(cid:12)bL(a; w)

[
dec(a + (cid:14); a (cid:0) (cid:14); w)

supw2S
us to directly control the object L(a; w) of interest. The reason that this control obtains is
because the conditional expectation operator E(cid:14) [(cid:1) j a] “recouples” QL
; w) back to
the target L(a; w). In slogan form  (Gaussian) decoupling is recoupling.
“Recoupling” is Gaussian smoothing. A distinctive feature in convolutional phase re-
trieval is that L is not a polynomial. Hence  it may be challenging to posit a QL
dec which
“recouples” back to L. In other words  in the existing form  we need to tolerate an approxi-

mation error as bL ̸= L. By the triangle inequality 
(cid:12)(cid:12)(cid:12) + sup
(cid:12)(cid:12)(cid:12)bL(a; w)
(cid:12)(cid:12)(cid:12) can be sharply controlled via its moments bound
(cid:12)(cid:12)(cid:12)bL(a; w)

As discussed above  the supw2S
in (19). Now the bound (20) is useful to derive tight control for L(a; w)  if L(a; w) is very

(cid:12)(cid:12)(cid:12) :
(cid:12)(cid:12)(cid:12)bL(a; w) (cid:0) L(a; w)

jL(a; w)j (cid:20) sup
w2S

(cid:12)(cid:12)(cid:12)bL(a; w)

dec(g1; g2; w)

′
dec(a; a

(cid:20)

Lp

(cid:12)(cid:12)(cid:13)(cid:13)(cid:13)(cid:13)

:

Lp

w2S

w2S

sup
w2S

(19)

(20)

close to bL(a; w) uniformly. The question is: for what L is it possible to ﬁnd a “well-behaved”
mechanism that links Qdec back to bL is the conditional expectation operator E(cid:14) [(cid:1) j a]. For

QL
dec for which the approximation error is small? To understand this question  recall that the

our case  from (18) orthogonality leads to

bL(a; w) = wA

⊤

Thus  by combining the results in (20) and (21)  we have

diag (h(Ax)) Aw;

s(cid:24)CN (0;∥x∥2) [ (t + s)] :

(cid:12)(cid:12)(cid:12)bL(a; w)

:
= E

h(t)

(cid:12)(cid:12)(cid:12) + ∥h (cid:0) ∥
{z

|

}

L1

∥A∥2 :

approximation error

jL(a; w)j (cid:20) sup
w2S

sup
w2S

(21)

(22)

Therefore  L(a; w) is a poor target for approximation with a smooth function bL = E(cid:14)[QL

Note that the function h is not exactly   but generated by convolving with a multivariate
Gaussian pdf : indeed  recoupling is Gaussian smoothing. The Fourier transform of a multivari-
ate Gaussian is again a Gaussian; it decays quickly with frequency. So  in order to admit a
small approximation error  the target L must be smooth. However  in our case  the function
 (t) = exp((cid:0)2iϕ(t)) is discontinuous at t = 0; it changes extremely rapidly in the vicinity of
t = 0  and hence its Fourier transform (appropriately deﬁned) does not decay quickly at all.
dec].
From Fig. 1  the diﬀerence between h and increases as jtj ↘ 0. The poor approximation
error ∥h (cid:0) ∥
Decoupling and convolutional phase retrieval. The key idea to reduce the approxima-
tion error ∥ (cid:0) h∥
L1 = 1 is to smooth . More speciﬁcally  we introduce a new objective
(6) with Gaussian weighting b = (cid:16)(cid:27)2 (y) in (10)  replacing the analyzing target T2 with

L1 = 1 results in a trivial bound for supw2S jL(a; w)j instead of (16).

ℑ ((Aw) ⊙ exp ((cid:0)iϕ(Ax)))
Consequently  we obtain a smoothed variant Ls(a; w) of L(a; w) 

b1=2

A

Ls(a; w) = w

diag ((cid:16)(cid:27)2(y) ⊙ (Ax)) Aw:

⊤
Now the approximation error ∥h (cid:0) ∥
L1. As
observed from Fig. 1  the function (cid:16)(cid:27)2(t) smoothes (t) especially near the vicinity of t =
0  such that the new approximation error ∥f (t) (cid:0) (cid:16)(cid:27)2(t) (t)∥
L1 is signiﬁcantly reduced.
Thus  by using similar ideas as above  we can prove a desired bound supw2S jLs(a; w)j <
(1 (cid:0) (cid:17)s)m. Finally  because the new weighting b = (cid:16)(cid:27)2 (y)  the overall analysis needs to be
slightly modiﬁed correspondingly. We refer the readers to our full paper for more details.

⊤
L1 in (22) is replaced by ∥h(t) (cid:0) (cid:16)(cid:27)2(t) (t)∥

(

(cid:13)(cid:13)(cid:13)diag

bT2 =

)

(cid:13)(cid:13)(cid:13) :

Figure 1: Plots of functions (cid:16)(cid:27)2(t) 
f (t) and (t) for t 2 R+.

Figure 2: Phase transition for recov-
ering the signal x 2 CSn(cid:0)1 with dif-
ferent ∥Cx∥.

4 Experiments
Dependence of sample complexity on ∥Cx∥. First  we investigate the dependence of the
sample complexity m on ∥Cx∥. We assume the ground truth x 2 CSn(cid:0)1  and consider three

8

cases: (1) x = e1 with ∥Cx∥ = 1  where e1 the standard basis vector; (2) x is uniformly ran-
dom generated from CSn(cid:0)1; (3) x = 1p
n. For each case  we ﬁx the signal
length n = 1000 and vary the ratio m=n. For each ratio m=n  we randomly generate the
kernel a (cid:24) CN (0; I) and repeat the experiment for 100 times. We initialize the algorithm
by the spectral method [29  Algorithm 1] and run the gradient descent (8). Given the al-
(cid:0)5. From Fig. 2  we can see that the larger the ∥Cx∥  the more samples are needed

gorithm output bx  we judge the success of recovery by inf ϕ2[0;2(cid:25))

n 1  with ∥Cx∥ =

ϵ = 10
for exact recovery.

p

(cid:13)(cid:13)bx (cid:0) xeiϕ

(cid:13)(cid:13) (cid:20) ϵ  where

Figure 3: Experiment on real images.

Experiments on real image. Finally  we run the experiment on some real dataset to demon-
strate the eﬀectiveness and the eﬃciency of the proposed method. We choose an image of
size 200 (cid:2) 300 as in Fig. 4  we use m = 5n log n samples for reconstruction. The kernel
a 2 Cm is randomly generated as complex Gaussian CN (0; I). We run power method
for 100 iterations for initialization  and stop the algorithm once the error is smaller than
1 (cid:2) 10
(cid:0)4. It takes 197:08s to reconstruct all the RGB channels. Experiment using general
Gaussian measurements A 2 Cm(cid:2)n could easily run out of memory on a personal computer
for problems of this size.

Figure 4: Experiment with real antenna pattern.

Experiments on signal AoA phase recovery for 5G communications. Finally  we demon-
strate the eﬀectiveness of the proposed method on a problem arising in 5G communication 
as we mentioned in the introduction. Fig. 4 (left) shows an antenna pattern a 2 C361 ob-
tained from Bell labs. We observe the modulus of the convolution of this pattern with the
signal of interest. For three diﬀerent types of signals with length n = 20  (1) x = e1   (2) x is
uniformly random generated from CSn(cid:0)1  (3) x = 1p
n 1  our result in Fig. 4 shows that we
can achieve almost perfect recovery.

9

5 Acknowledgement

This work was partially supported by the grants NSF CCF 1527809 and NSF IIS 1546411 
the grants from the European Unions Horizon 2020 research and innovation program un-
der grant agreement No. 646804-ERCCOGBNYQ  and the grant from the Israel Science
Foundation under grant no. 335/14. QQ thanks the generous support of the Microsoft
graduate research fellowship. We would like to thank Shan Zhong for the helpful discus-
sion for real applications and providing the antenna data for experiments  and we thank Ju
Sun and Han-wen Kuo for helpful discussion and input regarding the analysis of this work.

References
[1] Sercan Ö. Arik and Joseph M. Kahn. Direct-detection mode-division multiplexing in modal basis

using phase retrieval. Opt. Lett.  41(18):4265–4268  Sep 2016.

[2] T. Bendory  Y. C. Eldar  and N. Boumal. Non-convex phase retrieval from stft measurements.

IEEE Transactions on Information Theory  PP(99):1–1  2017.

[3] Emmanuel J. Candès  Yonina C. Eldar  Thomas Strohmer  and Vladislav Voroninski. Phase re-

trieval via matrix completion. SIAM Journal on Imaging Sciences  6(1)  2013.

[4] Emmanuel J. Candès  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval from coded diﬀrac-

tion patterns. Applied and Computational Harmonic Analysis  39(2):277–299  2015.

[5] Emmanuel J. Candès  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval via wirtinger ﬂow:

Theory and algorithms. Information Theory  IEEE Transactions on  61(4):1985–2007  April 2015.

[6] Emmanuel J Candès  Justin Romberg  and Terence Tao. Robust uncertainty principles: Exact
signal reconstruction from highly incomplete frequency information. IEEE Transactions on infor-
mation theory  52(2):489–509  2006.

[7] Emmanuel J Candes  Justin K Romberg  and Terence Tao. Stable signal recovery from incomplete
and inaccurate measurements. Communications on pure and applied mathematics  59(8):1207–1223 
2006.

[8] Emmanuel J. Candès  Thomas Strohmer  and Vladislav Voroninski. Phaselift: Exact and stable
signal recovery from magnitude measurements via convex programming. Communications on
Pure and Applied Mathematics  66(8):1241–1274  2013.

[9] Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections:

Universal encoding strategies? IEEE transactions on information theory  52(12):5406–5425  2006.

[10] Yuxin Chen and Emmanuel J. Candès. Solving random quadratic systems of equations is nearly

as easy as solving linear systems. arXiv preprint arXiv:1505.05114  2015.

[11] Victor De la Pena and Evarist Giné. Decoupling: from dependence to independence. Springer  1999.

[12] Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge Uni-

versity Press  2012.

[13] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing. Springer 

2013.

[14] Robert M Gagliardi and Sherman Karp. Optical communications. New York  Wiley-Interscience 

1976. 445 p.  1  1976.

[15] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points—online stochas-
tic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory 
pages 797–842  2015.

[16] R. W. Gerchberg and W. Owen Saxton. A practical algorithm for the determination of the phase

from image and diﬀraction plane pictures. Optik  35:237–246  1972.

[17] David Gross  Felix Krahmer  and Richard Kueng. A partial derandomization of phaselift using

spherical designs. arXiv preprint arXiv:1310.2267  2013.

10

[18] Felix Heide  Wolfgang Heidrich  and Gordon Wetzstein. Fast and ﬂexible convolutional sparse
coding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
5135–5143  2015.

[19] Kishore Jaganathan  Yonina C. Eldar  and Babak Hassibi. Phase retrieval: An overview of recent

developments. Chapter V  Optical Compressive Imaging  2016.

[20] Felix Krahmer  Shahar Mendelson  and Holger Rauhut. Suprema of chaos processes and the
restricted isometry property. Communications on Pure and Applied Mathematics  67(11):1877–1904 
2014.

[21] Felix Krahmer and Holger Rauhut. Structured random measurements in signal processing.

GAMM-Mitteilungen  37(2):217–238  2014.

[22] Ken Kreutz-Delgado. The complex gradient operator and the CR-calculus.

arXiv:0906.4835  2009.

arXiv preprint

[23] Stanislaw Kwapien. Decoupling inequalities for polynomial chaos. The Annals of Probability 

pages 1062–1071  1987.

[24] Antonio Mecozzi  Cristian Antonelli  and Mark Shtaif. Kramers–kronig coherent receiver. Optica 

3(11):1220–1227  Nov 2016.

[25] Praneeth Netrapalli  Prateek Jain  and Sujay Sanghavi. Phase retrieval using alternating mini-

mization. In Advances in Neural Information Processing Systems  pages 2796–2804  2013.

[26] Holger Rauhut. Compressive sensing and structured random matrices. Theoretical foundations

and numerical methods for sparse recovery  9:1–92  2010.

[27] Arash Shahmansoori  Gabriel E Garcia  Giuseppe Destino  Gonzalo Seco-Granados  and Henk
Wymeersch. 5g position and orientation estimation through millimeter wave mimo. In Globecom
Workshops (GC Wkshps)  2015 IEEE  pages 1–6. IEEE  2015.

[28] Yoav Shechtman  Yonina C. Eldar  Oren Cohen  Henry N. Chapman  Jianwei Miao  and
Mordechai Segev. Phase retrieval with application to optical imaging: A contemporary overview.
Signal Processing Magazine  IEEE  32(3):87–109  May 2015.

[29] Mahdi Soltanolkotabi. Algorithms and theory for clustering and nonconvex quadratic programming.

PhD thesis  Stanford University  2014.

[30] Mahdi Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sam-

ple complexity barriers via nonconvex optimization. CoRR  abs/1702.06175  2017.

[31] Milica Stojanovic  Josko A Catipovic  and John G Proakis. Phase-coherent digital communica-
tions for underwater acoustic channels. IEEE Journal of Oceanic Engineering  19(1):100–111  1994.
[32] Ju Sun  Qing Qu  and John Wright. Complete dictionary recovery over the sphere. arXiv preprint

arXiv:1504.06785  2015.

[33] Ju Sun  Qing Qu  and John Wright. When are nonconvex problems not scary? arXiv preprint

arXiv:1510.06096  2015.

[34] Ju Sun  Qing Qu  and John Wright. A geometric analysis of phase retreival. arXiv preprint

arXiv:1602.06664  2016.

[35] Irène Waldspurger. Phase retrieval with random gaussian sensing vectors by alternating projec-

tions. arXiv preprint arXiv:1609.03088  2016.

[36] Irène Waldspurger  Alexandre d`Aspremont  and Stéphane Mallat. Phase recovery  maxcut and

complex semideﬁnite programming. Mathematical Programming  149(1-2):47–81  2015.

[37] P. Walk  H. Becker  and P. Jung. OFDM channel estimation via phase retrieval. In Asilomar 2015 

2015.

[38] G. Wang  G. B. Giannakis  and Y. C. Eldar. Solving systems of random quadratic equations via

truncated amplitude ﬂow. IEEE Transactions on Information Theory  PP(99):1–1  2017.

[39] Huishuai Zhang and Yingbin Liang. Reshaped wirtinger ﬂow for solving quadratic systems of

equations. arXiv preprint arXiv:1605.07719  2016.

[40] Yuqian Zhang  Yenson Lau  Han-wen Kuo  Sky Cheung  Abhay Pasupathy  and John Wright. On
the global geometry of sphere-constrained sparse blind deconvolution. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  2017.

11

,Qing Qu
Yuqian Zhang
Yonina Eldar
John Wright
AmirEmad Ghassami
Negar Kiyavash
Biwei Huang
Kun Zhang