2014,Analysis of Variational Bayesian Latent Dirichlet Allocation: Weaker Sparsity Than MAP,Latent Dirichlet allocation (LDA) is a popular generative model of various objects such as texts and images  where an object is expressed as a mixture of latent topics. In this paper  we theoretically investigate variational Bayesian (VB) learning in LDA. More specifically  we analytically derive the leading term of the VB free energy under an asymptotic setup  and show that there exist transition thresholds in Dirichlet hyperparameters around which the sparsity-inducing behavior drastically changes. Then we further theoretically reveal the notable phenomenon that VB tends to induce weaker sparsity than MAP in the LDA model  which is opposed to other models. We experimentally demonstrate the practical validity of our asymptotic theory on real-world Last.FM music data.,Analysis of Variational Bayesian Latent Dirichlet

Allocation: Weaker Sparsity than MAP

Shinichi Nakajima

Berlin Big Data Center  TU Berlin

Berlin 10587 Germany

nakajima@tu-berlin.de

Issei Sato

University of Tokyo
Tokyo 113-0033 Japan

sato@r.dl.itc.u-tokyo.ac.jp

Masashi Sugiyama
University of Tokyo

Tokyo 113-0033  Japan

sugi@k.u-tokyo.ac.jp

Kazuho Watanabe

Toyohashi University of Technology

Aichi 441-8580 Japan

wkazuho@cs.tut.ac.jp

Hiroko Kobayashi
Nikon Corporation

Kanagawa 244-8533 Japan

hiroko.kobayashi@nikon.com

Abstract

Latent Dirichlet allocation (LDA) is a popular generative model of various objects
such as texts and images  where an object is expressed as a mixture of latent top-
ics. In this paper  we theoretically investigate variational Bayesian (VB) learning
in LDA. More speciﬁcally  we analytically derive the leading term of the VB free
energy under an asymptotic setup  and show that there exist transition thresholds
in Dirichlet hyperparameters around which the sparsity-inducing behavior drasti-
cally changes. Then we further theoretically reveal the notable phenomenon that
VB tends to induce weaker sparsity than MAP in the LDA model  which is op-
posed to other models. We experimentally demonstrate the practical validity of
our asymptotic theory on real-world Last.FM music data.

1 Introduction

Latent Dirichlet allocation (LDA) [5] is a generative model successfully used in various applications
such as text analysis [5]  image analysis [15]  genometrics [6  4]  human activity analysis [12] 
and collaborative ﬁltering [14  20]1. Given word occurrences of documents in a corpora  LDA
expresses each document as a mixture of multinomial distributions  each of which is expected to
capture a topic. The extracted topics provide bases in a low-dimensional feature space  in which
each document is compactly represented. This topic expression was shown to be useful for solving
various tasks including classiﬁcation [15]  retrieval [26]  and recommendation [14].
Since rigorous Bayesian inference is computationally intractable in the LDA model  various approx-
imation techniques such as variational Bayesian (VB) learning [3  7] are used. Previous theoretical
studies on VB learning revealed that VB tends to produce sparse solutions  e.g.  in mixture models
[24  25  13]  hidden Markov models [11]  Bayesian networks [23]  and fully-observed matrix fac-
torization [17]. Here  we mean by sparsity that VB exhibits the automatic relevance determination
1 For simplicity  we use the terminology in text analysis below. However  the range of application of our

theory given in this paper is not limited to texts.

1

(ARD) effect [19]  which automatically prunes irrelevant degrees of freedom under non-informative
or weakly sparse prior. Therefore  it is naturally expected that VB-LDA also produces a sparse solu-
tion (in terms of topics). However  it is often observed that VB-LDA does not generally give sparse
solutions.
In this paper  we attempt to clarify this gap by theoretically investigating the sparsity-inducing mech-
anism of VB-LDA. More speciﬁcally  we ﬁrst analytically derive the leading term of the VB free
energy in some asymptotic limits  and show that there exist transition thresholds in Dirichlet hy-
perparameters around which the sparsity-inducing behavior changes drastically. We then analyze
the behavior of MAP and its variants in a similar way  and show that the VB solution is less sparse
than the MAP solution in the LDA model. This phenomenon is completely opposite to other mod-
els such as mixture models [24  25  13]  hidden Markov models [11]  Bayesian networks [23]  and
fully-observed matrix factorization [17]  where VB tends to induce stronger sparsity than MAP. We
numerically demonstrate the practical validity of our asymptotic theory using artiﬁcial and real-
world Last.FM music data for collaborative ﬁltering  and further discuss the peculiarity of the LDA
model in terms of sparsity.
The free energy of VB-LDA was previously analyzed in [16]  which evaluated the advantage of
collapsed VB [21] over the original VB learning. However  that work focused on the difference
between VB and collapsed VB  and neither the absolute free energy nor the sparsity was investigated.
The update rules of VB was compared with those of MAP [2]. However  that work is based on
approximation  and rigorous analysis was not made. To the best of our knowledge  our paper is the
ﬁrst work that theoretically elucidates the sparsity-inducing mechanism of VB-LDA.

2 Formulation

In this section  we introduce the latent Dirichlet allocation model and variational Bayesian learning.

2.1 Latent Dirichlet Allocation

Suppose that we observe M documents  each of which consists of N (m) words. Each word is
included in a vocabulary with size L. We assume that each word is associated with one of the H
topics  which is not observed. We express the word occurrence by an L-dimensional indicator vector
w  where one of the entries is equal to one and the others are equal to zero. Similarly  we express
the topic occurrence as an H-dimensional indicator vector z. We deﬁne the following functions that
give the item numbers chosen by w and z  respectively:

´l(w) = l if wl = 1 and wl′ = 0 for l′ ̸= l 

´h(z) = h if zh = 1 and zh′ = 0 for h′ ̸= h.

In the latent Dirichlet allocation (LDA) model [5]  the word occurrence w(n m) of the n-th position
in the m-th document is assumed to follow the multinomial distribution:

p(w(n m)|Θ  B) =!L

(1)
where Θ ∈ [0  1]M×H and B ∈ [0  1]L×H are parameter matrices to be estimated. The rows of Θ
and the columns of B are probability mass vectors that sum up to one. We denote a column vector
of a matrix by a bold lowercase letter  and a row vector by a bold lowercase letter with a tilde  i.e. 

l=1"(BΘ⊤)l m#w(n m)

= (BΘ⊤)´l(w(n m)) m 

l

Θ = (θ1  . . .   θH) = ($θ1  . . .  $θM )⊤ 

B = (β1  . . .   βH) =%$β1  . . .  $βL&⊤.

distribution of the h-th topic.
Given the topic occurrence latent variable z(n m)  the complete likelihood is written as

With this notation $θm denotes the topic distribution of the m-th document  and βh denotes the word
p(w(n m)  z(n m)|Θ  B) = p(w(n m)|z(n m)  B)p(z(n m)|Θ) 
where p(w(n m)|z(n m)  B) =!L
m=1!H

  p(z(n m)|Θ) =!H

p(Θ|α) ∝!M

p(B|η) ∝!H

We assume the Dirichlet prior on Θ and B:

(2)
h=1(Θm h)z(n m)
.

h

h=1!L

l=1!H

h=1(Bl h)w(n m)

l

z(n m)
h

h=1(Θm h)α−1 

l=1(Bl h)η−1 

(3)

2

Outlined font

Figure 1: Graphical model of LDA.

where α and η are hyperparameters that control the prior sparsity. We can make α dependent on m
and/or h  and η dependent on l and/or h  and they can be estimated from observation. However  we
ﬁx those hyperparameters as given constants for simplicity in our analysis below. Figure 1 shows
the graphical model of LDA.

2.2 Variational Bayesian Learning

The Bayes posterior of LDA is written as

p(Θ  B {z(n m)}|{w(n m)} α η ) = p({w(n m)} {z(n m)}|Θ B)p(Θ|α)p(B|η)

p({w(n m)})

where p({w(n m)}) = ’ p({w(n m)} {z(n m)}|Θ  B)p(Θ|α)p(B|η)dΘdBd{z(n m)} is in-

tractable to compute and thus requires some approximation method.
the variational Bayesian (VB) approximation and investigate its behavior theoretically.
In the VB approximation  we assume that our approximate posterior is factorized as

In this paper  we focus on

 

(4)

and minimize the free energy:

q(Θ  B {z(n m)}) = q(Θ  B)q({z(n m)}) 
p({w(n m)} {z(n m)}|Θ B)p(Θ|α)p(B|η))q(Θ B {z(n m)})

q(Θ B {z(n m)})

(5)

(6)

 

F =(log

where ⟨·⟩p denotes the expectation over the distribution p. This amounts to ﬁnding the distribution
that is closest to the Bayes posterior (4) under the constraint (5). Using the variational method  we
can obtain the following stationary condition:

q(Θ) ∝ p(Θ|α) exp(log p({w(n m)} {z(n m)}|Θ  B))q(B)q({z(n m)})
q(B) ∝ p(B|η) exp(log p({w(n m)} {z(n m)}|Θ  B))q(Θ)q({z(n m)})

q({z(n m)}) ∝ exp(log p({w(n m)} {z(n m)}|Θ  B))q(Θ)q(B)

.

 

 

(7)

(8)

(9)

m=1!H

q(Θ) ∝!M

From this  we can conﬁrm that {q($θm)} and {q(βh)} follow the Dirichlet distribution and
{q(z(n m))} follows the multinomial distribution:
h=1(Θm h) ˘Θm h−1 
q({z(n m)}) =!M
m=1!N (m)
n=1 !H
˘Θm h = α ++N (m)
n=1 *z(n m)
exp!Ψ ( ˘Θm h)+"L
*z(n m)
h′=1 exp!Ψ ( ˘Θm h′ )+"L
"H

where  for ψ(·) denoting the Digamma function  the variational parameters satisfy
m=1+N (m)
n=1 w(n m)
˘Bl′ h))#
˘Bl′ h′))# .

h=1(*z(n m)
˘Bl h = η ++M
(Ψ ( ˘Bl h)−Ψ("L
(Ψ ( ˘Bl h′ )−Ψ("L

q(B) ∝!H

h=1!L

l=1(Bl h) ˘Bl h−1 

h

*z(n m)

2.3 Partially Bayesian Learning and MAP Estimation

l=1 w(n m)

l=1 w(n m)

l

)z(n m)

h

 

(10)

(11)

l′=1

(12)

(13)

 

h

h

=

l

 

l

l′=1

h

We can partially apply VB learning by approximating the posterior of Θ or B by the delta function.
This approach is called the partially Bayesian (PA) learning [18]  whose behavior was analyzed

3

and compared with VB in fully-observed matrix factorization. We call it PBA learning if Θ is
marginalized and B is point-estimated  and PBB learning if B is marginalized and Θ is point-
estimated. Note that the original VB algorithm for LDA proposed by [5] corresponds to PBA in
our terminology. We also analyze the behavior of MAP estimation  where both of Θ and B are
point-estimated. This corresponds to the probabilistic latent semantic analysis (pLSA) model [10] 
if we assume the ﬂat prior α = η = 1 [8].

3 Theoretical Analysis

In this section  we ﬁrst give an explicit form of the free energy in the LDA model. We then investigate
its asymptotic behavior for VB learning  and further conduct similar analyses to the PBA  PBB  and
MAP methods. Finally  we discuss the sparsity-inducing mechanism of these learning methods  and
the relation to previous theoretical studies.

3.1 Explicit Form of Free Energy

We ﬁrst express the free energy (6) as a function of the variational parameters ˘Θ and ˘B:

F = R + Q 

where

(14)

h′=1

l′=1

1

(15)

(16)

h=1

h′=1

˘Θm h′ ))

l′=1

q(Θ)q(B)

Γ (α)H

Γ (η)L

exp(Ψ ( ˘Θm h))

exp(Ψ ( ˘Bl h))

l=1

˘Bl h)
l=1 Γ ( ˘Bl h)

h=1

˘Θm h)
h=1 Γ ( ˘Θm h)

h=1" ˘Θm h − α#"Ψ ( ˘Θm h) − Ψ (+H
l=1" ˘Bl h − η#"Ψ ( ˘Bl h) − Ψ (+L

p(Θ|α)p(B|η))q(Θ B)
R =(log
m=1"log Γ ("H
=+M
$H
h=1"log Γ ("L
++H
$L
p({w(n m)} {z(n m)}|Θ B))q(Θ B {z(n m)})
Q =(log
q({z(n m)})
= −+M
m=1 N (m)+L
exp(Ψ ("H
Here  V ∈ RL×M is the empirical word distribution matrix with its entries given by Vl m =
N (m)+N (m)
occurrence latent variables by using the stationary condition (13).

Γ (Hα) ++H
Γ (Lη) ++L
l=1 Vl m log +H
. Note that we have eliminated the variational parameters {*z(n m)} for the topic

˘Bl′ h)##  
˘Bl′ h))- .

˘Θm h′)##

exp(Ψ ("L

3.2 Asymptotic Analysis of VB Solution
Below  we investigate the leading term of the free energy in the asymptotic limit when N ≡
minm N (m) → ∞. Unlike the previous analysis for latent variable models [24]  we do not as-
sume L  M ≪ N  but 1 ≪ L  M  N at this point. This amounts to considering the asymptotic
limit when L  M  N → ∞ with a ﬁxed mutual ratio  or equivalently  assuming L  M ∼ O(N ).
Throughout the paper  H is set at H = min(L  M ) (i.e.  the matrix BΘ⊤ can express any multino-
mial distribution). We assume that the word distribution matrix V is a sample from the multinomial
distribution with the true parameter U∗ ∈ RL×M whose rank is H∗ ∼ O(1)  i.e.  U∗ = B∗Θ∗⊤
where Θ∗ ∈ RM×H∗ and B∗ ∈ RL×H∗.2 We assume that α  η ∼ O(1).
The stationary condition (12) leads to the following lemma (the proof is given in Appendix A):

n=1 w(n m)

l

Lemma 1 Let *B*Θ⊤ = ⟨BΘ⊤⟩q(Θ B). Then  it holds that

m=1 N (m)+L
where Op(·) denotes the order in probability.

⟨(BΘ⊤ − *B*Θ⊤)2
Q = −+M

l m⟩q(Θ B) = Op(N−2) 

l=1 Vl m log(*B*Θ⊤)l m + Op(M ) 

2 More precisely  U∗ = B∗Θ∗⊤ + O(N−1) is sufﬁcient.

4

(17)

(18)

Eq.(17) implies the convergence of the posterior. Let

(19)

*J =+L

l=1+M

m=1 κ"(*B*Θ⊤)l m ̸= (B∗Θ∗⊤)l m + Op(N−1)#

where

the indicator function equal to one if the event is true  and zero otherwise. Then  Eq.(18) leads to
the following lemma:

be the number of entries of *B*Θ⊤ that do not converge to the true values. Here  we denote by κ(·)
Lemma 2 Q is minimized when *B*Θ⊤ = B∗Θ∗⊤ + Op(N−1)  and it holds that
Q = S + Op(*JN + M ) 
S = − log p({w(n m)} {z(n m)}|Θ∗  B∗) = −+M

Lemma 2 simply states that Q/N converges to the normalized entropy S/N of the true distribution
(which is the lowest achievable value with probability 1)  if and only if VB converges to the true

m=1 N (m)+L

l=1 Vl m log(B∗Θ∗)l m.

h=1 κ( 1

distribution (i.e.  *J = 0).
Let *H = +H
M+M
.M (h) = +M
*L(h) =+L

m=1 *Θm h ∼ Op(1)) be the number of topics used in the whole corpus 
m=1 κ(*Θm h ∼ Op(1)) be the number of documents that contain the h-th topic  and
l=1 κ(*Bl h ∼ Op(1)) be the number of words of which the h-th topic consist. We have

the following lemma (the proof is given in Appendix B):
Lemma 3 R is written as follows:

R =/M%Hα − 1

2& + *H%Lη − 1
2& −+%H
+ (H − *H)%Lη − 1

h=1".M (h)%α − 1
2& log L + Op(H(M + L)).

2& +*L(h)%η − 1

2&#0 log N

(20)

Since we assumed that the true matrices Θ∗ and B∗ are of the rank of H∗  *H = H∗ ∼ O(1) is
sufﬁcient for the VB posterior to converge to the true distribution. However  *H can be much larger
than H∗ with ⟨BΘ⊤⟩q(Θ B) unchanged because of the non-identiﬁability of matrix factorization—
duplicating topics with divided weights  for example  does not change the distribution.
Based on Lemma 2 and Lemma 3  we obtain the following theorem (the proof is given in Ap-
pendix C):

and

Theorem 1 In the limit when N → ∞ with L  M ∼ O(1)  it holds that *J = 0 with probability 1 
F = S +/M%Hα − 1
2&#0 log N

+ Op(1).

In the limit when N  M → ∞ with M

F = S +/M%Hα − 1

In the limit when N  L → ∞ with L

In the limit when N  L  M → ∞ with L

h=1".M (h)%α − 1

2& + *H%Lη − 1
2& +*L(h)%η − 1
N   L ∼ O(1)  it holds that *J = op(log N )  and
2&0 log N + op(N log N ).
2& −+%H
N   M ∼ O(1)  it holds that *J = op(log N )  and

2& −+%H
h=1.M (h)%α − 1
N ∼ O(1)  it holds that *J = op(N log N )  and

F = S + H(Mα + Lη) log N + op(N 2 log N ).

F = S + HLη log N + op(N log N ).

N   M

Since Eq.(17) was shown to hold  the predictive distribution converges to the true distribution if

*J = 0. Accordingly  Theorem 1 states that the consistency holds in the limit when N → ∞ with
L  M ∼ O(1).
Theorem 1 also implies that  in the asymptotic limits with small L ∼ O(1)  the leading term depends
on *H  meaning that it dominates the topic sparsity of the VB solution. We have the following
corollary (the proof is given in Appendix D):

5

Table 1: Sparsity thresholds of VB  PBA  PBB  and MAP methods (see Theorem 2). The ﬁrst four
columns show the thresholds (αsparse α dense)  of which the function forms depend on the range of
η  in the limit when N → ∞ with L  M ∼ O(1). A single value is shown if αsparse = αdense. The
last column shows the threshold αM→∞

N   L ∼ O(1).

in the limit when N  M → ∞ with M
!αsparse α dense"

2

0 <η ≤ 1
2L
2 −Lη
2 −

minh M∗(h)

1

1

1

η range

VB
PBA

PBB
MAP

1

1

2L <η ≤ 1
2 + Lη− 1
maxh M∗(h)
—

2

1 + Lη− 1

2

maxh M∗(h)
—

1
2 <η< 1
2 +

# 1
#1 +

αM→∞
0 <η< ∞

2

2

1
2

1
2

L−1

L−1

2 maxh M∗(h)   1
2   1

1 ≤ η< ∞
minh M∗(h)$
2 + Lη− 1
minh M∗(h)$
# 1
2 + L(η−1)
minh M∗(h)$
2 maxh M∗(h)   1 + Lη− 1
#1  1 + L(η−1)
minh M∗(h)$
l=1 κ(B∗l h ∼ O(1)). Consider
2L  the VB solution is sparse (i.e.  *H ≪

2 −

1

1

1

1

2−Lη

2 −

2  the VB solution is sparse if α< 1
2  the VB solution is sparse if α< 1

m=1 κ(Θ∗m h ∼ O(1)) and L∗(h) =+L
minh M∗(h)   and dense (i.e.  *H ≈ H) if α> 1

Corollary 1 Let M∗(h) =+M
the limit when N → ∞ with L  M ∼ O(1). When 0 <η ≤ 1
H = min(L  M )) if α< 1
1
2L <η ≤ 1
When η> 1
In the limit when N  M → ∞ with M
2.
α> 1
In the case when L  M ≪ N and in the case when L ≪ M  N  Corollary 1 provides information
on the sparsity of the VB solution  which will be compared with other methods in Section 3.3. On
the other hand  although we have successfully derived the leading term of the free energy also in the
case when M ≪ L  N and in the case when 1 ≪ L  M  N  it unfortunately provides no information
on sparsity of the solution.

maxh M∗(h)   and dense if α> 1
2 maxh M∗(h)   and dense if α> 1
N   L ∼ O(1)  the VB solution is sparse if α< 1

2−Lη
minh M∗(h) . When
2 + Lη− 1
maxh M∗(h) .
2 + Lη− 1
minh M∗(h) .
2  and dense if

2 + Lη− 1
L−1
2 +

2

2

2

3.3 Asymptotic Analysis of PBA  PBB  and MAP

By applying similar analysis to PBA learning  PBB learning  and MAP estimation  we can obtain
the following theorem (the proof is given in Appendix E):
Theorem 2 In the limit when N → ∞ with L  M ∼ O(1)  the solution is sparse if α<α sparse 
and dense if α>α dense. In the limit when N  M → ∞ with M
N   L ∼ O(1)  the solution is sparse if
α<α M→∞
A notable ﬁnding from Table 1 is that the threshold that determines the topic sparsity of PBB-LDA
2 larger than the threshold of VB-LDA. The same relation is observed
is (most of the case exactly) 1
between MAP-LDA and PBA-LDA. From these  we can conclude that point-estimating Θ  instead
2 in the LDA model. We will validate this observation
of integrating it out  increases the threshold by 1
by numerical experiments in Section 4.

. Here  αsparse  αdense  and αM→∞

  and dense if α>α M→∞

are given in Table 1.

3.4 Discussion

The above theoretical analysis (Thereom 2) showed that VB tends to induce weaker sparsity than
MAP in the LDA model3  i.e.  VB requires sparser prior (smaller α) than MAP to give a sparse
solution (mean of the posterior). This phenomenon is completely opposite to other models such
as mixture models [24  25  13]  hidden Markov models [11]  Bayesian networks [23]  and fully-
observed matrix factorization [17]  where VB tends to induce stronger sparsity than MAP. This
phenomenon might be partly explained as follows: In the case of mixture models  the sparsity
3 Although this tendency was previously pointed out [2] by using the approximation exp(ψ(n)) ≈ n − 1
2
and comparing the stationary condition  our result has ﬁrst clariﬁed the sparsity behavior of the solution based
on the asymptotic free energy analysis without using such an approximation.

6

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

α

0.5
1
(a) VB

 

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

α

0.5
1
(b) PBA

 

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

α

0.5
1
(c) PBB

 

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

α

0.5
1
(d) MAP

 

100
90
80
70
60
50
40
30
20
10
0

Figure 2: Estimated number *H of topics by (a) VB  (b) PBA  (c) PBB  and (d) MAP  for the artiﬁcial
data with L = 100  M = 100  H∗ = 20  and N ∼ 10000.

 

 

 

 

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

α

0.5
1
(a) VB

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

α

0.5
1
(b) PBA

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

α

0.5
1
(c) PBB

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

α

0.5
1
(d) MAP

100
90
80
70
60
50
40
30
20
10
0

Figure 3: Estimated number *H of topics for the Last.FM data with L = 100  M = 100  and
N ∼ 700.
threshold depends on the degree of freedom of a single component [24]. This is reasonable because
adding a single component increases the model complexity by this amount. Also  in the case of
LDA  adding a single topic requires additional L + 1 parameters. However  the added topic is
shared over M documents  which could discount the increased model complexity relative to the
increased data ﬁdelity. Corollary 1  which implies the dependency of the threshold for α on L and
M  might support this conjecture. However  the same applies to the matrix factorization  where VB
was shown to give a sparser solution than MAP [17]. Investigation on related models  e.g.  Poisson
MF [9]  would help us fully explain this phenomenon.
Technically  our theoretical analysis is based on the previous asymptotic studies on VB learning con-
ducted for latent variable models [24  25  13  11  23]. However  our analysis is not just a straight-
forward extension of those works to the LDA model. For example  the previous analysis either
implicitly [24] or explicitly [13] assumed the consistency of VB learning  while we also analyzed
the consistency of VB-LDA  and showed that the consistency does not always hold (see Theorem 1).
Moreover  we derived a general form of the asymptotic free energy  which can be applied to different
asymptotic limits. Speciﬁcally  the standard asymptotic theory requires a large number N of words
per document  compared to the number M of documents and the vocabulary size L. This may be
reasonable in some collaborative ﬁltering data such as the Last.FM data used in our experiments in
Section 4. However  L and/or M would be comparable to or larger than N in standard text analysis.
Our general form of the asymptotic free energy also allowed us to elucidate the behavior of the
VB free energy when L and/or M diverges with the same order as N. This attempt successfully
revealed the sparsity of the solution for the case when M diverges while L ∼ O(1). However  when
L diverges  we found that the leading term of the free energy does not contain interesting insight into
the sparsity of the solution. Higher-order asymptotic analysis will be necessary to further understand
the sparsity-inducing mechanism of the LDA model with large vocabulary.

4 Numerical Illustration

In this section  we conduct numerical experiments on artiﬁcial and real data for collaborative ﬁlter-
ing.
The artiﬁcial data were created as follows. We ﬁrst sample the true document matrix Θ∗ of size
m of Θ∗ follows
the Dirichlet distribution with α∗ = 1/H∗  while each column β∗h of B∗ follows the Dirichlet
distribution with η∗ = 1/L. The document length N (m) is sampled from the Poisson distribution
with its mean N. The word histogram N (m)vm for each document is sampled from the multinomial

M × H∗ and the true topic matrix B∗ of size L × H∗. We assume that each row$θ∗

7

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

0.5

α

1

 

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

 

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

 

100
90
80
70
60
50
40
30
20
10
0

η

1.2
1
0.8
0.6
0.4
0.2
0
 
0

 

100
90
80
70
60
50
40
30
20
10
0

0.5

α

1

0.5

α

1

0.5

α

1

(a) L = 100  M = 100

(b) L = 100  M = 1000

(c) L = 500  M = 100

(d) L = 500  M = 1000

Figure 4: Estimated number *H of topics by VB-LDA for the artiﬁcial data with H∗ = 20 and
N ∼ 10000. For the case when L = 500  M = 1000  the maximum estimated rank is limited to
100 for computational reason.

distribution with the parameter speciﬁed by the m-th row vector of B∗Θ∗⊤. Thus  we obtain the
L × M matrix V   which corresponds to the empirical word distribution over M documents.
As a real-world dataset  we used the Last.FM dataset.4 Last.FM is a well-known social music web
site  and the dataset includes the triple (“user ” “artist ” “Freq”) which was collected from the play-
lists of users in the community by using a plug-in in users’ media players. This triple means that
“user” played “artist” music “Freq” times  which indicates users’ preferred artists. A user and a
played artist are analogous to a document and a word  respectively. We randomly chose L artists
from the top 1000 frequent artists  and M users who live in the United States. To ﬁnd a better local
solution (which hopefully is close to the global solution)  we adopted a split and merge strategy [22] 
and chose the local solution giving the lowest free energy among different initialization schemes.

Figure 2 shows the estimated number *H of topics by different approximation methods  i.e.  VB 
PBA  PBB  and MAP  for the Artiﬁcial data with L = 100  M = 100  H∗ = 20  and N ∼ 10000.
We can clearly see that the sparsity threshold in PBB and MAP  where Θ is point-estimated  is
larger than that in VB and PBA  where Θ is marginalized. This result supports the statement by
Theorem 2. Figure 3 shows results on the Last.FM data with L = 100  M = 100 and N ∼ 700. We
see a similar tendency to Figure 2 except the region where η< 1 for PBA  in which our theory does
not predict the estimated number of topics.
Finally  we investigate how different asymptotic settings affect the topic sparsity. Figure 4 shows
the sparsity dependence on L and M for the artiﬁcial data. The graphs correspond to the four
cases mentioned in Theorem 1  i.e  (a) L  M ≪ N  (b) L ≪ N  M  (c) M ≪ N  L  and (d)
1 ≪ N  L  M. Corollary 1 explains the behavior in (a) and (b)  and further analysis is required to
explain the behavior in (c) and (d).

5 Conclusion

In this paper  we considered variational Bayesian (VB) learning in the latent Dirichlet allocation
(LDA) model and analytically derived the leading term of the asymptotic free energy. When the
vocabulary size is small  our result theoretically explains the phase-transition phenomenon. On the
other hand  when vocabulary size is as large as the number of words per document  the leading term
tells nothing about sparsity. We need more accurate analysis to clarify the sparsity in such cases.
Throughout the paper  we assumed that the hyperparameters α and η are pre-ﬁxed. However  α
would often be estimated for each topic h  which is one of the advantages of using the LDA model
in practice [5].
In the future work  we will extend the current line of analysis to the empirical
Bayesian setting where the hyperparameters are also learned  and further elucidate the behavior of
the LDA model.

Acknowledgments
The authors thank the reviewers for helpful comments. Shinichi Nakajima thanks the support
from Nikon Corporation  MEXT Kakenhi 23120004  and the Berlin Big Data Center project (FKZ
01IS14013A). Masashi Sugiyama thanks the support from the JST CREST program. Kazuho Watan-
abe thanks the support from JSPS Kakenhi 23700175 and 25120014.

4http://mtg.upf.edu/node/1671

8

References
[1] H. Alzer. On some inequalities for the Gamma and Psi functions. Mathematics of Computation 

66(217):373–389  1997.

[2] A. Asuncion  M. Welling  P. Smyth  and Y. W. Teh. On smoothing and inference for topic models. In

Proc. of UAI  pages 27–34  2009.

[3] H. Attias. Inferring parameters and structure of latent variable models by variational Bayes. In Proc. of

UAI  pages 21–30  1999.

[4] M. Bicego  P. Lovato  A. Ferrarini  and M. Delledonne. Biclustering of expression microarray data with

topic models. In Proc. of ICPR  pages 2728–2731  2010.

[5] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[6] X. Chen  X. Hu  X. Shen  and G. Rosen. Probabilistic topic modeling for genomic data interpretation. In
2010 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)  pages 149–152  2010.
In Advanced Mean Field

[7] Z. Ghahramani and M. J. Beal. Graphical models and variational methods.

Methods  pages 161–177. MIT Press  2001.

[8] M. Girolami and A. Kaban. On an equivalence between PLSI and LDA. In Proc. of SIGIR  pages 433–

434  2003.

[9] P. Gopalan  J. M. Hofman  and D. M. Blei. Scalable recommendation with Poisson factorization.

arXiv:1311.1704 [cs.IR]  2013.

[10] T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning  42:177–

196  2001.

[11] T. Hosino  K. Watanabe  and S. Watanabe. Stochastic complexity of hidden markov models on the varia-

tional Bayesian learning. IEICE Trans. on Information and Systems  J89-D(6):1279–1287  2006.

[12] T. Huynh  Mario F.  and B. Schiele. Discovery of activity patterns using topic models. In International

Conference on Ubiquitous Computing (UbiComp)  2008.

[13] D. Kaji  K. Watanabe  and S. Watanabe. Phase transition of variational Bayes learning in Bernoulli

mixture. Australian Journal of Intelligent Information Processing Systems  11(4):35–40  2010.

[14] R. Krestel  P. Fankhauser  and W. Nejdl. Latent dirichlet allocation for tag recommendation. In Proceed-

ings of the Third ACM Conference on Recommender Systems  pages 61–68  2009.

[15] F.-F. Li and P. Perona. A bayesian hierarchical model for learning natural scene categories. In Proc. of

CVPR  pages 524–531  2005.

[16] I. Mukherjee and D. M. Blei. Relative performance guarantees for approximate inference in latent Dirich-

let allocation. In Advances in NIPS  2008.

[17] S. Nakajima and M. Sugiyama. Theoretical analysis of Bayesian matrix factorization. Journal of Machine

Learning Research  12:2579–2644  2011.

[18] S. Nakajima  M. Sugiyama  and S. D. Babacan. On Bayesian PCA: Automatic dimensionality selection

and analytic solution. In Proc. of ICML  pages 497–504  2011.

[19] R. M. Neal. Bayesian Learning for Neural Networks. Springer  1996.
[20] S. Purushotham  Y. Liu  and C. C. J. Kuo. Collaborative topic regression with social matrix factorization

for recommendation systems. In Proc. of ICML  2012.

[21] Y. W. Teh  D. Newman  and M. Welling. A collapsed variational Bayesian inference algorithm for latent

Dirichlet allocation. In Advances in NIPS  2007.

[22] N. Ueda  R. Nakano  Z. Ghahramani  and G. E. Hinton. SMEM algorithm for mixture models. Neural

Computation  12(9):2109–2128  2000.

[23] K. Watanabe  M. Shiga  and S. Watanabe. Upper bound for variational free energy of Bayesian networks.

Machine Learning  75(2):199–215  2009.

[24] K. Watanabe and S. Watanabe. Stochastic complexities of Gaussian mixtures in variational Bayesian

approximation. Journal of Machine Learning Research  7:625–644  2006.

[25] K. Watanabe and S. Watanabe. Stochastic complexities of general mixture models in variational Bayesian

learning. Neural Networks  20(2):210–219  2007.

[26] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In Prof. of SIGIR  pages

178–185  2006.

9

,Shinichi Nakajima
Issei Sato
Masashi Sugiyama
Kazuho Watanabe
Hiroko Kobayashi
Vignesh Ganapathiraman
Xinhua Zhang
Yaoliang Yu
Junfeng Wen