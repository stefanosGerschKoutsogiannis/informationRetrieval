2016,Sparse Support Recovery with Non-smooth Loss Functions,In this paper  we study the support recovery guarantees of underdetermined sparse regression using the $\ell_1$-norm as a regularizer and a non-smooth loss function for data fidelity. More precisely  we focus in detail on the cases of $\ell_1$ and $\ell_\infty$ losses  and contrast them with the usual $\ell_2$ loss.While these losses are routinely used to account for either sparse ($\ell_1$ loss) or uniform ($\ell_\infty$ loss) noise models  a theoretical analysis of their performance is still lacking. In this article  we extend the existing theory from the smooth $\ell_2$ case to these non-smooth cases. We derive a sharp condition which ensures that the support of the vector to recover is stable to small additive noise in the observations  as long as the loss constraint size is tuned proportionally to the noise level. A distinctive feature of our theory is that it also explains what happens when the support is unstable. While the support is not stable anymore  we identify an "extended support" and show that this extended support is stable to small additive noise. To exemplify the usefulness of our theory  we give a detailed numerical analysis of the support stability/instability of compressed sensing recovery with these different losses. This highlights different parameter regimes  ranging from total support stability to progressively increasing support instability.,Sparse Support Recovery with
Non-smooth Loss Functions

Kévin Degraux

ISPGroup/ICTEAM  FNRS

Université catholique de Louvain
Louvain-la-Neuve  Belgium 1348
kevin.degraux@uclouvain.be

Gabriel Peyré
CNRS  DMA

École Normale Supérieure

Paris  France 75775

gabriel.peyre@ens.fr

Jalal M. Fadili

Normandie Univ  ENSICAEN 

CNRS  GREYC 

Caen  France 14050

Jalal.Fadili@ensicaen.fr

Laurent Jacques

ISPGroup/ICTEAM  FNRS

Université catholique de Louvain
Louvain-la-Neuve  Belgium 1348
laurent.jacques@uclouvain.be

Abstract

In this paper  we study the support recovery guarantees of underdetermined sparse
regression using the (cid:96)1-norm as a regularizer and a non-smooth loss function for
data ﬁdelity. More precisely  we focus in detail on the cases of (cid:96)1 and (cid:96)∞ losses 
and contrast them with the usual (cid:96)2 loss. While these losses are routinely used to
account for either sparse ((cid:96)1 loss) or uniform ((cid:96)∞ loss) noise models  a theoretical
analysis of their performance is still lacking. In this article  we extend the existing
theory from the smooth (cid:96)2 case to these non-smooth cases. We derive a sharp
condition which ensures that the support of the vector to recover is stable to small
additive noise in the observations  as long as the loss constraint size is tuned
proportionally to the noise level. A distinctive feature of our theory is that it also
explains what happens when the support is unstable. While the support is not stable
anymore  we identify an “extended support” and show that this extended support
is stable to small additive noise. To exemplify the usefulness of our theory  we
give a detailed numerical analysis of the support stability/instability of compressed
sensing recovery with these different losses. This highlights different parameter
regimes  ranging from total support stability to progressively increasing support
instability.

Introduction

1
1.1 Sparse Regularization

This paper studies sparse linear regression problems of the form

y = Φx0 + w 

where x0 ∈ Rn is the unknown vector to estimate  supposed to be non-zero and sparse  w ∈ Rm
is some additive noise and the design matrix Φm×n is in general rank deﬁcient corresponding to
a noisy underdetermined linear system of equations  i.e.  typically in the high-dimensional regime
where m (cid:28) n. This can also be understood as an inverse problem in imaging sciences  a particular
instance of which being the compressed sensing problem [3]  where the matrix Φ is drawn from some
appropriate random matrix ensemble.
In order to recover a sparse vector x0  a popular regularization is the (cid:96)1-norm  in which case we
consider the following constrained sparsity-promoting optimization problem

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

x∈Rn {||x||1 s.t. ||Φx − y||α (cid:54) τ}  
min

α(y))

(P τ

= ((cid:80)

def.

x

min

i |ui|α)1/α denotes the (cid:96)α-norm  and the constraint size τ (cid:62) 0
where for α ∈ [1  +∞]  ||u||α
should be adapted to the noise level. To avoid trivialities  through the paper  we assume that
α(y)) is feasible  which is of course the case if τ (cid:62) ||w||α. In the special situation where
problem (P τ
there is no noise  i.e.  w = 0  it makes sense to consider τ = 0 and solve the so-called Lasso [14] or
Basis-Pursuit problem [4]  which is independent of α  and reads
{||x||1 s.t. Φx = Φx0} .

(P 0(Φx0))
The case α = 2 corresponds to the usual (cid:96)2 loss function  which entails a smooth constraint set  and
has been studied in depth in the literature (see Section 1.6 for an overview). In contrast  the cases
α ∈ {1  +∞} correspond to very different setups  where the loss function || · ||α is polyhedral and
non-smooth. They are expected to lead to signiﬁcantly different estimation results and require to
develop novel theoretical results  which is the focus of this paper. The case α = 1 corresponds to
a “robust” loss function  and is important to cope with impulse noise or outliers contaminating the
data (see for instance [11  13  9]). At the extreme opposite  the case α = +∞ is typically used to
handle uniform noise such as in quantization (see for instance [10]). This paper studies the stability
of the support supp(xτ ) of minimizers xτ of (P τ
α(y)). In particular  we provide a sharp analysis
for the polyhedral cases α ∈ {1  +∞} that allows one to control the deviation of supp(xτ ) from
supp(x0) if ||w||α is not too large and τ is chosen proportionally to ||w||α. The general case is studied
numerically in a compressed sensing experiment where we compare supp(xτ ) and supp(x0) for
α ∈ [1  +∞].
1.2 Notations.

def.

def.

def.
= supp(x0) where supp(u)

= {i | ui (cid:54)= 0}. The saturation support
The support of x0 is noted I
of a vector is deﬁned as sat(u)
= {i | |ui| = ||u||∞}. The sub-differential of a convex function f
= R(C − C). A∗ is the
is denoted ∂f. The subspace parallel to a nonempty convex set C is par(C)
transpose of a matrix A and A+ is the Moore-Penrose pseudo-inverse of A. Id is the identity matrix
and δi the canonical vector of index i. For a subspace V ⊂ Rn  PV is the orthogonal projector onto
V . For sets of indices S and I  we denote ΦS I the submatrix of Φ restricted to the rows indexed
by S and the columns indexed by I. When all rows or all columns are kept  a dot replaces the
corresponding index set (e.g.  Φ· I). We denote Φ∗
= (ΦS I )∗  i.e. the transposition is applied after
the restriction.

S I

def.

def.

1.3 Dual Certiﬁcates
Before diving into our theoretical contributions  we ﬁrst give important deﬁnitions. Let Dx0 be the
set of dual certiﬁcates (see  e.g.  [17]) deﬁned by

∗

def.

Dx0

= {p ∈ Rm | Φ

(1)
The ﬁrst order optimality condition (see  e.g.  [12]) states that x0 is a solution of (P 0(Φx0)) if and
only if Dx0 (cid:54)= ∅. Assuming this is the case  our main theoretical ﬁnding (Theorem 1) states that the
stability (and instability) of the support of x0 is characterized by the following speciﬁc subset of
certiﬁcates

∗
· I p = sign(x0 I ) ||Φ

∗

p ∈ ∂||x0||1} =(cid:8)p ∈ Rm (cid:12)(cid:12) Φ

p||∞ (cid:54) 1(cid:9) .

pβ ∈ Argmin
p∈Dx0

||p||β where

1

α + 1

β = 1.

(2)

We call such a certiﬁcate pβ a minimum norm certiﬁcate. Note that for 1 < α < +∞  this pβ is
actually unique but that for α ∈ {1 ∞} it might not be the case.
Associated to such a minimal norm certiﬁcate  we deﬁne the extended support as

J

∗
def.
= sat(Φ

∗
pβ) = {i ∈ {1  . . .   n} | |(Φ

(3)
When the certiﬁcate pβ from which J is computed is unclear from the context  we write it explicitly
as an index Jpβ . Note that  from the deﬁnition of Dx0  one always has I ⊆ J. Intuitively  J indicates
the set of indexes that will be activated in the signal estimate when a small noise w is added to the
observation  and thus the situation when I = J corresponds to the case where the support of x0 is
stable.

pβ)i| = 1} .

2

par(∂||p1||1)
∂||p1||1
e1
p1

•

•

T1

•0
{p | ||p||1 (cid:54) 1}

Fig. 1: Model tangent subspace Tβ in R2 for (α  β) = (∞  1).

1.4 Lagrange multipliers and restricted injectivity conditions
In the case of noiseless observations (w = 0) and when τ > 0  the following general lemma
whose proof can be found in Section 2 associate to a given dual certiﬁcate pβ an explicit solution of
α(Φx0)). This formula depends on a so-called Lagrange multiplier vector vβ ∈ Rn  which will
(P τ
be instrumental to state our main contribution (Theorem 1). Note that this lemma is valid for any
α ∈ [1 ∞]. Even though this goes beyond the scope of our main result  one can use the same lemma
for an arbitrary (cid:96)α-norm for α ∈ [1 ∞] (see Section 3) or for even more general loss functions.
Lemma 1 (Noiseless solution). We assume that x0 is identiﬁable  i.e. it is a solution to (P 0(Φx0)) 
and consider τ > 0. Then there exists a vβ ∈ Rn supported on J such that
∗
·  ˜J pβ

and − sign(vβ  ˜J ) = Φ

||vβ I||∞   with x = mini∈I |x0 I|  then a

α(Φx0)) with support equal to J is given by
¯xτ J = x0 J − τ vβ J .

where we denoted ˜J
solution ¯xτ of (P τ
Moreover  its entries have the same sign as those of x0 on its support I  i.e.  sign(¯xτ I ) = sign(x0 I ).
An important question that arises is whether vβ can be computed explicitly. For this  let us deﬁne the
= par(∂||pβ||β)⊥  i.e.  Tβ is the orthogonal to the subspace parallel to
model tangent subspace Tβ
∂||pβ||β  which uniquely deﬁnes the model vector  eβ
= PTβ ∂||pβ||β  as shown on Figure 1 (see [17]
for details). Using this notation  vβ J is uniquely deﬁned and expressed in closed-form as
(4)

vβ J = (PTβ Φ· J )+eβ

Φ· J vβ J ∈ ∂||pβ||β
= J\I. If τ is such that 0 < τ <

def.

def.

def.

x

if and only if the following restricted injectivity condition holds

def.
= supp(p1) is used.

Ker(PTβ Φ· J ) = {0}.

(INJα)
For the special case (α  β) = (∞  1)  the following lemma  proved in Section 2  gives easily veriﬁable
sufﬁcient conditions  which ensure that (INJ∞) holds. The notation S
Lemma 2 (Restricted injectivity for α = ∞). Assume x0 is identiﬁable and ΦS J has full rank. If

(cid:48)
∗
S(cid:48) J ) ∀S
sJ /∈ Im(Φ
(cid:48)
qS /∈ Im(ΦS J(cid:48)) ∀J
|J|  and qS = sign(p1 S) ∈ {−1  1}

⊆ {1  . . .   m} 
⊆ {1  . . .   n} 
where sJ = Φ∗
|S|  then  |S| = |J| and ΦS J is
invertible  i.e.  since PT1 Φ· J = Id· SΦS J  (INJ∞) holds.
Remark 1. If Φ is randomly drawn from a continuous distribution with i.i.d. entries  e.g.  Gaussian 
then as soon as x0 is identiﬁable  the conditions of Lemma 2 hold with probability 1 over the
distribution of Φ.
(cid:20)
For (α  β) = (1 ∞)  we deﬁne Z

| < |J| and
|S
(cid:48)
| < |S| 
|J

· J p1 ∈ {−1  1}

(cid:21)

(cid:48)

def.
= sat(p∞) 
sign(p∗

IdZc ·
∞ Z)IdZ ·

def.
=

Θ

and (cid:101)Φ

def.
= ΘΦ· J .

|J| and(cid:101)Φ is invertible. In that case  (INJ1) holds as Ker(PT∞Φ· J ) = Ker((cid:101)Φ). Table 1 summarizes

Following similar reasoning as in Lemma 2 and Remark 1  we can reasonably assume that |Z c| + 1 =
for the three speciﬁc cases α ∈ {1  2  +∞} the quantities introduced here.

Table 1: Model tangent subspace  restricted injectivity condition and Lagrange multipliers.
α

(INJα)

vβ J

(PTβ Φ· J )+

Tβ
Rm

2
∞

1

{u | supp(u) = S }

{u | uZ = ρ sign(p∞ Z )  ρ ∈ R}

Ker(Φ· J ) = {0}
Ker(ΦS J ) = {0}
Ker((cid:101)Φ) = {0}

3

Φ+
· J

Φ−1
S J IdS ·

(cid:101)Φ−1Θ

Φ−1

S J sign(p1 S)

Φ+
· J

p2
||p2||2

(cid:101)Φ−1δ|J|

Fig. 2: (best observed in color) Simulated compressed sensing example showing xτ (above) for
increasing values of τ and random noise w respecting the hypothesis of Theorem 1 and Φ∗pβ (bellow)
which predicts the support of xτ when τ > 0.
1.5 Main result

Our main contribution is Theorem 1 below. A similar result is known to hold in the case of the smooth
(cid:96)2 loss (α = 2  see Section 1.6). Our paper extends it to the more challenging case of non-smooth
losses α ∈ {1  +∞}. The proof for α = +∞ is detailed in Section 2. It is important to emphasize
that the proof strategy is signiﬁcantly different from the classical approach developed for α = 2 
mainly because of the lack of smoothness of the loss function. The proof for α = 1 follows a similar
structure  and due to space limitation  it can be found in the supplementary material.
Theorem 1. Let α ∈ {1  2  +∞}. Suppose that x0 is identiﬁable  and let pβ be a minimal norm
certiﬁcate (see (2)) with associated extended support J (see (3)). Suppose that the restricted injectivity
condition (INJα) is satisﬁed so that vβ J can be explicitly computed (see (4)). Then there exist
constants c1  c2 > 0 depending only on Φ and pβ such that  for any (w  τ ) satisfying

||w||α < c1τ

and

τ (cid:54) c2x where x

def.
= min

i∈I |x0 I| 

a solution xτ of (P τ

def.

α(Φx0 + w)) with support equal to J is given by
= x0 J + (PTβ Φ· J )+w − τ vβ J .

xτ J

(5)

(6)

def.

This theorem shows that if the signal-to-noise ratio is large enough and τ is chosen in proportion
to the noise level ||w||α   then there is a solution supported exactly in the extended support J. Note
in particular that this solution (6) has the correct sign pattern sign(xτ I ) = sign(x0 I )  but might
exhibit outliers if ˜J
= J\I (cid:54)= ∅. The special case I = J characterizes the exact support stability
(“sparsistency”)  and in the case α = 2  the assumptions involving the dual certiﬁcate correspond to a
condition often referred to as “irrepresentable condition” in the literature (see Section 1.6).
In Section 3  we propose numerical simulations to illustrate our theoretical ﬁndings on a compressed
sensing (CS) scenario. Using Theorem 1  we are able to numerically assess the degree of support
instability of CS recovery using (cid:96)α ﬁdelity. As a prelude to shed light on this result  we show on
Figure 2  a smaller simulated CS example for (α  β) = (∞  1). The parameters are n = 20  m = 10
and |I| = 4 and x0 and Φ are generated as in the experiment of Section 3 and we use CVX/MOSEK
[8  7] at best precision to solve the optimization programs. First  we observe that x0 is indeed
identiﬁable by solving (P 0(Φx0)). Then we solve (2) to compute pβ and predict the extended support
J. Finally  we add uniformly distributed noise w with wi ∼i.i.d. U(−δ  δ) and δ chosen appropriately
to ensure that the hypotheses hold and we solve (P τ
α(y)). Observe that as we increase τ  new non-zero
entries appear in xτ but because w and τ are small enough  as predicted  we have supp(xτ ) = J.
Let us now comment on the limitations of our analysis. First  this result does not trivially extend to
the general case α ∈ [1  +∞] as there is  in general  no simple closed form for xτ . A generalization
would require more material and is out of the scope of this paper. Nevertheless  our simulations in
Section 3 stand for arbitrary α ∈ [1  +∞] which is why the general formulation was presented.
Second  larger noise regime  though interesting  is also out of the scope. Let us note that no other
results in the literature (even for (cid:96)2) provide any insight about sparsistency in the large noise regime.
In that case  we are only able to provide bounds on the distance between x0 and the recovered vector
but this is the subject of a forthcoming paper.
Finally our work is agnostic with respect to the noise models. Being able to distinguish between
different noise models would require further analysis of the constant involved and some additional
constraint on Φ. However  our result is a big step towards the understanding of the solutions behavior
and can be used in this analysis.

4

...xτΦ∗pβx0xτ1xτ2Jc˜JI1100−1−11.6 Relation to Prior Works

1

To the best of our knowledge  Theorem 1 is the ﬁrst to study the support stability guarantees by
minimizing the (cid:96)1-norm with non-smooth loss function  and in particular here the (cid:96)1 and (cid:96)∞ losses.
The smooth case α = 2 is however much more studied  and in particular  the associated support
stability results we state here are now well understood. Note that most of the corresponding literature
studies in general the penalized form  i.e.  minx
2||Φx − y||2 + λ||x||1 instead of our constrained
α(y)). In the case α = 2  since the loss is smooth  this distinction is minor and the
formulation (P τ
proof is almost the same for both settings. However  for α ∈ {1  +∞}  it is crucial to study the
constrained problems to be able to state our results. The support stability (also called “sparsistency” 
α(y)) in the case α = 2 has been proved
corresponding to the special case I = J of our result) of (P τ
by several authors in slightly different setups. In the signal processing literature  this result can be
traced back to the early work of J-J. Fuchs [6] who showed Theorem 1 when α = 2 and I = J. In
the statistics literature  sparsistency is also proved in [19] in the case where Φ is random  the result of
support stability being then claimed with high probability. The condition that I = J  i.e.  that the
minimal norm certiﬁcate pβ (for α = β = 2) is saturating only on the support  is often coined the
“irrepresentable condition” in the statistics and machine learning literature. These results have been
extended recently in [5] to the case where the support I is not stable  i.e. I (cid:40) J. One could also cite
[15]  whose results are somewhat connected but are restricted to the (cid:96)2 loss and do not hold in our
case. Note that “sparsistency”-like results have been proved for many “low-complexity” regularizers
beyond the (cid:96)1-norm. Let us quote among others: the group-lasso [1]  the nuclear norm [2]  the total
variation [16] and a very general class of “partly-smooth” regularizers [17]. Let us also point out
that one of the main sources of application of these results is the analysis of the performance of
compressed sensing problems  where the randomness of Φ allows to derive sharp sample complexity
bounds as a function of the sparsity of x0 and n  see for instance [18]. Let us also stress that these
support recovery results are different from those obtained using tools such as the Restricted Isometry
Property and alike (see for instance [3]) in many respects. For instance  the guarantees they provide
are uniform (i.e.  they hold for any sparse enough vector x0)  though they usually lead to quite
pessimistic worst-case bounds  and the stability is measured in (cid:96)2 sense.

2 Proof of Theorem 1

In this section  we prove the main result of this paper. For the sake of brevity  when part of the proof
will become speciﬁc to a particular choice of α  we will only write the details for α = ∞. The details
of the proof for α = 1 can be found in the supplementary material.
It can be shown that the Fenchel-Rockafellar dual problem to (P τ

α(y)) is [12]

β(y))
From the corresponding (primal-dual) extremality relations  one can deduce that (ˆx  ˆp) is an optimal
primal-dual Kuhn-Tucker pair if  and only if 

p∈Rm {−(cid:104)y  p(cid:105) + τ||p||β s.t. ||Φ
min

p||∞ (cid:54) 1} .

(Dτ

∗

Φ

∗
·  ˆI ˆp = sign(ˆx ˆI )
y − Φˆx

∗

and

||Φ

ˆp||∞ (cid:54) 1.

(7)

where ˆI = supp(ˆx)  and

(8)
The ﬁrst relationship comes from the sub-differential of the (cid:96)1 regularization term while the second is
speciﬁc to a particular choice of α for the (cid:96)α-norm data ﬁdelity constraint. We start by proving the
Lemma 1 and Lemma 2.
Proof of Lemma 1 Let us rewrite the problem (2) by introducing the auxiliary variable η = Φ∗p
as

∈ ∂||ˆp||β.

τ

∗
p η {||p||β + ιB∞ (η) | η = Φ
min

p  ηI = sign(x0 I )}  

(9)

where ιB∞ is the indicator function of the unit (cid:96)∞ ball. Deﬁne the Lagrange multipliers v and zI and
the associated Lagrangian function

L(p  η  v  zI ) = ||p||β + ιB∞ (η) + (cid:104)v  η − Φ

p(cid:105) + (cid:104)zI   ηI − sign(x0 I )(cid:105).

Deﬁning zI c = 0  the ﬁrst order optimality conditions (generalized KKT conditions) for p and η read

∗

Φv ∈ ∂||p||β

and − v − z ∈ ∂ιB∞ (η) 

5

From the normal cone of the B∞ at η on its boundary  the second condition is

−v − z ∈ {u | uJ c = 0  sign(uJ ) = ηJ }  

where J = sat(η) = sat(Φ∗p). Since I ⊆ J  v is supported on J. Moreover  on ˜J = J\I  we
have − sign(v ˜J ) = η ˜J. As pβ is a solution to (9)  we can deﬁne a corresponding vector of Lagrange
multipliers vβ supported on J such that − sign(vβ  ˜J ) = Φ∗
α(y))  i.e.  it obeys (7) and
To prove the lemma  it remains to show that ¯xτ is indeed a solution to (P τ
(8) for some dual variable ˆp. We will show that this is the case with ˆp = pβ. Observe that pβ (cid:54)= 0 as
otherwise  it would mean that x0 = 0  which contradicts our initial assumption of non-zero x0. We
can then directly see that (8) is satisﬁed. Indeed  noting y0

·  ˜J pβ and Φ· J vβ J ∈ ∂||pβ||β.

def.
= Φx0  we can write

y0 − Φ· J ¯xτ J = τ Φ· J vβ J ∈ τ ∂||pβ||β.
By deﬁnition of pβ  we have ||Φ∗pβ||∞ (cid:54) 1. In addition  it must satisfy Φ∗
I  the condition is always satisﬁed since − sign(vβ  ˜J ) = Φ∗
sign(x0 I ). The condition on τ is thus |x0 i| > τ |vβ i|  ∀i ∈ I  or equivalently  τ <
Proof of Lemma 2 As established by Lemma 1  the existence of p1 and of v1 are implied by the
identiﬁability of x0. We have the following 

·  ˜J pβ. On I  we know that Φ∗
||vβ I||∞ .

· J pβ = sign(¯xτ J ). Outside
· I pβ =

x

∗
S J is surjective ⇔ |S| (cid:62) |J|
∃p1 ⇒ ∃pS  Φ
∃v1 ⇒ ∃vJ   ΦS J vJ = qS ⇔ ΦS J is surjective ⇔ |J| (cid:62) |S| 

∗
S J pS = sJ ⇔ Φ

S J is full rank  |S| (cid:62) |J| is equivalent to surjectivity.
S J is not surjective so that |S| < |J|  then sJ /∈ Im(Φ∗
S J ) and the over-determined system
S J is
∗ †
∗ †
S J is any right-inverse of Φ∗
S J sJ as a solution where Φ
S J. This

To clarify  we detail the ﬁrst line. Since Φ∗
Assume Φ∗
Φ∗
S J pS = sJ has no solution in pS  which contradicts the existence of p1. Now assume Φ∗
surjective  then we can take pS = Φ
proves that ΦS J is invertible.
We are now ready to prove the main result in the particular case α = ∞.
Proof of Theorem 1 (α = ∞) Our proof consists in constructing a vector supported on J  obeying
the implicit relationship (6) and which is indeed a solution to (P τ∞(Φx0 + w)) for an appropriate
regime of the parameters (τ ||w||α). Note that we assume that the hypothesis of Lemma 2 on Φ holds
and in particular  ΦS J is invertible. When (α  β) = (∞  1)  the ﬁrst order condition (8)  which holds
for any optimal primal-dual pair (x  p)  reads  with Sp
(10)
One should then look for a candidate primal-dual pair (ˆx  ˆp) such that supp(ˆx) = J and satisfying
(11)
yS ˆp − ΦS ˆp J ˆxJ = τ sign(ˆpS ˆp ).
We now need to show that the ﬁrst order conditions (7) and (10) hold for some p = ˆp solution of
the “perturbed” dual problem (Dτ
1 (Φx0 + w)) with x = ˆx. Actually  we will show that under the
conditions of the theorem  this holds for ˆp = p1  i.e.  p1 is solution of (Dτ

ySp − ΦSp ·x = τ sign(pSp )

def.
= supp(p) 
and

||y − Φx||∞ (cid:54) τ.

1 (Φx0 + w)) so that

−1
S J yS − τ Φ
Let us start by proving the equality part of (7)  Φ∗
ˆpS = p1 S if and only if sign(ˆxJ ) = Φ∗

ˆxJ = Φ

−1
S J sign(p1 S) = x0 J + Φ

−1
S J wS − τ v1 J .

S J ˆpS = sign(ˆxJ ). Since ΦS J is invertible  we have

S J p1 S. Noting IdI J the restriction from J to I  we have

sign

as soon as

It is sufﬁcient to require

(cid:16)
(cid:12)(cid:12)(cid:12)(cid:16)

−1
S J wS − τ v1 I

(cid:17)
(cid:12)(cid:12)(cid:12) < |x0 I|

i − τ v1 i

x0 I + IdI J Φ

(cid:17)

−1
S J wS

Φ

= sign (x0 I )

∀i ∈ I.

−1
S J wS − τ v1 I||∞ < x
||IdI J Φ
−1
S J||∞ ∞||w||∞ + τ||v1 I||∞ < x 

||Φ

with x = mini∈I |x0 I|. Injecting the fact that ||w||∞ < c1τ (the value of c1 will be derived later)  we
get the condition

6

with b = ||Φ

−1
S J||∞ ∞ and ν = ||v1||∞ (cid:54) b. Rearranging the terms  we obtain

τ (bc1 + ν) (cid:54) x 

τ (cid:54)

x

bc1 + ν

= c2x 

which guarantees sign(ˆxI ) = sign(x0 I ). Outside I  deﬁning Id ˜J J as the restriction from J to ˜J 
we must have

(cid:16)
(cid:12)(cid:12)(cid:12)(cid:12) < τ|v1 j| ∀j ∈ ˜J.
From Lemma 1  we know that − sign(v1  ˜J ) = Φ∗

∗
S  ˜J p1 S = sign

−1
S J wS − τ v1  ˜J

(cid:12)(cid:12)(cid:12)(cid:12)(cid:16)

−1
S J wS

(cid:17)

Φ

Φ

j

(cid:17)

Id ˜J J Φ
S  ˜J p1 S  so that the condition is satisﬁed as soon as

.

Noting v = minj∈ ˜J |v1 j|  we get the sufﬁcient condition for (7) 

||Φ

−1
S J wS||∞ < τ v 
v
||w||∞ < τ
.
b

(c1a)

We can now verify (10). From (11) we see that the equality part is satisﬁed on S. Outside S  we have

ySc − ΦSc · ˆx = wSc − ΦSc J Φ

−1
S J wS + τ ΦSc J v1 J  

which must be smaller than τ  i.e. 

It is thus sufﬁcient to have

||wSc − ΦSc J Φ

−1
S J wS + τ ΦSc J v1 J||∞ (cid:54) τ.
−1
S J||∞ ∞)||w||∞ + τ µ (cid:54) τ 

(1 + ||ΦSc J Φ

with µ

def.

= ||ΦSc J v1 J||∞. Noting a = ||ΦSc J Φ

−1
S J||∞ ∞  we get

||w||∞ (cid:54) 1 − µ

1 + a

τ.

(c1b)

(c1a) and (c1b) together give the value of c1. This ensures that the inequality part of (10) is satisﬁed
for ˆx and with that  that ˆx is solution to (P τ∞(Φx0 + w)) and p1 solution to (Dτ
1 (Φx0 + w))  which
concludes the proof.
Remark 2. From Lemma 1  we know that in all generality µ (cid:54) 1. If the inequality was saturated  it
would mean that c1 = 0 and no noise would be allowed. Fortunately  it is easy to prove that under a
mild assumption on Φ  similar to the one of Lemma 2 (which holds with probability 1 for Gaussian
matrices)  the inequality is strict  i.e.  µ < 1.

3 Numerical experiments

In order to illustrate support stability in Lemma 1 and Theorem 1  we address numerically the
problem of comparing supp(xτ ) and supp(x0) in a compressed sensing setting. Theorem 1 shows
that supp(xτ ) does not depend on w (as long as it is small enough); simulations thus do not involve
noise. All computations are done in Matlab  using CVX [8  7]  with the MOSEK solver at “best”
precision setting to solve the convex problems. We set n = 1000  m = 900 and generate 200 times a
random sensing matrix Φ ∈ Rm×n with Φij ∼i.i.d N (0  1). For each sensing matrix  we generate
60 different k-sparse vectors x0 with support I where k
= |I| varies from 10 to 600. The non-zero
entries of x0 are randomly picked in {±1} with equal probability. Note that this choice does not
impact the result because the deﬁnition of Jpβ only depends on sign(x0) (see (1)). It will only affect
the bounds in (5). For each case  we verify that x0 is identiﬁable and for α ∈ {1  2 ∞} (which
correspond to β ∈ {∞  2  1})  we compute the minimum (cid:96)β-norm certiﬁcate pβ  solution to (2) and
= sat(Φ∗pβ)\I. It is important to emphasize that there is no
in particular  the support excess ˜Jpβ
noise in these simulations. As long as the hypotheses of the theorem are satisﬁed  we can predict that
supp(xτ ) = Jpβ ⊂ I without actually computing xτ   or choosing τ  or generating w.

def.

def.

7

Fig. 3: (best observed in color) Sweep over se ∈ {0  10  ...} of the empirical probability as a function
of the sparsity k that x0 is identiﬁable and | ˜Jp∞| (cid:54) se (left)  | ˜Jp2| (cid:54) se (middle) or | ˜Jp1| (cid:54) se
(right). The bluest corresponds to se = 0 and the redest to the maximal empirical value of | ˜Jpβ|.

α ∈ [0  1] of the empirical probability as a function of k

Fig. 4: (best observed in color) Sweep over 1
that x0 is identiﬁable and | ˜Jpβ| (cid:54) se for three values of se. The dotted red line indicates α = 2.
We deﬁne a support excess threshold se ∈ N varying from 0 to ∞. On Figure 3 we plot the probability
that x0 is identiﬁable and | ˜Jpβ|  the cardinality of the predicted support excess  is smaller or equal
to se. It is interesting to note that the probability that | ˜Jp1| = 0 (the bluest horizontal curve on the
right plot) is 0  which means that even for extreme sparsity (k = 10) and a relatively high m/n
rate of 0.9  the support is never predicted as perfectly stable for α = ∞ in this experiment. We can
observe as a rule of thumb  that a support excess of | ˜Jp1| ≈ k is much more likely. In comparison  (cid:96)2
recovery provides a much more likely perfect support stability for k not too large and the expected
size of ˜Jp2 increases slower with k. Finally  we can comment that the support stability with (cid:96)1 data
ﬁdelity is in between. It is possible to recover the support perfectly but the requirement on k is a bit
more restrictive than with (cid:96)2 ﬁdelity.
As previously noted  Lemma 1 and its proof remain valid for smooth loss functions such as the
(cid:96)α-norm when α ∈ (1 ∞). Therefore  it makes sense to compare the results with the ones obtained
for α ∈ (1 ∞) . On Figure 4 we display the result of the same experiment but with 1/α as the
vertical axis. To realize the ﬁgure  we compute pβ and ˜Jpβ for β corresponding to 41 equispaced
values of 1/α ∈ [0  1]. The probability that | ˜Jpβ| (cid:54) se is represented by the color intensity. The three
different plots correspond to three different values for se. On this ﬁgure  the yellow to blue transition
can be interpreted as the maximal k to ensure  with high probability  that | ˜Jpβ| does not exceeds se. It
is always (for all se) further to the right at α = 2. It means that the (cid:96)2 data ﬁdelity constraint provides
the highest support stability. Interestingly  we can observe that this maximal k decreases gracefully
as α moves away from 2 in one way or the other. Finally  as already observed on Figure 3  we see
that  especially when se is small  the (cid:96)1 loss function has a small advantage over the (cid:96)∞ loss.
4 Conclusion
In this paper  we provided sharp theoretical guarantees for stable support recovery under small enough
noise by (cid:96)1 minimization with non-smooth loss functions. Unlike the classical setting where the data
loss is smooth  our analysis reveals the difﬁculties arising from non-smoothness  which necessitated a
novel proof strategy. Though we focused here on the case of (cid:96)α data loss functions  for α ∈ {1  2 ∞} 
our analysis can be extended to more general non-smooth losses  including coercive gauges. This
will be our next milestone.

Acknowledgments

KD and LJ are funded by the Belgian F.R.S.-FNRS. JF is partly supported by Institut Universitaire de
France. GP is supported by the European Research Council (ERC project SIGMA-Vision).

8

kkkP[|˜Jp∞|6se]P[|˜Jp2|6se]P[|˜Jp1|6se]ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ∞ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ2ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1ℓ1000111200200200400400400600600600kkk1αse=0se=50se=15010.50125125125250250250375375375500500500References
[1] F.R. Bach. Consistency of the group Lasso and multiple kernel learning. Journal of Machine Learning

Research  9:1179–1225  2008.

[2] F.R. Bach. Consistency of trace norm minimization. Journal of Machine Learning Research  9:1019–1048 

2008.

[3] E. J. Candès  J. K. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate measure-

ments. Communications on pure and . . .   40698(8):1–15  aug 2006.

[4] S. S. Chen  D. L. Donoho  and M. A. Saunders. Atomic Decomposition by Basis Pursuit. SIAM Journal

on Scientiﬁc Computing  20(1):33–61  jan 1998.

[5] V. Duval and G. Peyré. Sparse spikes deconvolution on thin grids. Preprint 01135200  HAL  2015.

[6] J.-J. Fuchs. On sparse representations in arbitrary redundant bases. IEEE Transactions on Information

Theory  50(6):1341–1344  2004.

[7] M. Grant and S. Boyd. Graph implementations for nonsmooth convex programs. In V. Blondel  S. Boyd  and
H. Kimura  editors  Recent Advances in Learning and Control  Lecture Notes in Control and Information
Sciences  pages 95–110. Springer-Verlag Limited  2008. http://stanford.edu/~boyd/graph_dcp.
html.

[8] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming  version 2.1. http:

//cvxr.com/cvx  March 2014.

[9] L. Jacques. On the optimality of a L1/L1 solver for sparse signal recovery from sparsely corrupted

compressive measurements. Technical Report  TR-LJ-2013.01  arXiv preprint arXiv:1303.5097  2013.

[10] L. Jacques  D. K. Hammond  and Jalal M. Fadili. Dequantizing Compressed Sensing: When Oversampling
and Non-Gaussian Constraints Combine. IEEE Transactions on Information Theory  57(1):559–571  jan
2011.

[11] M. Nikolova. A variational approach to remove outliers and impulse noise. Journal of Mathematical

Imaging and Vision  20(1)  2004.

[12] R. T. Rockafellar. Conjugate duality and optimization  volume 16. Siam  1974.

[13] C. Studer  P. Kuppinger  G. Pope  and H. Bolcskei. Recovery of Sparsely Corrupted Signals. IEEE

Transactions on Information Theory  58(5):3115–3130  may 2012.

[14] R. Tibshirani. Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society.

Series B: Statistical Methodology  58(1):267–288  1995.

[15] Ryan J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics  7:1456–1490 

2013.

[16] S. Vaiter  G. Peyré  C. Dossal  and M.J. Fadili. Robust sparse analysis regularization. IEEE Transactions

on Information Theory  59(4):2001–2016  2013.

[17] S. Vaiter  G. Peyré  and J. Fadili. Model consistency of partly smooth regularizers. Preprint 00987293 

HAL  2014.

[18] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1-constrained

quadratic programming (lasso). IEEE Transactions on Information Theory  55(5):2183–2202  2009.

[19] P. Zhao and B. Yu. On model selection consistency of Lasso. J. Mach. Learn. Res.  7:2541–2563  December

2006.

9

,Kévin Degraux
Gabriel Peyré
Jalal Fadili
Laurent Jacques