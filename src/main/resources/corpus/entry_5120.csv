2013,Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis,Multitask learning can be effective when features useful in one task are also useful for other tasks  and the group lasso is a standard method for selecting a common subset of features. In this paper  we are interested in a less restrictive form of multitask learning  wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar  but not necessarily identical  to the features best suited for other tasks. The main contribution of this paper is a new procedure called {\em Sparse Overlapping Sets (SOS) lasso}  a convex optimization that automatically selects similar features for related learning tasks.  Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular   SOSlasso is motivated by multi-subject fMRI studies in which functional activity is classified using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso.,Sparse Overlapping Sets Lasso for Multitask
Learning and its Application to fMRI Analysis

Nikhil S. Rao†

nrao2@wisc.edu

Robert D. Nowak†

nowak@ece.wisc.edu

Christopher R. Cox#
crcox@wisc.edu

Timothy T. Rogers#

ttrogers@wisc.edu

† Department of Electrical and Computer Engineering  # Department of Psychology

University of Wisconsin- Madison

Abstract

Multitask learning can be effective when features useful in one task are also useful
for other tasks  and the group lasso is a standard method for selecting a common
subset of features. In this paper  we are interested in a less restrictive form of mul-
titask learning  wherein (1) the available features can be organized into subsets
according to a notion of similarity and (2) features useful in one task are simi-
lar  but not necessarily identical  to the features best suited for other tasks. The
main contribution of this paper is a new procedure called Sparse Overlapping Sets
(SOS) lasso  a convex optimization that automatically selects similar features for
related learning tasks. Error bounds are derived for SOSlasso and its consistency
is established for squared error loss. In particular  SOSlasso is motivated by multi-
subject fMRI studies in which functional activity is classiﬁed using brain voxels
as features. Experiments with real and synthetic data demonstrate the advantages
of SOSlasso compared to the lasso and group lasso.

Introduction

1
Multitask learning exploits the relationships between several learning tasks in order to improve
performance  which is especially useful if a common subset of features are useful for all tasks at
hand. The group lasso (Glasso) [19  8] is naturally suited for this situation: if a feature is selected
for one task  then it is selected for all tasks. This may be too restrictive in many applications  and
this motivates a less rigid approach to multitask feature selection. Suppose that the available features
can be organized into overlapping subsets according to a notion of similarity  and that the features
useful in one task are similar  but not necessarily identical  to those best suited for other tasks. In
other words  a feature that is useful for one task suggests that the subset it belongs to may contain
the features useful in other tasks (Figure 1).
In this paper  we introduce the sparse overlapping sets lasso (SOSlasso)  a convex program to re-
cover the sparsity patterns corresponding to the situations explained above. SOSlasso generalizes
lasso [16] and Glasso  effectively spanning the range between these two well-known procedures.
SOSlasso is capable of exploiting the similarities between useful features across tasks  but unlike
Glasso it does not force different tasks to use exactly the same features. It produces sparse solutions 
but unlike lasso it encourages similar patterns of sparsity across tasks. Sparse group lasso [14] is
a special case of SOSlasso that only applies to disjoint sets  a signiﬁcant limitation when features
cannot be easily partitioned  as is the case of our motivating example in fMRI. The main contribu-
tion of this paper is a theoretical analysis of SOSlasso  which also covers sparse group lasso as a
special case (further differentiating us from [14]). The performance of SOSlasso is analyzed  error

1

bounds are derived for general loss functions  and its consistency is shown for squared error loss.
Experiments with real and synthetic data demonstrate the advantages of SOSlasso relative to lasso
and Glasso.

1.1 Sparse Overlapping Sets

SOSlasso encourages sparsity patterns that are similar  but not identical  across tasks. This is ac-
complished by decomposing the features of each task into groups G1 . . . GM   where M is the same
for each task  and Gi is a set of features that can be considered similar across tasks. Conceptually 
SOSlasso ﬁrst selects subsets that are most useful for all tasks  and then identiﬁes a unique sparse
solution for each task drawing only from features in the selected subsets. In the fMRI application
discussed later  the subsets are simply clusters of adjacent spatial data points (voxels) in the brains of
multiple subjects. Figure 1 shows an example of the patterns that typically arise in sparse multitask
learning applications  where rows indicate features and columns correspond to tasks.
Past work has focused on recovering variables that exhibit within and across group sparsity  when
the groups do not overlap [14]  ﬁnding application in genetics  handwritten character recognition
[15] and climate and oceanography [2]. Along related lines  the exclusive lasso [21] can be used
when it is explicitly known that variables in certain sets are negatively correlated.

(a) Sparse

(b) Group sparse

(c) Group sparse
plus sparse

(d) Group sparse
and sparse

Figure 1: A comparison of different sparsity patterns. (a) shows a standard sparsity pattern. An
example of group sparse patterns promoted by Glasso [19] is shown in (b). In (c)  we show the
patterns considered in [6]. Finally  in (d)  we show the patterns we are interested in this paper.

1.2

fMRI Applications

In psychological studies involving fMRI  multiple participants are scanned while subjected to ex-
actly the same experimental manipulations. Cognitive Neuroscientists are interested in identifying
the patterns of activity associated with different cognitive states  and construct a model of the activity
that accurately predicts the cognitive state evoked on novel trials. In these datasets  it is reasonable
to expect that the same general areas of the brain will respond to the manipulation in every partici-
pant. However  the speciﬁc patterns of activity in these regions will vary  both because neural codes
can vary by participant [4] and because brains vary in size and shape  rendering neuroanatomy only
an approximate guide to the location of relevant information across individuals. In short  a voxel
useful for prediction in one participant suggests the general anatomical neighborhood where useful
voxels may be found  but not the precise voxel. While logistic Glasso [17]  lasso [13]  and the elas-
tic net penalty [12] have been applied to neuroimaging data  these methods do not exclusively take
into account both the common macrostructure and the differences in microstructure across brains.
SOSlasso  in contrast  lends itself well to such a scenario  as we will see from our experiments.
1.3 Organization
The rest of the paper is organized as follows: in Section 2  we outline the notations that we will
use and formally set up the problem. We also introduce the SOSlasso regularizer. We derive cer-
tain key properties of the regularizer in Section 3. In Section 4  we specialize the problem to the
multitask linear regression setting (2)  and derive consistency rates for the same  leveraging ideas
from [9]. We outline experiments performed on simulated data in Section 5. In this section  we also
perform logistic regression on fMRI data  and argue that the use of the SOSlasso yields interpretable
multivariate solutions compared to Glasso and lasso.

2

2 Sparse Overlapping Sets Lasso
We formalize the notations used in the sequel. Lowercase and uppercase bold letters indicate
vectors and matrices respectively. We assume a multitask learning framework  with a data ma-
trix Φt ∈ Rn×p for each task t ∈ {1  2  . . .  T }. We assume there exists a vector x(cid:63)
t ∈ Rp
t + ηt ηt ∼ N (0  σ2I). Let
such that measurements obtained are of the form yt = Φtx(cid:63)
2 . . . x(cid:63)T ] ∈ Rp×T . Suppose we are given M (possibly overlapping) groups
X (cid:63) := [x(cid:63)
˜G = { ˜G1  ˜G2  . . .   ˜GM}  so that ˜Gi ⊂ {1  2  . . .   p} ∀i  of maximum size B. These groups contain
sets of “similar” features  the notion of similarity being application dependent. We assume that all
but k (cid:28) M groups are identically zero. Among the active groups  we further assume that at most
only a fraction α ∈ (0  1) of the coefﬁcients per group are non zero. We consider the following
optimization program in this paper

1 x(cid:63)

(cid:40) T(cid:88)

(cid:41)
LΦt(xt) + λnh(x)

ˆX = arg min
x

t=1

(1)

(2)

2 . . . xTT ]T   h(x) is a regularizer and Lt := LΦt(xt) denotes the loss function 
where x = [xT
whose value depends on the data matrix Φt. We consider least squares and logistic loss functions. In
the least squares setting  we have Lt = 1
2n(cid:107)yt − Φtxt(cid:107)2. We reformulate the optimization problem

1 xT

(1) with the least squares loss as(cid:98)x = arg min

(cid:26) 1

x

2n

(cid:27)

(cid:107)y − Φx(cid:107)2

2 + λnh(x)

1 yT

2 . . . yTT ]T and the block diagonal matrix Φ is formed by block concatenating the
where y = [yT
Φ(cid:48)
ts. We use this reformulation for ease of exposition (see also [8] and references therein). Note
and Φ ∈ RT n×T p. We also deﬁne G = {G1  G2  . . .   GM} to be the
that x ∈ RT p  y ∈ RT n 
set of groups deﬁned on RT p formed by aggregating the rows of X that were originally in ˜G  so that
x is composed of groups G ∈ G.
We next deﬁne a regularizer h that promotes sparsity both within and across overlapping sets of
similar features:

(αG(cid:107)wG(cid:107)2 + (cid:107)wG(cid:107)1) s.t. (cid:88)

h(x) = infW

wG = x

(3)

G∈G

(cid:88)

G∈G

where the αG > 0 are constants that balance the tradeoff between the group norms and the (cid:96)1 norm.
Each wG has the same size as x  with support restricted to the variables indexed by group G. W is
a set of vectors  where each vector has a support restricted to one of the groups G ∈ G:

W = {wG ∈ RT p| [wG]i = 0 if i /∈ G}

where [wG]i is the ith coefﬁcient of wG. The SOSlasso is the optimization in (1) with h(x) as
deﬁned in (3).
We say the set of vectors wG is an optimal decomposition of x if they achieve the inf in (3). The
objective function in (3) is convex and coercive. Hence  ∀x  an optimal decomposition always exists.
As the αG → ∞ the (cid:96)1 term becomes redundant  reducing h(x) to the overlapping group lasso
penalty introduced in [5]  and studied in [10  11]. When the αG → 0  the overlapping group lasso
term vanishes and h(x) reduces to the lasso penalty. We consider αG = 1 ∀G. All the results in the
paper can be easily modiﬁed to incorporate different settings for the αG.

Support
{1  4  9}
{1  3  4}

{1  2  3  4  5}

Values
{3  4  7}
{3  4  7}

{2  5  2  4  5}

(cid:80)
G (cid:107)xG(cid:107)2
12

8.602
8.602

(cid:107)x(cid:107)1
14
18
14

(cid:80)
G ((cid:107)xG(cid:107)2 + (cid:107)xG(cid:107)1)

26

26.602
22.602

Table 1: Different instances of a 10-d vector and their corresponding norms.

The example in Table 1 gives an insight into the kind of sparsity patterns preferred by the function
h(x). The optimization problems (1) and (2) will prefer solutions that have a small value of h(·).

3

Consider 3 instances of x ∈ R10  and the corresponding group lasso  (cid:96)1  and h(x) function values.
The vector is assumed to be made up of two groups  G1 = {1  2  3  4  5} and G2 = {6  7  8  9  10}.
h(x) is smallest when the support set is sparse within groups  and also when only one of the two
groups is selected. The (cid:96)1 norm does not take into account sparsity across groups  while the group
lasso norm does not take into account sparsity within groups.
To solve (1) and (2) with the regularizer proposed in (3)  we use the covariate duplication method of
[5]  to reduce the problem to a non overlapping sparse group lasso problem. We then use proximal
point methods [7] in conjunction with the MALSAR [20] package to solve the optimization problem.
3 Error Bounds for SOSlasso with General Loss Functions
We derive certain key properties of the regularizer h(·) in (3)  independent of the loss function used.
Lemma 3.1 The function h(x) in (3) is a norm
The proof follows from basic properties of norms and because if wG  vG are optimal decompositions
of x  y  then it does not imply that wG + vG is an optimal decomposition of x + y. For a detailed
proof  please refer to the supplementary material.
The dual norm of h(x) can be bounded as

h∗(u) = max

{xT u} s.t. h(x) ≤ 1

x

G∈G

= maxW {(cid:88)
(i)≤ maxW {(cid:88)
= maxW {(cid:88)

G∈G

wT

wT

GuG} s.t. (cid:88)
GuG} s.t. (cid:88)
GuG} s.t. (cid:88)

G∈G

G∈G

G∈G

wT

G∈G
(cid:107)uG(cid:107)2
1
2

⇒ h∗(u) ≤ max
G∈G

((cid:107)wG(cid:107)2 + (cid:107)wG(cid:107)1) ≤ 1

2(cid:107)wG(cid:107)2 ≤ 1

(cid:107)wG(cid:107)2 ≤ 1
2

(4)

2(cid:107)uG∗(cid:107)2

(i) follows from the fact that the constraint set in (i) is a superset of the constraint set in the previous
statement  since (cid:107)a(cid:107)2 ≤ (cid:107)a(cid:107)1. (4) follows from noting that the maximum is obtained by setting
  where G∗ = arg maxG∈G (cid:107)uG(cid:107)2. The inequality (4) is far more tractable than
wG∗ = uG∗
the actual dual norm  and will be useful in our derivations below. Since h(·) is a norm  we can apply
methods developed in [9] to derive consistency rates for the optimization problems (1) and (2). We
will use the same notations as in [9] wherever possible.
Deﬁnition 3.2 A norm h(·) is decomposable with respect to the subspace pair sA ⊂ sB if h(a +
b) = h(a) + h(b) ∀a ∈ sA  b ∈ sB⊥.
Lemma 3.3 Let x(cid:63) ∈ Rp be a vector that can be decomposed into (overlapping) groups with within-
group sparsity. Let G(cid:63) ⊂ G be the set of active groups of x(cid:63). Let S = supp(x(cid:63)) indicate the support
set of x. Let sA be the subspace spanned by the coordinates indexed by S  and let sB = sA. We
then have that the norm in (3) is decomposable with respect to sA  sB

The result follows in a straightforward way from noting that supports of decompositions for vectors
in sA and sB⊥ do not overlap. We defer the proof to the supplementary material.
Deﬁnition 3.4 Given a subspace sB  the subspace compatibility constant with respect to a norm
(cid:107) (cid:107) is given by

Lemma 3.5 Consider a vector x that can be decomposed into G(cid:63) ⊂ G active groups. Suppose the
maximum group size is B  and also assume that a fraction α ∈ (0  1) of the coordinates in each
active group is non zero. Then 

(cid:27)
(cid:26) h(x)
(cid:107)x(cid:107) ∀x ∈ sB\{0}
Bα)(cid:112)|G(cid:63)|(cid:107)x(cid:107)2

√

Ψ(B) = sup

h(x) ≤ (1 +

4

√

G∈G(cid:63)

(cid:88)

((cid:107)wG(cid:107)2 + (cid:107)wG(cid:107)1) ≤ (1 +

that the supports of the different wG do not overlap. Then 

Proof For any vector x with supp(x) ⊂ G(cid:63)  there exists a representation x =(cid:80)
h(x) ≤ (cid:88)

Bα)(cid:112)|G(cid:63)|(cid:107)x(cid:107)2
Bα)(cid:112)|G(cid:63)| (Lemma 3.5) gives an upper bound on the subspace compatibility

We see that (1 +
constant with respect to the (cid:96)2 norm for the subspace indexed by the support of the vector  which is
contained in the span of the union of groups in G(cid:63).
Deﬁnition 3.6 For a given set S  and given vector x(cid:63)  the loss function LΦ(x) satisﬁes the Re-
stricted Strong Convexity(RSC) condition with parameter κ and tolerance τ if

(cid:107)wG(cid:107)2 ≤ (1 +

G∈G(cid:63) wG  such

G∈G(cid:63)

Bα)

√

√

LΦ(x(cid:63) + ∆) − LΦ(x(cid:63)) − (cid:104)∇LΦ(x(cid:63))  ∆(cid:105) ≥ κ(cid:107)∆(cid:107)2

2 − τ 2(x(cid:63)) ∀∆ ∈ S

In this paper  we consider vectors x(cid:63) that lie exactly in k (cid:28) M groups  and display within-group
sparsity. This implies that the tolerance τ (x(cid:63)) = 0  and we will ignore this term henceforth.
We also deﬁne the following set  which will be used in the sequel:

C(sA  sB  x(cid:63)) := {∆ ∈ Rp|h(ΠsB⊥∆) ≤ 3h(ΠsB∆) + 4h(ΠsA⊥ x(cid:63))}

(5)
where ΠsA(·) denotes the projection onto the subspace sA. Based on the results above  we can now
apply a result from [9] to the SOSlasso:

Theorem 3.7 (Corollary 1 in [9]) Consider a convex and differentiable loss function such that RSC
holds with constants κ and τ = 0 over (5)  and a norm h(·) decomposable over sets sA and sB. For
the optimization program in (1)  using the parameter λn ≥ 2h∗(∇LΦ(x(cid:63)))  any optimal solution
ˆxλn to (1) satisﬁes

(cid:107)(cid:98)xλn − x(cid:63)(cid:107)2 ≤ 9λ2

n
κ

Ψ2(sB)

The result above shows a general bound on the error using the lasso with sparse overlapping sets.
Note that the regularization parameter λn as well as the RSC constant κ depend on the loss function
LΦ(x). Convergence for logistic regression settings may be derived using methods in [1]. In the
next section  we consider the least squares loss (2)  and show that the estimate using the SOSlasso
is consistent.

4 Consistency of SOSlasso with Squared Error Loss

n ΦT (Φx − y) = 1

We ﬁrst need to bound the dual norm of the gradient of the loss function  so as to bound λn. Consider
L := LΦ(x) = 1
2n(cid:107)y − Φx(cid:107)2. The gradient of the loss function with respect to x is given by
∇L = 1
2 . . . ηTT ]T (see Section 2). Our goal now is to
ﬁnd an upper bound on the quantity h∗(∇L)  which from (4) is
G∈G (cid:107)ΦT

G∈G (cid:107)∇LG(cid:107)2 =

n ΦT η where η = [ηT

Gη(cid:107)2

1 ηT

max

max

1
2n

1
2

Gη ∼ σN (0  ΦT

where ΦG is the matrix Φ restricted to the columns indexed by the group G. We will prove an upper
bound for the above quantity in the course of the results that follow.
Since η ∼ N (0  σ2I)  we have ΦT
GΦG} to be
2 ∼
the maximum singular value  we have (cid:107)ΦT
χ2|G|  where χ2
d is a chi-squared random variable with d degrees of freedom. This allows us to work
with the more tractable chi squared random variable when we look to bound the dual norm of ∇L.
The next lemma helps us obtain a bound on the maximum of χ2 random variables.
Lemma 4.1 Let z1  z2  . . .   zM be chi-squared random variables with d degrees of freedom. Then
for some constant c 

GΦG). Deﬁning σmG := σmax{ΦT

2  where γ ∼ N (0  I|G|) ⇒ (cid:107)γ(cid:107)2

2 ≤ σ2σ2

mG(cid:107)γ(cid:107)2

Gη(cid:107)2

(cid:18)

P

max

i=1 2 ... M

zi ≤ c2d

(cid:18)
log(M ) − (c − 1)2d

(cid:19)

2

(cid:19)

≥ 1 − exp

5

Proof From the chi-squared tail bound in [3]  P(zi ≥ c2d) ≤ exp
from a union bound and inverting the expression.

(cid:16)− (c−1)2d

2

(cid:17)

. The result follows

(cid:80)T
t=1 (cid:107)yt − Φtxt(cid:107)2 = 1

Lemma 4.2 Consider the loss function L := 1
2n(cid:107)y − Φx(cid:107)2  with the
Φ(cid:48)
ts deterministic and the measurements corrupted with AWGN of variance σ2. For the regularizer
in (3)  the dual norm of the gradient of the loss function is bounded as

2n

h∗(∇L)2 ≤ σ2σ2
4

m

(log(M ) + T B)

n

with probability at least 1 − c1 exp(−c2n)  for c1  c2 > 0  and where σm = maxG∈G σmG
Proof Let γ ∼ χ2T |G|. We begin with the upper bound obtained for the dual norm of the regularizer
in (4):
(cid:19)

(cid:18)
log(M ) − (cn − 1)2T B

max
G∈G
c2T B w. p. 1 − exp

(cid:13)(cid:13)(cid:13)(cid:13) 1

h∗(∇L)2

σ2
mGγ
n2

≤ σ2
4

(cid:13)(cid:13)(cid:13)(cid:13)2

ΦT

Gη

n

m

2

(i)≤ 1
4

max
G∈G
(ii)≤ σ2σ2
4

m

max
G∈G

γ
n2

(iii)≤ σ2σ2
4

2

where (i) follows from the formulation of the gradient of the loss function and the fact that the
square of maximum of non negative numbers is the maximum of the squares of the same numbers.
In (ii)  we have deﬁned σm = maxG σmG. Finally  we have made use of Lemma 4.1 in (iii). We
then set

to obtain the result.

c2 =

log(M ) + T B

T Bn

We combine the results developed so far to derive the following consistency result for the SOS lasso 
with the least squares loss function.

Theorem 4.3 Suppose we obtain linear measurements of a sparse overlapping grouped matrix
X (cid:63) ∈ Rp×T   corrupted by AWGN of variance σ2. Suppose the matrix X (cid:63) can be decomposed
into M possible overlapping groups of maximum size B  out of which k are active. Furthermore 
assume that a fraction α ∈ (0  1] of the coefﬁcients are non zero in each active group. Consider the
following vectorized SOSlasso multitask regression problem (2):

(cid:26) 1

2n

(cid:98)x = arg min
(cid:88)

x

h(x) = infW

G∈G

(cid:107)y − Φx(cid:107)2

2 + λnh(x)

 

((cid:107)wG(cid:107)2 + (cid:107)wG(cid:107)1)

wG = x

(cid:27)

s.t. (cid:88)

G∈G

Suppose the data matrices Φt are non random  and the loss function satisﬁes restricted strong
convexity assumptions with parameter κ. Then  for λn ≥ σ2σ2
  the following holds
with probability at least 1 − c1 exp(−c2n)  with c1  c2 > 0:
√T Bα

k(log(M ) + T B)

m(log(M )+T B)

(cid:17)2

(cid:16)

1 +

4n

σ2σ2
m

where we deﬁne σm := maxG∈G σmax{ΦT
Proof Follows from substituting in Theorem 3.7 the results from Lemma 3.5 and Lemma 4.2.

GΦG}

nκ

(cid:107)(cid:98)x − x(cid:63)(cid:107)2 ≤ 9

4

From [9]  we see that the convergence rate matches that of the group lasso  with an additional
multiplicative factor α. This stems from the fact that the signal has a sparse structure “embedded”
within a group sparse structure. Visualizing the optimization problem as that of solving a lasso
within a group lasso framework lends some intuition into this result. Note that since α < 1  this
bound is much smaller than that of the standard group lasso.

6

5 Experiments and Results
5.1 Synthetic data  Gaussian Linear Regression
For T = 20 tasks  we deﬁne a N = 2002 element vector divided into M = 500 groups of size
B = 6. Each group overlaps with its neighboring groups (G1 = {1  2  . . .   6}  G2 = {5  6  . . .   10} 
G3 = {9  10  . . .   14}  . . . ). 20 of these groups were activated uniformly at random  and populated
from a uniform [−1  1] distribution. A proportion α of these coefﬁcients with largest magnitude
were retained as true signal. For each task  we obtain 250 linear measurements using a N (0  1
250 I)
matrix. We then corrupt each measurement with Additive White Gaussian Noise (AWGN)  and
assess signal recovery in terms of Mean Squared Error (MSE). The regularization parameter was
clairvoyantly picked to minimize the MSE over a range of parameter values. The results of applying
lasso  standard latent group lasso [5  10]  and our SOSlasso to these data are plotted in Figures 2(a) 
varying σ  α = 0.2  and 2(b)  varying α  σ = 0.1. Each point in Figures 2(a) and 2(b)  is the
average of 100 trials  where each trial is based on a new random instance of X (cid:63) and the Gaussian
data matrices.

(a) Varying σ

(b) Varying α

(c) Sample pattern

Figure 2: As the noise is increased (a)  our proposed penalty function (SOSlasso) allows us to
recover the true coefﬁcients more accurately than the group lasso (Glasso). Also  when alpha is
large  the active groups are not sparse  and the standard overlapping group lasso outperforms the
other methods. However  as α reduces  the method we propose outperforms the group lasso (b). (c)
shows a toy sparsity pattern  with different colors denoting different overlapping groups

5.2 The SOSlasso for fMRI

In this experiment  we compared SOSlasso  lasso  and Glasso in analysis of the star-plus dataset [18].
6 subjects made judgements that involved processing 40 sentences and 40 pictures while their brains
were scanned in half second intervals using fMRI1. We retained the 16 time points following each
stimulus  yielding 1280 measurements at each voxel. The task is to distinguish  at each point in time 
which stimulus a subject was processing. [18] showed that there exists cross-subject consistency in
the cortical regions useful for prediction in this task. Speciﬁcally  experts partitioned each dataset
into 24 non overlapping regions of interest (ROIs)  then reduced the data by discarding all but 7 ROIs
and  for each subject  averaging the BOLD response across voxels within each ROI and showed that
a classiﬁer trained on data from 5 subjects generalized when applied to data from a 6th.
We assessed whether SOSlasso could leverage this cross-individual consistency to aid in the dis-
covery of predictive voxels without requiring expert pre-selection of ROIs  or data reduction  or
any alignment of voxels beyond that existing in the raw data. Note that  unlike [18]  we do not
aim to learn a solution that generalizes to a withheld subject. Rather  we aim to discover a group
sparsity pattern that suggests a similar set of voxels in all subjects  before optimizing a separate
solution for each individual. If SOSlasso can exploit cross-individual anatomical similarity from
this raw  coarsely-aligned data  it should show reduced cross-validation error relative to the lasso
applied separately to each individual. If the solution is sparse within groups and highly variable
across individuals  SOSlasso should show reduced cross-validation error relative to Glasso. Finally 
if SOSlasso is ﬁnding useful cross-individual structure  the features it selects should align at least
somewhat with the expert-identiﬁed ROIs shown by [18] to carry consistent information.

1Data and documentation available at http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-81/www/

7

00.050.10.150.200.0050.010.0150.02σMSE  GlassoSOSlasso00.20.40.60.8100.0050.010.0151 − αMSE  GlassoSOSlassolassoFigure 3: Results from fMRI exper-
iments.
(a) Aggregated sparsity pat-
terns for a single brain slice. (b) Cross-
validation error obtained with each
method. Lines connect data for a sin-
gle subject. (c) The full sparsity pattern
obtained with SOSlasso.

Method % ROI
46.11
50.89
70.31

lasso
Glasso
SOSlasso

t(5)   p

6.08  0.001
5.65  0.002

Table 2: Proportion of selected voxels
in the 7 relevant ROIS aggregated over
subjects  and corresponding two-tailed
signiﬁcance levels for the contrast of
lasso and Glasso to SOSlasso.

(b)

(c)

(a)

We trained 3 classiﬁers using 4-fold cross validation to select the regularization parameter  consid-
ering all available voxels without preselection. We group regions of 5× 5× 1 voxels and considered
overlapping groups “shifted” by 2 voxels in the ﬁrst 2 dimensions.2 Figure 3(b) shows the individual
error rates across the 6 subjects for the three methods. Across subjects  SOSlasso had a signiﬁcantly
lower cross-validation error rate (27.47 %) than individual lasso (33.3 %; within-subjects t(5) = 4.8;
p = 0.004 two-tailed)  showing that the method can exploit anatomical similarity across subjects to
learn a better classiﬁer for each. SOSlasso also showed signiﬁcantly lower error rates than glasso
(31.1 %; t(5) = 2.92; p = 0.03 two-tailed)  suggesting that the signal is sparse within selected regions
and variable across subjects.
Figure 3(a) presents a sample of the the sparsity patterns obtained from the different methods  aggre-
gated over all subjects. Red points indicate voxels that contributed positively to picture classiﬁcation
in at least one subject  but never to sentences; Blue points have the opposite interpretation. Purple
points indicate voxels that contributed positively to picture and sentence classiﬁcation in different
subjects. The remaining slices for the SOSlasso are shown in Figure 3(c). There are three things to
note from Figure 3(a). First  the Glasso solution is fairly dense  with many voxels signaling both
picture and sentence across subjects. We believe this “purple haze” demonstrates why Glasso is ill-
suited for fMRI analysis: a voxel selected for one subject must also be selected for all others. This
approach will not succeed if  as is likely  there exists no direct voxel-to-voxel correspondence or if
the neural code is variable across subjects. Second  the lasso solution is less sparse than the SOSlasso
because it allows any task-correlated voxel to be selected. It leads to a higher cross-validation error 
indicating that the ungrouped voxels are inferior predictors (Figure 3(b)). Third  the SOSlasso not
only yields a sparse solution  but also clustered. To assess how well these clusters align with the
anatomical regions thought a-priori to be involved in sentence and picture representation  we calcu-
lated the proportion of selected voxels falling within the 7 ROIs identiﬁed by [18] as relevant to the
classiﬁcation task (Table 2). For SOSlasso an average of 70% of identiﬁed voxels fell within these
ROIs  signiﬁcantly more than for lasso or Glasso.

6 Conclusions and Extensions
We have introduced SOSlasso  a function that recovers sparsity patterns that are a hybrid of overlap-
ping group sparse and sparse patterns when used as a regularizer in convex programs  and proved
its theoretical convergence rates when minimizing least squares. The SOSlasso succeeds in a multi-
task fMRI analysis  where it both makes better inferences and discovers more theoretically plausible
brain regions that lasso and Glasso. Future work involves experimenting with different parameters
for the group and l1 penalties  and using other similarity groupings  such as functional connectivity
in fMRI.

2The irregular group size compensates for voxels being larger and scanner coverage being smaller in the

z-dimension (only 8 slices relative to 64 in the x- and y-dimensions).

8

SOSlassoGlassolassoR0.240.260.280.30.320.340.36lassoGlassoSOSlassoErrorPicture onlySentence onlyPicture and SentenceReferences
[1] Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic

regression. arXiv preprint arXiv:1303.6149  2013.

[2] S. Chatterjee  A. Banerjee  and A. Ganguly. Sparse group lasso for regression on land climate variables.
In Data Mining Workshops (ICDMW)  2011 IEEE 11th International Conference on  pages 1–8. IEEE 
2011.

[3] S. Dasgupta  D. Hsu  and N. Verma. A concentration theorem for projections.

arXiv:1206.6813  2012.

arXiv preprint

[4] Eva Feredoes  Giulio Tononi  and Bradley R Postle. The neural bases of the short-term storage of verbal
information are anatomically variable across individuals. The Journal of Neuroscience  27(41):11003–
11008  2007.

[5] L. Jacob  G. Obozinski  and J. P. Vert. Group lasso with overlap and graph lasso. In Proceedings of the

26th Annual International Conference on Machine Learning  pages 433–440. ACM  2009.

[6] A. Jalali  P. Ravikumar  S. Sanghavi  and C. Ruan. A dirty model for multi-task learning. Advances in

Neural Information Processing Systems  23:964–972  2010.

[7] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for hierarchical sparse coding. arXiv

preprint arXiv:1009.2139  2010.

[8] K. Lounici  M. Pontil  A. B. Tsybakov  and S. van de Geer. Taking advantage of sparsity in multi-task

learning. arXiv preprint arXiv:0903.1468  2009.

[9] S. N. Negahban  P. Ravikumar  M. J Wainwright  and Bin Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Statistical Science  27(4):538–557  2012.

[10] G. Obozinski  L. Jacob  and J.P. Vert. Group lasso with overlaps: The latent group lasso approach. arXiv

preprint arXiv:1110.0413  2011.

[11] N. Rao  B. Recht  and R. Nowak. Universal measurement bounds for structured sparse signal recovery.

In Proceedings of AISTATS  volume 2102  2012.

[12] Irina Rish  Guillermo A Cecchia  Kyle Heutonb  Marwan N Balikic  and A Vania Apkarianc. Sparse
regression analysis of task-relevant information distribution in the brain. In Proceedings of SPIE  volume
8314  page 831412  2012.

[13] Srikanth Ryali  Kaustubh Supekar  Daniel A Abrams  and Vinod Menon. Sparse logistic regression for

whole brain classiﬁcation of fmri data. NeuroImage  51(2):752  2010.

[14] N. Simon  J. Friedman  T. Hastie  and R. Tibshirani. A sparse-group lasso. Journal of Computational and

Graphical Statistics  (just-accepted)  2012.

[15] P. Sprechmann  I. Ramirez  G. Sapiro  and Y. Eldar. Collaborative hierarchical sparse modeling.
Information Sciences and Systems (CISS)  2010 44th Annual Conference on  pages 1–6. IEEE  2010.

In

[16] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society. Series B (Methodological)  pages 267–288  1996.

[17] Marcel van Gerven  Christian Hesse  Ole Jensen  and Tom Heskes. Interpreting single trial data using

groupwise regularisation. NeuroImage  46(3):665–676  2009.

[18] X. Wang  T. M Mitchell  and R. Hutchinson. Using machine learning to detect cognitive states across

multiple subjects. CALD KDD project paper  2003.

[19] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  68(1):49–67  2006.

[20] J. Zhou  J. Chen  and J. Ye. Malsar: Multi-task learning via structural regularization  2012.
[21] Y. Zhou  R. Jin  and S. C. Hoi. Exclusive lasso for multi-task feature selection. In Proceedings of the

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2010.

9

,Nikhil Rao
Christopher Cox
Rob Nowak
Timothy Rogers
Changxiao Cai
Gen Li
H. Vincent Poor
Yuxin Chen