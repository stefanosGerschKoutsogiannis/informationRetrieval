2014,Multilabel Structured Output Learning with Random Spanning Trees of Max-Margin Markov Networks,We show that the usual score function for conditional Markov networks can be written as the expectation over the scores of their spanning trees. We also show that a small random sample of these output trees can attain a significant fraction of the margin obtained by the complete graph and we provide conditions under which we can perform tractable inference. The experimental results confirm that practical learning is scalable to realistic datasets using this approach.,Multilabel Structured Output Learning with Random

Spanning Trees of Max-Margin Markov Networks

D´epartement d’informatique et g´enie logiciel

Mario Marchand

Universit´e Laval

Qu´ebec (QC)  Canada

mario.marchand@ift.ulaval.ca

Hongyu Su

Helsinki Institute for Information Technology
Dept of Information and Computer Science

Aalto University  Finland
hongyu.su@aalto.fi

Emilie Morvant∗

LaHC  UMR CNRS 5516
Univ. of St-Etienne  France

emilie.morvant@univ-st-etienne.fr

Juho Rousu

Helsinki Institute for Information Technology
Dept of Information and Computer Science

Aalto University  Finland
juho.rousu@aalto.fi

John Shawe-Taylor

Department of Computer Science

University College London

London  UK

j.shawe-taylor@ucl.ac.uk

Abstract

We show that the usual score function for conditional Markov networks can be
written as the expectation over the scores of their spanning trees. We also show
that a small random sample of these output trees can attain a signiﬁcant fraction
of the margin obtained by the complete graph and we provide conditions under
which we can perform tractable inference. The experimental results conﬁrm that
practical learning is scalable to realistic datasets using this approach.

Introduction

1
Finding an hyperplane that minimizes the number of misclassiﬁcations is NP-hard. But the support
vector machine (SVM) substitutes the hinge for the discrete loss and  modulo a margin assumption 
can nonetheless efﬁciently ﬁnd a hyperplane with a guarantee of good generalization. This paper
investigates whether the problem of inference over a complete graph in structured output prediction
can be avoided in an analogous way based on a margin assumption.
We ﬁrst show that the score function for the complete output graph can be expressed as the expec-
tation over the scores of random spanning trees. A sampling result then shows that a small random
sample of these output trees can attain a signiﬁcant fraction of the margin obtained by the complete
graph. Together with a generalization bound for the sample of trees  this shows that we can obtain
good generalization using the average scores of a sample of trees in place of the complete graph.
We have thus reduced the intractable inference problem to a convex optimization not dissimilar to
a SVM. The key inference problem to enable learning with this ensemble now becomes ﬁnding the
maximum violator for the (ﬁnite sample) average tree score. We then provide the conditions under
which the inference problem is tractable. Experimental results conﬁrm this prediction and show that

∗Most of this work was carried out while E. Morvant was afﬁliated with IST Austria  Klosterneurburg.

1

practical learning is scalable to realistic datasets using this approach with the resulting classiﬁcation
accuracy enhanced over more naive ways of training the individual tree score functions.
The paper aims at exploring the potential ramiﬁcations of the random spanning tree observation
both theoretically and practically. As such  we think that we have laid the foundations for a fruitful
approach to tackle the intractability of inference in a number of scenarios. Other attractive features
are that we do not require knowledge of the output graph’s structure  that the optimization is convex 
and that the accuracy of the optimization can be traded against computation. Our approach is ﬁrmly
rooted in the maximum margin Markov network analysis [1]. Other ways to address the intractability
of loopy graph inference have included using approximate MAP inference with tree-based and LP
relaxations [2]  semi-deﬁnite programming convex relaxations [3]  special cases of graph classes for
which inference is efﬁcient [4]  use of random tree score functions in heuristic combinations [5].
Our work is not based on any of these approaches  despite superﬁcial resemblances to  e.g.  the
trees in tree-based relaxations and the use of random trees in [5]. We believe it represents a distinct
approach to a fundamental problem of learning and  as such  is worthy of further investigation.

2 Deﬁnitions and Assumptions
We consider supervised learning problems where the input space X is arbitrary and the output space
Y consists of the set of all (cid:96)-dimensional multilabel vectors (y1  . . .   y(cid:96)) def= y where each yi ∈
{1  . . .   ri} for some ﬁnite positive integer ri. Each example (x  y) ∈ X ×Y is mapped to a joint
feature vector φφφ(x  y). Given a weight vector w in the space of joint feature vectors  the predicted
output yw(x) at input x ∈ X   is given by the output y maximizing the score F (w  x  y)  i.e. 

y∈Y

F (w  x  y)

yw(x) def= argmax

; where F (w  x  y) def= (cid:104)w  φφφ(x  y)(cid:105)  

(1)
and where (cid:104)· ·(cid:105) denotes the inner product in the joint feature space. Hence  yw(x) is obtained by
solving the so-called inference problem  which is known to be NP-hard for many output feature
maps [6  7]. Consequently  we aim at using an output feature map for which the inference prob-
lem can be solved by a polynomial time algorithm such as dynamic programming. The margin
Γ(w  x  y) achieved by predictor w at example (x  y) is deﬁned as 

Γ(w  x  y)

def= min
y(cid:48)(cid:54)=y

[F (w  x  y) − F (w  x  y(cid:48))] .

We consider the case where the feature map φφφ is a potential function for a Markov network deﬁned
by a complete graph G with (cid:96) nodes and (cid:96)((cid:96) − 1)/2 undirected edges. Each node i of G represents
an output variable yi and there exists an edge (i  j) of G for each pair (yi  yj) of output variables.
For any example (x  y) ∈ X × Y  its joint feature vector is given by

where ⊗ is the Kronecker product. Hence  any predictor w can be written as w = (wi j)(i j)∈G
where wi j is w’s weight on φφφi j(x  yi  yj). Therefore  for any w and any (x  y)  we have

(i j)∈G

(i j)∈G

 

F (w  x  y) = (cid:104)w  φφφ(x  y)(cid:105) =

(cid:104)wi j  φφφi j(x  yi  yj)(cid:105) =

Fi j(wi j  x  yi  yj)  

(i j)∈G

where we denote by Fi j(wi j  x  yi  yj) = (cid:104)wi j  φφφi j(x  yi  yj) the score of labeling the edge (i  j)
by (yi  yj) given input x.
For any vector a  let (cid:107)a(cid:107) denote its L2 norm. Throughout the paper  we make the assumption that
we have a normalized joint feature space such that (cid:107)φφφ(x  y)(cid:107) = 1 for all (x  y) ∈ X × Y and

(cid:107)φφφi j(x  yi  yj)(cid:107) is the same for all (i  j) ∈ G. Since the complete graph G has(cid:0)(cid:96)
that (cid:107)φφφi j(x  yi  yj)(cid:107)2 =(cid:0)(cid:96)

(cid:1)−1 for all (i  j) ∈ G.

(cid:1) edges  it follows

2

2

We also have a training set S def= {(x1  y1)  . . .   (xm  ym)} where each example is generated in-
dependently according to some unknown distribution D. Mathematically  we do not assume the
existence of a predictor w achieving some positive margin Γ(w  x  y) on each (x  y) ∈ S. Indeed 

2

φφφ(x  y) =(cid:0)φφφi j(x  yi  yj)(cid:1)
(cid:88)

(i j)∈G

=(cid:0)ϕϕϕ(x) ⊗ ψψψi j(yi  yj)(cid:1)
(cid:88)

for some S  there might not exist any w where Γ(w  x  y) > 0 for all (x  y) ∈ S. However  the
generalization guarantee will be best when w achieves a large margin on most training points.
Given any γ > 0  and any (x  y) ∈ X ×Y  the hinge loss (at scale γ) incurred on (x  y) by a unit L2
norm predictor w that achieves a (possibly negative) margin Γ(w  x  y) is given by Lγ(Γ(w  x  y)) 
where the so-called hinge loss function Lγ is deﬁned as Lγ(s) def= max (0  1 − s/γ) ∀s ∈ R . We
will also make use of the ramp loss function Aγ deﬁned by Aγ(s) def= min(1 Lγ(s)) ∀s ∈ R .
The proofs of all the rigorous results of this paper are provided in the supplementary material.

3 Superposition of Random Spanning Trees

Given a complete graph G of (cid:96) nodes (representing the Markov network)  let S(G) denote the set of
all (cid:96)(cid:96)−2 spanning trees of G. Recall that each spanning tree of G has (cid:96) − 1 edges. Hence  for any

edge (i  j) ∈ G  the number of trees in S(G) covering that edge (i  j) is given by (cid:96)(cid:96)−2((cid:96)−1)/(cid:0)(cid:96)

(cid:1) =

(2/(cid:96))(cid:96)(cid:96)−2. Therefore  for any function f of the edges of G we have

2

(cid:88)

(cid:88)

T∈S(G)

(i j)∈T

f ((i  j)) = (cid:96)(cid:96)−2 2
(cid:96)

f ((i  j)) .

(cid:88)

(i j)∈G

Given any spanning tree T of G and given any predictor w  let wT denote the projection of w on the
edges of T . Namely  (wT )i j = wi j if (i  j) ∈ T   and (wT )i j = 0 otherwise. Let us also denote
by φφφT (x  y)  the projection of φφφ(x  y) on the edges of T . Namely  (φφφT (x  y))i j = φφφi j(x  yi  yj)

if (i  j) ∈ T   and (φφφT (x  y))i j = 0 otherwise. Recall that (cid:107)φφφi j(x  yi  yj)(cid:107)2 =(cid:0)(cid:96)

(cid:1)−1 ∀(i  j) ∈ G.

Thus  for all (x  y) ∈ X × Y and for all T ∈ S(G)  we have

2

(cid:107)φφφT (x  y)(cid:107)2 =

(cid:107)φφφi j(x  yi  yj)(cid:107)2 =

(cid:88)

(i j)∈T

(cid:96) − 1(cid:0)(cid:96)
(cid:1) =

2

2
(cid:96)

.

(cid:114)

We now establish how F (w  x  y) can be written as an expectation over all the spanning trees of G.
= φφφT /(cid:107)φφφT(cid:107). Let U(G) denote the uniform distribution on
def
Lemma 1. Let ˆwT
S(G). Then  we have

= wT /(cid:107)wT(cid:107)  ˆφφφT

def

F (w  x  y) =

E

T∼U (G)

Moreover  for any w such that (cid:107)w(cid:107) = 1  we have: E

aT(cid:104) ˆwT   ˆφφφT (x  y)(cid:105)  where aT

(cid:107)wT(cid:107) .

def
=

(cid:96)
2
T = 1  and E
a2

T∼U (G)

aT ≤ 1 .

T∼U (G)

Let T def= {T1  . . .   Tn} be a sample of n spanning trees of G where each Ti is sampled independently
according to U(G). Given any unit L2 norm predictor w on the complete graph G  our task is to
investigate how the margins Γ(w  x  y)  for each (x  y) ∈ X ×Y  will be modiﬁed if we approximate
the (true) expectation over all spanning trees by an average over the sample T .
For this task  we consider any (x  y) and any w of unit L2 norm. Let FT (w  x  y) denote the
estimation of F (w  x  y) on the tree sample T  
1
n

aTi(cid:104) ˆwTi  ˆφφφTi(x  y)(cid:105)  

FT (w  x  y)

n(cid:88)

def=

and let ΓT (w  x  y) denote the estimation of Γ(w  x  y) on the tree sample T  
[FT (w  x  y) − FT (w  x  y(cid:48))] .

ΓT (w  x  y) def= min
y(cid:48)(cid:54)=y

i=1

The following lemma states how ΓT relates to Γ.
Lemma 2. Consider any unit L2 norm predictor w on the complete graph G that achieves a margin
of Γ(w  x  y) for each (x  y) ∈ X × Y  then we have

ΓT (w  x  y) ≥ Γ(w  x  y) − 2 ∀(x  y) ∈ X × Y  
whenever we have |FT (w  x  y) − F (w  x  y)| ≤  for all (x  y) ∈ X × Y.

3

√

Lemma 2 has important consequences whenever |FT (w  x  y) − F (w  x  y)| ≤  for all (x  y) ∈
X × Y. Indeed  if w achieves a hard margin Γ(w  x  y) ≥ γ > 0 for all (x  y) ∈ S  then we have
that w also achieves a hard margin of ΓT (w  x  y) ≥ γ − 2 on each (x  y) ∈ S when using the tree
sample T instead of the full graph G. More generally  if w achieves a ramp loss of Aγ(Γ(w  x  y))
for each (x  y) ∈ X ×Y  then w achieves a ramp loss of Aγ(ΓT (w  x  y)) ≤ Aγ (Γ(w  x  y) − 2)
for all (x  y) ∈ X × Y when using the tree sample T instead of the full graph G. This last property
follows directly from the fact that Aγ(s) is a non-increasing function of s.
n) dependence  a sample of n ∈ Θ((cid:96)2/2)
The next lemma tells us that  apart from a slow ln2(
spanning trees is sufﬁcient to assure that the condition of Lemma 2 holds with high probability for all
(x  y) ∈ X × Y. Such a fast convergence rate was made possible by using PAC-Bayesian methods
which  in our case  prevented us of using the union bound over all possible y ∈ Y.
Lemma 3. Consider any  > 0 and any unit L2 norm predictor w for the complete graph G acting
on a normalized joint feature space. For any δ ∈ (0  1)  let
√
8

(2)
Then with probability of at least 1 − δ/2 over all samples T generated according to U(G)n  we
have  simultaneously for all (x  y) ∈ X × Y  that |FT (w  x  y) − F (w  x  y)| ≤ .
Given a sample T of n spanning trees of G  we now consider an arbitrary set W def= { ˆwT1   . . .   ˆwTn}
of unit L2 norm weight vectors where each ˆwTi operates on a unit L2 norm feature vector ˆφφφTi(x  y).
For any T and any such set W  we consider an arbitrary unit L2 norm conical combination of each
i = 1 and
each qi ≥ 0. Given any (x  y) and any T   we deﬁne the score FT (W  q  x  y) achieved on (x  y)
by the conical combination (W  q) on T as
FT (W  q  x  y) def=

weight in W realized by a n-dimensional weight vector q def= (q1  . . .   qn)  where(cid:80)n

qi(cid:104) ˆwTi  ˆφφφTi(x  y)(cid:105)  

n ≥ (cid:96)2
2

(cid:18) 1

n(cid:88)

(cid:19)2

i=1 q2

+

ln

n

δ

.

(3)

1
2

16

1√
n

i=1

that(cid:80)n

√

i=1 qi can be as large as

n denominator ensures that we always have FT (W  q  x  y) ≤ 1 in view of the fact
where the
n. Note also that FT (W  q  x  y) is the score of the feature vector
obtained by the concatenation of all the weight vectors in W (and weighted by q) acting on a feature
n. Hence  given T   we deﬁne the
vector obtained by concatenating each ˆφφφTi multiplied by 1/
margin ΓT (W  q  x  y) achieved on (x  y) by the conical combination (W  q) on T as

√

√

ΓT (W  q  x  y) def= min
y(cid:48)(cid:54)=y

[FT (W  q  x  y) − FT (W  q  x  y(cid:48))] .

(4)
For any unit L2 norm predictor w that achieves a margin of Γ(w  x  y) for all (x  y) ∈ X × Y  we
now show that there exists  with high probability  a unit L2 norm conical combination (W  q) on T
achieving margins that are not much smaller than Γ(w  x  y).
Theorem 4. Consider any unit L2 norm predictor w for the complete graph G  acting on a normal-
ized joint feature space  achieving a margin of Γ(w  x  y) for each (x  y) ∈ X × Y. Then for any
 > 0  and any n satisfying Lemma 3  for any δ ∈ (0  1]  with probability of at least 1 − δ over all
samples T generated according to U(G)n  there exists a unit L2 norm conical combination (W  q)
on T such that  simultaneously for all (x  y) ∈ X × Y  we have

ΓT (W  q  x  y) ≥

1√
1 + 

[Γ(w  x  y) − 2] .

From Theorem 4  and since Aγ(s) is a non-increasing function of s  it follows that  with proba-
bility at least 1 − δ over the random draws of T ∼ U(G)n  there exists (W  q) on T such that 
simultaneously for all ∀(x  y) ∈ X × Y  for any n satisfying Lemma 3 we have

Aγ(ΓT (W  q  x  y)) ≤ Aγ(cid:16)

[Γ(w  x  y) − 2] (1 + )−1/2(cid:17)

.

Hence  instead of searching for a predictor w for the complete graph G that achieves a small ex-
pected ramp loss E(x y)∼DAγ(Γ(w  x  y)  Theorem 4 tells us that we can settle the search for a

4

unit L2 norm conical combination (W  q) on a sample T of randomly-generated spanning trees of
G that achieves small E(x y)∼DAγ(ΓT (W  q  x  y)). But recall that ΓT (W  q  x  y)) is the margin
of a weight vector obtained by the concatenation of all the weight vectors in W (weighted by q) on
√
n)ˆφφφTi. It thus follows
a feature vector obtained by the concatenation of the n feature vectors (1/
that any standard risk bound for the SVM applies directly to E(x y)∼DAγ(ΓT (W  q  x  y)). Hence 
by adapting the SVM risk bound of [8]  we have the following result.
Theorem 5. Consider any sample T of n spanning trees of the complete graph G. For any γ > 0
and any 0 < δ ≤ 1  with probability of at least 1 − δ over the random draws of S ∼ Dm 
simultaneously for all unit L2 norm conical combinations (W  q) on T   we have

E

(x y)∼D

Aγ(ΓT (W  q  x  y)) ≤ 1
m

Aγ(ΓT (W  q  xi  yi)) +

√
2

γ

m

+ 3

ln(2/δ)

2m

.

(cid:114)

m(cid:88)

i=1

Hence  according to this theorem  the conical combination (W  q) having the best generalization
guarantee is the one which minimizes the sum of the ﬁrst two terms on the right hand side of
the inequality. Note that the theorem is still valid if we replace  in the empirical risk term  the
non-convex ramp loss Aγ by the convex hinge loss Lγ. This provides the theoretical basis of the
proposed optimization problem for learning (W  q) on the sample T .

4 A L2-Norm Random Spanning Tree Approximation Approach

def= γ · Lγ(ΓT (W  q  xk  yk)  Theorem 5 suggests that
If we introduce the usual slack variables ξk
we should minimize 1
k=1 ξk for some ﬁxed margin value γ > 0. Rather than performing this
γ
task for several values of γ  we show in the supplementary material that we can  equivalently  solve
the following optimization problem for several values of C > 0.
Deﬁnition 6. Primal L2-norm Random Tree Approximation.

(cid:80)m

n(cid:88)
n(cid:88)

i=1

1
2

min
wTi  ξk

s.t.

||wTi||2

2 + C

ξk

m(cid:88)

k=1

(cid:104)wTi  ˆφφφTi (xk  yk)(cid:105) − max
y(cid:54)=yk

i=1

ξk ≥ 0  ∀ k ∈ {1  . . .   m} 

n(cid:88)

i=1

(cid:104)wTi  ˆφφφTi(xk  y)(cid:105) ≥ 1 − ξk 

where {wTi|Ti ∈ T } are the feature weights to be learned on each tree  ξk is the margin slack
allocated for each xk  and C is the slack parameter that controls the amount of regularization.

This primal form has the interpretation of maximizing the joint margins from individual trees be-
tween (correct) training examples and all the other (incorrect) examples.
The key for the efﬁcient optimization is solving the ’argmax’ problem efﬁciently. In particular  we
note that the space of all multilabels is exponential in size  thus forbidding exhaustive enumeration
over it. In the following  we show how exact inference over a collection T of trees can be imple-
(cid:80)n
mented in Θ(Kn(cid:96)) time per data point  where K is the smallest number such that the average score
i=1(cid:104)wTi  ˆφφφTi (x  y)(cid:105).
of the K’th best multilabel for each tree of T is at most FT (x  y) def= 1
Whenever K is polynomial in the number of labels  this gives us exact polynomial-time inference
over the ensemble of trees.

n

4.1 Fast inference over a collection of trees

It is well known that the exact solution to the inference problem

ˆyTi(x) = argmax

y∈Y

FwTi

(x  y) def= argmax

y∈Y

(cid:104)wTi   ˆφφφTi(x  y)(cid:105) 

(5)

on an individual tree Ti can be obtained in Θ((cid:96)) time by dynamic programming. However  there is
no guarantee that the maximizer ˆyTi of Equation (5) is also a maximizer of FT . In practice  ˆyTi

5

can differ for each spanning tree Ti ∈ T . Hence  instead of using only the best scoring multil-
abel ˆyTi from each individual Ti ∈ T   we consider the set of the K highest scoring multilabels
YTi K = {ˆyTi 1 ···   ˆyTi K} of FwTi
(x  y). In the supplementary material we describe a dynamic
programming to ﬁnd the K highest multilabels in Θ(K(cid:96)) time. Running this algorithm for all of the
trees gives us a candidate set of Θ(Kn) multilabels YT  K = YT1 K ∪ ··· ∪ YTn K. We now state a
key lemma that will enable us to verify if the candidate set contains the maximizer of FT .
FT (x  y) be the highest scoring multilabel in YT  K. Suppose that
Lemma 7. Let y(cid:63)

K = argmax
y∈YT  K

FT (x  y(cid:63)

K) ≥ 1
n

FwTi

(x  yTi K)

def
= θx(K).

It follows that FT (x  y(cid:63)

K) = maxy∈Y FT (x  y).

n(cid:88)

i=1

K is a

We can use any K satisfying the lemma as the length of K-best lists  and be assured that y(cid:63)
maximizer of FT .
We now examine the conditions under which the highest scoring multilabel is present in our can-
didate set YT  K with high probability. For any x ∈ X and any predictor w  let ˆy def= yw(x) def=
F (w  x  y) be the highest scoring multilabel in Y for predictor w on the complete graph G.
argmax
y∈Y
For any y ∈ Y  let KT (y) be the rank of y in tree T and let ρT (y) def= KT (y)/|Y| be the normalized
rank of y in tree T . We then have 0 < ρT (y) ≤ 1 and ρT (y(cid:48)) = miny∈Y ρT (y) whenever y(cid:48) is a
highest scoring multilabel in tree T . Since w and x are arbitrary and ﬁxed  let us drop them momen-
tarily from the notation and let F (y) def= F (w  x  y)  and FT (y) def= FwT (x  y). Let U(Y) denote the
uniform distribution of multilabels on Y. Then  let µT
Let T ∼ U(G)n be a sample of n spanning trees of G. Since the scoring function FT of each tree
T of G is bounded in absolute value  it follows that FT is a σT -sub-Gaussian random variable for
some σT > 0. We now show that  with high probability  there exists a tree T ∈ T such that ρT (ˆy)
is decreasing exponentially rapidly with (F (ˆy) − µ)/σ  where σ2 def= ET∼U (G)σ2
T .
Lemma 8. Let the scoring function FT of each spanning tree of G be a σT -sub-Gaussian random
variable under the uniform distribution of labels; i.e.  for each T on G  there exists σT > 0 such
that for any λ > 0 we have

def= Ey∼U (Y)FT (y) and µ def= ET∼U (G)µT .

Let σ2 def

= E

T∼U (G)

T   and let α
σ2

(cid:18)

Pr

T ∼U (G)n

eλ(FT (y)−µT ) ≤ e

λ2
2 σ2

T .

(cid:16)

E

y∼U (Y)
def
= Pr

T∼U (G)

(cid:19)

∃T ∈ T : ρT (ˆy) ≤ e

− 1

2

(F (ˆy)−µ)2

σ2

≥ 1 − (1 − α)n .

µT ≤ µ ∧ FT (ˆy) ≥ F (ˆy) ∧ σ2

. Then 

T ≤ σ2(cid:17)

Thus  even for very small α  when n is large enough  there exists  with high probability  a tree T ∈ T
such that ˆy has a small ρT (ˆy) whenever [F (ˆy) − µ]/σ is large for G. For example  when |Y| = 2(cid:96)
(the multiple binary classiﬁcation case)  we have with probability of at least 1− (1− α)n  that there
exists T ∈ T such that KT (ˆy) = 1 whenever F (ˆy) − µ ≥ σ

2(cid:96) ln 2.

√

4.2 Optimization

To optimize the L2-norm RTA problem (Deﬁnition 6) we convert it to the marginalized dual form
(see the supplementary material for the derivation)  which gives us a polynomial-size problem (in
the number of microlabels) and allows us to use kernels to tackle complex input spaces efﬁciently.
Deﬁnition 9. L2-norm RTA Marginalized Dual

max
µµµ∈Mm

1
|ET |

µ(k  e  ue) − 1
2

µ(k  e  ue)K eT (xk  ue; x(cid:48)

k  u(cid:48)

e)µ(k(cid:48)  e  u(cid:48)

e)  

(cid:88)

e k ue

where ET is the union of the sets of edges appearing in T   and µµµ ∈ Mm are the marginal dual
def
= (µ(k  e  ue))k e ue  with the triplet (k  e  ue) corresponding to labeling the edge
variables µµµ

(cid:88)

e k ue 
k(cid:48) u(cid:48)

e

6

MICROLABEL LOSS (%)

DATASET
EMOTIONS

YEAST
SCENE
ENRON
CAL500

SVM MTL MMCRF MAM RTA
18.8
22.4
19.8
20.0
8.8
9.8
5.3
6.4
13.7
13.8
FINGERPRINT 10.3
10.7
14.9
15.3
2.1
2.6
0.6
4.7
5.7
3.8

NCI60
MEDICAL
CIRCLE10
CIRCLE50

20.2
20.7
11.6
6.5
13.8
17.3
16.0
2.6
6.3
6.2

20.1
21.7
18.4
6.2
13.7
10.5
14.6
2.1
2.6
1.5

19.5
20.1
17.0
5.0
13.7
10.5
14.3
2.1
2.5
2.1

0/1 LOSS (%)

MTL MMCRF MAM
69.6
74.5
86.0
88.7
55.2
94.6
87.9
99.6
100.0
100.0
99.6
100.0
53.0
60.0
63.1
91.8
17.7
33.2
46.2
72.3

71.3
93.0
72.2
92.7
100.0
99.6
63.1
63.8
20.3
38.8

SVM
77.8
85.9
47.2
99.6
100.0
99.0
56.9
91.8
28.9
69.8

RTA
66.3
77.7
30.2
87.7
100.0
96.7
52.9
58.8
4.0
52.8

Table 1: Prediction performance of each algorithm in terms of microlabel loss and 0/1 loss. The best
performing algorithm is highlighted with boldface  the second best is in italic.

e = (v  v(cid:48)) ∈ ET of the output graph by ue = (uv  uv(cid:48))∈Yv×Yv(cid:48) for the training example xk. Also 
Mm is the marginal dual feasible set and
(cid:48)
K eT (xk  ue; xk(cid:48)   u
e)
is the joint kernel of input features and the differences of output features of true and competing
multilabels (yk  u)  projected to the edge e. Finally  NT (e) denotes the number of times e appears
among the trees of the ensemble.

|ET |2 K(xk  xk(cid:48) )(cid:10)ψψψe(ykv  ykv(cid:48) ) − ψψψe(uv  uv(cid:48) )  ψψψe(yk(cid:48)v  yk(cid:48)v(cid:48) ) − ψψψe(u

v(cid:48) )(cid:11)

NT (e)

(cid:48)
v  u

def
=

(cid:48)

The master algorithm described in the supplementary material iterates over each training example
until convergence. The processing of each training example xk proceeds by ﬁnding the worst vio-
lating multilabel of the ensemble deﬁned as

¯yk

def= argmax
y(cid:54)=yk

FT (xk  y)  

(6)

using the K-best inference approach of the previous section  with the modiﬁcation that the correct
multilabel is excluded from the K-best lists. The worst violator ¯yk is mapped to a vertex

¯µµµ(xk) = C · ([¯ye = ue])e ue

∈ Mk

corresponding to the steepest feasible ascent direction (c.f  [9]) in the marginal dual feasible set Mk
of example xk  thus giving us a subgradient of the objective of Deﬁnition 9. An exact line search is
used to ﬁnd the saddle point between the current solution and ¯µµµ.

5 Empirical Evaluation

We compare our method RTA to Support Vector Machine (SVM) [10  11]  Multitask Feature Learn-
ing (MTL) [12]  Max-Margin Conditional Random Fields (MMCRF) [9] which uses the loopy be-
lief propagation algorithm for approximate inference on the general graph  and Maximum Average
Marginal Aggregation (MAM) [5] which is a multilabel ensemble model that trains a set of random
tree based learners separately and performs the ﬁnal approximate inference on a union graph of the
edge potential functions of the trees. We use ten multilabel datasets from [5]. Following [5]  MAM
is constructed with 180 tree based learners  and for MMCRF a consensus graph is created by pool-
ing edges from 40 trees. We train RTA with up to 40 spanning trees and with K up to 32. The linear
kernel is used for methods that require kernelized input. Margin slack parameters are selected from
{100  50  10  1  0.5  0.1  0.01}. We use 5-fold cross-validation to compute the results.
Prediction performance. Table 1 shows the performance in terms of microlabel loss and 0/1 loss.
The best methods are highlighted in ’boldface’ and the second best in ’italics’ (see supplementary
material for full results). RTA quite often improves over MAM in 0/1 accuracy  sometimes with
noticeable margin except for Enron and Circle50. The performances in microlabel accuracy are
quite similar while RTA is slightly above the competition. This demonstrates the advantage of RTA
that gains by optimizing on a collection of trees simultaneously rather than optimizing on individual
trees as MAM. In addition  learning using approximate inference on a general graph seems less

7

Figure 1: Percentage of examples with provably optimal y∗ being in the K-best lists plotted as a
function of K  scaled with respect to the number of microlabels in the dataset.

favorable as the tree-based methods  as MMCRF quite consistently trails to RTA and MAM in
both microlabel and 0/1 error  except for Circle50 where it outperforms other models. Finally  we
notice that SVM  as a single label classiﬁer  is very competitive against most multilabel methods for
microlabel accuracy.
Exactness of inference on the collection of trees. We now study the empirical behavior of the
inference (see Section 4) on the collection of trees  which  if taken as a single general graph  would
call for solving an NP-hard inference problem. We provide here empirical evidence that we can
perform exact inference on most examples in most datasets in polynomial time.
We ran the K-best inference on eleven datasets where the RTA models were trained with different
amounts of spanning trees |T | ={5  10  40} and values for K ={2  4  8  16  32  40  60}. For each pa-
rameter combination and for each example  we recorded whether the K-best inference was provably
exact on the collection (i.e.  if Lemma 7 was satisﬁed). Figure 1 plots the percentage of examples
where the inference was indeed provably exact. The values are shown as a function of K  expressed
as the percentage of the number of microlabels in each dataset. Hence  100% means K = (cid:96)  which
denotes low polynomial (Θ(n(cid:96)2)) time inference in the exponential size multilabel space.
We observe  from Figure 1  on some datasets (e.g.  Medical  NCI60)  that the inference task is very
easy since exact inference can be computed for most of the examples even with K values that are
below 50% of the number of microlabels. By setting K = (cid:96) (i.e.  100%) we can perform exact
inference for about 90% of the examples on nine datasets with ﬁve trees  and eight datasets with
40 trees. On two of the datasets (Cal500  Circle50)  inference is not (in general) exact with low
values of K. Allowing K to grow superlinearly on (cid:96) would possibly permit exact inference on these
datasets. However  this is left for future studies.
Finally  we note that the difﬁculty of performing provably exact inference slightly increases when
more spanning trees are used. We have observed that  in most cases  the optimal multilabel y∗ is
still on the K-best lists but the conditions of Lemma 7 are no longer satisﬁed  hence forbidding us
to prove exactness of the inference. Thus  working to establish alternative proofs of exactness is a
worthy future research direction.

6 Conclusion

The main theoretical result of the paper is the demonstration that if a large margin structured output
predictor exists  then combining a small sample of random trees will  with high probability  generate
a predictor with good generalization. The key attraction of this approach is the tractability of the
inference problem for the ensemble of trees  both indicated by our theoretical analysis and supported
by our empirical results. However  as a by-product  we have a signiﬁcant added beneﬁt: we do not
need to know the output structure a priori as this is generated implicitly in the learned weights
for the trees. This is used to signiﬁcant advantage in our experiments that automatically leverage
correlations between the multiple target outputs to give a substantive increase in accuracy. It also
suggests that the approach has enormous potential for applications where the structure of the output
is not known but is expected to play an important role.

8

lllllll020406080100|T| = 5 K (% of |Y|) Y* being verified (% of data)llEmotionsYeastSceneEnronCal500FingerprintNCI60MedicalCircle10Circle50131032100316100013103210031610001310321003161000131032100316100013103210031610001310321003161000131032100316100013103210031610001310321003161000lllllll1310321003161000lllllll020406080100|T| = 10 K (% of |Y|) Y* being verified (% of data)131032100316100013103210031610001310321003161000131032100316100013103210031610001310321003161000131032100316100013103210031610001310321003161000lllllll1310321003161000lllllll020406080100|T| = 40 K (% of |Y|) Y* being verified (% of data)131032100316100013103210031610001310321003161000131032100316100013103210031610001310321003161000131032100316100013103210031610001310321003161000lllllll1310321003161000References
[1] Ben Taskar  Carlos Guestrin  and Daphne Koller. Max-margin markov networks. In S. Thrun 
L.K. Saul  and B. Sch¨olkopf  editors  Advances in Neural Information Processing Systems 16 
pages 25–32. MIT Press  2004.

[2] Martin J. Wainwright  Tommy S. Jaakkola  and Alan S. Willsky. MAP estimation via agree-
ment on trees: message-passing and linear programming. IEEE Transactions on Information
Theory  51(11):3697–3717  2005.

[3] Michael I. Jordan and Martin J Wainwright. Semideﬁnite relaxations for approximate inference
on graphs with cycles. In S. Thrun  L.K. Saul  and B. Sch¨olkopf  editors  Advances in Neural
Information Processing Systems 16  pages 369–376. MIT Press  2004.

[4] Amir Globerson and Tommi S. Jaakkola. Approximate inference using planar graph decom-
position. In B. Sch¨olkopf  J.C. Platt  and T. Hoffman  editors  Advances in Neural Information
Processing Systems 19  pages 473–480. MIT Press  2007.

[5] Hongyu Su and Juho Rousu. Multilabel classiﬁcation through random graph ensembles. Ma-

chine Learning  dx.doi.org/10.1007/s10994-014-5465-9  2014.

[6] Robert G. Cowell  A. Philip Dawid  Steffen L. Lauritzen  and David J. Spiegelhalter. Proba-

bilistic Networks and Expert Systems. Springer  New York  1999.

[7] Thomas G¨artner and Shankar Vembu. On structured output training: hard cases and an efﬁcient

alternative. Machine Learning  79:227–242  2009.

[8] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge

University Press  2004.

[9] J. Rousu  C. Saunders  S. Szedmak  and J. Shawe-Taylor. Efﬁcient algorithms for max-margin

structured classiﬁcation. Predicting Structured Data  pages 105–129  2007.

[10] Kristin P. Bennett. Combining support vector and mathematical programming methods for
classiﬁcations. In B. Sch¨olkopf  C. J. C. Burges  and A. J. Smola  editors  Advances in Kernel
Methods—Support Vector Learning  pages 307–326. MIT Press  Cambridge  MA  1999.

[11] Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and
Other Kernel-Based Learning Methods. Cambridge University Press  Cambridge  U.K.  2000.
[12] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Convex multi-task feature

learning. Machine Learning  73(3):243–272  2008.

[13] Yevgeny Seldin  Franc¸ois Laviolette  Nicol`o Cesa-Bianchi  John Shawe-Taylor  and Peter
Auer. PAC-Bayesian inequalities for martingales. IEEE Transactions on Information Theory 
58:7086–7093  2012.

[14] Andreas Maurer. A note on the PAC Bayesian theorem. CoRR  cs.LG/0411099  2004.
[15] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning  51:5–21 

2003.

[16] Juho Rousu  Craig Saunders  Sandor Szedmak  and John Shawe-Taylor. Kernel-based learn-
ing of hierarchical multilabel classiﬁcation models. Journal of Machine Learning Research 
7:1601–1626  December 2006.

9

,Mario Marchand
Hongyu Su
Emilie Morvant
Juho Rousu
John Shawe-Taylor
Yoshinobu Kawahara