2016,Designing smoothing functions for improved worst-case competitive ratio in online optimization,Online optimization covers problems such as online resource allocation  online bipartite matching  adwords (a central problem in e-commerce and advertising)  and adwords with separable concave returns.  We analyze the worst case competitive ratio of two primal-dual algorithms for a class of online convex (conic) optimization problems that contains the previous examples as special cases defined on the positive orthant.  We derive a sufficient condition on the objective function that guarantees a constant worst case competitive ratio (greater than or equal to $\frac{1}{2}$) for monotone objective functions.  We provide new examples of online problems on the positive orthant % and the positive semidefinite cone  that satisfy the sufficient condition.  We show how smoothing can improve the competitive ratio of these algorithms  and in particular for separable functions  we show that the optimal smoothing can be derived by solving a convex optimization problem. This result allows us to directly optimize the competitive ratio bound over a class of smoothing functions  and hence  design effective smoothing customized for a given cost function.,Designing smoothing functions for improved

worst-case competitive ratio in online optimization

Reza Eghbali

Department of Electrical Engineering

University of Washington

Seattle  WA 98195
eghbali@uw.edu

Maryam Fazel

Department of Electrical Engineering

University of Washington

Seattle  WA 98195
mfazel@uw.edu

Abstract

Online optimization covers problems such as online resource allocation  online
bipartite matching  adwords (a central problem in e-commerce and advertising) 
and adwords with separable concave returns. We analyze the worst case com-
petitive ratio of two primal-dual algorithms for a class of online convex (conic)
optimization problems that contains the previous examples as special cases deﬁned
on the positive orthant. We derive a sufﬁcient condition on the objective function
that guarantees a constant worst case competitive ratio (greater than or equal to
2) for monotone objective functions. We provide new examples of online prob-
1
lems on the positive orthant that satisfy the sufﬁcient condition. We show how
smoothing can improve the competitive ratio of these algorithms  and in particular
for separable functions  we show that the optimal smoothing can be derived by
solving a convex optimization problem. This result allows us to directly optimize
the competitive ratio bound over a class of smoothing functions  and hence design
effective smoothing customized for a given cost function.

ψ ((cid:80)m

xt ∈ Ft 

maximize
subject to

t=1 Atxt)

∀t ∈ [m] 

Introduction

1
Given a proper convex cone K ⊂ Rn  let ψ : K (cid:55)→ R be an upper semi-continuous concave function.
Consider the optimization problem

(1)

where for all t ∈ [m] := {1  2  . . .   m}  xt ∈ Rl are the optimization variables and Ft are compact
convex constraint sets. We assume At ∈ Rn×l maps Ft to K; for example  when K = Rn
+ and
Ft ⊂ Rl
+  this assumption is satisﬁed if At has nonnegative entries. We consider problem (1) in the
online setting  where it can be viewed as a sequential game between a player (online algorithm) and
an adversary. At each step t  the adversary reveals At  Ft and the algorithm chooses ˆxt ∈ Ft. The
performance of the algorithm is measured by its competitive ratio  i.e.  the ratio of objective value
at ˆx1  . . .   ˆxm to the ofﬂine optimum. Problem (1) covers (convex relaxations of) various online
combinatorial problems including online bipartite matching [14]  the “adwords” problem [16]  and
the secretary problem [15]. More generally  it covers online linear programming (LP) [6]  online
packing/covering with convex cost [3  4  7]  and generalization of adwords [8]. In this paper  we
study the case where ∂ψ(u) ⊂ K∗ for all u ∈ K  i.e.  ψ is monotone with respect to the cone K.
The competitive performance of online algorithms has been studied mainly under the worst-case
model (e.g.  in [16]) or stochastic models (e.g.  in [15]). In the worst-case model one is interested
in lower bounds on the competitive ratio that hold for any (A1  F1)  . . .   (Am  Fm). In stochas-
tic models  adversary choses a probability distribution from a family of distributions to generate

(A1  F1)  . . .   (Am  Fm)  and the competitive ratio is calculated using the expected value of the
algorithm’s objective value. Online bipartite matching and its generalization  the “adwords” problem 
are the two main problems that have been studied under the worst case model. The greedy algorithm
achieves a competitive ratio of 1/2 while the optimal algorithm achieves a competitive ratio of 1−1/e
(as bid to budget ratio goes to zero) [16  5  14  13]. A more general version of Adwords in which
each agent (advertiser) has a concave cost has been studied in [8].
The majority of algorithms proposed for the problems mentioned above rely on a primal-dual
framework [5  6  3  8  4]. The differentiating point among the algorithms is the method of updating
the dual variable at each step  since once the dual variable is updated the primal variable can be
assigned using a simple complementarity condition. A simple and efﬁcient method of updating
the dual variable is through a ﬁrst order online learning step. For example  the algorithm stated in
[9] for online linear programming uses mirror descent with entropy regularization (multiplicative
weight updates algorithm) once written in the primal dual language. Recently  the work in [9] was
independently extended to random permutation model in [12  2  11]. In [2]  the authors provide
competitive difference bound for online convex optimization under random permutation model as a
function of the regret bound for the online learning algorithm applied to the dual.
In this paper  we consider two versions of the greedy algorithm for problem (1)  a sequential
update and a simultaneous update algorithm. The simultaneous update algorithm  Algorithm 2 
provides a direct saddle-point representation of what has been described informally in the literature
as “continuous updates” of primal and dual variables. This saddle point representation allows us to
generalize this type of updates to non-smooth function. In section 2  we bound the competitive ratios
of the two algorithms. A sufﬁcient condition on the objective function that guarantees a non-trivial
worst case competitive ratio is introduced. We show that the competitive ratio is at least 1
2 for a
monotone non-decreasing objective function. Examples that satisfy the sufﬁcient condition (on
the positive orthant and the positive semideﬁnite cone) are given. In section 3  we derive optimal
algorithms  as variants of greedy algorithm applied to a smoothed version of ψ. For example  Nesterov
smoothing provides optimal algorithm for the adwords problem. The main contribution of this paper
is to show how one can derive the optimal smoothing function (or from the dual point of view the
optimal regularization function) for separable ψ on positive orthant by solving a convex optimization
problem. This gives an implementable algorithm that achieves the optimal competitive ratio derived
in [8]. We also show how this convex optimization can be modiﬁed for the design of smoothing
function speciﬁcally for the sequential algorithm. In contrast  [8] only considers continuous updates.
The algorithms considered in this paper and their general analysis are the same as those we considered
in [10].
In [10]  the focus is on non-monotone functions and online problems on the positive
semideﬁnite cone. However  the focus of this paper is on monotone functions on the positive orthant.
In [10]  we only considered Nesterov smoothing and only derived competitive ratio bounds for the
simultaneous algorithm.
Notation. Given a function ψ : Rn (cid:55)→ R  ψ∗ denotes the concave conjugate of ψ deﬁned as
ψ∗(y) = inf u(cid:104)y  u(cid:105) − ψ(u)  for all y ∈ Rn. For a concave function ψ  ∂ψ(u) denotes the set of
supergradients of ψ at u  i.e.  the set of all y ∈ Rn such that ∀u(cid:48) ∈ Rn : ψ(u(cid:48)) ≤ (cid:104)y  u(cid:48) − u(cid:105)+ψ(u).
The set ∂ψ is related to the concave conjugate function ψ∗ as follows. For an upper semi-continuous
concave function ψ we have ∂ψ(u) = argminy(cid:104)y  u(cid:105) − ψ∗(y). A differentiable function ψ has
a Lipschitz continuous gradient with respect to (cid:107)·(cid:107) with continuity parameter 1/µ > 0 if for all
u  u(cid:48) ∈ Rn  (cid:107)∇ψ(u(cid:48)) − ∇ψ(u)(cid:107)∗ ≤ 1/µ(cid:107)u − u(cid:48)(cid:107)  where (cid:107)·(cid:107)∗ is the dual norm to (cid:107)·(cid:107).
The dual cone K∗ of a cone K ⊂ Rn is deﬁned as K∗ = {y | (cid:104)y  u(cid:105) ≥ 0 ∀u ∈ K}. Two examples
+ and the cone of n × n positive semideﬁnite matrices
of self-dual cones are the positive orthant Rn
+. A proper cone (pointed convex cone with nonempty interior) K induces a partial ordering on Rn
Sn
which is denoted by ≤K and is deﬁned as x ≤K y ⇔ y − x ∈ K.

1.1 Two primal-dual algorithms

The (Fenchel) dual problem for problem (1) is given by
t=1 σt(AT

(2)
where the optimization variable is y ∈ Rn  and σt denotes the support function for the set Ft deﬁned
as σt(z) = supx∈Ft(cid:104)x  z(cid:105). A pair (x∗  y∗) ∈ (F1 × . . . × Fm) × K∗ is an optimal primal-dual pair
if and only if

minimize (cid:80)m

t y) − ψ∗(y) 

2

t ∈ argmax
x∗
x∈Ft

(cid:104)x  AT

t y∗(cid:105) 

m(cid:88)

y∗ ∈ ∂ψ(

t=1

Atx∗
t ) 

∀t ∈ [m].

viewed as ˆyt+1 ∈ argminy(cid:104)(cid:80)t

Based on these optimality conditions  we consider two algorithms. Algorithm 1 updates the primal
and dual variables sequentially  by maintaining a dual variable ˆyt and using it to assign ˆxt ∈
argmaxx∈Ft(cid:104)x  AT
t ˆyt(cid:105). The then algorithm updates the dual variable based on the second optimality
condition. By the assignment rule  we have At ˆxt ∈ ∂σt(ˆyt)  and the dual variable update can be
s=1 As ˆxs  y(cid:105) − ψ∗(y). Therefore  the dual update is the same as the
update in dual averaging [18] or Follow The Regularized Leader (FTRL) [20  19  1] algorithm with
regularization −ψ∗(y).

Algorithm 1 Sequential Update

Initialize ˆy1 ∈ ∂ψ(0)
for t ← 1 to m do
Receive At  Ft
ˆxt ∈ argmaxx∈Ft(cid:104)x  AT
s=1 As ˆxs)

ˆyt+1 ∈ ∂ψ((cid:80)t

t ˆyt(cid:105)

end for

Algorithm 2 updates the primal and dual variables simultaneously  ensuring that

˜xt ∈ argmax
x∈Ft

(cid:104)x  AT

t ˜yt(cid:105) 

˜yt ∈ ∂ψ(

As ˜xs).

t(cid:88)

s=1

This algorithm is inherently more complicated than algorithm 1  since ﬁnding ˜xt involves solving a
saddle-point problem. This can be solved by a ﬁrst order method like mirror descent algorithm for
saddle point problems. In contrast  the primal and dual updates in algorithm 1 solve two separate
maximization and minimization problems 1.

Algorithm 2 Simultaneous Update

for t ← 1 to m do
Receive At  Ft

(˜yt  ˜xt) ∈ arg miny maxx∈Ft (cid:104)y  Atx +(cid:80)t−1

end for

s=1 As ˜xs(cid:105) − ψ∗(y)

2 Competitive ratio bounds and examples for ψ

In this section  we derive bounds on the competitive ratios of Algorithms 1 and 2 by bounding their
respective duality gaps. We begin by stating a sufﬁcient condition on ψ that leads to non-trivial
competitive ratios  and we assume this condition holds in the rest of the paper. Roughly  one can
interpret this assumption as having “diminishing returns” with respect to the ordering induced by a
cone. Examples of functions that satisfy this assumption will appear later in this section.
Assumption 1 Whenever u ≥K v  there exists y ∈ ∂ψ(u) that satisﬁes y ≤K∗ z for all z ∈ ∂ψ(v).
When ψ is differentiable  assumption 1 simpliﬁes to u ≥K v ⇒ ∇ψ(u) ≤K∗ ∇ψ(v). That is  the
gradient  as a map from Rn (equipped with ≤K) to Rn (equipped with ≤K∗)  is order-reversing.When
ψ is twice differentiable  assumption 1 is equivalent to (cid:104)w ∇2ψ(u)v(cid:105) ≤ 0  for all u  v  w ∈ K. For
+.
example  this is equivalent to Hessian being element-wise non-positive when K = Rn

Let deﬁne ˜ym+1 to be the minimum element in ∂ψ((cid:80)m
an element exists in the superdifferential by Assumption (1)). Let Pseq = ψ ((cid:80)m
ψ ((cid:80)m

t=1 At ˜xt) with respect to ordering ≤K∗ (such
t=1 At ˆxt) and Psim =
t=1 At ˜xt) denote the primal objective values for the primal solution produced by the algorithms
1Also if the original problem is a convex relaxation of an integer program  meaning that each Ft = convFt
where Ft ⊂ Zl  then ˆxt can always be chosen to be integral while integrality may not hold for the solution of
the second algorithm.

3

1 and 2  and Dseq =(cid:80)m

t=1 σt(AT

t ˆyt)− ψ∗(ˆym+1) and Dsim =(cid:80)m

the corresponding dual objective values.
The next lemma provides a lower bound on the duality gaps of both algorithms.

t=1 σt(AT

t ˜yt)− ψ∗(˜ym+1) denote

Lemma 1 The duality gaps for the two algorithms can be lower bounded as
Psim − Dsim ≥ ψ∗(˜ym+1) + ψ(0)  Pseq − Dseq ≥ ψ∗(ˆym+1) + ψ(0) +

m(cid:88)

(cid:104)At ˆxt  ˆyt+1 − ˆyt(cid:105)

Furthermore  if ψ has a Lipschitz continuous gradient with parameter 1/µ with respect to (cid:107)·(cid:107) 

Pseq − Dseq ≥ ψ∗(ˆym+1) + ψ(0) − 1

2µ

t=1

(cid:80)m
t=1 (cid:107)At ˆxt(cid:107)2 .

(3)

(4)

Note that right hand side of (3) is exactly the regret bound of the FTRL algorithm (with a negative
sign) [19]. The proof is given in the appendix. To simplify the notation in the rest of the paper  we
assume ψ(0) = 0 by replacing ψ(u) with ψ(u) − ψ(0). To quantify the competitive ratio of the
algorithms  we deﬁne αψ as

αψ = sup{c | ψ∗(y) ≥ cψ(u)  y ∈ ∂ψ(u)  u ∈ K} 

Since ψ∗(y) + ψ(u) = (cid:104)y  u(cid:105) for all y ∈ ∂ψ(u)  αψ is equivalent to

αψ = sup{c | (cid:104)y  u(cid:105) ≥ (c + 1)ψ(u)  y ∈ ∂ψ(u) u ∈ K}.

(5)
Note that −1 ≤ αψ ≤ 0  since for any u ∈ K and y ∈ ∂ψ(u)  by concavity of ψ and the fact
that y ∈ K∗  we have 0 ≤ (cid:104)y  u(cid:105) ≤ ψ(u) − ψ(0). If ψ is a linear function then αψ = 0  while if
0 ∈ ∂ψ(u) for some u ∈ K  then αψ = −1.
The next theorem provides lower bounds on the competitive ratio of the two algorithms.

Theorem 1 If Assumption 1 holds  we have
D(cid:63)  Pseq ≥

Psim ≥

1

1 − αψ

m(cid:88)

(cid:104)At ˆxt  ˆyt+1 − ˆyt(cid:105))

t=1

1

1 − αψ

(D(cid:63) +

(cid:80)m
t=1 (cid:107)At ˆxt(cid:107)2).

where D(cid:63) is the dual optimal objective. If ψ has a Lipschitz continuous gradient with parameter 1/µ
with respect to (cid:107)·(cid:107) 

Pseq ≥ 1
1−αψ

(D(cid:63) − 1

2µ

Proof: Consider the simultaneous update algorithm. We have(cid:80)t
t  since AsFs ⊂ K for all s. Since ˜yt ∈ ∂ψ((cid:80)t
element in ∂ψ((cid:80)m

s=1 As ˜xs for all
s=1 As ˜xs) and ˜ym+1 was picked to be the minimum
s=1 As ˜xs) with respect to ≤K∗  by Assumption 1  we have ˜yt ≥K∗ ˜ym+1. Since
m(cid:88)
t ˜ym). Thus

t ˜yt) − ψ∗(˜ym) ≥ m(cid:88)

t ˜ym+1) − ψ∗(˜ym) ≥ D∗.

t ˜yt) ≥ σt(AT

Atx ∈ K for all x ∈ Ft  we get (cid:104)Atx  ˜yt(cid:105) ≥ (cid:104)Atx  ˜ym+1(cid:105); therefore  σt(AT

s=1 As ˜xs ≤K

(cid:80)m

Dsim =

σt(AT

σt(AT

(6)

t=1

t=1

Now Lemma 1 and deﬁnition of αψ give the desired result. The proof for Algorithm 1 follows similar
(cid:3)
steps.
We now consider examples of ψ that satisfy Assumption 1 and derive lower bound on αψ for those
examples.
+ and note that K∗ = K. To simplify the notation we
Examples on positive orthant. Let K = Rn
use ≤ instead of ≤Rn
. Assumption 1 is satisﬁed for a twice differentiable function if and only if
the Hessian is element-wise non-positive over Rn
i=1 ψi(ui) 
Assumption 1 is satisﬁed since by concavity for each ψi we have ∂ψi(ui) ≤ ∂ψi(vi) when ui ≤ vi.
In the basic adwords problem  for all t  Ft = {x ∈ Rl
+ | 1T x ≤ 1}  At is a diagonal matrix with
non-negative entries  and
i=1(ui − 1)+ 

+. If ψ is separable  i.e.  ψ(u) = (cid:80)n

i=1 ui −(cid:80)n

ψ(u) =(cid:80)n

(7)

+

4

(cid:80)m
t=1(cid:104)At ˆxt  ˆyt+1 − ˆyt(cid:105) ≤ nr. Therefore  the competitive ratio of algorithm 1 goes to 1
For any p ≥ 1 let Bp be the lp-norm ball. We can rewrite the penalty function −(cid:80)n
the adwords objective using the distance from B∞: we have(cid:80)n

where (·)+ = max{·  0}. In this problem  ψ∗(y) = 1T (y − 1). Since 0 ∈ ∂ψ(1) we have αψ = −1
2. Let r = maxt i j At i j  then we have
by (5); therefore  the competitive ratio of algorithm 2 is 1
2 as r (bid to
budget ratio) goes to zero. In adwords with concave returns studied in [8]  At is diagonal for all t and
ψ is separable 2.
i=1(ui − 1)+ in
i=1(ui − 1)+ = d1(u B∞)  where
d1(·  C) is the l1 norm distance from set C. For p ∈ [1 ∞) the function −d1(u Bp) although not
separable it satisﬁes Assumption 1. The proof is given in the supplementary materials.
+ and note that K∗ = K. Two examples
Examples on the positive semideﬁnite cone. Let K = Sn
that satisfy Assumption 1 are ψ(U ) = log det(U + A0)  and ψ(U ) = trU p with p ∈ (0  1). We
refer the reader to [10] for examples of online problems that entails log det in the objective function
and competitive ratio analysis of the simultanuous algorithm for these problems.

3 Smoothing of ψ for improved competitive ratio

The technique of “smoothing” an (potentially non-smooth) objective function  or equivalently adding
a strongly convex regularization term to its conjugate function  has been used in several areas. In
convex optimization  a general version of this is due to Nesterov [17]  and has led to faster convergence
rates of ﬁrst order methods for non-smooth problems. In this section  we study how replacing ψ
with a appropriately smoothed function ψS helps improve the performance of the two algorithms
discussed in section 1.1  and show that it provides optimal competitive ratio for two of the problems
mentioned in section 2  adwords and online LP. We then show how to maximize the competitive
ratio of both algorithms for a separable ψ and compute the optimal smoothing by solving a convex
optimization problem. This allows us to design the most effective smoothing customized for a given
ψ: we maximize the bound on the competitive ratio over the set of smooth functions.(see subsection
3.2 for details).
Let ψS denote an upper semi-continuous concave function (a smoothed version of ψ)  and suppose
ψS satisﬁes Assumption 1. The algorithms we consider in this section are the same as Algorithms
1 and 2  but with ψ replacing ψS. Note that the competitive ratio is computed with respect to the
original problem  that is the ofﬂine primal and dual optimal values are still the same P (cid:63) and D(cid:63) as
before.
t=1 At ˆxt)−
t=1(cid:104)At ˆxt  ˆyt+1 − ˆyt(cid:105). To simplify the notation  assume ψS(0) = 0 as before. Deﬁne
αψ ψS = sup{c |ψ∗(y) ≥ ψS(u) + (c − 1)ψ(u)  y ∈ ∂ψS(u)  u ∈ K}.

From Lemma 1  we have that Dsim ≤ ψS ((cid:80)m
ψ∗(ˆym+1)−(cid:80)m

t=1 At ˜xt)−ψ∗(˜ym+1) and Dseq ≤ ψS ((cid:80)m

Then the conclusion of Theorem 1 for Algorithms 1 and 2 applied to the smoothed function holds
with αψ replaced by αψ ψS .

3.1 Nesterov Smoothing

We ﬁrst consider Nesterov smoothing [17]  and apply it to examples on non-negative orthant. Given a
proper upper semi-continuous concave function φ : Rn (cid:55)→ R ∪ {−∞}  let

ψS = (ψ∗ + φ∗)∗.

Note that ψS is the supremal convolution of ψ and φ. If ψ and φ are separable  then ψS satisﬁes
Assumption 1 for K = Rn
+. Here we provide example of Nesterov smoothing for functions on
non-negative orthant.
Adwords: The optimal competitive ratio for the Adwords problem is 1 − e−1. This is achieved by

smoothing ψ with φ∗(y) =(cid:80)m

(cid:40) eui−exp (ui)+1
e−1 ) log(e − (e − 1)yi) − 2yi  which gives

i=1(yi − e
ψS i(ui) − ψS i(0) =

e−1
1
e−1

ui ∈ [0  1]
ui > 1 

2Note that in this case one can remove the assumption that ∂ψi ⊂ R+ since if ˜yt i = 0 for some t and i 

then ˜xs i = 0 for all s ≥ t.

5

3.2 Computing optimal smoothing for separable functions on Rn

+

i=1 ψi(ui) and ψS(u) =(cid:80)n

smoothing. Given a separable monotone ψ(u) =(cid:80)n
We now tackle the problem of ﬁnding the optimal smoothing for separable functions on the positive
orthant  which as we show in an example at the end of this section is not necessarily given by Nesterov
i=1 ψS i(ui) on Rn
we have that αψ ψS ≥ mini αψi ψS i.
+
To simplify the notation  drop the index i and consider ψ : R+ (cid:55)→ R. We formulate the problem
(cid:82) u
of ﬁnding ψS to maximize αψ ψS as an optimization problem. In section 4 we discuss the relation
between this optimization method and the optimal algorithm presented in [8]. We set ψS(u) =
0 y(s)ds with y a continuous function (y ∈ C[0 ∞))  and state the inﬁnite dimensional convex
optimization problem with y as a variable 

minimize
subject to

β

(cid:82) u
0 y(s)ds − ψ∗(y(u)) ≤ βψ(u) 
y ∈ C[0 ∞) 

∀u ∈ [0 ∞)

(8)

where β = 1 − αψ ψS (theorem 1 describes the dependence of the competitive ratios on this
parameter). Note that we have not imposed any condition on y to be non-increasing (i.e.  the
corresponding ψS to be concave). The next lemma establishes that every feasible solution to the
problem (8) can be turned into a non-increasing solution.
Lemma 2 Let (y  β) be a feasible solution for problem (8) and deﬁne ¯y(t) = inf s≤t y(s). Then
(¯y  β) is also a feasible solution for problem (8).

In particular if (y  β) is an optimal solution  then so is (¯y  β). The proof is given in the supplement. Re-
visiting the adwords problem  we observe that the optimal solution is given by y(u) =
 
+
which is the derivative of the smooth function we derived using Nesterov smoothing in section 3.1.
The optimality of this y can be established by providing a dual certiﬁcate  a measure ν correspond-
ing to the inequality constraint  that together with y satisﬁes the optimality condition. If we set
dν = exp (1 − u)/(e − 1) du  the optimality conditions are satisﬁed with β = (1 − 1/e)−1. Also
note that if ψ plateaus (e.g.  as in the adwords objective)  then one can replace problem (8) with a
problem over a ﬁnite horizon.
Theorem 2 Suppose ψ(t) = c on [u(cid:48) ∞) (ψ plateaus). Then problem (8) is equivalent to

e−1

(cid:16) e−exp(u)

(cid:17)

minimize
subject to

β

(cid:82) u
0 y(s)ds − ψ∗(y(u)) ≤ βψ(u) 

y(u(cid:48)) = 0 

y ∈ C[0  u(cid:48)].

∀u ∈ [0  u(cid:48)]

(9)

So for a function ψ with a plateau  one can discretize problem (9) to get a ﬁnite dimensional problem 

minimize
subject to

β

h(cid:80)t

s=1 y[s] − ψ∗(y[t]) ≤ βψ(ht)

∀t ∈ [d]

(10)

y[d] = 0 

where h = u(cid:48)/d is the discretization step. Figure 1a shows the optimal smoothing for the piecewise
linear function ψ(u) = min(.75  u 
.5u + .25) by solving problem (10). We point out that the
optimal smoothing for this function is not given by Nesterov’s smoothing (even though the optimal
smoothing can be derived by Nesterov’s smoothing for a piecewise linear function with only two
pieces  like the adwords cost function). Figure 1d shows the difference between the conjugate of the
optimal smoothing function and ψ∗ for the piecewise linear function  which we can see is not concave.
We simulated the performance of the simultaneous algorithm on a dataset with n = m  Ft simplex 
and At diagonal. We varied m in the range from 1 to 30 and for each m calculated the the smallest
competitive ratio achieved by the algorithm over (10m)2 random permutation of A1  . . .   Am. Figure
1i depicts this quantity vs. m for the optimal smoothing and the Nesterov smoothing. For the Nesterov
√
smoothing we used the function φ∗(y) = (y − √
In cases where a bound umax on(cid:80)m
e√
e−1 ) log(

t=1 AtFt is known  we can restrict t to [0  umax] and discretize
problem (8) over this interval. However  the conclusion of Lemma 2 does not hold for a ﬁnite horizon

e − 1)y) − 3

√
e − (

2 y.

6

and we need to impose additional linear constraints y[t] ≤ y[t − 1] to ensure the monotonicity of
y. We ﬁnd the optimal smoothing for two examples of this kind: ψ(u) = log(1 + u) over [0  100]
(Figure 1b)  and ψ(u) =
u over [0  100] (Figure 1c). Figure 1e shows the competitive ratio achieved
with the optimal smoothing of ψ(u) = log(1 + u) over [0  umax] as a function of umax. Figure 1f
depicts this quantity for ψ(u) =

√

√

u.

3.3 Competitive ratio bound for the sequential algorithm

In this section we provide a lower bound on the competitive ratio of the sequential algorithm
(Algorithm 1). Then we modify Problem (8) to ﬁnd a smoothing function that optimizes this
competitive ratio bound for the sequential algorithm.

Theorem 3 Suppose ψS is differentiable on an open set containing K and satisﬁes Assumption 1. In
addition  suppose there exists c ∈ K such that AtFt ≤K c for all t  then

Pseq ≥

1

1 − αψ ψS + κc ψ ψS

D(cid:63) 

where κ is given by

κc ψ ψS = inf{r | (cid:104)c ∇ψS(0) − ∇ψS(u)(cid:105) ≤ rψ(u)  u ∈ K}
(cid:80)m
t=1(cid:104)At ˆxt  ˆyt − ˆyt+1(cid:105) ≤(cid:80)m

t=1(cid:104)c  ˆyt − ˆyt+1(cid:105) = (cid:104)c  ˆy0 − ˆym+1(cid:105)

Proof: Since ψS satisﬁes Assumption 1  we have ˆyt+1 ≤K∗ ˆyt. Therefore  we can write:

Now by combining the duality gap given by Lemma 1 with 11  we get Dseq ≤ ψS ((cid:80)m
ψ∗(ˆym+1)+(cid:104)c ∇ψS(0) − ∇ψS ((cid:80)m

(11)
t=1 At ˆxt) −
t=1 At ˆxt)(cid:105). The conclusion follows from the deﬁnition of αψ ψS  
κc ψ ψS and the fact that Dseq ≥ D(cid:63).
(cid:3)
Based on the result of the previous theorem we can modify the optimization problem set up in Section
3.2 for separable functions on Rn
+ to maximize the lower bound on the competitive ratio of the
sequential algorithm. Note that when ψ and ψS are separable  we have κc ψ ψS ≤ maxi κci ψi ψS i.
Therefore  similar to the previous section to simplify the notation we drop the index i and assume
ψ is a function of a scalar variable. The optimization problem for ﬁnding ψS that minimizes
κc ψ ψS − αψ ψS is as follows:

minimize
subject to

β

(cid:82) u
0 y(s)ds + c(ψ(cid:48)(0) − y(u)) − ψ∗(y(u)) ≤ βψ(u) 
y ∈ C[0 ∞).

∀u ∈ [0 ∞)

(12)

(cid:16) u−1

(cid:17)(cid:17)

(cid:16)

1 − exp

For adwords  the optimal solution is given by β =
1−exp(− 1
+
which gives a competitive ratio of 1 − exp
. In Figure 1h we have plotted the competitive
ratio achieved by solving problem 12 for ψ(u) = log det(1 + u) with umax = 100 as a function
of c. Figure 1g shows the competitive ratio as a function of c for the piecewise linear function
ψ(u) = min(.75  u  .5u + .25).

c+1 ) and y(u) = β

c+1

1+c

1

 

(cid:16) −1

(cid:17)

4 Discussion and Related Work

We discuss results and papers from two communities  computer science theory and machine learning 
related to this work.
Online optimization. In [8]  the authors proposed an optimal algorithm for adwords with differ-
entiable concave returns (see examples in section 2). Here  “optimal” means that they construct
an instance of the problem for which competitive ratio bound cannot be improved  hence showing
the bound is tight. The algorithm is stated and analyzed for a twice differentiable  separable ψ(u).
The assignment rule for primal variables in their proposed algorithm is explained as a continuous
process. A closer look reveals that this algorithm falls in the framework of algorithm 2  with the only
difference being that at each step  (˜xt  ˜yt) are chosen such that

˜xt ∈ argmax(cid:104)x  AT
∀i ∈ [n] :

t ˜yt(cid:105)

˜yt i = ∇ψi(vi(ui)) 

ui = ((cid:80)t

t=1 As ˜xs)i 

7

(a)

(d)

(g)
√

(b)

(e)

(h)

(c)

(f)

(i)

Figure 1: Optimal smoothing for ψ(u) = min(.75  u  .5u+.25) (a)  ψ(u) = log(1+u) over [0  100]
(b)  and ψ(u) =
u over [0  100] (c). The competitive ratio achieved by the optimal smoothing as a
S − ψ∗ for the piecewise linear
function of umax for ψ(u) = log(1 + u) (e) and ψ(u) =
function (d). The competitive ratio achieved by the optimal smoothing for the sequential algorithm as
a function of c for ψ(u) = min(.75  u  .5u + .25) (g) and ψ(u) = log(1 + u) with umax = 100 (h).
i  Competitive ratio of the simultaneous algorithm for ψ(u) = min(.75  u  .5u + .25) as a function
of m with optimal smoothing and Nesterov smoothing (see text).

u (f). ψ∗

√

where vi : R+ (cid:55)→ R+ is an increasing differentiable function given as a solution of a nonlinear
differential equation that involves ψi and may not necessarily have a closed form. The competitive
ratio is also given based on the differential equation. They prove that this gives the optimal competitive
ratio for the instances where ψ1 = ψ2 = . . . = ψm.
Note that this is equivalent of setting ψS i(ui) = ψ(vi(ui))). Since vi is nondecreasing ψS i is a
concave function. On the other hand  given a concave function ψS i(R+) ⊂ ψi(R+)  we can set
vi : R+ (cid:55)→ R+ as vi(u) = inf{z | ψi(z) ≥ ψS i(u)}. Our formulation in section 3.2 provides a
constructive way of ﬁnding the optimal smoothing. It also applies to non-smooth ψ.
Online learning. As mentioned before  the dual update in Algorithm 1 is the same as in Follow-the-
Regularized-Leader (FTRL) algorithm with −ψ∗ as the regularization. This primal dual perspective
has been used in [20] for design and analysis of online learning algorithms. In the online learning
literature  the goal is to derive a bound on regret that optimally depends on the horizon  m. The goal
in the current paper is to provide competitive ratio for the algorithm that depends on the function ψ.
Regret provides a bound on the duality gap  and in order to get a competitive ratio the regularization
function should be crafted based on ψ. A general choice of regularization which yields an optimal
regret bound in terms of m is not enough for a competitive ratio argument  therefore existing results
in online learning do not address our aim.

8

u00.511.5200.20.40.60.8ψSψu020406080100012345ψSψu0204060801000510152025ψSψy00.20.40.60.81ψ∗S(y)−ψ∗(y)-0.100.10.20.30.4umax05001000comp. ratio0.70.750.80.850.90.95umax02004006008001000comp. ratio0.70.750.80.850.90.951c00.20.40.60.81Competitive ratio0.30.40.50.60.7c020406080100Competitive ratio00.20.40.60.8m0102030competitive ratio0.70.750.80.850.90.951Optimal smoothingNesterov smoothingReferences

[1] Jacob Abernethy  Elad Hazan  and Alexander Rakhlin. Competing in the dark: An efﬁcient

algorithm for bandit linear optimization. In COLT  pages 263–274  2008.

[2] Shipra Agrawal and Nikhil R Devanur. Fast algorithms for online stochastic convex program-

ming. arXiv preprint arXiv:1410.7596  2014.

[3] Yossi Azar  Ilan Reuven Cohen  and Debmalya Panigrahi. Online covering with convex

objectives and applications. arXiv preprint arXiv:1412.3507  2014.

[4] Niv Buchbinder  Shahar Chen  Anupam Gupta  Viswanath Nagarajan  et al. Online packing and

covering framework with convex objectives. arXiv preprint arXiv:1412.8347  2014.

[5] Niv Buchbinder  Kamal Jain  and Joseph Sefﬁ Naor. Online primal-dual algorithms for maxi-

mizing ad-auctions revenue. In Algorithms–ESA 2007  pages 253–264. Springer  2007.

[6] Niv Buchbinder and Joseph Naor. Online primal-dual algorithms for covering and packing.

Mathematics of Operations Research  34(2):270–286  2009.

[7] TH Chan  Zhiyi Huang  and Ning Kang. Online convex covering and packing problems. arXiv

preprint arXiv:1502.01802  2015.

[8] Nikhil R Devanur and Kamal Jain. Online matching with concave returns. In Proceedings of
the forty-fourth annual ACM symposium on Theory of computing  pages 137–144. ACM  2012.
[9] Nikhil R Devanur  Kamal Jain  Balasubramanian Sivan  and Christopher A Wilkens. Near
optimal online algorithms and fast approximation algorithms for resource allocation problems.
In Proceedings of the 12th ACM conference on Electronic commerce  pages 29–38. ACM  2011.
[10] R. Eghbali  M. Fazel  and M. Mesbahi. Worst Case Competitive Analysis for Online Conic

Optimization. In 55th IEEE conference on decision and control (CDC). IEEE  2016.

[11] Reza Eghbali  Jon Swenson  and Maryam Fazel. Exponentiated subgradient algorithm for
online optimization under the random permutation model. arXiv preprint arXiv:1410.7171 
2014.

[12] Anupam Gupta and Marco Molinaro. How the experts algorithm can help solve lps online.

arXiv preprint arXiv:1407.5298  2014.

[13] Bala Kalyanasundaram and Kirk R Pruhs. An optimal deterministic algorithm for online

b-matching. Theoretical Computer Science  233(1):319–325  2000.

[14] Richard M Karp  Umesh V Vazirani  and Vijay V Vazirani. An optimal algorithm for on-line
bipartite matching. In Proceedings of the twenty-second annual ACM symposium on Theory of
computing  pages 352–358. ACM  1990.

[15] Robert Kleinberg. A multiple-choice secretary algorithm with applications to online auctions.
In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms  pages
630–631. Society for Industrial and Applied Mathematics  2005.

[16] Aranyak Mehta  Amin Saberi  Umesh Vazirani  and Vijay Vazirani. Adwords and generalized

online matching. Journal of the ACM (JACM)  54(5):22  2007.

[17] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming 

103(1):127–152  2005.

[18] Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical program-

ming  120(1):221–259  2009.

[19] Shai Shalev-Shwartz and Yoram Singer. Online learning: Theory  algorithms  and applications.

2007.

[20] Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of online learning algorithms.

Machine Learning  69(2-3):115–142  2007.

9

,Reza Eghbali
Maryam Fazel
Yucen Luo
TIAN TIAN
Jiaxin Shi
Jun Zhu
Bo Zhang