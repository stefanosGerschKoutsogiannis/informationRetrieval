2017,Nonlinear Acceleration of Stochastic Algorithms,Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here  we study extrapolation methods in a stochastic setting  where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary  potentially biased  perturbations  then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally  we apply this acceleration technique to stochastic algorithms such as SGD  SAGA  SVRG and Katyusha in different settings  and show significant performance gains.,Nonlinear Acceleration of Stochastic Algorithms

Damien Scieur
INRIA  ENS 

Paris France

Francis Bach
INRIA  ENS 

Paris France

Alexandre d’Aspremont

CNRS  ENS 

Paris France

PSL Research University 

PSL Research University 

PSL Research University 

damien.scieur@inria.fr

francis.bach@inria.fr

aspremon@ens.fr

Abstract

Extrapolation methods use the last few iterates of an optimization algorithm to
produce a better estimate of the optimum. They were shown to achieve optimal
convergence rates in a deterministic setting using simple gradient iterates. Here 
we study extrapolation methods in a stochastic setting  where the iterates are
produced by either a simple or an accelerated stochastic gradient algorithm. We
ﬁrst derive convergence bounds for arbitrary  potentially biased perturbations  then
produce asymptotic bounds using the ratio between the variance of the noise and
the accuracy of the current point. Finally  we apply this acceleration technique
to stochastic algorithms such as SGD  SAGA  SVRG and Katyusha in different
settings  and show signiﬁcant performance gains.

1

Introduction

We focus on the problem

min
x∈Rd

f (x)

(1)

where f is a L-smooth and µ-strongly convex function with respect to the Euclidean norm  i.e. 

µ
2

(cid:107)y − x(cid:107)2 ≤ f (y) − f (x) − ∇f (x)T (y − x) ≤ L
2

(cid:107)y − x(cid:107)2.

∇εf (x) = ∇f (x) + ε 

We consider a stochastic ﬁrst-order oracle  which gives a noisy estimate of the gradient of f (x)  with
(2)
where ε is a noise term with bounded variance. This is the case for example when f is a sum of
strongly convex functions  and we only have access to the gradient of one randomly selected function.
Stochastic optimization (2) is typically challenging as classical algorithms are not convergent (for
example  gradient descent or Nesterov’s accelerated gradient). Even the averaged version of stochastic
gradient descent with constant step size does not converge to the solution of (1)  but to another point
whose proximity to the real minimizer depends of the step size [Nedi´c and Bertsekas  2001; Moulines
and Bach  2011].
When f is a ﬁnite sum of N functions  then algorithms such as SAG [Schmidt et al.  2013]  SAGA
[Defazio et al.  2014]  SDCA [Shalev-Shwartz and Zhang  2013] and SVRG [Johnson and Zhang 
2013] accelerate convergence using a variance reduction technique akin to control variate in Monte-
Carlo methods. Their rate of convergence depends on 1 − µ/L and thus does not exhibit an

accelerated rate on par with the deterministic setting (in 1 −(cid:112)µ/L). Recently a generic acceleration

algorithm called Catalyst [Lin et al.  2015]  based on the proximal point method improved this rate
of convergence  but the practical performances highly depends on the input parameters. On the
other hand  recent papers  for example [Shalev-Shwartz and Zhang  2014] (Accelerated SDCA) and

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

[Allen-Zhu  2016] (Katyusha)  propose algorithms with accelerated convergence rates  if the strong
convexity parameter is given.
When f is a quadratic function then averaged SGD converges  but the rate of decay of initial conditions
is very slow. Recently  some results have focused on accelerated versions of SGD for quadratic
optimization  showing that with a two step recursion it is possible to enjoy both the optimal rate for
the bias and variance terms [Flammarion and Bach  2015]  given an estimate of the ratio between the
distance to the solution and the variance of ε.
A novel generic acceleration technique was recently proposed by Scieur et al. [2016] in the determin-
istic setting. This uses iterates from a slow algorithm to extrapolate estimates of the solution with
asymptotically optimal convergence rate. Moreover  this rate is reached without prior knowledge of
the strong convexity constant  whose online estimation is still a challenge (even in the deterministic
case [Fercoq and Qu  2016]) but required if one wants to obtain optimal rates of convergence.
Convergence bounds are derived by Scieur et al. [2016]  tracking the difference between the determin-
istic ﬁrst-order oracle of (1) and iterates from a linearized model. The main contribution of this paper
is to extend the analysis to arbitrary perturbations  including stochastic ones  and to present numerical
results when this acceleration method is used to speed up stochastic optimization algorithms.
In Section 2 we recall the extrapolation algorithm  and quickly summarize its main convergence
bounds in Section 3. In Section 4  we consider a stochastic oracle and analyze its asymptotic
convergence in Section 5. Finally  in Section 6 we describe numerical experiments which conﬁrm the
theoretical bounds and show the practical efﬁciency of this acceleration.

2 Regularized Nonlinear Acceleration

Consider the optimization problem

min
x∈Rd

f (x)

where f is a L−smooth and µ−strongly convex function [Nesterov  2013]. Applying the ﬁxed-step
gradient method to this problem yields the following iterates
∇f (˜xt).

(3)

˜xt+1 = ˜xt − 1
L

Let x∗ be the unique optimal point  this algorithm is proved to converge with

(cid:107)˜xt − x∗(cid:107) ≤ (1 − κ)t(cid:107)˜x0 − x∗(cid:107)

(4)
where (cid:107) · (cid:107) stands for the (cid:96)2 norm and κ = µ/L ∈ [0  1[ is the (inverse of the) condition number of f
[Nesterov  2013]. Using a two-step recurrence  the accelerated gradient descent by Nesterov [2013]
achieves the improved convergence rate

(cid:16)

κ)t(cid:107)˜x0 − x∗(cid:107)(cid:17)

(1 − √

(cid:107)˜xt − x∗(cid:107) ≤ O

.

(5)

Indeed  (5) converges faster than (4) but the accelerated algorithm requires the knowledge of µ and L.
Extrapolation techniques however obtain a similar convergence rate  but do not need estimates of the
parameters µ and L. The idea is based on the comparison between the process followed by ˜xi with a
linearized model around the optimum (obtained by the ﬁrst-order approximation of ∇f (x))  written

(cid:17)

+∇2f (x∗)(xt − x∗)

 

x0 = ˜x0.

xt+1 = xt − 1
L

which can be rewritten as

(cid:16)∇f (x∗)
(cid:124) (cid:123)(cid:122) (cid:125)

=0

xt+1 − x∗ = (I − ∇2f (x∗)/L)(xt − x∗) 

(6)
A better estimate of the optimum in (6) can be obtained by forming a linear combination of the
iterates (see [Anderson  1965; Cabay and Jackson  1976; Mešina  1977])  with

x0 = ˜x0.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) (cid:28) (cid:107)xt − x∗(cid:107) 

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) t(cid:88)

i=0

cixi − x∗

2

for some speciﬁc ci (either data driven  or derived from Chebyshev polynomials). These procedures
were limited to quadratic functions only  i.e. when ˜xi = xi but this was recently extended to generic
convex problems by Scieur et al. [2016] and we brieﬂy recall these results below.
To simplify the notations  we write

(7)
to be one step of algorithm g. We have that g is differentiable  Lipchitz-continuous with constant
(1 − κ) < 1  g(x∗) = x∗ and g(cid:48)(x∗) is symmetric. For example  the gradient method (3) matches
exactly this deﬁnition with g(x) = x − ∇f (x)/L. Running k steps of (7) produces a sequence
{˜x0  ...  ˜xk}  which we extrapolate using Algorithm 1 from Scieur et al. [2016].

˜xt+1 = g(˜xt)

Algorithm 1 Regularized Nonlinear Acceleration (RNA)
Input: Iterates ˜x0  ˜x1  ...  ˜xk+1 ∈ Rd produced by (7)  and a regularization parameter λ > 0.
1: Compute ˜R = [˜r0  ...  ˜rk]  where ˜ri = ˜xi+1 − ˜xi is the ith residue.
2: Solve

Output: Approximation of x∗ computed as(cid:80)k

or equivalently solve ( ˜RT ˜R + λI)z = 1 then set ˜cλ = z/1T z.

i=0 ˜cλ

i ˜xi

˜cλ = argmin
cT 1=1

(cid:107) ˜Rc(cid:107)2 + λ(cid:107)c(cid:107)2 

For a good choice of λ  the output of Algorithm (1) is a much better estimate of the optimum than
˜xk+1 (or any other points of the sequence). Using a simple grid search on a few values of λ is usually
sufﬁcient to improve convergence (see [Scieur et al.  2016] for more details).

3 Convergence of Regularized Nonlinear Acceleration

We quickly summarize the argument behind the convergence of Algorithm (1). The theoretical bound
compares ˜xi to the iterates produced by the linearized model
xt+1 = x∗ + ∇g(x∗)(xt − x∗) 

x0 = ˜x0.

(8)

This sequence is useful to extend the convergence results to the nonlinear case  using sensivity
analysis. We write cλ the coefﬁcients computed by Algorithm (1) from the “linearized” sequence
{x0  ...  xk+1} and the error term can be decomposed into three parts 

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) k(cid:88)

i=0

i ˜xi − x∗
˜cλ

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) k(cid:88)
(cid:124)

i=0

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:125)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) k(cid:88)
(cid:124)

i=0

(cid:16)

(cid:17)
i − cλ
(cid:123)(cid:122)
˜cλ

i

Stability

+

i xi − x∗
(cid:123)(cid:122)
cλ

Acceleration

(xi − x∗)

+

˜xi − xi

˜cλ
i

.

(9)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:125)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) k(cid:88)
(cid:124)

i=0

(cid:16)
(cid:123)(cid:122)

Nonlinearity

(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:125)

Scieur et al. [2016] show that convergence is guaranteed as long as the errors (˜xi − x∗) and (xi − ˜xi)
converge to zero fast enough  which ensures a good rate of decay for the regularization parameter
λ  leading to an asymptotic rate equivalent to the accelerated rate in (5). In this section  we will use
results from Scieur et al. [2016] to bound each individual term  but in this paper we improve the ﬁnal
convergence result.
The stability term (in ˜cλ − cλ) is bounded using the perturbation matrix

P (cid:44) RT R − ˜RT ˜R 

where R and ˜R are the matrices of residuals 
R (cid:44) [r0...rk]
˜R (cid:44) [˜r0...˜rk]

rt = xt+1 − xt 
˜rt = ˜xt+1 − ˜xt.

The proofs of the following propositions were obtained by Scieur et al. [2016].

(10)

(11)
(12)

3

Proposition 3.1 (Stability). Let ∆cλ = ˜cλ − cλ be the gap between the coefﬁcients computed
by Algorithm (1) using the sequences {˜xi} and {xi} with regularization parameter λ. Let P =
RT R − ˜RT ˜R be deﬁned in (10)  (11) and (12). Then

(cid:107)∆cλ(cid:107) ≤ (cid:107)P(cid:107)

λ (cid:107)cλ(cid:107).

This implies that the stability term is bounded by

i=0 ∆cλ

i (xi − x∗)(cid:107) ≤ (cid:107)P(cid:107)

λ (cid:107)cλ(cid:107) O((cid:107)x0 − x∗(cid:107)).

(cid:107)(cid:80)k

The term Nonlinearity is bounded by the norm of the coefﬁcients ˜cλ (controlled thanks to the
regularization parameter) times the norm of the noise matrix

E = [x0 − ˜x0  x1 − ˜x1  ...  xk − ˜xk].

(15)
Proposition 3.2 (Nonlinearity). Let ˜cλ be computed by Algorithm 1 using the sequence
{˜x0  ...  ˜xk+1} with regularization parameter λ and ˜R be deﬁned in (12). The norm of ˜cλ is
bounded by

(13)

(14)

(16)

(17)

This bounds the nonlinearity term because

(cid:107)˜cλ(cid:107) ≤(cid:113)(cid:107) ˜R(cid:107)2+λ
(cid:13)(cid:13)(cid:13)(cid:80)k
(cid:13)(cid:13)(cid:13) ≤

i (˜xi − xi)

(k+1)λ ≤ 1√
(cid:113)

i=0 ˜cλ

(cid:113)

1 +

(cid:107) ˜R(cid:107)2
λ .

k+1

1 +

(cid:107) ˜R(cid:107)2

λ

(cid:107)E(cid:107)√

k+1

 

where E is deﬁned in (15).
These two propositions show that the regularization in Algorithm 1 limits the impact of the noise: the
higher λ is  the smaller these terms are. It remains to control the acceleration term. For small λ  this
term decreases as fast as the accelerated rate (5)  as shown in the following proposition.
Proposition 3.3 (Acceleration). Let Pk be the subspace of real polynomials of degree at most k and
Sκ(k  α) be the solution of the Regularized Chebychev Polynomial problem 

Sκ(k  α) (cid:44) min
p∈Pk

max

x∈[0 1−κ]

p2(x) + α(cid:107)p(cid:107)2

s.t. p(1) = 1.

Let ¯λ (cid:44)

λ

(cid:107)x0−x∗(cid:107)2 be the normalized value of λ. The acceleration term is bounded by

(cid:13)(cid:13)(cid:13)(cid:80)k

i xi − x∗(cid:13)(cid:13)(cid:13) ≤ 1

κ

i=0 cλ

(cid:112)Sκ(k  ¯λ)(cid:107)x0 − x∗(cid:107)2 − λ(cid:107)cλ(cid:107)2.

(18)

(19)

We also get the following corollary  which will be useful for the asymptotic analysis of the rate of
convergence of Algorithm 1.
Corollary 3.4. If λ → 0  the bound (19) becomes

(cid:13)(cid:13)(cid:13)(cid:80)k

i=0 cλ

(cid:16) 1−√

i xi − x∗(cid:13)(cid:13)(cid:13) ≤ 1
(cid:112)Sκ(k  0)(cid:107)x0−x∗(cid:107). The exact value of(cid:112)Sκ(k  0) is obtained

(cid:17)k (cid:107)x0 − x∗(cid:107).

√

κ
κ

1+

κ

Proof. When λ = 0  (19) becomes 1
κ
by using the coefﬁcients of a re-scaled Chebyshev polynomial  derived by Golub and Varga [1961];
Scieur et al. [2016]  and is equal to 1−√
√
κ .

1+

κ

These last results controlling stability  nonlinearity and acceleration are proved by Scieur et al. [2016].
We now reﬁne the ﬁnal step of Scieur et al. [2016] to produce a global bound on the error that will
allow us to extend these results to the stochastic setting in the next sections.
Theorem 3.5. If Algorithm 1 is applied to the sequence ˜xi with regularization parameter λ  it
converges with rate

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) k(cid:88)

i=0

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:107)x0 − x∗(cid:107)S

˜cλ
i ˜xi

(cid:114)

1
2

κ (k  ¯λ)

(cid:115)

(cid:107) ˜R(cid:107)2
λ

.

1 +

(20)

1
κ2 +

O((cid:107)x − x∗(cid:107)2)(cid:107)P(cid:107)2

λ3

(cid:107)E(cid:107)√

k + 1

+

4

Proof. The proof is inspired by Scieur et al. [2016] and is straightforward. We can bound (9) using
(14) (Stability)  (17) (Nonlinearity) and (19) (Acceleration). It remains to maximize over the value
of (cid:107)cλ(cid:107) using the result of Proposition A.2.

This last bound is not very explicit  in particular because of the regularized Chebyshev term Sκ(k  ¯λ).
The solution is well known when ¯λ = 0 since it corresponds exactly to the rescaled Chebyshev
polynomial [Golub and Varga  1961]  but as far as we know there is no known result about its
regularized version  thus making the "ﬁnite-step" version hard to analyze. However  an asymptotic
analysis simpliﬁes it considerably. The next new proposition shows that when x0 is close to x∗  then
extrapolation converges as fast as in (5) in some cases.
Proposition 3.6. Assume (cid:107) ˜R(cid:107) = O((cid:107)x0− x∗(cid:107))  (cid:107)E(cid:107) = O((cid:107)x0− x∗(cid:107)2) and (cid:107)P(cid:107) = O((cid:107)x0− x∗(cid:107)3).
If we chose λ = O((cid:107)x0 − x∗(cid:107)s) with s ∈ [2  8

3 ] then the bound (20) becomes
i ˜xi(cid:107)

(cid:18) 1 − √

(cid:107)(cid:80)k
i=0 ˜cλ
(cid:107)x0 − x∗(cid:107) ≤ 1

(cid:19)k

1 +

√

κ
κ

κ

.

lim(cid:107)x0−x∗(cid:107)→0

Proof. (Sketch) The proof is based on the fact that λ decreases slowly enough to ensure that the
(cid:107)x0−x∗(cid:107)2 → 0.
Stability and Nonlinearity terms vanish over time  but fast enough to have ¯λ =
Then it remains to bound Sκ(k  0) with Corollary 3.4. The complete proof can be found in the
Supplementary materials.

λ

Note: The assumptions are satisﬁed if we apply the gradient method on a twice differentiable 
smooth and strongly convex function with Lipchitz-continuous Hessian [Scieur et al.  2016].
The efﬁciency of Algorithm 1 is thus ensured by two conditions. First  we need to be able to bound
(cid:107) ˜R(cid:107)  (cid:107)P(cid:107) and (cid:107)E(cid:107) by decreasing quantities. Second  we have to ﬁnd a proper rate of decay for λ
and ¯λ such that the stability and nonlinearity terms go to zero when perturbations also go to zero. If
these two conditions are met  then the accelerated rate in Proposition 3.6 holds.

4 Nonlinear and Noisy Updates

In (7) we deﬁned g(x) to be non linear  which generates a sequence ˜xi. We now consider noisy
iterates

˜xt+1 = g(˜xt) + ηt+1 

where ηt is a stochastic noise. To simplify notations  we write (21) as

˜xt+1 = x∗ + G(˜xt − x∗) + εt+1 

(21)

(22)

where εt is a stochastic noise (potentially correlated with the iterates xi) with bounded mean νt 
(cid:107)νt(cid:107) ≤ ν and bounded covariance Σt (cid:22) (σ2/d)I. We also assume 0I (cid:22) G (cid:22) (1 − κ)I and G is
symmetric. For example  (22) can be linked to (21) if we set εt = ηt + O((cid:107)˜xt − x∗(cid:107)2)  which
corresponds to the combination of the noise ηt+1 with the Taylor remainder of g(x) around x∗  i.e. 

˜xt+1 = g(˜xt) + ηt+1 = g(x∗)
=x∗

(cid:124)(cid:123)(cid:122)(cid:125)

+∇g(x∗)

(cid:124) (cid:123)(cid:122) (cid:125)

=G

(cid:124)

(cid:123)(cid:122)

=t+1

(˜xt − x∗) + O((cid:107)˜xt − x∗(cid:107)) + ηt+1

.

(cid:125)

The recursion (22) is also valid when we apply the stochastic gradient method with ﬁxed step size h
to the quadratic problem

minx

1

2(cid:107)Ax − b(cid:107)2.

This corresponds to (22) with G = I − hAT A and mean ν = 0. For the theoretical results  we will
compare ˜xt with their noiseless counterpart to control convergence 

xt+1 = x∗ + G(xt − x∗) 

x0 = ˜x0.

(23)

5

5 Convergence Analysis when Accelerating Stochastic Algorithms

We will control convergence in expectation. Bound (9) now becomes

(cid:34)(cid:13)(cid:13)(cid:13) k(cid:88)

i=0

E

i ˜xi − x∗(cid:13)(cid:13)(cid:13)(cid:35)

˜cλ

≤(cid:13)(cid:13)(cid:13) k(cid:88)

i=0

i xi − x∗(cid:13)(cid:13)(cid:13) + O((cid:107)x0 − x∗(cid:107))E(cid:104)(cid:107)∆cλ(cid:107)(cid:105)

cλ

+ E(cid:104)(cid:107)˜cλ(cid:107)(cid:107)E(cid:107)(cid:105)

.

(24)

We now need to enforce bounds (14)  (17) and (19) in expectation. The proofs of the two next
propositions are in the supplementary material. For simplicity  we will omit all constants in what
follows.
Proposition 5.1. Consider the sequences xi and ˜xi generated by (21) and (23). Then 

E[(cid:107) ˜R(cid:107)] ≤ O((cid:107)x0 − x∗(cid:107)) + O(ν + σ) 
E[(cid:107)E(cid:107)] ≤ O(ν + σ) 
E[(cid:107)P(cid:107)] ≤ O((σ + ν)(cid:107)x0 − x∗(cid:107)) + O((ν + σ)2).

(25)
(26)
(27)

We deﬁne the following stochastic condition number
τ (cid:44) ν + σ

(cid:107)x0 − x∗(cid:107) .

The Proposition 5.2 gives the result when injecting these bounds in (24).
Proposition 5.2. The accuracy of extrapolation Algorithm 1 applied to the sequence {˜x0  ...  ˜xk}
generated by (21) is bounded by

(cid:32)(cid:114)

(cid:33)(cid:33)

E(cid:104)(cid:107)(cid:80)k

i ˜xi − x∗(cid:107)(cid:105)

(cid:32)

i=0 ˜cλ
(cid:107)x0 − x∗(cid:107)

(cid:114)

≤

Sκ(k  ¯λ)

1
κ2 +

O(τ 2(1 + τ )2)

¯λ3

+ O

τ 2 +

τ 2(1 + τ 2)

¯λ

. (28)

Consider a situation where τ is small  e.g. when using stochastic gradient descent with ﬁxed step-size 
with x0 far from x∗. The following proposition details the dependence between ¯λ and τ ensuring the
upper convergence bound remains stable when τ goes to zero.
Proposition 5.3. When τ → 0  if ¯λ = Θ(τ s) with s ∈]0  2

3 [  we have the accelerated rate

√
Moreover  if λ → ∞  we recover the averaged gradient 

1+

κ

κ
κ

i=0 ˜cλ

(cid:16) 1−√
i ˜xi − x∗(cid:107)(cid:3) ≤ 1
i ˜xi − x∗(cid:107)(cid:3) = E(cid:104)(cid:13)(cid:13)(cid:13) 1

(cid:17)k (cid:107)x0 − x∗(cid:107).
E(cid:2)(cid:107)(cid:80)k
i=0 ˜xi − x∗(cid:13)(cid:13)(cid:13)(cid:105)
E(cid:2)(cid:107)(cid:80)k
(cid:80)k
(cid:113) 1
i ˜xi − x∗(cid:13)(cid:13)(cid:13)(cid:105) ≤ (cid:107)x0 − x∗(cid:107)Sκ(k  τ s)

i=0 ˜cλ

k+1

E(cid:104)(cid:13)(cid:13)(cid:13)(cid:80)k

i=0 ˜cλ

Proof. Let ¯λ = Θ(τ s)  using (28) we have

(29)

.

+(cid:107)x0 − x∗(cid:107)O((cid:112)τ 2 + τ 2−3s(1 + τ 2)).

κ2 O(τ 2−3s(1 + τ )2)

Because s ∈]0  2
exactly (29). If λ → ∞  we have also

3 [  means 2 − 3s > 0  thus limτ→0 τ 2−3s = 0. The limits when τ → 0 is thus

lim
λ→∞ ˜cλ = lim

λ→∞ argminc:1T c=1 (cid:107) ˜Rc(cid:107) + λ(cid:107)c(cid:107)2 = argminc:1T c=1 (cid:107)c(cid:107)2 = 1

k+1

which yields the desired result.

Proposition 5.3 shows that Algorithm 1 is thus asymptotically optimal provided λ is well chosen
because it recovers the accelerated rate for smooth and strongly convex functions when the perturba-
tions goes to zero. Moreover  we recover Proposition 3.6 when t is the Taylor remainder  i.e. with
ν = O((cid:107)x0 − x∗(cid:107)2) and σ = 0  which matches the deterministic results.
Algorithm 1 is particularly efﬁcient when combined with a restart scheme [Scieur et al.  2016].
From a theoretical point of view  the acceleration peak arises for small values of k. Empirically  the

6

improvement is usually more important at the beginning  i.e. when k is small. Finally  the algorithmic
complexity is O(k2d)  which is linear in the problem dimension when k remains bounded.
The beneﬁts of extrapolation are limited in a regime where the noise dominates. However  when
τ is relatively small then we can expect a signiﬁcant speedup. This condition is satisﬁed in many
cases  for example at the initial phase of the stochastic gradient descent or when optimizing a sum of
functions with variance reduction techniques  such as SAGA or SVRG.

6 Numerical Experiments

6.1 Stochastic gradient descent

We want to solve the least-squares problem

min
x∈Rd

F (x) =

(cid:107)Ax − b(cid:107)2 

1
2

where AT A satisﬁes µI (cid:22) (AT A) (cid:22) LI. To solve this problem  we have access to the stochastic
ﬁrst-order oracle

∇εF (x) = ∇F (x) + ε 

d I. We will compare several methods.

where ε is a zero-mean noise of covariance matrix Σ (cid:22) σ2
L∇εF (xt).

• SGD. Fixed step-size  xt+1 = xt − 1
• Averaged SGD. Iterate xk is the mean of the k ﬁrst iterations of SGD.
• AccSGD. The optimal two-step algorithm in Flammarion and Bach [2015]  with optimal
• RNA+SGD. The regularized nonlinear acceleration Algorithm 1 applied to a sequence of k

parameters (this implies (cid:107)x0 − x∗(cid:107) and σ are known exactly).
iterates of SGD  with k = 10 and λ = (cid:107) ˜RT ˜R(cid:107)/10−6.

By Proposition 5.2  we know that RNA+SGD will not converge to arbitrary precision because the
noise is additive with a non-vanishing variance. However  Proposition 5.3 predicts an improvement
of the convergence at the beginning of the process. We illustrate this behavior in Figure 1. We
clearly see that at the beginning  the performance of RNA+SGD is comparable to that of the optimal
accelerated algorithm. However  because of the restart strategy  in the regime where the level of
noise becomes more important the acceleration becomes less effective and ﬁnally the convergence
stalls  as for SGD. Of course  for practical purposes  the ﬁrst regime is the most important because it
effectively minimizes the generalization error [Défossez and Bach  2015; Jain et al.  2016].

1

i=1

6.2 Finite sums of functions

We focus on the composite problem minx∈Rd F (x) =(cid:80)N

N fi(x) + µ

2(cid:107)x(cid:107)2  where fi are convex
and L-smooth functions and µ is the regularization parameter. We will use classical methods for
minimizing F (x) such as SGD (with ﬁxed step size)  SAGA [Defazio et al.  2014]  SVRG [Johnson
and Zhang  2013]  and also the accelerated algorithm Katyusha [Allen-Zhu  2016]. We will compare
their performance with and without the (potential) acceleration provided by Algorithm 1 with restart
after k data passes. The parameter λ is found by a grid search of size k  the size of the input sequence 
but it adds only one data pass at each extrapolation. Actually  the grid search can be faster if we
approximate F (x) with fewer samples  but we choose to present Algorithm 1 in its simplest version.
We set k = 10 for all the experiments.
In order to balance the complexity of the extrapolation algorithm and the optimization method we wait
several data queries before adding the current point (the “snapshot”) of the method to the sequence.
Indeed  the extrapolation algorithm has a complexity of O(k2d) + O(N ) (computing the coefﬁcients
˜cλ and the grid search over λ). If we wait at least O(N ) updates  then the extrapolation method is of
the same order of complexity as the optimization algorithm.

• SGD. We add the current point after N data queries (i.e. one epoch) and k snapshots of

SGD cost kN data queries.

7

Left: σ = 10  κ = 10−2. Center: σ = 1000  κ = 10−2. Right: σ = 1000  κ = 10−6.

Left: σ = 10  κ = 1/d. Center: σ = 100  κ = 1/d. Right: σ = 1000  κ = 1/d.

Figure 1: Comparison of performance between SGD  averaged SGD  Accelerated SGD [Flammarion
and Bach  2015] and RNA+SGD. We tested the performance on a matrix AT A of size d = 500  with
(top) random eigenvalues between κ and 1 and (bottom) decaying eigenvalues from 1 to 1/d. We
start at (cid:107)x0 − x∗(cid:107) = 104  where x0 and x∗ are generated randomly.

• SAGA. We compute the gradient table exactly  then we add a new point after N queries 
and k snapshots of SAGA cost (k + 1)N queries. Since we optimize a sum of quadratic or
logistic losses  we used the version of SAGA which stores O(N ) scalars.

• SVRG. We compute the gradient exactly  then perform N queries (the inner-loop of SVRG) 

and k snapshots of SVRG cost 2kN queries.

• Katyusha. We compute the gradient exactly  then perform 4N gradient calls (the inner-loop

of Katyusha)  and k snapshots of Katyusha cost 3kN queries.

We compare these various methods for solving least-squares regression and logistic regression
on several datasets (Table 1)  with several condition numbers κ: well (κ = 100/N)  moderately
(κ = 1/N) and badly (κ = 1/100N) conditioned. In this section  we present the numerical results
on Sid (Sido0 dataset  where N = 12678 and d = 4932) with bad conditioning  see Figure 2. The
other experiments are highlighted in the supplementary material.
In Figure 2  we clearly see that both SGD and RNA+SGD do not converge. This is mainly due to
the fact that we do not average the points. In any case  except for quadratic problems  the averaged
version of SGD does not converge to the minimum of F with arbitrary precision.
We also notice that Algorithm 1 is unable to accelerate Katyusha. This issue was already raised
by Scieur et al. [2016]: when the algorithm has a momentum term (like Nesterov’s method)  the
underlying dynamical system is harder to extrapolate  in particular because the matrix presents in the
linearized version of such systems is not symmetric.
Because the iterates of SAGA and SVRG have low variance  their accelerated version converges
faster to the optimum  and their performance are then comparable to Katyusha. In our experiments 
Katyusha was faster than RNA+SAGA only once  when solving a least square problem on Sido0

8

PSfragreplacementsSGDAve.SGDAcc.SGDRNA+SGD10010210410-2100102PSfragreplacementsSGDAve.SGDAcc.SGDRNA+SGDIterationf(x)−f(x∗)100102104100102PSfragreplacementsSGDAve.SGDAcc.SGDRNA+SGDIterationf(x)−f(x∗)100102104101102103104PSfragreplacementsSGDAve.SGDAcc.SGDRNA+SGDIterationf(x)−f(x∗)10010210410-2100102PSfragreplacementsSGDAve.SGDAcc.SGDRNA+SGDIterationf(x)−f(x∗)f(x)−f(x∗)100102104100102PSfragreplacementsSGDAve.SGDAcc.SGDRNA+SGDIterationf(x)−f(x∗)f(x)−f(x∗)100102104100101102103PSfragreplacementsSGDAve.SGDAcc.SGDRNA+SGDIterationf(x)−f(x∗)f(x)−f(x∗)Figure 2: Optimization of quadratic loss (Top) and logistic loss (Bottom) with several algorithms 
using the Sid dataset with bad conditioning. The experiments are done in Matlab. Left: Error vs
epoch number. Right: Error vs time.

with a bad condition number. Recall however that the acceleration Algorithm 1 does not require the
speciﬁcation of the strong convexity parameter  unlike Katyusha.

Acknowledgments

The authors would like to acknowledge support from a starting grant from the European Research
Council (ERC project SIPA)  from the European Union’s Seventh Framework Programme (FP7-
PEOPLE-2013-ITN) under grant agreement number 607290 SpaRTaN  as well as support from the
chaire Économie des nouvelles données with the data science joint research initiative with the fonds
AXA pour la recherche and a gift from Société Générale Cross Asset Quantitative Research.

9

020040010-1010-5PSfragreplacementsf(x)−f(x∗)Epoch05010015020010-1010-5PSfragreplacementsf(x)−f(x∗)EpochTime(sec)020040010-1010-5PSfragreplacementsf(x)−f(x∗)EpochTime(sec)f(x)−f(x∗)Epoch010020030010-1010-5PSfragreplacementsf(x)−f(x∗)EpochTime(sec)f(x)−f(x∗)EpochTime(sec)PSfragreplacementsf(x)−f(x∗)EpochTime(sec)f(x)−f(x∗)EpochTime(sec)SAGASGDSVRGKatyushaRNA+SAGARNA+SGDRNA+SVRGRNA+Kat.References
Allen-Zhu  Z. [2016]  ‘Katyusha: The ﬁrst direct acceleration of stochastic gradient methods’  arXiv preprint

arXiv:1603.05953 .

Anderson  D. G. [1965]  ‘Iterative procedures for nonlinear integral equations’  Journal of the ACM (JACM)

12(4)  547–560.

Cabay  S. and Jackson  L. [1976]  ‘A polynomial extrapolation method for ﬁnding limits and antilimits of vector

sequences’  SIAM Journal on Numerical Analysis 13(5)  734–752.

Defazio  A.  Bach  F. and Lacoste-Julien  S. [2014]  Saga: A fast incremental gradient method with support
for non-strongly convex composite objectives  in ‘Advances in Neural Information Processing Systems’ 
pp. 1646–1654.

Défossez  A. and Bach  F. [2015]  Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling

distributions  in ‘Artiﬁcial Intelligence and Statistics’  pp. 205–213.

Fercoq  O. and Qu  Z. [2016]  ‘Restarting accelerated gradient methods with a rough strong convexity estimate’ 

arXiv preprint arXiv:1609.07358 .

Flammarion  N. and Bach  F. [2015]  From averaging to acceleration  there is only a step-size  in ‘Conference on

Learning Theory’  pp. 658–695.

Golub  G. H. and Varga  R. S. [1961]  ‘Chebyshev semi-iterative methods  successive overrelaxation iterative

methods  and second order richardson iterative methods’  Numerische Mathematik 3(1)  147–156.

Jain  P.  Kakade  S. M.  Kidambi  R.  Netrapalli  P. and Sidford  A. [2016]  ‘Parallelizing stochastic approximation

through mini-batching and tail-averaging’  arXiv preprint arXiv:1610.03774 .

Johnson  R. and Zhang  T. [2013]  Accelerating stochastic gradient descent using predictive variance reduction 

in ‘Advances in Neural Information Processing Systems’  pp. 315–323.

Lin  H.  Mairal  J. and Harchaoui  Z. [2015]  A universal catalyst for ﬁrst-order optimization  in ‘Advances in

Neural Information Processing Systems’  pp. 3384–3392.

Mešina  M. [1977]  ‘Convergence acceleration for the iterative solution of the equations x= ax+ f’  Computer

Methods in Applied Mechanics and Engineering 10(2)  165–173.

Moulines  E. and Bach  F. R. [2011]  Non-asymptotic analysis of stochastic approximation algorithms for

machine learning  in ‘Advances in Neural Information Processing Systems’  pp. 451–459.

Nedi´c  A. and Bertsekas  D. [2001]  Convergence rate of incremental subgradient algorithms  in ‘Stochastic

optimization: algorithms and applications’  Springer  pp. 223–264.

Nesterov  Y. [2013]  Introductory lectures on convex optimization: A basic course  Vol. 87  Springer Science &

Business Media.

Schmidt  M.  Le Roux  N. and Bach  F. [2013]  ‘Minimizing ﬁnite sums with the stochastic average gradient’ 

Mathematical Programming pp. 1–30.

Scieur  D.  d’Aspremont  A. and Bach  F. [2016]  Regularized nonlinear acceleration  in ‘Advances In Neural

Information Processing Systems’  pp. 712–720.

Shalev-Shwartz  S. and Zhang  T. [2013]  ‘Stochastic dual coordinate ascent methods for regularized loss

minimization’  Journal of Machine Learning Research 14(Feb)  567–599.

Shalev-Shwartz  S. and Zhang  T. [2014]  Accelerated proximal stochastic dual coordinate ascent for regularized

loss minimization.  in ‘ICML’  pp. 64–72.

10

,Damien Scieur
Francis Bach
Alexandre d'Aspremont