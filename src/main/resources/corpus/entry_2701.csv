2010,Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains,The reinforcement learning community has explored many approaches to obtain- ing value estimates and models to guide decision making; these approaches  how- ever  do not usually provide a measure of confidence in the estimate. Accurate estimates of an agent’s confidence are useful for many applications  such as bi- asing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing confidence intervals on reinforcement learning value estimates  however  is challenging because data generated by the agent- environment interaction rarely satisfies traditional assumptions. Samples of value- estimates are dependent  likely non-normally distributed and often limited  partic- ularly in early learning when confidence estimates are pivotal. In this work  we investigate how to compute robust confidences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute confidence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applica- bility of our confidence estimation algorithms with experiments on exploration  parameter estimation and tracking.,Interval Estimation for Reinforcement-Learning

Algorithms in Continuous-State Domains

Martha White

Department of Computing Science

University of Alberta

whitem@cs.ualberta.ca

Adam White

Department of Computing Science

University of Alberta

awhite@cs.ualberta.ca

Abstract

The reinforcement learning community has explored many approaches to obtain-
ing value estimates and models to guide decision making; these approaches  how-
ever  do not usually provide a measure of conﬁdence in the estimate. Accurate
estimates of an agent’s conﬁdence are useful for many applications  such as bi-
asing exploration and automatically adjusting parameters to reduce dependence
on parameter-tuning. Computing conﬁdence intervals on reinforcement learning
value estimates  however  is challenging because data generated by the agent-
environment interaction rarely satisﬁes traditional assumptions. Samples of value-
estimates are dependent  likely non-normally distributed and often limited  partic-
ularly in early learning when conﬁdence estimates are pivotal. In this work  we
investigate how to compute robust conﬁdences for value estimates in continuous
Markov decision processes. We illustrate how to use bootstrapping to compute
conﬁdence intervals online under a changing policy (previously not possible) and
prove validity under a few reasonable assumptions. We demonstrate the applica-
bility of our conﬁdence estimation algorithms with experiments on exploration 
parameter estimation and tracking.

1

Introduction

In reinforcement learning  an agent interacts with the environment  learning through trial-and-error
based on scalar reward signals. Many reinforcement learning algorithms estimate values for states to
enable selection of maximally rewarding actions. Obtaining conﬁdence intervals on these estimates
has been shown to be useful in practice  including directing exploration [17  19] and deciding when
to exploit learned models of the environment [3]. Moreover  there are several potential applications
using conﬁdence estimates  such as teaching interactive agents (using conﬁdence estimates as feed-
back)  adjusting behaviour in non-stationary environments and controlling behaviour in a parallel
multi-task reinforcement learning setting.
Computing conﬁdence intervals was ﬁrst studied by Kaelbling for ﬁnite-state Markov decision pro-
cesses (MDPs) [11]. Since this preliminary work  many model-based algorithms have been proposed
for evaluating conﬁdences for discrete-state MDPs. The extension to continuous-state spaces with
model-free learning algorithms  however  has yet to be undertaken. In this work we focus on con-
structing conﬁdence intervals for online model-free reinforcement learning agents.
The agent-environment interaction in reinforcement learning does not satisfy classical assumptions
typically used for computing conﬁdence intervals  making accurate conﬁdence estimation challeng-
ing.
In the discrete case  certain simplifying assumptions make classical normal intervals more
appropriate; in the continuous setting  we will need a different approach.
The main contribution of this work is a method to robustly construct conﬁdence intervals for approx-
imated value functions in continuous-state reinforcement learning setting. We ﬁrst describe boot-

1

strapping  a non-parametric approach to estimating conﬁdence intervals from data. We then prove
that bootstrapping can be applied to our setting  addressing challenges due to sample dependencies 
changing policies and non-stationarity (because of learning). Then  we discuss how to address com-
plications in computing conﬁdence intervals for sparse or local linear representations  common in
reinforcement learning  such as tile coding  radial basis functions  tree-based representations and
sparse distributed memories. Finally  we propose several potential applications of conﬁdence inter-
vals in reinforcement learning and conclude with an empirical investigation of the practicality of our
conﬁdence estimation algorithm for exploration  tuning the temporal credit parameter and tracking.

2 Related Work

Kaelbling was the ﬁrst to employ conﬁdence interval estimation method for exploration in ﬁnite-
state MDPs [11]. The agent estimates the probability of receiving a reward of 1.0 for a given state-
action pair and constructs an upper conﬁdence bound on this estimate using a Bernoulli conﬁdence
interval. Exploration is directed by selecting the action with the highest upper conﬁdence bound 
which corresponds to actions for which it has high uncertainty or high value estimates [11].
Interval estimation for model-based reinforcement learning with discrete state spaces has been quite
extensively studied. Mannor et al. (2004) investigated conﬁdence estimates for the parameters of the
learned transition and reward models  assuming Gaussian rewards [5  16]. The Model Based Interval
Estimation Algorithm (MBIE) uses upper conﬁdence bounds on the model transition probabilities to
select the model that gives the maximal reward [22]. The Rmax algorithm uses a heuristic notion of
conﬁdence (state visitiation counts) to determine when to explore  or exploit the learned model [3].
Both Rmax and MBIE are guaranteed to converge to the optimal policy in polynomially many steps.
These guarantees  however  become difﬁcult for continuous state spaces.
A recently proposed framework  KWIK (“Knows What It Knows”)  is a formal framework for algo-
rithms that explore efﬁciently by minimizing the number of times an agent must return the response
“I do not know” [23]. For example  for reinforcement learning domains  KWIK-RMAX biases ex-
ploration toward states that the algorithm currently does not “know” an accurate estimate of the
value [23]. KWIK-RMAX provides an uncertainty estimate (not a conﬁdence interval) on a linear
model by evaluating if the current feature vector is contained in the span of previously observed
feature vectors. Though quite general  the algorithm remains theoretical due to the requirement of a
solution to the model.
Bayesian methods (e.g.  GPTD [6]) provide a natural measure of conﬁdence: one can use the poste-
rior distribution to form credible intervals for the mean value of a state-action pair. However  if one
wants to use non-Gaussian priors and likelihoods  then the Bayesian approach is intractable without
appropriate approximations. Although this approach is promising  we are interested in computing
classical frequentist conﬁdence intervals for agents  while not restricting the underlying learning
algorithm to use a model or particular update mechanism.
Several papers have demonstrated the empirical beneﬁts of using heuristic conﬁdence estimates to
bias exploration [14  17  19] and guide data collection in model learning [9  18]. For example  Nouri
et al. [19] discretize the state space with a KD-tree and mark the state as “known” after reaching a
visitation count threshold.
In the remainder of this work  we provide the ﬁrst study of estimating conﬁdence intervals for
model-free  online reinforcement learning value estimates in the continuous-state setting.

3 Background

In this section  we will introduce the reinforcement learning model of sequential decision making
and bootstrapping  a family of techniques used to compute conﬁdence intervals for means of depen-
dent data from an unknown (likely non-normal) underlying distribution.

3.1 Reinforcement Learning

In reinforcement learning  an agent interacts with its environment  receiving observations and se-
lecting actions to maximize a scalar reward signal provided by the environment. This interaction is

2

k=0 γkrt+k+1|st = s  at = a(cid:3)  where γ ∈ [0 1] discounts the contribution of future rewards.
(cid:2)(cid:80)∞

usually modeled by a Markov decision process (MDP). An MDP consists of (S  A  P  R) where S is
the set of states; A is a ﬁnite set of actions; P   the transition function  which describes the probability
of reaching a state s(cid:48) from a given state and action (s  a); and ﬁnally the reward function R(s  a  s(cid:48)) 
which returns a scalar value for transitioning from state-action (s  a) to state s(cid:48). The state of the
environment is said to be Markov if P r(st+1  rt+1|st  at) = P r(st+1  rt+1|st  at  . . .   s0  a0). The
agent’s objective is to learn a policy  π : S → A  such that R is maximized for all s ∈ S.
Many reinforcement learning algorithms maintain an state-action value function  Qπ(s  a)  equal
to the expected discounted sum of future rewards for a given state-action pair: Qπ(s  a) =
Eπ
The optimal state-action value function  Q∗(s  a)  is the maximum achievable value given the agent
starts in state s and selects action a. The optimal policy  π∗  is greedy with respect to the opti-
mal value function: π∗(s) = argmaxa∈A Q∗(s  a) for all s ∈ S. During learning the agent must
balance selecting actions to achieve high reward (according to ˆQ(s  a)) or selecting actions to gain
more information about the environment. This is called the exploration-exploitation trade-off.
In many practical applications  the state space is too large to store in a table. In this case  a function
approximator is used to estimate the value of a state-action pair. A linear function approximator
produces a value prediction using a linear combination of basis units: ˆQ(s  a) = θT φ(s  a). We
refer the reader to the introductory text [25] for a more detailed discussion on reinforcement learning.

3.2 Bootstrapping a conﬁdence interval for dependent data

we can use bootstrapping to compute a conﬁdence interval around the mean  Tn = n−1(cid:80)n

Bootstrapping is a statistical procedure for estimating the distribution of a statistic (such as the
sample mean)  particularly when the underlying distribution is complicated or unknown  samples
are dependent and power calculations (e.g. variance) are estimated with limited sample sizes [21].
This estimate can then be used to approximate a 1 − α conﬁdence interval around the statistic: an
interval for which the probability of seeing the statistic outside of the interval is low (probability α).
For example  for potentially dependent data sampled from an unknown distribution P (X1  X2  . . .) 
i=1 xn.
The key idea behind bootstrapping is that the data is an appropriate approximation  Pn  of the true
distribution: resampling from the data represents sampling from Pn. Samples are “drawn” from Pn
to produce a bootstrap sample  x∗
n  of the statistic.
This process is repeated B times  giving B samples of the statistic  T ∗
n B. These  for
n b − T

example  can be used to estimate VarP (Tn) ≈ VarPn (Tn) =(cid:80)(T ∗

n ⊂ {x1  . . .   xn}  and an estimate  T ∗
n 1  . . .   T ∗
∗
n)2/(B − 1).

1  . . .   x∗

√
Bootstrapped intervals have been shown to have a lower coverage error than normal intervals for
dependent  non-normal data. A normal interval has a coverage error of O(1/
n)  whereas boot-
strapping has a coverage error of O(n−3/2) [29]. The coverage error represents how quickly the
estimated interval converges to the true interval: higher order coverage error indicates faster con-
vergence1. Though the theoretical conditions for these guarantees are somewhat restrictive [29] 
bootstrapping has nevertheless proved very useful in practice for more general data [4  21].
With the bootstrapped samples  a percentile-t (studentized) interval is constructed by

P (T ∈ (2Tn − T ∗

1−α/2  2Tn − T ∗

α/2)) ≥ 1 − α

where T ∗
tion of size n is the continuous sample quantile:

β is the β sample quantile of T ∗

n 1  . . .   T ∗

n B. Usually  the β-quantile of an ordered popula-

(1 − r)T ∗

n j + rT ∗

n j+1

where

j = (cid:98)nβ(cid:99) + m  r = nβ − j + m

where m is dependent on quantile type  with m = β+1
3
The remaining question is how to bootstrap from the sequence of samples. In the next section  we
describe the block bootstrap  applicable to Markov processes  which we will show represents the
structure of data for value estimates in reinforcement learning.

common for non-normal distributions.

1More theoretically  coverage error is the approximation error in the Edgeworth expansions used to approx-

imate the distribution in bootstrap proofs.

3

3.2.1 Moving Block Bootstrap

In the moving block bootstrap method  blocks of consecutive samples are drawn with replacement
from a set of overlapping blocks  making the k-th block {xk−1+t : t = 1 . . .   l}. The bootstrap
resample is the concatenation of n/l blocks chosen randomly with replacement  making a time
series of length n; B of these concatenated resamples are used in the bootstrap estimate. The
block bootstrap is appropriate for sequential processes because the blocks implicitly maintain a
time-dependent structure. An common heuristic for the block length  l  is n1/3 [8].
The moving block bootstrap was designed for stationary  dependent data; however  our scenario
involves nonstationary data. Lahiri [12] proved a coverage error of o(n−1/2) when applying the
moving block bootstrap to nonstationary  dependent data  better than the normal coverage error.
Fortunately  the conditions are not restrictive for our scenario  described further in the next section.
Note that there are other bootstrapping techniques applicable to sequential  dependent data with
lower coverage error  such as the double bootstrap [13]  block-block bootstrap [1] and Markov or
Sieve bootstrap [28]. In particular  the Markov bootstrap has been shown to have a lower cover-
age error for Markov data than the block bootstrap under certain restricted conditions [10]. These
techniques  however  have not been shown to be valid for nonstationary data.

4 Conﬁdence intervals for continuous-state Markov decision processes

In this section  we present a theoretically sound approach to constructing conﬁdence intervals for
parametrized Q(s  a) using bootstrapping for dependent data. We then discuss how to address sparse
representations  such as tile coding  which make conﬁdence estimation more complicated.

4.1 Bootstrapped Conﬁdence Intervals for Global Representations

The goal is to compute a conﬁdence estimate for Q(st  at) on time step t. Assume that we are
learning a parametrized value function Q(s  a) = f (θ  s  a)  with θ ∈ Rd and a smooth function
f : Rd × S × A → R. A common example is a linear value function Q(s  a) = θT φ(s  a)  with
φ : S × A → Rd. During learning  we have a sequence of changing weights  {θ1  θ2  . . .   θn} up
to time step n  corresponding to the random process {Θ1  . . .   Θn}. If this process were stationary 
then we could compute an interval around the mean of the process. In almost all cases  however  the
process will be nonstationary with means {µ1  . . .   µn}. Instead  our goal is to estimate

n(cid:88)

¯fn(s  a) = n−1

E[f (Θt  s  a)]

t=1

which represents the variability in the current estimation of the function ˆQ for any given state-action
pair  (s  a) ∈ S × A. Because Q is parametrized  the sequence of weights  {Θt}  represents the
variability for the uncountably many state-action pairs.
Assume that the weight vector on time step t + 1 is drawn from the unknown distribution
Pa[(Θt+1  st+1)|(θt  st)  . . .   (θt−k  st−k)]  giving a k-order Markov dependence on previous
states and weight vectors. Notice that Pa incorporates P and R  using st  θt (giving the policy
π) and R to determine the reward passed to the algorithm to then obtain θt+1. This allows the learn-
ing algorithm to select actions using conﬁdence estimates based on the history of the k most recent
θ  without invalidating that the sequence of weights are drawn from Pa. In practice  the length of
the dependence  k  can be estimated using auto-correlation [2].
Applying the Moving Block Bootstrap method to a non-stationary sequence of θ’s requires several
assumptions on the underlying MDP and the learning algorithm. We require two assumptions on the
underlying MDP: a bounded density function and a strong mixing requirement. The assumptions
on the algorithm are less strict  only requiring that the algorithm be non-divergent and produce a
sequence of {Qt(s  a)} that 1) satisfy a smoothness condition (a dependent Cramer condition)  2)
have a bounded twelfth moment and 3) satisfy an m-dependence relation where sufﬁciently sepa-
rated Qi(s  a)  Qj(s  a) are independent. Based on these assumptions (stated formally in the sup-
plement)  we can prove that the moving block bootstrap produces an interval with a coverage error
of o(n−1/2 for the studentized interval on fn(s  a).

4

Theorem 1 Given that Assumption 1-7 are satisﬁed and there exists constants C1  C2 > 0  0 <
α ≤ β < 1/4 such that C1nα < l < C2nβ (i.e. l increases with n)  then the moving block bootstrap
produces a one-sided conﬁdence interval that is consistent and has a coverage error of o(n−1/2) for
the studentization of the mean of the process {f (θt  s  a)}  where Qt(s  a) = f (θt  s  a).
The proof for the above theorem follows Lahiri’s proof [12] for the coverage error of the moving
block bootstrap for nonstationary data. The general approach for coverage error proofs involve
approximating the unknown distribution with an Edgeworth expansion (see [7])  with the coverage
error dependent on the order of the expansion  similar to the the idea of a Taylor series expansion.
Assuming Pa is k-order Markov results in two important practical implications on the learning
algorithm: 1) inability to use eligibility traces and 2) restrictions on updates to parameters (such
as the learning rate). These potential issues  however  are actually not restrictive. First  the tail of
eligibility traces has little effect  particularly for larger k; the most recent k weights incorporate the
most important information for the eligibility traces. Second  the learning rate  for example  cannot
be updated based on time. The learning rate  however  can still be adapted based on changes between
weight vectors  a more principled approach taken  by the meta-learning algorithm  IDBD [24].
The ﬁnal algorithm is summarized in the pseudocode below. In practice  a window of data of length
w is stored due to memory restrictions; other data selection techniques are possible. Corresponding
to the notation in Section 3.2  Qi represents the data samples (of ˆQ(s  a))  (Q∗
i 1  . . .   Q∗i  M ) the
dependently sampled blocks for the ith resample and T ∗
Algorithm 1 GetUpperConﬁdence(f (·  s  a) {θn−w  . . . θn}  α)
l = block length  B = num bootstrap resamples
last w weights and conﬁdence level α (= 0.05)
1: QN ← {f (θn−w  s  a)  . . . f (θn  s  a)}
2: Blocks = {[Qn−w  . . .   Qn−w+l−1]  [Qn−w+1  . . .   Qn−w+l]  . . .   [Qn−l+1  . . .   Qn]}
3: M ← (cid:98)w/l(cid:99)
4: for all i = 1 to B do
(Q∗
1  Q∗
2  . . .   Q∗
5:
T ∗
i = 1
6:
M∗l
7: end for
B})
8: sort({T ∗
1   . . .   T ∗
9: j ← (cid:98) Bα
6 (cid:99)  r ← Bα
2 + α+2
α/2 ← (1 − r)T ∗
10: T ∗
j + rT ∗
11: Return 2mean(QN ) − T ∗

M∗l) ← concatMRandomBlocks(Blocks  M)

i the mean of the i resample.

2 + α+2

6 − j

(cid:80) Q∗

j

the number of length lblocks to sample with replacement and concatenate

j+1

α/2

4.2 Bootstrapped Conﬁdence Intervals for Sparse Representations

We have shown that bootstrapping is a principled approach for computing intervals for global rep-
resentations; sparse representations  however  complicate the solution. In an extreme case  for ex-
ample  for linear representations  features active on time step t may have never been active before.
Samples Q1(st  at)  . . .   Qt(st  at) would therefore all equal Q0(st  at)  because the weights would
have never been updated for those features. Consequently  the samples erroneously indicate low
variance for Q(st  at).
We propose that  for sparse linear representations  the samples for the weights can be treated inde-
pendently and still produce a reasonable  though currently unproven  bootstrap interval. Notice that
for θ(i) the ith feature
Pa[(θt  st)|(θt−1  st−1)  . . .   (θt−k  st−k)] = Πd
i=1Pa[(θt(i)  st)|(θt−1  st−1)  . . .   (θt−k  st−k)]
because updates to weights θ(i)  θ(j) are independent given the previous states and weights vectors
for all i  j ∈ {1  . . .   d}. We could  therefore  estimate upper conﬁdence bounds on the individual
i=1 ucbi(s  a) ∗ φi(s  a)  to produce
an upper conﬁdence bound on Q(st  at). To approximate the variance of θ(i) on time step t  we can
use the last w samples of θ(i) where θ(i) changed.

weights  ucbi(s  a)  and then combine them  via ucb(s  a) =(cid:80)d

5

Proving coverage error results for sparse representations will require analyzing the covariance be-
tween components of θ over time. The above approach for sparse representations does not capture
this covariance; due to sparsity  however  the dependence between many of the samples for θ(i)
and θ(j) will likely be weak. We could potentially extend the theoretical results by bounding the
covariance between the samples and exploiting independencies. The means for individual weights
could likely be estimated separately  therefore  and still enable a valid conﬁdence interval. In future
work  a potential extension is to estimate the covariances between the individual weights to improve
the interval estimate.

5 Applications of conﬁdence intervals for reinforcement learning

The most obvious application of interval estimation is to bias exploration to select actions with
high uncertainty. Conﬁdence-based exploration should be comparable to optimistic initialization
in domains where exhaustive search is required and ﬁnd better policies in domains where noisy
rewards and noisy dynamics can cause the optimistic initialization to be prematurely decreased and
inhibit exploration. Furthermore  conﬁdence-based exploration reduces parameter tuning because
the policy does not require knowledge of the reward range  as in softmax and optimistic initialization.
Conﬁdence-based exploration could be beneﬁcial in domains where the problem dynamics and re-
ward function change over time. In an extreme case  the agent may converge to a near-optimal policy
before the goal is teleported to another portion of the space. If the agent continues to act greedily
with respect to its action-value estimates without re-exploring  it may act sub-optimally indeﬁnitely.
These tracking domains require that the agent “notice” that its predictions are incorrect and begin
searching for a better policy. AN example of a changing reward signals arises in interactive teaching.
In this scenario  the a human teaching shapes the agent by providing a drifting reward signal. Even
in stationary domains  tracking the optimal policy may be more effective than converging due to the
non-stationarity introduced by imperfect function approximation [26].
Another potential application of conﬁdence estimation is to automate parameter tuning online. For
example  many TD-based reinforcement learning algorithms use an eligibility parameter (λ) to ad-
dress the credit assignment problem. Learning performance can be sensitive to γ. There has been
little work  however  exploring the effects of different decay functions for λ; using different λ values
for each state/feature; or for meta-learning λ. Conﬁdence estimates could be used to increase λ
when the agent is uncertain  reﬂecting and decrease λ for conﬁdent value estimates [25].
Conﬁdence estimates could also be used to guide the behaviour policy for a parallel multi-task
reinforcement learning system. Due to recent theoretical developments [15]  several target value
functions can be learned in parallel  off-policy  based on a single stream of data from a behaviour
policy. The behaviour policy should explore to provide samples that generalize well between the
various target policies  speeding overall convergence. For example  if one-sided intervals are main-
tained for each target value functions  the behaviour policy could select an action corresponding to
the maximal sum of those intervals. Exploration is then biased to highly uncertain areas where more
samples are required.
Finally  conﬁdence estimates could be used to determine when features should be evaluated in a
feature construction algorithm. Many feature construction algorithms  such as cascade correlation
networks  interleave proposing candidate features and evaluation. In an online reinforcement learn-
ing setting  these methods freeze the representation for a ﬁxed window of time to accurately evaluate
the candidate [20]. Instead of using a ﬁxed window  a more principled approach is to evaluate the
features after the conﬁdence on the weights of the candidate features reached some threshold.

6 Experimental Results

In this section  we provide a preliminary experimental investigation into the practicality of conﬁ-
dence estimation in continuous-state MDPs. We evaluate a naive implementation of the block boot-
strap method for (1) exploration in a noisy reward domain  (2) automatically tuning λ in the Cartpole
domain and (3) tracking a moving goal in a navigation task. In all tests we used the Sarsa(λ) learn-
ing algorithm with tile coding function approximation (see Sutton and Barto [25]). All experiments
were evaluated using RL-Glue [27] and averaged over 30 independent runs.

6

(a) Exploration: convergence

(b) Exploration: comparison

Figure 1: Results showing (a) convergence of various exploration techniques in the navigation task
and (b) average cumulative reward of various exploration techniques on the navigation task.

6.1 Exploration

To evaluate the effectiveness of conﬁdence-based exploration  we use a simple two-goal continuous
navigation task. The small goal yields a reward of 1.0 on every visit. The ﬂashing goal yields a
reward selected uniformly from {100 −100  5 −5  50}. The reward on all other steps is zero and
γ = 0.99 (similar results for -1 per step and γ = 1.0). The agent’s observation is a continuous (x  y)
position and actions move the agent {N S E W} perturbed by uniform noise 10% of the time. We
present only the ﬁrst 200 episodes to highlight early learning performance.
Similar to Kaelbling  we select the action with the highest upper conﬁdence in each state. We
compare our conﬁdence exploration algorithm to three baselines commonly used in continuous state
MDPs: (1) -greedy (selecting the highest-value action with probability 1 −   random otherwise) 
(2) optimistic initialization (initializing all weights to a high ﬁxed value to encourage exploration)
and (3) softmax (choosing actions probabilistically according to their values). We also compare our
algorithm to an exploration policy using normal (instead of bootstrapped) intervals to investigate the
effectiveness of making simplifying assumptions on the data distribution. We present the results for
the best parameter setting for each exploration policy for clarity. Figure 1 summarizes the results.
The -greedy policy convergences slowly to the small goal. The optimistic policy slowly converges
to the small goal for lower initializations and does not favour either goal for higher initializations.
The softmax policy navigates to the small goal on most runs and also convergences slowly. The
normal-interval exploration policy does prefer the ﬂashing goal but not as quickly as the bootstrap
policy. Finally  the bootstrap-interval exploration policy achieves highest cumulative reward and is
the only policy that converges to the ﬂashing goal  despite the large variance in the reward signal.

6.2 Adjusting Lambda

To illustrate the effect of adjusting λ based on conﬁdence intervals  we study the Cartpole problem.
We selected Cartpole because the performance of Sarsa is particularly sensitive to λ in this domain.
The objective in Cartpole is to apply forces to a cart on a track to keep a pole from falling over.
An episode ends when the pole falls past a given angle or the cart reaches the end of the track.
The reward is +1 for each step of the episode. The agent’s observations are the cart position and
velocity and the poles’ angle and angular velocity. The Cartpole environment is based on Sutton and
Barto’s [25] pole-balancing task and is available in RL-library [27].
To adjust the λ value  we reset λ on every time step: λ = normalized(ucb) where ucb = 0.9 ∗
ucb + 0.1∗ getUpperConﬁdence(φ(s  a)  θ  α). The conﬁdence estimates were only used to adjust λ
for clarity: exploration was performed using optimistic initialization. Figure 2 presents the average
balancing time on the last episode for various values of λ. The ﬂat line depicts the average balancing
time for Sarsa with λ tuned via conﬁdence estimates. Setting λ via conﬁdence estimates achieves
performance near the best value of λ. We also tested adjusting λ using normal conﬁdence intervals 
however  the normal conﬁdence intervals resulted in worse performance then any ﬁxed value of λ.

7

 0 100 200 300 400 500 600 700 800 900 1000 0 20 40 60 80 100 120 140 160 180 200Average steps until terminationEpisode NumberConﬁdence ExplorationNormale-greedyOptimisticsoftmaxe-greedy-200 0 200 400 600 800 1000 1200 1400 1600 1800 2000 0 20 40 60 80 100 120 140 160 180 200Cummulative RewardEpisode NumberConﬁdence ExplorationNormalOptimisticsoftMaxFigure 2: Performance of Sarsa(λ) on
Cartpole for various values of λ. The
straight
line depicts the performance
of Sarsa with λ adjusted using the
conﬁdence estimation algorithm.

6.3 Non-stationary Navigation Task

One natural source of non-stationarity is introduced by shaping a robot through successive approx-
imations to a goal task (e.g.  changing the reward function). We studied the effects of this form
of non-stationarity  where the agent learns to go to a goal and then another  better goal becomes
available (near the ﬁrst goal to better guide it to the next goal). In our domain  the agent receives -1
reward per step and +10 at termination in a goal region. After 150 episodes  the goal region is tele-
ported to a new location within 50 steps of the previous goal. The agent receives +10 in the new goal
and now 0 in the old goal. We used  = 0 to enable exploration only with optimistic initialization.
We recorded the number of times the agent converged to the new goal with the change after an
initial learning period of 150 episodes. The bootstrap-based explorer found the new goal 70% of
the time. It did not always ﬁnd the new goal because the -1 structure biased it to stay with the
safe 0 goal. Interestingly  optimistic initialization was unable to ﬁnd the new goal because of this
bias  illustrating that the conﬁdence-based explorer detected the increase in variance and promoted
re-exploration automatically.

7 Conclusion

In this work  we investigated constructing conﬁdence intervals on value estimates in the continuous-
state reinforcement learning setting. We presented a robust approach to computing conﬁdence es-
timates for function approximation using bootstrapping  a nonparametric estimation technique. We
proved that our conﬁdence estimate has low coverage error under mild assumptions on the learning
algorithm. In particular  we did so even for a changing policy that uses the conﬁdence estimates. We
illustrated the usefulness of our estimates for three applications: exploration  tuning λ and tracking.
We are currently exploring several directions for future work. We have begun testing the conﬁdence-
based exploration on a mobile robot platform. Despite the results presented in this work  many
traditional deterministic  negative cost-to-goal problems (e.g.  Mountain Car  Acrobot and Puddle
World) are efﬁciently solved using optimistic exploration. Robotic tasks  however  are often more
naturally formulated as continual learning tasks with a sparse reward signal  such as negative reward
for bumping into objects  or a positive reward for reaching some goal. We expect conﬁdence based
techniques to perform better in these settings where the reward range may be truly unknown (e.g.
generated dynamically by a human teacher) and under natural variability in the environment (noisy
sensors and imperfect motion control). We have also begun evaluating conﬁdence-interval driven
behaviour for large-scale  parallel off-policy learning on the same robot platform.
There are several potential algorithmic directions  in addition to those mentioned throughout this
work. We could potentially improve coverage error by extending other bootstrapping techniques 
such as the Markov bootstrap  to non-stationary data. We could also explore the theoretical work
on exponential bounds  such as the Azuma-Hoeffding inequality  to obtain different conﬁdence es-
timates with low coverage error. Finally  it would be interesting to extend the theoretical results in
the paper to sparse representations.
Acknowledgements: We would like to thank Csaba Szepesv´ari  Narasimha Prasad and Daniel Li-
zotte for their helpful comments and NSERC  Alberta Innovates and the University of Alberta for
funding the research.

8

 300 400 500 600 700 800 900 10000.00.10.50.91.0Average steps until terminationLambdaReferences
[1] D.W.K. Andrews. The block-block bootstrap:

72(3):673–700  2004.

Improved asymptotic reﬁnements. Econometrica 

[2] G.E.P. Box  G.M. Jenkins  and G.C. Reinsel. Time series analysis: forecasting and control. Holden-day

San Francisco  1976.

[3] R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal rein-

forcement learning. Journal of Machine Learning Research  3:213–231  2002.

[4] A.C. Davison and DV Hinkley. Bootstrap methods and their application. Cambridge Univ Pr  1997.
[5] E. Delage and S. Mannor. Percentile optimization for Markov decision processes with parameter uncer-

tainty. Operations Research  58(1):203  2010.

[6] Y. Engel  S. Mannor  and R. Meir. Reinforcement learning with Gaussian processes. In Proceedings of

the 22nd international conference on Machine learning  page 208. ACM  2005.

[7] P Hall. The bootstrap and Edgeworth expansion. Springer Series in Statistics  Jan 1997.
[8] Peter Hall  Joel L. Horowitz  and Bing-Yi Jing. On blocking rules for the bootstrap with dependent data.

Biometrika  82(3):561–74  1995.

[9] Todd Hester and Peter Stone. Generalized model learning for reinforcement learning in factored domains.
In The Eighth International Conference on Autonomous Agents and Multiagent Systems (AAMAS)  2009.

[10] J.L. Horowitz. Bootstrap methods for Markov processes. Econometrica  71(4):1049–1082  2003.
[11] Leslie P. Kaelbling. Learning in Embedded Systems (Bradford Books). The MIT Press  May 1993.
[12] SN Lahiri. Edgeworth correction by moving blockbootstrap for stationary and nonstationary data. Ex-

ploring the Limits of Bootstrap  pages 183–214  1992.

[13] S. Lee and PY Lai. Double block bootstrap conﬁdence intervals for dependent data. Biometrika  2009.
[14] L. Li  M.L. Littman  and C.R. Mansley. Online exploration in least-squares policy iteration. In Proc. of

The 8th Int. Conf. on Autonomous Agents and Multiagent Systems  volume 2  pages 733–739  2009.

[15] H.R. Maei  C. Szepesv´ari  S. Bhatnagar  and R.S. Sutton. Toward off-policy learning control with function

approximation. ICM (2010)  50  2010.

[16] S. Mannor  D. Simester  P. Sun  and J.N. Tsitsiklis. Bias and variance in value function estimation. In

Proceedings of the twenty-ﬁrst international conference on Machine learning  page 72. ACM  2004.

[17] Lilyana Mihalkova and Raymond J. Mooney. Using active relocation to aid reinforcement learning. In

FLAIRS Conference  pages 580–585  2006.

[18] Peter Stone Nicholas K. Jong. Model-based exploration in continuous state spaces. In The 7th Symposium

on Abstraction  Reformulation  and Approximation  July 2007.

[19] A. Nouri and M.L. Littman. Multi-resolution exploration in continuous spaces. In NIPS  pages 1209–

1216  2008.

[20] Franc¸ois Rivest and Doina Precup. Combining td-learning with cascade-correlation networks. In ICML 

pages 632–639  2003.

[21] J. Shao and D. Tu. The jackknife and bootstrap. Springer  1995.
[22] A.L. Strehl and M.L. Littman. An empirical evaluation of interval estimation for markov decision pro-

cesses. In Proc. of the 16th Int. Conf. on Tools with Artiﬁcial Intelligence (ICTAI04)  2004.

[23] Alexander L. Strehl and Michael L. Littman. Online linear regression and its application to model-based

reinforcement learning. In NIPS  2007.

[24] R.S. Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In Proceedings

of the National Conference on Artiﬁcial Intelligence  pages 171–171  1992.

[25] R.S. Sutton and A.G. Barto. Introduction to reinforcement learning. MIT Press Cambridge  USA  1998.
[26] R.S. Sutton  A. Koop  and D. Silver. On the role of tracking in stationary environments. In Proceedings

of the 24th international conference on Machine learning  page 878. ACM  2007.

[27] Brian Tanner and Adam White. RL-Glue : Language-independent software for reinforcement-learning

experiments. JMLR  10:2133–2136  September 2009.

[28] B A. Turlach. Bandwidth selection in kernel density estimation: A review.

Statistique  1993.

In CORE and Institut de

[29] J. Zvingelis. On bootstrap coverage probability with dependent data. Computer-Aided Econ.  2001.

9

,Marylou Gabrie
Eric Tramel
Florent Krzakala
Chris Junchi Li
Mengdi Wang
Han Liu
Tong Zhang
Shupeng Su
Chao Zhang
Kai Han
Yonghong Tian