2016,Matrix Completion has No Spurious Local Minimum,Matrix completion is a basic machine learning problem that has wide applications  especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point  it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for matrix completion has no spurious local minima \--- all local minima must also be global. Therefore  many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with \textit{arbitrary} initialization in polynomial time.,Matrix Completion has No Spurious Local Minimum

Rong Ge

Duke University

308 Research Drive  NC 27708
rongge@cs.duke.edu.

Jason D. Lee

University of Southern California
3670 Trousdale Pkwy  CA 90089

jasonlee@marshall.usc.edu.

Tengyu Ma

Princeton University

35 Olden Street  NJ 08540

tengyu@cs.princeton.edu.

Abstract

Matrix completion is a basic machine learning problem that has wide applica-
tions  especially in collaborative ﬁltering and recommender systems. Simple
non-convex optimization algorithms are popular and effective in practice. Despite
recent progress in proving various non-convex algorithms converge from a good
initial point  it remains unclear why random or arbitrary initialization sufﬁces in
practice. We prove that the commonly used non-convex objective function for
positive semideﬁnite matrix completion has no spurious local minima – all local
minima must also be global. Therefore  many popular optimization algorithms such
as (stochastic) gradient descent can provably solve positive semideﬁnite matrix
completion with arbitrary initialization in polynomial time. The result can be
generalized to the setting when the observed entries contain noise. We believe that
our main proof strategy can be useful for understanding geometric properties of
other statistical problems involving partial or noisy observations.

1

Introduction

Matrix completion is the problem of recovering a low rank matrix from partially observed entries. It
has been widely used in collaborative ﬁltering and recommender systems [Kor09  RS05]  dimension
reduction [CLMW11] and multi-class learning [AFSU07]. There has been extensive work on
designing efﬁcient algorithms for matrix completion with guarantees. One earlier line of results
(see [Rec11  CT10  CR09] and the references therein) rely on convex relaxations. These algorithms
achieve strong statistical guarantees  but are quite computationally expensive in practice.
More recently  there has been growing interest in analyzing non-convex algorithms for matrix
completion [KMO10  JNS13  Har14  HW14  SL15  ZWL15  CW15]. Let M 2 Rd⇥d be the target
matrix with rank r ⌧ d that we aim to recover  and let ⌦= {(i  j) : Mi j is observed} be the
set of observed entries. These methods are instantiations of optimization algorithms applied to the
objective1 

f (X) =

1

2 X(i j)2⌦⇥Mi j  (XX>)i j⇤2

 

(1.1)

These algorithms are much faster than the convex relaxation algorithms  which is crucial for their
empirical success in large-scale collaborative ﬁltering applications [Kor09].

1In this paper  we focus on the symmetric case when the true M has a symmetric decomposition M = ZZ T .
Some of previous papers work on the asymmetric case when M = ZW T   which is harder than the symmetric
case.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

All of the theoretical analysis for the nonconvex procedures require careful initialization schemes:
the initial point should already be close to optimum. In fact  Sun and Luo [SL15] showed that after
this initialization the problem is effectively strongly-convex  hence many different optimization
procedures can be analyzed by standard techniques from convex optimization.
However  in practice people typically use a random initialization  which still leads to robust and
fast convergence. Why can these practical algorithms ﬁnd the optimal solution in spite of the non-
convexity? In this work we investigate this question and show that the matrix completion objective
has no spurious local minima. More precisely  we show that any local minimum X of objective
function f (·) is also a global minimum with f (X) = 0  and recovers the correct low rank matrix M.
Our characterization of the structure in the objective function implies that (stochastic) gradient
descent from arbitrary starting point converge to a global minimum. This is because gradient
descent converges to a local minimum [GHJY15  LSJR16]  and every local minimum is also a global
minimum.

1.1 Main results
Assume the target matrix M is symmetric and each entry of M is observed with probability p
independently 2. We assume M = ZZ> for some matrix Z 2 Rd⇥r.
There are two known issues with matrix completion. First  the choice of Z is not unique since
M = (ZR)(ZR)> for any orthonormal matrix Z. Our goal is to ﬁnd one of these equivalent
solutions.
Another issue is that matrix completion is impossible when M is “aligned” with standard basis. For
example  when M is the identity matrix in its ﬁrst r ⇥ r block  we will very likely be observing only
0 entries. To address this issue  we make the following standard assumption:
Assumption 1. For any row Zi of Z  we have kZik 6 µ/pd ·k ZkF . Moreover  Z has a bounded

condition number max(Z)/min(Z) = .

Throughout this paper we think of µ and  as small constants  and the sample complexity depends
polynomially on these two parameters. Also note that this assumption is independent of the choice of
Z: all Z such that ZZT = M have the same row norms and Frobenius norm.
This assumption is similar to the “incoherence” assumption [CR09]. Our assumption is the same as
the one used in analyzing non-convex algorithms [KMO10  SL15].
We enforce X to also satisfy this assumption by a regularizer

f (X) =

+ R(X) 

(1.2)

1

2 X(i j)2⌦⇥Mi j  (XX>)i j⇤2

where R(X) is a function that penalizes X when one of its rows is too large. See Section 4 and
Section A for the precise deﬁnition. Our main result shows that in this setting  the regularized
objective function has no spurious local minimum:
Theorem 1.1. [Informal] All local minimum of the regularized objective (1.1) satisfy XX T =
ZZT = M when p > poly(  r  µ  log d)/d.
Combined with the results in [GHJY15  LSJR16] (see more discussions in Section 1.2)  we have 
Theorem 1.2 (Informal). With high probability  stochastic gradient descent on the regularized
objective (1.1) will converge to a solution X such that XX T = ZZT = M in polynomial time from
any starting point. Gradient descent will converge to such a point with probability 1 from a random
starting point.

Our results are also robust to noise. Even if each entry is corrupted with Gaussian noise of standard
F /d (comparable to the magnitude of the entry itself!)  we can still guarantee that
deviation µ2kZk2
all the local minima satisfy kXX T  ZZTkF 6 " when p is large enough. See the discussion in
Appendix B for results on noisy matrix completion.

2The entries (i  j) and (j  i) are the same. With probability p we observe both entries and otherwise we

observe neither.

2

Our main technique is to show that every point that satisﬁes the ﬁrst and second order necessary
conditions for optimality must be a desired solution. To achieve this we use new ideas to analyze the
effect of the regularizer and show how it is useful in modifying the ﬁrst and second order conditions
to exclude any spurious local minimum.

1.2 Related Work

Matrix Completion. The earlier theoretical works on matrix completion analyzed the nuclear
norm heuristic [Rec11  CT10  CR09]. This line of work has the cleanest and strongest theoretical
guarantees; [CT10  Rec11] showed that if |⌦| & drµ2 log2 d the nuclear norm convex relaxation
recovers the exact underlying low rank matrix. The solution can be computed via the solving a
convex program in polynomial time. However the primary disadvantage of nuclear norm methods
is their computational and memory requirements. The fastest known algorithms have running time
O(d3) and require O(d2) memory  which are both prohibitive for moderate to large values of d.
These concerns led to the development of the low-rank factorization paradigm of [BM03]; Burer and

Monteiro proposed factorizing the optimization variablecM = XX T   and optimizing over X 2 Rd⇥r
instead ofcM 2 Rd⇥d . This approach only requires O(dr) memory  and a single gradient iteration
takes time O(r|⌦|)  so has much lower memory requirement and computational complexity than the
nuclear norm relaxation. On the other hand  the factorization causes the optimization problem to be
non-convex in X  which leads to theoretical difﬁculties in analyzing algorithms. Under incoherence
and sufﬁcient sample size assumptions  [KMO10] showed that well-initialized gradient descent
recovers M. Similary  [HW14  Har14  JNS13] showed that well-initialized alternating least squares
or block coordinate descent converges to M  and [CW15] showed that well-initialized gradient
descent converges to M. [SL15  ZWL15] provided a more uniﬁed analysis by showing that with
careful initialization many algorithms  including gradient descent and alternating least squres  succeed.
[SL15] accomplished this by showing an analog of strong convexity in the neighborhood of the
solution M.

Non-convex Optimization. Recently  a line of work analyzes non-convex optimization by separat-
ing the problem into two aspects: the geometric aspect which shows the function has no spurious
local minimum and the algorithmic aspect which designs efﬁcient algorithms can converge to local
minimum that satisfy ﬁrst and (relaxed versions) of second order necessary conditions.
Our result is the ﬁrst that explains the geometry of the matrix completion objective. Similar geometric
results are only known for a few problems: phase retrieval/synchronization  orthogonal tensor
decomposition  dictionary learning [GHJY15  SQW15  BBV16]. The matrix completion objective
requires different tools due to the sampling of the observed entries  as well as carefully managing the
regularizer to restrict the geometry. Parallel to our work Bhojanapalli et al.[BNS16] showed similar
results for matrix sensing  which is closely related to matrix completion. Loh and Wainwright [LW15]
showed that for many statistical settings that involve missing/noisy data and non-convex regularizers 
any stationary point of the non-convex objective is close to global optima; furthermore  there is a
unique stationary point that is the global minimum under stronger assumptions [LW14].
On the algorithmic side  it is known that second order algorithms like cubic regularization [NP06]
and trust-region [SQW15] algorithms converge to local minima that approximately satisfy ﬁrst and
second order conditions. Gradient descent is also known to converge to local minima [LSJR16] from
a random starting point. Stochastic gradient descent can converge to a local minimum in polynomial
time from any starting point [Pem90  GHJY15]. All of these results can be applied to our setting 
implying various heuristics used in practice are guaranteed to solve matrix completion.

2 Preliminaries

Notations: For ⌦ ⇢ [d] ⇥ [d]  let P⌦ be the linear operator that maps a matrix A to P⌦(A) 
where P⌦(A) has the same values as A on ⌦  and 0 outside of ⌦. We will use the following
matrix norms: k·k F the frobenius norm  k·k spectral norm  |A|1 elementwise inﬁnity norm  and
|A|p!q = maxkxkp=1 kAkq. We use the shorthand kAk⌦ = kP⌦AkF . The trace inner product of
two matrices is hA  Bi = tr(A>B)  and min(X)  max(X) are the smallest and largest singular
values of X. We also use Xi to denote the i-th row of a matrix X.

3

@2

2.1 Necessary conditions for Optimality
Given an objective function f (x) : Rn ! R  we use rf (x) to denote the gradient of the function 
and r2f (x) to denote the Hessian of the function (r2f (x) is an n ⇥ n matrix where [r2f (x)]i j =
f (x)). It is well known that local minima of the function f (x) must satisfy some necessary
@xi@xj
conditions:
Deﬁnition 2.1. A point x satisﬁes the ﬁrst order necessary condition for optimality (later abbreviated
as ﬁrst order optimality condition) if rf (x) = 0. A point x satisﬁes the second order necessary
condition for optimality (later abbreviated as second order optimality condition)if r2f (x) ⌫ 0.
These conditions are necessary for a local minimum because otherwise it is easy to ﬁnd a direction
where the function value decreases. We will also consider a relaxed second order necessary condition 
where we only require the smallest eigenvalue of the Hessian r2f (x) to be not very negative:
Deﬁnition 2.2. For ⌧ > 0  a point x satisﬁes the ⌧-relaxed second order optimality condition  if
r2f (x) ⌫ ⌧ · I.
This relaxation to the second order condition makes the conditions more robust  and allows for
efﬁcient algorithms.
Theorem 2.3. [NP06  SQW15  GHJY15] If every point x that satisﬁes ﬁrst order and ⌧-relaxed
second order necessary condition is a global minimum  then many optimization algorithms (cubic
regularization  trust-region  stochastic gradient descent) can ﬁnd the global minimum up to " error in
function value in time poly(1/"  1/⌧  d).

3 Proof Strategy: “simple” proofs are more generalizable

In this section  we demonstrate the key ideas behind our analysis using the rank r = 1 case. In
particular  we ﬁrst give a “simple” proof for the fully observed case. Then we show this simple
proof can be easily generalized to the random observation case. We believe that this proof strategy is
applicable to other statistical problems involving partial/noisy observations. The proof sketches in
this section are only meant to be illustrative and may not be fully rigorous in various places. We refer
the readers to Section 4 and Section A for the complete proofs.
In the rank r = 1 case  we assume M = zz>  where kzk = 1  and kzk1 6 µpd. Let " ⌧ 1 be the
target accuracy that we aim to achieve in this section and let p = poly(µ  log d)/(d").
For simplicity  we focus on the following domain B of incoherent vectors where the regularizer R(x)
vanishes 

(3.1)

B =⇢x : kxk1 <

2µ

pd .

Inside this domain B  we can restrict our attention to the objective function without the regularizer 
deﬁned as 

˜g(x) =

1
2 ·k P⌦(M  xx>)k2
F .

(3.2)

The global minima of ˜g(·) are z and z with function value 0. Our goal of this section is to
(informally) prove that all the local minima of ˜g(·) are O(p")-close to ±z. In later section we will
formally prove that the only local minima are ±z.
Lemma 3.1 (Partial observation case  informally stated). Under the setting of this section  in the
domain B  all local mimina of the function ˜g(·) are O(p")-close to ±z.
It turns out to be insightful to consider the full observation case when ⌦= [ d]⇥[d]. The corresponding
objective is

g(x) =

1
2 ·k M  xx>k2
F .

(3.3)

Observe that ˜g(x) is a sampled version of the g(x)  and therefore we expect that they share the same
geometric properties. In particular  if g(x) does not have spurious local minima then neither does
˜g(x).

4

Lemma 3.2 (Full observation case  informally stated). Under the setting of this section  in the domain
B  the function g(·) has only two local minima {±z} .
Before introducing the “simple” proof  let us ﬁrst look at a delicate proof that does not generalize
well.

Difﬁcult to Generalize Proof of Lemma 3.2. We compute the gradient and Hessian of g(x) 
rg(x) = M x  kxk2x 
r2g(x) = 2xx>  M + kxk2 · I .Therefore  a critical point x satisﬁes rg(x) = M x  kxk2x = 0 
and thus it must be an eigenvector of M and kxk2 is the corresponding eigenvalue. Next  we
prove that the hessian is only positive deﬁnite at the top eigenvector . Let x be an eigenvector with
eigenvalue  = kxk2  and  is strictly less than the top eigenvalue ⇤. Let z be the top eigenvector.
We have that hz r2g(x)zi = hz  M zi + kxk2 = ⇤ + < 0  which shows that x is not a local
minimum. Thus only z can be a local minimizer  and it is easily veriﬁed that r2g(z) is indeed
positive deﬁnite.
The difﬁculty of generalizing the proof above to the partial observation case is that it uses the
properties of eigenvectors heavily. Suppose we want to imitate the proof above for the partial
observation case  the ﬁrst difﬁculty is how to solve the equation ˜g(x) = P⌦(M  xx>)x = 0.
Moreover  even if we could have a reasonable approximation for the critical points (the solution of
r˜g(x) = 0)  it would be difﬁcult to examine the Hessian of these critical points without having the
orthogonality of the eigenvectors.

“Simple” and Generalizable proof. The lessons from the subsection above suggest us ﬁnd an
alternative proof for the full observation case which is generalizable. The alternative proof will be
simple in the sense that it doesn’t use the notion of eigenvectors and eigenvalues. Concretely  the key
observation behind most of the analysis in this paper is the following 

Proofs that consist of inequalities that are linear in 1⌦ are often easily generalizable to partial
observation case.

Here statements that are linear in 1⌦ mean the statements of the formPij 1(i j)2⌦Tij 6 a. We
will call these kinds of proofs “simple” proofs in this section. Roughly speaking  the observation
follows from the law of large numbers — Suppose Tij  (i  j) 2 [d]⇥ [d] is a sequence of bounded real
numbers  then the sampled sumP(i j)2⌦ Tij =Pi j 1(i j)2⌦Tij is an accurate estimate of the sum
pPi j Tij  when the sampling probability p is relatively large. Then  the mathematical implications
of pP Tij 6 a are expected to be similar to the implications ofP(i j)2⌦ Tij 6 a  up to some small

error introduced by the approximation. To make this concrete  we give below informal proofs for
Lemma 3.2 and Lemma 3.1 that only consists of statements that are linear in 1⌦. Readers will see that
due to the linearity  the proof for the partial observation case (shown on the right column) is a direct
generalization of the proof for the full observation case (shown on the left column) via concentration
inequalities (which will be discussed more at the end of the section).

A “simple” proof for Lemma 3.2.
Claim 1f. Suppose x 2B satisﬁes rg(x) = 0 
then hx  zi2 = kxk4.
Proof. We have 

Generalization to Lemma 3.1.
Claim 1p. Suppose x 2B satisﬁes r˜g(x) = 0 
then hx  zi2 = kxk4  ".
Proof. Imitating the proof on the left  we have

rg(x) = (zz>  xx>)x = 0

) hx rg(x)i = hx  (zz>  xx>)xi = 0
) hx  zi2 = kxk4
Intuitively  this proof says that the norm of a criti-
cal point x is controlled by its correlation with z.
Here at the lasa sampling version of the f the lasa
sampling ver the f the lasa sampling vesio

(3.4)

5

r˜g(x) = P⌦(zz>  xx>)x = 0

) hx r˜g(x)i = hx  P⌦(zz>  xx>)xi = 0
) hx  zi2 > kxk4  "
The last step uses the fact that equation (3.4)
and (3.5) are approximately equal up to scaling
factor p for any x 2B   since (3.5) is a sampled
version of (3.4).

(3.5)

If x 2B

has positive Hessian

Claim 2f.
r2g(x) ⌫ 0  then kxk2 > 1/3.
Proof. By the assumption on x  we have that
hz r2g(x)zi > 0. Calculating the quadratic
form of the Hessian (see Proposition 4.1 for de-
tails) 

If x 2B

has positive Hessian

Claim 2p.
r2˜g(x) ⌫ 0  then kxk2 > 1/3  ".
Proof. Imitating the proof on the left  cal-
culating the quadratic form over
the Hes-
sian at z (see Proposition 4.1)   we have
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

hz r2g(x)zi
= kzx> + xz>k2
 2z>(zz>  xx>)z > 0aaaaaa
) kxk2 + 2hz  xi2 > 1
) kxk2 > 1/3

(since hz  xi2 6 kxk2)

(3.6)

hz r2˜g(x)zi
= kP⌦(zx> + xz>)k2
 2z>P⌦(zz>  xx>)z > 0
)······
) kxk2 > 1/3  "

(3.7)
(same step as the left)

aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

Here we use the fact
phz r2g(x)zi for any x 2B .

that hz r2˜g(x)zi ⇡

With these two claims  we are ready to prove Lemma 3.2 and 3.1 by using another step that is linear
in 1⌦.

Proof of Lemma 3.2. By Claim 1f and 2f  we
have x satisﬁes hx  zi2 > kxk4 > 1/9. More-
over  we have that rg(x) = 0 implies

Proof of Lemma 3.1. By Claim 1p and 2p  we
have x satisﬁes hx  zi2 > kxk4 > 1/9  O(").
Moreover  we have that r˜g(x) = 0 implies

(3.8)

hz rg(x)i = hz  (zz>  xx>)xi = 0

) hx  zi(1  kxk2) = 0
) kxk2 = 1

(by hx  zi2 > 1/9)
Then by Claim 1f again we obtain hx  zi2 = 1 
and therefore x = ±z. aaaaaaaaaaaaaaaaaaa
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

hz r˜g(x)i = hz  P⌦(zz>  xx>)xi = 0
(3.9)
(same step as the left)
)······
(same step as the left)
) kxk2 = 1 ± O(")
Since
(3.9) is the sampled version of equa-
tion (3.8)  we expect they lead to the same con-
clusion up to some approximation. Then by
Claim 1p again we obtain hx  zi2 = 1±O(")  and
therefore x is O(p")-close to either of ±z.
Subtleties regarding uniform convergence.
In the proof sketches above  our key idea is to use
concentration inequalities to link the full observation objective g(x) with the partial observation
counterpart. However  we require a uniform convergence result. For example  we need a statement
like “w.h.p over the choice of ⌦  equation (3.4) and (3.5) are similar to each other up to scaling”. This
type of statement is often only true for x inside the incoherent ball B. The ﬁx to this is the regularizer.
For non-incoherent x  we will use a different argument that uses the property of the regularizer. This
is besides the main proof strategy of this section and will be discussed in subsequent sections.

4 Warm-up: Rank-1 Case

In this section  using the general proof strategy described in previous section  we provide a formal
proof for the rank-1 case. In subsection 4.1  we formally work out the proof sketches of Section 3
inside the incoherent ball. The rest of the proofs is deferred to supplementary material.
In the rank-1 case  the objective function simpliﬁes to 

f (x) =

1
2kP⌦(M  xx>)k2

F + R(x) .

(4.1)

Here we use the the regularization R(x)

R(x) =

dXi=1

h(xi)  and h(t) = (|t| ↵)4 It>↵ .

6

The parameters  and ↵ will be chosen later as in Theorem 4.2. We will choose ↵> 10µ/pd so
that R(x) = 0 for incoherent x  and thus it only penalizes coherent x. Moreover  we note R(x) has
Lipschitz second order derivative. 3
We ﬁrst state the optimality conditions  whose proof is deferred to Appendix A.
Proposition 4.1. The ﬁrst order optimality condition of objective (4.1) is 

and the second order optimality condition requires:

2P⌦(M  xx>)x = rR(x)  

8v 2 Rd  kP⌦(vx> + xv>)k2

F + v>r2R(x)v > 2v>P⌦(M  xx>)v .

Moreover  The ⌧-relaxed second order optimality condition requires

(4.2)

(4.3)

d

(4.4)

8v 2 Rd  kP⌦(vx> + xv>)k2

F + v>r2R(x)v > 2v>P⌦(M  xx>)v  ⌧kvk2 .
where c is a large enough absolute constant  set ↵ = 10µp1/d

We give the precise version of Theorem 1.1 for the rank-1 case.
Theorem 4.2. For p > cµ6 log1.5 d
and  > µ2p/↵2.Then  with high probability over the randomness of ⌦  the only points in Rd that
satisfy both ﬁrst and second order optimality conditions (or ⌧-relaxed optimality conditions with
⌧< 0.1p) are z and z.
In the rest of this section  we will ﬁrst prove that when x is constrained to be incoherent (and hence
the regularizer is 0 and concentration is straightforward) and satisﬁes the optimality conditions  then
x has to be z or z. Then we go on to explain how the regularizer helps us to change the geometry
of those points that are far away from z so that we can rule out them from being local minimum. For
simplicity  we will focus on the part that shows a local minimum x must be close enough to z.
Lemma 4.3. In the setting of Theorem 4.2  suppose x satisﬁes the ﬁrst-order and second-order
optimality condition (4.2) and (4.3). Then when p is deﬁned as in Theorem 4.2 

where " = µ3(pd)1/2.

xx>  zz>2

F 6 O(") .

This turns out to be the main challenge. Once we proved x is close  we can apply the result of Sun
and Luo [SL15] (see Lemma C.1)  and obtain Theorem 4.2.

4.1 Handling incoherent x
To demonstrate the key idea  in this section we restrict our attention to the subset of Rd which contains
incoherent x with `2 norm bounded by 1  that is  we consider 

B =⇢x : kxk1 6 2µ
pd

 kxk 6 1 .

(4.5)

Note that the desired solution z is in B  and the regularization R(x) vanishes inside B.
The following lemmas assume x satisﬁes the ﬁrst and second order optimality conditions  and deduce
a sequence of properties that x must satisfy.
Lemma 4.4. Under the setting of Theorem 4.2   with high probability over the choice of ⌦  for any
x 2B that satisﬁes second-order optimality condition (4.3) we have 

The same is true if x 2B only satisﬁes ⌧-relaxed second order optimality condition for ⌧ 6 0.1p.
Proof. We plug in v = z in the second-order optimality condition (4.3)  and obtain that

F > 2z>P⌦(M  xx>)z .
3This is the main reason for us to choose 4-th power instead of 2-nd power.

P⌦(zx> + xz>)2

(4.6)

7

kxk2 > 1/4.

Intuitively  when restricted to ⌦  the squared Frobenius on the LHS and the quadratic form on the
RHS should both be approximately a p fraction of the unrestricted case. In fact  both LHS and RHS
can be written as the sum of terms of the form hP⌦(uvT )  P⌦(stT )i  because

P⌦(zx> + xz>)2

F = 2hP⌦(zxT )  P⌦(zxT )i + 2hP⌦(zxT )  P⌦(xzT )i
2z>P⌦(M  xx>)z = 2hP⌦(zzT )  P⌦(zzT )i  2hP⌦(xxT )  P⌦(zzT )i.

Therefore we can use concentration inequalities (Theorem D.1)  and simplify the equation

LHS of (4.6) = pzx> + xz>2

F ± O(ppdkxk2
= 2pkxk2kzk2 + 2phx  zi2 ± O(p")  
pd ). Similarly  by Theorem D.1 again  we have

1kzk2

1kxk2kzk2)

(Since x  z 2B )

where " = O(µ2q log d

RHS of (4.6) = 2hP⌦(zz>)  P⌦(zz>)i  hP⌦(xx>)  P⌦(zz>)i

(Since M = zz>)
(by Theorem D.1 and x  z 2B )
(Note that even we use the ⌧-relaxed second order optimality condition  the RHS only becomes
1.99pkzk4  2phx  zi2 ± O(p") which does not effect the later proofs.)
Therefore plugging in estimates above back into equation (4.6)  we have that

= 2pkzk4  2phx  zi2 ± O(p")

2pkxk2kzk2 + 2phx  zi2 ± O(p") > 2kzk4  2hx  zi2 ± O(p")  

which implies that 6pkxk2kzk2 > 2pkxk2kzk2 + 4phx  zi2 > 2pkzk4  O(p"). Using kzk2 = 1 
and " being sufﬁciently small  we complete the proof.

Next we use ﬁrst order optimality condition to pin down another property of x – it has to be close
to z after scaling. Note that this doesn’t mean directly that x has to be close to z since x = 0 also
satisﬁes ﬁrst order optimality condition (and therefore the conclusion (4.7) below).
Lemma 4.5. With high probability over the randomness of ⌦  for any x 2B that satisﬁes ﬁrst-order
optimality condition (4.2)  we have that x also satisﬁes

where " = ˜O(µ3(pd)1/2).

hz  xiz  kxk2x 6 O(") .

(4.7)

Finally we combine the two optimality conditions and show equation (4.7) implies xxT must be
close to zzT .

Lemma 4.6. Suppose vector x satisﬁes that kxk2 > 1/4  and thathz  xiz  kxk2x 6 . Then

for  2 (0  0.1) 

5 Conclusions

xx>  zz>2

F 6 O() .

Although the matrix completion objective is non-convex  we showed the objective function has very
nice properties that ensures the local minima are also global. This property gives guarantees for many
basic optimization algorithms. An important open problem is the robustness of this property under
different model assumptions: Can we extend the result to handle asymmetric matrix completion? Is
it possible to add weights to different entries (similar to the settings studied in [LLR16])? Can we
replace the objective function with a different distance measure rather than Frobenius norm (which is
related to works on 1-bit matrix sensing [DPvdBW14])? We hope this framework of analyzing the
geometry of objective function can be applied to other problems.

8

References

[AFSU07] Yonatan Amit  Michael Fink  Nathan Srebro  and Shimon Ullman. Uncovering shared structures in multiclass classiﬁcation. In

Proceedings of the 24th international conference on Machine learning  pages 17–24. ACM  2007.

[BBV16] Afonso S Bandeira  Nicolas Boumal  and Vladislav Voroninski. On the low-rank approach for semideﬁnite programs arising

in synchronization and community detection. arXiv preprint arXiv:1602.04426  2016.

[BM03] Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semideﬁnite programs via low-rank

factorization. Mathematical Programming  95(2):329–357  2003.

[BNS16] S. Bhojanapalli  B. Neyshabur  and N. Srebro. Global Optimality of Local Search for Low Rank Matrix Recovery. ArXiv

e-prints  May 2016.

[CLMW11] Emmanuel J Cand`es  Xiaodong Li  Yi Ma  and John Wright. Robust principal component analysis?

(JACM)  58(3):11  2011.

Journal of the ACM

[CR09] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational

mathematics  9(6):717–772  2009.

[CT10] Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. Information Theory 

IEEE Transactions on  56(5):2053–2080  2010.

[CW15] Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent: General statistical and algo-

rithmic guarantees. arXiv preprint arXiv:1509.03025  2015.

[DPvdBW14] Mark A Davenport  Yaniv Plan  Ewout van den Berg  and Mary Wootters. 1-bit matrix completion. Information and Inference 

3(3):189–223  2014.

[GHJY15] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor decom-

position. arXiv:1503.02101  2015.

[Har14] Moritz Hardt. Understanding alternating minimization for matrix completion. In FOCS 2014. IEEE  2014.

[HKZ12] Daniel Hsu  Sham M Kakade  and Tong Zhang. A tail inequality for quadratic forms of subgaussian random vectors. Electron.

Commun. Probab  17(52):1–6  2012.

[HW14] Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. In COLT 2014  pages 638–678  2014.

[Imb10] R. Imbuzeiro Oliveira. Sums of random Hermitian matrices and an inequality by Rudelson. ArXiv e-prints  April 2010.

[JNS13] Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Pro-

ceedings of the forty-ﬁfth annual ACM symposium on Theory of computing  pages 665–674. ACM  2013.

[KMO10] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from a few entries. Information Theory 

IEEE Transactions on  56(6):2980–2998  2010.

[Kor09] Yehuda Koren. The bellkor solution to the netﬂix grand prize. Netﬂix prize documentation  81  2009.

[LLR16] Yuanzhi Li  Yingyu Liang  and Andrej Risteski. Recovery guarantee of weighted low-rank approximation via alternating

minimization. arXiv preprint arXiv:1602.02262  2016.

[LSJR16] Jason D Lee  Max Simchowitz  Michael I Jordan  and Benjamin Recht. Gradient descent converges to minimizers. University

of California  Berkeley  1050:16  2016.

[LW14] Po-Ling Loh and Martin J Wainwright. Support recovery without incoherence: A case for nonconvex regularization. arXiv

preprint arXiv:1412.5632  2014.

[LW15] Po-Ling Loh and Martin J. Wainwright. Regularized m-estimators with nonconvexity: statistical and algorithmic theory for

local optima. Journal of Machine Learning Research  16:559–616  2015.

[NP06] Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance. Mathematical Pro-

gramming  108(1):177–205  2006.

[Pem90] Robin Pemantle. Nonconvergence to unstable points in urn models and stochastic approximations. The Annals of Probability 

pages 698–712  1990.

[Rec11] Benjamin Recht. A simpler approach to matrix completion. The Journal of Machine Learning Research  12:3413–3430  2011.

[RS05] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings

of the 22nd international conference on Machine learning  pages 713–719. ACM  2005.

[SL15] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization.

Science (FOCS)  2015 IEEE 56th Annual Symposium on  pages 270–289. IEEE  2015.

In Foundations of Computer

[SQW15] Ju Sun  Qing Qu  and John Wright. When are nonconvex problems not scary? arXiv preprint arXiv:1510.06096  2015.

[ZWL15] Tuo Zhao  Zhaoran Wang  and Han Liu. A nonconvex optimization framework for low rank matrix estimation. In Advances in

Neural Information Processing Systems  pages 559–567  2015.

9

,Falk Lieder
Dillon Plunkett
Jessica Hamrick
Stuart Russell
Nicholas Hay
Tom Griffiths
Andrew Miller
Albert Wu
Jeff Regier
Mr. Prabhat
Ryan Adams
Rong Ge
Jason Lee
Tengyu Ma
Zhiqiang Xu