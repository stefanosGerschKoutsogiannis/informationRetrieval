2010,Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models,A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include $K$-fold cross-validation ($K$-CV)  Akaike information criterion (AIC)  and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems  they are not suitable in high dimensional settings. In this paper  we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions  we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability  all the true edges will be included in the selected model even when the graph size asymptotically increases with the sample size. Empirically  the performance of StARS is compared with the state-of-the-art model selection procedures  including $K$-CV  AIC  and BIC  on both synthetic data and a real microarray dataset. StARS outperforms all competing procedures.,Stability Approach to Regularization Selection
(StARS) for High Dimensional Graphical Models

Han Liu Kathryn Roeder Larry Wasserman

Carnegie Mellon University

Pittsburgh  PA 15213

Abstract

A challenging problem in estimating high-dimensional graphical models is to
choose the regularization parameter in a data-dependent way. The standard tech-
niques include K-fold cross-validation (K-CV)  Akaike information criterion
(AIC)  and Bayesian information criterion (BIC). Though these methods work
well for low-dimensional problems  they are not suitable in high dimensional set-
tings. In this paper  we present StARS: a new stability-based method for choosing
the regularization parameter in high dimensional inference for undirected graphs.
The method has a clear interpretation: we use the least amount of regularization
that simultaneously makes a graph sparse and replicable under random sampling.
This interpretation requires essentially no conditions. Under mild conditions  we
show that StARS is partially sparsistent in terms of graph estimation: i.e. with
high probability  all the true edges will be included in the selected model even
when the graph size diverges with the sample size. Empirically  the performance
of StARS is compared with the state-of-the-art model selection procedures  in-
cluding K-CV  AIC  and BIC  on both synthetic data and a real microarray dataset.
StARS outperforms all these competing procedures.

1 Introduction
Undirected graphical models have emerged as a useful tool because they allow for a stochastic
description of complex associations in high-dimensional data. For example  biological processes in
a cell lead to complex interactions among gene products. It is of interest to determine which features
of the system are conditionally independent. Such problems require us to infer an undirected graph
from i.i.d. observations. Each node in this graph corresponds to a random variable and the existence
of an edge between a pair of nodes represent their conditional independence relationship.
Gaussian graphical models [4  23  5  9] are by far the most popular approach for learning high di-
mensional undirected graph structures. Under the Gaussian assumption  the graph can be estimated
using the sparsity pattern of the inverse covariance matrix. If two variables are conditionally inde-
pendent  the corresponding element of the inverse covariance matrix is zero. In many applications 
estimating the the inverse covariance matrix is statistically challenging because the number of fea-
tures measured may be much larger than the number of collected samples. To handle this challenge 
the graphical lasso or glasso [7  24  2] is rapidly becoming a popular method for estimating sparse
undirected graphs. To use this method  however  the user must specify a regularization parameter
λ that controls the sparsity of the graph. The choice of λ is critical since different λ’s may lead to
different scientiﬁc conclusions of the statistical inference. Other methods for estimating high dimen-
sional graphs include [11  14  10]. They also require the user to specify a regularization parameter.
The standard methods for choosing the regularization parameter are AIC [1]  BIC [19] and cross
validation [6]. Though these methods have good theoretical properties in low dimensions  they are
not suitable for high dimensional problems. In regression  cross-validation has been shown to overﬁt
the data [22]. Likewise  AIC and BIC tend to perform poorly when the dimension is large relative to
the sample size. Our simulations conﬁrm that these methods perform poorly when used with glasso.

1

A new approach to model selection  based on model stability  has recently generated some interest
in the literature [8]. The idea  as we develop it  is based on subsampling [15] and builds on the
approach of Meinshausen and B¨uhlmann [12]. We draw many random subsamples and construct a
graph from each subsample (unlike K-fold cross-validation  these subsamples are overlapping). We
choose the regularization parameter so that the obtained graph is sparse and there is not too much
variability across subsamples. More precisely  we start with a large regularization which corresponds
to an empty  and hence highly stable  graph. We gradually reduce the amount of regularization
until there is a small but acceptable amount of variability of the graph across subsamples. In other
words  we regularize to the point that we control the dissonance between graphs. The procedure
is named StARS: Stability Approach to Regularization Selection. We study the performance of
StARS by simulations and theoretical analysis in Sections 4 and 5. Although we focus here on
graphical models  StARS is quite general and can be adapted to other settings including regression 
classiﬁcation  clustering  and dimensionality reduction.
In the context of clustering  results of stability methods have been mixed. Weaknesses of stability
have been shown in [3]. However  the approach was successful for density-based clustering [17].
For graph selection  Meinshausen and B¨uhlmann [12] also used a stability criterion; however  their
approach differs from StARS in its fundamental conception. They use subsampling to produce a new
and more stable regularization path then select a regularization parameter from this newly created
path  whereas we propose to use subsampling to directly select one regularization parameter from
the original path. Our aim is to ensure that the selected graph is sparse  but inclusive  while they
aim to control the familywise type I errors. As a consequence  their goal is contrary to ours: instead
of selecting a larger graph that contains the true graph  they try to select a smaller graph that is
contained in the true graph. As we will discuss in Section 3  in speciﬁc application domains like
gene regulatory network analysis  our goal for graph selection is more natural.

(

X(1)  . . .   X(p)

)T be a random vector with distribution P . The undirected graph G =

2 Estimating a High-dimensional Undirected Graph
Let X =
(V  E) associated with P has vertices V = {X(1)  . . .   X(p)} and a set of edges E corresponding to
pairs of vertices. In this paper  we also interchangeably use E to denote the adjacency matrix of the
graph G. The edge corresponding to X(j) and X(k) is absent if X(j) and X(k) are conditionally
independent given the other coordinates of X. The graph estimation problem is to infer E from i.i.d.
observed data X1  . . .   Xn where Xi = (Xi(1)  . . .   Xi(p))T .
Suppose now that P is Gaussian with mean vector µ and covariance matrix Σ. Then the edge
corresponding to X(j) and X(k) is absent if and only if Ωjk = 0 where Ω = Σ−1. Hence  to
estimate the graph we only need to estimate the sparsity pattern of Ω. When p could diverge with n 
estimating Ω is difﬁcult. A popular approach is the graphical lasso or glasso [7  24  2]. Using glasso 
we estimate Ω as follows: Ignoring constants  the log-likelihood (after maximizing over µ) can be
written as ℓ(Ω) = log |Ω| − trace

where bΣ is the sample covariance matrix. With a positive
regularization parameter λ  the glasso estimator bΩ(λ) is obtained by minimizing the regularized

negative log-likelihood

)

(bΣΩ
bΩ(λ) = arg min

{
−ℓ(Ω) + λ||Ω||1

}

∑

(1)

j;k

Ω≻0

where ||Ω||1 =

|Ωjk| is the elementwise ℓ1-norm of Ω. The estimated graph bG(λ) =
(V bE(λ)) is then easily obtained from bΩ(λ): for i ̸= j  an edge (i  j) ∈ bE(λ) if and only if
the corresponding entry inbΩ(λ) is nonzero. Friedman et al. [7] give a fast algorithm for calculating
bΩ(λ) over a grid of λs ranging from small to large. By taking advantage of the fact that the objec-
in each iteration by solving a lasso regression [21]. The resulting regularization path bΩ(λ) for all
bΩ(λ) could recover the true graph with high probability. However  these types of results are either

λs has been shown to have excellent theoretical properties [18  16]. For example  Ravikumar et al.
[16] show that  if the regularization parameter λ satisﬁes a certain rate  the corresponding estimator

tive function in (1) is convex  their algorithm iteratively estimates a single row (and column) of Ω

asymptotic or non-asymptotic but with very large constants. They are not practical enough to guide
the choice of the regularization parameter λ in ﬁnite-sample settings.

2

3 Regularization Selection

In Equation (1)  the choice of λ is critical because λ controls the sparsity level of bG(λ). Larger values
our goal of graph regularization parameter selection is to choose one bΛ ∈ Gn  such that the true
graph E is contained in bE(bΛ) with high probability. In other words  we want to “overselect” instead

of λ tend to yield sparser graphs and smaller values of λ yield denser graphs. It is convenient to
deﬁne Λ = 1/λ so that small Λ corresponds to a more sparse graph. In particular  Λ = 0 corresponds
to the empty graph with no edges. Given a grid of regularization parameters Gn = {Λ1  . . .   ΛK} 

of “underselect”. Such a choice is motivated by application problems like gene regulatory networks
reconstruction  in which we aim to study the interactions of many genes. For these types of studies 
we tolerant some false positives but not false negatives. Speciﬁcally  it is acceptable that an edge
presents but the two genes corresponding to this edge do not really interact with each other. Such
false positives can generally be screened out by more ﬁne-tuned downstream biological experiments.
However  if one important interaction edge is omitted at the beginning  it’s very difﬁcult for us to
re-discovery it by follow-up analysis. There is also a tradeoff: we want to select a denser graph
which contains the true graph with high probability. At the same time  we want the graph to be as
sparse as possible so that important information will not be buried by massive false positives. Based
on this rationale  an “underselect” method  like the approach of Meinshausen and B¨uhlmann[12] 
does not really ﬁt our goal. In the following  we start with an overview of several state-of-the-art
regularization parameter selection methods for graphs. We then introduce our new StARS approach.

3.1 Existing Methods

The regularization parameter is often chosen using AIC or BIC. Let bΩ(Λ) denote the estimator

corresponding to Λ. Let d(Λ) denote the degree of freedom (or the effective number of free pa-
rameters) of the corresponding Gaussian model. AIC chooses Λ to minimize −2ℓ
+ 2d(Λ)
and BIC chooses Λ to minimize −2ℓ
+ d(Λ) · log n. The usual theoretical justiﬁcation for
these methods assumes that the dimension p is ﬁxed as n increases; however  in the case where
p > n this justiﬁcation is not applicable.
In fact  it’s even not straightforward how to estimate
the degree of freedom d(Λ) when p is larger than n . A common practice is to calculate d(Λ) as

d(Λ) = m(Λ)(m(Λ)− 1)/2 + p where m(Λ) denotes the number of nonzero elements ofbΩ(Λ). As

(bΩ(Λ)

(bΩ(Λ)

we will see in our experiments  AIC and BIC tend to select overly dense graphs in high dimensions.
Another popular method is K-fold cross-validation (K-CV). For this procedure the data is parti-
tioned into K subsets. Of the K subsets one is retained as the validation data  and the remaining
K − 1 ones are used as training data. For each Λ ∈ Gn  we estimate a graph on the K − 1 training
sets and evaluate the negative log-likelihood on the retained validation set. The results are averaged
over all K folds to obtain a single CV score. We then choose Λ to minimize the CV score over he
whole grid Gn. In regression  cross-validation has been shown to overﬁt [22]. Our experiments will
conﬁrm this is true for graph estimation as well.

)

)

3.2 StARS: Stability Approach to Regularization Selection
The StARS approach is to choose Λ based on stability. When Λ is 0  the graph is empty and two
datasets from P would both yield the same graph. As we increase Λ  the variability of the graph
increases and hence the stability decreases. We increase Λ just until the point where the graph
becomes variable as measured by the stability. StARS leads to a concrete rule for choosing Λ.
Let b = b(n) be such that 1 < b(n) < n. We draw N random subsamples S1  . . .   SN from
X1  . . .   Xn  each of size b. There are
sub-
samples. However  Politis et al. [15] show that it sufﬁces in practice to choose a large number N
of subsamples at random. Note that  unlike bootstrapping [6]  each subsample is drawn without
replacement. For each Λ ∈ Gn  we construct a graph using the glasso for each subsample. This
N (Λ). Focus for now on one edge (s  t) and one
value of Λ. Let ψΛ(·) denote the glasso algorithm with the regularization parameter Λ. For any
st(Sj) = 0 if the algorithm does
subsample Sj let ψΛ
st(Λ)  we
not put an edge between (s  t). Deﬁne θb

results in N estimated edge matrices bEb
use a U-statistic of order b  namely bθb

)
(
1(Λ)  . . .  bEb
N∑

st(Sj) = 1 if the algorithm puts an edge and ψΛ

st(Λ) = P(ψΛ
st(Sj).
ψΛ

such subsamples. Theoretically one uses all

st(X1  . . .   Xb) = 1). To estimate θb

st(Λ) =

)

(

n
b

n
b

1
N

j=1

3

p
2

s<t

st/

}

st(Λ) = 2θb

st(Λ)(1 − θb

st(Λ)) be
Now deﬁne the parameter ξb
st(Λ)  in addition to being twice the variance of the Bernoulli indicator of the
its estimate. Then ξb
edge (s  t)  has the following nice interpretation: For each pair of graphs  we can ask how often they
st(Λ) is the fraction of times they disagree. For Λ ∈ Gn  we
disagree on the presence of the edge: ξb
regard ξb

st(Λ)(1 −bθb
)
(
st(Λ) as a measure of instability of the edge across subsamples  with 0 ≤ ξb
st(Λ) ≤ 1/2.
. Clearly on the

st(Λ)) and letbξb
st(Λ) = 2bθb
∑
bξb
Deﬁne the total instability by averaging over all edges: bDb(Λ) =
boundary bDb(0) = 0  and bDb(Λ) generally will increase as Λ increases. However  when Λ gets very
large  all the graphs will become dense and bDb(Λ) will begin to decrease. Subsample stability for
bDb(t). Finally  our StARS
For this reason we monotonize bDb(Λ) by deﬁning Db(Λ) = sup0≤t≤Λ
{
approach chooses Λ by deﬁningbΛs = sup
Λ : Db(Λ) ≤ β
is that all quantities bE bθ bξ bD depend on the subsampling block size b. Since StARS is based on

It may seem that we have merely replaced the problem of choosing Λ with the problem of choosing
β  but β is an interpretable quantity and we always set a default value β = 0.05. One thing to note

large Λ is essentially an artifact. We are interested in stability for sparse graphs not dense graphs.

for a speciﬁed cut point value β.

subsampling  the effective sample size for estimating the selected graph is b instead of n. Compared
with methods like BIC and AIC which fully utilize all n data points. StARS has some efﬁciency
loss in low dimensions. However  in high dimensional settings  the gain of StARS on better graph
selection signiﬁcantly dominate this efﬁciency loss. This fact is conﬁrmed by our experiments.
4 Theoretical Properties
The StARS procedure is quite general and can be applied with any graph estimation algorithms.
Here  we provide its theoretical properties. We start with a key theorem which establishes the rates
of convergence of the estimated stability quantities to their population means. We then discuss the
implication of this theorem on general gaph regularization selection problems.
Let Λ be an element in the grid Gn = {Λ1  . . .   ΛK} where K is a polynomial of n. We denote

Db(Λ) = E(bDb(Λ)). The quantity bξb

st(Λ) and bDb(Λ) is an estimate of

st(Λ) is an estimate of ξb

Db(Λ). Standard U-statistic theory guarantees that these estimates have good uniform convergence
properties to their population quantities:
Theorem 1. (Uniform Concentration) The following statements hold with no assumptions on P .
For any δ ∈ (0  1)  with probability at least 1 − δ  we have

√

|bξb
√
st(Λ) − ξb
st(Λ)| ≤

18b (2 log p + log(2/δ))

n

 

(2)

(3)

st(Λ) is a U-statistic of order b. Hence  by Hoeffding’s inequality for U-statistics

∀Λ ∈ Gn  max

s<t

max
Λ∈Gn

[20]  we have  for any ϵ > 0 

Proof. Note thatbθb
Nowbξb
st(Λ) is just a function of the U-statisticbθb
st(Λ)(1 −bθb
|bξb
st(Λ) −(bθb
st(Λ) − ξb

|bDb(Λ) − Db(Λ)| ≤
P(|bθb
st(Λ) − θb
st(Λ)| = 2|bθb
= 2|bθb
≤ 2|bθb
≤ 2|bθb
st(Λ) − θb
≤ 2|bθb
st(Λ) − θb
= 6|bθb
st(Λ) − θb
st(Λ)| ≤ 6|bθb
st(Λ) − θb
st(Λ) − θb
|bξb
st(Λ) − ξb

we obtain: for each Λ ∈ Gn 
P(max

we have |bξb

st(Λ) − ξb

s<t

18b (log K + 4 log p + log (1/δ))

.

n

.

(4)

)

st(Λ)| > ϵ) ≤ 2 exp

(−2nϵ2/b
)2|
(
st(Λ)(1 − θb
st(Λ))|
)2|
)2 −(
st(Λ) +
st(Λ)
θb
st(Λ))(bθb
st(Λ)
st(Λ)
θb
st(Λ) − θb
st(Λ) − θb

st(Λ). Note that
st(Λ)) − θb
st(Λ)

)2 − θb
st(Λ)| + 2|(bθb
st(Λ)| + 2|(bθb
st(Λ)| + 4|bθb

(5)
(6)
(7)
(8)
(9)
st(Λ)| 
(10)
st(Λ)|. Using (4) and the union bound over all the edges 
st(Λ)| > 6ϵ) ≤ 2p2 exp

(−2nϵ2/b
)

st(Λ))|

st(Λ) + θb

st(Λ)|

(11)

.

4

)

(

Using two union bound arguments over the K values of Λ and all the p(p − 1)/2 edges  we have:
st(Λ)| > ϵ) (12)
(13)

|bξb
(−nϵ2/(18b)
)
st(Λ) − ξb

|bDb(Λ) − Db(Λ)| ≥ ϵ

≤ |Gn| · p(p − 1)
2
≤ K · p4 · exp

· P(max

max
Λ∈Gn

s<t

P

.

Equations (2) and (3) follow directly from (11) and the above exponential probability inequality.

Theorem 1 allows us to explicitly characterize the high-dimensional scaling of the sample size n 
dimensionality p  subsampling block size b  and the grid size K. More speciﬁcally  we get

(

n
np4K

b log

) → ∞ =⇒ max

Λ∈Gn

|bDb(Λ) − Db(Λ)| P→ 0

K = nc2  and p ≤ exp (n(cid:13)) for some γ < 1/2  the estimated total stability bDb(Λ) still converges
procedures satisfying certain conditions. Let ψ be a graph estimation procedure. We denote bEb(Λ)

by setting δ = 1/n in Equation (3). From (14)  let c1  c2 be arbitrary positive constants  if b = c1
to its mean Db(Λ) uniformly over the whole grid Gn.
We now discuss the implication of Theorem 1 to graph regularization selection problems. Due to
the generality of StARS  we provide theoretical justiﬁcations for a whole family of graph estimation

as the estimated edge set using the regularization parameter Λ by applying ψ on a subsampled dataset
with block size b. To establish graph selection result  we start with two technical assumptions:

n 

(A1) ∃Λo ∈ Gn  such that maxΛ≤Λo∧Λ∈Gn Db(Λ) ≤ β/2 for large enough n.
(A2) For any Λ ∈ Gn and Λ ≥ Λo  P

E ⊂ bEb(Λ)

) → 1 as n → ∞.

(

(14)
√

Note that Λo here depends on the sample size n and does not have to be unique. To understand
the above conditions  (A1) assumes that there exists a threshold Λo ∈ Gn  such that the population
quantity Db(Λ) is small for all Λ ≤ Λo. (A2) requires that all estimated graphs using regularization
parameters Λ ≥ Λo contain the true graph with high probability. Both assumptions are mild and
should be satisﬁed by most graph estimation algorithm with reasonable behaviors. More detailed
analysis on how glasso satisﬁes (A1) and (A2) will be provided in the full version of this paper.
There is a tradeoff on the design of the subsampling block size b . To make (A2) hold  we require
√
suggested value is b = ⌊10
n⌋  which balances both the theoretical and empirical performance
well. The next theorem provides the graph selection performance of StARS:
√
Theorem 2. (Partial Sparsistency): Let ψ to be a graph estimation algorithm. We assume (A1) and
(A2) hold for ψ using b = ⌊10
be the selected regularization parameter using the StARS procedure with a constant cutting point β.
Then  if p ≤ exp (n(cid:13)) for some γ < 1/2  we have

b to be large. However  to make bDb(Λ) concentrate to Db(Λ) fast  we require b to be small. Our
n⌋ and |Gn| = K = nc1 for some constant c1 > 0. LetbΛs ∈ Gn
(
E ⊂ bEb(bΛs)

) → 1 as n → ∞.

(15)

P

|bDb(Λ) − Db(Λ)| ≤ β/2. The scaling of

Proof. We deﬁne An to be the event that maxΛ∈Gn
n  K  b  p in the theorem satisﬁes the L.H.S. of (14)  which implies that P(An) → 1 as n → ∞.
Using (A1)  we know that  on An 

bDb(Λ) ≤ max

|bDb(Λ) − Db(Λ)| + max

max

This implies that  on An bΛs ≥ Λo. The result follows by applying (A2) and a union bound.

Λ≤Λo∧Λ∈Gn

Λ≤Λo∧Λ∈Gn

Λ∈Gn

Db(Λ) ≤ β.

(16)

5 Experimental Results
We now provide empirical evidence to illustrate the usefulness of StARS and compare it with several
state-of-the-art competitors  including 10-fold cross-validation (K-CV)  BIC  and AIC. For StARS
n] and set the cut point β = 0.05. We ﬁrst

we always use subsampling block size b(n) = ⌊10 · √

5

quantitatively evaluate these methods on two types of synthetic datasets  where the true graphs are
known. We then illustrate StARS on a microarray dataset that records the gene expression levels
from immortalized B cells of human subjects. On all high dimensional synthetic datasets  StARS
signiﬁcantly outperforms its competitors. On the microarray dataset  StARS obtains a remarkably
simple graph while all competing methods select what appear to be overly dense graphs.

5.1 Synthetic Data
To quantitatively evaluate the graph estimation performance  we adapt the criteria including pre-
cision  recall  and F1-score from the information retrieval literature. Let G = (V  E) be a p-

dimensional graph and let bG = (V bE) be an estimated graph. We deﬁne precision = |bE ∩ E|/|bE| 
recall = |bE ∩ E|/|E|  and F1-score = 2 · precision · recall/(precision + recall). In other words 

Precision is the number of correctly estimated edges divided by the total number of edges in the
estimated graph; recall is the number of correctly estimated edges divided by the total number of
edges in the true graph; the F1-score can be viewed as a weighted average of the precision and recall 
where an F1-score reaches its best value at 1 and worst score at 0. On the synthetic data where we
know the true graphs  we also compare the previous methods with an oracle procedure which selects
the optimal regularization parameter by minimizing the total number of different edges between the
estimated and true graphs along the full regularization path. Since this oracle procedure requires the
knowledge of the truth graph  it is not a practical method. We only present it here to calibrate the
inherent challenge of each simulated scenario. To make the comparison fair  once the regulariza-
√
tion parameters are selected  we estimate the oracle and StARS graphs only based on a subsampled
n⌋. In contrast  the K-CV  BIC  and AIC graphs are estimated using
dataset with size b(n) = ⌊10
the full dataset. More details about this issue were discussed in Section 3.
We generate data from sparse Gaussian graphs  neighborhood graphs and hub graphs  which mimic
characteristics of real-wolrd biological networks. The mean is set to be zero and the covariance
matrix Σ = Ω−1. For both graphs  the diagonal elements of Ω are set to be one. More speciﬁcally:

(√

)−1 exp

(−4∥yi − yj∥2

)

2π

1. Neighborhood graph: We ﬁrst uniformly sample y1  . . .   yn from a unit square. We then set
Ωij = Ωji = ρ with probability
. All the rest Ωij are set to
be zero. The number of nonzero off-diagonal elements of each row or column is restricted
to be smaller than ⌊1/ρ⌋. In this paper  ρ is set to be 0.245.
2. Hub graph: The rows/columns are partitioned into J equally-sized disjoint groups: V1 ∪
V2 . . . ∪ VJ = {1  . . .   p}  each group is associated with a “pivotal” row k. Let |V1| = s.
We set Ωik = Ωki = ρ for i ∈ Vk and Ωik = Ωki = 0 otherwise. In our experiment 
J = ⌊p/s⌋  k = 1  s + 1  2s + 1  . . .  and we always set ρ = 1/(s + 1) with s = 20.

We generate synthetic datasets in both low-dimensional (n = 800  p = 40) and high-dimensional
(n = 400  p = 100) settings. Table 1 provides comparisons of all methods  where we repeat the
experiments 100 times and report the averaged precision  recall  F1-score with their standard errors.

Table 1: Quantitative comparison of different methods on the datasets from the neighborhood and hub graphs.

Neighborhood graph: n =800  p=40

Neighborhood graph: n=400  p =100

Methods

Precision

Recall

F1-score

Precision

Recall

F1-score

Oracle
StARS
K-CV
BIC
AIC

0:9222 (0:05)
0:7204 (0:08)
0:1394 (0:02)
0:9738 (0:03)
0:8696 (0:11)

0:9070 (0:07)
0:9530 (0:05)
1:0000 (0:00)
0:9948 (0:02)
0:9996 (0:01)

0:9119 (0:04)
0:8171 (0:05)
0:2440 (0:04)
0:9839 (0:01)
0:9236 (0:07)

Hub graph: n =800  p=40

0:7473 (0:09)
0:6366 (0:07)
0:1383 (0:01)
0:1796 (0:11)
0:1279 (0:00)

0:8001 (0:06)
0:8718 (0:06)
1:0000 (0:00)
1:0000 (0:00)
1:0000 (0:00)
Hub graph: n=400  p =100

0:7672 (0:07)
0:7352 (0:07)
0:2428 (0:01)
0:2933 (0:13)
0:2268 (0:01)

Methods

Precision

Recall

F1-score

Precision

Recall

F1-score

Oracle
StARS
K-CV
BIC
AIC

0:9793 (0:01)
0:4377 (0:02)
0:2383 (0:09)
0:4879 (0:05)
0:2522 (0:09)

1:0000 (0:00)
1:0000 (0:00)
1:0000 (0:00)
1:0000 (0:00)
1:0000 (0:00)

0:9895 (0:01)
0:6086 (0:02)
0:3769 (0:01)
0:6542 (0:05)
0:3951 (0:00)

0:8976 (0:02)
0:4572 (0:01)
0:1574 (0:01)
0:2155 (0:00)
0:1676 (0:00)

1:0000 (0:00)
1:0000 (0:00)
1:0000 (0:00)
1:0000 (0:00)
1:0000 (0:00)

0:9459 (0:01)
0:6274 (0:01)
0:2719 (0:00)
0:3545 (0:01)
0:2871 (0:00)

For low-dimensional settings where n ≫ p  the BIC criterion is very competitive and performs the
best among all the methods. In high dimensional settings  however  StARS clearly outperforms all

6

only the subsampled dataset with size b(n) = ⌊10 · √

the competing methods for both neighborhood and hub graphs. This is consistent with our theory.
At ﬁrst sight  it might be surprising that for data from low-dimensional neighborhood graphs  BIC
and AIC even outperform the oracle procedure! This is due to the fact that both BIC and AIC
graphs are estimated using all the n = 800 data points  while the oracle graph is estimated using
n⌋ = 282. Direct usage of the full sample
is an advantage of model selection methods that take the general form of BIC and AIC. In high
dimensions  however  we see that even with this advantage  StARS clearly outperforms BIC and
AIC. The estimated graphs for different methods in the setting n = 400  p = 100 are provided in
Figures 1 and 2  from which we see that the StARS graph is almost as good as the oracle  while the
K-CV  BIC  and AIC graphs are overly too dense.

(a) True graph

(b) Oracle graph

(c) StARS graph

(d) K-CV graph

(e) BIC graph

(f) AIC graph

Figure 1: Comparison of different methods on the data from the neighborhood graphs (n = 400; p = 100).

5.2 Microarray Data
We apply StARS to a dataset based on Affymetrix GeneChip microarrays for the gene expression
levels from immortalized B cells of human subjects. The sample size is n = 294. The expression
levels for each array are pre-processed by log-transformation and standardization as in [13]. Using
a sub-pathway subset of 324 correlated genes  we study the estimated graphs obtained from each
method under investigation. The StARS and BIC graphs are provided in Figure 3. We see that
the StARS graph is remarkably simple and informative  exhibiting some cliques and hub genes. In
contrast  the BIC graph is very dense and possible useful association information is buried in the
large number of estimated edges. The selected graphs using AIC and K-CV are even more dense
than the BIC graph and will be reported elsewhere. A full treatment of the biological implication of
these two graphs validated by enrichment analysis will be provided in the full version of this paper.

6 Conclusions
The problem of estimating structure in high dimensions is very challenging. Casting the problem
in the context of a regularized optimization has led to some success  but the choice of the regu-
larization parameter is critical. We present a new method  StARS  for choosing this parameter in
high dimensional inference for undirected graphs. Like Meinshausen and B¨uhlmann’s stability se-
lection approach [12]  our method makes use of subsampling  but it differs substantially from their

7

(a) True graph

(b) Oracle graph

(c) StARS graph

(d) K-CV graph

(e) BIC graph

(f) AIC graph

Figure 2: Comparison of different methods on the data from the hub graphs (n = 400; p = 100).

(a) StARS graph

(b) BIC graph

Figure 3: Microarray data example. The StARS graph is more informative graph than the BIC graph.

approach in both implementation and goals. For graphical models  we choose the regularization pa-
rameter directly based on the edge stability. Under mild conditions  StARS is partially sparsistent.
However  even without these conditions  StARS has a simple interpretation: we use the least amount
of regularization that simultaneously makes a graph sparse and replicable under random sampling.
Empirically  we show that StARS works signiﬁcantly better than existing techniques on both syn-
thetic and microarray datasets. Although we focus here on graphical models  our new method is
generally applicable to many problems that involve estimating structure  including regression  clas-
siﬁcation  density estimation  clustering  and dimensionality reduction.

8

References
[1] Hirotsugu Akaike. Information theory and an extension of the maximum likelihood principle.

Second International Symposium on Information Theory  (2):267–281  1973.

[2] Onureena Banerjee  Laurent El Ghaoui  and Alexandre d’Aspremont. Model selection through
sparse maximum likelihood estimation. Journal of Machine Learning Research  9:485–516 
March 2008.

[3] Shai Ben-david  Ulrike Von Luxburg  and David Pal. A sober look at clustering stability. In

Proceedings of the Conference of Learning Theory  pages 5–19. Springer  2006.

[4] Arthur P. Dempster. Covariance selection. Biometrics  28:157–175  1972.
[5] David Edwards. Introduction to graphical modelling. Springer-Verlag Inc  1995.
[6] Bradley Efron. The jackknife  the bootstrap and other resampling plans. SIAM [Society for

Industrial and Applied Mathematics]  1982.

[7] Jerome H. Friedman  Trevor Hastie  and Robert Tibshirani. Sparse inverse covariance estima-

tion with the graphical lasso. Biostatistics  9(3):432–441  2007.

[8] Tilman Lange  Volker Roth  Mikio L. Braun  and Joachim M. Buhmann. Stability-based vali-

dation of clustering solutions. Neural Computation  16(6):1299–1323  2004.

[9] Steffen L. Lauritzen. Graphical Models. Oxford University Press  1996.
[10] Han Liu  John Lafferty  and J. Wainwright. The nonparanormal: Semiparametric estimation of
high dimensional undirected graphs. Journal of Machine Learning Research  10:2295–2328 
2009.

[11] Nicolai Meinshausen and Peter B¨uhlmann. High dimensional graphs and variable selection

with the Lasso. The Annals of Statistics  34:1436–1462  2006.

[12] Nicolai Meinshausen and Peter B¨uhlmann. Stability selection. To Appear in Journal of the

Royal Statistical Society  Series B  Methodological  2010.

[13] Renuka R. Nayak  Michael Kearns  Richard S. Spielman  and Vivian G. Cheung. Coexpression
network based on natural variation in human gene expression reveals gene interactions and
functions. Genome Research  19(11):1953–1962  November 2009.

[14] Jie Peng  Pei Wang  Nengfeng Zhou  and Ji Zhu. Partial correlation estimation by joint sparse
regression models. Journal of the American Statistical Association  104(486):735–746  2009.
[15] Dimitris N. Politis  Joseph P. Romano  and Michael Wolf. Subsampling (Springer Series in

Statistics). Springer  1 edition  August 1999.

[16] Pradeep Ravikumar  Martin Wainwright  Garvesh Raskutti  and Bin Yu. Model selection in
In Ad-

Gaussian graphical models: High-dimensional consistency of ℓ1-regularized MLE.
vances in Neural Information Processing Systems 22  Cambridge  MA  2009. MIT Press.

[17] Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. arXiv/0907.3454 

2009.

[18] Adam J. Rothman  Peter J. Bickel  Elizaveta Levina  and Ji Zhu. Sparse permutation invariant

covariance estimation. Electronic Journal of Statistics  2:494–515  2008.

[19] Gideon Schwarz. Estimating the dimension of a model. The Annals of Statistics  6:461–464 

1978.

[20] Robert J. Serﬂing. Approximation theorems of mathematical statistics. John Wiley and Sons 

1980.

[21] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society  Series B  Methodological  58:267–288  1996.

[22] Larry Wasserman and Kathryn Roeder. High dimensional variable selection. Annals of statis-

tics  37(5A):2178–2201  January 2009.

[23] Joe Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley  1990.
[24] Ming Yuan and Yi Lin. Model selection and estimation in the Gaussian graphical model.

Biometrika  94(1):19–35  2007.

9

,Daniel Russo
Benjamin Van Roy