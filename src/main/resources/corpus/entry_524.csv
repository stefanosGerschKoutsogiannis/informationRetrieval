2018,An Off-policy Policy Gradient Theorem Using Emphatic Weightings,Policy gradient methods are widely used for control in reinforcement learning  particularly for the continuous action setting. There have been a host of theoretically sound algorithms proposed for the on-policy setting  due to the existence of the policy gradient theorem which provides a simplified form for the gradient. In off-policy learning  however  where the behaviour policy is not necessarily attempting to learn and follow the optimal policy for the given task  the existence of such a theorem has been elusive. In this work  we solve this open problem by providing the first off-policy policy gradient theorem. The key to the derivation is the use of emphatic weightings. We develop a new actor-critic algorithm—called Actor Critic with Emphatic weightings (ACE)—that approximates the simplified gradients provided by the theorem. We demonstrate in a simple counterexample that previous off-policy policy gradient methods—particularly OffPAC and DPG—converge to the wrong solution whereas ACE finds the optimal solution.,An Off-policy Policy Gradient Theorem Using

Emphatic Weightings

Ehsan Imani⇤  Eric Graves⇤  Martha White

Reinforcement Learning and Artiﬁcial Intelligence Laboratory

{imani graves whitem}@ualberta.ca

Department of Computing Science

University of Alberta

Abstract

Policy gradient methods are widely used for control in reinforcement learning 
particularly for the continuous action setting. There have been a host of theoret-
ically sound algorithms proposed for the on-policy setting  due to the existence
of the policy gradient theorem which provides a simpliﬁed form for the gradient.
In off-policy learning  however  where the behaviour policy is not necessarily at-
tempting to learn and follow the optimal policy for the given task  the existence
of such a theorem has been elusive. In this work  we solve this open problem by
providing the ﬁrst off-policy policy gradient theorem. The key to the derivation is
the use of emphatic weightings. We develop a new actor-critic algorithm—called
Actor Critic with Emphatic weightings (ACE)—that approximates the simpliﬁed
gradients provided by the theorem. We demonstrate in a simple counterexam-
ple that previous off-policy policy gradient methods—particularly OffPAC and
DPG—converge to the wrong solution whereas ACE ﬁnds the optimal solution.

1

Introduction

Off-policy learning holds great promise for learning in an online setting  where an agent generates
a single stream of interaction with its environment. On-policy methods are limited to learning about
the agent’s current policy. Conversely  in off-policy learning  the agent can learn about many poli-
cies that are different from the policy being executed. Methods capable of off-policy learning have
several important advantages over on-policy methods. Most importantly  off-policy methods allow
an agent to learn about many different policies at once  forming the basis for a predictive under-
standing of an agent’s environment [Sutton et al.  2011  White  2015] and enabling the learning of
options [Sutton et al.  1999  Precup  2000]. With options  an agent can determine optimal (short)
behaviours  starting from its current state. Off-policy methods can also learn from data generated
by older versions of a policy  known as experience replay  a critical factor in the recent success of
deep reinforcement learning [Lin  1992  Mnih et al.  2015  Schaul et al.  2015]. They also enable
learning from other forms of suboptimal data  including data generated by human demonstration 
non-learning controllers  and even random behaviour. Off-policy methods also enable learning about
the optimal policy while executing an exploratory policy [Watkins and Dayan  1992]  thereby ad-
dressing the exploration-exploitation tradeoff.
Policy gradient methods are a general class of algorithms for learning optimal policies  for both the
on and off-policy setting. In policy gradient methods  a parameterized policy is improved using
gradient ascent [Williams  1992]  with seminal work in actor-critic algorithms [Witten  1977  Barto
et al.  1983] and many techniques since proposed to reduce variance of the estimates of this gradient
[Konda and Tsitsiklis  2000  Weaver and Tao  2001  Greensmith et al.  2004  Peters et al.  2005 

⇤These authors contributed equally.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Bhatnagar et al.  2008  2009  Grondman et al.  2012  Gu et al.  2016]. These algorithms rely on
a fundamental theoretical result: the policy gradient theorem. This theorem [Sutton et al.  2000 
Marbach and Tsitsiklis  2001] simpliﬁes estimation of the gradient  which would otherwise require
difﬁcult-to-estimate gradients with respect to the stationary distribution of the policy and potentially
of the action-values if they are used.
Off-policy policy gradient methods have also been developed  particularly in recent years where
the need for data efﬁciency and decorrelated samples in deep reinforcement learning require the
use of experience replay and so off-policy learning. This work began with OffPAC [Degris et al. 
2012a]  where an off-policy policy gradient theorem was provided that parallels the on-policy policy
gradient theorem  but only for tabular policy representations.2 This motivated further development 
including a recent actor-critic algorithm proven to converge when the critic uses linear function
approximation [Maei  2018]  as well as several methods using the approximate off-policy gradient
such as Deterministic Policy Gradient (DPG) [Silver et al.  2014  Lillicrap et al.  2015]  ACER
[Wang et al.  2016]  and Interpolated Policy Gradient (IPG) [Gu et al.  2017]. However  it remains an
open question whether the foundational theorem that underlies these algorithms can be generalized
beyond tabular representations.
In this work  we provide an off-policy policy gradient theorem  for general policy parametrization.
The key insight is that the gradient can be simpliﬁed if the gradient in each state is weighted with an
emphatic weighting. We use previous methods for incrementally estimating these emphatic weight-
ings [Yu  2015  Sutton et al.  2016] to design a new off-policy actor-critic algorithm  called Actor-
Critic with Emphatic weightings (ACE). We show in a simple three state counterexample  with two
states aliased  that solutions are suboptimal with the (semi)-gradients used in previous off-policy
algorithms—such as OffPAC and DPG. We demonstrate both the theorem and the counterexample
under stochastic and deterministic policies  and that ACE converges to the optimal solution.

2 Problem Formulation

We consider a Markov decision process (S  A  P  r)  where S denotes the ﬁnite set of states  A
denotes the ﬁnite set of actions  P : S ⇥ A ⇥ S ! [0 1) denotes the one-step state transition
dynamics  and r : S ⇥ A ⇥ S ! R denotes the transition-based reward function. At each timestep
t = 1  2  . . .  the agent selects an action At according to its behaviour policy µ  where µ : S ⇥ A !
[0  1]. The environment responds by transitioning into a new state St+1 according to P  and emits a
scalar reward Rt+1 such that E [Rt+1|St  At  St+1] = r(St  At  St+1).
The discounted sum of future rewards given actions are selected according to some target policy ⇡
is called the return  and deﬁned as:

Gt

def= Rt+1 + t+1Rt+2 + t+1t+2Rt+3 + . . .
= Rt+1 + t+1Gt+1

(1)

We use transition-based discounting  : S ⇥ A ⇥ S ! [0  1]  as it uniﬁes continuing and episodic
tasks [White  2017]. Then the state value function for policy ⇡ and  is deﬁned as:

P(s  a  s0)[r(s  a  s0) + (s  a  s0)v⇡(s0)] 8a 2 A 8s 2 S

(2)

v⇡(s) def= E⇡[Gt|St = s] 8s 2 S

=Xa2A

⇡(s  a)Xs02S

In off-policy control  the agent’s goal is to learn a target policy ⇡ while following the behaviour
policy µ. The target policy ⇡✓ is a differentiable function of a weight vector ✓ 2 Rd. The goal is to
learn ✓ to maximize the following objective function:

Jµ(✓) def=Xs2S

dµ(s)i(s)v⇡✓ (s)

(3)

where i : S ! [0 1) is an interest function  dµ(s) def= limt!1 P(St = s|s0  µ) is the limiting
distribution of states under µ (which we assume exists)  and P(St = s|s0  µ) is the probability that
2See B. Errata in Degris et al. [2012b] for the clariﬁcation that the theorem only applies to tabular policy

representations.

2

St = s when starting in state s0 and executing µ. The interest function—introduced by Sutton et al.
[2016]—provides more ﬂexibility in weighting states in the objective. If i(s) = 1 for all states  the
objective reduces to the standard off-policy objective. Otherwise  it naturally encompasses other
settings  such as the start state formulation by setting i(s) = 0 for all states but the start state(s).
Because it adds no complications to the derivations  we opt for this more generalized objective.

3 Off-Policy Policy Gradient Theorem using Emphatic Weightings

The policy gradient theorem with function approximation has only been derived for the on-policy
setting thus far  for stochastic policies [Sutton et al.  2000  Theorem 1] and deterministic policies
[Silver et al.  2014]. The policy gradient theorem for the off-policy case has only been established
for the setting where the policy is tabular [Degris et al.  2012b  Theorem 2].3 In this section  we
show that the policy gradient theorem does hold in the off-policy setting  when using function ap-
proximation for the policy  as long as we use emphatic weightings. These results parallel those in
off-policy policy evaluation  for learning the value function  where issues of convergence for tem-
poral difference methods are ameliorated by using an emphatic weighting [Sutton et al.  2016].
Theorem 1 (Off-policy Policy Gradient Theorem).

q⇡(s  a)

(4)

@Jµ(✓)

@✓

@⇡(s  a; ✓)

@✓

m(s)Xa

=Xs
m> def= i>(I  P⇡  )1

where m : S ! [0 1) is the emphatic weighting  in vector form deﬁned as

(5)
where the vector i 2 R|S| has entries i(s) def= dµ(s)i(s) and P⇡  2 R|S|⇥|S| is the matrix with
entries P⇡  (s  s0) def=Pa ⇡(s  a; ✓)P(s  a  s0)(s  a  s0)
@Ps i(s)v⇡(s)

Proof. First notice that

@Jµ(✓)

@v⇡(s)

i(s)

@✓

@✓

@✓

=

Therefore  to compute the gradient of Jµ  we need to compute the gradient of the value function
with respect to the policy parameters. A recursive form of the gradient of the value function can be
derived  as we show below. Before starting  for simplicity of notation we will use

=Xs

g(s) =Xa

@⇡(s  a; ✓)

@✓

q⇡(s  a)

where g : S ! Rd. Now let us compute the gradient of the value function.

@v⇡(s)

@✓

⇡(s  a; ✓)q⇡(s  a)

@⇡(s  a; ✓)

@

=

@✓

@✓Xa
=Xa
= g(s) +Xa
= g(s) +Xa

⇡(s  a; ✓)

⇡(s  a; ✓)Xs0

q⇡(s  a) +Xa

⇡(s  a; ✓)

@q⇡(s  a)

@✓

@Ps0 P(s  a  s0)(r(s  a  s0) + (s  a  s0)v⇡(s0))

@✓

(6)

P(s  a  s0)(s  a  s0)

@v⇡(s0)

@✓

We can simplify this more easily using vector form. Let ˙v⇡ 2 R|S|⇥d be the matrix of gradients
(with respect to the policy parameters ✓) of v⇡ for each state s  and G 2 R|S|⇥d the matrix where
each row corresponding to state s is the vector g(s). Then
(7)

˙v⇡ = G + P⇡  ˙v⇡

=) ˙v⇡ = (I  P⇡  )1G

3Note that the statement in the paper is stronger  but in an errata published by the authors  they highlight an

error in the proof. Consequently  the result is only correct for the tabular setting.

3

Therefore  we obtain

i(s)

@v⇡(s)

@✓

Xs

= i> ˙v⇡ = i>(I  P⇡  )1G
= m>G

=Xs

m(s)Xa

@⇡(s  a; ✓)

@✓

q⇡(s  a)

We can prove a similar result for the deterministic policy gradient objective  for a deterministic
policy  ⇡ : S ! A. The objective remains the same  but the space of possible policies is constrained 
resulting in a slightly different gradient.
Theorem 2 (Deterministic Off-policy Policy Gradient Theorem).

@⇡(s; ✓)

@q⇡(s  a)

@✓

@a

ds

(8)

where m : S ! [0 1) is the emphatic weighting for a deterministic policy  which is the solution to
the recursive equation

P(s  ⇡(s; ✓)  s0)(s  ⇡(s; ✓)  s0)m(s) ds

(9)

m(s)

@✓

@Jµ(✓)

=ZS
m(s0) def= dµ(s0)i(s0) +ZS

a=⇡(s;✓)

The proof is presented in Appendix A.

4 Actor-Critic with Emphatic Weightings

In this section  we develop an incremental actor-critic algorithm with emphatic weightings  that uses
the above off-policy policy gradient theorem. To perform a gradient ascent update on the policy
parameters  the goal is to obtain a sample of the gradient
@⇡(s  a; ✓)

(10)

Xs

m(s)Xa

q⇡(s  a).

@✓

Comparing this expression with the approximate gradient used by OffPAC and subsequent methods
(which we refer to as semi-gradient methods) reveals that the only difference is in the weighting of
states:

Xs

dµ(s)Xa

@⇡(s  a; ✓)

@✓

q⇡(s  a).

(11)

@✓

@⇡(s a;✓)

sample ofPa

Therefore  we can use standard solutions developed for other actor-critic algorithms to obtain a
q⇡(s  a). Explicit details for our off-policy setting are given in Appendix B.

The key difﬁculty is then in estimating m(s) to reweight this gradient  which we address below.
The policy gradient theorem assumes access to the true value function  and provides a Bellman
equation that deﬁnes the optimal ﬁxed point. However  approximation errors can occur in practice 
both in estimating the value function (the critic) and the emphatic weighting. For the critic  we
can take advantage of numerous algorithms that improve estimation of value functions  including
through the use of -returns to mitigate bias  with  = 1 corresponding to using unbiased samples
of returns [Sutton  1988]. For the emphatic weighting  we introduce a similar parameter a 2 [0  1] 
that introduces bias but could help reduce variability in the weightings
m>a = i>(I  P⇡  )1(I  (1  a)P⇡  ).
(12)
For a = 1  we get ma = m and so get an unbiased emphatic weighting.4 For a = 0  the
emphatic weighting is simply i  and the gradient with this weighting reduces to the regular off-
policy actor critic update [Degris et al.  2012b]. For a = 0  therefore  we obtain a biased gradient
4Note that the original emphatic weightings [Sutton et al.  2016] use  = 1  a. This is because their
emphatic weightings are designed to balance bias introduced from using  for estimating value functions: larger
 means the emphatic weighting plays less of a role. For this setting  we want larger a to correspond to the
full emphatic weighting (the unbiased emphatic weighting)  and smaller a to correspond to a more biased
estimate  to better match the typical meaning of such trace parameters.

4

estimate  but the emphatic weightings themselves are easy to estimate—they are myopic estimates
of interest—which could signiﬁcantly reduce variance when estimating the gradient. Selecting a
between 0 and 1 could provide a reasonable balance  obtaining a nearly unbiased gradient to enable
convergence to a valid stationary point but potentially reducing some variability when estimating
the emphatic weighting.
Now we can draw on previous work estimating emphatic weightings incrementally to obtain an em-
phatically weighted policy gradient. Assume access to an estimate of the gradient @⇡(s a;✓)
q⇡(s  a) 
such as the commonly-used estimate: ⇢ttr✓ ln ⇡(s  a; ✓)  where ⇢t is the importance sampling ra-
tio (described further in Appendix B)  and t is the temporal difference error  which—as an estimate
of the advantage function a⇡(s  a) = q⇡(s  a)  v⇡(s)—implicitly includes a state value baseline.
Because this is an off-policy setting  the states s from which we would sample this gradient are
weighted according to dµ. We need to adjust this weighting from dµ(s) to m(s). We can do so by
using an online algorithm previously derived to obtain a sample Mt of the emphatic weighting

@✓

Mt

.F t

def= t⇢t1Ft1 + i(St)

def= (1  a)i(St) + aFt

(13)
for F0 = 0. The actor update is then multiplied by Mt to give the emphatically-weighted actor up-
date: ⇢tMttr✓ ln ⇡(s  a; ✓). Previous work by Thomas [2014] to remove bias in natural actor-critic
algorithms is of interest here  as it suggests weighting actor updates by an accumulating product of
discount factors  which can be thought of as an on-policy precursor to emphatic weightings. We
prove that our update is an unbiased estimate of the gradient for a ﬁxed policy in Proposition 1. We
provide the complete Actor-Critic with Emphatic weightings (ACE) algorithm  with pseudo-code
and additional algorithm details  in Appendix B.
Proposition 1. For a ﬁxed policy ⇡  and with the conditions on the MDP from [Yu  2015] 

Eµ[⇢tMttr✓ ln ⇡(St  At; ✓)] =Xs

m(s)Xa

@⇡(s  a; ✓)

@✓

q⇡(s  a)

Proof. Emphatic weightings were previously shown to provide an unbiased estimate for value func-
tions with Emphatic TD. We use the emphatic weighting differently  but can rely on the proof
from [Sutton et al.  2016] to ensure that (a) dµ(s)Eµ[Mt|St = s] = m(s) and the fact that (b)
Eµ[Mt|St = s] = Eµ[Mt|St = s  At  St+1]. Using these equalities  we obtain
Eµ[⇢tMttr✓ ln ⇡(s  a; ✓)]
=Xs
=Xs
=Xs
=Xs
=Xs

dµ(s)Eµ[⇢tMttr✓ ln ⇡(St  At; ✓)|St = s]
dµ(s)EµhEµ[⇢tMttr✓ ln ⇡(St  At; ✓)|St = s  At  St+1]i
dµ(s)EµhEµ[Mt|St = s  At  St+1] Eµ[⇢ttr✓ ln ⇡(St  At; ✓)|St = s  At  St+1]i
dµ(s)Eµ[Mt|St = s]EµhEµ[⇢ttr✓ ln ⇡(St  At; ✓)|St = s  At  St+1]i
m(s)Xa

. law of total expectation

. using (a).

@⇡(s  a; ✓)

. using (b)

q⇡(s  a)

@✓

5 Experiments

We empirically investigate the utility of using the true off-policy gradient  as opposed to the previous
approximation used by OffPAC; the impact of the choice of a; and the efﬁcacy of estimating
emphatic weightings in ACE. We present a toy problem to highlight the fact that OffPAC—which
uses an approximate semi-gradient—can converge to suboptimal solutions  even in ideal conditions 
whereas ACE—with the true gradient—converges to the optimal solution. We conduct several other
experiments on the same toy problem  to elucidate properties of ACE.

5

5.1 The Drawback of Semi-Gradient Updates

We design a world with aliased states to highlight the problem with semi-gradient updates. The toy
problem  depicted in Figure 1a  has three states  where S0 is a start state with feature vector [1  0] 
and S1 and S2 are aliased  both with feature vector [0  1]. This aliased representation forces the
actor to take a similar action in S1 and S2. The behaviour policy takes actions A0 and A1 with
probabilities 0.25 and 0.75 in all non-terminal states  so that S0  S1  and S2 will have probabilities
0.5  0.125  and 0.375 under dµ. Under this aliasing  the optimal action in S1 and S2 is A0  for the
off-policy objective Jµ. The target policy is initialized to take A0 and A1 with probabilities 0.9 and
0.1 in all states  which is near optimal.
We ﬁrst compared an idealized semi-gradient actor (a = 0) and gradient actor (a = 1)  with
exact value function (critic) estimates. Figures 1b and 1c clearly indicate that the semi-gradient
update—which corresponds to an idealized version of the OffPAC update—converges to a subopti-
mal solution. This occurs even if it is initialized close to the optimal solution  which highlights that
the true solution is not even a stationary point for the semi-gradient objective. On the other hand 
the gradient solution—corresponding to ACE—increases the objective until converging to optimal.
We show below  in Section 5.3 and Figure 5  that this is similarly a problem in the continuous action
setting with DPG.
The problem with the semi-gradient updates is made clear from the fact that it corresponds to the
a = 0 solution  which uses the weighting dµ instead of m. In an expected semi-gradient update 
each state tries to increase the probability of the action with the highest action-value. There will
be a conﬂict between the aliased states S1 and S2  since their highest-valued actions differ. If the
states are weighted by dµ in the expected update  S1 will appear insigniﬁcant to the actor  and the
update will increase the probability of A1 in the aliased states. (The ratio between q⇡(S1  A0) and
q⇡(S2  A1) is not enough to counterbalance this weighting.) However  S1 has an importance that a
semi-gradient update overlooks. Taking a suboptimal action at S1 will also reduce q(S0  A0) and 
after multiple updates  the actor gradually prefers to take A1 in S0. Eventually  the target policy
will be to take A1 at all states  which has a lower objective function than the initial target policy.
This experiment suggests why the weight of a state should depend not only on its own share of dµ 
but also on its predecessors  and the behaviour policy’s state distribution is not the proper deciding
factor in the competition between S1 and S2.

(a) Counterexample

(b) Learning curves

(c) Action probability

Figure 1: (a) A counterexample that identiﬁes suboptimal behaviour when using semi-gradient up-
dates. The semi-gradients converge for the tabular setting [Degris et al.  2012b]  but not necessarily
under function approximation—such as with the state aliasing in this MDP. S0 is the start state and
the terminal state is denoted by T3. S1 and S2 are aliased to the actor. The interest i(s) is set to one
for all states. (b) Learning curves comparing semi-gradient updates and gradient updates  averaged
over 30 runs with negligible standard error bars. The actor has a softmax output on a linear trans-
formation of features and is trained with a step-size of 0.1 (though results were similar across all the
stepsizes tested). The dashed line shows the highest attainable objective function under the aliased
representation. (c) The probability of taking A0 at the aliased states  where taking A0 is optimal
under the aliased representation.

6

(a) Stepsize sensitivity

(b) Learning curves

(c) Action probability

Figure 2: Performance with different values of a in the 3-state MDP  averaged over 30 runs. (a)
ACE performs well  for a range of stepsizes and even a that gets quite small. (b) For a = 0  which
corresponds to OffPAC  the algorithm decreases performance  to get to the suboptimal ﬁxed point.
Even with as low a value as a = 0.25  ACE improves the value from the starting point  but does
converge to a worse solution than a  0.5. The learning curves correspond to each algorithm’s
best step-size. (c) The optimal behaviour is to take A0 with probability 1  in the aliased states.
ACE(0)—corresponding to OffPAC—quickly converges to the suboptimal solution of preferring the
optimal action for S2 instead of S1. Even with a just a bit higher than 0  convergence is to a more
reasonable solution  preferring the optimal action the majority of the time.

5.2 The Impact of the Trade-Off Parameter
The parameter a in (12) has the potential to trade off bias and variance. For a = 0  the bias can
be signiﬁcant  as shown in the previous section. A natural question is how the bias changes as a
decreases from 1 to 0. There is unlikely to be signiﬁcant variance reduction—it is a low variance
domain—but we can nonetheless gain some insight into bias.
We repeated the experiment in 5.1  with a chosen from {0  0.25  0.5  0.75  1} and the step-size
chosen from {0.01  0.02  0.05  0.1  0.2  0.5  1}. To highlight the rate of learning  the actor parame-
ters are now initialized to zero. Figure 2 summarizes the results. As shown in Figure 2a  with a
close to one  a small and carefully-tuned step-size is needed to make the most of the method. As
a decreases  the actor is able to learn with higher step-sizes and increases the objective function
faster. The quality of the ﬁnal solution  however  deteriorates with small values of a since the actor
follows biased gradients. Even for surprisingly small a = 0.5 the actor converged to optimal  and
even a = 0.25 produced a much more reasonable solution than a = 0.
We ran a similar experiment  this time using value estimates from a critic trained with Gradient
TD  called GTD() [Maei  2011] to examine whether the impact of a values with the actual (non-
idealized) ACE algorithm persists in an actor-critic architecture. The step-size ↵v was chosen from
{105  104  103  102  101  100}  ↵w was chosen from {1010  108  106  104  102}  and
{0  0.5  1.0} was the set of candidate values of  for the critic. The results in Figure 3 show that 
as before  even relatively low a values can still get close to the optimal solution. However  semi-
gradient updates  corresponding to a = 0  still ﬁnd a suboptimal policy.

(a) Stepsize sensitivity

(b) Learning curves

(c) Action probability

Figure 3: Performance of ACE with a GTD() critic and different values of a in the 3-state MDP.
The results are averaged over 10 runs. The outcomes are similar to Figure 2  though noisier due to
learning the critic rather than using the true critic.

7

5.3 Challenges in Estimating the Emphatic Weightings
We have been using an online algorithm to estimate the emphatic weightings. There can be different
sources of inaccuracy in these approximations. First  the estimate depends on importance sampling
ratios of previous actions in the trajectory. This can result in high variance if the behaviour policy
and the target policy are not close. Secondly  derivation of the online algorithm assumes a ﬁxed
target policy [Sutton et al.  2016]  while the actor is updated at every time step. Therefore  the
approximation is less reliable in long trajectories  as it is partly decided by older target policies in
the beginning of the trajectory. We designed experiments to study the efﬁcacy of these estimates in
an aliased task with more states.
The ﬁrst environment  shown in Figure 6 in the appendix  is an extended version of the previous
MDP with two long chains before the aliased states. The addition of new states makes the trajecto-
ries considerably longer  allowing errors to build up in emphatic weighting estimates. The actor is
initialized with zero weights and uses true state values in its updates. The behaviour policy takes A0
with probability 0.25 and A1 with probability 0.75 in all non-terminal states. The actor’s step-size
is picked from {5 · 105  104  2 · 104  5 · 104  103  2 · 103  5 · 103  102}. We also trained
an actor  called True-ACE  that uses true emphatic weightings for the current target policy and be-
haviour policy  computed at each timestep. The performance of True-ACE is included here for the
sake of comparison  and computing the exact emphatic weightings is not generally possible in an
unknown environment. The results in Figure 4 show that  even though performance improves as a
is increased  there is a gap between ACE with a = 1 and True-ACE. This shows the inaccuracies
pointed out above indeed disturb the updates in long trajectories.

(a) Stepsize sensitivity

(b) Learning curves

(c) Action probability

Figure 4: Performance of ACE with different values of a and True-ACE on the 11-state MDP. The
results are averaged over 10 runs. Unlike Figure 2  the methods now have more difﬁculty getting
near the optimal solution  though ACE with larger a does still clearly get a signiﬁcantly better
solution that a = 0.

The second environment is similar to Figure 1a  but with one continuous unbounded action. Taking
action with value a at S0 will result in a transition to S1 with probability 1  (a) and a transition
to S2 with probability (a)  where  denotes the logistic sigmoid function. For all actions from
S0  the reward is zero. From S1 and S2  the agent can only transition to the terminal state  with
reward 2(a) and (a) respectively. The behaviour policy takes actions drawn from a Gaussian
distribution with mean 1.0 and variance 1.0.
Because the environment has continuous actions  we can include both stochastic and deterministic
policies  and so can include DPG in the comparison. DPG is built on the semi-gradient  like OffPAC.
We compare to DPG with Emphatic weightings (DPGE)  with the true emphatic weightings rather
than estimated ones. We compare to True-DPGE to avoiding confounding factors of estimating
the emphatic weighting  and focus the investigation on if DPG converges to a suboptimal solution.
Estimation of the emphatic weightings for a deterministic target policy is left for future work. The
stochastic actor in ACE has a linear output unit and a softplus output unit to represent the mean and
the standard deviation of a Gaussian distribution. All actors are initialized with zero weights.
Figure 5 summarizes the results. The ﬁrst observation is that DPG demonstrates suboptimal be-
haviour similar to OffPAC. As training goes on  DPG prefers to take positive actions in all states 
because S2 is updated more often. This problem goes away in True-DPGE. The emphatic weightings
emphasize updates in S1 and  thus  the actor gradually prefers negative actions and surpasses DPG
in performance. Similarly  True-ACE learns to take negative actions but  being a stochastic policy  it

8

cannot achieve True-DPGE’s performance on this domain. ACE with different a values  however 
cannot outperform DPG  and this result suggests that an alternative to importance sampling ratios is
needed to extend ACE to continuous actions.

(a) Stepsize sensitivity

(b) Learning curves

(c) Mean action

Figure 5: Performance of ACE with different values of a  True-ACE  DPG  and True-DPGE on the
continuous action MDP. The results are averaged over 30 runs. For continuous actions  the methods
have even more difﬁculty getting to the optimal solutions  given by True-DPGE and True-ACE 
though the action selection graphs suggest that ACE for higher a is staying nearer the optimal
action selection than ACE(0) and DPG.

6 Conclusions and Future Work

In this paper we proved the off-policy policy gradient theorem  using emphatic weightings. The
result is generally applicable to any differentiable policy parameterization. Using this theorem  we
derived an off-policy actor-critic algorithm that follows the gradient of the objective function  as
opposed to previous method like OffPAC and DPG that followed an approximate semi-gradient. We
designed a simple MDP to highlight issues with existing methods—namely OffPAC and DPG—
particularly highlighting that the stationary points of these semi-gradient methods for this problem
do not include the optimal solution. Our algorithm  called Actor-Critic with Emphatic Weightings 
on the other hand  which follows the gradient  reaches the optimal solution  both for an idealized
setting given the true critic and when learning the critic. We conclude with a result suggesting that
more work needs to be done to effectively estimate emphatic weightings  and that important next
steps for developing Actor-Critic algorithm for the off-policy setting are to improve estimation of
these weightings.

7 Acknowledgements

The authors would like to thank Alberta Innovates for funding the Alberta Machine Intelligence
Institute and by extension this research. We would also like to thank Hamid Maei  Susan Murphy 
and Rich Sutton for their helpful discussions and insightful comments.

References
Andrew G Barto  Richard S Sutton  and Charles W Anderson. Neuronlike adaptive elements that can
solve difﬁcult learning control problems. IEEE transactions on systems  man  and cybernetics 
(5):834–846  1983.

S Bhatnagar  R Sutton  and M Ghavamzadeh. Natural actor-critic algorithms. Automatica  2009.

Shalabh Bhatnagar  Mohammad Ghavamzadeh  Mark Lee  and Richard S Sutton. Incremental natu-
ral actor-critic algorithms. In Advances in neural information processing systems  pages 105–112 
2008.

Thomas Degris  Patrick M Pilarski  and Richard S Sutton. Model-free reinforcement learning with
continuous action in practice. In American Control Conference (ACC)  2012  pages 2177–2182.
IEEE  2012a.

9

Thomas Degris  Martha White  and Richard S Sutton. Off-policy actor-critic.

Conference on Machine Learning  2012b.

In International

Evan Greensmith  Peter L Bartlett  and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research  5(Nov):1471–1530 
2004.

Ivo Grondman  Lucian Busoniu  Gabriel AD Lopes  and Robert Babuska. A survey of actor-critic
reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems 
Man  and Cybernetics  Part C (Applications and Reviews)  42(6):1291–1307  2012.

Shixiang Gu  Timothy Lillicrap  Zoubin Ghahramani  Richard E Turner  and Sergey Levine. Q-prop:
Sample-efﬁcient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247  2016.

Shixiang Gu  Tim Lillicrap  Richard E Turner  Zoubin Ghahramani  Bernhard Schölkopf  and Sergey
Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for
deep reinforcement learning. In Advances in Neural Information Processing Systems  pages 3849–
3858  2017.

Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information

Processing Systems  pages 1008–1014  2000.

Timothy P Lillicrap  Jonathan J Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval Tassa 
David Silver  and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971  2015.

Long-Ji Lin. Self-improving reactive agents based on reinforcement learning  planning and teaching.

Machine learning  8(3-4):293–321  1992.

H Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis  University of Alberta 

2011.

Hamid Reza Maei. Convergent actor-critic algorithms under off-policy training and function ap-

proximation. arXiv preprint arXiv:1802.07842  2018.

Peter Marbach and John N Tsitsiklis. Simulation-based optimization of markov reward processes.

IEEE Transactions on Automatic Control  46(2):191–209  2001.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G Belle-
mare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al. Human-level
control through deep reinforcement learning. Nature  518(7540):529  2015.

Jan Peters  Sethu Vijayakumar  and Stefan Schaal. Natural actor-critic. In European Conference on

Machine Learning  pages 280–291. Springer  2005.

Doina Precup. Temporal abstraction in reinforcement learning. PhD thesis  PhD thesis  University

of Massachusetts Amherst  2000.

Tom Schaul  John Quan  Ioannis Antonoglou  and David Silver. Prioritized experience replay. arXiv

preprint arXiv:1511.05952  2015.

D Silver  G Lever  N Heess  T Degris  and D Wierstra. Deterministic policy gradient algorithms. In

Proceedings of the 31st . . .   2014.

Richard S Sutton  Doina Precup  and Satinder Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artiﬁcial intelligence  112(1-2):181–
211  1999.

Richard S Sutton  Joseph Modayil  Michael Delp  Thomas Degris  Patrick M Pilarski  Adam White 
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and
Multiagent Systems-Volume 2  pages 761–768. International Foundation for Autonomous Agents
and Multiagent Systems  2011.

10

Richard S Sutton  A R Mahmood  and Martha White. An emphatic approach to the problem of

off-policy temporal-difference learning. The Journal of Machine Learning Research  2016.

R.S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning  1988.
RS Sutton  D McAllester  S Singh  and Y Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural Information Processing Systems 
2000.

Philip Thomas. Bias in natural actor-critic algorithms.

Learning  pages 441–448  2014.

In International Conference on Machine

Ziyu Wang  Victor Bapst  Nicolas Heess  Volodymyr Mnih  Rémi Munos  Koray Kavukcuoglu  and

Nando de Freitas. Sample Efﬁcient Actor-Critic with Experience Replay. arXiv.org  2016.

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning  8(3-4):279–292  1992.
Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning.
In Proceedings of the Seventeenth conference on Uncertainty in artiﬁcial intelligence  pages 538–
545. Morgan Kaufmann Publishers Inc.  2001.

Adam White. Developing a predictive approach to knowledge. PhD thesis  PhD thesis  University

of Alberta  2015.

Martha White. Unifying task speciﬁcation in reinforcement learning. In International Conference

on Machine Learning  pages 3742–3750  2017.

R Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learn-

ing. Machine Learning  1992.

Ian H Witten. An adaptive optimal controller for discrete-time markov environments. Information

and control  34(4):286–295  1977.

Huizhen Yu. On convergence of emphatic temporal-difference learning. In Annual Conference on

Learning Theory  2015.

11

,Ehsan Imani
Eric Graves
Martha White
Mark Bun
Thomas Steinke