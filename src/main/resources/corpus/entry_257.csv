2018,A convex program for bilinear inversion of sparse vectors,We consider the bilinear inverse problem of recovering two vectors   x in R^L and w in R^L  from their entrywise product. We consider the case where x and w have known signs and are sparse with respect to known dictionaries of size K and N  respectively.  Here   K and N may be larger than  smaller than  or equal to L.  We introduce L1-BranchHull  which is a convex program posed in the natural parameter space and does not require an approximate solution or initialization in order to be stated or solved. We study the case where x and w are S1- and S2-sparse with respect to a random dictionary  with the sparse vectors satisfying an effective sparsity condition  and present a recovery guarantee that depends on the number of measurements as L > Omega(S1+S2)(log(K+N))^2. Numerical experiments verify that the scaling constant in the theorem is not too large.  One application of this problem is the sweep distortion removal task in dielectric imaging  where one of the signals is a nonnegative reflectivity  and the other signal lives in a known subspace  for example that given by dominant wavelet coefficients. We also introduce a variants of L1-BranchHull for the purposes of tolerating noise and outliers  and for the purpose of recovering piecewise constant signals.  We provide an ADMM implementation of these variants and show they can extract piecewise constant behavior from real images.,A convex program for bilinear inversion of sparse

vectors

Alireza Aghasi

Georgia State Business School

GSU  GA

aaghasi@gsu.edu

Dept. of Electrical Engineering

Ali Ahmed

ITU  Lahore

ali.ahmed@itu.edu.pk

Dept. of Mathematics and College of Computer and Information Science

Paul Hand

Northeastern University  MA
p.hand@northeastern.edu

Dept. of Computational and Applied Mathematics

Babhru Joshi

Rice University  TX

babhru.joshi@rice.edu

Abstract

We consider the bilinear inverse problem of recovering two vectors  x 2 RL and
w 2 RL  from their entrywise product. We consider the case where x and w have
known signs and are sparse with respect to known dictionaries of size K and N 
respectively. Here  K and N may be larger than  smaller than  or equal to L. We
introduce `1-BranchHull  which is a convex program posed in the natural parameter
space and does not require an approximate solution or initialization in order to
be stated or solved. We study the case where x and w are S1- and S2-sparse
with respect to a random dictionary  with the sparse vectors satisfying an effective
sparsity condition  and present a recovery guarantee that depends on the number of
measurements as L  ⌦(S1 + S2) log2(K + N ). Numerical experiments verify
that the scaling constant in the theorem is not too large. One application of this
problem is the sweep distortion removal task in dielectric imaging  where one of the
signals is a nonnegative reﬂectivity  and the other signal lives in a known subspace 
for example that given by dominant wavelet coefﬁcients. We also introduce a
variants of `1-BranchHull for the purposes of tolerating noise and outliers  and
for the purpose of recovering piecewise constant signals. We provide an ADMM
implementation of these variants and show they can extract piecewise constant
behavior from real images.

1 Introduction

We study the problem of recovering two unknown signals x and w in RL from observations y =
A(w  x)  where A is a bilinear operator. Let B 2 RL⇥K and C 2 RL⇥N such that w = Bh and
x = Cm with khk0  S1 and kmk0  S2. Let the bilinear operator A : RL ⇥ RL ! RL satisfy
(1)
where  denotes entrywise product. The bilinear inverse problem (BIP) we consider is to ﬁnd w and
x from y  B  C and sign (w)  up to the inherent scaling ambiguity.

y = A(w  x) = w  x 

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

BIPs  in general  have many applications in signal processing and machine learning and include
fundamental practical problems like phase retrieval (Fienup [1982]  Candès and Li [2012]  Candès
et al. [2013])  blind deconvolution (Ahmed et al. [2014]  Stockham et al. [1975]  Kundur and
Hatzinakos [1996]  Aghasi et al. [2016a])  non-negative matrix factorization (Hoyer [2004]  Lee and
Seung [2001])  self-calibration (Ling and Strohmer [2015])  blind source separation (D. et al. [2005]) 
dictionary learning (Tosic and Frossard [2011])  etc. These problems are in general challenging and
suffer from identiﬁability issues that make the solution set non-unique and non-convex. A common
identiﬁability issue  also shared by the BIP in (1)  is the scaling ambiguity. In particular  if (w\  x\)
solves a BIP  then so does (cw\  c1x\) for any nonzero c 2 R. In this paper  we resolve this scaling
ambiguity by ﬁnding the point in the solution set closest to the origin with respect to the `1 norm.

w`

0

`

` w

x

y `

=

x`

Convex Hull

m1

h2

h1

(a) Convex relaxation

(b) Geometry of `1-BranchHull

Figure 1: Panel (a) shows the convex hull of the relevant branch of a hyperbola given a measurement
y` and the sign information sign(w`). Panel (b) shows the interaction between the `1-ball in the
objective of (3) with its feasibility set. The feasibility set is ‘pointy’ along a hyperbola  which
allows for signal recovery where the `1 ball touches it. The gray hyperplane segments correspond
to linearizations of the hyperbolic measurements  which is an important component of our recovery
proof.
Another identiﬁability issue of the BIP in (1) is if (w\  x\) solves (1)  then so does (1  w\  x\) 
where 1 is the vector of ones. In prior works like Ahmed et al. [2014]  which studies the blind
deconvolution problem and is a BIP in the Fourier Domain  the identiﬁability issue is resolved by
assuming the signals live in a known subspace. In comparison to Ahmed et al. [2014]  we resolve the
identiﬁability issue with a much weaker structural assumption of sparsity in known bases at the cost
of known signs; justiﬁed in actual applications  especially  in imaging. Natural choices for such bases
include the standard basis  the Discrete Cosine Transform (DCT) basis  and a wavelet basis.
Recent work on sparse rank-1 matrix recovery problem in Lee et al. [2017]  which is motivated by con-
sidering the lifted version of the sparse blind deconvolution problem  provides an exact recovery guar-
antee of the sparse vectors h and m that satisfy a "peakiness" condition  i.e. min{khk1 kmk1} c
for some absolute constant c 2 R. This result holds with high probability for random measurements
if the number of measurement  up to a log factor  satisfy L  ⌦(S1 + S2). For general vectors
without the peakiness condition  the same work shows exact recovery is possible if the number of
measurements  up to a log factor  satisfy L  ⌦(S1S2).
The main contribution of this paper is to introduce an algorithm for the sparse BIP described in (1)
which recovers sparse vectors that satisfy a comparable effective sparsity condition. Precisely  we say
the sparse vectors h\ and m\ have comparable effective sparsity if there exist an ↵ 2 R such that
(2)

.

C  ↵  C for some C 2 R+. Intuitively  the ratios kh\k1
kh\k2

are about
with ↵ satisfying 1
the same if the sparsity levels of h\ and m\ are close and the magnitudes of the nonzero entries of
h\ and m\ are about the same. Under this assumption on the sparse signals  we present a convex
program stated in the natural parameter space  which in the noiseless setting with random B and

and km\k1
km\k2

kh\k1
kh\k2

= ↵km\k1
km\k2

2

C  exactly recovers the sparse vectors with at most S1 + S2 combined nonzero entries with high
probability if the number measurements satisfy L  ⌦(S1 + S2) log2(K + N ).
1.1 Convex program and main results
We introduce a convex program written in the natural parameter space for the bilinear inverse problem
described in (1). Let (h\  m\) 2 RK ⇥ RN with kh\k0  S1 and km\k0  S2. Let w` = b|
` h\ 
x` = c|
` are the `th row of B and C. Also  let s = sign(y)
and t = sign(Bh\). The convex program we consider to recover (h\  m\) is the `1-BranchHull
program

` m\ and y` = b|

` m\  where b|

` and c|

` h\c|

`1-BH :

minimize
h2RK  m2RN khk1 + kmk1

subject to s`(b|

` hc|
t`b|

` m) | y`|
` h  0 ` = 1  2  . . .   L.

(3)

The motivation for the feasible set in program (3) follows from the observation that each measurement
y` = w` · x` deﬁnes a hyperbola in R2. As shown in Figure (1a)  the sign information t` = w`
restricts (w`  x`) to one of the branch of the hyperbola. The feasible set in (3) corresponds to the
convex hull of particular branches of the hyperbola for each y`. This also implies that the feasible set
is convex as it is the intersection of L convex sets.
The objective function in (3) is an `1 minimization over (h  m) that ﬁnds a sparse point (ˆh  ˆm) with
kˆhk1 = k ˆmk1. Geometrically  this happens as the solution lies at the intersection of the `1-ball 
and the hyperbolic curve (constraint) as shown in Figure 1a and 1b. So  the minimizer of (3)  under
successful recovery  is✓h\qkm\k1

km\k1◆.
  m\q kh\k1

Our main result is that under the structural assumptions that w and x live in random subspaces
with (h\  m\) containing at most S1 + S2 non zero entries and (h\  m\) satisﬁng the effective
sparsity condition (2)  the `1-BranchHull program (3) recovers h\  and m\ (to within the scaling
ambiguity) with high probability  provided the number of measurements  up to log factors  satisfy
L  ⌦(S1 + S2) log2(K + N ).
Theorem 1. Suppose we observe the pointwise product of two vectors Bh\  and Cm\ through a bilin-
ear measurement model in (1)  where B  and C are standard Gaussian random matrices. If (h\  m\)

satisfy (2)  then the `1-BranchHull program (3) uniquely recovers✓h\qkm\k1
km\k1◆ when-
ever L  CpS1 + S2 log(K + N ) + t2 for any t  0 with probability at least 1  e2Lt2. Here

  m\q kh\k1

C is an absolute constant.

kh\k1

kh\k1

1.2 Prior art for bilinear inverse problems
Recent approaches to solving bilinear inverse problems like blind deconvolution and phase retrieval
have been to lift the problems into a low rank matrix recovery task or to formulate an optimization
programs in the natural parameter space. Lifting transforms the problem of recovering h 2 RK and
m 2 RN from bilinear measurements to the problem of recovering a low rank matrix hm| from
linear measurements. The low rank matrix can then be recovered using a semideﬁnite program. The
result in Ahmed et al. [2014] for blind deconvolution showed that if h and m are representations
of the target signals with respect to Fourier and Gaussian subspaces  respectively  then the lifting
method successfully recovers the low rank matrix. The recovery occurs with high probability under
near optimal sample complexity. Unfortunately  solving the semideﬁnite program is prohibitively
computationally expensive because they operate in high-dimension space. Also  it is not clear how to
enforce additional structure like sparsity of h and m in the lifted formulation in a way that allows
optimal sample complexity (Li and Voroninski [2013]  Oymak et al. [2015]).
In comparison to the lifting approach for blind deconvolution and phase retrieval  methods that
formulate an algorithm in the natural parameter space like alternating minimization and gradient
descent based method are computationally efﬁcient and also enjoy rigorous recovery guarantees
under optimal or near optimal sample complexity (Li et al. [2016]  Candès et al. [2015]  Netrapalli
et al. [2013]  Sun et al. [2016]). In fact  the work in Lee et al. [2017] for sparse blind deconvolution

3

is based on alternating minimization. In the paper  the authors use an alternating minimization that
successively approximate the sparse vectors while enforcing the low rank property of the lifted matrix.
However  because these methods are non-convex  convergence to the global optimal requires a good
initialization (Tu et al. [2015]  Chen and Candes [2015]  Li et al. [2016]).
Other approaches that operate in the natural parameter space include PhaseMax (Bahmani and
Romberg [2016]  Goldstein and Studer [2016]) and BranchHull (Aghasi et al. [2016b]). PhaseMax
is a linear program which has been proven to ﬁnd the target signal in phase retrieval under optimal
sample complexity if a good anchor vector is available. As with alternating minimization and
gradient descent based approach  PhaseMax requires a good initialization. However  in PhaseMax
the initialization is part of the optimization program but in alternating minimization the initialization
is part of the algorithmic implementation. BranchHull is a convex program which solves the BIP
described in (3) excluding the sparsity assumption under optimal sample complexity. Like the
`1-BranchHull presented in this paper  BranchHull does not require an initialization but requires the
sign information of the signals.
The `1-BranchHull program (3) combines strengths of both the lifting method and the gradient
descent based method. Speciﬁcally  the `1-BranchHull program is a convex program that operates in
the natural parameter space  without a need for an initialization  and without restrictive assumptions
on the class of recoverable signals. These strengths are achieved at the cost of the sign information of
the target signals w and x. However  the sign assumption can be justiﬁed in imaging applications
where the goal might be to recover pixel values of a target image  which are non-negative. Also  as in
PhaseMax  the sign information can be thought of as an anchor vector which anchors the solution to
one of the branches of the L hyperbolic measurements.

1.3 Extension to noise and outlier

RBH:

Extending the theory of the `1-BranchHull program (3) to the case with noise is important as most
real data contain signiﬁcant noise. Formulation 3 may be particularly susceptible to noise that changes
the sign of even a single measurement. For the bilinear inverse problem as described in (1) with small
dense noise and arbitrary outliers  we propose the following robust `1-BranchHull program
` m + ⇠`)b|
` h | y`| 
` h  0 ` = 1  . . .   L.

h2RK  m2RN  ⇠2RL khk1 + kmk1 + k⇠k1

subject to s`(c|
t`b|

minimize

The slack variable ⇠ controls the shape of the feasible set. For measurements y` with incorrect sign 
the corresponding slack variables ⇠` shifts the feasible set so that the target signal is feasible. In the
outlier case  the `1 penalty promotes sparsity of slack variable ⇠. We implement a slight variation of
the above program  detailed in Section 1.4  to remove distortions from real and synthetic images.

(4)

1.4 Total variation extension of `1-BranchHull

The robust `1-BranchHull program (4) is ﬂexible and can be altered to remove distortions from an
otherwise piecewise constant signal. In the case where w = Bh\ is a piecewise constant signal 
x = Cm\ is a distortion signal and y = w  x is the distorted signal  the total variation version (5)
of the robust BranchHull program (4)  under successful recovery  produces the piecewise constant
signal Bh\  up to a scaling.

TV BH :

minimize

h2RK  m2RN  ⇠2RL

TV (Bh) + kmk1 + k⇠k1

(5)
t`b>` h  0 ` = 1  2  . . .   L.
In (5)  TV(·) is a total variation operator and is the `1 norm of the vector containing pairwise
difference of neighboring elements of the target signal Bh. We implement (5) to remove distortions
from images in Section 3.2.

subject to s`(⇠` + c>` m)b>` h | y`|

1.5 Notation

Vectors and matrices are written with boldface  while scalars and entries of vectors are written in
plain font. For example  c` is the `the entry of the vector c. We write 1 as the vector of all ones with

4

dimensionality appropriate for the context. We write I N as the N ⇥N identity matrix. For any x 2 R 
let (x) 2 Z such that x  1 < (x)  x. For any matrix A  let kAkF be the Frobenius norm of A.
For any vector x  let kxk0 be the number of non-zero entries in x. For x 2 RK and y 2 RN  (x  y)
is the corresponding vector in RK ⇥ RN  and h(x1  y1)  (x2  y2)i = hx1  x2i + hy1  y2i. For a set
A⇢ Rm  and a vector a 2 Rm  we deﬁne by a A   a set obtained by incrementing every element
of A by a.
2 Algorithm

In this section  we present an Alternating Direction Method of Multipliers (ADMM) implementation
of an extension of the robust `1-BranchHull program (4). The ADMM implementation of the `1-
BranchHull program (3) is similar to the ADMM implementation of (6) and we leave it to the readers.
The extension of the robust `1-BranchHull program we consider is

minimize

h2RK  m2RN  ⇠2RL kP hk1 + kmk1 + k⇠k1

subject to s`(⇠` + c>` m)b>` h | y`|

(6)

t`b>` h  0 ` = 1  2  . . .   L 

where P 2 RJ⇥K for some J 2 Z. The above extension reduces to the robust `1-BranchHull
program if P = I K. Recalling that w = Bh and x = Cm  we make use of the following notations

u = x

⇠!   v = m

w

h

⇠!   E =0@

C 0
0 B
0
0

0
0

1I L

1A and Q = I N

0
0 P
0
0

0
0

I L! .

Using this notation  our convex program can be compactly written as

minimize

v2RN +K+L u2R3L kQvk1 subject to u = Ev  u 2C .

Here C =(x  w  ⇠) 2 R3L| s`(⇠` + x`)w` | y`|  t`w`  0 ` = 1  . . .   L is the convex feasible

set of (6). Introducing a new variable z the resulting convex program can be written as

minimize

v u z

kzk1 subject to u = Ev  Qv = z  u 2C .

We may now form the scaled ADMM steps as follows

uk+1 = arg min

u

zk+1 = arg min

z

vk+1 = arg min

v

⇢
2 ku + ↵k  Evkk2
IC(u) +
⇢
2 kz + k  Qvkk2
kzk1 +
⇢
⇢
2 kk + zk+1  Qvk2  
2 k↵k + uk+1  Evk2 +

(7)

(8)

(9)

↵k+1 = ↵k + uk+1  Evk+1 
k+1 = k + vk+1  Qvk+1.

where IC(·) in (7) is the indicator function on C such that IC(u) = 0 if u 2C and inﬁnity otherwise.
We would like to note that the ﬁrst three steps of the proposed ADMM scheme can be presented in
closed form. The update in (7) is the following projection

where proj
C(v) is the projection of v onto C. Details of computing the projection onto C are presented
in the Supplementary material. The update in (8) can be written in terms of the soft-thresholding
operator

uk+1 = proj

C (Evk  ↵k)  

zk+1 = S1/⇢ (Qvk  k)  

where

(Sc(z))i =( zi  c

zi + c

0

zi > c
|zi| c
zi < c

 

where c > 0 and (Sc(z))i is the ith entry of Sc(z). Finally  the update in (9) takes the following
form

vk+1 =⇣E>E + Q|Q⌘1⇣E> (↵k + uk+1) + Q>(k + zk+1)⌘ .

In our implementation of the ADMM scheme  we initialize the algorithm with the v0 = 0  ↵0 = 0 
0 = 0.

5

3 Numerical Experiments

In this section  we provide numerical experiments on synthetic and real data where the signals follow
the multiplicative model (1)  which is compatible with physics of lighting (Hold [1986]). This is in
contrast to well-known methods for image de-illumination like He et al. [2011] where the external
light has an additive contribution to the image. Other methods like Chen et al. [2006] work with
additive models by working with the images in the log domain  while we directly work with the
multiplicative model in a robust-to-noise way. The experiment on real data presented in this section
shows total variation `1-BranchHull program can be used to remove distortions from an image. The
synthetic experiment numerically veriﬁes Theorem 1 with a low scaling constant.

3.1 Phase Portrait

15

13

11

9 

7 

5 

3 

1 

4 12 20 28 36 44 52 60 68 76 84 92 100 108 116 124 132 140

Figure 2: The empirical recovery probability from synthetic data with sparsity level S as a function
of total number of measurements L. Each block correspond to the average from 10 independent
trials. White blocks correspond to successful recovery and black blocks correspond to unsuccessful
recovery. The area to the right of the line satisﬁes L > 0.25(S1 + S2) log2(N + K).

We ﬁrst show a phase portrait that veriﬁes Theorem 1. Consider the following measurements: ﬁx
N 2{ 20  40  . . .   300}  L 2{ 4  8  . . .   140} and let K = N. Let the target signal (h\  m\) 2
RK ⇥ RN be such that both h\ and m\ have 0.05N non-zero entries with the nonzero indices
randomly selected and set to ±1. Let S1 and S2 be the number of nonzero entries in h\ and m\ 
respectively. Let B 2 RL⇥K and C 2 RL⇥N such that Bij ⇠ 1pLN (0  1) and Cij ⇠ 1pLN (0  1).
Lastly  let y = Bh\  Cm\ and t = sign(Bh\).
Figure 2 shows the fraction of successful recoveries from 10 independent trials using (3) for the
bilinear inverse problem (1) from data as described above. Let (ˆh  ˆm) be the output of (3) and let
(˜h  ˜m) be the candidate minimizer. We solve (3) using an ADMM implementation similar to the
ADMM implementation detailed in Section 2 with the step size parameter ⇢ = 1. For each trial 
we say (3) successfully recovers the target signal if k(ˆh  ˆm)  (˜h  ˜m)k2 < 1010. Black squares
correspond to no successful recovery and white squares correspond to 100% successful recovery.
The line corresponds to L = C(S1 + S2) log2(K + N ) with C = 0.25 and indicates that the sample
complexity constant in Theorem 1 is not very large.

3.2 Distortion removal from images
We use the total variation BranchHull program (5) to remove distortions from real images ˜y 2 Rp⇥q.
In the experiments  The observation y 2 RL is the column-wise vectorization of the image ˜y  the
target signal w = Bh is the vectorization of the piecewise constant image and x = Cm corresponds
to the distortions in the image. We use (5) to recover piecewise constant target images like in the
Dh  in block form. Here 
foreground of Figure 3a with TV(Bh) = kDBhk1  where D = Dv
Dv 2 R(Lq)⇥L and Dh 2 R(Lp)⇥L with
p1⌘
if j = i +⇣ i1
  (Dh)ij =( 1
if j = i + 1 +⇣ i1
p1⌘

if j = i
if j = i + p
otherwise

otherwise

(Dv)ij =8>><>>:

1
1
0

1
0

.

6

Lastly  we solve (5) using the ADMM algorithm detailed in Section 2 with P = DB.

(a) Distorted image

(b) Recovered image

(c) Distorted image

(d) Recovered image

Figure 3: Panel (a) shows an image of a mousepad with distortions and panel(b) is the piecewise
constant image recovered using total variation `1-BranchHull. Similarly  panel (d) shows an image
containing rice grains and panel (e) is the recovered image.

We now show two experiments on real images. The ﬁrst image  shown in Figure 3a  was captured
using a camera and resized to a 115 ⇥ 115 image. The measurement y 2 RL is the vectorization of
the image with L = 13225. Let B be the L ⇥ L identity matrix. Let F be the L ⇥ L inverse DCT
matrix. Let C 2 RL⇥300 with the ﬁrst column set to 1 and remaining columns randomly selected
from columns of F without replacement. The matrix C is scaled so that kCkF = kBkF = pL.
The vector of known sign t is set to 1. Let (ˆh  ˆm  ˆ⇠) be the output of (5) with  = 103 and ⇢ = 104.
Figure 3b corresponds to B ˆh and shows that the object in the center was successfully recovered.
The second real image  shown in Figure 3c  is an image of rice grains. The size of the image is
128 ⇥ 128. The measurement y 2 RL is the vectorization of the image with L = 16384. Let B be
the L ⇥ L identity matrix. Let C 2 RL⇥50 with the ﬁrst column set to 1. The remaining columns
of C are sampled from Bessel function of the ﬁrst kind J⌫() with each column corresponding to
a ﬁxed  2 R. Speciﬁcally  ﬁx g 2 RL with gi = 9 + 14 i1
L1. For each remaining column c
+5|⇣2|(0.1 + 10|⇣3|). The matrix C is scaled so that
of C  ﬁx ⇣ ⇠N (0  I3) and let ci = J
kCkF = kBkF = pL. The vector of known sign t is set to 1. Let (ˆh  ˆm  ˆ⇠) be the output of (5)
with  = 103 and ⇢ = 107. Figure 3d corresponds to B ˆh.

6+0.1|⇣1|

gi

4 Proof Outline

` m\ and y` = b|

In this section  we provide a proof of Theorem 1 by considering a related linear program with larger
feasible set. Let (h\  m\) 2 RK ⇥ RN with kh\k0  S1 and km\k0  S2. Let w` = b|
` h\ 
x` = c|
` m\. Also  let s = sign(y) and t = sign(Bh\). We will shows that
the (3) recovers (˜h  ˜m) such that (˜h  ˜m) =✓h\qkm\k1

km\k1◆.
  m\q kh\k1

Consider program (10) which has a linear constraint set that contains the feasible set of the `1-
BrachHull program (3).

` h\ · c|

kh\k1

minimize

h2RK  m2RN khk1 + kmk1subject to s`(b|

` hc|

` ˜m + b|
` = 1  2  . . .   L 

`

˜hc|

` m)  2|y`|

(10)

LP :

Let

S :=n(h  m) 2 RK ⇥ RN | (h  m) = ↵(˜h  ˜m)  and ↵ 2 [1  1]o .

(11)

Observe that if (˜h  ˜m) is a minimizer of (10) then so are all the points in the set (˜h  ˜m) S .
Lemma 1. If the optimization program (10) recovers (h  m) 2 (˜h  ˜m) S   then the BranchHull
program (3) recovers (˜h  ˜m).

7

A proof of Lemma 1  provided in Supplementary material  follows from the observations that the
feasible set of (10) contains the feasible set of (3) and (˜h  ˜m) is the only feasible point in (3) among
all (h  m) 2 (˜h  ˜m) S .
We now show that the solution of (10) lies in the set (˜h  ˜m) S . Let a|
` ) 2
RK+N denote the `th row of a matrix A. The linear constraint in (10) are now simply sA(h  m) 
2|y|. Note that S⇢N := span(˜h  ˜m) ✓ Null(A).
Our strategy will be to show that for any feasible perturbation (h  m) 2N ? the objective of the
linear program (10) strictly increases  where N? is the orthogonal complement of the subspace N .
This will be equivalent to showing that the solution of (10) lies in the set (˜h  ˜m) S .
The subgradient of the `1-norm at the proposed solution (˜h  ˜m) is
@k(˜h  ˜m)k1 := {g 2 RK+N : kgk1  1 and gh = sign(h\

)   gm = sign(m\

` = (c|

` ˜mb|

`   b|

˜hc|

)} 

m

h

`

where h  and m denote the support of non-zeros in h\  and m\  respectively. To show the linear
program converges to a solution (ˆh  ˆm) 2 (˜h  ˜m) S   it sufﬁces to show that the set of following
descent directions

n(h  m) 2N ? :⌦g  (h  m)↵  0  8g 2 @k(˜h  ˜m)k1o
✓(h  m) 2N ? : hgh  hhi + hgm  mmi + k(hc
✓(h  m) 2N ? : kgh[mk2k(hh  mm)k2 + k(hc
=n(h  m) 2N ? : k(hc

m)k1  0 
m)k1  0 
m)k1 pS1 + S2k(hh  mm)k2o =: D

  mc

  mc

  mc

h

h

h

(12)

does not contain any vector (h  m) that is consistent with the constraints. We do this by quantifying
the “width" of the set D through a Rademacher complexity  and a probability that the gradients of the
constraint functions lie in a certain half space. This allows us to use small ball method developed in
Koltchinskii and Mendelson [2015]  Mendelson [2014] to ultimately show that it is highly unlikely to
have descent directions in D that meet the constraints in (10). We now concretely state the deﬁnitions
of the Rademacher complexity  and probability term mentioned above.
Deﬁne linear functions

f`(h  m) :=D(b|

` ˜mb`)  (h  m)E  ` = 1  2  3  . . .   L.
(h  m) at (˜h  ˜m) are then simply rf` = ( @f`(˜h  ˜m)

The linear constraints in the LP (10) are deﬁned these linear functions as s`f`(h  m)2|y`|.
The gradients of f` w.r.t.
@m ) =
(s`c|

  @f`(˜h  ˜m)

˜hc`  c|

` ˜mb`  s`b|

@h

`

`

˜hc`). Deﬁne the Rademacher complexity of a set D⇢ RM as
k(h m)k2E  

"`Drf` 

C(D) := E sup

(h m)2D

1pL

(h m)

LX`=1

where "1 " 2  . . .  " L are iid Rademacher random variables that are independent of everything else.
For a set D  the quantity C(D) is a measure of width of D around the origin in terms of the gradients
of the constraint functions. For example  an equally distributed random set of gradient functions
might lead to a smaller value of C(D).
Our results also depend on a probability p⌧ (D)  and a positive parameter ⌧ introduced below

(13)

(14)

p⌧ (D) = inf

(h m)2D

P⇣Drf` 

(h m)

k(h m)k2E  ⌧⌘ .

Intuitively  p⌧ (D) quantiﬁes the size of D through the gradient vectors. For a small enough ﬁxed
parameter  a small value of p⌧ (D) means that the D is mainly invisible to to the gradient vectors.
Lemma 2. Let D be the set of descent directions  already characterized in (12)  for which C(D)  and
p⌧ (D) can be determined using (13)  and (14). Choose L ⇣ 2C(D)+t⌧
⌧ p⌧ (D) ⌘2
for any t > 0. Then the
solution (ˆh  ˆm) of the LP in (10) lies in the set (˜h  ˜m) S with probability at least 1  e2Lt2.

8

Proof of this lemma is based on small ball method developed in Koltchinskii and Mendelson [2015] 
Mendelson [2014] and further studied in Lecué et al. [2018]  Lecué and Mendelson [2017]. The
proof is mainly repeated using the argument in Bahmani and Romberg [2017]  and is provided in
the supplementary material for completeness. We now state the main theorem for linear program
(10). The theorems states that if the sparse signals satisfy the effective sparsity condition (2) and
L  Ct(S1 +S2) log2(K +N )  then the minimizer of the linear program (10) is in the set (˜h  ˜m)S
with high probability.
Theorem 2 (Exact recovery). Suppose we observe pointwise product of two vectors Bh\  and Cm\
through a bilinear measurement model in (1)  where B  and C are standard Gaussian random
matrices. If (h\  m\) satisfy (2)  then the linear program (10) recovers (ˆh  ˆm) 2 (˜h  ˜m) S with
probability at least 1  e2Lt2 whenever L  CpS1 + S2 log(K + N ) + t2  where C is an
absolute constant.
In light of Lemma 2  the proof of Theorem 2 reduces to computing the Rademacher complexity C(D)
deﬁned in (13)  and the tail probability estimate p⌧ (D) deﬁned in (14) of the set of descent directions
D deﬁned in (12). The Rademacher complexity is bounded from above by
2(S1 + S2) log2(K + N ).
and for ⌧ = min{k˜hk2 k ˜mk2}  the tail probability is bounded by p⌧ (D)  1
8c4   where both C and
c are constants. These bounds are shown in the Supplementary material. The proof of Theorem 1
follows by applying Lemma 1 to Theorem 2.

C(D)  Cqk ˜mk2

2 + k˜hk2

Acknowledgements

Ali Ahmed would like to acknowledge the partial support through the grant for the National center of
cyber security (NCCS) from HEC  Pakistan. Paul Hand would like to acknowledge funding by the
grant NSF DMS-1464525.

References
James R Fienup. Phase retrieval algorithms: a comparison. Applied optics  21(15):2758–2769  1982.

E. Candès and X. Li. Solving quadratic equations via phaselift when there are about as many

equations as unknowns. Found. Comput. Math.  pages 1–10  2012.

E. Candès  T. Strohmer  and V. Voroninski. Phaselift: Exact and stable signal recovery from magnitude

measurements via convex programming. Commun. Pure Appl. Math.  66(8):1241–1274  2013.

Ali Ahmed  Benjamin Recht  and Justin Romberg. Blind deconvolution using convex programming.

IEEE Trans. Inform. Theory  60(3):1711–1732  2014.

Thomas G Stockham  Thomas M Cannon  and Robert B Ingebretsen. Blind deconvolution through

digital signal processing. Proceedings of the IEEE  63(4):678–692  1975.

Deepa Kundur and Dimitrios Hatzinakos. Blind image deconvolution. IEEE signal processing

magazine  13(3):43–64  1996.

Alireza Aghasi  Barmak Heshmat  Albert Redo-Sanchez  Justin Romberg  and Ramesh Raskar.
Sweep distortion removal from terahertz images via blind demodulation. Optica  3(7):754–762 
2016a.

Patrik O Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine

learning research  5(Nov):1457–1469  2004.

Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances

in neural information processing systems  pages 556–562  2001.

Shuyang Ling and Thomas Strohmer. Self-calibration and biconvex compressive sensing. Inverse

Problems  31(11):115002  2015.

9

O’Grady Paul D.  Pearlmutter Barak A.  and Rickard Scott T. Survey of sparse and non-sparse
methods in source separation. International Journal of Imaging Systems and Technology  15(1):
18–33  2005.

Ivana Tosic and Pascal Frossard. Dictionary learning. IEEE Signal Processing Magazine  28(2):

27–38  2011.

Kiryung Lee  Yihing Wu  and Yoram Bresler. Near optimal compressed sensing of a class of sparse

low-rank matrices via sparse power factorization. arXiv preprint arXiv:1702.04342  2017.

Xiaodong Li and Vladislav Voroninski. Sparse signal recovery from quadratic measurements via

convex programming. SIAM Journal on Mathematical Analysis  45(5):3019–3033  2013.

Samet Oymak  Amin Jalali  Maryam Fazel  Yonina C Eldar  and Babak Hassibi. Simultaneously
structured models with application to sparse and low-rank matrices. IEEE Trans. Inform. Theory 
61(5):2886–2908  2015.

Xiaodong Li  Shuyang Ling  Thomas Strohmer  and Ke Wei. Rapid  robust  and reliable blind

deconvolution via nonconvex optimization. arXiv preprint arXiv:1606.04933  2016.

Emmanuel Candès  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval via wirtinger ﬂow:

Theory and algorithms. IEEE Trans. Inform. Theory  61(4):1985–2007  2015.

Praneeth Netrapalli  Prateek Jain  and Sujay Sanghavi. Phase retrieval using alternating minimization.

In Advances Neural Inform. Process. Syst.  pages 2796–2804  2013.

Ju Sun  Qing Qu  and John Wright. A geometric analysis of phase retrieval. In Information Theory

(ISIT)  2016 IEEE International Symposium on  pages 2379–2383. IEEE  2016.

Stephen Tu  Ross Boczar  Max Simchowitz  Mahdi Soltanolkotabi  and Benjamin Recht. Low-rank
solutions of linear matrix equations via procrustes ﬂow. arXiv preprint arXiv:1507.03566  2015.
Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly as easy

as solving linear systems. In Advances Neural Inform. Process. Syst.  pages 739–747  2015.

Sohail Bahmani and Justin Romberg. Phase retrieval meets statistical learning theory: A ﬂexible

convex relaxation. arXiv preprint arXiv:1610.04210  2016.

Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit. arXiv

preprint arXiv:1610.07531  2016.

Alireza Aghasi  Ali Ahmed  and Paul Hand. Branchhull: Convex bilinear inversion from the entrywise

product of signals with known signs. arXiv preprint arXiv:1312.0525v2  2016b.

Berthold K. P. Hold. Robot Vision. The MIT Press  1986.
K. He  J. Sun  and X. Tang. Single image haze removal using dark channel prior. IEEE Transactions
on Pattern Analysis and Machine Intelligence  33(12):2341–2353  Dec 2011. ISSN 0162-8828.
doi: 10.1109/TPAMI.2010.168.

T. Chen  Wotao Yin  Xiang Sean Zhou  D. Comaniciu  and T. S. Huang. Total variation models for
variable lighting face recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 
28(9):1519–1524  Sept 2006. ISSN 0162-8828. doi: 10.1109/TPAMI.2006.195.

Vladimir Koltchinskii and Shahar Mendelson. Bounding the smallest singular value of a random

matrix without concentration. Int. Math. Research Notices  2015(23):12991–13008  2015.

Shahar Mendelson. Learning without concentration. In Conference on Learning Theory  pages 25–39 

2014.

Guillaume Lecué  Shahar Mendelson  et al. Regularization and the small-ball method i: sparse

recovery. The Annals of Statistics  46(2):611–641  2018.

Guillaume Lecué and Shahar Mendelson. Regularization and the small-ball method ii: complexity

dependent error rates. The Journal of Machine Learning Research  18(1):5356–5403  2017.

10

Sohail Bahmani and Justin Romberg. Anchored regression: Solving random convex equations via

convex programming. arXiv preprint arXiv:1702.05327  2017.

Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics  141(1):148–188 

1989.

Aad W van der Vaart and Jon A Wellner. Weak convergence and empirical processes with applications
to statistics. Journal of the Royal Statistical Society-Series A Statistics in Society  160(3):596–608 
1997.

Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.

Springer Science &amp; Business Media  2013.

Michael G Akritas  S Lahiri  and Dimitris N Politis. Topics in nonparametric statistics. Springer 

2016.

Sara van de Geer and Johannes Lederer. The bernstein–orlicz norm and deviation inequalities.

Probability theory and related ﬁelds  157(1-2):225–250  2013.

11

,Alireza Aghasi
Ali Ahmed
Paul Hand
Babhru Joshi