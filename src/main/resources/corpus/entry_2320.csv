2016,Global Optimality of Local Search for Low Rank Matrix Recovery,We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements.  With noisy measurements we show all local minima are very close to a global optimum.  Together with a curvature bound at saddle points  this yields a polynomial time global convergence guarantee for stochastic gradient descent {\em  from random initialization}.,Global Optimality of Local Search
for Low Rank Matrix Recovery

Srinadh Bhojanapalli
srinadh@ttic.edu

Behnam Neyshabur

bneyshabur@ttic.edu

Nathan Srebro
nati@ttic.edu

Toyota Technological Institute at Chicago

Abstract

We show that there are no spurious local minima in the non-convex factorized
parametrization of low-rank matrix recovery from incoherent linear measurements.
With noisy measurements we show all local minima are very close to a global
optimum. Together with a curvature bound at saddle points  this yields a polynomial
time global convergence guarantee for stochastic gradient descent from random
initialization.

1

Introduction

Low rank matrix recovery problem is heavily studied and has numerous applications in collaborative
ﬁltering  quantum state tomography  clustering  community detection  metric learning and multi-task
learning [21  12  9  27].
We consider the “matrix sensing” problem of recovering a low-rank (or approximately low rank)
p.s.d. matrix1 X⇤ 2 Rn⇥n  given a linear measurement operator A : Rn⇥n ! Rm and noisy
measurements y = A(X⇤) + w  where w is an i.i.d. noise vector. An estimator for X⇤ is given by
the rank-constrained  non-convex problem
(1)

minimize
X:rank(X)r kA(X)  yk2.

This matrix sensing problem has received considerable attention recently [30  29  26]. This and other
rank-constrained problems are common in machine learning and related ﬁelds  and have been used
for applications discussed above. A typical theoretical approach to low-rank problems  including (1)
is to relax the low-rank constraint to a convex constraint  such as the trace-norm of X. Indeed  for
matrix sensing  Recht et al. [20] showed that if the measurements are noiseless and the measurement
operator A satisﬁes a restricted isometry property  then a low-rank X⇤ can be recovered as the unique
solution to a convex relaxation of (1). Subsequent work established similar guarantees also for the
noisy and approximate case [14  6].
However  convex relaxations to the rank are not the common approach employed in practice. In this
and other low-rank problems  the method of choice is typically unconstrained local optimization (via
e.g. gradient descent  SGD or alternating minimization) on the factorized parametrization

minimize
U2Rn⇥r

f (U ) = kA(U U>)  yk2 

(2)

1We study the case where X⇤ is PSD. We believe the techniques developed here can be used to extend

results to the general case.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

where the rank constraint is enforced by limiting the dimensionality of U. Problem (2) is a non-
convex optimization problem that could have many bad local minima (as we show in Section 5)  as
well as saddle points. Nevertheless  local optimization seems to work very well in practice. Working
on (2) is much cheaper computationally and allows scaling to large-sized problems—the number
of optimization variables is only O(nr) rather than O(n2)  and the updates are usually very cheap 
especially compared to typical methods for solving the SDP resulting from the convex relaxation.
There is therefore a signiﬁcant disconnect between the theoretically studied and analyzed methods
(based on convex relaxations) and the methods actually used in practice.
Recent attempts at bridging this gap showed that  some form of global “initialization”  typically
relying on singular value decomposition  yields a solution that is already close enough to X⇤; that
local optimization from this initializer gets to the global optima (or to a good enough solution). Jain
et al. [15]  Keshavan [17] proved convergence for alternating minimization algorithm provided the
starting point is close to the optimum  while Zheng and Lafferty [30]  Zhao et al. [29]  Tu et al.
[26]  Chen and Wainwright [8]  Bhojanapalli et al. [2] considered gradient descent methods on the
factor space and proved local convergence. But all these studies rely on global initialization followed
by local convergence  and do not tackle the question of the existence of spurious local minima or deal
with optimization starting from random initialization. There is therefore still a disconnect between
this theory and the empirical practice of starting from random initialization and relying only on the
local search to ﬁnd the global optimum.
In this paper we show that  under a suitable incoherence condition on the measurement operator A
(deﬁned in Section 2)  with noiseless measurements and with rank(X⇤)  r  the problem (2) has no
spurious local minima (i.e. all local minima are global and satisfy X⇤ = U U>). Furthermore  under
the same conditions  all saddle points have a direction with signiﬁcant negative curvature  and so
using a recent result of Ge et al. [10] we can establish that stochastic gradient descent from random
initialization converges to X⇤ in polynomial number of iterations. We extend the results also to
the noisy and approximately-low-rank settings  where we can guarantee that every local minima is
close to a global minimum. The incoherence condition we require is weaker than conditions used
to establish recovery through local search  and so our results also ensures recovery in polynomial
time under milder conditions than what was previously known. In particular  with i.i.d. Gaussian
measurements  we ensure no spurious local minima and recovery through local search with the
optimal number O(nr) of measurements.

Related Work Our work is heavily inspired by Bandeira et al. [1]  who recently showed similar
behavior for the problem of community detection—this corresponds to a speciﬁc rank-1 problem with
a linear objective  elliptope constraints and a binary solution. Here we take their ideas  extend them
and apply them to matrix sensing with general rank-r matrices. In the past several months  similar
type of results were also obtained for other non-convex problems (where the source of non-convexity
is not a rank constraint)  speciﬁcally complete dictionary learning [24] and phase recovery [25]. A
related recent result of a somewhat different nature pertains to rank unconstrained linear optimization
on the elliptope  showing that local minima of the rank-constrained problem approximate well the
global optimum of the rank unconstrained convex problem  even though they might not be the global
minima (in fact  the approximation guarantee for the actual global optimum is better) [18].
Another non-convex low-rank problem long known to not possess spurious local minima is the
PCA problem  which can also be phrased as matrix approximation with full observations  namely
minrank(X)r kA  XkF (e.g. [23]). Indeed  local search methods such as the power-method
are routinely used for this problem. Recently local optimization methods for the PCA problem
working more directly on the optimized formulation have also been studied  including SGD [22]
and Grassmannian optimization [28]. These results are somewhat orthogonal to ours  as they study
a setting in which it is well known there are never any spurious local minima  and the challenge is
obtaining satisfying convergence rates.
The seminal work of Burer and Monteiro [3] proposed low-rank factorized optimization for SDPs 
and showed that for extremely high rank r > pm (number of constraints)  an Augmented Lagrangian
method converges asymptotically to the optimum. It was also shown that (under mild conditions)
any rank deﬁcient local minima is a global minima [4  16]  providing a post-hoc veriﬁable sufﬁcient
condition for global optimality. However  this does not establish any a-priori condition  based on
problem structure  implying the lack of spurious local minima.

2

While preparing this manuscript  we also became aware of parallel work [11] studying the same
question for the related but different problem of matrix completion. For this problem they obtain
a similar guarantee  though with suboptimal dependence on the incoherence parameters and so
suboptimal sample complexity  and requiring adding a speciﬁc non-standard regularizer to the
objective—this is not needed for our matrix sensing results.
We believe our work  together with the parallel work of [11]  are the ﬁrst to establish the lack of
spurious local minima and the global convergence of local search from random initialization for a
non-trivial rank-constrained problem (beyond PCA with full observations) with rank r > 1.

Notation. For matrices X  Y 2 Rn⇥n  their inner product is hX  Y i = traceX>Y. We use
kXkF   kXk2 and kXk⇤ for the Frobenius  spectral and nuclear norms of a matrix respectively.
Given a matrix X  we use i (X) to denote singular values of X in decreasing order. Xr =
arg minrank(Y )r kX  Y kF denotes the rank-r approximation of X  as obtained via its truncated
singular value decomposition. We use plain capitals R and Q to denote orthonormal matrices.

2 Formulation and Assumptions
We write the linear measurement operator A : Rn⇥n ! Rm as A(X)i = hAi  Xi where Ai 2
Rn⇥n  yielding yi = hAi  X⇤i + wi  i = 1 ···   m. We assume wi ⇠N (0  2
w) is i.i.d Gaussian
noise. We are generally interested in the high dimensional regime where the number of measurements
m is usually much smaller than the dimension n2.
Even if we know that rank(X⇤)  r  having many measurements might not be sufﬁcient for recovery
if they are not “spread out” enough. E.g.  if all measurements only involve the ﬁrst n/2 rows and
columns  we would never have any information on the bottom-right block. A sufﬁcient condition for
identiﬁability of a low-rank X⇤ from linear measurements by Recht et al. [20] is based on restricted
isometry property deﬁned below.
Deﬁnition 2.1 (Restricted Isometry Property). Measurement operator A : Rn⇥n ! Rm (with rows
Ai  i = 1 ···   m) satisﬁes (r  r) RIP if for any n ⇥ n matrix X with rank  r 

(1  r)kXk2

F 

1
m

mXi=1

hAi  Xi2  (1 + r)kXk2
F .

(3)

In particular  X⇤ of rank r is identiﬁable if 2r < 1 [see 20  Theorem 3.2]. One situation in which
RIP is obtained is for random measurement operators. For example  matrices with i.i.d. N (0  1)
entries satisfy (r  r)-RIP when m = O( nr
2 ) [see 6  Theorem 2.3]. This implies identiﬁability based
on i.i.d. Gaussian measurement with m = O(nr) measurements (coincidentally  the number of
degrees of freedom in X⇤  optimal up to a constant factor).

3 Main Results

We are now ready to present our main result about local minima for the matrix sensing problem (2).
We ﬁrst present the results for noisy sensing of exact low rank matrices  and then generalize the
results also to approximately low rank matrices.
Now we will present our result characterizing local minima of f (U )  for low-rank X⇤. Recall that
measurements are y = A(X⇤) + w  where entries of w are i.i.d. Gaussian - wi ⇠N (0  2
w).
Theorem 3.1. Consider the optimization problem (2) where y = A(X⇤) + w  w is i.i.d. N (0  2
w) 
10  and rank(X⇤)  r. Then  with probability  1  10
A satisﬁes (4r  4r)-RIP with 4r < 1
n2 (over
the noise)  for any local minimum U of f (U ):

kU U>  X⇤kF  20r log(n)

m

w.

In particular  in the noiseless case (w = 0) we have U U> = X⇤ and so f (U ) = 0 and every local
minima is global. In the noiseless case  we can also relax the RIP requirement to 4r < 1/5 (see
Theorem 4.1 in Section 4). In the noisy case we cannot expect to ensure we always get to an exact
global minima  since the noise might cause tiny ﬂuctuations very close to the global minima possibly

3

creating multiple very close local minima. But we show that all local minima are indeed very close
to some factorization U⇤U⇤> = X⇤ of the true signal  and hence to a global optimum  and this
“radius” of local minima decreases as we have more observations.
The proof of the Theorem for the noiseless case is presented in Section 4. The proof for the general
setting follows along the same lines and can be found in the Appendix.
So far we have discussed how all local minima are global  or at least very close to a global minimum.
Using a recent result by Ge et al. [10] on the convergence of SGD for non-convex functions  we
can further obtain a polynomial bound on the number of SGD iterations required to reach the global
minima. The main condition that needs to be established in order to ensure this  is that all saddle
points of (2) satisfy the “strict saddle point condition”  i.e. have a direction with signiﬁcant negative
curvature:
Theorem 3.2 (Strict saddle). Consider the optimization problem (2) in the noiseless case  where
y = A(X⇤)  A satisﬁes (4r  4r)-RIP with 4r < 1
10  and rank(X⇤)  r. Let U be a ﬁrst order
critical point of f (U ) with U U> 6= X⇤. Then the smallest eigenvalue of the Hessian satisﬁes

min 1

mr2(f (U ))  2

5

r(X⇤).

Now consider the stochastic gradient descent updates 

U + = Projb U  ⌘ mXi=1

(⌦Ai  U U>↵  yi)AiU + !!  

(4)

where is uniformly distributed on the unit sphere and Projb is a projection onto kUkF  b. Using
Theorem 3.2 and the result of Ge et al. [10] we can establish:
Theorem 3.3 (Convergence from random initialization). Consider the optimization problem (2)
under the same noiseless conditions as in Theorem 3.2. Using b  kU⇤kF   for some global optimum
U⇤ of f (U )  for any ✏  c > 0  after T = poly⇣
✏   log(1/c)⌘ iterations of (4)
with an appropriate stepsize ⌘  starting from a random point uniformly distributed on kUkF = b 
with probability at least 1  c  we reach an iterate UT satisfying

r(X⇤)   1(X⇤)  b  1

1

kUT  U⇤kF  ✏.

The above result guarantees convergence of noisy gradient descent to a global optimum. Alternatively 
second order methods such as cubic regularization (Nesterov and Polyak [19]) and trust region (Cartis
et al. [7]) that have guarantees based on the strict saddle point property can also be used here.
RIP Requirement: Our results require (4r  1/10)-RIP for the noisy case and (4r  1/5)-RIP for the
noiseless case. Requiring (2r  2r)-RIP with 2r < 1 is sufﬁcient to ensure uniqueness of the global
optimum of (1)  and thus recovery in the noiseless setting [20]  but all known efﬁcient recovery
methods require stricter conditions. The best guarantees we are aware of require (5r  1/10)-RIP [20]
or (4r  0.414)-RIP [6] using a convex relaxation. Alternatively  (6r  1/10)-RIP is required for global
initialization followed by non-convex optimization [26]. In terms of requirements on (2r  2r)-RIP
for non-convex methods  the best we are aware of is requiring 2r < ⌦(1/r) [15  29  30]–this is a
much stronger condition than ours  and it yields a suboptimal required number of spherical Gaussian
measurements of ⌦(nr3). So  compared to prior work our requirement is very mild—it ensures
efﬁcient recovery  and requires the optimal number of spherical Gaussian measurements (up to a
constant factor) of O(nr).

Extension to Approximate Low Rank We can also obtain similar results that deteriorate gracefully
if X⇤ is not exactly low rank  but is close to being low-rank (see proof in the Appendix):
Theorem 3.4. Consider the optimization problem (2) where y = A(X⇤) and A satisﬁes (4r  4r)-
RIP with 4r < 1

100  Then  for any local minima U of f (U ):

kU U>  X⇤kF  4(kX⇤  X⇤rkF + 2rkX⇤  X⇤rk⇤) 

where X⇤r is the best rank r approximation of X⇤.

4

This theorem guarantees that any local optimum of f (U ) is close to X⇤ upto an error depending on
kX⇤  X⇤rk. For the low-rank noiseless case we have X⇤ = X⇤r and the right hand side vanishes.
When X⇤ is not exactly low rank  the best recovery error we can hope for is kX⇤  X⇤rkF   since
U U> is at most rank k. On the right hand side of Theorem 3.4  we have also a nuclear norm term 
which might be higher  but it also gets scaled down by 2r  and so by the number of measurements.

20
18
16
14
12
10
8 
6 
4 

k
n
a
R

20
18
16
14
12
10
8 
6 
4 

k
n
a
R

Random
SVD

20
18
16
14
12
10
8
6
4
2

k
n
a
R

10

20
m/n

30

40

10

20
m/n

30

40

5

10

15

25

30

35

40

20
m/n

Figure 1: The plots in this ﬁgure compare the success probability of gradient descent between
(left) random and (center) SVD initialization (suggested in [15])  for problem (2)  with increasing
number of samples m and various values of rank r. Right most plot is the ﬁrst m for a given r 
where the probability of success reaches the value 0.5. A run is considered success if kU U> 
X⇤kF /kX⇤kF  1e  2. White cells denote success and black cells denote failure of recovery. We
set n to be 100. Measurements yi are inner product of entrywise i.i.d Gaussian matrix and a rank-r
p.s.d matrix with random subspace. We notice no signiﬁcant difference between the two initialization
methods  suggesting absence of local minima as shown. Both methods have phase transition around
m = 2 · n · r.
4 Proof for the Noiseless Case

In this section we present the proof characterizing the local minima of problem (2). For ease of
exposition we ﬁrst present the results for the noiseless case (w = 0). Proof for the general case can
be found in the Appendix.
Theorem 4.1. Consider the optimization problem (2) where y = A(X⇤)  A satisﬁes (4r  4r)-RIP
with 4r < 1

5  and rank(X⇤)  r. Then  for any local minimum U of f (U ):

U U> = X⇤.

For the proof of this theorem we ﬁrst discuss the implications of the ﬁrst and second order optimality
conditions and then show how to combine them to yield the result.
Invariance of f (U ) over r ⇥ r orthonormal matrices introduces additional challenges in comparing a
given stationary point to a global optimum. We have to ﬁnd the best orthonormal matrix R to align a
given stationary point U to a global optimum U⇤  where U⇤U⇤> = X⇤  to combine results from
the ﬁrst and second order conditions  without degrading the isometry constants.
Consider a local optimum U that satisﬁes ﬁrst and second order optimality conditions of problem (2).
In particular U satisﬁes rf (U ) = 0 and z>r2f (U )z  0 for any z 2 Rn·r. Now we will see how
these two conditions constrain the error U U>  U⇤U⇤>.
First we present the following consequence of the RIP assumption [see 5  Lemma 2.1].
Lemma 4.1. Given two n ⇥ n rank-r matrices X and Y   and a (4r  )-RIP measurement operator
A  the following holds:

(5)

1
m

mXi=1



4.1 First order optimality
First we will consider the ﬁrst order condition  rf (U ) = 0. For any stationary point U this implies
(6)

hAi  XihAi  Y i  hX  Y i  kXkFkY kF .
Xi DAi  U U>  U⇤U⇤>E AiU = 0.

5

Now using the isometry property of Ai gives us the following result.
Lemma 4.2. [First order condition] For any ﬁrst order stationary point U of f (U )  and A satisfying
the (4r  )-RIP (3)  the following holds:

where Q is an orthonormal matrix that spans the column space of U.

k(U U>  U⇤U⇤>)QQ>kF  U U>  U⇤U⇤>F

 

This lemma states that any stationary point of f (U ) is close to a global optimum U⇤ in the subspace
spanned by columns of U. Notice that the error along the orthogonal subspace Q?  kX⇤Q?Q>
?kF
can still be large making the distance between X and X⇤ arbitrarily far.

Proof of Lemma 4.2. Let U = QR  for some orthonormal Q. Consider any matrix of the form
ZQR†> 2. The ﬁrst order optimality condition then implies 

mXi=1DAi  U U>  U⇤U⇤>E⌦Ai  U R†Q>Z>↵ = 0

The above equation together with Restricted Isometry Property (equation (5)) gives us the following
inequality:

DU U>  U⇤U⇤>  QQ>Z>E  U U>  U⇤U⇤>FQQ>Z>F .

Note that for any matrix A  ⌦A  QQ>Z↵ = ⌦QQ>A  Z↵. Furthermore  for any matrix A 
sup{Z:kZkF 1} hA  Zi = kAkF . Hence the above inequality implies the lemma statement.
4.2 Second order optimality
We now consider the second order condition to show that the error along Q?Q>
is indeed bounded
?
well. Let r2f (U ) be the hessian of the objective function. Note that this is an n · r ⇥ n · r matrix.
Fortunately for our result we need to only evaluate the Hessian along vec(U  U⇤R) for some
orthonormal matrix R. Here vec(.) denotes writing a matrix in vector form.
Lemma 4.3. [Hessian computation] Let U be a ﬁrst order critical point of f (U ). Then for any r⇥ r
orthonormal matrix R and j = eje>j ( = U  U⇤R) 
 2DAi  U U>  U⇤U⇤>E2
rXj=1
vec (j)>⇥r2f (U )⇤ vec (j) =
Hence from second order optimality of U we get 
Corollary 4.1. [Second order optimality] Let U be a local minimum of f (U ) . For any r ⇥ r
orthonormal matrix R 

4⌦Ai  U >j↵2

rXj=1

(

mXi=1

) 

mXi=1

4⌦Ai  U >j↵2

1
2



mXi=1DAi  U U>  U⇤U⇤>E2

 

Further for A satisfying (2r  ) -RIP (equation (3)) we have 

rXj=1
rXj=1

(7)

(8)

kU eje>j (U  U⇤R)>k2

F 

1  
2(1 + )kU U>  U⇤U⇤>k2
F .

The proof of this result follows simply by applying Lemma 4.3. The above Lemma gives a bound
on the distance in the factor (U ) space kU (U  U⇤R)>k2
F . To be able to compare the second
order condition to the ﬁrst order condition we need a relation between kU (U  U⇤R)>k2
F and
kX  X⇤k2

F . Towards this we show the following result.

2R† is the pseudo inverse of R

6

kU eje>j (U  U⇤R)>k2

Lemma 4.4. Let U and U⇤ be two n ⇥ r matrices  and Q is an orthonormal matrix that spans the
column space of U. Then there exists an r ⇥ r orthonormal matrix R such that for any ﬁrst order
stationary point U of f (U )  the following holds:
rXj=1
F ) with kU U>U⇤U⇤>k2
This Lemma bounds the distance in the factor space (k(U U⇤R)U>k2
F
and k(U U>U⇤U⇤>)QQ>k2
F . Combining this with the result from second order optimality (Corol-
lary 4.1) shows kU U>U⇤U⇤>k2
F is bounded by a constant factor of k(U U>U⇤U⇤>)QQ>k2
F .
This implies kX⇤Q?Q?kF is bounded  opposite to what the ﬁrst order condition implied
(Lemma 4.2). The proof of the above lemma is in Section B. Hence from the above optimality
conditions we get the proof of Theorem 4.1.

34
8 k(U U>  U⇤U⇤>)QQ>k2
F .

1
8kU U>  U⇤U⇤>k2

F 

F +

Proof of Theorem 4.1. Assuming U U> 6= U⇤U⇤>  from Lemmas 4.2  4.4 and Corollary 4.1 we
get 

✓ 1  

2(1 + ) 

1

8◆kU U>  U⇤U⇤>k2

F 

34
8

2(U U>  U⇤U⇤>)

2

F

.

5 the above inequality holds only if U U> = U⇤U⇤>.

If   1
5 Necessity of RIP

We showed that there are no spurious local minima only under a restricted isometry assumption.
A natural question is whether this is necessary  or whether perhaps the problem (2) never has any

spurious local minima  perhaps similarly to the non-convex PCA problem minUA  U U>.

A good indication that this is not the case is that (2) is NP-hard  even in the noiseless case when
y = A(X⇤) for rank(X⇤)  k [20] (if we don’t require RIP  we can have each Ai be non-zero on
a single entry in which case (2) becomes a matrix completion problem  for which hardness has been
shown even under fairly favorable conditions [13])3. That is  we are unlikely to have a poly-time
algorithm that succeeds for any linear measurement operator. Although this doesn’t formally preclude
the possibility that there are no spurious local minima  but it just takes a very long time to ﬁnd a local
minima  this scenario seems somewhat unlikely.
To resolve the question  we present an explicit example of a measurement operator A and y = A(X⇤)
(i.e. f (X⇤) = 0)  with rank(X⇤) = r  for which (1)  and so also (2)  have a non-global local minima.
Example 1: Let f (X) = (X11 + X22  1)2 + (X11  1)2 + X 2
21 and consider (1) with r = 1
0 0 we have f (X⇤) = 0 and rank(X⇤) = 1. But X =0 0
0 1
(i.e. a rank-1 constraint). For X⇤ =1 0
We can be extended the construction to any rank r by simply addingPr+2
i=3 (Xii  1)2 to the objective 
and padding both the global and local minimum with a diagonal beneath the leading 2 ⇥ 2 block.
In Example 1  we had a rank-r problem  with a rank-r exact solution  and a rank-r local minima.
Another question we can ask is what happens if we allow a larger rank than the rank of the optimal
solution. That is  if we have f (X⇤) = 0 with low rank(X⇤)  even rank(X⇤) = 1  but consider (1)
or (2) with a high r. Could we still have non-global local minima? The answer is yes...

is a rank 1 local minimum with f (X) = 1.

Example 2: Let f (X) = (X11 + X22 + X33  1)2 + (X11  1)2 + (X22  X33)2 +Pi j:i6=j X 2
and consider the problem (1) with a rank r = 2 constraint. We can verify that X⇤ = 24
0 0 035
0 0 1/235 is a local minimum with
is a rank=1 global minimum with f (X⇤) = 0  but X = 24

0 0
0
0 1/2 0

12 + X 2

1 0 0
0 0 0

3Note that matrix completion is tractable under incoherence assumptions  similar to RIP

ij

7

f (X) = 1. Also for an arbitrary large rank constraint r > 1 (taking r to be odd for simplicity) 

⇥(X11 + X2i 2i + X(2i+1) (2i+1)  1)2
extend the objective to f (X) = (X11  1)2 +P(r1)/2
+(X2i 2i  X(2i+1) (2i+1))2⇤. We still have a rank-1 global minimum X⇤ with a single non-zero
entry X⇤11 = 1  while X = (I  X⇤)/2 is a local minimum with f (X) = 1.
6 Conclusion

i=1

We established that under conditions similar to those required for convex relaxation recovery guaran-
tees  the non-convex formulation of matrix sensing (2) does not exhibit any spurious local minima (or 
in the noisy and approximate settings  at least not outside some small radius around a global minima) 
and we can obtain theoretical guarantees on the success of optimizing it using SGD from random
initialization. This matches the methods frequently used in practice  and can explain their success.
This guarantee is very different in nature from other recent work on non-convex optimization for
low-rank problems  which relied heavily on initialization to get close to the global optimum  and on
local search just for the ﬁnal local convergence to the global optimum. We believe this is the ﬁrst
result  together with the parallel work of Ge et al. [11]  on the global convergence of local search for
common rank-constrained problems that are worst-case hard.
Our result suggests that SVD initialization is not necessary for global convergence  and random
initialization would succeed under similar conditions (in fact  our conditions are even weaker than in
previous work that used SVD initialization). To investigate empirically whether SVD initialization
is indeed helpful for ensuring global convergence  in Figure 1 we compare recovery probability
of random rank-k matrices for random and SVD initialization—there is no signiﬁcant difference
between the two.
Beyond the implications for matrix sensing  we are hoping these type of results could be a ﬁrst step
and serve as a model for understanding local search in deep networks. Matrix factorization  such as
in (2)  is a depth-two neural network with linear transfer—an extremely simple network  but already
non-convex and arguably the most complicated network we have a good theoretical understanding of.
Deep networks are also hard to optimize in the worst case  but local search seems to do very well
in practice. Our ultimate goal is to use the study of matrix recovery as a guide in understating the
conditions that enable efﬁcient training of deep networks.

Acknowledgements
Authors would like to thank Afonso Bandeira for discussions  Jason Lee and Tengyu Ma for sharing
and discussing their work. This research was supported in part by an NSF RI/AF grant 1302662.

References
[1] A. S. Bandeira  N. Boumal  and V. Voroninski. On the low-rank approach for semideﬁnite programs arising

in synchronization and community detection. arXiv preprint arXiv:1602.04426  2016.

[2] S. Bhojanapalli  A. Kyrillidis  and S. Sanghavi. Dropping convexity for faster semi-deﬁnite optimization.

arXiv preprint arXiv:1509.03917  2015.

[3] S. Burer and R. D. Monteiro. A nonlinear programming algorithm for solving semideﬁnite programs via

low-rank factorization. Mathematical Programming  95(2):329–357  2003.

[4] S. Burer and R. D. Monteiro. Local minima and convergence in low-rank semideﬁnite programming.

Mathematical Programming  103(3):427–444  2005.

[5] E. J. Candès. The restricted isometry property and its implications for compressed sensing. Comptes

Rendus Mathematique  346(9):589–592  2008.

[6] E. J. Candes and Y. Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of

noisy random measurements. Information Theory  IEEE Transactions on  57(4):2342–2359  2011.

[7] C. Cartis  N. I. Gould  and P. L. Toint. Complexity bounds for second-order optimality in unconstrained

optimization. Journal of Complexity  28(1):93–108  2012.

[8] Y. Chen and M. J. Wainwright. Fast low-rank estimation by projected gradient descent: General statistical

and algorithmic guarantees. arXiv preprint arXiv:1509.03025  2015.

8

[9] S. Flammia  D. Gross  Y.-K. Liu  and J. Eisert. Quantum tomography via compressed sensing: Error

bounds  sample complexity and efﬁcient estimators. New Journal of Physics  14(9):095022  2012.

[10] R. Ge  F. Huang  C. Jin  and Y. Yuan. Escaping from saddle points—online stochastic gradient for tensor

decomposition. In Proceedings of The 28th Conference on Learning Theory  pages 797–842  2015.

[11] R. Ge  J. Lee  and T. Ma. Matrix completion has no spurious local minimum.

arXiv:1605.07272  2016.

arXiv preprint

[12] D. Gross  Y.-K. Liu  S. T. Flammia  S. Becker  and J. Eisert. Quantum state tomography via compressed

sensing. Physical review letters  105(15):150401  2010.

[13] M. Hardt  R. Meka  P. Raghavendra  and B. Weitz. Computational limits for matrix completion. In

Proceedings of The 27th Conference on Learning Theory  pages 703–725  2014.

[14] P. Jain  R. Meka  and I. S. Dhillon. Guaranteed rank minimization via singular value projection. In

Advances in Neural Information Processing Systems  pages 937–945  2010.

[15] P. Jain  P. Netrapalli  and S. Sanghavi. Low-rank matrix completion using alternating minimization. In

Proceedings of the 45th annual ACM Symposium on theory of computing  pages 665–674. ACM  2013.

[16] M. Journée  F. Bach  P.-A. Absil  and R. Sepulchre. Low-rank optimization on the cone of positive

semideﬁnite matrices. SIAM Journal on Optimization  20(5):2327–2351  2010.

[17] R. H. Keshavan. Efﬁcient algorithms for collaborative ﬁltering. PhD thesis  STANFORD  2012.

[18] A. Montanari. A Grothendieck-type inequality for local maxima. arXiv preprint arXiv:1603.04064  2016.

[19] Y. Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance.

Mathematical Programming  108(1):177–205  2006.

[20] B. Recht  M. Fazel  and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via

nuclear norm minimization. SIAM review  52(3):471–501  2010.

[21] J. D. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In

Proceedings of the 22nd international conference on Machine learning  pages 713–719. ACM  2005.

[22] C. D. Sa  C. Re  and K. Olukotun. Global convergence of stochastic gradient descent for some non-convex
matrix problems. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15) 
pages 2332–2341  2015.

[23] N. Srebro and T. Jaakkola. Weighted low-rank approximations. In Proceedings of the 20th International

Conference on Machine Learning (ICML-03)  pages 720–727  2003.

[24] J. Sun  Q. Qu  and J. Wright. Complete dictionary recovery using nonconvex optimization. In Proceedings

of The 32nd International Conference on Machine Learning  pages 2351–2360  2015.

[25] J. Sun  Q. Qu  and J. Wright. A geometric analysis of phase retrieval. preprint arXiv:1602.06664  2016.

[26] S. Tu  R. Boczar  M. Soltanolkotabi  and B. Recht. Low-rank solutions of linear matrix equations via

Procrustes ﬂow. arXiv preprint arXiv:1507.03566  2015.

[27] H.-F. Yu  P. Jain  P. Kar  and I. Dhillon. Large-scale multi-label learning with missing labels. In Proceedings

of The 31st International Conference on Machine Learning  pages 593–601  2014.

[28] D. Zhang and L. Balzano. Global convergence of a grassmannian gradient descent algorithm for subspace

estimation. arXiv preprint arXiv:1506.07405  2015.

[29] T. Zhao  Z. Wang  and H. Liu. A nonconvex optimization framework for low rank matrix estimation. In

Advances in Neural Information Processing Systems  pages 559–567  2015.

[30] Q. Zheng and J. Lafferty. A convergent gradient descent algorithm for rank minimization and semideﬁnite
programming from random linear measurements. In Advances in Neural Information Processing Systems 
pages 109–117  2015.

9

,Yuancheng Zhu
John Lafferty
Srinadh Bhojanapalli
Behnam Neyshabur
Nati Srebro
David Durfee
Ryan Rogers