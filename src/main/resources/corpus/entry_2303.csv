2017,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks,Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs  and imposing smoothness priors on these graphs. However  such techniques do not fully exploit the local stationary structures on user/item graphs  and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines a novel multi-graph convolutional neural network that can learn meaningful statistical graph-structured patterns from users and items  and a recurrent neural network that applies a learnable diffusion on the score matrix. Our neural network system is computationally attractive as it requires a constant number of parameters independent of the matrix size. We apply our method on several standard datasets  showing that it outperforms state-of-the-art matrix completion techniques.,Geometric Matrix Completion with Recurrent

Multi-Graph Neural Networks

Federico Monti

Università della Svizzera italiana

Lugano  Switzerland

federico.monti@usi.ch

Michael M. Bronstein

Università della Svizzera italiana

Lugano  Switzerland

michael.bronstein@usi.ch

Xavier Bresson

School of Computer Science and Engineering

NTU  Singapore

xbresson@ntu.edu.sg

Abstract

Matrix completion models are among the most common formulations of recom-
mender systems. Recent works have showed a boost of performance of these
techniques when introducing the pairwise relationships between users/items in the
form of graphs  and imposing smoothness priors on these graphs. However  such
techniques do not fully exploit the local stationary structures on user/item graphs 
and the number of parameters to learn is linear w.r.t. the number of users and items.
We propose a novel approach to overcome these limitations by using geometric
deep learning on graphs. Our matrix completion architecture combines a novel
multi-graph convolutional neural network that can learn meaningful statistical
graph-structured patterns from users and items  and a recurrent neural network that
applies a learnable diffusion on the score matrix. Our neural network system is
computationally attractive as it requires a constant number of parameters indepen-
dent of the matrix size. We apply our method on several standard datasets  showing
that it outperforms state-of-the-art matrix completion techniques.

Introduction

1
Recommender systems have become a central part of modern intelligent systems. Recommending
movies on Netﬂix  friends on Facebook  furniture on Amazon  and jobs on LinkedIn are a few
examples of the main purpose of these systems. Two major approaches to recommender systems are
collaborative [5] and content [32] ﬁltering techniques. Systems based on collaborative ﬁltering use
collected ratings of items by users and offer new recommendations by ﬁnding similar rating patterns.
Systems based on content ﬁltering make use of similarities between items and users to recommend
new items. Hybrid systems combine collaborative and content techniques.
Matrix completion. Mathematically  a recommendation method can be posed as a matrix com-
pletion problem [9]  where columns and rows represent users and items  respectively  and matrix
values represent scores determining whether a user would like an item or not. Given a small subset of
known elements of the matrix  the goal is to ﬁll in the rest. A famous example is the Netﬂix challenge
[22] offered in 2009 and carrying a 1M$ prize for the algorithm that can best predict user ratings for
movies based on previous user ratings. The size of the Netﬂix matrix is 480k movies × 18k users
(8.5B entries)  with only 0.011% known entries.
Recently  there have been several attempts to incorporate geometric structure into matrix completion
problems [27  19  33  24]  e.g. in the form of column and row graphs representing similarity of users

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

and items  respectively. Such additional information deﬁnes e.g. the notion of smoothness of the
matrix and was shown beneﬁcial for the performance of recommender systems. These approaches
can be generally related to the ﬁeld of signal processing on graphs [37]  extending classical harmonic
analysis methods to non-Euclidean domains (graphs).
Geometric deep learning. Of key interest to the design of recommender systems are deep learning
approaches. In the recent years  deep neural networks and  in particular  convolutional neural networks
(CNNs) [25] have been applied with great success to numerous applications. However  classical CNN
models cannot be directly applied to the recommendation problem to extract meaningful patterns in
users  items and ratings because these data are not Euclidean structured  i.e. they do not lie on regular
lattices like images but rather irregular domains like graphs. Recent works applying deep learning to
recommender systems used networks with fully connected or auto-encoder architectures [44  35  14].
Such methods are unable to extract the important local stationary patterns from the data  which is
one of the key properties of CNN architectures. New neural networks are necessary and this has
motivated the recent development of geometric deep learning techniques that can mathematically deal
with graph-structured data  which arises in numerous applications  ranging from computer graphics
and vision [28  2  4  3  30] to chemistry [12]. We recommend the review paper [6] to the reader not
familiar with this line of works.
The earliest attempts to apply neural networks to graphs are due to Scarselli et al. [13  34] (see more
recent formulation [26  40]). Bruna et al. [7  15] formulated CNN-like deep neural architectures
on graphs in the spectral domain  employing the analogy between the classical Fourier transforms
and projections onto the eigenbasis of the graph Laplacian operator [37]. Defferrard et al. [10]
proposed an efﬁcient ﬁltering scheme using recurrent Chebyshev polynomials  which reduces the
complexity of CNNs on graphs to the same complexity of classical (Euclidean) CNNs. This model
was later extended to deal with dynamic data [36]. Kipf and Welling [21] proposed a simpliﬁcation of
Chebychev networks using simple ﬁlters operating on 1-hop neighborhoods of the graph. Monti et al.
[30] introduced a spatial-domain generalization of CNNs to graphs local patch operators represented
as Gaussian mixture models  showing signiﬁcantly better generalization across different graphs.
Contributions. We present two main contributions. First  we introduce a new multi-graph CNN
architecture that generalizes [10] to multiple graphs. This new architecture is able to extract local
stationary patterns from signals deﬁned on multiple graphs simultaneously. While in this work we
apply multi-graph CNNs in the context of recommender systems to the graphs of users and items 
however  our architecture is generic and can be used in other applications  such as neuroscience
(autism detection with network of people and brain connectivity [31  23])  computer graphics (shape
correspondence on product manifold [41])  or social network analysis (abnormal spending behavior
detection with graphs of customers and stores [39]). Second  we approach the matrix completion
problem as learning on user and item graphs using the new deep multi-graph CNN framework. Our
architecture is based on a cascade of multi-graph CNN followed by Long Short-Term Memory
(LSTM) recurrent neural network [16] that together can be regarded as a learnable diffusion process
that reconstructs the score matrix.

2 Background

2.1 Matrix Completion
Matrix completion problem. Recovering the missing values of a matrix given a small fraction
of its entries is an ill-posed problem without additional mathematical constraints on the space of
solutions. It is common to assume that the variables lie in a smaller subspace  i.e.  the matrix is of
low rank 

rank(X)

min
X

s.t. xij = yij  ∀ij ∈ Ω 

where X denotes the matrix to recover  Ω is the set of the known entries and yij are their values.
Unfortunately  rank minimization turns out to be an NP-hard combinatorial problem that is computa-
tionally intractable in practical cases. The tightest possible convex relaxation of problem (1) is to
replace the rank with the nuclear norm (cid:107) · (cid:107)(cid:63) equal to the sum of its singular values [8] 

(1)

(2)

min
X

(cid:107)X(cid:107)(cid:63) +

(cid:107)Ω ◦ (X − Y)(cid:107)2
F;

µ
2

the equality constraint is also replaced with a penalty to make the problem more robust to noise (here
Ω is the indicator matrix of the known entries Ω and ◦ denotes the Hadamard pointwise product).

2

ij = wc

ji  wc

ij = 0 if (i  j) /∈ Ec and wc

Candès and Recht [8] proved that under some technical conditions the solutions of problems (2)
and (1) coincide.
Geometric matrix completion An alternative relaxation of the rank operator in (1) can be achieved
constraining the space of solutions to be smooth w.r.t. some geometric structure on the rows and
columns of the matrix [27  19  33  1]. The simplest model is proximity structure represented as an
undirected weighted column graph Gc = ({1  . . .   n} Ec  Wc) with adjacency matrix Wc = (wc
ij) 
ij > 0 if (i  j) ∈ Ec. In our setting  the column
where wc
graph could be thought of as a social network capturing relations between users and the similarity of
their tastes. The row graph Gr = ({1  . . .   m} Er  Wr) representing the items similarities is deﬁned
similarly.
On each of these graphs one can construct the (normalized) graph Laplacian  an n × n symmetric
j(cid:54)=i wij) is the degree
matrix. We denote the Laplacian associated with row and column graphs by ∆r and ∆c  respec-
tively. Considering the columns (respectively  rows) of matrix X as vector-valued functions on the
column graph Gc (respectively  row graph Gr)  their smoothness can be expressed as the Dirichlet
= trace(X(cid:62)∆rX) (respecitvely  (cid:107)X(cid:107)2Gc
norm (cid:107)X(cid:107)2Gr
= trace(X∆cX(cid:62))). The geometric matrix
completion problem [19] thus boils down to minimizing

positive-semideﬁnite matrix ∆ = I − D−1/2WD−1/2  where D = diag((cid:80)

min
X

(cid:107)X(cid:107)2Gr

+ (cid:107)X(cid:107)2Gc

+

(cid:107)Ω ◦ (X − Y)(cid:107)2
F.

µ
2

(3)

Factorized models. Matrix completion algorithms introduced in the previous section are well-posed
as convex optimization problems  guaranteeing existence  uniqueness and robustness of solutions.
Besides  fast algorithms have been developed for the minimization of the non-differentiable nuclear
norm. However  the variables in this formulation are the full m × n matrix X  making it hard to scale
up to large matrices such as the Netﬂix challenge.
A solution is to use a factorized representation [38  22  27  43  33  1] X = WH(cid:62)  where W  H are
m × r and n × r matrices  respectively  with r (cid:28) min(m  n). The use of factors W  H reduces the
number of degrees of freedom from O(mn) to O(m + n); this representation is also attractive as
people often assumes the original matrix to be low-rank for solving the matrix completion problem 
and rank(WH(cid:62)) ≤ r by construction.
The nuclear norm minimization problem (2) can be rewritten in a factorized form as [38]:

min
W H

(cid:107)W(cid:107)2

F +

1
2

(cid:107)H(cid:107)2

F +

1
2

µ
2

(cid:107)Ω ◦ (WH(cid:62) − Y)(cid:107)2
F.

and the factorized formulation of the graph-based minimization problem (3) as

min
W H

(cid:107)W(cid:107)2Gr

+

1
2

(cid:107)H(cid:107)2Gc

+

1
2

µ
2

(cid:107)Ω ◦ (WH(cid:62) − Y)(cid:107)2
F.

(4)

(5)

The limitation of model (5) is that it decouples the regularization previously applied simultaneously
on the rows and columns of X in (3)  but the advantage is linear instead of quadratic complexity.
2.2 Deep learning on graphs
The key concept underlying our work is geometric deep learning  an extension of CNNs to graphs. In
particular  we focus here on graph CNNs formulated in the spectral domain. A graph Laplacian admits
a spectral eigendecomposition of the form ∆ = ΦΛΦ(cid:62)  where Φ = (φ1  . . . φn) denotes the matrix
of orthonormal eigenvectors and Λ = diag(λ1  . . .   λn) is the diagonal matrix of the corresponding
eigenvalues. The eigenvectors play the role of Fourier atoms in classical harmonic analysis and the
eigenvalues can be interpreted as frequencies. Given a function x = (x1  . . .   xn)(cid:62) on the vertices
of the graph  its graph Fourier transform is given by ˆx = Φ(cid:62)x. The spectral convolution of two
functions x  y can be deﬁned as the element-wise product of the respective Fourier transforms 

x (cid:63) y = Φ(Φ(cid:62)y) ◦ (Φ(cid:62)x) = Φ diag(ˆy1  . . .   ˆyn) ˆx 

(6)

by analogy to the Convolution Theorem in the Euclidean case.
Bruna et al. [7] used the spectral deﬁnition of convolution (6) to generalize CNNs on graphs. A
spectral convolutional layer in this formulation has the form

˜xl = ξ

l = 1  . . .   q 

(7)

 q(cid:48)(cid:88)

l(cid:48)=1

  

Φ ˆYll(cid:48)Φ(cid:62)xl(cid:48)

3

respectively 

input and output channels 

where q(cid:48)  q denote the number of
ˆYll(cid:48) =
diag(ˆyll(cid:48) 1  . . .   ˆyll(cid:48) n) is a diagonal matrix of spectral multipliers representing a learnable ﬁlter
in the spectral domain  and ξ is a nonlinearity (e.g. ReLU) applied on the vertex-wise function values.
Unlike classical convolutions carried out efﬁciently in the spectral domain using FFT  the computa-
tions of the forward and inverse graph Fourier transform incur expensive O(n2) multiplication by
the matrices Φ  Φ(cid:62)  as there are no FFT-like algorithms on general graphs. Second  the number of
parameters representing the ﬁlters of each layer of a spectral CNN is O(n)  as opposed to O(1) in
classical CNNs. Third  there is no guarantee that the ﬁlters represented in the spectral domain are
localized in the spatial domain  which is another important property of classical CNNs.
Henaff et al. [15] argued that spatial localization can be achieved by forcing the spectral mul-
tipliers to be smooth. The ﬁlter coefﬁcients are represented as ˆyk = τ (λk)  where τ (λ) is a
smooth transfer function of frequency λ; its application to a signal x is expressed as τ (∆)x =
Φ diag(τ (λ1)  . . .   τ (λn))Φ(cid:62)x  where applying a function to a matrix is understood in the operator
sense and boils down to applying the function to the matrix eigenvalues. In particular  the authors
used parametric ﬁlters of the form

(8)
where β1(λ)  . . .   βr(λ) are some ﬁxed interpolation kernels  and θ = (θ1  . . .   θp) are p = O(1)
interpolation coefﬁcients acting as parameters of the spectral convolutional layer.
Defferrard et al. [10] used polynomial ﬁlters of order p represented in the Chebyshev basis 

θjβj(λ) 

τθ(λ) =

p(cid:88)

j=1

p(cid:88)

τθ(˜λ) =

θjTj(˜λ) 

(9)

j=0

Laplacian eigenvectors  as applying a Chebyshev ﬁlter to x amounts to τθ( ˜∆)x =(cid:80)p

where ˜λ is frequency rescaled in [−1  1]  θ is the (p+1)-dimensional vector of polynomial coefﬁcients
parametrizing the ﬁlter  and Tj(λ) = 2λTj−1(λ) − Tj−2(λ) denotes the Chebyshev polynomial of
n ∆ − I is
degree j deﬁned in a recursive manner with T1(λ) = λ and T0(λ) = 1. Here  ˜∆ = 2λ−1
the rescaled Laplacian with eigenvalues ˜Λ = 2λ−1
This approach beneﬁts from several advantages. First  it does not require an explicit computation of the
j=0 θjTj( ˜∆)x;
due to the recursive deﬁnition of the Chebyshev polynomials  this incurs applying the Laplacian p
times. Multiplication by Laplacian has the cost of O(|E|)  and assuming the graph has |E| = O(n)
edges (which is the case for k-nearest neighbors graphs and most real-world networks)  the overall
complexity is O(n) rather than O(n2) operations  similarly to classical CNNs. Moreover  since the
Laplacian is a local operator affecting only 1-hop neighbors of a vertex and accordingly its pth power
affects the p-hop neighborhood  the resulting ﬁlters are spatially localized.

n Λ − I in the interval [−1  1].

3 Our approach
In this paper  we propose formulating matrix completion as a problem of deep learning on user and
item graphs. We consider two architectures summarized in Figures 1 and 2. The ﬁrst architecture
works on the full matrix model producing better accuracy but requiring higher complexity. The
second architecture used factorized matrix model  offering better scalability at the expense of slight
reduction of accuracy. For both architectures  we consider a combination of multi-graph CNN and
RNN  which will be described in detail in the following sections. Multi-graph CNNs are used to
extract local stationary features from the score matrix using row and column similarities encoded by
user and item graphs. Then  these spatial features are fed into a RNN that diffuses the score values
progressively  reconstructing the matrix.
3.1 Multi-Graph CNNs
Multi-graph convolution. Our ﬁrst goal is to extend the notion of the aforementioned graph
Fourier transform to matrices whose rows and columns are deﬁned on row- and column-graphs. We
recall that the classical two-dimensional Fourier transform of an image (matrix) can be thought of as
applying a one-dimensional Fourier transform to its rows and columns. In our setting  the analogy of
the two-dimensional Fourier transform has the form
ˆX = Φ(cid:62)

(10)

r XΦc

4

where Φc  Φr and Λc = diag(λc 1  . . .   λc n) and Λr = diag(λr 1  . . .   λr m) denote the n × n and
m × m eigenvector- and eigenvalue matrices of the column- and row-graph Laplacians ∆c  ∆r 
respectively. The multi-graph version of the spectral convolution (6) is given by

X (cid:63) Y = Φr( ˆX ◦ ˆY)Φ(cid:62)
c  

(11)
and in the classical setting can be thought as the analogy of ﬁltering a 2D image in the spectral domain
(column and row graph eigenvalues λc and λr generalize the x- and y-frequencies of an image).
As in [7]  representing multi-graph ﬁlters as their spectral multipliers ˆY would yield O(mn) parame-
ters  prohibitive in any practical application. To overcome this limitation  we follow [15]  assuming
that the multi-graph ﬁlters are expressed in the spectral domain as a smooth function of both frequen-
cies (eigenvalues λc  λr of the row- and column graph Laplacians) of the form ˆYk k(cid:48) = τ (λc k  λr k(cid:48)).
In particular  using Chebychev polynomial ﬁlters of degree p 1

τΘ(˜λc  ˜λr) =

θjj(cid:48)Tj(˜λc)Tj(cid:48)(˜λr) 

(12)

p(cid:88)

j j(cid:48)=0

p(cid:88)

j j(cid:48)=0

where ˜λc  ˜λr are the frequencies rescaled [−1  1] (see Figure 4 for examples). Such ﬁlters are
parametrized by a (p + 1) × (p + 1) matrix of coefﬁcients Θ = (θjj(cid:48))  which is O(1) in the input
size as in classical CNNs on images. The application of a multi-graph ﬁlter to the matrix X

˜X =

θjj(cid:48)Tj( ˜∆r)XTj(cid:48)( ˜∆c)

(13)

incurs an O(mn) computational complexity (here  as previously  ˜∆c = 2λ−1
2λ−1
Similarly to (7)  a multi-graph convolutional layer using the parametrization of ﬁlters according
to (13) is applied to q(cid:48) input channels (m × n matrices X1  . . .   Xq(cid:48) or a tensor of size m × n × q(cid:48)) 

r m∆r − I denote the scaled Laplacians).
p(cid:88)

c n∆c − I and ˜∆r =

 = ξ

 q(cid:48)(cid:88)

θjj(cid:48) ll(cid:48)Tj( ˜∆r)Xl(cid:48)Tj(cid:48)( ˜∆c)

l = 1  . . .   q 

(14)

  

Xl(cid:48) (cid:63) Yll(cid:48)

˜Xl = ξ

 q(cid:48)(cid:88)

l(cid:48)=1

l(cid:48)=1

j j(cid:48)=0

producing q outputs (tensor of size m × n × q). Several layers can be stacked together. We call such
an architecture a Multi-Graph CNN (MGCNN).
Separable convolution. A simpliﬁcation of the multi-graph convolution is obtained considering
the factorized form of the matrix X = WH(cid:62) and applying one-dimensional convolutions to the
respective graph to each factor. Similarly to the previous case  we can express the ﬁlters resorting to
Chebyshev polynomials 

˜wl =

j Tj( ˜∆r)wl 
θr

˜hl =

j(cid:48)Tj(cid:48)( ˜∆c)hl 
θc

l = 1  . . .   r

(15)

p(cid:88)

j(cid:48)=0

p(cid:88)

j=0

where wl  hl denote the lth columns of factors W  H and θr = (θr
0  . . .   θc
p)
are the parameters of the row- and column- ﬁlters  respectively (a total of 2(p + 1) = O(1)).
Application of such ﬁlters to W and H incurs O(m + n) complexity. Convolutional layers (14) thus
take the form

p) and θc = (θc

0  . . .   θr

 q(cid:48)(cid:88)

p(cid:88)

l(cid:48)=1

j=0

˜wl = ξ

  

 q(cid:48)(cid:88)

p(cid:88)

l(cid:48)=1

j(cid:48)=0

j ll(cid:48)Tj( ˜∆r)wl(cid:48)
θr

˜hl = ξ

j(cid:48) ll(cid:48)Tj(cid:48)( ˜∆c)hl(cid:48)
θc

(16)

 .

We call such an architecture a separable MGCNN or sMGCNN.
3.2 Matrix diffusion with RNNs
The next step of our approach is to feed the spatial features extracted from the matrix by the MGCNN
or sMGCNN to a recurrent neural network (RNN) implementing a diffusion process that progressively
reconstructs the score matrix (see Figure 3). Modelling matrix completion as a diffusion process

1For simplicity  we use the same degree p for row- and column frequencies.

5

X(t+1) = X(t) + dX(t)

dX(t)

X

X(t)

MGCNN

˜X(t)

RNN

row+column ﬁltering

Figure 1: Recurrent MGCNN (RMGCNN) architecture using the full matrix completion model and
operating simultaneously on the rows and columns of the matrix X. Learning complexity is O(mn).

H(t+1) = H(t) + dH(t)

H(cid:62)

H(t)

GCNN

˜H(t)

RNN

column ﬁltering

W

W(t+1) = W(t) + dW(t)

W(t)

GCNN

˜W(t)

RNN

row ﬁltering

dH(t)

dW(t)

Figure 2: Separable Recurrent MGCNN (sRMGCNN) architecture using the factorized matrix
completion model and operating separately on the rows and columns of the factors W  H(cid:62). Learning
complexity is O(m + n).

t = 0

1

2

3

4

5

6

7

8

9

10

2.26

1.89

1.60

1.78

1.31

0.52

0.48

0.63

0.38

0.07

0.01

1.15

1.04

0.94

0.89

0.84

0.76

0.69

0.49

0.27

0.11

0.01

Figure 3: Evolution of matrix X(t) with our architecture using full matrix completion model
RMGCNN (top) and factorized matrix completion model sRMGCNN (bottom). Numbers indi-
cate the RMS error.

appears particularly suitable for realizing an architecture which is independent of the sparsity of
the available information. In order to combine the few scores available in a sparse input matrix  a
multilayer CNN would require very large ﬁlters or many layers to diffuse the score information across
matrix domains. On the contrary  our diffusion-based approach allows to reconstruct the missing
information just by imposing the proper amount of diffusion iterations. This gives the possibility
to deal with extremely sparse data  without requiring at the same time excessive amounts of model
parameters. See Table 3 for an experimental evaluation on this aspect.
We use the classical LSTM architecture [16]  which has demonstrated to be highly efﬁcient to
learn complex non-linear diffusion processes due to its ability to keep long-term internal states
(in particular  limiting the vanishing gradient issue). The input of the LSTM gate is given by the
static features extracted from the MGCNN  which can be seen as a projection or dimensionality
reduction of the original matrix in the space of the most meaningful and representative information
(the disentanglement effect). This representation coupled with LSTM appears particularly well-suited
to keep a long term internal state  which allows to predict accurate small changes dX of the matrix
X (or dW  dH of the factors W  H) that can propagate through the full temporal steps.

6

Figures 1 and 2 and Algorithms 1 and 2 summarize the proposed matrix completion architectures.
We refer to the whole architecture combining the MGCNN and RNN in the full matrix completion
setting as recurrent multi-graph CNN (RMGCNN). The factorized version with separable MGCNN
and RNN is referred to as separable RMGCNN (sRMGCNN). The complexity of Algorithm 1 scales
quadratically as O(mn) due to the use of MGCNN. For large matrices  Algorithm 2 that processes
the rows and columns separately with standard GCNNs and scales linearly as O(m + n) is preferable.
We will demonstrate in Section 4 that the proposed RMGCNN and sRMGCNN architectures show
themselves very well on different settings of matrix completion problems. However  we should note
that this is just one possible conﬁguration  which we by no means claim to be optimal. For example 
in all our experiments we used only one convolutional layer; it is likely that better yet performance
could be achieved with more layers.
Algorithm 1 (RMGCNN)
input m × n matrix X(0) containing initial val-

Algorithm 2 (sRMGCNN)
input m × r factor H(0) and n × r factor W(0)

ues

1: for t = 0 : T do
2:

3:
4:

Apply the Multi-Graph CNN (13) on X(t)
producing an m × n × q output ˜X(t).
for all elements (i  j) do

Apply RNN to q-dim ˜x(t)
ij
(˜x(t)
ij1  . . .   ˜x(t)
update dx(t)
ij

=
ijq) producing incremental

end for
Update X(t+1) = X(t) + dX(t)

5:
6:
7: end for

representing the matrix X(0)

1: for t = 0 : T do
2:

3:
4:

Apply the Graph CNN on H(t) producing
an n × q output ˜H(t).
for j = 1 : n do

Apply RNN to q-dim ˜h(t)
j
(˜h(t)
j1   . . .   ˜h(t)
update dh(t)
j

=
jq ) producing incremental

end for
Update H(t+1) = H(t) + dH(t)
Repeat steps 2-6 for W(t+1)

5:
6:
7:
8: end for

3.3 Training
Training of the networks is performed by minimizing the loss

(cid:96)(Θ  σ) = (cid:107)X(T )

Θ σ(cid:107)2Gr

+ (cid:107)X(T )

Θ σ(cid:107)2Gc

+

(cid:107)Ω ◦ (X(T )

Θ σ − Y)(cid:107)2
F.

µ
2

(17)

Here  T denotes the number of diffusion iterations (applications of the RNN)  and we use the
notation X(T )
Θ σ to emphasize that the matrix depends on the parameters of the MGCNN (Chebyshev
polynomial coefﬁcients Θ) and those of the LSTM (denoted by σ). In the factorized setting  we use
the loss

(cid:96)(θr  θc  σ) = (cid:107)W(T )

θc σ(cid:107)2Gc
where θc  θr are the parameters of the two GCNNs.

+ (cid:107)H(T )

θr σ(cid:107)2Gr

+

µ
2

(cid:107)Ω ◦ (W(T )

θr σ(H(T )

θc σ)(cid:62) − Y)(cid:107)2

F

(18)

4 Results2
Experimental settings. We closely followed the experimental setup of [33]  using ﬁve standard
datasets: Synthetic dataset from [19]  MovieLens [29]  Flixster [18]  Douban [27]  and YahooMusic
[11]. We used disjoint training and test sets and the presented results are reported on test sets in all
our experiments. As in [33]  we evaluated MovieLens using only the ﬁrst of the 5 provided data splits.
For Flixster  Douban and YahooMusic  we evaluated on a reduced matrix of 3000 users and items 
considering 90% of the given scores as training set and the remaining as test set. Classical Matrix
Completion (MC) [9]  Inductive Matrix Completion (IMC) [17  42]  Geometric Matrix Completion
(GMC) [19]  and Graph Regularized Alternating Least Squares (GRALS) [33] were used as baseline
methods. In all the experiments  we used the following settings for our RMGCNNs: Chebyshev
polynomials of order p = 4  outputting k = 32-dimensional features  LSTM cells with 32 features
and T = 10 diffusion steps (for both training and test). The number of diffusion steps T has been
estimated on the Movielens validation set and used in all our experiments. A better estimate of T
can be done by cross-validation  and thus can potentially only improve the ﬁnal results. All the

2Code: https://github.com/fmonti/mgcnn

7

models were implemented in Google TensorFlow and trained using the Adam stochastic optimization
algorithm [20] with learning rate 10−3. In factorized models  ranks r = 15 and 10 was used for
the synthetic and real datasets  respectively. For all methods  hyperparameters were chosen by
cross-validation.

Figure 4: Absolute value |τ (˜λc  ˜λr)| of the ﬁrst
ten spectral ﬁlters learnt by our MGCNN model.
In each matrix  rows and columns represent
frequencies ˜λr and ˜λc of the row and column
graphs  respectively.

0.8

0.6

0.4

0.2

e
s
n
o
p
s
e
R

r
e
t
l
i

F

0

0

0.5

1

1.5

2

λr   λc

Figure 5: Absolute values |τ (˜λc)| and |τ (˜λr)|
of the ﬁrst four column (solid) and row (dashed)
spectral ﬁlters learned by our sMGCNN model.

4.1 Synthetic data
We start the experimental evaluation showing the performance of our approach on a small synthetic
dataset  in which the user and item graphs have strong communities structure. Though rather simple 
such a dataset allows to study the behavior of different algorithms in controlled settings.
The performance of different matrix completion methods is reported in Table 1  along with their
theoretical complexity. Our RMGCNN and sRMGCNN models achieve better accuracy than other
methods with lower complexity. Different diffusion time steps of these two models are visualized in
Figure 3. Figures 4 and 5 depict the spectral ﬁlters learnt by MGCNN and row- and column-GCNNs.
We repeated the same experiment assuming only the column (users) graph to be given. In this setting 
RMGCNN cannot be applied  while sRMGCNN has only one GCNN applied on the factor H (the
other factor W is free). Table 2 summarizes the results of this experiment  again  showing that our
approach performs the best.
Table 3 compares our RMGCNN with more classical multilayer MGCNNs. Our recurrent solutions
outperforms deeper and more complex architectures  requiring at the same time a lower amount of
parameters.

Table 1: Comparison of different matrix comple-
tion methods using users+items graphs in terms
of number of parameters (optimization variables)
and computational complexity order (operations
per iteration). Big-O notation is avoided for clar-
ity reasons. Rightmost column shows the RMS
error on Synthetic dataset.

Table 2: Comparison of different matrix comple-
tion methods using users graph only in terms of
number of parameters (optimization variables)
and computational complexity order (operations
per iteration). Big-O notation is avoided for clar-
ity reasons. Rightmost column shows the RMS
error on Synthetic dataset.

METHOD
GMC
GRALS
sRMGCNN
RMGCNN

PARAMS NO. OP.

mn

m + n

1
1

mn

m + n
m + n

mn

RMSE
0.3693
0.0114
0.0106
0.0053

METHOD
GRALS
sRMGCNN

PARAMS NO. OP.
m + n
m + n
m + n

m

RMSE
0.0452
0.0362

Table 3: Reconstruction errors for the synthetic dataset between multiple convolutional layers
architectures and the proposed architecture. Chebyshev polynomials of order 4 have been used for
both users and movies graphs (q(cid:48)MGCq denotes a multi-graph convolutional layer with q(cid:48) input
features and q output features).

Method
MGCNN3layers
MGCNN4layers
MGCNN5layers
MGCNN6layers
RMGCNN

Params

Architecture

1MGC32  32MGC10  10MGC1

9K
1MGC32  32MGC32 × 2  32MGC1
53K
1MGC32  32MGC32 × 3  32MGC1
78K
104K 1MGC32  32MGC32 × 4  32MGC1
9K

1MGC32 + LSTM

RMSE
0.0116
0.0073
0.0074
0.0064
0.0053

8

4.2 Real data

Following [33]  we evaluated the proposed approach on the MovieLens  Flixster  Douban and
YahooMusic datasets. For the MovieLens dataset we constructed the user and item (movie) graphs as
unweighted 10-nearest neighbor graphs in the space of user and movie features  respectively. For
Flixster  the user and item graphs were constructed from the scores of the original matrix. On this
dataset  we also performed an experiment using only the users graph. For the Douban dataset  we
used only the user graph (provided in the form of a social network). For the YahooMusic dataset 
we used only the item graph  constructed with unweighted 10-nearest neighbors in the space of
item features (artists  albums  and genres). For the latter three datasets  we used a sub-matrix of
3000 × 3000 entries for evaluating the performance. Tables 4 and 5 summarize the performance of
different methods. sRMGCNN outperforms the competitors in all the experiments.

Table 4: Performance (RMS error)
of different matrix completion meth-
ods on the MovieLens dataset.

METHOD
GLOBAL MEAN
USER MEAN
MOVIE MEAN
MC [9]
IMC [17  42]
GMC [19]
GRALS [33]
sRMGCNN

RMSE
1.154
1.063
1.033
0.973
1.653
0.996
0.945
0.929

Table 5: Performance (RMS error) on several datasets. For
Douban and YahooMusic  a single graph (of users and items
respectively) was used. For Flixster  two settings are shown:
users+items graphs / only users graph.

METHOD
GRALS
sRMGCNN

FLIXSTER

1.3126 / 1.2447
1.1788 / 0.9258

DOUBAN YAHOOMUSIC
0.8326
0.8012

38.0423
22.4149

5 Conclusions
In this paper  we presented a new deep learning approach for matrix completion based on multi-graph
convolutional neural network architecture. Among the key advantages of our approach compared to
traditional methods is its low computational complexity and constant number of degrees of freedom
independent of the matrix size. We showed that the use of deep learning for matrix completion allows
to beat related state-of-the-art recommender system methods. To our knowledge  our work is the ﬁrst
application of deep learning on graphs to this class of problems. We believe that it shows the potential
of the nascent ﬁeld of geometric deep learning on non-Euclidean domains  and will encourage future
works in this direction.

Acknowledgments
FM and MB are supported in part by ERC Starting Grant No. 307047 (COMET)  ERC Consolidator
Grant No. 724228 (LEMAN)  Google Faculty Research Award  Nvidia equipment grant  Radcliffe
fellowship from Harvard Institute for Advanced Study  and TU Munich Institute for Advanced Study 
funded by the German Excellence Initiative and the European Union Seventh Framework Programme
under grant agreement No. 291763. XB is supported in part by NRF Fellowship NRFF2017-10.

References
[1] K. Benzi  V. Kalofolias  X. Bresson  and P. Vandergheynst. Song recommendation with non-

negative matrix factorization and graph total variation. In Proc. ICASSP  2016.

[2] D. Boscaini  J. Masci  S. Melzi  M. M. Bronstein  U. Castellani  and P. Vandergheynst. Learning
class-speciﬁc descriptors for deformable shapes using localized spectral convolutional networks.
Computer Graphics Forum  34(5):13–23  2015.

[3] D. Boscaini  J. Masci  E. Rodolà  and M. M. Bronstein. Learning shape correspondence with

anisotropic convolutional neural networks. In Proc. NIPS  2016.

[4] D. Boscaini  J. Masci  E. Rodolà  M. M. Bronstein  and D. Cremers. Anisotropic diffusion

descriptors. Computer Graphics Forum  35(2):431–441  2016.

[5] J. Breese  D. Heckerman  and C. Kadie. Empirical Analysis of Predictive Algorithms for

Collaborative Filtering. In Proc. Uncertainty in Artiﬁcial Intelligence  1998.

9

[6] M. M. Bronstein  J. Bruna  Y. LeCun  A. Szlam  and P. Vandergheynst. Geometric deep learning:

going beyond euclidean data. IEEE Signal Processing Magazine  34(4):18–42  2017.

[7] J. Bruna  W. Zaremba  A. Szlam  and Y. LeCun. Spectral networks and locally connected

networks on graphs. 2013.

[8] E. Candès and B. Recht. Exact Matrix Completion via Convex Optimization. Foundations of

Computational Mathematics  9(6):717–772  2009.

[9] E. Candes and B. Recht. Exact matrix completion via convex optimization. Comm. ACM 

55(6):111–119  2012.

[10] M. Defferrard  X. Bresson  and P. Vandergheynst. Convolutional neural networks on graphs

with fast localized spectral ﬁltering. In Proc. NIPS  2016.

[11] G. Dror  N. Koenigstein  Y. Koren  and M. Weimer. The Yahoo! music dataset and KDD-Cup’11.

In KDD Cup  2012.

[12] D. K. Duvenaud et al. Convolutional networks on graphs for learning molecular ﬁngerprints. In

Proc. NIPS  2015.

[13] M. Gori  G. Monfardini  and F. Scarselli. A new model for learning in graph domains. In Proc.

IJCNN  2005.

[14] X. He  L. Liao  H. Zhang  L. Nie  X. Hu  and T. Chua. Neural collaborative ﬁltering. In Proc.

WWW  2017.

[15] M. Henaff  J. Bruna  and Y. LeCun. Deep convolutional networks on graph-structured data.

arXiv:1506.05163  2015.

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation  9(8):1735–

1780  1997.

[17] P. Jain and I. S. Dhillon. Provable inductive matrix completion. arXiv:1306.0626  2013.

[18] M. Jamali and M. Ester. A matrix factorization technique with trust propagation for recommen-

dation in social networks. In Proc. Recommender Systems  2010.

[19] V. Kalofolias  X. Bresson  M. M. Bronstein  and P. Vandergheynst. Matrix completion on

graphs. 2014.

[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. 2015.

[21] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

2017.

[22] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems.

Computer  42(8):30–37  2009.

[23] S. I. Ktena  S. Parisot  E. Ferrante  M. Rajchl  M. Lee  B. Glocker  and D. Rueckert. Distance
metric learning using graph convolutional networks: Application to functional brain networks.
In Proc. MICCAI  2017.

[24] D. Kuang  Z. Shi  S. Osher  and A. L. Bertozzi. A harmonic extension approach for collaborative

ranking. CoRR  abs/1602.05127  2016.

[25] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proc. IEEE  86(11):2278–2324  1998.

[26] Y. Li  D. Tarlow  M. Brockschmidt  and R. Zemel. Gated graph sequence neural networks.

2016.

[27] H. Ma  D. Zhou  C. Liu  M. Lyu  and I. King. Recommender systems with social regularization.

In Proc. Web Search and Data Mining  2011.

10

[28] J. Masci  D. Boscaini  M. M. Bronstein  and P. Vandergheynst. Geodesic convolutional neural

networks on Riemannian manifolds. In Proc. 3DRR  2015.

[29] B. N. Miller et al. MovieLens unplugged: experiences with an occasionally connected recom-

mender system. In Proc. Intelligent User Interfaces  2003.

[30] F. Monti  D. Boscaini  J. Masci  E. Rodolà  J. Svoboda  and M. M. Bronstein. Geometric deep

learning on graphs and manifolds using mixture model CNNs. In Proc. CVPR  2017.

[31] S. Parisot  S. I. Ktena  E. Ferrante  M. Lee  R. Guerrerro Moreno  B. Glocker  and D. Rueckert.
Spectral graph convolutions for population-based disease prediction. In Proc. MICCAI  2017.

[32] M. Pazzani and D. Billsus. Content-based Recommendation Systems. The Adaptive Web  pages

325–341  2007.

[33] N. Rao  H.-F. Yu  P. K. Ravikumar  and I. S. Dhillon. Collaborative ﬁltering with graph

information: Consistency and scalable methods. In Proc. NIPS  2015.

[34] F. Scarselli  M. Gori  A. C. Tsoi  M. Hagenbuchner  and G. Monfardini. The graph neural

network model. IEEE Trans. Neural Networks  20(1):61–80  2009.

[35] S. Sedhain  A. Menon  S. Sanner  and L. Xie. Autorec: Autoencoders meet collaborative

ﬁltering. In Proc. WWW  2015.

[36] Y. Seo  M. Defferrard  P. Vandergheynst  and X. Bresson. Structured sequence modeling with

graph convolutional recurrent networks. arXiv:1612.07659  2016.

[37] D. I. Shuman  S. K. Narang  P. Frossard  A. Ortega  and P. Vandergheynst. The emerging ﬁeld
of signal processing on graphs: Extending high-dimensional data analysis to networks and other
irregular domains. IEEE Sig. Proc. Magazine  30(3):83–98  2013.

[38] N. Srebro  J. Rennie  and T. Jaakkola. Maximum-Margin Matrix Factorization. In Proc. NIPS 

2004.

[39] Y. Suhara  X. Dong  and A. S. Pentland. Deepshop: Understanding purchase patterns via deep

learning. In Proc. International Conference on Computational Social Science  2016.

[40] S. Sukhbaatar  A. Szlam  and R. Fergus. Learning multiagent communication with backpropa-

gation. In Advances in Neural Information Processing Systems  pages 2244–2252  2016.

[41] M. Vestner  R. Litman  E. Rodolà  A. Bronstein  and D. Cremers. Product manifold ﬁlter:
Non-rigid shape correspondence via kernel density estimation in the product space. In Proc.
CVPR  2017.

[42] M. Xu  R. Jin  and Z.-H. Zhou. Speedup matrix completion with side information: Application

to multi-label learning. In Proc. NIPS  2013.

[43] F. Yanez and F. Bach. Primal-dual algorithms for non-negative matrix factorization with the
kullback-leibler divergence. In Acoustics  Speech and Signal Processing (ICASSP)  2017 IEEE
International Conference on  pages 2257–2261. IEEE  2017.

[44] Y. Zheng  B. Tang  W. Ding  and H. Zhou. A neural autoregressive approach to collaborative

ﬁltering. In Proc. ICML  2016.

11

,Federico Monti
Michael Bronstein
Xavier Bresson
Jalaj Upadhyay