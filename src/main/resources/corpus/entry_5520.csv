2018,The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network,An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface  very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work  we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature  namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity  the spectrum can be tuned so as to improve the efficiency of first-order optimization methods.,TheSpectrumoftheFisherInformationMatrixofaSingle-Hidden-LayerNeuralNetworkJeffreyPenningtonGoogleBrainjpennin@google.comPratikWorahGoogleResearchpworah@google.comAbstractAnimportantfactorcontributingtothesuccessofdeeplearninghasbeentheremarkableabilitytooptimizelargeneuralnetworksusingsimpleﬁrst-orderop-timizationalgorithmslikestochasticgradientdescent.Whiletheefﬁciencyofsuchmethodsdependscruciallyonthelocalcurvatureofthelosssurface verylittleisactuallyknownabouthowthisgeometrydependsonnetworkarchitectureandhyperparameters.Inthiswork weextendarecently-developedframeworkforstudyingspectraofnonlinearrandommatricestocharacterizeanimportantmeasureofcurvature namelytheeigenvaluesoftheFisherinformationmatrix.Wefocusonasingle-hidden-layerneuralnetworkwithGaussiandataandweightsandprovideanexactexpressionforthespectruminthelimitofinﬁnitewidth.Weﬁndthatlinearnetworkssufferworseconditioningthannonlinearnetworksandthatnonlinearnetworksaregenericallynon-degenerate.Wealsopredictanddemonstrateempiricallythatbyadjustingthenonlinearity thespectrumcanbetunedsoastoimprovetheefﬁciencyofﬁrst-orderoptimizationmethods.1IntroductionInrecentyears thesuccessofdeeplearninghasspreadfromclassicalproblemsinimagerecogni-tion[1] audiosynthesis[2] translation[3] andspeechrecognition[4]tomorediverseapplicationsinunexpectedareassuchasproteinstructureprediction[5] quantumchemistry[5]anddrugdiscov-ery[6].Theseempiricalsuccessescontinuetooutpacethedevelopmentofaconcretetheoreticalunderstandingofhowandinwhatcontextsdeeplearningworks.Acentraldifﬁcultyinanalyzingdeeplearningsystemsstemsfromthecomplexityofneuralnetworklosssurfaces whicharehighlynon-convexfunctions oftenofmillionsorevenbillions[7]ofparameters.Optimizationinsuchhigh-dimensionalspacesposesmanychallenges.Formostproblemsindeeplearning second-ordermethodsaretoocostlytoperformexactly.Despiterecentdevelopmentsonefﬁcientapproximationsofthesemethods suchastheNeumannoptimizer[8]andK-FAC[9] mostpractitionersusegradientdescentanditsvariants[10] [11].Despitetheirwidespreaduse itisnotobviouswhyﬁrst-ordermethodsareoftensuccessfulindeeplearningsinceitisknownthatﬁrst-ordermethodsperformpoorlyinthepresenceofpathologicalcurvature.Animportantopenquestioninthisdirectionistowhatextentpathologicalcurvaturepervadesdeeplearningandhowitcanbemitigated.Morebroadly inordertocontinueimprovingneuralnetworkmodelsandperformance weaimtobetterunderstandtheconditionsunderwhichﬁrst-ordermethodswillworkwell andhowthoseconditionsdependonmodeldesignchoicesandhyperparameters.Amongthevarietyofobjectsthatmaybeusedtoquantifythegeometryofthelosssurface twomatriceshaveelevatedimportance:theHessianmatrixandtheFisherinformationmatrix.FromtheperspectiveofEuclideancoordinatespace theHessianmatrixisthenaturalobjectwithwhichtoquantifythelocalgeometryofthelosssurface.Itisalsothefundamentalobjectunderlyingmanysecond-orderoptimizationschemesanditsspectrumprovidesinsightsastothenatureofcritical32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018) Montréal Canada.points.Fromtheperspectiveofinformationgeometry distancesaremeasuredinmodelspaceratherthanincoordinatespace andtheFisherinformationmatrixdeﬁnesthemetricanddeterminestheupdatedirectionsinnaturalgradientdescent[12].Incontrasttothestandardgradient thenaturalgradientdeﬁnesthedirectionintheparameterspacewhichgivesthelargestchangeintheobjectiveperunitchangeinthemodel asmeasuredbyKullback-Leiblerdivergence.AswediscussinSection2 theHessianandtheFisherarerelated;forthesquarederrorlossfunctionsthatweconsiderinthiswork itturnsoutthattheFisherequalstheGauss-NewtonapproximationoftheHessian sotheconnectionisconcrete.Acentraldifﬁcultyinbuildinguparobustunderstandingofthepropertiesofthesecurvaturematricesstemsfromthefactthattheyarehigh-dimensionalandtheempiricalestimationoftheirspectraislimitedbymemoryandcomputationalconstraints.Theselimitationstypicallypreventdirectcalculationsformodelswithmorethanafewtensofthousandsofparametersanditisdifﬁculttoknowwhetherconclusionsdrawnfromsuchsmallmodelswouldgeneralizetothemega-orgiga-dimensionalnetworksusedinpractice.Itisthereforeimportanttodeveloptheoreticaltoolstoanalyzethespectraofthesematrices.Ingeneral thespectrawilldependinintimatewaysonthespeciﬁcparametervaluesoftheweightsandthedistributionofinputdatatothenetwork.Itisnotfeasibletopreciselycaptureallofthesedetails andevenifatheoryweredevelopedthatdidso itwouldnotbeclearhowtoderivegeneralizableconclusionsfromit.Wethereforefocusonasimpliﬁedconﬁgurationinwhichtheweightsandinputsaretakentoberandomvariables.Theanalysisthenbecomesawell-deﬁnedcomputationinrandommatrixtheory.TheFisherisanonlinearfunctionoftheweightsanddata.Tocomputeitsspectrum weextendtheframeworkdevelopedbyPenningtonandWorah[13]tostudyrandommatriceswithnonlineardependencies.AswedescribeinSection2.4 theFisheralsohasaninternalblockstructurethatcomplicatestheresultingcombinatorialanalysis.Themaintechnicalcontributionofthisworkistoextendthenonlinearrandommatrixtheoryof[13]tomatriceswithnontrivialinternalstructure.TheresultofouranalysisisanexplicitcharacterizationofthespectrumoftheFisherinformationmatrixofasingle-hidden-layerneuralnetworkwithsquaredloss randomGaussianweightsandrandomGaussianinputdatainthelimitoflargewidth.Wedrawseveralnontrivialandpotentiallysurprisingconclusionsaboutthespectrum.Forexample linearnetworkssufferworseconditioningthananynonlinearnetwork andalthoughnonlinearnetworksmayhavemanysmalleigenvaluestheyaregenericallynon-degenerate.Ourresultsalsosuggestprecisewaystotunethenonlinearityinordertoimproveconditioningofthespectrum andourempiricalsimulationsshowimprovementsinthespeedofﬁrst-orderoptimizationasaresult.2Preliminaries2.1NotationandproblemstatementConsiderasingle-hidden-layerneuralnetworkwithweightmatricesW(1) W(2)∈Rn×nandpointwiseactivationfunctionf:R→R.ForinputX∈Rn theoutputofthenetworkˆY(X)∈RnisgivenbyˆY(X)=W(2)f(W(1)X).Forconcreteness wefocusouranalysisonthecaseofsquaredloss inwhichcase L(θ)=EX Y12kY−ˆY(X)k22 (1)whereY∈Rnaretheregressiontargetsandθdenotesthevectorofallparameters{W(1) W(2)}.ThematrixofsecondderivativesorHessianofthelosswithrespecttotheparameterscanbewrittenas H=H(0)+H(1) (2)where H(0)ij=EXXα∂ˆYα∂θi∂ˆYα∂θj andH(1)ij=EXXα(ˆY(X)−Y)α∂2ˆYα∂θi∂θj.(3)Inthisworkwefocusonthepositive-semi-deﬁnitematrixH(0) whichisknownastheGauss-Newtonmatrix.ItcanalsobewrittenasH(0)=JTJ whereJ∈Rn×2n2istheJacobianmatrixofˆYwith2respecttotheparametersθ.Formodelswithsquaredloss itisknownthattheGauss-NewtonmatrixisequaltotheFisherinformationmatrixofthemodeldistributionwithrespecttoitsparameters[14].Assuch bystudyingH(0)wesimultaneouslyexaminetheGauss-NewtonmatrixandtheFisherinformationmatrix.ThedistributionofeigenvaluesorspectrumofcurvaturematriceslikeH(0)playsanimportantroleinoptimization asitcharacterizesthelocalgeometryofthelosssurfaceandtheefﬁciencyofﬁrst-orderoptimizationmethods.Inthiswork weseektobuildadetailedunderstandingofthisspectrumandhowthearchitecturalcomponentsoftheneuralnetworkinﬂuenceit.Inordertoisolatethesefactorsfromidiosyncraticbehaviorrelatedtothespeciﬁcsofthedataandweightconﬁgurations wefocusontheavanillabaselineconﬁgurationinwhichthedataandtheweightsarebothtakentobeiidGaussianrandomvariables.Concretely wetakeX∼N(0 In) W(l)ij∼N(0 1n) andwewillbeinterestedincomputingtheexpecteddistributionofeigenvaluesH(0)forlargen.Fromthisperspective theproblemcanbeframedasacomputationinrandommatrixtheory theprinciplesbehindwhichwenowreview.2.2SpectraldensityandtheStieltjestransformTheempiricalspectraldensityofamatrixMisdeﬁnedas ρM(λ)=1nnXj=1δ(λ−λj(M)) (4)wheretheλj(M) j=1 ... n denotetheneigenvaluesofM includingmultiplicity andδistheDiracdeltafunction.Thelimitingspectraldensityisthelimitofeqn.(4)asn→∞ ifitexists.Forz∈C\supp(ρM)theStieltjestransformGofρMisdeﬁnedas G(z)=ZρM(t)z−tdt=−1nEtr(M−zIn)−1 (5)wheretheexpectationiswithrespecttotherandomvariablesWandX.Thequantity(M−zIn1)−1istheresolventofM.ThespectraldensitycanberecoveredfromtheStieltjestransformusingtheinversionformula ρM(λ)=−1πlim→0+ImG(λ+i).(6)2.3MomentmethodOneofthemaintoolsforcomputingthelimitingspectraldistributionsofrandommatricesisthemomentmethod which asthenamesuggests isbasedoncomputationsofthemomentsofρM.Theasymptoticexpansionofeqn.(5)forlargezgivestheLaurentseries G(z)=∞Xk=0mkzk+1 (7)wheremkisthekthmomentofthedistributionρM mk=ZdtρM(t)tk=1nEtrMk.(8)Ifonecancomputemk thenthedensityρMcanbeobtainedviaeqns.(7)and(6).TheideabehindthemomentmethodistocomputemkbyexpandingoutpowersofMinsidethetraceas 1nEtrMk=1nEXi1 ... ik∈[n]Mi1i2Mi2i3···Mik−1ikMiki1 (9)andevaluatingtheleadingcontributionstothesumasn→∞.WewillusethemomentmethodinordertocomputethelimitingspectraldensityoftheFisher.Asaﬁrststepinthatdirection wefocusonthepropertiesofthelayer-wiseblockstructureintheFisherinducedbytheneuralnetworkarchitecture.32.4BlockstructureoftheFisherAsdescribedabove inoursingle-hidden-layersettingwithsquaredloss theFisherisgivenbyH(0)=EX(cid:2)JTJ(cid:3) Jαi=∂ˆYα∂θi.(10)Becausetheparametersofthemodelareorganizedintotwolayers itisconvenienttopartitiontheFisherintoa2×2blockmatrix H(0)= H(0)11H(0)12H(0)12TH(0)22!.Furthermore becausetheparametersofeachlayerarematrices itisusefultoregardeachblockoftheFisherasafour-indextensor.Inparticular [H(0)11]a1b1 a2b2=EX"XiJ(1)i a1b1J(1)i a2b2# [H(0)12]a1b1 c1d1=EX"XiJ(1)i a1b1J(2)i c1d1# [H(0)22]c1d1 c2d2=EX"XiJ(2)i c1d1J(2)i c2d2#.TheJacobianentriesJ(l)i abequalthederivativesofˆYiwithrespecttotheweightvariablesW(l)ab J(1)i ab=W(2)iaf0(cid:0)XkW(1)akXk(cid:1)Xb J(2)j cd=δcjf(cid:0)XlW(1)dlXl(cid:1) (11)whereδcjdenotestheKroneckerdeltafunctioni.e. itis1ifc=j and0otherwise.Inordertoproceedbythemethodofmoments wewillneedtocomputethenormalizedtraceofpowersoftheFisher i.e.1ntr[H(0)]d foranyd.TheblockstructureoftheFishermakestheexplicitrepresentationofthesetracessomewhatcomplicated.Thefollowingpropositionhelpssimplifytheexpressions.Proposition1.LetA1∈Rn×k1 A2∈Rn×k2andA=[A1 A2]∈Rn×(k1+k2).Then tr[(ATA)d]=Xb∈{1 2}dtrdYi=1AbiATbi=Xb∈{1 2}dtrATbdAb1d−1Yi=1ATbiAbi+1.(12)UsingProposition1withA1=J(1)andA2=J(2) wehave tr[(H(0))d]=Xb∈{1 2}dtrEX(cid:2)J(bd)TJ(b1)(cid:3)d−1Yi=1EX(cid:2)J(bi)TJ(bi+1)(cid:3) (13)whichexpressesthetracesoftheblockFisherentirelyintermsofproductsofitsconstituentblocks.Inordertocarryoutthemomentmethodtocompletion weneedtheexpectednormalizedtracesmk mk=1nEWtr[(H(0))k] (14)inthelimitoflargen.Becausethenonlinearitysigniﬁcantlycomplicatestheanalysis weﬁrstillustratethebasicsofthemethodologyinthelinearcasebeforemovingontothegeneralcase.2.5AnIllustrativeExample:TheLinearCaseLetusassumethatfistheidentityfunctioni.e. f(z)=z.Inthiscase eqn.(11)canbewrittenas J(1)=W(2)T⊗X J(2)=I⊗W(1)X.(15)4(cid:1)((cid:2))0246810-40.0010.0100.100110(cid:2)(cid:1)((cid:2))24680.050.100.150.200.25(cid:2)(a)f(x)=x(cid:1)((cid:2))0246810-40.0010.0100.100110(cid:2)(cid:1)((cid:2))24680.51.01.52.02.53.0(cid:2)(b)f(x)=erf1(x)Figure1:EmpiricalspectraofFisherforsingle-hidden-layernetworksofwidth128(orange)andtheoreticalpredictionofspectra(black)for(a)linearand(b)erf1(seeeqn.30)networks.Insetsshowlogarithmicscale.UsingthefactthatEX[XXT]=In eqn.(13)gives tr[(H(0))d]=EWdXk=0(cid:18)dk(cid:19)tr(W(2)W(2)T)d−ktr(W(1)W(1)T)k=dXk=0(cid:18)dk(cid:19)Cd−kCk (16)whereCnisthenthCatalannumber.TheseriescanbesummedtoobtaintheStieltjestransform whoseimaginarypartgivesthefollowingexplicitformforthespectrum ρ(λ)=12δ(λ)+h12π2E(cid:16)116(8−λ)λ(cid:17)+4−λ8π2K(cid:16)116(8−λ)λ(cid:17)i1[0 8] (17)whereKandEarethecompleteellipticintegralsoftheﬁrst-andsecond-kind K(k)=Zπ20dθ1p1−ksin2θ E(k)=Zπ20dθp1−ksin2θ.(18)Noticethatthespectrumishighlydegenerate withhalfoftheeigenvaluesequalingzero.Thisdegen-eracycanbeattributedtotheGL(n2)symmetryoftheproductW(2)W(1)under{W(1) W(2)}→{GW(1) W(2)G−1}.Fig.1ashowsexcellentagreementbetweenthepredictedspectraldensityandﬁnite-widthempiricalsimulations.3TheStieltjestransformofH(0)3.1MainResultIff:R→RisanactivationfunctionwithzeroGaussianmeanandﬁniteGaussianmoments Zdx√2πe−x22f(x)=0 (cid:12)(cid:12)(cid:12)(cid:12)Zdx√2πe−x22f(x)k(cid:12)(cid:12)(cid:12)(cid:12)<∞ fork>1 (19)thentheStieltjestransformofthelimitingspectraldensityofH(0)isgivenbythefollowingtheorem.Theorem1.TheStieltjestransformofthespectraldensityoftheFisherinformationmatrixofasingle-hidden-layerneuralnetworkwithsquaredloss activationfunctionf weightmatricesW(1) W(2)∈Rn×nwithiidentriesW(l)ij∼N(0 1n) nobiases andiidinputsX∼N(0 In)isgivenbythefollowingintegralasn→∞:G(z)=ZRZRλ1+λ2−2z2ζ2(cid:0)(η−ζ)(η0−ζ)+λ1(z−η+ζ)+λ2(z−η0+ζ)−z2(cid:1)dµ1(λ1)dµ2(λ2) (20)wheretheconstantsη η0 andζaredeterminedbythenonlinearity η=ZRf(x)2e−x2/2√2πdx η0=ZRf0(x)2e−x2/2√2πdx ζ= ZRf0(x)e−x2/2√2πdx!2 (21)5ρ(λ)Nonlinearityxerf1(x)slrelu0(x)fopt(x)0.100.5015100.0010.0100.100110λ(a)Spectraforvariousnonlinearitiesρ(λ)Width163264128∞0.00.10.20.30.40.501234λ(b)f(x)=erf1(x)forvariouswidthsFigure2:(a)Theoreticalpredictionsforspectraofvariousnonlinearities;seeeqns.(28)and(30).Thelinearcaseisdegenerateandmorepoorlyconditionedthanthenonlinearcases.(b)Theoreticalpredictionofspectrumforerf1comparedwithempiricalsimulations.Practicalconstraintsrestrictthewidthtosmallvalues butslowconvergencetowardtheasymptoticpredictioncanbeobserved.andthemeasuresdµ1anddµ2aregivenby dµ1(λ1)=12πsη0+3ζ−λ1λ1−η0+ζ1[η0−ζ η0+3ζ] dµ2(λ2)=12πsη+3ζ−λ2λ2−η+ζ1[η−ζ η+3ζ].(22)Remark1.AstraightforwardapplicationofCarlson’salgorithm[15]canreducetheintegralineqn.(20)toacombinationofthreestandardellipticintegrals.Remark2.Thespectraldensitycanberecoveredfromeqn.(20)throughtheinversionformula eqn.(6).Remark3.AlthoughtheresultinTheorem1iswrittenintermsoff0 itisnotnecessarythatfbedifferentiable.Infact theweakderivativecanbeusedinplaceofthederivative astheproofofthereduction(seealso[13])toﬁnalformusesintegrationbypartsonly.Therefore justtheexistenceofaweakderivativeforfsufﬁces.Inparticular theresultwouldholdfor|x|andRelufunctions.TheproofofTheorem1isquitelongandtechnical soit’sdeferredtotheSupplementaryMaterial.Thebasicideaunderlyingtheproofisverysimilartothatutilizedin[13].Thecalculationofthemomentsisdividedintotwosub-problems oneofenumeratingcertainconnectedouter-planargraphs andanotherofevaluatingcertainhigh-dimensionalintegralsthatcorrespondtowalksinthosegraphs.Fig.1showstheexcellentagreementofthepredictedspectrumwithempiricalsimulationsofﬁnite-widthnetworks.Fig.2highlightstheregionofthespectrumforwhichtheasymptoticbehaviorisslowtosetinandsuggeststhatempiricalsimulationswithsmallnetworksmaynotprovideanaccurateportrayalofthebehavioroflargenetworks.Fig.2ashowsthepredictedspectraforavarietyofnonlinearities.3.2FeaturesofthespectrumOwingtoeqn.(6) thebranchpointsandpolesofG(z)encodeinformationaboutthedeltafunctionpeaks spectraledges anddiscontinuitiesinthederivativeofρ(λ).ThesespecialpointscanbedetermineddirectlyfromtheintegralrepresentationforG(z)ineqn.(20)byexaminingthezerosofthedenominatoroftheintegrand.Inparticular thefollowingsixvaluesofzarelocationsofthepolesattheintegrationendpointsanddeterminethesalientfeaturesofthespectraldensity:z1=η−ζ z2=η+3ζ z3=12(cid:0)η+η0+6ζ−p(η0−η)2+64ζ2(cid:1) (23)z4=η0−ζ z5=η0+3ζ z6=12(cid:0)η+η0+6ζ+p(η0−η)2+64ζ2(cid:1).(24)IntheSupplementaryMaterial weestablishtherelativeorderingofconstants0≤ζ≤η≤η0 whichimpliesthattheminimumandmaximumeigenvaluesofH(0)aregivenby λmin=z1 andλmax=z6.(25)6Table1:PropertiesofnonlinearitiesLocationsofspectralfeaturesηη0ζz1z2z3z4z5z6x111040048erf1(x)11.2260.9140.0863.7410.1980.3123.9667.51srelu0(x)11.4670.7330.2673.2000.4910.7333.6676.377fopt11.9230.0770.9231.2311.1381.8462.1542.247TheSupplementaryMaterialalsoshowsthattheequalityη=ζonlyholdsforlinearnetworks whichimpliesthattheminimumeigenvalueisnonzeroforeverynonlinearactivationfunction.Therearetwodeltafunctionpeaksinspectrum whicharelocatedat λ(1)peak=λmin=z1 andλ(2)peak=z4.(26)Thesepeaksindicatespeciﬁceigenvaluesthathavenonvanishingprobabilityofoccurrence.Thesepeakscoalescewhenη=η0 whichcanonlyhappenforlinearactivationfunctions inwhichcaseη=η0=ζ sothepeaksoccuratλ=0 asillustratedinFig.2a.Thatﬁgurealsoshowsthatthespectrummayconsistoftwodisconnectedcomponents inwhichcasez2isthelocationoftherightedgeoftheleftcomponent.Finally thederivativeofthespectrumisdiscontinuousatz3andz5.ThesepredictionscanbeveriﬁedinFig.2abyconsultingTable1 whichprovidesnumericalvaluesforthesespecialpointsforthevariousnonlinearitiesappearingintheﬁgure.4Empiricalanalysis4.1AmeasureofconditioningUsingtheresultsfromSection4.1 theﬁrsttwomomentscanbegivenexplicitlyas m1=limn→∞1ntr[H(0)]=12(η+η0)m2=limn→∞1ntr[H(0)2]=12(η2+η02+4ζ2)(27)Ascale-invariantmeasureofconditioningoftheFisherisjustm2/m21 whichislower-boundedby1 andwhichquantiﬁeshowtightlyconcentratedthespectrumisarounditsmean.Ideally thisquantityshouldbeassmallaspossibletoavoidpathologicalcurvatureandtoenablefastﬁrst-orderoptimization.Oneadvantageofm2/m21comparedtootherconditionnumberssuchasλmax/λminorλmaxisthatitisscale-invariantandwell-deﬁnedeveninthepresenceofdegeneracyinthespectrum.ByexpandingfinabasisofHermitepolynomials weshowintheSupplementaryMaterialthatamongthefunctionswithzeroGaussianmeanthatfopt(x)=1√13(cid:0)x+√6(x2−1)(cid:1)(28)minimizestheratiom2/m21.Notethatwehaveremovedthefreedomtorescalefoptbyaconstantbyenforcingη=1.Curiously alinearactivationfunctionactuallymaximizestheratio implyingthatnonlinearityinvariablyimprovesconditioning atleastbythismeasure.TherelativeconditioningofspectraresultingfromvariousactivationfunctionscanbeobservedinFig.2a.Thefunctionfopt(x)growsquicklyforlarge|x|andmaybetoounstabletouseinactualneuralnetworks.Alternativefunctionscouldbefoundbysolvingtheoptimizationproblem f∗=argminfm2m21 (29)subjecttosomeconstraints forexamplethatfbemonotoneincreasing havezeroGaussianmean andsaturateforlarge|x|.Suchaproblemcouldbesolvedviavariationalcalculus;seetheSupplementaryMaterial.7m2m120.00.51.01.52.02.51.181.201.221.241.261.28-0.0132-0.0130-0.0128-0.0126-0.0124-0.0122αΔL(a)sreluαm2m121.01.52.02.53.03.54.01.51.61.71.81.9-0.010-0.009-0.008-0.007αΔL(b)erfαFigure3:Comparisonoftheconditioningmeasurem2/m21andsingle-steplossreduction∆L(eqn.(33))astheactivationfunctionchangesfor(a)sreluαand(b)erfα(eqn.(30)).Thecurvesarehighlycorrelated suggestingthepossibilityofimprovedﬁrst-orderoptimizationperformancebytuningthespectrumoftheFisherthroughthechoiceofactivationfunction.4.2EfﬁciencyofgradientdescentAnotherwaytoinvestigatetheratiom2/m21istoseehowwellitcorrelateswiththeefﬁciencyofﬁrst-orderoptimization.Forthispurpose weexaminetwoone-parameterclassesofwell-behavedactivationfunctionsrelatedtoReLUandtheerrorfunction sreluα(x)=[x]++α[−x]+−1+α√2πq12(1+α2)−12π(1+α)2 erfα(x)=erf(α2x)q4πtan−1√1+4α4−1.(30)HeresreluαistheshiftedleakyReLUfunctionstudiedin[13].BothsreluαanderfαhavezeroGaussianmeanandarenormalizedsuchthatη=1forallα.Changingαdoesaffectη0 ζandtheratiom2/m21 whichimpliesthatdifferentfunctionswithintheseone-parameterfamiliesmaybehavequitedifferentlyundergradientdescent.Wedesignedasimpleandcontrolledexperimenttoexplorethesedifferencesinthecontextofneuralnetworktraining.Thesetupisamodiﬁedstudent-teacherframework inwhichthestudentisinitializedwiththeteacher’sparameters buttheregressiontargetsareperturbedsothatstudent’sparametersaresuboptimal.Thenweaskbyhowmuchcanthestudentdecreasethelossbyoneoptimally-chosenstepinthegradientdirection.Concretely wedeﬁneYi=W(2)tf(W(1)tXi)+i i=1 ... M (31)forteacherweights[W(l)t]ij∼N(0 1n) Xi∼N(0 In) andi∼N(0 ε2In) withwidthn=27 numberofsamplesM=217 andperturbationsizeε=10−3.Thelossisdeﬁnedas L(Ws)=MXi=112kYi−W(2)sf(W(1)sXi)k22.(32)Weareinterestedinthemaximalsingle-steplossdecreasewhenWsisinitializedatWt i.e. ∆L=minη(cid:2)L(cid:0)Wt−η∇L|Wt(cid:1)−L(Wt)(cid:3).(33)Forthetwoclassesofactivationfunctionsineqn.(30) weempiricallymeasured∆Lasafunctionofα.InFig.3wecomparetheresultswithourtheoreticalpredictionsform2/m21asafunctionofα.Theagreementisexcellent suggestingthatourtheorymaybeabletomakepracticalpredictionsregardingtrainingefﬁciencyofactualneuralnetworks.5ConclusionsInthiswork wecomputedthespectrumoftheFisherinformationmatrixofasingle-hidden-layerneuralnetworkwithsquaredlossandGaussianweightsandGaussiandatainthelimitoflargenetworkwidth.Ourexplicitresultsindicatethatlinearnetworkssufferworseconditioningthan8nonlinearnetworksandthatalthoughnonlinearnetworksmayhavenumeroussmalleigenvaluestheyaregenericallynon-degenerate.Wealsoshowedthatbytuningthenonlinearityitispossibletoadjustthespectruminsuchawaythattheefﬁciencyofﬁrst-orderoptimizationmethodscanbeimproved.Byundertakingthisanalysis wedemonstratedhowtoextendthetechniquesdevelopedin[13]forstudyingrandommatriceswithnonlineardependenciestotheblock-structuredcurvaturematricesthatarerelevantforoptimizationindeeplearning.Thetechniquespresentedherepavethewayforfutureworkstudyingdeeplearningviarandommatrixtheory.References[1]AlexKrizhevsky IlyaSutskever andGeoffreyEHinton.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinneuralinformationprocessingsystems pages1097–1105 2012.[2]AaronvandenOord SanderDieleman HeigaZen KarenSimonyan OriolVinyals AlexGraves NalKalchbrenner AndrewSenior andKorayKavukcuoglu.Wavenet:Agenerativemodelforrawaudio.arXivpreprintarXiv:1609.03499 2016.[3]YonghuiWu MikeSchuster ZhifengChen QuocV.Le MohammadNorouzi WolfgangMacherey MaximKrikun YuanCao QinGao KlausMacherey etal.Google’sneuralmachinetranslationsystem:Bridgingthegapbetweenhumanandmachinetranslation.arXivpreprintarXiv:1609.08144 2016.[4]GeoffreyHinton LiDeng DongYu GeorgeE.Dahl Abdel-rahmanMohamed NavdeepJaitly AndrewSenior VincentVanhoucke PatrickNguyen TaraNSainath etal.Deepneuralnetworksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearchgroups.IEEESignalProcessingMagazine 29(6):82–97 2012.[5]GarrettGoh NathanHodas andAbhinavVishnu.DeepLearningforComputationalChemistry.arXivpreprintarXiv:1701.04503 2017.[6]HanAltae-Tran BharathRamsundar AneeshS.Pappu andVijayPande.LowDataDrugDiscoverywithOne-ShotLearning.AmericanChemicalSocietyCentralScience 2017.[7]N.Shazeer A.Mirhoseini K.Maziarz A.Davis Q.Le G.Hinton andJ.Dean.Outrageouslylargeneurallanguagemodelsusingsparselygatedmixturesofexperts.ICLR 2017.URLhttp://arxiv.org/abs/1701.06538.[8]ShankarKrishnan YingXiao andRifA.Saurous.Neumannoptimizer:Apracticaloptimizationalgorithmfordeepneuralnetworks.InInternationalConferenceonLearningRepresentations 2018.[9]RogerB.GrosseandJamesMartens.Akronecker-factoredapproximateﬁshermatrixforconvolutionlayers.InProceedingsofthe33ndInternationalConferenceonMachineLearning ICML pages573–582 2016.[10]JohnDuchi EladHazan andYoramSinger.AdaptiveSubgradientMethodsforOnlineLearningandStochasticOptimization.JournalofMachineLearningResearch 2011.[11]DiedrikKingmaandJimmyBa.Adam:AMethodforStochasticOptimization.arxiv:1412.6980 2014.[12]S.I.Amari.Naturalgradientworksefﬁcientlyinlearning.NeuralComputation 1998.[13]JeffreyPenningtonandPratikWorah.Nonlinearrandommatrixtheoryfordeeplearning.InAdvancesinNeuralInformationProcessingSystems pages2634–2643 2017.[14]TomHeskes.On“natural”learningandpruninginmultilayeredperceptrons.NeuralComputa-tion 12(4):881–901 2000.[15]BCCarlson.Atableofellipticintegralsofthethirdkind.Mathematicsofcomputation 51(183):267–280 1988.[16]MarianoGiaquintaandStefanHilderbrandt.CalculusofVariations1.Springer 1994.9[17]RichardStanley.PolygonDissectionsandStandardYoungTableaux.JournalofCombinatorialTheory SeriesA 1996.10,Chengtao Li
Stefanie Jegelka
Suvrit Sra
Jeffrey Pennington
Pratik Worah