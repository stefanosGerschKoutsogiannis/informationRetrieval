2019,Cross Attention Network for Few-shot Classification,Few-shot classification aims to recognize unlabeled samples from unseen classes given only few labeled samples. The unseen classes and low-data problem make few-shot classification very challenging. Many existing approaches extracted features from labeled and unlabeled samples independently  as a result  the features are not discriminative enough. In this work  we propose a novel Cross Attention Network to address the challenging problems in few-shot classification. Firstly  Cross Attention Module is introduced to deal with the problem of unseen classes.  The module generates cross attention maps for each pair of class feature and query sample feature so as to highlight the target object regions  making the extracted feature more discriminative. Secondly  a transductive inference algorithm is proposed to alleviate the low-data problem  which iteratively utilizes the unlabeled query set to augment the support set  thereby making the class features more representative. Extensive experiments on two benchmarks show our method is a simple  effective and computationally efficient framework and outperforms the state-of-the-arts.,Cross Attention Network for Few-shot Classiﬁcation

Ruibing Hou1 2  Hong Chang1 2  Bingpeng Ma2  Shiguang Shan1 2 3  Xilin Chen1 2

1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS) 

Institute of Computing Technology  CAS  China

2University of Chinese Academy of Sciences  China

3CAS Center for Excellence in Brain Science and Intelligence Technology  China

ruibing.hou@vipl.ict.ac.cn  {changhong  sgshan xlchen}@ict.ac.cn  bpma@ucas.ac.cn

Abstract

Few-shot classiﬁcation aims to recognize unlabeled samples from unseen classes
given only few labeled samples. The unseen classes and low-data problem make
few-shot classiﬁcation very challenging. Many existing approaches extracted fea-
tures from labeled and unlabeled samples independently  as a result  the features
are not discriminative enough. In this work  we propose a novel Cross Attention
Network to address the challenging problems in few-shot classiﬁcation. Firstly 
Cross Attention Module is introduced to deal with the problem of unseen classes.
The module generates cross attention maps for each pair of class feature and query
sample feature so as to highlight the target object regions  making the extracted fea-
ture more discriminative. Secondly  a transductive inference algorithm is proposed
to alleviate the low-data problem  which iteratively utilizes the unlabeled query set
to augment the support set  thereby making the class features more representative.
Extensive experiments on two benchmarks show our method is a simple  effective
and computationally efﬁcient framework and outperforms the state-of-the-arts.

1

Introduction

Few-shot classiﬁcation aims at classifying unlabeled samples (query set) into unseen classes given
very few labeled samples (support set). Compared to traditional classiﬁcation  few-shot classiﬁcation
has two main challenges: One is unseen classes  i.e.  the non-overlap between training and test
classes; The other is the low-data problem  i.e.  very few labeled samples for the test unseen classes.
Solving few-shot classiﬁcation problem requires the model trained with seen classes to generalize
well to unseen classes with only few labeled samples. A straightforward approach is ﬁne-tuning a pre-
trained model using the few labeled samples from the unseen classes. However  it may cause severe
overﬁtting. Regularization and data augmentation can alleviate but cannot fully solve the overﬁtting
problem. Recently  meta-learning paradigm [38  39  22] is widely used for few-shot learning. In
meta-learning  the transferable meta-knowledge  which can be an optimization strategy [31  1]  a
good initial condition [7  16  24]  or a metric space [35  40  37]  is extracted from a set of training
tasks and generalizes to new test tasks. The tasks in the training phase usually mimic the settings in
the test phase to reduce the gap between training and test settings and enhance the generalization
ability of the model.
While promising  few of them pay enough attention to the discriminability of the extracted features.
They generally extract features from the support classes and unlabeled query samples independently 
as a result  the features are not discriminative enough. For one thing  the test images in the
support/query set are from unseen classes  thus their features can hardly attend to the target
objects. To be speciﬁc  for a test image containing multiple objects  the extracted feature may attend
to the objects from seen classes which have large number of labeled samples in the training set 
while ignore the target object from unseen class. As illustrated in Fig. 1 (c) and (d)  for the two

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1. An example of the class activation maps [48] of training and test images of existing method [35] and
our method CAN. Warmer color with higher value.
images from the test class curtain  the extracted features only capture the information of the objects
that are related to the training classes  such as person or chair in Fig. 1 (a) and (b). For another 
the low-data problem makes the feature of each test class not representative for the true class
distribution  as it is obtained from very few labeled support samples. In a word  the independent
feature representations may fail in few-shot classiﬁcation.
In this work  we propose a novel Cross Attention Network (CAN) to enhance the feature discrim-
inability for few-shot classiﬁcation. Firstly  Cross Attention Module (CAM) is introduced to deal
with the unseen class problem. The cross attention idea is inspired by the human few-shot learning
behavior. To recognize a sample from unseen class given a few labeled samples  human tends to
ﬁrstly locate the most relevant regions in the pair of labeled and unlabeled samples. Similarly  given
a class feature map and a query sample feature map  CAM generates a cross attention map for each
feature to highlight the target object. Correlation estimation and meta fusion are adopted to achieve
this purpose. In this way  the target object in the test samples can get attention and the features
weighted by the cross attention maps are more discriminative. As shown in Fig. 1 (e)  the extracted
features with CAM can roughly localize the regions of target object curtain. Secondly  we introduce
a transductive inference algorithm that utilizes the entire unlabeled query set to alleviate the low-data
problem. The proposed algorithm iteratively predicts the labels for the query samples  and selects
pseudo-labeled query samples to augment the support set. With more support samples per class  the
obtained class features can be more representative  thus alleviating the low-data problem.
Experiments are conducted on multiple benchmark datasets to compare the proposed CAN with
existing few-shot meta-learning approaches. Our method achieves new state-of-the art results on all
dataset  which demonstrates the effectiveness of CAN.

2 Related Work

Few-Shot Classiﬁcation. On the basis of the availability of the entire unlabeled query set  few-shot
classiﬁcation can be divided into two categories: inductive and transductive few-shot classiﬁcation.
In this work  we mainly explore the few-shot approaches based on meta-learning.
Inductive Few-shot Learning has been a well studied area in recent years. One promising way is the
meta-learning [38  39  22] paradigm. It usually trains a meta-learner from a set of tasks  which extracts
meta-knowledge to transfer into new tasks with scarce data. Meta learning approaches for few-shot
classiﬁcation can be roughly categorized into three groups. Optimization-based methods designed the
meta-learner as an optimizer that learned to update model parameters [1  31  18]. Further  the works
[7  33  36] learned a good initialization so that the learner could rapidly adapt to novel tasks within a
few optimization steps. Parameter-generating based methods [2  20  21  3] usually designed the meta-
learner as a parameter predicting network. Metric-learning based methods [40  35  37  26  5] learned
a common feature space where categories can distinguish with each other based on a distance metric.
For example  Matching Network [40] produced a weighted nearest neighbor classiﬁer. Prototypical
Network [35] performed nearest neighbor classiﬁcation with learned class features (prototypes). The
works [37  26  15] improved the prototypical network with a learnable similarity metric [37]  a task
adaptive metric [26]  or a image-to-class local metric [5].
Our proposed framework belongs to metric-learning based method. Different from existing metric-
learning based methods which extracted the support and query sample features independently  our
method exploits the semantic relevance between support and query features to highlight the target
object. Although the parameter-generating based methods also consider the relationship between
support and query samples  these approaches require an additional complex parameter prediction
network. With less overload  our approach outperforms these methods by a large margin.
Transductive Algorithm.
Transductive few-shot classiﬁcation is ﬁrstly introduced in [17]  which
constructed a graph on the support set and the entire query set  and propagated labels within the

2

(c) Input test image(d) Class Activation Mapof existing method(e) Class Activation Map of our CAN(a) Input training image(b) Class Activation Mapof existing methodgraph. However  the method required a speciﬁc architecture  making it less universal. Inspired
by the self-training strategy in semi-supervised learning [6  25  42  32]  we propose a simper and
more general transductive few-shot algorithm  which explicitly augments the labeled support set
with unlabeled query samples to achieve more representative class features. Moreover  the proposed
transductive algorithm can be directly applied to the existing models  e.g.  prototypical network [35] 
matching network [40]  and relation network [37].
Attention Model. Attention mechanisms aim to highlight important local regions to extract more
discriminative features. It has achieved great success in computer vision applications  such as image
classiﬁcation [12  41  27]  person re-identiﬁcation [10  47  11]  image caption [29  44  4] and visual
question answering [43  45  46  30]. In image classiﬁcation  SENet [12] proposed a channel attention
block to boost the representational power of a network. Woo et al. [41  27] further integrated the
channel and spatial attention modules to a block. In image caption  the attention blocks [44  4]
usually used the last generated words to search for related regions in the image to generate the next
word. And in visual question answering  the attention blocks [43  45  46  30] used the questions to
localize the related regions in the image to answer. Speciﬁcally  [30] uses the questions to generate a
convolutional kernel which is used to convolve with the image feature. On the contrary  our method
uses a meta-learner to generate a kernel which is used to fuse the relations to get the ﬁnal attention
map. For few-shot image classiﬁcation  in this paper  we design a meta-learner to compute the cross
attention between support (or class) and query feature maps  which helps to locate the important
regions of the target object and enhance the feature discriminability.

3 Cross Attention Module

a  ys

Problem Deﬁne.
Few-shot classiﬁcation usually involves a training set  a support set and a query
set. The training set contains a large number of classes and labeled samples. The support set of few
labeled samples and the query set of unlabeled samples share the same label space  which is disjoint
with that of the training set. Few-shot classiﬁcation aims to classify the unlabeled query samples
given the training set and support set. If the support set consists of C classes and K labeled samples
per class  the target few-shot problem is called C-way K-shot.
Following [40  35  34  19  9  14  7]  we adopt the episode training mechanism  which has been
demonstrated as an effective approach for few-shot learning. The episodes used in training simulate
the settings in test. Each episode is formed by randomly sampling C classes and K labeled samples
a)}ns
a=1 (ns = C × K)  and a fraction of the rest samples
per class as the support set S = {(xs
from the C classes as the query set Q = {(xq
b=1. And we denote S k as the support subset of
b )}nq
b  yq
the kth class. How to represent each support class S k and query sample xq
b and measure the similarity
between them is a key issue for few-shot classiﬁcation.
CAM Overview.
In this work  we resort to metric-learning to obtain proper feature representations
for each pair of support class and query sample. Different from existing methods which extract the
class and query features independently  we propose Cross Attention Module (CAM) which can model
the semantic relevance between the class feature and query feature  thus draw attention to the target
objects and beneﬁt the subsequent matching.
CAM is illustrated in Fig. 2. The class feature map P k ∈ Rc×h×w is extracted from the support
samples in S k (k ∈ {1  2  . . .   C}) and the query feature map Qb ∈ Rc×h×w is extracted from the
b (b ∈ {1  2  . . .   nq})  where c  h and w denote the number of channel  height and
query sample xq
width of the feature maps respectively. CAM generates cross attention map Ap (Aq) for P k (Qb) 
which is then used to weight the feature map to achieve more discriminative feature representation ¯P k
b
( ¯Qb
k). For simplicity  we omit the superscripts and subscripts  and denote the input class and query
feature maps as P and Q  and the output class and query feature maps as ¯P and ¯Q  respectively.
Correlation Layer. As shown in Fig. 2  we ﬁrst design a correlation layer to calculate a correlation
map between P and Q  which is then used to guide the generation of the cross attention maps. To
this end  we ﬁrst reshape P and Q to Rc×m  i.e.  P = [p1  p2  . . .   pm] and Q = [q1  q2  . . .   qm] 
where m (m = h × w) is the number of spatial positions on each feature map. pi  qi ∈ Rc are the
feature vectors at the ith spatial position in P and Q respectively. The correlation layer computes
the semantic relevance between {pi}m
i=1 with cosine distance to get the correlation map

i=1 and {qi}m

3

Figure 2. (a) Cross Attention Module (CAM). (b) the Fusion Layer in CAM. In the ﬁgure  Rp (Rq) ∈ Rm×m is
reshaped to Rm×h×w for a better visualization. As seen  CAM can generate the feature maps that attend to the
regions of target object (coated retriever in the ﬁgure).
R ∈ Rm×m as:

  i  j = 1  . . .   m.

(1)

(cid:18) pi

||pi||2

Rij =

(cid:19)T(cid:18) qj

(cid:19)

||qj||2

= R = [rq

1  rq

2  . . .   rq

2  . . .   rp

m] and the query correlation map Rq .

Furthermore  we deﬁne two correlation maps based on R: the class correlation map Rp .
= RT =
i ∈ Rm denotes
1  rp
[rp
the relevance between the local class feature vector pi and all query feature vectors {qi}m
i=1  and
i ∈ Rm is the relevance between local query feature vector qi and all class feature vectors {pi}m
rq
i=1.
In this way  Rp and Rq characterize the local correlations between the class and query feature maps.
Meta Fusion Layer. A meta fusion layer is then used to generate the class and query attention maps 
respectively  based on the corresponding correlation maps. We take the class attention map as an
example. As shown in Fig. 2 (b)  the fusion layer takes the class correlation map Rp as input  and
applies convolutional operation with a m × 1 kernel  w ∈ Rm×1  to fuse each local correlation vector
{rp
i } of Rp into an attention scalar. A softmax function is then used to normalize the attention scalar
to obtain the class attention at the ith position:

m]  where rp

i )/τ(cid:1)
exp(cid:0)(wT rp
j=1 exp(cid:0)(wT rp
j )/τ(cid:1)  
(cid:80)h×w

Ap

i =

(2)

where τ is the temperature hyperparameter. Lower temperature leads to lower entropy  making the
distribution concentrate on a few high conﬁdence positions. The class attention map is then obtained
by reshaping Ap to matrix in Rh×w. Note that the kernel w plays a crucial role in the fusion. It
aggregates the correlations between the local class feature pi and all local query features {qj}m
j=1
as the attention scalar at the ith position. More importantly  the weighted aggregation should draw
attention to the target object  instead of simply highlighting the visually similar regions across support
class and query sample.
Based on above analysis  we design a meta-learner to adaptively generate the kernel based on the
correlation between the class and the query features. To this end  we apply global average pooling
(GAP) operation (i.e.  row-wise averaging) to Rp to obtain an averaged query correlation vector 
which is then fed into the meta-learner to generate the kernel w ∈ Rm:

w = W2(σ(W1(GAP(Rp)) 

(3)

r ×m and W2 ∈ Rm× m

where W1 ∈ R m
r are the parameters of the meta-learner  and r is the reduction
ratio. σ refers to the ReLU function [23]. The nonlinearity in the meta-learning model allows a
ﬂexible transformation. For each pair of class and query features  the meta-learner is expected to
generate a kernel w which can draw cross attention to the target object. This is achieved in meta
training by minimizing the classiﬁcation errors on the query samples.
In a similar way  we can get the query attention map Aq ∈ Rh×w. At last  we use a residual attention
mechanism  where the initial feature maps P and Q are elementwisely weighted by 1 + Ap and
1 + Aq  to form more discriminative feature maps ¯P ∈ Rc×h×w and ¯Q ∈ Rc×h×w  respectively.
Complexity Analysis. The time and space cost of CAM is mainly on correlation layer. The time
complexity of CAM is O(h2w2c) and the space complexity is O(hwc)  which both varies with the
size of input feature map. So we insert CAM after the last convolutional layer to avoid excessive cost.

4

CorrelationLayerFusionLayer𝑐×ℎ×𝑤𝑐×ℎ×𝑤𝑚×ℎ×𝑤ℎ×𝑤𝑐×ℎ×𝑤𝑐×ℎ×𝑤（a) The Cross Attention Module (CAM) （b) TheFusion Layer in CAMMetaLearner𝑚𝑚×1Spatial GAPGAP: Global Average Pooling𝑃𝑘𝑄𝑏𝑅𝑝𝑅𝑞𝐴𝑝𝐴𝑞ത𝑃𝑏𝑘ത𝑄𝑘𝑏𝐴𝑝𝐴𝑞𝑅𝑝𝑅𝑞kernel 𝒘ConvolutionalOperationsoftmax𝑟𝑖𝑝(𝑟𝑖𝑞)𝑚×ℎ×𝑤ℎ×𝑤𝑚×ℎ×𝑤ℎ×𝑤Figure 3. The framework of the proposed CAN approach.

4 Cross Attention Network

b as inputs  and produces the class feature map P k = 1|S k|

The overall Cross Attention Network (CAN) is illustrated in Fig. 3  which consists of three modules:
an embedding module  a cross attention module and a classiﬁcation module. The embedding module
E consists of several cascaded convolutional layers  which maps an input image x into a feature map
E(x) ∈ Rc×h×w. Following prototypical network [35]  we deﬁne the class feature as the the mean of
its support set in the embedding space. As shown in Fig. 3  the embedding module E takes the support
set S and a query sample xq
a∈S k E(xs
a)
and a query feature map Qb = E(xq
b). Each pair of feature maps (P k and Qb) are then fed through
the cross attention module  which highlights the relevant regions and outputs more discriminative
feature pairs ( ¯P k
Model Training via Optimization. CAN is trained via minimizing the classiﬁcation loss on the
query samples of training set. The classiﬁcation module consists of a nearest neighbor and a global
classiﬁer. The nearest neighbor classiﬁer classiﬁes the query samples into C support classes based
on pre-deﬁned similarity measure. To obtain precise attention maps  we constrain each position in
the query feature maps to be correctly classiﬁed. Speciﬁcally  for each local query feature qb
i at ith
position  the nearest neighbor classiﬁer produces a softmax-like label distribution over C support
classes. The probability of predicting qb

k) for classiﬁcation.

b and ¯Qb

(cid:80)

xs

i as kth class is:

exp(cid:0)−d(cid:0)( ¯Qb
j=1 exp(cid:0)−d(cid:0)( ¯Qb
(cid:80)C

p(y = k|qb

i ) =

b )(cid:1)(cid:1)
b )(cid:1)(cid:1)  

k)i  GAP( ¯P k

j)i  GAP( ¯P j

(4)

nq(cid:88)

m(cid:88)

k)i denotes the feature vector in the ith spatial position of ¯Qb

where ( ¯Qb
k  and GAP is the global
average pooling operation to get the mean class feature. Note that ¯Qb
j represent the query
sample xq
b from somewhat different views as they are correlated with different support classes. In
Eq. 4  the cosine distance d is calculated in the feature space generated by CAM. The nearest neighbor
classiﬁcation loss is then deﬁned as the negative log-probability according to the true class label
b ∈ {1  2  . . .   C}:
yq

k and ¯Qb

L1 = −

log p(y = yq

b|qb
i ).

(5)

b=1

i=1

m(cid:88)

nq(cid:88)

i is computed as zb

)i). The global classiﬁcation loss is then expressed as:

i ∈ Rl for each local query feature qb

The global classiﬁer uses a fully connected layer followed by softmax to classify each query sample
among all available training classes. Suppose there are overall l classes in the training set. The
classiﬁcation probability vector zb
i =
softmax(Wc( ¯Qb
yq
b

(cid:17)
(6)
where Wc ∈ Rl×c is the weight of the fully connected layer and lq
b ∈ {1  2  . . .   l} is the true global
class of xq
b. Finally  the overall classiﬁcation loss is deﬁned as L = λL1 + L2  where λ is the weight
to balance the effects of different losses. The network can be trained end-to-end by optimizing L
with gradient descent algorithm.
Inductive Inference.
In inductive inference phase  the embedding module is directly used for a
novel task to extract the class and query feature maps. Then each pair of class and query feature
maps are fed into CAM to get the attention weighted features. The global averaging pooling is then

L2 = −

(cid:16)

(zb

i )lq

b=1

i=1

log

 

b

5

Support SetEmbeddingQueryCross AttentionModuleSimilarity scoreSoftmaxNearest NeighborclassificationGlobal class classificationSoftmaxLinearത𝑃𝑏2ത𝑄2𝑏ത𝑃𝑏1ത𝑄1𝑏ത𝑃𝑏3ത𝑄3𝑏ത𝑃𝑏4ത𝑄4𝑏ത𝑃𝑏5ത𝑄5𝑏𝑃2𝑃1𝑄𝑏𝑃3𝑄𝑏𝑃4𝑄𝑏𝑃5𝑄𝑏𝑄𝑏𝑆1𝑆2𝑆3𝑆4𝑆5𝑥𝑏𝑞performed to the outputs of CAM to get the mean class and query features. Finally  the label ˆyq
query sample xq

b for a
b is predicted by ﬁnding the nearest mean class feature under cosine distance metric:

(cid:16)

(cid:17)

ˆyq
b = arg min

k

d

GAP( ¯Qb

k)  GAP( ¯P k
b )

(7)

Transductive Inference.
In few-shot classiﬁcation task  each class has very few labeled samples 
so the class feature can hardly represent the true class distribution. In order to alleviate the problem 
we propose a simple and effective transductive inference algorithm which utilizes the unlabeled query
samples to enrich the class feature.
b}nq
Speciﬁcally  we ﬁrstly utilize the initial class feature map P k to predict the labels {ˆyq
b=1 of
b}nq
the unlabeled query samples {xq
b=1 using Eq. 7. Then  we deﬁne a label conﬁdence criterion
b and its nearest class neighbor: cq
using the cosine distance between the query sample xq
b =
mink d(GAP( ¯Qb
b  the higher the conﬁdence of the predicted
b )|sb = 1  xq
b ∈ Q} 
label {ˆyq
b}. Based on this criterion  we can obtain a candidate set D = {(xq
(cid:80)nq
where sb ∈ {0  1} denotes the selection indicator for the query sample xq
b. The selection indicator
s ∈ {0  1}nq is determined by the top t conﬁdent query samples: s = arg min||s||0=t
b=1 sbcq
b.
Finally  the candidate set D along with the support set S is used to generate a more representative
class feature map (P k)∗:

b )). The lower the value cq

k)  GAP( ¯P k

b  ˆyq

∗

(P k)

=

1

|S k| + |Dk|
Here Dk = {(xq
b = k}. (P k)∗ is then used to re-estimate the pseudo label for each
query sample. We repeat above process for a certain number of iterations. And the number of selected
samples in the candidates set D is gradually increased with a ﬁxed ratio in each iteration. In this way 
we can progressively enrich the class features to be more representative and robust.

b ∈ D  ˆyq

b )|xq

b∈Dk
xq

a∈Sk
xs

b  ˆyq

E(xs

a) +

E(xq
b)

(8)

 (cid:88)

(cid:88)

 .

5 Experiments

5.1 Experiment Setup

Datasets. We use miniImageNet [40] which is a subset of ILSVRC-12 [13]. It contains 100
classes with 600 images per class. We use the standard split following [31  37  26  15  33]: 64 classes
for training  16 for validation and 20 for testing. We also use tieredImageNet dataset [32]  a much
larger subset of ILSVRC-12 [13]. It contains 34 categories and 608 classes in total. These are divided
into 20 categories (351 classes) for training  6 categories (97 classes) for validation  and 8 categories
(160 classes) for testing  as in [32  7  35  37].
Experimental setting. We experiment our approach on 5-way 1-shot and 5-way 5-shot settings.
For a C-way K-shot setting  the episode is formed with C classes and each class includes K support
samples  and 6 and 15 query samples are used for training and inference respectively. When inference 
2000 episodes are randomly sampled from the test set. We report the average accuracy and the
corresponding 95% conﬁdence interval over the 2000 episodes.
Implementation details.1
Pytorch [28] is used to implement all our experiments on one NVIDIA
1080Ti GPU. Following [26  17  36  21]  we use ResNet-12 network as our embedding module. The
input images size is 84 × 84. During training  we adopt horizontal ﬂip  random crop and random
erasing [49] as data augmentation. SGD is used as the optimizer. Each mini-batch contains 8 episodes.
The model is trained for 80 epochs  with each epoch consisting of 1  200 episodes. For miniImageNet 
the initial learning rate is 0.1 and decreased to 0.006 and 0.0012 at 60 and 70 epochs  respectively.
For tieredImageNet  the initial learning rate is set to 0.1 with a decay factor 0.1 at every 20 epochs.
The temperature hyperparameter (τ in Eq. 3) is set to 0.025  the reduction ratio in the meta-learner
is set to 6  and the weight hyperparameter (λ) in the overall loss function is set to 0.5. For the
transductive algorithm  the selected number of query samples in the ﬁrst iteration (t) is set to 35  and
the number of iterations and enlarging factor of candidate set are both set to 2. All hyperparameters
are cross-validated in the validation sets and ﬁxed afterwards in all experiments.

1The code and models are available on https://github.com/blue-blue272/fewshot-CAN

6

Table 1. Comparison to state-of-the-arts with 95% conﬁdence intervals on 5-way classiﬁcation on miniImageNet
and tieredImageNet datasets.
IT: Inference Time per query data in a 5-way 1-shot task on one NVIDIA
1080Ti GPU. CAN+T denotes CAN with transductive inference. The methods are separated into four groups:
optimization-based (O)  parameter-generating (P)  metric-learning (M) and transductive methods (T).

miniImageNet

tieredImageNet

1-shot

51.67 ± 1.81
66.33 ± 0.05
65.99 ± 0.72

-

5-shot

70.30 ± 1.75
81.44 ± 0.09
81.56 ± 0.53

-

-
-
-
-

-
-

model
MAML [7]
MTL [36]
LEO [33]
MetaOpt [14]
MetaNet [20]
MM-Net [3]
adaNet [21]
MN [40]
PN [35]
RN [37]
DN4 [15]
TADAM [26]
Our CAN
TPN [17]
Our CAN+T

Embedding
ConvNet
ResNet-12
WRN-28
ResNet-12
ConvNet
ConvNet
ResNet-12
ConvNet
ConvNet
ConvNet
ConvNet
ResNet-12
ResNet-12
ResNet-12
ResNet-12

O

P

M

T

IT(s)
0.103
2.020

0.096

1.371
0.021
0.018
0.033
0.049
0.079
0.044

-

-
-

-
-

1-shot

48.70 ± 0.84
61.20 ± 1.80
61.76 ± 0.08
62.64 ± 0.62
49.21 ± 0.96
53.37 ± 0.48
56.88 ± 0.62
43.44 ± 0.77
49.42 ± 0.78
50.44 ± 0.82
51.24 ± 0.74
58.50 ± 0.30
63.85 ± 0.48
67.19 ± 0.55

59.46

5-shot

-

55.31 ± 0.73
75.50 ± 0.80
77.59 ± 0.12
78.63 ± 0.46
66.97 ± 0.35
71.94 ± 0.57
60.60 ± 0.71
68.20 ± 0.66
65.32 ± 0.70
71.02 ± 0.64
76.70 ± 0.30
79.44 ± 0.34
80.64 ± 0.35

75.65

-
-
-
-

-
-

53.31 ± 0.89
54.48 ± 0.93

72.69 ± 0.74
71.32 ± 0.78

69.89 ± 0.51
73.21 ± 0.58

-

84.23 ± 0.37
84.93 ± 0.38

-

5.2 Comparison with State-of-the-arts

Tab. 1 compares our method with existing few-shot methods on miniImageNet and tieredImageNet.
The comparative methods are categorized into four groups  i.e.  optimization-based methods (O) 
parameter-generating methods (P)  metric-learning methods (M)  and transductive methods (T). Our
method outperforms the optimization-based methods [7  18  33  36]. It is noted that the optimization-
based methods need ﬁne-tuning on the target task  making the classiﬁcation time consuming. On the
contrary  our method requires no model updating solves the tasks in an feed-forward manner  which
is much faster and simpler than above methods and has better results.
Our method performs better than the parameter-generating methods [20  21  3]  with an improvement
up to 7%. These approaches generate the parameters of the feature extractor based on the support set
and extract the query features adaptively. However  these methods suffer from the high dimensionality
of the parameter space. Instead  our method uses a cross attention module to adaptively extract the
support and query features  which is computationally lightweight and achieves a better performance.
Our method belongs to the metric-learning methods. Existing metric-based methods [40  35  37 
26  15] extract features of support and query samples independently  making the features attend to
non-target objects. Instead  our CAN highlights the target object regions and gets more discriminative
features. Compared to TADAM [26]  CAN with almost the same number of parameters achieves 5%
higher performance on 1-shot  which demonstrates the superiority of our cross attention module.
In the transductive setting  CAN with transduction (CAN+T) outperforms the prior work TPN [17]
by a large margin  up to 8% and 5% improvements on 1-shot and 5-shot respectively. TPN uses a
graph network to propagate the labels of the support set to the query set. In contrast  our algorithm
selects the top conﬁdent query samples to augment the support set  which can explicitly alleviate the
low-data problem. In addition  our transductive algorithm can be easily applied to other few-shot
learning models  e.g.  matching network [40]  prototypical network [35] and relation network [37].
Time complexity comparison. Tab. 1 further compares the time cost of our method to others. Some
methods [40  35  37  15  7] use a 4-layer ConvNet as the backbone thus take relatively lower time
cost. Even though  our CAN is still comparable even superior to these methods in term of time
cost  with a performance improvement up to 10%. The others use the same backbone as CAN  but
require following up modules such as model update per task [36  14]  gradient-based parameter
generation [21]  or expensive condition generation [26]  which all incur more time overhead than
CAM. Overall  Tab. 1 shows that CAN outperforms other methods without excessive overhead.

5.3 Ablation Study

In this subsection  we empirically show the effectiveness of each component of CAN. We ﬁrstly
introduce two baselines to be used for comparison. In R12-proto [35]  the features from embedding

7

Table 2. Ablation study on miniImageNet and complexity comparisons. PN: Parameter Number; GFLOPs: the
number of ﬂoating-point operations; CIT: CPU Inference Time of a task with 15 query samples per class.

5-way 1-shot

5-way 5-shot

Description
R12-proto
R12-proto-ac
CAN-NoML-1
CAN-NoML-2
CAN
CAN+T

PN

GFLOPs
8.04M 101.550
8.04M 101.550
8.04M 101.812
8.04M 101.812
8.04M 101.813
8.04M 101.930

CIT
0.96s
0.97s
1.01s
1.01s
1.02s
1.11s

accuracy GFLOPs
126.938
126.938
127.201
127.201
127.203
127.320

55.46
61.30
63.55
63.38
63.85
67.19

CIT
1.25s
1.26s
1.29s
1.30s
1.31s
1.43s

accuracy

69.00
76.70
78.88
79.08
79.44
80.64

Table 3. The proposed transductive algorithm for other few-shot learning models. * indicates our re-implemented
results using the code provided by LwoF [8]. () indicates the results reported in the paper.

Models
Matching Network [40]
Prototypical Network [35]
Relation Network [37]

Inductive

1-shot

53.52* (43.77)
53.68* (49.42)
50.65* (50.44)

5-shot

66.20* (60.60)
70.44* (68.20)
64.18* (65.32)

Transductive
1-shot
5-shot
69.80
56.31
71.12
55.15
52.40
65.36

module are directly fed to the nearest neighbor classiﬁer and the model is trained with nearest neighbor
classiﬁcation loss. In R12-proto-ac  the only difference from R12-proto is that R12-proto-ac has
an additional logit head for global classiﬁcation (the normal 64-way classiﬁcation in miniImageNet
case) and the model is trained with the joint of global and nearest neighbor classiﬁcation loss.
Inﬂuence of global classiﬁcation. The comparison results are shown in Tab. 2. By comparing
R12-proto-ac to R12-proto  we can ﬁnd large improvements on both 1-shot (5.8%) and 5-shot (7.7%).
We further try another meta-learner matching network (MN) [40]2  and the proposed joint learning
schema improves MN from 55.29% to 59.14% on 1-shot setting and 67.74% to 73.81% on 5-shot
setting. The consistent improvements demonstrate the effectiveness of the joint leaning schema. We
argue that the global classiﬁcation loss provides regularization on the embedding module and forces
it to perform well on two decoupled tasks  nearest neighbor classiﬁcation and global classiﬁcation.
Inﬂuence of cross attention module. By comparing our CAN to R12-pro-ac  we observe consistent
improvements on both 1-shot and 5-shot scenarios. The reason is that when using the cross attention
module  our model is able to highlight the relevant regions and extract more discriminative feature.
The performance gap also provides evidence that (1) conventionally independently extracted features
tend to focus on non-target region and produce inaccurate similarities. (2) cross attention module can
help to highlight target regions and reduce such inaccuracy with small overhead.
Inﬂuence of meta-learner in CAM. To verify the effectiveness of the meta-learner in CAM  we
develop two variants of CAM without meta-learner. Speciﬁcally  one variant named CAN-NoML-1
sets the kernel w (shown in Fig. 2 (b)) to be a ﬁxed mean kernel  i.e.  performing global average
pooling on the correlation map R to get the attention maps A. The other variant  CAM-NoML-2  sets
the kernel w to a vanilla learnable convolutional kernel that remains the same for all input samples.
As shown in Tab. 2  both variants outperform R12-proto-ac consistently  which further demonstrates
the effectiveness of the proposed cross attention mechanism. The improvements of CAN-NoML-1
shows the mean of correlators can roughly estimate the relevant semantic information  which furthers
veriﬁes the reasonability of our designed meta-leaner. As seen  CAN outperforms both variants. The
improvement can be attributed to the meta-learning schema which learns to adaptively generate the
kernel w according to the input pair of feature maps.
Inﬂuence of transductive inference algorithm. As shown in Tab. 2  CAN+T greatly improves CAN
especially in 1-shot where the low-data problem is more serious. To further verity its effectiveness 
we apply it to other few-shot models  i.e.  matching network [40]  prototypical network [35] and
relation network [37]. We re-implement these models using the code provided by [8] to ensure
a fair comparison. As shown in Tab. 3  our algorithm consistently improves the performance of
these models  which demonstrates its generalization ability. Nevertheless  the improvements to these
models are inferior to CAN. We argue that CAN can predict more precise pseudo labels for query
samples and augment the support set more effectively  thus leading to better performance.

2We re-implement matching network with ResNet-12 as backbone on miniImageNet.

8

Figure 4. Class activation mapping (Cam) visualization on a 5-way 1-shot task with 1 query sample per class.

Complexity comparisons. To illustrate the cost of CAN  we report the number of parameters
(PN)  the number of ﬂoating-point operations (GFLOPs) and the average CPU inference time (CIT)
for a 5-way 1-shot and 5-way 5-shot task with 15 query samples per class. As shown in Tab. 2 
CAN introduces negligible parameters (the parameters W1 and W2 of the meta-learner in CAM)
and small computational overhead. For example  CAN requires 101.81 GFLOPs for 5-way 1-shot 
corresponding to only 0.25% relative increase over original R12-proto-ac. Notably  the correlation
map in CAM can be worked out by one matrix multiplication  which occupies less time in GPU
libraries. The transductive inference algorithm also introduces small computational overhead (0.37%
on 1-shot and 0.31% on 5-shot) since it directly utilizes the extracted embedding features to regenerate
the class feature and only passes the lightweight CAM again.

5.4 Visualization Analysis

To qualitatively evaluate the proposed cross attention mechanism  we compare the class activation
maps [48] visualization results of CAN to other meta-learners  RN [37]  MAML [7] and TADAM [26].
As shown in Fig. 4 (a)  the features of RN usually contain non-target objects since it lacks an explicit
mechanism for feature adaptation. MAML performs gradient-based adaptation  which makes the
model merely learn some conspicuous discriminative features in the support images without deeping
into the intrinsic characteristic of the target objects. As shown in Fig. 4 (b)  MAML attends to
ship for the groenendael support image to better distinguish it from the golden retriever category 
resulting in a confusing location and misclassiﬁcation of the groenendael category. TADAM performs
task-dependent adaptation and applies the same adaptive parameters to all query images of a task 
thus it is difﬁcult to locate different target objects for different categories. As shown in Fig. 4 (c) 
TADAM mistakenly attends to the dog for worm fence query image. In contrast  CAN processes the
query samples with different adaptive parameters  which allows it to focus on the different target
objects for different categories shown in Fig. 4 (d).

6 Conclusion

In this paper  we proposed a cross attention network for few-shot classiﬁcation. Firstly  a cross
attention module is designed to model the semantic relevance between class and query features. It
can adaptively localize the relevant regions and generate more discriminative features. Secondly  we
propose a transductive inference algorithm to alleviate the low-data problem. It utilizes the unlabeled
query samples to enrich the class features to be more representative. Extensive experiments show
that our method is far simpler and more efﬁcient than recent few-shot meta-learning approaches  and
produces state-of-the-art results.
Acknowledgement This work is partially supported by National Key R&D Program of China
(No.2017YFA0700800)  Natural Science Foundation of China (NSFC): 61876171 and Beijing
Natural Science Foundation under Grant L182054.

9

Support(a) Camof RN (b) Camof MAML (c) Camof TADAM(d) Camof CAN Querygolden retieverworm fencegroenedaelpark benchSeashoreReferences
[1] M. Andrychowicz  M. Denil  S. Gomez  M. W. Hoffman  D. P. T. Schaul  and N. d. Freitas.

Learning to learn by gradient descent by gradient descent. In NeurIPS  2018.

[2] L. Bertinetto  J. F. Henriques  J. Valmadre  P. Torr  and A. Vedaldi. Learning feed-forward

one-shot learners. In NeurIPS  pages 523–531  2016.

[3] Q. Cai  Y. Pan  T. Yao  C. Yan  and T. Mei. Memory matching networks for one-shot image

recognition. In CVPR  pages 4080–4088  2018.

[4] L. Chen  H. Zhang  J. Xiao  L. Nie  J. Shao  W. Liu  and T. S. Chua. Sca-cnn: Spatial
and channel-wise attention in convolutional networks for image captioning. In CVPR  pages
5659–5667  2017.

[5] D D. Zhou  O. Bousquet  T. N. Lal  J. Weston  and B. Schölkopf. Learning with local and global

consistency. In NeurIPS  pages 321–328  2004.

[6] L. Dong-Hyun. Pseudo-label: The simple and efﬁcient semi-supervised learning method for

deep neural networks. In ICML workshop  2013.

[7] C. Finn  P. Abbeel  and S. Levine. Model-agnostic meta-learning for fast adaptation of deep

networks. In ICML  pages 1126–1135  2017.

[8] S. Gidaris and N. Komodakis. Dynamic few-shot visual learning without forgetting. In CVPR 

2018.

[9] S. Gidaris and N. Komodakis. Generating classiﬁcation weights with gnn denoising autoen-

coders for few-shot learning. In CVPR  2019.

[10] R. Hou  B. Ma  H. Chang  X. Gu  S. Shan  and X. Chen. Interaction-and-aggregation network

for person re-identiﬁcation. In CVPR  pages 9317–9326  2019.

[11] R. Hou  B. Ma  H. Chang  X. Gu  S. Shan  and X. Chen. Vrstc: Occlusion-free video person

re-identiﬁcation. In CVPR  pages 7183–7192  2019.

[12] J. Hu  L. Shen  and G. Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507 

2017.

[13] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NeurIPS  pages 1097–1105  2012.

[14] K. Lee  S. Maji  A. Ravichandran  and S. Soatto. Meta-learning with differentiable convex

optimization. In CVPR  2019.

[15] W. Li  L. Wang  J. Xu  J. Huo  Y. Gao  and J. Luo. Revisiting local descriptor based image-to-

class measure for few-shot learning. In CVPR  2019.

[16] Z. Li  F. Zhou  F. Chen  and H. Li. Meta-sgd: Learning to learn quickly for few-shot learning.

arXiv preprint arXiv:1707.09835  2017  2017.

[17] Y. Liu  J. Lee  M. Park  S. Kim  E. Yang  S. J. Hwang  and Y. Yang. Learning to propagate

labels: Transductive propagation network for few-shot learning. In ICLR  2018.

[18] Y. Liu  Q. Sun  A. A. Liu ad Y. Su  B. Schiele  and T. S. Chua. Lcc: Learning to customize and

combine neural networks for few-shot learning. arXiv preprint arXiv:1904.08479  2019.

[19] N. Mishra  M. Rohaninejad  X. Chen  and P. Abbeel. A simple neural attentive meta-learner. In

ICLR  2018.

[20] T. Munkhdalai and H. Yu. Meta networks. In ICML  pages 2554–2563  2017.

[21] T. Munkhdalai  X. Yuan  S. Mehri  and A. Trischler. Rapid adaptation with conditionally shifted

neurons. In ICML  2018.

10

[22] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In IJCNN  pages

437–442  1992.

[23] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In

ICML  2010.

[24] A. Nichol  J. Achiam  and J. Schulman. On ﬁrst-order meta-learning algorithms. arXiv preprint

arXiv:1803.02999  2018.

[25] A. Oliver  A. Odena  C. A. Raffel  E. D. Cubuk  and I. J. Goodfellow. Realistic evaluation of

deep semi-supervised learning algorithms. In NeurIPS  2018.

[26] B. Oreshkin  P. R. Lopez  and A. Lacoste. Tadam: Task dependent adaptive metric for improved

few-shot learning. In NeurIPS  pages 719–729  2018.

[27] J. Park  S. Woo  J. Y. Lee  and I. S. Kweon. Bam: Bottleneck attention module. In BMVC  2018.

[28] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison 

L. Antiga  and A. Lerer. Automatic differentiation in pytorch. In NIPS workshop  2017.

[29] M. Pedersoli  T. Lucas  C. Schmid  and J. Verbeek. Areas of attention for image captioning. In

ICCV  pages 1251–1259  2017.

[30] G. Peng  L. Hongsheng  L. Shuang  L. Pan  L. Yikang  H. Steven CH  and W. Xiaogang.
Question-guided hybrid convolution for visual question answering. In ECCV  pages 469–485 
2018.

[31] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In ICLR  2017.

[32] M. Ren  E. Triantaﬁllou  S. Ravi  J. Snell  K. Swersky  J. B. Tenenbaum  H. Larochelle 
and R. S. Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. arXiv preprint
arXiv:1803.00676  2018.

[33] A. A. Rusu  D. Rao  J. Sygnowski  O. Vinyals  R. Pascanu  S. Osindero  and R. Hadsell.

Meta-learning with latent embedding optimization. In ICLR  2019.

[34] A. Santoro  S. Bartunov  M. Botvinick  D. Wierstra  and T. Lillicrap. Meta-learning with

memory-augmented neural networks. In ICML  pages 1842–1850  2016.

[35] J. Snell  K. Swersky  and R. Zemel. Prototypical networks for few-shot learning. In NeurIPS 

pages 4077–4087  2017.

[36] Q. Sun  Y. liu  T. S. Chua  and B. Schiele. Meta-transfer learning for few-shot learning. In

CVPR  2019.

[37] F. Sung  Y. Yang  L. Zhang  T. Xiang  P. H. Torr  and T. M. Hospedales. Learning to compare:

Relation network for few-shot learning. In CVPR  pages 1199–1208  2018.

[38] S. Thrun. Lifelong learning algorithms. In Learning to Learn  pages 181–209  1998.

[39] S. Thrun and L. Pratt. Learning to learn: Introduction and overview. In Learning to Learn 

pages 3–17  1998.

[40] O. Vinyals  C. Blundell  T. Lillicrap  and D. Wierstra. Matching networks for one shot learning.

In NeurIPS  pages 3630–3638  2016.

[41] S. Woo  J. Park  J. Y. Lee  and I. S. Kweon. Cbam: Convolutional block attention module. In

ECCV  pages 3–19  2018.

[42] Y. Wu  Y. Lin  X. Dong  Y. Yan  W. Quyang  and Y. Yang. Exploit the unknown gradually:
One-shot video-based person re-identiﬁcation by stepwise learning. In CVPR  pages 5177–5186 
2018.

[43] H. Xu and K. Saenko. Ask  attend and answer: Exploring question-guided spatial attention for

visual question answering. In ECCV  pages 451–466  2016.

11

[44] K. Xu  J. Ba  R. Kiros  K. Cho  A. Courville  R. Salakhutdinov  R. Zemel  and Y. Bengio. Show 

attend and tell: Neural image caption generation with visual attention. 2015.

[45] Z. Yang  X. He  J. Gao  L. Deng  and A. Smola. Stacked attention networks for image question

answering. In CVPR  pages 21–29  2016.

[46] D. Yu  J. Fu  T. Mei  and Y. Rui. Multi-level attention networks for visual question answering.

In CVPR  pages 4709–4717  2017.

[47] L. Zhao  X. Li  J. Wang  and Y. Zhuang. Deeply-learned part-aligned representations for person

re-identiﬁcation. In ICCV  pages 3239 – 3248  2017.

[48] B. Zhou  A. Khosla  A. Lapedriza  A. Oliva  and A. Torralba. Learning deep features for

discriminative localization. In CVPR  pages 2921–2929  2016.

[49] Z. Zhun  Z. Liang  K. Guoliang  L. Shaozi  and Y. Yi. Random erasing data augmentation.

arXiv preprint arXiv:1708.04896  2017.

12

,Robert Mattila
Cristian Rojas
Vikram Krishnamurthy
Bo Wahlberg
Ruibing Hou
Hong Chang
Bingpeng MA
Shiguang Shan
Xilin Chen