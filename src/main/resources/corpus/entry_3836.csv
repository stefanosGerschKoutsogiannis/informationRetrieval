2015,Nonparametric von Mises Estimators for Entropies  Divergences and Mutual Informations,We propose and analyse estimators for statistical functionals of one or moredistributions under nonparametric assumptions.Our estimators are derived from the von Mises expansion andare based on the theory of influence functions  which appearin the semiparametric statistics literature.We show that estimators based either on data-splitting or a leave-one-out techniqueenjoy fast rates of convergence and other favorable theoretical properties.We apply this framework to derive estimators for several popular informationtheoretic quantities  and via empirical evaluation  show the advantage of thisapproach over existing estimators.,Nonparametric von Mises Estimators for Entropies 

Divergences and Mutual Informations

Kirthevasan Kandasamy
Carnegie Mellon University
kandasamy@cs.cmu.edu

Akshay Krishnamurthy
Microsoft Research  NY

akshaykr@cs.cmu.edu

Barnab´as P´oczos  Larry Wasserman

Carnegie Mellon University

bapoczos@cs.cmu.edu  larry@stat.cmu.edu

James M. Robins
Harvard University

robins@hsph.harvard.edu

Abstract

We propose and analyse estimators for statistical functionals of one or more dis-
tributions under nonparametric assumptions. Our estimators are derived from the
von Mises expansion and are based on the theory of inﬂuence functions  which ap-
pear in the semiparametric statistics literature. We show that estimators based ei-
ther on data-splitting or a leave-one-out technique enjoy fast rates of convergence
and other favorable theoretical properties. We apply this framework to derive es-
timators for several popular information theoretic quantities  and via empirical
evaluation  show the advantage of this approach over existing estimators.

1

Introduction

Entropies  divergences  and mutual informations are classical information-theoretic quantities that
play fundamental roles in statistics  machine learning  and across the mathematical sciences. In
addition to their use as analytical tools  they arise in a variety of applications including hypothesis
testing  parameter estimation  feature selection  and optimal experimental design. In many of these
applications  it is important to estimate these functionals from data so that they can be used in down-
stream algorithmic or scientiﬁc tasks. In this paper  we develop a recipe for estimating statistical
functionals of one or more nonparametric distributions based on the notion of inﬂuence functions.
Entropy estimators are used in applications ranging from independent components analysis [15] 
intrinsic dimension estimation [4] and several signal processing applications [9]. Divergence es-
timators are useful in statistical tasks such as two-sample testing. Recently they have also gained
popularity as they are used to measure (dis)-similarity between objects that are modeled as distribu-
tions  in what is known as the “machine learning on distributions” framework [5  28]. Mutual infor-
mation estimators have been used in in learning tree-structured Markov random ﬁelds [19]  feature
selection [25]  clustering [18] and neuron classiﬁcation [31]. In the parametric setting  conditional
divergence and conditional mutual information estimators are used for conditional two sample test-
ing or as building blocks for structure learning in graphical models. Nonparametric estimators for
these quantities could potentially allow us to generalise several of these algorithms to the nonpara-
metric domain. Our approach gives sample-efﬁcient estimators for all these quantities (and many
others)  which often outperfom the existing estimators both theoretically and empirically.
Our approach to estimating these functionals is based on post-hoc correction of a preliminary esti-
mator using the Von Mises Expansion [7  36]. This idea has been used before in the semiparametric
statistics literature [3  30]. However  most studies are restricted to functionals of one distribution
and have focused on a “data-split” approach which splits the samples for density estimation and
functional estimation. While the data-split (DS) estimator is known to achieve the parametric con-

1

vergence rate for sufﬁciently smooth densities [3  14]  in practical settings  as we show in our simu-
lations  splitting the data results in poor empirical performance.
In this paper we introduce the method of inﬂuence function based nonparametric estimators to the
machine learning community and expand on this technique in several novel and important ways.
The main contributions of this paper are:
1. We propose a “leave-one-out” (LOO) technique to estimate functionals of a single distribution.
We prove that it has the same convergence rates as the DS estimator. However  the LOO estimator
has better empirical performance in our simulations since it makes efﬁcient use of the data.

2. We extend both DS and LOO methods to functionals of multiple distributions and analyse their
convergence. Under sufﬁcient smoothness both estimators achieve the parametric rate and the
DS estimator has a limiting normal distribution.

3. We prove a lower bound for estimating functionals of multiple distributions. We use this to

establish minimax optimality of the DS and LOO estimators under sufﬁcient smoothness.

4. We use the approach to construct and implement estimators for various entropy  diver-
information quantities and their conditional versions. A subset of these
gence  mutual
functionals are listed in Table 1 in the Appendix. Our software is publicly available at
github.com/kirthevasank/if-estimators.

5. We compare our estimators against several other approaches in simulation. Despite the generality
of our approach  our estimators are competitive with and in many cases superior to existing
specialised approaches for speciﬁc functionals. We also demonstrate how our estimators can be
used in machine learning applications via an image clustering task.

Our focus on information theoretic quantities is due to their relevance in machine learning applica-
tions  rather than a limitation of our approach. Indeed our techniques apply to any smooth functional.
History: We provide a brief history of the post-hoc correction technique and inﬂuence functions.
We defer a detailed discussion of other approaches to estimating functionals to Section 5. To our
knowledge  the ﬁrst paper using a post-hoc correction estimator was that of Bickel and Ritov [2].
The line of work following this paper analysed integral functionals of a single one dimensional

density of the form(cid:82) ν(p) [2  3  11  14]. A recent paper by Krishnamurthy et al. [12] also extends
(cid:82) pαqβ for densities p and q. All approaches above of use data splitting. Our work contributes to

this line to functionals of multiple densities  but only considers polynomial functionals of the form

this line of research in two ways: we extend the technique to a more general class of functionals and
study the empirically superior LOO estimator.
A fundamental quantity in the design of our estimators is the inﬂuence function  which appears both
in robust and semiparametric statistics. Indeed  our work is inspired by that of Robins et al. [30]
and Emery et al. [6] who propose a (data-split) inﬂuence-function based estimator for functionals of
a single distribution. Their analysis for nonparametric problems rely on ideas from semiparametric
statistics:
they deﬁne inﬂuence functions for parametric models and then analyse estimators by
looking at all parametric submodels through the true parameter.

2 Preliminaries
Let X be a compact metric space equipped with a measure µ  e.g.
the Lebesgue measure. Let
F and G be measures over X that are absolutely continuous w.r.t µ. Let f  g ∈ L2(X ) be the
Radon-Nikodym derivatives with respect to µ. We focus on estimating functionals of the form:

(cid:18)(cid:90)

(cid:19)

(cid:18)(cid:90)

(cid:19)

T (F ) = T (f ) = φ

ν(f )dµ

or

T (F  G) = T (f  g) = φ

ν(f  g)dµ

 

(1)

where φ  ν are real valued Lipschitz functions that twice differentiable. Our framework permits
more general functionals (e.g. functionals based on the conditional densities)  but we will focus on
this form for ease of exposition. To facilitate presentation of the main deﬁnitions  it is easiest to
work with functionals of one distribution T (F ). Deﬁne M to be the set of all measures that are
absolutely continuous w.r.t µ  whose Radon-Nikodym derivatives belong to L2(X ).

2

Central to our development is the Von Mises expansion (VME)  which is the distributional analog
of the Taylor expansion. For this we introduce the Gˆateaux derivative which imposes a notion of
differentiability in topological spaces. We then introduce the inﬂuence function.
: M → R
Deﬁnition 1. Let P  H ∈ M and U : M → R be any functional. The map U(cid:48)
|t=0 is called the Gˆateaux derivative at P if the derivative exists and
where U(cid:48)(H; P ) = ∂U (P +tH)
is linear and continuous in H. U is Gˆateaux differentiable at P if the Gˆateaux derivative exists at P .
Deﬁnition 2. Let U be Gˆateaux differentiable at P . A function ψ(·; P ) : X → R which satisﬁes

U(cid:48)(Q − P ; P ) =(cid:82) ψ(x; P )dQ(x)  is the inﬂuence function of U w.r.t the distribution P .

∂t

By the Riesz representation theorem  the inﬂuence function exists uniquely since the domain of U is
a bijection of L2(X ) and consequently a Hilbert space. The classical work of Fernholz [7] deﬁnes
the inﬂuence function in terms of the Gˆateaux derivative by 

ψ(x; P ) = U(cid:48)(δx − P ; P ) =

∂U ((1 − t)P + tδx)

 

(2)

(cid:12)(cid:12)(cid:12)t=0

where δx is the dirac delta function at x. While our functionals are deﬁned only on non-atomic
distributions  we can still use (2) to compute the inﬂuence function. The function computed this
way can be shown to satisfy Deﬁnition 2.
Based on the above  the ﬁrst order VME is 

U (Q) = U (P ) + U(cid:48)(Q − P ; P ) + R2(P  Q) = U (P ) +

ψ(x; P )dQ(x) + R2(P  Q) 

(3)

where R2 is the second order remainder. Gˆateaux differentiability alone will not be sufﬁcient for

our purposes. In what follows  we will assign Q → F and P → (cid:98)F   where F   (cid:98)F are the true and
(cid:98)F . For functionals T of the form (1)  we restrict the domain to be only measures with continuous

estimated distributions. We would like to bound the remainder in terms of a distance between F and

densities  Then  we can control R2 using the L2 metric of the densities. This essentially means that
our functionals satisfy a stronger form of differentiability called Fr´echet differentiability [7  36] in
the L2 metric. Consequently  we can write all derivatives in terms of the densities  and the VME
reduces to a functional Taylor expansion on the densities (Lemmas 9  10 in Appendix A):

∂t

(cid:90)

T (q) = T (p) + φ(cid:48)(cid:18)(cid:90)
(cid:90)

= T (p) +

(cid:19)(cid:90)

ν(p)

(q − p)ν(cid:48)(p) + R2(p  q)

ψ(x; p)q(x)dµ(x) + O((cid:107)p − q(cid:107)2
2).

(4)

This expansion will be the basis for our estimators.
These ideas generalise to functionals of multiple distributions and to settings where the functional
involves quantities other than the density.
A functional T (P  Q) of two distributions has two
i (·; P  Q) for i = 1  2 formed by perturbing the ith argument with the other
Gˆateaux derivatives  T (cid:48)
ﬁxed. The inﬂuence functions ψ1  ψ2 satisfy  ∀P1  P2 ∈ M 
∂T (P1 + t(Q1 − P1)  P2)
∂T (P1  P2 + t(Q2 − P2))

1(Q1 − P1; P1  P2) =
T (cid:48)
2(Q2 − P2; P1  P2) =
T (cid:48)

ψ1(u; P1  P2)dQ1(u) 

ψ2(u; P1  P2)dQ2(u).

(cid:90)
(cid:90)

(5)

(cid:12)(cid:12)(cid:12)t=0
(cid:12)(cid:12)(cid:12)t=0

∂t

∂t

=

=

(cid:90)

The VME can be written as 

T (q1  q2) = T (p1  p2) +

(cid:90)

+ O((cid:107)p1 − q1(cid:107)2

ψ1(x; p1  p2)q1(x)dx +
2) + O((cid:107)p2 − q2(cid:107)2
2).

ψ2(x; p1  p2)q2(x)dx

(6)

3 Estimating Functionals

First consider estimating a functional of a single distribution  T (f ) = φ((cid:82) ν(f )dµ) from samples
1 ∼ f. We wish to ﬁnd an estimator (cid:98)T with low expected mean squared error (MSE) E[((cid:98)T − T )2].

X n

3

Using the VME (4)  Emery et al. [6] and Robins et al. [30] suggest a natural estimator. If we use
half of the data X n/2

1

to construct an estimate ˆf (1) of the density f  then by (4):
ψ(x; ˆf (1))f (x)dµ + O((cid:107)f − ˆf (1)(cid:107)2
2).

T (f ) − T ( ˆf (1)) =

(cid:90)

n(cid:88)

(cid:98)T (1)

As the inﬂuence function does not depend on (the unknown) F   the ﬁrst term on the right hand side
is simply an expectation of ψ(X; ˆf (1)) w.r.t F . We can use the second half of the data X n
n/2+1 to
estimate this expectation with its sample mean. This leads to the following preliminary estimator:

DS = T ( ˆf (1)) +

1

ψ(Xi; ˆf (1)).

(7)

n/2

i=n/2+1

DS by using X n

DS + (cid:98)T (2)

n/2+1 for density estimation and X n/2

We can similarly construct an estimator (cid:98)T (2)
averaging. Our ﬁnal estimator is obtained via (cid:98)TDS = ((cid:98)T (1)

for
DS )/2. In what follows  we shall
refer to this estimator as the Data-Split (DS) estimator. The DS estimator for functionals of one
distribution has appeared before in the statistics literature [2  3  30].
The rate of convergence of this estimator is determined by the O((cid:107)f − ˆf (1)(cid:107)2
2) error in the VME
and the n−1 rate for estimating an expectation. Lower bounds from several literature [3  14] conﬁrm
minimax optimality of the DS estimator when f is sufﬁciently smooth. The data splitting trick is
common approach [3  12  14] as the analysis is straightforward. While in theory DS estimators enjoy
good rates of convergence  data splitting is unsatisfying from a practical standpoint since using only
half the data each for estimation and averaging invariably decreases the accuracy.
To make more effective use of the sample  we propose a Leave-One-Out (LOO) version of the above
estimator 

1

T ( ˆf−i) + ψ(Xi; ˆf−i)

.

(8)

(cid:98)TLOO =

n(cid:88)

(cid:16)

i=1

1
n

(cid:17)

m(cid:88)

where ˆf−i is a density estimate using all the samples X n
1 except for Xi. We prove that the LOO
Estimator achieves the same rate of convergence as the DS estimator but empirically performs much
better. Our analysis is specialised to the case where ˆf−i is a kernel density estimate (Section 4).
We can extend this method to estimate functionals of two distributions. Say we have n i.i.d samples
from g. Akin to the one distribution case  we propose the following
X n
DS and LOO versions.

1 from f and m samples Y m
1

DS = T ( ˆf (1)  ˆg(1)) +

ψf (Xi; ˆf (1)  ˆg(1)) +

1

m/2

i=n/2+1

j=m/2+1

ψg(Yj; ˆf (1)  ˆg(1)).

(9)

T ( ˆf−i  ˆg−i) + ψf (Xi; ˆf−i  ˆg−i) + ψg(Yi; ˆf−i  ˆg−i)

.

(10)

n(cid:88)

1

n/2

max(n m)(cid:88)

(cid:16)

1

max(n  m)

i=1

(cid:98)T (1)
(cid:98)TLOO =
compute (cid:98)T (2)

Here  ˆg(1)  ˆg−i are deﬁned similar to ˆf (1)  ˆf−i.

DS and average. For the LOO estimator  if n > m we cycle through the points Y m

1 or vice versa. (cid:98)TLOO is asymmetric when n (cid:54)= m. A seemingly natural

For the DS estimator  we swap the samples to
1 until

we have summed over all X n
alternative would be to sum over all nm pairings of Xi’s and Yj’s. However  this is computationally
more expensive. Moreover  a straightforward modiﬁcation of our proof in Appendix D.2 shows that
both approaches converge at the same rate if n and m are of the same order.
Examples: We demonstrate the generality of our framework by presenting estimators for several
entropies  divergences mutual informations and their conditional versions in Table 1 (Appendix H).
For many functionals in the table  these are the ﬁrst computationally efﬁcient estimators proposed.
We hope this table will serve as a good reference for practitioners. For several functionals (e.g.
conditional and unconditional R´enyi-α divergence  conditional Tsallis-α mutual information) the
estimators are not listed only because the expressions are too long to ﬁt into the table. Our software
implements a total of 17 functionals which include all the estimators in the table. In Appendix F we
illustrate how to apply our framework to derive an estimator for any functional via an example.

(cid:17)

4

As will be discussed in Section 5  when compared to other alternatives  our technique has several
favourable properties: the computational complexity of our method is O(n2) when compared to
O(n3) of other methods; for several functionals we do not require numeric integration; unlike most
other methods [28  32]  we do not require any tuning of hyperparameters.

4 Analysis

Some smoothness assumptions on the densities are warranted to make estimation tractable. We use
the H¨older class  which is now standard in nonparametrics literature.

Deﬁnition 3. Let X ⊂ Rd be a compact space. For any r = (r1  . . .   rd)  ri ∈ N  deﬁne |r| =(cid:80)

i ri

. The H¨older class Σ(s  L) is the set of functions on L2(X ) satisfying 

and Dr =

∂|r|
1 ...∂x

∂xr1

rd
d

|Drf (x) − Drf (y)| ≤ L(cid:107)x − y(cid:107)s−r 

h

for all r s.t. |r| ≤ (cid:98)s(cid:99) and for all x  y ∈ X .
Moreover  deﬁne the Bounded H¨older Class Σ(s  L  B(cid:48)  B) to be {f ∈ Σ(s  L) : B(cid:48) < f < B}.
1 from a d-dimensional density
Note that large s implies higher smoothness. Given n samples X n

f  the kernel density estimator (KDE) with bandwidth h is ˆf (t) = 1/(nhd)(cid:80)n

i=1 K(cid:0) t−Xi

(cid:1). Here

K : Rd → R is a smoothing kernel [35]. When f ∈ Σ(s  L)  by selecting h ∈ Θ(n
−1
2s+d ) the KDE
achieves the minimax rate of OP (n
−2s
2s+d ) in mean squared error. Further  if f is in the bounded
H¨older class Σ(s  L  B(cid:48)  B) one can truncate the KDE from below at B(cid:48) and from above at B and
achieve the same convergence rate [3]. In our analysis  the density estimators ˆf (1)  ˆf−i  ˆg(1)  ˆg−i are
formed by either a KDE or a truncated KDE  and we will make use of these results.
We will also need the following regularity condition on the inﬂuence function. This is satisﬁed for
smooth functionals including those in Table 1. We demonstrate this in our example in Appendix F.
Assumption 4. For a functional T (f ) of one distribution  the inﬂuence function ψ satisﬁes 

E(cid:2)(ψ(X; f(cid:48)) − ψ(X; f ))2(cid:3) ∈ O((cid:107)f − f(cid:48)(cid:107)2) as (cid:107)f − f(cid:48)(cid:107)2 → 0.

(cid:104)
(ψf (X; f(cid:48)  g(cid:48)) − ψf (X; f  g))2(cid:105) ∈ O((cid:107)f − f(cid:48)(cid:107)2 + (cid:107)g − g(cid:48)(cid:107)2) as (cid:107)f − f(cid:48)(cid:107)2 (cid:107)g − g(cid:48)(cid:107)2 → 0.
(cid:104)
(ψg(Y ; f(cid:48)  g(cid:48)) − ψg(Y ; f  g))2(cid:105) ∈ O((cid:107)f − f(cid:48)(cid:107)2 + (cid:107)g − g(cid:48)(cid:107)2) as (cid:107)f − f(cid:48)(cid:107)2 (cid:107)g − g(cid:48)(cid:107)2 → 0.
single distribution achieves MSE E[((cid:98)TDS−T (f ))2] ∈ O(n

For a functional T (f  g) of two distributions  the inﬂuence functions ψf   ψg satisfy 
Ef
Eg
Under the above assumptions  Emery et al. [6]  Robins et al. [30] show that the DS estimator on a
−4s
2s+d +n−1) and further is asymptotically
normal when s > d/2. Their analysis in the semiparametric setting contains the nonparametric
setting as a special case.
In Appendix B we review these results with a simpler self contained
analysis that directly uses the VME and has more interpretable assumptions. An attractive property
of our proof is that it is agnostic to the density estimator used provided it achieves the correct rates.
For the LOO estimator (Equation (8))  we establish the following result.
Theorem 5 (Convergence of LOO Estimator for T (f )). Let f ∈ Σ(s  L  B  B(cid:48)) and ψ satisfy
2s+d ) when s < d/2 and O(n−1) when s ≥ d/2.
The key technical challenge in analysing the LOO estimator (when compared to the DS estimator)
is in bounding the variance as there are several correlated terms in the summation. The bounded
difference inequality is a popular trick used in such settings  but this requires a supremum on the in-
ﬂuence functions which leads to signiﬁcantly worse rates. Instead we use the Efron-Stein inequality
which provides an integrated version of bounded differences that can recover the correct rate when
coupled with Assumption 4. Our proof is contingent on the use of the KDE as the density estimator.

While our empirical studies indicate that (cid:98)TLOO’s limiting distribution is normal (Fig 2(c))  the proof
seems challenging due to the correlation between terms in the summation. We conjecture that (cid:98)TLOO

Assumption 4. Then  E[((cid:98)TLOO − T (f ))2] is O(n

−4s

is indeed asymptotically normal but for now leave it to future work.

5

We reiterate that while the convergence rates are the same for both DS and LOO estimators  the data

splitting degrades empirical performance of (cid:98)TDS as we show in our simulations.

Now we turn our attention to functionals of two distributions. When analysing asymptotics we will
assume that as n  m → ∞  n/(n + m) → ζ ∈ (0  1). Denote N = n + m. For the DS estimator (9)
we generalise our analysis for one distribution to establish the theorem below.
Theorem 6 (Convergence/Asymptotic Normality of DS Estimator for T (f  g)). Let f  g ∈
−4s
2s+d )
when s < d/2 and O(n−1 + m−1) when s ≥ d/2. Further  when s > d/2 and when ψf   ψg (cid:54)= 0 

Σ(s  L  B  B(cid:48)) and ψf   ψg satisfy Assumption 4. Then  E[((cid:98)TDS − T (f  g))2] is O(n
(cid:98)TDS is asymptotically normal 
N ((cid:98)TDS − T (f  g))

Vf [ψf (X; f  g)] +

Vg [ψg(Y ; f  g)]

−4s
2s+d + m

.

(11)

(cid:19)

(cid:18)

√

D−→ N

0 

1
ζ

1
1 − ζ

The convergence rate is analogous to the one distribution case with the estimator achieving the
parametric rate under similar smoothness conditions. The asymptotic normality result allows us to
construct asymptotic conﬁdence intervals for the functional. Even though the asymptotic variance
of the inﬂuence function is not known  by Slutzky’s theorem any consistent estimate of the variance
gives a valid asymptotic conﬁdence interval. In fact  we can use an inﬂuence function based esti-
mator for the asymptotic variance  since it is also a differentiable functional of the densities. We
demonstrate this in our example in Appendix F.
The condition ψf   ψg (cid:54)= 0 is somewhat technical. When both ψf and ψg are zero  the ﬁrst order
terms vanishes and the estimator converges very fast (at rate 1/n2). However  the asymptotic behav-
ior of the estimator is unclear. While this degeneracy occurs only on a meagre set  it does arise for
important choices  such as the null hypothesis f = g in two-sample testing problems.
Finally  for the LOO estimator (10) on two distributions we have the following result. Convergence
is analogous to the one distribution setting and the parametric rate is achieved when s > d/2.
Theorem 7 (Convergence of LOO Estimator for T (f  g)). Let f  g ∈ Σ(s  L  B  B(cid:48)) and ψf   ψg
−4s
2s+d ) when s < d/2 and
O(n−1 + m−1) when s ≥ d/2.
For many functionals  a H¨olderian assumption (Σ(s  L)) alone is sufﬁcient to guarantee the rates in
Theorems 5 6 and 7. However  for some functionals (such as the α-divergences) we require ˆf   ˆg  f  g
to be bounded above and below. Existing results [3  12] demonstrate that estimating such quantities
is difﬁcult without this assumption.
Now we turn our attention to the question of statistical difﬁculty. Via lower bounds given by Birg´e
and Massart [3] and Laurent [14] we know that the DS and LOO estimators are minimax optimal
when s > d/2 for functionals of one distribution. In the following theorem  we present a lower
bound for estimating functionals of two distributions.

satisfy Assumption 4. Then  E[((cid:98)TLOO − T (f  g))2] is O(n

Theorem 8 (Lower Bound for T (f  g)). Let f  g ∈ Σ(s  L) and (cid:98)T be any estimator for T (f  g).

Deﬁne τ = min{8s/(4s + d)  1}. Then there exists a strictly positive constant c such that 

−4s
2s+d + m

E(cid:2)((cid:98)T − T (f  g))2(cid:3) ≥ c(cid:0)n−τ + m−τ(cid:1) .

lim inf

n→∞ inf(cid:98)T

sup

f g∈Σ(s L)

Our proof  given in Appendix E  is based on LeCam’s method [35] and generalises the analysis of
Birg´e and Massart [3] for functionals of one distribution. This establishes minimax optimality of the
DS/LOO estimators for functionals of two distributions when s ≥ d/2. However  when s < d/2
there is a gap between our upper and lower bounds. It is natural to ask if it is possible to improve
on our rates in this regime. A series of work [3  11  14] shows that  for integral functionals of one
distribution  one can achieve the n−1 rate when s > d/4 by estimating the second order term in the
functional Taylor expansion. This second order correction was also done for polynomial functionals
of two distributions with similar statistical gains [12]. While we believe this is possible here  these
estimators are conceptually complicated and computationally expensive – requiring O(n3 + m3)
running time compared to the O(n2 + m2) running time for our estimator. The ﬁrst order estimator
has a favorable balance between statistical and computational efﬁciency. Further  not much is known
about the limiting distribution of second order estimators.

6

Figure 1: Comparison of DS/LOO estimators against alternatives on different functionals. The y-axis is the

error |(cid:98)T − T (f  g)| and the x-axis is the number of samples. All curves were produced by averaging over 50

experiments. Discretisation in hyperparameter selection may explain some of the unsmooth curves.

5 Comparison with Other Approaches

Estimation of statistical functionals under nonparametric assumptions has received considerable at-
tention over the last few decades. A large body of work has focused on estimating the Shannon
entropy– Beirlant et al. [1] gives a nice review of results and techniques. More recent work in the
single-distribution setting includes estimation of R´enyi and Tsallis entropies [17  24]. There are also
several papers extending some of these techniques to divergence estimation [10  12  26  27  37].
Many of the existing methods can be categorised as plug-in methods: they are based on estimating
the densities either via a KDE or using k-Nearest Neighbors (k-NN) and evaluating the functional
on these estimates. Plug-in methods are conceptually simple but unfortunately suffer several draw-
backs. First  they typically have worse convergence rate than our approach  achieving the parametric
rate only when s ≥ d as opposed to s ≥ d/2 [19  32]. Secondly  using either the KDE or k-NN 
obtaining the best rates for plug-in methods requires undersmoothing the density estimate and we
are not aware for principled approaches for selecting this smoothing parameter. In contrast  the
bandwidth used in our estimators is the optimal bandwidth for density estimation so we can select
it using a number of approaches  e.g. cross validation. This is convenient from a practitioners per-
spective as the bandwidth can be selected automatically  a convenience that other estimators do not
enjoy. Secondly  plugin methods based on the KDE always require computationally burdensome
numeric integration. In our approach  numeric integration can be avoided for many functionals of
interest (See Table 1).
Another line of work focuses more speciﬁcally on estimating f-Divergences. Nguyen et al. [22]
estimate f-divergences by solving a convex program and analyse the method when the likelihood
ratio of the densities belongs to an RKHS. Comparing the theoretical results is not straightforward
as it is not clear how to port the RKHS assumption to our setting. Further  the size of the convex
program increases with the sample size which is problematic for large samples. Moon and Hero [21]
use a weighted ensemble estimator for f-divergences. They establish asymptotic normality and the
parametric convergence rate only when s ≥ d  which is a stronger smoothness assumption than is
required by our technique. Both these works only consider f-divergences  whereas our method has
wider applicability and includes f-divergences as a special case.

6 Experiments

We compare the estimators derived using our methods on a series of synthetic examples. We com-
pare against the methods in [8  20  23  26–29  33]. Software for the estimators was obtained either

7

10210310−1n|bT−T|ShannonEntropy1D Plug-inDSLOOkNNKDPVasicek-KDE10210310−1n|bT−T|ShannonEntropy2D Plug-inDSLOOkNNKDPVoronoi10210310−410−310−210−1n|bT−T|KLDivergence Plug-inDSLOOkNN10210310−1100n|bT−T|Renyi-0.75Divergence Plug-inDSLOOkNN10210310−410−310−210−1n|bT−T|HellingerDivergence Plug-inDSLOOkNN10210310−410−310−210−1n|bT−T|Tsallis-0.75Divergence Plug-inDSLOOkNN(a)

(b)

(c)

Figure 2: Fig (a): Comparison of the LOO vs DS estimator on estimating the Conditional Tsallis divergence
in 4 dimensions. Note that the plug-in estimator is intractable due to numerical integration. There are no other
known estimators for the conditional tsallis divergence. Figs (b)  (c): QQ plots obtained using 4000 samples
for Hellinger divergence estimation in 4 dimensions using the DS and LOO estimators respectively.

directly from the papers or from Szab´o [34]. For the DS/LOO estimators  we estimate the density
via a KDE with the smoothing kernels constructed using Legendre polynomials [35]. In both cases
and for the plug in estimator we choose the bandwidth by performing 5-fold cross validation. The
integration for the plug in estimator is approximated numerically.
We test the estimators on a series of synthetic datasets in 1 − 4 dimension. The speciﬁcs of the
densities used in the examples and methods compared to are given in Appendix G. The results are
shown in Figures 1 and 2. We make the following observations. In most cases the LOO estimator
performs best. The DS estimator approaches the LOO estimator when there are many samples but
is generally inferior to the LOO estimator with few samples. This  as we have explained before is
because data splitting does not make efﬁcient use of the data. The k-NN estimator for divergences
[28] requires choosing a k. For this estimator  we used the default setting for k given in the software.
As performance is sensitive to the choice of k  it performs well in some cases but poorly in other
cases. We reiterate that the hyper-parameter of our estimator (bandwidth of the kernel) can be
selected automatically using cross validation.
Next  we test the DS and LOO estimators for asymptotic normality on a 4-dimensional Hellinger
divergence estimation problem. We use 4000 samples for estimation. We repeat this experiment 200
times and compare the empiriical asymptotic distribution (i.e. the

4000((cid:98)T − T (f  g))/(cid:98)S values
where (cid:98)S is the estimated asymptotic variance) to a N (0  1) distribution on a QQ plot. The results in

√

Figure 2 suggest that both estimators are asymptotically normal.
Image clustering: We demonstrate the use of our nonparametric divergence estimators in an image
clustering task on the ETH-80 datset [16]. Using our Hellinger divergence estimator we achieved an
accuracy of 92.47% whereas a naive spectral clustering approach achieved only 70.18%. When we
used a k-NN estimator for the Hellinger divergence [28] we achieved 90.04% which attests to the
superiority of our method. Since this is not the main focus of this work we defer this to Appendix G.

7 Conclusion
We generalise existing results in Von Mises estimation by proposing an empirically superior LOO
technique for estimating functionals and extending the framework to functionals of two distributions.
We also prove a lower bound for the latter setting. We demonstrate the practical utility of our
technique via comparisons against other alternatives and an image clustering application. An open
problem arising out of our work is to derive the limiting distribution of the LOO estimator.

Acknowledgements

This work is supported in part by NSF Big Data grant IIS-1247658 and DOE grant DESC0011114.

References
[1] Jan Beirlant  Edward J. Dudewicz  L´aszl´o Gy¨orﬁ  and Edward C. Van der Meulen. Nonparametric entropy

estimation: An overview. International Journal of Mathematical and Statistical Sciences  1997.

8

10210310−1n|bT−T|ConditionalTsallis-0.75Divergence DSLOO−3−2−10123−3−2−10123QuantilesofN(0 1)Quantilesofn−1/2(bTDS−T)/ˆσ−3−2−10123−3−2−10123QuantilesofN(0 1)Quantilesofn−1/2(bTLOO−T)/ˆσ[2] Peter J. Bickel and Ya’acov Ritov. Estimating integrated squared density derivatives: sharp best order of

convergence estimates. Sankhy¯a: The Indian Journal of Statistics  1988.

[3] Lucien Birg´e and Pascal Massart. Estimation of integral functionals of a density. Ann. of Stat.  1995.
[4] Kevin M. Carter  Raviv Raich  and Alfred O. Hero. On local intrinsic dimension estimation and its

applications. IEEE Transactions on Signal Processing  2010.

[5] Inderjit S. Dhillon  Subramanyam Mallela  and Rahul Kumar. A Divisive Information Theoretic Feature

Clustering Algorithm for Text Classiﬁcation. J. Mach. Learn. Res.  2003.

[6] M Emery  A Nemirovski  and D Voiculescu. Lectures on Prob. Theory and Stat. Springer  1998.
[7] Luisa Fernholz. Von Mises calculus for statistical functionals. Lecture notes in statistics. Springer  1983.
[8] Mohammed Nawaz Goria  Nikolai N Leonenko  Victor V Mergel  and Pier Luigi Novi Inverardi. A new

class of random vector entropy estimators and its applications. Nonparametric Statistics  2005.

[9] Hero  Bing Ma  O. J. J. Michel  and J. Gorman. Applications of entropic spanning graphs. IEEE Signal

Processing Magazine  19  2002.

[10] David K¨allberg and Oleg Seleznjev. Estimation of entropy-type integral functionals. arXiv  2012.
[11] G´erard Kerkyacharian and Dominique Picard. Estimating nonquadratic functionals of a density using

haar wavelets. Annals of Stat.  1996.

[12] Akshay Krishnamurthy  Kirthevasan Kandasamy  Barnabas Poczos  and Larry Wasserman. Nonparamet-

ric Estimation of R´enyi Divergence and Friends. In ICML  2014.

[13] Akshay Krishnamurthy  Kirthevasan Kandasamy  Barnabas Poczos  and Larry Wasserman. On Estimating

L2

2 Divergence. In Artiﬁcial Intelligence and Statistics  2015.

[14] B´eatrice Laurent. Efﬁcient estimation of integral functionals of a density. Ann. of Stat.  1996.
[15] Erik Learned-Miller and Fisher John. ICA using spacings estimates of entropy. Mach. Learn. Res.  2003.
[16] Bastian Leibe and Bernt Schiele. Analyzing Appearance and Contour Based Methods for Object Catego-

rization. In CVPR  2003.

[17] Nikolai Leonenko and Oleg Seleznjev. Statistical inference for the epsilon-entropy and the quadratic

R´enyi entropy. Journal of Multivariate Analysis  2010.

[18] Jeremy Lewi  Robert Butera  and Liam Paninski. Real-time adaptive information-theoretic optimization

of neurophysiology experiments. In NIPS  2006.

[19] Han Liu  Larry Wasserman  and John D Lafferty. Exponential concentration for mutual information

estimation with application to forests. In NIPS  2012.

[20] Erik G Miller. A new class of Entropy Estimators for Multi-dimensional Densities. In ICASSP  2003.
[21] Kevin Moon and Alfred Hero. Multivariate f-divergence Estimation With Conﬁdence. In NIPS  2014.
[22] XuanLong Nguyen  Martin J. Wainwright  and Michael I. Jordan. Estimating divergence functionals and

the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory  2010.

[23] Havva Alizadeh Noughabi and Reza Alizadeh Noughabi. On the Entropy Estimators. Journal of Statisti-

cal Computation and Simulation  2013.

[24] D´avid P´al  Barnab´as P´oczos  and Csaba Szepesv´ari. Estimation of R´enyi Entropy and Mutual Information

Based on Generalized Nearest-Neighbor Graphs. In NIPS  2010.

[25] Hanchuan Peng  Fulmi Long  and Chris Ding. Feature selection based on mutual information criteria of

max-dependency  max-relevance  and min-redundancy. IEEE PAMI  2005.

[26] Fernando P´erez-Cruz. KL divergence estimation of continuous distributions. In IEEE ISIT  2008.
[27] Barnab´as P´oczos and Jeff Schneider. On the estimation of alpha-divergences. In AISTATS  2011.
[28] Barnab´as P´oczos  Liang Xiong  and Jeff G. Schneider. Nonparametric Divergence Estimation with Ap-

plications to Machine Learning on Distributions. In UAI  2011.

[29] David Ramırez  Javier Vıa  Ignacio Santamarıa  and Pedro Crespo. Entropy and Kullback-Leibler Diver-

gence Estimation based on Szegos Theorem. In EUSIPCO  2009.

[30] James Robins  Lingling Li  Eric Tchetgen  and Aad W. van der Vaart. Quadratic semiparametric Von

Mises Calculus. Metrika  2009.

[31] Elad Schneidman  William Bialek  and Michael J. Berry II. An Information Theoretic Approach to the

Functional Classiﬁcation of Neurons. In NIPS  2002.

[32] Shashank Singh and Barnabas Poczos. Exponential Concentration of a Density Functional Estimator. In

NIPS  2014.

[33] Dan Stowell and Mark D Plumbley. Fast Multidimensional Entropy Estimation by k-d Partitioning. IEEE

Signal Process. Lett.  2009.

[34] Zolt´an Szab´o. Information Theoretical Estimators Toolbox. J. Mach. Learn. Res.  2014.
[35] Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer  2008.
[36] Aad W. van der Vaart. Asymptotic Statistics. Cambridge University Press  1998.
[37] Qing Wang  Sanjeev R. Kulkarni  and Sergio Verd´u. Divergence estimation for multidimensional densities

via k-nearest-neighbor distances. IEEE Transactions on Information Theory  2009.

9

,Kirthevasan Kandasamy
Akshay Krishnamurthy
Barnabas Poczos
Larry Wasserman
james robins
Maja Rudolph
Francisco Ruiz
Stephan Mandt
David Blei