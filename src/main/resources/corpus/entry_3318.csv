2013,Machine Teaching for Bayesian Learners in the Exponential Family,What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner?  We propose an optimal teaching framework aimed at learners who employ Bayesian models.  Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher.  This optimization problem is in general hard.  In the case where the learner employs conjugate exponential family models  we present an approximate algorithm for finding the optimal teaching set.  Our algorithm optimizes the aggregate sufficient statistics  then unpacks them into actual teaching examples.  We give several examples to illustrate our framework.,Machine Teaching for Bayesian Learners

in the Exponential Family

Department of Computer Sciences  University of Wisconsin-Madison

Xiaojin Zhu

Madison  WI  USA 53706

jerryzhu@cs.wisc.edu

Abstract

What if there is a teacher who knows the learning goal and wants to design good
training data for a machine learner? We propose an optimal teaching framework
aimed at learners who employ Bayesian models. Our framework is expressed as
an optimization problem over teaching examples that balance the future loss of the
learner and the effort of the teacher. This optimization problem is in general hard.
In the case where the learner employs conjugate exponential family models  we
present an approximate algorithm for ﬁnding the optimal teaching set. Our algo-
rithm optimizes the aggregate sufﬁcient statistics  then unpacks them into actual
teaching examples. We give several examples to illustrate our framework.

1

Introduction

Consider the simple task of learning a threshold classiﬁer in 1D (Figure 1). There is an unknown
threshold θ ∈ [0  1]. For any item x ∈ [0  1]  its label y is white if x < θ and black otherwise.
After seeing n training examples the learner’s estimate is ˆθ. What is the error |ˆθ − θ|? The answer
depends on the learning paradigm. If the learner receives iid noiseless training examples where
x ∼ uniform[0  1]  then with large probability |ˆθ − θ| = O( 1
n ). This is because the inner-most
white and black items are 1/(n + 1) apart on average. If the learner performs active learning and
an oracle provides noiseless labels  then the error reduces faster |ˆθ − θ| = O( 1
2n ) since the optimal
strategy is binary search. However  a helpful teacher can simply teach with n = 2 items (θ −
/2  white)  (θ + /2  black) to achieve an arbitrarily small error . The key difference is that an
active learner still needs to explore the boundary  while a teacher can guide.

Figure 1: Teaching can require far fewer examples than passive or active learning

We impose the restriction that teaching be conducted only via teaching examples (rather than some-
how directly giving the parameter θ to the learner). What  then  are the best teaching examples?
Understanding the optimal teaching strategies is important for both machine learning and education:
(i) When the learner is a human student (as modeled in cognitive psychology)  optimal teaching
theory can design the best lessons for education. (ii) In cyber-security the teacher may be an adver-
sary attempting to mislead a machine learning system via “poisonous training examples.” Optimal
teaching quantiﬁes the power and limits of such adversaries. (iii) Optimal teaching informs robots
as to the best ways to utilize human teaching  and vice versa.

1

θθO(1/2 )nθ{{O(1/n)passive learning "waits"active learning "explores"teaching "guides"Our work builds upon three threads of research. The ﬁrst thread is the teaching dimension theory by
Goldman and Kearns [10] and its extensions in computer science(e.g.  [1  2  11  12  14  25]). Our
framework allows for probabilistic  noisy learners with inﬁnite hypothesis space  arbitrary loss func-
tions  and the notion of teaching effort. Furthermore  in Section 3.2 we will show that the original
teaching dimension is a special case of our framework. The second thread is the research on rep-
resentativeness and pedagogy in cognitive science. Tenenbaum and Grifﬁths is the ﬁrst to note that
representative data is one that maximizes the posterior probability of the target model [22]. Their
work on Gaussian distributions  and later work by Rafferty and Grifﬁths on multinomial distribu-
tions [19]  ﬁnd representative data by matching sufﬁcient statistics. Our framework can be viewed
as a generalization. Speciﬁcally  their work corresponds to the speciﬁc choice (to be deﬁned in Sec-
tion 2) of loss() = KL divergence and eﬀort() being either zero or an indicator function to ﬁx the
data set size at n. We made it explicit that these functions can have other designs. Importantly  we
also show that there are non-trivial interactions between loss() and eﬀort()  such as not-teaching-
at-all in Example 4  or non-brute-force-teaching in Example 5. An interesting variant studied in
cognitive science is when the learner expects to be taught [20  8]. We defer the discussion on this
variant  known as “collusion” in computational teaching theory  and its connection to information
theory to section 5. In addition  our optimal teaching framework may shed light on the optimality
of different method of teaching humans [9  13  17  18]. The third thread is the research on better
ways to training machine learners such as curriculum learning or easy-to-hard ordering of train-
ing items [3  15  16]  and optimal reward design in reinforcement learning [21]. Interactive systems
have been built which employ or study teaching heuristics [4  6]. Our framework provides a unifying
optimization view that balances the future loss of the learner and the effort of the teacher.

2 Optimal Teaching for General Learners

We start with a general framework for teaching and gradually specialize the framework in later
sections. Our framework consists of three entities: the world  the learner  and the teacher. (i) The
world is deﬁned by a target model θ∗. Future test items for the learner will be drawn iid from this
model. This is the same as in standard machine learning. (ii) The learner has to learn θ∗ from
training data. Without loss of generality let θ∗ ∈ Θ  the hypothesis space of the learner (if not  we
can always admit approximation error and deﬁne θ∗ to be the distribution in Θ closest to the world
distribution). The learner is the same as in standard machine learning (learners who anticipate to
be taught are discussed in section 5). The training data  however  is provided by a teacher. (iii)
The teacher is the new entity in our framework. It is almost omnipotent: it knows the world θ∗ 
the learner’s hypothesis space Θ  and importantly how the learner learns given any training data.1
However  it can only teach the learner by providing teaching (or  from the learner’s perspective 
training) examples. The teacher’s goal is to design a teaching set D so that the learner learns θ∗ as
accurately and effortlessly as possible. In this paper  we consider batch teaching where the teacher
presents D to the learner all at once  and the teacher can use any item in the example domain.
Being completely general  we leave many details unspeciﬁed. For instance  the world’s model can
be supervised p(x  y; θ∗) or unsupervised p(x; θ∗); the learner may or may not be probabilistic; and
when it is  Θ can be parametric or nonparametric. Nonetheless  we can already propose a generic
optimization problem for optimal teaching:

loss((cid:99)fD  θ∗) + eﬀort(D).

minD

The function loss() measures the learner’s deviation from the desired θ∗. The quantity(cid:99)fD represents

the state of the learner after seeing the teaching set D. The function eﬀort() measures the difﬁculty
the teacher experiences when teaching with D. Despite its appearance  the optimal teaching prob-
lem (1) is completely different from regularized parameter estimation in machine learning. The
desired parameter θ∗ is known to the teacher. The optimization is instead over the teaching set D.
This can be a difﬁcult combinatorial problem – for instance we need to optimize over the cardinality
of D. Neither is the effort function a regularizer. The optimal teaching problem (1) so far is rather
abstract. For the sake of concreteness we next focus on a rich family of learners  namely Bayesian
models. However  we note that our framework can be adapted to other types of learners  as long as
we know how they react to the teaching set D.

(1)

1This is a strong assumption. It can be relaxed in future work  where the teacher has to estimate the state of

the learner by “probing” it with tests.

2

3 Optimal Teaching for Bayesian Learners

We focus on Bayesian learners because they are widely used in both machine learning and cognitive
science [7  23  24] and because of their predictability: they react to any teaching examples in D by
performing Bayesian updates.2 Before teaching  a Bayesian learner’s state is captured by its prior
distribution p0(θ). Given D  the learner’s likelihood function is p(D | θ). Both the prior and the
likelihood are assumed to be known to the teacher. The learner’s state after seeing D is the posterior

distribution(cid:99)fD ≡ p(θ | D) =(cid:0)(cid:82)

Θ p0(π)p(D | π)dπ(cid:1)−1

p0(θ)p(D | θ).

3.1 The KL Loss and Various Effort Functions  with Examples

we will use the Kullback-Leibler divergence so that loss((cid:99)fD  θ∗) = KL (δθ∗(cid:107)p(θ | D))  where δθ∗

The choice of loss() and eﬀort() is problem-speciﬁc and depends on the teaching goal. In this paper 
is a point mass distribution at θ∗.3 This loss encourages the learner’s posterior to concentrate around
the world model θ∗. With the KL loss  it is easy to verify that the optimal teaching problem (1) can
be equivalently written as

(2)

minD − log p(θ∗ | D) + eﬀort(D).
We remind the reader that this is not a MAP estimate problem. Instead  the intuition is to ﬁnd a good
teaching set D to make θ∗ “stand out” in the posterior distribution.
The eﬀort() function reﬂects resource constraints on the teacher and the learner: how hard is it to
create the teaching examples  to deliver them to the learner  and to have the learner absorb them? For
most of the paper we use the cardinality of the teaching set eﬀort(D) = c|D| where c is a positive
per-item cost. This assumes that the teaching effort is proportional to the number of teaching items 
which is reasonable in many problems. We will demonstrate a few other effort functions in the
examples below.
How good is any teaching set D? We hope D guides the learner’s posterior toward the world’s θ∗ 
but we also hope D takes little effort to teach. The proper quality measure is the objective value (2)
which balances the loss() and eﬀort() terms.
Deﬁnition 1 (Teaching Impedance). The Teaching Impedance (TI) of a teaching set D is the objec-
tive value − log p(θ∗ | D) + eﬀort(D). The lower the TI  the better.

We now give examples to illustrate our optimal teaching framework for Bayesian learners.
Example 1 (Teaching a 1D threshold classiﬁer). The classiﬁcation task is the same as in Figure 1 
with x ∈ [0  1] and y ∈ {−1  1}. The parameter space is Θ = [0  1]. The world has a threshold
θ∗ ∈ Θ. Let the learner’s prior be uniform p0(θ) = 1. The learner’s likelihood function is p(y =
1 | x  θ) = 1 if x ≥ θ and 0 otherwise.
The teacher wants the learner to arrive at a posterior p(θ | D) peaked at θ∗ by designing a small
D. As discussed above  this can be formulated as (2) with the KL loss() and the cardinality eﬀort()
functions: minD − log p(θ∗ | D) + c|D|. For any teaching set D = {(x1  y1)  . . .   (xn  yn)} 
the learner’s posterior is simply p(θ | D) = uniform [maxi:yi=−1(xi)  mini:yi=1(xi)]  namely
uniform over the version space consistent with D.
The optimal teaching problem becomes
minn x1 y1 ... xn yn − log
+ cn. One solution is the limiting case
mini:yi=1(xi)−maxi:yi=−1(xi)
with a teaching set of size two D = {(θ∗ − /2 −1)  (θ∗ + /2  1)} as  → 0  since the Teaching
Impedance T I = log() + 2c approaches −∞. In other words  the teacher teaches by two examples
arbitrarily close to  but on the opposite sides of  the decision boundary as in Figure 1(right).
Example 2 (Learner cannot tell small differences apart). Same as Example 1  but the learner has
poor perception (e.g.  children or robots) and cannot distinguish similar items very well. We may

(cid:16)

(cid:17)

1

2Bayesian learners typically assume that the training data is iid; optimal teaching intentionally violates this
assumption because the designed teaching examples in D will typically be non-iid. However  the learners are
oblivious to this fact and will perform Bayesian update as usual.
3If we allow the teacher to be uncertain about the world θ∗  we may encode the teacher’s own belief as a
distribution p∗(θ) and replace δθ∗ with p∗(θ).

3

c

encode this in eﬀort() as  for example  eﬀort(D) =
minxi xj∈D |xi−xj| . That is  the teaching ex-
amples require more effort to learn if any two items are too close. With two teaching examples
as in Example 1  T I = log() + c/. It attains minimum at  = c. The optimal teaching set is
D = {(θ∗ − c/2 −1)  (θ∗ + c/2  1)}.
Example 3 (Teaching to pick one model out of two). There are two Gaussian distributions θA =
N (− 1
2 ). The learner has Θ = {θA  θB}  and we want to teach it the fact
that the world is using θ∗ = θA. Let the learner have equal prior p0(θA) = p0(θB) = 1
2 . The
learner observes examples x ∈ R  and its likelihood function is p(x | θ) = N (x | θ). Let D =
{x1  . . .   xn}. With these speciﬁc parameters  the KL loss can be shown to be − log p(θ∗ | D) =

2 )  θB = N ( 1

4   1

4   1

log (1 +(cid:81)n
cn +(cid:80)n

i=1 exp(xi)).

i=1

For this example  let us suppose that teaching with extreme item values is undesirable (note
xi → −∞ minimizes the KL loss). We combine cardinality and range preferences in eﬀort(D) =
I(|xi| ≤ d)  where the indicator function I(z) = 0 if z is true  and +∞ otherwise.
In other words  the teaching items must be in some interval [−d  d]. This leads to the optimal
I(|xi| ≤ d). This is a
teaching problem minn x1 ... xn
mixed integer program (even harder–the number of variables has to be optimized as well). We
ﬁrst relax n to real values. By inspection  the solution is to let all xi = −d and let n minimize
T I = log (1 + exp(−dn)) + cn. The minimum is achieved at n = 1
on the left  and showing the learner n copies of −d lends the most support to that model. Note  how-
ever  that n = 0 for certain combinations of c  d (e.g.  when c ≥ d): the effort of teaching outweighs
the beneﬁt. The teacher may choose to not teach at all and maintain the status quo (prior p0) of the
learner!

i=1 exp(xi)) + cn +(cid:80)n
c − 1(cid:1). We then round n
d log(cid:0) d
c − 1(cid:1)(cid:3)(cid:1). This D is sensible: θ∗ = θA is the model

and force nonnegativity: n = max(cid:0)0 (cid:2) 1

log (1 +(cid:81)n
d log(cid:0) d

i=1

3.2 Teaching Dimension is a Special Case

(cid:26) 1 

In this section we provide a comparison to one of the most inﬂuential teaching models  namely the
original teaching dimension theory [10]. It may seem that our optimal teaching setting (2) is more
restrictive than theirs  since we make strong assumptions about the learner (that it is Bayesian  and
the form of the prior and likelihood). Their query learning setting in fact makes equally strong
assumptions  in that the learner updates its version space to be consistent with all teaching items.
Indeed  we can cast their setting as a Bayesian learning problem  showing that their problem is a
special case of (2). Corresponding to the concept class C = {c} in [10]  we deﬁne the conditional
if c(x) = +
probability P (y = 1 | x  θc) =
if c(x) = − and the joint distribution P (x  y | θc) =
P (x)P (y | x  θc) where P (x) is uniform over the domain X . The world has θ∗ = θc∗ corresponding
to the target concept c∗ ∈ C. The learner has Θ = {θc | c ∈ C}. The learner’s prior is p0(θ) =
uniform(Θ) = 1|C|  and its likelihood function is P (x  y | θc). The learner’s posterior after teaching
with D is
P (θc | D) =
(3)
Teaching dimension T D(c∗) is the minimum cardinality of D that uniquely identiﬁes the target
concept. We can formulate this using our optimal teaching framework

(cid:26) 1/(number of concepts in C consistent with D) 

if c is consistent with D
otherwise

0 

0 

minD − log P (θc∗ | D) + γ|D| 

(4)

where we used the cardinality eﬀort() function (and renamed the cost γ for clarity). We can make
sure that the loss term is minimized to 0  corresponding to successfully identifying the target concept 
T D(c∗). But since T D(c∗) is unknown beforehand  we can set γ ≤ 1|C| since |C| ≥ T D(c∗)
if γ < 1
(one can at least eliminate one concept from the version space with each well-designed teaching
item). The solution D to (4) is then a minimum teaching set for the target concept c∗  and |D| =
T D(c∗).

4 Optimal Teaching for Bayesian Learners in the Exponential Family

While we have proposed an optimization-based framework for teaching any Bayesian learner and
provided three examples  it is not clear if there is a uniﬁed approach to solve the optimization

4

s ≡ n(cid:88)

i=1

problem (2). In this section  we further restrict ourselves to a subset of Bayesian learners whose
prior and likelihood are in the exponential family and are conjugate. For this subset of Bayesian
learners  ﬁnding the optimal teaching set D naturally decomposes into two steps: In the ﬁrst step
one solves a convex optimization problem to ﬁnd the optimal aggregate sufﬁcient statistics for D. In
the second step one “unpacks” the aggregate sufﬁcient statistics into actual teaching examples. We
present an approximate algorithm for doing so.
We recall that an exponential family distribution (see e.g. [5]) takes the form p(x | θ) =
θ ∈ RD is the natural parameter  A(θ) is the log partition function  and h(x) modiﬁes the base
measure. For a set D = {x1  . . .   xn}  the likelihood function under the exponential family takes a

h(x) exp(cid:0)θ(cid:62)T (x) − A(θ)(cid:1) where T (x) ∈ RD is the D-dimensional sufﬁcient statistics of x 
similar form p(D | θ) = ((cid:81)n

i=1 h(xi)) exp(cid:0)θ(cid:62)s − nA(θ)(cid:1)  where we deﬁne

min
n s

−θ∗(cid:62)

T (xi)

(5)
to be the aggregate sufﬁcient statistics over D. The corresponding conjugate prior is the ex-
ponential family distribution with natural parameters (λ1  λ2) ∈ RD × R: p(θ | λ1  λ2) =

h0(θ) exp(cid:0)λ(cid:62)
1 θ − λ2A(θ) − A0(λ1  λ2)(cid:1). The posterior distribution is p(θ | D  λ1  λ2) =
h0(θ) exp(cid:0)(λ1 + s)(cid:62)θ − (λ2 + n)A(θ) − A0(λ1 + s  λ2 + n)(cid:1). The posterior has the same form

as the prior but with natural parameters (λ1 + s  λ2 + n). Note that the data D enters the posterior
only via the aggregate sufﬁcient statistics s and cardinality n. If we further assume that eﬀort(D)
can be expressed in n and s  then we can write our optimal teaching problem (2) as

(λ1 + s) + A(θ∗)(λ2 + n) + A0(λ1 + s  λ2 + n) + eﬀort(n  s) 

where n ∈ Z≥0 and s ∈ {t ∈ RD | ∃{xi}i∈I such that t =(cid:80)

(6)
i∈I T (xi)}. We relax the problem
to n ∈ R and s ∈ RD  resulting in a lower bound of the original objective.4 Since the log partition
function A0() is convex in its parameters  we have a convex optimization problem (6) at hand if we
design eﬀort(n  s) to be convex  too. Therefore  the main advantage of using the exponential family
distribution and conjugacy is this convex formulation  which we use to efﬁciently optimize over n
and s. This forms the ﬁrst step in ﬁnding D.
However  we cannot directly teach with the aggregate sufﬁcient statistics. We ﬁrst turn n back into
an integer by max(0  [n]) where [] denotes rounding.5 We then need to ﬁnd n teaching examples
whose aggregate sufﬁcient statistics is s. The difﬁculty of this second “unpacking” step depends
on the form of the sufﬁcient statistics T (x). For some exponential family distributions unpacking
is trivial. For example  the exponential distribution has T (x) = x. Given n and s we can easily
unpack the teaching set D = {x1  . . .   xn} by x1 = . . . = xn = s/n. The Poisson distribution
has T (x) = x as well  but the items x need to be integers. This is still relatively easy to achieve
by rounding x1  . . .   xn and making adjustments to make sure they still sum to s. The univariate
Gaussian distribution has T (x) = (x  x2) and unpacking is harder: given n = 3  s = (3  5) it
may not be immediately obvious that we can unpack into {x1 = 0  x2 = 1  x3 = 2} or even
{x1 = 1
In this paper  we use an approximate unpacking algorithm. We initialize the n teaching examples
iid∼ p(x | θ∗)  i = 1 . . . n. 6 We then improve the examples by solving an unconstrained
by xi
optimization problem to match the examples’ aggregate sufﬁcient statistics to the given s:

}. Clearly  unpacking is not unique.

  x3 = 5−√

2   x2 = 5+

13

4

√

4

13

(cid:107)s − n(cid:88)

i=1

min

x1 ... xn

T (xi)(cid:107)2.

(7)

4For higher solution quality we may impose certain convex constraints on s based on the structure of T (x).
For example  univariate Gaussian has T (x) = (x  x2). Let s = (s1  s2). It is easy to show that s must satisfy
the constraint s2 ≥ s2

5Better results can be obtained by comparing the objective of (6) under several integers around n and picking

1/n.

the smallest one.

6As we will see later  such iid samples from the target distribution are not great teaching examples for two
main reasons: (i) We really should compensate for the learner’s prior by aiming not at the target distribution
but overshooting a bit in the opposite direction of the prior. (ii) Randomness in the samples also prevents them
from achieving the aggregate sufﬁcient statistics.

5

= −2 (s −(cid:80)

This problem is non-convex in general but can be solved up to a local minimum. The gradient is
T (cid:48)(xj). Additional post-processing such as enforcing x to be integers
∂
∂xj
is then carried out if necessary. The complete algorithm is summarized in Algorithm 1.

i T (xi))

(cid:62)

Algorithm 1 Approximately optimal teaching for Bayesian learners in the exponential family
input target θ∗; learner information T ()  A()  A0()  λ1  λ2; eﬀort()

Step 1: Solve for aggregate sufﬁcient statistics n  s by convex optimization (6)
Step 2: Unpacking: n ← max(0  [n]); ﬁnd x1  . . .   xn by (7)

output D = {x1  . . .   xn}

We illustrate Algorithm 1 with several examples.
Example 4 (Teaching the mean of a univariate Gaussian). The world consists of a Gaussian
N (x; µ∗  σ2) where σ2 is ﬁxed and known to the learner while µ∗ is to be taught.
In expo-
nential family form p(x | θ) = h(x) exp (θT (x) − A(θ)) with T (x) = x alone (since σ2
is ﬁxed)  θ = µ
Its con-
jugate prior (which is the learner’s initial state) is Gaussian with the form p(θ | λ1  λ2) =
h0(θ) exp
To ﬁnd a good teaching set D  in step 1 we ﬁrst ﬁnd its optimal cardinality n and aggregate sufﬁcient

2   and h(x) = (cid:0)√

(cid:17)
σ2   A(θ) = µ2
2 − A0(λ)

where A0(λ1  λ2) = λ2
2σ2λ2

2πσ(cid:1)−1

(cid:16)− x2

2σ2 = θ2σ2

2 log(σ2λ2).

λ1θ − λ2

(cid:16)
statistics s =(cid:80)

− 1

(cid:17)

exp

θ2σ2

2σ2

.

1

i∈D xi using (6). The optimization problem becomes
−θ∗s +

(λ1 + s)2

σ2θ∗2

n +

2

2σ2(λ2 + n)

− 1
2

min
n s

log(σ2(λ2 + n)) + eﬀort(n  s)

(8)

where θ∗ = µ∗/σ2. The result is more intuitive if we rewrite the conjugate prior in its standard form
µ ∼ N (µ | µ0  σ2
. With this notation  the optimal aggregate
sufﬁcient statistics is

0) with the relation λ1 = µ0σ2
σ2
0

  λ2 = σ2
σ2
0

(µ∗ − µ0) + µ∗n.

σ2
σ2
0

1

σ2
0

σ2
0

s =

+ n

(cid:17)

2 eﬀort(cid:48)(n) + σ2

2 log σ2(cid:16) σ2

= 0. For example  with the cardinality eﬀort(n) = cn we have n = 1

(9)
n is not the target µ∗  but should
Note an interesting fact here: the average of teaching examples s
compensate for the learner’s initial belief µ0. This is the “overshoot” discussed earlier. Putting (9)
back in (8) the optimization over n is minn − 1
+ eﬀort(n). Consider any differ-
(cid:48)
entiable effort function (w.r.t. the relaxed n) with derivative eﬀort
(n)  the optimal n is the solution
to n−
2c − σ2
.
In step 2 we unpack n and s into D. We discretize n by max(0  [n]). Another interesting fact is that
the optimal teaching strategy may be to not teach at all (n = 0). This is the case when the learner
0 is the learner’s prior variance on the
has literally a narrow mind to start with: σ2
mean). Intuitively  the learner is too stubborn to change its prior belief by much  and such minuscule
change does not justify the teaching effort.
Having picked n  unpacking s is trivial since T (x) = x. For example  we can let D be x1 = . . . =
xn = s/n as discussed earlier  without employing optimization (7). Yet another interesting fact is
that such an alarming teaching set (with n identical examples) is likely to contradict the world’s
model variance σ2  but the discrepancy does not affect teaching because the learner ﬁxes σ2.
Example 5 (Teaching a multinomial distribution). The world is a multinomial distribution π∗ =
(cid:81)K
Γ((cid:80) βk)
K) of dimension K. The learner starts with a conjugate Dirichlet prior p(π | β) =
1  . . .   π∗
(π∗
(cid:81) Γ(βk)
number of teaching items n and the split s = (s1  . . .   sK) where n =(cid:80)K
k=1 πβk−1
. Each teaching item is x ∈ {1  . . .   K}. The teacher needs to decide the total

0 < 2cσ2 (recall σ2

σ2
0

k

k=1 sk.

In step 1  the sufﬁcient statistics is s1  . . .   sK−1 but for clarity we write (6) using s and standard
parameters:

− log Γ

min

s

(βk + sk)

+

(βk + sk − 1) log π∗

k + eﬀort(s). (10)

(cid:32) K(cid:88)

(cid:33)

K(cid:88)

log Γ(βk + sk)− K(cid:88)

k=1

k=1

k=1

6

∂sk

(cid:17)

= −ψ

k=1(βk + sk)

(cid:16)(cid:80)K

+ ψ(βk + sk)− log π∗

This is an integer program; we relax s ∈ RK≥0  making it a continuous optimization problem with
nonnegativity constraints. Assuming a differentiable eﬀort()  the optimal aggregate sufﬁcient statis-
tics can be readily solved with the gradient ∂
k +
∂sk
∂eﬀort(s)
  where ψ() is the digamma function. In step 2  unpacking is again trivial: we simply let

sk ← [sk] for k = 1 . . . K.
Let us look at a concrete problem. Let the teaching target be π∗ = ( 1
10 ). Let the
10   3
If we say that teach-
learner’s prior Dirichlet parameters be quite different: β = (6  3  1).
ing requires no effort by setting eﬀort(s) = 0  then the optimal teaching set D found by Algo-
rithm 1 is s = (317  965  1933) as implemented with Matlab fmincon. The MLE from D is
(0.099  0.300  0.601) and is very close to π∗. In fact  in our experiments  fmincon stopped be-
cause it exceeded the default function evaluation limit. Otherwise  the counts would grow even
higher with MLE→ π∗. This is “brute-force teaching”: using unlimited data to overwhelm the
prior in the learner.

But if we say teaching is costly by setting eﬀort(s) = 0.3(cid:80)K

k=1 sk  the optimal D found by Al-
gorithm 1 is instead s = (0  2  8) with merely ten items. Note that it did not pick (1  3  6) which
also has ten items and whose MLE is π∗: this is again to compensate for the biased prior Dir(β)
in the learner. Our optimal teaching set (0  2  8) has Teaching Impedance T I = 2.65. In contrast 
the set (1  3  6) has T I = 4.51 and the previous set (317  965  1933) has T I = 956.25 due to its
size. We can also attempt to sample teaching sets of size ten from multinomial(10  π∗). In 100 000
simulations with such random teaching sets the average T I = 4.97 ± 1.88 (standard deviation) 
minimum T I = 2.65  and maximum T I = 18.7. In summary  our optimal teaching set (0  2  8) is
very good.

10   6

(cid:17)

(cid:16) mk1

We remark that one can teach complex models using simple ones as building blocks. For instance 
with the machinery in Example 5 one can teach the learner a full generative model for a Na¨ıve Bayes
classiﬁer. Let the target Na¨ıve Bayes classiﬁer have K classes with class probability p(y = k) = π∗
k.
Let v be the vocabulary size. Let the target class conditional probability be p(x = i | y = k) =
θ∗
ki for word type i = 1 . . . v and label k = 1 . . . K. Then the aggregate sufﬁcient statistics are
n1 . . . nK  m11 . . . m1v  . . .   mK1 . . . mKv where nk is the number of documents with label k  and
mki is the number of times word i appear in all documents with label k. The optimal choice of
these n’s and m’s for teaching can be solved separately as in Example 5 as long as eﬀort() can be
separated. The unpacking step is easy: we know we need nk teaching documents with label k. These
nk documents together need mki counts of word type i. They can evenly split those counts. In the
end  each teaching document with label k will have the bag-of-words
  subject to
rounding.
Example 6 (Teaching a multivariate Gaussian). Now we consider the general case of
teaching both the mean and the covariance of a multivariate Gaussian.
The world
has the target µ∗ ∈ RD and Σ∗ ∈ RD×D.
| µ  Σ).
(cid:18)
(cid:16)(cid:81)D
i=1 Γ(cid:0) ν0+1−i
|
The
starts with a Normal-Inverse-Wishart
learner
2 (µ − µ0)(cid:62)Σ−1(µ − µ0)(cid:1).
exp(cid:0)− 1
|Σ|− ν0+D+2
µ0  κ0  ν0  Λ−1
0 )
i=1 xi  S = (cid:80)n
aggregate sufﬁcient statistics are s = (cid:80)n
2 tr(Σ−1Λ0) − κ0
the
i=1 xix(cid:62)
The posterior is NIW
i .
p(µ  Σ | µn  κn  νn  Λ−1
n ) with parameters µn = κ0
κ0+n s  κn = κ0 + n  νn = ν0 + n 
κ0+n µ0 + 1
0 − 2κ0
κ0+n µ0s(cid:62) − 1
κ0+n µ0µ(cid:62)
κ0+n ss(cid:62). We formulate the optimal aggregate
Λn = Λ0 + S + κ0n
sufﬁcient statistics problem by putting the posterior into (6). Note S by deﬁnition needs to be
positive semi-deﬁnite. In addition  with Cauchy-Schwarz inequality one can show that Sii ≥ s2
(cid:19)
(cid:18) νn + 1 − i
i /2
D(cid:88)
for i = 1 . . . n. Step 1 is thus the following SDP:

Given data x1  . . .   xn ∈ RD 

The likelihood is N (x
(NIW)

(cid:1)(cid:17)|Λ0|− ν0

2(cid:19)−1
(cid:17) D

conjugate prior p(µ  Σ

(cid:16) 2π

  . . .   mkv
nk

D log 2

ν0D
2 π

D(D−1)

=

nk

2

κ0

2

2

4

2

− νn
2

log |Λn| − D
2

log κn +

log |Σ∗|

νn
2

2

(µ∗ − µn)(cid:62)Σ∗−1(µ∗ − µn) + eﬀort(n  s  S)

min
n s S

s.t.

νn +

log Γ

i=1

2
tr(Σ∗−1Λn) +
Sii ≥ s2

+
S (cid:23) 0;

1
2

κn
2

i /2  ∀i.

7

(11)

(12)

iid∼ N (µ∗  Σ∗). Again  such iid samples are
In step 2  we unpack s  S by initializing x1  . . .   xn
typically not good teaching examples. We improve them with the optimization (7) where T (x) is the
(D + D2)-dim vector formed by the elements of x and xx(cid:62)  and similarly the aggregate sufﬁcient
statistics vector s is formed by the elements of s and S.
We illustrate the results on a concrete problem in D = 3. The target Gaussian is µ∗ = (0  0  0) and
Σ∗ = I. The target mean is visualized in each plot of Figure 2 as a black dot. The learner’s initial
state is captured by the NIW with parameters µ0 = (1  1  1)  κ0 = 1  ν0 = 2 + 10−5  Λ0 = 10−5I.
Note the learner’s prior mean µ0 is different than µ∗  and is shown by the red dot in Figure 2. The
red dot has a stem extending to the z-axis=0 plane for better visualization. We used an “expensive”
effort function eﬀort(n  s  S) = n. Algorithm 1 decides to use n = 4 teaching examples with s =
. These unpack into D = {x1 . . . x4}  visualized by the
(−1 −1 −1) and S =
four empty blue circles. The three panels of Figure 2 show unpacking results starting from different
initial seeds sampled from N (µ∗  Σ∗). These teaching examples form a tetrahedron (edges added for
clarity). This is sensible: in fact  one can show that the minimum teaching set for a D-dimensional
Gaussian is the D + 1 points at the vertices of a D-dimensional tetrahedron. Importantly the mean
of D  (−1/4 −1/4 −1/4) shown as the solid blue dot with a stem  is offset from the target µ∗ and
to the opposite side of the learner’s prior µ0. This again shows that D compensates for the learner’s
prior. Our optimal teaching set D has T I = 1.69. In contrast  teaching sets with four iid random
samples from the target N (µ∗  Σ∗) have worse TI. In 100 000 simulations such random teaching
sets have average T I = 9.06 ± 3.34  minimum T I = 1.99  and maximum T I = 35.51.

(cid:32)4.63 −1 −1

(cid:33)

−1
−1 −1

4.63 −1
4.63

Figure 2: Teaching a multivariate Gaussian

5 Discussions and Conclusion

What if the learner anticipates teaching? Then the teaching set may be further reduced. For ex-
ample  the task in Figure 1 may only require a single teaching example D = {x1 = θ∗}  and the
learner can ﬁgure out that this x1 encodes the decision boundary. Smart learning behaviors simi-
lar to this have been observed in humans by Shafto and Goodman [20]. In fact  this is known as
“collusion” in computational teaching theory (see e.g. [10])  and has strong connections to compres-
sion in information theory. In one extreme of collusion  the teacher and the learner agree upon an
information-theoretical coding scheme beforehand. Then  the teaching set D is not used in a tradi-
tional machine learning training set sense  but rather as source coding. For example  x1 itself would
be a ﬂoating-point encoding of θ∗ up to machine precision. In contrast  the present paper assumes
that the learner does not collude.
We introduced an optimal teaching framework that balances teaching loss and effort. we hope this
paper provides a “stepping stone” for follow-up work  such as 0-1 loss() for classiﬁcation  non-
Bayesian learners  uncertainty in learner’s state  and teaching materials beyond training items.

Acknowledgments

We thank Bryan Gibson  Robert Nowak  Stephen Wright  Li Zhang  and the anonymous reviewers
for suggestions that improved this paper. This research is supported in part by National Science
Foundation grants IIS-0953219 and IIS-0916038.

8

−101−202−1−0.500.51−101−202−101−101−202−2−1.5−1−0.500.51References
[1] D. Angluin. Queries revisited. Theor. Comput. Sci.  313(2):175–194  2004.
[2] F. J. Balbach and T. Zeugmann. Teaching randomized learners.

In COLT  pages 229–243.

Springer  2006.

[3] Y. Bengio  J. Louradour  R. Collobert  and J. Weston. Curriculum learning. In ICML  2009.
[4] B. Biggio  B. Nelson  and P. Laskov. Poisoning attacks against support vector machines. In

ICML  2012.

[5] L. D. Brown. Fundamentals of statistical exponential families: with applications in statistical

decision theory. Institute of Mathematical Statistics  Hayworth  CA  USA  1986.

[6] M. Cakmak and M. Lopes. Algorithmic and human teaching of sequential decision tasks. In

AAAI Conference on Artiﬁcial Intelligence  2012.

[7] N. Chater and M. Oaksford. The probabilistic mind: prospects for Bayesian cognitive science.

OXFORD University Press  2008.

[8] M. C. Frank and N. D. Goodman. Predicting Pragmatic Reasoning in Language Games. Sci-

ence  336(6084):998  May 2012.

[9] G. Gigu`ere and B. C. Love. Limits in decision making arise from limits in memory retrieval.

Proceedings of the National Academy of Sciences  Apr. 2013.

[10] S. Goldman and M. Kearns. On the complexity of teaching. Journal of Computer and Systems

Sciences  50(1):20–31  1995.

[11] S. Hanneke. Teaching dimension and the complexity of active learning. In COLT  page 6681 

2007.

[12] T. Heged¨us. Generalized teaching dimensions and the query complexity of learning. In COLT 

pages 108–117  1995.

[13] F. Khan  X. Zhu  and B. Mutlu. How do humans teach: On curriculum learning and teaching

dimension. In Advances in Neural Information Processing Systems (NIPS) 25. 2011.

[14] H. Kobayashi and A. Shinohara. Complexity of teaching by a restricted number of examples.

In COLT  pages 293–302  2009.

[15] M. P. Kumar  B. Packer  and D. Koller. Self-paced learning for latent variable models. In NIPS 

2010.

[16] Y. J. Lee and K. Grauman. Learning the easy things ﬁrst: Self-paced visual category discovery.

In CVPR  2011.

[17] B. D. McCandliss  J. A. Fiez  A. Protopapas  M. Conway  and J. L. McClelland. Success
and failure in teaching the [r]-[l] contrast to Japanese adults: Tests of a Hebbian model of
plasticity and stabilization in spoken language perception. Cognitive  Affective  & Behavioral
Neuroscience  2(2):89–108  2002.

[18] H. Pashler and M. C. Mozer. When does fading enhance perceptual category learning? Journal

of Experimental Psychology: Learning  Memory  and Cognition  2013. In press.

[19] A. N. Rafferty and T. L. Grifﬁths. Optimal language learning: The importance of starting

representative. 32nd Annual Conference of the Cognitive Science Society  2010.

[20] P. Shafto and N. Goodman. Teaching Games: Statistical Sampling Assumptions for Learning

in Pedagogical Situations. In CogSci  pages 1632–1637  2008.

[21] S. Singh  R. L. Lewis  A. G. Barto  and J. Sorg. Intrinsically motivated reinforcement learning:

An evolutionary perspective. IEEE Trans. on Auton. Ment. Dev.  2(2):70–82  June 2010.

[22] J. B. Tenenbaum and T. L. Grifﬁths. The rational basis of representativeness. 23rd Annual

Conference of the Cognitive Science Society  2001.

[23] J. B. Tenenbaum  T. L. Grifﬁths  and C. Kemp. Theory-based Bayesian models of inductive

learning and reasoning. Trends in Cognitive Sciences  10(7):309–318  2006.

[24] F. Xu and J. B. Tenenbaum. Word learning as Bayesian inference. Psychological review 

114(2)  2007.

[25] S. Zilles  S. Lange  R. Holte  and M. Zinkevich. Models of cooperative teaching and learning.

Journal of Machine Learning Research  12:349–384  2011.

9

,Jerry Zhu
Damien Garreau
Rémi Lajugie
Sylvain Arlot
Francis Bach
Steven Cheng-Xian Li
Benjamin Marlin