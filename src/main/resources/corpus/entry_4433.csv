2013,Probabilistic Movement Primitives,Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. Many state-of-the-art robot learning successes are based MPs  due to their compact representation of the inherently continuous and high dimensional robot movements. A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. To this effect  a MP representation has to allow for blending between motions  adapting to altered task variables  and co-activating multiple MPs in parallel. We present a probabilistic formulation of the MP concept that maintains a distribution over trajectories. Our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework. In order to use such a trajectory distribution for robot movement control  we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios.,Probabilistic Movement Primitives

Alexandros Paraschos  Christian Daniel  Jan Peters  and Gerhard Neumann

Intelligent Autonomous Systems  Technische Universität Darmstadt

{paraschos daniel peters neumann}@ias.tu-darmstadt.de

Hochschulstr. 10  64289 Darmstadt  Germany

Abstract

Movement Primitives (MP) are a well-established approach for representing mod-
ular and re-usable robot movement generators. Many state-of-the-art robot learn-
ing successes are based MPs  due to their compact representation of the inherently
continuous and high dimensional robot movements. A major goal in robot learn-
ing is to combine multiple MPs as building blocks in a modular control architec-
ture to solve complex tasks. To this effect  a MP representation has to allow for
blending between motions  adapting to altered task variables  and co-activating
multiple MPs in parallel. We present a probabilistic formulation of the MP con-
cept that maintains a distribution over trajectories. Our probabilistic approach
allows for the derivation of new operations which are essential for implementing
all aforementioned properties in one framework. In order to use such a trajectory
distribution for robot movement control  we analytically derive a stochastic feed-
back controller which reproduces the given trajectory distribution. We evaluate
and compare our approach to existing methods on several simulated as well as
real robot scenarios.

1

Introduction

Movement Primitives (MPs) are commonly used for representing and learning basic movements
in robotics  e.g.  hitting and batting  grasping  etc.
[1  2  3]. MP formulations are compact pa-
rameterizations of the robot’s control policy. Modulating their parameters permits imitation and
reinforcement learning as well as adapting to different scenarios. MPs have been used to solve
many complex tasks  including ‘Ball-in-the-Cup’ [4]  Ball-Throwing [5  6]  Pancake-Flipping [7]
and Tetherball [8].
The aim of MPs is to allow for composing complex robot skills out of elemental movements with a
modular control architecture. Hence  we require a MP architecture that supports parallel activation
and smooth blending of MPs for composing complex movements of sequentially [9] and simulta-
neously [10] activated primitives. Moreover  adaptation to a new task or a new situation requires
modulation of the MP to an altered desired target position  target velocity or via-points [3]. Ad-
ditionally  the execution speed of the movement needs to be adjustable to change the speed of  for
example  a ball-hitting movement. As we want to learn the movement from data  another crucial re-
quirement is that the parameters of the MPs should be straightforward to learn from demonstrations
as well as through trial and error for reinforcement learning approaches. Ideally  the same archi-
tecture is applicable for both stroke-based and periodic movements  and capable of representing
optimal behavior in deterministic and stochastic environments.
While many of these properties are implemented by one or more existing MP architectures [1  11 
10  2  12  13  14  15]  no approach exists which exhibits all of these properties in one framework. For
example  [13] also offers a probabilistic interpretation of MPs by representing an MP as a learned
graphical model. However  this approach heavily depends on the quality of the used planner and the

1

movement can not be temporally scaled. Rozo et. al. [12  16] use a combination of primitives  yet 
their control policy of the MP is based on heuristics and it is unclear how the combination of MPs
affects the resulting movements.
In this paper  we introduce the concept of probabilistic movement primitives (ProMPs) as a general
probabilistic framework for representing and learning MPs. Such a ProMP is a distribution over
trajectories. Working with distributions enables us to formulate the described properties by oper-
ations from probability theory. For example  modulation of a movement to a novel target can be
realized by conditioning on the desired target’s positions or velocities. Similarly  consistent parallel
activation of two elementary behaviors can be accomplished by a product of two independent trajec-
tory probability distributions. Moreover  a trajectory distribution can also encode the variance of the
movement  and  hence  a ProMP can often directly encode optimal behavior in stochastic systems
[17]. Finally  a probabilistic framework allows us to model the covariance between trajectories of
different degrees of freedom  that can be used to couple the joints of the robot.
Such properties of trajectory distributions have so far not been properly exploited for representing
and learning MPs. The main reason for the absence of such an approach has been the difﬁculty of
extracting a policy for controlling the robot from a trajectory distribution. We show how this step can
be accomplished and derive a control policy that exactly reproduces a given trajectory distribution.
To the best of our knowledge  we present the ﬁrst principled MP approach that can exploit the power
of operations from probability theory.
While the ProMPs’ representation introduces many novel components  it incorporates many ad-
vantages from well-known previous movement primitive representations [18  10]  such as phase
variables for timing of the movement that enable temporal rescaling of movements  and the ability
to represent both rhythmic and stroke based movements. However  since ProMPs incorporate the
variance of demonstrations  the increased ﬂexibility and advantageous properties of the representa-
tion come at the price of requiring multiple demonstrations to learn the primitives as opposed to past
approaches [18  3] that can clone movements from a single demonstration.

2 Probabilistic Movement Primitives (ProMPs)

Table 1: Desirable properties and their implemen-
tation in the ProMP

A movement primitive representation should
exhibit several desirable properties  such as co-
activation  adaptability and optimality in order
to be a powerful MP representation. The goal
of this paper is to unify these properties in one
framework. We accomplish this objective by
using a probabilistic formulation for MPs. We
summarized all the properties and how they are
implemented in our framework in Table 1. In
this section  we will sequentially explain the
importance of each of these property and dis-
cuss the implementation in our framework. As
crucial part of our objective  we will introduce
conditioning and a product of ProMPs as new
operations that can be applied on the ProMPs due to the probabilistic formulation. Finally  we show
how to derive a controller which follows a given trajectory distribution.

Property
Co-Activation
Modulation
Optimality
Coupling
Learning
Temporal Scaling
Rhythmic Movements

Implementation

Product

Conditioning

Encode variance
Mean  Covariance
Max. Likelihood
Modulate Phase
Periodic Basis

2.1 Probabilistic Trajectory Representation
We model a single movement execution as a trajectory τ = {qt}t=0...T   deﬁned by the joint angles
qt over time.
In our framework  a MP describes multiple ways to execute a movement  which
naturally leads to a probability distribution over trajectories.

Encoding a Time-Varying Variance of Movements. Our movement primitive representation
models the time-varying variance of the trajectories to be able to capture multiple demonstrations
with high-variability. Representing the variance information is crucial as it reﬂects the importance of

2

single time points for the movement execution and it is often a requirement for representing optimal
behavior in stochastic systems [17].
We use a weight vector w to compactly represent a single trajectory. The probability of observing a
trajectory τ given the underlying weight vector w is given as a linear basis function model

p(τ|w) =(cid:81)

tN(cid:16)

yt|ΦT

(cid:17)

(cid:20) qt

(cid:21)

 

˙qt

= ΦT

yt =

t w  Σy

t w + y 

(1)
where Φt = [φt  ˙φt] deﬁnes the n × 2 dimensional time-dependent basis matrix for the joint posi-
tions qt and velocities ˙qt  n deﬁnes the number of basis functions and y ∼ N (0  Σy) is zero-mean
i.i.d. Gaussian noise. By weighing the basis functions Ψt with the parameter vector w  we can
represent the mean of a trajectory.
In order to capture the variance of the trajectories  we introduce a distribution p(w; θ) over the
weight vector w  with parameters θ. The trajectory distribution p(τ ; θ) can now be computed
p(τ|w)p(w; θ)dw. The distribution
by marginalizing out the weight vector w  i.e.  p(τ ; θ) =
p(τ ; θ) deﬁnes a Hierarchical Bayesian Model (HBM) whose parameters are given by the observa-
tion noise variance Σy and the parameters θ of p(w; θ).

´

Temporal Modulation. Temporal modulation is needed for a faster or slower execution of the
movement. We introduce a phase variable z to decouple the movement from the time signal as for
previous non-probabilistic approaches [18]. The phase can be any function monotonically increasing
with time z(t). By modifying the rate of the phase variable  we can modulate the speed of the
movement. Without loss of generality  we deﬁne the phase as z0 = 0 at the beginning of the
movement and as zT = 1 at the end. The basis functions φt now directly depend on the phase
(cid:48)
instead of time  such that φt = φ(zt) and the corresponding derivative becomes ˙φt = φ

(zt) ˙zt.

Rhythmic and Stroke-Based Movements. The choice of the basis functions depends on the type
of movement  which can be either rhythmic or stroke-based. For stroke-based movements  we use
Gaussian basis functions bG
i   while for rhythmic movements we use Von-Mises basis functions bVM
to model periodicity in the phase variable z  i.e. 

i

(cid:18) cos(2π(zt − ci))

(cid:19)

h

 

bVM
i

(z) = exp

 

(2)

(cid:18)
− (zt − ci)2
basis functions with φi(zt) = bi(z)/(cid:80)

bG
i (z) = exp

2h

(cid:19)

j bj(z).

where h deﬁnes the width of the basis and ci the center for the ith basis function. We normalize the

Encoding Coupling between Joints. So far  we have considered each degree of freedom to be
modeled independently. However  for many tasks we have to coordinate the movement of the joints.
A common way to implement such coordination is via the phase variable zt that couples the mean of
the trajectory distribution [18]. Yet  it is often desirable to also encode higher-order moments of the
coupling  such as the covariance of the joints at time point t. Hence  we extend our model to multiple
dimensions. For each dimension i  we maintain a parameter vector wi  and we deﬁne the combined 
n ]T . The basis matrix Φt now extends to a block-diagonal
weight vector w as w = [wT
matrix containing the basis functions and their derivatives for each dimension. The observation
vector yt consists of the angles and velocities of all joints. The probability of an observation y at
time t is given by

1   . . .   wT


 y1 t

...
yd t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

 ΦT

...

t

0

0

. . .
...
...
··· ΦT

t

p(yt|w) = N

 w  Σy

 = N (yt|Ψtw  Σy)

(3)

where yi t = [qi t  ˙qi t]T denotes the joint angle and velocity for the ith joint. We now maintain a
distribution p(w; θ) over the combined parameter vector w. Using this distribution  we can also
capture the covariance between joints.

Learning from Demonstrations. One crucial requirement of a MP representation is that the pa-
rameters of a single primitive are easy to acquire from demonstrations. To facilitate the estimation

3

ˆ

yt|ΨT

N(cid:16)

(cid:17)N (w|µw  Σw) dw = N(cid:16)

of the parameters  we will assume a Gaussian distribution for p(w; θ) = N (w|µw  Σw) over the
parameters w. Consequently  the distribution of the state p(yt|θ) for time step t is given by
t ΣwΨt + Σy
p (yt; θ) =
and  thus  we can easily evaluate the mean and the variance for any time point t. As a ProMP
represents multiple ways to execute an elemental movement  we also need multiple demonstrations
to learn p(w; θ). The parameters θ = {µw  Σw} can be learned from multiple demonstrations by
maximum likelihood estimation  for example  by using the expectation maximization algorithm for
HBMs with Gaussian distributions [19].

t µw  ΨT

yt|ΨT

t w  Σy

  (4)

(cid:17)

2.2 New Probabilistic Operators for Movement Primitives

The ProMPs allow for the formulation of new operators from probability theory  e.g.  conditioning
for modulating the trajectory and a product of distributions for co-activating MPs. We will now
describe both operators in our general framework and  subsequently  discuss their implementation
for our speciﬁc choice of Gaussian distributions for p(w; θ).

t ) ∝ N(cid:16)

Modulation of Via-Points  Final Positions or Velocities by Conditioning. The modulation of
via-points and ﬁnal positions are important properties of any MP framework such that the MP can
be adapted to new situations. In our probabilistic formulation  such operations can be described
by conditioning the MP to reach a certain state y∗
t at time t. Conditioning is performed by adding
a desired observation xt = [y∗
y] to our probabilistic model and applying Bayes theorem  i.e. 
p(w|x∗
t w  Σ∗
t represents the desired position and veloc-
ity vector at time t and Σ∗
y describes the accuracy of the desired observation. We can also condition
on any subset of y∗
t . For example  by specifying a desired joint position q1 for the ﬁrst joint the
trajectory distribution will automatically infer the most probable joint positions for the other joints.
For Gaussian trajectory distributions the conditional distribution p (w|x∗
t ) for w is Gaussian with
mean and variance

(cid:17)
t   Σ∗
p(w). The state vector y∗

t|ΨT
y∗

y

(cid:16)
(cid:16)

µ[new]

w

= µw + ΣwΨt
= Σw − ΣwΨt

Σ∗
y + ΨT
Σ∗
y + ΨT

(cid:17)−1(cid:16)
(cid:17)−1

(cid:17)

t ΣwΨt

t − ΨT
y∗

t µw

 

(5)

Σ[new]

w

(6)
Conditioning a ProMP to different target states is also illustrated in Figure 1(a). We can see that  de-
spite the modulation of the ProMP by conditioning  the ProMP stays within the original distribution 
and  hence  the modulation is also learned from the original demonstrations. Modulation strategies
in current approaches such as the DMPs do not show this beneﬁcial effect [18].

t ΣwΨt

t Σw.

ΨT

the products of distributions  i.e.  pnew(τ ) ∝ (cid:81)

Combination and Blending of Movement Primitives. Another beneﬁcial probabilistic operation
is to continuously combine and blend different MPs into a single movement. Suppose that we
maintain a set of i different primitives that we want to combine. We can co-activate them by taking
ipi(τ )α[i]where theα[i] ∈ [0  1] factors denote the
activation of the ith primitive. This product captures the overlapping region of the active MPs  i.e. 
the part of the trajectory space where all MPs have high probability mass.
However  we also want to be able to modulate the activations of the primitives  for example  to
continuously blend the movement execution from one primitive to the next. Hence  we decompose
the trajectory into single time steps and use time-varying activation functions α[i]
pi(yt|w[i])pi(w[i])dw[i].
(cid:17)−1

(7)
t )  the resulting distribution p∗(yt) is again

For Gaussian distributions pi(yt) = N (yt|µ[i]
Gaussian with variance and mean

p∗(τ ) ∝(cid:81)
(cid:18)(cid:80)
(cid:16)

(cid:17)−1(cid:19)−1

pi(yt) =
t   Σ[i]

(cid:18)(cid:80)

ipi(yt)α[i]
t  

t   i.e. 

(cid:81)

(cid:19)

(cid:16)

−1

´

t

Σ∗
t =

Σ[i]

t /α[i]

t

i

Σ[i]

t /α[i]

t

i

µ[i]
t

  µ∗

t = (Σ∗
t )

(8)

Both terms  and their derivatives  are required to obtain the stochastic feedback controller which is
ﬁnally used to control the robot. We illustrated the co-activation of two ProMPs in Figure 1(b) and
the blending of two ProMPs in Figure 1(c).

4

(a) Conditioning

(b) Combination

(c) Blending

Figure 1: (a) Conditioning on different target states. The blue shaded area represents the learned
trajectory distribution. We condition on different target positions  indicated by the ‘x’-markers. The
produced trajectories exactly reach the desired targets while keeping the shape of the demonstrations.
(b) Combination of two ProMPs. The trajectory distributions are indicated by the blue and red
shaded areas. Both primitives have to reach via-points at different points in time  indicated by
the ‘x’-markers. We co-activate both primitives with the same activation factor. The trajectory
distribution generated by the resulting feedback controller now goes through all four via-points.
(c) Blending of two ProMPs. We smoothly blend from the red primitive to the blue primitive. The
activation factors are shown in the bottom. The resulting movement (green) ﬁrst follows the red
primitive and  subsequently  switches to following the blue primitive.

2.3 Using Trajectory Distributions for Robot Control

In order to fully exploit the properties of trajectory distributions  a policy for controlling the robot
is needed that reproduces these distributions. To this effect  we analytically derivate a stochastic
feedback controller that can accurately reproduce the mean vectors µt and the variances Σt for all t
of a given trajectory distribution.
We follow a model-based approach. First  we approximate the continuous time dynamics of the
system by a linearized discrete-time system with step duration dt 

(9)
where the system matrices At  the input matrices Bt and the drift vectors ct can be obtained by ﬁrst
order Taylor expansion of the dynamical system1. We assume a stochastic linear feedback controller
with time varying feedback gains is generating the control actions  i.e. 

yt+dt = (I + Atdt) yt + Btdtu + ctdt 

(10)
where the matrix Kt denotes a feedback gain matrix and kt a feed-forward component. We use a
control noise which behaves like a Wiener process [21]  and  hence  its variance grows linearly with
the step duration2 dt. By substituting Eq. (10) into Eq. (9)  we rewrite the next state of the system as

u = Ktyt + kt + u 

 ∼ N (u|0  Σu/dt)  

yt+dt = (I + (At + BtKt) dt) yt + Btdt(kt + u) + cdt = F tyt + f t + Btdtu 

with F t = (I + (At + BtKt) dt)  

(11)
For improved clarity  we will omit the time-index as subscript for most matrices in the remainder
of the paper. From Eq. 4 we know that the distribution for our current state yt is Gaussian with
mean µt = ΨT
t ΣwΨt. As the system dynamics are modeled by a
Gaussian linear model  we can obtain the distribution of the next state p (yt+dt) analytically from
the forward model

t µw and covariance3 Σt = ΨT

f t = Btktdt + cdt.

p(cid:0)yt+dt

(cid:1) =
N(cid:0)yt+dt|F yt + f   Σsdt(cid:1)N (yt|µt  Σt) dyt
ˆ
=N(cid:16)

yt+dt|F µt + f   F ΣtF T + Σsdt

(cid:17)

 

(12)

1If inverse dynamics control [20] is used for the robot  the system reduces to a linear system where the terms

At  Bt and ct are constant in time.

obtain this desired behavior.

next state.

2As we multiply the noise by Bdt  we need to divide the covariance Σu of the control noise u by dt to

3The observation noise is omitted as it represents independent noise which is not used for predicting the

5

time [s]00.30.71q[rad]time[s]00.30.71-2-10123Demonstration1Demonstration2Combination00.30.7101α1α2q [rad]00.30.71-2-10123Demonstration 1Demonstration 2Blending00.30.7101α1α2where dtΣs = dtBΣuBT represents the system noise matrix. Both sides of Eq. 12 are Gaussian
distributions  where the left-hand side can also be computed by our desired trajectory distribution
p(τ ; θ). We match the mean and the variances of both sides with our control law  i.e. 

(13)
where F is given in Eq. (11) and contains the time varying feedback gains K. Using both con-
straints  we can now obtain the time dependend gains K and k.

µt+dt = F µt + (Bk + c)dt 

Σt+dt = F ΣtF T + Σsdt 

Derivation of the Controller Gains. By rearranging terms  the covariance constraint becomes

Σt+dt − Σt = Σsdt + (A + BK) Σtdt + Σt (A + BK)T dt + O(dt2) 

(14)
where O(dt2) denotes all second order terms in dt. After dividing by dt and taking the limit of
dt → 0  the second order terms disappear and we obtain the time derivative of the covariance

˙Σt = lim
dt→0

Σt+dt − Σt

dt

= (A + BK)Σt + Σt(A + BK)T + Σs.

(15)

The matrix ˙Σt can also be obtained from the trajectory distribution ˙Σt = ˙Ψ
which we substitute into Eq. (15). After rearranging terms  the equation reads

T
t ΣwΨt + ΨT

t Σw ˙Ψt 

Setting M = BKΣt and solving for the gain matrix K

M + M T = BKΣt + (BKΣt)T   with M= ˙ΦtΣwΦT

t -AΣt-Σs/2 .

K = B†(cid:16) ˙ΨT

(cid:17)
t ΣwΨt − AΣt − Σs/2

Σ−1

t

 

(16)

(17)

yields the solution  where B† denotes the pseudo-inverse of the control matrix B.

Derivation of the Feed-Forward Controls. Similarly  we obtain the feed-forward control signal k
by matching the mean of the trajectory distribution µt+dt with the mean computed with the forward
model. After rearranging terms  dividing by dt and taking the limit of dt → 0  we arrive at the
continuous time constraint for the vector k 

(18)
We can again use the trajectory distribution p(τ ; θ) to obtain µt = Ψtµw and ˙µt = ˙Ψtµw and
solve Eq. (18) for k 

˙µt = (A + BK)µt + Bk + c.

k = B†(cid:16) ˙Ψtµw − (A + BK) Ψtµw − c

(cid:17)

(19)

Estimation of the Control Noise.
In order to match a trajectory distribution  we also need to
match the control noise matrix Σu which has been applied to generate the distribution. We ﬁrst
compute the system noise covariance Σs = BΣuBT by examining the cross-correlation between

time steps of the trajectory distribution. To do so  we compute the joint distribution p(cid:0)yt  yt+dt

(cid:1) of

the current state yt and the next state yt+dt 

(cid:1) = N

(cid:18)(cid:20) yt

(cid:20) Σt Ct
(cid:21)
(cid:21)(cid:12)(cid:12)(cid:12)(cid:20) µt
p(cid:0)yt  yt+dt
(cid:1) = N (yt|µt  Σt)N(cid:0)yt+dt|F yt + f   Σu
(cid:1) which yields
(cid:20) Σt
(cid:21)
p(cid:0)yt  yt+dt

(cid:18)(cid:20) yt

(cid:1) = N

(cid:21)(cid:12)(cid:12)(cid:12)(cid:20) µt

µt+dt

yt+dt

ΣtF T

 

 

yt+dt

F µt + f

CT

t Σt+dt

(cid:21)(cid:19)

p(cid:0)yt  yt+dt

where Ct = ΨtΣwΨT
t+dt is the cross-correlation. We can again use our model to match the
cross correlation. The joint distribution for yt and yt+dt is obtained by our system dynamics by

 

(20)

(cid:21)(cid:19)

F Σt F ΣtF T + Σsdt

.

(21)

The noise covariance Σs can be obtained by matching both covariance matrices given in Eq. (20)
and (21) 

Σsdt = Σt+dt − F ΣtF T = Σt+dt − F ΣtΣ−1

(22)
The variance Σu of the control noise is then given by Σu = B†ΣsB†T . As we can see from
Eq. (22) the variance of our stochastic feedback controller does not depend on the controller gains
and can be pre-computed before estimating the controller gains.

t ΣtF T = Σt+dt − CT

t Σ−1

t Ct

6

Figure 2: A 7-link planar robot has to
reach a target position at T = 1.0s
with its end-effector while passing a
via-point at t1 = 0.25s (top) or t2 =
0.75s (middle). The plot shows the
mean posture of the robot at different
time steps in black and samples gen-
erated by the ProMP in gray.
The
ProMP approach was able to exactly re-
produce the demonstration which have
been generated by an optimal control
law. The combination of both learned
ProMPs is shown in the bottom. The
resulting movement reached both via-
points with high accuracy.

Figure 3: Robot Hockey. The robot shoots a hockey puck. We demonstrate ten straight shots for
varying distances and ten shots for varying angles. The pictures show samples from the ProMP
model for straight shots (b) and angled shots (c). Learning from combined data set yields a model
that represents variance in both  distance and angle (d). Multiplying the individual models leads to a
model that only reproduces shots where both models had probability mass  in the center at medium
distance (e). The last picture shows the effect of conditioning on only left and right angles (f).

3 Experiments

We evaluated our approach on two different real robot tasks  one stroke based movement and one
rhythmic movements. Additionally  we illustrate our approach on a 7-link simulated planar robot.
For all real robot experiments we use a seven degrees of freedom KUKA lightweight robot arm. A
more detailed description of the experiments is given in the supplementary material.

7-link Reaching Task.
In this task  a seven link planar robot has to reach a target position in
end-effector space. While doing so  it also has to reach a via-point at a certain time point. We
generated the demonstrations for learning the MPs with an optimal control law [22]. In the ﬁrst set of
demonstrations  the robot has to reach the via-point at t1 = 0.25s. The reproduced behavior with the
ProMPs is illustrated in Figure 2(top). We learned the coupling of all seven joints with one ProMP.
The ProMP exactly reproduced the via-points in task space while exhibiting a large variability in
between the time points of the via-points. Moreover  the ProMP could also reproduce the coupling
of the joints from the optimal control law which can be seen by the small variance of the end-effector
in comparison to the rather large variance of the single joints at the via-points. The ProMP could
achieve an average cost value of a similar quality as the optimal controller. We also used a second set
of demonstrations where the ﬁrst via-point was located at time step t2 = 0.75  which is illustrated
in Figure 2(middle). We combined the ProMPs learned from both demonstrations  which resulted
in the movement illustrated in Figure 2(bottom). The combination of both MPs accurately reaches
both via-points at t1 = 0.25 and t2 = 0.75.

7

−202460246−20246−20246x−axis [m]−20246−202460246y−axis [m]0246t = 0st = 0.25st = 0.5st = 0.75st = 1.0s(a)

(b)

(c)

Figure 4: (a)The maracas task. (b) Trajectory distribution for playing maracas (joint number 4). By
modulating the speed of the phase signal zt  the speed of the movement can be adapted. The plot
shows the desired distribution in blue and the generated distribution from the feedback controller
in green. Both distributions match. (c) Blending between two rhythmic movements (blue and red
shaded areas) for playing maracas. The green shaded is produced by continuously switching from
the blue to the red movement.

Robot Hockey.
In the hockey task  the robot has to shoot a hockey puck in different directions and
distances. The task setup can be seen in Figure 3(a). We record two different sets of demonstrations 
one that contains straight shots with varying distances while the second set contains shots with a
varying shooting angle. Both data sets contain ten demonstrations each. Sampling from the two
models generated by the different data sets yields shots that exhibit the demonstrated variance in
either angle or distance  as shown in Figure 3(b) and 3(c). When combining the two individual
primitives  the resulting model shoots only in the center at medium distance  i.e.  the intersection
of both MPs. We also learn a joint distribution over the ﬁnal puck position and the weight vectors
w and condition on the angle of the shot. The conditioning yields a model that shoots in different
directions  depending on the conditioning  see Figure 3(f).

Robot Maracas. A maracas is a musical instrument containing grains  such that shaking it pro-
duces sounds. Demonstrating fast movements can be difﬁcult on the robot arm  due to the inertia
of the arm. Instead  we demonstrate a slower movement of ten periods to learn the motion. We
use this slow demonstration and change the phase after learning the model to achieve a shaking
movement of appropriate speed to generate the desired sound of the instrument. Using a variable
phase also allows us to change the speed of the motion during one execution to achieve different
sound patterns. We show an example movement of the robot in Figure 4(a). The desired trajectory
distribution of the rhythmic movement and the resulting distribution generated from the feedback
controller are shown in Figure 4(b). Both distributions match. We also demonstrated a second type
of rhythmic shaking movement which we use to continuously blend between both movements to
produce different sounds. One such transition between the two ProMPs is shown for one joint in
Figure 4(c).

4 Conclusion

Probabilistic movement primitives are a promising approach for learning  modulating  and re-using
movements in a modular control architecture. To effectively take advantage of such a control archi-
tecture  ProMPs support simultaneous activation  match the quality of the encoded behavior from the
demonstrations  are able to adapt to different desired target positions  and efﬁciently learn by imita-
tion. We parametrize the desired trajectory distribution of the primitive by a Hierarchical Bayesian
Model with Gaussian distributions. The trajectory distribution can be easily obtained from demon-
strations. Our probabilistic formulation allows for new operations for movement primitives  includ-
ing conditioning and combination of primitives. Future work will focus on using the ProMPs in a
modular control architecture and improving upon imitation learning by reinforcement learning.

Acknowledgements

The research leading to these results has received funding from the European Community’s Frame-
work Programme CoDyCo (FP7-ICT-2011-9 Grant.No.600716)  CompLACS (FP7-ICT-2009-6
Grant.No.270327)  and GeRT (FP7-ICT-2009-4 Grant.No.248273).

8

q [rad]time [s]123456789101.31.41.51.61.7DesiredFeedback Controllerq [rad]time [s]2.533.544.555.566.577.5-0.2-0.100.10.20.30.40.5Demonstration 1Demonstration 2CombinationReferences
[1] A. Ijspeert and S. Schaal. Learning Attractor Landscapes for Learning Motor Primitives. In Advances in

Neural Information Processing Systems 15  (NIPS). MIT Press  Cambridge  MA  2003.

[2] M. Khansari-Zadeh and A. Billard. Learning Stable Non-Linear Dynamical Systems with Gaussian Mix-

ture Models. IEEE Transaction on Robotics  2011.

[3] J. Kober  K. Mülling  O. Kroemer  C. Lampert  B. Schölkopf  and J. Peters. Movement Templates for
Learning of Hitting and Batting. In International Conference on Robotics and Automation (ICRA)  2010.
[4] J. Kober and J. Peters. Policy Search for Motor Primitives in Robotics. Machine Learning  pages 1–33 

2010.

[5] A. Ude  A. Gams  T. Asfour  and J. Morimoto. Task-Speciﬁc Generalization of Discrete and Periodic

Dynamic Movement Primitives. Trans. Rob.  (5)  October 2010.

[6] B. da Silva  G. Konidaris  and A. Barto. Learning Parameterized Skills. In International Conference on

Machine Learning  2012.

[7] P. Kormushev  S. Calinon  and D. Caldwell. Robot Motor Skill Coordination with EM-based Reinforce-
In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and

ment Learning.
Systems (IROS)  2010.

[8] C. Daniel  G. Neumann  and J. Peters. Learning Concurrent Motor Skills in Versatile Solution Spaces. In

IEEE/RSJ International Conference on Intelligent Robots and Systems  2012.

[9] George Konidaris  Scott Kuindersma  Roderic Grupen  and Andrew Barto. Robot Learning from Demon-
stration by Constructing Skill Trees. International Journal of Robotics Research  31(3):360–375  March
2012.

[10] A. dAvella and E. Bizzi. Shared and Speciﬁc Muscle Synergies in Natural Motor Behaviors. Proceedings

of the National Academy of Sciences (PNAS)  102(3):3076–3081  2005.

[11] M. Williams  B.and Toussaint and A. Storkey. Modelling Motion Primitives and their Timing in Biologi-

cally Executed Movements. In Advances in Neural Information Processing Systems (NIPS)  2007.

[12] L. Rozo  S. Calinon  D. G. Caldwell  P. Jimenez  and C. Torras. Learning Collaborative Impedance-Based

Robot Behaviors. In AAAI Conference on Artiﬁcial Intelligence  2013.

[13] E. Rueckert  G. Neumann  M. Toussaint  and W.Pr Maass. Learned Graphical Models for Probabilistic

Planning provide a new Class of Movement Primitives. 2012.

[14] L. Righetti and A Ijspeert. Programmable central pattern generators: an application to biped locomotion
control. In Proceedings of the 2006 IEEE International Conference on Robotics and Automation  2006.
In

[15] A. Paraschos  G Neumann  and J. Peters. A probabilistic approach to robot trajectory generation.

Proceedings of the International Conference on Humanoid Robots (HUMANOIDS)  2013.

[16] S. Calinon  P. Kormushev  and D. Caldwell. Compliant Skills Acquisition and Multi-Optima Policy
Search with EM-based Reinforcement Learning. Robotics and Autonomous Systems (RAS)  61(4):369 –
379  2013.

[17] E. Todorov and M. Jordan. Optimal Feedback Control as a Theory of Motor Coordination. Nature

Neuroscience  5:1226–1235  2002.

[18] S. Schaal  J. Peters  J. Nakanishi  and A. Ijspeert. Learning Movement Primitives.

Symposium on Robotics Research  (ISRR)  2003.

In International

[19] A. Lazaric and M. Ghavamzadeh. Bayesian Multi-Task Reinforcement Learning. In Proceedings of the

27th International Conference on Machine Learning (ICML)  2010.

[20] J. Peters  M. Mistry  F. E. Udwadia  J. Nakanishi  and S. Schaal. A Unifying Methodology for Robot

Control with Redundant DOFs. Autonomous Robots  (1):1–12  2008.

[21] H. Stark and J. Woods. Probability and Random Processes with Applications to Signal Processing (3rd

Edition). 3 edition  August 2001.

[22] M. Toussaint. Robot Trajectory Optimization using Approximate Inference. In Proceedings of the 26th

International Conference on Machine Learning  (ICML)  2009.

9

,Alexandros Paraschos
Christian Daniel
Jan Peters
Gerhard Neumann
Huining Hu
Zhentao Li
Adrian Vetta