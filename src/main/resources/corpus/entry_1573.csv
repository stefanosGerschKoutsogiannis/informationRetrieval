2018,Distributed Multitask Reinforcement Learning with Quadratic Convergence,Multitask reinforcement learning (MTRL) suffers from scalability issues when the number of tasks or trajectories grows large. The main reason behind this drawback is the reliance on centeralised solutions. Recent methods exploited the connection between MTRL and general consensus to propose scalable solutions. These methods  however  suffer from two drawbacks. First  they rely on predefined objectives  and  second  exhibit linear convergence guarantees. In this paper  we improve over state-of-the-art by deriving multitask reinforcement learning from a variational inference perspective. We then propose a novel distributed solver for MTRL with quadratic convergence guarantees.,Distributed Multitask Reinforcement Learning with

Quadratic Convergence

Rasul Tutunov
PROWLER.io

Cambridge  United Kingdom

rasul@prowler.io

Dongho Kim
PROWLER.io

Cambridge  United Kingdom

dongho@prowler.io

Haitham Bou-Ammar

PROWLER.io

Cambridge  United Kingdom
haitham@prowler.io

Abstract

Multitask reinforcement learning (MTRL) suffers from scalability issues when
the number of tasks or trajectories grows large. The main reason behind this
drawback is the reliance on centeralised solutions. Recent methods exploited the
connection between MTRL and general consensus to propose scalable solutions.
These methods  however  suffer from two drawbacks. First  they rely on predeﬁned
objectives  and  second  exhibit linear convergence guarantees. In this paper  we
improve over state-of-the-art by deriving multitask reinforcement learning from a
variational inference perspective. We then propose a novel distributed solver for
MTRL with quadratic convergence guarantees.

1

Introduction

Reinforcement learning (RL) allows agents to solve sequential decision-making problems with limited
feedback. Applications with these characteristics are ubiquitous ranging from stock-trading [1] to
robotics control [2  3]. Though successful  RL methods typically require substantial amounts of data
and computation for successful behaviour. Multitask and transfer learning [4–6  2  7] techniques have
been developed to remedy these problems by allowing for knowledge reuse between tasks to bias
initial behaviour. Unfortunately  such methods suffer from scalability constraints when the number of
tasks or policy dimensions grows large.
Two promising directions remedy these scalability problems. In the ﬁrst  tasks are streamed online and
models are ﬁt iteratively. Such an alternative has been well-explored under the name of lifelong RL [8 
9]. When considering lifelong learning  however  one comes to recognise that these improvements
in computation come hand-in-hand with a decrease in the model’s accuracy due to the usage of
approximations to the original loss (e.g.  second-order expansions [10])  as well as the unavailability
of all tasks in batch. Interested readers are referred to [11] for an in-depth discussion of the limitations
of lifelong reinforcement learners.
The other direction based on decentralised optimisation remedies scalability and accuracy constraints
by distributing computation across multiple units. Though successful in supervised learning [12] 
this direction is still to be well-explored in the context of MTRL. Recently  however  the authors
in [11] proposed a distributed solver for MTRL with linear convergence guarantees based on the
Alternating Direction Method of Multipliers (ADMM). Their method relied on a connection between
MTRL and distributed general consensus. However  such ADMM-based techniques suffer from the

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

following drawbacks. First  these algorithms only achieve linear convergence in the order of O (1/k)
with k being the iteration count. Second  for linear convergence additional restrictive assumptions on
the penalty terms have to be imposed. Finally  they require large number of iterations to arrive at
accurate (in terms of consensus error) solutions as noted by [13] and validated in our experiments 
see Section 5.
In this paper  we remedy the above problems by proposing a distributed solver for MTRL that exhibits
quadratic convergence. Contrary to [11]  our technique does not impose restrictive assumptions on
the reinforcement learning loss function and can thus be deemed more general. We achieve our results
in two-steps. First  we reformulate MTRL as variational inference. Second  we map the resultant
objective to general consensus that allows us to exploit the symmetric and diagonal dominance
property of the curvature of our dual problem. We show our novel distributed solver using Chebyshev
polynomials has quadratic convergence guarantees.
We analyse the performance of our method both theoretically and empirically. On the theory side 
we formally prove quadratic convergence. On the empirical side  we show that our new technique
outperforms state-of-the-art methods from both distributed optimisation and lifelong reinforcement
learning on a variety of graph topologies. We further show that these improvements arrive at relatively
small increases in the communication overhead between the nodes.

2 Background

Reinforcement learning (RL) [14] algorithms are successful in solving sequential decision making
(SDM) tasks. In RL  the agent’s goal is to sequentially select actions that maximise its total expected
return. We formalise such problems as a Markov decision process (MDP) Z = (cid:104)X  A P R  γ(cid:105)
where X ⊆ Rd is the set of states  A ⊆ Rm is the set of possible actions  P : X × A × X (cid:55)→ [0  1]
represents the state transition probability describing the task’s dynamics  R : X × A × X (cid:55)→ R is
the reward function measuring the agent’s performance  and γ ∈ [0  1) is the discount factor. The
dynamics of an RL problem commence as follows: at each time step h  the agent is at state xh ∈ X
and has to choose an action ah ∈ A transitioning it to a new state xh+1 ∼ p(xh+1|xh  ah) as
given by P. This transition yields a reward rh+1 = R(xh  ah  xh+1). We assume that actions are
generated by a policy π : X × A (cid:55)→ [0  1]  which is deﬁned as a distribution over state-action pairs 
i.e.  π(ah|xh) is the probability of choosing action ah in a state xh. The goal of the agent is to ﬁnd
an optimal policy π∗ that maximises its expected return given by: Eπ
the horizon length.
Policy Search RL parameterises a policy by a vector of unknown parameters θ. As such  the RL
problem is transformed to a one of searching over the parameter space for θ(cid:63) that maximises:

(cid:3)  with H being

(cid:2)(cid:80)H

h=1 γhrh

(cid:90)

τ

J (θ) = Epθ (τ )[R(τ )] =

pθ(τ )R(τ )dτ  

(1)

H

(cid:80)H

deﬁned as: pθ(τ ) = P0(x0)(cid:81)H

h=1 p(xh+1|xh  ah)πθ(ah|xh)  and R(τ ) = 1

where a trajectory τ is a sequence of accumulated state-action pairs [x0:H   a0:H ]. Furthermore 
the probability of acquiring a certain trajectory  pθ(τ )  and the total reward R(τ ) for a trace τ are
h=0 rh+1  with
P0 : X (cid:55)→ [0  1] being the initial state distribution.
Policy search can also be cast as variational inference by connecting RL and probabilistic infer-
ence [15–18]. In this formulation the goal is to derive the posterior distribution over trajectories
conditioned on a desired output  given a prior trajectory distribution. The desired output is denoted
as a binary random variable ˆR  where ˆR = 1 indicates the optimal reward event. This is typically
objective J (θ) in Equation 1 is pθ( ˆR = 1) = (cid:82)
related to trajectory rewards using p( ˆR = 1|τ ) ∝ exp(R(τ )). With this deﬁnition  the optimisation
τ p( ˆR = 1|τ )pθ(τ )dτ . From the log-marginal
of the binary event  we can write the evidence lower bound (ELBO). The ELBO is derived by
(cid:21)
introducing a variational distribution qφ(τ ) and applying Jensen’s inequality:
log pθ( ˆR) ≥
log p( ˆR|τ )
with DKL (q(τ )||p(τ ))) being the Kullback Leibler divergence between q(τ ) and p(τ ).

(cid:105) − DKL(qφ(τ )(cid:107)pθ(τ )) 

log p( ˆR|τ ) + log

dτ = Eqφ(τ )

pθ(τ )
qφ(τ )

(cid:104)

(cid:90)

(cid:20)

qφ(τ )

τ

2

3 Multitask Reinforcement Learning as Variational Inference

RL algorithms require substantial amounts of trajectories and learning times for successful behaviour.
Acquiring large training samples easily leads to wear and tear on the system and thus worsens
the problem. When data is scarce  learning task policies jointly through multi-task reinforcement
learning (MTRL) rather than independently signiﬁcantly improves performance [4  19]. In MTRL 
the agent is faced with a series of T SDM tasks Z (1)  ... Z (T ). Each task is an MDP denoted by
Z (t) = (cid:104)X (t) A(t) P (t) R(t)  γ(t)(cid:105)  and the goal for the agent is to learn a set of optimal policies
Π(cid:63) = {π(cid:63)
θ(T )} with corresponding parameters Θ(cid:63) = {θ(1)(cid:63)  ...  θ(T )(cid:63)}. Rather than
deﬁning the optimisation objective directly as done in [10  4]  we provide a probabilistic modeling
view of the problem by framing MTRL as an instance of variational inference. We deﬁne a set of
reward binary events ˆR1  . . .   ˆRT ∈ {0  1}T where p( ˆRk|τk) ∝ exp (Rk(τk)). Here  trajectories
are assumed to be latent  and the goal of the agent is to determine a set of policy parameters that
assign high density to trajectories with high rewards. In other words  the goal is to ﬁnd a set of
policies that maximise the log-marginal of the reward events:

θ(1)  ...  π(cid:63)

(cid:16) ˆR1  . . . ˆRT

(cid:17)

(cid:90)
(cid:16)

(cid:90)

T(cid:89)

t=1

= log

(cid:17)

···

(cid:17)

τ1

τT
density
h |x(t)
a(t)
.

h

p( ˆRt|τt)pθt(τt)dτ1 . . . dτT  

log pθ1:θT

h=1 p(t)(cid:16)
0 (x0)(cid:81)Ht
(cid:90)
(cid:90)
T(cid:89)

···

log

pθt(τt)

where
P (t)
puting the above integrals  we derive an ELBO using a variational distribution qφ(τ1  . . .   τT ):

for
=
To handle the intractability in com-

the
is
h+1|x(t)
x(t)

trajectory

h   a(t)

pθt(τt)

task

πθt

t:

h

(cid:34) T(cid:88)

(cid:16) ˆRt|τt
(cid:17)
(cid:34)
(cid:81)T

(cid:35)

(cid:81)T

t=1 pθt(τ )
qφ(·)

p( ˆRt|τt)pθt(τt)dτ1 . . . dτT ≥ Eqφ(·)

log p

+ log

τ1

τT

t=1

Using the above  the optimisation objective of multitask reinforcement learning can be written as:

(cid:35)
We assume a mean-ﬁeld variational approximation [20]  i.e.  qφ(τ1  . . .   τT ) = (cid:81)T

t=1 pθt(τ )
qφ(τ1  . . .   τT )

+ Eqφ(τ1 ... τT )

(cid:34) T(cid:88)

Eqφ(τ1 ... τT )

(cid:17)(cid:35)

max
φ θ1:θT

Furthermore  we assume that the distribution1 qφt(τt) follows that of pθt(τt). Hence  we write:

log p

log

t=1

t=1

.

t=1 qφt(τt).

(cid:16) ˆRt|τt
(cid:16) ˆRt|τt

log p

(cid:104)

(cid:17)(cid:105) − T(cid:88)

max

φ1:φT  θ1:θT

Eqφt (τt)

t=1

t=1

DKL (qφt(τt)||pθt(τt)) .

(2)

T(cid:88)

So far  we discussed MTRL assuming independence between policy parameters θ1  . . .   θT . To
beneﬁt from shared knowledge between tasks  we next introduce coupling by allowing for parameter
sharing across MDPs. Inspired by stochastic variational inference [21]  we decompose θt = Θsh ˜θt 
where Θsh is a shared set of parameters between tasks  while ˜θt represents task-speciﬁc parameters
introduced to “specialise” shared knowledge to the peculiarities of each task t ∈ {1  . . .   T}. For
instance  if a task parameter θt ∈ Rd  our decomposition yields Θsh ∈ Rd×k  and ˜θt ∈ Rk×1 with k
representing the dimensions of the shared latent knowledge.
Solving the problem in Equation 2 amounts to determining both variational and model parameters 
i.e.  φ1  . . .   φT   and Θsh  and ˜θ1  . . .   ˜θT . We propose an expectation-maximisation style algorithm
for computing each of the above free variables. Namely  in the E-step we solve for φ1  . . .   φT
while keeping Θsh  and ˜θ1  . . .   ˜θT ﬁxed. In the M-step  on the other hand  we determine Θsh and
˜θ1  . . .   ˜θT given the updated variational parameters. In both these steps  solving for the task-speciﬁc
and variational parameters can be made efﬁcient using parallelisation. Determining Θsh  however 
requires knowledge of all tasks making it unscalable as the number of tasks grows large. To remedy
this problem  we next propose a novel distributed Newton method with quadratic convergence
guarantees2. Applying this method to determine Θsh results in a highly scalable learner as shown in
the following sections.

1Please note that we leave exploring other forms of the variational distribution as an interesting direction for

future work.

2Contrary to stochastic variational inference  we are not restricted to exponential family distributions

3

4 Scalable Multitask Reinforcement Learning

As mentioned earlier  the problem of determining Θsh can become computationally intensive with
an increasing number of tasks. In this section  we devise a distributed Newton method for Θsh to
aid in scalability. Given an updated variational (i.e.  E-step) and ﬁxed task-speciﬁc parameters  the
optimisation problem for Θsh can be written as:

T(cid:88)

t=1

1
Nt

(cid:34) Nt(cid:88)

it=1

max
Θsh

(cid:104)

(cid:16) ˆR(it)

t

log

p

|τ (it)

t

(cid:17) × pΘsh  ˜θt

(cid:16)

τ (it)
t

(cid:17)(cid:105)(cid:35)

(cid:16)

(cid:17)

T(cid:88)

t=1

≡ max

Θsh

J (t)

MTRL

Θsh ˜θt

 

(3)

(4)

Θ(1)

i=1

t=1

−J (t)

MTRL

t ⊗ Id×d

(cid:16)(cid:16) ˜θ(cid:62)

sh = ··· = Θ(n)
sh  

where it = {1  . . .   Nt} denotes the index of trajectory i for task t ∈ {1  . . .   T}. The above
equation omits functions independent of Θsh  and estimates the variational expectation by sampling
Nt trajectories for each of the tasks.
Our scaling strategy is to allow for a distributed framework generalising to any topology of connected
processors. Hence  we assume an undirected graph G = (V E) of computational units. Here  V
denotes the set of nodes (processors) and E the set of edges. Similar to [11]  we assume n nodes
connected via |E| edges. Contrary to their work  however  no speciﬁc node-ordering assumptions
are imposed. Before writing the problem above in an equivalent distributed fashion  we ﬁrstly
introduce “vec(A)” to denote the column-wise vectorisation of a matrix A. This notation allows us
to rewrite the by-product Θsh ˜θt in terms of a vectorised version of the optimisation variable Θsh 
t ⊗ Id×d)vec(Θsh) ∈ Rd×1. Hence  the equivalent distributed formulation
where vec(Θsh ˜θt) = ( ˜θ(cid:62)
Ti(cid:88)
of Equation 3 is given by:

n(cid:88)

(cid:17)

(cid:17)

s.t. Θ(1)

where Ti is the total number of tasks assigned to node i such that(cid:80)n

vec(Θ(i)
sh )

min
sh :Θ(n)
sh

i=1 Ti = T . Intuitively  the
above is distributing Equation 3 among n nodes  where each computes its local copy of Θsh. For the
distributed version to coincide with the centeralised one  all nodes have to arrive to a consensus (in
a fully distributed fashion) on the value of Θsh. As such  a feasible solution for the distributed and
centeralised versions coincide making the two problems equivalent.
Now  we can apply any off-the-shelf distributed optimisation algorithm. Unfortunately  current
techniques suffer from drawbacks prohibiting their direct usage for MTRL. Generally  there are two
popular classes of algorithms for distributed optimisation. The ﬁrst is sub-gradient based  while
the second relies on a decomposition-coordination procedure. Sub-gradient algorithms proceed by
taking a gradient step then followed by an averaging step at each iteration. The computation of
each step is relatively cheap and can be implemented in a distributed fashion [22]. Though cheap to
√
compute  the best known convergence rate of sub-gradient methods is slow given by O (1/
K) with
K being the total number of iterations [23  24]. The second class of algorithms solve constrained
problems by relying on dual methods. One of the well-known state-of-the-art methods from this
class is the Alternating Direction Method of Multipliers (ADMM) [13]. ADMM decomposes the
original problem to two subproblems which are then solved sequentially leading to updates of dual
variables. In [23]  the authors show that ADMM can be fully distributed over a network leading
to improved convergence rates to the order of O (1/K). Recently  the authors in [11] applied the
method [23] for distributed MTRL. In our experiments  we signiﬁcantly outperform [11]  especially
in high-dimensional environments.
Much rate improvements can be gained from adopting second-order (Newton) methods. Though
a variety of techniques have been proposed in [25–27]  less progress has been made at leveraging
ADMM’s accuracy and convergence rate issues. In a recent attempt [25]  the authors propose a
distributed second-order method for general consensus by using the approach in [27] to compute the
Newton direction. As detailed in Section 6  this method suffers from two problems. First  it fails to
outperform ADMM and second  faces storage and computational deﬁciencies for large data sets  thus
ADMM retains state-of-the-art status.
Next  we develop a distributed solver that outperforms others both theoretically and empirically. On
the theory side  we develop the ﬁrst distributed MTRL algorithm with provable quadratic convergence
guarantees. On the empirical side  we demonstrate the superiority of our method on a variety of
benchmarks.

4

Figure 1: High-level depiction of our distribution framework for the shared parameters. Each of the
vectors yi holds the ith components of the shared parameters across all nodes n.

4.1 Laplacian-Based Distributed Multitask Reinforcement Learning

For maximum performance boost  we aim to have our algorithm exploit (locally) the structure of
the computational graph connecting the processing units. To consider such an effect  we rewrite
our distributed MTRL in terms of the graph Laplacian L; a matrix that reﬂects the graph structure.
Formally  the L is an n × n matrix such L(i  j) = degree(i) when i = j  -1 when (i  j) ∈ E  and 0
otherwise. Of course  this matrix cannot be known to all the nodes in the network. We ensure full
distribution by allowing each node to only access its local neighbourhood. To view the problem in
Equation 4 from a graph topology perspective  we introduce a set of dk vectors y1  . . .   ydk  each
in Rn. The goal of these vectors is to hold the ith component of vec(Θ(1)
sh ). This
process is depicted in Figure 1  where  for instance  the ﬁrst vector y1 accumulates the ﬁrst component
of the shared parameters  θ(1)
We can now describe consensus on the copies of the shared parameters as consensus between
the components of y1  . . .   ydk. Clearly  the components in yr coincide  if the rth component of
the shared parameters equate across nodes. Hence  consensus between the components (for all r)
corresponds to consensus on all dimensions of the shared parameters. This is exactly the constraint in
Equation 4. One can think of a vector with equal components as that parallel to 1  namely  yr = cr1.
Consequently  we can introduce the graph Laplacian in our constraints by having y1  . . .   ydk to be
the solution of Ly1 = 0  . . .  Lydk = 0. This is true since the only solution to Lv = 0 is a vector  v 
parallel to the vector of ones. Hence  a vector yr satisfying the above system has to be of the form
cr1  i.e.  its components equate. Hence  we write:

sh )  . . .   vec(Θ(n)

1 sh  from all nodes.

1 sh  . . .   θ(n)

−J (t)

MTRL

t ⊗ Id×d

˜yi

s.t. Ly1 = 0  . . .  Lydk = 0 ⇐⇒ M y = 0 

(5)

n(cid:88)

Ti(cid:88)

i=1

t=1

min
y1:ydk

(cid:16)(cid:16) ˜θ(cid:62)

(cid:17)

(cid:17)

with ˜yi = [y1(i)  . . .   ydk(i)]
size ndk × ndk having Laplacian elements  and y ∈ Rndk a vector collecting y1  . . .   ydk.

sh )  M = Idk×dk ⊗ L a block-diagonal matrix of

(cid:62) denoting vec(Θ(i)

4.2 Solution Methodology

The problem in Equation 5 is a constrained optimisation one that can be solved by descending (in a
distributed fashion) in the dual function. Though adopting second-order techniques (e.g.  Newton
iteration) can lead to improved convergence speeds  direct application of standard Newton is difﬁcult
as we require a distributed procedure to accurately compute the direction of descent3.
In the following  we propose an accurate and scalable distributed Newton method. Our solution is
decomposed in two steps. First  we write the constraint problem as an unconstraint one by introducing
the dual functional to Equation 5. Second  we exploit the symmetric diagonally dominant (SDD)
property of the Hessian  previously proved for a broader setting in Lemma 2 of [28]  by developing a
Chebyshev solver to compute the Newton direction. To formulate the dual  we introduce a vector of
dk](cid:62) ∈ Rndk  where λi ∈ Rn is a vector of multipliers  one
Lagrange multipliers λ = [λ(cid:62)
for each dimension of vec (Θsh). For fully distributed computations  we assume each node to only
store its corresponding components λ1(i)  . . .   λdk(i). After deriving the Lagrangian  we can write

1   . . .   λ(cid:62)

3It is worth noting that some techniques for determining the Newton direction in a distributed fashion exist.

These techniques  however  are inaccurate  see Section 5.

5

© PROWLER.io 2018 – www.prowler.io.........the dual function q(λ) as 4:

q(λ) =

inf

y1(i):ydk(i)

t=1

n(cid:88)

i=1

(cid:32) Ti(cid:88)

(cid:16)(cid:16) ˜θ(cid:62)

t ⊗ Id×d

(cid:17)

(cid:17)

˜yi

−J (t)

MTRL

+ y1(i)[Lλ1]i + ··· + ydk(i)[Lλdk]i

(cid:33)

 

. . .

 

Ti(cid:88)

= −[Lλ1]i 

∂fi(·)
∂ydk(i)

= −[Lλdk]i where fi(·) =

have: −[Lλr]i =(cid:80)

which is clearly separable across the computational nodes in G. Before discussing the SDD properties
of the dual Hessian  we still require a procedure that allows us to infer about the primal (i.e.  y)
given updated parameters λ. We recognise that primal variables can be found as the solution to the
following system of equations:
∂fi(·)
∂y1(i)
(6)
It is also clear that Equation 6 is locally deﬁned for every node i ∈ V since for each r = 1  . . .   dk  we
j∈N (i) λr(j) − d(i)λr(i)  where N (i) is the neighbourhood of node i. As such 
each node i can construct its own system of equations by collecting {λ1(j)  . . .   λdk(j)} from its
neighbours without the need for full communication. These can then be locally solved for determining
the primal variables5.
As mentioned earlier  we update λ using a distributed Newton method. At every iteration s in the
optimisation algorithm  the descent direction is thus computed to be the solution of H (λs) ds = −gs 
where H (λs) is the Hessian  ds the Newton direction  and gs the gradient. The Hessian and the
gradient of our objective are given by:

(cid:16)(cid:16) ˜θ(cid:62)

t ⊗ Id×d

(cid:17)

−J (t)

MTRL

(cid:17)

t=1

˜yi

.

H(λs) = −M

−∇2J (t)

MTRL (y (λs))

M and ∇q(λs) = M y (λs) .

(cid:34) n(cid:88)

Ti(cid:88)

i=1

t=1

(cid:35)−1

Unfortunately  inverting H(λs) to determine the Newton direction is not possible in a distributed
setting since computing the inverse requires global information. Given the form of M and following
the results in [28]  one can show that the above Hessian exhibits the SDD property. Luckily  this
property can be exploited for a distributed solver as we show next.
The story of computing an approximation to the exact solution of an SDD system of linear equation
starts with standard splitting of symmetric matrices. Given a symmetric matrix6 H the standard
splitting is given by H = D0− A0  where D0 is a diagonal matrix that consists of diagonal elements
in H  while A0 is a matrix collecting the negate of the off-diagonal components in H. As the goal is
to determine a solution of the SDD system  we will be interested in inverses of H. Generalising the
work in [29]  we recognise that the inverse can be written as:

O(log m)(cid:89)

(cid:20)

(cid:104)

(cid:105)2(cid:96)(cid:21)

(D0 − A0)−1 ≈ D

− 1
0

2

I +

− 1
0 A0D

2

− 1
0

2

D

− 1
0 = ˆPm(H) 

2

D

(cid:96)=0

where ˆPm(H) is a polynomial of degree m ∼ κ(H) of matrix H. All computations do not need
access to the Hessian nor its inverse. We can describe these only using local Hessian vector products 
hence allowing for fast implementation using automatic differentiation. Hence  the goal of the
Newton update is to ﬁnd a solution of the form d(m)
is an
-close solution to d(cid:63)

s = Pm(H(λs))∇q(λs) such that d(m)

s − d(cid:63)

s. Consequently  the differential d(m)
s − d(cid:63)
s = [H(λs)Pm(H(λs)) − I] d(cid:63)
d(m)

s can be written as:
s = −Qm(H(λs))d(cid:63)
s 

where Qm(H(λs)) = −H(λs)Pm(H(λs)) + I. Therefore  instead of seeking Pm(·)  one can
think of constructing polynomials Qm(·) that reduce the term d(m)
s as fast as possible. This
can be formalised in terms of the properties of Qm(·) by requiring the polynomial to have a minimal
degree  as well as satisfying the following for a precision parameter : Qm(0) = 1 and |Qm(µi)| ≤  

s − d(cid:63)

s

4Please notice that for a dual function we use notation q(λ) and for the variational distribution qφt (τt)
5Please note that for the case of log-concave policies  we can determine the relation between primal and dual

variables in closed form by simple algebraic manipulation.

6Please note that we use H to denote H(λs).

6

(b) Running Times
(SM  DM  & CP)

(c) Running Times (HC
& HR)

(a) Communication Over-
head
Figure 2: (a) Communication overhead in the HC case. Our method has an increase proportional to
the condition number of the graph  which is slower compared to the other techniques. (b) and (c)
Running times till convergence to a threshold of 10−5. (d) Number of iterations for a 10−5 consensus
error on the HC dynamical system on different graph topologies.
with µi being the ith smallest eigenvalue of H(λs). The ﬁrst condition is a result of observing
Qm(z) = −zPm(z) + 1  while the second guarantees an -approximate solution:

(d) Effect of Graph Topol-
ogy

||d(m)

k − d(cid:63)

k||2

H(λs) ≤ max

|Qm(µi)|2||d(cid:63)

s||2

s||2
H(λs) ≤ 2||d(cid:63)

H(λs).

i

1

/Tm

(cid:17)

(cid:17)

µN−µ2

m(z) = Tm

(cid:16) µN +µ2

(cid:16) µN +µ2−2z

In other words  ﬁnding Qm(z) that has minimal degree and satisﬁes the above two conditions
guarantees an efﬁcient and -close solution to d(cid:63)
s. Chebyshev polynomials of the ﬁrst kind satisfy
our requirements. Their form is deﬁned as Tm(z) = cos(m arccos(z)) if z ∈ [−1  1]  and 1
z2 − 1)m + (z − √
√
2 ((z +
z2 − 1)m) otherwise. Interestingly  |Tm(z)| ≤ 1 on [−1  1]  and among all
polynomials of degree m with a leading coefﬁcient 1  the polynomial
2m−1 Tm(z) acquires its minimal
and maximal values on this interval (i.e.  sharpest increase outside the range [−1  1]). We posit that
a good candidate is Q(cid:63)
  with µi being the ith smallest
eigenvalue of symmetric matrix H describing the system of linear equations. First  it is easy to see
shown that for any s and z ∈ [µ2  µndk]  |Q(cid:63)(z)|2 is bounded as |Q(cid:63)(z)|2 ≤ 4 exp (−4m/(cid:112)κ(H) + 1).
that when z = 0  these polynomials attain a value of unity (i.e.  Q(cid:63)
m(0) = 1). Secondly  it can be
s = −H(λs)−1 (I − Qm(H(λs))) gs guar-
Therefore  choosing the approximate solution as7 d(m)
antees an -close solution. Please note that by exploiting the recursive properties of Chebyshev
polynomials we can derive an approximate solution without the need to compute H−1 explicitly. In
addition to the time and message complexities of this new solver  other implementation details can be
found in the appendix. We now show quadratic convergence of the distributed Newton method:
Theorem 1. Distributed Newton method using the Chebyshev solver exhibits the following two
convergence phases for some constants c1 and c2:
2(L)
Strict Decrease: if ||∇q(λs)||2 > c1  then ||∇q(λs)||2 − ||∇q(λs)||2 ≤ c2
µ4
n(L)
µ3
Quadratic Decrease: if ||∇q(λs)||2 ≤ c1  then for any l ≥ 1: ||∇q(λs+l)||2 ≤ 2c1

µN−µ2

22l + O()

5 Experiments & Results

We conducted two sets of experiments to compare against distributed and multitask learning methods.
On the distributed side  we evaluated our algorithm against ﬁve other approaches being: 1) ADD [27] 
2) ADMM [11]  3) distributed averaging [30]  4) network-newton [26  25]  and 5) sub-gradients.
We are chieﬂy interested in the convergence speeds of both the objective value and consensus error 
as well as the communication overhead and running times of these approaches. The comparison
against [11] (which we title as distributed ADMM in the ﬁgures) allows us to understand whether
we surpass state-of-the-art  while that against ADD and network-newton sheds the light on the
accuracy of our Newton’s direction approximation. When it comes to online methods  we compare
our performance in terms of jump-start and asymptotic performance to policy gradients [31  32] 
PG-ELLA [10]  and GO-MTL [33]. Our experiments ran on ﬁve systems  simple mass (SM)  double
mass (DM)  cart-pole (CP)  helicopter (HC)  and humanoid robots (HR).
We followed the experimental protocol in [10  33] where we generated 5000 SM  500 DM  and 1000
CP tasks by varying the dynamical parameters of each of the above systems. These tasks were then

7Please note that the solution of this system can be split into dk linear systems that can be solved efﬁciently

using the distributed Chebyshev solver. Due to space constraints these details can be found in the appendix.

7

123456789Accuracy00.20.40.60.811.21.41.61.82Local Communication Exchange×104Distributed SDDM NewtonDistributed ADD NewtonDistribted ADMMDistribted AveragingNewtork NewtonDistribted GradientsTime to Convergence [sec]016.7533.550.2567SMDMCPSDD-NewtonADD-NewtonADMMDist. AveragingNetwork NewtonDistributed GradientsTime to Convergence [sec]01150230034504600HCHRSDD-NewtonADD-NewtonADMMDist. AveragingNetwork NewtonDistributed Gradients# Iterations to Convergence03006009001200S. RandomM. RandomL. RandomBar-Bell SDD-NewtonADD-NewtonADMMDist. AveragingNetwork NewtonDistributed Gradients> 700> 700> 700> 1000> 700> 700> 700> 1000> 700> 700> 700> 1000> 700> 700> 700> 1000(a) Jump-Start

(b) Asymptotic Performance

Figure 3: Demonstration of jump-start and asymptotic results.

distributed over graphs with edges generated uniformly at random. Namely  a graph of 10 nodes
and 25 edges was used for both SM and DM experiments  while a one with 50 nodes and 150 edges
for the CP. To distribute our computations  we made use of MATLAB’s parallel pool running on 10
nodes. For all methods  tasks were assigned evenly across 10 agents8. An  = 1/100 was provided to
the Chebyshev solver for determining the approximate Newton direction in all cases. Step-sizes were
determined separately for each algorithm using a grid-search-like technique over {0.01  . . .   1} to
ensure best operating conditions. Results reporting improvements in the consensus error (i.e.  the
error measuring the deviation from agreement among the nodes) can be found in the appendix due to
space constraints.
Communication Overhead & Running Times: It can be argued that our improved results arrive at
a high communication cost between processors. This may be true as our method relies on an SDD-
solver while others allow for only few messages per iteration. We conducted an experiment measuring
local communication exchange with respect to accuracy requirements. Results on the HC system 
reported in Figure 2a  demonstrate that this increase is negligible compared to other methods. Clearly 
as accuracy demands increase so does the communication overhead of all algorithms. Distributed
SDD-Newton has a growth rate proportional to the condition number of the graph being much slower
compared to the exponential growth observed by other techniques. Having shown small increase
in communication cost  now we turn our attention to assess running times to convergence on all
dynamical systems. Figures 2b and 2c report running times to convergence computed according
to a 10−5 error threshold. All these experiments were run on a small random graph of 20 nodes
and 50 edges. Clearly  our method is faster when compared with others in both cases of low and
high-dimensional policies. A ﬁnal question to be answered is the effect of different graph topologies
on the performance of SDD-Newton. Taking the HC benchmark  we generated four graph topologies
representing small (S. Random)  medium (M. Random)  and large (L. Random) random networks 
and a bar-bell graph with nodes varying from 10 to 150 and edges from 25 to 250. The bar-bell
contained 2 cliques formed by 10 nodes each and a 10 node line graph connecting them. We then
measured the number of iterations required by all algorithms to achieve a consensus error of 10−5.
Figure 2d reports these results showing that our method is again faster than others.
Benchmarking Against RL: We ﬁnally assessed our method in comparison to current MTRL
the technique described in [11]  where the reward function was given by −√
literature  including PG-ELLA [10] and GO-MTL [33]. For the experimental procedure  we followed
xh − xref  with xref
being the reference state. As base-learners we used policy gradients as detailed in [34]  which acquired
1000 trajectories with a length of 150 each. We report jump-start and asymptotic performance in
Figures 3a and 3b. These results show that our method can outperform others in terms of jump-start
and asymptotic performance while requiring fewer iterations. Moreover  it is clear that our method
outperforms streaming models  e.g.  PG-ELLA.

6 Conclusions & Future Work

We proposed a distributed solver for multitask reinforcement learning with quadratic convergence.
Our next steps include developing an incremental version of our algorithm using generalised Hessians 
and conducting experiments running on true distributed architectures to quantify the trade-off between
communication and computation.

8When graphs grew larger  nodes were grouped together and provided to one processor.

8

Jump-Start Improvement %SMCPHCDM69593423605230215042181940442217PG-ELLAGO-MTLDist-ADMMSDD-NewtonAsymptotic Performance %SMCPHCDM7116941078458741076PG-ELLAGO-MTLDist-ADMMSDD-NewtonReferences
[1] Thira Chavarnakul and David Enke. A hybrid stock trading system for intelligent technical analysis-based

equivolume charting. Neurocomput.  72(16-18)  2009.

[2] Marc Peter Deisenroth  Peter Englert  Jochen Peters  and Dieter Fox. Multi-task policy search for robotics.
In Robotics and Automation (ICRA)  2014 IEEE International Conference on  pages 3876–3881. IEEE 
2014.

[3] Jens Kober and Jan R Peters. Policy search for motor primitives in robotics. In Advances in neural

information processing systems  pages 849–856  2009.

[4] Matthew E. Taylor and Peter Stone. Transfer Learning for Reinforcement Learning Domains: A Survey.

Journal of Machine Learning Research  10:1633–1685  2009.

[5] Mohammad Gheshlaghi Azar  Alessandro Lazaric  and Emma Brunskill. Sequential Transfer in Multi-
armed Bandit with Finite Set of Models. In Advances in Neural Information Processing Systems 26.
2013.

[6] A. Lazaric. Transfer in Reinforcement Learning: A Framework and a Survey. In M. Wiering and M. van

Otterlo  editors  Reinforcement Learning: State of the Art. Springer  2011.

[7] Matthijs Snel and Shimon Whiteson. Learning potential functions and their representations for multi-task

reinforcement learning. Autonomous agents and multi-agent systems  28(4):637–681  2014.

[8] Sebastian Thrun and Joseph O’Sullivan. Learning More From Less Data: Experiment in Lifelong Learning.

In Seminar Digest  1996.

[9] Paul Ruvolo and Eric Eaton. ELLA: An Efﬁcient Lifelong Learning Algorithm. In Proceedings of the 30th

International Conference on Machine Learning (ICML-13)  2013.

[10] Haitham Bou-Ammar  Eric Eaton  Paul Ruvolo  and Matthew Taylor. Online Multi-task Learning for
Policy Gradient Methods. In Tony Jebara and Eric P. Xing  editors  Proceedings of the 31st International
Conference on Machine Learning. JMLR Workshop and Conference Proceedings  2014.

[11] El Bsat  Bou-Ammar Haitham  and Mathew Taylor. Scalable Multitask Policy Gradient Reinforcement

Learning. In AAAI  February 2017.

[12] Pedro A. Forero  Alfonso Cano  and Georgios B. Giannakis. Consensus-based distributed support vector

machines. J. Mach. Learn. Res.  11:1663–1707  August 2010.

[13] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press  New York 

NY  USA  2004.

[14] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press  Cambridge 

MA  USA  1st edition  1998.

[15] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th

Annual International Conference on Machine Learning  pages 1049–1056  2009.

[16] Gerhard Neumann. Variational Inference for Policy Search in changing situations. In Proceedings of the

28th International Conference on Machine Learning (ICML-11)  pages 817–824  2011.

[17] Sergey Levine and Vladlen Koltun. Variational policy search via trajectory optimization. In Advances in

Neural Information Processing Systems 26  pages 207–215  2013.

[18] Abbas Abdolmaleki  Jost Tobias Springenberg  Yuval Tassa  Rémi Munos  Nicolas Heess  and Martin

Riedmiller. Maximum a posteriori policy optimization. In ICLR  pages 1–22  2018.

[19] Hui Li  Xuejun Liao  and Lawrence Carin. Multi-task reinforcement learning in partially observable

stochastic environments. The Journal of Machine Learning Research  10:1131–1186  2009.

[20] David M. Blei  Alp Kucukelbir  and Jon D. McAuliffe. Variational Inference: A Review for Statisticians.

Journal of the American Statistical Association  112(518):859–877  2017.

[21] Matthew D. Hoffman  David M. Blei  Chong Wang  and John Paisley. Stochastic Variational Inference.

Journal of Machine Learning Research  14:1303–1347  2013.

[22] A. Nedic and A.E. Ozdaglar. Distributed Subgradient Methods for Multi-Agent Optimization. IEEE Trans.

Automat. Contr.  (1):48–61  2009.

9

[23] Ermin Wei and Asuman Ozdaglar. Distributed alternating direction method of multipliers. In Decision and

Control (CDC)  2012 IEEE 51st Annual Conference on  pages 5445–5450. IEEE  2012.

[24] J. L. Gofﬁn. On convergence Rates of Subgradient Optimization Methods. Mathematical Programming 

13  1977.

[25] A. Mokhtari  Q. Ling  and A. Ribeiro. Network Newton-Part I: Algorithm and Convergence. ArXiv

e-prints  2015  1504.06017.

[26] A. Mokhtari  Q. Ling  and A. Ribeiro. Network Newton-Part II: Convergence Rate and Implementation.

ArXiv e-prints  2015  1504.06020.

[27] M. Zargham  A. Ribeiro  A. E. Ozdaglar  and A. Jadbabaie. Accelerated Dual Descent for Network Flow

Optimization. IEEE Transactions on Automatic Control  2014.

[28] R. Tutunov  H. Bou-Ammar  and A. Jadbabaie. Distributed SDDM Solvers: Theory & Applications. ArXiv

e-prints  2015.

[29] Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for preconditioning and solving

symmetric  diagonally dominant linear systems. CoRR  abs/cs/0607105  2006.

[30] A. Olshevsky. Linear Time Average Consensus on Fixed Graphs and Implications for Decentralized

Optimization and Multi-Agent Control. ArXiv e-prints  2014.

[31] Jens Kober and Jan Peters. Policy Search for Motor Primitives in Robotics. Machine Learning  84(1-2) 

July 2011.

[32] John Schulman  Sergey Levine  Philipp Moritz  Michael Jordan  and Pieter Abbeel. Trust Region Policy

Optimization. In ICML  2015.

[33] Abhishek Kumar and Hal Daumé III. Learning Task Grouping and Overlap in Multi-Task Learning. In

International Conference on Machine Learning (ICML)  2012.

[34] Jan Peters and Stefan Schaal. Natural Actor-Critic. Neurocomputing  71  2008.

10

,Rasul Tutunov
Dongho Kim
Haitham Bou Ammar