2016,Online Convex Optimization with Unconstrained Domains and Losses,We propose an online convex optimization algorithm (RescaledExp) that achieves optimal regret in the unconstrained setting without prior knowledge of any bounds on the loss functions. We prove a lower bound showing an exponential separation between the regret of existing algorithms that require a known bound on the loss functions and any algorithm that does not require such knowledge. RescaledExp matches this lower bound asymptotically in the number of iterations. RescaledExp is naturally hyperparameter-free and we demonstrate empirically that it matches prior optimization algorithms that require hyperparameter optimization.,OnlineConvexOptimizationwithUnconstrainedDomainsandLossesAshokCutkoskyDepartmentofComputerScienceStanfordUniversityashokc@cs.stanford.eduKwabenaBoahenDepartmentofBioengineeringStanfordUniversityboahen@stanford.eduAbstractWeproposeanonlineconvexoptimizationalgorithm(RESCALEDEXP)thatachievesoptimalregretintheunconstrainedsettingwithoutpriorknowledgeofanyboundsonthelossfunctions.Weprovealowerboundshowinganexponentialsep-arationbetweentheregretofexistingalgorithmsthatrequireaknownboundonthelossfunctionsandanyalgorithmthatdoesnotrequiresuchknowledge.RESCALEDEXPmatchesthislowerboundasymptoticallyinthenumberofitera-tions.RESCALEDEXPisnaturallyhyperparameter-freeandwedemonstrateempir-icallythatitmatchesprioroptimizationalgorithmsthatrequirehyperparameteroptimization.1OnlineConvexOptimizationOnlineConvexOptimization(OCO)[1 2]providesanelegantframeworkformodelingnoisy antagonisticorchangingenvironments.Theproblemcanbestatedformallywiththehelpofthefollowingdeﬁnitions:ConvexSet:AsetWisconvexifWiscontainedinsomerealvectorspaceandtw+(1−t)w0∈Wforallw w0∈Wandt∈[0 1].ConvexFunction:f:W→Risaconvexfunctioniff(tw+(1−t)w0)≤tf(w)+(1−t)f(w0)forallw w0∈Wandt∈[0 1].AnOCOproblemisagameofrepeatedroundsinwhichonroundtalearnerﬁrstchoosesanelementwtinsomeconvexspaceW thenreceivesaconvexlossfunction‘t andsuffersloss‘t(wt).Theregretofthelearnerwithrespecttosomeotheru∈WisdeﬁnedbyRT(u)=TXt=1‘t(wt)−‘t(u)Theobjectiveistodesignanalgorithmthatcanachievelowregretwithrespecttoanyu eveninthefaceofadversariallychosen‘t.ManypracticalproblemscanbeformulatedasOCOproblems.Forexample thestochasticoptimiza-tionproblemsfoundwidelythroughoutmachinelearninghaveexactlythesameform butwithi.i.d.lossfunctions asubsetoftheOCOproblems.Inthissettingthegoalistoidentifyavectorw?withlowgeneralizationerror(E[‘(w?)−‘(u)]).WecansolvethisbyrunninganOCOalgorithmforTroundsandsettingw?tobetheaveragevalueofwt.Byonline-to-batchconversionresults[3 4] thegeneralizationerrorisboundedbytheexpectationoftheregretoverthe‘tdividedbyT.Thus OCOalgorithmscanbeusedtosolvestochasticoptimizationproblemswhilealsoperformingwellinnon-i.i.d.settings.30thConferenceonNeuralInformationProcessingSystems(NIPS2016) Barcelona Spain.TheregretofanOCOproblemisupper-boundedbytheregretonacorrespondingOnlineLinearOptimization(OLO)problem inwhicheach‘tisfurtherconstrainedtobealinearfunction:‘t(w)=gt·wtforsomegt.Thereductionfollows withthehelpofonemoredeﬁnition:Subgradient:g∈Wisasubgradientoffatw denotedg∈∂f(w) ifandonlyiff(w)+g·(w0−w)≤f(w0)forallw0.Notethat∂f(w)6=∅iffisconvex.1ToreduceOCOtoOLO supposegt∈∂‘t(wt) andconsiderreplacing‘t(w)withthelinearapproximationgt·w.Thenusingthedeﬁnitionofsubgradient RT(u)=TXt=1‘t(wt)−‘t(u)≤TXt=1gt(wt−u)=TXt=1gtwt−gtusothatreplacing‘t(w)withgt·wcanonlymaketheproblemmoredifﬁcult.AlloftheanalysisinthispaperthereforeaddressesOLO accessingconvexlossesfunctionsonlythroughsubgradients.TherearetwomajorfactorsthatinﬂuencetheregretofOLOalgorithms:thesizeofthespaceWandthesizeofthesubgradientsgt.WhenWisaboundedset(the“constrained”case) thengivenB=maxw∈Wkwk thereexistOLOalgorithms[5 6]thatcanachieveRT(u)≤O(cid:16)BLmax√T(cid:17)withoutknowingLmax=maxtkgtk.WhenWisunbounded(the“unconstrained”case) thengivenLmax thereexistalgorithms[7 8 9]thatachieveRT(u)≤˜O(kuklog(kuk)Lmax√T)orRt(u)≤˜O(kukplog(kuk)Lmax√T) where˜OhidesfactorsthatdependlogarithmicallyonLmaxandT.Thesealgorithmsareknowntobeoptimal(uptoconstants)fortheirrespectiveregimes[10 7].Allalgorithmsfortheunconstrainedsettingto-daterequireknowledgeofLmaxtoachievetheseoptimalbounds.2Thusanaturalquestionis:canweachieveO(kuklog(kuk))regretintheunconstrained unknown-Lmaxsetting?ThisproblemhasbeenposedasaCOLT2016openproblem[12] andissolvedinthispaper.AsimpleapproachistomaintainanestimateofLmaxanddoubleitwheneverweseeanewgtthatviolatestheassumedbound(theso-called“doublingtrick”) therebyturningaknown-Lmaxalgorithmintoanunknown-Lmaxalgorithm.Thisstrategyfailsforpreviousknown-LmaxalgorithmsbecausetheiranalysismakesstronguseoftheassumptionthateachandeverykgtkisboundedbyLmax.Theexistenceofevenasmallnumberofbound-violatinggtcanthrowofftheentireanalysis.Inthispaper weprovethatitisactuallyimpossibletoachieveregretO(cid:18)kuklog(kuk)Lmax√T+Lmaxexp(cid:20)(cid:16)maxtkgtkL(t)(cid:17)1/2−(cid:21)(cid:19)forany>0whereLmaxandL(t)=maxt0<tkgt0kareunknowninadvance(Section2).Thisimmediatelyrulesoutthe“ideal”boundof˜O(kukplog(kuk)Lmax√T)whichispossibleintheknown-Lmaxcase.Secondly weprovideanalgorithm RESCALEDEXP thatmatchesourlowerboundwithoutpriorknowledgeofLmax leadingtoanaturallyhyperparameter-freealgorithm(Section3).Toourknowledge thisistheﬁrstalgorithmtoaddresstheunknown-LmaxissuewhilemaintainingO(kuklogkuk)dependenceonu.Finally wepresentempiricalresultsshowingthatRESCALEDEXPperformswellinpractice(Section4).2LowerBoundwithUnknownLmaxThefollowingtheoremrulesoutalgorithmsthatachieveregretO(ulog(u)Lmax√T)withoutpriorknowledgeofLmax.Infact anysuchalgorithmmustpayanup-frontpenaltythatisexponentialinT.ThislowerboundresolvesaCOLT2016openproblem(Parameter-FreeandScale-FreeOnlineAlgorithms)[12]inthenegative.1Infullgenerality asubgradientisanelementofthedualspaceW∗.However wewillonlyconsidercaseswherethesubgradientisnaturallyidentiﬁedwithanelementintheoriginalspaceW(e.g.Wisﬁnitedimensional)sothatthedeﬁnitionintermsofdot-productssufﬁces.2TherearealgorithmsthatdonotrequireLmax butachieveonlyregretO(kuk2)[11]2Theorem1.Foranyconstantsc k >0 thereexistsaTandanadversarialstrategypickinggt∈Rinresponsetowt∈Rsuchthatregretis:RT(u)=TXt=1gtwt−gtu≥(k+ckuklogkuk)Lmax√Tlog(Lmax+1)+kLmaxexp((2T)1/2−)≥(k+ckuklogkuk)Lmax√Tlog(Lmax+1)+kLmaxexp"(cid:18)maxtkgtkL(t)(cid:19)1/2−#forsomeu∈RwhereLmax=maxt≤TkgtkandL(t)=maxt0<tkgt0k.Proof.WeprovethetheorembyshowingthatforsufﬁcientlylargeT theadversarycan“checkmate”thelearnerbypresentingitonlywiththesubgradientgt=−1.Ifthelearnerfailstohavewtincreasequickly thenthereisau(cid:29)1againstwhichthelearnerhashighregret.Ontheotherhand ifthelearnereverdoesmakewthigherthanaparticularthreshold theadversaryimmediatelypunishesthelearnerwithasubgradientgt=2T againresultinginhighregret.LetTbelargeenoughsuchthatbothofthefollowinghold:T4exp(T1/24log(2)c)>klog(2)√T+kexp((2T)1/2−)(1)T2exp(T1/24log(2)c)>2kTexp((2T)1/2−)+2kT√Tlog(2T+1)(2)Theadversaryplaysthefollowingstrategy:forallt≤T solongaswt<12exp(T1/2/4log(2)c) givegt=−1.Assoonaswt≥12exp(T1/2/4log(2)c) givegt=2Tandgt=0forallsubsequentt.Let’sanalyzetheregretattimeTinthesetwocases.Case1:wt<12exp(T1/2/4log(2)c)forallt:Inthiscase letu=exp(T1/2/4log(2)c).ThenLmax=1 maxtkgtkL(t)=1 andusing(1)thelearner’sregretisatleastRT(u)≥Tu−T12exp(T1/24log(2)c)=12Tu=culog(u)√Tlog(2)+T4exp(T1/24log(2)c)>culog(u)Lmax√Tlog(Lmax+1)+kLmax√Tlog(Lmax+1)+kLmaxexp((2T)1/2−)=(k+culogu)Lmax√Tlog(Lmax+1)+kLmaxexphmaxt(2T)1/2−iCase2:wt≥12exp(T1/2/4log(2)c)forsomet:Inthiscase Lmax=2TandmaxtkgtkL(t)=2T.Foru=0 using(2) theregretisatleastRT(u)≥T2exp(T1/24log(2)c)≥2kTexp((2T)1/2−)+2kT√Tlog(2T+1)=kLmaxexp((2T)1/2−)+kLmax√Tlog(Lmax+1)=(k+culogu)Lmax√Tlog(Lmax+1)+kLmaxexphmaxt(2T)1/2−iTheexponentiallower-boundarisesbecausethelearnerhastomoveexponentiallyfastinordertodealwithexponentiallyfarawayu butthenexperiencesexponentialregretiftheadversaryprovidesagradientofunprecedentedmagnitudeintheoppositedirection.However ifweplayagainstanadversarythatisconstrainedtogivelossvectorskgtk≤LmaxforsomeLmaxthatdoesnotgrowwithtime orifthelossesdonotgrowtooquickly thenwecanstillachieveRT(u)=O(kuklog(kuk)Lmax√T)asymptoticallywithoutknowingLmax.Inthefollowingsectionswedescribeanalgorithmthataccomplishesthis.33RESCALEDEXPOuralgorithm RESCALEDEXP adaptstotheunknownLmaxusingaguess-and-doublestrategythatisrobusttoasmallnumberofbound-violatinggts.WeinitializeaguessLforLmaxtokg1k.Thenwerunanovelknown-Lmaxalgorithmthatcanachievegoodregretintheunconstrainedusetting.Assoonasweseeagtwithkgtk>2L weupdateourguesstokgtkandrestarttheknown-Lmaxalgorithm.Toprovethatthisschemeiseffective weshow(Lemma3)thatourknown-Lmaxalgorithmdoesnotsuffertoomuchregretwhenitseesagtthatviolatesitsassumedbound.Ourknown-LmaxalgorithmusestheFollow-the-Regularized-Leader(FTRL)framework.FTRLisanintuitivewaytodesignOCOalgorithms[13]:Givenfunctionsψt:W→R attimeTweplaywT=argminhψT−1(w)+PT−1t=1‘t(w)i.Thefunctionsψtarecalledregularizers.AlargenumberofOCOalgorithms(e.g.gradientdescent)canbecleanlyformulatedasinstancesofthisframework.Ourknown-LmaxalgorithmisFTRLwithregularizersψt(w)=ψ(w)/ηt whereψ(w)=(kwk+1)log(kwk+1)−kwkandηtisascale-factorthatweadaptovertime.Speciﬁcally wesetη−1t=k√2pMt+kgk21:t whereweusethecompressedsumnotationsg1:T=PTt=1gtandkgk21:T=PTt=1kgtk2.MtisdeﬁnedrecursivelybyM0=0andMt=max(Mt−1 kg1:tk/p−kgk21:t) sothatMt≥Mt−1 andMt+kgk21:t≥kg1:tk/p.kandpareconstants:k=√2andp=L−1max.RESCALEDEXP’sstrategyistomaintainanestimateLtofLmaxatalltimesteps.Wheneveritobserveskgtk≥2Lt itupdatesLt+1=kgtk.WecallperiodsduringwhichLtisconstantepochs.EverytimeitupdatesLt itrestartsourknown-Lmaxalgorithmwithp=1Lt beginninganewepoch.NoticethatsinceLtatleastdoubleseveryepoch therewillbeatmostlog2(Lmax/L1)+1totalepochs.Toaddressedgecases wesetwt=0untilwesufferanon-constantlossfunction andwesettheinitialvalueofLttobetheﬁrstnon-zerogt.Pseudo-codeisgiveninAlgorithm1 andTheorem2statesourregretbound.Forsimplicity were-indexsothatthatg1istheﬁrstnon-zerogradientreceived.Noregretissufferedwhengt=0sothisdoesnotaffectouranalysis.Algorithm1RESCALEDEXPInitialize:k←√2 M0←0 w1←0 t?←1//t?isthestart-timeofthecurrentepoch.fort=1toTdoPlaywt receivesubgradientgt∈∂‘t(wt).ift=1thenL1←kg1kp←1/L1endifMt←max(Mt−1 kgt?:tk/p−kgk2t?:t).ηt←1k√2(Mt+kgk2t?:t)//Setwt+1usingFTRLupdatewt+1←−gt?:tkgt?:tk[exp(ηtkgt?:tk)−1]//=argminwhψ(w)ηt+gt?:twiifkgtk>2Ltthen//Beginanewepoch:updateLandrestartFTRLLt+1←kgtkp←1/Lt+1t?←t+1Mt←0wt+1←0elseLt+1←LtendifendforTheorem2.LetWbeaseparablerealinner-productspacewithcorrespondingnormk·kandsuppose(withmildabuseofnotation)everylossfunction‘t:W→Rhassomesubgradientgt∈W∗atwtsuchthatgt(w)=gt·wforsomegt∈W.LetMmax=maxtMt.ThenifLmax=maxtkgtk4andL(t)=maxt0<tkgtk rescaledexpachievesregret:RT(u)≤(2ψ(u)+96)(cid:18)log2(cid:18)LmaxL1(cid:19)+1(cid:19)qMmax+kgk21:T+8Lmax(cid:18)log2(cid:18)LmaxL1(cid:19)+1(cid:19)min(cid:20)exp(cid:18)8maxtkgtk2L(t)2(cid:19) exp(pT/2)(cid:21)=O(cid:18)Lmaxlog(cid:18)LmaxL1(cid:19)(cid:20)(kuklog(kuk)+2)√T+exp(cid:18)8maxtkgtk2L(t)2(cid:19)(cid:21)(cid:19)TheconditionsonWinTheorem2arefairlymild.InparticulartheyaresatisﬁedwheneverWisﬁnite-dimensionalandinmostkernelmethodsettings[14].Inthekernelmethodsetting WisanRKHSoffunctionsX→Randourlossestaketheform‘t(w)=‘t(hw kxti)wherekxtistherepresentingelementinWofsomext∈X sothatgt=gtkxtwheregt∈∂‘t(hw kxti).Althoughwenearlymatchourlower-boundexponentialtermofexp((2T)1/2−) inordertohaveapracticalalgorithmweneedtodomuchbetter.Fortunately themaxtkgtk2L(t)2termmaybesigniﬁcantlysmallerwhenthelossesarenotfullyadversarial.Forexample ifthelossvectorsgtsatisfykgtk=t2 thentheexponentialterminourboundreducestoamanageableconstanteventhoughkgtkisgrowingquicklywithoutbound.ToproveTheorem2 weboundtheregretofRESCALEDEXPduringeachepoch.Recallthatduringanepoch RESCALEDEXPisrunningFTRLwithψt(w)=ψ(w)/ηt.ThereforeourﬁrstorderofbusinessistoanalyzetheregretofFTRLacrossoneoftheseepochs whichwedoinLemma3(provedinappendix):Lemma3.Setk=√2.Supposekgtk≤Lfort<T 1/L≤p≤2/L gT≤LmaxandLmax≥L.LetWmax=maxt∈[1 T]kwtk.ThentheregretofFTRLwithregularizersψt(w)=ψ(w)/ηtis:RT(u)≤ψ(u)/ηT+96qMT+kgk21:T+2Lmaxmin(cid:20)Wmax 4exp(cid:18)4L2maxL2(cid:19) exp(pT/2)(cid:21)≤(2ψ(u)+96)vuutT−1Xt=1L|gt|+L2max+8Lmaxmin(cid:20)exp(cid:18)4L2maxL2(cid:19) exp(pT/2)(cid:21)≤Lmax(2((kuk+1)log(kuk+1)−kuk)+96)√T+8Lmaxmin(cid:20)e4L2maxL2 e√T/2(cid:21)Lemma3requiresustoknowthevalueofLinordertosetp.However thecrucialpointisthatitencompassesthecaseinwhichLismisspeciﬁedonthelastlossvector.ThisallowsustoshowthatRESCALEDEXPdoesnotsuffertoomuchbyupdatingpon-the-ﬂy.ProofofTheorem2.ThetheoremfollowsbyapplyingLemma3toeachepochinwhichLtisconstant.Let1=t1 t2 t3 ··· tnbethevariousincreasingvaluesoft?(asdeﬁnedinAlgorithm1) andwedeﬁnetn+1=T+1.ThendeﬁneRa:b(u)=b−1Xt=agt(wt−u)sothatRT(u)≤Pnj=1Rtj:tj+1(u).WewillboundRtj:tj+1(u)foreachj.Fixaparticularj<n.ThenRtj:tj+1(u)issimplytheregretofFTRLwithk=√2 p=1Ltj ηt=1kq2(Mt+kgk2tj:t)andregularizersψ(w)/ηt.BydeﬁnitionofLt fort∈[1 tj+1−2]wehavekgtk≤2Ltj.Further ifL=maxt∈[1 tj+1−2]kgtkwehaveL≥Ltj.Therefore Ltj≤L≤2Ltjsothat1L≤p≤2L.Further wehavekgtj+1−1k/Ltj≤2maxtkgtk/L(t).ThusbyLemma3we5haveRtj:tj+1(u)≤ψ(u)/ηtj+1−1+96qMtj+1−1+kgk2tj:tj+1−1+2Lmaxmin"Wmax 4exp 4kgtj+1−1k2L2tj! exp(cid:18)√tj+1−tj√2(cid:19)#≤ψ(u)/ηtj+1−1+96qMmax+kgk2tj:tj+1−1+8Lmaxmin(cid:20)e8maxtkgtk2L(t)2 e√T/2(cid:21)≤(2ψ(u)+96)qMmax+kgk21:T+8Lmaxmin(cid:20)exp(cid:18)8maxtkgtk2L(t)2(cid:19) exp(pT/2)(cid:21)Summingacrossepochs wehaveRT(u)=nXj=1Rtj:tj+1(u)≤n(cid:20)(2ψ(u)+96)qMmax+kgk21:T+8Lmaxmin(cid:20)exp(cid:18)8maxtkgtk2L(t)2(cid:19) exp(cid:16)pT/2(cid:17)(cid:21)(cid:21)Observethatn≤log2(Lmax/L1)+1toprovetheﬁrstlineofthetheorem.Thebig-Ohexpressionfollowsfromtheinequality:Mtj+1−1≤LtjPtj+1−1t=tjkgtk≤LmaxPTt=1kgtk.Ourspeciﬁcchoicesforkandparesomewhatarbitrary.Wesuspect(althoughwedonotprove)thattheprecedingtheoremsaretrueforlargervaluesofkandanypinverselyproportionaltoLt albeitwithdifferingconstants.InSection4weperformexperimentsusingthevaluesfork pandLtdescribedinAlgorithm1.Inkeepingwiththespiritofdesigningahyperparameter-freealgorithm noattemptwasmadetoempiricallyoptimizethesevaluesatanytime.4Experiments4.1LinearClassiﬁcationTovalidateourtheoreticalresultsinpractice weevaluatedRESCALEDEXPon8classiﬁcationdatasets.Thedataforeachtaskwaspulledfromthelibsvmwebsite[15] andcanbefoundindividuallyinavarietyofsources[16 17 18 19 20 21 22].Weuselinearclassiﬁerswithhinge-lossforeachtaskandwecompareRESCALEDEXPtoﬁveotheroptimizationalgorithms:ADAGRAD[5] SCALEINVARIANT[23] PISTOL[24] ADAM[25] andADADELTA[26].EachofthesealgorithmsrequirestuningofsomehyperparameterforunconstrainedproblemswithunknownLmax(usuallyascale-factoronalearningrate).Incontrast ourRESCALEDEXPrequiresnosuchtuning.Weevaluateeachalgorithmwiththeaveragelossafteronepassthroughthedata computingaprediction anerror andanupdatetomodelparametersforeachexampleinthedataset.Notethatthisisnotthesameasacross-validatederror butisclosertothenotionofregretaddressedinourtheorems.WeplotthisaveragelossversushyperparametersettingforeachdatasetinFigures1and2.ThesedatabearouttheeffectivenessofRESCALEDEXP:whileitisnotunilaterallythehighestperformeronalldatasets itshowsremarkablerobustnessacrossdatasetswithzeromanualtuning.4.2ConvolutionalNeuralNetworksWealsoevaluatedRESCALEDEXPontwoconvolutionalneuralnetworkmodels.Thesemodelshavedemonstratedremarkablesuccessincomputervisiontasksandarebecomingincreasinglymorepopularinavarietyofareas butcanrequiresigniﬁcanthyperparametertuningtotrain.WeconsidertheMNIST[18]andCIFAR-10[27]imageclassiﬁcationtasks.OurMNISTarchitectureconsistedoftwoconsecutive5×5convolutionand2×2max-poolinglayersfollowedbya512-neuronfully-connectedlayer.OurCIFAR-10architecturewastwoconsecutive5×5convolutionand3×3max-poolinglayersfollowedbya384-neuronfully-connectedlayeranda192-neuronfully-connectedlayer.610-510-410-310-210-1100101102103hyperparameter setting10-210-1100101average losscovtypePiSTOLScale InvariantADAMAdaDeltaAdaGradRescaledExp10-510-410-310-210-1100101102103hyperparameter setting10-210-1100average lossgisette_scalePiSTOLScale InvariantADAMAdaDeltaAdaGradRescaledExp10-510-410-310-210-1100101102103hyperparameter setting0.300.350.400.450.500.550.60average lossmadelonPiSTOLScale InvariantADAMAdaDeltaAdaGradRescaledExp10-510-410-310-210-1100101102103hyperparameter setting10-210-1100average lossmnistPiSTOLScale InvariantADAMAdaDeltaAdaGradRescaledExpFigure1:Averagelossvshyperparametersettingforeachalgorithmacrosseachdataset.RESCALED-EXPhasnohyperparametersandsoisrepresentedbyaﬂatyellowline.Manyoftheotheralgorithmsdisplaylargesensitivitytohyperparametersetting.Thesemodelsarehighlynon-convex sothatnoneofourtheoreticalanalysisapplies.OuruseofRESCALEDEXPismotivatedbythefactthatinpracticeconvexmethodsareusedtotrainthesemodels.WefoundthatRESCALEDEXPcanmatchtheperformanceofotherpopularalgorithms(seeFigure3).Inordertoachievethisperformance wemadeaslightmodiﬁcationtoRESCALEDEXP:whenweupdateLt insteadofresettingwttozero were-centerthealgorithmaboutthepreviouspredictionpoint.Weprovidenotheoreticaljustiﬁcationforthismodiﬁcation butonlynotethatitmakesintuitivesenseinstochasticoptimizationproblems whereonecanreasonablyexpectthatthepreviouspredictionvectorisclosertotheoptimalvaluethanzero.5ConclusionsWehavepresentedRESCALEDEXP anOnlineConvexOptimizationalgorithmthatachievesregret˜O(kuklog(kuk)Lmax√T+exp(8maxtkgtk2/L(t)2))whereLmax=maxtkgtkisunknowninadvance.SinceRESCALEDEXPdoesnotuseanyprior-knowledgeaboutthelossesorcomparisonvectoru itishyperparameterfreeandsodoesnotrequireanytuningoflearningrates.Wealsoprovealower-boundshowingthatanyalgorithmthataddressestheunknown-Lmaxscenariomustsufferanexponentialpenaltyintheregret.WecompareRESCALEDEXPtoprioroptimizationalgorithmsempiricallyandshowthatitmatchestheirperformance.Whileourlower-boundmatchesourregretboundforRESCALEDEXPintermsofT clearlythereismuchworktobedone.Forexample whenRESCALEDEXPisrunontheadversariallosssequencepresentedinTheorem1 itsregretmatchesthelower-bound suggestingthattheoptimalitygapcouldbeimprovedwithsuperioranalysis.Wealsohopethatourlower-boundinspiresworkinalgorithmsthatadapttonon-adversarialpropertiesofthelossestoavoidtheexponentialpenalty.710-510-410-310-210-1100101102103hyperparameter setting10-210-1100average lossijcnn1PiSTOLScale InvariantADAMAdaDeltaAdaGradRescaledExp10-510-410-310-210-1100101102103hyperparameter setting10-1100average lossepsilon_normalizedPiSTOLScale InvariantADAMAdaDeltaAdaGradRescaledExp10-510-410-310-210-1100101102103hyperparameter setting10-1100average lossrcv1_train.multiclassPiSTOLScale InvariantADAMAdaDeltaAdaGradRescaledExp10-510-410-310-210-1100101102103hyperparameter setting10-1100average lossSenseIT Vehicle CombinedPiSTOLScale InvariantADAMAdaDeltaAdaGradRescaledExpFigure2:Averagelossvshyperparametersetting continuedfromFigure1.Figure3:WecompareRESCALEDEXPtoADAM ADAGRAD andstochasticgradientdescent(SGD) withlearning-ratehyperparameteroptimizationforthelatterthreealgorithms.Allalgorithmsachieveaﬁnalvalidationaccuracyof99%onMNISTand84% 84% 83%and85%respectivelyonCIFAR-10(after40000iterations).References[1]MartinZinkevich.Onlineconvexprogrammingandgeneralizedinﬁnitesimalgradientascent.InProceed-ingsofthe20thInternationalConferenceonMachineLearning(ICML-03) pages928–936 2003.[2]ShaiShalev-Shwartz.Onlinelearningandonlineconvexoptimization.FoundationsandTrendsinMachineLearning 4(2):107–194 2011.[3]NickLittlestone.Fromon-linetobatchlearning.InProceedingsofthesecondannualworkshoponComputationallearningtheory pages269–284 2014.8[4]NicoloCesa-Bianchi AlexConconi andClaudioGentile.Onthegeneralizationabilityofon-linelearningalgorithms.InformationTheory IEEETransactionson 50(9):2050–2057 2004.[5]J.Duchi E.Hazan andY.Singer.Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.InConferenceonLearningTheory(COLT) 2010.[6]H.BrendanMcMahanandMatthewStreeter.Adaptiveboundoptimizationforonlineconvexoptimization.InProceedingsofthe23rdAnnualConferenceonLearningTheory(COLT) 2010.[7]BrendanMcmahanandMatthewStreeter.No-regretalgorithmsforunconstrainedonlineconvexoptimiza-tion.InAdvancesinneuralinformationprocessingsystems pages2402–2410 2012.[8]FrancescoOrabona.Dimension-freeexponentiatedgradient.InAdvancesinNeuralInformationProcessingSystems pages1806–1814 2013.[9]BrendanMcMahanandJacobAbernethy.Minimaxoptimalalgorithmsforunconstrainedlinearoptimiza-tion.InAdvancesinNeuralInformationProcessingSystems pages2724–2732 2013.[10]JacobAbernethy PeterLBartlett AlexanderRakhlin andAmbujTewari.Optimalstrategiesandmin-imaxlowerboundsforonlineconvexgames.InProceedingsofthenineteenthannualconferenceoncomputationallearningtheory 2008.[11]FrancescoOrabonaandDávidPál.Scale-freeonlinelearning.arXivpreprintarXiv:1601.01974 2016.[12]FrancescoOrabonaandDávidPál.Openproblem:Parameter-freeandscale-freeonlinealgorithms.InConferenceonLearningTheory 2016.[13]S.Shalev-Shwartz.OnlineLearning:Theory Algorithms andApplications.PhDthesis TheHebrewUniversityofJerusalem 2007.[14]ThomasHofmann BernhardSchölkopf andAlexanderJSmola.Kernelmethodsinmachinelearning.Theannalsofstatistics pages1171–1220 2008.[15]Chih-ChungChangandChih-JenLin.Libsvm:Alibraryforsupportvectormachines.ACMTransactionsonIntelligentSystemsandTechnology(TIST) 2(3):27 2011.[16]IsabelleGuyon SteveGunn AsaBen-Hur andGideonDror.Resultanalysisofthenips2003featureselectionchallenge.InAdvancesinNeuralInformationProcessingSystems pages545–552 2004.[17]Chih-chungChangandChih-JenLin.Ijcnn2001challenge:Generalizationabilityandtextdecoding.InInProceedingsofIJCNN.IEEE.Citeseer 2001.[18]YannLeCun LéonBottou YoshuaBengio andPatrickHaffner.Gradient-basedlearningappliedtodocumentrecognition.ProceedingsoftheIEEE 86(11):2278–2324 1998.[19]DavidDLewis YimingYang TonyGRose andFanLi.Rcv1:Anewbenchmarkcollectionfortextcategorizationresearch.TheJournalofMachineLearningResearch 5:361–397 2004.[20]MarcoFDuarteandYuHenHu.Vehicleclassiﬁcationindistributedsensornetworks.JournalofParallelandDistributedComputing 64(7):826–838 2004.[21]M.Lichman.UCImachinelearningrepository 2013.[22]ShimonKogan DimitryLevin BryanRRoutledge JacobSSagi andNoahASmith.Predictingriskfromﬁnancialreportswithregression.InProceedingsofHumanLanguageTechnologies:The2009AnnualConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics pages272–280.AssociationforComputationalLinguistics 2009.[23]FrancescoOrabona KobyCrammer andNicoloCesa-Bianchi.Ageneralizedonlinemirrordescentwithapplicationstoclassiﬁcationandregression.MachineLearning 99(3):411–435 2014.[24]FrancescoOrabona.Simultaneousmodelselectionandoptimizationthroughparameter-freestochasticlearning.InAdvancesinNeuralInformationProcessingSystems pages1116–1124 2014.[25]DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980 2014.[26]MatthewDZeiler.Adadelta:Anadaptivelearningratemethod.arXivpreprintarXiv:1212.5701 2012.[27]AlexKrizhevskyandGeoffreyHinton.Learningmultiplelayersoffeaturesfromtinyimages 2009.[28]H.BrendanMcMahan.Asurveyofalgorithmsandanalysisforadaptiveonlinelearning.arXivpreprintarXiv:1403.3465 2014.9,Ashok Cutkosky
Kwabena Boahen
Wittawat Jitkrittum
Heishiro Kanagawa
Patsorn Sangkloy
James Hays
Bernhard Schölkopf
Arthur Gretton