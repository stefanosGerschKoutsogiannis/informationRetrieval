2013,One-shot learning and big data with n=2,We model a one-shot learning" situation  where very few (scalar) observations $y_1 ... y_n$ are available. Associated with each observation $y_i$ is a very high-dimensional vector $x_i$  which provides context for $y_i$ and enables us to predict subsequent observations  given their own context. One of the salient features of our analysis is that the problems studied here are easier when the dimension of $x_i$ is large; in other words  prediction becomes easier when more context is provided. The proposed methodology is a variant of principal component regression (PCR). Our rigorous analysis sheds new light on PCR. For instance  we show that classical PCR estimators may be inconsistent in the specified setting  unless they are multiplied by a scalar $c > 1$; that is  unless the classical estimator is expanded. This expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods ($c < 1$)  which are far more common in big data analyses. ",One-shot learning and big data with n = 2

Lee H. Dicker

Rutgers University

Piscataway  NJ

ldicker@stat.rutgers.edu

Dean P. Foster

University of Pennsylvania

Philadelphia  PA

dean@foster.net

Abstract

We model a “one-shot
learning” situation  where very few observations
y1  ...  yn ∈ R are available. Associated with each observation yi is a very high-
dimensional vector xi ∈ Rd  which provides context for yi and enables us to pre-
dict subsequent observations  given their own context. One of the salient features
of our analysis is that the problems studied here are easier when the dimension
of xi is large; in other words  prediction becomes easier when more context is
provided. The proposed methodology is a variant of principal component regres-
sion (PCR). Our rigorous analysis sheds new light on PCR. For instance  we show
that classical PCR estimators may be inconsistent in the speciﬁed setting  unless
they are multiplied by a scalar c > 1; that is  unless the classical estimator is ex-
panded. This expansion phenomenon appears to be somewhat novel and contrasts
with shrinkage methods (c < 1)  which are far more common in big data analyses.

1

Introduction

The phrase “one-shot learning” has been used to describe our ability – as humans – to correctly
recognize and understand objects (e.g. images  words) based on very few training examples [1  2].
Successful one-shot learning requires the learner to incorporate strong contextual information into
the learning algorithm (e.g. information on object categories for image classiﬁcation [1] or “function
words” used in conjunction with a novel word and referent in word-learning [3]). Variants of one-
shot learning have been widely studied in literature on cognitive science [4  5]  language acquisition
(where a great deal of relevant work has been conducted on “fast-mapping”) [3  6–8]  and computer
vision [1  9]. Many recent statistical approaches to one-shot learning  which have been shown to
perform effectively in a variety of examples  rely on hierarchical Bayesian models  e.g. [1–5  8].
In this article  we propose a simple latent factor model for one-shot learning with continuous out-
comes. We propose effective methods for one-shot learning in this setting  and derive risk approx-
imations that are informative in an asymptotic regime where the number of training examples n
is ﬁxed (e.g. n = 2) and the number of contextual features for each example d diverges. These
approximations provide insight into the signiﬁcance of various parameters that are relevant for one-
shot learning. One important feature of the proposed one-shot setting is that prediction becomes
“easier” when d is large – in other words  prediction becomes easier when more context is provided.
Binary classiﬁcation problems that are “easier” when d is large have been previously studied in
the literature  e.g. [10–12]; this article may contain the ﬁrst analysis of this kind with continuous
outcomes.
The methods considered in this paper are variants of principal component regression (PCR) [13].
Principal component analysis (PCA) is the cornerstone of PCR. High-dimensional PCA (i.e. large
d) has been studied extensively in recent literature  e.g. [14–22]. Existing work that is especially
relevant for this paper includes that of Lee et al. [19]  who studied principal component scores in
high dimensions  and work by Hall  Jung  Marron and co-authors [10  11  18  21]  who have studied
“high dimension  low sample size” data  with ﬁxed n and d → ∞  in a variety of contexts  including

1

PCA. While many of these results address issues that are clearly relevant for PCR (e.g. consis-
tency or inconsistency of sample eigenvalues and eigenvectors in high dimensions)  their precise
implications for high-dimensional PCR are unclear.
In addition to addressing questions about one-shot learning  which motivate the present analysis 
the results in this paper provide new insights into PCR in high dimensions. We show that the clas-
sical PCR estimator is generally inconsistent in the one-shot learning regime  where n is ﬁxed and
d → ∞. To remedy this  we propose a bias-corrected PCR estimator  which is obtained by expand-
ing the classical PCR estimator (i.e. multiplying it by a scalar c > 1). Risk approximations obtained
in Section 5 imply that the bias-corrected estimator is consistent when n is ﬁxed and d → ∞. These
results are supported by a simulation study described in Section 7  where we also consider an “ora-
cle” PCR estimator for comparative purposes. It is noteworthy that the bias-corrected estimator is an
expanded version of the classical estimator. Shrinkage  which would correspond to multiplying the
classical estimator by a scalar 0 ≤ c < 1  is a far more common phenomenon in high-dimensional
data analysis  e.g. [23–25] (however  expansion is not unprecedented; Lee et al. [19] argued for
bias-correction via expansion in the analysis of principal component scores).

2 Statistical setting
Suppose that the observed data consists of (y1  x1)  ...  (yn  xn)  where yi ∈ R is a scalar outcome
and xi ∈ Rd is an associated d-dimensional “context” vector  for i = 1  ...  n. Suppose that yi and
xi are related via

√

du + i ∈ Rd  i ∼ N (0  τ 2I)  i = 1  ...  n.

yi = hiθ + ξi ∈ R  hi ∼ N (0  η2)  ξi ∼ N (0  σ2) 
xi = hiγ

(1)
(2)
The random variables hi  ξi and the random vectors i = (i1  ...  id)T   1 ≤ i ≤ n  are all assumed
to be independent; hi is a latent factor linking the outcome yi and the vector xi; ξi and i are
random noise. The unit vector u = (u1  ...  ud)T ∈ Rd and real numbers θ  γ ∈ R are taken to be
non-random. It is implicit in our normalization that the “x-signal” ||hiγ
du||2 (cid:16) d is quite strong.
Observe that (yi  xi) ∼ N (0  V ) are jointly normal with

√

(cid:18) θ2η2 + σ2

√

V =

√

θη2γ

duT

(cid:19)

θη2γ

du τ 2I + η2γ2duuT

.

(3)

To further simplify notation in what follows  let y = (y1  ...  yn)T = hθ + ξ ∈ Rn  where h =
(h1  ...  hn)T   ξ = (ξ1  ...  ξn)T ∈ Rn  and let X = (x1  ...  xn)T = γ
dhuT + E  where E =
(ij)1≤i≤n  1≤j≤d.
Given the observed data (y  X)  our objective is to devise prediction rules ˆy : Rd → R so that the
risk

√

RV (ˆy) = EV {ˆy(xnew) − ynew}2 = EV {ˆy(xnew) − hnewθ}2 + σ2

(4)

is small  where (ynew  xnew) = (hnewθ + ξnew  hnewγ
du + new) has the same distribution as
(yi  xi) and is independent of (y  X). The subscript “V ” in RV and EV indicates that the parameters
θ  η  σ  τ  γ  u are speciﬁed by V   as in (3); similarly  we will write PV (·) to denote probabilities with
the parameters speciﬁed by V .
We are primarily interested in identifying methods ˆy that perform well (i.e. RV (ˆy) is small) in
an asymptotic regime whose key features are (i) n is ﬁxed  (ii) d → ∞  (iii) σ2 → 0  and (iv)
inf η2γ2/τ 2 > 0. We suggest that this regime reﬂects a one-shot learning setting  where n is small
and d is large (captured by (i)-(ii) from the previous sentence)  and there is abundant contextual
information for predicting future outcomes (which is ensured by (iii)-(iv)). In a speciﬁed asymptotic
regime (not necessarily the one-shot regime)  we say that a prediction method ˆy is consistent if
RV (ˆy) → 0. Weak consistency is another type of consistency that is considered below. We say that
ˆy is weakly consistent if |ˆy − ynew| → 0 in probability. Clearly  if ˆy is consistent  then it is also
weakly consistent.

√

2

new

√

i β  where β =
du/(τ 2 + η2γ2d). This suggests studying linear prediction rules of the form ˆy(xnew) =
ˆβ  for some estimator ˆβ of β. In this paper  we restrict our attention to linear prediction rules 

3 Principal component regression
By assumption  the data (yi  xi) are multivariate normal. Thus  EV (yi|xi) = xT
θγη2
xT
focusing on estimators related to principal component regression (PCR).
Let l1 ≥ ··· ≥ ln∧d ≥ 0 denote the ordered n largest eigenvalues of X T X and let ˆu1  ...  ˆun∧d de-
note corresponding eigenvectors with unit length; ˆu1  ...  ˆun∧d are also referred to as the “principal
components” of X. Let Uk = (ˆu1 ··· ˆuk) be the d × k matrix with columns given by ˆu1  ...  ˆuk 
for 1 ≤ k ≤ n ∧ d. In its most basic form  principal component regression involves regressing y
on XUk for some (typically small) k  and taking ˆβ = Uk(U T
k X T y. In the problem
considered here the predictor covariance matrix Cov(xi) = τ 2I + η2γ2duuT has a single eigen-
value larger than τ 2 and the corresponding eigenvector is parallel to β. Thus  it is natural to restrict
our attention to PCR with k = 1; more explicitly  consider
1
l1

k X T XUk)−1U T

ˆuT
1 X T yˆu1.

ˆuT
1 X T y
1 X T X ˆu1

ˆβpcr =

ˆu1 =

(5)

ˆuT

In the following sections  we study consistency and risk properties of ˆβpcr and related estimators.

4 Weak consistency and big data with n = 2

Before turning our attention to risk approximations for PCR in Section 5 below (which contains
the paper’s main technical contributions)  we discuss weak consistency in the one-shot asymptotic
regime  devoting special attention to the case where n = 2. This serves at least two purposes. First 
it provides an illustrative warm-up for the more complex risk bounds obtained in Section 5. Second 
it will become apparent below that the risk of the consistent PCR methods studied in this paper
depends on inverse moments of χ2 random variables. For very small n  these inverse moments do
not exist and  consequently  the risk of the associated prediction methods may be inﬁnite. The main
implication of this is that the risk bounds in Section 5 require n ≥ 9 to ensure their validity. On the
other hand  the weak consistency results obtained in this section are valid for all n ≥ 2.

4.1 Heuristic analysis for n = 2

Recall the PCR estimator (5) and let ˆypcr(x) = xT ˆβpcr be the associated linear prediction rule.
For n = 2  the largest eigenvalue of X T X and the corresponding eigenvector are given by simple
explicit formulas:

l1 =

||x1||2 + ||x2||2 +

(||x1||2 − ||x2||2)2 + 4(xT

1 x2)2

||x1||2 − ||x2||2 +

(||x1||2 − ||x2||2)2 + 4(xT

1 x2)2

x1 + x2.

These expressions for l1 and ˆu1 yield an explicit expression for ˆβpcr when n = 2 and facilitate
a simple heuristic analysis of PCR  which we undertake in this subsection. This analysis suggests
that ˆypcr is not consistent when σ2 → 0 and d → ∞ (at least for n = 2). However  the analysis
also suggests that consistency can be achieved by multiplying ˆβpcr by a scalar c ≥ 1; that is  by
expanding ˆβpcr. This observation leads us to consider and rigorously analyze a bias-corrected PCR
method  which we ultimately show is consistent in ﬁxed n settings  if σ2 → 0 and d → ∞. On the
other hand  it will also be shown below that ˆypcr is inconsistent in one-shot asymptotic regimes.
For large d  the basic approximations ||xi||2 ≈ γ2dh2
following approximation for ˆypcr(xnew):

1 x2 ≈ γ2dhihj lead to the

1 + τ 2d and xT

ˆypcr(xnew) = xT

new

ˆβpcr ≈ γ2(h2

1 + h2
2)

γ2(h2

1 + h2

2) + τ 2 hnewθ + epcr 

(6)

3

(cid:26)
(cid:26)

1
2

and ˆu1 = ˆv1/||ˆv1||2  where

ˆv1 =

1
2xT
1 x2

(cid:113)
(cid:113)

(cid:27)
(cid:27)

where

Thus 

epcr =

{γ2d(h2

γ2hnew
1 + h2

ˆuT
1 X T ξ.

2) + τ 2d}2
τ 2
2) + τ 2 hnewθ + epcr − ξnew.
1 + h2

ˆypcr(xnew) − ynew ≈ −

γ2(h2

(7)
The second and third terms on the right-hand side in (7)  epcr − ξnew  represent a random error
that vanishes as d → ∞ and σ2 → 0. On the other hand  the ﬁrst term on the right-hand side in
(7)  −τ 2hnewθ/{γ2(h2
2) + τ 2}  is a bias term that is  in general  non-zero when d → ∞ and
σ2 → 0; in other words ˆypcr is inconsistent. This bias is apparent in the expression for ˆypcr(xnew)
given in (6); in particular  the ﬁrst term on the right-hand side of (6) is typically smaller than hnewθ.
One way to correct for the bias of ˆypcr is to multiply ˆβpcr by
2) + τ 2

1 + h2

1 + h2

l1

≥ 1 

≈ γ2(h2

γ2(h2

1 + h2
2)

(cid:26)

l1 − l2

||x1||2 + ||x2||2 −(cid:113)

(||x1||2 − ||x2||2)2 + 4(xT

1 x2)2

≈ τ 2d

(cid:27)

where

l2 =

1
2

is the second-largest eigenvalue of X T X. Deﬁne the bias-corrected principal component regression
estimator

ˆβbc =

l1

l1 − l2

ˆβpcr =

1

l1 − l2

ˆuT
1 X T y

and let ˆybc(x) = xT ˆβbc be the associated linear prediction rule. Then ˆybc(xnew) = xT
hnewθ + ebc  where

new

ˆβbc ≈

ebc =

{γ2(h2

1 + h2

hnew
2) + τ 2}(h2

1 + h2

2)d2

ˆuT
1 X T ξ.

One can check that if d → ∞  σ2 → 0 and θ  η2  η2  τ 2 are well-behaved (e.g. contained in a
compact subset of (0 ∞))  then ˆybc(xnew) − ynew ≈ ebc → 0 in probability; in other words  ˆybc
is weakly consistent. Indeed  weak consistency of ˆybc follows from Theorem 1 below. On the other
hand  note that E|ebc| = ∞. This suggests that RV (ˆybc) = ∞  which in fact may be conﬁrmed by
direct calculation. Thus  when n = 2  ˆybc is weakly consistent  but not consistent.

4.2 Weak consistency for bias-corrected PCR
Now suppose that n ≥ 2 is arbitrary and that d ≥ n. Deﬁne the bias-corrected PCR estimator

ˆβbc =

l1

l1 − ln

ˆβpcr =

1

l1 − ln

ˆuT
1 X T yˆu1

(8)

and the associated linear prediction rule ˆybc(x) = xT ˆβbc. The main weak consistency result of the
paper is given below.
Theorem 1. Suppose that n ≥ 2 is ﬁxed and let C ⊆ (0 ∞) be a compact set. Let r > 0 be an
arbitrary but ﬁxed positive real number. Then

PV {|ˆybc(xnew) − ynew| > r} = 0.

On the other hand 

lim
d→∞
σ2→0

θ η τ γ∈C

sup
u∈Rd

lim inf
d→∞
σ2→0

θ η τ γ∈C

inf
u∈Rd

PV {|ˆypcr(xnew) − ynew| > r} > 0.

(9)

(10)

A proof of Theorem 1 follows easily upon inspection of the proof of Theorem 2  which may be
found in the Supplementary Material. Theorem 1 implies that in the speciﬁed ﬁxed n asymptotic
setting  bias-corrected PCR is weakly consistent (9) and that the more standard PCR method ˆypcr
is inconsistent (10). Note that the condition θ  η  τ  γ ∈ C in (9) ensures that the x-data signal-to-
noise ratio η2γ2/τ 2 is bounded away from 0. In (8)  it is noteworthy that l1/(l1 − ln) ≥ 1: in
order to achieve (weak) consistency  the bias corrected estimator ˆβbc is obtained by expanding ˆβpcr.
By contrast  shrinkage is a far more common method for obtaining improved estimators in many
regression and prediction settings (the literature on shrinkage estimation is vast  perhaps beginning
with [23]).

4

5 Risk approximations and consistency
In this section  we present risk approximations for ˆypcr and ˆybc that are valid when n ≥ 9. A more
careful analysis may yield approximations that are valid for smaller n; however  this is not pursued
further here.
Theorem 2. Let Wn ∼ χ2

n be a chi-squared random variable with n degrees of freedom.

(a) If n ≥ 9 and d ≥ 1  then

RV (ˆypcr) = σ2

(cid:20)

(b) If d ≥ n ≥ 9  then

(cid:40)

RV (ˆybc) = σ2

1 + E

1 + E

+θ2η2EV

η2γ2

(cid:32)
η2γ2Wn + τ 2(cid:112)n/d
(cid:26) l1
(cid:40)

(uT ˆu1)2 − 1

l1 − ln

(cid:27)(cid:21)

+ O

(cid:40)

(η2γ2Wn + τ 2)2

η4γ4Wn

(cid:26)
(cid:8)(uT ˆu1)2 − 1(cid:9)2
(cid:33)(cid:41)
(cid:27)2

+ O

(cid:19)

n

+ O

(cid:114) n
(cid:18) σ2
(cid:19)
(cid:18) θ2η2τ 2
(cid:32)

η2γ2d + τ 2

.

d + n

η2γ2n + τ 2(cid:112)n/d

τ 2

σ2√
dn

(cid:32)
(cid:40)

η2γ2 + τ 2

η2γ2Wn + τ 2(cid:112)n/d
η2γ2n + τ 2(cid:112)n/d

τ 2

+

(cid:33)(cid:41)
(η2γ2n + τ 2(cid:112)n/d)2

τ 4

(11)

(12)

(cid:41)(cid:35)

.

(cid:33)(cid:41)

+θ2η2EV

+

θ2η2τ 2

η2γ2d + τ 2

(cid:34)

+O

θ2η2τ 2

η2γ2d + τ 2

d

1 + E

(cid:114) n

A proof of Theorem 2 (along with intermediate lemmas and propositions) may be found in the
Supplementary Material. The necessity of the more complex error term in Theorem 2 (b) (as opposed
to that in part (a)) will become apparent below.
When d is large  σ2 is small  and θ  η  τ  γ ∈ C  for some compact subset C ⊆ (0 ∞)  Theorem 2
suggests that

RV (ˆypcr) ≈ θ2η2EV
RV (ˆybc) ≈ θ2η2EV

(cid:8)(uT ˆu1)2 − 1(cid:9)2
(cid:26) l1

 

l1 − ln

(uT ˆu1)2 − 1

.

(cid:27)2

Thus  consistency of ˆypcr and ˆybc in the one-shot regime hinges on asymptotic properties of
EV {(uT ˆu1)2 − 1}2 and EV {l1/(l1 − ln)(uT ˆu1)2 − 1}2. The following proposition is proved
in the Supplementary Material.
Proposition 1. Let Wn ∼ χ2

n be a chi-squared random variable with n degrees of freedom.

(a) If n ≥ 9 and d ≥ 1  then

(b) If d ≥ n ≥ 9  then

(cid:19)2

(cid:18)(cid:114) n

= E

η2γ2Wn + τ 2

+ O

d + n

EV

(cid:8)(uT ˆu1)2 − 1(cid:9)2
(cid:26) l1

(cid:18)
(cid:27)2

τ 2

(cid:40)

EV

l1 − ln

(uT ˆu1)2 − 1

= O

(η2γ2n + τ 2(cid:112)n/d)2

τ 4

·

n
d

(cid:19)
(cid:41)

.

.

Proposition 1 (a) implies that in the one-shot regime  EV {(uT ˆu1)2 − 1}2 → E{τ 2/(η2γ2Wn +
τ 2)2} (cid:54)= 0; by Theorem 2 (a)  it follows that ˆypcr is inconsistent. On the other hand  Proposition 1
(b) implies that EV
(b)  ˆybc is consistent. These results are summarized in Corollary 1  which follows immediately from
Theorem 2 and Proposition 1.

(cid:8)l1/(l1 − ln)(uT ˆu1)2 − 1(cid:9)2 → 0 in the one-shot regime; thus  by Theorem 2

5

Corollary 1. Suppose that n ≥ 9 is ﬁxed and let C ⊆ (0 ∞) be a compact set. Let Wn ∼ χ2
chi-squared random variable with n degrees of freedom. Then

n be a

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)RV (ˆypcr) − θ2η2E

(cid:18)

τ 2

η2γ2Wn + τ 2

(cid:19)2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = 0.

lim
d→∞
σ2→0

θ η τ γ∈C

sup
u∈Rd

and

lim
d→∞
σ2→0

θ η τ γ∈C

sup
u∈Rd

RV (ˆybc) = 0.

(cid:8)l1/(l1 − ln)(uT ˆu1)2 − 1(cid:9)2; this could potentially be leveraged to obtain better

For ﬁxed n  and inf η2γ2/τ 2 > 0  the bound in Proposition 1 (b) is of order 1/d. This suggests that
both terms (11)-(12) in Theorem 2 (b) have similar magnitude and  consequently  are both necessary
to obtain accurate approximations for RV (ˆybc). (It may be desirable to obtain more accurate approx-
imations for EV
approximations for RV (ˆybc).) In Theorem 2 (a)  the only non-vanishing term in the one-shot ap-
proximation for RV (ˆypcr) involves EV {(uT ˆu1)2 − 1}2; this helps to explain the relative simplicity
of this approximation  in comparison with Theorem 2 (b).
Theorem 2 and Proposition 1 give risk approximations that are valid for all d and n ≥ 9. How-
ever  as illustrated by Corollary 1  these approximations are most effective in a one-shot asymptotic
setting  where n is ﬁxed and d is large. In the one-shot regime  standard concepts  such as sam-
ple complexity – roughly  the sample size n required to ensure a certain risk bound – may be of
secondary importance. Alternatively  in a one-shot setting  one might be more interested in metrics
like “feature complexity”: the number of features d required to ensure a given risk bound. Approx-
imate feature complexity for ˆybc is easily computed using Theorem 2 and Proposition 1 (clearly 
feature complexity depends heavily on model parameters  such as θ  the y-data noise level σ2  and
the x-data signal-to-noise ratio η2γ2/τ 2).

6 An oracle estimator

In this section  we discuss a third method related to ˆypcr and ˆybc  which relies on information that
is typically not available in practice. Thus  this method is usually non-implementable; however  we
believe it is useful for comparative purposes.
Recall that both ˆybc and ˆypcr depend on the ﬁrst principal component ˆu1  which may be viewed as
an estimate of u. If an oracle provides knowledge of u in advance  then it is natural to consider the
oracle PCR estimator

ˆβor =

uT X T y
uT X T Xu

u

and the associated linear prediction rule ˆyor(x) = xT ˆβor. A basic calculation yields the following
result.
Proposition 2. If n ≥ 3  then

(cid:18)

RV (ˆyor) =

σ2 +

θ2η2τ 2

η2γ2d + τ 2

1 +

1

n − 2

(cid:19)(cid:18)

(cid:19)

.

Clearly  ˆyor is consistent in the one-shot regime: if C ⊆ (0 ∞) is compact and n ≥ 3 is ﬁxed  then

lim
d→∞
σ2→0

θ η τ γ∈C

sup
u∈Rd

RV (ˆyor) = 0.

7 Numerical results

In this section  we describe the results of a simulation study where we compared the performance of
ˆypcr  ˆybc  and ˆyor. We ﬁxed θ = 4  σ2 = 1/10  η2 = 4  γ2 = 1/4  τ 2 = 1  and u = (1  0  ....  0) ∈

6

Rd and simulated 1000 independent datasets with various d  n. Observe that η2γ2/τ 2 = 1. For each
simulated dataset  we computed ˆβpcr  ˆβbc  ˆβor and the corresponding conditional prediction error

RV (ˆy|y  X) = E(cid:2){ˆy(xnew) − ynew}2(cid:12)(cid:12) y  X(cid:3)

= (ˆβ − β)T (τ 2I + η2γ2duuT )(ˆβ − β) + σ2 +

θ2η2

ψ2d + 1

 

for ˆy = ˆypcr  ˆybc  ˆyor. The empirical prediction error for each method ˆy was then computed by av-
eraging RV (ˆy|y  X) over all 1000 simulated datasets. We also computed the“theoretical” prediction
error for each method  using the results from Sections 5-6  where appropriate. More speciﬁcally  for
ˆypcr and ˆybc  we used the leading terms of the approximations in Theorem 2 and Proposition 1 to
obtain the theoretical prediction error; for ˆyor  we used the formula given in Proposition 2 (see Table
1 for more details). Finally  we computed the relative error between the empirical prediction error

Table 1: Formulas for theoretical prediction error used in simulations (derived from Theorem 2 and
Propositions 1-2). Expectations in theoretical prediction error expressions for ˆypcr and ˆybc were
computed empirically.

(cid:26)

σ2

ˆypcr

ˆybc

ˆyor

σ2(cid:104)

(cid:18)

1 + E

1 + E

η2γ2
η2γ2Wn+τ 2

+ θ2η2τ 2
η2γ2d+τ 2

l1−ln

+ θ2η2E

(η2γ2Wn+τ 2)2

+ θ2η2EV

η4γ4Wn
√

n/d

(cid:18)

(cid:111)(cid:105)
(cid:19)(cid:27)

Theoretical prediction error formula

(cid:16)
(cid:110)
(cid:110) l1
(cid:19)(cid:27)
(cid:26)
(cid:17)
(cid:16)
(cid:12)(cid:12)(cid:12)(cid:12) (Empirical PE) − (Theoretical PE)

σ2 + θ2η2τ 2
η2γ2d+τ 2

n/d
1 + 1
n−2

Empirical PE

(cid:17)(cid:16)

η2γ2+τ 2

√

η2γ2Wn+τ 2

1 + E

(cid:17)2

(cid:111)2

τ 2

η2γ2Wn+τ 2

(uT ˆu1)2 − 1

(cid:12)(cid:12)(cid:12)(cid:12) × 100%.

Relative Error =

and the theoretical prediction error for each method 

Table 2: d = 500. Prediction error for ˆypcr (PCR)  ˆybc (Bias-corrected PCR)  and ˆyor (oracle). Rel-
ative error for comparing Empirical PE and Theoretical PE is given in parentheses. “NA” indicates
that Theoretical PE values are unknown.

Theoretical PE (Relative Error) NA

Theoretical PE (Relative Error) NA

n = 2 Empirical PE

n = 4 Empirical PE

n = 9 Empirical PE

n = 20 Empirical PE

PCR

18.7967

6.4639

1.4187

0.4513

Bias-corrected

PCR

(∞)

4.8668

∞

0.8023

NA

0.3565

0.2732

Oracle

1.5836

∞

(∞)

0.3268
0.3416 (4.53%)
0.2587

0.2398

Theoretical PE (Relative Error) 1.2514 (11.79%) 0.2857 (19.86%) 0.2603 (0.62%)

Theoretical PE (Relative Error) 0.2987 (33.81%) 0.2497 (8.60%) 0.2404 (0.25%)

The results of the simulation study are summarized in Tables 2-3. Observe that ˆybc has smaller
empirical prediction error than ˆypcr in every setting considered in Tables 2-3  and ˆybc substantially
outperforms ˆypcr in most settings. Indeed  the empirical prediction error for ˆybc when n = 9 is
smaller than that of ˆypcr when n = 20 (for both d = 500 and d = 5000); in other words  ˆybc
outperforms ˆypcr  even when ˆypcr has more than twice as much training data. Additionally  the
empirical prediction error of ˆybc is quite close to that of the oracle method ˆyor  especially when n
is relatively large. These results highlight the effectiveness of the bias-corrected PCR method ˆybc in
settings where σ2 and n are small  η2γ2/τ 2 is substantially larger than 0  and d is large.
For n = 2  4  theoretical prediction error is unavailable in some instances. Indeed  while Proposition
2 and the discussion in Section 4 imply that if n = 2  then RV (ˆybc) = RV (ˆyor) = ∞  we have not

7

Table 3: d = 5000. Prediction error for ˆypcr (PCR)  ˆybc (Bias-corrected PCR)  and ˆyor (oracle).
Relative error comparing Empirical PE and Theoretical PE is given in parentheses. “NA”’ indicates
that Theoretical PE values are unknown.

PCR

Bias-corrected

PCR

Oracle

Theoretical PE (Relative Error) NA

Theoretical PE (Relative Error) NA

n = 2 Empirical PE

n = 4 Empirical PE

n = 9 Empirical PE

n = 20 Empirical PE

17.9564

6.1220

1.2274

0.3150

2.0192

∞

0.2039

NA

0.1378

0.1226

(∞)

1.0316

∞

(∞)

0.1637
0.1692 (3.36%)
0.1281

0.1189

Theoretical PE (Relative Error) 1.2485 (1.72%) 0.1314 (4.64%) 0.1289 (0.62%)

Theoretical PE (Relative Error) 0.2997 (4.86%) 0.1200 (2.12%) 0.1191 (0.17%)

pursued an expression for RV (ˆypcr) when n = 2 (it appears that RV (ˆypcr) < ∞); furthermore  the
approximations in Theorem 2 for RV (ˆypcr)  RV (ˆybc) do not apply when n = 4. In instances where
theoretical prediction error is available  is ﬁnite  and d = 500  the relative error between empirical
and theoretical prediction error for ˆypcr and ˆybc ranges from 8.60%-33.81%; for d = 5000  it ranges
from 1.72%-4.86%. Thus  the accuracy of the theoretical prediction error formulas tends to improve
as d increases  as one would expect. Further improved measures of theoretical prediction error
for ˆypcr and ˆybc could potentially be obtained by reﬁning the approximations in Theorem 2 and
Proposition 1.

8 Discussion

In this article  we have proposed bias-corrected PCR for consistent one-shot learning in a simple
latent factor model with continuous outcomes. Our analysis was motivated by problems in one-shot
learning  as discussed in Section 1. However  the results in this paper may also be relevant for
other applications and techniques related to high-dimensional data analysis  such as those involving
reproducing kernel Hilbert spaces. Furthermore  our analysis sheds new light on PCR  a long-studied
method for regression and prediction.
Many open questions remain. For instance  consider the semi-supervised setting  where additional
unlabeled data xn+1  ...  xN is available  but the corresponding yi’s are not provided. Then the
additional x-data could be used to obtain a better estimate of the ﬁrst principal component u and
perhaps devise a method whose performance is closer to that of the oracle procedure ˆyor (indeed 
ˆyor may viewed as a semi-supervised procedure that utilizes an inﬁnite amount of unlabeled data
to exactly identify u). Is bias-correction via inﬂation necessary in this setting? Presumably  bias-
correction is not needed if N is large enough  but can this be made more precise? The simulations
described in the previous section indicate that ˆybc outperforms the uncorrected PCR method ˆypcr
in settings where twice as much labeled data is available for ˆypcr. This suggests that role of bias-
correction will remain signiﬁcant in the semi-supervised setting  where additional unlabeled data
(which is less informative than labeled data) is available. Related questions involving transductive
learning [26  27] may also be of interest for future research.
A potentially interesting extension of the present work involves multi-factor models. As opposed
to the single-factor model (1)-(2)  one could consider a more general k-factor model  where yi =
i θ + ξi and xi = Shi + i; here hi = (hi1  ...  hik)T ∈ Rk is a multivariate normal random vector
hT
d(γ1u1 ··· γkuk)
(a k-dimensional factor linking yi and xi)  θ = (θ1  ...  θk)T ∈ Rk  and S =
is a k × d matrix  with γ1  ...  γk ∈ R and unit vectors u1  ...  uk ∈ Rd. It may also be of interest
to work on relaxing the distributional (normality) assumptions made in this paper. Finally  we point
out that the results in this paper could potentially be used to develop ﬂexible probit (latent variable)
models for one-shot classiﬁcation problems.

√

References
[1] L. Fei-Fei  R. Fergus  and P. Perona. One-shot learning of object categories. Pattern Analysis and Machine

Intelligence  IEEE Transactions on  28:594–611  2006.

8

[2] R. Salakhutdinov  J.B. Tenenbaum  and A. Torralba. One-shot learning with a hierarchical nonparametric
Bayesian model. JMLR Workshop and Conference Proceedings Volume 26: Unsupervised and Transfer
Learning Workshop  27:195–206  2012.

[3] M.C. Frank  N.D. Goodman  and J.B. Tenenbaum. A Bayesian framework for cross-situational word-

learning. Advances in Neural Information Processing Systems  20:20–29  2007.

[4] J.B. Tenenbaum  T.L. Grifﬁths  and C. Kemp. Theory-based Bayesian models of inductive learning and

reasoning. Trends in Cognitive Sciences  10:309–318  2006.

[5] C. Kemp  A. Perfors  and J.B. Tenenbaum. Learning overhypotheses with hierarchical Bayesian models.

Developmental Science  10:307–321  2007.

[6] S. Carey and E. Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language

Conference  15:17–29  1978.

[7] L.B. Smith  S.S. Jones  B. Landau  L. Gershkoff-Stowe  and L. Samuelson. Object name learning provides

on-the-job training for attention. Psychological Science  13:13–19  2002.

[8] F. Xu and J.B. Tenenbaum. Word learning as Bayesian inference. Psychological Review  114:245–272 

2007.

[9] M. Fink. Object classiﬁcation from a single example utilizing class relevance metrics. Advances in Neural

Information Processing Systems  17:449–456  2005.

[10] P. Hall  J.S. Marron  and A. Neeman. Geometric representation of high dimension  low sample size data.

Journal of the Royal Statistical Society: Series B (Statistical Methodology)  67:427–444  2005.

[11] P. Hall  Y. Pittelkow  and M. Ghosh. Theoretical measures of relative performance of classiﬁers for high
dimensional data with small sample sizes. Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  70:159–173  2008.

[12] Y.I. Ingster  C. Pouet  and A.B. Tsybakov. Classiﬁcation of sparse high-dimensional vectors. Philosoph-
ical Transactions of the Royal Society A: Mathematical  Physical and Engineering Sciences  367:4427–
4448  2009.

[13] W.F. Massy. Principal components regression in exploratory statistical research. Journal of the American

Statistical Association  60:234–256  1965.

[14] I.M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis. Annals of

Statistics  29:295–327  2001.

[15] D. Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance model. Statistica

Sinica  17:1617–1642  2007.

[16] B. Nadler. Finite sample approximation results for principal component analysis: A matrix perturbation

approach. Annals of Statistics  36:2791–2817  2008.

[17] I.M. Johnstone and A.Y. Lu. On consistency and sparsity for principal components analysis in high

dimensions. Journal of the American Statistical Association  104:682–693  2009.

[18] S. Jung and J.S. Marron. PCA consistency in high dimension  low sample size context. Annals of Statis-

tics  37:4104–4130  2009.

[19] S. Lee  F. Zou  and F.A. Wright. Convergence and prediction of principal component scores in high-

dimensional settings. Annals of Statistics  38:3605–3629  2010.

[20] Q. Berthet and P. Rigollet. Optimal detection of sparse principal components in high dimension. arXiv

preprint arXiv:1202.5070  2012.

[21] S. Jung  A. Sen  and J.S Marron. Boundary behavior in high dimension  low sample size asymptotics of

PCA. Journal of Multivariate Analysis  109:190–203  2012.

[22] Z. Ma. Sparse principal component analysis and iterative thresholding. Annals of Statistics  41:772–801 

2013.

[23] C. Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In
Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability  volume 1 
pages 197–206  1955.

[24] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B (Methodological)  58:267–288  1996.

[25] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning: Data Mining  Inference 

and Prediction. Springer  2nd edition  2009.

[26] V.N. Vapnik. Statistical Learning Theory. Wiley  1998.
[27] K.S. Azoury and M.K. Warmuth. Relative loss bounds for on-line density estimation with the exponential

family of distributions. Machine Learning  43:211–246  2001.

9

,Lee Dicker
Dean Foster