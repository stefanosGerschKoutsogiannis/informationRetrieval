2019,Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost,Despite the empirical success of the actor-critic algorithm  its theoretical understanding lags behind. In a broader context  actor-critic can be viewed as an online alternating update algorithm for bilevel optimization  whose convergence is known to be fragile. To understand the instability of actor-critic  we focus on its application to linear quadratic regulators  a simple yet fundamental setting of reinforcement learning. We establish a nonasymptotic convergence analysis of actor- critic in this setting. In particular  we prove that actor-critic finds a globally optimal pair of actor (policy) and critic (action-value function) at a linear rate of convergence. Our analysis may serve as a preliminary step towards a complete theoretical understanding of bilevel optimization with nonconvex subproblems  which is NP-hard in the worst case and is often solved using heuristics.,Provably Global Convergence of Actor-Critic: A Case

for Linear Quadratic Regulator with Ergodic Cost

Zhuoran Yang

Princeton University
zy6@princeton.edu

Yongxin Chen

Georgia Institute of Technology

yongchen@gatech.edu

Mingyi Hong

University of Minnesota

mhong@umn.edu

Zhaoran Wang

Northwestern University

zhaoran.wang@northwestern.edu

Abstract

Despite the empirical success of the actor-critic algorithm  its theoretical under-
standing lags behind. In a broader context  actor-critic can be viewed as an online
alternating update algorithm for bilevel optimization  whose convergence is known
to be fragile. To understand the instability of actor-critic  we focus on its application
to linear quadratic regulators  a simple yet fundamental setting of reinforcement
learning. We establish a nonasymptotic convergence analysis of actor-critic in this
setting. In particular  we prove that actor-critic ﬁnds a globally optimal pair of
actor (policy) and critic (action-value function) at a linear rate of convergence. Our
analysis may serve as a preliminary step towards a complete theoretical understand-
ing of bilevel optimization with nonconvex subproblems  which is NP-hard in the
worst case and is often solved using heuristics.

1

Introduction

The actor-critic algorithm [36] is one of the most used algorithms in reinforcement learning [46].
Compared with the classical policy gradient algorithm [73]  actor-critic tracks the action-value
function (critic) in policy gradient in an online manner  and alternatively updates the policy (actor)
and the critic. On the one hand  the online update of critic signiﬁcantly reduces the variance of policy
gradient and hence leads to faster convergence. On the other hand  it also introduces algorithmic
instability  which is often observed in practice [33] and parallels the notoriously unstable training
of generative adversarial networks [50]. Such instability of actor-critic originates from several
intertwining challenges  including (i) function approximation of actor and critic  (ii) improper choice
of stepsizes  (iii) the noise arising from stochastic approximation  (iv) the asynchrony between actor
and critic  and (v) possibly off-policy data used in the update of critic. As a result  the convergence
of actor-critic remains much less well understood than that of policy gradient  which itself is open.
Consequently  the practical use of actor-critic often lacks theoretical guidance.
In this paper  we aim to theoretically understand the algorithmic instability of actor-critic.
In
particular  under a bilevel optimization framework  we establish the global rate of convergence and
sample complexity of actor-critic for linear quadratic regulators (LQR) with ergodic cost  a simple
yet fundamental setting of reinforcement learning [54]  which captures all the above challenges.
Compared with the classical two-timescale analysis of actor-critic [11]  which is asymptotic in
nature and requires ﬁnite action space  our analysis is fully nonasymptotic and allows for continuous
action space. Moreover  beyond the convergence to a stable equilibrium obtained by the classical
two-timescale stochastic approximation via ordinary differential equations  we for the ﬁrst time
establish the linear rate of convergence to a globally optimal pair of actor and critic. In addition  we

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

characterize the required sample complexity. As a technical ingredient and byproduct  we for the
ﬁrst time establish the sublinear rate of convergence for the gradient temporal difference algorithm
[66  67] for ergodic cost and dependent data  which is of independent interest. Furthermore  although
we only focus on the setting of LQR  our theoretical analysis framework can be readily extended to
general RL problems with other policy optimization methods for the actor (e.g. trust-region policy
optimization (TRPO) [57] and proximal policy optimization (PPO) [58]) and other policy evaluation
methods for the critic such as TD(0) [65].
Our work adds to two lines of works in machine learning  stochastic analysis  and optimization:

(i) Actor-critic falls into the more general paradigm of bilevel optimization [42  24  4]. Bilevel
optimization is deﬁned by two nested optimization problems  where the upper-level optimization
problem relies on the output of the lower-level one. As a special case of bilevel optimization  minimax
optimization is prevalent in machine learning. Recent instances include training generative adversarial
neural networks [27]  (distributionally) robust learning [63]  and imitation learning [31  15]. Such
instances of minimax optimization remain challenging as they lack convexity-concavity in general
[25  56  16  53  38  17  18  19  41]. The more general paradigm of bilevel optimization remains even
more challenging  as there does not exist a uniﬁed objective function for simultaneous minimization
and maximization. In particular  actor-critic couples the nonconvex optimization of actor (policy
gradient) as its upper level and the convex-concave minimax optimization of critic (gradient temporal
difference) as its lower level  each of which is challenging to analyze by itself. Most existing
convergence analysis of bilevel optimization is based on two-timescale analysis [10]. However 
as two-timescale analysis abstracts away most technicalities via the lens of ordinary differential
equations  which is asymptotic in nature  it often lacks the resolution to capture the nonasymptotic
rate of convergence and sample complexity  which are obtained via our analysis.

(ii) As a proxy for analyzing more general reinforcement learning settings  LQR is studied in a recent
line of works [12  54  26  69  70  22  23  61  21  30]. In particular  a part of our analysis is based
on the breakthrough of [26]  which gives the global convergence of the population-version policy
gradient algorithm for LQR and its ﬁnite-sample version based on the zeroth-order estimation of
policy gradient based on the cumulative reward or cost. However  such zeroth-order estimation of
policy gradient often suffers from large variance  as it involves the randomness of an entire trajectory.
In contrast  actor-critic updates critic in an online manner  which reduces such variance but also
introduces instability and complicates the convergence analysis. In particular  as the update of critic
interleaves with the update of actor  the policy gradient for the update of actor is biased due to the
inexactness of critic. Meanwhile  the update of critic has a “moving target”  as it attempts to evaluate
an actor that evolves along the iterations. A key to our analysis is to handle such asynchrony between
actor and critic  which is a ubiquitous challenge in bilevel optimization. We hope our analysis may
serve as the ﬁrst step towards analyzing actor-critic in more general reinforcement learning settings.
Notation. For any integer n > 0  we denote {1  . . .   n} to be [n]. For any symmetric matrix X  let
svec(X) denote the vectorization of the upper triangular submatrix of X with off-diagonal entries
weighted by p2. Hence  for any symmetric matrices X and Y   we have tr(XY ) = hX  Y i =
svec(X)> svec(Y ). Meanwhile  let smat(·) be the inverse operation of svec(·)  which maps a vector
to a symmetric matrix. Besides  we denote by A ⌦s B the symmetric Kronecker product of A and B.
We use kvk2 to denote the `2-norm of a vector v. Finally  for a matrix A  we use kAk  kAkfro  and
⇢(A) to denote its the operator norm  Frobenius norm  and spectral radius  respectively.

2 Background

In the following  we introduce the background of actor-critic and LQR. In particular  we show that
actor-critic can be cast as a ﬁrst-order online alternating update algorithm for a bilevel optimization
problem [42  24  4].

2.1 Actor-Critic Algorithm
We consider a Markov decision process  which is deﬁned by (X  U  P  c  D0). Here X and U are
the state and action spaces  respectively  P : X⇥U!P
(X ) is the Markov transition kernel 
c : X⇥U! R is the cost function  and D0 2P (X ) is the distribution of the initial state x0. For

2

any t  0  at the t-th time step  the agent takes action ut 2U at state xt 2X   which incurs a cost
c(xt  ut) and moves the environment into a new state xt+1 ⇠ P (·| xt  ut). A policy speciﬁes how
the action ut is taken at a given state xt. Speciﬁcally  in order to handle inﬁnite state and action
spaces  we focus on a parametrized policy class {⇡! : X!P (U) ! 2 ⌦}  where ! is the parameter
of policy ⇡!  and the agent takes action u ⇠ ⇡!(·| x) at a given state x 2X . The agent aims to ﬁnd
a policy that minimizes the inﬁnite-horizon time-average cost  that is 

minimize

!2⌦

J(!) = lim
T!1

E 1

T

Moreover  for policy ⇡!  we deﬁne the (advantage) action-value and state-value functions respectively
as

TXt=0

c(xt  ut) x0 ⇠ D0  ut ⇠ ⇡!(·| xt) 8t  0.
E!⇥c(xt  ut)| x0 = x  u0 = u⇤  J(!) 

(2.1)

(2.2)

Q!(x  u) =Xt0
V!(x) = Eu⇠⇡!(· | x)⇥Q!(x  u)⇤ 

r!J(!) = Ex⇠⇢! u⇠⇡!(· | x)⇥r! log ⇡!(u| x) · Q!(x  u)⇤ 

where we use E! to indicate that the state-action pairs {(xt  ut)}t1 are obtained from policy ⇡!.
Actor-critic is based on the idea of solving the minimization problem in (2.1) via ﬁrst-order optimiza-
tion  which uses an estimator of r!J(!). In detail  by the policy gradient theorem [68  6  36]  we
have
(2.3)
where ⇢! 2P (X ) is the stationary distribution induced by ⇡!. Based on (2.3)  actor-critic [36]
consists two steps: (i) a policy evaluation step that estimates the action-value function Q! (critic)
via temporal difference learning [20]  where Q! is estimated using a parametrized function class
{Q✓ : ✓ 2 ⇥}  and (ii) a policy improvement step that updates the parameter ! of policy ⇡! (actor)
using a stochastic version of the policy gradient in (2.3)  where Q! is replaced by the corresponding
estimator Q✓.
As shown in [74]  actor-critic can be cast as solving a bilevel optimization problem  which takes the
form

minimize

!2⌦

Ex⇠⇢! u⇠⇡!(· | x)⇥Q✓(x  u)⇤ 

subject to (✓  J ) = argmin
✓2⇥ J2R

Ex⇠⇢! u⇠⇡!(· | s)n⇥Q✓(x  u) + J  c(x  u)  (B!Q✓)(x  u)⇤2o 

(2.5)
where B! is an operator that depends on ⇡!. In this problem  the actor and critic correspond to the
upper-level and lower-level variables  respectively. Under this framework  the policy update can be
viewed as a stochastic gradient step for the upper-level problem in (2.4). The objective in (2.5) is
usually the mean-squared Bellman error or mean-squared projected Bellman error [20]. Moreover 
when B! is the Bellman evaluation operator associated with ⇡! and we solve the lower-level problem
in (2.5) via stochastic semi-gradient descent  we obtain the TD(0) update for policy evaluation [65].
Similarly  when B! is the projected Bellman evaluation operator associated with ⇡!  solving the
lower level problem naturally recovers the GTD2 and TDC algorithms for policy evaluation [8].
Therefore  the actor-critic algorithm is a ﬁrst-order online algorithm for the bilevel optimization
problem in (2.4) and (2.5). We remark that bilevel optimization contains a family of extremely
challenging problems. Even when the objective functions are linear  bilevel programming is NP-hard
[29]. In practice  various heuristic algorithms are applied to solve them approximately [62].

(2.4)

2.2 Linear Quadratic Regulator
As the simplest optimal control problem  linear quadratic regulator serves as a perfect baseline to
examine the performance of reinforcement learning methods. Viewing LQR from the lens of an MDP 
the state and action spaces are X = Rd and U = Rk  respectively. The state transition dynamics and
cost function are speciﬁed by

xt+1 = Axt + But + ✏t 

c(x  u) = x>Qx + u>Ru 

(2.6)

3

where ✏t ⇠ N (0  ) is the random noise that is i.i.d. for each t  0  and A  B  Q  R  are matrices
of proper dimensions with Q  R   0. Moreover  we assume that the dimensions d and k are
ﬁxed throughout this paper. For the problem of minimizing the inﬁnite-horizon time-average cost
lim supT!1 T 1PT
t=0 E[c(xt  ut)] with x0 ⇠ D0  it is known that the optimal actions are linear in
the corresponding state [76  3  7]. Speciﬁcally  the optimal actions {u⇤t}t0 satisfy u⇤t = K⇤xt for
all t  0  where K⇤ 2 Rk⇥d can be written as K⇤ = (R + B>P ⇤B)1B>P ⇤A  with P ⇤ being the
solution to the discrete algebraic Riccati equation

P ⇤ = Q + A>P ⇤A + A>P ⇤B(R + B>P ⇤B)1B>P ⇤A.

(2.7)

In the optimal control literature  it is common to solve LQR by ﬁrst estimating matrices A  B  Q 
R and then solving the Riccati equation in (2.7) with these matrices replaced by their estimates.
Such an approach is known as model-based as it requires estimating the model parameters and the
performance of the planning step in (2.7) hinges on how well the true model is estimated. See  e.g 
[21  70] for theoretical guarantees of model-based methods.
In contrast  from a purely data-driven perspective  the framework of model-free reinforcement
learning offers a general treatment for optimal control problems without the prior knowledge of the
model. Thanks to its simple structure  LQR enables us to assess the performances of reinforcement
learning algorithms from a theoretical perspective. Speciﬁcally  it is shown that policy iteration
[12  14  45]  adaptive dynamic programming [52]  and policy gradient methods [26  44  70] are all
able to obtain the optimal policy of LQR. Also see [54] for a thorough review of reinforcement
learning methods in the setting of LQR.

3 Actor-Critic Algorithm for LQR
In this section  we establish the actor-critic algorithm for the LQR problem introduced in §2.2. Recall
that the optimal policy of LQR is a linear function of the state. Throughout the rest of this paper  we
focus on the family of linear-Gaussian policies

⇡K(·| x) = N (Kx  2Ik)  K 2 Rk⇥d  

(3.1)
where > 0 is a ﬁxed constant. That is  for any t  0  at state xt  we could write the action ut
by ut = Kxt +  · ⌘t  where ⌘t ⇠ N (0  Ik). We note that if  = 0  then the optimal policy
u = K⇤x belongs to our policy class. Here  instead of focusing on deterministic policies  we
adopt Gaussian policies to encourage exploration. For policy ⇡K  the corresponding time-average
cost J(K)  state-value function VK  and action-value function QK are speciﬁed as in (2.1) and (2.2) 
respectively.
In the following  we ﬁrst establish the policy gradient and value functions for the ergodic LQR in
§3.1. Then  in §3.2  we present the on-policy natural actor-critic algorithm  which is further extended
to the off-policy setting in §B.
3.1 Policy Gradient Theorem for Ergodic LQR

For any policy ⇡K  by (2.6)  the state dynamics are given by a linear dynamical system
"t = ✏t +  · B⌘t ⇠ N (0  ).

(3.2)
Here we deﬁne  := + 2 · BB> in (3.2) to simplify the notation. It is known that  when
⇢(A  BK) < 1  the Markov chain in (3.2) has stationary distribution N (0  ⌃K)  denoted by ⌫K
hereafter  where ⌃K is the unique positive deﬁnite solution to the Lyapunov equation

xt+1 = (A  BK)xt + "t 

where

⌃K =  + (A  BK)⌃K(A  BK)>.

(3.3)

In the following proposition  we establish J(K)  the value functions  and the gradient rKJ(K).
Proposition 3.1. For any K 2 Rk⇥d such that ⇢(A  BK) < 1  let PK be the unique positive
deﬁnite solution to the Bellman equation

PK = (Q + K>RK) + (A  BK)>PK(A  BK).

(3.4)

4

In the setting of LQR  for policy ⇡K  both the state- and action-value functions are quadratic.
Speciﬁcally  we have

VK(x) = x>PKx  tr(PK⌃K) 

QK(x  u) = x>⇥11

K x + x>⇥12

K u + u>⇥21

K x + u>⇥22

(3.5)
K u  2 · tr(R + PKBB>)  tr(PK⌃K) 
(3.6)

where ⌃K is speciﬁed in (3.3)  and we deﬁne matrix ⇥K by

Moreover  the time-average cost J(K) and its gradient are given by

⇥21

A>PKB

B>PKA

K ⇥12
K
K ⇥22

K◆ =✓Q + A>PKA

⇥K =✓⇥11
J(K) = tr⇥(Q + K>RK)⌃K⇤ + 2 · tr(R) = tr(PK ) + 2 · tr(R).
rKJ(K) = 2⇥(R + B>PKB)K  B>PKA⇤⌃K = 2EK⌃K 

R + B>PKB◆ .

(3.7)

(3.8)
(3.9)

where we deﬁne EK := (R + B>PKB)K  B>PKA.
Proof. See §D.1 for a detailed proof.
To see the connection between (3.9) and the policy gradient theorem in (2.3)  note that by direct
computation we have

rK log ⇡K(u| x) = rK⇥(22)1 · (u + Kx)2⇤ = 2 · (u + Kx)x>.

(3.10)
Thus  combining  (3.6)  (3.10)  and the fact that u = Kx +  · ⌘ under ⇡K  the right-hand side of
(2.3) can be written as
Ex⇠⌫K  u⇠⇡K⇥rK log ⇡K(u| x) · QK(x  u)⇤ = 2 · Ex⇠⌫K  ⌘⇠N (0 Ik)[ · ⌘x> · QK(x Kx +  · ⌘)].
Recall that for ⌘ 2 N (0  Ik)  Stein’s identity [64]  E[⌘·f (⌘)] = E[rf (⌘)]  holds for all differentiable
function f : Rk ! R  which implies that

rKJ(K) = Ex⇠⌫K  ⌘⇠N (0 Id)[(ruQK)(x Kx +  · ⌘) · x>]
= 2Ex⇠⌫K  ⌘⇠N (0 Id){[(R + B>PKB)(Kx +  · ⌘) + B>PKAx]x>}
= 2⇥(R + B>PKB)K  B>PKA⇤⌃K = 2EK⌃K.

(3.11)
Thus  (3.9) is exactly the policy gradient theorem (2.3) in the setting of LQR. Moreover  it is worth
noting that Proposition 3.1 also holds for  = 0. Thus  setting  = 0 in (3.11)  we obtain

rKJ(K) = Ex⇠⌫K [(ruQK)(x Kx) · x>] = Ex⇠⌫K⇥(ruQK)(x  u)u=⇡K (x)rK⇡K(x)⇤ 

where ⇡K(x) = Kx. Thus we obtain the deterministic policy gradient theorem [60] for LQR.
Although the optimal policy for LQR is deterministic  due to the lack of exploration  behaving
according to a deterministic policy may lead to suboptimal solutions. Thus  we focus on the family of
stochastic policies where a Gaussian noise  · ⌘ is added to the action so as to promote exploration.
3.2 Natural Actor-Critic Algorithm
Natural policy gradient updates the variable along the steepest direction with respect to Fisher metric.
For the Gaussian policies deﬁned in (3.1)  by (3.10)  the Fisher’s information of policy ⇡K  denoted
by I(K)  is given by
[I(K)](i j) (i0 j0) = Ex⇠⌫K  a⇠⇡K⇥rKij log ⇡K(u| x) ·r Ki0j0 log ⇡K(u| x)⇤
(3.12)
where i  i0 2 [k]  j  j0 2 [d]  Kij and Ki0j0 are the (i  j)- and (i0  j0)-th entries of K  respectively 
and [⌃K]jj0 is the (j  j0)-th entry of ⌃K. Thus  in view of (3.9) in Proposition 3.1 and (3.12)  natural
policy gradient algorithm updates the policy parameter in the direction of

= 2 · Ex⇠⌫K  ⌘⇠N (0 Ik)[⌘ixj · ⌘i0xj0] = 2 · 1{i = i0} · [⌃K]jj0 

[I(K)]1rKJ(K) = rKJ(K)⌃1

K = 2EK.

5

By (3.7)  we can write EK as ⇥22
K   where ⇥K is the coefﬁcient matrix of the quadratic
component of QK. Such a connection lays the foundation of the natural actor-critic algorithm.

K K  ⇥21

Speciﬁcally  in each iteration of the algorithm  the actor updates the policy via K   · (b⇥22K b⇥21) 
where  is the stepsize and b⇥ is an estimator of ⇥K returned by any policy evaluation algorithm.
We present such a general natural actor-critic method in Algorithm 1 §A  under the assumption that
we are given a stable policy K0 for initialization. Such an assumption is standard in literatures on
model-free methods for LQR [22  26  44].
To obtain an online actor-critic algorithm  in the sequel  we propose an online policy evaluation
algorithm for ergodic LQR based on temporal difference learning. Let ⇡K be the policy of interest.
For notational simplicity  for any state-action pair (x  u) 2 Rd+k  we deﬁne the feature function

(x  u) = svec"✓x

u◆✓x

u◆># 

(3.13)

and denote svec(⇥K) by ✓⇤K. Using this notation  the quadratic component in QK can be written as
(x  u)>✓⇤K  and the Bellman equation for QK becomes

h(x  u) ✓ ⇤Ki = c(x  u)  J(K) +⌦E[(x0  u0)| x  u] ✓ ⇤K↵  8(x  u) 2 Rd+k.

(3.14)
In order to further simplify the notation  hereafter  we deﬁne #⇤K = (J(K) ✓ ⇤K>)>  denote by E(x u)
the expectation with respect to x ⇠ ⌫K and u ⇠ ⇡K(·| x)  and let (x0  u0) be the state-action pair
subsequent to (x  u).
Furthermore  to estimate J(K) and ✓⇤K in (3.14) simultaneously  we deﬁne

0

1

✓

(3.15)
Notice that J(K) = E(x u)[c(x  u)]. By direct computation  it can be shown that #⇤K satisﬁes the
following linear equation

⌅K = E(x u)(x  u)⇥(x  u)  (x0  u0)⇤>  
E(x u)[(x  u)] ⌅K◆✓#1

bK = E(x u)⇥c(x  u)(x  u)⇤ 
#2◆ =✓J(K)
bK ◆  

whose solution is unique if and only if ⌅K in (3.15) is invertible. The following lemma shows that 
when ⇡K is a stable policy  ⌅K is indeed invertible.
Lemma 3.2. When ⇡K is stable in the sense that ⇢(A BK) < 1  ⌅K deﬁned in (3.15) is invertible
and thus #⇤K is the unique solution to the linear equation (3.16). Furthermore  the minimum singular
value of the matrix in the left-hand side of (3.16) is lower bounded by a constant ⇤K > 0  where ⇤K
only depends on ⇢(A  BK)    and min( ).
Proof. See §D.2 for a detailed proof.
By this lemma  when ⇢(A BK) < 1  policy evaluation for ⇡K can be reduced to ﬁnding the unique
solution to a linear equation. Instead of solving the equation directly  it is equivalent to minimize the
least-squares loss:

(3.16)

minimize

#

n[#1  J(K)]2 +#1 · E(x u)[(x  u)] + ⌅K#2  bK2
2o 

(3.17)

where #1 2 R and #2 has the same shape as svec(⇥K)  which are the two components of #. It is clear
that the global minimizer of (3.17) is #⇤K. Note that we have Fenchel’s duality x2 = supy{2x·yy2}.
By this relation  we further write (3.17) as a minimax optimization problem

min
#2X⇥

max
!2X⌦

F (#  !) = [#1  J(K)] · !1

+⌦#1 · E(x u)[(x  u)] + ⌅K#2  bK ! 2↵  1/2 ·k !k2

2 

(3.18)

where the dual variable ! = (!1 ! 2) has the same shape as #. Here we restrict the primal and
dual variables to compact sets X⇥ and X⌦ for algorithmic stability  which will be speciﬁed in the
next section. Note that the objective in (3.18) can be estimated unbiasedly using two consecutive
state-action pairs (x  u) and (x0  u0). Solving the minimax optimization in (3.18) using stochastic

6

gradient method  we obtain the gradient-based temporal difference (GTD) algorithm for policy
evaluation [66  67]. See Algorithm 2 for details. More speciﬁcally  by direct computation  we have
(3.19)

r#2F (✓  !) = E(x u)⇥(  0) · >!2⇤ 
r!2F (✓  !) = ✓1 · E(x u)() +⌅ K#2  bK  !2 

r#1F (✓  !) = !1 +⌦E(x u)() ! 2↵ 
r!1F (✓  !) = #1  J(K)  !1 

(3.20)
where we denote (x  u) and (x0  u0) by  and 0  respectively. In the GTD algorithm  we update #
and ! in gradient directions where the gradients in (3.19) and (3.20) are replaced by their sample
t=1 ↵t ·
#2
to update the current policy ⇡K. Therefore  we obtain the online natural actor-critic algorithm [9] for
ergodic LQR.
Meanwhile  using the perspective of bilevel optimization  similar to (2.4) and (2.5)  our actor-critic
algorithm can be viewed as a ﬁrst-order online algorithm for

estimates. After T iterations of the algorithm  we output the averaged update b#2 = (PT
t )/(PT

t=1 ↵t) and useb⇥= smat(b#2) to estimate ⇥K in (3.7)  which is further used in Algorithm 1

minimize
K2Rk⇥d

Ex⇠⌫K  u⇠⇡K⇥h(x  u) # 2i⇤ 

F (#  !) 

subject to (#  !) = argmin
✓2X⇥

argmax
!2X⌦

where F (#  !) is deﬁned in (3.18) and depends on ⇡K. In our algorithm  we solve the upper-level
problem via natural gradient descent and solve the lower-level saddle point optimization problem
using stochastic gradient updates.
Furthermore  we emphasize that our method deﬁned by Algorithms 1 and 2 is online in the sense
that each update only requires a single transition. More speciﬁcally  let {(xn  un  cn)}n0 be the
sequence of transitions experienced by the agent. Combining Algorithms 1 and 2 and neglecting the
projections  we can write the updating rule as

Kn+1 = Kn  n ·[smat(#2

n)]22Kn  [smat(#2
#n+1 = #n  ↵n · g#(xn  un  cn  xn+1  un+1) 
!n+1 = !n + ↵n · g!(xn  un  cn  xn+1  un+1) 

n)]21  

(3.21)

(3.22)

where g# and g! are the update directions of # and ! whose deﬁnitions are clear from Algorithm 2 
{n} and {↵n} are the stepsizes. Moreover  there exists a monotone increasing sequence {Nt}t0
such that n =  if n = Nt for some t and n = 0 otherwise. Such a choice of the stepsizes reﬂects
the intuition that  although both the actor and the critic are updated simultaneously  the critic should
be updated at a faster pace. From the same viewpoint  classical actor-critic algorithms [36  9  28]
establish convergence results under the assumption that
n + ↵2

n/↵n = 0.

(2

Xn0

n =Xn0

↵n = 1  Xn0

n) < 1 

lim
n!1

The condition that n/↵n = 0 ensures that the critic updates at a faster timescale  which enables
the asymptotic analysis utilizing two-timescale stochastic approximation [10  37]. However  such
an approach uses two ordinary differential equations (ODE) to approximate the updates in (3.22)
and thus only offers asymptotic convergence results. In contrast  as shown in §4  our choice of the
stepsizes yields nonasymptotic convergence results which shows that natural actor gradient converges
in linear rate to the global optimum.
In addition  we note that in Algorithm 2 we assume that the initial state x0 is sampled from the
stationary distribution ⌫K. Such an assumption is made only to simplify theoretical analysis. In
practice  we could start the algorithm after sampling a sufﬁcient number of transitions so that the
Markov chain induced by ⇡K approximately mixes. Moreover  as shown in [69]  when ⇡K is a stable
policy such that ⇢(A  BK) < 1  the Markov chain induced by ⇡K is geometrically -mixing and
thus mixes rapidly.
Finally  we remark that the minimax formulation of the policy evaluation problem is ﬁrst proposed
in [40]  which studies the sample complexity of the GTD algorithm for discounted MDPs with i.i.d.
data. Using the same formulation  [72] establishes ﬁnite sample bounds with data generated from a
Markov process. Our optimization problem in (3.18) can be viewed as the extension of their minimax
formulation to the ergodic setting. Besides  our GTD algorithm can be applied to ergodic MDPs in
general with dependent data  which might be of independent interest.

7

4 Theoretical Results

In this section  we establish the global convergence of the natural actor-critic algorithm. To this end 
we ﬁrst focus on the problem of policy evaluation by assessing the ﬁnite sample performance of the
on-policy GTD algorithm.

policy update. Thus  in the policy evaluation problem for a linear policy ⇡K  we only need to study
fro  which characterizes the closeness between the direction of policy

Note that onlyb⇥ returned by the GTD algorithm is utilized in the natural actor-critic algorithm for
the estimation error kb⇥  ⇥Kk2
update in Algorithm 1 and the true natural policy gradient.
Furthermore  recall that we restrict the primal and dual variables respectively to compact sets X⇥ and
X⌦ for algorithmic stability. We make the following assumption on X⇥ and X⌦.
Assumption 4.1. Let ⇡K0 be the initial policy in Algorithm 1. We assume that ⇡K0 is a stable
policy such that ⇢(A  BK0) < 1. Consider the policy evaluation problem for ⇡K. We assume that
J(K)  J(K0). Moreover  let X⇥ and X⌦ in (3.18) be deﬁned as
X⇥ =# : 0  #1  J(K0) k#2k2  eR⇥  
X⌦ =! : |!1| J(K0) k!2k2  (1 + kKk2
eR⇥ = kQkfro + kRkfro + pd/min( ) · (kAk2
eR⌦ = C · eR⇥ · 2

Here  eR⇥ and eR⌦ are two parameters that do not depend on K. Speciﬁcally  we have

fro)2 · eR⌦ .

min(Q) · [J(K0)]2 

where C > 0 is a constant.

fro) · J(K0) 

fro + kBk2

(4.1)
(4.2)

(4.3)
(4.4)

saddle-point of the minimax optimization in (3.18). In other words  the solution to (3.18) is the same
as the unconstrained problem min# max! F (#  !). When replacing the population problem by a
sample-based optimization problem  restrictions on the primal and dual variables ensures that the

The assumption that we have access to a stable policy K0 for initialization is commonly made in
literatures on model-free methods for LQR [22  26  44]. Besides  ⇢(A  BK0) < 1 implies that
J(K0) is ﬁnite. Here we assume J(K)  J(K0) for simplicity. Even if J(K) > J(K0)  we can
replace J(K0) in (4.1) – (4.4) by J(K) and the theory of policy evaluation still holds. Moreover  as
we will show in Theorem 4.3  the actor-critic algorithm creates a sequence policies whose objective
values decreases monotonically. Thus  here we assume J(K)  J(K0) without loss of generality.
Furthermore  as shown in the proof  the construction of eR⇥ and eR⌦ ensures that (#⇤K  0) is the
iterates of the GTD algorithm remains bounded. Thus  setting eR⇥ and eR⌦ essentially guarantees that
restricting (#  !) to X⇥ ⇥X ⌦ incurs no “bias” in the optimization problem.
We present the theoretical result for the online GTD algorithm as follows.
Theorem 4.2 (Policy evaluation). Letb#1 andb⇥ be the output of Algorithm 2 based on T iterations.
We set the stepsize to be ↵t = ↵/pt with ↵> 0 being a constant. Under Assumption 4.1  for any
⇢ 2 (⇢(A  BK)  1)  when the number of iterations T is sufﬁciently large  with probability at least
1  T 4  we have

⇤K

min(Q)⇤

⌥⇥eR⇥ eR⌦  J(K0) kKkfro  1
min(Q)⇤ is a polynomial of eR⌦  eR⌦  J(K0)  kKkfro  and

2 · (1  ⇢)

log6 T
pT

(4.5)

·

 

kb⇥  ⇥Kk2

fro 

where ⌥⇥eR⇥ eR⌦  J(K0) kKkfro  1

1/min(Q).

Proof. See §C.1 for a detailed proof.
This theorem establishes the statistical rate of convergence for the on-policy GTD algorithm. Speciﬁ-
cally  if we regard ⌥[eR⇥ eR⌦  J(K0) kKkfro  1
min(Q)]  ⇢  and ⇤K as constant  (4.5) implies that
the estimation error is of order log6 T /pT . Thus  ignoring the logarithmic term  we conclude that
the GTD algorithm converges in the sublinear rate O(1/pT )  which is optimal for convex-concave

8

Tt  ⌥⇥kKtk  J(K0)⇤ · ⇤Kt

5 · [1  ⇢(A  BKt)]5/2 · ✏5 

stochastic optimization [49] and is also identical to the rate of convergence of the GTD algorithm
in the discounted setting with bounded data [40  72]. Note that we focus on the ergodic case and
the feature mapping (x  u) deﬁned in (3.13) is unbounded. We believe this theorem might be
of independent interest. Furthermore  1/⇤K is approximately the condition number of the linear
equation of (3.16)  which reﬂects the fundamental difﬁculty of estimating ⇥K. Speciﬁcally  when
⇤K is close to zero  the matrix on the left-hand side of (3.16) is close to a singular matrix. In this case 
estimating ⇥K can be viewed as solving an ill-conditioned regression problem and thus huge sample
size is required for consistent estimation. Finally  1/[1  ⇢(A  BK)] also reﬂects the intrinsic
hardness of estimating ⇥K. Speciﬁcally  for any ⇢ 2 (⇢(A  BK)  1)  the Markov chain induced by
⇡K is -mixing where the k-th mixing coefﬁcients is bounded by C · ⇢k for some constant C > 0
[69]. Thus  when ⇢ is close to one  this Markov chain becomes more dependent  which makes the
estimation problem more difﬁcult.
Equipped with the ﬁnite sample error of the policy evaluation algorithm  now we are ready to present
the global convergence of the actor-critic algorithm. For ease of presentation  we assume that Q  R 
A  B  are all constant matrices.
Theorem 4.3 (Global convergence of actor-critic). Let the initial policy K0 be stable. We set the
stepsize  = [kRk + 1
min( ) ·k Bk2 · J(K0)] in Algorithm 1 and perform N actor updates in
the actor-critic algorithm. Let {Kt}0tN be the sequence of policy parameters generated by the
algorithm. For any sufﬁciently small ✏> 0  we set N > C ·k ⌃K⇤k/ · log2[J(K0)  J(K⇤)]/✏ 
for some constant C > 0. Moreover  for any t 2{ 0  1  . . .   N}  in the t-th iteration  we set the
number Tt of GTD updates in Algorithm 2 to be

J(K0t+1)  J(K⇤) ⇥1  C1 ·  ·k ⌃K⇤k1⇤ ·⇥J(Kt)  J(K⇤)⇤

where ⌥[kKtk  J(K0)] is a polynomial of kKtk and J(K0). Then with probability at least 1  ✏10 
we have J(KN )  J(K⇤)  ✏.
Proof Sketch. The proof of this Theorem is based on combining the convergence of the natural policy
gradient and the ﬁnite sample analysis of the GTD algorithm established in Theorem 4.2. Speciﬁcally 
for each Kt  we deﬁne K0t+1 = Kt  ⌘ · EKt  which is the one-step natural policy gradient update
starting from Kt. Similar to [26]  for ergodic LQR  it can be shown that
(4.6)
for some constant C1 > 0. In addition  for policy ⇡Kt  when the number of GTD iteration Tt is
sufﬁciently large  Kt+1 is close to K0t+1  which further implies that |J(K0t+1)  J(Kt+1)| is small.
Thus  combining this and (4.6)  we obtain the linear convergence of the actor-critic algorithm. See
§C.2 for a detailed proof.
This theorem shows that natural actor-critic algorithm combined with GTD converges linearly to
the optimal policy of LQR. Furthermore  the number of policy updates in this theorem matches
those obtained by natural policy gradient algorithm [26  44]. To the best of our knowledge  this
result seems to be the ﬁrst nonasymptotic convergence result for actor-critic algorithms with function
approximation  whose existing theory are mostly asymptotic and based on ODE approximation.
Furthermore  from the viewpoint of bilevel optimization  Theorem 4.3 offers theoretical guarantees
for the actor-critic algorithm as a ﬁrst-order online method for the bilevel program deﬁned in
(3.21)  which serves a ﬁrst attempt of understanding bilevel optimization with possibly nonconvex
subproblems.
Furthermore  although we only consider the problem of LQR and analyze the natural actor-critic
with GTD for policy evaluation  our theoretical framework can be applied to general reinforcement
learning problems with other policy optimization methods for the actor (e.g. vanilla policy gradient
[68]  trust-region policy optimization [57] and proximal policy optimization [58]) and other policy
evaluation methods for the critic such as TD(0) [65]  least-square temporal-difference (LSTD) [13] 
and Retrace [47]. In particular  suppose the critic adopts compatible features [68] for policy evaluation
using nonconvex optimization techniques  it can be shown that vanilla policy gradient converges to a
local minimum of the expected total return with a sublinear rate [75]. Moreover  by leveraging the
geometry of the expected total return as a functional of the policy  recently  [1  39  71  59] prove that
natural policy gradient [34]  TRPO and PPO are all able to ﬁnd the globally optimal policy. Using
similar approaches  we can establish the convergence and global optimality of actor-critic methods.

9

References
[1] Agarwal  A.  Kakade  S. M.  Lee  J. D. and Mahajan  G. (2019). Optimality and approximation
with policy gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261.
[2] Alizadeh  F.  Haeberly  J.-P. A. and Overton  M. L. (1998). Primal-dual interior-point methods
for semideﬁnite programming: convergence rates  stability and numerical results. SIAM Journal
on Optimization  8 746–768.

[3] Anderson  B. D. and Moore  J. B. (2007). Optimal control: linear quadratic methods. Courier

Corporation.

[4] Bard  J. F. (2013). Practical bilevel optimization: algorithms and applications  vol. 30. Springer

Science & Business Media.

[5] Basar  T. and Olsder  G. J. (1999). Dynamic noncooperative game theory  vol. 23. SIAM.
[6] Baxter  J. and Bartlett  P. L. (2001). Inﬁnite-horizon policy-gradient estimation. Journal of

Artiﬁcial Intelligence Research  15 319–350.

[7] Bertsekas  D. P. (2012). Dynamic programming and optimal control  Vol. II  4th Edition. Athena

scientiﬁc.

[8] Bhatnagar  S.  Precup  D.  Silver  D.  Sutton  R. S.  Maei  H. R. and Szepesv´ari  C. (2009).
Convergent temporal-difference learning with arbitrary smooth function approximation. In
Advances in Neural Information Processing Systems.

[9] Bhatnagar  S.  Sutton  R. S.  Ghavamzadeh  M. and Lee  M. (2009). Natural actor–critic algo-

rithms. Automatica  45 2471–2482.

[10] Borkar  V. S. (1997). Stochastic approximation with two time scales. Systems & Control Letters 

29 291–294.

[11] Borkar  V. S. and Konda  V. R. (1997). The actor-critic algorithm as multi-time-scale stochastic

approximation. Sadhana  22 525–543.

[12] Bradtke  S. J. (1993). Reinforcement learning applied to linear quadratic regulation. In Advances

in Neural Information Processing Systems.

[13] Bradtke  S. J. and Barto  A. G. (1996). Linear least-squares algorithms for temporal difference

learning. Machine learning  22 33–57.

[14] Bradtke  S. J.  Ydstie  B. E. and Barto  A. G. (1994). Adaptive linear quadratic control using

policy iteration. In American Control Conference  vol. 3. IEEE.

[15] Cai  Q.  Hong  M.  Chen  Y. and Wang  Z. (2019). On the global convergence of imitation

learning: A case for linear quadratic regulator. arXiv preprint arXiv:1901.03674.

[16] Chen  X.  Wang  J. and Ge  H. (2018). Training generative adversarial networks via primal-dual

subgradient methods: A lagrangian perspective on gan. arXiv preprint arXiv:1802.01765.

[17] Dai  B.  He  N.  Pan  Y.  Boots  B. and Song  L. (2017). Learning from conditional distributions

via dual embeddings. In International Conference on Artiﬁcial Intelligence and Statistics.

[18] Dai  B.  Shaw  A.  He  N.  Li  L. and Song  L. (2018). Boosting the actor with dual critic.

International Conference on Learning Representations.

[19] Dai  B.  Shaw  A.  Li  L.  Xiao  L.  He  N.  Liu  Z.  Chen  J. and Song  L. (2018). Sbeed: Con-
vergent reinforcement learning with nonlinear function approximation. In International Confer-
ence on Machine Learning.

[20] Dann  C.  Neumann  G. and Peters  J. (2014). Policy evaluation with temporal differences: A

survey and comparison. The Journal of Machine Learning Research  15 809–883.

[21] Dean  S.  Mania  H.  Matni  N.  Recht  B. and Tu  S. (2017). On the sample complexity of the

linear quadratic regulator. arXiv preprint arXiv:1710.01688.

10

[22] Dean  S.  Mania  H.  Matni  N.  Recht  B. and Tu  S. (2018). Regret bounds for robust adaptive

control of the linear quadratic regulator. arXiv preprint arXiv:1805.09388.

[23] Dean  S.  Tu  S.  Matni  N. and Recht  B. (2018). Safely learning to control the constrained

linear quadratic regulator. arXiv preprint arXiv:1809.10121.

[24] Dempe  S. (2002). Foundations of bilevel programming. Springer Science & Business Media.
[25] Du  S. S. and Hu  W. (2018). Linear convergence of the primal-dual gradient method for convex-

concave saddle point problems without strong convexity. arXiv preprint arXiv:1802.01504.

[26] Fazel  M.  Ge  R.  Kakade  S. and Mesbahi  M. (2018). Global convergence of policy gradient
methods for the linear quadratic regulator. In International Conference on Machine Learning.
Pouget-Abadie  J.  Mirza  M.  Xu  B.  Warde-Farley  D.  Ozair  S. 
In Advances in Neu-

Courville  A. and Bengio  Y. (2014). Generative adversarial nets.
ral Information Processing Systems.

[27] Goodfellow  I. 

[28] Grondman  I.  Busoniu  L.  Lopes  G. A. and Babuska  R. (2012). A survey of actor-critic
reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems 
Man  and Cybernetics  Part C (Applications and Reviews)  42 1291–1307.

[29] Hansen  P.  Jaumard  B. and Savard  G. (1992). New branch-and-bound rules for linear bilevel

programming. SIAM Journal on scientiﬁc and Statistical Computing  13 1194–1217.

[30] Hardt  M.  Ma  T. and Recht  B. (2018). Gradient descent learns linear dynamical systems. The

Journal of Machine Learning Research  19 1025–1068.

[31] Ho  J. and Ermon  S. (2016). Generative adversarial imitation learning. In Advances in Neural

Information Processing Systems.

[32] Horn  R. A.  Horn  R. A. and Johnson  C. R. (2013). Matrix analysis. Cambridge university

press.

[33] Islam  R.  Henderson  P.  Gomrokchi  M. and Precup  D. (2017).

benchmarked deep reinforcement learning tasks for continuous control.
arXiv:1708.04133.

Reproducibility of
arXiv preprint

[34] Kakade  S. M. (2002). A natural policy gradient. In Advances in neural information processing

systems.

[35] Kirk  D. E. (1970). Optimal control theory: an introduction. Springer.
[36] Konda  V. R. and Tsitsiklis  J. N. (2000). Actor-critic algorithms.

Information Processing Systems.

In Advances in Neural

[37] Kushner  H. and Yin  G. G. (2003). Stochastic approximation and recursive algorithms and

applications  vol. 35. Springer.

[38] Lin  Q.  Liu  M.  Raﬁque  H. and Yang  T. (2018).

Solving weakly-convex-weakly-
concave saddle-point problems as successive strongly monotone variational inequalities.
arXiv:1810.10207.

[39] Liu  B.  Cai  Q.  Yang  Z. and Wang  Z. (2019). Neural proximal/trust region policy optimiza-

tion attains globally optimal policy. arXiv preprint arXiv:1906.10306.

[40] Liu  B.  Liu  J.  Ghavamzadeh  M.  Mahadevan  S. and Petrik  M. (2015). Finite-sample analy-
sis of proximal gradient TD algorithms. In Conference on Uncertainty in Artiﬁcial Intelligence.
[41] Lu  S.  Singh  R.  Chen  X.  Chen  Y. and Hong  M. (2019). Understand the dynamics of GANs

via primal-dual optimization.
https://openreview.net/forum?id=rylIy3R9K7

[42] Luo  Z.-Q.  Pang  J.-S. and Ralph  D. (1996). Mathematical programs with equilibrium con-

straints. Cambridge University Press.

11

[43] Magnus  J. R. (1978). The moments of products of quadratic forms in normal variables. Statis-

tica Neerlandica  32 201–210.

[44] Malik  D.  Pananjady  A.  Bhatia  K.  Khamaru  K.  Bartlett  P. L. and Wainwright  M. J.
(2018). Derivative-free methods for policy optimization: Guarantees for linear quadratic
systems. arXiv preprint arXiv:1812.08305.

[45] Meyn  S. P. (1997). The policy iteration algorithm for average reward markov decision processes

with general state space. IEEE Transactions on Automatic Control  42 1663–1680.

[46] Mnih  V.  Badia  A. P.  Mirza  M.  Graves  A.  Lillicrap  T.  Harley  T.  Silver  D. and
In In-

Kavukcuoglu  K. (2016). Asynchronous methods for deep reinforcement learning.
ternational Conference on Machine Learning.

[47] Munos  R.  Stepleton  T.  Harutyunyan  A. and Bellemare  M. (2016). Safe and efﬁcient off-

policy reinforcement learning. In Advances in Neural Information Processing Systems.

[48] Nagar  A. L. (1959). The bias and moment matrix of the general k-class estimators of the
parameters in simultaneous equations. Econometrica: Journal of the Econometric Society
575–595.

[49] Nemirovski  A.  Juditsky  A.  Lan  G. and Shapiro  A. (2009). Robust stochastic approximation

approach to stochastic programming. SIAM Journal on optimization  19 1574–1609.

[50] Pfau  D. and Vinyals  O. (2016). Connecting generative adversarial networks and actor-critic

methods. arXiv preprint arXiv:1610.01945.

[51] Polyak  B. T. (1963). Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi

Matematiki i Matematicheskoi Fiziki  3 643–653.

[52] Powell  W. B. and Ma  J. (2011). A review of stochastic algorithms with continuous value func-
tion approximation and some new approximate policy iteration algorithms for multidimensional
continuous applications. Journal of Control Theory and Applications  9 336–352.

[53] Raﬁque  H.  Liu  M.  Lin  Q. and Yang  T. (2018). Non-convex min-max optimization: Provable

algorithms and applications in machine learning. arXiv:1810.02060.

[54] Recht  B. (2018). A tour of reinforcement learning: The view from continuous control. arXiv

preprint arXiv:1806.09460.

[55] Rudelson  M.  Vershynin  R. et al. (2013). Hanson-wright inequality and sub-gaussian concen-

tration. Electronic Communications in Probability  18.

[56] Sanjabi  M.  Razaviyayn  M. and Lee  J. D. (2018). Solving non-convex non-concave min-max

games under polyak-lojasiewicz condition. arXiv preprint arXiv:1812.02878.

[57] Schulman  J.  Levine  S.  Abbeel  P.  Jordan  M. and Moritz  P. (2015). Trust region policy

optimization. In International conference on machine learning.

[58] Schulman  J.  Wolski  F.  Dhariwal  P.  Radford  A. and Klimov  O. (2017). Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347.

[59] Shani  L.  Efroni  Y. and Mannor  S. (2019). Adaptive trust region policy optimization: Global

convergence and faster rates for regularized mdps. arXiv preprint arXiv:1909.02769.

[60] Silver  D.  Lever  G.  Heess  N.  Degris  T.  Wierstra  D. and Riedmiller  M. (2014). Determin-

istic policy gradient algorithms. In International Conference on Machine Learning.

[61] Simchowitz  M.  Mania  H.  Tu  S.  Jordan  M. I. and Recht  B. (2018). Learning without mix-
ing: Towards a sharp analysis of linear system identiﬁcation. arXiv preprint arXiv:1802.08334.

[62] Sinha  A.  Malo  P. and Deb  K. (2018). A review on bilevel optimization: from classical to
evolutionary approaches and applications. IEEE Transactions on Evolutionary Computation 
22 276–295.

12

[63] Sinha  A.  Namkoong  H. and Duchi  J. (2017). Certiﬁable distributional robustness with prin-

cipled adversarial training. arXiv preprint arXiv:1710.10571.

[64] Stein  C. M. (1981). Estimation of the mean of a multivariate normal distribution. Annals of

Statistics 1135–1151.

[65] Sutton  R. S. (1988). Learning to predict by the methods of temporal differences. Machine

learning  3 9–44.

[66] Sutton  R. S.  Maei  H. R.  Precup  D.  Bhatnagar  S.  Silver  D.  Szepesv´ari  C. and
Wiewiora  E. (2009). Fast gradient-descent methods for temporal-difference learning with
linear function approximation. In International Conference on Machine Learning. ACM.

[67] Sutton  R. S.  Maei  H. R. and Szepesv´ari  C. (2009). A convergent o(n) temporal-difference
algorithm for off-policy learning with linear function approximation. In Advances in Neural
Information Processing Systems.

[68] Sutton  R. S.  McAllester  D. A.  Singh  S. P. and Mansour  Y. (2000). Policy gradient methods
for reinforcement learning with function approximation. In Advances in neural information
processing systems.

[69] Tu  S. and Recht  B. (2017). Least-squares temporal difference learning for the linear quadratic

regulator. arXiv preprint arXiv:1712.08642.

[70] Tu  S. and Recht  B. (2018). The gap between model-based and model-free methods on the

linear quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565.

[71] Wang  L.  Cai  Q.  Yang  Z. and Wang  Z. (2019). Neural policy gradient methods: Global

optimality and rates of convergence. arXiv preprint arXiv:1909.01150.

[72] Wang  Y.  Chen  W.  Liu  Y.  Ma  Z.-M. and Liu  T.-Y. (2017). Finite sample analysis of the gtd
policy evaluation algorithms in markov setting. In Advances in Neural Information Processing
Systems.

[73] Williams  R. J. (1992). Simple statistical gradient-following algorithms for connectionist rein-

forcement learning. Machine Learning  8 229–256.

[74] Yang  Z.  Fu  Z.  Zhang  K. and Wang  Z. (2018). Convergent reinforcement learning with

function approximation: A bilevel optimization perspective. Manuscript.

[75] Zhang  K.  Koppel  A.  Zhu  H. and Bas¸ar  T. (2019). Global convergence of policy gradient

methods to (almost) locally optimal policies. arXiv preprint arXiv:1906.08383.

[76] Zhou  K.  Doyle  J. C. and Glover  K. (1996). Robust and optimal control  vol. 40. Prentice

Hall.

13

,Zhuoran Yang
Yongxin Chen
Mingyi Hong
Zhaoran Wang