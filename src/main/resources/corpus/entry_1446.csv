2013,Designed Measurements for Vector Count Data,We consider design of linear projection measurements for a vector Poisson signal model. The projections are performed on the vector Poisson rate  $X\in\mathbb{R}_+^n$  and the observed data are a vector of counts  $Y\in\mathbb{Z}_+^m$. The projection matrix is designed by maximizing mutual information between $Y$ and $X$  $I(Y;X)$. When there is a latent class label $C\in\{1 \dots L\}$ associated with $X$  we consider the mutual information with respect to $Y$ and $C$  $I(Y;C)$. New analytic expressions for the gradient of $I(Y;X)$ and $I(Y;C)$ are presented  with gradient performed with respect to the measurement matrix. Connections are made to the more widely studied Gaussian measurement model. Example results are presented for compressive topic modeling of a document corpora (word counting)  and hyperspectral compressive sensing for chemical classification (photon counting).,Designed Measurements for Vector Count Data

1Liming Wang  1David Carlson  2Miguel Dias Rodrigues  3David Wilcox 

1Robert Calderbank and 1Lawrence Carin

1Department of Electrical and Computer Engineering  Duke University

2Department of Electronic and Electrical Engineering  University College London

{liming.w  david.carlson  robert.calderbank  lcarin}@duke.edu

3Department of Chemistry  Purdue University

m.rodrigues@ucl.ac.uk

wilcoxds@purdue.edu

Abstract

We consider design of linear projection measurements for a vector Poisson signal
model. The projections are performed on the vector Poisson rate  X ∈ Rn
+  and the
observed data are a vector of counts  Y ∈ Zm
+ . The projection matrix is designed
by maximizing mutual information between Y and X  I(Y ; X). When there is
a latent class label C ∈ {1  . . .   L} associated with X  we consider the mutual
information with respect to Y and C  I(Y ; C). New analytic expressions for the
gradient of I(Y ; X) and I(Y ; C) are presented  with gradient performed with re-
spect to the measurement matrix. Connections are made to the more widely stud-
ied Gaussian measurement model. Example results are presented for compressive
topic modeling of a document corpora (word counting)  and hyperspectral com-
pressive sensing for chemical classiﬁcation (photon counting).

Introduction

1
There is increasing interest in exploring connections between information and estimation theory. For
example  mutual information and conditional mean estimation have been discovered to possess close
interrelationships. The derivative of mutual information in a scalar Gaussian channel [11] has been
expressed in terms of the minimum mean-squared error (MMSE). The connections have also been
extended from the scalar Gaussian to the scalar Poisson channel model [12]. The gradient of mutual
information in a vector Gaussian channel [17] has been expressed in terms of the MMSE matrix. It
has also been found that the relative entropy can be represented in terms of the mismatched MMSE
estimates [23  24]. Recently  parallel results for scalar binomial and negative binomial channels have
been established [22  10].
Inspired by the Lipster-Shiryaev formula [16]  it has been demonstrated that for certain channels
(or measurement models)  investigation of the gradient of mutual information can often lead to a
relatively simple formulation  relative to computing mutual information itself. Further  it has been
shown that the derivative of mutual information with respect to key system parameters also relates to
the conditional mean estimates in other channel settings beyond Gaussian and Poisson models [18].
This paper pursues this overarching theme for a vector Poisson measurement model. Results for
scalar Poisson signal models have been developed recently [12  1] for signal recovery; the vector
results presented here are new  with known scalar results recovered as a special case. Further  we
consider the gradient of mutual information for Poisson data in the context of classiﬁcation  for
which there are no previous results  even in the scalar case.
The results we present for optimizing mutual information in vector Poisson measurement models are
general  and may be applied to optical communication systems [15  13]. The speciﬁc applications
that motivate this study are compressive measurements for vector Poisson data. Direct observation
of long vectors of counts may be computationally or experimentally expensive  and therefore it is
of interest to design compressive Poisson measurements. Almost all existing results for compres-

1

sive sensing (CS) directly or implicitly assume a Gaussian measurement model [6]  and extension to
Poisson measurements represents an important contribution of this paper. To the authors knowledge 
the only previous examination of CS with Poisson data was considered in [20]  and that paper con-
sidered a single special (random) measurement matrix  it did not consider design of measurement
matrices  and the classiﬁcation problems was not addressed. It has been demonstrated in the context
of Gaussian measurements that designed measurement matrices  using information-theoretic met-
rics  may yield substantially improved performance relative to randomly constituted measurement
matrices [7  8  21]. In this paper we extend these ideas to vector Poisson measurement systems  for
both signal recovery and classiﬁcation  and make connections to the Gaussian measurement model.
The theory is demonstrated by considering compressive topic modeling of a document corpora  and
chemical classiﬁcation with a compressive photon-counting hyperspectral camera [25].
2 Mutual Information for Designed Compressive Measurements
2.1 Motivation
A source random variable X ∈ Rn  with probability density function PX (X)  is sent through a
measurement channel  the output of which is characterized by random variable Y ∈ Rm  with
conditional probability density function PY |X (Y |X); we are interested in the case m < n  relevant
for compressive measurements  although the theory is general. Concerning PY |X (Y |X)  in this
paper we focus on Poisson measurement models  but we also make connections to the much more
widely considered Gaussian case. For the Poisson and Gaussian measurement models the mean of
PY |X (Y |X) is ΦX  where Φ ∈ Rm×n is the measurement matrix. For the Poisson case the mean
may be modiﬁed as ΦX + λ for “dark current” λ ∈ Rm
+   and positivity constraints are imposed on
the elements of Φ and X.
c=1 πcPX|C(X|C =
c=1 πc = 1  and C may correspond to a latent class label. In this context 
for each draw X there is a latent class random variable C ∈ {1  . . .   L}  where the probability of
class c is πc.
Our goal is to design Φ such that the observed Y is most informative about the underlying X or C.
When the interest is in recovering X  we design Φ with the goal of maximizing mutual information
I(X; Y )  while when interested in inferring C we design Φ with the goal of maximizing I(C; Y ).
To motivate use of the mutual information as the design metric  we note several results from the
literature. For the case in which we are interested in recovering X from Y   it has been shown [19]
that

Often the source statistics are characterized as a mixture model: PX (X) =(cid:80)L
c)  where πc > 0 and(cid:80)L

MMSE ≥ 1
2πe

(1)
where h(X) is the differential entropy of X and MMSE = E{trace[(X − E(X|Y ))(X −
E(X|Y ))T ]} is the minimum mean-square error.

For the classiﬁcation problem  we deﬁne the Bayesian classiﬁcation error as Pe = (cid:82) PY (y)[1 −

exp{2[h(X) − I(X; Y )]}

maxcPC|Y (c|y)]dy. It has been shown in [14] that

[H(C|Y ) − H(Pe)]/ log L ≤ Pe ≤ 1
2

(2)
where H(C|Y ) = H(C) − I(C; Y )  0 ≤ H(Pe) ≤ 1  and H(·) denotes the entropy of a discrete
random variable. By minimizing H(C|Y ) we minimize the upper bound to Pe  and since H(C) is
independent of Φ  to minimize the upper bound to Pe our goal is to design Φ such that I(C; Y ) is
maximized.

H(C|Y )

2.2 Existing results for Gaussian measurements
There are recent results for the gradient of mutual information for vector Gaussian measurements 
which we summarize here. Consider the case C ∼ PC(C)  X|C ∼ PX|C(X|C)  and Y |X ∼
N (Y ; ΦX  Λ−1)  where Λ ∈ Rm×m is a known precision matrix. Note that PC and PX|C are
arbitrary  while PY |X = N (Y ; ΦX  Λ−1) corresponds to a Gaussian measurement with mean ΦX.
It has been established that the gradient of mutual information between the input and the output of
the vector Gaussian channel model obeys [17]

(3)

∇ΦI(X; Y ) = ΛΦE 

2

where E = E(cid:2)(X − E(X|Y ))(X − E(X|Y ))T(cid:3) denotes the MMSE matrix. The gradient of mu-
where ˜E = E(cid:2)(E(X|Y  C) − E(X|Y ))(E(X|Y  C) − E(X|Y ))T(cid:3) denotes the equivalent MMSE

tual information between the class label and the output for the vector Gaussian channel is [8]

∇ΦI(C; Y ) = ΛΦ ˜E 

matrix.
2.3 Conditional-mean estimation
Note from the above discussion that for a Gaussian measurement  ∇ΦI(X; Y ) = E[f (X  E(X|Y ))]
and ∇ΦI(C; Y ) = E[g(E(X|Y  C)  E(X|Y ))]  where f (·) and g(·) are matrix-valued functions of
the respective arguments. These results highlight the connection between the gradient of mutual
information with respect to the measurement matrix Φ and conditional-mean estimation  constituted
by E(X|Y ) and E(X|Y  C). We will see below that these relationships hold as well for the vector
Poisson case  with distinct functions ˜f (·) and ˜g(·).
3 Vector Poisson Data
3.1 Model

(4)

The vector Poisson channel model is deﬁned as

Pois(Y ; ΦX + λ) = PY |X (Y |X) =

m(cid:89)

i=1

m(cid:89)

PYi|X (Yi|X) =

Pois (Yi; (ΦX)i + λi)

(5)

i=1

+

+ represents the dark current.

+ represents the channel output  Φ ∈ Rm×n

+ represents the channel input  the random
represents a mea-

where the random vector X = (X1  X2  . . .   Xn) ∈ Rn
vector Y = (Y1  Y2  . . .   Ym) ∈ Zm
surement matrix  and the vector λ = (λ1  λ2  . . .   λm) ∈ Rm
The vector Poisson channel model associated with arbitrary m and n is a generalization of the scalar
Poisson model  for which m = n = 1 [12  1]. In the scalar case PY |X (Y |X) = Pois(Y ; φX + λ) 
where here scalar random variables X ∈ R+ and Y ∈ Z+ are associated with the input and output
of the scalar channel  respectively  φ ∈ R+ is a scaling factor  and λ ∈ R+ is associated with the
dark current.
The goal is to design Φ to maximize the mutual information between X and Y . Toward that end  we
consider the gradient of mutual information with respect to Φ: ∇ΦI(X; Y ) = [∇ΦI(X; Y )ij] 
where ∇ΦI(X; Y )ij represents the (i  j)-th entry of the matrix ∇ΦI(X; Y ). We also con-
sider the gradient with respect to the vector dark current  ∇λI(X; Y ) = [∇λI(X; Y )i]  where
∇λI(X; Y )i represents the i-th entry of the vector ∇λI(X; Y ). For a mixture-model source
c=1 πcPX|C=c(X|C = c)  for which there is more interest in recovering C than
in recovering X  we seek ∇ΦI(C; Y ) and ∇λI(C; Y ).

PX (X) = (cid:80)L

Y with respect to QY provided that P θ

Y |X be the Radon-Nikodym derivative of probability measure P θ

3.2 Gradient of Mutual Information for Signal Recovery
In order to take full generality of the input distribution into consideration  we utilize the Radon-
Nikodym derivatives to represent the probability measures of interests. Consider random variables
X ∈ Rn and Y ∈ Rm. Let f θ
Y |X
with respect to an arbitrary measure QY   provided that P θ
Y |X is absolutely continuous with respect
Y |X (cid:28) QY . θ ∈ R is a parameter. f θ
Y is the Radon-Nikodym derivative of the
to QY   i.e.  P θ
Y (cid:28) QY . Note that in the continuous or
probability measure P θ
Y are simply probability density or mass functions with QY chosen to be
discrete case  f θ
the Lebesgue measure or the counting measure  respectively. We note that similar notation is also
used for the signal classiﬁcation case  except that we may also need to condition both on X and C.
Some results of the paper require the assumption on the regularity conditions (RC)  which are listed
in the Supplementary Material. We will assume all four regularity conditions RC1–RC4 whenever
necessary in the proof and the statement of the results. Recall [9] that for a function f (x  θ) :
Rn × R → R with a Lebesgue measure µ on Rn  we have ∂
∂θ f (x  θ)dµ(x) 
if f (x  θ) ≤ g(x)  where g ∈ L1(µ). Hence  in light of this criterion  it is straightforward to
verify that the RC are valid for many common distributions of X. Proofs of the below theorems are
provided in the Supplementary Material.

(cid:82) f (x  θ)dµ(x) =(cid:82) ∂

Y |X and f θ

∂θ

3

Theorem 1. Consider the vector Poisson channel model in (5). The gradient of mutual information
between the input and output of the channel  with respect to the matrix Φ  is given by:

[∇ΦI(X; Y )ij] =(cid:2)E [Xj log((ΦX)i + λi)] − E [E[Xj|Y ] log E[(ΦX)i + λi|Y ]](cid:3) 

and with respect to the dark current is given by:

[∇λI(X; Y )i] =(cid:2)E[log((ΦX)i + λi)] − E[log E[(ΦX)i + λi|Y ]](cid:3).

(6)

(7)

irrespective of the input distribution PX (X)  provided that the regularity conditions hold.

3.3 Gradient of Mutual Information for Classiﬁcation

Theorem 2. Consider the vector Poisson channel model in (5) and mixture signal model. The
gradient with respect to Φ of mutual information between the class label and output of the channel
is

(cid:20)

(cid:20)

(cid:21)

E[(ΦX)i + λi|Y  C]
E[(ΦX)i + λi|Y ]

(cid:21)

.

 

(8)

(9)

[∇ΦI(C; Y )ij] = E

E[Xj|Y  C] log

and with respect to the dark current is given by

(∇λI(C; Y ))i =E

log

E[(ΦX)i + λi|Y  C]
E[(ΦX)i + λi|Y ]

irrespective of the input distribution PX|C(X|C)  provided that the regularity conditions hold.

3.4 Relationship to known scalar results

It is clear that Theorem 1 represents a multi-dimensional generalization of Theorems 1 and 2 in [12].
The scalar result follows immediately from the vector counterpart by taking m = n = 1.
Corollary 1. For the scalar Poisson channel model PY |X (Y |X) = Pois(Y ; φX + λ)  we have

I(X; Y ) = E [X log((φX) + λ)] − E [E[X|Y ] log E[φX + λ|Y ]]  

∂
∂φ

I(X; Y ) = E[log(φX + λ)] − E[log E[φX + λ|Y ]].

∂
∂λ

irrespective of the input distribution PX (X)  provided that the regularity conditions hold.

(10)

(11)

While the scalar result in [12] for signal recovery is obtained as a special case of our Theorem 1  for
recovery of the class label C there are no previous results for our Theorem 2  even in the scalar case.

3.5 Conditional mean and generalized Bregman divergence
Considering the results in Theorem 1  and recognizing that E[(ΦX) + λ|Y ] = ΦE(X|Y ) + λ  it
is clear that for the Poisson case ∇ΦI(X; Y ) = E[ ˜f (X  E(X|Y ))]. Similarly  for the classiﬁcation
case  ∇ΦI(C; Y ) = E[˜g(E(X|Y  C)  E(X|Y ))]. The gradient with respect to the dark current λ
has no analog for the Gaussian case  but similarly we have ∇λI(X; Y ) = E[ ˜f1(X  E(X|Y ))] and
∇λI(C; Y ) = E[˜g1(E(X|Y  C)  E(X|Y ))].
For the scalar Poisson channel in Corollary 1  it has been shown in [1] that ∂
∂φ I(X; Y ) =
E[(cid:96)(X  E(X|Y ))]  where (cid:96)(X  E(X|Y )) is deﬁned by the right side of (10)  and is related to the
Bregman divergence [5  2].

scope of

this paper 

one may show that

While beyond the
and
˜g(E(X|Y  C)  E(X|Y )) may be interpreted as generalized Bregman divergences  where here
the generalization is manifested by the fact that these are matrix-valued measures  rather than the
scalar one in [1]. Further  for the vector Gaussian cases one may also show that f (X  E(X|Y ))
and g(E(X|Y  C)  E(X|Y )) are also generalized Bregman divergences. These facts are primarily
of theoretical interest  as they do not affect the way we perform computations. Nevertheless 
these theoretical results  through generalized Bregman divergence  underscore the primacy the
conditional mean estimators E(X|Y ) and E(X|Y  C) within the gradient of mutual information
with respect to Φ  for both the Gaussian and Poisson vector measurement models.

˜f (X  E(X|Y ))

4

i=1

(a)

+  Ψ ∈ Rn×T

+

and Sd ∈ RT

(b)

bution to Ydk = (cid:80)n

Figure 1: Results on the 20 Newsgroups dataset. Random denotes
a random binary matrix with 1% non-zero values. Rand-Ortho de-
notes a random binary matrix restricted to an orthogonal matrix
with one non-zero entry per column. Optimized denotes the meth-
ods discussed in Section 4.3. Full denotes when each word is ob-
served. The error estimates were obtained by running the algorithm
over 10 different random splits of the corpus. (a) Per-word predic-
tive log-likelihood estimate versus the number of projections. (b)
KL Divergence versus the number of projections.

4 Applications
4.1 Topic Models
Consider the case for which the Poisson rate vector for document d may be represented Xd = ΨSd 
where Xd ∈ Rn
+. Here T represents the number of topics  and in
the context of documents  n represents the total number of words in dictionary D. The count for
the number of times each of the n words is manifested in document d may often be modeled as
Yd|Sd ∼ Pois(Yd; ΨSd); see [26] and the extensive set of references therein.
Rather than counting the number of
times each of the n words are sepa-
rately manifested  we may more ef-
ﬁciently count the number of times
words in particular subsets of D are
manifested. Speciﬁcally  consider a
compressive measurement for docu-
ment d  as Yd|Xd ∼ Pois(Yd; ΦXd) 
where Φ ∈ {0  1}m×n  with m (cid:28)
Let φk ∈ {0  1}n represent
n.
the kth row of Φ  with Ydk the kth
component of Yd. Then Ydk|Xd ∼
k Xd) is equal in distri-
Pois(Ydk; φT
˜Ydki  where
˜Ydki|Xdi ∼ Pois(φkiXdi)  with
φki ∈ {0  1} the ith component of
φk and Xdi the ith component of Xd.
Therefore  Ydk represents the number
of times words in the set deﬁned by
the non-zero elements of φk are man-
ifested in document d; Yd therefore
represents the number of times words are manifested in a document in m distinct sets.
Our goal is to use the theory developed above to design the binary Φ such that the compressive
Yd|Xd ∼ Pois(Yd; ΦXd) is as informative as possible. In our experiments we assume that Ψ may
be learned separately based upon a small subset of the corpus  and then with Ψ so ﬁxed the statistics
of Xd are driven by the statistics of Sd. When performing learning of Ψ  each column of Ψ is
assumed drawn from an n-dimensional Dirichlet distribution  and Sd is assumed drawn from a
gamma process  as speciﬁed in [26]. We employ variational Bayesian (VB) inference on this model
[26] to estimate Ψ (and retain the mean).
With Ψ so ﬁxed  we then design Φ under two cases. For the case in which we are interested in
inferring Sd from the compressive measurements  i.e.  based on counts of words in sets  we employ
a gamma process prior for pS(Sd)  as in [26]. The result in Theorem 1 is then used to perform
gradients for design of Φ. For the classiﬁcation case  for each document class c ∈ {1  . . .   L} we
learn a p(Sd|C) based on a training sub-corpus for class C. This is done for all document classes 
and we design a compressive matrix Φ ∈ {0  1}m×n  with gradient performed using Theorem 2.
In the testing phase  using held-out documents  we employ the matrix Φ to group the counts of
words in document d into counts on m sets of words  with sets deﬁned by the rows of Φ. Using
these Yd  which we assume are drawn Yd|Sd ∼ Pois(Yd; ΦΨSd)  for known Φ and Ψ  we then use
VB computations for the model in [26] to infer a posterior distribution on Sd or class C  depending
on the application. The VB inference for this model was not considered in [26]  and the update
equations are presented in the Supplementary Material.
4.2 Model for Chemical Sensing
The model employed for the chemical sensing [25] considered below is very similar in form to that
used for topic modeling  so we reuse notation. Assume that there are T fundamental (building-block)
chemicals of interest  and that the hyperspectral sensor performs measurements at n wavelengths.
Then the observed data for sample d may be represented Yd|Sd ∼ Pois(Yd; ΨSd + λ)  where Yd ∈
+ represents the count of photons at the n sensor wavelengths  λ ∈ Rn
Zn
+ represents the sensor
dark current  and the tth column of Ψ ∈ Rn×T
reﬂects the mean Poisson rate for chemical t (the

+

5

20406080100120140−9−8.5−8−7.520 Newsgroups: PLL of Hold−out SetNumber of ProjectionsPer−word Predictive Log−Likelihood  RandomOrthoNNMFLDAOptimizedFull0501001500.811.21.41.61.822.22.420Newsgroups: KL−Divergence on Topic Mixture EstimatesNumber of ProjectionsPer−Document K−L Divergence  RandomRand−OrthoNNMFLDAOptimized(a)

(b)

(c)

+ reﬂects the amount of

Figure 2: Results on the NYTimes corpus. Optimized denotes the methods discussed in Section 4.3. Full
denotes when each word is observed. The error estimates were obtained by running the algorithm over 10
different random subsets of 20 000 documents. (a) Predictive log-likelihood estimate versus the number of pro-
jections. (b) KL Divergence versus the number of projections. (c) Predictive log-likelihood versus processing
time.
different chemicals play a role analogous to topics). The vector Sd ∈ RT
each fundamental chemical present in the sample under test.
For the compressive chemical-sensing system discussed in Section 4.5  the measurement matrix is
again binary  Φ ∈ {0  1}m×n. Through calibrations and known properties of chemicals and charac-
teristics of the camera  one may readily constitute Ψ and λ  and a model similar to that employed for
topic modeling is utilized to model Sd; here λ is a characteristic of the camera  and is not optimized.
In the experiments reported below the analysis of the chemical-sensing data is performed analo-
gously to how the documents were modeled (which we detail)  and therefore no further modeling
details are provided explicitly for the chemical-sensing application  for brevity. For the chemical
sensing application  the goal is to classify the chemical sample under test  and therefore Φ is deﬁned
based on optimization using the Theorem 2 gradient.
4.3 Details on Designing Φ
We wish to use Theorems 1 and 2 to design a binary Φ  for the document-analysis and chemical-
sensing applications. To do this  instead of directly optimizing Φ  we put a logistic link on each
value Φij = logit(Mij). We can state the gradient with respect to M as:
[∇M I(X; Y )ij] = [∇ΦI(X; Y )ij][∇M Φij]

(12)
Similar results hold for ∇M I(C; Y )ij.Φ was initialized at random  and we threshold the logistic at
0.5 to get the ﬁnal binary Φ.
To estimate the expectations needed for the results in Theorems 1 and 2  we used Monte Carlo
integration methods  where we simulated X and Y from the appropriate distribution. The number
of samples in the Monte Carlo integration was set to n (data dimension)  and 1000 gradient steps
were used for optimizing Φ.
The explicit forms for the gradients in Theorems 1 and 2 play an important role in making opti-
mization of Φ tractable for the practical applications considered here. One could in principle take a
brute-force gradient of I(Y ; X) and I(Y ; C) with respect to Φ  and evaluate all needed integrals via
Monte Carlo sampling. This leads to a cumbersome set of terms that need be computed. The “clean”
forms of the gradients in Theorems 1 and 2 signiﬁcantly simpliﬁed design implementation within
the below experiments  with the added value of allowing connections to be made to the Gaussian
measurement model.
4.4 Examples for Document Corpora

We demonstrate designed projections on the NYTimes and 20 Newsgroups data. The NYTimes data
has n = 8000 unique words  and the Newsgroup data has n = 8052 unique words. When learning
Ψ  we placed the prior Dir(0.1  . . .   0.1) on the columns of Ψ  and the components Sdk had a prior
Gamma(0.1  0.1). We tried many different settings for these priors  and as in [26]  the learned Ψ
was insensitive to “reasonable” settings. The number of topics (columns) in Ψ was set to T = 100.
In addition to designing Φ using the proposed theory  we also considered four comparative designs:
(i) binary Φ constituted uniformly at random  with 1% of the entries non-zero; (ii) orthogonal
binary rows of Φ  with one non-zero element in each column selected uniformly at random; (iii)
performing non-negative matrix factorization [3] on (NNMF) Ψ  and projecting onto the principal
vectors; and (iv) performing latent Dirichlet allocation [4] on the documents  and projecting onto
the topic-dependent probabilities of words. For (iii) and (iv)  the top (highest amplitude) 5% of

6

20406080100120140−8.4−8.2−8−7.8−7.6−7.4−7.2NYTimes: PLL of Hold−out SetNumber of ProjectionsPer−word Predictive Log−Likelihood  RandomOrthoNNMFLDAOptimizedFull0501001500.811.21.41.61.822.22.4NYTimes: KL−Divergence on Topic Mixture EstimatesNumber of ProjectionsPer−Document K−L Divergence  RandomRand−OrthoNNMFLDAOptimized00.511.52−9−8.5−8−7.5NYTimes: Predictive Log−Likelihood vs TimePer−Document Processing Time  msHoldout Per−Word PLL  RandomRand−OrthoNNMFLDAOptimized(a)

(b)

Figure 3:
(a) Classiﬁcation accuracy of projected measurements and the fully observed case. Random uses
10% non-zero values  Ortho is a random matrix limited to orthogonal projections  and Optimized uses designed
projections. The error bars are the standard deviation of the algorithm run independently on 10 random splits of
the dataset. (b) Subset of confusion matrix of of the fully observed counts. White numbers denote percentage
of documents classiﬁed in that manner. Only those classes in the “comp” subgroup are shown. The “comp”
group is the least accurate subgroup. (c) The confusion matrix on the “comp” subgroup for 150 compressive
measurements.

(c)

(cid:80)K
k=1 S(cid:48)

dk p log(S(cid:48)

d p||S(cid:48)

d as the normalized version of Sd. We calculate DKL(S(cid:48)

the words in each vector on which we project (e.g.  topic) were set to have projection amplitude 1 
and all the rest were set to zero. The settings on (i)  (iii) and (iv)  i.e.  with regard to the fraction
of words with non-zero values in Φ  were those that yielded the best results (other settings often
performed much worse).
We show results using two metrics  Kullback-Leibler (KL) divergence and predictive log-likelihood.
For the KL divergence  we compare the topic mixture learned from the projection measurements to
the topic mixture learned from the case where each word is observed (no compressive measurement).
We deﬁne the topic mixture S(cid:48)
d f ) =
dk p/S(cid:48)
dk f )  where S(cid:48)
dk p is the relative weight on document d  topic k for the
full set of words  and S(cid:48)
dk p is the same for the compressive topic model. We also calculate per-
word predictive log-likelihood. Because different projection metrics are in different dimensions 
we use 75% of a document’s words to get the projection measurements Yd and use the remaining
25% as the original word tokens Wd. We then calculate the predictive log-likelihood (PLL) as
log(Wd|Ψ  Φ  Yd).
We split the 20 Newgroups corpus into 10 random splits of 60% training and 40% testing to get an
estimate of uncertainty. The results are shown in Figure 1. Figure 1(a) shows the per-word predic-
tive log-likelihood (PLL). At very low numbers of compressive measurements we get similar PLL
between the designed matrix and the random methods. As we increase the number of measurements 
we get dramatic improvements by optimizing the sensing matrix and the optimized methods quickly
approach the fully observed case. The same trends can be seen in the KL divergence shown in Figure
1(b). Note that the relative quality of the NNMF and LDA based designs of Φ depends on the metric
(KL or PLL)  but for both metrics the proposed mutual-information-based design of Φ yields best
performance.
To test the NYTimes corpus  we split the corpus into 10 random subsets with 20 000 training docu-
ments and 20 000 testing documents. The results are shown in Figure 2. As in the 20 Newsgroups
results  the predictive log-likelihood and KL divergence of the random and designed measurements
are similar when the number of projections are low. As we increase the number of projections the
optimized projection matrix offers dramatic improvements over the random methods. We also con-
sider predictive log-likelihood versus time in Figure 2(c). The compressive measurements give near
the same performance with half the per-document processing time. Since the total processing time
increases linearly with the total number of documents  a 50% decrease in processing time can make
a signiﬁcant difference in large corpora.
We also consider the classiﬁcation problem over the 20 classes in the 20 Newsgroups dataset  split
into 10 groups of 60% training and 40% testing. We learn a Ψ with T = 20 columns (topics) and
with the prior on the columns as above. Within the prior  we draw Sdcd|cd ∼ Gamma(1  1) and
Sdc(cid:48)|cd = 0 for all c(cid:48) (cid:54)= cd. Separate topics are associated with each of the 20 classes  and we use
d = arg max(c|Yd). Classiﬁcation versus number of pro-
the MAP estimate to get the class label c∗
jections for random projections and designed projections are shown in Figure 3(a). It is also useful
to look at the type of errors made in the classiﬁer when we use the designed projections. Figure
3(b) and Figure 3(c) show the newsgroups under the “comp” (computer) heading  which is the least

7

05010015000.10.20.30.40.50.60.70.8Number of ProjectionsHold−out Classification Accuracy20 Newsgroups: Classification Accuracy  FullRandomRand−OrthoNNMFLDAOptimizedConfusion Matrix for Fully Observed Word Counts  72 5 6 3 510 66611 210 6 1 775 7 2 7 2 2 977 0 910 5 1 178 4alt.atheismcomp.graphicscomp.os.ms−windows.misccomp.sys.ibm.pc.hardwarecomp.sys.mac.hardwarecomp.windows.xOthercomp.graphicscomp.os.ms−windows.misccomp.sys.ibm.pc.hardwarecomp.sys.mac.hardwarecomp.windows.x00.20.40.60.8Confusion Matrix for Projected (N=150) Counts  67 6 7 3 511 95615 113 6 2 873 8 1 8 3 31470 1 9 9 7 2 075 7alt.atheismcomp.graphicscomp.os.ms−windows.misccomp.sys.ibm.pc.hardwarecomp.sys.mac.hardwarecomp.windows.xOthercomp.graphicscomp.os.ms−windows.misccomp.sys.ibm.pc.hardwarecomp.sys.mac.hardwarecomp.windows.x00.20.40.6accurate section. In the compressed case  many of the additional errors go into nearby topics with
overlapping ideas. For example  most additional misclassiﬁcations in “comp.os.ms-windows.misc”
go into “comp.sys.ibm.pc.hardware” and “comp.windows.x ” which have many similar discussions.
Additionally  4% of the articles were originally posted in more than one topic  showing the intimate
relationship between similar discussion groups  and so misclassifying into a related (and overlap-
ping) class is less of a problem than misclassiﬁcation into a completely disjoint class.
4.5 Poisson Compressive Sensing for Chemical Classiﬁcation
We consider chemical sensing based on the wavelength-dependent signature of chemicals  at optical
frequencies (here we consider a 850-1000 nm laser system). In Figure 4(a) the measurement system
is summarized; details of this system are described in [25]. In Part 1 of Figure 4(a) multi-wavelength
photons are scattered off a chemical sample. In Part 2 of this ﬁgure a volume holographic grating
(VHG) is employed to diffract the photons in a wavelength-dependent manner  and therefore pho-
tons are distributed spatially across a digital mirror microdevice (DMD); distinct wavelengths are
associated with each micromirror. The DMD consists of 1920 × 1080 aluminum mirrors. Each mir-
ror is in a binary state  either reﬂecting light back to a detector  or not. Each mirror approximately
samples a single wavelength  as a result of the VHG  and the photon counter counts all photons at
wavelengths for which the mirrors direct light to the sensor. Hence  the sensor counts all photons at
a subset of the wavelengths  those for which the mirror is at the appropriate angle.
The measurement may be repre-
sented Y |Sd ∼ Pois[Φ(ΨSd + λ0)] 
where λ0 ∈ Rn
+ is known from cali-
bration. The elements of the rate vec-
tor of λ0 vary from .07 to 1.5 per bin 
and the cumulative dark current Φλ0
can provide in excess of 50% of the
signal energy  depending on the mea-
surement (very noisy measurements).
Design of Φ was based on Theorem
2  and λ0 here is treated as the sig-
nature of an additional chemical (ac-
tually associated with measurement
noise); ﬁnally  λ = Φλ0 is the mea-
surement dark current.
The ten chemicals considered in this
test were acetone  acetonitrile  ben-
zene  dimethylacetamide  dioxane 
ethanol  hexane  methylcyclohexane 
octane  and toluene  and we note
from Figure 4 that after only ﬁve compressive measurements excellent chemical classiﬁcation is
manifested based on designed CS measurements. There are n > 1000 wavelengths in a conven-
tional measurement of these data  this system therefore reﬂecting signiﬁcant compression. In Figure
4(b) we show results of measured data and performance predictions based on our model  with good
agreement manifested. Note that designed projection measurements perform markedly better than
random  where here the probability of a one in the random design was 10% (this yielded best random
results in simulations).
5 Conclusions
New results are presented for the gradient of mutual information with respect to the measurement
matrix and a dark current  within the context of a Poisson model for vector count data. The mutual
information is considered for signal recovery and classiﬁcation. For the former we recover known
scalar results as a special case  and the latter results for classiﬁcation have not been addressed in any
form previously. Fundamental connections between the gradient of mutual information and condi-
tional expectation estimates have been made for the Poisson model. Encouraging applications have
been demonstrated for compressive topic modeling  and for compressive hyperspectral chemical
sensing (with demonstration on a real compressive camera).
Acknowledgments
The work reported here was supported in part by grants from ARO  DARPA  DOE  NGA and ONR.

Figure 4:
(a) Measurement system. The VHG is a volume holo-
graphic grating  that spatially spreads photons in a wavelength-
dependent manner across the digital mirror microdevice (DMD) 
and the DMD is employed to implement binary coding. (b) Per-
formance of the compressive-measurement classiﬁer as a function
of the number of compressive measurements; ten chemicals are
considered. Experimental results are shown (Exp)  as well as pre-
dictions from simulations (Sim).

(a)

(b)

8

D.S. Wilcox et al. / Analytica Chimica Acta 755 (2012) 17– 2721Fig. 1. Schematic of the DMD-based near infrared digital compressive detection instrument.As for which vector  we should use in (7)  we believe that apractical set of ﬁlters F can be designed assuming that the purecomponent emission rates are normalized to the same value i= j(8)for all i and j  i.e.  we design measurement ﬁlters F to min-imize the error in estimating a mixture where the rate ofphotons emitted by all chemical species are the same. Setting = (1  1  . . .   1)Tsufﬁces. This determines A = FTP  B  and T. Mat-lab software to determine OB ﬁlters is available on request. Seewww.math.purdue.edu/∼buzzard/software/ for more details.3. Experimental3.1. Experimental apparatusThe compressive detection spectrometer  shown in Fig. 1 employs a Raman backscattering collection geometry. Part 1 issimilar to that described in [2]. The excitation source is a 785 nmsingle mode laser (Innovative Photonic Solutions). After passingthrough a laser-line bandpass ﬁlter (Semrock  LL01-785-12.5)  thelaser is focused onto the sample with a NIR lens (Olympus  LMPlanIR  20×). The Raman scattering is collected and separated fromthe laser Rayleigh scattering with a dichroic mirror (Semrock LPD01-785RS-25) and a 785 nm notch ﬁlter (Semrock  NF03-785E-25).The Raman scattered light is then sent to Part 2  where it is ﬁrstﬁltered with a 900 nm shortpass ﬁlter (Thorlabs  FES0900) and sub-sequently directed to a volume holographic grating (1200 L mm−1 center wavelength 830 nm  Edmund Optics  48–590). The windowof the dispersed light is ∼200–1700 cm−1with a spectral resolutionof 30 cm−1(this resolution is limited by the beam quality andhence the image of the diode laser focal spot size  which spansapproximately 15 mirrors on the surface of the DMD). The light iscollimated with an achromatic lens with a focal length of f = 50 mm(Thorlabs  AC254-050-B) and focused onto the DMD (Texas Instru-ments  DLP Discovery 4000). The DMD consists of 1920 × 1080aluminum mirrors (10.8 ␮m pitch) that can tilt ±12◦relative tothe ﬂat state of the array  controlled by an interface card (DLPD4000  Texas Instruments). All 1080 mirrors in each rows of thearray are set to the same angle  and the 1920 columns are dividedinto adjacent groupings –e.g.  if we want to divide the energy ofthe photons into 128 “bins”  then groups of 15 adjacent columnsare set in unison. The DMD is mounted at an angle such that the−12◦mirror position directs photons back with a vertical offsetof ∼1◦below the incident light in order to spatially separate theincident and reﬂected photons. The latter photons are recombinedin a second pass through the holographic grating  and focusedonto a ﬁber optic cable that is connected to a photodiode photoncounting module (PerkinElmer  SPCMCD2969PE). The photoncounting module has a dark count rate of ∼200 photons s−1andno read noise. A TTL pulse is output by the photon counter aseach photon is detected  and the pulses are counted in a USB dataacquisition (DAQ) card (National Instruments  USB-6212BNC).Integration timing is controlled by setting the sampling rate andnumber of samples to acquire with the DAQ card in Labview 2009.Binary ﬁlter functions (F)  optimal times (T)  and the estimator(B) were generated from the spectra of all pure components (seeSection 3.2 for more information) using functions from Matlab 7.13R2011b. The input binary optical ﬁlter function determined whichmirrors will point toward the detector (assigned a value of 1) orpoint away (assigned a value of 0). The binary (0–1) mathematicalﬁlters are conﬁgured to the DMD through Labview software (TexasInstruments  DDC4100  Load Blocks.vi) that sets blocks of mirrorson the DMD array corresponding to different wavelengths to theappropriate ±12◦position. Labview scripts were used to sequen-tially apply the ﬁlters and integrate for the corresponding times  tostore the raw photon counts  and to calculate the photon rates. Lin-ear and quadratic discriminant analyses were performed in Matlab7.13 R2011b. Data was further processed and plotted in Igor Pro6.04.3.2. Constructing ﬁltersGenerating accurate ﬁlters for a given application requires highsignal-to-noise training spectra of each of the components of inter-est. Measuring full spectra with the DMD is achieved by notchscanning. This is done by sequentially directing one mirror (ora small set of mirrors) toward the detector (with all other mir-rors directed away) and counting the number of photons detectedat each notch position. Notch scanning measurements were per-formed using 1 s per notch to obtain spectra with a signal-to-noiseratio of ∼500:1. A background spectrum is present in all of our train-ing spectra  arising from the interaction of the excitation laser andthe intervening optical elements. We have implemented two com-pressive detection strategies for removing this background. Theﬁrst method involves measuring the background (with no sample)123450.20.30.40.50.60.70.80.91Classification of 10 ChemicalsNumber of MeasurementsAccuracy  Exp−DesignedExp−RandomSim−DesignedSim−RandomReferences
[1] R. Atar and T. Weissman. Mutual information  relative entropy  and estimation in the Poisson channel.

IEEE Transactions on Information Theory  58(3):1302–1318  March 2012.

[2] A. Banerjee  S. Merugu  I.S. Dhillon  and J. Ghosh. Clustering with bregman divergences. JMLR  2005.
[3] M.W Berry  M. Browne  A.N. Langville  V.P. Pauca  and R. J. Plemmons. Algorithms and applications

for approximate nonnegative matrix factorization. Computational Statistics & Data Analysis  2007.

[4] D.M. Blei  A.Y. Ng  and M.I. Jordan. Latent Dirichlet allocation. JMLR  2003.
[5] L.M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to
the solution of problems in convex programming. USSR computational mathematics and mathematical
physics  1967.

[6] E. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruction from

highly incomplete frequency information. IEEE Trans. on Inform. Theory  2006.

[7] W.R. Carson  M. Chen  M.R.D. Rodrigues  R. Calderbank  and L. Carin. Communications-inspired pro-

jection design with application to compressive sensing. SIAM J. Imaging Sciences  2013.

[8] M. Chen  W. Carson  M. Rodrigues  R. Calderbank  and L. Carin. Communications inspired linear dis-

criminant analysis. In ICML  2012.

[9] G.B. Folland. Real Analysis: Modern Techniques and Their Applications. Wiley New York  1999.
[10] D. Guo.

Information and estimation over binomial and negative binomial models. arXiv preprint

arXiv:1207.7144  2012.

[11] D. Guo  S. Shamai  and S. Verd´u. Mutual information and minimum mean-square error in Gaussian

channels. IEEE Transactions on Information Theory  51(4):1261–1282  April 2005.

[12] D. Guo  S. Shamai  and S. Verd´u. Mutual information and conditional mean estimation in Poisson chan-

nels. IEEE Transactions on Information Theory  54(5):1837–1849  May 2008.

[13] S.M. Haas and J.H. Shapiro. Capacity of wireless optical communications. IEEE Journal on Selected

Areas in Communications  21(8):1346–1357  Aug. 2003.

[14] M. Hellman and J. Raviv. Probability of error  equivocation  and the Chernoff bound. IEEE Transactions

on Information Theory  1970.

[15] A. Lapidoth and S. Shamai. The poisson multiple-access channel. IEEE Transactions on Information

Theory  44(2):488–501  Feb. 1998.

[16] R.S. Liptser and A.N. Shiryaev. Statistics of Random Processes: II. Applications  volume 2. Springer 

2000.

[17] D.P. Palomar and S. Verd´u. Gradient of mutual information in linear vector Gaussian channels. IEEE

Transactions on Information Theory  52(1):141–154  Jan. 2006.

[18] D.P. Palomar and S. Verd´u. Representation of mutual information via input estimates. IEEE Transactions

on Information Theory  53(2):453–470  Feb. 2007.
Certain relations between mutual

[19] S. Prasad.

http://arxiv.org/pdf/1010.1508v1.pdf  2012.

information and ﬁdelity of statistical estimation.

[20] M. Raginsky  R.M. Willett  Z.T. Harmany  and R.F. Marcia. Compressed sensing performance bounds

under poisson noise. IEEE Trans. Signal Processing  2010.

[21] M. Seeger  H. Nickisch  R. Pohmann  and B. Schoelkopf. Optimization of k-space trajectories for com-

pressed sensing by bayesian experimental design. Magnetic Resonance in Medicine  2010.

[22] C.G. Taborda and F. Perez-Cruz. Mutual information and relative entropy over the binomial and negative
binomial channels. In IEEE International Symposium on Information Theory Proceedings (ISIT)  pages
696–700. IEEE  2012.

[23] S. Verd´u. Mismatched estimation and relative entropy.

56(8):3712–3720  Aug. 2010.

IEEE Transactions on Information Theory 

[24] T. Weissman. The relationship between causal and noncausal mismatched estimation in continuous-time

awgn channels. IEEE Transactions on Information Theory  2010.

[25] D.S. Wilcox  G.T. Buzzard  B.J. Lucier  P. Wang  and D. Ben-Amotz. Photon level chemical classiﬁcation

using digital compressive detection. Analytica Chimica Acta  2012.

[26] M. Zhou  L. Hannah  D. Dunson  and L. Carin. Beta-negative binomial process and Poisson factor anal-

ysis. AISTATS  2012.

9

,Liming Wang
David Carlson
Miguel Rodrigues
David Wilcox
Robert Calderbank
Lawrence Carin