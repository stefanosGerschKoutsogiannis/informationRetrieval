2017,Active Exploration for Learning Symbolic Representations,We introduce an online active exploration algorithm for data-efficiently learning an abstract symbolic model of an environment. Our algorithm is divided into two parts: the first part quickly generates an intermediate Bayesian symbolic model from the data that the agent has collected so far  which the agent can then use along with the second part to guide its future exploration towards regions of the state space that the model is uncertain about. We show that our algorithm outperforms random and greedy exploration policies on two different computer game domains. The first domain is an Asteroids-inspired game with complex dynamics but basic logical structure. The second is the Treasure Game  with simpler dynamics but more complex logical structure.,Active Exploration for Learning

Symbolic Representations

Garrett Andersen

PROWLER.io

Cambridge  United Kingdom
garrett@prowler.io

George Konidaris

Department of Computer Science

Brown University

gdk@cs.brown.edu

Abstract

We introduce an online active exploration algorithm for data-efﬁciently learning
an abstract symbolic model of an environment. Our algorithm is divided into two
parts: the ﬁrst part quickly generates an intermediate Bayesian symbolic model
from the data that the agent has collected so far  which the agent can then use along
with the second part to guide its future exploration towards regions of the state
space that the model is uncertain about. We show that our algorithm outperforms
random and greedy exploration policies on two different computer game domains.
The ﬁrst domain is an Asteroids-inspired game with complex dynamics but basic
logical structure. The second is the Treasure Game  with simpler dynamics but
more complex logical structure.

1

Introduction

Much work has been done in artiﬁcial intelligence and robotics on how high-level state abstractions
can be used to signiﬁcantly improve planning [19]. However  building these abstractions is difﬁcult 
and consequently  they are typically hand-crafted [15  13  7  4  5  6  20  9].
A major open question is then the problem of abstraction: how can an intelligent agent learn high-
level models that can be used to improve decision making  using only noisy observations from its
high-dimensional sensor and actuation spaces? Recent work [11  12] has shown how to automatically
generate symbolic representations suitable for planning in high-dimensional  continuous domains.
This work is based on the hierarchical reinforcement learning framework [1]  where the agent has
access to high-level skills that abstract away the low-level details of control. The agent then learns
representations for the (potentially abstract) effect of using these skills. For instance  opening a door
is a high-level skill  while knowing that opening a door typically allows one to enter a building would
be part of the representation for this skill. The key result of that work was that the symbols required to
determine the probability of a plan succeeding are directly determined by characteristics of the skills
available to an agent. The agent can learn these symbols autonomously by exploring the environment 
which removes the need to hand-design symbolic representations of the world.
It is therefore possible to learn the symbols by naively collecting samples from the environment 
for example by random exploration. However  in an online setting the agent shall be able to use
its previously collected data to compute an exploration policy which leads to better data efﬁciency.
We introduce such an algorithm  which is divided into two parts: the ﬁrst part quickly generates
an intermediate Bayesian symbolic model from the data that the agent has collected so far  while
the second part uses the model plus Monte-Carlo tree search to guide the agent’s future exploration
towards regions of the state space that the model is uncertain about. We show that our algorithm is
signiﬁcantly more data-efﬁcient than more naive methods in two different computer game domains.
The ﬁrst domain is an Asteroids-inspired game with complex dynamics but basic logical structure.
The second is the Treasure Game  with simpler dynamics but more complex logical structure.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

2 Background

As a motivating example  imagine deciding the route you are going to take to the grocery store;
instead of planning over the various sequences of muscle contractions that you would use to complete
the trip  you would consider a small number of high-level alternatives such as whether to take one
route or another. You also would avoid considering how your exact low-level state affected your
decision making  and instead use an abstract (symbolic) representation of your state with components
such as whether you are at home or an work  whether you have to get gas  whether there is trafﬁc  etc.
This simpliﬁcation reduces computational complexity  and allows for increased generalization over
past experiences. In the following sections  we introduce the frameworks that we use to represent the
agent’s high-level skills  and symbolic models for those skills.

2.1 Semi-Markov Decision Processes

We assume that the agent’s environment can be described by a semi-Markov decision process
(SMDP)  given by a tuple D = (S  O  R  P  γ)  where S ⊆ Rd is a d-dimensional continuous state
space  O(s) returns a set of temporally extended actions  or options [19] available in state s ∈ S 
R(s(cid:48)  t  s  o) and P (s(cid:48)  t | s  o) are the reward received and probability of termination in state s(cid:48) ∈ S
after t time steps following the execution of option o ∈ O(s) in state s ∈ S  and γ ∈ (0  1] is a
discount factor. In this paper  we are not concerned with the time taken to execute o  so we use

P (s(cid:48) | s  o) =(cid:82) P (s(cid:48)  t | s  o)dt.

An option o is given by three components: πo  the option policy that is executed when the option is
invoked  Io  the initiation set consisting of the states where the option can be executed from  and
βo(s) → [0  1]  the termination condition  which returns the probability that the option will terminate
upon reaching state s. Learning models for the initiation set  rewards  and transitions for each option 
allows the agent to reason about the effect of its actions in the environment. To learn these option
models  the agent has the ability to collect observations of the forms (s  O(s)) when entering a state
s and (s  o  s(cid:48)  r  t) upon executing option o from s.

2.2 Abstract Representations for Planning

We are speciﬁcally interested in learning option models which allow the agent to easily evaluate the
success probability of plans. A plan is a sequence of options to be executed from some starting state 
and it succeeds if and only if it is able to be run to completion (regardless of the reward). Thus  a plan
{o1  o2  ...  on} with starting state s succeeds if and only if s ∈ Io1 and the termination state of each
option (except for the last) lies in the initiation set of the following option  i.e. s(cid:48) ∼ P (s(cid:48) | s  o1) ∈ Io2 
s(cid:48)(cid:48) ∼ P (s(cid:48)(cid:48) | s(cid:48)  o2) ∈ Io3  and so on.
Recent work [11  12] has shown how to automatically generate a symbolic representation that supports
such queries  and is therefore suitable for planning. This work is based on the idea of a probabilistic
symbol  a compact representation of a distribution over inﬁnitely many continuous  low-level states.
For example  a probabilistic symbol could be used to classify whether or not the agent is currently in
front of a door  or one could be used to represent the state that the agent would ﬁnd itself in after
executing its ‘open the door’ option. In both cases  using probabilistic symbols also allows the agent
to be uncertain about its state.
The following two probabilistic symbols are provably sufﬁcient for evaluating the success probability
of any plan [12]; the probabilistic precondition: Pre(o) = P (s ∈ Io)  which expresses the probability
that an option o can be executed from each state s ∈ S  and the probabilistic image operator:

(cid:82)

(cid:82)
S P (s(cid:48) | s  o)Z(s)P (Io | s)ds

S Z(s)P (Io | s)ds

Im(o  Z) =

 

which represents the distribution over termination states if an option o is executed from a distribution
over starting states Z. These symbols can be used to compute the probability that each successive
option in the plan can be executed  and these probabilities can then be multiplied to compute the
overall success probability of the plan; see Figure 1 for a visual demonstration of a plan of length 2.
Subgoal Options Unfortunately  it is difﬁcult to model Im(o  Z) for arbitrary options  so we focus
on restricted types of options. A subgoal option [17] is a special class of option where the distribution
over termination states (referred to as the subgoal) is independent of the distribution over starting

2

Figure 1: Determining the probability that a plan consisting of two options can be executed from
a starting distribution Z0. (a): Z0 is contained in Pre(o1)  so o1 can deﬁnitely be executed. (b):
Executing o1 from Z0 leads to distribution over states Im(o1  Z0). (c): Im(o1  Z0) is not completely
contained in Pre(o2)  so the probability of being able to execute o2 is less than 1. Note that Pre is a
set and Im is a distribution  and the agent typically has uncertain models for them.

states that it was executed from  e.g. if you make the decision to walk to your kitchen  the end result
will be the same regardless of where you started from.
For subgoal options  the image operator can be replaced with the effects distribution: Eﬀ(o) =
Im(o  Z) ∀Z(S)  the resulting distribution over states after executing o from any start distribution
Z(S). Planning with a set of subgoal options is simple because for each ordered pair of options oi and
oj  it is possible to compute and store G(oi  oj)  the probability that oj can be executed immediately

after executing oi: G(oi  oj) =(cid:82)

S Pre(oj  s)Eﬀ(oi)(s)ds.

We use the following two generalizations of subgoal options: abstract subgoal options model the
more general case where executing an option leads to a subgoal for a subset of the state variables
(called the mask)  leaving the rest unchanged. For example  walking to the kitchen leaves the amount
of gas in your car unchanged. More formally  the state vector can be partitioned into two parts
s = [a  b]  such that executing o leaves the agent in state s(cid:48) = [a  b(cid:48)]  where P (b(cid:48)) is independent of
the distribution over starting states. The second generalization is the (abstract) partitioned subgoal
option  which can be partitioned into a ﬁnite number of (abstract) subgoal options. For instance  an
option for opening doors is not a subgoal option because there are many doors in the world  however
it can be partitioned into a set of subgoal options  with one for every door.
The subgoal (and abstract subgoal) assumptions propose that the exact state from which option
execution starts does not really affect the options that can be executed next. This is somewhat
restrictive and often does not hold for options as given  but can hold for options once they have been
partitioned. Additionally  the assumptions need only hold approximately in practice.

3 Online Active Symbol Acquisition

Previous approaches for learning symbolic models from data [11  12] used random exploration.
However  real world data from high-level skills is very expensive to collect  so it is important to use a
more data-efﬁcient approach. In this section  we introduce a new method for learning abstract models
data-efﬁciently. Our approach maintains a distribution over symbolic models which is updated
after every new observation. This distribution is used to choose the sequence of options that in
expectation maximally reduces the amount of uncertainty in the posterior distribution over models.
Our approach has two components: an active exploration algorithm which takes as input a distribution
over symbolic models and returns the next option to execute  and an algorithm for quickly building
a distribution over symbolic models from data. The second component is an improvement upon
previous approaches in that it returns a distribution and is fast enough to be updated online  both of
which we require.

3.1 Fast Construction of a Distribution over Symbolic Option Models

Now we show how to construct a more general model than G that can be used for planning with
abstract partitioned subgoal options. The advantages of our approach versus previous methods are
that our algorithm is much faster  and the resulting model is Bayesian  both of which are necessary
for the active exploration algorithm introduced in the next section.
Recall that the agent can collect observations of the forms (s  o  s(cid:48)) upon executing option o from s 
and (s  O(s)) when entering a state s  where O(s) is the set of available options in state s. Given
a sequence of observations of this form  the ﬁrst step of our approach is to ﬁnd the factors [12] 

3

(a)(b)(c)Pre(o1)Z0Im(o1 Z0)o1o1?Pre(o2)o2?partitions of state variables that always change together in the observed data. For example  consider a
robot which has options for moving to the nearest table and picking up a glass on an adjacent table.
Moving to a table changes the x and y coordinates of the robot without changing the joint angles of
the robot’s arms  while picking up a glass does the opposite. Thus  the x and y coordinates and the
arm joint angles of the robot belong to different factors. Splitting the state space into factors reduces
the number of potential masks (see end of Section 2.2) because we assume that if state variables i
and j always change together in the observations  then this will always occur  e.g. we assume that
moving to the table will never move the robot’s arms.1
Finding the Factors Compute the set of observed masks M from the (s  o  s(cid:48)) observations: each
observation’s mask is the subset of state variables that differ substantially between s and s(cid:48). Since
we work in continuous  stochastic domains  we must detect the difference between minor random
noise (independent of the action) and a substantial change in a state variable caused by action
execution. In principle this requires modeling action-independent and action-dependent differences 
and distinguishing between them  but this is difﬁcult to implement. Fortunately we have found that in
practice allowing some noise and having a simple threshold is often effective  even in more noisy and
complex domains. For each state variable i  let Mi ⊆ M be the subset of the observed masks that
contain i. Two state variables i and j belong to the same factor f ∈ F if and only if Mi = Mj. Each
factor f is given by a set of state variables and thus corresponds to a subspace Sf . The factors are
updated after every new observation.
f be the projection of S∗ onto the
Let S∗ be the set of states that the agent has observed and let S∗
subspace Sf for some factor f  e.g. in the previous example there is a S∗
f which consists of the set
of observed robot (x  y) coordinates. It is important to note that the agent’s observations come only
from executing partitioned abstract subgoal options. This means that S∗
f consists only of abstract
subgoals  because for each s ∈ S∗  sf was either unchanged from the previous state  or changed to
another abstract subgoal. In the robot example  all (x  y) observations must be adjacent to a table
because the robot can only execute an option that terminates with it adjacent to a table or one that
does not change its (x  y) coordinates. Thus  the states in S∗ can be imagined as a collection of
abstract subgoals for each of the factors. Our next step is to build a set of symbols for each factor to
represent its abstract subgoals  which we do using unsupervised clustering.
Finding the Symbols For each factor f ∈ F   we ﬁnd the set of symbols Z f by clustering S∗
f . Let
Z f (sf ) be the corresponding symbol for state s and factor f. We then map the observed states s ∈ S∗
to their corresponding symbolic states sd = {Z f (sf ) ∀f ∈ F}  and the observations (s  O(s)) and
(s  o  s(cid:48)) to (sd  O(s)) and (sd  o  s(cid:48)d)  respectively.
In the robot example  the (x  y) observations would be clustered around tables that the robot could
travel to  so there would be a symbol corresponding to each table.
We want to build our models within the symbolic state space Sd. Thus we deﬁne the symbolic
precondition  Pre(o  sd)  which returns the probability that the agent can execute an option from
some symbolic state  and the symbolic effects distribution for a subgoal option o  Eﬀ (o)  maps to a
subgoal distribution over symbolic states. For example  the robot’s ‘move to the nearest table’ option
maps the robot’s current (x  y) symbol to the one which corresponds to the nearest table.
The next step is to partition the options into abstract subgoal options (in the symbolic state space) 
e.g. we want to partition the ‘move to the nearest table’ option in the symbolic state space so that the
symbolic states in each partition have the same nearest table.
Partitioning the Options For each option o  we initialize the partitioning P o so that each symbolic
state starts in its own partition. We use independent Bayesian sparse Dirichlet-categorical models [18]
for the symbolic effects distribution of each option partition.2 We then perform Bayesian Hierarchical
Clustering [8] to merge partitions which have similar symbolic effects distributions.3

1The factors assumption is not strictly necessary as we can assign each state variable to its own factor.
However  using this uncompressed representation can lead to an exponential increase in the size of the symbolic
state space and a corresponding increase in the sample complexity of learning the symbolic models.

2We use sparse Dirichlet-categorical models because there are a combinatorial number of possible symbolic

state transitions  but we expect that each partition has non-zero probability for only a small number of them.

3We use the closed form solutions for Dirichlet-multinomial models provided by the paper.

4

Algorithm 1 Fast Construction of a Distribution over Symbolic Option Models
1: Find the set of observed masks M.
2: Find the factors F .
3: ∀f ∈ F   ﬁnd the set of symbols Z f .
4: Map the observed states s ∈ S∗ to symbolic states sd ∈ S∗d.
5: Map the observations (s  O(s)) and (s  o  s(cid:48)) to (sd  O(s)) and (sd  o  s(cid:48)d).
6: ∀o ∈ O  initialize P o and perform Bayesian Hierarchical Clustering on it.
7: ∀o ∈ O  ﬁnd Ao and F o∗ .

a  but has yet to actually execute it from any sd ∈ Sd

There is a special case where the agent has observed that an option o was available in some symbolic
states Sd
a. These are not included in the Bayesian
Hierarchical Clustering  instead we have a special prior for the partition of o that they belong to. After
completing the merge step  the agent has a partitioning P o for each option o. Our prior is that with
probability qo 4 each sd ∈ Sd
a belongs to the partition po ∈ P o which contains the symbolic states
most similar to sd  and with probability 1 − qo each sd belongs to its own partition. To determine the
partition which is most similar to some symbolic state  we ﬁrst ﬁnd Ao  the smallest subset of factors
which can still be used to correctly classify P o. We then map each sd ∈ Sd
a to the most similar
partition by trying to match sd masked by Ao with a masked symbolic state already in one of the
partitions. If there is no match  sd is placed in its own partition.
Our ﬁnal consideration is how to model the symbolic preconditions. The main concern is that many
factors are often irrelevant for determining if some option can be executed. For example  whether or
not you have keys in your pocket does not affect whether you can put on your shoe.
Modeling the Symbolic Preconditions Given an option o and subset of factors F o  let Sd
F o be the
symbolic state space projected onto F o. We use independent Bayesian Beta-Bernoulli models for the
symbolic precondition of o in each masked symbolic state sd
F o. For each option o  we use
Bayesian model selection to ﬁnd the the subset of factors F o∗ which maximizes the likelihood of the
symbolic precondition models.
The ﬁnal result is a distribution over symbolic option models H  which consists of the combined sets
of independent symbolic precondition models {Pre(o  sd
F o∗ );∀o ∈ O ∀sd
F o∗ } and independent
symbolic effects distribution models {Eﬀ (o  po);∀o ∈ O ∀po ∈ P o}.
The complete procedure is given in Algorithm 1. A symbolic option model h ∼ H can be sampled
by drawing parameters for each of the Bernoulli and categorical distributions from the corresponding
Beta and sparse Dirichlet distributions  and drawing outcomes for each qo. It is also possible to
consider distributions over other parts of the model such as the symbolic state space and/or a more
complicated one for the option partitionings  which we leave for future work.

F o ∈ Sd

F o∗ ∈ Sd

3.2 Optimal Exploration

In the previous section we have shown how to efﬁciently compute a distribution over symbolic option
models H. Recall that the ultimate purpose of H is to compute the success probabilities of plans
(see Section 2.2). Thus  the quality of H is determined by the accuracy of its predicted plan success
probabilities  and efﬁciently learning H corresponds to selecting the sequence of observations which
maximizes the expected accuracy of H. However  it is difﬁcult to calculate the expected accuracy of
H over all possible plans  so we deﬁne a proxy measure to optimize which is intended to represent
the amount of uncertainty in H. In this section  we introduce our proxy measure  followed by an
algorithm for ﬁnding the exploration policy which optimizes it. The algorithm operates in an online
manner  building H from the data collected so far  using H to select an option to execute  updating
H with the new observation  and so on.
First we deﬁne the standard deviation σH  the quantity we use to represent the amount of uncertainty
in H. To deﬁne the standard deviation  we need to also deﬁne the distance and mean.

4This is a user speciﬁed parameter.

5

We deﬁne the distance K from h2 ∈ H to h1 ∈ H  to be the sum of the Kullback-Leibler (KL)
divergences5 between their individual symbolic effect distributions plus the sum of the KL divergences
between their individual symbolic precondition distributions:6

DKL(Pre h1(o  sd

F o∗ ) || Pre h2 (o  sd

F o∗ ))

sd

F o∗ ∈Sd
F o∗
DKL(Eﬀ h1(o  po) || Eﬀ h2(o  po))].

K(h1  h2) =

(cid:88)

(cid:88)
(cid:88)

o∈O

[

+

po∈P o

We deﬁne the mean  E[H]  to be the symbolic option model such that each Bernoulli symbolic
precondition and categorical symbolic effects distribution is equal to the mean of the corresponding
Beta or sparse Dirichlet distribution:

∀o ∈ O  ∀po ∈ P o  Eﬀ

∀o ∈ O  ∀sd

F o∗ ∈ Sd

F o∗   Pre

E[H](o  po) = Eh∼H [Eﬀ h(o  po)] 
E[H](o  sd
F o∗ )) = Eh∼H [Pre h(o  sd

F o∗ ))].

The standard deviation σH is then simply: σH = Eh∼H [K(h  E[H])]. This represents the expected
amount of information which is lost if E[H] is used to approximate H.
Now we deﬁne the optimal exploration policy for the agent  which aims to maximize the expected
reduction in σH after H is updated with new observations. Let H(w) be the posterior distribution
over symbolic models when H is updated with symbolic observations w (the partitioning is not
updated  only the symbolic effects distribution and symbolic precondition models)  and let W (H  i  π)
be the distribution over symbolic observations drawn from the posterior of H if the agent follows
policy π for i steps. We deﬁne the optimal exploration policy π∗ as:

π∗ = argmax

π

σH − Ew∼W (H i π)[σH(w)].

For the convenience of our algorithm  we rewrite the second term by switching the order of the
expectations: Ew∼W (H i π)[Eh∼H(w)[K(h  E[H(w)])]] = Ew∼W (h i π)[K(h  E[H(w)])]].
Note that the objective function is non-Markovian because H is continuously updated with the
agent’s new observations  which changes σH. This means that π∗ is non-stationary  so Algorithm 2
approximates π∗ in an online manner using Monte-Carlo tree search (MCTS) [3] with the UCT tree
policy [10]. πT is the combined tree and rollout policy for MCTS  given tree T .
There is a special case when the agent simulates the observation of a previously unobserved transition 
which can occur under the sparse Dirichlet-categorical model. In this case  the amount of information
gained is very large  and furthermore  the agent is likely to transition to a novel symbolic state. Rather
than modeling the unexplored state space  instead  if an unobserved transition is encountered during
an MCTS update  it immediately terminates with a large bonus to the score  a similar approach to
that of the R-max algorithm [2]. The form of the bonus is -zg  where g is the depth that the update
terminated and z is a constant. The bonus reﬂects the opportunity cost of not experiencing something
novel as quickly as possible  and in practice it tends to dominate (as it should).

4 The Asteroids Domain

The Asteroids domain is shown in Figure 2a and was implemented using physics simulator pybox2d.
The agent controls a ship by either applying a thrust in the direction it is facing or applying a torque
in either direction. The goal of the agent is to be able to navigate the environment without colliding
with any of the four “asteroids.” The agent’s starting location is next to asteroid 1. The agent is given
the following 6 options (see Appendix A for additional details):

1. move-counterclockwise and move-clockwise: the ship moves from the current face it is
adjacent to  to the midpoint of the face which is counterclockwise/clockwise on the same
asteroid from the current face. Only available if the ship is at an asteroid.

5The KL divergence has previously been used in other active exploration scenarios [16  14].
6Similarly to other active exploration papers  we deﬁne the distance to depend only on the transition models

and not the reward models.

6

Algorithm 2 Optimal Exploration
Input: Number of remaining option executions i.
1: while i ≥ 0 do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end while

Build H from observations (Algorithm 1).
Initialize tree T for MCTS.
while number updates < threshold do
Sample a symbolic model h ∼ H.
Do an MCTS update of T with dynamics given by h.
Terminate current update if depth g is ≥ i  or unobserved transition is encountered.
Store simulated observations w ∼ W (h  g  πT ).
Score = K(h  E[H]) − K(h  E[H(w)]) − zg.

end while
return most visited child of root node.
Execute corresponding option; Update observations; i--.

2. move-to-asteroid-1  move-to-asteroid-2  move-to-asteroid-3  and move-to-asteroid-4:
the ship moves to the midpoint of the closest face of asteroid 1-4 to which it has an
unobstructed path. Only available if the ship is not already at the asteroid and an unobstructed
path to some face exists.

Exploring with these options results in only one factor (for the entire state space)  with symbols
corresponding to each of the 35 asteroid faces as shown in Figure 2a.

(a)

(b)

Figure 2: (a): The Asteroids Domain  and the 35 symbols which can be encountered while exploring
with the provided options. (b): The Treasure Game domain. Although the game screen is drawn
using large image tiles  sprite movement is at the pixel level.

Results We tested the performance of three exploration algorithms: random  greedy  and our
algorithm. For the greedy algorithm  the agent ﬁrst computes the symbolic state space using steps
1-5 of Algorithm 1  and then chooses the option with the lowest execution count from its current
symbolic state. The hyperparameter settings that we use for our algorithm are given in Appendix A.
Figures 3a  3b  and 3c show the percentage of time that the agent spends on exploring asteroids 1  3 
and 4  respectively. The random and greedy policies have difﬁculty escaping asteroid 1  and are rarely
able to reach asteroid 4. On the other hand  our algorithm allocates its time much more proportionally.
Figure 4d shows the number of symbolic transitions that the agent has not observed (out of 115
possible).7 As we discussed in Section 3  the number of unobserved symbolic transitions is a good
representation of the amount of information that the models are missing from the environment.
Our algorithm signiﬁcantly outperforms random and greedy exploration. Note that these results are
using an uninformative prior and the performance of our algorithm could be signiﬁcantly improved by

7We used Algorithm 1 to build symbolic models from the data gathered by each exploration algorithms.

7

(a)

(c)

(b)

(d)

Figure 3: Simulation results for the Asteroids domain. Each bar represents the average of 100 runs.
The error bars represent a 99% conﬁdence interval for the mean. (a)  (b)  (c): The fraction of time that
the agent spends on asteroids 1  3  and 4  respectively. The greedy and random exploration policies
spend signiﬁcantly more time than our algorithm exploring asteroid 1 and signiﬁcantly less time
exploring asteroids 3 and 4. (d): The number of symbolic transitions that the agent has not observed
(out of 115 possible). The greedy and random policies require 2-3 times as many option executions
to match the performance of our algorithm.

starting with more information about the environment. To try to give additional intuition  in Appendix
A we show heatmaps of the (x  y) coordinates visited by each of the exploration algorithms.

5 The Treasure Game Domain
The Treasure Game [12]  shown in Figure 2b  features an agent in a 2D  528 × 528 pixel video-game
like world  whose goal is to obtain treasure and return to its starting position on a ladder at the top of
the screen. The 9-dimensional state space is given by the x and y positions of the agent  key  and
treasure  the angles of the two handles  and the state of the lock.
The agent is given 9 options: go-left  go-right  up-ladder  down-ladder  jump-left  jump-right  down-
right  down-left  and interact. See Appendix A for a more detailed description of the options and
the environment dynamics. Given these options  the 7 factors with their corresponding number of
symbols are: player-x  10; player-y  9; handle1-angle  2; handle2-angle  2; key-x and key-y  3;
bolt-locked  2; and goldcoin-x and goldcoin-y  2.
Results We tested the performance of the same three algorithms: random  greedy  and our algorithm.
Figure 4a shows the fraction of time that the agent spends without having the key and with the lock
still locked. Figures 4b and 4c show the number of times that the agent obtains the key and treasure 
respectively. Figure 4d shows the number of unobserved symbolic transitions (out of 240 possible).
Again  our algorithm performs signiﬁcantly better than random and greedy exploration. The data

8

200400600800100012001400160018002000Option Executions0.00.10.20.30.40.50.60.70.8Fraction of Time SpentAsteroid 1randomgreedyMCTS200400600800100012001400160018002000Option Executions0.000.050.100.150.200.25Fraction of Time SpentAsteroid 3randomgreedyMCTS200400600800100012001400160018002000Option Executions0.000.050.100.150.200.250.300.35Fraction of Time SpentAsteroid 4randomgreedyMCTS200400600800100012001400160018002000Option Executions0102030405060No. Unobserved Symbolic TransitionsUnobserved TransitionsrandomgreedyMCTS(a)

(c)

(b)

(d)

Figure 4: Simulation results for the Treasure Game domain. Each bar represents the average of 100
runs. The error bars represent a 99% conﬁdence interval for the mean. (a): The fraction of time
that the agent spends without having the key and with the lock still locked. The greedy and random
exploration policies spend signiﬁcantly more time than our algorithm exploring without the key and
with the lock still locked. (b)  (c): The average number of times that the agent obtains the key and
treasure  respectively. Our algorithm obtains both the key and treasure signiﬁcantly more frequently
than the greedy and random exploration policies. There is a discrepancy between the number of times
that our agent obtains the key and the treasure because there are more symbolic states where the
agent can try the option that leads to a reset than where it can try the option that leads to obtaining
the treasure. (d): The number of symbolic transitions that the agent has not observed (out of 240
possible). The greedy and random policies require 2-3 times as many option executions to match the
performance of our algorithm.

from our algorithm has much better coverage  and thus leads to more accurate symbolic models. For
instance in Figure 4c you can see that random and greedy exploration did not obtain the treasure
after 200 executions; without that data the agent would not know that it should have a symbol that
corresponds to possessing the treasure.

6 Conclusion

We have introduced a two-part algorithm for data-efﬁciently learning an abstract symbolic represen-
tation of an environment which is suitable for planning with high-level skills. The ﬁrst part of the
algorithm quickly generates an intermediate Bayesian symbolic model directly from data. The second
part guides the agent’s exploration towards areas of the environment that the model is uncertain about.
This algorithm is useful when the cost of data collection is high  as is the case in most real world
artiﬁcial intelligence applications. Our results show that the algorithm is signiﬁcantly more data
efﬁcient than using more naive exploration policies.

9

200400600800100012001400160018002000Option Executions0.00.20.40.60.81.0Fraction of Time SpentNo KeyrandomgreedyMCTS200400600800100012001400160018002000Option Executions012345678Number of TimesKey ObtainedrandomgreedyMCTS200400600800100012001400160018002000Option Executions0.00.51.01.52.02.53.03.5Number of TimesTreasure ObtainedrandomgreedyMCTS200400600800100012001400160018002000Option Executions0255075100125150175200No. Unobserved Symbolic TransitionsUnobserved TransitionsrandomgreedyMCTS7 Acknowledgements

This research was supported in part by the National Institutes of Health under award number
R01MH109177. The U.S. Government is authorized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copyright notation thereon. The content is solely the
responsibility of the authors and does not necessarily represent the ofﬁcial views of the National
Institutes of Health.

References
[1] A.G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete

Event Dynamic Systems  13(4):341–379  2003.

[2] Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for
near-optimal reinforcement learning. Journal of Machine Learning Research  3(Oct):213–231 
2002.

[3] C.B. Browne  E. Powley  D. Whitehouse  S.M. Lucas  P.I. Cowling  P. Rohlfshagen  S. Tavener 
D. Perez  S. Samothrakis  and S. Colton. A survey of Monte-Carlo tree search methods. IEEE
Transactions on Computational Intelligence and AI in Games  4(1):1–43  2012.

[4] S. Cambon  R. Alami  and F. Gravot. A hybrid approach to intricate motion  manipulation and

task planning. International Journal of Robotics Research  28(1):104–126  2009.

[5] J. Choi and E. Amir. Combining planning and motion planning. In Proceedings of the IEEE

International Conference on Robotics and Automation  pages 4374–4380  2009.

[6] Christian Dornhege  Marc Gissler  Matthias Teschner  and Bernhard Nebel. Integrating symbolic
and geometric planning for mobile manipulation. In IEEE International Workshop on Safety 
Security and Rescue Robotics  November 2009.

[7] E. Gat. On three-layer architectures. In D. Kortenkamp  R.P. Bonnasso  and R. Murphy  editors 

Artiﬁcial Intelligence and Mobile Robots. AAAI Press  1998.

[8] K.A. Heller and Z. Ghahramani. Bayesian hierarchical clustering. In Proceedings of the 22nd

international conference on Machine learning  pages 297–304. ACM  2005.

[9] L. Kaelbling and T. Lozano-Pérez. Hierarchical planning in the Now. In Proceedings of the

IEEE Conference on Robotics and Automation  2011.

[10] L. Kocsis and C. Szepesvári. Bandit based Monte-Carlo planning. In Machine Learning: ECML

2006  pages 282–293. Springer  2006.

[11] G.D. Konidaris  L.P. Kaelbling  and T. Lozano-Perez. Constructing symbolic representations for
high-level planning. In Proceedings of the Twenty-Eighth Conference on Artiﬁcial Intelligence 
pages 1932–1940  2014.

[12] G.D. Konidaris  L.P. Kaelbling  and T. Lozano-Perez. Symbol acquisition for probabilistic
high-level planning. In Proceedings of the Twenty Fourth International Joint Conference on
Artiﬁcial Intelligence  pages 3619–3627  2015.

[13] C. Malcolm and T. Smithers. Symbol grounding via a hybrid architecture in an autonomous

assembly system. Robotics and Autonomous Systems  6(1-2):123–144  1990.

[14] S.A. Mobin  J.A. Arnemann  and F. Sommer. Information-based learning by agents in un-
bounded state spaces. In Advances in Neural Information Processing Systems  pages 3023–3031 
2014.

[15] N.J. Nilsson. Shakey the robot. Technical report  SRI International  April 1984.

[16] L. Orseau  T. Lattimore  and M. Hutter. Universal knowledge-seeking agents for stochastic
environments. In International Conference on Algorithmic Learning Theory  pages 158–172.
Springer  2013.

10

[17] D. Precup. Temporal Abstraction in Reinforcement Learning. PhD thesis  Department of

Computer Science  University of Massachusetts Amherst  2000.

[18] N.F.Y. Singer. Efﬁcient Bayesian parameter estimation in large discrete domains. In Advances
in Neural Information Processing Systems 11: Proceedings of the 1998 Conference  volume 11 
page 417. MIT Press  1999.

[19] R.S. Sutton  D. Precup  and S.P. Singh. Between MDPs and semi-MDPs: A framework for
temporal abstraction in reinforcement learning. Artiﬁcial Intelligence  112(1-2):181–211  1999.

[20] J. Wolfe  B. Marthi  and S.J. Russell. Combined task and motion planning for mobile manipula-

tion. In International Conference on Automated Planning and Scheduling  2010.

11

,Garrett Andersen
George Konidaris