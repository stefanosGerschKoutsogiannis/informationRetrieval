2010,Lower Bounds on Rate of Convergence of Cutting Plane Methods,In a recent paper Joachims (2006) presented SVM-Perf  a cutting plane method (CPM) for training linear Support Vector Machines (SVMs) which converges to an $\epsilon$ accurate solution in $O(1/\epsilon^{2})$ iterations. By tightening the analysis  Teo et al. (2010) showed that $O(1/\epsilon)$ iterations suffice. Given the impressive convergence speed of CPM on a number of practical problems  it was conjectured that these rates could be further improved. In this paper we disprove this conjecture. We present counter examples which are not only applicable for training linear SVMs with hinge loss  but also hold for support vector methods which optimize a \emph{multivariate} performance score. However  surprisingly  these problems are not inherently hard. By exploiting the structure of the objective function we can devise an algorithm that converges in $O(1/\sqrt{\epsilon})$ iterations.,Lower Bounds on Rate of Convergence of Cutting

Plane Methods

Xinhua Zhang

Dept. of Computing Science

University of Alberta
xinhua2@ualberta.ca

Ankan Saha

Dept. of Computer Science

University of Chicago

ankans@cs.uchicago.edu

S.V. N. Vishwanathan
Dept. of Statistics and

Dept. of Computer Science

Purdue University

vishy@stat.purdue.edu

Abstract

In a recent paper Joachims [1] presented SVM-Perf  a cutting plane method
(CPM) for training linear Support Vector Machines (SVMs) which converges to
an  accurate solution in O(1/2) iterations. By tightening the analysis  Teo et al.
[2] showed that O(1/) iterations sufﬁce. Given the impressive convergence speed
of CPM on a number of practical problems  it was conjectured that these rates
could be further improved. In this paper we disprove this conjecture. We present
counter examples which are not only applicable for training linear SVMs with
hinge loss  but also hold for support vector methods which optimize a multivari-
ate performance score. However  surprisingly  these problems are not inherently
√
hard. By exploiting the structure of the objective function we can devise an algo-
rithm that converges in O(1/

) iterations.

w

(cid:107)w(cid:107)2

(cid:124) (cid:123)(cid:122) (cid:125)

λ
2
regularizer

(cid:124)

(cid:123)(cid:122)

(cid:125)

1
n

Introduction

1
There has been an explosion of interest in machine learning over the past decade  much of which
has been fueled by the phenomenal success of binary Support Vector Machines (SVMs). Driven by
numerous applications  recently  there has been increasing interest in support vector learning with
linear models. At the heart of SVMs is the following regularized risk minimization problem:
max(0  1 − yi (cid:104)w  xi(cid:105)).

with Remp(w) :=

n(cid:88)

+ Remp(w)

(1)

min

J(w) :=

Here we assume access to a training set of n labeled examples {(xi  yi)}n

{−1  +1}  and use the square Euclidean norm (cid:107)w(cid:107)2 =(cid:80)

empirical risk

i=1

i=1 where xi ∈ Rd and yi ∈
i as the regularizer. The parameter λ

i w2

controls the trade-off between the empirical risk and the regularizer.
There has been signiﬁcant research devoted to developing specialized optimizers which minimize
J(w) efﬁciently.
In an award winning paper  Joachims [1] presented a cutting plane method
(CPM)1  SVM-Perf  which was shown to converge to an  accurate solution of (1) in O(1/2) iter-
ations  with each iteration requiring O(nd) effort. This was improved by Teo et al. [2] who showed
that their Bundle Method for Regularized Risk Minimization (BMRM) (which encompasses SVM-
Perf as a special case) converges to an  accurate solution in O(nd/) time.
While online learning methods are becoming increasingly popular for solving (1)  a key advantage
of CPM such as SVM-Perf and BMRM is their ability to directly optimize nonlinear multivariate
performance measures such as F1-score  ordinal regression loss  and ROCArea which are widely
used in some application areas. In this case Remp does not decompose into a sum of losses over
individual data points like in (1)  and hence one has to employ batch algorithms. Letting ∆(y  ¯y)
denote the multivariate discrepancy between the correct labels y := (y1  . . .   yn)(cid:62) and a candidate
labeling ¯y (to be concretized later)  the Remp for the multivariate measure is formulated by [3] as

1In this paper we use the term cutting plane methods to denote specialized solvers employed in machine

learning. While clearly related  they must not be confused with cutting plane methods used in optimization.

1

(cid:34)

Remp(w) = max

¯y∈{−1 1}n

∆(y  ¯y) +

(cid:35)

(cid:104)w  xi(cid:105) (¯yi − yi)

n(cid:88)

i=1

1
n

.

(2)

In another award winning paper by Joachims [3]  the regularized risk minimization problems corre-
sponding to these measures are optimized by using a CPM.
Given the widespread use of CPM in machine learning  it is important to understand their conver-
gence guarantees in terms of the upper and lower bounds on the number of iterations needed to
converge to an  accurate solution. The tightest  O(1/)  upper bounds on the convergence speed
of CPM is due to Teo et al. [2]  who analyzed a restricted version of BMRM which only optimizes
over one dual variable per iteration. However  on practical problems the observed rate of conver-
gence is signiﬁcantly faster than predicted by theory. Therefore  it had been conjectured that the
upper bounds might be further tightened via a more reﬁned analysis. In this paper we construct
counter examples for both decomposable Remp like in equation (1) and non-decomposable Remp
like in equation (2)  on which CPM requires Ω(1/) iterations to converge  thus disproving this con-
jecture2. We will work with BMRM as our prototypical CPM. As Teo et al. [2] point out  BMRM
includes many other CPM such as SVM-Perf as special cases.
Our results lead to the following natural question: Do the lower bounds hold because regularized
risk minimization problems are fundamentally hard  or is it an inherent limitation of CPM? In other
words  to solve problems such as (1)  does there exist a solver which requires less than O(nd/)
effort (better in n  d and )? We provide partial answers. To understand our contribution one needs
to understand the two standard assumptions that are made when proving convergence rates:

• A1: The data points xi lie inside a L2 (Euclidean) ball of radius R  that is  (cid:107)xi(cid:107) ≤ R.
• A2: The subgradient of Remp is bounded  i.e.  at any point w  there exists a subgradient g

of Remp such that (cid:107)g(cid:107) ≤ G < ∞.

√

(cid:104)· ·(cid:105) denotes the Euclidean dot product (cid:104)x  w(cid:105) = (cid:80)

Clearly assumption A1 is more restrictive than A2. By adapting a result due to [6] we show that one
) algorithm for the case when assumption A1 holds. Finding a fast optimizer
can devise an O(nd/
under assumption A2 remains an open problem.
Notation: Lower bold case letters (e.g.  w  µ) denote vectors  wi denotes the i-th component of
w  0 refers to the vector with all zero components  ei is the i-th coordinate vector (all 0’s except
1 at the i-th coordinate) and ∆k refers to the k dimensional simplex. Unless speciﬁed otherwise 
i xiwi  and (cid:107)·(cid:107) refers to the Euclidean norm
(cid:107)w(cid:107) := ((cid:104)w  w(cid:105))1/2. We denote R := R ∪ {∞}  and [t] := {1  . . .   t}.
Our paper is structured as follows. We brieﬂy review BMRM in Section 2. Two types of lower
bounds are subsequently deﬁned in Section 3  and Section 4 contains descriptions of various counter
examples that we construct. In Section 5 we describe an algorithm which provably converges to an
) iterations under assumption A1. The paper concludes with a
 accurate solution of (1) in O(1/
discussion and outlook in Section 6. Technical proofs and a ready reckoner of the convex analysis
concepts used in the paper can be found in [7  Appendix A].

√

2 BMRM
At every iteration  BMRM replaces Remp by a piecewise linear lower bound Rcp

k and optimizes [2]

(cid:107)w(cid:107)2 + Rcp

λ
2

min

Jk(w) :=

w

(3)
to obtain the next iterate wk. Here ai ∈ ∂Remp(wi−1) denotes an arbitrary subgradient of Remp
at wi−1 and bi = Remp(wi−1) − (cid:104)wi−1  ai(cid:105). The piecewise linear lower bound is successively
tightened until the gap
(4)

k (w) := max
1≤i≤k

k (w)  where Rcp

J(wt) − Jk(wk)

(cid:104)w  ai(cid:105) + bi 

k := min
0≤t≤k

falls below a predeﬁned tolerance .
Since Jk in (3) is a convex objective function  one can compute its dual. Instead of minimizing Jk
with respect to w one can equivalently maximize the dual [2] over the k dimensional simplex:

Dk(α) = − 1
2λ

(cid:107)Akα(cid:107)2 + (cid:104)bk  α(cid:105)  

where α ∈ ∆k 

(5)

2Because of the specialized nature of these solvers  lower bounds for general convex optimizers such as

those studied by Nesterov [4] and Nemirovski and Yudin [5] do not apply.

2

Algorithm 1: qp-bmrm: solving the inner loop
of BMRM exactly via full QP.
Require: Previous subgradients {ai}k
1: Set Ak := (a1  . . .   ak)   bk := (b1  . . . bk)(cid:62).

intercepts {bi}k

i=1 and

i=1.

(cid:8)− 1
2λ(cid:107)Akα(cid:107)2 + (cid:104)α  bk(cid:105)(cid:9).

2: αk ← argmax
α∈∆k

3: return wk = −λ−1Akαk.

i=1.

i=1 and

intercepts {bi}k

Algorithm 2: ls-bmrm: solving the inner loop
of BMRM approximately via line search.
Require: Previous subgradients {ai}k
1: Set Ak := (a1  . . .   ak)   bk := (b1  . . . bk)(cid:62).

k−1  1 − η(cid:1)(cid:62)
2: Set α(η) :=(cid:0)ηα(cid:62)
(cid:8)−1
2λ (cid:107)Akα(η)(cid:107)2 +(cid:104)α(η) bk(cid:105)(cid:9).
4: αk ←(cid:0)ηkα(cid:62)
(cid:1)(cid:62)
k−1  1 − ηk

3: ηk←argmax
η∈[0 1]

.
5: return wk = −λ−1Akαk.

.

and set αk = argmaxα∈∆k
Dk(α). Note that Ak and bk in (5) are deﬁned in Algorithm 1. Since
maximizing Dk(α) is a quadratic programming (QP) problem  we call this algorithm qp-bmrm.
Pseudo-code can be found in Algorithm 1.
Note that at iteration k the dual Dk(α) is a QP with k variables. As the number of iterations increases
the size of the QP also increases. In order to avoid the growing cost of the dual optimization at each
iteration  [2] proposed using a one-dimensional line search to calculate an approximate maximizer
αk on the line segment {(ηα(cid:62)
k−1  (1− η))(cid:62) : η ∈ [0  1]}  and we call this variant ls-bmrm. Pseudo-
code can be found in Algorithm 2. We refer the reader to [2] for details.
Even though qp-bmrm solves a more expensive optimization problem Dk(α) per iteration  Teo
et al. [2] could only show that both variants of BMRM converge at O(1/) rates:

Theorem 1 ([2]) Suppose assumption A2 holds. Then for any  < 4G2/λ  both ls-bmrm and qp-
bmrm converge to an  accurate solution of (1) as measured by (4) after at most the following
number of steps:

log2

λJ(0)
G2 +

8G2
λ

− 1.

Generality of BMRM Thanks to the formulation in (3) which only uses Remp  BMRM is applica-
ble to a wide variety of Remp. For example  when used to train binary SVMs with Remp speciﬁed by
(1)  it yields exactly the SVM-Perf algorithm [1]. When applied to optimize the multivariate score 
e.g. F1-score with Remp speciﬁed by (2)  it immediately leads to the optimizer given by [3].

3 Upper and Lower Bounds
Since most rates of convergence discussed in the machine learning community are upper bounds 
it is important to rigorously deﬁne the meaning of a lower bound with respect to   and to study
its relationship with the upper bounds. At this juncture it is also important to clarify an important
technical point. Instead of minimizing the objective function J(w) deﬁned in (1)  if we minimize a
scaled version cJ(w) this scales the approximation gap (4) by c. Assumptions such as A1 and A2
ﬁx this degree of freedom by bounding the scale of the objective function.
Given a function f ∈ F and an optimization algorithm A  suppose {wk} are the iterates produced
by the algorithm A when minimizing f. Deﬁne T (; f  A) as the ﬁrst step index k when wk becomes
an  accurate solution3:

T (; f  A) = min{k : f (wk) − minw f (w) ≤ } .

(6)
Upper and lower bounds are both properties for a pair of F and A. A function g() is called an
upper bound of (F  A) if for all functions f ∈ F and all  > 0  it takes at most order g() steps for
A to reduce the gap to less than   i.e. 

∀  > 0 ∀ f ∈ F  T (; f  A) ≤ g().

(7)
On the other hand  lower bounds can be deﬁned in two different ways depending on how the above
two universal qualiﬁers are ﬂipped to existential qualiﬁers.
3 The initial point also matters  as in the best case we can just start from the optimal solution. Thus the quan-
tity of interest is actually T (; f  A) := maxw0 min{k : f (wk)− minw f (w) ≤   starting point being w0}.
However  without loss of generality we assume some pre-speciﬁed way of initialization.

(UB)

3

Algorithms

ls-bmrm
qp-bmrm
Nesterov

Assuming A1

Assuming A2

UB

SLB
Ω(1/)
open
√
) Ω(1/

WLB
Ω(1/)
open
√
) Ω(1/

)

O(1/)
√
O(1/)
O(1/

UB

SLB

WLB
O(1/) Ω(1/) Ω(1/)
O(1/)
Ω(1/)

n/a

n/a

open
n/a

Table 1: Summary of the known upper bounds and our lower bounds. Note: A1 ⇒ A2  but not vice
versa. SLB ⇒ WLB  but not vice versa. UB is tight  if it matches WLB.

• Strong lower bounds (SLB) h() is called a SLB of (F  A) if there exists a function ˜f ∈ F 

such that for all  > 0 it takes at least h() steps for A to ﬁnd an  accurate solution of ˜f:

(SLB)

∃ ˜f ∈ F  s.t. ∀  > 0  T (; ˜f   A) ≥ h().

(8)
• Weak lower bound (WLB) h() is called a WLB of (F  A) if for any  > 0  there exists a
function f ∈ F depending on   such that it takes at least h() steps for A to ﬁnd an  accurate
solution of f:

(WLB)

∀  > 0 ∃ f ∈ F  s.t. T (; f  A) ≥ h().

(9)

Clearly  the existence of a SLB implies a WLB. However  it is usually much harder to establish SLB
than WLB. Fortunately  WLBs are sufﬁcient to refute upper bounds or to establish their tightness.
The size of the function class F affects the upper and lower bounds in opposite ways. Suppose
F(cid:48) ⊂ F. Proving upper (resp. lower) bounds on (F(cid:48)  A) is usually easier (resp. harder) than proving
upper (resp. lower) bounds for (F  A).

4 Constructing Lower Bounds
Letting the minimizer of J(w) be w∗  we are interested in bounding the primal gap of the iterates
wk : J(wk) − J(w∗). Datasets will be constructed explicitly whose resulting objective J(w) will
be shown to attain the lower bounds of the algorithms. The Remp for both the hinge loss in (1)
and the F1-score in (2) will be covered  and our results are summarized in Table 1. Note that as
assumption A1 implies A2 and SLB implies WLB  some entries of the table imply others.

4.1 Strong Lower Bounds for Solving Linear SVMs using ls-bmrm

We ﬁrst prove the Ω(1/) lower bound for ls-bmrm on SVM problems under assumption A1. Con-
2  −1) 
sider a one dimensional training set with four examples: (x1  y1) = (−1 −1)  (x2  y2) = (− 1
16  the regularized risk (1) can be written as (using
(x3  y3) = ( 1
w instead of w as it is now a scalar):

2   1)  (x4  y4) = (1  1). Setting λ = 1

(cid:104)

(cid:105)

min
w∈R J(w) =

of J at w∗ : 0 ∈ ∂J(2) =(cid:8) 2

16 − 1

2

1
32

w2 +

1
2

1 − w
2

+

1
2

[1 − w]+ .

2 α : α ∈ [0  1](cid:9). So J(w∗) = 1

+

1

The minimizer of J(w) is w∗ = 2  which can be veriﬁed by the fact that 0 is in the subdifferential

8. Choosing w0 = 0  we have

(10)

Theorem 2 limk→∞ k (J(wk) − J(w∗)) = 1
The proof relies on two lemmata. The ﬁrst shows that the iterates generated by ls-bmrm on J(w)
satisfy the following recursive relations.
Lemma 3 For k ≥ 1  the following recursive relations hold true

4   i.e. J(wk) converges to J(w∗) at 1/k rate.

w2k+1 = 2 +

8α2k−1 1 (w2k−1 − 4α2k−1 1)
w2k−1 (w2k−1 + 4α2k−1 1)

> 2 

and w2k = 2 − 8α2k−1 1
w2k−1

∈ (1  2).

(11)

α2k+1 1 =

2k−1 + 16α2

w2
(w2k−1 + 4α2k−1 1)2 α2k−1 1  where α2k+1 1 is the ﬁrst coordinate of α2k+1.

2k−1 1

(12)

4

The proof is lengthy and is available at [7  Appendix B]. These recursive relations allow us to derive
the convergence rate of α2k−1 1 and wk (see proof in [7  Appendix C]):
4 . Combining with (11)  we get limk→∞ k|2 − wk| = 2.
Lemma 4 limk→∞ kα2k−1 1 = 1
Now that wk approaches 2 at the rate of O(1/k)  it is ﬁnally straightforward to translate it into the
rate at which J(wk) approaches J(w∗). See the proof of Theorem 2 in [7  Appendix D].
4.2 Weak Lower Bounds for Solving Linear SVMs using qp-bmrm

√

Theorem 1 gives an upper bound on the convergence rate of qp-bmrm  assuming that Remp satisﬁes
the assumption A2. In this section we further demonstrate that this O(1/) rate is also a WLB (hence
tight) even when the Remp is specialized to SVM objectives satisfying A2.
Given  > 0  deﬁne n = (cid:100)1/(cid:101) and construct a dataset {(xi  yi)}n
(−1)i (nei+1 +
(cid:107)w(cid:107)2
2

ne1) ∈ Rn+1. Then the corresponding objective function (1) is
n(cid:88)
[1−√
(cid:17)(cid:62)
n )(cid:62) and J(w∗) = 1
i=1 αi  α1  . . .   αn

n(cid:88)
w∗ −(cid:16) 1√

(13)
4n. In fact  simply
: αi ∈ [0  1]
  and

i=1 as yi = (−1)i and xi =

+Remp(w)  where Remp(w) =

[1−yi(cid:104)w  xi(cid:105)]+ =

(cid:80)n

n   . . .   1

nw1−nwi+1]+.

J(w) =

2 ( 1√

(cid:27)

(cid:26)

n   1

n   1

1
n

1
n

i=1

i=1

n

It is easy to see that the minimizer w∗ = 1
check that yi (cid:104)w∗  xi(cid:105) = 1  so ∂J(w∗) =
setting all αi = 1
Theorem 5 Let w0 = ( 1√

2n yields the subgradient 0. Our key result is the following theorem.

produces iterates w1  . . .   wk  . . .. Then it takes qp-bmrm at least(cid:4) 2

n   0  0  . . .)(cid:62). Suppose running qp-bmrm on the objective function (13)

(cid:5) steps to ﬁnd an  accurate

3

solution. Formally 
min
i∈[k]

J(wi) − J(w∗) =

1
2k

+

1
4n

for all k ∈ [n]  hence min
i∈[k]

J(wi) − J(w∗) >  for all k <

2
3

.

 

n

.

(cid:18)

1
n

1
n

n

=

(cid:32)

w

2

=

  and

y1x1 =

1
n

(cid:122)

1
k

  1  0  . . .

(cid:19)(cid:62)

(cid:19)(cid:62)

(cid:26) 1

− 1√
n

w1 = argmin

 −1  0  . . .

w1 − w2 +

a1 = − 1
n

(cid:107)w(cid:107)2 − 1√
n

b1 = Remp(w0) − (cid:104)a1  w0(cid:105) = 0 +

In general  we claim that the k-th iterate wk produced by qp-bmrm is given by

Indeed  after taking n steps  wn will cut a subgradient an+1 = 0 and bn+1 = 0  and then the
minimizer of Jn+1(w) gives exactly w∗.

Proof Since Remp(w0) = 0 and ∂Remp(w0) =(cid:8)−1
i=1 αiyixi : αi ∈ [0  1](cid:9)  we can choose
(cid:80)n
(cid:18) 1√
(cid:27)
(cid:33)(cid:62)
(cid:123)
(cid:125)(cid:124)
i=k+1 αiyixi : αi ∈ [0  1](cid:9). Thus we
easy to check that Remp(wk) = 0 and ∂Remp(wk) =(cid:8)−1
(cid:80)n
(cid:33)(cid:62)
(cid:32)
(cid:122)
(cid:27)
(cid:110)
(cid:111) (cid:51)
wk+1 +(cid:80)
2n while J(w∗) = 1

which can be veriﬁed by checking that ∂Jk+1(wk+1) =
0. All that remains is to observe that J(wk) = 1
2k + 1
that J(wk) − J(w∗) = 1

We prove this claim by induction on k. Assume the claim holds true for steps 1  . . .   k  then it is

  0  . . .
k + 1
i∈[k+1] αiai : α ∈ ∆k+1

bk+1 = Remp(wk) − (cid:104)ak+1  wk(cid:105) =

(cid:107)w(cid:107)2 + max
1≤i≤k+1

4n from which it follows

ak+1 = − 1
n

{(cid:104)ai  w(cid:105) + bi}

can again choose

wk+1 = argmin

(cid:26) 1

yk+1xk+1 

1√
n

1√
n

2k + 1

4n as claimed.

1
n

(cid:125)(cid:124)

 

  . . .  

  0  . . .

1

 

1

k + 1

w

2

=

  . . .  

  so

k+1 copies

1
k

n

wk =

.

k copies

and

(cid:123)

 

5

√

As an aside  the subgradient of the Remp in (13) does have Euclidean norm
2n at w = 0. However 
in the above run of qp-bmrm  ∂Remp(w0)  . . .   ∂Remp(wn) always contains a subgradient with

norm 1. So if we restrict the feasible region to(cid:8)n−1/2(cid:9) × [0 ∞]n  then J(w) does satisfy the

assumption A2 and the optimal solution does not change. This is essentially a local satisfaction of
A2. In fact  having a bounded subgradient of Remp at all wk is sufﬁcient for qp-bmrm to converge
at the rate in Theorem 1.
However when we assume A1 which is more restrictive than A2  it remains an open question to
determine whether the O(1/) rates are optimal for qp-bmrm on SVM objectives. Also left open is
the SLB for qp-bmrm on SVMs.

4.3 Weak Lower Bounds for Optimizing F1-score using qp-bmrm
F1-score is deﬁned by using the contingency table: F1(¯y  y) := 2a
2a+b+c.
Given  > 0  deﬁne n = (cid:100)1/(cid:101)+1 and construct a dataset {(xi  yi)}n
i=1 as
e1− n
2 ei+1 ∈ Rn+1 with yi = −1 for all i ∈ [n−1] 
follows: xi = − n
√
2
2 en+1 ∈ Rn+1 with yn = +1. So there is only one
and xn =
positive training example. Then the corresponding objective function is

√
2 e1 + n

3n

3

(cid:107)w(cid:107)2 + max

n(cid:88)
e1. Then qp-bmrm takes at least(cid:4) 1
(cid:18) 1

1 − F1(y  ¯y) +

1
n

i=1

¯y

3

3

J(w) =

1
2
Theorem 6 Let w0 = 1√
J(w) ≥ 1
J(wk)−min
2

w

y = 1 y =−1
a
c

b
d

¯y = 1
¯y =−1

Contingency table.

(cid:35)

yi (cid:104)w  xi(cid:105) (yi ¯yi − 1)

(cid:5) steps to ﬁnd an  accurate solution.

(14)

.

1
3
Proof A rigorous proof can be found in [7  Appendix E]  we provide a sketch here. The crux is to
show

∀k ∈ [n−1]  hence min
i∈[k]

J(w) >  ∀k <

J(wi)−min

− 1

n − 1

k

w

.

(cid:34)
(cid:19)

(cid:122)

1
k



We prove (15) by induction. Assume it holds for steps 1  . . .   k. Then at step k + 1 we have

(cid:32)

1√
3

 

(cid:125)(cid:124)

k copies

(cid:123)

1
k

(cid:33)(cid:62)

wk =

  . . .  

  0  . . .

∀k ∈ [n − 1].

yi (cid:104)wk  xi(cid:105) =

1
n

1

6 + 1

2k

1
6
1
2

if i ∈ [k]
if k + 1 ≤ i ≤ n − 1
if i = n

.

(15)

(16)

For convenience  deﬁne the term in the max in (14) as

Υk(¯y) := 1 − F1(y  ¯y) +

1
n

yi (cid:104)wk  xi(cid:105) (yi ¯yi − 1).

n(cid:88)

i=1

Then it is not hard to see that the following assignments of ¯y (among others) maximize Υk: a)
correct labeling  b) only misclassify the positive training example xn (i.e.  ¯yn = −1)  c) only
misclassify one negative training example in xk+1  . . .   xn−1 into positive. And Υk equals 0 at all
these assignments. For a proof  consider two cases. If ¯y misclassiﬁes the positive training example 
then F1(y  ¯y) = 0 and by (16) we have
Υk(¯y) = 1−0 +

yi(cid:104)wk  xi(cid:105) (yi ¯yi−1)+

(yi ¯yi−1)+

(−1−1) =

n−1(cid:88)

n−1(cid:88)

k(cid:88)

(yi ¯yi−1)≤ 0.

k + 3

6k

i=1

1
6

i=k+1

1
n

i=1

1
2

Suppose ¯y correctly labels the positive example  but misclassiﬁes t1 examples in x1  . . .   xk and t2
examples in xk+1  . . .   xn−1 (into positive). Then F1(y  ¯y) =

  and

2

Υk(¯y) = 1 −

2

2 + t1 + t2
−

t1 + t2

2 + t1 + t2

=

+

(cid:18) 1

3

2+t1+t2

(cid:18) 1

6

1
k

+

+

(cid:19)

(cid:19) k(cid:88)
(yi ¯yi − 1) +
t2 ≤ t − t2

i=1

1
2k
t1 − 1
3

3(2 + t)

1
6
≤ 0

n−1(cid:88)

i=k+1

(yi ¯yi − 1)

(t := t1 + t2).

6

So we can pick ¯y as (

ak+1 =

−2
n

yk+1xk+1 = − 1√
3

(cid:122)

(cid:125)(cid:124)

(cid:123)

k copies

−1  . . .  −1  +1 

(cid:122)

(cid:125)(cid:124)

(cid:123)

(cid:125)(cid:124)

:=Jk+1(w)

n−k−1 copies
−1  . . .  −1  +1)(cid:62) which only misclassiﬁes xk+1  and get
1
3

bk+1 = Remp(wk) − (cid:104)ak+1  wk(cid:105) = 0 +

1
3

=

 

e1 − ek+2 

(cid:125)(cid:124)

k+1 copies

(cid:122)

(cid:32)
(cid:123)
(cid:110)
wk+1 +(cid:80)k+1

 

1

1

  . . .  

k + 1

1√
k + 1
3
i=1 αiai : α ∈ ∆k+1

(cid:33)(cid:62)

(cid:123)
(cid:111) (cid:51) 0 (just set all

  0  . . .

.

(cid:122)

1
2

wk+1 = argmin

w

(cid:107)w(cid:107)2 + max
i∈[k+1]

{(cid:104)ai  w(cid:105) + bi} =

which can be veriﬁed by ∂Jk+1(wk+1) =
αi = 1

k+1). So (15) holds for step k + 1. End of induction.

All that remains is to observe that J(wk) = 1
3 + 1
from which it follows that J(wk) − minw J(w) ≥ 1

2 ( 1

k ) while minw J(w) ≤ J(wn−1) = 1
2 ( 1
n−1 ) as claimed in Theorem 6.
2 ( 1

k − 1

3 + 1

n−1 )

) Algorithm for Training Binary Linear SVMs

√
5 An O(nd/
The lower bounds we proved above show that CPM such as BMRM require Ω(1/) iterations to
converge. We now show that this is an inherent limitation of CPM and not an artifact of the problem.
√
To demonstrate this  we will show that one can devise an algorithm for problems (1) and (2) which
will converge in O(1/
) iterations. The key difﬁculty stems from the non-smoothness of the
objective function  which renders second and higher order algorithms such as L-BFGS inapplicable.
However  thanks to [7  Theorem 7 in Appendix A]  the Fenchel dual of (1) is a convex smooth
function with a Lipschitz continuous gradient  which are easy to optimize.
To formalize the idea of using the Fenchel dual  we can abstract from the objectives (1) and (2) a
composite form of objective functions used in machine learning with linear models:
J(w) = f (w) + g(cid:63)(Aw)  where Q1 is a closed convex set.

(17)

min
w∈Q1

Here  f (w) is a strongly convex function corresponding to the regularizer  Aw stands for the output
of a linear model  and g(cid:63) encodes the empirical risk measuring the discrepancy between the correct
labels and the output of the linear model. Let the domain of g be Q2. It is well known that [e.g. 8 
Theorem 3.3.5] under some mild constraint qualiﬁcations  the adjoint form of J(w):

D(α) = −g(α) − f (cid:63)(−A(cid:62)α)  α ∈ Q2

(18)

satisﬁes J(w) ≥ D(α) and inf w∈Q1 J(w) = supα∈Q2 D(α).
(cid:80)n
Example 1: binary SVMs with bias. Let A := −Y X(cid:62) where Y := diag(y1  . . .   yn) and X :=
i=1 [1 + ui − yib]+ which corresponds to
(x1  . . .   xn)  f (w) = λ
(cid:110)
i αi. Then the adjoint form turns out to be the well known SVM dual objective function:

2 (cid:107)w(cid:107)2  g(cid:63)(u) = minb∈R 1

g(α) =−(cid:80)

n

(cid:88)

α(cid:62)Y X(cid:62)XY α  α ∈ Q2 =

D(α) =

(19)

αi − 1
2λ

Example 2: multivariate scores. Denote A as a 2n-by-d matrix where the ¯y-th row is
n u¯y
¯y ∆(y  ¯y)α¯y  we recover the primal objective (2) for multi-

i (¯yi − yi) for each ¯y ∈ {−1  +1}n  f (w) = λ

i

(cid:80)n
which corresponds to g(α) = −n(cid:80)
i=1 x(cid:62)
(cid:88)

variate performance measure. Its adjoint form is
D(α) =− 1
2λ

α(cid:62)AA(cid:62)α + n

¯y

∆(y  ¯y)α¯y  α ∈ Q2 =

i

.

yiαi = 0

(cid:88)

α ∈ [0  n−1]n :

(cid:111)
(cid:2)∆(y  ¯y) + 1
2 (cid:107)w(cid:107)2  g(cid:63)(u) = max¯y
(cid:111)
(cid:110)
(cid:88)

α ∈ [0  n−1]2n

α¯y =

:

1
n

. (20)

¯y

(cid:3)

In a series of papers [6  9  10]  Nesterov developed optimal gradient based methods for minimizing
the composite objectives with primal (17) and adjoint (18). A sequence of wk and αk is produced
such that under assumption A1 the duality gap J(wk) − D(αk) is reduced to less than  after at
√
most k = O(1/

) steps. We refer the readers to [9  11] for details.

7

(cid:16)

(cid:88)

i

(cid:17)

i αi log αi.

(21)

p(¯y; w) ∝ exp

c∆(¯y  y) +

ai (cid:104)xi  w(cid:105) ¯yi

V (α  g) := argmin
¯α∈Q2

F ( ¯α) − (cid:104)∇F (α) − g  ¯α(cid:105) .

5.1 Efﬁcient Projections in Training SV Models with Optimal Gradient Methods
However  applying Nesterov’s algorithm is challenging  because it requires an efﬁcient subroutine
for computing projections onto the set of constraints Q2. This projection can be either an Euclidean
projection or a Bregman projection.
Example 1: binary SVMs with bias.
In this case we need to compute the Euclidean projection to
Q2 deﬁned by (19)  which entails solving a Quadratic Programming problem with a diagonal Hes-
sian  many box constraints  and a single equality constraint. We present an O(n) algorithm for
√
this task in [11  Section 5.5.1]. Plugging this into the algorithm described in [9] and noting that
all intermediate steps of the algorithm can be computed in O(nd) time directly yield a O(nd/
)
algorithm. More detailed description of the algorithm is available in [11].
Example 2: multivariate scores. Since the dimension of Q2 in (20) is exponentially large in
n  Euclidean projection is intractable and we resort to Bregman projection. Given a differentiable
convex function F on Q2  a point α  and a direction g  we can deﬁne the Bregman projection as:

Scaling up α by a factor of n  we can choose F (α) as the negative entropy F (α) = −(cid:80)
The solver will request the expectation E¯y [(cid:80)

Then the application of the algorithm in [9] will endow a distribution over all possible labelings:

  where c and ai are constant scalars.

√

√

i aixi ¯yi] which in turn requires that marginal distri-
bution of p(¯yi). This is not as straightforward as in graphical models because ∆(¯y  y) may not
decompose. Fortunately  for multivariate scores deﬁned by contingency tables  it is possible to com-
pute the marginals in O(n2) time by using dynamic programming  and this cost is similar to the
algorithm proposed by [3]. The detail of the dynamic programming is given in [11  Section 5.4].
6 Outlook and Conclusion
CPM are widely employed in machine learning especially in the context of structured prediction
[12]. While upper bounds on their rates of convergence were known  lower bounds were not studied
before. In this paper we set out to ﬁll this gap by exhibiting counter examples in binary classiﬁcation
on which CPM require Ω(1/) iterations. Our examples are substantially different from the one in
√
[13] which requires an increasing number of classes. The Ω(1/) lower bound is a fundamental lim-
itation of these algorithms and not an artifact of the problem. We show this by devising an O(1/
)
algorithm borrowing techniques from [9]. However  this algorithm assumes that the dataset is con-
tained in a ball of bounded radius (assumption A1 Section 1). Devising a O(1/
) algorithm under
the less restrictive assumption A2 remains an open problem.
It is important to note that the linear time algorithm in [11  Section 5.5.1] is the key to obtaining a
) computational complexity for binary SVMs with bias mentioned in Section 5.1. How-
O(nd/
ever  this method has been rediscovered independently by many authors (including us)  with the
earliest known reference to the best of our knowledge being [14] in 1990. Some recent work in
optimization [15] has focused on improving the practical performance  while in machine learning
[16] gave an expected linear time algorithm via randomized median ﬁnding.
Choosing an optimizer for a given machine learning task is a trade-off between a number of poten-
tially conﬂicting requirements. CPM are one popular choice but there are others. If one is interested
in classiﬁcation accuracy alone  without requiring deterministic guarantees  then online to batch
conversion techniques combined with stochastic subgradient descent are a good choice [17]. While
the dependence on  is still Ω(1/) or worse [18]  one gets bounds independent of n. However  as
we pointed out earlier  these algorithms are applicable only when the empirical risk decomposes
over the examples.
On the other hand  one can employ coordinate descent in the dual as is done in the Sequential Mini-
mal Optimization (SMO) algorithm of [19]. However  as [20] show  if the kernel matrix obtained by
stacking xi into a matrix X and X(cid:62)X is not strictly positive deﬁnite  then SMO requires O(n/)
iterations with each iteration costing O(nd) effort. However  when the kernel matrix is strictly pos-
itive deﬁnite  then one can obtain an O(n2 log(1/)) bound on the number of iterations  which has
better dependence on   but is prohibitively expensive for large n. Even better dependence on  can
be achieved by using interior point methods [21] which require only O(log(log(1/)) iterations  but
the time complexity per iteration is O(min{n2d  d2n}).

8

References
[1] T. Joachims. Training linear SVMs in linear time. In Proc. ACM Conf. Knowledge Discovery

and Data Mining (KDD)  pages 217–226  2006.

[2] C. H. Teo  S. V. N. Vishwanthan  A. J. Smola  and Q. V. Le. Bundle methods for regularized

risk minimization. J. Mach. Learn. Res.  11:311–365  January 2010.

[3] T. Joachims. A support vector method for multivariate performance measures. In Proc. Intl.

Conf. Machine Learning  pages 377–384  2005.

[4] Y. Nesterov. Introductory Lectures On Convex Optimization: A Basic Course. Springer  2003.
[5] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization.

John Wiley and Sons  1983.

[6] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of con-

vergence O(1/k2). Soviet Math. Docl.  269:543–547  1983.

[7] Xinhua Zhang  Ankan Saha  and S.V.N. Vishwanathan.
of convergence of cutting plane methods (long version).
http://www.stat.purdue.edu/∼vishy/papers/ZhaSahVis10 long.pdf.

Lower bounds on rate
Technical
report  2010.

[8] J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and

Examples. CMS books in Mathematics. Canadian Mathematical Society  2000.

[9] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on

Optimization  16(1):235–249  2005. ISSN 1052-6234.

[10] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Re-

port 76  CORE Discussion Paper  UCL  2007.

[11] Xinhua Zhang  Ankan Saha  and S.V.N. Vishwanathan. Regularized risk minimization by Nes-
terov’s accelerated gradient methods: Algorithmic extensions and empirical studies. Technical
report arXiv:1011.0472  2010. http://arxiv.org/abs/1011.0472.

[12] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured

and interdependent output variables. J. Mach. Learn. Res.  6:1453–1484  2005.

[13] T. Joachims  T. Finley  and C.N.˜J. Yu. Cutting-plane training of structural SVMs. Machine

Learning Journal  77(1):27–59  2009.

[14] P. M. Pardalos and N. Kovoor. An algorithm for singly constrained class of quadratic programs

subject to upper and lower bounds. Mathematical Programming  46:321–328  1990.

[15] Y.-H. Dai and R. Fletcher. New algorithms for singly linearly constrained quadratic programs
subject to lower and upper bounds. Mathematical Programming: Series A and B archive  106
(3):403–421  2006.

[16] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the (cid:96)1-ball
for learning in high dimensions. In Proc. Intl. Conf. Machine Learning  pages 272–279  2008.
[17] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver

for SVM. In Proc. Intl. Conf. Machine Learning  pages 807–814  2007.

[18] A. Agarwal  P. L. Bartlett  P. Ravikumar  and M. Wainwright.

Information-theoretic lower
bounds on the oracle complexity of convex optimization. In Neural Information Processing
Systems  pages 1–9  2009.

[19] J. C. Platt. Sequential minimal optimization: A fast algorithm for training support vector

machines. Technical Report MSR-TR-98-14  Microsoft Research  1998.

[20] N. List and H. U. Simon. SVM-optimization and steepest-descent line search. In S. Dasgupta

and A. Klivans  editors  Proc. Annual Conf. Computational Learning Theory  2009.

[21] M. C. Ferris and T. S. Munson. Interior-point methods for massive support vector machines.

SIAM Journal on Optimization  13(3):783–804  2002.

9

,Alexandros Paraschos
Christian Daniel
Jan Peters
Gerhard Neumann
Huining Hu
Zhentao Li
Adrian Vetta