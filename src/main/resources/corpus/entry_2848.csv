2019,Learnable Tree Filter for Structure-preserving Feature Transform,Learning discriminative global features plays a vital role in semantic segmentation. And most of the existing methods adopt stacks of local convolutions or non-local blocks to capture long-range context. However  due to the absence of spatial structure preservation  these operators ignore the object details when enlarging receptive fields. In this paper  we propose the learnable tree filter to form a generic tree filtering module that leverages the structural property of minimal spanning tree to model long-range dependencies while preserving the details. Furthermore  we propose a highly efficient linear-time algorithm to reduce resource consumption. Thus  the designed modules can be plugged into existing deep neural networks conveniently. To this end  tree filtering modules are embedded to formulate a unified framework for semantic segmentation. We conduct extensive ablation studies to elaborate on the effectiveness and efficiency of the proposed method. Specifically  it attains better performance with much less overhead compared with the classic PSP block and Non-local operation under the same backbone. Our approach is proved to achieve consistent improvements on several benchmarks without bells-and-whistles. Code and models are available at https://github.com/StevenGrove/TreeFilter-Torch.,Learnable Tree Filter for Structure-preserving

Feature Transform

Lin Song1∗ Yanwei Li2 3∗ Zeming Li4 Gang Yu4 Hongbin Sun1†

Jian Sun4 Nanning Zheng1

1 Institute of Artiﬁcial Intelligence and Robotics  Xi’an Jiaotong Univeristy.

2 Institute of Automation  Chinese Academy of Sciences.

3 University of Chinese Academy of Sciences. 4 Megvii Inc. (Face++).

stevengrove@stu.xjtu.edu.cn  liyanwei2017@ia.ac.cn 

{hsun  nnzheng}@mail.xjtu.edu.cn  {lizeming  yugang  sunjian}@megvii.com

Abstract

Learning discriminative global features plays a vital role in semantic segmentation.
And most of the existing methods adopt stacks of local convolutions or non-local
blocks to capture long-range context. However  due to the absence of spatial struc-
ture preservation  these operators ignore the object details when enlarging receptive
ﬁelds. In this paper  we propose the learnable tree ﬁlter to form a generic tree ﬁlter-
ing module that leverages the structural property of minimal spanning tree to model
long-range dependencies while preserving the details. Furthermore  we propose a
highly efﬁcient linear-time algorithm to reduce resource consumption. Thus  the
designed modules can be plugged into existing deep neural networks conveniently.
To this end  tree ﬁltering modules are embedded to formulate a uniﬁed framework
for semantic segmentation. We conduct extensive ablation studies to elaborate on
the effectiveness and efﬁciency of the proposed method. Speciﬁcally  it attains bet-
ter performance with much less overhead compared with the classic PSP block and
Non-local operation under the same backbone. Our approach is proved to achieve
consistent improvements on several benchmarks without bells-and-whistles. Code
and models are available at https://github.com/StevenGrove/TreeFilter-Torch.

1

Introduction

Scene perception  based on semantic segmentation  is a fundamental yet challenging topic in the
vision ﬁeld. The goal is to assign each pixel in the image with one of several predeﬁned categories.
With the developments of convolutional neural networks (CNN)  it has achieved promising results
using improved feature representations. Recently  numerous approaches have been proposed to
capture larger receptive ﬁelds for global context aggregation [1–5]  which can be divided into local
and non-local solutions according to their pipelines.
Traditional local approaches enlarge receptive ﬁelds by stacking conventional convolutions [6–8]
or their variants (e.g.  atrous convolutions [9  2]). Moreover  the distribution of impact within a
receptive ﬁeld in deep stacks of convolutions converges to Gaussian [10]  without detailed structure
preservation (the pertinent details  which are proved to be effective in feature representation [11  12]).
Considering the limitation of local operations  several non-local solutions have been proposed to
model long-range feature dependencies directly  such as convolutional methods (e.g.  non-local
operations [13]  PSP [3] and ASPP modules [2  14  15]) and graph-based neural networks [16–
18]. However  due to the absence of structure-preserving property  which considers both spatial

∗Equal contribution. This work was done in Megvii Research.
†Corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Toy illustration of the tree ﬁltering module. Given a detail-rich feature map from low-level
stage  we ﬁrst measure the dissimilarity between each pixel and its’ quad neighbours. Then  the MST
is built upon the 4-connected planar graph to formulate a learnable tree ﬁlter. The edge between
two vertices denotes the distance calculated from high-level semantics. Red edges indicate the close
relation with vertex k. The intra-class inconsistency could be alleviated after feature transform.

distance and feature dissimilarity  the object details are still neglected. Going one step further  the
abovementioned operations can be viewed as coarse feature aggregation methods  which means they
fail to explicitly preserve the original structures when capturing long-range context cues.
In this work  we aim to ﬁx this issue by introducing a novel network component that enables efﬁcient
structure-preserving feature transform  called learnable tree ﬁlter. Motivated by traditional tree
ﬁlter [19]  a widely used image denoising operator  we utilize tree-structured graphs to model long-
range dependencies while preserving the object structure. To this end  we ﬁrst build the low-level
guided minimum spanning trees (MST)  as illustrated in Fig. 1. Then the distance between vertices in
MST are calculated based on the high-level semantics  which can be optimized in backpropagation.
For example  the dissimilarity wk m between vertex k and m in Fig. 1 is calculated from semantic-
rich feature embeddings. Thus  combined with the structural property of MST  the spatial distance
and feature dissimilarity have been modeled into tree-structured graph simultaneously (e.g.  the
distance between vertex k and its spatially adjacent one n has been enlarged in Fig. 1  for that more
edges with dissimilarities are calculated when approaching n). To enable the potential for practical
application  we further propose an efﬁcient algorithm which reduces the O(N 2) complexity of brute
force implementation to linear-time consumption. Consequently  different from conditional random
ﬁelds (CRF) [20–22]  the formulated modules can be embedded into several neural network layers
for differentiable optimization.
In principle  the proposed tree ﬁltering module is fundamentally different from most CNN based
methods. The approach exploits a new dimension: tree-structure graph is utilized for structure-
preserving feature transform  bring detailed object structure as well as long-range dependencies.
With the designed efﬁcient implementation  the proposed approach can be applied for multi-scale
feature aggregation with much less resource consumption. Moreover  extensive ablation studies have
been conducted to elaborate on its superiority in both performance and efﬁciency even comparing
with PSP block [3] and Non-local block [13]. Experiments on two well-known datasets (PASCAL
VOC 2012 [23] and Cityscapes [24]) also prove the effectiveness of the proposed method.

2 Learnable Tree Filter

To preserve object structures when capturing long-range dependencies  we formulate the proposed
learnable tree ﬁlter into a generic feature extractor  called tree ﬁltering module. Thus  it can be easily
embedded in deep neural networks for end-to-end optimization. In this section  we ﬁrstly introduce
the learnable tree ﬁltering operator. And then the efﬁcient implementation is presented for practical
applications. The constructed framework for semantic segmentation is elaborated at last.

2.1 Formulation

First  we represent the low-level feature as a undirected graph G = (V  E)  with the dissimilarity
weight ω for edges. The vertices V are the semantic features  and the interconnections of them can

2

be denoted as E. Low-level stage feature map  which contains abundant object details  is adopted as
the guidance for 4-connected planar graph construction  as illustrated in Fig.1. Thus  a spanning tree
can be generated from the graph by performing a pruning algorithm to remove the edges with the
substantial dissimilarity. From this perspective  the graph G turns out to be the minimum spanning
tree (MST) whose sum of dissimilarity weights is minimum out of all spanning trees. The property of
MST ensures preferential interaction among similar vertices. Motivated by traditional tree ﬁlter [19] 
a generic tree ﬁltering module in the deep neural network can be formulated as:

S (Ei j) f (xj) .

(1)

(cid:88)

∀j∈Ω

yi =

1
zi

Where i and j indicate the index of vertices  Ω denotes the set of all vertices in the tree G  x represents
the input encoded features and y means the output signal sharing the same shape with x. Ei j is a
hyperedge which contains a set of vertices traced from vertex i to j in G. The similarity function S
projects the features of the hyperedge into a positive scalar value  as described in Eq. 2. The unary
function f (·) represents the feature embedding transformation. zi is the summation of similarity
S(Ei j) alone with j to normalize the response.

S (Ei j) = exp (−D (i  j))   where D (i  j) = D (j  i) =

ωk m.

(2)

(cid:88)

(k m)∈Ei j

According to the formula in Eq. 1  the tree ﬁltering operation can be considered as one kind of
weighted-average ﬁlter. The variable ωk m indicates the dissimilarity between adjacent vertices
(k and m) that can be computed by a pairwise function (here we adopt Euclidean distance). The
distance D between two vertices (i and j) is deﬁned as summation of dissimilarity ωk m along the
path in hyperedge Ei j. Note that D degenerates into spatial distance when ω is set to a constant
matrix. Since ω actually measures pairwise distance in the embedded space  the aggregation along
the pre-generated tree considers spatial distance and feature difference simultaneously.

exp (−ωk m)   where zi =

exp (−ωk m) .

(3)

(cid:88)

∀j∈Ω

yi =

1
zi

(cid:89)

f (xj)

(k m)∈Ei j

(cid:88)

(cid:89)

∀j∈Ω

(k m)∈Ei j

The tree ﬁltering module can be reformulated to Eq. 3. Obviously  the input feature xj and dissimi-
larity ωk m take responsibility for the output response yi. Therefore  the derivative of output with
respect to input variables can be derived as Eq. 4 and Eq. 5. V i
m in Eq. 5 is deﬁned with the children
of vertex m in the tree whose root node is the vertex i.

∂yi
∂xj

=

S (Ei j)

∂f (xj)

 

∂xj

zi

(cid:88)

j∈V i

m

∂yi

∂ωk m

=

S (Ei k)

∂S (Ek m)

zi

∂ωk m

S (Em j) f (xj) − yizm

 .

(4)

(5)

In this way  the proposed tree ﬁltering operator can be formulated as a differentiable module  which
can be optimized by the backpropagation algorithm in an end-to-end manner.

2.2 Efﬁcient Implementation

Let N denotes the number of vertices in the tree G. The tree ﬁltering module needs to be accumulated
N times for each output vertex. For each channel  the computational complexity of brute force imple-
mentation is O(N 2) that is prohibitive for practical applications. Deﬁnition of the tree determines
the absence of loop among the connections of vertices. According to this property  a well-designed
dynamic programming algorithm can be used to speed up the optimization and inference process.
We introduce two sequential passes  namely aggregation and propagation  which are performed by
traversing the tree structure. Let one vertex to be the root node. In the aggregation pass  the process is
traced from the leaf nodes to the root node in the tree. For a vertex  its features do not update until all
the children have been visited. In the propagation pass  the features will propagate from the updated

3

vertex to their children recursively.

Aggr(ξ)i = ξi +

S (Ei j) Aggr(ξ)j.

(cid:88)
(cid:26) Aggr(ξ)r
S(cid:0)Epar(i) i

par(j)=i

Prop(ξ)i =

(cid:1) Prop(ξ)par(i) +(cid:0)1 − S2(cid:0)Ei par(i)

(cid:1)(cid:1) Aggr(ξ)i)

i = r
i (cid:54)= r

(6)

(7)

The sequential passes can be formulated into the recursive operators for the input ξ: the aggregation
pass and the propagation pass are respectively illustrated in Eq. 6 and Eq. 7  where par(i) indicates
the parent of vertex i in the tree whose root is vertex r. Prop(ξ)r is initialized from the updated value
Aggr(ξ)r of root vertex r.

Algorithm 1 Linear time algorithm for Learnable Tree Filter
Input: Tree G ∈ N(N−1)×2; Input feature x ∈ RC×N ; Pairwise distance ω ∈ RN ; Gradient of loss

w.r.t. output feature φ ∈ RC×N ; channel C  vertex N; Set of vertices Ω.

Output: Output feature y; Gradient of loss w.r.t. input feature ∂loss
Preparation:

∂x   w.r.t. pairwise distance ∂loss
∂ω .
(cid:46) Root vertex sampled with uniform distribution
(cid:46) Breadth-ﬁrst topological order for Aggr and Prop
(cid:46) All-ones matrix for normalization coefﬁcient

Forward:

r = Uniform(Ω)
T = BFS(G  r)
J = 1 ∈ R1×N
1. { ˆρ  ˆz} = Aggr({f (x)  J})
2. {ρ  z} = Prop({ ˆρ  ˆz})
3. y = ρ/z

Backward:

1. { ˆψ  ˆν} = Aggr({φ/z  φ · y/z})
2. {ψ  ν} = Prop({ ˆψ  ˆν})

· ψ

x

∂x = ∂f (x)
3. ∂loss
4. for i ∈ T\r do
j = par(i)
i = ˆψi · ρi + ψi · ˆρi − 2S(Ei j) ˆψi · ˆρi
γs
i = ˆνizi + νi ˆzi − 2S(Ei j) ˆνi ˆzi
γz

(cid:80) (γs

i − γz
i )

∂loss
∂ωi j

= ∂S(Ei j )

∂ωi j

end

(cid:46) Aggregation from leaves to root

(cid:46) Propagation from root to leaves

(cid:46) Normalized output feature

(cid:46) Aggregation from leaves to root

(cid:46) Propagation from root to leaves

(cid:46) Gradient of loss w.r.t. input feature

(cid:46) Parent of vertex i

(cid:46) Gradient of unnormalized output feature
(cid:46) Gradient of normalization coefﬁcient

(cid:46) Gradient of loss w.r.t. pairwise distance

As shown in the algorithm 1  we propose a linear-time algorithm for the tree ﬁltering module  whose
proofs are provided in the appendix. In the preparation stage  we uniformly sample a vertex as the
root and perform breadth-ﬁrst sorting (BFS) algorithm to obtain the topological order of tree G. The
BFS algorithm can be accelerated by the parallel version on GPUs and ensure the efﬁciency of the
following operations.
To compute the normalization coefﬁcient  we construct an all-ones matrix as J. Since the embedded
feature f (x) is independent of the matrix J  the forward computation can be factorized into two
dynamic programming processes. Furthermore  we propose an efﬁcient implementation for the
backward process. To reduce the unnecessary intermediate process  we combine the gradient of
module and loss function. Note that the output y and normalization coefﬁcient z have been already
computed in the inference phase. Thus the key of the backward process is to compute the intermediate
variables ψ and ν. The computation of these variables can be accelerated by the proposed linear time
algorithm. And we adopt another iteration process for the gradient of pairwise distance.

4

Figure 2: Overview of the proposed framework for semantic segmentation. The network is composed
of a backbone encoder and a naive decoder. GAP denotes the extra global average pooling block.
Multi-groups means using different feature splits to generate multiple groups of tree weights. The
right diagram elaborates on the process details of a single stage tree ﬁltering module  denoted as the
green node in the decoder. Red arrows represent upsample operations. Best viewed in color.

Computational complexity. Since the number of batch and channel is much smaller than the
vertices in the input feature  we only consider the inﬂuence of the vertices. For each channel  the
computational complexity of all the processes in algorithm 1  including the construction process of
MST and the computation of pairwise distance  is O(N )  which is linearly dependent on the number
of vertices. It is necessary to point out that MST can be built in linear time using Contractive Bor˚uvka
algorithm if given a planar graph  as is designed in this paper. Note that the batches and channels
are independent of each other. For the practical implementation on GPUs  we can naturally perform
the algorithm for batches and channels in parallel. Also  we adopt an effective scheduling scheme
to compute vertices of the same depth on the tree parallelly. Consequently  the proposed algorithm
reduces the computational complexity and time consumption dramatically.

2.3 Network Architecture for Segmentation

Based on the efﬁcient implementation algorithm  the proposed tree ﬁltering module can be easily
embedded into deep neural networks for resource-friendly feature aggregation. To illustrate the
effectiveness of the proposed module  here we employ ResNet [8] as our encoder to build up a uniﬁed
network. The encoded features from ResNet are usually computed with output stride 32. To remedy
for the resolution damage  we design a naive decoder module following previous works [14  15].
In details  the features in the decoder are gradually upsampled by a factor of 2 and summed with
corresponding low-level features in the encoder  similar to that in FPN [25]. After that  the bottom-
up embedding functions in the decoder are replaced by tree ﬁlter modules for multi-scale feature
transform  as intuitively illustrated in Fig. 2.
To be more speciﬁc  given a low-level feature map Ml in top-down pathway  which riches in instance
details  a 4-connected planar graph can be constructed easily with the guidance of Ml. Then  the
edges with substantial dissimilarity are removed to formulate MST using the Bor˚uvka [26] algorithm.
High level semantic cues contained in Xl are extracted using a simpliﬁed embedding function (Conv
1 × 1). To measure the pairwise dissimilarity ω in Eq. 2 (ωl in Fig. 2)  the widely used Euclidean
distance [27] is adopted. Furthermore  different groups of tree weights ωl are generated to capture
component dependent features  which will be analyzed in Sec 3.2. To highlight the effectiveness
of the proposed method  the feature transformation f (·) is simpliﬁed to identity mapping  where
f (Xl) = Xl. Thus  the learnable tree ﬁlter can be formulated by the algorithm elaborated on
Sec. 2.1. Finally  the low-level feature map Ml is fused with the operation output yl using pixel-wise
summation. For multi-stage feature aggregation  the building blocks (green nodes in Fig. 1) are
applied to different resolutions (Stage 1 to 3 in Fig. 1). An extra global average pooling operation is
added to capture global context and construct another tree ﬁltering module (Stage 4). The promotion
brought by the extra components will be detailed discussed in ablation studies.

5

3 Experiments

In this section  we ﬁrstly describe the implementation details. Then the proposed approach will
be decomposed step-by-step to reveal the effect of each component. Comparisons with several
state-of-the-art benchmarks on PASCAL VOC 2012 [23] and Cityscapes [24] are presented at last.

3.1

Implementation Details

Following traditional protocols [3  15  28]  ResNet [8] is adopted as our backbone for the following
experiments. Speciﬁcally  we employ the “poly” schedule with an initial learning rate 0.004 and
power 0.9. The networks are optimized for 40K iterations using mini-batch stochastic gradient
descent (SGD) with a weight decay of 1e-4 and a momentum of 0.9. We construct each mini-batch
for training from 32 random crops (512 × 512 for PASCAL VOC 2012 [23] and 800 × 800 for
Cityscapes [24]) after randomly ﬂipping and scaling each image by 0.5 to 2.0×.

3.2 Ablation Studies

To elaborate on the effectiveness of the proposed approach  we conduct extensive ablation studies.
First  we give detailed structure-preserving relation analysis as well as the visualization  as presented
in Fig. 3. Next  the equipped stage and group number of the tree ﬁltering module is explored. Different
building blocks are compared to illustrate the effectiveness and efﬁciency of the tree ﬁltering module.
Structure-preserving relations. As intuitively presented in the ﬁrst row of Fig. 3  given different
positions  the corresponding instance details are fully activated with the high response  which means
that the proposed module has learned object structures and the long-range intra-class dependencies.
Speciﬁcally  the object details (e.g.  boundaries of the train rather than the coarse regions in Non-
local [13] blocks) have been highlighted in the afﬁnity maps. Qualitative results are also given to
illustrate the preserved structural details  as presented in Fig. 4.
Which stage to equip the module? Tab. 1 presents the results when applying the tree ﬁltering
module to different stages (group number is ﬁxed to 1). The convolutional operations are replaced by
the building block (green nodes in Fig. 2) to form the equipped stage. As can be seen from Tab. 1  the

Figure 3: Visualization of afﬁnity maps in the speciﬁc position (marked by the red cross in each input
image). TF and NL denotes using the proposed Tree Filtering module and Non-local block [13] 
respectively. Different positions  resolution stages  and selected groups are explored in (a)  (b)  and
(c)  respectively. Our approach preserves more detailed structures than Non-local block. All of the
input images are sampled from PASCAL VOC 2012 val set.

6

(a) Affinity maps of different anchorsTF: Left TF: RightInputNL: Left NL: RightTF: Left TF: RightInputNL: Left NL: Right(a) Affinity maps of different anchorsTF: Left TF: RightInputNL: Left NL: Right(b) Affinity maps of different stagesInputTF: Stage 1TF: Stage 2TF: Stage 3NL(b) Affinity maps of different stagesInputTF: Stage 1TF: Stage 2TF: Stage 3NL(c) Affinity maps of different groupsInputNLTF: Group 1TF: Group 2TF: Group 3(c) Affinity maps of different groupsInputNLTF: Group 1TF: Group 2TF: Group 3Figure 4: Qualitative results on PASCAL VOC 2012 val set. Given an input image from the top row 
the structural cues are preserved in the corresponding prediction (the middle row). The generated
results contains rich details even compared with its ground truth in the bottom row.

network performance consistently improves with more tree ﬁltering modules equipped. This can also
be concluded from the qualitative results (the second row in Fig. 3)  where the higher stage (Stage 3)
contains more semantic cues and lower stages (Stage 1 and 2) focus more on complementary details.
The maximum gap between the multi-stage equipped network and the raw one even up to 4.5% based
on ResNet-50 backbone. Even with the powerful ResNet-101  the proposed approach still attains a
2.1% absolute gain  reaching 77.1% mIoU on PASCAL VOC 2012 val set.
Different group numbers. Different group settings are used to generate weights for the single-stage
tree ﬁltering module when ﬁxing the channel number to 256. As shown in Tab. 2  the network reaches
top performance (Group Num=16) when the group number approaching the category number (21 in
PASCAL VOC 2012)  and additional groups afford no more contribution. We guess the reason is that
different kinds of tree weights are needed to deal with similar but different components  as shown in
the third row of Fig. 3.
Different building blocks. We further compare the proposed tree ﬁltering module with classic
context modeling blocks (e.g.  PSP block [3] and Non-local block [13]) and prove the superiority
both in accuracy and efﬁciency. As illustrated in Tab. 3  the proposed module (TF) achieves better
performance than others (PSP and NL) with much less resource consumption. What’s more  the tree
ﬁltering module brings consistent improvements in different backbones (5.2% for ResNet-50 with
stride 32 and 2.6% for ResNet-101) with additional 0.7M parameters and 1.3G FLOPs overheads.
Due to the structural-preserving property  the proposed module achieves additional 1.1% improvement
over the PSP block with neglectable consumption  as presented in Tab. 3. And the extra Non-local
block contributes no more gain over the tree ﬁltering module  which could be attributed to the already
modeled feature dependencies in the tree ﬁlter.
Extra components. Same with other works [15  28]  we adopt some simple components for further
improvements  including an extra global average pooling operation and additional ResBlocks in the
decoder. In details  the global average pooling block combined with the Stage 4 module (see Fig. 2)
is added after the backbone to capture global context  and the “Conv1×1” operations in the decoder
(refer to the detailed diagram in Fig. 2) are replaced by ResBlocks (with “Conv3x3”). As presented
in Tab. 4  the proposed method achieves consistent improvements and attains 79.4% on PASCAL
VOC 2012 val set without applying data augmentation strategies.

3.3 Experiments on Cityscapes

To further illustrate the effectiveness of the proposed method  we evaluate the Cityscapes [24] dataset.
Our experiments involve the 2975  500  1525 images in train  val  and test set  respectively. With
multi-scale and ﬂipping strategy  the proposed method achieves 80.8% mIoU on Cityscapes test set

7

Table 1: Comparisons among different stages to
equip tree ﬁltering module on PASCAL VOC
2012 val set when using the proposed decoder
architecture. Multi-scale and ﬂipping strategy
are adopted for testing.

Table 2: Comparisons among different group
settings of tree ﬁltering module on PASCAL
VOC 2012 val set when using the proposed de-
coder structure. Multi-scale and ﬂipping strat-
egy are adopted for testing.

Backbone

ResNet-50

ResNet-101

Stage
None

+ Stage 1
+ Stage 1-2
+ Stage 1-3

None

+ Stage 1-3

mIoU (%)

Backbone

Group Num mIoU (%)

70.2
72.1
73.1
74.7
75.0
77.1

ResNet-50

0
1
4
8
16
32

70.2
72.1
73.2
74.0
74.4
74.4

Table 3: Comparisons among different building blocks on PASCAL VOC 2012 val set when using
ResNet-50 as feature extractor without decoder. TF  PSP  and NL denotes using the proposed Tree
Filtering module  PSP block [3]  and Non-local block [13] as the building block  respectively. OS
represents the output stride used in the backbone. We calculate FLOPs when given a single scale
512 × 512 input. All of the data augmentation strategies are dropped.

Backbone

ResNet-50

ResNet-50

ResNet-101

Block
None
NL
PSP
TF

NL+TF
PSP+TF

None
TF
None
TF








Decoder OS
8
8
8
8
8
8
32
32
32
32







Params (M)

FLOPs (G) ∆ FLOPs (G) mIoU (%)

129.3
158.3
178.3
133.3
162.3
182.3
102.0
102.7
175.0
175.7

162.1
199.1
171.1
163.1
200.1
172.1
39.2
40.5
65.0
66.3

0.0
+37.0
+9.0
+1.0
+38.0
+10.0
0.0
+1.3
0.0
+1.3

69.2
74.2
74.3
74.9
74.9
75.4
67.3
72.5
72.8
75.4

Table 4: Comparisons among different compo-
nents on PASCAL VOC 2012 val set. TF(multi)
denotes multi-stage Tree Filtering modules with
decoder. Extra represents extra components. All
of the data augmentation strategies are dropped.

Backbone

TF(multi) Extra mIoU (%)

ResNet-50

ResNet-101

















67.3
72.5
72.5
75.6
78.3
79.4

Table 5: Comparisons with state-of-the-arts results
on Cityscapes test set trained on ﬁne annotation. We
adopt vanilla ResNet-101 as our backbone.

Backbone
Method
ResNet-101
ReﬁneNet [29]
ResNet-101
DSSPN [30]
ResNet-101
PSPNet [3]
ResNet-101
BiSeNet [31]
ResNet-101
DFN [28]
PSANet [32]
ResNet-101
DenseASPP [33] DenseNet-161
Ours
ResNet-101

mIoU (%)

73.6
77.8
78.4
78.9
79.3
80.1
80.6
80.8

when trained on ﬁne annotation data only. Compared with previous leading algorithms  our method
achieves superior performance using ResNet-101 without bells-and-whistles.

3.4 Experiments on PASCAL VOC

We carry out experiments on PASCAL VOC 2012 [23] that contains 20 object categories and one
background class. Following the procedure of [28]  we use the augmented data [38] with 10 582
images for training and raw train-val set for further ﬁne-tuning. In inference stage  multi-scale and
horizontally ﬂipping strategy are adopted for data augmentation. As shown in Tab. 6  the proposed
method achieves the state-of-the-art performance on PASCAL VOC 2012 [23] test set. In details  our

8

Table 6: Comparisons with state-of-the-arts results on PASCAL VOC 2012 test set. We adopt vanilla
ResNet-101 without atrous convolutions as our backbone.

without MS-COCO pretrain

with MS-COCO pretrain

Method
FCN [1]
Deeplab v2 [2]
DPN [34]
Piecewise [35]
PSPNet [3]
DFN [28]
EncNet [4]
Ours

Backbone
VGG 16
VGG 16
VGG 16
VGG 16

ResNet-101
ResNet-101
ResNet-101
ResNet-101

mIoU (%)

62.2
71.6
74.1
75.3
82.6
82.7
82.9
84.2

Backbone
Method
ResNet-152
GCN [36]
ResNet-101
ReﬁneNet [29]
PSPNet [3]
ResNet-101
Deeplab v3 [14] ResNet-101
EncNet [4]
ResNet-101
ResNet-101
DFN [28]
ResNet-101
ExFuse [37]
Ours
ResNet-101

mIoU (%)

82.2
84.2
85.4
85.7
85.9
86.2
86.2
86.3

approach reaches 84.2% mIoU without MS-COCO [39] pre-train when adopting vanilla ResNet-101
as our backbone. If MS-COCO [39] is added for pre-training  our approach achieves the leading
performance with 86.3% mIoU.

4 Conclusion

In this work  we propose the learnable tree ﬁlter for structure-preserving feature transform. Different
from most existing methods  the proposed approach leverages tree-structured graph for long-range
dependencies modeling while preserving detailed object structures. We formulate the tree ﬁltering
module and give an efﬁcient implementation with linear-time source consumption. Extensive ablation
studies have been conducted to elaborate on the effectiveness and efﬁciency of the proposed method 
which is proved to bring consistent improvements on different backbones with little computational
overhead. Experiments on PASCAL VOC 2012 and Cityscapes prove the superiority of the proposed
approach on semantic segmentation. More potential domains with structure relations (e.g.  detection
and instance segmentation) remain to be explored in the future.

5 Acknowledgment

We would like to thank Lingxi Xie for his valuable suggestions. This research was supported by the
National Key R&D Program of China (No. 2017YFA0700800).

9

References
[1] Jonathan Long  Evan Shelhamer  and Trevor Darrell. Fully convolutional networks for semantic segmenta-

tion. In IEEE Conference on Computer Vision and Pattern Recognition  2015.

[2] Liang-Chieh Chen  George Papandreou  Iasonas Kokkinos  Kevin Murphy  and Alan L Yuille. Deeplab:
Semantic image segmentation with deep convolutional nets  atrous convolution  and fully connected crfs.
IEEE Transactions on Pattern Analysis and Machine Intelligence  2018.

[3] Hengshuang Zhao  Jianping Shi  Xiaojuan Qi  Xiaogang Wang  and Jiaya Jia. Pyramid scene parsing

network. In IEEE Conference on Computer Vision and Pattern Recognition  2017.

[4] Hang Zhang  Kristin Dana  Jianping Shi  Zhongyue Zhang  Xiaogang Wang  Ambrish Tyagi  and Amit
Agrawal. Context encoding for semantic segmentation. In IEEE Conference on Computer Vision and
Pattern Recognition  2018.

[5] Yanwei Li  Xinze Chen  Zheng Zhu  Lingxi Xie  Guan Huang  Dalong Du  and Xingang Wang. Attention-
guided uniﬁed network for panoptic segmentation. In IEEE Conference on Computer Vision and Pattern
Recognition  2019.

[6] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. In International Conference on Learning Representations  2014.

[7] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov  Dumitru
Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions. In IEEE Conference
on Computer Vision and Pattern Recognition  2015.

[8] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In IEEE Conference on Computer Vision and Pattern Recognition  2016.

[9] Liang-Chieh Chen  George Papandreou  Iasonas Kokkinos  Kevin Murphy  and Alan L Yuille. Semantic
image segmentation with deep convolutional nets and fully connected crfs. In International Conference on
Learning Representations  2015.

[10] Wenjie Luo  Yujia Li  Raquel Urtasun  and Richard Zemel. Understanding the effective receptive ﬁeld in

deep convolutional neural networks. In Advances in Neural Information Processing Systems  2016.

[11] Jifeng Dai  Haozhi Qi  Yuwen Xiong  Yi Li  Guodong Zhang  Han Hu  and Yichen Wei. Deformable

convolutional networks. In IEEE International Conference on Computer Vision  2017.

[12] Xizhou Zhu  Han Hu  Stephen Lin  and Jifeng Dai. Deformable convnets v2: More deformable  better

results. In IEEE Conference on Computer Vision and Pattern Recognition  2019.

[13] Xiaolong Wang  Ross Girshick  Abhinav Gupta  and Kaiming He. Non-local neural networks. In IEEE

Conference on Computer Vision and Pattern Recognition  2018.

[14] Liang-Chieh Chen  George Papandreou  Florian Schroff  and Hartwig Adam. Rethinking atrous convolution

for semantic image segmentation. arXiv preprint arXiv:1706.05587  2017.

[15] Liang-Chieh Chen  Yukun Zhu  George Papandreou  Florian Schroff  and Hartwig Adam. Encoder-decoder
with atrous separable convolution for semantic image segmentation. In European Conference on Computer
Vision  2018.

[16] Xiaodan Liang  Liang Lin  Xiaohui Shen  Jiashi Feng  Shuicheng Yan  and Eric P Xing. Interpretable

structure-evolving lstm. In IEEE Conference on Computer Vision and Pattern Recognition  2017.

[17] Yin Li and Abhinav Gupta. Beyond grids: Learning graph representations for visual recognition. In

Advances in Neural Information Processing Systems  2018.

[18] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint

graphs. In IEEE Conference on Computer Vision and Pattern Recognition  2018.

[19] Qingxiong Yang. Stereo matching using tree ﬁltering. IEEE Transactions on Pattern Analysis and Machine

Intelligence  2015.

[20] Siddhartha Chandra  Nicolas Usunier  and Iasonas Kokkinos. Dense and low-rank gaussian crfs using deep

embeddings. In IEEE International Conference on Computer Vision  2017.

[21] Adam W Harley  Konstantinos G Derpanis  and Iasonas Kokkinos. Segmentation-aware convolutional

networks using local attention masks. In IEEE International Conference on Computer Vision  2017.

10

[22] Sifei Liu  Shalini De Mello  Jinwei Gu  Guangyu Zhong  Ming-Hsuan Yang  and Jan Kautz. Learning
afﬁnity via spatial propagation networks. In Advances in Neural Information Processing Systems  2017.

[23] Mark Everingham  Luc Van Gool  Christopher KI Williams  John Winn  and Andrew Zisserman. The

pascal visual object classes (voc) challenge. International Journal of Computer Vision  2010.

[24] Marius Cordts  Mohamed Omran  Sebastian Ramos  Timo Rehfeld  Markus Enzweiler  Rodrigo Benenson 
Uwe Franke  Stefan Roth  and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.
In IEEE Conference on Computer Vision and Pattern Recognition  2016.

[25] Tsung-Yi Lin  Piotr Dollár  Ross Girshick  Kaiming He  Bharath Hariharan  and Serge Belongie. Feature
pyramid networks for object detection. In IEEE Conference on Computer Vision and Pattern Recognition 
2017.

[26] Robert G Gallager  Pierre A Humblet  and Philip M Spira. A distributed algorithm for minimum-weight

spanning trees. ACM Transactions on Programming Languages and systems  1983.

[27] Feng Wang  Xiang Xiang  Jian Cheng  and Alan Loddon Yuille. Normface: l 2 hypersphere embedding for

face veriﬁcation. In ACM International Conference on Multimedia  2017.

[28] Changqian Yu  Jingbo Wang  Chao Peng  Changxin Gao  Gang Yu  and Nong Sang. Learning a discrimi-
native feature network for semantic segmentation. In IEEE Conference on Computer Vision and Pattern
Recognition  2018.

[29] Guosheng Lin  Anton Milan  Chunhua Shen  and Ian Reid. Reﬁnenet: Multi-path reﬁnement networks for
high-resolution semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition 
2017.

[30] Xiaodan Liang  Hongfei Zhou  and Eric Xing. Dynamic-structured semantic propagation network. In

IEEE Conference on Computer Vision and Pattern Recognition  2018.

[31] Changqian Yu  Jingbo Wang  Chao Peng  Changxin Gao  Gang Yu  and Nong Sang. Bisenet: Bilateral
segmentation network for real-time semantic segmentation. In European Conference on Computer Vision 
2018.

[32] Hengshuang Zhao  Yi Zhang  Shu Liu  Jianping Shi  Chen Change Loy  Dahua Lin  and Jiaya Jia. Psanet:
Point-wise spatial attention network for scene parsing. In European Conference on Computer Vision  2018.

[33] Maoke Yang  Kun Yu  Chi Zhang  Zhiwei Li  and Kuiyuan Yang. Denseaspp for semantic segmentation in

street scenes. In IEEE Conference on Computer Vision and Pattern Recognition  2018.

[34] Ziwei Liu  Xiaoxiao Li  Ping Luo  Chen-Change Loy  and Xiaoou Tang. Semantic image segmentation via

deep parsing network. In IEEE International Conference on Computer Vision  2015.

[35] Guosheng Lin  Chunhua Shen  Anton Van Den Hengel  and Ian Reid. Efﬁcient piecewise training of
deep structured models for semantic segmentation. In IEEE Conference on Computer Vision and Pattern
Recognition  2016.

[36] Chao Peng  Xiangyu Zhang  Gang Yu  Guiming Luo  and Jian Sun. Large kernel matters–improve semantic
segmentation by global convolutional network. In IEEE Conference on Computer Vision and Pattern
Recognition  2017.

[37] Zhenli Zhang  Xiangyu Zhang  Chao Peng  Xiangyang Xue  and Jian Sun. Exfuse: Enhancing feature

fusion for semantic segmentation. In European Conference on Computer Vision  2018.

[38] Bharath Hariharan  Pablo Arbeláez  Lubomir Bourdev  Subhransu Maji  and Jitendra Malik. Semantic

contours from inverse detectors. In IEEE International Conference on Computer Vision  2011.

[39] Tsung-Yi Lin  Michael Maire  Serge Belongie  James Hays  Pietro Perona  Deva Ramanan  Piotr Dollár 
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on
Computer Vision  2014.

11

,Tamir Hazan
Subhransu Maji
Joseph Keshet
Tommi Jaakkola
Lin Song
Yanwei Li
Zeming Li
Gang Yu
Hongbin Sun
Jian Sun
Nanning Zheng