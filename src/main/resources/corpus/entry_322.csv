2009,Accelerated Gradient Methods for Stochastic Optimization and Online Learning,Regularized risk minimization often involves non-smooth optimization  either because of the loss function (e.g.  hinge loss) or the regularizer (e.g.  $\ell_1$-regularizer). Gradient descent methods  though highly scalable and easy to implement  are known to converge slowly on these problems. In this paper  we develop novel accelerated gradient methods for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm  called SAGE (Stochastic Accelerated GradiEnt)  exhibits fast convergence rates on stochastic optimization with both convex and strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS  SMIDAS and SCD. Moreover  SAGE can also be extended for online learning  resulting in a simple but powerful algorithm.,Accelerated Gradient Methods for

Stochastic Optimization and Online Learning

Chonghai Hu♯†  James T. Kwok♯  Weike Pan♯
♯ Department of Computer Science and Engineering
Hong Kong University of Science and Technology

Clear Water Bay  Kowloon  Hong Kong

† Department of Mathematics  Zhejiang University

Hangzhou  China

hino.hu@gmail.com  {jamesk weikep}@cse.ust.hk

Abstract

Regularized risk minimization often involves non-smooth optimization  either be-
cause of the loss function (e.g.  hinge loss) or the regularizer (e.g.  ℓ1-regularizer).
Gradient methods  though highly scalable and easy to implement  are known to
converge slowly. In this paper  we develop a novel accelerated gradient method
for stochastic optimization while still preserving their computational simplicity
and scalability. The proposed algorithm  called SAGE (Stochastic Accelerated
GradiEnt)  exhibits fast convergence rates on stochastic composite optimization
with convex or strongly convex objectives. Experimental results show that SAGE
is faster than recent (sub)gradient methods including FOLOS  SMIDAS and SCD.
Moreover  SAGE can also be extended for online learning  resulting in a simple
algorithm but with the best regret bounds currently known for these problems.

1

Introduction

Risk minimization is at the heart of many machine learning algorithms. Given a class of models
parameterized by w and a loss function ℓ(· ·)  the goal is to minimize EXY [ℓ(w; X  Y )] w.r.t. w 
where the expectation is over the joint distribution of input X and output Y . However  since the joint
distribution is typically unknown in practice  a surrogate problem is to replace the expectation by
its empirical average on a training sample {(x1  y1)  . . .   (xm  ym)}. Moreover  a regularizer Ω(·)
is often added for well-posedness. This leads to the minimization of the regularized risk

min

w

1
m

m

Xi=1

ℓ(w; xi  yi) + λΩ(w) 

(1)

where λ is a regularization parameter. In optimization terminology  the deterministic optimization
problem in (1) can be considered as a sample average approximation (SAA) of the corresponding
stochastic optimization problem:

min

w

EXY [ℓ(w; X  Y )] + λΩ(w).

(2)

Since both ℓ(· ·) and Ω(·) are typically convex  (1) is a convex optimization problem which can be
conveniently solved even with standard off-the-shelf optimization packages.

However  with the proliferation of data-intensive applications in the text and web domains  data sets
with millions or trillions of samples are nowadays not uncommon. Hence  off-the-shelf optimization
solvers are too slow to be used. Indeed  even tailor-made softwares for speciﬁc models  such as the
sequential minimization optimization (SMO) method for the SVM  have superlinear computational

1

complexities and thus are not feasible for large data sets. In light of this  the use of stochastic meth-
ods have recently drawn a lot of interest and many of these are highly successful. Most are based
on (variants of) the stochastic gradient descent (SGD). Examples include Pegasos [1]  SGD-QN [2] 
FOLOS [3]  and stochastic coordinate descent (SCD) [4]. The main advantages of these methods
are that they are simple to implement  have low per-iteration complexity  and can scale up to large
data sets. Their runtime is independent of  or even decrease with  the number of training samples
[5  6]. On the other hand  because of their simplicity  these methods have a slow convergence rate 
and thus may require a large number of iterations.

While standard gradient schemes have a slow convergence rate  they can often be “accelerated”.
This stems from the pioneering work of Nesterov in 1983 [7]  which is a deterministic algorithm
for smooth optimization. Recently  it is also extended for composite optimization  where the objec-
tive has a smooth component and a non-smooth component [8  9]. This is particularly relevant to
machine learning since the loss ℓ and regularizer Ω in (2) may be non-smooth. Examples include
loss functions such as the commonly-used hinge loss used in the SVM  and regularizers such as the
popular ℓ1 penalty in Lasso [10]  and basis pursuit. These accelerated gradient methods have also
been successfully applied in the optimization problems of multiple kernel learning [11] and trace
norm minimization [12]. Very recently  Lan [13] made an initial attempt to further extend this for
stochastic composite optimization  and obtained the convergence rate of

O(cid:16)L/N 2 + (M + σ)/√N(cid:17) .

(3)
Here  N is the number of iterations performed by the algorithm  L is the Lipschitz parameter of
the gradient of the smooth term in the objective  M is the Lipschitz parameter of the nonsmooth
term  and σ is the variance of the stochastic subgradient. Moreover  note that the ﬁrst term of (3)
is related to the smooth component in the objective while the second term is related to the non-
smooth component. Complexity results [14  13] show that (3) is the optimal convergence rate for
any iterative algorithm solving stochastic (general) convex composite optimization.

However  as pointed out in [15]  a very useful property that can improve the convergence rates in ma-
chine learning optimization problems is strong convexity. For example  (2) can be strongly convex
either because of the strong convexity of ℓ (e.g.  log loss  square loss) or Ω (e.g.  ℓ2 regularization).
On the other hand  [13] is more interested in general convex optimization problems and so strong
convexity is not utilized. Moreover  though theoretically interesting  [13] may be of limited practi-
cal use as (1) the stepsize in its update rule depends on the often unknown σ; and (2) the number of
iterations performed by the algorithm has to be ﬁxed in advance.

Inspired by the successes of Nesterov’s method  we develop in this paper a novel accelerated sub-
gradient scheme for stochastic composite optimization. It achieves the optimal convergence rate

of O(cid:16)L/N 2 + σ/√N(cid:17) for general convex objectives  and O(cid:0)(L + µ)/N 2 + σµ−1/N(cid:1) for µ-
ing. We obtain O(√N ) regret for general convex problems and O(log N ) regret for strongly convex

strongly convex objectives. Moreover  its per-iteration complexity is almost as low as that for stan-
dard (sub)gradient methods. Finally  we also extend the accelerated gradient scheme to online learn-

problems  which are the best regret bounds currently known for these problems.

2 Setting and Mathematical Background

First  we recapitulate a few notions in convex analysis.
(Lipschitz continuity) A function f (x) is L-Lipschitz if kf (x) − f (y)k ≤ Lkx − yk.
Lemma 1. [14] The gradient of a differentiable function f (x) is Lipschitz continuous with Lipschitz
parameter L if  for any x and y 

f (y) ≤ f (x) + h∇f (x)  y − xi +

L
2 kx − yk2.

(4)

(Strong convexity) A function φ(x) is µ-strongly convex if φ(y) ≥ φ(x)+hg(x)  y−xi+ µ
for any x  y and subgradient g(x) ∈ ∂φ(x).
Lemma 2. [14] Let φ(x) be µ-strongly convex and x∗ = arg minx φ(x). Then  for any x 

2ky−xk2

µ
2 kx − x∗k2.

φ(x) ≥ φ(x∗) +

2

(5)

We consider the following stochastic convex stochastic optimization problem  with a composite
objective function

min
x {φ(x) ≡ E[F (x  ξ)] + ψ(x)} 

(6)
where ξ is a random vector  f (x) ≡ E[F (x  ξ)] is convex and differentiable  and ψ(x) is convex
but non-smooth. Clearly  this includes the optimization problem (2). Moreover  we assume that the
gradient of f (x) is L-Lipschitz and φ(x) is µ-strongly convex (with µ ≥ 0). Note that when φ(x) is
smooth (ψ(x) = 0)  µ lower bounds the smallest eigenvalue of its Hessian.
Recall that in smooth optimization  the gradient update xt+1 = xt − λ∇f (xt) on a function f (x)
can be seen as proximal regularization of the linearized f at the current iterate xt [16]. In other
words  xt+1 = arg minx(h∇f (xt)  x − xti + 1
2λkx − xtk2). With the presence of a non-smooth
component  we have the following more general notion.
(Gradient mapping) [8] In minimizing f (x) + ψ(x)  where f is convex and differentiable and ψ is
convex and non-smooth 

xt+1 = arg min

x (cid:18)h∇f (x)  x − xti +

1

2λkx − xtk2 + ψ(x)(cid:19)

(7)

is called the generalized gradient update  and δ = 1
λ (xt − xt+1) is the (generalized) gradient map-
ping. Note that the quadratic approximation is made to the smooth component only. It can be shown
that the gradient mapping is analogous to the gradient in smooth convex optimization [14  8]. This
is also a common construct used in recent stochastic subgradient methods [3  17].

3 Accelerated Gradient Method for Stochastic Learning

Let G(xt  ξt) ≡ ∇xF (x  ξt)|x=xt be the stochastic gradient of F (x  ξt). We assume that it is an
unbiased estimator of the gradient ∇f (x)  i.e.  Eξ[G(x  ξ)] = ∇f (x). Algorithm 1 shows the pro-
It involves the
posed algorithm  which will be called SAGE (Stochastic Accelerated GradiEnt).
updating of three sequences {xt}  {yt} and {zt}. Note that yt is the generalized gradient update 
and xt+1 is a convex combination of yt and zt. The algorithm also maintains two parameter se-
quences {αt} and {Lt}. We will see in Section 3.1 that different settings of these parameters lead
to different convergence rates. Note that the only expensive step of Algorithm 1 is the computation
of the generalized gradient update yt  which is analogous to the subgradient computation in other
subgradient-based methods. In general  its computational complexity depends on the structure of
ψ(x). As will be seen in Section 3.3  this can often be efﬁciently obtained in many regularized risk
minimization problems.

Algorithm 1 SAGE (Stochastic Accelerated GradiEnt).

Input: Sequences {Lt} and {αt}.
Initialize: y−1 = z−1 = 0  α0 = λ0 = 1. L0 = L + µ.
for t = 0 to N do

xt = (1 − αt)yt−1 + αtzt−1.
yt = arg minx(cid:8)hG(xt  ξt)  x − xti + Lt
zt = zt−1 − (Ltαt + µ)−1[Lt(xt − yt) + µ(zt−1 − xt)].

2 kx − xtk2 + ψ(x)(cid:9).

end for
Output yN .

3.1 Convergence Analysis

Deﬁne ∆t ≡ G(xt  ξt) − ∇f (xt). Because of the unbiasedness of G(xt  ξt)  Eξt [∆t] = 0. In the
following  we will show that the value of φ(yt) − φ(x) can be related to that of φ(yt−1) − φ(x) for
any x. Let δt ≡ Lt(xt − yt) be the gradient mapping involved in updating yt. First  we introduce
the following lemma.
Lemma 3. For t ≥ 0  φ(x) is quadratically bounded from below as

φ(x) ≥ φ(yt) + hδt  x − xti +

µ
2 kx − xtk2 + h∆t  yt − xi +

2Lt − L

2L2
t

kδtk2.

3

Proposition 1. Assume that for each t ≥ 0  k∆tk∗ ≤ σ and Lt > L  then

φ(yt) − φ(x) +

Ltα2

t + µαt
2

≤ (1 − αt)[φ(yt−1) − φ(x)] +

kx − ztk2
Ltα2
2 kx − zt−1k2 +
t

(8)

+ αth∆t  x − zt−1i.

σ2

2(Lt − L)
2 kx − zt−1k2.

Proof. Deﬁne Vt(x) = hδt  x − xti + µ
It is easy to see that
zt = arg minx∈Rd Vt(x). Moreover  notice that Vt(x) is (Ltαt + µ)-strongly convex. Hence on
applying Lemmas 2 and 3  we obtain that for any x 

2kx − xtk2 + Ltαt

Vt(zt) ≤ Vt(x) −
= hδt  x − xti +
≤ φ(x)−φ(yt)−

Ltαt + µ

2

kx − ztk2
Ltαt
2 kx − zt−1k2 −
Ltαt
2 kx−zt−1k2−

µ
2 kx − xtk2 +
2Lt−L
kδtk2 +
2L2
t

Ltαt + µ

Ltαt +µ

2

2

kx − ztk2
kx−ztk2 +h∆t  x−yti.

Then  φ(yt) can be bounded from above  as:

φ(yt) ≤φ(x) + hδt  xt − zti −
Ltαt
2 kx − zt−1k2 −

+

2Lt − L

2L2
t
Ltαt + µ

Ltαt
2 kzt − zt−1k2
kδtk2 −
kx − ztk2 + h∆t  x − yti 

2

(9)

where the non-positive term − µ
hand  by applying Lemma 3 with x = yt−1  we get

2kzt − xtk2 has been dropped from its right-hand-side. On the other

φ(yt) − φ(yt−1) ≤ hδt  xt − yt−1i + h∆t  yt−1 − yti −

2Lt − L

2L2
t

kδtk2 

(10)

2kyt−1 − xtk2 has also been dropped from the right-hand-side. On

where the non-positive term − µ
multiplying (9) by αt and (10) by 1 − αt  and then adding them together  we obtain
Ltα2
2 kzt− zt−1k2  (11)
t
φ(yt)− φ(x) ≤ (1− αt)[φ(yt−1)− φ(x)]−
where A = hδt  αt(xt − zt) + (1− αt)(xt − yt−1)i B = αth∆t  x− yti + (1− αt)h∆t  yt−1 − yti 
and C = Ltα2
kx − ztk2. In the following  we consider to upper bound A
and B. First  by using the update rule of xt in Algorithm 1 and the Young’s inequality1  we have

2 kx − zt−1k2 − Ltα2

kδtk2 +A +B +C−

2Lt − L

t +µαt
2

2L2
t

t

A = hδt  αt(xt − zt−1) + (1 − αt)(xt − yt−1)i + αthδt  zt−1 − zti

= αthδt  zt−1 − zti ≤
On the other hand  B can be bounded as

Ltα2
t

2 kzt − zt−1k2 + kδtk2

2Lt

.

(12)

(13)

B = h∆t  αtx + (1 − αt)yt−1 − xti + h∆t  xt − yti = αth∆t  x − zt−1i + h∆t  δti
≤ αth∆t  x − zt−1i +

σkδtk
Lt

Lt

 

where the second equality is due to the update rule of xt  and the last step is from the Cauchy-
Schwartz inequality and the boundedness of ∆t. Hence  plugging (12) and (13) into (11) 

φ(yt) − φ(x) ≤ (1−αt)[φ(yt−1)−φ(x)]−

(Lt−L)kδtk2

+

σkδtk
Lt

+ αth∆t  x−zt−1i + C

≤ (1 − αt)[φ(yt−1) − φ(x)] +

+ αth∆t  x − zt−1i + C 

where the last step is due to the fact that −ax2 + bx ≤ b2
obtain (8).

4a with a  b > 0. On re-arranging terms  we

2L2
t
σ2

2(Lt − L)

1The Young’s inequality states that hx  yi ≤ kxk2

2a

+ akyk2

2

for any a > 0.

4

Let the optimal solution in problem (6) be x∗. From the update rules in Algorithm 1  we observe
that the triplet (xt  yt−1  zt−1) depends on the random process ξ[t−1] ≡ {ξ0  . . .   ξt−1} and hence is
also random. Clearly  zt−1 and x∗ are independent of ξt. Thus 

Eξ[t]h∆t  x∗ − zt−1i = Eξ[t−1]

Eξ[t] [h∆t  x∗ − zt−1i|ξ[t−1]] = Eξ[t−1]

= Eξ[t−1]hx∗ − zt−1  Eξt [∆t]i = 0 

Eξt [h∆t  x∗ − zt−1i]

where the ﬁrst equality uses Ex[h(x)] = Ey Ex[h(x)|y]  and the last equality is from our assumption
that the stochastic gradient G(x  ξ) is unbiased. Taking expectations on both sides of (8) with x =
x∗  we obtain the following corollary  which will be useful in proving the subsequent theorems.
Corollary 1.

E[φ(yt)] − φ(x∗) +

Ltα2

t + µαt
2

E[kx∗ − ztk2]
Ltα2
t

≤ (1 − αt)(E[φ(yt−1)] − φ(x∗)) +

E[kx∗ − zt−1k2] +

2

σ2

2(Lt − L)

.

So far  the choice of Lt and αt in Algorithm 1 has been left unspeciﬁed. In the following  we will
show that with a good choice of Lt and αt  (the expectation of) φ(yt) converges rapidly to φ(x∗).
Theorem 1. Assume that E[kx∗ − ztk2] ≤ D2 for some D. Set
2

3

Lt = b(t + 1)

2 + L  αt =

 

t + 2

(14)

where b > 0 is a constant. Then the expected error of Algorithm 1 can be bounded as

E[φ(yN )] − φ(x∗) ≤

3D2L

N 2 +(cid:18)3D2b +

5σ2

3b (cid:19) 1
√N

.

(15)

.

3D   and the bound in (15) becomes 3D2L

If σ were known  we can set b to the optimal choice of √5σ
2√5σD√N
Note that so far φ(x) is only assumed to be convex. As is shown in the following theorem  the
convergence rate can be further improved by assuming strong convexity. This also requires another
setting of αt and Lt which is different from that in (14).
Theorem 2. Assume the same conditions as in Theorem 1  except that φ(x) is µ-strongly convex.
Set

N 2 +

Lt = L + µλ−1
t−1 

for t ≥ 1; αt =sλt−1 +

λ2
t−1
4 −

λt−1
2

 

for t ≥ 1 

(16)

where λt ≡ Πt
bounded as

k=1(1 − αt) for t ≥ 1 and λ0 = 1. Then  the expected error of Algorithm 1 can be
(17)

2(L + µ)D2

+

E[φ(yN )] − φ(x∗) ≤

N 2

6σ2
N µ

.

In comparison  FOLOS only converges as O(log(N )/N) for strongly convex objectives.
3.2 Remarks

As in recent studies on stochastic composite optimization [13]  the error bounds in (15) and (17) con-
sist of two terms: a faster term which is related to the smooth component and a slower term related to
the non-smooth component. SAGE beneﬁts from using the structure of the problem and accelerates
the convergence of the smooth component. On the other hand  many stochastic (sub)gradient-based
algorithms like FOLOS do not separate the smooth from the non-smooth part  but simply treat the
whole objective as non-smooth. Consequently  convergence of the smooth component is also slowed

down to O(1/√N).

As can be seen from (15) and (17)  the convergence of SAGE is essentially encumbered by the
variance of the stochastic subgradient. Recall that the variance of the average of p i.i.d. random

5

variables is equal to 1/p of the original variance. Hence  as in Pegasos [1]  σ can be reduced by
estimating the subgradient from a data subset.
Unlike the AC-SA algorithm in [13]  the settings of Lt and αt in (14) do not require knowledge of
σ and the number of iterations  both of which can be difﬁcult to estimate in practice. Moreover 
with the use of a sparsity-promoting ψ(x)  SAGE can produce a sparse solution (as will be exper-
imentally demonstrated in Section 5) while AC-SA cannot. This is because in SAGE  the output
yt is obtained from a generalized gradient update. With a sparsity-promoting ψ(x)  this reduces to
a (soft) thresholding step  and thus ensures a sparse solution. On the other hand  in each iteration
of AC-SA  its output is a convex combination of two other variables. Unfortunately  adding two
vectors is unlikely to produce a sparse vector.

3.3 Efﬁcient Computation of yt

The computational efﬁciency of Algorithm 1 hinges on the efﬁcient computation of yt. Recall that
yt is just the generalized gradient update  and so is not signiﬁcantly more expensive than the gradient
update in traditional algorithms. Indeed  the generalized gradient update is often a central compo-
nent in various optimization and machine learning algorithms. In particular  Duchi and Singer [3]
showed how this can be efﬁciently computed with the various smooth and non-smooth regulariz-
2  ℓ∞  Berhu and matrix norms. Interested readers are referred to [3] for
ers  including the ℓ1  ℓ2  ℓ2
details.

4 Accelerated Gradient Method for Online Learning

In this section  we extend the proposed accelerated gradient scheme for online learning of (2). The
algorithm  shown in Algorithm 2  is similar to the stochastic version in Algorithm 1.

Algorithm 2 SAGE-based Online Learning Algorithm.

Inputs: Sequences {Lt} and {αt}  where Lt > L and 0 < αt < 1.
Initialize: z1 = y1.
loop

xt = (1 − αt)yt−1 + αtzt−1.
Output yt = arg minx(cid:8)h∇ft−1(xt)  x − xti + Lt
zt = zt−1 − αt(Lt + µαt)−1[Lt(xt − yt) + µ(zt−1 − xt)].

2 kx − xtk2 + ψ(x)(cid:9).

end loop

First  we introduce the following lemma  which plays a similar role as its stochastic counterpart of
Lemma 3. Moreover  let δt ≡ Lt(xt − yt) be the gradient mapping related to the updating of yt.
Lemma 4. For t > 1  φt(x) can be quadratically bounded from below as

φt−1(x) ≥ φt−1(yt) + hδt  x − xti +

µ
2 kx − xtk2 +

2Lt − L

2L2
t

kδtk2.

Proposition 2. For any x and t ≥ 1  assume that there exists a subgradient ˆg(x) ∈ ∂ψ(x) such that
k∇ft(x) + ˆg(x)k∗ ≤ Q. Then for Algorithm 2 

Q2

2(1 − αt)(Lt − L)
+

(1 − α2

t )Lt − αt(1 − αt)L

2

+

Lt
2αtkx − zt−1k2 −

Lt + µαt

2αt
kyt−1 − zt−1k2 −

kx − ztk2
Lt
2 kzt − ytk2.

φt−1(yt−1) − φt−1(x) ≤

(18)

Proof Sketch. Deﬁne τt = Ltα−1

t

. From the update rule of zt  one can check that

Vt(x) ≡ hδt  x − xti +
Similar to the analysis in obtaining (9)  we can obtain

zt = arg min

x

µ
2 kx − xtk2 +

τt
2 kx − zt−1k2.

φt−1(yt)−φt−1(x)≤hδt  xt−zti−

2Lt−L
2L2
t

τt
τt
kδtk2−
2 kzt−zt−1k2+
2 kx−zt−1k2−

τt +µ

kx−ztk2. (19)

2

6

On the other hand 

hδt  xt − zti − kδtk2

2Lt

Lt
(kzt − xtk2 − kzt − ytk2)
2
Lt(1 − αt)
Lt
2αtkzt − zt−1k2 +

2

=

≤

kzt−1 − yt−1k2 −

Lt
2 kzt − ytk2 

(20)

on using the convexity of k · k2. Using (20)  the inequality (19) becomes

φt−1(yt) − φt−1(x) ≤

kzt−1 − yt−1k2 −

2

Lt(1 − αt)
Lt − L
2L2

−

t kδtk2 +

τt
2 kx − zt−1k2 −

kx − ztk2.

2

(21)

Lt
2 kzt − ytk2
τt + µ

On the other hand  by the convexity of φt−1(x) and the Young’s inequality  we have

φt−1(yt−1) − φt−1(yt) ≤ h∇ft−1(yt−1) + ˆgt−1(yt−1)  yt−1 − yti
(1 − αt)(Lt − L)

Q2

+

≤

2(1 − αt)(Lt − L)

Moreover  by using the update rule of xt and the convexity of k · k2  we have

2

kyt−1 − ytk2.

(22)

kyt−1 − ytk2 = k(yt−1 − xt) + (xt − yt)k2 = kαt(yt−1 − zt−1) + (xt − yt)k2
≤ αtkyt−1 − zt−1k2 + (1 − αt)−1kxt − ytk2 = αtkyt−1 − zt−1k2 + kδtk2
(1 − αt)L2

t

.

(23)

On using (23)  it follows from (22) that

φt−1(yt−1) − φt−1(yt) ≤
Inequality (18) then follows immediately by adding this to (21).

2(1−αt)(Lt−L)

+

2

Q2

αt(1−αt)(Lt−L)

kyt−1−zt−1k2 +

Lt−L
2L2

t kδtk2.

Theorem 3. Assume that µ = 0  and kx∗−ztk ≤ D for t ≥ 1. Set αt = a and Lt = aL√t − 1+L 
where a ∈ (0  1) is a constant. Then the regret of Algorithm 2 can be bounded as
a(1 − a)L(cid:21)√N .

[φt(yt) − φt(x∗)] ≤

+(cid:20) LD2

LD2
2a

Q2

+

2

N

Theorem 4. Assume that µ > 0  and kx∗ − ztk ≤ D for t ≥ 1. Set αt = a  and Lt = aµt + L +
a−1(µ − L)+  where a ∈ (0  1) is a constant. Then the regret of Algorithm 2 can be bounded as

Xt=1

N

Xt=1

[φt(yt) − φt(x∗)] ≤(cid:20) (2a + a−1)µ + L
2   the regret bound reduces to(cid:0) 3µ

(cid:21) D2 +
2a(1 − a)µ
2 + L(cid:1) D2 + 2Q2

Q2

2a

In particular  with a = 1

5 Experiments

log(N + 1).

µ log(N + 1).

In this section  we perform experiments on the stochastic optimization of (2). Two data sets are
used2 (Table 1). The ﬁrst one is the pcmac data set  which is a subset of the 20-newsgroup data set
from [18]  while the second one is the RCV1 data set  which is a ﬁltered collection of the Reuters
RCV1 from [19]. We choose the square loss for ℓ(· ·) and the ℓ1 regularizer for Ω(·) in (2). As
discussed in Section 3.3 and [3]  the generalized gradient update can be efﬁciently computed by soft
thresholding in this case. Moreover  we do not use strong convexity and so µ = 0.
We compare the proposed SAGE algorithm (with Lt and αt in (14)) with three recent algorithms: (1)
FOLOS [3]; (2) SMIDAS [4]; and (3) SCD [4]. For fair comparison  we compare their convergence

2Downloaded from http://people.cs.uchicago.edu/∼vikass/svmlin.html and http://www.cs.ucsb.edu/∼wychen/sc.html.

7

behavior w.r.t. both the number of iterations and the number of data access operations  the latter
of which has been advocated in [4] as an implementation-independent measure of time. Moreover 
the efﬁciency tricks for sparse data described in [4] are also implemented. Following [4]  we set the
regularization parameter λ in (2) to 10−6. The η parameter in SMIDAS is searched over the range
of {10−6  10−5  10−4  10−3  10−2  10−1}  and the one with the lowest ℓ1-regularized loss is used.
As in Pegasos [1]  the (sub)gradient is computed from small sample subsets. The subset size p is set
to min(0.01m  500)  where m is the data set size. This is used on all the algorithms except SCD 
since SCD is based on coordinate descent and is quite different from the other stochastic subgradient
algorithms.3 All the algorithms are trained with the same maximum amount of “time” (i.e.  number
of data access operations).

Table 1: Summary of the data sets.

data set
pcmac
RCV1

#features

#instances

7 511
47 236

1 946
193 844

sparsity
0.73%
0.12%

Results are shown in Figure 1. As can be seen  SAGE requires much fewer iterations for convergence
than the others (Figures 1(a) and 1(e)). Moreover  the additional costs on maintaining xt and zt are
small  and the most expensive step in each SAGE iteration is in computing the generalized gradient
update. Hence  its per-iteration complexity is comparable with the other (sub)gradient schemes  and
its convergence in terms of the number of data access operations is still the fastest (Figures 1(b) 
1(c)  1(f) and 1(g)). Moreover  the sparsity of the SAGE solution is comparable with those of the
other algorithms (Figures 1(d) and 1(h)).

s
s
o

l
 

d
e
z
i
r
a
u
g
e
r
 

l

L

1

s
s
o

l
 

d
e
z
i
r
a
u
g
e
r
 

l

L

1

1

0.8

0.6

0.4

0.2

 

0
0

1

0.8

0.6

0.4

0.2

 

0
0

SAGE
FOLOS
SMIDAS

 

s
s
o

l
 

d
e
z
i
r
a
u
g
e
r
 

l

L

1

1000
3000
Number of Iterations

2000

(a)

SAGE
FOLOS
SMIDAS

1000
3000
Number of Iterations

2000

(e)

4000

 

4000

s
s
o

l
 

d
e
z
i
r
a
u
g
e
r
 

l

L

1

1

0.8

0.6

0.4

0.2

 

0
0

1

0.8

0.6

0.4

0.2

 

0
0

 

100

 

8000

 

SAGE
FOLOS
SMIDAS
SCD

2

4

6

8

Number of Data Accesses

10
x 106

(b)

)

%

(
 
r
o
r
r

E

80

60

40

20

 

0
0

SAGE
FOLOS
SMIDAS
SCD

Number of Data Accesses

10
x 106

2

4

6

8

(c)

w

 
f

o

 
y
t
i
s
n
e
D

6000

4000

2000

 

0
0

SAGE
FOLOS
SMIDAS
SCD

2

8
Number of Data Accesses

6

4

(d)

10
x 106

 

SAGE
FOLOS
SMIDAS
SCD

 

100

 

SAGE
FOLOS
SMIDAS
SCD

x 104

4

3

2

1

w

 
f

o

 
y
t
i
s
n
e
D

SAGE
FOLOS
SMIDAS
SCD

0.5

1

1.5

2

Number of Data Accesses

2.5
x 108

(f)

)

%

(
 
r
o
r
r

E

80

60

40

20

 

0
0

0.5

2
Number of Data Accesses

1.5

1

(g)

2.5
x 108

 

0
0

0.5

1

1.5

2

Number of Data Accesses

(h)

2.5
x 108

Figure 1: Performance of the various algorithms on the pcmac (upper) and RCV1 (below) data sets.

6 Conclusion

In this paper  we developed a novel accelerated gradient method (SAGE) for stochastic con-
vex composite optimization. It enjoys the computational simplicity and scalability of traditional
(sub)gradient methods but are much faster  both theoretically and empirically. Experimental results
show that SAGE outperforms recent (sub)gradient descent methods. Moreover  SAGE can also be
extended to online learning  obtaining the best regret bounds currently known.

Acknowledgment

This research has been partially supported by the Research Grants Council of the Hong Kong Special
Administrative Region under grant 615209.

3For the same reason  an SCD iteration is also very different from an iteration in the other algorithms.

Hence  SCD is not shown in the plots on the regularized loss versus number of iterations.

8

References

[1] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver
for SVM. In Proceedings of the 24th International Conference on Machine Learning  pages
807–814  Corvalis  Oregon  USA  2007.

[2] A. Bordes  L. Bottou  and P. Gallinari. SGD-QN: Careful Quasi-Newton Stochastic Gradient

Descent. Journal of Machine Learning Research  10:1737–1754  2009.

[3] J. Duchi and Y. Singer. Online and batch learning using forward looking subgradients. Tech-

nical report  2009.

[4] S. Shalev-Shwartz and A. Tewari. Stochastic methods for ℓ1 regularized loss minimization.
In Proceedings of the 26th International Conference on Machine Learning  pages 929–936 
Montreal  Quebec  Canada  2009.

[5] L. Bottou and O. Bousquet. The tradeoffs of large scale learning.

Information Processing Systems 20. 2008.

In Advances in Neural

[6] S. Shalev-Shwartz and N. Srebro. SVM optimization: Inverse dependence on training set size.
In Proceedings of the 25th International Conference on Machine Learning  pages 928–935 
Helsinki  Finland  2008.

[7] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of con-

vergence o( 1

k2 ). Doklady AN SSSR (translated as Soviet. Math. Docl.)  269:543–547  1983.

[8] Y. Nesterov. Gradient methods for minimizing composite objective function. CORE Discus-

sion Paper 2007/76  Catholic University of Louvain  September 2007.

[9] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2:183–202  2009.

[10] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society: Series B  58:267–288  1996.

[11] S. Ji  L. Sun  R. Jin  and J. Ye. Multi-label multiple kernel learning. In Advances in Neural

Information Processing Systems 21. 2009.

[12] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In Proceedings

of the International Conference on Machine Learning. Montreal  Canada  2009.

[13] G. Lan. An optimal method for stochastic composite optimization. Technical report  School

of Industrial and Systems Engineering  Georgia Institute of Technology  2009.

[14] Y. Nesterov and I.U.E. Nesterov.

Course. Kluwer  2003.

Introductory Lectures on Convex Optimization: A Basic

[15] S.M. Kakade and S. Shalev-Shwartz. Mind the duality gap: Logarithmic regret algorithms for

online optimization. In Advances in Neural Information Processing Systems 21. 2009.

[16] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters  31(3):167–175  2003.

[17] S.J. Wright  R.D. Nowak  and M.A.T. Figueiredo. Sparse reconstruction by separable approx-
In Proceedings of the International Conference on Acoustics  Speech  and Signal

imation.
Processing  Las Vegas  Nevada  USA  March 2008.

[18] V. Sindhwani and S.S. Keerthi. Large scale semi-supervised linear SVMs. In Proceedings of
the SIGIR Conference on Research and Development in Information Retrieval  pages 477–484 
Seattle  WA  USA  2006.

[19] Y. Song  W.Y. Chen  H. Bai  C.J. Lin  and E.Y. Chang. Parallel spectral clustering. In Proceed-
ings of the European Conference on Machine Learning  pages 374–389  Antwerp  Belgium 
2008.

9

,Tengyu Ma
Avi Wigderson
Pan Zhou
Xiaotong Yuan
Jiashi Feng