2016,Tracking the Best Expert in Non-stationary Stochastic Environments,We study the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. We introduce a new parameter $\W$  which measures the total statistical variance of the loss distributions over $T$ rounds of the process  and study how this amount affects the regret. We investigate the interaction between $\W$ and $\Gamma$  which counts the number of times the distributions change  as well as $\W$ and $V$  which measures how far the distributions deviates over time. One striking result we find is that even when $\Gamma$  $V$  and $\Lambda$ are all restricted to constant  the regret lower bound in the bandit setting still grows with $T$. The other highlight is that in the full-information setting  a constant regret becomes achievable with constant $\Gamma$ and $\Lambda$  as it can be made independent of $T$  while with constant $V$ and $\Lambda$  the regret still has a $T^{1/3}$ dependency. We not only propose algorithms with upper bound guarantee  but prove their matching lower bounds as well.,Tracking the Best Expert in Non-stationary Stochastic

Environments

Chen-Yu Wei

Yi-Te Hong

Chi-Jen Lu

Institute of Information Science

{bahh723  ted0504  cjlu}@iis.sinica.edu.tw

Academia Sinica  Taiwan

Abstract

We study the dynamic regret of multi-armed bandit and experts problem in non-
stationary stochastic environments. We introduce a new parameter Λ  which
measures the total statistical variance of the loss distributions over T rounds of the
process  and study how this amount affects the regret. We investigate the interaction
between Λ and Γ  which counts the number of times the distributions change  as
well as Λ and V   which measures how far the distributions deviates over time. One
striking result we ﬁnd is that even when Γ  V   and Λ are all restricted to constant 
the regret lower bound in the bandit setting still grows with T . The other highlight
is that in the full-information setting  a constant regret becomes achievable with
constant Γ and Λ  as it can be made independent of T   while with constant V and
Λ  the regret still has a T 1/3 dependency. We not only propose algorithms with
upper bound guarantee  but prove their matching lower bounds as well.

1

Introduction

√

Many situations in our daily life require us to make repeated decisions which result in some losses
corresponding to our chosen actions. This can be abstracted as the well-known online decision
problem in machine learning [5]. Depending on how the loss vectors are generated  two different
worlds are usually considered. In the adversarial world  loss vectors are assumed to be deterministic
and controlled by an adversary  while in the stochastic world  loss vectors are assumed to be sampled
independently from some distributions. In both worlds  good online algorithms are known which
can achieve a regret of about
T over T time steps  where the regret is the difference between the
total loss of the online algorithm and that of the best ofﬂine one. Another distinction is about the
information the online algorithm can receive after each action. In the full-information setting  it gets
to know the whole loss vector of that step  while in the bandit setting  only the loss value of the
chosen action is received. Again  in both settings  a regret of about
T turns out to be achievable.
While the regret bounds remain in the same order in those general scenarios discussed above  things
become different when some natural conditions are considered. One well-known example is that in
the stochastic multi-armed bandit (MAB) problem  when the best arm (or action) is substantially
better than the second best  with a constant gap between their means  then a much lower regret  of the
order of log T   becomes possible. This motivates us to consider other possible conditions which can
have ﬁner characterization of the problem in terms of the achievable regret.
In the stochastic world  most previous works focused on the stationary setting  in which the loss (or
reward) vectors are assumed to be sampled from the same distribution for all time steps. With this
assumption  although one needs to balance between exploration and exploitation in the beginning 
after some trials  one can be conﬁdent about which action is the best and rest assured that there are
no more surprises. On the other hand  the world around us may not be stationary  in which existing
learning algorithms for the stationary case may no longer work. In fact  in a non-stationary world  the

√

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

dilemma between exploration and exploitation persists as the underlying distribution may drift as
time evolves. How does the non-stationarity affect the achievable regret? How does one measure the
degree of non-stationarity?
In this paper  we answer the above questions through the notion of dynamic regret  which measures
the algorithm’s performance against an ofﬂine algorithm allowed to select the best arm at every step.

√

Related Works. One way to measure the non-stationarity of a sequence of distributions is to count
the number of times the distribution at a time step differs from its previous one. Let Γ − 1 be
this number so that the whole time horizon can be partitioned into Γ intervals  with each interval
having a stationary distribution. In the bandit setting  a regret of about
ΓT is achieved by the
EXP3.S algorithm in [2]  as well as the discounted UCB and sliding-window UCB algorithms in
[8]. The dependency on T can be reﬁned in the full-information setting: AdaNormalHedge [10]
and Adapt-ML-Prod [7] can both achieve regret in the form of
ΓC  where C is the total ﬁrst-order
and second-order excess loss respectively  which is upper-bounded by T . From a slightly different
Online Mirror Descent approach  [9] can also achieve a regret of about
ΓD  where D is the sum of
differences between consecutive loss vectors.
Another measure of non-stationarity  denoted by V   is to compute the difference between the means
of consecutive distributions and sum them up. Note that this allows the possibility for the best arm to
change frequently  with a very large Γ  while still having similar distributions with a small V . For
such a measure V   [3] provided a bandit algorithm which achieves a regret of about V 1/3T 2/3. This
regret upper bound is unimprovable in general even in the full-information setting  as a matching
lower bound was shown in [4]. Again  [9] reﬁned the upper bound in the full-information setting

through the introduction of D  achieving the regret of about 3(cid:112) ˜V DT   for a parameter ˜V different but

√

√

related to V : ˜V calculates the sum of differences between consecutive realized loss vectors  while V
measures that between mean loss vectors. This makes the results of [3] and [9] incomparable. The
problem stems from the fact that [9] considers the traditional adversarial setting  while [3] studies the
non-stationary stochastic setting. In this paper  we will provide a framework that bridges these two
seemingly disparate worlds.

Our Results. We base ourselves in the stochastic world with non-stationary distributions  charac-
terized by the parameters Γ and V . In addition  we introduce a new parameter Λ  which measures
the total statistical variance of the distributions. Note that traditional adversarial setting corresponds
to the case with Λ = 0 and Γ ≈ V ≈ T   while the traditional stochastic setting has Λ ≈ T and
Γ = V = 1. Clearly  with a smaller Λ  the learning problem becomes easier  and we would like to
understand the tradeoff between Λ and other parameters  including Γ  V   and T . In particular  we
would like to know how the bounds described in the related works would be changed. Would all the
dependency on T be replaced by Λ  or would only some partial dependency on T be shifted to Λ?
First  we consider the effect of the variance Λ with respect to the parameter Γ. We show that in
the full-information setting  a regret of about
ΓΛ + Γ can be achieved  which is independent of
T . On the other hand  we show a sharp contrast that in the bandit setting  the dependency on T is
unavoidable  and a lower bound of the order of
ΓT exists. That is  even when there is no variance in
distributions  with Λ = 0  and the distributions only change once  with Γ = 2  any bandit algorithm
cannot avoid a regret of about
T   while a full-information algorithm can achieve a constant regret
independent of T .
√
Next  we study the tradeoff between Λ and V . We show that in the bandit setting  a regret of about
V T is achievable. Note that this recovers the V 1/3T 2/3 regret bound of [3] as Λ is at
most of the order of T   but our bound becomes better when Λ is much smaller than T . Again  one
may notice the dependency on T and wonder if this can also be removed in the full-information
setting. We show that in the full-information setting  the regret upper bound and lower bound are
√
both about 3
adversarial setting corresponds to Λ = 0 and their D can be as large as T in our setting. Moreover 
we see that while the full-information regret bound is slightly better than that in the bandit setting 
there is still an unavoidable T 1/3 dependency.

ΛV T + V . Our upper bound is incomparable to the 3(cid:112) ˜V DT bound of [9]  since their

√
√

ΛV T +

√

√

3

2

Our results provide a big picture of the regret landscape in terms of the parameters Λ  Γ  V   and T  
in both full-information and bandit settings. A table summarizing our bounds as well as previous
ones is given in Appendix A in the supplementary material. Finally  let us remark that our effort
mostly focuses on characterizing the achievable (minimax) regrets  and most of our upper bounds are
achieved by algorithms which need the knowledge of the related parameters and may not be practical.
To complement this  we also propose a parameter-free algorithm  which still achieve a good regret
bound and may have independent interest of its own.

2 Preliminaries
Let us ﬁrst introduce some notations. For an integer K > 0  let [K] denote the set {1  . . .   K}. For
a vector (cid:96) ∈ RK  let (cid:96)i denote its i’th component. When we need to refer to a time-indexed vector
(cid:96)t ∈ RK  we will write (cid:96)t i to denote its i’th component. We will use the indicator function 1C for a
condition C  which gives the value 1 if C holds and 0 otherwise. For a vector (cid:96)  we let (cid:107)(cid:96)(cid:107)b denote its
Lb-norm. While standard notation O(·) is used to hide constant factors  we will use the notation ˜O(·)
to hide logarithmic factors.
Next  let us describe the problem we study in this paper. Imagine that a learner is given the choice
of a total of K actions  and has to play iteratively for a total of T steps. At step t  the learner
needs to choose an action at ∈ [K]  and then suffers a corresponding loss (cid:96)t i ∈ [0  1]  which is
independently drawn from a non-stationary distribution with expected loss E[(cid:96)t i] = µt i  which
may drift over time. After that  the learner receives some feedback from the environment. In the
full-information setting  the feedback gives the whole loss vector (cid:96)t = ((cid:96)t 1  ...  (cid:96)t K)  while in
the bandit setting  only the loss (cid:96)t at of the chosen action is revealed. A standard way to evaluate
the learner’s performance is to measure her (or his) regret  which is the difference between the
total loss she suffers and that of an ofﬂine algorithm. While most prior works consider ofﬂine
algorithms which can only play a ﬁxed action for all the steps  we consider stronger ofﬂine algorithms
which can take different actions in different steps. Our consideration is natural for non-stationary
distributions  although this would make the regret large when compared to such stronger ofﬂine
algorithms. Formally  we measure the learner’s performance by its expected dynamic pseudo-regret 
t = arg mini µt i is the best
action at step t. For convenience  we will simply refer it as the regret of the learner later in our paper.
We will consider the following parameters characterizing different aspects of the environments:

deﬁned as(cid:80)T

(cid:0)µt at − µt u∗

t=1

t

t=1 E(cid:2)(cid:96)t at − (cid:96)t u∗
T(cid:88)

t

(cid:3) = (cid:80)T
T(cid:88)

(cid:1)   where u∗
T(cid:88)

E(cid:2)(cid:107)(cid:96)t − µt(cid:107)2

2

(cid:3)  

(cid:107)µt − µt−1(cid:107)∞  and Λ =

Γ = 1 +

1µt(cid:54)=µt−1   V =

(1)

t=2

t=1

t=1

where we let µ0 be the all-zero vector. Here  Γ − 1 is the number of times the distributions switch 
V measures the distance the distributions deviate  and Λ is the total statistical variance of these T
distributions. We will call distributions with a small Γ switching distributions  while we will call
distributions with a small V drifting distributions and call V the total drift of the distributions.
Finally  we will need the following large deviation bound  known as empirical Bernstein inequality.
Theorem 2.1. [11] Let X = (X1  ...  Xn) be a vector of independent random variables taking

values in [0  1]  and let ΛX =(cid:80)

1≤i<j≤n(Xi − Xj)2/(n(n − 1)). Then for any δ > 0  we have

(cid:115)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

Pr

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > ρ(n  ΛX   δ)

E [Xi] − Xi

n

≤ δ 

for ρ(n  Λ  δ) =

2Λ log 2
δ

n

+

7 log 2
δ
3(n − 1)

.

3 Algorithms

We would like to characterize the achievable regret bounds for both switching and drifting distribu-
tions  in both full-information and bandit settings. In particular  we would like to understand the
interplay among the parameters Γ  V  Λ  and T   deﬁned in (1). The only known upper bound which is
good enough for our purpose is that by [8] for switching distributions in the bandit setting  which is
close to the lower bound in our Theorem 4.1. In subsection 3.1  we provide a bandit algorithm for
drifting distributions which achieves an almost optimal regret upper bound  when given the parameters

3

Algorithm 1 Rerun-UCB-V

Initialization: Set B according to (2) and δ = 1/(KT ).
for m = 1  . . .   T /B do

for t = (m − 1)B + 1  . . .   mB do

Choose arm at := argmini(ˆµt i − λt i)  with ˆµt i and λt i computed according to (3).

end for

end for

V  Λ  T . In subsection 3.2  we provide a full-information algorithm which works for both switching
and drifting distributions. The regret bounds it achieves are also close to optimal  but it again needs
the knowledge of the related parameters. To complement this  we provide a full-information algorithm
in subsection 3.3  which does not need to know the parameters but achieves slightly larger regret
bounds.

3.1 Parameter-Dependent Bandit Algorithm

√

ΛV T +

√
In this subsection  we consider drifting distributions parameterized by V and Λ. Our main result is a
bandit algorithm which achieves a regret of about 3
V T . As we aim to achieve smaller
regrets for distributions with smaller statistical variances  we adopt a variant of the UCB algorithm
developed by [1]  called UCB-V  which takes variances into account when building its conﬁdence
interval.
Our algorithm divides the time steps into T /B intervals I1  . . .  IT /B  each having B steps 1 with
(2)
For each interval  our algorithm clears all the information from previous intervals  and starts a fresh
run of UCB-V. More precisely  before step t in an interval I  it maintains for each arm i its empirical
mean ˆµt i  empirical variance ˆΛt i  and size of conﬁdence interval λt i  deﬁned as

B = 3(cid:112)K 2ΛT /V 2 if KΛ2 ≥ T V and B =(cid:112)KT /V otherwise.

ˆµt i =

(cid:96)s i
|St i|   ˆΛt i =

((cid:96)r i − (cid:96)s i)2
|St i|(|St i| − 1)

  and λt i = ρ(|St i|  ˆΛt i  δ) 

(3)

where St i denotes the set of steps before t in I that arm i was played  and ρ is the function given in
Theorem 2.1. Here we use the convention that ˆµt i = 0 if |St i| = 0  while ˆΛt i = 0 and λt i = 1 if
|St i| ≤ 1. Then at step t  our algorithm selects the optimistic arm

(cid:88)

s∈St i

(cid:88)

r s∈St i

at := argmin

i

(ˆµt i − λt i) 

receives the corresponding loss  and updates the statistics.
Our algorithm is summarized in Algorithm 1  and its regret is guaranteed by the following  which we
prove in Appendix B in the supplementary material.
√
Theorem 3.1. The expected regret of Algorithm 1 is at most ˜O( 3

K 2ΛV T +

KV T ).

√

3.2 Parameter-Dependent Full-Information Algorithms

t (cid:107)(cid:96)t − (cid:96)t−1(cid:107)2

where D =(cid:80)

In this subsection  we provide full-information algorithms for switching and drifting distributions. In
fact  they are based on an existing algorithm from [6]  which is known to work in a different setting:
√
the loss vectors are deterministic and adversarial  and the ofﬂine comparator cannot switch arms. In
that setting  one of their algorithms  based on gradient-descent (GD)  can achieve a regret of O(
D)
2  which is small when the loss vectors have small deviation. Our ﬁrst
√
observation is that their algorithm in fact can work against a dynamic ofﬂine comparator which
switches arms less than N times  given any N  with its regret becoming O(
N D). Our second
observation is that when Λ is small  each observed loss vector (cid:96)t is likely to be close to its true mean
1For simplicity of presentation  let us assume here and later in the paper that taking divisions and roots to
produce blocks of time steps all yield integers. It is easy to modify our analysis to the general case without
affecting the order of our regret bound.

4

Algorithm 2 Full-information GD-based algorithm
Initialization: Let x1 = ˆx1 = (1/K  . . .   1/K)(cid:62).
for t = 1  2  . . .   T do
(cid:107)ˆx − xt(cid:107)2

Play ˆxt = arg minˆx∈X ((cid:104)(cid:96)t−1  ˆx(cid:105) + 1
Update xt+1 = arg minx∈X ((cid:104)(cid:96)t  x(cid:105) + 1

ηt

(cid:107)x − xt(cid:107)2
2).

ηt

end for

2)  and then receive loss vector (cid:96)t.

µt  and when V is small  (cid:96)t is likely to be close to (cid:96)t−1. These two observations make possible for us
to adopt their algorithm to our setting.
We show the ﬁrst algorithm in Algorithm 2  with the feasible set X being the probability simplex.
The idea is to use (cid:96)t−1 as an estimate for (cid:96)t to move ˆxt further in a possibly beneﬁcial direction. Its
regret is guaranteed by the following  which we prove in Appendix C in the supplementary material.
Theorem 3.2. For switching distributions parameterized by Γ and Λ  the regret of Algorithm 2 with

ηt = η =(cid:112)Γ/(Λ + KΓ)  is at most O(

KΓ).

ΓΛ +

√

√

Note that for switching distributions  the regret of Algorithm 2 does not depend on T   which means
that it can achieve a constant regret for constant Γ and Λ. Let us remark that although using a variant
based on multiplicative updates could result in a better dependency on K  an additional factor of
log T would then emerge when using existing techniques for dealing with dynamic comparators.
For drifting distributions  one can show that Algorithm 2 still works and has a good regret bound.
However  a slightly better bound can be achieved as we describe next. The idea is to divide the time

steps into T /B intervals of size B  with B = 3(cid:112)ΛT /V 2 if ΛT > V 2 and B = 1 otherwise  and

re-run Algorithm 2 in each interval with an adaptive learning rate. One way to have an adaptive
learning rate can be found in [9]  which works well when there is only one interval. A natural way to
adopt it here is to reset the learning rate at the start of each interval  but this does not lead to a good
enough regret bound as it results in some constant regret at the start of every interval. To avoid this 
some careful changes are needed. Speciﬁcally  in an interval [t1  t2]  we run Algorithm 2 with the
learning rate reset as

(cid:118)(cid:117)(cid:117)(cid:116)4

t−1(cid:88)

ηt = 1/

(cid:107)(cid:96)τ − (cid:96)τ−1(cid:107)2

2

τ =t1

for t > t1  with ηt1 = ∞ initially for every interval. This has the beneﬁt of having small or even no
regret at the start of an interval when the loss vectors across the boundary have small or no deviation.
The regret of this new algorithm is guaranteed by the following  which we prove in Appendix D in
the supplementary material.
√
√
Theorem 3.3. For drifting distributions parameterized by V and Λ  the regret of this new algorithm
is at most O( 3

V ΛT +

KV ).

3.3 Parameter-Free Full-Information Algorithm

(cid:113)(cid:80)

The reason that our algorithm for Theorem 3.3 needs the related parameters is to set its learning rate
properly. To have a parameter-free algorithm  we would like to adjust the learning rate dynamically
in a data-driven way. One way for doing this can be found in [7]  which is based on the multiplicative
updates variant of the mirror-descent algorithm. It achieves a static regret of about
t k against
any expert k  where rt k = (cid:104)pt  (cid:96)t(cid:105) − (cid:96)t k is its instantaneous regret for playing pt at step t. However 
in order to work in our setting  we would like the regret bound to depend on (cid:96)t − (cid:96)t−1 as seen
previously. This suggests us to modify the Adapt-ML-Prod algorithm of [7] using the idea of [6] 
which takes (cid:96)t−1 as an estimate of (cid:96)t to move pt further in an optimistic direction.
Recall that the algorithm of [7] maintains a separate learning rate ηt k for each arm k at time t  and it
updates the weight wt k as well as ηt k using the instantaneous regret rt k. To modify the algorithm
using the idea of [6]  we would like to have an estimate mt k for rt k in order to move pt k further
using mt k and update the learning rate accordingly. More precisely  at step t  we now play pt  with
(4)

pt k = ηt−1 k ˜wt−1 k/(cid:104)ηt−1  ˜wt−1(cid:105) where ˜wt−1 k = wt−1 k exp(ηt−1 kmt k) 

t r2

5

Algorithm 3 Optimistic-Adapt-ML-Prod

Initialization: Let w0 k = 1/K and (cid:96)0 k = 0 for every k ∈ [K].
for t = 1  2  . . .   T do

Play pt according to (4)  and then receive loss vector (cid:96)t.
Update each weight wt k according to (5) and each learning rate ηt k according to (6).

end for

which uses the estimate mt k to move further from wt−1 k. Then after receiving the loss vector (cid:96)t 
we update each weight

wt k =(cid:0)wt−1 k exp(cid:0)ηt−1 krt k − η2
(cid:18)

(cid:115)

(cid:40)

ηt k = min

1/4 

(ln K)/

1 +

t−1 k(rt k − mt k)2(cid:1)(cid:1)ηt k/ηt−1 k
(cid:19)(cid:41)
(cid:88)

(rs k − ms k)2

.

s∈[t]

as well as each learning rate

(5)

(6)

(cid:112)(cid:80)

ηt−1 k ˜wt−1 k(α)/(cid:80)

Our algorithm is summarized in Algorithm 3  and we will show that it achieves a regret of about
t(rt k − mt k)2 against arm k. It remains to choose an appropriate estimate mt k. One attempt
is to have mt k = rt−1 k  but rt k − rt−1 k = ((cid:104)pt  (cid:96)t(cid:105) − (cid:96)t k) − ((cid:104)pt−1  (cid:96)t−1(cid:105) − (cid:96)t−1 k)  which does
not lead to a desirable bound. The other possibility is to set mt k = (cid:104)pt  (cid:96)t−1(cid:105) − (cid:96)t−1 k  which can
be shown to have (rt k − mt k)2 ≤ (2(cid:107)(cid:96)t − (cid:96)t−1(cid:107)∞)2. However  it is not clear how to compute
such mt k because it depends on pt k which in turns depends on mt k itself. Fortunately  we can
approximate it efﬁciently in the following way.
Note that the key quantity is (cid:104)pt  (cid:96)t−1(cid:105). Given its value α  ˜wt−1 k and pt k can be seen as func-
tions of α  deﬁned according to (5) as ˜wt−1 k(α) = wt−1 k exp(ηt k(α − (cid:96)t−1 k)) and pt k(α) =
i ηt−1 i ˜wt−1 i(α). Then we would like to show the existence of α such that
(cid:104)pt(α)  (cid:96)t−1(cid:105) = α and to ﬁnd it efﬁciently. For this  consider the function f (α) = (cid:104)pt(α)  (cid:96)t−1(cid:105) 
with pt(α) deﬁned above. It is easy to check that f is a continuous function bounded in [0  1]  which
implies the existence of some ﬁxed point α ∈ [0  1] with f (α) = α. Using a binary search  such an α
can be approximated within error 1/T in log T iterations. As such a small error does not affect the
order of the regret  we will ignore it for simplicity of presentation  and assume that we indeed have
(cid:104)pt  (cid:96)t−1(cid:105) and hence mt k = (cid:104)pt  (cid:96)t−1(cid:105) − (cid:96)t−1 k without error.
Then we have the following regret bound (c.f. [7  Corollary 4])  which we prove in Appendix E in
the supplementary material.
Theorem 3.4. The static regret of Algorithm 3 w.r.t. any arm (or expert) k ∈ [K] is at most
(cid:107)(cid:96)t − (cid:96)t−1(cid:107)2∞ ln K + ln K

(cid:18)(cid:114)(cid:88)

(cid:18)(cid:114)(cid:88)

(rt k − mt k)2 ln K + ln K
where the notation ˆO(·) hides a ln ln T factor.
The regret in the theorem above is measured against a ﬁxed arm. To achieve a dynamic regret against
an ofﬂine algorithm which can switch arms  one can use a generic reduction to the so-called sleeping
experts problem. In particular  we can use the idea in [7] by creating ˜K = KT sleeping experts  and
run our Algorithm 3 on these ˜K experts (instead of on the K arms). More precisely  each sleeping
expert is indexed by some pair (s  k)  and it is asleep for steps before s and becomes awake for steps
t ≥ s. At step t  it calls Algorithm 3 for the distribution ˜pt over the ˜K experts  and computes its own
s=1 ˜pt (s k). Then it plays pt  receives loss
vector (cid:96)t  and feeds some modiﬁed loss vector ˜(cid:96)t and estimate vector ˜mt to Algorithm 3 for update.
Here  we set ˜(cid:96)t (s k) to its expected loss (cid:104)pt  (cid:96)t(cid:105) if expert (s  k) is asleep and to (cid:96)t k otherwise  while
we set ˜mt (s k) to 0 if expert (s  k) is asleep and to mt k = (cid:104)pt  (cid:96)t−1(cid:105)− (cid:96)t−1 k otherwise. This choice
allows us to relate the regret of Algorithm 3 to that of the new algorithm  which can be seen in the
proof of the following theorem  given in Appendix F in the supplementary material.
√
Theorem 3.5. The dynamic expected regret of the new algorithm is ˜O(
√
switching distributions and ˜O( 3

distribution pt over K arms  with pt k proportional to(cid:80)t

V T ln K) for drifting distributions.

ΓΛ ln K + Γ ln K) for

V ΛT ln K +

≤ ˆO

(cid:19)

(cid:19)

t∈[T ]

t∈[T ]

√

ˆO

 

6

4 Lower Bounds

√
We study regret lower bounds in this section. In subsection 4.1  we show that for switching distribu-
tions with Γ − 1 ≥ 1 switches  there is an Ω(
ΓT ) lower bound for bandit algorithms  even when
there is no variance (Λ = 0) and there are constant loss gaps between the optimal and suboptimal arms.
We also show a full-information lower bound  which almost matches our upper bound in Theorem 3.2.
In subsection 4.2  we show that for drifting distributions  our upper bounds in Theorem 3.1 and
√
Theorem 3.2 are almost tight. In particular  we show that now even for full-information algorithms 
a large 3
T dependency in the regret turns out to be unavoidable  even for small V and Λ. This
provides a sharp contrast to the upper bound of our Theorem 3.2  which shows that a constant regret
is in fact achievable by a full-information algorithm for switching distributions with constant Γ and
Λ. For simplicity of presentation  we will only discuss the case with K = 2 actions  as it is not hard
to extend our proofs to the general case.

4.1 Switching Distributions

In contrast to the full-information setting  the existence of switches presents a dilemma with lose-lose
situation for a bandit algorithm: in order to detect any possible switch early enough  it must explore
aggressively  but this has the consequence of playing suboptimal arms too often. To fool any bandit
algorithm  we will switch between two deterministic distributions  with no variance  which have
mean vectors (cid:96)(1) = (1/2  1)(cid:62) and (cid:96)(2) = (1/2  0)(cid:62)  respectively. Our result is the following.
ΓT )  for Γ ≥ 2.
Theorem 4.1. The worst-case expected regret of any bandit algorithm is Ω(

√

√

√

√

√

B/2 =

Proof. Consider any bandit algorithm A  and let us partition the T steps into Γ/2 intervals  each
consisting of B = 2T /Γ steps. Our goal is to make A suffer in each interval an expected regret
of Ω(
B) by switching the loss vectors at most once. As mentioned before  we will only switch
between two different deterministic distributions with mean vectors: (cid:96)(1) and (cid:96)(2). Note that we can
see these two distributions simply as two loss vectors  with (cid:96)(i) having arm i as the optimal arm.
In what follows  we focus on one of the intervals  and assume that we have chosen the distributions in
all previous intervals. We would like to start the interval with the loss vector (cid:96)(1). Let N2 denote the
expected number of steps A plays the suboptimal arm 2 in this interval if (cid:96)(1) is used for the whole
interval. If N2 ≥ √
A suffer an expected regret of at least (1/2) · √
B/2  we can actually use (cid:96)(1) for the whole interval with no switch  which makes
B/4 in this interval. Thus  it remains to
B/2. In this case  A does not explore arm 2 often enough  and we
consider the case with N2 <
let it pay by choosing an appropriate step to switch to the other loss vector (cid:96)(2) = (1/2  0)(cid:62)  which
has arm 2 as the optimal one. For this  let us divide the B steps of the interval into
B blocks  each
B/2  there must be a block in which the expected number of
consisting of
steps that A plays arm 2 is at most N2/
B < 1/2. By a Markov inequality  the probability that A
ever plays arm 2 in this block is less than 1/2. This implies that when given the loss vector (cid:96)(1) for
all the steps till the end of this block  A never plays arm 2 in the block with probability more than
1/2. Therefore  if we make the switch to the loss vector (cid:96)(2) = (1/2  0)(cid:62) at the beginning of the
block  then A with probability more than 1/2 still never plays arm 2 and never notices the switch in
this block. As arm 2 is the optimal one with respect to (cid:96)(2)  the expected regret of A in this block is
more than (1/2) · (1/2) · √
Now if we choose distributions in each interval as described above  then there are at most Γ/2 · 2 = Γ
periods of stationary distribution in the whole horizon  and the total expected regret of A can be made
at least Γ/2 · √

B/4 = Γ/2 ·(cid:112)2T /Γ/4 = Ω(

ΓT )  which proves the theorem.

B steps. As N2 <

B =

B/4.

√

√

√

√

√

For full-information algorithms  we have the following lower bound  which almost matches our upper
bound in Theorem 3.2. We provide the proof in Appendix G in the supplementary material.
Theorem 4.2. The worst-case expected regret of any full-information algorithm is Ω(

ΓΛ + Γ).

√

7

4.2 Drifting Distributions

√

V T ).

In this subsection  we show that the regret upper bounds achieved by our bandit algorithm and
full-information algorithm are close to optimal by showing almost matching lower bounds. More
precisely  we have the following.
√
√
Theorem 4.3. The worst-case expected regret of any full-information algorithm is Ω( 3
while that of any bandit algorithm is Ω( 3
Proof. Let us ﬁrst consider the full-information case. When ΛT ≤ 32KV 2  we immediately have
√
from Theorem 4.2 the regret lower bound of Ω(Γ) ≥ Ω(V ) ≥ Ω( 3
√
Thus  let us focus on the case with ΛT ≥ 32KV 2. In this case  V ≤ O( 3
√
ΛV T )  so it sufﬁces to
ΛV T ). Fix any full-information algorithm A  and we will show the
prove a lower bound of Ω( 3
existence of a sequence of loss distributions for A to suffer such an expected regret. Following [3] 

we divide the time steps into T /B intervals of length B  and we set B = 3(cid:112)ΛT /(32KV 2) ≥ 1.

ΛV T + V ).

ΛV T + V ) 

ΛV T +

For each interval  we will pick some arm i as the optimal one  and give it some loss distribution
P  while other arms are sub-optimal and all have some loss distribution Q. We need P and Q to
satisfy the following three conditions: (a) P’s mean is smaller than Q’s by   (b) their variances are at
most σ2  and (c) their KL divergence satisﬁes (ln 2)KL(Q P) ≤ 2/σ2  for some   σ ∈ (0  1) to be
speciﬁed later. Their existence is guaranteed by the following  which we prove in Appendix H in the
supplementary material.
√
Lemma 4.4. For any 0 ≤ σ ≤ 1/2 and 0 ≤  ≤ σ/
the three conditions above.
Let Di denote the joint distribution of such K distributions  with arm i being the optimal one  and we
will use the same Di for all the steps in an interval. We will show that for any interval  there is some
i such that using Di this way can make algorithm A suffer a large expected regret in the interval 
conditioned on the distributions chosen for previous intervals. Before showing that  note that when
we choose distributions in this way  their total variance is at most T Kσ2 while their total drift is at

most (T /B). To have them bounded by Λ and V respectively  we choose σ =(cid:112)Λ/(4KT ) and

2  there exist distributions P and Q satisfying

 = V B/T   which satisfy the condition of Lemma 4.4  with our choice of B.
To ﬁnd the distributions  we deal with the intervals one by one. Consider any interval  and assume
that the distributions for previous intervals have been chosen. Let Ni denote the number of steps A
plays arm i in this interval  and let Ei[Ni] denote its expectation when Di is used for every step of
the interval  conditioned on the distributions of previous intervals. One can bound this conditional
expectation in terms of a related one  denoted as Eunif[Ni]  when every arm has the distribution Q for
every step of the interval  again conditioned on the distributions of previous intervals. Speciﬁcally 
using an almost identical argument to that in [2  proof of Theorem A.2.]  one can show that

Ei [Ni] ≤ Eunif [Ni] +

B
2

(cid:112)B(2 ln 2) · KL(Q P).2

i

(2/σ2) ≤ 1/4. Summing both sides of (7) over arm i  and using the fact that(cid:80)
we get (cid:80)

(7)
According to Lemma 4.4 and our choice of parameters  we have B(2 ln 2) · KL(Q P) ≤ 2B ·
Eunif [Ni] = B 
Ei [Ni] ≤ B + BK/4  which implies the existence of some i such that Ei [Ni] ≤
B/K + B/4 ≤ (3/4)B. Therefore  if we choose this distribution Di  the conditional expected regret
of algorithm A in this interval is at least (B − Ei[Ni]) ≥ B/4.
By choosing distributions inductively in this way  we can make A suffer a total expected regret of at
√
least (T /B) · (B/4) ≥ Ω( 3
Next  let us consider the bandit case. From Theorem 4.1  we immediately have a lower bound of
V T ≤
√
Ω(
ΛV T   and we can then use the full-
3

√
√
ΛV T   we have V ≤ Λ2/T which implies that V ≤ 3

ΛV T ). This completes the proof for the full-information case.
√

V T )  which implies the required bound when

√
ΓT ) ≥ Ω(

√
V T ≥ 3

ΛV T . When

√

i

ΛV T ) just proved before. This completes the proof of the theorem.

√
information bound of Ω( 3

2Note that inside the square root  we use B instead of Eunif[Ni] as in [2]. This is because in their bandit
setting  Ni is the number of steps when arm i is sampled and has its information revealed to the learner  while in
our full-information case  information about arm i is revealed in every step and there are at most B steps.

8

References
[1] Jean-Yves Audibert  Rémi Munos  and Csaba Szepesvári. Exploration-exploitation tradeoff
using variance estimates in multi-armed bandits. Theor. Comput. Sci.  410(19):1876–1902 
2009.

[2] Peter Auer  Nicolò Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic

multiarmed bandit problem. SIAM J. Comput.  32(1):48–77  2002.

[3] Omar Besbes  Yonatan Gur  and Assaf J. Zeevi. Stochastic multi-armed-bandit problem with
non-stationary rewards. In Advances in Neural Information Processing Systems 27: Annual
Conference on Neural Information Processing Systems (NIPS)  December 2014.

[4] Omar Besbes  Yonatan Gur  and Assaf J. Zeevi. Non-stationary stochastic optimization. Opera-

tions Research  63(5):1227–1244  2015.

[5] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction  learning  and games. Cambridge University

Press  2006.

[6] Chao-Kai Chiang  Tianbao Yang  Chia-Jung Lee  Mehrdad Mahdavi  Chi-Jen Lu  Rong Jin 
and Shenghuo Zhu. Online optimization with gradual variations. In The 25th Conference on
Learning Theory (COLT)  June 2012.

[7] Pierre Gaillard  Gilles Stoltz  and Tim van Erven. A second-order bound with excess losses. In

The 27th Conference on Learning Theory (COLT)  June 2014.

[8] Aurélien Garivier and Eric Moulines. On upper-conﬁdence bound policies for switching bandit
problems. In The 22nd International Conferenc on Algorithmic Learning Theory (ALT)  October
2011.

[9] Ali Jadbabaie  Alexander Rakhlin  Shahin Shahrampour  and Karthik Sridharan. Online
optimization : Competing with dynamic comparators. In Proceedings of the 18th International
Conference on Artiﬁcial Intelligence and Statistics (AISTAT)  May 2015.

[10] Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adanormalhedge. In

The 28th Conference on Learning Theory (COLT)  July 2015.

[11] Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample-variance

penalization. In The 22nd Conference on Learning Theory (COLT)  June 2009.

9

,Chen-Yu Wei
Yi-Te Hong
Chi-Jen Lu