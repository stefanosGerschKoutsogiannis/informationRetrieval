2018,Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data,Continuous-time Bayesian networks (CTBNs) constitute a general and powerful framework for modeling continuous-time stochastic processes on networks. This makes them particularly attractive for learning the directed structures among interacting entities. However  if the available data is incomplete  one needs to simulate the prohibitively complex CTBN dynamics. Existing approximation techniques  such as sampling and low-order variational methods  either scale unfavorably in system size  or are unsatisfactory in terms of accuracy. Inspired by recent advances in statistical physics  we present a new approximation scheme based on cluster-variational methods  that significantly improves upon existing variational approximations. We can analytically marginalize the parameters of the approximate CTBN  as these are of secondary importance for structure learning. This recovers a scalable scheme for direct structure learning from incomplete and noisy time-series data. Our approach outperforms existing methods in terms of scalability.,Cluster Variational Approximations for Structure
Learning of Continuous-Time Bayesian Networks

from Incomplete Data

Dominik Linzner1 and Heinz Koeppl1 2

1Department of Electrical Engineering and Information Technology

2Department of Biology

Technische Universität Darmstadt

{dominik.linzner  heinz.koeppl}@bcs.tu-darmstadt.de

Abstract

Continuous-time Bayesian networks (CTBNs) constitute a general and powerful
framework for modeling continuous-time stochastic processes on networks. This
makes them particularly attractive for learning the directed structures among inter-
acting entities. However  if the available data is incomplete  one needs to simulate
the prohibitively complex CTBN dynamics. Existing approximation techniques 
such as sampling and low-order variational methods  either scale unfavorably in
system size  or are unsatisfactory in terms of accuracy. Inspired by recent advances
in statistical physics  we present a new approximation scheme based on cluster
variational methods that signiﬁcantly improves upon existing variational approxi-
mations. We can analytically marginalize the parameters of the approximate CTBN 
as these are of secondary importance for structure learning. This recovers a scalable
scheme for direct structure learning from incomplete and noisy time-series data.
Our approach outperforms existing methods in terms of scalability.

1

Introduction

Learning directed structures among multiple entities from data is an important problem with broad
applicability  especially in biological sciences  such as genomics [1] or neuroscience [20]. With
prevalent methods of high-throughput biology  thousands of molecular components can be monitored
simultaneously in abundance and time. Changes of biological processes can be modeled as transitions
of a latent state  such as expression or non-expression of a gene  or activation/inactivation of protein
activity. However  processes at the bio-molecular level evolve across vastly different time-scales [12].
Hence  tracking every transition between states is unrealistic. Additionally  biological systems are  in
general  strongly corrupted by measurement- or intrinsic noise.
In previous numerical studies  continuous-time Bayesian networks (CTBNs) [13] have been shown
to outperform competing methods for reconstruction of directed networks  such as ones based on
Granger causality or the closely related dynamic Bayesian networks [1]. Yet  CTBNs suffer from
the curse of dimensionality  prevalent in multi-component systems. This becomes problematic if
observations are incomplete  as then the latent state of a CTBN has to be laboriously estimated [15].
In order to tackle this problem  approximation methods through sampling [8  7  19]  or variational
approaches [5  6] have been investigated. These  however  either fail to treat high-dimensional spaces
because of sample sparsity  are unsatisfactory in terms of accuracy  or provide good accuracy at the
cost of an only locally consistent description.
In this manuscript  we present  to the best of our knowledge  the ﬁrst direct structure learning method
for CTBNs based on variational inference. We extend the framework of variational inference for

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

multi-component Markov chains by borrowing results from statistical physics on cluster variational
methods [23  22  17]. Here the previous result in [5] is recovered as a special case. We derive
approximate dynamics of CTBNs in form of a new set of ordinary differential equations (ODEs).
We show that these are more accurate than existing approximations. We derive a parameter-free
formulation of these equations  that depends only on the observations  prior assumptions  and the
graph structure. Lastly  we recover an approximation for the structure score  which we use to
implement a scalable structure learning algorithm. The notion of using marginal CTBN dynamics
for network reconstruction from noisy and incomplete observations was recently explored in [21]
to successfully reconstruct networks of up to eleven nodes by sampling from the exact marginal
posterior of the process  albeit using large computational effort. Yet  the method is sampling-based
and thus still scales unfavorably in high dimensions. In contrast  we can recover the marginal CTBN
dynamics at once  using a standard ODE solver.

2 Background

2.1 Continuous-time Bayesian networks
We consider continuous-time Markov chains (CTMCs) {X(t)}t≥0 taking values in a countable state-
space S. A time-homogeneous Markov chain evolves according to an intensity matrix R : S×S → R 
whose elements are denoted by R(s  s(cid:48))  where s  s(cid:48)
∈ S. A continuous-time Bayesian network [13]
is deﬁned as an N-component process over a factorized state-space S = X1 × ··· × XN evolving
jointly as a CTMC. For local states xn  x(cid:48)
n ∈ Xn  we will drop the states’ component index n  if
evident by the context and no ambiguity arises. We impose a directed graph structure G = (V  E) 
encoding the relationship among the components V ≡ {V1  . . .   VN}  which we refer to as nodes.
These are connected via an edge set E ⊆ V × V . This quantity – the structure – is what we will later
learn. The instantaneous state of each component is denoted by Xn(t) assuming values in Xn  which
depends only on the states of a subset of nodes  called the parent set pa(n) ≡ {m | (m  n) ∈ E}.
Conversely  we deﬁne the child set ch(n) ≡ {m | (n  m) ∈ E}. The dynamics of a local state
Xn(t) are described as a Markov process conditioned on the current state of all its parents Un(t)
taking values in Un ≡ {Xm | m ∈ pa(n)}. They can then be expressed by means of the conditional
intensity matrices (CIMs) Ru
n : Xn × Xn → R  where un ≡ (u1  . . . uL) ∈ Un denotes the current
state of the parents (L = |pa(n)|). Speciﬁcally  we can express the probability of ﬁnding node n in
state x(cid:48) after some small time-step h  given that it was in state x at time t with x  x(cid:48)

∈ Xn as

(cid:48)
n(x  x

P (Xn(t + h) = x

(cid:48)

where Ru
state u ∈ Un. It holds that Ru
intensity matrix R of the CTMC via amalgamation – see  for example  [13].

n(x  x(cid:48)) is the matrix element of Ru
n(x  x) = −

x(cid:48)(cid:54)=x Ru

| Xn(t) = x  Un(t) = u) = δx x(cid:48) + Ru

(cid:80)
n corresponding to the transition x → x(cid:48) given the parents’
n(x  x(cid:48)). The CIMs are connected to the joint

)h + o(h) 

2.2 Variational lower bound

The foundation of this work is to derive a variational lower bound on the evidence of the data for
a CTMC. Such variational lower bounds are of great practical signiﬁcance and pave the way to a
multitude of approximate inference methods (variational inference). We consider paths X[0 T ] ≡
{X(ξ) | 0 ≤ ξ ≤ T} of a CTMC with a series of noisy state observations Y ≡ (Y 0  . . .   Y I )
at times (t0  . . .   tI )  drawn according to an observation model Y i ∼ P (Y i | X(ti)). We con-
sider the posterior Kullback–Leibler (KL) divergence DKL(Q(X[0 T ])||P (X[0 T ] | Y )) given a
candidate distribution Q(X[0 T ])  which can be decomposed as DKL(Q(X[0 T ])||P (X[0 T ] | Y )) =
DKL(Q(X[0 T ])||P (X[0 T ])) − E[ln P (Y | X[0 T ])] + ln P (Y )  where E[·] denotes the expectation
with respect to Q(X[0 T ])  unless speciﬁed otherwise. As DKL(Q(X[0 T ])||P (X[0 T ] | Y )) ≥ 0 this
recovers a lower bound on the evidence

(1)
where the variational lower bound F ≡ −DKL(Q(X[0 T ])||P (X[0 T ])) + E[ln P (Y | X[0 T ])] is
also known as the Kikuchi functional [11]. The Kikuchi functional has recently found heavy use in
variational approximations for probabilistic models [23  22  17]  because of the freedom it provides
for choosing clusters in space and time. We will now make use of this feature.

ln P (Y ) ≥ F 

2

Figure 1: Sketch of different cluster choices for a CTBN in discretized time: a) star approximation b)
naive mean-ﬁeld.

3 Cluster variational approximations for CTBNs

The idea behind cluster variational approximations  derived subsequently  is to ﬁnd an approximation
of the variational lower bound using M cluster functionals Fj of smaller sub-graphs Aj(t) for a
CTBN using its h-discretization (see Figure 1)

(cid:90) T

M(cid:88)

F ≈

dt

0

j=1

Fj(Aj(t)).

Examples for Aj(t) are the completely local naive mean-ﬁeld approximation Amf
j (t) = {Xj(t +
h)  Xj(t)}  or the star approximation As
j(t) = {Xj(t + h)  Uj(t)  Xj(t)} on which our method is
| s) ≡ Q(X(t + h) = s(cid:48)
based. In order to lighten the notation  we deﬁne Qh(s(cid:48)
| X(t) = s) and
Q(s  t) ≡ Q(X(t) = s) for Q and P   respectively. Marginal probabilities of individual nodes carry
their node index as a subindex. The formulation of CTBNs imposes structure on the transition matrix

(cid:48)
P h(s

| s) =

P h

n (x

(cid:48)
n | xn  un) 

(2)

N(cid:89)

n=1

suggesting a node-wise factorization to be a natural choice. In order to arrive at the variational lower
bound in the star approximation  we assume that Q(X[0 T ]) describes a CTBN  i.e. its transition
matrices satisfy (2). However  to render our approximation tractable  we further restrict the set of
approximating processes. Speciﬁcally  we require the existence of some expansion in orders of the
coupling strength ε 

(cid:48)

(cid:48)

Qh

n(x

n(x

| x  u) = Qh

| x) + O(ε) ∀n ∈ {1  . . .   N} 

(3)
where the remainder O(ε) contains the dependency on the parents.1 In the following  we derive
the star approximation of a factorized stochastic process. While the star approximation can be
constructed according to the rules of cluster variational methods  see [23  22  17]  we present a
novel derivation via a perturbative expansion of the lower bound. This is meaningful as in cluster
variational approximations  the assumptions on the approximating process (and similarity measures)
and thus the resulting approximation error cannot be quantiﬁed analytically [23]. This new derivation
also highlights the difference to conventional mean-ﬁeld approximations  where only the class of
approximating distributions is restricted. The exact expression of the variational lower bound F for a
continuous-time Markov process decomposes into time-wise components F = limh→0
0 dt f h(t)
(cid:48)
| s)Q(s  t) ln Qh(s
| s)
 

(cid:48)
| s)Q(s  t) ln P h(s

(cid:48)
Qh(s

(cid:48)
Qh(s

f h(t) =

| s)

−

1
h

(cid:88)
(cid:124)

s(cid:48) s

(cid:125)

(cid:82) T
(cid:125)

where we identiﬁed the time-dependent energy E(t) and the entropy H(t). Following (2)  we can
write Qh(s(cid:48)

(cid:88)
n | xn  un). For now  we consider the time-dependent energy
Q(s  t)

n Qh
E(t) =

(cid:89)

(cid:89)

n(x(cid:48)

Qh

P h

(cid:48)
n | xn  un) ln
n(x

k

k (x

(cid:48)
k | xk  uk).

s s(cid:48)

n

1An example of a function with such an expansion is a Markov random ﬁeld with coupling strength ε.

3

(cid:123)(cid:122)

≡E(t)

(cid:123)(cid:122)

≡H(t)

s(cid:48) s

(cid:88)
(cid:124)
| s) =(cid:81)

a)b)tX1X2X3X4X1X2X3X4t+hWe start by making use of the assumption in (3). Subsequently  we arrive at an expansion of the
energy by using the formula from Appendix B.1 

E(t) =

Q(s  t)

Qh

m(x

(cid:48)
m | xm)

n(x(cid:48)
Qh
n | xn  un)
n | xn) − (N − 1)
n(x(cid:48)
Qh

(cid:41)

(cid:40)(cid:88)

n

(cid:89)

m

(cid:88)
(cid:88)
(cid:88)

s s(cid:48)
×

k

ln P h

(cid:48)
k | xk  uk) + O(ε2).
k (x
(cid:88)

(cid:48)

(cid:48)

n

s s(cid:48)

Qh

n(x

n (x

Q(s  t)

E(t) =

f h(t) =

| x  u) + O(ε2).

For each k  we can sum over x(cid:48) for each n (cid:54)= k. This leaves us with
Q(s  t) =(cid:81)

| x  u) ln P h

(cid:88)

(cid:88)

| x  u)Qn(x  t)Qu

The exact same treatment can be done for the entropy term. Finally  assuming marginal independence
n Qn(xn  t)  we arrive at the weak coupling expansion of the variational lower bound

(cid:88)
(cid:81)
n
with the shorthand Qu
l∈pa(n) Ql(ul  t). The variational lower bound F in star approximation
(up to ﬁrst order in ε) decomposes on the h-discretized network spanned by the CTBN process 
into local star-shaped terms  see Figure 1. We emphasize that the variational lower bound in star
approximation is no longer a lower bound on the evidence but provides an approximation. We note
that in contrast to the naive mean-ﬁeld approximation employed in [16  5]  we do not have to drop
the dependence on the parents state of the variational transition matrix. Indeed  if we consider the
variational lower bound in star approximation in zeroth order of ε  we recover exactly their previous
result  demonstrating the generality of our method (see Appendix B.3).

n (x(cid:48)
P h
| x  u)
n(x(cid:48) | x  u)
Qh

x(cid:48) x∈Xn
n ≡

+ O(ε2) 

u∈Un

n ln

n(x

Qh

(cid:48)

3.1 CTBN dynamics in star approximation

(cid:48)
τ u
n (x  x

We will now derive differential equations governing CTBN dynamics in star approximation. In order
to perform the continuous-time limit h → 0  we deﬁne 
(cid:80)

n (x  x  t) ≡
n (x  x(cid:48)  t). The variational transition probability can then be written as an expansion in h

n(x(cid:48)  x  u)
Qt
  t) ≡ lim
h→0
n(x(cid:48)  x  u) ≡ Qh
with the variational transition probability Qt
−
n + hτ u
(cid:88)

Checking self-consistency of this quantity via marginalization recovers an inhomogeneous Master
equation

for x (cid:54)= x
| x  u)Qn(x  t)Qu

  x  u) = δx x(cid:48)Qn(x  t)Qu

(cid:48)
n (x  x

n and τ u

  t) + o(h).

x(cid:48)(cid:54)=x τ u

n(x(cid:48)

n(x

Qt

h

(cid:48)

(cid:48)

 

  t)].

(4)

˙Qn(x  t) =

(cid:48)

[τ u

n (x

  x  t) − τ u

(cid:48)
n (x  x

x(cid:48)(cid:54)=x u

Because of the intrinsic asynchronous update constraint on CTBNs  only local probability ﬂow inside
the state-space Xn is allowed. This renders the above equation equivalent to a continuity constraint
on the global probability distribution. After plugging in the variational transition probability into the
variational lower bound  we arrive at a functional that is only dependent on the marginal distributions.
Performing the limit of h → 0  we recover at a sum of node-wise functionals in continuous-time (see
Appendix B.2)

where we identiﬁed the variational lower bound in star approximation FS  the entropy Hn and the
energy En  respectively  as

(Hn + En) + F0 

N(cid:88)

n=1

(cid:21)

 

n (x  x(cid:48)  t)
τ u
Qn(x  t)Qu
n

(cid:88)

(cid:88)

x u

x(cid:48)(cid:54)=x

  

)

(cid:48)
τ u
n (x  x

  t) ln Ru

(cid:48)
n(x  x

(cid:90) T
(cid:90) T

0

Hn =

dt

En =

dt

0

(cid:20)

F = FS + O(ε2)  FS ≡
(cid:88)
(cid:88)
(cid:88)

Qn(x  t)En[Ru

(cid:48)
τ u
n (x  x

1 − ln

x(cid:48)(cid:54)=x

  t)

x u

n(x  x)] +

x

4

Algorithm 1 Stationary points of Euler–Lagrange equation
1: Input: Initial trajectories Qn(x  t)  boundary conditions Q(x  0) and ρ(x  T ) and data Y .
2: repeat
3:
4:
5:
6:
7:
8:
9: until Convergence
10: Output: Set of Qn(x  t) and ρn(x  t).

for all n ∈ {1  . . .   N} do
for all Y i ∈ Y do
Update ρn(x  t) by backward propagation from ti to ti−1 using (5) fulﬁlling (6).
end for
Update Qn(x  t) by forward propagation using (4) given ρn(x  t).

end for

(cid:80)
and F0 = E[ln P (Y | X[0 T ])]. The neighborhood average is deﬁned as En[f u(x)] ≡
u(cid:48) Qu(cid:48)
(x) for any function f u(x). In principle  higher-order clusters can be considered [22  17].
Lastly  we enforce continuity by (4) fulﬁlling the constraint. We can then derive the Euler–Lagrange
equations corresponding to the Lagrangian 

n f u(cid:48)

(cid:90) T

(cid:88)

L = FS −

dt

λn(x  t)

0

n x

 ˙Qn(x  t) −

(cid:88)

x(cid:48)(cid:54)=x u

(cid:48)

[τ u

n (x

  x  t) − τ u

(cid:48)
n (x  x

  t)]

with Lagrange multipliers λn(x  t).
The approximate dynamics of the CTBN can be recovered as stationary points of the Lagrangian 
satisfying the Euler–Lagrange equation. Differentiating L with respect to Qn(x  t)  its time-derivative
n (x  x(cid:48)  t) and the Lagrange multiplier λn(x  t) yield a closed set of coupled ODEs for
˙Qn(x  t)  τ u
the posterior process of the marginal distributions Qn(x  t) and transformed Lagrange multipliers
ρn(x  t) ≡ exp(λn(x  t))  eliminating τ u

n (x  x(cid:48)  t) 

En [Ru

(cid:48)
n(x  x

(cid:48)

)] ρn(x

  t) 

(5)

n(x  x)] + ψn(x  t)}ρn(x  t) −

(cid:88)

x(cid:48)(cid:54)=x

Qn(x

  t)En[Ru

n(x

  x)]

(cid:48)

ρn(x  t)
ρn(x(cid:48)  t) − Qn(x  t)En[Ru

(cid:48)
n(x  x

)]

ρn(x(cid:48)  t)
ρn(x  t)

˙Qn(x  t) =

˙ρn(x  t) ={En [Ru
(cid:88)
(cid:88)

x(cid:48)(cid:54)=x

ψn(y  t) =

(cid:48)

(cid:88)

with

Qj(x  t)

j∈ch(n)

x x(cid:48)(cid:54)=x

Ej[Ru

(cid:48)
j (x  x

j (x  x) | y]

(6)

 

 

where Ej[· | y] for y ∈ Xn is the neighborhood average with the state of node n being ﬁxed to y.
Furthermore  we recover the reset condition

  

(cid:27)
  

ρj(x  t)

(cid:26) ρj(x(cid:48)  t)
 (cid:88)

) | y] + Ej[Ru
N(cid:89)

k=1 k(cid:54)=n

t→ti− ρn(x  t) = lim
lim
t→ti+

ρn(x  t) exp

ln P (Y i | s)

s∈X|sn=x

Qk(xk  t)

(7)

n and ρn(x  T ) = Y I

which incorporates the conditioning of the dynamics on noisy observations. For the full derivation
we refer the reader to Appendix B.4. We require boundary conditions for the evolution interval
in order to determine a unique solution to the set of equations (5) and (6). We thus set either
n in the case of noiseless observations  or – if the observations
Qn(x  0) = Y 0
have been corrupted by noise – Qn(x  0) = 1
2 and ρn(x  T ) = 1 as boundaries before and after the
ﬁrst and the last observation  respectively. The coupled set of ODEs can then be solved iteratively as a
ﬁxed-point procedure in the same manner as in previous works [16  5] (see Algorithm 1) in a forward-
backward procedure. In order to incorporate noisy observations into the CTBN dynamics  we need to
assume an observation model. In the following we assume that the data likelihood factorizes P (Y i |
n | Xn)  allowing us to condition on the data by enforcing limt→ti− ρn(x  t) =
limt→ti+ Pn(Y i | x)ρn(x  t). In Figure 2  we exemplify CTBN dynamics (N = 3) conditioned
on observations corrupted by independent Gaussian noise. We ﬁnd close agreement with the exact
posterior dynamics. Because we only need to solve 2N ODEs to approximate the dynamics of an
N-component system  we recover a linear complexity in the number of components  rendering our
method scalable.

X) = (cid:81)

n P (Y i

5

Figure 2: Dynamics in star approximation of a three node CTBN following Glauber dynamics at
a = 1 and b = 0.6 conditioned on noisy observations (diamonds). We plotted the expected state
(blue) plus variance (grey area). The observation model is the latent state plus gaussian random noise
of variance σ = 0.8 and zero mean. The latent state (dashed) is well estimated for X2  even when no
data has been provided. For comparison  we plotted the exact posterior mean (dots). We did not plot
the exact variance  which depends only on the mean  for better visibility.

3.2 Parameter estimation

Maximization of the variational lower bound with respect to transition rates Ru
expected result for the estimator of transition rates

n(x  x(cid:48)) yields the

(cid:48)
n(x  x

ˆRu

) =

E[M u
E[T u

n (x  x(cid:48))]
n (x)]

 

given the expected sufﬁcient statistics [15]

E[T u

n (x)] =

dt Qn(x  t)Qu

n  E[M u

(cid:48)
n (x  x

(cid:90) T

0

(cid:90) T

0

)] =

dt τ u

(cid:48)
n (x  x

  t) 

n (x)] is the expected dwelling time for the n’th node in state x and E[M u

n (x  x(cid:48))] are
where E[T u
the expected number of transitions from state x to x(cid:48)  both conditioned on the parents state u.
Following a standard expectation–maximization (EM) procedure  e.g. [16]  we can estimate the
systems’ parameters given the underlying network.

3.3 Benchmark

(cid:16)

(cid:17)(cid:17)

1 + x tanh

l∈pa(n) ul

b(cid:80)

In the following  we compare the accuracy of the star approximation with the naive mean-ﬁeld
(cid:16)
approximation. Throughout this section  we will consider a binary local state-space (spins) Xn =
{+1 −1}. We consider a system obeying Glauber dynamics [10] with the rates Ru
n(x −x) =
. Here  b is the inverse temperature of the system. With increasing
a
2
b  the dynamics of each node depend more strongly on the dynamics of its neighbors. This corresponds
to increasing the perturbation parameter ε. The pre-factor a scales the overall rate of the process.
This system is an appropriate toy-example for biological networks as it encodes additive threshold
behavior. In Figure 3 a) and c)  we show the mean-squared-error (MSE) between the expected
sufﬁcient statistics and the true ones for a tree network and an undirected chain with periodic
boundaries of eight nodes  so that comparison with the exact result is still tractable. In this application 
we restrict ourselves to noiseless observations to better connect to previous results as in [5]. We
compare the estimation of the evidence using the variational lower bound in Figure 3 b) and d). We
ﬁnd that while our estimate using the star approximation is a much closer approximation  it does not
provide a lower bound .

6

510152025-2-1.5-1-0.500.511.52510152025-2-1.5-1-0.500.511.52510152025-2-1.5-1-0.500.511.52X1X2X322010203011tE[X1]22010203011t22010203011tE[X2]E[X3]Figure 3: We perform inference on a tree network  see a) and b)  and an undirected chain  displayed
in c) and d). In both plots we consider CTBN of eight nodes with noiseless evidence as denoted in
sketch inlet (black: x = −1  white: x = 1) in a) and c) obeying Glauber dynamics with a = 8. In a) 
we plot the mean-squared-error (MSE) for the expected dwelling times (dashed) and the expected
number of transitions for the naive mean-ﬁeld (circle  red) and star approximation (diamond  blue)
with respect to the predictions of the exact simulation as a function of temperature b. In b) and d)  we
plot the approximation of logarithmic evidence as a function of temperature. We ﬁnd that for both
approximations (star approximation in blue  naive mean-ﬁeld in red dashed and exact result in black)
better performance on the tree network  while the star approximation clearly improves upon the naive
mean-ﬁeld approximation in both scenarios.

4 Cluster variational structure learning for CTBNs

For structure learning tasks  knowing the exact parameters of a model is in general unnecessary.
For this reason  we will derive a parameter-free formulation of the variational approximation for
the evidence lower bound and the latent state dynamics  analogous to the ones in the previous
section. We derive an approximate CTBN structure score  for which we need to marginalize over
the parameters of the variational lower bound. To this end  we assume that the parameters of the
CTBN are random variables distributed according to a product of local and independent Gamma
n(x)] given a graph
structure G. In star approximation  the evidence is approximately given by P (Y | R G) ≈ exp(FS).
By a simple analytical integration  we recover an approximation to the CTBN structure score

distributions P (R | α  β G) =(cid:81)
(cid:90) ∞

n(x  x(cid:48)) | αu

n(x  x(cid:48))  βu

x(cid:48)(cid:54)=x Gam [Ru

(cid:81)

(cid:81)

n

x u

(cid:18)

P (G | Y  α  β) ≈ P (G)

0

∝ eH(cid:89)

n

(cid:89)

(cid:89)

x u

x(cid:48)(cid:54)=x

dR e

FS P (R | α  β G)
(cid:19)αu

n(x x(cid:48)

βu
n(x)
n (x)] + βu

(E[T u

n(x))M u

n (x x(cid:48))

) Γ (E[M u

n (x  x(cid:48))] + αn
n(x  x(cid:48)))
Γ (αu

n(x  x(cid:48)))

 

(8)

with Γ being the Gamma-function. The approximated CTBN structure score still satisﬁes structural
modularity  if not broken by the structure prior P (G). However  an implementation of a k-learn
structure learning strategy as originally proposed in [14] is prohibited  as the latent state estimation
depends on the entire network. For a detailed derivation  see Appendix B.5. Finally  we note that 
in contrast to the evidence in Figure 3  we have no analytical expression for the structure score (the
integral is intractable)  so that we can not compare with the exact result after integration.

4.1 Marginal dynamics of CTBNs

The evaluation of the approximate CTBN structure score requires the calculation of the latent
state dynamics of the marginal CTBN. For this  we approximate the Gamma function in (8) via
Stirling’s approximation. As Stirling’s approximation becomes accurate asymptotically  we imply
that sufﬁciently many transitions have been recorded across samples or have been introduced via a
sufﬁciently strong prior assumption. By extremization of the marginal variational lower bound  we
recover a set of integro-differential equations describing the marginal self-exciting dynamics of the
CTBN (see Appendix B.6). Surprisingly  the only difference of this parameter-free version compared
to (5) and (6) is that the conditional intensity matrix has been replaced by its posterior estimate

(9)
n(x  x(cid:48)) is thus determined recursively by the dynamics generated by itself  conditioned
The rate ¯Ru
on the observations and prior information. We notice the similarity of our result to the one recovered

) ≡

¯Ru

.

(cid:48)
n(x  x

E[M u

n (x  x(cid:48))] + αu
E[T u
n (x)] + βu

n(x  x(cid:48))
n(x)

7

12345678910-11-10-9-8-7-6-5b)119571234567891000.050.10.150.20.250.30.350.4bMSEa)lnˆP(Y)b00.10.20.30.400.5112345678910-7.5-7-6.5-6-5.5-5-4.5-4-3.5-3d)6754312345678910012345678MSEc)bb24680lnˆP(Y)00.5100.5100.51Table 1: Experimental results with datasets generated from random CTBNs (N = 5) with families
of up to kmax parents. To demonstrate that our score prevents over-ﬁtting we search for families of
up to k = 2 parents. When changing one parameter the other default values are ﬁxed to D = 10 
b = 0.6 and σ = 0.2.

kmax
1

Experiment
Number of
trajectories

2

Measurement
noise
Number of
trajectories

Measurement
noise

Variable AUROC

D = 5
D = 10
D = 20

σ = 0.6
σ = 1.0

D = 5
D = 10
D = 20

σ = 0.6
σ = 1.0

0.78± 0.03
0.87± 0.03
0.96± 0.02
0.81± 0.10
0.69± 0.07
0.64± 0.09
0.68± 0.12
0.75± 0.11
0.71± 0.13
0.64± 0.11

AUPR
0.64± 0.01
0.76± 0.00
0.92± 0.00
0.71± 0.00
0.49± 0.01
0.50± 0.17
0.54± 0.14
0.68± 0.16
0.58± 0.20
0.53± 0.15

in [21]  where  however  the expected sufﬁcient statistics had to be computed self-consistently during
each sample path. We employ a ﬁxed-point iteration scheme to solve the integro-differential equation
for the marginal dynamics in a manner similar to EM (for the detailed algorithm  see Appendix A.2).

5 Results and discussion

For the purpose of learning  we employ a greedy hill-climbing strategy. We exhaustively score all
possible families for each node with up to k parents and set the highest scoring family as the current
one. We do this repeatedly until our network estimate converges  which usually takes only two of such
sweeps. We can transform the scores to probabilities and generate Reciever-Operator-Characteristics
(ROCs) and Precision-Recall (PR) curves by thresholding the averaged graphs. As a measure of
performance  we calculate the averaged Area-Under-Curve (AUC) for both. We evaluate our method
using both synthetic and real-world data from molecular biology 2. In order to stabilize our method
in the presence of sparse data  we augment our algorithm with a prior α = 5 and β = 10  which is
uninformative of the structure  for both experiments. We want to stress that  while we removed the
bottleneck of exponential scaling of latent state estimation of CTBNs  Bayesian structure learning via
scoring still scales super-exponentially in the number of components [9]. Our method can thus not be
compared to shrinkage based network inference methods such as fused graphical lasso.
The synthetic experiments are performed on CTBNs encoded with Glauber dynamics. For each of
the D trajectories  we recorded 10 observations Y i at random time-points ti and corrupted them with
Gaussian noise with variance σ = 0.6 and zero mean. In Table 1  we apply our method to random
graphs consisting of N = 5 nodes and up to kmax parents. We note that ﬁxing kmax does not ﬁx the
possible degree of the node (which can go up to N − 1). For random graphs with kmax = 1  our
method performs best  as expected  and we are able to reliably recover the correct graph if enough
data are provided. To demonstrate that our score penalizes over-ﬁtting  we search for families of up to
k = 2 parents. For the more challenging scenario of kmax = 2  we ﬁnd a drop in performance. This
can be explained by the presence of strong correlations in more connected graphs and the increased
model dimension with larger kmax. In order to prove that our method outperforms existing methods
in terms of scalability  we successfully learn a tree-network  with a leaf-to-root feedback  of 14 nodes
with a = 1  b = 0.6  see Figure 4 II). This is the largest inferred CTBN from incomplete data reported
(in [21] a CTBN of 11 nodes is learned  albeit with incomparably larger computational effort).
Finally  we apply our algorithm to the In vivo Reverse-engineering and Modeling Assessment (IRMA)
network [4]  a gene regulatory network that has been implemented on cultures of yeast  as a benchmark
for network reconstruction algorithms  see Figure 4 I). Special care has been taken in order to isolate
this network from crosstalk with other cellular components. It is thus  to best of our knowledge 
the only molecular biological network with a ground truth. The authors of [4] provide time course
data from two perturbation experiments  referred to as “switch on” and “switch off”  and attempted

2Our toolbox and code for experiments are available at https://github.com/dlinzner-bcs/.

8

Figure 4: I) Reconstruction of a gene regulatory network (IRMA) from real-world data. To the left we
show the inferred network for the “switch off” and “switch on” dataset. The ground truth network is
displayed by black thin edges  the correctly inferred edges are thick (all inferred edges were correct).
The the red edge was identiﬁed only in “switch on”  the teal edge only in “switch off”. On the right
we show a small table summarizing the reconstruction capabilities of our method  TSNI and BANJO
(PPV of random guess is 0.5). II) Reconstruction of large graphs. We tested our method on a ground
truth graph with 14 nodes  as displayed in a) with node-relations sketched in the inlet  encoded with
Glauber dynamics and searched for a maximum of k = 1 parents. Although we used relatively few
observations that have been strongly corrupted  the averaged learned graph b) is visibly close to the
ground truth. We framed the prediction of the highest scoring graph  where correctly learned edges
are framed white and the incorrect ones are framed red.

reconstruction using different methods. In order to compare to their results we adopt their metrics
Positive Predicted Value (PPV) and the Sensitivity score (SE) [2]. The best performing method is
ODE-based (TSNI [3]) and required additional information on the perturbed genes in each experiment 
which may not always be available. As can be seen in Figure 4 I)  our method performs accurately on
the “switch off” and the “switch on” data set regarding the PPV. The SE is slightly worse than for
TSNI on “switch off”. In both cases  we perform better than the other method based on Bayesian
networks (BANJO [24]). Lastly  we note that in [1] more correct edges could be inferred using
CTBNs  however with parameters tuned with respect to the ground truth to reproduce the IRMA
network. For details on our processing of the IRMA data  see Appendix C. For comparison with other
methods tested in [18] we refer to Appendix D where our method is consistently a top performer
using AUROC and AUPR as metrics.

6 Conclusion

We develop a novel method for learning directed graphs from incomplete and noisy data based on
a continuous-time Bayesian network. To this end  we approximate the exact but intractable latent
process by a simpler one using cluster variational methods. We recover a closed set of ordinary
differential equations that are simple to implement using standard solvers and retain a consistent and
accurate approximation of the original process. Additionally  we provide a close approximation to
the evidence in the form of a variational lower bound that can be used for learning tasks. Lastly  we
demonstrate how marginal dynamics of continuous-time Bayesian networks  which only depend on
data  prior assumptions  and the underlying graph structure  can be derived by the marginalization of
the variational lower bound. Marginalization of the variational lower bound provides an approximate
structure score. We use this to detect the best scoring graph using a greedy hill-climbing procedure.
It would be beneﬁcial to identify higher-order approximations of the variational lower bound in the
future. We test our method on synthetic as well as real data and show that our method produces
meaningful results while outperforming existing methods in terms of scalability.

Acknowledgements

We thank the anonymous reviewers for helpful comments on the previous version of this manuscript.
Dominik Linzner is funded by the European Union’s Horizon 2020 research and innovation pro-
gramme under grant agreement 668858.

9

2468101214246810121400.10.20.30.40.50.60.70.80.9246810121424681012141114700.514171417PPV=0.77a)b)II)SWI5CBF1ASH1PPV=1SE=0.5PPV=1SE=0.5Switch onSwitch oﬀI)PPV=1SE=0.67PPV=0.75SE=0.5TSNICTBNTSNICTBNBANJOPPV=0.5SE=0.33BANJOPPV=0.33SE=0.33GLA4\GLA80References
[1] Enzo Acerbi  Teresa Zelante  Vipin Narang  and Fabio Stella. Gene network inference us-
ing continuous time Bayesian networks: a comparative study and application to Th17 cell
differentiation. BMC Bioinformatics  15  2014.

[2] Mukesh Bansal  Vincenzo Belcastro  Alberto Ambesi-Impiombato  and Diego di Bernardo.
How to infer gene networks from expression proﬁles. Molecular systems biology  3:78  2007.

[3] Mukesh Bansal  Giusy Della Gatta  and Diego di Bernardo. Inference of gene regulatory net-
works and compound mode of action from time course gene expression proﬁles. Bioinformatics 
22(7):815–822  apr 2006.

[4] Irene Cantone  Lucia Marucci  Francesco Iorio  Maria Aurelia Ricci  Vincenzo Belcastro 
Mukesh Bansal  Stefania Santini  Mario Di Bernardo  Diego di Bernardo  and Maria Pia Cosma.
A Yeast Synthetic Network for In Vivo Assessment of Reverse-Engineering and Modeling
Approaches. Cell  137(1):172–181  apr 2009.

[5] Ido Cohn  Tal El-Hay  Nir Friedman  and Raz Kupferman. Mean ﬁeld variational approximation
for continuous-time Bayesian networks. Journal Of Machine Learning Research  11:2745–2783 
2010.

[6] Tal El-Hay  Ido Cohn  Nir Friedman  and Raz Kupferman. Continuous-Time Belief Propagation.
Proceedings of the 27th International Conference on Machine Learning  pages 343–350  2010.

[7] Tal El-Hay  R Kupferman  and N Friedman. Gibbs sampling in factorized continuous-time
Markov processes. Proceedings of the 22th Conference on Uncertainty in Artiﬁcial Intelligence 
2011.

[8] Yu Fan and CR Shelton. Sampling for approximate inference in continuous time Bayesian

networks. AI and Math  2008.

[9] Nir Friedman  Lise Getoor  Daphne Koller  and Avi Pfeffer. Learning Probabilistic Relational
Models. In Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence
(IJCAI-99)  August 1999.

[10] Roy J Glauber. Time-Dependent Statistics of the Ising Model. J. Math. Phys.  4(1963):294–307 

1963.

[11] Ryoichi Kikuchi. A theory of cooperative phenomena. Physical Review  81(6):988–1003  mar

1951.

[12] Michael Klann and Heinz Koeppl. Spatial Simulations in Systems Biology: From Molecules to

Cells. International Journal of Molecular Sciences  13(6):7798–7827  2012.

[13] Uri Nodelman  Christian R Shelton  and Daphne Koller. Continuous Time Bayesian Networks.
Proceedings of the 18th Conference on Uncertainty in Artiﬁcial Intelligence  pages 378–387 
1995.

[14] Uri Nodelman  Christian R. Shelton  and Daphne Koller. Learning continuous time Bayesian
networks. Proceedings of the 19th Conference on Uncertainty in Artiﬁcial Intelligence  pages
451–458  2003.

[15] Uri Nodelman  Christian R Shelton  and Daphne Koller. Expectation Maximization and Complex
Duration Distributions for Continuous Time Bayesian Networks. Proc. Twenty-ﬁrst Conference
on Uncertainty in Artiﬁcial Intelligence  pages pages 421–430  2005.

[16] Manfred Opper and Guido Sanguinetti. Variational inference for Markov jump processes.

Advances in Neural Information Processing Systems 20  pages 1105–1112  2008.

[17] Alessandro Pelizzola and Marco Pretti. Variational approximations for stochastic dynamics on

graphs. Journal of Statistical Mechanics: Theory and Experiment  2017(7):1–28  2017.

[18] Christopher A. Penfold and David L. Wild. How to infer gene networks from expression proﬁles 

revisited. Interface Focus  1(6):857–870  dec 2011.

10

[19] Vinayak Rao and Yee Whye Teh. Fast MCMC sampling for Markov jump processes and

extensions. Journal of Machine Learning Research  14:3295–3320  2012.

[20] Eric E Schadt  John Lamb  Xia Yang  Jun Zhu  Steve Edwards  Debraj Guha Thakurta  Solveig K
Sieberts  Stephanie Monks  Marc Reitman  Chunsheng Zhang  Pek Yee Lum  Amy Leonardson 
Rolf Thieringer  Joseph M Metzger  Liming Yang  John Castle  Haoyuan Zhu  Shera F Kash 
Thomas A Drake  Alan Sachs  and Aldons J Lusis. An integrative genomics approach to infer
causal associations between gene expression and disease. Nature Genetics  37(7):710–717  jul
2005.

[21] Lukas Studer  Christoph Zechner  Matthias Reumann  Loic Pauleve  Maria Rodriguez Mar-
tinez  and Heinz Koeppl. Marginalized Continuous Time Bayesian Networks for Network
Reconstruction from Incomplete Observations. Proceedings of the 30th Conference on Artiﬁcial
Intelligence (AAAI 2016)  pages 2051–2057  2016.

[22] Eduardo Domínguez Vázquez  Gino Del Ferraro  and Federico Ricci-Tersenghi. A simple
analytical description of the non-stationary dynamics in Ising spin systems. Journal of Statistical
Mechanics: Theory and Experiment  2017(3):033303  2017.

[23] Jonathan S Yedidia  William T Freeman  and Yair Weiss. Bethe free energy  Kikuchi approx-
imations  and belief propagation algorithms. Advances in neural information  13:657–663 
2000.

[24] Jing Yu  V. Anne Smith  Paul P. Wang  Alexander J. Hartemink  and Erich D. Jarvis. Advances
to Bayesian network inference for generating causal networks from observational biological
data. Bioinformatics  20(18):3594–3603  dec 2004.

11

,Dominik Linzner
Heinz Koeppl