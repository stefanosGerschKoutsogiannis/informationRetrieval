2017,Graph Matching via Multiplicative Update Algorithm,As a fundamental problem in computer vision  graph matching problem can usually be formulated as a Quadratic Programming (QP) problem with doubly stochastic and discrete (integer) constraints. Since it is NP-hard  approximate algorithms are required. In this paper  we present a new algorithm  called Multiplicative Update Graph Matching (MPGM)  that develops a multiplicative update technique to solve the QP matching problem. MPGM has three main benefits: (1) theoretically  MPGM solves the general QP problem with doubly stochastic constraint naturally whose convergence and KKT optimality are guaranteed. (2) Em- pirically  MPGM generally returns a sparse solution and thus can also incorporate the discrete constraint approximately. (3) It is efficient and simple to implement. Experimental results show the benefits of MPGM algorithm.,Graph Matching via Multiplicative Update Algorithm

Bo Jiang

Jin Tang

School of Computer Science

School of Computer Science

Chris Ding

CSE Department 

University of Texas at

Arlington  Arlington  USA

and Technology

Anhui University  China
jiangbo@ahu.edu.cn

and Technology

Anhui University  China

tj@ahu.edu.cn

chqding@uta.edu

Yihong Gong

School of Electronic

and Information Engineering

Xi’an Jiaotong University  China
ygong@mail.xjtu.edu.cn

Bin Luo

School of Computer Science

and Technology 

Anhui University  China
luobin@ahu.edu.cn

Abstract

As a fundamental problem in computer vision  graph matching problem can
usually be formulated as a Quadratic Programming (QP) problem with doubly
stochastic and discrete (integer) constraints. Since it is NP-hard  approximate
algorithms are required. In this paper  we present a new algorithm  called Multi-
plicative Update Graph Matching (MPGM)  that develops a multiplicative update
technique to solve the QP matching problem. MPGM has three main beneﬁts: (1)
theoretically  MPGM solves the general QP problem with doubly stochastic con-
straint naturally whose convergence and KKT optimality are guaranteed. (2) Em-
pirically  MPGM generally returns a sparse solution and thus can also incorporate
the discrete constraint approximately. (3) It is efﬁcient and simple to implement.
Experimental results show the beneﬁts of MPGM algorithm.

1 Introduction

In computer vision and machine learning area  many problems of interest can be formulated by
graph matching problem. Previous approaches [3–5  15  16] have formulated graph matching as a
Quadratic Programming (QP) problem with both doubly stochastic and discrete constraints. Since
it is known to be NP-hard  many approximate algorithms have been developed to ﬁnd approximate
solutions for this problem [8  16  21  24  20  13].
One kind of approximate methods generally ﬁrst develop a continuous problem by relaxing the dis-
crete constraint and aim to ﬁnd the optimal solution for this continuous problem. After that  they
obtain the ﬁnal discrete solution by using a discretization step such as Hungarian or greedy algo-
rithm [3  15  16]. Obviously  the discretization step of these methods is generally independent of the
matching objective optimization process which may lead to weak local optimum for the problem.
Another kind of methods aim to obtain a discrete solution for QP matching problem [16  1  24].
For example  Leordeanu et al. [16] proposed an iterative matching method (IPFP) which optimized
the QP matching problem in a discrete domain. Zhou et al. [24  25] proposed an effective graph
matching method (FGM) which optimized the QP matching problem approximately using a convex-
concave relaxation technique [21] and thus returns a discrete solution for the problem. From opti-
mization aspect  the core optimization algorithm used in both IPFP [16] and FGM [24] is related to
Frank-Wolfe [9] algorithm and FGM [24  25] further uses a path following procedure to alleviate the
local-optimum problem more carefully. The core of Frank-Wolfe [9] algorithm is to optimize the
quadratic problem by sequentially optimizing the linear approximations of QP problem. In addition

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

to optimization-based methods  probabilistic methods can also be used for solving graph matching
problems [3  19  23].
In this paper  we propose a new algorithm  called Multiplicative Update Graph Matching (MPGM) 
that develops a multiplicative update technique for the general QP problem with doubly stochas-
tic constraint. Generally  MPGM has the following three main aspects. First  MPGM solves the
general QP problem with doubly stochastic constraint directly and naturally. In MPGM algorithm 
each update step has a closed-form solution and the convergence of the algorithm is also guaranteed.
Moreover  the converged solution is guaranteed to be Karush-Kuhn-Tucker (KKT) optimality. Sec-
ond  empirically  MPGM can generate a sparse solution and thus incorporates the discrete constraint
naturally in optimization. Therefore  MPGM can obtain a local optimal discrete solution for the
QP matching problem. Third  it is efﬁcient and simple to implement. Experimental results on both
synthetic and real-world matching tasks demonstrate the effectiveness and beneﬁts of the proposed
MPGM algorithm.

2 Problem Formulation and Related Works

′

′

; E

= (V

∈ V

′
j)  there is an afﬁnity Sa(ai; a′

′. Also  for each correspondence pair (vi; v

′
Problem Formulation. Assume G = (V; E) and G
) are two attributed graphs to be
matched  where each node vi ∈ V or edge eik ∈ E has an attribute vector ai or rik. The aim of graph
′. For each corre-
matching problem is to establish the correct correspondences between V and V
j) that measures how well node vi ∈ V matches node
spondence (vi; v
′
′
l)  there is an afﬁnity Sr(rik; r′
′
j) and (vk; v
v
jl)
′
′
j
l). One can deﬁne an afﬁnity
that measures the compatibility between node pair (vi; vk) and (v
j; v
matrix W whose diagonal term Wij;ij represents Sa(ai; a′
j)  and the non-diagonal element Wij;kl
contains Sr(rik; r′
jl). The one-to-one correspondences can be represented by a permutation matrix
X ∈ {0; 1}n×n  where n = |V | = |V
′|1. Here  Xij = 1 implies that node vi in G corresponds to
′  and Xij = 0 otherwise. In this paper  we denote x = (X11:::Xn1; :::; X1n:::Xnn)T
node v
as a column-wise vectorized replica of X. The graph matching problem is generally formulated as
a Quadratic Programming (QP) problem with doubly stochastic and discrete constraints [16  3  10] 
i.e. 

′
j in G

where P is deﬁned as 

P = {x | ∀i

x∗

(xTWx)

x

n

= arg max

∑
j=1 xij = 1;∀j
∑

j=1 xij = 1;∀j

n

n

s:t: x ∈ P;

∑
i=1 xij = 1; xij ∈ {0; 1}}
∑

i=1 xij = 1; xij ≥ 0}:

n

D = {x|∀i

(1)

(2)

(3)

(4)

The above QP problem is NP-hard and thus approximate relaxations are usually required. One
popular way is to relax the permutation domain P to the doubly stochastic domain D 

That is solving the following relaxed matching problem [21  20  10] 
s:t: x ∈ D:

= arg max

(xTWx)

x∗

x

Since W is not necessarliy positive (or negative) semi-deﬁnite  thus this problem is generally not a
concave or convex problem.
Related Works. Many algorithms have been proposed to ﬁnd a local optimal solution for the above
QP matching problem (Eq.(4)). One kind of popular methods is to use constraint relaxation and pro-
jection  such as GA [10] and RRWM [3]. Generally  they iteratively conduct the following two steps:
(a) searching for a solution by ignoring the doubly stochastic constraint temporarily; (b) Projecting
the current solution onto the desired doubly stochastic domain to obtain a feasible solution. Note
that the projection step (b) is generally independent of the optimization step (a) and thus may lead to
weak local optimum. Another kind of important methods is to use objective function approximation
and thus solves the problem approximately  such as Frank-Wolfe algorithm [9]. Frank-Wolfe aims
to optimize the above quadratic problem by sequentially solving the approximate linear problems.
This algorithm has been widely adopted in many recent matching methods [16  24  21]  such as IPFP
[16] and FGM [24].

1Here  we focus on equal-size graph matching problem. For graphs with different sizes  one can add dummy

isolated nodes into the smaller graph and transform them to equal-size case [21  10]

2

3 Algorithm

[

]1=2

−
l

Our aim in this paper is to develop a new algorithm to solve the general QP matching problem Eq.(4).
We call it as Multiplicative Update Graph Matching (MPGM). Formally  starting with an initial
solution vector x(0)  MPGM solves the problem Eq.(4) by iteratively updating a current solution
vector x(t); t = 0; 1::: as follows 

x(t+1)
kl
k = (|(cid:3)k| + (cid:3)k)=2  (cid:3)

−
2(Wx(t))kl + (cid:3)
k + (cid:0)
= x(t)
kl
k + (cid:0)+
k = (|(cid:3)k| − (cid:3)k)=2  (cid:0)+
−
)−1
where (cid:3)+
and the Lagrangian multipliers ((cid:3); (cid:0)) are computed as 

[

(cid:3)+

l

(
(
I − X(t)TX(t)

(
K(t)X(t)T) − X(t)(cid:0)

diag

) − X(t)T

K(t)TX(t)

(cid:0) =2

;

(5)
k = (|(cid:0)k| − (cid:0)k)=2 
k = (|(cid:0)k| + (cid:0)k)=2  (cid:0)
−

(

K(t)X(t)T)]

diag

(cid:3) =2 diag

kl = x(t)

kl = (Wx(t))kl; X(t)

(6)
where K(t)  X(t) are the matrix forms of vector (Wx(t)) and x(t)  respectively  i.e.  K(t); X(t) ∈
kl . (cid:3) = ((cid:3)1;··· (cid:3)n)T ∈ Rn×1; (cid:0) = ((cid:0)1;··· (cid:0)n)T ∈
Rn×n and K(t)
Rn×1. The iteration starts with an initial x(0) and is repeated until convergence.
Complexity. The main complexity in each iteration is on computing Wx(t). Thus  the total com-
putational complexity for MPGM is less than O(M N 2)  where N = n2 is the length of vector x(t)
and M is the maximum iteration. Our experience is that the algorithm converges quickly and the
average maximum iteration M is generally less than 200. Theoretically  the complexity of MPGM
is the same with RRWM [3] and IPFP [16]  but obviously lower than GA [10] and FGM [24].
Comparison with Related Works. Multiplicative update algorithms have been studied in solving
matching problems [6  13  11  12]. Our work is signiﬁcantly different from previous works in the
following aspects. Previous works [6  13  11] generally ﬁrst develop a kind of approximation (or
relaxation) for QP matching problem by ignoring the doubly stochastic constraint  and then aim
to ﬁnd the optimum of the relaxation problem by developing an algorithm. In contrast  our work
focus on the general and challengeable QP problem with doubly stochastic constraint (Eq.(4))  and
derive a simple multiplicative algorithm to solve the problem Eq.(4) directly. Note that  the proposed
algorithm is not limited to solving QP matching problem only. It can also be used in some other QP
(or general continuous objective function) problems with doubly stochastic constraint (e.g. MAP
inference  clustering) in machine learning area. In this paper  we focus on graph matching problem.
Starting Point. To alleviate the local optima and provide a feasible starting point for MPGM algo-
rithm  given an initial vector x(0)  we ﬁrst use the simple projection x(0) = P (Wx(0)) several times
to obtain a kind of the feasible start point for MPGM algorithm. Here P denotes the projection [22]
or normalization [20] to make x(0) satisfy the doubly stochastic constraint.

4 Theoretical Analysis
Theorem 1. Under update Eq.(5)  the Lagrangian function L(x) is monotonically increasing 

L(x) = xTWx (cid:0) n∑

n∑

xij (cid:0) 1) (cid:0) n∑

Λi(

n∑

Γj(

xij (cid:0) 1)

(7)

i=1
where (cid:3); (cid:0) are Lagrangian multipliers.
Proof. To prove it  we use the auxiliary function approach [7  14]. An auxiliary function function
(cid:8)(x; ~x) of Lagrangian function L(x) satisﬁes following 

j=1

j=1

i=1

(cid:8)(x; x) = L(x); (cid:8)(x; ~x) ≤ L(x):

Using the auxiliary function (cid:8)(x; ~x)  we deﬁne

x(t+1) = arg max

x

(cid:8)(x; x(t)):

3

(8)

(9)

Then by construction of (cid:8)(x; ~x)  we have

L(x(t)) = (cid:8)(x(t); x(t)) ≤ L(x(t+1)):

This proves that L(x(t)) is monotonically increasing.
The main step in the following of the proof is to provide an appropriate auxiliary function and ﬁnd
the global maximum for the auxiliary function. We rewrite Eq.(7) as
xij (cid:0) 1)

n∑

Λi(

Γj(

=

i=1

j=1

j=1

n∑

xij (cid:0) 1) (cid:0) n∑
L(x) = xT Wx (cid:0) n∑
Wij;klxijxkl (cid:0) n∑
n∑
n∑
n∑
n∑
(
n∑
n∑
n∑
n∑
We show that one auxiliary function (cid:8)(x; ~x) of L(x) is 
[ n∑
n∑
[ n∑
n∑

]
+ ~xij) (cid:0) 1
]
+ ~xij) (cid:0) 1

(cid:0) n∑
(cid:0) n∑

Wij;kl~xij~xkl

k=1
x2
ij
~xij

(cid:8)(x  ~x) =

Λ+
i

Λi(

1
2

k=1

j=1

j=1

j=1

j=1

i=1

i=1

i=1

i=1

i=1

+

+

l=1

l=1

(

Γ+
j

(cid:0)
Λ
i

(cid:0)
Γ
j

1
2

(

x2
ij
~xij

1 + log

[ n∑
[ n∑

j=1

i=1

n∑
xij (cid:0) 1) (cid:0) n∑
)

j=1

xijxkl
~xij~xkl

j=1

i=1

j=1

i=1

]
) (cid:0) 1
]

) (cid:0) 1

.

~xij(1 + log

~xij(1 + log

xij
~xij

xij
~xij

n∑

Γj(

i=1

xij (cid:0) 1).

(10)

(11)

(12)

Using the inequality z ≥ 1 + log z and ab ≤ 1
2 ( a2
b + b))  one can prove that Eq.(12)
is a lower bound of Eq.(11). Thus  Z(x; ~x) is an auxiliary function of L(x). According to Eq.(9)  we
need to ﬁnd the global maximum of (cid:8)(x; ~x) for x. The gradient is

2 (a2 + b2)(a ≤ 1

[(

@(cid:8)(x; ~x)

@xkl

= 2(W~x)kl

~xkl
xkl

− (cid:3)+

k

xkl
~xkl

−
k

~xkl
xkl

− (cid:0)+

l

xkl
~xkl

−
+ (cid:0)
l

~xkl
xkl

Note that  for graph matching problem  we have WT = W. Thus  the second derivative is
(cid:14)ki(cid:14)lj ≤ 0;

−
−
2(W~x)kl + (cid:3)
k + (cid:0)
l

k + (cid:0)+
l )

= −

((cid:3)+

+

@2(cid:8)(x; ~x)
@xkl@xij

1
~xkl

(13)

Therefore  (cid:8)(x; ~x) is a concave function in x and has a unique global maximum. It can be obtained
by setting the ﬁrst derivative to zero ( @(cid:8)(x;~x)
@xkl

+ (cid:3)

) ~xkl

x2
kl

]

[

= 0)  which gives
−
−
k + (cid:0)
l

2(W~x)kl + (cid:3)

(cid:3)+

k + (cid:0)+

l

]1=2

xkl = ~xkl

:

(14)

Therefore  we obtain the update rule in Eq.(5) by setting x(t+1) = x and x(t) = ~x. (cid:3)
Theorem 2. Under update Eq.(5)  the converged solution x∗ is Karush-Kuhn-Tucker (KKT) optimal.
Proof. The standard Lagrangian function is

L(x) = xTWx (cid:0) n∑

n∑

xij (cid:0) 1) (cid:0) n∑

Λi(

n∑

xij (cid:0) 1) (cid:0) n∑

n∑

Γj(

∆ijxij

(15)

Here  we use the Lagrangian function to induce KKT optimal condition. Using Eq.(15)  we have

i=1

j=1

j=1

i=1

i=1

j=1

The corresponding KKT condition is
∂L(x)
∂xkl

∂L(x)
∂xkl

= 2(Wx)kl (cid:0) (cid:21)k (cid:0) (cid:22)l.

= 2(Wx)kl (cid:0) Λk (cid:0) Γl (cid:0) ∆kl = 0
xkl (cid:0) 1) = 0

= (cid:0)(

∑
∑

l

∂L(x)
∂Λk
∂L(x)
∂Γl

xkl (cid:0) 1) = 0

k

∆klxkl = 0.

= (cid:0)(

4

(16)

(17)

(18)

(19)

(20)

This leads to the following KKT complementary slackness condition 

(21)
k xkl = 1  summing over indexes k and l respectively  we obtain the follow-

xkl = 0:

∑

∑

Because
ing two group equations 

l xkl = 1;

]

[
2(Wx)kl − (cid:3)k − (cid:0)l
xkl(Wx)kl − n∑
n∑
n∑
xkl(Wx)kl − n∑

l=1

l=1

k=1

k=1

2

2

(cid:0)lxkl − (cid:3)k = 0;

(cid:3)kxkl − (cid:0)l = 0:

(22)

(23)

Eqs.(22  23) can be equivalently reformulated as the following matrix forms 

2 diag(KXT) − (cid:3) − X(cid:0) = 0;
2 diag(KTX) − (cid:0) − XT(cid:3) = 0:

(24)
(25)
where k = 1; 2;··· n  l = 1; 2;··· n. K  X are the matrix forms of vector (Wx) and x  respectively 
i.e.  K; X ∈ Rn×n and Kkl = (Wx)kl; Xkl = xkl. Thus  we can obtain the values for (cid:3) and (cid:0) as 
(26)
(27)

On the other hand  from update Eq.(5)  at convergence 
)kl + (cid:3)
k + (cid:0)+
(cid:3)+

2(Wx∗

(cid:0) = 2(I − XTX)
−1(diag(KTX) − XT diag(KXT))
(cid:3) = 2 diag(KXT) − X(cid:0)
[
)kl − (cid:3)k − (cid:0)l)x∗2
2(Wx∗

kl = x∗
x∗
[

]1=2

−
−
k + (cid:0)
l

]

kl

l

kl = 0  which is identical to the following KKT condition 
)kl − (cid:3)k − (cid:0)l
(29)

x∗
kl = 0:

Thus  we have (2(Wx∗

Substituting the values of (cid:3)k; (cid:0)l in Eq.(28) from Eqs.(26 27)  we obtain update rule Eq.(5). (cid:3)
Remark. Similar to the above analysis  we can also derive another similar update as 

(28)

x(t+1)
kl

= x(t)
kl

−
−
2(Wx(t))kl + (cid:3)
k + (cid:0)
l
k + (cid:0)+

(cid:3)+

l

:

(30)

The optimality and convergence of this update are also guaranteed. We omit the further discussion
of them due to the lack of space. In real application  one can use both of these two update algorithms
(Eq.(5)  Eq.(30)) to obtain better results.

5 Sparsity and Discrete Solution

One property of the proposed MPGM is that it can result in a sparse optimal solution  although the
discrete binary constraint have been dropped in MPGM optimization process. This suggests that
MPGM can search for an optimal solution nearly on the permutation domain P  i.e.  the boundary
of the doubly stochastic domain D. Unfortunately  here we cannot provide a theoretical proof on the
sparsity of MPGM solution  but demonstrate it experimentally.
Figure 1 (a) shows the solution x(t) across different iterations. Note that  regardless of initialization 
as the iteration increases  the solution vector x(t) of MPGM becomes more and more sparse and
converges to a discrete binary solution. Note that  in MPGM update Eq.(5)  when xt
kl closes to zero 
it can keep closing to zero in the following update process because of the particular multiplicative
operation. Therefore  as the iteration increases  the solution vector xt+1 is guaranteed to be more
sparse than solution vector xt. Figure 1 (b) shows the objective and sparsity2 of the solution vector
x(t). We can observe that (1) the objective of x(t) increases and converges after some iterations 
demonstrating the convergence of MPGM algorithm. (2) The sparsity of the solution x(t) increases
and converges to the baseline  which demonstrates the ability of MPGM algorithm to maintain the
discrete constraint in the converged solution.

2Sparsity measures the percentage of zero (close-to-zero) elements in Z. Firstly  set the threshold ϵ =
0.001 (cid:2) mean(Z)  then renew Zij = 0 if Zij (cid:20) ϵ. Finally  the sparsity is deﬁned as the percentage of zero
elements in the renewed Z.

5

Figure 1: (a) Solution vector x(t) of MPGM across different iterations (top: start from uniform
solution; middle: start from SM solution; bottom: start from RRWM solution).

6 Experiments

We have applied MPGM algorithm to several matching tasks. Our method has been compared with
some other state-of-the-art methods including SM [15]  IPFP [16]  SMAC [5]  RRWM [3] and FGM
[24]. We implemented IPFP [16] with two versions: (1) IPFP-U that is initialized by the uniform
solution; (2) IPFP-S that is initialized by SM method [15]. In experiments  we initialize our MPGM
with uniform solution and obtain similar results when initializing with SM solution.

6.1 Synthetic Data

Similar to the works [3  24]  we have randomly generated data sets of nin 2D points as inlier nodes
′ by transforming the whole point set with
for G. We obtain the corresponding nodes in graph G
a random rotation and translation and then adding Gaussian noise N (0; (cid:27)) to the point positions
from graph G. In addition  we also added nout outlier nodes in both graphs respectively at random
∥2
positions. The afﬁnity matrix W has been computed as Wij;kl = exp(−∥rik − r′
F =0:0015) 
where rik is the Euclidean distance between two nodes in G and similarly for r′
jl.
Figure 2 summarizes the comparison results. We can note that: (1) similar to IPFP [16] and FGM
[24] which return discrete matching solutions  MPGM always generates sparse solutions on doubly
stochastic domain. (2) MPGM returns higher objective score and accuracy than IPFP [16] and FGM
[24] methods  which demonstrate that MPGM can ﬁnd the sparse solution more optimal than these
methods. (3) MPGM generally performs better than the continuous domain methods including SM
[15]  SMAC [5] and RRWM [3]. Comparing with these methods  MPGM incorporates the doubly
stochastic constraint more naturally and thus ﬁnds the solution more optimal than RRWM method.
(4) MPGM generally has similar time cost with RRWM [3]. We have not shown the time cost of
FGM [24] method in Fig.2  because FGM uses a hybrid optimization method and has obviously
higher time cost than other methods.

jl

6.2

Image Sequence Data

In this section  we perform feature matching on CMU and YORK house sequences [3  2  18]. For
CMU "hotel" sequence  we have matched all images spaced by 5  10 ··· 75 and 80 frames and com-
puted the average performances per separation gap. For YORK house sequence  we have matched
all images spaced by 1  2 ··· 8 and 9 frames and computed the average performances per separation
∥2
gap. The afﬁnity matrix has been computed by Wij;kl = exp(−∥rik − r′
F =1000)  where rik is
the Euclidean distance between two points.
Figure 3 summarizes the performance results. It is noted that MPGM outperforms the other methods
in both objective score and matching accuracy  indicating the effectiveness of MPGM method. Also 

jl

6

Figure 2: Comparison results of different methods on synthetic point sets matching

MPGM can generate sparse solutions. These are generally consistent with the results on the synthetic
data experiments and further demonstrate the beneﬁts of MPGM algorithm.

Figure 3: Comparison results of different methods on CMU and YORK image sequences. Top:
CMU images; Bottom: YORK images.

6.3 Real-world Image Data

In this section  we tested our method on some real-world image datasets. We evaluate our MPGM
on the dataset [17] whose images are selected from Pascal 2007 3. In this dataset  there are 30 pairs
of car images and 20 pairs of motorbike images. For each image pair  feature points and ground-
truth matches were manually marked and each pair contains 30-60 ground-truth correspondences.
The afﬁnity between two nodes is computed as Wij;ij = exp(
)  where pi is the orientation
′
of normal vector at the sampled point (node) i to the contour  similarly to p
j. Also  the afﬁnity

−|pi−p

0:05

|

′
j

3http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html

7

0.020.040.060.080.10.30.40.50.60.70.80.91Deformation noise σAccuracyinliers nin = 20outliers nout = 0 FGMRRWMSMIPFP−UIPFP−SSMACMPGM0.020.040.060.080.10.650.70.750.80.850.90.951Deformation noise σObjective scoreinliers nin = 20outliers nout = 0 FGMRRWMSMIPFP−UIPFP−SSMACMPGM0.020.040.060.080.100.20.40.60.81Deformation noise σSparsityinliers nin = 20outliers nout = 0 FGMRRWMSMIPFP−UIPFP−SSMACMPGM00.020.040.060.080.100.0050.010.0150.020.0250.030.0350.04Deformation noise σTimeinliers nin = 20 outliers nout = 0 RRWMSMIPFP−UIPFP−SSMACMPGM2468100.40.50.60.70.80.9# of outliers noututliers n Accuracyinliers nin = 15deformation noise σ = 0.04 FGMRRWMSMIPFP−UIPFP−SSMACMPGM2468100.20.30.40.50.60.70.80.91# of outliers noutObjective scoreinliers nin = 15deformation noise σ = 0.04 FGMRRWMSMIPFP−UIPFP−SSMACMPGM24681000.20.40.60.81# of outliers noutSparsityinliers nin = 15deformation noise σ = 0.04 FGMRRWMSMIPFP−UIPFP−SSMACMPGM24681000.020.040.060.080.10.12# of outliers noutTimeinliers nin = 15deformation noise σ = 0.04 RRWMSMIPFP−UIPFP−SSMACMPGM0.020.040.060.080.10.20.30.40.50.60.70.80.91Deformation noise σAccuracyinliers nin = 15outliers nout = 5 FGMRRWMSMIPFP−UIPFP−SSMACMPGM0.020.040.060.080.10.20.30.40.50.60.70.80.91Deformation noise σObjective scoreinliers nin = 15outliers nout = 5 FGMRRWMSMIPFP−UIPFP−SSMACMPGM0.020.040.060.080.100.20.40.60.81Deformation noise σSparsityinliers nin = 15outliers nout = 5 FGMRRWMSMIPFP−UIPFP−SSMACMPGM00.020.040.060.080.10.010.020.030.040.05Deformation noise σTimeinliers nin = 15outliers nout = 5 RRWMSMIPFP−UIPFP−SSMACMPGM10203040506070800.40.50.60.70.80.91SeparationAccuracy FGMRRWMSMIPFP−UIPFP−SSMACMPGM10203040506070800.650.70.750.80.850.90.951SeparationObjective score FGMRRWMSMIPFP−UIPFP−SSMACMPGM102030405060708000.20.40.60.81SeparationSparsity FGMRRWMSMIPFP−UIPFP−SSMACMPGM246800.20.40.60.8SeparationAccuracy FGMRRWMSMIPFP−UIPFP−SSMACMPGM24680.50.60.70.80.91SeparationObjective score FGMRRWMSMIPFP−UIPFP−SSMACMPGM246800.20.40.60.81SeparationSparsity FGMRRWMSMIPFP−UIPFP−SSMACMPGMFigure 4: Some examples of image matching on Pascal 2007 dataset (LEFT: original image pair 
MIDDLE: FGM result  RIGHT: MPGM result. Incorrect matches are marked by red lines)

Figure 5: Comparison results of different graph matching methods on the Pascal 2007 dataset

−|dik−d
between two correspondences has been computed as Wij;kl = exp(
)  where dik denotes
′
the Euclidean distance between feature point i and k  similarly to d
jl. Some matching examples
are shown in Figure 4. To test the performance against outlier noise  we have randomly added 0-
20 outlier features for each image pair. The overall results of matching accuracy across different
outlier features are summarized in Figure 5. From Figure 5  we can note that MPGM outperforms
the other competing methods including RRWM [3] and FGM [24]  which further demonstrates the
effectiveness and practicality of MPGM on conducting real-world image matching tasks.

0:15

|

′
jl

7 Conclusions and Future work

This paper presents an effective algorithm  Multiplicative Update Graph Matching (MPGM)  that de-
velops a multiplicative update technique to solve the QP matching problem with doubly stochastic
mapping constraint. The KKT optimality and convergence properties of MPGM algorithms are theo-
retically guaranteed. We show experimentally that MPGM solution is sparse and thus approximately
incorporates the discrete constraint in optimization naturally. In our future  the theoretical analysis
on the sparsity of MPGM needs to be further studied. Also  we will incorporate our MPGM in some
path-following strategy to ﬁnd a more optimal solution for the matching problem. We will adapt the
proposed algorithm to solve some other optimization problems with doubly stochastic constraint in
machine learning and computer vision area.

Acknowledgment

This work is supported by the NBRPC 973 Program (2015CB351705); National Natural Sci-
ence Foundation of China (61602001 61671018  61572030); Natural Science Foundation of An-
hui Province (1708085QF139); Natural Science Foundation of Anhui Higher Education Institutions
of China (KJ2016A020); Co-Innovation Center for Information Supply & Assurance Technology 
Anhui University; The Open Projects Program of National Laboratory of Pattern Recognition.

8

References
[1] K. Adamczewski  Y. Suh  and K. M. Lee. Discrete tabu search for graph matching.

In ICCV  pages

109–117  2015.

[2] T. S. Caetano  J. J. McAuley  L. Cheng  Q. V. Le  and A. J. Smola. Learning graph matching. IEEE

Transactions on Pattern Analysis and Machine Intelligence  31(6):1048–1058  2009.

[3] M. Cho  J. Lee  and K. M. Lee. Reweighted random walks for graph matching. In European Conference

on Computer Vision  pages 492–505  2010.

[4] D. Conte  P. Foggia  C. Sansone  and M. Vento. Thirty years of graph matching in pattern recognition.

International Journal of Pattern Recognition and Artiﬁcial Intelligence  pages 265–298  2004.

[5] M. Cour  P. Srinivasan  and J.Shi. Balanced graph matching. In Neural Information Processing Systems 

pages 313–320  2006.

[6] C. Ding  T. Li  and M. I. Jordan. Nonnegative matrix factorization for combinatorial optimization: Spec-
tral clustering  graph matching and clique ﬁnding. In IEEE International Conference on Data Mining 
pages 183–192  2008.

[7] C. Ding  T. Li  and M. I. Jordan. Convex and semi-nonnegative matrix factorization. IEEE Transactions

on Pattern Analysis and Machine Intelligence  32(1):45–55  2010.

[8] O. Enqvist  K. Josephon  and F. Kahl. Optimal correspondences from pairwise constraints.

In IEEE

International Conference on Computer Vision  pages 1295–1302  2009.

[9] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly 

3(1-2):95–110  1956.

3790–3796  2015.

[10] S. Gold and A. Rangarajan. A graduated assignment algorithm for graph matching. IEEE Transactions

on Pattern Analysis and Machine Intelligence  18(4):377–388  1996.

[11] B. Jiang  J. Tang  C. Ding  and B. Luo. A local sparse model for matching problem. In AAAI  pages

[12] B. Jiang  J. Tang  C. Ding  and B. Luo. Nonnegative orthogonal graph matching. In AAAI  2017.
[13] B. Jiang  H. F. Zhao  J. Tang  and B. Luo. A sparse nonnegative matrix factorization technique for graph

matching problem. Pattern Recognition  47(1):736–747  2014.

[14] D. D. Lee and H. S. Seung. Algorithms for nonnegative matrix factorization.

In Neural Information

Processing Systems  pages 556–562  2001.

[15] M. Leordeanu and M. Hebert. A spectral technique for correspondence problem using pairwise constraints.

In IEEE International Conference on Computer Vision  pages 1482–1489  2005.

[16] M. Leordeanu  M. Hebert  and R. Sukthankar. An integer projected ﬁxed point method for graph macthing

and map inference. In Neural Information Processing Systems  pages 1114–1122  2009.

[17] M. Leordeanu  R. Sukthankar  and M. Hebert. Unsupervised learning for graph mathing. International

Journal of Computer Vision  95(1):1–18  2011.
[18] B. Luo  R. C. Wilson  and E. R. Hancock.

36(10):2213–2230  2003.

Spectal embedding of graphs. Pattern Recognition 

[19] J. J. MuAuley and T. S. Caetano. Fast matching of large point sets under occlusions. Pattern Recognition 

45(1):563–569  2012.

[20] B. J. van Wyk and M. A. van Wyk. A pocs-based graph matching algorithm.

IEEE Transactions on

Pattern Analysis and Machine Intelligence  16(11):1526–1530  2004.

[21] M. Zaslavskiy  F. Bach  and J. P. Vert. A path following algorithm for the graph matching problem. IEEE

Transactions on Pattern Analysis and Machine Intelligence  31(12):2227–2242  2009.

[22] R. Zass and A. Shashua. Doubly stochastic normalization for spectral clustering. In Proceedings of the

conference on Neural Information Processing Systems (NIPS)  pages 1569–1576  2006.

[23] Z. Zhang  Q. Shi  J. McAuley  W. Wei  Y. Zhang  and A. V. D. Hengel. Pairwise matching through

max-weight bipartite belief propagation. In CVPR  pages 1202–1210  2016.

[24] F. Zhou and F. D. la Torre. Factorized graph matching. In IEEE Conference on Computer Vision and

[25] F. Zhou and F. D. la Torre. Deformable graph matching. In IEEE Conference on Computer Vision and

Pattern Recognition  pages 127–134  2012.

Pattern Recognition  pages 127–134  2013.

9

,Bo Jiang
Chris Ding