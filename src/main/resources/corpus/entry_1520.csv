2011,Directed Graph Embedding: an Algorithm based on Continuous Limits of Laplacian-type Operators,This paper considers the problem of embedding directed graphs in Euclidean space while retaining directional information. We model the observed graph as a sample from a manifold endowed with a vector field  and we design an algo- rithm that separates and recovers the features of this process: the geometry of the manifold  the data density and the vector field. The algorithm is motivated by our analysis of Laplacian-type operators and their continuous limit as generators of diffusions on a manifold. We illustrate the recovery algorithm on both artificially constructed and real data.,Directed Graph Embedding: an Algorithm based on

Continuous Limits of Laplacian-type Operators

Dominique C. Perrault-Joncas

Department of Statistics
University of Washington

Seattle  WA 98195

dcpj@stat.washington.edu

mmp@stat.washington.edu

Marina Meil˘a

Department of Statistics
University of Washington

Seattle  WA 98195

Abstract

This paper considers the problem of embedding directed graphs in Euclidean
space while retaining directional information. We model the observed graph as
a sample from a manifold endowed with a vector ﬁeld  and we design an algo-
rithm that separates and recovers the features of this process: the geometry of the
manifold  the data density and the vector ﬁeld. The algorithm is motivated by our
analysis of Laplacian-type operators and their continuous limit as generators of
diffusions on a manifold. We illustrate the recovery algorithm on both artiﬁcially
constructed and real data.

1 Motivation

Recent advances in graph embedding and visualization have focused on undirected graphs  for which
the graph Laplacian properties make the analysis particularly elegant [1  2]. However  there is
an important number of graph data  such as social networks  alignment scores between biological
sequences  and citation data  which are naturally asymmetric. A commonly used approach for this
type of data is to disregard the asymmetry by studying the spectral properties of W +W T or W T W  
where W is the afﬁnity matrix of the graph.
Some approaches have been offered to preserve the asymmetry information contained in data: [3] 
[4]  [5] or to deﬁne directed Laplacian operators [6]. Although quite successful  these works adopt
a purely graph-theoretical point of view. Thus  they are not concerned with the generative process
that produces the graph  nor with the interpretability and statistical properties of their algorithms.
In contrast  we view the nodes of a directed graph as a ﬁnite sample from a manifold in Euclidean
space  and the edges as macroscopic observations of a diffusion kernel between neighboring points
on the manifold. We explore how this diffusion kernel determines the overall connectivity and
asymmetry of the resulting graph and demonstrate how Laplacian-type operators of this graph can
offer insights into the underlying generative process.
Based on the analysis of the Laplacian-type operators  we derive an algorithm that  in the limit of in-
ﬁnite sample and vanishing bandwidth  recovers the key features of the sampling process: manifold
geometry  sampling distribution  and local directionality  up to their intrinsic indeterminacies.

2 Model
The ﬁrst premise here is that we observe a directed graph G  with n nodes  having weights
W = [Wij] for the edge from node i to node j. In following with common Laplacian-based embed-
ding approaches  we assume that G is a geometric random graph constructed from n points sampled
according to distribution p = e−U on an unobserved compact smooth manifold M ⊆ Rl of known
intrinsic dimension d ≤ l. The edge weight Wij is then determined by a directed similarity kernel
k(xi  xj) with bandwidth . The directional component of k(xi  xj) will be taken to be derived

1

from a vector ﬁeld r on M  which assigns a preferred direction between weights Wij and Wji. The
choice of a vector ﬁeld r to characterize the directional component of G might seem restrictive at
ﬁrst. In the asymptotic limit of  → 0 and n → ∞ however  kernels are characterized by their
diffusion  drift  and source components [7]. As such  r is sufﬁcient to characterize any directionality
associated with a drift component and as it turns out  the component of r normal M in Rl can also
be use to characterize any source component. As for the diffusion component  it is not possible
to uniquely identify it from G alone [8]. Some absolute knownledge of M is needed to say any-
thing about it. Hence  without loss of generality  we will construct k(xi  xj) so that the diffusion
component ends being isotropic and constant  i.e. equal to Laplace-Beltrami operator ∆ on M.
The schematic of this generative process is shown in the top left of Figure 1 below.

From left to right: the graph gen-
erative process mapping the sam-
ple on M to geometric random
graph G via the kernel k(x  y) 
then the subsequent embedding
Ψn of G by operators H (α)
aa n 
H (α)
ss n (deﬁned in section 3.1).
As these operators converge to
their respective limits  H (α)
aa and
ss   so will Ψn → Ψ  pn → p 
H (α)
and rn → r.
We design an algorithm that 
given G  produces the top right
embedding (Ψn  pn  and rn).

Figure 1: Schematic of our framework.

The question is then as follows: can the generative process’ geometry M  distribution p = e−U   and
directionality r  be recovered from G? In other words  is there an embedding of G in Rm  m ≥ d
that approximates all three components of the process and that is also consistent as sample size
increases and the bandwidth vanishes? In the case of undirected graphs  the theory of Laplacian
eigenmaps [1] and Diffusion maps [9] answers this question in the afﬁrmative  in that the geometry
of M and p = e−U can be inferred using spectral graph theory. The aim here is to build on
the undirected problem and recover all three components of the generative process from a directed
graph G.
The spectral approach to undirected graph embedding relies on the fact that eigenfunctions of the
Laplace-Beltrami operator are known to preserve the local geometry of M [1]. With a consistent
empirical Laplace-Beltrami operator based on G  its eigenvectors also recover the geometry of M
and converge to the corresponding eigenfunctions on M. For a directed graph G  an additional
operator is needed to recover the local directional component r  but the principle remains the same.
The schematic for this is shown in Figure 1 where two operators - H (α)
ss n  introduced in [9] for
undirected embeddings  and H (α)
aa n  a new operator deﬁned in section 3.1 - are used to obtain the
embedding Ψn  distribution pn  and vector ﬁeld rn. As H (α)
aa and
H (α)
ss   Ψn  pn  and rn also converge to Ψ  p  and r  where Ψ is the local geometry preserving the
embedding of M into Rm.
The algorithm we propose in Section 4 will calculate the matrices corresponding to H (α)
· n from the
graph G  and with their eigenvectors  will ﬁnd estimates for the node coordinates Ψ  the directional
component r  and the sampling distribution p. In the next section we brieﬂy describe the mathemat-
ical models of the diffusion processes that our model relies on.

ss n converge to H (α)

aa n and H (α)

2

(cid:90)

2.1 Problem Setting
The similarity kernel k(x  y) can be used to deﬁne transport operators on M. The natural transport
operator is deﬁned by normalizing k(x  y) as

(cid:90)

T[f](x) =

k(x  y)p(y)/(cid:82) k(x  y(cid:48))p(y(cid:48))dy(cid:48).

represents

M

k(x  y)
p(x) f(y)p(y)dy   where p(x) =

M

k(x  y)p(y)dy .

(1)

the diffusion of

T[f](x)
transition density
The eigenfunctions of this inﬁnitesimal operator are the
continuous limit of the eigenvectors of the transition probability matrix P = D−1W given by
normalizing the afﬁnity matrix W of G by D = diag(W 1) [10]. Meanwhile  the inﬁnitesimal
transition

a distribution f(y) by the

(2)
deﬁnes the backward equation for this diffusion process over M based on kernel k. Obtaining the
explicit expression for transport operators like (2) is then the main technical challenge.

= lim
→0



∂f
∂t

(T − I)f

2.2 Choice of Kernel

In order for T[f] to have the correct asymptotic form  some hypotheses about the similarity ker-
nel k(x  y) are required. The hypotheses are best presented by considering the decomposition of
k(x  y) into symmetric h(x  y) = h(y  x) and anti-symmetric a(x  y) = −a(y  x) components:

k(x  y) = h(x  y) + a(x  y) .

(3)
The symmetric component h(x  y) is assumed to satisfy the following properties: 1. h(||y −
x||2) = h(||y−x||2/)
  and 2. h ≥ 0 and h is exponentially decreasing as ||y − x|| → ∞. This form
of symmetric kernel was used in [9] to analyze the diffusion map. For the asymmetric part of the
similarity kernel  we assume the form

d/2

r(x  y)

· (y − x) h(||y − x||2/)

 

2

d/2

a(x  y) =

(4)
with r(x  y) = r(y  x) so that a(x  y) = −a(y  x). Here r(x  y) is a smooth vector ﬁeld on the
manifold that gives an orientation to the asymmetry of the kernel k(x  y). It is worth noting that the
dependence of r(x  y) on both x and y implies that r : M × M → Rl with Rl the ambient space of
M; however in the asymptotic limit  the dependence in y is only important “locally” (x = y)  and
as such it is appropriate to think of r(x  x) being a vector ﬁeld on M. As a side note  it is worth
pointing out that even though the form of a(x  y) might seem restrictive at ﬁrst  it is sufﬁciently
rich to describe any vector ﬁeld . This can be seen by taking r(x  y) = (w(x) + w(y))/2 so that at
x = y the resulting vector ﬁeld is given by r(x  x) = w(x) for an arbitrary vector ﬁeld w(x).

3 Continuous Limit of Laplacian Type Operators

We are now ready to state the main asymptotic result.
Proposition 3.1 Let M be a compact  closed  smooth manifold of dimension d and k(x  y) an
asymmetric similarity kernel satisfying the conditions of section 2.2  then for any function f ∈
C 2(M)  the integral operator based on k has the asymptotic expansion

(cid:90)

k(x  y)f(y)dy = m0f(x) + g(f(x)  x) + o()  

(5)

(6)

M

where

and m0 =(cid:82)

g(f(x)  x) = m2
2

Rd h(||u||2)du  m2 =(cid:82)

i h(||u||2)du.

Rd u2

3

(ω(x)f(x) + ∆f(x) + r · ∇f(x) + f(x)∇ · r + c(x)f(x))

The proof can be found in [8] along with the deﬁnition of ω(x) and c(x) in (6). For now  it sufﬁces
to say that ω(x) corresponds to an interaction between the symmetric kernel h and the curvature of
M and was ﬁrst derived in [9]. Meanwhile  c(x) is a new term that originates from the interaction
between h and the component of r that is normal to M in the ambient space Rl. Proposition 3.1
foreshadows a general fact about spectral embedding algorithms: in most cases  Laplacian operators
confound the effects of spatial proximity  sampling density and directional ﬂow due to the presence
of the various terms above.

3.1 Anisotropic Limit Operators

Proposition 3.1 above can be used to derive the limits of a variety of Laplacian type operators
associated with spectral embedding algorithms like [5  6  3]. Although we will focus primarily on
a few operators that give the most insight into the generative process and enable us to recover the
model deﬁned in Figure 1  we ﬁrst present four distinct families of operators for completeness.
These operator families are inspired by the anisotropic family of operators that [9] introduced for
undirected graphs  which make use of anisotropic kernels of the form:

(x  y) = k(x  y)
 (x)pα
pα

as p(α)





k(α)


 (y)  

(x) = (cid:82)

asymmetric p or symmetric q =(cid:82)

(7)
with α ∈ [0  1] where α = 0 is the isotropic limit. To normalize the anisotropic kernels  we need
to redeﬁne the outdegrees distribution of k(α)
(x  y)p(y)dy. From (7)  four
families of diffusion processes of the form ft = H (α)[f](x) can be derived depending on which
kernel is normalized and which outdegree distribution is used for the normalization. Speciﬁcally 
we deﬁne transport operators by normalizing the asymmetric k(α)
kernels with the
M h(x  y)p(y)dy outdegree distribution1. To keep track of all
options  we introduce the following notation: the operators will be indexed by the type of kernel and
outdegree distribution they correspond to (symmetric or asymmetric)  with the ﬁrst index identifying
the kernel and the second index identifying the outdegree distribution. For example  the family of
anisotropic limit operators introduced by [9] is deﬁned by normalizing the symmetric kernel by
the symmetric outdegree distribution  hence they will be denoted as H (α)
ss   with the superscript
corresponding to the anisotropic power α.

or symmetric h(α)

M k(α)







Proposition 3.2 With the above notation 

aa [f] = ∆f − 2 (1 − α)∇U·∇f + r·∇f
(8)
H (α)
as [f] = ∆f − 2 (1 − α)∇U · ∇f − cf + (α − 1)(r · ∇U)f − (∇ · r)f + r · ∇f (9)
H (α)
sa [f] = ∆f − 2 (1 − α)∇U · ∇f + (c + ∇ · r + (α − 1)r · ∇U)f
(10)
H (α)
ss [f] = ∆f − 2(1 − α)∇U · ∇f.
(11)
H (α)

1

p + 2r · ∇p

p + 2∇ · r + c)] + o().

The proof of this proposition  which can be found in [8]  follows from repeated application of
Proposition 3.1 to p(y) or q(y) and then to kα(x  y) or hα(x  y)  as well as the fact that 1
=
pα
p−α [1 − α(ω + ∆p

Thus  if we use the asymmetric k and p  we get H (α)
tion (8). In general  H (α)
embedding directed graphs with this operator problematic. Nevertheless  H (1)
tant role in extracting the directionality of the sampling process.
If we use the symmetric kernel h but the asymmetric outdegree distribution p  we get the family
of operators H (α)
sa   of which the WCut of [3] is a special case (α = 0). If we reverse the above  i.e.
use k and q  we obtain H (α)

aa   deﬁned by the advected diffusion equa-
aa is not hermitian  so it commonly has complex eigenvectors. This makes
aa will play an impor-

as . This turns out to be merely a combination of H (α)

aa and H (α)
sa .

1The reader may notice that there are in fact eight possible combinations of kernel and degree distribution 
since the anisotripic kernel (7) could also be deﬁned using a symmetric or asymmetric outdegree distribution.
However  there are only four distinct asymptotic results and they are all covered by using one kernel (symmetric
or asymmetric) and one degree distribution (symmetric or asymmetric) throughout.

4

Algorithm 1 Directed Embedding

j=1 Si j  Q = diag(q)

2. qi ←(cid:80)n
i ←(cid:80)n

Input: Afﬁnity matrix Wi j and embedding dimension m  (m ≥ d)
1. S ← (W + W T )/2 (Steps 1–6 estimate the coordinates as in [11])
3. V ← Q−1SQ−1
4. q(1)
5. H (1)
6. Compute the Ψ the n× (m + 1) matrix with orthonormal columns containing the m + 1 largest
ss n as well as the Λ the (m + 1)× (m + 1) diagonal matrix
right eigenvector (by eigenvalue) of H (1)
of eigenvalues. Eigenvectors 2 to m + 1 from Ψ are the m coordinates of the embedding.
7. Compute π the left eigenvector of H (1)

ss n with eigenvalue 1. (Steps 7–8 estimate the density)

j=1 Vi j  Q(1) = diag(q(1))

ss n ← Q(1)−1

V

i=1 πi is the density distribution over the embedding.

j=1 Wi j  P = diag(p) (Steps 9–13 estimate the vector ﬁeld r)

8. π ← π/(cid:80)n
9. pi ←(cid:80)n
i ←(cid:80)n

j=1 Ti j  P (1) = diag(p(1))

10. T ← P −1W P −1
11. p(1)
12. H (1)
13. R ← (H (1)
direction of the corresponding coordinates of the embedding.

aa n ← P (1)−1

aa n − H (1)

T
ss n)Ψ/2. Columns 2 to m + 1 of R are the vector ﬁeld components in the

Finally  if we only consider the symmetric kernel h and degree distribution q  we recover H (α)
ss   the
anisotropic kernels of [9] for symmetric graphs. This operator for α = 1 is shown to separate the
manifold from the probability distribution [11] and will be used as part of our recovery algorithm.

Isolating the Vector Field r

4
Our aim is to esimate the manifold M  the density distribution p = e−U   and the vector ﬁeld r. The
ﬁrst two components of the data can be recovered from H (1)
ss as shown in [11] and summarized in
Algorithm 1.
At this juncture  one feature of generative process is missing: the vector ﬁeld r. The natural approach
for recovering r is to isolate the linear operator r · ∇ from H (α)

aa by substracting H (α)
ss :

aa − H (α)
H (α)

ss = r · ∇ .

(12)
The advantage of recovering r in operator form as in (12) is that r · ∇ is coordinate free. In other
words  as long as the chosen embedding of M is diffeomorphic to M2  (12) can be used to express
the component of r that lies in the tangent space TM  which we denote by r||.
Speciﬁcally  let Ψ be a diffeomorphic embedding of M ; the component of r along coordinate ψk is
then given by r · ∇ψk = rk  and so  in general 

r|| = r · ∇Ψ .

(13)
The subtle point that only r|| is recovered from (13) follows from the fact that the operator r · ∇ is
only deﬁned along M and hence any directional derivative is necessarily along TM.
Equation (13) and the previous observations are the basis for Algorithm 1  which recovers the three
important features of the generative process for an asymmetric graph with afﬁnity matrix W .
A similar approach can be employed to recover c + ∇ · r  or simply ∇ · r if r has no component
perpendicular to the tangent space TM (meaning that c ≡ 0). Recovering c + ∇ · r is achieved by
taking advantage of the fact that

(H (1)

sa − H (1)

ss ) = (c + ∇ · r)  

(14)

2A diffeomorphic embedding is guaranteed by using the eigendecomposition of H (1)
ss .

5

which is a diagonal operator. Taking into account that for ﬁnite n (H (1)
diagonal  using ψn ≡ 1n (vector of ones)  i.e. (H (1)
empirically to be more stable than simply extracting the diagonal of (H (1)

sa n − H (1)

ss n) is not perfectly
ss n)[1n] = (cn +∇· rn)  has been found

sa n − H (1)

sa n − H (1)

ss n).

5 Experiments

Artiﬁcial Data For illustrative purposes  we begin by applying our method to an artiﬁcial example.
We use the planet Earth as a manifold with a topographic density distribution  where sampling
probability is proportional to elevation. We also consider two vector ﬁelds: the ﬁrst is parallel to the
line of constant latitude and purely tangential to the sphere  while the second is parallel to the line
of constant longitude with a component of the vector ﬁeld perpendicular to the manifold. The true
model with constant latitude vector ﬁeld is shown in Figure 2  along with the estimated density and
vector ﬁeld projected on the true manifold (sphere).

Model

Recovered

Latitudinal

Longitudinal

(a)

(b)

Figure 2:
(a): Sphere with latitudinal vector ﬁeld  i.e East-West asymmetry  with Wew > Wwe if node w
lies to the West of node e. The graph nodes are sampled non-uniformly  with the topographic map of the world
as sampling density. We sample n = 5000 nodes  and observe only the resulting W matrix  but not the node
locations. From W   our algorithm estimates the sample locations (geometry)  the vector ﬁeld (black arrows)
generating the observed asymmetries  and the sampling distribution at each data point (colormap). (b) Vector
ﬁelds on a spherical region (blue)  and their estimates (red): latitudinal vector ﬁeld tangent to the manifold
(left) and longitudinal vector ﬁeld with component perpendicular to manifold tangent plane (right).
Both the estimated density and vector ﬁeld agree with the true model  demonstrating that for artiﬁcial
data  the recovery algorithm 1 performs quite well. We note that the estimated density does not
recover all the details of the original density  even for large sample size (here n = 5000 with  =
0.07). Meanwhile  the estimated vector ﬁeld performs quite well even when the sampling is reduced
to n = 500 with  = 0.1. This can be seen in Figure 2  b  where the true and estimated vector ﬁelds
are superimposed. Figure 2 also demonstrates how r · ∇ only recovers the tangential component of
r. The estimated geometry is not shown on any of these ﬁgures  since the success of the diffusion
map in recovering the geometry for such a simple manifold is already well established [2  9].
Real DataThe National Longitudinal Survey of Youth (NLSY) 1979 Cohort is a representative sam-
ple of young men and women in the United States who were followed from 1979 to 2000 [12  13].
The aim here is to use this survey to obtain a representation of the job market as a diffusion process
over a manifold.
The data set consists of a sample of 7 816 individual career sequences of length 64  listing the jobs
a particular individual held every quarter between the ages of 20 and 36. Each token in the sequence
identiﬁes a job. Each job corresponds to an industry × occupation pair. There are 25 unique industry
and 20 unique occupation indices. Out of the 500 possible pairings  approximately 450 occur in the
data  with only 213 occurring with sufﬁcient frequency to be included here. Thus  our graph G has
213 nodes - the jobs - and our observations consist of 7 816 walks between the graph nodes.
We convert these walks to a directed graph with afﬁnity matrix W . Speciﬁcally  Wij represents the
number of times a transition from job i to job j was observed (Note that this matrix is asymmetric 

6

i.e Wij (cid:54)= Wji). Normalizing each row i of W by its outdegree di gives P = diag(di)−1W   the
non-parametric maximum likelihood estimator for the Markov chain over G for the progression
of career sequences. This Markov chain has as limit operator H (0)
aa   as the granularity of the job
market increases along with the number of observations. Thus  in trying to recover the geometry 
distribution and vector ﬁeld  we are actually interested in estimating the full advective effect of the
aa ; that is  we want to estimate r·∇− 2∇U ·∇ where we can use
diffusion process generated by H (0)
−2∇U · ∇ = H (0)

ss to complement Algorithm 1.

ss − H (1)

(a)

(b)

Figure 3: Embedding the job market along with ﬁeld r − 2∇U over the ﬁrst two non-constant eigenvectors.
The color map corresponds to the mean monthly wage in dollars (a) and to the female proportion (b) for each
job.
We obtain an embedding of the job market that describes the relative position of jobs  their distri-
bution  and the natural time progression from each job. Of these  the relative position and natural
time progression are the most interesting. Together  they summarize the job market dynamics by
describing which jobs are naturally “close” as well as where they can lead in the future. From a
public policy perspective  this can potentially improve focus on certain jobs for helping individuals
attain better upward mobility.
The job market was found to be a high dimensional manifold. We present only the ﬁrst two dimen-
sions  that is  the second and third eigenvectors of H (0)
ss   since the ﬁrst eigenvector is uninformative
(constant) by construction. The eigenvectors showed correlation with important demographic data 
such as wages and gender. Figure 3 displays this two-dimensional sub-embedding along with the
directional information r − 2∇U for each dimension. The plot shows very little net progression
toward regions of increasing mean salary3. This is somewhat surprising  but it is easy to overstate
this observation: diffusion alone would be enough to move the individuals towards higher salary.
What Figure 3 (a) suggests is that there appear to be no “external forces” advecting individuals to-
wards higher salary. Nevertheless  there appear to be other external forces at play in the job market:
Figure 3 (b)  which is analogous to Figure 3 (a)  but with gender replacing the salary color scheme 
suggests that these forces push individuals towards greater gender differentiation. This is especially
true amongst male-dominated jobs which appear to be advected toward the left edge of the embed-
ding. Hence  this simple analysis of the job market can be seen as an indication that males and
females tend to move away from each other over time  while neither seems to have a monopoly on
high- or low- paying jobs.

6 Discussion

This paper makes three contributions: (1) it introduces a manifold-based generative model for di-
rected graphs with weighted edges  (2) it obtains asymptotic results for operators constructed from
the directed graphs  and (3) these asymptotic results lead to a natural algorithm for estimating the
model.

3It is worth noting that in the NLSY data set  high paying jobs are teacher  nurse and mechanic. This is due

to the fact that the career paths observed stop at at age 36  which is relatively early in an individual’s career.

7

−0.1−0.0500.050.10.150.2−0.25−0.2−0.15−0.1−0.0500.050.10.150.20.25  !2!3800100012001400160018002000−0.1−0.0500.050.10.150.2−0.25−0.2−0.15−0.1−0.0500.050.10.150.20.25  !2!30.10.20.30.40.50.60.70.80.9Generative Models that assume that data are sampled from a manifold are standard for undirected
graphs  but to our knowledge  none have yet been proposed for directed graphs. When W is sym-
metric  it is natural to assume that it depends on the points’ proximity. For asymmetric afﬁnities W  
one must include an additional component to explain the asymmetry. In the asymptotic limit  this is
tantamount to deﬁning a vector ﬁeld on the manifold.
Algorithm We have used from [9] the idea of deﬁning anisotropic kernels (indexed by α) in order to
separate the density p and the manifold geometry M. Also  we adopted their general assumptions
about the symmetric part of the kernel. As a consequence  the recovery algorithm for p and M is
identical to theirs.
However  insofar as the asymmetric part of the kernel is concerned  everything  starting from the
deﬁnition and the introduction of the vector ﬁeld r as a way to model the asymmetry  through the
derivation of the asymptotic expression for the symmetric plus asymmetric kernel  is new. We go
signiﬁcantly beyond the elegant idea of [9] regarding the use of anisotropic kernels by analyzing the
four distinct renormalizations possible for a given α  each of them combining different aspects of
M  p and r. Only the successful (and novel) combination of two different anisotropic operators is
able to recover the directional ﬂow r.
Algorithm 1 is natural  but we do not claim it is the only possible one in the context of our model.
sa to recover the operator ∇ · r (which empirically seems to have
For instance  we can also use H (α)
worse numerical properties than r · ∇). In the National Longitudinal Survery of Youth study  we
were interested in the whole advective term  so we estimated it from a different combination of
operators. Depending on the speciﬁc question  other features of the model could be obtained
Limit Results Proposition 3.1 is a general result on the asymptotics of asymmetric kernels. Re-
covering the manifold and r is just one  albeit the most useful  of the many ways of exploiting these
results. For instance  H (0)
sa is the limit operator of the operators used in [3] and [5]. The limit analysis
could be extended to other digraph embedding algorithms such as [4  6].
How general is our model? Any kernel can be decomposed into a symmetric and an asymmetric
part  as we have done. The assumptions on the symmetric part h are standard. The paper of [7] goes
one step further from these assumptions; we will discuss it in relationship with our work shortly.
The more interesting question is how limiting are our assumptions regarding the choice of kernel 
especially the asymmetric part  which we parameterized as a(x  y) = r/2 · (y − x)h(x  y) in (4).
In the asymptotic limit  this choice turns out to be fully general  at least up to the identiﬁable aspects
of the model. For a more detailed discussion of this issue  see [8].
In [7]  Ting  Huang and Jordan presented asymptotic results for a general family of kernels that
includes asymmetric and random kernels. Our k can be expressed in the notation of [7] by taking
wx(y) ← 1+r(x  y)·(y−x)  rx(y) ← 1  K0 ← h  h ← . Their assumptions are more general than
the assumptions we make here  yet our model is general up to what can be identiﬁed from G alone.
The distinction arises because [7] focuses on the graph construction methods from an observed
sample of M  while we focus on explaining an observed directed graph G through a manifold
generative process. Moreover  while the [7] results can be used to analyze data from directed graphs 
they differ from our Proposition 3.1. Speciﬁcally  with respect to the limit in Theorem 3 from
[7]  we obtain the additional source terms f(x)∇ · r and c(x)f(x) that follow from not enforcing
conservation of mass while deﬁning operators H (α)
We applied our theory of directed graph embedding to the analysis of the career sequences in
Section 5  but asymmetric afﬁnity data abound in other social contexts  and in the physical and
life sciences.
Indeed  any “similarity” score that is obtained from a likelihood of the form
Wvu =likelihood(u|v) is generally asymmetric. Hence our methods can be applied to study not
only social networks  but also patterns of human movement  road trafﬁc  and trade relations  as well
as alignment scores in molecular biology. Finally  the physical interpretation of our model also
makes it naturally applicable to physical models of ﬂows.

sa and H (α)
as .

Acknowledgments

This research was partially supported by NSW awards IIS-0313339 and IIS-0535100.

8

References
[1] Belkin and Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural

Computation  15:1373–1396  2002.

[2] Nadler  Lafon  and Coifman. Diffusion maps  spectral clustering and eigenfunctions of fokker-planck

operators. In Neural Information Processing Systems Conference  2006.

[3] Meila and Pentney. Clustering by weighted cuts in directed graphs. In SIAM Data Mining Conference 

2007.

[4] Zhou  Huang  and Scholkopf. Learning from labeled and unlabeled data on a directed graph. In Interna-

tional Conference on Machine Learning  pages 1041–1048  2005.

[5] Zhou  Schlkopf  and Hofmann. Semi-supervised learning on directed graphs.

Information Processing Systems  volume 17  pages 1633–1640  2005.

In Advances in Neural

[6] Fan R. K. Chung. The diameter and laplacian eigenvalues of directed graphs. Electr. J. Comb.  13  2006.
[7] Ting  Huang  and Jordan. An analysis of the convergence of graph Laplacians. In International Confer-

ence on Machine Learning  2010.

[8] Dominique Perrault-Joncas and Marina Meil˘a. Directed graph embedding: an algorithm based on contin-
uous limits of laplacian-type operators. Technical Report TR 587  University of Washington - Department
of Statistics  November 2011.

[9] Coifman and Lafon. Diffusion maps. Applied and Computational Harmonic Analysis  21:6–30  2006.
[10] Mikhail Belkin and Partha Niyogi. Convergence of laplacian eigenmaps. preprint  short version NIPS

2008  2008.

[11] Coifman  Lafon  Lee  Maggioni  Warner  and Zucker. Geometric diffusions as a tool for harmonic analysis
and structure deﬁnition of data: Diffusion maps. In Proceedings of the National Academy of Sciences 
pages 7426–7431  2005.

[12] United States Department of Labor.

http://www.bls.gov/nls/  retrived October 2011.

National

longitudinal

survey of youth 1979 cohort.

[13] Marc A. Scott. Afﬁnity models for career sequences. Journal of the Royal Statistical Society: Series C

(Applied Statistics)  60(3):417–436  2011.

9

,Assaf Glazer
Omer Weissbrod
Michael Lindenbaum
Shaul Markovitch
Heiko Strathmann
Dino Sejdinovic
Samuel Livingstone
Zoltan Szabo
Arthur Gretton
Albert Berahas
Jorge Nocedal
Martin Takac