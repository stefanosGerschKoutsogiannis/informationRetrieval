2017,Deep Hyperalignment,This paper proposes Deep Hyperalignment (DHA) as a regularized  deep extension  scalable Hyperalignment (HA) method  which is well-suited for applying functional alignment to fMRI datasets with nonlinearity  high-dimensionality (broad ROI)  and a large number of subjects. Unlink previous methods  DHA is not limited by a restricted fixed kernel function. Further  it uses a parametric approach  rank-m Singular Value Decomposition (SVD)  and stochastic gradient descent for optimization. Therefore  DHA has a suitable time complexity for large datasets  and DHA does not require the training data when it computes the functional alignment for a new subject. Experimental studies on multi-subject fMRI analysis confirm that the DHA method achieves superior performance to other state-of-the-art HA algorithms.,Deep Hyperalignment

Muhammad Yousefnezhad  Daoqiang Zhang
College of Computer Science and Technology

Nanjing University of Aeronautics and Astronautics

{myousefnezhad dqzhang}@nuaa.edu.cn

Abstract

This paper proposes Deep Hyperalignment (DHA) as a regularized  deep extension 
scalable Hyperalignment (HA) method  which is well-suited for applying func-
tional alignment to fMRI datasets with nonlinearity  high-dimensionality (broad
ROI)  and a large number of subjects. Unlink previous methods  DHA is not limited
by a restricted ﬁxed kernel function. Further  it uses a parametric approach  rank-m
Singular Value Decomposition (SVD)  and stochastic gradient descent for opti-
mization. Therefore  DHA has a suitable time complexity for large datasets  and
DHA does not require the training data when it computes the functional alignment
for a new subject. Experimental studies on multi-subject fMRI analysis conﬁrm
that the DHA method achieves superior performance to other state-of-the-art HA
algorithms.

1

Introduction

The multi-subject fMRI analysis is a challenging problem in the human brain decoding [1–7]. On
the one hand  the multi-subject analysis can verify the developed models across subjects. On the
other hand  this analysis requires authentic functional and anatomical alignments among neuronal
activities of different subjects  which these alignments can signiﬁcantly improve the performance
of the developed models [1  4]. In fact  multi-subject fMRI images must be aligned across subjects
in order to take between-subject variability into account. There are technically two main alignment
methods  including anatomical alignment and functional alignment  which can work in unison.
Indeed  anatomical alignment is only utilized in the majority of the fMRI studies as a preprocessing
step. It is applied by aligning fMRI images based on anatomical features of standard structural MRI
images  e.g. Talairach [2  7]. However  anatomical alignment can limitedly improve the accuracy
because the size  shape and anatomical location of functional loci differ across subjects [1  2  7]. By
contrast  functional alignment explores to precisely align the fMRI images across subjects. Indeed  it
has a broad range of applications in neuroscience  such as localization of the Brain’s tumor [8].
As the widely used functional alignment method [1–7]  Hyperalignment (HA) [1] is an ‘anatomy free’
functional alignment method  which can be mathematically formulated as a multiple-set Canonical
Correlation Analysis (CCA) problem [2  3  5]. Original HA does not work in a very high dimensional
space. In order to extend HA into the real-world problems  Xu et al. developed the Regularized
Hyperalignment (RHA) by utilizing an EM algorithm to iteratively seek the regularized optimum
parameters [2]. Further  Chen et al. developed Singular Value Decomposition Hyperalignment
(SVDHA)  which ﬁrstly provides dimensionality reduction by SVD  and then HA aligns the functional
responses in the reduced space [4]. In another study  Chen et al. introduced Shared Response Model
(SRM)  which is technically equivalent to Probabilistic CCA [5]. In addition  Guntupalli et al.
developed SearchLight (SL) model  which is actually an ensemble of quasi-CCA models ﬁts on
patches of the brain images [9]. Lorbert et al.
illustrated the limitation of HA methods on the
linear representation of fMRI responses. They also proposed Kernel Hyperalignment (KHA) as
a nonlinear alternative in an embedding space for solving the HA limitation [3]. Although KHA

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

can solve the nonlinearity and high-dimensionality problems  its performance is limited by the
ﬁxed employed kernel function. As another nonlinear HA method  Chen et al. recently developed
Convolutional Autoencoder (CAE) for whole brain functional alignment.
Indeed  this method
reformulates the SRM as a multi-view autoencoder [5] and then uses the standard SL analysis [9] in
order to improve the stability and robustness of the generated classiﬁcation (cognitive) model [6].
Since CAE simultaneously employs SRM and SL  its time complexity is so high. In a nutshell  there
are three main challenges in previous HA methods for calculating accurate functional alignments  i.e.
nonlinearity [3  6]  high-dimensionality [2  4  5]  and using a large number of subjects [6].
As the main contribution of this paper  we propose a novel kernel approach  which is called Deep
Hyperalignment (DHA)  in order to solve mentioned challenges in HA problems. Indeed  DHA
employs deep network  i.e. multiple stacked layers of nonlinear transformation  as the kernel function 
which is parametric and uses rank-m SVD [10] and Stochastic Gradient Descent (SGD) [13] for
optimization. Consequently  DHA generates low-runtime on large datasets  and the training data is
not referenced when DHA computes the functional alignment for a new subject. Further  DHA is not
limited by a restricted ﬁxed representational space because the kernel in DHA is a multi-layer neural
network  which can separately implement any nonlinear function [11–13] for each subject to transfer
the brain activities to a common space.
The proposed method is related to RHA [2] and MVLSA [10]. Indeed  the main difference between
DHA and the mentioned methods lies in the deep kernel function. Further  KHA [3] is equivalent
to DHA  where the proposed deep network is employed as the kernel function. In addition  DHA
can be looked as a multi-set regularized DCCA [11] with stochastic optimization [13]. Finally 
DHA is related to DGCCA [12]  when DGCCA is reformulated for functional alignment by using
regularization  and rank-m SVD [10].
The rest of this paper is organized as follows: In Section 2  this study brieﬂy introduces HA method.
Then  DHA is proposed in Section 3. Experimental results are reported in Section 4; and ﬁnally  this
paper presents conclusion and pointed out some future works in Section 5.

2 Hyperalignment

(cid:110)

(cid:111) ∈

As a training set  preprocessed fMRI time series for S subjects can be denoted by X((cid:96)) =
RT×V   (cid:96) = 1:S  m = 1:T  n = 1:V   where V denotes the number of voxels  T is the number of time
mn ∈ R denotes the functional activity for the (cid:96)-th
points in units of TRs (Time of Repetition)  and x((cid:96))
subject in the m-th time point and the n-th voxel. For assuring temporal alignment  the stimuli in
the training set are considered time synchronized  i.e. the m-th time point for all subjects illustrates
the same simulation [2  3]. Original HA can be deﬁned based on Inter-Subject Correlation (ISC) 
which is a classical metric in order to apply functional alignment: [1-4  7]

x((cid:96))
mn

S(cid:88)

S(cid:88)

(cid:16)

(X(i)R(i))(cid:62)X(j)R(j)(cid:17)

tr

(1)

S(cid:88)

S(cid:88)

i=1

j=i+1

max

R(i) R(j)

ISC(X(i)R(i)  X(j)R(j)) ≡ max

(cid:0)X((cid:96))R((cid:96))(cid:1)(cid:62)

s.t.

R(i) R(j)

j=i+1
X((cid:96))R((cid:96)) = I  (cid:96) = 1:S 

i=1

where tr() denotes the trace function  I is the identity matrix  R((cid:96)) ∈ RV ×V denotes the solution
for (cid:96)-th subject. For avoiding overﬁtting  the constrains must be imposed in R((cid:96)) [2  7]. If X((cid:96)) ∼
N (0  1)  (cid:96) = 1:S are column-wise standardized  the ISC lies in [−1  +1]. Here  the large values
illustrate better alignment [2  3]. In order to seek an optimum solution  solving (1) may not be the best
approach because there is no scale to evaluate the distance between current result and the optimum
(fully maximized) solution [2  4  7]. Instead  we can reformulate (1) as a minimization problem by
using a multiple-set CCA: [1–4]

S(cid:88)

S(cid:88)

i=1

j=i+1

(cid:13)(cid:13)(cid:13)X(i)R(i) − X(j)R(j)(cid:13)(cid:13)(cid:13)2

F

min

R(i) R(j)

(cid:16)

X((cid:96))R((cid:96))(cid:17)(cid:62)

 

s.t.

X((cid:96))R((cid:96)) = I 

(cid:96) = 1:S 

(2)

where (2) approaches zero for an optimum result. Indeed  the main assumption in the original HA
is that the R((cid:96))  (cid:96) = 1:S are noisy ‘rotations’ of a common template [1  9]. This paper provides a
detailed description of HA methods in the supplementary materials (https://sourceforge.net/
projects/myousefnezhad/files/DHA/).

2

3 Deep Hyperalignment

Objective function of DHA is deﬁned as follows:

θ(i) R(i)
θ(j) R(j)

min

(cid:0)X(i);θ(i)(cid:1)R(i) − fj

(cid:0)X(j);θ(j)(cid:1)R(j)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)fi
S(cid:88)
S(cid:88)
R((cid:96))(cid:17)(cid:62)(cid:18)(cid:16)
(cid:0)X((cid:96));θ((cid:96))(cid:1)(cid:17)(cid:62)
m   m=2:C(cid:9) denotes all parameters in (cid:96)-th deep network belonged to (cid:96)-th

(cid:0)X((cid:96));θ((cid:96))(cid:1) + I

R((cid:96)) = I 

(cid:96) = 1:S 

(cid:19)

j=i+1

(3)

i=1

f(cid:96)

f(cid:96)

F

m   b((cid:96))

(cid:16)
where θ((cid:96))=(cid:8)W((cid:96))

s.t.

 

f(cid:96)

h((cid:96))

(cid:16)

(cid:17)

(cid:16)

W((cid:96))

m h((cid:96))

m = g
h((cid:96))

C   T  Vnew

m−1 + b((cid:96))
m

subject  R((cid:96)) ∈ RVnew×Vnew is the DHA solution for (cid:96)-th subject  Vnew ≤ V denotes the number of
features after transformation  the regularized parameter  is a small constant  e.g. 10−8  and deep
multi-layer kernel function f(cid:96)

(cid:0)X((cid:96));θ((cid:96))(cid:1) ∈ RT×Vnew is denoted as follows:
(cid:0)X((cid:96));θ((cid:96))(cid:1) = mat

1 = vec(cid:0)X((cid:96))(cid:1)

(4)
where T denotes the number of time points  C ≥ 3 is number of deep network layers 
mat(x  m  n):Rmn → Rm×n denotes the reshape (matricization) function  and h((cid:96))
C ∈ RT Vnew
is the output layer of the following multi-layer deep network:

(cid:17)
1 = vec(cid:0)X((cid:96))(cid:1) ∈ RT V . Notably  this paper considers both
vec() and mat() functions are linear transformations  where X ∈ Rm×n = mat(cid:0)vec(X)  m  n(cid:1) for

(5)
Here  g:R → R is a nonlinear function applied componentwise  vec:Rm×n → Rmn denotes the
vectorization function  consequently h((cid:96))

(cid:0)X((cid:96));θ((cid:96))(cid:1) are deﬁned by following properties: W((cid:96))

any matrix X. By considering U (m) units in the m-th intermediate layer  parameters of distinctive
C ∈
layers of f(cid:96)
2 ∈ RU (2) for the ﬁrst intermediate layer 
RT Vnew for the output layer  W((cid:96))
m ∈ RU (m) for m-th intermediate layer (3 ≤ m ≤
and W((cid:96))
C − 1).
Since (3) must be calculated for any new subject in the testing phase  it is not computationally
efﬁcient. In other words  the transformed training data must be referenced by the current objective
function for each new subject in the testing phase.
Lemma 1. The equation (3) can be reformulated as follows where G ∈ RT×Vnew is the HA template:

C ∈ RT Vnew×U (C-1) and b((cid:96))

2 ∈ RU (2)×T V and b((cid:96))

m ∈ RU (m)×U (m-1)  b((cid:96))

m ∈ RU (m) and h((cid:96))

  where h((cid:96))

and m = 2:C.

S(cid:88)

i=1

(cid:13)(cid:13)(cid:13)G − fi
(cid:0)X(i);θ(i)(cid:1)R(i)(cid:13)(cid:13)(cid:13)2
(cid:18)(cid:16)
(cid:0)X((cid:96));θ((cid:96))(cid:1)R((cid:96))(cid:17)(cid:62)

In a nutshell 

both (3)

f(cid:96)

f(cid:96)

F

min

G R(i) θ(i)

Proof.

(cid:32)
S(cid:80)S

(cid:96)=1 tr

s.t. G(cid:62)G = I  where G =

S(cid:88)

(cid:0)X(j);θ(j)(cid:1)R(j).
rewritten as −S2tr(cid:0)G(cid:62)G(cid:1) +

1
S

(6)

j=1

fj

.

Please see supplementary materi-

and (6)

(cid:0)X((cid:96));θ((cid:96))(cid:1)R((cid:96))

(cid:19)(cid:33)

can be

als for proof in details.
Remark 1. G is called DHA template  which can be used for functional alignment in the testing
phase.
Remark 2. Same as previous approaches for HA problems [1–7]  a DHA solution is not unique. If
a DHA template G is calculated for a speciﬁc HA problem  then QG is another solution for that
speciﬁc HA problem  where Q ∈ RVnew×Vnew can be any orthogonal matrix. Consequently  if two
independent templates G1  G2 are trained for a speciﬁc dataset  the solutions can be mapped to each

(cid:13)(cid:13)  where Q can be used as a coefﬁcient for functional alignment in

other by calculating(cid:13)(cid:13)G2 − QG1

the ﬁrst solution in order to compare its results to the second one. Indeed  G1 and G2 are located in
different positions on the same contour line [5  7].

3

3.1 Optimization

This section proposes an effective approach for optimizing the DHA objective function by using
rank-m SVD [10] and SGD [13]. This method seeks an optimum solution for the DHA objective
function (6) by using two different steps  which iteratively work in unison. By considering ﬁxed
network parameters (θ((cid:96)))  a mini-batch of neural activities is ﬁrstly aligned through the deep network.
Then  back-propagation algorithm [14] is used to update the network parameters. The main challenge
for solving the DHA objective function is that we cannot seek a natural extension of the correlation
object to more than two random variables. Consequently  functional alignments are stacked in a
S × S matrix and maximize a certain matrix norm for that matrix [10  12].
As the ﬁrst step  we consider network parameters are in an optimum state. Therefore  the mappings
(R((cid:96))  (cid:96) = 1:S) and template (G) must be calculated to solve the DHA problem. In order to scale
DHA approach  this paper employs the rank-m SVD [10] of the mapped neural activities as follows:
(7)
where Σ((cid:96)) ∈ Rm×m denotes the diagonal matrix with m-largest singular values of the mapped
feature f(cid:96)
and right singular vectors. Based on (7)  the projection matrix for (cid:96)-th subject can be generated as
follows: [10]

(cid:0)X((cid:96));θ((cid:96))(cid:1)  Ω((cid:96)) ∈ RT×m and Ψ((cid:96)) ∈ Rm×Vnew are respectively the corresponding left

(cid:0)X((cid:96));θ((cid:96))(cid:1) SV D= Ω((cid:96))Σ((cid:96))(cid:0)Ψ((cid:96))(cid:1)(cid:62)

(cid:96) = 1:S

f(cid:96)

 

(cid:0)X((cid:96));θ((cid:96))(cid:1)(cid:18)(cid:16)
= Ω((cid:96))(cid:0)Σ((cid:96))(cid:1)(cid:62)(cid:16)

P((cid:96)) = f(cid:96)

f(cid:96)

Σ((cid:96))(cid:0)Σ((cid:96))(cid:1)(cid:62)

where P((cid:96)) ∈ RT×T is symmetric and idempotent [10  12]  and diagonal matrix D((cid:96)) ∈ Rm×m is
(9)

Further  the sum of projection matrices can be deﬁned as follows  where (cid:101)A(cid:101)A(cid:62) is the Cholesky

D((cid:96))(cid:0)D((cid:96))(cid:1)(cid:62)

Σ((cid:96)).

+ I

decomposition [10] of A:

 

f(cid:96)

f(cid:96)

+ I

(cid:0)X((cid:96));θ((cid:96))(cid:1)(cid:17)(cid:62)
Ω((cid:96))D((cid:96))(cid:17)(cid:62)

(cid:19)−1(cid:16)
= Ω((cid:96))D((cid:96))(cid:16)
(cid:17)−1

(cid:0)X((cid:96));θ((cid:96))(cid:1)(cid:17)(cid:62)
(cid:0)X((cid:96));θ((cid:96))(cid:1) + I
(cid:17)−1
Σ((cid:96))(cid:0)Ω((cid:96))(cid:1)(cid:62)
=(cid:0)Σ((cid:96))(cid:1)(cid:62)(cid:16)
Σ((cid:96))(cid:0)Σ((cid:96))(cid:1)(cid:62)
(cid:101)A ∈ RT×mS =(cid:2)Ω(1)D(1) . . . Ω(S)D(S)(cid:3).
(cid:0)X(i);θ(i)(cid:1)R(i)(cid:13)(cid:13)(cid:13) ≡ max

tr(cid:0)G(cid:62)AG(cid:1)(cid:17)

where

(cid:16)

.

G

(cid:13)(cid:13)(cid:13)G − fi

(8)

(10)

(11)

A =

S(cid:88)

i=1

P(i) = (cid:101)A(cid:101)A(cid:62) 
S(cid:88)

min

G R(i) θ(i)

i=1

Lemma 2. Based on (10)  the objective function of DHA (6) can be rewritten as follows:

Proof. Since P((cid:96)) is idempotent  the trace form of (6) can be reformulated as maximizing the sum of
projections. Please see the supplementary materials for proof in details.

Based on Lemma 2  the ﬁrst optimization step of DHA problem can be expressed as eigendecom-

position of AG = GΛ  where Λ =(cid:8)λ1 . . . λT
(cid:9) and G respectively denote the eigenvalues and
left singular vectors of (cid:101)A = G(cid:101)Σ(cid:101)Ψ(cid:62)  where G(cid:62)G = I [10]. This paper utilizes Incremental SVD

eigenvectors of A. Further  the matrix G that we are interested in ﬁnding  can be calculated by the

[15] for calculating these left singular vectors. Further  DHA mapping for (cid:96)-th subject is denoted as
follows:

R((cid:96)) =

Lemma 3. In order to update network parameters as the second step  the derivative of Z =(cid:80)T

(cid:96)=1 λ(cid:96) 
which is the sum of eigenvalues of A  over the mapped neural activities of (cid:96)-th subject is deﬁned as
follows:

(12)

G.

f(cid:96)

(cid:19)−1(cid:16)

(cid:18)(cid:16)

f(cid:96)

f(cid:96)

(cid:0)X((cid:96));θ((cid:96))(cid:1)(cid:17)(cid:62)
(cid:0)X((cid:96));θ((cid:96))(cid:1) + I
(cid:0)X((cid:96));θ((cid:96))(cid:1) = 2R((cid:96))G(cid:62) − 2R((cid:96))(cid:0)R((cid:96))(cid:1)(cid:62)(cid:16)

∂Z

f(cid:96)

.

(13)

(cid:0)X((cid:96));θ((cid:96))(cid:1)(cid:17)(cid:62)

(cid:0)X((cid:96));θ((cid:96))(cid:1)(cid:17)(cid:62)

∂f(cid:96)

Proof. This derivative can be solved by using the chain and product rules in the matrix derivative as
well as considering ∂Z/∂A = GG(cid:62) [12]. Please see the supplementary materials for proof in details.

4

Algorithm 1 Deep Hyperalignment (DHA)

Input: Data X(i)  i = 1:S  Regularized parameter   Number of layers C  Number of units U (m)

for m = 2:C  HA template (cid:98)G for testing phase (default ∅)  Learning rate η (default 10−4 [13]).

Output: DHA mappings R((cid:96)) and parameters θ((cid:96))  HA template G just from training phase
Method:
01. Initialize iteration counter: m ← 1 and θ((cid:96)) ∼ N (0  1) for (cid:96) = 1:S.
02. Construct f(cid:96)

(cid:0)X((cid:96));θ((cid:96))(cid:1) based on (4) and (5) by using θ((cid:96))  C  U (m) for (cid:96) = 1:S.

% The ﬁrst step of DHA: ﬁxed θ((cid:96)) and calculating G and R((cid:96)) ↓

06. ELSE

03. IF ((cid:98)G (cid:54)= ∅) THEN
04. Generate (cid:101)A by using (8) and (10).
05. Calculate G by applying Incremental SVD [15] to (cid:101)A = G(cid:101)Σ(cid:101)Ψ(cid:62).
07. G = (cid:98)G.
(cid:13)(cid:13)(cid:13)fi
(cid:0)X(i);θ(i)(cid:1)R(i) − fj
10. Estimate error of iteration γm =(cid:80)S
11. IF(cid:0)(m > 3) and (γm ≥ γm−1 ≥ γm−2)(cid:1) THEN

08. END IF
09. Calculate mappings R((cid:96))  (cid:96) = 1:S by using (12).

12. Return calculated G  R((cid:96))  θ((cid:96))((cid:96) = 1:S) related to (m-2)-th iteration.
13. END IF
14. ∇θ((cid:96)) ← backprop
15. Update θ((cid:96)) ← θ((cid:96)) − η∇θ((cid:96)) for (cid:96) = 1:S and then m ← m + 1
16. SAVE all DHA parameters related to this iteration and GO TO Line 02.

by using (13) for (cid:96) = 1:S.

∂Z/∂f(cid:96)

i=1

(cid:80)S
(cid:0)X((cid:96));θ((cid:96))(cid:1)  θ((cid:96))(cid:17)

(cid:16)

(cid:0)X(j);θ(j)(cid:1)R(j)(cid:13)(cid:13)(cid:13)2

j=i+1

.
% This is the ﬁnishing condition.
% The second step of DHA: ﬁxed G and R((cid:96)) and updating θ((cid:96)) ↓

F

Algorithm 1 illustrates the DHA method for both training and testing phases. As depicted in this
algorithm  (12) is just needed as the ﬁrst step in the testing phase because the DHA template G
is calculated for this phase based on the training samples (please see Lemma 1). As the second
step in the DHA method  the networks’ parameters (θ((cid:96))) must be updated. This paper employs
the back-propagation algorithm (backprop() function) [14] as well as Lemma 3 for this step. In
addition  ﬁnishing condition is deﬁned by tackling errors in last three iterations  i.e. the average of the
difference between each pair correlations of aligned functional activities across subjects (γm for last
three iterations). In other words  DHA will be ﬁnished if the error rates in the last three iterations are
going to be worst. Further  a structure (nonlinear function for componentwise  and numbers of layers
and units) for the deep network can be selected based on the optimum-state error (γopt) generated by
training samples across different structures (see Experiment Schemes in the supplementary materials).
In summary  this paper proposes DHA as a ﬂexible deep kernel approach to improve the performance
of functional alignment in fMRI analysis. In order to seek an efﬁcient functional alignment  DHA uses
a deep network (multiple stacked layers of nonlinear transformation) for mapping fMRI responses of
each subject to an embedded space (f(cid:96) : RT×V → RT×Vnew  (cid:96) = 1:S). Unlike previous methods
that use a restricted ﬁxed kernel function  mapping functions in DHA are ﬂexible across subjects
because they employ multi-layer neural networks  which can implement any nonlinear function [12].
Therefore  DHA does not suffer from disadvantages of the previous kernel approach. In order to
deal with high-dimensionality (broad ROI)  DHA can also apply an optional feature selection by
considering Vnew < V for constructing the deep networks. The performance of the optional feature
selection will be analyzed in Section 4. Finally  DHA can be scaled across a large number of subjects
by using the proposed optimization algorithm  i.e. rank-m SVD  regularization  and mini-batch SGD.

4 Experiments

The empirical studies are reported in this section. Like previous studies [1–7  9]  this paper employs
the ν-SVM algorithms [16] for generating the classiﬁcation model. Indeed  we use the binary ν-SVM
for datasets with just two categories of stimuli and multi-label ν-SVM [3  16] as the multi-class
approach. All datasets are separately preprocessed by FSL 5.0.9 (https://fsl.fmrib.ox.ac.uk) 
i.e. slice timing  anatomical alignment  normalization  smoothing. Regions of Interests (ROI) are
also denoted by employing the main reference of each dataset. In addition  leave-one-subject-out

5

Table 1: Accuracy of HA methods in post-alignment classiﬁcation by using simple task datasets

Table 2: Area under the ROC curve (AUC) of different HA methods in post-alignment classiﬁcation
by using simple task datasets

↓Algorithms  Datasets→
ν-SVM [17]
HA [1]
RHA [2]
KHA [3]
SVD-HA [4]
SRM [5]
SL [9]
CAE [6]
DHA

↓Algorithms  Datasets→
ν-SVM [17]
HA [1]
RHA [2]
KHA [3]
SVD-HA [4]
SRM [5]
SL [9]
CAE [6]
DHA

DS005

71.65±0.97
81.27±0.59
83.06±0.36
85.29±0.49
90.82±1.23
91.26±0.34
90.21±0.61
94.25±0.76
97.92±0.82

DS105

22.89±1.02
30.03±0.87
32.62±0.52
37.14±0.91
40.21±0.83
48.77±0.94
49.86±0.4
54.52±0.80
60.39±0.68

DS107

38.84±0.82
43.01±0.56
46.82±0.37
52.69±0.69
59.54±0.99
64.11±0.37
64.07±0.98
72.16±0.43
73.05±0.63

DS116

67.26±1.99
74.23±1.40
78.71±0.76
78.03±0.89
81.56±0.54
83.31±0.73
82.32±0.28
91.49±0.67
90.28±0.71

DS117

73.32±1.67
77.93±0.29
84.22±0.44
83.32±0.41
95.62±0.83
95.01±0.64
94.96±0.24
95.92±0.67
97.99±0.94

DS005

68.37±1.01
70.32±0.92
82.22±0.42
80.91±0.21
88.54±0.71
90.23±0.74
89.79±0.25
91.24±0.61
96.91±0.82

DS105

21.76±0.91
28.91±1.03
30.35±0.39
36.23±0.57
37.61±0.62
44.48±0.75
47.32±0.92
52.16±0.63
59.57±0.32

DS107

36.84±1.45
40.21±0.33
43.63±0.61
50.41±0.92
57.54±0.31
62.41±0.72
61.84±0.32
72.33±0.79
70.23±0.92

DS116

62.49±1.34
70.67±0.97
76.34±0.45
75.28±0.94
78.66±0.82
79.20±0.98
80.63±0.81
87.53±0.72
89.93±0.24

DS117

70.17±0.59
76.14±0.49
81.54±0.92
80.92±0.28
92.14±0.42
93.65±0.93
93.26±0.72
91.49±0.33
96.13±0.32

cross-validation is utilized for partitioning datasets to the training set and testing set. Different HA
methods are employed for functional aligning and then the mapped neural activities are used to
generate the classiﬁcation model. The performance of the proposed method is compared with the
ν-SVM algorithm as the baseline  where the features are used after anatomical alignment without
applying any hyperalignment mapping. Further  performances of the standard HA [1]  RHA [2] 
KHA [3]  SVDHA [4]  SRM [5]  and SL [9] are reported as state-of-the-arts HA methods. In this
paper  the results of HA algorithm is generated by employing Generalized CCA proposed in [10].
In addition  regularized parameters (α  β) in RHA are optimally assigned based on [2]. Further 
KHA algorithm is used by the Gaussian kernel  which is evaluated as the best kernel in the original
paper [3]. As another deep-learning-based alternative for functional alignment  the performance
of CAE [6] is also compared with the proposed method. Like the original paper [6]  this paper
employs k1 = k3 = {5  10  15  20  25}  ρ = {0.1  0.25  0.5  0.75  0.9}  λ = {0.1  1  5  10}. Then 
aligned neural activities (by using CAE) are applied to the classiﬁcation algorithm same as other
HA techniques. This paper follows the CAE setup to set the same settings in the proposed method.
Consequently  three hidden layers (C = 5) and the regularized parameters  = {10−4  10−6  10−8}
are employed in the DHA method. In addition  the number of units in the intermediate layers are
considered U (m) = KV   where m = 2:C-1  C is the number of layers  V denotes the number of
voxels and K is the number of stimulus categories in each dataset1. Further  three distinctive activation
functions are employed  i.e. Sigmoid (g(x) = 1/1 + exp(−x))  Hyperbolic (g(x) = tanh(x))  and
Rectiﬁed Linear Unit or ReLU (g(x) = ln(1 + exp(x))). In this paper  the optimum parameters for
DHA and CAE methods are reported for each dataset. Moreover  all algorithms are implemented by
Python 3 on a PC with certain speciﬁcations2 by authors in order to generate experimental results.
Experiment schemes are also described in supplementary materials.

4.1 Simple Tasks Analysis

This paper utilizes 5 datasets  shared by Open fMRI (https://openfmri.org)  for running em-
pirical studies of this section. Further  numbers of original and aligned features are considered

1Although we can use any settings for DHA  we empirically ﬁgured out this setting is acceptable to seek an
optimum solution. Indeed  we followed CAE setup in the network structure but used the number of categories
(K) rather than a series of parameters. In the current format of DHA  we just need to set the regularized constant
and the nonlinear activation function  while a wide range of parameters must be set in the CAE.
2DEL  CPU = Intel Xeon E5-2630 v3 (8×2.4 GHz)  RAM = 64GB  GPU = GeForce GTX TITAN X (12GB
memory)  OS = Ubuntu 16.04.3 LTS  Python = 3.6.2  Pip = 9.0.1  Numpy = 1.13.1  Scipy = 0.19.1  Scikit-Learn
= 0.18.2  Theano = 0.9.0.

6

(a) Forrest Gump

(TRs = 100)

(b) Forrest Gump

(TRs = 400)

(c) Forrest Gump

(TRs = 800)

(d) Forrest Gump

(TRs = 2000)

(e) Raiders
(TRs = 100)

(f) Raiders
(TRs = 400)

(g) Raiders
(TRs = 800)

(h) Raiders
(TRs = 2000)

Figure 1: Comparison of different HA algorithms on complex task datasets by using ranked voxels.
equal (V = Vnew) for all HA methods. As the ﬁrst dataset  ‘Mixed-gambles task’ (DS005) includes
S = 48 subjects. It also contains K = 2 categories of risk tasks in the human brain  where the
chance of selection is 50/50. In this dataset  the best results for CAE is generated by following
parameters k1 = k3 = 20  ρ = 0.75  λ = 1 and for DHA by using  = 10−8 and Hyperbolic
function. In addition  ROI is deﬁned based on the original paper [17]. As the second dataset  ‘Visual
Object Recognition’ (DS105) includes S = 71 subjects. It also contains K = 8 categories of
visual stimuli  i.e. gray-scale images of faces  houses  cats  bottles  scissors  shoes  chairs  and
scrambles (nonsense patterns). In this dataset  the best results for CAE is generated by following
parameters k1 = k3 = 25  ρ = 0.9  λ = 5 and for DHA by using  = 10−6 and Sigmoid func-
tion. Please see [1  7] for more information. As the third dataset  ‘Word and Object Processing’
(DS107) includes S = 98 subjects. It contains K = 4 categories of visual stimuli  i.e. words 
objects  scrambles  consonants. In this dataset  the best results for CAE is generated by following
parameters k1 = k3 = 10  ρ = 0.5  λ = 10 and for DHA by using  = 10−6 and ReLU function.
Please see [18] for more information. As the fourth dataset  ‘Multi-subject  multi-modal human
neuroimaging dataset’ (DS117) includes MEG and fMRI images for S = 171 subjects. This paper
just uses the fMRI images of this dataset. It also contains K = 2 categories of visual stimuli  i.e.
human faces  and scrambles. In this dataset  the best results for CAE is generated by following
parameters k1 = k3 = 20  ρ = 0.9  λ = 5 and for DHA by using  = 10−8 and Sigmoid function.
Please see [19] for more information. The responses of voxels in the Ventral Cortex are analyzed
for these three datasets (DS105  DS107  DS117). As the last dataset  ‘Auditory and Visual Oddball
EEG-fMRI’ (DS116) includes EEG signals and fMRI images for S = 102 subjects. This paper only
employs the fMRI images of this dataset. It contains K = 2 categories of audio and visual stimuli 
including oddball tasks. In this dataset  the best results for CAE is generated by following parameters
k1 = k3 = 10  ρ = 0.75  λ = 1 and for DHA by using  = 10−4 and ReLU function. In addition 
ROI is deﬁned based on the original paper [20]. This paper also provides the technical information of
the employed datasets in the supplementary materials. Table 1 and 2 respectively demonstrate the
classiﬁcation Accuracy and Area Under the ROC Curve (AUC) in percentage (%) for the predictors.
As these tables demonstrate  the performances of classiﬁcation analysis without HA method are
signiﬁcantly low. Further  the proposed algorithm has generated better performance in comparison
with other methods because it provided a better embedded space in order to align neural activities.

4.2 Complex Tasks Analysis

This section uses two fMRI datasets  which are related to watching movies. The numbers of original
and aligned features are considered equal (V = Vnew) for all HA methods. As the ﬁrst dataset  ‘A
high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie’ (DS113)
includes the fMRI data of S = 18 subjects  who watched ‘Forrest Gump (1994)’ movie during
the experiment. This dataset provided by Open fMRI. In this dataset  the best results for CAE is
generated by following parameters k1 = k3 = 25  ρ = 0.9  λ = 10 and for DHA by using  = 10−8
and Sigmoid function. Please see [7] for more information. As the second dataset  S = 10 subjects
watched ‘Raiders of the Lost Ark (1981)’  where whole brain volumes are 48. In this dataset  the best
results for CAE is generated by following parameters k1 = k3 = 15  ρ = 0.75  λ = 1 and for DHA

7

1002004006008001000120025303540455055606570758085Classification Accuracy (%)# of voxels per hemisphere vSVM HA KHA RHA SL SVDHA SRM CAE DHA10020040060080010001200303540455055606570758085Classification Accuracy (%)# of voxels per hemisphere vSVM HA KHA RHA SL SVDHA SRM CAE DHA10020040060080010001200303540455055606570758085Classification Accuracy (%)# of voxels per hemisphere vSVM HA KHA RHA SL SVDHA SRM CAE DHA10020040060080010001200303540455055606570758085Classification Accuracy (%)# of voxels per hemisphere vSVM HA KHA RHA SL SVDHA SRM CAE DHA70140210280350420490303540455055606570Classification Accuracy (%)# of voxels per hemisphere vSVM HA KHA RHA SL SVDHA SRM CAE DHA70140210280350420490303540455055606570Classification Accuracy (%)# of voxels per hemisphere vSVM HA KHA RHA SL SVDHA SRM CAE DHA70140210280350420490303540455055606570Classification Accuracy (%)# of voxels per hemisphere vSVM HA KHA RHA SL SVDHA SRM CAE DHA7014021028035042049030354045505560657075Classification Accuracy (%)# of voxels per hemisphere vSVM HA KHA RHA SL SVDHA SRM CAE DHA(A) DS105

(B) DS107

(A) DS105

(B) DS107

Figure 3: Runtime Analysis

Figure 2: Classiﬁcation by using feature selection.
by using  = 10−4 and Sigmoid function. Please see [3-5] for more information. In these two datasets 
the ROI is deﬁned in the ventral temporal cortex (VT). Figure 1 depicts the generated results  where
the voxels in ROI are ranked by the method proposed in [1] based on their neurological priorities same
as previous studies [1  4  7  9]. Then  the experiments are repeated by using the different number of
ranked voxels per hemisphere  i.e. in Forrest: [100  200  400  600  800  1000  1200]  and in Raiders:
[70  140  210  280  350  420  490]. In addition  the empirical studies are reported by using the ﬁrst
T Rs = [100  400  800  2000] in both datasets. Figure 1 shows that the DHA achieves superior
performance to other HA algorithms.

4.3 Classiﬁcation analysis by using feature selection

In this section  the effect of features selection (Vnew < V ) on the performance of classiﬁcation
methods will be discussed by using DS105 and DS107 datasets. Here  the performance of the
proposed method is compared with SVDHA [4]  SRM [5]  and CAE [6] as the state-of-the-art
HA techniques  which can apply feature selection before generating a classiﬁcation model. Here 
multi-label ν-SVM [16] is used for generating the classiﬁcation models after each of the mentioned
methods applied on preprocessed fMRI images for functional alignment. In addition  the setup of this
experiment is same as the previous sections (cross-validation  the best parameters  etc.). Figure 2
illustrates the performance of different methods by employing 100% to 60% of features. As depicted
in this ﬁgure  the proposed method has generated better performance in comparison with other
methods because it provides better feature representation in comparison with other techniques.

4.4 Runtime Analysis

In this section  the runtime of the proposed method is compared with the previous HA methods by
using DS105 and DS107 datasets. As mentioned before  all of the results in this experiment are
generated by a PC with certain speciﬁcations. Figure 3 illustrates the runtime of the mentioned
methods  where runtime of other methods are scaled based on the DHA (runtime of the proposed
method is considered as the unit). As depicted in this ﬁgure  CAE generated the worse runtime
because it concurrently employs modiﬁed versions of SRM and SL for functional alignment. Further 
SL also includes high time complexity because of the ensemble approach. By considering the
performance of the proposed method in the previous sections  it generates acceptable runtime. As
mentioned before  the proposed method employs rank-m SVD [10] as well as Incremental SVD [15] 
which can signiﬁcantly reduce the time complexity of the optimization procedure [10  12].

5 Conclusion

This paper extended a deep approach for hyperalignment methods in order to provide accurate
functional alignment in multi-subject fMRI analysis. Deep Hyperalignment (DHA) can handle fMRI
datasets with nonlinearity  high-dimensionality (broad ROI)  and a large number of subjects. We
have also illustrated how DHA can be used for post-alignment classiﬁcation. DHA is parametric and
uses rank-m SVD and stochastic gradient descent for optimization. Therefore  DHA generates low-
runtime on large datasets  and DHA does not require the training data when the functional alignment
is computed for a new subject. Further  DHA is not limited by a restricted ﬁxed representational space
because the kernel in DHA is a multi-layer neural network  which can separately implement any
nonlinear function for each subject to transfer the brain activities to a common space. Experimental
studies on multi-subject fMRI analysis conﬁrm that the DHA method achieves superior performance
to other state-of-the-art HA algorithms. In the future  we will plan to employ DHA for improving the
performance of other techniques in fMRI analysis  e.g. Representational Similarity Analysis (RSA).

8

10090807060404550556065Classification AccuracyThe Percentage of Selected Features SVDHA SRM CAE DHA10090807060606264666870727476788082Classification AccuracyThe Percentage of Selected Features SVDHA SRM CAE DHACAECAEDHADHASLSLSRMSRMSVDHASVDHAKHAKHARHARHAHAHAννSVMSVMRuntime (%)00.511.52CAECAEDHADHASLSLSRMSRMSVDHASVDHAKHAKHARHARHAHAHAννSVMSVMRuntime (%)00.511.52Acknowledgments

This work was supported in part by the National Natural Science Foundation of China (61422204 
61473149  and 61732006)  and NUAA Fundamental Research Funds (NE2013105).

References

[1] Haxby  J.V. & Connolly  A.C. & Guntupalli  J.S. (2014) Decoding neural representational spaces using
multivariate pattern analysis. Annual Review of Neuroscience. 37:435–456 
[2] Xu  H. & Lorbert  A. & Ramadge  P.J. & Guntupalli  J.S. & Haxby  J.V. (2012) Regularized hyperalignment
of multi-set fMRI data. IEEE Statistical Signal Processing Workshop (SSP). pp. 229–232  Aug/5–8  USA.
[3] Lorbert  A. & Ramadge  P.J. (2012) Kernel hyperalignment. 25th Advances in Neural Information Processing
Systems (NIPS). pp. 1790–179. Dec/3–8  Harveys.
[4] Chen  P.H. & Guntupalli  J.S. & Haxby  J.V. & Ramadge  P.J. (2014) Joint SVD-Hyperalignment for multi-
subject FMRI data alignment. 24th IEEE International Workshop on Machine Learning for Signal Processing
(MLSP). pp. 1–6  Sep/21–24  France.
[5] Chen  P.H. & Chen  J. & Yeshurun  Y. & Hasson  U. & Haxby  J.V. & Ramadge  P.J. (2015) A reduced-
dimension fMRI shared response model. 28th Advances in Neural Information Processing Systems (NIPS). pp.
460–468  Dec/7–12  Canada.
[6] Chen  P.H. & Zhu  X. & Zhang  H. & Turek  J.S. & Chen  J. & Willke  T.L. & Hasson  U. & Ramadge  P.J.
(2016) A convolutional autoencoder for multi-subject fMRI data aggregation. 29th Workshop of Representation
Learning in Artiﬁcial and Biological Neural Networks. NIPS  Dec/5–10  Barcelona.
[7] Yousefnezhad  M. & Zhang D. (2017) Local Discriminant Hyperalignment for multi-subject fMRI data
alignment. 34th AAAI Conference on Artiﬁcial Intelligence. pp. 59–61  Feb/4–9  San Francisco  USA.
[8] Langs  G. & Tie  Y. & Rigolo  L. & Golby  A. & Golland  P. (2010) Functional geometry alignment and
localization of brain areas  23th Advances in Neural Information Processing Systems (NIPS). Dec/6–11  Canada.
[9] Guntupalli  J.S. & Hanke  M. & Halchenko  Y.O. & Connolly  A.C. & Ramadge  P.J. & Haxby  J.V. (2016) A
model of representational spaces in human cortex. Cerebral Cortex. Oxford University Press.
[10] Rastogi  P. & Van D.B. & Arora  R. (2015) Multiview LSA: Representation Learning via Generalized
CCA. 14th Annual Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies (HLT-NAACL). pp. 556–566  May/31 to Jun/5  Denver  USA.
[11] Andrew  G. & Arora  R. & Bilmes  J. & Livescu  K. (2012) Deep Canonical Correlation Analysis. 30th
International Conference on Machine Learning (ICML). pp. 1247–1255  Jun/16–21  Atlanta  USA.
[12] Benton  A. & Khayrallah  H. & Gujral  B. & Reisinger  D. & Zhang  S. & Arora  R. (2017) Deep Generalized
Canonical Correlation Analysis. 5th International Conference on Learning Representations (ICLR).
[13] Wang  W. & Arora  R. & Livescu  K. & Srebro  N. Stochastic optimization for deep CCA via nonlinear
orthogonal iterations. 53rd Annual Allerton Conference on Communication  Control  and Computing (Allerton).
pp. 688–695  Oct/3–6  Urbana-Champaign  USA.
[14] Rumelhart  D.E. & Hinton  G.E. & Williams  R.J. (1986) Learning representations by back-propagating
errors. Nature. 323(6088):533–538.
[15] Brand  M. (2002) Incremental Singular Value Decomposition of uncertain data with missing values. 7th
European Conference on Computer Vision (ECCV). pp. 707–720  May/28–31  Copenhagen  Denmark.
[16] Smola  A.J. & Schölkopf  B. (2004) A tutorial on support vector regression. Statistics and Computing.
14(3):199–222.
[17] Sabrina  T.M. & Craig  F.R. & Trepel  C. & Poldrack  R.A. (2007) The neural basis of loss aversion in
decision-making under risk. American Association for the Advancement of Science. 315(5811):515–518.
[18] Duncan  K.J. & Pattamadilok  C. & Knierim  I. & Devlin  Joseph T. (2009) Consistency and variability in
functional localisers. NeuroImage. 46(4):1018–1026.
[19] Wakeman  D.G. & Henson  R.N. (2015) A multi-subject  multi-modal human neuroimaging dataset.
Scientiﬁc Data. vol. 2.
[20] Walz J.M. & Goldman R.I. & Carapezza M. & Muraskin J. & Brown T.R. & Sajda P. (2013) Simultaneous
EEG-fMRI reveals temporal evolution of coupling between supramodal cortical attention networks and the
brainstem. Journal of Neuroscience. 33(49):19212-22.

9

,Jinwoo Shin
Andrew Gelfand
Misha Chertkov
Isabel Valera
Francisco Ruiz
Lennart Svensson
Muhammad Yousefnezhad
Daoqiang Zhang