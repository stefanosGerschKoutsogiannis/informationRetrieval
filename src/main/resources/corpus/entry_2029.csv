2017,Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe,We consider the problem of bandit optimization  inspired by stochastic optimization and online learning problems with bandit feedback. In this problem  the objective is to minimize a global loss function of all the actions  not necessarily a cumulative loss. This framework allows us to study a very general class of problems  with applications in statistics  machine learning  and other fields. To solve this problem  we analyze the Upper-Confidence Frank-Wolfe algorithm  inspired by techniques for bandits and convex optimization. We give theoretical guarantees for the performance of this algorithm over various classes of functions  and discuss the optimality of these results.,Fast Rates for Bandit Optimization with

Upper-Conﬁdence Frank-Wolfe

Quentin Berthet ∗

University of Cambridge

q.berthet@statslab.cam.ac.uk

Vianney Perchet †

ENS Paris-Saclay & Criteo Research  Paris

vianney.perchet@normalesup.org

Abstract

We consider the problem of bandit optimization  inspired by stochastic optimiza-
tion and online learning problems with bandit feedback. In this problem  the ob-
jective is to minimize a global loss function of all the actions  not necessarily a
cumulative loss. This framework allows us to study a very general class of prob-
lems  with applications in statistics  machine learning  and other ﬁelds. To solve
this problem  we analyze the Upper-Conﬁdence Frank-Wolfe algorithm  inspired
by techniques for bandits and convex optimization. We give theoretical guaran-
tees for the performance of this algorithm over various classes of functions  and
discuss the optimality of these results.

Introduction
In online optimization problems  a decision maker choses at each round t ≥ 1 an action πt from
some given action space  observes some information through a feedback mechanism in order to
minimize a loss  function of the set of actions {π1  . . .   πT}. Traditionally  this objective is computed

as a cumulative loss of the form(cid:80)

t (cid:96)t(πt) [20  34]  or as a function thereof [2  3  16  32].

Examples include classical multi-armed bandit problems where the action space is ﬁnite with K
elements  in stochastic or adversarial settings [9]. In these problems  the loss at round t can be
written as (cid:96)t(eπt) for a linear form (cid:96)t on RK  and basis vectors ei. More generally  this includes
also bandit problems over a convex body C  where the action at each round consists in picking xt ∈ C
and where the loss (cid:96)t(xt) is for some convex function (cid:96)t [see  e.g. 9  12  19  10].
In this work  we consider the online learning problem of bandit optimization. Similarly to other
problems of this type  a decision maker chooses at each round an action πt from a set of size K  and
observes information about an unknown convex loss function L. The difference is that the objective

(cid:1)  not a cumulative one. At each round  choosing

is to minimize a global convex loss L(cid:0) 1
(cid:80)T
(cid:80)T

the i-th action increases the information about the local dependency of L on its i-th coefﬁcient. This
problem can be contrasted with the objective of minimizing the average pseudo-regret in a stochastic
bandit problem  i.e. of minimizing 1
t=1 L(eπt) with observation (cid:96)t(eπt)  a noisy estimate of
T
L(eπt). At the intersection of these frameworks  when L is a linear form  is the stochastic multi-
armed bandit problem. Our problem is also related to maximization of known convex objectives
[2  3]. We compare our framework to these settings in Section 1.4.
Bandit optimization shares some similarities with stochastic optimization problems  where the ob-
jective is to minimize f (xT ) for an unknown function f  while choosing at each round a variable xt
∗Supported by an Isaac Newton Trust Early Career Support Scheme and by The Alan Turing Institute under
†Supported by the ANR (grant ANR- 13-JS01-0004-01)  and the FMJH Program Gaspard Monge in Opti-

the EPSRC grant EP/N510129/1.

T

t=1 eπt

mization and operations research (supported in part by EDF) and from the Labex LMH.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

(cid:80)t

and observing some noisy information about the function f. Our problem can be seen as a stochastic
optimization problem over the simplex  with the caveat that the list of actions π1  . . .   πT determines
the variable  as xt = 1
s=1 eπs  as well as the manner in which additional information about the
t
function can be gathered. This setting allows us to study a more general class of problems than
multi-armed bandits  and to cover examples where there is not one optimal action  but rather an op-
timal global strategy  that is an optimal mix of actions. We describe several natural problems from
machine learning  statistics  or economics that are cases of bandit optimization.
This problem draws inspiration from the world of multi-armed bandit problems and that of stochas-
tic convex optimization  and our solution to it does as well. We analyze the Upper-Conﬁdence
Frank-Wolfe algorithm  a modiﬁcation of the Frank-Wolfe algorithm [17] and of the UCB algorithm
for bandits [5]. The link with Frank-Wolfe is related to the choice of one action  and encourages
exploitation  while the link with UCB encourages to chose rarely picked actions in order to increase
knowledge about the function  encouraging exploration. This algorithm can be used for all convex
functions L  and performs in a near-optimal manner over various classes of functions. Indeed  if
√
it has been already proved that it achieves slow rates of convergence in some cases  i.e.  the error
decreases as 1/
These fast rates are surprising  as they sometimes even hold for non-strongly convex functions  and
in many problems with bandit feedback they cannot be reached [23  35]. As shown in our lower
bounds  the main complexity of this problem is statistical and comes from the limited information
available about the unknown function L. Usual results in optimization with a known function are
not necessarily relevant to our problem. As an example  while linear rates in e−cT are possible
in deterministic settings with variants in the Frank-Wolfe algorithm  we are limited to fast rates in
1/T under similar assumptions. Interestingly  while linear functions are one of the settings in which
the deterministic Frank-Wolfe algorithm is the most efﬁcient  it is among the most complicated for
bandit optimization  and only slow rates are possible (see theorems 2 and 6).
Our work is organized in the following manner: we describe in Section 1 the problem of bandit
optimization. The main algorithm is introduced in Section 2  and its performance in various settings
is studied in Section 3  4  and 5. All proofs of the main results are in the supplementary material.
Notations: For any positive integer n  denote by [n] the set {1  . . .   n} and  for any positive integer

i∈[K]pi = 1(cid:9) the unit simplex of RK. Finally  ei stands

K  by ∆K :=(cid:8)p ∈ RK : pi ≥ 0 and (cid:80)

T   we are able to exhibit fast rates decreasing in 1/T   up to logarithmic terms.

for the i-th vector of the canonical basis of RK. Notice that ∆K is their convex hull.

1 Bandit Optimization

We describe the bandit optimization problem  generalizing multi-armed bandits. This stochastic
optimization problem is doubly related to bandits: The decision variable cannot be chosen freely but
is tied to the past actions  and information about the function is obtained via a bandit feedback.

a cumulative loss(cid:80)

1.1 Problem description
A each time step t ≥ 1  a decision maker chooses an action πt ∈ [K] from K different actions with
the objective of minimizing an unknown convex loss function L : ∆K → R. Unlike in traditional
online learning problems  we do not assume that the overall objective of the agent is to minimize
t L(eπt) but rather to minimize the global loss L(pT )  where pt ∈ ∆K is the
pt =(cid:0)T1(t)/t  . . .   TK(t)/t(cid:1) with Ti(t) =(cid:80)t
(cid:80)t

vector of proportions of each action (also called occupation measure)  i.e. 

i=1 eπs. As usual in stochastic optimization  the performance of a policy is

s=11{πs = i} .

Alternatively  pt = 1
t
evaluated by controlling the difference

r(T ) := E[L(pT )] − min
p∈∆K

L(p) .

The information available to the policy is a feedback of bandit type: given the choice πt = i  it is
an estimate ˆgt of ∇L(pt). Its precision  with respect to each coefﬁcient i ∈ [K]  is speciﬁed by a
deviation function αt i  meaning that for all δ ∈ (0  1)  it holds with probability 1 − δ that

|ˆgt i − ∇iL(pt)| ≤ αt i(Ti(t)  δ) .

2

At each round  it is possible to improve the precision for one of the coefﬁcients of the gradient but
possibly at a cost of increasing the global loss. The most typical case  described in the following

section  is of αt i(Ti  δ) = (cid:112)2 log(t/δ)/Ti  when the information consists of observations from

different distributions. In general  this type of feedback mechanism is indicative of a bandit feedback
(and not of a full information setting)  as motivated by the following parametric setting.

1.2 Bandit feedback and parametric setting

One of the motivations is the minimization of a loss function L belonging to a known class
{L(µ ·)  µ ∈ RK} with an unknown parameter µ. Choosing the i-th action provides information
about µi  through an observation of some auxiliary distribution νi.
As an example  the classical stochastic multi-armed bandit problem [9] falls within our framework.
Denoting by µi the expected loss of arm i ∈ [K]  the average pseudo-regret ¯R can be expressed as

t(cid:88)

K(cid:88)

¯R(t) =

1
t

µπs − µ(cid:63) =

µi

Ti(t)

t

− µ(cid:63) = p(cid:62)

t µ − p(cid:63)(cid:62)µ  with p(cid:63) = ei(cid:63)  

s=1

i=1

Hence the choice of L(µ  p) = µ(cid:62)p corresponds the problem of multi-armed bandits. Since
∇L(µ  p) = µ  the feedback mechanism for ˆgt is induced by having a sample Xt from νπt at
time step t  taking ˆgt i = ¯Xt i  the empirical mean of the Ti(t) observations νi. In this case  if νi is

sub-Gaussian with parameter 1  we have αt i(Ti  δ) = 2(cid:112)2 log(t/δ)/Ti.

More generally  for any parametric model  we can consider the following observation setting: For
all i ∈ [K]  let νi be a sub-Gaussian distribution with mean µi and tail parameter σ2. At time t 
for an action πt ∈ [K]  we observe a realization from νπt. We estimate µi by the empirical mean
ˆµt i of the Ti(t) draws from νi  and ˆgt = ∇pL(ˆµt  pt) as an estimate of the gradient of L = L(µ ·)
at pt. The following bound on αi under smoothness conditions on the parametric model is a direct
application of Hoeffding’s inequality.
Proposition 1. Let L = L(µ ·) for some µ ∈ RK being µ-gradient-Lipschitz  i.e.  such that

(cid:12)(cid:12)(cid:12)(cid:0)∇pL(µ  p)(cid:1)

i −(cid:0)∇pL(µ(cid:48)  p)(cid:1)

(cid:12)(cid:12)(cid:12) ≤ |µi − µ(cid:48)

Under the sub-Gaussian observation setting above  ˆgt = ∇pL(ˆµt  pt) is a valid gradient feedback

with deviation bounds αt i(Ti  δ) =(cid:112)2σ2 log(t/δ)/Ti.

i

i|   ∀p ∈ ∆([K]).

This Lipschitz condition on the parameter µ gives a motivation for our gradient bandit feedback.

1.3 Examples

Stochastic multi-armed bandit:
As noted above  the stochastic multi-armed bandit problem is a special case of our setting for a loss
L(p) = µ(cid:62)p  and the bandit feedback allows to construct a proxy for the gradient ˆgt with deviations
√
Ti. The UCB algorithm used to solve this problem inspires our algorithm that
αi decaying in 1/
generalizes to any loss function L  as discussed in Section 2.

Online experimental design: In the context of statistical estimation with heterogenous data sources
[8]  consider the problem of allocating samples in order to minimize the variance of the ﬁnal esti-
mate. At time t  it is possible to sample from one of K distributions N (θi  σ2
i ) for i ∈ [K]  the
objective being to minimize the average variance of the simple unbiased estimator

2] =(cid:80)

E[(cid:107)ˆθ − θ(cid:107)2

i∈[K]σ2

i /Ti

equivalent to L(p) =(cid:80)

i∈[K]σ2

i /pi .

For unknown σi  this problem falls within our framework and the gradient with coordinates −σ2
i /p2
can be estimated by using the Ti draws from N (θi  σ2
i
i . This function is only deﬁned
on the interior of the simplex and is unbounded  matters that we discuss further in Section 4.3. Other
objective functions than the expected (cid:96)2 norm of the error can be used  as in [11]  who consider the
(cid:96)∞ norm of the actual estimated deviations  not its expectation.

i ) to construct ˆσ2

3

Utility maximization: A classical model to describe the utility of an agent purchasing xi units of
K different goods is the Cobb-Douglas utility (see e.g. [27]) deﬁned for parameters βi ∈ (0  1) by

U (x1  . . .   xK) =(cid:81)

i∈[K]xβi

i

.

equivalent to minimizing in pi (the proportion of good i in the basket) L(p) = −(cid:80)

Maximizing this utility for unknown βi under a budget constraint - where each price is assumed
to be 1 for ease of notations - by buying one unit of one of K goods at each round  is therefore
i∈[K]βi log(pi).
Other examples: More generally  the notion of bandit optimization can be applied to any situation
where one optimizes a strategy through actions that are taken sequentially  with information gained
at each round  and where the objective depends only on the proportions of actions. Other examples
include a problem inspired by online Markovitz portfolio optimization  where the goal is to minimize
L(p) = p(cid:62)Σp − λµ(cid:62)p  with a known covariance matrix Σ and unknown returns µ  or several
i∈[K]fi(µi)pi when observations

generalizations of bandit problems such as minimizing L(p) =(cid:80)

are drawn from a distribution with mean µi  for known fi.

1.4 Comparison with other problems

(cid:80)

(cid:80)

T

t eπt).

t (cid:96)t(xt)  we focus on L( 1

As mentioned in the introduction  the problem of bandit optimization is different from online learn-
ing problems related to regret minimization [21  1  10]  even in a stochastic setting. While the usual
objective is to minimize a cumulative regret related to 1
T
Problems related to online optimization of global costs or objectives have been studied in similar
T V ) where V is a K × d
settings [2  3  16  32]. They are equivalent to minimizing a loss L(p(cid:62)
unknown matrix and L(·) : Rd → R is known. The feedback at stage t is a noisy evaluations of
Vπt. In the stochastic case [2  3]  this is close to our setting - even though none of them subsumes
directly the other one. Only slow rates of convergence of order 1/
T are derived for the variant
of Frank-Wolfe  while we aim at fast rates  which are optimal. In contrast  in the adversarial case
[16  32]  there are instances of the problem where the average regret cannot decrease to zero [26].
Using the Frank-Wolfe algorithm in a stochastic optimization problem has also already been consid-
ered  particularly in [25]  where the estimates of the gradients are increasingly precise in t  indepen-
dently of the actions of the decision maker. This setting  where the action at each round is to pick
xt in the domain in order to minimize f (xT ) is therefore closer to classical stochastic optimization
than online learning problems related to bandits [9  19  10].

√

2 Upper-Conﬁdence Frank-Wolfe algorithm

√
With linear functions  as in multi-armed bandits  an estimate of the gradient can be established by
using the past observations  as well as conﬁdence intervals on each coefﬁcient in 1/
Ti. The UCB
algorithm instructs to pick the action with the smallest lower conﬁdence estimate µ
for the loss.
This is equivalent to making a step of size 1/(t + 1) in the direction of the corner of the simplex e
that minimizes e(cid:62)µ
. Following this intuition  we introduce the UCB Frank-Wolfe algorithm that
uses a proxy of the gradient  penalized by the size of conﬁdence intervals.

t i

t

Algorithm 0: UCB Frank-Wolfe algorithm
Input: K  p0 = 1[K]/K  sequence (δt)t≥0;
for t ≥ 0 do
Observe ˆgt  noisy estimate of ∇L(pt);
for i ∈ [K] do

ˆUt i = ˆgti − αt i(Ti(t)  δt)

end
Select πt+1 ∈ argmini∈[K]
Update pt+1 = pt + 1

ˆUt i;

t+1 (eπt+1 − pt)

end

4

Notice that for any algorithm  the selection of an action πt+1 ∈ [K] at time step t + 1 updates the
variable p with respect to the following dynamics

pt+1 =

1 − 1
t + 1

pt +

1

t + 1

eπt+1 = pt +

1

t + 1

(eπt+1 − pt) .

(1)

(cid:16)

(cid:17)

This is implied by the mechanism of the problem  and is not dependent on the choice of an algorithm.
If the choice of eπt+1 is e(cid:63)t+1  the minimizer of s(cid:62)∇L(pt) over all s ∈ ∆K  this would precisely
be the Frank-Wolfe algorithm with step size 1/(t + 1). Inspired by this similarity  our selection rule
is driven by the same principle  using a proxy ˆUt for ∇L(pt) based on the information up to time t.
Our selection rule is therefore driven by two principles  borrowing from tools in convex optimization
(the Frank-Wolfe algorithm) and classical bandit problems (Upper-conﬁdence bounds).
The choice of action πt+1 is equivalent to taking eπt+1 ∈ argmins∈∆K s(cid:62) ˆUt. The computational
cost of this procedure is very light  and apart from gradient computations  it is linear in K at each
iteration  leading to a global cost of order KT .

3 Slow rates

√
In this section we show that when αi is of order 1/

Section 1.2  our algorithm has an approximation error of order(cid:112)log(T )/T over the very general

Ti  as motivated by the parametric model of

class of smooth convex functions. We refer to this as the slow rate. Our analysis is based on the
classical study of the Frank-Wolfe algorithm [see  e.g. 22  and references therein]. We consider the
case of C-smooth convex functions on the unit simplex  for which we recall the deﬁnition.
Deﬁnition (Smooth functions). For a set D ⊂ Rn  a function f : D → R is said to be a C-smooth
function if it is differentiable and if its gradient is C-Lipshitz continuous  i.e. the following holds

(cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ C(cid:107)x − y(cid:107)2   ∀x  y ∈ D .

We denote by FC K the set of C-smooth convex functions. They attain their minimum at a point
p(cid:63) ∈ ∆K and their Hessian is uniformly bounded  ∇2L(p) (cid:22) CIK  if they are twice differentiable.
We establish in this general setting a slow rate when αi decreases like 1/
Theorem 2 (Slow rate). Let L be a C-smooth convex function over the unit simplex ∆K. For
any T ≥ 1  after T steps of the UCB Frank-Wolfe algorithm with a bandit feedback such that

αt i(Ti  δ) = 2(cid:112)log(t/δ)/Ti and the choice δt = 1/t2  it holds that
(cid:16) π2

(cid:17) 2(cid:107)∇L(cid:107)∞ + (cid:107)L(cid:107)∞

E(cid:2)L(pT )(cid:3) − L(p(cid:63)) ≤ 4

3K log(T )

C log(eT )

(cid:114)

+ K

√

Ti.

+

.

T

T

+

6

T

√

K is expected  and optimal  in bandit setting.

The proof draws inspiration from the analysis of the Frank-Wolfe algorithm with stepsize of 1/(t+1)
and of the UCB algorithm. Notice that our algorithm is adaptive to the gradient Lipschitz constant
C  and that the leading term of the error does not depend on it. We also emphasize the fact that the
dependency in
For linear mappings L(p) = p(cid:62)µ  our analysis is equivalent to studying the UCB algorithm in

multi-armed bandits. The slow rate in Theorem 2 corresponds to a regret of order(cid:112)KT log(T )  the
distribution-independent (or worst case) performance of UCB. The extra dependency in(cid:112)log(T )
could be reduced to(cid:112)log(K) or even optimally to 1 by using conﬁdence intervals more carefully
order(cid:112)K/T for smooth convex functions. We discuss lower bounds further in Section 5.
For the sake of clarity  we state all our results when αt i(Ti  δ) = 2(cid:112)log(t/δ)/Ti  but our techniques
handle more general deviations as αt i(Ti  δ) =(cid:0)θ log(t/δ)/Ti
(cid:1)β where θ ∈ R and β > 0 are some

tailored  for instance by replacing the log(t) term appearing in the deﬁnition of the estimated gradi-
ents by log(T /Ti(t)) or log(T /KTi(t)) if the horizon T is known in advance as in the algorithms
MOSS or ETC (see [4  29  30])  but at the cost of a more involved analysis.
Thus  multi-armed bandits provide a lower bound for the approximation error E[L(pT )] − L(p(cid:63)) of

known parameters. More general results can be found in the supplementary material.

5

4 Fast rates

In this section  we describe situations where the approximation error rate can be improved to a fast
rate of order log(T )/T   when we consider various classes of functions  with additional assumptions.

4.1 Stochastic multi-armed bandits and functions minimized on vertices

A very natural and well-known - yet illustrative - example of such a restricted class of functions is
simply the case of classical bandits where ∆(i) := µi − µ(cid:63) is bounded away from 0 for i (cid:54)= (cid:63). Our
analysis of the algorithm can be adapted to this special case with the following result.
Proposition 3. Let L be the linear function p (cid:55)→ p(cid:62)µ. After T steps of the UCB Frank-Wolfe

algorithm with a bandit feedback such that αt i(Ti  δ) = 2(cid:112)log(t/δ)/Ti  the choices of δt = 1/t2

hold the following

E[L(pT )] − L(p(cid:63)) ≤ 48 log(T )

T

(cid:88)

1

∆(i)

i(cid:54)=(cid:63)

(cid:16) π2

3

+ 3

+ K

(cid:17)√

K(cid:107)µ(cid:107)∞
T

.

The constants of this proposition are sub-optimal (for instance the 48 can be reduced up to 2 us-
ing more careful but involved analysis). It is provided here to show that this classical bound on
the pseudo-regret in stochastic multi-armed bandits [see e.g. 9  and references therein] can be re-
covered with Frank-Wolfe type of techniques illustrating further the links between bandit problems
and convex optimization [20  34]. This result can actually be generalized to any convex functions
which is minimized on a vertex of the simplex with a gradient whose component-wise differences
are bounded away from 0.
Proposition 4. Let L be a convex mapping that attains its minimum on ∆K at a vertex p(cid:63) = ei(cid:63)
and such that ∆(i)(L) := ∇iL(p(cid:63)) − ∇i(cid:63) L(p(cid:63)) > 0 for all i (cid:54)= i(cid:63). Then  after T steps of the UCB

Frank-Wolfe algorithm with a bandit feedback such that αt i(Ti  δ) = 2(cid:112)log(t/δ)/Ti  the choices
(cid:17)

of δt = 1/t2 hold the following
E[L(pT )]−L(p(cid:63)) ≤ ρ(L)

2(cid:107)∇L(cid:107)∞ + (cid:107)L(cid:107)∞

(cid:88)

C log(eT )

+K)

+(

+

1

 

π2
6

T

∆(i)(L)

i(cid:54)=(cid:63)

T

(cid:16) 48 log(T )
(cid:17)

T

(cid:16)

∆min(L)

1 + CK

and ∆min(L) = mini(cid:54)=i(cid:63) ∆(i)(L).

where ρ(L) =
The KKT conditions imply that ∆(i)(L) ≥ 0 whenever p(cid:63) is in a corner  but the strict inequality is
not always guaranteed. In particular  this result may not hold if p(cid:63) is the global minimum of L over
RK. This type of condition has also been linked with rates of convergence in stochastic optimization
problems [15].
The extra multiplicative factor ρ(L) can be large  but it would be of the order of 1 + o(1) using vari-
ants of our algorithms with results that holds only with great probability (typically with conﬁdence

bounds of the form 2(cid:112)log(1/δ)/Ti).

4.2 Strongly convex functions

Another classical assumption in convex optimization is strong convexity  as recalled below. We
denote by Sµ K the set of µ-strongly convex functions of ∆K. This assumption usually improves
the rates in errors of approximation in many settings  even in stochastic optimization or some settings
of online learning [see  e.g. 31  14  33  6  18  19  7]. Interestingly enough though  strong convexity
√
cannot be leveraged to improve rates of convergence in online convex optimization [35  23]  where
the 1/
T rate of convergence cannot be improved. Moreover  leveraging strong convexity usually
implies to adapt step size of gradient descents or with linear search and/or away steps for classical
Frank-Wolfe methods. Those techniques cannot be adapted to our setting where step sizes are ﬁxed.
. For a set D ⊂ Rn  a function f : D → R is said to be a
Deﬁnition (Strongly convex functions).
µ-strongly convex if for all x  y ∈ D  we have

f (x) ≥ f (y) + ∇f (x)(cid:62)(x − y) +

(cid:107)x − y(cid:107)2
2 .

µ
2

6

We already covered the case where the convex functions are minimized outside the simplex. We will
now assume that the minimum lies in its relative interior.
Theorem 5. Let L : ∆K → R be a C-smooth  µ-strongly convex function such that its minimum
p(cid:63) satisﬁes dist(p(cid:63)  ∂∆K) ≥ η  for some η ∈ (0  1/K]. After T steps of the UCB Frank-Wolfe

algorithm with a bandit feedback such that αt i(Ti  δ) = 2(cid:112)log(t/δ)/Ti  it holds that  with the

choice of δt = 1/t2 

E[L(pT )] − L(p(cid:63)) ≤ c1

log2(T )

T

+ c2

log(T )

T

+ c3

1
T

 

for constants c1 = 96K

µη2   c2 = 24

µη3 + C and c3 = 24( 20

µη2 )2K + µη2

2 + C.

The proof is based on an improvement in the analysis of the UCB Frank-Wolfe algorithm  based on
a better control on the duality gap  possible in the strongly convex case. It is a consequence of an
inequality due to Lacoste-Julien and Jaggi [24  Lemma 2]. In order to get the result  we adapt these
ideas to a case of unknown gradient  with bandit feedback. We note that this approach is similar to
the one in [25] that focuses on stochastic optimization problems  as discussed in Section 1.4.
Our framework is more complicated in some aspects than typical settings in stochastic optimization 
where strong assumptions can usually be made over the noisy gradient feedback. These include
stochastic gradients that are independent unbiased estimates of the true gradient  or with error terms
that are decreasing in t. Here  such properties do not hold: as an example  in a parametric setting 
information is only obtained about one of the coefﬁcients  and there are strong dependencies between
successive gradients feedbacks. Dealing with these aspects  as well as the fact that our gradient proxy
is penalized by the size of the conﬁdence intervals  are some of the main challenges of the proof.

4.3 Interior-smooth functions

E[(cid:107)ˆθ − θ(cid:107)2

2] =(cid:80)

Many interesting examples of bandit optimization are not exactly covered by the case of functions
that are C-smooth on the whole unit simplex. In particular  for several applications  the function
diverges at its boundary  as in the examples of Cobb-Douglas utility maximization and variance
minimization from Section 1.3. Recall the the loss was deﬁned by

(cid:80)
i ≥ pi := σi/(cid:80)
max((cid:80)
lower/upper bounds are known beforehand. This leads to a Lipchitz constant C = (9(cid:80)

The gradient Lipschitz constant is inﬁnite but if we knew for instance that σi ∈ [σi   σi]  we could
j σj. We would
safely sample ﬁrst each arm i a linear number of time because p(cid:63)
have (pt)i ≥ pi at all stages and our analysis holds with the constant C = 2σ2
j σj)3/σ3
min .
Even without knowledge on σ2
i   it is possible to quickly have rough estimates  as illustrated by
Lemma 2 in the appendix. Only a logarithmic number of sample of each action are needed. Once
they are gathered  one can keep sampling each arm a linear number of times  as suggested when the
j σj)3/σmin 

which is  up to to a multiplicative factor  the gradient Lipschitz constant at the minimum.

i∈[K]

= 1

T L(p) = 1

T

i∈[K]

σ2
i
Ti

σ2
i
pi

.

5 Lower bounds

The results shown in Sections 3 and 4 exhibit different theoretical guarantees for our algorithm
depending on the class of function considered. We discuss here the optimality of these results.

5.1 Slow rate lower bound

In Theorem 2  we show a slow rate of order(cid:112)K log(T )/T for the error approximation of our algo-

rithm over the class of C-smooth convex functions of RK. Up to the logarithmic term  this result is
optimal: no algorithm based on the same feedback can signiﬁcantly improve the rate of approxima-
tion. This is a consequence of the following theorem  a direct corollary of a result by [5].

7

Theorem 6. For any algorithm based on a bandit feedback such that αt i(Ti  δ) =(cid:112)2 log(t/δ)/Ti

and that outputs ˆpT   we have over the class of linear forms LK that for some constant c > 0

inf
ˆpT

sup
L∈LK

E[L(ˆpT )] − L(p(cid:63))

(cid:111) ≥ c(cid:112)K/T .

This result is established over the class of linear functions over the simplex (for which C = 0)  when
the feedback consists of a draw from a distribution with mean µi. As mentioned in Section 3  the
extra logarithmic term in our upper bound comes from our algorithm  which has the same behavior as
UCB. Nevertheless  as mentioned before  modifying our algorithm to recover the behavior of MOSS
[4]  or even ETC  [see e.g. 29  30]  would improve the upper bound and remove the logarithmic term.

5.2 Fast rate lower bound

We have shown that in the case of strongly convex smooth functions  there is an approximation error
upper bound of order (K/η4) log(T )/T for the performance of our algorithm  where η < 1/K. We
provide a lower bound over this class of functions in the following theorem.

Theorem 7. For any algorithm with a bandit feedback such that αt i(Ti  δ) =(cid:112)2 log(t/δ)/Ti and

output ˆpT   we have over the class S1 K of 1-strongly convex functions that for some constant c > 0

inf
ˆp

sup
L∈S1 K

E[L(ˆpT )] − L(p(cid:63))

(cid:111) ≥ c K 2/T .

(cid:110)

(cid:110)

(cid:110)

(cid:110)

2 when observing a
The proof relies on the complexity of minimizing quadratic functions 1
draw from distribution with mean θi. Our upper bound is in the best case of order K 5 log(T )/T   as
η ≤ 1/K. Understanding more precisely the optimal rate is an interesting venue for future research.

2(cid:107)p − θ(cid:107)2

5.3 Mixed feedbacks lower bound

In our analysis of this problem  we have only considered settings where the feedback upon choosing
action i gives information about the i-th coefﬁcient of the gradient. The two following cases show
that even in simple settings  our upper bounds will not hold if the relationship between action and
feedback is different  when the feedback corresponds to another coefﬁcient.
Proposition 8. For L in the class of 1-strongly convex functions on ∆3  we have in the case of a
mixed bandit feedback that

inf
ˆp

sup
L∈S1 3

E[L(ˆpT )] − L(p(cid:63))

(cid:111) ≥ c/T 2/3 .

For strongly convex functions  even with K = 3  there are therefore pathological mixed feedback
settings where the error is at least of order 1/T 2/3 instead of 1/T . The case of smooth convex
√
functions is covered by the existing lower bounds for the problem of partial monitoring [13]  and
gives a lower bound of order 1/T 1/3 instead of 1/
Proposition 9. For L in the class of linear forms F3 on ∆3  with a mixed bandit feedback we have

T .

(cid:111) ≥ c/T 1/3 .

E[L(ˆpT )] − Lθ(p(cid:63))

inf
ˆp

sup
L∈F3

6 Discussion

We study the online minimization of stochastic global loss with a bandit feedback. This is naturally
motivated by many applications with a parametric setting  and tradeoffs between exploration and
exploitation. The UCB Frank-Wolfe algorithm performs optimally in a generic setting.
The fast rates of convergence obtained for some clases of functions are a signiﬁcant improvement
over the slow rates that hold for smooth convex functions. In bandit-type problems similar to our
problem  it is not always possible to leverage additional assumptions such as strong convexity: It has
been proved impossible in the closely related setting of online convex optimization [23  35]. When it

8

is possible  step sizes must usually depend on the strong convexity parameter  as in gradient descent
[28]. This is not the case here  where the step size is ﬁxed by the mechanics of the problem. We
have also shown that fast rates are possible without requiring strong convexity  with a gap condition
on the gradient at an extreme point  more commonly associated with bandit problems.
We mention that several extensions of our models  motivated by heterogenous estimations  are quite
interesting but out of scope. For instance  assume an experimentalist can chose one of K known
covariates Xi in order to estimate an unknown β ∈ RK  and observes yt = X(cid:62)
(β + ξt)  where
ξt ∼ N (0  Σ). Variants of that problem with covariates or contexts [29] can also be considered.
i (.) are regular functions of covariates ω ∈ Rd. The objective
Assume for instance that µi(.) and σ2
is to estimate all the functions µi(.).

πt

References
[1] A. Agarwal  D. P. Foster  D. Hsu  S. Kakade  and A. Rakhlin. Stochastic convex optimiza-
In Proceedings of the 24th International Conference on Neural

tion with bandit feedback.
Information Processing Systems  2011.

[2] S. Agrawal and N. Devanur. Bandits with concave rewards and convex knapsacks. In Pro-
ceedings of the Fifteenth ACM Conference on Economics and Computation  EC ’14  pages
989–1006  New York  NY  USA  2014. ACM.

[3] S. Agrawal  N. Devanur  and L. Li. An efﬁcient algorithm for contextual bandits with knap-
sacks  and an extension to concave objectives. Proceedings of the Annual Conference on
Learning Theory (COLT)  2016.

[4] J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. Pro-

ceedings of the Annual Conference on Learning Theory (COLT)  2009.

[5] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. Schapire. The non-stochastic multi-armed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[6] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with conver-

gence rate o(1/n). In Adv. NIPS  2013.

[7] F. Bach and V. Perchet. Highly-smooth zero-th order online optimization. COLT 2016  2016.

[8] Q. Berthet and V. Chandrasekaran. Resource allocation for statistical estimation. Proceedings

of the IEEE  104(1):115–125  2016.

[9] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Machine Learning  5(1):1–122  2012.

[10] S. Bubeck  R. Eldan  and Y. T. Lee. Kernel-based methods for bandit convex optimization.

CoRR  abs/1607.03084  2016.

[11] A. Carpentier  A. Lazaric  M. Ghavamzadeh  R. Munos  and A. Antos. Upper-conﬁdence-

bound algorithms for active learning in multi-armed bandits. Preprint  2015.

[12] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University

Press  2006.

[13] N. Cesa-Bianchi  G. Lugosi  and G. Stoltz. Regret minimization under partial monitoring.

Math. Oper. Res.  31(3):562–580  August 2006.

[14] J. Dippon. Accelerated randomized stochastic optimization. Ann. Statist.  31(4):1260–1281 

08 2003.

[15] J. Duchi and F. Ruan. Local asymptotics for some stochastic optimization problems: Optimal-

ity  constraint identiﬁcation  and dual averaging. Arxiv Preprint  2016.

[16] E. Even-Dar  R. Kleinberg  S. Mannor  and Y. Mansour. Online learning for global cost func-

tions. In Proceedings of COLT  2009.

9

[17] M Frank and P. Wolfe. An algorithm for quadratic programming. Naval Res. Logis. Quart. 

3:95–110  1956.

[18] E. Hazan  T. Koren  and K. Levy. Logistic regression: Tight bounds for stochastic and online

optimization. In Proc. Conference On Learning Theory (COLT)  2014.

[19] E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Adv. NIPS  2014.

[20] E. Hazan. The convex optimization approach to regret minimization. Optimization for machine

learning  pages 287–303  2012.

[21] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimiza-

tion. Mach. Learn.  69(2-3):169–192  2007.

[22] M. Jaggi. Sparse Convex Optimization Methods for Machine Learning. PhD thesis  ETH

Zurich  2011.

[23] K. Jamieson  R. Nowak  and B. Recht. Query complexity of derivative-free optimization.

Advances in Neural Information Processing Systems  2012.

[24] S. Lacoste-Julien and M. Jaggi. An afﬁne invariant linear convergence analysis for frank-wolfe

algorithms. NIPS 2013  2013.

[25] J. Lafond  H.-T. Wai  and E. Moulines. On the online frank-wolfe algorithms for convex and

non-convex optimizations. Arxiv Preprint  2015.

[26] S. Mannor  V. Perchet  and G. Stoltz. Approachability in unknown games: Online learning

meets multi-objective optimization. In Proceedings of COLT  2014.

[27] A. Mas-Colell  M.D. Whinston  and J. Green. Microeconomic theory. Oxford University press 

New York  1995.

[28] Y. Nesterov. Introductory Lectures on Convex Optimization. Springer  2003.

[29] V. Perchet and P. Rigollet. The multi-armed bandit problem with covariates. Ann. Statist.. 

41:693–721  2013.

[30] V. Perchet  P. Rigollet  S. Chassang  and E. Snowberg. Batched bandit problems. Ann. Statist. 

44(2):660–681  04 2016.

[31] B. T. Polyak and A. B. Tsybakov. Optimal order of accuracy of search algorithms in stochastic

optimization. Problemy Peredachi Informatsii  26(2):45–53  1990.

[32] A. Rakhlin  K. Sridharan  and A. Tewari. Online learning: Beyond regret. In Sham M. Kakade
and Ulrike von Luxburg  editors  Proceedings of the 24th Annual Conference on Learning
Theory  volume 19 of Proceedings of Machine Learning Research  pages 559–594  Budapest 
Hungary  09–11 Jun 2011. PMLR.

[33] A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization
with bandit feedback. In Proc. International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS)  2011.

[34] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends

in Machine Learning  4(2):107–194  2011.

[35] O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In

Proc. Conference on Learning Theory  2013.

10

,Rishabh Iyer
Shengjie Wang
Wenruo Bai
Jeff Bilmes
Yaniv Tenzer
Alex Schwing
Kevin Gimpel
Tamir Hazan
Quentin Berthet
Vianney Perchet
Xindian Ma
Peng Zhang
Shuai Zhang
Nan Duan
Yuexian Hou
Ming Zhou
Dawei Song