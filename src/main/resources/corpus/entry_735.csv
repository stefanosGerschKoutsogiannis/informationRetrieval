2008,Bayesian Network Score Approximation using a Metagraph Kernel,Many interesting problems  including Bayesian network structure-search  can be cast in terms of finding the optimum value of a function over the space of graphs. However  this function is often expensive to compute exactly. We here present a method derived from the study of reproducing-kernel Hilbert spaces which takes advantage of the regular structure of the space of all graphs on a fixed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate  but that the BDe score itself varies quadratically over the space of all graphs.,Bayesian Network Score Approximation using a

Metagraph Kernel

Benjamin Yackley

Eduardo Corona

Department of Computer Science

Courant Institute of Mathematical Sciences

University of New Mexico

New York University

Terran Lane

Department of Computer Science

University of New Mexico

Abstract

Many interesting problems  including Bayesian network structure-search 
can be cast in terms of ﬁnding the optimum value of a function over the
space of graphs. However  this function is often expensive to compute
exactly. We here present a method derived from the study of Reproducing
Kernel Hilbert Spaces which takes advantage of the regular structure of the
space of all graphs on a ﬁxed number of nodes to obtain approximations
to the desired function quickly and with reasonable accuracy. We then test
this method on both a small testing set and a real-world Bayesian network;
the results suggest that not only is this method reasonably accurate  but
that the BDe score itself varies quadratically over the space of all graphs.

1 Introduction

The problem we address in this paper is  broadly speaking  function approximation. Specif-
ically  the application we present here is that of estimating scores on the space of Bayesian
networks as a ﬁrst step toward a quick way to obtain a network which is optimal given a set
of data. Usually  the search process requires a full recomputation of the posterior likelihood
of the graph at every step  and is therefore slow. We present a new approach to the problem
of approximating functions such as this one  where the mapping is of an object (the graph 
in this particular case) to a real number (its BDe score). In other words  we have a function
f : Γn → R (where Γn is the set of all directed graphs on n nodes) from which we have a
small number of samples  and we would like to interpolate the rest. The technique hinges
on the set Γn having a structure which can be factored into a Cartesian product  as well as
on the function we approximate being smooth over this structure.

Although Bayesian networks are by deﬁnition acyclic  our approximation technique applies
to the general directed-graph case. Because a given directed graph has n2 possible edges 
we can imagine the set of all graphs as itself being a Hamming cube of degree n2 – a
“metagraph” with 2n2
nodes  since each edge can be independently present or absent. We
say that two graphs are connected with an edge in our metagraph if they diﬀer in one and
only one edge. We can similarly identify each graph with a bit string by “unraveling” the
adjacency matrix into a long string of zeros and ones. However  if we know beforehand an
ordering on the nodes of our graph to which all directed graphs must stay consistent (to

2 ). The same correspondence can then be made between these graphs and bit

2(cid:1) possible edges  and the size of our metagraph

drops to 2( n

enforce acyclicness)  then there are only (cid:0) n
strings of length (cid:0) n
2(cid:1).

Since the eigenvectors of the Laplacian of a graph form a basis for all smooth functions on
the graph  then we can use our known sampled values (which correspond to a mapping from
a subset of nodes on our metagraph to the real numbers) to interpolate the others. Despite
the incredible size of the metagraph  we show that this problem is by no means intractable 
and functions can in fact be approximated in polynomial time. We also demonstrate this
technique both on a small network for which we can exhaustively compute the score of every
possible directed acyclic graph  as well as on a larger real-world network. The results show
that the method is accurate  and additionally suggest that the BDe scoring metric used is
quadratic over the metagraph.

2 Spectral Properties of the Hypercube

2.1 The Kronecker Product and Kronecker Sum

The matrix operators known as the Kronecker product and Kronecker sum  denoted ⊗ and
⊕ respectively  play a key role in the derivation of the spectral properties of the hypercube.
Given matrices A ∈ Ri×j and B ∈ Rk×l  A ⊗ B is the matrix in Rik×jl such that:

a11B a12B · · · a1jB
a2jB
a21B a22B

...

. . .

aj1B aj2B

aij B




A ⊗ B =


The Kronecker sum is deﬁned over a pair of square matrices A ∈ Rm×m and B ∈ Rn×n as
A ⊕ B = A ⊗ In + Im ⊗ B  where In denotes an n × n identity matrix[8].

2.2 Cartesian Products of Graphs

The Cartesian product of two graphs G1 and G2  denoted G1 × G2  is intuitively deﬁned as
the result of replacing every node in G1 with a copy of G2 and connecting corresponding
edges together. More formally  if the product is the graph G = G1 × G2  then the vertex
set of G is the Cartesian product of the vertex sets of G1 and G2. In other words  for any
vertex v1 in G1 and any vertex v2 in G2  there exists a vertex (v1  v2) in G. Additionally 
the edge set of G is such that  for any edge (u1  u2) → (v1  v2) in G  either u1 = v1 and
u2 → v2 is an edge in G2  or u2 = v2 and u1 → v1 is an edge in G1.[7]

In particular  the set of hypercube graphs (or  identically  the set of Hamming cubes) can be
derived using the Cartesian product operator. If we denote the graph of an n-dimensional
hypercube as Qn  then Qn+1 = Qn × Q1  where the graph Q1 is a two-node graph with a
single bidirectional edge.

2.3 Spectral Properties of Cartesian Products

The Cartesian product has the property that  if we denote the adjacency matrix of a graph
G as A(G)  then A(G1 × G2) = A(G1) ⊕ A(G2). Additionally  if A(G1) has m eigenvectors
φk and corresponding eigenvalues λk (with k = 1...m) while A(G2) has n eigenvectors ψl
with corresponding eigenvalues µl (with l = 1...n)  then the full spectral decomposition of
A(G1 × G2) is simple to obtain by the properties of the Kronecker sum; A(G1 × G2) will
have mn eigenvectors  each of them of the form φk ⊗ ψl for every possible φk and ψl in the
original spectra  and each of them having the corresponding eigenvalue λk + µl[2].

It should also be noted that  because hypercubes are all k-regular graphs (in particular 
the hypercube Qn is n-regular)  the form of the normalized Laplacian becomes simple. The
usual formula for the normalized Laplacian is:

˜L = I − D−1/2AD−1/2

However  since the graph is regular  we have D = kI  and so

˜L = I − (kI)−1/2A(kI)−1/2 = I −

1
k

A.

Also note that  because the formula for the combinatorial Laplacian is L = D − A  we also
have ˜L = 1

k L.

The Laplacian also distributes over graph products  as shown in the following theorem.

Theorem 1 Given two simple  undirected graphs G1 = (V1  E1) and G2 = (V2  E2)  with
combinatorial Laplacians LG1 and LG2  the combinatorial Laplacian of the Cartesian product
graph G1 × G2 is then given by:

Proof.

LG1×G2 = LG1 ⊕ LG2

LG1 = DG1 − A(G1)

LG2 = DG2 − A(G2)

Here  DG denotes the degree diagonal matrix of the graph G. Now  by the deﬁnition of the
Laplacian 

LG1×G2 = DG1×G2 − A(G1) ⊕ A(G2)

However  the degree of any vertex uv in the Cartesian product is deg(u) + deg(v)  because
all edges incident to a vertex will either be derived from one of the original graphs or the
other  leading to corresponding nodes in the product graph. So  we have

Substituting this in  we obtain

DG1×G2 = DG1 ⊕ DG2

LG1×G2 = DG1 ⊕ DG2 − A(G1) ⊕ A(G2)

= DG1 ⊗ Im + In ⊗ DG2 − A(G1) ⊗ Im − I ⊗ A(G2)
= DG1 ⊗ Im − A(G1) ⊗ Im + In ⊗ DG2 − In ⊗ A(G2)

Because the Kronecker product is distributive over addition[8] 

LG1×G2 = (DG1 − A(G1)) ⊗ Im + In ⊗ (DG2 − A(G2))

= LG1 ⊕ LG2

Additionally  if G1 × G2 is k-regular 

˜LG1×G2 = ˜LG1 ⊕ ˜LG2 =

1
k

(LG1 ⊕ LG2)

Therefore  since the combinatorial Laplacian operator distributes across a Kronecker sum 
we can easily ﬁnd the spectra of the Laplacian of an arbitrary hypercube through a recursive
process if we just ﬁnd the spectrum of the Laplacian of Q1.

2.4 The Spectrum of the Hypercube Qn

First  consider that

A(Q1) =(cid:20) 0 1
1 0 (cid:21) .

This is a k-regular graph with k = 1. So 

LQ1 = I −

1
k

A(Q1) =(cid:20) 1 −1
1 (cid:21)

−1

Its eigenvectors and eigenvalues can be easily computed; it has the eigenvector (cid:2) 1
eigenvalue 0 and the eigenvector h 1

1(cid:3) with
−1i with eigenvalue 2. We can use these to compute the

four eigenvectors of LQ2  the Laplacian of the 2-dimensional hypercube; LQ2 = LQ1×Q1 =
LQ1 ⊕LQ1  so the four possible Kronecker products are [1 1 1 1]T   [1 1 −1 −1]T   [1 −1 1 −1]T  
and [1 − 1 − 1 1]T   with corresponding eigenvalues 0  1  1  and 2 (renormalized by a factor
of 1
2 to take into account that our new hypercube is now degree 2 instead of degree 1;
the combinatorial Laplacian would require no normalization). It should be noted here that
an n-dimensional hypercube graph will have 2n eigenvalues with only n + 1 distinct values;
they will be the values 2k

k = 1

n for k = 0...n  each of which will have multiplicity (cid:0) n

k(cid:1)[4].

If we arrange these columns in the proper order as a matrix  a familiar shape emerges:




1
1
1 −1
1
1 −1 −1

1
1
1 −1
1 −1 −1
1




This is  in fact  the Hadamard matrix of order 4  just as placing our original two eigenvectors
side-by-side creates the order-2 Hadamard matrix. In fact  the eigenvectors of the Laplacian
on a hypercube are simply the columns of a Hadamard matrix of the appropriate size; this
can be seen by the recursive deﬁnition of the Hadamard matrix in terms of the Kronecker
product:

H2n+1 = H2n ⊗ H2

Recall that the eigenvectors of the Kronecker sum of two matrices are themselves all possible
Kronecker products of eigenvectors of those matrices. Since hypercubes can be recursively
constructed using Kronecker sums  the basis for smooth functions on hypercubes (i.e. the
set of eigenvectors of their graph Laplacian) is the Hadamard basis. Consequently  there is
no need to ever compute a full eigenvector explicitly; there is an explicit formula for a given
entry of any Hadamard matrix:

(H2n )ij = (−1)hbi bj i

The notation bx here means “the n-bit binary expansion of x interpreted as a vector of 0s
and 1s”. This is the key to computing our kernel eﬃciently  not only because it takes very
little time to compute arbitrary elements of eigenvectors  but because we are free to compute
only the elements we need instead of entire eigenvectors at once.

3 The Metagraph Kernel

3.1 The Optimization Framework

Given the above  we now formulate the regression problem that will allow us to approximate
our desired function at arbitrary points. Given a set of k observations {yi}k
i=1 corresponding
to nodes xi in the metagraph  we wish to ﬁnd the ˆf which minimizes the squared error
between our estimate and all observed points and also which is a suﬃciently smooth function
on the graph to avoid overﬁtting. In other words 

ˆf = arg min

f ( 1

k

kf (xi) − yik2 + cf T Lmf)

k

Xi=1

The variable m in this expression controls the type of smoothing; if m = 1  then we are
penalizing ﬁrst-diﬀerences (i.e. the gradient of the function). We will take m = 2 in our ex-
periments  to penalize second-diﬀerences (the usual case when using spline interpolation)[6].
This problem can be formulated and solved within the Reproducing Kernel Hilbert Space
framework[9]; consider the space of functions on our metagraph as the sum of two orthogonal
spaces  one (called Ω0) consisting of functions which are not penalized by our regularization

term (which is c ˆf Lm ˆf )  and one (called Ω1) consisting of functions orthogonal to those. In
the case of our hypercube graph  Ω0 turns out to be particularly simple; it consists only of
constant functions (i.e. vectors of the form 1T d  where 1 is a vector of all ones). Meanwhile 
the space Ω1 is formulated under the RKHS framework as a set of columns of the kernel
matrix (denoted K1). Consequently  we can write ˆf = 1T d + K1e  and so our formulation
becomes:

ˆf = arg min

f ( 1

k

k

Xi=1(cid:13)(cid:13)(1T d + K1e)(xi) − yi(cid:13)(cid:13)

2

+ ceT K1e)

The solution to this optimization problem is for our coeﬃcients d and e to be linear estimates
on y  our vector of observed values.
In other words  there exist matrices Υd(c  m) and
Υe(c  m)  dependent on our smoothing coeﬃcient c and our exponent m  such that:

ˆd = Υd(c  m)y
ˆe = Υe(c  m)y

ˆf = 1T ˆd + K1ˆe = Υ(c  m)y

Υ(c  m) = 1T Υd(c  m) + K1Υe(c  m) is the inﬂuence matrix[9] which provides the function
estimate over the entire graph. Because Υ(c  m) is entirely dependent on the two matrices
Υd and Υe as well as our kernel matrix  we can calculate an estimate for any set of nodes
in the graph by explicitly calculating only those rows of Υ which correspond to those nodes
and then simply multiplying that sub-matrix by the vector y. Therefore  if we have an
eﬃcient way to compute arbitrary entries of the kernel matrix K1  we can estimate functions
anywhere in the graph.

3.2 Calculating entries of K1

First  we must choose an order r ∈ {1  2...n}; this is equivalent to selecting the degree of a
polynomial used to perform standard interpolation on the hypercube. The eﬀect that r will
have on our problem will be to select the set of basis functions we consider; the eigenvectors
corresponding to a given eigenvalue 2k
into identically-valued regions which are themselves (n − k)-dimensional hypercubes. For
example  the 3 eigenfunctions on the 3-dimensional hypercube which correspond to the
eigenvalue 2
3 (so k = 1) are those which separate the space into a positive plane and a
negative plane along each of the three axes. Because these eigenfunctions are all equivalent
apart from rotation  there is no reason to choose one to be in our basis over another  and
so we can say that the total number of eigenfunctions we use in our approximation is equal

k(cid:1) eigenvectors which divide the space

n are the (cid:0) n

to Pr

k=0(cid:0) n

k(cid:1) for our chosen value of r.

All eigenvectors can be identiﬁed with a number l corresponding to its position in the
natural-ordered Hadamard matrix; the columns where l is an exact power of 2 are ones that
alternate in identically-sized blocks of +1 and -1  while the others are element-wise products
of the columns correponsing to the ones in l’s binary expansion. Therefore  if we use the
notation |x|1 to mean “the number of ones in the binary expansion of x”  then choosing the
order r is equivalent to choosing a basis of eigenvectors φl such that |l|1 is less than or equal
to r. Therefore  we have:

2k(cid:17)m
(K1)ij = X1≤|l|1≤r(cid:16) n

HilHjl

Because k is equal to |l|1  and because we already have an explicit form for any Hxy  we
can write

(K1)ij =

=

1

2|l|1(cid:19)m
n X1≤|l|1≤r(cid:18) n
2k(cid:17)m
Xk=1(cid:16) n
X|l|1=k

r

1
n

(−1)<bi l>+<bj  l>

(−1)<bi ˙∨bj  l>

The ˙∨ symbol here denotes exclusive-or  which is equivalent to addition mod 2 in this
domain. The justiﬁcation for this is that only the parity of the exponent (odd or even)
matters  and locations in the bit strings bi and bj which are both zero or both one contribute
no change to the overall parity. Notably  this shows that the value of the kernel between
any two bit strings bi and bj is dependent only on bi ˙∨bj  the key result which allows us to

compute these values quickly. If we let Sk(bi  bj) =P|l|1=k(−1)<bi ˙∨bj  l>  there is a recursive

formulation for the computation of Sk(bi  bj) in terms of Sk−1(bi  bj)  which is the method
used in the experiments due to its speed and feasability of computation.

4 Experiments

4.1 The 4-node Bayesian Network

The ﬁrst set of experiments we performed were on a four-node Bayesian Network. We
generated a random “base truth” network and sampled it 1000 times  creating a data set.
We then created an exhaustive set of 26 = 64 directed graphs; there are six possible edges
in a four-node graph  assuming we already have some sort of node ordering that allows us
to orient edges  and so this represented all possibilities. Because we chose the node ordering
to be consistent with our base network  one of these graphs was in fact the correct network.
We then gave each of the set of 64 graphs a log-marginal-likelihood score (i.e. the BDe
score) based on the generated data. As expected  the correct network came out to have
the greatest likelihood. Additionally  computation of the Rayleigh quotient shows that the
function is a globally smooth one over the graph topology. We then performed a set of
experiments using the metagraph kernel.

4.1.1 Randomly Drawn Observations

First  we partitioned the set of 64 observations randomly into two groups. The training
group ranged in size from 3 to 63 samples  with the rest used as the testing group. We
then used the training group as the set of observations  and queried the metagraph kernel to
predict the values of the networks in the testing group. We repeated this process 50 times for
each of the diﬀerent sizes of the training group  and the results averaged to obtain Figure 1.
Note that order 3 performs the best overall for large numbers of observations  overtaking the
order-2 approximation at 41 values observed and staying the best until the end. However 
order 1 performs the best for small numbers of observations (perhaps due to overﬁtting
errors caused by the higher orders) and order 2 performs the best in the middle. The data
suggests that the proper order to which to compute the kernel in order to obtain the best
approximations is a function of both the size of the space and the number of observations
made within that space.

4.1.2 Best/worst-case Observations

Secondly  we performed experiments where the observations were obtained from networks
which were in the neighborhood around the known true maximum  as well as ones from
networks which were as far from it as possible. These results are Figures 2 and 3. Despite
small diﬀerences in shape  the results are largely identical  indicating that the distribution
of the samples throughout Γn matters very little.

4.2 The Alarm Network

The Alarm Bayesian network[1] contains 37 nodes  and has been used in much Bayes-net-
related research[3]. We ﬁrst generated data according to the true network  sampling it 1000
times  then generated random directed graphs over the 37 nodes to see if their scores could
be predicted as well as in the smaller four-node case. We generated two sets of these graphs:
a set of 100  and a set of 1000. We made no attempt to enforce an ordering; although
the graphs were all acyclic  we placed no assumption on the graphs being consistent with
the same node-ordering as the original. The scores of these sets  calculated using the data
drawn from the true network  served as our observed data. We then used the kernel to

r
o
r
r

E
 
d
e
r
a
u
q
s
−
n
a
e
m
−
t
o
o
R

103

102

101

100

10−1

10−2

10−3

 
0

Order 1
Order 2
Order 3
Order 4
Order 5

10

20

Random samples

 

r
o
r
r

 

E
d
e
r
a
u
q
s
−
n
a
e
m
−
o
o
R

t

50

60

70

30

40

Observed Nodes

103

102

101

100

10−1

10−2

10−3

 
0

Samples near Maximum

 

Order 1
Order 2
Order 3
Order 4
Order 5
Order 6

10

20

30

40

Observed Nodes

50

60

70

(a) Figure 1: Randomly-drawn Samples

(b) Figure 2: Samples drawn near maximum

r
o
r
r

E
 
d
e
r
a
u
q
s
−
n
a
e
m
−
o
o
R

t

103

102

101

100

10−1

10−2

10−3

 
0

Samples near Minimum

 

800

750

700

r
o
r
r

Error on approximations of Alarm network data

 

E
 
d
e
r
a
u
q
s
−
n
a
e
m
−
o
o
R

t

650

600

30

40

Observed Nodes

50

60

70

550

500

 
0

2

4

6

8

10

12

14

16

18

20

Order of approximation

Mean of sampled scores
100 observations
1000 observations

Order 1
Order 2
Order 3
Order 4
Order 5
Order 6

10

20

(c) Figure 3: Samples drawn near minimum

(d) Figure 4: Samples from ALARM network

approximate  given the observed scores  the score of an additional 100 randomly-generated
graphs  with the order of the kernel varying from 1 to 20. The results  with root-mean-
squared error plotted against the order of the kernel  are shown in Figure 4. Additionally 
we calculated a baseline by taking the mean of the 1000 sampled scores and calling that the
estimated score for every graph in our testing set.

The results show that the metagraph approximation method performs signiﬁcantly better
than the baseline for low orders of approximation with higher amounts of sampled data.
This makes intuitive sense; the more data there is  the better the approximation should be.
Additionally  the spike at order 2 suggests that the BDe score itself varies quadratically over
the metagraph. To our knowledge  we are the ﬁrst to make this observation. In current work 
we are analyzing the BDe in an attempt to analytically validate this empirical observation.
If true  this observation may lead to improved optimization techniques for ﬁnding the BDe-
maximizing Bayesian network. Note  however  that  even if true  exact optimization is still
unlikely to be polynomial-time because the quadratic form is almost certainly indeﬁnite and 
therefore  NP-hard to optimize.

5 Conclusion

Functions of graphs to real numbers  such as the posterior likelihood of a Bayesian network
given a set of data  can be approximated to a high degree of accuracy by taking advantage of
a hybercubic “metagraph” structure. Because the metagraph is regular  standard techniques
of interpolation can be used in a straightforward way to obtain predictions for the values at
unknown points.

6 Future Work

Although this technique allows for quick and accurate prediction of function values on the
metagraph  it oﬀers no hints (as of yet) as to where the maximum of the function might
be. This could  for instance  allow one to generate a Bayesian network which is likely to be
close to optimal  and if true optimality is required  that approximate graph could be used

as a starting point for a stepwise method such as MCMC. Even without a direct way to
ﬁnd such an optimum  though  it may be worth using this approximation technique inside
an MCMC search instead of the usual exact-score computation in order to quickly converge
on a something close to the desired optimum.

Also  many other problems have a similar ﬂavor.
In fact  this technique should be able
to be used unchanged on any problem which involves the computation of a real-valued
function over bit strings. For other objects  however  the structure is not necessarily a
hypercube. For example  one may desire an approximation to a function of permutations
of some number of elements to real numbers. The set of permutations of a given number
of elements  denoted Sn  has a similarly regular structure (which can be seen as a graph in
which two permutations are connected if a single swap leads from one to the other)  but not
a hypercubic one. The structure-search problem on Bayes Nets can also be cast as a search
over orderings of nodes alone[5]  so a way to approximate a function over permutations
would be useful there as well.

Other domains have this ability to be turned into regular graphs – the integers mod n with
edges between numbers that diﬀer by 1 form a loop  for example. It should be possible to
apply a similar trick to obtain function approximations not only on these domains  but on
arbitrary Cartesian products of them. So  for instance  remembering that the directions of
the edges of Bayesian network are completely speciﬁed given an ordering on the nodes  the
network structure search problem on n nodes can be recast as a function approximation
over the set Sn × Q( n
2 ). Many problems can be cast into the metagraph framework; we have
only just scratched the surface here.

Acknowledgments

The authors would like to thank Curtis Storlie and Joshua Neil from the UNM Department of
Mathematics and Statistics  as well as everyone in the Machine Learning Reading Group at
UNM. This work was supported by NSF grant #IIS-0705681 under the Robust Intelligence
program.

References

[1] I. Beinlich  H.J. Suermondt  R. Chavez  G. Cooper  et al. The ALARM monitoring
system: A case study with two probabilistic inference techniques for belief networks.
Proceedings of the Second European Conference on Artiﬁcial Intelligence in Medicine 
256  1989.

[2] D.S. Bernstein. Matrix Mathematics: Theory  Facts  and Formulas with Application to

Linear Systems Theory. Princeton University Press  2005.

[3] D.M. Chickering  D. Heckerman  and C. Meek. A Bayesian approach to learning Bayesian

networks with local structure. UAI’97  pages 80–89  1997.

[4] Fan R. K. Chung. Spectral Graph Theory. Conference Board of the Mathematical

Sciences. AMS  1997.

[5] N. Friedman and D. Koller. Being Bayesian about network structure. Machine Learning 

50(1-2):95–125  2003.

[6] Chong Gu. Smoothing Splines ANOVA Models. Springer Verlag  2002.

[7] G. Sabidussi. Graph multiplication. Mathematische Zeitschrift  72(1):446–457  1959.

[8] Kathrin Schacke. On the kronecker product. Master’s thesis  University of Waterloo 

2004.

[9] Grace Wahba. Spline Models for Observational Data. CBMS-NSF Regional Conference

Series in Applied Mathematics. SCIAM  1990.

,Elad Richardson
Rom Herskovitz
Boris Ginsburg
Michael Zibulevsky
Mingsheng Long
ZHANGJIE CAO
Jianmin Wang
Michael Jordan