2018,Total stochastic gradient algorithms and applications in reinforcement learning,Backpropagation and the chain rule of derivatives have been prominent; however 
the total derivative rule has not enjoyed the same amount of attention. In this work
we show how the total derivative rule leads to an intuitive visual framework for
creating gradient estimators on graphical models. In particular  previous ”policy
gradient theorems” are easily derived. We derive new gradient estimators based
on density estimation  as well as a likelihood ratio gradient  which ”jumps” to an
intermediate node  not directly to the objective function. We evaluate our methods
on model-based policy gradient algorithms  achieve good performance  and present evidence towards demystifying the success of the popular PILCO algorithm.,Total stochastic gradient algorithms and applications

in reinforcement learning

Paavo Parmas

Neural Computation Unit

Okinawa Institute of Science and Technology Graduate University

Okinawa  Japan

paavo.parmas@oist.jp

Abstract

Backpropagation and the chain rule of derivatives have been prominent; however 
the total derivative rule has not enjoyed the same amount of attention. In this work
we show how the total derivative rule leads to an intuitive visual framework for
creating gradient estimators on graphical models. In particular  previous ”policy
gradient theorems” are easily derived. We derive new gradient estimators based
on density estimation  as well as a likelihood ratio gradient  which ”jumps” to an
intermediate node  not directly to the objective function. We evaluate our methods
on model-based policy gradient algorithms  achieve good performance  and present
evidence towards demystifying the success of the popular PILCO algorithm [5].

1

Introduction

A central problem in machine learning is estimating the gradient of the expectation of a random
Ex∼p(x;ζ) [φ(x)]. Some examples include:
variable with respect to the parameters of the distribution d
dζ
the gradient of the expected classiﬁcation error of a model over the data generating distribution 
the gradient of the expected evidence lower bound w.r.t. the variational parameters in variational
inference [9]  or the gradient of the expected reward w.r.t. the policy parameters in reinforcement
learning [20]. Usually  such an estimator is needed not just through a single computation  but
through a computation graph; a good overview of related problems is given by [18]. Previously 
Schulman et al. provided a method to obtain gradient estimators on stochastic computation graphs by
differentiating a surrogate loss [18]. While the work provided an elegant method to obtain gradient
estimators using automatic differentiation  the resulting stochastic computation graph framework
has formal rules  which uniquely deﬁne one speciﬁc type of estimator  and it is not suitable for
describing general gradient estimation techniques. For example  determinstic policy gradients [19] or
total propagation [14] are not covered by the framework. In contrast  in probabilistic inference  the
successful probabilistic graphical model framework [15] only describes the structure of a model  while
there are many different choices of algorithms to perform inference. We aim for a similar framework
for gradient computation  which we call probabilistic computation graphs. Our framework uses the
total derivative rule df
db
da to decompose the gradient into a sum of partial derivatives along
different computational paths  while leaving open the choice of estimator for the partial derivatives.
We begin by introducing typical gradient estimators in the literature  then explain our new theorem 
novel estimators using a non-standard decomposition of the total derivative  and experimental results.

da = ∂f

∂a + ∂f

∂b

Nomenclature All variables will be considered as column vectors  and gradients are represented
as matrices where each row corresponds to one output variable  and each column corresponds
to one input variable—this allows applying the chain rule by simple matrix multiplication  i.e.
df (x)
dy = ∂f

∂y . Matrices are vectorised with the vec(∗) operator  i.e. dΣ

dx means dvec(Σ)

dx

.

∂x

∂x

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

2 Background: Gradients of expectations

2.1 Pathwise derivative estimators

(cid:104) dφ(x)

(cid:105)

(cid:104) d2φ(x)

(cid:105)

2

dx

dx2

and d
dΣ

Ex∼N (µ Σ)

Ex∼N (µ Σ) [φ(x)] = 1

Ex∼N (µ Σ) [φ(x)] = Ex∼N (µ Σ)

This type of estimator relies on gradients of φ w.r.t. x  e.g.
the Gaussian gradient identities:
d
 
dµ
cited in [17]. The most prominent type of pathwise derivative estimator are reparameterization
(RP) gradients. We focus our discussion on RP gradients  but we mentioned the Gaussian identities
to emphasize that RP gradients are not the only possible pathwise estimators  e.g. the derivative w.r.t.
Σ given above does not correspond to an RP gradient. See [17] for an overview of various options.
RP gradient for a univariate Gaussian To sample from N (µ  σ2)  sample from a standard normal
 ∼ N (0  1)  then transform this: x = µ + σ. The gradients are dx/dµ = 1 and dx/dσ = . The
gradient can then be estimated by sampling: d
. For multivariate Gaussians 
dζ
one can use the Cholesky factor L of Σ = LLT instead of σ. To differentiate the Cholesky
decomposition see [12]. See [17] for other distributions. For a general distribution p(x; ζ)  the RP
gradient deﬁnes a sampling procedure  ∼ p() and a transformation x = f (ζ  )  which allows
moving the derivative inside the expectation d
. The RP gradient
dζ
allows backpropagating the gradient through sampling operations in a graph. It computes partial
derivatives through a speciﬁc operation.

E [φ(x)] = E(cid:104) dφ(x)

Ex∼p(x;ζ) [φ(x)] = E∼p()

(cid:104) dφ

(cid:105)

(cid:105)

dx
dζ

df
dζ

dx

df

2.2 Jump gradient estimators

dζ

We introduce the categorization of jump gradient estimators. Unlike pathwise derivatives  which
compute local partial derivatives and apply the chain rule through numerous computations  jump
gradient estimators can estimate the total derivative directly using only local computations—hence the
naming: the gradient estimator jumps over multiple nodes in a graph without having to differentiate
the nodes inbetween (this will become clearer in later sections in the paper).

Likelihood ratio estimators (LR) Any function f (x) can be stochastically integrated by sampling
q(x) dx = Ex∼q [f (x)/q(x)]. The gradient
dx. By picking q(x) = p(x)  and stochastically
. One must subtract a baseline

from an arbitrary distribution q(x):(cid:82) f (x)dx =(cid:82) q(x) f (x)
of an expectation can be written as(cid:82) φ(x) dp(x;ζ)
integrating  one obtains the LR gradient estimator: E(cid:104) dp(x;ζ)/dζ
from the φ(x) values for this estimator to have acceptable variance: E(cid:104) dp(x;ζ)/dζ

. In
practice using b = E [φ] is a reasonable choice. If b does not depend on the samples  then this leads
to an unbiased gradient estimator. Leave-one-out baseline estimates can be performed to achieve an
unbiased gradient estimator [11]. Other control variate techniques also exist  and this is an active
area of research [7].
In our recent work [14]  we introduced the batch importance weighted LR estimator (BIW-LR)
/P   where we use a mixture
i p(x; ζi)/P   and each ζi depends on another set of parameters θ (in our case
j(cid:54)=i cj i  where the importance

and baselines: BIW-LR:(cid:80)P
distribution q =(cid:80)P
weights are cj i = p(xj; ζi)/(cid:80)P

the policy parameters)  BIW-Baseline: bi =
k=1 p(xj; ζk).

(cid:16) dp(xj ;ζi(θ))/dθ
(cid:80)P
(cid:16)(cid:80)P

(cid:17)
/(cid:80)P

(φ(xj) − bi)

(φ(x) − b)

j(cid:54)=i cj iφ(xj)

p(x;ζ) φ(x)

(cid:80)P

k=1 p(xj ;ζk)

(cid:17)

(cid:105)

(cid:105)

p(x;ζ)

j=1

i=1

Value function based estimators
Instead of using φ(x) directly  one can learn an approximator
ˆφ(x). The approximator will often require less computational time to evaluate  and could be used for
estimating the derivatives. Both LR gradients and pathwise derivatives could be used with evaluations
from the approximator. Moreover  it is not necessary to evaluate just one x point of the estimator 
but one could either use a larger number of samples  or try to directly compute the expectation—this
leads to a Rao-Blackwellized estimator  which is known to have lower variance. Such estimators have
been considered for example in RL in expected sarsa [24  20] as well as in the stochastic variational
inference literature [2  23]  and also in policy gradients [3  1].

2

3 Total stochastic gradient theorem

Sec. 2 explained how to obtain estimators of the expectation through a single computation  while
here we explain how to decompose the gradient of a complicated graph of computations into smaller
sections  which can be readily estimated using the methods in Sec. 2. In our framework  we work
with the gradient of the marginal distribution. This more general problem directly gives one the
gradient of the expectation as well  as the expectation is just a function of the marginal distribution.

3.1 Explanation of framework

We deﬁne probabilistic computation graphs (PCG). The deﬁnition is exactly equivalent to the
deﬁnition of a standard directed graphical model  but it highlights our methods better  and emphasizes
our interest in computing gradients  rather than performing inference. The main difference is the
explicit inclusion of the distribution parameters ζ  e.g. for a Gaussian  the mean µ and covariance Σ.
Deﬁnition 1 (Probabilistic computation graph (PCG)) An acyclic graph with nodes/vertices V
and edges E  which satisfy the following properties:

1. Each node i ∈ V corresponds to a collection of random variables with marginal joint
probability density p(xi; ζi)  where ζi are the possibly inﬁnite parameters of the distribution.
Note that the parameterization is not unique  and any parameterization is acceptable.

2. The probability density at each node is conditionally dependent on the parent nodes:

p(xi|Pai) where Pai are the random variables at the direct parents of node i.

3. The joint probability density satisﬁes: p(x1  ...  xn) =(cid:81)n
at the parents of node i. In particular: p(xi; ζi) =(cid:82) p(xi|Pai)p(Pai; Pzi)dPai

4. Each ζi is a function of its parents: ζi = f (Pzi) where Pzi are the distribution parameters

i=1 p(xi|Pai)

We emphasize that there is nothing stochastic in our formulation. Each computation is determinstic 
although they may be analytically intractable. We also emphasize that this deﬁnition does not exclude
deterministic nodes  i.e. the distribution at a node may be a Dirac delta distribution (a point mass).
Later we will use this formulation to derive stochastic estimates of the gradients.

3.2 Derivation of theorem

We are interested in computing the total derivative of the distribution parameters at one node ζi w.r.t.
the parameters at another node dζi/dζj  e.g. nodes i and j could correspond to φ and x in Sec. 2
respectively. By the total derivative rule: dζi
∂ζi
. Iterating this equation on the
dζj
∂ζm
dζm/dζj terms leads to a sum over paths from node j to node i:

ζm∈Pzi

dζm
dζj

=(cid:80)

(cid:88)

dζi
dζj

=

(cid:89)

P aths(j→i)

Edges(k l)∈P ath

∂ζl
∂ζk

(1)

This equation holds for any deterministic computation graph  and is also well known in e.g. the OJA
community [13]. This equation trivially leads to our total stochastic gradient theorem  which states
that the sum over paths from A to B can be written as a sum over paths from A to intermediate nodes
and from the intermediate nodes to B. Fig. 1 provides examples of the paths in Eq. 2 below.
Theorem 1 (Total stochastic gradient theorem) Let i and j be distinct nodes in a probabilistic
computation graph  and let IN be any set of intermediate nodes  which block the paths from j to i 
i.e. IN is such that there does not exist a path from j to i  which does not pass through a node in
IN. We denote {a → b} is the set of paths from a to b  and {a → b}/c is the set of paths from a to b 
where no node along the path except for b is allowed to be in set c. Then the total derivative dζi/dζj
can be written with the equation below:

∂ζl
∂ζk

3

(cid:88)

 (cid:88)

(cid:89)

 (cid:88)

dζi
dζj

=

m∈IN

s∈{m→i}

(k l)∈s

r∈{j→m}/IN

(p t)∈r

(cid:89)



∂ζt
∂ζp

(2)

i

m

j

i

m

j

(b) {m → i} paths may pass through green nodes.
(a) {j → m} paths may not pass through green nodes.
Figure 1: Example paths in Equation 2. The green nodes correspond to the intermediate nodes IN.

Equations 1 and 2 can be combined to give:

(cid:88)

m∈IN

dζi
dζj

=

(cid:88)

dζi
dζj

=

dζm

(cid:18) dζi
 (cid:88)

(cid:19) (cid:88)

r∈{j→m}/IN

(cid:89)

(p t)∈r

∂ζt
∂ζp

(cid:89)

(cid:18)dζm

dζj

∂ζt
∂ζp


(cid:19)

(3)

(4)

Note that an analogous theorem could be derived by swapping r ∈ {j → m}/IN and s ∈ {m → i}
with r ∈ {j → m} and s ∈ {m → i}/IN respectively. This leads to the equation below:

m∈IN

r∈{m→i}/IN

(p t)∈r

We will refer to Equations 3 and 4 as the second and ﬁrst half total gradient equations respectively.

3.3 Gradient estimation on a graph
Here we clarify one method how the partial derivatives through the nodes m ∈ IN in the previous
section can be estimated. We use the following properties of the estimators in Sec. 2:

• Pathwise derivative estimators compute partial derivatives through a single edge  e.g. ∂ζm
• Jump gradient estimators sum the gradients across all computational paths between two

∂ζj

nodes and directly compute total derivatives  e.g. dζi
dζm

d
dζj

The task is to estimate the derivative of the expectation at a distal node i w.r.t. the parameters at an
Exi∼p(xi;ζi) [xi]  through an intermediate node m. Note that E [xi] can be picked as
earlier node j:
one of the distribution parameters in ζi. The true ζ are intractable  so we perform an ancestral sampling
based estimate ˆζ  i.e. we sample sequentially from each p(x∗|Pa∗) to get a sample through the whole
graph  then ˆζ∗ will simply be the parameters of p(x∗|Pa∗). We refer to one such sample as a particle.
We use a batch of P such particles ˆζ∗ = {ˆζ∗ c}P
c to obtain a mixture distribution as an approximation

to the true distribution. Such a sampling procedure has the properties p(x; ζ) =(cid:82) p(x; ˆζ)p(ˆζ)dˆζ
assume that the sampling is reparameterizable  i.e. p(ˆζm; ζj) =(cid:82) f (ˆζm; ζj  m)p(m)dm. We can

and Exi∼p(xi;ζi) [xi] = Eˆζi∼p(ˆζi;ζj )

. For simplicity in the explanation  we further

xi∼p(xi;ˆζi) [xi]

(cid:104)E

(cid:105)

xi∼p(xi;ˆζi) [xi]

Eˆζi∼p(ˆζi;ζj )

write d
dζj
will be estimated with a pathwise derivative estimator. The remaining term d
dˆζm
be estimated with any other estimator  e.g. a jump estimator could be used.
We summarize the procedure for creating gradient estimators from j to i on the whole graph:

E
. The term ∂ ˆζm
xi∼p(xi;ˆζi) [xi]
∂ζj
E
xi∼p(xi;ˆζi) [xi] will

= Em∼p(m)

d
dˆζm

∂ζj

(cid:104) ∂ ˆζm

(cid:104)E

(cid:105)

(cid:105)

1. Choose a set of intermediate nodes IN  which block the paths from j to i.
2. Construct pathwise derivative estimators from j to the intermediate nodes IN.
3. Construct total derivative estimators from IN to i  and apply Eq. 3 to combine the gradients.

4

G

c2

x2

u2

c3

x3

x0

u0

c1

x1

u1

θ

G

c2

x2

u2

c3

x3

x0

u0

c1

x1

u1

θ

(a) Classical model-free policy gradient

(b) Model-based state-space LR gradient

Figure 2: Probabilistic computation graphs for model-based and model-free LR gradient estimation.

4 Relationship to policy gradient theorems
In typical model-free RL problems [20] an agent performs actions u ∼ π(ut|xt; θ) according to a
stochastic policy π  transitions through states xt  and obtains costs ct (or conversely rewards). The
t=0 ct for

agent’s goal is to ﬁnd the policy parameters θ  which optimize the expected return G =(cid:80)H

each episode. The corresponding probabilistic computation graph is provided in Fig. 2a.
In the literature  two ”gradient theorems” are widely applied: the policy gradient theorem [21]  and
the deterministic policy gradient theorem [19]. These two are equivalent in the limit of no noise [19].

(5)

(6)

Policy gradient theorem

E [G] = E

d
dθ

Deterministic policy gradient theorem

(cid:34)H−1(cid:88)
(cid:34)H−1(cid:88)

t=0

dθ

dut
dθ

E [G] = E

d
dθ

(cid:35)

d log π(ut|xt; θ)

ˆQt(ut  xt)

(cid:35)

d ˆQt(ut  xt)

ˆQt corresponds to an estimator of the remaining return(cid:80)H−1

dut

t=0

h=t ch+1 from a particular state x when
choosing action u. For Eq. 5 any estimator is acceptable  even a sample based estimate could
be used. For Eq. 6  ˆQ is usually a differentiable surrogate model. Fig. 2a shows how these two
theorems correspond to the same probabilistic computation graph. The intermediate nodes are the
actions selected at each time step. The difference lies in the choice of jump estimator to estimate the
total derivative following the intermediate nodes—the policy gradient theorem uses an LR gradient 
whereas the deterministic policy gradient theorem uses a pathwise derivative to a surrogate model. We
believe that the derivation based on a PCG is more intuitive than previous algebraic proofs [21  19].

5 Novel algorithms

In Sec. 3.3 we explained how a particle-based mixture distribution is used for creating gradient
estimators. In the following sections  we instead take advantage of these particles to estimate a
different parameterization Γ  directly for the marginal distribution. Although the algorithms have
general applicability  to make a concrete example  we explain them in reference to model-based
policy gradients using a differentiable model considered in our previous work [14]  for which the
PCG is given in Fig. 2b. Stochastic value gradients [8]  for example  share the same PCG.

5.1 Density estimation LR (DEL)

Following the explanation in Sec. 5  one could attempt to estimate the distribution parameters Γ from
a set of sampled particles  then apply the LR gradient using the estimated distribution q(x; Γ). In

5

particular  we will approximate the density as a Gaussian by estimating the mean ˆµ =(cid:80)P
and variance ˆΣ =(cid:80)P
gradient(cid:80)P

i xi/P
i (xi − ˆµ)2/(P − 1). Then  using the standard LR trick  one can estimate the
(Gi − b)  where q(x) = N (ˆµ  ˆΣ). To use this method  one must compute
derivatives of ˆµ and ˆΣ w.r.t. the particles xi  then carry the gradient to the policy parameters using
the chain rule while differentiating through the model  which is straight-forward. We refer to our new
method as the DEL estimator. Importantly  note that while q(x) is used for estimating the gradient  it
is not in any way used for modifying the trajectory sampling.
Advantages of DEL: One can use LR gradients even if no noise is injected into the computations.
Disadvantages of DEL: The estimator is biased  and density estimation can be difﬁcult.

d log q(xi)

dθ

i

5.2 Gaussian shaping gradient (GS)

c1

θ

cm

c4

u2

u3

x0

u0

xk

u1

x2

xm

x4

G

c2

(p t)∈r

∂ζt
∂ζp

(cid:81)

r∈{θ→xk}/IN

terms(cid:80)

Until now  all RL methods have used the second half total
gradient equation (Eq. 3). Might one create estimators that
use the ﬁrst half equation (Eq. 4)? Fig.3 gives an exam-
ple of how this might be done. We propose to estimate
the density at xm by ﬁtting a Gaussian on the particles.
Then dE [cm] /dΓm (the pink edges) will be estimated by
sampling from this distribution (or by any other method of
integration). This leaves the question of how to estimate
dΓm/dθ (all paths from θ to xm). Using the RP method is
straight-forward. To use the LR method  we ﬁrst apply the
second half total gradient equation on dΓm/dθ to obtain
(blue edges) and dΓm
dζxk
(red edges). In the scenarios we consider  the ﬁrst of these
terms is a single path  and will be estimated using RP. The
second term is more interesting  and we will estimate this
using an LR method.
As we are using a Gaussian approximation  the distribution parameters Γm are the mean and vari-
m. We can
(xm − bµ)
 

ance of xm  which can be estimated as µm = E [xm] and Σm = E(cid:2)xmxT
E(cid:2)xmxT

d
dζxk
In practice  we perform a sampling based estimate ˆζxk  and one might be concerned that the
estimators are conditional on the sample ˆζxk  but we are interested in unconditional estimates.
We will explain that the conditional estimate is equivalent. For the variance  note that µm is
an estimate of the unconditional mean  so the whole estimate directly corresponds to an es-
timate of the unconditional variance. For the mean  apply the rule of iterated expectations:
Exk∼p(xk;ζxk ) [xm] = Eˆζxk∼p(ˆζxk )
from which it is clear that the conditional
gradient estimate is an unbiased estimator for the gradient of the unconditional mean.

Figure 3: Computational paths in Gaus-
sian shaping gradient

(cid:3) − µmµT
(cid:104) d log p(xk;ζxk )

(cid:3) = Exk∼p(xk;ζxk )

(cid:105)
m − bΣ)

E [xm] = Exk∼p(xk;ζxk )
(xmxT

(cid:104) d log p(xk;ζxk )

obtain LR gradient estimates of these terms

(µµT ) = 2µ d
dζxk

xk∼p(xk;ˆζxk ) [xm]

E(cid:2)xT

(cid:105)
(cid:3).

(cid:104)E

and

d
dζxk

d
dζxk

m

dζxk

(cid:105)

m

dζxk

m

(

duk−1

dΓm
dζxk

dζxk
duk−1

Efﬁcient algorithm for accumulating gradients
In Fig. 3  for each xk node  we want to perform
an LR jump to every xm node after k and compute a gradient with the Gaussian approximation
of the distribution at node m. We will accumulate across all nodes during a backwards pass in
a backpropagation like manner. Note that for each k and each m  we can write the gradient as
dE[cm]
d log p(xk;ζxk )
  where zm
dΓm
corresponds to a vector summarizing the xm − bµ  etc. terms above. Note that dE[cm]
zm is just a
scalar quantity gm. We thus use an algorithm which accumulates a sum of all g during a backwards
pass  and sums over all m nodes at each k node. See Alg. 1 for a detailed explanation of how
it ﬁts together with total propagation [14]. The ﬁnal algorithm essentially just replaces the usual
cost/reward with a modiﬁed value  and such an approach would also be applicable in model-free
policy gradient algorithms using a stochastic policy and LR gradients.

is estimated as dE[cm]
dΓm

). The term dE[cm]
dΓm

dΓm
dζxk

dζxk

zm

dΓm

dθ

Two interpretations of GS 1. We are making a Gaussian approximation of the marginal distri-
bution at a node. 2. We are performing a type of reward shaping based on the distribution of the

6

Algorithm 1 Gaussian shaping gradient with total propagation

Gaussian shaping gradient for model-based policy search while combining both LR and RP variants
using total propagation—an algorithm introduced in our previous work [14].
Forward pass: Sample a set of particle trajectories.
Backward pass:

(cid:46) ζ are the distribution parameters  e.g. all of the µ

vectorization operator which stacks the elements in a matrix/tensor into a column vector

(cid:46) Estimate the marginal distribution as a Gaussian
  e.g. by sampling from this Gaussian  and using the RP gradient
(cid:46) vec(∗) is a

(cid:3)(cid:1); wi t = vec(cid:0)mi tµT

i t − E(cid:2)xtxT

(cid:1)

t

t

(vi t − 2wi t)

(cid:46) g is a scalar replacing the usual cost/reward
(cid:46) G is the return (the cost of the remaining trajectory)
(cid:46) Direct derivative of expected cost for the RP gradient

= 0  dJ

dθ = 0  GT +1 = 0

Initialise: dGT +1
dζT +1
and σ for each particle
for t = T to 1 do

µt = E [xt]; Σt = E(cid:2)xtxT
(cid:3) − µtµT
mi t = xi t − µt; vi t = vec(cid:0)xi txT

Compute: dE[ct]
dµt
for each particle i do

and dE[ct]
dΣt

t

t

dµt
dxi t

mi t + dE[ct]
dΣt
+ dE[ct]
dΣt
+ dζi t+1
dui t
dζi t+1
dxi t
d log p(xi t)

gi t = dE[ct]
dµt
Gi t = Gi t+1 + gi t
dE[ct]
= dE[ct]
dµt
dxi t
dζi t+1
= ∂ζi t+1
dxi t
∂xi t
dGRP
= ( dGi t+1
i t
dζi t+1
dζi t
dGLR
i t
dζi t
dGRP
i t
dθ =
dGLR
i t
dθ =

= Gi t
dGRP
i t
dζi t
dGLR
i t
dζi t

dui t−1
dζi t

dζi t
dζi t

dui t−1

dθ

dui t−1

dθ

dui t−1

dΣt
dxi t
dui t
dxi t
+ dE[ct]
dxi t

) dxi t
dζi t

(cid:46) In principle  one could further subtract a baseline from G

(cid:21)

(cid:20) dGRP
(cid:17)
(cid:80)P

i t
dθ

LR

end for
RP = trace(V
σ2

(cid:16)

RP

1 + σ2
σ2

kLR = 1/
dθ = dJ
dJ
i
for each particle i do
dGLR
i t
dζi t

dθ + kLR

= kLR

1
P

dGi t
dζi t
end for

(cid:20) dGLR
(cid:80)P

i t
dθ

i

(cid:21)

)

(cid:46) The sample variance of the particles

(cid:46) Weight to combine LR and RP estimators
dGRP
i t
dθ

(cid:46) Combine LR and RP in θ space

(cid:46) Combine LR and RP in state space

); σ2

LR = trace(V

dGLR
i t

dθ + (1 − kLR) 1

P

+ (1 − kLR)

dGRP
i t
dζi t

end for

particles. In particular we are essentially promoting the trajectory distributions to stay unimodal 
such that all of the particles concentrate at one ”island” of reward rather than splitting the distribution
between multiple regions of reward—this may simplify optimization.

6 Experiments

We performed model-based RL simulation experiments from the PILCO papers [5  4]. We tested the
cart-pole swing-up and balancing problems to test our GS approach  as well as combinations with total
propagation [14]. We also tested the DEL approach on the simpler cart-pole balancing-only-problem
to show the feasibility of the idea. We compared particle-based gradients with our new estimators to
PILCO. In our previous work [14]  we had to change the cost function to obtain reliable results using
particles—one of the primary motivations of the current experiments was to match PILCO’s results
using the same cost as the original PILCO had used (this is explained in greater detail in Section 6.4).

6.1 Model-based policy search background

We consider a model-based analogue to the model-free policy search methods introduced in Section 4.
The corresponding probabilistic computation graph is given in Fig. 2b. Our notation follows our

7

previous work [14]. After each episode all of the data is used to learn separate Gaussian process
models [16] of each dimension of the dynamics  s.t. p(∆xa
t ]T
and x ∈ RD  u ∈ RF . This model is then used to perform ”mental simulations” between the
episodes to optimise the policy by gradient descent. We used a squared exponential covariance
a (˜x− ˜x(cid:48))). We use a Gaussian likelihood function  with
a exp(−(˜x− ˜x(cid:48))T Λ−1
function ka(˜x  ˜x(cid:48)) = s2
n a. The hyperparameters  {s  Λ  σn} are trained by maximizing the marginal
noise hyperparameter σ2
likelihood. The predictions have the form p(xa
f (˜xt) is an
uncertainty about the model  and depends on the availability of data in a region of the state-space.

t+1) = GP(˜xt)  where ˜x = [xT

t+1) = N (µ(˜xt)  σ2

t   uT

f (˜xt) + σ2

n)  where σ2

6.2 Setup

The cart-pole consists of a cart that can be pushed back and forth  and an attached pole. The state
space is [s  β  ˙s  ˙β]  where s is the cart position and β the angle. The control is a force on the cart.
The dynamics were the same as in a PILCO paper [4]. The setup follows our prior work [14].

Common properties in tasks The experiments consisted of 1 random episode followed by 15
episodes with a learned policy  where the policy is optimized between episodes. Each episode
length was 3s  with a 10Hz control frequency. Each task was evaluated separately 100 times with
different random number seeds to test repeatability. The random number seeds were shared across
different algorithms. Each episode was evaluated 30 times  and the cost was averaged  but note
that this was done only for evaluation purposes—the algorithms only had access to 1 episode. The
policy was optimized using an RMSprop-like learning rule [22] from our previous work [14]  which
normalizes the gradients using the sample variance of the gradients from different particles. In the
model-based policy optimization  we performed 600 gradient steps using 300 particles for each
policy gradient evaluation. The learning rate and momentum parameters were α = 5 × 10−4 
γ = 0.9 respectively—the same as in our previous work. The output from the policy was saturated by
sat(u) = 9 sin(u)/8 + sin(3u)/8  where u = ˜π(x). The policy ˜π was a radial basis function network
(a sum of Gaussians) with 50 basis functions and a total of 254 parameters. The cost functions were
of the type 1 − exp(−(x − t)T Q(x − t))  where t is the target. We considered two types of cost
functions: 1) Angle Cost  a cost where Q = diag([1  1  0  0]) is a diagonal matrix  2) Tip Cost  a cost
from the original PILCO papers  which depends on the distance of the tip of the pendulum to the
position of the tip when it is balanced. These cost functions are conceptually different—with the Tip
Cost the pendulum could be swung up from either direction  with the Angle Cost there is only one
correct direction. The base observation noise levels were σs = 0.01 m  σβ = 1 deg  σ ˙s = 0.1 m/s 
σ ˙β = 10 deg/s  and these were modiﬁed with a multiplier k ∈ {10−2  1}  such that σ2 = kσ2
Cart-pole swing-up and balancing In this task the pendulum starts hanging downwards  and
must be swung up and balanced. We took some results from our previous work [14]: PILCO;
reparameterization gradients (RP); Gaussian resampling (GR); batch importance weighted LR  with a
batch importance weighted baseline (LR); total propagation combining BIW-LR and RP (TP). We
compared to the new methods: Gaussian shaping gradients using the BIW-LR component (GLR) 
Gaussian shaping gradients combining BIW-LR and RP variants using total propagation (GTP).
Moreover  we tested GTP when the model noise variance was multiplied by 25 (GTP+σn).

base.

Cart-pole balancing with DEL estimator This task is much simpler—the pole starts upright and
must be balanced. The experiment was devised to show that DEL is feasible and may be useful if
further developed. The Angle Cost and the base noise level were used.

6.3 Results

The results are presented in Table 1 and in Fig. 4. Similarly to our previous work [14]  with low noise 
methods which include LR components do not work well. However  the GTP+σn experiments show
that injecting more noise into the model predictions can solve the problem. The main important result
is that GTP matches PILCO in the Tip Cost scenarios. In our previous work [14]  one of the concerns
was that TP had not matched PILCO in this scenario. Looking only at the costs in Fig. 4b and 4c does
not adequately display the difference. In contrast  the success rates show that TP did not perform as
well. The success rates were measured both by a threshold which was calibrated in previous work
(ﬁnal loss below 15) as well as by visually classifying all experimental runs. Both methods agreed.

8

Table 1: Success rate of learning cart-pole swing-up

Cost func.
Angle Cost
Angle Cost
Tip Cost
Tip Cost

o multiplier
σ2
k = 10−2
k = 1
k = 10−2
k = 1

PILCO RP
0.88
0.69
0.79
0.74
0.92
0.44
0.73
0.15

GR
0.63
0.89
0.47
0.68

LR
0.57
0.96
0.36
0.28

TP
0.82
0.99
0.54
0.48

GTP GLR GTP+σn
0.65
0.9
0.6
0.69

0.42
0.93
0.45
0.35

0.88

0.8

(a) Cart-pole balancing only

(b) Swing-up and balancing

All experimental runs

(c) Swing-up and balancing
Top 40 experimental runs

Figure 4: Data-efﬁciency and performance of learning algorithms on cart-pole tasks. Figures 4b and
4c correspond to the k = 1  Tip Cost case.

The losses of the peak performers at the ﬁnal episode were TP: 11.14 ± 1.73  GTP: 9.78 ± 0.40 
PILCO: 9.10 ± 0.22  which also show that TP was signiﬁcantly worse. While the peak performers
were still improving  the remaining experiments had converged. PILCO still appears slightly more
data-efﬁcient; however  the difference has little practical signiﬁcance as the required amount of data
is low. Also note that in Fig. 4b TP has smaller variance. The larger variance of GTP and PILCO
is caused by outliers with a large loss. These outliers converged to a local minimum  which takes
advantage of the tail of the Gaussian approximation of the state distribution—this contrasts with prior
suggestions that PILCO performs exploration using the tail of the Gaussian [5].

6.4 Discussion

Our work demystiﬁes the factors which contributed to the success of PILCO. It was previously
suggested that the Gaussian approximations in PILCO smooth the reward  and cause unimodal
trajectory distributions  simplifying the optimization problem [10  6]. In our previous work [14]  we
showed that the main advantage was actually that it prevents the curse of chaos/exploding gradients.
In the current work we decoupled the gradient and reward effects  and provided evidence that both
factors contributed to the success of Gaussian distributions. While GR often has similar performance
to GTP  there is an important conceptual difference: GR performs resampling  hence the trajectory
distribution is not an estimate of the true trajectory distribution. Moreover  unlike resampling  GTP
does not remove the temporal dependence in particles  which may be important in some applications.

7 Conclusions & future work

We have created an intuitive graphical framework for visualizing and deriving gradient estimators in
a graph of probabilistic computations. Our method provides new insights towards previous policy
gradient theorems in the literature. We derived new gradient estimators based on density estimation
(DEL)  as well as based on the idea to perform a jump estimation to an intermediate node  not
directly to the expected cost (GS). The DEL estimator needs to be further developed  but it has good
conceptual properties as it should not suffer from the curse of chaos nor does it require injecting noise
into computations. The GS estimator allows differentiating through discrete computations in a manner
that will still allow backpropagating pathwise derivatives. Finally  we provided additional evidence
towards demystifying the success of the popular PILCO algorithm. We hope that our work could lead
towards new automatic gradient estimation software frameworks which are not only concerned with
computational speed  but also the accuracy of the estimated gradients.

9

5101505101520253035DEL top 20 performersDEL all experimental runsEpisode #Average cumulative cost5101505101520253035GTPPILCOTPEpisode #Average cumulative cost5101505101520253035GTPPILCOTPEpisode #Average cumulative costAcknowledgments

We thank the anonymous reviewers for useful comments. This work was supported by OIST Graduate
School funding and by JSPS KAKENHI Grant Number JP16H06563 and JP16K21738.

References
[1] Asadi  K.  Allen  C.  Roderick  M.  Mohamed  A.-r.  Konidaris  G.  and Littman  M. (2017).

Mean actor critic. arXiv preprint arXiv:1709.00503.

[2] AUEB  M. T. R. and L´azaro-Gredilla  M. (2015). Local expectation gradients for black box
variational inference. In Advances in neural information processing systems  pages 2638–2646.

[3] Ciosek  K. and Whiteson  S. (2017). Expected policy gradients. arXiv preprint arXiv:1706.05374.

[4] Deisenroth  M. P.  Fox  D.  and Rasmussen  C. E. (2015). Gaussian processes for data-efﬁcient
learning in robotics and control. IEEE Transactions on Pattern Analysis and Machine Intelligence 
37(2):408–423.

[5] Deisenroth  M. P. and Rasmussen  C. E. (2011). PILCO: A model-based and data-efﬁcient

approach to policy search. In International Conference on Machine Learning  pages 465–472.

[6] Gal  Y.  McAllister  R.  and Rasmussen  C. (2016). Improving PILCO with bayesian neural

network dynamics models. In Workshop on Data-efﬁcient Machine Learning  ICML.

[7] Greensmith  E.  Bartlett  P. L.  and Baxter  J. (2004). Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research  5(Nov):1471–1530.

[8] Heess  N.  Wayne  G.  Silver  D.  Lillicrap  T.  Erez  T.  and Tassa  Y. (2015). Learning continuous
control policies by stochastic value gradients. In Advances in Neural Information Processing
Systems  pages 2944–2952.

[9] Hoffman  M. D.  Blei  D. M.  Wang  C.  and Paisley  J. (2013). Stochastic variational inference.

The Journal of Machine Learning Research  14(1):1303–1347.

[10] McHutchon  A. (2014). Modelling nonlinear dynamical systems with Gaussian Processes. PhD

thesis  University of Cambridge.

[11] Mnih  A. and Rezende  D. (2016). Variational inference for Monte Carlo objectives.

International Conference on Machine Learning  pages 2188–2196.

In

[12] Murray  I. (2016). Differentiation of the Cholesky decomposition.

arXiv:1602.07527.

arXiv preprint

[13] Naumann  U. (2008). Optimal Jacobian accumulation is NP-complete. Mathematical Program-

ming  112(2):427–441.

[14] Parmas  P.  Rasmussen  C. E.  Peters  J.  and Doya  K. (2018). PIPPS: Flexible model-based

policy search robust to the curse of chaos. In International Conference on Machine Learning.

[15] Pearl  J. (2014). Probabilistic reasoning in intelligent systems: networks of plausible inference.

Elsevier.

[16] Rasmussen  C. E. and Williams  C. K. I. (2006). Gaussian Processes for Machine Learning.

MIT Press.

[17] Rezende  D. J.  Mohamed  S.  and Wierstra  D. (2014). Stochastic backpropagation and

approximate inference in deep generative models. arXiv preprint arXiv:1401.4082.

[18] Schulman  J.  Heess  N.  Weber  T.  and Abbeel  P. (2015). Gradient estimation using stochastic
computation graphs. In Advances in Neural Information Processing Systems  pages 3528–3536.

[19] Silver  D.  Lever  G.  Heess  N.  Degris  T.  Wierstra  D.  and Riedmiller  M. (2014). Determinis-

tic policy gradient algorithms. In International Conference on Machine Learning.

10

[20] Sutton  R. S. and Barto  A. G. (1998). Reinforcement learning: An introduction  volume 1. MIT

press Cambridge.

[21] Sutton  R. S.  McAllester  D. A.  Singh  S. P.  and Mansour  Y. (2000). Policy gradient methods
In Advances in neural information

for reinforcement learning with function approximation.
processing systems  pages 1057–1063.

[22] Tieleman  T. and Hinton  G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning  4(2):26–31.

[23] Tokui  S. and Sato  I. (2017). Evaluating the variance of likelihood-ratio gradient estimators. In

International Conference on Machine Learning  pages 3414–3423.

[24] Van Seijen  H.  Van Hasselt  H.  Whiteson  S.  and Wiering  M. (2009). A theoretical and
empirical analysis of expected sarsa. In Adaptive Dynamic Programming and Reinforcement
Learning  2009. ADPRL’09. IEEE Symposium on  pages 177–184. IEEE.

11

,Paavo Parmas