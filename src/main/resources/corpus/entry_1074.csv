2019,Concentration of risk measures: A Wasserstein distance approach,Known finite-sample concentration bounds for the Wasserstein distance between the empirical and true distribution of a random variable are used to derive a two-sided  concentration bound for the error between the true conditional value-at-risk (CVaR) of a (possibly unbounded) random variable and a standard estimate of its CVaR computed from an i.i.d. sample. The bound applies under fairly general assumptions on the random variable  and improves upon previous bounds which were either one sided  or applied only to bounded random variables. Specializations of the bound to sub-Gaussian and sub-exponential random variables are also derived.   A similar procedure is followed to derive concentration bounds for the error between the true and estimated Cumulative Prospect Theory (CPT) value of a random variable  in cases where the random variable is bounded or sub-Gaussian. These bounds are shown to match a known bound in the bounded case  and improve upon the known bound in the sub-Gaussian case. The usefulness of the bounds is illustrated through an algorithm  and corresponding regret bound for a stochastic bandit problem  where the underlying risk measure to be optimized is CVaR.,Concentration of risk measures:
A Wasserstein distance approach

Sanjay P. Bhat

Tata Consultancy Services Limited

Hyderabad  Telangana  India
sanjay.bhat@tcs.com

Department of Computer Science and Engineering

Indian Institute of Technology Madras  India

prashla@cse.iitm.ac.in

Prashanth L.A.

∗

Abstract

Known ﬁnite-sample concentration bounds for the Wasserstein distance between
the empirical and true distribution of a random variable are used to derive a two-
sided concentration bound for the error between the true conditional value-at-risk
(CVaR) of a (possibly unbounded) random variable and a standard estimate of its
CVaR computed from an i.i.d. sample. The bound applies under fairly general
assumptions on the random variable  and improves upon previous bounds which
were either one sided  or applied only to bounded random variables. Specializations
of the bound to sub-Gaussian and sub-exponential random variables are also
derived. Using a different proof technique  the results are extended to the class
of spectral risk measures having a bounded risk spectrum. A similar procedure
is followed to derive concentration bounds for the error between the true and
estimated Cumulative Prospect Theory (CPT) value of a random variable  in cases
where the random variable is bounded or sub-Gaussian. These bounds are shown
to match a known bound in the bounded case  and improve upon the known bound
in the sub-Gaussian case. The usefulness of the bounds is illustrated through an
algorithm  and corresponding regret bound for a stochastic bandit problem  where
the underlying risk measure to be optimized is CVaR.

1

Introduction

Conditional Value-at-Risk (CVaR) and cumulative prospect theory (CPT) value are two popular risk
measures. CVaR is popular in ﬁnancial applications  where it is necessary to minimize the worst-case
losses  say in a portfolio optimization context. CVaR is a special instance of the class of spectral
risk measures [Acerbi  2002]. CVaR is an appealing risk measure because it is coherent [Artzner
et al.  1999]  and spectral risk measures retain this property. CPT value is a risk measure  proposed
by Tversky and Kahnemann  that is useful for modeling human preferences. The central premise in
risk-sensitive optimization is that the expected value is not an appealing objective in several practical
applications  and it is necessary to incorporate some notion of risk in the optimization process. The
reader is referred to extensive literature on risk-sensitive optimization  in particular  the shortcomings
of the expected value - cf. [Allais  1953  Ellsberg  1961  Kahneman and Tversky  1979  Rockafellar
and Uryasev  2000].
In practical applications  the information about the underlying distribution is typically unavailable.
However  one can often obtain samples from the distribution  and the aim is to estimate the chosen
risk measure using these samples. We consider this problem of estimation in the context of three
risk measures: CVaR  a general spectral risk measure  and CPT-value. For each of the three
risk measures  we examine the estimator obtained by applying the risk measure to the empirical

∗Supported in part by a DST grant under the ECRA program.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

distribution constructed from an i.i.d. sample. In the case of CVaR and CPT value  the estimators
obtained in this way are already available in the literature. Our goal is to derive concentration bounds
for estimators of all three risk measures  and we achieve this in a novel manner by relating the
estimation error to the Wasserstein distance between the empirical and true distributions  and then
using known concentration bounds for the latter. We summarize our contributions below  which apply
when the underlying distribution has a bounded exponential moment  or a higher-order moment.
Sub-Gaussian distributions are a popular class that satisfy the former condition  while the latter
includes sub-exponential distributions.
(1) For the case of CVaR  we provide a two-sided concentration bound for both classes of distributions
mentioned above. In particular  for the special case of sub-Gaussian distributions  our tail bound

is of the order O(cid:0)exp(cid:0)−cn2(cid:1)(cid:1)  where n is the number of samples   is the accuracy parameter 

and c is a universal constant. Our bound matches the rate obtained for distributions with bounded
support in [Brown  2007]  and features improved dependence on  as compared to the one derived
for sub-Gaussian distributions in [Kolla et al.  2019b]. Further  unlike the latter work  we provide
two-sided concentration bounds for CVaR estimation. Similar bounds are shown to hold for any
spectral risk measure having a bounded risk spectrum.

(2) For the case of CPT-value  we obtain an order O(cid:0)exp(cid:0)−cn2(cid:1)(cid:1) for the case of distributions with

bounded support  matching the rate in [Cheng et al.  2018]. For the case of sub-Gaussian distributions 
we provide a bound that has an improved dependence on the number of samples n  as compared to
the corresponding bound derived by [Cheng et al.  2018].
(3) As a minor contribution  our concentration bounds open avenues for bandit applications  and we
illustrate this claim by considering a risk-sensitive bandit setting  with CVaR as the underlying risk
measure. For this bandit problem with underlying arms’ distribution assumed to be sub-Gaussian 
we derive a regret bound using the CVaR concentration bound mentioned above. Previous works
(cf. [Galichet et al.  2013]) consider CVaR optimization in a bandit context  with arms’ distributions
having bounded support.
Since CVaR and spectral risk measures are weighted averages of the underlying distribution quantiles 
a natural alternative to a Wasserstein-distance-based approach is to employ concentration results for
quantiles such as in Kolla et al. [2019b]. While such an approach can provide bounds with better
constants  the resulting bounds also involve distribution-dependent quantities (see Kolla et al. [2019b] 
for instance)  and require different proofs for sub-Gaussian and sub-exponential random variables. In
contrast  our approach provides a uniﬁed method of proof.
The rest of the paper is organized as follows: In Section 2  we cover background material that includes
Wasserstein distance  and sub-Gaussian and sub-exponential distributions. In Section 3–5  we present
concentration bounds for CVaR  spectral risk measures and CPT-value estimation  respectively. In
Section 6  we discuss a bandit application  and ﬁnally  in Section 7  we provide the concluding
remarks. The proofs of all the claims in Sections 3–5 are given in the supplementary material.

2 Wasserstein Distance

In this section  we introduce the notion of Wasserstein distance  a popular metric for measuring the
proximity between two distributions. The reader is referred to Chapter 6 of [Villani  2008] for a
detailed introduction.
Given two cumulative distribution functions (CDFs) F1 and F2 on R  let Γ(F1  F2) denote the set of
all joint distributions on R2 having F1 and F2 as marginals.
Deﬁnition 1. Given two CDFs F1 and F2 on R  the Wasserstein distance between them is deﬁned by

(cid:20)

(cid:90)

(cid:21)

W1(F1  F2) =

inf

F∈Γ(F1 F2)

R2

|x − y|dF (x  y)

.

(1)

Given L > 0 and p > 0  a function f : R → R is L-Hölder of order p if |f (x) − f (y)| ≤ L|x − y|p
for all x  y ∈ R. The function f : R → R is L-Lipschitz if it is L-Hölder of order 1. Finally  if F is
a CDF on R  we deﬁne the generalized inverse F −1 : [0  1] → R of F by F −1(β) = inf{x ∈ R :
F (x) ≥ β}. In the case where F is strictly increasing and continuous  F −1 equals the usual inverse
of a bijective function.

2

The Wasserstein distance between the CDFs F1 and F2 of two random variables X and Y   respectively 
may be alternatively written as follows:
sup|E (f (X)) − E(f (Y ))| = W1(F1  F2) =

|F1(s)−F2(s)|ds =

(cid:90) ∞

(cid:90) 1

|F −1

(β)−F −1

(β)|dβ 
(2)
where the supremum in (2) is over all functions f : R → R that are 1-Lipschitz. Equation (2) is
stated and proved as a lemma in Bhat and Prashanth [2019].
The results that we provide in this paper pertain to the case where a r.v. X satisﬁes either an
exponential moment bound or a higher-order moment bound. We make these conditions precise
below.

(C1) There exist β > 0 and γ > 0 such that E(cid:0)exp(cid:0)γ|X − µ|β(cid:1)(cid:1) < (cid:62) < ∞  where µ = E(X).
(C2) There exists β > 0 such that E(cid:0)|X − µ|β(cid:1) < (cid:62) < ∞  where µ = E(X).

−∞

0

1

2

We next deﬁne sub-Gaussian and sub-exponential r.v.s.  which are two popular classes of unbounded
r.v.s  that satisfy assumptions (C1) and (C2)  respectively.
Deﬁnition 2. A r.v. X with mean µ is sub-Gaussian if there exists a σ > 0 such that

E(exp (λ(X − µ))) ≤ exp

for any λ ∈ R.

(cid:18)

(cid:18) (X − µ)2

(cid:19)(cid:19)

E

exp

4σ2

√

≤

A sub-Gaussian r.v. X with mean µ satisﬁes (see items (II) and (IV) in Theorem 2.1 of [Wainwright 
2019] for a proof)

2  and P (X − µ > η) ≤ 8 exp(− η2

2σ2 )  for η ≥ 0.

(3)
√

The ﬁrst bound above implies that sub-Gaussian r.v.s satisfy (C1) with β = 2  γ = 1
In particular  bounded r.v.s are sub-Gaussian  and satisfy (C1) with β = 2.
Deﬁnition 3. Given σ > 0  a r.v. X with mean µ is σ sub-exponential if there exist non-negative
parameters σ and b such that

2.

4σ2 and (cid:62) =

(cid:18) λ2σ2

(cid:19)

2

(cid:18) λ2σ2

(cid:19)

2

E(exp (λ(X − µ))) ≤ exp

for any |λ| <

1
b

.

(cid:34)E(cid:2)(X − µ)k(cid:3)

(cid:35) 1

k

sup
k≥2

k!

A sub-exponential r.v. X with mean µ satisﬁes (see items (III) and (IV) in Theorem 2.2 of [Wainwright 
2019] for a proof)

<∞  and ∃k1  k2 > 0 such that P (X − µ > η) ≤ k1 exp(−k2η) ∀η ≥ 0.
(4)

The bound (4) implies that sub-exponential r.v.s satisfy (C2) for integer values of β ≥ 2.
The following result from Fournier and Guillin [2015] bounds the Wasserstein distance between the
empirical distribution function (EDF) of an i.i.d. sample and the underlying CDF from which the
sample is drawn. Recall that  given X1  . . .   Xn i.i.d. samples from the distribution F of a r.v. X  the
EDF Fn is deﬁned by

I{Xi ≤ x}  

for any x ∈ R.

(5)

n(cid:88)

i=1

Fn (x) =

1
n

Lemma 1. (Wasserstein distance bound) Let X be a r.v. with CDF F and mean µ. Suppose that
either (i) X satisﬁes (C1) with β > 1  or (ii) X satisﬁes (C2) with β > 2. Then  for any  ≥ 0  we
have

where  under (i) 

P (W1(Fn  F ) > ) ≤ B(n  ) 

B(n  ) = C(cid:0)exp(cid:0)−cn2(cid:1) I{ ≤ 1} + exp(cid:0)−cnβ(cid:1) I{ > 1}(cid:1)  

3

for some C  c that depend on the parameters β  γ and (cid:62) speciﬁed in (C1); and under (ii) 

(cid:16)

exp(cid:0)−cn2(cid:1) I{ ≤ 1} + n (n)

−(β−η)/p I{ > 1}(cid:17)

.

B(n  ) = C

where η could be chosen arbitrarily from (0  β)  while C  c depend on the parameters β  η and (cid:62)
speciﬁed in (C2).

Proof. The lemma follows directly by applying Theorem 2 in [Fournier and Guillin  2015] to the
random variable X − µ  and noting from (2) that the Wasserstein distance remains invariant if the
same constant is added to both random variables.

3 Conditional Value-at-Risk

We now introduce the notion of CVaR  a risk measure that is popular in ﬁnancial applications.
Deﬁnition 4. The CVaR at level α ∈ (0  1) for a r.v X is deﬁned by

Cα(X) = inf
ξ

ξ +

1

(1 − α)

E (X − ξ)+

  where (y)+ = max(y  0).

(cid:26)

(cid:27)

n(cid:88)

i=1

It is well known (see [Rockafellar and Uryasev  2000]) that the inﬁmum in the deﬁnition of CVaR
above is achieved for ξ = VaRα(X)  where VaRα(X) = F −1(α) is the value-at-risk of the random
variable X at conﬁdence level α. Thus CVaR may also be written alternatively as given  for instance 
in [Kolla et al.  2019b]. In the special case where X has a continuous distribution  Cα(X) equals the
expectation of X conditioned on the event that X exceeds VaRα(X).
All our results below pertain to i.i.d. samples X1  . . .   Xn drawn from the distribution of X. Follow-
ing Brown [2007]  we estimate Cα(X) from such a sample by

(cid:40)

(cid:41)

cn α = inf
ξ

ξ +

1

n(1 − α)

(Xi − ξ)+

.

(6)

We now provide a concentration bound for the empirical CVaR estimate (6)  by relating the estima-
tion error |cn α − Cα(X)| to the Wasserstein distance between the true and empirical distribution
functions  and subsequently invoking Lemma 1 that bounds the Wasserstein distance between these
two distributions. The proof is given in section 5 of Bhat and Prashanth [2019].
Proposition 1. Suppose X either satisﬁes (C1) for some β > 1 or satisﬁes (C2) for some β > 2.
Under (C1)  for any  > 0  we have

P (|cn α − Cα(X)| > ) ≤ C(cid:2)exp(cid:2)−cn(1 − α)22(cid:3) I{ ≤ 1} + exp(cid:2)−cn(1 − α)ββ(cid:3) I{ > 1}(cid:3).
−(β−η) I{ > 1}(cid:105)

(cid:104)

Under (C2)  for any  > 0  we have
P (|cn α − Cα(X)| > ) ≤ C
In the above  the constants C  c and η are as in Lemma 1.

exp(cid:2)−cn(1 − α)22(cid:3) I{ ≤ 1} + n (n(1 − α))

.

The following corollary  which specializes Proposition 1 to sub-Gaussian random r.v.s.  is immediate 
as sub-Gaussian random variables satisfy (C1) with β = 2.
Corollary 1. For a sub-Gaussian r.v. X  we have that

P (|cn α − Cα(X)| > ) ≤ 2C exp(cid:0)−cn(1 − α)22(cid:1)   for any  ≥ 0 

where C  c are constants that depend on the sub-Gaussianity parameter σ.

In terms of dependence on n and   the tail bound above is better than the one-sided concentration
bound in [Kolla et al.  2019b]. In fact  the dependence on n and  matches that in the case of bounded
distributions (cf. [Brown  2007  Wang and Gao  2010]).
The case of sub-exponential distributions can be handled by specializing the second result in Propo-
sition 1. In particular  observing that sub-exponential distributions satisfy (C2) for any β ≥ 2  and
Proposition 1 requires β > 2 in case (ii)  we obtain the following bound:

4

Corollary 2. For a sub-exponential r.v. X  for any  ≥ 0  we have

(cid:104)

exp(cid:2)−cn(1 − α)22(cid:3) I{ ≤ 1}+n [n(1 − α)]η−3 I{ > 1}(cid:105)

 

P (|cn α − Cα(X)| > )≤ C

where C  c and η are as in Lemma 1.
For small deviations  i.e.   ≤ 1  the bound above is satisfactory  as the tail decay matches that of
a Gaussian r.v. with constant variance. On the other hand  for large   the second term exhibits
polynomial decay. The latter polynomial term is not an artifact of our analysis  and instead  it relates
to the rate obtained in case (ii) of Lemma 1. Sub-exponential distributions satisfy an exponential
moment bound with β = 1  and for this case  the authors in [Fournier and Guillin  2015] remark
that they were not able to obtain a satisfactory concentration result. Recently  Prashanth et al. [2019]
have derived an improved bound for the sub-exponential case using a technique not based on the
Wasserstein distance.

4 Spectral risk measures
Spectral risk measures are a generalization of CVaR. Given a weighting function φ : [0  1] → [0 ∞) 
the spectral risk measure Mφ associated with φ is deﬁned by

Mφ(X) =

φ(β)F −1(β)dβ 

(7)

where X is a random variable with CDF F . If the weighting function  also known as the risk spectrum 
is increasing and integrates to 1  then Mφ is a coherent risk measure like CVaR. In fact  CVaR is
itself a special case of (7)  with Cα(X) = Mφ for the risk spectrum φ = (1 − α)−1I{β ≥ α} (see
Acerbi [2002] and Dowd and Blake [2006] for details).
Given an i.i.d. sample X1  . . .   Xn drawn from the CDF F of a random variable X  a natural
empirical estimate of the spectral risk measure Mφ(X) of X is

(cid:90) 1

0

(cid:90) 1

0

mn φ =

φ(β)F −1

n (β)dβ.

(8)

In this section  we restrict ourselves to a spectral risk measure Mφ whose associated risk spectrum
φ is bounded. Speciﬁcally  we assume that |φ(β)| ≤ K for all β ∈ [0  1] for some K > 0. It
immediately follows from (7) and (2) that  if X and Y are random variables with CDFs F1 and F2 
then

(9)
On noting from (8) that the empirical estimate mn φ of Mφ(X) is simply the spectral risk measure
Mφ applied to a random variable whose CDF is Fn  we conclude from (9) that

|Mφ(X) − Mφ(Y )| ≤ KW1(F1  F2).

|Mφ(X) − mn φ| ≤ KW1(F  Fn).

(10)
Equation (10) relates the estimation error |Mφ(X) − mn φ| to the Wasserstein distance between the
true and empirical CDFs of X. As in the case of CVaR  invoking Lemma 1 provides concentration
bounds for the empirical spectral risk measure estimate (8). The detailed proof is available in Section
5 of Bhat and Prashanth [2019].
Proposition 2. Suppose X either satisﬁes (C1) for some β > 1 or satisﬁes (C2) for some β > 2. Let
K > 0 and let φ : [0  1] → [0  K] be a risk spectrum for some K > 0. Under (C1)  for any  > 0 
we have
P (|mn φ − Mφ(X)| > ) ≤ C

I{ ≤ 1} + exp

I{ > 1}

−cn

−cn

exp

.

(cid:20)
(cid:20)

(cid:20)
(cid:20)

(cid:111)2(cid:21)
(cid:110) 
(cid:111)2(cid:21)
(cid:110) 

K

K

(cid:20)
(cid:110) 

K

(cid:16)

n

(cid:111)β(cid:21)

(cid:21)
(cid:110) 
(cid:111)(cid:17)−(β−η)/p I{ > 1}

K

(cid:21)

.

Under (C2)  for any  > 0  we have
P (|mn φ − Mφ(X)| > ) ≤ C

exp

−cn

I{ ≤ 1} + n

In the above  the constants C  c and η are as in Lemma 1.

5

The following corollary specializing Proposition 2 to sub-Gaussian random r.v.s. is immediate  as
sub-Gaussian random variables satisfy (C1) with β = 2.
Corollary 3. For a sub-Gaussian r.v. X and a risk spectrum as in Proposition 2  we have

P (|mn φ − Mφ(X)| > ) ≤ 2C exp(cid:0)−cn2/K 2(cid:1)   for any  ≥ 0 

where C  c are constants that depend on σ.

It is possible to specialize Proposition 2 to the case of sub-exponential random variables to obtain a
corollary similar to Corollary 2. However  in the interests of space  we do not present it here.
Technically speaking  the concentration bounds for CVaR from Section 3 follow from the results of
this section  since CVaR is a special case of a spectral risk measure. However  the proof technique of
Section 3 uses a different characterization of the Wassserstein distance  and is based on a different
formula for CVaR. We therefore believe that the independent proofs given for the results of Section 3
are interesting in their own right.

5 CPT-value estimation

For any r.v. X  the CPT-value is deﬁned as

(cid:90) ∞

w+(cid:0)P(cid:0)u+(X) > z(cid:1)(cid:1) dz −

(cid:90) ∞

w−(cid:0)P(cid:0)u−(X) > z(cid:1)(cid:1) dz 

0

0

C(X) =

(11)
Let us deconstruct the above deﬁnition. The functions u+  u− : R → R+ are utility functions that are
assumed to be continuous  with u+(x) = 0 when x ≤ 0 and increasing otherwise  and with u−(x) =
0 when x ≥ 0 and decreasing otherwise. The utility functions capture the human inclination to play
safe with gains and take risks with losses – see Fig 1. Second  w+  w− : [0  1] → [0  1] are weight
functions  which are assumed to be continuous  non-decreasing and satisfy w+(0) = w−(0) = 0
and w+(1) = w−(1) = 1. The weight functions w+  w− capture the human inclination to view
probabilities in a non-linear fashion. Tversky and Kahneman [1992]  Barberis [2013] (see Fig 2
from Tversky and Kahneman [1992]) recommend the following choices for w+ and w−  based on
inference from experiments involving human subjects:

w+(p) =

p0.61

(p0.61 + (1 − p)0.61)

  and w−(p) =

1

0.61

p0.69

(p0.69 + (1 − p)0.69)

.

1

0.69

(12)

Utility

Losses

Gains

1

0.8

0.6

0.4

0.2

)
p
(
w

t
h
g
i
e

W

u+

−u−

Figure 1: Utility function

0

0

0.2

0.4
0.6
Probability p

0.8

1

w(p) =

p0.61

(p0.61+(1−p)0.61)1/0.61

w(p) = p

Figure 2: Weight function

We now recall CPT-value estimation proposed in [Prashanth et al.  2016]. Let Xi  i = 1  . . .   n
denote n samples from the distribution of X. The EDF for u+(X) and u−(X)  for any given real-
valued functions u+ and u−  is deﬁned as follows: ˆF +
n (x) =

I{(u+(Xi) ≤ x)}   ˆF −

I{(u−(Xi) ≤ x)} . Using EDFs  the CPT-value is estimated as follows:

n (x) = 1
n

(cid:80)n

i=1

1
n

i=1

(cid:80)n

w−(1 − ˆF −

n (x))dx.

(13)

(cid:90) ∞

0

Cn =

(cid:90) ∞

0

w+(1 − ˆF +

n (x))dx −

6

(cid:16)

(cid:17)

(cid:16)

(cid:17)

1 − ˆF +

1 − ˆF −

and

n (x)

n (x)

for
Notice that we have substituted the complementary EDFs
P (u+(X) > x) and P (u−(X) > x)  respectively  in (11)  and then performed an integration of the
weight function composed with the complementary EDF. As shown in Section III of [Prashanth
et al.  2016]  the ﬁrst and second integral in (13) can be easily computed using the order statistics
{X(1)  . . .   X(n)}.
For the purpose of analysis  as in [Cheng et al.  2018]  we make the following assumption:
(C3) The weight functions w+  w− are L-Hölder continuous of order α ∈ (0  1) for some constant
L > 0.
In this paper  we are interested in deriving a concentration bound for the estimator in (13). To put
things in context  in [Cheng et al.  2018]  the authors derive a concentration bound assuming that
the underlying distribution has bounded support  and for this purpose  they employ the Dvoretzky-
Kiefer-Wolfowitz (DKW) theorem (cf. Chapter 2 of [Wasserman  2015]). Interestingly  we are able to
provide a matching bound for the case of distributions with bounded support  using a proof technique
that relates the the estimation error |Cn − C(X)| to the Wasserstein distance between the empirical
and true CDF  and this is the content of the proposition below (see Section 5 of Bhat and Prashanth
[2019] for the proof).
Proposition 3. (CPT concentration for bounded r.v.s) Let X1  . . .   Xn be i.i.d. samples of a r.v.
X that is bounded a.s. in [−T1  T2]  where T1  T2 ≥ 0  and at least one of T1  T2 is positive. Let
T (cid:44) max{u+(T2)  u−(−T1)}. Then  under (C3)  we have

(cid:18)

(cid:104)

(cid:105)1/α(cid:19)

P (|Cn − C(X)| > ) ≤ 2B



n 

2LT 1−α

  for any  ≥ 0 

where B(· ·) is as given in i) of Lemma 1 with β = 2.
From the form for B(· ·) in Lemma 1  it is apparent that |Cn − C(X)| <  with probability 1 − δ  if

the number of samples n is of the order O(cid:0)1/2/α log(cid:0) 1
(cid:1)(cid:1)  for any δ ∈ (0  1).
(cid:90) τn

Next  we provide a CPT concentration result for the case when the underlying r.v. is unbounded 
but sub-Gaussian. For this case  we consider a modiﬁed CPT value estimator based on truncation 
namely 

(cid:90) τn

δ

˜Cn =

w+(1 − ˆF +

n (z))dz −

w−(1 − ˆF −

n (z))dz 

0

log n +

where the sample-size-dependent truncation threshold τn is speciﬁed in the result below. The proof is
available in Bhat and Prashanth [2019].
Proposition 4. (CPT concentration for sub-Gaussian r.v.s) Let X1  . . .   Xn be i.i.d. samples from
the distribution of X. Suppose that u+(X) and u−(X) are sub-Gaussian r.v.s with parameter
log log n >
max (E(u+(X))  E(u−(X))) + 1  we have

log log n(cid:1) for all n ≥ 1. Then  for all n satisfying σ
σ. Set τn = σ(cid:0)√
α for every  >
(cid:33) 2
P(cid:16)(cid:12)(cid:12)(cid:12) ˜Cn − C(X)
(cid:12)(cid:12)(cid:12) > 
(cid:17) ≤ 2C exp
(cid:19)

is
  and it is apparent that the bound we obtain is signiﬁcantly

corresponding bound provided in Proposition 3 of Cheng et

where C  c are constants that depend on the sub-Gaussianity parameter σ.

−cn

 − 8Lσ2
√
αnα/2
L
log n

2+α + 2e−n

8Lσ2
αnα/2

α
2+α ( 

[2018]

(cid:32)

2
α

2H )

(cid:18)

√

al.

 

0

√

α

The
2ne−n
improved.

6 CVaR-sensitive bandits

The concentration bound for CVaR estimation in Proposition 1 opens avenues for bandit applications.
We illustrate this claim by using the regret minimization framework in a stochastic K-armed bandit
problem  with an objective based on CVaR. While CVaR optimization has been considered in a bandit

7

setting in the literature (cf. Galichet et al. [2013])  the underlying arms’ distributions there have
bounded support. We relax this assumption  and consider the case of sub-Gaussian distributions for
the K arms. The tail bounds in Kolla et al. [2019b] and Kolla et al. [2019a] do not allow a bandit
application  because forming the conﬁdence term (required for UCB-type algorithms) using their
bound would require knowledge of the density in a neighborhood of the true VaR. In contrast  the
constants in our bounds depend only on the sub-Gaussian parameter σ  and several classic MAB
algorithms (including UCB) assume this information.
Suppose we are given K arms with unknown distributions Pi  i = 1  . . .   K. The interaction of
the bandit algorithm with the environment proceeds  over n rounds  as follows: (i) select an arm
It ∈ {1  . . .   K}; (ii) Observe a sample cost from the distribution PIt corresponding to the arm It.
Let Cα(i) denote the CVaR  with conﬁdence α ∈ (0  1)  of the distribution Pi corresponding to arm i 
for i = 1  . . .   K. Let C∗ = mini=1 ... K Cα(i) denote the lowest CVaR among the K distributions 
and ∆i = (Cα(i) − C∗) denote the gap in CVaR values of arm i and that of the best arm.
The classic objective in a bandit problem is to ﬁnd the arm with the lowest expected value. We
consider an alternative formulation  where the goal is to ﬁnd the arm with the lowest CVaR. Using
the notion of regret  this objective is formalized as follows:

Rn =(cid:80)K

i=1 Cα(i)Ti(n) − nC∗ =(cid:80)K

i=1 Ti(n)∆i 

I{It = i} is the number of pulls of arm i up to time instant n.

where Ti(n) =(cid:80)n

t=1

Next  we present a straightforward adaptation CVaR-LCB of the well-known UCB algorithm [Auer
et al.  2002] to handle an objective based on CVaR. The algorithm plays each arm once in the
initialization phase  and in each of the remaining rounds t = K + 1  . . .   n  plays the arm  say It 
with the lowest UCB value  that is  It = arg min
i=1 ... K

LCBt(i) with

(cid:115)

LCBt(i) = ci Ti(t−1) − 2
1 − α

log (Ct)
c Ti(t − 1)

 

where ci Ti(t−1) is the empirical CVaR for arm i computed using (6) from Ti(t − 1) samples  and
C  c are constants that depend on the sub-Gaussianity parameter σ (see Corollary 1).
The result below bounds the regret of CVaR-LCB algorithm  and the proof is a straightforward
adaptation of that used to establish the regret bound of the regular UCB algorithm in [Auer et al. 
2002] (see Bhat and Prashanth [2019] for details).
Theorem 1. For a K-armed stochastic bandit problem where the the arms’ distributions are sub-
Gaussian with parameter σ = 1  the regret Rn of CVaR-LCB satisﬁes
π2
3

E(Rn) ≤ (cid:88)

16 log(Cn)
(1 − α)2∆i

+ K

∆i.

1 +

{i:∆i>0}

Further  Rn satisﬁes the following bound that does not scale inversely with the gaps:

(cid:18)
(cid:18) π2

3

(cid:19)
(cid:19)(cid:88)

i

(cid:112)Kn log(Cn) +

+ 1

∆i.

E(Rn) ≤

8

(1 − α)

7 Conclusions

We used ﬁnite sample bounds from Fournier and Guillin [2015] for the Wasserstein distance between
the empirical and true distributions of a random variable to derive two-sided concentration bounds
for the error between the true and empirical CVaR  spectral risk measure and CPT-value of a random
variable. Our bounds hold for random variables that either have ﬁnite exponential moment  or ﬁnite
higher-order moment  and specialize nicely to sub-Gaussian and sub-exponential random variables.
The bound further improves upon previous similar results  which either gave one-sided bounds  or
applied only to bounded random variables. In addition  to illustrate the usefulness of our concentration
bounds  we used our CVaR concentration bound to provide a regret-bound analysis for an algorithm
for a bandit problem where the risk measure to be optimized is CVaR.

8

References
C. Acerbi. Spectral measures of risk: A coherent representation of subjective risk aversion. Journal

of Banking and Finance  26:1505 – 1518  2002.

M. Allais. Le comportement de l’homme rationel devant le risque: Critique des postulats et axioms

de l’ecole americaine. Econometrica  21:503–546  1953.

T. M. Apostol. Mathematical Analysis. Addison-Wesley  2nd edition  1974.

P. Artzner  F. Delbaen  J. Eber  and D. Heath. Coherent measures of risk. Mathematical ﬁnance  9(3):

203–228  1999.

P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47(2-3):235–256  2002.

N. C. Barberis. Thirty years of prospect theory in economics: A review and assessment. Journal of

Economic Perspectives  pages 173–196  2013.

S. P. Bhat and L. A. Prashanth. Concentration of risk measures: A Wasserstein distance approach.

arXiv preprint arXiv:1902.10709v2  2019.

D. B. Brown. Large deviations bounds for estimating conditional value-at-risk. Operations Research

Letters  35(6):722–730  2007.

J. Cheng  L. A. Prashanth  M. C. Fu  S. I. Marcus  and C. Szepesvári. Stochastic optimization in
a cumulative prospect theory framework. IEEE Transactions on Automatic Control  2018. doi:
10.1109/TAC.2018.2822658.

K. Dowd and D. Blake. After VaR: The theory  estimation and insurance applications of quantile-

based risk measures. The Journal of Risk and Insurance  73(2):193–229  2006.

D. A. Edwards. On the Kantorovich–Rubinstein Theorem. Expositiones Mathematicae  29(4):

387–398  2011.

D. Ellsberg. Risk  ambiguity and the Savage’s axioms. The Quarterly Journal of Economics  75(4):

643–669  1961.

N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical

measure. Probability Theory and Related Fields  162(3-4):707–738  2015.

N. Galichet  M. Sebag  and O. Teytaud. Exploration vs exploitation vs safety: Risk-aware multi-armed

bandits. In Asian Conference on Machine Learning  pages 245–260  2013.

C. R. Givens and R. M. Shortt. A class of Wasserstein metrics for probability distributions. Michigan

Mathematical Journal  31(2):231–240  1984.

D. Kahneman and A. Tversky. Prospect theory: An analysis of decision under risk. Econometrica:

Journal of the Econometric Society  pages 263–291  1979.

R. K. Kolla  L. A. Prashanth  and K. P. Jagannathan. Risk-aware multi-armed bandits using conditional

value-at-risk. CoRR  abs/1901.00997  2019a.

R. K. Kolla  L.A. Prashanth  S. P. Bhat  and K. Jagannathan. Concentration bounds for empirical
conditional value-at-risk: The unbounded case. Operations Research Letters  47(1):16 – 20  2019b.

L. A. Prashanth  J. Cheng  M. C. Fu  S. I. Marcus  and C. Szepesvári. Cumulative prospect theory
meets reinforcement learning: prediction and control. In International Conference on Machine
Learning  pages 1406–1415  2016.

L. A. Prashanth  K. Jagannathan  and R. K. Kolla. Concentration bounds for CVaR estimation: The

cases of light-tailed and heavy-tailed distributions  2019.

R. T. Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of Risk  2(3):

21–41  2000.

9

A. Tversky and D. Kahneman. Advances in prospect theory: Cumulative representation of uncertainty.

Journal of Risk and Uncertainty  5(4):297–323  1992.

S. S. Vallander. Calculation of the Wasserstein distance between probability distributions on the line.

Theory of Probability and its Applications  18(4):784–786  1974.

C. Villani. Optimal transport: old and new  volume 338. Springer Science & Business Media  2008.

M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in

Statistical and Probabilistic Mathematics. Cambridge University Press  2019.

Y. Wang and F. Gao. Deviation inequalities for an estimator of the conditional value-at-risk. Opera-

tions Research Letters  38(3):236–239  2010.

L. A. Wasserman. All of Nonparametric Statistics. Springer  2015.

10

,Sanjay P. Bhat
Prashanth L.A.