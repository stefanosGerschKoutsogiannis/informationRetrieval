2019,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks,Recent efforts to improve the interpretability of deep neural networks use saliency to characterize the importance of input features to predictions made by models. Work on interpretability using saliency-based methods on Recurrent Neural Networks (RNNs) has mostly targeted language tasks  and their applicability to time series data is less understood. In this work we analyze saliency-based methods for RNNs  both classical and gated cell architectures. We show that RNN saliency vanishes over time  biasing detection of salient features only to later time steps and are  therefore  incapable of reliably detecting important features at arbitrary time intervals. To address this vanishing saliency problem  we propose a novel RNN cell structure (input-cell attention)  which can extend any RNN cell architecture. At each time step  instead of only looking at the current input vector  input-cell attention uses a fixed-size matrix embedding  each row of the matrix attending to different inputs from current or previous time steps.  Using synthetic data  we show that the saliency map produced by the input-cell attention RNN is able to faithfully detect important features regardless of their occurrence in time. We also apply the input-cell attention RNN on a neuroscience task analyzing functional Magnetic Resonance Imaging (fMRI) data for human subjects performing a variety of tasks. In this case  we use saliency to characterize brain regions (input features) for which activity is important to distinguish between tasks. We show that standard RNN architectures are only capable of detecting important brain regions in the last few time steps of the fMRI data  while the input-cell attention model is able to detect important brain region activity across time without latter time step biases.,Input-Cell Attention Reduces Vanishing Saliency

of Recurrent Neural Networks

Aya Abdelsalam Ismail1  Mohamed Gunady1  Luiz Pessoa2

Héctor Corrada Bravo∗1   Soheil Feizi ∗1

{asalam mgunady}@cs.umd.edu  pessoa@umd.edu 

hcorrada@umiacs.umd.edu  sfeizi@cs.umd.edu
1 Department of Computer Science  University of Maryland

2 Department of Psychology  University of Maryland

Abstract

Recent efforts to improve the interpretability of deep neural networks use saliency to
characterize the importance of input features to predictions made by models. Work
on interpretability using saliency-based methods on Recurrent Neural Networks
(RNNs) has mostly targeted language tasks  and their applicability to time series
data is less understood.
In this work we analyze saliency-based methods for
RNNs  both classical and gated cell architectures. We show that RNN saliency
vanishes over time  biasing detection of salient features only to later time steps and
are  therefore  incapable of reliably detecting important features at arbitrary time
intervals. To address this vanishing saliency problem  we propose a novel RNN
cell structure (input-cell attention†)  which can extend any RNN cell architecture.
At each time step  instead of only looking at the current input vector  input-cell
attention uses a ﬁxed-size matrix embedding  each row of the matrix attending to
different inputs from current or previous time steps. Using synthetic data  we show
that the saliency map produced by the input-cell attention RNN is able to faithfully
detect important features regardless of their occurrence in time. We also apply the
input-cell attention RNN on a neuroscience task analyzing functional Magnetic
Resonance Imaging (fMRI) data for human subjects performing a variety of tasks.
In this case  we use saliency to characterize brain regions (input features) for which
activity is important to distinguish between tasks. We show that standard RNN
architectures are only capable of detecting important brain regions in the last few
time steps of the fMRI data  while the input-cell attention model is able to detect
important brain region activity across time without latter time step biases.

1

Introduction

Deep Neural Networks (DNNs) are successfully applied to a variety of tasks in different domains 
often achieving accuracy that was not possible with conventional statistical and analysis methods.
Nevertheless  practitioners in ﬁelds such as neuroscience  medicine  and ﬁnance are hesitant to use
models like DNNs that can be difﬁcult to interpret. For example  in clinical research  one might like
to ask  "Why did you predict this person as more likely to develop Alzheimer’s disease?" Making
DNNs amenable to queries like this remains an open area of research.
The problem of interpretability for deep networks has been tackled in a variety of ways [20  30  22 
3  19  13  14  16  18  21  31]. The majority of this work focuses on vision and language tasks and
their application to time series data  speciﬁcally when using recurrent neural nets (RNNs)  is poorly

∗Authors contributed equally
†Code available at https://github.com/ayaabdelsalam91/Input-Cell-Attention

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

understood. Interpretability in time series data requires methods that are able to capture changes
in the importance of features over time. The goal of our paper is to understand the applicability of
feature importance methods to time series data  where detecting importance in speciﬁc time intervals
is necessary. We will concentrate on the use of saliency as a measure of feature importance.
As an illustration of the type of problem we seek to solve  consider the following task classiﬁcation
problem from neuroimaging [25]: a subject is performing a certain task (e.g.  a memory or other
cognitive task) while being scanned in an fMRI machine. After preprocessing of the raw image signal 
the data will consist of a multivariate time series  with each feature measuring activity in a speciﬁc
brain region. To characterize brain region activity pertinent to the task performed by the subject  a
saliency method should be able to capture changes in feature importance (corresponding to brain
regions) over time. This is in contrast to similar text classiﬁcation problems [2]  where the goal of
saliency methods is to give a relevance score to each word in the sequence  whereas the saliency of
individual features in each word embedding is not important.

Time step = 0

Time step = 20

Time step = 40

Time step = 0

(a) LSTM

Time step = 20

Time step = 40

(b) LSTM with input-cell attention

Figure 1: A subject performs a task while scanned by an fMRI machine. Images are processed and
represented as a multivariate time series  with each feature corresponding to a brain region. RNNs are
used to classify time series based on the task performed by the subject. Figure (a) shows the saliency
map produced by LSTM. Importance detected at later time steps (40) is signiﬁcantly higher then that
detected in earlier time steps. Figure (b) shows the saliency map produced by LSTM with input-cell
attention. We observe no time interval bias in the detected importance.

Motivated by problems of this type  our paper presents three main contributions:

1. We study the effect of RNNs  speciﬁcally LSTMs  on saliency for time series data and show
theoretically and empirically that saliency vanishes over time and is therefore incapable of
reliably detecting important features at arbitrary time intervals on RNNs.

2. We propose and evaluate a modiﬁcation for LSTMs ("input-cell attention") that applies an
attention mechanism to the input of an LSTM cell (Figure 2) allowing the LSTM to "attend"
to time steps that it ﬁnds important.

3. We apply input-cell attention to an openly available fMRI dataset from the Human Connec-
tome Project (HCP) [26]  in a task classiﬁcation setting and show that by using input-cell
attention we are able to capture changes in the importance of brain activity across time in
different brain regions as subjects perform a variety of tasks (Figure 1).

"Gating mechanisms"  introduced in LSTMs [9]  help RNN models carry information from previous
time steps  thus diminishing the vanishing gradient problem to improve prediction accuracy. We show 
however  that these mechanisms do not diminish the vanishing gradient problem enough to allow
saliency to capture feature importance at arbitrary time intervals. For example  Figure 1a shows the
saliency map produced by an LSTM applied to the task classiﬁcation problem outlined above. In this
case  the LSTM reports feature importance only in the last few time steps  ignoring the earlier ones.
The input-cell attention mechanism uses a ﬁxed-size matrix embedding at each time step t  to
represent the input sequence up to time t. Each row of the embedding matrix is designed to attend

2

Figure 2: LSTM with input-cell attention  at time t matrix Xt = [x0  x1  . . .   xt] is passed to an
attention mechanism; the output At is multiplied with Xt to produce Mt (i.e Mt = AtXt). Matrix
Mt is now the input to LSTM cell (Mt has dimension r × N  where r is the attention parameter and
N is the number of inputs).

to different inputs including time t or previous time steps. This provides a direct gradient path
from the output at the ﬁnal time step  through the embedding matrix  to all input time steps thereby
circumventing the vanishing saliency problem (Figure 1b). We show via simulation and application
to our illustrative neuroimaging task classiﬁcation problem  that saliency maps produced by input-cell
attention are able to faithfully detect important features regardless of their occurrence in time.

2 Related Work

Attribution methods include perturbation-based methods [30] that compute the attribution of an
input feature by measuring the difference in network’s output with and without that feature. Other
methods compute the attributions for all input features by backpropagating through the network
[19  24  3  20] this is known as gradient-based or backpropagation attribution methods. It has been
proven by Ancona et al. [1] that complex gradient-based attribution methods including -LRP [3] and
DeepLift [19] can be reformulated as computing backpropagation for a modiﬁed gradient function.
Since the goal of our paper is to study the behavior of feature importance detection in RNNs  we have
chosen saliency  perhaps the simplest gradient-based attribution method  to represent the other  more
complex  gradient-based attribution methods.
Neural attention mechanisms are popular techniques that allow models to attend to different input
features of interest. Bahdanau et al. [4] used attention for alignment in machine translation. Xu et al.
[28] implemented attention for computer vision to identify important regions of an image. In addition 
attention was also used to extract important portions of text in a document [29  7]. Lin et al. [12]
deployed self-attention to create a sentence embedding by attending to the hidden state of each word
in the sentence. Vaswani et al. [27] introduced the Transformer  a neural architecture based solely on
attention mechanisms. Current attention mechanisms are mainly applied to hidden states across time
steps. In contrast  we utilize attention in this work to detect salient features over time without bias
towards the last time steps  by attending on different time steps of an input at the cell level of RNNs.
Feature visualization is an attempt to better understand LSTMs by visualizaiton of hidden state
dynamics. Hasani et al. [6] ranks the contribution of individual cells to the ﬁnal output to help
understand LSTM hidden state dynamics. LSTMVis [23] explains individual cell’s functionality
by matching local hidden-state patterns to similar ones in larger networks. IMV-LSTM [5] uses a
mixture attention mechanism to summarize contribution of speciﬁc features on hidden state. Karpathy
et al. [11] uses character-level language models as an interpretable testbed. Olah et al. [15] presents
general user visual interfaces to explore model interpretation measures from DNNs.

3

3 Problem Deﬁnition

We study the problem of assigning feature importance  or "relevance"  at a given time step to each
input feature in a network. We denote input as X = (x1  x2  . . .   xt  . . .   xT )  where T is the
last time step and vector xt = [xt1  . . .   xtN ] ∈ RN is the feature vector at time step t. N is the
number of features  xti is input feature i at time t. An RNN takes input X and produces output
S(X) = [S1(X)  ...  SC(X)]  where C is the total number of output neurons. Given a speciﬁc target
] ∈ RN of each
neuron c  the goal is to ﬁnd the contribution Rc = [Rc
11
input feature xti to the output Sc.

  . . .   Rc
tN

  . . .   Rc
t1

  . . .   Rc

TN

4 Vanishing Saliency: a Recurrent Neural Network Limitation

Consider the example shown in ﬁgure (3a)  where all important features are located in the ﬁrst few
time steps (red box) and the rest is Gaussian noise. One would expect the saliency map to highlight
the important features at the beginning of the time series. However  the saliency produced by LSTM
(Figure 3b) shows some feature importance at the last few time steps with no evidence of importance
at the earlier ones. Methods such as hidden layer pooling and self-attention [12] are used to consider
outputs from different time steps but they fail in producing a reasonable saliency map (refer to section
6.1 and supplementary material for more details). In this section we investigate the reasons behind
LSTM’s bias towards last few time steps in saliency maps.

 

(a) Example

(b) LSTM

(c) LSTM + input-cell At.

Figure 3: (a) A sample from a simulated dataset where the horizontal axis represents time and vertical
axis represents feature values. (b) Saliency map produced by LSTM; importance is only captured in
the last few time steps. (c) Saliency map produced by LSTM with input-cell attention; our model is
able to differentiate between important and non-important feature regardless of there location in time.

The gating mechanisms of LSTM [9] are shown in equation (1)  where σ(·) denotes the sigmoid (i.e.
logistic) function and (cid:12) denotes element-wise vector product. LSTM has three gates: input   forget
and output gates  given as it  ft and ot respectively. These gates determine whether or not to let new
input in (it)  delete information from all previous time steps (ft) or to let it impact the output at the
current time step (ot).

it = σ (Wxixt + Whiht−1 + bi)
ft = σ (Wxf xt + Whf ht−1 + bf )
ot = σ (Wxoxt + Whoht−1 + bo)
˜ct = tanh (Wx˜cxt + Wh˜cht−1 + b˜c)
ct = ft (cid:12) ct−1 + it (cid:12) ˜ct
ht = ot (cid:12) tanh (ct)

(1)

The amount of saliency preserved is controlled by ft; this can be demonstrated by calculating the
saliency Rc

T (xt) where t < T (further details in supplementary material)

(cid:33)

∂ht
∂xt

∂hi
∂hi−1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Rc

T (xt) =

=

(cid:12)(cid:12)(cid:12)(cid:12) ∂Sc(xT )
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∂Sc
(cid:32) t+1(cid:89)

∂hT

∂xt

i=T

4

∂ht

∂ht−1

∂ht
∂ht−1

(cid:16)

is the only term affected by the number of time steps. Solving its partial derivative we get 

= tanh (ct)

Who (ot (cid:12) (1 − ot))

ct−1

Whf

(ft (cid:12) (1 − ft))

+

(cid:17)

+ ot (cid:12)(cid:0)1 − tanh2 (ct)(cid:1)(cid:20)

(cid:16)

(cid:16)
(cid:16)

˜ct

it

Whi (it (cid:12) (1 − it))
Wh˜c (1 − ˜ct (cid:12) ˜ct)

(cid:17)

(cid:17)
(cid:21)

(cid:17)

+

+ ft

As t decreases (i.e earlier time steps)  those terms multiplied by the weight matrix (black box in
above equation) will eventually vanish if the largest eigenvalue of the weight matrix is less then 1 
this is known as the "vanishing gradient problem" [8].

will be reduced to :

∂ht

≈ ot (cid:12)(cid:0)1 − tanh2 (ct)(cid:1)(cid:20)

∂ht−1

ft

(cid:21)

∂ht
∂ht−1

From the equation above  one can see that the amount of information preserved depends on the
LSTM’s "forget gate" (ft); hence  as t decreases (i.e earlier time steps) its contribution to the relevance
decreases and eventually disappears  as we empirically observe in ﬁgure (3b).

5

Input-Cell Attention For Recurrent Neural Networks

To address the vanishing saliency problem described in Section 4  we propose a novel RNN cell
structure  called "input-cell attention." The proposed cell structure is shown in ﬁgure (2); at each time
step t  instead of looking only at the current input vector xt  all inputs accumulated and available to
current time steps are considered by passing them through an attention mechanism. The attention
module provides a set of summation weight matrices for the inputs. The set of summation weight
vectors is multiplied with the inputs  producing a ﬁxed size matrix of weighted inputs Mt attending
to different time steps. Mt is then passed to the LSTM cell. To accommodate the changes in gates
inputs  the classical LSTM gating equations are changed from those shown in (1) to the new ones
shown (2). Note that input-cell attention can be added to any RNN cell architecture; however  the
LSTM architecture is the focus of this paper.

(cid:16)
(cid:18)
(cid:16)

(cid:16)

it = σ

ft = σ

WM iMt + Whiht−1 + bi

WM f Mt + Whf ht−1 + bf

WM ˜cMt + Wh˜cht−1 + b˜c

˜ct = tanh
ct = ft (cid:12) ct−1 + it (cid:12) ˜ct
ht = ot (cid:12) tanh (ct)

(cid:17)
(cid:19)
(cid:17)

(cid:17)

ot = σ

WM oMt + Whoht−1 + bo

(2)

At = softmax(cid:0)W2 tanh(cid:0)W1X T

We use the same attention mechanism that was introduced for self-attention [12]. However  in our
architecture attention is performed at the cell level rather than the hidden layer level. Using the same
notation as in section 3  matrix Xt = [x1  . . .   xt] with dimensions t × N where N is the size of the
feature embedding. The attention mechanism takes Xt as input  and outputs a matrix of weights At:
(3)
W1 is a weight matrix with dimensions da × N where da is a hyper-parameter. The number of time
steps the attention mechanism will attend to is r  known as "attention hops". W2 is also a weight
matrix that has dimension r × da. Finally  the output weight matrix At has dimension r × t; At
has a weight for each time step and the softmax() ensures that all the computed weights sum to
1. The inputs Xt are projected linearly according to the weights provided by At to get a matrix
Mt = AtXt with ﬁxed dimension r × N. One can the view attention mechanism as a two-layer

(cid:1)(cid:1)

t

5

unbiased feed-forward network  with parameters {W1  W2} and da hidden units. Mt is ﬂattened to a
vector of length r ∗ N and passed as the input to the LSTM cell as shown in Figure 2. The dimensions
of each learned weight matrix Wx in the standard LSTM equation (1) is N × h  where h is size of
hidden layer. The input-cell attention weight matrix WM the learned parameters in equation (2) have
dimensions h × (r ∗ N ).
Approximation: To reduce the dimensionality of the LSTM input at each time step  matrix Mt can
be modiﬁed to be the average of features across attention hops. By doing so  the value of a feature in
the embedding will equal its average value across all time steps the model attends to. As mentioned
previously  Mt has dimensions r × N let mij be value of feature j at attention hop i

This reduces matrix Mt to vector (cid:102)mt with dimension N. The dimensions of weight matrix WM

equations (2) return to N × h as in original LSTM equations (1). Self-attention [12] used this
approximation in the github code they provided. We used this version of input-cell attention for the
experiments in Section (6).

(4)

(cid:80)r

(cid:102)mj =

i=1 mij

r

6 Experimental Results

6.1 Synthetic Data for Evaluation

To capture the behavior of saliency methods applied to RNNs  we create synthetic labeled datasets
with two classes. A single example is Gaussian noise with zero mean and unit variance; a box of 1’s
is added to the important features in the positive class or subtracted in the negative class; the feature
embedding size for each sample N = 100 and the number of time steps T = 100. This conﬁguration
helps us differentiate between feature importance in different time intervals. The speciﬁc features
and the time intervals (boxes) on which they are considered important is varied between datasets to
test each model’s ability to capture importance at different time intervals. Figure 4 shows 3 example
datasets. The same ﬁgure also shows how important time intervals and features are speciﬁed in
various experimental setups.

(a) Middle

(b) Latter

(c) 3 Middle

Figure 4: Synthetic Datasets  where red represents important features and blue is Gaussian noise.

6.1.1 Saliency Performance Measurements
Euclidean distance: Since we know the time interval of important features in each example  we
create a reference sample which has value 1 for important features and 0 for noise. We measure the
normalized Euclidean distance between the saliency map R(X) produced by each model for given
sample X (where X = [x1  . . .   xn]) and its reference sample ref  the distance is calculated by the

equation below  where n = N × T (cid:80)n

(cid:113)
(cid:80)n

i=1

(ref i − R (xi))2
i=1 ref i

(5)

Weighted Jaccard similarity [10]: The value of saliency represents the importance of the feature at
a speciﬁc time. We measure the concordance between the set of high saliency features to the known

6

set of important features in simulation. Jaccard measures similarity as the size of the intersection
divided by the size of the union of the sample sets  meaning that high values and low ones have
equal weight. Weighted Jaccard addresses this by considering values  since the higher saliency value
represents higher importance  it is a better measure of similarity for this problem. Weighted Jaccard
similarity J between absolute value of sample |X| and its saliency R(X) is deﬁned as

(cid:80)n
(cid:80)n
i=1 min (|xi|   R(xi))
i=1 max (|xi|   R(xi))

(6)

J (|X|   R(X)) =

6.1.2 Performance on Synthetic Datasets

We compared LSTMs with input-cell attention with standard LSTMs [9]  bidrectional LSTMs [17]
and LSTMs with self-attention [12] (other LSTMs with various pooling architectures were also
compared; performance is reported in the supplementary material).
Static Box Experiments: To test how the methods perform when important features are located at
different time steps we create: "Earlier Box" dataset ﬁgure (3a)  "Middle Box" dataset ﬁgure (4a) and
"Latter Box" datasets ﬁgure (4b); important features are located from t0 to t30  from t30 to t70 and
from t70 to t100 respectively; the results are shown in the table (1a).
To avoid bias we also tested on 1. "Mixed Boxes" dataset in which the location of the importance box
differs in each sample. 2. "3 Earlier Boxes"  "3 Latter Boxes" and "3 Middle Boxes"(similar to ﬁgure
4c ) where not all features are important at one speciﬁc time.; the results are shown in the table (1b).
LSTM with input-cell attention outperforms other methods in both metrics for all datasets. One
important observation is that LSTM performance is higher in the latter box problem  which aligns
with our observed bias towards reporting importance in later time steps.

Model
LSTM
Bi-LSTM
LSTM+self At.
LSTM+in.cell At.

WJac
0.000
0.000
0.048
0.103

Ealier Box

Euc
1.006
1.004
0.973
0.914

Middle Box

Acc WJac
0.019
98.6
0.013
53.2
0.045
100.0
0.110
100.0

Latter Box

Euc
0.985
0.990
0.973
0.903

Acc
100.0
100.0
100.0
100.0

Mixed Boxes

3 Ealier Boxes

3 Middle Boxes

Model
LSTM
Bi-LSTM
LSTM+self At.
LSTM+in.cell At.

WJac
0.000
0.000
0.060
0.104

Euc
1.003
1.002
0.953
0.912

Acc WJac
0.000
52.2
0.000
51.3
0.075
100.0
0.106
100.0

Euc
1.004
1.003
0.939
0.905

Acc
51.8
51.3
99.9
100.0

Acc WJac
0.000
53.4
0.000
50.7
0.048
100.0
0.124
100.0
(a)

Acc WJac
0.000
49.1
0.000
51.5
0.025
100.0
0.108
77.6
(b)

Euc
1.003
1.003
0.963
0.891

Euc
1.003
1.003
0.985
0.903

Table 1: Saliency performance: weighted Jaccard (WJac) and Euclidean distance (Euc). For LSTM 
bidirectional LSTM  LSTM with self-attention and LSTM with input-cell attention on different
datasets where important features are located at different time steps (ACC is the model accuracy).

Moving Box Experiments: To identify the time step effect on the presence of a feature in a saliency
map  we created ﬁve datasets that differ in the start and end time of importance box; images from the
datasets are available in the supplementary material. This experiment reports the effect of changing
the feature importance time interval on the ability of each method to detect saliency.
We plot the change of weighted Jaccard and Euclidean distance against the change in starting time
step of important features in Figure 5. LSTM with input-cell attention and LSTM with self-attention
are unbiased towards time. However  LSTM with input-cell attention outperforms LSTM with
self-attention in both metrics. Classical LSTM and bidirectional LSTM are able to detect salient
features at later time steps only (comparison with other architectures is in the supplementary material).

7

Figure 5: The effect of changing the location of importance features in time on weighted Jaccard
(WJac) and Euclidean distance (Euc). For LSTM  bidirectional LSTM  LSTM with self-attention and
LSTM with input-cell attention.

6.2 MNIST as a Time Series

Figure 6: Saliency maps of samples from MNIST with time as y-axis. Saliency maps are shown for
both vanilla LSTM and LSTM with input-cell attention. The vanishing gradients in the saliency is
clear in LSTM which fails to provide informative maps  whereas adding input-cell attention recovers
gradient values for features at different time steps.

In the previous synthetic datasets  we evaluated saliency maps obtained by different approaches
on a simple setting were continuous blocks of important features are distributed over time. In
order to validate the resulting saliency maps in cases where important features have more structured
distributions of different shapes  we treat the MNIST image dataset as a time series. In other words  a
28 × 28 image is turned into a sequence of 28 time steps  each of which is a vector of 28 features.
Time is represented in the y-axis. For more interpretable visualization of saliency maps  we trained
the models to perform a three-class classiﬁcation task by subsetting the dataset to learn only the digits
"1"  "6"  and "7". These digits were selected since the three share some common features  while
having distinctive features at different time steps.
Both standard LSTMs and LSTMs with input-cell attention were trained to convergence. Figure 6
shows the saliency maps for three samples; saliency maps obtained from LSTMs exhibit consistent
decay over time. When assisted with input-cell attention mechanism  our architecture overcomes
that decay and can successfully highlight important features at different time steps. Supplementary
material shows more samples exhibiting similar behavior.

6.3 Human Connectome Project fMRI Data

To evaluate our method in a more realistic setting  we apply input-cell attention to an openly available
fMRI dataset of the Human Connectome Project (HCP) [26]. In this dataset  subjects are performing
certain tasks while scanned by an fMRI machine. Our classiﬁcation problem is to identify the task
performed given the fMRI scans (more details about tasks and preprocessing fMRI data is available
in supplementary material). Recurrent networks have been used for this task before  e.g. in the
DLight framework [25]. DLight uses LSTM to analyze whole-brain neuro-imaging data then applies
layer-wise relevance propagation (LRP) [3  2] to the trained model identifying those regions of
interest in the brain (ROIs) whose activity the model used to make a prediction. However  this
framework gives a single interpretation for the entire time series; applying input-cell attention enables
us to see changes in the importance of brain region activity across time.

8

(a)

(b)

Figure 7: (a) The effect of dropping salient features identiﬁed by each model. Dropping LSTM’s
top 10% salient features reduces accuracy by 2%  while for LSTM with input-cell attention accuracy
dropped by 9.5%. If features identiﬁed as salient by LSTM with input-cell attention are removed
from standard LSTM its accuracy drops 6%. (b) Percentage of on-task off-task features identiﬁed as
salient  more than 70% of top 10% salient features identiﬁed by LSTM are from off-task period.

6.3.1 Experiments and Results
We performed two types of experiments: (1) On-Task: data was taken while the subject was
actively performing task. (2) On-Task off-Task: data was taken while the subject was both actively
performing task (on-task) and during rest period between tasks (off-task). The off-task time is used
as a negative control for importance at the end of the time series since models should not be able to
differentiate between tasks based on data obtained during off-task periods. More details about the
experimental setup can be found in the supplementary material.

On-Task Experiment: First we trained an LSTM on a binary classiﬁcation task until convergence.
On each correctly classiﬁed task we produced a saliency map. We plotted the saliency map to see
which ROIs are important while the subject is performing the task. Figure (1a) shows that the LSTM
was only able to capture changes in ROI importance at the last few time steps. We repeated the
same experiment using LSTM with input-cell attention with results shown in ﬁgure (1b). Input-cell
attention was able to capture changes in importance for different brain regions across time that were
not recovered by LSTM.

On-Task Off-Task Experiment: Models were ﬁrst trained on the off-task period only  accuracy
produced by the models was random conﬁrming our assumption that off-task period data does not
contain any useful information for task classiﬁcation. Models were then trained on the on-task period
followed by off-task period  saliency maps were used to identify important features. Figure (7 a)
shows the effect of removing features identiﬁed as salient on model accuracy (note that the model
with the ability of correctly detect salient features will result in a larger drop in accuracy on feature
removal). Figure (7 b) shows percentage of on-task off-task features identiﬁed as salient by each
model. Our architecture is faithfully able to detect salient features during on-task portions of time.

7 Summary and Conclusion

We have shown empirically and theoretically that saliency maps produced by LSTMs vanish over
time. Importance is only ascribed to later time steps in a time series and earlier time steps are not
considered. We reduced this vanishing saliency problem by applying an attention mechanism at
the cell level. By attending to inputs across different time steps  the LSTM was able to consider
important features from previous time steps. We applied our methods to fMRI data from the Human
Connectome Project and observed the same phenomenon of LSTM vanishing saliency in a task
detection problem. This last result  taken together with a belief that assigning importance only to
the last few time steps in this neuro-imaging application severely limits the interpretability of LSTM
models  and considering our results on synthetic data  indicates that our work opens a path towards
solving a critical shortcoming in the application of modern recurrent DNNs to problems where
interpretability of time series models is important.

9

Acknowledgments

This work was supported by the U.S. National Institutes of Health grant [R01GM 114267] and NSF
award [CDS&E:1854532]. The funders had no role in study design  data collection and analysis 
decision to publish  or preparation of the manuscript.

References
[1] Marco Ancona  Enea Ceolini  Cengiz Öztireli  and Markus Gross. Towards better understanding
of gradient-based attribution methods for deep neural networks. International Conference on
Learning Representations  2018.

[2] Leila Arras  Grégoire Montavon  Klaus-Robert Müller  and Wojciech Samek. Explaining
recurrent neural network predictions in sentiment analysis. Workshop on Computational
Approaches to Subjectivity  Sentiment and Social Media Analysis  2017.

[3] Montavon G Klauschen F Müller K-R Samek W Bach S  Binder A. On pixel-wise explanations

for non-linear classiﬁer decisions by layer-wise relevance propagation. PLoS ONE  2015.

[4] Dzmitry Bahdanau  Kyunghyun Cho  and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. International Conference on Learning Representations  2015.

[5] Tian Guo  Tao Lin  and Nino Antulov-Fantulin. Exploring interpretable lstm neural networks

over multi-variable data. International Conference on Machine Learning  2019.

[6] Ramin M Hasani  Alexander Amini  Mathias Lechner  Felix Naser  Radu Grosu  and Daniela
Rus. Response characterization for auditing cell dynamics in long short-term memory networks.
arXiv preprint arXiv:1809.03864  2018.

[7] Karl Moritz Hermann  Tomas Kocisky  Edward Grefenstette  Lasse Espeholt  Will Kay  Mustafa
Suleyman  and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural
information processing systems  2015.

[8] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and
problem solutions. International Journal of Uncertainty  Fuzziness and Knowledge-Based
Systems  1998.

[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation 

1997.

[10] Sergey Ioffe.

Improved consistent sampling  weighted minhash and l1 sketching.

International Conference on Data Mining  2010.

IEEE

[11] Andrej Karpathy  Justin Johnson  and Li Fei-Fei. Visualizing and understanding recurrent

networks. arXiv preprint arXiv:1506.02078  2015.

[12] Zhouhan Lin  Minwei Feng  Cicero Nogueira dos Santos  Mo Yu  Bing Xiang  Bowen Zhou 
and Yoshua Bengio. A structured self-attentive sentence embedding. International Conference
on Learning Representation  2017.

[13] Grégoire Montavon  Sebastian Lapuschkin  Alexander Binder  Wojciech Samek  and Klaus-
Robert Müller. Explaining nonlinear classiﬁcation decisions with deep taylor decomposition.
Pattern Recognition  2017.

[14] Grégoire Montavon  Wojciech Samek  and Klaus-Robert Müller. Methods for interpreting and

understanding deep neural networks. Digital Signal Processing  2018.

[15] Chris Olah  Arvind Satyanarayan  Ian Johnson  Shan Carter  Ludwig Schubert  Katherine Ye 

and Alexander Mordvintsev. The building blocks of interpretability. Distill  2018.

[16] Wojciech Samek  Alexander Binder  Grégoire Montavon  Sebastian Lapuschkin  and Klaus-
Robert Müller. Evaluating the visualization of what a deep neural network has learned. IEEE
transactions on neural networks and learning systems  2016.

10

[17] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transac-

tions on Signal Processing  1997.

[18] Ramprasaath R Selvaraju  Michael Cogswell  Abhishek Das  Ramakrishna Vedantam  Devi
Parikh  and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based
localization. IEEE International Conference on Computer Vision  2017.

[19] Avanti Shrikumar  Peyton Greenside  and Anshul Kundaje. Learning important features through

propagating activation differences. International Conference on Machine Learning  2017.

[20] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034 
2013.

[21] Daniel Smilkov  Nikhil Thorat  Been Kim  Fernanda Viégas  and Martin Wattenberg. Smooth-

grad: removing noise by adding noise. arXiv preprint arXiv:1706.03825  2017.

[22] Jost Tobias Springenberg  Alexey Dosovitskiy  Thomas Brox  and Martin Riedmiller. Striving
for simplicity: The all convolutional net. International Conference on Learning Representations 
2015.

[23] Hendrik Strobelt  Sebastian Gehrmann  Hanspeter Pﬁster  and Alexander M Rush. Lstmvis: A
tool for visual analysis of hidden state dynamics in recurrent neural networks. IEEE transactions
on visualization and computer graphics  2017.

[24] Mukund Sundararajan  Ankur Taly  and Qiqi Yan. Axiomatic attribution for deep networks.

International Conference on Machine Learning  2017.

[25] Armin W Thomas  Hauke R Heekeren  Klaus-Robert Müller  and Wojciech Samek. Interpretable

lstms for whole-brain neuroimaging analyses. arXiv preprint arXiv:1810.09945  2018.

[26] David C Van Essen  Stephen M Smith  Deanna M Barch  Timothy EJ Behrens  Essa Yacoub 
Kamil Ugurbil  Wu-Minn HCP Consortium  et al. The wu-minn human connectome project: an
overview. Neuroimage  2013.

[27] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez 
Łukasz Kaiser  and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems  2017.

[28] Kelvin Xu  Jimmy Ba  Ryan Kiros  Kyunghyun Cho  Aaron Courville  Ruslan Salakhudinov 
Rich Zemel  and Yoshua Bengio. Show  attend and tell: Neural image caption generation with
visual attention. International conference on machine learning  2015.

[29] Zichao Yang  Diyi Yang  Chris Dyer  Xiaodong He  Alex Smola  and Eduard Hovy. Hierarchical
attention networks for document classiﬁcation. Association for Computational Linguistics 
2016.

[30] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.

European conference on computer vision  2014.

[31] Luisa M Zintgraf  Taco S Cohen  Tameem Adel  and Max Welling. Visualizing deep neural
International Conference on Learning

network decisions: Prediction difference analysis.
Representations  2017.

11

,Aya Abdelsalam Ismail
Mohamed Gunady
Luiz Pessoa
Hector Corrada Bravo
Soheil Feizi