2014,Sparse Random Feature Algorithm as Coordinate Descent in Hilbert Space,In this paper  we propose a Sparse Random Feature algorithm  which learns a sparse non-linear predictor by minimizing an $\ell_1$-regularized objective function over the Hilbert Space induced from kernel function. By interpreting the algorithm as Randomized Coordinate Descent in the infinite-dimensional space  we show the proposed approach converges to a solution comparable within $\eps$-precision to exact kernel method by drawing $O(1/\eps)$ number of random features  contrasted to the $O(1/\eps^2)$-type convergence achieved by Monte-Carlo analysis in current Random Feature literature. In our experiments  the Sparse Random Feature algorithm obtains sparse solution that requires less memory and prediction time while maintains comparable performance on tasks of regression and classification. In the meantime  as an approximate solver for infinite-dimensional $\ell_1$-regularized problem  the randomized approach converges to better solution than Boosting approach when the greedy step of Boosting cannot be performed exactly.,Sparse Random Features Algorithm as
Coordinate Descent in Hilbert Space

Ian E.H. Yen 1 Ting-Wei Lin 2

Shou-De Lin 2

Pradeep Ravikumar 1

Inderjit S. Dhillon 1

Department of Computer Science

1: University of Texas at Austin 
2: National Taiwan University
1: {ianyen pradeepr inderjit}@cs.utexas.edu 

2: {b97083 sdlin}@csie.ntu.edu.tw

Abstract

In this paper  we propose a Sparse Random Features algorithm  which learns a
sparse non-linear predictor by minimizing an ℓ1-regularized objective function
over the Hilbert Space induced from a kernel function. By interpreting the al-
gorithm as Randomized Coordinate Descent in an inﬁnite-dimensional space  we
show the proposed approach converges to a solution within ϵ-precision of that us-
ing an exact kernel method  by drawing O(1/ϵ) random features  in contrast to the
O(1/ϵ2) convergence achieved by current Monte-Carlo analyses of Random Fea-
tures. In our experiments  the Sparse Random Feature algorithm obtains a sparse
solution that requires less memory and prediction time  while maintaining compa-
rable performance on regression and classiﬁcation tasks. Moreover  as an approx-
imate solver for the inﬁnite-dimensional ℓ1-regularized problem  the randomized
approach also enjoys better convergence guarantees than a Boosting approach in
the setting where the greedy Boosting step cannot be performed exactly.

1 Introduction

Kernel methods have become standard for building non-linear models from simple feature represen-
tations  and have proven successful in problems ranging across classiﬁcation  regression  structured
prediction and feature extraction [16  20]. A caveat however is that they are not scalable as the
number of training samples increases. In particular  the size of the models produced by kernel meth-
ods scale linearly with the number of training samples  even for sparse kernel methods like support
vector machines [17]. This makes the corresponding training and prediction computationally pro-
hibitive for large-scale problems.
A line of research has thus been devoted to kernel approximation methods that aim to preserve pre-
dictive performance  while maintaining computational tractability. Among these  Random Features
has attracted considerable recent interest due to its simplicity and efﬁciency [2  3  4  5  10  6]. Since
ﬁrst proposed in [2]  and extended by several works [3  4  5  10]  the Random Features approach is a
sampling based approximation to the kernel function  where by drawing D features from the distri-
√
bution induced from the kernel function  one can guarantee uniform convergence of approximation
error to the order of O(1/
D). On the ﬂip side  such a rate of convergence suggests that in order
to achieve high precision  one might need a large number of random features  which might lead to
model sizes even larger than that of the vanilla kernel method.
One approach to remedy this problem would be to employ feature selection techniques to prevent
the model size from growing linearly with D. A simple way to do so would be by adding ℓ1-
regularization to the objective function  so that one can simultaneously increase the number of ran-
dom features D  while selecting a compact subset of them with non-zero weight. However  the re-
sulting algorithm cannot be justiﬁed by existing analyses of Random Features  since the Representer
theorem does not hold for the ℓ1-regularized problem [15  16]. In other words  since the prediction

1

cannot be expressed as a linear combination of kernel evaluations  a small error in approximating
the kernel function cannot correspondingly guarantee a small prediction error.
In this paper  we propose a new interpretation of Random Features that justiﬁes its usage with
ℓ1-regularization — yielding the Sparse Random Features algorithm. In particular  we show that
the Sparse Random Feature algorithm can be seen as Randomized Coordinate Descent (RCD) in
the Hilbert Space induced from the kernel  and by taking D steps of coordinate descent  one can
achieve a solution comparable to exact kernel methods within O(1/D) precision in terms of the
objective function. Note that the surprising facet of this analysis is that in the ﬁnite-dimensional
case  the iteration complexity of RCD increases with number of dimensions [18]  which would
trivially yield a bound going to inﬁnity for our inﬁnite-dimensional problem. In our experiments 
the Sparse Random Features algorithm obtains a sparse solution that requires less memory and
prediction time  while maintaining comparable performance on regression and classiﬁcation tasks
with various kernels. Note that our technique is complementary to that proposed in [10]  which aims
to reduce the cost of evaluating and storing basis functions  while our goal is to reduce the number
of basis functions in a model.
Another interesting aspect of our algorithm is that our inﬁnite-dimensional ℓ1-regularized objective
is also considered in the literature of Boosting [7  8]  which can be interpreted as greedy coordinate
descent in the inﬁnite-dimensional space. As an approximate solver for the ℓ1-regularized problem 
we compare our randomized approach to the boosting approach in theory and also in experiments.
As we show  for basis functions that do not allow exact greedy search  a randomized approach enjoys
better guarantees.

2 Problem Setup
We are interested in estimating a prediction function f: X→Y from training data set D =
n=1  (xn  yn) ∈ X × Y by solving an optimization problem over some Reproducing
{(xn  yn)}N
Kernel Hilbert Space (RKHS) H:

∗

f

f∈H

= argmin

(1)
where L(z  y) is a convex loss function with Lipschitz-continuous derivative satisfying |L
(z1  y) −
′
(z2  y)| ≤ β|z1 − z2|  which includes several standard loss functions such as the square-loss
′
L
2 (z − y)2  square-hinge loss L(z  y) = max(1 − zy  0)2 and logistic loss L(z  y) =
L(z  y) = 1
log(1 + exp(−yz)).

L(f (xn)  yn) 

n=1

N∑

1
N

∥f∥2H +

λ
2

2.1 Kernel and Feature Map
There are two ways in practice to specify the space H. One is via specifying a positive-deﬁnite
kernel k(x  y) that encodes similarity between instances  and where H can be expressed as the
completion of the space spanned by {k(x ·)}x∈X   that is 

}

H =

αik(xi ·) | αi ∈ R  xi ∈ X

.

K∑

i=1

{
f (·) =
∫

{

The other way is to ﬁnd an explicit feature map { ¯ϕh(x)}h∈H  where each h ∈ H deﬁnes a basis
function ¯ϕh(x) : X → R. The RKHS H can then be deﬁned as

}
w(h) ¯ϕh(·)dh = ⟨w  ¯ϕ(·)⟩H | ∥f∥2H < ∞

(2)
where w(h) is a weight distribution over the basis {ϕh(x)}h∈H. By Mercer’s theorem [1]  every
positive-deﬁnite kernel k(x  y) has a decomposition s.t.

f (·) =

H =

h∈H

 

k(x  y) =

(3)
√
p ◦ ϕ. However  the decomposition
where p(h) ≥ 0 and ¯ϕh(.) =
is not unique. One can derive multiple decompositions from the same kernel k(x  y) based on

h∈H
p(h)ϕh(.)  denoted as ¯ϕ =

p(h)ϕh(x)ϕh(y)dh = ⟨ ¯ϕ(x)  ¯ϕ(y)⟩H 

∫
√

2

different sets of basis functions {ϕh(x)}h∈H. For example  in [2]  the Laplacian kernel k(x  y) =
exp(−γ∥x − y∥1) can be decomposed through both the Fourier basis and the Random Binning
basis  while in [7]  the Laplacian kernel can be obtained from the integrating of an inﬁnite number
of decision trees.
On the other hand  multiple kernels can be derived from the same set of basis functions via different
distribution p(h). For example  in [2  3]  a general decomposition method using Fourier basis func-
ω∈Rd was proposed to ﬁnd feature map for any shift-invariant kernel of
tions
the form k(x − y)  where the feature maps (3) of different kernels k(∆) differ only in the distri-
bution p(ω) obtained from the Fourier transform of k(∆). Similarly  [5] proposed decomposition
based on polynomial basis for any dot-product kernel of the form k(⟨x  y⟩).

ϕω(x) = cos(ωT x)

}

{

2.2 Random Features as Monte-Carlo Approximation

∑

{
f (·) =

}
The standard kernel method  often referred to as the “kernel trick ” solves problem (1) through
∗ ∈ H lies in
the Representer Theorem [15  16]  which states that the optimal decision function f
the span of training samples HD =
  which
reduces the inﬁnite-dimensional problem (1) to a ﬁnite-dimensional problem with N variables
{αn}N
n=1. However  it is known that even for loss functions with dual-sparsity (e.g. hinge-loss) 
the number of non-zero αn increases linearly with data size [17].
Random Features has been proposed as a kernel approximation method [2  3  10  5]  where a Monte-
Carlo approximation

n=1 αnk(xn ·) | αn ∈ R  (xn  yn) ∈ D

N

k(xi  xj) = Ep(h)[ϕh(xi)ϕh(xj)] ≈ 1

D

ϕhk (xi)ϕhk (xj) = z(xi)T z(xj)

(4)

is used to approximate (3)  so that the solution to (1) can be obtained by

The corresponding approximation error

(cid:12)(cid:12)wT

RF z(x) − f

∗

(x)

wRF = argmin
w∈RD

∥w∥2 +

1
N

L(wT z(xn)  yn).

λ
2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N∑

n=1

(cid:12)(cid:12) =

n z(xn)T z(x) − N∑

αRF

n=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  

∗
nk(xn  x)

α

D∑

k=1

N∑

n=1

as proved in [2 Appendix B]  can be bounded by ϵ given D = Ω(1/ϵ2) number of random fea-
tures  which is a direct consequence of the uniform convergence of the sampling approximation (4).
Unfortunately  the rate of convergence suggests that to achieve small approximation error ϵ  one
needs signiﬁcant amount of random features  and since the model size of (5) grows linearly with
D  such an algorithm might not obtain a sparser model than kernel method. On the other hand  the
ℓ1-regularized Random-Feature algorithm we are proposing aims to minimize loss with a selected
subset of random feature that neither grows linearly with D nor with N. However  (6) does not hold
for ℓ1-regularization  and thus one cannot transfer guarantee from kernel approximation (4) to the
learned decision function.

3 Sparse Random Feature as Coordinate Descent

√
In this section  we present the Sparse Random Features algorithm and analyze its convergence by
interpreting it as a fully-corrective randomized coordinate descent in a Hilbert space. Given a feature
p(h)ϕh(x)}h∈H  the optimization program (1) can
map of orthogonal basic functions { ¯ϕh(x) =
N∑
be written as the inﬁnite-dimensional optimization problem

min
w∈H

∥w∥2

2 +

λ
2

1
N

L(⟨w  ¯ϕ(xn)⟩H  yn).

n=1

3

(5)

(6)

(7)

Instead of directly minimizing (7)  the Sparse Random Features algorithm optimizes the related
ℓ1-regularized problem deﬁned as

N∑

min

∫

(cid:22)w∈H F ( ¯w) = λ∥ ¯w∥1 +
(8)
√
p ◦ ϕ(x) is replaced by ϕ(x) and ∥ ¯w∥1 is deﬁned as the ℓ1-norm in function
where ¯ϕ(x) =
| ¯w(h)|dh. The whole procedure is depicted in Algorithm 1. At each iteration 
space ∥ ¯w∥1 =
h∈H
we draw R coordinates h1  h2  ...  hR from distribution p(h)  add them into a working set At  and
minimize (8) w.r.t. the working set At as

L(⟨ ¯w  ϕ(xn)⟩H  yn) 

1
N

n=1

∑

N∑

∑

min

(cid:22)w(h) h∈At

λ

h∈At

| ¯w(h)| +

1
N

L(

h∈At

n=1

¯w(h)ϕh(xn)  yn).

(9)

At the end of each iteration  the algorithm removes features with zero weight to maintain a compact
working set.

Algorithm 1 Sparse Random-Feature Algorithm

Initialize ¯w0 = 0  working set A(0) = {}  and t = 0.
repeat

4. A(t+1) = A(t) \{

1. Sample h1  h2  ...  hR i.i.d. from distribution p(h).
2. Add h1  h2  ...  hR to the set A(t).
3. Obtain ¯wt+1 by solving (9).
h| ¯wt+1(h) = 0
5. t ← t + 1.

}

.

until t = T

3.1 Convergence Analysis

In this section  we analyze the convergence behavior of Algorithm 1. The analysis comprises of two
parts. First  we estimate the number of iterations Algorithm 1 takes to produce a solution wt that
is at most ϵ away from some arbitrary reference solution wref on the ℓ1-regularized program (8).
∗ of (7)  we obtain an approximation guarantee for
Then  by taking wref as the optimal solution w
wt with respect to w
Lemma 1. Suppose loss function L(z  y) has β-Lipschitz-continuous derivative and |ϕh(x)| ≤
B ∀h ∈ H ∀x ∈ X . The loss term Loss( ¯w; ϕ) = 1

∗. The proofs for most lemmas and corollaries will be in the appendix.
n=1 L(⟨ ¯w  ϕ(xn)⟩  yn) in (8) has

∑

N

N

Loss( ¯w + ηδh; ϕ) − Loss( ¯w; ϕ) ≤ ghη +

γ
2

η2 

where δh = δ(∥x − h∥) is a Dirac function centered at h  and gh = ∇ (cid:22)wLoss( ¯w; ϕ)(h) is the
Frechet derivative of the loss term evaluated at h  and γ = βB2.

N∑

1
N

The above lemma states smoothness of the loss term  which is essential to guarantee descent amount
√
obtained by taking a coordinate descent step. In particular  we aim to express the expected progress
p ◦ w) deﬁned as
made by Algorithm 1 as the proximal-gradient magnitude of ¯F (w) = F (

¯F (w) = λ∥√

p ◦ w∥1 +

L(⟨w  ¯ϕ(xn)⟩  yn).

(10)
. Let g = ∇ (cid:22)wLoss( ¯w  ϕ)  ¯g = ∇wLoss(w  ¯ϕ) be the gradients of loss terms in (8)  (10) respec-
tively  and let ρ ∈ ∂ (λ∥ ¯w∥1). We have following relations between (8) and (10):

n=1

√

p ◦ ρ ∈ ∂ (λ∥√

p ◦ w∥1) 

√
p ◦ g 

¯ρ :=

(11)
by simple applications of the chain rule. We then analyze the progress made by each iteration of
Algorithm 1. Recalling that we used R to denote the number of samples drawn in step 1 of our
algorithm  we will ﬁrst assume R = 1  and then show that same result holds also for R > 1.

¯g =

4

Theorem 1 (Descent Amount). The expected descent of the iterates of Algorithm 1 satisﬁes

E[F ( ¯wt+1)] − F ( ¯wt) ≤ − γ∥¯ηt∥2

 

2

where ¯η is the proximal gradient of (10)  that is 

λ∥√

p ◦ (wt + η)∥1 − λ∥√

p ◦ wt∥1 + ⟨¯g  η⟩ +

¯η = argmin

(cid:17)

and ¯g = ∇wLoss(wt  ¯ϕ) is the derivative of loss term w.r.t. w.
Proof. Let gh = ∇ (cid:22)wLoss( ¯wt  ϕ)(h). By Corollary 1  we have

F ( ¯wt + ηδh) − F ( ¯wt) ≤ λ| ¯wt(h) + η| − λ| ¯wt(h)| + ghη +

Minimizing RHS w.r.t. η  the minimizer ηh should satisfy

(12)

(13)

(14)

∥η∥2

γ
2

γ
2

η2.

(15)
for some sub-gradient ρh ∈ ∂ (λ| ¯wt(h) + ηh|). Then by deﬁnition of sub-gradient and (15) we have
(16)

λ| ¯wt(h) + η| − λ| ¯wt(h)| + ghη +

gh + ρh + γηh = 0

γ
2

η2 ≤ ρhηh + ghηh +
η2
h
h = − γ
η2

= −γη2

h +

γ
2

γ
2

η2
h.

(17)

2

Note the equality in (16) holds if ¯wt(h) = 0 or the optimal ηh = 0  which is true for Algorithm
1. Since ¯wt+1 minimizes (9) over a block At containing h  we have F ( ¯wt+1) ≤ F ( ¯wt + ηhδh).
Combining (14) and (16)  taking expectation over h on both sides  and then we have

E[F ( ¯wt+1)] − F ( ¯wt) ≤ − γ
2

h] = ∥√

E[η2

p ◦ η∥2 = ∥¯η∥2

√
p ◦ η is the proximal gradient (13) of ¯F (wt)  which is true

Then it remains to verify that ¯η =
since ¯η satisﬁes the optimality condition of (13)
√
p ◦ (g + ρ + γη) = 0 

¯g + ¯ρ + γ ¯η =

where ﬁrst equality is from (11) and the second is from (15).

Theorem 2 (Convergence Rate). Given any reference solution wref   the sequence {wt}∞

t=1 satisﬁes

E[ ¯F (wt)] ≤ ¯F (wref ) +

2γ∥wref∥2

k

 

(18)

∥¯η∥2 = min

− γ
2

where k = max{t − c  0} and c = 2( (cid:22)F (0)− (cid:22)F (wref ))
Proof. First  the equality actually holds in inequality (16)  since for h /∈ A(t−1)  we have wt(h) = 0 
which implies λ|wt(h) + η| − λ|wt(h)| = ρη  ρ ∈ ∂(λ|wt(h) + η|)  and for h ∈ At−1 we have
¯ηh = 0  which gives 0 to both LHS and RHS. Therefore  we have

is a constant.

γ∥wref∥2

λ∥√

p ◦ (wt + η)∥1 − λ∥√

p ◦ wt∥1 + ¯gT η +

(cid:17)

∂(|√

(19)
Note the minimization in (19) is separable for different coordinates. For h ∈ A(t−1)  the weight
wt(h) is already optimal in the beginning of iteration t  so we have ¯ρh + ¯gh = 0 for some ¯ρh ∈
p(h)(w(h) + ηh)| + ¯ghηh)
}

p(h)w(h)|). Therefore  ηh = 0  h ∈ A(t−1) is optimal both to (|√
}

p ◦ (wt + η)∥1 − λ∥√
∫

h. Set ηh = 0 for the latter  we have
2 η2
∥¯η∥2 = min
≤ min

{
λ∥√
{
¯F (wt + η) − ¯F (wt) +

p ◦ wt∥1 + ⟨¯g  η⟩ +

and to γ
− γ
2

η2
hdh

h /∈A(t(cid:0)1)

∫

γ
2

(cid:17)

∥η∥2.

γ
2

(cid:17)

η2
hdh

h /∈A(t(cid:0)1)

γ
2

5

from convexity of ¯F (w). Consider solution of the form η = α(wref − wt)  we have
− γ
2

¯F

∥¯η∥2 ≤ min
α∈[0 1]
≤ min
α∈[0 1]
≤ min
α∈[0 1]
{

∗

∫
) − ¯F (wt) +
}

γα2
2

∥wref∥2

(

(

¯F (wref ) − ¯F (wt)

{
{
{
¯F (wt) + α
−α
(
(cid:22)F (wt)− (cid:22)F (wref )
  1
¯F (wt) − ¯F (wref )
∥wref∥2

) − ¯F (wt) +
wt + α(wref − wt)
)
(
¯F (wt) − ¯F (wref )
)
and
/(2γ∥wref∥2)

γ∥wref∥2

γα2
2

+

 

}
}
(wref (h) − wt(h))2dh

∫

wref (h)2dh

h /∈A(t(cid:0)1)

h /∈A(t(cid:0)1)
γα2
2

where the second inequality results from wt(h) = 0  h /∈ A(t−1). Minimizing last expression w.r.t.
α  we have α
− γ
∥¯η∥2 ≤
2

  if ¯F (wt) − ¯F (wref ) < γ∥wref∥2
  o.w.

−(

)2

= min

Note  since the function value { ¯F (wt)}∞
second case of (20)  and the number of such iterations is at most c = ⌈ 2( (cid:22)F (0)− (cid:22)F (wref ))
we have

(20)
t=1 is non-increasing  only iterations in the beginning fall in
⌉. For t > c 

− γ

2

.

E[ ¯F (wt+1)] − ¯F (wt) ≤ − γ∥¯ηt∥2

2

2

γ∥wref∥2
≤ − ( ¯F (wt) − ¯F (wref ))2

2γ∥wref∥2

.

(21)

[
E
= max{D − c  0}  where w

λ∥ ¯w(D)∥1 + Loss( ¯w(D); ϕ)

The recursion then leads to the result.
Note the above bound does not yield useful result if ∥wref∥2 → ∞. Fortunately  the optimal
solution of our target problem (7) has ﬁnite ∥w
∗∥2 as long as in (7) λ > 0  so it always give a useful
bound when plugged into (18)  as following corollary shows.
]

Corollary 1 (Approximation Guarantee). The output of Algorithm 1 satisﬁes

; (cid:22)ϕ)

′

(22)
∗ is the optimal solution of problem (7)  c is a constant deﬁned

with D
in Theorem 2.
Then the following two corollaries extend the guarantee (22) to any R ≥ 1  and a bound holds with
high probability. The latter is a direct result of [18 Theorem 1] applied to the recursion (21).
Corollary 2. The bound (22) holds for any R ≥ 1 in Algorithm 1  where if there are T iterations
then D = T R.
Corollary 3. For D ≥ 2γ∥w

(1 + log 1

+

2

λ∥w

∗∥2 + Loss(w

∗

∗∥2

2γ∥w
D′

≤{

}

(cid:3)∥2

λ∥ ¯w(D)∥1 + Loss( ¯w(D); ϕ) ≤{

ρ ) + 2 − 4

ϵ

with probability 1 − ρ  where c is as deﬁned in Theorem 2 and w

∗ is the optimal solution of (7).

c + c   the output of Algorithm 1 has
λ∥w

∗∥2 + Loss(w

; (cid:22)ϕ)

+ ϵ

∗

(23)

}

3.2 Relation to the Kernel Method

Our result (23) states that  for D large enough  the Sparse Random Features algorithm achieves either
a comparable loss to that of the vanilla kernel method  or a model complexity (measured in ℓ1-norm)
∗ is not the optimal
less than that of kernel method (measured in ℓ2-norm). Furthermore  since w
solution of the ℓ1-regularized program (8)  it is possible for the LHS of (23) to be much smaller than
∗ of ﬁnite ℓ2-norm can be the reference solution wref   the λ
the RHS. On the other hand  since any w
used in solving the ℓ1-regularized problem (8) can be different from the λ used in the kernel method.
The tightest bound is achieved by minimizing the RHS of (23)  which is equivalent to minimizing
(7) with some unknown ˜λ(λ) due to the difference of ∥w∥1 and ∥w∥2
2. In practice  we can follow
a regularization path to ﬁnd small enough λ that yields comparable predictive performance while
maintains model as compact as possible. Note  when using different sampling distribution p(h) from
the decomposition (3)  our analysis provides different bounds (23) for the Randomized Coordinate
Descent in Hilbert Space. This is in contrast to the analysis in the ﬁnite-dimensional case  where
RCD with different sampling distribution converges to the same solution [18].

6

3.3 Relation to the Boosting Method

Boosting is a well-known approach to minimize inﬁnite-dimensional problems with ℓ1-
regularization [8  9]  and which in this setting  performs greedy coordinate descent on (8). For
each iteration t  the algorithm ﬁnds the coordinate h(t) yielding steepest descent in the loss term

N∑

n=1

h(t) = argmin

h∈H

1
N

′
L
nϕh(xn)

(24)

to add into a working set At and minimize (8) w.r.t. At. When the greedy step (24) can be solved
exactly  Boosting has fast convergence to the optimal solution of (8) [13  14]. On the contrary 
randomized coordinate descent can only converge to a sub-optimal solution in ﬁnite time when there
are inﬁnite number of dimensions. However  in practice  only a very limited class of basis functions
allow the greedy step in (24) to be performed exactly. For most basis functions (weak learners)
such as perceptrons and decision trees  the greedy step (24) can only be solved approximately. In
such cases  Boosting might have no convergence guarantee  while the randomized approach is still
guaranteed to ﬁnd a comparable solution to that of the kernel method. In our experiments  we found
that the randomized coordinate descent performs considerably better than approximate Boosting
with the perceptron basis functions (weak learners)  where as adopted in the Boosting literature
[19  8]  a convex surrogate loss is used to solve (24) approximately.

4 Experiments

In this section  we compare Sparse Random Features (Sparse-RF) to the existing Random Fea-
tures algorithm (RF) and the kernel method (Kernel) on regression and classiﬁcation problems with
kernels set to Gaussian RBF  Laplacian RBF [2]  and Perceptron kernel [7] 1. For Gaussian and
Laplacian RBF kernel  we use Fourier basis function with corresponding distribution p(h) derived
in [2]; for Perceptron kernel  we use perceptron basis function with distribution p(h) being uniform
over unit-sphere as shown in [7]. For regression  we solve kernel ridge regression (1) and RF regres-
sion (6) in closed-form as in [10] using Eigen  a standard C++ library of numerical linear algebra.
For Sparse-RF  we solve the LASSO sub-problem (9) by standard RCD algorithm. In classiﬁcation 
we use LIBSVM2as solver of kernel method  and use Newton-CG method and Coordinate Descent
method in LIBLINEAR [12] to solve the RF approximation (6) and Sparse-RF sub-problem (9) re-
spectively. We set λN = N λ = 1 for the kernel and RF methods  and for Sparse-RF  we choose
λN ∈ {1  10  100  1000} that gives RMSE (accuracy) closest to the RF method to compare spar-
sity and efﬁciency. The results are in Tables 1 and 2  where the cost of kernel method grows at
least quadratically in the number of training samples. For YearPred  we use D = 5000 to maintain
tractability of the RF method. Note for Covtype dataset  the ℓ2-norm ∥w
∗∥2 from kernel machine is
signiﬁcantly larger than that of others  so according to (22)  a larger number of random features D
are required to obtain similar performance  as shown in Figure 1.
In Figure 1  we compare Sparse-RF (randomized coordinate descent) to Boosting (greedy coordinate
descent) and the bound (23) obtained from SVM with Perceptron kernel and basis function (weak
learner). The ﬁgure shows that Sparse-RF always converges to a solution comparable to that of
the kernel method  while Boosting with approximate greedy steps (using convex surrogate loss)
converges to a higher objective value  due to bias from the approximation.

Acknowledgement

S.-D.Lin acknowledges the support of Telecommunication Lab.  Chunghwa Telecom Co.  Ltd via TL-103-
8201  AOARD via No. FA2386-13-1-4045  Ministry of Science and Technology  National Taiwan University
and Intel Co. via MOST102-2911-I-002-001  NTU103R7501  102-2923-E-002-007-MY2  102-2221-E-002-
170  103-2221-E-002-104-MY2. P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via
IIS-1149803  IIS-1320894  IIS-1447574  and DMS-1264033. This research was also supported by NSF grants
CCF-1320746 and CCF-1117055.

2Data set for classiﬁcation can be downloaded from LIBSVM data set web page  and data set for regression can be found at UCI Machine

Learning Repository and Ali Rahimi’s page for the paper [2].

2We follow the FAQ page of LIBSVM to replace hinge-loss by square-hinge-loss for comparison.

7

Table 1: Results for Kernel Ridge Regression. Fields from top to bottom are model size (# of support
vectors or # of random features or # of non-zero weights respectively)  testing RMSE  training time 
testing prediction time  and memory usage during training.

Data set
CPU
Ntr =6554
Nt =819
d =21

Census
Ntr =18186
Nt =2273
d =119

YearPred
Ntr =463715
Nt =51630
d =90

Kernel
SV=6554
RMSE=0.038
Ttr=154 s
Tt=2.59 s
Mem=1.36 G
SV=18186
RMSE=0.029
Ttr=2719 s
Tt=74 s
Mem=10 G
SV=#
RMSE=#
Ttr=#
Tt=#
Mem=#

Gaussian RBF

RF
D=10000
0.037
875 s
6 s
4.71 G
D=10000
0.032
1615 s
80 s
8.2 G
D=5000
0.103
7697 s
697 s
76.7G

Sparse-RF
NZ=57
0.032
22 s
0.04 s
0.069 G
NZ=1174
0.030
229 s
8.6 s
0.55 G
NZ=1865
0.104
1618 s
97 s
45.6G

Kernel
SV=6554
0.034
157 s
3.13 s
1.35 G
SV=18186
0.146
3268 s
68 s
10 G
SV=#
#
#
#
#

Laplacian RBF

RF
D=10000
. 0.035
803 s
6.99 s
4.71 G
D=10000
0.168
1633 s
88 s
8.2 G
D=5000
0.286
9417 s
715 s
76.6 G

Sparse-RF
NZ=289
0.027
43 s
0.18 s
0.095 G
NZ=5269
0.179
225 s
38s
1.7 G
NZ=3739
0.273
1453 s
209 s
54.3 G

Kernel
SV=6554
0.026
151 s
2.48 s
1.36 G
SV=18186
0.010
2674 s
67.45 s
10 G
SV=#
#
#
#
#

Perceptron Kernel

RF
D=10000
0.038
776 s
6.37 s
4.71 G
D=10000
0.016
1587 s
76 s
8.2 G
D=5000
0.105
8636 s
688 s
76.7 G

Sparse-RF
NZ=251
0.027
27 s
0.13 s
0.090 G
NZ=976
0.016
185 s
6.7 s
0.49 G
NZ=896
0.105
680 s
51 s
38.1 G

Table 2: Results for Kernel Support Vector Machine. Fields from top to bottom are model size (#
of support vectors or # of random features or # of non-zero weights respectively)  testing accuracy 
training time  testing prediction time  and memory usage during training.

Data set
Cod-RNA
Ntr =59535
Nt =10000
d =8

IJCNN
Ntr =127591
Nt =14100
d =22

Covtype
Ntr =464810
Nt =116202
d =54

Kernel
SV=14762
Acc=0.966
Ttr=95 s
Tt=15 s
Mem=3.8 G
SV=16888
Acc=0.991
Ttr=636 s
Tt=34 s
Mem=12 G
SV=335606
Acc=0.849
Ttr=74891 s
Tt=3012 s
Mem=78.5 G

Gaussian RBF

RF
D=10000
0.964
214 s
56 s
9.5 G
D=10000
0.989
601 s
88 s
20 G
D=10000
0.829
9909 s
735 s
74.7 G

Sparse-RF
NZ=180
0.964
180 s
0.61 s
0.66 G
NZ=1392
0.989
292 s
11 s
7.5 G
NZ=3421
0.836
6273 s
132 s
28.1 G

Kernel
SV=13769
0.971
89 s
15 s
3.6 G
SV=16761
0.995
988 s
34 s
12 G
SV=224373
0.954
64172 s
2004 s
80.8 G

Laplacian RBF

RF
D=10000
. 0.969
290 s
46 s
9.6 G
D=10000
0.992
379 s
86 s
20 G
D=10000
0.888
10170 s
635 s
74.6 G

Sparse-RF
NZ=1195
0.970
137 s
6.41 s
1.8 G
NZ=2508
0.992
566 s
25 s
9.9 G
NZ=3141
0.869
2788 s
175 s
56.5 G

Kernel
SV=15201
0.967
57.34 s
7.01 s
3.6 G
SV=26563
0.991
634 s
16 s
11 G
SV=358174
0.905
79010 s
1774 s
80.5 G

Perceptron Kernel

RF
D=10000
0.964
197 s
71.9 s
9.6 G
D=10000
0.987
381 s
77 s
20 G
D=10000
0.835
6969 s
664 s
74.7 G

Sparse-RF
NZ=1148
0.963
131 s
3.81 s
1.4 G
NZ=1530
0.988
490 s
11 s
7.8 G
NZ=1401
0.836
1706 s
70 s
44.4 G

Figure 1: The ℓ1-regularized objective (8) (top) and error rate (bottom) achieved by Sparse Random
Feature (randomized coordinate descent) and Boosting (greedy coordinate descent) using perceptron
basis function (weak learner). The dashed line shows the ℓ2-norm plus loss achieved by kernel
method (RHS of (22)) and the corresponding error rate using perceptron kernel [7].

8

050010001500200025000.10.20.30.40.50.60.70.8timeobjectiveCod−RNA−Objective BoostingSparse−RFKernel00.511.522.5x 10400.050.10.150.20.25timeobjectiveIJCNN−Objective BoostingSparse−RFKernel0500010000150000.350.40.450.50.550.60.65timeobjectiveCovtype−Objective BoostingSparse−RFKernel0500100015002000250000.050.10.150.20.250.30.35timeerrorCod−RNA−Error BoostingSparse−RFKernel00.511.522.5x 10400.010.020.030.040.050.060.070.080.09timeerrorIJCNN−Error BoostingSparse−RFKernel0500010000150000.080.10.120.140.160.180.20.22timeerrorCovtype−Error BoostingSparse−RFKernelReferences
[1] Mercer  J. Functions of positive and negative type and their connection with the theory of inte-

gral equations. Royal Society London  A 209:415 446  1909.

[2] Rahimi  A. and Recht  B. Random features for large-scale kernel machines. NIPS 20  2007.
[3] Rahimi  A. and Recht  B. Weighted sums of random kitchen sinks: Replacing minimization with

randomization in learning. NIPS 21  2008.

[4] Vedaldi  A.  Zisserman  A.: Efﬁcient additive kernels via explicit feature maps. In CVPR. (2010)
[5] P. Kar and H. Karnick. Random feature maps for dot product kernels. In Proceedings of AIS-

TATS’12  pages 583 591  2012.

[6] T. Yang  Y.-F. Li  M. Mahdavi  R. Jin  and Z.-H. Zhou. Nystrom method vs. random Fourier

features: A theoretical and empirical comparison. In Adv. NIPS  2012.

[7] Husan-Tien Lin  Ling Li  Support Vector Machinery for Inﬁnite Ensemble Learnings. JMLR

2008.

[8] Saharon Rosset  Ji Zhu  and Trevor Hastie. Boosting as a Regularized Path to a Maximum

Margin Classiﬁer. JMLR  2004.

[9] Saharon Rosset  Grzegorz Swirszcz  Nathan Srebro  and Ji Zhu. ℓ1-regularization in inﬁnite
dimensional feature spaces. In Learning Theory: 20th Annual Conference on Learning Theory 
2007.

[10] Q. Le  T. Sarlos  and A. J. Smola. Fastfood - approximating kernel expansions in loglinear

time. In The 30th International Conference on Machine Learning  2013.

[11] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transac-

tions on Intelligent Systems and Technology  2011.

[12] R.-E. Fan  K.-W. Chang  C.-J. Hsieh  X.-R. Wang  and C.-J. Lin. LIBLINEAR: A library for

large linear classiﬁcation. Journal of Machine Learning Research  9:1871 1874  2008.

[13] Gunnar Ratsch  Sebastian Mika  and Manfred K. Warmuth. On the convergence of leveraging.

In NIPS  2001.

[14] Matus Telgarsky. The Fast Convergence of Boosting. In NIPS  2011.
[15] Kimeldorf  G. S. and Wahba  G. A correspondence between Bayesian estimation on stochastic

processes and smoothing by splines. Annals of Mathematical Statistics  41:495502  1970.

[16] Scholkopf  Bernhard and Smola  A. J. Learning with Kernels. MIT Press  Cambridge  MA 

2002.

[17] Steinwart  Ingo and Christmann  Andreas. Support Vector Machines. Springer  2008.
[18] P. Ricktarik and M. Takac  Iteration complexity of randomized block-coordinate descent meth-
ods for minimizing a composite function  School of Mathematics  University of Edinburgh 
Tech. Rep.  2011.

[19] Chen  S.-T.  Lin  H.-T. and Lu  C.-J. An online boosting algorithm with theoretical justiﬁca-

tions. ICML 2012.

[20] Taskar  B.  Guestrin  C.  and Koller  D. Max-margin Markov networks. NIPS 16  2004.
[21] G. Song et.al. Reproducing kernel Banach spaces with the l1 norm. Journal of Applied and

Computational Harmonic Analysis  2011.

9

,Ian En-Hsu Yen
Ting-Wei Lin
Shou-De Lin
Pradeep Ravikumar
Inderjit Dhillon
Maria Lomeli
Stefano Favaro
Yee Whye Teh
Weihao Gao
Sewoong Oh
Pramod Viswanath