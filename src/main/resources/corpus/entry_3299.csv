2015,M-Best-Diverse Labelings for Submodular Energies and Beyond,We consider the problem of finding M best diverse solutions of energy minimization problems for graphical models. Contrary to the sequential method of Batra et al.  which greedily finds one solution after another  we infer all $M$ solutions jointly. It was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones. The only obstacle for using this new technique is the complexity of the corresponding inference problem  since it is considerably slower algorithm than the method of Batra et al. In this work we show that the joint inference of $M$ best diverse solutions can be formulated as a submodular energy minimization if the original MAP-inference problem is submodular  hence fast inference techniques can be used. In addition to the theoretical results we provide practical algorithms that outperform the current state-of-the art and can be used in both submodular and non-submodular case.,M-Best-Diverse Labelings

for Submodular Energies and Beyond

Alexander Kirillov1

Dmitrij Schlesinger1

Dmitry Vetrov2

Carsten Rother1

Bogdan Savchynskyy1

1 TU Dresden  Dresden  Germany

2 Skoltech  Moscow  Russia

alexander.kirillov@tu-dresden.de

Abstract

We consider the problem of ﬁnding M best diverse solutions of energy minimiza-
tion problems for graphical models. Contrary to the sequential method of Batra
et al.  which greedily ﬁnds one solution after another  we infer all M solutions
jointly. It was shown recently that such jointly inferred labelings not only have
smaller total energy but also qualitatively outperform the sequentially obtained
ones. The only obstacle for using this new technique is the complexity of the cor-
responding inference problem  since it is considerably slower algorithm than the
method of Batra et al. In this work we show that the joint inference of M best
diverse solutions can be formulated as a submodular energy minimization if the
original MAP-inference problem is submodular  hence fast inference techniques
can be used. In addition to the theoretical results we provide practical algorithms
that outperform the current state-of-the-art and can be used in both submodular
and non-submodular case.

1

Introduction

A variety of tasks in machine learning can be formulated in the form of an energy minimization
problem  known also as maximum a posteriori (MAP) or maximum likelihood estimation (MLE)
inference in an undirected graphical models (related to Markov or conditional random ﬁelds). Its
modeling power and importance are well-recognized  which resulted into specialized benchmark 
i.e. [18] and computational challenges [8] for its solvers. This underlines the importance of ﬁnding
the most probable solution. Following [3] and [25] we argue  however  that ﬁnding M > 1 diverse
conﬁgurations with low energies is also of importance in a number of scenarios  such as: (a) Ex-
pressing uncertainty of the found solution [27]; (b) Faster training of model parameters [14]; (c)
Ranking of inference results [32]; (d) Empirical risk minimization [26].
We build on the new formulation for ﬁnding M-best-diverse-conﬁgurations  which was recently
proposed in [19]. In this formulation all M conﬁgurations are inferred jointly  contrary to the es-
tablished method [3]  where a sequential greedy procedure is used. As shown in [19]  the new
formulation does not only reliably produce conﬁgurations with lower total energy  but also leads
to better results in several application scenarios. In particular  for the image segmentation scenario
the results of [19] signiﬁcantly outperform those of [3]. This is true even when [19] uses a plain
Hamming distance as a diversity measure and [3] uses more powerful diversity measures.
Our contributions.

• We show that ﬁnding M-best-diverse conﬁgurations of a binary submodular energy min-
imization can be formulated as a submodular MAP-inference problem  and hence can be solved
This project has received funding from the European Research Council (ERC) under the European Unions
Horizon 2020 research and innovation programme (grant agreement No 647769). D. Vetrov was supported by
RFBR proj. (No. 15-31-20596) and by Microsoft (RPD 1053945).

1

efﬁciently for any node-wise diversity measure.

• We show that for certain diversity measures  such as e.g. Hamming distance  the M-best-
diverse conﬁgurations of a multilabel submodular energy minimization can be formulated as a
submodular MAP-inference problem  which also implies applicability of efﬁcient graph cut-based
solvers.• We give the insight that if the MAP-inference problem is submodular then the M-best-diverse
conﬁgurations can be always fully ordered with respect to the natural partial order  induced in the
space of all conﬁgurations.
• We show experimentally that if the MAP-inference problem is submodular  we are quantita-
tively at least as good as [19] and considerably better than [3]. The main advantage of our method
is a major speed up over [19]  up to the order of two magnitudes. Our method has the same order of
magnitude run-time as [3]. In the non-submodular case our results are slightly inferior to [19]  but
the advantage with respect to gain in speed up still holds.
Related work. The importance of the considered problem may be justiﬁed by the fact that a proce-
dure of computing M-best solutions to discrete optimization problems was proposed in [23]  which
dates back to 1972. Later  more efﬁcient specialized procedures were introduced for MAP-inference
on a tree [29  Ch. 8]  junction-trees [24] and general graphical models [33  12  2]. Such methods
are however not suited for scenarios where diversity of the solutions is required (like in machine
translation  search engines  producing M-best hypothesis in cascaded algorithms)  since they do not
enforce it explicitly.
Structural Determinant Point Processes [22] is a tool to model probabilistic distributions over struc-
tured models. Unfortunately an efﬁcient sampling procedure is feasible for tree-structured graphical
models only. The recently proposed algorithm [7] to ﬁnd M best modes of a distribution is limited
to the same narrow class of problems.
Training of M independent graphical models to produce diverse solutions was proposed in [13  15].
In contrast  we assume a single ﬁxed model supporting reasonable MAP-solutions.
Along with [3]  the most related to our work is the recent paper [25]  which proposes a subclass of
new diversity penalties  for which the greedy nature of the algorithm [3] can be substantiated due
to submodularity of the used diversity measures. In contrast to [25] we do not limit ourselves to
diversity measures fulﬁlling such properties and moreover  we deﬁne a class of problems  for which
our joint inference approach leads to polynomially and efﬁciently solvable problems in practice.
We build on top of the work [19]  which is explained in detail in Section 2.
Organization of the paper. Section 2 provides background necessary for formulation of our results:
energy minimization for graphical models and existing approaches to obtain diverse solutions. In
Section 3 we introduce submodularity for graphical models and formulate the main results of our
work. Finally  Section 4 and 5 are devoted to the experimental evaluation of our technique and
conclusions. Supplementary material contains proofs of all mathematical claims and the concurrent
submission [19].

Lv. The set LA = (cid:81)

2 Preliminaries
Energy minimization. Let 2A denote the powerset of a set A. The pair G = (V F) is called
a hyper-graph and has V as a ﬁnite set of variable nodes and F ⊆ 2V as a set of factors. Each
variable node v ∈ V is associated with a variable yv taking its values in a ﬁnite set of labels
v∈A Lv denotes a Cartesian product of sets of labels corresponding to
the subset A ⊆ V of variables. Functions θf : Lf → R  associated with factors f ∈ F  are
called potentials and deﬁne local costs on values of variables and their combinations. Potentials
θf with |f| = 1 are called unary  with |f| = 2 pairwise and |f| > 2 higher order. The set
{θf : f ∈ F} of all potentials is referred by θ. For any factor f ∈ F the corresponding set of
variables {yv : v ∈ f} will be denoted by yf . The energy minimization problem consists of ﬁnd-
ing a labeling y∗ = {yv : v ∈ V} ∈ LV  which minimizes the total sum of corresponding potentials:
(1)
Problem (1) is also known as MAP-inference. Labeling y∗ satisfying (1) will be later called a solu-
tion of the energy-minimization or MAP-inference problem  shortly MAP-labeling or MAP-solution.

y∗ = arg min
y∈LV

(cid:88)

f∈F

E(y) = arg min
y∈LV

θf (yf ) .

2

E(y1)

θ1
y1
1

θ1 2

θ2
y1
2

θ2 3

θ3
y1
3

θ3 4

θ4
y1
4

−∆M (y1  y2  y3)

E(y2)

E(y3)

y2
1

θ1

y3
1

θ1

θ1 2

θ1 2

y2
2

θ2

y3
2

θ2

θ2 3

θ2 3

y2
3

θ3

y3
3

θ3

θ3 4

θ3 4

y2
4

θ4

y3
4

θ4

y1
1

θ1

θ1 2

y1
2

θ2

θ2 3

y1
3

θ3

θ3 4

y1
4

θ4

−∆M

1 (y1

1   y2

1   y3
1 )

−∆M

3 (y1

3   y2

3   y3
3 )

y1
1

θ1

θ1 2

y1
2

θ2

θ2 3

y1
3

θ3

θ3 4

y1
4

θ4

−(cid:74)y1
1(cid:75)
1 (cid:54)= y3

−(cid:74)y1
1(cid:75)
1 (cid:54)= y2
−(cid:74)y1
2(cid:75)
2 (cid:54)= y3

−(cid:74)y1
2(cid:75)
2 (cid:54)= y2
−(cid:74)y1
3(cid:75)
3 (cid:54)= y3

−(cid:74)y1
3(cid:75)
3 (cid:54)= y2
−(cid:74)y1
4(cid:75)
4 (cid:54)= y3

−(cid:74)y1

4(cid:75)
4 (cid:54)= y2

−∆M

2 (y1

2   y2

2   y3
2 )

−∆M

4 (y1

4   y2

4   y3
4 )

y2
1

θ1

y2
2

θ2

θ1 2

θ2 3

θ3 4

y2
3

θ3

y2
4

θ4

y2
1

θ1

θ1 2

y2
2

θ2

θ2 3

y2
3

θ3

θ3 4

y2
4

θ4

−(cid:74)y2

1(cid:75)
1 (cid:54)= y3

−(cid:74)y2

2(cid:75)
2 (cid:54)= y3

−(cid:74)y2

3(cid:75)
3 (cid:54)= y3

−(cid:74)y2

4(cid:75)
4 (cid:54)= y3

y3
1

θ1

θ1 2

y3
2

θ2

θ2 3

y3
3

θ3

θ3 4

y3
4

θ4

y3
1

θ1

θ1 2

y3
2

θ2

θ2 3

y3
3

θ3

θ3 4

y3
4

θ4

(a) General diversity measure

(b) Node-wise diversity measure

(c) Hamming distance diversity

Figure 1: Examples of factor graphs for 3 diverse solutions of the original MRF (1) with different
diversity measures. The circles represent nodes of the original model that are copied 3 times. For
clarity the diversity factors of order higher than 2 are shown as squares. Pairwise factors are depicted
by edges connecting the nodes. We omit λ for readability. (a) The most general diversity measure
(4)  (b) the node-wise diversity measure (6)  (c) Hamming distance as a diversity measure (5).
Finally  a model is deﬁned by the triple (G LV   θ)  i.e. the underlying hyper-graph  the sets of labels
and the potentials.
In the following  we use brackets to distinguish between upper index and power  i.e. (A)n means
the n-th power of A  whereas n is an upper index in the expression An. We will keep  however  the
standard notation Rn for the n-dimensional vector space.
Sequential Computation of M Best Diverse Solutions [3]. Instead of looking for a single labeling
with lowest energy  one might ask for a set of labelings with low energies  yet being signiﬁcantly
different from each other.
In order to ﬁnd such M diverse labelings y1  . . .   yM   the method
proposed in [3] solves a sequence of problems of the form

(cid:34)

(cid:35)

m−1(cid:88)

i=1

ym = arg min

y

E(y) − λ

∆(y  yi)

(2)

v∈V

for m = 1  2 . . .   M  where λ > 0 determines a trade-off between diversity and energy  y1 is the
MAP-solution and the function ∆ : LV × LV → R deﬁnes the diversity of two labelings. In other
words  ∆(y  y(cid:48)) takes a large value if y and y(cid:48) are diverse  in a certain sense  and a small value
otherwise. This problem can be seen as an energy minimization problem  where additionally to the
initial potentials θ the potentials −λ∆(·  yi)  associated with an additional factor V  are used. In the
simplest and most commonly used form  ∆(y  y(cid:48)) is represented by a sum of node-wise diversity
measures ∆v : Lv × Lv → R 
(3)

∆(y  y(cid:48)) =

(cid:88)

∆v(yv  y(cid:48)

v)  

and the potentials are split to a sum of unary potentials  i.e. those associated with additional factors
{v}  v ∈ V. This implies that in case efﬁcient graph-cut based inference methods (including α-
expansion [6]  α-β-swap [6] or their generalizations [1  10]) are applicable to the initial problem (1)
then they remain applicable to the augmented problem (2)  which assures efﬁciency of the method.
Joint computation of M-best-diverse labelings. The notation f M ({y}) will be used as a shortcut
for f M (y1  . . .   yM )  for any function f M : (LV )M → R.
Instead of the greedy sequential procedure (2)  in [19] it was suggested to infer all M labelings
jointly  by minimizing

EM ({y}) =

E(yi) − λ∆M ({y})

(4)

M(cid:88)

i=1

(cid:80)M

for y1  . . .   yM and some λ > 0. Function ∆M deﬁnes the total diversity of any M labelings.
It was shown in [19] that the M labelings obtained according to (4) have both lower total energy
i=1 E(yi) and are better from the applied point of view  than those obtained by the sequential

method (2). Hence we will build on the formulation (4) in this work.

3

1 = (cid:83)M

1 = (cid:83)M

i=1 V i. Factors are F M

1 = (V M
i=1 F i ∪ {V M

Though the expression (4) looks complicated  it can be nicely represented in the form (1) and hence
constitutes an energy minimization problem. To achieve this  one creates M copies (Gi LiV   θi) =
(G LV   θ) of the initial model (G LV   θ). The hyper-graph GM
1  F M
1 ) for the new task
is deﬁned as follows. The set of nodes in the new graph is the union of the node sets from the
1 }  i.e. again the union of
considered copies V M
the initial ones extended by a special factor corresponding to the diversity penalty that depends on
all nodes of the new graph. Each node v ∈ V i is associated with the label set Li
v = Lv. The
1 are deﬁned as {−λ∆M   θ1  . . .   θM}  see Fig. 1a for illustration. The
corresponding potentials θM
model (GM
1 ) corresponds to the energy (4). An optimal M-tuple of these labelings 
corresponding to a minimum of (4)  is a trade-off between low energy of individual labelings yi and
their total diversity.
Complexity of the Diversity Problem (4). Though the formulation (4) leads to better results than
those of (2)  minimization of EM is computationally demanding even if the original energy E can
be easily (approximatively) optimized. This is due to the intrinsic repulsive structure of the diversity
potentials −λ∆M : according to the intuitive meaning of the diversity  similar labels are penalized
more than different one. Consider the simplest case with the Hamming distance applied node-wise
as a diversity measure

1  LVM

  θM

1

M−1(cid:88)

M(cid:88)

(cid:88)

∆M ({y}) =

∆v(yi

v  yj

v)  where ∆v(y  y(cid:48)) =(cid:74)y (cid:54)= y(cid:48)(cid:75) .

(5)

i=1

j=i+1

v∈V

v  . . .   yM

Here expression (cid:74)A(cid:75) equals 1 if A is true and 0 otherwise. The corresponding factor graph is

sketched in Fig. 1c. Such potentials can not be optimized with efﬁcient graph-cut based methods
and moreover  as shown in [19]  the bounds delivered by LP-relaxation [31] based solvers are very
loose in practice. Indeed  solutions delivered by such solvers are signiﬁcantly inferior even to the
results of the sequential method (2).
To cope with this issue a clique encoding representation of (4) was proposed in [19]. In this rep-
v (in the M nodes corresponding to the single initial node
resentation M-tuples of labels y1
v) were considered as the new labels. In this way the difﬁcult diversity factors were incorporated
into the unary factors of the new representation and the pairwise factors were adjusted respectively.
This allowed to (approximately) solve the problem (4) with graph-cuts based techniques if those
techniques were applicable to the energy E of a single labeling. The disadvantage of the clique
encoding representation is the exponential growth of the label space  which was reﬂected in a sig-
niﬁcantly higher inference time for the problem (4) compared to the procedure (2). In what follows 
we show an alternative transformation of the problem (4)  which (i) does not have this drawback (its
size is basically the same as those of (4)) and (ii) allows to exactly solve (4) in the case the energy
E is submodular.
Node-wise Diversity. In what follows we will mainly consider the node-wise diversity measures 
i.e. those  which can be represented in the form
∆M ({y}) =
v : (Lv)M → R  see Fig. 1b for illustration.

for some node diversity measures ∆M

v ({y}v)
∆M

(cid:88)

v∈V

(6)

3 M-Best-Diverse Labelings for Submodular Problems
Submodularity. In what follows we will assume that the sets Lv  v ∈ V  of labels are completely
ordered. This implies that for any s  t ∈ Lv their maximum and minimum  denoted as s∨ t and s∧ t
respectively  are well-deﬁned. Similarly let y1 ∨ y2 and y1 ∧ y2 denote the node-wise maximum
and minimum of any two labelings y1  y2 ∈ LA  A ⊆ V. Potential θf is called submodular  if for
any two labelings y1  y2 ∈ Lf it holds1:

θf (y1) + θf (y2) ≥ θf (y1 ∨ y2) + θf (y1 ∧ y2) .

(7)

Potential θ will be called supermodular  if (−θ) is submodular.

1Pairwise binary potentials satisfying θf (0  1) + θf (1  0) ≥ θf (0  0) + θf (1  1) build an important special

case of this deﬁnition.

4

Energy E is called submodular if for any two labelings y1  y2 ∈ LV it holds:

E(y1) + E(y2) ≥ E(y1 ∨ y2) + E(y1 ∧ y2) .

(8)

Submodularity of energy trivially follows from the submodularity of all its non-unary potentials θf  
f ∈ F  |f| > 1. In the pairwise case the inverse also holds: submodularity of energy implies also
submodularity of all its (pairwise) potentials (e.g. [31  Thm. 12]). There are efﬁcient methods for
solving energy minimization problems with submodular potentials  based on its transformation into
min-cut/max-ﬂow problem [21  28  16] in case all potentials are either unary or pairwise or to a
submodular max-ﬂow problem in the higher-order case [20  10  1].
Ordered M Solutions. In what follows we will write z ≤ z for any two vectors z1 and z
meaning that the inequality holds coordinate-wise.
For an arbitrary set A we will call a function f : (A)n → R of n variables permutation in-
variant if for any (x1  x2  . . .   xn) ∈ (A)n and any permutation π it holds f (x1  x2  . . .   xn) =
f (xπ(1)  xπ(2)  . . .   xπ(n)). In what follows we will consider mainly permutation invariant diversity
measures.
Let us consider two arbitrary labelings y1  y2 ∈ LV and their node-wise minimum y1 ∧ y2 and
maximum y1 ∨ y2. Since (y1
v)  for any
v). This in
permutation invariant node diversity measure it holds ∆2
its turn implies ∆2(y1 ∧ y2  y1 ∨ y2) = ∆2(y1  y2) for any node-wise diversity measure of the
form (6). If E is submodular  then from (8) it additionally follows that
E2(y1 ∧ y2  y1 ∨ y2) ≤ E2(y1  y2)  

(9)
where E2 is deﬁned as in (4). Note  that (y1 ∧ y2) ≤ (y1 ∨ y2). Generalizing these considerations
to M labelings one obtains
Theorem 1. Let E be submodular and ∆M be a node-wise diversity measure with each component
v being permutation invariant. Then there exists an ordered M-tuple (y1  . . .   yM )  yi ≤ yj for
∆M
1 ≤ i < j ≤ M  such that for any (z1  . . .   zM ) ∈ (LV )M it holds

v) is either equal to (y1
v) = ∆2

v) or to (y2
v ∧ y2
v  y1

v  y1
v ∨ y2

v  y2
v(y1

v ∨ y2

v ∧ y2

v  y1

v  y2

v(y1

EM ({y}) ≤ EM ({z})  

(10)

where EM is deﬁned as in (4).
Theorem 1 in particular claims that in the binary case Lv = {0  1}  v ∈ V  the optimal M labelings
deﬁne nested subsets of nodes  corresponding to the label 1.
Submodular formulation of M-Best-Diverse problem. Due to Theorem 1  for submodular ener-
gies and node-wise diversity measures it is sufﬁcient to consider only ordered M-tuples of labelings.
This order can be enforced by modifying the diversity measure accordingly:

ˆ∆M

v (y1  . . .   yM ) :=

v (y1  . . .   yM ) 
−∞ 

y1 ≤ y2 ≤ ··· ≤ yM
otherwise

 

(11)

and using it instead of the initial measure ∆M
practice one can use sufﬁciently big numbers in place of ∞ in (11). This implies
Lemma 1. Let E be submodular and ∆M be a node-wise diversity measure with each component
v being permutation invariant. Then any solution of the ordering enforcing M-best-diverse prob-
∆M
lem

is not permutation invariant.

v . Note that ˆ∆M

In

v

ˆEM ({y}) =

E(yi) − λ

ˆ∆M

v (y1

v  . . .   yM
v )

is a solution of the corresponding M-best-diverse problem (4)

EM ({y}) =

E(yi) − λ

∆M

v (y1

v  . . .   yM

v )  

i=1
v are related by (11).

v and ∆M

where ˆ∆M
We will say that a vector (y1  . . .   yM ) ∈ (Lv)M is ordered  if it holds y1 ≤ y2 ≤ ··· ≤ yM .

(cid:26)∆M

M(cid:88)
M(cid:88)

i=1

(cid:88)
(cid:88)

v∈V

v∈V

5

(12)

(13)

Given submodularity of E the submodularity (an hence – solvability) of EM in (13) would triv-
ially follow from the supermodularity of ∆M . However there hardly exist supermodular diver-
sity measures. The ordering provided by Theorem 1 and the corresponding form of the ordering-
enforcing diversity measure ˆ∆M signiﬁcantly weaken this condition  which is precisely stated by
the following lemma. In the lemma we substitute ∞ of (11) with a sufﬁciently big values such as
C∞ ≥ max{y} EM ({y}) for the sake of numerical implementation. Moreover  this values will
differ from each other to keep ˆ∆M
Lemma 2. Let for any two ordered vectors y = (y1  . . .   yM ) ∈ (Lv)M and z = (z1  . . .   zM ) ∈
(Lv)M it holds

(14)
where y ∨ z and y ∧ z are element-wise maximum and minimum respectively. Then ˆ∆v  deﬁned as

∆v(y ∨ z) + ∆v(y ∧ z) ≥ ∆v(y) + ∆v(z) 

v supermodular.

M−1(cid:88)

M(cid:88)



ˆ∆v(y1  . . .   yM ) = ∆v(y1  . . .   yM ) − C∞ ·

3max(0 yi−yj ) − 1

(15)

is supermodular.

i=1

j=i+1

Note  eq. (11) and (15) are the same up to the inﬁnity values in (11). Though condition (14) re-
sembles the supermodularity condition  it has to be fulﬁlled for ordered vectors only. The following
corollaries of Lemma 2 give two most important examples of the diversity measures fulﬁlling (14).
Corollary 1. Let |Lv| = 2 for all v ∈ V. Then the statement of Lemma 2 holds for arbitrary
∆v : (Lv)M → R.
Corollary 2. Let ∆M
is equivalent to

v (y1  . . .   yM ) =(cid:80)M−1

j=i+1 ∆ij(yi  yj). Then the condition of Lemma 2

(cid:80)M

∆ij(yi  yj) + ∆ij(yi + 1  yj + 1) ≥ ∆ij(yi + 1  yj) + ∆ij(yi  yj + 1) for yi < yj

(16)

i=1

In particular  condition (16) is satisﬁed for the Hamming distance ∆ij(y  y(cid:48)) =(cid:74)y (cid:54)= y(cid:48)(cid:75).

The following theorem trivially summarizes Lemmas 1 and 2:
Theorem 2. Let energy E and diversity measure ∆M satisfy conditions of Lemmas 1 and 2. Then
the ordering enforcing problem (12) delivers solution to the M-best-diverse problem (13) and is
submodular. Moreover  submodularity of all non-unary potentials of the energy E implies submod-
ularity of all non-unary potentials of the ordering enforcing energy ˆEM .

and 1 ≤ i < j ≤ M.

4 Experimental evaluation
We have tested our algorithms in two application scenarios: (a) interactive foreground/background
image segmentation  where annotation is available in the form of scribbles [3] and (b) Category level
segmentation on PASCAL VOC 2012 data [9].
As baselines we use: (i) the sequential method DivMBest (2) proposed in [3  25] and (ii) the
clique-encoding CE method [19] for an (approximate) joint computation of M-best-diverse label-
ings. As mentioned in Section 2  this method addresses the energy EM deﬁned in (4)  however it
has the disadvantage that its label space grows exponentially with M.
Our method that solves the problem (12) with the Hamming diversity measure (5) by trans-
forming it into min-cut/max-ﬂow problem [21  28  16] and running the solver [5] is denoted as
Joint-DivMBest.
Diversity measures used in experiments are: the Hamming distance (5) HD  Label Cost LC  Label
Transitions LT and Hamming Ball HB. The last three measures are higher order diversity potentials
introduced in [25] and used only in connection with the DivMBest algorithm. If not stated other-
wise  the Hamming distance (5) is used as a diversity measure. Both the clique encoding (CE) based
approaches and the submodularity-based methods proposed in this work use only the Hamming
distance as a diversity measure.
As [25] suggests  certain combinations of different diversity measures may lead to better results. To
denote such combinations  the signs ⊗ and ⊕ were used in [25]. We refer to [25] for a detailed
description of this notation and treat such combined methods as a black box for our comparison.

6

DivMBest
CE
Joint-DivMBest

M=2

M=6

M=10

quality
93.16
95.13
95.13

time
0.45
2.9
0.77

quality
95.02
96.01
96.01

time
2.4
47.6
5.2

quality
95.16
96.19
96.19

time
4.4
1247
20.4

Table 1: Interactive segmentation: per-pixel accuracies (quality) for the best segmentation out of M
ones and run-time. Compare to the average quality 91.57 of a single labeling. Hamming distance
is used as a diversity measure. The run-time is in milliseconds (ms). Joint-DivMBest quantita-
tively outperforms DivMBest  and is equal to CE  however  it is considerably faster than CE.

4.1

Interactive segmentation

Instead of returning a single segmentation corresponding to a MAP-solution  diversity methods pro-
vide to the user a small number of possible low-energy results based on the scribbles. Following [3]
we model only the ﬁrst iteration of such an interactive procedure  i.e. we consider user scribbles to
be given and compare the sets of segmentations returned by the compared diversity methods.
Authors of [3] kindly provided us their 50 graphical model instances  corresponding to the MAP-
inference problem (1). They are based on a subset of the PASCAL VOC 2010 [9] segmentation chal-
lenge with manually added scribbles. Pairwise potentials constitute contrast sensitive Potts terms [4] 
which are submodular. This implies that (i) the MAP-inference is solvable by min-cut/max-ﬂow al-
gorithms [21] and (ii) Theorem 2 is applicable and the M-best-diverse solutions can be found by
reducing the ordering preserving problem (12) to min-cut/max-ﬂow and applying the corresponding
algorithm.
Quantitative comparison and run-time of the considered methods is provided in Table 1  where
each method was used with the parameter λ (see (2)  (4))  optimally tuned via cross-validation.
Following [3]  as a quality measure we used the per pixel accuracy of the best solution for each
sample averaged over all test images. Methods CE and Joint-DivMBest gave the same quality 
which conﬁrms the observation made in [19]  that CE returns an exact MAP solution for each sample
in this dataset. Combined methods with more sophisticated diversity measures return results that are
either inferior to DivMBest or only negligibly improved once  hence we omitted them. The run-
time provided is also averaged over all samples. The max-ﬂow algorithm was used for DivMBest
and Joint-DivMBest and α-expansion for CE.
Summary. It can be seen that the Joint-DivMBest qualitatively outperforms DivMBest and
is equal to CE. However  it is considerably faster than the latter (the difference grows exponentially
with M) and the runtime is of the same order of magnitude as the one of DivMBest.

4.2 Category level segmentation

ing pairwise models with contrast sensitive Potts terms of the form θuv(y  y(cid:48)) = wuv(cid:74)y (cid:54)= y(cid:48)(cid:75) 

The category level segmentation from PASCAL VOC 2012 challenge [9] contains 1449 validation
images with known ground truth  which we used for evaluation of diversity methods. Correspond-
uv ∈ F  were used in [25] and kindly provided to us by the authors. Contrary to interactive segmen-
tation  the label sets contain 21 elements and hence the respective MAP-inference problem (1) is not
submodular anymore. However it still can be approximatively solved by α-expansion or α-β-swap.
Since the MAP-inference problem (1) is not submodular in this experiment  Theorem 2 is not ap-
plicable. We used two ways to overcome it. First  we modiﬁed the diversity potentials according
to (15)  as if Theorem 2 were to be correct. This basically means we were explicitly looking for
ordered M best diverse labelings. The resulting inference problem was addressed with α-β-swap
(since neither max-ﬂow nor the α-expansion algorithms are applicable). We refer to this method as
to Joint-DivMBest-ordered. The second way to overcome the non-submodularity problem 
is based on learning. Using structured SVM technique we trained pairwise potentials with addi-
tional constraints enforcing their submodularity  as it is done in e.g. [11]. We kept the contrast terms
ˆθ(y  y(cid:48))  uv ∈ F. We refer to

wuv and learned only a single submodular function ˆθ(y  y(cid:48))  which we used in place of(cid:74)y (cid:54)= y(cid:48)(cid:75).

After the learning  all our potentials had the form θuv(y  y(cid:48)) = wuv

7

DivMBest
HB∗
DivMBest∗⊕HB∗
HB∗⊗LC∗⊗LT∗
DivMBest∗⊗HB∗⊗LC∗⊗LT∗
CE
CE3
Joint-DivMBest-ordered
Joint-DivMBest-learned
Joint-DivMBest-learned

MAP inference

M=5

M=15

M=16

α-exp[4]

HB-HOP-MAP[30] 51.71
HB-HOP-MAP[30]
LT – coop. cuts[17]
LT – coop. cuts[17]

-
-
-

α-exp[4]
α-exp[4]

α-β-swap[4]
max-ﬂow[5]

α-exp[4]

quality time quality time quality time
51.21 0.01 52.90 0.03
53.07 0.03

-
-
-

-

-
-
-
-
-

55.32
55.89
56.97

-
-
-
-
54.22 733
58.36 7.24
54.14 2.28 57.76 5.87
53.81 0.01 56.08 0.08
56.31 0.08
53.85 0.38 56.14 35.47 56.33 38.67
53.84 0.01 56.08 0.08
56.31 0.08

57.39

-
-
-
-
-

-
-

Table 2: PASCAL VOC 2012.
Intersection over union quality measure/running time. The best
segmentation out of M is considered. Compare to the average quality 43.51 of a single labeling.
Time is in seconds (s). Notation ’-’ correspond to absence of result due to computational reasons or
inapplicability of the method. (∗)- methods were not run by us and the results were taken from [25]
directly. The MAP-inference column references the slowest inference technique out of those used
by the method.

this method as to Joint-DivMBest-learned. For the model we use max-ﬂow[5] as an exact
inference method and α-expansion[4] as a fast approximate inference method.
Quantitative comparison and run-time of the considered methods is provided in Table 2 
where each method was used with the parameter λ (see (2)  (4)) optimally tuned via cross-
validation on the validation set in PASCAL VOC 2012. Following [3]  we used the Intersec-
tion over union quality measure  averaged over all images. Among combined methods with
higher order diversity measures we selected only those providing the best results. The method
CE3 [19] is a hybrid of DivMBest and CE delivering a reasonable trade-off between run-
ning time and accuracy of inference for the model EM (4). Quantitative results delivered by
Joint-DivMBest-ordered and Joint-DivMBest-learned are very similar (though the
latter is negligibly better)  signiﬁcantly outperform those of DivMBest and only slightly infe-
rior to those of CE3. However the run-time for Joint-DivMBest-ordered and α-expansion
version of Joint-DivMBest-learned are comparable to those of DivMBest and outper-
form all other competitors due to use of the fast inference algorithms and linearly growing label
space  contrary to the label space of CE3  which grows as (Lv)3. Though we do not know ex-
act run-time for the combined methods (where ⊕ and ⊗ are used) we expect them to be signiﬁ-
cantly higher then those for DivMBest and Joint-DivMBest-ordered because of the intrin-
sically slow MAP-inference techniques used. However contrary to the latter one the inference in
Joint-DivMBest-learned can be exact due to submodularity of the underlying energy.

5 Conclusions

We have shown that submodularity of the MAP-inference problem implies a fully ordered set of
M best diverse solutions given a node-wise permutation invariant diversity measure. Enforcing
such ordering leads to a submodular formulation of the joint M-best-diverse problem and implies
its efﬁcient solvability. Moreover  we have shown that even in non-submodular cases  when the
MAP-inference is (approximately) solvable with efﬁcient graph-cut based methods  enforcing this
ordering leads to the M-best-diverse problem  which is (approximately) solvable with graph-cut
based methods as well. In our test cases (and there are likely others)  such an approximative tech-
nique lead to notably better results then those provided by the established sequential DivMBest
technique [3]  whereas its run-time remains quite comparable to the run-time of DivMBest and is
much smaller than the run-time of other competitors.

8

References

[1] C. Arora  S. Banerjee  P. Kalra  and S. Maheshwari. Generalized ﬂows for optimal inference in higher

order MRF-MAP. TPAMI  2015.

[2] D. Batra. An efﬁcient message-passing algorithm for the M-best MAP problem. arXiv:1210.4841  2012.
[3] D. Batra  P. Yadollahpour  A. Guzman-Rivera  and G. Shakhnarovich. Diverse M-best solutions in markov

random ﬁelds. In ECCV. Springer Berlin/Heidelberg  2012.

[4] Y. Boykov and M.-P. Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects

in N-D images. In ICCV  2001.

[5] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy

minimization in vision. TPAMI  26(9):1124–1137  2004.

[6] Y. Boykov  O. Veksler  and R. Zabih. Fast approximate energy minimization via graph cuts. TPAMI 

23(11):1222–1239  2001.

[7] C. Chen  V. Kolmogorov  Y. Zhu  D. N. Metaxas  and C. H. Lampert. Computing the M most probable

modes of a graphical model. In AISTATS  2013.

[8] G. Elidan and A. Globerson. The probabilistic inference challenge (PIC2011).
[9] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman. The PASCAL Visual Object

Classes Challenge 2012 (VOC2012) Results.

[10] A. Fix  A. Gruber  E. Boros  and R. Zabih. A graph cut algorithm for higher-order Markov random ﬁelds.

In ICCV  2011.

[11] V. Franc and B. Savchynskyy. Discriminative learning of max-sum classiﬁers. JMLR  9:67–104  2008.
[12] M. Fromer and A. Globerson. An lp view of the m-best map problem. In NIPS 22  2009.
[13] A. Guzman-Rivera  D. Batra  and P. Kohli. Multiple choice learning: Learning to produce multiple

structured outputs. In NIPS 25  2012.

[14] A. Guzman-Rivera  P. Kohli  and D. Batra. DivMCuts: Faster training of structural SVMs with diverse

M-best cutting-planes. In AISTATS  2013.

[15] A. Guzman-Rivera  P. Kohli  D. Batra  and R. A. Rutenbar. Efﬁciently enforcing diversity in multi-output

structured prediction. In AISTATS  2014.

[16] H. Ishikawa. Exact optimization for Markov random ﬁelds with convex priors. TPAMI  2003.
[17] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In

CVPR  2011.

[18] J. H. Kappes  B. Andres  F. A. Hamprecht  C. Schn¨orr  S. Nowozin  D. Batra  S. Kim  B. X. Kausler 
T. Kr¨oger  J. Lellmann  N. Komodakis  B. Savchynskyy  and C. Rother. A comparative study of modern
inference techniques for structured discrete energy minimization problems. IJCV  pages 1–30  2015.

[19] A. Kirillov  B. Savchynskyy  D. Schlesinger  D. Vetrov  and C. Rother. Inferring M-best diverse labelings

in a single one. In ICCV  2015.

[20] V. Kolmogorov. Minimizing a sum of submodular functions. Discrete Applied Mathematics  2012.
[21] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? TPAMI  2004.
[22] A. Kulesza and B. Taskar. Structured determinantal point processes. In NIPS 23  2010.
[23] E. L. Lawler. A procedure for computing the K best solutions to discrete optimization problems and its

application to the shortest path problem. Management Science  18(7)  1972.

[24] D. Nilsson. An efﬁcient algorithm for ﬁnding the m most probable conﬁgurationsin probabilistic expert

systems. Statistics and Computing  8(2):159–173  1998.

[25] A. Prasad  S. Jegelka  and D. Batra.

Submodular meets structured: Finding diverse subsets in

exponentially-large structured item sets. In NIPS 27  2014.

[26] V. Premachandran  D. Tarlow  and D. Batra. Empirical minimum bayes risk prediction: How to extract

an extra few % performance from vision models with just three more parameters. In CVPR  2014.

[27] V. Ramakrishna and D. Batra. Mode-marginals: Expressing uncertainty via diverse M-best solutions. In

NIPS Workshop on Perturbations  Optimization  and Statistics  2012.

[28] D. Schlesinger and B. Flach. Transforming an arbitrary minsum problem into a binary one. TU Dresden 

Fak. Informatik  2006.

[29] M. I. Schlesinger and V. Hlavac. Ten lectures on statistical and structural pattern recognition  volume 24.

Springer Science & Business Media  2002.

[30] D. Tarlow  I. E. Givoni  and R. S. Zemel. Hop-map: Efﬁcient message passing with high order potentials.

In AISTATS  2010.

[31] T. Werner. A linear programming approach to max-sum problem: A review. TPAMI  29(7)  2007.
[32] P. Yadollahpour  D. Batra  and G. Shakhnarovich. Discriminative re-ranking of diverse segmentations. In

CVPR  2013.

[33] C. Yanover and Y. Weiss. Finding the M most probable conﬁgurations using loopy belief propagation. In

NIPS 17  2004.

9

,Alexander Kirillov
Dmytro Shlezinger
Dmitry Vetrov
Carsten Rother
Bogdan Savchynskyy