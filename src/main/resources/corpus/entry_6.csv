2013,(More) Efficient Reinforcement Learning via Posterior Sampling,Most provably efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration  posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode  PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple  computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\tilde{O}(\tau S \sqrt{AT} )$ bound on the expected regret  where $T$ is time  $\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.,(More) Eﬃcient Reinforcement Learning via

Posterior Sampling

Osband  Ian

Stanford University
Stanford  CA 94305

iosband@stanford.edu

Russo  Daniel

Stanford University
Stanford  CA 94305

djrusso@stanford.edu

Van Roy  Benjamin
Stanford University
Stanford  CA 94305
bvr@stanford.edu

Abstract

Most provably-eﬃcient reinforcement learning algorithms introduce opti-
mism about poorly-understood states and actions to encourage exploration.
We study an alternative approach for eﬃcient exploration: posterior sam-
pling for reinforcement learning (PSRL). This algorithm proceeds in re-
peated episodes of known duration. At the start of each episode  PSRL
updates a prior distribution over Markov decision processes and takes one
sample from this posterior. PSRL then follows the policy that is optimal
for this sample during the episode. The algorithm is conceptually simple 
computationally eﬃcient and allows an agent to encode prior knowledge
in a natural way. We establish an ˜O(τ S
AT) bound on expected regret 
where T is time  τ is the episode length and S and A are the cardinali-
ties of the state and action spaces. This bound is one of the ﬁrst for an
algorithm not based on optimism  and close to the state of the art for any
reinforcement learning algorithm. We show through simulation that PSRL
signiﬁcantly outperforms existing algorithms with similar regret bounds.

√

1 Introduction

We consider the classical reinforcement learning problem of an agent interacting with its
environment while trying to maximize total reward accumulated over time [1  2]. The agent’s
environment is modeled as a Markov decision process (MDP)  but the agent is uncertain
about the true dynamics of the MDP. As the agent interacts with its environment  it observes
the outcomes that result from previous states and actions  and learns about the system
dynamics. This leads to a fundamental tradeoﬀ: by exploring poorly-understood states
and actions the agent can learn to improve future performance  but it may attain better
short-run performance by exploiting its existing knowledge.
Na¨ıve optimization using point estimates for unknown variables overstates an agent’s knowl-
edge  and can lead to premature and suboptimal exploitation. To oﬀset this  the majority of
provably eﬃcient learning algorithms use a principle known as optimism in the face of uncer-
tainty [3] to encourage exploration. In such an algorithm  each state and action is aﬀorded
some optimism bonus such that their value to the agent is modeled to be as high as is statis-
tically plausible. The agent will then choose a policy that is optimal under this “optimistic”
model of the environment. This incentivizes exploration since poorly-understood states and
actions will receive a higher optimism bonus. As the agent resolves its uncertainty  the ef-
fect of optimism is reduced and the agent’s behavior approaches optimality. Many authors
have provided strong theoretical guarantees for optimistic algorithms [4  5  6  7  8]. In fact 
almost all reinforcement learning algorithms with polynomial bounds on sample complexity
employ optimism to guide exploration.

1

We study an alternative approach to eﬃcient exploration  posterior sampling  and provide
ﬁnite time bounds on regret. We model the agent’s initial uncertainty over the environment
through a prior distribution.1 At the start of each episode  the agent chooses a new pol-
icy  which it follows for the duration of the episode. Posterior sampling for reinforcement
learning (PSRL) selects this policy through two simple steps. First  a single instance of the
environment is sampled from the posterior distribution at the start of an episode. Then 
PSRL solves for and executes the policy that is optimal under the sampled environment over
the episode. PSRL randomly selects policies according to the probability they are optimal;
exploration is guided by the variance of sampled policies as opposed to optimism.
The idea of posterior sampling goes back to 1933 [9] and has been applied successfully to
multi-armed bandits.
In that literature  the algorithm is often referred to as Thompson
sampling or as probability matching. Despite its long history  posterior sampling was largely
neglected by the multi-armed bandit literature until empirical studies [10  11] demonstrated
that the algorithm could produce state of the art performance. This prompted a surge of
interest  and a variety of strong theoretical guarantees are now available [12  13  14  15].
Our results suggest this method has great potential in reinforcement learning as well.
PSRL was originally introduced in the context of reinforcement learning by Strens [16]
under the name “Bayesian Dynamic Programming” 2 where it appeared primarily as a
heuristic method. In reference to PSRL and other “Bayesian RL” algorithms  Kolter and
Ng [17] write “little is known about these algorithms from a theoretical perspective  and
it is unclear  what (if any) formal guarantees can be made for such approaches.” Those
Bayesian algorithms for which performance guarantees exist are guided by optimism. BOSS
[18] introduces a more complicated version of PSRL that samples many MDPs  instead
of just one  and then combines them into an optimistic environment to guide exploration.
BEB [17] adds an exploration bonus to states and actions according to how infrequently
they have been visited. We show it is not always necessary to introduce optimism via a
complicated construction  and that the simple algorithm originally proposed by Strens [16]
satisﬁes strong bounds itself.
Our work is motivated by several advantages of posterior sampling relative to optimistic
algorithms. First  since PSRL only requires solving for an optimal policy for a single sam-
pled MDP  it is computationally eﬃcient both relative to many optimistic methods  which
require simultaneous optimization across a family of plausible environments [4  5  18]  and
to computationally intensive approaches that attempt to approximate the Bayes-optimal
solutions directly [18  19  20]. Second  the presence of an explicit prior allows an agent to
incorporate known environment structure in a natural way. This is crucial for most prac-
tical applications  as learning without prior knowledge requires exhaustive experimentation
in each possible state. Finally  posterior sampling allows us to separate the algorithm from
the analysis. In any optimistic algorithm  performance is greatly inﬂuenced by the manner
in which optimism is implemented. Past works have designed algorithms  at least in part  to
facilitate theoretical analysis for toy problems. Although our analysis of posterior sampling
is closely related to the analysis in [4]  this worst-case bound has no impact on the algo-
rithm’s actual performance. In addition  PSRL is naturally suited to more complex settings
where design of an eﬃciently optimistic algorithm might not be possible. We demonstrate
through a computational study in Section 6 that PSRL outperforms the optimistic algorithm
UCRL2 [4]: a competitor with similar regret bounds over some example MDPs.

2 Problem formulation

We consider the problem of learning to optimize a random ﬁnite horizon MDP M =
(S A  RM   P M   τ  ρ) in repeated ﬁnite episodes of interaction. S is the state space  A is
the action space  RM
a (s) is a probability distribution over reward realized when selecting
a (s0|s) is the probability of transitioning
action a while in state s whose support is [0  1]  P M
to state s0 if action a is selected while at state s  τ is the time horizon  and ρ the initial
state distribution. We deﬁne the MDP and all other random variables we will consider with

1For an MDP  this might be a prior over transition dynamics and reward distributions.
2We alter terminology since PSRL is neither Bayes-optimal  nor a direct approximation of this.

2

respect to a probability space (Ω F  P). We assume S  A  and τ are deterministic so the
agent need not learn the state and action spaces or the time horizon.
A deterministic policy µ is a function mapping each state s ∈ S and i = 1  . . .   τ to an action
a ∈ A. For each MDP M = (S A  RM   P M   τ  ρ) and policy µ  we deﬁne a value function

 τX

j=i

  
(cid:12)(cid:12)(cid:12)si = s

µ i(s) := EM µ
V M

R

M
aj

(sj)

M

(·|sj) for j = i  . . .   τ. A policy µ is said to be optimal for MDP M if V M

where R
a (s) denotes the expected reward realized when action a is selected while in state
s  and the subscripts of the expectation operator indicate that aj = µ(sj  j)  and sj+1 ∼
µ i(s) =
P M
aj
µ0 i(s) for all s ∈ S and i = 1  . . .   τ. We will associate with each MDP M a policy
maxµ0 V M
µM that is optimal for M.
The reinforcement learning agent interacts with the MDP over episodes that begin at times
tk = (k − 1)τ + 1  k = 1  2  . . .. At each time t  the agent selects an action at  observes
a scalar reward rt  and then transitions to st+1. If an agent follows a policy µ then when
in state s at time t during episode k  it selects an action at = µ(s  t − tk). Let Ht =
(s1  a1  r1  . . .   st−1  at−1  rt−1) denote the history of observations made prior to time t. A
reinforcement learning algorithm is a deterministic sequence {πk|k = 1  2  . . .} of functions 
each mapping Htk to a probability distribution πk(Htk) over policies. At the start of the kth
episode  the algorithm samples a policy µk from the distribution πk(Htk). The algorithm
then selects actions at = µk(st  t − tk) at times t during the kth episode.
We deﬁne the regret incurred by a reinforcement learning algorithm π up to time T to be

where ∆k denotes regret over the kth episode  deﬁned with respect to the MDP M∗ by

Regret(T  π) :=

∆k 

dT /τeX

k=1

ρ(s)(V M∗

µ∗ 1(s) − V M∗

µk 1(s)) 

∆k =X

s∈S

with µ∗ = µM∗ and µk ∼ πk(Htk). Note that regret is not deterministic since it can
depend on the random MDP M∗  the algorithm’s internal random sampling and  through
the history Htk  on previous random transitions and random rewards. We will assess and
compare algorithm performance in terms of regret and its expectation.

3 Posterior sampling for reinforcement learning
The use of posterior sampling for reinforcement learning (PSRL) was ﬁrst proposed by
Strens [16]. PSRL begins with a prior distribution over MDPs with states S  actions A and
horizon τ. At the start of each kth episode  PSRL samples an MDP Mk from the posterior
distribution conditioned on the history Htk available at that time. PSRL then computes
and follows the policy µk = µMk over episode k.

Algorithm: Posterior Sampling for Reinforcement Learning (PSRL)

Data: Prior distribution f  t=1
for episodes k = 1  2  . . . do

sample Mk ∼ f(·|Htk)
compute µk = µMk
for timesteps j = 1  . . .   τ do

sample and apply at = µk(st  j)
observe rt and st+1
t = t + 1

end

end

3

We show PSRL obeys performance guarantees intimately related to those for learning algo-
rithms based upon OFU  as has been demonstrated for multi-armed bandit problems [15].
We believe that a posterior sampling approach oﬀers some inherent advantages. Optimistic
algorithms require explicit construction of the conﬁdence bounds on V M∗
µ 1 (s) based on ob-
served data  which is a complicated statistical problem even for simple models. In addition 
even if strong conﬁdence bounds for V M∗
µ 1 (s) were known  solving for the best optimistic
policy may be computationally intractable. Algorithms such as UCRL2 [4] are computa-
tionally tractable  but must resort to separately bounding R
a (s) with high
probability for each s  a. These bounds allow a “worst-case” mis-estimation simultaneously
in every state-action pair and consequently give rise to a conﬁdence set which may be far
too conservative.
By contrast  PSRL always selects policies according to the probability they are optimal.
Uncertainty about each policy is quantiﬁed in a statistically eﬃcient way through the pos-
terior distribution. The algorithm only requires a single sample from the posterior  which
may be approximated through algorithms such as Metropolis-Hastings if no closed form
exists. As such  we believe PSRL will be simpler to implement  computationally cheaper
and statistically more eﬃcient than existing optimistic methods.

a (s) and P M

M

3.1 Main results
The following result establishes regret bounds for PSRL. The bounds have ˜O(τ S
AT)
expected regret  and  to our knowledge  provide the ﬁrst guarantees for an algorithm not
based upon optimism:
Theorem 1. If f is the distribution of M∗ then 

√

E(cid:2)Regret(T  πPS

τ )(cid:3) = O

(cid:16)

τ SpAT log(SAT)(cid:17)

(1)

This result holds for any prior distribution on MDPs  and so applies to an immense class
of models. To accommodate this generality  the result bounds expected regret under the
prior distribution (sometimes called Bayes risk or Bayesian regret). We feel this is a natural
measure of performance  but should emphasize that it is more common in the literature to
bound regret under a worst-case MDP instance. The next result provides a link between
these notions of regret. Applying Markov’s inequality to (1) gives convergence in probability.
Corollary 1. If f is the distribution of M∗ then for any α > 1
2 

τ )
Regret(T  πPS

T α

→

p

0.

√

√

As shown in the appendix  this also bounds the frequentist regret for any MDP with non-zero
probability. State-of-the-art guarantees similar to Theorem 1 are satisﬁed by the algorithms
UCRL2 [4] and REGAL [5] for the case of non-episodic RL. Here UCRL2 gives regret
AT) where D = maxs06=s minπ E[T(s0|M  π  s)] and T(s0|M  π  s) is the ﬁrst
bounds ˜O(DS
time step where s0 is reached from s under the policy π. REGAL improves this result to
AT) where Ψ ≤ D is the span of the of the optimal value function. However  there
˜O(ΨS
is so far no computationally tractable implementation of this algorithm.
In many practical applications we may be interested in episodic learning tasks where the
constants D and Ψ could be improved to take advantage of the episode length τ. Simple
modiﬁcations to both UCRL2 and REGAL will produce regret bounds of ˜O(τ S
AT)  just
as PSRL. This is close to the theoretical lower bounds of

SAT-dependence.

√

√

4 True versus sampled MDP

A simple observation  which is central to our analysis  is that  at the start of each kth
episode  M∗ and Mk are identically distributed. This fact allows us to relate quantities that
depend on the true  but unknown  MDP M∗  to those of the sampled MDP Mk  which is

4

fully observed by the agent. We introduce σ(Htk) as the σ-algebra generated by the history
up to tk. Readers unfamiliar with measure theory can think of this as “all information
known just before the start of period tk.” When we say that a random variable X is σ(Htk)-
measurable  this intuitively means that although X is random  it is deterministically known
given the information contained in Htk. The following lemma is an immediate consequence
of this observation [15].
Lemma 1 (Posterior Sampling). If f is the distribution of M∗ then  for any σ(Htk)-
measurable function g 
(2)
Note that taking the expectation of (2) shows E[g(M∗)] = E[g(Mk)] through the tower
property.

E[g(M∗)|Htk] = E[g(Mk)|Htk].

µk 1(s)) to be the regret over period k.
A signiﬁcant hurdle in analyzing this equation is its dependence on the optimal policy µ∗ 
which we do not observe. For many reinforcement learning algorithms  there is no clean way
to relate the unknown optimal policy to the states and actions the agent actually observes.
The following result shows how we can avoid this issue using Lemma 1. First  deﬁne

s∈S ρ(s)(V M∗

µ∗ 1(s)− V M∗

Recall  we have deﬁned ∆k =P

˜∆k =X

s∈S

ρ(s)(V Mk

µk 1(s) − V M∗

µk 1(s))

as the diﬀerence in expected value of the policy µk under the sampled MDP Mk  which is
known  and its performance under the true MDP M∗  which is observed by the agent.
Theorem 2 (Regret equivalence).

" mX

#

#

˜∆k

" mX

k=1

= E
and for any δ > 0 with probability at least 1 − δ 

∆k

k=1

E

Proof. Note  ∆k − ˜∆k =P

(3)

(4)

(5)

s∈S ρ(s)(V M∗

µ∗ 1(s) − V Mk

µk 1(s)) ∈ [−τ  τ]. By Lemma 1  E[∆k −

˜∆k|Htk] = 0. Taking expectations of these sums therefore establishes the claim.
This result bounds the agent’s regret in epsiode k by the diﬀerence between the agent’s
estimate V Mk
µk 1(stk) of the expected reward in Mk from the policy it chooses  and the expected
µk 1(stk) in M∗. If the agent has a poor estimate of the MDP M∗  we expect it to
reward V M∗
learn as the performance of following µk under M∗ diﬀers from its expectation under Mk.
As more information is gathered  its performance should improve. In the next section  we
formalize these ideas and give a precise bound on the regret of posterior sampling.

5 Analysis
An essential tool in our analysis will be the dynamic programming  or Bellman operator
T M
µ   which for any MDP M = (S A  RM   P M   τ  ρ)  stationary policy µ : S → A and value
function V : S → R  is deﬁned by

T M
µ V (s) := R

µ(s)(s0|s)V (s0).
P M

µ (s  µ) +X

M

s0∈S

This operation returns the expected value of state s where we follow the policy µ under the
laws of M  for one time step. The following lemma gives a concise form for the dynamic
programming paradigm in terms of the Bellman operator.
Lemma 2 (Dynamic programming equation). For any MDP M = (S A  RM   P M   τ  ρ)
and policy µ : S × {1  . . .   τ} → A  the value functions V M

satisfy

µ

µ i = T M
V M

µ(· i)V M

µ i+1

for i = 1 . . . τ  with V M

µ τ+1 := 0.

5

In order to streamline our notation we will let V ∗
T ∗
µ := T M∗

µ(·|s) := P M∗

µ(s)(·|s).

and P ∗

µ

µ i := V M∗

µ i

  V k

µ i(s) := V Mk

5.1 Rewriting regret in terms of Bellman error

E(cid:2) ˜∆k

(cid:12)(cid:12)M∗  Mk

(cid:3) = E

" τX

i=1

h(T k

µk(· i) − T ∗

µk(· i))V k

 

µ

µ i (s)  T k
µ := T Mk
#

(6)

µk i+1(stk+i)i(cid:12)(cid:12)(cid:12)(cid:12)M∗  Mk

To see why (6) holds  simply apply the Dynamic programming equation inductively:
µk 1 − V ∗
(V k

µk 2)(stk+1)

µk 2 − T ∗

µk(· 1)V ∗

µk 1)(stk+1) = (T k
= (T k

µk(· 1)V k
µk(· 1) − T ∗

{P ∗

µk(· 1))V k

µk(· 1))V k

µk 2(stk+1)

= (T k
= . . .
=

µk(· 1)(s0|stk+1)(V ∗

µk 2 − V k
µk 2(stk+1) + (V ∗

+X
s0∈S
µk(· 1) − T ∗
τX
(T k
µk(· i) − T ∗
µk(· i)(s0|stk+i)(V ∗
µk i+1(stk+i)i under the sampled MDP Mk. Crucially  (6) de-

µk 2)(s0)}
µk 2 − V k
τX
µk i+1(stk+i) +
dtk+i 
µk i+1 − V k
µk i+1)(s0)} − (V ∗

µk(· i))V k
µk i+1 − V k

µk 2)(stk+1) + dtk+1

i=1

i=1

µk i+1)(stk+i).
This expresses the regret in terms two factors. The ﬁrst factor is the one step Bellman

s0∈S{P ∗

where dtk+i :=P
error h(T k

µk(· i) − T ∗

µk(· i))V k

pends only the Bellman error under the observed policy µk and the states s1  ..  sT that are
actually visited over the ﬁrst T periods. We go on to show the posterior distribution of Mk
concentrates around M∗ as these actions are sampled  and so this term tends to zero.
The second term captures the randomness in the transitions of the true MDP M∗.
In state st under policy µk  the expected value of (V ∗
µk i+1)(stk+i) is exactly
µk i+1)(s0)}. Hence  conditioned on the true MDP M∗

P
and the sampled MDP Mk  the termPτ
s0∈S{P ∗

µk(· i)(s0|stk+i)(V ∗

i=1 dtk+i has expectation zero.

µk i+1 − V k

µk i+1 − V k

Introducing conﬁdence sets

5.2
The last section reduced the algorithm’s regret to its expected Bellman error. We will
proceed by arguing that the sampled Bellman operator T k
µk(· i) concentrates around the
true Bellman operatior T ∗
µk(· i). To do this  we introduce high probability conﬁdence sets
a(·|s) denote the emprical distribution up period
similar to those used in [4] and [5]. Let ˆP t
t of transitions observed after sampling (s  a)  and let ˆRt
a(s) denote the empirical average
t=1 1{(st at)=(s a)} to be the number of times (s  a)

was sampled prior to time tk. Deﬁne the conﬁdence set for episode k:

reward. Finally  deﬁne Ntk(s  a) = Ptk−1
(cid:13)(cid:13)(cid:13)1
Mk :=n
Where βk(s  a) :=q 14S log(2SAmtk)

a(·|s) − P M

(cid:13)(cid:13)(cid:13) ˆP t

a (·|s)

M :

≤ βk(s  a) & | ˆRt

a(s) − RM

a (s)| ≤ βk(s  a) ∀(s  a)o

max{1 Ntk (s a)} is chosen conservatively so that Mk contains both M∗
and Mk with high probability. It’s worth pointing out that we have not tried to optimize
this conﬁdence bound  and it can be improved  at least by a numerical factor  with more
careful analysis. Now  using that ˜∆k ≤ τ we can decompose regret as follows:

mX

˜∆k ≤ mX

k=1

k=1

mX

k=1

˜∆k1{Mk M∗∈Mk} + τ

6

[1{Mk /∈Mk} + 1{M∗ /∈Mk}]

(7)

since Mk

1  E[1{Mk /∈Mk}|Htk] =
Now 
E[1{M∗ /∈Mk}|Htk]. Lemma 17 of [4] shows3 P(M∗ /∈ Mk) ≤ 1/m for this choice of βk(s  a) 
which implies

is σ(Htk)-measureable 

by Lemma

#
mX
(cid:3) 1{Mk M∗∈Mk}

+ 2τ

k=1

#

+ 2τ

P{M∗ /∈ Mk}.

#

" mX

k=1

E

˜∆k

≤ E

≤ E

≤ E

˜∆k1{Mk M∗∈Mk}

k=1

" mX
" mX
E(cid:2) ˜∆k|M∗  Mk
mX
τX
mX
τX

|(T k

k=1

k=1

i=1

µk(· i) − T ∗

µk(· i))V k

µk i+1(stk+i)|1{Mk M∗∈Mk} + 2τ

≤ τ E

min{βk(stk+i  atk+i)  1} + 2τ.

We also have the worst–case bound Pm
Pτ
to provide a worst case bound on min{τPm
τ SpAT log(SAT)  which completes our analysis.
In the technical appendix we go on
i=1 min{βk(stk+i  atk+i)  1}  T} of order

˜∆k ≤ T.
k=1

k=1

k=1

i=1

(8)

6 Simulation results

We compare performance of PSRL to UCRL2 [4]: an optimistic algorithm with similar
regret bounds. We use the standard example of RiverSwim [21]  as well as several randomly
generated MDPs. We provide results in both the episodic case  where the state is reset
every τ = 20 steps  as well as the setting without episodic reset.

Figure 1: RiverSwim - continuous and dotted arrows represent the MDP under the actions
“right” and “left”.

RiverSwim consists of six states arranged in a chain as shown in Figure 1. The agent begins
at the far left state and at every time step has the choice to swim left or right. Swimming left
(with the current) is always successful  but swimming right (against the current) often fails.
The agent receives a small reward for reaching the leftmost state  but the optimal policy is
to attempt to swim right and receive a much larger reward. This MDP is constructed so
that eﬃcient exploration is required in order to obtain the optimal policy. To generate the
random MDPs  we sampled 10-state  5-action environments according to the prior.
We express our prior in terms of Dirichlet and normal-gamma distributions over the tran-
sitions and rewards respectively.4 In both environments we perform 20 Monte Carlo sim-
ulations and compute the total regret over 10 000 time steps. We implement UCRL2 with
δ = 0.05 and optimize the algorithm to take account of ﬁnite episodes where appropriate.
PSRL outperformed UCRL2 across every environment  as shown in Table 1. In Figure 2 
we show regret through time across 50 Monte Carlo simulations to 100 000 time–steps in
the RiverSwim environment: PSRL’s outperformance is quite extreme.

3Our conﬁdence sets are equivalent to those of [4] when the parameter δ = 1/m.
4These priors are conjugate to the multinomial and normal distribution. We used the values

α = 1/S  µ = σ2 = 1 and pseudocount n = 1 for a diﬀuse uniform prior.

7

Table 1: Total regret in simulation. PSRL outperforms UCRL2 over diﬀerent environments.

Algorithm

PSRL
UCRL2

Random MDP Random MDP RiverSwim RiverSwim
τ-episodes ∞-horizon
1.06 × 102
6.88 × 101
3.64 × 103
1.26 × 103

∞-horizon
7.30 × 103
1.13 × 105

τ-episodes
1.04 × 104
5.92 × 104

6.1 Learning in MDPs without episodic resets
The majority of practical problems in reinforcement learning can be mapped to repeated
episodic interactions for some length τ. Even in cases where there is no actual reset of
episodes  one can show that PSRL’s regret is bounded against all policies which work over
horizon τ or less [6]. Any setting with discount factor α can be learned for τ ∝ (1 − α)−1.
One appealing feature of UCRL2 [4] and REGAL [5] is that they learn this optimal timeframe
τ. Instead of computing a new policy after a ﬁxed number of periods  they begin a new
episode when the total visits to any state-action pair is doubled. We can apply this same
rule for episodes to PSRL in the ∞-horizon case  as shown in Figure 2. Using optimism
with KL-divergence instead of L1 balls has also shown improved performance over UCRL2
[22]  but its regret remains orders of magnitude more than PSRL on RiverSwim.

(a) PSRL outperforms UCRL2 by large margins (b) PSRL learns quickly despite misspeciﬁed prior

Figure 2: Simulated regret on the ∞-horizon RiverSwim environment.

7 Conclusion

√

We establish posterior sampling for reinforcement learning not just as a heuristic  but as a
provably eﬃcient learning algorithm. We present ˜O(τ S
AT) Bayesian regret bounds  which
are some of the ﬁrst for an algorithm not motivated by optimism and are close to state of the
art for any reinforcement learning algorithm. These bounds hold in expectation irrespective
of prior or model structure. PSRL is conceptually simple  computationally eﬃcient and can
easily incorporate prior knowledge. Compared to feasible optimistic algorithms we believe
that PSRL is often more eﬃcient statistically  simpler to implement and computationally
cheaper. We demonstrate that PSRL performs well in simulation over several domains. We
believe there is a strong case for the wider adoption of algorithms based upon posterior
sampling in both theory and practice.

Acknowledgments
Osband and Russo are supported by Stanford Graduate Fellowships courtesy of PACCAR
inc.  and Burt and Deedee McMurty  respectively. This work was supported in part by
Award CMMI-0968707 from the National Science Foundation.

8

References
[1] A. N. Burnetas and M. N. Katehakis. Optimal adaptive policies for markov decision processes.

Mathematics of Operations Research  22(1):222–255  1997.

[2] P. R. Kumar and P. Varaiya. Stochastic systems: estimation  identiﬁcation and adaptive

control. Prentice-Hall  Inc.  1986.

[3] T.L. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in

applied mathematics  6(1):4–22  1985.

[4] T. Jaksch  R. Ortner  and P. Auer. Near-optimal regret bounds for reinforcement learning.

The Journal of Machine Learning Research  99:1563–1600  2010.

[5] P. L. Bartlett and A. Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence  pages 35–42. AUAI Press  2009.

[6] R. I. Brafman and M. Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. The Journal of Machine Learning Research  3:213–231  2003.
[7] S. M. Kakade. On the sample complexity of reinforcement learning. PhD thesis  University of

London  2003.

[8] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine

Learning  49(2-3):209–232  2002.

[9] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3/4):285–294  1933.

[10] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Neural Information

Processing Systems (NIPS)  2011.

[11] S.L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in

Business and Industry  26(6):639–658  2010.

[12] S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. arXiv

preprint arXiv:1209.3353  2012.

[13] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoﬀs. arXiv

preprint arXiv:1209.3352  2012.

[14] E. Kauﬀmann  N. Korda  and R. Munos. Thompson sampling: an asymptotically optimal

ﬁnite time analysis. In International Conference on Algorithmic Learning Theory  2012.

[15] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. CoRR  abs/1301.2609 

2013.

[16] M. Strens. A Bayesian framework for reinforcement learning.

International Conference on Machine Learning  pages 943–950  2000.

In Proceedings of the 17th

[17] J. Z. Kolter and A. Y. Ng. Near-Bayesian exploration in polynomial time. In Proceedings of
the 26th Annual International Conference on Machine Learning  pages 513–520. ACM  2009.
[18] T. Wang  D. Lizotte  M. Bowling  and D. Schuurmans. Bayesian sparse sampling for on-line
reward optimization. In Proceedings of the 22nd international conference on Machine learning 
pages 956–963. ACM  2005.

[19] A. Guez  D. Silver  and P. Dayan. Eﬃcient bayes-adaptive reinforcement learning using sample-

based search. arXiv preprint arXiv:1205.3109  2012.

[20] J. Asmuth and M. L. Littman. Approaching bayes-optimalilty using monte-carlo tree search.

In Proc. 21st Int. Conf. Automat. Plan. Sched.  Freiburg  Germany  2011.

[21] A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov

decision processes. Journal of Computer and System Sciences  74(8):1309–1331  2008.

[22] S. Filippi  O. Capp´e  and A. Garivier. Optimism in reinforcement learning based on kullback-

leibler divergence. CoRR  abs/1004.5229  2010.

9

A Relating Bayesian to frequentist regret
Let M be any family of MDPs with non-zero probability under the prior. Then  for any  > 0 
α > 1
2:

(cid:18)Regret(T  πP S

τ

)

P

T α

> (cid:12)(cid:12)M

(cid:19)

∗ ∈ M

→ 0

This provides regret bounds even if M∗ is not distributed according to f. As long as the true
MDP is not impossible under the prior  we will have an asymptotic frequentist regret close to the
√
theoretical lower bounds of in T -dependence of O(

T ).

Proof. We have for any  > 0:
E[Regret(T  πP S

τ

)]

T α

≥ E

≥ P

(cid:18)Regret(T  πP S

Therefore via theorem (1)  for any α > 1
2:
∗ ∈ M

(cid:19)

(cid:12)(cid:12)M

P

)

τ

≤

T α

τ

)

T α

(cid:21)

∗ ∈ M

(cid:20)Regret(T  πP S
(cid:12)(cid:12)M
(cid:18)Regret(T  πP S
(cid:12)(cid:12)M
(cid:19) E[Regret(T  πP Sτ)]
(cid:18)

∗ ∈ M)

∗ ∈ M

(cid:19)

P (M

P (M

T α

1

)

τ

∗ ∈ M)

P (M∗ ∈ M)

T α

→ 0

i=1 min{βkstk+i  atk+i)  1}  T} which we claim is
(s a)} .

mX

τX

k=1

i=1

1{Ntk

>τ}

r 14S log(2SAmtk)

max{1  Ntk(s  a)}

B Bounding the sum of conﬁdence set widths

i=1

k=1

k=1

max{1 Ntk

τX

mX

Proof. In a manner similar to [4] we can say:

r 14S log(2SAmtk)
max{1  Ntk(s  a)} ≤

We are interested in bounding min{τPm
Pτ
O(τ SpAT log(SAT ) for βk(s  a) :=q 14S log(2SAmtk)
τX
mX
2τ times per state action pair. Therefore  Pm
Pτ
r
mX
tk+1−1X
mX
NT +1(s a)X
2X
s
X

r1(Ntk(st  at) > τ)

tk+1−1X

Ntk(st  at)

k=1
√

1{Ntk

t=tk

t=tk

k=1

k=1

≤

≤

j=1

i=1

s a

≤τ} +

Now  the consider the event (st  at) = (s  a) and (Ntk(s  a) ≤ τ). This can happen fewer than
i=1 1(Ntk(s  a) ≤ τ) ≤ 2τ SA.Now  suppose
Ntk(s  a) > τ. Then for any t ∈ {tk  ..  tk+1 − 1}  Nt(s  a) + 1 ≤ Ntk(s  a) + τ ≤ 2Ntk(s  a).
Therefore:

k=1

2

Nt(st  at) + 1 =

(Nt(st  at) + 1)−1/2

−1/2 ≤

j

−1/2

x

dx

TX
Z NT +1(s a)

t=1

2

√

2X

√

√

x=0

s a

2SAT

≤

2SA

NT +1(s  a) =

s a

Note that since all rewards and transitions are absolutely constrained ∈ [0  1] our regret
min{τ

SA + τp28S2AT log(SAT )  T}

min{βk(stk+i  atk+i)  1}  T} ≤ min{2τ

τX

mX

2

2τ 2SAT + τp28S2AT log(SAT ) ≤ τ Sp30AT log(SAT )

√

k=1

i=1

Which is our required result.

≤

10

,Ian Osband
Daniel Russo
Benjamin Van Roy