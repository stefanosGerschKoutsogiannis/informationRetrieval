2016,The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization with Stochastic Asynchronous PALM,We introduce the Stochastic Asynchronous Proximal Alternating Linearized Minimization (SAPALM) method  a block coordinate stochastic proximal-gradient method for solving nonconvex  nonsmooth optimization problems. SAPALM is the first asynchronous parallel optimization method that provably converges on a large class of nonconvex  nonsmooth problems. We prove that SAPALM matches the best known rates of convergence --- among synchronous or asynchronous methods --- on this problem class. We provide upper bounds on the number of workers for which we can expect to see a linear speedup  which match the best bounds known for less complex problems  and show that in practice SAPALM achieves this linear speedup. We demonstrate state-of-the-art performance on several matrix factorization problems.,The Sound of APALM Clapping: Faster Nonsmooth

Nonconvex Optimization with Stochastic

Asynchronous PALM

Damek Davis and Madeleine Udell

Cornell University

{dsd95 mru8}@cornell.edu

Brent Edmunds

University of California  Los Angeles
brent.edmunds@math.ucla.edu

Abstract

We introduce the Stochastic Asynchronous Proximal Alternating Linearized Min-
imization (SAPALM) method  a block coordinate stochastic proximal-gradient
method for solving nonconvex  nonsmooth optimization problems. SAPALM is the
ﬁrst asynchronous parallel optimization method that provably converges on a large
class of nonconvex  nonsmooth problems. We prove that SAPALM matches the
best known rates of convergence — among synchronous or asynchronous methods
— on this problem class. We provide upper bounds on the number of workers
for which we can expect to see a linear speedup  which match the best bounds
known for less complex problems  and show that in practice SAPALM achieves
this linear speedup. We demonstrate state-of-the-art performance on several matrix
factorization problems.

1

Introduction

Parallel optimization algorithms often feature synchronization steps: all processors wait for the last to
ﬁnish before moving on to the next major iteration. Unfortunately  the distribution of ﬁnish times is
heavy tailed. Hence as the number of processors increases  most processors waste most of their time
waiting. A natural solution is to remove any synchronization steps: instead  allow each idle processor
to update the global state of the algorithm and continue  ignoring read and write conﬂicts whenever
they occur. Occasionally one processor will erase the work of another; the hope is that the gain from
allowing processors to work at their own paces offsets the loss from a sloppy division of labor.
These asynchronous parallel optimization methods can work quite well in practice  but it is difﬁcult
to tune their parameters: lock-free code is notoriously hard to debug. For these problems  there
is nothing as practical as a good theory  which might explain how to set these parameters so as to
guarantee convergence.
In this paper  we propose a theoretical framework guaranteeing convergence of a class of asynchronous
algorithms for problems of the form

m(cid:88)

j=1

minimize

(x1 ... xm)∈H1×...×Hm

f (x1  . . .   xm) +

rj(xj) 

(1)

where f is a continuously differentiable (C 1) function with an L-Lipschitz gradient  each rj is a lower
semicontinuous (not necessarily convex or differentiable) function  and the sets Hj are Euclidean
spaces (i.e.  Hj = Rnj for some nj ∈ N). This problem class includes many (convex and nonconvex)
signal recovery problems  matrix factorization problems  and  more generally  any generalized low
rank model [20]. Following terminology from these domains  we view f as a loss function and each
rj as a regularizer. For example  f might encode the misﬁt between the observations and the model 
while the regularizers rj encode structural constraints on the model such as sparsity or nonnegativity.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Many synchronous parallel algorithms have been proposed to solve (1)  including stochastic proximal-
gradient and block coordinate descent methods [22  3]. Our asynchronous variants build on these
synchronous methods  and in particular on proximal alternating linearized minimization (PALM) [3].
These asynchronous variants depend on the same parameters as the synchronous methods  such as
a step size parameter  but also new ones  such as the maximum allowable delay. Our contribution
here is to provide a convergence theory to guide the choice of those parameters within our control
(such as the stepsize) in light of those out of our control (such as the maximum delay) to ensure
convergence at the rate guaranteed by theory. We call this algorithm the Stochastic Asynchronous
Proximal Alternating Linearized Minimization method  or SAPALM for short.
Lock-free optimization is not a new idea. Many of the ﬁrst theoretical results for such algorithms
appear in the textbook [2]  written over a generation ago. But within the last few years  asynchronous
stochastic gradient and block coordinate methods have become newly popular  and enthusiasm in
practice has been matched by progress in theory. Guaranteed convergence for these algorithms has
been established for convex problems; see  for example  [13  15  16  12  11  4  1].
Asynchrony has also been used to speed up algorithms for nonconvex optimization  in particular 
for learning deep neural networks [6] and completing low-rank matrices [23]. In contrast to the
convex case  the existing asynchronous convergence theory for nonconvex problems is limited to the
following four scenarios: stochastic gradient methods for smooth unconstrained problems [19  10];
block coordinate methods for smooth problems with separable  convex constraints [18]; block
coordinate methods for the general problem (1) [5]; and deterministic distributed proximal-gradient
methods for smooth nonconvex loss functions with a single nonsmooth  convex regularizer [9]. A
general block-coordinate stochastic gradient method with nonsmooth  nonconvex regularizers is still
missing from the theory. We aim to ﬁll this gap.

Contributions. We introduce SAPALM  the ﬁrst asynchronous parallel optimization method that
provably converges for all nonconvex  nonsmooth problems of the form (1). SAPALM is a a block
coordinate stochastic proximal-gradient method that generalizes the deterministic PALM method
of [5  3]. When applied to problem (1)  we prove that SAPALM matches the best  known rates of
convergence  due to [8] in the case where each rj is convex and m = 1: that is  asynchrony carries
no theoretical penalty for convergence speed. We test SAPALM on a few example problems and
compare to a synchronous implementation  showing a linear speedup.
Notation. Let m ∈ N denote the number of coordinate blocks. We let H = H1 × . . . × Hm. For
every x ∈ H  each partial gradient ∇jf (x1  . . .   xj−1 ·  xj+1  . . .   xm) : Hj → Hj is Lj-Lipschitz
continuous; we let L = minj{Lj} ≤ maxj{Lj} = L. The number τ ∈ N is the maximum
j=1 rj(xj). For
each j ∈ {1  . . .   m}  y ∈ Hj  and γ > 0  deﬁne the proximal operator

allowable delay. Deﬁne the aggregate regularizer r : H → (−∞ ∞] as r(x) =(cid:80)m

(cid:26)

(cid:27)

proxγrj (y) := argmin
xj∈Hj

rj(xj) +

(cid:107)xj − y(cid:107)2

1
2γ

For convex rj  proxγrj (y) is uniquely deﬁned  but for nonconvex problems  it is  in general  a set.
We make the mild assumption that for all y ∈ Hj  we have proxγrj (y) (cid:54)= ∅. A slight technicality
arises from our ability to choose among multiple elements of proxγrj (y)  especially in light of the
stochastic nature of SAPALM. Thus  for all y  j and γ > 0  we ﬁx an element

ζj(y  γ) ∈ proxγrj (y).

(2)
By [17  Exercise 14.38]  we can assume that ζj is measurable  which enables us to reason with expec-
tations wherever they involve ζj. As shorthand  we use proxγrj (y) to denote the (unique) choice

ζj(y  γ). For any random variable or vector X  we let Ek [X] = E(cid:2)X | xk  . . .   x0  νk  . . .   ν0(cid:3)

denote the conditional expectation of X with respect to the sigma algebra generated by the history of
SAPALM.

2 Algorithm Description

Algorithm 1 displays the SAPALM method.
We highlight a few features of the algorithm which we discuss in more detail below.

2

Algorithm 1 SAPALM [Local view]
Input: x ∈ H
1: All processors in parallel do
2: loop
3:
4:
5:
6:
7:

Randomly select a coordinate block j ∈ {1  . . .   m}
Read x from shared memory
Compute g = ∇jf (x) + νj
Choose stepsize γj ∈ R++
xj ← proxγj rj (xj − γjg)

(cid:46) According to Assumption 3
(cid:46) According to (2)

from memory.

• Inconsistent iterates. Other processors may write updates to x in the time required to read x
• Coordinate blocks. When the coordinate blocks xj are low dimensional  it reduces the
• Noise. The noise ν ∈ H is a random variable that we use to model injected noise. It can be

likelihood that one update will be immediately erased by another  simultaneous update.

set to 0  or chosen to accelerate each iteration  or to avoid saddle points.

Algorithm 1 has an equivalent (mathematical) description which we present in Algorithm 2  using an
iteration counter k which is incremented each time a processor completes an update. This iteration
counter is not required by the processors themselves to compute the updates.
In Algorithm 1  a processor might not have access to the shared-memory’s global state  xk  at
iteration k. Rather  because all processors can continuously update the global state while other
processors are reading  local processors might only read the inconsistently delayed iterate xk−dk =
(xk−dk 1

)  where the delays dk are integers less than τ  and xl = x0 when l < 0.

  . . .   xk−dk m

m

1

Algorithm 2 SAPALM [Global view]
Input: x0 ∈ H
1: for k ∈ N do
2:
3:
4:
5:
6:
7:
8:
9:
10:

Randomly select a coordinate block jk ∈ {1  . . .   m}
Read xk−dk = (xk−dk 1
Compute gk = ∇jk f (xk−dk ) + νk
Choose stepsize γk
jk
for j = 1  . . .   m do
if j = jk then

xk+1
jk
j ← xk
xk+1

  . . .   xk−dk m

← proxγk

∈ R++

− γk

jk

else

(xk
jk

rjk

jk

m

jk

1

j

) from shared memory

(cid:46) According to Assumption 3

(cid:46) According to (2)

gk)

2.1 Assumptions on the Delay  Independence  Variance  and Stepsizes
Assumption 1 (Bounded Delay). There exists some τ ∈ N such that  for all k ∈ N  the sequence of
coordinate delays lie within dk ∈ {0  . . .   τ}m.
Assumption 2 (Independence). The indices {jk}k∈N are uniformly distributed and collectively IID.
They are independent from the history of the algorithm xk  . . .   x0  νk  . . .   ν0 for all k ∈ N.
We employ two possible restrictions on the noise sequence νk and the sequence of allowable stepsizes
j   all of which lead to different convergence rates:
γk
Assumption 3 (Noise Regimes and Stepsizes). Let σ2
norm of the noise  and let a ∈ (1 ∞). Assume that Ek
weights {ck}k∈N ⊆ [1 ∞) such that

(cid:2)(cid:107)νk(cid:107)2(cid:3) denote the expected squared
(cid:2)νk(cid:3) = 0 and that there is a sequence of

k := Ek

(∀k ∈ N)   (∀j ∈ {1  . . .   m})

γk
j :=

1

ack(Lj + 2Lτ m−1/2)

.

3

which we choose using the following two rules  both of which depend on the growth of σk:

(cid:80)∞
k = O((k + 1)−α) =⇒ ck = Θ((k + 1)(1−α)).
σ2

Summable.
α-Diminishing. (α ∈ (0  1))
More noise  measured by σk  results in worse convergence rates and stricter requirements regarding
which stepsizes can be chosen. We provide two stepsize choices which  depending on the noise regime 
interpolate between Θ(1) and Θ(k1−α) for any α ∈ (0  1). Larger stepsizes lead to convergence
rates of order O(k−1)  while smaller ones lead to order O(k−α).

=⇒ ck ≡ 1;

k < ∞

k=0 σ2

2.2 Algorithm Features

Inconsistent Asynchronous Reading. SAPALM allows asynchronous access patterns. A proces-
sor may  at any time  and without notifying other processors:

1. Read. While other processors are writing to shared-memory  read the possibly out-of-sync 

delayed coordinates xk−dk 1

  . . .   xk−dk m

m

.

1

2. Compute. Locally  compute the partial gradient ∇jk f (xk−dk 1
3. Write. After computing the gradient  replace the jkth coordinate with
1
2γk
jk

rjk (y) + (cid:104)∇jk f (xk−dk ) + νk

∈ argmin

  y − xk

xk+1
jk

(cid:105) +

jk

jk

y

1

  . . .   xk−dk m

m

).

(cid:107)y − xk

jk

(cid:107)2.

Uncoordinated access eliminates waiting time for processors  which speeds up computation. The
processors are blissfully ignorant of any conﬂict between their actions  and the paradoxes these
conﬂicts entail: for example  the states xk−dk 1
need never have simultaneously existed
in memory. Although we write the method with a global counter k  the asynchronous processors need
not be aware of it; and the requirement that the delays dk remain bounded by τ does not demand
coordination  but rather serves only to deﬁne τ.

  . . .   xk−dk m

m

1

What Does the Noise Model Capture? SAPALM is the ﬁrst asynchronous PALM algorithm to
allow and analyze noisy updates. The stochastic noise  νk  captures three phenomena:

1. Computational Error. Noise due to random computational error.
2. Avoiding Saddles. Noise deliberately injected for the purpose of avoiding saddles  as in [7].
3. Stochastic Gradients. Noise due to stochastic approximations of delayed gradients.

Of course  the noise model also captures any combination of the above phenomena. The last one is 
perhaps  the most interesting: it allows us to prove convergence for a stochastic- or minibatch-gradient
version of APALM  rather than requiring processors to compute a full (delayed) gradient. Stochastic
gradients can be computed faster than their batch counterparts  allowing more frequent updates.

(cid:2)∇f (xk−dk ; ξ)(cid:3) = ∇f (xk−dk )  and Ek

2.3 SAPALM as an Asynchronous Block Mini-Batch Stochastic Proximal-Gradient Method
In Algorithm 1  any stochastic estimator ∇f (xk−dk ; ξ) of the gradient may be used  as long as
Ek
if Problem 1 takes the form

(cid:2)(cid:107)∇f (xk−dk ; ξ) − ∇f (xk−dk )(cid:107)2(cid:3) ≤ σ2. In particular 

1
m

x∈H

rj(xj) 

minimize

in Algorithm 2 

Eξ [f (x1  . . .   xm; ξ)] +

the stochastic mini-batch estimator gk = m−1

(cid:2)(cid:107)gk − ∇f (xk−dk )(cid:107)2(cid:3) = O(m−1

(cid:80)mk
i=1 ∇f (xk−dk ; ξi) 
then 
where ξi are IID  may be used in place of ∇f (xk−dk ) + νk. A quick calculation shows that
k ). Thus  any increasing batch size mk = Ω((k + 1)−α)  with
Ek
α ∈ (0  1)  conforms to Assumption 3.
When nonsmooth regularizers are present  all known convergence rate results for nonconvex stochastic
gradient algorithms require the use of increasing  rather than ﬁxed  minibatch sizes; see [8  22] for
analogous  synchronous algorithms.

j=1

k

m(cid:88)

4

3 Convergence Theorem

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 ;

 m(cid:88)

j=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

Sk := E

Measuring Convergence for Nonconvex Problems. For nonconvex problems  it is standard to
measure convergence (to a stationary point) by the expected violation of stationarity  which for us is
the (deterministic) quantity:

(wk

j ) + νk

j − xk
γk
j
j − γk
j (∇jf (xk−dk ) + νk
j − xk
j + γk
j ∈ −γk
j νk

j = −γk

j (∂Lrj(wk

j νk

(∀j ∈ {1  . . .   m})

wk

j )).

where

j = proxγk

hence  Sk = E(cid:2)(cid:107)∇f (xk)(cid:107)2(cid:3). More generally  wk

j rj (xk
A reduction to the case r ≡ 0 and dk = 0 reveals that wk
j + γk

(3)
j ∇jf (xk) and 
j ) + ∇jf (xk−dk ))
where ∂Lrj is the limiting subdifferential of rj [17] which  if rj is convex  reduces to the standard
convex subdifferential familiar from [14]. A messy but straightforward calculation shows that our
convergence rates for Sk can be converted to convergence rates for elements of ∂Lr(wk) + ∇f (wk).
We present our main convergence theorem now and defer the proof to Section 4.
Theorem 1 (SAPALM Convergence Rates). Let {xk}k∈N ⊆ H be the SAPALM sequence created
by Algorithm 2. Then  under Assumption 3 the following convergence rates hold: for all T ∈ N  if
{νk}k∈N is

j − rk

1. Summable  then

min

k=0 ... T

2. α-Diminishing  then

Sk ≤ Ek∼PT [Sk] = O

Sk ≤ Ek∼PT [Sk] = O

min

k=0 ... T

(cid:18) m(L + 2Lτ m−1/2)

(cid:19)

T + 1

;

(cid:18) m(L + 2Lτ m−1/2) + m log(T + 1)

(cid:19)

;

(T + 1)−α

where  for all T ∈ N  PT is the distribution {0  . . .   T} such that PT (X = k) ∝ c−1
k .
Effects of Delay and Linear Speedups. The m−1/2 term in the convergence rates presented in
√
Theorem 1 prevents the delay τ from dominating our rates of convergence. In particular  as long as
m)  the convergence rate in the synchronous (τ = 0) and asynchronous cases are within a
τ = O(
small constant factor of each other. In that case  because the work per iteration in the synchronous
and asynchronous versions of SAPALM is the same  we expect a linear speedup: SAPALM with p
processors will converge nearly p times faster than PALM  since the iteration counter will be updated
p times as often. As a rule of thumb  τ is roughly proportional to the number of processors. Hence
we can achieve a linear speedup on as many as O(

m) processors.

√

3.1 The Asynchronous Stochastic Block Gradient Method

If the regularizer r is identically zero  then the noise νk need not vanish in the limit. The following
theorem guarantees convergence of asynchronous stochastic block gradient descent with a constant
minibatch size. See the supplemental material for a proof.
Theorem 2 (SAPALM Convergence Rates (r ≡ 0)). Let {xk}k∈N ⊆ H be the SAPALM sequence
created by Algorithm 2 in the case that r ≡ 0. If  for all k ∈ N  {Ek
necessarily diminishing) and

(cid:2)(cid:107)νk(cid:107)2(cid:3)}k∈N is bounded (not

(∃a ∈ (1 ∞))   (∀k ∈ N)   (∀j ∈ {1  . . .   m})

then for all T ∈ N  we have

Sk ≤ Ek∼PT [Sk] = O

min

k=0 ... T

γk
j :=

√
a

1

k(Lj + 2M τ m−1/2)

 

(cid:18) m(L + 2Lτ m−1/2) + m log(T + 1)

(cid:19)

 

√

T + 1

where PT is the distribution {0  . . .   T} such that PT (X = k) ∝ k−1/2.

5

4 Convergence Analysis

4.1 The Asynchronous Lyapunov Function
Key to the convergence of SAPALM is the following Lyapunov function  deﬁned on H1+τ   which
aggregates not only the current state of the algorithm  as is common in synchronous algorithms  but
also the history of the algorithm over the delayed time steps: (∀x(0)  x(1)  . . .   x(τ ) ∈ H)

Φ(x(0)  x(1)  . . .   x(τ )) = f (x(0)) + r(x(0)) +

√
L
2

m

(τ − h + 1)(cid:107)x(h) − x(h − 1)(cid:107)2.

τ(cid:88)

h=1

This Lyapunov function appears in our convergence analysis through the following inequality  which
is proved in the supplemental material.
Lemma 1 (Lyapunov Function Supermartingale Inequality). For all k ∈ N 
(xk  . . .   xk−τ ) ∈ H1+τ . Then for all  > 0  we have

let zk =

(cid:18)

(cid:32)
(cid:19)(cid:33)
m(cid:88)
j (1 + −1)(cid:0)Lj + 2Lτ m−1/2(cid:1)(cid:1) Ek
(cid:0)1 + γk

− (1 + )

2Lτ
m1/2

1
γk
j

Lj +

Ek

j=1

(cid:2)(cid:107)wk
(cid:2)(cid:107)νk
j (cid:107)2(cid:3)

j (cid:107)2(cid:3)

j − xk

j + γk

j νk

Ek

(cid:2)Φ(zk+1)(cid:3) ≤ Φ(zk) − 1
m(cid:88)

γk
j

2m

+

j=1

2m

j − γk
where for all j ∈ {1  . . .   m}  we have wk
for σk = 0  we can take  = 0 and assume the last line is zero.

j = proxγk

j rj (xk

j (∇jf (xk−dk ) + νk

j )). In particular 

Notice that if σk =  = 0 and γk
j is chosen as suggested in Algorithm 2  the (conditional) expected
value of the Lyapunov function is strictly decreasing. If σk is nonzero  the factor  will be used in
concert with the stepsize γk

j to ensure that noise does not cause the algorithm to diverge.

γk
j

1
γk
j

+

j=1

2m

j=1

(cid:19)

(cid:18)

(cid:107)Ak

j(cid:107)2

j + γk

j νk

j := wk

m(cid:88)

j   we have

1 − 1 + 
ack

(cid:2)(cid:107)νk
j (cid:107)2(cid:3)(cid:3)

.

(4)

Two upper bounds follow from the the deﬁnition of γk
ward inequalities (ack)−1(L + 2M τ m−1/2)−1 ≥ γk

4.2 Proof of Theorem 1
For either noise regime  we deﬁne  for all k ∈ N and j ∈ {1  . . .   m}  the factor  := 2−1(a − 1).
With the assumed choice of γk
j and   Lemma 1 implies that the expected Lyapunov function decreases 
up to a summable residual: with Ak

 1

j − xk
m(cid:88)
E(cid:2)Φ(zk+1)(cid:3) ≤ E(cid:2)Φ(zk)(cid:3) − E
j (1 + −1)(cid:0)Lj + 2Lτ m−1/2(cid:1)(cid:1) E(cid:2)Ek
(cid:0)1 + γk
 1
j (1 + −1)(cid:0)Lj + 2Lτ m−1/2(cid:1)(cid:1) Ek
Sk ≤ f (x0) + r(x0) − inf x∈H{f (x) + r(x)} +(cid:80)T
(cid:80)T
k=0 c−1

m(cid:88)
Now rearrange (4)  use E(cid:2)Φ(zk+1)(cid:3) ≥ inf x∈H{f (x) + r(x)} and E(cid:2)Φ(z0)(cid:3) = f (x0) + r(x0)  and
1(cid:80)T
k=0 c−1

j   the lower bound ck ≥ 1  and the straightfor-
j ≥ (ack)−1(L + 2M τ m−1/2)−1:
m(cid:88)
j (cid:107)2(cid:3)
(cid:2)(cid:107)νk

≤ (1 + (ack)−1(1 + −1))(σ2
2a(L + 2Lτ m−1/2)

1 − 1 + 
ack

(1+(ack)−1(1+−1))(σ2
2a(L+2Lτ m−1/2)

(cid:0)1 + γk

sum (4) over k to get

2ma(L+2Lτ m−1/2)

(1−(1+)a−1)

(1−(1+)a−1)

T(cid:88)

(cid:107)Ak

j(cid:107)2

Sk ≤



k/ck)

.

2m

j=1

(cid:18)

(cid:19)

k=0

k

1
γk
j

1

E

1
ck

and

γk
j

j=1

1
ck

k

k=0

2ma(L+2Lτ m−1/2)

2m

2m

k/ck)

.

6

The left hand side of this inequality is bounded from below by mink=0 ... T Sk and is precisely the
term Ek∼PT [Sk]. What remains to be shown is an upper bound on the right hand side  which we will
now call RT .

If the noise is summable  then ck ≡ 1  so(cid:80)T
that RT = O(m(L + 2Lτ m−1/2)(T + 1)−1). If the noise is α-diminishing  then ck = Θ(cid:0)k(1−α)(cid:1) 
so(cid:80)T
(cid:80)T

k/ck < ∞  which implies
k/ck = O(k−1)  there exists a B > 0 such that
k=0 Bk−1 = O(log(T + 1))  which implies that RT = O((m(L + 2Lτ m−1/2) +

k = (T +1) and(cid:80)T

k = Θ((T + 1)α) and  because σ2

k=0 c−1

k/ck ≤(cid:80)T

k=0 σ2

m log(T + 1))(T + 1)−α).

k=0 c−1

k=0 σ2

5 Numerical Experiments

In this section  we present numerical results to conﬁrm that SAPALM delivers the expected perfor-
mance gains over PALM. We conﬁrm two properties: 1) SAPALM converges to values nearly as
low as PALM given the same number of iterations  2) SAPALM exhibits a near-linear speedup as
the number of workers increases. All experiments use an Intel Xeon machine with 2 sockets and 10
cores per socket.
We use two different nonconvex matrix factorization problems to exhibit these properties  to which
we apply two different SAPALM variants: one without noise  and one with stochastic gradient noise.
For each of our examples  we generate a matrix A ∈ Rn×n with iid standard normal entries  where
n = 2000. Although SAPALM is intended for use on much larger problems  using a small problem
size makes write conﬂicts more likely  and so serves as an ideal setting to understand how asynchrony
affects convergence.

1. Sparse PCA with Asynchronous Block Coordinate Updates. We minimize

argmin

X Y

1
2

||A − X T Y ||2

F + λ(cid:107)X(cid:107)1 + λ(cid:107)Y (cid:107)1 

(5)

where X ∈ Rd×n and Y ∈ Rd×n for some d ∈ N. We solve this problem using SAPALM
with no noise νk = 0.

2. Quadratically Regularized Firm Thresholding PCA with Asynchronous Stochastic

Gradients. We minimize

argmin

X Y

1
2

||A − X T Y ||2

F + λ((cid:107)X(cid:107)Firm + (cid:107)Y (cid:107)Firm) +

((cid:107)X(cid:107)2

F + (cid:107)Y (cid:107)2
F ) 

µ
2

(6)

where X ∈ Rd×n  Y ∈ Rd×n  and (cid:107)·(cid:107)Firm is the ﬁrm thresholding penalty proposed in [21]:
a nonconvex  nonsmooth function whose proximal operator truncates small values to zero
and preserves large values. We solve this problem using the stochastic gradient SAPALM
variant from Section 2.3.

In both experiments X and Y are treated as coordinate blocks. Notice that for this problem  the
SAPALM update decouples over the entries of each coordinate block. Each worker updates its
coordinate block (say  X) by cycling through the coordinates of X and updating each in turn 
restarting at a random coordinate after each cycle.
In Figures (1a) and (1c)  we see objective function values plotted by iteration. By this metric 
SAPALM performs as well as PALM  its single threaded variant; for the second problem  the curves
for different thread counts all overlap. Note  in particular  that SAPALM does not diverge. But
SAPALM can add additional workers to increment the iteration counter more quickly  as seen in
Figure 1b  allowing SAPALM to outperform its single threaded variant.
We measure the speedup Sk(p) of SAPALM by the (relative) time for p workers to produce k iterates

Sk(p) =

Tk(p)
Tk(1)

 

(7)

where Tk(p) is the time to produce k iterates using p workers. Table 2 shows that SAPALM achieves
near linear speedup for a range of variable sizes d. (Dashes — denote experiments not run.)

7

(a) Iterates vs objective
(d) Time (s) vs. objective
Figure 1: Sparse PCA ((1a) and (1b)) and Firm Thresholding PCA ((1c) and (1d)) tests for d = 10.

(b) Time (s) vs. objective

(c) Iterates vs. objective

threads
1
2
4
8
16

d=10
65.9972
33.464
17.5415
9.2376
4.934

d=100
6144.9427
–
–
833.5635
416.8038
Table 1: Sparse PCA timing for 16 iterations
by problem size and thread count.

d=20
253.387
127.8973
67.3267
34.5614
17.4362

threads
1
2
4
8
16

d=10
1
1.9722
3.7623
7.1444
13.376

d=20
1
1.9812
3.7635
7.3315
14.5322

d=100
1
–
–
7.3719
14.743

Table 2: Sparse PCA speedup for 16 iterations
by problem size and thread count.

Deviations from linearity can be attributed to a breakdown in the abstraction of a “shared memory”
computer: as each worker modiﬁes the “shared” variables X and Y   some communication is required
to maintain cache coherency across all cores and processors. In addition  Intel Xeon processors
share L3 cache between all cores on the processor. All threads compete for the same L3 cache space 
slowing down each iteration. For small d  write conﬂicts are more likely; for large d  communication
to maintain cache coherency dominates.

6 Discussion

A few straightforward generalizations of our work are possible; we omit them to simplify notation.

Removing the log factors. The log factors in Theorem 1 can easily be removed by ﬁxing a
maximum number of iterations for which we plan to run SAPALM and adjusting the ck factors
accordingly  as in [14  Equation (3.2.10)].
Cluster points of {xk}k∈N. Using the strategy employed in [5]  it’s possible to show that all cluster
points of {xk}k∈N are (almost surely) stationary points of f + r.

Weakened Assumptions on Lipschitz Constants. We can weaken our assumptions to allow Lj
to vary: we can assume Lj(x1  . . .   xj−1 ·  xj+1  . . .   xm)-Lipschitz continuity each partial gradient
∇jf (x1  . . .   xj−1 ·  xj+1  . . .   xm) : Hj → Hj  for every x ∈ H.

7 Conclusion

This paper presented SAPALM  the ﬁrst asynchronous parallel optimization method that provably
converges on a large class of nonconvex  nonsmooth problems. We provide a convergence theory for
SAPALM  and show that with the parameters suggested by this theory  SAPALM achieves a near linear
speedup over serial PALM. As a special case  we provide the ﬁrst convergence rate for (synchronous
or asynchronous) stochastic block proximal gradient methods for nonconvex regularizers. These
results give speciﬁc guidance to ensure fast convergence of practical asynchronous methods on a
large class of important  nonconvex optimization problems  and pave the way towards a deeper
understanding of stability of these methods in the presence of noise.

8

0100200300400105106107108 124816050100150200105106107108 12481601234x 1050.511.522.533.5x 107 12481605101520250.511.522.533.5x 107 124816References
[1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In 2012 IEEE 51st IEEE

Conference on Decision and Control (CDC)  pages 5451–5452  Dec 2012.

[2] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods  volume 23.
[3] J. Bolte  S. Sabach  and M. Teboulle. Proximal alternating linearized minimization for nonconvex and

nonsmooth problems. Mathematical Programming  146(1-2):459–494  2014.

[4] D. Davis. SMART: The Stochastic Monotone Aggregated Root-Finding Algorithm. arXiv preprint

arXiv:1601.00698  2016.

[5] D. Davis. The Asynchronous PALM Algorithm for Nonsmooth Nonconvex Problems. arXiv preprint

arXiv:1604.00526  2016.

[6] J. Dean  G. Corrado  R. Monga  K. Chen  M. Devin  M. Mao  M. Ranzato  A. Senior  P. Tucker  K. Yang 
Q. V. Le  and A. Y. Ng. Large Scale Distributed Deep Networks. In F. Pereira  C. J. C. Burges  L. Bottou 
and K. Q. Weinberger  editors  Advances in Neural Information Processing Systems 25  pages 1223–1231.
Curran Associates  Inc.  2012.

[7] R. Ge  F. Huang  C. Jin  and Y. Yuan. Escaping from saddle points—online stochastic gradient for tensor

decomposition. In Proceedings of The 28th Conference on Learning Theory  pages 797–842  2015.

[8] S. Ghadimi  G. Lan  and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic

composite optimization. Mathematical Programming  155(1):267–305  2016.

[9] M. Hong. A distributed  asynchronous and incremental algorithm for nonconvex optimization: An admm

based approach. arXiv preprint arXiv:1412.6058  2014.

[10] X. Lian  Y. Huang  Y. Li  and J. Liu. Asynchronous Parallel Stochastic Gradient for Nonconvex Optimiza-

tion. In Advances in Neural Information Processing Systems  pages 2719–2727  2015.

[11] J. Liu  S. J. Wright  C. Ré  V. Bittorf  and S. Sridhar. An Asynchronous Parallel Stochastic Coordinate

Descent Algorithm. Journal of Machine Learning Research  16:285–322  2015.

[12] J. Liu  S. J. Wright  and S. Sridhar. An Asynchronous Parallel Randomized Kaczmarz Algorithm. arXiv

preprint arXiv:1401.4780  2014.

[13] H. Mania  X. Pan  D. Papailiopoulos  B. Recht  K. Ramchandran  and M. I. Jordan. Perturbed Iterate

Analysis for Asynchronous Stochastic Optimization. arXiv preprint arXiv:1507.06970  2015.

[14] Y. Nesterov. Introductory Lectures on Convex Optimization : A Basic Course. Applied optimization.

Kluwer Academic Publ.  Boston  Dordrecht  London  2004.

[15] Z. Peng  Y. Xu  M. Yan  and W. Yin. ARock: an Algorithmic Framework for Asynchronous Parallel

Coordinate Updates. arXiv preprint arXiv:1506.02396  2015.

[16] B. Recht  C. Re  S. Wright  and F. Niu. Hogwild: A Lock-Free Approach to Parallelizing Stochastic

Gradient Descent. In Advances in Neural Information Processing Systems  pages 693–701  2011.

[17] R. T. Rockafellar and R. J.-B. Wets. Variational Analysis  volume 317. Springer Science & Business

Media  2009.

[18] P. Tseng. On the Rate of Convergence of a Partially Asynchronous Gradient Projection Algorithm. SIAM

Journal on Optimization  1(4):603–619  1991.

[19] J. Tsitsiklis  D. Bertsekas  and M. Athans. Distributed asynchronous deterministic and stochastic gradient

optimization algorithms. IEEE Transactions on Automatic Control  31(9):803–812  Sep 1986.

[20] M. Udell  C. Horn  R. Zadeh  and S. Boyd. Generalized Low Rank Models. arXiv preprint arXiv:1410.0342 

2014.

[21] J. Woodworth and R. Chartrand. Compressed sensing recovery via nonconvex shrinkage penalties. arXiv

preprint arXiv:1504.02923  2015.

[22] Y. Xu and W. Yin. Block Stochastic Gradient Iteration for Convex and Nonconvex Optimization. SIAM

Journal on Optimization  25(3):1686–1716  2015.

[23] H. Yun  H.-F. Yu  C.-J. Hsieh  S. V. N. Vishwanathan  and I. Dhillon. NOMAD: Non-locking  Stochastic
Multi-machine Algorithm for Asynchronous and Decentralized Matrix Completion. Proc. VLDB Endow. 
7(11):975–986  July 2014.

9

,Damek Davis
Brent Edmunds
Madeleine Udell