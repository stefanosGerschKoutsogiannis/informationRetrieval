2009,Fast Image Deconvolution using Hyper-Laplacian Priors,The heavy-tailed distribution of gradients in natural scenes have proven effective priors for a range of problems such as denoising  deblurring and super-resolution. However  the use of sparse distributions makes the problem non-convex and impractically slow to solve for multi-megapixel images. In this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use hyper-Laplacian priors. We adopt an alternating minimization scheme where one of the two phases is a non-convex problem that is separable over pixels. This per-pixel sub-problem may be solved with a lookup table (LUT). Alternatively  for two specific values of α  1/2 and 2/3 an analytic solution can be found  by finding the roots of a cubic and quartic polynomial  respectively. Our approach (using either LUTs or analytic formulae) is able to deconvolve a 1 megapixel image in less than ∼3 seconds  achieving comparable quality to existing methods such as iteratively reweighted least squares (IRLS) that take ∼20 minutes. Furthermore  our method is quite general and can easily be extended to related image processing problems  beyond the deconvolution application demonstrated.,Fast Image Deconvolution

using Hyper-Laplacian Priors

Dilip Krishnan 

Dept. of Computer Science 

Courant Institute 

New York University
dilip@cs.nyu.edu

Rob Fergus 

Dept. of Computer Science 

Courant Institute 

New York University

fergus@cs.nyu.edu

Abstract

The heavy-tailed distribution of gradients in natural scenes have proven effective
priors for a range of problems such as denoising  deblurring and super-resolution.

These distributions are well modeled by a hyper-Laplacian(cid:0)p(x) ∝ e−k|x|α(cid:1)  typ-
ically with 0.5 ≤ α ≤ 0.8. However  the use of sparse distributions makes the
problem non-convex and impractically slow to solve for multi-megapixel images.
In this paper we describe a deconvolution approach that is several orders of mag-
nitude faster than existing techniques that use hyper-Laplacian priors. We adopt
an alternating minimization scheme where one of the two phases is a non-convex
problem that is separable over pixels. This per-pixel sub-problem may be solved
with a lookup table (LUT). Alternatively  for two speciﬁc values of α  1/2 and 2/3
an analytic solution can be found  by ﬁnding the roots of a cubic and quartic poly-
nomial  respectively. Our approach (using either LUTs or analytic formulae) is
able to deconvolve a 1 megapixel image in less than ∼3 seconds  achieving com-
parable quality to existing methods such as iteratively reweighted least squares
(IRLS) that take ∼20 minutes. Furthermore  our method is quite general and can
easily be extended to related image processing problems  beyond the deconvolu-
tion application demonstrated.

1 Introduction

Natural image statistics are a powerful tool in image processing  computer vision and computational
photography. Denoising [14]  deblurring [3]  transparency separation [11] and super-resolution [20] 
are all tasks that are inherently ill-posed. Priors based on natural image statistics can regularize these
problems to yield high-quality results. However  digital cameras now have sensors that record im-
ages with tens of megapixels (MP)  e.g. the latest Canon DSLRs have over 20MP. Solving the above
tasks for such images in a reasonable time frame (i.e. a few minutes or less)  poses a severe challenge
to existing algorithms. In this paper we focus on one particular problem: non-blind deconvolution 
and propose an algorithm that is practical for very large images while still yielding high quality
results.

Numerous deconvolution approaches exist  varying greatly in their speed and sophistication. Simple
ﬁltering operations are very fast but typically yield poor results. Most of the best-performing ap-
proaches solve globally for the corrected image  encouraging the marginal statistics of a set of ﬁlter
outputs to match those of uncorrupted images  which act as a prior to regularize the problem. For
these methods  a trade-off exists between accurately modeling the image statistics and being able to
solve the ensuing optimization problem efﬁciently. If the marginal distributions are assumed to be
Gaussian  a closed-form solution exists in the frequency domain and FFTs can be used to recover the
image very quickly. However  real-world images typically have marginals that are non-Gaussian  as
shown in Fig. 1  and thus the output is often of mediocre quality. A common approach is to assume
the marginals have a Laplacian distribution. This allows a number of fast ℓ1 and related TV-norm
methods [17  22] to be deployed  which give good results in a reasonable time. However  studies

1

 

0

−5

y
t
i
l
i

b
a
b
o
r
P

 

2

g
o

l

−10

 

−15
−100

Empirical
Gaussian (α=2)
Laplacian (α=1)
Hyper−Laplacian (α=0.66)

−80

−60

−40

−20

0

Gradient

20

40

60

80

100

Figure 1: A hyper-Laplacian with exponent α = 2/3 is a better model of image gradients than
a Laplacian or a Gaussian. Left: A typical real-world scene. Right: The empirical distribution
of gradients in the scene (blue)  along with a Gaussian ﬁt (cyan)  a Laplacian ﬁt (red) and a hyper-
Laplacian with α = 2/3 (green). Note that the hyper-Laplacian ﬁts the empirical distribution closely 
particularly in the tails.

of real-world images have shown the marginal distributions have signiﬁcantly heavier tails than a
Laplacian  being well modeled by a hyper-Laplacian [4  10  18]. Although such priors give the best
quality results  they are typically far slower than methods that use either Gaussian or Laplacian pri-
ors. This is a direct consequence of the problem becoming non-convex for hyper-Laplacians with
α < 1  meaning that many of the fast ℓ1 or ℓ2 tricks are no longer applicable. Instead  standard
optimization methods such as conjugate gradient (CG) must be used. One variant that works well
in practice is iteratively reweighted least squares (IRLS) [19] that solves a series of weighted least-
squares problems with CG  each one an ℓ2 approximation to the non-convex problem at the current
point. In both cases  typically hundreds of CG iterations are needed  each involving an expensive
convolution of the blur kernel with the current image estimate.

In this paper we introduce an efﬁcient scheme for non-blind deconvolution of images using a hyper-
Laplacian image prior for 0 < α ≤ 1. Our algorithm uses an alternating minimization scheme where
the non-convex part of the problem is solved in one phase  followed by a quadratic phase which can
be efﬁciently solved in the frequency domain using FFTs. We focus on the ﬁrst phase where at each
pixel we are required to solve a non-convex separable minimization. We present two approaches to
solving this sub-problem. The ﬁrst uses a lookup table (LUT); the second is an analytic approach
speciﬁc to two values of α. For α = 1/2 the global minima can be determined by ﬁnding the
roots of a cubic polynomial analytically. In the α = 2/3 case  the polynomial is a quartic whose
roots can also be found efﬁciently in closed-form. Both IRLS and our approach solve a series of
approximations to the original problem. However  in our method each approximation is solved by
alternating between the two phases above a few times  thus avoiding the expensive CG descent used
by IRLS. This allows our scheme to operate several orders of magnitude faster. Although we focus
on the problem of non-blind deconvolution  it would be straightforward to adapt our algorithm to
other related problems  such as denoising or super-resolution.

1.1 Related Work
Hyper-Laplacian image priors have been used in a range of settings: super-resolution [20]  trans-
parency separation [11] and motion deblurring [9]. In work directly relevant to ours  Levin et al. [10]
and Joshi et al. [7] have applied them to non-blind deconvolution problems using IRLS to solve for
the deblurred image. Other types of sparse image prior include: Gaussian Scale Mixtures (GSM)
[21]  which have been used for image deblurring [3] and denoising [14] and student-T distributions
for denoising [25  16]. With the exception of [14]  these methods use CG and thus are slow.

The alternating minimization that we adopt is a common technique  known as half-quadratic split-
ting  originally proposed by Geman and colleagues [5  6]. Recently  Wang et al. [22] showed how it
could be used with a total-variation (TV) norm to deconvolve images. Our approach is closely re-
lated to this work: we also use a half-quadratic minimization  but the per-pixel sub-problem is quite
different. With the TV norm it can be solved with a straightforward shrinkage operation. In our
work  as a consequence of using a sparse prior  the problem is non-convex and solving it efﬁciently
is one of the main contributions of this paper.
Chartrand [1  2] has introduced non-convex compressive sensing  where the usual ℓ1 norm on the
signal to be recovered is replaced with a ℓp quasi-norm  where p < 1. Similar to our approach  a
splitting scheme is used  resulting in a non-convex per-pixel sub-problem. To solve this  a Huber

2

approximation (see [1]) to the quasi-norm is used  allowing the derivation of a generalized shrinkage
operator to solve the sub-problem efﬁciently. However  this approximates the original sub-problem 
unlike our approach.

2 Algorithm
We now introduce the non-blind deconvolution problem. x is the original uncorrupted linear
grayscale image of N pixels; y is an image degraded by blur and/or noise  which we assume to
be produced by convolving x with a blur kernel k and adding zero mean Gaussian noise. We as-
sume that y and k are given and seek to reconstruct x. Given the ill-posed nature of the task  we
regularize using a penalty function |.|α that acts on the output of a set of ﬁlters f1  . . .   fj applied
to x. A weighting term λ controls the strength of the regularization. From a probabilistic perspec-
tive  we seek the MAP estimate of x: p(x|y  k) ∝ p(y|x  k)p(x)  the ﬁrst term being a Gaussian
likelihood and second being the hyper-Laplacian image prior. Maximizing p(x|y  k) is equivalent
to minimizing the cost − log p(x|y  k):
λ
2

(x ⊕ k − y)2

i +

min

(1)

N

J

x

Xj=1

|(x ⊕ fj)i|α


Xi=1




where i is the pixel index  and ⊕ is the 2-dimensional convolution operator. For simplicity  we
use two ﬁrst-order derivative ﬁlters f1 = [1 -1] and f2 = [1 -1]T   although additional ones can
easily be added (e.g. learned ﬁlters [13  16]  or higher order derivatives). For brevity  we denote
F j
i x ≡ (x ⊕ fj)i for j = 1  ..  J.
Using the half-quadratic penalty method [5  6  22]  we now introduce auxiliary variables w1
(together denoted as w) at each pixel that allow us to move the F j
giving a new cost function:

i and w2
i
i x terms outside the |.|α expression 
(2)

β

(x ⊕ k − y)2

i +

2 (cid:0)kF 1

i x − w1
i k2

2 + kF 2

i k2
i x − w2
2(cid:1) + |w1

where β is a weight that we will vary during the optimization  as described in Section 2.3. As
β → ∞  the solution of Eqn. 2 converges to that of Eqn. 1. Minimizing Eqn. 2 for a ﬁxed β can
be performed by alternating between two steps  one where we solve for x  given values of w and
vice-versa. The novel part of our algorithm lies in the w sub-problem  but ﬁrst we brieﬂy describe
the x sub-problem and its straightforward solution.
2.1 x sub-problem
Given a ﬁxed value of w from the previous iteration  Eqn. 2 is quadratic in x. The optimal x is thus:

min

x w Xi

(cid:18) λ

2

i |α + |w2

i |α(cid:19)

λ
β

λ
β

(cid:18)F 1T

K T K(cid:19) x = F 1T

F 1 + F 2T

w1 + F 2T

K T y

F 2 +

w2 +

x = F −1(cid:18) F(F 1)∗ ◦ F(w1) + F(F 2)∗ ◦ F(w2) + (λ/β)F(K)∗ ◦ F(y)
F(F 1)∗ ◦ F(F 1) + F(F 2)∗ ◦ F(F 2) + (λ/β)F(K)∗ ◦ F(K)(cid:19)

(3)
where Kx ≡ x ⊕ k. Assuming circular boundary conditions  we can apply 2D FFT’s which diago-
nalize the convolution matrices F 1  F 2  K  enabling us to ﬁnd the optimal x directly:
(4)
where ∗ is the complex conjugate and ◦ denotes component-wise multiplication. The division is also
performed component-wise. Solving Eqn. 4 requires only 3 FFT’s at each iteration since many of
the terms can be precomputed. The form of this sub-problem is identical to that of [22].
2.2 w sub-problem
Given a ﬁxed x  ﬁnding the optimal w consists of solving 2N independent 1D problems of the form:

(w − v)2
i x. We now describe two approaches to ﬁnding w∗.

w |w|α +

w∗ = arg min

β
2

where v ≡ F j
2.2.1 Lookup table

(5)

For a ﬁxed value of α  w∗ in Eqn. 5 only depends on two variables  β and v  hence can easily be
tabulated off-line to form a lookup table. We numerically solve Eqn. 5 for 10  000 different values
of v over the range encountered in our problem (−0.6 ≤ v ≤ 0.6). This is repeated for different β
values  namely integer powers of √2 between 1 and 256. Although the LUT gives an approximate
solution  it allows the w sub-problem to be solved very quickly for any α > 0.

3

2.2.2 Analytic solution

For some speciﬁc values of α  it is possible to derive exact analytical solutions to the w sub-problem.
For α = 2  the sub-problem is quadratic and thus easily solved. If α = 1  Eqn. 5 reduces to a 1-D
shrinkage operation [22]. For some special cases of 1 < α < 2  there exist analytic solutions [26].
Here  we address the more challenging case of α < 1 and we now describe a way to solve Eqn. 5
for two special cases of α = 1/2 and α = 2/3. For non-zero w  setting the derivative of Eqn. 5 w.r.t
w to zero gives:

(6)

For α = 1/2  this becomes  with successive simpliﬁcation:

α|w|α−1sign(w) + β(w − v) = 0
|w|−1/2sign(w) + 2β(w − v) = 0
|w|−1 = 4β2(v − w)2
w3 − 2vw2 + v2w − sign(w)/4β2 = 0

(7)
(8)
(9)
At ﬁrst sight Eqn. 9 appears to be two different cubic equations with the ±1/4β2 term  however we
need only consider one of these as v is ﬁxed and w∗ must lie between 0 and v. Hence we can replace
sign(w) with sign(v) in Eqn. 9:

w3 − 2vw2 + v2w − sign(v)/4β2 = 0

For the case α = 2/3  using a similar derivation  we arrive at:
w4 − 3vw3 + 3v2w2 − v3w +

8
27β3 = 0

(10)

(11)

there being no sign(w) term as it conveniently cancels in this case. Hence w∗  the solution of Eqn. 5 
is either 0 or a root of the cubic polynomial in Eqn. 10 for α = 1/2  or equivalently a root of the
quartic polynomial in Eqn. 10 for α = 2/3. Although it is tempting to try the same manipulation
for α = 3/4  this results in a 5th order polynomial  which can only be solved numerically.
Finding the roots of the cubic and quartic polynomials: Analytic formulae exist for the roots
of cubic and quartic polynomials [23  24] and they form the basis of our approach  as detailed in
Algorithms 2 and 3. In both the cubic and quartic cases  the computational bottleneck is the cube
root operation. An alternative way of ﬁnding the roots of the polynomials Eqn. 10 and Eqn. 11 is
to use a numerical root-ﬁnder such as Newton-Raphson. In our experiments  we found Newton-
Raphson to be slower and less accurate than either the analytic method or the LUT approach (see
[8] for futher details).
Selecting the correct roots: Given the roots of the polynomial  we need to determine which one
corresponds to the global minima of Eqn. 5. When α = 1/2  the resulting cubic equation can have:
(a) 3 imaginary roots; (b) 2 imaginary roots and 1 real root  or (c) 3 real roots. In the case of (a) 
the |w|α term means Eqn. 5 has positive derivatives around 0 and the lack of real roots implies the
derivative never becomes negative  thus w∗ = 0. For (b)  we need to compare the costs of the single
real root and w = 0  an operation that can be efﬁciently performed using Eqn. 13 below. In (c)
we have 3 real roots. Examining Eqn. 7 and Eqn. 8  we see that the squaring operation introduces
a spurious root above v when v > 0  and below v when v < 0. This root can be ignored  since
w∗ must lie between 0 and v. The cost function in Eqn. 5 has a local maximum near 0 and a local
minimum between this local maximum and v. Hence of the 2 remaining roots  the one further from
0 will have a lower cost. Finally  we need to compare the cost of this root with that of w = 0 using
Eqn. 13.

We can use similar arguments for the α = 2/3 case. Here we can potentially have: (a) 4 imaginary
roots  (b) 2 imaginary and 2 real roots  or (c) 4 real roots. In (a)  w∗ = 0 is the only solution. For
(b)  we pick the larger of the 2 real roots and compare the costs with w = 0 using Eqn. 13  similar
to the case of 3 real roots for the cubic. Case (c) never occurs: the ﬁnal quartic polynomial Eqn. 11
was derived with a cubing operation from the analytic derivative. This introduces 2 spurious roots
into the ﬁnal solution  both of which are imaginary  thus only cases (a) and (b) are possible.

In both the cubic and quartic cases  we need an efﬁcient way to pick between w = 0 and a real root
that is between 0 and v. We now describe a direct mechanism for doing this which does not involve
the expensive computation of the cost function in Eqn. 51.
Let r be the non-zero real root. 0 must be chosen if it has lower cost in Eqn. 5. This implies:

1This requires the calculation of a fractional power  which is slow  particularly if α = 2/3.

4

|r|α +
sign(r)|r|α−1 +

β
2
β
2

βv2
2

(r − v)2 >
(r − 2v) ≶ 0   r ≶ 0

(12)

Since we are only considering roots of the polynomial  we can use Eqn. 6 to eliminate sign(r)|r|α−1
from Eqn. 6 and Eqn. 12  yielding the condition:

r ≶ 2v

  v ≷ 0

(13)

(α − 1)
(α − 2)

since sign(r) = sign(v). So w∗ = r if r is between 2v/3 and v in the α = 1/2 case or between
v/2 and v in the α = 2/3 case. Otherwise w∗ = 0. Using this result  picking w∗ can be efﬁciently
coded  e.g. lines 12–16 of Algorithm 2. Overall  the analytic approach is slower than the LUT  but
it gives an exact solution to the w sub-problem.

2.3 Summary of algorithm

We now give the overall algorithm using a LUT for the w sub-problem. As outlined in Algorithm
1 below  we minimize Eqn. 2 by alternating the x and w sub-problems T times  before increasing
the value of β and repeating. Starting with some small value β0 we scale it by a factor βInc until it
exceeds some ﬁxed value βMax. In practice  we ﬁnd that a single inner iteration sufﬁces (T = 1) 
although more can sometimes be needed when β is small.

Algorithm 1 Fast image deconvolution using hyper-Laplacian priors
Require: Blurred image y  kernel k  regularization weight λ  exponent α (¿0)
Require: β regime parameters: β0  βInc  βMax
Require: Number of inner iterations T .
1: β = β0  x = y
2: Precompute constant terms in Eqn. 4.
3: while β < βMax do
4:
5:
6:
7:
8:
9:
10: end while
11: return Deconvolved image x

Given x  solve Eqn. 5 for all pixels using a LUT to give w
Given w  solve Eqn. 4 to give x

iter = 0
for i = 1 to T do

end for
β = βInc · β

As with any non-convex optimization problem  it is difﬁcult to derive any guarantees regarding the
convergence of Algorithm 1. However  we can be sure that the global optimum of each sub-problem
will be found  given the ﬁxed x and w from the previous iteration. Like other methods that use
this form of alternating minimization [5  6  22]  there is little theoretical guidance for setting the β
schedule. We ﬁnd that the simple scheme shown in Algorithm 1 works well to minimize Eqn. 2 and
its proxy Eqn. 1. The experiments in Section 3 show our scheme achieves very similar SNR levels
to IRLS  but at a greatly lower computational cost.

3 Experiments
We evaluate the deconvolution performance of our algorithm on images  comparing them to numer-
ous other methods: (i) ℓ2 (Gaussian) prior on image gradients; (ii) Lucy-Richardson [15]; (iii) the
algorithm of Wang et al. [22] using a total variation (TV) norm prior and (iv) a variant of [22] using
an ℓ1 (Laplacian) prior; (v) the IRLS approach of Levin et al. [10] using a hyper-Laplacian prior
with α = 1/2  2/3  4/5. Note that only IRLS and our method use a prior with α < 1. For the
IRLS scheme  we used the implementation of [10] with default parameters  the only change being
the removal of higher order derivative ﬁlters to enable a direct comparison with other approaches.
Note that IRLS and ℓ2 directly minimize Eqn. 1  while our method  and the TV and ℓ1 approaches of
[22] minimize the cost in Eqn. 2  using T = 1  β0 = 1  βInc = 2√2  βMax = 256. In our approach 
we use α = 1/2 and α = 2/3  and compare the performance of the LUT and analytic methods as
well. All runs were performed with multithreading enabled (over 4 CPU cores).

5

We evaluate the algorithms using a set of blurry images  created in the following way. 7 in-focus
grayscale real-world images were downloaded from the web. They were then blurred by real-world
camera shake kernels from [12]. 1% Gaussian noise was added  followed by quantization to 255
discrete values.
In any practical deconvolution setting the blur kernel is never perfectly known.
Therefore  the kernel passed to the algorithms was a minor perturbation of the true kernel  to mimic
kernel estimation errors. In experiments with non-perturbed kernels (not shown)  the results are
similar to those in Tables 3 and 1 but with slightly higher SNR levels. See Fig. 2 for an example of a
kernel from [12] and its perturbed version. Our evaluation metric was the SNR between the original
image ˆx and the deconvolved output x  deﬁned as 10 log10
  µ(ˆx) being the mean of ˆx.
In Table 1 we compare the algorithms on 7 different images  all blurred with the same 19×19 kernel.
For each algorithm we exhaustively searched over different regularization weights λ to ﬁnd the value
that gave the best SNR performance  as reported in the table. In Table 3 we evaluate the algorithms
with the same 512×512 image blurred by 8 different kernels (from [12]) of varying size. Again 
the optimal value of λ for each kernel/algorithm combination was chosen from a range of values
based on SNR performance. Table 2 shows the running time of several algorithms on images up
to 3072×3072 pixels. Figure 2 shows a larger 27×27 blur being deconvolved from two example
images  comparing the output of different methods.

kˆx−µ(ˆx)k2

kˆx−xk2

The tables and ﬁgures show our method with α = 2/3 and IRLS with α = 4/5 yielding higher
quality results than other methods. However  our algorithm is around 70 to 350 times faster than
IRLS depending on whether the analytic or LUT method is used. This speedup factor is independent
of image size  as shown by Table 2. The ℓ1 method of [22] is the best of the other methods  being
of comparable speed to ours but achieving lower SNR scores. The SNR results for our method are
almost the same whether we use LUTs or analytic approach. Hence  in practice  the LUT method is
preferred  since it is approximately 5 times faster than the analytic method and can be used for any
value of α.

Image

#
1
2
3
4
5
6
7

Av. SNR gain
Av. Time

(secs)

Blurry
6.42
10.73
12.45
8.51
12.74
10.85
11.76

ℓ2

14.13
17.56
19.30
16.02
16.59
15.46
17.40
6.14
79.85

Lucy
12.54
15.15
16.68
14.27
13.28
12.00
15.22
3.67
1.55

TV
15.87
19.37
21.83
17.66
19.34
17.13
18.58
8.05
0.66

ℓ1

16.18
19.86
22.77
18.02
20.25
17.59
18.85
8.58
0.75

IRLS
α=1/2
14.61
18.43
21.53
16.34
19.12
15.59
17.08
7.03
354

IRLS
α=2/3
15.45
19.37
22.62
17.31
19.99
16.58
17.99
7.98
354

IRLS
α=4/5
16.04
20.00
22.95
17.98
20.20
17.04
18.61
8.48
354

Ours
Ours
α=2/3
α=1/2
16.44
16.05
20.26
19.78
23.27
23.26
18.17
17.70
21.28
21.00
17.89
17.79
18.96
18.58
8.93
8.71
L:1.01
L:1.00
A:5.27 A:4.08

Table 1: Comparison of SNRs and running time of 9 different methods for the deconvolution of
7 576×864 images  blurred with the same 19×19 kernel. L=Lookup table  A=Analytic. The best
performing algorithm for each kernel is shown in bold. Our algorithm with α = 2/3 beats IRLS
with α = 4/5  as well as being much faster. On average  both these methods outperform ℓ1  demon-
strating the beneﬁts of a sparse prior.

Ours (LUT) Ours (Analytic)

Image
size

256×256
512×512
1024×1024
2048×2048
3072×3072

ℓ1

0.24
0.47
2.34
9.34
22.40

IRLS
α=4/5
78.14
256.87
1281.3
4935

-

α=2/3
0.42
0.55
2.78
10.72
24.07

α=2/3

0.7
2.28
10.87
44.64
100.42

Table 2: Run-times of different methods for a range of image sizes  using a 13×13 kernel. Our LUT
algorithm is more than 100 times faster than the IRLS method of [10].

4 Discussion
We have described an image deconvolution scheme that is fast  conceptually simple and yields
high quality results. Our algorithm takes a novel approach to the non-convex optimization prob-

6

Original

  L

2

SNR=14.89
t=0.1

  L

1

SNR=18.10
t=0.8

Blurred
SNR=7.31

Original

Ours  α=2/3
SNR=18.96
t=1.2

  L

2

SNR=11.58
t=0.1

IRLS  α=4/5
SNR=19.05
t=483.9

  L

1

SNR=13.64
t=0.8

Blurred
SNR=2.64

Ours  α=2/3
SNR=14.15
t=1.2

IRLS  α=4/5
SNR=14.28
t=482.1

Figure 2: Crops from two images (#1 & #5) being deconvolved by 4 different algorithms  including
ours using a 27×27 kernel (#7). In the bottom left inset  we show the original kernel from [12]
(lower) and the perturbed version provided to the algorithms (upper)  to make the problem more
realistic. This ﬁgure is best viewed on screen  rather than in print.

7

Kernel
# / size
#1: 13×13
#2: 15×15
#3: 17×17
#4: 19×19
#5: 21×21
#6: 23×23
#7: 27×27
#8: 41×41
Av. SNR gain
Av. Time

(sec)

Blurry
10.69
11.28
8.93
10.13
9.26
7.87
6.76
6.00

ℓ2

17.22
16.14
14.94
15.27
16.55
15.40
13.81
12.80
6.40
57.44

Lucy
14.49
13.81
12.16
12.38
13.60
13.32
11.55
11.19
3.95
1.22

TV
19.21
17.94
16.50
16.83
18.72
17.01
15.42
13.53
8.03
0.50

ℓ1

19.41
18.29
16.86
17.25
18.83
17.42
15.69
13.62
8.31
0.55

IRLS
α=1/2
17.20
16.17
15.34
15.97
17.23
15.66
14.59
12.68
6.74
271

IRLS
α=2/3
18.22
17.26
16.36
16.98
18.36
16.73
15.68
13.60
7.78
271

IRLS
α=4/5
18.87
18.02
16.99
17.57
18.88
17.40
16.38
14.25
8.43
271

Ours
Ours
α=2/3
α=1/2
19.66
19.36
18.64
18.14
17.25
16.73
17.67
17.29
19.34
19.11
17.77
17.26
16.29
15.92
13.68
13.73
8.67
8.33
L:0.81
L:0.78
A:2.15 A:2.23

Table 3: Comparison of SNRs and running time of 9 different methods for the deconvolution of a
512×512 image blurred by 7 different kernels. L=Lookup table  A=Analytic. Our algorithm beats
all other methods in terms of quality  with the exception of IRLS on the largest kernel size. However 
our algorithm is far faster than IRLS  being comparable in speed to the ℓ1 approach.

lem arising from the use of a hyper-Laplacian prior  by using a splitting approach that allows the
non-convexity to become separable over pixels. Using a LUT to solve this sub-problem allows for
orders of magnitude speedup in the solution over existing methods. Our Matlab implementation is
available online at http://cs.nyu.edu/˜dilip/wordpress/?page_id=122.
A potential drawback to our method  common to the TV and ℓ1 approaches of [22]  is its use of
frequency domain operations which assume circular boundary conditions  something not present in
real images. These give rise to boundary artifacts which can be overcome to some extend with edge
tapering operations. However  our algorithm is suitable for very large images where the boundaries
are a small fraction of the overall image.

Although we focus on deconvolution  our scheme can be adapted to a range of other problems which
rely on natural image statistics. For example  by setting k = 1 the algorithm can be used to denoise 
or if k is a defocus kernel it can be used for super-resolution. The speed offered by our algorithm
makes it practical to perform these operations on the multi-megapixel images from modern cameras.

Algorithm 2: Solve Eqn. 5 for α = 1/2

Algorithm 3: Solve Eqn. 5 for α = 2/3

Require: Target value v  Weight β
1: ǫ = 10−6
2: {Compute intermediary terms m  t1  t2  t3}
3: m = −sign(v)/4β 2
4: t1 = 2v/3
5: t2 = 3p−27m − 2v3 + 3√3√27m2 + 4mv3
6: t3 = v2/t2
7: {Compute 3 roots  r1  r2  r3:}
8: r1 = t1 + 1/(3 · 21/3) · t2 + 21/3/3 · t3
9: r2 = t1 − (1 − √3i)/(6 · 21/3) · t2
− (1 + √3i)/(3 · 22/3) · t3
10: r3 = t1 − (1 + √3i)/(6 · 21/3) · t2
− (1 − √3i)/(3 · 22/3) · t3
11: {Pick global minimum from (0  r1  r2  r3)}
12: r = [r1  r2  r3]
13: c1 = (abs(imag(r)) < ǫ) {Root must be real}
14: c2 = real(r)sign(v) > (2/3 · abs(v))
15: c3 = real(r)sign(v) < abs(v) {Root < v}
16: w∗= max((c1&c2&c3)real(r)sign(v))sign(v)
return w∗

{Root must obey bound of Eqn. 13}

Require: Target value v  Weight β
1: ǫ = 10−6
2: {Compute intermediary terms m  t1  . . .   t7:}
3: m = 8/(27β 3)
4: t1 = −9/8 · v2
5: t2 = v3/4
6: t3 = −1/8 · mv2
7: t4 = −t3/2 +p−m3/27 + m2v4/256
8: t5 = 3√t4
9: t6 = 2(−5/18 · t1 + t5 + m/(3 · t5))
10: t7 = pt1/3 + t6
11: {Compute 4 roots  r1  r2  r3  r4:}
12: r1 = 3v/4 + (t7 +p−(t1 + t6 + t2/t7))/2
13: r2 = 3v/4 + (t7 −p−(t1 + t6 + t2/t7))/2
14: r3 = 3v/4 + (−t7 +p−(t1 + t6 − t2/t7))/2
15: r4 = 3v/4 + (−t7 −p−(t1 + t6 − t2/t7))/2
16: {Pick global minimum from (0  r1  r2  r3  r4)}
17: r = [r1  r2  r3  r4]
18: c1 = (abs(imag(r)) < ǫ) {Root must be real}
19: c2 = real(r)sign(v) > (1/2 · abs(v))
20: c3 = real(r)sign(v) < abs(v) {Root < v}
21: w∗ = max((c1&c2&c3)real(r)sign(v))sign(v)
return w∗

{Root must obey bound in Eqn. 13}

8

References

[1] R. Chartrand. Fast algorithms for nonconvex compressive sensing: Mri reconstruction from

very few data. In IEEE International Symposium on Biomedical Imaging (ISBI)  2009.

[2] R. Chartrand and V. Staneva. Restricted isometry properties and nonconvex compressive sens-

ing. Inverse Problems  24:1–14  2008.

[3] R. Fergus  B. Singh  A. Hertzmann  S. T. Roweis  and W. Freeman. Removing camera shake

from a single photograph. ACM TOG (Proc. SIGGRAPH)  25:787–794  2006.

[4] D. Field. What is the goal of sensory coding? Neural Computation  6:559–601  1994.
[5] D. Geman and G. Reynolds. Constrained restoration and recovery of discontinuities. PAMI 

14(3):367–383  1992.

[6] D. Geman and C. Yang. Nonlinear image recovery with half-quadratic regularization. PAMI 

4:932–946  1995.

[7] N. Joshi  L. Zitnick  R. Szeliski  and D. Kriegman. Image deblurring and denoising using color

priors. In CVPR  2009.

[8] D. Krishnan and R. Fergus. Fast image deconvolution using hyper-laplacian priors  supple-

mentary material. NYU Tech. Rep. 2009  2009.

[9] A. Levin. Blind motion deblurring using image statistics. In NIPS  2006.
[10] A. Levin  R. Fergus  F. Durand  and W. Freeman. Image and depth from a conventional camera

with a coded aperture. ACM TOG (Proc. SIGGRAPH)  26(3):70  2007.

[11] A. Levin and Y. Weiss. User assisted separation of reﬂections from a single image using a

sparsity prior. PAMI  29(9):1647–1654  Sept 2007.

[12] A. Levin  Y. Weiss  F. Durand  and W. T. Freeman. Understanding and evaluating blind decon-

volution algorithms. In CVPR  2009.

[13] S. Osindero  M. Welling  and G. Hinton. Topographic product models applied to natural scene

statistics. Neural Computation  1995.

[14] J. Portilla  V. Strela  M. J. Wainwright  and E. P. Simoncelli. Image denoising using a scale
mixture of Gaussians in the wavelet domain. IEEE TIP  12(11):1338–1351  November 2003.

[15] W. Richardson. Bayesian-based iterative method of image restoration. 62:55–59  1972.
[16] S. Roth and M. J. Black. Fields of Experts: A Framework for Learning Image Priors. In CVPR 

volume 2  pages 860–867  2005.

[17] L. Rudin  S. Osher  and E. Fatemi. Nonlinear total variation based noise removal algorithms.

Physica D  60:259–268  1992.

[18] E. Simoncelli and E. H. Adelson. Noise removal via bayesian wavelet coring. In ICIP  pages

379–382  1996.

[19] C. V. Stewart. Robust parameter estimation in computer vision. SIAM Reviews  41(3):513–537 

Sept. 1999.

[20] M. F. Tappen  B. C. Russell  and W. T. Freeman. Exploiting the sparse derivative prior for

super-resolution and image demosaicing. In SCTV  2003.

[21] M. Wainwright and S. Simoncelli. Scale mixtures of gaussians and teh statistics of natural

images. In NIPS  pages 855–861  1999.

[22] Y. Wang  J. Yang  W. Yin  and Y. Zhang. A new alternating minimization algorithm for total

variation image reconstruction. SIAM J. Imaging Sciences  1(3):248–272  2008.

[23] E. W. Weisstein.

CubicFormula.html.

Cubic

formula.

http://mathworld.wolfram.com/

[24] E. W. Weisstein.

Quartic equation.

QuarticEquation.html.

http://mathworld.wolfram.com/

[25] M. Welling  G. Hinton  and S. Osindero. Learning sparse topographic representations with

products of student-t distributions. In NIPS  2002.

[26] S. Wright  R. Nowak  and M. Figueredo. Sparse reconstruction by separable approximation.

IEEE Trans. Signal Processing  page To appear  2009.

9

,José Bento
Nate Derbinsky
Javier Alonso-Mora
Jonathan Yedidia
Dongsheng Li
Chao Chen
Wei Liu
Tun Lu
Ning Gu
Stephen Chu