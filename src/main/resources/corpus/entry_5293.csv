2011,Structure Learning for Optimization,We describe a family of global optimization procedures that automatically decompose optimization problems into smaller loosely coupled problems  then combine the solutions of these with message passing algorithms.  We show empirically that these methods excel in avoiding local minima and produce better solutions with fewer function evaluations than existing global optimization methods.  To develop these methods  we introduce a notion of coupling between variables of optimization that generalizes the notion of coupling that arises from factoring functions into terms that involve small subsets of the variables. It therefore subsumes the notion of independence between random variables in statistics  sparseness of the Hessian in nonlinear optimization  and the generalized distributive law. Despite being more general  this notion of coupling is easier to verify empirically -- making structure estimation easy -- yet it allows us to migrate well-established inference methods on graphical models to the setting of global optimization.,Structure Learning for Optimization

Shulin (Lynn) Yang

Department of Computer Science

University of Washington

Seattle  WA 98195

yang@cs.washington.edu

Ali Rahimi
Red Bow Labs

Berkeley  CA 94704

ali@redbowlabs.com

Abstract

We describe a family of global optimization procedures that automatically decom-
pose optimization problems into smaller loosely coupled problems. The solutions
of these are subsequently combined with message passing algorithms. We show
empirically that these methods produce better solutions with fewer function eval-
uations than existing global optimization methods. To develop these methods  we
introduce a notion of coupling between variables of optimization. This notion
of coupling generalizes the notion of independence between random variables in
statistics  sparseness of the Hessian in nonlinear optimization  and the general-
ized distributive law. Despite its generality  this notion of coupling is easier to
verify empirically  making structure estimation easy  while allowing us to migrate
well-established inference methods on graphical models to the setting of global
optimization.

Introduction

1
We consider optimization problems where the objective function is costly to evaluate and may be
accessed only by evaluating it at requested points. In this setting  the function is a black box  and
have no access to its derivative or its analytical structure. We propose solving such optimization
problems by ﬁrst estimating the internal structure of the black box function  then optimizing the
function with message passing algorithms that take advantage of this structure. This lets us solve
global optimization problems as a sequence of small grid searches that are coordinated by dynamic
programming. We are motivated by the problem of tuning the parameters of computer programs to
improve their accuracy or speed. For the programs that we consider  it can take several minutes to
evaluate these performance measures under a particular parameter setting.
Many optimization problems exhibit only loose coupling between many of the variables of opti-
mization. For example  to tune the parameters of an audio-video streaming program  the parameters
of the audio codec could conceivably be tuned independently of the parameters of the video codec.
Similarly  to tune the networking component that glues these codecs together it sufﬁces to consider
only a few parameters of the codecs  such as their output bit-rate. Such notions of conditional decou-
pling are conveniently depicted in a graphical form that represents the way the objective function
factors into a sum or product of terms each involving only a small subset of the variables. This
factorization structure can then be exploited by optimization procedures such as dynamic program-
ming on trees or junction trees. Unfortunately  the factorization structure of a function is difﬁcult to
estimate from function evaluation queries only.
We introduce a notion of decoupling that can be more readily estimated from function evaluations.
At the same time  this notion of decoupling is more general than the factorization notion of decou-
pling in that functions that do not factorize may still exhibit this type of decoupling. We say that two
variables are decoupled if the optimal setting of one variable does not depend on the setting of the
other. This is formalized below in a way that parallels the notion of conditional decoupling between
random variables in statistics. This parallel allows us to migrate much of the machinery developed

1

for inference on graphical models to global optimization . For example  decoupling can be visual-
ized with a graphical model whose semantics are similar to those of a Markov network. Analogs of
the max-product algorithm on trees  the junction tree algorithm  and loopy belief propagation can be
readily adapted to global optimization. We also introduce a simple procedure to estimate decoupling
structure.
The resulting recipe for global optimization is to ﬁrst estimate the decoupling structure of the objec-
tive function  then to optimize it with a message passing algorithm that utilises this structure. The
message passing algorithm relies on a simple grid search to solve the sub-problems it generates. In
many cases  using the same number of function evaluations  this procedure produces solutions with
objective values that improve over those produced by existing global optimizers by as much as 10%.
This happens because knowledge of the independence structure allows this procedure to explore the
objective function only along directions that cause the function to vary  and because the grid search
that solves the sub-problems does not get stuck in local minima.
2 Related work
The idea of estimating and exploiting loose coupling between variables of optimization appears
implicitly in Quasi-Newton methods that numerically estimate the Hessian matrix  such as BFGS
(Nocedal & Wright  2006  Chap. 6). Indeed  the sparsity pattern of the Hessian indicates the pairs
of terms that do not interact with each other in a second-order approximation of the function. This
is strictly a less powerful notion of coupling than the factorization model  which we argue below  is
in turn less powerful than our notion of decoupling.
Others have proposed approximating the objective function while simultaneously optimizing over
it Srinivas et al. (2010). The procedure we develop here seeks only to approximate decoupling
structure of the function  a much simpler task to carry out accurately.
A similar notion of decoupling has been explored in the decision theory literature Keeney & Raiffa
(1976); Bacchus & Grove (1996)  where decoupling was used to reason about preferences and util-
ities during decision making. In contrast  we use decoupling to solve black-box optimization prob-
lems and present a practical algorithm to estimate the decoupling structure.
3 Decoupling between variables of optimization
A common way to minimize an objective function over many variables is to factorize it into terms 
each of which involves only a small subset of the variables Aji & McEliece (2000). Such a repre-
sentation  if it exists  can be optimized via a sequence of small optimization problems with dynamic
programming. This insight motivates message passing algorithms for inference on graphical mod-
els. For example  rather than minimizing the function f1(x  y  z) = g1(x  y) + g2(y  z) over its three
variables simultaneously  one can compute the function g3(y) = minz g2(y  z)  then the function
g4(x) = miny g1(x  y) + g3(y)  and ﬁnally minimizing g4 over x. A similar idea works for the
function f2(x  y  z) = g1(x  y)g2(y  z) and indeed  whenever the operator that combines the factors
is associative  commutative  and allows the “min” operator to distribute over it.
However  it is not necessary for a function to factorize for it to admit a simple dynamic programming
procedure. For example  a factorization for the function f3(x  y  z) = x2y2z2 + x2 + y2 + z2 is
elusive  yet the arguments of f3 are decoupled in the sense that the setting of any two variables does
not affect the optimal setting of the third. For example  argminx f3(x  y0  z0) is always x = 0  and
similarly for y and z. This decoupling allows us to optimize over the variables separately. This is
not a trivial property. For example  the function f4(x  y  z) = (x  y)2 + (y  z)2 exhibits no
such decoupling between x and y because the minimizer of argminx f4(x  y0  z0) is y0  which is
obviously a function of the second argument of f. The following deﬁnition formalizes this concept:
Deﬁnition 1 (Blocking and decoupling). Let f :⌦ ! R be a function on a compact domain and let
⌦ be a subset of the domain. We say that the coordinates Z block X from Y under
X⇥Y⇥Z✓
f if the set of minimizers of f over X does not change for any setting of the variables Y given a
setting of the variables Z:

f (X  Y2  Z).

We will say that X and Y are decoupled conditioned on Z under f  or X? f YZ  if Z blocks X

from Y and Z blocks Y from X under f at the same time.

8

argmin
X2X

f (X  Y1  Z) = argmin
X2X

Y12Y Y22Y

Z2Z

2

 

We will simply say that X and Y are decoupled  or X? f Y  when X? f YZ  ⌦= X⇥Y⇥Z

and f is understood from context.

For a given function f (x1  . . .   xn)  decoupling between the variables can be represented graphically
with an undirected graph analogous to a Markov network:
Deﬁnition 2. A graph G = ({x1  . . .   xn}  E) is a coupling graph for a function f (x1  . . .   xn) if
(i  j) /2 E implies xi and xj are decoupled under f.
The following result mirrors the notion of separation in Markov networks and makes it easy to reason
about decoupling between groups of variables with coupling graphs (see the appendix for a proof):
Proposition 1. Let X  Y Z be groups of nodes in a coupling graph for a function f. If every path

s=1 gs(Xs)  then X? f YZ whenever X? ⌦ YZ.

from a node in X to a node in Y passes through a node in Z  then X? f YZ.
X  Y Z  we say X is conditionally separated from Y by Z by factorization  or X? ⌦ YZ  if X and

Functions that factorize as a product of terms exhibit this type of decoupling. For subsets of variables
Y are separated in that way in the Markov network induced by the factorization of f. The following
is a generalization of the familiar result that factorization implies the global Markov property (Koller
& Friedman  2009  Thm. 4.3) and follows from Aji & McEliece (2000):
Theorem 1 (Factorization implies decoupling). Let f (x1  . . .   xn) be a function on a compact do-
main  and let X1  . . .  XS X  Y Z be subsets of {x1  . . .   xn}. Let ⌦ be any commutative associa-
tive semi-ring operator over which the min operator distributes. If f factorizes as f (x1  . . .   xn) =
⌦S
However decoupling is strictly more powerful than factorization. While X? ⌦ Y implies X? f Y 
the reverse is not necessarily true: there exist functions that admit no factorization at all  yet whose
arguments are completely mutually decoupled. Appendix B gives an example.
4 Optimization procedures that utilize decoupling
When a cost function factorizes  dynamic programming algorithms can be used to optimize over the
variables Aji & McEliece (2000). When a cost function exhibits decoupling as deﬁned above  the
same dynamic programming algorithms can be applied with a few minor modiﬁcations.
The algorithms below refer to a function f whose arguments are partitioned over the sets
X1  . . .  Xn. Let X⇤i denote the optimal value of Xi 2X i. We will take simplifying liberties with
the order of the arguments of f when this causes no ambiguity. We will also replace the variables
that do not participate in the optimization (per decoupling) with an ellipsis.

4.1 Optimization over trees
Suppose the coupling graph between some partitioning X1  . . .  Xm of the arguments of f is tree-
structured  in the sense that Xi ?f Xj unless the edge (i  j) is in the tree. To optimize over f with
dynamic programming  deﬁne X0 arbitrarily as the root of the tree  let pi denote the index of the
parent of Xi  and let C1
i   . . . denote the indices of its children. At each leaf node `  construct the
functions

i   C2

ˆX` (Xp`) := argmin
X`2X`

f (X`  Xp`).

(1)

By decoupling  the optimal value of X` depends only on the optimal value of its parent  so X⇤` =
ˆX`(X⇤p`).
For all other nodes i  deﬁne recursively starting from the parents of the leaf nodes the functions

ˆXi(Xpi) = argmin
Xi2Xi

f (Xi  Xpi  ˆXC1

i

(Xi)  ˆXC2

i

(Xi)  . . .)

(2)

Again  the optimal value of Xi depends only on the optimal setting of its parent  X⇤pi   and it can be
veriﬁed that X⇤i = ˆXi(X⇤pi).
In our implementation of this algorithm  to represent a function ˆXi(X)  we discretize its argument
into a grid  and store the function as a table. To compute the entries of the table  a subordinate global
optimizer computes the minimization that appears in the deﬁnition of ˆXi.

3

4.2 Optimization over junction trees

Even when the coupling graph for a function is not tree-structured  a thin junction tree can often be
constructed for it. A variant of the above algorithm that mirrors the junction tree algorithm can be
used to efﬁciently search for the optima of the function.
Recall that a tree T of cliques is a junction tree for a graph G if it satisﬁes the following three
properties: there is one path between each pair of cliques; for each clique C of G there is some
clique A in T such that C ✓ A; for each pair of cliques A and B in T that contain node i of G  each
clique on the unique path between A and B also contains i.
These properties guarantee that T is tree-structured  that it covers all nodes and edges in G  and that
two nodes v and u in two different cliques Xi and Xj are decoupled from each other conditioned on
the union of the cliques on the path between u and v in T . Many heuristics exist for constructing a
thin junction tree for a graph Jensen & Graven-Nielsen (2007); Huang & Darwiche (1996).
To search for the minimizers of f  using a junction tree for its coupling graph  denote by Xij :=
Xi \X j the intersection of the groups of variables Xi and Xj and by Xi\j = Xi \Xj the set of nodes
in Xi but not in Xj. At every leaf clique ` of the junction tree  construct the function

ˆX` (X` p`) := argmin

f (X`).

X`\p`2X`\p`

(3)

For all other cliques i  compute recursively starting from the parents of the leaf cliques

ˆXi(Xi pi) = argmin
Xi pi2Xi\pi

f (Xi  ˆXC1

i

(Xi C1

i

)  ˆXC2

i

(Xi C2

i

)  . . .).

(4)

As before  decoupling between the cliques  conditioned on the intersection of the cliques  guarantees
that ˆXi(X⇤i pi) = X⇤i . And as before  our implementation of this algorithm stores the intermediate
functions as tables by discretizing their arguments.

4.3 Other strategies

When the cliques of the junction tree are large  the subordinate optimizations in the above algorithm
become costly. In such cases  the following adaptations of approximate inference algorithms are
useful:

graph.

• The algorithm of Section 4.1 can be applied to a maximal spanning tree of the coupling
• Analogously to Loopy Belief Propagation Pearl (1997)  an arbitrary neighbor of each node
can be declared as its parent  and the steps of Section 4.1 can be applied to each node until
convergence.

• Loops in the coupling graph can be broken by conditioning on a node in each loop  resulting
in a tree-structured coupling graph conditioned on those nodes. The optimizer of Section
4.1 then searches for the minima conditioned on the value of those nodes in the inner loop
of a global optimizer that searches for good settings for the conditioned nodes.

5 Graph structure learning
It is possible to estimate decoupling structure between the arguments of a function f with the help
of a subordinate optimizer that only evaluates f.
A straightforward application of deﬁnition 1 to assess empirically whether groups of variables X
and Y are decoupled conditioned on a group of variables Z would require comparing the minimizer
of f over X for every possible value of Z and Y. This is not practical because it is at least as difﬁcult
as minimizing f. Instead  we rely on the following proposition  which follows directly from 1:
Proposition 2 (Invalidating decoupling).
If for some Z 2Z and Y0  Y1 2Y   we have
argminX2X f (X  Y0  Z) 6= argminX2X f (X  Y1  Z)  then X 6?f Y|Z.
Following this result  an approximate coupling graph can be constructed by positing and invalidating
decoupling relations. Starting with a graph containing no edges  we consider all groupings X =

4

{xi} Y = {xj} Z =⌦ \{xi  xj}  of variables x1  . . .   xn. We posit various values of Z 2Z   Y0 2
Y and Y1 2Y under this grouping  and compute the minimizers over X 2X of f (X  Y0  Z) and
f (X  Y1  Z) with a subordinate optimizer. If the minimizers differ  then by the above proposition 
X and Y are not decoupled conditioned on Z  and an edge is added between xi and xj in the graph.
Algorithm 1 summarizes this procedure.

Algorithm 1 Estimating the coupling graph of a function.
input A function f : X1 ⇥···X n ! R  with Xi compact; A discretization ˆXi of Xi; A similarity
threshold ✏> 0; The number of times  NZ  to sample Z.
output A coupling graph G = ([x1  . . .   xn]  E).
E ;
for i  j 2 [1  . . .   n]; y0  y1 2 ˆXj; 1 . . . NZ do
Z ⇠ U ( ˆX1 ⇥···⇥ ˆXn \ ˆXi ⇥ ˆXj)
ˆx0 argminx2 ˆXi
if kˆx0  ˆx1k  ✏ then
E E [{ (i  j)}
end if
end for

f (x  y0  Z); ˆx1 argminx2 ˆXi

f (x  y1  Z)

In practice  we ﬁnd that decoupling relationships are correctly recovered if values of Y0 and Y1 are
chosen by quantizing Y into a set ˆY of 4 to 10 uniformly spaced discrete values and exhaustively
examining the settings of Y0 and Y1 in ˆY. A few values of Z (fewer than ﬁve) sampled uniformly at
random from a similarly discretized set ˆZ sufﬁce.
6 Experiments

We evaluate a two step process for global optimization: ﬁrst estimating decoupling between vari-
ables using the algorithm of Section 5  then optimizing with this structure using an algorithm from
Section 4. Whenever Algorithm 1 detects tree-structured decoupling  we use the tree optimizer of
Section 4.1. Otherwise we either construct a junction tree and apply the junction tree optimizer of
Section 4.2 if the junction tree is thin  or we approximate the graph with a maximum spanning tree
and apply the tree solver of Section 4.1.
We compare this approach with three state-of-the-art black-box optimization procedures: Direct
Search Perttunen et al. (1993) (a deterministic space carving strategy)  FIPS Mendes et al. (2004) (a
biologically inspired randomized algorithm)  and MEGA Hazen & Gupta (2009) (a multiresolution
search strategy with numerically computed gradients). We use a publicly available implementation
of Direct Search 1  and an implementation of FIPS and MEGA available from the authors of MEGA.
We set the number of particles for FIPS and MEGA to the square of the dimension of the problem
plus one  following the recommendation of their authors.
As the subordinate optimizer for Algorithm 1  we use a simple grid search for all our experiments.
As the subordinate optimizer for the algorithms of Section 4  we experiment with grid search and
the aforementioned state-of-the-art global optimizers.
We report results on both synthetic and real optimization problems. For each experiment  we report
the quality of the solution each algorithm produces after a preset number of function calls. To vary
the number of function calls the baseline methods invoke  we vary the number of time they iterate.
Since our method does not iterate  we vary the number of function calls its subordinate optimizer
invokes (when the subordinate optimizer is grid search  we vary the grid resolution).
The experiments demonstrate that using grid search as a subordinate strategy is sufﬁcient to produce
better solutions than all the other global optimizers we evaluated.

1Available from http://www4.ncsu.edu/˜ctk/Finkel_Direct/.

5

Table 1: Value of the iterates of the functions of Table 2 after 10 000 function evaluations (for
our approach  this includes the function evaluations for structure learning). MIN is the ground
truth optimal value when available. GR is the number of discrete values along each dimension for
optimization. Direct Search (DIR)  FIPS and MEGA are three state-of-the-art algorithms for global
optimization.

Function (n=50) min GR
100
Colville
400
Levy
Michalewics
400
400
Rastrigin
400
Schwefel
20
Dixon&Price
20
Rosenbrock
Trid
20
6
Powell

0
0
n/a
0
0
0
0
n/a
0

Ours

0

0.013
-48.9

0
8.6
1
0

-2.2e4
19.4

DIR
3e-6
2.80
-18.2

0

1.9e4
0.667
2.9e4
-185
324

FIPS MEGA
3.75
2e-14
3.22
4.20
-18.4
-1.3e-3
4.2e-3
23.6
1.4e4
1.6e4
0.914
16.8
48.4
5.7e4
3.3e4
-41
0.014
121

6.1 Synthetic objective functions
We evaluated the above strategies on a standard benchmark of synthetic optimization problems 2
shown in Appendix A. These are functions of 50 variables and are used as black-box functions
in our experiments. In these experiments  the subordinate grid search of Algorithm 1 discretized
each dimension into four discrete values. The algorithms of Section 4 also used grid search as a

subordinate optimizer. For this grid search  each dimension was discretized into GR =⇣ Emax
Nmc⌘ 1

discrete values where Emax is a cap on the number of function evaluations to perform  Smc is the
size of the largest clique in the junction tree  and Nmc is the number of nodes in the junction tree.
Figure 1 shows that in all cases  Algorithm 1 recovered decoupling structure exactly even for very
coarse grids. Values of NZ greater than 1 did not improve the quality of the recovered graph 
justifying our heuristic of keeping NZ small. We used NZ = 1 in the remainder of this subsection.
Table 1 summarizes the quality of the solutions produced by the various algorithms after 10 000
function evaluations. Our approach outperformed the others on most of these problems. As ex-
pected  it performed particularly well on functions that exhibit sparse coupling  such as Levy  Rast-
rigin  and Schwefel.
In addition to achieving better solutions given the same number of function evaluations  our approach
also imposed lower computational overhead than the other methods: to process the entire benchmark
of this section takes our approach 2.2 seconds  while Direct Search  FIPS and MEGA take 5.7
minutes  3.7 minutes and 53.3 minutes respectively.

Smc

100% 
080% 
060% 
040% 
020% 
000% 

Number of evaluations 

4.9e3 1.1e4 1.9e4 3.1e4 4.4e4 
Colville 

2 3 4 5 6 

Grid resolution 

100% 
080% 
060% 
040% 
020% 
000% 

Number of evaluations 

4.9e3 1.1e4 1.9e4 3.1e4 4.4e4 
Levy 

2 3 4 5 6 

Grid resolution 

100% 
080% 
060% 
040% 
020% 
000% 

Number of evaluations 

4.9e3 1.1e4 1.9e4 3.1e4 4.4e4 
Rosenbrock 

2 3 4 5 6 

Grid resolution 

100% 
080% 
060% 
040% 
020% 
000% 

Number of evaluations 

4.9e3 1.1e4 1.9e4 3.1e4 4.4e4 
Powell 

2 3 4 5 6 

Grid resolution 

Figure 1: Very coarse gridding is sufﬁcient in Algorithm 1 to correctly recover decoupling structure.
The plots show percentage of incorrectly recovered edges in the coupling graph on four synthetic
cost functions as a function of the grid resolution (bottom x-axis) and the number of function evalu-
ations (top x-axis). NZ = 1 in these experiments.
6.2 Experiments on real applications

We considered the real-world problem of automatically tuning the parameters of machine vision
and machine learning programs to improve their accuracy on new datasets. We sought to tune the

2Acquired from http://www-optima.amp.i.kyoto-u.ac.jp/member/student/hedar/

Hedar_files/go.htm.

6

parameters of a face detector  a document topic classiﬁer  and a scene recognizer to improve their
accuracy on new application domains. Automatic parameter tuning allows a user to quickly tune
a program’s default parameters to their speciﬁc application domain without tedious trial and error.
To perform this tuning automatically  we treated the accuracy of a program as a black box function
of the parameter values passed to it. These were challenging optimization problems because the
derivative of the function is elusive and each function evaluation can take minutes. Because the
output of a program tends to depend in a structured way on its parameters  our method achieved
signiﬁcant speedups over existing global optimizers.

6.2.1 Face detection
The ﬁrst application was a face detector. The program has ﬁve parameters: the size  in pixels  of
the smallest face to consider  the minimum distance  in pixels  between detected faces; a ﬂoating
point subsampling rate for building a multiresolution pyramid of the input image; a boolean ﬂag that
determines whether to apply non-maximal suppression; and the choice of one of four wavelets to
use. Our goal was to minimize the detection error rate of this program on the GENKI-SZSL dataset
of 3  500 faces 3. Depending on the parameter settings  evaluating the accuracy of the program on
this dataset takes between 2 seconds and 2 minutes.
Algorithm 1 was run with a grid search as a subordinate optimizer with three discrete values along
the continuous dimensions.
It invoked 90 function evaluations and produced a coupling graph
wherein the ﬁrst three of the above parameters formed a clique and where the remaining two pa-
rameter were decoupled of the others. Given this coupling graph  our junction tree optimizer with
grid search (with the continuous dimensions quantized into 10 discrete values) invoked 1000 func-
tion evaluations  and found parameter settings for which the accuracy of the detector was 7% better
than the parameter settings found by FIPS and Direct Search after the same number of function eval-
uations. FIPS and Direct Search fail to improve their solution even after 1800 evaluations. MEGA
fails to improve over the initial detection error of 50.84% with any number of iterations. To evaluate
the accuracy of our method under different numbers of function invocations  we varied the grid res-
olution between 2 to 12. See Figure 2. These experiments demonstrate how a grid search can help
overcome local minima that cause FIPS and Direct Search to get stuck.

)

%

80

(
 
r
o
r
r
e

r
o

t
c
e

 

t

e
d

n
o

 

e
c
a
F

i
t

a
c
i
f
i
s
s
a
c

l

60

40

 

20
0

Junction tree solver with grid search
Direct search
FIPS

 

500
Number of evaluations

1000

1500

Figure 2: Depending on the number of function evaluations allowed  our method produces parameter
settings for the face detector that are better than those recovered by FIPS or Direct Search by as much
as 7%.
6.2.2 Scene recognition
The second application was a visual scene recognizer. It extracts GIST features Oliva & Torralba
(2001) from an input image and classiﬁes these features with a linear SVM. Our task was to tune
the six parameters of GIST to improve the recognition accuracy on a subset of the LabelMe dataset
4  which includes images of scenes such as coasts  mountains  streets  etc. The parameters of the
recognizer include a radial cut-off frequency (in cycles/pixel) of a circular ﬁlter that reduces illumi-
nation effects  the number of bins in a radial histogram of the response of a spatial spacial ﬁlter  and
the number of image regions in which to compute these histograms. Evaluating the classiﬁcation
error under a set of parameters requires extracting GIST features with these parameters on a training
set  training a linear SVM  then applying the extractor and classiﬁer to a test set. Each evaluation
takes between 10 and 20 minutes depending on the parameter settings.

3Available from http://mplab.ucsd.edu.
4Available from http://labelme.csail.mit.edu.

7

Algorithm 1 was run with a grid search as the subordinate optimizer  discretizing the search space
into four discrete values along each dimension. This results in a graph that admits no thin junction
tree  so we approximate it with a maximal spanning tree. We then apply the tree optimizer of Section
4.1 using as subordinate optimizers Direct Search  FIPS  and grid search (with ﬁve discrete values
along each dimension). After a total of roughly 300 function evaluations  the tree optimizer with
FIPS produces parameters that result in a classiﬁcation error of 29.17%. With the same number
of function evaluations  Direct Search and FIPS produce parameters that resulted in classiﬁcation
errors of 33.33% and 31.13% respectively. The tree optimizer with Direct Search and grid search as
subordinate optimizers resulted in error rates of 31.72% and 33.33%.
In this application  the proposed method enjoys only modest gains of ⇠ 2% because the variables
are tightly coupled  as indicated by the denseness of the graph and the thickness of the junction tree.

6.2.3 Multi-class classiﬁcation
The third application was to tune the hyperparameters of a multi-class SVM classiﬁer on the RCV1-
v2 text categorization dataset 5. This dataset consists of a training set of 23 149 documents and a
test set of 781 265 documents each labeled with one of 101 topics Lewis et al. (2004). Our task
was to tune the 101 regularization parameters of the 1 vs. all classiﬁers that comprise a multi-class
classiﬁer. The objective was the so-called macro-average F -score Tague (1981) on the test set. The
F score for one category is F = 2rp/(r + p)  where r and p are the recall and precision rates
for that category. The macro-average F score is the average of the F scores over all categories.
Each evaluation requires training the classiﬁer using the given hyperparameters and evaluating the
resulting classiﬁer on the test set  and takes only a second since the text features have been pre-
computed.
Algorithm 1 with grid search as a subordinate optimizer with a grid resolution of three discrete values
along each dimension found no coupling between the hyperparameters. As a result  the algorithms
of Section 4.1 reduce to optimizing over each one-dimensional parameter independently. We carried
out these one-dimensional optimizations with Direct Search  FIPS  and grid search (discretizing each
dimension into 100 values). After roughly 100 000 evaluations  these resulted in similar scores of
F = 0.6764  0.6720  and 0.6743  respectively. But with the same number of evaluations  off-the-
shelf Direct Search and FIPS result in scores of F = 0.6324 and 0.6043  respectively  nearly 11%
worse.
The cost of estimating the structure in this problem was large  since it grows quadratically with the
number of classes  but worth the effort because it indicated that each variable should be optimized
independently  ultimately resulting in huge speedups 6.
7 Conclusion
We quantiﬁed the coupling between variables of optimization in a way that parallels the notion of
independence in statistics. This lets us identify decoupling between variables in cases where the
function does not factorize  making it strictly stronger than the notion of decoupling in statistical
estimation. This type of decoupling is also easier to evaluate empirically. Despite these differences 
this notion of decoupling allows us to migrate to global optimization many of the message pass-
ing algorithms that were developed to leverage factorization in statistics and optimization. These
include belief propagation and the junction tree algorithm. We show empirically that optimizing
cost functions by applying these algorithms to an empirically estimated decoupling structure out-
performs existing black box optimization procedures that rely on numerical gradients  deterministic
space carving  or biologically inspired searches. Notably  we observe that it is advantageous to
decompose optimization problems into a sequence of small deterministic grid searches using this
technique  as opposed to employing existing black box optimizers directly.

5Available from http://trec.nist.gov/data/reuters/reuters.html.
6After running these experiments  we discovered a result of Fan & Lin (2007) showing that optimizing the
macro-average F-measure is equivalent to optimizing per-category F-measure  thereby validating decoupling
structure recovered by Algorithm 1.

8

References
Aji  S. and McEliece  R. The generalized distributive law and free energy minimization.

Transaction on Informaion Theory  46(2)  March 2000.

IEEE

Bacchus  F. and Grove  A. Utility independence in a qualitative decision theory. In Proceedings of
the 6th International Conference on Principles of Knowledge Representation and Reasoning  pp.
542–552  1996.

Fan  R. E. and Lin  C. J. A study on threshold selection for multi-label classiﬁcation. Technical

report  National Taiwan University  2007.

Hazen  M. and Gupta  M. Gradient estimation in global optimization algorithms. Congress on

Evolutionary Computation  pp. 1841–1848  2009.

Huang  C. and Darwiche  A. Inference in belief networks: A procedural guide. International Journal

of Approximate Reasoning  15(3):225–263  1996.

Jensen  F. and Graven-Nielsen  T. Bayesian Networks and Decision Graphs. Springer  2007.
Keeney  R. L. and Raiffa  H. Decisions with Multiple Objectives: Preferences and Value Trade-offs.

Wiley  1976.

Koller  D. and Friedman  N. Probabilistic Graphical Models: Principles and Techniques. MIT

Press  2009.

Lewis  D.  Yang  Y.  Rose  T.  and Li  F. RCV1: A new benchmark collection for text categorization

research. Journal of Machine Learning Research  2004.

Mendes  R.  Kennedy  J.  and Neves  J. The fully informed particle swarm: Simpler  maybe better.

IEEE Transactions on Evolutionary Computation  1(1):204–210  2004.

Nocedal  J. and Wright  S. Numerical Optimization. Springer  2nd edition  2006.
Oliva  A. and Torralba  A. Modeling the shape of the scene: a holistic representation of the spatial

envelope. International Journal of Computer Vision  43:145–175  2001.

Pearl  J. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan

Kaufmann  1997.

Perttunen  C.  Jones  D.  and Stuckman  B. Lipschitzian optimization without the Lipschitz constant.

Journal of Optimization Theory and Application  79(1):157–181  1993.

Srinivas  N.  Krause  A.  Kakade  S.  and Seeger  M. Gaussian process optimization in the bandit
setting: No regret and experimental design. In International Conference on Machine Learning
(ICML)  2010.

Tague  J. M. The pragmatics of information retrieval experimentation. Information Retrieval Exper-

iment  pp. 59–102  1981.

9

,Navid Zolghadr
Gabor Bartok
Russell Greiner
András György
Csaba Szepesvari
Nisheeth Srivastava
Ed Vul
Paul Schrater
Sashank J. Reddi
Suvrit Sra
Barnabas Poczos
Alexander Smola
Zhengyang Shen
Francois-Xavier Vialard
Marc Niethammer