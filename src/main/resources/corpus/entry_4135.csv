2010,Optimal learning rates for Kernel Conjugate Gradient regression,We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm  where regularization against overfitting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares  a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: first  on the regularity of the target regression function and second  on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature  and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if  the true regression function belongs to the reproducing kernel Hilbert space. If the latter assumption is not fulfilled  we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates in these two situations match state-of-the-art results that were recently obtained for the least squares support vector machine and for linear regularization operators.,Optimal learning rates

for Kernel Conjugate Gradient regression

Gilles Blanchard

Mathematics Institute  University of Potsdam

Am neuen Palais 10  14469 Potsdam

blanchard@math.uni-potsdam.de

Nicole Kr¨amer

Weierstrass Institute

Mohrenstr. 39  10117 Berlin  Germany

nicole.kraemer@wias-berlin.de

Abstract

We prove rates of convergence in the statistical sense for kernel-based least
squares regression using a conjugate gradient algorithm  where regularization
against overﬁtting is obtained by early stopping. This method is directly related
to Kernel Partial Least Squares  a regression method that combines supervised
dimensionality reduction with least squares projection. The rates depend on two
key quantities: ﬁrst  on the regularity of the target regression function and sec-
ond  on the effective dimensionality of the data mapped into the kernel space.
Lower bounds on attainable rates depending on these two quantities were estab-
lished in earlier literature  and we obtain upper bounds for the considered method
that match these lower bounds (up to a log factor) if the true regression func-
tion belongs to the reproducing kernel Hilbert space. If this assumption is not
fulﬁlled  we obtain similar convergence rates provided additional unlabeled data
are available. The order of the learning rates match state-of-the-art results that
were recently obtained for least squares support vector machines and for linear
regularization operators.

1

Introduction

The contribution of this paper is the learning theoretical analysis of kernel-based least squares regres-
sion in combination with conjugate gradient techniques. The goal is to estimate a regression function
f∗ based on random noisy observations. We have an i.i.d. sample of n observations (Xi  Yi) ∈ X×R
from an unknown distribution P (X  Y ) that follows the model

Y = f∗(X) + ε  

where ε is a noise variable whose distribution can possibly depend on X  but satisﬁes E [ε|X] = 0.
We assume that the true regression function f∗ belongs to the space L2(PX) of square-integrable
functions. Following the kernelization principle  we implicitly map the data into a reproducing
kernel Hilbert space H with a kernel k. We denote by Kn = 1
n(k(Xi  Xj)) ∈ Rn×n the normalized
kernel matrix and by Υ = (Y1  . . .   Yn)> ∈ Rn the n-vector of response observations. The task is
to ﬁnd coefﬁcients α such that the function deﬁned by the normalized kernel expansion

fα(X) =

nX
£(fα(X) − f∗(X))2⁄ = EXY

1
n

i=1

αik(Xi  X)

is an adequate estimator of the true regression function f∗. The closeness of the estimator fα to the
target f∗ is measured via the L2(PX) distance 
kfα − f∗k2
The last equality recalls that this criterion is the same as the excess generalization error for the
squared error loss ‘(f  x  y) = (f(x) − y)2.

£(fα(X) − Y )2⁄ − EXY

£(f∗(X) − Y )2⁄  

2 = EX∼PX

1

In empirical risk minimization  we use the training data empirical distribution as a proxy for the gen-
erating distribution  and minimize the training squared error. This gives rise to the linear equation

with α ∈ Rn .

Knα = Υ

α = (Kn + λI)−1Υ 

(1)
Assuming Kn invertible  the solution of the above equation is given by α = K−1
n Υ  which yields
a function in H interpolating perfectly the training data but having poor generalization error. It is
well-known that to avoid overﬁtting  some form of regularization is needed. There is a considerable
variety of possible approaches (see e.g. [10] for an overview). Perhaps the most well-known one is
(2)
known alternatively as kernel ridge regression  Tikhonov’s regularization  least squares support vec-
tor machine  or MAP Gaussian process regression. A powerful generalization of this is to consider
(3)
where Fλ : R+ → R+ is a ﬁxed function depending on a parameter λ. The notation Fλ(Kn) is
to be interpreted as Fλ applied to each eigenvalue of Kn in its eigen decomposition. Intuitively 
Fλ should be a “regularized” version of the inverse function F (x) = x−1. This type of regular-
ization  which we refer to as linear regularization methods  is directly inspired from the theory of
inverse problems. Popular examples include as particular cases kernel ridge regression  principal
components regression and L2-boosting. Their application in a learning context has been studied
extensively [1  2  5  6  12]. Results obtained in this framework will serve as a comparison yardstick
in the sequel.
In this paper  we study conjugate gradient (CG) techniques in combination with early stopping for
the regularization of the kernel based learning problem (1). The principle of CG techniques is to
restrict the learning problem onto a nested set of data-dependent subspaces  the so-called Krylov
subspaces  deﬁned as

n Υ“ .
Km(Υ  Kn) = span'Υ  KnΥ  . . .   K m−1

α = Fλ(Kn)Υ 

(4)
Denote by h.  .i the usual euclidean scalar product on Rn rescaled by the factor n−1. We deﬁne
:= hα  Knαi . The CG solution after m iterations is formally
the Kn-norm as kαk2
deﬁned as

:= hα  αiKn

Kn

αm = arg

min

α∈Km(Υ Kn)

kΥ − KnαkKn ;

(5)

and the number m of CG iterations is the model parameter. To simplify notation we deﬁne
fm := fαm. In the learning context considered here  regularization corresponds to early stopping.
Conjugate gradients have the appealing property that the optimization criterion (5) can be computed
by a simple iterative algorithm that constructs basis vectors d1  . . .   dm of Km(Υ  Kn) by using
only forward multiplication of vectors by the matrix Kn. Algorithm 1 displays the computation of
the CG kernel coefﬁcients αm deﬁned by (5).

Algorithm 1 Kernel Conjugate Gradient regression

Input kernel matrix Kn  response vector Υ  maximum number of iterations m
Initialization: α0 = 0n; r1 = Υ; d1 = Υ; t1 = KnΥ
for i = 1  . . .   m do
ti = ti/ktikKn ; di = di/ktikKn (normalization of the basis  resp. update vector)
γi = hΥ  tiiKn
αi = αi−1 + γidi (update)
ri+1 = ri − γiti (residuals)
di+1 = ri+1 − di hti  Knri+1iKn
end for

Return: CG kernel coefﬁcients αm  CG function fm =Pn

; ti+1 = Kndi+1 (new update  resp. basis vector)

(proj. of Υ on basis vector)

i=1 αi mk(Xi ·)

The CG approach is also inspired by the theory of inverse problems  but it is not covered by the
framework of linear operators deﬁned in (3): As we restrict the learning problem onto the Krylov
space Km(Υ  Kn)   the CG coefﬁcients αm are of the form αm = qm(Kn)Υ with qm a polynomial
of degree ≤ m − 1. However  the polynomial qm is not ﬁxed but depends on Υ as well  making the
CG method nonlinear in the sense that the coefﬁcients αm depend on Υ in a nonlinear fashion.

2

We remark that in machine learning  conjugate gradient techniques are often used as fast solvers
for operator equations  e.g. to obtain the solution for the regularized equation (2). We stress that
in this paper  we study conjugate gradients as a regularization approach for kernel based learning 
where the regularity is ensured via early stopping. This approach is not new. As mentioned in
the abstract  the algorithm that we study is closely related to Kernel Partial Least Squares [18].
The latter method also restricts the learning problem onto the Krylov subspace Km(Υ  Kn)  but
it minimizes the euclidean distance kΥ − Knαk instead of the distance kΥ − KnαkKn deﬁned
above1. Kernel Partial Least Squares has shown competitive performance in benchmark experiences
(see e.g [18  19]). Moreover  a similar conjugate gradient approach for non-deﬁnite kernels has been
proposed and empirically evaluated by Ong et al [17]. The focus of the current paper is therefore not
to stress the usefulness of CG methods in practical applications (and we refer to the above mentioned
references) but to examine its theoretical convergence properties. In particular  we establish the
existence of early stopping rules that lead to optimal convergence rates. We summarize our main
results in the next section.

2 Main results

For the presentation of our convergence results  we require suitable assumptions on the learning
problem. We ﬁrst assume that the kernel space H is separable and that the kernel function is
measurable. (This assumption is satisﬁed for all practical situations that we know of.) Further-
more  for all results  we make the (relatively standard) assumption that the kernel is bounded:
k(x  x) ≤ κ for all x ∈ X . We consider – depending on the result – one of the following as-
sumptions on the noise:
(Bounded) (Bounded Y ): |Y | ≤ M almost surely.
(Bernstein) (Bernstein condition): E [εp|X] ≤ (1/2)p!M p almost surely  for all integers p ≥ 2.
The second assumption is weaker than the ﬁrst. In particular  the ﬁrst assumption implies that not
only the noise  but also the target function f∗ is bounded in supremum norm  while the second
assumption does not put any additional restriction on the target function.
The regularity of the target function f∗ is measured in terms of a source condition as follows. The
kernel integral operator is given by

K : L2(PX) → L2(PX)  g 7→

k(.  x)g(x)dP (x) .

Z

The source condition for the parameters r > 0 and ρ > 0 is deﬁned by:

SC(r  ρ) : f∗ = K ru

with

kuk ≤ κ−rρ.

It is a known fact that if r ≥ 1/2  then f∗ coincides almost surely with a function belonging to Hk.
We refer to r ≥ 1/2 as the “inner case” and to r < 1/2 as the “outer case”.
The regularity of the kernel operator K with respect to the marginal distribution PX is measured in
terms of the so-called effective dimensionality condition  deﬁned by the two parameters s ∈ (0  1) 
D ≥ 0 and the condition

ED(s  D) : tr(K(K + λI)−1) ≤ D2(κ−1λ)−s for all λ ∈ (0  1].

This notion was ﬁrst introduced in [22] in a learning context  along with a number of fundamental
analysis tools which we rely on and have been used in the rest of the related literature cited here.
It is known that the best attainable rates of convergence  as a function of the number of examples
n  are determined by the parameters r and s in the above conditions: It was shown in [6] that the
minimax learning rate given these two parameters is lower bounded by O(n−2r/(2r+s)).
We now expose our main results in different situations. In all the cases considered  the early stopping
rule takes the form of a so-called discrepancy stopping rule: For some sequence of thresholds
Λm > 0 to be speciﬁed (and possibly depending on the data)  deﬁne the (data-dependent) stopping

iteration bm as the ﬁrst iteration m for which

(6)
1This is generalized to a CG-l algorithm (l ∈ N≥0) by replacing the Kn-norm in (5) with the norm deﬁned
n. Corresponding fast iterative algorithms to compute the solution exist for all l (see e.g. [11]).

< Λm .

by K l

kΥ − KnαmkKn

3

Only in the ﬁrst result below  the threshold Λm actually depends on the iteration m and on the data.
stopping rule always has bm ≤ n.
It is not difﬁcult to prove from (4) and (5) that kΥ − KnαnkKn
= 0  so that the above type of

r

‡√

+ Mplog(2γ−1)

·

Inner case without knowledge on effective dimension

2.1
The inner case corresponds to r ≥ 1/2  i.e.
the target function f∗ lies in H almost surely. For
some constants τ > 1 and 1 > γ > 0  we consider the discrepancy stopping rule with the threshold
sequence

κ log(2γ−1)

κkαmkKn

n

Λm = 4τ

For technical reasons  we consider a slight variation of the rule in that we stop at stepbm−1 instead of
bm if qbm(0) ≥ 4κplog(2γ−1)/n  where qm is the iteration polynomial such that αm = qm(Kn)Υ.
Denote em the resulting stopping step. We obtain the following result.
for r ≥ 1/2. With probability 1 − 2γ   the estimator fem obtained by the (modiﬁed) discrepancy
¶ 2r
(cid:181)log2 γ−1

Theorem 2.1. Suppose that Y is bounded (Bounded)  and that the source condition SC(r  ρ) holds

stopping rule (7) satisﬁes

(7)

2r+1

.

kfem − f∗k2

2 ≤ c(r  τ)(M + ρ)2

n

.

We present the proof in Section 4.

2.2 Optimal rates in inner case

We now introduce a stopping rule yielding order-optimal convergence rates as a function of the
two parameters r and s in the “inner” case (r ≥ 1/2  which is equivalent to saying that the target
function belongs to H almost surely). For some constant τ0 > 3/2 and 1 > γ > 0  we consider the
discrepancy stopping rule with the ﬁxed threshold
√

¶ 2r+1

2r+s

.

(8)

Λm ≡ Λ = τ0M

κ

(cid:181) 4D√

n

log

6
γ

for which we obtain the following:
Theorem 2.2. Suppose that the noise fulﬁlls the Bernstein assumption (Bernstein)  that the source
condition SC(r  ρ) holds for r ≥ 1/2  and that ED(s  D) holds. With probability 1 − 3γ   the
estimator fbm obtained by the discrepancy stopping rule (8) satisﬁes
log2 6
γ

2 ≤ c(r  τ0)(M + ρ)2

kfbm − f∗k2

(cid:181)16D2

¶ 2r

2r+s

n

.

Due to space limitations  the proof is presented in the supplementary material.

2.3 Optimal rates in outer case  given additional unlabeled data

We now turn to the “outer” case. In this case  we make the additional assumption that unlabeled
data is available. Assume that we have ˜n i.i.d. observations X1  . . .   X˜n  out of which only the ﬁrst
n (Y1  . . .   Yn  0  . . .   0) ∈ R˜n and run the CG
n are labeled. We deﬁne a new response vector ˜Υ = ˜n
algorithm 1 on X1  . . .   X˜n and ˜Υ. We use the same threshold (8) as in the previous section for the
stopping rule  except that the factor M is replaced by max(M  ρ).
Theorem 2.3. Suppose assumptions (Bounded)  SC(r  ρ) and ED(s  D)  with r + s ≥ 1

2 . Assume

unlabeled data is available with en

(cid:181)16D2

n

≥

n

¶− (1−2r)+

2r+s

.

log2 6
γ

4

Then with probability 1 − 3γ   the estimator fbm obtained by the discrepancy stopping rule deﬁned

above satisﬁes

kfbm − f∗k2

2 ≤ c(r  τ0)(M + ρ)2

(cid:181)16D2

n

log2 6
γ

¶ 2r

2r+s

.

A sketch of the proof can be found in the supplementary material.

3 Discussion and comparison to other results
For the inner case – i.e. f∗ ∈ H almost surely – we provide two different consistent stopping
criteria. The ﬁrst one (Section 2.1) is oblivious to the effective dimension parameter s  and the
obtained bound corresponds to the “worst case” with respect to this parameter (that is  s = 1).
However  an interesting feature of stopping rule (7) is that the rule itself does not depend on the
a priori knowledge of the regularity parameter r  while the achieved learning rate does (and with
the optimal dependence in r when s = 1). Hence  Theorem 2.1 implies that the obtained rule is
automatically adaptive with respect to the regularity of the target function. This contrasts with the
results obtained in [1] for linear regularization schemes of the form (3)  (also in the case s = 1)
for which the choice of the regularization parameter λ leading to optimal learning rates required the
knowledge or r beforehand.
When taking into account also the effective dimensionality parameter s  Theorem 2.2 provides the
order-optimal convergence rate in the inner case (up to a log factor). A noticeable difference to
Theorem 2.1 however  is that the stopping rule is no longer adaptive  that is  it depends on the a
priori knowledge of parameters r and s. We observe that previously obtained results for linear
regularization schemes of the form (2) in [6] and of the form (3) in [5]  also rely on the a priori
knowledge of r and s to determine the appropriate regularization parameter λ.
The outer case – when the target function does not lie in the reproducing Kernel Hilbert space H – is
more challenging and to some extent less well understood. The fact that additional assumptions are
made is not a particular artefact of CG methods  but also appears in the studies of other regularization
techniques. Here we follow the semi-supervised approach that is proposed in e.g. [5] (to study linear
regularization of the form (3)) and assume that we have sufﬁcient additional unlabeled data in order
to ensure learning rates that are optimal as a function of the number of labeled data. We remark that
other forms of additional requirements can be found in the recent literature in order to reach optimal
rates. For regularized M-estimation schemes studied in [20]  availability of unlabeled data is not
required  but a condition is imposed of the form kfk∞ ≤ C kfkpH kfk1−p
for all f ∈ H and some
p ∈ (0  1]. In [13]  assumptions on the supremum norm of the eigenfunctions of the kernel integral
operator are made (see [20] for an in-depth discussion on this type of assumptions).
Finally  as explained in the introduction  the term ’conjugate gradients’ comprises a class of methods
that approximate the solution of linear equations on Krylov subspaces. In the context of learning 
our approach is most closely linked to Partial Least Squares (PLS) [21] and its kernel extension
[18]. While PLS has proven to be successful in a wide range of applications and is considered one
of the standard approaches in chemometrics  there are only few studies of its theoretical properties.
In [8  14]  consistency properties are provided for linear PLS under the assumption that the target
function f∗ depends on a ﬁnite known number of orthogonal latent components. These ﬁndings
were recently extended to the nonlinear case and without the assumption of a latent components
model [3]  but all results come without optimal rates of convergence. For the slightly different CG
approach studied by Ong et al [17]  bounds on the difference between the empirical risks of the CG
approximation and of the target function are derived in [16]  but no bounds on the generalization
error were derived.

2

4 Proofs

Convergence rates for regularization methods of the type (2) or (3) have been studied by casting
kernel learning methods into the framework of inverse problems (see [9]). We use this framework
for the present results as well  and recapitulate here some important facts.

5

We ﬁrst deﬁne the empirical evaluation operator Tn as follows:

Tn :

g ∈ H 7→ Tng := (g(X1)  . . .   g(Xn))> ∈ Rn

and the empirical integral operator T ∗

n as:

n : u = (u1  . . .   un) ∈ Rn 7→ T ∗
T ∗

n u :=

1
n

uik(Xi ·) ∈ H.

nX

i=1

Using the reproducing property of the kernel  it can be readily checked that Tn and T ∗
operators  i.e.
TnT ∗

n are adjoint
n u  giH = hu  Tngi  for all u ∈ Rn  g ∈ H . Furthermore  Kn =
= kfαkH. Based on these facts  equation (5) can be rewritten as
fm = arg

n  and therefore kαkKn

they satisfy hT ∗

n Y − SnfkH  

n Υ Sn)

min
f∈Km(T ∗

(9)
n Tn is a self-adjoint operator of H  called empirical covariance operator. This deﬁni-
where Sn = T ∗
tion corresponds to that of the “usual” conjugate gradient algorithm formally applied to the so-called
normal equation (in H)

Snfα = T ∗
nΥ  
which is obtained from (1) by left multiplication by T ∗
n. The advantage of this reformulation is that
it can be interpreted as a “perturbation” of a population  noiseless version (of the equation and of
the algorithm)  wherein Y is replaced by the target function f∗ and the empirical operator T ∗
n   Tn
are respectively replaced by their population analogues  the kernel integral operator

kT ∗

T ∗ : g ∈ L2(PX) 7→ T ∗g :=

k(.  x)g(x)dPX(x) = E [k(X ·)g(X)] ∈ H  

Z

and the change-of-space operator

T :

g ∈ H 7→ g ∈ L2(PX) .

The latter maps a function to itself but between two Hilbert spaces which differ with respect to their
geometry – the inner product of H being deﬁned by the kernel function k  while the inner product of
L2(PX) depends on the data generating distribution (this operator is well deﬁned: since the kernel
is bounded  all functions in H are bounded and therefore square integrable under any distribution
PX).
The following results  taken from [1] (Propositions 21 and 22) quantify more precisely that the
empirical covariance operator Sn = T ∗
n Tn and the empirical integral operator applied to the data 
nΥ  are close to the population covariance operator S = T ∗T and to the kernel integral operator
T ∗
applied to the noiseless target function  T ∗f∗ respectively.
Proposition 4.1. Assume that k(x  x) ≤ κ for all x ∈ X . Then the following holds:

≥ 1 − γ  

(10)

•
kSn − SkHS ≤ 4κ√

r

n

log

2
γ

‚

‚

√
κ√
n

log

2
γ

P

•

where k.kHS denotes the Hilbert-Schmidt norm. If the representation f∗ = T f∗
assumption (Bernstein)  we have the following:

H holds  and under

P

kT ∗

nY − Sf∗

Hk ≤ 4M

≥ 1 − γ .

(11)

We note that f∗ = T f∗
H belonging
to H (remember that T is just the change-of-space operator). Hence  the second result (11) is valid
for the case with r ≥ 1/2  but it is not true in general for r < 1/2 .

H implies that the target function f∗ coincides with a function f∗

4.1 Nemirovskii’s result on conjugate gradient regularization rates

We recall a sharp result due to Nemirovskii [15] establishing convergence rates for conjugate gradi-
ent methods in a deterministic context. We present the result in an abstract context  then show how 
combined with the previous section  it leads to a proof of Theorem 2.1. Consider the linear equation

Az∗ = b  

6

where A is a bounded linear operator over a Hilbert space H . Assume that the above equation has a
solution and denote z∗ its minimal norm solution; assume further that a self-adjoint operator ¯A  and
an element ¯b ∈ H are known such that

(12)
(with δ and ε known positive numbers). Consider the CG algorithm based on the noisy operator ¯A
and data ¯b  giving the output at step m

(cid:176)(cid:176)A − ¯A(cid:176)(cid:176) ≤ δ ;

(cid:176)(cid:176)b − ¯b(cid:176)(cid:176) ≤ ε  
(cid:176)(cid:176) ¯Az − ¯b(cid:176)(cid:176)2

.

(13)

zm = Arg Min
z∈Km( ¯A ¯b)

¯m = min'm ≥ 0 :(cid:176)(cid:176) ¯Azm − ¯b(cid:176)(cid:176) < τ(δ kzmk + ε)“ .
bm =

if q ¯m(0) < ηδ−1
otherwise 

max(0  ¯m − 1)

‰ ¯m

The discrepancy principle stopping rule is deﬁned as follows. Consider a ﬁxed constant τ > 1 and
deﬁne

We output the solution obtained at step max(0  ¯m − 1) . Consider a minor variation of of this rule:

where q ¯m is the degree m − 1 polynomial such that z ¯m = q ¯m( ¯A)¯b   and η is an arbitrary positive
constant such that η < 1/τ . Nemirovskii established the following theorem:

Theorem 4.2. Assume that (a) max(kAk  (cid:176)(cid:176) ¯A(cid:176)(cid:176)) ≤ L; and that (b) z∗ = Aµu∗ with ku∗k ≤ R for
some µ > 0. Then for any θ ∈ [0  1]   provided that bm < ∞ it holds that

(cid:176)(cid:176)Aθ (zbm − z∗)(cid:176)(cid:176)2 ≤ c(µ  τ  η)R

2(1−θ)
1+µ (ε + δRLµ)2(θ+µ)/(1+µ) .

2): By identifying the approximate
nY  we see that the CG algorithm considered by Nemirovskii

4.2 Proof of Theorem 2.1
We apply Nemirovskii’s result in our setting (assuming r ≥ 1
operator and data as ¯A = Sn and ¯b = T ∗
(13) is exactly (9)  more precisely with the identiﬁcation zm = fm.
For the population version  we identify A = S  and z∗ = f∗
source condition  then there exists f∗
Condition (a) of Nemirovskii’s theorem 4.2 is satisﬁed with L = κ by the boundedness of the kernel.
Condition (b) is satisﬁed with µ = r − 1/2 ≥ 0 and R = κ−rρ  as implied by the source condition
SC(r  ρ). Finally  the concentration result 4.1 ensures that the approximation conditions (12) are
satisﬁed with probability 1 − 2γ   more precisely with δ = 4κ√
γ . (Here
we replaced γ in (10) and (11) by γ/2  so that the two conditions are satisﬁed simultaneously  by
the union bound). The operator norm is upper bounded by the Hilbert-Schmidt norm  so that the
deviation inequality for the operators is actually stronger than what is needed.
We consider the discrepancy principle stopping rule associated to these parameters  the choice η =
1/(2τ)  and θ = 1

H (remember that provided r ≥ 1
H ∈ H such that f∗ = T f∗
H).
q

√
κ√
n log 2

2   thus obtaining the result  since

γ and ε = 4M

2 in the

log 2

n

(cid:176)(cid:176)(cid:176)A

(cid:176)(cid:176)(cid:176)2
2 (zbm − z∗)

1

(cid:176)(cid:176)(cid:176)S

=

(cid:176)(cid:176)(cid:176)2

H

1

2 (fbm − f∗
H)

= kfbm − f∗
Hk2
2 .

4.3 Notes on the proof of Theorems 2.2 and 2.3

The above proof shows that an application of Nemirovskii’s fundamental result for CG regularization
of inverse problems under deterministic noise (on the data and the operator) allows us to obtain our
ﬁrst result. One key ingredient is the concentration property 4.1 which allows to bound deviations
in a quasi-deterministic manner.
To prove the sharper results of Theorems 2.2 and 2.3  such a direct approach does not work unfor-
tunately  and a complete rework and extension of the proof is necessary. The proof of Theorem 2.2
is presented in the supplementary material to the paper. In a nutshell  the concentration result 4.1
is too coarse to prove the optimal rates of convergence taking into account the effective dimension

7

(cid:176)(cid:176)(cid:176)(S + λI)− 1

2 (T ∗

(cid:176)(cid:176)(cid:176)HS

(cid:176)(cid:176)(cid:176) for the data  and

(cid:176)(cid:176)(cid:176)(S + λI)− 1

2 (Sn − S)

nY − T ∗f∗)

parameter. Instead of that result  we have to consider the deviations from the mean in a “warped”
norm  i.e. of the form
for the operator (with an appropriate choice of λ > 0) respectively. Deviations of this form were
introduced and used in [5  6] to obtain sharp rates in the framework of Tikhonov’s regularization (2)
and of the more general linear regularization schemes of the form (3). Bounds on deviations of this
form can be obtained via a Bernstein-type concentration inequality for Hilbert-space valued random
variables.
On the one hand  the results concerning linear regularization schemes of the form (3) do not apply
to the nonlinear CG regularization. On the other hand  Nemirovskii’s result does not apply to de-
viations controlled in the warped norm. Moreover  the “outer” case introduces additional technical
difﬁculties. Therefore  the proofs for Theorems 2.2 and 2.3  while still following the overall funda-
mental structure and ideas introduced by Nemirovskii  are signiﬁcantly different in that context. As
mentioned above  we present the complete proof of Theorem 2.2 in the supplementary material and
a sketch of the proof of Theorem 2.3.

5 Conclusion

In this work  we derived early stopping rules for kernel Conjugate Gradient regression that provide
optimal learning rates to the true target function. Depending on the situation that we study  the rates
are adaptive with respect to the regularity of the target function in some cases. The proofs of our
results rely most importantly on ideas introduced by Nemirovskii [15] and further developed by
Hanke [11] for CG methods in the deterministic case  and moreover on ideas inspired by [5  6].
Certainly  in practice  as for a large majority of learning algorithms  cross-validation remains the
standard approach for model selection. The motivation of this work is however mainly theoretical 
and our overall goal is to show that from the learning theoretical point of view  CG regulariza-
tion stands on equal footing with other well-studied regularization methods such as kernel ridge
regression or more general linear regularization methods (which includes between many others L2
boosting). We also note that theoretically well-grounded model selection rules can generally help
cross-validation in practice by providing a well-calibrated parametrization of regularizer functions 
or  as is the case here  of thresholds used in the stopping rule.
One crucial property used in the proofs is that the proposed CG regularization schemes can be con-
veniently cast in the reproducing kernel Hilbert space H as displayed in e.g (9). This reformulation
is not possible for Kernel Partial Least Squares: It is also a CG type method  but uses the standard
Euclidean norm instead of the Kn-norm used here. This point is the main technical justiﬁcation
on why we focus on (5) rather than kernel PLS. Obtaining optimal convergence rates also valid for
Kernel PLS is an important future direction and should build on the present work.
Another important direction for future efforts is the derivation of stopping rules that do not depend on
the conﬁdence parameter γ. Currently  this dependence prevents us to go from convergence in high
probability to convergence in expectation  which would be desirable. Perhaps more importantly  it
would be of interest to ﬁnd a stopping rule that is adaptive to both parameters r (target function
regularity) and s (effective dimension parameter) without their a priori knowledge. We recall that
our ﬁrst stopping rule is adaptive to r but at the price of being worst-case in s. In the literature on
linear regularization methods  the optimal choice of regularization parameter is also non-adaptive 
be it when considering optimal rates with respect to r only [1] or to both r and s [5]. An approach to
alleviate this problem is to use a hold-out sample for model selection; this was studied theoretically
in [7] for linear regularization methods (see also [4] for an account of the properties of hold-out
in a general setup). We strongly believe that the hold-out method will yield theoretically founded
adaptive model selection for CG as well. However  hold-out is typically regarded as inelegant in that
it requires to throw away part of the data for estimation. It would be of more interest to study model
selection methods that are based on using the whole data in the estimation phase. The application of
Lepskii’s method is a possible step towards this direction.

8

References
[1] F. Bauer  S. Pereverzev  and L. Rosasco. On Regularization Algorithms in Learning Theory.

Journal of Complexity  23:52–72  2007.

[2] N. Bissantz  T. Hohage  A. Munk  and F. Ruymgaart. Convergence Rates of General Regular-
ization Methods for Statistical Inverse Problems and Applications. SIAM Journal on Numerical
Analysis  45(6):2610–2636  2007.

[3] G. Blanchard and N. Kr¨amer. Kernel Partial Least Squares is Universally Consistent. Pro-
ceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics  JMLR
Workshop & Conference Proceedings  9:57–64  2010.

[4] G. Blanchard and P. Massart. Discussion of V. Koltchinskii’s ”Local Rademacher complexities

and oracle inequalities in risk minimization”. Annals of Statistics  34(6):2664–2671  2006.

[5] A. Caponnetto. Optimal Rates for Regularization Operators in Learning Theory. Technical
Report CBCL Paper 264/ CSAIL-TR 2006-062  Massachusetts Institute of Technology  2006.
[6] A. Caponnetto and E. De Vito. Optimal Rates for Regularized Least-squares Algorithm. Foun-

dations of Computational Mathematics  7(3):331–368  2007.

[7] A. Caponnetto and Y. Yao. Cross-validation based Adaptation for Regularization Operators in

Learning Theory. Analysis and Applications  8(2):161–183  2010.

[8] H. Chun and S. Keles. Sparse Partial Least Squares for Simultaneous Dimension Reduction

and Variable Selection. Journal of the Royal Statistical Society B  72(1):3–25  2010.

[9] E. De Vito  L. Rosasco  A. Caponnetto  U. De Giovannini  and F. Odone. Learning from

Examples as an Inverse Problem. Journal of Machine Learning Research  6(1):883  2006.

[10] L. Gy¨orﬁ  M. Kohler  A. Krzyzak  and H. Walk. A Distribution-Free Theory of Nonparametric

Regression. Springer  2002.

[11] M. Hanke. Conjugate Gradient Type Methods for Linear Ill-posed Problems. Pitman Research

Notes in Mathematics Series  327  1995.

[12] L. Lo Gerfo  L. Rosasco  E. Odone  F.and De Vito  and A. Verri. Spectral Algorithms for

Supervised Learning. Neural Computation  20:1873–1897  2008.

[13] S. Mendelson and J. Neeman. Regularization in Kernel Learning. The Annals of Statistics 

38(1):526–565  2010.

[14] P. Naik and C.L. Tsai. Partial Least Squares Estimator for Single-index Models. Journal of the

Royal Statistical Society B  62(4):763–771  2000.

[15] A. S. Nemirovskii. The Regularizing Properties of the Adjoint Gradient Method in Ill-posed

Problems. USSR Computational Mathematics and Mathematical Physics  26(2):7–16  1986.

[16] C. S. Ong. Kernels: Regularization and Optimization. Doctoral dissertation  Australian Na-

tional University  2005.

[17] C. S. Ong  X. Mary  S. Canu  and A. J. Smola. Learning with Non-positive Kernels.

In
Proceedings of the 21st International Conference on Machine Learning  pages 639 – 646 
2004.

[18] R. Rosipal and L.J. Trejo. Kernel Partial Least Squares Regression in Reproducing Kernel

Hilbert Spaces. Journal of Machine Learning Research  2:97–123  2001.

[19] R. Rosipal  L.J. Trejo  and B. Matthews. Kernel PLS-SVC for Linear and Nonlinear Classiﬁ-
cation. In Proceedings of the Twentieth International Conference on Machine Learning  pages
640–647  Washington  DC  2003.

[20] I. Steinwart  D. Hush  and C. Scovel. Optimal Rates for Regularized Least Squares Regression.

In Proceedings of the 22nd Annual Conference on Learning Theory  pages 79–93  2009.

[21] S. Wold  H. Ruhe  H. Wold  and W.J. Dunn III. The Collinearity Problem in Linear Regression.
The Partial Least Squares (PLS) Approach to Generalized Inverses. SIAM Journal of Scientiﬁc
and Statistical Computations  5:735–743  1984.

[22] T. Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural

Computation  17(9):2077–2098  2005.

9

,Liming Wang
David Carlson
Miguel Rodrigues
David Wilcox
Robert Calderbank
Lawrence Carin