2010,Deep Coding Network,This paper proposes a principled extension of the traditional single-layer flat sparse coding scheme  where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically  it is shown that the deep coding approach yields improved performance in benchmark datasets.,Deep Coding Network

Yuanqing Lin† Tong Zhang‡

Shenghuo Zhu† Kai Yu†

†NEC Laboratories America  Cupertino  CA 95129

‡Rutgers University  Piscataway  NJ 08854

Abstract

This paper proposes a principled extension of the traditional single-layer ﬂat
sparse coding scheme  where a two-layer coding scheme is derived based on the-
oretical analysis of nonlinear functional approximation that extends recent results
for local coordinate coding. The two-layer approach can be easily generalized
to deeper structures in a hierarchical multiple-layer manner. Empirically  it is
shown that the deep coding approach yields improved performance in benchmark
datasets.

1 Introduction

Sparse coding has attracted signiﬁcant attention in recent years because it has been shown to be
effective for some classiﬁcation problems [12  10  9  13  11  14  2  5]. In particular  it has been em-
pirically observed that high-dimensional sparse coding plus linear classiﬁer is successful for image
classiﬁcation tasks such as PASCAL 2009 [7  15].

The empirical success of sparse coding can be justiﬁed by theoretical analysis [17]  which showed
that a modiﬁcation of sparse coding with added locality constraint  called local coordinate cod-
ing (LCC)  represents a new class of effective high dimensional non-linear function approximation
methods with sound theoretical guarantees. Speciﬁcally  LCC learns a nonlinear function in high
dimension by forming an adaptive set of basis functions on the data manifold  and it has nonlinear
approximation power. A recent extension of LCC with added local tangent directions [16] demon-
strated the possibility to achieve locally quadratic approximation power when the underlying data
manifold is relatively ﬂat. This also indicates that the nonlinear function approximation view of
sparse coding not only yields deeper theoretical understanding of its success  but also leads to im-
proved algorithms based on reﬁned analysis. This paper follows the same idea  where we propose a
principled extension of single-layer sparse coding based on theoretical analysis of a two level coding
scheme.

The algorithm derived from this approach has some advantages over the single-layer approach  and
can also be extended into multi-layer hierarchical systems. Such extension draws connection to
deep belief networks (DBN) [8]  and hence we call this approach deep coding network. Hierarchi-
cal sparse coding has two main advantages over its single-layer counter-part. First  at the intuitive
level  the ﬁrst layer (traditional single-layer basis) yields a crude description of the data at each ba-
sis function  and multi-layer basis functions provide a natural way to zoom into each single basis
for ﬁner local details — this intuition can be reﬂected more rigorously in our nonlinear function
approximation result. Due to the more localized zoom-in effect  it also alleviates the problem of
overﬁtting when many basis functions are needed. Second  it is computationally more efﬁcient than
ﬂat coding because we only need to look at locations in the second (or higher) layer corresponding
to basis functions with nonzero coefﬁcients in the ﬁrst (or previous) layer. Since sparse coding pro-
duces many zero coefﬁcients  the hierarchical structure signiﬁcantly eliminates many of the coding
computation. Moreover  instead of ﬁtting a single model with many variables as in a ﬂat single layer
approach  our proposal of multi-layer coding requires ﬁtting many small models separately  each

1

with a small number of parameters. In particular  ﬁtting the small models can be done in parallel 
e.g. using Hadoop  so that learning a fairly big number of codebooks can still be fast.

2 Sparse Coding and Nonlinear Function Approximation

This section reviews the nonlinear function approximation results of single-layer coding scheme in
[17]  and then presents our multi-layer extension. Since the result of [17] requires a modiﬁcation of
the traditional sparse coding scheme called local coordinate coding (LCC)  our analysis will rely on
a similar modiﬁcation.

Consider the problem of learning a nonlinear function f (x) in high dimension: x ∈ Rd with large
d. While there are many algorithms in traditional statistics that can learn such a function in low
dimension  when the dimensionality d is large compared to n  the traditional statistical methods will
suffer the so called “curse of dimensionality”. The recently popularized coding approach addresses
this issue. Speciﬁcally  it was theoretically shown in [17] that a speciﬁc coding scheme called Local
Coordinate Coding can take advantage of the underlying data manifold geometric structure in order
to learn a nonlinear function in high dimension and alleviate the curse of dimensionality problem.

The main idea of LCC  described in [17]  is to locally embed points on the underlying data manifold
into a lower dimensional space  expressed as coordinates with respect to a set of anchor points. The
main theoretical observation was relatively simple: it was shown in [17] that on the data manifold  a
nonlinear function can be effectively approximated by a globally linear function with respect to the
local coordinate coding. Therefore the LCC approach turns a very difﬁcult high dimensional nonlin-
ear learning problem into a much simpler linear learning problem  which can be effectively solved
using standard machine learning techniques such as regularized linear classiﬁers. This linearization
is effective because the method naturally takes advantage of the geometric information.

In order to describe the results more formally  we introduce a number of notations. First we denote
by k · k the Euclidean norm (2-norm) on Rd:

kxk = kxk2 = qx2

1 + · · · + x2
d.

Deﬁnition 2.1 (Smoothness Conditions) A function f (x) on Rd is (α  β  ν) Lipschitz smooth with
respect to a norm k · k if

k∇f (x)k ≤ α 

and

and

(cid:12)(cid:12)f (x′) − f (x) − ∇f (x)⊤(x′ − x)(cid:12)(cid:12) ≤ βkx′ − xk2 
(cid:12)(cid:12)f (x′) − f (x) − 0.5(∇f (x′) + ∇f (x))⊤(x′ − x)(cid:12)(cid:12)

≤νkx − x′k3 

where we assume α  β  ν ≥ 0.

These conditions have been used in [16]  and they characterize the smoothness of f under zero-th 
ﬁrst  and second order approximations. The parameter α is the Lipschitz constant of f (x)  which
is ﬁnite if f (x) is Lipschitz; in particular  if f (x) is constant  then α = 0. The parameter β is
the Lipschitz derivative constant of f (x)  which is ﬁnite if the derivative ∇f (x) is Lipschitz; in
particular  if ∇f (x) is constant (that is  f (x) is a linear function of x)  then β = 0. The parameter
ν is the Lipschitz Hessian constant of f (x)  which is ﬁnite if the Hessian of f (x) is Lipschitz;
in particular  if the Hessian ∇2f (x) is constant (that is  f (x) is a quadratic function of x)  then
ν = 0. In other words  these parameters measure different levels of smoothness of f (x): locally
when kx − x′k is small  α measures how well f (x) can be approximated by a constant function  β
measures how well f (x) can be approximated by a linear function in x  and ν measures how well
f (x) can be approximated by a quadratic function in x. For local constant approximation  the error
term αkx−x′k is the ﬁrst order in kx−x′k; for local linear approximation  the error term βkx−x′k2
is the second order in kx − x′k; for local quadratic approximation  the error term νkx − x′k3 is the
third order in kx − x′k. That is  if f (x) is smooth with relatively small α  β  ν  the error term
becomes smaller (locally when kx − x′k is small) if we use a higher order approximation.

2

Similar to the single-layer coordinate coding in [17]  here we deﬁne a two-layer coordinate coding
as the following.

Deﬁnition 2.2 (Coordinate Coding) A single-layer coordinate coding is a pair (γ 1  C 1)  where
C 1 ⊂ Rd is a set of anchor points (aka basis functions)  and γ is a map of x ∈ Rd to [γ 1
v (x)]v∈C 1 ∈
R|C 1| such that Pv∈C 1 γ 1

v (x) = 1. It induces the following physical approximation of x in Rd:

hγ 1 C 1(x) = X

v∈C 1

γ 1
v (x)v.

A two-layer coordinate coding (γ  C) consists of coordinate coding systems {(γ 1  C 1)} ∪
{(γ 2 v  C 2 v) : v ∈ C 1}. The pair (γ 1  C 1) is the ﬁrst layer coordinate coding  (γ 2 v  C 2 v) are
second layer coordinate-coding pairs that reﬁne the ﬁrst layer coding for every ﬁrst-layer anchor
point v ∈ C 1.

The performance of LCC is characterized in [17] using the following nonlinear function approxima-
tion result.

Lemma 2.1 (Single-layer LCC Nonlinear Function Approximation) Let (γ 1  C 1) be an arbi-
trary single-layer coordinate coding scheme on Rd. Let f be an (α  β  ν)-Lipschitz smooth function.
We have for all x ∈ Rd:

f (x) − X

v∈C 1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where wv = f (v) for v ∈ C 1.

wvγ 1

v (x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ α(cid:13)(cid:13)x − hγ 1 C 1(x)(cid:13)(cid:13) + β X

v∈C 1

|γ 1

v (x)|kv − xk2 

(1)

This result shows that a high dimensional nonlinear function can be globally approximated by a
linear function with respect to the single-layer coding [γ 1
v (x)]  with unknown linear coefﬁcients
[wv]v∈C 1 = [f (v)]v∈C 1  where the approximation on the right hand size is second order. This
bounds directly suggests the following learning method: for each x  we use its coding [γ 1
v (x)] ∈
R|C 1| as features. We then learn a linear function of the form Pv wvγ 1
v(x) using a standard linear
learning method such as SVM  where [wv] is the unknown coefﬁcient vector to be learned. The
optimal coding can be learned using unlabeled data by optimizing the right hand side of (1) over
unlabeled data.

In the same spirit  we can extend the above result on LCC by including additional layers. This leads
to the following bound.

Lemma 2.2 (Two-layer LCC Nonlinear Function Approximation) Let (γ  C) = {(γ 1  C 1)} ∪
{(γ 2 v  C 2 v) : v ∈ C 1} be an arbitrary two-layer coordinate coding on Rd. Let f be an (α  β  ν)-
Lipschitz smooth function. We have for all x ∈ Rd:

kf (x) − X

v∈C 1

wvγ 1

v(x) − X

v∈C 1

v (x) X
γ 1

u∈C 2 v

wv uγ 2 v

u (x)k

≤0.5αkx − hγ 1 C 1(x)k + 0.5α X

v∈C 1

v (x)|kx − hγ 2 v  C 2 v (x)k + ν X
|γ 1

v∈C 1

|γ 1

v (x)|kx − vk3 

(2)

where wv = f (v) for v ∈ C 1 and wv u = 0.5∇f (v)⊤(u − v) for u ∈ C 2 v  and

kf (x) − X

v∈C 1

v (x) X
γ 1

u∈C 2 v

wv uγ 2 v

u (x)k

≤α X

v∈C 1

v (x)|kx − hγ 2 v C 2 v (x)k + β X
|γ 1

v∈C 1

|γ 1

v(x)|kx − hγ 2 v C 2 v (x)k2

+ β X

v∈C 1

v (x)| X
|γ 1

u∈C 2 v

|γ 2 v

u (x)|ku − hγ 2 v C 2 v (x)k2 

(3)

where wv u = f (u) for u ∈ C 2 v.

3

Similar to the interpretation of Lemma 2.1  bounds in Lemma 2.2 implies that we can approximate
a nonlinear function f (x) with linear function of the form

X

v∈C 1

wvγ 1

v (x) + X

v∈C 1

wv uγ 1

v (x)γ 2 v

u (x) 

X

u∈C 2 v

v (x)]v∈C 1 and
u (x)]v∈C 1 u∈C 2 v form the feature vector. The coding can be learned from unlabeled data by

where [wv] and [wv u] are the unknown linear coefﬁcients to be learned  and [γ 1
[γ 2 v
minimizing the right hand side of (2) or (3).

Compare with the single-layer coding  we note that the second term on the right hand side of (1)
is replaced by the third term on the right hand side of (2). That is  the linear approximation power
of the single-layer coding scheme (with a quadratic error term) becomes quadratic approximation
power of the two-layer coding scheme (with a cubic error term). The ﬁrst term on the right hand
side of (1) is replaced by the ﬁrst two terms on the right hand of (2). If the manifold is relatively ﬂat 
then the error terms kx − hγ 1 C 1(x)k and kx − hγ 2 v C 2 v (x)k will be relatively small in comparison
to the second term on the right hand side of (1). In such case the two-layer coding scheme can
potentially improve the single-layer system signiﬁcantly. This result is similar to that of [16]  where
the second layer uses local PCA instead of another layer of nonlinear coding. However  the bound in
Lemma 2.2 is more reﬁned and speciﬁcally applicable to nonlinear coding. The bound in (2) shows
the potential of the two-layer coding scheme in achieving higher order approximation power than
single layer coding. Higher order approximation gives meaningful improvement when each |C 2 v|
is relatively small compared to |C 1|. On the other hand  if |C 1| is small but each |C 2 v| is relatively
large  then achieving higher order approximation does not lead to meaningful improvement. In such
case  the bound in (3) shows that the performance of the two-level coding is still comparable to that
of one-level coding scheme in (1). This is the situation where the 1st layer is mainly used to par-
tition the space (while its approximation accuracy is not important)  while the main approximation
power is achieved with the second layer. The main advantage of two-layer coding in this case is
to save computation. This is because instead of solving a single layer coding system with many
parameters  we can solve many smaller coding systems  each with a small number of parameters.
This is the situation when including nonlinearity in the second layer becomes useful  which means
that the deep-coding network approach in this paper has some advantage over [16] which can only
approximate linear function with local PCA in the second layer.

3 Deep Coding Network

We shall discuss the computational algorithm motivated by Lemma 2.2. While the two bounds (2)
and (3) consider different scenarios depending on the relative size of the ﬁrst layer versus the second
layer  in reality it is difﬁcult to differentiate and usually both bounds play a role at the same time.
Therefore we have to consider a mixed effect. Instead of minimizing one bound versus another  we
shall use them to motivate our algorithm  and design a method that accommodate the underlying
intuition reﬂected by the two bounds.

3.1 Two Layer Formulation

In the following  we let C 1 = {v1  . . .   vL1}  γ 1
j  C 2 vj = {vj 1  . . .   vj L2}  and
2 vj
j k  where L1 is the size of the ﬁrst-layer codebook  and L2 is the size of each
vj k (Xi) = γi
γ
individual codebook at the second layer. We take a layer-by-layer approach for training  where the
second layer is regarded as a reﬁnement of the ﬁrst layer  which is consistent with Lemma 2.2. In
the ﬁrst layer  we learn a simple sparse coding model with all data:

vj (Xi) = γi

[γ 1  C 1] = arg min
γ v

Xi −

L1

X

j=1

γi
jvj

2

2







(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

n

1
2


X


(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)



j ≥ 0 X

i=1

j

subject to γi

γi
j = 1  kvjk ≤ κ 

(4)

where κ is some constant  e.g.  if all Xi are normalized to have unit length  κ can be set to be
1. For convenience  we not only enforce sum-to-one-constraint on the sparse coefﬁcients  but also

4

j = 1 for all i. This presents a probability
impose nonnegative constraints so that Pj |γi
interpretation of the data  and allow us to approximate the following term on the right hand sides of
(2) and (3):

j| = Pj γi

γi
j

X

j

Xi −

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

L2

X

k=1

γi

j kvj k(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

≤ 
X

j

γi
j

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Xi −

1/2

.

L2

X

k=1

γi

j kvj k(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2


Note that neither sum to one or 1-norm regularization of coefﬁcients is needed in the derivation of
(2)  while such constraints are needed in (3). This means additional constraints may hurt perfor-
mance in the case of (2) although it may help in the case of (3). Since we don’t know which case
is the dominant effect  as a compromise we remove the sum-to-one constraint but put in 1-norm
regularization which is tunable. We still keep the positivity constraint for interpretability. This leads
to the following formulation for the second layer:

[γ 2 vj   C 2 vj ] = arg min
γ v

n



subject to γi

X

i=1

γi
j




1
2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Xi −

j k ≥ 0  kvj kk ≤ 1 

L2

X

k=1

γi

2

2

j kvj k(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

+ λ2

L2

X

k=1

γi
j k







(5)

where λ2 is a l1-norm sparsity regularization parameter controlling the sparseness of solutions. With
the codings on both layers  the sparse representation of Xi is (cid:2)sγi
j 1  γi
j L2](cid:3)j=1 ...L1
where s is a scaling factor balances the coding from the two different layers.

j 2  ...  γi

j  γi

j[γi

3.2 Multi-layer Extension

The two-level coding scheme can be easily extended to the third and higher layers. For example 
at the third layer  for each base vj k  the third-layer coding is to solve the following weighted opti-
mization:

[γj k

3   Cj k

3 ] = arg min
γ v

n


X

subject to γi

i=1

γi
j k




1
2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Xi −

L3

X

l=1

γi

j k lvj k l(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

2

2

+ λ3 X

l

γi
j k l







(6)

j k l ≥ 0  kvj k lk ≤ 1.

3.3 Optimization

The optimization problems in Equations (4) to (6) can be generally solved by alternating the follow-
ing two steps: 1) given current cookbook estimation v  compute the optimal sparse coefﬁcients γ;
2) given the new estimates of the sparse coefﬁcients  optimize the cookbooks.

Step 1 requires solving an independent optimization problem for each data sample  and it can be
computationally very expensive when there are many training examples. In such case  computa-
tional efﬁciency becomes an important issue. We developed some efﬁcient algorithms for solving
the optimizations problem in Step 1 by exploiting the fact that the solutions of the optimization
problems are sparse. The optimization problem in Step 1 of (4) can be posed as a nonnegative
quadratic programming problem with a single sum-to-one equality constraint. We employ an ac-
tive set method for this problem that easily handles the constraints [4]. Most importantly  since the
optimal solutions are very sparse  the active set method often gives the exact solution after a few
dozen of iterations. The optimization problem in (5) contains only nonnegative constraints (but not
the sum-to-one constraint)  for which we employ a pathwise projected Newton (PPN) method [3]
that optimizes a block of coordinates per iteration instead of one coordinate at a time in the active
set method. As a result  in typical sparse coding settings (for example  in the experiments that we
will present shortly in Section 4)  the PPN method is able to give the exact solution of a median size
(e.g. 2048 dimension) nonnegative quadratic programming problem in milliseconds.

Step 2 can be solved in its dual form  which is convex optimization with nonnegative constraints [9].
Since the dual problem contains only nonnegative constraints  we can still employ projected Newton
method.
It is known that the projected Newton method has superlinear convergence rate under

5

fairly mild conditions [3]. The computational cost in Step 2 is often negligible compared to the
computational cost in Step 1 when the cookbook size is no more than a few thousand.

A signiﬁcant advantage of the second layer optimization in our proposal is parallelization. As shown
in (5)  the second-layer sparse coding is decomposed into L1 independent coding problems  and thus
can be naturally parallelized. In our implementation  this is done through Hadoop.

4 Experiments

4.1 MNIST dataset

We ﬁrst demonstrate the effectiveness of the proposed deep coding scheme on the popular MNIST
benchmark data [1]. MNIST dataset consists of 60 000 training digits and 10 000 testing digits. In
our experiments of deep coding network  the entire training set is used to learn the ﬁrst-layer coding 
with codebook of size 64. For each of the 64 bases in the ﬁrst layer  a second-layer codebook was
learned – the deep coding scheme presented in the paper ensures that the codebook learning can
be done independently. We implemented a Hadoop parallel program that solved the 64 codebook
learning tasks in about an hour – which would have taken 64 hours on single machine. This shows
that easy parallelization is a very attractive aspect of the proposed deep coding scheme  especially
for large scale problems.

Table 1 shows the performance of deep coding network on MNIST compared to some previous
coding schemes. There are a number of interesting observations in these results. First  adding
an extra layer yields signiﬁcant improvement on classiﬁcation; e.g. for L1 = 512  the classiﬁcation
error rate for single layer LCC is 2.60% [17] while extended LCC achieves 1.98% [16] (the extended
LCC method in [16] may also be regarded as a two layer method but the second layer is linear); the
two-layer coding scheme here signiﬁcantly improves the performance with classiﬁcation error rate
of 1.51% . Second  the two-layer coding is less prone to overﬁtting than its single-layer counterpart.
In fact  for the single-layer coding  our experiment shows that further increasing the codebook size
will cause overﬁtting (e.g.  with L1 = 8192  the classiﬁcation error deteriorates to 1.78%).
In
contrast  the performance of two-layer coding still improves when the second-layer codebook is as
large as 512 (and the total codebook size is 64 × 512 = 32768  which is very high-dimensional
considering the total number of training data is only 60 000). This property is desirable especially
when high-dimensional representation is preferred in the case of using sparse coding plus linear
classiﬁer for classiﬁcations.

Figure 1 shows some ﬁrst-layer bases and their associated second-layer bases. We can see that the
second-layer bases provide deeper details that helps to further explain their ﬁrst layer parent basis;
on the other hand  the parent ﬁrst-layer basis provides an informative context for its child second-
layer bases. For example  in the seventh row in Fig. 1 where the ﬁrst-layer basis is like Digit 7  this
basis can come from Digit 7  Digit 9 or even Digit 4. Then  its second-layer bases help to further
explain the meaning of the ﬁrst-layer basis: in its associated second-layer bases  the ﬁrst two bases
in that row are parts of Digit 9 while the last basis in that row is a part of Digit ’4’. Meanwhile  the
ﬁrst-layer 7-like basis provides important context for its second-layer part-like bases – without the
ﬁrst-layer basis  the fragmented parts (like the ﬁrst two second-layer bases in that row) may not be
very informative. The zoomed-in details contained in deeper bases signiﬁcantly help a classiﬁer to
resolve difﬁcult examples  and interestingly  coarser details provide useful context for ﬁner details.

Number of bases (L1)
Local coordinate coding
Extended LCC

Single layer sparse coding
1024
2.17
1.82
Two-layer sparse coding

512
2.60
1.95

2048
1.79
1.78

4096
1.75
1.64

Number of bases (L2)
L1 = 64

64
1.85

128
1.69

256
1.53

512
1.51

Table 1: The classiﬁcation error rate (in %) on MNIST dataset with different sparse coding schemes.

6

First−layer bases

Second−layer bases

 

 

Figure 1: Example of bases from a two-layer coding network on MNIST data. For each row  the
ﬁrst image is a ﬁrst-layer basis  and the remaining images are its associated second-layer bases.
The colorbar is the same for all images  but the range it represents differs from image to image –
generally  the color of the background of a image represent zero value  and the colors above and
below that color respectively represent positive and negative values.

4.2 PASCAL 2007

The PASCAL 2007 dataset [6] consists of 20 categories of images such as airplanes  persons  cats 
tables  and so on. It consists of 2501 training images and 2510 validation images  and the task is to
classify an image into one or more of the 20 categories. Therefore  this task can be casted as training
20 binary classiﬁers. The critical issue is how to extract effective visual features from the images.
Among different methods  one particularly effective approach is to use sparse coding to derive a
codebook of low-level features (such as SIFT) and represent an image as a bag of visual words [15].
Here  we intend to learn two-layer hierarchical codebooks instead of single ﬂat codebook for the
bag-of-word image representation.

In our experiments  we ﬁrst sampled dense SIFT descriptors (each is represented by a 128×1 vector)
on each image using four scales  7 × 7  16 × 16  25 × 25 and 31 × 31 with stepsize of 4. Then 
the SIFT descriptors from all images (both training and validation images) were utilized to learn
ﬁrst-layer codebooks with different dimensions  L1 = 512  1024 and 2048. Then  given a ﬁrst-
layer codebook  for each basis in the codebook  we learned its second-layer codebook of size 64
by solving the weighted optimization in (5). Again  the second-layer codebook learning was done
in parallel using Hadoop. With the ﬁrst-layer and second-layer codebooks  each SIFT feature was
coded into a very high dimensional space: using L1 = 1024 as an example  the coding dimension

7

Dimension of the ﬁrst layer (L1)
Single-layer sparse coding
Two-layer sparse coding (L2=64)

512
42.7
51.1

1024
45.3
52.8

2048
48.4
53.3

Table 2: Average precision (in %) of classiﬁcation on PASCAL07 dataset using different sparse
coding schemes.

in total is 1024 + 1024 × 64 = 66  560. For each image  we employed 1 × 1  2 × 2 and 1 × 3
spatial pyramid matching with max-pooling. Therefore in the end  each image is represented by a
532  480(= 66  560×8)×1 high-dimensional vector for L1 = 1024. Table 2 shows the classiﬁcation
results. It is clear that the two-layer sparse coding performs signiﬁcantly better than its single-layer
counterpart.

We would like to point out that  although we simply employed max-pooling in the experiments  it
may not be the best pooling strategy for the hierarchical coding scheme presented in this paper. We
believe a better pooling scheme needs to take the hierarchical structure into account  but this remains
as an open problem and is one of our future work.

5 Conclusion

This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme 
where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional
approximation that extends recent results for local coordinate coding. The two-layer approach can be
easily generalized to deeper structures in a hierarchical multiple-layer manner. There are two main
advantages of multi-layer coding: it can potentially achieve better performance because the deeper
layers provide more details and structures; it is computationally more efﬁcient because coding are
decomposed into smaller problems. Experiment showed that the performance of two-layer coding
can signiﬁcantly improve that of single-layer coding.

For the future directions  it will be interesting to explore the deep coding network with more than
two layers. The formulation proposed in this paper grants a straightforward extension from two
layers to multiple layers. For small datasets like MNIST  the two-layer scheme seems to be already
very powerful. However  for more complicated data  deeper coding with multiple layers may be an
effective way for gaining ﬁner and ﬁner features. For example  the ﬁrst layer coding picks up some
large categories such as human  bikes  cups  and so on; then for the human category  the second-
layer coding may ﬁnd difference among adult  teenager  and senior person; and then the third layer
may ﬁnd even ﬁner features such as race feature at different ages.

References

[1] http://yann.lecun.com/exdb/mnist/.
[2] Samy Bengio  Fernando Pereira  Yoram Singer  and Dennis Strelow. Group sparse coding. In

NIPS’ 09  2009.

[3] D P. Bertsekas. Projected newton methods for optimization problems with simple constraints.

SIAM J. Control Optim.  20(2):221–246  1982.

[4] Dimitri P. Bertsekas. Nonlinear programming. Athena Scientiﬁc  2003.
[5] David Bradley and J. Andrew (Drew) Bagnell. Differentiable sparse coding. In Proceedings

of Neural Information Processing Systems 22  December 2008.

[6] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman.

The
PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.

[7] Mark Everingham. Overview and results of the classiﬁcation challenge. The PASCAL Visual

Object Classes Challenge Workshop at ICCV  2009.

[8] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural net-

works. Science  313(5786):504 – 507  July 2006.

8

[9] Honglak Lee  Alexis Battle  Rajat Raina  and Andrew Y. Ng. Efﬁcient sparse coding algo-

rithms. In Proceedings of the Neural Information Processing Systems (NIPS) 19  2007.

[10] Michael S. Lewicki and Terrence J. Sejnowski. Learning overcomplete representations. Neural

Computation  12:337–365  2000.

[11] J. Mairal  F. Bach  J. Ponce  G. Sapiro  and A. Zisserman. Supervised dictionary learning. In

NIPS’ 08  2008.

[12] B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning

a sparse code for nature images. Nature  381:607–609  1996.

[13] Rajat Raina  Alexis Battle  Honglak Lee  Benjamin Packer  and Andrew Y. Ng. Self-taught
learning: Transfer learning from unlabeled data. International Conference on Machine Learn-
ing  2007.

[14] Marc Aurelio Ranzato  Y-Lan Boureau  and Yann LeCun. Sparse feature learning for deep

belief networks. In NIPS’ 07  2007.

[15] Jianchao Yang  Kai Yu  Yihong Gong  and Thomas Huang. Linear spatial pyramid matching
In IEEE Conference on Computer Vision and

using sparse coding for image classiﬁcation.
Pattern Recognition  2009.

[16] Kai Yu and Tong Zhang. Improved local coordinate coding using local tangents. In ICML’ 09 

2010.

[17] Kai Yu  Tong Zhang  and Yihong Gong. Nonlinear learning using local coordinate coding. In

NIPS’ 09  2009.

9

,Mohammad Gheshlaghi azar
Alessandro Lazaric
Emma Brunskill
William Hoiles
Mihaela van der Schaar
Sagie Benaim
Lior Wolf