2007,Fitted Q-iteration in continuous action-space MDPs,We consider continuous state  continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by another policy. We study a variant of fitted Q-iteration  where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous theoretical analysis of this algorithm  proving what we believe is the first finite-time bounds for value-function based algorithms for continuous state- and action-space problems.,Fitted Q-iteration in continuous action-space MDPs

Andr´as Antos

Computer and Automation Research Inst.
of the Hungarian Academy of Sciences
Kende u. 13-17  Budapest 1111  Hungary

antos@sztaki.hu

R´emi Munos

SequeL project-team  INRIA Lille
59650 Villeneuve d’Ascq  France

remi.munos@inria.fr

Csaba Szepesv´ari∗

Department of Computing Science

University of Alberta

Edmonton T6G 2E8  Canada

szepesva@cs.ualberta.ca

Abstract

We consider continuous state  continuous action batch reinforcement learning
where the goal is to learn a good policy from a sufﬁciently rich trajectory gen-
erated by some policy. We study a variant of ﬁtted Q-iteration  where the greedy
action selection is replaced by searching for a policy in a restricted set of can-
didate policies by maximizing the average action values. We provide a rigorous
analysis of this algorithm  proving what we believe is the ﬁrst ﬁnite-time bound
for value-function based algorithms for continuous state and action problems.

1 Preliminaries

We will build on the results from [1  2  3] and for this reason we use the same notation as these
papers. The unattributed results cited in this section can be found in the book [4].
A discounted MDP is deﬁned by a quintuple (X  A  P  S  γ)  where X is the (possible inﬁnite)
state space  A is the set of actions  P : X × A → M(X ) is the transition probability kernel with
P (·|x  a) deﬁning the next-state distribution upon taking action a from state x  S(·|x  a) gives the
corresponding distribution of immediate rewards  and γ ∈ (0  1) is the discount factor. Here X is
a measurable space and M(X ) denotes the set of all probability measures over X . The Lebesgue-
measure shall be denoted by λ. We start with the following mild assumption on the MDP:
Assumption A1 (MDP Regularity) X is a compact subset of the dX -dimensional Euclidean space 
A is a compact subset of [−A∞  A∞]dA. The random immediate rewards are bounded by ˆRmax
rS(dr|x  a)  is uniformly bounded
and that the expected immediate reward function  r(x  a) =
by Rmax: (cid:107)r(cid:107)∞ ≤ Rmax.
A policy determines the next action given the past observations. Here we shall deal with stationary
(cid:80)∞
(Markovian) policies which choose an action in a stochastic way based on the last observation only.
The value of a policy π when it is started from a state x is deﬁned as the total expected discounted
t=0 γtRt|X0 = x]. Here
reward that is encountered while the policy is executed: V π(x) = Eπ [
Rt ∼ S(·|Xt  At) is the reward received at time step t  the state  Xt  evolves according to Xt+1 ∼
∗Also with: Computer and Automation Research Inst. of the Hungarian Academy of Sciences Kende u.

(cid:82)

13-17  Budapest 1111  Hungary.

1

(cid:80)∞
t=0 γtRt|X0 = x  A0 = a].

P (·|Xt  At)  where At is sampled from the distribution determined by π. We use Qπ : X ×A → R
to denote the action-value function of policy π: Qπ(x  a) = Eπ [
The goal is to ﬁnd a policy that attains the best possible values  V ∗(x) = supπ V π(x)  at all states
x ∈ X . Here V ∗ is called the optimal value function and a policy π∗ that satisﬁes V π∗(x) =
V ∗(x) for all x ∈ X is called optimal. The optimal action-value function Q∗(x  a) is Q∗(x  a) =
supπ Qπ(x  a). We say that a (deterministic stationary) policy π is greedy w.r.t. an action-value
function Q ∈ B(X × A)  and we write π = ˆπ(·; Q)  if  for all x ∈ X   π(x) ∈ argmaxa∈A Q(x  a).
(cid:82)
Under mild technical assumptions  such a greedy policy always exists. Any greedy policy w.r.t. Q∗
is optimal. For π : X → A we deﬁne its evaluation operator  T π : B(X × A) → B(X × A)  by
(cid:82)
X Q(y  π(y)) P (dy|x  a). It is known that Qπ = T πQπ. Further  if
(T πQ)(x  a) = r(x  a) + γ
we let the Bellman operator  T : B(X × A) → B(X × A)  deﬁned by (T Q)(x  a) = r(x  a) +
X supb∈A Q(y  b) P (dy|x  a) then Q∗ = T Q∗. It is known that V π and Qπ are bounded by
γ
Rmax/(1 − γ)  just like Q∗ and V ∗. For π : X → A  the operator Eπ : B(X × A) → B(X ) is
deﬁned by (EπQ)(x) = Q(x  π(x))  while E : B(X × A) → B(X ) is deﬁned by (EQ)(x) =
supa∈A Q(x  a).
(cid:82)
Throughout the paper F ⊂ {f : X × A → R} will denote a subset of real-valued functions over
(cid:82)
the state-action space X × A and Π ⊂ AX will be a set of policies. For ν ∈ M(X ) and f : X → R
X |f(x)|pν(dx). We simply write (cid:107)f(cid:107)ν for (cid:107)f(cid:107)2 ν.
measurable  we let (for p ≥ 1) (cid:107)f(cid:107)p
Further  we extend (cid:107)·(cid:107)ν to F by (cid:107)f(cid:107)2
X |f|2(x  a) dν(x) dλA(a)  where λA is the uniform
distribution over A. We shall use the shorthand notation νf to denote the integral
f(x)ν(dx). We
denote the space of bounded measurable functions with domain X by B(X ). Further  the space of
measurable functions bounded by 0 < K < ∞ shall be denoted by B(X ; K). We let (cid:107)·(cid:107)∞ denote
the supremum norm.

p ν =
ν =
A

(cid:82)

(cid:82)

2 Fitted Q-iteration with approximate policy maximization
We assume that we are given a ﬁnite trajectory  {(Xt  At  Rt)}1≤t≤N   generated by some stochastic
stationary policy πb  called the behavior policy: At ∼ πb(·|Xt)  Xt+1 ∼ P (·|Xt  At)  Rt ∼
S(·|Xt  At)  where πb(·|x) is a density with π0
The generic recipe for ﬁtted Q-iteration (FQI) [5] is

def= inf (x a)∈X×A πb(a|x) > 0.

(1)
where Regress is an appropriate regression procedure and Dk(Qk) is a dataset deﬁning a regression
problem in the form of a list of data-point pairs:

Qk+1 = Regress(Dk(Qk)) 

(cid:105)

(cid:190)

(cid:189)(cid:104)

Dk(Qk) =

(Xt  At)  Rt + γ max

b∈A Qk(Xt+1  b)

1≤t≤N

.1

(cid:82)

Fitted Q-iteration can be viewed as approximate value iteration applied to action-value func-
tions. To see this note that value iteration would assign the value (T Qk)(x  a) = r(x  a) +
maxb∈A Qk(y  b) P (dy|x  a) to Qk+1(x  a) [6]. Now  remember that the regression function for
γ
the jointly distributed random variables (Z  Y ) is deﬁned by the conditional expectation of Y given
Z: m(Z) = E [Y |Z]. Since for any ﬁxed function Q  E [Rt + γ maxb∈A Q(Xt+1  b)|Xt  At] =
(T Q)(Xt  At)  the regression function corresponding to the data Dk(Q) is indeed T Q and hence if
FQI solved the regression problem deﬁned by Qk exactly  it would simulate value iteration exactly.
However  this argument itself does not directly lead to a rigorous analysis of FQI: Since Qk is
obtained based on the data  it is itself a random function. Hence  after the ﬁrst iteration  the “target”
function in FQI becomes random. Furthermore  this function depends on the same data that is used
to deﬁne the regression problem. Will FQI still work despite these issues? To illustrate the potential
difﬁculties consider a dataset where X1  . . .   XN is a sequence of independent random variables 
which are all distributed uniformly at random in [0  1]. Further  let M be a random integer greater
than N which is independent of the dataset (Xt)N
t=1. Let U be another random variable  uniformly
distributed in [0  1]. Now deﬁne the regression problem by Yt = fM U (Xt)  where fM U (x) =
sgn(sin(2M 2π(x + U))). Then it is not hard to see that no matter how big N is  no procedure can

1Since the designer controls Qk  we may assume that it is continuous  hence the maximum exists.

2

estimate the regression function fM U with a small error (in expectation  or with high probability) 
even if the procedure could exploit the knowledge of the speciﬁc form of fM U . On the other hand 
if we restricted M to a ﬁnite range then the estimation problem could be solved successfully. The
example shows that if the complexity of the random functions deﬁning the regression problem is
uncontrolled then successful estimation might be impossible.
Amongst the many regression methods in this paper we have chosen to work with least-squares
methods. In this case Equation (1) takes the form

(cid:181)

(cid:183)

(cid:184)(cid:182)2

Qk+1 = argmin

Q∈F

1

πb(At|Xt)

Q(Xt  At) −

Rt + γ max

b∈A Qk(Xt+1  b)

.

(2)

N(cid:88)

t=1

We call this method the least-squares ﬁtted Q-iteration (LSFQI) method. Here we introduced the
weighting 1/πb(At|Xt) since we do not want to give more weight to those actions that are preferred
by the behavior policy.
Besides this weighting  the only parameter of the method is the function set F. This function set
should be chosen carefully  to keep a balance between the representation power and the number of
samples. As a speciﬁc example for F consider neural networks with some ﬁxed architecture. In
this case the function set is generated by assigning weights in all possible ways to the neural net.
Then the above minimization becomes the problem of tuning the weights. Another example is to use
linearly parameterized function approximation methods with appropriately selected basis functions.
In this case the weight tuning problem would be less demanding. Yet another possibility is to let F
be an appropriate restriction of a Reproducing Kernel Hilbert Space (e.g.  in a ball). In this case the
training procedure becomes similar to LS-SVM training [7].
As indicated above  the analysis of this algorithm is complicated by the fact that the new dataset
is deﬁned in terms of the previous iterate  which is already a function of the dataset. Another
complication is that the samples in a trajectory are in general correlated and that the bias introduced
by the imperfections of the approximation architecture may yield to an explosion of the error of the
procedure  as documented in a number of cases in  e.g.  [8].
Nevertheless  at least for ﬁnite action sets  the tools developed in [1  3  2] look suitable to show
that under appropriate conditions these problems can be overcome if the function set is chosen in
a judicious way. However  the results of these works would become essentially useless in the case
of an inﬁnite number of actions since these previous bounds grow to inﬁnity with the number of
actions. Actually  we believe that this is not an artifact of the proof techniques of these works  as
suggested by the counterexample that involved random targets. The following result elaborates this
point further:
Proposition 2.1. Let F ⊂ B(X × A). Then even if the pseudo-dimension of F is ﬁnite  the fat-
shattering function of

(cid:190)

(cid:189)

F∨
max =

can be inﬁnite over (0  1/2).2

VQ : VQ(·) = max

a∈A Q(·  a)  Q ∈ F

Without going into further details  let us just note that the ﬁniteness of the fat-shattering function is a
sufﬁcient and necessary condition for learnability and the ﬁniteness of the fat-shattering function is
implied by the ﬁniteness of the pseudo-dimension [9].The above proposition thus shows that without
imposing further special conditions on F  the learning problem may become infeasible.
One possibility is of course to discretize the action space  e.g.  by using a uniform grid. However  if
the action space has a really high dimensionality  this approach becomes unfeasible (even enumer-
ating 2dA points could be impossible when dA is large). Therefore we prefer alternate solutions.
Another possibility is to make the functions in F  e.g.  uniformly Lipschitz in their state coordinates.
Then the same property will hold for functions in F∨
max and hence by a classical result we can bound
the capacity of this set (cf. pp. 353–357 of [10]). One potential problem with this approach is that
this way it might be difﬁcult to get a ﬁne control of the capacity of the resulting set.

2The proof of this and the other results are given in the appendix  available in the extended version of this

paper  downloadable from http://hal.inria.fr/inria-00185311/en/.

3

In the approach explored here we modify the ﬁtted Q-iteration algorithm by introducing a policy
set Π and a search over this set for an approximately greedy policy in a sense that will be made
precise in a minute. Our algorithm thus has four parameters: F  Π  K  Q0. Here F is as before  Π
is a user-chosen set of policies (mappings from X to A)  K is the number of iterations and Q0 is an
initial value function (a typical choice is Q0 ≡ 0). The algorithm computes a sequence of iterates
(Qk  ˆπk)  k = 0  . . .   K  deﬁned by the following equations:

N(cid:88)
N(cid:88)
N(cid:88)

t=1

t=1

t=1

ˆπ0 = argmax

Q0(Xt  π(Xt)) 

π∈Π

Qk+1 = argmin

Q∈F

1

πb(At|Xt)

(cid:179)

Q(Xt  At) −(cid:163)

ˆπk+1 = argmax

Qk+1(Xt  π(Xt)).

π∈Π

Rt + γQk(Xt+1  ˆπk(Xt+1))

(cid:164)(cid:180)2

 

(3)

(4)

Thus  (3) is similar to (2)  while (4) deﬁnes the policy search problem. The policy search will
generally be solved by a gradient procedure or some other appropriate method. The cost of this step
will be primarily determined by how well-behaving the iterates Qk+1 are in their action arguments.
For example  if they were quadratic and if π was linear then the problem would be a quadratic
optimization problem. However  except for special cases3 the action value functions will be more
complicated  in which case this step can be expensive. Still  this cost could be similar to that of
searching for the maximizing actions for each t = 1  . . .   N if the approximately maximizing actions
are similar across similar states.
This algorithm  which we could also call a ﬁtted actor-critic algorithm  will be shown to overcome
the above mentioned complexity control problem provided that the complexity of Π is controlled
appropriately. Indeed  in this case the set of possible regression problems is determined by the set

and the proof will rely on controlling the complexity of F∨

Π by selecting F and Π appropriately.

F∨
Π = { V : V (·) = Q(·  π(·))  Q ∈ F  π ∈ Π}  

3 The main theoretical result

3.1 Outline of the analysis

In order to gain some insight into the behavior of the algorithm  we provide a brief summary of its
error analysis. The main result will be presented subsequently. For f Q ∈ F and a policy π  we
deﬁne the tth TD-error as follows:

dt(f; Q  π) = Rt + γQ(Xt+1  π(Xt+1)) − f(Xt  At).

Further  we deﬁne the empirical loss function by

ˆLN (f; Q  π) =

1
N

N(cid:88)

t=1

t (f; Q  π)
d2

λ(A)πb(At|Xt)  

where the normalization with λ(A) is introduced for mathematical convenience. Then (3) can be
written compactly as Qk+1 = argminf∈F ˆLN (f; Qk  ˆπk).
The algorithm can then be motivated by the observation that for any f Q  and π  ˆLN (f; Q  π) is an
unbiased estimate of

(5)
where the ﬁrst term is the error we are interested in and the second term captures the variance of the
random samples:

L(f; Q  π) def= (cid:107)f − T πQ(cid:107)2

ν + L∗(Q  π) 

(cid:90)

L∗(Q  π) =

E [Var [R1 + γQ(X2  π(X2))|X1  A1 = a]] dλA(a).

A

3Linear quadratic regulation is such a nice case. It is interesting to note that in this special case the obvious

choices for F and Π yield zero error in the limit  as can be proven based on the main result of this paper.

4

This result is stated formally by E

ˆLN (f; Q  π)

= L(f; Q  π).

(cid:104)

(cid:105)

is

(5)

the

variance

independent

=
term in
Since
argminf∈F (cid:107)f − T πQ(cid:107)2
ν. Thus  if ˆπk were greedy w.r.t. Qk then argminf∈F L(f; Qk  ˆπk) =
argminf∈F (cid:107)f − T Qk(cid:107)2
ν. Hence we can still think of the procedure as approximate value iteration
over the space of action-value functions  projecting T Qk using empirical risk minimization on the
space F w.r.t. (cid:107)·(cid:107)ν distances in an approximate manner. Since ˆπk is only approximately greedy  we
will have to deal with both the error coming from the approximate projection and the error coming
from the choice of ˆπk. To make this clear  we write the iteration in the form

argminf∈F L(f; Q  π)

of

f 

Qk+1 = T ˆπk Qk + ε(cid:48)

k = T Qk + ε(cid:48)

k + (T ˆπk Qk − T Qk) = T Qk + εk 

k is the error committed while computing T ˆπk Qk  ε(cid:48)(cid:48)

def= T ˆπk Qk − T Qk is the error commit-
where ε(cid:48)
ted because the greedy policy is computed approximately and εk = ε(cid:48)
k is the total error of step
k. Hence  in order to show that the procedure is well behaved  one needs to show that both errors are
controlled and that when the errors are propagated through these equations  the resulting error stays
controlled  too. Since we are ultimately interested in the performance of the policy obtained  we
will also need to show that small action-value approximation errors yield small performance losses.
For these we need a number of assumptions that concern either the training data  the MDP  or the
function sets used for learning.

k + ε(cid:48)(cid:48)

k

3.2 Assumptions

3.2.1 Assumptions on the training data

We shall assume that the data is rich  is in a steady state  and is fast-mixing  where  informally 
mixing means that future depends weakly on the past.
Assumption A2 (Sample Path Properties) Assume that {(Xt  At  Rt)}t=1 ... N is the sample path
of πb  a stochastic stationary policy. Further  assume that {Xt} is strictly stationary (Xt ∼ ν ∈
M(X )) and exponentially β-mixing with the actual rate given by the parameters (β  b  κ).4 We
further assume that the sampling policy πb satisﬁes π0 = inf (x a)∈X×A πb(a|x) > 0.

The β-mixing property will be used to establish tail inequalities for certain empirical processes.5
Note that the mixing coefﬁcients do not need to be known. In the case when no mixing condition is
satisﬁed  learning might be impossible. To see this just consider the case when X1 = X2 = . . . =
XN . Thus  in this case the learner has many copies of the same random variable and successful
generalization is thus impossible. We believe that the assumption that the process is in a steady state
is not essential for our result  as when the process reaches its steady state quickly then (at the price
of a more involved proof) the result would still hold.

3.2.2 Assumptions on the MDP

In order to prevent the uncontrolled growth of the errors as they are propagated through the updates 
we shall need some assumptions on the MDP. A convenient assumption is the following one [11]:
Assumption A3 (Uniformly stochastic transitions) For all x ∈ X and a ∈ A  assume that
P (·|x  a) is absolutely continuous w.r.t. ν and the Radon-Nikodym derivative of P w.r.t. ν is bounded
uniformly with bound Cν: Cν
Note that by the deﬁnition of measure differentiation  Assumption A3 means that P (·|x  a) ≤
Cνν(·). This assumption essentially requires the transitions to be noisy. We will also prove (weaker)
results under the following  weaker assumption:

(cid:176)(cid:176)(cid:176)∞ < +∞.

(cid:176)(cid:176)(cid:176) dP (·|x a)

def= supx∈X  a∈A

dν

4For the deﬁnition of β-mixing  see e.g. [2].
5We say “empirical process” and “empirical measure”  but note that in this work these are based on depen-

dent (mixing) samples.

5

Assumption A4 (Discounted-average concentrability of future-state distributions) Given ρ 
ν  m ≥ 1 and an arbitrary sequence of stationary policies {πm}m≥1  assume that the future-
(cid:80)
state distribution ρP π1P π2 . . . P πm is absolutely continuous w.r.t. ν. Assume that c(m) def=
(cid:80)
m≥1 mγm−1c(m) < +∞. We shall call Cρ ν
def=
supπ1 ... πm
satisﬁes
m≥1 mγm−1c(m)  (1 − γ)
m≥1 γmc(m)
max
the discounted-average concentra-
bility coefﬁcient of the future-state distributions.

(cid:176)(cid:176)(cid:176) d(ρP π1 P π2 ...P πm )
(1 − γ)2(cid:80)

(cid:176)(cid:176)(cid:176)∞

(cid:169)

(cid:170)

dν

The number c(m) measures how much ρ can get ampliﬁed in m steps as compared to the reference
distribution ν. Hence  in general we expect c(m) to grow with m. In fact  the condition that Cρ µ is
ﬁnite is a growth rate condition on c(m). Thanks to discounting  Cρ µ is ﬁnite for a reasonably large
class of systems (see the discussion in [11]).
A related assumption is needed in the error analysis of the approximate greedy step of the algorithm:
Assumption A5 (The random policy “makes no peak-states”) Consider the distribution µ = (ν×
λA)P which is the distribution of a state that results from sampling an initial state according to ν and
then executing an action which is selected uniformly at random.6 Then Γν = (cid:107)dµ/dν(cid:107)∞ < +∞.
Note that under Assumption A3 we have Γν ≤ Cν. This (very mild) assumption means that after
one step  starting from ν and executing this random policy  the probability of the next state being in
a set is upper bounded by Γν-times the probability of the starting state being in the same set.
Besides  we assume that A has the following regularity property: Let Py(a  h  ρ)
(a(cid:48)  v) ∈ RdA+1 : (cid:107)a − a(cid:48)(cid:107)1 ≤ ρ  0 ≤ v/h ≤ 1 − (cid:107)a − a(cid:48)(cid:107)1 /ρ
h and base given by the (cid:96)1-ball B(a  ρ) def=

(cid:170)
a(cid:48) ∈ RdA : (cid:107)a − a(cid:48)(cid:107)1 ≤ ρ

def=
denote the pyramid with hight

centered at a.

(cid:169)

(cid:169)

(cid:170)

Assumption A6 (Regularity of the action space) We assume that there exists α > 0  such that for
all a ∈ A  for all ρ > 0 

λ(Py(a  1  ρ) ∩ (A × R))

λ(Py(a  1  ρ))

≥ min

λ(A)

λ(B(a  ρ))

α 

(cid:181)

(cid:182)

.

For example  if A is an (cid:96)1-ball itself  then this assumption will be satisﬁed with α = 2−dA.
Without assuming any smoothness of the MDP  learning in inﬁnite MDPs looks hard (see  e.g. 
[12  13]). Here we employ the following extra condition:

Assumption A7 (Lipschitzness of the MDP in the actions) Assume that the transition probabilities
and rewards are Lipschitz w.r.t. their action variable  i.e.  there exists LP   Lr > 0 such that for all
(x  a  a(cid:48)) ∈ X × A × A and measurable set B of X  
|P (B|x  a) − P (B|x  a(cid:48))| ≤ LP (cid:107)a − a(cid:48)(cid:107)1  

|r(x  a) − r(x  a(cid:48))| ≤ Lr (cid:107)a − a(cid:48)(cid:107)1 .

Note that previously Lipschitzness w.r.t. the state variables was used  e.g.  in [11] to construct con-
sistent planning algorithms.

3.2.3 Assumptions on the function sets used by the algorithm

These assumptions are less demanding since they are under the control of the user of the algorithm.
However  the choice of these function sets will greatly inﬂuence the performance of the algorithm 
as we shall see it from the bounds. The ﬁrst assumption concerns the class F:
Assumption A8 (Lipschitzness of candidate action-value functions) Assume F ⊂ B(X × A)
and that any elements of F is uniformly Lipschitz in its action-argument in the sense that |Q(x  a)−
Q(x  a(cid:48))| ≤ LA (cid:107)a − a(cid:48)(cid:107)1 holds for any x ∈ X   a a(cid:48) ∈ A  and Q ∈ F.
6Remember that λA denotes the uniform distribution over the action set A.

6

We shall also need to control the capacity of our function sets. We assume that the reader is familiar
with the concept of VC-dimension.7 Here we use the pseudo-dimension of function sets that builds
upon the concept of VC-dimension:
Deﬁnition 3.1 (Pseudo-dimension). The pseudo-dimension VF + of F is deﬁned as the VC-
dimension of the subgraphs of functions in F (hence it is also called the VC-subgraph dimension of
F).
Since A is multidimensional  we deﬁne VΠ+ to be the sum of the pseudo-dimensions of the coordi-
nate projection spaces  Πk of Π:

VΠ+ =

  Πk = { πk : X → R : π = (π1  . . .   πk  . . .   πdA) ∈ Π} .

VΠ+

k

dA(cid:88)

k=1

Now we are ready to state our assumptions on our function sets:
Assumption A9 (Capacity of the function and policy sets) Assume that F ⊂ B(X × A; Qmax)
for Qmax > 0 and VF + < +∞. Also  A ⊂ [−A∞  A∞]dA and VΠ+ < +∞.
Besides their capacity  one shall also control the approximation power of the function sets involved.
Let us ﬁrst consider the policy set Π. Introduce
e∗(F  Π) = sup
Q∈F

ν(EQ − EπQ).

inf
π∈Π

inf π∈Π ν(EQ − EπQ) measures the quality of approximating νEQ by νEπQ. Hence 
Note that
e∗(F  Π) measures the worst-case approximation error of νEQ as Q is changed within F. This can
be made small by choosing Π large.
Another related quantity is the one-step Bellman-error of F w.r.t. Π. This is deﬁned as follows: For
a ﬁxed policy π  the one-step Bellman-error of F w.r.t. T π is deﬁned as
Q(cid:48)∈F (cid:107)Q(cid:48) − T πQ(cid:107)ν .
inf

E1(F; π) = sup
Q∈F

Taking again a pessimistic approach  the one-step Bellman-error of F is deﬁned as

E1(F  Π) = sup
π∈Π

E1(F; π).

Typically by increasing F  E1(F  Π) can be made smaller (this is discussed at some length in
[3]). However  it also holds for both Π and F that making them bigger will increase their capacity
(pseudo-dimensions) which leads to an increase of the estimation errors. Hence  F and Π must be
selected to balance the approximation and estimation errors  just like in supervised learning.

3.3 The main result
Theorem 3.2. Let πK be a greedy policy w.r.t. QK  i.e. πK(x) ∈ argmaxa∈A QK(x  a). Then
under Assumptions A1  A2  and A5–A9  for all δ > 0 we have with probability at least 1 − δ: given
Assumption A3 (respectively A4)  (cid:107)V ∗ − V πK(cid:107)∞ (resp. (cid:107)V ∗ − V πK(cid:107)1 ρ)  is bounded by


E1(F  Π) + e∗(F  Π) +

C

(log N + log(K/δ))

κ+1
4κ

N 1/4

 1

dA+1

+ γK

  

where C depends on dA  VF +  (VΠ+
Qmax  Rmax  ˆRmax  and A∞. In particular  C scales with V
plays the role of the “combined effective” dimension of F and Π.

k=1  γ  κ  b  β  Cν (resp. Cρ ν)  Γν  LA  LP  Lr  α  λ(A)  π0 
)dA
4κ(dA+1)   where V = 2VF + + VΠ+

κ+1

k

7Readers not familiar with VC-dimension are suggested to consult a book  such as the one by Anthony and

Bartlett [14].

7

4 Discussion

We have presented what we believe is the ﬁrst ﬁnite-time bounds for continuous-state and action-
space RL that uses value functions. Further  this is the ﬁrst analysis of ﬁtted Q-iteration  an algorithm
that has proved to be useful in a number of cases  even when used with non-averagers for which no
previous theoretical analysis existed (e.g.  [15  16]). In fact  our main motivation was to show that
there is a systematic way of making these algorithms work and to point at possible problem sources
the same time. We discussed why it can be difﬁcult to make these algorithms work in practice. We
suggested that either the set of action-value candidates has to be carefully controlled (e.g.  assuming
uniform Lipschitzness w.r.t. the state variables)  or a policy search step is needed  just like in actor-
critic algorithms. The bound in this paper is similar in many respects to a previous bound of a
Bellman-residual minimization algorithm [2].
It looks that the techniques developed here can be
used to obtain results for that algorithm when it is applied to continuous action spaces. Finally 
although we have not explored them here  consistency results for FQI can be obtained from our
results using standard methods  like the methods of sieves. We believe that the methods developed
here will eventually lead to algorithms where the function approximation methods are chosen based
on the data (similar to adaptive regression methods) so as to optimize performance  which in our
opinion is one of the biggest open questions in RL. Currently we are exploring this possibility.

Acknowledgments
Andr´as Antos would like to acknowledge support for this project from the Hungarian Academy of Sciences
(Bolyai Fellowship). Csaba Szepesv´ari greatly acknowledges the support received from the Alberta Ingenuity
Fund  NSERC  the Computer and Automation Research Institute of the Hungarian Academy of Sciences.
References
[1] A. Antos  Cs. Szepesv´ari  and R. Munos. Learning near-optimal policies with Bellman-residual mini-

mization based ﬁtted policy iteration and a single sample path. In COLT-19  pages 574–588  2006.

[2] A. Antos  Cs. Szepesv´ari  and R. Munos. Learning near-optimal policies with Bellman-residual mini-

mization based ﬁtted policy iteration and a single sample path. Machine Learning  2007. (accepted).

[3] A. Antos  Cs. Szepesv´ari  and R. Munos. Value-iteration based ﬁtted policy iteration: learning with a

single trajectory. In IEEE ADPRL  pages 330–337  2007.

[4] D. P. Bertsekas and S.E. Shreve. Stochastic Optimal Control (The Discrete Time Case). Academic Press 

New York  1978.

[5] D. Ernst  P. Geurts  and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine

Learning Research  6:503–556  2005.

[6] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. Bradford Book. MIT Press  1998.
[7] N. Cristianini and J. Shawe-Taylor. An introduction to support vector machines (and other kernel-based

learning methods). Cambridge University Press  2000.

[8] J.A. Boyan and A.W. Moore. Generalization in reinforcement learning: Safely approximating the value

function. In NIPS-7  pages 369–376  1995.

[9] P.L. Bartlett  P.M. Long  and R.C. Williamson. Fat-shattering and the learnability of real-valued functions.

Journal of Computer and System Sciences  52:434–452  1996.

[10] A.N. Kolmogorov and V.M. Tihomirov. -entropy and -capacity of sets in functional space. American

Mathematical Society Translations  17(2):277–364  1961.

[11] R. Munos and Cs. Szepesv´ari. Finite time bounds for sampling based ﬁtted value iteration. Technical
report  Computer and Automation Research Institute of the Hungarian Academy of Sciences  Kende u.
13-17  Budapest 1111  Hungary  2006.

[12] A.Y. Ng and M. Jordan. PEGASUS: A policy search method for large MDPs and POMDPs. In Proceed-

ings of the 16th Conference in Uncertainty in Artiﬁcial Intelligence  pages 406–415  2000.

[13] P.L. Bartlett and A. Tewari. Sample complexity of policy search with known dynamics. In NIPS-19. MIT

Press  2007.

[14] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University

Press  1999.

[15] M. Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data efﬁcient neural reinforcement

learning method. In 16th European Conference on Machine Learning  pages 317–328  2005.

[16] S. Kalyanakrishnan and P. Stone. Batch reinforcement learning in a complex domain. In AAMAS-07 

2007.

8

,Hao Yu
Michael Neely
Xiaohan Wei