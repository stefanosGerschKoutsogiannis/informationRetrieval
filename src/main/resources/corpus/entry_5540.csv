2019,Theoretical Analysis of Adversarial Learning: A Minimax Approach,In this paper  we propose a general theoretical method for analyzing the risk bound in the presence of adversaries. Specifically  we try to fit the adversarial learning problem into the minimax framework. We first show that the original adversarial learning problem can be transformed into a minimax statistical learning problem by introducing a transport map between distributions. Then  we prove a new risk bound for this minimax problem in terms of covering numbers under a weak version of Lipschitz condition. Our method can be applied to multi-class classification and popular loss functions including the hinge loss and ramp loss. As some illustrative examples  we derive the adversarial risk bounds for SVMs and deep neural networks  and our bounds have two data-dependent terms  which can be optimized for achieving adversarial robustness.,Theoretical Analysis of Adversarial Learning:

A Minimax Approach

1UBTECH Sydney AI Centre  School of Computer Science  The University of Sydney  Australia

Zhuozhuo Tu1  Jingwei Zhang2 1  Dacheng Tao1

2Department of Computer Science and Engineering  HKUST  Hong Kong

zhtu3055@uni.sydney.edu.au  jzhangey@cse.ust.hk  dacheng.tao@sydney.edu.au

Abstract

In this paper  we propose a general theoretical method for analyzing the risk
bound in the presence of adversaries. Speciﬁcally  we try to ﬁt the adversarial
learning problem into the minimax framework. We ﬁrst show that the original
adversarial learning problem can be transformed into a minimax statistical learning
problem by introducing a transport map between distributions. Then  we prove
a new risk bound for this minimax problem in terms of covering numbers under
a weak version of Lipschitz condition. Our method can be applied to multi-class
classiﬁcation and popular loss functions including the hinge loss and ramp loss. As
some illustrative examples  we derive the adversarial risk bounds for SVMs and
deep neural networks  and our bounds have two data-dependent terms  which can
be optimized for achieving adversarial robustness.

1

Introduction

Machine learning models  especially deep neural networks  have achieved impressive performance
across a variety of domains including image classiﬁcation  natural language processing  and speech
recognition. However  these techniques can easily be fooled by adversarial examples  i.e.  carefully
perturbed input samples aimed to cause misclassiﬁcation during the test phase. This phenomenon was
ﬁrst studied in spam ﬁltering [14  31  32] and has attracted considerable attention since 2014  when
Szegedy et al. [42] noticed that small perturbations in images can cause misclassiﬁcation in neural
network classiﬁers. Since then  there has been considerable focus on developing adversarial attacks
against machine learning algorithms [21  9  8  4  44]  and  in response  many defense mechanisms
have also been proposed to counter these attacks [22  20  15  41  33]. These works focus on creating
optimization-based robust algorithms  but their generalization performance under adversarial input
perturbations is still not fully understood.
Schmidt et al. [38] recently discussed the generalization problem in the adversarial setting and
showed that the sample complexity of learning a speciﬁc distribution in the presence of l∞-bounded
adversaries increases by an order of
d for all classiﬁers. The same paper recognized that deriving
the agnostic-distribution generalization bound remained an open problem [38]. In a subsequent
study  Cullina et al. [13] extended the standard PAC-learning framework to the adversarial setting by
deﬁning a corrupted hypothesis class and showed that the VC dimension of this corrupted hypothesis
class for halfspace classiﬁers which controlled the sample complexity does not increase in the
presence of an adversary. While their work provided a theoretical understanding of the problem of
learning with adversaries  it had two limitations. First  their results could only be applied to binary
problems  whereas in practice we usually need to handle multi-class problems. Second  the 0-1 loss
function used in their work is not convex and thus very hard to optimize.
In this paper  we propose a general theoretical method for analyzing generalization performance in
the presence of adversaries. In particular  we ﬁt the adversarial learning problem into the minimax

√

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

framework [28]. In contrast to traditional statistical learning  where the underlying data distribution
P is unknown but ﬁxed  the minimax framework considers the uncertainty about the distribution P
by introducing an ambiguity set and then aims to minimize the risk with respect to the worst-case
distribution in this set. Motivated by Lee & Raginsky [28]  we ﬁrst note that the adversarial expected
risk over a distribution P is equivalent to the standard expected risk under a new distribution P (cid:48).
Since this new distribution is not ﬁxed and depends on the hypothesis  we instead consider the worst
case. In this way  the original adversarial learning problem is reduced to a minimax problem  and we
use the minimax approach to derive the risk bound for the adversarial expected risk. Our contributions
can be summarized as follows.

• We propose a general method for analyzing the risk bound in the presence of adversaries.
Our method is general in several respects. First  the adversary we consider is general and
encompasses all lq bounded adversaries. Second  our method can be applied to multi-class
problems and commonly used loss functions such as the hinge loss and ramp loss  whereas
Cullina et al. [13] only considered the binary classiﬁcation problem and the 0-1 loss.

• We prove a new bound for the local worst-case risk under a weak version of Lipschitz
condition. Our bound is always better than that of Lee & Raginsky [29]  and can recover the
usual risk bound by setting the radius B of the Wasserstein ball to 0  whereas they give a
B-free bound.

• We derive the adversarial risk bounds for SVMs and deep neural networks. Our bounds
have two data-dependent terms  suggesting that minimizing the sum of the two terms can
help achieve adversarial robustness.

The remainder of this paper is structured as follows. In Section 2  we discuss related works. Section 3
formally deﬁnes the problem  and we present our theoretical method in Section 4. The adversarial
risk bounds for SVMs and neural networks are described in Section 5  and we conclude and discuss
future directions in Section 6.

2 Related work

Our work leverages some of the beneﬁts of statistical machine learning  summarized as follows.

2.1 Generalization in supervised learning

Generalization is a central problem in supervised learning  and the generalization capability of
learning algorithms has been extensively studied. Here we review the salient aspects of generalization
in supervised learning relevant to this work.
Two main approaches are used to analyze the generalization bound of a learning algorithm. The ﬁrst
is based on the complexity of the hypothesis class  such as the VC dimension [45  46] for binary
classiﬁcation  Rademacher and Gaussian complexities [7  5]  and the covering number [53  52  6].
Note that hypothesis complexity-based analyses of generalization error are algorithm independent
and consider the worst-case generalization over all functions in the hypothesis class. In contrast 
the second approach is based on the properties of a learning algorithm and is therefore algorithm
dependent. The properties characterizing the generalization of a learning algorithm include  for
example  algorithmic stability [11  39  30]  robustness [50]  and algorithmic luckiness [24]. Some
other methods exist for analyzing the generalization error in machine learning such as the PAC-
Bayesian approach [35  2]  compression-based bounds [27  3]  and information-theoretic approaches
[49  1  37].

2.2 Minimax statistical learning

In contrast to standard empirical risk minimization in supervised learning  where test data follow the
same distribution as training data  minimax statistical learning arises in problems of distributionally
robust learning [16  18  28  29  40] and minimizes the worst-case risk over a family of probability
distributions. Thus  it can be applied to the learning setting in which the test data distribution differs
from that of the training data  such as in domain adaptation and transfer learning [12]. In particular 
Gao & Kleywegt [18] proposed a dual representation of worst-case risk over the ambiguity set of
probability distributions  which was given by balls in Wasserstein space. Then  Lee & Raginsky

2

[28] derived the risk bound for minimax learning by exploiting the dual representation of worst-case
risk. However  their minimax risk bound would go to inﬁnity and thus become vacuous as B → 0.
Despite that the same authors later presented a new bound [29] by imposing a Lipschitz assumption
to avoid this problem  their new bound was B-free and cannot recover the usual risk bound by setting
B = 0. Sinha et al. [40] also provided a similar upper bound on the worst-case population loss over
distributions deﬁned by the Wasserstein metric via a Lagrangian penalty formulation  and their bound
was efﬁciently computable by a principled adversarial training procedure  which provably certiﬁed
distributional robustness. However their training procedure required that the penalty parameter should
be large enough and thus can only achieve a small amount of robustness. Here we improve on the
results in Lee & Raginsky [28  29] and present a new risk bound for the minimax problem.

2.3 Learning with adversaries

The existence of adversaries during the test phase of a learning algorithm may render predictions
made by learning system unthrustworthy. There is extensive literature on analysis of adversarial
robustness [47  17  23  19] and design of provable defense against adversarial attacks[48  36  33  40] 
in contrast to the relatively limited literature on risk bound analysis of adversarial learning. A
comprehensive review of works on adversarial machine learning can be found in Biggio & Roli
[10]. Concurrently to our work  Khim & Loh [25] and Yin et al. [51] provided different approaches
to deriving adversarial risk bounds. Khim & Loh [25] derived adversarial risk bounds for linear
classiﬁers and neural networks using a method called supremum transform. However  their approach
can only be applied to binary classiﬁcation. Yin et al. [51] gave similar adversarial risk bounds
through the lens of Rademacher complexity. Although they provided risk bounds in multi-class
setting  their work focused on l∞ adversarial attacks and was limited to one-hidden layer ReLU
neural networks. After the initial preprint of this paper  Khim & Loh [26] extended their method to
multi-class setting by considering the binary supremum transform on each component of classiﬁer 
which as a result incurred an extra factor of the number of classes in their bound. Instead we used
covering number analysis to derive the multi-class bound  which can avoid explicit dependence on
this number.

3 Problem setup
We consider a standard statistical learning framework. Let Z = X × Y be a measurable instance
space where X and Y represent feature and label spaces  respectively. We assume that examples
are independently and identically distributed according to some ﬁxed but unknown distribution P .
The learning problem is then formulated as follows. The learner considers a class H of hypothesis
h : X → Y(cid:48) where Y(cid:48) sometimes differs from Y and a loss function l : Y(cid:48) × Y → R+. The learner
receives n training examples denoted by S = ((x1  y1)  (x2  y2) ···   (xn  yn)) drawn i.i.d. from
P and tries to select a hypothesis h ∈ H that has a small expected risk. However  in the presence
of adversaries  there will be imperceptible perturbations to the input of examples  which are called
adversarial examples. Throughout this paper  we assume that the adversarial examples are generated
by adversarially choosing an example from neighborhood N (x) = {x(cid:48) : x(cid:48) − x ∈ B} where B is
a nonempty set. Note that the deﬁnition of N (x) is very general and encompasses all lq-bounded
adversaries. We next give the formal deﬁnition of adversarial expected and empirical risk to measure
the learner’s performance in the presence of adversaries.
Deﬁnition 1. (Adversarial Expected Risk). The adversarial expected risk of a hypothesis h ∈ H over
the distribution P in the presence of an adversary constrained by B is
l(h(x(cid:48))  y)].

RP (h B) = E(x y)∼P [ max
x(cid:48)∈N (x)

If B is the zero-dimensional space {0}  then the adversarial expected risk will reduce to the standard
expected risk without an adversary. Since the true distribution is usually unknown  we instead use the
empirical distribution to approximate the true distribution  which is equal to (xi  yi) with probability
1/n for each i ∈ {1 ···   n}. That gives us the following deﬁnition of adversarial empirical risk.
Deﬁnition 2. (Adversarial Empirical Risk ). The adversarial empirical risk of h in the presence of
an adversary constrained by B is

RPn(h B) =

1
n

(cid:2) max

x(cid:48)∈N (xi)

l(h(x(cid:48))  yi)(cid:3).

n(cid:88)

i=1

3

4 Main results

In this section  we present our main results. The trick is to pushforward the original distribution P
into a new distribution P (cid:48) using a transport map Th : Z → Z satisfying

RP (h B) = RP (cid:48)(h) 

where RP (cid:48)(h) = E(x y)∼P (cid:48)l(h(x)  y) is the standard expected risk without the adversary. Therefore 
an upper bound on the expected risk over the new distribution leads to an upper bound on the
adversarial expected risk.
Note that the new distribution P (cid:48) is not ﬁxed and depends on the hypothesis h. As a result  traditional
statistical learning cannot be directly applied. However  note that these new distributions lie within
a Wasserstein ball centered on P   which we will show in Section 4.2. If we consider the worst
case within this Wasserstein ball  then the original adversarial learning problem can be reduced to a
minimax problem. We can therefore use the minimax approach to derive the adversarial risk bound.
We ﬁrst introduce the Wasserstein distance and minimax framework.

4.1 Wasserstein distance and local worst-case risk
Let (Z  dZ ) be a metric space where Z = X × Y and dZ is deﬁned as

dpZ (z  z(cid:48)) = dpZ ((x  y)  (x(cid:48)  y(cid:48))) = (dpX (x  x(cid:48)) + dpY (y  y(cid:48)))

with dX and dY representing the metric in the feature space and label space respectively. For example 
if Y = {1 −1}  dY (y  y(cid:48)) can be 1(y(cid:54)=y(cid:48))  and if Y = [−B  B]  dY (y  y(cid:48)) can be (y − y(cid:48))2. In this
paper  we require that dX is translation invariant  i.e.  dX (x  x(cid:48)) = dX (x − x(cid:48)  0). With this metric 
we denote with P(Z) the space of all Borel probability measures on Z  and with Pp(Z) the space of
all P ∈ P(Z) with ﬁnite pth moments for p ≥ 1:

Pp(Z) := {P ∈ P(Z) : EP [dpZ (z  z0)] < ∞ f or z0 ∈ Z}.

Then  the p-Wasserstein distance between two probability measures P  Q ∈ Pp(Z) is deﬁned as

Wp(P  Q) :=

inf

M∈Γ(P Q)

(E(z z(cid:48))∼M [dpZ (z  z(cid:48))])1/p 

where Γ(P  Q) denotes the collection of all measures on Z × Z with marginals P and Q on the ﬁrst
and second factors  respectively.
Now we deﬁne the local worst-case risk of h at P  

R p(P  h) := sup
Q∈BW

 p(P )

RQ(h) 

 p(P ) := {Q ∈ Pp(Z) : Wp(P  Q)) ≤ } is the p-Wasserstein ball of radius  ≥ 0 centered

where BW
at P .
With these deﬁnitions  we next show the adversarial expected risk can be related to the local worst-case
risk by a transport map Th.

4.2 Transport map
Deﬁne a mapping Th : Z → Z
where x∗ = arg maxx(cid:48)∈N (x) l(h(x(cid:48))  y). By the deﬁnition of dZ 
is easy to obtain
dZ ((x  y)  (x∗  y)) = dX (x  x∗). We now prove that the adversarial expected risk can be related to
the standard expected risk via the mapping Th.
Lemma 1. Let P (cid:48) = Th#P   the pushforward of P by Th  then we have

z = (x  y) → (x∗  y) 

it

RP (h B) = RP (cid:48)(h).

Proof. By the deﬁnition  we have
RP (h B) = E(x y)∼P [maxx(cid:48)∈N (x) l(h(x(cid:48))  y)] = E(x y)∼P [l(h(x∗)  y)] = E(x y)∼P (cid:48)[l(h(x)  y)] .
So RP (h B) = RP (cid:48)(h).

4

By this lemma  the adversarial expected risk over a distribution P is equivalent to the standard
expected risk over a new distribution P (cid:48). However since the new distribution is not ﬁxed and depends
on the hypothesis h  traditional statistical learning cannot be directly applied. Luckily  the following
lemma proves that all these new distributions locate within a Wasserstein ball centered at P .
Lemma 2. Deﬁne the radius of the adversary constrained by B as B := supx∈B dX (x  0). For any
hypothesis h and the corresponding P (cid:48) = Th#P   we have
Wp(P  P (cid:48)) ≤ B.

Proof. By the deﬁnition of Wasserstein distance 

p (P  P (cid:48)) ≤ EP [dpZ (Z  Th(Z))] = EP [dpX (x  x∗)] ≤ pB 
W p

where the last inequality uses the translation invariant property of dX . Therefore  we have
Wp(P  P (cid:48)) ≤ B.

From this lemma  we can see that all possible new distributions lie within a Wasserstein ball of radius
B centered on P . So  by upper bounding the worst-case risk in the ball  we can bound the adversarial
expected risk. The relationship between local worst-case risk and adversarial expected risk is as
follows. Note that this inequality holds for any p ≥ 1. For ease of exposition  in the rest of the paper 
we only discuss the case p = 1; that is 

RP (h B) ≤ RB 1(P  h) 

∀h ∈ H.

(1)

4.3 Adversarial risk bounds

In this subsection  we ﬁrst prove a bound for the local worst-case risk. Then  the adversarial
risk bounds can be derived directly by (1). To simplify notation  we denote a function class F by
compositing the functions in H with the loss function l(· ·)  i.e.  F = {(x  y) → l(h(x)  y) : h ∈ H}.
The key ingredient of a bound on the local worst-case risk is the following strong duality result by
Gao & Kleywegt [18]:
Proposition 1. For any upper semicontinuous function f : Z → R and for any P ∈ Pp(Z) 

RB 1(P  f ) = min
λ≥0

{λB + EP [ϕλ f (z)]} 

where ϕλ f (z) := supz(cid:48)∈Z{f (z(cid:48)) − λ · dZ (z  z(cid:48))}.
We begin with some assumptions.
Assumption 1. The instance space Z is bounded: diam(Z) := supz z(cid:48)∈Z dZ (z  z(cid:48)) < ∞.
Assumption 2. The functions in F are upper semicontinuous and uniformly bounded: 0 ≤ f (z) ≤
M < ∞ for all f ∈ F and z ∈ Z.
Assumption 3. For any function f ∈ F and any z ∈ Z  there exists λf z such that f (z(cid:48)) − f (z) ≤
λf zdZ (z  z(cid:48)) for any z(cid:48) ∈ Z.
Note that Assumption 3 is a weak version of Lipschitz condition since λf z is not ﬁxed and depends
on f and z. It is easy to see that if the function f ∈ F is L-Lipschitz with respect to the metric dZ 
i.e.  |f (z) − f (z(cid:48))| ≤ LdZ (z  z(cid:48))  Assumption 3 automatically holds with λf z always being L. Now
we give an equivalent expression for Assumption 3 which is easier to use in our proofs.
Lemma 3. Assumption 3 holds if and only if for any function f ∈ F and any empirical distribution
Pn  the set {λ : ψf Pn(λ) = 0} is nonempty  where ψf Pn(λ) := EPn(supz(cid:48)∈Z{f (z(cid:48))−λdZ (z  z(cid:48))−
f (z)}).
The proof of Lemma 3 is contained in Appendix A.
We denote the smallest value in the set as λ+
local worst-case risk bound  we need two technical lemmas.
Lemma 4. Fix some f ∈ F. Deﬁne ¯λ via

:= inf{λ : ψf Pn (λ) = 0}. In order to prove the

f Pn

¯λ := arg min
λ≥0

{λB + EPn [ϕλ f (Z)]}.

5

Then

¯λ ∈

 [0 

M
B
[λ−

f Pn

]

  λ+

f Pn

]

if B ≥ M
λ+
f Pn
M
λ+
f Pn

if B <

 

(2)

f Pn

f Pn

f Pn

:= 0.

· B} is nonempty 

:= sup{λ : ψf Pn (λ) = λ+

· B} if the set {λ : ψf Pn (λ) = λ+

where λ−
otherwise λ−
Remark 1. We can show that limB→0 λ−
deﬁne δ =
deﬁnition of λ−
f Pn
have λ−
> λ+
Lemma 5. Deﬁne the function class Φ := {ϕλ f : λ ∈ [a  b]  f ∈ F} where b ≥ a ≥ 0. Then  the
expected Rademacher complexity of the function class Φ satisﬁes

by using (  δ) language as follows. ∀ > 0 
· B. By the
· B. Since ψf Pn(λ) is monotonically non-increasing  we

  ψf Pn(λ−
− . Therefore  limB→0 λ−

. Then  for any B < δ  we have ψf Pn(λ+

− ) > λ+

ψf Pn (λ+

) = λ+

= λ+

= λ+

−)

f Pn

f Pn

f Pn

f Pn

f Pn

f Pn

λ+

f Pn

f Pn

.

f Pn

f Pn

f Pn

f Pn

f Pn

Rn(Φ) ≤ 12C(F)√
(cid:112)logN (F || · ||∞  u/2)du and N (F ||·||∞  u/2) denotes the covering number

(b − a) · diam(Z) 

+

n

√
π√
6
n

where C(F) :=(cid:82) ∞

0

of F.

(2)  [ζ−  ζ +] := (cid:83)

The proofs of Lemma 4 and 5 is contained in Appendix B.
We are now ready to prove the local worst-case risk bound. Let ¯λ ∈ [ζ−

] denotes expression
] and ΛB := ζ + − ζ−. It is straightforward to check that
[ζ−  ζ +] ⊂ [0  M/B] from expression (2). The generalization bound for local worst-case risk is
given by the following lemma.
Lemma 6. If the assumptions 1- 3 hold  then for any f ∈ F  we have

[ζ−

  ζ +

  ζ +

f Pn

f Pn

f Pn

f Pn

f Pn

RB 1(P  f ) − RB 1(Pn  f ) ≤ 24C(F)√

n

+ M

log( 1
δ )
2n

+

√
π√
12
n

ΛB · diam(Z)

(cid:114)

with probability at least 1 − δ.
Remark 2. Lee & Raginsky [29] proved a bound with ΛB ≡ L under the Lipschitz assumption
where L represents the Lipschitz constant. Our result improves a lot on theirs. First  our Assumption
3 is weaker than their Lipschitz assumption. Second  even under the weaker assumptions  our bound
is always better than their results since [ζ−  ζ +] ⊂ [0  L] by expression (2) and the deﬁnition of λ+
.
ΛB · diam(Z) in our bound will vanish  recovering the
Finally  by setting B = 0  the term 12
usual risk bound  whereas they gave a B-free bound with ΛB always being the constant L.
This leads to our main theorem for the adversarial expected risk.
Theorem 1. If the assumptions 1- 3 hold  for any f ∈ F  we have
24C(F)√

{λB + ψf Pn(λ)} +

ΛB · diam(Z) + M

√
π√
n

(cid:115)

f Pn

+

RP (f B) ≤ 1
n

i=1 f (zi) + min
λ≥0

√
π√
12
n

n

24C(F)√

n

+

√
π√
12
n

B +

ΛB · diam(Z) + M

log( 1
δ )
2n

(cid:114)

log( 1
δ )
2n

(3)

(4)

(cid:80)n
(cid:80)n

and

RP (f B) ≤ 1
n

i=1 f (zi) + λ+

f Pn

with probability at least 1 − δ.

6

(cid:80)n

RP (h) ≤ 1
n

i=1 f (zi) +

Remark 3. We are interested in how the adversarial risk bounds differ from the case in which the
adversary is absent. Plugging B = 0 into inequality (3) or (4) yields the usual generalization bound
of the form

(cid:114) log(1/δ)

.

24C(F)√

+ M

n

f Pn

n and

Remark 5. There are two data dependent terms 1/n(cid:80)n

2n
√
√
πΛB · diam(Z)/
So the effect of an adversary is to introduce an extra complexity term 12
an additional term on B which contributes to the empirical risk.
Remark 4. The extra complexity term will decrease as B gets bigger if B ≥ M/λ+
by
expression (2)  indicating that a stronger adversary might have a negative impact on the hypothesis
class complexity. This is intuitive  since different hypotheses might have the same performance in
the presence of a strong adversary and  therefore  the hypothesis class complexity will decrease. We
emphasize that this phenomenon does not occur in concurrent works [25  51]. In both of their work 
this term will increase linearly as B grows.
i=1 f (zi) and minλ≥0{λB + ψf Pn(λ)} (or
λ+
B) in bound (3) (or (4))  corresponding to the empirical risk and the effect of adversary on
f Pn
empirical risk  respectively. Although the bound (3) is tighter  it is hard to optimize because of the
inner minimization problem. The bound (4) cannot be directly minimized either because λ+
is
computationally intractable in practice. But we can consider an upper bound for λ+
. For example 
≤ L. See Section 5 for more examples.
if f is L-Lipschitz  by the deﬁnition of λ+
This upper bound for λ+
can be used in optimization  as we will discuss in Section 6. In particular 
if ψf Pn (λ) ≡ 0 for any λ ≥ 0  we get λ+
B in inequality (4)
will disappear.

= 0  and the additional term λ+

  we have λ+

f Pn

f Pn

f Pn

f Pn

f Pn

f Pn

f Pn

5 Example bounds

In this section  we illustrate the application of Theorem 1 to two commonly-used models: SVMs and
neural networks.

5.1 Support vector machines
We ﬁrst start with SVMs. Let Z = X × Y  where the feature space X = {x ∈ Rd : ||x||2 ≤ r} and
the label space Y = {−1  +1}. Equip Z with the Euclidean metric

dZ (z  z(cid:48)) = dZ ((x  y)  (x(cid:48)  y(cid:48))) = ||x − x(cid:48)||2 + 1(y(cid:54)=y(cid:48)).

Consider the hypothesis space F = {(x  y) → max{0  1 − yh(x)} : h ∈ H}  where H = {x →
w · x : ||w||2 ≤ Λ}. We can now derive the expected risk bound for SVMs in the presence of an
adversary.
Corollary 1. For the SVMs setting considered above  for any f ∈ F  with probability at least 1 − δ 
log( 1
RP (f B) ≤ 1
δ )
n
2n
≤ max

i=1 f (zi) + λ+
{2yiw · xi ||w||2}.

ΛB · (2r + 1) + (1 + Λr)

√
π√
12
n

(cid:80)n

where λ+

(cid:114)

144√
n

B +

d +

√

f Pn

Λr

 

f Pn

i

The proof of Corollary 1 can be found in Appendix E.

5.2 Neural networks

We next consider feed-forward neural networks. To demonstrate the generality of our method  we
consider a multi-class prediction problem. Let Z = X × Y  where the feature space X = {x ∈ Rd :
||x||2 ≤ B} and the label space Y = {1  2 ···   k}; k represents the number of classes. The network
uses L ﬁxed nonlinear activation functions (σ1  σ2 ···   σL)  where σi is ρi-Lipschitz and satisﬁes
σi(0) = 0. Given L weight matrices A = (A1  A2 ···   AL)  the network computes the following
function

HA(x) := σL(ALσL−1(AL−1σL−2(··· σ2(A2σ1(A1x)·)) 

7

RP (f B) ≤ 1
n
√
π√
12
n

(cid:80)n
i=1 f (zi) + λ+
ΛB · (2B + 1) +
(cid:26) 2
L(cid:89)

f Pn

ρi||Ai||σ 

1
γ

γ

i=1

where λ+

f Pn

≤ max

j

+

 

si

i=1

B +

√
288
γ
n

i=1 ρisiBW

(cid:18) bi

(cid:32)(cid:80)L

2(cid:33)2
(cid:19) 1
(cid:81)L
(cid:114) log(1/δ)
(cid:0)M(HA(xj)  yj) + maxHA(xj) − minHA(xj)(cid:1)(cid:27)
(cid:19)1/2(cid:33)2

2n

(cid:32)(cid:80)L

(cid:81)L

i=1 ρisiBW

i=1

.

+

√
288
n
γ

(cid:18) bi

si

.

(5)

where Ai ∈ Rdi×di−1 and HA : Rd → Rk with d0 = d and dL = k. Let W = max{d0  d1 ···   dL}.
Deﬁne a margin operator M : Rk × {1  2 ···   k} → R as M(v  y) := vy − maxj(cid:54)=y vj and the
ramp loss lγ : R → R+ as

(cid:40) 0

lγ :=

1 + r/γ
1

r < −γ
r ∈ [−γ  0]
r > 0

.

the hypothesis class F = {(x  y) → lγ(−M(HA(x)  y))

: A =
Consider
(A1  A2 ···   AL) ||Ai||σ ≤ si ||Ai||F ≤ bi}  where || · ||σ represents spectral norm and || · ||F
denotes the Frobenius norm. The metric in space Z is deﬁned as

dZ (z  z(cid:48)) = dZ ((x  y)  (x(cid:48)  y(cid:48))) = ||x − x(cid:48)||2 + 1(y(cid:54)=y(cid:48)).

Now we derive the adversarial expected risk for neural networks.
Corollary 2. For the neural networks setting deﬁned above  for any f ∈ F  with probability of 1 − δ 
the following inequality holds

The proof of this Corollary is provided in Appendix F.
Remark 6. Setting B = 0  we obtain a risk bound for neural networks:

(cid:80)n

RP (f ) ≤ 1
n

i=1 f (zi) +

(cid:114) log(1/δ)

2n

The bound is in terms of the spectral norm and the Frobenius norm. Although inequality (5) is similar
to the results in Bartlett et al. [6] and Neyshabur et al. [35]  since our proof technique is different  our
approach may provide a different perspective on the generalization of deep neural networks.

6 Conclusions

In this paper  we propose a theoretical method for deriving adversarial risk bounds. Our method
is general and can easily be applied to multi-class problems and most of the commonly used loss
functions. The bound may be loose in some cases  since we consider the worst case distribution in
the Wasserstein ball to avoid computing the transport map. However  for some problems  it may be
possible to derive the transport map and thus get tighter bounds. Furthermore  our bounds may be
made tighter by relying on the expected Rademacher complexity directly instead of using covering
numbers.
In the future  one interesting problem is to develop adversarial robust algorithms based on our results.
For example  our bounds suggest that minimizing the sum of empirical risk and the term λ+
B can
help achieve adversarial robustness. However  since λ+
is computationally intractable in practice 
instead of using the exact λ+
in the objective function  we may consider the data-dependent upper
which is usually easier to obtain and a regularization parameter η ∈ [0  1] selected
bound for λ+
via grid search. For a ﬁxed η  we multiply it by the upper bound for λ+
and use this product as a
surrogate of the true λ+
in the objective function. Afterward  we minimize this surrogate objective
function and obtain the optimal solution for this speciﬁc η. Each such η corresponds to a solution.
Finally we choose the best one from these candidates.

f Pn

f Pn

f Pn

f Pn

f Pn

f Pn

Acknowledgments

We thank the reviewers for their constructive comments that helped improve the paper signiﬁcantly.
This work was supported by the ARC FL-170100117.

8

References
[1] I. M. Alabdulmohsin. Algorithmic stability and uniform generalization. In Advances in Neural

Information Processing Systems  pages 19–27  2015.

[2] A. Ambroladze  E. Parrado-Hernández  and J. S. Shawe-taylor. Tighter pac-bayes bounds. In

Advances in neural information processing systems  pages 9–16  2007.

[3] S. Arora  R. Ge  B. Neyshabur  and Y. Zhang. Stronger generalization bounds for deep nets via

a compression approach. arXiv preprint arXiv:1802.05296  2018.

[4] A. Athalye  N. Carlini  and D. Wagner. Obfuscated gradients give a false sense of security:

Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420  2018.

[5] P. L. Bartlett  O. Bousquet  S. Mendelson  et al. Local rademacher complexities. The Annals of

Statistics  33(4):1497–1537  2005.

[6] P. L. Bartlett  D. J. Foster  and M. J. Telgarsky. Spectrally-normalized margin bounds for neural

networks. In Advances in Neural Information Processing Systems  pages 6240–6249  2017.

[7] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

[8] B. Biggio  I. Corona  D. Maiorca  B. Nelson  N. Šrndi´c  P. Laskov  G. Giacinto  and F. Roli.
Evasion attacks against machine learning at test time. In Joint European conference on machine
learning and knowledge discovery in databases  pages 387–402. Springer  2013.

[9] B. Biggio  B. Nelson  and P. Laskov. Poisoning attacks against support vector machines. arXiv

preprint arXiv:1206.6389  2012.

[10] B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning.

Pattern Recognition  84:317–331  2018.

[11] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of machine learning

research  2(Mar):499–526  2002.

[12] N. Courty  R. Flamary  D. Tuia  and A. Rakotomamonjy. Optimal transport for domain
adaptation. IEEE transactions on pattern analysis and machine intelligence  39(9):1853–1865 
2017.

[13] D. Cullina  A. N. Bhagoji  and P. Mittal. Pac-learning in the presence of adversaries.

Advances in Neural Information Processing Systems  pages 228–239  2018.

In

[14] N. Dalvi  P. Domingos  S. Sanghai  D. Verma  et al. Adversarial classiﬁcation. In Proceedings
of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining 
pages 99–108. ACM  2004.

[15] O. Dekel  O. Shamir  and L. Xiao. Learning to classify with missing and corrupted features.

Machine learning  81(2):149–178  2010.

[16] F. Farnia and D. Tse. A minimax approach to supervised learning. In Advances in Neural

Information Processing Systems  pages 4240–4248  2016.

[17] A. Fawzi  S.-M. Moosavi-Dezfooli  and P. Frossard. Robustness of classiﬁers: from adversarial
to random noise. In Advances in Neural Information Processing Systems  pages 1632–1640 
2016.

[18] R. Gao and A. J. Kleywegt. Distributionally robust stochastic optimization with wasserstein

distance. arXiv preprint arXiv:1604.02199  2016.

[19] J. Gilmer  L. Metz  F. Faghri  S. S. Schoenholz  M. Raghu  M. Wattenberg  and I. Goodfellow.

Adversarial spheres. arXiv preprint arXiv:1801.02774  2018.

[20] A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature deletion. In
Proceedings of the 23rd international conference on Machine learning  pages 353–360. ACM 
2006.

9

[21] I. J. Goodfellow  J. Shlens  and C. Szegedy. Explaining and harnessing adversarial examples.

arXiv preprint arXiv:1412.6572  2014.

[22] S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples.

arXiv preprint arXiv:1412.5068  2014.

[23] M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classiﬁer against
adversarial manipulation. In Advances in Neural Information Processing Systems  pages 2266–
2276  2017.

[24] R. Herbrich and R. C. Williamson. Algorithmic luckiness. Journal of Machine Learning

Research  3(Sep):175–212  2002.

[25] J. Khim and P.-L. Loh. Adversarial risk bounds for binary classiﬁcation via function transfor-

mation. arXiv preprint arXiv:1810.09519  2018.

[26] J. Khim and P.-L. Loh. Adversarial risk bounds via function transformation. arXiv preprint

arXiv:1810.09519v2  2018.

[27] J. Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of machine

learning research  6(Mar):273–306  2005.

[28] J. Lee and M. Raginsky. Minimax statistical learning and domain adaptation with wasserstein

distances. arXiv preprint arXiv:1705.07815  2017.

[29] J. Lee and M. Raginsky. Minimax statistical learning with wasserstein distances. In Advances

in Neural Information Processing Systems  pages 2692–2701  2018.

[30] T. Liu  G. Lugosi  G. Neu  and D. Tao. Algorithmic stability and hypothesis complexity. arXiv

preprint arXiv:1702.08712  2017.

[31] D. Lowd and C. Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD
international conference on Knowledge discovery in data mining  pages 641–647. ACM  2005.

[32] D. Lowd and C. Meek. Good word attacks on statistical spam ﬁlters. In CEAS  volume 2005 

2005.

[33] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In International Conference on Learning Representations  2018.

[34] M. Mohri  A. Rostamizadeh  and A. Talwalkar. Foundations of machine learning. MIT press 

2012.

[35] B. Neyshabur  S. Bhojanapalli  D. McAllester  and N. Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564 
2017.

[36] A. Raghunathan  J. Steinhardt  and P. Liang. Certiﬁed defenses against adversarial examples.

arXiv preprint arXiv:1801.09344  2018.

[37] D. Russo and J. Zou. Controlling bias in adaptive data analysis using information theory. In
A. Gretton and C. C. Robert  editors  Proceedings of the 19th International Conference on
Artiﬁcial Intelligence and Statistics  volume 51 of Proceedings of Machine Learning Research 
pages 1232–1240  Cadiz  Spain  09–11 May 2016. PMLR.

[38] L. Schmidt  S. Santurkar  D. Tsipras  K. Talwar  and A. M ˛adry. Adversarially robust generaliza-

tion requires more data. arXiv preprint arXiv:1804.11285  2018.

[39] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Learnability  stability and uniform

convergence. Journal of Machine Learning Research  11(Oct):2635–2670  2010.

[40] A. Sinha  H. Namkoong  and J. Duchi. Certiﬁable distributional robustness with principled

adversarial training. In International Conference on Learning Representations  2018.

10

[41] A. S. Suggala  A. Prasad  V. Nagarajan  and P. Ravikumar. On adversarial risk and training.

arXiv preprint arXiv:1806.02924  2018.

[42] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus.

Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199  2013.

[43] M. Talagrand. Upper and lower bounds for stochastic processes: modern methods and classical

problems  volume 60. Springer Science & Business Media  2014.

[44] J. Uesato  B. O’Donoghue  A. v. d. Oord  and P. Kohli. Adversarial risk and the dangers of

evaluating against weak attacks. arXiv preprint arXiv:1802.05666  2018.

[45] V. Vapnik. The nature of statistical learning theory. Springer science & business media  2013.

[46] V. N. Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks 

10(5):988–999  1999.

[47] Y. Wang  S. Jha  and K. Chaudhuri. Analyzing the robustness of nearest neighbors to adversarial

examples. arXiv preprint arXiv:1706.03922  2017.

[48] E. Wong and Z. Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning  pages 5283–5292 
2018.

[49] A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning
algorithms. In Advances in Neural Information Processing Systems  pages 2524–2533  2017.

[50] H. Xu and S. Mannor. Robustness and generalization. Machine learning  86(3):391–423  2012.

[51] D. Yin  K. Ramchandran  and P. Bartlett. Rademacher complexity for adversarially robust

generalization. arXiv preprint arXiv:1810.11914  2018.

[52] T. Zhang. Covering number bounds of certain regularized linear function classes. Journal of

Machine Learning Research  2(Mar):527–550  2002.

[53] D.-X. Zhou. The covering number in learning theory. Journal of Complexity  18(3):739–767 

2002.

11

,Zhuozhuo Tu
Jingwei Zhang
Dacheng Tao