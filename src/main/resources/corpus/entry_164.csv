2018,A Practical Algorithm for Distributed Clustering and Outlier Detection,We study the classic k-means/median clustering  which are fundamental problems in unsupervised learning  in the setting where data are partitioned across multiple sites  and where we are allowed to discard a small portion of the data by labeling them as outliers.  We propose a simple approach based on constructing small summary for the original dataset. The proposed method is time and communication efficient  has good approximation guarantees  and can identify the global outliers effectively.  
To the best of our knowledge  this is the first practical algorithm with theoretical guarantees for distributed clustering with outliers. Our experiments on both real and synthetic data have demonstrated the clear superiority of our algorithm against all the baseline algorithms in almost all metrics.,A Practical Algorithm for Distributed Clustering and

Outlier Detection∗

Jiecao Chen

Indiana University Bloomington

Bloomington  IN

jiecchen@indiana.edu

Erfan Sadeqi Azer

Indiana University Bloomington

Bloomington  IN

esadeqia@indiana.edu

Qin Zhang

Indiana University Bloomington

Bloomington  IN

qzhangcs@indiana.edu

Abstract

We study the classic k-means/median clustering  which are fundamental problems
in unsupervised learning  in the setting where data are partitioned across multiple
sites  and where we are allowed to discard a small portion of the data by labeling
them as outliers. We propose a simple approach based on constructing small
summary for the original dataset. The proposed method is time and communication
efﬁcient  has good approximation guarantees  and can identify the global outliers
effectively. To the best of our knowledge  this is the ﬁrst practical algorithm with
theoretical guarantees for distributed clustering with outliers. Our experiments
on both real and synthetic data have demonstrated the clear superiority of our
algorithm against all the baseline algorithms in almost all metrics.

1

Introduction

The rise of big data has brought the design of distributed learning algorithm to the forefront. For
example  in many practical settings the large quantities of data are collected and stored at different
locations  while we want to learn properties of the union of the data. For many machine learning
tasks  in order to speed up the computation we need to partition the data into a number of machines
for a joint computation. In a different dimension  since real-world data often contain background
noise or extreme values  it is desirable for us to perform the computation on the “clean data” by
discarding a small portion of the data from the input. Sometimes these outliers are interesting by
themselves; for example  in the study of statistical data of a population  outliers may represent those
people who deserve special attention. In this paper we study clustering with outliers  a fundamental
problem in unsupervised learning  in the distributed model where data are partitioned across multiple
sites  who need to communicate to arrive at a consensus on the cluster centers and labeling of outliers.
For many clustering applications it is common to model data objects as points in Rd  and the similarity
between two objects is represented as the Euclidean distance of the two corresponding points. In
this paper we assume for simplicity that each point can be sent by one unit of communication.
Note that when d is large  we can apply standard dimension reduction tools (for example  the
Johnson-Lindenstrauss lemma) before running our algorithms.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

∗A full version of this paper is available at https://arxiv.org/abs/1805.09495

We focus on the two well-studied objective functions (k  t)-means and (k  t)-median  deﬁned in
Deﬁnition 1. It is worthwhile to mention that our algorithms also work for other metrics as long as
the distance oracles are given.

outliers O ⊆ X of size at most t so that the objective function(cid:80)
the (k  t)-means we simply replace the objective function with(cid:80)

Deﬁnition 1 ((k  t)-means/median) Let X be a set of points  and k  t be two parameters. For the
(k  t)-median problem we aim for computing a set of centers C ⊆ Rd of size at most k and a set of
p∈X\O d(p  C) is minimized. For
p∈X\O d2(p  C).

Computation Model. We study the clustering problems in the coordinator model  a well-adopted
model for distributed learning Balcan et al. (2013); Chen et al. (2016); Guha et al. (2017); Diakoniko-
las et al. (2017). In this model we have s sites and a central coordinator; each site can communicate
with the coordinator. The input data points are partitioned among the s sites  who  together with the
coordinator  want to jointly compute some function on the global data. The data partition can be
either adversarial or random. The former can model the case where the data points are independently
collected at different locations  while the latter is common in the scenario where the system uses a
dispatcher to randomly partition the incoming data stream into multiple workers/sites for a parallel
processing (and then aggregates the information at a central server/coordinator).
In this paper we focus on the one-round communication model (also called the simultaneous commu-
nication model)  where each site sends a sketch of its local dataset to the coordinator  and then the
coordinator merges these sketches and extracts the answer. This model is arguably the most practical
one since multi-round communication will cost a large system overhead.
Our goals for computing (k  t)-means/median in the coordinator model are the following: (1) to
minimize the clustering objective functions; (2) to accurately identify the set of global outliers; and
(3) to minimize the computation time and the communication cost of the system. We will elaborate
on how to quantify the quality of outlier detection in Section 4.
Our Contributions. A natural way of performing distributed clustering in the simultaneous commu-
nication model is to use the two-level clustering framework (see e.g.  Guha et al. (2003  2017)). In
this framework each site performs the ﬁrst level clustering on its local dataset X  getting a subset
X(cid:48) ⊆ X with each point being assigned a weight; we call X(cid:48) the summary of X. The site then sends
X(cid:48) to the coordinator  and the coordinator performs the second level clustering on the union of the
s summaries. We note that the second level clustering is required to output at most k centers and t
outliers  while the summary returned by the ﬁrst level clustering can possibly have more than (k + t)
weighted points. The size of the summary will contribute to the communication cost as well as the
running time of the second level clustering.
The main contribution of this paper is to propose a simple and practical summary construction at sites
with the following properties.

1. It is extremely fast: runs in time O(max{k  log n} · n)  where n is the size of the dataset.
2. The summary has small size: O(k log n + t) for adversarial data partition and O(k log n +

t/s) for random data partition.

3. When coupled with a second level (centralized) clustering algorithm that γ-approximates
(k  t)-means/median  we obtain an O(γ)-approximation algorithm for distributed (k  t)-
means/median.2

4. It can be used to effectively identify the global outliers.

We emphasize that both the ﬁrst and the second properties are essential to make the distributed
clustering algorithm scalable on large datasets. Our extensive set of experiments have demonstrated
the clear superiority of our algorithm against all the baseline algorithms in almost all metrics.
To the best of our knowledge  this is the ﬁrst practical algorithm with theoretical guarantees for
distributed clustering with outliers.
Related Work. Clustering is a fundamental problem in computer science and has been studied for
more than ﬁfty years. A comprehensive review of the work on k-means/median is beyond the scope

2We say an algorithm γ-approximates a problem if it outputs a solution that is at most γ times the optimal

solution.

2

of this paper  and we will focus on the literature for centralized/distributed k-means/median clustering
with outliers and distributed k-means/median clustering.
In the centralized setting  several O(1)-approximation or (O(1)  O(1))-approximation3 algorithms
have been proposed Charikar et al. (2001); Chen (2009). These algorithms make use of linear
programming and need time at least Ω(n3)  which is prohibitive on large datasets. Feldman and
Schulman (2012) studied (k  t)-median via coresets  but the running times of their algorithm includes
a term O(n(k + t)k+t)) which is not practical.
Chawla and Gionis (2013) proposed for (k  t)-means an algorithm called k-means--  which is an
iterative procedure and can be viewed as a generalization of Llyod’s algorithm Lloyd (1982). Like
Llyod’s algorithm  the centers that k-means-- outputs are not the original input points; we thus
cannot use it for the summary construction in the ﬁrst level clustering at sites because some of
the points in the summary will be the outliers we report at the end. However  we have found that
k-means-- is a good choice for the second level clustering: it outputs exactly k centers and t outliers 
and its clustering quality looks decent on datasets that we have tested  though it does not have any
worst case theoretical guarantees.
Recently Gupta et al. (2017) proposed a local-search based (O(1)  O(k log(n))-approximation
algorithm for (k  t)-means. The running time of their algorithm is ˜O(k2n2) 4 which is again not quite
scalable. The authors mentioned that one can use the k-means++ algorithm Arthur and Vassilvitskii
(2007) as a seeding step to boost the running time to ˜O(k2(k + t)2 + nt). We note that ﬁrst  this
running time is still worse than ours. And second  since in the ﬁrst level clustering we only need a
summary – all that we need is a set of weighted points that can be fed into the second level clustering
at the coordinator  we can in fact directly use k-means++ with a budget of O(k log n + t) centers
for constructing a summary. We will use this approach as a baseline algorithm in our experimental
studies.
In the past few years there has been a growing interest in studying k-means/median clustering in the
distributed models Ene et al. (2011); Bahmani et al. (2012); Balcan et al. (2013); Liang et al. (2014);
Cohen et al. (2015); Chen et al. (2016). In the case of allowing outliers  Guha et al. Guha et al.
(2017) gave a ﬁrst theoretical study for distributed (k  t)-means/median. However  their algorithms
need Θ(n2) running time at sites and are thus again not quite practical on large-scale datasets. In
a concurrent work  Li and Guo (2018) further reduced the value of the objective function  but the
proposed method does not output the outliers.
We note that the k-means(cid:107) algorithm proposed by Bahmani et al. (2012) can be extended (again
by increasing the budget of centers from k to O(k log n + t)) and used as a baseline algorithm for
comparison. The main issue with k-means(cid:107) is that it needs O(log n) rounds of communication
which holds back its overall performance.

2 The Summary Construction

In this section we present our summary construction for (k  t)-median/means in the centralized model.
In Section 3 we will show how to use this summary construction for solving the problems in the
distributed model. Table 1 is the list of notations we are going to use.

X
k
t
σ

φX (σ)
OPTmed

k t (X)

input dataset

number of centers
number of outliers

φX (σ) =(cid:80)

clustering mapping σ : X → X
x∈X d(x  σ(x))
d(p  C)

(cid:80)

O⊆X |C|≤k

min
|O|≤t

p∈X\O

n
κ
O∗

d(y  X)
φ(X  Y )

OPTmea

k t (X)

n = |X|  size of the dataset

κ = max{k  log n}
outliers chosen by OPT

φ(X  Y ) =(cid:80)
(cid:80)

d(y  X) = minx∈X d(y  x)
y∈Y d(y  X)
d2(p  C)

O⊆X |C|≤k

min
|O|≤t

p∈X\O

Table 1: List of Notations

where C is the cost of the optimal solution excluding t points.

3We say a solution is an (a  b)-approximation if the cost of the solution is a · C while excluding b · t points 
4 ˜O(·) hides some logarithmic factors.

3

:dataset X  number of centers k  number of outliers t

Algorithm 1: Summary-Outliers(X  k  t)
Input
Output :a weighted dataset Q as a summary of X
1 i ← 0  Xi ← X  Q ← ∅
2 ﬁx a β such that 0.25 ≤ β < 0.5
3 κ ← max{log n  k}
4 let σ : X → X be a mapping to be constructed  and α be a constant to be determined in the
5 while |Xi| > 8t do

analysis.

construct a set Si of size ακ by random sampling (with replacement) from Xi
for each point in Xi  compute the distance to its nearest point in Si
let ρi be the smallest radius s.t. |B(Si  Xi  ρi)| ≥ β|Xi|. Let Ci ← B(Si  Xi  ρi)
for each x ∈ Ci  choose the point y ∈ Si that minimizes d(x  y) and assign σ(x) ← y
Xi+1 ← Xi\Ci
i ← i + 1

6
7
8
9
10
11
12 r ← i
13 for each x ∈ Xr  assign σ(x) ← x
14 for each x ∈ Xr ∪ (∪r−1
15 return Q

i=0 Si)  assign weight wx ← |σ−1(x)| and add (x  wx) into Q

2.1 The Algorithm

Our algorithm is presented in Algorithm 1. It works for both the k-means and k-median objective
functions. We note that Algorithm 1 is partly inspired by the algorithm for clustering without outliers
proposed in Mettu and Plaxton (2002). But since we have to handle outliers now  the design and
analysis of our algorithm require new ideas.
For a set S and a scalar value ρ  deﬁne B(S  X  ρ) = {x ∈ X | d(x  S) ≤ ρ}. Algorithm 1 works
in rounds indexed by i. Let X0 = X be the initial set of input points. The idea is to sample a set
of points Si of size αk for a constant α (assuming k ≥ log n) from Xi  and grow a ball of radius
ρi centered at each s ∈ Si. Let Ci be the set of points in the union of these balls. The radius ρi is
chosen such that at least a constant fraction of points of Xi are in Ci.
Deﬁne Xi+1 = Xi\Ci. In the i-th round  we add the αk points in Si to the set of centers  and assign
points in Ci to their nearest centers in Si. We then recurse on the rest of the points Xi+1  and stop
until the number of points left unclustered becomes at most 8t. Let r be the ﬁnal value of i. Deﬁne
the weight of each point x in ∪r−1
i=0 Si to be the number of points in X that are assigned to x  and the
weight of each point in Xr to be 1. Our summary Q consists of points in Xr ∪ (∪r−1
i=0 Si) together
with their weights.

2.2 The Analysis

We now try to analyze the performance of Algorithm 1. The analysis will be conducted for the
(k  t)-median objective function  while the results also hold for (k  t)-means; we will discuss this
brieﬂy at the end of this section. Due to space constraints  all missing proofs in this section can be
found in the supplementary material.
We start by introducing the following concept. Note that the summary constructed by Algorithm 1 is
fully determined by the mapping function σ (σ is also constructed in Algorithm 1).

Deﬁnition 2 (Information Loss) For a summary Q constructed by Algorithm 1  we deﬁne the infor-
mation loss of Q as
That is  the sum of distances of moving each point x ∈ X to the corresponding center σ(x) (we can
view each outlier as a center itself).

loss(Q) = φX (σ).

We will prove the following theorem  which says that the information loss of the summary Q
constructed by Algorithm 1 is bounded by the optimal (k  t)-median clustering cost on X.

4

(cid:16)

(cid:17)

Theorem 1 Algorithm 1 outputs a summary Q such that with probability (1 − 1/n2) we have that
. The running time of Algorithm 1 is bounded by O(max{log n  k} · n) 
loss(Q) = O
and the size of the outputted summary Q is bounded by O(k log n + t).

OPTmed

k t (X)

k t (X). Namely  φX (σ) = O((cid:80)

The proof of this theorem relies on building an upper bound on φX (σ) and a lower bound on
i ρi|Di|)  where Di =
OPTmed
Ci\O∗  where Ci is constructed in the i-th round of Algorithm 1 and O∗ is the set of outliers returned
by the optimal algorithm. See the detailed proof in the supplementary material.
As a consequence of Theorem 1  we obtain by triangle inequality arguments the following corollary
that directly characterizes the quality of the summary in the task of (k  t)-median. We include a proof
in the supplementary material for completeness.

i ρi|Di|) and OPTmed

k t (X) = Ω((cid:80)

Corollary 1 If we run a γ-approximation algorithm for (k  t)-median on Q  we can obtain a set
of centers C and a set of outliers O such that φ(X\O  C) = O(γ · OPTmed
k t (X)) with probability
(1 − 1/n2).
The running time. We now analyze the running time of Algorithm 1. At the i-th iteration  the
sampling step at Line 6 can be done in O(|Xi|) time. The nearest-center assignments at Line 7 and 9
can be done in |Si| · |Xi| = O(κ|Xi|) time. Line 8 can be done by ﬁrst sorting the distances in the
increasing order and then scanning the shorted list until we get enough points. In this way the running
time is bounded by |Xi| log |Xi| = O(κ|Xi|). Thus the total running time can be bounded by

O(κ|Xi|) = O(κn) = O(max{log n  k} · n) 

(cid:88)

i=0 1 ... r−1

where the ﬁrst equation holds since the size of Xi decreases geometrically  and the second equation
is due to the deﬁnition of κ.
Finally  we comment that we can get a similar result for (k  t)-means by appropriately adjusting
various constant parameters in the proof. Please refer to the supplementary material for a more
detailed discussion.

2.3 An Augmentation
In the case when t (cid:29) k  which is typically the case in practice since the number of centers k does
not scale with the size of the dataset while the number of outliers t does  we add an augmentation
procedure to Algorithm 1 to achieve a better practical performance. The pseudocode can be found in
the supplementary materials and the full version of this paper.
The augmentation is as follows  after computing the set of outliers Xr and the set of centers
S = ∪r−1
i=0 Si in Algorithm 1  we sample randomly from X\(Xr ∪ S) an additional set of center
points S(cid:48) of size |Xr| − |S|. That is  we try to make the number of centers and the number of outliers
in the summary to be balanced. We then reassign each point in the set X\Xr to its nearest center in
S ∪ S(cid:48). Denote the new mapping by π. Finally  we include points in Xr and S  together with their
weights  into the summary Q.
It is clear that the augmentation procedure preserves the size of the summary asymptotically. And by
including more centers we have loss(Q) ≤ φX (π) ≤ φX (σ)  where σ is the mapping returned by
Algorithm 1. The running time will increase to O(tn) due to the reassignment step  but our algorithm
is still much faster than all the baseline algorithms  as we shall see in Section 4.

3 Distributed Clustering with Outliers

In this section we discuss distributed (k  t)-median/means using the summary constructed in Algo-
rithm 1. Our main result is the following theorem  which is based on the work by Guha et al. (2003 
2017). The proof for this theorem can be found in the supplementary material.

Theorem 2 Suppose Algorithm 2 uses a γ-approximation algorithm for (k  t)-median in the second
level clustering (Line 2). We have with probability (1 − 1/n) that:

5

:For each i ∈ [s]  Site i gets input dataset Ai where (A1  . . .   As) is a random partition
of X

Algorithm 2: Distributed-Median(A1  . . .   As  k  t)
Input
Output :a (k  t)-median clustering for X = ∪i∈[s]Ai
1 for each i ∈ [s]  Site i constructs a summary Qi by running Summary-Outliers(Ai  k  2t/s)
2 the coordinator then performs a second level clustering on Q = Q1 ∪ Q2 ∪ . . . ∪ Qs using an

(Algorithm 1) and sends Qi to the coordinator

off-the-shelf (k  t)-median algorithm  and returns the resulting clustering.

• it outputs a set of centers C ⊆ Rd and a set of outliers O ⊆ X such that φ(X\O  C) ≤

O(γ) · OPTmed

k t (X);

• it uses one round of communication whose cost is bounded by O(sk log n + t);
• the running time at the i-th site is bounded by O(max{log n  k} · |Ai|)  and the running

time at the coordinator is that of the second level clustering.

We note that in Mettu and Plaxton (2002) it was shown that under some mild assumption  Ω(kn) time
is necessary for any O(1)-approximate randomized algorithm to compute k-median on n points with
nonnegligible success probability (e.g.  1/100). Thus the running time of our algorithm is optimal up
to a log n factor under the same assumption.
In the case that the dataset is adversarially partitioned  the total communication increases to
O(s(k log n + t)). This is because all of the t outliers may go to the same site and thus 2t/s
in Line 1 needs to be replaced by t.
Finally  we comment that the result above also holds for the summary constructed using the augu-
mented version (Sec. 2.3)  except  as discussed in Section 2  that the local running time at the i-th
site will increase to O(t|Ai|).

4 Experiments

4.1 Experimental Setup

4.1.1 Datasets and Algorithms

Due to space constraints  we only present the experimental results for two data sets (kddFull and
kddSp). One can ﬁnd results for a number of other datasets in our supplementary materials and the
full paper.

• kddFull. This dataset is from 1999 kddcup competition and contains instances describing
connections of sequences of tcp packets. There are about 4.9M data points. We only consider
the 34 numerical features of this dataset. We also normalize each feature so that it has zero
mean and unit standard deviation. There are 23 classes in this dataset  98.3% points of
the dataset belong to 3 classes (normal 19.6%  neptune 21.6%  and smurf 56.8%). We
consider small clusters as outliers and there are 45747 outliers.
• kddSp. This data set contains about 10% points of kddFull (released by the original

provider). This dataset is also normalized and there are 8752 outliers.

We comment that ﬁnding appropriate k and t values for the task of clustering with outliers is a
separate problem  and is not part of the topic of this paper. In all our experiments  k and t are naturally
suggested by the datasets we use.
We compare the performance of following algorithms  each of which is implemented using the MPI
framework and run in the coordinator model. The data are randomly partitioned among the sites.

• ball-grow. Algorithm 2 proposed in this paper  with the augmented version Algorithm 1 for
the summary construction. As mentioned we use k-means-- as the second level clustering
at Line 2. We ﬁx α = 2 and β = 4.5 in the subroutine Algorithm 1.

6

• rand. Each site constructs a summary by randomly sampling points from its local dataset.
Each sampled point p is assigned a weight equal to the number of points in the local dataset
that are closer to p than other points in the summary. The coordinator then collects all
weighted samples from all sites and feeds to k-means-- for a second level clustering.

• k-means++. Each site constructs a summary of the local dataset using the k-means++
algorithm Arthur and Vassilvitskii (2007)  and sends it to the coordinator. The coordinator
feeds the unions all summaries to k-means-- for a second level clustering.

• k-means(cid:107). An MPI implementation of the k-means(cid:107) algorithm proposed by Bahmani
et al. (2012) for distributed k-means clustering. To adapt their algorithm to solve the outlier
version  we increase the parameter k in the algorithm to O(k +t)  and then feed the outputted
centers to k-means-- for a second level clustering.

4.1.2 Measurements

Let C and O be the sets of centers and outliers respectively returned by a tested algorithm. To
evaluate the quality of the clustering results we use two metrics: (a) (cid:96)1-loss (for (k  t)-median):

(cid:80)
p∈X\O d(p  C); (b) (cid:96)2-loss (for (k  t)-means):(cid:80)

p∈X\O d2(p  C).

To measure the performance of outlier detection we use three metrics. Let S be the set of points fed
into the second level clustering k-means-- in each algorithm  and let O∗ be the set of actual outliers
(i.e.  the ground truth)  we use the following metrics: (a) preRec: the proportion of actual outliers
that are included in the returned summary  deﬁned as |S∩O∗|
; (b) recall: the proportion of actual
|O∗|
outliers that are returned by k-means--  deﬁned as |O∩O∗|
; (c) prec: the proportion of points in O
|O∗|
that are actually outliers  deﬁned as |O∩O∗|
|O|

.

4.1.3 Computation Environments

All algorithms are implemented in C++ with Boost.MPI support. We use Armadillo Sanderson (2010)
as the numerical linear library and -O3 ﬂag is enabled when compile the code. All experiments are
conducted in a PowerEdge R730 server equipped with 2 x Intel Xeon E5-2667 v3 3.2GHz. This
server has 8-core/16-thread per CPU  192GB Memeory and 1.6TB SSD.

4.2 Experimental Results

We now present our experimental results. All results take the average of 10 runs. In our supplementary
material  results for more datasets can be found  but all the conclusions remain the same.

4.2.1 Quality
We ﬁrst compare the qualities of the summaries returned by ball-grow  rand and k-means(cid:107). Note
that the size of the summary returned by ball-grow is determined by the parameters k and t  and
we can not control the exact size. In k-means(cid:107)  the summary size is determined by the sample ratio 
and again we can not control the exact size. On the other hand  the summary sizes of rand and
k-means++ can be fully controlled. To be fair  we manually tune those parameters so that the sizes
of summaries returned by different algorithms are roughly the same (the difference is less than 10%).
In this set of experiments  each dataset is randomly partitioned into 20 sites.
Table 2 presents the experimental results on kddSp and kddFull datasets. We observe that
ball-grow gives better (cid:96)1-loss and (cid:96)2-loss than k-means(cid:107) and k-means++  and rand performs
the worst among all.
For outlier detection  rand fails completely. In both kddFull and kddSp  ball-grow outperforms
k-means++ and k-means(cid:107) in almost all metrics. k-means(cid:107) slightly outperforms k-means++.

4.2.2 Communication Costs

We next compare the communication cost of different algorithms. Figure 1a presents the experimental
results. The communication cost is measured by the number of points exchanged between the

7

dataset

kddSp

kddFull

algo
ball-grow
k-means++
k-means(cid:107)
rand
ball-grow
k-means++
k-means(cid:107)
rand

summarySize
3.37e+4
3.37e+4
3.30e+4
3.37e+4
1.83e+5
1.83e+5

1.83e+5

(cid:96)1-loss
8.00e+5
8.38e+5
8.18e+5
8.85e+5
7.38e+6
8.21e+6

9.60e+6

(cid:96)2-loss
3.46e+6
4.95e+6
4.19e+6
1.06e+7
3.54e+7
4.65e+7

preRec
0.6102
0.3660
0.2921
0.0698
0.7754
0.2188

does not stop after 8 hours
0.0378691

1.11e+8

prec
0.5586
0.3676
0.3641
0.5076
0.5992
0.2828

recall
0.5176
0.1787
0.1552
0.0374
0.5803
0.1439

0.6115

0.0241

Table 2: Clustering quality. k = 3  t = 8752 for kddSp and t = 45747 for kddFull

(a) communication cost

(b) running time (log10 scale)

(c) running time  #sites = 20

Figure 1: experiments on kddSp dataset

coordinator and all sites. In this set of experiments we only change the number of partitions (i.e.  # of
sites s). The summaries returned by all algorithms have almost the same size.
We observe that the communication costs of ball-grow  k-means++ and rand are almost indepen-
dent of the number of sites. Indeed  ball-grow  k-means++ and rand all run in one round and their
communication cost is simply the size of the union of the s summaries. k-means(cid:107) incurs signiﬁcantly
more communication  and it grows almost linearly to the number of sites. This is because k-means(cid:107)
grows its summary in multiple rounds; in each round  the coordinator needs to collect messages from
all sites and broadcasts the union of those messages. When there are 20 sites  k-means(cid:107) incurs 20
times more communication cost than its competitors.

4.2.3 Running Time

We ﬁnally compare the running time of different algorithms. All experiments in this part are conducted
on kddSp dataset since k-means(cid:107) does not scale to kddFull; similar results can also be observed on
other datasets. The running time we show is only the time used to construct the input (i.e.  the union
of the s summaries) for the second level clustering  and we do not include the running time of the
second level clustering since it is always the same for all tested algorithms (i.e.  the k-means--).
Figure 1b shows the running time when we change the number of sites while ﬁx the size of the
summary produced by each site. We observe that k-means(cid:107) uses signiﬁcantly more time than
ball-grow  k-means++ and rand. This is predictable because k-means(cid:107) runs in multiple rounds
and communicates more than its competitors. ball-grow uses signiﬁcantly less time than others 
typically 1/25 of k-means(cid:107)  1/7 of k-means++ and 1/2 of rand. The reason that ball-grow is
even faster than rand is that ball-grow only needs to compute weights for about half of the points
in the constructed summary. As can be predicted  when we increase the number of sites  the total
running time of each algorithm decreases.
We also investigate how the size of the summary will affect the running time. Note that for ball-grow
the summary size is controlled by the parameter t. We ﬁx k = 3 and vary t  resulting different
summary sizes for ball-grow. For other algorithms  we tune the parameters so that they output
summaries of similar sizes as ball-grow outputs. Figure 1c shows that when the size of summary
increases  the running time increases almost linearly for all algorithms.

8

Acknowledgments

Jiecao Chen  Erfan Sadeqi Azer and Qin Zhang are supported in part by NSF CCF-1525024  NSF
CCF-1844234 and IIS-1633215.

References
Arthur  D. and Vassilvitskii  S. (2007). k-means++: the advantages of careful seeding. In SODA 

pages 1027–1035.

Bahmani  B.  Moseley  B.  Vattani  A.  Kumar  R.  and Vassilvitskii  S. (2012). Scalable k-means++.

PVLDB  5(7)  622–633.

Balcan  M.  Ehrlich  S.  and Liang  Y. (2013). Distributed k-means and k-median clustering on

general communication topologies. In NIPS  pages 1995–2003.

Baldi  P.  Sadowski  P.  and Whiteson  D. (2014). Searching for exotic particles in high-energy

physics with deep learning. Nature communications  5.

Charikar  M.  Khuller  S.  Mount  D. M.  and Narasimhan  G. (2001). Algorithms for facility location

problems with outliers. In SODA  pages 642–651.

Chawla  S. and Gionis  A. (2013). k-means-: A uniﬁed approach to clustering and outlier detection.

In SDM  pages 189–197.

Chen  J.  Sun  H.  Woodruff  D. P.  and Zhang  Q. (2016). Communication-optimal distributed

clustering. In NIPS  pages 3720–3728.

Chen  K. (2009). On coresets for k-median and k-means clustering in metric and euclidean spaces

and their applications. SIAM J. Comput.  39(3)  923–947.

Cohen  M. B.  Elder  S.  Musco  C.  Musco  C.  and Persu  M. (2015). Dimensionality reduction for

k-means clustering and low rank approximation. In STOC  pages 163–172.

Diakonikolas  I.  Grigorescu  E.  Li  J.  Natarajan  A.  Onak  K.  and Schmidt  L. (2017).
Communication-efﬁcient distributed learning of discrete distributions. In NIPS  pages 6394–6404.

Ene  A.  Im  S.  and Moseley  B. (2011). Fast clustering using mapreduce. In SIGKDD  pages

681–689.

Feldman  D. and Schulman  L. J. (2012). Data reduction for weighted and outlier-resistant clustering.

In SODA  pages 1343–1354.

Guha  S.  Meyerson  A.  Mishra  N.  Motwani  R.  and O’Callaghan  L. (2003). Clustering data

streams: Theory and practice. IEEE Trans. Knowl. Data Eng.  15(3)  515–528.

Guha  S.  Li  Y.  and Zhang  Q. (2017). Distributed partial clustering. In SPAA  pages 143–152.

Gupta  S.  Kumar  R.  Lu  K.  Moseley  B.  and Vassilvitskii  S. (2017). Local search methods for

k-means with outliers. PVLDB  10(7)  757–768.

Li  S. and Guo  X. (2018). Distributed k-clustering for data with heavy noise. arXiv preprint

arXiv:1810.07852.

Liang  Y.  Balcan  M.  Kanchanapally  V.  and Woodruff  D. P. (2014). Improved distributed principal

component analysis. In NIPS  pages 3113–3121.

Lloyd  S. P. (1982). Least squares quantization in PCM. IEEE Trans. Information Theory  28(2) 

129–136.

Mettu  R. R. and Plaxton  C. G. (2002). Optimal time bounds for approximate clustering. In UAI 

pages 344–351.

Sanderson  C. (2010). Armadillo: An open source c++ linear algebra library for fast prototyping and

computationally intensive experiments.

9

,Jeffrey Pennington
Felix Xinnan Yu
Sanjiv Kumar
Jiecao Chen
Erfan Sadeqi Azer
Qin Zhang