2009,Regularized Distance Metric Learning:Theory and Algorithm,In this paper  we examine the generalization error of regularized distance metric learning. We show that with appropriate constraints  the generalization error of regularized distance metric learning could be independent from the dimensionality  making it suitable for handling high dimensional data. In addition  we present an efficient online learning algorithm for regularized distance metric learning. Our empirical studies with data classification and face recognition show that the proposed algorithm is (i) effective for distance metric learning when compared to the state-of-the-art methods  and (ii) efficient and robust for high dimensional data.,Regularized Distance Metric Learning:

Theory and Algorithm

Rong Jin1

Shijun Wang2

Yang Zhou1

1Dept. of Computer Science & Engineering  Michigan State University  East Lansing  MI 48824

2Radiology and Imaging Sciences  National Institutes of Health  Bethesda  MD 20892
rongjin@cse.msu.edu wangshi@cc.nih.gov zhouyang@msu.edu

Abstract

In this paper  we examine the generalization error of regularized distance metric
learning. We show that with appropriate constraints  the generalization error of
regularized distance metric learning could be independent from the dimensional-
ity  making it suitable for handling high dimensional data. In addition  we present
an efﬁcient online learning algorithm for regularized distance metric learning. Our
empirical studies with data classiﬁcation and face recognition show that the pro-
posed algorithm is (i) effective for distance metric learning when compared to the
state-of-the-art methods  and (ii) efﬁcient and robust for high dimensional data.

1 Introduction

Distance metric learning is a fundamental problem in machine learning and pattern recognition. It is
critical to many real-world applications  such as information retrieval  classiﬁcation  and clustering
[6  7]. Numerous algorithms have been proposed and examined for distance metric learning. They
are usually classiﬁed into two categories: unsupervised metric learning and supervised metric learn-
ing. Unsupervised distance metric learning  or sometimes referred to as manifold learning  aims to
learn a underlying low-dimensional manifold where the distance between most pairs of data points
are preserved. Example algorithms in this category include ISOMAP [13] and Local Linear Embed-
ding (LLE) [8]. Supervised metric learning attempts to learn distance metrics from side information
such as labeled instances and pairwise constraints. It searches for the optimal distance metric that
(a) keeps data points of the same classes close  and (b) keeps data points from different classes far
apart. Example algorithms in this category include [17  10  15  5  14  19  4  12  16]. In this work 
we focus on supervised distance metric learning.

Although a large number of studies were devoted to supervised distance metric learning (see the sur-
vey in [18] and references therein)  few studies address the generalization error of distance metric
learning. In this paper  we examine the generalization error for regularized distance metric learning.
Following the idea of stability analysis [1]  we show that with appropriate constraints  the general-
ization error of regularized distance metric learning is independent from the dimensionality of data 
making it suitable for handling high dimensional data. In addition  we present an online learning
algorithm for regularized distance metric learning  and show its regret bound. Note that although
online metric learning was studied in [9]  our approach is advantageous in that (a) it is computation-
ally more efﬁcient in handling the constraint of SDP cone  and (b) it has a proved regret bound while
[9] only shows a mistake bound for the datasets that can be separated by a Mahalanobis distance. To
verify the efﬁcacy and efﬁciency of the proposed algorithm for regularized distance metric learning 
we conduct experiments with data classiﬁcation and face recognition. Our empirical results show
that the proposed online algorithm is (1) effective for metric learning compared to the state-of-the-art
methods  and (2) robust and efﬁcient for high dimensional data.

1

2 Regularized Distance Metric Learning

Let D = {zi = (xi  yi)  i = 1  . . .   n} denote the labeled examples  where xk = (x1
k) ∈ Rd
k  . . .   xd
is a vector of d dimension and yi ∈ {1  2  . . .   m} is class label. In our study  we assume that
the norm of any example is upper bounded by R  i.e.  supx |x|2 ≤ R. Let A ∈ Sd×d
be the
distance metric to be learned  where the distance between two data points x and x′ is calculated as
|x − x′|2
Following the idea of maximum margin classiﬁers  we have the following framework for regularized
distance metric learning:

A = (x − x′)⊤A(x − x′).

+




min

A

where

1
2|A|2

F +

2C

n(n − 1)Xi<j

g(cid:0)yi j(cid:2)1 − |xi − xj|2

A(cid:3)(cid:1) : A (cid:23) 0  tr(A) ≤ η(d)


(1)

• yi j is derived from class labels yi and yj  i.e.  yi j = 1 if yi = yj and −1 otherwise.
• g(z) is the loss function. It outputs a small value when z is a large positive value  and a large
value when z is large negative. We assume g(z) to be convex and Lipschitz continuous with
Lipschitz constant L.

F is the regularizer that measures the complexity of the distance metric A.

• |A|2
• tr(A) ≤ η(d) is introduced to ensure a bounded domain for A. As will be revealed later 
this constraint will become active only when the constraint constant η(d) is sublinear in
d  i.e.  η ∼ O(dp) with p < 1. We will also show how this constraint could affect the
generalization error of distance metric learning.

3 Generalization Error

Let AD be the distance metric learned by the algorithm in (1) from the training examples D. Let
ID(A) denote the empirical loss   i.e. 
2

g(cid:0)yi j(cid:2)1 − |xi − xj|2
A(cid:3)(cid:1)
For the convenience of presentation  we also write g(cid:0)yi j(1 − |xi − xj|2
A)(cid:1) = V (A  zi  zj) to high-

light its dependence on A and two examples zi and zj. We denote by I(A) the loss of A over the
true distribution  i.e. 

n(n − 1)Xi<j

ID(A) =

(2)

(3)
Given the empirical loss ID(A) and the loss over the true distribution I(A)  we deﬁne the estimation
error as

I(A) = E(zi zj)[V (A  zi  zj)]

DD = I(AD) − ID(AD)

(4)
In order to show the behavior of estimation error  we follow the analysis based on the stability of
the algorithm [1]. The uniform stability of an algorithm determines the stability of the algorithm
when one of the training examples is replaced with another. More speciﬁcally  an algorithm A has
uniform stability β if
(5)

∀(D  z)  ∀i  sup

u v |V (AD  u  v) − V (ADz i  u  v)| ≤ β

where Dz i stands for the new training set that is obtained by replacing zj ∈ D with a new example
z. We further deﬁne β = κ/n as the uniform stability β behaves like O(1/n).
The advantage of using stability analysis for the generalization error of regularized distance metric
learning. This is because the example pair (zi  zj) used for training distance metrics are not I.I.D.
although zi is  making it difﬁcult to directly utilize the results from statistical learning theory.
In the analysis below  we ﬁrst show how to derive the generalization error bound for regularized
distance metric learning given the uniform stability β (or κ). We then derive the uniform stability
constant for the regularized distance metric learning framework in (1).

2

3.1 Generalization Error Bound for Given Uniform Stability

Analysis in this section follows closely [1]  and we therefore omit the detailed proofs.

Our analysis utilizes the McDiarmid inequality that is stated as follows.
Theorem 1. (McDiarmid Inequality) Given random variables {vi}l
R satisfying

i=1  v′

i  and a function F : vl →

the following statement holds

′

′

sup

v1  ... vl v

i˛˛˛

F (v1  . . .   vl) − F (v1  . . .   vi−1  v

i   vi+1  . . .   vl)˛˛˛
Pr (|F (v1  . . .   vl) − E(F (v1  . . .   vl))| > ǫ) ≤ 2 exp −

≤ ci 

2ǫ
i=1 c2

i !

Pl

To use the McDiarmid inequality  we ﬁrst compute E(DD).
Lemma 1. Given a distance metric learning algorithm A has uniform stability κ/n  we have the
following inequality for E(DD)

E(DD) ≤ 2

κ
n

(6)

where n is the number of training examples in D.
The result in the following lemma shows that the condition in McDiarmid inequality holds.
Lemma 2. Let D be a collection of n randomly selected training examples  and Di z be the collec-
tion of examples that replaces zi in D with example z. We have |DD − DDi z| bounded as follows
(7)

2κ + 8Lη(d) + 2g0

|DD − DDi z| ≤

n

where g0 = supz z′ |V (0  z  z′)| measures the largest loss when distance metric A is 0.
Combining the results in Lemma 1 and 2  we can now derive the the bound for the generalization
error by using the McDiarmid inequality.
Theorem 2. Let D denote a collection of n randomly selected training examples  and AD be the
distance metric learned by the algorithm in (1) whose uniform stability is κ/n. With probability
1 − δ  we have the following bound for I(AD)

I(AD) − ID(AD) ≤

2κ
n

+ (2κ + 4Lη(d) + 2g0)r ln(2/δ)

2n

3.2 Generalization Error for Regularized Distance Metric Learning

First  we show that the superium of tr(AD) is O(d1/2)  which veriﬁes that η(d) should behave
sublinear in d. This is summarized by the following proposition.
Proposition 1. The trace constraint in (1) will be activated only when

η(d) ≤p2dg0C

where g0 = supz z′ |V (0  z  z′)|.
Proof. It follows directly from [tr(AD)/d]2 ≤ |AD|2

F ≤ 2C sup

z z′ |V (0  z  z′)| ≤ Cg0.

(8)

(9)

To bound the uniform stability  we need the following proposition
Proposition 2. For any two distance metrics A and A′  we have the following inequality hold for
any examples zu and zv

|V (A  zu  zv) − V (A′  zu  zv)| ≤ 4LR2|A − A′|F

(10)

3

The above proposition follows directly from the fact that (a) V (A  z  z′) is Lipschitz continuous and
(b) |x|2 ≤ R for any example x. The following lemma bounds |AD − AD′|F .
Lemma 3. Let D denote a collection of n randomly selected training examples  and by z = (x  y) a
randomly selected example. Let AD be the distance metric learned by the algorithm in (1). We have

|AD − ADi z|F ≤

8CLR2

n

(11)

The proof of the above lemma can be found in Appendix A.

By putting the results in Lemma 3 and Proposition 2  we have the following theorem for the stability
of the Frobenius norm based regularizer.
Theorem 3. The uniform stability for the algorithm in (1) using the Frobenius norm regularizer 
denoted by β  is bounded as follows

β =

κ
n ≤

32CL2R4

n

(12)

where κ = 32CL2R4

Combing Theorem 3 and 2  we have the following theorem for the generalization error of distance
metric learning algorithm in (1) using the Frobenius norm regularizer
Theorem 4. Let D be a collection of n randomly selected examples  and AD be the distance metric
learned by the algorithm in (1) with h(A) = |A|2
F . With probability 1 − δ  we have the following
bound for the true loss function I(AD) where AD is learned from (1) using the Frobenius norm
regularizer

I(AD) − ID(AD) ≤

32CL2R4

n

+(cid:0)32CL2R4 + 4Ls(d) + 2g0(cid:1)r ln(2/δ)

2n

where s(d) = min(cid:0)√2dg0C  η(d)(cid:1).
O(s(d)/√n). By choosing η(d) to have a low dependence of d (i.e.  η(d) ∼ dp with p ≪ 1)  the

Remark The most important feature in the estimation error is that it converges in the order of

proposed framework for regularized distance metric learning will be robust to the high dimensional
data. In the extreme case  by setting η(d) to be a constant  the estimation error will be independent
from the dimensionality of data.

(13)

4 Algorithm

In this section  we discuss an efﬁcient algorithm for solving (1). We assume a hinge loss for g(z) 
i.e.  g(z) = max(0  b − z)  where b is the classiﬁcation margin. To design an online learning
algorithm for regularized distance metric learning  we follow the theory of gradient based online
learning [2] by deﬁning potential function Φ(A) = |A|2
F /2. Algorithm 1 shows the online learning
algorithm.

The theorem below shows the regret bound for the online learning algorithm in Figure 1.
Theorem 5. Let the online learning algorithm 1 run with learning rate λ > 0 on a sequence
t)  yt  t = 1  . . .   n. Assume |x|2 ≤ R for all the training examples. Then  for all distance
(xt  x′
metric M ∈ Sd×d

+   we have

F(cid:19)
1
2λ|M|2

bLn ≤

1

1 − 8R4λ/b(cid:18)Ln(M ) +
nXt=1

max(cid:0)0  b − yt(1 − |xt − x′
M )(cid:1)  bLn =
t|2

where

Łn(M ) =

nXt=1

max(cid:16)0  b − yt(1 − |xt − x′
At−1 )(cid:17)
t|2

4

Algorithm 1 Online Learning Algorithm for Regularized Distance Metric Learning
1: INPUT: predeﬁned learning rate λ
2: Initialize A0 = 0
3: for t = 1  . . .   T do
4:
5:
6:
7:
8:
9:

Receive a pair of training examples {(x1
Compute the class label yt: yt = +1 if y1
if the training pair (x1

t )  yt is classiﬁed correctly  i.e.  yt(cid:16)1 − |x1

t   y2
t   and yt = −1 otherwise.
At−1(cid:17) > 0 then
t|2
t − x2

t)⊤)  where πS+ (M ) projects matrix M into the

t   y1
t = y2

At = At−1.

t )  (x2

t   x2

t )}

else

At = πS+ (At−1 − λyt(xt − x′
SDP cone.

t)(xt − x′

end if
10:
11: end for

t)(xt − x′

The proof of this theorem can be found in Appendix B. Note that the above online learning algorithm
require computing πS+ (M )  i.e.  projecting matrix M onto the SDP cone  which is expensive for
high dimensional data. To address this challenge  ﬁrst notice that M ′ = πS+ (M ) is equivalent to the
optimization problem M ′ = arg minM ′(cid:23)0 |M ′ − M|F . We thus approximate At = πS+ (At−1 −
t)⊤ where λt is computed as
λyt(xt − x′
follows

t)⊤) with At = At−1 − λtyt(xt − x′
t)(xt − x′
(cid:8)|λt − λ| : λt ∈ [0  λ]  At−1 − λtyt(xt − x′

t)(xt − x′
The following theorem shows the solution to the above optimization problem.
Theorem 6. The optimal solution λt to the problem in (14) is expressed as

t)⊤ (cid:23) 0(cid:9)

λt = arg min

(14)

λt

λt =(cid:26) λ

min(cid:0)λ  [(xt − x′

t)⊤A−1

t−1(xt − x′

t)]−1(cid:1)

yt = −1
yt = +1

Proof of this theorem can be found in the supplementary materials. Finally  the quantity (xt −
t)A−1
x′

t) can be computed by solving the following optimization problem

t−1(xt − x′

max

u

2u⊤(xt − x′

t) − u⊤Au

whose optimal value can be computed efﬁciently using the conjugate gradient method [11].

Note that compared to the online metric learning algorithm in [9]  the proposed online learning
algorithm for metric learning is advantageous in that (i) it is computationally more efﬁcient by
avoiding projecting a matrix into a SDP cone  and (ii) it has a provable regret bound while [9] only
presents the mistake bound for the separable datasets.

5 Experiments

We conducted an extensive study to verify both the efﬁciency and the efﬁcacy of the proposed
algorithms for metric learning. For the convenience of discussion  we refer to the propoesd online
distance metric learning algorithm as online-reg. To examine the efﬁcacy of the learned distance
metric  we employed the k Nearest Neighbor (k-NN) classiﬁer. Our hypothesis is that the better the
distance metric is  the higher the classiﬁcation accuracy of k-NN will be. We set k = 3 for k-NN
for all the experiments according to our experience.

puted as the inverse of covariance matrix of training samples  i.e.  (Pn

We compare our algorithm to the following six state-of-the-art algorithms for distance metric learn-
ing as baselines: (1) Euclidean distance metric; (2) Mahalanobis distance metric  which is com-
i=1 xixi)−1; (3) Xing’s algo-
rithm proposed in [17]; (4) LMNN  a distance metric learning algorithm based on the large margin
nearest neighbor classiﬁer [15]; (5) ITML  an Information-theoretic metric learning based on [4];
and (6) Relevance Component Analysis (RCA) [10]. We set the maximum number of iterations for
Xing’s method to be 10  000. The number of target neighbors in LMNN and parameter γ in ITML

5

Table 1: Classiﬁcation error (%) of a k-NN (k = 3) classiﬁer on the ten UCI data sets using seven
different metrics. Standard deviation is included.

Dataset

1
2
3
4
5
6
7
8
9

Eclidean
19.5 ± 2.2
39.9 ± 2.3
36.0 ± 2.0
4.0 ± 1.7
30.6 ± 1.9
25.4 ± 4.2
31.9 ± 2.8
18.9 ± 0.5
2.0 ± 0.4

Mahala

18.8 ± 2.5
6.7 ± 0.6
42.1 ± 4.0
10.4 ± 2.7
29.1 ± 2.1
18.4 ± 3.4
10.0 ± 2.8
37.3 ± 0.5
6.1 ± 0.5

Xing

29.3 ± 17.2
40.1 ± 2.6
43.5 ± 12.5

3.1 ± 2.0
30.6 ± 1.9
23.3 ± 3.4
24.6 ± 7.5
16.1 ± 0.6
12.4 ± 0.8

LMNN

13.8 ± 2.5
3.6 ± 1.1
33.1 ± 0.6
3.9 ± 1.6
29.6 ± 1.8
15.2 ± 3.1
4.5 ± 2.4
18.4 ± 0.4
1.6 ± 0.3

ITML

8.6 ± 1.7
40.0 ± 2.3
39.8 ± 3.3
3.2 ± 1.6
28.8 ± 2.1
17.1 ± 4.1
28.7 ± 3.7
23.3 ± 1.3
2.5 ± 0.4

RCA

17.4 ± 1.5
3.8 ± 0.4
41.6 ± 0.7
2.9 ± 1.5
28.6 ± 2.3
13.9 ± 2.2
1.8 ± 1.5
30.6 ± 0.7
2.8 ± 0.4

Online-reg
13.2 ± 2.2
3.7 ± 1.2
37.3 ± 4.1
3.2 ± 1.3
27.7 ± 1.3
12.9 ± 2.2
1.8 ± 1.1
19.8 ± 0.6
2.9 ± 0.4

Table 2: p-values of the Wilcoxon signed-rank test of the 7 methods on the 9 datasets.
LMNN ITML RCA Online-reg
0.004
0.008
0.027
1.000
0.129
0.496
0.734

Eclidean Mahala Xing
0.641
0.301
1.000
0.027
0.359
0.074
0.027

1.000
0.734
0.641
0.004
0.496
0.301
0.129

Methods
Euclidean
Mahala
Xing
LMNN
ITML
RCA

Online-reg

0.734
1.000
0.301
0.008
0.570
0.004
0.004

0.496
0.570
0.359
0.129
1.000
0.820
0.164

0.301
0.004
0.074
0.496
0.820
1.000
0.074

0.129
0.004
0.027
0.734
0.164
0.074
1.000

were tuned by cross validation over the range from 10−4 to 104. All the algorithms are implemented
and run using Matlab. All the experiment are run on a AMD Processor 2.8G machine  with 8GMB
RAM and Linux operation system.

5.1 Experiment (I): Comparison to State-of-the-art Algorithms

We conducted experiments of data classiﬁcation over the following nine datasets from UCI repos-
itory: (1) balance-scale  with 3 classes  4 features  and 625 instances; (2) breast-cancer  with 2
classes  10 features  and 683 instance; (3) glass  with 6 classes  9 features  and 214 instances; (4)
iris  with 3 classes  4 features  and 150 instances; (5) pima  with 2 classes  8 features  and 768 in-
stances; (6) segmentation  with 7 classes  19 features  and 210 instances; (7)wine  with 3 classes 
13 features  and 178 instances; (8) waveform  with 3 classes  21 features  and 5000 instances; (9)
optdigits  with 10 classes  64 features  3823 instances. For all the datasets  we randomly select 50%
samples for training  and use the remaining samples for testing. Table 1 shows the classiﬁcation
errors of all the metric learning methods over 9 datasets averaged over 10 runs  together with the
standard deviation. We observe that the proposed metric learning algorithm deliver performance that
comparable to the state-of-the-art methods. In particular  for almost all datasets  the classiﬁcation
accuracy of the proposed algorithm is close to that of LMNN  which has yielded overall the best
performance among six baseline algorithms. This is consistent with the results of the other studies 
which show LMNN is among the most effective algorithms for distance metric learning.

To further verify if the proposed method performs statistically better than the baseline methods  we
conduct statistical test by using Wilcoxon signed-rank test [3]. The Wilcoxon signed-rank test is a
non-parametric statistical hypothesis test for the comparisons of two related samples. It is known to
be safer than the Student’s t-test because it does not assume normal distributions. From table 2  we
ﬁnd that the regularized distance metric learning improves the classiﬁcation accuracy signiﬁcantly
compared to Mahalanobis distance  Xing’s method and RCA at signiﬁcant level 0.1. It performs
slightly better than ITML and is comparable to LMNN.

6

y
c
a
r
u
c
c
a
n
o

 

i
t

a
c
i
f
i
s
s
a
C

l

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 
0.1

0.12

att−face

 

Euclidean
Mahalanobis
LMNN
ITML
RCA
Online_reg

0.18

0.2

0.14

0.16

Image resize ratio

att−face

 

LMNN
ITML
RCA
Online_reg

7000

6000

5000

4000

3000

2000

1000

)
s
d
n
o
c
e
s
(
 

e
m

i
t
 

i

g
n
n
n
u
R

0
 
0.1

0.12

0.14

0.16

0.18

0.2

Image resize ratio

(a)

(b)

Figure 1: (a) Face recognition accuracy of kNN and (b) running time of LMNN  ITML  RCA and
online reg algorithms on the “att-face” dataset with varying image sizes.

5.2 Experiment (II): Results for High Dimensional Data

To evaluate the dependence of the regularized metric learning algorithms on data dimensions  we
tested it by the task of face recognition. The AT&T face database 1 is used in our study. It consists
of grey images of faces from 40 distinct subjects  with ten pictures for each subject. For every
subject  the images were taken at different times  with varied the lighting condition and different
facial expressions (open/closed-eyes  smiling/not-smiling) and facial details (glasses/no-glasses).
The original size of each image is 112 × 92 pixels  with 256 grey levels per pixel.
To examine the sensitivity to data dimensionality  we vary the data dimension (i.e.  the size of
images) by compressing the original images into size different sizes with the image aspect ratio
preserved. The image compression is achieved by bicubic interpolation (the output pixel value is a
weighted average of pixels in the nearest 4-by-4 neighborhood). For each subject  we randomly spit
its face images into training set and test set with ratio 4 : 6. A distance metric is learned from the
collection of training face images  and is used by the kNN classiﬁer (k = 3) to predict the subject ID
of the test images. We conduct each experiment 10 times  and report the classiﬁcation accuracy by
averaging over 40 subjects and 10 runs. Figure 1 (a) shows the average classiﬁcation accuracy of the
kNN classiﬁer using different distance metric learning algorithms. The running times of different
metric learning algorithms for the same dataset is shown in Figure 1 (b). Note that we exclude
Xing’s method in comparison because its extremely long computational time. We observed that
with increasing image size (dimensions)  the regularized distance metric learning algorithm yields
stable performance  indicating that the it is resilient to high dimensional data. In contrast  for almost
all the baseline methods except ITML  their performance varied signiﬁcantly as the size of the input
image changed. Although ITML yields stable performance with respect to different size of images 
its high computational cost (Figure 1)  arising from solving a Bregman optimization problem in each
iteration  makes it unsuitable for high-dimensional data.

6 Conclusion

In this paper  we analyze the generalization error of regularized distance metric learning. We show
that with appropriate constraint  the regularized distance metric learning could be robust to high
dimensional data. We also present efﬁcient learning algorithms for solving the related optimiza-
tion problems. Empirical studies with face recognition and data classiﬁcation show the proposed
approach is (i) robust and efﬁcient for high dimensional data  and (ii) comparable to the state-of-the-
art approaches for distance learning. In the future  we plan to investigate different regularizers and
their effect for distance metric learning.

1http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

7

ACKNOWLEDGEMENTS

The work was supported in part by the National Science Foundation (IIS-0643494) and the U. S.
Army Research Laboratory and the U. S. Army Research Ofﬁce (W911NF-09-1-0421). Any opin-
ions  ﬁndings  and conclusions or recommendations expressed in this material are those of the au-
thors and do not necessarily reﬂect the views of NSF and ARO.

Appendix A: Proof of Lemma 3

Proof. We introduce the Bregmen divergence for the proof of this lemma. Given a convex function
of matrix ϕ(X)  the Bregmen divergence between two matrices A and B is computed as follows:

We deﬁne convex function N (X) and VD(X) as follows:

dϕ(A  B) = ϕ(B) − ϕ(A) − tr(cid:0)∇ϕ(A)⊤(B − A)(cid:1)
N (X) = kXk2
F  

VD(X) =

n(n − 1)Xi<j

2

V (X  zi  zj)

and furthermore convex function TD(X) = N (X) + CVD(X). We thus have

dN (AD  ADi z ) + dN (ADi z   AD) ≤ dTD (AD  ADi z ) + dTDi z (ADi z   AD)

[V (ADi z   zi  zj) − V (ADi z   z  zj) + V (AD  z  zj) − V (AD  zi  zj)]

C

n(n − 1)Xj6=i

8CLR2

n

|AD − ADi z|F

=

≤

The ﬁrst inequality follows from the fact that both N (X) and VD(X) are convex in X. The second
step holds because matrix AD and ADi z minimize the objective function TD(X) and TDi z (X) 
respectively  and therefore

(ADi z − AD)⊤ ∇TD(AD) ≥ 0 

(AD − ADi z )⊤ ∇TDi z (ADi z ) ≥ 0

Since dN (A  B) = kA − Bk2

F   we therefore have

|AD − ADi z|2
which leads to the result in the lemma.

F ≤

Appendix B: Proof of Theorem 7

8CLR2

n

|AD − ADi z|F  

Proof. We denote by A′
11.1 and Theorem 11.4 [2]  we have

t = At−1 − λy(xt − x′

t)(xt − x′

t)⊤ and At = πS+ (A′

t). Following Theorem

where

bLn − Ln(M ) ≤

1
λ

DΦ∗ (M  A0) +

1
λ

nXt=1

DΦ∗ (At−1  A′
t)

DΦ∗ (A  B) =

1
2|A − B|2

F   Φ(A) = Φ∗(A) =

1
2|A|2

F

Using the relation A′

t)⊤ and A0 = 0  we have

t = At−1 − λy(xt − x′
t)(xt − x′
nXt=1
1
2λ

1
2λ|M|2

bLn − Ln(M ) ≤

At−1 ) < 0i|xt − x′
Ihyt(1 − |xt − x′
t|4
t|2
By assuming |x|2 ≤ R for any training example  we have |xt − x′
2 ≤ 16R4. Since
t|4
nXt=1
t|2
max(0  b − yt(1 − |xt − x′
At−1 ))

At−1 ) < 0i|xt − x′
Ihyt(1 − |xt − x′
t|2

nXt=1

t|4 ≤

F +

16R4

b

we thus have the result in the theorem

8

=

16R4

b bLn

References

[1] Bousquet  Olivier  and Andr´e Elisseeff. Stability and generalization. Journal of Machine

Learning Research  2:499–526  March 2002.

[2] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge Uni-

versity Press  New York  NY  USA  2006.

[3] G.W. Corder and D.I. Foreman. Nonparametric Statistics for Non-Statisticians: A Step-by-Step

Approach. New Jersey: Wiley  2009.

[4] J.V. Davis  B. Kulis  P. Jain  S. Sra  and I.S. Dhillon. Information-theoretic metric learning. In

Proceedings of the 24th international conference on Machine Learning  2007.

[5] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural

Information Processing Systems  2005.

[6] Steven C.H. Hoi  Wei Liu  and Shih-Fu Chang. Semi-supervised distance metric learning for
collaborative image retrieval. In Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  2008.

[7] Steven C.H. Hoi  Wei Liu  Michael R. Lyu  and Wei-Ying Ma. Learning distance metrics with
contextual constraints for image retrieval. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2006.

[8] L. K. Saul and S. T. Roweis. Think globally  ﬁt locally: Unsupervised learning of low dimen-

sional manifolds. Journal of Machine Learning Research  4  2003.

[9] Shai Shalev-Shwartz  Yoram Singer  and Andrew Y. Ng. Online and batch learning of pseudo-
In Proceedings of the twenty-ﬁrst international conference on Machine learning 

metrics.
pages 94–101  2004.

[10] N. Shental  T. Hertz  D. Weinshall  and M. Pavel. Adjustment learning and relevant component
analysis. In Proceedings of the Seventh European Conference on Computer Vision  volume 4 
pages 776–792  2002.

[11] Jonathan R Shewchuk. An introduction to the conjugate gradient method without the agonizing

pain. Technical report  Carnegie Mellon University  Pittsburgh  PA  USA  1994.

[12] Luo Si  Rong Jin  Steven C. H. Hoi  and Michael R. Lyu. Collaborative image retrieval via

regularized metric learning. In ACM Multimedia Systems Journal (MMSJ)  2006.

[13] J.B. Tenenbaum  V. de Silva  and J. C. Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290  2000.

[14] I.W. Tsang  P.M. Cheung  and J.T. Kwok. Kernel relevant component analysis for distance
metric learning. In IEEE International Joint Conference on Neural Networks (IJCNN)  2005.
[15] K. Weinberger  J. Blitzer  and L. Saul. Distance metric learning for large margin nearest neigh-

bor classiﬁcation. In Advances in Neural Information Processing Systems  2005.

[16] Lei Wu  Steven C.H. Hoi  Rong Jin  Jianke Zhu  and Nenghai Yu. Distance metric learning
from uncertain side information with application to automated photo tagging. In Proceedings
of ACM International Conference on Multimedia (MM)  2009.

[17] E.P. Xing  A.Y. Ng  M.I. Jordan  and S. Russell. Distance metric learning  with application
to clustering with side-information. In Advances in Neural Information Processing Systems 
2002.

[18] L. Yang and R. Jin. Distance metric learning: A comprehensive survey. Michigan State

University  Tech. Rep.  2006.

[19] L. Yang  R. Jin  R. Sukthankar  and Y. Liu. An efﬁcient algorithm for local distance metric
learning. In the Proceedings of the Twenty-First National Conference on Artiﬁcial Intelligence
Proceedings (AAAI)  2006.

9

,Yuhui Wang
Hao He
Xiaoyang Tan
Yaozhong Gan