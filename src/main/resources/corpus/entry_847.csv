2011,Boosting with Maximum Adaptive Sampling,Classical Boosting algorithms  such as AdaBoost  build a strong classifier without concern about the computational cost.  Some applications  in particular in computer vision  may involve up to millions of training examples and features.  In such contexts  the training time may become prohibitive.  Several methods exist to accelerate training  typically either by sampling the features  or the examples  used to train the weak learners.  Even if those methods can precisely quantify the speed improvement they deliver  they offer no guarantee of being more efficient than any other  given the same amount of time.    This paper aims at shading some light on this problem  i.e. given a fixed amount of time  for a particular problem  which strategy is optimal in order to reduce the training loss the most.  We apply this analysis to the design of new algorithms which estimate on the fly at every iteration the optimal trade-off between the number of samples and the number of features to look at in order to maximize the expected loss reduction.  Experiments in object recognition with two standard computer vision data-sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.,Boosting with Maximum Adaptive Sampling

Charles Dubout

Idiap Research Institute

charles.dubout@idiap.ch

Franc¸ois Fleuret

Idiap Research Institute

francois.fleuret@idiap.ch

Abstract

Classical Boosting algorithms  such as AdaBoost  build a strong classiﬁer without
concern about the computational cost. Some applications  in particular in com-
puter vision  may involve up to millions of training examples and features. In
such contexts  the training time may become prohibitive. Several methods exist
to accelerate training  typically either by sampling the features  or the examples 
used to train the weak learners. Even if those methods can precisely quantify the
speed improvement they deliver  they offer no guarantee of being more efﬁcient
than any other  given the same amount of time.
This paper aims at shading some light on this problem  i.e. given a ﬁxed amount
of time  for a particular problem  which strategy is optimal in order to reduce
the training loss the most. We apply this analysis to the design of new algo-
rithms which estimate on the ﬂy at every iteration the optimal trade-off between
the number of samples and the number of features to look at in order to maximize
the expected loss reduction. Experiments in object recognition with two standard
computer vision data-sets show that the adaptive methods we propose outperform
basic sampling and state-of-the-art bandit methods.

1

Introduction

Boosting is a simple and efﬁcient machine learning algorithm which provides state-of-the-art per-
formance on many tasks. It consists of building a strong classiﬁer as a linear combination of weak-
learners  by adding them one after another in a greedy manner. However  while textbook AdaBoost
repeatedly selects each of them using all the training examples and all the features for a predeter-
mined number of rounds  one is not obligated to do so and can instead choose only to look at a
subset of examples and features.
For the sake of simplicity  we identify the space of weak learners and the feature space by consid-
ering all the thresholded versions of the latter. More sophisticated combinations of features can be
envisioned in our framework by expanding the feature space.
The computational cost of one iteration of Boosting is roughly proportional to the product of the
number of candidate weak learners Q and the number of samples T considered  and the perfor-
mance increases with both. More samples allow a more accurate estimation of the weak-learners’
performance  and more candidate weak-learners increase the performance of the best one. There-
fore  one wants at the same time to look at a large number of candidate weak-learners  in order to
ﬁnd a good one  but also needs to look at a large number of training examples  to get an accurate
estimate of the weak-learner performances. As Boosting progresses  the candidate weak-learners
tend to behave more and more similarly  as their performance degrades. While a small number of
samples is initially sufﬁcient to characterize the good weak-learners  it becomes more and more
difﬁcult  and the optimal values for a ﬁxed product Q T moves to larger T and smaller Q.
We focus in this paper on giving a clear mathematical formulation of the behavior described above.
Our main analytical results are Equations (13) and (17) in § 3. They give exact expressions of the

1

expected edge of the selected weak-learner – that is the immediate loss reduction it provides in the
considered Boosting iteration – as a function of the number T of samples and number Q of weak-
learners used in the optimization process. From this result we derive several algorithms described in
§ 4  and estimate their performance compared to standard and state-of-the-art baselines in § 5.

2 Related works

The most computationally intensive operation performed in Boosting is the optimization of the
weak-learners. In the simplest version of the procedure  it requires to estimate for each candidate
weak-learner a score dubbed “edge”  which requires to loop through every training example. Re-
ducing this computational cost is crucial to cope with high-dimensional feature spaces or very large
training sets. This can be achieved through two main strategies: sampling the training examples  or
the feature space  since there is a direct relation between features and weak-learners.
Sampling the training set was introduced historically to deal with weak-learners which can not be
trained with weighted samples. This procedure consists of sampling examples from the training set
according to their Boosting weights  and of approximating a weighted average over the full set by
a non-weighted average over the sampled subset. See § 3.1 for formal details. Such a procedure
has been re-introduced recently for computational reasons [5  8  7]  since the number of subsampled
examples controls the trade-off between statistical accuracy and computational cost.
Sampling the feature space is the central idea behind LazyBoost [6]  and consists simply of replacing
the brute-force exhaustive search over the full feature set by an optimization over a subset produced
by sampling uniformly a predeﬁned number of features. The natural redundancy of most of the
families of features makes such a procedure particularly efﬁcient.
Recently developed methods rely on multi-arms bandit methods to balance properly the exploitation
of features known to be informative  and the exploration of new features [3  4]. The idea behind
those methods is to associate a bandit arm to every feature  and to see the loss reduction as a reward.
Maximizing the overall reduction is achieved with a standard bandit strategy such as UCB [1]  or
Exp3.P [2]  see § 5.2 for details.
These techniques suffer from three important drawbacks. First they make the assumption that the
quality of a feature – the expected loss reduction of a weak-learner using it – is stationary. This goes
against the underpinning of Boosting  which is that at any iteration the performance of the learners
is relative to the sample weights  which evolves over the training (Exp3.P does not make such an
assumption explicitly  but still rely only on the history of past rewards). Second  without additional
knowledge about the feature space  the only structure they can exploit is the stationarity of individual
features. Hence  improvement over random selection can only be achieved by sampling again the
exact same features one has already seen in the past. We therefore only use those methods in a
context where features come from multiple families. This allows us to model the quality  and to bias
the sampling  at the level of families instead of individual features.
Those approaches exploit information about features to bias the sampling  hence making it more
efﬁcient  and reducing the number of weak-learners required to achieve the same loss reduction.
However  they do not explicitly aim at controlling the computational cost. In particular  there is no
notion of varying the number of samples used for the estimation of the weak-learners’ performance.

3 Boosting with noisy maximization

We present in this section some analytical results to approximate a standard round of AdaBoost – or
most other Boosting algorithms – by sampling both the training examples and the features used to
build the weak-learners. Our main goal is to devise a way of selecting the optimal numbers of weak-
learners Q and samples T to look at  so that their product is upper-bounded by a given constant  and
that the expectation of the real performance of the selected weak-learner is maximal.
In § 3.1 we recall standard notation for Boosting  the concept of the edge of a weak-learner  and
In § 3.2 we formalize the
how it can be approximated by a sampling of the training examples.
optimization of the learners and derive the expectation E[G∗] of the true edge G∗ of the selected
weak-learner  and we illustrate these results in the Gaussian case in § 3.3.

2

1{condition} is equal to 1 if the condition is true  0 otherwise
N number of training examples
F number of weak-learners
K number of families of weak-learners
T number of examples subsampled from the full training set
Q number of weak-learners sampled in the case of a single family of features
Q1  . . .   QK number of weak-learners sampled from each one of the K families
(xn  yn) ∈ X × {−1  1} training examples
ωn ∈ R weights of the nth training example in the considered Boosting iteration
Gq true edge of the qth weak-learner
G∗ true edge of the selected weak-learner
e(Q  T ) value of E[G∗]  as a function of Q and T
e(Q1  . . .   QK  T ) value of E[G∗]  in the case of K families of features
Hq approximated edge of the qth weak-learner  estimated from the subsampled T examples
∆q estimation error in the approximated edge Hq − Gq

Table 1: Notations

As stated in the introduction  we will ignore the feature space itself  and only consider in the fol-
lowing sections the set of weak-learners built from it. Also  note that both the Boosting procedure
and our methods are presented in the context of binary classiﬁcation  but can be easily extended to a
multi-class context using for example AdaBoost.MH  which we used in all our experiments.

3.1 Edge estimation with weighting-by-sampling

Given a training set

(1)
and a set H of weak-learners  the standard Boosting procedure consists of building a strong classiﬁer

(xn  yn) ∈ X × {−1  1}  n = 1  . . .   N

f (x) =

αi hi(x)

(2)

by choosing the terms αi ∈ R and hi ∈ H in a greedy manner to minimize a loss estimated over the
training samples.
At every iteration  choosing the optimal weak-learner boils down to ﬁnding the weak-learner with
the largest edge  which is the derivative of the loss reduction w.r.t. the weak-learner weight. The
higher this value  the more the loss can be reduced locally  and thus the better the weak-learner. The
edge is a linear function of the responses of the weak-learner over the samples

(cid:88)

i

N(cid:88)

T(cid:88)

t=1

3

where the ωns depend on the current responses of f over the xns. We consider without loss of

G(h) =

ynωnh(xn) 

n=1

generality that they have been normalized such that(cid:80)N
(cid:20) yN ωN

G(h) = EN∼η

n=1 ωn = 1.

(cid:21)

h(xN )

η(N )

Given an arbitrary distribution η over the sample indexes  with a non-zero mass over every index 
we can rewrite the edge as

(3)

(4)

which  for η(n) = ωn gives

(5)
The idea of weighting-by-sampling consists of replacing the expectation in that expression with an
approximation obtained by sampling. Let N1  . . .   NT   be i.i.d. of distribution η  we deﬁne the
approximated edge as

G(h) = EN∼η [yN h(xN )]

H(h) =

1
T

yNth(xNt) 

(6)

Figure 1: To each of the Q weak-learner corresponds a real edge Gq computed over all the training
examples  and an approximated edge Hq computed from a subsampling of T training examples.
The approximated edge ﬂuctuates around the true value  with a binomial distribution. The Boosting
algorithm selects the weak-learner with the highest approximated edge  which has a real edge G∗.
On this ﬁgure  the largest approximated edge is H1  hence the real edge G∗ of the selected weak-
learner is equal to G1  which is less than G3.

which follows a binomial distribution centered on the true edge  with a variance decreasing with the
number of samples T . It is accurately modeled with

H(h) ∼ N

G 

(1 + G)(1 − G)

T

.

(7)

(cid:18)

(cid:19)

3.2 Formalization of the noisy maximization

Let G1  . . .   GQ be a series of independent  real-valued random variables standing for the true edges
of Q weak-learners sampled randomly. Let ∆1  . . .   ∆Q be a series of independent  real-valued
random variables standing for the noise in the estimation of the edges due to the sampling of only T
training examples  and ﬁnally ∀q  let Hq = Gq + ∆q be the approximated edge.
We deﬁne G∗ as the true edge of the weak-learner  which has the highest approximated edge

G∗ = Gargmax1≤q≤Q Hq

(8)
This quantity is random due to both the sampling of the weak-learners  and the sampling of the
training examples.
The quantity we want to optimize is e(Q  T ) = E[G∗]  the expectation of the true edge of the
selected learner  which increases with both Q and T . A higher Q increases the number of terms in
the maximization of Equation (8)  and a higher T reduces the variance of the ∆s  ensuring that G∗ is
close to maxq Gq. In practice  if the variance of the ∆s is of the order of  or higher than  the variance
of the Gs  the maximization is close to a pure random selection  and looking at many weak-learners
is useless.
We have:

(cid:105)



e(Q  T ) = E[G∗]

= E

Gargmax1≤q≤Q Hq

(cid:104)
Q(cid:88)
Q(cid:88)
Q(cid:88)

q=1

q=1

=

=

=

u(cid:54)=q

Gq
(cid:89)
Gq
 E
(cid:89)
 E[Gq|Hq]

u(cid:54)=q

E

E

E

q=1

4

(9)

(10)

(11)

(12)

(13)

(cid:3)

1{Hq>Hu}

1{Hq>Hu}

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) Hq

E(cid:2)1{Hq>Hu}(cid:12)(cid:12) Hq

(cid:89)

u(cid:54)=q

G*P(H | G )P(H | G )GGGHHH331223123P(H | G )112alytical expressions for both E[Gq|Hq] and E(cid:2)1{Hq>Hu}(cid:12)(cid:12) Hq

(cid:3)  and compute the value of e(Q  T )

If the distributions of the Gqs and the ∆qs are Gaussians or mixtures of Gaussians  we can derive an-

efﬁciently.
In the case of multiple families of weak-learners  it makes sense to model the distributions of the
edges Gq separately for each family  as they often have a more homogeneous behavior inside a
family than across families. We can easily adapt the framework developed in the previous sections
to that case  and we deﬁne e(Q1  . . .   QK  T )  the expected edge of the selected weak-learner when
we sample T examples from the training set  and Qk weak-learners from the kth family.

3.3 Gaussian case

As an illustrative example  we consider here the case where the Gqs  the ∆qs  and hence also the
Hqs all follow Gaussian distributions. We take Gq ∼ N (0  1)  and ∆q ∼ N (0  σ2)  and obtain:

e(Q  T ) = Q E

= Q E

(cid:3)

u(cid:54)=1

 E[G1|H1]
(cid:89)
E(cid:2)1{H1>Hu}(cid:12)(cid:12) H1
(cid:34)
(cid:19)Q−1(cid:35)
(cid:18) H1√
(cid:104)
Q G1Φ (G1)Q−1(cid:105)
(cid:20)
(cid:21)

σ2 + 1

E

H1

σ2 + 1

Φ

=

=

1√
σ2 + 1
1√
σ2 + 1

E

max
1≤q≤Q

Gq

.

(14)

(15)

(16)

(17)

Where  Φ stands for the cumulative distribution function of the unit Gaussian  and σ depends on T .
See Figure 2 for an illustration of the behavior of e(Q  T ) for two different variances of the Gqs.
There is no reason to expect the distribution of the Gqs to be Gaussian  contrary to the ∆qs  as shown
by Equation (7)  but this is not a problem as it can always be approximated by a mixture  for which
we can still derive analytical expressions  even if the Gqs or the ∆qs have different distributions for
different qs.

4 Adaptive Boosting Algorithms

We propose here several new algorithms to sample features and training examples adaptively at each
Boosting step.
While all the formulation above deals with uniform sampling of weak learners  we actually sample
features  and optimize thresholds to build stumps. We observed that after a small number of Boosting
iterations  the Gaussian model of Equation (7) is sufﬁciently accurate.

4.1 Maximum Adaptive Sampling

At every Boosting step  our ﬁrst algorithm MAS Naive models Gq with a Gaussian mixture model
ﬁtted on the edges estimated at the previous iteration  computes from that density model the pair
(Q  T ) maximizing e(Q  T )  samples the corresponding number of examples and features  and keeps
the weak-learner with the highest approximated edge.
The algorithm MAS 1.Q takes into account the decomposition of the feature set into K families of
feature extractors. It models the distributions of the Gqs separately  estimating the distribution of
each on a small number of features and examples sampled at the beginning of each iteration  chosen
so as to account for 10% of the total cost. From these models  it optimizes Q  T and the index l of
the family to sample from  to maximize e(Q1{l=1}  . . .   Q 1{l=K}  T ). Hence  in a given Boosting
step  it does not mix weak-learners based on features from different families.
Finally MAS Q.1 similarly models the distributions of the Gqs  but it optimizes Q1  . . .   QK  T
greedily  starting from Q1 = 0  . . .   QK = 0  and iteratively incrementing one of the Ql so as to
maximize e(Q1  . . .   QK  T ).

5

Figure 2: Simulation of the expectation of G∗ in the case where both the Gqs and the ∆qs follow
Gaussian distributions. Top: Gq ∼ N (0  10−2). Bottom: Gq ∼ N (0  10−4). In both simulations
∆q ∼ N (0  1/T ). Left: Expectation of G∗ vs.
the number of sampled weak-learner Q and the
number of samples T . Right: same value as a function of Q alone  for different ﬁxed costs (product
of the number of examples T and Q). As this graphs illustrates  the optimal value for Q is greater
for larger variances of the Gq. In such a case the Gq are more spread out  and identifying the largest
one can be done despite a large noise in the estimations  hence with a limited number of samples.

4.2 Laminating

The fourth algorithm we have developed tries to reduce the requirement for a density model of
the Gq. At every Boosting step it iteratively reduces the number of considered weak-learners  and
increases the number of samples.
More precisely: given a ﬁxed Q0 and T0  at every Boosting iteration  the Laminating ﬁrst samples
Q0 weak-learners and T0 training examples. Then  it computes the approximated edges and keeps
the Q0/2 best weak-learners. If more than one remains  it samples 2T0 examples  and re-iterates.
The cost of each iteration is constant  equal to Q0T0  and there are at most log2(Q0) of them  leading
to an overall cost of O(log2(Q0)Q0T0). In the experiments  we equalize the computational cost with
the MAS approaches parametrized by T  Q by forcing log2(Q0)Q0T0 = T Q.

5 Experiments

We demonstrate the validity of our approach for pattern recognition on two standard data-sets  using
multiple types of image features. We compare our algorithms both to different ﬂavors of uniform
sampling and to state-of-the-art bandit based methods  all tuned to deal properly with multiple fam-
ilies of features.

5.1 Datasets and features

For the ﬁrst set of experiments we use the well known MNIST handwritten digits database [10]  con-
taining respectively 60 000/10 000 train/test grayscale images of size 28 × 28 pixels  divided in ten
classes. We use features computed by multiple image descriptors  leading to a total of 16 451 fea-
tures. Those descriptors can be broadly divided in two categories. (1) Image transforms: Identity 
Gradient image  Fourier and Haar transforms  Local Binary Patterns (LBP/iLBP). (2) Histograms:

6

1101001 00010 0001101001 00010 00001234 Number of samples TNumber of features Q E[G*]1101001 00010 0001101001 00010 00001234 Number of samples TNumber of features Q E[G*]1101001 00010 00001234Number of features QE[G*] for a given cost TQ TQ = 1 000TQ = 10 000TQ = 100 0001101001 00010 00001234Number of features QE[G*] for a given cost TQ TQ = 1 000TQ = 10 000TQ = 100 000E[G*]TQ = 1 000TQ = 10 000TQ = 100 000E[G*]TQ = 1 000TQ = 10 000TQ = 100 000sums of the intensities in random image patches  histograms of (oriented and non oriented) gradients
at different locations  Haar-like features.
For the second set of experiments we use the challenging CIFAR-10 data-set [9]  a subset of the
80 million tiny images data-set. It contains respectively 50 000/10 000 train/test color images of
size 32 × 32 pixels  also divided in 10 classes. We dub it as challenging as state-of-the-art results
without using additional training data barely reach 65% accuracy. We use directly as features the
same image descriptors as described above for MNIST  plus additional versions of some of them
making use of color information.

5.2 Baselines

We ﬁrst deﬁne three baselines extending LazyBoost in the context of multiple feature families. The
most naive strategy one could think of  that we call Uniform Naive  simply ignores the families  and
picks features uniformly at random. This strategy does not properly distribute the sampling among
the families  thus if one of them had a far greater cardinality than the others  all features would come
from it. We deﬁne Uniform 1.Q to pick one of the feature families at random  and then samples
the Q features from that single family  and Uniform Q.1 to pick uniformly at random Q families of
features  and then pick one feature uniformly in each family.
The second family of baselines we have tested bias their sampling at every Boosting iteration ac-
cording to the observed edges in the previous iterations  and balance the exploitation of families of
features known to perform well with the exploration of new family by using bandit algorithms [3  4].
We use three such baselines (UCB  Exp3.P  -greedy)  which differ only by the underlying bandit
algorithm used.
We tune the meta-parameters of these techniques – namely the scale of the reward and the
exploration-exploitation trade-off – by training them multiple times over a large range of parameters
and keeping only the results of the run with the smallest ﬁnal Boosting loss. Hence  the computa-
tional cost is around one order of magnitude higher than for our methods in the experiments.

Nb. of
stumps

Naive

Uniform

1.Q

Q.1

UCB(cid:63)

Bandits
Exp3.P(cid:63)

MNIST

-greedy(cid:63)

Naive

MAS
1.Q

Q.1

Laminating

-0.34 (0.01) -0.33 (0.02) -0.35 (0.02) -0.33 (0.01) -0.32 (0.01) -0.34 (0.02) -0.51 (0.02) -0.50 (0.02) -0.52 (0.01)
10
-0.80 (0.01) -0.73 (0.03) -0.81 (0.01) -0.73 (0.01) -0.73 (0.02) -0.73 (0.03) -1.00 (0.01) -1.00 (0.01) -1.03 (0.01)
100
1 000
-1.70 (0.01) -1.45 (0.02) -1.68 (0.01) -1.64 (0.01) -1.52 (0.02) -1.60 (0.04) -1.83 (0.01) -1.80 (0.01) -1.86 (0.00)
10 000 -5.32 (0.01) -3.80 (0.02) -5.04 (0.01) -5.26 (0.01) -5.35 (0.04) -5.38 (0.09) -5.35 (0.01) -5.05 (0.02) -5.30 (0.00)

-0.43 (0.00)
-1.01 (0.01)
-1.99 (0.01)
-6.14 (0.01)

-0.26 (0.00) -0.25 (0.01) -0.26 (0.00) -0.25 (0.01) -0.25 (0.01) -0.26 (0.00) -0.28 (0.00) -0.28 (0.00) -0.28 (0.01)
10
-0.33 (0.00) -0.33 (0.01) -0.34 (0.00) -0.33 (0.00) -0.33 (0.00) -0.33 (0.00) -0.35 (0.00) -0.35 (0.00) -0.37 (0.01)
100
1 000
-0.47 (0.00) -0.46 (0.00) -0.48 (0.00) -0.48 (0.00) -0.47 (0.00) -0.48 (0.00) -0.48 (0.00) -0.48 (0.00) -0.49 (0.01)
10 000 -0.93 (0.00) -0.85 (0.00) -0.91 (0.00) -0.90 (0.00) -0.91 (0.00) -0.91 (0.00) -0.93 (0.00) -0.88 (0.00) -0.89 (0.01)

-0.28 (0.00)
-0.37 (0.00)
-0.50 (0.00)
-0.90 (0.00)

CIFAR-10

Table 2: Mean and standard deviation of the Boosting loss (log10) on the two data-sets and for
each method  estimated on ten randomized runs. Methods highlighted with a (cid:63) require the tuning of
meta-parameters which have been optimized by training fully multiple times.

5.3 Results and analysis
We report the results of the proposed algorithms against the baselines introduced in § 5.2 on the
two data-sets of § 5.1 using the standard train/test cuts in tables 2 and 3. We ran each conﬁguration
ten times and report the mean and standard deviation of each. We set the maximum cost of all the
algorithms to 10N  setting Q = 10 and T = N for the baselines  as this conﬁguration leads to the
best results after 10 000 Boosting rounds of AdaBoost.MH.
These results illustrate the efﬁciency of the proposed methods. For 10  100 and 1 000 weak-learners 
both the MAS and the Laminating algorithms perform far better than the baselines. Performance
tend to get similar for 10 000 stumps  which is unusually large.
As stated in § 5.2  the meta-parameters of the bandit methods have been optimized by running the
training fully ten times  with the corresponding computational effort.

7

Nb. of
stumps

Naive

Uniform

1.Q

Q.1

UCB(cid:63)

Bandits
Exp3.P(cid:63)

MNIST

-greedy(cid:63)

Naive

MAS
1.Q

Q.1

Laminating

51.18 (4.22) 54.37 (7.93) 48.15 (3.66) 52.86 (4.75) 53.80 (4.53) 51.37 (6.35) 25.91 (2.04) 25.94 (2.57) 25.73 (1.33) 35.70 (2.35)
10
8.95 (0.41) 11.64 (1.06) 8.69 (0.48) 11.39 (0.53) 11.58 (0.93) 11.59 (1.12) 4.87 (0.29) 4.78 (0.16) 4.54 (0.21)
4.85 (0.16)
100
1.34 (0.08)
1 000
1.75 (0.06) 2.37 (0.12) 1.76 (0.08) 1.80 (0.08) 2.18 (0.14) 1.83 (0.16) 1.50 (0.06) 1.59 (0.08) 1.45 (0.04)
10 000 0.94 (0.06) 1.13 (0.03) 0.94 (0.04) 0.90 (0.05) 0.84 (0.02) 0.85 (0.07) 0.92 (0.03) 0.97 (0.05) 0.94 (0.04)
0.85 (0.04)

76.27 (0.97) 78.57 (1.94) 76.00 (1.60) 77.04 (1.65) 77.51 (1.50) 77.13 (1.15) 71.54 (0.69) 71.13 (0.49) 70.63 (0.34) 71.54 (1.06)
10
56.94 (1.01) 58.33 (1.30) 54.48 (0.64) 57.49 (0.46) 58.47 (0.81) 58.19 (0.83) 53.94 (0.55) 52.79 (0.09) 50.15 (0.64) 50.44 (0.68)
100
1 000
39.13 (0.61) 39.97 (0.37) 37.70 (0.38) 38.13 (0.30) 39.23 (0.31) 38.36 (0.72) 38.79 (0.28) 38.31 (0.27) 36.95 (0.25) 36.39 (0.58)
10 000 31.83 (0.29) 31.16 (0.29) 30.56 (0.30) 30.55 (0.24) 30.39 (0.22) 29.96 (0.45) 32.07 (0.27) 31.36 (0.13) 32.51 (0.38) 31.17 (0.22)

CIFAR-10

Table 3: Mean and standard deviation of the test error (in percent) on the two data-sets and for
each method  estimated on ten randomized runs. Methods highlighted with a (cid:63) require the tuning of
meta-parameters which have been optimized by training fully multiple times.

On the MNIST data-set  when adding 10 or 100 weak-learners  our methods roughly divides the
error rate by two  and still improves it by (cid:39) 30% with 1 000 stumps. The loss reduction follows the
same pattern.
The CIFAR data-set is a very difﬁcult pattern recognition problem. Still our algorithms performs
substantially better than the baselines for 10 and 100 weak-learners  gaining more than 10% in the
test error rates  and behave similarly to the baselines for larger number of stumps.
As stated in § 1  the optimal values for a ﬁxed product Q T moves to larger T and smaller Q.
For instance  On the MNIST data-set with MAS Naive  averaging on ten randomized runs  for
respectively 10  100  1 000  10 000 stumps  T = 1 580  13 030  37 100  43 600  and Q = 388  73 
27  19. We obtain similar and consistent results across settings.
The overhead of MAS algorithms compared to Uniform ones is small  in our experiments  taking
into account the time spent computing features  it is approximately 0.2% for MAS Naive  2% for
MAS 1.Q and 8% for MAS Q.1. The Laminating algorithm has no overhead.
The poor behavior of bandit methods for small number of stumps may be related to the large varia-
tions of the sample weights during the ﬁrst iterations of Boosting  which goes against the underlying
assumption of stationarity of the loss reduction.

6 Conclusion

We have improved Boosting by modeling the statistical behavior of the weak-learners’ edges. This
allowed us to maximize the loss reduction under strict control of the computational cost. Experi-
ments demonstrate that the algorithms perform well on real-world pattern recognition tasks.
Extensions of the proposed methods could be investigated along two axis. The ﬁrst one is to blur the
boundary between the MAS procedures and the Laminating  by deriving an analytical model of the
loss reduction for generalized sampling procedures: Instead of doubling the number of samples and
halving the number of weak-learners  we could adapt both set sizes optimally. The second is to add a
bandit-like component to our methods by adding a variance term related to the lack of samples  and
their obsolescence in the Boosting process. This would account for the degrading density estimation
when weak-learner families have not been sampled for a while  and induce an exploratory sampling
which may be missing in the current algorithms.

Acknowledgments

This work was supported by the European Community’s 7th Framework Programme under grant
agreement 247022 – MASH  and by the Swiss National Science Foundation under grant 200021-
124822 – VELASH. We also would like to thank Dr. Robert B. Israel  Associate Professor Emeritus
at the University of British Columbia for his help on the derivation of the expectation of the true
edge of the weak-learner with the highest approximated edge (equations (9) to (13)).

8

References
[1] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit prob-

lem. Machine Learning  47(2):235–256  2002.

[2] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2003.

[3] R. Busa-Fekete and B. Kegl. Accelerating AdaBoost using UCB. JMLR W&CP  Jan 2009.
[4] R. Busa-Fekete and B. Kegl. Fast Boosting using adversarial bandits. In ICML  2010.
[5] N. Dufﬁeld  C. Lund  and M. Thorup. Priority sampling for estimation of arbitrary subset

sums. J. ACM  54  December 2007.

[6] G. Escudero  L. M`arquez  and G. Rigau. Boosting applied to word sense disambiguation.

Machine Learning: ECML 2000  pages 129–141  2000.

[7] F. Fleuret and D. Geman. Stationary features and cat detection. Journal of Machine Learning

[8] Z. Kalal  J. Matas  and K. Mikolajczyk. Weighted sampling for large-scale Boosting. British

Research (JMLR)  9:2549–2578  2008.

machine vision conference  2008.

[9] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Master’s thesis  2009.

http://www.cs.toronto.edu/˜kriz/cifar.html.

[10] Y. Lecun and C. Cortes. The mnist database of handwritten digits. http://yann.lecun.

com/exdb/mnist/.

9

,Trung Nguyen
Edwin Bonilla
Yunwen Lei
Ke Tang