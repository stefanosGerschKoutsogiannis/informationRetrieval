2012,Sparse Prediction with the $k$-Support Norm,We derive a novel norm that corresponds to the tightest convex   relaxation of sparsity combined with an $\ell_2$ penalty. We show   that this new norm provides a tighter relaxation than the elastic   net  and is thus a good replacement for the Lasso or the elastic net   in sparse prediction problems.  But through studying our new norm    we also bound the looseness of the elastic net  thus shedding new   light on it and providing justification for its use.,Sparse Prediction with the k-Support Norm

Andreas Argyriou
´Ecole Centrale Paris

argyrioua@ecp.fr

Rina Foygel

Department of Statistics  Stanford University

rinafb@stanford.edu

Nathan Srebro

Toyota Technological Institute at Chicago

nati@ttic.edu

Abstract

We derive a novel norm that corresponds to the tightest convex relaxation of spar-
sity combined with an (cid:96)2 penalty. We show that this new k-support norm provides
a tighter relaxation than the elastic net and can thus be advantageous in in sparse
prediction problems. We also bound the looseness of the elastic net  thus shedding
new light on it and providing justiﬁcation for its use.

1

Introduction

Regularizing with the (cid:96)1 norm  when we expect a sparse solution to a regression problem  is often
justiﬁed by (cid:107)w(cid:107)1 being the “convex envelope” of (cid:107)w(cid:107)0 (the number of non-zero coordinates of a
vector w ∈ Rd). That is  (cid:107)w(cid:107)1 is the tightest convex lower bound on (cid:107)w(cid:107)0. But we must be careful
with this statement—for sparse vectors with large entries  (cid:107)w(cid:107)0 can be small while (cid:107)w(cid:107)1 is large.
In order to discuss convex lower bounds on (cid:107)w(cid:107)0  we must impose some scale constraint. A more
accurate statement is that (cid:107)w(cid:107)1 ≤ (cid:107)w(cid:107)∞(cid:107)w(cid:107)0  and so  when the magnitudes of entries in w are
bounded by 1  then (cid:107)w(cid:107)1 ≤ (cid:107)w(cid:107)0  and indeed it is the largest such convex lower bound. Viewed as
a convex outer relaxation 
S(∞)

:=(cid:8)w(cid:12)(cid:12)(cid:107)w(cid:107)0 ≤ k (cid:107)w(cid:107)∞ ≤ 1(cid:9) ⊆(cid:8)w(cid:12)(cid:12)(cid:107)w(cid:107)1 ≤ k(cid:9) .
(cid:8)w(cid:12)(cid:12)(cid:107)w(cid:107)1 ≤ k (cid:107)w(cid:107)∞ ≤ 1(cid:9) = conv(S(∞)

) .

Intersecting the right-hand-side with the (cid:96)∞ unit ball  we get the tightest convex outer bound (convex
hull) of S(∞)

:

k

k

k

However  in our view  this relationship between (cid:107)w(cid:107)1 and (cid:107)w(cid:107)0 yields disappointing learning guar-
antees  and does not appropriately capture the success of the (cid:96)1 norm as a surrogate for sparsity. In
particular  the sample complexity1 of learning a linear predictor with k non-zero entries by empirical
risk minimization inside this class (an NP-hard optimization problem) scales as O(k log d)  but re-
laxing to the constraint (cid:107)w(cid:107)1 ≤ k yields a sample complexity which scales as O(k2 log d)  because
the sample complexity of (cid:96)1-regularized learning scales quadratically with the (cid:96)1 norm [11  20].
Perhaps a better reason for the (cid:96)1 norm being a good surrogate for sparsity is that  not only do we
expect the magnitude of each entry of w to be bounded  but we further expect (cid:107)w(cid:107)2 to be small. In
a regression setting  with a vector of features x  this can be justiﬁed when E[(x(cid:62)w)2] is bounded
(a reasonable assumption) and the features are not too correlated—see  e.g. [15]. More broadly 

1We deﬁne this as the number of observations needed in order to ensure expected prediction error no more
than  worse than that of the best k-sparse predictor  for an arbitrary constant  (that is  we suppress the
dependence on  and focus on the dependence on the sparsity k and dimensionality d).

1

especially in the presence of correlations  we might require this as a modeling assumption to aid
in robustness and generalization. In any case  we have (cid:107)w(cid:107)1 ≤ (cid:107)w(cid:107)2
interested in predictors with bounded (cid:96)2 norm  we can motivate the (cid:96)1 norm through the following
relaxation of sparsity  where the scale is now set by the (cid:96)2 norm:

(cid:8)w(cid:12)(cid:12)(cid:107)w(cid:107)0 ≤ k (cid:107)w(cid:107)2 ≤ B(cid:9) ⊆(cid:8)w(cid:12)(cid:12)(cid:107)w(cid:107)1 ≤ B

(cid:112)(cid:107)w(cid:107)0  and so if we are
k(cid:9) .

√

The sample complexity when using the relaxation now scales as2 O(k log d).

Sparse + (cid:96)2 constraint. Our starting point is then that of combining sparsity and (cid:96)2 regularization 
and learning a sparse predictor with small (cid:96)2 norm. We are thus interested in classes of the form

:=(cid:8)w(cid:12)(cid:12)(cid:107)w(cid:107)0 ≤ k (cid:107)w(cid:107)2 ≤ 1(cid:9) .
As discussed above  the class {(cid:107)w(cid:107)1 ≤ √
k} (corresponding to the standard Lasso) provides a
(cid:111) (cid:40)(cid:110)
w(cid:12)(cid:12)(cid:107)w(cid:107)1 ≤
k . But clearly we can get a tighter relaxation by keeping the (cid:96)2 constraint:
√
(1)

w(cid:12)(cid:12)(cid:107)w(cid:107)1 ≤

convex relaxation of S(2)

k ) ⊆(cid:110)

k (cid:107)w(cid:107)2 ≤ 1

conv(S(2)

S(2)
k

(cid:111)

√

k

.

Constraining (or equivalently  penalizing) both the (cid:96)1 and (cid:96)2 norms  as in (1)  is known as the “elastic
net” [5  21] and has indeed been advocated as a better alternative to the Lasso. In this paper  we ask
whether the elastic net is the tightest convex relaxation to sparsity plus (cid:96)2 (that is  to S(2)
k ) or whether
a tighter  and better  convex relaxation is possible.

A new norm. We consider the convex hull (tightest convex outer bound) of S(2)
k  

k ) = conv(cid:8)w(cid:12)(cid:12)(cid:107)w(cid:107)0 ≤ k (cid:107)w(cid:107)2 ≤ 1(cid:9) .

√

Ck := conv(S(2)

(2)
We study the gauge function associated with this convex set  that is  the norm whose unit ball is
given by (2)  which we call the k-support norm. We show that  for k > 1  this is indeed a tighter
convex relaxation than the elastic net (that is  both inequalities in (1) are in fact strict inequalities) 
and is therefore a better convex constraint than the elastic net when seeking a sparse  low (cid:96)2-norm
linear predictor. We thus advocate using it as a replacement for the elastic net.
However  we also show that the gap between the elastic net and the k-support norm is at most a factor
of
2  corresponding to a factor of two difference in the sample complexity. Thus  our work can
also be interpreted as justifying the use of the elastic net  viewing it as a fairly good approximation
to the tightest possible convex relaxation of sparsity intersected with an (cid:96)2 constraint. Still  even a
factor of two should not necessarily be ignored and  as we show in our experiments  using the tighter
k-support norm can indeed be beneﬁcial.
To better understand the k-support norm  we show in Section 2 that it can also be described as

the group lasso with overlaps norm [10] corresponding to all(cid:0)d

(cid:1) subsets of k features. Despite the

exponential number of groups in this description  we show that the k-support norm can be calculated
efﬁciently in time O(d log d) and that its dual is given simply by the (cid:96)2 norm of the k largest entries.
We also provide efﬁcient ﬁrst-order optimization algorithms for learning with the k-support norm.

k

Related Work In many learning problems of interest  Lasso has been observed to shrink too many
of the variables of w to zero. In particular  in many applications  when a group of variables is highly
correlated  the Lasso may prefer a sparse solution  but we might gain more predictive accuracy by
including all the correlated variables in our model. These drawbacks have recently motivated the use
of various other regularization methods  such as the elastic net [21]  which penalizes the regression
coefﬁcients w with a combination of (cid:96)1 and (cid:96)2 norms:

(cid:27)

(cid:26) 1

2

min

(cid:107)Xw − y(cid:107)2 + λ1 (cid:107)w(cid:107)1 + λ2 (cid:107)w(cid:107)2

2 : w ∈ Rd

 

(3)

2More precisely  the sample complexity is O(B2k log d)  where the dependence on B2 is to be expected.
Note that if feature vectors are (cid:96)∞-bounded (i.e. individual features are bounded)  the sample complexity when
using only (cid:107)w(cid:107)2 ≤ B (without a sparsity or (cid:96)1 constraint) scales as O(B2d). That is  even after identifying
the correct support  we still need a sample complexity that scales with B2.

2

where for a sample of size n  y ∈ Rn is the vector of response values  and X ∈ Rn×d is a matrix
with column j containing the values of feature j.
The elastic net can be viewed as a trade-off between (cid:96)1 regularization (the Lasso) and (cid:96)2 regular-
ization (Ridge regression [9])  depending on the relative values of λ1 and λ2. In particular  when
λ2 = 0  (3) is equivalent to the Lasso. This method  and the other methods discussed below  have
been observed to signiﬁcantly outperform Lasso in many real applications.
The pairwise elastic net (PEN) [13] is a penalty function that accounts for similarity among features:

(cid:107)w(cid:107)P EN

R

= (cid:107)w(cid:107)2

2 + (cid:107)w(cid:107)2

1 − |w|(cid:62)R|w|  

where R ∈ [0  1]p×p is a matrix with Rjk measuring similarity between features Xj and Xk. The
trace Lasso [6] is a second method proposed to handle correlations within X  deﬁned by

(cid:107)w(cid:107)trace

X = (cid:107)Xdiag(w)(cid:107)∗  

where (cid:107)·(cid:107)∗ denotes the matrix trace-norm (the sum of the singular values) and promotes a low-rank
solution. If the features are orthogonal  then both the PEN and the Trace Lasso are equivalent to
the Lasso. If the features are all identical  then both penalties are equivalent to Ridge regression
(penalizing (cid:107)w(cid:107)2). Another existing penalty is OSCAR [3]  given by

(cid:107)w(cid:107)OSCAR

c

= (cid:107)w(cid:107)1 + c

(cid:88)

max{|wj| |wk|} .

Like the elastic net  each one of these three methods also “prefers” averaging similar features over
selecting a single feature.

j<k

2 The k-Support Norm

One argument for the elastic net has been the ﬂexibility of tuning the cardinality k of the regres-
sion vector w. Thus  when groups of correlated variables are present  a larger k may be learned 
which corresponds to a higher λ2 in (3). A more natural way to obtain such an effect of tuning the
cardinality is to consider the convex hull of cardinality k vectors 

k ) = conv{w ∈ Rd(cid:12)(cid:12)(cid:107)w(cid:107)0 ≤ k (cid:107)w(cid:107)2 ≤ 1}.

Ck = conv(S(2)

 

I∈Gk

I∈Gk

I∈Gk

Clearly the sets Ck are nested  and C1 and Cd are the unit balls for the (cid:96)1 and (cid:96)2 norms  respectively.
Consequently we deﬁne the k-support norm as the norm whose unit ball equals Ck (the gauge
function associated with the Ck ball).3 An equivalent deﬁnition is the following variational formula:
Deﬁnition 2.1. Let k ∈ {1  . . .   d}. The k-support norm (cid:107) · (cid:107)sp
(cid:107)vI(cid:107)2 : supp(vI ) ⊆ I 

k is deﬁned  for every w ∈ Rd  as
(cid:88)

(cid:40)(cid:88)

(cid:107)w(cid:107)sp

(cid:41)

k := min

Ck ∀I ∈ Gk (cid:80)

µI = 1. In addition  this immediately implies that (cid:107) · (cid:107)sp

vI = w
where Gk denotes the set of all subsets of {1  . . .   d} of cardinality at most k.
The equivalence is immediate by rewriting vI = µI zI in the above deﬁnition  where µI ≥ 0  zI ∈
k is indeed a norm. In
fact  the k-support norm is equivalent to the norm used by the group lasso with overlaps [10]  when
the set of overlapping groups is chosen to be Gk (however  the group lasso has traditionally been
used for applications with some speciﬁc known group structure  unlike the case considered here).
Although the variational deﬁnition 2.1 is not amenable to computation because of the exponen-
tial growth of the set of groups Gk  the k-support norm is computationally very tractable  with an
O(d log d) algorithm described in Section 2.2.
d = (cid:107) · (cid:107)2. The unit ball of this new norm in
As already mentioned  (cid:107) · (cid:107)sp
R3 for k = 2 is depicted in Figure 1. We immediately notice several differences between this unit
ball and the elastic net unit ball. For example  at points with cardinality k and (cid:96)2 norm equal to 1 
the k-support norm is not differentiable  but unlike the (cid:96)1 or elastic-net norm  it is differentiable at
points with cardinality less than k. Thus  the k-support norm is less “biased” towards sparse vectors
than the elastic net and the (cid:96)1 norm.

1 = (cid:107) · (cid:107)1 and (cid:107) · (cid:107)sp

3The gauge function γCk : Rd → R ∪ {+∞} is deﬁned as γCk (x) = inf{λ ∈ R+ : x ∈ λCk}.

3

Figure 1: Unit ball of the 2-support norm (left) and of the elastic net (right) on R3.

2.1 The Dual Norm
It is interesting and useful to compute the dual of the k-support norm. For w ∈ Rd  denote |w| for
the vector of absolute values  and w

↓
i for the i-th largest element of w [2]. We have

(cid:33) 1

2



(cid:32)(cid:88)

i∈I

 =

(cid:32) k(cid:88)

i=1

(cid:33) 1

2

(cid:107)u(cid:107)sp∗

k = max{(cid:104)w  u(cid:105) : (cid:107)w(cid:107)sp

k ≤ 1} = max

u2
i

: I ∈ Gk

(|u|↓
i )2

=: (cid:107)u(cid:107)(2)
(k) .

This is the (cid:96)2-norm of the largest k entries in u  and is known as the 2-k symmetric gauge norm [2].
Not surprisingly  this dual norm interpolates between the (cid:96)2 norm (when k = d and all entries
are taken) and the (cid:96)∞ norm (when k = 1 and only the largest entry is taken). This parallels the
interpolation of the k-support norm between the (cid:96)1 and (cid:96)2 norms.

2.2 Computation of the Norm

In this section  we derive an alternative formula for the k-support norm  which leads to computation
of the value of the norm in O(d log d) steps.

Proposition 2.1. For every w ∈ Rd  (cid:107)w(cid:107)sp
where  letting |w|↓

|w|↓
0 denote +∞  r is the unique integer in {0  . . .   k − 1} satisfying

i )2 + 1
r+1

(|w|↓

i=k−r

k =

i

k−r−1(cid:80)
d(cid:88)

i=1

(cid:32) d(cid:80)

(cid:33)2 1

2

 

(4)

|w|↓

k−r−1 >

1

|w|↓

i ≥ |w|↓

k−r .

r + 1

i=k−r

((cid:107)w(cid:107)sp

k )2 = max

1
2

(cid:104)u  w(cid:105) − 1
2

((cid:107)u(cid:107)(2)

This result shows that (cid:107) · (cid:107)sp
k trades off between the (cid:96)1 and (cid:96)2 norms in a way that favors sparse
vectors but allows for cardinality larger than k. It combines the uniform shrinkage of an (cid:96)2 penalty
for the largest components  with the sparse shrinkage of an (cid:96)1 penalty for the smallest components.
Proof of Proposition 2.1. We will use the inequality (cid:104)w  u(cid:105) ≤ (cid:104)w↓  u↓(cid:105) [7]. We have

(cid:26)
(cid:41)

(cid:27)
(k))2 : u ∈ Rd
d(cid:88)

αi|w|↓

i + αk

(cid:40)k−1(cid:88)

= max

|w|↓

i − 1
2

(cid:40) d(cid:88)
k(cid:88)

i=1

k(cid:88)

i :

α2

αi|w|↓

i − 1
2

(cid:41)
i : α1 ≥ ··· ≥ αk ≥ 0
α2

i=1

.

α1 ≥ ··· ≥ αd ≥ 0

= max

i=1

i=k

i=k−r

|w|↓

Let Ar :=
αi = |w|↓
αk−1 lie between |w|↓

i for r ∈ {0  . . .   k − 1}. If A0 < |w|↓

d(cid:80)
i for i = 1  . . .   (k − 1)  αi = A0 for i = k  . . .   d. If A0 ≥ |w|↓
(cid:40)k−2(cid:88)

k−1 and A0  and have to be equal. So  the maximization becomes
k−1 : α1 ≥ ··· ≥ αk−1 ≥ 0

i + A1αk−1 − α2
α2

k−2(cid:88)

αi|w|↓

max

(cid:41)

i=1

.

i − 1
2

i=1

k−1 then the solution α is given by

k−1 then the optimal αk 

i=1

4

k−1 and |w|↓

If A0 ≥ |w|↓
i for i = 1  . . .   (k − 2)  αi = A1
k−2 > A1
for i = (k − 1)  . . .   d. Otherwise we proceed as before and continue this process. At stage r the
process terminates if A0 ≥ |w|↓
k−r−1 and all but the last two
inequalities are redundant. Hence the condition can be rewritten as (4). One optimal solution is
αi = |w|↓

2 then the solution is αi = |w|↓
k−1  . . .   Ar−1
i for i = 1  . . .   k − r − 1  αi = Ar

r ≥ |w|↓
r+1 for i = k − r  . . .   d. This proves the claim.

r+1 < |w|↓

k−r  Ar

2

2.3 Learning with the k-support norm

(cid:26) 1

We thus propose using learning rules with k-support norm regularization. These are appropriate
when we would like to learn a sparse predictor that also has low (cid:96)2 norm  and are especially relevant
when features might be correlated (that is  in almost all learning tasks) but the correlation structure
is not known in advance. E.g.  for squared error regression problems we have:

(cid:107)Xw − y(cid:107)2 +

((cid:107)w(cid:107)sp

k )2 : w ∈ Rd

min

(5)
with λ > 0 a regularization parameter and k ∈ {1  . . .   d} also a parameter to be tuned. As typical
in regularization-based methods  both λ and k can be selected by cross validation [8]. Despite the
relationship to S(2)
k   the parameter k does not necessarily correspond to the sparsity of the actual
minimizer of (5)  and should be chosen via cross-validation rather than set to the desired sparsity.

2

λ
2

3 Relation to the Elastic Net

(cid:26) 1

2

(cid:27)

(cid:27)

(cid:27)

Recall that the elastic net with penalty parameters λ1 and λ2 selects a vector of coefﬁcients given by

arg min

(cid:107)Xw − y(cid:107)2 + λ1 (cid:107)w(cid:107)1 + λ2 (cid:107)w(cid:107)2

2

.

(6)

For ease of comparison with the k-support norm  we ﬁrst show that the set of optimal solutions for
the elastic net  when the parameters are varied  is the same as for the norm

(cid:110)(cid:107)w(cid:107)2 (cid:107)w(cid:107)1/

√

(cid:111)

k

 

(cid:107)w(cid:107)el

k := max

when k ∈ [1  d]  corresponding to the unit ball in (1) (note that k is not necessarily an integer). To
see this  let ˆw be a solution to (6)  and let k := ((cid:107) ˆw(cid:107)1/(cid:107) ˆw(cid:107)2)2 ∈ [1  d] . Now for any w (cid:54)= ˆw 
if (cid:107)w(cid:107)el
k   then (cid:107)w(cid:107)p ≤ (cid:107) ˆw(cid:107)p for p = 1  2. Since ˆw is a solution to (6)  therefore 
(cid:107)Xw − y(cid:107)2

2. This proves that  for some constraint parameter B 

2 ≥ (cid:107)X ˆw − y(cid:107)2

k ≤ (cid:107) ˆw(cid:107)el

ˆw = arg min

(cid:107)Xw − y(cid:107)2

2 : (cid:107)w(cid:107)el

k ≤ B

.

(cid:26) 1

n

Like the k-support norm  the elastic net interpolates between the (cid:96)1 and (cid:96)2 norms. In fact  when k
is an integer  any k-sparse unit vector w ∈ Rd must lie in the unit ball of (cid:107) · (cid:107)el
k . Since the k-support
norm gives the convex hull of all k-sparse unit vectors  this immediately implies that

(cid:107)w(cid:107)el

k ≤ (cid:107)w(cid:107)sp

k

∀ w ∈ Rd .

The two norms are not equal  however. The difference between the two is illustrated in Figure 1 
where we see that the k-support norm is more “rounded”.
To see an example where the two norms are not equal  we set d = 1 + k2 for some large k  and let
w = (k1.5  1  1  . . .   1)(cid:62) ∈ Rd. Then

(cid:26)(cid:112)

(cid:107)w(cid:107)el

k = max

k1.5 + k2√
k3 + k2 
)(cid:62)  we have (cid:107)u(cid:107)(2)

k

(cid:27)

(cid:18)

(cid:19)

.

1√
k

= k1.5

1 +

Taking u = ( 1√
2
k-support norm:

 

1√
2k

 

1√
2k

  . . .  

1√
2k

(k) < 1  and recalling this norm is dual to the

(cid:107)w(cid:107)sp

k > (cid:104)w  u(cid:105) =

k1.5√
2

+ k2 ·

1√
2k

=

√

2 · k1.5 .

√

2. We now show

In this example  we see that the two norms can differ by as much as a factor of
that this is actually the most by which they can differ.

5

√

k ≤ (cid:107) · (cid:107)sp

Proposition 3.1. (cid:107) · (cid:107)el
Proof. We show that these bounds hold in the duals of the two norms. First  since (cid:107) · (cid:107)el
maximum over the (cid:96)1 and (cid:96)2 norms  its dual is given by
√

2(cid:107) · (cid:107)el
k .

k <

(cid:111)

k is a

(cid:110)(cid:107)a(cid:107)2 +

(cid:107)u(cid:107)(el)∗

k

:= inf
a∈Rd

k · (cid:107)u − a(cid:107)∞

Now take any u ∈ Rd. First we show (cid:107)u(cid:107)(2)
u1 ≥ ··· ≥ ud ≥ 0. For any a ∈ Rd 

(k) ≤ (cid:107)u(cid:107)(el)∗

k

(cid:107)u(cid:107)(2)

(k) = (cid:107)u1:k(cid:107)2 ≤ (cid:107)a1:k(cid:107)2 + (cid:107)u1:k − a1:k(cid:107)2 ≤ (cid:107)a(cid:107)2 +

√

k(cid:107)u − a(cid:107)∞ .

. Without loss of generality  we take

Finally  we show that (cid:107)u(cid:107)(el)∗

k

<

2(cid:107)u(cid:107)(2)

(k). Let a = (u1 − uk+1  . . .   uk − uk+1  0  . . .   0)(cid:62). Then

(cid:107)u(cid:107)(el)∗

k

≤ (cid:107)a(cid:107)2 +

k · (cid:107)u − a(cid:107)∞ =

√

(ui − uk+1)2 +

√

k · |uk+1|

√

(cid:118)(cid:117)(cid:117)(cid:116) k(cid:88)

(cid:113)

i=1

k u2

k+1 ≤

√

2 ·

(cid:118)(cid:117)(cid:117)(cid:116) k(cid:88)

(cid:118)(cid:117)(cid:117)(cid:116) k(cid:88)

≤

i − u2
(u2

k+1) +

i − u2
(u2

k+1) + k u2

k+1 =

√

2(cid:107)u(cid:107)(2)
(k) .

i=1

i=1

Furthermore  this yields a strict inequality  because if u1 > uk+1  the next-to-last inequality is strict 
while if u1 = ··· = uk+1  then the last inequality is strict.

4 Optimization

Solving the optimization problem (5) efﬁciently can be done with a ﬁrst-order proximal algorithm.
Proximal methods – see [1  4  14  18  19] and references therein – are used to solve composite
problems of the form min{f (x) + ω(x) : x ∈ Rd}  where the loss function f (x) and the regularizer
ω(x) are convex functions  and f is smooth with an L-Lipschitz gradient. These methods require
fast computation of the gradient ∇f and the proximity operator

(cid:27)

proxω(x) := argmin

(cid:107)u − x(cid:107)2 + ω(u) : u ∈ Rd

.

(cid:26) 1

2

To obtain a proximal method for k-support regularization  it sufﬁces to compute the proximity map
of g = 1
λ ). This
computation can be done in O(d(k + log d)) steps with Algorithm 1.

k )2  for any β > 0 (in particular  for problem (5) β corresponds to L

2β ((cid:107) · (cid:107)sp

Algorithm 1 Computation of the proximity operator.

Input v ∈ Rd
Output q = prox 1
Find r ∈ {0  . . .   k − 1}  (cid:96) ∈ {k  . . .   d} such that

2β ((cid:107)·(cid:107)sp

k )2(v)

1

β+1 zk−r−1 >

Tr (cid:96)

(cid:96)−k+(β+1)r+β+1 ≥ 1
(cid:96)−k+(β+1)r+β+1 ≥ z(cid:96)+1

Tr (cid:96)

β+1 zk−r

z(cid:96) >

(cid:96)(cid:80)

where z := |v|↓  z0 := +∞  zd+1 := −∞  Tr (cid:96) :=

zi

 β

β+1 zi
zi −
0

qi ←

Tr (cid:96)

(cid:96)−k+(β+1)r+β+1

i=k−r
if i = 1  . . .   k − r − 1
if i = k − r  . . .   (cid:96)
if i = (cid:96) + 1  . . .   d

Reorder and change signs of q to conform with v

6

(7)

(8)

Figure 2: Solutions learned for the synthetic data. Left to right: k-support  Lasso and elastic net.

(cid:16)

(cid:96)(cid:80)

Proof of Correctness of Algorithm 1. Since the support-norm is sign and permutation invariant 
proxg(v) has the same ordering and signs as v. Hence  without loss of generality  we may assume
that v1 ≥ ··· ≥ vd ≥ 0 and require that q1 ≥ ··· ≥ qd ≥ 0  which follows from inequality (7) and
the fact that z is ordered.
Now  q = proxg(v) is equivalent to βz − βq = βv − βq ∈ ∂ 1
k )2(q). It sufﬁces to show
d(cid:80)
that  for w = q  βz − βq is an optimal α in the proof of Proposition 2.1. Indeed  Ar corresponds to
(cid:96)−k+(β+1)r+β+1
i=k−r
and (4) is equivalent to condition (7). For i ≤ k− r− 1  we have βzi − βqi = qi. For k− r ≤ i ≤ (cid:96) 
we have βzi − βqi = 1
r+1 Ar  which
is true by (8).
We can now apply a standard accelerated proximal method  such as FISTA [1]  to (5)  at each
iteration using the gradient of the loss and performing a prox step using Algorithm 1. The FISTA
guarantee ensures us that  with appropriate step sizes  after T such iterations  we have:

r+1 Ar. For i ≥ (cid:96) + 1  since qi = 0  we only need βzi − βqi ≤ 1

= Tr (cid:96) − ((cid:96)−k+r+1)Tr (cid:96)

(cid:96)−k+(β+1)r+β+1 = (r + 1)

(cid:96)−k+(β+1)r+β+1

2 ((cid:107) · (cid:107)sp

zi −

qi =

i=k−r

β Tr (cid:96)

(cid:17)

Tr (cid:96)

(cid:32)

(cid:33)

(cid:107)Xw∗ − y(cid:107)2 +

1
2

((cid:107)w∗(cid:107)sp
k )2

λ
2

2L(cid:107)w∗ − w1(cid:107)2

(T + 1)2

.

+

(cid:107)XwT − y(cid:107)2 +

1
2

((cid:107)wT(cid:107)sp

k )2 ≤

λ
2

5 Empirical Comparisons

√

Our theoretical analysis indicates that the k-support norm and the elastic net differ by at most a factor
of
2  corresponding to at most a factor of two difference in their sample complexities and general-
ization guarantees. We thus do not expect huge differences between their actual performances  but
would still like to see whether the tighter relaxation of the k-support norm does yield some gains.

Synthetic Data For the ﬁrst simulation we follow [21  Sec. 5  example 4]. In this experimental
protocol  the target (oracle) vector equals w∗ = (3  . . .   3

)  with y = (w∗)(cid:62)x + N (0  1).

  0 . . .   0

(cid:124) (cid:123)(cid:122) (cid:125)

15

(cid:124) (cid:123)(cid:122) (cid:125)

25

The input data X were generated from a normal distribution such that components 1  . . .   5 have the
same random mean Z1 ∼ N (0  1)  components 6  . . .   10 have mean Z2 ∼ N (0  1) and components
11  . . .   15 have mean Z3 ∼ N (0  1). A total of 50 data sets were created in this way  each containing
50 training points  50 validation points and 350 test points. The goal is to achieve good prediction
performance on the test data.
We compared the k-support norm with Lasso and the elastic net. We considered the ranges k =
{1  . . .   d} for k-support norm regularization  λ = 10i  i = {−15  . . .   5}  for the regularization
parameter of Lasso and k-support regularization and the same range for the λ1  λ2 of the elastic net.
For each method  the optimal set of parameters was selected based on mean squared error on the
validation set. The error reported in Table 5 is the mean squared error with respect to the oracle w∗ 
namely M SE = ( ˆw − w∗)(cid:62)V ( ˆw − w∗)  where V is the population covariance matrix of Xtest.
To further illustrate the effect of the k-support norm  in Figure 5 we show the coefﬁcients learned
by each method  in absolute value. For each image  one row corresponds to the w learned for one
of the 50 data sets. Whereas all three methods distinguish the 15 relevant variables  the elastic net
result varies less within these variables.

South African Heart Data This is a classiﬁcation task which has been used in [8]. There are
9 variables and 462 examples  and the response is presence/absence of coronary heart disease. We

7

510152025303540510152025303540455051015202530354051015202530354045505101520253035405101520253035404550Table 1: Mean squared errors and classiﬁcation accuracy for the synthetic data (median over 50 repetition) 
SA heart data (median over 50 replications) and for the “20 newsgroups” data set. (SE = standard error)

Method
Lasso

Elastic net
k-support

Synthetic
MSE (SE)

0.2685 (0.02)
0.2274 (0.02)
0.2143 (0.02)

MSE (SE)
0.18 (0.005)
0.18 (0.005)
0.18 (0.005)

Heart

Newsgroups

Accuracy (SE) MSE Accuracy
66.41 (0.53)
66.41 (0.53)
66.41 (0.53)

73.02
73.02
73.40

0.70
0.70
0.69

normalized the data so that each predictor variable has zero mean and unit variance. We then split the
data 50 times randomly into training  validation  and test sets of sizes 400  30  and 32 respectively.
For each method  parameters were selected using the validation data. In Tables 5  we report the
MSE and accuracy of each method on the test data. We observe that all three methods have identical
performance.

20 Newsgroups This is a binary classiﬁcation version of 20 newsgroups created in [12] which can
be found in the LIBSVM data repository.4 The positive class consists of the 10 groups with names of
form sci.*  comp.*  or misc.forsale and the negative class consists of the other 10 groups. To reduce
the number of features  we removed the words which appear in less than 3 documents. We randomly
split the data into a training  a validation and a test set of sizes 14000 1000 and 4996  respectively.
We report MSE and accuracy on the test data in Table 5. We found that k-support regularization
gave improved prediction accuracy over both other methods.5

6 Summary

√

We introduced the k-support norm as the tightest convex relaxation of sparsity plus (cid:96)2 regularization 
and showed that it is tighter than the elastic net by exactly a factor of
2. In our view  this sheds
light on the elastic net as a close approximation to this tightest possible convex relaxation  and
motivates using the k-support norm when a tighter relaxation is sought. This is also demonstrated
in our empirical results.
We note that the k-support norm has better prediction properties  but not necessarily better sparsity-
inducing properties  as evident from its more rounded unit ball. It is well understood that there
is often a tradeoff between sparsity and good prediction  and that even if the population optimal
predictor is sparse  a denser predictor often yields better predictive performance [3  10  21]. For
example  in the presence of correlated features  it is often beneﬁcial to include several highly corre-
lated features rather than a single representative feature. This is exactly the behavior encouraged by
(cid:96)2 norm regularization  and the elastic net is already known to yield less sparse (but more predictive)
solutions. The k-support norm goes a step further in this direction  often yielding solutions that are
even less sparse (but more predictive) compared to the elastic net.
Nevertheless  it is interesting to consider whether compressed sensing results  where (cid:96)1 regulariza-
tion is of course central  can be reﬁned by using the k-support norm  which might be able to handle
more correlation structure within the set of features.

Acknowledgements The construction showing that the gap between the elastic net and the k-
2 is due to joint work with Ohad Shamir. Rina Foygel was
overlap norm can be as large as
supported by NSF grant DMS-1203762.

√

References
[1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal of Imaging Sciences  2(1):183–202  2009.

[2] R. Bhatia. Matrix Analysis. Graduate Texts in Mathematics. Springer  1997.
4http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/
5Regarding other sparse prediction methods  we did not manage to compare with OSCAR  due to memory

limitations  or to PEN or trace Lasso  which do not have code available online.

8

[3] H.D. Bondell and B.J. Reich. Simultaneous regression shrinkage  variable selection  and su-

pervised clustering of predictors with OSCAR. Biometrics  64(1):115–123  2008.

[4] P.L. Combettes and V.R. Wajs. Signal recovery by proximal forward-backward splitting. Mul-

tiscale Modeling and Simulation  4(4):1168–1200  2006.

[5] C. De Mol  E. De Vito  and L. Rosasco. Elastic-net regularization in learning theory. Journal

of Complexity  25(2):201–230  2009.

[6] E. Grave  G. R. Obozinski  and F. Bach. Trace lasso: a trace norm regularization for correlated
In J. Shawe-Taylor  R.S. Zemel  P. Bartlett  F.C.N. Pereira  and K.Q. Weinberger 

designs.
editors  Advances in Neural Information Processing Systems 24  2011.

[7] G. H. Hardy  J. E. Littlewood  and G. P´olya. Inequalities. Cambridge University Press  1934.
[8] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning: Data Mining 

Inference and Prediction. Springer Verlag Series in Statistics  2001.

[9] A.E. Hoerl and R.W. Kennard. Ridge regression: Biased estimation for nonorthogonal prob-

lems. Technometrics  pages 55–67  1970.

[10] L. Jacob  G. Obozinski  and J.P. Vert. Group Lasso with overlap and graph Lasso. In Pro-
ceedings of the 26th Annual International Conference on Machine Learning  pages 433–440.
ACM  2009.

[11] S.M. Kakade  K. Sridharan  and A. Tewari. On the complexity of linear prediction: Risk
In Advances in Neural Information Processing

bounds  margin bounds  and regularization.
Systems  volume 22  2008.

[12] S. S. Keerthi and D. DeCoste. A modiﬁed ﬁnite Newton method for fast solution of large scale

linear SVMs. Journal of Machine Learning Research  6:341–361  2005.

[13] A. Lorbert  D. Eis  V. Kostina  D.M. Blei  and P.J. Ramadge. Exploiting covariate similarity
In Proceedings of the 13th International

in sparse regression via the pairwise elastic net.
Conference on Artiﬁcial Intelligence and Statistics  2010.

[14] Y. Nesterov. Gradient methods for minimizing composite objective function. CORE  2007.
[15] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low-noise and fast rates. In Advances in

Neural Information Processing Systems 23  2010.

[16] T. Suzuki and R. Tomioka. SpicyMKL: a fast algorithm for multiple kernel learning with

thousands of kernels. Machine learning  pages 1–32  2011.

[17] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society  Series B (Statistical Methodology)  58(1):267–288  1996.

[18] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization.

Preprint  2008.

[19] P. Tseng. Approximation accuracy  gradient methods  and error bound for structured convex

optimization. Mathematical Programming  125(2):263–295  2010.

[20] T. Zhang. Covering number bounds of certain regularized linear function classes. The Journal

of Machine Learning Research  2:527–550  2002.

[21] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  67(2):301–320  2005.

9

,Christina Lee
Asuman Ozdaglar
Devavrat Shah
Cem Tekin
Mihaela van der Schaar