2015,Max-Margin Majority Voting for Learning from Crowds,Learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers. This paper presents max-margin majority voting (M^3V) to improve the discriminative ability of majority voting and further presents a Bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices. We formulate the joint learning as a regularized Bayesian inference problem  where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label. Our Bayesian model naturally covers the Dawid-Skene estimator and M^3V. Empirical results demonstrate that our methods are competitive  often achieving better results than state-of-the-art estimators.,Max-Margin Majority Voting for

Learning from Crowds

Department of Computer Science & Technology; Center for Bio-Inspired Computing Research

Tsinghua National Lab for Information Science & Technology

Tian Tian  Jun Zhu

State Key Lab of Intelligent Technology & Systems; Tsinghua University  Beijing 100084  China

tiant13@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn

Abstract

Learning-from-crowds aims to design proper aggregation strategies to infer the
unknown true labels from the noisy labels provided by ordinary web workers.
This paper presents max-margin majority voting (M3V) to improve the discrimi-
native ability of majority voting and further presents a Bayesian generalization to
incorporate the ﬂexibility of generative methods on modeling noisy observations
with worker confusion matrices. We formulate the joint learning as a regularized
Bayesian inference problem  where the posterior regularization is derived by max-
imizing the margin between the aggregated score of a potential true label and that
of any alternative label. Our Bayesian model naturally covers the Dawid-Skene
estimator and M3V. Empirical results demonstrate that our methods are competi-
tive  often achieving better results than state-of-the-art estimators.

1

Introduction

Many learning tasks require labeling large datasets. Though reliable  it is often too expensive and
time-consuming to collect labels from domain experts or well-trained workers. Recently  online
crowdsourcing platforms have dramatically decreased the labeling cost by dividing the workload
into small parts  then distributing micro-tasks to a crowd of ordinary web workers [17  20]. However 
the labeling accuracy of web workers could be lower than expected due to their various backgrounds
or lack of knowledge. To improve the accuracy  it is usually suggested to label every task multiple
times by different workers  then the redundant labels can provide hints on resolving the true labels.
Much progress has been made in designing effective aggregation mechanisms to infer the true labels
from noisy observations. From a modeling perspective  existing work includes both generative ap-
proaches and discriminative approaches. A generative method builds a ﬂexible probabilistic model
for generating the noisy observations conditioned on the unknown true labels and some behavior
assumptions  with examples of the Dawid-Skene (DS) estimator [5]  the minimax entropy (Entropy)
estimator1 [24  25]  and their variants. In contrast  a discriminative approach does not model the ob-
servations; it directly identiﬁes the true labels via some aggregation rules. Examples include major-
ity voting and the weighted majority voting that takes worker reliability into consideration [10  11].
In this paper  we present a max-margin formulation of the most popular majority voting estimator to
improve its discriminative ability  and further present a Bayesian generalization that conjoins the ad-
vantages of both generative and discriminative approaches. The max-margin majority voting (M3V)
directly maximizes the margin between the aggregated score of a potential true label and that of any
alternative label  and the Bayesian model consists of a ﬂexible probabilistic model to generate the
noisy observations by conditioning on the unknown true labels. We adopt the same approach as the

1A maximum entropy estimator can be understood as a dual of the MLE of a probabilistic model [6].

1

classical Dawid-Skene estimator to build the probabilistic model by considering worker confusion
matrices  though many other generative models are also possible. Then  we strongly couple the
generative model and M3V by formulating a joint learning problem under the regularized Bayesian
inference (RegBayes) [27] framework  where the posterior regularization [7] enforces a large mar-
gin between the potential true label and any alternative label. Naturally  our Bayesian model covers
both the David-Skene estimator and M3V as special cases by setting the regularization parameter to
its extreme values (i.e.  0 or ∞). We investigate two choices on deﬁning the max-margin posterior
regularization: (1) an averaging model with a variational inference algorithm; and (2) a Gibbs model
with a Gibbs sampler under a data augmentation formulation. The averaging version can be seen
as an extension to the MLE learner of Dawid-Skene model. Experiments on real datasets suggest
that max-margin learning can signiﬁcantly improve the accuracy of majority voting  and that our
Bayesian estimators are competitive  often achieving better results than state-of-the-art estimators
on true label estimation tasks.

2 Preliminary
We consider the label aggregation problem with a dataset consisting of M items (e.g.  pictures or
paragraphs). Each item i has an unknown true label yi ∈ [D]  where [D] := {1  . . .   D}. The task
ti is to label item i. In crowdsourcing  we have N workers assigning labels to these items. Each
worker may only label a part of the dataset. Let Ii ⊆ [N ] denote the workers who have done task
ti. We use xij to denote the label of ti provided by worker j  xi to denote the labels provided to
task ti  and X is the collection of these worker labels  which is an incomplete matrix. The goal of
learning-from-crowds is to estimate the true labels of items from the noisy observations X.

2.1 Majority Voting Estimator
Majority voting (MV) is arguably the simplest method. It posits that for every task the true label is
always most commonly given. Thus  it selects the most frequent label for each task as its true label 
by solving the problem:

(1)
where I(·) is an indicator function. It equals to 1 whenever the predicate is true  otherwise it equals to
0. Previous work has extended this method to weighted majority voting (WMV) by putting different
weights on workers to measure worker reliability [10  11].

ˆyi = argmax
d∈[D]

j=1

I(xij = d) ∀i ∈ [M ] 

N(cid:88)

2.2 Dawid-Skene Estimator

The method of Dawid and Skene [5] is a generative approach by considering worker confusability.
It posits that the performance of a worker is consistent across different tasks  as measured by a
confusion matrix whose diagonal entries denote the probability of assigning correct labels while off-
diagonal entries denote the probability of making speciﬁc mistakes to label items in one category as
another. Formally  let φj be the confusion matrix of worker j. Then  φjkd denotes the probability
that worker j assigns label d to an item whose true label is k. Under the basic assumption that
workers ﬁnish each task independently  the likelihood of observed labels can be expressed as

φjkd

njkd  

(2)

M(cid:89)

N(cid:89)

D(cid:89)

p(X|Φ  y) =

jkd = I(xij = d  yi = k)  and njkd =(cid:80)M

φjkd

d k=1

i=1

j=1

ni
jkd =

N(cid:89)

D(cid:89)

j=1

d k=1

i=1 ni

jkd is the number of tasks with true label k

where ni
but being labeled to d by worker j.
The unknown labels and parameters can be estimated by maximum-likelihood estimation (MLE) 
{ ˆy  ˆΦ} = argmaxy Φ log p(X|Φ  y)  via an expectation-maximization (EM) algorithm that itera-
tively updates the true labels y and the parameters Φ. The learning procedure is often initialized
by majority voting to avoid bad local optima. If we assume some structure of the confusion matrix 
various variants of the DS estimator have been studied  including the homogenous DS model [15]
and the class-conditional DS model [11]. We can also put a prior over worker confusion matrices
and transform the inference into a standard inference problem in graphical models [12]. Recently 
spectral methods have also been applied to better initialize the DS model [23].

2

3 Max-Margin Majority Voting
Majority voting is a discriminative model that directly ﬁnds the most likely label for each item.
In this section  we present max-margin majority voting (M3V)  a novel extension of (weighted)
majority voting with a new notion of margin (named crowdsourcing margin).

N g(xi  d) ≥ 0 

N g(xi  yi) − 1(cid:62)
1(cid:62)

Figure 1: A geometric interpretation of the crowd-
sourcing margin.

3.1 Geometric Interpretation of Crowdsourcing Margin
Let g(xi  d) be a N-dimensional vector  with
the element j equaling to I(j ∈ Ii  xij = d).
Then  the estimation of the vanilla majority vot-
ing in Eq. (1) can be formulated as ﬁnding so-
lutions {yi}i∈[M ] that satisfy the following con-
straints:
∀i  d  (3)
where 1N is the N-dimensional all-one vector
and 1(cid:62)
N g(xi  k) is the aggregated score of the
potential true label k for task ti. By using the
all-one vector  the aggregated score has an intu-
itive interpretation — it denotes the number of
workers who have labeled ti as class k.
Apparently  the all-one vector treats all workers equally  which may be unrealistic in practice due
to the various backgrounds of the workers. By simply choosing what the majority of workers agree
on  the vanilla MV is prone to errors when many workers give low quality labels. One way to tackle
this problem is to take worker reliability into consideration. Let η denote the worker weights. When
these values are known  we can get the aggregated score η(cid:62)g(xi  k) of a weighted majority voting
(WMV)  and estimate the true labels by the rule: ˆyi = argmaxd∈[D] η(cid:62)g(xi  d). Thus  reliable
workers contribute more to the decisions.
Geometrically  g(xi  d) is a point in the N-dimensional space for each task ti. The aggregated
score 1(cid:62)
N g(xi  d) measures the distance (up to a constant scaling) from this point to the hyperplane
1(cid:62)
N x = 0. So the MV estimator actually ﬁnds a point that has the largest distance to that hyperplane
N x−b = 0 which
for each task  and the decision boundary of majority voting is another hyperplane 1(cid:62)
separates the point g(xi  ˆyi) from the other points g(xi  k)  k (cid:54)= ˆyi. By introducing the worker
weights η  we relax the constraint of the all-one vector to allow for more ﬂexible decision boundaries
η(cid:62)x−b = 0. All the possible decision boundaries with the same orientation are equivalent. Inspired
by the generalized notion of margin in multi-class SVM [4]  we deﬁne the crowdsourcing margin as
the minimal difference between the aggregated score of the potential true label and the aggregated
scores of other alternative labels. Then  one reasonable choice of the best hyperplane (i.e. η) is the
one that represents the largest margin between the potential true label and other alternatives.
Fig. 1 provides an illustration of the crowdsourcing margin for WMV with D = 3 and N = 2 
where each axis represents the label of a worker. Assume that both workers provide labels 3 and 1
to item i. Then  the vectors g(xi  y)  y ∈ [3] are three points in the 2D plane. Given the worker
weights η  the estimated label should be 1  since g(xi  1) has the largest distance to line P0. Line P1
and line P2 are two boundaries that separate g(xi  1) and other points. The margin is the distance
between them. In this case  g(xi  1) and g(xi  3) are support vectors that decide the margin.

3.2 Max-Margin Majority Voting Estimator
Let (cid:96) be the minimum margin between the potential true label and all other alternatives. We deﬁne
the max-margin majority voting (M3V) as solving the constrained optimization problem to estimate
the true labels y and weights η:

(cid:107)η(cid:107)2
1
2
i (d) ≥ (cid:96)∆
i (d) ∀i ∈ [M ]  d ∈ [D] 
i (d) = (cid:96)I(yi (cid:54)= d). And in practice  the worker
i (d) := g(xi  yi) − g(xi  d) 2 and (cid:96)∆
where g∆
labels are often linearly inseparable by a single hyperplane. Therefore  we relax the hard constraints

s. t. : η(cid:62)g∆

(4)

inf
η y

2

2The offset b is canceled out in the margin constraints.

3

𝑥𝑖2𝑥𝑖1𝒈𝒙𝑖 1:(𝟎 𝟏)𝑻𝒈𝒙𝑖 2:(𝟎 𝟎)𝑻𝒈𝒙𝑖 3:(𝟏 𝟎)𝑻by introducing non-negative slack variables {ξi}M
max-margin majority voting as
(cid:107)η(cid:107)2
1
inf
2
i (d) − ξi ∀i ∈ [M ]  d ∈ [D] 
i (d) ≥ (cid:96)∆

s. t. : η(cid:62)g∆

ξi≥0 η y

(cid:88)

2 + c

ξi

i

i=1  one for each task  and deﬁne the soft-margin

(5)

(6)

(7)

where c is a positive regularization parameter and (cid:96) − ξi is the soft-margin for task ti. The value of
ξi reﬂects the difﬁculty of task ti — a small ξi suggests a large discriminant margin  indicating that
the task is easy with a rare chance to make mistakes; while a large ξi suggests that the task is hard
with a higher chance to make mistakes. Note that our max-margin majority voting is signiﬁcantly
different from the unsupervised SVMs (or max-margin clustering) [21]  which aims to assign cluster
labels to the data points by maximizing some different notion of margin with balance constraints to
avoid trivial solutions. Our M3V does not need such balance constraints.
Albeit not jointly convex  problem (5) can be solved by iteratively updating η and y to ﬁnd a local
i (d) by the fact that the

optimum. For η  the solution can be derived as η = (cid:80)M
(cid:88)

subproblem is convex. The parameters ω are obtained by solving the dual problem

(cid:80)D

(cid:88)

d=1 ωd

i g∆

i=1

η(cid:62)η +

− 1
2

sup
0≤ωd

i ≤c

i

d

ωd
i (cid:96)∆

i (d) 

which is exactly the QP dual problem in standard SVM [4]. So it can be efﬁciently solved by well-
developed SVM solvers like LIBSVM [2]. For updating y  we deﬁne (x)+ := max(0  x)  and then
it is a weighted majority voting with a margin gap constraint:

(cid:18)

(cid:0)(cid:96)∆

ˆyi = argmax
yi∈[D]

−c max
d∈[D]

i (d) − ˆη(cid:62)g∆

i (d)(cid:1)

(cid:19)

 

+

Overall  the algorithm is a max-margin iterative weighted majority voting (MM-IWMV). Comparing
with the iterative weighted majority voting (IWMV) [11]  which tends to maximize the expected gap
of the aggregated scores under the Homogenous DS model  our M3V directly maximizes the data
speciﬁed margin without further assumption on data model. Empirically  as we shall see  our M3V
could have more powerful discriminative ability with better accuracy than IWMV.

4 Bayesian Max-Margin Estimator

With the intuitive and simple max-margin principle  we now present a more sophisticated Bayesian
max-margin estimator  which conjoins the discriminative ability of M3V and the ﬂexibility of the
generative DS estimator. Though slightly more complicated in learning and inference  the Bayesian
models retain the intuitive simplicity of M3V and the ﬂexibility of DS  as explained below.

4.1 Model Deﬁnition

We adopt the same DS model to generate observations conditioned on confusion matrices  with the
full likelihood in Eq. (2). We further impose a prior p0(Φ  η) for Bayesian inference. Assuming that
the true labels y are given  we aim to get the target posterior p(Φ  η|X  y)  which can be obtained
by solving an optimization problem:

inf

q(Φ η)

(8)
where L(q; y) := KL(q(cid:107)p0(Φ  η)) − Eq[log p(X|Φ  y)] measures the Kullback-Leibler (KL) di-
vergence between a desired post-data posterior q and the original Bayesian posterior  and p0(Φ  η)
is the prior  often factorized as p0(Φ)p0(η). As we shall see  this Bayesian DS estimator often leads
to better performance than the vanilla DS.
Then  we explore the ideas of regularized Bayesian inference (RegBayes) [27] to incorporate
max-margin majority voting constraints as posterior regularization on problem (8)  and deﬁne the
Bayesian max-margin estimator (denoted by CrowdSVM) as solving:

L (q(Φ  η); y)  

L(q(Φ  η); y) + c ·(cid:88)

inf

ξi≥0 q∈P y
s. t. : Eq[η(cid:62)g∆

i (d)] ≥ (cid:96)∆

i (d) − ξi ∀i ∈ [M ]  d ∈ [D] 

i

ξi

(9)

4

where P is the probabilistic simplex  and we take expectation over q to deﬁne the margin constraints.
Such posterior constraints will inﬂuence the estimates of y and Φ to get better aggregation  as we
shall see. We use a Dirichlet prior on worker confusion matrices  φmk|α ∼ Dir(α)  and a spherical
Gaussian prior on η  η ∼ N (0  vI). By absorbing the slack variables  CrowdSVM solves the
equivalent unconstrained problem:

where Rm(q; y) =(cid:80)M

inf
q∈P y

(cid:0)(cid:96)∆

L(q(Φ  η); y) + c · Rm(q(Φ  η); y) 
i (d)−Eq[η(cid:62)g∆

i (d)](cid:1)

i=1maxD

d=1

+ is the posterior regularization.

Remark 1. From the above deﬁnition  we can see that both the Bayesian DS estimator and the max-
margin majority voting are special cases of CrowdSVM. Speciﬁcally  when c → 0  it is equivalent
to the DS model. If we set v = v(cid:48)/c for some positive parameter v(cid:48)  then when c → ∞ CrowdSVM
reduces to the max-margin majority voting.

(10)

4.2 Variational Inference

2. For each worker j and category k:

Algorithm 1: The CrowdSVM algorithm
1. Initialize y by majority voting.
while Not converge do

q(φjk) ← Dir(njk + α).
3. Solve the dual problem (11).
4. For each item i: ˆyi ← argmaxyi∈[D] f (yi  xi; q).

Since it is intractable to directly solve
problem (9) or
(10)  we introduce
the structured mean-ﬁeld assumption
on the post-data posterior  q(Φ  η) =
q(Φ)q(η)  and solve the problem by
alternating minimization as outlined in
Alg. 1. The algorithm iteratively per-
forms the following steps until a local
optimum is reached:
Infer q(Φ): Fixing the distribution q(η) and the true labels y  the problem in Eq. (9) turns to a
standard Bayesian inference problem with the closed-form solution: q∗(Φ) ∝ p0(Φ)p(X|Φ  y).
Since the prior is a Dirichlet distribution  the inferred distribution is also Dirichlet  q∗(φjk) =
Dir(njk + α)  where njk is a D-dimensional vector with element d being njkd.
i (d)(cid:1)  where ω = {ωd
Infer q(η) and solve for ω:
Fixing the distribution q(Φ) and the true labels y  we opti-
mize Eq. (9) over q(η)  which is also convex. We can derive the optimal solution: q∗(η) ∝
i } are Lagrange multipliers. With the normal
prior  p0(η) = N (0  vI)  the posterior is a normal distribution: q∗(η) = N (µ  vI)   whose mean
(cid:88)
(cid:88)
i (d). Then the parameters ω are obtained by solving the dual problem

p0(η) exp(cid:0)η(cid:62)(cid:80)
(cid:80)
(cid:80)D
is µ = v(cid:80)M

d=1 ωd

i g∆

d ωd

i g∆

end

i=1

i

− 1
2v

µ(cid:62)µ +

sup
0≤ωd

i ≤c

i

d

ωd
i (cid:96)∆

i (d) 

which is same as the problem (6) in max-margin majority voting.
Infer y: Fixing the distributions of Φ and η at their optimum q∗  we ﬁnd y by solving problem
(10). To make the prediction more efﬁcient  we approximate the distribution q∗(Φ) by a Dirac delta
mass δ(Φ− ˆΦ)  where ˆΦ is the mean of q∗(Φ). Then since all tasks are independent  we can derive
the discriminant function of yi as

f (yi  xi; q∗) = log p(xi| ˆΦ  yi) − c max
d∈[D]

i (d) − ˆµ(cid:62)g∆

i (d))+

(11)

(12)

(cid:0)((cid:96)∆

(cid:1) 

where ˆµ is the mean of q∗(η). Then we can make predictions by maximize this function.
Apparently  the discriminant function (12) represents a strong coupling between the generative
model and the discriminative margin constraints. Therefore  CrowdSVM jointly considers these
two factors when estimating true labels. We also note that the estimation rule used here reduces to
the rule (7) of MM-IWMV by simply setting c = ∞.

5 Gibbs CrowdSVM Estimator

CrowdSVM adopts an averaging model to deﬁne the posterior constraints in problem (9). Here  we
further provide an alternative strategy which leads to a full Bayesian model with a Gibbs sampler.
The resulting Gibbs-CrowdSVM does not need to make the mean-ﬁeld assumption.

5

5.1 Model Deﬁnition
Suppose the target posterior q(Φ  η) is given  we perform the max-margin majority voting by draw-
ing a random sample η. This leads to the crowdsourcing hinge-loss

M(cid:88)

i=1

(cid:0)(cid:96)∆

i (d)(cid:1)

R(η  y) =

i (d) − η(cid:62)g∆

max
d∈[D]

+  

(13)

which is a function of η. Since η are random  we deﬁne the overall hinge-loss as the expectation over
q(η)  that is  R(cid:48)
m(q(Φ  η); y) = Eq [R(η  y)]. Due to the convexity of max function  the expected
loss is in fact an upper bound of the average loss  i.e.  R(cid:48)
m(q(Φ  η); y) ≥ Rm(q(Φ  η); y). Dif-
fering from CrowdSVM  we also treat the hidden true labels y as random variables with a uniform
prior. Then we deﬁne Gibbs-CrowdSVM as solving the problem:

q∈P L(cid:16)

inf

(cid:17)

(cid:34) M(cid:88)

i=1

(cid:35)

q(Φ  η  y)

+ Eq

2c(ζisi)+

 

(14)

M(cid:89)

i (d) − η(cid:62)g∆

i (d)  si = argmaxd(cid:54)=yi ζid  and the factor 2 is introduced for simplicity.
where ζid = (cid:96)∆
Data Augmentation In order to build an efﬁcient Gibbs sampler for this problem  we derive the
posterior distribution with the data augmentation [3  26] for the max-margin regularization term.
We let ψ(yi|xi  η) = exp(−2c(ζisi )+) to represent the regularizer. According to the equality:
(λi + cζisi)2) is
a (unnormalized) joint distribution of yi and the augmented variable λi [14]  the posterior of Gibbs-
CrowdSVM can be expressed as the marginal of a higher dimensional distribution  i.e.  q(Φ  η  y) =

0 ψ(yi  λi|xi  η)dλi  where ψ(yi  λi|xi  η) = (2πλi)− 1

ψ(yi|xi  η) =(cid:82) ∞
(cid:82) q(Φ  η  y  λ)dλ  where

2 exp( −1

2λi

q(Φ  η  y  λ) ∝ p0(Φ  η  y)

p(xi|Φ  yi)ψ(yi  λi|xi  η).

(15)

Putting the last two terms together  we can view q(Φ  η  y  λ) as a standard Bayesian posterior  but

with the unnormalized likelihood (cid:101)p(xi  λi|Φ  η  yi) ∝ p(xi|Φ  yi)ψ(yi  λi|xi  η)  which jointly

considers the noisy observations and the large margin discrimination between the potential true
labels and alternatives.

i=1

5.2 Posterior Inference
With the augmented representation  we can do Gibbs sampling to infer the posterior distribution
q(Φ  η  y  λ) and thus q(Φ  η  y) by discarding λ. The conditional distributions for {Φ  η  λ  y}
are derived in Appendix A. Note that when sample λ from the inverse Gaussian distribution  a fast
sampling algorithm [13] can be applied with O(1) time complexity. And for the hidden variables y 
we initially set them as the results of majority voting. After removing burn-in samples  we use their
most frequent values of as the ﬁnal outputs.

6 Experiments
We now present experimental results to demonstrate the strong discriminative ability of max-margin
majority voting and the promise of our Bayesian models  by comparing with various strong com-
petitors on multiple real datasets.

6.1 Datasets and Setups
We use four real world crowd labeling datasets as summarized in Table 1. Web Search [24]: 177
workers are asked to rate a set of 2 665 query-URL pairs on a relevance rating scale from 1 to 5.
In total 15 567 labels are collected. Age [8]: It
Each task is labeled by 6 workers on average.
consists of 10 020 labels of age estimations for 1 002 face images. Each image was labeled by 10
workers. And there are 165 workers involved in these tasks. The ﬁnal estimations are discretized
into 7 bins. Bluebirds [19]: It consists of 108 bluebird pictures. There are 2 breeds among all the
images  and each image is labeled by all 39 workers. 4 214 labels in total. Flowers [18]: It contains
2 366 binary labels for a dataset with 200 ﬂower pictures. Each worker is asked to answer whether
the ﬂower in picture is peach ﬂower. 36 workers participate in these tasks.

6

Table 1: Datasets Overview.

DATASET

WEB SEARCH

We compare M3V  as well as its Bayesian ex-
tensions CrowdSVM and Gibbs-CrowdSVM 
with various baselines  including majority vot-
ing (MV)  iterative weighted majority voting
(IWMV) [11]  the Dawid-Skene (DS) estima-
tor [5]  and the minimax entropy (Entropy) es-
timator [25]. For Entropy estimator  we use the implementation provided by the authors  and show
both the performances of its multiclass version (Entropy (M)) and the ordinal version (Entropy (O)).
All the estimators that require an iterative updating are initialized by majority voting to avoid bad lo-
cal minima. All experiments were conducted on a PC with Intel Core i5 3.00GHz CPU and 12.00GB
RAM.

ITEMS WORKERS
2 665
1 002
108
200

LABELS
15 567
10 020
4 214
2 366

AGE

BLUEBIRDS
FLOWERS

177
165
39
36

6.2 Model Selection

Due to the special property of crowdsourcing  we cannot simply split the training data into multiple
folds to cross-validate the hyperparameters by using accuracy as the selection criterion  which may
bias to over-optimistic models. Instead  we adopt the likelihood p(X| ˆΦ  ˆy) as the criterion to select
parameters  which is indirectly related to our evaluation criterion (i.e.  accuracy). Speciﬁcally  we
test multiple values of c and (cid:96)  and select the value that produces a model with the maximal likelihood
on the given dataset. This method ensures us to select model without any prior knowledge on the
true labels. For the special case of M3V  we ﬁx the learned true labels y after training the model
with certain parameters  and learn confusion matrices that optimize the full likelihood in Eq. (2).
Note that the likelihood-based cross-validation strategy [25] is not suitable for CrowdSVM  because
this strategy uses marginal likelihood p(X|Φ) to select model and ignores the label information
of y  through which the effect of constraints is passed for CrowdSVM. If we use this strategy on
CrowdSVM  it will tend to optimize the generative component without considering the discriminant
constraints  thus resulting in c → 0  which is a trivial solution for model selection.

6.3 Experimental Results

We ﬁrst test our estimators on the task of estimating true labels. For CrowdSVM  we set α = 1
and v = 1 for all experiments  since we ﬁnd that the results are insensitive to them. For
M3V  CrowdSVM and Gibbs-CrowdSVM  the regularization parameters (c  (cid:96)) are selected from
c = 2ˆ[−8 : 0] and (cid:96) = [1  3  5] by the method in Sec. 6.2. As for Gibbs-CrowdSVM  we generate
50 samples in each run and discard the ﬁrst 10 samples as burn-in steps  which are sufﬁciently large
to reach convergence of the likelihood. The reported error rate is the average over 5 runs.
Table 2 presents the error rates of various estimators. We group the comparisons into three parts:

I. All the MV  IWMV and M3V are purely discriminative estimators. We can see that our M3V
produces consistently lower error rates on all the four datasets compared with the vanilla MV
and IWMV  which show the effectiveness of max-margin principle for crowdsourcing;

II. This part analyzes the effects of prior and max-margin regularization on improving the DS
model. We can see that DS+Prior is better than the vanilla DS model on the two larger datasets
by using a Dirichlet prior. Furthermore  CrowdSVM consistently improves the performance of
DS+Prior by considering the max-margin constraints  again demonstrating the effectiveness of
max-margin learning;

III. This part compares our Gibbs-CrowdSVM estimator to the state-of-the-art minimax entropy es-
timators. We can see that Gibbs-CrowdSVM performs better than CrowdSVM on Web-Search 
Age and Flowers datasets  while worse on the small Bluebuirds dataset. And it is comparable
to the minimax entropy estimators  sometimes better with faster running speed as shown in
Fig. 2 and explained below. Note that we only test Entropy (O) on two ordinal datasets  since
this method is speciﬁcally designed for ordinal labels  while not always effective.

Fig. 2 summarizes the training time and error rates after each iteration for all estimators on the
largest Web-Search dataset.
It shows that the discriminative methods (e.g.  IWMV and M3V)
run fast but converge to high error rates. Compared to the minimax entropy estimator  CrowdSVM is

7

Table 2: Error-rates (%) of different estimators on four datasets.

WEB SEARCH

BLUEBIRDS

FLOWERS

METHODS
MV

IWMV
M3V
DS

I

II

III

26.90
15.04
12.74
16.92
13.26
9.42
11.10
10.40

DS+PRIOR
CROWDSVM
ENTROPY (M)
ENTROPY (O)
G-CROWDSVM 7.99 ± 0.26

AGE
34.88
34.53
33.33
39.62
34.53
33.33
31.14
37.32

32.98 ± 0.36

24.07
27.78
20.37
10.19
10.19
10.19
8.33
−

22.00
19.00
13.50
13.00
13.50
13.50
13.00
−

10.37±0.41

12.10 ± 1.07

Figure 2: Error rates per iteration of various esti-
mators on the web search dataset.

computationally more efﬁcient and also con-
verges to a lower error rate. Gibbs-CrowdSVM
runs slower than CrowdSVM since it needs to
compute the inversion of matrices. The per-
formance of the DS estimator seems mediocre
— its estimation error rate is large and slowly
increases when it runs longer. Perhaps this is
partly because the DS estimator cannot make
good use of the initial knowledge provided by
majority voting.
We further investigate the effectiveness of the
generative component and the discriminative
component of CrowdSVM again on the largest
Web-Search dataset. For the generative part  we
compared CrowdSVM (c = 0.125  (cid:96) = 3) with
DS and M3V (c = 0.125  (cid:96) = 3). Fig. 3(a)
compares the negative log likelihoods (NLL) of
these models  computed with Eq. (2). For M3V 
we ﬁx its estimated true labels and ﬁnd the con-
fusion matrices to optimize the likelihood. The
results show that CrowdSVM achieves a lower
NLL than DS; this suggests that by incorporat-
ing M3V constraints  CrowdSVM ﬁnds a better
solution of the true labels as well as the confu-
sion matrices than that found by the original EM algorithm. For the discriminative part  we use the
mean of worker weights ˆµ to estimate the true labels as yi = argmaxd∈[D] ˆµ(cid:62)g(xi  d)  and show
the error rates in Fig. 3(b). Apparently  the weights learned by CrowdSVM are also better than those
learned by the other MV estimators. Overall  these results suggest that CrowdSVM can achieve a
good balance between the generative modeling and the discriminative prediction.

Figure 3: NLLs and ERs when separately test the
generative and discriminative components.

(b)

(a)

7 Conclusions and Future Work
We present a simple and intuitive max-margin majority voting estimator for learning-from-crowds
as well as its Bayesian extension that conjoins the generative modeling and discriminative predic-
tion. By formulating as a regularized Bayesian inference problem  our methods naturally cover the
classical Dawid-Skene estimator. Empirical results demonstrate the effectiveness of our methods.
Our model is ﬂexible to ﬁt speciﬁc complicated application scenarios [22]. One seminal feature of
Bayesian methods is their sequential updating. We can extend our Bayesian estimators to the online
setting where the crowdsourcing labels are collected in a stream and more tasks are distributed. We
have some preliminary results as shown in Appendix B. It would also be interesting to investigate
more on active learning  such as selecting reliable workers to reduce costs [9].
Acknowledgments
The work was supported by the National Basic Research Program (973 Program) of China (Nos.
2013CB329403  2012CB316301)  National NSF of China (Nos. 61322308  61332007)  Tsinghua
National Laboratory for Information Science and Technology Big Data Initiative  and Tsinghua
Initiative Scientiﬁc Research Program (Nos. 20121088071  20141080934).

8

1001011020.100.140.18Time (Seconds)Error rate IWMVM3VDawid−SkeneEntropy (M)Entropy (O)CrowdSVMGibbs−CrowdSVMDSCSVMG−CSVMM^3V22.522.622.722.8NLL ( x103 )22.8422.5522.6222.65MVIWMVCSVMG−CSVMM^3V0.10.20.3Error rate0.26930.15040.10690.10210.1274References
[1] A. Carlson  J. Betteridge  B. Kisiel  B. Settles  E. R. Hruschka Jr  and T. M. Mitchell. Toward

an architecture for never-ending language learning. In AAAI  2010.

[2] C. C. Chang and C. J. Lin. LIBSVM: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology  2:27:1–27:27  2011.

[3] C. Chen  J. Zhu  and X. Zhang. Robust Bayesian max-margin clustering. In NIPS  2014.
[4] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based

vector machines. JMLR  2:265–292  2002.

[5] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using

the EM algorithm. Applied Statistics  pages 20–28  1979.

[6] M. Dud´ık  S. J. Phillips  and R. E. Schapire. Maximum entropy density estimation with gener-

alized regularization and an application to species distribution modeling. JMLR  8(6)  2007.

[7] K. Ganchev  J. Grac¸a  J. Gillenwater  and B. Taskar. Posterior regularization for structured

latent variable models. JMLR  11:2001–2049  2010.

[8] Otto C. Liu X. Han  H. and A. Jain. Demographic estimation from face images: Human vs.

machine performance. IEEE Trans. on PAMI  2014.

[9] S. Jagabathula  L. Subramanian  and A. Venkataraman. Reputation-based worker ﬁltering in

crowdsourcing. In NIPS  2014.

[10] D. R. Karger  S. Oh  and D. Shah. Iterative learning for reliable crowdsourcing systems. In

NIPS  2011.

[11] H. Li and B. Yu. Error rate bounds and iterative weighted majority voting for crowdsourcing.

arXiv preprint arXiv:1411.4086  2014.

[12] Q. Liu  J. Peng  and A. Ihler. Variational inference for crowdsourcing. In NIPS  2012.
[13] J. R. Michael  W. R. Schucany  and R. W. Haas. Generating random variates using transforma-

tions with multiple roots. The American Statistician  30(2):88–90  1976.

[14] N. G. Polson and S. L. Scott. Data augmentation for support vector machines. Bayesian

Analysis  6(1):1–23  2011.

[15] V. C. Raykar  S. Yu  L. H. Zhao  G. H. Valadez  C. Florin  L. Bogoni  and L. Moy. Learning

from crowds. JMLR  11:1297–1322  2010.

[16] T. Shi and J. Zhu. Online Bayesian passive-aggressive learning. In ICML  2014.
[17] R. Snow  B. O’Connor  D. Jurafsky  and A. Y. Ng. Cheap and fast—but is it good?: evaluating

non-expert annotations for natural language tasks. In EMNLP  2008.

[18] T. Tian and J. Zhu. Uncovering the latent structures of crowd labeling. In PAKDD  2015.
[19] P. Welinder  S. Branson  P. Perona  and S. J. Belongie. The multidimensional wisdom of

crowds. In NIPS  2010.

[20] J. Whitehill  T. F. Wu  J. Bergsma  J. R. Movellan  and P. L. Ruvolo. Whose vote should count

more: Optimal integration of labels from labelers of unknown expertise. In NIPS  2009.

[21] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector ma-

chines. In AAAI  2005.

[22] O. F. Zaidan and C. Callison-Burch. Crowdsourcing translation: Professional quality from

non-professionals. In ACL  2011.

[23] Y. Zhang  X. Chen  D. Zhou  and M. I. Jordan. Spectral methods meet EM: A provably optimal

algorithm for crowdsourcing. In NIPS  2014.

[24] D. Zhou  S. Basu  Y. Mao  and J. C. Platt. Learning from the wisdom of crowds by minimax

entropy. In NIPS  2012.

[25] D. Zhou  Q. Liu  J. Platt  and C. Meek. Aggregating ordinal labels from crowds by minimax

conditional entropy. In ICML  2014.

[26] J. Zhu  N. Chen  H. Perkins  and B. Zhang. Gibbs max-margin topic models with data aug-

mentation. JMLR  15:1073–1110  2014.

[27] J. Zhu  N. Chen  and E. P. Xing. Bayesian inference with posterior regularization and applica-

tions to inﬁnite latent svms. JMLR  15:1799–1847  2014.

9

,TIAN TIAN
Jun Zhu