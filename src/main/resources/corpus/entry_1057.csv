2014,Compressive Sensing of Signals from a GMM with Sparse Precision Matrices,This paper is concerned with compressive sensing of signals drawn from a Gaussian mixture model (GMM) with sparse precision matrices. Previous work has shown: (i) a signal drawn from a given GMM can be perfectly reconstructed from r noise-free measurements if the (dominant) rank of each covariance matrix is less than r; (ii) a sparse Gaussian graphical model can be efficiently estimated from fully-observed training signals using graphical lasso. This paper addresses a problem more challenging than both (i) and (ii)  by assuming that the GMM is unknown and each signal is only partially observed through incomplete linear measurements. Under these challenging assumptions  we develop a hierarchical Bayesian method to simultaneously estimate the GMM and recover the signals using solely the incomplete measurements and a Bayesian shrinkage prior that promotes sparsity of the Gaussian precision matrices. In addition  we provide theoretical performance bounds to relate the reconstruction error to the number of signals for which measurements are available  the sparsity level of precision matrices  and the “incompleteness” of measurements. The proposed method is demonstrated extensively on compressive sensing of imagery and video  and the results with simulated and hardware-acquired real measurements show significant performance improvement over state-of-the-art methods.,Compressive Sensing of Signals from a GMM with

Sparse Precision Matrices

1Jianbo Yang

1Department of Electrical and Computer Engineering  Duke University

2Department of Statistics & Department of Computer Science  University of Chicago

{jianbo.yang;xjliao;lcarin@duke@duke.edu} {dukemeeting@gmail.com}

1Xuejun Liao

2Minhua Chen

1Lawrence Carin

Abstract

This paper is concerned with compressive sensing of signals drawn from a Gaus-
sian mixture model (GMM) with sparse precision matrices. Previous work has
shown: (i) a signal drawn from a given GMM can be perfectly reconstructed from
r noise-free measurements if the (dominant) rank of each covariance matrix is
less than r; (ii) a sparse Gaussian graphical model can be efﬁciently estimated
from fully-observed training signals using graphical lasso. This paper addresses a
problem more challenging than both (i) and (ii)  by assuming that the GMM is un-
known and each signal is only observed through incomplete linear measurements.
Under these challenging assumptions  we develop a hierarchical Bayesian method
to simultaneously estimate the GMM and recover the signals using solely the in-
complete measurements and a Bayesian shrinkage prior that promotes sparsity of
the Gaussian precision matrices. In addition  we provide theoretical performance
bounds to relate the reconstruction error to the number of signals for which mea-
surements are available  the sparsity level of precision matrices  and the “incom-
pleteness” of measurements. The proposed method is demonstrated extensively
on compressive sensing of imagery and video  and the results with simulated and
hardware-acquired real measurements show signiﬁcant performance improvement
over state-of-the-art methods.

1

Introduction

Gaussian mixture models (GMMs) [1  2  3] have become a popular signal model for compressive
sensing [4  5] of imagery and video  partly because the information domain in these problems can
be decomposed into subdomains known as pixel/voxel patches [3  6]. A GMM employs a Gaussian
precision matrix to capture the statistical relations between local pixels/voxels within a patch  and
meanwhile captures the global statistics between patches using its clustering mechanism.
Compressive sensing (CS) of signals drawn from a GMM admits closed-form minimum mean
squared error (MMSE) reconstruction from linear measurements. Recent theoretical analysis in
[7] shows that  given a sensing matrix with entries i.i.d. drawn from a zero-mean  ﬁxed-variance 
Gaussian distribution or Bernoulli distribution with parameter 0.5  if the GMM is known and the
(dominant) rank of each covariance matrix is less than r  each signal can be perfectly reconstructed
from r noise-free measurements. Though this is a much less stringent reconstruction condition than
that prescribed by standard restricted-isometry-property (RIP) bounds  it relies on the assumption
of knowing the exact GMM. If a sufﬁcient number of fully observed signals are available before-
hand  one can use maximum likelihood (ML) estimators to train a GMM [8  9  7  1  10] for use in
reconstructing the signals in question. Unfortunately  ﬁnding an accurate GMM a priori is usually a
challenge in practice  because it is difﬁcult to obtain training signals that match the statistics of the
interrogated signals.

1

Recent work [2] on GMM-based methods proposes to solve this problem by estimating the Gaus-
sian components  based on measurements of the signals under interrogation  without resorting to
any fully-observed signals to train a model in advance. The method of [2] has two drawbacks: (i)
it estimates full dense Gaussian covariance matrices  with the number of free parameters to be esti-
mated growing quadratically fast with the signal dimensionality n; (ii) it does not have performance
guarantees  because all previous theoretical results  including those in [7]  assume the GMM is given
and thus are no longer applicable to the method of [2]. This paper addresses these two issues.
First  we effectively reduce the number of GMM parameters by restricting the GMM to have sparse
precision matrices with group sparsity patterns  making the GMM a mixture of group-sparse Gaus-
sian graphical models. The group sparsity is motivated by the Markov random ﬁeld (MRF) property
of natural images and video [11  12  13]. Instead of having n2 parameters for each Gaussian com-
ponent as in [2]  we have only n + s parameters  where s is the number of nonzero off-diagonals
of the precision matrix. We develop a variational maximum-marginal-likelihood estimator (varia-
tional MMLE) to simultaneously estimate the GMM and reconstruct the signals  with a Bayesian
shrinkage prior used to promote sparsity of the Gaussian precision matrices. Our variational MM-
LE maximizes the marginal likelihood of the GMM given only the linear measurements  with the
unknown signals treated as random variables and integrated out of the likelihood. A key step of
the variational MMLE is using Bayesian graphical lasso to reestimate the sparse Gaussian precision
matrices based on a posteriori signal samples conditional on the linear measurements.
Second  we provide theoretical performance bounds under the assumption that the GMM is not
exactly known. Assuming the GMM has sparse precision matrices  our theoretical results relate
the signal reconstruction error to the number of signals for which measurements are available  the
sparsity level of the precision matrices  and the “incompleteness” of measurements  where the last
is deﬁned as the uncertainty (variance) of a signal given its linear measurements.
In the experiments  we present reconstruction results of the proposed method on both simulated
measurements and real measurements acquired by actual hardware [6]. The proposed method out-
performs the state-of-art CS reconstruction algorithms by signiﬁcant margins.
Notations. Let N (x|µ  Ω−1) denote a Gaussian density of x with mean µ and precision matrix Ω 
(cid:107)M(cid:107)F denote the Frobenius matrix norm of matrix M  (cid:107)M(cid:107)max denote the largest entry of M in
terms of magnitude  tr(M ) denote the trace of M  Ω0 = Σ−1
0 denote the true precision matrix (i.e. 
the inverse of true covariance matrix Σ0)  Ω∗ denote the estimate of Ω0 by the proposed model.
Herein  the eigenvalues of Σ0 are assumed to be bounded in a constant interval [τ1  τ2] ⊂ (0 ∞)  to
guarantee the existence of Ω0. For functions f (x) and g(x)  we write f (x) (cid:16) g(x) when f (x) =
O(g(x)) and g(x) = O(f (x)) hold simultaneously.
2 Learning a GMM of Unknown Signals from Linear Measurements
2.1 Signal Reconstruction with a Given GMM
The linear measurement of an unknown signal x ∈ Rn can be written as y = Φx +   where
Φ ∈ Rm×n is a sensing matrix  and  ∈ Rm denote measurement noises (we are interested in
m < n). Assuming  ∈ N (|0  R)  one has p(y|x) = N (y|Φx  R). We further assume R to be a
scaled identity matrix  R = κ−1I  and thus the noise is white Gaussian.
z=1 π(z)N (x|µ(z)  Ω(z)−1
p(y  x  z) = π(z)N (y|Φx  R)N (x|µ(z)  Ω(z)−1

If x is governed by a GMM  i.e.  p(x) =(cid:80)K

)  one may obtain

) 

p(y) =

π(z)N (y|Φµ(z)  R + ΦΩ(z)−1

(cid:48)

Φ

) 

p(x  z|y) = ρ(z)N (x|η(z)  (C(z))

−1) 

(1)

K(cid:88)

(cid:48)
η(z) = µz + C(z)Φ

−1(y − Φµz) 
R

z=1

where

C(z) =

ρ(z) =

−1Φ + Ω(z)(cid:17)−1

(cid:16)
(cid:48)
Φ
R
(cid:80)K
π(z)N (y|Φµ(z)  R + ΦΩ(z)−1
Φ(cid:48))
l=1 π(l)N (y|Φµ(l)  R + ΦΩ(l)−1

 

Φ(cid:48))

When the GMM is exactly known  the signal is reconstructed analytically as the conditional mean 
(3)

z=1ρ(z)η(z).

(cid:98)x (cid:44) E(x|y) =(cid:80)K

.

(2)

2

It has been shown in [7] that  if the (dominant) rank of each Gaussian covariance matrix is less than
r  the signal can be perfectly reconstructed from only r measurements in the low-noise regime.

2.2 Restriction of the GMM to a mixture of Gaussian Markov Random Fields
A Markov random ﬁeld (MRF)  also known as an undirected graphical model  provides a graphical
representation of the joint probability distribution over multiple random variables  by considering
the conditional dependences among the variables [11  12  13].
In image analysis  each node of
an MRF corresponds to a pixel of the image in question  and an edge between two nodes is often
modeled by a potential function to characterize the conditional dependence between the associated
pixels. Because of the local smoothness structure of images  the edges of an MRF are usually
chosen based on a pairwise neighborhood structure: each pixel only has edge connections with
its neighbors. The widely used scheme is that each pixel only has edge connections with its four
immediate neighboring pixels to the left  right  top and bottom [11]. Therefore  an MRF for image
representation is an undirected graph with only a limited number of edges between its nodes.
Generally  learning and inference of an MRF are nontrivial  due to the nonlinearity and noncon-
vexity of the potential functions [14]. A popular special case of MRF is the Gaussian Markov
random ﬁeld (GMRF) which is an MRF with a multivariate Gaussian distribution over node vari-
ables. The best-known advantage of a GMRF is its simplicity of learning and inference  because
of the nice properties of a multivariate Gaussian distribution. According to Hammersley-Clifford’s
theorem [15]  the conditional dependence of the node variables in a GMRF is encoded in the pre-
cision matrix. As mentioned before  an MRF is sparse for image analysis problems  on account of
the neighborhood structure in the pixel domain. Therefore  the multivariate Gaussian distribution
associated with a GMRF has a sparse precision matrix. This property of a GMRF in image analysis
is demonstrated in Section 1 of the Supplementary Material.
Inspired by the GMRF interpretation  we place a shrinkage prior on each precision matrix to promote
sparsity when estimating the GMM. The Laplacian shrinkage prior used in [16] is chosen  but other
shrinkage priors [17] could also be used. Speciﬁcally  we impose a Laplacian shrinkage prior on the
off-diagonal elements of each of K precision matrices 

(cid:113)

n(cid:89)

(cid:89)

i=1

j<i

τ (k)γ(k)
ij

2

exp(−(cid:113)

p(Ω(k)) =

τ (k)γ(k)

ij |ω(k)

ij |)  ∀k = 1  . . .   K 

(4)

ij = ω(k)

ij |i = 1  ...  n  j < i} and generally ﬁxed to be one [18]  and γ(k)

with the symmetry constraints ω(k)
ji . In (4)  τ (k) > 0 is a “global” scaling parameter for all
the elements of {ω(k)
is a “local”
weight for the element ω(k)
ij . With the Laplacian prior (4)  many off-diagonal elements of Ω(k) are
encouraged to be close to zero. However  in the inference procedure  the above Laplacian shrinkage
prior (4) is inconvenient due to the lack of analytic updating expressions. This issue is overcome by
using an equivalent scale mixture of normals representation [16] of (4) as shown below:

ij

(cid:113)

τ (k)γ(k)
ij

exp(−(cid:113)

2
where α(k)
ij
place a gamma prior on γ(k)

(cid:90)

τ (k)γ(k)

ij |ω(k)

ij |) =

N (ω(k)

ij |0  τ (k)−1

α(k)
ij

−1

)InvGa(α(k)

ij |1 

γ(k)
ij
2

)dα(k)
ij

(5)

is an augmented variable drawn from an inverse gamma distribution. Further  one may

ij . Then  a draw of the precision matrix may be represented by

−1

α(k)
ij

i=1

j<i

)  α(k)

(cid:89)

N (ω(k)

ij |0  τ (k)−1

ij ∼ InvGa(α(k)

Ω(k) ∼ n(cid:89)
where a0  b0 are the hyperparameters.
(cid:80)N
i=1 are samples drawn from N (x|0  Ω(k)−1
Suppose {xi}N
i=1(xi − x)(xi − x)(cid:48) where x is the empirical mean of {xi}N
matrix 1
(cid:113)
N
are drawn as in (6)  the logarithm of the joint likelihood can be expressed as

log det(Ω(k)) − tr(SΩ(k)) − n(cid:88)

log p({xi}N

(cid:88)

γ(k)
ij
2

ij |1 

(cid:32)

i=1  Ω(k)) ∝ N
2

2
N

i=1

j<i

) and S denotes the empirical covariance
i=1. If the elements Ω(k)

(cid:33)

τ (k)γ(k)

ij |ω(k)
ij |

.

(7)

)  γ(k)

ij ∼ Ga(γ(k)

ij |a0  b0)

(6)

From the optimization perspective  the maximum a posterior (MAP) estimations of Ω(k) in (7) is
known as the adaptive graphical lasso problem [18].

3

2.3 Group sparsity based on banding patterns
The Bayesian adaptive graphical lasso described above assumes the precision matrix is sparse  and
the same Laplacian prior is imposed on all off-diagonal elements of the precision matrix without any
discrimination. However  the aforementioned neighborhood structure of image pixels implies that
the entries of the precision matrix corresponding to the pairs between neighboring pixels tend to have
signiﬁcant values. This is consistent with the observations as seen from the demonstration in Section
1 of the Supplementary Material: (i) the bands scattered along a few lines above or below the main
diagonal are constituted by the entries with signiﬁcant values in the precision matrix; (ii) the entries
in the bands correspond to the pairwise neighborhood structure of the graph  since vectorization of
an image patch is constituted by stacking all columns of pixels in a patch on the top of each other;
(iii) the existence of multiple bands in some Gaussian components reveals that  besides the four
immediate neighboring pixels  other indirected neighboring pixels may also lead to nonnegligible
conditional dependence  though the entries in the associated bands have relatively smaller values.
Inspired by the banding patterns mentioned above  we categorize the elements in the set
ij |(i  j) ∈ L2}  where L1 denotes the
{ω(k)
ij |(i  j) ∈ L1} and {ω(k)
set of indices corresponding to the elements in the bands and L2 represents the set of indices for the
elements not in the bands. For the elements in the group {ω(k)
ij |(i  j) ∈ L2}  the Laplacian prior is
used to encourage a sparse precision matrix. For the elements in the group {ω(k)
ij |(i  j) ∈ L1}   the
sparsity is not desired so a normal prior with Gamma hyperparameters is used instead. Accordingly 
the expressions in (6) can be replaced by

i=1 j<i into two groups {ω(k)

ij }n

N (ω(k)

ij |0  τ (k)−1

−1

)

α(k)
ij

Ω(k) ∼ n(cid:89)

(cid:89)
(cid:40) Ga(α(k)

i=1

i<j

ij ∼
α(k)

ij |c0  d0) 
ij |1 

γ

InvGa(α(k)

(k)
ij

2 )  γ(k)

ij ∼ Ga(γ(k)

ij |a0  b0) 

if (i  j) ∈ L1
if (i  j) ∈ L2

.

(8)

With the prior distribution of Ω(k) in (6) replaced with that in (8)  the joint log-likelihood in (7)
changes to

log p({xi}N

i=1  Ω(k))

log det(Ω(k)) − tr(SΩ(k)) − (cid:88)

∝ N
2

(i j)∈L1

ij (cid:107)2 − (cid:88)

(i j)∈L2

τ (k)α(k)

ij (cid:107)ω(k)

2
N

(cid:113)

2
N

 .

τ (k)γ(k)

ij |
ij |ω(k)

(9)

To the best of our knowledge  the maximum a posterior (MAP) estimations of Ω(k) in (9) has not
been studied in the family of graphical lasso or its variants  from the optimization perspective.
2.4 Hierarchical Bayesian model and inference
We consider the collective compressive sensing of the signals X = {xi ∈ Rn}N
i=1 that are drawn
from an unknown GMM. The noisy linear measurements of X are given by Y = {yi ∈ Rm : yi =
Φixi + i}N
i=1. We assume the sensing matrices to be signal-dependent to account for generality
(i.e.  Φi depends on the signal index i).
The uniﬁcation of signal reconstruction with a given GMM (presented in Section 2.1) and GM-
RF learning with fully-observed training signals (presented in Section 2.2) leads to the following
Bayesian model 

−1I)  xi ∼ K(cid:88)

yi|xi ∼ N (yi|Φixi  κ

Ω(k) ∼ n(cid:89)

(cid:89)

i=1

i<j

π(z)N (xi|µ(z)  Ω(z)−1

)  κ ∼ Ga(κ|e0  f0)

N (ω(k)

ij |0  τ (k)−1

z=1
−1

α(k)
ij

)  α(k)

ij ∼ InvGa(α(k)

ij |1 

γ(k)
ij
2

)  γ(k)

ij ∼ Ga(γ(k)

ij |a0  b0) 

(10)

(11)

The expression in (11) could be replaced by (8) if the group sparsity is considered in the precision
matrix. In addition to the precision matrices  we further add the following standard priors on the
other parameters of the GMM to make the proposed model a full hierarchical Bayesian model 

µ(k) ∼ N (µ(k)|m0  (β0Ω(k))

−1)  π ∼ Dirichlet(π(1)  . . .   π(K)|a0) 

(12)

4

where m0  a0 and β0 are hyperparameters.
We develop the inference procedure for the proposed Bayesian hierarchical model. Let the symbols
Z  µ  Ω  π  α  γ denote the sets {zi}  {µ(k)}  {Ω(k)}  {π(k)}  {α(k)}  {γ(k)} respectively. The
marginalized likelihood function is written as
L(Θ) = ln

p(Y  Π  Θ)dΠ

(cid:90)

where Π (cid:44) {X  Z  α  γ} and Θ (cid:44) {µ  Ω  π  κ} denote the set of the latent variables and parame-
ters of the model  respectively. An expectation-maximization (EM) algorithm [19] could be used to
ﬁnd the optimal Θ by alternating the following two steps

• E-step: Find p(Π|Y  Θ∗) with Θ∗ computed at the M-step  and obtain the expected com-
• M-step: Find an improved estimate of Θ∗ by maximizing the expected complete log-

plete log-likelihood EΠ(ln p(Y  Π  Θ∗)).

likelihood given at the E-step.

However  it is intractable to compute the exact posterior p (Π|Y  Θ) at the E step. We develop a
variational inference approach to overcome the intractability. Based on the mean ﬁeld theory [20] 
we approximate the posterior distribution p (Π|Y  Θ) by a proposal distribution q(Π) that factorizes
over the variables as follows

KL(q(Π)||p(Π|Y  Θ)) =(cid:82) q(Π) ln

(13)
Then  we ﬁnd an optimal distribution q(Π) that minimizes the Kullback-Leibler (KL) divergence
p(Π|Y Θ) dΠ  or equivalently  maximizes the evidence lower

q(Π) = q(X  Z  α  γ) = q(X  Z)q(α)q(γ).

bound (ELBO) of the log-marginal data likelihood [21]  denoted by F(q(Π)  Θ) 

q(Π)

ln p(Y  Θ) = ln

q(Π)

p (Y  Π  Θ)

q(Π)

dΠ ≥

q(Π) ln

p (Y  Π  Θ)

q(Π)

dΠ (cid:44) F(q(Π)  Θ)

(14)

(cid:90)

(cid:90)

where the inequality is held based on the Jensen’s inequality.
With the above approximation  the entire algorithm becomes a variational EM algorithm and it
iterates between the following VE-step and VM-step until convergence:

• VE-step: Find the optimal posterior distribution q∗ (Π) that maximizes F(q(Π)  Θ∗) with
• VM-step: Find the optimal Θ∗ that maximizes F(q∗(Π)  Θ) with q∗(Π) computed at the

Θ∗ computed at the VM-step.

VE-step.

The full update equations of the variational EM algorithm are given in Section 2 of the Supplemen-
tary Material.
3 Theoretical Analysis
The proposed hierarchical Bayesian model uniﬁes the task of signal recovery and the task of esti-
mating the mixture of GMRF  with a common goal of maximizing the ELBO of the log-marginal
likelihood of the measurements. This section provides a theoretical analysis to further reveal the
mutual inﬂuence between these two tasks (Theorem 1 and Theorem 2)  and establish a theoretical
performance bound (Theorem 3) to relate the reconstruction error to the number of signals being
measured  the sparsity level of precision matrices  and the “incompleteness” of measurements. The
proofs of these theorems are presented in Sections 3-5 of the Supplementary Material. For conve-
nience  we consider the single Gaussian case  so the superscript (k) is omitted in the sequel. We
begin with the deﬁnitions and assumptions used in the theorems.

matrix Ω0 and the estimated precision matrix Ω∗ respectively  according to (3) 
−1 (yi − Φiµ)
−1
(cid:48)
Φ
iR

Deﬁnition 3.1 Let(cid:98)xi and(cid:101)xi be the signals estimated from measurement yi  using the true precision
(cid:98)xi =µ +(cid:0)Ω0 + Φ
(cid:101)xi =µ +(cid:0)Ω0 + ∆ + Φ
Assuming yi ∈ Rr is noise-free and the (dominant) rank of Ω0 is less than r  one obtains(cid:98)xi as the
true signal xi [7]  i.e. (cid:98)xi = xi. Then the reconstruction error of(cid:101)xi is (cid:107)δi(cid:107)2  where δi =(cid:101)xi −(cid:98)xi.

−1 (yi − Φiµ) = µ +(cid:0)C

−1 (yi − Φiµ) = µ + CiΦ
Φ

i + ∆(cid:1)−1

−1 (yi − Φiµ) .

−1Φi
(cid:48)
iR

Φ
−1Φi

(cid:48)
iR

(cid:1)−1

(cid:1)−1

(cid:48)
iR

(cid:48)
iR

(cid:48)
iR

5

Deﬁnition 3.2 The estimation error of Ω∗ is deﬁned as (cid:107)∆(cid:107)F where ∆ = Ω∗ − Ω0.
At each VM-step of the variational EM algorithm developed in Section 2.4  Ω∗ is updated based on

the empirical covariance matrix Σem computed from {(cid:101)xi}  i.e. 
N(cid:88)
(cid:124)

(cid:98)xi(cid:98)x
(cid:125)

N(cid:88)
(cid:123)(cid:122)

(cid:101)xi(cid:101)x

N(cid:88)

N(cid:88)

Σem =

(cid:48)
i +

Ci =

1
N

1
N

1
N

1
N

(cid:124)

i=1

i=1

i=1

(cid:48)
i

+

i=1

where {(cid:98)xi} and {(cid:101)xi} are considered to both have zero mean  as one can always center the signals

Σde

Σ0

em

(cid:48)
i + δiδ

(cid:48)
i + Ci)

 

(15)

with respect to their means [2].

(2(cid:98)xiδ

(cid:123)(cid:122)

(cid:125)

√

√

em is deﬁned as Σde = Σem−Σ0

Deﬁnition 3.3 The deviation of empirical matrix Σ0
em according to
(15)  and we use ¯σde (cid:44) (cid:107)Σde(cid:107)max to measure this deviation. Considering the developed variational
EM algorithm can converge to a local minimum  we assume ¯σde ≤ c
N for a constant c > 01.
3.1 Theoretical results
Theorem 1 Assuming (cid:107)Ci(cid:107)F (cid:107)∆(cid:107)F < 1  the reconstruction error of the i-th signal is upper bound-
ed as (cid:107)δi(cid:107)2 ≤ (cid:107)Ci(cid:107)F (cid:107)∆(cid:107)F
1−(cid:107)Ci(cid:107)F (cid:107)∆(cid:107)F
Theorem 1 establishes the error bound of signal recovery in terms of ∆. In this theorem  Ω∗ can be
obtained by any GMRF estimation methods  including [1  2] and the proposed method.

(cid:107)(cid:98)xi(cid:107)2.

(cid:113) log n

τ γij

√

τ γij
N   η = max(i j)∈S

N   S = {(i  j) : ωij (cid:54)= 0  i (cid:54)= j}  Sc = {(i  j) :
Let η = min(i j)∈Sc
ωij = 0  i (cid:54)= j} and the cardinality of S be s. The following theorem establishes an upper bound of
(cid:107)∆(cid:107)F on account of Σde.

Theorem 2 Given the empirical covariance matrix Σem  if η  η (cid:16) (cid:113) log n
(cid:107)∆(cid:107)F = Op{(cid:112)(n + s) log n/N +

N + ¯σde  then we have

n + s¯σde}.

(cid:80)N
i=1 (cid:107)(cid:98)xi − µ(cid:107)2  υ = 1

Note that the standard graphical lasso and its variants [18  23] assume the true signal samples {xi}
are fully observed when estimating Ω∗  so they correspond to the simple case that ¯σde = 0. Loh
and Wainwright [22  Corollary 5] also provides an upper bound of (cid:107)∆(cid:107)F taking Σde into account.
However  they assume Σ0
em is attainable and the proof of their corollary relies on their proposed
GMRF estimation algorithm  so the theoretical result in [22] cannot be used here.

(cid:80)N
i=1 tr(Ci)  δmax = supi (cid:107)δi(cid:107)2 (cid:98)xmax = supi (cid:107)(cid:98)xi(cid:107)2 and
Let 0 = 1
ξ = maxi (cid:107)Ci(cid:107)F . A combination of Theorem 1 and 2 leads to the following theorem which re-
N
lates the error bound of signal reconstruction to the number of partially-observed signals (observed
through incomplete linear measurements)  the sparsity level of precision matrices  and the uncertain-
ty of signal reconstruction (i.e.  υ and ξ) which represent the “incompleteness” of the measurements.
n + s > M 0(δmax + 2(cid:98)xmax)ξ with M being an appropriate
N + ¯σde  ξ(cid:107)∆(cid:107)F < ζ

Theorem 3 Given the empirical covariance matrix Σem  if η  η (cid:16) (cid:113) log n
constant to make (cid:107)∆(cid:107)F ≤ M(cid:112)(n + s) log n/N + M
(cid:80)N
i=1 (cid:107)(cid:101)xi −(cid:98)xi(cid:107)2 is close to zero with high probability.

From Theorem 3  we ﬁnd that when the number of partially-observed signals N tends to inﬁnity and
the uncertainty of signal reconstruction tr(Ci) tends to zero ∀ i  the average reconstruction error

√
n+s−M 0(δmax+2(cid:98)xmax)ξ M 0ξ.

√
where ζ is a constant and (1 − ζ)/

(cid:80)N
i=1 (cid:107)(cid:101)xi −(cid:98)xi(cid:107)2 ≤

n + s¯σde hold with high probability  then

we obtain that 1
N

(log n)/N +υ

(1−ζ)/

√

1
N

√

N

4 Experiments
The performance of the proposed methods is evaluated on the problems of compressive
sensing (CS) of imagery and high-speed video2.
the proposed method
is termed as Sparse-GMM when using the non-group sparsity described in Section 2.2 

For convenience 

1A similar assumption is made in expression (3.13) of [22].
2The complete results can be found at the website: https://sites.google.com/site/nipssgmm/.

6

and is termed Sparse-GMM(G) when using the group sparsity described in Section 2.3.
the two groups L1 and L2 as follows : L1 =
For Sparse-GMM(G)  we construct
{(i  j)
: pixel i is one of four immediate neighbors  in the spatial domain  of pixel j  i (cid:54)= j} and
L2 = {(i  j) : i  j = 1  2 ···   n  i (cid:54)= j} \ L1. The proposed methods are compared with state-of-
the-art methods  including: a GMM pre-trained from training patches (GMM-TP) [7  8]  a piecewise
linear estimator (PLE) [2]  generalized alternating projection (GAP) [24]  Two-step Iterative Shrink-
age/Thresholding (TwIST) [25]  KSVD-OMP [26].
For the proposed methods  the hyperparameters of the scaled mixture of Gaussians are set as

(cid:112)a0/b0/N ≈ 300  c0 = d0 = 10−6  the hyperparameter of Dirichlet prior α0 is set as a vec-
set as β0 = 1  and m0 is set to the mean of the initialization of {(cid:98)xi}N

tor with all elements being one  the hyperparameters of the mean of each Gaussian component are
i=1. We ﬁxed κ = 10−6 for
the proposed methods  GMM-TP and PLE. The number of dictionary elements in KSVD is set to
the best in {64  128  256  512}. The TwIST adopts the total-variation (TV) norm  and the results of
TwIST reported here represented the best among the different settings of regularization parameter in
the range of [10−4  1]. In GAP  the spatial transform is chosen between DCT and waveletes and the
one with the best result is reported  and the temporal transform for video is ﬁxed to be DCT.
4.1 Simulated measurements
Compressive sensing of still images. Following the single pixel camera [27]  an image xi is pro-
jected onto the rows of a random sensing matrix Φi ∈ Rm×n to obtain the compressive mea-
surements yi for i = 1  . . .   N. Each sensing matrix Φi is constituted by the elements drawn
from a uniform distribution in [0  1]. The USPS handwritten digits dataset 3 and the
face dataset [28] are used in this experiment. In each dataset  we randomly select 300 images
and each image is resized to the scale of 12 × 12. Eight settings of CS ratios are adopted with
n ∈ {0.05  0.10  0.15  0.20  0.25  0.30  0.35  0.40}. Since signal xi in the single pixel camera rep-
m
resents an entire image which generally has unique statistics  it is infeasible to ﬁnd suitable training
data in practice. Therefore  GMM-TP and KSVD-OMP are not compared to in this experiment4. For
(cid:98)xi = arg minx{(cid:107)x(cid:107)2
PLE  Sparse-GMM and Sparse-GMM(G)  the minimum-norm estimates from the measurements 
i)−1yi  i = 1  . . .   N  are used to initialize the
GMM. The number of GMM components K in PLE  Sparse-GMM  and Sparse-GMM(G) is tuned
among 2 ∼ 10 based on Bayesian information criterion (BIC).

2 : Φix = yi} = Φ(cid:48)

i(ΦiΦ(cid:48)

Figure 1: A comparison of reconstruction performances  in terms of PSNR  among different methods
for CS of imagery on USPS handwritten digits (left) and face datasets (middle)  and CS
of video on NBA game dataset (right)  with the average PSNR over frames shown in the brackets.
Compressive sensing of high-speed video. Following the Coded Aperture Compressive Temporal
Imaging (CACTI) system [6]  each frame of video to be reconstructed is encoded with a shifted
binary mask which is designed by randomly drawing values from {0  1} at every pixel location 
with a 0.5 probability of drawing 1. Each signal xi represents the vectorization of T consecutive
spatial frames  obtained by ﬁrst vectorizing each frame into a column and then stacking the resulting
T columns on top of each other. The measurement yi is constituted by yi = Φixi where Φi =
[Φi 1  . . .   Φi T ] and Φi t is a diagonal matrix with its diagonal being the mask that is applied to
the t-th frame. A video containing NBA game scenes is used in the experiment. It has 32 frames 
each of size 256 × 256  and T is set to be 8. For GMM-TP  KSVD-OMP  PLE  Sparse-GMM and
Sparse-GMM(G)  we partition each 256 × 256 measurement frame into a set of 64 × 64 blocks 
and each block is considered as if it were a small frame and is processed independently of other
blocks.5 The patch is of size 4 × 4 × T . Since each block is only 64 × 64  a small number of GMM
components are sufﬁcient to capture its statistics  and we ﬁnd the results are robust to K as long as
2 ≤ K ≤ 5 for PLE  Sparse-GMM and Sparse-GMM(G). Following [8  26]  we use the patches

3It is downloaded from http://cs.nyu.edu/∼roweis/data.html.
4The results of other settings can be found at https://sites.google.com/site/nipssgmm/.
5This subimage processing strategy has also been used in [2].

7

5101520253020222426283032FramesPSRN (dB) GAP (23.72)TwIST (24.81)GMM-TP (24.47)KSVD-OMP (22.37)PLE (25.35)Sparse-GMM (27.3)Sparse-GMM(G) (28.05)0.050.10.150.20.250.30.350.4510152025CS measurements fraction in a patchPSRN (dB) GAPTwISTPLESparse-GMMSparse-GMM(G)0.050.10.150.20.250.30.350.481012141618CS measurements fraction in a patchPSRN (dB) GAPTwISTPLESparse-GMMSparse-GMM(G)Figure 2: Plots of an example precision matrix (in mag-
nitude) learned by different GMM methods on the Face
dataset with m/n = 0.4. It is preferred to view the ﬁgure
electronically. The magnitudes in each precision matrix
are scaled to the range of [0  1].

of a randomly-selected video containing trafﬁc scenes6  which are irrelevant to the NBA game  as
training data to learn a GMM for GMM-TP with 20 components  and we use it to initialize PLE 
Sparse-GMM  and Sparse-GMM(G). The same training data are used to learn the dictionaries for
KSVD-OMP.
Results.
From the results shown in
Figure 1  we observe that the proposed
methods  especially Sparse-GMM(G) 
outperforms other methods with sig-
niﬁcant margins in all considered set-
tings. The better performance of Sparse-
GMM(G) over Sparse-GMM validates
the advantage of considering group s-
parsity in the model. Figure 2 shows the
an example precision matrix of one of K
Gaussian components that are learned
by the methods of PLE  Sparse-GMM  and Sparse-GMM(G) on the face dataset. From this ﬁgure 
we can see that Sparse-GMM and Sparse-GMM(G) show much clearer groups sparsity than PLE 
demonstrating the beniﬁts of using group sparsity constructed from the banding patterns.
4.2 Real measurements
We demonstrate the efﬁcacy of
the proposed methods on the CS
of video  with the measurements
acquired by the actual hardware
of CACTI camera [6]. A letter is
placed on the blades of a chop-
per wheel that rotates at an angu-
lar velocity of 15 blades per sec-
ond. The training data are ob-
tained from the videos of a chop-
per wheel rotating at several ori-
entations  positions and veloci-
ties. These training videos are
captured by a regular camcorder
at frame-rates that are differen-
t from the high-speed frame rate
achieved by CACTI reconstruc-
tion. Other settings of the meth-
ods are the same as in the experi-
ments on simulated data. The reconstruction results are shown in Figure 3  which shows that Sparse-
GMM(G) generally yields sharper reconstructed frames with less ghost effects than other methods.
5 Conclusions
The success of compressive sensing of signals from a GMM highly depends on the quality of the
estimator of the unknown GMM. In this paper  we have developed a hierarchical Bayesian method
to simultaneously estimate the GMM and recover the signals  all based on using only incomplete
linear measurements and a Bayesian shrinkage prior for promoting sparsity of the Gaussian preci-
sion matrices. In addition  we have obtained theoretical results under the challenging assumption
that the underlying GMM is unknown and has to be estimated from measurements that contain only
incomplete information about the signals. Our results extend substantially from previous theoretical
results in [7] which assume the GMM is exactly known. The experimental results with both sim-
ulated and hardware-acquired measurements show the proposed method signiﬁcantly outperforms
state-of-the-art methods.
Acknowledgement
The research reported here was funded in part by ARO  DARPA  DOE  NGA and ONR.

Figure 3: Reconstructed images 256 × 256 × T by differen-
t methods from the “raw measurement” acquired from CACTI
with T = 14. The region in the red boxes are enlarged and
shown at the right bottom part for better comparison.

6The results of the training videos containing general scenes can be found at the aforementioned website.

8

Max-Max 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81MMLE-GMM 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81MMLE-MFA 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81Sparse-GMM 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81Sparse-GMM(G) 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81Max-Max 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81MMLE-GMM 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81MMLE-MFA 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81Sparse-GMM 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81Sparse-GMM(G) 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81Max-Max 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81MMLE-GMM 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81MMLE-MFA 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81Sparse-GMM 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81Sparse-GMM(G) 204060801001201402040608010012014000.20.40.60.81 204060801001201402040608010012014000.20.40.60.81PLE Sparse-GMMSparse-GMM(G)#1#2#3#4#5#6#7#8#9#10#11#12#13#14Raw measurement (Coded image)GMM-TPTwISTGAP PLE #1#2#3#4#5#6#7#8#9#10#11#12#13#14#1#2#3#4#5#6#7#8#9#10#11#12#13#14#1#2#3#4#5#6#7#8#9#10#11#12#13#14 KSVD-OMP#1#2#3#4#5#6#7#8#9#10#11#12#13#14#1#2#3#4#5#6#7#8#9#10#11#12#13#14Sparse-GMMSparse-GMM(G)GMM-TPSparse-GMMSparse-GMM(G)#1#2#3#4#5#6#7#8#9#10#11#12#13#14GAPTwISTReferences
[1] M. Chen  J. Silva  J. Paisley  C. Wang  D. Dunson  and L. Carin  “Compressive sensing on manifolds
using a nonparametric mixture of factor analyzers: Algorithm and performance bounds ” IEEE Trans. on
Signal Processing  2010.

[2] G. Yu  G. Sapiro  and S. Mallat  “Solving inverse problems with piecewise linear estimators: From Gaus-

sian mixture models to structured sparsity ” IEEE Trans. on Image Processing  2012.

[3] G. Yu and G. Sapiro  “Statistical compressed sensing of Gaussian mixture models ” IEEE Trans. on Signal

Processing  2011.

[4] E. J. Cand`es  J. Romberg  and T. Tao  “Robust uncertainty principles: Exact signal reconstruction from

highly incomplete frequency information ” IEEE Trans. on Inform. Theory  2006.

[5] D. L. Donoho  “Compressed sensing ” IEEE Trans. on Inform. Theory  2006.
[6] P. Llull  X. Liao  X. Yuan  J. Yang  D. Kittle  L. Carin  G. Sapiro  and D. J. Brady  “Coded aperture

compressive temporal imaging ” Optics Express  2013.

[7] F. Renna  R. Calderbank  L. Carin  and M. Rodrigues  “Reconstruction of signals drawn from a Gaussian

mixture via noisy compressive measurements ” IEEE Trans. Signal Processing  2014.

[8] J. Yang  X. Yuan  X. Liao  P. Llull  G. Sapiro  D. J. Brady  and L. Carin  “Video compressive sensing using

Gaussian mixture models ” IEEE Trans. on Image Processing  vol. 23  no. 11  pp. 4863–4878  2014.

[9] ——  “Gaussian mixture model for video compressive sensing ” ICIP  pp. 19–23  2013.
[10] D. Zoran and Y. Weiss  “From learning models of natural image patches to whole image restoration ” in

ICCV  2011.

[11] S. Roth and M. J. Black  “Fields of experts ” Int. J. Comput. Vision  2009.
[12] F. Heitz and P. Bouthemy  “Multimodal estimation of discontinuous optical ﬂow using Markov random

ﬁelds.” IEEE Trans. Pattern Anal. Mach. Intell.  1993.

[13] V. Cevher  P. Indyk  L. Carin  and R. Baraniuk  “Sparse signal recovery and acquisition with graphical

models ” IEEE Signal Processing Magazine  2010.

[14] M. Tappen  C. Liu  E. Adelson  and W. Freeman  “Learning Gaussian conditional random ﬁelds for low-

level vision ” in CVPR  2007.

[15] H. Rue and L. Held  Gaussian Markov Random Fields: Theory and Applications  2005.
[16] T. Park and G. Casella  “The Bayesian lasso ” Journal of the American Statistical Association  2008.
[17] N. G. Polson and J. G. Scott  “Shrink globally  act locally: Sparse Bayesian regularization and prediction ”

Bayesian Statistics  2010.

[18] J. Fan  Y. Feng  and Y. Wu  “Network exploration via the adaptive lasso and scad penalties ” Ann. Appl.

Stat.  2009.

[19] A. P. Dempster  N. M. Laird  and D. B. Rubin  “Maximum likelihood from incomplete data via the EM

algorithm ” Journal of the Royal Statistical Society: Series B  1977.

[20] G. Parisi  Statistical Field Theory. Addison-Wesley  1998.
[21] M. I. Jordan  Z. Ghahramani  T. S. Jaakkola  and L. K. Saul  “An introduction to variational methods for

graphical models ” Machine Learning  1999.

[22] P.-L. Loh and M. J. Wainwright  “High-dimensional regression with noisy and missing data: Provable

guarantees with nonconvexity ” Ann. Statist.  2012.

[23] J. Friedman  T. Hastie  and R. Tibshirani  “Sparse inverse covariance estimation with the graphical lasso ”

Biostatistics  2008.

[24] X. Liao  H. Li  and L. Carin  “Generalized alternating projection for weighted-(cid:96)2 1 minimization with

applications to model-based compressive sensing ” SIAM Journal on Imaging Sciences  2014.

[25] J. Bioucas-Dias and M. Figueiredo  “A new TwIST: Two-step iterative shrinkage/thresholding algorithms

for image restoration ” IEEE Trans. on Image Processing  2007.

[26] Y. Hitomi  J. Gu  M. Gupta  T. Mitsunaga  and S. K. Nayar  “Video from a single coded exposure photo-

graph using a learned over-complete dictionary ” ICCV  2011.

[27] M. F. Duarte  M. A.Davenport  D. Takhar  J. N. Laska  S. Ting  K. F. Kelly  and R. G. Baraniuk  “Single-

pixel imaging via compressive sampling ” IEEE Signal Processing Magazine  2008.

[28] J. B. Tenenbaum  V. Silva  and J. C. Langford  “A global geometric framework for nonlinear dimension-

ality reduction ” Science  2000.

9

,Jianbo Yang
Xuejun Liao
Minhua Chen
Lawrence Carin
Noah Apthorpe
Alexander Riordan
Robert Aguilar
Jan Homann
Yi Gu
David Tank
H. Sebastian Seung