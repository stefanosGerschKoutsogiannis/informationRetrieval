2017,Efficient Online Linear Optimization with Approximation Algorithms,We revisit the problem of Online Linear Optimization in case the set of feasible actions is accessible through an approximated linear optimization oracle with a factor $\alpha$ multiplicative approximation guarantee. This setting is in particular interesting since it captures natural online extensions of well-studied offline linear optimization problems which are NP-hard  yet admit efficient approximation algorithms. The goal here is to minimize the $\alpha$-regret which is the natural extension of the standard regret in online learning to this setting.   We present new  algorithms with significantly improved oracle complexity for both the full information and bandit variants of the problem. Mainly  for both variants  we present $\alpha$-regret bounds of $O(T^{-1/3})$  were $T$ is the number of prediction rounds  using only $O(\log(T))$ calls to the approximation oracle per iteration  on average. These are the first results to obtain both average oracle complexity of $O(\log(T))$ (or even poly-logarithmic in $T$) and $\alpha$-regret bound $O(T^{-c})$ for a positive constant $c$  for both variants.,Efﬁcient Online Linear Optimization

with Approximation Algorithms

Dan Garber

Technion - Israel Institute of Technology

dangar@technion.ac.il

Abstract

We revisit the problem of online linear optimization in case the set of feasible ac-
tions is accessible through an approximated linear optimization oracle with a factor
↵ multiplicative approximation guarantee. This setting is in particular interesting
since it captures natural online extensions of well-studied ofﬂine linear optimiza-
tion problems which are NP-hard  yet admit efﬁcient approximation algorithms.
The goal here is to minimize the ↵-regret which is the natural extension of the
standard regret in online learning to this setting. We present new algorithms with
signiﬁcantly improved oracle complexity for both the full information and bandit
variants of the problem. Mainly  for both variants  we present ↵-regret bounds of
O(T 1/3)  were T is the number of prediction rounds  using only O(log(T )) calls
to the approximation oracle per iteration  on average. These are the ﬁrst results to
obtain both average oracle complexity of O(log(T )) (or even poly-logarithmic in
T ) and ↵-regret bound O(T c) for a constant c > 0  for both variants.

1

Introduction

In this paper we revisit the problem of Online Linear Optimization (OLO) [14]  which is a specialized
case of Online Convex Optimization (OCO) [12] with linear loss functions  in case the feasible set of
actions is accessible through an oracle for approximated linear optimization with a multiplicative
approximation error guarantee. In the standard setting of OLO  a decision maker is repeatedly
required to choose an action  a vector in some ﬁxed feasible set in Rd. After choosing his action 
the decision maker incurs loss (or payoff) given by the inner product between his selected vector
and a vector chosen by an adversary. This game between the decision maker and the adversary then
repeats itself. In the full information variant of the problem  after the decision maker receives his
loss (payoff) on a certain round  he gets to observe the vector chosen by the adversary. In the bandit
version of the problem  the decision maker only observes his loss (payoff) and does not get to observe
the adversary’s vector. The standard goal of the decision maker in OLO is to minimize a quantity
known as regret  which measures the difference between the average loss of the decision maker on
a game of T consecutive rounds (where T is ﬁxed and known in advance)  and the average loss
of the best feasible action in hindsight (i.e.  chosen with knowledge of all actions of the adversary
throughout the T rounds) (in case of payoffs this difference is reversed). The main concern when
designing algorithms for choosing the actions of the decision maker  is guaranteeing that the regret
goes to zero as the length of the game T increases  as fast as possible (i.e.  the rate of the regret in
terms of T ). It should be noted that in this paper we focus on the case in which the adversary is
oblivious (a.k.a. non-adaptive)  which means the adversary chooses his entire sequence of actions for
the T rounds beforehand.
While there exist well known algorithms for choosing the decision maker’s actions which guarantee
optimal regret bounds in T   such as the celebrated Follow the Perturbed Leader (FPL) and Online
Gradient Descent (OGD) algorithms [14  17  12]  efﬁcient implementation of these algorithms hinges

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

on the ability to efﬁciently solve certain convex optimization problems (e.g.  linear minimization
for FPL or Euclidean projection for OGD) over the feasible set (or the convex hull of feasible
points). However  when the feasible set corresponds for instance to the set of all possible solutions
to some NP-Hard optimization problem  no such efﬁcient implementations are known (or even
widely believed to exist)  and thus these celebrated regret-minimizing procedures cannot be efﬁciently
applied. Luckily  many NP-Hard linear optimization problems (i.e.  the objective function to either
minimize or maximize is linear) admit efﬁcient approximation algorithms with a multiplicative
approximation guarantee. Some examples include MAX-CUT (factor 0.87856 approximation due to
[9])   METRIC TSP (factor 1.5 approximation due to [6])  MINIMUM WEIGHTED VERTEX COVER
(factor 2 approximation [4])  and WEIGHTED SET COVER (factor (log n + 1) approximation due to
[7]). It is thus natural to ask wether an efﬁcient factor ↵ approximation algorithm for an NP-Hard
ofﬂine linear optimization problem could be used to construct  in a generic way  an efﬁcient algorithm
for the online version of the problem. Note that in this case  even efﬁciently computing the best ﬁxed
action in hindsight is not possible  and thus  minimizing regret via an efﬁcient algorithm does not
seem likely (given an approximation algorithm we can however compute in hindsight a decision
that corresponds to at most (at least) ↵ times the average loss (payoff) of the best ﬁxed decision in
hindsight).
In their paper [13]  Kakade  Kalai and Ligett were the ﬁrst to address this question in a fully generic
way. They showed that using only an ↵-approximation oracle for the set of feasible actions  it
is possible  at a high level  to construct an online algorithm which achieves vanishing (expected)
↵-regret  which is the difference between the average loss of the decision maker and ↵ times the
average loss of the best ﬁxed point in hindsight (for loss minimization problems and ↵  1; a
corresponding deﬁnition exists for payoff maximization problems and ↵< 1). Concretely  [13]
showed that one can guarantee O(T 1/2) expected ↵-regret in the full-information setting  which is
optimal  and O(T 1/3) in the bandit setting under the additional assumption of the availability of a
Barycentric Spanner (which we discuss in the sequel).
While the algorithm in [13] achieves an optimal ↵-regret bound (in terms of T ) for the full information
setting  in terms of computational complexity  the algorithm requires  in worst case  to perform on
each round O(T ) calls to the approximation oracle  which might be prohibitive and render the
algorithm inefﬁcient  since as discussed  in general  T is assumed to grow to inﬁnity and thus the
dependence of the runtime on T is of primary interest. Similarly  their algorithm for the bandit setting
requires O(T 2/3) calls to the approximation oracle per iteration.
The main contribution of our work is in providing new low ↵-regret algorithms for the full information
and bandit settings with signiﬁcantly improved oracle complexities. A detailed comparison with [13]
is given in Table 1. Concretely  for the full-information setting  we show it is possible to achieve
O(T 1/3) expected ↵-regret using only O(log(T )) calls to the approximation oracle per iteration 
on average  which signiﬁcantly improves over the O(T ) bound of [13]1. We also show a bound
of O(T 1/2) on the expected ↵-regret (which is optimal) using only O(pT log(T )) calls to the
oracle per iteration  on average  which gives nearly quadratic improvement over [13]. In the bandit
setting we show it is possible to obtain a O(T 1/3) bound on the expected ↵-regret (same as in [13])
using only O(log(T )) calls to the oracle per iteration  on average  under the same assumption on the
availability of a Barycentric Spanner (BS). It is important to note that while there exist algorithms
for OLO with bandit feedback which guarantee ˜O(T 1/2) expected regret [1  11] (where the ˜O(·)
hides poly-logarithmic factors in T )  these require on each iteration to either solve to arbitrarily small
accuracy a convex optimization problem over the feasible set [1]  or sample a point from the feasible
set according to a speciﬁed distribution [11]  both of which cannot be implemented efﬁciently in our
setting. On the other-hand  as we formally show in the sequel  at a high level  using a BS (originally
introduced in [2]) simply requires to ﬁnd a single set of d points from the feasible set which span the
entire space Rd (assuming this is possible  otherwise the set could be mapped to a lower dimensional
space). The process of ﬁnding these vectors can be viewed as a preprocessing step and thus can be
carried out ofﬂine. Moreover  as discussed in [13]  for many NP-Hard problems it is possible to
compute a BS in polynomial time and thus even this preprocessing step is efﬁcient. Importantly  [13]
shows that the approximation oracle by itself is not strong enough to guarantee non-trivial ↵-regret in
the bandit setting  and hence this assumption on the availability of a BS seems reasonable. Since the

1as we show in the appendix  even if we relax the algorithm of [13] to only guarantee O(T 1/3) ↵-regret  it

will still require O(T 2/3) calls to the oracle per iteration  on average.

2

full information

bandit information

Reference
KKL [13]
This paper (Thm. 4.1  4.2)
This paper (Thm. 4.1)

↵  regret
T 1/2
T 1/3
T 1/2

oracle complexity

T

log(T )
pT log(T )

↵  regret
T 1/3
T 1/3

-

oracle complexity

T 2/3
log(T )

-

Table 1: comparison of expected ↵ regret bounds and average number of calls to the approximation
oracle per iteration. In all bounds we give only the dependence on the length of the game T and
omit all other dependencies which we treat as constants. In the bandit setting we report the expected
number of calls to the oracle per iteration.

best general regret bound known using a BS is O(T 1/3)  the ↵-regret bound of our bandit algorithm
is the best achievable to date via an efﬁcient algorithm.
Technically  the main challenge in the considered setting is that as discussed  we cannot readily apply
standard tools such as FPL and OGD. At a high level  in [13] it was shown that it is possible to
apply the OGD method by replacing the exact projection step of OGD with an iterative algorithm
which ﬁnds an infeasible point  but one that both satisﬁes the projection property required by OGD
and is dominated by a convex combination of feasible points for every relevant linear loss (payoff)
function. Unfortunately  in worst case  the number of queries to the approximation oracle required by
this so-called projection algorithm per iteration is linear in T . While our online algorithms are also
based on an application of OGD  our approach to computing the so-called projections is drastically
different than [13]  and is based on a coupling of two cutting plane methods  one that is based on
the Ellipsoid method  and the other that resembles Gradient Descent. This approach might be of
independent interest and might prove useful to similar problems.

1.1 Additional related work
Kalai and Vempala [14] showed that approximation algorithms which have point-wise approximation
guarantee  such as the celebrated MAX-CUT algorithm of [9]  could be used to instantiate their
Follow the Perturbed Leader framework to achieve low ↵-regret. However this construction is far
from generic and requires the oracle to satisfy additional non-trivial conditions. This approach was
also used in [3]. In [14] it was also shown that FPL could be instantiated with a FPTAS to achieve low
↵-regret  however the approximation factor in the FPTAS needs to be set to roughly (1 + O(T 1/2)) 
which may result in prohibitive running times even if a FPTAS for the underlying problem is available.
Similarly  in [8] it was shown that if the approximation algorithm is based on solving a convex
relaxation of the original  possibly NP-Hard  problem  this additional structure can be used with
the FPL framework to achieve low ↵-regret efﬁciently. To conclude all of the latter works consider
specialized cases in which the approximation oracle satisﬁes additional non-trivial assumptions
beyond its approximation guarantee  whereas here  similarly to [13]  we will be interested in a generic
as possible conversion from the ofﬂine problem to the online one  without imposing additional
structure on the ofﬂine oracle.

2 Preliminaries

2.1 Online linear optimization with approximation oracles
Let K F be compact sets of points in Rd
+ (non-negative orthant in Rd) such that maxx2K kxk 
R  maxf2F kfk  F   for some R > 0  F > 0 (throughout this work we let k·k denote the standard
Euclidean norm)  and for all x 2K   f 2F it holds that C  x · f  0  for some C > 0.
We assume K is accessible through an approximated linear optimization oracle OK : Rd
+ !K with
parameter ↵> 0 such that:
if ↵  1;
OK(c) · c  ↵ maxx2K x · c if ↵< 1.

OK(c) 2K and ⇢ OK(c) · c  ↵ minx2K x · c

8c 2 Rd
+ :

Here K is the feasible set of actions for the player  and F is the set of all possible loss/payoff vectors2.
+ are
made for ease of presentation and clarity  and since these naturally hold for many NP-Hard optimization problem
that are relevant to our setting. Nevertheless  these assumptions could be easily generalized as done in [13].

2we note that both of our assumptions that K⇢ Rd

+ and that the oracle takes inputs from Rd

+ F⇢ Rd

3

↵  regret({(xt  ft)}t2[T ]) :=8<:

T PT

t=1 xt · ft  ↵ · minx2K
t=1 x · ft  1

1

↵ · maxx2K

T PT

T PT
T PT

t=1 x · ft
t=1 xt · ft

if ↵  1;
if ↵< 1.

(1)

Since naturally a factor ↵> 1 for the approximation oracle is reasonable only for loss minimization
problems  and a value ↵< 1 is reasonable for payoff maximization problems  throughout this
work it will be convenient to use the value of ↵ to differentiate between minimization problems and
maximization problems.
Given a sequence of linear loss/payoff functions {f1  ...  fT}2F T and a sequence of feasible points
{x1  ....  xT}2K T   we deﬁne the ↵  regret of the sequence {xt}t2[T ] with respect to the sequence
{ft}t2[T ] as

1

1

When the sequences {xt}t2[T ] {ft}t2[T ] are obvious from context we will simply write ↵  regret
without stating these sequences. Also  when the sequence {xt}t2[T ] is randomized we will use
E[↵  regret] to denote the expected ↵-regret.
2.1.1 Online linear optimization with full information
In OLO with full information  we consider a repeated game of T prediction rounds  for a ﬁxed
T   where on each round t  the decision maker is required to choose a feasible action xt 2K .
After committing to his choice  a linear loss function ft 2F is revealed  and the decision maker
incurs loss of xt · ft.
In the payoff version  the decision maker incurs payoff of xt · ft. The
game then continues to the next round. The overall goal of the decision maker is to guarantee
that ↵  regret({(xt  ft)}t2[T ]) = O(T c) for some c > 0  at least in expectation (in fact using
randomization is mandatory since K need not be convex). Here we assume that the adversary is
oblivious (aka non-adaptive)  i.e.  the sequence of losses/payoffs f1  ...  fT is chosen in advance
(before the ﬁrst round)  and does not depend on the actions of the decision maker.

2.1.2 Bandit feedback
The bandit version of the problem is identical to the full information setting with one crucial difference:
on each round t  after making his choice  the decision maker does not observe the vector ft  but only
the value of his loss/payoff  given by xt · ft.
2.2 Additional notation
For any two sets S K⇢ Rd and a scalar  2 R we deﬁne the sets S +K := {x + y | x 2S   y 2K}  
S := {x | x 2S} . We also denote by CH(K) the convex-hull of all points in a set K. For a
convex and compact set S⇢ Rd and a point x 2 Rd we deﬁne dist(x S) := minz2S kz  xk. We
let B(c  r) denote the Euclidean ball or radius r centered in c.
2.3 Basic algorithmic tools

We now brieﬂy describe two very basic ideas that are essential for constructing our algorithms 
namely the extended approximation oracle and the online gradient descent without feasibility method.
These were already suggested in [13] to obtain their low ↵-regret algorithms. We note that in the
appendix we describe in more detail the approach of [13] and discuss its shortcomings in obtaining
oracle-efﬁcient algorithms.

2.3.1 The extended approximation oracle
As discussed  a key difﬁculty of our setting that prevents us from directly applying well studied
algorithms for OLO  is that essentially all standard algorithms require to exactly solve (or up to
arbitrarily small error) some linear/convex optimization problem over the convexiﬁcation of the
feasible set CH(K). However  not only that our approximation oracle OK(·) cannot perform exact
minimization  even for ↵ = 1 it is applicable only with inputs in Rd
+  and hence cannot optimize in
all directions. A natural approach  suggested in [13]  to overcome the approximation error of the
oracle OK(·)  is to consider optimization with respect to the convex set CH(↵K) (i.e. convex hull
of all points in K scaled by a factor of ↵) instead of CH(K). Indeed  if we consider for instance
the case ↵  1  it is straightforward to see that for any c 2 Rd
+  OK(c) · c  ↵ minx2K x · c =

4

+  although the oracle returns points in the original set K.

↵ minx2CH(K) x · c = minx2CH(↵K) x · c. Thus  in a certain sense  OK(·) can optimize with respect
to CH(↵K) for all directions in Rd
The following lemma shows that one can easily extend the oracle OK(·) to optimize with respect to
all directions in Rd.
Lemma 2.1 (Extended approximation oracle). Given c 2 Rd write c = c+ + c where c+ equals
to c on all non-negative coordinates of c and zero everywhere else  and c equals c on all negative
coordinates and zero everywhere else. The extended approximation oracle is a mapping ˆOK : Rd !
(K + B(0  (1 + ↵)R)  K) deﬁned as:

ˆOK(c) = (v  s) :=⇢ (OK(c+)  ↵R¯c  OK(c+))
(OK(c)  R¯c+  OK(c))

if ↵  1;
if ↵< 1 

(2)

where for any vector v 2 Rd we denote ¯v = v/kvk if kvk > 0 and ¯v = 0 otherwise  and it satisﬁes
the following three properties:
1. v · c  minx2↵K x · c
2. 8f 2F : s · f  v · f if ↵  1 and s · f  v · f if ↵< 1
3. kvk  (↵ + 2)R

The proof is given in the appendix for completeness.
It is important to note that while the extended oracle provides solutions with values at least as low as
any point in CH(↵K)  still in general the output point v need not be in either K or CH(↵K)  which
means that it is not a feasible point to play in our OLO setting  nor does it allow us to optimize
over CH(↵K). This is why we also need the oracle to output the feasible point s 2K which
dominates v for any possible loss/payoff vector in F. While we will use the outputs v to solve a
certain optimization problem involving CH(↵K)  this dominance relation will be used to convert the
solutions to these optimization problems into feasible plays for our OLO algorithms.

2.3.2 Online gradient descent with and without feasibility
As in [13]  our online algorithms will be based on the well known Online Gradient Descent method
(OGD) for online convex optimization  originally due to [17]. For a sequence of loss vectors
{f1  ...  fT}⇢ Rd OGD produces a sequence of plays {x1  ...  xT}⇢S   for a convex and compact
set S⇢ Rd via the following updates: 8t  1 : yt+1 xt⌘ft  xt+1 arg minx2S kx  yt+1k2 
where x1 is initialized to some arbitrary point in S and ⌘ is some pre-determined step-size. The
obvious difﬁculty in applying OGD to online linear optimization over S = CH(↵K) is the step of
computing xt+1 by projecting yt+1 onto the feasible set S  since as discussed  even with the extended
approximation oracle  one cannot exactly optimize over CH(↵K). Instead we will consider a variant
of OGD which may produce infeasible points  i.e.  outside of S  but which guarantees low regret
with respect to any point in S. This algorithm  which we refer to as online gradient descent without
feasibility  is given below (Algorithm 1).

Algorithm 1 Online Gradient Descent Without Feasibility
1: input: learning rate ⌘> 0
2: x1 some point in S
3: for t = 1 . . . T do
play xt and receive loss/payoff vector ft 2 Rd
4:
yt+1 ⇢ xt  ⌘ft
ﬁnd xt+1 2 Rd such that

for losses
for payoffs

xt + ⌘ft

5:

6:

7: end for

8z 2S :

kz  xt+1k2  kz  yt+1k2

(3)

Lemma 2.2. [Online gradient descent without feasibility] Fix ⌘> 0. Suppose Algorithm 1 is applied
for T rounds and let {ft}T
t=1 ⇢ Rd be the sequence of observed loss/payoff vectors  and let {xt}T
t=1

5

be the sequence of points played by the algorithm. Then for any x 2S it holds that

2T PT
2T PT

t=1 kftk2
t=1 kftk2

for losses;

for payoffs.

1

T PT
T PT

T PT
t=1 x · ft  1
t=1 xt · ft  1
T PT
t=1 xt · ft  1
t=1 x · ft  1

1

2T⌘ kx1  xk2 + ⌘
2T⌘ kx1  xk2 + ⌘

The proof is given in the appendix for completeness.

3 Oracle-efﬁcient Computation of (infeasible) Projections onto CH(↵K)
In this section we detail our main technical tool for obtaining oracle-efﬁcient online algorithms 
i.e.  our algorithm for computing projections  in the sense of Eq. (3)  onto the convex set CH(↵K).
Before presenting our projection algorithm  Algorithm 2 and detailing its theoretical guarantees 
we ﬁrst present the main algorithmic building block in the algorithm  which is described in the
following lemma. Lemma 3.1 shows that for any point x 2 Rd  we can either ﬁnd a near-by point
p which is a convex combination of points outputted by the extended approximation oracle (and
hence  p is dominated by a convex combination of feasible points in K for any vector in F  as
discussed in Section 2.3.1)  or we can ﬁnd a separating hyperplane that separates x from CH(↵K)
with sufﬁciently large margin. We achieve this by running the well known Ellipsoid method [10  5]
in a very specialized way. This application of the Ellipsoid method is similar in spirit to those in
[15  16]  which applied this idea to computing correlated equilibrium in games and algorithmic
mechanism design  though the implementation details and the way in which we apply this technique
are quite different.
The proof of the following lemma is given in the appendix.
Lemma 3.1 (Separation-or-Decomposition via the Ellipsoid method). Fix x 2 Rd  ✏ 2
⌘  where c is a positive univer-
(0  (↵ + 2)R]  and a positive integer N  cd2 ln⇣ (↵+1)R+kxk
sal constant. Consider an attempt to apply the Ellipsoid method for N iterations to the following
feasibility problem:

✏

kwk  1 
such that each iteration of the Ellipsoid method applies the following consecutive steps:

ﬁnd w 2 Rd such that:

(x  z) · w  ✏

8z 2 ↵K :

and

(4)

separating hyperplane for the Ellipsoid method and continue to to the next iteration

1. (v  s) ˆOK(w)  where w is the current iterate. If (x  v) · w <✏   use v  x as a
2. if kwk > 1  use w as a separating hyperplane for the Ellipsoid method and continue to the

next iteration

3. otherwise (kwk  1 and (x  v) · w  ✏)  declare Problem (4) feasible and return the

vector w.

2

1

min

(a1 ... aN )

Then  if the Ellipsoid method terminates declaring Problem 4 feasible  the returned vector w is a
feasible solution to Problem (4). Otherwise (the Ellipsoid method completes N iterations without
declaring Problem (4) feasible)  let (v1  s1)  ...  (vN   sN ) be the outputs of the extended approxima-
tion oracle gathered throughout the run of the algorithm  and let (a1  ...  aN ) be an optimal solution
to the following convex optimization problem:

aivi  x

such that 8i 2{ 1  ...  N} : ai  0 

2
NXi=1
Then the point p =PN
We are now ready to present our algorithm for computing projections onto CH(↵K) (in the sense of
Eq. (3)). Consider now an attempt to project a point y 2 Rd  and note that in particular  y itself is a
valid projection (again  in the sense of Eq. (3))  however  in general  it is not a feasible point nor is
it dominated by a convex combination of feasible points. When attempting to project y 2 Rd  our
algorithm continuously applies the separation-or-decomposition procedure described in Lemma 3.1.

i=1 aivi satisﬁes kx  pk  3✏.

NXi=1

ai = 1.

(5)

6

In case the procedure returns a decomposition  then by Lemma 3.1  we have a point that is sufﬁciently
close to y and is dominated for any vector in F by a convex combination (given explicitly) of feasible
points in K. Otherwise  the procedure returns a separating hyperplane which can be used to to “pull
y closer" to CH(↵K) in a way that the resulting point still satisﬁes the projection inequality given in
Eq. (3)  and the process then repeats itself. Since each time we obtain a hyperplane separating our
current iterate from CH(↵K)  we pull the current iterate sufﬁciently towards CH(↵K)  this process
must terminate. Lemma 3.2 gives exact bounds on the performance of the algorithm.

else

call the SEPARATION-OR-DECOMPOSTION procedure (Lemma 3.1) with parameters (˜y ✏ )
if the procedure outputs a separating hyperplane w then

˜y ˜y  ✏w
let (a1  ...  aN )  {(v1  s1)  ...  (vN   sN )} be the decomposition returned
return ˜y  (a1  ...  aN )  {(v1  s1)  ...  (vN   sN )}

Algorithm 2 (infeasible) Projection onto CH(↵K)
1: input: point y 2 Rd  tolerance ✏> 0
2: ˜y y/ max{1  kyk/(↵R)}
3: for t = 1 . . . do
4:
5:
6:
7:
8:
9:
end if
10:
11: end for
Lemma 3.2. Fix y 2 Rd and ✏ 2 (0  (↵ + 2)R]. Algorithm 2 terminates after at most d↵2R2/✏2e
iterations  returning a point ˜y 2 Rd  a distribution (a1  ...  aN ) and a set {(v1  s1)  ...  (vN   sN )}
outputted by the extended approximation oracle  where N is as deﬁned in Lemma 3.1  such that
1. 8z 2 CH(↵K) :
Moreover  if the for loop was entered a total number of k times  then the ﬁnal value of ˜y satisﬁes

for p := Xi2[N ]

k˜y  zk2  ky  zk2  

2.

kp  ˜yk  3✏

aivi.

dist2(˜y  CH(↵K))  min{2↵2R2  dist2(y  CH(↵K))  (k  1)✏2} 

and the overall number of queries to the approximation oracle is Okd2 ln ((↵ + 1)R/✏).

It is important to note that the worst case iteration bound in Lemma 3.2 does not seem so appealing
for our purposes  since it depends polynomially on 1/✏  and in our online algorithms naturally we
will need to take ✏ = O(T c) for some c > 0  which seems to contradict our goal of achieving
poly-logarithmic in T oracle complexity  at least on average. However  as Lemma 3.2 shows  the
more iterations Algorithm 2 performs  the closer it brings its ﬁnal iterate to the set CH(↵K). Thus  as
we will show when analyzing the oracle complexity of our online algorithms  while a single call to
Algorithm 2 can be expensive  when calling it sequentially  where each input is a small perturbation
of the output of the previous call  the average number of iterations performed per such call cannot be
too high.

4 Efﬁcient Algorithms for the Full Information and Bandit Settings

We now turn to present our online algorithms for the full-information and bandit settings together
with their regret bounds and oracle-complexity guarantees.

4.1 Algorithm for the full information setting
Our algorithm for the full-information setting  Algorithm 3  is given below.
Theorem 4.1. [Main Theorem] Fix ⌘> 0 ✏ 2 (0  (↵ + 2)R]. Suppose Algorithm 3 is applied for T
rounds and let {ft}T
t=1 be the
sequence of points played by the algorithm. Then it holds that

t=1 ✓F be the sequence of observed loss/payoff vectors  and let {st}T

and the average number of calls to the approximation oracle of K per iteration is upper bounded by

E⇥↵  regret{(st  ft)}t2[T ]⇤  ↵2R2T 1⌘1 + ⌘F 2/2 + 3F✏ 
K(⌘  ✏) := O1 +⌘↵RF + ⌘2F 2 ✏2 d2 ln ((↵ + 1)R/✏) .

7

Algorithm 3 Online Gradient Descent with Infeasible Projections onto CH(↵K)
1: input: learning rate ⌘> 0  projection error parameter ✏> 0
2: s1 some point in K  ˜y1 ↵s1
3: for t = 1 . . . T do
4:

5:

6:

˜yt + ⌘ft

play st and receive loss/payoff vector ft 2F
yt+1 ⇢ ˜yt  ⌘ft
call Algorithm 2 with inputs (yt+1 ✏ ) to obtain an approximated projection ˜yt+1  a distribution
(a1  ...  aN ) and {(v1  s1)  ...  (vN   sN )}✓ Rd ⇥K   for some N 2 N.
sample st+1 2{ s1  ...  sN} according to distribution (a1  ...  aN )

if ↵  1
if ↵< 1

7:
8: end for

In particular  setting ⌘ = ↵RT 2/3/F   ✏ = ↵RT 1/3 gives E [↵  regret] = O↵RF T 1/3 
K = Od2 ln ↵+1
↵ T. Alternatively  setting ⌘ = ↵RT 1/2/F   ✏ = ↵RT 1/2 gives
E [↵  regret] = O↵RF T 1/2  K = O⇣pT d2 ln ↵+1

↵ T⌘.

The proof is given in the appendix.

4.2 Algorithm for the bandit information setting

i=1 qiq>i

Our algorithm for the bandit setting follows from a very well known reduction from the bandit setting
to the full information setting  also applied in the bandit algorithm of [13]. The algorithm simply
simulates the full information algorithm  Algorithm 3  by providing it with estimated loss/payoff
vectors ˆf1  ...  ˆfT instead of the true vectors f1  ...  fT which are not available in the bandit setting.
This reduction is based on the use of a Barycentric Spanner (deﬁned next) for the feasible set K.
As standard  we assume the points in K span the entire space Rd  otherwise we can reformulate the
problem in a lower-dimensional space  in which this assumption holds.
Deﬁnition 4.1 (Barycentric Spanner3). We say that a set of d vectors {q1  ...  qd}⇢ Rd is a
Barycentric Spanner with parameter > 0 for a set S⇢ Rd  denoted by -BS(S)  if it holds that
{q1  ...  qd}⇢S   and the matrix Q :=Pd
Importantly  as discussed in [13]  the assumption on the availability of such a set -BS(K) seems
reasonable  since i) for many sets that correspond to the set of all possible solutions to some well-
studied NP-Hard optimization problem  one can still construct in poly(d) time a barycentric spanner
with  = poly(d)  ii) -BS(K) needs to be constructed only once and then stored in memory (overall
d vectors in Rd)  and hence its construction can be viewed as a pre-processing step  and iii) as
illustrated in [13]  without further assumptions  the approximation oracle by itself is not sufﬁcient to
guarantee nontrivial regret bounds in the bandit setting.
The algorithm and the proof of the following theorem are given in the appendix.
Theorem 4.2. Fix ⌘> 0 ✏ 2 (0  (↵ + 2)R]  2 (0  1). Suppose Algorithm 5 is applied for T
rounds and let {ft}T
t=1 be the
sequence of points played by the algorithm. Then it holds that

is not singular and maxi2[d] kQ1qik  .

t=1 ✓F be the sequence of observed loss/payoff vectors  and let {ˆst}T
E⇥↵  regret{(ˆst  ft)}t2[T ]⇤  ↵2R2⌘1T 1 + ⌘d2C221/2 + 3✏F + C 
E [K(⌘  ✏   )] := O1 +⌘↵dCR + (⌘dC )2/ ✏2 d2 ln ((↵ + 1)R/✏) .

and the expected number of calls to the approximation oracle of K per iteration is upper bounded by

dC T 2/3  ✏ = ↵RT 1/3   = T 1/3 gives E [↵  regret] =

In particular  setting ⌘ = ↵R

O(↵dCR + ↵RF + C)T 1/3  E[K] = Od2 ln ↵+1

↵ T.

3this deﬁnition is somewhat different than the classical one given in [2]  however it is equivalent to a

C-approximate barycentric spanner [2]  with an appropriately chosen constant C().

8

References
[1] Jacob Abernethy  Elad Hazan  and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm for

bandit linear optimization. In COLT  pages 263–274  2008.

[2] Baruch Awerbuch and Robert D Kleinberg. Adaptive routing with end-to-end feedback: Distributed
learning and geometric approaches. In Proceedings of the thirty-sixth annual ACM symposium on Theory
of computing  pages 45–53. ACM  2004.

[3] Maria-Florina Balcan and Avrim Blum. Approximation algorithms and online mechanisms for item pricing.

In Proceedings of the 7th ACM Conference on Electronic Commerce  pages 29–35. ACM  2006.

[4] Reuven Bar-Yehuda and Shimon Even. A linear-time approximation algorithm for the weighted vertex

cover problem. Journal of Algorithms  2(2):198–203  1981.

[5] Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends R in

Machine Learning  8(3-4):231–357  2015.

[6] Nicos Christoﬁdes. Worst-case analysis of a new heuristic for the travelling salesman problem. Technical

report  DTIC Document  1976.

[7] V. Chvatal. A greedy heuristic for the set-covering problem. Mathematics of Operations Research 

4(3):233–235  1979.

[8] Takahiro Fujita  Kohei Hatano  and Eiji Takimoto. Combinatorial online prediction via metarounding. In

ALT  pages 68–82. Springer  2013.

[9] Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut and
satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM)  42(6):1115–1145 
1995.

[10] M. Grötschel  L. Lovász  and A. Schrijver. The ellipsoid method and its consequences in combinatorial

optimization. Combinatorica  1(2):169–197  1981.

[11] Elad Hazan  Zohar Shay Karnin  and Raghu Meka. Volumetric spanners: an efﬁcient exploration basis for

learning. In COLT  volume 35  pages 408–422  2014.

[12] Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization. In Proceedings
of the 33nd International Conference on Machine Learning  ICML 2016  New York City  NY  USA  June
19-24  2016  pages 1263–1271  2016.

[13] Sham M. Kakade  Adam Tauman Kalai  and Katrina Ligett. Playing games with approximation algorithms.

SIAM J. Comput.  39(3):1088–1106  2009.

[14] Adam Kalai and Santosh Vempala. Efﬁcient algorithms for online decision problems. Journal of Computer

and System Sciences  71(3):291–307  2005.

[15] Christos H Papadimitriou and Tim Roughgarden. Computing correlated equilibria in multi-player games.

Journal of the ACM (JACM)  55(3):14  2008.

[16] S Matthew Weinberg. Algorithms for strategic agents. PhD thesis  Massachusetts Institute of Technology 

2014.

[17] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Machine
Learning  Proceedings of the Twentieth International Conference (ICML 2003)  August 21-24  2003 
Washington  DC  USA  pages 928–936  2003.

9

,Wojciech Zaremba
Arthur Gretton
Matthew Blaschko
Dan Garber