2013,On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization,Motivated by various applications in machine learning  the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately.  Currently  a popular method for solving such problem is the proximal gradient method (PGM)  which is known to have a sublinear rate of convergence.  In this paper  we show that for a large class of loss functions  the convergence rate of the PGM is in fact linear.  Our result is established without any strong convexity assumption on the loss function.  A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm-regularized problem  which may be of independent interest.,On the Linear Convergence of the Proximal Gradient

Method for Trace Norm Regularization

Ke Hou  Zirui Zhou  Anthony Man–Cho So

Department of Systems Engineering & Engineering Management

The Chinese University of Hong Kong

Shatin  N. T.  Hong Kong

{khou zrzhou manchoso}@se.cuhk.edu.hk

Department of Electrical & Computer Engineering

Zhi–Quan Luo

University of Minnesota

Minneapolis  MN 55455  USA

luozq@ece.umn.edu

Abstract

Motivated by various applications in machine learning  the problem of minimiz-
ing a convex smooth loss function with trace norm regularization has received
much attention lately. Currently  a popular method for solving such problem is
the proximal gradient method (PGM)  which is known to have a sublinear rate of
convergence. In this paper  we show that for a large class of loss functions  the
convergence rate of the PGM is in fact linear. Our result is established without any
strong convexity assumption on the loss function. A key ingredient in our proof
is a new Lipschitzian error bound for the aforementioned trace norm–regularized
problem  which may be of independent interest.

1 Introduction

The problem of ﬁnding a low–rank matrix that (approximately) satisﬁes a given set of conditions
has recently generated a lot of interest in many communities. Indeed  such a problem arises in a
wide variety of applications  including approximation algorithms [17]  automatic control [5]  matrix
classiﬁcation [20]  matrix completion [6]  multi–label classiﬁcation [1]  multi–task learning [2] 
network localization [7]  subspace learning [24]  and trace regression [9]  just to name a few. Due to
the combinatorial nature of the rank function  the task of recovering a matrix with the desired rank
and properties is generally intractable. To circumvent this  a popular approach is to use the trace
norm1 (also known as the nuclear norm) as a surrogate for the rank function. Such an approach is
quite natural  as the trace norm is the tightest convex lower bound of the rank function over the set
of matrices with spectral norm at most one [13]. In the context of machine learning  the trace norm
is typically used as a regularizer in the minimization of certain convex loss function. This gives rise
to convex optimization problems of the form

min

X∈Rm×n

{F (X) = f (X) + τ kXk∗}  

(1)

where f : Rm×n → R is the convex loss function  kXk∗ denotes the trace norm of X  and τ > 0
is a regularization parameter. By standard results in convex optimization [4]  the above formulation
is tractable (i.e.  polynomial–time solvable) for many choices of the loss function f . In practice 

1Recall that the trace norm of a matrix is deﬁned as the sum of its singular values.

1

however  one is often interested in settings where the decision variable X is of high dimension.
Thus  there has been much research effort in developing fast algorithms for solving (1) lately.

Currently  a popular method for solving (1) is the proximal gradient method (PGM)  which exploits
the composite nature of the objective function F and certain smoothness properties of the loss func-
tion f [8  19  11]. The attractiveness of PGM lies not only in its excellent numerical performance 
but also in its strong theoretical convergence rate guarantees. Indeed  for the trace norm–regularized
problem (1) with f being convex and continuously differentiable and ∇f being Lipschitz continu-
ous  the standard PGM will achieve an additive error of O(1/k) in the optimal value after k itera-
tions. Moreover  this error can be reduced to O(1/k2) using acceleration techniques; see  e.g.  [19].
The sublinear O(1/k2) convergence rate is known to be optimal if f is simply given by a ﬁrst–order
oracle [12]. On the other hand  if f is strongly convex  then the convergence rate can be improved to
O(ck) for some c ∈ (0  1) (i.e.  a linear convergence rate) [16]. However  in machine learning  the
loss functions of interest are often highly structured and hence not just given by an oracle  but they
are not necessarily strongly convex either. For instance  in matrix completion  a commonly used loss
2/2  where A : Rm×n → Rp is a linear measurement
function is the square loss f (·) = kA(·) − bk2
operator and b ∈ Rp is a given set of observations. Clearly  f is not strongly convex when A has a
non–trivial nullspace (or equivalently  when A is not injective). In view of this  it is natural to ask
whether linear convergence of the PGM can be established for a larger class of loss functions.

In this paper  we take a ﬁrst step towards answering this question. Speciﬁcally  we show that when
the loss function f takes the form f (X) = h(A(X))  where A : Rm×n → Rp is an arbitrary
linear operator and h : Rp → R is strictly convex with certain smoothness and curvature properties 
the PGM for solving (1) has an asymptotic linear rate of convergence. Note that f need not be
strictly convex even if h is  as A is arbitrary. Our result covers a wide range of loss functions used
in the literature  such as square loss and logistic loss. Moreover  to the best of our knowledge  it
is the ﬁrst linear convergence result concerning the application of a ﬁrst–order method to the trace
norm–regularized problem (1) that does not require the strong convexity of f .

The key to our convergence analysis is a new Lipschitzian error bound for problem (1). Roughly 
it says that the distance between a point X ∈ Rm×n and the optimal solution set of (1) is on the
order of the residual norm kproxτ (X − ∇f (X)) − XkF   where proxτ is the proximity operator
associated with the regularization term τ kXk∗. Once we have such a bound  a routine applica-
tion of the powerful analysis framework developed by Luo and Tseng [10] will yield the desired
linear convergence result. Prior to this work  Lipschitzian error bounds for composite function min-
imization are available for cases where the non–smooth part either has a polyhedral epigraph (such
as the ℓ1–norm) [23] or is the (sparse) group LASSO regularization [22  25]. However  the ques-
tion of whether a similar bound holds for trace norm regularization has remained open  despite its
apparent similarity to ℓ1–norm regularization. Indeed  unlike the ℓ1–norm  the trace norm has a non–
polyhedral epigraph; see  e.g.  [18]. Moreover  the existing approach for establishing error bounds
for ℓ1–norm or (sparse) group LASSO regularization is based on splitting the decision variables into
groups  where variables from different groups do not interfere with one another  so that each group
can be analyzed separately. However  the trace norm of a matrix is determined by its singular values 
and each of them depends on every single entry of the matrix. Thus  we cannot use the same split-
ting approach to analyze the entries of the matrix. To overcome the above difﬁculties  we make the
crucial observation that if ¯X is an optimal solution to (1)  then both ¯X and −∇f ( ¯X) have the same
set of left and right singular vectors; see Proposition 4.2. As a result  we can use matrix perturbation
theory to get hold of the spectral structure of the points that are close to the optimal solution set. This
in turn allows us to establish a Lipschitzian error bound for the trace norm–regularized problem (1) 
thereby resolving the aforementioned open question in the afﬁrmative.

2 Preliminaries

2.1 Basic Setup

We consider the trace norm–regularized optimization problem (1)  in which the loss function f :
Rm×n → R takes the form

(2)
where A : Rm×n → Rp is a linear operator and h : Rp → R is a function satisfying the following
assumptions:

f (X) = h(A(X)) 

2

Assumption 2.1

(a) The effective domain of h  denoted by dom(h)  is open and non–empty.

(b) The function h is continuously differentiable with Lipschitz–continuous gradient on dom(h)

and is strongly convex on any convex compact subset of dom(h).

Note that Assumption 2.1(b) implies the strict convexity of h on dom(h) and the Lipschitz continuity
of ∇f . Now  let X denote the set of optimal solutions to problem (1). We make the following
assumption concerning X :

Assumption 2.2 The optimal solution set X is non–empty.

The above assumptions can be justiﬁed in various applications. For instance  in matrix completion 
the square loss f (·) = kA(·) − bk2
2/2 induced by the linear measurement operator A and the set
of observations b ∈ Rp is of the form (2)  with h(·) = k(·) − bk2
2/2. Moreover  it is clear that
such an h satisﬁes Assumptions 2.1 and 2.2. In multi–task learning  the loss function takes the form
t=1 ℓ(At(·)  yt)  where T is the number of learning tasks  At : Rm×n → Rp is the linear
operator deﬁned by the input data for the t–th task  yt ∈ Rp is the output data for the t–th task  and
ℓ : Rp × Rp → R measures the learning error. Note that f can be put into the form (2)  where
A : Rm×n → RT p is given by A(X) = (A1(X)  A2(X)  . . .   AT (X))  and h : RT p → R is
t=1 ℓ(zt  yt) with zt ∈ Rp for t = 1  . . .   T and z = (z1  . . .   zT ). Moreover 
2/2) or the logistic loss (i.e. 

in the case where ℓ is  say  the square loss (i.e.  ℓ(zt  yt) = kzt − ytk2

i=1 log(1 + exp(−ztiyti)))  it can be veriﬁed that Assumptions 2.1 and 2.2 hold.

f (·) = PT

given by h(z) = PT
ℓ(zt  yt) = Pp

2.2 Some Facts about the Optimal Solution Set

Since f (·) = h(A(·)) by (2) and h(·) is strictly convex on dom(h) by Assumption 2.1(b)  it is easy
to verify that the map X 7→ A(X) is invariant over the optimal solution set X . In other words  there
exists a ¯z ∈ dom(h) such that for any X ∗ ∈ X   we have A(X ∗) = ¯z. Thus  we can express X as

X = (cid:8)X ∈ Rm×n : τ kXk∗ = v∗ − h(¯z)  A(X) = ¯z(cid:9)  

where v∗ > −∞ is the optimal value of (1). In particular  X is a non–empty convex compact set.
This implies that every X ∈ Rm×n has a unique projection ¯X ∈ X onto X   which is given by the
solution to the following optimization problem:

dist(X  X ) = min
Y ∈X

kX − Y kF .

In addition  since X is bounded and F is convex  it follows from [14  Corollary 8.7.1] that the level
set {X ∈ Rm×n : F (X) ≤ ζ} is bounded for any ζ ∈ R.

2.3 Proximal Gradient Method and the Residual Map

To motivate the PGM for solving (1)  we recall an alternative characterization of the optimal solution
set X . Consider the proximity operator proxτ : Rm×n → Rm×n  which is deﬁned as

proxτ (X) = arg min

Z∈Rm×n(cid:26)τ kZk∗ +

1
2

kX − Zk2

F(cid:27) .

(3)

By comparing the optimality conditions for (1) and (3)  it is immediate that a solution X ∗ ∈ Rm×n
is optimal for (1) if and only if it satisﬁes the following ﬁxed–point equation:

This naturally lead to the following PGM for solving (1):

X ∗ = proxτ (X ∗ − ∇f (X ∗)).

(cid:26) Y k+1 = X k − αk∇f (X k) 
X k+1 = proxτ αk (Y k+1) 

(4)

(5)

where αk > 0 is the step size in the k–th iteration  for k = 0  1  . . .; see  e.g.  [8  19  11]. As is
well–known  the proximity operator deﬁned above can be expressed in terms of the so–called matrix

3

shrinkage operator. To describe this result  we introduce some deﬁnitions. Let µ > 0 be given. The
non–negative vector shrinkage operator sµ : Rp
+ is deﬁned as (sµ(z))i = max{0  zi − µ} 
where i = 1  . . .   p. The matrix shrinkage operator Sµ : Rm×n → Rm×n is deﬁned as Sµ(X) =
U ΣµV T   where X = U ΣV T is the singular value decomposition of X with Σ = Diag(σ(X)) and
σ(X) being the vector of singular values of X  and Σµ = Diag(sµ(σ(X))). Then  it can be shown
that

+ → Rp

proxτ (X) = Sτ (X);

(6)

see  e.g.  [11  Theorem 3].

Our goal in this paper is to study the convergence rate of the PGM (5). Towards that end  we need a
measure to quantify its progress towards optimality. One natural candidate would be dist(·  X )  the
distance to the optimal solution set X . Despite its intuitive appeal  such a measure is hard to compute
or analyze. In view of (4) and (6)  a reasonable alternative would be the norm of the residual map
R : Rm×n → Rm×n  which is deﬁned as

R(X) = Sτ (X − ∇f (X)) − X.

(7)
Intuitively  the residual map measures how much a solution X ∈ Rm×n violates the optimality
condition (4). In particular  X is an optimal solution to (1) if and only if R(X) = 0. However  since
kR(·)kF is only a surrogate of dist(·  X )  we need to establish a relationship between them. This
motivates the development of a so–called error bound for problem (1).

3 Main Results

Key to our convergence analysis of the PGM (5) is the following error bound for problem (1)  which
constitutes the main contribution of this paper:

Theorem 3.1 (Error Bound for Trace Norm Regularization) Suppose that in problem (1)  f is of
the form (2)  and Assumptions 2.1 and 2.2 are satisﬁed. Then  for any ζ ≥ v∗  there exist constants
η > 0 and ǫ > 0 such that

dist(X  X ) ≤ ηkR(X)kF whenever F (X) ≤ ζ  kR(X)kF ≤ ǫ.

(8)

Armed with Theorem 3.1 and some standard properties of the PGM (5)  we can apply the con-
vergence analysis framework developed by Luo and Tseng [10] to establish the linear conver-
gence of (5). Recall that a sequence of vectors {wk}k≥0 is said to converge Q–linearly (resp. R–
linearly) to a vector w∞ if there exist an index K ≥ 0 and a constant ρ ∈ (0  1) such that
kwk+1 − w∞k2/kwk − w∞k2 ≤ ρ for all k ≥ K (resp. if there exist constants γ > 0 and ρ ∈ (0  1)
such that kwk − w∞k2 ≤ γ · ρk for all k ≥ 0).

Theorem 3.2 (Linear Convergence of the Proximal Gradient Method) Suppose that in problem
(1)  f is of the form (2)  and Assumptions 2.1 and 2.2 are satisﬁed. Moreover  suppose that the step
size αk in the PGM (5) satisﬁes 0 < α < αk < ¯α < 1/Lf for k = 0  1  2  . . .  where Lf is the
Lipschitz constant of ∇f   and α  ¯α are given constants. Then  the sequence of solutions {X k}k≥0
generated by the PGM (5) converges R–linearly to an element in the optimal solution set X   and the
associated sequence of objective values {F (X k)}k≥0 converges Q–linearly to the optimal value v∗.

Proof. Under the given setting  it can be shown that there exist scalars κ1  κ2  κ3 > 0  which depend
on α  ¯α  and Lf   such that

F (X k) − F (X k+1) ≥ κ1kX k − X k+1k2
F  

F (X k+1) − v∗ ≤ κ2(cid:2)(dist(X k  X ))2 + kX k+1 − X kk2
F(cid:3)  

(9)

(10)

kR(X k)kF ≤ κ3kX k − X k+1kF ;

(11)
see the supplementary material. Since {F (X k)}k≥0 is a monotonically decreasing sequence by (9)
and F (X k) ≥ v∗ for all k ≥ 0  we conclude  again by (9)  that X k − X k+1 → 0. This  together
with (11)  implies that R(X k) → 0. Thus  by (9)  (10) and Theorem 3.1  there exist an index K ≥ 0
and a constant κ4 > 0 such that for all k ≥ K 

F (X k+1) − v∗ ≤ κ4kX k − X k+1k2

F ≤

κ4
κ1

(F (X k) − F (X k+1)).

4

It follows that

F (X k+1) − v∗ ≤

κ4

κ1 + κ4

(F (X k) − v∗) 

(12)

which establishes the Q–linear convergence of {F (X k)}k≥0 to v∗. Using (9) and (12)  we can
show that {kX k+1 − X kk2
F }k≥0 converges R–linearly to 0  which  together with (11)  implies that
{X k}k≥0 converges R–linearly to a point in X ; see the supplementary material.
(cid:3)

4 Proof of the Error Bound

The structure of our proof of Theorem 3.1 largely follows that laid out in [22  Section 6]. However 
as explained in Section 1  some new ingredients are needed in order to analyze the spectral properties
of a point that is close to the optimal solution set X . Before we proceed  let us set up the notation
that will be used in the proof. Let L > 0 denote the Lipschitz constant of ∇h and ∂k · k∗ denote the
subdifferential of k · k∗. Given a sequence {X k}k≥0 ∈ Rm×n\X   deﬁne

Rk = R(X k) 

¯X k = arg minY ∈X kX k − Y kF  
zk = A(X k)  Gk = ∇f (X k) = A∗(∇h(zk)) 

(13)
where A∗ is the adjoint operator of A. The crux of the proof of Theorem 3.1 is the following lemma:

δk = kX k − ¯X kkF  
¯G = A∗(∇h(¯z)) 

Lemma 4.1 Under the setting of Theorem 3.1  suppose that there exists a convergent sequence
{X k}k≥0 ∈ Rm×n\X satisfying

F (X k) ≤ ζ for all k ≥ 0  Rk → 0 

Rk
δk

→ 0.

(14)

Then  the following hold:

(a) (Asymptotic Optimality) The limit point ¯X of {X k}k≥0 belongs to X .

(b) (Bounded Iterates) There exists a convex compact subset Z of dom(h) such that zk  ¯z ∈ Z

for all k ≥ 0. Consequently  there exists a constant σ ∈ (0  L] such that for all k ≥ 0 

(∇h(zk) − ∇h(¯z))T (zk − ¯z) ≥ σkzk − ¯zk2
2.

(15)

(c) (Restricted Invertibility) There exists a constant κ > 0 such that
kX k − ¯X kkF ≤ κkzk − ¯zk2 = κkA(X k − ¯X k)k2

for all k ≥ 0.

(16)

It is clear that kA(X k − ¯X k)k2 ≤ kAk · kX k − ¯X kkF   where kAk = supkY kF =1 kA(Y )k2 is
the spectral norm of A. Thus  the key element in Lemma 4.1 is the restricted invertibility property
(16). For the sake of continuity  let us proceed to prove Theorem 3.1 by assuming the validity of
Lemma 4.1.

Proof. [Theorem 3.1] We argue by contradiction. Suppose that there exists ζ ≥ v∗ such that (8) fails
to hold for all η > 0 and ǫ > 0. Then  there exists a sequence {X k}k≥0 ∈ Rm×n \X satisfying (14).
Since {X ∈ Rm×n : F (X) ≤ ζ} is bounded (see Section 2.2)  by passing to a subsequence if
necessary  we may assume that {X k}k≥0 converges to some ¯X. Hence  the premises of Lemma 4.1
are satisﬁed. Now  by Fermat’s rule [15  Theorem 10.1]  for each k ≥ 0 

Hence  we have

Rk ∈ arg min

D (cid:8)hGk + Rk  Di + τ kX k + Dk∗(cid:9) .

(17)

hGk + Rk  Rki + τ kX k + Rkk∗ ≤ hGk + Rk  ¯X k − X ki + τ k ¯X kk∗.
Since ¯X k ∈ X and ∇f ( ¯X k) = ¯G  we also have − ¯G ∈ τ ∂k ¯X kk∗  which implies that

τ k ¯X kk∗ ≤ h ¯G  X k + Rk − ¯X ki + τ kX k + Rkk∗.

Adding the two inequalities above and simplifying yield

hGk − ¯G  X k − ¯X ki + kRkk2

F ≤ h ¯G − Gk  Rki + hRk  ¯X k − X ki.

(18)

5

Since zk = A(X k) and ¯z = A( ¯X k)  by Lemma 4.1(b c) 

hGk − ¯G  X k − ¯X ki = (∇h(zk) − ∇h(¯z))T (zk − ¯z) ≥ σkzk − ¯zk2

2 ≥

σ
κ2 kX k − ¯X kk2
F .

(19)

Hence  it follows from (15)  (18)  (19) and the Lipschitz continuity of ∇h that

σ
κ2 kX k − ¯X kk2

F + kRkk2

F ≤ (∇h(¯z) − ∇h(zk))T A(Rk) + hRk  ¯X k − X ki

≤ LkAk2kX k − ¯X kkF kRkkF + kX k − ¯X kkF kRkkF .

In particular  this implies that

σ
κ2 kX k − ¯X kk2

F ≤ (LkAk2 + 1)kX k − ¯X kkF kRkkF

for all k ≥ 0  which  upon dividing both sides by kX k − ¯X kkF   yields a contradiction to (14). (cid:3)

4.1 Proof of Lemma 4.1

We now return to the proof of Lemma 4.1. Since Rk → 0 by (14) and R is continuous  we have
R( ¯X) = 0  which implies that ¯X ∈ X . This establishes (a). To prove (b)  observe that due to (a)  the
sequence {X k}k≥0 is bounded. Hence  the sequence {A(X k)}k≥0 is also bounded  which implies
that the points zk = A(X k) and ¯z = A( ¯X) lie in a convex compact subset Z of dom(h) for all
k ≥ 0. The inequality (15) then follows from Assumption 2.1(b). Note that we have σ ≤ L  as ∇h
is Lipschitz continuous with parameter L.

To prove (c)  we argue by contradiction. Suppose that (16) is false. Then  by further passing to a
subsequence if necessary  we may assume that

(20)
In the sequel  we will also assume without loss of generality that m ≤ n. The following proposition
establishes a property of the optimal solution set X that will play a crucial role in our proof.

kA(X k) − ¯zk2(cid:14)kX k − ¯X kkF → 0.

Proposition 4.2 Consider a ﬁxed ¯X ∈ X . Let ¯X − ¯G = ¯U [Diag(¯σ) 0] ¯V T be the singular
value decomposition of ¯X − ¯G  where ¯U ∈ Rm×m  ¯V ∈ Rn×n are orthogonal matrices and ¯σ
is the vector of singular values of ¯X − ¯G. Then  the matrices ¯X and − ¯G can be simultaneously
singular–value–decomposed by ¯U and ¯V . Moreover  the set Xc ⊂ X   which is deﬁned as

is a non–empty convex compact set.

Xc = (cid:8)X ∈ X : X = ¯U [Diag(σ(X)) 0] ¯V T(cid:9)  

By Proposition 4.2  for every k ≥ 0  the point X k has a unique projection ˜X k ∈ Xc onto Xc. Let

γk = kX k − ˜X kkF = min
Y ∈Xc

kX k − Y kF .

(21)

Since Xc ⊂ X   we have γk = kX k − ˜X kkF ≥ kX k − ¯X kkF = δk. It follows from (20) that
kA(X k) − ¯zk2(cid:14)kX k − ˜X kkF → 0. This is equivalent to A(Qk) → 0  where

Qk =

for all k ≥ 0.

(22)

X k − ˜X k

γk

In particular  we have kQkkF = 1 for all k ≥ 0. By further passing to a subsequence if necessary 
we will assume that {Qk}k≥0 converges to some ¯Q. Clearly  we have A( ¯Q) = 0 and k ¯QkF = 1.

4.1.1 Decomposing ¯Q

Our goal now is to show that for k sufﬁciently large and ǫ > 0 sufﬁciently small  the point ˆX =
˜X k + ǫ ¯Q belongs to Xc and is closer to X k than ˜X k is to X k. This would then contradict the
fact that ˜X k is the projection of X k onto Xc. To begin  let σk be the vector of singular values of
X k − Gk. Since X k − Gk → ¯X − ¯G  the sequence {σk}k≥0 is bounded. Hence  for i = 1  . . .   m 
by passing to a subsequence if necessary  we can classify the sequence {σk
i }k≥0 into one of the
i > τ and σi( ˜X k) > 0 for all k ≥ 0; (C)
following three cases: (A) σk
i > τ and σi( ˜X k) = 0 for all k ≥ 0. The following proposition gives the key structural properties
σk
of ¯Q that will lead to the desired contradiction:

i ≤ τ for all k ≥ 0; (B) σk

6

Proposition 4.3 The matrix ¯Q admits the decomposition ¯Q = ¯U [Diag(λ) 0] ¯V T   where

= − lim
k→∞

σi( ˜X k)

γk

≤ 0

in Case (A) 

∈ R

> 0

in Case (B) 

in Case (C) 

for i = 1  . . .   m.

λi




It should be noted that the decomposition given in Proposition 4.3 is not necessarily the singular
value decomposition of ¯Q  as λ could have negative components. A proof of Proposition 4.3 can be
found in the supplementary material.

4.1.2 Completing the Proof

Armed with Proposition 4.3  we are now ready to complete the proof of Lemma 4.1(c). Since
Qk 6= 0 for all k ≥ 0  it follows from (22) that hX k − ˜X k  ¯Qi > 0 for all k sufﬁciently large. Fix
any such k and let ˆX = ˜X k + ǫ ¯Q  where ǫ > 0 is a parameter to be determined. Since A( ¯Q) = 0 
it follows from (13) that ∇f ( ˆX) = ∇f ( ˜X k) = ¯G. Moreover  since ˜X k ∈ Xc  by the optimality
condition (4) and Proposition 4.2  we have

maxn0  σi( ˜X k) + σi(− ¯G) − τo = σi( ˜X k)

Now  we claim that for ǫ > 0 sufﬁciently small  ˆX satisﬁes

for i = 1  . . .   m.

Sτ ( ˆX − ¯G)¯vi = ˆX ¯vi
i Sτ ( ˆX − ¯G) = ¯uT
ˆX
¯uT

i

for i = 1  . . .   n 

for i = 1  . . .   m 

(23)

(24)

where ¯ui (resp. ¯vi) is the i–th column of ¯U (resp. ¯V ). This would then imply that ˆX ∈ Xc. To prove
the claim  observe that for i = m + 1  . . .   n  both sides of (24) are equal to 0. Moreover  since
˜X k ∈ Xc  Propositions 4.2 and 4.3 give

Thus  it sufﬁces to show that for ǫ > 0 sufﬁciently small 

ˆX − ¯G = ¯U (cid:2)Diag(σ( ˜X k) + ǫλ + σ(− ¯G)) 0(cid:3) ¯V T .

σi( ˜X k) + ǫλi + σi(− ¯G) ≥ 0

sτ (σi( ˜X k) + ǫλi + σi(− ¯G)) = σi( ˜X k) + ǫλi

for i = 1  . . .   m 

for i = 1  . . .   m.

(25)

(26)

Towards that end  ﬁx an index i = 1  . . .   m and consider the three cases deﬁned in Section 4.1.1:
Case (A). If σi( ˜X k) = 0 for all k sufﬁciently large  then Proposition 4.3 gives λi = 0. Moreover 
we have σi(− ¯G) ≤ τ by (23). This implies that both (25) and (26) are satisﬁed for any choice of
ǫ > 0. On the other hand  if σi( ˜X k) > 0 for all k sufﬁciently large  then Proposition 4.3 gives
λi < 0. Moreover  we have σi(− ¯G) = τ by (23). By choosing ǫ > 0 so that σi( ˜X k) + ǫλi ≥ 0  we
can guarantee that both (25) and (26) are satisﬁed.
Case (B). Since σi( ˜X k) > 0 for all k ≥ 0  we have σi(− ¯G) = τ by (23). Hence  both (25) and (26)
can be satisﬁed by choosing ǫ > 0 so that σi( ˜X k) + ǫλi ≥ 0.
Case (C). By Proposition 4.2  we have ¯X ∈ Xc. Since X k → ¯X and γk = kX k − ˜X kkF ≤
kX k − ¯XkF   we have ˜X k → ¯X as well. It follows that σi( ¯X) = 0  as σi( ˜X k) = 0 for all k ≥ 0 by
assumption. Now  since X k − Gk → ¯X − ¯G and σk
i > τ   we have ¯σi ≥ τ . Thus  Proposition 4.2
implies that τ ≤ ¯σi = σi( ¯X − ¯G) = σi( ¯X) + σi(− ¯G) = σi(− ¯G). This  together with (23)  yields
σi(− ¯G) = τ . Since λi > 0 by Proposition 4.3  we conclude that both (25) and (26) can be satisﬁed
by any choice of ǫ > 0.
Thus  in all three cases  the claim is established. In particular  we have ˆX ∈ Xc. This  together with
hX k − ˜X k  ¯Qi > 0 and k ¯QkF = 1  yields
F − 2ǫhX k − ˜X k  ¯Qi + ǫ2 < kX k − ˜X kk2
kX k − ˆXk2
for ǫ > 0 sufﬁciently small  which contradicts the fact that ˜X k is the projection of X k onto Xc. This
completes the proof of Lemma 4.1(c).

F = kX k − ˜X k − ǫ ¯Qk2

F = kX k − ˜X kk2

F

7

5 Numerical Experiments

In this section  we complement our theoretical results by testing the numerical performance of the
PGM (5) on two problems: matrix completion and matrix classiﬁcation.

Matrix Completion: We randomly generate an n × n matrix M with a prescribed rank r. Then 
we ﬁx a sampling ratio θ ∈ (0  1] and sample p = ⌊θn2⌋ entries of M uniformly at random. This
induces a sampling operator P : Rm×n → Rp and an observation vector b ∈ Rp. In our experiments 
we ﬁx the rank r = 3 and use the square loss f (·) = kP(·) − bk2
2/2 with regularization parameter
µ = 1 in problem (1). We then solve the resulting problem for different values of n and θ using the
PGM (5) with a ﬁxed step size α = 1. We stop the algorithm when F (X k) − F (X k+1) < 10−8.
Figure 1 shows the semi–log plots of the error in objective value and the error in solution against the
number of iterations. It can be seen that as long as the iterates are close enough to the optimal set 
both the objective values and the solutions converge linearly.

j

l

)
e
u
a
v
 
e
v
i
t
c
e
b
o
 
f
o
 
r
o
r
r
e
(
g
o
L

Convergence Performance of Objective Value

 

θ=0.1
θ=0.3
θ=0.5

105

100

10−5

10−10
 
0

200

400
600
Iterations

800

1000

j

l

)
e
u
a
v
 
e
v
i
t
c
e
b
o
 
f
o
 
r
o
r
r
e
(
g
o
L

105

100

10−5

 
f

o

 
r
o
r
r

E
(
g
o
L

250

300

10−10
 
0

50

100

150

200

Iterations

10−10
 
0

200

400

600

800

Iterations

Convergence Performance of Objective Value

Convergence Performance of Objective Value

 

n=100
n=500
n=1000

l

)
e
u
a
V
 
e
v
i
t
c
e
b
O

j

105

100

10−5

 

θ=0.2
θ=0.5
θ=0.8

1000

1200

n = 1000

θ = 0.3

n = 40

l

)
n
o
i
t
u
o
s
 
f
o
 
r
o
r
r
e
(
g
o
L

104

102

100

10−2

10−4
 
0

Convergence Performance of Solution 

 

θ=0.1
θ=0.3
θ=0.5

200

400
600
Iterations

800

1000

)
n
o

i
t

l

u
o
s
 
f

o

 
r
o
r
r
e
(
g
o
L

104

102

100

10−2

10−4
 
0

Convergence Performance of Solution 

 

n=100
n=500
n=1000

Convergence Performance of Solution 

 

θ=0.2
θ=0.5
θ=0.8

101

100

10−1

10−2

10−3

)
n
o

i
t

l

u
o
S

 
f

o

 
r
o
r
r

E
(
g
o
L

50

100

150

200

Iterations

250

300

10−4
 
0

200

400

600

800

1000

1200

Iterations

n = 1000

θ = 0.3

n = 40

Figure 1: Matrix Completion

Figure 2: Matrix Classiﬁcation

Matrix Classiﬁcation: We consider a matrix classiﬁcation problem under the setting described
in [21]. Speciﬁcally  we ﬁrst randomly generate a low-rank matrix classiﬁer X ∗  which is an n × n
symmetric matrix of rank r. Then  we specify a sampling ratio θ ∈ (0  1] and sample p = ⌊θn2⌋/2
independent n × n symmetric matrices W1  . . .   Wp from the standard Wishart distribution with n
degrees of freedom. The label of Wi  denoted by yi  is given by sgn(hX ∗  Wii). In our experiments 
i=1 log(1 +
exp(−yih·  Wii)) with regularization parameter µ = 1 in problem (1). Since a good lower bound
on the Lipschitz constant Lf of ∇f is not readily available in this case  a backtracking line search
was adopted at each iteration to achieve an acceptable step size; see  e.g.  [3]. We stop the algorithm
when F (X k) − F (X k+1) < 10−6. Figure 2 shows the convergence performance of the PGM (5)
as θ varies. Again  it can be seen that both the objective values and the solutions converge linearly.

we ﬁx the rank r = 3  the dimension n = 40  and use the logistic loss f (·) = Pp

6 Conclusion

In this paper  we have established the linear convergence of the PGM for solving a class of trace
norm–regularized problems. Our convergence result does not require the objective function to be
strongly convex and is applicable to many settings in machine learning. The key technical tool in
the proof is a Lipschitzian error bound for trace norm–regularized problems  which could be of
independent interest. A future direction is to study error bounds for more general matrix norm–
regularized problems and their implications on the convergence rates of ﬁrst–order methods.

Acknowledgments The authors would like to thank the anonymous reviewers for their careful
reading of the manuscript and insightful comments. The research of A. M.–C. So is supported in
part by a gift grant from Microsoft Research Asia.

8

References

[1] Y. Amit  M. Fink  N. Srebro  and S. Ullman. Uncovering Shared Structures in Multiclass Classiﬁcation.

In Proc. 24th ICML  pages 17–24  2007.

[2] A. Argyriou  T. Evgeniou  and M. Pontil. Convex Multi–Task Feature Learning. Mach. Learn.  73(3):243–

272  2008.

[3] A. Beck and M. Teboulle. A Fast Iterative Shrinkage–Thresholding Algorithm for Linear Inverse Prob-

lems. SIAM J. Imaging Sci.  2(1):183–202  2009.

[4] A. Ben-Tal and A. Nemirovski. Lectures on Modern Convex Optimization: Analysis  Algorithms  and
Engineering Applications. MPS–SIAM Series on Optimization. Society for Industrial and Applied Math-
ematics  Philadelphia  Pennsylvania  2001.

[5] M. Fazel  H. Hindi  and S. P. Boyd. A Rank Minimization Heuristic with Application to Minimum Order

System Approximation. In Proc. 2001 ACC  pages 4734–4739  2001.

[6] D. Gross. Recovering Low–Rank Matrices from Few Coefﬁcients in Any Basis. IEEE Trans. Inf. Theory 

57(3):1548–1566  2011.

[7] S. Ji  K.-F. Sze  Z. Zhou  A. M.-C. So  and Y. Ye. Beyond Convex Relaxation: A Polynomial–Time
Non–Convex Optimization Approach to Network Localization. In Proc. 32nd IEEE INFOCOM  pages
2499–2507  2013.

[8] S. Ji and J. Ye. An Accelerated Gradient Method for Trace Norm Minimization. In Proc. 26th ICML 

pages 457–464  2009.

[9] V. Koltchinskii  K. Lounici  and A. B. Tsybakov. Nuclear–Norm Penalization and Optimal Rates for

Noisy Low–Rank Matrix Completion. Ann. Stat.  39(5):2302–2329  2011.

[10] Z.-Q. Luo and P. Tseng. Error Bounds and Convergence Analysis of Feasible Descent Methods: A

General Approach. Ann. Oper. Res.  46(1):157–178  1993.

[11] S. Ma  D. Goldfarb  and L. Chen. Fixed Point and Bregman Iterative Methods for Matrix Rank Mini-

mization. Math. Program.  128(1–2):321–353  2011.

[12] Yu. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Pub-

lishers  Boston  2004.

[13] B. Recht  M. Fazel  and P. A. Parrilo. Guaranteed Minimum–Rank Solutions of Linear Matrix Equations

via Nuclear Norm Minimization. SIAM Rev.  52(3):471–501  2010.

[14] R. T. Rockafellar. Convex Analysis. Princeton Landmarks in Mathematics and Physics. Princeton Univer-

sity Press  Princeton  New Jersey  1997.

[15] R. T. Rockafellar and R. J.-B. Wets. Variational Analysis  volume 317 of Grundlehren der mathematis-

chen Wissenschaften. Springer–Verlag  Berlin Heidelberg  second edition  2004.

[16] M. Schmidt  N. Le Roux  and F. Bach. Convergence Rates of Inexact Proximal–Gradient Methods for

Convex Optimization. In Proc. NIPS 2011  pages 1458–1466  2011.

[17] A. M.-C. So  Y. Ye  and J. Zhang. A Uniﬁed Theorem on SDP Rank Reduction. Math. Oper. Res. 

33(4):910–920  2008.

[18] W. So. Facial Structures of Schatten p–Norms. Linear and Multilinear Algebra  27(3):207–212  1990.

[19] K.-C. Toh and S. Yun. An Accelerated Proximal Gradient Algorithm for Nuclear Norm Regularized

Linear Least Squares Problems. Pac. J. Optim.  6(3):615–640  2010.

[20] R. Tomioka and K. Aihara. Classifying Matrices with a Spectral Regularization. In Proc. of the 24th

ICML  pages 895–902  2007.

[21] R. Tomioka  T. Suzuki  M. Sugiyama  and H. Kashima. A Fast Augmented Lagrangian Algorithm for

Learning Low–Rank Matrices. In Proc. 27th ICML  pages 1087–1094  2010.

[22] P. Tseng. Approximation Accuracy  Gradient Methods  and Error Bound for Structured Convex Opti-

mization. Math. Program.  125(2):263–295  2010.

[23] P. Tseng and S. Yun. A Coordinate Gradient Descent Method for Nonsmooth Separable Minimization.

Math. Program.  117(1–2):387–423  2009.

[24] M. White  Y. Yu  X. Zhang  and D. Schuurmans. Convex Multi–View Subspace Learning. In Proc. NIPS

2012  pages 1682–1690  2012.

[25] H. Zhang  J. Jiang  and Z.-Q. Luo. On the Linear Convergence of a Proximal Gradient Method for a Class

of Nonsmooth Convex Minimization Problems. J. Oper. Res. Soc. China  1(2):163–186  2013.

9

,Ke Hou
Zirui Zhou
Anthony Man-Cho So
Zhi-Quan Luo