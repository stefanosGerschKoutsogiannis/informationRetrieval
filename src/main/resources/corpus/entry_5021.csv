2013,Similarity Component Analysis,Measuring similarity is crucial to many learning tasks. It is also a richer and broader notion than what most metric learning algorithms can model. For example  similarity can arise from the process of aggregating the decisions of multiple latent components  where each latent component compares data in its own way by focusing on a different subset of features. In this paper  we propose Similarity Component Analysis (SCA)  a probabilistic graphical model that discovers those latent components from data. In SCA  a latent component generates a local similarity value  computed with its own metric  independently of other components. The final similarity measure is then obtained by combining the local similarity values with a (noisy-)OR gate. We derive an EM-based algorithm for fitting the model parameters with similarity-annotated data from pairwise comparisons. We validate the SCA model on synthetic datasets where SCA discovers the ground-truth about the latent components. We also apply SCA to a multiway classification task and a link prediction task. For both tasks  SCA attains significantly better prediction accuracies than competing methods. Moreover  we show how SCA can be instrumental in exploratory analysis of data  where we gain insights about the data by examining patterns hidden in its latent components' local similarity values.,Similarity Component Analysis

Soravit Changpinyo∗
Dept. of Computer Science
U. of Southern California
Los Angeles  CA 90089
schangpi@usc.edu

Kuan Liu∗

Dept. of Computer Science
U. of Southern California
Los Angeles  CA 90089

kuanl@usc.edu

Fei Sha

Dept. of Computer Science
U. of Southern California
Los Angeles  CA 90089

feisha@usc.edu

Abstract

Measuring similarity is crucial to many learning tasks. To this end  metric learning
has been the dominant paradigm. However  similarity is a richer and broader no-
tion than what metrics entail. For example  similarity can arise from the process of
aggregating the decisions of multiple latent components  where each latent com-
ponent compares data in its own way by focusing on a different subset of features.
In this paper  we propose Similarity Component Analysis (SCA)  a probabilistic
graphical model that discovers those latent components from data. In SCA  a la-
tent component generates a local similarity value  computed with its own metric 
independently of other components. The ﬁnal similarity measure is then obtained
by combining the local similarity values with a (noisy-)OR gate. We derive an
EM-based algorithm for ﬁtting the model parameters with similarity-annotated
data from pairwise comparisons. We validate the SCA model on synthetic datasets
where SCA discovers the ground-truth about the latent components. We also ap-
ply SCA to a multiway classiﬁcation task and a link prediction task. For both
tasks  SCA attains signiﬁcantly better prediction accuracies than competing meth-
ods. Moreover  we show how SCA can be instrumental in exploratory analysis of
data  where we gain insights about the data by examining patterns hidden in its
latent components’ local similarity values.

1

Introduction

Learning how to measure similarity (or dissimilarity) is a fundamental problem in machine learning.
Arguably  if we have the right measure  we would be able to achieve a perfect classiﬁcation or
clustering of data.
If we parameterize the desired dissimilarity measure in the form of a metric
function  the resulting learning problem is often referred to as metric learning. In the last few years 
researchers have invented a plethora of such algorithms [18  5  11  13  17  9]. Those algorithms have
been successfully applied to a wide range of application domains.
However  the notion of (dis)similarity is much richer than what metric is able to capture. Consider
the classical example of CENTAUR  MAN and HORSE. MAN is similar to CENTAUR and CENTAUR
is similar to HORSE. Metric learning algorithms that model the two similarities well would need to
assign small distances among those two pairs. On the other hand  the algorithms will also need to
strenuously battle against assigning a small distance between MAN and HORSE due to the triangle in-
equality  so as to avoid the fallacy that MAN is similar to HORSE too! This example (and others [12])
thus illustrates the important properties  such as non-transitiveness and non-triangular inequality  of
(dis)similarity that metric learning has not adequately addressed.
Representing objects as points in high-dimensional feature spaces  most metric learning learning al-
gorithms assume that the same set of features contribute indistinguishably to assessing similarity. In

∗Equal contributions

1

Figure 1: Similarity Component Analysis and its application to the example of CENTAUR  MAN and HORSE.
SCA has K latent components which give rise to local similarity values sk conditioned on a pair of data xm
and xn. The model’s output s is a combination of all local values through an OR model (straightforward to
extend to a noisy-OR model). Θk is the parameter vector for p(sk|xm  xn). See texts for details.

particular  the popular Mahalanobis metric weights each feature (and their interactions) additively
when calculating distances. In contrast  similarity can arise from a complex aggregation of com-
paring data instances on multiple subsets of features  to which we refer as latent components. For
instance  there are multiple reasons for us to rate two songs being similar: being written by the same
composers  being performed by the same band  or of the same genre. For an arbitrary pair of songs 
we can rate the similarity between them based on one of the many components or an arbitrary sub-
set of components  while ignoring the rest. Note that  in the learning setting  we observe only the
aggregated results of those comparisons — which components are used is latent.
Multi-component based similarity exists also in other types of data. Consider a social network where
the network structure (i.e.  links) is a supposition of multiple networks where people are connected
for various organizational reasons: school  profession  or hobby. It is thus unrealistic to assume that
the links exist due to a single cause. More appropriately  social networks are “multiplex” [6  15].
In this paper  we propose Similarity Component Analysis (SCA) to model the richer similarity re-
lationships beyond what current metric learning algorithms can offer. SCA is a Bayesian network 
illustrated in Fig. 1. The similarity (node s) is modeled as a probabilistic combination of multiple
latent components. Each latent component (sk) assigns a local similarity value to whether or not two
objects are similar  inferring from only a subset (but unknown) of features. The (local) similarity
values of those latent components are aggregated with a (noisy-) OR model. Intuitively  two objects
are likely to be similar if they are considered to be similar by at least one component. Two objects
are likely to be dissimilar if none of the components voices up.
We derive an EM-based algorithm for ﬁtting the model with data annotated with similarity relation-
ships. The algorithm infers the intermediate similarity values of latent components and identiﬁes the
parameters for the (noisy-)OR model  as well as each latent component’s conditional distribution 
by maximizing the likelihood of the training data.
We validate SCA on several learning tasks. On synthetic data where ground-truth is available  we
conﬁrm SCA’s ability in discovering latent components and their corresponding subsets of features.
On a multiway classiﬁcation task  we contrast SCA to state-of-the-art metric learning algorithms
and demonstrate SCA’s superior performance in classifying data samples. Finally  we use SCA to
model the network link structures among research articles published at NIPS proceedings. We show
that SCA achieves the best link prediction accuracy among competitive algorithms. We also conduct
extensive analysis on how learned latent components effectively represent link structures.
In section 2  we describe the SCA model and inference and learning algorithms. We report our
empirical ﬁndings in section 3. We discuss related work in section 4 and conclude in section 5.

2 Approach

We start by describing in detail Similarity Component Analysis (SCA)  a Bayesian network for
modeling similarity between two objects. We then describe the inference procedure and learning
algorithm for ﬁtting the model parameters with similarity-annotated data.

2

kQmxnxkSSNN·KK12S32S31S91.0)1(==sp9.01=p1S2S1x2x2x3x1x3x1S2S1S2S91.0)1(==sp19.0)1(==sp1.02=p1.01=p9.02=p1.01=p1.02=p2.1 Probabilistic model of similarity
In what follows  let (u  v  s) denote a pair of D-dimensional data points u  v ∈ RD and their as-
sociated value of similarity s ∈ {DISSIMILAR  SIMILAR} or {0  1} accordingly. We are interested
in modeling the process of assigning s to these two data points. To this end  we propose Similarity
Component Analysis (SCA) to model the conditional distribution p(s|u  v)  illustrated in Fig. 1.
In SCA  we assume that p(s|u  v) is a mixture of multiple latent components’s local similarity
values. Each latent component evaluates its similarity value independently  using only a subset
of the D features.
Intuitively  there are multiple reasons of annotating whether or not two data
instances are similar and each reason focuses locally on one aspect of the data  by restricting itself
to examining only a different subset of features.
Latent components Formally  let u[k] denote the subset of features from u corresponding to the
k-th latent component where [k] ⊂ {1  2  . . .   D}. The similarity assessment sk of this component
alone is determined by the distance between u[k] and v[k]

dk = (u − v)TMk(u − v)

(1)
where Mk (cid:23) 0 is a D × D positive semideﬁnite matrix  used to measure the distance more ﬂexibly
than the standard Euclidean metric. We restrict Mk to be sparse  in particular  only the correspond-
ing [k]-th rows and columns are non-zeroes. Note that in principle [k] needs to be inferred from
data  which is generally hard. Nonetheless  we have found that empirically even without explicitly
constraining Mk  we often obtain a sparse solution.
The distance dk is transformed to the probability for the Bernoulli variable sk according to

P (sk = 1|u  v) = (1 + e−bk )[1 − σ(dk − bk)]

(2)
where σ(·) is the sigmoid function σ(t) = (1 + e−t)−1 and bk is a bias term. Intuitively  when
the (biased) distance (dk − bk) is large  sk is less probable to be 1 and the two data points are
regarded less similar. Note that the constraint Mk being positive semideﬁnite is important as this
will constrain the probability to be bounded above by 1.
Combining local similarities Assume that there are K latent components. How can we combine
all the local similarity assessments? In this work  we use an OR-gate. Namely 

P (s = 1|s1  s2 ···   sK) = 1 − K(cid:89)

k=1

I[sk = 0]

(3)

Thus  the two data points are similar (s = 1) if at least one of the aspects deems so  corresponding
to sk = 1 for a particular k. The OR-model can be extended to the noisy-OR model [14]. To this
end  we model the non-deterministic effect of each component on the ﬁnal similarity value 

P (s = 1|sk = 1) = τk = 1 − θk  P (s = 1|sk = 0) = 0

(4)
In essence  the uncertainty comes from our probability of failure θk (false negative) to identify
the similarity if we are only allowed to consider one component at a time.
If we can consider
all components at the same time  this failure probability would be reduced. The noisy-OR model
captures precisely this notion:

P (s = 1|s1  s2 ···   sK) = 1 − K(cid:89)

k=1

I[sk=1]
k

θ

(5)

where the more sk = 1  the less the false-negative rate is after combination. Note that the noisy-OR
model reduces to the OR-model eq. (3) when θk = 0 for all k.
Similarity model Our desired model for the conditional probability p(s|u  v) is obtained by
marginalizing all possible conﬁgurations of the latent components s = {s1  s2 ···   sK}
P (sk|u  v)

P (s = 0|u  v) =

P (sk|u  v) =

(cid:88)

(cid:89)

(cid:89)

θ

I[sk=1]
k

P (s = 0|s)

(cid:89)
conditional probability simpliﬁes to P (s = 0|u  v) =(cid:81)

[θkpk + 1 − pk] =

=

k

k

k

where pk = p(sk = 1|u  v) is a shorthand for eq. (2). Note that despite the exponential number of
conﬁgurations for s  the marginalized probability is tractable. For the OR-model where θk = 0  the

k

(6)

(cid:88)
(cid:89)

s

s

[1 − τkpk]

k[1 − pk].

3

Inference and learning

2.2
Given an annotated training dataset D = {(xm  xn  smn)}  we learn the parameters  which include
all the positive semideﬁnite matrices Mk  the biases bk and the false negative rates θk (if noisy-OR
is used)  by maximizing the likelihood of D. Note that we will assume that K is known throughout
this work. We develop an EM-style algorithm to ﬁnd the local optimum of the likelihood.
Posterior The posteriors over the hidden variables are computationally tractable:

qk = P (sk = 1|u  v  s = 0) =

pkθk

rk = P (sk = 1|u  v  s = 1) =

pk

(cid:81)
l(cid:54)=k [1 − τlpl]
(cid:17)
(cid:81)
l(cid:54)=k [1 − τlpl]

P (s = 0|u  v)
1 − θk

(cid:16)

P (s = 1|u  v)

(7)

For OR-model eq. (3)  these posteriors can be further simpliﬁed as all θk = 0.
Note that  these posteriors are sufﬁcient to learn the parameters Mk and bk. To learn the parameters
θk  however  we need to compute the expected likelihood with respect to the posterior P (s|u  v  s).
While this posterior is tractable  the expectation of the likelihood is not and variational inference is
needed [10]. We omit the derivation for brevity. In what follows  we focus on learning Mk and bk.
For the k-th component  the relevant terms in the expected log-likelihood  given the posteriors  from
a single similarity assessment s on (u  v)  is

Jk = q1−s

k

k log P (sk = 1|u  v) + (1 − q1−s
rs

k) log(1 − P (sk = 1|u  v))
rs

k

(8)

Learning the parameters Note that Jk is not jointly convex in bk and Mk. Thus  we optimize them
alternatively. Concretely  ﬁxing Mk  we grid search and optimize over bk. Fixing bk  maximizing
Jk with respect to Mk is convex optimization as Jk is a concave function in Mk given the linear
dependency of the distance eq. (1) on this parameter.
We use the method of projected gradient ascent. Essentially  we take a gradient ascent step to update
Mk iteratively. If the update violates the positive semideﬁnite constraint  we project back to the
feasible region by setting all negative eigenvalues of Mk to zeroes. Alternatively  we have found that
reparameterizing Jk in the following form Mk = LT
kLk is more computationally advantageous  as
Lk is unconstrained. We use L-BFGS to optimize with respect to Lk and obtain faster convergence
and better objective function values. (While this procedure only guarantees local optima  we observe
no signiﬁcant detrimental effect of arriving at those solutions.) We give the exact form of gradients
with respect to Mk and Lk in the Suppl. Material.

2.3 Extensions

Variants to local similarity models The choice of using logistic-like functions eq. (2) for modeling
local similarity of the latent components is orthogonal to how those similarities are combined in
eq. (3) or eq. (5). Thus  it is relatively straightforward to replace eq. (2) with a more suitable one.
For instance  in some of our empirical studies  we have constrained Mk to be a diagonal matrix
with nonnegative diagonal elements. This is especially useful when the feature dimensionality is
extremely high. We view this ﬂexibility as a modeling advantage.
Disjoint components We could also explicitly express our desiderata that latent components focus
on non-overlapping features. To this end  we penalize the likelihood of the data with the following
regularizer to promote disjoint components
R({Mk}) =

diag(Mk)Tdiag(Mk(cid:48))

(cid:88)

(9)

where diag(·) extracts the diagonal elements of the matrix. As the metrics are constrained to be pos-
itive semideﬁnite  the inner product attains its minimum of zero when the diagonal elements  which
are nonnegative  are orthogonal to each other. This will introduce zero elements on the diagonals
of the metrics  which will in turn deselect the corresponding feature dimensions  because the corre-
sponding rows and columns of those elements are necessarily zero due to the positive semideﬁnite
constraints. Thus  metrics that have orthogonal diagonal vectors will use non-overlapping subsets
of features.

k k(cid:48)

4

(a) Disjoint ground-truth metrics

(b) Overlapping ground-truth metrics

Figure 2: On synthetic datasets  SCA successfully identiﬁes the sparse structures and
(non)overlapping patterns of ground-truth metrics. See texts for details. Best viewed in color.

3 Experimental results

We validate the effectiveness of SCA in modeling similarity relationships on three tasks. In sec-
tion 3.1  we apply SCA to synthetic datasets where the ground-truth is available to conﬁrm SCA’s
ability in identifying correctly underlying parameters. In section 3.2  we apply SCA to a multiway
classiﬁcation task to recognize images of handwritten digits where similarity is equated to having
the same class label. SCA attains superior classiﬁcation accuracy to state-of-the-art metric learning
algorithms. In section 3.3  we apply SCA to a link prediction problem for a network of scientiﬁc
articles. On this task  SCA outperforms competing methods signiﬁcantly  too.
Our baseline algorithms for modeling similarity are information-theoretic metric learning (ITML) [5]
and large margin nearest neighbor (LMNN) [18]. Both methods are discriminative approaches where
a metric is optimized to reduce the distances between data points from the same label class (or similar
data instances) and increase the distances between data points from different classes (or dissimilar
data instances). When possible  we also contrast to multiple metric LMNN (MM-LMNN) [18]  a
variant to LMNN where multiple metrics are learned from data.

3.1 Synthetic data

Data We generate a synthetic dataset according to the graphical model in Fig. 1. Speciﬁcally  our
feature dimensionality is D = 30 and the number of latent components is K = 5. For each com-
ponent k  the corresponding metric Mk is a D × D sparse positive semideﬁnite matrix where only
elements in a 6 × 6 matrix block on the diagonal are nonzero. Moreover  for different k  these block
matrices do not overlap in rows and columns indices. In short  these metrics mimic the setup where
each component focuses on its own 1/K-th of total features that are disjoint from each other. The
ﬁrst row of Fig. 2(a) illustrates these 5 matrices while the black background color indicates zero ele-
ments. The values of nonzero elements are randomly generated as long as they maintain the positive
semideﬁniteness of the metrics. We set the bias term bk to zeroes for all components. We sample
N = 500 data points randomly from RD. We select a random pair and compute their similarity
according to eq. (6) and threshold at 0.5 to yield a binary label s ∈ {0  1}. We select randomly
74850 pairs for training  24950 for development  24950 for testing.
Method We use the OR-model eq. (3) to combine latent components. We evaluate the results of
SCA on two aspects: how well we can recover the ground-truth metrics (and biases) and how well
we can use the parameters to predict similarities on the test set.
Results The second row of Fig. 2(a) contrasts the learned metrics to the ground-truth (the ﬁrst row).
Clearly  these two sets of metrics have almost identical shapes and sparse structures. Note that for
this experiment  we did not use the disjoint regularizer (described in section 2.3) to promote sparsity
and disjointness in the learned metrics. Yet  the SCA model is still able to identify those structures.
For the biases  SCA identiﬁes them as being close to zero (details are omitted for brevity).

5

1020305101520253010203051015202530true metrics1020305101520253010203051015202530102030510152025301020305101520253010203051015202530recovered metrics1020305101520253010203051015202530102030510152025301020305101520253010203051015202530true metrics1020305101520253010203051015202530102030510152025301020305101520253010203051015202530recovered metrics102030510152025301020305101520253010203051015202530Table 1: Similarity prediction accuracies and standard errors (%) on the synthetic dataset

BASELINES

SCA

ITML
72.7±0.0

K = 1
72.8±0.0

K = 10
LMNN
71.3±0.2
91.8±0.1
Table 2: Misclassiﬁcation rates (%) on the MNIST recognition task

K = 3
82.1±0.1

K = 5
91.5±0.1

K = 7
91.7±0.1

K = 20
90.2±0.4

BASELINES

D
25
50
100

EUC.
21.6
18.7
18.1

ITML
15.1
13.35
11.85

LMNN MM-LMNN
20.6
16.5
13.4

20.2
13.6
9.9

K = 1
17.7 ± 0.9
13.8 ± 0.3
12.1 ± 0.1

SCA
K = 5
16.0 ± 1.5
12.0 ± 1.1
10.8 ± 0.6

K = 10
14.5 ± 0.6
11.4 ± 0.6
11.1 ± 0.3

Table 1 contrasts the prediction accuracies by SCA to competing methods. Note that ITML  LMNN
and SCA with K = 1 perform similarly. However  when the number of latent components increases 
SCA outperforms other approaches by a large margin. Also note that when the number of latent
components exceeds the ground-truth K = 5  SCA reaches a plateau until overﬁtting.
In real-world data  “true metrics” may overlap  that is  it is possible that different components of
similarity rely on overlapping set of features. To examine SCA’s effectiveness in this scenario 
we create another synthetic data where true metrics heavily overlap  illustrated in the ﬁrst row of
Fig. 2(b). Nonetheless  SCA is able to identify the metrics correctly  as seen in the second row.

3.2 Multiway classiﬁcation

For this task  we use the MNIST dataset  which consists of 10 classes of hand-written digit images.
We use PCA to reduce the original dimension from 784 to D = 25  50 and 100  respectively. We
use 4200 examples for training  1800 for development and 2000 for testing.
The data is in the format of (xn  yn) where yn is the class label. We convert them into the format
(xm  xn  smn) that SCA expects. Speciﬁcally  for every training data point  we select its 15 nearest
neighbors among samples in the same class and formulate 15 similar relationships. For dissimilar
relationships  we select its 80 nearest neighbors among samples from the rest classes. For testing 
the label y of x is determined by

(cid:88)

x(cid:48)∈Bc(x)

y = arg maxc sc = arg maxc

P (s = 1|x  x(cid:48))

(10)

where sc is the similarity score to the c-th class  computed as the sum of 5 largest similarity values
Bc to samples in that class. In Table 2  we show classiﬁcation error rates for different values of D.
For K > 1  SCA clearly outperforms single-metric based baselines. In addition  SCA performs well
compared to MM-LMNN  achieving far better accuracy for small D.

3.3 Link prediction

We evaluate SCA on the task of link prediction in a “social” network of scientiﬁc articles. We aim to
demonstrate SCA’s power to model similarity/dissimilarity in “multiplex” real-world network data.
In particular  we are interested in not only link prediction accuracies  but also the insights about data
that we gain from analyzing the identiﬁed latent components.
Setup We use the NIPS 0-12 dataset [1] to construct the aforementioned network. The dataset
contains papers from the NIPS conferences between 1987 and 1999. The papers are organized into
9 sections (topics) (cf. Suppl. Material). We sample randomly 80 papers per section and use them
to construct the network. Each paper is a vertex and two papers are connected with an edge and
deemed as similar if both of them belong to the same section.
We experiment three representations for the papers: (1) Bag-of-words (BoW) uses normalized oc-
currences (frequencies) of words in the documents. As a preprocessing step  we remove “rare”
words that appear less than 75 times and appear more than 240. Those words are either too special-
ized (thus generalize poorly) or just functional words. After the removal  we obtain 1067 words. (2)
Topic (ToP) uses the documents’ topic vectors (mixture weights of topics) after ﬁtting the corpus

6

Table 3: Link prediction accuracies and their standard errors (%) on a network of scientiﬁc papers

BASELINES

SCA-DIAG

SCA

SVM

Feature
type
BoW 73.3±0.0
ToW 75.3±0.0
71.2±0.0
ToP

ITML

LMNN

-
-

-
-

81.1±0.1

80.7±0.1

K = 1
64.8 ± 0.1
67.0 ± 0.0
62.6 ± 0.0

K∗

87.0 ± 1.2
88.1 ± 1.4
81.0 ± 0.8

K = 1

-
-

81.0 ± 0.0

87.6 ± 1.0

K∗
-
-

to a 50-topic LDA [4]. (3) Topic-words (ToW) is essentially BoW except that we retain only 1036
frequent words used by the topics of the LDA model (top 40 words per topic).
Methods We compare the proposed SCA extensively to several competing methods for link pre-
diction. For BoW and ToW represented data  we compare SCA with diagonal metrics (SCA-DIAG 
cf. section 2.3) to Support Vector Machines (SVM) and logistic regression (LOGIT) to avoid high
computational costs associated with learning high-dimensional matrices (the feature dimensionality
D ≈ 1000). To apply SVM/LOGIT  we treat the link prediction as a binary classiﬁcation problem
where the input is the absolute difference in feature values between the two data points.
For 50-dimensional ToP represented data  we compare SCA (SCA) and SCA-DIAG to SVM/LOGIT 
information-theoretical metric learning (ITML)  and large margin nearest neighbor (LMNN).
Note that while LMNN was originally designed for nearest-neighbor based classiﬁcation  it can be
adapted to use similarity information to learn a global metric to compute the distance between any
pair of data points. We learn such a metric and threshold on the distance to render a decision on
whether two data points are similar or not (i.e.  whether there is a link between them). On the other
end  multiple-metric LMNN  while often having better classiﬁcation performance  cannot be used
for similarity and link prediction as it does not provide a principled way of computing distances
between two arbitrary data points when there are multiple (local) metrics.
Link or not? In Table 3  we report link prediction accuracies  which are averaged over several runs
of randomly generated 70/30 splits of the data. SVM and LOGIT perform nearly identically so we
report only SVM. For both SCA and SCA-DIAG  we report results when a single component is used
as well as when the optimal number of components are used (under columns K∗).
Both SCA-DIAG and SCA outperform the rest methods by a signiﬁcant margin  especially when the
number of latent components is greater than 1 (K∗ ranges from 3 to 13  depending on the methods
and the feature types). The only exception is SCA-DIAG with one component (K = 1)  which is an
overly restrictive model as the diagonal metrics constrain features to be combined additively. This
restriction is overcome by using a larger number of components.
Edge component analysis Why does learning latent components in SCA achieve superior link pre-
diction accuracies? The (noisy-)OR model used by SCA is naturally inclined to favoring “positive”
opinions — a pair of samples are regarded as being similar as long as there is one latent compo-
nent strongly believing so. This implies that a latent component can be tuned to a speciﬁc group of
samples if those samples rely on common feature characteristics to be similar.
Fig. 3(a) conﬁrms our intuition. The plot displays in relative strength —darker being stronger —
how much each latent component believes a pair of articles from the same section should be similar.
Concretely  after ﬁtting a 9-component SCA (from documents in ToP features)  we consider edges
connecting articles in the same section and compute the average local similarity values assigned by
each component. We observe two interesting sparse patterns: for each section  there is a dominant
latent component that strongly supports the fact that the articles from that section should be similar
(e.g.  for section 1  the dominant one is the 9-th component). Moreover  for each latent component 
it often strongly “voices up” for one section – the exception is the second component which seems
to support both section 3 and 4. Nonetheless  the general picture is that  each section has a signature
in terms of how similarity values are distributed across latent components.
This notion is further illustrated  with greater details  in Fig. 3(b). While Fig. 3(a) depicts averaged
signature for each section  the scatterplot displays 2D embeddings computed with the t-SNE algo-
rithm  on each individual edge’s signature — 9-dimensional similarity values inferred with the 9
latent components. The embeddings are very well organized in 9 clusters  colored with section IDs.

7

(a) Averaged component-
wise similarity values of
edges within each section

Embedding

(b)
of
links 
represented with
component-wise similarity
values

(c) Embedding of network
nodes (documents) 
repre-
sented in LDA’s topics

Figure 3: Edge component analysis. Representing network links with local similarity values reveals interest-
ing structures  such as nearly one-to-one correspondence between latent components and sections  as well as
clusters. However  representing articles in LDA’s topics does not reveal useful clustering structures such that
links can be inferred. See texts for details. Best viewed in color.

In contrast  embedding documents using their topic representations does not reveal clear cluster-
ing structures such that network links can be inferred. This is shown in Fig. 3(c) where each dot
corresponds to a document and the low-dimensional coordinates are computed using t-SNE (sym-
metrized KL divergence between topics is used as a distance measure). We observe that while topics
themselves do not reveal intrinsic (network) structures  latent components are able to achieve so by
applying highly-specialized metrics to measure local similarities and yield characteristic signatures.
We also study whether or not the lack of an edge between a pair of dissimilar documents from
different sections  can give rise to characteristic signatures from the latent components. In summary 
we do not observe those telltale signatures for those pairs. Detailed results are in the Suppl. Material.

4 Related Work

Our model learns multiple metrics  one for each latent component. However  the similarity (or
associated dissimilarity) from our model is deﬁnitely non-metric due to the complex combination.
This stands in stark contrast to most metric learning algorithms [19  8  7  18  5  11  13  17  9].
[12] gives an information-theoretic deﬁnition of (non-metric) similarity as long as there is a proba-
bilistic model for the data. Our approach of SCA focuses on the relationship between data but not
data themselves. [16] proposes visualization techniques for non-metric similarity data.
Our work is reminiscent of probabilistic modeling of overlapping communities in social networks 
such as the mixed membership stochastic blockmodels [3]. The key difference is that those works
model vertices with a mixture of latent components (communities) where we model the interactions
between vertices with a mixture of latent components. [2] studies a social network whose edge
set is the union of multiple edge sets in hidden similarity spaces. Our work explicitly models the
probabilistic process of combining latent components with a (noisy-)OR gate.

5 Conclusion

We propose Similarity Component Analysis (SCA) for probabilistic modeling of similarity relation-
ship for pairwise data instances. The key ingredient of SCA is to model similarity as a complex
combination of multiple latent components  each giving rise to a local similarity value. SCA attains
signiﬁcantly better accuracies than existing methods on both classiﬁcation and link prediction tasks.
Acknowledgements We thank reviewers for extensive discussion and references on the topics of similarity and
learning similarity. We plan to include them as well as other suggested experimentations in a longer version
of this paper. This research is supported by a USC Annenberg Graduate Fellowship (S.C.) and the IARPA via
DoD/ARL contract # W911NF-12-C-0012. The U.S. Government is authorized to reproduce and distribute
reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclu-
sions contained herein are those of the authors and should not be interpreted as necessarily representing the
ofﬁcial policies or endorsements  either expressed or implied  of IARPA  DoD/ARL  or the U.S. Government.

8

Metric IDSection ID2468123456789  123456789  123456789References
[1] NIPS0-12 dataset. http://www.stats.ox.ac.uk/˜teh/data.html.
[2] I. Abraham  S. Chechik  D. Kempe  and A. Slivkins. Low-distortion Inference of Latent Simi-

larities from a Multiplex Social Network. CoRR  abs/1202.0922  2012.

[3] E. M. Airoldi  D. M. Blei  S. E. Fienberg  and E. P. Xing. Mixed Membership Stochastic

Blockmodels. Journal of Machine Learning Research  9:1981–2014  June 2008.

[4] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine

Learning Research  3:993–1022  2003.

[5] J. V. Davis  B. Kulis  P. Jain  S. Sra  and I. S. Dhillon. Information-theoretic Metric Learning.

In ICML  2007.

[6] S. E. Fienberg  M. M. Meyer  and S. S. Wasserman. Statistical Analysis of Multiple Sociomet-

ric Relations. Journal of the American Statistical Association  80(389):51–67  March 1985.

[7] A. Globerson and S. Roweis. Metric Learning by Collapsing Classes. In NIPS  2005.
[8] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighbourhood Components

Analysis. In NIPS  2004.

[9] S. Hauberg  O. Freifeld  and M. Black. A Geometric take on Metric Learning. In NIPS  2012.
[10] T. S. Jaakkola and M. I. Jordan. Variational Probabilistic Inference and the QMR-DT Network.

Journal of Artiﬁcial Intelligence Research  10(1):291–322  May 1999.

[11] P. Jain  B. Kulis  I. Dhillon  and K. Grauman. Online Metric Learning and Fast Similarity

Search. In NIPS  2008.

[12] D. Lin. An Information-Theoretic Deﬁnition of Similarity. In ICML  1998.
[13] S. Parameswaran and K. Weinberger. Large Margin Multi-Task Metric Learning. In NIPS 

2010.

[14] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.

Morgan Kaufmann Publishers Inc.  San Francisco  CA  USA  1988.

[15] M. Szell  R. Lambiotte  and S. Thurner. Multirelational Organization of Large-scale Social

Networks in an Online World. Proceedings of the National Academy of Sciences  2010.

[16] L. van der Maaten and G. Hinton. Visualizing Non-Metric Similarities in Multiple Maps.

Machine Learning  33:33–55  2012.

[17] J. Wang  A. Woznica  and A. Kalousis. Parametric Local Metric Learning for Nearest Neighbor

Classiﬁcation. In NIPS  2012.

[18] K. Q. Weinberger and L. K. Saul. Distance Metric Learning for Large Margin Nearest Neigh-

bor Classiﬁcation. Journal of Machine Learning Research  10:207–244  2009.

[19] E. P. Xing  A. Y. Ng  M. I. Jordan  and S. Russell. Distance Metric Learning  with Application

to Clustering with Side-information. In NIPS  2002.

9

,Soravit Changpinyo
Kuan Liu
Fei Sha
Suriya Gunasekar
Oluwasanmi Koyejo
Joydeep Ghosh