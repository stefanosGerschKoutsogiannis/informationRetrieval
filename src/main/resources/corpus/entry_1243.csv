2014,Covariance shrinkage for autocorrelated data,The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly  hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting  i.i.d. data is assumed  however  in practice  time series typically exhibit strong autocorrelation structure  which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator  while being consistent in the high-dimensional limit  suffers from a high bias in finite sample sizes. We propose an alternative estimator  which is (1) unbiased  (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.,Covariance shrinkage for autocorrelated data

Daniel Bartz

Department of Computer Science

TU Berlin  Berlin  Germany

daniel.bartz@tu-berlin.de

klaus-robert.mueller@tu-berlin.de

Klaus-Robert M¨uller

TU Berlin  Berlin  Germany
Korea University  Korea  Seoul

Abstract

The accurate estimation of covariance matrices is essential for many signal pro-
cessing and machine learning algorithms. In high dimensional settings the sample
covariance is known to perform poorly  hence regularization strategies such as
analytic shrinkage of Ledoit/Wolf are applied. In the standard setting  i.i.d. data
is assumed  however  in practice  time series typically exhibit strong autocorrela-
tion structure  which introduces a pronounced estimation bias. Recent work by
Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute
in this work by showing that the Sancetta estimator  while being consistent in the
high-dimensional limit  suffers from a high bias in ﬁnite sample sizes. We propose
an alternative estimator  which is (1) unbiased  (2) less sensitive to hyperparame-
ter choice and (3) yields superior performance in simulations on toy data and on a
real world data set from an EEG-based Brain-Computer-Interfacing experiment.

1

Introduction and Motivation

Covariance matrices are a key ingredient in many algorithms in signal processing  machine learning
and statistics. The standard estimator  the sample covariance matrix S  has appealing properties in
the limit of large sample sizes n: its entries are unbiased and consistent [HTF08]. On the other hand 
for sample sizes of the order of the dimensionality p or even smaller  its entries have a high variance
and the spectrum has a large systematic error. In particular  large eigenvalues are overestimated and
small eigenvalues underestimated  the condition number is large and the matrix difﬁcult to invert
[MP67  ER05  BS10]. One way to counteract this issue is to shrink S towards a biased estimator T
(the shrinkage target) with lower variance [Ste56] 

Csh := (1 − λ)S + λT 

the default choice being T = p−1trace(S)I  the identity multiplied by the average eigenvalue.
For the optimal shrinkage intensity λ(cid:63)  a reduction of the expected mean squared error is guaran-
teed [LW04]. Model selection for λ can be done by cross-validation (CV) with the known draw-
backs: for (i) problems with many hyperparameters  (ii) very high-dimensional data sets  or (iii)
online settings which need fast responses  CV can become unfeasible and a faster model selection
method is required. A popular alternative to CV is Ledoit and Wolf’s analytic shrinkage proce-
dure [LW04] and more recent variants [CWEH10  BM13]. Analytic shrinkage directly estimates the
shrinkage intensity which minimizes the expected mean squared error of the convex combination
with a negligible computational cost  especially for applications which rely on expensive matrix
inversions or eigendecompositions in high dimensions.
All of the above algorithms assume i.i.d. data. Real world time series  however  are often non-i.i.d. as
they possess pronounced autocorrelation (AC). This makes covariance estimation in high dimensions
the data dependence lowers the effective sample size available for constructing the
even harder:
estimator [TZ84]. Thus  stronger regularization λ will be needed. In Figure 1 the simple case of an
autoregressive model serves as an example for an arbitrary generative model with autocorrelation.

1

Figure 1: Dependency of the eigendecomposition on autocorrelation. p = 200  n = 250.

The Figure shows  for three levels of autocorrelation (left)  the population and sample eigenvalues
(middle): with increasing autocorrelation the sample eigenvalues become more biased. This bias
is an optimistic measure for the quality of the covariance estimator: it neglects that population and
sample eigenbasis also differ [LW12]. Comparing sample eigenvalues to the population variance in
the sample eigenbasis  the bias is even larger (right).
In practice  violations of the i.i.d. assumption are often ignored [LG11  SBMK13  GLL+14]  al-
though Sancetta proposed a consistent shrinkage estimator under autocorrelation [San08]. In this
paper  we contribute by showing in theory  simulations and on real world data  that (i) ignoring au-
tocorrelations for shrinkage leads to large estimation errors and (ii) for ﬁnite samples Sancetta’s es-
timator is still substantially biased and highly sensitive to the number of incorporated time lags. We
propose a new bias-corrected estimator which (iii) outperforms standard shrinkage and Sancetta’s
method under the presence of autocorrelation and (iv) is robust to the choice of the lag parameter.

2 Shrinkage for autocorrelated data

Ledoit and Wolf derived a formula for the optimal shrinkage intensity [LW04  SS05]:

λ(cid:63) =

(cid:1)
ij Var(cid:0)Sij
(cid:80)
E(cid:104)(cid:0)Sij − Tij
(cid:1)2(cid:105) .
(cid:16)
n(cid:88)
(cid:1) =

(cid:80)
(cid:1) −→ (cid:100)Var(cid:0)Sij
(cid:1)2(cid:105) −→ (cid:98)E(cid:2)(Sij − Tij)2(cid:3) = (Sij − Tij)2 

xisxjs − 1
n

1
n2

s=1

ij

Var(cid:0)Sij
E(cid:104)(cid:0)Sij − Tij

(cid:17)2

xitxjt

n(cid:88)

t=1

The analytic shrinkage estimator ˆλ is obtained by replacing expectations with sample estimates:

(1)

(2)

(3)

where xit is the tth observation of variable i. While the estimator eq. (3) is unbiased even under a
violation of the i.i.d. assumption  the estimator eq. (2) is based on

Var

xitxjt

i.i.d.
=

1
n

Var (xitxjt) .

If the data are autocorrelated  cross terms cannot be ignored and we obtain

Var

xitxjt

=

Cov(xitxjt  xisxjs)

Cov(xitxjt  xitxjt) +

n − s
n

Cov(xitxjt  xi t+sxj t+s)

n−1(cid:88)

s=1

2
n

Γij(s)

(4)

Figure 2 illustrates the effect of ignoring the cross terms for increasing autocorrelation (larger AR-
coefﬁcients  see section 3 for details on the simulation). It compares standard shrinkage to an oracle
shrinkage based on the population variance of the sample covariance1. The population variance of S

1calculated by resampling.

2

(cid:32)

n(cid:88)

t=1

1
n

(cid:33)

n(cid:88)

t=1

(cid:32)
n(cid:88)

1
n

s t=1

(cid:33)

1
n2

=

1
n

=:

1
n

Γij(0) +

2
n

n−1(cid:88)

s=1

02040608010000.20.40.60.81time lagautocorrelation (AC) no AC (AR−coeff. = 0)low AC (AR−coeff. = 0.7)high AC (AR−coeff. = 0.95)150160170180190200010203040#eigenvalueeigenvalue populationsample150160170180190200010203040#sample eigendirectionvarianceFigure 2: Dependency of shrinkage on autocorrelation. p = 200  n = 250.

increases because the effective sample size is reduced [TZ84]  yet the standard shrinkage variance
estimator eq. (2) does not increase (outer left). As a consequence  for oracle shrinkage the shrinkage
intensity increases  for the standard shrinkage estimator it even decreases because the denominator
in eq. (1) grows (middle left). With increasing autocorrelation  the sample covariance becomes
a less precise estimator: for optimal (stronger) shrinkage more improvement becomes possible 
yet standard shrinkage does not improve (middle right). Looking at the variance estimates in the
sample eigendirections for AR-coefﬁcients of 0.7  we see that the bias of standard shrinkage is only
marginally smaller than the bias of the sample covariance  while oracle shrinkage yields a substantial
bias reduction (outer right).

Sancetta-estimator An estimator for eq. (4) was proposed by [San08]:

n−s(cid:88)
(cid:32)

t=1

1
n

1
n

ˆΓSan
ij (s) :=

(cid:100)Var(cid:0)Sij

(cid:1)San b

(xitxjt − Sij) (xi t+sxj t+s − Sij)  

(5)

(cid:33)

n−1(cid:88)

s=1

:=

ˆΓSan
ij (0) + 2

κ(s/b)ˆΓSan

ij (s)

 

b > 0 

where κ is a kernel which has to fulﬁll Assumption B in [And91]. We will restrict our analysis to
the truncated kernel κTR(x) = {1 for |x| ≤ 1  0 otherwise} to obtain less cluttered formulas2. The
kernel parameter b describes how many time lags are taken into account.
The Sancetta estimator behaves well in the high dimensional limit: the main theoretical result states
that for (i) a ﬁxed decay of the autocorrelation  (ii) b  n → ∞ and (iii) b2 increasing at a lower rate
than n  the estimator is consistent independently of the rate of p (for details  see [San08]). This is
in line with the results in [LW04  CWEH10  BM13]: as long as n increases  all of these shrinkage
estimators are consistent.

Bias of the Sancetta-estimator
In the following we will show that the Sancetta-estimator is sub-
optimal in ﬁnite samples: it has a non-negligible bias. To understand this  consider a lag s large
enough to have Γij(s) ≈ 0. If we approximate the expectation of the Sancetta-estimator  we see that
it is biased downwards:

E(cid:104)ˆΓSan

ij (s)

ij

t=1

1
n

(cid:1)(cid:35)
(cid:34)
(cid:105) ≈ E
n−s(cid:88)
(cid:0)xitxjtxi t+sxj t+s − S2
(cid:3)(cid:1) = − n − s
(cid:0)E2 [Sij] − E(cid:2)S2
≈ n − s
n−s(cid:88)
(cid:0)xitxjtxi t+sxj t+s − S2
(cid:1)  
(cid:32)

t=1

n

n

ij

ij

n−1(cid:88)

s=1

1
n

ˆΓBC
ij (s) :=

(cid:100)Var(cid:0)Sij

(cid:1)BC b

.

Var (Sij) < 0.

Bias-corrected (BC) estimator We propose a bias-corrected estimator for the variance of the
entries in the sample covariance matrix:

:=

n − 1 − 2b + b(b + 1)/n

1

ˆΓBC
ij (0) + 2

κTR(s/b)ˆΓBC

ij (s)

 

b > 0.

2in his simulations  Sancetta uses the Bartlett kernel. For ﬁxed b  this increases the truncation bias.

3

(cid:33)

(6)

00.20.40.60.8100.020.040.060.080.1norm. Σij var(Sij)00.20.40.60.8100.20.40.60.81shrinkage intensity λAR−coefficients050100150200051015#sample eigendirectionvarianceAR−coeff. = 0.7 populationsamplepop. var(S) shrinkagestandard shrinkage00.20.40.60.810.20.40.60.81impr. over sample cov.The estimator ˆΓBC

is the denominator in(cid:100)Var(cid:0)Sij

(cid:1)BC b: it is smaller than n and thus corrects the downwards bias.

ij (s)  but slightly easier to compute. The main difference

ij (s) is very similar to ˆΓSan

2.1 Theoretical results

It is straightforward to extend the theoretical results on the Sancetta estimator ([San08]  see sum-
mary above) to our proposed estimator. In the following  to better understand the limitations of the
Sancetta estimator  we will provide a complementary theoretical analysis on the behaviour of the
estimator for ﬁnite n.
Our theoretical results are based on the analysis of a sequence of statistical models indexed by p. Xp
denotes a p × n matrix of n observations of p variables with mean zero and covariance matrix Cp.
Yp = R(cid:62)
p Xp denotes the same observations rotated in their eigenbasis  having diagonal covariance
Λp = R(cid:62)
it denote the entries of Xp and Yp  respectively3. The
it and yp
p CpRp. Lower case letters xp
analysis is based on the following assumptions:
Assumption 1 (A1  bound on average eighth moment). There exists a constant K1 independent of
p such that

p(cid:88)

i=1

1
p

E[(xp

i1)8] ≤ K1.

Assumption 2 (A2  uncorrelatedness of higher moments). Let Q denote the set of quadruples

{i j k l} of distinct integers.(cid:80)
(cid:80)

and

∀s :

i j kl l∈Qp

Cov2[yp
i1yp
|Qp|

j1  yp

k 1+syp

l 1+s]

i j kl l∈Qp

Cov

j1)2  (yp

k 1+syp

= O(cid:0)p−1(cid:1)  
l 1+s)2(cid:105)

= O(cid:0)p−1(cid:1)  

(cid:104)

(yp
i1yp
|Qp|

p(cid:88)

i=1

1
p

E[(xp

i1)2] ≥ K2.

hold.
Assumption 3 (A3  non-degeneracy). There exists a constant K2 such that

Assumption 4 (A4  moment relation). There exist constants α4  α8  β4 and β8 such that

E[y8
E[y8

i ] ≤ (1 + α8)E2[y4
i ] 
i ] ≥ (1 + β8)E2[y4
i ] 

E[y4
E[y4

i ] ≤ (1 + α4)E2[y2
i ] 
i ] ≥ (1 + β4)E2[y2
i ].

Remarks on the assumptions A restriction on the eighth moment (assumption A1) is necessary
because the estimators eq. (2)  (3)  (5) and (6) contain fourth moments  their variances therefore
contain eighths moments. Note that  contrary to the similar assumption in the eigenbasis in [LW04] 
A1 poses no restriction on the covariance structure [BM13]. To quantify the effect of averaging
over dimensions  assumption A2 restricts the correlations of higher moments in the eigenbasis. This
assumption is trivially fulﬁlled for Gaussian data  but much weaker (see [LW04]). Assumption A3
rules out the degenerate case of adding observation channels without any variance and assumption
A4 excludes distributions with arbitrarily heavy tails.
Based on these assumptions  we can analyse the difference between the Sancetta-estimator and our
proposed estimator for large p:
Theorem 1 (consistency under “ﬁxed n”-asympotics). Let A1  A2  A3  A4 hold. We then have

(cid:88)

ij

1
p2

Var (Sij) = Θ(1)

3We shall often drop the sequence index p and the observation index t to improve readability of formulas.

4

San b

(Sij) − Var (Sij)

BC b

(Sij) − Var (Sij)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

p2

p2

(cid:88)
(cid:88)

ij

ij

E

E

(cid:16)(cid:100)Var
(cid:16)(cid:100)Var
(cid:40)
(cid:88)
(cid:88)

ij

(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

TR

(cid:1)2
=(cid:0)BiasSan b + BiasSan b
(cid:32) (cid:80)
=(cid:0)BiasBC b
((cid:80)
n(cid:88)
n(cid:88)

b(cid:88)

(cid:1)2

+ O

TR

+ O

j γ2
j
j γj)2

Var (Sij) − 4
n3

t=n−s

s=1

u=1

(cid:33)

j γ2
j
j γj)2

(cid:32) (cid:80)
((cid:80)
(cid:33)

(cid:41)

Cov [xitxjt  xiuxju]

Figure 3: Dependence of the variance estimates on the dimensionality. Averaged over R = 50
models. n = 250.

where the γi denote the eigenvalues of C and
1 + 2b − b(b + 1)/n

BiasSan b := − 1
p2

n
n − s
n

n(cid:88)

2

ij

s=b+1

BiasSan b

TR

:= − 1
p2

2
n

BiasBC b
TR

:= − 1
p2

Cov [xitxjt  xi t+sxj t+s]

(cid:88)

n−1(cid:88)

n − 1 − 2b + b(b+1)

n

ij

s=b+1

Cov [xitxjt  xi t+sxj t+s]

Proof. see the supplemental material.

(iii) The variance of both estimators behaves as O((cid:80)

Remarks on Theorem 1
(i) The mean squared error of both estimators consists of a bias and a
variance term. Both estimators have a truncation bias which is a consequence of including only a
limited number of time lags into the variance estimation. When b is chosen sufﬁciently high  this
term gets close to zero. (ii) The Sancetta-estimator has an additional bias term which is smaller than
zero in each dimension and therefore does not average out. Simulations will show that  as a conse-
quence  the Sancetta-estimator has a strong bias which gets larger with increasing lag parameter b.
i γi)2): the more the variance of the
data is spread over the eigendirections  the smaller the variance of the estimators. This bound is
minimal if the eigenvalues are identical. (iv) Theorem 1 does not make a statement on the relative
sizes of the variances of the estimators. Note that the BC estimator mainly differs by a multiplicative
factor > 1  hence the variance is larger  but not relative to the expectation of the estimator.

i / ((cid:80)

i γ2

3 Simulations

Our simulations are based on those in [San08]: We average over R = 50 multivariate Gaussian
AR(1) models
with parameter matrix4 A = ψAC · I   with ψno AC = 0  ψlow AC = 0.7  and ψhigh AC = 0.95 (see Fig-
i drawn from a log-normal distribution
ure 1). The innovations it are Gaussian with variances σ2
4more complex parameter matrices or a different generative model do not pose a problem for the bias-

(cid:126)xt = A(cid:126)xt−1 + (cid:126)t 

corrected estimator. The simple model was chosen for clarity of presentation.

5

010020030040050066.577.588.599.5x 10−3no AC (b = 10)Σij var(Sij)/p2dimensionality01002003004005000.020.030.040.050.060.070.080.09low AC (b = 20)dimensionality0100200300400500051015202530high AC (b = 90)dimensionality populationshrinkageSancettabias−corrFigure 4: Robustness to the choice of lag parameter b. Variance estimates (upper row)  shrinkage
intensities (middle row) and improvement over sample covariance (lower row). Averaged over R =
50 models. p = 200  n = 250.

calculate the std. deviations of the estimators and to obtain an approximation of p−2(cid:80)

with mean µ = 1 and scale parameter σ = 0.5. For each model  we generate K = 50 data sets to
ij Var (Sij).
Simulation 1 analyses the dependence of the estimators on the dimensionality of the data. The num-
ber of observations is ﬁxed at n = 250 and the lag parameter b chosen by hand such that the whole
autocorrelation is covered5: bno AC = 10  blow AC = 20 and bhigh AC = 90. Figure 3 shows that the
standard shrinkage estimator is unbiased and has low variance in the no AC-setting  but under the
presence of autocorrelation it strongly underestimates the variance. As predicted by Theorem 1 
the Sancetta estimator is also biased; its remains stays constant for increasing dimensionality. Our
proposed estimator has no visible bias. For increasing dimensionality the variances of all estima-
tors decrease. Relative to the average estimate  there is no visible difference between the standard
deviations of the Sancetta and the BC estimator.
Simulation 2 analyses the dependency on the lag parameter b for ﬁxed dimensionality p = 200
and number of observations n = 250. In addition to variance estimates  Figure 4 reports shrinkage
intensities and the percentage improvement in absolute loss (PRIAL) over the sample covariance
matrix:

PRIAL(cid:0)C{pop.  shr  San.  BC}(cid:1) =

E(cid:107)S − C(cid:107) − E(cid:107)C{pop.  shr  San.  BC} − C(cid:107)

E(cid:107)S − C(cid:107)

.

The three quantities show very similar behaviour. Standard shrinkage performs well in the no AC-
case  but is strongly biased in the autocorrelated settings. The Sancetta estimator is very sensitive to
the choice of the lag parameter b. For low AC  the bias at the optimal b is small: only a small number
of biased terms are included. For high AC the optimal b is larger  the higher number of biased terms
causes a larger bias. The BC-estimator is very robust: it performs well for all b large enough to
capture the autocorrelation. For very large b its variance increases slightly  but this has practically

5for b < 1  optimal in the no AC-setting  Sancetta and BC estimator are equivalent to standard shrinkage.

6

02550751002468x 10−3no ACΣij var(Sij)/p202550751000.10.20.30.40.5shrinkage intensity λ02550751000.20.250.30.350.4PRIAL02550751000.020.040.060.080.1low AC02550751000.10.20.30.40.50.602550751000.20.30.40.50.60.7lag parameter b025507510005101520high AC025507510000.20.40.60.81025507510000.20.40.60.81 pop. var(S)shrinkageSancettabias−corrFigure 5: BCI motor imagery data for lag parameter b = 75 (upper row) and b = 300 (lower row).
Averaged over subjects and K = 100 runs.

no effect on the PRIAL. An interesting aspect is that our proposed estimator even outperforms
shrinkage based on the the population Var (Sij) (calculated by resampling). This results from the

(cid:1)BC b with the sample estimate eq. (3) of the denominator in

correlation of the estimator (cid:100)Var(cid:0)Sij

eq. (1).

4 Real World Data: Brain Computer Interface based on Motor Imagery

As an example of autocorrelated data we reanalyzed a data set from a motor imagery experiment. In
the experiment  brain activity for two different imagined movements was measured via EEG (p = 55
channels  80 subjects  150 trials per subject  each trial with ntrial = 390 measurements [BSH+10]).
The frequency band was optimized for each subject and from the class-wise covariance matrices  1-3
ﬁlters per class were extracted by Common Spatial Patterns (CSP)  adaptively chosen by a heuristic
(see [BTL+08]). We trained Linear Discriminant Analysis on log-variance features.
To improve the estimate of the class covariances on these highly autocorrelated data  standard shrink-
age  Sancetta shrinkage  cross-validation and and our proposed BC shrinkage estimator were ap-
plied. The covariance structure is far from diagonal  therefore  for each subject  we used the average
of the class covariances of the other subjects as shrinkage target [BLT+11]. Shrinkage is domi-
nated by the inﬂuence of high-variance directions [BM13]  which are pronounced in this data set.
To reduce this effect  we rescaled  only for the calculation of the shrinkage intensities  the ﬁrst ﬁve
principal components to have the same variance as the sixth principal component.
We analyse the dependency of the four algorithms on the number of supplied training trials. Figure 5
(upper row) shows results for an optimized time lag (b = 75) which captures well the autocorre-
lation of the data (outer left). Taking the autocorrelation into account makes a clear difference
(middle left/right): while standard shrinkage outperforms the sample covariance  it is clearly out-
performed by the autocorrelation-adjusted approaches. The Sancetta-estimator is slightly worse
than our proposed estimator. The shrinkage intensities (outer right) are extremely low for standard
shrinkage and the negative bias of the Sancetta-estimator shows clearly for small numbers of train-
ing trials. Figure 5 (lower row) shows results for a too large time lag (b = 300). The performance
of the Sancetta-estimator strongly degrades as its shrinkage intensities get smaller  while our pro-
posed estimator is robust to the choice of b  only for the smallest number of trials we observe a
small degradation in performance. Figure 6 (left) compares our bias-corrected estimator to the four
other approaches for 10 training trials: it signiﬁcantly outperforms standard shrinkage and Sancetta
shrinkage for both the larger (b = 300  p ≤ 0.01) and the smaller time lag (b = 75  p ≤ 0.05).

7

051015200.550.60.650.70.750.8number of trials per class accuracy sample covstandard shrinkagesancettabias−corrcross−val.0510152000.010.020.030.040.05number of trials per class accuracy − acc(sample cov)0510152000.050.10.150.2number of trials per class shrinkage intensity0255075−0.500.51time lagb = 75AC051015200.550.60.650.70.750.8number of trials per class accuracy sample covstandard shrinkagesancettabias−corrcross−val.0510152000.010.020.030.040.05number of trials per class accuracy − acc(sample cov)0510152000.050.10.150.2number of trials per class shrinkage intensity0100200300−0.500.51time lagb = 300ACFigure 6: Subject-wise BCI classiﬁcation accuracies for 10 training trials (left) and time demands
(right). ∗∗/∗ := signiﬁcant at p ≤ 0.01 or p ≤ 0.05  respectively.

Analytic shrinkage procedures optimize only the mean squared error of the covariance matrix  while
cross-validation directly optimizes the classiﬁcation performance. Yet  Figure 5 (middle) shows that
for small numbers of training trials our proposed estimator outperforms CV  although the difference
is not signiﬁcant (see Fig. 6). For larger numbers of training trials CV performs better. This shows
that the MSE is not a very good proxy for classiﬁcation accuracies in the context of CSP: for op-
timal MSE  shrinkage intensities decrease with increasing number of observations. CV shrinkage
intensities instead stay on a constant level between 0.1 and 0.15. Figure 6 (right) shows that the
three shrinkage approaches (b = 300) have a huge performance advantage over cross-validation (10
folds/10 parameter candidates) with respect to runtime.

5 Discussion

Analytic Shrinkage estimators are highly useful tools for covariance matrix estimation in time-
critical or computationally expensive applications: no time-consuming cross-validation procedure
is required. In addition  it has been observed that in some applications  cross-validation is not a
good predictor for out-of-sample performance [LG11  BKT+07]. Its speed and good performance
has made analytic shrinkage widely used: it is  for example  state-of-the-art in ERP experiments
[BLT+11]. While standard shrinkage assumes i.i.d. data  many real world data sets  for example
from video  audio  ﬁnance  biomedical engineering or energy systems clearly violate this assump-
tion as strong autocorrelation is present. Intuitively this means that the information content per data
point becomes lower  and thus the covariance estimation problem becomes harder: the dimension-
ality remains unchanged but the effective number of samples available decreases. Thus stronger
regularization is required and standard analytic shrinkage [LW04] needs to be corrected.
Sancetta already moved the ﬁrst step into this important direction by providing a consistent estimator
under i.i.d. violations [San08]. In this work we analysed ﬁnite sample sizes and showed that (i) even
apart from truncation bias —which results from including a limited number of time lags— Sancetta’s
estimator is biased  (ii) this bias is only negligible if the autocorrelation decays fast compared to
the length of the time series and (iii) the Sancetta estimator is very sensitive to the choice of lag
parameter.
We proposed an alternative estimator which is (i) both consistent and —apart from truncation bias—
unbiased and (ii) highly robust to the choice of lag parameter: In simulations on toy and real world
data we showed that the proposed estimator yields large improvements for small samples and/or sub-
optimal lag parameter. Even for optimal lag parameter there is a slight but signiﬁcant improvement.
Analysing data from BCI motor imagery experiments we see that (i) the BCI data set possesses
signiﬁcant autocorrelation  that (ii) this adversely affects CSP based on the sample covariance and
standard shrinkage (iii) this effect can be alleviated using our novel estimator  which is shown to
(iv) compare favorably to Sancetta’s estimator.

Acknowledgments

This research was also supported by the National Research Foundation grant (No. 2012-005741)
funded by the Korean government. We thank Johannes H¨ohne  Sebastian Bach and Duncan Blythe
for valuable discussions and comments.

8

0.50.60.70.80.90.50.60.70.80.91**90%**90%**8.75%**8.75%bias−corrsample covariance0.50.60.70.80.90.50.60.70.80.91**77.50%**78.75%**21.25%**20%standard shrinkage0.50.60.70.80.90.50.60.70.80.91 51.25% 53.75% 47.50% 45.00%CVsubject−wise classification accuracies 0.50.60.70.80.90.50.60.70.80.91 *60%**60% *38.75%**38.75%Sancetta estimator b = 75b = 300SCShrSanBCCV020406080100120time demandnormalized runtimeReferences

[And91] Donald WK Andrews. Heteroskedasticity and autocorrelation consistent covariance matrix esti-

mation. Econometrica: Journal of the Econometric Society  pages 817–858  1991.

[BKT+07] Benjamin Blankertz  Motoaki Kawanabe  Ryota Tomioka  Friederike Hohlefeld  Klaus-Robert
M¨uller  and Vadim V Nikulin. Invariant common spatial patterns: Alleviating nonstationarities in
brain-computer interfacing. In Advances in Neural Information Processing Systems  pages 113–
120  2007.

[BLT+11] Benjamin Blankertz  Steven Lemm  Matthias Treder  Stefan Haufe  and Klaus-Robert M¨uller.
Single-trial analysis and classiﬁcation of ERP components – a tutorial. NeuroImage  56(2):814–
825  2011.

[BM13] Daniel Bartz and Klaus-Robert M¨uller. Generalizing analytic shrinkage for arbitrary covariance
structures. In C.J.C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K.Q. Weinberger  editors 
Advances in Neural Information Processing Systems 26  pages 1869–1877. Curran Associates 
Inc.  2013.

[BS10] Zhidong Bai and Jack William Silverstein. Spectral Analysis of Large Dimensional Random Ma-

trices. Springer Series in Statistics. Springer New York  2010.

[BSH+10] Benjamin Blankertz  Claudia Sannelli  Sebastian Halder  Eva M Hammer  Andrea K¨ubler  Klaus-
Robert M¨uller  Gabriel Curio  and Thorsten Dickhaus. Neurophysiological predictor of SMR-
based BCI performance. Neuroimage  51(4):1303–1309  2010.

[BTL+08] Benjamin Blankertz  Ryota Tomioka  Steven Lemm  Motoaki Kawanabe  and Klaus-Robert
M¨uller. Optimizing spatial ﬁlters for robust EEG single-trial analysis. Signal Processing Mag-
azine  IEEE  25(1):41–56  2008.

[CWEH10] Yilun Chen  Ami Wiesel  Yonina C Eldar  and Alfred O Hero. Shrinkage algorithms for MMSE

covariance estimation. Signal Processing  IEEE Transactions on  58(10):5016–5029  2010.
[ER05] Alan Edelman and N. Raj Rao. Random matrix theory. Acta Numerica  14:233–297  2005.

[GLL+14] Alexandre Gramfort  Martin Luessi  Eric Larson  Denis A. Engemann  Daniel Strohmeier  Chris-
tian Brodbeck  Lauri Parkkonen  and Matti S. H¨am¨al¨ainen. MNE software for processing MEG
and EEG data. NeuroImage  86(0):446 – 460  2014.

[HTF08] Trevor Hastie  Robert Tibshirani  and Jerome Friedman. The Elements of Statistical Learning.

Springer  2008.

[LG11] Fabien Lotte and Cuntai Guan. Regularizing common spatial patterns to improve BCI designs:
uniﬁed theory and new algorithms. Biomedical Engineering  IEEE Transactions on  58(2):355–
362  2011.

[LW04] Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covariance

matrices. Journal of Multivariate Analysis  88(2):365–411  2004.

[LW12] Olivier Ledoit and Michael Wolf. Nonlinear shrinkage estimation of large-dimensional covariance

matrices. The Annals of Statistics  40(2):1024–1060  2012.

[MP67] Vladimir A. Marˇcenko and Leonid A. Pastur. Distribution of eigenvalues for some sets of random

matrices. Mathematics of the USSR-Sbornik  1(4):457  1967.

[San08] Alessio Sancetta. Sample covariance shrinkage for high dimensional dependent data. Journal of

Multivariate Analysis  99(5):949–967  May 2008.

[SBMK13] Wojciech Samek  Duncan Blythe  Klaus-Robert M¨uller  and Motoaki Kawanabe. Robust spatial
ﬁltering with beta divergence. In L. Bottou  C.J.C. Burges  M. Welling  Z. Ghahramani  and K.Q.
Weinberger  editors  Advances in Neural Information Processing Systems 26  pages 1007–1015.
2013.

[SS05] Juliane Sch¨afer and Korbinian Strimmer. A shrinkage approach to large-scale covariance matrix
estimation and implications for functional genomics. Statistical Applications in Genetics and
Molecular Biology  4(1):1175–1189  2005.

[Ste56] Charles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal dis-
In Proc. 3rd Berkeley Sympos. Math. Statist. Probability  volume 1  pages 197–206 

tribution.
1956.

[TZ84] H. Jean Thi´ebaux and Francis W. Zwiers. The interpretation and estimation of effective sample

size. Journal of Climate and Applied Meteorology  23(5):800–811  1984.

9

,Daniel Bartz
Klaus-Robert Müller