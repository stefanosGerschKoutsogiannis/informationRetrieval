2014,The Blinded Bandit: Learning with Adaptive Feedback,We study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action. Such model of \emph{adaptive feedback} naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions. This motivates a variant of the multi-armed bandit problem  which we call the \emph{blinded multi-armed bandit}  in which no feedback is given to the algorithm whenever it switches arms. We develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem. This result stands in stark contrast to another recent result  which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn  and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem. We also extend our results to the general prediction framework of bandit linear optimization  again attaining near-optimal regret bounds.,The Blinded Bandit:

Learning with Adaptive Feedback

Ofer Dekel

Microsoft Research

Elad Hazan

Technion

Tomer Koren

Technion

oferd@microsoft.com

ehazan@ie.technion.ac.il

tomerk@technion.ac.il

Abstract

We study an online learning setting where the player is temporarily deprived of
feedback each time it switches to a different action. Such a model of adaptive feed-
back naturally occurs in scenarios where the environment reacts to the player’s ac-
tions and requires some time to recover and stabilize after the algorithm switches
actions. This motivates a variant of the multi-armed bandit problem  which we call
the blinded multi-armed bandit  in which no feedback is given to the algorithm
whenever it switches arms. We develop efﬁcient online learning algorithms for
this problem and prove that they guarantee the same asymptotic regret as the op-
timal algorithms for the standard multi-armed bandit problem. This result stands
in stark contrast to another recent result  which states that adding a switching cost
to the standard multi-armed bandit makes it substantially harder to learn  and pro-
vides a direct comparison of how feedback and loss contribute to the difﬁculty
of an online learning problem. We also extend our results to the general predic-
tion framework of bandit linear optimization  again attaining near-optimal regret
bounds.

1

Introduction

The adversarial multi-armed bandit problem [4] is a T -round prediction game played by a random-
ized player in an adversarial environment. On each round of the game  the player chooses an arm
(also called an action) from some ﬁnite set  and incurs the loss associated with that arm. The player
can choose the arm randomly  by choosing a distribution over the arms and then drawing an arm
from that distribution. He observes the loss associated with the chosen arm  but he does not observe
the loss associated with any of the other arms. The player’s cumulative loss is the sum of all the loss
values that he incurs during the game. To minimize his cumulative loss  the player must trade-off
exploration (trying different arms to observe their loss values) and exploitation (choosing a good
arm based on historical observations).
The loss values are assigned by the adversarial environment before the game begins. Each of the
loss values is constrained to be in [0  1] but otherwise they can be arbitrary. Since the loss values are
set beforehand  we say that the adversarial environment is oblivious to the player’s actions.
The performance of a player strategy is measured in the standard way  using the game-theoretic
notion of regret (formally deﬁned below). Auer et al. [4] present a player strategy called EXP3 
prove that it guarantees a worst-case regret of O(
T ) on any oblivious assignment of loss values 
and prove that this guarantee is the best possible. A sublinear upper bound on regret implies that the
√
player’s strategy improves over time and is therefore a learning strategy  but if this upper bound has
a rate of O(

T ) then the problem is called an easy online learning problem.1

√

1The classiﬁcation of online problems into easy vs. hard is borrowed from Antos et al. [2].

1

In this paper  we study a variant of the standard multi-armed bandit problem where the player is
temporarily blinded each time he switches arms. In other words  if the player’s current choice is
different than his choice on the previous round then we say that he has switched arms  he incurs the
loss as before  but he does not observe this loss  or any other feedback. On the other hand  if the
player chooses the same arm that he chose on the previous round  he incurs and observes his loss as
usual.2 We call this setting the blinded multi-armed bandit.
For example  say that the player’s task is to choose an advertising campaign (out of k candidates) to
reduce the frequency of car accidents. Even if a new advertising campaign has an immediate effect 
the new accident rate can only be measured over time (since we must wait for a few accidents to
occur) and the environment’s reaction to the change cannot be observed immediately.
The blinded bandit setting can also be used to model problems where a switch introduces a tempo-
rary bias into the feedback  which makes this feedback useless. A good example is the well-known
primacy and novelty effect [14  15] that occurs in human-computer interaction. Say that we operate
an online restaurant directory and the task is to choose the best user interface (UI) for our site (from
a set of k candidates). The quality of a UI is measured by the the time it takes the user to complete
a successful interaction with our system. Whenever we switch to a new UI  we encounter a primacy
effect: users are initially confused by the unfamiliar interface and interaction times artiﬁcially in-
crease. In some situations  we may encounter the opposite  a novelty effect: a fresh new UI could
intrigue users  increase their desire to engage with the system  and temporarily decrease interac-
tion times. In both cases  feedback is immediately available  but each switch makes the feedback
temporarily unreliable.
There are also cases where switching introduces a variance in the feedback  rather than a bias.
Almost any setting where the feedback is measured by a physical sensor  such as a photometer or a
digital thermometer  ﬁts in this category. Most physical sensors apply a low-pass ﬁlter to the signal
they measure and a low-pass ﬁlter in the frequency domain is equivalent to integrating the signal
over a sliding window in the time domain. While the sensor may output an immediate reading  it
needs time to stabilize and return to an adequate precision.
The blinded bandit setting bears a close similarity to another setting called the adversarial multi-
armed bandit with switching costs. In that setting  the player incurs an additional loss each time he
switches arms. This penalty discourages the player from switching frequently. At ﬁrst glance  it
would seem that the practical problems described above could be formulated and solved as multi-
armed bandit problems with switching costs and one might question the need for our new blinded
bandit setting. However  Dekel et al. [12] recently proved that the adversarial multi-armed bandit
with switching costs is a hard online learning problem  which is a problem where the best possible

regret guarantee is (cid:101)Θ(T 2/3). In other words  for any learning algorithm  there exists an oblivious
setting of the loss values that forces a regret of(cid:101)Ω(T 2/3).

√
In this paper  we present a new algorithm for the blinded bandit setting and prove that it guarantees a
regret of O(
T ) on any oblivious sequence of loss values. In other words  we prove that the blinded
bandit is surprisingly as easy as the standard multi-armed bandit setting  despite its close similarity to
the hard multi-armed bandit with switching costs problem. Our result has a theoretical signiﬁcance
and a practical signiﬁcance. Theoretically  it provides a direct comparison of how feedback and
loss contribute to the difﬁculty of an online learning problem. Practically  it identiﬁes a rich and
important class of online learning problems that would seem to be a natural ﬁt for the multi-armed
bandit setting with switching costs  but are in fact much easier to learn. Moreover  to the best of our
knowledge  our work is the ﬁrst to consider online learning in an setting where the loss values are
oblivious to the player’s past actions but the feedback is adaptive.
We also extend our results and study a blinded version of the more general bandit linear optimization
setting. The bandit linear optimization framework is useful for efﬁciently modeling problems of
learning under uncertainty with extremely large  yet structured decision sets. For example  consider
the problem of online routing in networks [5]  where our task is to route a stream of packets between
two nodes in a computer network. While there may be exponentially many paths between the two
nodes  the total time it takes to send a packet is simply the sum of the delays on each edge in the
path. If the route is switched in the middle of a long streaming transmission  the network protocol

2More generally  we could deﬁne a setting where the player is blinded for m rounds following each switch 

but for simplicity we focus on m = 1.

2

needs a while to ﬁnd the new optimal transmission rate  and the delay of the ﬁrst few packets after
the switch can be arbitrary. This view on the packet routing problem demonstrates the need for a
blinded version of bandit linear optimization.
The paper is organized as follows. In Section 2 we formalize the setting and lay out the necessary
deﬁnitions. Section 3 is dedicated to presenting our main result  which is an optimal algorithm for
the blinded bandit problem. In Section 4 we extend this result to the more general setting of bandit
linear optimization. We conclude in Section 5.

2 Problem Setting

To describe our contribution to this problem and its signiﬁcance compared to previous work  we ﬁrst
deﬁne our problem setting more formally and give some background on the problem.
As mentioned above  the player plays a T -round prediction game against an adversarial environment.
Before the game begins  the environment picks a sequence of loss functions (cid:96)1  . . .   (cid:96)T : K (cid:55)→ [0  1]
that assigns loss values to arms from the set K = {1  . . .   k}. On each round t  the player chooses an
arm xt ∈ K  possibly at random  which results in a loss (cid:96)t(xt). In the standard multi-armed bandit
setting  the feedback provided to the player at the end of round t is the number (cid:96)t(xt)  whereas the
other values of the function (cid:96)t are never observed.

The player’s expected cumulative loss at the end of the game equals E[(cid:80)T

t=1 (cid:96)t(xt)]. Since the loss
values are assigned adversarially  the player’s cumulative loss is only meaningful when compared
to an adequate baseline; we compare the player’s cumulative loss to the cumulative loss of a ﬁxed
policy  which chooses the same arm on every round. Deﬁne the player’s regret as

R(T ) = E

(cid:96)t(xt)

(cid:96)t(x) .

(1)

(cid:34) T(cid:88)

t=1

(cid:35)

T(cid:88)

t=1

− min
x∈K

Regret can be positive or negative. If R(T ) = o(T ) (namely  the regret is either negative or grows at
most sublinearly with T )  we say that the player is learning. Otherwise  if R(T ) = Θ(T ) (namely 
the regret grows linearly with T )  it indicates that the player’s per-round loss does not decrease with
time and therefore we say that the player is not learning.
In the blinded version of the problem  the feedback on round t  i.e. the number (cid:96)t(xt)  is revealed to
the player only if he chooses xt to be the same as xt−1. On the other hand  if xt (cid:54)= xt−1  then the
player does not observe any feedback. The blinded bandit game is summarized in Fig. 1.

Parameters: action set K  time horizon T
• Environment determines a sequence of loss functions (cid:96)1  . . .   (cid:96)T : K (cid:55)→ [0  1]
• On each round t = 1  2  . . .   T :

1. Player picks an action xt ∈ K and suffers the loss (cid:96)t(xt) ∈ [0  1]
2. If xt = xt−1  the number (cid:96)t(xt) is revealed as feedback to the player
3. Otherwise  if xt (cid:54)= xt−1  the player gets no feedback from the environment

Figure 1: The blinded bandit game.

Bandit Linear Optimization.
In Section 4  we consider the more general setting of online linear
optimization with bandit feedback [10  11  1]. In this problem  on round t of the game  the player
chooses an action  possibly at random  which is a point xt in a ﬁxed action set K ⊂ Rn. The loss
he suffers on that round is then computed by a linear function (cid:96)t(xt) = (cid:96)t · xt  where (cid:96)t ∈ Rn is a
loss vector chosen by the oblivious adversarial environment before the game begins. To ensure that
the incurred losses are bounded  we assume that the loss vectors (cid:96)1  . . .   (cid:96)T are admissible  that is 
they satisfy |(cid:96)t · x| ≤ 1 for all t and x ∈ K (in other words  the loss vectors reside in the polar set
of K). As in the multi-armed bandit problem  the player only observes the loss he incurred  and the
full loss vector (cid:96)t is never revealed to him. The player’s performance is measured by his regret  as
deﬁned above in Eq. (1).

3

3 Algorithm

We recall the classic EXP3 algorithm for the standard multi-armed bandit problem  and speciﬁcally
focus on the version presented in Bubeck and Cesa-Bianchi [6]. The player maintains a probability
distribution over the arms  which we denote by pt ∈ ∆(K) (where ∆(K) denotes the set of probabil-
ity measures over K  which is simply the k-dimensional simplex when K = {1  2  . . .   k}). Initially 
p1 is set to the uniform distribution ( 1
k ). On round t  the player draws xt according to pt 
incurs and observes the loss (cid:96)t(xt)  and applies the update rule
−η

pt+1(x) ∝ pt(x) · exp

∀ x ∈ K 

k   . . .   1

(cid:19)

(cid:18)

(cid:96)t(xt)
pt(xt)

· 11x=xt

.

EXP3 provides the following regret guarantee  which depends on the user-deﬁned learning rate
parameter η:
Theorem 1 (due to Auer et al. [4]  taken from Bubeck and Cesa-Bianchi [6]). Let (cid:96)1  . . .   (cid:96)T be an
arbitrary loss sequence  where each (cid:96)t : K (cid:55)→ [0  1]. Let x1  . . .   xT be the random sequence of
arms chosen by EXP3 (with learning rate η > 0) as it observes this sequence. Then 

R(T ) ≤ ηkT
2

+

log k

η

.

√

EXP3 cannot be used in the blinded bandit setting because the EXP3 update rule cannot be called
on rounds where a switch occurs. Also  since switching actions Ω(T ) times is  in general  required
for obtaining the optimal O(
T ) regret (see [12])  the player must avoid switching actions too fre-
quently and often stick with the action that was chosen on the previous round. Due to the adversarial
nature of the problem  randomization must be used in controlling the scheme of action switches.
We propose a variation on EXP3  which is presented in Algorithm 1. Our algorithm begins by
drawing a sequence of independent Bernoulli random variables b0  b1  . . .   bT +1 (i.e.  such that
P(bt = 0) = P(bt = 1) = 1
2). This sequence determines the schedule of switches and updates
for the entire game. The algorithm draws a new arm (and possibly switches) only on rounds where
bt−1 = 0 and bt = 1 and invokes the EXP3 update rule only on rounds where bt = 0 and bt+1 = 1.
Note that these two events can never co-occur. Speciﬁcally  the algorithm always invokes the update
rule one round before the potential switch occurs. This conﬁrms that the algorithm relies on the
value of (cid:96)t(xt) only on non-switching rounds.

k   . . .   1

k )  draw x0 ∼ p1

Algorithm 1: BLINDED EXP3
set p1 ← ( 1
draw b0  . . .   bT +1 i.i.d. unbiased Bernoullis
for t = 1  2  . . .   T
draw xt ∼ pt
set xt ← xt−1

if bt−1 = 0 and bt = 1

else

// possible switch

// no switch

play arm xt and incur loss (cid:96)t(xt)
if bt = 0 and bt+1 = 1

observe (cid:96)t(xt) and for all x ∈ K  update

wt+1(x) ← pt(x) · exp

else

set pt+1 ← wt+1/(cid:107)wt+1(cid:107)1
set pt+1 ← pt

(cid:18)

−η

(cid:19)

(cid:96)t(xt)
pt(xt)

· 11x=xt

We set out to prove the following regret bound.

4

: K (cid:55)→ [0  1]. Let
Theorem 2. Let (cid:96)1  . . .   (cid:96)T be an arbitrary loss sequence  where each (cid:96)t
x1  . . .   xT be the random sequence of arms chosen by Algorithm 1 as it plays the blinded ban-
dit game on this sequence (with learning rate ﬁxed to η =

kT ). Then 

(cid:113) 2 log k
R(T ) ≤ 6(cid:112)T k log k .

We prove Theorem 2 with the below sequence of lemmas. In the following  we let (cid:96)1  . . .   (cid:96)T be
an arbitrary loss sequence and let x1  . . .   xT be the sequence of arms chosen by Algorithm 1 (with
parameter η > 0). First  we deﬁne the set

In words  S is a random subset of [T ] that indicates the rounds on which Algorithm 1 uses its
feedback and applies the EXP3 update.
Lemma 1. For any x ∈ K  it holds that

S = (cid:8)t ∈ [T ] : bt = 0 and bt+1 = 1(cid:9) .
(cid:34)(cid:88)

(cid:96)t(xt) −(cid:88)

(cid:96)t(x)

(cid:35)

+

log k

≤ ηkT
8

t∈S

t∈S

.

η

E

Proof. For any concrete instantiation of b0  . . .   bT +1  the set S is ﬁxed and the sequence ((cid:96)t)t∈S is
an oblivious sequence of loss functions. Note that the steps performed by Algorithm 1 on the rounds
indicated in S are precisely the steps that the standard EXP3 algorithm would perform if it were
presented with the loss sequence ((cid:96)t)t∈S. Therefore  Theorem 1 guarantees that

(cid:34)(cid:88)

t∈S

E

(cid:96)t(xt) −(cid:88)

t∈S

(cid:96)t(x)

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) S

≤ ηk|S|

2

+

log k

η

.

Taking expectations on both sides of the above and noting that E[|S|] ≤ T /4 proves the lemma.

Lemma 1 proves a regret bound that is restricted to the rounds indicated by S. The following lemma
relates that regret to the total regret  on all T rounds.
Lemma 2. For any x ∈ K  we have

(cid:96)t(xt)

(cid:96)t(x) ≤ 4 E

(cid:96)t(x)

+ E

(cid:107)pt − pt−1(cid:107)1

.

(cid:34)(cid:88)

t∈S

(cid:96)t(xt) −(cid:88)

t∈S

(cid:35)

(cid:34) T(cid:88)

t=1

(cid:35)

(cid:34) T(cid:88)

t=1

E

(cid:35)

− T(cid:88)
(cid:34)(cid:88)

t=1

E

(cid:35)
(cid:34)(cid:88)

E

Proof. Using the deﬁnition of S  we have

t∈S

Similarly  we have

(3)
We focus on the t’th summand in the right-hand side above. Since bt+1 is independent of (cid:96)t(xt)(1−
bt)  it holds that

(cid:96)t(xt)

t∈S

t=1

=

(cid:96)t(x)

=

(cid:96)t(x) E[(1 − bt)bt+1] =

(2)

T(cid:88)
(cid:35)

t=1

T(cid:88)

t=1

1
4

(cid:96)t(x) .

T(cid:88)
(cid:3) .
E(cid:2)(cid:96)t(xt) (1 − bt)bt+1
E(cid:2)(cid:96)t(xt)(1 − bt)(cid:3) .
(cid:12)(cid:12)(cid:12) bt = 1
(cid:105)

(cid:96)t(xt)(1 − bt)

E(cid:104)

1
2

+

Using the law of total expectation  we get

E(cid:2)(cid:96)t(xt)(1 − bt)bt+1
E(cid:2)(cid:96)t(xt)(1 − bt)(cid:3) =

1
2

(cid:3) = E[bt+1]E(cid:2)(cid:96)t(xt)(1 − bt)(cid:3) =
(cid:12)(cid:12)(cid:12) bt = 0
E(cid:104)
(cid:105)
E(cid:2)(cid:96)t(xt)(cid:12)(cid:12) bt = 0(cid:3) .

(cid:96)t(xt)(1 − bt)

1
4

1
4
1
4

=

5

If bt = 0 then Algorithm 1 sets xt ← xt−1 so we have that xt = xt−1. Therefore  the above equals
E[(cid:96)t(xt−1) | bt = 0]. Since xt−1 is independent of bt  this simply equals 1
E[(cid:96)t(xt−1)]. H¨older’s
1
4
inequality can be used to upper bound

4

(cid:0)pt(x) − pt−1(x)(cid:1) (cid:96)t(x)

(cid:105) ≤ E[(cid:107)pt − pt−1(cid:107)1] · max

E[(cid:96)t(xt) − (cid:96)t(xt−1)] = E(cid:104)(cid:88)

x∈K (cid:96)t(x)  

x∈K

where we have used the fact that xt and xt−1 are distributed according to pt and pt−1 respectively
(regardless of whether an update took place or not). Since it is assumed that (cid:96)t(x) ∈ [0  1] for all t
and x ∈ K  we obtain

4

1
4

(cid:0)E(cid:2)(cid:96)t(xt)(cid:3) − E[(cid:107)pt − pt−1(cid:107)1](cid:1) .
E(cid:2)(cid:96)t(xt−1)(cid:3) ≥ 1
(cid:0)E(cid:2)(cid:96)t(xt)(cid:3) − E[(cid:107)pt − pt−1(cid:107)1](cid:1) .
(cid:3) ≥ 1
E(cid:2)(cid:96)t(xt)(1 − bt)bt+1
(cid:34) T(cid:88)
(cid:35)
(cid:35)
(cid:34)(cid:88)
(cid:96)t(xt) − T(cid:88)

(cid:107)pt − pt−1(cid:107)1

(cid:96)t(xt)

E

E

.

≥ 1
4

t=1

t=1

t∈S

Overall  we have shown that

4
Plugging this inequality back into Eq. (3) gives

Summing the inequality above with the one in Eq. (2) concludes the proof.

Next  we prove that the probability distributions over arms do not change much on consecutive
rounds of EXP3.
Lemma 3. The distributions p1  p2  . . .   pT generated by the BLINDED EXP3 algorithm satisfy
E[(cid:107)pt+1 − pt(cid:107)1] ≤ 2η for all t.
Proof. Fix a round t; we shall prove the stronger claim that (cid:107)pt+1 − pt(cid:107)1 ≤ 2η with probability 1.
If no update had occurred on round t and pt+1 = pt  this holds trivially. Otherwise  we can use the
triangle inequality to bound

(cid:107)pt+1 − pt(cid:107)1 ≤ (cid:107)pt+1 − wt+1(cid:107)1 + (cid:107)wt+1 − pt(cid:107)1  

with the vector wt+1 as speciﬁed in Algorithm 1. Letting Wt+1 = (cid:107)wt+1(cid:107)1 we have pt+1 =
wt+1/Wt+1  so we can rewrite the ﬁrst term on the right-hand side above as

(cid:107)pt+1 − Wt+1 · pt+1(cid:107)1 = |1 − Wt+1| · (cid:107)pt+1(cid:107)1 = 1 − Wt+1 = (cid:107)pt − wt+1(cid:107)1  

where the last equality follows by observing that pt ≥ wt+1 entrywise  (cid:107)pt(cid:107)1 = 1 and (cid:107)wt+1(cid:107)1 =

Wt+1. By the deﬁnition of wt+1  the second term on the right-hand side above equals pt(xt) ·(cid:0)1 −
e−η(cid:96)t(xt)/pt(xt)(cid:1). Overall  we have

(cid:107)pt+1 − pt(cid:107)1 ≤ 2pt(xt) ·(cid:0)1 − e−η(cid:96)t(xt)/pt(xt)(cid:1) .

Using the inequality 1 − exp(−α) ≤ α  we get (cid:107)pt+1 − pt(cid:107)1 ≤ 2η(cid:96)t(xt). The claim now follows
from the assumption that (cid:96)t(xt) ∈ [0  1].

We can now proceed to prove our regret bound.
Proof of Theorem 2. Combining the bounds of Lemmas 1–3 proves that for any ﬁxed arm x ∈ K  it
holds that

(cid:34) T(cid:88)

E

(cid:35)

− T(cid:88)

(cid:96)t(xt)

t=1

t=1

(cid:96)t(x) ≤ ηkT
2

+

4 log k

η

+ 2ηT

Speciﬁcally  the above holds for the best arm in hindsight. Setting η =

≤ 2ηkT +

4 log k

η

.

(cid:113) 2 log k

kT

proves the theorem.

6

4 Blinded Bandit Linear Optimization

In this section we extend our results to the setting of linear optimization with bandit feedback 
√
formally deﬁned in Section 2. We focus on the GEOMETRICHEDGE algorithm [11]  that was the
ﬁrst algorithm for the problem to attain the optimal O(
T ) regret  and adapt it to the blinded setup.
Our BLINDED GEOMETRICHEDGE algorithm is detailed in Algorithm 2. The algorithm uses a
mechanism similar to that of Algorithm 1 for deciding when to avoid switching actions. Following
the presentation of [11]  we assume that K ⊆ [−1  1]n is ﬁnite and that the standard basis vectors
e1  . . .   en are contained in K. Then  the set E = {e1  . . .   en} is a barycentric spanner of K [5] that
serves the algorithm as an exploration basis. We denote the uniform distribution over E by uE.

Algorithm 2: BLINDED GEOMETRICHEDGE
Parameter: learning rate η > 0
let q1 be the uniform distribution over K  and draw x0 ∼ q1
draw b0  . . .   bT +1 i.i.d. unbiased Bernoullis
set γ ← n2η
for t = 1  2  . . .   T

set pt ← (1 − γ) qt + γ uE
compute covariance Ct ← Ex∼pt[xx(cid:62)]
if bt−1 = 0 and bt = 1

else

draw xt ∼ pt
set xt ← xt−1

play arm xt and incur loss (cid:96)t(xt) = (cid:96)t · xt
if bt = 0 and bt+1 = 1

observe (cid:96)t(xt) and let ˆ(cid:96)t ← (cid:96)t(xt) · C−1
update qt+1(x) ∝ qt(x) · exp(−η ˆ(cid:96)t · x)
set qt+1 ← qt

else

t xt

// possible switch

// no switch

√

The main result of this section is an O(
T ) upper-bound over the expected regret of Algorithm 2.
Theorem 3. Let (cid:96)1  . . .   (cid:96)T be an arbitrary sequence of linear loss functions  admissible with respect
to the action set K ⊆ Rn. Let x1  . . .   xT be the random sequence of arms chosen by Algorithm 2 as
it plays the blinded bandit game on this sequence  with learning rate ﬁxed to η =
10nT . Then 

(cid:113) log(nT )

R(T ) ≤ 4n3/2(cid:112)T log(nT ) .

With minor modiﬁcations  our technique can also be applied to variants of the GEOMET-
RICHEDGE algorithm (that differ by their exploration basis) for obtaining regret bounds with im-
proved dependence of the dimension n. This includes the COMBAND algorithm [8]  EXP2 with
John’s exploration [7]  and the more recent version employing volumetric spanners [13].
We now turn to prove Theorem 3. Our ﬁrst step is proving an analogue of Lemma 1  using the regret
bound of the GEOMETRICHEDGE algorithm proved by Dani et al. [11].

Lemma 4. For any x ∈ K  it holds that E(cid:2)(cid:80)

t∈S (cid:96)t(x)(cid:3) ≤ ηn2T

t∈S (cid:96)t(xt) −(cid:80)

2 + n log(nT )

2η

.

We proceed to prove that the distributions generated by Algorithm 2 do not change too quickly.
Lemma 5. The distributions p1  p2  . . .   pT produced by the BLINDED GEOMETRICHEDGE algo-
rithm (from which the actions x1  x2  . . .   xT are drawn) satisfy E[(cid:107)pt+1 − pt(cid:107)1] ≤ 4η
n for all t.
The proofs of both lemmas are omitted due to space constraints. We now prove Theorem 3.

√

7

Proof of Theorem 3. Notice that the bound of Lemma 2 is independent of the construction of the
distributions p1  p2  . . .   pT and the structure of K  and thus applies for Algorithm 2 as well. Com-
bining this bound with the results of Lemmas 4 and 5  it follows that for any ﬁxed action x ∈ K 

(cid:96)t(x) ≤ ηn2T
2

+

n log(nT )

2η

√

+ 4η

nT ≤ 5ηn2T +

n log(nT )

2η

.

(cid:34) T(cid:88)

E

(cid:35)
− T(cid:88)
(cid:113) log(nT )

t=1

(cid:96)t(xt)

t=1

Setting η =

10nT proves the theorem.

5 Discussion and Open Problems

In this paper  we studied a new online learning scenario where the player receives feedback from
the adversarial environment only when his action is the same as the one from the previous round  a
setting that we named the blinded bandit. We devised an optimal algorithm for the blinded multi-
armed bandit problem based on the EXP3 strategy  and used similar ideas to adapt the GEOMET-
RICHEDGE algorithm to the blinded bandit linear optimization setting. In fact  a similar analysis
can be applied to any online algorithm that does not change its underlying prediction distributions
too quickly (in total variation distance).
In the practical examples given in the introduction  where each switch introduces a bias or a vari-
ance  we argued that the multi-armed bandit problem with switching costs is an inadequate solution 
since it is unreasonable to solve an easy problem by reducing it to one that is substantially harder.
Alternatively  one might consider simply ignoring the noise in the feedback after each switch and
using a standard adversarial multi-armed bandit algorithm like EXP3 despite the bias or the vari-
ance. However  if we do that  the player’s observed losses would no longer be oblivious (as the
√
observed loss on round t would depend on xt−1)  and the regret guarantees of EXP3 would no
longer hold.3 Moreover  any multi-armed bandit algorithm with O(
T ) regret can be forced to
make Θ(T ) switches [12]  so the loss observed by the player could actually be non-oblivious in a
constant fraction of the rounds  which would deteriorate the performance of EXP3.
Our setting might seem similar to the related problem of label-efﬁcient prediction (with bandit feed-
back)  see [9]. In the label-efﬁcient prediction setting  the feedback for the action performed on
some round is received only if the player explicitly asks for it. The player may freely choose when
to observe feedback  subject to a global constraint on the number of total feedback queries. In con-
trast  in our setting there is a strong correlation between the actions the player takes and the presence
of the feedback signal. As a consequence  the player is not free to decide when he observes feedback
as in the label-efﬁcient setting. Another setting that may seem closely related to our setting is the
multi-armed bandit problem with delayed feedback [16  17]. In this setting  the feedback for the
action performed on round t is received at the end of round t + 1. However  note that in all of the ex-
amples we have discussed  the feedback is always immediate  but is either nonexistent or unreliable
right after a switch. The important aspect of our setup  which does not apply to the label-efﬁcient
and delayed feedback settings  is that the feedback adapts to the player’s past actions.
Our work leaves a few interesting questions for future research. A closely related adaptive-feedback
√
problem is one where feedback is revealed only on rounds where the player does switch actions.
T ) regret in this setting as well  or is the need to constantly switch actions
Can the player attain O(
detrimental to the player? More generally  we can consider other multi-armed bandit problems with
adaptive feedback  where the feedback depends on the player’s actions on previous rounds. It would
be quite interesting to understand what kind of adaptive-feedback patterns give rise to easy problems 
T ) is attainable. Speciﬁcally  is there a problem with oblivious losses and
for which a regret of O(

adaptive feedback whose minimax regret is(cid:101)Θ(T 2/3)  as is the case with adaptive losses?

√

Acknowledgments

The research leading to these results has received funding from the Microsoft-Technion EC center 
and the European Union’s Seventh Framework Programme (FP7/2007-2013]) under grant agreement
n◦ 336078 ERC-SUBLRN.

3Auer et al. [4] also present an algorithm called EXP3.P and seemingly prove O(

against non-oblivious adversaries. These bounds are irrelevant in our setting—see Arora et al. [3].

√
T ) regret guarantees

8

References
[1] J. Abernethy  E. Hazan  and A. Rakhlin. Competing in the dark: An efﬁcient algorithm for

bandit linear optimization. In COLT  pages 263–274  2008.

[2] A. Antos  G. Bart´ok  D. P´al  and C. Szepesv´ari. Toward a classiﬁcation of ﬁnite partial-

monitoring games. Theoretical Computer Science  2012.

[3] R. Arora  O. Dekel  and A. Tewari. Online bandit learning against an adaptive adversary:
from regret to policy regret. In Proceedings of the Twenty-Ninth International Conference on
Machine Learning  2012.

[4] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[5] B. Awerbuch and R. D. Kleinberg. Adaptive routing with end-to-end feedback: Distributed
learning and geometric approaches. In Proceedings of the thirty-sixth annual ACM symposium
on Theory of computing  pages 45–53. ACM  2004.

[6] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

[7] S. Bubeck  N. Cesa-Bianchi  and S. M. Kakade. Towards minimax policies for online linear
optimization with bandit feedback. In Proceedings of the 25th Annual Conference on Learning
Theory (COLT)  volume 23  pages 41.1–41.14  2012.

[8] N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences  78(5):1404–1422  2012.

[9] N. Cesa-Bianchi  G. Lugosi  and G. Stoltz. Minimizing regret with label efﬁcient prediction.

IEEE Transactions on Information Theory  51(6):2152–2162  2005.

[10] V. Dani and T. P. Hayes. Robbing the bandit: Less regret in online geometric optimization
against an adaptive adversary. In Proceedings of the Seventeenth Annual ACM-SIAM Sympo-
sium on Discrete Algorithms  2006.

[11] V. Dani  S. M. Kakade  and T. P. Hayes. The price of bandit information for online optimiza-

tion. In Advances in Neural Information Processing Systems  pages 345–352  2007.

[12] O. Dekel  J. Ding  T. Koren  and Y. Peres. Bandits with switching costs: T 2/3 regret. arXiv

preprint arXiv:1310.2997  2013.

[13] E. Hazan  Z. Karnin  and R. Mehka. Volumetric spanners and their applications to machine

learning. In arXiv:1312.6214  2013.

[14] R. Kohavi  R. Longbotham  D. Sommerﬁeld  and R. M. Henne. Controlled experiments on
the web: survey and practical guide. Data Mining and Knowledge Discovery  18(1):140–181 
2009.

[15] R. Kohavi  A. Deng  B. Frasca  R. Longbotham  T. Walker  and Y. Xu. Trustworthy online
controlled experiments: Five puzzling outcomes explained. In Proceedings of the 18th ACM
SIGKDD international conference on Knowledge discovery and data mining  pages 786–794.
ACM  2012.

[16] C. Mesterharm. Online learning with delayed label feedback. In Proceedings of the Sixteenth

International Conference on Algorithmic Learning Theory  2005.

[17] G. Neu  A. Gy¨orgy  C. Szepesv´ari  and A. Antos. Online Markov decision processes under
bandit feedback. In Advances in Neural Information Processing Systems 23  pages 1804–1812 
2010.

9

,Ofer Dekel
Elad Hazan
Tomer Koren
Cameron Musco
Christopher Musco
Naganand Yadati
Madhav Nimishakavi
Prateek Yadav
Vikram Nitin
Anand Louis
Partha Talukdar