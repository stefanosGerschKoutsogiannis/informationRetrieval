2016,Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels,Maximum Mean Discrepancy (MMD) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing. This distance is based on the notion of embedding probabilities in a reproducing kernel Hilbert space. In this paper  we present the first known lower bounds for the estimation of MMD based on finite samples. Our lower bounds hold for any radial universal kernel on $\R^d$ and match the existing upper bounds up to constants that depend only on the properties of the kernel. Using these lower bounds  we establish the minimax rate optimality of the empirical estimator and its $U$-statistic variant  which are usually employed in applications.,Minimax Estimation of Maximum Mean Discrepancy

with Radial Kernels

Ilya Tolstikhin

Department of Empirical Inference

MPI for Intelligent Systems
Tübingen 72076  Germany
ilya@tuebingen.mpg.de

Bharath K. Sriperumbudur

Department of Statistics

Pennsylvania State University
University Park  PA 16802  USA

bks18@psu.edu

Bernhard Schölkopf

Department of Empirical Inference

MPI for Intelligent Systems
Tübingen 72076  Germany
bs@tuebingen.mpg.de

Abstract

Maximum Mean Discrepancy (MMD) is a distance on the space of probability
measures which has found numerous applications in machine learning and nonpara-
metric testing. This distance is based on the notion of embedding probabilities in a
reproducing kernel Hilbert space. In this paper  we present the ﬁrst known lower
bounds for the estimation of MMD based on ﬁnite samples. Our lower bounds
hold for any radial universal kernel on Rd and match the existing upper bounds up
to constants that depend only on the properties of the kernel. Using these lower
bounds  we establish the minimax rate optimality of the empirical estimator and its
U-statistic variant  which are usually employed in applications.

1

Introduction

Over the past decade  the notion of embedding probability measures in a Reproducing Kernel
Hilbert Space (RKHS) [1  13  18  17] has gained a lot of attention in machine learning  owing to
its wide applicability. Some popular applications of RKHS embedding of probabilities include two-
sample testing [5  6]  independence [7] and conditional independence testing [3]  feature selection
[14]  covariate-shift [13]  causal discovery [9]  density estimation [15]  kernel Bayes’ rule [4] 
and distribution regression [20]. This notion of embedding probability measures can be seen as a
generalization of classical kernel methods which deal with embedding points of an input space as
elements in an RKHS. Formally  given a probability measure P and a continuous positive deﬁnite
real-valued kernel k (we denote H to be the corresponding RKHS) deﬁned on a separable topological
space X   P is embedded into H as µP :=R k(·  x) dP (x)  called the mean element or the kernel
mean assuming k and P satisfyRXpk(x  x) dP (x) < 1. Based on the above embedding of P   [5]

deﬁned a distance—called the Maximum Mean Discrepancy (MMD)—on the space of probability
measures as the distance between the corresponding mean elements  i.e. 

MMDk(P  Q) = kµP  µQkH

.

We refer the reader to [18  17] for a detailed study on the properties of MMD and its relation to other
distances on probabilities.
Estimation of kernel mean.
In all the above mentioned applications  since the only knowledge of
the underlying distribution is through random samples drawn from it  an estimate of µP is employed

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

in practice. In applications such as two-sample test [5  6] and independence test [7] that involve
MMD  an estimate of MMD is constructed based on the estimates of µP and µQ respectively. The
nPn
i=1 k(·  Xi) which
simple and most popular estimator of µP is the empirical estimator  µPn := 1
is a Monte Carlo approximation of µP based on random samples (Xi)n
i=1 drawn i.i.d. from P .
Recently  [10] proposed a shrinkage estimator of µP based on the idea of James-Stein shrinkage 
which is demonstrated to empirically outperform µPn. While both these estimators are shown to
be pn-consistent [13  5  10]  it was not clear until the recent work of [21] whether any of these
estimators are minimax rate optimal  i.e.  is there an estimator of µP that yields a convergence rate
faster than n1/2? Based on the minimax optimality of the sample mean (i.e.  X := 1
i=1 Xi) for
the estimation of a ﬁnite dimensional mean of a normal distribution at a minimax rate of n1/2 [8 
Chapter 5  Example 1.14]  while one can intuitively argue that the empirical and shrinkage estimators
of µP are minimax rate optimal  it is difﬁcult to extend the ﬁnite dimensional argument in a rigorous
manner to the estimation of the inﬁnite dimensional object  µP . Note that H is inﬁnite dimensional
if k is universal [19  Chapter 4]  e.g.  Gaussian kernel. By establishing a remarkable relation between
the MMD of two Gaussian distributions and the Euclidean distance between their means for any
bounded continuous translation invariant universal kernel on X = Rd  [21] rigorously showed that
the estimation of µP is only as hard as the estimation of the ﬁnite dimensional mean of a normal
distribution and thereby established the minimax rate of estimating µP to be n1/2. This in turn
demonstrates the minimax rate optimality of empirical and shrinkage estimators of µP .
Estimation of MMD.
In this paper  we are interested in the minimax optimal estimation of
MMDk(P  Q). The question of ﬁnding optimal estimators of MMD is of interest in applications such
as kernel-based two-sample [5] and independence tests [7] as the test statistic is indeed an estimate of
MMD and it is important to use statistically optimal estimators in the construction of these kernel
based tests. An estimator of MMD that is currently employed in these applications is based on the
empirical estimators of µP and µQ  i.e. 

nPn

MMDn m := kµPn  µQmkH 
i.i.d.⇠ P and (Yi)m

i=1

i.i.d.⇠ Q. [5  7] also considered a
which is constructed from samples (Xi)n
U-statistic variant of MMDn m as a test statistic in these applications. As discussed above  while µPn
and µQm are minimax rate optimal estimators of µP and µQ respectively  they need not guarantee
that MMDn m is minimax rate optimal. Using the fact that kµPn  µPkH = Op(n1/2) and

i=1

|MMDk(P  Q)  MMDn m| k µP  µPnkH + kµQm  µQkH 

it is easy to see that

|MMDk(P  Q)  MMDn m| = Op(n1/2 + m1/2).

(1)
In fact  if k is a bounded kernel  it can be shown that the constants (which are hidden in the order
notation in (1)) depend only on the bound on the kernel and are independent of X   P and Q. The
goal of this work is to ﬁnd the minimax rate rn m k(P) and a positive constant ck(P) (independent
of m and n) such that

inf
ˆFn m

sup
P Q2P

P n ⇥ Qmnr1

n m k(P) | ˆFn m  MMDk(P  Q)| ck(P)o > 0 

(2)

i=1  (Yi)m

where P is a suitable subset of Borel probability measures on X   the inﬁmum is taken over all
estimators ˆFn m mapping the i.i.d. sample {(Xi)n
i=1} to R+  and P n ⇥ Qm denotes the
i.i.d.⇠ Q. In addition
i.i.d.⇠ P and (Yi)m
probability measure associated with the sample when (Xi)n
to the rate  we are also interested in the behavior of ck(P) in terms of its dependence on k  X and P.
Contributions. The main contribution of the paper is in establishing m1/2 + n1/2  i.e. 
rn m k(P) = p(m + n)/mn as the minimax rate for estimating MMDk(P  Q) when k is a ra-
dial universal kernel (examples include the Gaussian  Matérn and inverse multiquadric kernels) on Rd
and P is the set of all Borel probability measures on Rd with inﬁnitely differentiable densities. This
result guarantees that MMDn m and its U-statistic variant are minimax rate optimal estimators of
MMDk(P  Q)  which thereby ensures the minimax optimality of the test statistics used in kernel
two-sample and independence tests. We would like to highlight the fact that our result of the minimax
lower bound on MMDk(P  Q) implies part of the results of [21] related to the minimax estimation

i=1

i=1

2

of µP   as it can be seen that any ✏-accurate estimators ˆµP and ˆµQ of µP and µQ respectively in the
RKHS norm lead to the 2✏-accurate estimator ˆFn m := kˆµP  ˆµQkH of MMDk(P  Q)  i.e. 
ck(P)(n1/2 + m1/2) | MMDk(P  Q)  ˆFn m| k µP  ˆµPkH + kµQ  ˆµQkH.

In Section 2  we present the main results of our work wherein Theorem 1 is developed by employing
the ideas of [21] involving Le Cam’s method (see Theorem 3) [22  Sections 2.3 and 2.6]. However 
we show that while the minimax rate is m1/2 + n1/2  there is a sub-optimal dependence on d in
the constant ck(P) which makes the result uninteresting in high dimensional scenarios. To alleviate
this issue  we present a reﬁned result in Theorem 2 based on the method of two fuzzy hypotheses (see
Theorem 4) [22  Section 2.7.4] which shows that ck(P) in (2) is independent of d (i.e.  X ). This result
provides a sharp lower bound for MMD estimation both in terms of the rate and the constant (which
is independent of X ) that matches with behavior of the upper bound for MMDn m. The proofs of
these results are provided in Section 3 while supplementary results are collected in an appendix.
Notation.
In this work we focus on radial kernels  i.e.  k(x  y) = (kx  yk2) for all x  y 2 Rd.
Schoenberg’s theorem [12] states that a radial kernel k is positive deﬁnite for every d if and only if
there exists a non-negative ﬁnite Borel measure ⌫ on [0 1) such that

etkxyk2

d⌫(t)

(3)

k(x  y) =Z 1

0

for all x  y 2 Rd. An important example of a radial kernel is the Gaussian kernel k(x  y) =
exp{kx  yk2/(2⌘2)} for ⌘2 > 0. [17  Proposition 5] showed that k in (3) is universal if and only
if supp(⌫) 6= {0}  where for a ﬁnite non-negative Borel measure µ on Rd we deﬁne supp(µ) =
{x 2 Rd | if x 2 U and U is open then µ(U ) > 0}.
2 Main results

In this section  we present the main results of our work wherein we develop minimax lower bounds for
the estimation of MMDk(P  Q) when k is a radial universal kernel on Rd. We show that the minimax
i.i.d.⇠ Q is
rate for estimating MMDk(P  Q) based on random samples (Xi)n
m1/2+n1/2  thereby establishing the minimax rate optimality of the empirical estimator MMDn m
of MMD(P  Q). First  we present the following result (proved in Section 3.1) for Gaussian kernels 
which is based on an argument similar to the one used in [21] to obtain a minimax lower bound for
the estimation of µP .
Theorem 1. Let P be the set of all Borel probability measures over Rd with continuously inﬁnitely
differentiable densities. Let k be a Gaussian kernel with bandwidth parameter ⌘2 > 0. Then the
following holds:

i.i.d.⇠ P and (Yi)m

i=1

i=1

inf
ˆFn m

sup
P Q2P

P n ⇥ Qm(MMDk(P  Q)  ˆFn m 

1

8r 1

d + 1

max⇢ 1

pn

 

1

pm) 

1
5

.

(4)

The following remarks can be made about Theorem 1.
(a) Theorem 1 shows that MMDk(P  Q) cannot be estimated at a rate faster than max{n1/2  m1/2}
by any estimator ˆFn m for all P  Q 2P . Since max{m1/2  n1/2} 1
2 (m1/2 + n1/2)  the
result combined with (1) therefore establishes the minimax rate optimality of the empirical estimator 
MMDn m.
(b) While Theorem 1 shows the right order of dependence on m and n  the dependence on d seems
to be sub-optimal as the upper bound on |MMDn m  MMDk(P  Q)| depends only on the bound
on the kernel and is independent of d. This sub-optimal dependence on d may be due to the fact the
proof of Theorem 1 (see Section 3.1) as aforementioned is closely based on the arguments applied
in [21] for the minimax estimation of µP . While the lower bounding technique used in [21]—which
is commonly known as Le Cam’s method based on many hypotheses [22  Chapter 2]—provides
optimal results in the problem of estimation of functions (e.g.  estimation of µP in the norm of H)  it
often fails to do so in the case of estimation of real-valued functionals  which is precisely the focus of
our work. Even though Theorem 1 is sub-optimal  we presented the result to highlight the fact that

3

the minimax lower bounds for estimation of µP may not yield optimal results for MMDk(P  Q). In
Theorem 2  we will develop a new argument based on two fuzzy hypotheses  which is a method of
choice for nonparametric estimation of functionals [22  Section 2.7.4]. This will allow us to get rid of
the superﬂuous dependence on the dimensionality d in the lower bound.
(c) While Theorem 1 holds for only Gaussian kernels  we would like to mention that by using
the analysis of [21]  Theorem 1 can be straightforwardly improved in various ways: (i) it can be
generalized to hold for a wide class of radial universal kernels  (ii) the factor d1/2 in (4) can be
removed altogether for the case when P consists of all Borel discrete distributions on Rd. However 
these improvements do not involve any novel ideas than those captured by the proof of Theorem 1
and so will not be discussed in this work. For details  we refer an interested reader to Theorems 2
and 6 of [21] for extension to radial universal kernels and discrete measures  respectively.
(d) Finally  it is worth mentioning that any lower bound on the minimax probability (including the
bounds of Theorems 1 and 2) leads to a lower bound on the minimax risk  which is based on a simple

application of the Markov’s inequality: EP n⇥Qm⇥s1

n m · |An m|⇤  P n ⇥ Qm{|An m| sn m}.

The following result (proved in Section 3.2) is the main contribution of this work. It provides a
minimax lower bound for the problem of MMD estimation  which holds for general radial universal
kernels. In contrast to Theorem 1  it avoids the superﬂuous dependence on d and depends only on the
properties of k while exhibiting the correct rate.
Theorem 2. Let P be the set of all Borel probability measures over Rd with continuously inﬁnitely
differentiable densities. Let k be a radial kernel on Rd of the form (3)  where ⌫ is a bounded non-
negative measure on [0 1). Assume that there exist 0 < t0  t1 < 1 and 0 << 1 such that
⌫([t0  t1])  . Then the following holds:

inf
ˆFn m

sup
P Q2P

P n ⇥ Qm(MMDk(P  Q)  ˆFn m 

1

20r t0

t1e

max⇢ 1

pn

 

1

pm) 

1
14

.

(5)

Note that the existence of 0 < t0  t1 < 1 and 0 << 1 such that ⌫([t0  t1])   ensures that
supp(⌫) 6= {0} (i.e.  the kernel is not a constant function)  which implies k is universal. If k is a
Gaussian kernel with bandwidth parameter ⌘2 > 0  it is easy to verify that t0 = t1 = (2⌘2)1 and
 = 1 satisfy ⌫([t0  t1])   as the Gaussian kernel is generated by ⌫ = 1/(2⌘2) in (3)  where x is
a Dirac measure supported at x. Therefore we obtain a dimension independent constant in (5) for
Gaussian kernels compared to the bound in (4).

3 Proofs

In this section  we present the proofs of Theorems 1 and 2. Before we present the proofs  we ﬁrst
introduce the setting of nonparametric estimation. Let F :⇥ ! R be a functional deﬁned on a
measurable space ⇥ and P⇥ = {P✓ : ✓ 2 ⇥} be a family of probability distributions indexed by ⇥
and deﬁned over a measurable space X associated with data. We observe the data D 2X distributed
according to an unknown element P✓ 2P ⇥ and the goal is to estimate F (✓). Usually X   D  and P✓
will depend on sample size n. Let ˆFn := ˆFn(D) be an estimator of F (✓) based on D. The following
well known result [22  Theorem 2.2] provides a lower bound on the minimax probability of this
problem. We refer the reader to Appendix A for a proof of its more general version.
Theorem 3. Assume there exist ✓0 ✓ 1 2 ⇥ such that |F (✓0)  F (✓1)| 2s > 0 and
KL(P✓1kP✓0)  ↵ with 0 <↵< 1. Then

P✓n| ˆFn(D)  F (✓)| so  max 1

4

e↵ 

1 p↵/2

2

!  

inf
ˆFn

sup
✓2⇥

where KL(P✓1kP✓0) :=R log⇣ dP✓1

dP✓0⌘ dP✓1 denotes the Kullback-Leibler divergence between P✓1
and P✓0.
The above result (also called the Le Cam’s method) provides the recipe for obtaining minimax lower
bounds  where the goal is to construct two hypotheses ✓0 ✓ 1 2 ⇥ such that (i) F (✓0) and F (✓1) are
far apart  while (ii) the corresponding distributions  P✓0 and P✓1 are close enough. The requirement
(i) can be relaxed by introducing two random (fuzzy) hypotheses ✓0 ✓ 1 2 ⇥  and requiring F (✓0)

4

and F (✓1) to be far apart with high probability. This weaker requirement leads to a lower bounding
technique  called the method of two fuzzy hypotheses. This method is captured by the following
theorem [22  Theorem 2.14] and is commonly used to derive lower bounds on the minimax risk in
the problem of estimation of functionals [22  Section 2.7.4].
Theorem 4. Let µ0 and µ1 be any probability distributions over ⇥. Assume that

1. There exist c 2 R  s > 0  0  0  1 < 1 such that µ0✓ : F (✓)  c  1  0 and
µ1✓ : F (✓)  c + 2s  1  1.
2. There exist ⌧> 0 and 0 <↵< 1 such that P1⇣ dPa
Pi(D) =Z P✓(D)µi(d✓) 

dP1  ⌧⌘  1  ↵  where

i 2{ 0  1}

0 is the absolutely continuous component of P0 with respect to P1.

and Pa

0

Then

inf
ˆFn

sup
✓2⇥

P✓n| ˆFn(D)  F (✓)| so 

⌧ (1  ↵  1)  0

1 + ⌧

.

With this set up and background  we are ready to prove Theorems 1 and 2.

P n⇥QmnMMDk(P  Q)  ˆFn m  so  sup

3.1 Proof of Theorem 1
The proof is based on Theorem 3 and treats two cases m  n and m < n separately. We consider
only the case m  n as the second one follows the same steps. Let Gd denote a class of multivariate
Gaussian distributions over Rd with covariance matrices proportional to identity matrix Id 2 Rd⇥d.
In our case Gd ✓P   which leads to the following lower bound for any s > 0:
sup
P Q2P
Note that every element G(µ  2Id) 2G d is indexed by a pair (µ  2) 2 Rd ⇥ (0 1) =: ˜⇥. Given
two elements P  Q 2G d  the data is distributed according to P n ⇥ Qm. This brings us into the
2 for ✓ = (˜✓1  ˜✓2) 2 ⇥
context of Theorem 3 with ⇥:= ˜⇥ ⇥ ˜⇥  X := (Rd)n+m  P✓ := Gn
with Gaussian distributions G1 and G2 corresponding to parameters ˜✓1  ˜✓2 2 ˜⇥ respectively  and
F (✓) = MMDk(G1  G2).
In order to apply Theorem 3 we need to choose two probability distributions P✓0 and P✓1. We deﬁne
four different d-dimensional Gaussian distributions:
0   2Id)  Q0 = G(µQ

P n⇥QmnMMDk(P  Q)  ˆFn m  so .

0   2Id)  P1 = Q1 = G(0  2Id)

1 ⇥ Gm

P0 = G(µP

P Q2Gd

with

1

n

c2⌘2

c1⌘2

2 =

kµP

m⌘  

d ⇣2 +

m◆  
d ✓ 1
n qc2 1
this construction is possible as long asp c3

0 k2 =

+

n

where c1  c2  c3 > 0 are positive constants independent of m and n to be speciﬁed later. Note that
m   which is clearly satisﬁed if

n + 1

c3  c2.
First we will check the upper bound on the KL divergence between the distributions. Using the chain
rule of KL divergence and its closed form expression for Gaussian distributions we write

kµQ
0 k2 =
m +p c2

c2⌘2
dm

 

kµP

0  µQ

0 k2 =

c3⌘2
dn

 

Next we need to lower bound an absolute value between MMDk(P0  Q0) and MMDk(P1  Q1). Note
that

|MMDk(P0  Q0)  MMDk(P1  Q1)| = MMDk(P0  Q0).

(6)

5

KL(P n

1 ⇥ Qm

1 kP n

0 ⇥ Qm

22 + m · kµQ
0 k2
0 k2
0 ) = n · kµP
22 = n ·

n + 1

m
c2⌘2 1
2c1⌘22 + n
m + m ·
2c12 + n
m =

2 + n
m

c2
2c1

.

= c2

c2⌘2 1
m

2c1⌘22 + n
m

Using a closed-form expression for the MMD between Gaussian distribution [21  Eq. 25] we write

MMD2

k(P0  Q0) = 2✓

⌘2

⌘2 + 22◆d/2 1  exp kµP

2⌘2 + 42 !! .
0  µQ
0 k2

(7)

Assume

0  µQ
kµP
0 k2
2⌘2 + 42  1.
Using 1  ex  x/2  which holds for x 2 [0  1]  we write

|MMDk(P0  Q0)  MMDk(P1  Q1)| 

Since m  n and (1  1
m!d
 
d + 2c12 + n

d

4

Using this and setting c3 = c2 we get

0  µQ
0 k2
2⌘2 + 42 .

d

x )x1 monotonically decreases to e1 for x  1  we have
✓ d

m!d/4skµP
d + 2c12 + n
= ✓1 
1 + d/(6c1)◆(1+d/(6c1)1)! 6c1
2 r

d + 6c1◆d

1
pn

1
pn

e 3c1

c2

1

4

e 3c1

d · d

4

2 .

 e 3c1
2 r c2

2d + 12c1

.

2d + 4c12 + n
m 
2 r c2

and

1
8

>

2

1

d + 6c1

>

1

d + 1

|MMDk(P0  Q0)  MMDk(P1  Q1)|
Now we set c1 = 0.16  c2 = 0.23. Checking that Condition (7) is satisﬁed and noting that

max 1

4

e c2
2c1  

1 pc2/(4c1)

2

! >

1
5

 

1
2

e 3c1

we conclude the proof with an application of Theorem 3.

3.2 Proof of Theorem 2
First  we repeat the argument presented in the proof of Theorem 1 to bring ourselves into the context
of minimax estimation  introduced in the beginning of Section 3.1. Namely  we reduce the class of
distributions P to its subset Gd containing all the multivariate Gaussian distributions over Rd with
covariance matrices proportional to identity matrix Id 2 Rd⇥d. The proof is based on Theorem 4 and
treats two cases m  n and m < n separately. We consider only the case m  n as the second one
follows the same steps.
In order to apply Theorem 4 we need to choose two “fuzzy hypotheses”  that is two probability
distributions µ0 and µ1 over ⇥.
In our setting there is a one-to-one correspondence between
parameters ✓ 2 ⇥ and pairs of Gaussian distributions (G1  G2) 2G d ⇥G d. Throughout the proof it
will be more convenient to treat µ0 and µ1 as distributions over Gd ⇥G d. We will set µ0 to be a Dirac
measure supported on (P0  Q0) with P0 = Q0 = G(0  2Id). Clearly  MMDk(P0  Q0) = 0. This
gives

and the ﬁrst inequality of Condition 1 in Theorem 4 holds with c = 0 and 0 = 0. Next we set µ1 to
be a distribution of a random pair (P  Q) with

µ0✓ : F (✓) = 0 = 1

Q = Gd(0  2Id)  P = Gd(µ  2Id) 

2 =

1

2t1d

 

where µ ⇠ Pµ for some probability distribution Pµ over Rd to be speciﬁed later. Next we are going to
check Condition 2 of Theorem 4. For D = (x1  . . .   xn  y1  . . .   ym) deﬁne “posterior” distributions

as in Theorem 4. Using Markov’s inequality we write

Pi(D) =Z P✓(D)µi(d✓) 
<⌧ ◆ = P1✓ dP1

dP0

P1✓ dP0

dP1

i 2{ 0  1}

>⌧ 1◆  ⌧E1 dP1
dP0 .

(8)

6

We have

2

22

dP0

hPn

j=1 xj  µi

dPµ(µ).

dP1
dP0

e nkµk2

22 dPµ(µ)

e nkµk2
22 e

k=1 e kykk2
k=1 e kykk2
Now we compute the expected value appearing in (8):

(D) = RRdQn
22 Qm
j=1 e kxjµk2
Qn
22 Qm
j=1 e kxjk2
ED⇠P1 dP1
(D) =ZRd
=ZRd

=ZRd
22 ED⇠P1hehPn
j=1 xj   µi/2i dPµ(µ)
Ee
22 ✓ZRd
2DPn
j   µE ⇠ Gnhµ0  µi  n 2kµk2. Using
j ⇠ Gd(nµ0  n 2Id) and as a resultDPn
j   µE = e

Pn
the closed form for the moment generating function of a Gaussian distribution Z ⇠ G(µ  2) 
E⇥etZ⇤ = eµte 1

n are independent and distributed according to Gd(µ0  2Id). Note that

j   µE dPµ(µ0)◆ dPµ(µ) 

where X µ0
j=1 X µ0

1   . . .   X µ0

2 2t2  we get

j=1 X µ0

e nkµk2

nkµk2
22 .

j=1 X µ0

(9)

e

1

Together with (9) this gives

ED⇠P1 dP1

dP0

(D) =ZRd

22 dPµ(µ0)◆ dPµ(µ) = Ee

nhµ0 µi

2   

(10)

where µ and µ0 are independent random variables both distributed according to Pµ. Now we set Pµ
to be a uniform distribution in the d-dimensional cube of appropriate size

In this case  using Lemma B.1 presented in Appendix B we get

Ee

nhµ0 µi

2  =

dYi=1

Using (10) and also assuming

dnt1◆ =✓ 1

4c2
1

1◆d
Shi2c2

.

(11)

1

e

e

2

2

nkµk2

nhµ0 µi

nhµ0 µi

j=1 X µ0

e nkµk2

Ee
2DPn
22 ✓ZRd
Pµ := Uhc1/pdnt1  c1/pdnt1id
Ee
2  =

dn2t1
2nc2
1

nµiµ0i

c2
1

2

.

Shi✓ n
1  1

1
4c2
1

dYi=1
Shi2c2
(D) 
<⌧ ⌘  ⌧

we get

ED⇠P1 dP1
Combining with (8) we ﬁnally get P1⇣ dP0
1  ⌧
Finally  we need to check the second inequality of Condition 1 in Theorem 4. Take two Gaussian
distributions P = Gd(µ  2Id) and Q = Gd(0  2Id). Using [21  Eq. 30] we have

Shi2c2
1 .
dP1  ⌧⌘ 
1 or equivalently P1⇣ dP0
Shi2c2
1 . This shows that Condition 2 of Theorem 4 is satisﬁed with ↵ = ⌧
Shi2c2
1.

Shi2c2

1
4c2
1

dP0

dP1

4c2
1

4c2
1

4c2
1

MMD2

k(P  Q) 

2

t0

e ✓1 

2 + d◆kµk2
and t1kµk2  1 + 4t12.

1

given

2t1d

2 =

(12)
Notice that the largest diagonal of a d-dimensional cube scales as pd. Using this we conclude that
for µ ⇠ Pµ with probability 1 it holds that kµk2  c2
t1n and the second condition in (12) holds as
long as c2
t1en)  P

(P Q)⇠µ1(MMDk(P  Q)  c2r t0

1  n. Using this we get for any c2 > 0

µ⇠Pµ⇢kµk2 

t1n✓ 2 + d

d ◆ .

(13)

c2
2

P

1

7

Note that for µ ⇠ Pµ  kµk2 =Pd

computations show that

i=1 µ2

i is a sum of d i.i.d. bounded random variables. Also simple

Ekµk2 =

dXi=1

Eµ2

i = d

c2
1

3dnt1

=

c2
1
3nt1

and

Vkµk2 =

Vµ2

i =

4c4
1

45dn2t2
1

.

dXi=1

Using Chebyshev-Cantelli’s inequality of Theorem B.2 (Appendix B) we get for any ✏> 0

or equivalently for any ✏> 0 

P

µ⇠Pµkµk2 > Ekµk2 + ✏  1 
µ⇠Pµkµk2  Ekµk2  ✏ = 1  P
µ⇠Pµ⇢kµk2  c2
1✓ 1
P
2 ⇣ c2
c1⌘2
2  9p5
p5

nt1  1 

  we can further lower bound (13):

3p5d◆ 1

1 + ✏2 .

3 

2✏

1

Choosing ✏ 

P

t1en)  P

(P Q)⇠µ1(MMDk(P  Q)  c2r t0

µ⇠Pµ⇢kµk2  c2
1✓ 1
2 ⇣ c2
c1⌘2
p5
2  9p5
We ﬁnally set ⌧ = 0.4  c1 = 0.8  c2 = 0.1 ✏ =
and the second condition of (12) are satisﬁed  while
1+✏2⌘
1  1

⌧⇣1  ⌧

Shi2c2

1 + ⌧

4c2
1

>

1
14

3 

2✏

3p5d◆ 1

nt1  1

  and check that inequality (11)

.

1

1 + 45dn2t2

1

4c4
1

✏2

1

1 + ✏2 .

We complete the proof by application of Theorem 4.

4 Discussion

In this paper  we provided the ﬁrst known lower bounds for the estimation of maximum mean
discrepancy (MMD) based on ﬁnite random samples. Based on this result  we established the minimax
rate optimality of the empirical estimator. Interestingly  we showed that for radial kernels on Rd  the
optimal speed of convergence depends only on the properties of the kernel and is independent of d.
However  the paper does not address an important question about the minimax rates for MMD based
tests. We believe that the minimax rates of testing with MMD matches with that of the minimax rates
for MMD estimation and we intend to build on this work in future to establish minimax testing results
involving MMD.
Since MMD is an integral probability metric (IPM) [11]  a related problem of interest is the minimax
estimation of IPMs.
IPM is a class of distances on probability measures  which is deﬁned as
(P  Q) := sup{R f (x) d(P  Q)(x) : f 2F}   where F is a class of bounded measurable
functions on a topological space X with P and Q being Borel probability measures. It is well known
[16] that the choice of F = {f 2H : kfkH  1} yields MMDk(P  Q) where H is a reproducing
kernel Hilbert space with a bounded reproducing kernel k. [16] studied the empirical estimation
of (P  Q) for various choices of F and established the consistency and convergence rates for the
empirical estimator. However  it remains an open question as to whether these rates are minimax
optimal.

References
[1] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and

Statistics. Kluwer Academic Publishers  London  UK  2004.

[2] S. Boucheron  G. Lugosi  and P. Massart. Concentration Inequalities: A Nonasymptotic Theory

of Independence. Oxford University Press  2013.

8

[3] K. Fukumizu  A. Gretton  X. Sun  and B. Schölkopf. Kernel measures of conditional dependence.
In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information
Processing Systems 20  pages 489–496  Cambridge  MA  2008. MIT Press.

[4] K. Fukumizu  L. Song  and A. Gretton. Kernel Bayes’ rule: Bayesian inference with positive

deﬁnite kernels. J. Mach. Learn. Res.  14:3753–3783  2013.

[5] A. Gretton  K. M. Borgwardt  M. Rasch  B. Schölkopf  and A. Smola. A kernel method for the
two sample problem. In B. Schölkopf  J. Platt  and T. Hoffman  editors  Advances in Neural
Information Processing Systems 19  pages 513–520  Cambridge  MA  2007. MIT Press.

[6] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Schölkopf  and A. J. Smola. A kernel two-sample

test. Journal of Machine Learning Research  13:723–773  2012.

[7] A. Gretton  K. Fukumizu  C. H. Teo  L. Song  B. Schölkopf  and A. J. Smola. A kernel statistical
test of independence. In J. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in
Neural Information Processing Systems 20  pages 585–592. MIT Press  2008.

[8] E. L. Lehmann and G. Casella. Theory of Point Estimation. Springer-Verlag  New York  2008.
[9] D. Lopez-Paz  K. Muandet  B. Schölkopf  and I. Tolstikhin. Towards a learning theory of cause-
effect inference. In Proceedings of the 32nd International Conference on Machine Learning 
ICML 2015  Lille  France  6-11 July 2015  2015.

[10] K. Muandet  B. Sriperumbudur  K. Fukumizu  A. Gretton  and B. Schölkopf. Kernel mean

shrinkage estimators. Journal of Machine Learning Research  2016. To appear.

[11] A. Müller. Integral probability metrics and their generating classes of functions. Advances in

Applied Probability  29:429–443  1997.

[12] I. J. Schoenberg. Metric spaces and completely monotone functions. The Annals of Mathematics 

39(4):811–841  1938.

[13] A. J. Smola  A. Gretton  L. Song  and B. Schölkopf. A Hilbert space embedding for distributions.
In Proceedings of the 18th International Conference on Algorithmic Learning Theory (ALT) 
pages 13–31. Springer-Verlag  2007.

[14] L. Song  A. Smola  A. Gretton  J. Bedo  and K. Borgwardt. Feature selection via dependence

maximization. Journal of Machine Learning Research  13:1393–1434  2012.

[15] L. Song  X. Zhang  A. Smola  A. Gretton  and B. Schölkopf. Tailoring density estimation via
reproducing kernel moment matching. In Proceedings of the 25th International Conference on
Machine Learning  ICML 2008  pages 992–999  2008.

[16] B. K. Sriperumbudur  K. Fukumizu  A. Gretton  B. Schölkopf  and G. R. G. Lanckriet. On the
empirical estimation of integral probability metrics. Electronic Journal of Statistics  6:1550–
1599  2012.

[17] B. K. Sriperumbudur  K. Fukumizu  and G. R. G. Lanckriet. Universality  characteristic kernels

and RKHS embedding of measures. J. Mach. Learn. Res.  12:2389–2410  2011.

[18] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Schölkopf  and G. R. G. Lanckriet. Hilbert
space embeddings and metrics on probability measures. J. Mach. Learn. Res.  11:1517–1561 
2010.

[19] I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.
[20] Z. Szabó  A. Gretton  B. Póczos  and B. K. Sriperumbudur. Two-stage sampled learning
theory on distributions. In Proceedings of the Eighteenth International Conference on Artiﬁcial
Intelligence and Statistics  volume 38  pages 948–957. JMLR Workshop and Conference
Proceedings  2015.

[21] I. Tolstikhin  B. Sriperumbudur  and K. Muandet. Minimax estimation of kernel mean embed-

dings. arXiv:1602.04361 [math.ST]  2016.

[22] A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer  NY  2008.

9

,Ilya Tolstikhin
Bharath Sriperumbudur
Bernhard Schölkopf