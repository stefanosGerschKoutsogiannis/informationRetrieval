2010,Decomposing Isotonic Regression for Efficiently Solving Large Problems,A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efficient methods for each partitioning subproblem through an equivalent representation as a network flow problem  and prove that this sequence of partitions converges to the global solution. These network flow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm's favorable computational properties are demonstrated through simulated examples as large as 2x10^5 variables and 10^7 constraints.,Decomposing Isotonic Regression for Efﬁciently

Solving Large Problems

Ronny Luss

Dept. of Statistics and OR

Tel Aviv University

Saharon Rosset

Dept. of Statistics and OR

Tel Aviv University

Moni Shahar

Dept. of Electrical Eng.

Tel Aviv University

ronnyluss@gmail.com

saharon@post.tau.ac.il

moni@eng.tau.ac.il

Abstract

A new algorithm for isotonic regression is presented based on recursively par-
titioning the solution space. We develop efﬁcient methods for each partitioning
subproblem through an equivalent representation as a network ﬂow problem  and
prove that this sequence of partitions converges to the global solution. These net-
work ﬂow problems can further be decomposed in order to solve very large prob-
lems. Success of isotonic regression in prediction and our algorithm’s favorable
computational properties are demonstrated through simulated examples as large
as 2 × 105 variables and 107 constraints.

1 Introduction

Assume we have a set of n data observations (x1  y1)  ...  (xn  yn)  where x ∈ X (usually X =Rp)
is a vector of covariates or independent variables  y ∈ R is the response  and we wish to ﬁt a
model ˆf : X → R to describe the dependence of y on x  i.e.  y ≈ ˆf (x). Isotonic regression is a
non-parametric modeling approach which only restricts the ﬁtted model to being monotone in all
independent variables [1]. Deﬁne G as the family of isotonic functions  that is  g ∈ G satisﬁes

x1 (cid:22) x2 ⇒ g(x1) ≤ g(x2) 

where the partial order (cid:22) here will usually be the standard Euclidean one  i.e.  x1 (cid:22) x2 if x1j ≤ x2j
∀j. Given these deﬁnitions  isotonic regression solves

ˆf = arg min
g∈G

ky − g(x)k2.

(1)

As many authors have noted  the optimal solution to this problem comprises a partitioning of the
space X into regions obeying a monotonicity property with a constant ﬁtted to ˆf in each region.
It is clear that isotonic regression is a very attractive model for situations where monotonicity is a
reasonable assumption  but other common assumptions like linearity or additivity are not. Indeed 
this formulation has found useful applications in biology [2]  medicine [3]  statistics [1] and psy-
chology [4]  among others. Practicality of isotonic regression has already been demonstrated in
various ﬁelds and in this paper we focus on algorithms for computing isotonic regressions on large
problems.
An equivalent formulation of L2 isotonic regression seeks an optimal isotonic ﬁt ˆyi at every point
by solving

minimize

subject to

n

X

(ˆyi − yi)2

i=1
ˆyi ≤ ˆyj

∀(i  j) ∈ I

(2)

where I denotes a set of isotonic constraints. This paper assumes that I contains no redundant
constraints  i.e. (i  j)  (j  k) ∈ I ⇒ (i  k) 6∈ I. Problem (2) is a quadratic program subject to

1

simple linear constraints  and  according to a literature review  appears to be largely ignored due to
computational difﬁculty on large problems. The worst case O(n4) complexity (a large overstatement
in practice as will be shown) has resulted in overlooking the results that follow [5  6].

The discussion of isotonic regression originally focused on the case x ∈ R  where (cid:22) denoted a com-
plete order [4]. For this case  the well known pooled adjacent violators algorithm (PAVA) efﬁciently
solves the isotonic regression problem. For the partially ordered case  many different algorithms
have been developed over the years  with most early efforts concentrated on generalizations of PAVA
[7  5]. These algorithms typically have no polynomial complexity guarantees and are impractical
when data size exceed a few thousand observations. Problem (1) can also be treated as a separa-
ble quadratic program subject to simple linear equality constraints. Such was done  for example 
in [8]  which applies active set methods to solve the problem. While such algorithms can often be
efﬁcient in practice  the algorithm of [8] gives no complexity guarantees. Related algorithms in [9]
to those described here were applied to problems for scheduling reorder intervals in production sys-
tems and are of complexity O(n4) and connections to isotonic regression can be made through [1].
Interior point methods are another tool for solving Problem (1)  and have time complexity guaran-
tees of O(n3) when the number of constraints is on the same order as the number of variables (see
[10]). However  the excessive memory requirements of interior point methods from solving large
systems of linear equations typically make them impractical for large data sizes. Recently  [6] and
[11] gave an O(n2) approximate generalized PAVA algorithm  however solution quality can only be
demonstrated via experimentation. An even better complexity of O(n log n) can be obtained for the
optimal solution when the isotonic constraints take a special structure such as a tree  e.g. [12].

1.1 Contribution

Our novel approach to isotonic regression offers an exact solution of (1) with a complexity bounded
by O(n4)  but acts on the order of O(n3) for practical problems. We demonstrate here that it accom-
modates problems with tens of thousands of observations  or even more with our decomposition. The
main goal of this paper is to make isotonic regression a reasonable computational tool for large data
sets  as the assumptions in this framework are very applicable in real-world applications. Our frame-
work solves quadratic programs with 2 × 105 variables and more than 107 constraints  a problem
of size not solved anywhere in previous isotonic regression literature  and with the decomposition
detailed below  even larger problems can be solved.

The paper is organized as follows. Section 2 describes a partitioning algorithm for isotonic re-
gression and proves convergence to the globally optimal solution. Section 3 explains how the sub-
problems (creating a single partition) can be solved efﬁciently and decomposed in order to solve
large-scale problems. Section 4 demonstrates that the partitioning algorithm is signiﬁcantly better
in practice than the O(n4) worst-case complexity. Finally  Section 5 gives numerical results and
demonstrates favorable predictive performance on large simulated data sets and Section 6 concludes
with future directions.
Notation
The weight of a set of points A is deﬁned as yA = 1
|A| Pi∈A yi. A subset U of A is an upper set
of A if x ∈ U   y ∈ A  x ≺ y ⇒ y ∈ U. A set B ⊆ A is deﬁned as a block of A if yU ∩B ≤ yB
for each upper set U of A such that U ∩ B 6= {}. A general block A is considered a block of the
entire space. For two blocks A and B  we denote A (cid:22) B if ∃x ∈ A  y ∈ B such that x (cid:22) y and
∄x ∈ A  y ∈ B such that y (cid:22) x (i.e. there is at least one comparable pair of points that satisfy the
direction of isotonicity). A and B are then said to be isotonic blocks (or obey isotonicity). A group
of nodes X majorizes (minorizes) another group Y if X (cid:23) Y (X (cid:22) Y ). A group X is a majorant
(minorant) of X ∪ A where A = ∪k

i=1Ai if X 6(cid:22) Ai (X 6(cid:23) Ai) ∀i = 1 . . . k.

2 Partitioning Algorithm

We ﬁrst describe the structure of the classic L2 isotonic regression problem and continue to detail
the partitioning algorithm. The section concludes by proving convergence of the algorithm to the
globally optimal isotonic regression solution.

2

2.1 Structure

Problem (2) is a quadratic program subject to simple linear constraints. The structure of the opti-
mal solution to (2) is well-known. Observations are divided into k groups where the ﬁts in each
group take the group mean observation value. This can be seen through the equations given by the
following Karush-Kuhn-Tucker (KKT) conditions:

(a) ˆyi = yi −

1
2

( X
j:(i j)∈I

λij − X

λji)

j:(j i)∈I

(b) ˆyi ≤ ˆyj ∀(i  j) ∈ I
(c) λij ≥ 0 ∀(i  j) ∈ I
(d) λij (ˆyi − ˆyj) = 0 ∀(i  j) ∈ I.

This set of conditions exposes the nature of the optimal solution  since condition (d) implies that
λij > 0 ⇒ ˆyi = ˆyj. Hence λij can be non-zero only within blocks in the isotonic solution which
have the same ﬁtted value. For observations in different blocks  λij = 0. Furthermore  the ﬁt within
each block is trivially seen to be the average of the observations in the block  i.e. the ﬁts minimize the
block’s squared loss. Thus  we get the familiar characterization of the isotonic regression problem
as one of ﬁnding a division into isotonic blocks.

2.2 Partitioning

In order to take advantage of the optimal solution’s structure  we propose solving the isotonic re-
gression problem (2) as a sequence of subproblems that divides a group of nodes into two groups
at each iteration. An important property of our partitioning approach is that nodes separated at one
iteration are never rejoined into the same group in future iterations. This gives a clear bound on the
total number of iterations in the worst case.

We now describe the partitioning criterion used for each subproblem. Suppose a current block V is
i = yV ∀i ∈ V. From condition (a) of the KKT conditions  we deﬁne the net
optimal and thus ˆy∗
outﬂow of a group V as Pi∈V (yi − ˆyi). Finding two groups within V such that the net outﬂow from
the higher group is greater than the net outﬂow from the lower group should be infeasible  according
to the KKT conditions. The partition here looks for two such groups. Denote by CV the set of all
feasible (i.e. isotonic) cuts through the network deﬁned by nodes in V. A cut is called isotonic if the
two blocks created by the cut are isotonic. The optimal cut is determined as the cut that solves the
problem

max
c∈CV

X
i∈V +

c

(yi − yV) − X
i∈V −

c

(yi − yV )

(3)

c (V +

c ) is the group on the lower (upper) side of the edges of cut c. In terms of isotonic
where V −
regression  the optimal cut is such that the difference in the sum of the normalized ﬁts (yi − yV ) at
each node of a group is maximized. If this maximized difference is zero  then the group must be an
optimal block. The optimal cut problem (3) can also be written as the binary program

maximize Pi xi(yi − yV )
subject to xi ≤ xj
xi ∈ {−1  +1}

∀(i  j) ∈ I
∀i ∈ V.

(4)

Well-known results from [13] (due to the fact that the constraint matrix is totally unimodular) say
that the following relaxation to this binary program is optimal with x∗ on the boundary  and hence
the optimal cut can be determined by solving the linear program

maximize
subject to xi ≤ xj

zT x

∀(i  j) ∈ I

(5)

−1 ≤ xi ≤ 1 ∀i ∈ V

where zi = yi − yV. This group-wise partitioning operation is the basis for our partitioning al-
gorithm which is explicitly given in Algorithm 1. It starts with all observations as one group (i.e. 
V = {1  . . .   n})  and recursively splits each group optimally by solving subproblem (5). At each

3

iteration  a list C of potential optimal cuts for each group generated thus far is maintained  and the
cut among them with the highest objective value is performed. The list C is updated with the opti-
mal cuts in both sub-groups generated. Partitioning ends whenever the solution to (5) is trivial (i.e. 
no split is found because the group is a block). As proven next  this algorithm terminates with the
optimal global (isotonic) solution to the isotonic regression problem (2).

Let (val  w−  w+) ∈ C be the potential cut with largest val.
Update V = (V \ (w− ∪ w+)) ∪ {w−  w+}  C = C \ (val  w−  w+) .
for all v ∈ {w−  w+} do

Algorithm 1 Paritioning Algorithm
Require: Observations y1  . . .   yn and partial order I.
Require: V = {{1  . . .   n}}  C = {(0  {1  . . .   n}  {})}  W = {}.
1: while V 6= {} do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
end if
12:
end for
13:
14: end while
15: return W the optimal groups

Let v− = {i : x∗
Update C = C ∪ {(zT x∗  v−  v+)}

1 = . . . = x∗
Update V = V \ v and W = W ∪ v.

i = −1}  v+ = {i : x∗

Set zi = yi − yv ∀i ∈ v where yv is the mean of observations in v.
Solve LP (5) with input z and get x∗.
if x∗

n (group is optimally divided) then

i = +1}.

else

2.3 Convergence

Theorem 1 next states the main result that allows for a no-regret partitioning algorithm for isotonic
regression. This will lead to our convergence result. We assume that group V is isotonic (i.e. has no
holes) and is the union of optimal blocks.

Theorem 1 Assume a group V is a union of blocks from the optimal solution to problem (2). Then
a cut made by solving (5) does not cut through any block in the global optimal solution.

k (M U

Proof. The following is a brief sketch of the proof idea: Let M be the union of K optimal blocks in
V that get broken by the cut. Deﬁne M1 (MK) to be a minorant (majorant) block in M. For each Mk
k ) as the groups in Mk below (above) the algorithm cut. Using the deﬁnitions of how
deﬁne M L
< yMK by
the algorithm makes partitions  the following two consequences can be proven: (1) yM1
optimality (i.e. according to KKT conditions) and isotonicity and (2) yM1
> yV and yMK < yV.
This is proven by showing that yM U
1 block would be on the
> yV  because otherwise the M U
> yV since
lower side of the cut  resulting in M1 being on the lower side of the cut  and thus yM1
by the optimality assumption on block M1 (with symmetric arguments for MK). This
yM L
leads to the contradiction yV < yM1

< yMK < yV  and hence M must be empty.

> yM U

1

1

1

Since Algorithm 1 starts with V = {1  ...  n} which is a union of (all) optimal blocks  we can
conclude from this theorem that partitions never cut an optimal block. The following corollary is
then a direct consequence of repeatedly applying Theorem 1 in Algorithm 1:

Corollary 2 Algorithm 1 converges to the global optimal solution of (2) with no regret (i.e. without
having to rejoin observations that are divided at a previous iteration).

3 Efﬁcient solutions of the subproblems

Linear program (5) has a special structure that can be taken advantage of in order to solve larger
problems faster. We ﬁrst show why these problems can be solved faster than typical linear programs 
followed by a novel decomposition of the structure that allows problems of extremely large size to
be solved efﬁciently.

4

3.1 Network ﬂow problems

The dual to Problem (2) is a network ﬂow problem with quadratic objective. The network ﬂow
constraints are identical to those in (6) below  but the objective is 1
i )  which  to the
author’s knowledge  currently still precludes this dual from being efﬁciently solved with special
network algorithms.

i=1 (s2

4 Pn

i + t2

While this structure does not help solve directly the quadratic program  the network structure allows
the linear program for the subproblems to be solved very efﬁciently. The dual program to (5) is

minimize X
i∈V

(si + ti)

subject to X

λij − X

λji − si + ti = zi ∀i ∈ V

j:(i j)∈I
λ  s  t ≥ 0

j:(j i)∈I

(6)

where again zi = yi − yV. Linear program (6) is a network ﬂow problem with |V| + 2 nodes and
|I| + 2|V| arcs. Variable s denotes links directed from a source node into each other node  while
t denotes links connecting each node into a sink node. The network ﬂow problem here minimizes
the total sum of ﬂow over links from the source and into the sink with the goal to leave zi units of
ﬂow at each node i ∈ V. Note that this is very similar to the network ﬂow problem solved in [14]
where zi there represents the classiﬁcation performance on node i. Specialized simplex methods for
such network ﬂow problems are typically much faster ([15] documents an average speedup factor
of 10 to 100 over standard simplex solvers) due to several reasons such as simpler operations on
network data structures rather than maintaining and operating on the simplex tableau (see [16] for
an overview of network simplex methods).

3.2 Large-scale decompositions

In addition to having a very efﬁcient method for solving this network ﬂow problem  further enhance-
ments can be made on extremely large problems of similar structure that might suffer from memory
problems. It is already assumed that no redundant arcs exist in I (i.e. (i  j)  (j  k) ∈ I ⇒ (i  k) 6∈
I). One simple reduction involves eliminating negative (positive) nodes  i.e. nodes with zi < 0
(zi ≥ 0) where where zi = yi − yV  that are bounded only from above (below). It is trivial to
observe that these nodes will be be equal to −1 (+1) in the optimal solution and that eliminating
them does not affect solving (5) without them. However  in practice  this trivial reduction has a
computationally minimal affect on large data sets. These reductions were also discussed in [14].

We next consider a novel reduction for the primal linear program (5). The main idea is that it can
be solved through a sequence of smaller linear programs that reduce the total size of the full linear
program on each iteration. Consider a minorant group of nodes J ⊆ V and the subset of arcs
IJ ⊆ I connecting them. Solving problem (5) on this reduced network with the original input z
divides the nodes in J into a lower and upper group  denoted JL and JU . Nodes in JL are not
bounded from above and will be in the lower group of the full problem solved on V. In addition 
the same problem solved on the remaining nodes in V \ JL will give the optimal solutions to these
nodes. This is formalized in Proposition 3.

Proposition 3 Let J ⊆ V be a minorant group of nodes in V. Let w∗ and x∗ be optimal solutions
to Problem (5) on the reduced set J and full set V of nodes  respectively. If w∗
i = −1
∀i ∈ J . The optimal solution for the remaining nodes (V \ J ) can be found by solving (5) over only
those nodes. The same claims can be made when J ⊆ V is a majorant group of nodes in V where
instead w∗

i = −1  then x∗

i = +1 ∀i ∈ J .

i = +1 ⇒ x∗

i = −1 and ˆW = V \ W. Clearly  the solution to
Proof. Denote W the set of nodes such that w∗
Problem (5) over nodes in W has the solution with all variables equal to −1. Problem (5) can be
written in the following form with separable objective:

maximize X
i∈W

zixi + X
i∈V\W

subject to xi ≤ xj
xi ≤ xj
−1 ≤ xi ≤ 1

zixi

5

∀(i  j) ∈ I  i  j ∈ W
∀(i  j) ∈ I  i ∈ V  j ∈ V \ W
∀i ∈ V

(7)

Start with an initial solution xi = 1 ∀i ∈ V. Variables in W can be optimized over ﬁrst and by
assumption have the optimal value with all variables equal to −1. Optimization over variables in ˆW
is not bounded from below by variables in W since those variables are all at the lower bound. Hence
the optimal solution to variables in ˆW is given by optimizing over only these variables. The result
for minorant groups follows. The ﬁnal claim is easily argued in the same way as for the minorant
groups.

Given Proposition 3  Algorithm 2  which iteratively solves (5)  can be stated. The subtrees are built
as follows. First  an upper triangular adjacency matrix C can be constructed to represent I  where
Cij = 1 if xi ≤ xj is an isotonic constraint and Cij = 0 otherwise. A minorant (majorant) subtree
with k nodes is then constructed as the upper left (lower right) k × k sub-matrix of C.

Algorithm 2 Iterative algorithm for linear program (5)
Require: Observations y1  . . .   yn and partial order I.
Require: M AXSIZE of problem to be solved by general LP solver
Require: V = {1  . . .   n}  L = U = {}.
1: while |V| ≥ M AXSIZE do
2:
3:
4:
5:
6:
7:
8:
9:
10: end while
11: Solve linear program (5) on V and get solution ˆy ∈ {−1  +1}|V|.
12: L = L ∪ {v ∈ T : ˆyv = −1}  U = U ∪ {v ∈ T : ˆyv = +1}.

ELIMINATE A MINORANT SET OF NODES:
Build a minorant subtree T .
Solve linear program (5) on T and get solution ˆy ∈ {−1  +1}|T |.
L = L ∪ {v ∈ T : ˆyv = −1}  V = V \ {v ∈ T : ˆyv = −1}.
ELIMINATE A MAJORANT SET OF NODES:
Build majorant subtree T .
Solve linear program (5) on T and get solution ˆy ∈ {−1  +1}|T |.
U = U ∪ {v ∈ T : ˆyv = +1}  V = V \ {v ∈ T : ˆyv = +1}.

The computational bottleneck of Algorithm 2 is solving linear program (5)  which is done efﬁciently
by solving the dual network ﬂow problem (6). This shows that  if the ﬁrst network ﬂow problem is
too large to solve  it can be solved by a sequence of smaller network ﬂow problems as illustrated
in Figure 1. Lemma 4 below proves that this reduction optimally solves the full problem (5). In
the worst case  many network ﬂow problems will be solved until the original full-size network
ﬂow problem is solved. However  in practice on large problems  this artifact is never observed.
Computational performance of this reduction is demonstrated in Section 5.

Lemma 4 Algorithm 2 optimally solves Problem (5).

Proof. The result follows from repeated application of Proposition 3 over the set of nodes V that has
not yet been optimally solved for.

4 Complexity of the partitioning algorithm

Linear program (5) can be solved in O(n3) using interior point methods. Given that the algorithm
performs at most n iterations  the worst case complexity of Algorithm 1 is O(n4). However  the
practical complexity of IRP is signiﬁcantly better than the worst case. Each iteration of LP (5)
solves smaller problems. Consider the case of balanced partitioning at each iteration until there
are n ﬁnal blocks. In this case  we can represent the partitioning path as a binary tree with log n
levels  and at each level k  LP (5) is solved 2k times on instances of size n
2k which leads to a total
complexity of

log n

X

k=0

2k(

n

2k )3 = n3(

log n

X

k=0

(

1
4

)k) = n3(

1 − .25log n+1

.75

) 

subject to additional constants. For n ≥ 10  the summation is approximately 1.33  and hence in this
case the partitioning algorithm has complexity O(1.33n3) (considering the complexity of interior

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.05

0.1

0.15

0.2

0.05

0.1

0.15

0.2

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.05

0.1

0.15

0.2

0.05

0.1

0.15

0.2

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.05

0.1

0.15

0.2

0.05

0.1

0.15

0.2

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.05

0.1

0.15

0.2

0.05

0.1

0.15

0.2

Figure 1: Illustration of LP (5) decomposition. Data here is 2 dimensional with only 1000 nodes in order to
leave a clear picture. First 7 iterations and the ﬁnal iteration 16 of the decomposition are shown from left to
right and top to bottom. The remaining nodes (blue circles) to identify as ±1 decreases through the iterations.
LP (5) solved on the entire set of nodes in the ﬁrst picture may be too large for memory. Hence subproblems are
solved on the lower left (red dots) and upper right (green dots) of the networks and some nodes are ﬁxed from
the solution of these subproblems. This is repeated until the number of unidentiﬁed nodes in the last iteration
is of small enough size for memory. Note that at each iteration the three groups obey isotonicity.

point methods for partitioning). More generally  let p and 1 − p be the percentages on each split.
Table 1 displays the constants c representing the complexity from O(cn3) over varying p and n. As
demonstrated  the problem size rapidly decreases and the complexity is in practice O(n3).

p=0.55
p=0.65
p=0.75
p=0.85
p=0.95

n=100
1.35n3
1.46n3
1.77n3
2.56n3
6.41n3

n=1000
1.35n3
1.46n3
1.78n3
2.61n3
6.94n3

n=10000
1.35n3
1.47n3
1.78n3
2.61n3
7.01n3

Table 1: Complexity: Groups are split with ratio p at each iteration. Complexity in practice is O(n3).

5 Numerical experiments

We here demonstrate that exact isotonic regression is computationally tractable for very large prob-
lems  and compare against the time it takes to get an approximation. We ﬁrst show the computational
performance of isotonic regression on simulated data sets as large as 2 × 105 training points with
more than 107 constraints. We then show the favorable predictive performance of isotonic regression
on large simulated data sets.

5.1 Large-Scale Computations

Figure 2 demonstrates that the partitioning algorithm with decompositions of the partitioning step
can solve very large isotonic regressions. Three dimensional data is simulated from U(0  2) and the
responses are created as linear functions plus noise. The size of the training sets varies from 104
to 2 × 105 points. The left ﬁgure shows that the partitioning algorithm ﬁnds the globally optimal
isotonic regression solution in not much more time than it takes to ﬁnd an approximation as done
in [6] for very large problems. Although the worst-case complexity of our exact algorithm is much
worse  the two algorithms scale comparably in practice.

Figure 2 (right) shows how the number of partitions (left axis) increases as the number of training
points increases. It is not clear why the approximation in [6] has less partitions as the size of the
problem grows. More partitions (left axis) require solving more network ﬂow problems  however 
as discussed  they reduce in size very quickly over the partitioning path  resulting in the practical
complexity seen in the ﬁgure on the left. The bold black line also shows the number of constraints
(right axis) which goes up to more than 107 constraints.

7

Time vs # Training Points

# Partitions vs # Training Points

IRP
GPAV

)
s
d
n
o
c
e
s
(

e
m
T

i

450

400

350

300

250

200

150

100

50

0

 
0

 

IRP
GPAV

2
x 105

7000

6000

5000

4000

3000

2000

1000

0

 
0
0

s
n
o
i
t
i
t
r
a
P
r
e
b
m
u
N

0.5

1

1.5

Number Training Points

x 106
 
10

9

8
s
t
n
7
i
a
r
6
t
s
n
5
o
C
4
r
e
b
3
m
u
2
N
1

0.5
0.5

1
1

1.5
1.5

Number Training Points

0

2
2
x 105
x 105

Figure 2: IRP performance on large-scale simulations. Data x ∈ R3 has xi ∼ U(0  2). Responses y are linear
functions plus noise. Number of training points varies from 104 to 2 × 105. Results shown are averages of 5
simulations with dotted lines at ± one standard deviation. Time (seconds) versus number of training points is
on the left. On the right  the number of partitions is illustrated using the left axis and the bold black line shows
the average number of constraints per test using the right axis.

5.2 Predictive Performance

Here we show that isotonic regression is a useful tool when the data ﬁts the monotonic framework.
Data is simulated as above and responses are constructed as yi = Qi xi + N (0  .52) where p varies
from 2 to 6. The training set varies from 500 to 5000 to 50000 points and the test size is ﬁxed at 5000.
Results are averaged over 10 trials and 95% conﬁdence intervals are given. A comparison is made
between isotonic regression and linear least squares regression. With only 500 training points  the
model is poorly ﬁtted and a simple linear regression performs much better. 5000 training points is
sufﬁcient to ﬁt the model well with up to 4 dimensions  after which linear regression outperforms the
isotonic regression  and 50000 training points ﬁts the model well up with up to 5 dimensions. Two
trends are observed. Larger training sets allow better models to be ﬁt which improves performance
while higher dimensions increase overﬁtting which  in turn  decreases performance.

Dim

2
3
4
5
6

IRP MSE

n=500

0.69 ± 0.01
0.76 ± 0.03
1.45 ± 0.08
4.61 ± 0.65
12.89 ± 1.30

LS MSE
n=500

0.37 ± 0.00
0.65 ± 0.01
1.08 ± 0.01
1.76 ± 0.02
3.06 ± 0.04

IRP MSE
n=5000

0.27 ± 0.00
0.31 ± 0.00
0.61 ± 0.02
2.61 ± 0.16
8.41 ± 1.36

LS MSE
n=5000

0.36 ± 0.00
0.61 ± 0.01
1.08 ± 0.02
1.88 ± 0.04
2.84 ± 0.07

IRP MSE
n=50000

0.25 ± 0.00
0.26 ± 0.00
0.34 ± 0.01
0.93 ± 0.04
3.37 ± 0.06

LS MSE
n=50000

0.36 ± 0.00
0.62 ± 0.00
1.06 ± 0.03
1.86 ± 0.05
2.83 ± 0.12

Table 2: Statistics for simulation generated with yi = Qi xi + N (0  .52). A comparison between the results of
IRP and a least squares linear regression is shown. Bold demonstrates statistical signiﬁcance at 95% conﬁdence.

6 Conclusion

This paper demonstrates that isotonic regression can be used to solve extremely large problems. Fast
approximations are useful  however  as shown  globally optimal solutions are also computationally
tractable. Indeed  isotonic regression as done here performs with a complexity of O(n3) in practice.
As also shown  isotonic regression performs well at reasonable dimensions  but suffers from over-
ﬁtting as the dimension of the data increases. Extensions of this algorithm will analyze the path of
partitions in order to control overﬁtting by stopping the algorithm early. Statistical complexity of
the models generated by partitioning will be examined. Furthermore  similar results will be made
for isotonic regression with different loss functions.

8

References

[1] R.E. Barlow and H.D. Brunk. The isotonic regression problem and its dual. Journal of the American

Statistical Association  67(337):140–147  1972.

[2] G. Obozinski  G. Lanckriet  C. Grant  M.I. Jordan  and W.S. Noble. Consistent probabilistic outputs for

protein function prediction. Genome Biology  9:247–254  2008. Open Access.

[3] M.J. Schell and B. Singh. The reduced monotonic regression method. Journal of the American Statistical

Association  92(437):128–135  1997.

[4] J.B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis. Psy-

chometrika  29(1)  1964.

[5] H. Block  S. Qian  and A. Sampson. Structure algorithms for partially ordered isotonic regression. Journal

of Computational and Graphical Statistcs  3(3):285–300  1994.

[6] O. Burdakov  O. Sysoev  A. Grimvall  and M. Hussian. An o(n2) algorithm for isotonic regression. 83:25–
83  2006. In: G. Di Pillo and M. Roma (Eds) Large-Scale Nonlinear Optimization. Series: Nonconvex
Optimization and Its Applications.

[7] C.-I. C. Lee. The min-max algorithm and isotonic regression. The Annals of Statistics  11(2):467–477 

1983.

[8] J. de Leeuw  K. Hornik  and P. Mair. Isotone optimization in r: Pool-adjacent-violators algorithm (pava)
and active set methods. 2009. UC Los Angeles: Department of Statistics  UCLA. Retrieved from:
http://cran.r-project.org/web/packages/isotone/vignettes/isotone.pdf.

[9] W.L. Maxwell and J.A. Muckstadt. Establishing consistent and realistic reorder intervals in production-

distribution systems. Operations Research  33(6):1316–1341  1985.

[10] R.D.C. Monteiro and I. Adler. Interior path following primal-dual algorithms. part II: Convex quadratic

programming. Mathematical Programming  44:43–66  1989.

[11] O. Burdakov  O. Sysoev  and A. Grimvall. Generalized PAV algorithm with block reﬁnement for partially
ordered monotonic regression. pages 23–37  2009. In: A. Feelders and R. Potharst (Eds.) Proc. of the
Workshop on Learning Monotone Models from Data at the European Conference on Machine Learning
and Principles and Practice of Knowledge Discovery in Databases.

[12] P.M. Pardalos and G. Xue. Algorithms for a class of isotonic regression problems. Algorithmica  23:211–

222  1999.

[13] K.G. Murty. Linear Programming. John Wiley & Sons  Inc.  1983.
[14] R. Chandrasekaran  Y.U. Ryu  V.S. Jacob  and S. Hong.

Computing  17(4):462–474  2005.

Isotonic separation.

INFORMS Journal on

[15] MOSEK ApS. The MOSEK optimization tools manual. version 6.0  revision 61. 2010. Software available

at http://www.mosek.com.

[16] R. K. Ahuja  T. L. Magnanti  and J. B. Orlin. Network Flows: Theory  Algorithms  and Applications.

Prentice-Hall  Inc.  1993.

9

,Simon Du
Yining Wang
Xiyu Zhai
Sivaraman Balakrishnan
Russ Salakhutdinov
Aarti Singh