2017,Countering Feedback Delays in Multi-Agent Learning,We consider a model of game-theoretic learning based on online mirror descent (OMD) with asynchronous and delayed feedback information. Instead of focusing on specific games  we consider a broad class of continuous games defined by the general equilibrium stability notion  which we call λ-variational stability. Our first contribution is that  in this class of games  the actual sequence of play induced by OMD-based learning converges to Nash equilibria provided that the feedback delays faced by the players are synchronous and bounded. Subsequently  to tackle fully decentralized  asynchronous environments with (possibly) unbounded delays between actions and feedback  we propose a variant of OMD which we call delayed mirror descent (DMD)  and which relies on the repeated leveraging of past information. With this modification  the algorithm converges to Nash equilibria with no feedback synchronicity assumptions and even when the delays grow superlinearly relative to the horizon of play.,Countering Feedback Delays in Multi-Agent Learning

Zhengyuan Zhou
Stanford University

zyzhou@stanford.edu

Panayotis Mertikopoulos

Univ. Grenoble Alpes  CNRS  Inria  LIG
panayotis.mertikopoulos@imag.fr

Nicholas Bambos
Stanford University

Peter Glynn

Stanford University

Claire Tomlin
UC Berkeley

bambos@stanford.edu

glynn@stanford.edu

tomlin@eecs.berkeley.edu

Abstract

We consider a model of game-theoretic learning based on online mirror de-
scent (OMD) with asynchronous and delayed feedback information. Instead of
focusing on speciﬁc games  we consider a broad class of continuous games deﬁned
by the general equilibrium stability notion  which we call λ-variational stabil-
ity. Our ﬁrst contribution is that  in this class of games  the actual sequence of
play induced by OMD-based learning converges to Nash equilibria provided that
the feedback delays faced by the players are synchronous and bounded. Subse-
quently  to tackle fully decentralized  asynchronous environments with (possibly)
unbounded delays between actions and feedback  we propose a variant of OMD
which we call delayed mirror descent (DMD)  and which relies on the repeated
leveraging of past information. With this modiﬁcation  the algorithm converges to
Nash equilibria with no feedback synchronicity assumptions and even when the
delays grow superlinearly relative to the horizon of play.

1

Introduction

Online learning is a broad and powerful theoretical framework enjoying widespread applications and
great success in machine learning  data science  operations research  and many other ﬁelds [3  7  22].
The prototypical online learning problem may be described as follows: At each round t = 0  1  . . .   a
player selects an action xt from some convex  compact set  and obtains a reward ut(xt) based on
some a priori unknown payoff function ut. Subsequently  the player receives some feedback (e.g. the
past history of the reward functions) and selects a new action xt+1 with the goal of maximizing the
obtained reward. Aggregating over the rounds of the process  this is usually quantiﬁed by asking that
t=1 [ut(x) − ut(xt)] grow sublinearly with the

the player’s (external) regret Reg(T ) ≡ maxx∈X(cid:80)T

horizon of play T   a property known as “no regret”.
One of the most widely used algorithmic schemes for learning in this context is the online mirror
descent (OMD) class of algorithms [23]. Tracing its origins to [17] for ofﬂine optimization problems 
OMD proceeds by taking a gradient step in the dual (gradient) space and projecting it back to the
primal (decision) space via a mirror map generated by a strongly convex regularizer function (with
different regularizers giving rise to different algorithms). In particular  OMD includes as special cases
several seminal learning algorithms  such as Zinkevich’s online gradient descent (OGD) scheme
[29]  and the multiplicative/exponential weights (EW) algorithm [1  13]. Several variants of this
class also exist and  perhaps unsurprisingly  they occur with a variety of different names – such as
“Follow-the-Regularized-Leader" [9]  dual averaging [18  25]  and so on.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

√

When ut is concave  OMD enjoys a sublinear O(
T ) regret bound which is known to be universally
tight.1 A common instantiation of this is found in repeated multi-player games  where each player’s
payoff function is determined by the actions of all other players via a ﬁxed mechanism – the stage
game. Even though this mechanism may be unknown to the players  the universality of the OMD
regret bounds raises high expectations in terms of performance guarantees  so it is natural to assume
that players adopt some variant thereof when faced with such online decision processes. This leads to
the following central question: if all players of a repeated game employ an OMD updating rule  do
their actions converge to a Nash equilibrium of the underlying one-shot game?

established the convergence of the so-called “ergodic average” T −1(cid:80)T

Related Work. Given the prominence of Nash equilibrium as a solution concept in game theory
(compared to coarser notions such as correlated equilibria or the Hannan set)  this problem lies at
the heart of multi-agent learning [4]. However  convergence to a Nash equilibrium is  in the words
of [4]  “considerably more difﬁcult” than attaining a no-regret state for all players (which leads to
weaker notion of coarse correlated equilibrium in ﬁnite games). To study this question  a growing
body of literature has focused on special classes of games (e.g. zero-sum games  routing games) and
t=1 xt of OMD [2  10  12].
In general  the actual sequence of play may fail to converge altogether  even in simple  ﬁnite games
[16  24]. On the other hand  there is a number of recent works establishing the convergence of play in
potential games with ﬁnite action sets under different assumptions for the number of players involved
(continuous or ﬁnite) and the quality of the available feedback (perfect  semi-bandit/imperfect  or
bandit/payoff-based) [5  11  14  19]. However  these works focus on games with ﬁnite action sets and
feedback is assumed to be instantly available to the players (i.e. with no delays or asynchronicities) 
two crucial assumptions that we do not make in this paper.
A further major challenge arises in decentralized environments (such as transportation networks) 
where a considerable delay often occurs between a player’s action and the corresponding received
feedback. To study learning in such settings  [20] recently introduced an elegant and ﬂexible delay
framework where the gradient at round t is only available at round t + dt − 1  with dt being the
delay associated with the player’s action at round t.2 [20] then considered a very natural extension of
OMD under delays: updating the set of gradients as they are received (see Algorithm 1 for details).
t=1 dt  [20] showed that OMD enjoys an O(D(T )1/2)
regret bound. This natural extension has several strengths: ﬁrst  no assumption is made on how the
gradients are received (the delayed gradients can be received out-of-order); further  as pointed out
in [6  8]  a gradient “does not need to be timestamped by the round s from which it originates ” as
required for example by the pooling strategies of [6  8].

If the total delay after time T is D(T ) =(cid:80)T

Our Contributions. Our investigations here differ from existing work in the following aspects:
First  we consider learning in games with asynchronous and delayed feedback by extending the
general single-agent feedback delay framework introduced in [20]. Previous work on the topic has
focused on the regret analysis of single-agent learning with delays  but the convergence properties
of such processes in continuous games are completely unknown. Second  we focus throughout
on the convergence of the actual sequence of play generated by OMD (its “last iterate” in the
parlance of optimization)  as opposed to the algorithm’s ergodic average 1
t=1 xt. This last point
T
is worth emphasizing for several reasons: a) this mode of convergence is stronger and theoretically
more appealing because it implies ergodic convergence; b) in a game-theoretic setting  payoffs
are determined by the actual sequence of play  so ergodic convergence diminishes in value if it
is not accompanied by similar conclusions for the players’ realized actions; and c) because there
is no inherent averaging  the techniques used to prove convergence of xt provide a much ﬁner
understanding of the evolution of OMD.
The starting point of our paper is the introduction of an equilibrium stability notion which we
call λ-variational stability  a notion that is motivated by the concept of evolutionary stability in
population games and builds on the characterization of stable Nash equilibria as solutions to a Minty-
type variational inequality [15]. This stability notion is intimately related to monotone operators in
variational analysis [21] and can be seen as a strict generalization of operator monotonicity in the

(cid:80)T

1In many formulations  a cost function (as opposed to a reward function) is used  in which case such cost

functions need to be convex.

2Of course  taking dt = 1 yields the classical no-delay setting.

2

current game-theoretic context.3 By means of this notion  we are able to treat convergence questions
in general games with continuous action spaces  without having to focus on a speciﬁc class of games
– such as concave potential or strictly monotone games (though our analysis also covers such games).
Our ﬁrst result is that  assuming variational stability  the sequence of play induced by OMD converges
to the game’s set of Nash equilibria  provided that the delays of all players are synchronous and
bounded (see Theorems 4.3 and 4.4). As an inherited beneﬁt  players adopting this learning algorithm
can receive gradients out-of-order and do not need to keep track of the timestamps from which the
gradients originate. In fact  even in the special case of learning without delays  we are not aware of a
similar convergence result for the actual sequence of play.
An important limitation of this result is that delays are assumed synchronous and bounded  an
assumption which might not hold in large  decentralized environments. To lift this barrier  we
introduce a modiﬁcation of vanilla OMD which we call delayed mirror descent (DMD)  and which
leverages past information repeatedly  even in rounds where players receive no feedback. Thanks
to this modiﬁcation  play under DMD converges to variationally stable sets of Nash equilibria
(Theorem 5.2)  even if the players experience asynchronous and unbounded delays: in particular 
delays could grow superlinearly in the game’s horizon  and DMD would still converge.
We mention that the convergence proofs for both OMD and DMD rely on designing a particular
Lyapunov function  the so-called λ-Fenchel coupling which serves as a “primal-dual divergence”
measure between actions and gradient variables. Thanks to its Lyapunov properties  the λ-Fenchel
coupling provides a potent tool for proving convergence and we exploit it throughout. Further  we
present a uniﬁed theoretical framework that puts the analysis of both algorithms under different delay
assumptions on the same footing.

2 Problem Setup

2.1 Games with Continuous Action Sets

Deﬁnition 2.1. A continuous game G is a tuple G = (N  X =(cid:81)N

We start with the deﬁnition of a game with continuous action sets  which serves as a stage game and
provides a reward function for each player in an online learning process.
i=1)  where N is the
set of N players {1  2  . . .   N}  Xi is a compact convex subset of some ﬁnite-dimensional vector
space Rdi representing the action space of player i  and ui : X → R is the i-th player’s payoff
function.
Regarding the players’ payoff functions  we make the following assumptions throughout:

i=1 Xi {ui}N

1. For each i ∈ N   ui(x) is continuous in x.
2. For each i ∈ N   ui is continuously differentiable in xi and the partial gradient ∇xi ui(x) is

Lipschitz continuous in x.

Throughout the paper  x−i denotes the joint action of all players but player i. Consequently  the joint
action4 x will frequently be written as (xi  x−i). Two important quantities in the current context are:
Deﬁnition 2.2. We let v(x) be the proﬁle of the players’ individual payoff gradients 5 i.e. v(x) =
(v1(x)  . . .   vN (x))  where vi(x) (cid:44) ∇xi ui(x).
Deﬁnition 2.3. Given a continuous game G  x∗ ∈ X is called a (pure-strategy) Nash equilibrium if
for each i ∈ N   ui(x∗

i   x∗

−i) ∀xi ∈ Xi.
2.2 Online Mirror Descent in Games under Delays

−i) ≥ ui(xi  x∗

In what follows  we consider a general multi-agent delay model extending the single-agent delay
model of [20] to the multi-agent learning case. At a high level  for each agent there can be an arbitrary

3In the supplement  we give two well-known classes of games that satisfy this equilibrium notion.
4Note that boldfaced letters are only used to denote joint actions. In particular  xi is a vector even though it

is not boldfaced.
always exists and is a continuous function on the joint action space X .

5Note that per the last assumption in the deﬁnition of a concave game (Deﬁnition 2.1)  the gradient v(x)

3

delay between the stage at which an action is played and the stage at which feedback is received about
said action (typically in the form of gradient information). There is no extra assumption imposed on
the feedback delays – in particular  feedback can arrive out-of-order and in a completely asynchronous
manner across agents. Further  the received feedback is not time-stamped – so the player might not
know to which iteration a speciﬁc piece of feedback corresponds.
When OMD is applied in this setting  we obtain the following scheme:

i + αt(cid:80)

i   xi(cid:105) − hi(xi)}
vi(xs)

i = arg maxxi∈Xi{(cid:104)yt
xt
yt+1
i = yt
s∈Gt
end for

Algorithm 1 Online Mirror Descent on Games under Delays
1: Each player i chooses an initial y0
i .
2: for t = 0  1  2  . . . do
for i = 1  . . .   N do
3:
4:
5:
6:
7: end for
Three comments are in order here. First  each hi is a regularizer on Xi  as deﬁned below:
Deﬁnition 2.4. Let D be a compact and convex subset of Rm. We say that g : D → R is a regularizer
if g is continuous and strongly convex on D  i.e. there exists some K > 0 such that
Kt(1 − t)(cid:107)d(cid:48) − d(cid:107)2

g(td + (1 − t)d(cid:48)) ≤ tg(d) + (1 − t)g(d(cid:48)) − 1
2

(2.1)

i

(cid:48) ∈ D.

for all t ∈ [0  1]  bd  bd
Second  the gradient step size αt in Algorithm 1 can be any positive and non-increasing sequence

that satisﬁes the standard summability assumption:(cid:80)∞

t=0 αt = ∞ (cid:80)∞

t=0(αt)2 < ∞.

Third  regarding the delay model: in Algorithm 1  Gt
i denotes the set of rounds whose gradients
become available for player i at the current round t. Denote player i’s delay of the gradient at round
i − 1  i.e.
s to be ds
s ∈ Gs+ds
i = 1 for all s  player i doesn’t experience any feedback delays. Note
here again that each player can receive feedback out of order: this can happen if the gradient at an
earlier round has a much larger delay than that of the gradient at a later round.

i (a positive integer)  then this gradient vi(xs) will be available at round s + ds
i−1

. In particular  if ds

i

3 λ-Variational Stability: A Key Criterion

In this section  we deﬁne a key stability notion  called λ-variational stability. This notion allows us to
obtain strong convergence results for the induced sequence of play  as opposed to results that only hold
in speciﬁc classes of games. The supplement provides two detailed special classes of games (convex
potential games and asymmetric Cournot oligopolies) that admit variationally stable equilibria. Other
examples include monotone games (discussed later in this section)  pseudo-monotone games [28] 
non-atomic routing games [26  27]  symmetric inﬂuence network games [11] and many others.

3.1 λ-Variational Stability

Deﬁnition 3.1. Given a game with continuous actions (N  X =(cid:81)N

called λ-variationally stable for some λ ∈ RN

++ if

i=1 Xi {ui}N

i=1)  a set C ⊂ X is

N(cid:88)

λi(cid:104)vi(x)  xi − x∗

i (cid:105) ≤ 0 

for all x ∈ X   x∗ ∈ C.

(3.1)

i=1

with equality if and only if x ∈ C.
Remark 3.1. If C is λ-stable with λi = 1 for all i  it is called simply stable [15].
We emphasize that in a game setting  λ-variational stability is more general than an important
concept called operator monotonicity in variational analysis. Speciﬁcally  v(·) is called a monotone

4

operator [21] if the following holds (with equality if and only if x = ˜x):

(cid:104)v(x) − v(˜x)  x − ˜x(cid:105) (cid:44) N(cid:88)

(cid:104)vi(x) − vi(˜x)  xi − ˜xi(cid:105) ≤ 0 ∀x  ˜x ∈ X .

(3.2)

i=1

If v(·) is monotone  the game admits a unique Nash equilibrium x∗ which (per the property of a Nash
equilibrium) satisﬁes (cid:104)v(x∗)  x − x∗(cid:105) ≤ 0. Consequently  if v(·) is a monotone operator  it follows
that (cid:104)v(x)  x − x∗(cid:105) ≤ (cid:104)v(x∗)  x − x∗(cid:105) ≤ 0  where equality is achieved if and only if x = x∗. This
implies that when v(x) is a monotone operator  the singleton set of the unique Nash equilibrium is
1-variationally stable  where 1 is the all-ones vector. The converse is not true: when v(x) is not a
monotone operator  we can still have a unique Nash equilibrium that is λ-variationally stable  or more
generally  have a λ-variationally stable set C.

3.2 Properties of λ-Variational Stability
Lemma 3.2. If C is nonempty and λ-stable  then it is closed  convex and contains all Nash equilibria
of the game.

The following lemma gives us a convenient sufﬁcient condition ensuring that a singleton λ-
variationally stable set {x∗} exists; in this case  we simply say that x∗ is λ-variationally stable.

Lemma 3.3. Given a game with continuous actions (N  X =(cid:81)N

i=1)  where each ui is
twice continuously differentiable. For each x ∈ X   deﬁne the λ-weighted Hessian matrix H λ(x) as
follows:

i=1 Xi {ui}N

λi ∇xj vi(x) +

λj(∇xi vj(x))T .

H λ

ij(x) =

(3.3)
If H λ(x) is negative-deﬁnite for every x ∈ X   then the game admits a unique Nash equilibrium x∗
that is λ-globally variational stable.
Remark 3.2. It is important to note that the Hessian matrix so deﬁned is a block matrix: each H λ
is a di×dj matrix. Writing it in terms of the utility function  we have H λ
2 λj(∇xi ∇xj uj(x))T .

ij(x)
2 λi ∇xj ∇xi ui(x)+

ij(x) = 1

1
2

1
2

1

4 Convergence under Synchronous and Bounded Delays

In this section  we tackle the convergence of the last iterate of OMD under delays. We start by
deﬁning an important divergence measure  λ-Fenchel coupling  that generalizes Bregman divergence.
We then establish its useful properties that play an indispensable role in both this and next sections.

4.1 λ-Fenchel Coupling

Deﬁnition 4.1. Fix a game with continuous action spaces (N  X =(cid:81)N

i=1) and for each
player i  let hi : Xi → R be a regularizer with respect to the norm (cid:107) · (cid:107)i that is Ki-strongly convex.

i=1 Xi {ui}N

1. The convex conjugate function h∗

i : Rdi → R of hi is deﬁned as:
{(cid:104)xi  yi(cid:105) − hi(xi)}.

h∗
i (yi) = max
xi∈Xi

2. The choice function Ci : Rdi → Xi associated with regularizer hi for player i is deﬁned as:

N(cid:88)

i=1

5

Ci(yi) = arg max
xi∈Xi

++  the λ-Fenchel coupling F λ : X × R(cid:80)N

3. For a λ ∈ RN

{(cid:104)xi  yi(cid:105) − hi(xi)}.

i=1 di → R is deﬁned as:

F λ(x  y) =

λi(hi(xi) − (cid:104)xi  yi(cid:105) + h∗

i (yi)).

Note that although the domain of hi is Xi ⊂ Rdi  the domain of its conjugate (gradient space) h∗
is Rdi. The two key properties of λ-Fenchel coupling that will be important in establishing the
convergence of OMD are given next.
Lemma 4.2. For each i ∈ {1  . . .   N}  let hi : Xi → R be a regularizer with respect to the norm
(cid:107) · (cid:107)i that is Ki-strongly convex and let λ ∈ RN

i=1 di:

i

(cid:80)N
i=1 Kiλi(cid:107)Ci(yi) − xi(cid:107)2

++. Then ∀x ∈ X  ∀˜y  y ∈ R(cid:80)N
)(cid:80)N
i ≥ 1

2 (mini Kiλi)(cid:80)N

2 (maxi

λi
Ki

i=1 λi(cid:104) ˜yi − yi  Ci(yi)− xi(cid:105) + 1

1. F λ(x  y) ≥ 1

2. F λ(x  ˜y) ≤ F λ(x  y) +(cid:80)N

2

i is the dual norm of (cid:107) · (cid:107)i (i.e. (cid:107)yi(cid:107)∗

where (cid:107) · (cid:107)∗
(cid:80)N
Remark 4.1. Collecting each individual choice map into a vector  we obtain the aggregate choice
i=1 di → X   with C(y) = (C1(y1)  . . .   CN (yN )). Since each space Xi is endowed
i=1 (cid:107)xi(cid:107)i. We can also similarly deﬁne the aggregate dual norm: (cid:107)y(cid:107)∗ =(cid:80)N
(cid:107)x(cid:107) =(cid:80)N
map C : R
with norm (cid:107) · (cid:107)i  we can deﬁne the induced aggregate norm (cid:107) · (cid:107) on the joint space X as follows:
i=1 (cid:107)yi(cid:107)∗
i .
Henceforth  it shall be clear that the convergence in the joint space (e.g. C(yt) → x  yt → y) will
be deﬁned under the respective aggregate norm.

i = max(cid:107)xi(cid:107)i≤1(cid:104)xi  yi(cid:105).

i=1 (cid:107)Ci(yi) − xi(cid:107)2
i .

i=1((cid:107) ˜yi − yi(cid:107)∗

i )2 

Finally  we assume throughout the paper that the choice maps are regular in the following (very weak)
sense: a choice map C(·) is said to be λ-Fenchel coupling conforming if
C(yt) → x implies F λ(x  yt) → 0 as t → ∞.

(4.1)
Unless one aims for relatively pathological cases  choice maps induced by typical regularizers are
always λ-Fenchel coupling conforming: examples include the Euclidean and entropic regularizers.

4.2 Convergence of OMD to Nash Equilibrium

We start by characterizing the assumption on the delay model:
Assumption 1. The delays are assumed to be:

1. Synchronous: Gt
2. Bounded: dt

i = Gt

j ∀i  j ∀t.

Theorem 4.3. Fix a game with continuous action spaces (N  X =(cid:81)N

i ≤ D ∀i ∀t (for some positive integer D).

i=1 Xi {ui}N
i=1) that admits
x∗ as the unique Nash equilibrium that is λ-variationally stable. Under Assumption 1  the OMD
iterate xt given in Algorithm 1 converges to x∗  irrespective of the initial point x0.
Remark 4.2. The proof is rather long and involved. To aid the understanding and enhance the intuition 
we break it down into four main steps  each of which will be proved in the appendix in detail.

(cid:40)

yt+1
i = yt

1. Since the delays are synchronous  we denote by Gt the common set and dt the common

delay at round t. The gradient update in OMD under delays can then be written as:

vi(xs) = yt

i + αt (cid:88)
i =(cid:80)

(cid:88)
s∈Gt
s∈Gt{vi(xs) − vi(xt)}. We show limt→∞ (cid:107)bt
i(cid:107)∗
i = 0 for each player i.
1  . . .   bt

{vi(xs) − vi(xt)}

N ) and we have limt→∞ bt = 0 per Claim 1. Since each player’s
i) per Claim 1  we can then

i + αt(|Gt|vi(xt) + bt

|Gt|vi(xt) +

gradient update can be written as yt+1
write the joint OMD update (of all players) as:

i = yt

2. Deﬁne bt = (bt

Deﬁne bt

.

(4.2)

i + αt

(cid:41)

s∈Gt

xt = C(yt) 

yt+1 = yt + αt {|Gt|v(xt) + bt} .

(4.3)
(4.4)
Let B(x∗  ) (cid:44) {x ∈ X | (cid:107)x − x∗(cid:107) < } be the open ball centered around x∗ with radius
. Then  using λ-Fenchel coupling as a “energy" function and leveraging the handle on
bt given by Claim 1  we can establish that  for any  > 0 the iterate xt will eventually
enter B(x∗  ) and visit B(x∗  ) inﬁnitely often  no matter what the initial point x0 is.
Mathematically  the claim is that ∀ > 0 ∀x0 |{t | xt ∈ B(x∗  )}| = ∞.

6

3. Fix any δ > 0 and consider the set ˜B(x∗  δ) (cid:44) {C(y) | F λ(x∗  y) < δ}. In other words 
˜B(x∗  δ) is some “neighborhood" of x∗  which contains every x that is an image of some y
(under the choice map C(·)) that is within δ distance of x∗ under the λ-Fenchel coupling
“metric". Although F λ(x∗  y) is not a metric  ˜B(x∗  δ) contains an open ball within it.
Mathematically  the claim is that for any δ > 0  ∃(δ) > 0 such that: B(x∗  ) ⊂ ˜B(x∗  δ).
4. For any “neighborhood" ˜B(x∗  δ)  after long enough rounds  if xt ever enters ˜B(x∗  δ)  it
will be trapped inside ˜B(x∗  δ) thereafter. Mathematically  the claim is that for any δ > 0 
∃T (δ)  such that for any t ≥ T (δ)  if xt ∈ ˜B(x∗  δ)  then x˜t ∈ ˜B(x∗  δ) ∀˜t ≥ t.

Putting all four elements above together  we note that the signiﬁcance of Claim 3 is that  since the
iterate xt will enter B(x∗  ) inﬁnitely often (per Claim 2)  xt must enter ˜B(x∗  δ) inﬁnitely often. It
therefore follows that  per Claim 4  starting from iteration t  xt will remain in ˜B(x∗  δ). Since this is
true for any δ > 0  we have F λ(x∗  yt) → 0 as t → ∞. Per Statement 1 in Lemma 4.2  this leads to
that (cid:107)C(yt) − x∗(cid:107) → 0 as t → ∞  thereby establishing that xt = C(yt) → x∗ as t → 0.
In fact  the result generalizes straightforwardly to multiple Nash equilibria. The proof of the con-
vergence to the set case is line-by-line identical  provided we redeﬁne  in a standard way  every
quantity that measures the distance between two points to the corresponding quantity that measures
the distance between a point and a set (by taking the inﬁmum over the distances between the point
and a point in that set). We directly state the result below.

Theorem 4.4. Fix a game with continuous action spaces (N  X =(cid:81)N

i=1 Xi {ui}N
i=1) that admits
X ∗ as a λ-variationally stable set (of necessarily all Nash equilibria)  for some λ ∈ RN
++. Under As-
sumption 1  the OMD iterate xt given in Algorithm 1 satisﬁes limt→∞ dist(xt X ∗) = 0  irrespective
of x0  where dist(· ·) is the standard point-to-set distance function induced by the norm (cid:107) · (cid:107).

5 Delayed Mirror Descent: Asynchronous and Unbounded Delays

The synchronous and bounded delay assumption in Assumption 1 is fairly strong. In this section 
by a simple modiﬁcation of OMD  we propose a new learning algorithm called Delayed Mirror
Descent (DMD)  that allows the last-iterate convergence-to-Nash result to be generalized to cases
with arbitrary asynchronous delays among players as well as unbounded delay growth.

5.1 Delayed Mirror Descent in Games

The main idea for the modiﬁcation is that when player i doesn’t receive any gradient on round t 
instead of not doing any gradient updates as in OMD  he uses the most recent set of gradients to
perform updates. More formally  deﬁne the most recent information set6 as:

(cid:26)Gt

˜Gt
i =

i   if Gt
˜Gt−1

i (cid:54)= ∅
  if Gt

i = ∅.
Under this deﬁnition  Delayed Mirror Descent is (note that ˜Gt
We only make the following assumption on the delays:

Assumption 2. For each player i  limt→∞(cid:80)t

αs = 0.

i

s=min ˜Gt

i

i is always non-empty here):

This assumption essentially requires that no player’s delays grow too fast. Note that in particular 
players delays can be arbitrarily asynchronous. To make this assumption more concrete  we next give
two more explicit delay conditions that satisfy the main delay assumption. As made formal by the
following lemma  if the delays are bounded (but not necessarily synchronous)  then Assumption 2 is
satisﬁed. Furthermore  by appropriately choosing the sequence αt  Assumption 2 can accommodate
delays that are unbounded and grow super-linearly.

6There may not be any gradient information in the ﬁrst few rounds due to delays. Without loss of generality 
we can always start at the ﬁrst round when there is non-empty gradient information  or equivalently  assume that
some gradient is available at t = 0.

7

Algorithm 2 Delayed Mirror Descent on Games
1: Each player i chooses an initial y0
i .
2: for t = 0  1  2  . . . do
for i = 1  . . .   N do
3:
4:
5:
6:
7: end for

(cid:80)
i = arg maxxi∈Xi{(cid:104)yt
xt
yt+1
i = yt
s∈ ˜Gt
end for

i   xi(cid:105) − hi(xi)}
vi(xs)

i + αt
i|
| ˜Gt

i

Lemma 5.1. Let {ds

i}∞
s=1 be the delay sequences for player i.

1. If each player i’s delay is bounded (i.e. ∃d ∈ Z  ds

i ≤ d ∀s)  then Assumption 2 is satisﬁed

for any positive  non-increasing  not-summable-but-square-summable sequence {αt}.

2. There exists a positive  non-increasing  not-summable-but-square-summable sequence (e.g.

αt =

t log t log log t ) such that if ds

1

i = O(s log s) ∀i  then Assumption 2 is satisﬁed.

Proof: We will only prove Statement 2  the more interesting case. Take αt =

which is obviously positive  non-increasing and square-summable. Since(cid:82) t

t log t log log t 
s log s log log s ds =
i be given and let ˜t be the most recent

s=4

1

1

log log log t → ∞ as t → ∞  αt is not summable. Next  let ˜Gt
round (up to and including t) such that G ˜t
˜Gt
i = G ˜t

i is not empty. This means:
i  Gk

i = ∅ ∀k ∈ (˜t  t].

Note that since the gradient at time ˜t will be available at time ˜t + d˜t

i − 1  it follows that

(5.1)

t − ˜t ≤ d˜t
i.

(5.2)
Note that this implies ˜t → ∞ as t → ∞  because otherwise  ˜t is bounded  leading to the right-side d˜t
being bounded  which contradicts to the left-side diverging to inﬁnity.
Since ds
implies: t ≤ ˜t + K˜t log ˜t. Denote st
yielding st

i ≤ Ks log s for some K > 0. Consequently  Equation 5.2
i   thereby

i   Equation 5.1 implies that st

min = minG ˜t

min = min ˜Gt

i = O(s log s)  it follows that ds
i − 1 = ˜t. Therefore:
dst

min + dst

(5.3)
min → ∞ as t → ∞  because otherwise  the left-hand side of Equa-
Equation (5.3) implies that st
tion (5.3) is bounded while the right-hand side goes to inﬁnity (since ˜t → ∞ as t → ∞ as established
earlier).
With the above notation  it follows that:

= ˜t − st

min + 1.

min

min

i

i

t(cid:88)

lim
t→∞

s=min ˜Gt

i

 ˜t(cid:88)
min + (˜t log ˜t)α˜t(cid:111)

s=st

min

αs = lim
t→∞

min

min

αst

t(cid:88)

αs +

αs

s=˜t+1



min

dst
i
min) log log(st

min)

min) log(st

K˜t log ˜t

(˜t + 1) log(˜t + 1) log log(˜t + 1)

(5.4)

(5.5)

(cid:41)
(cid:27)

(5.6)

+

+

(cid:27)

K˜t log ˜t

(˜t + 1) log(˜t + 1) log log(˜t + 1)

(5.7)

= 0.

(5.8)
(cid:4)

i

s=st
dst

t(cid:88)
(cid:110)
(cid:40)
(cid:26)
(cid:26)

(st

(st

αs ≤ lim
t→∞

≤ lim
t→∞

= lim
t→∞

≤ lim
t→∞

≤ lim
t→∞

K(st

min) log(st

min)

min) log(st

min) log log(st

min)

K

log log(st

min)

+

K

log log(˜t + 1)

8

Remark 5.1. The proof to the second claim of Lemma 5.1 indicates that one can also easily obtain
slightly larger delay growth rates: O(t log t log log t)  O(t log t log log t log log log t) and so on  by
choosing the corresponding step size sequences. Further  it is conceivable that one can identify
meaningfully larger delay growth rates that still satisfy Assumption 2  particularly under more
restrictions on the degree of delay asynchrony among the players. We leave that for future work.

5.2 Convergence of DMD to Nash Equilibrium

Theorem 5.2. Fix a game with continuous action spaces (N  X =(cid:81)N

i=1 Xi {ui}N
i=1) that admits
x∗ as the unique Nash equilibrium that is λ-variationally stable. Under Assumption 2  the DMD
iterate xt given in Algorithm 2 converges to x∗  irrespective of the initial point x0.
The proof here uses a similar framework as the one in Remark 4.2  although the details are somewhat
different. Building on the notation and arguments given in Remark 4.2  we again outline three main
ingredients that together establish the result. Detailed proofs are omitted due to space limitation.

1. The gradient update in DMD can be rewritten as:

yt+1
i = yt

vi(xs) = yt

i + αtvi(xt) + αt (cid:88)

vi(xs) − vi(xt)

| ˜Gt
i|

.

s∈ ˜Gt

i

(cid:88)

i +

i =(cid:80)

αt
| ˜Gt
i|

s∈ ˜Gt

i

s∈ ˜Gt
vi(xs)−vi(xt)

i

| ˜Gt
i|
yt+1
i = yt

By deﬁning: bt

  we can write player i’s gradient update as:

i + αt(vi(xt) + bt

i).

By bounding bt
that bt

i’s magnitude using the delay sequence  Assumption 2 allows us to establish
i has negligible impact over time. Mathematically  the claim is that limt→∞ (cid:107)bt
i(cid:107)∗
i = 0.

2. The joint DMD update can be written as:

xt = C(yt) 

yt+1 = yt + αt(v(xt) + bt).

(5.9)
(5.10)
Here again using λ-Fenchel coupling as a “energy" function and leveraging the handle on bt
given by Claim 1  we show that for any  > 0 the iterate xt will eventually enter B(x∗  )
and visit B(x∗  ) inﬁnitely often  no matter what the initial point x0 is. Furthermore  per
Claim 3 in Remark 4.2  B(x∗  ) ⊂ ˜B(x∗  δ). This implies that xt must enter ˜B(x∗  δ)
inﬁnitely often.

3. Again using λ-Fenchel coupling  we show that under DMD  for any “neighborhood"
˜B(x∗  δ)  after long enough iterations  if xt ever enters ˜B(x∗  δ)  it will be trapped in-
side ˜B(x∗  δ) thereafter.

Combining the above three elements  it follows that under DMD  starting from iteration t  xt will
remain in ˜B(x∗  δ). Since this is true for any δ > 0  we have F λ(x∗  yt) → 0 as t → ∞  thereby
establishing that xt = C(yt) → x∗ as t → 0.
Here again  the result generalizes straightforwardly to the multiple Nash equilibria case (with identical
proofs modulo using point-to-set distance metric). We omit the statement.

6 Conclusion

We examined a model of game-theoretic learning based on OMD with asynchronous and delayed
information. By focusing on games with λ- stable equilibria  we showed that the sequence of play
induced by OMD converges whenever the feedback delays faced by the players are synchronous and
bounded. Subsequently  to tackle fully decentralized  asynchronous environments with unbounded
feedback delays (possibly growing sublinearly in the game’s horizon)  we showed that our conver-
gence result still holds under delayed mirror descent  a variant of vanilla OMD that leverages past
information even in rounds where no feedback is received. To further enhance the distributed aspect
of the algorithm  in future work we intend to focus on the case where the players’ gradient input is
not only delayed  but also subject to stochastic imperfections – or  taking this to its logical extreme 
when players only have observations of their in-game payoffs  and have no gradient information.

9

7 Acknowledgments

Zhengyuan Zhou is supported by Stanford Graduate Fellowship and he would like to thank Walid
Krichene and Alex Bayen for stimulating discussions (and their charismatic research style) that have
ﬁrmly planted the initial seeds for this work. Panayotis Mertikopoulos gratefully acknowledges
ﬁnancial support from the Huawei Innovation Research Program ULTRON and the ANR JCJC project
ORACLESS (grant no. ANR–16–CE33–0004–01). Claire Tomlin is supported in part by the NSF
CPS:FORCES grant (CNS-1239166).

References
[1] S. ARORA  E. HAZAN  AND S. KALE  The multiplicative weights update method: A meta-algorithm and

applications  Theory of Computing  8 (2012)  pp. 121–164.

[2] M. BALANDAT  W. KRICHENE  C. TOMLIN  AND A. BAYEN  Minimizing regret on reﬂexive Banach
spaces and Nash equilibria in continuous zero-sum games  in NIPS ’16: Proceedings of the 30th Interna-
tional Conference on Neural Information Processing Systems  2016.

[3] A. BLUM  On-line algorithms in machine learning  in Online algorithms  Springer  1998  pp. 306–325.
[4] N. CESA-BIANCHI AND G. LUGOSI  Prediction  learning  and games  Cambridge university press  2006.
[5] J. COHEN  A. HÉLIOU  AND P. MERTIKOPOULOS  Learning with bandit feedback in potential games  in
NIPS ’17: Proceedings of the 31st International Conference on Neural Information Processing Systems 
2017.

[6] T. DESAUTELS  A. KRAUSE  AND J. W. BURDICK  Parallelizing exploration-exploitation tradeoffs in
gaussian process bandit optimization.  Journal of Machine Learning Research  15 (2014)  pp. 3873–3923.
[7] E. HAZAN  Introduction to Online Convex Optimization  Foundations and Trends(r) in Optimization Series 

Now Publishers  2016.

[8] P. JOULANI  A. GYORGY  AND C. SZEPESVÁRI  Online learning under delayed feedback  in Proceedings

of the 30th International Conference on Machine Learning (ICML-13)  2013  pp. 1453–1461.

[9] A. KALAI AND S. VEMPALA  Efﬁcient algorithms for online decision problems  Journal of Computer and

System Sciences  71 (2005)  pp. 291–307.

[10] S. KRICHENE  W. KRICHENE  R. DONG  AND A. BAYEN  Convergence of heterogeneous distributed
learning in stochastic routing games  in Communication  Control  and Computing (Allerton)  2015 53rd
Annual Allerton Conference on  IEEE  2015  pp. 480–487.

[11] W. KRICHENE  B. DRIGHÈS  AND A. M. BAYEN  Online learning of nash equilibria in congestion games 

SIAM Journal on Control and Optimization  53 (2015)  pp. 1056–1081.

[12] K. LAM  W. KRICHENE  AND A. BAYEN  On learning how players learn: estimation of learning dynamics
in the routing game  in Cyber-Physical Systems (ICCPS)  2016 ACM/IEEE 7th International Conference
on  IEEE  2016  pp. 1–10.

[13] N. LITTLESTONE AND M. K. WARMUTH  The weighted majority algorithm  INFORMATION AND

COMPUTATION  108 (1994)  pp. 212–261.

[14] R. MEHTA  I. PANAGEAS  AND G. PILIOURAS  Natural selection as an inhibitor of genetic diversity:
Multiplicative weights updates algorithm and a conjecture of haploid genetics  in ITCS ’15: Proceedings
of the 6th Conference on Innovations in Theoretical Computer Science  2015.

[15] P. MERTIKOPOULOS  Learning in games with continuous action sets and unknown payoff functions.

https://arxiv.org/abs/1608.07310  2016.

[16] P. MERTIKOPOULOS  C. H. PAPADIMITRIOU  AND G. PILIOURAS  Cycles in adversarial regularized
learning  in SODA ’18: Proceedings of the 29th annual ACM-SIAM symposium on discrete algorithms  to
appear.

[17] A. S. NEMIROVSKI AND D. B. YUDIN  Problem Complexity and Method Efﬁciency in Optimization 

Wiley  New York  NY  1983.

[18] Y. NESTEROV  Primal-dual subgradient methods for convex problems  Mathematical Programming  120

(2009)  pp. 221–259.

[19] G. PALAIOPANOS  I. PANAGEAS  AND G. PILIOURAS  Multiplicative weights update with constant
step-size in congestion games: Convergence  limit cycles and chaos  in NIPS ’17: Proceedings of the 31st
International Conference on Neural Information Processing Systems  2017.

[20] K. QUANRUD AND D. KHASHABI  Online learning with adversarial delays  in Advances in Neural

Information Processing Systems  2015  pp. 1270–1278.

[21] R. T. ROCKAFELLAR AND R. J.-B. WETS  Variational analysis  vol. 317  Springer Science & Business

Media  2009.

10

[22] S. SHALEV-SHWARTZ ET AL.  Online learning and online convex optimization  Foundations and Trends R(cid:13)

in Machine Learning  4 (2012)  pp. 107–194.

[23] S. SHALEV-SHWARTZ AND Y. SINGER  Convex repeated games and Fenchel duality  in Advances in

Neural Information Processing Systems 19  MIT Press  2007  pp. 1265–1272.

[24] Y. VIOSSAT AND A. ZAPECHELNYUK  No-regret dynamics and ﬁctitious play  Journal of Economic

Theory  148 (2013)  pp. 825–842.

[25] L. XIAO  Dual averaging methods for regularized stochastic learning and online optimization  Journal of

Machine Learning Research  11 (2010)  pp. 2543–2596.

[26] Z. ZHOU  N. BAMBOS  AND P. GLYNN  Dynamics on linear inﬂuence network games under stochastic
environments  in International Conference on Decision and Game Theory for Security  Springer  2016 
pp. 114–126.

[27] Z. ZHOU  B. YOLKEN  R. A. MIURA-KO  AND N. BAMBOS  A game-theoretical formulation of inﬂuence

networks  in American Control Conference (ACC)  2016  IEEE  2016  pp. 3802–3807.

[28] M. ZHU AND E. FRAZZOLI  Distributed robust adaptive equilibrium computation for generalized convex

games  Automatica  63 (2016)  pp. 82–91.

[29] M. ZINKEVICH  Online convex programming and generalized inﬁnitesimal gradient ascent  in ICML ’03:

Proceedings of the 20th International Conference on Machine Learning  2003  pp. 928–936.

11

,Zhengyuan Zhou
Panayotis Mertikopoulos
Peter Glynn
Claire Tomlin