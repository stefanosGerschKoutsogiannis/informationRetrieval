2016,Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods,The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward  neural networks can be trained globally optimal with a linear convergence rate. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks  we restrict ourselves for simplicity in this paper to one- and two hidden layer networks. Our experiments confirms that these models are already rich enough to achieve good performance on a series of real-world datasets.,Globally Optimal Training of Generalized

Polynomial Neural Networks with Nonlinear

Spectral Methods

A. Gautier  Q. Nguyen and M. Hein

Department of Mathematics and Computer Science

Saarland Informatics Campus  Saarland University  Germany

Abstract

The optimization problem behind neural networks is highly non-convex.
Training with stochastic gradient descent and variants requires careful
parameter tuning and provides no guarantee to achieve the global optimum.
In contrast we show under quite weak assumptions on the data that a
particular class of feedforward neural networks can be trained globally
optimal with a linear convergence rate with our nonlinear spectral method.
Up to our knowledge this is the ﬁrst practically feasible method which
achieves such a guarantee. While the method can in principle be applied to
deep networks  we restrict ourselves for simplicity in this paper to one and
two hidden layer networks. Our experiments conﬁrm that these models are
rich enough to achieve good performance on a series of real-world datasets.

1 Introduction

Deep learning [13  16] is currently the state of the art machine learning technique in
many application areas such as computer vision or natural language processing. While the
theoretical foundations of neural networks have been explored in depth see e.g.
[1]  the
understanding of the success of training deep neural networks is a currently very active
research area [5  6  9]. On the other hand the parameter search for stochastic gradient descent
and variants such as Adagrad and Adam can be quite tedious and there is no guarantee that
one converges to the global optimum. In particular  the problem is even for a single hidden
layer in general NP hard  see [17] and references therein. This implies that to achieve global
optimality eﬃciently one has to impose certain conditions on the problem.
A recent line of research has directly tackled the optimization problem of neural networks
and provided either certain guarantees [2  15] in terms of the global optimum or proved
directly convergence to the global optimum [8  11]. The latter two papers are up to our
knowledge the ﬁrst results which provide a globally optimal algorithm for training neural
networks. While providing a lot of interesting insights on the relationship of structured
matrix factorization and training of neural networks  Haeﬀele and Vidal admit themselves
in their paper [8] that their results are “challenging to apply in practice”. In the work of
Janzamin et al. [11] they use a tensor approach and propose a globally optimal algorithm
for a feedforward neural network with one hidden layer and squared loss. However  their
approach requires the computation of the score function tensor which uses the density of
the data-generating measure. However  the data generating measure is unknown and also
diﬃcult to estimate for high-dimensional feature spaces. Moreover  one has to check certain
non-degeneracy conditions of the tensor decomposition to get the global optimality guarantee.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In contrast our nonlinear spectral method just requires that the data is nonnegative which is
true for all sorts of count data such as images  word frequencies etc. The condition which
guarantees global optimality just depends on the parameters of the architecture of the network
and boils down to the computation of the spectral radius of a small nonnegative matrix.
The condition can be checked without running the algorithm. Moreover  the nonlinear
spectral method has a linear convergence rate and thus the globally optimal training of the
network is very fast. The two main changes compared to the standard setting are that we
require nonnegativity on the weights of the network and we have to minimize a modiﬁed
objective function which is the sum of loss and the negative total sum of the outputs. While
this model is non-standard  we show in some ﬁrst experimental results that the resulting
classiﬁer is still expressive enough to create complex decision boundaries. As well  we achieve
competitive performance on some UCI datasets. As the nonlinear spectral method requires
some non-standard techniques  we use the main part of the paper to develop the key steps
necessary for the proof. However  some proofs of the intermediate results are moved to the
supplementary material.

2 Main result

In this section we present the algorithm together with the
main theorem providing the convergence guarantee. We limit
the presentation to one hidden layer networks to improve the
readability of the paper. Our approach can be generalized
to feedforward networks of arbitrary depth. In particular  we
present in Section 4.1 results for two hidden layers.
We consider in this paper multi-class classiﬁcation where d is
the dimension of the feature space and K is the number of
classes. We use the negative cross-entropy loss deﬁned for label
y ∈ [K] := {1  . . .   K} and classiﬁer f : Rd → RK as

!

= −fy(x) + log(cid:16) KX

efj(x)(cid:17)

.

PK

efy(x)
j=1 efj(x)

j=1

(cid:17)αl

n1X

(cid:16) dX

Figure 1: Classiﬁcation decision
boundaries in R2. (Best viewed in
colors.)

L(cid:0)y  f(x)(cid:1) = − log

 

The function class we are using is a feedforward neural network with one hidden layer with
n1 hidden units. As activation functions we use real powers of the form of a generalized
polyomial  that is for α ∈ Rn1 with αi ≥ 1  i ∈ [K]  we deﬁne:

fr(x) = fr(w  u)(x) =

wrl

ulmxm

 

(1)

m=1
l=1
  u ∈ Rn1×d

+

where R+ = {x ∈ R| x ≥ 0} and w ∈ RK×n1+
are the parameters of the network
which we optimize. The function class in (1) can be seen as a generalized polynomial in the
sense that the powers do not have to be integers. Polynomial neural networks have been
recently analyzed in [15]. Please note that a ReLU activation function makes no sense in
our setting as we require the data as well as the weights to be nonnegative. Even though
nonnegativity of the weights is a strong constraint  one can model quite complex decision
boundaries (see Figure 1  where we show the outcome of our method for a toy dataset in R2).
In order to simplify the notation we use w = (w1  . . .   wK) for the K output units wi ∈ Rn1+  
i = 1  . . .   K. All output units and the hidden layer are normalized. We optimize over the set

(cid:12)(cid:12) kukpu = ρu  kwikpw = ρw  ∀i = 1  . . .   K(cid:9).

S+ =(cid:8)(w  u) ∈ RK×n1+

We also introduce S++ where one replaces R+ with R++ = {t ∈ R | t > 0}. The ﬁnal
optimization problem we are going to solve is given as

× Rn1×d

+

nX

max

(w u)∈S+

h − L

Φ(w  u) with

(cid:16)
yi  f(w  u)(xi)(cid:17) +

i=1

Φ(w  u) = 1

n

KX

r=1

fr(w  u)(xi)i + 

(cid:16) KX

n1X

r=1

l=1

n1X

dX

l=1

m=1

wr l +

2

(2)

(cid:17)

ulm

 

00.010.020.030.040.050.060.070.080.090.100.010.020.030.040.050.060.07where (xi  yi) ∈ Rd+ × [K]  i = 1  . . .   n is the training data. Note that this is a maximization
problem and thus we use minus the loss in the objective so that we are eﬀectively minimizing
the loss. The reason to write this as a maximization problem is that our nonlinear spectral
method is inspired by the theory of (sub)-homogeneous nonlinear eigenproblems on convex
cones [14] which has its origin in the Perron-Frobenius theory for nonnegative matrices.
In fact our work is motivated by the closely related Perron-Frobenius theory for multi-
homogeneous problems developed in [7]. This is also the reason why we have nonnegative
weights  as we work on the positive orthant which is a convex cone. Note that  > 0 in the
objective can be chosen arbitrarily small and is added out of technical reasons.
In order to state our main theorem we need some additional notation. For p ∈ (1 ∞)  we
let p0 = p/(p − 1) be the Hölder conjugate of p  and ψp(x) = sign(x)|x|p−1. We apply ψp to
scalars and vectors in which case the function is applied componentwise. For a square matrix
A we denote its spectral radius by ρ(A). Finally  we write ∇wiΦ(w  u) (resp. ∇uΦ(w  u)) to
(cid:19)
denote the gradient of Φ with respect to wi (resp. u) at (w  u). The mapping

(cid:18) ρwψp0

 
(3)
deﬁnes a sequence converging to the global optimum of (2). Indeed  we prove:
i=1 ⊂ Rd+ × [K]  pw  pu ∈ (1 ∞)  ρw  ρu > 0  n1 ∈ N and α ∈
Theorem 1. Let {xi  yi}n
Pn1
Rn1 with αi ≥ 1 for every i ∈ [n1]. Deﬁne ρx  ξ1  ξ2 > 0 as ρx = maxi∈[n] kxik1  ξ1 =
l=1 αl(ρuρx)αl and let A ∈ R(K+1)×(K+1)
ρw
Al m = 4(p0
AK+1 m = 2(p0
If the spectral radius ρ(A) of A satisﬁes ρ(A) < 1  then (2) has a unique global maximizer
(w∗  u∗) ∈ S++. Moreover  for every (w0  u0) ∈ S++  there exists R > 0 such that

u − 1)(2ξ1 + 1)  AK+1 K+1 = 2(p0

u − 1)(2ξ2 + kαk∞ − 1) 

w − 1)(2ξ2 + kαk∞) 

l=1(ρuρx)αl  ξ2 = ρw

Al K+1 = 2(p0

ρuψp0
kψp0

(∇uΦ(w  u))
(∇uΦ(w  u))kpu

u

ρwψp0
kψp0

(∇wKΦ(w  u))
(∇wKΦ(w  u))kpw

w

∀m  l ∈ [K].

(∇w1Φ(w  u))
(∇w1Φ(w  u))kpw

w

kψp0

w

be deﬁned as

w − 1)ξ1 

GΦ(w  u) =

Pn1

  . . .  

w

 

u

++

k→∞(wk  uk) = (w∗  u∗)
lim

and

k(wk  uk) − (w∗  u∗)k∞ ≤ R ρ(A)k

∀k ∈ N 

where (wk+1  uk+1) = GΦ(wk  uk) for every k ∈ N.
Note that one can check for a given model (number of hidden units n1  choice of α  pw  pu 
ρu  ρw) easily if the convergence guarantee to the global optimum holds by computing the
spectral radius of a square matrix of size K + 1. As our bounds for the matrix A are very
conservative  the “eﬀective” spectral radius is typically much smaller  so that we have very
fast convergence in only a few iterations  see Section 5 for a discussion. Up to our knowledge
this is the ﬁrst practically feasible algorithm to achieve global optimality for a non-trivial
neural network model. Additionally  compared to stochastic gradient descent  there is no
free parameter in the algorithm. Thus no careful tuning of the learning rate is required. The
reader might wonder why we add the second term in the objective  where we sum over all
outputs. The reason is that we need that the gradient of GΦ is strictly positive in S+  this is
why we also have to add the third term for arbitrarily small  > 0. In Section 5 we show
that this model achieves competitive results on a few UCI datasets.

Choice of α:
It turns out that in order to get a non-trivial classiﬁer one has to choose
α1  . . .   αn1 ≥ 1 so that αi 6= αj for every i  j ∈ [n1] with i 6= j. The reason for this
lies in certain invariance properties of the network. Suppose that we use a permutation
invariant componentwise activation function σ  that is σ(P x) = P σ(x) for any permutation
matrix P and suppose that A  B are globally optimal weight matrices for a one hidden layer
architecture  then for any permutation matrix P 

Aσ(Bx) = AP T P σ(Bx) = AP T σ(P Bx) 

which implies that A0 = AP T and B0 = P B yield the same function and thus are also
globally optimal. In our setting we know that the global optimum is unique and thus it has
to hold that  A = AP T and B = P B for all permutation matrices P. This implies that
both A and B have rank one and thus lead to trivial classiﬁers. This is the reason why one
has to use diﬀerent α for every unit.

3

+

Let Q  ˜Q ∈ Rm×m

Dependence of ρ(A) on the model parameters:
and assume
0 ≤ Qi j ≤ ˜Qi j for every i  j ∈ [m]  then ρ(Q) ≤ ρ( ˜Q)  see Corollary 3.30 [3]. It follows
that ρ(A) in Theorem 1 is increasing w.r.t. ρu  ρw  ρx and the number of hidden units
n1. Moreover  ρ(A) is decreasing w.r.t. pu  pw and in particular  we note that for any
ﬁxed architecture (n1  α  ρu  ρw) it is always possible to ﬁnd pu  pw large enough so that
ρ(A) < 1. Indeed  we know from the Collatz-Wielandt formula (Theorem 8.1.26 in [10]) that
ρ(A) = ρ(AT ) ≤ maxi∈[K+1](AT v)i/vi for any v ∈ RK+1
++ . We use this to derive lower bounds
on pu  pw that ensure ρ(A) < 1. Let v = (pw − 1  . . .   pw − 1  pu − 1)  then (AT v)i < vi for
every i ∈ [K + 1] guarantees ρ(A) < 1 and is equivalent to

pw > 4(K + 1)ξ1 + 3

(4)
where ξ1  ξ2 are deﬁned as in Theorem 1. However  we think that our current bounds are
sub-optimal so that this choice is quite conservative. Finally  we note that the constant R in
Theorem 1 can be explicitly computed when running the algorithm (see Theorem 3).

pu > 2(K + 1)(kαk∞ + 2ξ2) − 1 

and

Proof Strategy:
algorithm. For that we need some further notation. We introduce the sets

The following main part of the paper is devoted to the proof of the

V+ = RK×n1+

B+ =(cid:8)(w  u) ∈ V+

× Rn1×d

(cid:12)(cid:12) kukpu ≤ ρu  kwikpw ≤ ρw  ∀i = 1  . . .   K} 

V++ = RK×n1++ × Rn1×d
++

+

 

and similarly we deﬁne B++ replacing V+ by V++ in the deﬁnition. The high-level idea of
the proof is that we ﬁrst show that the global maximum of our optimization problem in (2)
is attained in the “interior” of S+  that is S++. Moreover  we prove that any critical point of
(2) in S++ is a ﬁxed point of the mapping GΦ. Then we proceed to show that there exists a
unique ﬁxed point of GΦ in S++ and thus there is a unique critical point of (2) in S++. As
the global maximizer of (2) exists and is attained in the interior  this ﬁxed point has to be
the global maximizer.
Finally  the proof of the fact that GΦ has a unique ﬁxed point follows by noting that GΦ
maps B++ into B++ and the fact that B++ is a complete metric space with respect to the
Thompson metric. We provide a characterization of the Lipschitz constant of GΦ and in turn
derive conditions under which GΦ is a contraction. Finally  the application of the Banach
ﬁxed point theorem yields the uniqueness of the ﬁxed point of GΦ and the linear convergence
rate to the global optimum of (2). In Section 4 we show the application of the established
framework for our neural networks.

3 From the optimization problem to ﬁxed point theory
Lemma 1. Let Φ : V → R be diﬀerentiable. If ∇Φ(w  u) ∈ V++ for every (w  u) ∈ S+  then
the global maximum of Φ on S+ is attained in S++.
We now identify critical points of the objective Φ in S++ with ﬁxed points of GΦ in S++.
Lemma 2. Let Φ : V → R be diﬀerentiable. If ∇Φ(w  u) ∈ V++ for all (w  u) ∈ S++  then
(w∗  u∗) is a critical point of Φ in S++ if and only if it is a ﬁxed point of GΦ.
Our goal is to apply the Banach ﬁxed point theorem to GΦ : B++ → S++ ⊂ B++. We recall
this theorem for the convenience of the reader.
Theorem 2 (Banach ﬁxed point theorem e.g. [12]). Let (X  d) be a complete metric space
with a mapping T : X → X such that d(T(x)  T(y)) ≤ q d(x  y) for q ∈ [0  1) and all x  y ∈ X.
Then T has a unique ﬁxed-point x∗ in X  that is T(x∗) = x∗ and the sequence deﬁned as
xn+1 = T(xn) with x0 ∈ X converges limn→∞ xn = x∗ with linear convergence rate

d(xn  x∗) ≤ qn
1 − q

d(x1  x0).

So  we need to endow B++ with a metric µ so that (B++  µ) is a complete metric space. A
popular metric for the study of nonlinear eigenvalue problems on the positive orthant is the
so-called Thompson metric d: Rm++ × Rm++ → R+ [18] deﬁned as

ln(z) =(cid:0) ln(z1)  . . .   ln(zm)(cid:1).

d(z  ˜z) = k ln(z) − ln(˜z)k∞

where

4

Using the known facts that (Rn++  d) is a complete metric space and its topology coincides
with the norm topology (see e.g. Corollary 2.5.6 and Proposition 2.5.2 [14])  we prove:
Lemma 3. For p ∈ (1 ∞) and ρ > 0  ({z ∈ Rn++ | kzkp ≤ ρ}  d) is a complete metric space.
Now  the idea is to see B++ as a product of such metric spaces. For i = 1  . . .   K  let
Bi++ = {wi ∈ Rn1++ | kwikpw ≤ ρw} and di(wi  ˜wi) = γik ln(wi) − ln( ˜wi)k∞ for some
++ | kukpu ≤ ρu} and dK+1(u  ˜u) =
constant γi > 0. Furthermore  let BK+1
γK+1k ln(u) − ln(˜u)k∞. Then (Bi++  di) is a complete metric space for every i ∈ [K + 1] and
++ × . . . × BK++ × BK+1
B++ = B1
++ . It follows that (B++  µ) is a complete metric space with
µ: B++ × B++ → R+ deﬁned as
KX

++ = {u ∈ Rn1×d

γik ln(wi) − ln( ˜wi)k∞ + γK+1k ln(u) − ln(˜u)k∞.

µ(cid:0)(w  u)  ( ˜w  ˜u)(cid:1) =

i=1

The motivation for introducing the weights γ1  . . .   γK+1 > 0 is given by the next theorem.
We provide a characterization of the Lipschitz constant of a mapping F : B++ → B++ with
respect to µ. Moreover  this Lipschitz constant can be minimized by a smart choice of γ.
For i ∈ [K]  a  j ∈ [n1]  b ∈ [d]  we write Fwi j and Fuab to denote the components of F such
that F = (Fw1 1  . . .   Fw1 n1   Fw2 1  . . .   FwK n1   Fu11   . . .   Fun1 d).
Lemma 4. Suppose that F ∈ C1(B++  V++) and A ∈ R(K+1)×(K+1)

satisﬁes

(cid:10)|∇uFwi j(w  u)|  u(cid:11) ≤ Ai K+1 Fwi j(w  u)

(cid:10)|∇wk Fwi j(w  u)|  wk

(cid:11) ≤ Ai k Fwi j(w  u) 

+

and
h|∇wk Fuab(w  u)|  wki ≤ AK+1 k Fuab(w  u) 
h|∇uFuab(w  u)|  ui ≤ AK+1 K+1 Fuab(w  u)
for all i  k ∈ [K]  a  j ∈ [n1]  b ∈ [d] and (w  u) ∈ B++. Then  for every (w  u)  ( ˜w  ˜u) ∈ B++
it holds

µ(cid:0)F(w  u)  F( ˜w  ˜u)(cid:1) ≤ U µ(cid:0)(w  u)  ( ˜w  ˜u)(cid:1)

with

U = max
k∈[K+1]

(AT γ)k

γk

.

Note that  from the Collatz-Wielandt ratio for nonnegative matrices  we know that the
constant U in Lemma 4 is lower bounded by the spectral radius ρ(A) of A. Indeed  by
Theorem 8.1.31 in [10]  we know that if AT has a positive eigenvector γ ∈ RK+1

++   then

max
i∈[K+1]

(AT γ)i

γi

= ρ(A) = min
˜γ∈RK+1
++

max
i∈[K+1]

(AT ˜γ)i

˜γi

.

(5)

Therefore  in order to obtain the minimal Lipschitz constant U in Lemma 4  we choose the
weights of the metric µ to be the components of γ. A combination of Theorem 2  Lemma 4
and this observation implies the following result.
Theorem 3. Let Φ ∈ C1(V  R) ∩ C2(B++  R) with ∇Φ(S+) ⊂ V++. Let GΦ : B++ → B++
be deﬁned as in (3). Suppose that there exists a matrix A ∈ R(K+1)×(K+1)
such that GΦ and
A satisﬁes the assumptions of Lemma 4 and AT has a positive eigenvector γ ∈ RK+1
++ . If
ρ(A) < 1  then Φ has a unique critical point (w∗  u∗) in S++ which is the global maximum of
k deﬁned for any (w0  u0) ∈
S++ as (wk+1  uk+1) = GΦ(wk  uk)  k ∈ N  satisﬁes limk→∞(wk  uk) = (w∗  u∗) and

the optimization problem (2). Moreover  the sequence(cid:0)(wk  uk)(cid:1)
µ(cid:0)(w1  u1)  (w0  u0)(cid:1)
(cid:0)1 − ρ(A)(cid:1) min(cid:8) γK+1

k(wk  uk) − (w∗  u∗)k∞ ≤ ρ(A)k

(cid:9)(cid:19)

∀k ∈ N 

  mint∈[K] γt

(cid:18)

+

ρu

ρw

where the weights in the deﬁnition of µ are the entries of γ.

4 Application to Neural Networks
In the previous sections we have outlined the proof of our main result for a general objective
function satisfying certain properties. The purpose of this section is to prove that the
properties hold for our optimization problem for neural networks.

5

We recall our objective function from (2)
Φ(w  u) = 1

h − L(cid:0)yi  f(w  u)(xi)(cid:1) +

nX

n

i=1

KX

r=1

and the function class we are considering from (1)

fr(x) = fr(w  u)(x) =

(cid:17)

ulm

n1X

dX

l=1

m=1

wr l +

fr(w  u)(xi)i + 
(cid:16) dX
n1X

(cid:16) KX
n1X
(cid:17)αl

r=1

l=1

ulmxm

 

wr l

l=1

m=1

The arbitrarily small  in the objective is needed to make the gradient strictly positive on
the boundary of V+. We note that the assumption αi ≥ 1 for every i ∈ [n1] is crucial in the
following lemma in order to guarantee that ∇Φ is well deﬁned on S+.
Lemma 5. Let Φ be deﬁned as in (2)  then ∇Φ(w  u) is strictly positive for any (w  u) ∈ S+.
Next  we derive the matrix A ∈ R(K+1)×(K+1) in order to apply Theorem 3 to GΦ with
Φ deﬁned in (2). As discussed in its proof  the matrix A given in the following theorem
has a smaller spectral radius than that of Theorem 1. To express this matrix  we consider
p q : Rn1++ × R++ → R++ deﬁned for p  q ∈ (1 ∞) and α ∈ Rn1++ as
Ψα

i1− αp

(cid:19)1/p

Ψα
p q(δ  t) =

(δl tαl) p q
q−αp

q + max
j∈J c

(δj tαj)p

 

(6)

(cid:18)hX

l∈J

where J = {l ∈ [n1] | αlp ≤ q}  J c = {l ∈ [n1] | αlp > q} and α = minl∈J αl.
Theorem 4. Let Φ be deﬁned as above and GΦ be as in (3). Set Cw = ρw Ψα
p0
w pu
Cu = ρw Ψα
p0
w pu
of Lemma 4 with

(α  ρuρx) and ρx = maxi∈[n] kxikp0

(1  ρuρx) 
. Then A and GΦ satisfy all assumptions

u

A = 2 diag(cid:0)p0

w − 1  p0
w − 1  . . .   p0
++   Qu w ∈ R1×K

u − 1(cid:1)(cid:18)Qw w Qw u

Qu w Qu u

(cid:19)

where Qw w ∈ RK×K

++   Qw u ∈ RK×1

Qw w = 2Cw11T  
Qu w = (2Cw + 1)1T   Qu u = (2Cu + kαk∞ − 1).

++ and Qu u ∈ R++ are deﬁned as
Qw u = (2Cu + kαk∞)1 

In the supplementary material  we prove that Ψα
l=1 δltαl which yields the weaker
bounds ξ1  ξ2 given in Theorem 1. In particular  this observation combined with Theorems 3
and 4 implies Theorem 1.

4.1 Neural networks with two hidden layers
We show how to extend our framework for neural networks with 2 hidden layers. In future
work we will consider the general case. We brieﬂy explain the major changes. Let n1  n2 ∈ N
and α ∈ Rn1++  β ∈ Rn2++ with αi  βj ≥ 1 for all i ∈ [n1]  j ∈ [n2]  our function class is:

p q(δ  t) ≤Pn1

fr(x) = fr(w  v  u)(x) =

and the optimization problem becomes

n2X

(cid:16) n1X

(cid:16) dX

wr l

vlm

umsxs

l=1

m=1

s=1

(cid:17)αm(cid:17)βl

max

(w v u)∈S+

Φ(w  v  u)

+
S+ = {(w1  . . .   wK  v  u) ∈ V+ | kwikpw = ρw  kvkpv = ρv  kukpu = ρu} and
Φ(w  v  u) = 1

h−L(cid:0)yi  f(xi)(cid:1)+

fr(xi)i+

(cid:16) KX

V+ = RK×n2+
n2X

× Rn2×n1

nX

KX

n2X

n1X

wr l+

where

+

vlm+

× Rn1×d

 

(7)

n1X

dX

(cid:17)

.

ums

n

i=1

r=1

r=1

l=1

l=1

m=1

m=1

s=1

6

The map GΦ : S++ → S++ = {z ∈ S+ | z > 0}  GΦ = (GΦ
(∇wiΦ(w  u))

GΦ
wi

(w  v  u) = ρw

w

ψp0
(∇wiΦ(w  v  u))kpw

kψp0

w

w1  . . .   GΦ

wK

  GΦ

v   GΦ

u )  becomes

∀i ∈ [K]

(8)

and

(∇vΦ(w  v  u))
(∇vΦ(w  v  u))kpv

v

 

(∇uΦ(w  v  u))
(∇uΦ(w  v  u))kpu

u

.

u

as

 

u

ψp0
kψp0

v

GΦ
v (w  v  u) = ρv

GΦ
u (w  v  u) = ρu

(1  θ)  Cv = ρwΨβ
p0
w pv

(β  θ)  Cu = kαk∞Cv 

(1  ρuρx)  Cw = ρwΨβ
p0
w pv

Am K+1 = 2(p0
AK+1 l = 2(p0
AK+1 K+2 = 2(p0
AK+2 K+1 = 2(p0

u − 1)(2Cw + 1) 
u − 1)(2Cu + kαk∞kβk∞ − 1)

w − 1)(2Cv + kβk∞)

v − 1)(cid:0)2Cw + 1(cid:1)
v − 1)(cid:0)2Cu + kαk∞kβk∞(cid:1)

i=1 ⊂ Rd+ × [K]  pw  pv  pu ∈ (1 ∞)  ρw  ρv  ρu > 0  n1  n2 ∈ N and

w − 1)(cid:0)2Cu + kαk∞kβk∞(cid:1) 
v − 1)(cid:0)2Cv + kβk∞ − 1(cid:1) 

ψp0
kψp0
We have the following equivalent of Theorem 1 for 2 hidden layers.
Theorem 5. Let {xi  yi}n
α ∈ Rn1++  β ∈ Rn2++ with αi  βj ≥ 1 for all i ∈ [n1]  j ∈ [n2]. Let ρx = maxi∈[n] kxikp0
θ = ρvΨα
p0
v pu
and deﬁne A ∈ R(K+2)×(K+2)
++
w − 1)Cw 
Am l = 4(p0
Am K+2 = 2(p0
AK+1 K+1 = 2(p0
AK+2 l = 2(p0
AK+2 K+2 = 2(p0
If ρ(A) < 1  then (7) has a unique global maximizer (w∗  v∗  u∗) ∈ S++. Moreover  for every
(w0  v0  u0) ∈ S++  there exists R > 0 such that
k→∞(wk  vk  uk) = (w∗  v∗  u∗)
lim
where (wk+1  vk+1  uk+1) = GΦ(wk  vk  uk) for every k ∈ N and GΦ is deﬁned as in (8).
As for the case with one hidden layer  for any ﬁxed architecture ρw  ρv  ρu > 0  n1  n2 ∈ N
and α ∈ Rn1++  β ∈ Rn2++ with αi  βj ≥ 1 for all i ∈ [n1]  j ∈ [n2]  it is possible to derive lower
bounds on pw  pv  pu that guarantee ρ(A) < 1 in Theorem 5. Indeed  it holds

iβj
pw > 4(K+2)ζ1+5  pv > 2(K+2)(cid:2)2ζ2+kβk∞(cid:3)−1  pu > 2(K+2)kαk∞(2ζ2+kβk∞)−1. (9)

with ˜ρx = maxi∈[n] kxik1. Hence  the two hidden layers equivalent of (4) becomes

k(wk  vk  uk)−(w∗  v∗  u∗)k∞ ≤ R ρ(A)k ∀k ∈ N

u − 1)(2Cv + kβk∞) 

and Cv ≤ ζ2 = ρw

Cw ≤ ζ1 = ρw

∀m  l ∈ [K].

n2X

h

n2X

j=1

n1X

l=1

n1X

l=1

(ρu ˜ρx)αl

(ρu ˜ρx)αl

iβj

ρv

j=1

h

βj

ρv

and

 

5 Experiments

Table 1: Test accuracy on UCI datasets

NLSM1 NLSM2 ReLU1 ReLU2 SVM
Dataset
95.7
96.4
Cancer
100
Iris
90.0
100
Banknote 97.1
77.3
Blood
76.0
72.1
Haberman 75.4
Seeds
88.1
95.2
79.9
79.2
Pima

95.7
100
100
76.0
70.5
90.5
76.6

93.6
93.3
97.8
76.0
72.1
92.9
79.2

96.4
96.7
96.4
76.7
75.4
90.5
80.5

the optimal score p∗
Figure 2: Training score (left) w.r.t.
and test error (right) of NLSM1 and Batch-SGD with diﬀerent
step-sizes.
The shown experiments should be seen as a proof of concept. We do not have yet a
good understanding of how one should pick the parameters of our model to achieve good
performance. However  the other papers which have up to now discussed global optimality
for neural networks [11  8] have not included any results on real datasets. Thus  up to our

7

100101102epochs10-1410-1210-1010-8(p∗−f)/|p∗|NLSM1SGD 100SGD 10SGD 1SGD 0.1SGD 0.01100101102103epochs020406080100test errorNLSM1SGD 100SGD 10SGD 1SGD 0.1SGD 0.01Nonlinear Spectral Method for 1 hidden layer

Input: Model n1 ∈ N  pw  pu ∈ (1 ∞)  ρw  ρu > 0  α1  . . .   αn1 ≥ 1   > 0 so that the
matrix A of Theorem 1 satisﬁes ρ(A) < 1. Accuracy τ > 0 and (w0  u0) ∈ S++.
1 Let (w1  u1) = GΦ(w0  u0) and compute R as in Theorem 3
2 Repeat
3
4

(wk+1  uk+1) = GΦ(wk  uk)
k ← k + 1

5 Until k ≥ ln(cid:0)τ /R(cid:1)/ ln(cid:0)ρ(A)(cid:1)

Output: (wk  uk) fulﬁlls k(wk  uk) − (w∗  u∗)k∞ < τ.
With GΦ deﬁned as in (3). The method for two hidden layers is similar: consider GΦ

as in (8) instead of (3) and assume that the model satisﬁes Theorem 5.

knowledge  we show for the ﬁrst time a globally optimal algorithm for neural networks that
leads to non-trivial classiﬁcation results.
We test our methods on several low dimensional UCI datasets and denote our algorithms as
NLSM1 (one hidden layer) and NLSM2 (two hidden layers). We choose the parameters of our
model out of 100 randomly generated combinations of (n1  α  ρw  ρu) ∈ [2  20]× [1  4]× (0  1]2
(respectively (n1  n2  α  β  ρw  ρv  ρu) ∈ [2  10]2 × [1  4]2 × (0  1]2) and pick the best one based
on 5-fold cross-validation error. We use Equation (4) (resp. Equation (9)) to choose pu  pw
(resp. pu  pv  pw) so that every generated model satisﬁes the conditions of Theorem 1 (resp.
Theorem 5)  i.e. ρ(A) < 1. Thus  global optimality is guaranteed in all our experiments.
For comparison  we use the nonlinear RBF-kernel SVM and implement two versions of the
Rectiﬁed-Linear Unit network - one for one hidden layer networks (ReLU1) and one for
two hidden layers networks (ReLU2). To train ReLU  we use a stochastic gradient descent
method which minimizes the sum of logistic loss and L2 regularization term over weight
matrices to avoid over-ﬁtting. All parameters of each method are jointly cross validated.
More precisely  for ReLU the number of hidden units takes values from 2 to 20  the step-sizes
and regularizers are taken in {10−6  10−5  . . .   102} and {0  10−4  10−3  . . .   104} respectively.
For SVM  the hyperparameter C and the kernel parameter γ of the radius basis function
K(xi  xj) = exp(−γkxi − xjk2) are taken from {2−5  2−4 . . .   220} and {2−15  2−14 . . .   23}
respectively. Note that ReLUs allow negative weights while our models do not. The results
presented in Table 1 show that overall our nonlinear spectral methods achieve slightly worse
performance than kernel SVM while being competitive/slightly better than ReLU networks.
Notably in case of Cancer  Haberman and Pima  NLSM2 outperforms all the other models.
For Iris and Banknote  we note that without any constraints ReLU1 can easily ﬁnd an
architecture which achieves zero test error while this is diﬃcult for our models as we impose
constraints on the architecture in order to prove global optimality.
We compare our algorithms with Batch-SGD in order to optimize (2) with batch-size being
5% of the training data while the step-size is ﬁxed and selected between 10−2 and 102.
At each iteration of our spectral method and each epoch of Batch-SGD  we compute the
objective and test error of each method and show the results in Figure 2. One can see that
our method is much faster than SGDs  and has a linear convergence rate. We noted in
our experiments that as α is large and our data lies between [0  1]  all units in the network
tend to have small values that make the whole objective function relatively small. Thus  a
relatively large change in (w  u) might cause only small changes in the objective function
but performance may vary signiﬁcantly as the distance is large in the parameter space. In
other words  a small change in the objective may have been caused by a large change in the
parameter space  and thus  largely inﬂuences the performance - which explains the behavior
of SGDs in Figure 2.
The magnitude of the entries of the matrix A in Theorems 1 and 5 grows with the number
of hidden units and thus the spectral radius ρ(A) also increases with this number. As we
expect that the number of required hidden units grows with the dimension of the datasets
we have limited ourselves in the experiments to low-dimensional datasets. However  these
bounds are likely not to be tight  so that there might be room for improvement in terms of
dependency on the number of hidden units.

8

Acknowledgment
The authors acknowledge support by the ERC starting grant NOLEPRO 307793.

References
[1] M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge

University Press  New York  1999.

[2] S. Arora  A. Bhaskara  R. Ge  and T. Ma. Provable bounds for learning some deep representa-

tions. In ICML  2014.

[3] A. Berman and R. J. Plemmons. Nonnegative Matrices in the Mathematical Sciences. SIAM 

Philadelphia  1994.

[4] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  Belmont  Mass.  1999.
[5] A. Choromanska  M. Hena  M. Mathieu  G. B. Arous  and Y. LeCun. The loss surfaces of

multilayer networks. In AISTATS  2015.

[6] A Daniely  R. Frostigy  and Y. Singer. Toward deeper understanding of neural networks: The

power of initialization and a dual view on expressivity  2016. arXiv:1602.05897v1.

[7] A. Gautier  F. Tudisco  and M. Hein. The Perron-Frobenius Theorem for Multi-Homogeneous

Maps. in preparation  2016.

[8] B. D. Haeﬀele and Rene Vidal. Global optimality in tensor factorization  deep learning  and

beyond  2015. arXiv:1506.07540v1.

[9] M. Hardt  B. Recht  and Y. Singer. Train faster  generalize better: Stability of stochastic

gradient descent. In ICML  2016.

[10] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press  New York 

second edition  2013.

[11] M. Janzamin  H. Sedghi  and A. Anandkumar. Beating the perils of non-convexity:guaranteed

training of neural networks using tensor methods  2015. arXiv:1506.08473v3.

[12] W. A. Kirk and M. A. Khamsi. An Introduction to Metric Spaces and Fixed Point Theory.

John Wiley  New York  2001.

[13] Y. LeCun  Y. Bengio  and G. Hinton. Deep learning. Nature  521  2015.
[14] B. Lemmens and R. D. Nussbaum. Nonlinear Perron-Frobenius theory. Cambridge University

Press  New York  general edition  2012.

[15] R. Livni  S. Shalev-Shwartz  and O. Shamir. On the computational eﬃciency of training neural

networks. In NIPS  pages 855–863  2014.

[16] J. Schmidhuber. Deep Learning in Neural Networks: An Overview. Neural Networks  61:85–117 

2015.

[17] J. Sima. Training a single sigmoidal neuron is hard. Neural Computation  14:2709–2728  2002.
[18] A. C. Thompson. On certain contraction mappings in a partially ordered vector space.

Proceedings of the American Mathematical Society  14:438–443  1963.

9

,Ying Liu
Alan Willsky
Antoine Gautier
Quynh Nguyen
Matthias Hein