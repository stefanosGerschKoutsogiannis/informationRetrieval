2013,Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space,Incorporating invariance information is important for many learning problems. To exploit invariances  most existing methods resort to approximations that either lead to expensive optimization problems such as semi-definite programming  or rely on separation oracles to retain tractability.  Some methods further limit the space of functions and settle for non-convex models.  In this paper  we propose a framework for learning in reproducing kernel Hilbert spaces (RKHS) using local invariances that explicitly characterize the behavior of the target function around data instances.  These invariances are \emph{compactly} encoded as linear functionals whose value are penalized by some loss function.  Based on a representer theorem that we establish  our formulation can be efficiently optimized via a convex program. For the representer theorem to hold  the linear functionals are required to be bounded in the RKHS  and we show that this is true for a variety of commonly used RKHS and invariances. Experiments on learning with unlabeled data and transform invariances show that the proposed method yields better or similar results compared with the state of the art.,Learning with Invariance via Linear Functionals on

Reproducing Kernel Hilbert Space

Xinhua Zhang

Machine Learning Research Group
National ICT Australia and ANU

xinhua.zhang@nicta.com.au

Wee Sun Lee

Department of Computer Science
National University of Singapore

leews@comp.nus.edu.sg

Yee Whye Teh

Department of Statistics
University of Oxford
y.w.teh@stats.ox.ac.uk

Abstract

Incorporating invariance information is important for many learning problems. To
exploit invariances  most existing methods resort to approximations that either
lead to expensive optimization problems such as semi-deﬁnite programming  or
rely on separation oracles to retain tractability. Some methods further limit the
space of functions and settle for non-convex models. In this paper  we propose a
framework for learning in reproducing kernel Hilbert spaces (RKHS) using local
invariances that explicitly characterize the behavior of the target function around
data instances. These invariances are compactly encoded as linear functionals
whose value are penalized by some loss function. Based on a representer theo-
rem that we establish  our formulation can be efﬁciently optimized via a convex
program. For the representer theorem to hold  the linear functionals are required
to be bounded in the RKHS  and we show that this is true for a variety of com-
monly used RKHS and invariances. Experiments on learning with unlabeled data
and transform invariances show that the proposed method yields better or similar
results compared with the state of the art.

Introduction

1
Invariances are among the most useful prior information used in machine learning [1]. In many
vision problems such as handwritten digit recognition  detectors are often supposed to be invariant
to certain local transformations  such as translation  rotation  and scaling [2  3]. One way to utilize
this invariance is by assuming that the gradient of the function is small along the directions of trans-
formation at each data instance. Another important scenario is semi-supervised learning [4]  which
relies on reasonable priors over the relationship between the data distribution and the discriminant
function [5  6]. It is commonly assumed that the function does not change much in the proximity of
each observed data instance  which reﬂects the typical clustering structure in the data set: instances
from the same class are clustered together and away from those of different classes [7–9]. Another
popular assumption is that the function varies smoothly over the graph Laplacian [10–12].
A number of existing works have established a mathematical framework for learning with invari-
ance. Suppose T (x  θ) transforms a data point x by an operator T with a parameter θ (e.g. T for
rotation and θ for the degree of rotation). Then to incorporate invariance  the target function f is
assumed to be (almost) invariant over T (x  Θ) := {T (x  θ) : θ ∈ Θ}  where Θ controls the lo-
cality of invariance. The consistency of this framework was shown by [13] in the context of robust
optimization [14]. However  in practice it usually leads to a large or inﬁnite number of constraints 
and hence tractable formulations inevitably rely on approximating or restricting the invariance under
consideration. Finally  this paradigm gets further complicated when f comes from a rich space of
functions  e.g. the reproducing kernel Hilbert space (RKHS) induced by universal kernels.
In [15]  all perturbations within the ellipsoids around instances are treated as invariances. This led to
a second order cone program  which is difﬁcult to solve efﬁciently. In [16]  a discrete set of Θ that
corresponds to feature deletions [17] are considered. The problem is reduced to a quadratic program 
but at the cost of blowing up the number of variables which makes scaling to large problems chal-

1

lenging. A one step approximation of T (x  Θ) via the even-order Taylor expansions around θ = 0
is used in [18]. This results in a semi-deﬁnite programming  which is still hard to solve. A further
simpliﬁcation was introduced by [19]  which performed sparse approximations of T (x  Θ) by ﬁnd-
ing (via an oracle) the most violating instance under the current solution. Besides yielding a cheap
quadratic program at each iteration  it also improved upon the Virtual Support Vector approach in
[20]  which did not have a clear optimization objective despite a similar motivation of sparse ap-
proximation. However  tractable oracles are often unavailable  and so a simpler approximation can
be performed by merely enforcing the invariance of f along some given directions  e.g. the tangent
∂θ|θ=0T (x  θ). This idea was used in [21] in a nonlinear RKHS  but their direction of per-
direction ∂
turbation was not in the original space but in the RKHS. By contrast  [3] did penalize the gradient
of f in the original feature space  but their function space was limited to neural networks and only
locally optimal solutions were found.
The goal of our paper  therefore  is to develop a new framework that: (1) allows a variety of invari-
ances to be compactly encoded over a rich family of functions like RKHS  and (2) allows the search
of the optimal function to be formulated as a convex program that is efﬁciently solvable. The key
requirement to our approach is that the invariances can be characterized by linear functionals that
are bounded (§ 2). Under this assumption  we are able to formulate our model into a standard regu-
larized risk minimization problem  where the objective consists of the sum of loss functions on these
linear functionals  the usual loss functions on the labeled training data  and a regularization penalty
based on the RKHS norm of the function (§ 3). We give a representer theorem that guarantees that
the cost can be minimized by linearly combining a ﬁnite number of basis functions1. Using convex
loss  the resulting optimization problem is a convex program  which can be efﬁciently solved in a
batch or online fashion (§ 5). Note [23] also proposed an operator based model for invariance  but
did not derive a representer theorem and did not study the empirical performance.
We also show that a wide range of commonly used invariances can be encoded as bounded linear
functionals. This include derivatives  transformation invariances  and local averages in commonly
used RKHSs such as those deﬁned by Gaussian and polynomial kernels (§ 4). Experiment show that
the use of some of these invariances within our framework yield better or similar results compared
to the state of the art.
Finally  we point out that our focus is to ﬁnd a function in a given RKHS which respects the pre-
speciﬁed invariances. We are not constructing kernels that instantiate the invariance  e.g. [24  25].
2 Preliminaries
Suppose features of training examples lie in a domain X. A function k : X × X → R is called a
positive semi-deﬁnite kernel (or simply kernel) if for all l ∈ N and all x1  . . .   xl ∈ X  the l× l Gram
matrix K := (k(xi  xj))ij is symmetric positive semi-deﬁnite. Example kernels on Rn×Rn include
polynomial kernel of degree r  which are deﬁned as k(x1  x2) = (x1 · x2 + 1)r 2 as well as Gaus-
sian kernels  deﬁned as k(x1  x2) = κσ(x1  x2) where κσ(x1  x2) := exp(−(cid:107)x1 − x2(cid:107)2 /(2σ2)).
More comprehensive introductions to kernels are available in  e.g.  [26–28].
Given k  let H0 be the set of all ﬁnite linear combinations of functions in {k(x ·) : x ∈ X}  and en-
i=1 αik(xi ·)
j=1 βjk(yj ·). Note (cid:104)f  g(cid:105) is invariant to the form of expansion of f and g [26  27].
Using the positive semi-deﬁnite properties of k  it is easy to show that H0 is an inner product space 
and we call its completion under (cid:104)· ·(cid:105) as a reproducing kernel Hilbert space (RKHS) H induced by k.
For any f ∈ H  the reproducing property implies f (x) = (cid:104)f  k(x ·)(cid:105) and we denote (cid:107)f(cid:107)2 := (cid:104)f  f(cid:105).
2.1 Operators and Representers
Deﬁnition 1 (Bounded Linear Operator and Functional). A linear operator T is a mapping from a
vector space V to a vector space W  such that T (x + y) = T x + T y and T (αx) = α · T x for all
x  y ∈ V and scalar α ∈ R. T is also called a functional if W = R. In the case that V and W are
normed  T is called bounded if c := supx∈V (cid:107)x(cid:107)V=1 (cid:107)T x(cid:107)W is ﬁnite  and we call c as the norm of
the operator  denoted by (cid:107)T(cid:107).

dow on H0 an inner product as (cid:104)f  g(cid:105) =(cid:80)p
and g(·) =(cid:80)q

(cid:80)q
j=1 αiβjk(xi  yj) where f (·) =(cid:80)p

i=1

1A similar result was provided in [22]  but not in the context of learning with invariance.
2We write a variable in boldface if it is a vector in a Euclidean space.

2

Example 1. Let H be an RKHS induced by a kernel k deﬁned on X × X. Then for any x ∈ X  the
linear functional T : H → R deﬁned as T (f ) := f (x) is bounded since |f (x)| = |(cid:104)f  k(x ·)(cid:105)| ≤
(cid:107)k(x ·)(cid:107) · (cid:107)f(cid:107) = k(x  x)1/2(cid:107)f(cid:107) by the Cauchy-Schwarz inequality.
Boundedness of linear functionals is particularly useful thanks to the Riesz representation theorem
which establishes their one-to-one correspondence to V [29].
Theorem 1 (Riesz representation Theorem). Every bounded linear functional L on a Hilbert space
V can be represented in terms of an inner product L(x) = (cid:104)x  z(cid:105) for all x ∈ V  where the representer
of the functional  z ∈ V  has norm (cid:107)z(cid:107) = (cid:107)L(cid:107) and is uniquely determined by L.
Example 2. Let H be the RKHS induced by a kernel k. For any functional L on H  the representer
z can be constructed as z(x) = (cid:104)z  k(x ·)(cid:105) = L(k(x ·)) for all x ∈ X. By Theorem 1  z ∈ H.
Using Riesz’s representer theorem  it is not hard to show that for any bounded linear operator T :
V → V where V is Hilbertian  there exists a unique bounded linear operator T ∗ : V → V such that
(cid:104)T x  y(cid:105) = (cid:104)x  T ∗y(cid:105) for all x  y ∈ V. T ∗ is called the adjoint operator. So continuing Example 2:
Example 3. Suppose the functional L has the form L(f ) = T (f )(x0)  where x0 ∈ X and T : H →
H is a bounded linear operator on H. Then the representer of L is z = T ∗(k(x0 ·)) because
∀ x ∈ X  z(x) = L(k(x ·)) = T (k(x ·))(x0) = (cid:104)T (k(x ·))  k(x0 ·)(cid:105) = (cid:104)k(x ·)  T ∗(k(x0 ·))(cid:105). (1)
Riesz’s theorem will be useful for our framework since it allows us to compactly represent function-
als related to local invariances as elements of the RKHS.

3 Regularized Risk Minimization in RKHS with Invariances

To simplify the presentation  we ﬁrst describe our framework in the settings of semi-supervised
learning [SSL  4]  and later show how to extend it to other learning scenarios in a straightforward
way. In SSL  we wish to learn a target function f both from labeled data and from local invariances
extracted from labeled and unlabeled data. Let (x1  y1)  . . .   (xl  yl) be the labeled training data 
and (cid:96)1(f (x)  y) be the loss function on f when the training input x is labeled as y. In this paper we
restrict (cid:96)1 to be convex in its ﬁrst argument  e.g. logistic loss  hinge loss  and squared loss.
We measure deviations from local invariances around each labeled or unlabeled input instance  and
express them as bounded linear functionals Ll+1(f )  . . .   Ll+m(f ) on the RKHS H. The linear
functionals are associated with another convex loss function (cid:96)2(Li(f )) penalizing violations of the
local invariances. As an example  the derivative of f with respect to an input feature at some training
instance x is a linear functional in f  and the loss function penalizes large values of the derivative
at x using  e.g.  squared loss  absolute loss  and -insensitive loss. Section 4 describes other local
invariances we can consider and shows that these can be expressed as bounded linear functionals.
Finally  we penalize the complexity of f via the squared RKHS norm (cid:107)f(cid:107)2. Putting together the
loss and regularizer  we set out to minimize the regularized risk functional over f ∈ H:

min
f∈H

(cid:107)f(cid:107)2 + λ

1
2

(cid:96)1(f (xi)  yi) + ν

(cid:96)2 (Li(f ))  

(2)

i=1

i=l+1

where λ  ν > 0. By the convexity of (cid:96)1 and (cid:96)2  (2) must be a convex optimization problem. However
it is still in the function space and involves functionals. In order to derive an efﬁcient optimization
procedure  we now derive a representer theorem showing that the optimal solution lies in the span of
a ﬁnite number of functions associated with the labeled data and the representers of the functionals
Li. Similar results are available in [22].
Theorem 2. Let H be the RKHS deﬁned by the kernel k. Let Li (i = l + 1  . . .   l + m) be bounded
linear functionals on H with representers zi. Then the optimal solution to (2) must be in the form of

(3)
Furthermore  the parameters α = (α1  . . .   αl+m)(cid:48) (ﬁnite dimensional) can be found by minimizing

i=l+1

i=1

g(·) =

αik(xi ·) +

αizi(·)

(cid:96)1((cid:104)k(xi ·)  f(cid:105)   yi) + ν

αizi. (4)
λ
Here Kij =(cid:104)ˆki  ˆkj(cid:105)  where ˆki = k(xi ·) if i ≤ l  and ˆki = zi(·) otherwise. (Proof is in Appendix A.)

i=l+1

i=l+1

i=1

(cid:96)2 ((cid:104)zi  f(cid:105)) +

α(cid:48)Kα  where f =

αik(xi ·) +

l(cid:88)

l+m(cid:88)

l(cid:88)

i=1

l+m(cid:88)

l(cid:88)

l(cid:88)

l+m(cid:88)

l+m(cid:88)

1
2

3

Theorem 2 is similar to the results in [18  Proposition 3] and [19  Eq 8]  where the optimal function
lies in the span of a ﬁnite number of representers. However  our model is quite different in that it
uses the representers of the linear functionals corresponding to the invariance  rather than virtual
samples drawn from the invariant neighborhood. This could result in more compact models because
the invariance (e.g. rotation) is enforced by a single representer  rather than multiple virtual examples
(e.g. various degrees of rotation) drawn from the trajectory of invariant transforms. By the expansion
of f in (4)  the labeling of a new instance x depends not only on k(x  xi) that often measures the
similarity between x and training examples  but also takes into account the extent to which k(x ·) 
as a function  conforms to the prior invariances.
Computationally  (cid:104)k(xi ·)  zj(cid:105) is straightforward based on the deﬁnition of Lj. The efﬁcient com-
given the similarity measure wij between xi and xj  can be written as(cid:80)
putation of (cid:104)zi  zj(cid:105) depends on the speciﬁc kernels and invariances  as we will show in Section
(cid:80)
4. In the simplest case  consider the commonly used graph Laplacian regularizer [10  11] which 
ij wij(f (xi) − f (xj))2 =
ij wij(Lij(f ))2  where Lij(f ) = (cid:104)f  k(xi ·) − k(xj ·)(cid:105) is linear and bounded. Then (cid:104)zij  zpq(cid:105) =
k(xi  xp) + k(xj  xq) − k(xj  xp) − k(xi  xq). Another generic approach is to use the assumption
in Example 3 that (cid:104)zs  f(cid:105) = Ls(f ) = Ts(f )(xs)  ∀s ∈ {i  j}. Then
(cid:104)zi  zj(cid:105) = Li(zj) = Ti(zj)(xi) = (cid:104)Ti(zj)  k(xi ·)(cid:105) = (cid:104)zj  T ∗
i (k(xi ·)))(xj). (5)
In practice  classiﬁers such as the support vector machine often use an additional constant term (bias)
that is not penalized in the optimization. This is equivalent to searching f over F + H  where F is a
ﬁnite set of basis functions. A similar representer theorem can be established (see Appendix A).

i (k(xi ·))(cid:105) = Tj(T ∗

4 Local Invariances as Bounded Linear Functionals
Interestingly  many useful local invariances can be modeled as bounded linear functionals. If it can
be expressed in terms of function values f (x)  then it must be bounded as shown in Example 1.
In general  boundedness hinges on the functional and the RKHS H. When H is ﬁnite dimensional 
such as that induced by linear or polynomial kernels  all linear functionals on H must be bounded:
Theorem 3 ([29  Thm 2.7-8]). Linear functionals on ﬁnite dimensional normed space are bounded.
However  in most nonparametric statistics problems that are of interest  the RKHS is inﬁnite dimen-
sional. So the boundedness requires a more reﬁned analysis depending on the speciﬁc functional.

4.1 Differentiation Functional
In semi-supervised learning  a common prior is that the discriminant function f does not change
rapidly around sampled points. Therefore  we expect the norm of the gradient at these locations is
small. Suppose X ⊆ Rn is an open set  and k is continuously differentiable on X2. Then we are
∂xd |x=xi  where xd stands for the d-th component of
interested in linear functionals Lxi d(f ) := ∂f (x)
the vector x. Then Lxi d must be bounded:
Theorem 4. Lxi d is bounded on H with respect to the RKHS norm.
Proof. This result is immediate from the inequality given by [28  Corollary 4.36]:

(cid:12)(cid:12)(cid:12)x=y=xi

∂2

∂xd∂yd

(cid:17) 1

2

k(x  y)

.

(cid:4)

∂xd

(cid:12)(cid:12)(cid:12)(cid:12) ∂
(cid:12)(cid:12)(cid:12)(cid:12)y=xj

(cid:12)(cid:12)(cid:12)x=xi

f (x)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ (cid:107)f(cid:107)(cid:16)
(cid:12)(cid:12)(cid:12)(cid:12)y=xj

∂
∂yd(cid:48)

Let us denote the representer of Lxi d as zi d. Indeed  [28  Corollary 4.36] established the same
result for higher order partial derivatives  which can be easily used in our framework as well.
The inner product between representers can be computed by deﬁnition:
∂2
(cid:104)zi d  zj d(cid:48)(cid:105) =
(6)
∂yd(cid:48)
If k is considered as a function on (x  y) ∈ R2n  this implies that the inner product (cid:104)zi d  zj d(cid:48)(cid:105) is the
(xd  yd(cid:48)
)-th element of the Hessian of k evaluated at (xi  xj)  which could be interpreted as some
sort of “covariance” between the two invariances with respect to the touchstone function k.
Applying (6) to the polynomial kernel k(x  y) = ((cid:104)x  y(cid:105) + 1)r  we derive

(cid:12)(cid:12)(cid:12)(cid:12)x=xi y=xj

(cid:104)zi d  k(y ·)(cid:105) =

zi d(y) =

∂
∂yd(cid:48)

k(x  y).

∂xd

(cid:104)zi d  zj d(cid:48)(cid:105) = r((cid:104)xi  xj(cid:105) + 1)r−2[(r − 1)xd(cid:48)

j + ((cid:104)xi  xj(cid:105) + 1)δd=d(cid:48)] 

i xd

(7)

4

where δd=d(cid:48) = 1 if d = d(cid:48)  and 0 otherwise. For Gaussian kernels k(x  y) = κσ(x  y)  we can take
another path that is different from (6). Note that Lxi d(f ) = T (f )(xi) where T : f (cid:55)→ ∂f
∂xd   and it
is straightforward to verify that T is bounded with T ∗ = −T for Gaussian RKHS. So applying (5) 
(cid:104)zi d  zj d(cid:48)(cid:105) =
(8)

By Theorem 1  it immediately follows that the norm of Lxi d is(cid:112)(cid:104)zi d  zi d(cid:105) = 1/σ.

(cid:12)(cid:12)(cid:12)(cid:12)y=xj

[σ2δd=d(cid:48) − (xd

∂yd k(xi  y)

i − xd(cid:48)

i − xd

j )(xd(cid:48)

∂
∂yd(cid:48)

k(xi  xj)

− ∂

(cid:18)

(cid:19)

j )].

σ4

=

4.2 Transformation Invariance

(cid:18)x

(cid:19) t(3)

α(cid:55)−→

(cid:18)x + αx
(cid:19)

Invariance to known local transformations of input has been used successfully in supervised learning
[3]. Here we show that transformation invariance can be handled in our framework via representers
in RKHS. In particular  gradients with respect to the transformations are bounded linear functionals.
Following [3]  we ﬁrst require a differentiable function g that maps points from a space S to R 
where S lies in a Euclidean space.3 For example  an image can be considered as a function g that
maps points in the plane S = R2 to the intensity of the image at that point. Next  we consider a
family of bijective transformations tα : S (cid:55)→ S  which is differentiable in both the input and the
parameter α. For instance  translation  rotation  and scaling can be represented as mappings t(1)
α  
α   and t(3)
t(2)

α respectively:

(cid:18)x

(cid:19) t(1)

α(cid:55)−→

(cid:18)x + αx

(cid:19)

(cid:18)x

(cid:19) t(2)

α(cid:55)−→

(cid:18)x cos α − y sin α

x sin α − y cos α

(cid:19)

 

 

y

y

y

y + αy

y + αy

.
Based on tα  we deﬁne a family of operators Tα : RS → RS as Tα(g) = g ◦ t−1
α . The
function Tα(g)(x  y) gives us the intensity at location (x  y) of the image translated by an off-
set (αx  αy)  rotated by an angle α  or scaled by an amount α. Finally we sample from S a
ﬁxed number of locations S := {s1  . . .   sq}  and present to the learning algorithm a vector
I(g  α; S) := (Tα(g)(s1)(cid:48)  . . .   Tα(g)(sq)(cid:48))(cid:48). Digital images are discretization of real images where
we sample at ﬁxed pixel locations of the function g to obtain a ﬁxed sized vector. Clearly  for a ﬁxed
g  the sampled observation I(g  α; S) is a vector valued function of α.
The following result allows our framework to use derivatives with respect to the parameters in α.
Theorem 5. Let F be a normed vector space of functions that map from the range of I(g  α; S) to R.
Suppose the linear functional that maps f ∈ F to ∂f (u)
is bounded for any u0 and coordinate
j  and its norm is denoted as Cj. Then the functional Lg d S: f (cid:55)→ ∂
derivatives with respect to each of the components in α  must be bounded linear functionals on F.
Proof. Let I j(g  α; S) be the j-th component of I(g  α; S). Using the chain rule  for any f ∈ F

∂αd

∂uj

(cid:12)(cid:12)u=u0
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) q(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)α=0
(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:12) ∂I j(g  α; S)
(cid:12)(cid:12)(cid:12)(cid:12)α=0

∂αd

j=1

∂f (u)
∂uj

(cid:12)(cid:12)(cid:12)(cid:12)u=I(g 0;S)
(cid:12)(cid:12)(cid:12)(cid:12) = (cid:107)f(cid:107) ·
q(cid:88)

j=1

Cj

(cid:12)(cid:12)α=0f (I(g  α; S))  i.e.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)α=0
(cid:12)(cid:12)(cid:12)(cid:12)α=0
(cid:12)(cid:12)(cid:12)(cid:12) ∂I j(g  α; S)

· ∂I j(g  α; S)

(cid:12)(cid:12)(cid:12)(cid:12) .

∂αd

∂αd

The proof is completed by noting that the last summation is a ﬁnite constant independent of f. (cid:4)
Corollary 1. The derivatives ∂
∂αd
bounded linear functionals on the RKHS deﬁned by the polynomial and Gaussian kernels.

f (I(g  α; S)) with respect to each of the component in α are

To compute the inner product between representers  let zg d S be the representer of Lg d S and denote
vg d S = ∂I(g α;S)

|α=0. Then (cid:104)k(x ·)  zg d S(cid:105) =

and

∂αd

(cid:68) ∂
(cid:18)(cid:28)

∂y

(cid:12)(cid:12)y=I(g 0 S)k(x  y)  vg d S
(cid:12)(cid:12)(cid:12)y=I(g(cid:48) 0 S(cid:48))

∂
∂y

vg(cid:48) d(cid:48) S(cid:48) 

(cid:69)

(cid:29)(cid:19)(cid:29)

k(x  y)

.

(9)

(cid:28)

(cid:104)zg d S  zg(cid:48) d(cid:48) S(cid:48)(cid:105) =

vg d S 

(cid:12)(cid:12)(cid:12)α=0
(cid:12)(cid:12)(cid:12)x=I(g 0 S)

∂
∂x

|Lg d S(f )| =

(by deﬁnition of (cid:107)f(cid:107)) ≤ q(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12) ∂f (I(g  α; S))

∂αd

(Cj (cid:107)f(cid:107)) ·

j=1

3In practice  S can be a discrete domain such as the pixel coordinate of an image. Then g can be extended

to an interpolated continuous space via convolution with a Gaussian. See [3  § 2.3] for more details.

5

(cid:90)

X

Lxi(f ) =

f (τ )p(xi − τ )dτ − f (xi) 

4.3 Local Averaging
Using gradients to enforce the local invariance that the target function does not change much around
data instances increases the number of basis functions by a factor of n  where n is the number of
gradient directions that we use. The optimization problem can become computationally expensive if
n is large. When we do not have useful information about the invariant directions  it may be useful
to have methods that do not increase the number of basis functions by much. Consider functionals
(10)
where p(·) is a probability density function centered at zero. Minimizing a loss with such linear
functionals will favor functions whose local averages given by the integral are close to the function
values at data instances. If p(·) is selected to be a low pass ﬁlter  the function should be smoother
and less likely to change in regions with more data points but is less constrained to be smooth in
regions where the data points are sparse. Hence  such loss functions may be appropriate when we
believe that data instances from the same class are clustered together.
To use the framework we have developed  we need to select the probability density p(·) and the
kernel k such that Lxi(f ) is a bounded linear functional.
Theorem 6. Assume there is a constant C > 0 such that k(x  x) ≤ C 2 for all x ∈ X. Then the
linear functional Lxi in (10) is bounded in the RKHS deﬁned by k for any probability density p(·).
See proof in Appendix B. As a result  radial kernels such as Gaussian and exponential kernels make
Lxi bounded. k(x  x) is not bounded for polynomial kernel  but it has been covered by Theorem 3.
To allow for efﬁcient implementation  the inner product between representers of invariances must
be computed efﬁciently. Unlike differentiation  integration often does not result in a closed form
expression. Fortunately  analytic evaluation is feasible for the Gaussian kernel κσ(xi  xj) together
2π)−nκθ(x  0)  because the convolution of two
with the Gaussian density function p(x) = (θ
Gaussian densities is still a Gaussian density (see derivation in Appendix B):

(cid:104)zxi  zxj(cid:105) = κσ(xi  xj) + (1 + 2θ/σ)−nκσ+2θ(xi  xj) − 2(1 + θ/σ)−nκσ+θ(xi  xj).

√

5 Optimization
The objective (2) can be optimized by many algorithms  such as stochastic gradient [30]  bundle
method [19  31]  and (randomized) coordinate descent in its dual (4) [32]. Since all computational
strategies rely on kernel evaluation  we prefer the dual approach. In particular  (4) allows an uncon-
strained optimization and the nonsmooth regions in (cid:96)1 or (cid:96)2 can be easily approximated by smooth
surrogates [33]. So without loss of generality  we assume (cid:96)1 and (cid:96)2 are smooth. Our approach can
work in both batch and coordinate-wise  depending on the scale of different problems.
In the batch setting  the major challenge is the cost in computing the gradient when the number
of invariance is large. For example  consider all derivatives at all labeled examples x1  . . .   xl and
let N = ((cid:104)zi d(cid:48)  zj d(cid:48)(cid:105))(i d) (j d(cid:48)) ∈ Rnl×nl as in (8). Then given α = (α(cid:48)
l)(cid:48) ∈ Rnl  the
bottleneck of computation is g := N α  which costs O(l2n2) time and O(l2n2) space to store N in
a vanilla implementation. However  since the kernel matrices often employ rich structures  a careful
treatment can reduce the cost by an order of magnitude  e.g. into O(l2n) time and O(nl+l2) space in
this case. Speciﬁcally  denote K = (k(xi  xj))ij ∈ Rl×l  and three n×l matrices X = (x1  . . .   xl) 
A = (α1  . . .   αl)  and G = (gd i). Then one can show

lQ)(cid:3)   where Qij = Kij(Λji − Λii)  Λ = X(cid:48)A.

G = σ−4(cid:2)σ2AK + XQ − X ◦ (1n ⊗ 1(cid:48)

(11)
Here ◦ stands for Hadamard product  and ⊗ is Kronecker product. 1n ∈ Rn is a vector of straight
one. The computational cost is dominated by X(cid:48)A  AK  and XQ  which are all O(l2n).
When the number of invariance is huge  a batch solver can be slow and coordinate-wise updates can
be more efﬁcient. In each iteration  it picks a coordinate in α and optimizes the objective over all the
coordinates picked so far  leaving the rest elements to zero. [32] selected the coordinate randomly 
while another strategy is to choose the steepest descent coordinate. Clearly  it is useful only when
this selection can be performed efﬁciently  which depends heavily on the structure of the problem.

1  . . .   α(cid:48)

6 Experimental Result
We compared our approach which is henceforth referred to as InvSVM with state-of-the-art methods

6

Figure 1: Decision boundary for the two moon dataset with ν = 0  0.01  0.1  and 1 (left to right).

Invariance to Differentiation: Transduction on Two Moon Dataset

Invariance to Differentiation: Semi-supervised Learning on Real-world Data.

in (semi-)supervised learning using invariance to differentiation and transformation.
6.1
As a proof of concept  we experimented with the “two moon” dataset shown in Figure 1  with
only l = 2 labeled data instances (red circle and black square). We used the Gaussian kernel with
σ = 0.25 and the gradient invariances on both labeled and unlabeled data. The loss (cid:96)1 and (cid:96)2 are
logistic and squared loss respectively  with λ = 1  and ν ∈ {0  0.01  0.1  1}. (4) was minimized
by a L-BFGS solver [34]. In Figure 1  from left to right our method lays more and more emphasis
on placing the separation boundary in low density regions  which allows unlabeled data to improve
the classiﬁcation accuracy. We also tried hinge loss for (cid:96)1 and -insensitive loss for (cid:96)2 with similar
classiﬁcation results.
6.2
Datasets. We used 9 datasets for binary classiﬁcation from [4] and the UCI repository [35]. The
number of features (n) and instances (t) are given in Table 1. All feature vectors were normalized
to zero mean and unit length.
Algorithms. We trained InvSVM with hinge loss for (cid:96)1 and squared loss for (cid:96)2. The differentiation
invariance was used over the whole dataset  i.e. m = nt in (4)  and the gradient computation was
accelerated by (11). We compared InvSVM with the standard SVM  and a state-of-the-art semi-
supervised learning algorithm LapSVM [36]  which uses manifold regularization based on graph
Laplacian [11  37]. All three algorithms used Gaussian kernel with the bandwidth σ set to be the
median of pairwise distance among all instances.
Settings. We trained SVM  LapSVM  and InvSVM on a subset of l ∈ {30  60  90} labeled ex-
amples  and compared their test error on the other t − l examples in each dataset. We used 5 fold
stratiﬁed cross validation (CV) to select the values of λ and ν for InvSVM. CV was also applied
to LapSVM for choosing the number of nearest neighbor for graph construction  weight on the
Laplacian regularizer  and the standard RKHS norm regularizer. For each fold of CV  InvSVM and
LapSVM used the other 4 folds ( 4
5 l points as unla-
beled data. The error on the 1
5 l points was then used for CV. Finally the random selection of l labeled
examples was repeated for 10 times  and we reported the mean test error and standard deviation.
Results.
It is clear from Table 1 that in most cases InvSVM achieves lower or similar test er-
ror compared to SVM and LapSVM. Both LapSVM and InvSVM are implementations of the low
density prior. To this end  LapSVM enforces smoothness of the discrimination function over neigh-
boring instances  while InvSVM directly penalizes the gradient at instances and does not require a
notion of neighborhood. The lower error of InvSVM suggests the superiority of the use of gradient 
which is enabled by our representer based approach. Besides  SVM often performs quite well when
the number of labeled data is large  with similar error as LapSVM. But still  InvSVM can attain
even lower error.
6.3 Transformation Invariance
Next we study the use of transformation invariance for supervised learning. We used the handwritten
digits from the MNIST dataset [38] and compared InvSVM with the virtual sample SVM (VirSVM)
which constructs additional instances by applying the following transformations to the training data:
2-pixel shifts in 4 directions  rotations by ±10 degrees  scaling by ±0.1 unit  and shearing in vertical
∂α|α=0I(g  α; S) was approximated
or horizontal axis by ±0.1 unit. For InvSVM  the derivative ∂
with the difference that results from the above transformation. We considered binary classiﬁcation
problems by choosing four pairs of digits (4-vs-9  2-vs-3  and 6-vs-5 are hard  while 7-vs-1 is

5 l points) as labeled data  and the remaining t − 4

7

−1−0.500.511.522.5−0.6−0.4−0.200.20.40.60.811.2−1−0.500.511.522.5−0.6−0.4−0.200.20.40.60.811.2−1−0.500.511.522.5−0.6−0.4−0.200.20.40.60.811.2−1−0.500.511.522.5−0.6−0.4−0.200.20.40.60.811.2Table 1: Test error of SVM  LapSVM  and InvSVM for semi-supervised learning. The best result
(including tie) that is statistically signiﬁcant in each setting is highlighted in bold. No number is
highlighted if there is no signiﬁcant difference between the three methods.

Method

heart (n = 13  t = 270)

BCI (n = 117  t = 400)

bupa (n = 6  t = 245)

l = 30

l = 60

l = 60

l = 90

l = 30

l = 90

l = 30
l = 90
23.5±2.08 21.4±0.23 20.0±1.11 44.6±1.89 39.0±3.41 34.6±3.25 36.2±5.53 37.9±5.80 36.9±1.80
SVM
LapSVM 23.2±1.68 22.7±1.92 20.7±2.10 44.8±2.72 45.4±2.25 36.1±3.92 36.6±5.98 40.7±4.05 37.1±1.58
InvSVM 22.3±1.27 20.2±1.01 19.6±2.79 45.3±3.59 38.4±4.41 32.7±2.27 38.2±4.46 35.4±1.85 35.2±0.45
Australian (n = 14  t = 690)
l = 30
l = 90
l = 30
32.9±1.59 27.1±0.92 24.8±1.99 34.6±2.52 28.3±2.65 25.6±1.48 20.6±4.18 21.7±11.3 15.3±0.86
SVM
LapSVM 37.1±1.25 28.4±2.44 29.7±1.57 37.7±5.76 29.9±2.41 25.9±1.49 21.9±9.27 15.6±2.49 14.7±0.16
InvSVM 33.1±0.49 26.4±1.14 23.4±1.36 35.4±1.57 28.4±3.03 22.3±4.69 17.7±1.74 16.4±1.11 15.6±0.77

g241n (n = 241  t = 1500)
l = 90

g241c (n = 241  t = 1500)
l = 90

l = 60

l = 60

l = 60

l = 60

l = 30

ionosphere (n = 34  t = 351)
USPS (n = 241  t = 1500)
l = 30
l = 90
l = 90
12.5±3.12 8.71±1.62 7.17±1.67 30.9±2.53 22.7±0.39 20.6±4.81 14.9±0.26 12.6±2.04 11.3±2.06
SVM
LapSVM 14.9±1.73 9.05±1.05 7.66±1.53 29.4±2.33 22.9±0.17 24.9±1.29 15.3±1.10 12.3±1.74 11.1±1.81
InvSVM 7.58±1.29 7.90±0.23 7.02±0.88 31.6±4.68 24.1±2.06 21.8±3.91 15.4±4.02 12.2±2.93 11.3±1.63

sonar (n = 60  t = 208)

l = 90

l = 30

l = 60

l = 60

l = 60

l = 30

(a) 4 (pos) vs 9 (neg)

(b) 2 (pos) vs 3 (neg)

(c) 6 (pos) vs 5 (neg)

(d) 7 (pos) vs 1 (neg)

Figure 2: Test error (in percentage) of InvSVM versus SVM with virtual sample.

(cid:80)

i:yi=1 (cid:96)1(f (xi)  1) + n−1− (cid:80)

easier). As the real distribution of digit is imbalanced and invariance is more useful when the
number of labeled data is low  we randomly chose n+ = 50 labeled images for one class and
n− = 10 images for the other. Accordingly the supervised loss was normalized within each class:
i:yi=−1 (cid:96)1(f (xi) −1). Logistic loss and -insensitive loss were
n−1
+
used for (cid:96)1 and (cid:96)2 respectively. All parameters were set by 5 fold CV and the test error was measured
on the rest images in the dataset. The whole process was repeated for 20 times.
In Figure 2  InvSVM generally yields lower error than VirSVM  which suggests that compared
with drawing virtural samples from the invariance  it seems more effective to directly enforce a ﬂat
gradient like in [3].

7 Conclusion and Discussion
We have shown how to model local invariances by using representers in RKHS. This subsumes a
wide range of invariances that are useful in practice  and the formulation can be optimized efﬁciently.
For future work  it will be interesting to extend the framework to a broader range of learning tasks.
A potential application is to the problem of imbalanced data learning  where one wishes to keep
the decision boundary further away from instances of the minority class and closer to the instances
of majority class. It will also be interesting to generalize the framework to invariances that are not
a |f(cid:48)(x)| dx (a  b ∈ R) is not linear  but
we can treat the integral of absolute value as a loss function and take the derivative as the linear
a |Lx(f )| dx. As a result  the optimization may have to
resort to sparse approximation methods. Finally the norm of the linear functional does affect the
optimization efﬁciency  and a detailed analysis will be useful for choosing the linear functionals.

directly linear. For example  the total variation (cid:96)(f ) := (cid:82) b
functional: Lx(f ) = f(cid:48)(x) and (cid:96)(f ) = (cid:82) b

8

5101551015Error of InvSVMError of VirSVM46845678Error of InvSVMError of VirSVM51046810Error of InvSVMError of VirSVM0.511.520.511.52Error of InvSVMError of VirSVMReferences
[1] G. E. Hinton. Learning translation invariant recognition in massively parallel networks. In Proceedings

Conference on Parallel Architectures and Laguages Europe  pages 1–13. Springer  1987.

[2] M. Ferraro and T. M. Caelli. Lie transformation groups  integral transforms  and invariant pattern recog-

nition. Spatial Vision  8:33–44  1994.

[3] P. Simard  Y. LeCun  J. S. Denker  and B. Victorri. Transformation invariance in pattern recognition-
tangent distance and tangent propagation. In Neural Networks: Tricks of the Trade  pages 239–274  1996.

[4] O. Chapelle  B. Sch¨olkopf  and A. Zien  editors. Semi-Supervised Learning. MIT Press  2006.
[5] S. Ben-David  T. Lu  and D. Pal. Does unlabeled data provably help? Worst-case analysis of the sample

complexity of semi-supervised learning. In COLT  2008.

[6] T. Zhang and F. J. Oles. A probability analysis on the value of unlabeled data for classiﬁcation problems.

In ICML  2000.

[7] O. Bousquet  O. Chapelle  and M. Hein. Measure based regularization. In NIPS  2003.
[8] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML  1999.
[9] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML  2001.
[10] M. Belkin  P. Niyogi  and V. Sindhwani. On manifold regularization. In AI-Stats  2005.
[11] X. Zhu  Z. Ghahramani  and J. D. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In ICML  2003.

[12] D. Zhou and B. Sch¨olkopf. Discrete regularization. In Semi-Supervised Learning  pages 221–232. MIT

Press  2006.

[13] H. Xu  C. Caramanis  and S. Mannor. Robustness and regularization of support vector machines. Journal

of Machine Learning Research  10:3589–3646  2009.

[14] A. Ben-Tal  L. El Ghaoui  and A. Nemirovski. Robust Optimization. Princeton University Press  2008.
[15] C. Bhattacharyya  K. S. Pannagadatta  and A. J. Smola. A second order cone programming formulation

for classifying missing data. In NIPS  2005.

[16] A. Globerson and S. Roweis. Nightmare at test time: Robust learning by feature deletion. In ICML  2006.
[17] N. Dalvi  P. Domingos  Mausam  S. Sanghai  and D. Verma. Adversarial classiﬁcation. In KDD  2004.
[18] T. Graepel and R. Herbrich. Invariant pattern recognition by semideﬁnite programming machines. In

NIPS  2004.

[19] C. H. Teo  A. Globerson  S. Roweis  and A. Smola. Convex learning with invariances. In NIPS  2007.
[20] D. DeCoste and B. Sch¨olkopf. Training invariant support vector machines. Machine Learning  46:161–

190  2002.

[21] O. Chapelle and B. Sch¨olkopf. Incorporating invariances in nonlinear support vector machines. In NIPS 

2001.

[22] G. Wahba. An introduction to model building with reproducing kernel Hilbert spaces. Technical Report

TR 1020  University of Wisconsin-Madison  2000.

[23] A. J. Smola and B. Sch¨olkopf. On a kernel-based method for pattern recognition  regression  approxima-

tion and operator inversion. Algorithmica  22:211–231  1998.

[24] C. Walder and O. Chapelle. Learning with transformation invariant kernels. In NIPS  2007.
[25] C. Burges. Geometry and invariance in kernel based methods. In B. Sch¨olkopf  C. Burges  and A. Smola 

editors  Advances in Kernel Methods — Support Vector Learning  pages 89–116. MIT Press  1999.

[26] B. Sch¨olkopf and A. Smola. Learning with Kernels. MIT Press  2001.
[27] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernel-based

Learning Methods. Cambridge University Press  Cambridge  UK  2000.

[28] I. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics. Springer 

2008.

[29] E. Kreyszig. Introductory Functional Analysis with Applications. Wiley  1989.
[30] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In

ICML  2007.

[31] C. H. Teo  S. V. N. Vishwanthan  A. J. Smola  and Q. V. Le. Bundle methods for regularized risk mini-

mization. Journal of Machine Learning Research  11:311–365  January 2010.

[32] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM

Journal on Optimization  22(2):341–362  2012.

[33] O. Chapelle. Training a support vector machine in the primal. Neural Comput.  19(5):1155–1178  2007.
[34] http://www.cs.ubc.ca/∼pcarbo/lbfgsb-for-matlab.html.
[35] K. Bache and M. Lichman. UCI machine learning repository  2013. University of California  Irvine.
[36] http://www.dii.unisi.it/∼melacci/lapsvmp.
[37] V. Sindhwani  P. Niyogi  and M. Belkin. Beyond the point cloud: from transductive to semi-supervised
[38] http://www.cs.nyu.edu/∼roweis/data.html.

learning. In ICML  2005.

9

,Xinhua Zhang
Wee Sun Lee
Yee Whye Teh
Jinyan Liu
Zhiyi Huang
Xiangning Wang