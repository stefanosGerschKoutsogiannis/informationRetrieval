2019,Space and Time Efficient Kernel Density Estimation in High Dimensions,Recently  Charikar and Siminelakis (2017) presented a framework for kernel density estimation in provably sublinear query time  for kernels that possess a certain hashing-based property. However  their data structure requires a significantly increased super-linear storage space  as well as super-linear preprocessing time. These limitations inhibit the practical applicability of their approach on large datasets.

In this work  we present an improvement to their framework that retains the same query time  while requiring only linear space and linear preprocessing time. We instantiate our framework with the Laplacian and Exponential kernels  two popular kernels which possess the aforementioned property. Our experiments on various datasets verify that our approach attains accuracy and query time similar to Charikar and Siminelakis (2017)  with significantly improved space and preprocessing time.,Space and Time Efﬁcient Kernel Density Estimation

in High Dimensions

Arturs Backurs∗

TTIC

backurs@ttic.edu

Piotr Indyk

MIT

indyk@mit.edu

Tal Wagner

MIT

talw@mit.edu

Abstract

Recently  Charikar and Siminelakis (2017) presented a framework for kernel density
estimation in provably sublinear query time  for kernels that possess a certain
hashing-based property. However  their data structure requires a signiﬁcantly
increased super-linear storage space  as well as super-linear preprocessing time.
These limitations inhibit the practical applicability of their approach on large
datasets.
In this work  we present an improvement to their framework that retains the same
query time  while requiring only linear space and linear preprocessing time. We
instantiate our framework with the Laplacian and Exponential kernels  two popular
kernels which possess the aforementioned property. Our experiments on various
datasets verify that our approach attains accuracy and query time similar to Charikar
and Siminelakis (2017)  with signiﬁcantly improved space and preprocessing time.

1

Introduction

Kernel density estimation is a fundamental problem with many applications in statistics  machine
learning and scientiﬁc computing. For a kernel function k : Rd × Rd → [0  1]  and a set of points
X ⊂ Rd  the kernel density function of X at a point y ∈ Rd is deﬁned as:2

KDEX (y) =

1
|X|

k(x  y).

(cid:88)

x∈X

Typically the density function is evaluated on a multiple queries y from an input set Y . Unfortunately 
a naïve exact algorithm for this problem runs in a rectangular O(|X||Y |) time  which makes it
inefﬁcient for large datasets X and Y . Because of this  most of the practical algorithms for this
problem report approximate answers. Tree-based techniques [GS91  GM01  GB17] lead to highly
efﬁcient approximate algorithms in low-dimensional spaces  but their running times are exponential
in d. In high-dimensional spaces  until recently  the best approximation/runtime tradeoff was provided
by simple uniform random sampling. Speciﬁcally  for parameters τ   ∈ (0  1)  it can be seen that
constant probability3 as long as KDEX (y) ≥ τ.
This approximation/runtime tradeoff was recently improved in [CS17]  who proposed a framework
based on Hashing-Based Estimators (HBE). The framework utilizes locality-sensitive hash (LSH)

(cid:1) points from X  then KDEX(cid:48)(y) = (1 ± ) KDEX (y) with

if X(cid:48) is a random sample of O(cid:0) 1

1
2

τ

multiplied by a positive weight wx ≥ 0  see e.g.  [CS17].

∗Authors ordered alphabetically.
2We note that all algorithms discussed in this paper easily extend to the case where each term k(x  y) is
3The probability of correct estimation can be reduced to 1 − δ for any δ > 0 at the cost of increasing the
sample size by a factor of log(1/δ). Since the same observation applies to all algorithms considered in this
paper  we will ignore the dependence on δ from now on.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Comparison of runtime and space bounds. Notation: τ ∈ (0  1) denotes a lower bound for
KDE values; d denotes the dimension;  ∈ (0  1) denotes the approximation error.4

Query Time

Algorithm
Random Sampling O(d/τ · 1/2)
HBE
This paper

√
O(d/
√
O(d/

# Stored hashes
O(1/τ · 1/2)

τ · 1/2) O(1/τ 3/2 · 1/4)
τ · 1/2) O(1/τ · 1/2)

1
2

τ

(cid:17)

(cid:16) 1√

functions [IM98]  i.e.  randomly selected functions h : Rd → U with the property that for any
x  y ∈ Rd  the collision probability Prh[h(x) = h(y)] is “roughly” related to the kernel value
. A recent empirical evaluation of
k(x  y). HBE reduces the evaluation time to (about) O
this algorithm [SRB+19] showed that it is competitive with other state of the art methods  while
providing signiﬁcant (up to one order of magnitude) runtime reduction in many scenarios.
One drawback of HBE approach  however  is its space usage  which is super-linear in the dataset
size. Speciﬁcally  the algorithm constructs O
hash tables  and stores the hash of each data
point in each table. Consequently  the additional storage required for the hashes is proportional
to the number of tables times the number of data points. As mentioned above  we can uniformly

subsample the dataset down to O(cid:0) 1
also effects the preprocessing time of the HBE data structure  which requires O(cid:0) 1

(cid:1) points  leading to an overall space usage of O(cid:0) 1

times that of the simple random sampling approach. The increase in storage

(cid:1) 
(cid:1) hashes

(cid:16) 1√

(cid:16) 1√

which is O

computations due to having to store every point in every table. As τ and  can be very close to zero
in practice  these drawbacks may pose a substantial bottleneck in dealing with large datasets.

(cid:17)

(cid:17)

1
2

τ

1
2

τ

1
2

τ

τ 3/2

1
4

τ 3/2

1
4

Our results.
In this paper we show that the super-linear amount of storage is in fact not needed
to achieve the runtime bound guaranteed by the HBE algorithm. Speciﬁcally  we modify the HBE
algorithm in a subtle but crucial way  and show that this modiﬁcation reduces the storage to (roughly)

(cid:1)  i.e.  the same as simple random sampling. Table 1 summarizes the performance of the

O(cid:0) 1

respective algorithms. Our main result is the following theorem.
Theorem 1. Let k(x  y) be a kernel function  for which there exists a distribution H of hash functions
and M ≥ 1 such that for every x  y ∈ Rd 

1
2

τ

M−1 · k(x  y)1/2 ≤ Pr
h∼H

[h(x) = h(y)] ≤ M · k(x  y)1/2.

(1)

There exists a data structure for Kernel Density Estimation with the following properties:

(cid:16) 1
τ · TH M 3

2

(cid:17)

• Given a dataset X ⊂ Rd and parameters τ   ∈ (0  1)  we preprocess it in O

to store a point x ∈ X  and SH is the space needed to store a hash value h(x).

• The space usage of the data structure is O

(cid:16) 1
time  where TH is the time to compute a hash value h(x).
τ · (SX +SH )M 3
(cid:16) 1√

2

• Given a query point y such that KDEX (y) ≥ τ  we can return with constant probability a
(1 ± )-approximation of KDEX (y) in O
time  where Tk is the time to
compute a kernel value k(x  y).

τ · (Tk+TH )M 3

2

  where SX is the space needed

(cid:17)

(cid:17)

We empirically evaluate our approach on the Laplacian kernel k(x  y) = e−(cid:107)x−y(cid:107)1/σ and the expo-
nential kernel k(x  y) = e−(cid:107)x−y(cid:107)2/σ. Both are commonly used kernels  and ﬁt into the framework as

4For simplicity  the bounds in the table assume that the kernel takes O(d) time to compute  and that a hash
value takes O(d) time to compute. The kernels we consider have these properties (for bandwidth σ = Ω(1)).
See Theorem 1 for the full parameter dependence.

2

they satisfy the requirements of Theorem 1 with M = O(1)  Tk = O(d)  TH = O(min{d  d/σ})
and SH = O(min{d log(1/σ)  d/σ})  with high probability (over h ∼ H). Our experiments conﬁrm
the analytic bounds and show that our approach attains a similar query time to approximation tradeoff
as HBE  while using signiﬁcantly less space and preprocessing time.

(cid:16)(cid:112)1/τ · 1/2(cid:17)

Our techniques. Our algorithm builds on the HBE approach of [CS17]. Recall that the algorithm
selects L = Θ
LSH functions h1 . . . hL  and creates L hash tables  such that for
each i = 1 . . . L  each point x ∈ X is placed in the jth table in bin hj(x). To estimate KDEX (y) 
the algorithm selects one point from each bin h1(y) . . . hL(y)  and uses those points for estimation.
To achieve the performance as in Table 1  the algorithm is applied to a random sample of size
s = O(1/τ · 1/2). The total space is therefore bounded by O(sL) = O(1/τ 3/2 · 1/4).
A natural approach to improving the space bound would be to run HBE on a smaller sample.
Unfortunately  it is easy to observe that any algorithm must use at least Ω(1/τ · 1/2) samples
to guarantee (1 ± )-approximation. Therefore  instead of sub-sampling the whole input to the
HBE algorithm  we sub-sample the content of each hash table independently for each hash function
hj  i = 1 . . . L. Speciﬁcally  for each hash function hj  we include a point x ∈ X in the jth hash table
√
τ L). If we
with probability 1/(s
start from a sample of size s = Θ(1/τ · 1/2)  then
τ L = O(s)  yielding the desired space bound;
at the same time  each point is included in at least one hash table with constant probability  which
means that at least Ω(1/τ · 1/2) points will be included in the union of the hash tables with high
probability. Perhaps surprisingly  we show that this increases the variance of the overall estimator by
only a constant factor.
For an intuition of why subsampling by a factor
τ does not distort the kernel values by much 
consider a simple setting where  is a constant  n = 1/τ  and there is only one data point x that is
very close to the query y (contributing ≈ 1) while all other points are far from y (contributing ≈ 0).
In this case  the original HBE algorithm would collect the point x from every bin h1(y) . . . hL(y) 
τ  then x is expected to survive in one
table  and thus our algorithm is still likely to identify one such bin in expectation. Conditioned on
this event  the estimate of the algorithm is approximately correct. See more details in Section 3.

where L =(cid:112)1/τ. In contrast  if we subsample by a factor

τ ). This reduces the expected number of stored hashes to O(

√

√

√

√

1.1 Related work

There is a vast amount of work on fast kernel density estimation in low dimensions  including the
seminal Fast Gauss Transform [GS91] and other tree-based methods [GM01  GB17]. However  as
mentioned above  they entail an exponential dependence on the input dimension. The tree-based
ASKIT algorithm [MXB15] avoids this dependence and is suitable for the high-dimensional regime.
However  it lacks rigorous guarantees on the approximation quality. The empirical evaluation
in [SRB+19] showed that HBE is consistently competitive with ASKIT  and in some settings
outperforms it by an order of magnitude.
Another important line of research has focused on sparsifying (reducing the size) of the input pointset
while preserving kernel density function values. This can be accomplished by constructing core-
sets [Phi13  ZJPL13  PT18] or related approaches [CWS12  SRB+19]. Although effective in low
dimensions  in high dimensions such approaches require Ω(1/2) samples (for an additive error
of  > 0 [PT18])  which is the same as the simple random sampling approach.5 We note that the
sparsiﬁcation approach can be combined with our improvement  as we can run our algorithm on a
core-set instead of the original data set  and retain the core-set size while speeding up the query time.
In addition to the aforementioned works of [CS17  SRB+19]  LSH-based estimators have been
applied in [CXS18  LS18b  WCN18  LS18a] to a variety of machine learning tasks.

2 Preliminaries
Kernel Density Estimation. Consider a kernel map k : Rd × Rd → [0  1]. The kernel density
estimation problem can be formally stated as follows.

5However  core-sets preserve all KDE values with high probability  while simple random sampling only

preserves the KDE of any individual query with high probability.

3

Deﬁnition 2. Let X = {x1  . . .   xn} ⊂ Rd be an input dataset  and   τ ∈ (0  1) input parameters.
Our goal is to construct a data structure such that for every query point y ∈ Rd that satisﬁes
KDEX (y) ≥ τ  we can return an estimate (cid:94)KDE(y) such that with constant probability 

(1 − )KDEX (y) ≤ (cid:94)KDE(y) ≤ (1 + )KDEX (y).

An exact computation of KDEX (y) performs n kernel evaluations. By standard concentration
inequalities  the above approximation can be achieved by evaluating the kernel y with only O( 1
1
2 )
τ
points chosen uniformly at random from X  and returning the average. As a result  we can assume
without loss of generality (and up to scaling  by a constant) that n = O( 1
τ

2 ).

1

LSHable kernels. Locality-Sensitive Hashing (LSH) is a widely used framework for hashing
metric datasets in a way that relates the collision probability of each pair of points to their geometric
similarity. Kernel maps for which such hashing families exist are called “LSHable” [CK15]. The
precise variant we will need is deﬁned as follows.
Deﬁnition 3. The kernel k is called ( 1
h : Rd → {0  1}∗  such that for every x  y ∈ Rd  Equation (1) holds.6
Laplacian and Exponential kernels. The Laplacian kernel is k(x  y) = e−(cid:107)x−y(cid:107)1/σ  where σ > 0
is the bandwidth parameter. The exponential kernel is deﬁned similarly as k(x  y) = e−(cid:107)x−y(cid:107)2/σ
(the difference is in use of the (cid:96)2-norm instead of the (cid:96)1-norm). For our purposes the two are
essentially equivalent  as they give the same analytic and empirical results. We will mostly focus on
the Laplacian kernel  since as we will see  it is ( 1
2   1)-LSHable. As a corollary  a random rotation of
2   O(1))-LSHable.
the dataset [DIIM04  CS17] can be used to show that the Exponential kernel is ( 1

2   M )-LSHable if there exists a family H of hash functions

3 The Data Structure

1

1

1

2 )  these become O(TH ·

τ 2 )  and the space usage (in addition to the dataset) is O(SH ·
n√

We begin by recalling the HBE-based KDE data structure of [CS17]. For simplicity consider the
1√
case M = 1. During preprocessing  they sample L = O(
τ 2 ) hash functions h1  . . .   hL from the
LSH family H  and store hj(xi) for every i = 1  . . .   n and j = 1  . . .   L. The preprocessing time
is O(TH ·
n√
τ 2 )  where TH is the
time needed to evaluate the hash value of a point  and SH is the space needed to store it. Recalling
τ 1.54 ) and O(SH ·
we have assumed that n = O( 1
τ 1.54 ) respectively.
τ
Given a query point y  let bj(y) := {xi : hj(xi) = hj(y)} be the set (“bin”) of data points whose
hj-hash is the same as that of y. The estimator picks a uniformly random data point x from bj(y)
and computes Zj = 1
(cid:94)KDE(y) = 1
evaluate k on a single pair.
Our data structure is similar  except that for every hj  we store the hash of every data point only with
probability δ = 1/(n
τ ). Therefore  on average we only compute and store a constant number of
hashes of each data point  yielding expected preprocessing time of O(TH /(τ · 2)) and space usage
of O(SH /(τ · 2)). The exact algorithm is given in Algorithm 1. Theorem 1  whose proof appears in
the appendix  shows this still returns a sufﬁciently good estimate of KDEX (y).

n|bj(y)| ·(cid:112)k(x  y). If bj(y) is empty  then Zj = 0. The ﬁnal KDE estimate is

√
j=1 Zj. The query time is O((TH + Tk)/(

τ 2))  where Tk is the time it takes to

(cid:80)L

√

L

Example. Let us give an illustration of the different approaches on the setting mentioned in the
introduction. Suppose  = Θ(1) and n ≈ 1/τ. Consider a setting in which the query point is very
close to a unique data point and very far from the rest of the data points. Concretely  k(x1  y) ≈ 1 
while k(xi  y) ≈ 0 for every i > 1. The KDE value is KDEX (y) ≈ τ. Naïve random sampling
would have to sample Ω(1/τ ) points in order to pick up x1 and return a correct estimate.

6The HBE framework of [CS17] accommodates (β  M )-LSHable kernels  that satisfy M−1 · k(x  y)β ≤
2   1)  and lower β is better. Since the

Prh∼H [h(x) = h(y)] ≤ M · k(x  y)β  where β can take any value in [ 1
kernels we consider attain the optimal setting β = 1

2   we ﬁx this value throughout.

7This can be implemented in expected time O(L) by sampling ˜L ∼ Binomial(n  L

n )  and then sampling a

uniformly random subset of size ˜L.

4

Algorithm 1 : Space-Efﬁcient HBE
Proprocessing:

Input: Dataset X ⊂ Rd of n points; kernel k(· ·); LSH family H; integer 1 ≤ L ≤ n.
For j = 1  . . .   L:

Sample a random hash function hj from H.
Let Xj ⊂ X be a random subset that includes each point with independent probability L
n .7
For every x ∈ Xj  evaluate and store hj(x).

Query:

Input: Query point y ∈ Rd.
For j = 1  . . .   L:

Sample a uniformly random point x(j) from bj(y) = {x ∈ X(cid:48)
Let Zj ← k(x(j) y)·|bj (y)|

j : hj(x) = hj(y)}.

(cid:80)L

L·Prh∼H [h(x(j))=h(y)].
j=1 Zj.

Return 1
L

(cid:80)L

j=1

n|bj(y)|(cid:112)k(x1  y) ≈ τ. However  note that

1

In the HBE algorithm of [CS17]  essentially in all hash tables x1 would be the unique data point in
the same bin as y  leading to a correct estimate 1
L
all terms in the sum are equal (to τ)  which seems to be somewhat wasteful. Indeed  it would sufﬁce
to pick up x1 in just one hash table instead of all of them.
In our method  x1 would be stored in δL ≈ 1 hash tables in expectation  say only in h1  and
in that table it would be the unique data point in b1(y).
In the other tables (j > 1) bj(y)
would be empty  which means the estimator evaluates to zero. The resulting KDE estimate is
1
L
just once instead of L times.

(cid:17) ≈ τ  which is still correct  while we have stored a hash of x1

(cid:16) 1
nδ|bj(y)|(cid:112)k(x1  y) +(cid:80)L

j=2 0

3.1 LSH for the Laplacian Kernel
The Laplacian kernel k(x  y) = e−(cid:107)x−y(cid:107)1/σ is a popular kernel  which ﬁts naturally into the above
framework since it is ( 1
2   1)-LSHable. For simplicity  let us assume w.l.o.g. that in the dataset we
need to hash  all point coordinates are in [0  1]. This does not limit the generality since the Laplacian
kernel is shift-invariant  and the coordinates can be scaled by inversely scaling σ.
The LSHablility of the Laplacian kernel follows from the Random Binning Features construction of
Rahimi and Recht [RR07] (see details in the appendix). The expected hash size is O(d log(1/σ)) 
and the hash evaluation time is O(d). We also give a variant (described below) with better hash size
and evaluation time for σ ≥ 1. Together  the following lemma holds.
Lemma 4. There is an LSH family Hσ such that for every x  y ∈ Rd  Prh∼Hσ [h(x) = h(y)] =
e−(cid:107)x−y(cid:107)1/(2σ). The expected hash size is SHσ = O(min{d log(1/σ)  d/σ}) bits. The expected
hashing time is THσ = O(min{d  d/σ}).
The hashing family for the case σ ≥ 1 is given as follows. Sample ρ ∼ Poisson(d/(2σ)). Then
sample ξ1  . . .   ξρ ∈ {1  . . .   d} independently and uniformly at random  and ζ1  . . .   ζρ ∈ [0  1]
independently and uniformly at random. These random choices determine the hash function h. Next
we describe h. Given a point x to hash  for every i = 1  . . .   ρ set bi = 1 if xξi > ζξi and bi = 0
otherwise. The hash h(x) is the concatenation of b1  . . .   bρ. It is not hard to verify (see appendix)
that Prh[h(x) = h(y)] = e−(cid:107)x−y(cid:107)1/(2σ).
Using the LSH family from Lemma 4 in Theorem 1 yields the following concrete data structure.
Corollary 5 (Data structure for Laplacian KDE). For the Laplacian kernel  there is a data structure
for the KDE problem with expected space overhead O(min{d log(1/σ)  d/σ}/(τ 2))  expected
preprocessing time O(min{d  d/σ}/(τ 2))  and query time O(d/(

τ 2)).

√

5

Table 2: Properties of the datasets used in our experiments.

Name
Covertype
Census
GloVe
MNIST

Description
forest cover type
U.S. census
word embeddings
hand-written digits

Number of points Dimension
581  012
2  458  285
1  183  514
60  000

55
68
100
784

4 Empirical Evaluation

We empirically evaluate our data structure for the Laplacian kernel.8 For brevity  we will refer to
the random sampling method as RS. The experimental results presented in this section are for the
the Laplacian kernel k(x  y) = e−(cid:107)x−y(cid:107)1/σ. The results for the Exponential kernel are qualitatively
similar are included in the appendix.
Choice of datasets. While the worst-case analysis shows that the HBE approach has asymptotically
better query time than RS  it is important to note that RS can still attain superior performance in
some practical settings. Indeed  the recent paper [SRB+19] found this to be the case on various
standard benchmark datasets  such as GloVe word embeddings [PSM14]. To reﬂect this in our
experiments  we choose two datasets on which [SRB+19] found HBE to be superior to RS as well
as to state-of-the-art methods  and two datasets on which RS was found to be superior. The former
two are Covertype [BD99] and Census9  and the latter two are GloVe [PSM14] and MNIST [LC98].
Their properties are summarized in Table 2.
Experimental setting. We implement and evaluate Algorithm 1. Note that it is parameterized by the
√
number of hash tables L  while its analysis in Theorem 1 is parameterized in terms of τ    where we
recall that L = Θ(1/(
τ 2)). For practical implementation  parameterizing by L is more natural
since it acts as a smooth handle on the resources to accuracy tradeoff – larger L yields better KDE
√
estimates at the expense of using more time and space. τ   need not be speciﬁed explicitly; instead 
for any τ   that satisfy L = Ω(1/(
τ 2))  the guarantee of Theorem 1 holds (namely  for every
query whose true KDE is at least τ  the KDE estimate has up to  relative error with high probability).
We compare our method to the HBE method of [CS17]  as well as to RS as a baseline. The plots for
HBE and our method are generated by varying the number of hash functions L. The plots for RS
are generated by varying the sample size. Note that neither method has any additional parameters to
set. For each method and each parameter setting  we report the median result of 3 trials. For each
dataset we choose two bandwidth settings  one which yields median KDE values of order 10−2  and
the other of order 10−3.10 The bandwidth values and their precise method of choice are speciﬁed in
the appendix. The appendix also includes accuracy results for varying bandwidth values (Fig. 9).
Evaluation metrics. We evaluate the query time  space usage and preprocessing time. In all of the
plots  the y-axis measures the average relative error (which directly corresponds to ) of the KDE
estimate  over 100 query points randomly chosen from the dataset. In the query time plots  the x-axis
counts the number of kernel evaluations per query  which dominates and serves as a proxy for the
running time. In the space usage plots  the x-axis counts the number of stored hashes. We use this
measure for the space usage rather than actual size in bits  since there are various efﬁcient ways to
store each hash  and they apply equally to all algorithms. We also note that the plots do not account
for the space needed to store the sampled dataset itself  which is the same for all methods. RS is not
displayed on these plots since it has no additional space usage. In all three methods the preprocessing
time is proportional to the additional space usage  and is qualitatively captured by the same plots.
Results. The query time plots consistently show that the query time to approximation quality tradeoff
of our method is essentially the same as [CS17]  on all datasets. At the same time  the space usage
plots show that we have achieve a signiﬁcantly smaller space overhead  with the gap from [CS17]
substantially increasing as the target relative error becomes smaller. These ﬁndings afﬁrm the direct
advantage of our method as speciﬁed in Table 1.

8Our code is available at https://github.com/talwagner/efficient_kde.
9Available at https://archive.ics.uci.edu/ml/datasets/US+Census+Data+(1990).
10In all the considered settings  the average KDE value is within a factor of at most 2 from the median KDE.

6

Figure 1: Covertype dataset  typical KDE values of order 10−2.

Figure 2: Covertype dataset  typical KDE values of order 10−3.

Figure 3: Census dataset  typical KDE values of order 10−2.

Figure 4: Census dataset  typical KDE values of order 10−3.

7

Figure 5: MNIST dataset  typical KDE values of order 10−2.

Figure 6: MNIST dataset  typical KDE values of order 10−3.

Figure 7: GloVe dataset  typical KDE values of order 10−2.

Figure 8: GloVe dataset  typical KDE values of order 10−3.

8

Acknowledgments

We thank the anonymous reviewers for useful suggestions. Piotr Indyk was supported by NSF
TRIPODS award #1740751 and Simons Investigator Award.

References
[AI06]

[BD99]

[CK15]

[CS17]

Alexandr Andoni and Piotr Indyk  Near-optimal hashing algorithms for approximate
nearest neighbor in high dimensions  Foundations of Computer Science (FOCS)  IEEE 
2006  pp. 459–468.
Jock A Blackard and Denis J Dean  Comparative accuracies of artiﬁcial neural networks
and discriminant analysis in predicting forest cover types from cartographic variables 
Computers and electronics in agriculture 24 (1999)  no. 3  131–151.
Flavio Chierichetti and Ravi Kumar  Lsh-preserving functions and their applications 
Journal of the ACM (JACM) 62 (2015)  no. 5  33.
Moses Charikar and Paris Siminelakis  Hashing-based-estimators for kernel density in
high dimensions  Foundations of Computer Science (FOCS)  2017.

[CWS12] Yutian Chen  Max Welling  and Alex Smola  Super-samples from kernel herding  2012.
[CXS18] Beidi Chen  Yingchen Xu  and Anshumali Shrivastava  Lsh-sampling breaks the compu-

tational chicken-and-egg loop in adaptive stochastic gradient estimation.

[DIIM04] Mayur Datar  Nicole Immorlica  Piotr Indyk  and Vahab S Mirrokni  Locality-sensitive
hashing scheme based on p-stable distributions  Proceedings of the twentieth annual
symposium on Computational geometry  ACM  2004  pp. 253–262.
Edward Gan and Peter Bailis  Scalable kernel density classiﬁcation via threshold-based
pruning  Proceedings of the 2017 ACM International Conference on Management of
Data  ACM  2017  pp. 945–959.

[GB17]

[GM01] Alexander G Gray and Andrew W Moore  N-body’problems in statistical learning 

[GS91]

[IM98]

[JDH99]

[LC98]
[LS18a]

[LS18b]

Advances in neural information processing systems  2001  pp. 521–527.
Leslie Greengard and John Strain  The fast gauss transform  SIAM Journal on Scientiﬁc
and Statistical Computing 12 (1991)  no. 1  79–94.
Piotr Indyk and Rajeev Motwani  Approximate nearest neighbors: towards removing the
curse of dimensionality  Proceedings of the thirtieth annual ACM symposium on Theory
of computing  ACM  1998  pp. 604–613.
Tommi S Jaakkola  Mark Diekhans  and David Haussler  Using the ﬁsher kernel method
to detect remote protein homologies.  ISMB  vol. 99  1999  pp. 149–158.
Yann LeCun and Corinna Cortes  The mnist database of handwritten digits  1998.
Chen Luo and Anshumali Shrivastava  Arrays of (locality-sensitive) count estimators
(ace): Anomaly detection on the edge  Proceedings of the 2018 World Wide Web Confer-
ence on World Wide Web  2018  pp. 1439–1448.

  Scaling-up split-merge mcmc with locality sensitive sampling (lss)  Preprint.

Available at (2018).

[Phi13]

[PSM14]

[MXB15] William B March  Bo Xiao  and George Biros  Askit: Approximate skeletonization kernel-
independent treecode in high dimensions  SIAM Journal on Scientiﬁc Computing 37
(2015)  no. 2  A1089–A1110.
Jeff M Phillips  ε-samples for kernels  Proceedings of the twenty-fourth annual ACM-
SIAM symposium on Discrete algorithms  SIAM  2013  pp. 1622–1632.
Jeffrey Pennington  Richard Socher  and Christopher Manning  Glove: Global vectors
for word representation  Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)  2014  pp. 1532–1543.
Jeff M Phillips and Wai Ming Tai  Near-optimal coresets of kernel density estimates 
arXiv preprint arXiv:1802.01751 (2018).
Ali Rahimi and Benjamin Recht  Random features for large-scale kernel machines 
Advances in neural information processing systems  2007  pp. 1177–1184.

[PT18]

[RR07]

9

[SRB+19] Paris Siminelakis  Kexin Rong  Peter Bailis  Moses Charikar  and Philip Levis  Rehashing
kernel evaluation in high dimensions  International Conference on Machine Learning 
2019.

[WCN18] Xian Wu  Moses Charikar  and Vishnu Natchu  Local density estimation in high dimen-

sions  arXiv preprint arXiv:1809.07471 (2018).

[ZJPL13] Yan Zheng  Jeffrey Jestes  Jeff M Phillips  and Feifei Li  Quality and efﬁciency for kernel
density estimates in large data  Proceedings of the 2013 ACM SIGMOD International
Conference on Management of Data  ACM  2013  pp. 433–444.

10

,Arturs Backurs
Piotr Indyk
Tal Wagner