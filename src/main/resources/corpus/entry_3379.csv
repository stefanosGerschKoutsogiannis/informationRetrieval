2019,Maximum Expected Hitting Cost of a Markov Decision Process and Informativeness of Rewards,We propose a new complexity measure for Markov decision processes (MDPs)  the maximum expected hitting cost (MEHC). This measure tightens the closely related notion of diameter [JOA10] by accounting for the reward structure.
We show that this parameter replaces diameter in the upper bound on the optimal value span of an extended MDP  thus refining the associated upper bounds on the regret of several UCRL2-like algorithms.
Furthermore  we show that potential-based reward shaping [NHR99] can induce equivalent reward functions with varying informativeness  as measured by MEHC.
By analyzing the change in the maximum expected hitting cost  this work presents a formal understanding of the effect of potential-based reward shaping on regret (and sample complexity) in the undiscounted average reward setting.
We further establish that shaping can reduce or increase MEHC by at most a factor of two in a large class of MDPs with finite MEHC and unsaturated optimal average rewards.,Maximum Expected Hitting Cost of a Markov

Decision Process and Informativeness of Rewards

Toyota Technological Institute at Chicago

Toyota Technological Institute at Chicago

Matthew R. Walter

Chicago  IL  USA 60637

mwalter@ttic.edu

Falcon Z. Dai

Chicago  IL  USA 60637

dai@ttic.edu

Abstract

We propose a new complexity measure for Markov decision processes (MDPs)  the
maximum expected hitting cost (MEHC). This measure tightens the closely related
notion of diameter [JOA10] by accounting for the reward structure. We show that
this parameter replaces diameter in the upper bound on the optimal value span of an
extended MDP  thus reﬁning the associated upper bounds on the regret of several
UCRL2-like algorithms. Furthermore  we show that potential-based reward shaping
[NHR99] can induce equivalent reward functions with varying informativeness 
as measured by MEHC. We further establish that shaping can reduce or increase
MEHC by at most a factor of two in a large class of MDPs with ﬁnite MEHC and
unsaturated optimal average rewards.

1

Introduction

In the average reward setting of reinforcement learning (RL) [Put94; SB98]  an algorithm learns
to maximize its average rewards by interacting with an unknown Markov decision process (MDP).
Similar to analysis in multi-armed bandits and other online machine learning problems  (cumulative)
regret provides a natural model to evaluate the efﬁciency of a learning algorithm. With the UCRL2
algorithm  Jaksch  Ortner  and Auer [JOA10] show a problem-dependent bound of ˜O(DSpAT ) on
regret and an associated logarithmic bound on the expected regret  where D is the diameter of the
actual MDP (Deﬁnition 1)  S the size of the state space  and A the size of the action space. Many
subsequent algorithms [FLP19] enjoy similar diameter-dependent bounds. This establishes diameter
as an important measure of complexity for an MDP. However  strikingly  this measure is independent
of rewards and is a function of only the transitions. This is obviously peculiar as two MDPs differing
only in their rewards would have the same regret bounds even if one gives the maximum reward for
all transitions. We review the related key observation by Jaksch  Ortner  and Auer [JOA10]  and
reﬁne it with a new lemma (Lemma 1)  establishing a reward-sensitive complexity measure that we
refer to as the maximum expected hitting cost (MEHC  Deﬁnition 2)  which tightens the regret bounds
of UCRL2 and similar algorithms by replacing diameter (Theorem 1).
Next  with respect to this new complexity measure  we describe a notion of reward informativeness
(Section 2.4). Intuitively speaking  in an environment  the same desired policies can be motivated by
different (immediate) rewards. These differing deﬁnitions of rewards can be more or less informative
of useful actions  i.e.  yielding high long-term rewards. To formalize this intuition  we study a way
to reparametrize rewards via potential-based reward shaping (PBRS) [NHR99] that can produce
different rewards with the same near-optimal policies (Section 2.5). We show that the MEHC changes
under reparametrization by PBRS and  in turn  so do regret and sample complexity  substantiating
this notion of informativeness. Lastly  we study the extent of its impact. In particular  we show that
there is a factor-of-two limit on its impact on MEHC in a large class of MDPs (Theorem 2). This
result and the concept of reward informativeness may be useful for a task designer crafting a reward
function (Section 3). The detailed proofs are deffered to Appendix A.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

The main contributions of this work are two-fold:

• We propose a new MDP structural parameter  maximum expected hitting cost (MEHC)  that
accounts for both transitions and rewards. This parameter replaces diameter in the regret
bounds of several model-based RL algorithms.

• We show that potential-based reward shaping can change the maximum expected hitting
cost of an MDP and thus the regret bound. This results in a set of equivalent MDPs with
different learning difﬁculties as measured by regret. Moreover  we show that their MEHCs
differ by a factor of at most two in a large class of MDPs.

1.1 Related work

This work is closely related to the study of diameter as an MDP complexity measure [JOA10] 
which is prevalent in the regret bounds of RL algorithms in the average reward setting [FLP19]. As
noted by Jaksch  Ortner  and Auer [JOA10]  unlike some previous measures of MDP complexity
such as the return mixing time [KS02; BT02]  diameter depends only on the transitions  but not the
rewards. The core reason for the presence of diameter in the regret analysis is that it upper bounds
the optimal value span of the extended MDP that summarizes the observations (Section 2.3 and
Equation 8). We review and update this observation with a reward-dependent parameter we called
maximum expected hitting cost (Lemma 1). Interestingly  the gap between diameter and MEHC can
be arbitrarily large (M )  rmaxD(M ); there are MDPs with ﬁnite MEHC and inﬁnite diameter.
These MDPs are non-communicating but have saturated optimal average rewards ⇢⇤(M ) = rmax.
Intuitively  there is a state s in these MDPs from which the learner cannot visit some other state s0 
but can nonetheless achieve the maximum possible average reward  thus allowing for good regret
guarantees; the unreachable states will not seem better than the reachable ones under the principle of
optimism in the face of uncertainty (OFU). We will use UCRL2 [JOA10] as an example algorithm
throughout the rest of the article  however the main results do not depend on it. In particular  with
MEHC  its regret bounds are updated (Theorem 1).
Another important comparison is with optimal bias span [Put94; BT09; Fru+18]  a reward-dependent
parameter of MDPs. Here  we again ﬁnd that the gap can be arbitrarily large sp(M )  (M ).1 These
non-communicating MDPs would have unsaturated optimal average reward ⇢⇤(M ) < rmax. But as
shown elsewhere [FPL18; Fru+18]  extra knowledge of (some upper bound on) the optimal bias span
is necessary for an algorithm to enjoy a regret that scales with this smaller parameter. In contrast 
UCRL2  which scales with MEHC  does not need to know the diameter or MEHC of the actual MDP.
Potential-based reward shaping [NHR99] was originally proposed as a solution technique for a
programmer to inﬂuence the sample complexity of their reinforcement learning algorithm  without
changing the near-optimal policies in episodic and discounted settings. Prior theoretical analysis
involving PBRS [NHR99; Wie03; WCE03; ALZ08; Grz17] mostly focuses on the consistency of RL
against the shaped rewards  i.e.  the resulting learned behavior is also (near-)optimal in the original
MDP  while suggesting empirically that the sample complexity can be changed by a well speciﬁed
potential. In this work  we use PBRS to construct ⇧-equivalent reward functions in the average
reward setting (Section 2.4) and show that two reward functions related by a shaping potential can
have different MEHCs  and thus different regrets and sample complexities (Section 2.5). However  a
subtle but important technical requirement of [0  rmax]-boundedness of MDPs makes it difﬁcult to
immediately apply our results (Section 2.5 and Theorem 2) to the treatment of PBRS as a solution
technique because an arbitrary potential function picked without knowledge of the original MDP
may not preserve the [0  rmax]-boundedness. Nevertheless  we think our work may bring some new
perspectives to this topic.

1This inequality can be derived as a consequence of Lemma 1 as N (s  a) ! 1  M + has very tight
conﬁdence intervals around the actual transition and mean rewards of M. Observe that the span of ui is equal to
sp(M ) at the limit of i ! 1 [JOA10  remark 8].

2

2 Results

2.1 Markov decision process
A Markov decision process is deﬁned by the tuple M = (S A  p  r)  where S is the state space  A
is the action space  p : S⇥A!P (S) is the transition function  and r : S⇥A!P ([0  rmax]) is
the reward function with mean ¯r(s  a) := E[r(s  a)]. We assume that the state and action spaces are
ﬁnite  with sizes S := |S| and A := |A|  respectively. At each time step t = 0  1  2  . . .  an algorithm
L chooses an action at 2A based on the observations up to that point. The state transitions to st+1
with probability p(st+1|st  at) and a reward rt 2 [0  rmax] is drawn according to the distribution
r(st  at).2 The transition probabilities and reward function of the MDP are unknown to the learner.
The sequence of random variables (st  at  rt)t0 forms a stochastic process. Note that a stationary
deterministic policy ⇡ : S!A is a restrictive type of algorithm whose action at depends only on st.
We refer to stationary deterministic policies as policies in the rest of the paper.
Recall that in a Markov chain  the hitting time of state s0 starting at state s is a random variable
hs!s0 := inf{t 2 N0|st = s0 and s0 = s}3 [LPW08].
Deﬁnition 1 (Diameter  [JOA10]). Suppose in the stochastic process induced by following a policy
⇡ in MDP M  the time to hit state s0 starting at state s is hs!s0(M  ⇡). We deﬁne the diameter of M
to be

D(M ) := max
s s02S

min
⇡:S!A

E [hs!s0(M  ⇡)] .

We incorporate rewards into diameter  and introduce a novel MDP parameter.
Deﬁnition 2 (Maximum expected hitting cost). We deﬁne the maximum expected hitting cost of a
Markov decision process M to be

(M ) := max
s s02S

min
⇡:S!A

E24

hs!s0 (M ⇡)1Xt=0

rmax  rt35 .

Observe that MEHC is a smaller parameter  that is  (M )  rmaxD(M )  since for any s  s0 ⇡   we
have rmax  rt  rmax.
2.2 Average reward criterion  and regret

The accumulated reward of an algorithm L after T time steps in MDP M starting at state s is a
random variable

R(M  L  s  T ) :=

We deﬁne the average reward or gain [Put94] as

rt.

T1Xt=0

⇢(M  L  s) := lim
T!1

1
T E [R(M  L  s  T )] .

(1)

We will evaluate policies by their average reward. This can be maximized by a stationary deterministic
policy and we deﬁne the optimal average reward of M starting at state s as

⇢⇤(M  s) := max
⇡:S!A

⇢(M  ⇡  s).

(2)

Furthermore  we assume that the optimal average reward starting at any state to be the same  i.e. 
⇢⇤(M  s) = maxs0 ⇢⇤(M  s0) for any state s. This is a natural requirement of an MDP in the
online setting to allow for any hope for a vanishing regret. Otherwise  the learner may take actions
leading to states with a lower average optimal reward due to ignorance and incur linear regret

2It is important to assume that the support of rewards lies in a known bounded interval  often [0  1] by
convention. This is sometimes referred to as a bounded MDP in the literature. Analogous to bandits  the details
of the reward distribution often is unimportant  and it sufﬁces to specify an MDP with the mean rewards ¯r.

30-indexing ensures that hs!s = 0. Note also that by convention  inf ? = 1.

3

when compared with the optimal policy starting at the initial state. In particular  this condition is
true for communicating MDPs [Put94] by virtue of their transitions  but this is also possible for
non-communicating MDPs with appropriate rewards. We will write ⇢⇤(M ) := maxs0 ⇢⇤(M  s0).
We will compete with the expected cumulative reward of an optimal policy on its trajectory  and
deﬁne the regret of a learning algorithm L starting at state s after T time steps as

(M  L  s  T ) := T⇢ ⇤(M )  R(M  L  s  T ).
2.3 Optimism in the face of uncertainty  extended MDP  and UCRL2

(3)

The principle of optimism in the face of uncertainty (OFU) [SB98] states that for uncertain state-
action pairs  i.e.  those that we have not visited enough up to this point  we should be optimistic about
their outcome. The intuition for doing so is that taking reward-maximizing actions with respect to this
optimistic model (in terms of both transitions and immediate rewards for these uncertain state-action
pairs)  we will have no regret if the optimism is well placed and will otherwise quickly learn more
about these suboptimal state-action pairs to avoid them in the future. This fruitful idea has been the
basis for many model-based RL algorithms [FLP19] and in particular  UCRL2 [JOA10]  which keeps
track of the statistical uncertainty via upper conﬁdence bounds.
Suppose we have visited a particular state-action pair (s  a) N (s  a)-many times. With conﬁdence
at least 1    we can establish that a conﬁdence interval for both its mean reward ¯r(s  a) and its
transition p(·|s  a) from the Chernoff-Hoeffding inequality (or Bernstein  [FPL18]). Let b(  n) 2 R
be the -conﬁdence bound after observing n i.i.d. samples of a [0  1]-bounded random variable 
ˆr(s  a) the empirical mean of r(s  a)  ˆp(·|s  a) the empirical transition of p(·|s  a). The statistically
plausible mean rewards are

and the statistically plausible transitions are

B(s  a) :=r0 2 R : |r0  ˆr(s  a)| rmax b(  N(s  a)) \ [0  rmax]
C(s  a) :=p0 2P (S) : ||p0(·)  ˆp(·|s  a)||1  b(  N(s  a)) .

We deﬁne an extended MDP M + := (S A+  p+  r+) to summarize these statistics [GLD00; SL05;
TB07; JOA10]  where S is the same state space as in M  the action space A+ is a union over
state-speciﬁc actions

(4)
where A is the same action space in M  p+ the transitions according to the selected distribution p0
(5)

s :=(a  p0  r0) : a 2A   p0 2 C(s  a)  r0 2 B(s  a)  
A+

and r+ is the rewards according to the selected mean reward r0

p+ · |s  (a  p0  r0) := p0(·) 
r+s  (a  p0  r0) := r0.

(6)

It is not hard to see that M + is indeed an MDP with an inﬁnite but compact action space.
By OFU  we want to ﬁnd an optimal policy for an optimistic MDP within the set of statistically
plausible MDPs. As observed in [JOA10]  this is equivalent to ﬁnding an optimal policy ⇡+ : S!A +
in the extended MDP M +  which speciﬁes a policy in M via ⇡(s) := 1(⇡+(s))  where i is the

projection map onto the i-th coordinate (and an optimistic MDP fM = (S A ep er) via transitions
ep(·|s  ⇡(s)) := 2(⇡+(s)) and mean rewardser(s  ⇡(s)) := 3(⇡+(s)) over actions selected by ⇡4).

By construction of the extended MDP M +  M is in M + with high conﬁdence  i.e.  ¯r(s  a) 2 B(s  a)
and p(·|s  a) 2 C(s  a) for all s 2S   a 2A . At the heart of UCRL2-type regret analysis  there
is a key observation [JOA10  equation (11)] that we can bound the span of optimal values in the
extended MDP M + by the diameter of the actual MDP M under the condition that M is in M +.
This observation is needed to characterize how good following the “optimistic” policy 1(⇡+) in the
actual MDP M is. For i  0  the i-step optimal values ui(s) of M + is the expected total reward by

4We can set transitions and mean rewards over actions a 6= ⇡(s) to ˆp and ˆr  respectively.

4

following an optimal non-stationary i-step policy starting at state s 2S . We can also deﬁne them
recursively (via dynamic programming5)

u0(s) := 0

p+s0|s  (a  p0  r0) ui(s0)#

s "r+s  (a  p0  r0) +Xs0
p0(s0) ui(s0)#
s "r0 +Xs0

ui+1(s) := max

(a p0 r0)2A+
By (5) and (6)

= max

(a p0 r0)2A+

By (4)

a2A" max

r02B(s a)

= max

r0 + max

p02C(s a)Xs0

p0(s0) ui(s0)#

(7)

We are now ready to restate the observation. If M is in M +  which happens with high probability 
Jaksch  Ortner  and Auer [JOA10] observe that

max

s

ui(s)  min

s0

ui(s0)  rmaxD(M ).

(8)

However  this bound is too conservative because it fails to account for the rewards collected. By
patching this  we tighten the upper bound with MEHC.
Lemma 1 (MEHC upper bounds the span of values). Assuming that the actual MDP M is in the
extended MDP M +  i.e.  ¯r(s  a) 2 B(s  a) and p(·|s  a) 2 C(s  a) for all s 2S   a 2A   we have

max

s

ui(s)  min

s0

ui(s0)  (M )

where ui(s) is the i-step optimal undiscounted value of state s.

This reﬁned upper bound immediately plugs into the main theorems of [JOA10  equations 19 and 22 
theorem 2].
Theorem 1 (Reward-sensitive regret bound of UCRL2). With probability of at least 1    for any
initial state s and any T > 1  and  := (M )  the regret of UCRL2 is bounded by

(M  UCRL2  s  T )

8

 ◆ + pT + s 5

s 5
T log✓ 8T
+ s14S log✓ 2AT
 34 max{1 }SsAT log✓ T
◆.

2

T log✓ 8T
 ◆ +s14 log✓ 2SAT

 ◆ + SA log2✓ 8T
SA◆
 ◆ + 2!(p2 + 1)pSAT

" ⌘ sample complexity [Kak03] 
As a corollary  Theorem 1 implies that UCRL2 offers O⇣ 2S2A
by inverting the regret bound by demanding that the per-step regret is at most " with probability of at
least 1   [JOA10  corollary 3]. Similarly  we have an updated logarithmic bound on the expected
regret [JOA10  theorem 4]  E[(M  UCRL2  s  T )] = O( 2S2A log T
) where g is the gap in average
reward between the best policy and the second best policy.

log SA

"2

g

5In fact  the exact maximization of Equation 7 can be found via extended value iteration [JOA10  section 3.1]

5

Informativeness of rewards

2.4
Informally  it is not hard to appreciate the challenge imposed by delayed feedback inherent in
MDPs  as actions with high immediate rewards do not necessarily lead to a high optimal value. Are
there different but “equivalent” reward functions that differ in their informativeness with the more
informative ones being easier to reinforcement learn? Suppose we have two MDPs differing only in
their rewards  M1 = (S A  p  r1) and M2 = (S A  p  r2)  then they will have the same diameters
D(M1) = D(M2) and thus the same diameter-dependent regret bounds from previous works. With
MEHC  however  we may get a more meaningful answer.
Firstly  let us make precise a notion of equivalence. We say that r1 and r2 are ⇧-equivalent if
for any policy ⇡ : S!A   its average rewards are the same under the two reward functions
⇢(M1 ⇡  s ) = ⇢(M2 ⇡  s ). Formally  we will study the MEHC of a class of ⇧-equivalent reward
functions related via a potential.

2.5 Potential-based reward shaping
Originally introduced by Ng  Harada  and Russell [NHR99]  potential-based reward shaping (PBRS)
takes a potential ' : S! R and deﬁnes shaped rewards

r'
t := rt  '(st) + '(st+1).

(9)
t )t0 being generated from an MDP M ' =

We can think of the stochastic process (st  at  r'
(S A  p  r') with reward function r' : S⇥A!P ([0  rmax])6 whose mean rewards are

¯r'(s  a) = ¯r(s  a)  '(s) + Es0⇠p(·|s a) ['(s0)] .
It is easy to check that r' and r are indeed ⇧-equivalent. For any policy ⇡ 

⇢(M ' ⇡  s ) = lim
T!1

= lim
T!1

= lim
T!1

By telescoping sums of potential terms over consecutive t

1
T E [R(M ' ⇡  s  T )]
1

r'

t#
rt  '(st) + '(st+1)#
rt#
T1Xt=0

T E"T1Xt=0
T E"T1Xt=0
T E"'(s0) + '(sT ) +
T⇣  '(s) + E['(sT )] + E [R(M  ⇡  s  T )]⌘
1
T E [R(M  ⇡  s  T )]

1

1

1

The ﬁrst two terms vanish in the limit

= lim
T!1

= lim
T!1

= lim
T!1

= ⇢(M  ⇡  s).

(10)

To get some intuition  it is instructive to consider a toy example (Figure 1). Suppose 0 <<↵
and ✏ 2 (0  1)  then the optimal average reward in this MDP is 1    and the optimal stationary
deterministic policy is ⇡⇤(s1) := a2 and ⇡⇤(s2) := a1  as staying in state s2 yields the highest
average reward. As the expected number of steps needed to transition from state s1 to s2 and vice
versa are both 1/✏ via action a2  we conclude that (M ) = max{↵  ↵/✏  /✏  } = ↵/✏. Furthermore 
notice that taking action a2 in either state transitions to the other state with probability of ✏  however
the immediate rewards are the same as taking the alternative action a1 to stay in the current state—the
immediate rewards are not informative. We can differentiate the actions better by shaping with a
potential of '(s1) := 0 and '(s2) := (↵)/2✏. The shaped mean rewards become  at s1 

¯r'(s1  a2) = 1  ↵  '(s1) + ✏'(s2) + (1  ✏)'(s1) = 1  (↵+)/2 > 1  ↵ = ¯r'(s1  a1)
6One needs to ensure that ' respects the [0  rmax]-boundedness of M.

6

1  ↵

s1

a1

1

1  ↵

a2

✏

1  ✏

1  ✏

s2

a1

1  
1

✏

a2

1  

Figure 1: Circular nodes represent states and square nodes represent actions. The solid edges
are labeled by the transition probabilities and the dashed edges are labeled by the mean rewards.
Furthermore  rmax = 1. For concreteness  one can consider setting ↵ = 0.11  = 0.1 ✏ = 0.05.

and at s2 

¯r'(s2  a2) = 1    '(s2) + ✏'(s1) + (1  ✏)'(s2) = 1  (↵+)/2 < 1   = ¯r'(s2  a1).

This encourages taking actions a2 at state s1 and discourages taking actions a1 at state s2 simultane-
ously. The maximum expected hitting cost becomes smaller

↵
✏

 ' (s2)  '(s1) +



✏

(M ') = max⇢↵    '(s1)  '(s2) +
2✏ 

= max⇢↵   

2✏

↵ + 

↵ + 

 

=

<

↵ + 

2✏
= (M ).

↵
✏

In this example  MEHC is halved at best when  is made arbitrarily close to zero. Noting that the
original MDP M is equivalent to M ' shaped with potential '  i.e. M = (M ')' from (9)  we
see that MEHC can be almost doubled. It turns out that halving or doubling the MEHC is the most
PBRS can do in a large class of MDPs.
Theorem 2 (MEHC under PBRS). Given an MDP M with ﬁnite maximum expected hitting cost
(M ) < 1 and an unsaturated optimal average reward ⇢⇤(M ) < rmax  the maximum expected
hitting cost of any PBRS-parameterized MDP M ' is bounded by a multiplicative factor of two

1
2

(M )  (M ')  2(M ).

The key observation is that the expected total rewards along a loop remains unchanged by shaping 
which originally motivated PBRS [NHR99]. To see this  consider a loop as a concatenation of two
paths  one from s to s0 and the other from s0 to s. Under the shaping of a potential '  the expected
total rewards of the former is increased by '(s0)  '(s) and the latter is decreased by the same
amount. For more details  see Appendix A.2.

3 Discussion

If we view RL as an engineering tool that “compiles” an arbitrary reward function into a behavior (as
represented by a policy) in an environment  then a programmer’s primary responsibility would be to
craft a reward function that faithfully expresses the intended goal. However  this problem of reward
design is complicated by practical concerns for the difﬁculty of learning. As recognized by Kober 
Bagnell  and Peters [KBP13  section 3.4] 

“[t]here is also a trade-off between the complexity of the reward function and the
complexity of the learning problem.”

7

Accurate rewards are often easy to specify in a sparse manner (reaching a position  capturing the
king  etc)  thus hard to learn  whereas dense rewards  providing more feedback  are harder to specify
accurately  leading to incorrect trained behaviors. The recent rise of deep RL also exposes “bugs”
in some of these designed rewards [CA16]. Our results show that the informativeness of rewards 
an aspect of “the complexity of the learning problem” can be controlled to some extent by a well
speciﬁed potential without inadvertently changing the intended behaviors of the original reward.
Therefore  we propose to separate the deﬁnitional concern from the training concern. Rewards
should be ﬁrst deﬁned to faithfully express the intended task  and then any extra knowledge can be
incorporated via a shaping potential to reduce the sample complexity of training to obtain the same
desired behaviors. That is not to say that it is generally easy to ﬁnd a helpful potential making the
rewards more informative.
Though Theorem 2 might be a disappointing result for PBRS  we wish to emphasize that this result
most directly concerns algorithms whose regrets scale with MEHC  such as UCRL2. It is conceivable
that in a different setting such as discounted total rewards  or for a different RL algorithm  such as
SARSA with epsilon-greedy exploration [NHR99  footnote 4]  PBRS might have a greater impact on
the learning efﬁciency.

Acknowledgments
This work was supported in part by the National Science Foundation under Grant No. 1830660. We
thank Avrim Blum for many insightful comments. In particular  his challenge to ﬁnding a better
example has led to Theorem 2. We also thank Ronan Fruit for a discussion on a concept similar to
the proposed maximum expected hitting cost that he independently developed in his thesis draft.

References
[ALZ08]

[BT02]

[BT09]

[CA16]

[FLP19]

[FPL18]

John Asmuth  Michael L Littman  and Robert Zinkov. “Potential-based Shaping in
Model-based Reinforcement Learning.” In: Proceedings of the National Conference on
Artiﬁcial Intelligence (AAAI). 2008  pp. 604–609.
Ronen I Brafman and Moshe Tennenholtz. “R-max-a general polynomial time algorithm
for near-optimal reinforcement learning”. In: Journal of Machine Learning Research
3.Oct (2002)  pp. 213–231.
Peter L Bartlett and Ambuj Tewari. “REGAL: A regularization based algorithm for rein-
forcement learning in weakly communicating MDPs”. In: Proceedings of the Conference
on Uncertainty in Artiﬁcial Intelligence (UAI). AUAI Press. 2009  pp. 35–42.
Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild. 2016. URL: https:
//openai.com/blog/faulty-reward-functions/ (visited on 10/27/2019).
Ronan Fruit  Alessandro Lazaric  and Matteo Pirotta. “Exploration-Exploitation in Rein-
forcement Learning”. Tutorial at Algorithmic Learning Theory conference. 2019. URL:
https://rlgammazero.github.io/docs/2019_ALT_exptutorial.pdf.
Ronan Fruit  Matteo Pirotta  and Alessandro Lazaric. “Near optimal exploration-
exploitation in non-communicating markov decision processes”. In: Advances in Neural
Information Processing Systems (NeurIPS). 2018  pp. 2994–3004.

[Fru+18] Ronan Fruit  Matteo Pirotta  Alessandro Lazaric  and Ronald Ortner. “Efﬁcient Bias-
Span-Constrained Exploration-Exploitation in Reinforcement Learning”. In: Proceedings
of the International Conference on Machine Learning (ICML). 2018  pp. 1573–1581.

[GLD00] Robert Givan  Sonia Leach  and Thomas Dean. “Bounded-parameter Markov decision

processes”. In: Artiﬁcial Intelligence 122.1–2 (2000)  pp. 71–109.

[Grz17] Marek Grze´s. “Reward shaping in episodic reinforcement learning”. In: Proceedings of
the International Conference on Autonomous Agents and MultiAgent Systems (AAMAS).
International Foundation for Autonomous Agents and Multiagent Systems. 2017  pp. 565–
573.
Thomas Jaksch  Ronald Ortner  and Peter Auer. “Near-optimal regret bounds for rein-
forcement learning”. In: Journal of Machine Learning Research 11.Apr (2010)  pp. 1563–
1600.
Sham Machandranath Kakade. “On the sample complexity of reinforcement learning”.
PhD thesis. 2003.

[JOA10]

[Kak03]

8

[KBP13]

[KS02]

Jens Kober  J Andrew Bagnell  and Jan Peters. “Reinforcement learning in robotics: A
survey”. In: The International Journal of Robotics Research 32.11 (2013)  pp. 1238–
1274.
Michael Kearns and Satinder Singh. “Near-optimal reinforcement learning in polynomial
time”. In: Machine learning 49.2-3 (2002)  pp. 209–232.

[LPW08] David A Levin  Yuval Peres  and Elizabeth L Wilmer. Markov chains and mixing times.

American Mathematical Soc.  2008.

[Put94]

[NHR99] Andrew Y Ng  Daishi Harada  and Stuart Russell. “Policy invariance under reward
transformations: Theory and application to reward shaping”. In: Proceedings of the
International Conference on Machine Learning (ICML). Vol. 99. 1999  pp. 278–287.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Program-
ming. John Wiley & Sons  Inc.  1994.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT
press  1998.
Alexander L Strehl and Michael L Littman. “A theoretical analysis of model-based inter-
val estimation”. In: Proceedings of the International Conference on Machine Learning
(ICML). ACM. 2005  pp. 856–863.
Ambuj Tewari and Peter L Bartlett. “Bounded parameter Markov decision processes
with average reward criterion”. In: International Conference on Computational Learning
Theory (COLT). Springer. 2007  pp. 263–277.

[TB07]

[SB98]

[SL05]

[WCE03] Eric Wiewiora  Garrison W Cottrell  and Charles Elkan. “Principled methods for advising
reinforcement learning agents”. In: Proceedings of the International Conference on
Machine Learning (ICML). 2003  pp. 792–799.
Eric Wiewiora. “Potential-based shaping and Q-value initialization are equivalent”. In:
Journal of Artiﬁcial Intelligence Research 19 (2003)  pp. 205–208.

[Wie03]

9

,Falcon Dai
Matthew Walter