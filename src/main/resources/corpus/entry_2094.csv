2012,Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling,Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover  the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions)  which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First  we introduce the Fiedler delta statistic  based on the Laplacian spectrum of graphs  which allows to dispense with any parametric assumption concerning the modeled network properties. Second  we use the defined statistic to develop the Fiedler random field model  which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields  we estimate them over several real-world networks  showing that they achieve a much higher modeling accuracy than other well-known statistical approaches.,Fiedler Random Fields: A Large-Scale Spectral

Approach to Statistical Network Modeling

Antonino Freno

Mikaela Keller∗

Marc Tommasi∗

INRIA Lille – Nord Europe

40 avenue Halley – Bˆat A – Park Plaza

59650 Villeneuve d’Ascq (France)

{antonino.freno  mikaela.keller  marc.tommasi}@inria.fr

Abstract

Statistical models for networks have been typically committed to strong prior as-
sumptions concerning the form of the modeled distributions. Moreover  the vast
majority of currently available models are explicitly designed for capturing some
speciﬁc graph properties (such as power-law degree distributions)  which makes
them unsuitable for application to domains where the behavior of the target quan-
tities is not known a priori. The key contribution of this paper is twofold. First 
we introduce the Fiedler delta statistic  based on the Laplacian spectrum of graphs 
which allows to dispense with any parametric assumption concerning the modeled
network properties. Second  we use the deﬁned statistic to develop the Fiedler
random ﬁeld model  which allows for efﬁcient estimation of edge distributions
over large-scale random networks. After analyzing the dependence structure in-
volved in Fiedler random ﬁelds  we estimate them over several real-world net-
works  showing that they achieve a much higher modeling accuracy than other
well-known statistical approaches.

1 Introduction

Arising from domains as diverse as bioinformatics and web mining  large-scale data exhibiting net-
work structure are becoming increasingly available. Network models are commonly used to rep-
resent the relations among data units and their structural interactions. Recent studies  especially
targeted at social network modeling  have focused on random graph models of those networks. In
the simplest form  a random network is a conﬁguration of binary random variables Xuv such that
the value of Xuv stands for the presence or absence of a link between nodes u and v in the network.
The general idea underlying random graph modeling is that network conﬁgurations are generated
by a stochastic process governed by speciﬁc probability laws  so that different models correspond to
different families of distributions over graphs.

The simplest random graph model is the Erd˝os-R´enyi (ER) model [1]  which assumes that the prob-
ability of observing a link between two nodes in a given graph is constant for any pair of nodes in
that graph  and it is independent of which other edges are being observed. In preferential attachment
models [2]  the probability of linking to any speciﬁed node in a graph is proportional to the degree
of the node in the graph  leading to “rich get richer” effects. Small-world models [3] try to capture
instead such phenomena often observed in real networks as small diameters and high clustering co-
efﬁcients. An attempt to model potentially complex dependencies between graph edges in the form
of Gibbs-Boltzmann distributions is made by exponential random graph (ERG) models [4]  which
subsume the ER model as a special case. Finally  a recent attempt at modeling real networks through

∗Universit´e Charles de Gaulle – Lille 3  Domaine Universitaire du Pont de Bois – BP 60149  59653 Vil-

leneuve d’Ascq (France).

1

a stochastic generative process is made by Kronecker graphs [5]  which try to capture phenomena
such as heavy-tailed degree distributions and shrinking diameter properties while paying attention
to the temporal dynamics of network growth.

While some of these models behave better than others in terms of computational tractability  one
basic limitation affecting all of them is a sort of parametric assumption concerning the probability
laws underlying the observed network properties. In other words  currently available models of net-
work structure assume that the shape of the probability distribution generating the network is known
a priori. For example  typical formulations of ERG models assume that the building blocks of real
networks are given by such structures as k-stars and k-triangles  with different weights assigned to
different structures  whereas the preferential attachment model is committed to the assumption that
the observed degree distributions obey a power law. In such frameworks  estimating the model from
data reduces to ﬁtting the model parameters  where the parametric form of the target distribution is
ﬁxed a priori. Clearly  in order for such models to deliver accurate estimates of the distributions at
hand  their prior assumptions concerning the behavior of the target quantities must be satisﬁed by
the given data. But unfortunately  this is something that we can rarely assess a priori. To date  the
knowledge we have concerning large-scale real-world networks does not allow to assess whether
any particular parametric assumption is capturing in depth the target generative process  although
some observed network properties may happen to be modeled fairly well.

The aim of this paper is twofold. On the one hand  we take a ﬁrst step toward nonparametric
modeling of random networks by developing a novel network statistic  which we call the Fiedler
delta statistic. The Fiedler delta function allows to model different graph properties at once in an
extremely compact form. This statistic is based on the spectral analysis of the graph  and in particular
on the smallest non-zero eigenvalue of the Laplacian matrix  which is known as Fiedler value [6  7].
On the other hand  we use the Fiedler delta statistic to deﬁne a Boltzmann distribution over graphs 
leading to the Fiedler random ﬁeld (FRF) model. Roughly speaking  for each binary edge variable
Xuv  potentials in a FRF are functions of the difference determined in the Fiedler value by ﬂipping
the value of Xuv  where the spectral decomposition is restricted to a suitable subgraph incident to
nodes u  v. The intuition is that the information encapsulated in the Fiedler delta for Xuv gives
a measure of the role of Xuv in determining the algebraic connectivity of its neighborhood. As
a ﬁrst step in the theoretical analysis of FRFs  we prove that these models allow to capture edge
correlations at any distance within a given neighborhood  hence deﬁning a fairly general class of
conditional independence structures over networks.

The paper is organized as follows. Sec. 2 reviews some theoretical background concerning the
Laplacian spectrum of graphs. FRFs are then introduced in Sec. 3  where we also analyze their
dependence structure and present an efﬁcient approach for learning them from data. To avoid un-
warranted prior assumptions concerning the statistical behavior of the Fiedler delta  potentials are
modeled by non-linear functions  which we estimate from data by minimizing a contrastive diver-
gence objective. FRFs are evaluated experimentally in Sec. 4  showing that they are well suited for
large-scale estimation problems over real-world networks  while Sec. 5 draws some conclusions and
sketches a few directions for further work.

2 Graphs  Laplacians  and eigenvalues

Let G = (V  E) be an undirected graph with n nodes. In the following we assume that the graph is
unweighted with adjacency matrix A. The degree du of a node u ∈ V is deﬁned as the number of
connections of u to other nodes  that is du = |{v: {u  v} ∈ E}|. Accordingly  the degree matrix D of
a graph G corresponds to the diagonal matrix with the vertex degrees d1  . . .   dn on the diagonal. The
main tools exploited by the random graph model proposed here are the graph Laplacian matrices.
Different graph Laplacians have been deﬁned in the literature. In this work  we use consistently the
unnormalized graph Laplacian  given by L = D − A. Some basic facts related to the unnormalized
Laplacian matrix can be summarized as follows [7]:
Proposition 1. The unnormalized graph Laplacian L of an undirected graph G has the following
properties: (i) L is symmetric and positive semi-deﬁnite; (ii) the smallest eigenvalue of L is 0; (iii)
L has n non-negative  real-valued eigenvalues 0 = λ1 ≤ . . . ≤ λn; (iv) the multiplicity of the
eigenvalue 0 of L equals the number of connected components in the graph  that is  λ1 = 0 and
λ2 > 0 if and only if G is connected.

2

In the following  the (algebraic) multiplicity of an eigenvalue λi will be denoted by M (λi  G).
If the graph has one single connected component  then M (0  G) = 1  and the second smallest eigen-
value λ2(G) > 0 is called  in this case  the Fiedler eigenvalue. The Fiedler eigenvalue provides
insight into several graph properties: when there is a nontrivial spectral gap  i.e. λ2(G) is clearly
separated from 0  the graph has good expansion properties  stronger connectivity  and rapid conver-
gence of random walks in the graph. For example  it is known that λ2(G) ≤ µ(G)  where µ(G) is the
edge connectivity of the graph (i.e. the size of the smallest edge cut whose removal makes the graph
disconnected [7]). Notice that if the graph has more than one connected component  then λ2(G) will
be also equal to zero  thus implying that the graph is not connected. Without loss of generality  we
abuse the term Fiedler eigenvalue to denote the smallest eigenvalue different from zero  regardless
of the number of connected components. In this paper  by Fiedler value we mean the eigenvalue
λk+1(G)  where k = M (0  G).

= G or Guv−

= (V  E ∪ {{u  v}})  and Guv−

For any pair of nodes u and v in a graph G = (V  E)  we deﬁne two corresponding graphs Guv+ and
Guv− in the following way: Guv+
= (V  E \ {{u  v}}). Clearly  we
have that either Guv+
= G. A basic property concerning the Laplacian eigenvalues of
Guv+ and Guv− is the following [7  8  9]:
Lemma 1. If Guv+ and Guv− are two graphs with n nodes  such that {u  v} ⊆ V  Guv+
{{u  v}})  and Guv−
(ii) λi(Guv+

= (V  E \ {{u  v}})  then we have that: (i)Pn

) for any i such that 1 ≤ i ≤ n.

= (V  E ∪
) = 2;

i=1 λi(Guv+

) ≤ λi(Guv−

) − λi(Guv−

3 Fiedler random ﬁelds

Fiedler random ﬁelds are introduced in Sec. 3.1  while in Secs. 3.2–3.3 we discuss their dependence
structure and explain how to estimate them from data respectively.

3.1 Probability distribution

Using the notions reviewed above  we deﬁne the Fiedler delta function ∆λ2 in the following way:
Deﬁnition 1. Given graph G  let k = M (0  Guv+

∆λ2(u  v  G) =(cid:26) λk+1(Guv+

λk+1(Guv−

). Then 
) − λk+1(Guv−
) − λk+1(Guv+

if Xuv = 1
)
) otherwise

(1)

In other words  ∆λ2(u  v  G) is the variation in the Fiedler eigenvalue of the graph Laplacian that
would result from ﬂipping the value of Xuv in G. Concerning the range of the Fiedler delta function 
we can easily prove the following proposition:
Proposition 2. For any graph G = (V  E) and any pair of nodes {u  v} such that Xuv = 1  we have
that 0 ≤ ∆λ2(u  v  G) ≤ 2.

Proof. Let k = M (0  G). The proposition follows straightforwardly from Lemma 1  given that
∆λ2(u  v  G) = λk+1(G) − λk+1(Guv−

).

We now proceed to deﬁne FRFs. Given a graph G = (V  E)  for each (unordered) pair of nodes
{u  v} such that u 6= v  we take Xuv to denote a binary random variable such that Xuv = 1 if
{u  v} ∈ E  and Xuv = 0 otherwise. Since the graph is undirected  Xuv = Xvu. We also say that a

subgraph GS of G with edge set ES is incident to Xuv if {u  v} ⊆Se∈ES

Deﬁnition 2. Given a graph G  let XG denote the set of random variables deﬁned on G  i.e. XG =
{Xuv: u 6= v ∧ {u  v} ⊆ V}. For any Xuv ∈ XG  let Guv be a subgraph of G which is incident
to Xuv and ϕuv be a two-place real-valued function with parameter vector θ. We say that the
probability distribution of XG is a Fiedler random ﬁeld if it factorizes as

e. Then:

P (XG| θ) =

1

Z(θ)

exp
 XXuv ∈XG

ϕuv(cid:0)Xuv  ∆λ2(u  v  Guv); θ(cid:1)


(2)

3

where Z(θ) is the partition function.

In other words  a FRF is a Gibbs-Boltzmann distribution over graphs  with potential functions de-
ﬁned for each node pair {u  v} along with some neighboring subgraph Guv. In particular  in order
to model the dependence of each variable Xuv on Guv  potentials take as argument both the value of
Xuv and the Fiedler delta corresponding to {u  v} in Guv. The idea is to treat the Fiedler delta statis-
tic as a (real-valued) random variable deﬁned over subgraph conﬁgurations  and to exploit this ran-
dom variable as a compact representation of those conﬁgurations. This means that the dependence
structure of a FRF is ﬁxed by the particular choice of subgraphs Guv  so that the set XGuv \ {Xuv}
makes Xuv independent of XG \ XGuv . Three fundamental questions are then the following. First 
how do we ﬁx the subgraph Guv for each pair of nodes {u  v}? Second  how do we choose a shape
for the potential functions  so as to fully exploit the information contained in the Fiedler delta  while
avoiding unwarranted assumptions concerning their parametric form? Third  how does the Fiedler
delta statistic behave with respect to the Markov dependence property for random graphs? One basic
result related to the third question is presented in Sec. 3.2  while Sec. 3.3 will address the ﬁrst two
points.

3.2 Dependence structure

We ﬁrst recall the deﬁnition of Markov dependence for random graphs [10]. Let N (Xuv) denote
the set {Xwz: {w  z} ∈ E ∧ |{w  z} ∩ {u  v}| = 1}. Then:
Deﬁnition 3. A random graph G is said to be a Markov graph (or to have a Markov dependence
structure) if  for any pair of variables Xuv and Xwz in G such that {u  v} ∩ {w  z} = ∅  we have
that P (Xuv| Xwz  N (Xuv)) = P (Xuv| N (Xuv)).

Based on Def. 3  we say that the dependence structure of a random graph G is non-Markovian if 
for disjoint pairs of nodes {u  v} and {w  z}  it does not imply that P (Xuv| Xwz  N (Xuv)) =
P (Xuv| N (Xuv)) 
6=
P (Xuv| N (Xuv)). We can then prove the following proposition:
Proposition 3. There exist Fiedler random ﬁelds with non-Markovian dependence structure.

is consistent with the inequality P (Xuv| Xwz  N (Xuv))

it

i.e.

if

Proof sketch. Consider a graph G = (V  E) such that V = {u  v  w  z} and E =
{{u  v}  {v  w}  {w  z}  {u  z}}. The proof relies on the following result [6]:
if graphs G1
and G2 are  respectively  a path and a circuit of size n 
then λ2(G1) = 2 (1 − cos(π/n))
and λ2(G2) = 2 (1 − cos(2π/n)). Since adding exactly one edge to a path of size 4 can
yield a circuit of the same size  this property allows to derive analytic forms for the Fiedler
delta statistic in such graphs  showing that
there exist parameterizations of ϕuv such that
ϕuv(Xuv  ∆λ2(u  v  G); θ) 6= ϕuv(Xuv  ∆λ2(u  v  GS); θ). This means that the dependence struc-
ture of G is non-Markovian.1

Note that the proof of Prop. 3 can be straightforwardly generalized to the dependence between two
variables Xuv and Xwz in circuits/paths of arbitrary size n  since the expression used for the Fiedler
eigenvalues of such graphs holds for any n. This fact suggests that FRFs allow to model edge
correlations at virtually any distance within G  provided that each subgraph Guv is chosen in such a
way as to encompass the relevant correlation.

3.3 Model estimation

The problem of learning a FRF from an observed network can be split into the task of estimating
the potential functions once the network distribution has been factorized into a particular set of
subgraphs  and the task of factorizing the distribution through a suitable set of subgraphs  which
corresponds to estimating the dependence structure of the FRF. Here we focus on the problem of
learning the FRF potentials  while suggesting a heuristic way to ﬁx the dependence structure of the
model.

In order to estimate the FRF potentials  we need to specify on the one hand a suitable architecture
for such functions  and on the other hand the objective function that we want to optimize. As a

1For a complete proof  see the supplementary material.

4

preliminary step  we tested experimentally a variety of shapes for the potential functions. The tests
indicated the importance of avoiding limiting assumptions concerning the form of the potentials 
which motivated us to model them by a feed-forward multilayer perceptron (MLP)  due to its well-
known capabilities of approximating functions of arbitrary shape [12]. In particular  throughout
the applications described in this paper we use a simple MLP architecture with one hidden layer
and hyperbolic tangent activation functions. Therefore  our parameter vector θ simply consists of
the weights of the chosen MLP architecture. Notice that  as far as the estimation of potentials is
concerned  any regression model offering approximation capabilities analogous to the MLP family
could be used as well. Here  the only requirement is to avoid unwarranted prior assumptions with
respect to the shape of the potential functions. In this respect  we take our approach to be genuinely
nonparametric  since it does not require the parametric form of the target functions to be speciﬁed
a priori in order to estimate them accurately. Concerning instead the learning objective  the main
difﬁculty we want to avoid is the complexity of computing the partition function involved in the
Gibbs-Boltzmann distribution. The approach we adopt to this aim is to minimize a contrastive
If G = (V  E) is the network that we want to ﬁt our model to  and
divergence objective [13].
Guv = (Vuv  Euv) is a subgraph of G such that {u  v} ∈ Vuv  let G∗
uv denote the graph that we obtain

uv is the result of performing just one iteration
{xuv}; θ) predicted by our model. In other words  G∗
of Gibbs sampling on Xuv using θ  where the conﬁguration xGuv of Guv is used to initialize the
(single-step) Markov chain. Then  our goal is to minimize the function ℓCD(θ; G)  given by:

by resampling the value of Xuv in Guv according to the conditional distribution bP (Xuv| xGuv \
ℓCD(θ; G) = log

= XXuv ∈XG

exp
 XXuv ∈XG
(cid:26)ϕ(cid:0)x∗

− log bP (xG| θ)


uv); θ(cid:1)



uv  ∆λ2(u  v  G∗

ϕ(cid:0)x∗
uv); θ(cid:1) − ϕ(cid:0)xuv  ∆λ2(u  v  Guv); θ(cid:1)(cid:27)

1

Z(θ)

uv  ∆λ2(u  v  G∗

(3)

where ϕ is the function computed by our MLP architecture. The appeal of contrastive divergence
learning is that  while it does not require to compute the partition function  it is known to converge
to points which are very close to maximum-likelihood solutions [14].

If we want our learning objective to be usable in the large-scale setting  then it is not feasible to
sum over all node pairs {u  v} in the network  since the number of such pairs grows quadratically
with |V|. In this respect  a straightforward approach for scaling to very large networks consists in
sampling n objects from the set of all possible pairs of nodes  taking care that the sample contains a
good balance between linked and unlinked pairs. Another issue we need to address concerns the way
we sample a suitable set of subgraphs Gu1v1   . . .   Gun vn for the selected pairs of nodes. Although
different sampling techniques could be used in principle [15]  our goal is to model correlations
between each variable Xuv and some neighboring region Guv in G. Such a neighborhood should be
large enough to make ∆λ2(u  v  Guv) sufﬁciently informative with respect to the overall network  but
also small enough to keep the spectral decomposition of Guv computationally tractable. Therefore 
in order to sample Guv  we propose to draw Vuv by performing k ‘snowball waves’ on G [16]  using
u and v as seeds  and then setting Euv to be the edge set induced by Vuv in G (see Algorithm 1
for the details). In this way  we can empirically tune the k hyperparameter in order to trade-off the
informativeness of Guv for the tractability of its spectral decomposition  where it is known that the
complexity of computing ∆λ2(u  v  Guv) is cubic with respect to the number of nodes in Guv [17].

Algorithm 1 SampleSubgraph: Sampling a neighboring subgraph for a given node pair
Input: Undirected graph G = (V  E); node pair {u  v}; number k of snowball waves.
Output: Undirected graph Guv = (Vuv  Euv).

SampleSubgraph(G  {u  v}  k):
1. Vuv = {u  v}
2. for(i = 1 to k)
3.
4. Euv = {{w  z} ∈ E: {w  z} ⊆ Vuv}
5. return (Vuv  Euv)

Vuv = Vuv ∪Sw∈Vuv {z ∈ V: {w  z} ∈ E}

5

Once sampled our training set D = (cid:8)(xu1 v1  Gu1v1 )  . . .   (xun vn   Gunvn )(cid:9)  we learn the MLP

weights by minimizing the objective ℓCD(θ; D)  which which we obtain from ℓCD(θ; G) by re-
stricting the summation in Eq. 3 to the elements of D. Minimization is performed by iterative
gradient descent  using standard backpropagation for updating the MLP weights.

4 Experimental evaluation

trained on a data sample D = (cid:8)(xu1v1   Gu1v1 )  . . .   (xun vn   Gunvn )(cid:9)  where n ≪ |V| (|V|−1)

In order to investigate the empirical behavior of FRFs as models of large-scale networks  we design
two different groups of experiments (in link prediction and graph generation respectively)  using col-
laboration networks drawn from the arXiv e-print repository (http://snap.stanford.edu/
data/index.html)  where nodes represent scientists and edges represent paper coauthorships.
Some basic network statistics are reported in Table 1.
Link prediction.
In the ﬁrst kind of experiments  given a random network G = (V  E)  our
goal is to measure the accuracy of FRFs at estimating the conditional distribution of variables
Xuv given the conﬁguration of neighboring subgraphs Guv of G. This can be seen as a link
prediction problem where only local information (given by Guv) can be used for predicting the
presence of a link {u  v}. At the same time  we want to understand whether the overall net-
work size (in terms of the number of nodes) has an impact on the number of training examples
that will be necessary for FRFs to converge to stable prediction accuracy. Recall that FRFs are
.
Given this  converging to stable predictions for values of n which do not depend on |V| is a cru-
cial requirement for achieving large-scale applicability. Let us sample our training set D by ﬁrst
drawing n node pairs from V in such a way that linked and unlinked pairs from G are equally
represented in D  and then extracting the corresponding subgraphs Gui vi by Algorithm 1 using
one snowball wave. We then learn our model from D as described in Sec. 3.3.
In all the ex-
periments reported in this work  the number of hidden units in our MLP architecture is set to
5. A test set T containing m objects (xu1 v1   GS1)  . . .   (xumvm   GSm) is also sampled from G
so that T ∩ D = ∅  where pairs {ui  vi} in T are drawn uniformly at random from V × V.
Predictions are derived from the learned model
by ﬁrst computing the conditional probabil-
ity of observing a link for each pair of nodes
{uj  vj} in T   and then making a decision on
the presence/absence of links by thresholding
the predicted probability (where the threshold is
tuned by cross-validation). Prediction accuracy
is measured by averaging the recognition accu-
racy for linked and unlinked pairs in T respec-
tively (where |T | = 10  000). In Fig. 1  the ac-
curacy of FRFs on the test set is plotted against
a growing size n of D (where 12 ≤ n ≤ 48).
Interestingly  the number of training examples
required for the accuracy curve to stabilize does
not seem to depend at all on the overall network
size.
Indeed  fastest convergence is achieved
for the average-sized and the second largest
networks  i.e. HepPh and AstroPh respectively.
Notice how a training sample containing an extremely small percentage of node pairs is sufﬁcient
for our learning approach to converge to stable prediction accuracy. This result encourages to think
of FRFs as a convenient modeling option for the large-scale setting.

Figure 1: Prediction accuracy of FRFs on the
arXiv networks for a growing training set size.

GrQc (5 242 nodes)
HepTh (9 877 nodes)
HepPh (12 008 nodes)
AstroPh (18 772 nodes)
CondMat (23 133 nodes)

Training set size

i

i
t
c
d
e
r
P

t

e
s
 
t
s
e

t
 

n
o
 
y
c
a
r
u
c
c
a

 0.55

 0.45

 10

 

n
o

 0.65

 15

 20

 35

 40

 45

 50

 25

 30

 0.85

 0.8

 0.75

 0.7

 0.95

 0.9

2

 0.6

 0.5

Besides assessing whether the network size affects the number of training samples needed to accu-
rately learn FRFs  we want to evaluate the usefulness of the dependence structure involved in our
model in predicting the conditional distributions of edges given their neighboring subgraphs. That
is  we want to ascertain whether the effort of modeling the conditional independence structure of
the overall network through the FRF formalism is justiﬁed by a suitable gain in prediction accuracy
with respect to statistical models that do not focus explicitly on such dependence structure. To this
aim  we compare FRFs to two popular statistical models for large-scale networks  namely the Watts-
Strogatz (WS) and the Barab´asi-Albert (BA) models [3  2]. The WS formalism is mainly aimed

6

at modeling the short-diameter property often observed in real-world networks. Interestingly  the
degree distribution of WS networks can be expressed in closed form in terms of two parameters δ
and β  related to the average degree distribution and a network rewiring process respectively [18].
On the other hand  the BA model is aimed at explaining the emergence of power-law degree distri-
butions  where such distributions can be expressed in terms of an adaptive parameter α [19]. The
parameters of both the WS and the BA model can be estimated by standard maximum-likelihood
approaches and then used to predict conditional edge distributions  exploiting information from the
degrees observed in the given subgraphs [20  21]. The ER model is not considered in this group
of experiments  since the involved independence assumption makes it unusable (i.e. equivalent to
random guessing) for the purposes of conditional estimation tasks. On the other hand  ERG models
are not suitable for application to the large-scale setting. We tried them out using edge  k-star and
k-triangle statistics [4]  and the tests conﬁrmed this point. Although the prohibitive cost of ﬁtting the
models and computing the involved feature functions could be overcome in principle by sampling
strategies similar to the ones we employ for FRFs  the potentials used in ERGs become numerically
unstable in the large-scale setting  leading to numerical representation issues for which we are not
aware of any off-the-shelf solution. Accuracy values for the different models are reported in Ta-
ble 1. FRFs dramatically outperform the other two models on all networks. Since both the BA and
the WS model do not show relevant improvements over simple random guessing  this result clearly
suggests that exploiting the dependence structure involved in network edge conﬁgurations is crucial
to accurately predict the presence/absence of links.

Table 1: Edge prediction results on the arXiv networks. General network statistics are also reported 
where CCG and DG stand for average clustering coefﬁcient and network diameter respectively.

Dataset
AstroPh
CondMat
GrQc
HepPh
HepTh

|V|
18 772
23 133
5 242
12 008
9 877

Network Statistics

|E | CCG DG
14
15
17
13
17

0.63
0.63
0.52
0.61
0.47

396 160
186 936
28 980
237 010
51 971

Prediction Accuracy

BA

FRF

WS

50.98% 89.97% 50.14%
50.15% 91.62% 56.71%
52.57% 91.14% 53.72%
51.61% 86.57% 54.33%
58.33% 92.25% 50.30%

Graph generation. A second group of experiments is aimed at assessing whether the FRFs learned
on the arXiv networks can be considered as plausible models of the degree distribution (DD) and
the clustering coefﬁcient distribution (CC) observed in each network [15]. To this aim  we use the
estimated FRF models to generate artiﬁcial graphs of various size  using Gibbs sampling  and then
we compare the DD and CC observed in the artiﬁcial graphs with those estimated on the whole
networks. For scale-free networks such as the ones considered here  the BA model is known to be
the most accurate model currently available with respect to DD. On the other hand  for CC both BA
and WS are known to be more realistic models than ER random graphs. Therefore  we compare the
graphs generated by FRFs to those generated by the BA  ER  and WS models for the same networks.
The distance in DD and CC between the artiﬁcial graphs on the one hand and the corresponding real
network on the other hand is measured using the Kolmogorov-Smirnov D-statistic  following a
common use in graph mining research [15]. Here we only plot results for the CondMat and HepTh
networks  noticing that the results we collected on the other arXiv networks lend themselves to the
same interpretation as the ones displayed in Fig. 2. Values are averaged over 100 samples for each
considered graph size  where the standard deviation is typically in the order of 10−2. The outcome
motivates the following considerations. Concerning DD  FRFs are able to improve (at least slightly)
the accuracy of the state-of-the-art BA model  while they are very close that model with respect
to clustering coefﬁcient. In all cases  both BA and FRFs prove to be far more accurate than ER
or WS  where the only advantage of using WS is limited to improving CC over ER. These results
are particularly encouraging  since they show how the nonparametric approach motivating the FRF
model allows to accurately estimate network properties (such as DD) that are not aimed for explicitly
in the model design. This suggests that the Fiedler delta statistic is a promising direction for building
generative models capable of capturing different network properties through a uniﬁed approach.

7

D
D

 
r
o

f
 
c
i
t
s
i
t

a

t
s
-
D

D
D

 
r
o
f
 
c
i
t
s
i
t
a
t
s
-
D

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 40

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 40

BA
ER
FRF
WS

 60

 80

 100

 120

 140

 160

Artificial graph size

(b)

BA
ER
FRF
WS

BA
ER
FRF
WS

 60

 80

 100

 120

 140

 160

Artificial graph size

(a)

BA
ER
FRF
WS

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 40

 0.9

 0.8

 0.7

 0.6

 0.5

C
C

 
r
o

f
 
c
i
t
s
i
t

a

t
s
-
D

C
C

 
r
o
f
 
c
i
t
s
i
t
a
t
s
-
D

 60

 80

 100

 120

 140

 160

 0.4

 40

 60

 80

 100

 120

 140

 160

Artificial graph size

(c)

Artificial graph size

(d)

Figure 2: D-statistic values for DD and CC on the CondMat (a–b) and HepTh (c–d) networks.

5 Conclusions and future work

The main motivation inspiring this work was the observation that statistical modeling of networks
cries for genuinely nonparametric estimation  because of the inaccuracy often resulting from unwar-
ranted parametric assumptions. In this respect  we showed how the Fiedler delta statistic offers a
powerful building block for designing a nonparametric estimator  which we developed in the form
of the FRF model. Since here we only applied FRFs to collaboration networks  which are typically
scale-free  an important option for future work is to assess the ﬂexibility of FRFs in modeling net-
works from different families. In the second place  since we only addressed in a heuristic way the
problem of learning the dependence structure of FRFs  a stimulating direction for further research
consists in designing clever techniques for learning the structure of FRFs  e.g. considering the use
of alternative subgraph sampling techniques. Finally  we would like to assess the possibility of
modeling networks through mixtures of FRFs  so as to ﬁt different network regions (with possibly
conﬂicting properties) through specialized components of the mixture.

Acknowledgments

This work has been supported by the French National Research Agency (ANR-09-EMER-007). The
authors are grateful to Gemma Garriga  R´emi Gilleron  Liva Ralaivola  and Michal Valko for their
useful suggestions and comments.

References

[1] P. Erd˝os and A. R´enyi  “On Random Graphs  I ” Publicationes Mathematicae Debrecen  vol. 6 

pp. 290–297  1959.

[2] A.-L. Barab´asi and R. Albert  “Emergence of scaling in random networks ” Science  vol. 286 

pp. 509–512  1999.

8

[3] D. J. Watts and S. H. Strogatz  “Collective dynamics of ‘small-world’ networks ” Nature 

vol. 393  pp. 440–442  1998.

[4] T. A. B. Snijders  P. E. Pattison  G. L. Robins  and M. S. Handcock  “New Speciﬁcations for

Exponential Random Graph Models ” Sociological Methodology  vol. 36  pp. 99–153  2006.

[5] J. Leskovec  D. Chakrabarti  J. Kleinberg  C. Faloutsos  and Z. Ghahramani  “Kronecker
graphs: An approach to modeling networks ” Journal of Machine Learning Research  vol. 11 
pp. 985–1042  2010.

[6] M. Fiedler  “Algebraic connectivity of graphs ” Czechoslovak Mathematical Journal  vol. 23 

pp. 298–305  1973.

[7] B. Mohar  “The Laplacian Spectrum of Graphs ” in Graph Theory  Combinatorics  and Ap-
plications (Y. Alavi  G. Chartrand  O. R. Oellermann  and A. J. Schwenk  eds.)  pp. 871–898 
Wiley  1991.

[8] W. N. Anderson and T. D. Morley  “Eigenvalues of the Laplacian of a graph ” Linear and

Multilinear Algebra  vol. 18  pp. 141–145  1985.

[9] D. M. Cvetkovi´c  M. Doob  and H. Sachs  eds.  Spectra of Graphs: Theory and Application.

New York (NY): Academic Press  1979.

[10] O. Frank and D. Strauss  “Markov Graphs ” Journal of the American Statistical Association 

vol. 81  pp. 832–842  1986.

[11] J. Besag  “Spatial Interaction and the Statistical Analysis of Lattice Systems ” Journal of the

Royal Statistical Society. Series B  vol. 36  pp. 192–236  1974.

[12] K. Hornik  “Approximation capabilities of multilayer feedforward networks ” Neural Net-

works  vol. 4  no. 2  pp. 251–257  1991.

[13] G. E. Hinton  “Training Products of Experts by Minimizing Contrastive Divergence ” Neural

Computation  vol. 14  no. 8  pp. 1771–1800  2002.

[14] M. ´A. Carreira-Perpi˜n´an and G. E. Hinton  “On Contrastive Divergence Learning ” in Pro-
ceedings of the Tenth International Workshop on Articial Intelligence and Statistics (AISTATS
2005)  pp. 33–40  2005.

[15] J. Leskovec and C. Faloutsos  “Sampling from large graphs ” in Proceedings of the Twelfth
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD
2006)  pp. 631–636  2006.

[16] E. D. Kolaczyk  Statistical Analysis of Network Data. Methods and Models. New York (NY):

Springer  2009.

[17] Z. Bai  J. Demmel  J. Dongarra  A. Ruhe  and H. van der Vorst  eds.  Templates for the Solution

of Algebraic Eigenvalue Problems: A Practical Guide. Philadelphia (PA): SIAM  2000.

[18] A. Barrat and M. Weigt  “On the properties of small-world network models ” The European

Physical Journal B  vol. 13  pp. 547–560  2000.

[19] R. Albert and A.-L. Barab´asi  “Statistical mechanics of complex networks ” Reviews of Modern

Physics  vol. 74  pp. 47–97  2002.

[20] M. E. J. Newman  “Clustering and preferential attachment in growing networks ” Physical

Review E  vol. 64  p. 025102  2001.

[21] A. Barab´asi  H. Jeong  Z. N´eda  E. Ravasz  A. Schubert  and T. Vicsek  “Evolution of the social

network of scientic collaborations ” Physica A  vol. 311  pp. 590–614  2002.

9

,Yangqing Jia
Joshua Abbott
Joseph Austerweil
Tom Griffiths
Trevor Darrell
Peter Anderson
Stephen Gould
Mark Johnson
Bao Wang
Zuoqiang Shi
Stanley Osher