2013,Robust learning of low-dimensional dynamics from large neural ensembles,Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be fit efficiently and robustly. Here  we present an approach to dimensionality reduction for neural data that is convex  does not make strong assumptions about dynamics  does not require averaging over many trials and is extensible to more complex statistical models that combine local and global influences. The results can be combined with spectral methods to learn dynamical systems models. The basic method can be seen as an extension of PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered  and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally  we demonstrate improved prediction on real neural data from monkey motor cortex compared to fitting linear dynamical models without nuclear norm smoothing.,Robust learning of low-dimensional dynamics from

large neural ensembles

David Pfau

Liam Paninski

Eftychios A. Pnevmatikakis

Center for Theoretical Neuroscience

Department of Statistics

Grossman Center for the Statistics of Mind

Columbia University  New York  NY

pfau@neurotheory.columbia.edu

{eftychios liam}@stat.columbia.edu

Abstract

Recordings from large populations of neurons make it possible to search for hy-
pothesized low-dimensional dynamics. Finding these dynamics requires models
that take into account biophysical constraints and can be ﬁt efﬁciently and ro-
bustly. Here  we present an approach to dimensionality reduction for neural data
that is convex  does not make strong assumptions about dynamics  does not require
averaging over many trials and is extensible to more complex statistical models
that combine local and global inﬂuences. The results can be combined with spec-
tral methods to learn dynamical systems models. The basic method extends PCA
to the exponential family using nuclear norm minimization. We evaluate the effec-
tiveness of this method using an exact decomposition of the Bregman divergence
that is analogous to variance explained for PCA. We show on model data that
the parameters of latent linear dynamical systems can be recovered  and that even
if the dynamics are not stationary we can still recover the true latent subspace.
We also demonstrate an extension of nuclear norm minimization that can separate
sparse local connections from global latent dynamics. Finally  we demonstrate
improved prediction on real neural data from monkey motor cortex compared to
ﬁtting linear dynamical models without nuclear norm smoothing.

Introduction

1
Progress in neural recording technology has made it possible to record spikes from ever larger pop-
ulations of neurons [1]. Analysis of these large populations suggests that much of the activity can
be explained by simple population-level dynamics [2]. Typically  this low-dimensional activity is
extracted by principal component analysis (PCA) [3  4  5]  but in recent years a number of exten-
sions have been introduced in the neuroscience literature  including jPCA [6] and demixed principal
component analysis (dPCA) [7]. A downside of these methods is that they do not treat either the
discrete nature of spike data or the positivity of ﬁring rates in a statistically principled way. Standard
practice smooths the data substantially or averages it over many trials  losing information about ﬁne
temporal structure and inter-trial variability.
One alternative is to ﬁt a more complex statistical model directly from spike data  where temporal
dependencies are attributed to latent low dimensional dynamics [8  9]. Such models can account for
the discreteness of spikes by using point-process models for the observations  and can incorporate
temporal dependencies into the latent state model. State space models can include complex inter-
actions such as switching linear dynamics [10] and direct coupling between neurons [11]. These
methods have drawbacks too: they are typically ﬁt by approximate EM [12] or other methods that
are prone to local minima  the number of latent dimensions is typically chosen ahead of time  and a
certain class of possible dynamics must be chosen before doing dimensionality reduction.

1

In this paper we attempt to combine the computational tractability of PCA and related methods with
the statistical richness of state space models. Our approach is convex and based on recent advances
in system identiﬁcation using nuclear norm minimization [13  14  15]  a convex relaxation of matrix
rank minimization. Compared to recent work on spectral methods for ﬁtting state space models
[16]  our method more easily generalizes to handle different nonlinearities  non-Gaussian  non-
linear  and non-stationary latent dynamics  and direct connections between observed neurons. When
applied to model data  we ﬁnd that: (1) low-dimensional subspaces can be accurately recovered 
even when the dynamics are unknown and nonstationary (2) standard spectral methods can robustly
recover the parameters of state space models when applied to data projected into the recovered
subspace (3) the confounding effects of common input for inferring sparse synaptic connectivity can
be ameliorated by accounting for low-dimensional dynamics. In applications to real data we ﬁnd
comparable performance to models trained by EM with less computational overhead  particularly as
the number of latent dimensions grows.
The paper is organized as follows. In section 2 we introduce the class of models we aim to ﬁt 
which we call low-dimensional generalized linear models (LD-GLM). In section 3 we present a
convex formulation of the parameter learning problem for these models  as well as a generalization
of variance explained to LD-GLMs used for evaluating results. In section 4 we show how to ﬁt these
models using the alternating direction method of multipliers (ADMM). In section 5 we present
results on real and artiﬁcial neural datasets. We discuss the results and future directions in section 6.

2 Low dimensional generalized linear models
Our model is closely related to the generalized linear model (GLM) framework for neural data [17].
Unlike the standard GLM  where the inputs driving the neurons are observed  we assume that the
driving activity is unobserved  but lies on some low dimensional subspace. This can be a useful
way of capturing spontaneous activity  or accounting for strong correlations in large populations of
neurons. Thus  instead of ﬁtting a linear receptive ﬁeld  the goal of learning in low-dimensional
GLMs is to accurately recover the latent subspace of activity.
Let xt ∈ Rm be the value of the dynamics at time t. To turn this into spiking activity  we project
this into the space of neurons: yt = Cxt + b is a vector in Rn  n (cid:29) m  where each dimension of yt
corresponds to one neuron. C ∈ Rn×m denotes the subspace of the neural population and b ∈ Rn
the bias vector for all the neurons. As yt can take on negative values  we cannot use this directly as
a ﬁring rate  and so we pass each element of yt through some convex and log-concave increasing
point-wise nonlinearity f : R → R+. Popular choices for nonlinearities include f (x) = exp(x) and
f (x) = log(1 + exp(x)). To account for biophysical effects such as refractory periods  bursting  and
direct synaptic connections  we include a linear dependence on spike history before the nonlinearity.
The ﬁring rate f (yt) is used as the rate for some point process ξ such as a Poisson process to generate
a vector of spike counts st for all neurons at that time:

k(cid:88)

τ =1

yt = Cxt +
st ∼ ξ(f (yt))

Dτ st−τ + b

(1)

(2)

Much of this paper is focused on estimating yt  which is the natural parameter for the Poisson
distribution in the case f (·) = exp(·)  and so we refer to yt as the natural rate to avoid confusion
with the actual rate f (yt). We will see that our approach works with any point process with a
log-concave likelihood  not only Poisson processes.
We can extend this simple model by adding dynamics to the low-dimensional latent state  including
input-driven dynamics. In this case the model is closely related to the common input model used
in neuroscience [11]  the difference being that the observed input is added to xt rather than being
directly mapped to yt. The case without history terms and with linear Gaussian dynamics is a well-
studied state space model for neural data  usually ﬁt by EM [19  12  20]  though a consistent spectral
method has been derived [16] for the case f (·) = exp(·). Unlike these methods  our approach
largely decouples the problem of dimensionality reduction and learning dynamics: even in the case
of nonstationary  non-Gaussian dynamics where A  B and Cov[] change over time  we can still
robustly recover the latent subspace spanned by xt.

2

3 Learning
3.1 Nuclear norm minimization

Then rank(A(Y )) = m. Ideally we would minimize λnT rank(A(Y )) −(cid:80)T

In the case that the spike history terms D1:k are zero  the natural rate at time t is yt = Cxt + b  so all
yt are elements of some m-dimensional afﬁne space given by the span of the columns of C offset by
b. Ideally  our estimate of y1:T would trade off between making the dimension of this afﬁne space
as low as possible and the likelihood of y1:T as high as possible. Let Y = [y1  . . .   yT ] be the n × T
matrix of natural rates and let A(·) be the row mean centering operator A(Y ) = Y − 1
T .
T Y 1T 1T
t=1 log p(st|yt)  where
λ controls how much we trade off between a simple solution and the likelihood of the data  however
general rank minimization is a hard non convex problem. Instead we replace the matrix rank with
its convex envelope: the sum of singular values or nuclear norm (cid:107) · (cid:107)∗ [13]  which can be seen as
the analogue of the (cid:96)1 norm for vector sparsity. Our problem then becomes:

log p(st|yt)

(3)

nT||A(Y )||∗ − T(cid:88)

√
λ

min

Y

t=1

√

N(cid:88)

Since the log likelihood scales linearly with the size of the data  and the singular values scale with
the square root of the size  we also add a factor of
nT in front of the nuclear norm term. In the
examples in this paper  we assume spikes are drawn from a Poisson distribution:

log p(st|yt) =

sit log f (yit) − f (yit) − log sit!

(4)

However  this method can be used with any point process with a log-concave likelihood. This can be
viewed as a convex formulation of exponential family PCA [21  22] which does not ﬁx the number
of principal components ahead of time.

i=1

3.2 Stable principal component pursuit

The model above is appropriate for cases where the spike history terms Dτ are zero  that is the
observed data can entirely be described by some low-dimensional global dynamics. In real data
neurons exhibit history-dependent behavior like bursting and refractory periods. Moreover if the
recorded neurons are close to each other some may have direct synaptic connections. In this case
Dτ may have full column rank  so from Eq. 1 it is clear that yt is no longer restricted to a low-
dimensional afﬁne space. In most practical cases we expect Dτ to be sparse  since most neurons are
not connected to one another. In this case the natural rates matrix combines a low-rank term and a
sparse term  and we can minimize a convex function that trades off between the rank of one term via
the nuclear norm  the sparsity of another via the (cid:96)1 norm  and the data log likelihood:

√

min

Y D1:k L

λ

nT||A(L)||∗ + γ

k(cid:88)

k(cid:88)

||Dτ||1 − T(cid:88)

τ =1

t=1

T
n

log p(st|yt)

(5)

s.t. Y = L +

Dτ Sτ   with Sτ = [0n τ   s1  . . .   sT−τ ] 

τ =1

where 0n τ is a matrix of zeros of size n× τ  used to account for boundary effects. This is an exten-
sion of stable principal component pursuit [23]  which separates sparse and low-rank components
of a noise-corrupted matrix. Again to ensure that every term in the objective function of Eq. 5 has
roughly the same scaling O(nT ) we have multiplied each (cid:96)1 norm with T /n. One can also consider
the use of a group sparsity penalty where each group collects a speciﬁc synaptic weight across all
the k time lags.

3.3 Evaluation through Bregman divergence decomposition

We need a way to evaluate the model on held out data  without assuming a particular form for the
dynamics. As we recover a subspace spanned by the columns of Y rather than a single parameter 
this presents a challenge. One option is to compute the marginal likelihood of the data integrated

3

(cid:33)

Dτ st−τ + b

(6)

k(cid:88)

τ =1

onto the singular vectors. Then the divergence from the mean

q(cid:88)

i=1

y(q)
t

=

uiv(q)

it +

v(q)
t

= arg max

v

log p

st

k(cid:88)
(cid:32)

τ =1

i=1

uivit +

Dτ st−τ + b

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) q(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)y(q)
(cid:105)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)g(st)
(cid:105)

y(q−1)
t
y(0)
t

t

(cid:104)
(cid:104)

t

t

Here v(q)

is the projection of y(q)

explained by the qth dimension is given by(cid:80)
(cid:80)
Poisson noise g(x) = log(x) and F (x) =(cid:80)

t DF

t DF

over the entire subspace  but this is computationally difﬁcult. For the case of PCA  we can project
the held out data onto a subspace spanned by principal components and compute what fraction of
total variance is explained by this subspace. We extend this approach beyond the linear Gaussian
case by use of a generalized Pythagorean theorem.
For any exponential family with natural parameters θ  link function g  function F such that
∇F = g−1 and sufﬁcient statistic T   the log likelihood can be written as DF [θ||g(T (x))] − h(x) 
where D·[·||·] is a Bregman divergence [24]: DF [x||y] = F (x) − F (y) − (x − y)T∇F (y). Intu-
itively  the Bregman divergence between x and y is the difference between the value of F (x) and
the value of the best linear approximation around y. Bregman divergences obey a generalization
of the Pythagorean theorem: for any afﬁne set Ω and points x /∈ Ω and y ∈ Ω  it follows that
DF [x||y] = DF [x||ΠΩ(x)] + DF [ΠΩ(x)||y] where ΠΩ(x) = arg minω∈Ω DF [x||ω] is the projec-
tion of x onto Ω. In the case of squared error this is just a linear projection  and for the case of GLM
log likelihoods this is equivalent to maximum likelihood estimation when the natural parameters are
restricted to Ω.
Given a matrix of natural rates recovered from training data  we compute the fraction of Bregman
divergence explained by a sequence of subspaces as follows. Let ui be the ith singular vector of
the recovered natural rates. Let b be the mean natural rate  and let y(q)
be the maximum likelihood
natural rates restricted to the space spanned by u1  . . .   uq:

t

(7)

t

where y(0)
is the bias b plus the spike history terms. The sum of divergences explained over all q is
equal to one by virtue of the generalized Pythagorean theorem. For Gaussian noise g(x) = x and
2||x||2 and this is exactly the variance explained by each principal component  while for
F (x) = 1
i exp(xi). This decomposition is only exact if f = g−1
in Eq. 4  that is  if the nonlinearity is exponential. However  for other nonlinearities this may still be
a useful approximation  and gives us a principled way of evaluating the goodness of ﬁt of a learned
subspace.
4 Algorithms
Minimizing Eq. 3 and Eq. 5 is difﬁcult  because the nuclear and (cid:96)1 norm are not differentiable
everywhere. By using the alternating direction method of multipliers (ADMM)  we can turn these
problems into a sequence of tractable subproblems [25]. While not always the fastest method for
solving a particular problem  we use it for its simplicity and generality. We describe the algorithm
below  with more details in the supplemental materials.

4.1 Nuclear norm minimization

To ﬁnd the optimal Y we alternate between minimizing an augmented Lagrangian with respect to Y  
minimizing with respect to an auxiliary variable Z  and performing gradient ascent on a Lagrange
multiplier Λ. The augmented Lagrangian is

Lρ(Y  Z  Λ) = λ

√

nT||Z||∗ −(cid:88)

t

which is a smooth function of Y and can be minimized by Newton’s method. The gradient and
Hessian of Lρ with respect to Y at iteration k are

log p(st|yt) + (cid:104)Λ A(Y ) − Z(cid:105) +

||A(Y ) − Z||2

F

ρ
2

(8)

4

(cid:88)
(cid:88)

t

∇Y Lρ = −∇Y

log p(st|yt) + ρA(Y ) − AT (ρZk − Λk)

(9)

1
T

Y Lρ = −∇2
∇2

log p(st|yt) + ρInT − ρ

(1T ⊗ In)(1T ⊗ In)T

Y

(10)
where ⊗ is the Kronecker product. Note that the ﬁrst two terms of the Hessian are diagonal and
the third is low-rank  so the Newton step can be computed in O(nT ) time by using the Woodbury
matrix inversion lemma.
The minimum of Eq. 17 with respect to Z is given exactly by singular value thresholding:

t

(11)
where U ΣV T is the singular value decomposition of A(Yk+1) + Λk/ρ  and St(·) is the (pointwise)
soft thresholding operator St(x) = sgn(x)max(0 |x| − t). Finally  the update to Λ is a simple
gradient ascent step: Λk+1 = Λk + ρ(A(Yk+1) − Zk+1) where ρ is a step size that can be chosen.

nT /ρ(Σ)V T  

Zk+1 = USλ
√

4.2 Stable principal component pursuit

To extend ADMM to the problem in Eq. 5 we only need to add one extra step  taking the minimum
over the connectivity matrices with the other parameters held ﬁxed. To simplify the notation  we
group the connectivity matrices into a single matrix D = (D1  . . .   Dk)  and stack the different
time-shifted matrices of spike histories on top of one another to form a single spike history matrix
H. The objective then becomes

where we have substituted Y − DH for the variable L  and the augmented Lagrangian is

√

min
Y D

λ

nT||A(Y − DH)||∗ + γ

√
Lρ(Y  Z  D  Λ) = λ

nT||Z||∗ + γ

T
n

T
n

||D||1 −(cid:88)
||D||1 −(cid:88)

t

t

ρ
2

log p(st|yt)

log p(st|yt)

(12)

(13)

(14)

(15)

+(cid:104)Λ A(Y − DH) − Z(cid:105) +

||A(Y − DH) − Z||2

F

The updates for Λ and Z are almost unchanged  except that A(Y ) becomes A(Y − DH). Likewise
for Y the only change is one additional term in the gradient:

∇Y Lρ = −∇Y

log p(st|yt) + ρA(Y ) − AT (ρZ + ρA(DH) − Λ)

(cid:88)

t
Minimizing D requires solving:
T
n

arg min

γ

D

||D||1 +

ρ
2

||A(DH) + Z − A(Y ) − Λ/ρ||2

F

This objective has the same form as LASSO regression. We solve this using ADMM as well  but
any method for LASSO regression can be substituted.

5 Experiments
We demonstrate our method on a number of artiﬁcial datasets and one real dataset. First  we show
in the absence of spike history terms that the true low dimensional subspace can be recovered in
the limit of large data  even when the dynamics are nonstationary. Second  we show that spectral
methods can accurately recover the transition matrix when dynamics are linear. Third  we show
that local connectivity can be separated from low-dimensional common input. Lastly  we show that
nuclear-norm penalized subspace recovery leads to improved prediction on real neural data recorded
from macaque motor cortex.
Model data was generated with 8 latent dimension and 200 neurons  without any external input. For
linear dynamical systems  the transition matrix was sampled from a Gaussian distribution  and the

5

Figure 1: Recovering low-dimensional subspaces from nonstationary model data. While the subspace remains
the same  the dynamics switch between 5 different linear systems. Left top: one dimension of the latent
trajectory  switching from one set of dynamics to another (red line). Left middle: ﬁring rates of a subset of
neurons during the same switch. Left bottom: covariance between spike counts for different neurons during
each epoch of linear dynamics. Right top: Angle between the true subspace and top principal components
directly from spike data  from natural rates recovered by nuclear norm minimization  and from the true natural
rates. Right bottom: fraction of Bregman divergence explained by the top 1  5 or 10 dimensions from nuclear
norm minimization. Dotted lines are variance explained by the same number of principal components. For
λ < 0.1 the divergence explained by a given number of dimensions exceeds the variance explained by the
same number of PCs.

eigenvalues rescaled so the magnitude fell between .9 and .99 and the angle between ± π
10  yielding
slow and stable dynamics. The linear projection C was a random Gaussian matrix with standard
deviation 1/3  and the biases bi were sampled from N (−4  1)  which we found gave reasonable
ﬁring rates with nonlinearity f (x) = log(1 + exp(x)). To investigate the variance of our estimates 
we generated multiple trials of data with the same parameters but different innovations.
We ﬁrst sought to show that we could accurately recover the subspace in which the dynamics take
place even when those dynamics are not stationary. We split each trial into 5 epochs and in each
epoch resampled the transition matrix A and set the covariance of innovations t to QQT where Q
is a random Gaussian matrix. We performed nuclear norm minimization on data generated from
this model  varying the smoothing parameter λ from 10−3 to 10  and compared the subspace angle
between the top 8 principal components and the true matrix C. We repeated this over 10 trials to
compute the variance of our estimator. We found that when smoothing was optimized the recovered
subspace was signiﬁcantly closer to the true subspace than the top principal components taken di-
rectly from spike data. Increasing the amount of data from 1000 to 10000 time bins signiﬁcantly
reduced the average subspace angle at the optimal λ. The top PCs of the true natural rates Y   while
not spanning exactly the same space as C due to differences between the mean column and true bias
b  was still closer to the true subspace than the result of nuclear norm minimization.
We also computed the fraction of Bregman divergence explained by the sequence of spaces spanned
by successive principal components  solving Eq. 6 by Newton’s method. We did not ﬁnd a clear
drop at the true dimensionality of the subspace  but we did ﬁnd that a larger share of the divergence
could be explained by the top dimensions than by PCA directly on spikes. Results are presented in
Fig. 1.
To show that the parameters of a latent dynamical system can be recovered  we investigated the
performance of spectral methods on model data with linear Gaussian latent dynamics. As the model
is a linear dynamical system with GLM output  we call this a GLM-LDS model. After estimating
natural rates by nuclear norm minimization with λ = 0.01 on 10 trials of 10000 time bins with
unit-variance innovations t  we ﬁt the transition matrix A by subspace identiﬁcation (SSID) [26].
The transition matrix is only identiﬁable up to a change of coordinates  so we evaluated our ﬁt by
comparing the eigenvalues of the true and estimated A. Results are presented in Fig. 2. As expected 
SSID directly on spikes led to biased estimates of the transition. By contrast  SSID on the output of

6

1600170018001900200021002200230024002500−50050160017001800190020002100220023002400250051015202500.511.5λSubspace Angle  T = 1000  SpikesT = 10000  SpikesT = 1000  NNT = 10000  NNT = 1000  True YT = 10000  True Y1e−31e−21e−11e01e100.51λDivergence Explained  1 Dim5 Dim10 Dim1e−31e−21e−11e01e1(a)

(b)

(c)

Figure 2: Recovered eigenvalues for the transition matrix of a linear dynamical system from model neural data.
Black: true eigenvalues. Red: recovered eigenvalues. (2a) Eigenvalues recovered from the true natural rates.
(2b) Eigenvalues recovered from subspace identiﬁcation directly on spike counts. (2c) Eigenvalues recovered
from subspace identiﬁcation on the natural rates estimated by nuclear norm minimization.

nuclear norm minimization had little bias  and seemed to perform almost as well as SSID directly
on the true natural rates. We found that other methods for ﬁtting linear dynamical systems from the
estimated natural rates were biased  as was SSID on the result of nuclear norm minimization without
mean-centering (see the supplementary material for more details).
We incorporated spike history terms into our model data to see whether local connectivity and global
dynamics could be separated. Our model network consisted of 50 neurons  randomly connected with
95% sparsity  and synaptic weights sampled from a unit variance Gaussian. Data were sampled from
10000 time bins. The parameters λ and γ were both varied from 10−10 to 104. We found that we
could recover synaptic weights with an r2 up to .4 on this data by combining both a nuclear norm and
(cid:96)1 penalty  compared to at most .25 for an (cid:96)1 penalty alone  or 0.33 for a nuclear norm penalty alone.
Somewhat surprisingly  at the extreme of either no nuclear norm penalty or a dominant nuclear norm
penalty  increasing the (cid:96)1 penalty never improved estimation. This suggests that in a regime with
strong common inputs  some kind of correction is necessary not only for sparse penalties to achieve
optimal performance  but to achieve any improvement over maximum likelihood. It is also of interest
that the peak in r2 is near a sharp transition to total sparsity.

Figure 3: Connectivity matrices recovered by SPCP on model data. Left: r2 between true and recovered
synaptic weights across a range of parameters. The position in parameter space of the data to the right is
highlighted by the stars. Axes are on a log scale. Right: scatter plot of true versus recovered synaptic weights 
illustrating the effect of the nuclear norm term.

Finally  we demonstrated the utility of our method on real recordings from a large population of
neurons. The data consists of 125 well-isolated units from a multi-electrode recording in macaque
motor cortex while the animal was performing a pinball task in two dimensions. Previous studies on
this data [27] have shown that information about arm velocity can be reliably decoded. As the elec-
trodes are spaced far apart  we do not expect any direct connections between the units  and so leave
out the (cid:96)1 penalty term from the objective. We used 800 seconds of data binned every 100 ms for
training and 200 seconds for testing. We ﬁt linear dynamical systems by subspace identiﬁcation as in
Fig. 2  but as we did not have access to a “true” linear dynamical system for comparison  we evalu-
ated our model ﬁts by approximating the held out log likelihood by Laplace-Gaussian ﬁltering [28].

7

−0.2−0.100.10.20.80.850.90.9511.051.1Imaginary ComponentReal Component  TrueBest Empirical Estimate−0.2−0.100.10.20.80.850.90.9511.051.1Imaginary ComponentReal Component  TrueSSID−0.2−0.100.10.20.80.850.90.9511.051.1Imaginary ComponentReal Component  TrueNN+SSIDλγr2 for Synaptic Weights  1.00e−101.00e−071.00e−041.00e−011.00e+021.00e−101.00e−081.00e−061.00e−041.00e−021.00e+001.00e+021.00e+040.050.10.150.20.250.30.350.4−2−1012−1.5−1−0.500.511.5TrueRecoveredSynaptic Weights  Optimal−2−1012−1.5−1−0.500.511.5TrueRecoveredSynaptic Weights  Small λ−2−1012−1.5−1−0.500.511.5TrueRecoveredSynaptic Weights  Large λWe also ﬁt the GLM-LDS model by running ran-
domly initialized EM for 50 iterations for models
with up to 30 latent dimensions (beyond which train-
ing was prohibitively slow). We found that a strong
nuclear norm penalty improved prediction by several
hundred bits per second  and that fewer dimensions
were needed for optimal prediction as the nuclear
norm penalty was increased. The best ﬁt models pre-
dicted held out data nearly as well as models trained
via EM  even though nuclear norm minimization is
not directly maximizing the likelihood of a linear dy-
namical system.

Figure 4: Log likelihood of held out motor cortex
data versus number of latent dimensions for dif-
ferent latent linear dynamical systems. Prediction
improves as λ increases  until it is comparable to
EM.

6 Discussion
The method presented here has a number of straight-
forward extensions. If the dimensionality of the la-
tent state is greater than the dimensionality of the
data  for instance when there are long-range history
dependencies in a small population of neurons  we
would extend the natural rate matrix Y so that each
column contains multiple time steps of data. Y is then a block-Hankel matrix. Constructing the
block-Hankel matrix is also a linear operation  so the objective is still convex and can be efﬁciently
minimized [15]. If there are also observed inputs ut then the term inside the nuclear norm should
also include a projection orthogonal to the row space of the inputs. This could enable joint learning
of dynamics and receptive ﬁelds for small populations of neurons with high dimensional inputs.
Our model data results on connectivity inference have important implications for practitioners work-
ing with highly correlated data. GLM models with sparsity penalties have been used to infer connec-
tivity in real neural networks [29]  and in most cases these networks are only partially observed and
have large amounts of common input. We offer one promising route to removing the confounding
inﬂuence of unobserved correlated inputs  which explicitly models the common input rather than
conditioning on it [30].
It remains an open question what kinds of dynamics can be learned from the recovered natural
parameters. In this paper we have focused on linear systems  but nuclear norm minimization could
just as easily be combined with spectral methods for switching linear systems and general nonlinear
systems. We believe that the techniques presented here offer a powerful  extensible and robust
framework for extracting structure from neural activity.

Acknowledgments

Thanks to Zhang Liu  Michael C. Grant  Lars Buesing and Maneesh Sahani for helpful discussions 
and Nicho Hatsopoulos for providing data. This research was generously supported by an NSF
CAREER grant.

References
[1] I. H. Stevenson and K. P. Kording  “How advances in neural recording affect data analysis ” Nature

neuroscience  vol. 14  no. 2  pp. 139–142  2011.

[2] M. Okun  P. Yger  S. L. Marguet  F. Gerard-Mercier  A. Benucci  S. Katzner  L. Busse  M. Carandini  and
K. D. Harris  “Population rate dynamics and multineuron ﬁring patterns in sensory cortex ” The Journal
of Neuroscience  vol. 32  no. 48  pp. 17108–17119  2012.

[3] K. L. Briggman  H. D. I. Abarbanel  and W. B. Kristan  “Optical imaging of neuronal populations during

decision-making ” Science  vol. 307  no. 5711  pp. 896–901  2005.

[4] C. K. Machens  R. Romo  and C. D. Brody  “Functional  but not anatomical  separation of “what” and

“when” in prefrontal cortex ” The Journal of Neuroscience  vol. 30  no. 1  pp. 350–360  2010.

[5] M. Stopfer  V. Jayaraman  and G. Laurent  “Intensity versus identity coding in an olfactory system ”

Neuron  vol. 39  no. 6  pp. 991–1004  2003.

8

05101520253035404550−2000−1500−1000−500Number of Latent DimensionsLog Likelihood (bits/s)Prediction of Held out Data from GLM−LDS  λ = 1.00e−04λ = 1.00e−03λ = 1.00e−02λ = 3.16e−02EM[6] M. M. Churchland  J. P. Cunningham  M. T. Kaufman  J. D. Foster  P. Nuyujukian  S. I. Ryu  and K. V.

Shenoy  “Neural population dynamics during reaching ” Nature  2012.

[7] W. Brendel  R. Romo  and C. K. Machens  “Demixed principal component analysis ” Advances in Neural

Information Processing Systems  vol. 24  pp. 1–9  2011.

[8] L. Paninski  Y. Ahmadian  D. G. Ferreira  S. Koyama  K. R. Rad  M. Vidne  J. Vogelstein  and W. Wu  “A
new look at state-space models for neural data ” Journal of Computational Neuroscience  vol. 29  no. 1-2 
pp. 107–126  2010.

[9] B. M. Yu  J. P. Cunningham  G. Santhanam  S. I. Ryu  K. V. Shenoy  and M. Sahani  “Gaussian-process
factor analysis for low-dimensional single-trial analysis of neural population activity ” Journal of neuro-
physiology  vol. 102  no. 1  pp. 614–635  2009.

[10] B. Petreska  B. M. Yu  J. P. Cunningham  G. Santhanam  S. I. Ryu  K. V. Shenoy  and M. Sahani  “Dynam-
ical segmentation of single trials from population neural data ” Advances in neural information processing
systems  vol. 24  2011.

[11] J. E. Kulkarni and L. Paninski  “Common-input models for multiple neural spike-train data ” Network:

Computation in Neural Systems  vol. 18  no. 4  pp. 375–407  2007.

[12] A. Smith and E. Brown  “Estimating a state-space model from point process observations ” Neural Com-

putation  vol. 15  no. 5  pp. 965–991  2003.

[13] M. Fazel  H. Hindi  and S. P. Boyd  “A rank minimization heuristic with application to minimum order
system approximation ” Proceedings of the American Control Conference.  vol. 6  pp. 4734–4739  2001.
[14] Z. Liu and L. Vandenberghe  “Interior-point method for nuclear norm approximation with application to
system identiﬁcation ” SIAM Journal on Matrix Analysis and Applications  vol. 31  pp. 1235–1256  2009.
[15] Z. Liu  A. Hansson  and L. Vandenberghe  “Nuclear norm system identiﬁcation with missing inputs and

outputs ” Systems & Control Letters  vol. 62  no. 8  pp. 605–612  2013.

[16] L. Buesing  J. Macke  and M. Sahani  “Spectral learning of linear dynamics from generalised-linear obser-
vations with application to neural population data ” Advances in neural information processing systems 
vol. 25  2012.

[17] L. Paninski  J. Pillow  and E. Simoncelli  “Maximum likelihood estimation of a stochastic integrate-and-

ﬁre neural encoding model ” Neural computation  vol. 16  no. 12  pp. 2533–2561  2004.

[18] E. Chornoboy  L. Schramm  and A. Karr  “Maximum likelihood identiﬁcation of neural point process

systems ” Biological cybernetics  vol. 59  no. 4-5  pp. 265–275  1988.

[19] J. Macke  J. Cunningham  M. Byron  K. Shenoy  and M. Sahani  “Empirical models of spiking in neural

populations ” Advances in neural information processing systems  vol. 24  2011.

[20] M. Collins  S. Dasgupta  and R. E. Schapire  “A generalization of principal component analysis to the

exponential family ” Advances in neural information processing systems  vol. 14  2001.

[21] V. Solo and S. A. Pasha  “Point-process principal components analysis via geometric optimization ” Neu-

ral Computation  vol. 25  no. 1  pp. 101–122  2013.

[22] Z. Zhou  X. Li  J. Wright  E. Candes  and Y. Ma  “Stable principal component pursuit ” Proceedings of

the IEEE International Symposium on Information Theory  pp. 1518–1522  2010.

[23] A. Banerjee  S. Merugu  I. S. Dhillon  and J. Ghosh  “Clustering with Bregman divergences ” The Journal

of Machine Learning Research  vol. 6  pp. 1705–1749  2005.

[24] S. P. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein  “Distributed optimization and statistical learn-
ing via the alternating direction method of multipliers ” Foundations and Trends R(cid:13) in Machine Learning 
vol. 3  no. 1  pp. 1–122  2011.

[25] P. Van Overschee and B. De Moor  “Subspace identiﬁcation for linear systems: theory  implementation 

applications ” 1996.

[26] V. Lawhern  W. Wu  N. Hatsopoulos  and L. Paninski  “Population decoding of motor cortical activity
using a generalized linear model with hidden states ” Journal of neuroscience methods  vol. 189  no. 2 
pp. 267–280  2010.

[27] S. Koyama  L. Castellanos P´erez-Bolde  C. R. Shalizi  and R. E. Kass  “Approximate methods for state-

space models ” Journal of the American Statistical Association  vol. 105  no. 489  pp. 170–180  2010.

[28] J. W. Pillow  J. Shlens  L. Paninski  A. Sher  A. M. Litke  E. Chichilnisky  and E. P. Simoncelli  “Spatio-
temporal correlations and visual signalling in a complete neuronal population ” Nature  vol. 454  no. 7207 
pp. 995–999  2008.

[29] M. Harrison  “Conditional inference for learning the network structure of cortical microcircuits ” in 2012

Joint Statistical Meeting  (San Diego  CA)  2012.

9

,David Pfau
Eftychios Pnevmatikakis
Liam Paninski