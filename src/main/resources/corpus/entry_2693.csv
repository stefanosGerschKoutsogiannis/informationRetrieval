2015,Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm,We consider the problem of recovering a low-rank tensor from its noisy observation. Previous work has shown a recovery guarantee with signal to noise ratio $O(n^{\ceil{K/2}/2})$ for recovering a $K$th order rank one tensor of size $n\times \cdots \times n$ by recursive unfolding. In this paper  we first improve this bound to $O(n^{K/4})$ by a much simpler approach  but with a more careful analysis. Then we propose a new norm called the \textit{subspace} norm  which is based on the Kronecker products of factors obtained by the proposed simple estimator. The imposed Kronecker structure allows us to show a nearly ideal $O(\sqrt{n}+\sqrt{H^{K-1}})$ bound  in which the parameter $H$ controls the blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore  we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with $H=O(1)$.,Interpolating Convex and Non-Convex Tensor

Decompositions via the Subspace Norm

Qinqing Zheng

University of Chicago

qinqing@cs.uchicago.edu

Ryota Tomioka

Toyota Technological Institute at Chicago

tomioka@ttic.edu

Abstract

We consider the problem of recovering a low-rank tensor from its noisy obser-
vation. Previous work has shown a recovery guarantee with signal to noise ratio
O(n(cid:100)K/2(cid:101)/2) for recovering a Kth order rank one tensor of size n × ··· × n by
recursive unfolding. In this paper  we ﬁrst improve this bound to O(nK/4) by a
much simpler approach  but with a more careful analysis. Then we propose a new
norm called the subspace norm  which is based on the Kronecker products of fac-
tors obtained by the proposed simple estimator. The imposed Kronecker structure
√
H K−1) bound  in which the parameter
allows us to show a nearly ideal O(
H controls the blend from the non-convex estimator to mode-wise nuclear norm
minimization. Furthermore  we empirically demonstrate that the subspace norm
achieves the nearly ideal denoising performance even with H = O(1).

n +

√

1

Introduction

Tensor is a natural way to express higher order interactions for a variety of data and tensor decom-
position has been successfully applied to wide areas ranging from chemometrics  signal processing 
to neuroimaging; see [15  18] for a survey. Moreover  recently it has become an active area in the
context of learning latent variable models [3].
Many problems related to tensors  such as  ﬁnding the rank  or a best rank-one approaximation of
a tensor is known to be NP hard [11  8]. Nevertheless we can address statistical problems  such as 
how well we can recover a low-rank tensor from its randomly corrupted version (tensor denoising)
or from partial observations (tensor completion). Since we can convert a tensor into a matrix by
an operation known as unfolding  recent work [25  19  20  13] has shown that we do get nontrivial
guarantees by using some norms or singular value decompositions. More speciﬁcally  Richard &
Montanari [20] has shown that when a rank-one Kth order tensor of size n × ··· × n is corrupted
noise ratio β/σ (cid:37) n(cid:100)K/2(cid:101)/2 by a method called the recursive unfolding1. Note that β/σ (cid:37) √
by standard Gaussian noise  a nontrivial bound can be shown with high probability if the signal to
n
is sufﬁcient for matrices (K = 2) and also for tensors if we use the best rank-one approximation
(which is known to be NP hard) as an estimator. On the other hand  Jain & Oh [13] analyzed the
tensor completion problem and proposed an algorithm that requires O(n3/2·polylog(n)) samples for
K = 3; while information theoretically we need at least Ω(n) samples and the intractable maximum
likelihood estimator would require O(n · polylog(n)) samples. Therefore  in both settings  there is
a wide gap between the ideal estimator and current polynomial time algorithms. A subtle question
that we will address in this paper is whether we need to unfold the tensor so that the resulting matrix
become as square as possible  which was the reasoning underlying both [19  20].
As a parallel development  non-convex estimators based on alternating minimization or nonlinear
optimization [1  21] have been widely applied and have performed very well when appropriately

1We say an (cid:37) bn if there is a constant C > 0 such that an ≥ C · bn.

1

Table 1: Comparison of required signal-to-noise ratio β/σ of different algorithms for recovering
a Kth order rank one tensor of size n × ··· × n contaminated by Gaussian noise with Standard
deviation σ. See model (2). The bound for the ordinary unfolding is shown in Corollary 1. The
bound for the subspace norm is shown in Theorem 2. The ideal estimator is proven in Appendix A.

Overlapped/
Latent nuclear
norm[23]
O(n(K−1)/2) O(n(cid:100)K/2(cid:101)/2) O(nK/4)

Recursive
unfolding[20]/
square
norm[19]

Ordinary un-
folding

Subspace norm (pro-
posed)

Ideal

√

n +

O(

√

H K−1) O((cid:112)nK log(K))

set up. Therefore it would be of fundamental importance to connect the wisdom of non-convex
estimators with the more theoretically motivated estimators that recently emerged.
In this paper  we explore such a connection by deﬁning a new norm based on Kronecker products of
factors that can be obtained by simple mode-wise singular value decomposition (SVD) of unfoldings
(see notation section below)  also known as the higher-order singular value decomposition (HOSVD)
[6  7]. We ﬁrst study the non-asymptotic behavior of the leading singular vector from the ordinary
(rectangular) unfolding X (k) and show a nontrivial bound for signal to noise ratio β/σ (cid:37) nK/4.
Thus the result also applies to odd order tensors conﬁrming a conjecture in [20]. Furthermore  this
motivates us to use the solution of mode-wise truncated SVDs to construct a new norm. We propose
the subspace norm  which predicts an unknown low-rank tensor as a mixture of K low-rank tensors 
in which each term takes the form

foldk(M (k)((cid:98)P

(1) ⊗ ··· ⊗ (cid:98)P

(k−1) ⊗ (cid:98)P

(k+1) ⊗ ··· ⊗ (cid:98)P

where foldk is the inverse of unfolding (·)(k)  ⊗ denotes the Kronecker product  and (cid:98)P
sufﬁciently high signal-to-noise ratio the estimated (cid:98)P

(k) ∈ Rn×H
is a orthonormal matrix estimated from the mode-k unfolding of the observed tensor  for k =
1  . . .   K; H is a user-deﬁned parameter  and M (k) ∈ Rn×HK−1. Our theory tells us that with

(k) spans the true factors.

(K)

)(cid:62)) 

We highlight our contributions below:

1. We prove that the required signal-to-noise ratio for recovering a Kth order rank one tensor from
the ordinary unfolding is O(nK/4). Our analysis shows a curious two phase behavior: with high
probability  when nK/4 (cid:45) β/σ (cid:45) nK/2  the error shows a fast decay as 1/β4; for β/σ (cid:37) nK/2  the
error decays slowly as 1/β2. We conﬁrm this in a numerical simulation.
2. The proposed subspace norm is an interpolation between the intractable estimators that directly
control the rank (e.g.  HOSVD) and the tractable norm-based estimators. It becomes equivalent to
the latent trace norm [23] when H = n at the cost of increased signal-to-noise ratio threshold (see
Table 1).
3. The proposed estimator is more efﬁcient than previously proposed norm based estimators  because
the size of the SVD required in the algorithm is reduced from n × nK−1 to n × H K−1.
4. We also empirically demonstrate that the proposed subspace norm performs nearly optimally for
constant order H.

Notation
Let X ∈ Rn1×n2×···×nK be a Kth order tensor. We will often use n1 = ··· = nK = n to
simplify the notation but all the results in this paper generalizes to general dimensions. The inner
product between a pair of tensors is deﬁned as the inner products of them as vectors; i.e.  (cid:104)X  W(cid:105) =
(cid:104)vec(X )  vec(W)(cid:105). For u ∈ Rn1  v ∈ Rn2  w ∈ Rn3  u ◦ v ◦ w denotes the n1 × n2 × n3
rank-one tensor whose i  j  k entry is uivjwk. The rank of X is the minimum number of rank-one
tensors required to write X as a linear combination of them. A mode-k ﬁber of tensor X is an nk
dimensional vector that is obtained by ﬁxing all but the kth index of X . The mode-k unfolding X (k)
k(cid:48)(cid:54)=k nk(cid:48) matrix constructed by concatenating all the mode-k ﬁbers along
columns. We denote the spectral and Frobenius norms for matrices by (cid:107) · (cid:107) and (cid:107) · (cid:107)F   respectively.

of tensor X is an nk ×(cid:81)

2

2 The power of ordinary unfolding

2.1 A perturbation bound for the left singular vector
We ﬁrst establish a bound on recovering the left singular vector of a rank-one n × m matrix (with
m > n) perturbed by random Gaussian noise.
Consider the following model known as the information plus noise model [4]:

˜X = βuv(cid:62) + σE 

(1)

where u and v are unit vectors  β is the signal strength  σ is the noise standard deviation  and
the noise matrix E is assumed to be random with entries sampled i.i.d. from the standard normal
distribution. Our goal is to lower-bound the correlation between u and the top left singular vector ˆu
of ˜X for signal-to-noise ratio β/σ (cid:37) (mn)1/4 with high probability.
A direct application of the classic Wedin perturbation theorem [28] to the rectangular matrix ˜X does
not provide us the desired result. This is because it requires the signal to noise ratio β/σ ≥ 2(cid:107)E(cid:107).
√
m) [27]  this would mean that we require β/σ (cid:37)
Since the spectral norm of E scales as Op(
m1/2; i.e.  the threshold is dominated by the number of columns m  if m ≥ n.
Alternatively  we can view ˆu as the leading eigenvector of ˜X ˜X
is that we can decompose ˜X ˜X

  a square matrix. Our key insight

as follows:

n +

√

(cid:62)

(cid:62)

(cid:62)

˜X ˜X

= (β2uu(cid:62) + mσ2I) + (σ2EE(cid:62) − mσ2I) + βσ(uv(cid:62)E(cid:62) + Evu(cid:62)).

Note that u is the leading eigenvector of the ﬁrst term because adding an identity matrix does not
change the eigenvectors. Moreover  we notice that there are two noise terms: the ﬁrst term is a
centered Wishart matrix and it is independent of the signal β; the second term is Gaussian distributed
and depends on the signal β.
This implies a two-phase behavior corresponding to either the Wishart or the Gaussian noise term
being dominant  depending on the value of β. Interestingly  we get a different speed of convergence
for each of these phases as we show in the next theorem (the proof is given in Appendix D.1).
Theorem 1. There exists a constant C such that with probability at least 1 − 4e−n  if m/n ≥ C 

|(cid:104)ˆu  u(cid:105)| ≥

1 − Cnm

(β/σ)4  

1 − Cn
(β/σ)2  
(β/σ)2 if β/σ ≥ √

Cn.

≥ (Cnm)

1
4  

√

if

if β
σ

m >

≥ √

β
σ

m 

otherwise  |(cid:104)ˆu  u(cid:105)| ≥ 1 − Cn

In other words  if ˜X has sufﬁciently many more columns than rows  as the signal to noise ratio β/σ
increases  ˆu ﬁrst converges to u as 1/β4  and then as 1/β2. Figure 1(a) illustrates these results.
We randomly generate a rank-one 100 × 10000 matrix perturbed by Gaussian noise  and measure
the distance between ˆu and u. The phase transition happens at β/σ = (nm)1/4  and there are two
regimes of different convergence rates as Theorem 1 predicts.

2.2 Tensor Unfolding

Now let’s apply the above result to the tensor version of information plus noise model studied by
[20]. We consider a rank one n×···× n tensor (signal) contaminated by Gaussian noise as follows:

Y = X ∗ + σE = βu(1) ◦ ··· ◦ u(K) + σE 

(2)
where factors u(k) ∈ Rn  k = 1  . . .   K  are unit vectors  which are not necessarily identical  and
the entries of E ∈ Rn×···×n are i.i.d samples from the normal distribution N (0  1). Note that this is
slightly more general (and easier to analyze) than the symmetric setting studied by [20].

3

(a) Synthetic experiment showing phase transition
at β/σ = (nm)1/4 and regimes with different rates
of convergence. See Theorem 1.

at β = σ((cid:81)

Corollary 1.

(b) Synthetic experiment showing phase transition
k nk)1/4 for odd order tensors. See

Figure 1: Numerical demonstration of Theorem 1 and Corollary 1.

σ

 

||| ˆX − X ∗|||F /β ≤ Op

Several estimators for recovering X ∗ from its noisy version Y have been proposed (see Table 1).
Both the overlapped nuclear norm and latent nuclear norm discussed in [23] achives the relative
performance guarantee

(cid:16)
(3)
where ˆX is the estimator. This bound implies that if we want to obtain relative error smaller than ε 
we need the signal to noise ratio β/σ to scale as β/σ (cid:37) √
Mu et al. [19] proposed the square norm  deﬁned as the nuclear norm of the matrix obtained by
grouping the ﬁrst (cid:98)K/2(cid:99) indices along the rows and the last (cid:100)K/2(cid:101) indices along the columns.
n(cid:100)K/2(cid:101)/β)  which translates to
requiring β/σ (cid:37) √
This norm improves the right hand side of inequality (3) to Op(σ
n(cid:100)K/2(cid:101)/ε for obtaining relative error ε. The intuition here is the more square the

nK−1/ε.

nK−1/β

(cid:17)

√

√

√

√
n/ε2  nK/2)

unfolding is the better the bound becomes. However  there is no improvement for K = 3.
unfolding algorithm achieves the factor recovery error dist(ˆu(k)  u(k)) = ε with β/σ (cid:37) √
Richard and Montanari [20] studied the (symmetric version of) model (2) and proved that a recursive
n(cid:100)K/2(cid:101)/ε
with high probability  where dist(u  u(cid:48)) := min((cid:107)u − u(cid:48)(cid:107) (cid:107)u + u(cid:48)(cid:107)). They also showed that the
randomly initialized tensor power method [7  16  3] can achieve the same error ε with slightly worse
threshold β/σ (cid:37) max(
The reasoning underlying both [19] and [20] is that square unfolding is better. However  if we take
the (ordinary) mode-k unfolding

Y (k) = βu(k)(cid:0)u(k−1) ⊗ ··· ⊗ u(1) ⊗ u(K) ⊗ ··· ⊗ u(k+1)(cid:1)(cid:62)

(4)
we can see (4) as an instance of information plus noise model (1) where m/n = nK−2. Thus the
ordinary unfolding satisﬁes the condition of Theorem 1 for n or K large enough.
Corollary 1. Consider a K(≥ 3)th order rank one tensor contaminated by Gaussian noise as in
(2). There exists a constant C such that if nK−2 ≥ C  with probability at least 1− 4Ke−n  we have

K log K also with high probability.

+ σE(k) 



dist2(ˆu(k)  u(k)) ≤

2CnK
(β/σ)4  
2Cn
(β/σ)2  

if n

K−1

2 > β/σ ≥ C

1
4 n

K
4  

if β/σ ≥ n

K−1

2

 

for k = 1  . . .   K 

where ˆu(k) is the leading left singular vector of the rectangular unfolding Y (k).
This proves that as conjectured by [20]  the threshold β/σ (cid:37) nK/4 applies not only to the even
order case but also to the odd order case. Note that Hopkins et al.
[10] have shown a similar
result without the sharp rate of convergence. The above corollary easily extends to more general
n1 × ··· × nK tensor by replacing the conditions by
k=1 nk)1/4 and

(cid:113)(cid:81)
(cid:96)(cid:54)=k n(cid:96) > β/σ ≥ (C(cid:81)K

(cid:96)(cid:54)=k n(cid:96). The result also holds when X ∗ has rank higher than 1; see Appendix E.

β/σ ≥(cid:113)(cid:81)

4

log(β/σ)23456-10-8-6-4-20n=100 m=10000log(1−|hu ˆui|)−3.81log(β/α)+12.76−2.38log(β/α)+6.18log1(nm)1/42log(√m)σ(Qini)14051015202530correlation00.20.40.60.81|hu1 bu1i|-[204060]|hu2 bu2i|-[204060]|hu1 bu1i|-[4080120]|hu2 bu2i|-[4080120]β1β2We demonstrate this result in Figure 1(b). The models behind the experiment are slightly more
general ones in which [n1  n2  n3] = [20  40  60] or [40  80  120] and the signal X ∗ is rank two with
β1 = 20 and β2 = 10. The plot shows the inner products (cid:104)u(1)
2 (cid:105) as a measure
of the quality of estimating the two mode-1 factors. The horizontal axis is the normalized noise
k=1 nk)1/4. We can clearly see that the inner product decays symmetrically

standard deviation σ((cid:81)K

1 (cid:105) and (cid:104)u(1)

2   ˆu(1)

1   ˆu(1)

around β1 and β2 as predicted by Corollary 1 for both tensors.

◦ ··· ◦ u(K)

3 Subspace norm for tensors
Suppose the true tensor X ∗ ∈ Rn×···×n admits a minimum Tucker decomposition [26] of rank
(R  . . .   R):

(5)
If the core tensor C = (βi1...iK ) ∈ RR×···×R is superdiagonal  the above decomposition reduces to
the canonical polyadic (CP) decomposition [9  15]. The mode-k unfolding of the true tensor X ∗ can
be written as follows:

X ∗ =(cid:80)R
i1=1 ···(cid:80)R
U (1) ⊗ ··· ⊗ U (k−1) ⊗ U (k+1) ⊗ ··· ⊗ U (K)(cid:17)(cid:62)
(cid:16)

(6)
where C(k) is the mode-k unfolding of the core tensor C; U (k) is a n × R matrix U (k) =
[u(k)
Let X∗

R ] for k = 1  . . .   K. Note that U (k) is not necessarily orthogonal.

be the SVD of X∗

iK =1 βi1i2...iK u(1)

P (1) ⊗ ··· ⊗ P (k−1) ⊗ P (k+1) ⊗ ··· ⊗ P (K)(cid:17)

(k). We will observe that

(k) = P (k)Λ(k)Q(k)(cid:62)
Q(k) ∈ Span

(k) = U (k)C(k)

1   . . .   u(k)

X∗

(cid:16)

.

iK

(7)

i1

 

because of (6) and U (k) ∈ Span(P (k)).
Corollary 1 shows that the left singular vectors P (k) can be recovered under mild conditions; thus
the span of the right singular vectors can also be recovered. Inspired by this  we deﬁne a norm that
models a tensor X as a mixture of tensors Z (1)  . . .  Z (K). We require that the mode-k unfolding
of Z (k)  i.e. Z(k)
  where M (k) ∈ Rn×HK−1 is a
variable  and S(k) ∈ RnK−1×HK−1 is a ﬁxed arbitrary orthonormal basis of some subspace  which
we choose later to have the Kronecker structure in (7).
In the following  we deﬁne the subspace norm  suggest an approach to construct the right factor
S(k)  and prove the denoising bound in the end.

(k)  has a low rank factorization Z(k)

(k) = M (k)S(k)(cid:62)

3.1 The subspace norm
Consider a Kth order tensor of size n × ··· n.
Deﬁnition 1. Let S(1)  . . .   S(K) be matrices such that S(k) ∈ RnK−1×HK−1 with H ≤ n. The
subspace norm for a Kth order tensor X associated with {S(k)}K

k=1 is deﬁned as
if X ∈ Span({S(k)}K
otherwise 

k=1) 

:= (cid:8)X ∈ Rn×···×n

:

(cid:40)
∃M (1)  . . .   M (K) X =(cid:80)K

|||X|||s :=

inf{M (k)}K
+∞ 

k=1

(cid:80)K
k=1 (cid:107)M (k)(cid:107)∗ 

where (cid:107) · (cid:107)∗ is the nuclear norm  and Span({S(k)}K

k=1)

k=1 foldk(M (k)S(k)(cid:62)

)(cid:9).

√
In the next lemma (proven in Appendix D.2)  we show the dual norm of the subspace norm has a sim-
nK−1) scaling (see the ﬁrst column
ple appealing form. As we see in Theorem 2  it avoids the O(
of Table 1) by restricting the inﬂuence of the noise term in the subspace deﬁned by S(1)  . . .   S(K).
Lemma 1. The dual norm of |||·|||s is a semi-norm
|||X|||s∗ = max

(cid:107)X (k)S(k)(cid:107) 

k=1 ... K

where (cid:107) · (cid:107) is the spectral norm.

5

3.2 Choosing the subspace

(k) = P (k)Λ(k)Q(k) be the SVD of X∗

A natural question that arises is how to choose the matrices S(1)  . . .   S(k).
Lemma 2. Let the X∗
nK−1 × R. Assume that R ≤ n and U (k) has full column rank. It holds that for all k 
i) U (k) ∈ Span(P (k)) 
ii) Q(k) ∈ Span

P (1) ⊗ ··· ⊗ P (k−1) ⊗ P (k+1) ⊗ ··· ⊗ P (K)(cid:17)

(cid:16)

.

(k)  where P (k) is n × R and Q(k) is

Proof. We prove the lemma in Appendix D.4.

Corollary 1 shows that when the signal to noise ratio is high enough  we can recover P (k) with high
probability. Hence we suggest the following three-step approach for tensor denoising:

(i) For each k  unfold the observation tensor in mode k and compute the top H left singular

vectors. Concatenate these vectors to obtain a n × H matrix (cid:98)P
(ii) Construct S(k) as S(k) = (cid:98)P

(1) ⊗ ··· ⊗ (cid:98)P

(k−1) ⊗ (cid:98)P

(iii) Solve the subspace norm regularized minimization problem

(k).

(k+1) ⊗ ··· ⊗ (cid:98)P
F + λ|||X|||s 

(K).

minX

|||Y − X|||2

1
2

(8)

(9)

where the subspace norm is associated with the above deﬁned {S(k)}K

k=1.

See Appendix B for details.

3.3 Analysis
Let Y ∈ Rn×···×n be a tensor corrupted by Gaussian noise with standard deviation σ as follows:

Y = X ∗ + σE.

(cid:110) 1

ˆX = arg min
X  {M (k)}K

We deﬁne a slightly modiﬁed estimator ˆX as follows:

(cid:111)
k=1 ∈ M(ρ)
(10)
where M(ρ) is a restriction of the set of matrices M (k) ∈ Rn×HK−1  k = 1  . . .   K deﬁned as
follows:

M (k)S(k)(cid:62)(cid:17)

F + λ|||X|||s : X =

 {M (k)}K

|||Y − X|||2

K(cid:88)

foldk

(cid:16)

k=1

k=1

2

√

√

(

n +

H K−1) ∀k (cid:54)= (cid:96)

.

(cid:111)

(cid:110){M (k)}K

M(ρ) :=

k=1 : (cid:107)foldk(M (k))((cid:96))(cid:107) ≤ ρ
K

This restriction makes sure that M (k)  k = 1  . . .   K  are incoherent  i.e.  each M (k) has a spectral
norm that is as low as a random matrix when unfolded at a different mode (cid:96). Similar assumptions
were used in low-rank plus sparse matrix decomposition [2  12] and for the denoising bound for the
latent nuclear norm [23].
Then we have the following statement (we prove this in Appendix D.3).
Theorem 2. Let Xp be any tensor that can be expressed as

(cid:16)

p S(k)(cid:62)(cid:17)

 

Xp =

foldk

M (k)

K(cid:88)

which satisﬁes the above incoherence condition {M (k)
of M (k)

k=1 ∈ M(ρ) and let rk be the rank
In addition  we assume that each S(k) is constructed as S(k) =

for k = 1  . . .   K.

p }K

p

k=1

6

(k−1) ⊗ ··· ⊗ (cid:98)P
(cid:98)P
(cid:16)√
= I H. Then there are universal constants c0
and c1 such that any solution ˆX of the minimization problem (10) with λ = |||Xp − X ∗|||s∗ +
c0σ

(k+1) with ((cid:98)P
H K−1 +(cid:112)2 log(K/δ)

satisﬁes the following bound

)(cid:62)(cid:98)P
(cid:17)

n +

√

(k)

(k)

(cid:114)(cid:88)K

rk 

k=1

||| ˆX − X ∗|||F ≤ |||Xp − X ∗|||F + c1λ

with probability at least 1 − δ.

Note that the right-hand side of the bound consists of two terms. The ﬁrst term is the approximation
error. This term will be zero if X ∗ lies in Span({S(k)}K
k=1). This is the case  if we choose S(k) =
I nK−1 as in the latent nuclear norm  or if the condition of Corollary 1 is satisﬁed for the smallest βR
when we use the Kronecker product construction we proposed. Note that the regularization constant
λ should also scale with the dual subspace norm of the residual Xp − X ∗.
The second term is the estimation error with respect to Xp. If we take Xp to be the orthogonal pro-
jection of X ∗ to the Span({S(k)}K
k=1)  we can ignore the contribution of the residual to λ  because
(Xp − X ∗)(k)S(k) = 0. Then the estimation error scales mildly with the dimensions n  H K−1 and
with the sum of the ranks. Note that if we take S(k) = I nK−1  we have H K−1 = nK−1  and we
recover the guarantee (3) .

4 Experiments

In this section  we conduct tensor denoising experiments on synthetic and real datasets  to numeri-
cally conﬁrm our analysis in previous sections.

4.1 Synthetic data
We randomly generated the true rank two tensor X ∗ of size 20×30×40 with singular values β1 = 20
and β2 = 10. The true factors are generated as random matrices with orthonormal columns. The
observation tensor Y is then generated by adding Gaussian noise with standard deviation σ to X ∗.
Our approach is compared to the CP decomposition  the overlapped approach  and the latent ap-
proach. The CP decomposition is computed by the tensorlab [22] with 20 random initializations.
We assume CP knows the true rank is 2. For the subspace norm  we use Algorithm 2 described
(k)’s. We computed
the solutions for 20 values of regularization parameter λ logarithmically spaced between 1 and 100.
For the overlapped and the latent norm  we use ADMM described in [25]; we also computed 20
solutions with the same λ’s used for the subspace norm.

in Section 3. We also select the top 2 singular vectors when constructing (cid:98)U
We measure the performance in the relative error deﬁned as |||(cid:98)X −X ∗|||F /|||X ∗|||F . We report the min-

imum error obtained by choosing the optimal regularization parameter or the optimal initialization.
Although the regularization parameter could be selected by leaving out some entries and measuring
the error on these entries  we will not go into tensor completion here for the sake of simplicity.
Figure 2 (a) and (b) show the result of this experiment. The left panel shows the relative error for 3
representative values of λ for the subspace norm. The black dash-dotted line shows the minimum
error across all the λ’s. The magenta dashed line shows the error corresponding to the theoretically
motivated choice λ = σ(maxk(

H K−1) +(cid:112)2 log(K)) for each σ. The two vertical

lines are thresholds of σ from Corollary 1 corresponding to β1 and β2  namely  β1/((cid:81)
β2/((cid:81)
parameter λ leads to predicting with (cid:98)X = 0.

k nk)1/4 and
k nk)1/4. It conﬁrms that there is a rather sharp increase in the error around the theoretically
predicted places (see also Figure 1(b)). We can also see that the optimal λ should grow linearly with
σ. For large σ (small SNR)  the best relative error is 1 since the optimal choice of the regularization

nk +

√

√

Figure 2 (b) compares the performance of the subspace norm to other approaches. For each method
the smallest error corresponding to the optimal choice of the regularization parameter λ is shown.

7

(a)

(b)

(c)

Figure 2: Tensor denoising. (a) The subspace approach with three representative λ’s on synthetic
data. (b) Comparison of different methods on synthetic data. (c) Comparison on amino acids data.
In addition  to place the numbers in context  we plot the line corresponding to

(cid:112)R(cid:80)

Relative error =

k nk log(K)
|||X ∗|||F

· σ 

(11)

which we call “optimistic”. This can be motivated from considering the (non-tractable) maximum
likelihood estimator for CP decomposition (see Appendix A).
Clearly  the error of CP  the subspace norm  and “optimistic” grows at the same rate  much slower
than overlap and latent. The error of CP increases beyond 1  as no regularization is imposed (see
Appendix C for more experiments). We can see that both CP and the subspace norm are behaving
near optimally in this setting  although such behavior is guaranteed for the subspace norm whereas
it is hard to give any such guarantee for the CP decomposition based on nonlinear optimization.

4.2 Amino acids data

The amino acid dataset [5] is a semi-realistic dataset commonly used as a benchmark for low rank
tensor modeling. It consists of ﬁve laboratory-made samples  each one contains different amounts
of tyrosine  tryptophan and phenylalanine. The spectrum of their excitation wavelength (250-300
nm) and emission (250-450 nm) are measured by ﬂuorescence  which gives a 5 × 201 × 61 tensor.
As the true factors are known to be these three acids  this data perfectly suits the CP model. The true
rank is fed into CP and the proposed approach as H = 3. We computed the solutions of CP for 20
different random initializations  and the solutions of other approaches with 20 different values of λ.
For the subspace and the overlapped approach  λ’s are logarithmically spaced between 103 and 105.
For the latent approach  λ’s are logarithmically spaced between 104 and 106. Again  we include the
optimistic scaling (11) to put the numbers in context.
Figure 2(c) shows the smallest relative error achieved by all methods we compare. Similar to the
synthetic data  both CP and the subspace norm behaves near ideally  though the relative error of
CP can be larger than 1 due to the lack of regularization. Interestingly the theoretically suggested
scaling of the regularization parameter λ is almost optimal.

5 Conclusion

proposed norm is deﬁned with respect to a set of orthonormal matrices (cid:98)P

We have settled a conjecture posed by [20] and showed that indeed O(nK/4) signal-to-noise ratio
is sufﬁcient also for odd order tensors. Moreover  our analysis shows an interesting two-phase be-
havior of the error. This ﬁnding lead us to the development of the proposed subspace norm. The
(K)  which are
estimated by mode-wise singular value decompositions. We have analyzed the denoising perfor-
mance of the proposed norm  and shown that the error can be bounded by the sum of two terms 
which can be interpreted as an approximation error term coming from the ﬁrst (non-convex) step 
and an estimation error term coming from the second (convex) step.

  . . .  (cid:98)P

(1)

8

σ00.511.52Relative Error00.511.52λ1 = 1.62λ2 = 5.46λ3 = 14.38min errorsuggestedσ00.511.52Relative Error00.10.20.30.40.50.60.70.80.91cpsubspacelatentoverlapoptimisticσ0500100015002000Relative Error00.20.40.60.811.21.41.61.8cpsubspacelatentoverlapsuggestedoptimistic,Qinqing Zheng
Ryota Tomioka
Hongteng Xu
Dixin Luo
Lawrence Carin