2016,On Mixtures of Markov Chains,We study the problem of reconstructing a mixture of Markov chains from the trajectories generated by random walks through the state space.  Under mild non-degeneracy conditions  we show that we can uniquely reconstruct the underlying chains by only considering trajectories of length three  which represent triples of states. Our algorithm is spectral in nature  and is easy to implement.,On Mixtures of Markov Chains

Rishi Gupta∗
Stanford University
Stanford  CA 94305

rishig@cs.stanford.edu

Ravi Kumar

Google Research

Mountain View  CA 94043

ravi.k53@gmail.com

Sergei Vassilvitskii
Google Research

New York  NY 10011
sergeiv@google.com

Abstract

We study the problem of reconstructing a mixture of Markov chains from the
trajectories generated by random walks through the state space. Under mild non-
degeneracy conditions  we show that we can uniquely reconstruct the underlying
chains by only considering trajectories of length three  which represent triples of
states. Our algorithm is spectral in nature  and is easy to implement.

1

Introduction

Markov chains are a simple and incredibly rich tool for modeling  and act as a backbone in numerous
applications—from Pagerank for web search to language modeling for machine translation. While
the true nature of the underlying behavior is rarely Markovian [6]  it is nevertheless often a good
mathematical assumption.
In this paper  we consider the case where we are given observations from a mixture of L Markov
chains  each on the same n states  with n ≥ 2L. Each observation is a series of states  and is
generated as follows: a Markov chain and starting state are selected from a distribution S  and then
the selected Markov chain is followed for some number of steps. The goal is to recover S and the
transition matrices of the L Markov chains from the observations.
When all of the observations follow from a single Markov chain (namely  when L = 1)  recovering
the mixture parameters is easy. A simple calculation shows that the empirical starting distribution
and the empirical transition probabilities form the maximum likelihood Markov chain. So we are
largely interested in the case when L > 1.
As a motivating example  consider the usage of a standard maps app on a phone. There are a number
of different reasons one might use the app: to search for a nearby business  to get directions from
one point to another  or just to orient oneself. However  the users of the app never specify an explicit
intent  rather they swipe  type  zoom  etc.  until they are satisﬁed. Each one of the latent intents can be
modeled by a Markov chain on a small state space of actions. If the assignment of each session to an
intent were explicit  recovering these Markov chains would simply reduce to several instances of the
L = 1 case. Here we are interested in the unsupervised setting of ﬁnding the underlying chains when
this assignment is unknown. This allows for a better understanding of usage patterns. For example:
• Common uses for the app that the designers had not expected  or had not expected to be
common. For instance  maybe a good fraction of users (or user sessions) simply use the app
to check the trafﬁc.
• Whether different types of users use the app differently. For instance  experienced users
might use the app differently than ﬁrst time users  either due to having different goals  or
due to accomplishing the same tasks more efﬁciently.
• Undiscoverable ﬂows  with users ignoring a simple  but hidden menu setting  and instead

using a convoluted path to accomplish the same goal.

∗Part of this work was done while the author was visiting Google Research.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

The question of untangling mixture models has received a lot of attention in a variety of different
situations  particularly in the case of learning mixtures of Gaussians  see for example the seminal
work of [8]  as well as later work by [5  11  15] and the references therein. This is  to the best of our
knowledge  the ﬁrst work that looks at unraveling mixtures of Markov chains.
There are two immediate approaches to solving this problem. The ﬁrst is to use the Expectation-
Maximization (EM) algorithm [9]. The EM algorithm starts by guessing an initial set of parameters
for the mixture  and then performs local improvements that increase the likelihood of the proposed
solution. The EM algorithm is a useful benchmark and will converge to some local optimum  but it
may be slow to get there [12]  and has are no guarantees on the quality of the ﬁnal solution.
The second approach is to model the problem as a Hidden Markov Model (HMM)  and employ the
machinery for learning HMMs  particularly the recent tensor decomposition methods [2  3  10]. As
in our case  this machinery relies on having more observed states than hidden states. Unfortunately 
directly modeling a Markov chain mixture as an HMM (or as a mixture of HMMs  as in [13]) requires
nL hidden states for n observed states. Given that  one could try adapting the tensor decomposition
arguments from [3] to our problem  which is done in Section 4.3 of [14]. However  as the authors note 
this requires accurate estimates for the distribution of trajectories (or trails) of length ﬁve  whereas our
results only require estimates for the distribution of trails of length three. This is a large difference in
the amount of data one might need to collect  as one would expect to need Θ(nt) samples to estimate
the distribution of trails of length t.
An entirely different approach is to assume a Dirichlet prior on the mixture  and model the problem
as learning a mixture of Dirichlet distributions [14]. Besides requiring the Dirichlet prior  this
method also requires very long trails. Finally  we would like to note a connection to the generic
identiﬁability results for HMMs and various mixture models in [1]. Their results are existential rather
than algorithmic  but dimension three also plays a central role.
Our contributions. We propose and study the problem of reconstructing a mixture of Markov chains
from a set of observations  or trajectories. Let a t-trail be a trajectory of length t: a starting state
chosen according to S along with t − 1 steps along the appropriate Markov chain.
(i) We identify a weak non-degeneracy condition on mixtures of Markov chains and show that under
that non-degeneracy condition  3-trails are sufﬁcient for recovering the underlying mixture parameters.
We prove that for random instances  the non-degeneracy condition holds with probability 1.
(ii) Under the non-degeneracy condition  we give an efﬁcient algorithm for uniquely recovering the
mixture parameters given the exact distribution of 3-trails.
(iii) We show that our algorithm outperforms the most natural EM algorithm for the problem in some
regimes  despite EM being orders of magnitude slower.
Organization. In Section 2 we present the necessary background material that will be used in the
rest of the paper. In Section 3 we state and motivate the non-degeneracy condition that is sufﬁcient
for unique reconstruction. Using this assumption  in Section 4 we present our four-step algorithm
for reconstruction. In Section 5 we present our experimental results on synthetic and real data. In
Section 6 we show that random instances are non-degenerate with probability 1.

2 Preliminaries
Let [n] = {1  . . .   n} be a state space. We consider Markov chains deﬁned on [n]. For a Markov
chain given by its n × n transition matrix M  let M (i  j) denote the probability of moving from state
j M (i  j) = 1. (In general

i to state j. By deﬁnition  M is a stochastic matrix  M (i  j) ≥ 0 and(cid:80)

we use A(i  j) to denote the (i  j)th entry of a matrix A.)
For a matrix A  let A denote its transpose. Every n × n matrix A of rank r admits a singular value
decomposition (SVD) of the form A = U ΣV where U and V are n × r orthogonal matrices and Σ
is an r × r diagonal matrix with non-negative entries. For an L × n matrix B of full rank  its right
pseudoinverse B−1 is an n × L matrix of full rank such that BB−1 = I; it is a standard fact that
pseudoinverses exist and can be computed efﬁciently when n ≥ L.
We now formally deﬁne a mixture of Markov chains (M S). Let L ≥ 1 be an integer. Let
M = {M 1  . . .   M L} be L transition matrices  all deﬁned on [n]. Let S = {s1  . . .   sL} be a

2

(cid:96) i s(cid:96)

corresponding set of positive n-dimensional vectors of starting probabilities such that(cid:80)

i = 1.
Given M and S  a t-trail is generated as follows: ﬁrst pick the chain (cid:96) and the starting state i with
probability s(cid:96)
i  and then perform a random walk according to the transition matrix M (cid:96)  starting from
i  for t − 1 steps.
Throughout  we use i  j  k to denote states in [n] and (cid:96) to denote a particular chain. Let 1n be a
column vector of n 1’s.
Deﬁnition 1 (Reconstructing a Mixture of Markov Chains). Given a (large enough) set of trails
generated by a mixture of Markov chains and an L > 1  ﬁnd the parameters M and S of the mixture.
Note that the number of parameters is O(n2 · L). In this paper  we focus on a seemingly restricted
version of the reconstruction problem  where all of the given trails are of length three  i.e.  every trail
is of the form i → j → k for some three states i  j  k ∈ [n]. Surprisingly  we show that 3-trails are
sufﬁcient for perfect reconstruction.
By the deﬁnition of mixtures  the probability of generating a given 3-trail i → j → k is

(cid:88)

i · M (cid:96)(i  j) · M (cid:96)(j  k) 
s(cid:96)

(cid:96)

(1)
which captures the stochastic process of choosing a particular chain (cid:96) using S and taking two steps in
M (cid:96). Since we only observe the trails  the choice of the chain (cid:96) in the above process is latent. For
each j ∈ [n]  let Oj be an n × n matrix such that Oj(i  k) equals the value in (1). It is easy to see
that using O((n3 log n)/2) sample trails  every entry in Oj for every j is approximated to within
an additive ±. For the rest of the paper  we assume we know each Oj(i  k) exactly  rather than an
approximation of it from samples.
We now give a simple decomposition of Oj in terms of the transition matrices in M and the starting
probabilities in S. Let Pj be the L × n matrix whose ((cid:96)  i)th entry denotes the probability of
i · M (cid:96)(i  j). In a
using chain (cid:96)  starting in state i  and transitioning to state j  i.e.  Pj((cid:96)  i) = s(cid:96)
similar manner  let Qj be the L × n matrix whose ((cid:96)  k)th entry denotes the probability of starting
j · M (cid:96)(j  k). Finally  let
in state j  and transitioning to state k under chain (cid:96)  i.e.  Qj((cid:96)  k) = s(cid:96)
Sj = diag(s1

j ) be the L × L diagonal matrix of starting probabilities in state j. Then 

j   . . .   sL

Oj = Pj · S−1
j
This decomposition will form the key to our analysis.

· Qj.

(2)

with s(cid:96) + s(cid:96)(cid:48)

3 Conditions for unique reconstruction
Before we delve into the details of the algorithm  we ﬁrst identify a condition on the mixture (M S)
such that there is a unique solution to the reconstruction problem when we consider trails of length
three. (To appreciate such a need  consider a mixture where two of the matrices M (cid:96) and M (cid:96)(cid:48)
in
M are identical. Then for a ﬁxed vector v  any s(cid:96) and s(cid:96)(cid:48)
= v will give the same
observations  regardless of the length of the trails.) To motivate the condition we require  consider
again the sets of L × n matrices P = {P1  . . .   Pn} and Q = {Q1  . . .   Qn} as deﬁned in (2).
Together these matrices capture the n2L − 1 parameters of the problem  namely  n − 1 for each of
the n rows of each of the L transition matrices M (cid:96)  and nL − 1 parameters deﬁning S. However 
together P and Q have 2n2L entries  implying algebraic dependencies between them.
Deﬁnition 2 (Shufﬂe pairs). Two ordered sets X = {X1  . . .   Xn} and Y = {Y1  . . .   Yn} of L × n
matrices are shufﬂe pairs if the jth column of Xi is identical to the ith column of Yj for all i  j ∈ [n].
Note that P and Q are shufﬂe pairs. We state an equivalent way of specifying this deﬁnition. Consider
a 2nL× n2 matrix A(P Q) that consists of a top and a bottom half. The top half is an nL× n2 block
diagonal matrix with Pi as the ith block. The bottom half is a concatenation of n different nL × n
block diagonal matrices; the ith block of the jth matrix is the jth column of −Qi. A representation
of A is given in Figure 1. As intuition  note that in each column  the two blocks of L entries are the
same up to negation. Let F be the L × 2nL matrix consisting of 2n L × L identity matrices in a row.
It is straightforward to see that P and Q are shufﬂe pairs if and only if F · A(P Q) = 0.
Let the co-kernel of a matrix X be the vector space comprising the vectors v for which vX = 0. We
have the following deﬁnition.

3

Figure 1: A(P Q) for L = 2  n = 4. When P and Q are shufﬂe pairs  each column has two copies
of the same L-dimensional vector (up to negation). M is well-distributed if there are no non-trivial
vectors v for which v · A(P Q) = 0.

Deﬁnition 3 (Well-distributed). The set of matrices M is well-distributed if the co-kernel of A(P Q)
has rank L.
Equivalently  M is well-distributed if the co-kernel of A(P Q) is spanned by the rows of F . Section 4
shows how to uniquely recover a mixture from the 3-trail probabilities Oj when M is well-distributed
and S has only non-zero entries. Section 6 shows that nearly all M are well-distributed  or more
formally  that the set of non well-distributed M has (Lebesgue) measure 0.

4 Reconstruction algorithm

We present an algorithm to recover a mixture from its induced distribution on 3-trails. We assume for
the rest of the section that M is well-distributed (see Deﬁnition 3) and S has only non-zero entries 
which also means Pj  Qj  and Oj have rank L for each j.
At a high level  the algorithm begins by performing an SVD of each Oj  thus recovering both Pj and
Qj  as in (2)  up to unknown rotation and scaling. The key to undoing the rotation will be the fact
that the sets of matrices P and Q are shufﬂe pairs  and hence have algebraic dependencies.
More speciﬁcally  our algorithm consists of four high-level steps. We ﬁrst list the steps and provide
an informal overview; later we will describe each step in full detail.
(i) Matrix decomposition: Using SVD  we compute a decomposition Oj = UjΣjVj and let P (cid:48)
and Q(cid:48)
L × L matrices Yj and Zj so that Pj = YjP (cid:48)
(ii) Co-kernel: Let P(cid:48) = {P (cid:48)
1  . . .   Q(cid:48)
1  . . .   P (cid:48)
matrix A(P(cid:48) Q(cid:48)) as deﬁned in Section 3  to obtain matrices Y (cid:48)
single matrix R for which Yj = RY (cid:48)
(iii) Diagonalization: Let R(cid:48) be the matrix of eigenvectors of (Z(cid:48)
1 )−1(Z(cid:48)
is a permutation matrix Π and a diagonal matrix D such that R = DΠR(cid:48).
(iv) Two-trail matching: Given Oj it is easy to compute the probability distribution of the mixture
over 2-trails. We use these to solve for D  and using D  compute R  Yj  Pj  and Sj for each j.

j = Uj
j = ΣjVj. These are the initial guesses at (Pj  Qj). We prove in Lemma 4 that there exist

n}. We compute the co-kernel of
j and Z(cid:48)
j. We prove that there is a

j and Qj = ZjQ(cid:48)

j for each j ∈ [n].

n}  and Q(cid:48) = {Q(cid:48)
j and Zj = RZ(cid:48)

j for all j.

1Y (cid:48)

2Y (cid:48)

2 ). We prove that there

4.1 Matrix decomposition
j are L × n matrices of full rank. The following lemma states that
From the deﬁnition  both P (cid:48)
the SVD of the product of two matrices A and B returns the original matrices up to a change of basis.

j and Q(cid:48)

4

Lemma 4. Let A  B  C  D be L × n matrices of full rank  such that AB = CD. Then there is an
L × L matrix X of full rank such that C = X−1A and D = XB.

Proof. Note that A = ABB−1 = CDB−1 = CW for W = DB−1. Since A has full rank  W must
as well. We then get CD = AB = CW B  and since C has full column rank  D = W B. Setting
X = W completes the proof.

j Qj) and Oj = P (cid:48)

j  Lemma 4 implies that there exists an L × L matrix Xj of
Since Oj = Pj(S−1
full rank such that Pj = X−1
  and let Zj = SjXj. Note that
both Yj and Zj have full rank  for each j. Once we have Yj and Zj  we can easily compute both Pj
and Sj  so we have reduced our problem to ﬁnding Yj and Zj.

j and Qj = SjXjQ(cid:48)

j. Let Yj = X−1

j P (cid:48)

jQ(cid:48)

j

j)j∈[n]  (ZjQ(cid:48)

4.2 Co-kernel
Since (P Q) is a shufﬂe pair  ((YjP (cid:48)
j)j∈[n]) is also a shufﬂe pair. We can write the
latter fact as B(Y  Z)A(P (cid:48)  Q(cid:48)) = 0  where B(Y  Z) is the L × 2nL matrix comprising 2n matrices
concatenated together; ﬁrst Yj for each j  and then Zj for each j. We know A(P (cid:48)  Q(cid:48)) from the
matrix decomposition step  and we are trying to ﬁnd B(Y  Z). By well-distributedness  the co-kernel
of A(P  Q) has rank L. Let D be the 2nL × 2nL block diagonal matrix with the diagonal entries
n ). Then A(P (cid:48)  Q(cid:48)) = D A(P  Q). Since D has full rank 
(Y −1
the co-kernel of A(P (cid:48)  Q(cid:48)) has rank L as well.
We compute an arbitrary basis of the co-kernel of A(P (cid:48)  Q(cid:48))) 2 and write it as an L × 2nL matrix
as an initial guess B(Y (cid:48)  Z(cid:48)) for B(Y  Z). Since B(Y  Z) lies in the co-kernel of A(P (cid:48)  Q(cid:48))  and has
exactly L rows  there exists an L × L matrix R such that B(Y  Z) = R B(Y (cid:48)  Z(cid:48))  or equivalently 
such that Yj = RY (cid:48)
j and Zj = RZ(cid:48)
j for every j. Since Yj and Zj have full rank  so does R. Now our
problem is reduced to computing R.

2   . . .   Z−1

n   Z−1

  . . .   Y −1

1   Z−1

  Y −1

2

1

jY (cid:48)

4.3 Diagonalization
Recall from the matrix decomposition step that there exist matrices Xj such that Yj = X−1
and
Zj = SjXj. Hence Z(cid:48)
j = (R−1Zj)(Yj R−1) = R−1SjR−1. It seems difﬁcult to compute R
directly from equations of the form R−1SjR−1  but we can multiply any two of them together to get 
e.g.  (Z(cid:48)
1 )−1(Z(cid:48)
1Y (cid:48)
Since S−1
1 S2 is a diagonal matrix  we can diagonalize RS−1
1 S2R−1 as a step towards computing
R. Let R(cid:48) be the matrix of eigenvectors of RS−1
1 S2R−1. Now  R is determined up to a scaling and
ordering of the eigenvectors. In other words  there is a permutation matrix Π and diagonal matrix D
such that R = DΠR(cid:48).

2 ) = RS−1

1 S2R−1.

2Y (cid:48)

j

j Qj1n = Pj1L for each j  since each row of S−1

4.4 Two-trail matching
First  Oj1n = PjS−1
j Qj is simply the set of
transition probabilities out of a particular Markov chain and state. Another way to see it is that both
Oj1n and Pj1L are vectors whose ith coordinate is the probability of the trail i → j.
From the ﬁrst three steps of the algorithm  we also have Pj = YjP (cid:48)
1)−1  where the inverse is a pseudoinverse.
Hence 1LDΠ = 1LP1(R(cid:48)Y (cid:48)
We arbitrarily ﬁx Π  from which we can compute D  R  Yj  and ﬁnally Pj for each j. From the
diagonalization step (Section 4.3)  we can also compute Sj = R(Z(cid:48)
Note that the algorithm implicitly includes a proof of uniqueness  up to a setting of Π. Different
orderings of Π correspond to different orderings of M (cid:96) in M.

1)−1 = O11n(R(cid:48)Y (cid:48)

j = DΠR(cid:48)Y (cid:48)

j )R for each j.

j = RY (cid:48)

j P (cid:48)
j.

j P (cid:48)

1 P (cid:48)

1 P (cid:48)

jY (cid:48)

2For instance  by taking the SVD of A(P (cid:48)  Q(cid:48))  and looking at the singular vectors.

5

5 Experiments

We have presented an algorithm for reconstructing a mixture of Markov chains from the observations 
assuming the observation matrices are known exactly.
In this section we demonstrate that the
algorithm is efﬁcient  and performs well even when we use empirical observations. In addition  we
also compare its performance against the most natural EM algorithm for the reconstruction problem.
Synthetic data. We begin by generating well distributed instances M and S. Let Dn be the
uniform distribution over the n-dimensional unit simplex  namely  the uniform distribution over
vectors in Rn whose coordinates are non-negative and sum to 1.
For a speciﬁc n and L  we generate an instance (M  S) as follows. For each state i and Markov chain
M (cid:96)  the set of transition probabilities leaving i is distributed as Dn. We draw each s(cid:96) from Dn as
well  and then divide by L  so that the sum over all s(cid:96)(i) is 1. In other words  each trail is equally
likely to come from any of the L Markov chains. This restriction has little effect on our algorithm 
but is needed to make EM tractable. For each instance  we generate T samples of 3-trails. The results
that we report are the medians of 100 different runs.
Metric for synthetic data. Our goal is exact recovery of the underlying instance M. Given two
n × n matrices A and B  the error is the average total variation distance between the transition
i j |A(i  j) − B(i  j)|. Given a pair of instances M =
{M 1  . . .   M L} and N = {N 1  . . .   N L} on the same state space [n]  the recovery error is the
minimum average error over all matchings of chains in N to M. Let σ be a permutation on [L]  then:

probabilities: error(A  B) = 1/(2n) ·(cid:80)

recovery error(M N ) = min

σ

1
L

error(M (cid:96)  N σ((cid:96))).

(cid:88)

(cid:96)

Given all the pairwise errors error(M (cid:96)  N p)  this minimum can be computed in time O(L3) by the
Hungarian algorithm. Note that the recovery error ranges from 0 to 1.

Real data. We use the last.fm 1K dataset3  which contains the list of songs listened by heavy users
of Last.Fm. We use the top 25 artist genres4 as the states of the Markov chain. We consider the ten
heaviest users in the data set  and for each user  consider the ﬁrst 3001 state transitions that change
their state. We break each sequence into 3000 3-trails. Each user naturally deﬁnes a Markov chain on
the genres  and the goal is to recover these individual chains from the observed mixture of 3-trails.

Metric for real data. Given a 3-trail from one of the users  our goal is to predict which user the
3-trail came from. Speciﬁcally  given a 3-trail t and a mixture of Markov chains (M S)  we assign t
to the Markov chain most likely to have generated it. A recovered mixture (M S) thereby partitions
the observed 3-trails into L groups. The prediction error is the minimum over all matchings between
groups and users of the fraction of trails that are matched to the wrong user. The prediction error
ranges from 0 to 1 − 1/L.

Handling approximations. Because the algorithm operates on real data  rather than perfect obser-
vation matrices  we make two minor modiﬁcations to make it more robust. First  in the diagonalization
step (Section 4.3)  we sum (Z(cid:48)
i+1Yi+1)−1 over all i before diagonalizing to estimate R(cid:48) 
instead of just using i = 1. Second  due to noise  the matrices M that we recover at the end need not
be stochastic. Following the work of [7] we normalize the values by ﬁrst taking absolute values of all
entries  and then normalizing so that each of the columns sums to 1.

iYi)−1(Z(cid:48)

Baseline. We turn to EM as a practical baseline for this reconstruction problem. In our implementa-
tion  we continue running EM until the log likelihood changes by less than 10−7 in each iteration;
this corresponds to roughly 200-1000 iterations. Although EM continues to improve its solution past
this point  even at the 10−7 cutoff  it is already 10-50x slower than the algorithm we propose.

5.1 Recovery and prediction error

3http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz
4http://static.echonest.com/Lastfm-ArtistTags2007.tar.gz

6

(a)

(b)

(c)

Figure 3: (a) Performance of EM and our algorithm vs number of samples (b) Performance of EM
and our algorithm vs L (synthetic data) (c) Performance of EM and our algorithm (real data)

For the synthetic data  we ﬁx n = 6 and L = 3  and
for each of the 100 instances generate a progressively
larger set of samples. Recall that the number of unknown
parameters grows as Θ(n2L)  so even this relatively sim-
ple setting corresponds to over 100 unknown parameters.
Figure 3(a) shows the median recovery error of both ap-
proaches. It is clear that the proposed method signiﬁcantly
outperforms the EM approach  routinely achieving errors
of 10-90% lower. Furthermore  while we did not make
signiﬁcant attempts to speed up EM  it is already over
10x slower than our algorithm at n = 6 and L = 3  and
becomes even slower as n and L grow.
In Figure 3(b) we study the error as a function of L. Our
approach is signiﬁcantly faster  and easily outperforms
EM at 100 iterations. Running EM for 1000 iterations results with prediction error on par with our
algorithm  but takes orders of magnitude more time to complete.
For the real data  there are n = 25 states  and we tried L = 4  . . .   10 for the number of users. We run
EM for 500 iterations and show the results in Figure 3(c). While our algorithm slightly underperforms
EM  it is signiﬁcantly faster in practice.

Figure 2: Performance of the algorithm
as a function of n and L for a ﬁxed num-
ber of samples.

5.2 Dependence on n and L

To investigate the dependence of our approach on the size of the input  namely n and L  we ﬁx the
number of samples to 108 but vary both the number of states from 6 to 30  as well as the number
of chains from 3 to 9. Recall that the number of parameters grows as n2L  therefore  the largest
examples have almost 1000 parameters that we are trying to ﬁt.
We plot the results in Figure 2. As expected  the error grows linearly with the number of chains. This
is expected — since we are keeping the number of samples ﬁxed  the relative error (from the true
observations) grows as well. It is therefore remarkable that the error grows only linearly with L.
We see more interesting behavior with respect to n. Recall that the proofs required n ≥ 2L.
Empirically we see that at n = 2L the approach is relatively brittle  and errors are relatively high.
However  as n increases past that  we see the recovery error stabilizes. Explaining this behavior
formally is an interesting open question.

6 Analysis
We now show that nearly all M are well-distributed (see Deﬁnition 3)  or more formally  that the set
of non well-distributed M has (Lebesgue) measure 0 for every L > 1 and n ≥ 2L.
We ﬁrst introduce some notation. All arrays and indices are 1-indexed. In previous sections  we
have interpreted i  j  k  and (cid:96) as states or as indices of a mixture; in this section we drop these
interpretations and just use them as generic indices.

7

0.00.20.40.60.81.0Number of Samples1e90.000.010.020.030.040.050.060.07Median Recovery ErrorAlgEM45678910L0.000.050.100.150.200.25Median Recovery ErrorAlgEM 100EM 100045678910L0.00.20.40.60.81.0Median Prediction ErrorAlgEM 100051015202530n0.000.050.100.150.20Median Recovery Errorl=3l=5l=7l=9ij to the (i  j)th entry of
···
···



(cid:26)ej−i+1

···
···
···

e1
e2
eL e1
...
e3
e2
e1
e2
eL e1
...
e2

eL
eL−1
...
e1
eL
eL−1
...
e1
if i ≤ L or j ≤ L
if i  j > L

···

e3

e1
eL
...
e2
eL
eL−1
...
e1

···
···

e2
e1

···
e3
···
e1
eL ···

···

e2

(cid:19)



.

eL
eL−1
...
e1
eL−1
eL−2
...
eL

For vectors v1  . . .   vn ∈ RL  let v[n] denote (v1  . . .   vn)  and let ∗(v1  . . .   vn) denote the vi’s
concatenated together to form a vector in RnL. Let vi[j] denote the jth coordinate of vector vi.
We ﬁrst show that there exists at least one well-distributed P for each n and L.
Lemma 5 (Existence of a well-distributed P). For every n and L with n ≥ 2L  there exists a P for
which the co-kernel of A(P Q) has rank L.

Proof. It is sufﬁcient to show it for n = 2L  since for larger n we can pad with zeros. Also  recall
that F · A(P Q) = 0 for any P  where F is the L × 2nL matrix consisting of 2n identity matrices
concatenated together. So the co-kernel of any A(P Q) has rank at least L  and we just need to show
that there exists a P where the co-kernel of A(P Q) has rank at most L.
Now  let e(cid:96) be the (cid:96)th basis vector in RL. Let P∗ = (P ∗
of P ∗

ij denote the jth column

n )  and let p∗

1   . . .   P ∗

i . We set p∗

(cid:18) E E

ij =

ej−i 

E E(cid:48)

ij(cid:105) = (cid:104)bj  p∗

ij(cid:105) for each i and j.

  where subscripts are taken mod L. Note that we can

Formally  p∗
split the above matrix into four L × L blocks
where E(cid:48) is a horizontal “rotation” of E.
Now  let a[n]  b[n] be any vectors in RL such that v = ∗(a1  . . .   an  b1  . . .   bn) ∈ R2nL is in the
co-kernel of A(P∗ Q∗). Recall this means v · A(P∗ Q∗) = 0. Writing out the matrix A  it is not
too hard to see that this holds if and only if (cid:104)ai  p∗
ij = e1. For each k ∈ [L]  we have ak[1] = bk[1] from the upper
Consider the i and j where p∗
left quadrant  ak[1] = bL+k[1] from the upper right quadrant  aL+k[1] = bk[1] from the lower left
quadrant  and aL+k[1] = bL+(k+1 (mod L))[1] from the lower right quadrant. It is easy to see that
these combine to imply that ai[1] = bj[1] for all i  j ∈ [n].
A similar argument for each l ∈ [L] shows that ai[l] = bj[l] for all i  j and l. Equivalently  ai = bj
for each i and j  which means that v lives in a subspace of dimension L  as desired.
We now bootstrap from our one example to show that almost all P are well-distributed.
Theorem 6 (Almost all P are well-distributed). The set of non-well-distributed P has Lebesgue
measure 0 for every n and L with n ≥ 2L.
let h(P) =
Proof. Let A(cid:48)(P Q) be all but
det|A(cid:48)(P Q)A(cid:48)(P Q)|. Note that h(P) is non-zero if and only if P is well-distributed. Let P∗ be
the P∗ from Lemma 5. Since A(cid:48)(P∗ Q∗) has full row rank  h(P∗) (cid:54)= 0. Since h is a polynomial
function of the entries of P  and h is non-zero somewhere  h is non-zero almost everywhere [4].

the last L rows of A(P Q).

For any P 

7 Conclusions

In this paper we considered the problem of reconstructing Markov chain mixtures from given
observation trails. We showed that unique reconstruction is algorithmically possible under a mild
technical condition on the “well-separatedness” of the chains. While our condition is sufﬁcient  we
conjecture it is also necessary; proving this is an interesting research direction. Extending our analysis
to work for the noisy case is also a plausible research direction  though we believe the corresponding
analysis could be quite challenging.

8

References
[1] E. S. Allman  C. Matias  and J. A. Rhodes. Identiﬁability of parameters in latent structure

models with many observed variables. The Annals of Statistics  pages 3099–3132  2009.

[2] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and M. Telgarsky. Tensor decompositions for

learning latent variable models. JMLR  15(1):2773–2832  2014.

[3] A. Anandkumar  D. Hsu  and S. M. Kakade. A method of moments for mixture models and

hidden Markov models. In COLT  pages 33.1–33.34  2012.

[4] R. Caron and T. Traynor. The zero set of a polynomial. WSMR Report  pages 05–02  2005.
[5] K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and

independence. In COLT  pages 9–20  2008.

[6] F. Chierichetti  R. Kumar  P. Raghavan  and T. Sarlos. Are web users really Markovian? In

WWW  pages 609–618  2012.

[7] S. B. Cohen  K. Stratos  M. Collins  D. P. Foster  and L. Ungar. Experiments with spectral

learning of latent-variable PCFGs. In NAACL  pages 148–157  2013.

[8] S. Dasgupta. Learning mixtures of Gaussians. In FOCS  pages 634–644  1999.
[9] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via

the EM algorithm. Journal of the Royal Statistical Society  Series B  39(1):1–38  1977.

[10] D. Hsu  S. M. Kakade  and T. Zhang. A spectral algorithm for learning hidden Markov models.

JCSS  78(5):1460–1480  2012.

[11] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In

FOCS  pages 93–102  2010.

[12] R. A. Redner and H. F. Walker. Mixture densities  maximum likelihood  and the EM algorithm.

SIAM Review  26:195–239  1984.

[13] C. Subakan  J. Traa  and P. Smaragdis. Spectral learning of mixture of hidden Markov models.

In NIPS  pages 2249–2257  2014.

[14] Y. C. Sübakan. Probabilistic time series classiﬁcation. Master’s thesis  Bo˘gaziçi University 

2011.

[15] S. Vempala and G. Wang. A spectral algorithm for learning mixture models. JCSS  68(4):841–

860  2004.

9

,Rishi Gupta
Ravi Kumar
Sergei Vassilvitskii
Tianqi Chen
Lianmin Zheng
Eddie Yan
Ziheng Jiang
Thierry Moreau
Luis Ceze
Carlos Guestrin
Arvind Krishnamurthy