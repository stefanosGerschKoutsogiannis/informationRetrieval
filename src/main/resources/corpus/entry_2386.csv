2017,Group Additive Structure Identification for Kernel Nonparametric Regression,The additive model is one of the most popularly used models for high dimensional nonparametric regression analysis. However  its main drawback is that it neglects possible interactions between predictor variables. In this paper  we reexamine the group additive model proposed in the literature  and rigorously define the intrinsic group additive structure for the relationship between the response variable $Y$ and the predictor vector $\vect{X}$  and further develop an effective structure-penalized kernel method for simultaneous identification of the intrinsic group additive structure and nonparametric function estimation. The method utilizes a novel complexity measure we derive for group additive structures. We show that the proposed method is consistent in identifying the intrinsic group additive structure.  Simulation study and real data applications demonstrate the effectiveness of the proposed method as a general tool for high dimensional nonparametric regression.,Group Additive Structure Identiﬁcation for Kernel

Nonparametric Regression

Pan Chao

Department of Statistics

Purdue University

West Lafayette  IN 47906
panchao25@gmail.com

Michael Zhu

Department of Statistics  Purdue University

West Lafayette  IN 47906

Center for Statistical Science

Department of Industrial Engineering
Tsinghua University  Beijing  China

yuzhu@purdue.edu

Abstract

The additive model is one of the most popularly used models for high dimensional
nonparametric regression analysis. However  its main drawback is that it neglects
possible interactions between predictor variables. In this paper  we reexamine the
group additive model proposed in the literature  and rigorously deﬁne the intrinsic
group additive structure for the relationship between the response variable Y and
the predictor vector X  and further develop an effective structure-penalized kernel
method for simultaneous identiﬁcation of the intrinsic group additive structure
and nonparametric function estimation. The method utilizes a novel complexity
measure we derive for group additive structures. We show that the proposed method
is consistent in identifying the intrinsic group additive structure. Simulation study
and real data applications demonstrate the effectiveness of the proposed method as
a general tool for high dimensional nonparametric regression.

1

Introduction

Regression analysis is popularly used to study the relationship between a response variable Y
and a vector of predictor variables X. Linear and logistic regression analysis are arguably two
most popularly used regression tools in practice  and both postulate explicit parametric models on
f (X) = E[Y |X] as a function of X. When no parametric models can be imposed  nonparametric
regression analysis can instead be performed. On one hand  nonparametric regression analysis is
ﬂexible and not susceptible to model mis-speciﬁcation  whereas on the other hand  it suffers from a
number of well-known drawbacks especially in high dimensional settings. Firstly  the asymptotic
error rate of nonparametric regression deteriorates quickly as the dimension of X increases. [16]
shows that with some regularity conditions  the optimal asymptotic error rate for estimating a d-

times differentiable function is O(cid:0)n−d/(2d+p)(cid:1)  where p is the dimensionality of X. Secondly  the

resulting ﬁtted nonparametric function is often complicated and difﬁcult to interpret.
To overcome the drawbacks of high dimensional nonparametric regression  one popularly used
approach is to impose the additive structure [5] on f (X)  that is to assume that f (X) = f1(X1) +
···+fp(Xp) where f1  . . .   fp are p unspeciﬁed univariate functions. Thanks to the additive structure 
the nonparametric estimation of f or equivalently the individual fi’s for 1 ≤ i ≤ p becomes efﬁcient
and does not suffer from the curse of dimensionality. Furthermore  the interpretability of the resulting
model has also been much improved.
The key drawback of the additive model is that it does not assume interactions between the pre-
dictor variables. To address this limitation  functional ANOVA models were proposed to accom-
modate higher order interactions  see [4] and [13]. For example  by neglecting interactions of

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

d=1

(cid:80)

order higher than 2  the functional ANOVA model can be written as f (X) = (cid:80)p
(cid:80)
f (X) =(cid:80)D

i=1 fi(Xi) +
1≤i j≤p fij(Xi  Xj)  with some marginal constraints. Another higher order interaction model 
1≤i1 ... id≤p fj(Xi1  . . .   Xid )  is proposed by [6]. This model considers all in-
teractions of order up to D  which is estimated by Kernel Ridge Regression (KRR) [10] with the
elementary symmetric polynomial (ESP) kernel. Both of the two models assume the existence of
possible interactions between any two or more predictor variables. This can lead to a serious problem 
that is  the number of nonparametric functions that need to be estimated quickly increases as the
number of predictor variables increases.
There exists another direction to generalize the additive model. When proposing the Optimal Kernel
Group Transformation (OKGT) method for nonparametric regression  [11] considers the additive
structure of predictor variables in groups instead of individual predictor variables. Let G := {uj}d
j=1
be a index partition of the predictor variables  that is  uj ∩uk = ∅ if j (cid:54)= k and ∪d
j=1uj = {1  . . .   p}.
Let X uj = {Xk; k ∈ uj} for j = 1  . . .   d. Then {X1  . . .   Xd} = X u1 ∪ ··· ∪ X ud. For any
function f (X)  if there exists an index partition G = {u1  . . .   ud} such that

f (X) = fu1 (X u1) + . . . + fud (X ud ) 

(1)
where fu1(X u1)  . . .   fud (X ud ) are d unspeciﬁed nonparametric functions  then it is said that f (X)
admits the group additive structure G. We also refer to (1) as a group additive model for f (X). It is
clear that the usual additive model is a special case with G = {(1)  . . .   (p)}.
Suppose Xj1 and Xj2 are two predictor variables. Intuitively  if Xj1 and Xj2 interact to each other 
then they must appear in the same group in an reasonable group additive structure of f (X). On the
other hand  if Xj1 and Xj2 belong to two different groups  then they do not interact with each other.
Therefore  in terms of accommodating interactions  the group additive model can be considered
lying in the middle between the original additive model and the functional ANOVA or higher order
interaction models. When the group sizes are small  for example all are less than or equal to 3 
the group additive model can maintain the estimation efﬁciency and interpretability of the original
additive model while avoiding the problem of a high order model discussed earlier.
However  in [11]  there are two important issues not addressed. First  the group additive structure
may not be unique  which will lead to the nonidentiﬁability problem for the group additive model.
(See discussion in Section 2.1). Second  [11] has not proposed a systematic approach to identify the
group additive structure. In this paper  we intend to resolve these two issues. To address the ﬁrst issue 
we rigorously deﬁne the intrinsic group additive structure for any square integrable function  which
in some sense is the minimal group additive structure among all correct group additive structures.
To address the second issue  we propose a general approach to simultaneously identifying the
intrinsic group additive structure and estimating the nonparametric functions using kernel methods
and Reproducing Kernel Hilbert Spaces (RKHSs). For a given group additive structure G =
{u1  . . .   ud}  we ﬁrst deﬁne the corresponding direct sum RKHS as HG = Hu1 ⊕···⊕Hud where
Hui is the usual RKHS for the variables in uj only for j = 1  . . .   d. Based on the results on the
capacity measure of RKHSs in the literature  we derive a tractable capacity measure of the direct
sum RKHS HG which is further used as the complexity measure of G. Then  the identiﬁcation of the
intrinsic group additive structure and the estimation of the nonparametric functions can be performed
through the following minimization problem

ˆf   ˆG = arg min
f∈HG G

1
n

(yi − f (xi))2 + λ(cid:107)f(cid:107)2HG

+ µC(G).

We show that when the novel complexity measure of group additive structure C(G) is used  the
minimizer ˆG is consistent for the intrinsic group additive structure. We further develop two algorithms 
one uses exhaustive search and the other employs a stepwise approach  for identifying true additive
group structures under the small p and large p scenarios. Extensive simulation study and real data
applications show that our proposed method can successfully recover the true additive group structures
in a variety of model settings.
There exists a connection between our proposed group additive model and graphical models ([2] 
[7]). This is especially true when a sparse block structure is imposed [9]. However  a key difference
exists. Let’s consider the following example. Y = sin(X1 + X 2
6 ) + .
A graphical model typically considers the conditional dependence (CD) structure among all of the

2 + X3) + cos(X4 + X5 + X 2

n(cid:88)

i=1

2

variables including X1  . . .   X6 and Y   which is more complex than the group additive (GA) structure
{(X1  X2  X3)  (X4  X5  X6)}. The CD structure  once known  can be further examined to infer
the GA structure. In this paper  we however proposed methods that directly target the GA structure
instead of the more complex CD structure.
The rest of the paper is organized as follows. In Section 2  we rigorously formulate the problem
of Group Additive Structure Identiﬁcation (GASI) for nonparametric regression and propose the
structural penalty method to solve the problem. In Section 3  we prove the selection consistency for
the method. We report the experimental results based on simulation studies and real data application
in Section 4 and 5. Section 6 concludes this paper with discussion.

2 Method

2.1 Group Additive Structures

PX

2 + X 2

u(X ) := {g ∈ L2

additive model f (X) =(cid:80)d

In the Introduction  we discussed that the group additive structure for f (X) may not be unique. Here
is an example. Consider the model Y = 2 + 3X1 + 1/(1 + X 2
3 ) + arcsin ((X4 + X5)/2) +  
where  is the 0 mean error independent of X. According to our deﬁnition  this model admits
the group additive structure G0 = {(1)   (2  3)   (4  5)}. Let G1 = {(1  2  3)   (4  5)} and G2 =
{(1  4  5)   (2  3)}. The model can also be said to admit G1 and G2. However  there exists a major
difference between G0  G1 and G2. While the groups in G0 cannot be further divided into subgroups 
both G1 and G2 contain groups that can be further split. We deﬁne the following partial order between
group structures to characterize the difference and their relationship.
Deﬁnition 1. Let G and G(cid:48) be two group additive structures. If for every group u ∈ G there is a
group v ∈ G(cid:48) such that u ⊆ v  then G is called a sub group additive structure of G(cid:48). This relation is
denoted as G ≤ G(cid:48). Equivalently  G(cid:48) is a super group additive structure of G  denoted as G(cid:48) ≥ G.
In the previous example  G0 is a sub group additive structure of both G1 and G2. However  the
order between G1 and G2 is not deﬁned. Let X := [0  1]p be the p-dimensional unit cube for all
the predictor variables X with distribution PX. For a group of predictor variables u  we deﬁne
(X ) | g(X) = fu(X u)} 
the space of square integrable functions as L2
that is L2
u contains the functions that only depend on the variables in group u. Then the group
j=1 fuj (X uj ) is a member of the direct sum function space deﬁned as
u(X ). Let |u| be the cardinality of the group u. If u is the only group in a group
G(X ) := ⊕u∈GL2
L2
additive structure and |u| = p  then L2
The following proposition shows that the order of two different group additive structures is preserved
by their corresponding square integrable function spaces.
⊆ L2
Proposition 1. Let G1 and G2 be two group additive structures. If G1 ≤ G2  then L2
Furthermore  if X1  . . .   Xp are independent and G1 (cid:54)= G2  then L2
Deﬁnition 2. Let f (X) be an square integrable function. For a group additive structure G  if there
is a function fG ∈ L2
G such that fG = f  then G is called an amiable group additive structure for f.
In the example discussed in the beginning of the subsection  G0  G1 and G2 are all amiable group
structures. So amiable group structures may not be unique.
Proposition 2. Suppose G is an amiable group additive structure for f. If there is a second group
additive structure G(cid:48) such that G ≤ G(cid:48)  then G(cid:48) is also amiable for f.
We denote the collection of all amiable group structures for f (X) as Ga  which is partially ordered
and complete. Therefore  there exists a minimal group additive structure in Ga  which is the most
concise group additive structure for the target function. We state this result as a theorem.
Theorem 1. Let Ga be the set of amiable group additive structures for f. There is a unique minimal
group additive structure G∗ ∈ Ga such that G∗ ≤ G for all G ∈ Ga  where the order is given by
Deﬁnition 1. G∗ is called the intrinsic group additive structure for f.
For statistical modeling  G∗ achieves the greatest dimension reduction for the relationship between
Y and X. It induces the smallest function space which includes the model. In general  the intrinsic
group structure can help much mitigate the curse of dimensionality while improving both efﬁciency
and interpretability of high dimensional nonparametric regression.

u = L2

G and f is a fully non-parametric function.

⊂ L2

.

G2

G1

.

G2

G1

3

(cid:40)

n(cid:88)

i=1

1
n

2.2 Kernel Method with Known Intrinsic Group Additive Structure
Suppose the intrinsic group additive structure for f (X) = E[Y |X] is known to be G∗ = {uj}d
j=1 
that is  f (X) = fu1(X u1) + ··· + fud (X ud ). Then  we will use the kernel method to estimate the
functions fu1  fu2  . . .  fud. Let (Kuj  Huj ) be the kernel and its corresponding RKHS for the j-th
group uj. Then using kernel methods is to solve

(yi − fG∗ (xi))2 + λ(cid:107)fG∗(cid:107)2HG∗

 

(2)

where HG∗ := {f =(cid:80)d

ˆfλ G∗ = arg min
fG∗∈HG∗

j=1 fuj | fuj ∈ Huj}. The subscripts of ˆf are used to explicitly indicate its

dependence on the group additive structure G∗ and tuning parameter λ.
In general  an RKHS is usually smaller than the L2 space deﬁned on the same input domain. So  it is
not always true that ˆfλ G∗ achieves f. However  one can choose to use universal kernels Kuj so that
their corresponding RKHSs are dense in the L2 spaces (see [3]  [15]). Using universal kernels allows
ˆfλ G∗ to not only achieve unbiasedness but also recover the group additive structure of f (X). This is
the fundamental reason for the consistency property of our proposed method to identify the intrinsic
group additive structure. Two examples of universal kernel are Gaussian and Laplace.

(cid:41)

2.3

Identiﬁcation of Unknown Intrinsic Group Additive Structure

2.3.1 Penalization on Group Additive Structures
The success of the kernel method hinges on knowing the intrinsic group additive structure G∗. In
practice  however  G∗ is seldom known  and it may be of primary interest to identify G∗ while
estimating a group additive model. Recall that in Subsection 2.1  we have shown that G∗ exists and
is unique. The other group additive structures belong to two categories  amiable and non-amiable.
Let’s consider an arbitrary non-amiable group additive structure G ∈ G \ Ga ﬁrst. If G is used in the
place of G∗ in (2)  the solution ˆfλ G  as an estimator of f  will have a systematic bias because the L2
distance between any function fG ∈ HG and the true function f will be bounded below. In other
words  using a non-amiable structure will result in poor ﬁtting of the model.
Next we consider an arbitrary amiable group additive structure G ∈ Ga to be used in (2). Recall that
because G is amiable  we have fG∗ = fG almost surely (in population) for the true function f (X).
The bias of the resulting ﬁtted function ˆfλ G will vanish as the sample size increases. Although their
asymptotic rates are in general different  under ﬁxed sample size n  simply using goodness of ﬁt
will not be able to distinguish G from G∗. The key difference between G∗ and G is their structural
complexities  that is  G∗ is the smallest among all amiable structures (i.e. G∗ ≤ G ∀G ∈ Ga).
Suppose a proper complexity measure of a group additive structure G can be deﬁned (to be addressed
in the next section) and is denoted as C(G). We can then incorporate C(G) into (2) as an additional
penalty term and change the kernel method to the following structure-penalized kernel method.

ˆfλ µ  ˆG = arg min
fG∈HG G

(yi − fG(xi))2 + λ(cid:107)fG(cid:107)2HG

+ µC(G)

.

(3)

It is clear that the only difference between (2) and (3) is the term µC(G). As discussed above  the
intrinsic group additive structure G∗ can achieve the goodness of ﬁt represented by the ﬁrst two
terms in (3) and the penalty on the structural complexity represented by the last term. Therefore 
by properly choosing the tuning parameters  we expect that ˆG is consistent in that the probability
ˆG = G∗ increases to one as n increases (see the Theory Section below). In the next section  we
derive a tractable complexity measure for a group additive structure.

(cid:40)

n(cid:88)

i=1

1
n

(cid:41)

2.3.2 Complexity Measure of Group additive Structure
It is tempting to propose an intuitive complexity measure for a group additive structure C(·) such
that C(G1) ≤ C(G2) whenever G1 ≤ G2. The intuition however breaks down or at least becomes
less clear when the order between G1 and G2 cannot be deﬁned. From Proposition 1  it is known
that when G1 < G2  we have L2
. It is not difﬁcult to show that it is also true that when
G1

⊂ L2

G2

4

G1 < G2  then HK G1 ⊂ HK G2. This observation motivates us to deﬁne the structural complexity
measure of G through the capacity measure of its corresponding RKHS HG.
There exist a number of different capacity measures for RKHSs in the literature  including entropy
[18]  VC dimensions [17]  Rademacher complexity [1]  and covering numbers ([14]  [18]). After
investigating and comparing different measures  we use covering number to design a practically
convenient complexity measure for group additive structures.
It is known that an RKHS HK can be embedded in the continuous function space C(X ) (see [12] 
[18])  with the inclusion mapping denoted as IK : HK → C(X ). Let HK r = {h : (cid:107)h(cid:107)Hk ≤
r  and h ∈ HK} be an r-ball in HK and I (HK r) be the closure of I (HK r) ⊆ C(X ). One way
to measure the capacity of HK is through the covering number of I (HK r) in C(X )  denoted as
N (  I (HK r)  d∞)  which is the smallest cardinalty of a subset S of C(X ) such that I (HK r) ⊂
∪s∈S{t ∈ C(X ) : d∞(t  s) ≤ }. Here  is any small positive value and d∞ is the sup-norm.
The exact formula for N (  I (HK r)  d∞) is in general not available. Under certain conditions 
various upper bounds have been obtained in the literature. One such upper bound is presented below.
When K is a convolution kernel  i.e. K(x  t) = k(x − t)  and the Fourier transform of k decays
exponentially  then it is given in [18] that

lnN(cid:16)

  I(HK r)  d∞

(cid:17) ≤ Ck p

(cid:16)

ln

r


(cid:17)p+1

 

(4)

where Ck p is a constant depending on the kernel function k and input dimension p. In particular 
when K is a Gaussian kernel  [18] has obtained more elaborate upper bounds.
The upper bound in (4) depends on r and  through ln(r/). When  → 0 with r being ﬁxed (e g.
r = 1 when a unit ball is considered)  (ln(r/))p+1 dominates the upper bound. According to [8]  the
growth rate of N (  IK) or its logarithm can be viewed as a capacity measure of RKHS. So we use
(ln(r/))p+1 as the capacity measure  which can be reparameterized as αp+1 with α = ln(r/). Let
C(Hk) denote the capacity measure of Hk  which is deﬁned as C(Hk) = (ln(r/))p+1 = α()p+1.
We know  is the radius of a covering ball  which is the unit of measurement we use to quantify the
capacity. The capacity of two RKHSs with different input dimensions are easier to be differentiated
when  is small. This gives an interpretation of α.
We have deﬁned a capacity measure for a general RKHS. In Problem (3)  the model space HG is a
direct sum of a number of RKHSs. Let G = {u1  . . .   ud}; let HG Hu1  . . .  Hud be the RKHSs
corresponding to G  u1  . . .   ud  respectively; let IG  Iu1  . . .   Iud be the inclusion mappings of
HG Hu1  . . .  Hud into C(X ). Then  we have the following proposition.
Proposition 3. Let G be a group additive structure and HG be the induced direct sum RKHS deﬁned
in (3). Then  we have the following inequality relating the covering number of HG and the covering
numbers of Huj

lnN (  IG  d∞) ≤ d(cid:88)

lnN

(cid:18) 

j=1

(cid:19)

|G|   Iuj   d∞

 

(5)

where |G| denotes the number of groups in G.
By applying Proposition 3 and using the parameterized upper bound  we have lnN (  IG  d∞) =

u∈G α()|u|+1(cid:1) . The rate can be used as the explicit expression of the complexity measure

O(cid:0)(cid:80)
for group additive structures  that is C(G) =(cid:80)d

j=1 α()|uj|+1. Recall that there is another tuning
parameter µ which controls the effect of the complexity of group structure on the penalized risk.
By combining the common factor 1 in the exponent with µ  we could further simplify the penalty’s
expression. Thus  we have the following explicit formulation for GASI

ˆfλ µ  ˆG = arg min
fG∈HG G

(yi − fG(xi))2 + λ(cid:107)fG(cid:107)2HG

+ µ

α|uj|

(6)

 n(cid:88)

i=1

d(cid:88)

 .

j=1

5

2.4 Estimation

We assume that the value of λ is given. In practice  λ can be tuned separately. If the values of µ and
α are also given  Problem (6) can be solved by following a two-step procedure.
First  when the group structure G is known  fu can be estimated by solving the following problem

(cid:41)

ˆRλ
G = min
fG∈HG

1
n

(yi − fG(xi))2 + λ(cid:107)fG(cid:107)2HG

.

(7)

(cid:40)

n(cid:88)

i=1

Second  the optimal group structure is chosen to achieve both small ﬁtting error and complexity  i.e.

 ˆRλ

d(cid:88)

j=1

 .

ˆG = arg min

G∈G

G + µ

α|uj|

(8)

The two-step procedure above is expected to identify the intrinsic group structure  that is  ˆG = G∗.
Recall a group structure belongs to one of the three categories  intrinsic  amiable  or non-amiable
structures. If G is non-amiable  then ˆRλ
G is expected to be large  because G is a wrong structure
which will result in a biased estimate. If G is amiable  though ˆRλ
G is expected to be small  the
complexity penalty of G is larger than that for G∗. As a consequence  only G∗ can simultaneously
achieve a small ˆRλ
G∗ and a relatively small complexity. Therefore  when the sample size is large
enough  we expect ˆG = G∗ with high probability. If the values of µ and α are not given  a separate
validation set can be used to select tuning parameters.
The two-step estimation is summarized in Algorithm 1. When a model contains a large number of
predictor variables  such exhaustive search suffers high computational cost. In order to apply GASI
on a large model  we propose a backward stepwise algorithm which is illustrated in Algorithm 2.

for G ∈ G do

ˆRG  ˆfG ← solve (7) using G;
Calculate the sum in (8)  denoted by ˆRpen µ α

Algorithm 1: Exhaustive Search w/ Validation
1: Split data into training (T ) and validation (V) sets.
2: for (µ  α) in grid do
3:
4:
5:
end for
6:
ˆGµ α ← arg minG∈G ˆRpen µ α
7:
ˆyV ← ˆf(cid:98)Gµ α (xV );
8:
e2(cid:98)Gµ α ← (cid:107)yV − ˆyV(cid:107)2;
9:
10: end for
11: µ∗  α∗ ← arg minµ α e2(cid:98)Gµ α;
12: G∗ ← ˆGµ∗ α∗

G

G

;

;

;

Algorithm 2: Basic Backward Stepwise
1: Start with the group structure {(1  . . .   p)};
2: Solve (6) and obtain its minimum value ˆRpen
G ;
3: for each predictor variable j do
G(cid:48) ← either split j as a new group or add
4:

to an existing group;
Solve (6) and obtain its minimum value
ˆRpen
G(cid:48) ;
if ˆRpen

G(cid:48) < ˆRpen
Keep G(cid:48) as the new group structure;

G then

5:

6:
7:
end if
8:
9: end for
10: return G(cid:48);

3 Theory

1
n

In this section  we prove that the estimated group additive structure ˆG as a solution to (6) is consistent 
that is the probability P ( ˆG = G∗) goes to 1 as the sample size n goes to inﬁnity. The proof and
supporting lemmas are included in the supplementary material.
(cid:80)n
Let R(fG) = E[(Y − f (X))2] denote the population risk of a function f ∈ HG  and ˆR(f ) =
i=1(yi − f (xi))2 be the empirical risk. First  we show that for any amiable structure G ∈ Ga 
its minimized empirical risk ˆR( ˆfG) converges in probability to the optimal population risk R(f∗
G∗ )
achieved by the intrinsic group additive structure. Here ˆfG denotes the minimizer of Problem (7)
with the given G  and f∗
G∗ denotes the minimizer of the population risk when the intrinsic group
structure is used. The result is given below as Proposition 4.
Proposition 4. Let G∗ be the intrinsic group additive structure  G ∈ Ga a given amiable group
structure  and HG∗ and HG the respective direct sum RKHSs. If ˆf λ
G ∈ HG is the optimal solution of

6

ID
M1
M2
M3
M4
M5

2 + x3

y = 1

y = 2x1 + x2

+ arcsin(cid:0) x2+x3
y = arcsin(cid:0) x1+x3
(cid:1) + 1
(cid:110)(cid:112)x2

y = exp

1+x2
1

2

1+x2
2

Model

3 + sin(πx4) + log(x5 + 5) + |x6| + 

(cid:1) + arctan(cid:0)(x4 + x5 + x6)3(cid:1) + 
+ arctan(cid:0)(x4 + x5 + x6)3(cid:1) + 

2

y = x1 · x2 + sin((x3 + x4) · π) + log(x5 · x6 + 10) + 

1 + x2

2 + x2

3 + x2

4 + x2

5 + x2
6

+ 

(cid:111)

Intrinsic Group Structure

{(1)   (2)   (3)   (4)   (5)   (6)}
{(1)   (2  3)   (4  5  6)}
{(1  3)   (2)   (4  5  6)}
{(1  2)   (3  4)   (5  6)}
{(1  2  3  4  5  6)}

Table 1: Selected models for the simulation study using the exhaustive search method and the corresponding
additive group structures.

Problem (7)  then for any  > 0  we have

(cid:16)|(cid:98)R( ˆfG) − R(f

P

G∗ )| > 
∗

(cid:40)(cid:88)
(cid:17) ≤ 12n · exp
(cid:40)(cid:88)

12n · exp

u∈G
lnN

(cid:19)

lnN

(cid:18) 
12|G|  Hu  d∞
(cid:18) 
(cid:19)
12|G|  Hu  d∞

− n

− 2n
144

(cid:18) 

24

(cid:41)

+

− λn(cid:107)f∗

G∗(cid:107)2
12

(cid:19)2(cid:41)

.

(9)

u∈G

Note that λn in (9) must be chosen such that /24 − λn(cid:107)f∗
G∗(cid:107)2/12 is positive. For a ﬁxed p  the
number of amiable group additive structures is ﬁnite. Using a Bonferroni type of technique  we can
in fact obtain a uniform upper bound for all of the amiable group additive structures in Ga.
(cid:27)
Theorem 2. Let Ga be the set of all amiable group structures. For any  > 0 and n > 2/2  we have
(cid:19)2(cid:41)(cid:35)

|(cid:98)Rg( ˆf λ

≤ 12n|Ga| ·

G) − Rg(f

 HG  d∞

G∗ )| > 
∗

sup
G∈Ga

max
G∈Ga

(cid:40)

(cid:26)

(cid:18)

(cid:19)

exp

P

lnN(cid:16) 
(cid:18) 
(cid:17) − n

12

24

(cid:17) − 2n
− λn(cid:107)f∗

144
G∗(cid:107)2
12

(cid:34)
lnN(cid:16) 

 HG  d∞

12

+ exp

max
G∈Ga

(10)

G∗ )  and | ˆR( ˆfG) − R(f∗

Next we consider a non-amiable group additive structure G(cid:48) ∈ G \Ga. It turns out that ˆR( ˆfG) fails to
G∗ )| converges to a positive constant. Furthermore  because
converge to R(f∗
the number of non-amiable group additive structures is ﬁnite  we can show that | ˆR( ˆfG) − R(f∗
G∗ )|
is uniformly bounded below from zero with probability going to 1. We state the results below.
Theorem 3. (i) For a non-amiable group structure G ∈ G \ Ga  there exists a constant C > 0 such
G∗ )| converges to C in probability. (ii) There exits a constant ˜C such that
that | ˆRg( ˆf λ
P (| ˆRg( ˆf λ
G∗ )| > ˜C for all G ∈ G \ Ga) goes to 1 as n goes to inﬁnity.
By combining Theorem 2 and Theorem 3  we can prove consistency for our GASI method.
Theorem 4. Let λn ∗ n → 0. By choosing a proper tuning parameter µ > 0 for the structural
penalty   the estimated group structure ˆG is consistent for the intrinsic group additive structure G∗ 
that is  P ( ˆG = G∗) goes to one as the sample size n goes to inﬁnity.

G) − Rg(f∗
G) − Rg(f∗

4 Simulation

In this section  we evaluate the performance of GASI using synthetic data. Table 1 lists the ﬁve models
we are using. Observations of X are simulated independently from N (0  1) in M1  Unif(−1  1) in
M2 and M3  and Unif(0  2) in M4 and M5. The noise  is i.i.d. N (0  0.012). The grid values of µ
are equally spaced in [1e−10  1/64] on a log-scale and each α is an integer in [1  10].
We ﬁrst show that GASI has the ability to identify the intrinsic group additive structure. The two-step
procedure is carried out for each (µ  α) pair multiple times. If there are (µ  α) pairs for each model
that the true group structure can be often identiﬁed  then GASI has the power to identify true group
structures. We also apply Algorithm 1 which uses an additional validation set to select the parameters.
We simulate 100 different samples for each model. The frequency of the true group structure being
identiﬁed is calculated for each (µ  α).

7

Model Max freq.
M1
M2
M3
M4
M5

100
97
97
100
100

µ

1.2500e-06
1.2500e-06
1.2500e-06
1.2500e-06
1.2500e-06

α Max freq.
10
8
9
7
1

59
89
89
99
100

µ

1.2500e-06
1.2500e-06
1.2500e-06
1.2500e-06
1.2500e-06

α Max freq.
4
7
7
4
1

99
70
65
1
100

µ

1.5625e-02
1.3975e-04
1.3975e-04
1.3975e-04
1.2500e-06

α
10
9
8
8
1

Table 2: Maximum frequencies that the intrinsic group additive structures are identiﬁed for the ﬁve models
using exhaustive search algorithm without parameter tuning (left panel)  with parameter tuning (middle panel)
and stepwise algorithm (right panel). If different pairs share the same max frequency  a pair is randomly chosen.

Figure 1: Estimated transformation functions for selected groups. Top-left: group (1  6)  top-right:
group (3)  bottom-left: group (5  8)  bottom-right: group (10  12).

In Table 2  we report the maximum frequency and the corresponding (µ  α) for each model. The
complete results are included in the supplementary material. It can be seen from the left panel that
the intrinsic group additive structures can be successfully identiﬁed. When the parameters are tuned 
the middle panel shows that the performance of Model 1 deteriorated. This might be caused by the
estimation method (KRR to solve Problem (7)) used in the algorithm. It could also be affected by λ.
When the number of predictor variables increases  we use a backward stepwise algorithm. We apply
Algorithm 2 on the same models. The results are reported in the right panel in Figure 2. The true
group structures could be identiﬁed most of time for Model 1  2  3  5. The result of Model 4 is not
satisfying. Since stepwise algorithm is greedy  it is possible that the true group structures were never
visited. Further research is needed to develop a better algorithms.

5 Real Data

In this section  we report the results of applying GASI on the Boston Housing data (another real
data application is reported in the supplementary material). The data includes 13 predictor variables
used to predict the house median value. The sample size is 506. Our goal is to identify a probable
group additive structure for the predictor variables. The backward algorithm is used and the tuning
parameters µ and α are selected via 10-fold CV. The group structure that achieves the lowest average
validation error is {(1  6)   (2  11)   (3)   (4  9)   (5  8)   (7  13)   (10  12)}  which is used for further
investigation. Then the nonparametric functions for each group were estimated using the whole
data set. Because each group contains no more than two variables  the estimated functions can be
visualized. Figure 1 shows the selected results.
It is interesting to see some patterns emerging in the plots. For example  the top-left plot shows the
function of the average number of rooms per dwelling and per capita crime rate by town. We can see
the house value increases with more rooms and decreases as the crime rate increases. However  when
the crime rate is low  smaller sized houses (4 or 5 rooms) seem to be preferred. The top-right plot

8

shows that there is a changing point in terms of how house value is related to the size of non-retail
business in the area. The value initially drops when the percentage of non-retail business is small 
then increases at around 8%. The increase in the value might be due to the high demand of housing
from the employees of those business.

6 Discussion

We use group additive model for nonparametric regression and propose a RKHS complexity penalty
based approach for identifying the intrinsic group additive structure. There are two main directions
for future research. First  our penalty function is based on the covering number of RKHSs. It is of
interest to know if there exists other more effective penalty functions. Second  it is of great interest
to further improve the proposed method and apply it in general high dimensional nonparametric
regression.

9

References
[1] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results.

The Journal of Machine Learning Research  3:463–482  2003.

[2] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-

Verlag New York  Inc.  Secaucus  NJ  USA  2006.

[3] C. Carmeli  E. De Vito  A. Toigo  and V. Umanitá. Vector valued reproducing kernel hilbert spaces and

universality. Analysis and Applications  8(01):19–61  2010.

[4] C. Gu. Smoothing spline ANOVA models  volume 297. Springer Science & Business Media  2013.

[5] T. Hastie and R. Tibshirani. Generalized additive models. Statistical science  pages 297–310  1986.

[6] K. Kandasamy and Y. Yu. Additive approximations in high dimensional nonparametric regression via
the salsa. In Proceedings of the 33rd International Conference on International Conference on Machine
Learning - Volume 48  ICML’16  pages 69–78. JMLR.org  2016.

[7] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques - Adaptive

Computation and Machine Learning. The MIT Press  2009.

[8] T. Kühn. Covering numbers of Gaussian reproducing kernel Hilbert spaces. Journal of Complexity 

27(5):489–499  2011.

[9] B. M. Marlin and K. P. Murphy. Sparse gaussian graphical models with unknown block structure. In
Proceedings of the 26th Annual International Conference on Machine Learning  ICML ’09  pages 705–712 
New York  NY  USA  2009. ACM.

[10] K. P. Murphy. Machine learning: a probabilistic perspective. MIT press  2012.

[11] C. Pan  Q. Huang  and M. Zhu. Optimal kernel group transformation for exploratory regression analysis
and graphics. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining  pages 905–914. ACM  2015.

[12] T. Poggio and C. Shelton. On the mathematical foundations of learning. American Mathematical Society 

39(1):1–49  2002.

[13] J. O. Ramsay and B. W. Silverman. Applied functional data analysis: methods and case studies  volume 77.

Citeseer  2002.

[14] A. J. Smola and B. Schölkopf. Learning with kernels. Citeseer  1998.

[15] I. Steinwart and A. Christmann. Support vector machines. Springer Science & Business Media  2008.

[16] C. J. Stone. Optimal global rates of convergence for nonparametric regression. The annals of statistics 

pages 1040–1053  1982.

[17] V. Vapnik. The nature of statistical learning theory. Springer Science & Business Media  2013.

[18] D.-X. Zhou. The covering number in learning theory. Journal of Complexity  18(3):739 – 767  2002.

10

,Chao Pan
Michael Zhu