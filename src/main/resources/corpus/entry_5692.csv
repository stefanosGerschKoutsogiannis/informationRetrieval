2019,Two Time-scale Off-Policy TD Learning: Non-asymptotic Analysis over Markovian Samples,Gradient-based temporal difference (GTD) algorithms are widely used in off-policy learning scenarios. Among them  the two time-scale TD with gradient correction (TDC) algorithm has been shown to have superior performance. In contrast to previous studies that characterized the non-asymptotic convergence rate of TDC only under identical and independently distributed (i.i.d.) data samples  we provide the first non-asymptotic convergence analysis for two time-scale TDC under a non-i.i.d.\ Markovian sample path and linear function approximation. We show that the two time-scale TDC can converge as fast as O(log t/t^(2/3)) under diminishing stepsize  and can converge exponentially fast under constant stepsize  but at the cost of a non-vanishing error. We further propose a TDC algorithm with blockwisely diminishing stepsize  and show that it asymptotically converges with an arbitrarily small error at a blockwisely linear convergence rate. Our experiments demonstrate that such an algorithm converges as fast as TDC under constant stepsize  and still enjoys comparable accuracy as TDC under diminishing stepsize.,Two Time-scale Off-Policy TD Learning:

Non-asymptotic Analysis over Markovian Samples

Department of Electrical and Computer Engineering

Tengyu Xu

The Ohio State University

xu.3260@osu.edu

Shaofeng Zou

Department of Electrical Engineering

University at Buffalo  The State University of New York

szou3@buffalo.edu

Yingbin Liang

Department of Electrical and Computer Engineering

The Ohio State University

liang.889@osu.edu

Abstract

Gradient-based temporal difference (GTD) algorithms are widely used in off-policy
learning scenarios. Among them  the two time-scale TD with gradient correction
(TDC) algorithm has been shown to have superior performance. In contrast to
previous studies that characterized the non-asymptotic convergence rate of TDC
only under identical and independently distributed (i.i.d.) data samples  we provide
the ﬁrst non-asymptotic convergence analysis for two time-scale TDC under a
non-i.i.d. Markovian sample path and linear function approximation. We show
that the two time-scale TDC can converge as fast as O( log t
t2/3 ) under diminishing
stepsize  and can converge exponentially fast under constant stepsize  but at the cost
of a non-vanishing error. We further propose a TDC algorithm with blockwisely
diminishing stepsize  and show that it asymptotically converges with an arbitrarily
small error at a blockwisely linear convergence rate. Our experiments demonstrate
that such an algorithm converges as fast as TDC under constant stepsize  and still
enjoys comparable accuracy as TDC under diminishing stepsize.

1

Introduction

In practice  it is very common that we wish to learn the value function of a target policy based on
data sampled by a different behavior policy  in order to make maximum use of the data available. For
such off-policy scenarios  it has been shown that conventional temporal difference (TD) algorithms
[24  25] and Q-learning [33] may diverge to inﬁnity when using linear function approximation
[2]. To overcome the divergence issue in off-policy TD learning  [27  26  17] proposed a family
of gradient-based TD (GTD) algorithms  which were shown to have guaranteed convergence in
off-policy settings and are more ﬂexible than on-policy learning in practice [18  23]. Among those
GTD algorithms  the TD with gradient correction (TDC) algorithm has been veriﬁed to have superior
performance [17] [9] and is widely used in practice. To elaborate  TDC uses the mean squared
projected Bellman error as the objective function  and iteratively updates the function approximation
parameter with the assistance of an auxiliary parameter that is also iteratively updated. These two

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

parameters are typically updated with stepsizes diminishing at different rates  resulting the two
time-scale implementation of TDC  i.e.  the function approximation parameter is updated at a slower
time-scale and the auxiliary parameter is updated at a faster time-scale.
The convergence of two time-scale TDC and general two time-scale stochastic approximation (SA)
have been well studied. The asymptotic convergence has been shown in [4  6] for two time-scale
SA  and in [26] for two time-scale TDC  where both studies assume that the data are sampled in
an identical and independently distributed (i.i.d.) manner. Under non-i.i.d. observed samples  the
asymptotic convergence of the general two time-scale SA and TDC were established in [14  36].
All the above studies did not characterize how fast the two time-scale algorithms converge  i.e 
they did not establish the non-asymptotic convergence rate  which is specially important for a two
time-scale algorithm. In order for two time-scale TDC to perform well  it is important to properly
choose the relative scaling rate of the stepsizes for the two time-scale iterations. In practice  this can
be done by ﬁxing one stepsize and treating the other stepsize as a tuning hyper-parameter [9]  which
is very costly. The non-asymptotic convergence rate by nature captures how the scaling of the two
stepsizes affect the performance and hence can serve as a guidance for choosing the two time-scale
stepsizes in practice. Recently  [8] established the non-asymptotic convergence rate for the projected
two time-scale TDC with i.i.d. samples under diminishing stepsize.
• One important open problem that still needs to be addressed is to characterize the non-asymptotic
convergence rate for two time-scale TDC under non-i.i.d. samples and diminishing stepsizes  and
explore what such a result suggests for designing the stepsizes of the fast and slow time-scales
accordingly. Existing method developed in [8] that handles the non-asymptotic analysis for
i.i.d. sampled TDC does not accommodate a direct extension to the non-i.i.d. setting. Thus  new
technical developments are necessary to solve this problem.

Furthermore  although diminishing stepsize offers accurate convergence  constant stepsize is often
preferred in practice due to its much faster error decay (i.e.  convergence) rate. For example 
empirical results have shown that for one time-scale conventional TD  constant stepsize not only
yields fast convergence  but also results in comparable convergence accuracy as diminishing stepsize
[9]. However  for two time-scale TDC  our experiments (see Section 4.2) demonstrate that constant
stepsize  although yields faster convergence  has much bigger convergence error than diminishing
stepsize. This motivates to address the following two open issues.
• It is important to theoretically understand/explain why constant stepsize yields large convergence
error for two-time scale TDC. Existing non-asymptotic analysis for two time-scale TDC [8] focused
only on the diminishing stepsize  and does not characterize the convergence rate of two time-scale
TDC under constant stepsize.

• For two-time scale TDC  given the fact that constant stepsize yields large convergence error but
converges fast  whereas diminishing stepsize has small convergence error but converges slowly 
it is desirable to design a new update scheme for TDC that converges faster than diminishing
stepsize  but has as good convergence error as diminishing stepsize.

In this paper  we comprehensively address the above issues.

1.1 Our Contribution

Our main contributions are summarized as follows.
We develop a novel non-asymptotic analysis for two time-scale TDC with a single sample path
and under non-i.i.d. data. We show that under the diminishing stepsizes αt = cα/(1 + t)σ and
βt = cβ/(1 + t)ν respectively for slow and fast time-scales (where cα  cβ  ν  σ are positive constants
and 0 < ν < σ ≤ 1)  the convergence rate can be as large as O( log t
t2/3 )  which is achieved by
2 ν = 1. This recovers the convergence rate (up to log t factor due to non-i.i.d. data) in [8] for
σ = 3
i.i.d. data as a special case.
We also develop the non-asymptotic analysis for TDC under non-i.i.d. data and constant stepsize.
In contrast to conventional one time-scale analysis  our result shows that the training error (at slow
time-scale) and the tracking error (at fast time scale) converge at different rates (due to different
condition numbers)  though both converge linearly to the neighborhood of the solution. Our result
also characterizes the impact of the tracking error on the training error. Our result suggests that TDC

2

under constant stepsize can converge faster than that under diminishing stepsize at the cost of a large
training error  due to a large tracking error caused by the auxiliary parameter iteration in TDC.
We take a further step and propose a TDC algorithm under a blockwise diminishing stepsize inspired
by [35] in conventional optimization  in which both stepsizes are constants over a block  and decay
across blocks. We show that TDC asymptotically converges with an arbitrarily small training error at
a blockwisely linear convergence rate as long as the block length and the decay of stepsizes across
blocks are chosen properly. Our experiments demonstrate that TDC under a blockwise diminishing
stepsize converges as fast as vanilla TDC under constant stepsize  and still enjoys comparable accuracy
as TDC under diminishing stepsize.
From the technical standpoint  our proof develops new tool to handle the non-asymptotic analysis
of bias due to non-i.i.d. data for two time-scale algorithms under diminishing stepsize that does
not require square summability  to bound the impact of the fast-time-scale tracking error on the
slow-time-scale training error  and the analysis to recursively reﬁne the error bound in order to
sharpening the convergence rate.

1.2 Related Work

Due to extensive studies on TD learning  we here include only the most relevant work to this paper.
On policy TD and SA. The convergence of TD learning with linear function approximation with
i.i.d samples has been well established by using standard results in SA [5]. The non-asymptotic
convergence have been established in [4  12  30] for the general SA algorithms with martingale
difference noise  and in [7] for TD with i.i.d. samples. For the Markovian settings  the asymptotic
convergence has been established in [31  28] for TD(λ)  and the non-asymptotic convergence has
been provided for projected TD(λ) in [3] and for linear SA with Markovian noise in [13  22  21]. For
linear SA with dynamic Markovian noise  the non-asymptotic analysis of on-policy SARSA under
non-i.i.d. samples was recently studied in [37].
Off policy one time-scale GTD. The convergence of one time-scale GTD and GTD2 (which are
off-policy TD algorithms) were derived by applying standard results in SA [27  26  17]. The non-
asymptotic analysis for GTD and GTD2 have been conducted in [16] by converting the objective
function into a convex-concave saddle problem  and was further generalized to the Markovian setting
in [32]. However  such an approach cannot be generalized for analyzing two-time scale TDC that we
study here because TDC does not have an explicit saddle-point representation.
Off policy two time-scale TDC and SA. The asymptotic convergence of two time-scale TDC under
i.i.d. samples has been established in [26  17]  and the non-asymptotic analysis has been provided
in [8] as a special case of two time-scale linear SA. Under Markovian setting  the convergence of
various two time-scale GTD algorithms has been studied in [36]. The non-asymptotic analysis of two
time-scale TDC under non-i.i.d. data has not been studied before  which is the focus of this paper.
General two time-scale SA has also been studied. The convergence of two time-scale SA with
martingale difference noise was established in [4]  and its non-asymptotic convergence was provided
in [15  20  8  6]. Some of these results can be applied to two time-scale TDC under i.i.d. samples
(which can ﬁt into a special case of SA with martingale difference noise)  but not to the non-
i.i.d. setting. For two time-scale linear SA with more general Markovian noise  only asymptotic
convergence was established in [29  34  14]. In fact  our non-asymptotic analysis for two time-scale
TDC can be of independent interest here to be further generalized for studying linear SA with more
general Markovian noise.
Two concurrent and independent studies were posted online recently  which are related to our study.
[10] provided a non-asymptotic analysis for two time-scale linear SA under the non-i.i.d setting 
in which both variables are updated with constant stepsize. In contrast  our study provides the
convergence rate for the case with the two variables being updated by the stepsizes that diminish at
different rates  and hence our analysis technique is very different from that in [10]. Another study [11]
proposed an interesting approach to analyze the convergence rate of TD learning in the Markovian
setting via a Markov jump linear system. Such an approach  however  cannot be applied directly to
study the two time-scale TD algorithm that we study here.

3

2 Problem Formulation

ated Markov chain p(s(cid:48)|s) = (cid:80)
distribution of this MDP  i.e. (cid:80)
ﬁned as: vπ (s) = E[(cid:80)∞

2.1 Off-policy Value Function Evaluation
We consider the problem of policy evaluation for a Markov decision process (MDP) (S A  P  r  γ) 
where S ⊂ Rd is a compact state space  A is a ﬁnite action set  P = P(s(cid:48)|s  a) is the transi-
tion kernel  r(s  a  s(cid:48)) is the reward function bounded by rmax  and γ ∈ (0  1) is the discount
factor. A stationary policy π maps a state s ∈ S to a probability distribution π(·|s) over A.
At time-step t  suppose the process is in some state st ∈ S. Then an action at ∈ A is taken
based on the distribution π(·|st)  the system transitions to a next state st+1 ∈ S governed by the
transition kernel P(·|st  at)  and a reward rt = r(st  at  st+1) is received. Assuming the associ-
a∈A p(s(cid:48)|s  a)π(a|s) is ergodic  let µπ be the induced stationary
s p(s(cid:48)|s)µπ(s) = µπ(s(cid:48)). The value function for policy π is de-
t=0 γtr(st  at  st+1)|s0 = s  π]  and it is known that vπ(s) is the unique
ﬁxed point of the Bellman operator T π  i.e.  vπ(s) = T πvπ(s) := rπ(s) + γEs(cid:48)|svπ(s(cid:48))  where
rπ(s) = Ea s(cid:48)|sr(s  a  s(cid:48)) is the expected reward of the Markov chain induced by policy π.
We consider policy evaluation problem in the off-policy setting. Namely  a sample path
{(st  at  st+1)}t≥0 is generated by the Markov chain according to the behavior policy πb  but our
goal is to obtain the value function of a target policy π  which is different from πb.
2.2 Two Time-Scale TDC
When S is large or inﬁnite  a linear function ˆv(s  θ) = φ(s)(cid:62)θ is often used to approximate the
value function  where φ(s) ∈ Rd is a ﬁxed feature vector for state s and θ ∈ Rd is a parameter
vector. We can also write the linear approximation in the vector form as ˆv(θ) = Φθ  where Φ is
the |S| × d feature matrix. To ﬁnd a parameter θ∗ ∈ Rd with Eµπb
T π ˆv(s  θ∗).
The gradient-based TD algorithm TDC [26] updates the parameter by minimizing the mean-square
projected Bellman error (MSPBE) objective  deﬁned as

ˆv(s  θ∗) = Eµπb

J(θ) = Eµπb

[ˆv(s  θ) − ΠT π ˆv(s  θ)]2 

where Π = Φ(Φ(cid:62)ΞΦ)−1Φ(cid:62)Ξ is the orthogonal projection operation into the function space ˆV =
{ˆv(θ) | θ ∈ Rd and ˆv(·  θ) = φ(·)(cid:62)θ} and Ξ denotes the |S| × |S| diagonal matrix with the
components of µπb as its diagonal entries. Then  we deﬁne the matrices A  B  C and the vector b as

A := Eµπb

[ρ(s  a)φ(s)(γφ(s(cid:48)) − φ(s))(cid:62)]  B := −γEµπb

C := −Eµπb

[φ(s)φ(s)(cid:62)] 

b := Eµπb

[ρ(s  a)φ(s(cid:48))φ(s)(cid:62)] 
[ρ(s  a)r(s  a  s(cid:48))φ(s)] 

where ρ(s  a) = π(a|s)/πb(a|s) is the importance weighting factor with ρmax being its maximum
value. If A and C are both non-singular  J(θ) is strongly convex and has θ∗ = −A−1b as its global
minimum  i.e.  J(θ∗) = 0. Motivated by minimizing the MSPBE objective function using the
stochastic gradient methods  TDC was proposed with the following update rules:

θt+1 = ΠRθ (θt + αt(Atθt + bt + Btwt))  
wt+1 = ΠRw (wt + βt(Atθt + bt + Ctwt))  

(1)
(2)
where At = ρ(st  at)φ(st)(γφ(st+1) − φ(st))(cid:62)  Bt = −γρ(st  at)φ(st+1)φ(st)(cid:62)  Ct =
−φ(st)φ(st)(cid:62)  bt = ρ(st  at)r(st  at  st+1)φ(st)  and ΠR(x) = argminx(cid:48):||x(cid:48)||2≤R ||x − x(cid:48)||2 is
the projection operator onto a norm ball of radius R < ∞. The projection step is widely used in
the stochastic approximation literature. As we will show later  iterations (1)-(2) are guaranteed to
converge to the optimal parameter θ∗ if we choose the value of Rθ and Rw appropriately. TDC with
the update rules (1)-(2) is a two time-scale algorithm. The parameter θ iterates at a slow time-scale
determined by the stepsize {αt}  whereas w iterates at a fast time-scale determined by the stepsize
{βt}. Throughout the paper  we make the following standard assumptions [3  32  17].
Assumption 1 (Problem solvability). The matrix A and C are non-singular.
Assumption 2 (Bounded feature). (cid:107)φ(s)(cid:107)2 ≤ 1 for all s ∈ S and ρmax < ∞.
Assumption 3 (Geometric ergodicity). There exist constants m > 0 and ρ ∈ (0  1) such that

dT V (P(st ∈ ·|s0 = s)  µπb ) ≤ mρt ∀t ≥ 0 

sup
s∈S

where dT V (P  Q) denotes the total-variation distance between the probability measures P and Q.

4

In Assumption 1  the matrix A is required to be non-singular so that the optimal parameter θ∗ =
−A−1b is well deﬁned. The matrix C is non-singular when the feature matrix Φ has linearly
independent columns. Assumption 2 can be ensured by normalizing the basis functions {φi}d
i=1 and
when πb(·|s) is non-degenerate for all s. Assumption 3 holds for any time-homogeneous Markov
chain with ﬁnite state-space and any uniformly ergodic Markov chains with general state space.

Throughout the paper  we require Rθ ≥ (cid:107)A(cid:107)2 (cid:107)b(cid:107)2 and Rw ≥ 2(cid:13)(cid:13)C−1(cid:13)(cid:13)2 (cid:107)A(cid:107)2 Rθ. In practice  we

can estimate A  C and b as mentioned in [3] or simply let Rθ and Rw to be large enough.

3 Main Theorems

3.1 Non-asymptotic Analysis under Diminishing Stepsize

Our ﬁrst main result is the convergence rate of two time-scale TDC with diminishing stepsize. We
deﬁne the tracking error: zt = wt − ψ(θt)  where ψ(θt) = −C−1(b + Aθt) is the stationary point of
the ODE given by ˙w(t) = Cw(t) + Aθt + b  with θt being ﬁxed. Let λθ and λw be any constants
that satisfy λmax(2A(cid:62)C−1A) ≤ λθ < 0 and λmax(2C) ≤ λw < 0.
Theorem 1. Consider the projected two time-scale TDC algorithm in (1)-(2). Suppose Assumptions
1-3 hold. Suppose we apply diminishing stepsize αt = cα
(1+t)ν which satisfy 0 < ν <
σ < 1  0 < cα < 1|λθ| and 0 < cβ < 1|λw| . Suppose  and (cid:48) can be any constants in (0  σ − ν] and
(0  0.5]  respectively. Then we have for t ≥ 0:

(1+t)σ   βt = cβ

−|λθ|cα

1−σ (t1−σ−1)) + O(cid:16) log t
(cid:17)

+ O(h(σ  ν)) 

tσ

(cid:17)

+ O(cid:16) log t

tν + h(σ  ν)

(cid:17)1−(cid:48)

E(cid:107)θt − θ∗(cid:107)2
E(cid:107)zt(cid:107)2

2 ≤ O(e

2 ≤ O(cid:16) log t

tν

where

h(σ  ν) =

tν  
t2(σ−ν)−  

1

σ > 1.5ν 
ν < σ ≤ 1.5ν.

If 0 < ν < σ = 1  with cα = 1|λθ| and 0 < cβ < 1|λw|   we have for t ≥ 0

(cid:26) 1
2 ≤ O(cid:16) (log t)2

t

E(cid:107)θt − θ∗(cid:107)2

 

(3)

(4)

(5)

(6)

(cid:17)

+ O(cid:16) log t

tν + h(1  ν)

(cid:17)1−(cid:48)

.

1

log t

with h(σ  ν) = 1

t2/3 ) with σ = 3

tν when σ > 1.5ν  and h(σ  ν) =

For explicit expressions of (3)  (4) and (6)  please refer to (25)  (18) and (28) in the Appendix.
We further explain Theorem 1 as follows: (a) In (3) and (5)  since both  and (cid:48) can be arbitrarily
small  the convergence of E(cid:107)θt − θ∗(cid:107)2
t2(σ−ν) when ν < σ < 1.5ν  and
2 can be almost as fast as
tν when 1.5ν ≤ σ. Then best convergence rate is almost as fast as O( log t
2 ν = 1. (b)
If data are i.i.d. generated  then our bound reduces to E(cid:107)θt − θ∗(cid:107)2
2 ≤ O(exp(λθcα(t1−σ − 1)/(1 −
σ))) +O(1/tσ) +O(h(σ  ν))1−(cid:48)
t2(σ−ν)− when
ν < σ ≤ 1.5ν. The best convergence rate is almost as fast as
2 ν = 1 as given in [8].
Theorem 1 characterizes the relationship between the convergence rate of θt and stepsizes αt and
βt. The ﬁrst term of the bound in (3) corresponds to the convergence rate of θt with full gradient
∇J(θt)  which exponentially decays with t. The second term is introduced by the bias and variance
of the gradient estimator which decays sublinearly with t. The last term arises due to the accumulated
tracking error zt  which speciﬁcally arises in two time-scale algorithms  and captures how accurately
wt tracks ψ(θt). Thus  if wt tracks the stationary point ψ(θt) in each step perfectly  then we have only
the ﬁrst two terms in (3)  which matches the results of one time-scale TD learning [3  7]. Theorem
1 indicates that asymptotically  (3) is dominated by the tracking error term O(h(σ  ν)1−(cid:48)
)  which
depends on the diminishing rate of αt and βt. Since both  and (cid:48) can be arbitrarily small  if the
diminishing rate of αt is close to that of βt  then the tracking error is dominated by the slow drift 
which has an approximate order O(1/t2(σ−ν)); if the diminishing rate of αt is much faster than that
of βt  then the tracking error is dominated by the accumulated bias  which has an approximate order
O(log t/tν). Moreover  (5) and (6) suggest that for any ﬁxed σ ∈ (0  1]  the optimal diminishing rate
of βt is achieved by σ = 3

t2/3 with σ = 3

1

1

2 ν.

5

From the technical standpoint  we develop novel techniques to handle the interaction between the
training error and the tracking error and sharpen the error bounds recursively. The proof sketch and
the detailed steps are provided in Appendix A.

3.2 Non-asymptotic Analysis under Constant Stepsize

As we remark in Section 1  it has been demonstrated by empirical results [9] that the standard TD
under constant stepsize not only converges fast  but also has comparable training error as that under
diminishing stepsize. However  this does not hold for TDC. When the two variables in TDC are
updated both under constant stepsize  our experiments demonstrate that constant stepsize yields fast
convergence  but has large training error. In this subsection  we aim to explain why this happens by
analyzing the convergence rate of the two variables in TDC  and the impact of one on the other.
The following theorem provides the convergence result for TDC with the two variables iteratively
updated respectively by two different constant stepsizes.
Theorem 2. Consider the projected TDC algorithm in eqs. (1) and (2). Suppose Assumption 1-3 hold.
Suppose we apply constant stepsize αt = α  βt = β and α = ηβ which satisfy η > 0  0 < α < 1|λθ|
and 0 < β < 1|λw| . We then have for t ≥ 0:

E(cid:107)θt − θ∗(cid:107)2

2 ≤ (1 − |λθ|α)t((cid:107)θ0 − θ∗(cid:107)2

2 + C1)

(7)

(8)

E(cid:107)zt(cid:107)2

+ C2 max{α  α ln
1
α
2 ≤ (1 − |λw|β)t (cid:107)z0(cid:107)2
2 + C5 max{β  β ln
|λθ|(1−|λθ|α)T +1 with T = (cid:100) ln[C5 max{β ln( 1
1−(1−|λθ|α)T +1

} + (C3 max{β  β ln
1
β
β )β}/(cid:107)z0(cid:107)2
2]

1
β
} + C6η 

} + C4η)0.5

− ln(1−|λw|β)

(cid:101)  and C2  C3  C4 
where C1 = 4γρmaxRθRw
C5 and C6 are positive constants independent of α and β. For explicit expressions of C2  C3  C4  C5
and C6  please refer to (67)  (68)  (69)  (59)  and (60) in the Supplementary Materials.
Theorem 2 shows that TDC with constant stepsize converges to a neighborhood of θ∗ exponentially
fast. The size of the neighborhood depends on the second and the third terms of the bound in (7) 
which arise from the bias and variance of the update of θt and the tracking error zt in (8)  respectively.
Clearly  the convergence zt  although is also exponentially fast to a neighborhood  is under a different
rate due to the different condition number. We further note that as the stepsize parameters α  β
approach 0 in a way such that α/β → 0  θt approaches to θ∗ as t → ∞  which matches the asymptotic
convergence result for two time-scale TDC under constant stepsize in [36].
Diminishing vs Constant Stepsize: We next discuss the comparison between TDC under diminish-
ing stepsize and constant stepsize. Generally  Theorem 1 suggests that diminishing stepsize yields
better converge guarantee (i.e.  converges exactly to θ∗) than constant stepsize shown in Theorem 2
(i.e.  converges to the neighborhood of θ∗). In practice  constant stepsize is recommended because
diminishing stepsize may take much longer time to converge. However  as Figure 2 in Section 4.2
shows  although TDC with large constant stepsize converges fast  the training error due to the conver-
gence to the neighborhood is signiﬁcantly worse than the diminishing stepsize. More speciﬁcally 
when η = α/β is ﬁxed  as α grows  the convergence becomes faster  but as a consequence  the
term (C3 max{β  β ln 1
β} + C4η)0.5 due to the tracking error increases and results in a large training
error. Alternatively  if α gets small so that the training error is comparable to that under diminishing
stepsize  then the convergence becomes very slow. This suggests that simply setting the stepsize to be
constant for TDC does not yield desired performance. This motivates us to design an appropriate
update scheme for TDC such that it can enjoy as fast error convergence rate as constant stepsize
offers  but still have comparable accuracy as diminishing stepsize enjoys.

3.3 TDC under Blockwise Diminishing Stepsize

and study its theoretical convergence guarantee. In Algorithm 1  we deﬁne ts =(cid:80)s

In this subsection  we propose a blockwise diminishing stepsize scheme for TDC (see Algorithm 1) 

i=0 Ts.

The idea of Algorithm 1 is to divide the iteration process into blocks  and diminish the stepsize
blockwisely  but keep the stepsize to be constant within each block. In this way  within each block 

6

θs 0 = θs−1  ws 0 = ws−1
for i = 1  2  ...  Ts do

Algorithm 1 Blockwise Diminishing Stepsize TDC
Input: θ0 0 = θ0  w0 0 = w0 = 0  T0 = 0  block index S
1: for s = 1  2  ...  S do
2:
3:
4:
5:
6:
7:
8:
9: end for
Output: θS  wS

(cid:0)θs i−1 + αs(Ats−1+iθs i−1 + bts−1+i + Bts−1+iws i−1)(cid:1)
(cid:0)ws i−1 + βs(Ats−1+iθs i−1 + bts−1+i + Cts−1+iws i−1)(cid:1)

Sample (sts−1+i  ats−1+i  sts−1+i+1  rts−1+i) from trajetory
θs i = ΠRθ
ws i = ΠRw

end for
θs = θs Ts  ws = ws Ts

TDC can decay fast due to constant stepsize and still achieve an accurate solution due to blockwisely
decay of the stepsize  as we will demonstrate in Section 4. More speciﬁcally  the constant stepsizes
αs and βs for block s are chosen to decay geometrically  such that the tracking error and accumulated
variance and bias are asymptotically small; and the block length Ts increases geometrically across
blocks  such that the training error E(cid:107)θs − θ∗(cid:107)2
2 decreases geometrically blockwisely. We note that
the design of the algorithm is inspired by the method proposed in [35] for conventional optimization
problems.
The following theorem characterizes the convergence of Algorithm 1.
Theorem 3. Consider the projected TDC algorithm with blockwise diminishing stepsize
Suppose max{log(1/αs)αs  αs} ≤
as in Algorithm 1.
min{s−1/(4C7)  1/|λx|}  βs = ηαs and Ts = (cid:100)log1/(1−|λx|αs) 4(cid:101)  where λx < 0 and C7 > 0 are
constant independent of s (see (72) and (75) in the Supplementary Materials for explicit expression
of λx and C7)  s = (cid:107)θ0 − θ∗(cid:107)2 /2s and η ≥ 1/2 max{0  λmin(C−1(A(cid:62) + A))}. Then  after
S = (cid:100)log2(0/)(cid:101) blocks  we have

Suppose Assumptions 1-3 hold.

2 ≤ .

E(cid:107)θS − θ∗(cid:107)2
 ).

 log 1

The total sample complexity is O( 1
Theorem 3 indicates that the sample complexity of TDC under blockwise diminishing stepsize is
slightly better than that under diminishing stepsize. Our empirical results (see Section 4.3) also
demonstrate that blockwise diminishing stepsize yields as fast convergence as constant stepsize
and has comparable training error as diminishing stepsize. However  we want to point out that the
advantage of blockwise diminishing stepsize does not come for free  rather at the cost of some extra
parameter tuning in practice to estimate 0  |λx|  C7 and η; whereas diminishing stepsize scheme as
guided by our Theorem 1 requires to tune at most three parameters to obtain desirable performance.

4 Experimental Results

In this section  we provide numerical experiments to verify our theoretical results and the efﬁciency of
Algorithm 1. More precisely  we consider Garnet problems [1] denoted as G(nS  nA  p  q)  where ns
denotes the number of states  nA denotes the number of actions  p denotes the number of possible next
states for each state-action pair  and q denotes the number of features. The reward is state-dependent
and both the reward and the feature vectors are generated randomly. The discount factor γ is set to
0.95 in all experiments. We consider the G(500  20  50  20) problem. For all experiments  we choose
θ0 = w0 = 0. All plots report the evolution of the mean square error over 500 independent runs.

4.1 Optimal Diminishing Stepsize

In this subsection  we provide numerical results to verify Theorem 1. We compare the performance
of TDC updates with the same αt but different βt. We consider four different diminishing stepsize
settings: (1) cα = cβ = 0.03  σ = 0.15; (2) cα = cβ = 0.18  σ = 0.30; (3) cα = cβ = 1  σ = 0.45;
(4) cα = cβ = 4  σ = 0.60. For each case with ﬁxed slow time-scale parameter σ  the fast time-scale
stepsize βt has decay rate ν to be 1
6 σ  and σ. Our results are reported in Figure 1 

3 σ  5

9 σ  2

2 σ  1

3 σ  5

7

in which for each case the left ﬁgure reports the overall iteration process and the right ﬁgure reports
the corresponding zoomed tail process of the last 100000 iterations. It can be seen that in all cases 
TDC iterations with the same slow time-scale stepsize σ share similar error decay rates (see the left
plot)  and the difference among the fast time-scale parameter ν is reﬂected by the behavior of the
error convergence tails (see the right plot). We observe that ν = 2
3 σ yields the best error decay rate.
This corroborates Theorem 1  which illustrates that the fast time-scale stepsize βt with parameter ν
affects only the tracking error term in (3)  that dominates the error decay rate asymptotically.

(a) σ = 0.15 (left: full; right: tail)

(b) σ = 0.3 (left: full; right: tail)

(c) σ = 0.45 (left: full; right: tail)

(d) σ = 0.6 (left: full; right: tail)

Figure 1: Comparison among diminishing stepsize settings. For settings σ = 0.45 and σ = 0.6  the case
ν : σ = 1 : 3 has much larger training error than others and is not included in the tail ﬁgures.

4.2 Constant Stepsize vs Diminishing Stepsize

In this subsection  we compare the error decay of TDC under diminishing stepsize with that of TDC
under four different constant stepsizes. For diminishing stepsize  we set cα = cβ and σ = 3
2 ν  and
tune their values to the best  which are given by cα = cβ = 1.8  σ = 3
2 ν = 0.45. For the four
constant-stepsize cases  we ﬁx α for each case  and tune β to the best. The resulting parameter settings
are respectively as follows: αt = 0.01  βt = 0.006; αt = 0.02  βt = 0.008; αt = 0.05  βt = 0.02;
and αt = 0.1  βt = 0.02. The results are reported in Figure 2  in which for both the training and
tracking errors  the left plot illustrates the overall iteration process and the right plot illustrates the
corresponding zoomed error tails. The results suggest that although some large constant stepsizes
(αt = 0.05  βt = 0.02 and αt = 0.1  βt = 0.02) yield initially faster convergence than diminishing
stepsize  they eventually oscillate around a large neighborhood of θ∗ due to the large tracking error.
Small constant stepsize (αt = 0.02  βt = 0.008 and αt = 0.01  βt = 0.006) can have almost the
same asymptotic accuracy as that under diminishing stepsize  but has very slow convergence rate. We
can also observe strong correlation between the training and tracking errors under constant stepsize 
i.e.  larger training error corresponds to larger tracking error  which corroborates Theorem 2 and
suggests that the accuracy of TDC heavily depends on the decay of the tracking error (cid:107)zt(cid:107)2.

(a) Training error (left: full; right: tail)

(b) Tracking error (left: full; right: tail)

Figure 2: Comparison between TDC updates under constant stepsizes and diminishing stepsize.

8

024681012# of iterations105050100150200250300Error(||t-*||2): = 1:3: = 1:2: = 5:9: = 2:3: = 5:6: = 1:11.11.121.141.161.181.2# of iterations1060.060.070.080.09Error(||t-*||2): = 1:3: = 1:2: = 5:9: = 2:3: = 5:6: = 1:1024681012# of iterations105050100150200250300Error(||t-*||2): = 1:3: = 1:2: = 5:9: = 2:3: = 5:6: = 1:11.11.121.141.161.181.2# of iterations1060.050.10.150.20.250.3Error(||t-*||2): = 1:3: = 1:2: = 5:9: = 2:3: = 5:6: = 1:1051015# of iterations105050100150200250300Error(||t-*||2): = 1:2: = 5:9: = 2:3: = 5:6: = 1:11.41.421.441.461.481.5# of iterations10600.050.10.150.20.25Error(||t-*||2): = 1:2: = 5:9: = 2:3: = 5:6: = 1:10123# of iterations106050100150200250300Error(||t-*||2): = 1:2: = 5:9: = 2:3: = 5:6: = 1:12.92.922.942.962.983# of iterations10600.050.10.150.20.25Error(||t-*||2): = 1:2: = 5:9: = 2:3: = 5:6: = 1:1012345# of iterations105050100150200250300Error(||t-*||2)t=0.01  t=0.006t=0.02  t=0.008t=0.05  t=0.02t=0.1  t=0.02Diminishing44.24.44.64.85# of iterations105010203040Error(||t-*||2)t=0.01  t=0.006t=0.02  t=0.008t=0.05  t=0.02t=0.1  t=0.02Diminishing012345# of iterations105010203040Error(||zt||2)t=0.01  t=0.006t=0.02  t=0.008t=0.05  t=0.02t=0.1  t=0.02Diminishing44.24.44.64.85# of iterations105010203040Error(||zt||2)t=0.01  t=0.006t=0.02  t=0.008t=0.05  t=0.02t=0.1  t=0.02Diminishing4.3 Blockwise Diminishing Stepsize

In this subsection  we compare the error decay of TDC under blockwise diminishing stepsize with
that of TDC under diminishing stepsize and constant stepsize. We use the best tuned parameter
settings as listed in Section 4.2 for the latter two algorithms  i.e.  cα = cβ = 1.8 and σ = 3
2 ν = 0.45
for diminishing stepsize  and αt = 0.1  βt = 0.02 for constant stepsize. We report our results in
Figure 3. It can be seen that TDC under blockwise diminishing stepsize converges faster than that
under diminishing stepsize and almost as fast as that under constant stepsize. Furthermore  TDC
under blockwise diminishing stepsize also has comparable training error as that under diminishing
stepsize. Since the stepsize decreases geometrically blockwisely  the algorithm approaches to a very
small neighborhood of θ∗ in the later blocks. We can also observe that the tracking error under
blockwise diminishing stepsize decreases rapidly blockwisely.

(a) Training error (left: full; right: tail)

(b) Tracking error (left: full; right: tail)

Figure 3: Comparison between TDC updates under blockwise diminishing stepsizes  diminishing stepsize and
constant stepsize

4.4 Robustness to Blocksize
In this subsection  we investigate the robustness of TDC under blockwise diminishing stepsize with
respect to the blocksize. We consider the same setting as in Section 4.3  and perturb all blocksizes by
certain percentages of the original blocksize suggested in the algorithm. It can be seen from Figure 4
that the error decay rate changes only very slightly even with a substantial change in the blocksize.

Figure 4: Comparison between TDC updates under blockwise diminishing stepsizes with different blocksizes.

5 Conclusion

In this work  we provided the ﬁrst non-asymptotic analysis for the two time-scale TDC algorithm
over Markovian sample path. We developed a novel technique to handle the accumulative tracking
error caused by the two time-scale update  using which we characterized the non-asymptotic conver-
gence rate with general diminishing stepsize and constant stepsize. We also proposed a blockwise
diminishing stepsize scheme for TDC and proved its convergence. Our experiments demonstrated the
performance advantage of such an algorithm over both the diminishing and constant stepsize TDC
algorithms. Our technique for non-asymptotic analysis of two time-scale algorithms can be applied to
studying other off-policy algorithms such as actor-critic [18] and gradient Q-learning algorithms [19].

Acknowledgment

The work of T. Xu and Y. Liang was supported in part by the U.S. National Science Foundation under
the grants CCF-1761506  ECCS-1818904  and CCF-1801855.

9

00.511.522.53# of iterations1050100200300400Error(||t-*||2)BlockwiseDiminishingConstant22.22.42.62.83# of iterations10505101520Error(||t-*||2)BlockwiseDiminishingConstant00.511.522.53# of iterations105050100150200Error(||zt||2)BlockwiseDiminishingConstant22.22.42.62.83# of iterations10505101520Error(||zt||2)BlockwiseDiminishingConstant00.511.52# of iterations105050100150200250300Error(||t-*||2)Original10%20%30%-10%-20%-30%References
[1] T. Archibald  K. McKinnon  and L. Thomas. On the generation of Markov decision processes.

Journal of the Operational Research Society  46(3):354–361  1995.

[2] L. Baird. Residual algorithms: Reinforcement learning with function approximation.

Machine Learning Proceedings  pages 30–37. Morgan Kaufmann  1995.

In

[3] J. Bhandari  D. Russo  and R. Singal. A ﬁnite time analysis of temporal difference learning with
linear function approximation. In Conference on Learning Theory (COLT)  pages 1691–1692 
2018.

[4] V. S. Borkar. Stochastic approximation: a dynamical systems viewpoint  volume 48. Springer 

2009.

[5] V. S. Borkar and S. P. Meyn. The ODE method for convergence of stochastic approximation

and reinforcement learning. Journal on Control and Optimization  38(2):447–469  2000.

[6] V. S. Borkar and S. Pattathil. Concentration bounds for two time scale stochastic approximation.
In Proc. Allerton Conference on Communication  Control  and Computing (Allerton)  pages
504–511  2018.

[7] G. Dalal  B. Szörényi  G. Thoppe  and S. Mannor. Finite sample analyses for TD (0) with

function approximation. In Proc. AAAI Conference on Artiﬁcial Intelligence  2018.

[8] G. Dalal  B. Szorenyi  G. Thoppe  and S. Mannor. Finite sample analysis of two-timescale
stochastic approximation with applications to reinforcement learning. In Proc. Conference on
Learning Theory (COLT)  2018.

[9] C. Dann  G. Neumann  and J. Peters. Policy evaluation with temporal differences: A survey and

comparison. The Journal of Machine Learning Research  15(1):809–883  2014.

[10] H. Gupta  R. Srikant  and L. Ying. Finite-time performance bounds and adaptive learning rate
selection for two time-scale reinforcement learning. To appear in Proc. Advances in Neural
Information Processing Systems (NeurIPS)  2019.

[11] B. Hu and U. A. Syed. Characterizing the exact behaviors of temporal difference learning
algorithms using Markov jump linear system theory. To appear in Proc. Advances in Neural
Information Processing Systems (NeurIPS)  2019.

[12] S. Kamal. On the convergence  lock-in probability and sample complexity of stochastic

approximation. Journal on Control and Optimization  48(8):5178–5192  2010.

[13] P. Karmakar and S. Bhatnagar. Dynamics of stochastic approximation with Markov iterate-
dependent noise with the stability of the iterates not ensured. arXiv preprint arXiv:1601.02217 
2016.

[14] P. Karmakar and S. Bhatnagar. Two time-scale stochastic approximation with controlled
Markov noise and off-policy temporal-difference learning. Mathematics of Operations Research 
43(1):130–151  2017.

[15] V. R. Konda  J. N. Tsitsiklis  et al. Convergence rate of linear two-time-scale stochastic

approximation. The Annals of Applied Probability  14(2):796–819  2004.

[16] B. Liu  J. Liu  M. Ghavamzadeh  S. Mahadevan  and M. Petrik. Finite-sample analysis of
proximal gradient TD algorithms. In Proc. Uncertainty in Artiﬁcial Intelligence (UAI)  pages
504–513. AUAI Press  2015.

[17] H. R. Maei. Gradient temporal-difference learning algorithms. PhD thesis  University of

Alberta  2011.

[18] H. R. Maei. Convergent actor-critic algorithms under off-policy training and function approxi-

mation. arXiv preprint arXiv:1802.07842  2018.

10

[19] H. R. Maei and R. S. Sutton. GQ (lambda): A general gradient algorithm for temporal-difference
prediction learning with eligibility traces. In Proc. Artiﬁcial General Intelligence (AGI). Atlantis
Press  2010.

[20] A. Mokkadem and M. Pelletier. Convergence rate and averaging of nonlinear two-time-scale
stochastic approximation algorithms. The Annals of Applied Probability  16(3):1671–1702 
2006.

[21] L. Y. R. Srikant. Finite-time error bounds for linear stochastic approximation and TD learning.

arXiv preprint arXiv:1902.00923  2019.

[22] A. Ramaswamy and S. Bhatnagar. Stability of stochastic approximations with “controlled
markov” noise and temporal difference learning. Transactions on Automatic Control  64(6):2614–
2620  2018.

[23] D. Silver  G. Lever  N. Heess  T. Degris  D. Wierstra  and M. Riedmiller. Deterministic policy

gradient algorithms. In Proc. International Conference on Machine Learning (ICML)  2014.

[24] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning 

3(1):9–44  1988.

[25] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press  2018.

[26] R. S. Sutton  H. R. Maei  D. Precup  S. Bhatnagar  D. Silver  C. Szepesvári  and E. Wiewiora.
Fast gradient-descent methods for temporal-difference learning with linear function approx-
imation. In Proc. International Conference on Machine Learning (ICML)  pages 993–1000 
2009.

[27] R. S. Sutton  C. Szepesvári  and H. R. Maei. A convergent o(n) algorithm for off-policy temporal-
difference learning with linear function approximation. Advances in Neural Information
Processing Systems (NIPS)  21(21):1609–1616  2008.

[28] V. Tadi´c. On the convergence of temporal-difference learning with linear function approximation.

Machine Learning  42(3):241–267  Mar 2001.

[29] V. B. Tadic. Almost sure convergence of two time-scale stochastic approximation algorithms.

In Proc. American Control Conference  volume 4  pages 3802–3807  2004.

[30] G. Thoppe and V. Borkar. A concentration bound for stochastic approximation via Alekseev’s

formula. Stochastic Systems  9(1):1–26  2019.

[31] J. N. Tsitsiklis and B. Van Roy. Analysis of temporal-diffference learning with function
approximation. In Proc. Advances in Neural Information Processing Systems (NIPS)  pages
1075–1081  1997.

[32] Y. Wang  W. Chen  Y. Liu  Z.-M. Ma  and T.-Y. Liu. Finite sample analysis of the GTD policy
evaluation algorithms in Markov setting. In Proc. Advances in Neural Information Processing
Systems (NIPS)  pages 5504–5513  2017.

[33] C. J. Watkins and P. Dayan. Q-learning. Machine Learning  8(3-4):279–292  1992.

[34] V. Yaji and S. Bhatnagar. Stochastic recursive inclusions in two timescales with non-additive

iterate dependent Markov noise. arXiv preprint arXiv:1611.05961  2016.

[35] T. Yang  Y. Yan  Z. Yuan  and R. Jin. Why does stagewise training accelerate convergence of

testing error over SGD? arXiv preprint arXiv:1812.03934  2018.

[36] H. Yu. On convergence of some gradient-based temporal-differences algorithms for off-policy

learning. arXiv preprint arXiv:1712.09652  2017.

[37] S. Zou  T. Xu  and Y. Liang. Finite-sample analysis for SARSA with linear function approx-
imation. To appear in Proc. Advances in Neural Information Processing Systems (NeurIPS) 
2019.

11

,Lingjiao Chen
Hongyi Wang
Jinman Zhao
Dimitris Papailiopoulos
Paraschos Koutris
Tengyu Xu
Shaofeng Zou
Yingbin Liang