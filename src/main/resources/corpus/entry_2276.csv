2018,Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions,Embedding complex objects as vectors in low dimensional spaces is a longstanding problem in machine learning. We propose in this work an extension of that approach  which consists in embedding objects as elliptical probability distributions  namely distributions whose densities have elliptical level sets. We endow these measures with the 2-Wasserstein metric  with two important benefits: (i) For such measures  the squared 2-Wasserstein metric has a closed form  equal to a weighted sum of the squared Euclidean distance between means and the squared Bures metric between covariance matrices. The latter is a Riemannian metric between positive semi-definite matrices  which turns out to be Euclidean on a suitable factor representation of such matrices  which is valid on the entire geodesic between these matrices. (ii) The 2-Wasserstein distance boils down to the usual Euclidean metric when comparing Diracs  and therefore provides a natural framework to extend point embeddings. We show that for these reasons Wasserstein elliptical embeddings are more intuitive and yield tools that are better behaved numerically than the alternative choice of Gaussian embeddings with the Kullback-Leibler divergence. In particular  and unlike previous work based on the KL geometry  we learn elliptical distributions that are not necessarily diagonal. We demonstrate the advantages of elliptical embeddings by using them for visualization  to compute embeddings of words  and to reflect entailment or hypernymy.,Generalizing Point Embeddings using the
Wasserstein Space of Elliptical Distributions

Boris Muzellec
CREST  ENSAE

boris.muzellec@ensae.fr

Marco Cuturi

Google Brain and CREST  ENSAE

cuturi@google.com

Abstract

Embedding complex objects as vectors in low dimensional spaces is a longstanding
problem in machine learning. We propose in this work an extension of that
approach  which consists in embedding objects as elliptical probability distributions 
namely distributions whose densities have elliptical level sets. We endow these
measures with the 2-Wasserstein metric  with two important beneﬁts: (i) For such
measures  the squared 2-Wasserstein metric has a closed form  equal to a weighted
sum of the squared Euclidean distance between means and the squared Bures
metric between covariance matrices. The latter is a Riemannian metric between
positive semi-deﬁnite matrices  which turns out to be Euclidean on a suitable factor
representation of such matrices  which is valid on the entire geodesic between
these matrices. (ii) The 2-Wasserstein distance boils down to the usual Euclidean
metric when comparing Diracs  and therefore provides a natural framework to
extend point embeddings. We show that for these reasons Wasserstein elliptical
embeddings are more intuitive and yield tools that are better behaved numerically
than the alternative choice of Gaussian embeddings with the Kullback-Leibler
divergence. In particular  and unlike previous work based on the KL geometry  we
learn elliptical distributions that are not necessarily diagonal. We demonstrate the
advantages of elliptical embeddings by using them for visualization  to compute
embeddings of words  and to reﬂect entailment or hypernymy.

1

Introduction

One of the holy grails of machine learning is to compute meaningful low-dimensional embeddings
for high-dimensional complex data. That ability has recently proved crucial to tackle more advanced
tasks  such as for instance: inference on texts using word embeddings [Mikolov et al.  2013b 
Pennington et al.  2014  Bojanowski et al.  2017]  improved image understanding [Norouzi et al. 
2014]  representations for nodes in large graphs [Grover and Leskovec  2016].
Such embeddings have been traditionally recovered by seeking isometric embeddings in lower
dimensional Euclidean spaces  as studied in [Johnson and Lindenstrauss  1984  Bourgain  1985].
Given n input points x1  . . .   xn  one seeks as many embeddings y1  . . .   yn in a target space Y = Rd
whose pairwise distances kyi  yjk2 do not depart too much from the original distances dX (xi  xj)
in the input space. Note that when d is restricted to be 2 or 3  these embeddings (yi)i provide a useful
way to visualize the entire dataset. Starting with metric multidimensional scaling (mMDS) [De Leeuw 
1977  Borg and Groenen  2005]  several approaches have reﬁned this intuition [Tenenbaum et al. 
2000  Roweis and Saul  2000  Hinton and Roweis  2003  Maaten and Hinton  2008]. More general
criteria  such as reconstruction error [Hinton and Salakhutdinov  2006  Kingma and Welling  2014];
co-occurence [Globerson et al.  2007]; or relational knowledge  be it in metric learning [Weinberger
and Saul  2009] or between words [Mikolov et al.  2013b] can be used to obtain vector embeddings.
In such cases  distances kyi  yjk2 between embeddings  or alternatively their dot-products hyi  yji

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

must comply with sophisticated desiderata. Naturally  more general and ﬂexible approaches in which
the embedding space Y needs not be Euclidean can be considered  for instance in generalized MDS
on the sphere [Maron et al.  2010]  on surfaces [Bronstein et al.  2006]  in spaces of trees [B˘adoiu
et al.  2007  Fakcharoenphol et al.  2003] or  more recently  computed in the Poincaré hyperbolic
space [Nickel and Kiela  2017].
Probabilistic Embeddings. Our work belongs to a recent trend  pioneered by Vilnis and McCallum 
who proposed to embed data points as probability measures in Rd [2015]  and therefore generalize
point embeddings. Indeed  point embeddings can be regarded as a very particular—and degenerate—
case of probabilistic embedding  in which the uncertainty is inﬁnitely concentrated on a single point (a
Dirac). Probability measures can be more spread-out  or event multimodal  and provide therefore an
opportunity for additional ﬂexibility. Naturally  such an opportunity can only be exploited by deﬁning
a metric  divergence or dot-product on the space (or a subspace thereof) of probability measures.
Vilnis and McCallum proposed to embed words as Gaussians endowed either with the Kullback-
Leibler (KL) divergence or the expected likelihood kernel [Jebara et al.  2004]. The Kullback-Leibler
and expected likelihood kernel on measures have  however  an important drawback: these geometries
do not coincide with the usual Euclidean metric between point embeddings when the variances of
these Gaussians collapse. Indeed  the KL divergence and the `2 distance between two Gaussians
diverges to 1 or saturates when the variances of these Gaussians become small. To avoid numerical
instabilities arising from this degeneracy  Vilnis and McCallum must restrict their work to diagonal
covariance matrices. In a concurrent approach  Singh et al. represent words as distributions over their
contexts in the optimal transport geometry [Singh et al.  2018].
Contributions. We propose in this work a new framework for probabilistic embeddings  in which
point embeddings are seamlessly handled as a particular case. We consider arbitrary families of
elliptical distributions  which subsume Gaussians  and also include uniform elliptical distributions 
which are arguably easier to visualize because of their compact support. Our approach uses the
2-Wasserstein distance to compare elliptical distributions. The latter can handle degenerate measures 
and both its value and its gradients admit closed forms [Gelbrich  1990]  either in their natural
Riemannian formulation  as well as in a more amenable local Euclidean parameterization. We
provide numerical tools to carry out the computation of elliptical embeddings in different scenarios 
both to optimize them with respect to metric requirements (as is done in multidimensional scaling)
or with respect to dot-products (as shown in our applications to word embeddings for entailment 
similarity and hypernymy tasks) for which we introduce a proxy using a polarization identity.
Notations S d
vectors x  y 2 Rd and a matrix M 2S d
kx  ck2
Rd  V is the Lebesgue measure on that subspace. M† is the pseudo inverse of M.

+) is the set of positive (resp. semi-)deﬁnite d ⇥ d matrices. For two
+  we write the Mahalanobis norm induced by M as
M = (x  c)T M(x  c) and |M| for det(M). For V an afﬁne subspace of dimension m of

++ (resp. S d

2 The Geometry of Elliptical Distributions in the Wasserstein Space

We recall in this section basic facts about elliptical distributions in Rd. We adopt a general formulation
that can handle measures supported on subspaces of Rd as well as Dirac (point) measures. That level
of generality is needed to provide a seamless connection with usual vector embeddings  seen in the
context of this paper as Dirac masses. We recall results from the literature showing that the squared
2-Wasserstein distance between two distributions from the same family of elliptical distributions is
equal to the squared Euclidean distance between their means plus the squared Bures metric between
their scale parameter scaled by a suitable constant.
Elliptically Contoured Densities.
In their simplest form  elliptical distributions can be seen
as generalizations of Gaussian multivariate densities in Rd: their level sets describe concentric
ellipsoids  shaped following a scale parameter C 2S d
++  and centered around a mean parameter
c 2 Rd [Cambanis et al.  1981]. The density at a point x of such distributions is f (kxckC1)/p|C|
where the generator function f is such thatRRd f (kxk2)dx = 1. Gaussians are recovered with f =
g  g(·) / e·/2 while uniform distributions on full rank ellipsoids result from f = u  u(·) / 1· 1.
Because the norm induced by C1 appears in formulas above  the scale parameter C must have full
rank for these deﬁnitions to be meaningful. Cases where C does not have full rank can however

2

appear when a probability measure is supported on an afﬁne subspace1 of Rd  such as lines in R2  or
even possibly a space of null dimension when the measure is supported on a single point (a Dirac
measure)  in which case its scale parameter C is 0. We provide in what follows a more general
approach to handle these degenerate cases.
Elliptical Distributions. To lift this limitation  several reformulations of elliptical distributions have
been proposed to handle degenerate scale matrices C of rank rk C < d. Gelbrich [1990  Theorem 2.4]
deﬁnes elliptical distributions as measures with a density w.r.t the Lebesgue measure of dimension
rk C  in the afﬁne space c + Im C  where the image of C is Im C def={Cx  x 2 Rd}. This approach
is intuitive  in that it reduces to describing densities in their relevant subspace. A more elegant
approach uses the parameterization provided by characteristic functions [Cambanis et al.  1981  Fang
et al.  1990]. In a nutshell  recall that the characteristic function of a multivariate Gaussian is equal
to (t) = eitT cg(tT Ct) where  as in the paragraph above  g(·) = e·/2. A natural generalization
to consider other elliptical distributions is therefore to consider for g other functions h of positive
type [Ushakov  1999  Theo.1.8.9]  such as the indicator function u above  and still apply them to the
same argument tT Ct. Such functions are called characteristic generators and fully determine  along
with a mean c and a scale parameter C  an elliptical measure. This parameterization does not require
the scale parameter C to be invertible  and therefore allows to deﬁne probability distributions that do
not have necessarily a density w.r.t to the Lebesgue measure in Rd. Both constructions are relatively
complex  and we refer the interested reader to these references for a rigorous treatment.
Rank Deﬁcient Elliptical Distributions and their Variances. For the purpose of this work  we
will only require the following result: the variance of an elliptical measure is equal to its scale
parameter C multiplied by a scalar that only depends on its characteristic generator. Indeed  given a
mean vector c 2 Rd  a scale semi-deﬁnite matrix C 2S d
+ and a characteristic generator function h 
we deﬁne µh c C to be the measure with char-
acteristic function t 7! eitT ch(tT Ct). In that
case  one can show that the covariance matrix of
µh c C is equal to its scale parameter C times a
constant ⌧h that only depends on h  namely

B3= 3 1

1
1 4 1

1 1 6

var(µh c C) = ⌧hC .

(1)

For Gaussians  the scale parameter C and its
covariance matrice coincide  that is ⌧g = 1. For
uniform elliptical distributions  one has ⌧u =
1/(d + 2): the covariance of a uniform distribu-
tion on the volume {c+Cx  x 2 Rd kxk = 1} 
such as those represented in Figure 1  is equal
to C/(d + 2).
The 2-Wasserstein Bures Metric A natural
metric for elliptical distributions arises from op-
timal transport (OT) theory. We refer interested
readers to [Santambrogio  2015  Peyré and Cu-
turi  2018] for exhaustive surveys on OT. Re-
call that for two arbitrary probability measures
µ  ⌫ 2P (Rd)  their squared 2-Wasserstein dis-
tance is equal to

W 2

2 (µ  ⌫) def=

inf

X⇠µ Y ⇠⌫

EkXY k2

2

.

A = 8 2 0
0 4

2 8 0
0

B1= vvT

B0 = 03⇥3

B2 = 8 5 0
0 0

5 8 0
0

Figure 1: Five measures from the family of uni-
form elliptical distributions in R3. Each mea-
sure has a mean (location) and scale parameter.
In this carefully selected example  the reference
measure (with scale parameter A) is equidistant
(according to the 2-Wasserstein metric) to the
four remaining measures  whose scale parameters
B0  B1  B2  B3 have ranks equal to their indices
(here  v = [3  7 2]T ).

This formula rarely has a closed form. However 
in the footsteps of Dowson and Landau [1982] who proved it for Gaussians  Gelbrich [1990] showed
that for ↵ def= µh a A and  def= µh b B in the same family Ph = {µh c C  c 2 Rd  C 2S d
+}  one has
(2)
2 + ⌧hB2(A  B)  

2 + B2(var ↵  var ) = ka  bk2

2 (↵  ) = ka  bk2
W 2

1For instance  the random variable Y in R2 obtained by duplicating the same normal random variable X in

R  Y = [X  X]  is supported on a line in R2 and has no density w.r.t the Lebesgue measure in R2.

3

where B2 is the (squared) Bures metric on S d
and studied recently in [Bhatia et al.  2018  Malagò et al.  2018] 

+  proposed in quantum information geometry [1969]

1
2 YX

1
2 )

1
2 ) .

B2(X  Y) def= Tr(X + Y  2(X

(3)
The factor ⌧h next to the rightmost term B2 in (2) arises from homogeneity of B2 in its arguments (3) 
which is leveraged using the identity in (1).
A few remarks (i) When both scale matrices A = diag dA and B = diag dB are diagonal 
2 (↵  ) is the sum of two terms: the usual squared Euclidean distance between their means  plus ⌧h
W 2
times the squared Hellinger metric between the diagonals dA  dB: H2(dA  dB) def= kpdApdBk2
2.
(ii) The distance W2 between two Diracs a  b is equal to the usual distance between vectors
ka  bk2. (iii) The squared distance W 2
2 between a Dirac a and a measure µh b B in Ph reduces
to ka  bk2 + ⌧hTrB. The distance between a point and an ellipsoid distribution therefore always
increases as the scale parameter of the latter increases. Although this point makes sense from the
quadratic viewpoint of W 2
2 of points x in the ellipsoid
that stand further away from a than b will dominate that brought by points x that are closer  see
Figure 3) this may be counterintuitive for applications to visualization  an issue that will be addressed
in Section 4. (iv) The W2 distance between two elliptical distributions in the same family Ph is always
ﬁnite  no matter how degenerate they are. This is illustrated in Figure 1 in which a uniform measure
µa A is shown to be exactly equidistant to four other uniform elliptical measures  some of which are
degenerate. However  as can be hinted by the simple example of the Hellinger metric  that distance
may not be differentiable for degenerate measures (in the same sense that (px  py)2 is deﬁned
at x = 0 but not differentiable w.r.t x). (v) Although we focus in this paper on uniform elliptical
distributions  notably because they are easier to plot and visualize  considering any other elliptical
family simply amounts to changing the constant ⌧h next to the Bures metric in (2). Alternatively 
increasing (or tuning) that parameter ⌧h simply amounts to considering elliptical distributions with
increasingly heavier tails.

2 (in which the quadratic contribution ka  xk2

3 Optimizing over the Space of Elliptical Embeddings

Our goal in this paper is to use the set of elliptical distributions endowed with the W2 distance as
an embedding space. To optimize objective functions involving W2 terms  we study in this section
several parameterizations of the parameters of elliptical distributions. Location parameters only
appear in the computation of W2 through their Euclidean metric  and offer therefore no particular
challenge. Scale parameters are more tricky to handle since they are constrained to lie in S d
+. Rather
than keeping track of scale parameters  we advocate optimizing directly on factors (square roots) of
such parameters  which results in simple Euclidean (unconstrained) updates reviewed below.
Geodesics for Elliptical Distributions When A and B have full rank  the geodesic from ↵ to  is
a curve of measures in the same family of elliptic distributions  characterized by location and scale
parameters c(t)  C(t)  where

c(t) = (1  t)a + tb; C(t) =(1  t)I + tTAB A(1  t)I + tTAB  

(4)
and where the matrix TAB is such that x ! TAB(x  a) + b is the so-called Brenier optimal
transportation map [1987] from ↵ to   given in closed form as 
2 A 1

(5)
and is the unique matrix such that B = TABATAB [Peyré and Cuturi  2018  Remark 2.30]. When
A is degenerate  such a curve still exists as long as Im B ⇢ Im A  in which case the expression
above is still valid using pseudo-inverse square roots A†/2 in place of the usual inverse square-root.
Differentiability in Riemannian Parameterization Scale parameters are restricted to lie on the
+. For such problems  it is well known that a direct gradient-and-project based optimization
cone S d
on scale parameters would prove too expensive. A natural remedy to this issue is to perform manifold
optimization [Absil et al.  2009]. Indeed  as in any Riemannian manifold  the Riemannian gradient
2 d2(x  y) is given by  logx y [Lee  1997]. Using the expressions of the exp and log given in
gradx
[Malagò et al.  2018]  we can show that minimizing 1
2 B2(A  B) using Riemannian gradient descent
corresponds to making updates of the form  with step length ⌘

TAB def= A 1

1
2 BA

1
2 )

2 (A

1

1

2  

A0 =(1  ⌘)I + ⌘TAB A(1  ⌘)I + ⌘TAB .

4

(6)

When 0  ⌘  1  this corresponds to considering a new point A0 closer to B along the Bures
geodesic between A and B. When ⌘ is negative or larger than 1  A0 no longer lies on this geodesic
but is guaranteed to remain PSD  as can be seen from (6). Figure 2 shows a W2 geodesic between
two measures µ0 and µ1  as well as its extrapolation following exactly the formula given in (4).
That ﬁgure illustrates that µt is not necessarily geodesic outside of the boundaries [0  1] w.r.t. three
relevant measures  because its metric derivative is smaller than 1 [Ambrosio et al.  2006  Theorem
1.1.2]. When negative steps are taken (for instance when the W 2
2 distance needs to be increased)  this
lack of geodisicity has proved difﬁcult to handle numerically for a simple reason: such updates may
lead to degenerate scale parameters A0  as illustrated around time t = 1.5 of the curve in Figure 2.
Another obvious drawback of Riemannian approaches is that they are not as well studied as simpler
non-constrained Euclidean problems  for which a plethora of optimization techniques are available.
This observations motivates an alternative Euclidean parameterization  detailed in the next paragraph.

Metric derivative on curve

µ2

µ3

µ0

µ1

)
1
µ

 
0
µ
(
2

W
/
|
t
d
/
2
W
d
|

1

0.95

0.9

0.85

µt → µ−2
µt → µ0
µt → µ1
µt → µ3

2

3

0.8

-2

-1

0
1
curve time

Figure 2: (left) Interpolation (µt)t between two measures µ0 and µ1 following the geodesic equa-
tion (4). The same formula can be used to interpolate on the left and right of times 0  1. Displayed
times are [2 1 .5  0  .25  .5  .75  1  1.5  2  3]. Note that geodesicity is not ensured outside of the
boundaries [0  1]. This is illustrated in the right plot displaying normalized metric derivatives of the
curve µt to four relevant points: µ0  µ1  µ2  µ3. The curve µt is not always locally geodesic  as can
be seen by the fact that the metric derivative is strictly smaller than 1 in several cases.
Differentiability in Euclidean Parameterization A canonical way to handle a PSD constraint for
A is to rewrite it in factor form A = LLT . In the particular case of the Bures metric  we show that
this simple parametrization comes without losing the geometric interest of manifold optimization 
while beneﬁting from simpler additive updates. Indeed  one can (see supplementary material) that the
gradient of the squared Bures metric has the following gradient:

(7)

1
2

rL

B2(A  B) =I  TAB L  with updates L0 =(1  ⌘)I + ⌘TAB L .

Links between Euclidean and Riemannian Parameterization The factor updates in (7) are exactly
equivalent to the Riemannian ones (6) in the sense that A0 = L0L0T . Therefore  by using a factor
parameterization we carry out updates that stay on the Riemannian geodesic yet only require linear
updates on L  independently of the factor L chosen to represent A (given a factor L of A  any
right-side multiplication of that matrix by a unitary matrix remains a factor of A).
When considering a general loss function L that take as arguments squared Bures distances  one can
also show that L is geodesically convex w.r.t. to scale matrices A if and only if it is convex in the
usual sense with respect to L  where A = LLT . Write now LB = TABL. One can recover that
LBLT

B = B. Therefore  expanding the expression B2 for the right term below we obtain

B2(A  B) = B2LLT   LBLT

B = B2⇣LLT   TABLTABLT⌘ = kL  TABLk2

F

Indeed  the Bures distance simply reduces to the Frobenius distance between two factors of A and B.
However these factors need to be carefully chosen: given L for A  the factor for B must be computed
according to an optimal transport map TAB.
Polarization between Elliptical Distributions Some of the applications we consider  such as
the estimation of word embeddings  are inherently based on dot-products. By analogy with the
polarization identity  hx  yi = (kx 0k2 +ky 0k2 kx yk2)/2  we deﬁne a Wasserstein-Bures
pseudo-dot-product  where 0 = µ0d 0d⇥d is the Dirac mass at 0 
[µa A : µb B]def= 1

2 (µa A  0) + W 2

1
2 BA

1
2 )

1
2

2W 2

2 (µb B  0)  W 2

2 (µa A  µb B) =ha  bi+Tr (A

5

Note that [· : ·] is not an actual inner product since the Bures metric is not Hilbertian  unless we
restrict ourselves to diagonal covariance matrices  in which case it is the the inner product between
(a pdA) and (b pdB). We use [µa A : µb B] as a similarity measure which has  however  some
regularity: one can show that when a  b are constrained to have equal norms and A and B equal
traces  then [µa A : µb B] is maximal when a = b and A = B. Differentiating all three terms in that
sum  the gradient of this pseudo dot-product w.r.t. A reduces to rA[µa A : µb B] = TAB.
Computational Aspects The computational bottleneck of gradient-based Bures optimization lies in
the matrix square roots and inverse square roots operations that arise when instantiating transport
maps T as in (5). A naive method using eigenvector decomposition is far too time-consuming  and
there is not yet  to the best of our knowledge  a straightforward way to perform it in batches on a
GPU. We propose to use Newton-Schulz iterations (Algorithm 1  see [Higham  2008  Ch. 6]) to
approximate these root computations. These iterations producing both a root and an inverse root
approximation  and  relying exclusively on matrix-matrix multiplications  stream efﬁciently on GPUs.
Another problem lies in the fact that numerous roots and inverse-roots are required to form map T.
To solve this  we exploit an alternative formula for TAB (proof in the supplementary material):

TAB = A 1

2 (A

1
2 BA

1
2 )

1

2 A 1

2 = B

1
2 (B

1
2 AB

1

2 ) 1

2 B

1
2 .

(8)

In a gradient update  both the loss and the gradient of the metric are needed. In our case  we can use
the matrix roots computed during loss evaluation and leverage the identity above to compute on a
budget the gradients with respect to either scale matrices A and B. Indeed  a naive computation of
rAB2(A  B) and rBB2(A  B) would require the knowledge of 6 roots:

1
2   B

1
2   (A

1
2 BA

1
2 )

1
2   (B

1
2 AB

1
2 )

A

1

2   A 1

2   and B 1

2

to compute the following transport maps

TAB = A 1

2 (A

1
2 BA

1
2 )

1

2 A 1

2   TBA = B 1

2 (B

1
2 AB

1
2 )

1

2 B 1

2  

namely four matrix roots and two matrix inverse roots. We can avoid computing those six matrices
using identity (8) and limit ourselves to two runs of Algorithm 1  to obtain the same quantities as

1
2   Z1

def= A

def= (A
{Y1
TAB = Z1Y2Z1  TBA = Y1Z2Y1 .

2} {Y2

def= A 1

1
2 BA

1
2 )

1
2   Z2

def= (A

1
2 BA

1

2 ) 1
2}

1  Zi

(1+✏)kAk

Algorithm 1 Newton-Schulz
Input: PSD matrix A  ✏> 0

Y A
  Z I
while not converged do
T (3I  ZY)/2
Y YT
Z TZ
end while

When computing the gradients of n ⇥ m squared Wasserstein dis-
2 (↵i  j) in parallel  one only needs to run n Newton-
tances W 2
Schulz algorithms (in parallel) to compute matrices (Yi
1)in 
and then n⇥ m Newton-Schulz algorithms to recover cross matrices
2   Zi j
Yi j
2 . On the other hand  using an automatic differentiation
framework would require an additional backward computation of
the same complexity as the forward pass evaluating computation of
the roots and inverse roots  hence requiring roughly twice as many
operations per batch.
Avoiding Rank Deﬁciency at Optimization Time Although
B2(A  B) is deﬁned for rank deﬁcient matrices A and B  it is
not differentiable with respect to these matrices if they are rank
deﬁcient. Indeed  as mentioned earlier  this can be compared to the
non-differentiability of the Hellinger metric  (px  py)2 when x
or y becomes 0  at which point if becomes not differentiable. If
Im B 6⇢ Im A  which is notably the case if rk B > rk A  then rAB2(A  B) no longer exists.
However  even in that case  rBB2(A  B) exists iff Im A ⇢ Im B. Since it would be cumbersome to
account for these subtleties in a large scale optimization setting  we propose to add a small common
regularization term to all the factor products considered for our embeddings  and set A" = LLT + "I
were "> 0 is a hyperparameter. This ensures that all matrices are full rank  and thus that all gradients
exist. Most importantly  all our derivations still hold with this regularization  and can be shown to

Y p(1 + ✏)kAkY

Zp(1+✏)kAk
Z 
verse square root Z

leave the method to compute the gradients w.r.t L unchanged  namely remain equal toI  TA"B L.

Output: square root Y  in-

6

4 Experiments

We discuss in this section several applications of elliptical embeddings. We ﬁrst consider a simple
mMDS type visualization task  in which elliptical distributions in d = 2 are used to embed isomet-
rically points in high dimension. We argue that for such purposes  a more natural way to visualize
ellipses is to use their precision matrices. This is due to the fact that the human eye somewhat acts in
the opposite direction to the Bures metric  as discussed in Figure 3. We follow with more advanced
experiments in which we consider the task of computing word embeddings on large corpora as a
testing ground  and equal or improve on the state-of-the-art.

Figure 3: (left) three points on the plane. (middle) isometric elliptic embedding with the Bures
metric: ellipses of a given color have the same respective distances as points on the left. Although the
mechanics of optimal transport indicate that the blue ellipsoid is far from the two others  in agreement
with the left plot  the human eye tends to focus on those areas that overlap (below the ellipsoid center)
rather than those far away areas (north-east area) that contribute more signiﬁcantly to the W2 distance.
(right) the precision matrix visualization  obtained by considering ellipses with the same axes but
inverted eigenvalues  agree better with intuition  since they emphasize that overlap and extension of
the ellipse means on the contrary that those axis contribute less to the increase of the metric.

Figure 4: Toy experiment: visualization of a dataset of 10 PISA scores for 35 countries in the OECD.
(left) MDS embeddings of these countries on the plane (right) elliptical embeddings on the plane
using the precision visualization discussed in Figure 3. The normalized stress with standard MDS
is 0.62. The stress with elliptical embeddings is close to 5e  3 after 1000 gradient iterations  with
random initializations for scale matrices (following a Standard Wishart with 4 degrees of freedom)
and initial means located on the MDS solution.

Visualizing Datasets Using Ellipsoids Multidimensional scaling [De Leeuw  1977] aims at em-
bedding points x1  . . .   xn in a ﬁnite metric space in a lower dimensional one by minimizing
In our case  this translates to the minimization of

the stress Pij(kxi  xjk  kyi  yjk)2.
LMDS(a1  . . . an  A1  . . .   An) = Pij(kxi  xjk  W2(µai Ai  µaj  Aj ))2. This objective can

be crudely minimized with a simple gradient descent approach operating on factors as advocated in
Section 3  as illustrated in a toy example carried out using data from OECD’s PISA study2.
Word Embeddings The skipgram model [Mikolov et al.  2013a] computes word embeddings in
a vector space by maximizing the log-probability of observing surrounding context words given
an input central word. Vilnis and McCallum [2015] extended this approach to diagonal Gaussian
embeddings using an energy whose overall principles we adopt here  adapted to elliptical distributions
with full covariance matrices in the 2-Wasserstein space. For every word w  we consider an input
(as a word) and an ouput (as a context) representation as an elliptical measure  denoted respectively
µw and ⌫w  both parameterized by a location vector and a scale parameter (stored in factor form).

2http://pisadataexplorer.oecd.org/ide/idepisa/

7

Figure 5: Precision matrix visualization of trained embeddings of a set of words on the plane spanned
by the two principal eigenvectors of the covariance matrix of “Bach”.

Table 1: Results for elliptical embed-
dings (evaluated using our cosine mix-
ture) compared to diagonal Gaussian em-
beddings trained with the seomoz pack-
age (evaluated using expected likelihood
cosine similarity as recommended by
Vilnis and McCallum).

W2G/45/C Ell/12/CM

Given a set R of positive word/context pairs of words
(w  c)  and for each input word a set N (w) of n negative
contexts words sampled randomly  we adapt Vilnis and
McCallum’s loss function to the W 2
2 distance to minimize
the following hinge loss:

X(w c)2R

24M  [µw : ⌫c] + 1

n Xc02N (w)

[µw : ⌫c0]35+

Dataset
SimLex
WordSim
WordSim-R
WordSim-S

MEN
MC
RG
YP

MT-287
MT-771

RW

25.09
53.45
61.70
48.99
65.16
59.48
69.77
37.18
61.72
57.63
40.14

24.09
66.02
71.07
60.58
65.58
65.95
65.58
25.14
59.53
56.78
29.04

where M > 0 is a margin parameter. We train our em-
beddings on the concatenated ukWaC and WaCkypedia
corpora [Baroni et al.  2009]  consisting of about 3 bil-
lion tokens  on which we keep only the tokens appearing
more than 100 times in the text (for a total number of
261583 different words). We train our embeddings using
adagrad [Duchi et al.  2011]  sampling one negative con-
text per positive context and  in order to prevent the norms
of the embeddings to be too highly correlated with the cor-
responding word frequencies (see Figure in supplementary
material)  we use two distinct sets of embeddings for the
input and context words.
We compare our full elliptical to diagonal Gaussian embeddings trained using the methods described
in [Vilnis and McCallum  2015] on a collection of similarity datasets by computing the Spearman
rank correlation between the similarity scores provided in the data and the scores we compute based
on our embeddings. Note that these results are obtained using context (⌫w) rather than input (µw)
embeddings. For a fair comparison across methods  we set dimensions by ensuring that the number
of free parameters remains the same: because of the symmetry in the covariance matrix  elliptical
embeddings in dimension d have d + d(d + 1)/2 free parameters (d for the means  d(d + 1)/2 for the
covariance matrices)  as compared with 2d for diagonal Gaussians. For elliptical embeddings  we use
the common practice of using some form of normalized quantity (a cosine) rather than the direct dot
product. We implement this here by computing the mean of two cosine terms  each corresponding
separately to mean and covariance contributions:

SB[µa A  µb B] := ha  bi
kakkbk

Tr (A

1
1
2 BA
2 )
pTrATrB

1
2

+

Using this similarity measure rather than the Wasserstein-Bures dot product is motivated by
the fact that the norms of the embeddings show some dependency with word frequencies (see
ﬁgures in supplementary) and become dominant when comparing words with different fre-
quencies scales. An alternative could have been obtained by normalizing the Wasserstein-
Bures dot product in a more standard way that pools together means and covariances. How-
this choice makes it harder to deal with
ever  as discussed in the supplementary material 
the variations in scale of the means and covariances 
therefore decreasing performance.

8

Model

W2G/45/Cosine

W2G/45/KL
Ell/12/CM

F1
0.74
0.74
0.73

AP
0.70
0.72
0.70

e[µu µv ]

e[µu µv ]+Pv02N (u) e[µu µv0 ] .

Table 2: Entailment benchmark: we evaluate our
embeddings on the Entailment dataset using aver-
age precision (AP) and F1 scores. The threshold
for F1 is chosen to be the best at test time.

We also evaluate our embeddings on the Entail-
ment dataset ([Baroni et al.  2012])  on which
we obtain results roughly comparable to those
of [Vilnis and McCallum  2015]. Note that con-
trary to the similarity experiments  in this frame-
work using the (unsymmetrical) KL divergence
makes sense and possibly gives an advantage 
as it is possible to choose the order of the argu-
ments in the KL divergence between the entail-
ing and entailed words.
Hypernymy In this experiment  we use the framework of [Nickel and Kiela  2017] on hypernymy
relationships to test our embeddings. A word A is said to be a hypernym of a word B if any B is a type
of A  e.g. any dog is a type of mammal  thus constituting a tree-like structure on nouns. The WORDNET
dataset [Miller  1995] features a transitive closure of 743 241 hypernymy relations on 82 115 distinct
nouns  which we consider as an undirected graph of relations R. Similarly to the skipgram model 
for each noun u we sample a ﬁxed number n of negative examples and store them in set N (u) to
optimize the following loss:P(u v)2R log

Figure 6: Reconstruction performance of our embeddings against
Poincare embeddings (reported from [Nickel and Kiela  2017] 
as we were not able to reproduce scores comparable to these
values) evaluated by mean retrieved rank (lower=better) and MAP
(higher=better).

We train the model using SGD
with only one set of embeddings.
The embeddings are then eval-
uated on a link reconstruction
task: we embed the full tree
and rank the similarity of each
positive hypernym pair (u  v)
among all negative pairs (u  v0)
and compute the mean rank thus
achieved as well as the mean av-
erage precision (MAP)  using the
Wasserstein-Bures dot product as
the similarity measure. Elliptical
embeddings consistently outper-
form Poincare embeddings for di-
mensions above a small thresh-
old  as shown in Figure 6  which
conﬁrms our intuition that the ad-
dition of a notion of variance or
uncertainty to point embeddings allows for a richer and more signiﬁcant representation of words.
Conclusion We have proposed to use the space of elliptical distributions endowed with the W2 metric
to embed complex objects. This latest iteration of probabilistic embeddings  in which a point an
object is represented as a probability measure  can consider elliptical measures (including Gaussians)
with arbitrary covariance matrices. Using the W2 metric we can provides a natural and seamless
generalization of point embeddings in Rd. Each embedding is described with a location c and a
scale C parameter  the latter being represented in practice using a factor matrix L  where C is
recovered as LLT . The visualization part of work is still subject to open questions. One may seek a
different method than that proposed here using precision matrices  and ask whether one can include
more advanced constraints on these embeddings  such as inclusions or the presence (or absence) of
intersections across ellipses. Handling multimodality using mixtures of Gaussians could be pursued.
In that case a natural upper bound on the W2 distance can be computed by solving the OT problem
between these mixtures of Gaussians using a simpler proxy: consider them as discrete measures
putting Dirac masses in the space of Gaussians endowed with the W2 metric as a ground cost  and
use the optimal cost of that proxy as an upper bound of their Wasserstein distance. Finally  note that
the set of elliptical measures µc C endowed with the Bures metric can also be interpreted  given that
C = LLT   L 2 Rd⇥k  and writing ˜li = li  ¯l for the centered column vectors of L  as a discrete
˜li)i endowed with a W2 metric only looking at their ﬁrst and second order
point cloud (c + 1pk
moments. These k points  whose mean and covariance matrix match c and C  can therefore fully
characterize the geometric properties of the distribution µc C  and may provide a simple form of
multimodal embedding.

9

References
P-A Absil  Robert Mahony  and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton

University Press  2009.

L. Ambrosio  N. Gigli  and G. Savaré. Gradient ﬂows in metric spaces and in the space of probability measures.

Springer  2006.

Marco Baroni  Silvia Bernardini  Adriano Ferraresi  and Eros Zanchetta. The wacky wide web: a collection
of very large linguistically processed web-crawled corpora. Language Resources and Evaluation  43(3):
209–226  September 2009.

Marco Baroni  Raffaella Bernardi  Ngoc-Quynh Do  and Chung-chieh Shan. Entailment above the word level in
distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association
for Computational Linguistics  pages 23–32. ACL  2012.

Rajendra Bhatia  Tanvi Jain  and Yongdo Lim. On the Bures-Wasserstein distance between positive deﬁnite

matrices. Expositiones Mathematicae  2018.

Piotr Bojanowski  Edouard Grave  Armand Joulin  and Tomas Mikolov. Enriching word vectors with subword

information. Transactions of the Association for Computational Linguistics  5:135–146  2017.

Ingwer Borg and Patrick JF Groenen. Modern multidimensional scaling: Theory and applications. Springer

Science & Business Media  2005.

Jean Bourgain. On Lipschitz embedding of ﬁnite metric spaces in Hilbert space. Israel Journal of Mathematics 

52(1):46–52  1985.

Yann Brenier. Décomposition polaire et réarrangement monotone des champs de vecteurs. CR Acad. Sci. Paris

Sér. I Math  305(19):805–808  1987.

Alexander M Bronstein  Michael M Bronstein  and Ron Kimmel. Generalized multidimensional scaling: a
framework for isometry-invariant partial surface matching. Proceedings of the National Academy of Sciences 
103(5):1168–1172  2006.

Elia Bruni  Nam Khanh Tran  and Marco Baroni. Multimodal distributional semantics. J. Artif. Int. Res.  49(1):

1–47  January 2014.

Mihai B˘adoiu  Piotr Indyk  and Anastasios Sidiropoulos. Approximation algorithms for embedding general
metrics into trees. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms 
pages 512–521. Society for Industrial and Applied Mathematics  2007.

Donald Bures. An extension of Kakutani’s theorem on inﬁnite product measures to the tensor product of

semiﬁnite w*-algebras. Transactions of the American Mathematical Society  135:199–212  1969.

Stamatis Cambanis  Steel Huang  and Gordon Simons. On the theory of elliptically contoured distributions.

Journal of Multivariate Analysis  11(3):368 – 385  1981.

Jan De Leeuw. Applications of convex analysis to multidimensional scaling. In Recent Developments in

Statistics  1977.

DC Dowson and BV Landau. The Fréchet distance between multivariate normal distributions. Journal of

multivariate analysis  12(3):450–455  1982.

John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning and stochastic

optimization. Journal of Machine Learning Research  12(Jul):2121–2159  2011.

Jittat Fakcharoenphol  Satish Rao  and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree
metrics. In Proceedings of the thirty-ﬁfth annual ACM symposium on Theory of computing  pages 448–455.
ACM  2003.

KT Fang  S Kotz  and KW Ng. Symmetric Multivariate and Related Distributions. Chapman and Hall/CRC 

1990.

Lev Finkelstein  Evgeniy Gabrilovich  Yossi Matias  Ehud Rivlin  Zach Solan  Gadi Wolfman  and Eytan Ruppin.

Placing search in context: the concept revisited. ACM Trans. Inf. Syst.  20(1):116–131  2002.

Matthias Gelbrich. On a formula for the l2 Wasserstein metric between measures on Euclidean and Hilbert

spaces. Mathematische Nachrichten  147(1):185–203  1990.

10

Amir Globerson  Gal Chechik  Fernando Pereira  and Naftali Tishby. Euclidean embedding of co-occurrence

data. Journal of Machine Learning Research  8(Oct):2265–2295  2007.

Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd
ACM SIGKDD international conference on Knowledge discovery and data mining  pages 855–864. ACM 
2016.

Guy Halawi  Gideon Dror  Evgeniy Gabrilovich  and Yehuda Koren. Large-scale learning of word relatedness
with constraints. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining  KDD ’12  pages 1406–1414  New York  NY  USA  2012. ACM.

Nicholas J. Higham. Functions of Matrices: Theory and Computation (Other Titles in Applied Mathematics).

Society for Industrial and Applied Mathematics  Philadelphia  PA  USA  2008.

Felix Hill  Roi Reichart  and Anna Korhonen. Simlex-999: Evaluating semantic models with genuine similarity

estimation. Comput. Linguist.  41(4):665–695  December 2015.

Geoffrey E Hinton and Sam T Roweis. Stochastic neighbor embedding. In Advances in Neural Information

Processing Systems  pages 857–864  2003.

Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science  313(5786):504–507  2006.

Tony Jebara  Risi Kondor  and Andrew Howard. Probability product kernels. Journal of Machine Learning

Research  5:819–844  2004.

William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. In
Conference in modern analysis and probability (New Haven  Conn.  1982)  volume 26 of Contemp. Math. 
pages 189–206. Amer. Math. Soc.  Providence  RI  1984.

Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the International

Conference on Learning Representations  2014.

J.M. Lee. Riemannian Manifolds: An Introduction to Curvature. Graduate Texts in Mathematics. Springer New

York  1997.

Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning

Research  9(Nov):2579–2605  2008.

Luigi Malagò  Luigi Montrucchio  and Giovanni Pistone. Wasserstein-Riemannian geometry of positive-deﬁnite

matrices. arXiv preprint arXiv:1801.09269  2018.

Yariv Maron  Michael Lamar  and Elie Bienenstock. Sphere embedding: An application to part-of-speech

induction. In Advances in Neural Information Processing Systems  pages 1567–1575  2010.

Tomas Mikolov  Kai Chen  Greg Corrado  and Jeffrey Dean. Efﬁcient estimation of word representations in

vector space. ICLR Workshop  2013a.

Tomas Mikolov  Ilya Sutskever  Kai Chen  Greg S Corrado  and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems  pages
3111–3119  2013b.

George A. Miller. Wordnet: A lexical database for english. Commun. ACM  38(11):39–41  November 1995.

George A. Miller and Walter G. Charles. Contextual correlates of semantic similarity. Language and Cognitive

Processes  6(1):1–28  1991.

Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. In
I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors 
Advances in Neural Information Processing Systems 30  pages 6341–6350. Curran Associates  Inc.  2017.

Mohammad Norouzi  Tomas Mikolov  Samy Bengio  Yoram Singer  Jonathon Shlens  Andrea Frome  Greg
Corrado  and Jeffrey Dean. Zero-shot learning by convex combination of semantic embeddings. In Proceedings
of the International Conference on Learning Representations  2014.

Jeffrey Pennington  Richard Socher  and Christopher Manning. Glove: Global vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)  pages
1532–1543  2014.

11

Gabriel Peyré and Marco Cuturi. Computational optimal transport. arXiv preprint arXiv:1803.00567  2018.

Kira Radinsky  Eugene Agichtein  Evgeniy Gabrilovich  and Shaul Markovitch. A word at a time: Computing
word relatedness using temporal semantic analysis. In Proceedings of the 20th International Conference on
World Wide Web  WWW ’11  pages 337–346  New York  NY  USA  2011. ACM.

Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science 

290(5500):2323–2326  2000.

Herbert Rubenstein and John B. Goodenough. Contextual correlates of synonymy. Commun. ACM  8(10):

627–633  October 1965.

Filippo Santambrogio. Optimal Transport for Applied Mathematicians. Birkhauser  2015.

Sidak Pal Singh  Andreas Hug  Aymeric Dieuleveut  and Martin Jaggi. Context mover’s distance & barycenters:

Optimal transport of contexts for building representations. arXiv preprint arXiv:1808.09663  2018.

Joshua B Tenenbaum  Vin De Silva  and John C Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290(5500):2319–2323  2000.

Minh thang Luong  Richard Socher  and Christopher D. Manning. Better word representations with recursive
neural networks for morphology. In In Proceedings of the Thirteenth Annual Conference on Natural Language
Learning. Tomas Mikolov  Wen-tau  2013.

Nikolai G Ushakov. Selected topics in characteristic functions. Walter de Gruyter  1999.

Luke Vilnis and Andrew McCallum. Word representations via Gaussian embedding. Proceedings of the

International Conference on Learning Representations  2015. arXiv preprint arXiv:1412.6623.

K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. The

Journal of Machine Learning Research  10:207–244  2009.

Dongqiang Yang and David M. W. Powers. Measuring semantic similarity in the taxonomy of wordnet. In
Proceedings of the Twenty-eighth Australasian Conference on Computer Science - Volume 38  ACSC ’05 
pages 315–322  Darlinghurst  Australia  Australia  2005. Australian Computer Society  Inc.

12

,Boris Muzellec
Marco Cuturi