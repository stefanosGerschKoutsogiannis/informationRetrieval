2019,A Generic Acceleration Framework for Stochastic Composite Optimization,In this paper  we introduce various mechanisms to obtain accelerated first-order stochastic optimization algorithms when the objective function is convex or strongly convex. Specifically  we extend the Catalyst approach originally designed for deterministic objectives to the stochastic setting. Given an optimization method with mild convergence guarantees for strongly convex problems  the challenge is to accelerate convergence to a noise-dominated region  and then achieve convergence with an optimal worst-case complexity depending on the noise variance of the gradients. A side contribution of our work is also a generic analysis that can handle inexact proximal operators  providing new insights about the robustness of stochastic algorithms when the proximal operator cannot be exactly computed.,A Generic Acceleration Framework

for Stochastic Composite Optimization

Andrei Kulunchakov and Julien Mairal

Univ. Grenoble Alpes  Inria  CNRS  Grenoble INP  LJK  38000 Grenoble  France

❛♥❞r❡✐✳❦✉❧✉♥❝❤❛❦♦✈❅✐♥r✐❛✳❢r ❛♥❞ ❥✉❧✐❡♥✳♠❛✐r❛❧❅✐♥r✐❛✳❢r

Abstract

In this paper  we introduce various mechanisms to obtain accelerated ﬁrst-order
stochastic optimization algorithms when the objective function is convex or strongly
convex. Speciﬁcally  we extend the Catalyst approach originally designed for
deterministic objectives to the stochastic setting. Given an optimization method
with mild convergence guarantees for strongly convex problems  the challenge is to
accelerate convergence to a noise-dominated region  and then achieve convergence
with an optimal worst-case complexity depending on the noise variance of the
gradients. A side contribution of our work is also a generic analysis that can
handle inexact proximal operators  providing new insights about the robustness of
stochastic algorithms when the proximal operator cannot be exactly computed.

1

Introduction

In this paper  we consider stochastic composite optimization problems of the form

min

x∈Rp {F (x) := f (x) + ψ(x)} with

f (x) = Eξ[ ˜f (x  ξ)] 

(1)

where the function f is convex  or µ-strongly convex  and L-smooth (meaning differentiable with
L-Lipschitz continuous gradient)  and ψ is a possibly non-smooth convex lower-semicontinuous
function. For instance  ψ may be the ℓ1-norm  which is known to induce sparsity  or an indicator
function of a convex set [21]. The random variable ξ corresponds to data samples. When the amount
of training data is ﬁnite  the expectation Eξ[ ˜f (x  ξ)] can be replaced by a ﬁnite sum  a setting that
has attracted a lot of attention in machine learning recently  see  e.g.  [13  14  19  25  35  42  53] for
incremental algorithms and [1  26  30  33  47  55  56] for accelerated variants.

Yet  as noted in [8]  one is typically not interested in the minimization of the empirical risk—that is 
a ﬁnite sum of functions—with high precision  but instead  one should focus on the expected risk
involving the true (unknown) data distribution. When one can draw an inﬁnite number of samples
from this distribution  the true risk (1) may be minimized by using appropriate stochastic optimization
techniques. Unfortunately  fast methods designed for deterministic objectives would not apply to
this setting; methods based on stochastic approximations admit indeed optimal “slow” rates that are

typically O(1/√k) for convex functions and O(1/k) for strongly convex ones  depending on the
exact assumptions made on the problem  where k is the number of noisy gradient evaluations [38].

Better understanding the gap between deterministic and stochastic optimization is one goal of this
paper. Speciﬁcally  we are interested in Nesterov’s acceleration of gradient-based approaches [39  40].
In a nutshell  gradient descent or its proximal variant applied to a µ-strongly convex L-smooth
function achieves an exponential convergence rate O((1 − µ/L)k) in the worst case in function
values  and a sublinear rate O(L/k) if the function is simply convex (µ = 0). By interleaving the
algorithm with clever extrapolation steps  Nesterov showed that faster convergence could be achieved 

and the previous convergence rates become O((1 −pµ/L)k) and O(L/k2)  respectively. Whereas

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

no clear geometrical intuition seems to appear in the literature to explain why acceleration occurs 
proof techniques to show accelerated convergence [5  40  50] and extensions to a large class of other
gradient-based algorithms are now well established [1  10  33  41  47].

Yet  the effect of Nesterov’s acceleration to stochastic objectives remains poorly understood since
existing unaccelerated algorithms such as stochastic mirror descent [38] and their variants already
achieve the optimal asymptotic rate. Besides  negative results also exist  showing that Nesterov’s
method may be unstable when the gradients are computed approximately [12  16]. Nevertheless 
several approaches such as [4  11  15  17  18  23  28  29  52] have managed to show that acceleration
may be useful to forget faster the algorithm’s initialization and reach a region dominated by the
noise of stochastic gradients; then  “good” methods are expected to asymptotically converge with
a rate exhibiting an optimal dependency in the noise variance [38]  but with no dependency on the
initialization. A major challenge is then to achieve the optimal rate for these two regimes.
In this paper  we consider an optimization method M with the following property: given an auxiliary
strongly convex objective function h  we assume that M is able to produce iterates (zt)t≥0 with
expected linear convergence to a noise-dominated region—that is  such that

E[h(zt) − h⋆] ≤ C(1 − τ )t(h(z0) − h⋆) + Bσ2 

(2)

where C  τ  B > 0  h⋆ is the minimum function value  and σ2 is an upper bound on the variance of
stochastic gradients accessed by M  which we assume to be uniformly bounded. Whereas such an
assumption has limitations  it remains the most standard one for stochastic optimization (see [9  43]
for more realistic settings in the smooth case). The class of methods satisfying (2) is relatively large.
For instance  when h is L-smooth  the stochastic gradient descent method (SGD) with constant step
size 1/L and iterate averaging satisﬁes (2) with τ = µ/L  B = 1/L  and C = 1  see [28].

Main contribution.

In this paper  we extend the Catalyst approach [33] to general stochastic
problems.1 Under mild conditions  our approach is able to turn M into a converging algorithm with a
worst-case expected complexity that decomposes into two parts: the ﬁrst one exhibits an accelerated
convergence rate in the sense of Nesterov and shows how fast one forgets the initial point; the second
one corresponds to the stochastic regime and typically depends (optimally in many cases) on σ2.
Note that even though we only make assumptions about the behavior of M on strongly convex
sub-problems (2)  we also treat the case where the objective (1) is convex  but not strongly convex.

To illustrate the versatility of our approach  we consider the stochastic ﬁnite-sum problem [7  22  31 
54]  where the objective (1) decomposes into n components ˜f (x  ξ) = 1
˜fi(x  ξ) and ξ is a
stochastic perturbation  coming  e.g.  from data augmentation or noise injected during training to
improve generalization or privacy (see [28  35]). The underlying ﬁnite-sum structure may also result
from clustering assumptions on the data [22]  or from distributed computing [31]  a setting beyond
the scope of our paper. Whereas it was shown in [28] that classical variance-reduced stochastic
optimization methods such as SVRG [53]  SDCA [47]  SAGA [13]  or MISO [35]  can be made robust
to noise  the analysis of [28] is only able to accelerate the SVRG approach. With our acceleration
technique  all of the aforementioned methods can be modiﬁed such that they ﬁnd a point ˆx satisfying
E[F (ˆx) − F ⋆] ≤ ε with global iteration complexity  for the µ-strongly convex case 

nPn

i=1

˜O n +sn

L

µ! log(cid:18) F (x0) − F ⋆

ε

(cid:19) +

σ2

µε! .

(3)

The term on the left is the optimal complexity for ﬁnite-sum optimization [1  2]  up to logarithmic
terms in L  µ hidden in the ˜O(.) notation  and the term on the right is the optimal complexity for
µ-strongly convex stochastic objectives [17] where σ2 is due to the perturbations ξ. As Catalyst [33] 
the price to pay compared to non-generic direct acceleration techniques [1  28] is a logarithmic factor.

Other contributions.
In this paper  we generalize the analysis of Catalyst [33  44] to handle
various new cases. Beyond the ability to deal with stochastic optimization problems  our approach (i)
improves Catalyst by allowing sub-problems of the form (2) to be solved approximately in expectation 
which is more realistic than the deterministic requirement made in [33] and which is also critical

1All objectives addressed by the original Catalyst approach are deterministic  even though they may be large

ﬁnite sums. Here  we consider general expectations as deﬁned in (1).

2

for stochastic optimization  (ii) leads to a new accelerated stochastic gradient descent algorithms
for composite optimization with similar guarantees as [17  18  28]  (iii) handles the analysis of
accelerated proximal gradient descent methods with inexact computation of proximal operators 
improving the results of [45] while also treating the stochastic setting.

Finally  we note that the extension of Catalyst we propose is easy to implement. The original Catalyst
method introduced in [32] indeed required solving a sequence of sub-problems while controlling
carefully the convergence  e.g.  with duality gaps. For this reason  Catalyst has sometimes been seen as
theoretically appealing but not practical enough [46]. Here  we focus on a simpler and more practical
variant presented later in [33]  which consists of solving sub-problems with a ﬁxed computational
budget  thus removing the need to deﬁne stopping criterions for sub-problems. The code used for our
experiments is available here: ❤tt♣✿✴✴❣✐t❤✉❜✳❝♦♠✴❑✉❧✉❆♥❞r❡❥✴◆■P❙✲✷✵✶✾✲❝♦❞❡.

2 Related Work on Inexact and Stochastic Proximal Point Methods.

Catalyst is based on the inexact accelerated proximal point algorithm [20]  which consists in solving
approximately a sequence of sub-problems and updating two sequences (xk)k≥0 and (yk)k≥0 by

xk ≈ argmin

x∈Rp nhk(x) := F (x) +

κ

2kx − yk–1k2o and

yk = xk + βk(xk − xk–1) 

(4)

where βk in (0  1) is obtained from Nesterov’s acceleration principles [40]  κ is a well chosen
regularization parameter  and k · k2 is the Euclidean norm. The method M is used to obtain an
approximate minimizer of hk; when M converges linearly  it may be shown that the resulting
algorithm (4) enjoys a better worst-case complexity than if M was used directly on f   see [33].
Since asymptotic linear convergence is out of reach when f is a stochastic objective  a classical
strategy consists in replacing F (x) in (4) by a ﬁnite-sum approximation obtained by random sampling 
leading to deterministic sub-problems. Typically without Nesterov’s acceleration (with yk = xk) 
this strategy is often called the stochastic proximal point method [3  6  27  48  49]. The point of view
we adopt in this paper is different and is based on the minimization of surrogate functions hk related
to (4)  but which are more general and may take other forms than F (x) + κ

2kx − yk–1k2.

3 Preliminaries: Basic Multi-Stage Schemes

In this section  we present two simple multi-stage mechanisms to improve the worst-case complexities
of stochastic optimization methods  before introducing acceleration principles.

Basic restart with mini-batching or decaying step sizes. Consider an optimization method M
with convergence rate (2) and assume that there exists a hyper-parameter to control a trade-off
between the bias Bσ2 and the computational complexity. Speciﬁcally  we assume that the bias can be
reduced by an arbitrary factor η < 1  while paying a factor 1/η in terms of complexity per iteration
(or τ may be reduced by a factor η  thus slowing down convergence). This may occur in two cases:

• by using a mini-batch of size 1/η to sample gradients  which replaces σ2 by ησ2;
• or the method uses a step size proportional to η that can be chosen arbitrarily small.

For instance  stochastic gradient descent with constant step size and iterate averaging is compatible
with both scenarios [28]. Then  consider a target accuracy ε and deﬁne the sequences ηk = 1/2k
and εk = 2Bσ2ηk for k ≥ 0. We may now solve successively the problem up to accuracy εk—e.g. 
with a constant number O(1/τ ) steps of M when using mini-batches of size 1/ηk = 2k to reduce
the bias—and by using the solution of iteration k–1 as a warm restart. As shown in Appendix B  the
scheme converges and the worst-case complexity to achieve the accuracy ε in expectation is

O(cid:18) 1

τ

log(cid:18) C(F (x0) − F ⋆)

ε

(cid:19) +

Bσ2 log(2C)

τ ε

(cid:19) .

(5)

For instance  one may run SGD with constant step size ηk/L at stage k with iterate averaging as
in [28]  which yields B = 1/L  C = 1  and τ = µ/L. Then  the left term is the classical complexity
O((L/µ) log(1/ε)) of the (unaccelerated) gradient descent algorithm for deterministic objectives 
whereas the right term is the optimal complexity for stochastic optimization in O(σ2/µε). Similar
restart principles appear for instance in [4] in the design of a multistage accelerated SGD algorithm.

3

Restart: from sub-linear to linear rate with strong convexity. A natural question is whether
asking for a linear rate in (2) for strongly convex problems is a strong requirement. Here  we show
that a sublinear rate is in fact sufﬁcient for our needs by generalizing a restart technique introduced
in [18] for stochastic optimization  which was previously used for deterministic objectives in [24].
Speciﬁcally  consider an optimization method M such that the convergence rate (2) is replaced by
(6)

Dkz0 − z⋆k2

Bσ2

+

 

E[h(zt) − h⋆] ≤

2td

2

where D  d > 0 and z⋆ is a minimizer of h. Assume now that h is µ-strongly convex with D ≥ µ
and consider restarting s times the method M  each time running M for constant t′ = ⌈(2D/µ)1/d⌉
iterations. Then  it may be shown (see Appendix B) that the relation (2) holds with constant t = st′ 
τ = 1
2t′   and C = 1. If a mini-batch or step size mechanism is available  we may then proceed
as before and obtain a converging scheme with complexity (5)  e.g.  by using mini-batches of
exponentially increasing sizes once the method reaches a noise-dominated region  and by using a
restart frequency of order O(1/τ ).

4 Generic Multi-Stage Approaches with Acceleration

We are now in shape to introduce a generic acceleration framework that generalizes (4). Speciﬁcally 
given some point yk–1 at iteration k  we consider a surrogate function hk related to a parameter κ > 0 
an approximation error δk ≥ 0  and an optimization method M that satisfy the following properties:

(H1) hk is (κ + µ)-strongly convex  where µ is the strong convexity parameter of f ;
(H2) E[hk(x)|Fk–1] ≤ F (x) + κ
(H3) M can provide the exact minimizer x⋆

2kx − yk–1k2 for x = αk–1x⋆ + (1 − αk–1)xk–1  which is
deteministic given the past information Fk–1 up to iteration k–1 and αk–1 is given in Alg. 1;
k) such that
E[F (xk)] ≤ E[h⋆

k of hk and a point xk (possibly equal to x⋆

k] + δk where h⋆

k = minx hk(x).

The generic acceleration framework is presented in Algorithm 1. Note that the conditions on hk
bear similarities with estimate sequences introduced by Nesterov [40]; indeed  (H3) is a direct
generalization of (2.2.2) from [40] and (H2) resembles (2.2.1). However  the choices of hk and the
proof technique are signiﬁcantly different  as we will see with various examples below. We also
assume at the moment that the exact minimizer x⋆
k of hk is available  which differs from the Catalyst
framework [33]; the case with approximate minimization will be presented in Section 4.1.

eter for hk); K (number of iterations); (δk)k≥0 (approximation errors);
µ+κ ; α0 = 1 if µ = 0 or α0 = √q if µ 6= 0;

Algorithm 1 Generic Acceleration Framework with Exact Minimization of hk
1: Input: x0 (initial estimate); M (optimization method); µ (strong convexity constant); κ (param-
2: Initialization: y0 = x0; q = µ
3: for k = 1  . . .   K do
4:
5:
6:

Consider a surrogate hk satisfying (H1)  (H2) and obtain xk  x⋆
Compute αk in (0  1) by solving the equation α2
k = (1 − αk)α2
Update the extrapolated sequence

k using M satisfying (H3);
k–1 + qαk.

yk = x⋆

k + βk(x⋆

k − xk–1) +

(κ + µ)(1 − αk)

κ

(xk − x⋆

k) with βk =

αk–1(1 − αk–1)

α2

k–1 + αk

. (7)

7: end for
8: Output: xk (ﬁnal estimate).

Proposition 1 (Convergence analysis for Algorithm 1). Consider Algorithm 1. Then 

E[F (xk) − F ⋆] ≤


(1 − √q)k(cid:16)2(F (x0) − F ⋆) +Pk
(k+1)2 (cid:16)κkx0 − x⋆k2 +Pk

j=1 δj(j + 1)2(cid:17)

2

j=1(1 − √q)−jδj(cid:17) if µ 6= 0

otherwise

.

(8)

The proof of the proposition is given in Appendix C and is based on an extension of the analysis of
Catalyst [33]. Next  we present various application cases leading to algorithms with acceleration.

4

Accelerated proximal gradient method. When f is deterministic and the proximal operator of ψ
(see Appendix A for the deﬁnition) can be computed in closed form  choose κ = L − µ and deﬁne
(9)

hk(x) := f (yk–1) + ∇f (yk–1)⊤(x − yk–1) +
Consider M that minimizes hk in closed form: xk = x⋆
L∇f (yk–1)(cid:3). Then 
(H1) is obvious; (H2) holds from the convexity of f   and (H3) with δk = 0 follows from classical
inequalities for L-smooth functions [40]. Finally  we recover accelerated convergence rates [5  40].

k = Proxψ/L(cid:2)yk–1 − 1

L
2 kx − yk–1k2 + ψ(x).

Accelerated proximal point algorithm. We consider hk given in (4) with exact minimization (thus
an unrealistic setting  but conceptually interesting) with κ = L − µ. Then  the assumptions (H1) 
(H2)  and (H3) are satisﬁed with δk = 0 and we recover the accelerated rates of [20].
Accelerated stochastic gradient descent with prox. A more interesting choice of surrogate is

hk(x) := f (yk–1) + g⊤k (x − yk–1) +

κ + µ

2

kx − yk–1k2 + ψ(x) 

(10)

where κ ≥ L − µ and gk is an unbiased estimate of ∇f (yk–1)—that is  E[gk|Fk–1] = ∇f (yk–1)—
with variance bounded by σ2  following classical assumptions from the stochastic optimization
literature [17  18  23]. Then  (H1) and (H2) are satisﬁed given that f is convex. To characterize (H3) 
consider M that minimizes hk in closed form: xk = x⋆
κ+µ gk]  and deﬁne
uk–1 := Proxψ/(κ+µ)[yk–1 − 1
κ+µ∇f (yk–1)]  which is deterministic given Fk–1. Then  from (10) 

k = Proxψ/(κ+µ)[yk–1 − 1

F (xk) ≤ hk(xk) + (∇f (yk–1) − gk)⊤(xk − yk–1)

(from L-smoothness of f )

= h⋆

k + (∇f (yk–1) − gk)⊤(xk − uk–1) + (∇f (yk–1) − gk)⊤(uk–1 − yk–1).

When taking expectations  the last term on the right disappears since E[gk|Fk–1] = ∇f (yk–1):

E[F (xk)] ≤ E[h⋆
≤ E[h⋆

1

k] + E[kgk − ∇f (yk–1)kkxk − uk–1k]
k] +

E(cid:2)kgk − ∇f (yk–1)k2(cid:3) ≤ E[h⋆

κ + µ

k] +

σ2

κ + µ

 

(11)

k

where we used the non-expansiveness of the proximal operator [37]. Therefore  (H3) holds with
δk = σ2/(κ + µ). The resulting algorithm is similar to [28] and offers the same guarantees. The
novelty of our approach is then a uniﬁed convergence proof for the deterministic and stochastic cases.
Corollary 2 (Complexity of proximal stochastic gradient algorithm  µ > 0). Consider Algorithm 1
with hk deﬁned in (10). When f is µ-strongly convex  choose κ = L − µ. Then 
σ2
√µL
 

E[F (xk) − F ⋆] ≤(cid:18)1 −r µ
L(cid:19)

(F (x0) − F ⋆) +

which is of the form (2) with τ =pµ/L and B = σ2/(√µL). Interestingly  the optimal complex-
ity O(cid:16)pL/µ log((F (x0) − F ⋆)/ε) + σ2/µε(cid:17) can be obtained by using the ﬁrst restart strategy

presented in Section 3  see Eq. (5)  either by using increasing mini-batches or decreasing step sizes.
When the objective is convex  but not strongly convex  Proposition 1 gives a bias term O(σ2k/κ)
that increases linearly with k. Yet  the following corollary exhibits an optimal rate with ﬁnite horizon 
when both σ2 and an upper-bound on kx0 − x⋆k2 are available. Even though non-practical  the result
shows that our analysis recovers the optimal dependency in the noise level  as [18  28] and others.
Corollary 3 (Complexity of proximal stochastic gradient algorithm  µ = 0). Consider a ﬁxed budget
K of iterations of Algorithm 1 with hk deﬁned in (10). When κ = max(L  σ(K + 1)3/2/kx0 − x⋆k) 

E[F (xK) − F ⋆] ≤

2Lkx0 − x⋆k2

(K + 1)2 +

3σkx0 − x⋆k

√K + 1

.

While all the previous examples use the choice xk = x⋆
may choose xk 6= x⋆

k. Before that  we introduce a variant when x⋆

k is not available.

k  we will see in Section 4.2 cases where we

In principle  it is possible to design other surrogates  which would lead to new algorithms coming
with convergence guarantees given by Propositions 1 and 4  but the given examples (4)  (10)  and
(10) already cover all important cases considered in the paper for functions of the form (1).

5

4.1 Variant with Inexact Minimization

In this variant  presented in Algorithm 2  x⋆

k is not available and we assume that M also satisﬁes:

(H4) given εk ≥ 0  M can provide a point xk such that E[hk(xk) − h⋆

k] ≤ εk.

Algorithm 2 Generic Acceleration Framework with Inexact Minimization of hk
1: Input: same as Algorithm 2;
2: Initialization: y0 = x0; q = µ
3: for k = 1  . . .   K do
4:
5:
6:
7: end for
8: Output: xk (ﬁnal estimate).

Consider a surrogate hk satisfying (H1)  (H2) and obtain xk satisfying (H4);
Compute αk in (0  1) by solving the equation α2
Update the extrapolated sequence yk = xk + βk(xk − xk–1) with βk deﬁned in (7);

µ+κ ; α0 = 1 if µ = 0 or α0 = √q if µ 6= 0;

k = (1 − αk)α2

k–1 + qαk.

The next proposition  proven in Appendix C  gives us some insight on how to achieve acceleration.
Proposition 4 (Convergence analysis for Algorithm 2). Consider Alg. 2. Then  for any γ ∈ (0  1] 
2 (cid:17)−j(cid:16)δj + εj√q(cid:17)(cid:19) if µ 6= 0

2 (cid:17)k(cid:18)2(F (x0) − F ⋆) + 4Pk

j=1(cid:16)1 −

j=1(j + 1)2δj + (j+1)3+γ εj

if µ = 0.

√q

√q

γ

2e1+γ

(cid:16)1 −
(k+1)2 (cid:16)κkx0 − x⋆k2 +Pk

E[F (xk)−F ⋆] ≤


To maintain the accelerated rate  the sequence (δk)k≥0 needs to converge at a similar speed as in
Proposition 1  but the dependency in εk is slightly worse. Speciﬁcally  when µ is positive  we may
have both (εk)k≥0 and (δk)k≥0 decreasing at a rate O((1 − ρ)k) with ρ < √q/2  but we pay a
factor (1/√q) compared to (8). When µ = 0  the accelerated O(1/k2) rate is preserved whenever
εk = O(1/k4+2γ) and δk = O(1/k3+γ)  but we pay a factor O(1/γ) compared to (8).

(cid:17)

Catalyst [33]. When using hk deﬁned in (4)  we recover the convergence rates of [33]. In such
a case δk = εk since E[F (xk)] ≤ E[hk(xk)] ≤ E[h⋆
k] + δk. In order to analyze the complexity of
minimizing each hk with M and derive the global complexity of the multi-stage algorithm  the next
proposition  proven in Appendix C  characterizes the quality of the initialization xk–1.
Proposition 5 (Warm restart for Catalyst). Consider Alg. 2 with hk deﬁned in (4). Then  for k ≥ 2 
(12)

3εk–1

+ 54κ max(cid:0)kxk–1 − x⋆k2 kxk–2 − x⋆k2 kxk–3 − x⋆k2(cid:1)  

E[hk(xk–1) − h⋆

k] ≤

2

µ

k] = O( κ

where x–1 = x0. Following [33]  we may now analyze the global complexity. For instance  when f

is µ-strongly convex  we may choose εk = O((1 − ρ)k(F (x0) − F ⋆)) with ρ = √q/3. Then  it
is possible to show that Proposition (4) yields E[F (xk) − F ⋆] = O(εk/q) and from the inequality
2kxk − x⋆k2 ≤ F (xk) − F ⋆ and (12)  we have E[hk(xk–1) − h⋆
µq εk–1) = O(εk–1/q2).
Consider now a method M that behaves as (2). When σ = 0  xk can be obtained in O(log(1/q)/τ ) =
˜O(1/τ ) iterations of M after initializing with xk–1. This allows us to obtain the global complexity
˜O((1/τ√q) log(1/ε)). For example  when M is the proximal gradient descent method  κ = L and
τ = (µ + κ)/(L + κ) yield the global complexity ˜O(pL/µ log(1/ε)) of an accelerated method.
Our results improve upon Catalyst [33] in two aspects that are crucial for stochastic optimization:
(i) we allow the sub-problems to be solved in expectation  whereas Catalyst requires the stronger
condition hk(xk) − h⋆
k ≤ εk; (ii) Proposition 5 removes the requirement of [33] to perform a full
gradient step for initializing the method M in the composite case (see Prop. 12 in [33]).
Proximal gradient descent with inexact prox [45]. The surrogate (10) with inexact minimization
can be treated in the same way as Catalyst  which provides a uniﬁed proof for both problems. Then 
we recover the results of [45]  while allowing inexact minimization to be performed in expectation.

Stochastic Catalyst. With Proposition 5  we are in shape to consider stochastic problems when

using a method M that converges linearly as (2) with σ2 6= 0 for minimizing hk. As in Section 3 

6

we also assume that there exists a mini-batch/step-size parameter η that can reduce the bias by a
factor η < 1 while paying a factor 1/η in terms of inner-loop complexity. As above  we discuss the
strongly-convex case and choose the same sequence (εk)k≥0. In order to minimize hk up to accuracy
εk  we set ηk = min(1  εk/(2Bσ2)) such that ηkBσ2 ≤ εk/2. Then  the complexity to minimize hk
with M when using the initialization xk–1 becomes ˜O(1/τ ηk)  leading to the global complexity

˜O(cid:18) 1
τ√q

log(cid:18) F (x0) − F ⋆

ε

(cid:19) +

Bσ2

q3/2τ ε(cid:19) .

(13)

Details about the derivation are given in Appendix B. The left term corresponds to the Catalyst
accelerated rate  but it may be shown that the term on the right is sub-optimal. Indeed  consider M to
be ISTA with κ = L−µ. Then  B = 1/L  τ = O(1)  and the right term becomes ˜O((pL/µ)σ2/µε) 
which is sub-optimal by a factorpL/µ. Whereas this result is a negative one  suggesting that Catalyst

is not robust to noise  we show in Section 4.2 how to circumvent this for a large class of algorithms.

Accelerated stochastic proximal gradient descent with inexact prox. Finally  consider hk de-
ﬁned in (10) but the proximal operator is computed approximately  which  to our knowledge  has never
been analyzed in the stochastic context. Then  it may be shown (see Appendix B for details) that  even
though x⋆
k is not available  Proposition 4 holds nonetheless with δk = 2εk+3σ2/(2(κ + µ)). Then  an
interesting question is how small should εk be to guarantee the optimal dependency with respect to σ2
as in Corollary 2. In the strongly-convex case  Proposition 4 simply gives εk = O(√qσ2/(κ + µ))
such that δk ≈ εk/√q.
4.2 Exploiting methods M providing strongly convex surrogates
Among various application cases  we have seen an extension of Catalyst to stochastic problems. To
achieve convergence  the strategy requires a mechanism to reduce the bias Bσ2 in (2)  e.g.  by using
mini-batches or decreasing step sizes. Yet  the approach suffers from two issues: (i) some of the
parameters are based on unknown quantities such as σ2; (ii) the worst-case complexity exhibits a
sub-optimal dependency in σ2  typically of order 1/√q when µ > 0. Whereas practical workarounds
for the ﬁrst point are discussed in Section 5  we now show how to solve the second one in some
cases  by using Algorithm 1 with an optimization method M  which is able not only to minimize an
auxiliary objective Hk  but also at the same time is able to provide a model hk  typically a quadratic
function  which is easy to minimize. Consider then a method M satisfying (2) and which produces 
after T steps  a point xk and a surrogate hk such that

k +ξk–1)+Bσ2 with Hk(x) = F (x)+

k] ≤ C(1−τ )T (Hk(xk–1)−H ⋆

κ
2kx−yk–1k2 
E[Hk(xk)−h⋆
(14)
where Hk is approximately minimized by M  hk is a model of Hk that satisﬁes (H1)  (H2) and that
can be minimized in closed form  and ξk–1 = O(E[F (xk–1) − F ⋆]); it is easy to show that (H3) is
also satisﬁed with the choice δk = C(1 − τ )T (Hk(xk–1) − H ⋆
k + ξk–1) + Bσ2 since E[F (xk)] ≤
E[Hk(xk)] ≤ E[h⋆
k] + δk. In other words  M is used to perform approximate minimization of Hk 
but we consider cases where M also provides another surrogate hk with closed-form minimizer that
satisﬁes the conditions required to use Algorithm 1  which has better convergence guarantees than
Algorithm 2 (same convergence rate up to a better factor).

As shown in Appendix D  even though (14) looks technical  a large class of optimization techniques
are able to provide the condition (14)  including many variants of proximal stochastic gradient descent
methods with variance reduction such as SAGA [13]  MISO [35]  SDCA [47]  or SVRG [53].

Whereas (14) seems to be a minor modiﬁcation of (2)  an important consequence is that it will allow us
to gain a factor 1/√q in complexity when µ > 0  corresponding precisely to the sub-optimality factor.
Therefore  even though the surrogate Hk needs only be minimized approximately  the condition (14)
allows us to use Algorithm 1 instead of Algorithm 2. The dependency with respect to δk being better
than εk (by 1/√q)  we have then the following result:
Proposition 6 (Stochastic Catalyst with Optimality Gaps  µ > 0). Consider Algorithm 1 with a
method M and surrogate hk satisfying (14) when M is used to minimize Hk by using xk–1 as a
warm restart. Assume that f is µ-strongly convex and that there exists a parameter η that can reduce
the bias Bσ2 by a factor η < 1 while paying a factor 1/η in terms of inner-loop complexity.

7

Choose δk = O((1−√q/2)k(F (x0)−F ⋆)) and ηk = min(1  δk/(2Bσ2)). Then  the complexity to
solve (14) and compute xk is ˜O(1/τ ηk)  and the global complexity to obtain E[F (xk) − F ⋆] ≤ ε is

˜O(cid:18) 1
τ√q

log(cid:18) F (x0) − F ⋆

ε

(cid:19) +

Bσ2

qτ ε (cid:19) .

The term on the left is the accelerated rate of Catalyst for deterministic problems  whereas the
term on the right is potentially optimal for strongly convex problems  as illustrated in the next
table. We provide indeed practical choices for the parameters κ  leading to various values of B  τ  q 
for the proximal stochastic gradient descent method with iterate averaging as well as variants of
SAGA MISO SVRG that can cope with stochastic perturbations  which are discussed in Appendix D.
All the values below are given up to universal constants to simplify the presentation.

Method M
prox-SGD

κ

hk
(10) L − µ
n − µ

L

τ B
1
1
L
2

q
µ
L

µn
L

Complexity after Catalyst

1
n

n ≥ µ (14)

SAGA/MISO/SVRG with L

µε(cid:17)
ε (cid:1) + σ2
µε(cid:17)
ε (cid:1) + σ2
In this table  F0 := F (x0)−F ⋆ and the methods SAGA/MISO/SVRG are applied to the stochastic
ﬁnite-sum problem discussed in Section 1 with n L-smooth functions. As in the deterministic case 
we note that when L/n ≤ µ  there is no acceleration for SAGA/MISO/SVRG since the complexity
of the unaccelerated method M is ˜O(cid:0)n log (F0/ε) + σ2/µε(cid:1)  which is independent of the condition
number and already optimal [28]. In comparison  the logarithmic terms in L  µ that are hidden in the
notation ˜O do not appear for a variant of the SVRG method with direct acceleration introduced in [28].
Here  our approach is more generic. Note also that σ2 for prox-SGD and SAGA/MISO/SVRG cannot
be compared to each other since the source of randomness is larger for prox-SGD  see [7  28].

µ log(cid:0) F0
µ log(cid:0) F0

˜O(cid:16)q L
˜O(cid:16)qn L

1
L

5 Experiments

In this section  we perform numerical evaluations by following [28]  which was notably able to make
SVRG and SAGA robust to stochastic noise  and accelerate SVRG. Code to reproduce the experiments
is provided with the submission and more details and experiments are given in Appendix E.

Formulations. Given training data (ai  bi)i=1 ... n  with ai in Rp and bi in {−1  +1}  we consider

the optimization problem

1
n

min
x∈Rp

n

Xi=1

φ(bia⊤i x) +

µ
2 kxk2 

where φ is either the logistic loss φ(u) = log(1+e−u)  or the squared hinge loss φ(u) = 1
2 max(0  1−
u)2  which are both L-smooth  with L = 0.25 for logistic and L = 1 for the squared hinge loss.
Studying the squared hinge loss is interesting since its gradients are unbounded on the optimization
domain  which may break the bounded noise assumption. The regularization parameter µ acts as the
strong convexity constant for the problem and is chosen among the smallest values one would try
when performing parameter search  e.g.  by cross validation. Speciﬁcally  we consider µ = 1/10n
and µ = 1/100n  where n is the number of training points; we also try µ = 1/1000n to evaluate the
numerical stability of methods in very ill-conditioned problems. Following [7  28  54]  we consider
DropOut perturbations [51]—that is  setting each component (∇f (x))i to 0 with a probability δ and
to (∇f (x))i/(1 − δ) otherwise. This procedure is motivated by the need of a simple optimization
benchmark illustrating stochastic ﬁnite-sum problems  where the amount of perturbation is easy to
control. The settings used in our experiments are δ = 0 (no noise) and δ ∈ {0.01  0.1}.
Datasets. We consider three datasets with various number of points n and dimension p. All the
data points are normalized to have unit ℓ2-norm. The description comes from [28]:

• alpha is from the Pascal Large Scale Learning Challenge website2 and contains n = 250 000 points

in dimension p = 500.

2❤tt♣✿✴✴❧❛r❣❡s❝❛❧❡✳♠❧✳t✉✲❜❡r❧✐♥✳❞❡✴

8

breast cancer. This is a small dataset with n = 295 and p = 8 141.

• gene consists of gene expression data and the binary labels bi characterize two different types of
• ckn-cifar is an image classiﬁcation task where each image from the CIFAR-10 dataset3 is repre-
sented by using a two-layer unsupervised convolutional neural network [36]. We consider here
the binary classiﬁcation task consisting of predicting the class 1 vs. other classes  and use our
algorithms for the classiﬁcation layer of the network  which is convex. The dataset contains
n = 50 000 images and the dimension of the representation is p = 9 216.

Methods. We consider the variants of SVRG and SAGA of [28]  which use decreasing step sizes
when δ > 0 (otherwise  they do not converge). We use the sufﬁx “-d” each time decreasing step sizes
are used. We also consider Katyuasha [1] when δ = 0  and the accelerated SVRG method of [28] 
denoted by acc-SVRG. Then  SVRG-d  SAGA-d  acc-SVRG-d are used with the step size strategies
described in [28]  by using the code provided to us by the authors.

In all setups  we choose the parameter κ according to
Practical questions and implementation.
theory  which are described in the previous section  following Catalyst [33]. For composite problems 
Proposition 5 suggests to use xk–1 as a warm start for inner-loop problems. For smooth ones  [33]
shows that in fact  other choices such as yk–1 are appropriate and lead to similar complexity results.
In our experiments with smooth losses  we use yk–1  which has shown to perform consistently better.

The strategy for ηk discussed in Proposition 6 suggests to use constant step-sizes for a while in the
inner-loop  typically of order 1/(κ + L) for the methods we consider  before using an exponentially

decreasing schedule. Unfortunately  even though theory suggests a rate of decay in (1 − √q/2)k 

it does not provide useful insight on when decaying should start since the theoretical time requires
knowing σ2. A similar issue arise in stochastic optimization techniques involving iterate averaging
[9]. We adopt a similar heuristic as in this literature and start decaying after k0 epochs  with k0 = 30.
Finally  we discuss the number of iterations of M to perform in the inner-loop. When ηk = 1  the
theoretical value is of order ˜O(1/τ ) = ˜O(n)  and we choose exactly n iterations (one epoch)  as in
Catalyst [33]. After starting decaying the step-sizes (ηk < 1)  we use ⌈n/ηk⌉  according to theory.
Experiments and conclusions. We run each experiment ﬁve time with a different random seed and
average the results. All curves also display one standard deviation. Appendix E contains numerous
experiments  where we vary the amount of noise  the type of approach (SVRG vs. SAGA)  the amount
of regularization µ  and choice of loss function. In Figure 1  we show a subset of these curves. Most
of them show that acceleration may be useful even in the stochastic optimization regime  consistently
with [28]. At the same time  all acceleration methods may not perform well for very ill-conditioned
problems with µ = 1/1000n  where the sublinear convergence rates for convex optimization (µ = 0)
are typically better than the linear rates for strongly convex optimization (µ > 0). However  these
ill-conditioned cases are often unrealistic in the context of empirical risk minimization.

ckn-cifar

gene

alpha

10−1

10−2

10−3

10−1

10−2

10−3

cat-svrg-d
svrg-d
acc-svrg-d

cat-svrg-d
svrg-d
cat-saga-d
saga-d

0

20

40

60

80

100

120

140

160

0

20

40

60

80

100

120

140

160

0

20

40

60

80

100

120

140

160

Figure 1: Accelerating SVRG-like (top) and SAGA (bottom) methods for ℓ2-logistic regression with
µ = 1/(100n) (bottom) for δ = 0.1. All plots are on a logarithmic scale for the objective function
value  and the x-axis denotes the number of epochs. The colored tubes around each curve denote a
standard deviations across 5 runs. They do not look symmetric because of the logarithmic scale.

3❤tt♣s✿✴✴✇✇✇✳❝s✳t♦r♦♥t♦✳❡❞✉✴⑦❦r✐③✴❝✐❢❛r✳❤t♠❧

9

Acknowledgments

This work was supported by the ERC grant SOLARIS (number 714381) and ANR 3IA
MIAI@Grenoble Alpes. The authors would like to thank Anatoli Juditsky for numerous interesting
discussions that greatly improved the quality of this manuscript.

References

[1] Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In Proceedings of

Symposium on Theory of Computing (STOC)  2017.

[2] Y. Arjevani and O. Shamir. Dimension-free iteration complexity of ﬁnite sum optimization problems. In

Advances in Neural Information Processing Systems (NIPS)  2016.

[3] H. Asi and J. C. Duchi. Stochastic (approximate) proximal point methods: Convergence  optimality  and

adaptivity. SIAM Journal on Optimization  29(3):2257–2290  2019.

[4] N. S. Aybat  A. Fallah  M. Gurbuzbalaban  and A. Ozdaglar. A universally optimal multistage accelerated

stochastic gradient method. preprint arXiv:1901.08022  2019.

[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[6] D. P. Bertsekas.

Incremental proximal methods for large scale convex optimization. Mathematical

Programming  129(2):163  2011.

[7] A. Bietti and J. Mairal. Stochastic optimization with variance reduction for inﬁnite datasets with ﬁnite-sum

structure. In Advances in Neural Information Processing Systems (NIPS)  2017.

[8] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Advances in Neural Information

Processing Systems (NIPS)  2008.

[9] L. Bottou  F. E. Curtis  and J. Nocedal. Optimization methods for large-scale machine learning. SIAM

Review  60(2):223–311  2018.

[10] A. Chambolle and T. Pock. A remark on accelerated block coordinate descent for computing the proximity

operators of a sum of convex functions. SMAI Journal of Computational Mathematics  1:29–54  2015.

[11] M. B. Cohen  J. Diakonikolas  and L. Orecchia. On acceleration with noise-corrupted gradients. In

Proceedings of the International Conferences on Machine Learning (ICML)  2018.

[12] A. d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization 

19(3):1171–1183  2008.

[13] A. Defazio  F. Bach  and S. Lacoste-Julien. Saga: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems (NIPS) 
2014.

[14] A. Defazio  T. Caetano  and J. Domke. Finito: A faster  permutable incremental gradient method for big

data problems. In Proceedings of the International Conferences on Machine Learning (ICML)  2014.

[15] O. Devolder. Stochastic ﬁrst order methods in smooth convex optimization. Technical report  Université

catholique de Louvain  2011.

[16] O. Devolder  F. Glineur  and Y. Nesterov. First-order methods of smooth convex optimization with inexact

oracle. Mathematical Programming  146(1-2):37–75  2014.

[17] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic
composite optimization I: A generic algorithmic framework. SIAM Journal on Optimization  22(4):1469–
1492  2012.

[18] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic
composite optimization II: Shrinking procedures and optimal algorithms. SIAM Journal on Optimization 
23(4):2061–2089  2013.

[19] R. M. Gower  P. Richtárik  and F. Bach. Stochastic quasi-gradient methods: Variance reduction via Jacobian

sketching. preprint arXiv:1805.02632  2018.

10

[20] O. Güler. New proximal point algorithms for convex minimization. SIAM Journal on Optimization 

2(4):649–664  1992.

[21] J.-B. Hiriart-Urruty and C. Lemaréchal. Convex analysis and minimization algorithms. II. Springer  1996.

[22] T. Hofmann  A. Lucchi  S. Lacoste-Julien  and B. McWilliams. Variance reduced stochastic gradient

descent with neighbors. In Advances in Neural Information Processing Systems (NIPS)  2015.

[23] C. Hu  W. Pan  and J. T. Kwok. Accelerated gradient methods for stochastic optimization and online

learning. In Advances in Neural Information Processing Systems (NIPS). 2009.

[24] A. Iouditski and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex functions.

preprint arXiv:1401.1792  2014.

[25] J. Koneˇcn`y and P. Richtárik. Semi-stochastic gradient descent methods. Frontiers in Applied Mathematics

and Statistics  3:9  2017.

[26] D. Kovalev  S. Horvath  and P. Richtarik. Don’t jump through hoops and remove those loops: SVRG and

Katyusha are better without the outer loop. preprint arXiv:1901.08689  2019.

[27] B. Kulis and P. L. Bartlett. Implicit online learning. In Proceedings of the International Conferences on

Machine Learning (ICML)  2010.

[28] A. Kulunchakov and J. Mairal. Estimate sequences for stochastic composite optimization: Variance

reduction  acceleration  and robustness to noise. preprint arXiv:1901.08788  2019.

[29] G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming  133(1):365–

397  2012.

[30] G. Lan and Y. Zhou. An optimal randomized incremental gradient method. Mathematical Programming 

171(1–2):167–215  2018.

[31] G. Lan and Y. Zhou. Random gradient extrapolation for distributed and stochastic optimization. SIAM

Journal on Optimization  28(4):2753–2782  2018.

[32] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Advances in

Neural Information Processing Systems (NIPS)  2015.

[33] H. Lin  J. Mairal  and Z. Harchaoui. Catalyst acceleration for ﬁrst-order convex optimization: from theory

to practice. Journal of Machine Learning Research (JMLR)  18(212):1–54  2018.

[34] H. Lin  J. Mairal  and Z. Harchaoui. An inexact variable metric proximal point algorithm for generic

quasi-Newton acceleration. SIAM Journal on Optimization  29(2):1408–1443  2019.

[35] J. Mairal. Incremental majorization-minimization optimization with application to large-scale machine

learning. SIAM Journal on Optimization  25(2):829–855  2015.

[36] J. Mairal. End-to-end kernel learning with supervised convolutional kernel networks. In Advances in

Neural Information Processing Systems (NIPS)  2016.

[37] J.-J. Moreau. Proximité et dualité dans un espace hilbertien. Bulletins de la Socitété Mathématique de

France  93(2):273–299  1965.

[38] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[39] Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). Soviet

Mathematics Doklady  27(2):372–376  1983.

[40] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer  2004.

[41] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal

on Optimization  22(2):341–362  2012.

[42] L. M. Nguyen  J. Liu  K. Scheinberg  and M. Takáˇc. Sarah: A novel method for machine learning problems
using stochastic recursive gradient. In Proceedings of the International Conferences on Machine Learning
(ICML)  2017.

11

[43] L. M. Nguyen  P. H. Nguyen  M. van Dijk  P. Richtárik  K. Scheinberg  and M. Takáˇc. SGD and Hogwild!
convergence without the bounded gradients assumption. In Proceedings of the International Conferences
on Machine Learning (ICML)  2018.

[44] C. Paquette  H. Lin  D. Drusvyatskiy  J. Mairal  and Z. Harchaoui. Catalyst acceleration for gradient-based

non-convex optimization. preprint arXiv:1703.10993  2018.

[45] M. Schmidt  N. Le Roux  and F. Bach. Convergence rates of inexact proximal-gradient methods for convex

optimization. In Advances in Neural Information Processing Systems (NIPS)  2011.

[46] D. Scieur  F. Bach  and A. d’Aspremont. Nonlinear acceleration of stochastic algorithms. In Adv. in Neural

Information Processing Systems (NIPS)  2017.

[47] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized

loss minimization. Mathematical Programming  155(1):105–145  2016.

[48] P. Toulis  T. Horel  and E. M. Airoldi. Stable Robbins-Monro approximations through stochastic proximal

updates. preprint arXiv:1510.00967  2018.

[49] P. Toulis  D. Tran  and E. Airoldi. Towards stability and optimality in stochastic gradient descent. In

Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2016.

[50] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. 2008. unpublished.

[51] S. Wager  W. Fithian  S. Wang  and P. S. Liang. Altitude training: Strong bounds for single-layer dropout.

In Advances in Neural Information Processing Systems (NIPS)  2014.

[52] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of

Machine Learning Research (JMLR)  11(Oct):2543–2596  2010.

[53] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM

Journal on Optimization  24(4):2057–2075  2014.

[54] S. Zheng and J. T. Kwok. Lightweight stochastic optimization for minimizing ﬁnite sums with inﬁnite

data. In Proceedings of the International Conferences on Machine Learning (ICML)  2018.

[55] K. Zhou. Direct acceleration of SAGA using sampled negative momentum.

In Proceedings of the

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2019.

[56] K. Zhou  F. Shang  and J. Cheng. A simple stochastic variance reduced algorithm with fast convergence

rates. In Proceedings of the International Conferences on Machine Learning (ICML)  2018.

12

,Andrei Kulunchakov
Julien Mairal