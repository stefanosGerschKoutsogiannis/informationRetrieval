2018,GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration,Despite advances in scalable models  the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n^3) to O(n^2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition  BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally  we provide GPyTorch  a software platform for scalable GP inference via BBMM  built on PyTorch.,GPyTorch:BlackboxMatrix-MatrixGaussianProcessInferencewithGPUAccelerationJacobR.Gardner∗ GeoffPleiss∗ DavidBindel KilianQ.Weinberger AndrewGordonWilsonCornellUniversity{jrg365 kqw4 andrew}@cornell.edu {geoff bindel}@cs.cornell.eduAbstractDespiteadvancesinscalablemodels theinferencetoolsusedforGaussianpro-cesses(GPs)haveyettofullycapitalizeondevelopmentsincomputinghardware.WepresentanefﬁcientandgeneralapproachtoGPinferencebasedonBlackboxMatrix-Matrixmultiplication(BBMM).BBMMinferenceusesamodiﬁedbatchedversionoftheconjugategradientsalgorithmtoderivealltermsfortrainingandinferenceinasinglecall.BBMMreducestheasymptoticcomplexityofexactGPinferencefromO(n3)toO(n2).Adaptingthisalgorithmtoscalableapproxima-tionsandcomplexGPmodelssimplyrequiresaroutineforefﬁcientmatrix-matrixmultiplicationwiththekernelanditsderivative.Inaddition BBMMusesaspecial-izedpreconditionertosubstantiallyspeedupconvergence.InexperimentsweshowthatBBMMeffectivelyusesGPUhardwaretodramaticallyacceleratebothexactGPinferenceandscalableapproximations.Additionally weprovideGPyTorch asoftwareplatformforscalableGPinferenceviaBBMM builtonPyTorch.1IntroductionThepastyearshavewitnessedunprecedentedinnovationindeeplearning.Thisprogresshasinvolvedinnovationsinnetworkdesigns[18 20 24 26 30] butitalsohasbeneﬁtedvastlyfromimprovementsinoptimization[6] andexcellentsoftwareimplementationssuchasPyTorch MXNet TensorFlowandCaffe[1 8 28 38].Broadlyspeaking thegainsinoptimizationoriginateinlargepartfrominsightsinstochasticgradientoptimization[6 7 23 27 29 31] effectivelytradingoffunnecessaryexactnessforspeedandinsomecasesregularization.Moreover theadvantagesofmodernsoftwareframeworksfordeeplearningincluderapidprototyping easyaccesstospecialtycomputehardware(suchasGPUs) andblackboxoptimizationthroughautomaticdifferentiation.Similarly Gaussianprocessresearchhasundergonesigniﬁcantinnovationsinrecentyears[9 21 45 49–51]—inparticulartoimprovescalabilitytolargedatasets.However thetoolsmostcommonlyusedforGPinferencedonoteffectivelyutilizemodernhardware andnewmodelsrequiresigniﬁcantimplementationefforts.Often infact themodelandtheinferenceenginearetightlycoupledandconsequentlymanycomplexmodelslikemulti-outputGPsandscalableGPapproximationsrequirecustominferenceprocedures[5 22].Thisentanglementofmodelspeciﬁcationandinferenceprocedureimpedesrapidprototypingofdifferentmodeltypes andobstructsinnovationintheﬁeld.Inthispaper weaddressthisgapbyintroducingahighlyefﬁcientframeworkforGaussianprocessinference.WhereaspreviousinferenceapproachesrequiretheusertoprovideroutinesforcomputingthefullGPmarginalloglikelihoodforasufﬁcientlycomplexmodel ourframeworkonlyrequiresaccesstoablackboxroutinethatperformsmatrix-matrixmultiplicationswiththekernelmatrixanditsderivative.Accordingly werefertoourmethodasBlackboxMatrix-Matrix(BBMM)Inference.∗Equalcontribution.32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018) Montréal Canada.IncontrasttotheCholeskydecomposition whichisattheheartofmanyexistinginferenceengines matrix-matrixmultiplicationsfullyutilizeGPUacceleration.Wewilldemonstratethatthismatrix-matrixapproachalsosigniﬁcantlyeasesimplementationforawideclassofexistingGPmodelsfromtheliterature.Inparticular wemakethefollowingcontributions:1.Inspiredbyiterativematrix-vectormultiplication(MVM)-basedinferencemethods[9 13 43 50 51] weprovideamodiﬁedbatchedversionoflinearconjugategradients(mBCG)thatprovidesallcomputationsnecessaryforboththemarginallikelihoodanditsderivatives.Moreover mBCGuseslargematrix-matrixmultiplicationsthatmoreefﬁcientlyutilizemodernhardwarethanbothexistingCholeskyandMVMbasedinferencestrategies.Ourapproachalsocircumventsseveralcriticalspacecomplexityandnumericalstabilityissuespresentinexistinginferencemethods.Mostnotably BBMMreducesthetimecomplexityofexactGPinferencefromO(n3)toO(n2).2.WeintroduceamethodforpreconditioningthismodiﬁedconjugategradientsalgorithmbasedonthepivotedCholeskydecomposition[4 19].Allrequiredoperationswiththispreconditionerareefﬁcient andinpracticerequirenegligibletime.Wedemonstratebothempiricallyandtheoreticallythatthispreconditionersigniﬁcantlyacceleratesinference.3.WeintroduceGPyTorch anewsoftwareplatformusingBBMMinferenceforscalableGaussianprocesses whichisbuiltontopofPyTorch:https://gpytorch.ai.Ondatasetsaslargeas3000datapoints(untilweﬁllGPUmemory)wedemonstratethatexactGPswithBBMMareupto20×fasterthanGPsusingCholesky-basedapproaches.Moreover thepopularSKI[50]andSGPR[45]frameworkswithBBMMachieveupto15×and4×speedups(respectively)ondatasetsaslargeas500 000datapoints.Additionally SKI SGPRandotherscalableapproximationsareimplementedinlessthan50linesofcode requiringonlyanefﬁcientmatrix-matrixmultiplicationroutine.2RelatedWorkConjugategradients theLanczostridiagonalizationalgorithm andtheirrelativesaremethodsfromnumericallinearalgebraforcomputinglinearsolvesandsolvingeigenvalueproblemswithoutexplicitlycomputingamatrix.Thesetechniqueshavebeenaroundfordecades andarecoveredinpopularbooksandpapers[11 12 17 32 36 37 42].ThesealgorithmsbelongtoabroadclassofiterativemethodsknownasKrylovsubspacemethods whichaccessmatricesonlythroughmatrix-vectormultiplies(MVMs).Historically thesemethodshavebeenappliedtosolvinglargenumericallinearalgebraproblems particularlythoseinvolvingsparsematricesthataffordfastMVMs.Recently anumberofpapershaveusedtheseMVMmethodsforpartsofGPinference[9 13 15 34 39 43 49 50].OnekeyadvantageisthatMVMapproachescanexploitalgebraicstructureforincreasedcomputationalefﬁciencies.Notably thestructuredkernelinterpolation(SKI)method[50]usesstructuredkernelmatriceswithfastMVMstoachievearemarkableasymptoticcomplexity.Dongetal.[13]proposeMVMmethodsforcomputingstochasticestimatesoflogdeterminantsandtheirderivativesusingatechniquebasedonLanczostridiagonalization[16 46].WeutilizethesamelogdeterminantestimatorasDongetal.[13] exceptweavoidexplicitlyusingtheLanczostridiagonalizationalgorithmwhichhasstorageandnumericalstabilityissues[17].Preconditioningisaneffectivetoolforacceleratingtheconvergenceofconjugategradients.Thesetechniquesarefartoonumeroustoreviewadequatelyhere;however Saad[42]containstwochaptersdiscussingavarietyofpreconditioningtechniques.Cutajaretal.[10]exploresusingpreconditionedconjugategradientsforexactGPinference wheretheyusevarioussparseGPmethods(aswellassomeclassicalmethods)aspreconditioners.However themethodsinCutajaretal.[10]donotprovidegeneralpurposepreconditioners.Forexample methodslikeJacobipreconditioninghavenoeffectwhenusingastationarykernel[10 51] andmanyotherpreconditionershaveΩ(n2)complexity whichdominatesthecomplexityofmostscalableGPmethods.ThePivotedCholeskydecompositionisanefﬁcientalgorithmforcomputingalow-rankdecompo-sitionofapositivedeﬁnitematrix[4 19] whichweuseinthecontextofpreconditioning.Harbrechtetal.[19]explorestheuseofthepivotedCholeskydecompositionasalowrankapproximation althoughprimarilyinascientiﬁccomputingcontext.Inprovingconvergenceboundsforourpre-conditionerweexplicitlymakeuseofsometheoreticalresultsfrom[19](seeAppendixD).Bach[4]considersusingrandomcolumnsamplingaswellasthepivotedCholeskydecompositionasalow-rankapproximationtokernelmatrices.However Bach[4]treatsthisdecompositionasan2approximatetrainingmethod whereasweusethepivotedCholeskydecompositionprimarilyasapreconditioner whichavoidsanylossofaccuracyfromthelowrankapproximationaswellasthecomplexityofcomputingderivatives.3BackgroundNotation.Xwilldenoteasetofntrainingexamplesinddimensions orequivalentlyann×dmatrixwheretheithrow(denotedxi)istheithtrainingexample.ydenotesthetraininglabels.k(x x0)denotesakernelfunction andKXXdenotesthematrixcontainingallpairsofkernelentries i.e.[KXX]ij=k(xi xj).kXx∗denoteskernelvaluesbetweentrainingexamplesandatestpointx∗ e.g.[kXx∗]i=k(xi x∗).Ahatdenotesanaddeddiagonal:bKXX=KXX+σ2I.AGaussianprocess(GP)isakernelmethodthatdeﬁnesafulldistributionoverthefunctionbeingmodeled f(x)∼GP(µ(x) k(x x0)).PopularkernelsincludetheRBFkernel k(x x0)=sexp(cid:0)−(kx−x0k)/(2‘2)(cid:1)andtheMatérnfamilyofkernels[41].PredictionswithaGaussianprocess.PredictionswithaGParemadeutilizingthepredictiveposteriordistribution p(f(x∗)|X y).Giventwotestinputsx∗andx∗0 thepredictivemeanforx∗andthepredictivecovariancebetweenx∗andx∗0aregivenby:µf|D(x∗)=µ(x∗)+k>Xx∗bK−1XXy kf|D(x∗ x∗0)=kx∗x∗0−k>Xx∗bK−1XXkXx∗0 (1)TrainingaGaussianprocess.Gaussianprocessesdependonanumberofhyperparametersθ.Hyperparametersmayincludethelikelihoodnoise kernellengthscale inducingpointlocations[45] orneuralnetworkparametersfordeepkernellearning[52].Theseparametersarecommonlylearnedbyminimizationorsamplingviathenegativelogmarginallikelihood given(withderivative)byL(θ|X y)∝log(cid:12)(cid:12)(cid:12)bKXX(cid:12)(cid:12)(cid:12)−y>bK−1XXy dLdθ=y>bK−1XXdbKXXdθbK−1XXy+Tr bK−1XXdbKXXdθ!.(2)4GaussianprocessinferencethroughblackboxmatrixmultiplicationThegoalofourpaperistoreplaceexistinginferencestrategieswithauniﬁedframeworkthatutilizesmodernhardwareefﬁciently.WeadditionallydesirethatcomplexGPmodelscanbeusedinablackboxmannerwithoutadditionalinferencerules.Tothisend ourmethodreducesthebulkofGPinferencetooneofthemostefﬁciently-parallelizedcomputations:matrix-matrixmultiplication.WecallourmethodBlackboxMatrix-Matrixinference(BBMM)becauseitonlyrequiresausertospecifyamatrixmultiplyroutineforthekernelbKXXManditsderivativedbKXXdθM.Requiredoperations.Aninferenceengineisaschemeforcomputingalltheequationsdiscussedabove:thepredictivedistribution(1) theloss anditsderivative(2).Theseequationshavethreeoperationsincommonthatdominateitstimecomplexity:1)thelinearsolvebK−1XXy 2)thelogdeterminantlog|bKXX| and3)atracetermTr(bK−1XXdbKXXdθ).Inmanyimplementations thesethreequantitiesarecomputedusingtheCholeskydecompositionofbKXX whichiscomputationallyexpensive requiringO(n3)operations anddoesnoteffectivelyutilizeparallelhardware.Recently thereisagrowinglineofresearchthatcomputestheseoperationswithiterativeroutinesbasedonmatrix-vectormultiplications(MVMs).bK−1XXycanbecomputedusingconjugategradients(CG)[9 10 43 50] andtheothertwoquantitiescanbecomputedusingcallstotheiterativeLanczostridiagonalizationalgorithm[13 46].MVM-basedmethodsareasymptoticallyfasterandmorespaceefﬁcientthanCholeskybasedmethods[13 50].Additionally thesemethodsareabletoexploitalgebraicstructureinthedataforfurtherefﬁciencies[9 43 50].However theyalsohavedisadvantages.ThequantitiesarecomputedviaseveralindependentcallstotheCGandstochasticLanczosquadraturesubroutines whichareinherentlysequentialandthereforedonotfullyutilizeparallelhardware.Additionally theLanczostridiagonalizationalgorithmrequiresO(np)spaceforpiterationsandsuffersfromnumericalstabilityissuesduetolossoforthogonality[17].ModiﬁedCG.OurgoalistocapitalizeontheadvantagesofMVM-basedmethods(space-efﬁciency abilitytoexploitstructure etc.)butwithefﬁcientroutinesthatareoptimizedformodernparallel3computehardware.Forthispurpose ourmethodmakesuseofamodiﬁedBatchedConjugateGradientsAlgorithm(mBCG)algorithm.StandardconjugategradientstakesasinputavectoryandaroutineforcomputingamatrixvectorproductbKXXy and afterpiterations outputsanapproximatesolveup≈bK−1XXy(withexactequalitywhenp=n).Wemodifyconjugategradientsto(1)performlinearsolveswithmultiplerighthandsidessimultaneously and(2)returntridiagonalmatricescorrespondingtopartialLanczostridiagonalizationsofbKXXwithrespecttoeachrighthandside.2Speciﬁcally mBCGtakesasinputamatrix[yz1···zt] andoutputs:[u0u1···ut]=bK−1XX[yz1···zt]and˜T1 ... ˜Tt(3)where˜T1 ... ˜TtarepartialLanczostridiagonalizationsofbKXXwithrespecttothevectorsz1 ... zt whichwedescribeshortly.Inwhatfollows weshowhowtouseasinglecalltomBCGtocomputethethreeGPinferenceterms:bK−1XXy Tr(bK−1XX∂bKXX∂θ) andlog|bKXX|.bK−1XXyisequaltou0in(3) directlyreturnedfrommBCG.Wedescribetheothertwotermsbelow.EstimatingTr(bK−1XX∂bKXX∂θ)fromCGreliesonstochastictraceestimation[3 14 25] whichallowsustotreatthistermasasumoflinearsolves.Giveni.i.d.randomvariablesz1 ... ztsothatE[zi]=0andE(cid:2)ziz>i(cid:3)=I (e.g. zi∼N(0 I))thematrixtraceTr(A)canbewrittenasTr(A)=E(cid:2)z>iAzi(cid:3) suchthatTr bK−1XXdbKXXdθ!=E"z>ibK−1XXdbKXXdθzi#≈1ttXi=1(cid:16)z>ibK−1XX(cid:17) dbKXXdθzi!(4)isanunbiasedestimatorofthederivative.Thiscomputationmotivatesthez1 ... zttermsin(3):themBCGcallreturnsthesolvesbK−1XX[z1...zt] whichyieldsui=z>ibK−1XX.AsinglematrixmultiplywiththederivativedbKXXdθ[z1...zt]yieldstheremainingtermsontheRHS.Thefulltracecanthenbeestimatedbyelementwisemultiplyingthesetermstogetherandsumming asin(4).Estimatinglog|bKXX|canbeaccomplishedusingtheT1 ... TtmatricesfrommBCG.IfbKXX=QTQ> withQorthonormal thenbecausebKXXandThavethesameeigenvalues:log|bKXX|=Tr(logT)=E(cid:2)z>i(logT)zi(cid:3)≈tXi=1z>i(logT)zi(5)wherelogTheredenotesthematrixlogarithm andtheapproximationcomesfromthesamestochastictraceestimationtechniqueusedfor(4).OneapproachtoobtainadecompositionbKXX=QTQ>istousetheLanczostridiagonalizationalgorithm.ThisalgorithmtakesthematrixbKXXandaprobevectorzandoutputsthedecompositionQTQ>(wherezistheﬁrstcolumnofQ).However ratherthanrunningthefullalgorithm wecaninsteadrunpiterationsofthealgorithmttimes eachwithavectorz1 ... zttoobtaintdecompositions˜Q1˜T1˜Q>1 ... ˜Qt˜Tt˜Q>twith˜Qi∈Rn×pand˜Ti∈Rp×p.Wecanusethesepartialdecompositionstoestimate(5):E(cid:2)z>i(logT)zi(cid:3)=Ehz>i˜Qi(log˜Ti)˜Q>izii≈1ttXi=1z>i˜Qi(log˜Ti)˜Q>izi=1ttXi=1e>1(log˜Ti)e1 (6)wheree1istheﬁrstrowoftheidentitymatrix.RunningLanczoswithastartingvectorziensuresthatallcolumnsof˜Qiareorthogonaltoziexcepttheﬁrst so˜Qizi=e1[13 16 46].InmBCG weadaptatechniquefromSaad[42]whichallowsustocompute˜T1 ... ˜Ttcorrespondingtotheinputvectorsz1 ... zttomBCGfromthecoefﬁcientsofCGinO(1)additionalworkperiteration.Thisapproachallowsustocomputealogdeterminantestimateidenticalto(6)withoutrun-ningtheLanczosalgorithm.Thusweavoidtheextracomputation storage andnumericalinstabilityassociatedwithLanczositerations.WedescribethedetailsofthisadaptationinAppendixA.Runtimeandspace.Asshownabove weareabletoapproximateallinferencetermsfromasinglecalltomBCG.TheseapproximationsimprovewiththenumberofmBCGiterations.Eachiteration2mBCGdifferesfromBlockCGalgorithms[35]inthatmBCGreturnsLanczostridiagonalizationterms.4requiresonematrix-matrixmultiplywithbKXX andthesubsequentworktoderivetheseinferencetermstakesnegligibleadditionaltime(AppendixB).Therefore piterationsofmBCGrequiresO(nt)space(seeAppendixB)andO(pΞ(bKXX))time whereΞ(bKXX)isthetimetomultiplybKXXbyan×tmatrix.ThismultiplicationtakesO(n2t)timewithastandardmatrix.ItisworthnotingthatthisisalowerasymptoticcomplexitythatstandardCholesky-basedinference whichisO(n3).Therefore BBMMoffersacomputationalspeedupforexactGPinference.AswewillshowinSection5 thistimecomplexitycanbefurtherreducedwithstructureddataorsparseGPapproximations.4.1PreconditioningWhileeachiterationofmBCGperformslargeparallelmatrix-matrixoperationsthatutilizehardwareefﬁciently theiterationsthemselvesaresequential.Anaturalgoalforbetterutilizinghardwareistotradeofffewersequentialstepsforslightlymoreeffortperstep.Weaccomplishthisgoalusingpreconditioning[12 17 42 47] whichintroducesamatrixPtosolvetherelatedlinearsystemP−1bKXXu=P−1yinsteadofbK−1XXy.Bothsystemsareguaranteedtohavethesamesolution butthepreconditionedsystem’sconvergencedependsontheconditioningofP−1bKXXratherthanthatofbKXX.WeobservetworequirementsofapreconditionertobeusedingeneralforGPinference.First inordertoensurethatpreconditioningoperationsdonotdominaterunningtimewhenusingscalableGPmethods thepreconditionershouldaffordroughlylineartimesolvesandspace.Second weshouldbeabletoefﬁcientlycomputethelogdeterminantofthepreconditionermatrix log|P|.ThisisbecausethemBCGalgorithmappliedtothepreconditionedsystemestimateslog|P−1bKXX|ratherthanlog|bKXX|.Wemustthereforecomputelog|bKXX|=log|P−1bKXX|+log|P|.ThePivotedCholeskyDecomposition.Foronepossiblepreconditioner weturntothepivotedCholeskydecomposition.ThepivotedCholeskyalgorithmallowsustocomputealow-rankap-proximationofapositivedeﬁnitematrix KXX≈LkL>k[19].WepreconditionmBCGwith(LkL>k+σ2I)−1 whereσ2istheGaussianlikelihood’snoiseterm.Intuitively ifPk=LkL>kisagoodapproximationofKXX then(Pk+σ2I)−1bKXX≈I.WhilewereviewthepivotedCholeskyalgorithmfullyinAppendixC wewouldliketoemphasizethreekeyproperties.First itcanbecomputedinO(ρ(KXX)k2)time whereρ(KXX)isthetimetoaccessarow(nominallythisisO(n)).Second linearsolveswithbP=LkL>k+σ2IcanbeperformedinO(nk2)time.Finally thelogdeterminantofbPcanbecomputedinO(nk2)time.InFigure6weempiricallyshowthatthispreconditionerdramaticallyacceleratesCGconvergence.Further inAppendixD weprovethefollowinglemmaandtheoremforunivariateRBFkernels:Lemma1.LetKXX∈Rn×nbeaunivariateRBFkernelmatrix.LetLkL>kbetherankkpivotedCholeskydecompositionofKXX andletbPk=LkL>k+σ2I.Thenthereexistsaconstantb>0sothattheconditionnumberκ(bP−1bKXX)satisﬁesthefollowinginequality:κ(cid:16)bP−1kbKXX(cid:17) (cid:13)(cid:13)(cid:13)bP−1kbKXX(cid:13)(cid:13)(cid:13)2(cid:13)(cid:13)(cid:13)bK−1XXbPk(cid:13)(cid:13)(cid:13)2≤(1+O(nexp(−bk)))2.(7)Theorem1(ConvergenceofpivotedCholesky-preconditionedCG).LetKXX∈Rn×nbean×nunivariateRBFkernel andletLkL>kbeitsrankkpivotedCholeskydecomposition.AssumeweareusingpreconditionedCGtosolvethesystembK−1XXy=(KXX+σ2I)−1ywithpreconditionerbP=(LkL>k+σ2I).LetupbethepthsolutionofCG andletu∗=bK−1XXybetheexactsolution.Thenthereexistssomeb>0suchthat:ku∗−upkbKXX≤2(1/(1+O(exp(kb)/n))pku∗−u0kbKXX.(8)Theorem1impliesthatweshouldexpecttheconvergenceofconjugategradientstoimproveexponen-tiallywiththerankofthepivotedCholeskydecompositionusedforRBFkernels.Inourexperimentsweobservesigniﬁcantlyimprovedconvergenceforotherkernelsaswell(Figure6).Furthermore wecanleverageLemma1andexistingtheoryfrom[46]toarguethatpreconditioningimprovesourlogdeterminantestimate.Inparticular werestateTheorem4.1ofUbaruetal.[46]here:5Theorem2(Theorem4.1ofUbaruetal.[46]).LetKXX∈Rn×n andletLkL>kbeitsrankkpivotedCholeskydecomposition.Supposewerunp≥14rκ(cid:16)bP−1kbKXX(cid:17)logDiterationsofmBCG whereDisaterminvolvingthissameconditionnumberthatvanishesask→n(see[46]) andweuset≥242log(2/δ)vectorsziforthesolves.LetΓbethelogdeterminantestimatefrom(6).Then:Prh|log|bP−1bKXX|−Γ|≤|log|bP−1bKXX||i≥1−δ.(9)BecauseLemma1statesthattheconditionnumberκ(cid:16)bP−1kbKXX(cid:17)decaysexponentiallywiththerankofLk Theorem2impliesthatweshouldexpectthatthenumberofCGiterationsrequiredtoaccuratelyestimatelog|bP−1bKXX|decreasesquicklyaskincreases.Inaddition inthelimitask→nwehavethatlog|bKXX|=log|bP|.Thisisbecauselog|bP−1bKXX|→0(sincebP−1bKXXconvergestoI)andwehavethatlog|bKXX|=log|bP−1bKXX|+log|bP|.Sinceourcalculationoflog|bP|isexact ourﬁnalestimateoflog|bKXX|becomesmoreexactaskincreases.Infutureworkwehopetoderiveamoregeneralresultthatcoversmultivariatesettingsandotherkernels.5ProgrammabilitywithBBMMWehavediscussedhowtheBBMMframeworkismorehardwareefﬁcientthanexistinginferenceengines andavoidsnumericalinstabilitieswithLanczos.AnotherkeyadvantageofBBMMisthatitcaneasilybeadaptedtocomplexGPmodelsorstructuredGPapproximations.IndeedBBMMisblackboxbynature onlyrequiringaroutinetoperformmatrix-multiplicationswiththekernelmatrixanditsderivative.HereweprovideexamplesofhowexistingGPmodelsandscalableapproximationscanbeeasilyimplementedinthisframework.Thematrix-multiplicationroutinesforthemodelsrequireatmost50linesofPythoncode.Alloursoftware includingthefollowingGPimplementationswithBBMM areavailablethroughourGPyTorchlibrary:https://gpytorch.ai.BayesianlinearregressioncanbeviewedasGPregressionwiththespecialkernelmatrixbKXX=XX>+σ2I.Amatrixmultiplywiththiskernelagainstann×tmatrixV (XX>+σ2I)VrequiresO(tnd)time.Therefore BBMMrequiresO(ptnd)time andisexactinO(tnd2)time.ThisrunningtimecomplexitymatchesexistingefﬁcientalgorithmsforBayesianlinearregression withnoadditionalderivation.Multi-taskGaussianprocesses[5]canbeadaptedinthesamefashion[15].SparseGaussianProcessRegression(SGPR)[45]andmanyothersparseGPtechniques[21 40 44]usethesubsetofregressors(SoR)approximationforthekernel:bKXX≈(KXUK−1UUKUX+σ2I).Performingamatrix-matrixmultiplywiththismatrixrequiresO(tnm+tm3)timebydistribut-ingthevectormultiplyandgroupingtermscorrectly.ThiscomputationisasymptoticallyfasterthantheO(nm2+m3)timerequiredbyCholeskybasedinference.AugmentingtheSoRapproximationwithadiagonalcorrection e.g.asinFITC[44] issimilarlystraightforward.StructuredKernelInterpolation(SKI)[50] alsoknownasKISS-GP isaninducingpointmethoddesignedtoprovidefastmatrixvectormultiplies(MVMs)forusewithKrylovsubspacemethods.SKIisthusanaturalcandidateforBBMMandcanbeneﬁtgreatlyfromhardwareacceleration.SKIisageneralizationofSoR whichspeciﬁesKXU≈WKUU whereWisasparsematrix.ForexampleWcancorrespondtothecoefﬁcientsofsparselocalcubicconvolutioninterpolation.TheSKIapproximationappliedtothetrainingcovariancematrixgivesusbKXX≈(WKUUW>+σ2I).AssumingnostructureinKUUamatrixmultiplyrequiresO(tn+tm2)time.InKISS-GP[50 51] thematrixKUUisalsochosentohavealgebraicstructure suchasKroneckerorToeplitzstructure whichfurtheracceleratesMVMs.Forexample MVMswithaToeplitzKUUonlyrequireO(mlogm)time.ThusKISS-GPprovidesO(tn+tmlogm)matrix-matrixmultiplies[50].Compositionsofkernelscanoftenbehandledautomatically.Forexample givenaBBMMroutineforK1 K2 K3 wecanautomaticallyperform(K1K2+K3)M=K1(K2M)+K3M.SGPRandKISS-GPareimplementedinthisfashion.Givensomepre-deﬁnedbasiccompositionalitystrategies thekernelmatrixmultiplicationKMinSGPRreducestodeﬁninghowtoperformK−1UUM and6Skillcraftn=3338Gasn=2565Winen=1599Airfoiln=1503Autompgn=3920102030Speedup over Cholesky CPU (Exact GP)1x SpeedupCholesky (GPU)GPyTorch (GPU)KEGGn=49kProteinn=45kKin40kn=40kElevatorsn=17kPolTelen=15k0246810Speed up over Cholesky CPU (SGPR)1x SpeedupCholesky (GPU)GPyTorch (GPU)Songn=515kBuzzn=583kKEGGn=49kProteinn=45kKin40kn=40k0102030Speed up Over Dong et al. [13] CPU (DKL+SKI)1x SpeedupDong et al. [13] (GPU)GPyTorch (GPU)Speedup FactorSpeedup FactorSpeedup FactorFigure1:SpeedupofGPU-acceleratedinferenceengines.BBMMisinblue andcompetingGPUmethodsareingray.Left:ExactGPs.Middle:SGPR[21 45]–speedupoverCPUCholesky-basedinferenceengines.Right:SKI+DKL[50 52]–speedupoverCPUinferenceofDongetal.[13].similarlyforKISS-GPitreducestoperformingmultiplicationwithaToeplitzmatrixKUUM.ForproductkernelsonecanfollowGardneretal.[15].6ResultsWeevaluatetheBBMMframework demonstrating:(1)theBBMMinferenceengineprovidesasubstantialspeedbeneﬁtoverCholeskybasedinferenceandstandardMVM-basedCGinference especiallyforGPUcomputing;(2)BBMMachievescomparableorbetterﬁnaltesterrorcomparedtoCholeskyinference evenwithnokernelapproximations;and(3)preconditioningprovidesasubstantialimprovementintheefﬁciencyofourapproach.Baselinemethods.WetestBBMMonthreetypesofGPs:1.ExactGPmodels 2.SGPRinducingpointmodels[21 45] and3.SKImodelswithToeplitzKUUanddeepkernels[50 52].ForExactandSGPR wecompareBBMMagainstCholesky-basedinferenceenginesimplementedinGPFlow[33].GPFlowispresentlythefastestimplementationofthesemodelswithaCholeskyinferenceengine.SinceSKIisnotintendedforCholeskyinference wecompareBBMMtotheinferenceprocedureofDongetal.[13] implementedinourGPyTorchpackage.ThisproceduredifferersfromBBMMinthatitcomputesbK−1XXywithoutapreconditionerandcomputeslog|bKXX|anditsderivativewiththeLanczosalgorithm.Datasets.WetestExactmodelsonﬁvedatasetsfromtheUCIdatasetrepository[2]withupto3500trainingexamples(thelargestpossiblebeforeallimplementationsexhaustedGPUmemory):Skillcraft Gas Airfoil Autompg andWine.WetestSGPRonlargerdatasets(nupto50000):KEGG Protein Elevators Kin40k andPoleTele.ForSKIwetestﬁveofthelargestUCIdatasets(nupto515000):Song Buzz Protein Kin40k andKEGG.Experimentdetails.Allmethodsusethesameoptimizer(Adam)withidenticalhyperparameters.InBBMMexperimentsweuserankk=5pivotedCholeskypreconditionersunlessotherwisestated.Weuseamaximumofp=20iterationsofCGforeachsolve andweuset=10probevectorsﬁlledwithRademacherrandomvariablestoestimatethelogdeterminantandtraceterms.SGPRmodelsuse300inducingpoints.SKImodelsuse10 000inducingpointsandthedeepkernelsdescribedin[52].TheBBMMinferenceengineisimplementedinourGPyTorchpackage.152025300.00.10.20.30.40.5mBCG (ﬂoat)Cholesky (ﬂoat)Cholesky (double)CG Iterations PerformedResidual ErrorFigure2:SolveerrorformBCGandCholesky.AllspeedexperimentsarerunonanIntelXeonE5-2650CPUandanNVIDIATitanXpGPU.Speedcomparison.Figure1showsthespeedupobtainedbyGPU-acceleratedBBMMovertheleadingCPU-basedinferenceengines(CholeskyforExact/SGPR Dongetal.[13]forSKI).Aswouldbeexpected GPU-acceleratedBBMMisfasterthanCPU-basedinference.OnExactandSKI BBMMisupto32timesfasterthanCPUinference andupto10timesfasteronSGPR.Thelargestspeedupsoccuronthelargestdatasets sincesmallerdatasetsexperi-encelargerGPUoverhead.Notably BBMMachievesamuchlargerspeedupthanGPUac-7Skillcraftn=3338Gasn=2565Winen=1599Airfoiln=1503Autompgn=39201234Test MAEExact GP Errors (RBF)CholeskyGPyTorchSkillcraftn=3338Gasn=2565Winen=1599Airfoiln=1503Autompgn=39201234Test MAEExact GP Errors (Matern 5/2)CholeskyGPyTorchKEGGn=49kProteinn=45kKin40kn=40kElevatorsn=17kPolTelen=15k0.00.20.40.6Test MAESGPR Errors (Matern 5/2)CholeskyGPyTorchFigure3:ComparingﬁnalTestMAEwhenusingBBMMversusCholeskybasedinference.ThelefttwoplotscompareerrorsusingExactGPswithRBFandMatern-5/2kernels andtheﬁnalplotcompareserrorusingSGPRwithaMatern-5/2kernelonsigniﬁcantlylargerdatasets.20406080100CGIterations10−310−1SolveErrorProtein-DeepRBFKernelRank0(NoPrecond.)Rank2Rank5Rank920406080100CGIterations10−410−2100SolveErrorKEGG-DeepMatern-5/2KernelNoPreconditionerRank2Rank5Rank90255075100125150TestTime(s)02TestMAEProtein-WallclocktimeNoPreconditionerRank5Preconditioner0255075100125TestTime(s)01020TestMAEKEGG-WallclocktimeNoPreconditionerRank5PreconditionerFigure4:TheeffectofpreconditioningonsolveerrorskKx∗−yk/kykachievedbylinearconjugategradientsusingnopreconditionerversusrank2 5 and9pivotedCholeskypreconditionerson2UCIbenchmarkdatasetsusingdeepRBFanddeepMaternkernels.ThehyperparametersofKwerelearnedbymaximizingthemarginalloglikelihoodoneachdataset.celeratedCholeskymethods(Exact SGPR) whichonlyachievearoughly4×speedup.ThisresultunderscoresthefactthatCholeskymethodsarenotaswellsuitedforGPUacceleration.Additionally BBMMperformsbetterthantheGPU-acceleratedversionof[13]onSKI.ThisspeedupisbecauseBBMMisabletocalculateallinferencetermsinparallel while[13]computesthetermsinseries.Errorcomparison.InFigure3wereporttestmeanaverageerror(MAE)forExactandSGPRmodels.3WedemonstrateresultsusingboththeRBFkernelandaMatern-5/2kernel.Acrossalldatasets ourmethodisatleastasaccurateintermsofﬁnaltestMAE.Onafewdatasets(e.g.Gas Airfoil andWinewithExactGPs)BBMMevenimprovesﬁnaltesterror.CGhasaregularizingeffectswhichmayimprovemethodsinvolvingtheexactkernelovertheCholeskydecomposition wherenumericalissuesresultingfromextremelysmalleigenvaluesofthekernelmatrixareignored.Forexample Choleskymethodsfrequentlyaddnoise(or“jitter”)tothediagonalofthekernelmatrixfornumericalstability.Itispossibletoreducethenumericalinstabilitieswithdoubleprecision(seeFigure2);however thisrequiresanincreasedamountofcomputation.BBMMontheotherhandavoidsaddingthisnoise withoutrequiringdoubleprecision.Preconditioning.Todemonstratetheeffectivenessofpreconditioningatacceleratingtheconvergenceofconjugategradients weﬁrsttrainadeepRBFkernelmodelontwodatasets ProteinandKEGG andevaluatethesolveerrorofperformingbK−1XXyintermsoftherelativeresidualkbKXXu−yk/kykasafunctionofthenumberofCGiterationsperformed.Welookatthiserrorwhenusingnopreconditioner aswellasarank2 5 and9preconditioner.TodemonstratethatthepreconditionerisnotrestrictedtousewithanRBFkernel weevaluateusingadeepRBFkernelonProteinanda3SKImodelsareexcludedfromFigure3.ThisisbecausetheBBMMinferenceengineandtheinferenceengineofDongetal.[13]returnidenticaloutputs(seeAppendixA)eventhoughBBMMisfaster.8deepMatern-5/2kernelonKEGG.TheresultsareinthetopofFigure4.Asexpectedbasedonourtheoreticalintuitionsforthispreconditioner increasingtherankofthepreconditionersubstantiallyreducesthenumberofCGiterationsrequiredtoachieveconvergence.InthebottomofFigure4 weconﬁrmthatthesemoreaccuratesolvesindeedhaveaneffectontheﬁnaltestMAE.Weplot asafunctionofthetotalwallclocktimerequiredtocomputepredictions thetestMAEresultingfromusingnopreconditionerandfromusingarank5preconditioner.ThewallclocktimeisvariedbychangingthenumberofCGiterationsusedtocomputethepredictivemean.Weobservethat becausesuchalowrankpreconditionerissufﬁcient usingpreconditioningresultsinsigniﬁcantlymoreaccuratesolveswhilehavingvirtuallynoimpactontherunningtimeofeachCGiteration.Consequentially werecommendalwaysusingthepivotedCholeskypreconditionerwithBBMMsinceithasvirtuallynowall-clockoverheadandrapidlyacceleratesconvergence.7DiscussionInthispaper wediscussanovelframeworkforGaussianprocessinference(BBMM)basedonblackboxmatrix-matrixmultiplicationroutineswithkernelmatrices.Wehaveimplementedthisframeworkandseveralstate-of-the-artGPmodelsinournewpubliclyavailableGPyTorchpackage.Non-Gaussianlikelihoods.Althoughthispaperprimarilyfocusesontheregressionsetting BBMMisfullycompatiblewithvariationaltechniquessuchas[22 53] whicharealsosupportedinGPyTorch.Theseapproachesrequirecomputingthevariationallowerbound(orELBO)ratherthantheGPmarginalloglikelihood(2).WeleavetheexactdetailsoftheELBOderivationtootherpapers(e.g.[22]).However wenotethatasinglecalltomBCGcanbeusedtocomputetheKLdivergencebetweentwomultivariateGaussians whichisthemostcomputationallyintensivetermoftheELBO.AvoidingtheCholeskydecomposition.Asurprisingandimportanttake-awayofthispaperisthatitisbeneﬁcialtoavoidtheCholeskydecompositionforGPinference evenintheexactGPsetting.ThebasicalgorithmfortheCholeskydecomposition(describedinAppendixC)involvesadivide-andconquerapproachthatcanproveill-suitedforparallelhardware.Additionally theCholeskydecompositionperformsalargeamountofcomputationtogetalinearsolvewhenfastapproximatemethodssufﬁce.Ultimately theCholeskydecompositionofafullmatrixtakesO(n3)timewhileCGtakesO(n2)time.Indeed asshowninFigure2 CGmayevenprovidebetterlinearsolvesthantheCholeskydecomposition.Whileweuseapivotedversionofthisalgorithmforpreconditioning weonlycomputetheﬁrstﬁverowsofthisdecomposition.Byterminatingthealgorithmveryearly weavoidthecomputationalbottleneckandmanyofthenumericalinstabilities.ItisourhopethatthisworkdramaticallyreducesthecomplexityofimplementingnewGaussianprocessmodels whileallowingforinferencetobeperformedasefﬁcientlyaspossible.AcknowledgementsJRGandAGWaresupportedbyNSFIIS-1563887andbyFacebookResearch.GPandKQWaresupportedinpartbytheIII-1618134 III-1526012 IIS-1149882 IIS-1724282 andTRIPODS-1740822grantsfromtheNationalScienceFoundation.Inaddition theyaresupportedbytheBillandMelindaGatesFoundation theOfﬁceofNavalResearch andSAPAmericaInc.References[1]M.Abadi P.Barham J.Chen Z.Chen A.Davis J.Dean M.Devin S.Ghemawat G.Irving M.Isard etal.Tensorﬂow:Asystemforlarge-scalemachinelearning.InOSDI volume16 pages265–283 2016.[2]A.AsuncionandD.Newman.Ucimachinelearningrepository.https://archive.ics.uci.edu/ml/ 2007.Lastaccessed:2018-05-18.[3]H.AvronandS.Toledo.Randomizedalgorithmsforestimatingthetraceofanimplicitsymmetricpositivesemi-deﬁnitematrix.JournaloftheACM(JACM) 58(2):8 2011.[4]F.Bach.Sharpanalysisoflow-rankkernelmatrixapproximations.InCOLT 2013.[5]E.V.Bonilla K.M.Chai andC.Williams.Multi-taskGaussianprocessprediction.InNIPS 2008.9[6]L.Bottou.Large-scalemachinelearningwithstochasticgradientdescent.InCOMPSTAT pages177–186.Springer 2010.[7]P.Chaudhari A.Choromanska S.Soatto Y.LeCun C.Baldassi C.Borgs J.Chayes L.Sagun andR.Zecchina.Entropy-sgd:Biasinggradientdescentintowidevalleys.arXivpreprintarXiv:1611.01838 2016.[8]T.Chen M.Li Y.Li M.Lin N.Wang M.Wang T.Xiao B.Xu C.Zhang andZ.Zhang.Mxnet:Aﬂexibleandefﬁcientmachinelearninglibraryforheterogeneousdistributedsystems.arXivpreprintarXiv:1512.01274 2015.[9]J.P.Cunningham K.V.Shenoy andM.Sahani.FastGaussianprocessmethodsforpointprocessintensityestimation.InICML 2008.[10]K.Cutajar M.Osborne J.Cunningham andM.Filippone.Preconditioningkernelmatrices.InICML 2016.[11]B.N.Datta.Numericallinearalgebraandapplications volume116.Siam 2010.[12]J.W.Demmel.Appliednumericallinearalgebra volume56.Siam 1997.[13]K.Dong D.Eriksson H.Nickisch D.Bindel andA.G.Wilson.ScalablelogdeterminantsforGaussianprocesskernellearning.InNIPS 2017.[14]J.K.Fitzsimons M.A.Osborne S.J.Roberts andJ.F.Fitzsimons.Improvedstochastictraceestimationusingmutuallyunbiasedbases.arXivpreprintarXiv:1608.00117 2016.[15]J.R.Gardner G.Pleiss R.Wu K.Q.Weinberger andA.G.Wilson.ProductkernelinterpolationforscalableGaussianprocesses.InAISTATS 2018.[16]G.H.GolubandG.Meurant.Matrices momentsandquadraturewithapplications.PrincetonUniversityPress 2009.[17]G.H.GolubandC.F.VanLoan.Matrixcomputations volume3.JHUPress 2012.[18]R.H.Hahnloser R.Sarpeshkar M.A.Mahowald R.J.Douglas andH.S.Seung.Digitalselectionandanalogueampliﬁcationcoexistinacortex-inspiredsiliconcircuit.Nature 405(6789):947 2000.[19]H.Harbrecht M.Peters andR.Schneider.Onthelow-rankapproximationbythepivotedcholeskydecomposition.Appliednumericalmathematics 62(4):428–440 2012.[20]K.He X.Zhang S.Ren andJ.Sun.Deepresiduallearningforimagerecognition.InCVPR 2016.[21]J.Hensman N.Fusi andN.D.Lawrence.Gaussianprocessesforbigdata.InUAI 2013.[22]J.Hensman A.G.d.G.Matthews andZ.Ghahramani.ScalablevariationalGaussianprocessclassiﬁcation.InICML 2015.[23]S.HochreiterandJ.Schmidhuber.Flatminima.NeuralComputation 9(1):1–42 1997.[24]G.Huang Z.Liu K.Q.Weinberger andL.vanderMaaten.Denselyconnectedconvolutionalnetworks.InCVPR 2017.[25]M.F.Hutchinson.Astochasticestimatorofthetraceoftheinﬂuencematrixforlaplaciansmoothingsplines.CommunicationsinStatistics-SimulationandComputation 19(2):433–450 1990.[26]S.IoffeandC.Szegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.InICML 2015.[27]P.Izmailov D.Podoprikhin T.Garipov D.Vetrov andA.G.Wilson.Averagingweightsleadstowideroptimaandbettergeneralization.InUncertaintyinArtiﬁcialIntelligence(UAI) 2018.[28]Y.Jia E.Shelhamer J.Donahue S.Karayev J.Long R.Girshick S.Guadarrama andT.Darrell.Caffe:Convolutionalarchitectureforfastfeatureembedding.InACMMM pages675–678.ACM 2014.[29]N.S.Keskar D.Mudigere J.Nocedal M.Smelyanskiy andP.T.P.Tang.Onlarge-batchtrainingfordeeplearning:Generalizationgapandsharpminima.arXivpreprintarXiv:1609.04836 2016.[30]A.Krizhevsky I.Sutskever andG.E.Hinton.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.InNIPS 2012.10[31]A.Krizhevsky I.Sutskever andG.E.Hinton.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinneuralinformationprocessingsystems pages1097–1105 2012.[32]C.Lanczos.Aniterationmethodforthesolutionoftheeigenvalueproblemoflineardifferentialandintegraloperators.UnitedStatesGovernm.PressOfﬁceLosAngeles CA 1950.[33]A.G.d.G.Matthews M.vanderWilk T.Nickson K.Fujii A.Boukouvalas P.León-Villagrá Z.Ghahra-mani andJ.Hensman.Gpﬂow:AGaussianprocesslibraryusingTensorFlow.JournalofMachineLearningResearch 18(40):1–6 2017.[34]I.Murray.Gaussianprocessesandfastmatrix-vectormultiplies.InICMLWorkshoponNumericalMathematicsinMachineLearning 2009.[35]D.P.O’Leary.Theblockconjugategradientalgorithmandrelatedmethods.Linearalgebraanditsapplications 29:293–322 1980.[36]C.Paige.PracticaluseofthesymmetricLanczosprocesswithre-orthogonalization.BITNumericalMathematics 10(2):183–195 1970.[37]B.N.Parlett.AnewlookattheLanczosalgorithmforsolvingsymmetricsystemsoflinearequations.Linearalgebraanditsapplications 29:323–346 1980.[38]A.Paszke S.Gross S.Chintala G.Chanan E.Yang Z.DeVito Z.Lin A.Desmaison L.Antiga andA.Lerer.AutomaticdifferentiationinPyTorch.2017.[39]G.Pleiss J.R.Gardner K.Q.Weinberger andA.G.Wilson.Constant-timepredictivedistributionsforGaussianprocesses.InICML 2018.[40]J.Quiñonero-CandelaandC.E.Rasmussen.AunifyingviewofsparseapproximateGaussianprocessregression.JournalofMachineLearningResearch 6(Dec):1939–1959 2005.[41]C.E.RasmussenandC.K.Williams.Gaussianprocessesformachinelearning volume1.MITpressCambridge 2006.[42]Y.Saad.Iterativemethodsforsparselinearsystems volume82.siam 2003.[43]Y.Saatçi.ScalableinferenceforstructuredGaussianprocessmodels.PhDthesis UniversityofCambridge 2012.[44]E.SnelsonandZ.Ghahramani.SparseGaussianprocessesusingpseudo-inputs.InNIPS 2006.[45]M.K.Titsias.VariationallearningofinducingvariablesinsparseGaussianprocesses.InAISTATS pages567–574 2009.[46]S.Ubaru J.Chen andY.Saad.Fastestimationoftr(f(a))viastochasticLanczosquadrature.SIAMJournalonMatrixAnalysisandApplications 38(4):1075–1099 2017.[47]H.A.VanderVorst.IterativeKrylovmethodsforlargelinearsystems volume13.CambridgeUniversityPress 2003.[48]A.J.WathenandS.Zhu.Onspectraldistributionofkernelmatricesrelatedtoradialbasisfunctions.NumericalAlgorithms 70(4):709–726 2015.[49]A.G.Wilson.CovariancekernelsforfastautomaticpatterndiscoveryandextrapolationwithGaussianprocesses.PhDthesis UniversityofCambridge 2014.[50]A.G.WilsonandH.Nickisch.KernelinterpolationforscalablestructuredGaussianprocesses(KISS-GP).InICML 2015.[51]A.G.Wilson C.Dann andH.Nickisch.ThoughtsonmassivelyscalableGaussianprocesses.arXivpreprintarXiv:1511.01870 2015.[52]A.G.Wilson Z.Hu R.Salakhutdinov andE.P.Xing.Deepkernellearning.InAISTATS 2016.[53]A.G.Wilson Z.Hu R.R.Salakhutdinov andE.P.Xing.Stochasticvariationaldeepkernellearning.InNIPS 2016.11,Jacob Gardner
Geoff Pleiss
Kilian Weinberger
David Bindel
Andrew Wilson