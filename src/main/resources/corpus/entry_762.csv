2012,Bayesian estimation of discrete entropy with mixtures of stick-breaking priors,We consider the problem of estimating Shannon's entropy H in the   under-sampled regime  where the number of possible symbols may be   unknown or countably infinite.  Pitman-Yor processes (a   generalization of Dirichlet processes) provide tractable prior   distributions over the space of countably infinite discrete   distributions  and have found major applications in Bayesian   non-parametric statistics and machine learning. Here we show that   they also provide natural priors for Bayesian entropy estimation    due to the remarkable fact that the moments of the induced posterior   distribution over H can be computed analytically. We derive   formulas for the posterior mean (Bayes' least squares estimate) and   variance under such priors.  Moreover  we show that a fixed   Dirichlet or Pitman-Yor process prior implies a narrow prior on H    meaning the prior strongly determines the entropy estimate in the   under-sampled regime. We derive a family of continuous mixing   measures such that the resulting mixture of Pitman-Yor processes   produces an approximately flat (improper) prior over H.  We   explore the theoretical properties of the resulting estimator  and   show that it performs well on data sampled from both exponential and   power-law tailed distributions.,Bayesian estimation of discrete entropy with mixtures

of stick-breaking priors

Evan Archer⇤124  Il Memming Park⇤234  & Jonathan W. Pillow234

The University of Texas at Austin

1. Institute for Computational and Engineering Sciences
2. Center for Perceptual Systems  3. Dept. of Psychology 

4. Division of Statistics & Scientiﬁc Computation

Abstract

We consider the problem of estimating Shannon’s entropy H in the under-sampled
regime  where the number of possible symbols may be unknown or countably
inﬁnite. Dirichlet and Pitman-Yor processes provide tractable prior distributions
over the space of countably inﬁnite discrete distributions  and have found major
applications in Bayesian non-parametric statistics and machine learning. Here
we show that they provide natural priors for Bayesian entropy estimation  due
to the analytic tractability of the moments of the induced posterior distribution
over entropy H. We derive formulas for the posterior mean and variance of H
given data. However  we show that a ﬁxed Dirichlet or Pitman-Yor process prior
implies a narrow prior on H  meaning the prior strongly determines the estimate
in the under-sampled regime. We therefore deﬁne a family of continuous mixing
measures such that the resulting mixture of Dirichlet or Pitman-Yor processes
produces an approximately ﬂat prior over H. We explore the theoretical properties
of the resulting estimators and show that they perform well on data sampled from
both exponential and power-law tailed distributions.

1

Introduction

An important statistical problem in the study of natural systems is to estimate the entropy of an
unknown discrete distribution on the basis of an observed sample. This is often much easier than
the problem of estimating the distribution itself; in many cases  entropy can be accurately estimated
with fewer samples than the number of distinct symbols. Entropy estimation remains a difﬁcult
problem  however  as there is no unbiased estimator for entropy  and the maximum likelihood es-
timator exhibits severe bias for small datasets. Previous work has tended to focus on methods for
computing and reducing this bias [1–5]. Here  we instead take a Bayesian approach  building on a
framework introduced by Nemenman et al [6]. The basic idea is to place a prior over the space of
probability distributions that might have generated the data  and then perform inference using the
induced posterior distribution over entropy. (See Fig. 1).
We focus on the setting where our data are a ﬁnite sample from an unknown  or possibly even count-
ably inﬁnite  number of symbols. A Bayesian approach requires us to consider distributions over
the inﬁnite-dimensional simplex  1. To do so  we employ the Pitman-Yor (PYP) and Dirichlet
(DP) processes [7–9]. These processes provide an attractive family of priors for this problem  since:
(1) the posterior distribution over entropy has analytically tractable moments; and (2) distributions
drawn from a PYP can exhibit power-law tails  a feature commonly observed in data from social  bi-
ological  and physical systems [10–12]. However  we show that a ﬁxed PYP prior imposes a narrow

⇤ These authors contributed equally.

1

parameter

distribution

entropy

...

data

Figure 1: Graphical model illustrating the ingre-
dients for Bayesian entropy estimation. Arrows indi-
cate conditional dependencies between variables  and
the gray “plate” denotes multiple copies of a random
variable (with the number of copies N indicated at
bottom). For entropy estimation  the joint probabil-
ity distribution over entropy H  data x = {xj}  dis-
crete distribution ⇡ = {⇡i}  and parameter ✓ factor-
izes as: p(H  x  ⇡  ✓) = p(H|⇡)p(x|⇡)p(⇡|✓)p(✓).
Entropy is a deterministic function of ⇡  so p(H|⇡) =
(H Pi ⇡i log ⇡i).

prior over entropy  leading to severe bias and overly narrow credible intervals for small datasets. We
address this shortcoming by introducing a set of mixing measures such that the resulting Pitman-Yor
Mixture (PYM) prior provides an approximately non-informative (i.e.  ﬂat) prior over entropy.
The remainder of the paper is organized as follows. In Section 2  we introduce the entropy estimation
problem and review prior work. In Section 3  we introduce the Dirichlet and Pitman-Yor processes
and discuss key mathematical properties relating to entropy. In Section 4  we introduce a novel
entropy estimator based on PYM priors and derive several of its theoretical properties. In Section 5 
we show applications to data.

2 Entropy Estimation

Consider samples x := {xj}N
a ﬁnite or (countably) inﬁnite alphabet X. We wish to estimate the entropy of ⇡ 

j=1 drawn iid from an unknown discrete distribution ⇡ := {⇡i}A

i=1 on

H(⇡) = 

AXi=1

⇡i log ⇡i 

(1)

where we identify X = {1  2  . . .   A} as the set of alphabets without loss of generality (where the
alphabet size A may be inﬁnite)  and ⇡i > 0 denotes the probability of observing symbol i. We
focus on the setting where N ⌧ A.
A reasonable ﬁrst step toward estimating H is to estimate the distribution ⇡. The sum of ob-
served counts nk = PN
i=1 1{xi=k} for each k 2 X yields the empirical distribution ˆ⇡  where
ˆ⇡k = nk/N. Plugging this estimate for ⇡ into eq. 1  we obtain the so-called “plugin” estimator:
ˆHplugin = P ˆ⇡i log ˆ⇡i  which is also the maximum-likelihood estimator. It exhibits substantial

negative bias in the undersampled regime.

2.1 Bayesian entropy estimation

The Bayesian approach to entropy estimation involves formulating a prior over distributions ⇡  and
then turning the crank of Bayesian inference to infer H using the posterior distribution. Bayes’ least
squares (BLS) estimators take the form:

ˆH(x) = E[H|x] =Z H(⇡)p(⇡|x) d⇡

(2)

where p(⇡|x) is the posterior over ⇡ under some prior p(⇡) and categorical likelihood p(x|⇡) =
Qj p(xj|⇡)  where p(xj = i) = ⇡i. The conditional p(H|⇡) = (H Pi ⇡i log ⇡i)  since H is
deterministically related to ⇡. To the extent that p(⇡) expresses our true prior uncertainty over the
unknown distribution that generated the data  this estimate is optimal in a least-squares sense  and
the corresponding credible intervals capture our uncertainty about H given the data.
For distributions with known ﬁnite alphabet size A  the Dirichlet distribution provides an obvious
choice of prior due to its conjugacy to the discrete (or multinomial) likelihood. It takes the form

  for ⇡ on the A-dimensional simplex (⇡i  1 P ⇡i = 1)  with concentration

p(⇡) /QA

i=1 ⇡↵1

i

2

]

 

n
>

 
t

n
u
o
c
d
r
o
w
P

[

100

10(cid:239)1

10(cid:239)2

10(cid:239)3

10(cid:239)4

10(cid:239)5

 
100

Neural Alphabet Frequency

(27 spiking neurons)

]

 

n
>

 
t

n
u
o
c
d
r
o
w
P

[

cell data
95% confidence

101

102

103

wordcount n

104

105

100

10(cid:239)1

10(cid:239)2

10(cid:239)3

10(cid:239)4

10(cid:239)5

 
100

Word Frequency in Moby Dick

 

 

DP
PY
word data
95% confidence

101

102
wordcount n

103

Figure 2: Power-law frequency distributions from neural signals and natural language. We compare
samples from the DP (red) and PYP (blue) priors for two datasets with heavy tails (black). In both
cases  we compare the empirical CDF with distributions sampled given d and ↵ ﬁxed to their ML
estimates. For both datasets  the PYP better captures the heavy-tailed behavior of the data. Left:
Frequencies among N = 1.2e6 neural spike words from 27 simultaneously-recorded retinal ganglion
cells  binarized and binned at 10 ms [18]. Right: Frequency of N = 217826 words in the novel Moby
Dick by Herman Melville.

parameter ↵ [13]. Many previously proposed estimators can be viewed as Bayesian estimators with
a particular ﬁxed choice of ↵. (See [14] for an overview).

2.2 Nemenman-Shafee-Bialek (NSB) estimator

In a seminal paper  Nemenman et al [6] showed that Dirichlet priors impose a narrow prior over
entropy. In the under-sampled regime  Bayesian estimates using a ﬁxed Dirichlet prior are severely
biased  and have small credible intervals (i.e.  they give highly conﬁdent wrong answers!). To ad-
dress this problem  [6] suggested a mixture-of-Dirichlets prior:

(3)
where pDir(⇡|↵) denotes a Dir(↵) prior on ⇡. To construct an approximately ﬂat prior on entropy 
[6] proposed the mixing weights on ↵ given by 

p(⇡) =Z pDir(⇡|↵)p(↵)d↵ 

(4)
where E[H|↵] denotes the expected value of H under a Dir(↵) prior  and 1(·) denotes the tri-
gamma function. To the extent that p(H|↵) resembles a delta function  eq. 3 implies a uniform prior
for H on [0  log A].The BLS estimator under the NSB prior can then be written as 

d
d↵E[H|↵] = A 1(A↵ + 1)  1(↵ + 1) 

p(↵) /

ˆHnsb = E[H|x] =ZZ H(⇡)p(⇡|x  ↵) p(↵|x) d⇡ d↵ =Z E[H|x  ↵]

(5)
where E[H|x  ↵] is the posterior mean under a Dir(↵) prior  and p(x|↵) denotes the evidence 
which has a Polya distribution. Given analytic expressions for E[H|x  ↵] and p(x|↵)  this estimate
is extremely fast to compute via 1D numerical integration in ↵. (See Appendix for details).
Next  we shall consider the problem of extending this approach to inﬁnite-dimensional discrete
distributions. Nemenman et al proposed one such extension using an approximation to ˆHnsb in the
limit A ! 1 which we refer to as ˆHnsb1 [15  16]. Unfortunately  ˆHnsb1 increases unboundedly
with N (as noted by [17])  and it performs poorly for the examples we consider.

p(x|↵)p(↵)

p(x)

d↵ 

3 Stick-Breaking Priors

To construct a prior over countably inﬁnite discrete distributions we employ a class of distributions
from nonparametric Bayesian statistics known as stick-breaking processes [19]. In particular  we

3

focus on two well-known subclasses of stick-breaking processes: the Dirichlet Process (DP) and
Pitman-Yor process (PYP). Both are stochastic processes whose samples are discrete probability
distributions [7  20]. A sample from a DP or PYP may be written asP1i=1 ⇡ii  where ⇡ = {⇡i}
denotes a countably inﬁnite set of ‘weights’ on a set of atoms {i} drawn from some base probability
measure  where i denotes a delta function on the atom i.1 The prior distribution over ⇡ under
the DP and PYP is technically called the GEM distribution or the two-parameter Poisson-Dirichlet
distribution  but we will abuse terminology and refer to it more simply as script notation DP or PY.
The DP weight distribution DP(↵) may be described as a limit of the ﬁnite Dirichlet distributions
where the alphabet size grows and concentration parameter shrinks  A ! 1 and ↵0 ! 0  such that
↵0
A ! ↵ [20]. The PYP generalizes the DP to allow power-law tails  and includes DP as a special
case [7].
Let PY(d  ↵) denote the PYP weight distribution with discount parameter d and concentration pa-
rameter ↵ (also called the “Dirichlet parameter”)  for d 2 [0  1)  ↵ > d. When d = 0  this reduces
to the DP weight distribution  denoted DP(↵). The name “stick-breaking” refers to the fact that
the weights of the DP and PYP can be sampled by transforming an inﬁnite sequence of indepen-
dent Beta random variables in a procedure known as “stick-breaking” [21]. Stick-breaking provides
samples ⇡ ⇠ PY(d  ↵) according to:

i ⇠ Beta(1  d  ↵ + id)

˜⇡i =

i1Yk=1

(1  k)i 

(6)

where ˜⇡i is known as the i’th size-biased sample from ⇡. (The ˜⇡i sampled in this manner are not
strictly decreasing  but decrease on average such thatP1i=1 ˜⇡i = 1 with probability 1). Asymptoti-
cally  the tails of a (sorted) sample from DP(↵) decay exponentially  while for PY(d  ↵) with d 6= 0 
the tails approximately follow a power-law: ⇡i / (i) 1
d ( [7]  pp. 867)2. Many natural phenomena
such as city size  language  spike responses  etc.  also exhibit power-law tails [10  12]. (See Fig. 2).

3.1 Expectations over DP and PY weight distributions

A key virtue of PYP priors is a mathematical property called invariance under size-biased sampling 
which allows us to convert expectations over ⇡ on the inﬁnite-dimensional simplex to one or two-
dimensional integrals with respect to the distribution of the ﬁrst two size-biased samples [23  24].
These expectations are required for computing the mean and variance of H under the prior (or
posterior) over ⇡.
Proposition 1 (Expectations with ﬁrst two size-biased samples). For ⇡ ⇠ PY(d  ↵) and arbitrary
integrable functionals f and g of ⇡ 

E(⇡|d ↵)" 1Xi=1
E(⇡|d ↵)24Xi j6=i

f (⇡i)# = E(˜⇡1|d ↵) f (˜⇡1)
˜⇡1   
g(⇡i  ⇡j)35 = E(˜⇡1 ˜⇡2|d ↵) [g(˜⇡1  ˜⇡2)(1  ˜⇡1)]  

where ˜⇡1 and ˜⇡2 are the ﬁrst two size-biased samples from ⇡.

(7)

(8)

The ﬁrst result (eq. 7) appears in [7]  and we construct an analogous proof for eq. 8 (see Appendix).
The direct consequence of this lemma is that ﬁrst two moments of H(⇡) under the DP and PY
priors have closed forms   which can be obtained using (from eq. 6): ˜⇡1 ⇠ Beta(1  d  ↵ + d)  and
˜⇡2/(1˜⇡1)|˜⇡1 ⇠ Beta(1d  ↵+2d)  with f (⇡i) = ⇡i log(⇡i) for E[H]  and f (⇡i) = ⇡2
i (log ⇡i)2
and g(⇡i  ⇡j) = ⇡i⇡j(log ⇡i)(log ⇡j) for E[H 2].

1Here  we will assume the base measure is non-atomic  so that the atoms i are distinct with probability
1. This allows us to ignore the base measure  making entropy of the distribution equal to the entropy of the
weights ⇡.

2Note that the power-law exponent is given incorrectly in [9  22].

4

Prior Mean

Prior Uncertainty

2

1

)
s
t
a
n
(
 
n
o

i
t

i

a
v
e
d

 

d
r
a
d
n
a

t
s

d=0.9
d=0.8
d=0.7
d=0.6
d=0.5
d=0.4
d=0.3
d=0.2
d=0.1
d=0.0

30

20

10

)
s
t

a
n
(
 
y
p
o
r
t
n
e

 

d
e

t
c
e
p
x
e

0
100

105

1010

0
100

105

1010

Figure 3: Prior mean and standard deviation over entropy H under a ﬁxed PY prior  as a function of
↵ and d. Note that expected entropy is approximately linear in log ↵. Small prior standard deviations
(right) indicate that p(H(⇡)|d  ↵) is highly concentrated around the prior mean (left).

3.2 Posterior distribution over weights
A second desirable property of the PY distribution is that the posterior p(⇡post|x  d  ↵) takes the
form of a (ﬁnite) Dirichlet mixture of point masses and a PY distribution [8]. This makes it possible
to apply the above results to the posterior mean and variance of H.
Let ni denote the count of symbol i in an observed dataset. Then let ↵i = ni  d  N = P ni 
and A =P ↵i =Pi ni  Kd = N  Kd  where K =PA

i=1 1{ni>0} is the number of unique
symbols observed. Given data  the posterior over (countably inﬁnite) discrete distributions  written
as ⇡post = (p1  p2  p3  . . .   pK  p⇤⇡)  has the distribution (given in [19]):

(9)

(p1  p2  p3  . . .   pK  p⇤) ⇠ Dir(n1  d  n2  d  . . .   nK  d  ↵ + Kd)
⇡ := (⇡1  ⇡2  ⇡3  . . . ) ⇠ PY(d  ↵ + Kd).
4 Bayesian entropy inference with PY priors

4.1 Fixed PY priors

Using the results of the previous section (eqs. 7 and 8)  we can derive the prior mean and variance
of H under a PY(d  ↵) prior on ⇡:

E[H(⇡)|d  ↵] = 0(1 + ↵)  0(1  d) 
1  d
var[H(⇡)|d  ↵] =
1 + ↵

↵ + d

 1(2  d)  1(2 + ↵) 

(10)

(11)

(1 + ↵)2(1  d)

+

where n is the polygamma of n-th order (i.e.  0 is the digamma function). Fig. 3 shows these
functions for a range of d and ↵ values. These reveal the same phenomenon that [6] observed for
ﬁnite Dirichlet distributions: a PY prior with ﬁxed (d  ↵) induces a narrow prior over H. In the
undersampled regime  Bayesian estimates under PY priors will therefore be strongly determined by
the choice of (d  ↵)  and posterior credible intervals will be unrealistically narrow.3

4.2 Pitman-Yor process mixture (PYM) prior

The narrow prior on H induced by ﬁxed PY priors suggests a strategy for constructing a non-
informative prior: mix together a family of PY distributions with some hyper-prior p(d  ↵) selected
to yield an approximately ﬂat prior on H. Following the approach of [6]  we setting p(d  ↵) propor-
tional to the derivative of the expected entropy. This leaves one extra degree of freedom  since large
3The only exception is near the corner d ! 1 and ↵ ! d. There  one can obtain arbitrarily large prior
variance over H for given mean. However  these such priors have very heavy tails and seem poorly suited to
data with ﬁnite or exponential tails; we do not explore them further here.

5

)
s
t

a
n
(
 
y
p
o
r
t

n
e

20

15

10

5

1

0

(standard params)

(new params)

1

0

10

0

0

20

10

20

0.1

)

H
(
p

0.05

)

H
(
p

)

H
(
p

0
0.06
0.04
0.02
0
0.06
0.04
0.02
0

0

1

2
3
Entropy (H)

4

5

Figure 4: Expected entropy under Pitman-Yor and Pitman-Yor Mixture priors. (A) Left: expected
entropy as a function of the natural parameters (d  ↵). Right: expected entropy as a function of
(B) Sampled prior distributions (N = 5e3) over entropy implied
transformed parameters (h  ).
by three different PY mixtures: (1) p(  h) / (  1) (red)  a mixture of PY(d  0) distributions; (2)
1 ) (grey)  which
p(  h) / () (blue)  a mixture of DP(↵) distributions; and (3) p(  h) / exp( 10
provides a tradeoff between (1) & (2). Note that the implied prior over H is approximately ﬂat.

prior entropies can arise either from large values of ↵ (as in the DP) or from values of d near 1. (See
Fig. 4A). We can explicitly control this trade-off by reparametrizing the PY distribution  letting

h = 0(1 + ↵)  0(1  d) 

 =

 0(1)  0(1  d)

 0(1 + ↵)  0(1  d)

 

(12)

where h > 0 is equal to the expected entropy of the prior (eq. 10) and  > 0 captures prior beliefs
about tail behavior of ⇡. For  = 0  we have the DP (d = 0); for  = 1 we have a PY(d  0)
process (i.e.  ↵ = 0). Where required  the inverse transformation to standard PY parameters is given
by: ↵ = 01 (h(1  ) + 0(1))  1  d = 1  01 ( 0(1)  h)   where 01(·) denotes the
inverse digamma function.
We can construct an (approximately) ﬂat improper prior over H on [0 1] by setting p(h  ) = q() 
where q is any density on [0 1]. The induced prior on entropy is thus:
p(H) =ZZ p(H|⇡)pPY(⇡|  h)p(  h)d dh 

(13)
where pPY(⇡|  h) denotes a PY distribution on ⇡ with parameters   h. Fig. 4B shows samples
from this prior under three different choices of q()  for h uniform on [0  3]. We refer to the resulting
prior distribution over ⇡ as the Pitman-Yor mixture (PYM) prior. All results in the ﬁgures are
generated using the prior q() / max(1    0).
4.3 Posterior inference

Posterior inference under the PYM prior amounts to computing the two-dimensional integral over
the hyperparameters (d  ↵) 

ˆHPYM = E[H|x] =Z E[H|x  d  ↵]

p(x|d  ↵)p(↵  d)

p(x)

d(d  ↵)

(14)

Although in practice we parametrize our prior using the variables  and h  for clarity and consistency
with other literature we present results in terms of d and ↵. Just as the case with the prior mean  the
posterior mean E[H|x  d  ↵] is given by a convenient analytic form (derived in the Appendix) 
(ni  d) 0(ni  d + 1)# .
E[H|↵  d  x] = 0(↵ + N + 1) 

 0(1  d) 

↵ + Kd
↵ + N

The evidence  p(x|d  ↵)  is given by

p(x|d  ↵) = ⇣QK1

l=1 (↵ + ld)⌘⇣QK

(1  d)K(↵ + N )

6

1

↵ + N " KXi=1
i=1 (ni  d)⌘ (1 + ↵)

(15)

.

(16)

We can obtain conﬁdence regions for ˆHPYM by computing the posterior variance E[(H ˆHPYM)2|x].
The estimate takes the same form as eq. 14  except that we substitute var[H|x  d  ↵] for E[H|x  d  ↵].
Although var[H|x  d  ↵] has an analytic closed form that is fast to compute  it is a lengthy expression
that we do not have space to reproduce here; we provide it in the Appendix.

4.4 Computation

In practice  the two-dimensional integral over ↵ and d is fast to compute numerically. Computation
of the integrand can be carried out more efﬁciently using a representation in terms of multiplicities
(also known as the empirical histogram distribution function [4])  the number of symbols that have
occurred with a given frequency in the sample. Letting mk = |{i : ni = k}| denote the total
number of symbols with exactly k observations in the sample gives the compressed statistic m =
[m0  m1  . . .   mM ]>  where nmax is the largest number of samples for any symbol. Note that the
inner product [0  1  . . .   nmax] · m = N  the total number of samples.
The multiplicities representation signiﬁcantly reduces the time and space complexity of our compu-
tations for most datasets  as we need only compute sums and products involving the number symbols
with distinct frequencies (at most nmax)  rather than the total number of symbols K. In practice  we
compute all expressions not explicitly involving ⇡ using the multiplicities representation. For in-
stance  in terms of the multiplicities  the evidence takes the compressed form

p(x|d  ↵) = p(m1  . . .   mM|d  ↵) =

(1 + ↵)QK1

(↵ + n)

l=1 (↵ + ld)

MYi=1✓ (i  d)

i!(1  d)◆mi M !

mi!

.

(17)

4.5 Existence of posterior mean
Given that the PYM prior with p(h) / 1 on [0 1] is improper  the prior expectation E[H] does
not exist. It is therefore reasonable to ask what conditions on the data are sufﬁcient to obtain ﬁnite
posterior expectation E[H|x]. We give an answer to this question in the following short proposition 
the proof of which we provide in Appendix B.
Theorem 1. Given a ﬁxed dataset x of N samples and any bounded (potentially improper) prior
p(  h)  ˆHPYM < 1 when N  K  2.
This result says that the BLS entropy estimate is ﬁnite whenever there are at least two “coinci-
dences”  i.e.  two fewer unique symbols than samples  even though the prior expectation is inﬁnite.

5 Results

We compare PYM to other proposed entropy estimators using four example datasets in Fig. 5. The
Miller-Maddow estimator is a well-known method for bias correction based on a ﬁrst-order Taylor
expansion of the entropy functional. The CAE (“Coverage Adjusted Estimator”) addresses bias
by combining the Horvitz-Thompson estimator with a nonparametric estimate of the proportion
of total probability mass (the “coverage”) accounted for by the observed data x [17  25]. When
d = 0  PYM becomes a DP mixture (DPM). It may also be thought of as NSB with a very large
A  and indeed the empirical performance of NSB with large A is nearly identical to that of DPM.
All estimators appear to converge except ˆHnsb1  the asymptotic extension of NSB discussed in
Section 2.2  which increases unboundedly with data size. In addition PYM performs competitively
with other estimators. Note that unlike frequentist estimators  PYM error bars in Fig. 5 arise from
direct compution of the posterior variance of the entropy.

6 Discussion

In this paper we introduced PYM  a novel entropy estimator for distributions with unknown support.
We derived analytic forms for the conditional mean and variance of entropy under a DP and PY
prior for ﬁxed parameters. Inspired by the work of [6]  we deﬁned a novel PY mixture prior  PYM 
which implies an approximately ﬂat prior on entropy. PYM addresses two major issues with NSB:
its dependence on knowledge of A and its inability (inherited from the Dirichlet distribution) to

7

Figure 5: Convergence of entropy estimators with sample size  on two simulated and two real datasets.
We write “MiMa” for “Miller-Maddow” and “NSB1” for ˆHnsb1. Note that DPM (“DP mixture”) is
simply a PYM with  = 0. Credible intervals are indicated by two standard deviation of the posterior
for DPM and PYM. (A) Exponential distribution ⇡i / ei. (B) Power law distribution with exponent 2
(⇡i / i2). (C) Word frequency from the novel Moby Dick. (D) Neural words from 8 simultaneously-
recorded retinal ganglion cells. Note that for clarity ˆHnsb1 has been cropped from B and D. All plots
are average of 16 Monte Carlo runs.

account for the heavy-tailed distributions which abound in biological and other natural data. We
have shown that PYM performs well in comparison to other entropy estimators  and indicated its
practicality in example applications to data.
We note  however  that despite its strong performance in simulation and in many practical examples 
we cannot assure that PYM will always be well-behaved. There may be speciﬁc distributions for
which the PYM estimate is so heavily biased that the credible intervals fail to bracket the true en-
tropy. This reﬂects a general state of affairs for entropy estimation on countable distributions: any
convergence rate result must depend on restricting to a subclass of distributions [26]. Rather than
working within some analytically-deﬁned subclass of discrete distributions (such as  for instance 
those with ﬁnite “entropy variance” [17])  we work within the space of distributions parametrized
by PY which spans both the exponential and power-law tail distributions. Although PY parameter-
izes a large class of distributions  its structure allows us to use the PY parameters to understand the
qualitative features of the distributions made likely under a choice of prior. We feel this is a key
feature for small-sample inference  where the choice of prior is most relevant. Moreover  in a forth-
coming paper  we demonstrate the consistency of PYM  and show that its small-sample ﬂexibility
does not sacriﬁce desirable asymptotic properties.
In conclusion  we have deﬁned the PYM prior through a reparametrization that assures an approx-
imately ﬂat prior on entropy. Moreover  although parametrized over the space of countably-inﬁnite
discrete distributions  the computation of PYM depends primarily on the ﬁrst two conditional mo-
ments of entropy under PY. We derive closed-form expressions for these moments that are fast to
compute  and allow the efﬁcient computation of both the PYM estimate and its posterior credible
interval. As we demonstrate in application to data  PYM is competitive with previously proposed
estimators  and is especially well-suited to neural applications  where heavy-tailed distributions are
commonplace.

8

2090400100002.42.62.833.23.43.63.84Retinal Ganglion Cell Spike Trains# of samplesEntropy (nats)1060300100000.60.811.21.41.61.822.2(cid:51)(cid:82)(cid:90)(cid:72)(cid:85)(cid:239)(cid:79)(cid:68)(cid:90)1001600180002100004.555.566.577.5Moby Dick wordsEntropy (nats)ABDC102040902000.60.811.21.41.61.8Exponential distribution# of samplespluginMiMaDPMPYMCAENSB(cid:146)Acknowledgments

We thank E. J. Chichilnisky  A. M. Litke  A. Sher and J. Shlens for retinal data  and Y. .W. Teh for helpful
comments on the manuscript. This work was supported by a Sloan Research Fellowship  McKnight Scholar’s
Award  and NSF CAREER Award IIS-1150186 (JP).

References
[1] G. Miller. Note on the bias of information estimates. Information theory in psychology: Problems and

methods  2:95–100  1955.

[2] S. Panzeri and A. Treves. Analytical estimates of limited sampling biases in different information mea-

sures. Network: Computation in Neural Systems  7:87–107  1996.

[3] R. Strong  S. Koberle  de Ruyter van Steveninck R.  and W. Bialek. Entropy and information in neural

spike trains. Physical Review Letters  80:197–202  1998.

[4] L. Paninski. Estimation of entropy and mutual information. Neural Computation  15:1191–1253  2003.
arXiv preprint  January 2008 
[5] P. Grassberger.

Entropy estimates from insufﬁcient samplings.

[6] I. Nemenman  F. Shafee  and W. Bialek. Entropy and inference  revisited. Adv. Neur. Inf. Proc. Sys.  14 

arXiv:0307138 [physics].

2002.

[7] J. Pitman and M. Yor. The two-parameter Poisson-Dirichlet distribution derived from a stable subordina-

tor. The Annals of Probability  25(2):855–900  1997.

[8] H. Ishwaran and L. James. Generalized weighted chinese restaurant processes for species sampling mix-

ture models. Statistica Sinica  13(4):1211–1236  2003.

[9] S. Goldwater  T. Grifﬁths  and M. Johnson. Interpolating between types and tokens by estimating power-

law generators. Adv. Neur. Inf. Proc. Sys.  18:459  2006.

[10] G. Zipf. Human behavior and the principle of least effort. Addison-Wesley Press  1949.
[11] T. Dudok de Wit. When do ﬁnite sample effects signiﬁcantly affect entropy estimates? Eur. Phys. J. B -

Cond. Matter and Complex Sys.  11(3):513–516  October 1999.

[12] M. Newman. Power laws  Pareto distributions and Zipf’s law. Contemporary physics  46(5):323–351 

[13] M. Hutter. Distribution of mutual information. Adv. Neur. Inf. Proc. Sys.  14:399  2002.
[14] J. Hausser and K. Strimmer. Entropy inference and the James-Stein estimator  with application to nonlin-

ear gene association networks. The Journal of Machine Learning Research  10:1469–1484  2009.

[15] I. Nemenman  W. Bialek  and R. Van Steveninck. Entropy and information in neural spike trains: Progress

on the sampling problem. Physical Review E  69(5):056111  2004.

[16] I. Nemenman. Coincidences and estimation of entropies of random variables with large cardinalities.

2005.

[17] V. Q. Vu  B. Yu  and R. E. Kass. Coverage-adjusted entropy estimation.

Statistics in medicine 

[18] J. W. Pillow  J. Shlens  L. Paninski  A. Sher  A. M. Litke  and E. P. Chichilnisky  E. J. Simoncelli. Nature 

Entropy  13(12):2013–2023  2011.

26(21):4039–4060  2007.

454:995–999  2008.

[19] H. Ishwaran and M. Zarepour. Exact and approximate sum representations for the Dirichlet process.

Canadian Journal of Statistics  30(2):269–283  2002.

[20] J. Kingman. Random discrete distributions. Journal of the Royal Statistical Society. Series B (Method-

ological)  37(1):1–22  1975.

[21] H. Ishwaran and L. F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American

Statistical Association  96(453):161–173  March 2001.

[22] Y. Teh. A hierarchical Bayesian language model based on Pitman-Yor processes. Proceedings of the 21st
International Conference on Computational Linguistics and the 44th annual meeting of the Association
for Computational Linguistics  pages 985–992  2006.

[23] M. Perman  J. Pitman  and M. Yor. Size-biased sampling of poisson point processes and excursions.

Probability Theory and Related Fields  92(1):21–39  March 1992.

[24] J. Pitman. Random discrete distributions invariant under size-biased permutation. Advances in Applied

Probability  pages 525–539  1996.

[25] A. Chao and T. Shen. Nonparametric estimation of Shannon’s index of diversity when there are unseen

species in sample. Environmental and Ecological Statistics  10(4):429–443  2003.

[26] A. Antos and I. Kontoyiannis. Convergence properties of functional estimates for discrete distributions.

Random Structures & Algorithms  19(3-4):163–193  2001.

[27] D. Wolpert and D. Wolf. Estimating functions of probability distributions from a ﬁnite set of samples.

Physical Review E  52(6):6841–6854  1995.

9

,Yuval Harel
Ron Meir
Manfred Opper