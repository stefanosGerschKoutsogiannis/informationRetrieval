2017,Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation,Online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. Despite its importance in many practical applications  it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless NP$\subseteq$BPP  and only an exponential-time sublinear-regret algorithm has been found. In this paper  we introduce mild assumptions to solve the problem. Under these assumptions  we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. In addition  thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.,Efﬁcient Sublinear-Regret Algorithms for Online
Sparse Linear Regression with Limited Observation

Shinji Ito

NEC Corporation

s-ito@me.jp.nec.com

Daisuke Hatano

National Institute of Informatics

hatano@nii.ac.jp

Hanna Sumita

National Institute of Informatics

sumita@nii.ac.jp

Akihiro Yabe

NEC Corporation

a-yabe@cq.jp.nec.com

Takuro Fukunaga

JST  PRESTO

takuro@nii.ac.jp

Naonori Kakimura

Keio University

kakimura@math.keio.ac.jp

Ken-ichi Kawarabayashi

National Institute of Informatics

k-keniti@nii.ac.jp

Abstract

Online sparse linear regression is the task of applying linear regression analysis
to examples arriving sequentially subject to a resource constraint that a limited
number of features of examples can be observed. Despite its importance in many
practical applications  it has been recently shown that there is no polynomial-
time sublinear-regret algorithm unless NP⊆BPP  and only an exponential-time
sublinear-regret algorithm has been found. In this paper  we introduce mild as-
sumptions to solve the problem. Under these assumptions  we present polynomial-
time sublinear-regret algorithms for the online sparse linear regression. In addi-
tion  thorough experiments with publicly available data demonstrate that our al-
gorithms outperform other known algorithms.

1

Introduction

In online regression  a learner receives examples one by one  and aims to make a good prediction
from the features of arriving examples  learning a model in the process. Online regression has
attracted attention recently in the research community in managing massive learning data.In real-
world scenarios  however  with resource constraints  it is desired to make a prediction with only a
limited number of features per example. Such scenarios arise in the context of medical diagnosis of
a disease [3] and in generating a ranking of web pages in a search engine  in which it costs to obtain
features or only partial features are available in each round. In both these examples  predictions need
to be made sequentially because a patient or a search query arrives online.
To resolve the above issue of limited access to features  Kale [7] proposed online sparse regression.
In this problem  a learner makes a prediction for the labels of examples arriving sequentially over
a number of rounds. Each example has d features that can be potentially accessed by the learner.
However  in each round  the learner can acquire the values of at most k(cid:48) features out of the d features 
where k(cid:48) is a parameter set in advance. The learner then makes a prediction for the label of the
example. After the prediction  the true label is revealed to the learner  and the learner suffers a
loss for making an incorrect prediction. The performance of the prediction is measured here by the
standard notion of regret  which is the difference between the total loss of the learner and the total

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Table 1: Computational complexity of online sparse linear regression.

Assumptions
(a)
(1)
(2)
(cid:88) (cid:88)
(cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)

Time complexity

(b)

Hard [5]
Hard (Theorem 1)
Polynomial time (Algorithms 1  2)

(cid:88) Polynomial time (Algorithm 3)

loss of the best predictor. In [7]  the best predictor is deﬁned as the best k-sparse linear predictor 
i.e.  the label is deﬁned as a linear combination of at most k features.
Online sparse regression is a natural online variant of sparse regression; however  its computational
complexity was not well known until recently  as Kale [7] raised a question of whether it is possible
to achieve sublinear regret in polynomial time for online sparse linear regression. Foster et al. [5]
answered the question by proving that no polynomial-time algorithm achieves sublinear regret unless
NP⊆BPP. Indeed  this hardness result holds even when observing Ω(k log d) features per example.
On the positive side  they also proposed an exponential-time algorithm with sublinear regret  when
we can observe at least k + 2 features in each round. However  their algorithm is not expected to
k(cid:48) features in each round  which requires exponential time for any instance.

work efﬁciently in practice. In fact  the algorithm enumerates all the(cid:0) d

(cid:1) possibilities to determine

k(cid:48)

Our contributions.
In this paper  we show that online sparse linear regression admits a
polynomial-time algorithm with sublinear regret  under mild practical assumptions. First  we as-
sume that the features of examples arriving online are determined by a hidden distribution (Assump-
tion (1))  and the labels of the examples are determined by a weighted average of k features  where
the weights are ﬁxed through all rounds (Assumption (2)). These are natural assumptions in the
online linear regression. However  Foster et al. [5] showed that no polynomial-time algorithm can
achieve sublinear regret unless NP⊆BPP even under these two assumptions.1
Owing to this hardness  we introduce two types of conditions on the distribution of features  both
of which are closely related to the restricted isometry property (RIP) that has been studied in the
literature of sparse recovery. The ﬁrst condition  which we call linear independence of features
(Assumption (a))  is stronger than RIP. This condition roughly says that all the features are lin-
early independent. The second condition  which we call compatibility (Assumption (b))  is weaker
than RIP. Thus  an instance having RIP always satisﬁes the compatibility condition. Under these
assumptions  we propose the following three algorithms. Here  T is the number of rounds.

k(cid:48)−k

T ) regret under Assumption (b) for the case

We can also construct an algorithm achieving O( d
where k(cid:48) ≥ k + 2  analogous to Algorithm 1  but we omit it due to space limitations.
Assumptions (1)+(2)+(a) or (1)+(2)+(b) seem to be minimal assumptions needed to achieve sub-
linear regret in polynomial time. Indeed  as listed in Table 1  the problem is hard if any one of the
assumptions is violated  where hard means that no polynomial-time algorithm can achieve sublinear
regret unless NP⊆BPP. Note that Assumption (a) is stronger than (b).
In addition to proving theoretical regret bounds of our algorithms  we perform thorough experi-
ments to evaluate the algorithms. We veriﬁed that our algorithms outperform the exponential-time
algorithm [5] in terms of computational complexity as well as performance of the prediction. Our
algorithms also outperform (baseline) heuristic-based algorithms and algorithms proposed in [2  6]

1 Although the statement in [5] does not mention the assumptions  its proof indicates that the hardness holds

even with these assumptions.

2

√

• Algorithm 1: A polynomial-time algorithm that achieves O( d
k(cid:48)−k

T ) regret  under As-
sumptions (1)  (2)  and (a)  which requires at least k + 2 features to be observed per exam-
ple.

• Algorithm 2: A polynomial-time algorithm that achieves O(
√
• Algorithm 3: A polynomial-time algorithm that achieves O(

k(cid:48)16 ) regret  under
Assumptions (1)  (2)  and (a)  which requires at least k features to be observed per example.
k(cid:48)16 ) regret  under
Assumptions (1)  (2)  and (b)  which requires at least k features to be observed per example.

dT + d16

dT + d16

√

√

for online learning based on limited observation. Moreover  we observe that our algorithms perform
well even for a real dataset  which may not satisfy our assumptions (deciding whether the model
satisﬁes our assumptions is difﬁcult; for example  the RIP parameter cannot be approximated within
any constant factor under a reasonable complexity assumption [9]). Thus  we can conclude that our
algorithm is applicable in practice.

Overview of our techniques. One naive strategy for choosing a limited number of features is to
choose “large-weight” features in terms of estimated ground-truth regression weights. This strategy 
however  does not achieve sublinear regret  as it ignores small-weight features. When we have
Assumption (a)  we show that if we observe two more features chosen uniformly at random  together
with the largest k features  we can make a good prediction. More precisely  using the observed
features  we output the label that minimizes the least-square loss function  based on the technique
using an unbiased estimator of the gradient [2  6] and the regularized dual averaging (RDA) method
(see  e.g.  [11  4]). This idea gives Algorithm 1  and the details are given in Section 4. The reason
why we use RDA is that it is efﬁcient in terms of computational time and memory space as pointed
out in [11] and  more importantly  we will combine this with the (cid:96)1 regularization later. However 
this requires at least k + 2 features to be observed in each round.
To avoid the requirement of two extra observations  the main idea is to employ Algorithm 1 with
a partial dataset. As a by-product of Algorithm 1  we can estimate the ground-truth regression
weight vector with high probability  even without observing extra features in each round. We use
the ground-truth weight vector estimated by Algorithm 1 to choose k features. Combining this idea
with RDA adapted for the sparse regression gives Algorithm 2 (Section 5.1) under Assumption (a).
The compatibility condition (Assumption (b)) is often used in LASSO (Least Absolute Shrinkage
and Selection Operator)  and it is known that minimization with an (cid:96)1 regularizer converges to the
sparse solution under the compatibility condition [1]. We introduce (cid:96)1 regularization into Algo-
rithm 1 to estimate the ground-truth regression weight vector when we have Assumption (b) instead
of Assumption (a). This gives Algorithm 3 (Section 5.2).

t xt)2. The aim is to minimize the total loss(cid:80)T

Related work.
In the online learning problem  a learner aims to predict a model based on the
arriving examples. Speciﬁcally  in the linear function case  a learner predicts the coefﬁcient wt of
a linear function w(cid:62)
t xt whenever an example with features xt arrives in round t. The learner then
suffers a loss (cid:96)t(wt) = (yt − w(cid:62)
t=1((cid:96)t(wt)− (cid:96)t(w))
√
for an arbitrary w. It is known that both the gradient descent method [12] and the dual averaging
method [11] attain an O(
T ) regret even for the more general convex function case. However  these
methods require access to all features of the examples.
In linear regression with limited observation  the limited access to features in regression has been
considered [2  6]. In this problem  a learner can acquire only the values of at most k(cid:48) features among
d features. The purpose here is to estimate a good weight vector  e.g.  minimize the loss function
(cid:96)(w) or the loss function with (cid:96)1 regularizer (cid:96)(w) + (cid:107)w(cid:107)1. Let us note that  even if we obtain a
good weight vector w with small (cid:96)(w)  we cannot always compute w(cid:62)xt from limited observation
of xt and  hence  in our setting the prediction error might not be as small as (cid:96)(w). Thus  our setting
uses a different loss function  deﬁned in Section 2  to minimize the prediction error.
Another problem incorporating the limited access is proposed by Zolghadr et al. [13]. Here  instead
of observing k(cid:48) features  one considers the situation where obtaining a feature has an associated cost.
In each round  one chooses a set of features to pay some amount of money  and the purpose is to
minimize the sum of the regret and the total cost. They designed an exponential-time algorithm for
the problem.
Online sparse linear regression has been studied in [5  7]  but only an exponential-time algorithm
has been proposed so far. In fact  Foster et al. [5] suggested designing an efﬁcient algorithm for a
special class of the problem as future work. The present paper aims to follow this suggestion.
Recently  Kale et al. [8]2 presented computationally efﬁcient algorithms to achieve sublinear regret
under the assumption that input features satisfy RIP. Though this study includes similar results to
ours  we can realize some differences. Our paper considers the assumption of the compatibility
condition without extra observation (i.e.  the case of k(cid:48) = k)  whereas Kale et al. [8] studies a

2The paper [8] was published after our manuscript was submitted.

3

stronger assumption with extra observation (k(cid:48) ≥ k + 2) that yields a smaller regret bound than
ours. They also studies the agnostic (adversarial) setting.

2 Problem setting

Online sparse linear regression. We suppose that there are T rounds  and an example arrives
online in each round. Each example is represented by d features and is associated with a label 
where features and labels are all real numbers. We denote the features of the example arriving in
round t by xt = (xt1  . . .   xtd)(cid:62) ∈ {x ∈ Rd | (cid:107)x(cid:107) ≤ 1}  where the norm (cid:107) · (cid:107) without subscripts
denotes the (cid:96)2 norm. The label of each example is denoted by yt ∈ [−1  1].
The purpose of the online sparse regression is to predict the label yt ∈ R from a partial observation
of xt in each round t = 1  . . .   T . The prediction is made through the following four steps: (i) we
choose a set St ⊆ [d] := {1  . . .   d} of features to observe  where |St| is restricted to be at most k(cid:48);
(ii) observe the selected features {xti}i∈St; (iii) on the basis of observation {xti}i∈St  estimate a
predictor ˆyt of yt; and (iv) observe the true value of yt.
From St  we deﬁne Dt ∈ Rd×d to be the diagonal matrix such that its (i  i)th entries are 1 for i ∈ St
and the other entries are 0. Then  observing the selected features {xti}i∈St in (ii) is equivalent to
observing Dtxt. The predictor ˆyt is computed by ˆyt = w(cid:62)
Throughout the paper  we assume the following conditions  corresponding to Assumptions (1) and
(2) in Section 1  respectively.
Assumption (1) There exists a weight vector w∗ ∈ Rd such that (cid:107)w(cid:107) ≤ 1 and yt = w∗(cid:62)xt + t
for all t = 1  . . .   T   where t ∼ D  independent and identically distributed (i.i.d.)  and
2] = σ2. There exists a distribution Dx on Rd such that xt ∼ Dx  i.i.d. and
E[t] = 0  E[t
independent of {t}.

t Dtxt in (iii).

Assumption (2) The true weight vector w∗ is k-sparse  i.e.  S∗ = supp(w∗) = {i ∈ [d] | w∗

i (cid:54)= 0}

satisﬁes |S∗| ≤ k.

Regret. The performance of the prediction is evaluated based on the regret RT (w) deﬁned by

T(cid:88)

(ˆyt − yt)2 − T(cid:88)

t=1

t=1

RT (w) =

(w(cid:62)xt − yt)2.

(1)

Our goal is to achieve smaller regret RT (w) for an arbitrary w ∈ Rd such that (cid:107)w(cid:107) ≤ 1 and
(cid:107)w(cid:107)0 ≤ k. For random inputs and randomized algorithms  we consider the expected regret
maxw:(cid:107)w(cid:107)0≤k (cid:107)w(cid:107)≤1 E[RT (w)].
Deﬁne the loss function (cid:96)t(w) = (w(cid:62)xt − yt)2. If we compute a predictor ˆyt = w(cid:62)
t Dtxt using
a weight vector wt = (wt1  . . .   wtd)(cid:62) ∈ Rd in each step  we can rewrite the regret RT (w) in (1)
using Dt and wt as

RT (w) =

((cid:96)t(Dtwt) − (cid:96)t(w))

(2)

because (ˆyt − yt)2 = (w(cid:62)
construct wt that minimizes the loss function (cid:96)t(wt)  then the deﬁnition of the regret should be

t Dtxt − yt)2 = (cid:96)t(Dtwt). It is worth noting that if our goal is only to

R(cid:48)
T (w) =

((cid:96)t(wt) − (cid:96)t(w)).

(3)

t=1

However  the goal of online sparse regression involves predicting yt from the limited observation.
Hence  we use (2) to evaluate the performance. In terms of the regret deﬁned by (3)  several algo-
√
rithms based on limited observation have been developed. For example  the algorithms proposed by
Cesa-Bianchi et al. [3] and Hazan and Koren [6] achieve O(

T ) regret of (3).

4

T(cid:88)

t=1

T(cid:88)

3 Extra assumptions on features of examples

Foster et al. [5] showed that Assumptions (1) and (2) are not sufﬁcient to achieve sublinear regret.
Owing to this observation  we impose extra assumptions.
t xt] ∈ Rd×d and let L be the Cholesky decomposition of V (i.e.  V = L(cid:62)L). Denote
Let V := E[x(cid:62)
the largest and the smallest singular values of L by σ1 and σd  respectively. Under Assumption (1)
in Section 2  we have σ1 ≤ 1 because  for arbitrary unit vector u ∈ Rd  it holds that u(cid:62)V u =
E[(u(cid:62)x)2] ≤ 1. For a vector w ∈ R[d] and S ⊆ [d]  we let wS denote the restriction of w onto S.
For S ⊆ [d]  Sc denotes [d] \ S. We assume either one of the following conditions holds.

(a) Linear independence of features: σd > 0.
(b) Compatibility: There exists a constant φ0 > 0 that satisﬁes φ2

w ∈ Rd with (cid:107)w(S∗)c(cid:107)1 ≤ 2(cid:107)wS∗(cid:107)1.

0(cid:107)wS∗(cid:107)2

1 ≤ kw(cid:62)V w for all

We assume the linear independence of features in Sections 4 and 5.1  and the compatibility in Sec-
tion 5.2 to develop efﬁcient algorithms.
Note that condition (a) means that L is non-singular  and so is V . In other words  condition (a)
indicates that the features in xt are linearly independent. This is the reason why we call condition
(a) the “linear independence of features” assumption. Note that the linear independence of features
does not imply the stochastic independence of features.
Conditions (a) and (b) are closely related to RIP. Indeed  condition (b) is a weaker assumption than
RIP  and RIP is weaker than condition (a)  i.e.  (a) linear independence of features =⇒ RIP =⇒
(b) compatibility (see  e.g.  [1]). We now clarify how the above two assumptions are connected to
the regret. The expectation of the loss function (cid:96)t(w) is equal to

Ext yt[(cid:96)t(w)] = Ext∼Dx t∼D [(w(cid:62)xt − w∗(cid:62)xt − t)2]

= Ext∼Dx [((w − w∗)(cid:62)xt)2] + Et∼D [(cid:62)

t t] = (w − w∗)(cid:62)V (w − w∗) + σ2

for all t  where the second equality comes from E[t] = 0 and that xt and t are independent. Denote
this function by (cid:96)(w)  and then (cid:96)(w) is minimized when w = w∗. If Dt and wt are determined
independently of xt and yt  the expectation of the regret RT (w) satisﬁes
((cid:96)(Dtwt) − (cid:96)(w∗))]

((cid:96)(Dtwt) − (cid:96)(w))] ≤ E[

E[RT (w)] = E[

T(cid:88)

= E[

(Dtwt − w∗)(cid:62)V (Dtwt − w∗)] = E[

(cid:107)L(Dtwt − w∗)(cid:107)2].

(4)

We bound (4) in the analysis.

t=1

t=1

Hardness result. Similarly to [5]  we can show that it remains hard under Assumptions (1)  (2) 
and (a). Refer to Appendix A for the proof.
Theorem 1. Let D be any positive constant  and let cD ∈ (0  1) be a constant dependent on D.
Suppose that Assumptions (1) and (2) hold with k = O(dcD ) and k(cid:48) = (cid:98)kD ln d(cid:99). If an algorithm
for the online sparse regression problem runs in poly(d  T ) time per iteration and achieves a regret
at most poly(d  1/σd)T 1−δ in expectation for some constant δ > 0  then NP⊆BPP.

4 Algorithm with extra observations and linear independence of features
In this section  we present Algorithm 1. Here we assume k(cid:48) ≥ k + 2  in addition to the linear
independence of features (Assumption (a)). The additional assumption will be removed in Section 5.
As noted in Section 2  our algorithm ﬁrst computes a weight vector wt  chooses a set St of k(cid:48)
features to be observed  and computes a label ˆyt by ˆyt = w(cid:62)
t Dtxt in each round t. In addition 
our algorithm constructs an unbiased estimator ˆgt of the gradient gt of the loss function (cid:96)t(w) at
w = wt  i.e.  gt = ∇w(cid:96)t(wt) = 2xt(x(cid:62)
t wt − yt) at the end of the round. In the following  we
describe how to compute wt  St  and ˆgt in round t  respectively  assuming that wt(cid:48)  St(cid:48)  and ˆgt(cid:48) are
computed in the previous rounds t(cid:48) = 1  . . .   t− 1. The entire algorithm is described in Algorithm 1.

5

T(cid:88)
T(cid:88)

t=1

t=1

T(cid:88)

Algorithm 1
Input: {xt  yt} ⊆ Rd × R  {λt} ⊆ R>0  k(cid:48) ≥ 2 and k1 ≥ 0 such that k1 ≤ k(cid:48) − 2.
1: Set ˆh0 = 0.
2: for t = 1  . . .   T do
3:
4:
5:
6: end for

Deﬁne wt by (5) and deﬁne St by Observe(wt  k(cid:48)  k1).
Observe Dtxt and output ˆyt := w(cid:62)
Observe yt and deﬁne ˆgt by (6) and set ˆht = ˆht−1 + ˆgt

t Dtxt.

Deﬁne ˆht−1 = (cid:80)t−1

Computing wt. We use ˆg1  . . .   ˆgt−1 to estimate wt by the dual averaging method as follows.
j=1 ˆgj  which is the average of all estimators of gradients computed in the pre-
vious rounds. Moreover  let (λ1  . . .   λT ) be a monotonically non-decreasing sequence of positive
numbers. From these  we deﬁne wt by
ˆh(cid:62)
t−1w +

wt = arg min

(cid:107)w(cid:107)2

= −

(cid:26)

(cid:27)

(5)

1

max{λt (cid:107)ˆht−1(cid:107)} ˆht−1 

w∈Rd (cid:107)w(cid:107)≤1

λt
2

Computing St. Let k1 be an integer such that k1 ≤ k(cid:48) − 2. We deﬁne Ut ⊆ [d] as the set of the k1
largest features with respect to wt  i.e.  choose Ut so that |Ut| = k1 and all i ∈ Ut and j ∈ [d] \ Ut
satisfy |wti| ≥ |wtj|. Let Vt be the set of (k(cid:48) − k1) elements chosen from [d] \ Ut uniformly at
random. Then our algorithm observes the set St = Ut ∪ Vt of the k(cid:48) features. We call this procedure
to obtain St Observe(wt  k(cid:48)  k1).
Observation 1. We observe that Ut ⊆ St and Prob[i  j ∈ St] ≥ (k(cid:48)−k1)(k(cid:48)−k1−1)
Thus  Prob[i  j ∈ St] > 0 for all i  j ∈ [d] if k(cid:48) ≥ k1 + 2.

=: Cd k(cid:48) k1.

d(d−1)

For simplicity  we use the notation p(t)

i = Prob[i ∈ St] and p(t)
Computing ˆgt. Deﬁne ˜Xt = (˜xtij) ∈ Rd×d by ˜Xt = Dtx(cid:62)
whose (i  j)-th entry is ˜xtij/p(t)
deﬁning zt = (zti) ∈ Rd by zti = xti/p(t)
unbiased estimator of xt. Using Xt and zt  we deﬁne ˆgt to be
ˆgt = 2Xtwt − 2ytzt.

ij = Prob[i  j ∈ St] for i  j ∈ [d].
t xtDt and let Xt ∈ Rd×d be a matrix
t . Similarly 
for i ∈ St and zti = 0 for i /∈ St  we see that zt is an

ij . It follows that Xt is an unbiased estimator of xtx(cid:62)

(6)

i

√

k(cid:48)−k

T ) in expectation.

Regret bound of Algorithm 1. Let us show that
O( d
Theorem 2. Suppose that the linear independence of features is satisﬁed and k ≤ k(cid:48) − 2. Let k1
be an arbitrary integer such that k ≤ k1 ≤ k(cid:48) − 2. Then  for arbitrary w ∈ Rd with (cid:107)w(cid:107) ≤ 1 
Algorithm 1 achieves E[RT (w)] ≤ 3
t/Cd k(cid:48) k1
for each t = 1  . . .   T   we obtain

the regret achieved by Algorithm 1 is

. By setting λt = 8

(cid:80)T

+ λT +1

Cd k(cid:48)  k1

(cid:17)

√

1
λt

t=1

σ2
d

2

(cid:16) 16
(cid:115)

E[RT (w)] ≤ 24
σ2
d

d(d − 1)

(k(cid:48) − k1)(k(cid:48) − k1 − 1)

· √

T + 1.

(7)

E[(cid:80)T

The rest of this section is devoted to proving Theorem 2. By (4) 

it sufﬁces to evaluate
t=1 (cid:107)L(Dtwt − w∗)(cid:107)2] instead of E[RT (w)]. The following lemma asserts that each term
of (4) can be bounded  assuming the linear independence of features. Proofs of all lemmas are given
in the supplementary material.
Lemma 3. Suppose that the linear independence of features is satisﬁed. If St ⊇ Ut 

(cid:107)L(Dtwt − w∗)(cid:107)2 ≤ 3
σ2
d

(cid:107)L(wt − w∗)(cid:107)2.

(8)

6

Proof. We have

(cid:107)L(Dtwt − w∗)(cid:107)2 ≤ σ2

1(cid:107)Dtwt − w∗(cid:107)2 = σ2

1

≤ σ2

1

(wti − w∗

i )2 +

i∈S∗∩St

 (cid:88)
(cid:107)wt − w∗(cid:107)2 +
(cid:0)2w2

(cid:88)
  

i

w∗2

i∈S∗\St

(cid:88)
i )2(cid:1)

i∈S∗\St

(cid:88)

i∈St\S∗

w∗2
i +



w2
ti

(9)

(10)

where the second inequality holds since w∗

i = 0 for i ∈ [d] \ S∗. It holds that

(cid:88)

i∈S∗\St
≤ 2

i ≤ (cid:88)
(cid:88)

w∗2

w2

ti + 2

i∈S∗\Ut

i ≤ (cid:88)
(cid:88)

w∗2

i∈S∗\Ut
(wti − w∗

i∈Ut\S∗

i∈S∗\Ut

ti + 2(wti − w∗

i )2 ≤ 2(cid:107)wt − w∗(cid:107)2.

The ﬁrst and third inequalities come from Ut ⊆ St and the deﬁnition of Ut. Putting (10) into (9) 
we have

(cid:107)L(Dtwt − w∗)(cid:107)2 ≤ 3σ2

1(cid:107)wt − w∗(cid:107)2 ≤ 3σ2
1
σ2
d

(cid:107)L(wt − w∗)(cid:107)2.

.
√
t)  the right-hand side of (11) is O(

Gt +

t=1

2

includes the support of w∗. Moreover  it holds that(cid:80)T

t=1 E[(cid:107)L(wt − w∗)(cid:107)2] = E[(cid:80)T
T (w∗)]  since wt is independent of xt and yt. Thus  to bound(cid:80)T

It follows from the above lemma that  if wt converges to w∗  we have Dtwt = w∗  and hence St
t=1((cid:96)t(wt)−
t=1 E[(cid:107)L(wt −
(cid:96)t(w∗))] = E[R(cid:48)
w∗)(cid:107)2]  we shall evaluate E[R(cid:48)
Lemma 4 ([11]). Suppose that wt is deﬁned by (5) for each t = 1  . . .   T   and w ∈ Rd satisﬁes
(cid:107)w(cid:107) ≤ 1. Let Gt = E[(cid:107)ˆgt(cid:107)2] for t = 1  . . .   T . Then 
1
λt

T (w)] ≤ T(cid:88)

T (w∗)].

E[R(cid:48)

λT +1

(11)

ij = Ω(1).

T ). The following lemma shows

√
If Gt = O(1) and λt = Θ(
that this is true if p(t)
Lemma 5. Suppose that the linear independence of features is satisﬁed. Let t ∈ [T ]  and let q be a
positive number such that q ≤ min{p(t)
We are now ready to prove Theorem 2.

ij }. Then we have Gt ≤ 16/q.
  p(t)

Proof of Theorem 2. The expectation E[RT (w)] of the regret

(cid:80)T
t=1 E[(cid:107)L(wt − w∗)(cid:107)2] = 3
T (w∗)] ≤ HT := (cid:80)T

(cid:80)T
t=1 E[(cid:107)L(Dtwt − w∗)(cid:107)2] ≤ 3
Gt ≤ 16/Cd k(cid:48) k1. Hence  for λt = 8(cid:112)Cd k(cid:48) k1t  HT satisﬁes HT ≤(cid:80)T
(cid:80)T

is bounded as E[RT (w)] ≤
T (w∗)]  where the ﬁrst
T (w∗)]
. Lemma 5 and Observation 1 yield
2 =
T + 1. Combining the above three inequali-

inequality comes from (4) and the second comes from Lemma 3. From Lemma 4  E[R(cid:48)
is bounded by E[R(cid:48)

T + 1 ≤ 8

Gt + λT +1

16
Cd k(cid:48) k1

+ λT +1

E[R(cid:48)

√

√

1
λt

t=1

t=1

σ2
d

σ2
d

λt

2

i

1√
Cd k(cid:48)  k1

2√
Cd k(cid:48)  k1

+
ties  we obtain (7).

t=1

t

4√
Cd k(cid:48)  k1

5 Algorithms without extra observations

5.1 Algorithm 2: Assuming (a) the linear independence of features
In Section 4  Lemma 3 showed a connection between RT and R(cid:48)
under Ut ⊆ St. Then  Lemmas 4 and 5 gave an upper bound of E[R(cid:48)

T : E[RT (w)] ≤ 3σ2
T (w∗)]: E[R(cid:48)

σd2 E[R(cid:48)
T (w∗)] = O(

T (w∗)]
T )

√

1

7

(cid:80)s

ij = Ω(1). In the case of k(cid:48) = k  however  the conditions Ut ⊆ St and p(t)

under p(t)
ij = Ω(1) may
not be satisﬁed simultaneously  since  if Ut ⊆ St and |St| = k(cid:48) = k ≥ k1 = |Ut|  then we have
ij = 0 for i /∈ Ut or j /∈ Ut. Thus  we cannot use both relationships for the
Ut = St  which means p(t)
analysis. In Algorithm 2  we bound RT (w) without bounding R(cid:48)
T (w).
of {1  2  . . .   T} by the set of squares  i.e.  J = {s2 | s = 1  . . .  (cid:98)√
Let us describe an idea of Algorithm 2. To achieve the claimed regret  we ﬁrst deﬁne a subset J
T(cid:99)}. Let ts denote the s-th
smallest number in J for each s = 1  . . .  |J|. In each round t  the algorithm computes St  a weight
vector ˜wt  and a vector Dt˜gt  where ˜gt is the gradient of (cid:96)t(w) at w = Dt ˜wt. In addition  if t = ts 
j=1 wj  and an unbiased estimator
the algorithm computes other weight vectors ws and ¯ws := 1
s
ˆgs of the gradient of the loss function (cid:96)t(w) at ws.
At the beginning of round t  if t = ts  the algorithm ﬁrst computes ws  and ¯ws is deﬁned as the
average of w1  . . .   ws. Roughly speaking  ws is the weight vector computed with Algorithm 1
applied to the examples (xt1  yt1)  . . .   (xts  yts)  setting k1 to be at most k − 2. Then  we can
show that ¯ws is a consistent estimator of w∗. This step is only performed if t ∈ J. Then St is
deﬁned from ¯ws  where s is the largest number such that ts ≤ t. Thus  St does not change for any
t ∈ [ts  ts+1 − 1]. After this  the algorithm computes ˜wt from D1˜g1  . . .   Dt−1˜gt−1  and predicts
the label of xt as ˆyt := ˜w(cid:62)
t Dtxt. At the end of the round  the true label yt is observed  and Dt˜gt
is computed from wt and (Dtxt  yt). In addition  if t = ts  ˆgs is computed as in Algorithm 1. We
need ˆgs for computing ws(cid:48) with s(cid:48) > s in the subsequent rounds ts(cid:48).
The following theorem bounds the regret of Algorithm 2. See the supplementary material for details
of the algorithm and the proof of the theorem.
Theorem 6. Suppose that (a)  the linear independence of features  is satisﬁed and k ≤ k(cid:48). Then 
there exists a polynomial-time algorithm such that E[RT (w)] is at most

(cid:88)

i∈S∗

(cid:88)

i∈S∗

√

√

d)

8(1+

T + 1+12T

i | exp(− C 2
|w∗

d k(cid:48) 0(T 1

4 − 1)|w∗
18432

i |2σ2

d

)+4

|w∗
i |(

4096
d k(cid:48) 0w∗4
C 2

i σ4
d

+1)2 

for arbitrary w ∈ Rd with (cid:107)w(cid:107) ≤ 1  where Cd k(cid:48) 0 = k(cid:48)(k(cid:48)−1)

d(d−1) = O( k(cid:48)2

d2 ).2

5.2 Algorithm 3: Assuming (b) the compatibility condition

√

t=1 Prob[i /∈ St] = O(

generate {St} that satisﬁes(cid:80)T
by deﬁning St as the set of k largest features with respect to a weight vector ¯ws = (cid:80)s

Algorithm 3 adopts the same strategy as Algorithm 2 except for the procedure for determining ws
and ¯ws. In the analysis of Algorithm 2  we show that  to achieve the claimed regret  it sufﬁces to
T ) for i ∈ S∗. The condition was satisﬁed
j=1 wj/s.
The linear independence of features guarantees that ¯ws computed in Algorithm 2 converges to w∗ 
and hence {St} deﬁned as above possesses the required property. Unfortunately  if the assumption
of the independence of features is not satisﬁed  e.g.  if we have almost same features  then ¯ws does
not converge to w∗. However  if we introduce an (cid:96)1-regularization to the minimization problem in
the deﬁnition of ws and change the deﬁnition of ¯ws to a weighted average of the modiﬁed vectors
w1  . . .   ws  then we can generate a required set {St} under the compatibility assumption. See the
supplementary material for details and the proof of the following theorem.
Theorem 7. Suppose that (b)  the compatibility assumption  is satisﬁed and k ≤ k(cid:48). Then  there
exists a polynomial-time algorithm such that E[RT (w)] is at most
i |2φ2

(cid:88)

(cid:88)

(cid:112)

√

√

0

8(1+

d)

T +1 + 12T

|w∗

i | exp(− Cd k(cid:48) 0

4 −1|w∗
T 1
5832k

|w∗
i |(

64 · 364k2
d k(cid:48) 0w∗4
i φ4
C 2
0

+1)2 

) + 4

i∈S∗

i∈S∗

for arbitrary w ∈ Rd with (cid:107)w(cid:107) ≤ 1  where Cd k(cid:48) 0 = k(cid:48)(k(cid:48)−1)

d(d−1) = O( k(cid:48)2

d2 ).3 4

3 The asymptotic regret bound mentioned in Section 1  can be yielded by bounding the second term with

the aid of the following: maxT≥0 T exp(−αT β) = (αβ)

β exp(−1/β) for arbitrary α > 0  β > 0.
− 1

4Note that φ0 is the constant appearing in Assumption (b) in Section 3.

8

6 Experiments

In this section  we compare our algorithms with the following four baseline algorithms: (i) a greedy
method that chooses the k(cid:48) largest features with respect to wt computed as in Algorithm 1; (ii)
a uniform-random method that chooses k(cid:48) features uniformly at random; (iii) the algorithm of [6]
(called AELR); and (iv) the algorithm of [5] (called FKK). Owing to space limitations  we only
present typical results here. Other results and the detailed descriptions on experiment settings are
provided in the supplementary material.
Synthetic data. First we show results on two kinds of synthetic datasets: instances with (d  k  k(cid:48))
and instances with (d  k1  k). We set k1 = k in the setting of (d  k  k(cid:48)) and k(cid:48) = k in the setting of
(d  k1  k). The instances with (d  k  k(cid:48)) assume that Algorithm 1 can use the ground truth k  while
Algorithm 1 cannot use k in the instances with (d  k1  k). For each (d  k  k(cid:48)) and (d  k1  k)  we
executed all algorithms on ﬁve instances with T = 5000 and computed the averages of regrets and
run time  respectively. When (d  k  k(cid:48)) = (20  5  7)  FKK spent 1176 s on average  while AELR
spent 6 s  and the others spent at most 1 s.
Figures 1 and 2 plot the regrets given by (1) over the number of rounds on a typical instance with
(d  k  k(cid:48)) = (20  5  7). Tables 2 and 3 summarize the average regrets at T = 5000  where A1  A2 
A3  G  and U denote Algorithm 1  2  3  greedy  and uniform random  respectively. We observe that
Algorithm 1 achieves smallest regrets in the setting of (d  k  k(cid:48))  whereas Algorithms 2 and 3 are
better than Algorithm 1 in the setting of (d  k1  k). The results match our theoretical results.

Figure 1: Plot of regrets with
(d  k  k(cid:48)) = (20  5  7)

Figure 2: Plot of regrets with
(d  k1  k) = (20  5  7)

Figure 3: CT-slice datasets

Table 2: Values of RT /102 when changing
(d  k  k(cid:48)).

Table 3: Values of RT /102 when changing
(d  k1  k).

(d  k1  k)
(10 2 4)

A1
1.53

A2
2.38

A3
3.60

G

33.28

U

25.73

AELR FKK
24.05
60.76

(d  k1  k)
(10 2 4)

A1
26.88

A2
20.59

A3
17.19

G

43.03

U

60.02

AELR FKK
58.71
64.75

Real data. We next conducted experiments using a CT-slice dataset  which is available online [10].
Each data consists of 384 features retrieved from 53500 CT images associated with a label that
denotes the relative position of an image on the axial axis.
We executed all algorithms except FKK  which does not work due to its expensive run time. Since
we do not know the ground-truth regression weights  we measure the performance by the ﬁrst term
of (1)  i.e.  square loss of predictions. Figure 3 plots the losses over the number of rounds. The
parameters are k1 = 60 and k(cid:48) = 70. For this instance  the run times of Algorithms 1 and 2  greedy 
uniform random  and AELR were 195  35  147  382  and 477 s  respectively.
We observe that Algorithms 2 and 3 are superior to the others  which implies that Algorithm 2 and 3
are suitable for instances where the ground truth k is not known  such as real data-based instances.

Acknowledgement

This work was supported by JST ERATO Grant Number JPMJER1201  Japan.

References
[1] P. B¨uhlmann and S. van de Geer. Statistics for high-dimensional data. 2011.

9

010002000300040005000T01000200030004000500060007000RTAlgorithm 1Algorithm 2Algorithm 3greedyuniform randomAELRFKK010002000300040005000T01000200030004000500060007000RTAlgorithm 1Algorithm 2Algorithm 3greedyuniform randomAELRFKK01000020000300004000050000T0.000.250.500.751.001.251.50T∑t=0(Įyt−yt)21e8Algorithm 1Algorithm 2Algorithm 3greedyuniform randomAELR[2] N. Cesa-Bianchi  S. Shalev-Shwartz  and O. Shamir. Some impossibility results for budgeted

learning. In Joint ICML-COLT workshop on Budgeted Learning  2010.

[3] N. Cesa-Bianchi  S. Shalev-Shwartz  and O. Shamir. Efﬁcient learning with partially observed

attributes. Journal of Machine Learning Research  12:2857–2878  2011.

[4] X. Chen  Q. Lin  and J. Pena. Optimal regularized dual averaging methods for stochastic
optimization. In Advances in Neural Information Processing Systems  pages 395–403  2012.

[5] D. Foster  S. Kale  and H. Karloff. Online sparse linear regression. In 29th Annual Conference

on Learning Theory  pages 960–970  2016.

[6] E. Hazan and T. Koren. Linear regression with limited observation. In Proceedings of the 29th

International Conference on Machine Learning (ICML-12)  pages 807–814  2012.

[7] S. Kale. Open problem: Efﬁcient online sparse regression. In Proceedings of The 27th Con-

ference on Learning Theory  pages 1299–1301  2014.

[8] S. Kale  Z. Karnin  T. Liang  and D. P´al. Adaptive feature selection: Computationally efﬁcient
online sparse linear regression under rip. In Proceedings of the 34th International Conference
on Machine Learning (ICML-17)  pages 1780–1788  2017.

[9] P. Koiran and A. Zouzias. Hidden cliques and the certiﬁcation of the restricted isometry prop-

erty. IEEE Trans. Information Theory  60(8):4999–5006  2014.

[10] M. Lichman. UCI machine learning repository  2013.

[11] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization.

Journal of Machine Learning Research  11:2543–2596  2010.

[12] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03)  pages
928–936  2003.

[13] N. Zolghadr  G. Bart´ok  R. Greiner  A. Gy¨orgy  and C. Szepesv´ari. Online learning with costly
features and labels. In Advances in Neural Information Processing Systems  pages 1241–1249 
2013.

10

,Shinji Ito
Daisuke Hatano
Hanna Sumita
Takuro Fukunaga