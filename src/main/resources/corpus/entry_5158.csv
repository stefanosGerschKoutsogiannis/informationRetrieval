2014,Spectral Methods for Indian Buffet Process Inference,The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm  concentration of measure bounds  and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.,Spectral Methods for Indian Buffet Process Inference

Hsiao-Yu Fish Tung

Machine Learning Department
Carnegie Mellon University

Pittsburgh  PA 15213

Alexander J. Smola

Machine Learning Department

Carnegie Mellon University and Google

Pittsburgh  PA 15213

Abstract

The Indian Buffet Process is a versatile statistical tool for modeling distributions
over binary matrices. We provide an efﬁcient spectral algorithm as an alternative
to costly Variational Bayes and sampling-based algorithms. We derive a novel
tensorial characterization of the moments of the Indian Buffet Process proper and
for two of its applications. We give a computationally efﬁcient iterative inference
algorithm  concentration of measure bounds  and reconstruction guarantees. Our
algorithm provides superior accuracy and cheaper computation than comparable
Variational Bayesian approach on a number of reference problems.

1

Introduction

Inferring the distributions of latent variables is a key tool in statistical modeling. It has a rich history
dating back over a century to mixture models for identifying crabs [27] and has served as a key tool
for describing diverse sets of distributions ranging from text [10] to images [1] and user behavior [4].
In recent years spectral methods have become a credible alternative to sampling [19] and variational
methods [9  13] for the inference of such structures. In particular  the work of [6  5  11  21  29]
demonstrates that it is possible to infer latent variable structure accurately  despite the problem
being nonconvex  thus exhibiting many local minima. A particularly attractive aspect of spectral
methods is that they allow for efﬁcient means of inferring the model complexity in the same way
as the remaining parameters  simply by thresholding eigenvalue decomposition appropriately. This
makes them suitable for nonparametric Bayesian approaches.
While the issue of spectral inference in Dirichlet Distribution is largely settled [6  7]  the domain
of nonparametric tools is much richer and it is therefore desirable to see whether the methods can
be extended to other models such as the Indian Buffet Process (IBP). This is the main topic of our
paper. We provide a full analysis of the tensors arising from the IBP and how spectral algorithms
need to be modiﬁed  since a degeneracy in the third order tensor requires fourth order terms. To
recover the parameters and latent factors  we use Excess Correlation Analysis (ECA) [8] to whiten
the higher order tensors and to reduce their dimensionality. Subsequently we employ the power
method to obtain symmetric factorization of the higher-order terms. The method provided in this
work is simple to implement and has high efﬁciency in recovering the latent factors and related
parameters. We demonstrate how this approach can be used in inferring an IBP structure in the
models discussed in [18] and [24]. Moreover  we show that empirically the spectral algorithm
provides higher accuracy and lower runtime than variational methods [14]. Statistical guarantees for
recovery and stability of the estimates conclude the paper.
Outline: Section 2 gives a brief primer on the IBP. Section 3 contains the lower-order moments
of IBP and its application on different model. Section 5 discusses concentration of measure of
moments. Section 4 applies Excess Correlation Analysis to the moments and it provides the basic
structure of this Algorithm. Section 6 shows the empirical performance of our algorithm. Due to
space constraints we relegate most derivations and proofs to the appendix.

1

2 The Indian Buffet Process

The Indian Buffet Process deﬁnes a distribution over equivalence classes of binary matrices Z with
a ﬁnite number of rows and a (potentially) inﬁnite number of columns [17  18]. The idea is that
this allows for automatic adjustment of the number of binary entries  corresponding to the number
of independent sources  underlying causes  etc. This is a very useful strategy and it has led to many
applications including structuring Markov transition matrices [15]  learning hidden causes with a
bipartite graph [30] and ﬁnding latent features in link prediction [26]. n ∈ N the number of rows of
Z  i.e. the number of customers sampling dishes from the “ Indian Buffet”  let mk be the number of
customers who have sampled dish k  let K+ be the total number of dishes sampled  and denote by
Kh the number of dishes with a particular selection history h ∈ {0; 1}n. That is  Kh > 1 only if
there are two or more dishes that have been selected by exactly the same set of customers. Then the
probability of generating a particular matrix Z is given by [18]

(cid:34)

p(Z) =

−α

exp

αK+(cid:81)

h Kh!

(cid:35) K+(cid:89)

n(cid:88)

1
j

j=1

k=1

(n − mk)!(mk − 1)!

n!

(1)

Here α is a parameter determining the expected number of nonzero columns in Z. Due to the
conjugacy of the prior an alternative way of viewing p(Z) is that each column (aka dish) contains
nonzero entries Zij that are drawn from the binomial distribution Zij ∼ Bin(πi). That is  if we
knew K+  i.e. if we knew how many nonzero features Z contains  and if we knew the probabilities
πi  we could draw Z efﬁciently from it. We take this approach in our analysis: determine K+ and
infer the probabilities πi directly from the data. This is more reminiscent of the model used to derive
the IBP — a hierarchical Beta-Binomial model  albeit with a variable number of entries:

α

πi

Zij

i ∈ (cid:8)K+

(cid:9)

j ∈ {n}

In general  the binary attributes Zij are not observed. Instead  they capture auxiliary structure per-
tinent to a statistical model of interest. To make matters more concrete  consider the following two
models proposed by [18] and [24]. They also serve to showcase the algorithm design in our paper.

Linear Gaussian Latent Feature Model [18]. The assumption is that we observe vectorial data
x. It is generated by linear combination of dictionary atoms A and an associated unknown number
of binary causes z  all corrupted by some additive noise . That is  we assume that

x = Az +  where  ∼ N (0  σ21) and z ∼ IBP(α).

(2)
The dictionary matrix A is considered to be ﬁxed but unknown. In this model our goal is to infer both
A  σ2 and the probabilities πi associated with the IBP model. Given that  a maximum-likelihood
estimate of Z can be obtained efﬁciently.

Inﬁnite Sparse Factor Analysis [24]. A second model is that of sparse independent component
analysis. In a way  it extends (2) by replacing binary attributes with sparse attributes. That is  instead
of z we use the entry-wise product z.∗y. This leads to the model

x = A(z.∗y) +  where  ∼ N (0  σ21)   z ∼ IBP(α) and yi ∼ p(y)

(3)
Again  the goal is to infer A  the probabilities πi and then to associate likely values of Zij and Yij
with the data. In particular  [24] make a number of alternative assumptions on p(y)  namely either
that it is iid Gaussian or that it is iid Laplacian. Note that the scale of y itself is not so important
since an equivalent model can always be found by rescaling A suitably.
Note that in (3) we used the shorthand .∗ to denote point-wise multiplication of two vectors in
’Matlab’ notation. While (2) and (3) appear rather similar  the latter model is considerably more
complex since it not only amounts to a sparse signal but also to an additional multiplicative scale.
[24] refer to the model as Inﬁnite Sparse Factor Analysis (isFA) or Inﬁnite Independent Component
Analysis (iICA) depending on the choice of p(y) respectively.

2

3 Spectral Characterization

We are now in a position to deﬁne the moments of the associated binary matrix. In our approach
we assume that Z ∼ IBP(α). We assume that the number of nonzero attributes k is unknown
(but ﬁxed). Our analysis begins by deriving moments for the IBP proper. Subsequently we apply
this to the two models described above. All proofs are deferred to the Appendix. For notational
convenience we denote by S the symmetrized version of a tensor where care is taken to ensure
that existing multiplicities are satisﬁed. That is  for a generic third order tensor we set S6[A]ijk =
Aijk + Akij + Ajki + Ajik + Akji + Aikj. However  if e.g. A = B ⊗ c with Bij = Bji  we only
need S3[A]ijk = Aijk + Akij + Ajki to obtain a symmetric tensor.

3.1 Tensorial Moments for the IBP

A degeneracy in the third order tensor requires that we compute a fourth order moment. We can
exclude the cases of πi = 0 and πi = 1 since the former amounts to a nonexistent feature and the
latter to a constant offset. We use Mi to denote moments of order i and Si to denote diagonal(izable)
tensors of order i. Finally  we use π ∈ RK+ to denote the vector of probabilities πi.
Order 1 This is straightforward  since we have

Order 2 The second order tensor is given by

M1 := Ez [z] = π =: S1.

M2 := Ez [z ⊗ z] = π ⊗ π + diag(cid:0)π − π2(cid:1) = S1 ⊗ S1 + diag(cid:0)π − π2(cid:1) .
S2 := M2 − S1 ⊗ S1 = diag(cid:0)π − π2(cid:1) .

Solving for the diagonal tensor we have

(6)
The degeneracies {0  1} of π − π2 = (1 − π)π can be ignored since they amount to non-existent
and degenerate probability distributions.

Order 3 The third order moments yield

M3 :=Ez [z ⊗ z ⊗ z] = π ⊗ π ⊗ π + S3

=S1 ⊗ S1 ⊗ S1 + S3 [S1 ⊗ S2] + diag(cid:0)π − 3π2 + 2π3(cid:1) .
S3 :=M3 − S3 [S1 ⊗ S2] − S1 ⊗ S1 ⊗ S1 = diag(cid:0)π − 3π2 + 2π3(cid:1) .

(cid:2)π ⊗ diag(cid:0)π − π2(cid:1)(cid:3) + diag(cid:0)π − 3π2 + 2π3(cid:1)

Note that the polynomial π − 3π2 + 2π3 = π(2π − 1)(π − 1) vanishes for π = 1
undesirable for the power method — we need to compute a fourth order tensor to exclude this.

2. This is

(4)

(5)

(7)
(8)
(9)

M4 :=Ez [z ⊗ z ⊗ z ⊗ z] = S1 ⊗ S1 ⊗ S1 ⊗ S1 + S6 [S2 ⊗ S1 ⊗ S1] + S3 [S2 × S2]

S4 :=M4 − S1 ⊗ S1 ⊗ S1 ⊗ S1 − S6 [S2 ⊗ S1 ⊗ S1] − S3 [S2 × S2] − S4 [S3 ⊗ S1]

Order 4 The fourth order moments are

+ S4 [S3 ⊗ S1] + diag(cid:0)π − 7π2 + 12π3 − 6π4(cid:1)
=diag(cid:0)π − 7π2 + 12π3 − 6π4(cid:1) .
The roots of the polynomial are(cid:8)0  1

√
2 + 1/
their corresponding πk can be inferred either by S3 or S4.

2 − 1/

12  1

√

12  1(cid:9). Hence the latent factors and

(10)

3.2 Application of the IBP

The above derivation showed that if we were able to access z directly  we could infer π from it
by reading off terms from a diagonal tensor. Unfortunately  this is not quite so easy in practice
since z generally acts as a latent attribute in a more complex model. In the following we show how
the models of (2) and (3) can be converted into spectral form. We need some notation to indicate
multiplications of a tensor M of order k by a set of matrices Ai.

[T (M  A1  . . .   Ak)]i1 ...ik

:=

Mj1 ...jk [A1]i1j1

· . . . · [Ak]ikjk

.

(11)

(cid:88)

j1 ...jk

3

Note that this includes matrix multiplication. For instance  A(cid:62)
1 M A2 = T (M  A1  A2). Also note
that in the special case where the matrices Ai are vectors  this amounts to a reduction to a scalar.
Any such reduced dimensions are assumed to be dropped implicitly. The latter will become useful
in the context of the tensor power method in [6].
Linear Gaussian Latent Factor Model. When dealing with (2) our goal is to infer both A and
π. The main difference is that rather than observing z we have Az  hence all tensors are colored.
Moreover  we also need to deal with the terms arising from the additive noise . This yields

S1 :=M1 = T (π  A)
S2 :=M2 − S1 ⊗ S1 − σ21 = T (diag(π − π2)  A  A)
S3 :=M3 − S1 ⊗ S1 ⊗ S1 − S3 [S1 ⊗ S2] − S3 [m1 ⊗ 1]

=T(cid:0)diag(cid:0)π − 3π2 + 2π3(cid:1)   A  A  A(cid:1)
=T(cid:0)diag(cid:0)−6π4 + 12π3 − 7π2 + π(cid:1)   A  A  A  A(cid:1)

− σ2S6 [S2 ⊗ 1] − m4S3 [1 ⊗ 1]

S4 :=M4 − S1 ⊗ S1 ⊗ S1 ⊗ S1 − S6 [S2 ⊗ S1 ⊗ S1] − S3 [S2 ⊗ S2] − S4 [S3 ⊗ S1]

(12)
(13)
(14)

(15)

Here we used the auxiliary statistics m1 and m4. Denote by v the eigenvector with the smallest
eigenvalue of the covariance matrix of x. Then the auxiliary variables are deﬁned as

(cid:104)
x(cid:104)v  (x − E [x])(cid:105)2(cid:105)
(cid:104)(cid:104)v  (x − Ex [x])(cid:105)4(cid:105)

/3

m1 :=Ex

m4 :=Ex

= σ2T (π  A)

= σ4.

(16)

(17)

These terms are used in a tensor power method to infer both A and π (Appendix A has a derivation).
Inﬁnite Sparse Factor Analysis. Using the model of (3) it follows that z is a symmetric distribution
with mean 0 provided that p(y) has this property. From that it follows that the ﬁrst and third order
moments and tensors vanish  i.e. S1 = 0 and S3 = 0. We have the following statistics:

S2 :=M2 − σ21 = T (c · diag(π)  A  A)
S4 :=M4 − S3 [S2 ⊗ S2] − σ2S6 [S2 ⊗ 1] − m4S3 [1 ⊗ 1] = T (diag(f (π))  A  A  A  A) .

(19)
Here m4 is deﬁned as in (17). Whenever p(y) in (3) is Gaussian  we have c = 1 and f (π) = π− π2.
Moreover  whenever p(y) follows the Laplace distribution  we have c = 2 and f (π) = 24π − 12π2.
Lemma 1 Any linear model of the form (2) or (3) with the property that  is symmetric and satisﬁes

(cid:3) the same properties for y  will yield the same moments.

(cid:3) and E[4] = E(cid:2)4

E[2] = E(cid:2)2

(18)

Gauss

Gauss

Proof This follows directly from the fact that z   and y are independent and that the latter two
have zero mean and are symmetric. Hence the expectations carry through regardless of the actual
underlying distribution.

4 Parameter Inference
Having derived symmetric tensors that contain both A and polynomials of π  we need to separate
those two factors and the additive noise  as appropriate. In a nutshell the approach is as follows: we
ﬁrst identify the noise ﬂoor using the assumption that the number of nonzero probabilities in π is
lower than the dimensionality of the data. Secondly  we use the noise-corrected second order tensor
to whiten the data. This is akin to methods used in ICA [12]. Finally  we perform power iterations
on the data to obtain S3 and S4  or rather  their applications to data. Note that the eigenvalues in the
re-scaled tensors differ slightly since we use S

† 1
2 x directly rather than x.

2

Robust Tensor Power Method Our reasoning follows that of [6].
It is our goal to obtain an
orthogonal decomposition of the tensors Si into an orthogonal matrix V together with a set of
corresponding eigenvalues λ such that Si = T [diag(λ)  V (cid:62)  . . .   V (cid:62)]. This is accomplished by
generalizing the Rayleigh quotient and power iterations as described in [6  Algorithm 1]:

θ ← T [S  1  θ  . . .   θ] and θ ← (cid:107)θ(cid:107)−1 θ.

(20)

4

Algorithm 1 Excess Correlation Analysis for Linear-Gaussian model with IBP prior
Inputs: the moments M1  M2  M3  M4.
1: Infer K and σ2:
2: Optionally ﬁnd a subspace R ∈ Rd×K(cid:48)

with K < K(cid:48) by random projection.

Range (R) = Range (M2 − M1 ⊗ M1) and project down to R

4: Set S2 =(cid:0)M2 − M1 ⊗ M1 − σ21(cid:1)

3: Set σ2 := λmin (M2 − M1 ⊗ M1)

 by truncating to eigenvalues larger than 

2   where [U  Σ] = svd(S2)

5: Set K = rank S2
6: Set W = U Σ− 1
7: Whitening: (best carried out by preprocessing x)
8: Set W3 := T (S3  W  W  W )
9: Set W4 := T (S4  W  W  W  W )
10: Tensor Power Method:
11: Compute generalized eigenvalues and vectors of W3.
12: Keep all K1 ≤ K (eigenvalue  eigenvector) pairs (λi  vi) of W3
13: Deﬂate W4 with (λi  vi) for all i ≤ K1
14: Keep all K − K1 (eigenvalue  eigenvector) pairs (λi  vi) of deﬂated W4
15: Reconstruction: With corresponding eigenvalues {λ1 ···   λK}  return the set A:

(cid:26) 1

Zi

(cid:0)W †(cid:1)(cid:62)

A =

(cid:27)

where Zi =(cid:112)πi − π2

i with πi = f−1(λi). f (π) = −2π+1

√

otherwise. (The proof of Equation (21) is provided in the Appendix.)

vi : vi ∈ Λ

(21)
π−π2 if i ∈ [K1] and f (π) = 6π2−6π+1
π−π2

In a nutshell  we use a suitable number of random initialization l  perform a few iterations (v)
and then proceed with the most promising candidate for another d iterations. The rationale for
picking the best among l candidates is that we need a high probability guarantee that the selected
initialization is non-degenerate. After ﬁnding a good candidate and normalizing its length we deﬂate
(i.e. subtract) the term from the tensor S.

Excess Correlation Analysis (ECA) The algorithm for recovering A is shown in Algorithm 1.
We ﬁrst present the method of inferring the number of latent features  K  which can be viewed as
the rank of the covariance matrix. An efﬁcient way of avoiding eigendecomposition on a d × d
matrix is to ﬁnd a low-rank approximation R ∈ Rd×K(cid:48)
such that K < K(cid:48) (cid:28) d and R spans the
same space as the covariance matrix. One efﬁcient way to ﬁnd such matrix is to set R to be

R = (M2 − M1 × M1) Θ 

(22)

where Θ ∈ Rd×K(cid:48)
is a random matrix with entries sampled independently from a standard normal.
This is described  e.g. by [20]. Since there is noise in the data  it is not possible that we get exactly K
non-zero eigenvalues with the remainder being constant at noise ﬂoor σ2. An alternative strategy to
thresholding by σ2 is to determine K by seeking the largest slope on the curve of sorted eigenvalues.
Next  we whiten the observations by multiplying data with W ∈ Rd×K. This is computationally
efﬁcient  since we can apply this directly to x  thus yielding third and fourth order tensors W3 and
W4 of size k. Moreover  approximately factorizing S2 is a consequence of the decomposition and
random projection techniques arising from [20].
To ﬁnd the singular vectors of W3 and W4 we use the robust tensor power method  as described
above. From the eigenvectors we found in the last step  A could be recovered with Equation 21. The
fact that this algorithm only needs projected tensors makes it very efﬁcient. Streaming variants of
the robust tensor power method are subject of future research.
Further Details on the projected tensor power method.
Explicitly calculating tensors
M2  M3  M4 is not practical in high dimensional data. It may not even be desirable to compute
the projected variants of M3 and M4  that is  W3 and W4 (after suitable shifts). Instead  we can use

5

the analog of a kernel trick to simplify the tensor power iterations to
W (cid:62)xi (cid:104)xi  W u(cid:105)l−1 =

W (cid:62)T (Ml  1  W u  . . .   W u) =

m(cid:88)

i=1

1
m

(cid:10)W (cid:62)xi  u(cid:11)l−1

xi

m(cid:88)

i=1

W (cid:62)
m

By using incomplete expansions memory complexity and storage are reduced to O(d) per term.
Moreover  precomputation is O(d2) and it can be accomplished in the ﬁrst pass through the data.

5 Concentration of Measure Bounds

There exist a number of concentration of measure inequalities for speciﬁc statistical models using
rather speciﬁc moments [8]. In the following we derive a general tool for bounding such quantities 
both for the case where the statistics are bounded and for unbounded quantities alike. Our analysis
borrows from [3] for the bounded case  and from the average-median theorem  see e.g. [2].

5.1 Bounded Moments
We begin with the analysis for bounded moments. Denote by φ : X → F a set of statistics on X
and let φl be the l-times tensorial moments obtained from l.

φ1(x) := φ(x); φ2(x) := φ(x) ⊗ φ(x); φl(x) := φ(x) ⊗ . . . ⊗ φ(x)

(23)

In this case we can deﬁne inner products via

kl(x  x(cid:48)) := (cid:104)φl(x)  φl(x(cid:48))(cid:105) = T [φl(x)  φ(x(cid:48))  . . .   φ(x(cid:48))] = (cid:104)φ(x)  φ(x(cid:48))(cid:105)l = kl(x  x(cid:48))

as reductions of the statistics of order l for a kernel k(x  x(cid:48)) := (cid:104)φ(x)  φ(x(cid:48))(cid:105). Finally  denote by

m(cid:88)

j=1

Ml := Ex∼p(x)[φl(x)] and ˆMl :=

1
m

φl(xj)

(24)

the expectation and empirical averages of φl. Note that these terms are identical to the statistics
used in [16] whenever a polynomial kernel is used. It is therefore not surprising that an analogous
concentration of measure inequality to the one proven by [3] holds:
Theorem 2 Assume that the sufﬁcient statistics are bounded via (cid:107)φ(x)(cid:107) ≤ R for all x ∈ X . With
probability at most 1 − δ the following guarantee holds:

(cid:40)

Pr

sup

u:(cid:107)u(cid:107)≤1

(cid:12)(cid:12)(cid:12)T (Ml  u ···   u) − T ( ˆMl  u ···   u)

(cid:12)(cid:12)(cid:12) > l

(cid:41)

(cid:2)2 +

√−2 log δ(cid:3) Rl

√

m

.

≤ δ where l ≤

Using Lemma 1 this means that we have concentration of measure immediately for the moments
S1  . . . S4.Details are provided in the appendix. In particular  we need a chaining result (Lemma 4)
that allows us to compute bounds for products of terms efﬁciently. By utilizing an approach similar
to [8]  overall guarantees for reconstruction accuracy can be derived.

5.2 Unbounded Moments

We are interested in proving concentration of the following four tensors in (13)  (14)  (15) and one
scalar in (27). Whenever the statistics are unbounded  concentration of moment bounds are less
trivial and require the use of subgaussian and gaussian inequalities [22]. We derive a bound for
fourth-order subgaussian random variables (previous work only derived up to third order bounds).
Lemma 5 and 6 has details on how to obtain such guarantees. We further get the bounds for the ten-
sors based on the concentration of moment in Lemma 7 and 8. Bounds for reconstruction accuracy
of our algorithm are provided. The full proof is in the Appendix.
Theorem 3 (Reconstruction Accuracy) Let ςk [S2] be the k−th largest singular value of S2. Deﬁne

πmin = argmaxi∈[K] |πi − 0.5|  πmax = argmaxi∈[K] πi and ˜π =(cid:81){i:πi≤0.5} πi

(cid:81){i:πi>0.5}(1−

6

(cid:32)

m ≥ poly

d  K 

  log(1/δ) 

1


K(cid:80)

(cid:107)Ai(cid:107)2

2

1
˜π

 

ς1 [S2]
ςK [S2]

 

i=1
ςK [S2]

 

√

 

1

πmin − πmin2

 

(cid:112)πmax − π2

πmax

max

σ2

ςK [S2]

(cid:33)

πi). Pick any δ   ∈ (0  1). There exists a polynomial poly(·) such that if sample size m statisﬁes

with probability greater than 1 − δ  there is a permutation τ on [K] such that the ˆA returns by
Algorithm 1 satiﬁes

 for all i ∈ [K].

(cid:13)(cid:13)(cid:13) ˆAτ (i) − Ai

(cid:13)(cid:13)(cid:13) ≤(cid:16)(cid:107)Ai(cid:107)2 +(cid:112)ς1 [S2]

(cid:17)

6 Experiments
We evaluate the algorithm on a number of problems suitable for the two models of (2) and (3). The
problems are largely identical to those put forward in [18] in order to keep our results comparable
with a more traditional inference approach. We demonstrate that our algorithm is faster  simpler 
and achieves comparable or superior accuracy.

Synthetic data Our goal is to demonstrate the ability to recover latent structure of generated data.
Following [18] we generate images via linear noisy combinations of 6 × 6 templates. That is  we
use the binary additive model of (2). The goal is to recover both the above images and to assess their
respective presence in observed data. Using an additive noise variance of σ2 = 0.5 we are able to
recover the original signal quite accurately (from left to right: true signal  signal inferred from 100
samples  signal inferred from 500 samples). Furthermore  as the second row indicates  our algorithm
also correctly infers the attributes present in the images.

For a more quantitative evaluation we compared our results to the inﬁnite variational algorithm
of [14]. The data is generated using σ ∈ {0.1  0.2  0.3  0.4  0.5} and with sample size n ∈
{100  200  300  400  500}. Figure 1 shows that our algorithm is faster and comparatively accurate.

Figure 1: Comparison to inﬁnite variational approach. The ﬁrst plot compares the test negative log
likelihood training on N = 500 samples with different σ. The second plot shows the CPU time to
data size  N  between the two methods.

Image Source Recovery We repeated the same test using 100 photos from [18]. We ﬁrst reduce
dimensionality on the data set by representing the images with 100 principal components and apply
our algorithm on the 100-dimensional dataset (see Algorithm 1 for details). Figure 2 shows the
result. We used 10 initial iterations 50 random seeds and 30 ﬁnal iterations 50 in the Robust Power
Tensor Method. The total runtime was 0.2788s.

7

0 1 0 01 1 0 00 1 0 11 0 0 10 1 0 01 1 0 00 1 0 11 0 0 1Text1 0 1 00 1 1 00.10.20.30.40.50.6010002000300040005000600070008000negative log likelihood to σσnegative loglikelihood Infinite Variational ApproachSpectral Method on IBP2004006008001000050100150200250300CPU time to NNCPU time(sec) Infinite Variational ApproachSpectral Method on IBPFigure 2: Results of modeling 100 images from
[18] of size 240 × 320 by model (2). Row
1:
four sample images containing up to four
objects ($20 bill  Klein bottle  prehistoric han-
daxe  cellular phone). An object basically ap-
pears in the same location  but some small vari-
ation noise is generated because the items are
put into scene by hand; Row 2: Independent
attributes  as determined by inﬁnite variational
inference of [14] (note  the results in [18] are
black and white only); Row 3: Independent at-
tributes  as determined by spectral IBP; Row 4:
Reconstruction of the images via spectral IBP.
The binary superscripts indicate the items iden-
tiﬁed in the image.

Figure 3: Recovery of the source matrix A in
model (3) when comparing MCMC sampling
and spectral methods. MCMC sampling re-
quired 1.72 seconds and yielded a Frobenius
distance (cid:107)A − AMCM(cid:107)F = 0.77. Our spec-
tral algorithm required 0.77 seconds to achieve
a distance (cid:107)A − ASpectral(cid:107)F = 0.31.

derived

spectral

Figure 4: Gene sig-
by
natures
the
IBP.
They show that there
are common hidden
causes in the observed
expression
levels 
thus offering a con-
siderably
simpliﬁed
representation.

Gene Expression Data As a ﬁrst sanity check of the feasibility of our model for (3)  we generated
synthetic data using x ∈ R7 with k = 4 sources and n = 500 samples  as shown in Figure 3.
For a more realistic analysis we used a microarray dataset. The data consisted of 587 mouse liver
samples detecting 8565 gene probes  available as dataset GSE2187 as part of NCBI’s Gene Ex-
pression Omnibus www.ncbi.nlm.nih.gov/geo. There are four main types of treatments 
including Toxicant  Statin  Fibrate and Azole. Figure 4 shows the inferred latent factors arising from
expression levels of samples on 10 derived gene signatures. According to the result  the group of
ﬁbrate-induced samples and a small group of toxicant-induced samples can be classiﬁed accurately
by the special patterns. Azole-induced samples have strong positive signals on gene signatures 4
and 8  while statin-induced samples have strong positive signals only on the 9 gene signatures.
Summary
In this paper we introduced a spectral approach to inferring latent parameters in the
Indian Buffet Process. We derived tensorial moments for a number of models  provided an efﬁcient
inference algorithm  concentration of measure theorems and reconstruction guarantees. All this is
backed up by experiments comparing spectral and MCMC methods.
We believe that this is a ﬁrst step towards expanding spectral nonparametric tools beyond the
more common Dirichlet Process representations. Applications to more sophisticated models  larger
datasets and efﬁcient implementations are subject for future work.

8

Original GSpectral isFAMCMCReferences
[1] R. Adams  Z. Ghahramani  and M. Jordan. Tree-structured stick breaking for hierarchical data. In Neural

Information Processing Systems  pages 19–27  2010.

[2] N. Alon  Y. Matias  and M. Szegedy. The space complexity of approximating the frequency moments.

Journal of Computers and System Sciences  58(1):137–147  1999.

[3] Y. Altun and A. J. Smola. Unifying divergence minimization and statistical inference via convex duality.
In H.U. Simon and G. Lugosi  editors  Proc. Annual Conf. Computational Learning Theory  LNCS  pages
139–153. Springer  2006.

[4] M. Aly  A. Hatch  V. Josifovski  and V.K. Narayanan. Web-scale user modeling for targeting. In Confer-

ence on World Wide Web  pages 3–12. ACM  2012.

[5] A. Anandkumar  K. Chaudhuri  D. Hsu  S. Kakade  L. Song  and T. Zhang. Spectral methods for learning

multivariate latent tree structure. In Neural Information Processing Systems  2011.

[6] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and M. Telgarsky. Tensor decompositions for learning

latent variable models. arXiv preprint arXiv:1210.7559  2012.

[7] Anima Anandkumar  Rong Ge  Daniel Hsu  and Sham M Kakade. A tensor spectral approach to learning

mixed membership community models. In Proc. Annual Conf. Computational Learning Theory  2013.

[8] Animashree Anandkumar  Dean P. Foster  Daniel Hsu  Sham M. Kakade  and Yi-Kai Liu. Two svds
sufﬁce: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. CoRR 
abs/1204.6703  2012.

[9] D. Blei and M. Jordan. Variational inference for dirichlet process mixtures. In Bayesian Analysis  vol-

[10] D. Blei  A. Ng  and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research 

ume 1  pages 121–144  2005.

3:993–1022  January 2003.

[11] Byron Boots  Arthur Gretton  and Geoffrey J Gordon. Hilbert space embeddings of predictive state

representations. In Conference on Uncertainty in Artiﬁcial Intelligence  2013.

[12] J.-F. Cardoso. Blind signal separation: statistical principles. Proceedings of the IEEE  90(8):2009–2026 

[13] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via the EM

algorithm. Journal of the Royal Statistical Society B  39(1):1–22  1977.

[14] F. Doshi  K. Miller  J. Van Gael  and Y. W. Teh. Variational inference for the indian buffet process. Journal

of Machine Learning Research - Proceedings Track  5:137–144  2009.

[15] E. B. Fox  E. B. Sudderth  M. I. Jordan  and A. S. Willsky. Sharing features among dynamical systems

[16] A. Gretton  K. Borgwardt  M. Rasch  B. Schoelkopf  and A. Smola. A kernel two-sample test. JMLR 

with beta processes. nips  22  2010.

13:723–773  2012.

[17] T. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the indian buffet process. Advances in

Neural Information Processing Systems 18  pages 475–482  2006.

[18] T. Grifﬁths and Z. Ghahramani. The indian buffet process: An introduction and review. Journal of

Machine Learning Research  12:11851224  2011.

[19] T.L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proceedings of the National Academy of Sci-

ences  101:5228–5235  2004.

[20] N. Halko  P.G. Martinsson  and J. A. Tropp. Finding structure with randomness: Stochastic algorithms

for constructing approximate matrix decompositions  2009. oai:arXiv.org:0909.4061.

[21] D. Hsu  S. Kakade  and T. Zhang. A spectral algorithm for learning hidden markov models. In Proc.

Annual Conf. Computational Learning Theory  2009.

[22] D. Hsu  S. Kakade  and T. Zhang. Tail inequalities for sums of random matrices that depend on the

intrinsic dimension. Electron. Commun. Probab.  17:13  2012.

[23] D. Hsu and S.M. Kakade. Learning mixtures of spherical gaussians: moment methods and spectral

decompositions. 2012.

[24] D. Knowles and Z. Ghahramani.

Inﬁnite sparse factor analysis and inﬁnite independent components
analysis. In International Conference on Independent Component Analysis and Signal Separation  2007.
In Survey in Combinatorics  pages 148–188.

[25] C. McDiarmid. On the method of bounded differences.

[26] K.T. Miller  T.L. Grifﬁths  and M.I. Jordan. Latent feature models for link prediction. In Snowbird  page

1998.

Cambridge University Press  1989.

2 pages  2009.

Royal Society  pages 71–71  1894.

Cambridge  1989.

[27] K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the

[28] G. Pisier. The Volume of Convex Bodies and Banach Space Geometry. Cambridge University Press 

[29] L. Song  B. Boots  S. Siddiqi  G. Gordon  and A. J. Smola. Hilbert space embeddings of hidden markov

models. In International Conference on Machine Learning  2010.

[30] F. Wood  T. L. Grifths  and Z. Ghahramani. A non-parametric bayesian method for inferring hidden

causes. uai  2006.

9

,Aijun Bai
Feng Wu
Xiaoping Chen
Hsiao-Yu Tung
Alexander Smola
Jaya Kawale
Hung Bui
Branislav Kveton
Long Tran-Thanh
Sanjay Chawla
Stephan Rabanser
Stephan Günnemann
Zachary Lipton