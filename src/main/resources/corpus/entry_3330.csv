2013,Manifold-based Similarity Adaptation for Label Propagation,Label propagation is one of the state-of-the-art methods for semi-supervised learning  which estimates labels by propagating label information through a graph. Label propagation assumes that data points (nodes) connected in a graph should have similar labels. Consequently  the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair. We propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function. In this approach  edge weights represent both similarity and local reconstruction weight simultaneously  both being reasonable for label propagation. For further justification  we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space  and an error analysis based on a low dimensional manifold model. Experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets.,Manifold-based Similarity Adaptation

for Label Propagation

Masayuki Karasuyama and Hiroshi Mamitsuka

Bioionformatics Center  Institute for Chemical Research  Kyoto University  Japan

{karasuyama mami}@kuicr.kyoto-u.ac.jp

Abstract

Label propagation is one of the state-of-the-art methods for semi-supervised learn-
ing  which estimates labels by propagating label information through a graph.
Label propagation assumes that data points (nodes) connected in a graph should
have similar labels. Consequently  the label estimation heavily depends on edge
weights in a graph which represent similarity of each node pair. We propose a
method for a graph to capture the manifold structure of input features using edge
weights parameterized by a similarity function. In this approach  edge weights
represent both similarity and local reconstruction weight simultaneously  both be-
ing reasonable for label propagation. For further justiﬁcation  we provide analyt-
ical considerations including an interpretation as a cross-validation of a propaga-
tion model in the feature space  and an error analysis based on a low dimensional
manifold model. Experimental results demonstrated the effectiveness of our ap-
proach both in synthetic and real datasets.

1

Introduction

Graph-based learning algorithms have received considerable attention in machine learning commu-
nity. For example  label propagation (e.g.  [1  2]) is widely accepted as a state-of-the-art approach
for semi-supervised learning  in which node labels are estimated through the input graph structure.
A common important property of these graph-based approaches is that the manifold structure of the
input data can be captured by the graph. Their practical performance advantage has been demon-
strated in various application areas.

On the other hand  it is well-known that the accuracy of the graph-based methods highly depends on
the quality of the input graph (e.g.  [1  3–5])  which is typically generated from a set of numerical
input vectors (i.e.  feature vectors). A general framework of graph-based learning can be represented
as the following three-step procedure:
Step 1: Generating graph edges from given data  where nodes of the generated graph correspond to

the instances of input data.

Step 2: Giving weights to the graph edges.
Step 3: Estimating node labels based on the generated graph  which is often represented as an

adjacency matrix.

In this paper  we focus on the second step in the three-step procedure; estimating edge weights for
the subsequent label estimation. Optimizing edge weights is difﬁcult in semi-supervised learning 
because there are only a small number of labeled instances. Also this problem is important because
edge weights heavily affect ﬁnal prediction accuracy of graph-based methods  while in reality rather
simple heuristics strategies have been employed.

There are two standard approaches for estimating edge weights: similarity function based- and
locally linear embedding (LLE) [6] based-approaches. Each of these two approaches has its own

1

disadvantage. The similarity based approaches use similarity functions  such as Gaussian kernel 
while most similarity functions have tuning parameters (such as the width parameter of Gaussian
kernel) that are in general difﬁcult to be tuned. On the other hand  in LLE  the true underlying
manifold can be approximated by a graph which minimizes a local reconstruction error. LLE is
more sophisticated than the similarity-based approach  and LLE based graphs have been applied to
semi-supervised learning [5  7–9]. However LLE is noise-sensitive [10]. In addition  to avoid a kind
of degeneracy problem [11]  LLE has to have additional tuning parameters.

Our approach is a similarity-based method  yet also captures the manifold structure of the input data;
we refer to our approach as adaptive edge weighting (AEW). In AEW  graph edges are determined
by a data adaptive manner in terms of both similarity and manifold structure. The objective function
in AEW is based on local reconstruction  by which estimated weights capture the manifold structure 
where each edge is parameterized as a similarity function of each node pair. Consequently  in spite
of its simplicity  AEW has the following three advantages:

• Compared to LLE based approaches  our formulation alleviates the problem of over-ﬁtting
due to the parameterization of weights. In our experiments  we observed that AEW is robust
against noise of input data using synthetic data set  and we also show the performance
advantage of AEW in eight real-world datasets.

• Similarity based representation of edge weights is reasonable for label propagation because
transitions of labels are determined by those weights  and edge weights obtained by LLE
approaches may not represent node similarity.

• AEW does not have additional tuning parameters such as regularization parameters. Al-
though the number of edges in a graph cannot be determined by AEW  we show that per-
formance of AEW is robust against the number of edges compared to standard heuristics
and a LLE based approach.

We provide further justiﬁcations for our approach based on the ideas of feature propagation and local
linear approximation. Our objective function can be seen as a cross validation error of a propagation
model for feature vectors  which we call feature propagation. This allows us to interpret that AEW
optimizes graph weights through cross validation (for prediction) in the feature vector space instead
of label space  assuming that input feature vectors and given labels share the same local structure.
Another interpretation is provided through local linear approximation  by which we can analyze the
error of local reconstruction in the output (label) space under the assumption of low dimensional
manifold model.

2 Graph-based Semi-supervised Learning

In this paper we use label propagation  which is one of the state-of-the-art graph-based learning
algorithms  as the methods in the third step in the three-step procedure. Suppose that we have n
feature vectors X = {x1  . . .   xn}  where xi ∈ Rp. An undirected graph G is generated from X  
where each node (or vertex) corresponds to each data point xi. The graph G can be represented by
the adjacency matrix W ∈ Rn×n where (i  j)-element Wij is a weight of the edge between xi and
xj. The key idea of graph-based algorithms is so-called manifold assumption  in which instances
connected by large weights Wij on a graph have similar labels (meaning that labels smoothly change
on the graph).

For the adjacency matrix Wij   the following weighted k-nearest neighbor (k-NN) graph is com-
monly used in graph-based learning algorithms [1]:

Wij =( exp(cid:16)−Pp

0 

where xid is the d-th element of xi  Ni is a set of indices of the k-NN of xi  and {σd}p
d=1 is a set of
parameters. [1] shows this weighting can also be interpreted as the solution of the heat equation on
the graph.

d=1

(xid−xjd)2

σ2
d

(cid:17)  

j ∈ Ni or i ∈ Nj 
otherwise 

(1)

From this adjacency matrix  the graph Laplacian can be deﬁned by

L = D − W  

2

where D is a diagonal matrix with the diagonal entry Dii = Pj Wij . Instead of L  normalized

variants of Laplacian such as L = I − D−1W or L = I − D−1/2W D−1/2 is also used  where
I ∈ Rn×n is the identity matrix.

Among several label propagation algorithms  we mainly use the formulation by [1]  which is the
standard formulation of graph-based semi-supervised learning. Suppose that the ﬁrst ℓ data points
in X are labeled by Y = {y1  . . .   yℓ}  where yi ∈ {1  . . .   c} and c is the number of classes. The
goal of label propagation is to predict the labels of unlabeled nodes {xℓ+1  . . .   xn}. The scoring
matrix F gives an estimation of the label of xi by argmaxj Fij . Label propagation can be deﬁned
as estimating F in such a way that score F smoothly changes on a given graph as well as it can
predict given labeled points. The following is standard formulation  which is called the harmonic
Gaussian ﬁeld (HGF) model  of label propagation [1]:

min

F

trace(cid:16)F ⊤LF(cid:17) subject to Fij = Yij  for i = 1  . . .   ℓ.

where Yij is the label matrix with Yij = 1 if xi is labeled as yi = j; otherwise  Yij = 0  In this
formulation  the scores for labeled nodes are ﬁxed as constants. This formulation can be reduced
to linear systems  which can be solved efﬁciently  especially when Laplacian L has some sparse
structure.

3 Basic Framework of Proposed Approach

The performance of label propagation heavily depends on quality of an input graph. Our proposed
approach  adaptive edge weighting (AEW)  optimizes edge weights for the graph-based learning
algorithms. We note that AEW is for the second step of the three step procedure and has nothing
to do with the ﬁrst and third steps  meaning that any methods in the ﬁrst and third steps can be
combined with AEW. In this paper we consider that the input graph is generated by k-NN graph (the
ﬁrst step is based on k-NN)  while we note that AEW can be applied to any types of graphs.

First of all  graph edges should satisfy the following conditions:

• Capturing the manifold structure of the input space.
• Representing similarity between two nodes.

These two conditions are closely related to manifold assumption of graph-based learning algorithms 
in which labels vary smoothly along the input manifold. Since the manifold structure of the input
data is unknown beforehand  the graph is used to approximate the manifold (the ﬁrst condition).
Subsequent predictions are performed in such a way that labels smoothly change according to the
similarity structure provided by the graph (the second condition). Our algorithm simultaneously
pursues these two important aspects of the graph for the graph-based learning algorithms.

We deﬁne Wij as a similarity function of two nodes (1)  using Gaussian kernel in this paper (Note:
other similarity functions can also be used). We estimate σd so that the graph represents manifold
structure of the input data by the following optimization problem:

min
{σd}p

d=1

n

Xi=1

kxi −

1

Dii Xj∼i

Wij xjk2
2 

(2)

where j ∼ i means that j is connected to i.
This minimizes the reconstruction error by local
linear approximation  which captures the input manifold structure  in terms of the parameters of
the similarity function. We will describe the motivation and analytical properties of the objective
function in Section 4. We further describe advantages of this function over existing approaches
including well-known locally linear embedding (LLE) [6] based methods in Section 5  respectively.

To optimize (2)  we can use any gradient-based algorithm such as steepest descent and conjugate
gradient (in the later experiments  we used steepest descent method). Due to the non-convexity
of the objective function  we cannot guarantee that solutions converge to the global optimal which
means that the solutions depend on the initial σd. In our experiments  we employed well-known
median heuristics (e.g.  [12]) for setting initial values of σd (Section 6). Another possible strategy
is to use a number of different initial values for σd  which needs a high computational cost. The

3

gradient can be computed efﬁciently  due to the sparsity of the adjacency matrix. Since the number
of edges of a k-NN graph is O(nk)  the derivative of adjacency matrix W can be calculated by
O(nkp). Then the entire derivative of the objective function can be calculated by O(nkp2). Note
that k often takes a small value such as k = 10.

4 Analytical Considerations

In Section 3  we deﬁned our approach as the minimization of the local reconstruction error of input
features. We describe several interesting properties and interpretations of this deﬁnition.

4.1 Derivation from Feature Propagation

First  we show that our objective function can be interpreted as a cross-validation error of the HGF
model for the feature vector x on the graph. Let us divide a set of node indices {1  . . .   n} into a
training set T and a validation set V. Suppose that we try to predict x in the validation set {xi}i∈V
from the given training set {xi}i∈T and the adjacency matrix W . For this prediction problem  we
consider the HGF model for x:

min
ˆX

L ˆX(cid:17) subject to ˆxij = xij  for i ∈ T  

⊤

trace(cid:16) ˆX

where X = (x1  x2  . . . xn)⊤  ˆX = (ˆx1  ˆx2  . . . ˆxn)⊤  and xij and ˆxij indicate (i  j)-th entries of
X and ˆX respectively. In this formulation  ˆxi corresponds to a prediction for xi. Note that only
ˆxi in the validation set V is regarded as free variables in the optimization problem because the other
{ˆxi}i∈T is ﬁxed at the observed values by the constraint. This can be interpreted as propagating
{xi}i∈T to predict {xi}i∈V . We call this process as feature propagation.

When we employ leave-one-out as the cross-validation of the feature propagation model  we obtain

kxi − ˆx−ik2
2 

n

Xi=1

(3)

where ˆx−i is a prediction for xi with T = {1  . . .   i − 1  i + 1  . . .   n} and V = {i}. Due to the

local averaging property of HGF [1]  we see ˆx−i =Pj Wij xj/Dii  and then (3) is equivalent to our

objective function (2). From this equivalence  AEW can be interpreted as the parameter optimization
in graph weights of the HGF model for feature vectors through the leave-one-out cross-validation.
This also means that our framework estimates labels using the adjacency matrix W optimized in the
feature space instead of the output (label) space. Thus  if input features and labels share the same
adjacency matrix (i.e.  sharing the same local structure)  the minimization of the objective function
(2) should estimate the adjacency matrix which accurately propagates the labels of graph nodes.

4.2 Local Linear Approximation

The feature propagation model provides the interpretation of our approach as the optimization of the
adjacency matrix under the assumption that x and y can be reconstructed by the same adjacency ma-
trix. We here justify this assumption in a more formal way from a viewpoint of local reconstruction
with a lower dimensional manifold model.

As shown in [1]  HGF can be regarded as local reconstruction methods  which means the prediction
can be represented as weighted local averages:

Fik = Pj WijFjk

Dii

for i = ℓ + 1  . . .   n.

We show the relationship between the local reconstruction error in the feature space described by
our objective function (2) and the output space. For simplicity we consider the vector form of the
score function f ∈ Rn which can be considered as a special case of the score matrix F   and
discussions here can be applied to F . The same analysis can be approximately applied to other
graph learning methods such as local global consistency [2] because it has similar local averaging
form as the above  though we omitted here.

4

We assume the following manifold model for the input feature space  in which x is generated from
corresponding some lower dimensional variable τ ∈ Rq: x = g(τ ) + εx  where g : Rq → Rp
is a smooth function  and εx ∈ Rp represents noise. In this model  y is also represented by some
function form of τ : y = h(τ ) + εy  where h : Rq → R is a smooth function  and εy ∈ R represents
noise (for simplicity  we consider a continuous output rather than discrete labels). For this model 
the following theorem shows the relationship between the reconstruction error of the feature vector
x and the output y:
Theorem 1. Suppose xi can be approximated by its neighbors as follows

xi =

Wij xj + ei 

(4)

1

Dii Xj∼i

where ei ∈ Rp represents an approximation error. Then  the same adjacency matrix reconstructs
the output yi ∈ R with the following error:

1

Dii Xj∼i

where J = ∂h(τ i)
τ jk2

∂τ ⊤ (cid:17)+
∂τ ⊤ (cid:16) ∂g(τ i)

2).

yi −

Wijyj = J ei + O(δτ i) + O(εx + εy) 

(5)

with superscript + indicates pseudoinverse  and δτ i = maxj(kτ i −

See our supplementary material for the proof of this theorem. From (5)  we can see that the recon-
struction error of yi consists of three terms. The ﬁrst term includes the reconstruction error for xi
which is represented by ei  and the second term is the distance between τ i and {τ j}j∼i. These two
terms have a kind of trade-off relationship because we can reduce ei if we use a lot of data points
xj  but then δτ i would increase. The third term is the intrinsic noise which we cannot directly
control. In spite of its importance  this simple relationship has not been focused on in the context
of graph estimation for semi-supervised learning  in which a LLE based objective function has been
used without clear justiﬁcation [5  7–9].

A simple approach to exploit this theorem would be a regularization formulation  which can be a
minimization of a combination of the reconstruction error for x and a penalization term for distances
between data points connected by edges. Regularized LLE [5  8  13  14] can be interpreted as one
realization of such an approach. However  in semi-supervised learning  selecting appropriate values
of the regularization parameter is difﬁcult. We therefore optimize edge weights through parameters
of the similarity function  especially the bandwidth parameter of Gaussian similarity function σ. In
this approach  a very large bandwidth (giving large weights to distant data points) may cause a large
reconstruction error  while an extremely small bandwidth causes the problem of not giving enough
weights to reconstruct.

For symmetric normalized graph Laplacian  we can not apply Theorem 1 to our algorithm. See
supplementary material for modiﬁed version of Theorem 1 for normalized Laplacian. In the exper-
iments  we also report results for normalized Laplacian and show that our approach can improve
prediction accuracy as in the case of unnormalized Laplacian.

5 Related Topics

LLE [6] can also estimate graph edges based on a similar objective function  in which W is directly
optimized as a real valued matrix. This manner has been used in many methods for graph-based
semi-supervised learning and clustering [5  7–9]  but LLE is very noise-sensitive [10]  and resulting
weights Wij cannot necessarily represent similarity between the corresponding nodes (i  j). For
example  for two nearly identical points xj1 and xj2   both connecting to xi  it is not guaranteed
that Wij1 and Wij2 have similar values. To solve this problem  a regularization term can be in-
troduced [11]  while it is not easy to optimize the regularization parameter for this term. On the
other hand  we optimize parameters of the similarity (kernel) function. This parameterized form of
edge weights can alleviate the over-ﬁtting problem. Moreover  obviously  the optimized weights still
represent the node similarity.

Although several model selection approaches (such as cross-validation and marginal likelihood max-
imization) have been applied to optimizing graph edge weights by regarding them as usual hyper-

5

parameters in supervised learning [3  4  15]  most of them need labeled instances and become un-
reliable under the cases with few labels. Another approach is optimizing some criterion designed
speciﬁcally for graph-based algorithms (e.g.  [1  16]). These criteria often have degenerate (trivial)
solutions for which heuristics are used to prevent  but the validity of those heuristics is not clear.
Compared to these approaches  our approach is more general and ﬂexible for problem settings  be-
cause AEW is independent of the number of classes  the number of labels  and subsequent label
estimation algorithms.
In addition  model selection based approaches are basically for the third
step in the three-step procedure  by which AEW can be combined with such methods  like that the
optimized graph by AEW can be used as the input graph of these methods.

Besides k-NN  there have been several methods generating a graph (edges) from the feature vectors
(e.g.  [9  17]). Our approach can also be applied to those graphs because AEW only optimizes
weights of edges. In our experiments  we used the edges of the k-NN graph as the initial graph of
AEW. We then observed that AEW is not sensitive to the choice of k  comparing with usual k-NN
graphs. This is because the Gaussian similarity value becomes small if xi and xj are not close
to each other to minimize the reconstruction error (2). In other words  redundant weights can be
reduced drastically  because in the Gaussian kernel  weights decay exponentially according to the
squared distance.

Metric learning is another approach to adapting similarity  while metric learning is not for graphs.
A standard method for incorporating graph information into metric learning is to use some graph-
based regularization  in which graph weights must be determined beforehand. For example  in [18] 
a graph is generated by LLE  of which we already described the disadvantages. Another approach
is [19]  which estimates a distance metric so that the k-NN graph in terms of the obtained metric
should reproduce a given graph. This approach is however not for semi-supervised learning  and it
is unclear if this approach works for semi-supervised settings. Overall metric learning is developed
from a different context from our setting  by which it has not been justiﬁed that metric learning can
be applied to label propagation.

6 Experiments

We evaluated the performance of our approach using synthetic and real-world datasets. We investi-
gated the performance of AEW using the harmonic Gaussian ﬁeld (HGF) model. For comparison 
we used linear neighborhood propagation (LNP) [5]  which generates a graph using a LLE based
objective function. LNP can have two regularization parameters  one of which is for the LLE pro-
cess (the ﬁrst and second steps in the three-step procedure)  and the other is for the label estimation
process (the third step in the three-step procedure). For the parameter in the LLE process  we used
the heuristics suggested by [11]  and for the label propagation process  we chose the best parameter
value in terms of the test accuracy. HGF does not have such hyper-parameters. All results were
averaged over 30 runs with randomly sampled data points.

6.1 Synthetic datasets

We here use two datasets in Figure 1 having the same form  but Figure 1 (b) has several noisy data
points which may become bridge points (points connecting different classes [5]). In both cases  the
number of classes is 4 and each class has 100 data points (thus  n = 400).

Table 1 shows the error rates for the unlabeled nodes of HGF and LNP under 0-1 loss. For HGF 
we used the median heuristics to choose the parameter σd in similarity function (1)  meaning that a
common σ (= σ1 = . . . = σp) is set as the median distance between all connected pairs of xi.The
symmetric normalized version of graph Laplacian was used. The optimization of AEW started from
the median σd. The results by AEW are shown in the column ‘AEW + HGF’ of Table 1. The number
of labeled nodes was 10 in each class (ℓ = 40  i.e.  10% of the entire datasets)  and the number of
neighbors in the graphs was set as k = 10 or 20.

In Table 1  we see HGF with AEW achieved better prediction accuracies than the median heuristics
and LNP in all cases. Moreover  for both of datasets (a) and (b)  AEW was most robust against
the change of the number of neighbors k. This is because σd is automatically adjusted in such
a way that the local reconstruction error is minimized and then weights for connections between

6

1

0.5

0

−0.5

−1

−1

1

0.5

0

−0.5

−1

−1

−0.5

0

0.5

1

−0.5

0

0.5

1

(a)

(b)

Figure 1: Synthetic datasets.

Table 1: Test error comparison for synthetic
datasets. The best methods according to t-test
with the signiﬁcant level of 5% are highlighted
with boldface.

data
(a)
(a)
(b)
(b)

k
10
20
10
20

HGF

AEW + HGF

LNP

.057 (.039)
.261 (.048)
.119 (.054)
.280 (.051)

.020 (.027)
.020 (.028)
.073 (.035)
.077 (.035)

.039 (.026)
.103 (.042)
.103 (.038)
.148 (.047)

400

350

300

250

200

150

100

50

400

350

300

250

200

150

100

50

400

350

300

250

200

150

100

50

50

100 150 200 250 300 350 400

50

100 150 200 250 300 350 400

50

100 150 200 250 300 350 400

(a) k-NN

(b) AEW

(c) LNP

Figure 2: Resulting graphs for the synthetic dataset of Figure 1 (a) (k = 20).

different manifolds are reduced. Although LNP also minimizes the local reconstruction error  LNP
may connect data points far from each other if it reduces the reconstruction error.

Figure 2 shows the graphs generated by (a) k-NN  (b) AEW  and (c) LNP  under k = 20 for the
dataset of Figure 1 (a). In Figure 2  the k-NN graph connects a lot of nodes in different classes 
while AEW favorably eliminates those undesirable edges. LNP also has less edges between different
classes compared to k-NN  but it still connects different classes. AEW reveals the class structure
more clearly  which can lead the better prediction performance of subsequent learning algorithms.

Table 2: List of datasets.

COIL
USPS
MNIST
ORL
Vowel
Yale
optdigit
UMIST

n
500
1000
1000
360
792
250
1000
518

p
256
256
784
644
10
1200
256
644

# classes
10
10
10
40
11
5
10
20

6.2 Real-world datasets

We examined the performance of our approach on the
eight popular datasets shown in Table 2  namely COIL
(COIL-20) [20]  USPS (a preprocessed version from
[21])  MNIST [22]  ORL [23]  Vowel [24]  Yale (Yale
Face Database B) [25]  optdigit [24]  and UMIST [26].

We evaluated two variants of the HGF model.
In
what follows  ‘HGF’ indicates HGF using unnormalized
graph Laplacian L = D − W   and ‘N-HGF’ indi-
cates HGF using symmetric normalized Laplacian L =
I − D−1/2W D−1/2. For both of two variants  the me-
dian heuristics was used to set σd. To adapt the difference of local scale  we here use local scaling
kernel [27] as the similarity function. Figure 3 shows the test error for unlabeled nodes. In this
ﬁgure  two dashed lines with different markers are by HGF and N-HGF  while two solid lines with
the same markers are by HGF with AEW. The performance difference within the variants of HGF
was not large  compared to the effect of AEW  particularly in COIL  ORL  Vowel  Yale  and UMIST.
We can rather see that AEW substantially improved the prediction accuracy of HGF in most cases.
LNP is by the solid line without any markers. LNP outperformed HGF (without AEW  shown as the
dashed lines) in COIL  ORL  Vowel  Yale and UMIST  while HGF with AEW (at least one of three
variants) achieved better performance than LNP in all these datasets except for Yale (In Yale  LNP
and HGF with AEW attained a similar accuracy).

Overall AEW-N-HGF had the best prediction accuracy  where typical examples were USPS and
MNIST. Although Theorem 1 exactly holds only for AEW-HGF  we can see that AEW-N-HGF  in
which the degrees of the graph nodes are scaled by normalized Laplacian had highly stable perfor-
mance.

We further examined the effect of k. Figure 4 shows the test error for k = 20 and 10  using N-HGF 
AEW-N-HGF  and LNP for COIL dataset. The number of labeled instances is the midst value in

7

 

 

HGF

N−HGF

AEW−HGF

AEW−N−HGF

LNP

0.55

0.5

0.45

0.4

0.35

0.3

0.25

0.2

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

 

HGF

N−HGF

AEW−HGF

AEW−N−HGF

LNP

0.3

0.25

0.2

0.15

0.1

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

 

HGF

N−HGF

AEW−HGF

AEW−N−HGF

LNP

2

4

6

8

10

# labeled instances in each class

 

2

4

6

8

10

# labeled instances in each class

0.05

 
1

2

3

4

5

# labeled instances in each class

(b) USPS

(c) MNIST

(d) ORL

 

 

 

 

HGF

N−HGF

AEW−HGF

AEW−N−HGF

LNP

HGF

N−HGF

AEW−HGF

AEW−N−HGF

LNP

HGF

N−HGF

AEW−HGF

AEW−N−HGF

LNP

0.3

0.25

0.2

0.15

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

0.1

 

2

4

6

8

10

# labeled instances in each class

(a) COIL

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

0.2

 

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

0.35

0.3

0.25

0.2

0.15

0.1

 

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

HGF

N−HGF

AEW−HGF

AEW−N−HGF

LNP

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

0.16

0.14

0.12

0.1

0.08

0.06

0.04

 

HGF

N−HGF

AEW−HGF

AEW−N−HGF

LNP

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

 

2

4

6

8

10

# labeled instances in each class

2

4

6

8

10

# labeled instances in each class

5

10

15

20

# labeled instances in each class

 

2

4

6

8

10

# labeled instances in each class

(e) Vowel

(f) Yale

(g) optdigit

(h) UMIST

Figure 3: Performance comparison on real-world datasets. HGFs with AEW are by solid lines with
markers  while HGFs with median heuristics is by dashed lines with the same markers  and LNP
is by a solid line without any markers. For N-HGF and AWE-N-HGF  ‘N’ indicates normalized
Laplacian.

the horizontal axis of Figure 3 (a) (5 in each class). We can see that the test error of AEW is not
sensitive to k. Performance of N-HGF with k = 20 was worse than that with k = 10. On the other
hand  AEW-N-HGF with k = 20 had a similar performance to that with k = 10.

7 Conclusions

0.2

0.15

0.1

e

t

a
r
 
r
o
r
r
e

 
t
s
e
T

0.25

We have proposed the adaptive edge weighting (AEW)
method for graph-based semi-supervised learning. AEW
is based on the local reconstruction with the constraint
that each edge represents the similarity of each pair of
nodes. Due to this constraint  AEW has numerous ad-
vantages against LLE based approaches. For example 
noise sensitivity of LLE can be alleviated by the parame-
terized form of the edge weights  and the similarity form
for the edges weights is very reasonable for graph-based
methods. We also provide several interesting properties
of AEW  by which our objective function can be mo-
tivated analytically. We examined the performance of
AEW by using two synthetic and eight real benchmark
datasets. Experimental results demonstrated that AEW
can improve the performance of the harmonic Gaussian
ﬁeld (HGF) model substantially  and we also saw that AEW outperformed LLE based approaches in
all cases of real datasets except only one case.

Figure 4: Comparison in test error rates
of k = 10 and 20 (COIL ℓ = 50). Two
boxplots of each method correspond to
k = 10 in the left (with a smaller width)
and k = 20 in the right (with a larger
width).

AEW−N−HGF

N−HGF

0.05

LNP

References

[1] X. Zhu  Z. Ghahramani  and J. D. Lafferty  “Semi-supervised learning using Gaussian ﬁelds and harmonic
functions ” in Proc. of the 20th ICML (T. Fawcett and N. Mishra  eds.)  pp. 912–919  AAAI Press  2003.

[2] D. Zhou  O. Bousquet  T. N. Lal  J. Weston  and B. Sch¨olkopf  “Learning with local and global consis-

tency ” in Advances in NIPS 16 (S. Thrun  L. Saul  and B. Sch¨olkopf  eds.)  MIT Press  2004.

8

[3] A. Kapoor  Y. A. Qi  H. Ahn  and R. Picard  “Hyperparameter and kernel learning for graph based semi-
supervised classiﬁcation ” in Advances in NIPS 18 (Y. Weiss  B. Sch¨olkopf  and J. Platt  eds.)  pp. 627–
634  MIT Press  2006.

[4] X. Zhang and W. S. Lee  “Hyperparameter learning for graph based semi-supervised learning algorithms ”
in Advances in NIPS 19 (B. Sch¨olkopf  J. Platt  and T. Hoffman  eds.)  pp. 1585–1592  MIT Press  2007.

[5] F. Wang and C. Zhang  “Label propagation through linear neighborhoods ” IEEE TKDE  vol. 20  pp. 55–

67  2008.

[6] S. Roweis and L. Saul  “Nonlinear dimensionality reduction by locally linear embedding ” Science 

vol. 290  no. 5500  pp. 2323–2326  2000.

[7] S. I. Daitch  J. A. Kelner  and D. A. Spielman  “Fitting a graph to vector data ” in Proc. of the 26th ICML 

(New York  NY  USA)  pp. 201–208  ACM  2009.

[8] H. Cheng  Z. Liu  and J. Yang  “Sparsity induced similarity measure for label propagation ” in IEEE 12th

ICCV  pp. 317–324  IEEE  2009.

[9] W. Liu  J. He  and S.-F. Chang  “Large graph construction for scalable semi-supervised learning ” in Proc.

of the 27th ICML  pp. 679–686  Omnipress  2010.

[10] J. Chen and Y. Liu  “Locally linear embedding: a survey ” Artiﬁcial Intelligence Review  vol. 36  pp. 29–

48  2011.

[11] L. K. Saul and S. T. Roweis  “Think globally  ﬁt locally: unsupervised learning of low dimensional

manifolds ” JMLR  vol. 4  pp. 119–155  Dec. 2003.

[12] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Sch¨olkopf  and A. J. Smola  “A kernel method for the two-
sample-problem ” in Advances in NIPS 19 (B. Sch¨olkopf  J. C. Platt  and T. Hoffman  eds.)  pp. 513–520 
MIT Press  2007.

[13] E. Elhamifar and R. Vidal  “Sparse manifold clustering and embedding ” in Advances in NIPS 24

(J. Shawe-Taylor  R. Zemel  P. Bartlett  F. Pereira  and K. Weinberger  eds.)  pp. 55–63  2011.

[14] D. Kong  C. H. Ding  H. Huang  and F. Nie  “An iterative locally linear embedding algorithm ” in Proc.

of the 29th ICML (J. Langford and J. Pineau  eds.)  pp. 1647–1654  Omnipress  2012.

[15] X. Zhu  J. Kandola  Z. Ghahramani  and J. Lafferty  “Nonparametric transforms of graph kernels for semi-
supervised learning ” in Advances in NIPS 17 (L. K. Saul  Y. Weiss  and L. Bottou  eds.)  pp. 1641–1648 
MIT Press  2005.

[16] F. R. Bach and M. I. Jordan  “Learning spectral clustering ” in Advances in NIPS 16 (S. Thrun  L. K. Saul 

and B. Sch¨olkopf  eds.)  2004.

[17] T. Jebara  J. Wang  and S.-F. Chang  “Graph construction and b-matching for semi-supervised learning ”
in Proc. of the 26th ICML (A. P. Danyluk  L. Bottou  and M. L. Littman  eds.)  pp. 441–448  ACM  2009.

[18] M. S. Baghshah and S. B. Shouraki  “Metric learning for semi-supervised clustering using pairwise con-
straints and the geometrical structure of data ” Intelligent Data Analysis  vol. 13  no. 6  pp. 887–899 
2009.

[19] B. Shaw  B. Huang  and T. Jebara  “Learning a distance metric from a network ” in Advances in NIPS 24

(J. Shawe-Taylor  R. Zemel  P. Bartlett  F. Pereira  and K. Weinberger  eds.)  pp. 1899–1907  2011.

[20] S. A. Nene  S. K. Nayar  and H. Murase  “Columbia object image library ” tech. rep.  CUCS-005-96 

1996.

[21] T. Hastie  R. Tibshirani  and J. H. Friedman  The elements of statistical learning: data mining  inference 

and prediction. New York: Springer-Verlag  2001.

[22] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner  “Gradient-based learning applied to document recogni-

tion ” Proceedings of the IEEE  vol. 86  no. 11  pp. 2278–2324  1998.

[23] F. Samaria and A. Harter  “Parameterisation of a stochastic model for human face identiﬁcation ” in

Proceedings of the Second IEEE Workshop on Applications of Computer Vision  pp. 138–142  1994.

[24] A.

Asuncion

and

D.

J.

Newman 

“UCI

machine

learning

repository.”

http://www.ics.uci.edu/˜mlearn/MLRepository.html  2007.

[25] A. Georghiades  P. Belhumeur  and D. Kriegman  “From few to many: Illumination cone models for face

recognition under variable lighting and pose ” IEEE TPAMI  vol. 23  no. 6  pp. 643–660  2001.

[26] D. B. Graham and N. M. Allinson  “Characterizing virtual eigensignatures for general purpose face recog-
nition ” in Face Recognition: From Theory to Applications ; NATO ASI Series F  Computer and Systems
Sciences (H. Wechsler  P. J. Phillips  V. Bruce  F. Fogelman-Soulie  and T. S. Huang  eds.)  vol. 163 
pp. 446–456  1998.

[27] L. Zelnik-Manor and P. Perona  “Self-tuning spectral clustering ” in Advances in NIPS 17  pp. 1601–1608 

MIT Press  2004.

9

,Masayuki Karasuyama
Hiroshi Mamitsuka
Naoya Takeishi
Yoshinobu Kawahara
Takehisa Yairi
Danfei Xu
Roberto Martín-Martín
De-An Huang
Yuke Zhu
Silvio Savarese
Li Fei-Fei