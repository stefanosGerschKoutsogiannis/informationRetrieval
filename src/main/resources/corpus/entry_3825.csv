2019,Nonparametric Density Estimation & Convergence Rates for GANs under Besov IPM Losses,We study the problem of estimating a nonparametric probability distribution under a family of losses called Besov IPMs. This family is quite large  including  for example  L^p distances  total variation distance  and generalizations of both Wasserstein (earthmover's) and Kolmogorov-Smirnov distances. For a wide variety of settings  we provide both lower and upper bounds  identifying precisely how the choice of loss function and assumptions on the data distribution interact to determine the mini-max optimal convergence rate. We also show that  in many cases  linear distribution estimates  such as the empirical distribution or kernel density estimator  cannot converge at the optimal rate. These bounds generalize  unify  or improve on several recent and classical results. Moreover  IPMs can be used to formalize a statistical model of generative adversarial networks (GANs). Thus  we show how our results imply bounds on the statistical error of a GAN  showing  for example  that  in many cases  GANs can strictly outperform the best linear estimator.,Nonparametric Density Estimation and

Convergence of GANs under Besov IPM Losses

Ananya Uppal

Department of Mathematical Sciences

Carnegie Mellon University
auppal@andrew.cmu.edu

Shashank Singh∗ Barnabás Póczos

Machine Learning Department
Carnegie Mellon University

{sss1 bapoczos}@cs.cmu.edu

Abstract

We study the problem of estimating a nonparametric probability density under a
large family of losses called Besov IPMs  which include  for example  Lp distances 
total variation distance  and generalizations of both Wasserstein and Kolmogorov-
Smirnov distances. For a wide variety of settings  we provide both lower and upper
bounds  identifying precisely how the choice of loss function and assumptions
on the data interact to determine the minimax optimal convergence rate. We also
show that linear distribution estimates  such as the empirical distribution or kernel
density estimator  often fail to converge at the optimal rate. Our bounds generalize 
unify  or improve several recent and classical results. Moreover  IPMs can be used
to formalize a statistical model of generative adversarial networks (GANs). Thus 
we show how our results imply bounds on the statistical error of a GAN  showing 
for example  that GANs can strictly outperform the best linear estimator.

Introduction

1
This paper studies the problem of estimating a nonparametric probability density  using an integral
probability metric as a loss. That is  given a sample space X ⊆ RD  suppose we observe n IID
IID∼ p from a probability density p over X that is unknown but assumed to lie in
samples X1  ...  Xn

a regularity class P. We seek an estimator(cid:98)p : X n → P of p  with the goal of minimizing a loss

dF (p (cid:98)p(X1  ...  Xn)) := sup

f∈F

(cid:12)(cid:12)(cid:12)(cid:12) E

X∼p

(cid:12)(cid:12)(cid:12)(cid:12)  

[f (X)] −

E

X∼(cid:98)p(X1 ... Xn)

[f (X)]

(∗)

where F  called the discriminator class  is some class of bounded  measurable functions on X .
Metrics of the form (∗) are called integral probability metrics (IPMs)  or F-IPMs2  and can capture a
wide variety of metrics on probability distributions by choosing F appropriately [38]. This paper
studies the case where both F and P belong to the family of Besov spaces  a large family of
nonparametric smoothness spaces that include  as examples  Lp  Lipschitz/Hölder  and Hilbert-
Sobolev spaces. The resulting IPMs include  as examples  Lp  total variation  Kolmogorov-Smirnov 
and Wasserstein distances. We have two main motivations for studying this problem:

1. This problem uniﬁes nonparametric density estimation with the central problem of empirical

process theory  namely bounding quantities of the form dF (P (cid:98)P ) when (cid:98)P is the empirical distribution
P and ﬁxes the estimator (cid:98)P = Pn  focusing on the discriminator class F  nonparametric density
estimation typically ﬁxes the loss to be an Lp distance  and seeks a good estimator (cid:98)P for a given

i=1 δXi of the data [42]. Whereas empirical process theory typically avoids restricting

(cid:80)n

Pn = 1
n

∗Now at Google.
2While the name IPM seems most widely used [38  48  6  58]  many other names have been used for these

quantities  including adversarial loss [46  12]  MMD [16]  and F-distance or neural net distance [5].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

distribution class P. In contrast  we study how constraints on F and P jointly determine convergence

rates of a number of estimates (cid:98)P of P . In particular  since Besov spaces comprise perhaps the largest

commonly-studied family of nonparametric function spaces  this perspective allows us to unify 
generalize  and extend several classical and recent results in distribution estimation (see Section 3).
2. This problem is a theoretical framework for analyzing generative adversarial networks (GANs).
Speciﬁcally  given a GAN whose discriminator and generator networks encode functions in F and P 
respectively  recent work [31  27  28  46] showed that a GAN can be seen as a distribution estimate3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = argmin

Q∈P

(cid:16)

Q (cid:101)Pn

(cid:17)

dF

 

(1)

(cid:98)P = argmin

Q∈P

sup
f∈F

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) E

X∼Q

[f (X)] − E
X∼(cid:101)Pn

[f (X)]

1
n

i.e.  an estimate which directly minimizes empirical IPM risk with respect to a (regularized) empirical

distribution (cid:101)Pn. While  in the original GAN model [20]  (cid:101)Pn was the empirical distribution Pn =
(cid:80)n
distribution  performance is improved by replacing Pn with a regularized version (cid:101)Pn  equivalent to
that  when (cid:101)Pn is a wavelet-thresholding estimate  a GAN based on sufﬁciently large fully-connected

i=1 δXi of the data  Liang [27] showed that  under smoothness assumptions on the population

the instance noise trick that has become standard in GAN training [47  34]. We show  in particular 

neural networks with ReLU activations learns Besov probability distributions at the optimal rate.

2 Set up and Notation
< ∞  and
For non-negative real sequences {an}n∈N  {bn}n∈N  an (cid:46) bn indicates lim supn→∞ an
an (cid:16) bn indicates an (cid:46) bn (cid:46) an. For p ∈ [1 ∞]  p(cid:48) := p
p−1 denotes the Hölder conjugate of p
(with 1(cid:48) = ∞  ∞(cid:48) = 1). Lp(RD) (resp. lp) denotes the set of functions f (resp. sequences a) with

bn

< ∞ (resp. (cid:107)a(cid:107)lp :=(cid:0)(cid:80)

n∈N |an|p(cid:1)1/p

(cid:107)f(cid:107)p :=(cid:0)(cid:82) |f (x)|p dx(cid:1)1/p

< ∞).

j=−∞ Vj = L2(RD).

2.1 Multiresolution Approximation and Besov Spaces
We now provide some notation that is necessary to deﬁne the family of Besov spaces studied in
this paper. Since the statements and formal justiﬁcations behind these deﬁnitions are a bit complex 
some technical details are relegated to the Appendix  and several well-known examples from the
rich class of resulting spaces are given in Section 3. The diversity of Besov spaces arises from the
fact that  unlike the Hölder or Sobolev spaces that they generalize  Besov spaces model functions
simultaneously across multiple spatial scales. In particular  they rely on the following notion:
Deﬁnition 1. A multiresolution approximation (MRA) of L2(RD) is an increasing sequence {Vj}j∈Z
of closed linear subspaces of L2(RD) with the following properties:

1. (cid:84)∞
j=−∞ Vj = {0}  and the closure of(cid:83)∞

2. For f ∈ L2(RD)  k ∈ ZD  j ∈ Z  f (x) ∈ V0 ⇔ f (x − k) ∈ V0 & f (x) ∈ Vj ⇔ f (2x) ∈ Vj+1.
3. For some “father wavelet” φ ∈ V0  {φ(x−k) : k ∈ ZD} is an orthonormal basis of V0 ⊂ L2(RD).
For intuition  consider the best-known MRA of L2(R)  namely the Haar wavelet basis. Let φ(x) =
1{[0 1)} be the Haar father wavelet  let V0 = Span{φ(x − k) : k ∈ Z} be the span of translations of φ
by an integer  and let Vj deﬁned recursively for all j ∈ Z by Vj = {f (2x) : f (x) ∈ Vj−1} be the set
of horizontal scalings of functions in Vj−1 by 1/2. Then  {Vj}j∈Z is an MRA of L2(R).
The importance of an MRA is that it generates an orthonormal basis of L2(RD)  via the following:
Lemma 2 ([35]  Section 3.9). Let {Vj}j∈Z be an MRA of L2(RD) with father wavelet φ. Then  for
E = {0  1}D \ (0  . . .   0)  there exist “mother wavelets” {ψ}∈E such that {2Dj/2ψ(2jx − k) :
 ∈ E  k ∈ ZD} ∪ {2Dj/2φ(2jx − k) : k ∈ ZD} is an orthonormal basis of Vj ⊆ L2(RD).
Let Λj = {2−jk + 2−j−1 : k ∈ ZD   ∈ E} ⊆ RD. Then k   are uniquely determined for any
j∈Z Λj  we can let ψλ(x) = 2Dj/2ψ(2jx − k). Equipped with
the orthonormal basis {ψλ : λ ∈ Λ} of L2(RD)  we are almost ready to deﬁne Besov spaces.
For technical reasons (see  e.g.  [35  Section 3.9])  we need MRAs of smoother functions than Haar
wavelets  which are called r-regular. Due to space constraints  r-regularity is deﬁned precisely in

λ ∈ Λj. Thus  for all λ ∈ Λ :=(cid:83)

3We assume a good optimization algorithm for computing (1)  although this is also an active area of research.

2

Appendix A; we note here that standard r-regular MRAs exist  such as the Daubechies wavelet [10].
We assume for the rest of the paper that the wavelets deﬁned above are supported on [−A  A].
Deﬁnition 3 (Besov Space). Let 0 ≤ σ < r  and let p  q ∈ [1 ∞]. Given an r-regular MRA of
L2(RD) with father and mother wavelets φ  ψ respectively  the Besov space Bσ
p q(RD) is deﬁned as
the set of functions f : RD → R such that  the wavelet coefﬁcients

(cid:90)

(cid:90)

RD

αk :=

satisfy

f (x)φ(x − k)dx for k ∈ ZD
(cid:107)f(cid:107)Bσ
:= (cid:107){αk}k∈ZD(cid:107)lp +
The quantity (cid:107)f(cid:107)Bσ
the closed Besov ball Bσ
(e.g.  for rates of convergence)  Bσ

p q(L) = {f ∈ Bσ

p q

p q : (cid:107)f(cid:107)Bσ
p q denotes a ball Bσ

p q

RD

and

(cid:111)

βλ :=

(cid:13)(cid:13)(cid:13)(cid:13)(cid:110)
2j(σ+D(1/2−1/p))(cid:13)(cid:13){βλ}λ∈Λj

f (x)ψλ(x)dx for λ ∈ Λ 
< ∞

(cid:13)(cid:13)lp

j∈N
p q(L) to denote
≤ L}. When the constant L is unimportant
p q(L) of ﬁnite but arbitrary radius L.

(cid:13)(cid:13)(cid:13)(cid:13)lq

p q is called the Besov norm of f  and  for any L > 0  we write Bσ

σd

2.2 Formal Problem Statement
Having deﬁned Besov spaces  we now formally state the statistical problem we study in this paper.
IID∼ p from an unknown probability
Fix an r-regular MRA. We observe n IID samples X1  ...  Xn
density p lying in a Besov ball Bσg
pg qg (Lg) with σg < r. We want to estimate p  measuring error with
pd  qd (Ld). Speciﬁcally  for general σd  σg  pd  pg  qd  qg  we seek to bound minimax risk
an IPM dB

(cid:17)
:= inf(cid:98)p
pg qg  where the inﬁmum is taken over all estimators(cid:98)p(X1  . . .   Xn).
In the rest of this paper  we suppress dependence of(cid:98)p(X1  ...  Xn) on X1  ...  Xn  writing simply(cid:98)p.

(p (cid:98)p(X1  . . .   Xn))

of estimating densities in Fg = Bσg

sup
σg
pg  qg

E
X1:n

  Bσg

(cid:16)

σd
pd  qd

Bσd

p∈B

(cid:104)

(cid:105)

(2)

dB

M

pd qd

pg qg

3 Related Work
The current paper uniﬁes  extends  or improves upon a number of recent and classical results in the
nonparametric density estimation literature. Two areas of prior work are most relevant:

Nonparametric estimation over inhomogeneous smoothness spaces First is the classical study
of estimation over inhomogeneous smoothness spaces under Lp losses. Nemirovski [40] ﬁrst noticed
that  over classes of regression functions with inhomogeneous (i.e.  spatially-varying) smoothness 
many widely-used regression estimators  called “linear” estimators (deﬁned precisely in Section 4.2) 
are provably unable to converge at the minimax optimal rate  in L2 loss. Donoho et al. [13] identiﬁed
pg qg on R under Lp(cid:48)
a similar phenomenon for estimating probability densities in a Besov space Bσg
losses with p(cid:48)
d > pg  corresponding to the case σd = 0  D = 1 in our work. [13] also showed that
the wavelet-thresholding estimator we consider in Section 4.1 does converge at the minimax optimal
rate. We generalize these phenomena to many new loss functions; in many cases  linear estimators
continue to be sub-optimal  whereas the wavelet-thresholding estimator continues to be optimal. We
also show that sub-optimality of linear estimators is more pronounced in higher dimensions.

d

Distribution estimation under IPMs The second  more recent body of results [27  46  28] con-
cerns nonparametric distribution estimation under IPM losses. Prior work focused on the case where
F and P are both Sobolev ellipsoids  corresponding to the case pd = qd = pg = qg = 2 in our work.
Notably  over these smaller spaces (of homogeneous smoothness)  the linear estimators mentioned
above are minimax rate-optimal. Perhaps the most important ﬁnding of these works is that the curse of
dimensionality pervading classical nonparametric statistics is signiﬁcantly diminished under weaker
loss functions than Lp losses (namely  many IPMs). For example  Singh et al. [46] showed that  when
σd > D/2  one can estimate P at the parametric rate n−1/2 in the loss dB
  without any regularity
assumptions whatsoever on the probability distribution P . We generalize this to other losses dB
.
σd
pd  qd
These papers were motivated in part by a desire to understand theoretical properties of GANs  and 
in particular  Liang [27] and Singh et al. [46] helped establish (1) as a valid statistical model of
GANs. In particular  we note that Singh et al. [46] showed that the implicit generative modeling
problem (“sampling”) in terms of which GANs are usually framed  is equivalent  in terms of minimax

σd
2 2

3

convergence rates  to nonparametric density estmation  justifying our focus on the latter problem in
this paper. We show  in Section 4.3  that  given a sufﬁciently good optimization algorithm  GANs
based on appropriately constructed deep neural networks can learn Besov densities at the minimax
optimal rate. In this context  our results are among the ﬁrst to suggest theoretically that GANs can
outperform classical density estimators (namely  linear estimators mentioned above).
Liu et al. [31] provided general sufﬁcient conditions for weak consistency of GANs in a generalization
of the model (1). Since many IPMs  such as Wasserstein distances  metrize weak convergence of
probability measures under mild additional assumptions Villani [52]  this implies consistency under
these IPMs. However  Liu et al. [31] did not study rates of convergence.
We end this section with a brief survey of known results for estimating distributions under speciﬁc
Besov IPM losses  noting that our results (Equations (3) and (4) below) generalize all these rates:
p(cid:48) p(cid:48)  then  for distributions P  Q with densities p  q ∈ Lp 
1. Lp Distances: If Fd = Lp(cid:48)
dFd (P  Q) = (cid:107)p−q(cid:107)Lp. These are the most well-studied losses in nonparametric statistics  especially
for p ∈ {1  2 ∞} [41  53  51]. [13] studied the minimax rate of convergence of density estimation
over
over Besov spaces under Lp losses  obtaining minimax rates n

− σg +D(1−1/pg−1/pd)

2σg +D(1−2/pg)

2σg +D + n

= B0

− σg

− σg−D/pg +D/p(cid:48)
2σg +D−2D/pg +2D/p(cid:48)

d

− σg

2σg +D + n

d when restricted to linear estimators.

(p  q) ≤ Wp(p  q) ≤ m−1/p(cid:48)

general estimators  and n
2. Wasserstein Distance: If Fd = C 1(1) (cid:16) B1∞ ∞ is the space of 1-Lipschitz functions  then dFd
is the 1-Wasserstein or Earth mover’s distance (via the Kantorovich dual formulation [23  52]). A long
line of work has established convergence rates of the empirical distribution to the true distribution
in spaces as general as unbounded metric spaces [54  25  45]). In the Euclidean setting  this is
well understood [14  2  18]  although  to the best of our knowledge  minimax lower bounds have
been proven only recently [45]; this setting intersects with our work in the case σd = 1  σg = 0 
pd = ∞  matching our minimax rate of n−1/D + n−1/2. More general p-Wasserstein distances Wp
(p ≥ 1) cannot be expressed exactly as IPMs  but  our results complement recent results of Weed
and Berthet [55]  who showed that  for densities p and q that are bounded above and below (i.e. 
0 < m ≤ p  q ≤ M < ∞)  the bounds M−1/p(cid:48)
dB1
(p  q)
p(cid:48) ∞
− 1+σg
2σg +D + n−1/2) up to polylogarithmic factors.
hold; for such densities  our rates match theirs (n
Weed and Berthet [55] showed that  without the lower-boundedness assumption (m > 0)  minimax
rates under Wp are strictly slower (by a polynomial factor in n).
In machine learning applications  Arora et al. [5] recently used this rate to argue that  for data from
a continuous distribution  Wasserstein GANs [4] cannot generalize at a rate faster than n−1/D (at
least without additional regularization  as we use in Theorem 9). A variant in which Fd ⊂ C 1 ∩ L∞
is both uniformly bounded and 1-Lipschitz gives rise to the Dudley metric [15]  which has also
been suggested for use in GANs [1]. Finally  we note that the more general distances induced by
Fd = Bσd∞ ∞ have been useful for deriving central limit theorems [7  Section 4.8].
3. Kolmogorov-Smirnov Distance: If Fd = BV (cid:16) B1
1 · is the set of functions of bounded variation 
then  in the 1-dimensional case  dFd is the well-known Kolmogorov-Smirnov metric [9]  and so the
famous Dvoretzky–Kiefer–Wolfowitz inequality [33] gives a parametric convergence rate of n−1/2.
4. Sobolev Distances: If Fd = W σd 2 = Bσ
2 2 is a Hilbert-Sobolev space  for σ ∈ R  then dFd =
(cid:107) · − · (cid:107)W−σd 2 is the corresponding negative Sobolev pseudometric [57]. Recent work [27  46  28]
− σg +σd
2σg +1 + n−1/2 when Fg = W σg 2 is also a Hilbert-Sobolev space.
established a minimax rate of n

dB1

p(cid:48)  1

4 Main Results
The three main technical contributions of this paper are as follows:

1. We prove lower and upper bounds (Theorems 4 and 5  respectively) on minimax convergence
rates of distribution estimation under IPM losses when the distribution class P = Bσg
pg qg and the
discriminator class F = Bσd
pd qd are Besov spaces; these rates match up to polylogarithmic factors in
the sample size n. Our upper bounds use the wavelet-thresholding estimator proposed in Donoho et al.
[13]  which we show converges at the optimal rate for a much wider range of losses than previously
known. Speciﬁcally  if M (F P) denotes minimax risk (2)  we show that for p(cid:48)
d ≥ pg  σg ≥ D/pg 

4

(cid:16)

M

Bσd

pd qd

  Bσg

pg qg

(cid:40)

n−1/2  n

− σg +σd

2σg +D   n

− σg +σd+D(1−1/pg−1/pd)

2σg +D(1−2/pg)

(cid:41)

.

(3)

(cid:17) (cid:16) max
(cid:16)

Bσd

pd qd

2. We show (Theorem 7) that  for p(cid:48)
distribution estimators  called “linear estimators”  can converge at a rate faster than

d ≥ pg and σg ≥ D/pg  no estimator in a large class of

(cid:17) (cid:38) n

− σg +σd−D/pg +D/p(cid:48)
2σg +D(1−2/pg)+2D/p(cid:48)

d

d .

Mlin

  Bσg

pg qg

“Linear estimators” include the empirical distribution  kernel density estimates with uniform band-
width  and the orthogonal series estimators recently used in Liang [27] and Singh et al. [46]). The
lower bound (4) implies that  in many settings (discussed in Section 5)  linear estimators converge at
sub-optimal rates. This effect is especially pronounced when the data dimension D is large and the
distribution P has relatively sparse support (e.g.  if P is supported near a low-dimensional manifold).
3. We show that the minimax convergence rate can be achieved by a GAN with generator and
discriminator networks of bounded size  after some regularization. As one of the ﬁrst theoretical
results separating performance of GANs from that of classic nonparametric tools such as kernel
methods  this may help explain GANs’ successes with high-dimensional data such as images.

4.1 Minimax Rates over Besov Spaces
We now present our main lower and upper bounds for estimating densities that live in a Besov space
under a Besov IPM loss. Then  we have the following lower bound on the convergence rate:
Theorem 4. (Lower Bound) Let r > σg ≥ D/pg  then 

(cid:16)

M

Bσd

pd qd

  Bσg

pg qg

(cid:17) (cid:38) max

n

(cid:18) log n

(cid:19) σg +σd+D−D/pg−D/pd

2σg +D−2D/pg



− σg +σd
2σg +D  

n

(4)

(5)

Before giving a corresponding upper bound  we describe the estimator on which it depends.
Wavelet-Thresholding: Our upper bound uses the wavelet-thresholding estimator proposed by [13]:

(cid:88)

j0(cid:88)

(cid:88)

(cid:98)pn =

(cid:101)βλψλ.
(cid:98)αkφk +
(cid:98)pn estimates p via its truncated wavelet expansion  with (cid:98)αk = 1
(cid:80)n
i=1 ψλ(Xi)  and (cid:101)βλ = (cid:98)βλ1{(cid:98)βλ>

(cid:98)βλψλ +

λ∈Λj

λ∈Λj

k∈Z

√

j=j0

j=0

n

j1(cid:88)

(cid:88)

j/n} are empirical estimates of respective coefﬁcient of
1
n
the wavelet expansion of p. As [13] ﬁrst showed  attaining optimality over Besov spaces requires
truncating high-resolution terms (of order j ∈ [j0  j1]) when their empirical estimates are too small;
this “nonlinear” part of the estimator distinguishes it from the “linear” estimators we study in the next
section. The hyperparameters j0 and j1 are set to j0 = 1
Theorem 5. (Upper Bound) Let r > σg ≥ D/pg and p(cid:48)
only on p(cid:48)

2σg+D log2 n  j1 =
d > pg. Then  for a constant C depending

2σg+D−2D/pg

log2 n.

 

1

(6)

(cid:80)n
i=1 φk(Xi)  (cid:98)βλ =

(cid:18)
d  σg  pg  qg  D  Lg  Ld and (cid:107)ψ(cid:107)p(cid:48)
(cid:16)

(cid:18)(cid:112)log n

(cid:17) ≤ C

  Bσg

Bσd

d

pd qd

pg qg

M

− σg +σd

n

2σg +D + n

− σg +σd−D/pg +D/p(cid:48)
2σg +D−2D/pg

d

+ n−1/2

(7)

(cid:19)

(cid:19)

We will comment only brieﬂy on Theorems 4 and 5 here  leaving extended discussion for Section 5.
First  note that the lower bound (5) and upper bound (7) are essentially tight; they differ only by a
polylogarithmic factor in n. Second  both bounds contain two main terms of interest. The simpler
term  n
2σg +D   matches the rate observed in the Sobolev case by Singh et al. [46]. The other term is
unique to more general Besov spaces. Depending on the values of D  σd  σg  pd  and pg  one of these
two terms dominates  leading to two main regimes of convergence rates  which we call the “Sparse”
regime and the “Dense” regime. Section 5 discusses these and other interesting phenomena in detail.

− σg +σd

5

4.2 Minimax Rates of Linear Estimators over Besov Spaces
We now show that  for many Besov densities and IPM losses  many widely-used nonparametric
density estimators cannot converge at the optimal rate (5). These estimators are as follows:

Deﬁnition 6 (Linear Estimator). Let (Ω F  P ) be a probability space. An estimate (cid:98)P of P is said to

be linear if there exist functions Ti(Xi ·) : F → R such that for all measurable A ∈ F 

Ti(Xi  A).

(8)

n(cid:88)

i=1

(cid:98)P (A) =
(cid:82)

(cid:16)

(cid:17)

(cid:104)

(cid:16)

µp (cid:98)P

(cid:17)(cid:105) (cid:16) n− 1

Classic examples of linear estimators include the empirical distribution (Ti(Xi  A) = 1
the kernel density estimate (Ti(Xi  A) = 1
n
kernel K : X × X → R) and the orthogonal series estimate (Ti(Xi  A) = 1
some cutoff J and orthonormal basis {gj}∞
Theorem 7 (Minimax rate for Linear Estimators). Suppose r > σg ≥ D/pg 

n 1{Xi∈A} 
A K(Xi ·) for some bandwidth h > 0 and smoothing
A gj for

j=1 (e.g.  Fourier  wavelet  or polynomial) of L2(Ω)).

j=1 gj(Xi)(cid:82)
(cid:80)J

n

Bσd

  Bσg

pd qd

Mlin
where the inf is over all linear estimates of p ∈ Fg  and µp is the distribution with density p.

sup
p∈Fg

2 + n

d + n

pg qg

dFd

E
X1:n

:= inf(cid:98)Plin

− σg +σd−D/pg +D/p(cid:48)
2σg +D−2D/pg +2D/p(cid:48)

d

− σg +σd

2σg +D

− σg +σd+D−D/pg−D/pd

2σg +D−2D/pg

One can check that the above error decays no faster than n
. Comparing with
the rate in Theorem 5  this implies that  in certain cases  convergence the rate for linear estimators
is strictly slower than that for general estimators; i.e.  linear estimators fail to achieve the minimax
optimal rate over certain Besov space. We defer detailed discussion of this phenomenon to Section 5.
4.3 Upper Bounds on a Generative Adversarial Network
Pioneered by Goodfellow et al. [20] as a mechanism for applying deep neural networks to the problem
of unsupervised image generation  Generative adversarial networks (GANs) have since been widely
applied not only to computer vision [59  24]  but also to such diverse problems and data as machine
translation using natural language data [56]  discovering drugs [22] and designing materials [44] using
molecular structure data  inferring expression levels using gene expression data [11]  and sharing
patient data under privacy constraints using electronic health records [8]. Besides the Jensen-Shannon
divergence used by [20]  many GAN formulations have been proposed based on minimizing other
losses  including the Wasserstein metric [4  21]  total variation distance [30]  χ2 divergence [32] 
MMD [26]  Dudley metric [1]  and Sobolev metric [37]. The diversity of data types and losses with
which GANs have been used motivates studying GANs in a very general (nonparametric) setting.
In particular  Besov spaces likely comprise the largest widely-studied family of nonparametric
smoothness class; indeed  most of the losses listed above are Besov IPMs.
GANs are typically described as a two-player minimax game between a generator network Ng and a
discriminator network Nd; we denote by Fd the class of functions that can be implemented by Nd
and by Fg the class of distributions that can be implemented by Ng. A recent line of work has argued
that a natural statistical model for a GAN as a distribution estimator is

sup
f∈Fd

E
X∼Q

[f (X)] − E
X∼(cid:101)Pn

[f (X)]  

Q∈Fg

where (cid:101)Pn is an (appropriately regularized) empirical distribution  and that  when Fd and Fg respec-

tively approximate classes F and P well  one can bound the risk  under F-IPM loss  of estimating
distributions in P by (9) [31  27  46  28]. We emphasize  that  as Singh et al. [46] showed  the
minimax risk in this framework is identical to that under the “sampling” (or “implicit generative
modeling” [36]) framework in terms of which GANs are usually cast. 4
In this section  we show such a result for Besov spaces; namely  we show the existence of a particular
GAN (speciﬁcally  a sequence of GANs  necessarily growing with the sample size n)  that estimates
distributions in a Besov space at the minimax optimal rate (7) under Besov IPM losses. This

(cid:98)P := argmin

(9)

4As in these previous works  we assume implicitly that the optimum (9) can be computed; this complex
saddle-point problem is itself the subject of a related but distinct and highly active area of work [39  3  29  19].

6

construction uses a standard neural network architecture (a fully-connected neural network with

rectiﬁed linear unit (ReLU) activations)  and a simple data regularizer (cid:101)Pn  namely the wavelet-

thresholding estimator described in Section 4.1. Our results extend those of Liang [27] and Singh
et al. [46]  for Wasserstein loss over Sobolev spaces  to general Besov IPM losses over Besov spaces.
We begin with a formal deﬁnition of the network architectures that we consider:
Deﬁnition 8. A fully-connected ReLU network f(A1 ... AH ) (b1 ... bH ) : RW → R has the form

AH η (AH−1η (··· η(A1x + b1)··· ) + bH−1) + bH  

per)parameters: the depth H  the width W   the sparsity S :=(cid:80)

where  for each (cid:96) ∈ [H − 1]  A(cid:96) ∈ RW×W   and AH ∈ R1×W and the ReLU operation η(x) =
max{x  0} is applied element-wise to vectors in RW .
The size of f(A1 ... AH ) (b1 ... bH )(x) can be measured in terms of the following four (hy-
(cid:96)∈[H] (cid:107)A(cid:96)(cid:107)0 0 + (cid:107)b(cid:96)(cid:107)0 (i.e.  the total
number of non-zero weights)  and the maximum weight B := max{(cid:107)A(cid:96)(cid:107)∞ ∞ (cid:107)b(cid:96)(cid:107)∞ : (cid:96) ∈ [H]}.
For given size parameters H  W  S  B we write Φ(H  W  S  B) to denote the set of functions satisfying
the corresponding size constraints.
Our results rely on a recent construction (Lemma 17 in the Appendix)  by [49]  of a fully-connected
ReLU network that approximates Besov functions. [49] used this approximation to bound the risk of
a neural network for nonparametric regression over Besov spaces  under Lr loss. Here  we use this
approximation result Lemma 17 to bound the risk of a GAN for nonparametric distribution estimation
over Besov spaces  under the much larger class of Besov IPM losses. Our precise result is as follows:
Theorem 9 (Convergence Rate of a Well-Optimized GAN). Fix a Besov density class Bσg
pg qg with
σg > D/pg and discriminator class Bσd
with σd > D/pd. Then  for any desired approximation
Nd ∈ Φ(Hd  Wd  Sd  Bd) and generator network Ng ∈ Φ(Hg  Wg  Sg  Bg)  s.t. for all p ∈ Bσg

error  > 0  one can construct a GAN (cid:98)p of the form (9) (with (cid:101)pn) with discriminator network

pd qd

pg qg

E(cid:104)

dB

σd
pd qd

(cid:105) (cid:46)  + E dB

((cid:98)p  p)

((cid:101)pn  p)

σd
pd  qd

pd qd

and Bσg

where Hd  Hg grow logarithmically with 1/  Wd  Sd  Bd  Wg  Sg  Bg grow polynomially with 1/
and C > 0 is a constant that depends only on Bσd

This theorem implies that the rate of convergence of the GAN estimate(cid:98)p of the form 9 is the same as
the convergence rate of the estimator(cid:101)pn with which the GAN estimate is generated (Here we assume
with σd > D/pd there exists an appropriately constructed GAN estimate(cid:98)p s.t.

that all distributions have densities). Therefore  given our upper bound from theorem 5 we have the
following direct consequence.
Corollary 10. For a Besov density class Bσg

pg qg with σg > D/pg and discriminator class Bσd

pg qg.

pd qd

dFd ((cid:98)p  p) ≤(cid:16)
(cid:110) 1

n−η(D σd pd σg pg)(cid:112)log n
2σg+D   σg+σd+D−D/pg−D/p(cid:48)

2σg+D(1−2/pg)

2   σg+σd

d

(cid:17)
(cid:111)

where η(D  σd  pd  σg  pg) = min

is the exponent from (7).

In other words there is a GAN estimate that is minimax rate optimal for a smooth class of densities
over an IPM generated by a smooth class of discriminator functions.

5 Discussion of Results
In this section  we discuss some general phenomena that can be gleaned from our technical results.
First  we note that  perhaps surprisingly  qd and qg do not appear in our bounds. Tao [50] suggests
that qd and qg may have only logarithmic effects (contrasted with the polynomial effects of σd  pd 
σg  and pg). Thus  a more ﬁne-grained analysis to close the polylogarithmic gap between our lower
and upper bounds for general estimators (Theorems 4 and 5) might require incorporating qd and qg.
On the other hand  the parameters σd  pd  σg  and pg each play a signiﬁcant role in determining
minimax convergence rates  in both the linear and general cases. We ﬁrst discuss each of these
parameters independently  and then discuss some interactions between them.

7

(a) General Estimators

(b) Linear Estimators

Figure 1: Minimax convergence rates as functions of discriminator smoothness σd and distri-
bution function smoothness σg  for (a) general and (b) linear estimators  in the case D = 4 
pd = 1.2  pg = 2. Color shows exponent of minimax convergence rate (i.e.  α(σd  σg) such
that M

(cid:17) (cid:16) n−α(σd σg))  ignoring polylogarithmic factors.

(RD)  Bσg

(RD)

(cid:16)

Bσd

1.2 qd

2 qg

Roles of the smoothness orders σd and σg As a visual aid for understanding our results  Figure 1
show phase diagrams of minimax convergence rates  as functions of discriminator smoothness σd and
distribution smoothness σg  in the illustrative case D = 4  pd = 1.2  pg = 2. When 1/pg + 1/pd > 1 
a minimum total smoothness σd + σg ≥ D(1/pd + 1/pg − 1) is needed for consistent estimation to be
possible – this fails in the “Infeasible” region of the phase diagrams. Intuitively  this occurs because
Fd is not contained in the topological dual F(cid:48)
g of Fg. For linear estimators  even greater smoothness
σd+σg ≥ D(1/pd+1/pg) is needed. At the other extreme  for highly smooth discriminator functions 

both linear and nonlinear estimators converge at the parametric rate O(cid:0)n−1/2(cid:1)  corresponding to the

“Parametric” region. In between  rates for linear estimators vary smoothly with σd and σg  while rates
for nonlinear estimators exhibit another phase transition on the line σg + 3σd = D; to the left lies the
“Sparse” case  in which estimation error is dominated by a small number of large errors at locations
where the distribution exhibits high local variation; to the right lies the “Dense” case  where error is
relatively uniform on the sample space.
The left boundary σd = 0 corresponds to the classical results of Donoho et al. [13]  who consequently
identiﬁed the “Infeasible”  “Sparse”  and “Dense” phases  but not the “Parametric” phase. When
restricting to linear estimators  the “Infeasible” region grows and the “Parametric” region shrinks.
Role of the powers pd and pg At one extreme (pd = ∞) lie L1 or total variation loss (σd = 0) 
Wasserstein loss (σd = 1)  and its higher-order generalizations  for which we showed the rate

M

Bσd∞ qd

  Bσg

pg qg

− σg +σd
2σg +D + n−1/2 

(cid:16)

(cid:17) (cid:16) n

generalizing the rate ﬁrst shown by Singh et al. [46] for Hilbert-Sobolev classes to other distribution
classes  such as Fg = BV. Because discriminator functions in this class exhibit homogeneous
smoothness  these losses effectively weight the sample space relatively uniformly in importance  the
“Sparse” region in Figure (1a) vanishes  and linear estimators can perform optimally.
At the other extreme (pd = 1) lie L∞ loss (σd = 0)  Kolmogorov-Smirnov loss (σd = 1)  and its
higher-order generalizations  for which we have shown that the rate is always
+ n−1/2;

− σg +σd+D(1−1/pd−1/pg )

(cid:17) (cid:16) n

2σg +D(1−2/pg )

  Bσg

(cid:16)

M

Bσd
1 qd

pg qg

except in the parametric regime (D ≤ 2σd)  this rate differs from that of Singh et al. [46]. Because
discriminator functions can have inhomogeneous smoothness  and hence weight some portions of the
sample space much more heavily than others  the “Dense” region in Figure 1a vanishes  and linear
estimators are always sub-optimal. We note that Sadhanala et al. [43] recently proposed using these
higher-order distances (integer σd > 1) in a fast two-sample test that generalizes the well-known
Kolmogorov-Smirnov test  improving sensitivity to the tails of distributions; our results may provide
a step towards understanding theoretical properties of this test.
Comparison of linear and general rates Letting σ(cid:48)
sparse term of the linear minimax rate in the same form as the Dense rate  replacing σg with σ(cid:48)
g:

g := σg − D(1/pg + 1/pd)  one can write the

8

0.00.51.01.52.02.53.03.54.0d0.00.51.01.52.02.53.03.54.0gParametricDenseSparseInfeasible012345678d0.00.51.01.52.02.53.03.54.0ParametricNonparametricInfeasible0.00.10.20.30.40.5(cid:16)

Bσd

pd qd

(cid:17) (cid:16) n

− σ(cid:48)
g +σd
2σ(cid:48)
g +D .

  Bσg

pg qg

(10)

Mlin

pg pg ⊆ B

This is not a coincidence; Morrey’s inequality [17  Section 5.6.2] in functional analysis tells us that
g := σg − D(1/pg + 1/pd) is largest possible value such that the
for general σg > D(1/pg + 1/pd)  σ(cid:48)
σ(cid:48)
pd pd holds. In the extreme case pd = ∞ (corresponding to generalizations
embedding Bσg
of total variation loss)  one can interpret the rate (10) as saying that linear estimators beneﬁt only
from homogeneous (e.g.  Hölder) smoothness  and not from weaker inhomogeneous (e.g.  Besov)
smoothness. For general pd  linear estimator can still beneﬁt from inhomogeneous smoothness  but
to a lesser extent than general minimax optimal estimators.

g

Conclusions We have shown  up to log factors  uniﬁed minimax convergence rates for a large
class of pairs of Fd-IPM losses and distribution classes Fg. By doing so  we have generalized
several phenomena that had observed in special cases previously. First  under sufﬁciently weak loss
functions  distribution estimation is possible at the parametric rate O(n−1/2) even over very large
nonparametric distribution classes. Second  in many cases  optimal estimation requires estimators
that adapt to inhomogeneous smoothness conditions; many commonly used distribution estimators
fail to do this  and hence converge at sub-optimal rates  or even fail to converge. Finally  GANs with
sufﬁciently large fully-connected ReLU neural networks using wavelet-thresholding regularization
perform statistically minimax rate-optimal distribution estimation over inhomogeneous nonparametric
smoothness classes (assuming the GAN optimization problem can be solved accurately). Importantly 
since GANs optimize IPM losses much weaker than traditional Lp losses  they may be able to learn
reasonable approximations of even high-dimensional distributions with tractable sample complexity 
perhaps explaining why they excel in the case of image data. Thus  our results suggest that the curse
of dimensionality may be less severe than indicated by classical nonparametric lower bounds.

9

References
[1] Ehsan Abbasnejad  Javen Shi  and Anton van den Hengel. Deep Lipschitz networks and Dudley GANs 

2018. URL https://openreview.net/pdf?id=rkw-jlb0W.

[2] Miklós Ajtai  János Komlós  and Gábor Tusnády. On optimal matchings. Combinatorica  4(4):259–264 

1984.

[3] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial networks.

arXiv preprint arXiv:1701.04862  2017.

[4] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial networks. In

International Conference on Machine Learning  pages 214–223  2017.

[5] Sanjeev Arora  Rong Ge  Yingyu Liang  Tengyu Ma  and Yi Zhang. Generalization and equilibrium in

generative adversarial nets (GANs). arXiv preprint arXiv:1703.00573  2017.

[6] Leon Bottou  Martin Arjovsky  David Lopez-Paz  and Maxime Oquab. Geometrical insights for implicit
generative modeling. In Braverman Readings in Machine Learning. Key Ideas from Inception to Current
State  pages 229–268. Springer  2018.

[7] Louis HY Chen  Larry Goldstein  and Qi-Man Shao. Normal approximation by Stein’s method. Springer

Science & Business Media  2010.

[8] Edward Choi  Siddharth Biswal  Bradley Malin  Jon Duke  Walter F Stewart  and Jimeng Sun. Generating
multi-label discrete patient records using generative adversarial networks. arXiv preprint arXiv:1703.06490 
2017.

[9] Wayne W Daniel et al. Applied nonparametric statistics. Houghton Mifﬂin  1978.

[10] Ingrid Daubechies. Ten lectures on wavelets  volume 61. Siam  1992.

[11] Kamran Ghasedi Dizaji  Xiaoqian Wang  and Heng Huang. Semi-supervised generative adversarial network
for gene expression inference. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining  pages 1435–1444. ACM  2018.

[12] Hao-Wen Dong and Yi-Hsuan Yang. Towards a deeper understanding of adversarial losses. arXiv preprint

arXiv:1901.08753  2019.

[13] David L Donoho  Iain M Johnstone  Gérard Kerkyacharian  and Dominique Picard. Density estimation by

wavelet thresholding. The Annals of Statistics  pages 508–539  1996.

[14] RM Dudley. The speed of mean Glivenko-Cantelli convergence. The Annals of Mathematical Statistics  40

(1):40–50  1969.

[15] RM Dudley. Speeds of metric probability convergence. Zeitschrift für Wahrscheinlichkeitstheorie und

Verwandte Gebiete  22(4):323–332  1972.

[16] GK Dziugaite  DM Roy  and Z Ghahramani. Training generative neural networks via maximum mean
discrepancy optimization. In Uncertainty in Artiﬁcial Intelligence-Proceedings of the 31st Conference 
UAI 2015  pages 258–267  2015.

[17] Lawrence C Evans. Partial differential equations. American Mathematical Society  2010.

[18] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical

measure. Probability Theory and Related Fields  162(3-4):707–738  2015.

[19] Gauthier Gidel  Reyhane Askari Hemmat  Mohammad Pezeshki  Gabriel Huang  Remi Lepriol  Simon
Lacoste-Julien  and Ioannis Mitliagkas. Negative momentum for improved game dynamics. arXiv preprint
arXiv:1807.04740  2018.

[20] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron
Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems  pages 2672–2680  2014.

[21] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville. Improved
training of wasserstein gans. In Advances in Neural Information Processing Systems  pages 5767–5777 
2017.

10

[22] Artur Kadurin  Sergey Nikolenko  Kuzma Khrabrov  Alex Aliper  and Alex Zhavoronkov. drugan: an
advanced generative adversarial autoencoder model for de novo generation of new molecules with desired
molecular properties in silico. Molecular pharmaceutics  14(9):3098–3104  2017.

[23] Leonid Vasilevich Kantorovich and Gennady S Rubinstein. On a space of completely additive functions.

Vestnik Leningrad. Univ  13(7):52–59  1958.

[24] Christian Ledig  Lucas Theis  Ferenc Huszár  Jose Caballero  Andrew Cunningham  Alejandro Acosta 
Andrew Aitken  Alykhan Tejani  Johannes Totz  Zehan Wang  et al. Photo-realistic single image super-
resolution using a generative adversarial network. arXiv preprint  2017.

[25] Jing Lei. Convergence and concentration of empirical measures under Wasserstein distance in unbounded

functional spaces. arXiv preprint arXiv:1804.10556  2018.

[26] Chun-Liang Li  Wei-Cheng Chang  Yu Cheng  Yiming Yang  and Barnabás Póczos. Mmd gan: Towards
deeper understanding of moment matching network. In Advances in Neural Information Processing
Systems  pages 2203–2213  2017.

[27] Tengyuan Liang. How well can generative adversarial networks (GAN) learn densities: A nonparametric

view. arXiv preprint arXiv:1712.08244  2017.

[28] Tengyuan Liang. On how well generative adversarial networks learn densities: Nonparametric and

parametric results. arXiv preprint arXiv:1811.03179  2018.

[29] Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence of

generative adversarial networks. arXiv preprint arXiv:1802.06132  2018.

[30] Zinan Lin  Ashish Khetan  Giulia Fanti  and Sewoong Oh. Pacgan: The power of two samples in generative

adversarial networks. In Advances in Neural Information Processing Systems  pages 1505–1514  2018.

[31] Shuang Liu  Olivier Bousquet  and Kamalika Chaudhuri. Approximation and convergence properties of
generative adversarial learning. In Advances in Neural Information Processing Systems  pages 5551–5559 
2017.

[32] Xudong Mao  Qing Li  Haoran Xie  Raymond YK Lau  Zhen Wang  and Stephen Paul Smolley. Least
squares generative adversarial networks. In Proceedings of the IEEE International Conference on Computer
Vision  pages 2794–2802  2017.

[33] Pascal Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of Probability 

pages 1269–1283  1990.

[34] Lars Mescheder  Andreas Geiger  and Sebastian Nowozin. Which training methods for gans do actually

converge? arXiv preprint arXiv:1801.04406  2018.

[35] Yves Meyer. Wavelets and operators  volume 1. Cambridge university press  1992.

[36] Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv preprint

arXiv:1610.03483  2016.

[37] Youssef Mroueh  Chun-Liang Li  Tom Sercu  Anant Raj  and Yu Cheng. Sobolev gan. arXiv preprint

arXiv:1711.04894  2017.

[38] Alfred Müller. Integral probability metrics and their generating classes of functions. Advances in Applied

Probability  29(2):429–443  1997.

[39] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In Advances

in Neural Information Processing Systems  pages 5585–5595  2017.

[40] Arkadi S Nemirovski. Nonparametric estimation of smooth regression functions. Izv. Akad. Nauk. SSR

Teckhn. Kibernet  3:50–60  1985.

[41] Arkadi S Nemirovski. Topics in non-parametric. Ecole d’Eté de Probabilités de Saint-Flour  28:85  2000.

[42] David Pollard. Empirical processes: theory and applications. In NSF-CBMS regional conference series in

probability and statistics  pages i–86. JSTOR  1990.

[43] Veeranjaneyulu Sadhanala  Aaditya Ramdas  Yu-Xiang Wang  and Ryan Tibshirani. A higher-order

kolmogorov-smirnov test. In International Conference on Artiﬁcial Intelligence and Statistics  2019.

11

[44] Benjamin Sanchez-Lengeling  Carlos Outeiral  Gabriel L Guimaraes  and Alan Aspuru-Guzik. Optimizing
distributions over molecular space. an objective-reinforced generative adversarial network for inverse-
design chemistry (organic). ChemrXiv Preprint  2017.

[45] Shashank Singh and Barnabás Póczos. Minimax distribution estimation in Wasserstein distance. arXiv

preprint arXiv:1802.08855  2018.

[46] Shashank Singh  Ananya Uppal  Boyue Li  Chun-Liang Li  Manzil Zaheer  and Barnabas Poc-
In Advances in Neural Informa-
zos. Nonparametric density estimation under adversarial losses.
tion Processing Systems 31  pages 10246–10257  2018. URL http://papers.nips.cc/paper/
8225-nonparametric-density-estimation-under-adversarial-losses.pdf.

[47] Casper Kaae Sønderby  Jose Caballero  Lucas Theis  Wenzhe Shi  and Ferenc Huszár. Amortised map

inference for image super-resolution. arXiv preprint arXiv:1610.04490  2016.

[48] Bharath K. Sriperumbudur  Kenji Fukumizu  Arthur Gretton  Bernhard Schölkopf  and Gert RG Lanckriet.
Non-parametric estimation of integral probability metrics. In Information Theory Proceedings (ISIT)  2010
IEEE International Symposium on  pages 1428–1432. IEEE  2010.

[49] Taiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces:

optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033  2018.

[50] Terence Tao. A type diagram for function spaces.

besov-spaces/  2011.

https://terrytao.wordpress.com/tag/

[51] Alexandre B Tsybakov. Introduction to nonparametric estimation. Revised and extended from the 2004
French original. Translated by Vladimir Zaiats. Springer Series in Statistics. Springer  New York  2009.

[52] Cédric Villani. Optimal transport: old and new  volume 338. Springer Science & Business Media  2008.

[53] Larry Wassermann. All of nonparametric statistics. New York  2006.

[54] Jonathan Weed and Francis Bach. Sharp asymptotic and ﬁnite-sample rates of convergence of empirical

measures in Wasserstein distance. arXiv preprint arXiv:1707.00087  2017.

[55] Jonathan Weed and Quentin Berthet. Estimation of smooth densities in wasserstein distance. arXiv preprint

arXiv:1902.01778  2019.

[56] Zhen Yang  Wei Chen  Feng Wang  and Bo Xu. Improving neural machine translation with conditional

sequence generative adversarial nets. arXiv preprint arXiv:1703.04887  2017.

[57] Kosaku Yosida. Functional analysis. reprint of the sixth (1980) edition. classics in mathematics. Springer-

Verlag  Berlin  11:14  1995.

[58] Werner Zellinger  Bernhard A Moser  Thomas Grubinger  Edwin Lughofer  Thomas Natschläger  and
Susanne Saminger-Platz. Robust unsupervised domain adaptation for neural networks via moment
alignment. Information Sciences  2019.

[59] Han Zhang  Tao Xu  Hongsheng Li  Shaoting Zhang  Xiaogang Wang  Xiaolei Huang  and Dimitris N
Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.
In Proceedings of the IEEE International Conference on Computer Vision  pages 5907–5915  2017.

12

,Ananya Uppal
Shashank Singh
Barnabas Poczos