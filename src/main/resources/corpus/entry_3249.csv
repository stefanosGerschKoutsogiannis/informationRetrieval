2018,Local Differential Privacy for Evolving Data,There are now several large scale deployments of differential privacy used to collect statistical information about users. However  these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result  these systems do not provide meaningful privacy guarantees over long time scales. Moreover  existing techniques to mitigate this effect do not apply in the ``local model'' of differential privacy that these systems use.

In this paper  we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time  with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups  and at every time period each user draws a single bit from a common (but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation.,Local Differential Privacy for Evolving Data

Computer and Information Science

Computer and Information Science

Computer and Information Sciences

Computer and Information Science

Matthew Joseph

University of Pennsylvania
majos@cis.upenn.edu

Jonathan Ullman

Northeastern University
jullman@ccs.neu.edu

Aaron Roth

University of Pennsylvania
aaroth@cis.upenn.edu

Bo Waggoner

University of Pennsylvania
bowaggoner@gmail.com

Abstract

There are now several large scale deployments of differential privacy used to
collect statistical information about users. However  these deployments periodically
recollect the data and recompute the statistics using algorithms designed for a single
use. As a result  these systems do not provide meaningful privacy guarantees over
long time scales. Moreover  existing techniques to mitigate this effect do not apply
in the “local model” of differential privacy that these systems use.
In this paper  we introduce a new technique for local differential privacy that makes
it possible to maintain up-to-date statistics over time  with privacy guarantees that
degrade only in the number of changes in the underlying distribution rather than
the number of collection periods. We use our technique for tracking a changing
statistic in the setting where users are partitioned into an unknown collection of
groups  and at every time period each user draws a single bit from a common (but
changing) group-speciﬁc distribution. We also provide an application to frequency
and heavy-hitter estimation.

1

Introduction

After over a decade of research  differential privacy [12] is moving from theory to practice  with
notable deployments by Google [15  6]  Apple [2]  Microsoft [10]  and the U.S. Census Bureau [1].
These deployments have revealed gaps between existing theory and the needs of practitioners. For
example  the bulk of the differential privacy literature has focused on the central model  in which user
data is collected by a trusted aggregator who performs and publishes the results of a differentially
private computation [11]. However  Google  Apple  and Microsoft have instead chosen to operate in
the local model [15  6  2  10]  where users individually randomize their data on their own devices and
send it to a potentially untrusted aggregator for analysis [18]. In addition  the academic literature has
largely focused on algorithms for performing one-time computations  like estimating many statistical
quantities [7  22  16] or training a classiﬁer [18  9  4]. Industrial applications  however have focused
on tracking statistics about a user population  like the set of most frequently used emojis or words [2].
These statistics evolve over time and so must be re-computed periodically.
Together  the two problems of periodically recomputing a population statistic and operating in the local
model pose a challenge. Naïvely repeating a differentially private computation causes the privacy loss
to degrade as the square root of the number of recomputations  quickly leading to enormous values of
. This naïve strategy is what is used in practice [15  6  2]. As a result  Tang et al. [23] discovered that
the privacy parameters guaranteed by Apple’s implementation of differentially private data collection

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

can become unreasonably large even in relatively short time periods.1 Published research on Google
and Microsoft’s deployments suggests that they encounter similar issues [15  6  10].
On inspection the naïve strategy of regular statistical updates seems wasteful as aggregate population
statistics don’t change very frequently—we expect that the most frequently visited website today will
typically be the same as it was yesterday. However  population statistics do eventually change  and if
we only recompute them infrequently  then we can be too slow to notice these changes.
The central model of differential privacy allows for an elegant solution to this problem. For large
classes of statistics  we can use the sparse vector technique [13  22  16  11] to repeatedly perform
computations on a dataset such that the error required for a ﬁxed privacy level grows not with
the number of recomputations  but with the number of times the computation’s outcome changes
signiﬁcantly. For statistics that are relatively stable over time  this technique dramatically reduces the
overall error. Unfortunately  the sparse vector technique provably has no local analogue [18  24].
In this paper we present a technique that makes it possible to repeatedly recompute a statistic with
error that decays with the number of times the statistic changes signiﬁcantly  rather than the number
of times we recompute the current value of the statistic  all while satisfying local differential privacy.
This technique allows for tracking of evolving local data in a way that makes it possible to quickly
detect changes  at modest cost  so long as those changes are relatively infrequent. Our approach
guarantees privacy under any conditions  and obtains good accuracy by leveraging three assumptions:
(1) each user’s data comes from one of m evolving distributions; (2)  these distributions change
relatively infrequently; and (3) users collect a certain amount of data during each reporting period 
which we term an epoch. By varying the lengths of the epochs (for example  collecting reports hourly 
daily  or weekly)  we can trade off more frequent reports versus improved privacy and accuracy.

1.1 Our Results and Techniques

Although our techniques are rather general  we ﬁrst focus our attention on the problem of privately
estimating the average of bits  with one bit held by each user. This simple problem is widely applicable
because most algorithms in the local model have the following structure: on each individual’s device 
data records are translated into a short bit vector using sketching or hashing techniques. The bits
in this vector are perturbed to ensure privacy using a technique called randomized response  and
the perturbed vector is then sent to a server for analysis. The server collects the perturbed vectors 
averages them  and produces a data structure encoding some interesting statistical information about
the users as a whole. Thus many algorithms (for example  those based on statistical queries) can be
implemented using just the simple primitive of estimating the average of bits.
We analyze our algorithm in the following probabilistic model (see Section 3 for a formal description).
The population of n users has an unknown partition into subgroups  each of which has size at least
L  time proceeds in rounds  and in each round each user samples a private bit independently from
their subgroup-speciﬁc distribution. The private data for each user consists of the vector of bits
sampled across rounds  and our goal is to track the total population mean over time. We require
that the estimate be private  and ask for the strong (and widely known) notion of local differential
privacy—for every user  no matter how other users or the server behave  the distribution of the
messages sent by that user should not depend signiﬁcantly on that user’s private data.
To circumvent the limits of local differential privacy  we consider a slightly relaxed estimation
guarantee. Speciﬁcally  we batch the rounds into T epochs  each consisting of (cid:96) rounds  and aim in
each epoch t to estimate pt  the population-wide mean across the subgroups and rounds of epoch t.
Thus  any sufﬁciently large changes in this mean will be identiﬁed after the current epoch completes 
which we think of as introducing a small “delay".
Our main result is an algorithm that takes data generated according to our model  guarantees a ﬁxed
level of local privacy ε that grows (up to a certain point) with the number of distributional changes
rather than the number of epochs  and guarantees that the estimates released at the end of each epoch

are accurate up to error that scales sublinearly in 1~(cid:96) and only polylogarithmically with the total

number of epochs T . Our method improves over the naïve solution of simply recomputing the statistic
every epoch – which would lead to either privacy parameter or error that scales linearly with the
1Although the value of  that Apple guarantees over the course of say  a week  is not meaningful on its own 
Apple does take additional heuristic steps (as does Google) that make it difﬁcult to combine user data from
multiple data collections [2  15  6]. Thus  they may still provide a strong  if heuristic  privacy guarantee.

2

number of epochs—and offers a quantiﬁable way to reason about the interaction of collection times 
reporting frequency  and accuracy. We note that one can alternatively phrase our algorithm so as to
have a ﬁxed error guarantee  and a privacy cost that scales dynamically with the number of times the
distribution changes2.
Theorem 1.1 (Protocol for Bernoulli Means  Informal Version of Theorem 4.3). In the above
model  there is an ε-differentially private local protocol that achieves the following guarantee: with

probability at least 1−δ  while the total number of elapsed epochs t where some subgroup distribution
has changed is fewer than ε⋅ min
  the protocol outputs estimates ˜pt where
ln(nT~δ)

n ln(mT~δ)   ln(T)
L
ïïln(T)
˜pt− pt= O

ï

n
(cid:96)

(cid:96)

√

where L is the smallest subgroup size  n is the number of users  (cid:96) is the chosen epoch length  and T
is the resulting number of epochs.

To interpret the theorem  consider the setting where there is only one subgroup and L= n. Then
to achieve error α we need  ignoring log factors  (cid:96)≥ 1~α2 and that fewer than εα

n changes have
occured. We emphasize that our algorithm satisﬁes ε-differential privacy for all inputs without a
distributional assumption—only accuracy relies on distributional assumptions.
Finally  we demonstrate the versatility of our method as a basic building block in the design of
locally differentially private algorithms for evolving data by applying it to the well-known heavy
hitters problem. We do so by implementing a protocol due to [3] on top of our simple primitive.
This adapted protocol enables us to efﬁciently track the evolution of histograms rather than single
bits. Given a setting in which each user in each round independently draws an object from a
discrete distribution over a dictionary of d elements  we demonstrate how to maintain a frequency
oracle (a computationally efﬁcient representation of a histogram) for that dictionary with accuracy
guarantees that degrade with the number of times the distribution over the dictionary changes  and
only polylogarithmically with the number of rounds. We summarize this result below.
Theorem 1.2 (Protocol for Heavy-Hitters  Informal Version of Theorem 5.2). In the above model 
there is an ε-differentially private local protocol that achieves the following guarantee: with proba-

bility at least 1− δ  while the total number of elapsed epochs t where some subgroup distribution
has changed is fewer than ε⋅ min
 the protocol outputs estimate
L
oracles ˆf t such that for all v∈[d]
ln(dnT~δ)
 ˆf t(v)−P t(v)= O
where n is the number of users  L is the smallest subgroup size P t is the mean distribution over

n ln(mT~δ)   ln(T)
ïïln(T)
ln(nT~δ)

+

n ln(nT~δ)

ï .

dictionary elements in epoch t  d is the number of dictionary elements  (cid:96) is the chosen epoch length 
and T is the resulting number of epochs.

(cid:96)

(cid:96)

n

1.2 Related Work

The problem of privacy loss for persistent local statistics has been recognized since at least the original
work of Erlingsson et al. [15] on RAPPOR (the ﬁrst large-scale deployment of differential privacy
in the local model). Erlingsson et al. [15] offers a heuristic memoization technique that impedes a
certain straightforward attack but does not prevent the differential privacy loss from accumulating
linearly in the number of times the protocol is run. Ding et al. [10] give a formal analysis of a similar
memoization technique  but the resulting guarantee is not differential privacy—instead it is a privacy
guarantee that depends on the behavior of other users  and may offer no protection to users with
idiosyncratic device usage. In contrast  we give a worst-case differential privacy guarantee.
Our goal of maintaining a persistent statistical estimate is similar in spirit to the model of privacy
under continual observation Dwork et al. [14]. The canonical problem for differential privacy under

2We can achieve a dynamic  data-dependent privacy guarantee using the notion of ex-post differential privacy

[19]  for example by using a so-called privacy odometer [21].

3

continual observation is to maintain a running count of a stream of bits. However  the problem we
study is quite different. In the continual observation model  new users are arriving  while existing
users’ data does not change. In our model each user receives new information in each round. (Also 
we work in the local model  which has not been the focus of the work on continual observation.)
The local model was originally introduced by Kasiviswanathan et al. [18]  and the canonical algorith-
mic task performed in this model has become frequency estimation (and heavy hitters estimation).
This problem has been studied in a series of theoretical [17  3  5  8  2] and practical works [15  6  2].

2 Local Differential Privacy

We require that our algorithms satisfy local differential privacy. Informally  differential privacy is a
property of an algorithm A  and states that the distribution of the output of A is insensitive to changes

in one individual user’s input. Formally  for every pair of inputs x  x′ differing on at most one user’s
data  and every set of possible outputs Z  P[A(x)∈ Z]≤ eε⋅ P[A(x′)∈ Z]. A locally differentially
dent of all other messages. Non-interactive protocols can thus be written as A(x1  . . .   xn) =
f(A1(x1)  . . .   An(xn)) for some function f  where each algorithm Ai satisﬁes ε-differential pri-

private algorithm is one in which each user i applies a private algorithm Ai only to their data.
Most local protocols are non-interactive: each user i sends a single message that is indepen-

vacy. Our model requires an interactive protocol: each user i sends several messages over time  and
these may depend on the messages sent by other users. This necessitates a slightly more complex
formalism.
We consider interactive protocols among the n users and an additional center. Each user runs an
algorithm Ai (possibly taking a private input xi) and the central party runs an algorithm C. We let

the random variable tr(A1  . . .   An  C) denote the transcript containing all the messages sent by all
of the parties. For a given party i and a set of algorithms A′−i  C′  we let tri(xi; A′−i  C′) denote the
messages sent by user i in the transcript tr(Ai(xi)  A′−i  C′). As a shorthand we will write tri(xi) 
since A′−i  C′ will be clear from context. We say that the protocol is locally differentially private if
the function tri(xi) is differentially private for every user i and every (possibly malicious) A′−i  C′.
Deﬁnition 2.1. An interactive protocol(A1  . . .   An  C) satisﬁes ε-local differential privacy if for
every user i  every pair of inputs xi  x′
i for user i  and every set of algorithms A′−i  C′  the resulting
algorithm tri(xi)= tri(Ai(xi)  A′−i  C′) is ε-differentially private. That is  for every set of possible
i)∈ Z].
outputs Z  P[tri(xi)∈ Z]≤ eε⋅ P[tri(x′

3 Overview: The THRESH Algorithm

Here we present our main algorithm  THRESH. The algorithmic framework is quite general  but for
this high level overview we focus on the simplest setting where the data is Bernoulli. In Section 4 we
formally present the algorithm for the Bernoulli case and analyze the algorithm to prove Theorem 1.1.
To explain the algorithm we ﬁrst recall the distributional model. There are n users  each of whom

belongs to a subgroup Sj for some j∈[m]; denote user i’s subgroup by g(i). There are R= T (cid:96)
1+ . . .+Smµr
m).
(S1µr
For each epoch t  we use pt to denote the average of the Bernoulli means during epoch t  pt =
(cid:96)∑r∈Et µr. After every epoch t  our protocol outputs ˜pt such thatpt− ˜pt is small.

rounds divided into T epochs of length (cid:96)  denoted E1  . . .   ET . In each round r  each user i receives a
private bit xr

g(i)). We deﬁne the population-wide mean by µr= 1

n

i ∼ Ber(µr

1

The goal of THRESH is to maintain some public global estimate ˜pt of pt. After any epoch t  we can
update this global estimate ˜pt using randomized response: each user submits some differentially
private estimate of the mean of their data  and the center aggregates these responses to obtain ˜pt.
The main idea of THRESH is therefore to update the global estimate only when it might become
sufﬁciently inaccurate  and thus take advantage of the possibly small number of changes in the
underlying statistic pt. The challenge is to privately identify when to update the global estimate.

The Voting Protocol. We identify these “update needed” epochs through a voting protocol. Users
will examine their data and privately publish a vote for whether they believe the global estimate
needs to be updated. If enough users vote to update the global estimate  we do so (using randomized

4

response). The challenge for the voting protocol is that users must use randomization in their voting
process  to keep their data private  so we can only detect when a large number of users vote to update.
First  we describe a naïve voting protocol. In each epoch t  each user i computes a binary vote at
i.

This vote is 1 if the user concludes from their own samples that the global estimate ˜pt−1 is inaccurate 

and 0 otherwise. Each user casts a noisy vote using randomized response accordingly  and if the sum
of the noisy votes is large enough then a global update occurs.
The problem with this protocol is that small changes in the underlying mean pt may cause some users
to vote 1 and others to vote 0  and this might continue for an arbitrarily long time without inducing
a global update. As a result  each voter “wastes" privacy in every epoch  which is what we wanted
to avoid. We resolve this issue by having voters also estimate their conﬁdence that a global update
needs to occur  and vote proportionally. As a result  voters who have high conﬁdence will lose more
privacy per epoch (but the need for a global update will be detected quickly)  while voters with low
conﬁdence will lose privacy more slowly (but may end up voting for many rounds).

In more detail  each user i decides their conﬁdence level by comparingˆpt− ˆpf(t)

—the difference

between the local average of their data in the current epoch and their local average the last time a
global update occurred—to a small set of discrete thresholds. Users with the highest conﬁdence will
vote in every epoch  whereas users with lower conﬁdence will only vote in a small subset of the
epochs. We construct these thresholds and subsets so that in expectation no user votes in more than a
constant number of epochs before a global update occurs  and the amount of privacy each user loses
from voting will not grow with the number of epochs required before an update occurs.

i

4 THRESH: The Bernoulli Case

4.1 The THRESH Algorithm (Bernoulli Case)

We now present pseudocode for the algorithm THRESH  including both the general framework as well
as the speciﬁc voting and randomized response procedures. We emphasize that the algorithm only
touches user data through the subroutines VOTE  and EST  each of which accesses data from a single
user in at most two epochs. Thus  it is an online local protocol in which user i’s response in epoch t

viewable to all users). THRESH uses carefully chosen thresholds τb= 2(b+ 1)
depends only on user i’s data from at most two epochs t and t′ (and the global information that is
ln(12nT~δ)~2(cid:96) for
b=−1  0  . . .  ælog(T)æ to discretize the conﬁdence of each user; see Section 4.2 for details on this

choice.
We begin with a privacy guarantee for THRESH. Our proof uses the standard analysis of the privacy
properties of randomized response  combined with the fact that users have a cap on the number of
updates that prevents the privacy loss from accumulating. We remark that our privacy proof does not
depend on distributional assumptions  which are only used for the proof of accuracy. We sketch a
proof here. A full proof appears in the Supplement.
Theorem 4.1. The protocol THRESH satisﬁes ε-local differential privacy (Deﬁnition 2.1)

Proof Sketch: Naïvely applying composition would yield a privacy parameter that scales with T .
Instead  we will rely on our deﬁned privacy “caps" cV
that limit the number of truthful votes

and estimates each user sends. Intuitively  each user sends at most O( ε
bound the privacy “cost" of each of these O( ε

) messages that depend
) elements of a user’s transcript coming from a


on their private data  and the rest are sampled independently of their private data. Thus  we need only

different distribution and bound the sum of the costs by ε.

+ ε

+ ε

i and cE
i

a

a

b

4.2 Accuracy Guarantee



+ √

Our accuracy theorem needs the following assumption on L  the size of the smallest subgroup  to
guarantee that a global update occurs whenever any subgroup has all of its member users vote “yes".

Assumption 4.2. L> 3√
of rounds R  privacy parameter ε  and chosen epoch length (cid:96) and number of epochs T= R~(cid:96)  with

This brings us to our accuracy theorem  followed by a proof sketch (see Supplement for full details).
Theorem 4.3. Given number of users n  number of subgroups m  smallest subgroup size L  number

n ln(12mT~δ).

32
ε

2

b

5

Algorithm 1 Global Algorithm: THRESH
Require: number of users n  number of epochs T   minimum subgroup size L  number of subgroups

2

ln(12T~δ)~2n

1   . . .   cV

1   . . .   cE

User i publishes at

m  epoch length (cid:96)  privacy parameter ε  failure parameter δ

2: Initialize vote privacy counters cV
3: Initialize estimate privacy counters cE

1: Initialize global estimate ˜p0←−1
n ← 0  . . .   0
n ← 0  . . .   0
4: Initialize vote noise level a← 4

2n ln(12mT~δ)

n ln(12mT~δ)
L− 3√
5: Initialize estimate noise level b←

2 ln(12T~δ)~2n
ln(12nT~δ)~2(cid:96)−
log(T)
6: for each epoch t∈[T] do
for each user i∈[n] do
i← VOTE(i  t)
+

GlobalUpdatet← 1
i> 1
n∑n
i=1 at
ea+1
f(t)← t
for each i∈[n] do
i← EST(i  t)
Aggregate user estimates into global estimate: ˜pt← 1
n∑n
i=1
f(t)← f(t− 1)
for each i∈[n] do
˜pt← ˜pt−1

i← Ber( 1
eb+1

if GlobalUpdatet then

ln(10T~δ)

User i publishes ˜pt

end for

)

2n

7:
8:
9:
10:

end for

else

11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end for

i(eb+1)−1
eb−1

˜pt

User i publishes ˜pt

end for

end if
Analyst publishes ˜pt

i

Algorithm 2 Local Subroutine: VOTE
Require: user i  epoch t
1: Compute local estimate ˆpt

i← 1
(cid:96)∑r∈Et xr
2: b∗← highest b such thatˆpt
i− ˆpf(t)
> τb
i←(cV
i < ε~4 and 2ælog Tæ−b
i ← cV
i + a
)
i← Ber( ea
i then
ea+1
i← Ber( 1
)
ea+1

3: VoteYest
4: if VoteYest
cV
5:
at
6:
7: else
at
8:
9: end if
10: Output at
i

∗

i

divides t)

i < ε~4}
i←{cE
i + b
i ← cE
)
i← Ber( 1+ ˆpt
i then
i(eb−1)
eb+1
i← Ber( 1
)
eb+1

Algorithm 3 Local Subroutine: EST
Require: user i  epoch t
1: SendEstimatet
2: if SendEstimatet
cE
3:
˜pt
4:
5: else
˜pt
6:
7: end if
8: Output ˜pt
i

6

probability at least 1− δ  in every epoch t∈[T] such that fewer than
log(T)
ln(12nT~δ)

ïï
1√

2n ln(12mT~δ)− 1 
⋅ min
˜pt− pt≤ 4(ælog(T)æ+ 2)

changes have occurred in epochs 1  2  . . .   t  THRESH outputs ˜pt such that

ε
4

L

8

2

.

− 1ï

n
(cid:96)

2(cid:96)

Proof Sketch: We begin by proving correctness of the voting process. We show that (1) if every user
decides that their subgroup distribution has not changed then a global update does not occur  (2) if
every user in some subgroup decides that a change has occurred  then a global update occurs  and (3)
for each user i the individual user estimates driving these voting decisions are themselves accurate

g(i). Finally  we prove that if every user decides that a

ln(nT~δ)~(cid:96)) of the true µt

to within t(cid:96)= O(

i by at most 1.

change has occurred  then a global update occurs that produces a global estimate ˜pt that is within t(cid:96)
of the true pt.
To reason about how distribution changes across multiple epochs affect THRESH  we use the preceding
results to show that the number of global updates never exceeds the number of distribution changes.
A more granular guarantee then bounds the number of changes any user detects—and the number
of times they vote accordingly—as a function of the number of distribution changes. These results
enable us to show that each change increases a user’s vote privacy cap cV
i by at most 2 and estimate
privacy cap cE
Finally  recall that THRESH has each user i compare their current local estimate ˆpt

increasing the likelihood of a “yes" vote. This implies that if every user in some subgroup computes
a local estimate ˆpt
vote and a global update occurs  bringing with it the global accuracy guarantee proven above. In turn 


i− ˆpf(t)
  to decide how to vote  with higher thresholds forˆpt
estimate in the last global update  ˆpf(t)
 exceeds the highest threshold  then every user sends a “yes"
i− ˆpf(t)

we conclude that˜pt− pt never exceeds the highest threshold  and our accuracy result follows.
of size≤ c  paying and additive c term in the accuracy. Second  the accuracy’s dependence on (cid:96)
offers guidance for its selection: roughly  for desired accuracy α  one should set (cid:96)= 1~α2. Finally  in

We conclude this section with a few remarks about THRESH. First  while the provided guarantee
depends on the number of changes of any size  one can easily modify THRESH to be robust to changes

i such thatˆpt

i to their local

practice one may want to periodically assess how many users have exhausted their privacy budgets 
which we can achieve by extending the voting protocol to estimate the fraction of “live” users. We
primarily view this as an implementation detail outside of the scope of the exact problem we study.

i

i

i

5 An Application to Heavy Hitters

own dictionary value v ∈ D (e.g.

We now use the methods developed above to obtain similar guarantees for a common problem
in local differential privacy known as heavy hitters.
In this problem each of n users has their
their homepage)  and an aggregator wants to learn the most
frequently held dictionary values (e.g. the most common homepages)  known as “heavy hitters" 
while satisfying local differential privacy for each user. The heavy hitters problem has attracted
signiﬁcant attention [20  17  5  8]. Here  we show how our techniques combine with an approach
of Bassily and Smith [3] to obtain the ﬁrst guarantees for heavy hitters on evolving data. We note that
our focus on this approach is primarily for expositional clarity; our techniques should apply just as
well to other variants  which can lead to more efﬁcient algorithms.

5.1 Setting Overview

As in the simpler Bernoulli case  we divide time into (cid:96)⋅ T rounds and T epochs. Here  in each round
dictionaryD and trackP 1  . . .  P T   the weighted average dictionary distribution in each epoch. We
g(i) over the d values in
will require the same Assumption 4.2 as in the Bernoulli case  and we also suppose that dâ n  a

i from a subgroup-speciﬁc distributionP r

r each user i draws a sample vr

common parameter regime for this problem.

7

In the Bernoulli case users could reason about the evolution of µt

each epoch. Since it is reasonable to assume dâ (cid:96)  this is no longer possible in our new setting—P t

j directly from their own (cid:96) samples in
j
is too large an object to estimate from (cid:96) samples. However  we can instead adopt a common approach
in heavy hitters estimation and examine a “smaller" object using a hash on dictionary samples. We
will therefore have users reason about the distribution pt
j induces  which is a
much smaller joint distribution of m (transformed) Bernoulli distributions. Our hope is that users can
reliably “detect changes” by analyzing pt
j  and the feasibility of this method leans crucially on the
properties of the hash in question.

j over hashes thatP t

5.2 Details and Privacy Guarantee

i and then hashes it into Φˆpt

First we recall the details of the one-shot protocol from Bassily and Smith [3]. In their protocol  each

center aggregates these randomized values into a single ¯z which induces a frequency oracle.
We will modify this to produce a protocol HEAVYTHRESH in the vein of THRESH. In each epoch t
each user i computes an estimated histogram ˆpt
(we assume the existence of a subroutine GenProj for generating Φ). Each user votes on whether or
i to their estimate during the most recent update 
not a global update has occurred by comparing Φˆpt
  in HEAVYVOTE. Next  HEAVYTHRESH aggregates these votes to determine whether or
not a global update will occur. Depending on the result  each user then calls their own estimation

user starts with a dictionary value v∈[d] with an associated basis vector ev∈ Rd. The user hashes
this to a smaller vector h∈ Rw using a (population-wide) Φ  a w× d Johnson-Lindenstrauss matrix
where wâ d. The user then passes this hash ˆzt
i = Φev to their own local randomizerR  and the
i∈ Rw  where w= 20n
Φˆpf(t)
subroutine HEAVYEST and outputs a randomized response usingR accordingly. If a global update
HEAVYTHRESH publishes ˜yt−1. In either case  HEAVYTHRESH publishes(Φ  ˜yt) as well. This ﬁnal
output is a frequency oracle  which for any v∈[d] offers an estimateΦev  ˜yt ofP t(v).
HEAVYTHRESH will use the following thresholds with τb = 2(b+ 1)
2 ln(16wnT~δ)~w(cid:96) for
b=−1  0  . . .  ælog(T)æ. See Section 5.3 for details on this choice. Fortunately  the bulk of our
only additional analysis needed is for the estimation randomizerR (see Supplement). Using the
privacy ofR  privacy for HEAVYTHRESH follows by the same proof as for the Bernoulli case.

analysis uses tools already developed either in Section 4 or Bassily and Smith [3]. Our privacy
guarantee is almost immediate: since HEAVYTHRESH shares its voting protocols with THRESH  the

occurs  HEAVYTHRESH aggregates these responses into a new published global hash ˜yt; if not 

i

Theorem 5.1. HEAVYTHRESH is -local differentially private.

5.3 Accuracy Guarantee

result and its proof sketch follow  with details and full pseudocode in the Supplement.

As above  an accuracy guarantee for HEAVYTHRESH unfolds along similar lines as that for THRESH 
with additional recourse to results from Bassily and Smith [3]. We again require Assumption 4.2

and also assume d= 2o(n2~(cid:96)) (a weak assumption made primarily for neatness in Theorem 1.2). Our
Theorem 5.2. With probability at least 1− δ  in every epoch t∈[T] such that fewer than
−
− 2 ln(320nT~δ)
ïïï
ln(16dT~δ)
n ln(320n2T~δ)
⋅ min


ln(320nT~δ)1+ 20√
¿``(cid:192) ln( 16ndT
2 ln(320n2T~δ)

log(T)
2n ln(12mT~δ)− 1 
 ˆf t(v)−P t(v)< 4(log(T)+ 2)

changes have occurred in epochs 1  2  . . .   t 

ïïïï



+

)

.

10(cid:96)

10

5
n

ε
4

8

L

n

δ
n

(cid:96)

Proof Sketch: Our proof is similar to that of Theorem 4.3 and proceeds by proving analogous versions
of the same lemmas  with users checking for changes in the subgroup distribution over observed
hashes rather than observed bits. This leads to one new wrinkle in our argument: once we show
that the globally estimated hash is close to the true hash  we must translate from closeness of hashes
to closeness of the distributions they induce . The rest of the proof  which uses guarantees of user
estimate accuracy to 1. guarantee that sufﬁciently large changes cause global updates and 2. each
change incurs a bounded privacy loss  largely follows that of Theorem 4.3.



8

References
[1] John M. Abowd. The challenge of scientiﬁc reproducibility and privacy protection for statistical

agencies. Census Scientiﬁc Advisory Committee  2016.

[2] Differential Privacy Team Apple. Learning with privacy at scale. Technical report  Apple  2017.

[3] Raef Bassily and Adam Smith. Local  private  efﬁcient protocols for succinct histograms.
In Proceedings of the forty-seventh annual ACM symposium on Theory of computing  pages
127–135. ACM  2015.

[4] Raef Bassily  Adam Smith  and Abhradeep Thakurta. Differentially private empirical risk
minimization: Efﬁcient algorithms and tight error bounds. arXiv preprint arXiv:1405.7085 
2014.

[5] Raef Bassily  Uri Stemmer  and Abhradeep Guha Thakurta. Practical locally private heavy

hitters. In Advances in Neural Information Processing Systems  pages 2285–2293  2017.

[6] Andrea Bittau  Úlfar Erlingsson  Petros Maniatis  Ilya Mironov  Ananth Raghunathan  David
Lie  Mitch Rudominer  Usharsee Kode  Julien Tinnes  and Bernhard Seefeld. Prochlo: Strong
privacy for analytics in the crowd. arXiv preprint arXiv:1710.00901  2017.

[7] Avrim Blum  Katrina Ligett  and Aaron Roth. A learning theory approach to noninteractive

database privacy. Journal of the ACM (JACM)  60(2):12  2013.

[8] Mark Bun  Jelani Nelson  and Uri Stemmer. Heavy hitters and the structure of local privacy.

arXiv preprint arXiv:1711.04740  2017.

[9] Kamalika Chaudhuri  Claire Monteleoni  and Anand D Sarwate. Differentially private empirical

risk minimization. Journal of Machine Learning Research  12(Mar):1069–1109  2011.

[10] Bolin Ding  Janardhan Kulkarni  and Sergey Yekhanin. Collecting telemetry data privately. In

Proceedings of Advances in Neural Information Processing Systems 30 (NIPS 2017)  2017.

[11] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Founda-

tions and Trends® in Theoretical Computer Science  9(3–4):211–407  2014.

[12] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography Conference  pages 265–284.
Springer  2006.

[13] Cynthia Dwork  Moni Naor  Omer Reingold  Guy N Rothblum  and Salil Vadhan. On the
complexity of differentially private data release: efﬁcient algorithms and hardness results. In
Proceedings of the forty-ﬁrst annual ACM symposium on Theory of computing  pages 381–390.
ACM  2009.

[14] Cynthia Dwork  Moni Naor  Toniann Pitassi  and Guy N Rothblum. Differential privacy under
In Proceedings of the forty-second ACM symposium on Theory of

continual observation.
computing  pages 715–724. ACM  2010.

[15] Úlfar Erlingsson  Vasyl Pihur  and Aleksandra Korolova. Rappor: Randomized aggregatable
privacy-preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC conference on
computer and communications security  pages 1054–1067. ACM  2014.

[16] Moritz Hardt and Guy N Rothblum. A multiplicative weights mechanism for privacy-preserving
data analysis. In Foundations of Computer Science (FOCS)  2010 51st Annual IEEE Symposium
on  pages 61–70. IEEE  2010.

[17] Justin Hsu  Sanjeev Khanna  and Aaron Roth. Distributed private heavy hitters. In International

Colloquium on Automata  Languages  and Programming  pages 461–472. Springer  2012.

[18] Shiva Prasad Kasiviswanathan  Homin K Lee  Kobbi Nissim  Sofya Raskhodnikova  and Adam
In Proceedings of the 54th Annual Symposium on

Smith. What can we learn privately?
Foundations of Computer Science  pages 531–540  2008.

9

[19] Katrina Ligett  Seth Neel  Aaron Roth  Bo Waggoner  and Steven Z Wu. Accuracy ﬁrst:
Selecting a differential privacy level for accuracy constrained erm. In Advances in Neural
Information Processing Systems  pages 2563–2573  2017.

[20] Nina Mishra and Mark Sandler. Privacy via pseudorandom sketches. In Proceedings of the
twenty-ﬁfth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems 
pages 143–152. ACM  2006.

[21] Ryan M Rogers  Aaron Roth  Jonathan Ullman  and Salil Vadhan. Privacy odometers and ﬁlters:
Pay-as-you-go composition. In Advances in Neural Information Processing Systems  pages
1921–1929  2016.

[22] Aaron Roth and Tim Roughgarden. Interactive privacy via the median mechanism. In Pro-
ceedings of the forty-second ACM symposium on Theory of computing  pages 765–774. ACM 
2010.

[23] Jun Tang  Aleksandra Korolova  Xiaolong Bai  Xueqiang Wang  and Xiaofeng Wang. Pri-
vacy loss in apple’s implementation of differential privacy on macos 10.12. arXiv preprint
arXiv:1709.02753  2017.

[24] Jonathan Ullman. Tight lower bounds for locally differentially private selection. Manuscript 

2018.

10

,Peter Kairouz
Sewoong Oh
Pramod Viswanath
Matthew Joseph
Aaron Roth
Jonathan Ullman
Bo Waggoner