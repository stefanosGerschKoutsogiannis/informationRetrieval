2008,Estimating vector fields using sparse basis field expansions,We introduce a novel framework for estimating vector fields using sparse basis field expansions (S-FLEX). The notion of basis fields  which are an extension of scalar basis functions  arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector field  we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that significantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art.,Estimating vector ﬁelds using
sparse basis ﬁeld expansions

Stefan Haufe1  2  * Vadim V. Nikulin3  4 Andreas Ziehe1  2 Klaus-Robert M¨uller1  2  4

Guido Nolte2

1TU Berlin  Dept. of Computer Science  Machine Learning Laboratory  Berlin  Germany

2Fraunhofer Institute FIRST (IDA)  Berlin  Germany

3Charit´e University Medicine  Dept. of Neurology  Campus Benjamin Franklin  Berlin  Germany

4Bernstein Center for Computational Neuroscience  Berlin  Germany

* haufe@cs.tu-berlin.de

Abstract

We introduce a novel framework for estimating vector ﬁelds using sparse basis
ﬁeld expansions (S-FLEX). The notion of basis ﬁelds  which are an extension
of scalar basis functions  arises naturally in our framework from a rotational in-
variance requirement. We consider a regression setting as well as inverse prob-
lems. All variants discussed lead to second-order cone programming formula-
tions. While our framework is generally applicable to any type of vector ﬁeld  we
focus in this paper on applying it to solving the EEG/MEG inverse problem. It
is shown that signiﬁcantly more precise and neurophysiologically more plausible
location and shape estimates of cerebral current sources from EEG/MEG measure-
ments become possible with our method when comparing to the state-of-the-art.

1 Introduction

Current machine learning is frequently concerned with the estimation of functions with multivariate
output. While in many cases the outputs can be treated as mere collections of scalars (e.g. different
color channels in image processing)  in some contexts there might be a deeper interpretation of them
as spatial vectors with a direction and a magnitude. Such “truly” vectorial functions are called vector
ﬁelds and become manifest for example in optical ﬂow ﬁelds  electromagnetic ﬁelds and wind ﬁelds
in meteorology. Vector ﬁeld estimators have to take into account that the numerical representation of
a vector depends on the coordinate system it is measured in. That is  the estimate should be invariant
with respect to a rotation of the coordinate system.
Let v : RP (cid:55)→ RQ be a vector ﬁeld. Mathematically speaking  we are seeking to approximate v
by a ﬁeld ˆv using empirical measurements. Here we consider two types of measurements. The ﬁrst
type are direct samples (xn  yn)  xn ∈ RP   yn ∈ RQ  n = 1  . . .   N of v leading to a regression
problem. The second case occurs  if only indirect measurements zm ∈ R  m = 1  . . .   M are
available  which we assume to be generated by a known linear1 transformation of the vector ﬁeld
outputs yn belonging to nodes xn  n = 1  . . .   N. This kind of estimation problem is known as
an inverse problem. Let z = (z1  . . .   zM )T denote the vector of indirect measurements  Y =
N )T the N × Q matrix of vector ﬁeld outputs and vec(Y ) a column vector containing
(yT
the stacked transposed rows of Y . The linear relationship between Y and z can be written as z =
F vec(Y ) using the forward model F ∈ RM×N Q.

1   . . .   yT

1If the true relation is nonlinear  it is here assumed to be linearized.

1

As an example of an inverse problem consider the way humans localize acoustic sources. Here z
comprises the signal arriving at the ears  v is the spatial distribution of the sound sources and F
is given by physical equations of sound propagation. Using information from two ears  humans
do already very well in estimating the direction of incoming sounds. By further incorporating prior
knowledge  e.g. on the loudness of the sources  v can usually be well approximated. The use of prior
knowledge (a.k.a. regularization) is indeed the most effective strategy for solving inverse problems
[13]  which are inherently ambiguous. Hence  the same mechanisms used to avoid overﬁtting in 
e.g.  regression may be applied to cope with the ambiguity of inverse problems.
For the estimation of scalar functions  methods that utilize sparse linear combinations of basis func-
tions have gained considerable attention recently (e.g. the “lasso” [14]). Apart from the computa-
tional tractability that comes with the sparsity of the learned model  the possibility of interpreting the
estimates in terms of their basis functions is a particularly appealing feature of these methods. While
sparse expansions are also desirable in vector ﬁeld estimation  lasso and similar methods cannot be
used for that purpose  as they break rotational invariance in the output space RQ. This is easily seen
as sparse methods tend to select different basis functions in each of the Q dimensions.
Only few attempts have been made on rotation-invariant sparse vector ﬁeld expansions so far. In [8]
a dense expansion is discussed  which could be modiﬁed to a sparse version maintaining rotational
invariance. Unfortunately  this method is restricted to approximating curl-free ﬁelds. In contrast 
we here propose a method that can be used to decompose any vector ﬁeld. We will derive the
general framework in section 2. In section 3 we will apply the (appropriately customized) method
for solving the EEG/MEG inverse problem. Finally  we will draw a brief conclusion in section 4.

2 Method

Our model is based on the assumption that v can be well approximated by a linear combination
of some basis ﬁelds. A basis ﬁeld is deﬁned here (unlike in [8]) as a vector ﬁeld  in which all
output vectors point in the same direction  while the magnitudes are proportional to a scalar (basis)
function b : RP (cid:55)→ R. As demonstrated in Fig. 1  this model has an expressive power which
is comparable to a basis function expansion of scalar functions. Given a set (dictionary) of basis
functions bl(x)  l = 1  . . .   L  the basis ﬁeld expansion is written as

l=1

v(x) =

clbl(x)  

(1)
with coefﬁcients cl ∈ RQ  l = 1  . . .   L to be estimated. Note that by including one coefﬁcient for
each output dimension  both orientations and proportionality factors are learned in this model (the
term “basis ﬁeld” thus refers to a basis function with learned coefﬁcients). In order to select a small
set of ﬁelds  most of the coefﬁcient vectors cl have to vanish. This can be accomplished by solving
a least-squares problem with an additional lasso-like (cid:96)1-norm penalty on the coefﬁcients. However 
care has to be taken in order to maintain rotational invariance of the solution. We here propose to use
a regularizer that imposes sparsity and is invariant with respect to rotations  namely the (cid:96)1-norm of
the magnitudes of the coefﬁcient vectors. Let C = (c1  . . .   cL)T ∈ RL×Q contain the coefﬁcients
and

L(cid:88)

 b1(x1)

...

B =

. . .

bL(x1)

...

b1(xN )

. . .

bL(xN )

 ∈ RN×L

(2)

C

the basis functions evaluated at the xn. The parameters are estimated using

ˆC = arg min

L(C) + λR(C)  

where R(C) = (cid:107)C(cid:107)1 2 =(cid:80)L

(3)
l=1 (cid:107)cl(cid:107)2 is the regularizer (the so-called (cid:96)1 2-norm of the matrix C) 
L(C) is the quadratic loss function  which is deﬁned by L(C) = (cid:107) vec(Y −BC)(cid:107)2
2 in the regression
case and L(C) = (cid:107)z−F vec(BC)(cid:107)2
2 in the inverse reconstruction case  and λ is a positive constant.
In the statistics literature (cid:96)1 2-norm regularization is already known as a general mechanism for
achieving sparsity of grouped predictors [18]. Besides vector ﬁeld estimation  this concept has
natural applications in  e.g  multiple kernel learning [1] and channel selection for brain computer
interfacing [15]. It has also recently been considered in the general multiple output setting [17].

2

Figure 1: Complicated vector ﬁeld (SUM) as a sum of three basis ﬁelds (1-3).

2.1 Rotational Invariance

Rotational invariance  in the sense that the estimates after rotation of the coordinates axes are equal
to the rotated estimates  is a desirable property of an estimator. One has to distinguish invariance
in input- from invariance in output space. The former requirement may arise in many estimation
settings and can be fulﬁlled by the choice of appropriate basis functions bl(x). The latter one is
speciﬁc to vector ﬁeld estimation and has to be assured by formulating a rotationally invariant cost
function. Our proposed estimator Eq. 3 is rotationally invariant. This is due to the use of the (cid:96)2-
norm in output space RQ  which does not change under rotation. I.e. for an orthogonal matrix
R ∈ RQ×Q  RT R = I

L(cid:88)

(cid:107)Rcl(cid:107)2 =

L(cid:88)

(cid:113)

L(cid:88)

tr(cT

l RT Rcl) =

(cid:107)cl(cid:107)2 .

(4)

l=1

l=1

l=1

For the same argument  additional regularizers R∗(C) = (cid:107) vec(D∗C)(cid:107)2
2 (the well-known Tikhonov
regularizer) or R+(C) = (cid:107)D+C(cid:107)1 2 (promoting sparsity of the linearly transformed vectors) may
be introduced without breaking the rotational invariance in RQ.

2.2 Optimization
Eq. 3 is a convex problem  composed of the quadratic term L(C) and the convex nondifferentiable
term R(C). It is equivalent to the following program

L(cid:80)

ˆC = arg min

C u

s.t. (cid:107)cl(cid:107)2 ≤ ul  

ul

l = 1  . . .   L

l=1

L(C) ≤ ε  

(5)

in which a linear function of the variables is minimized subject to quadratic and second-order cone
constraints [6]. The latter constraints are obtained by introducing auxiliary variables ul ∈ R  l =
1  . . .   L encoding upper bounds of the magnitudes of the coefﬁcient vectors. Problem Eq. 5 is
an instance of second-order cone programming (SOCP)  a standard class of convex programs  for
which efﬁcient interior-point based solvers are available. The problem stays inside the SOCP class
even if the original formulation is modiﬁed in any of the following ways:

• Additional regularizers R+(C) or R∗(C) are used.
• The quadratic loss function is replaced by a more robust (cid:96)1-norm based loss (e.g. hinge
loss). In the regression case  this loss should be deﬁned based on the magnitude of the
residual vector  which leads to a formulation involving the (cid:96)1 2-norm (and thus additional
SOCP constraints).

• Complex basis functions (e.g. Fourier bases or Morlet wavelets) are used. This approach
also requires complex coefﬁcients  by which it is then possible not only to optimally scale
the basis functions  but also to optimally shift their phase. Similarly  it is possible to recon-
struct complex vector ﬁelds from complex measurements using real-valued basis functions.

3

123SUM3 Application to the EEG/MEG inverse problem

Vector ﬁelds occur  for example  in form of electrical currents in the brain  which are produced by
postsynaptic neuronal processes. Knowledge of the electrical ﬁelds during a certain experimental
condition allows one to draw conclusions about the locations in which the cognitive processing
takes place and is thus of high value for research and medical diagnosis. Invasive measurements
allow very local assessment of neuronal activations  but such procedure in humans is only possible
when electrodes are implanted for treatment/diagnosis of neurological diseases  e.g.  epilepsy. In
the majority of cases recordings of cortical activity are performed with non-invasive measures such
as electro- and magnetoencephalography  EEG and MEG respectively. The reconstruction of the
current density from such measurements is an inverse problem.

3.1 Method speciﬁcation

In the following the task is to infer the generating cerebral current density given an EEG measure-
ment z ∈ RM . The current density is a vector ﬁeld v : R3 (cid:55)→ R3 assigning a vectorial current source
to each location in the brain. We obtained a realistic head model from high-resolution MRI (mag-
netic resonance imaging) slices of a human head [4]. Inside the brain  we arranged 2142 nodes in a
regular grid of 1 cm distance. The forward mapping F ∈ RM×2142·3 from these nodes to the elec-
trodes was constructed according to [9] – taking into account the realistic geometry and conductive
properties of brain  skull and skin.

Dictionary

In most applications the “true” sources are expected to be small in number and spatial extent. How-
ever  many commonly used methods estimate sources that almost cover the whole brain (e.g. [11]).
Another group of methods delivers source estimates that are spatially sparse  but usually not ro-
tationally invariant (e.g. [7]). Here often too many sources  which are scattered around the true
sources  are estimated. Both the very smooth and the very sparse estimates are unrealistic from a
physiological point of view. Only very recently  approaches capable of achieving a compromise be-
tween these two extremes have been outlined [16  3]. For achieving a similar effect we here propose
a sparse basis ﬁeld expansion using radial basis functions. More speciﬁcally we consider spherical
Gaussians

(cid:19)

(cid:18)

−1
2

bn s(x) = (2πσs)− 3

2 exp

(cid:107)x − xn(cid:107)2

2 σ−2

s

(6)

s = 1  . . .   4  having spatial standard deviations σ1 = 0.5 cm  σ2 = 1 cm  σ3 = 1.5 cm  σ4 = 2 cm
and being centered at nodes xn  n = 1  . . .   N (see Fig. 2 for examples). Using this redundant
dictionary our expectation is that sources of different spatial extent can be reconstructed by selecting
the appropriate basis functions. Unlike the approaches taken in [16  3] this approach does not require
an additional hyperparameter for controlling the tradeoff between sparsity and smoothness.

Figure 2: Gaussian basis functions with ﬁxed center and standard deviations 0.5 cm − 2 cm.

Normalization

Our (cid:96)1 2-norm based regularization is a heuristic for selecting the smallest possible number of basis
ﬁelds necessary to explain the measurement. Using this approach  however  not only the number
of nonzero coefﬁcient vectors  but also their magnitudes enter the cost function.
It is therefore
important to normalize the basis functions in order not to a-priori prefer some of them. Let Bs
be the N × N matrix containing the basis functions with standard deviation σs. The large matrix
B = (B1/(cid:107) vec(B1)(cid:107)1  . . .   B4/(cid:107) vec(B4)(cid:107)1) ∈ RN×4N is then constructed using normalized Bs.
By this means  no length scale is artiﬁcially prefered.

4

 W1

...
0

W =

0
. . .
...
...
. . . WN

 ∈ R3N×3N  
(cid:80)L

l=1 ˆclbl(xn).

(7)

An estimation bias is also introduced by the location of the sources. Due to volume conduction  the
signal captured at the sensors is much stronger for superﬁcial sources compared to deep sources.

In [10] the variance estimate ˆS = ¯F T(cid:0) ¯F ¯F T(cid:1)−1 ¯F ∈ R3N×3N is derived for the (least-squares)

estimated sources  where ¯F = HF and H = I − 11T /1T 1 ∈ RM×M . We found that ˆS can be
used for removing the location bias. This can be done by either penalizing activity at locations with
high variance or by penalizing basis functions with high variance in the center. We here employ the
former approach  as the latter may be problematic for basis functions with large extent. Using this
approach  evaluation of ˆv(x) requires knowledge of the forward model for x. Therefore  we restrict
ourselves here to nodes xn  n = 1  . . .   N. Let Wn ∈ R3×3 denote the inverse matrix square root of
the part of ˆS belonging to node xn. Deﬁning

the coefﬁcients are estimated using ˆC = arg min
estimated current density at node xn is ˆv(xn) = Wn

C

(cid:107)C(cid:107)1 2

s.t. (cid:107)z − F W vec(BC)(cid:107)2

2 < ε. The

3.2 Experiments

Validation of methods for inverse reconstruction is generally difﬁcult due to the lack of a “ground
truth”. The measurements z cannot be used in this respect  as the main goal is not to predict the
EEG/MEG measurements  but the vector ﬁeld v(x) as accurately as possible. Therefore  the only
way to evaluate inverse methods is to assess their ability to reconstruct known functions. We do
this by reconstructing a) simulated current sources and b) sources of real EEG data that are already
well-localized by other studies. For each EEG measurement  simulated or not  we conduct a 5 × 5
crossvalidation  i.e. we perform 25 inverse reconstructions based on different training sets contain-
ing 80 % of the electrodes. In each crossvalidation run  we evaluate two criteria. Most important
is the reconstruction error  deﬁned as Cy = (cid:107) vec(Y )/(cid:107) vec(Y )(cid:107)2 − vec( ˆY tr)/(cid:107) vec( ˆY tr)(cid:107)2(cid:107)2 
where ˆY tr are the vector ﬁeld outputs at nodes xn  n = 1  . . .   N estimated using only the training
set. This criterion can only be evaluated for the simulated data. For real and simulated data we also
evaluate the generalization error  i.e. the error in the prediction of the remaining 20% (the test set)
of the EEG measurements. This is deﬁned as Cz = (cid:107)zte − F te vec( ˆY tr)(cid:107)2
2  where zte and F te are
the parts of z and F belonging to the test set.
We compared the sparse basis ﬁeld expansion (S-FLEX) approach using Gaussian basis functions
(see section 3.1) to the commonly used approaches of LORETA [11] and Minimum Current Estimate
(MCE) [7]  and the recently proposed Focal Vectorﬁeld Reconstruction (FVR) technique [3]. All
three competitors correspond to using unit impulses as basis functions while employing different
regularizers. The LORETA solution  e.g.  is a Tikhonov regularized least-squares estimate while
MCE is equivalent to applying lasso to each dimension separately  yielding current vectors that are
biased towards being axes-parallel. We here used a variant of MCE  in which the original depth
compensation approach was replaced by the approach outlined in section 3.1. Interestingly  FVR
can be interpreted as a special case of S-FLEX employing the rotation-invariant regularizer R+(C)
to enforce both sparsity and smoothness. The tradeoff parameter α of this method was chosen as
suggested in [3]. All methods were formulated such that the ﬁtness of the solution was ensured by
the constraint (cid:107)z − F vec( ˆY tr)(cid:107)2
2 < ε. The optimization was carried out using freely available
packages for convex programming [12  2].

Simulated data

We simulated current densities in the following way. First  we sampled outputs yn  n = 1  . . .   N
from a multivariate standard normal distribution. The function (xn  yn) was then spatially smoothed
using a Gaussian lowpass ﬁlter with standard deviation 2.5 cm. Finally  each yn was shortened by
the 90th percentile of the magnitudes of all yn – leaving only 10% of the current vectors active.
Current densities obtained by this procedure usually feature 2-3 active patches (sources) with small
to medium extent and smoothly varying magnitude and orientation (see Fig. 3 for an example). This

5

behaviour was considered consistent with the general believe on the sources. We simulated ﬁve
densities and computed respective pseudo-measurements for 118 channels using the forward model
F . As no noise was injected in the system  ε was set to zero in the following reconstruction.

Real data

We recorded 113-channel EEG of one healthy subject (male  26 years) during electrical median
nerve stimulation. The EEG electrodes were positioned according to the international 10-20 sys-
tem. The exact positions were obtained using a 3D digitizer and mapped onto the surface of the
head model. EEG data were recorded with sampling frequency of 2500 Hz and digitally bandpass-
ﬁltered between 15 Hz and 450 Hz. Left and right median nerves were stimulated in separate blocks
by applying constant square 0.2 ms current pulses to the respective thenars. Current pulses had
intensities above motor threshold (approx. 9 mA)  inducing unintended twitches of the thumbs.
The interstimulus interval varied randomly between 500 ms and 700 ms. About 1100 trials were
recorded for each hand. Artifactual trials as well as artifactual electrodes were excluded from the
analysis. For the remaining data  baseline correction was done based on the mean amplitude in the
prestimulus interval (-100 ms to -10 ms). Finally  a single measurement vector was constructed by
averaging the EEG amplitudes at 21 ms across 1946 trials (50% left hand  50% right hand). By this
means the EEG response to somatosensory input at the hands was captured with high signal-to-noise
ratio (SNR). Based on that the brain areas representing left and right hand were to be reconstructed
with ε set according to the estimated SNR.

3.3 Results

Fig. 3 shows a simulated current density along with reconstructions according to LORETA  MCE 
FVR and S-FLEX. From the ﬁgure it becomes apparent  that LORETA and MCE do not approximate
the true current density very well. While the LORETA solution is rather blurry  merging the two true
sources  the MCE solution exhibits many spikes  which could easily be misinterpreted as different
sources. Note that the strong orientation bias of MCE cannot be seen in Fig. 3 as only dipole
amplitudes are plotted. The estimates of FVR and S-FLEX approximately recover the shape of the
sources. S-FLEX comes closest to the true shape  as its estimates are less focal than the ones of
FVR. However  S-FLEX still slightly underestimate the extent of the sources.
The localization results of left and right N20 generators are shown in Fig. 4. The solutions of FVR
and S-FLEX are almost indistinguishable. Both show activity concentrated in two major patches 
one in each contralateral somatosensory cortex. This is in good agreement with the localization of
the hand areas reported in the literature (e.g. [5]). LORETA estimates only one large active region
over the whole central area  with the maximum lying exactly in between the hand areas. The MCE
solution consists of eight spikes scattered across the whole somatosensory area.
Tab. 1 shows that S-FLEX generalizes better than its competitors  although insigniﬁcantly. More
importantly S-FLEX outperforms its peers in terms of reconstruction accuracy. The distance to
the runner-up FVR is  however  larger than expected from Fig. 3. This is due to the fact that the
parameter of FVR controlling the tradeoff between sparsity and smoothness was ﬁxed here to a
value promoting “maximally sparse sources which are still smooth”. While this might be a good
assumption in practise  it was not rewarded in our validation setting. We here explicitly required
reconstruction rather than shrinkage of the sources.

Cy SIM

LORETA 1.00 ± 0.01
0.955 ± 0.02
FVR
0.71 ± 0.04
S-FLEX
1.21 ± 0.01
MCE

Cz SIM
2.87 ± 0.78
1.21 ± 1.00
0.952 ± 0.28
1.86 ± 0.57

Cz REAL
8.18 ± 1.38
8.01 ± 1.79
7.95 ± 1.84
8.13 ± 1.60

Table 1: Ability of LORETA  FVR  S-FLEX and MCE to reconstruct simulated currents (Cy SIM)
and generalization performance with respect to the EEG measurements (Cz SIM/REAL). Winning
entries (reaching signiﬁcance) are shown in bold face.

6

SIM

LORETA

FVR

S-FLEX

MCE

Figure 3: Simulated current density (SIM) and reconstruction according to LORETA  FVR  S-FLEX
and MCE. Color encodes current magnitude.

LORETA

FVR

S-FLEX

MCE

Figure 4: Localization of somatosensory evoked N20 generators according to LORETA  FVR 
S-FLEX and MCE. Color encodes current magnitude.

7

4 Conclusion and Outlook

This paper contributes a novel and general methodology for obtaining sparse decompositions of
vector ﬁelds. An important ingredient of our framework is the insight that the vector ﬁeld estimate
should be invariant with respect to a rotation of the coordinate system.
Interestingly  the latter
constraint together with sparsity leads to a second-order cone programming formulation.
We have focussed here on solving the EEG/MEG inverse problem  where our proposed S-FLEX
approach outperformed the state-of-the-art in approximating the true shape of the current sources.
However  other ﬁelds might as well beneﬁt from the use of S-FLEX: in meteorology for example  an
improved decomposition of wind ﬁelds into their driving components might provide novel insights
that could be useful for better weather forecasting.

Acknowledgments

This work was supported in part by the German BMBF grants BCCNB-A4 (FKZ 01GQ0415) 
BFNTB-A1 (FKZ 01GQ0850) and FaSor (FKZ 16SV2234). We thank Friederike Hohlefeld and
Monika Weber for help in preparing the experiment  and Ryota Tomioka for fruitful discussions.

References
[1] F.R. Bach  G.R.G. Lanckriet  and M.I. Jordan. Multiple kernel learning  conic duality and the SMO

algorithm. In Proceedings of the Twenty-ﬁrst International Conference on Machine Learning  2004.

[2] M. Grant  S. Boyd  and Y. Ye. CVX: Matlab Software for Disciplined Convex Programming  October

2006. http://www.stanford.edu/˜boyd/cvx/  Version 1.0RC.

[3] S. Haufe  V.V. Nikulin  A. Ziehe  K.-R. M¨uller  and G. Nolte. Combining sparsity and rotational invari-

ance in EEG/MEG source reconstruction. NeuroImage  42(2):26–738  2008.

[4] C.J. Holmes  R. Hoge  L. Collins  R. Woods  A.W. Toga  and A.C. Evans. Enhancement of MR images

using registration for signal averaging. J. Comput. Assist. Tomogr.  22(2):324–333  1998.

[5] J. Huttunen  S. Komssi  and L. Lauronen. Spatial dynamics of population activities at S1 after median

and ulnar nerve stimulation revisited: An MEG study. NeuroImage  32:1024–1031  2006.

[6] M.S. Lobo  L. Vandenberghe  S. Boyd  and H. Lebret. Applications of second-order cone programming.

Lin. Alg. Appl.  284:193–228  1998.

[7] K. Matsuura and Y. Okabe. Selective minimum-norm solution of the biomagnetic inverse problem. IEEE

Trans. Biomed. Eng.  42:608–615  1995.

[8] F.A. Mussa-Ivaldi. From basis functions to basis ﬁelds: vector ﬁeld approximation from sparse data. Biol.

Cybern.  67:479–489  1992.

[9] G. Nolte and G. Dassios. Analytic expansion of the EEG lead ﬁeld for realistic volume conductors. Phys.

Med. Biol.  50:3807–3823  2005.

[10] R.D. Pascual-Marqui. Standardized low-resolution brain electromagnetic tomography (sLORETA): tech-

nical details. Meth. Find. Exp. Clin. Pharmacol.  24(1):5–12  2002.

[11] R.D. Pascual-Marqui  C.M. Michel  and D. Lehmann. Low resolution electromagnetic tomography: a

new method for localizing electrical activity in the brain. Int. J. Psychophysiol.  18:49–65  1994.

[12] J.F. Sturm. Using SeDuMi 1.02  a MATLAB toolbox for optimization over symmetric cones. Optim.

Method. Softw.  11–12:625–653  1999.

[13] A. Tarantola. Inverse Problem Theory and Model Parameter Estimation. SIAM  Philadelphia  2005.
[14] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Stat. Soc. B Meth.  58(1):267–288 

1996.

[15] R. Tomioka and S. Haufe. Combined classiﬁcation and channel/basis selection with L1-L2 regularization
with application to P300 speller system. In Proceedings of the 4th International Brain-Computer Interface
Workshop and Training Course 2008. Verlag der Technischen Universit¨at Graz  2008.

[16] M. Vega-Hern´andez  E. Mart´ınez-Montes  J.M. S´anchez-Bornot  A. Lage-Castellanos  and P.A. Vald´es-
In

Sosa. Penalized least squares methods for solving the EEG inverse problem. Stat. Sinica  2008.
press.

[17] D.P. Wipf and B.D. Rao. An empirical bayesian strategy for solving the simultaneous sparse approxima-

tion problem. IEEE Trans. Signal Proces.  55(7):3704–3716  2007.

[18] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Roy. Stat.

Soc. B Meth.  68(1):49–67  2006.

8

,yan yang
Jian Sun
Huibin Li