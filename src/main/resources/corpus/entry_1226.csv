2016,Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated,Given a matrix of observed data  Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for a long time. However  to the best of our knowledge  all existing theoretical guarantees for it assume that the data and the corrupting noise  are mutually independent  or at least uncorrelated. This is valid in practice often  but not always. In this paper  we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as ``data-dependent noise". We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD  cluster-EVD  that improves upon EVD in certain regimes.,Correlated-PCA: Principal Components’ Analysis

when Data and Noise are Correlated

Namrata Vaswani and Han Guo

Iowa State University  Ames  IA  USA
Email: {namrata hanguo}@iastate.edu

Abstract

Given a matrix of observed data  Principal Components Analysis (PCA) computes
a small number of orthogonal directions that contain most of its variability. Prov-
ably accurate solutions for PCA have been in use for a long time. However  to
the best of our knowledge  all existing theoretical guarantees for it assume that the
data and the corrupting noise are mutually independent  or at least uncorrelated.
This is valid in practice often  but not always. In this paper  we study the PCA
problem in the setting where the data and noise can be correlated. Such noise is
often also referred to as “data-dependent noise”. We obtain a correctness result
for the standard eigenvalue decomposition (EVD) based solution to PCA under
simple assumptions on the data-noise correlation. We also develop and analyze a
generalization of EVD  cluster-EVD  that improves upon EVD in certain regimes.

Introduction

1
Principal Components Analysis (PCA) is among the most frequently used tools for dimension re-
duction. Given a matrix of data  it computes a small number of orthogonal directions that contain all
(or most) of the variability of the data. The subspace spanned by these directions is the “principal
subspace”. To use PCA for dimension reduction  one projects the observed data onto this subspace.
The standard solution to PCA is to compute the reduced singular value decomposition (SVD) of
the data matrix  or  equivalently  to compute the reduced eigenvalue decomposition (EVD) of the
empirical covariance matrix of the data. If all eigenvalues are nonzero  a threshold is used and all
eigenvectors with eigenvalues above the threshold are retained. This solution  which we henceforth
refer to as simple EVD  or just EVD  has been used for many decades and is well-studied in litera-
ture  e.g.  see [1] and references therein. However  to the best of our knowledge  all existing results
for it assume that the true data and the corrupting noise in the observed data are independent  or  at
least  uncorrelated. This is valid in practice often  but not always. Here  we study the PCA problem
in the setting where the data and noise vectors may be correlated (correlated-PCA). Such noise is
sometimes called “data-dependent” noise.
Contributions.
(1) Under a boundedness assumption on the true data vectors  and some other as-
sumptions  for a ﬁxed desired subspace error level  we show that the sample complexity of simple-
EVD for correlated-PCA scales as f 2r2 log n where n is the data vector length  f is the condition
number of the true data covariance matrix and r is its rank. Here “sample complexity” refers to
the number of samples needed to get a small enough subspace recovery error with high probability
(whp). The dependence on f 2 is problematic for datasets with large condition numbers  and  es-
pecially in the high dimensional setting where n is large. (2) To address this  we also develop and
analyze a generalization of simple-EVD  called cluster-EVD. Under an eigenvalues’ “clustering”
assumption  cluster-EVD weakens the dependence on f.
To our best knowledge  the correlated-PCA problem has not been explicitly studied. We ﬁrst en-
countered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive
Sensing (ReProCS) framework [2  3  4  5]. The version of correlated-PCA studied here is motivated

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

by these works. Some other somewhat related recent works include [6  7] that study stochastic
optimization based techniques for PCA; and [8  9  10  11] that study online PCA.
Notation. We use the interval notation [a  b] to mean all of the integers between a and b  inclusive 
and similarly for [a  b) etc. We use (cid:107) · (cid:107) to denote the l2 norm of a vector or the induced l2 norm of
a matrix. For other lp norms  we use (cid:107) · (cid:107)p. For a set T   IT refers to an n × |T | matrix of columns
of the identity matrix indexed by entries in T . For a matrix A  AT := AIT . A tall matrix with
orthonormal columns is referred to as a basis matrix. For basis matrices ˆP and P   we quantify the
subspace error (SE) between their range spaces using

SE( ˆP   P ) := (cid:107)(I − ˆP ˆP (cid:48))P(cid:107).

(1)

1.1 Correlated-PCA: Problem Deﬁnition
We are given a time sequence of data vectors  yt  that satisfy

yt = (cid:96)t + wt  with wt = Mt(cid:96)t and (cid:96)t = P at

(2)
where P is an n × r basis matrix with r (cid:28) n. Here (cid:96)t is the true data vector that lies in a low
dimensional subspace of Rn  range(P ); at is its projection into this r-dimensional subspace; and
wt is the data-dependent noise. We refer to Mt as the correlation / data-dependency matrix. The
goal is to estimate range(P ). We make the following assumptions on (cid:96)t and Mt.
Assumption 1.1. The subspace projection coefﬁcients  at  are zero mean  mutually independent
and bounded random vectors (r.v.)  with a diagonal covariance matrix Λ. Deﬁne λ− := λmin(Λ) 
λ+ := λmax(Λ) and f := λ+
λ− . Since the at’s are bounded  we can also deﬁne a ﬁnite constant
. Thus  (at)2
η := maxj=1 2 ...r maxt

j ≤ ηλj.

(at)2
j

λj

For most bounded distributions  η will be a small constant more than one  e.g.  if the distribution of
all entries of at is iid zero mean uniform  then η = 3. From Assumption 1.1  clearly  the (cid:96)t’s are also
zero mean  bounded  and mutually independent r.v.’s with a rank r covariance matrix Σ EVD= P ΛP (cid:48).
In the model  for simplicity  we assume Λ to be ﬁxed. However  even if we replace Λ by Λt and
deﬁne λ− = mint λmin(Λt) and λ+ = λmax(Λt)  all our results will still hold.
Assumption 1.2. Decompose Mt as Mt = M2 tM1 t. Assume that

(cid:107)M1 tP(cid:107) ≤ q < 1  (cid:107)M2 t(cid:107) ≤ 1 

and  for any sequence of positive semi-deﬁnite Hermitian matrices  At  the following holds

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

α

α(cid:88)

t=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ β

α

for a β < α 

M2 tAtM2 t

(cid:48)

(cid:107)At(cid:107).

max
t∈[1 α]

(3)

(4)

We will need the above to hold for all α ≥ α0 and for all β ≤ c0α with a c0 (cid:28) 1. We set α0 and c0
in Theorems 2.1 and 3.3; both will depend on q. Observe that  using (3)  (cid:107)wt(cid:107)
(cid:107)(cid:96)t(cid:107) ≤ q  and so q is an
upper bound on the signal-to-noise ratio (SNR).
To understand the assumption on M2 t  notice that  if we allow β = α  then (4) always holds and
is not an assumption. Let B denote the matrix on the LHS of (4). One example situation when (4)
holds with a β (cid:28) α is if B is block-diagonal with blocks At. In this case  it holds with β = 1. In
fact  it also holds with β = 1 if B is permutation-similar to a block diagonal matrix. The matrix
B will be of this form if M2 t = ITt with all the sets Tt being mutually disjoint. More generally 
if B is permutation-similar to a block-diagonal matrix with blocks given by the summation of At’s
over at most β0 < α time instants  then (4) holds with β = β0. This will happen if M2 t = ITt
with Tt = T [k] for at most β0 time instants and if sets T [k] are mutually disjoint for different
k. Finally  the T [k]’s need not even be mutually disjoint. As long as they are such that B is a
matrix with nonzero blocks on only the main diagonal and on a few diagonals near it  e.g.  if it is
block tri-diagonal  it can be shown that the above assumption holds. This example is generalized in
Assumption 1.3 given below.
1.2 Examples of correlated-PCA problems
One key example of correlated-PCA is the PCA with missing data (PCA-missing) problem. Let Tt
denote the set of missing entries at time t. Suppose  we set the missing entries of yt to zero. Then 
(5)

yt = (cid:96)t − ITtITt

(cid:48)(cid:96)t.

2

yt = (cid:96)t + ITtxt = (cid:96)t + ITtMs t(cid:96)t.

(cid:48). Thus  q is an upper bound on (cid:107)ITt

In this case M2 t = ITt and M1 t = −ITt
(cid:48)P(cid:107). Clearly  it
will be small if the columns of P are dense vectors. For the reader familiar with low-rank matrix
completion (MC)  e.g.  [12  13]  PCA-missing can also be solved by ﬁrst solving the low-rank matrix
completion problem to recover L  followed by PCA on the completed matrix. This would  of course 
be much more expensive than directly solving PCA-missing and would need more assumptions.
Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formula-
tion) [14  15  16] when the sparse component’s magnitude is correlated with (cid:96)t. Let Tt denote the
support set of wt and let xt be the |Tt|-length vector of its nonzero entries. If we assume linear
dependency of xt on (cid:96)t  we can write out yt as
(6)
Thus M2 t = ITt and M1 t = Ms t and so q is an upper bound on (cid:107)Ms tP(cid:107). In the rest of the
paper  we refer to this problem is “PCA with sparse data-dependent corruptions (PCA-SDDC)”.
One key application where it occurs is in foreground-background separation for videos consisting
of a slow changing background sequence (modeled as lying close to a low-dimensional subspace)
and a sparse foreground image sequence consisting typically of one or more moving objects [14].
The PCA-SDDC problem is to estimate the background sequence’s subspace. In this case  (cid:96)t is
the background image at time t  Tt is the support set of the foreground image at t  and xt is the
difference between foreground and background intensities on Tt. An alternative solution approach
for PCA-SDDC is to use an RPCA solution such as principal components’ pursuit (PCP) [14  15] or
Alternating-Minimization (Alt-Min-RPCA) [17] to ﬁrst recover the matrix L followed by PCA on
L. However  as shown in Sec. 5  Table 1  this approach will be much slower; and it will work only
if its required incoherence assumptions hold. For example  if the columns of P are sparse  it fails.
For both problems above  a solution for PCA will work only when the corrupting noise wt is small
compared to (cid:96)t. A sufﬁcient condition for this is that q is small.
A third example where correlated-PCA and its generalization  correlated-PCA with partial subspace
knowledge  occurs is in the subspace update step of Recursive Projected Compressive Sensing (Re-
ProCS) for dynamic robust PCA [3  5].
In all three of the above applications  the assumptions on the data-noise correlation matrix given in
Assumption 1.2 hold if there are enough changes of a certain type in the set of missing or corrupted
entries  Tt. One example where this is true is in case of a 1D object of length s or less that remains
static for at most β frames at a time. When it moves  it moves by at least a certain fraction of s
pixels. The following assumption is inspired by the object’s support.
Assumption 1.3. Let l denote the number of times the set Tt changes in the interval [1  α] (or in any
given interval of length α in case of dynamic robust PCA). So 0 ≤ l ≤ α − 1. Let t0 := 1; let tk 
with tk < tk+1  denote the time instants in this interval at which Tt changes; and let T [k] denote the
distinct sets. In other words  Tt = T [k] for t ∈ [tk  tk+1)  for each k = 1  2  . . .   l. Assume that the
following hold with a β < α:

1. (tk+1 − tk) ≤ ˜β and |T [k]| ≤ s;
2. ρ2 ˜β ≤ β where ρ is the smallest positive integer so that  for any 0 ≤ k ≤ l  T [k] and

T [k+ρ] are disjoint;

3. for any k1  k2 satisfying 0 ≤ k1 < k2 ≤ l  the sets (T [k1] \ T [k1+1]) and (T [k2] \ T [k2+1])

k=0 |T [k] \ T [k+1]| ≤ n. Observe that

are disjoint.

An implicit assumption for condition 3 to hold is that(cid:80)l

conditions 2 and 3 enforce an upper bound on the maximum support size s.

To connect Assumption 1.3 with the moving object example given above  condition 1 holds if the
object’s size is at most s and if it moves at least once every ˜β frames. Condition 2 holds  if  every
time it moves  it moves in the same direction and by at least s
ρ pixels. Condition 3 holds if  every
time it moves  it moves in the same direction and by at most d0 ≥ s
ρ pixels  with d0α ≤ n (or  more
generally  the motion is such that  if the object were to move at each frame  and if it started at the
top of the frame  it does not reach the bottom of the frame in a time interval of length α).
The following lemma [4] shows that  with Assumption 1.3 on Tt  M2 t = ITt satisﬁes the assump-
tion on M2 t given in Assumption 1.2. Its proof generalizes the discussion below Assumption 1.2.

3

Lemma 1.4. [[4]  Lemmas 5.2 and 5.3] Assume that Assumption 1.3 holds. For any sequence of
|Tt| × |Tt| symmetric positive-semi-deﬁnite matrices At 

(cid:107) α(cid:88)

t=1

ITt AtITt

(cid:48)(cid:107) ≤ (ρ2 ˜β) max
t∈[1 α]

(cid:107)At(cid:107) ≤ β max
t∈[1 α]

(cid:107)At(cid:107)

(cid:48)P(cid:107) ≤ q < 1  then the PCA-missing problem satisﬁes Assumption 1.2. If (cid:107)Ms tP(cid:107) ≤

Thus  if (cid:107)ITt
q < 1  then the PCA-SDDC problem satisﬁes Assumption 1.2.
Assumption 1.3 is one model on Tt that ensures that  if M2 t = ITt  the assumption on M2 t given
in Assumption 1.2 holds. For its many generalizations  see Supplementary Material  Sec. 7  or [4].
As explained in [18]  data-dependent noise also often occurs in molecular biology applications when
the noise affects the measurement levels through the very same process as the interesting signal.
2 Simple EVD
Simple EVD computes the top eigenvectors of the empirical covariance matrix  1
α
the observed data. The following can be shown.
Theorem 2.1 (simple-EVD result). Let ˆP denote the matrix containing all the eigenvectors of
(cid:48) with eigenvalues above a threshold  λthresh  as its columns. Pick a ζ so that
1
rζ ≤ 0.01. Suppose that yt’s satisfy (2) and the following hold.
α

(cid:80)α

(cid:80)α

t=1 ytyt

t=1 ytyt

(cid:48)  of

1. Assumption 1.1 on (cid:96)t holds. Deﬁne

(rζ)2 max(f  qf  q2f )2  C :=

32
0.012 .

2. Assumption 1.2 on Mt holds for any α ≥ α0 and for any β satisfying

α0 := Cη2 r211 log n
(cid:19)2

(cid:18) 1 − rζ

≤

β
α

(cid:18) (rζ)2

(cid:19)

(rζ)
q2f

4.1(qf )2  
3. Set algorithm parameters λthresh = 0.95λ− and α ≥ α0.

min

2

tion of measure result to(cid:80)

Then  with probability at least 1 − 6n−10  SE( ˆP   P ) ≤ rζ.
Proof: The proof involves a careful application of the sin θ theorem [19] to bound the subspace
error  followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the
terms in the sin θ bound. It is given in the Supplementary Material  Section 8.
Consider the lower bound on α. We refer to this as the “sample complexity”. Since q < 1  and η is a
small constant (e.g.  for the uniform distribution  η = 3)  for a ﬁxed error level  rζ  α0 simpliﬁes to
cf 2r2 log n. Notice that the dependence on n is logarithmic. It is possible to show that the sample
complexity scales as log n because we assume that the (cid:96)t’s are bounded r.v.s. As a result we can
apply the matrix Hoeffding inequality [20] to bound the perturbation between the observed data’s
empirical covariance matrix and that of the true data. The bounded r.v. assumption is actually a
more practical one than the usual Gaussian assumption since most sources of data have ﬁnite power.
By replacing matrix Hoeffding by Theorem 5.39 of [21] in places where one can apply a concentra-
(cid:48)/α (which is at r × r matrix)  and by matrix Bernstein [20] else-
where  it should be possible to further reduce the sample complexity to c max((qf )2r log n  f 2(r +
log n)). It should also be possible remove the boundedness assumption and replace it by a Gaussian
(or a sub-Gaussian) assumption  but  that would increase the sample complexity to c(qf )2n.
Consider the upper bound on β/α. Clearly  the smaller term is the ﬁrst one. This depends on
1/(qf )2. Thus  when f is large and q is not small enough  the bound required may be impractically
small. As will be evident from the proof (see Remark 8.3 in Supplementary Material)  we get this
(cid:48)] (cid:54)= 0.
bound because wt is correlated with (cid:96)t and this results in E[(cid:96)twt
If wt and (cid:96)t were uncorrelated  qf would get replaced by λmax(Cov(wt))
as well as in the sample complexity.
Application to PCA-missing and PCA-SDDC. By Lemma 1.4  the following is immediate.

in the upper bound on β/α

t atat

λ−

4

Figure 1: Eigenvalue clusters of the three low-rankiﬁed videos.

(cid:48)P(cid:107) ≤ q < 1;
Corollary 2.2. Consider the PCA-missing model  (5)  and assume that maxt (cid:107)ITt
or consider the PCA-SDDC model  (6)  and assume that maxt (cid:107)Ms tP(cid:107) ≤ q < 1. Assume that
everything in Theorem 2.1 holds except that we replace Assumption 1.2 by Assumption 1.3. Then 
with probability at least 1 − 6n−10  SE( ˆP   P ) ≤ rζ.

3 Cluster-EVD
To try to relax the strong dependence on f 2 of the result above  we develop a generalization of
simple-EVD that we call cluster-EVD. This requires the clustering assumption.
3.1 Clustering assumption
To state the assumption  deﬁne the following partition of the index set {1  2  . . . r} based on the
eigenvalues of Σ. Let λi denote its i-th largest eigenvalue.
Deﬁnition 3.1 (g-condition-number partition of {1  2  . . . r}). Deﬁne G1 = {1  2  . . . r1} where r1
> g. In words  to deﬁne G1  start with the index of the ﬁrst
is the index for which λ1
λr1
(largest) eigenvalue and keep adding indices of the smaller eigenvalues to the set until the ratio of
the maximum to the minimum eigenvalue ﬁrst exceeds g.

For each k > 1  deﬁne Gk = {r∗ + 1  r∗ + 2  . . .   r∗ + rk} where r∗ = ((cid:80)k−1

≤ g and λ1
λr1+1

≤ g and

i=1 ri)  rk is the index for
> g. In words  to deﬁne Gk  start with the index of the (r∗ + 1)-th

which λr∗+1
λr∗+rk
eigenvalue  and repeat the above procedure.
Stop when λr∗+rk+1 = 0  i.e.  when there are no more nonzero eigenvalues. Deﬁne ϑ = k as the
number of sets in the partition. Thus {G1 G2  . . .  Gϑ} is the desired partition.

λr∗+1

λr∗+rk +1

Deﬁne G0 = [.]  Gk := (P )Gk  λ+

k := maxi∈Gk λi (Λ)  λ−

k := mini∈Gk λi (Λ) and

χ := max

k=1 2 ... ϑ

λ+
k+1
λ−

k

.

χ quantiﬁes the “distance” between consecutive sets of the above partition. Moreover  by deﬁnition 
λ+
k
−
λ
k

≤ g. Clearly  g ≥ 1 and χ ≤ 1 always. We assume the following.

Assumption 3.2. For a 1 ≤ g+ < f and a 0 ≤ χ+ < 1  assume that there exists a g satisfying
1 ≤ g ≤ g+ and a χ satisfying 0 ≤ χ ≤ χ+  for which we can deﬁne a g-condition-number
partition of {1  2  . . . r} that satisﬁes χ ≤ χ+. The number of sets in the partition is ϑ. When g+
and χ+ are small  we say that the eigenvalues are “well-clustered” with “clusters”  Gk.
This assumption can be understood as a generalization of the eigen-gap condition needed by the
block power method  which is a fast algorithm for obtaining the k top eigenvectors of a matrix [22].
We expect it to hold for data that has variability across different scales. The large scale variations
would result in the ﬁrst (largest eigenvalues’) cluster and the smaller scale variations would form the
later clusters. This would be true  for example  for video “textures” such as moving waters or waving
trees in a forest. We tested this assumption on some such videos. We describe our conclusions here
for three videos - “lake” (video of moving lake waters)  “waving-tree” (video consisting of waving
trees)  and “curtain” (video of window curtains moving due to the wind). For each video  we ﬁrst
made it low-rank by keeping the eigenvectors corresponding to the smallest number of eigenvalues
that contain at least 90% of the total energy and projecting the video onto this subspace. For the
low-rankiﬁed lake video  f = 74 and Assumption 3.2 holds with ϑ = 6 clusters  g+ = 2.6 and
χ+ = 0.7. For the waving-tree video  f = 180 and Assumption 3.2 holds with ϑ = 6  g+ = 9.4
and χ+ = 0.72. For the curtain video  f = 107 and the assumption holds ϑ = 3  g+ = 16.1 and
χ+ = 0.5. We show the clusters of eigenvalues in Fig. 1.

5

05101520253035678910111213141516 log of eigs of curtain videolog of eigs of lake videolog of eigs of waving−tree videoAlgorithm 1 Cluster-EVD
Parameters: α  ˆg  λthresh.
Set ˆG0 ← [.]. Set the ﬂag Stop ← 0. Set k ← 1.
Repeat

(cid:48)). Notice that Ψ1 =

I. Compute

 1

1. Let ˆGdet k := [ ˆG0  ˆG1  . . . ˆGk−1] and let Ψk := (I − ˆGdet k ˆGdet k

 Ψk
(b) set ˆGk = {ˆr∗ + 1  ˆr∗ + 2  . . .   ˆr∗ + ˆrk} where ˆr∗ :=(cid:80)k−1

(a) ﬁnd the index ˆrk for which ˆλ1
ˆλˆrk

2. Find the k-th cluster  ˆGk: let ˆλi = λi( ˆDk);

≤ ˆg and either

t=(k−1)α+1

ˆDk = Ψk

kα(cid:88)

ˆλ1

ˆλˆrk +1

(cid:48)

ytyt

α

j=1 ˆrj;

(c) if ˆλˆrk+1 < λthresh  update the ﬂag Stop ← 1
3. Compute ˆGk ← eigenvectors( ˆDk  ˆrk); increment k

> ˆg or ˆλˆrk+1 < λthresh;

Until Stop == 1.
Set ˆϑ ← k. Output ˆP ← [ ˆG1 ··· ˆG ˆϑ].
eigenvectors(M  r) returns a basis matrix for the span of the top r eigenvectors of M.

t=1 ytyt

(cid:80)α

3.2 Cluster-EVD algorithm
The cluster-EVD approach is summarized in Algorithm 1. I Its main idea is as follows. We start
by computing the empirical covariance matrix of the ﬁrst set of α observed data points  ˆD1 :=
(cid:48). Let ˆλi denote its i-th largest eigenvalue. To estimate the ﬁrst cluster  ˆG1  we start
1
α
with the index of the ﬁrst (largest) eigenvalue and keep adding indices of the smaller eigenvalues
to it until the ratio of the maximum to the minimum eigenvalue exceeds ˆg or until the minimum
eigenvalue goes below a “zero threshold”  λthresh. Then  we estimate the ﬁrst cluster’s subspace 
range(G1) by computing the top ˆr1 eigenvectors of ˆD1. To get the second cluster and its subspace 
we project the next set of α yt’s orthogonal to ˆG1 followed by repeating the above procedure. This
is repeated for each k > 1. The algorithm stops when ˆλˆrk+1 < λthresh.
Algorithm 1 is related to  but signiﬁcantly different from  the ones introduced in [3  5] for the
subspace deletion step of ReProCS. The one introduced in [3] assumed that the clusters were known
to the algorithm (which is unrealistic). The one studied in [5] has an automatic cluster estimation
approach  but  one that needs a larger lower bound on α compared to what Algorithm 1 needs.
3.3 Main result
We give the performance guarantee for Algorithm 1 here. Its parameters are set as follows. We set
ˆg to a value that is a little larger than g. This is needed to allow for the fact that ˆλi is not equal to
the i-th eigenvalue of Λ but is within a small margin of it. For the same reason  we need to also use
a nonzero “zeroing” threshold  λthresh  that is larger than zero but smaller than λ−. We set α large
enough to ensure that SE( ˆP   P ) ≤ rζ holds with a high enough probability.
Pick a ζ so that r2ζ ≤
Theorem 3.3 (cluster-EVD result). Consider Algorithm 1.
0.0001  and r2ζf ≤ 0.01. Suppose that yt’s satisfy (2) and the following hold.

1. Assumption 1.1 and Assumption 3.2 on (cid:96)t hold with χ+ satisfying χ+ ≤ min(1 − rζ −

0.08
0.25  

α0 := Cη2 r2(11 log n + log ϑ)

1.01g++0.0001 − 0.0001). Deﬁne
g+−0.0001
(cid:112)
(cid:18) (1 − rζ − χ+)

q2f  q(rζ)f  (rζ)2f  q

(rζ)2

(cid:19)2

f g+)2  C :=
2. Assumption 1.2 on Mt holds with α ≥ α0 and with β satisfying

f g+  (rζ)

max(g+  qg+ 

(cid:112)
(cid:18) (rkζ)2

≤

β
α

min

4.1(qg+)2  

(rkζ)
q2f

2

6

32 · 16
0.012 .

(cid:19)

.

3. Set algorithm parameters ˆg = 1.01g+ + 0.0001  λthresh = 0.95λ− and α ≥ α0.

Then  with probability at least 1 − 12n−10  SE( ˆP   P ) ≤ rζ.

(rζ)2

Proof: The proof is given in Section 9 in Supplementary Material.
We can also get corollaries for PCA-missing and PCA-SDDC for cluster-EVD. We have given one
speciﬁc value for ˆg and λthresh in Theorem 3.3 for simplicity. One can  in fact  set ˆg to be anything
that satisﬁes (12) given in Supplementary Material and one can set λthresh to be anything satisfying
5rζλ− ≤ λthresh ≤ 0.95λ−. Also  it should be possible to reduce the sample complexity of cluster-
EVD to c max(q2(g+)2r log n  (g+)2(r + log n)) using the approach explained in Sec. 2.
4 Discussion
√
Comparing simple-EVD and cluster-EVD. Consider the lower bounds on α. In the cluster-EVD
(c-EVD) result  Theorem 3.3  if q is small enough (e.g.  if q ≤ 1/
f)  and if (r2ζ)f ≤ 0.01 
it is clear that the maximum in the max(.  .  .  .) expression is achieved by (g+)2. Thus  in this
regime  c-EVD needs α ≥ C r2(11 log n+log ϑ)
g2 and its sample complexity is ϑα. In the EVD result
(Theorem 2.1)  g+ gets replaced by f and ϑ by 1  and so  its sample complexity  α ≥ C r211 log n
(rζ)2 f 2.
In situations where the condition number f is very large but g+ is much smaller and ϑ is small (the
clustering assumption holds well)  the sample complexity of c-EVD will be much smaller than that
of simple-EVD. However  notice that  the lower bound on α for simple-EVD holds for any q < 1
√
and for any ζ with rζ < 0.01 while the c-EVD lower bound given above holds only when q is small
enough  e.g.  q = O(1/
f )  and ζ is small enough  e.g.  rζ = O(1/f ). This tighter bound on ζ
is needed because the error of the k-th step of c-EVD depends on the errors of the previous steps
times f. Secondly  the c-EVD result also needs χ+ and ϑ to be small (clustering assumption holds
well)  whereas  for simple-EVD  by deﬁnition  χ+ = 0 and ϑ = 1. Another thing to note is that the
constants in both lower bounds are very large with the c-EVD one being even larger.
To compare the upper bounds on β  assume that the same α is used by both 
i.e.  α =
max(α0(EVD)  α0(c-EVD)). As long as rk is large enough  χ+ is small enough  and g is small
enough  the upper bound on β needed by the c-EVD result is signiﬁcantly looser. For example  if
χ+ = 0.2  ϑ = 2  rk = r/2  then c-EVD needs β ≤ (0.5 · 0.79 · 0.5)2 (rζ)2
4.1q2g2 α while simple-EVD
needs β ≤ (0.5 · 0.99)2 (rζ)2
Comparison with other results for PCA-SDDC and PCA-missing. To our knowledge  there is no
other result for correlated-PCA. Hence  we provide comparisons of the corollaries given above for
the PCA-missing and PCA-SDDC special cases with works that also study these or related problems.
An alternative solution for either PCA-missing or PCA-SDDC is to ﬁrst recover the entire matrix L
and then compute its subspace via SVD on the estimated L. For the PCA-missing problem  this can
be done by using any of the low-rank matrix completion techniques  e.g.  nuclear norm minimization
(NNM) [13] or alternating minimization (Alt-Min-MC) [23]. Similarly  for PCA-SDDC  this can be
done by solving any of the recent provably correct RPCA techniques such as principal components’
pursuit (PCP) [14  15  16] or alternating minimization (Alt-Min-RPCA) [17].
However  as explained earlier doing the above has two main disadvantages. The ﬁrst is that it is
much slower (see Sec. 5). The difference in speed is most dramatic when solving the matrix-sized
convex programs such as NNM or PCP  but even the Alt-Min methods are slower. If we use the time
complexity from [17]  then ﬁnding the span of the top k singular vectors of an n × m matrix takes
O(nmk) time. Thus  if ϑ is a constant  both simple-EVD and c-EVD need O(nαr) time  whereas 
Alt-Min-RPCA needs O(nαr2) time per iteration [17]. The second disadvantage is that the above
methods for MC or RPCA need more assumptions to provably correctly recover L. All the above
methods need an incoherence assumption on both the left singular vectors  P   and the right singular
vectors  V   of L. Of course  it is possible that  if one studies these methods with the goal of only
recovering the column space of L correctly  the incoherence assumption on the right singular vectors
is not needed. From simulation experiments (see Sec. 5)  the incoherence of the left singular vectors
is deﬁnitely needed. On the other hand  for the PCA-SDDC problem  simple-EVD or c-EVD do not
even need the incoherence assumption on P .
The disadvantage of both EVD and c-EVD  or in fact of any solution for the PCA problem  is that
they work only when q is small enough (the corrupting noise is small compared to (cid:96)t).

4.1q2f 2 α. If g = 3 but f = 100  clearly the c-EVD bound is looser.

7

Mean Subspace Error (SE)

Average Execution Time

A-M-RPCA c-EVD
0.0549
0.0613

1.0000
0.4846

EVD
0.0255
0.0223

PCP
0.2361
1.6784

A-M-RPCA

c-EVD
0.0908
0.3626

EVD
0.0911
0.3821

PCP
1.0000
0.4970

0.0810
5.5144

Expt 1
Expt 2
Table 1: Comparison of SE( ˆP   P ) and execution time (in seconds). A-M-RPCA: Alt-Min-RPCA. Expt 1:
simulated data  Expt 2: lake video with simulated foreground.
5 Numerical Experiments
We use the PCA-SDDC problem as our case study example. We compare EVD and cluster-EVD
(c-EVD) with PCP [15]  solved using [24]  and with Alt-Min-RPCA [17] (implemented using code
from the authors’ webpage). For both PCP and Alt-Min-RPCA  ˆP is recovered as the top r eigenvec-
tors of of the estimated L. To show the advantage of EVD or c-EVD  we let (cid:96)t = P at with columns
of P being sparse. These were chosen as the ﬁrst r = 5 columns of the identity matrix. We gen-
erate at’s iid uniformly with zero mean and covariance matrix Λ = diag(100  100  100  0.1  0.1).
Thus the condition number f = 1000. The clustering assumption holds with ϑ = 2  g+ = 1 and
χ+ = 0.001. The noise wt is generated as wt = ITtMs t(cid:96)t with Tt generated to satisfy Assumption
1.3 with s = 5  ρ = 2  and ˜β = 1; and the entries of Ms t being iid N (0  q2) with q = 0.01. We
used n = 500. EVD and c-EVD (Algorithm 1) were implemented with α = 300  λthresh = 0.095 
ˆg = 3. 10000-time Monte Carlo averaged values of SE( ˆP   P ) and execution time are shown in the
ﬁrst row of Table 1. Since the columns of P are sparse  both PCP and Alt-Min-RPCA fail. Both
have average SE close to one whereas the average SE of c-EVD and EVD is 0.0908 and 0.0911
respectively. Also  both EVD and c-EVD are much faster than the other two. We also did an exper-
iment with the settings of this experiment  but with P dense. In this case  EVD and c-EVD errors
were similar  but PCP and Alt-Min-RPCA errors were less than 10−5.
For our second experiment  we used images of a low-rankiﬁed real video sequence as (cid:96)t’s.
We chose the escalator sequence from http://perception.i2r.a-star.edu.sg/bk_
model/bk_index.html since the video changes are only in the region where the escalator
moves (and hence can be modeled as being sparse). We made it exactly low-rank by retaining
its top 5 eigenvectors and projecting onto their subspace. This resulted in a data matrix L of size
n × 2α with n = 20800 and α = 50  of rank r = 5. We overlaid a simulated moving foreground
block on it. The intensity of the moving block was controlled to ensure that q is small. We used
α = 50 for EVD and c-EVD. We let P be the eigenvectors of the low-rankiﬁed video with nonzero
eigenvalues and computed SE( ˆP   P ). The errors and execution time are displayed in the second
row of Table 1. Since n is very large  the difference in speed is most apparent in this case.
Thus c-EVD outperforms PCP and AltMinRPCA when columns of P are sparse. It also outperforms
EVD but the advantage in mean error is not as much as our theorems predict. One reason is that the
constant in the required lower bounds on α is very large. It is hard to pick an α that is this large and
still only O(log n) unless n is very large. Secondly  both guarantees are only sufﬁcient conditions.
6 Conclusions and Future Work
We studied the problem of PCA in noise that is correlated with the data (data-dependent noise). We
obtained sample complexity bounds for the most commonly used PCA solution  simple EVD. We
also developed and analyzed a generalization of EVD  called cluster-EVD  that has lower sample
complexity under extra assumptions. We provided a detailed comparison of our results with those
for other approaches to solving its example applications - PCA with missing data and PCA with
sparse data-dependent corruptions.
We used matrix Hoeffding [20] to obtain our results. As explained in Sec. 2  we can signiﬁcantly
improve the sample complexity bounds if this is replaced by [21  Theorem 5.39] and matrix Bern-
stein at appropriate places. We have obtained such a result in ongoing work [25]. Moreover  as done
in [5] (for ReProCS)  the mutual independence of (cid:96)t’s can be easily replaced by a more practical as-
sumption of (cid:96)t’s following autoregressive model with almost no change to our assumptions. Thirdly 
by generalizing the proof techniques developed here  we can also study the problem of correlated-
PCA with partial subspace knowledge. The solution to the latter problem helps to greatly simplify
the proof of correctness of ReProCS for online dynamic RPCA. The boundedness assumption on
(cid:96)t’s can be replaced by a Gaussian or a well-behaved sub-Gaussian assumption but this will increase
the sample complexity to O(n). Finally  an open-ended question is how we relax Assumption 1.2
on Mt and still get results similar to Theorem 2.1 or Theorem 3.3.

8

References
[1] B. Nadler  “Finite sample approximation results for principal component analysis: A matrix

perturbation approach ” The Annals of Statistics  vol. 36  no. 6  2008.

[2] C. Qiu and N. Vaswani  “Real-time robust principal components’ pursuit ” in Allerton Conf.

on Communication  Control  and Computing  2010.

[3] C. Qiu  N. Vaswani  B. Lois  and L. Hogben  “Recursive robust pca or recursive sparse recovery

in large but structured noise ” IEEE Trans. Info. Th.  pp. 5007–5039  August 2014.

[4] B. Lois and N. Vaswani  “Online matrix completion and online robust pca ” in IEEE Intl.

Symp. Info. Th. (ISIT)  2015.

[5] J. Zhan  B. Lois  H. Guo  and N. Vaswani  “Online (and Ofﬂine) Robust PCA: Novel Algo-

rithms and Performance Guarantees ” in Intnl. Conf. Artif. Intell. and Stat. (AISTATS)  2016.

[6] R. Arora  A. Cotter  and N. Srebro  “Stochastic optimization of pca with capped msg ” in Adv.

Neural Info. Proc. Sys. (NIPS)  2013  pp. 1815–1823.

[7] O. Shamir 

“A stochastic pca and svd algorithm with an exponential convergence rate ”

arXiv:1409.2848  2014.

[8] C. Boutsidis  D. Garber  Z. Karnin  and E. Liberty  “Online principal components analysis ” in

Proc. ACM-SIAM Symposium on Discrete Algorithms (SODA)  2015  pp. 887–901.

[9] A. Balsubramani  S. Dasgupta  and Y. Freund  “The fast convergence of incremental pca ” in

Adv. Neural Info. Proc. Sys. (NIPS)  2013  pp. 3174–3182.

[10] Z. Karnin and E. Liberty  “Online pca with spectral bounds ” in Proce. Conference on Com-

putational Learning Theory (COLT)  2015  pp. 505–509.

[11] I. Mitliagkas  C. Caramanis  and P. Jain  “Memory limited  streaming pca ” in Adv. Neural

Info. Proc. Sys. (NIPS)  2013  pp. 2886–2894.

[12] M. Fazel  “Matrix rank minimization with applications ” PhD thesis  Stanford Univ  2002.
[13] E. J. Candes and B. Recht  “Exact matrix completion via convex optimization ” Found. of

Comput. Math    no. 9  pp. 717–772  2008.

[14] E. J. Cand`es  X. Li  Y. Ma  and J. Wright  “Robust principal component analysis? ” Journal of

ACM  vol. 58  no. 3  2011.

[15] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A. S. Willsky  “Rank-sparsity incoherence

for matrix decomposition ” SIAM Journal on Optimization  vol. 21  2011.

[16] D. Hsu  S.M. Kakade  and T. Zhang  “Robust matrix decomposition with sparse corruptions ”

IEEE Trans. Info. Th.  Nov. 2011.

[17] P. Netrapalli  U N Niranjan  S. Sanghavi  A. Anandkumar  and P. Jain  “Non-convex robust

pca ” in Neural Info. Proc. Sys. (NIPS)  2014.

[18] Jussi Gillberg  Pekka Marttinen  Matti Pirinen  Antti J Kangas  Pasi Soininen  Mehreen Ali 
Aki S Havulinna  Marjo-Riitta Marjo-Riitta J¨arvelin  Mika Ala-Korpela  and Samuel Kaski 
“Multiple output regression with latent noise ” Journal of Machine Learning Research  2016.
[19] C. Davis and W. M. Kahan  “The rotation of eigenvectors by a perturbation. iii ” SIAM J.

Numer. Anal.  vol. 7  pp. 1–46  Mar. 1970.

[20] J. A. Tropp  “User-friendly tail bounds for sums of random matrices ” Found. Comput. Math. 

vol. 12  no. 4  2012.

[21] R. Vershynin  “Introduction to the non-asymptotic analysis of random matrices ” Compressed

sensing  pp. 210–268  2012.

[22] G. H. Golub and H. A. Van der Vorst  “Eigenvalue computation in the 20th century ” Journal

of Computational and Applied Mathematics  vol. 123  no. 1  pp. 35–65  2000.

[23] P. Netrapalli  P. Jain  and S. Sanghavi  “Low-rank matrix completion using alternating mini-

mization ” in Symposium on Theory of Computing (STOC)  2013.

[24] Z. Lin  M. Chen  and Y. Ma  “Alternating direction algorithms for l1 problems in compressive

sensing ” Tech. Rep.  University of Illinois at Urbana-Champaign  November 2009.

[25] N. Vaswani  “PCA in Data-Dependent Noise (Correlated-PCA): Improved Finite Sample Guar-

antees ” to be posted on arXiV  2017.

9

,Namrata Vaswani
Han Guo