2018,Fast greedy algorithms for dictionary selection with generalized sparsity constraints,In dictionary selection  several atoms are selected from finite candidates that successfully approximate given data points in the sparse representation. We propose a novel efficient greedy algorithm for dictionary selection. Not only does our algorithm work much faster than the known methods  but it can also handle more complex sparsity constraints  such as average sparsity. Using numerical experiments  we show that our algorithm outperforms the known methods for dictionary selection  achieving competitive performances with dictionary learning algorithms in a smaller running time.,Fast greedy algorithms for dictionary selection

with generalized sparsity constraints

Kaito Fujii

Graduate School of Information Sciences and Technology

The University of Tokyo

kaito_fujii@mist.i.u-tokyo.ac.jp

Tasuku Soma

Graduate School of Information Sciences and Technology

The University of Tokyo

tasuku_soma@mist.i.u-tokyo.ac.jp

Abstract

In dictionary selection  several atoms are selected from ﬁnite candidates that suc-
cessfully approximate given data points in the sparse representation. We propose
a novel efﬁcient greedy algorithm for dictionary selection. Not only does our
algorithm work much faster than the known methods  but it can also handle more
complex sparsity constraints  such as average sparsity. Using numerical experi-
ments  we show that our algorithm outperforms the known methods for dictionary
selection  achieving competitive performances with dictionary learning algorithms
in a smaller running time.

1

Introduction

Learning sparse representations of data and signals has been extensively studied for the past decades in
machine learning and signal processing [16]. In these methods  a speciﬁc set of basis signals (atoms) 
called a dictionary  is required and used to approximate a given signal in a sparse representation. The
design of a dictionary is highly nontrivial  and many studies have been devoted to the construction of
a good dictionary for each signal domain  such as natural images and sounds. Recently  approaches
to construct a dictionary from data have shown the state-of-the-art results in various domains. The
standard approach is called dictionary learning [3  32  1]. Although many studies have been devoted
to dictionary learning  it is usually difﬁcult to solve  requiring a non-convex optimization problem
that often suffers from local minima. Also  standard dictionary learning methods (e.g.  MOD [14] or
k-SVD [2]) require a heavy time complexity.
Krause and Cevher [22] proposed a combinatorial analogue of dictionary learning  called dictionary
selection. In dictionary selection  given a ﬁnite set of candidate atoms  a dictionary is constructed
by selecting a few atoms from the set. Dictionary selection could be faster than dictionary learning
due to its discrete nature. Another advantage of dictionary selection is that the approximation
guarantees hold even in agnostic settings  i.e.  we do not need stochastic generating models of the
data. Furthermore  dictionary selection algorithms can be used for media summarization  in which
the atoms must be selected from given data points [8  9].
The basic dictionary selection is formalized as follows. Let V be a ﬁnite set of candidate atoms and
n = |V |. Throughout the paper  we assume that the atoms are unit vectors in Rd without loss of
generality. We represent the candidate atoms as a matrix A ∈ Rd×n whose columns are the atoms in
V . Let yt ∈ Rd (t ∈ [T ]) be data points  where [T ] = {1  . . .   T}  and k and s be positive integers

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

T(cid:88)

t=1

h(X) =

max

with k ≥ s. We assume that a utility function u : Rd×Rd → R+ exists  which measures the similarity
of the input vectors. For example  one can use the (cid:96)2-utility function u(y  x) = (cid:107)y(cid:107)2
2 as
in Krause and Cevher [22]. Then  the dictionary selection ﬁnds a set X ⊆ V of size k that maximizes

2 − (cid:107)y − x(cid:107)2

u(yt  AX w) 

w∈Rk : (cid:107)w(cid:107)0≤s

we can rewrite this as the following two-stage optimization: h(X) =(cid:80)T

(1)
where (cid:107)w(cid:107)0 is the number of nonzero entries in w and AX is the column submatrix of A with
respect to X. That is  we approximate a data point yt with a sparse representation in atoms in X 
where the approximation quality is measured by u. Letting ft(Zt) := maxw u(yt  AZtw) (t ∈ [T ]) 
t=1 maxZt⊆X : |Zt|≤s ft(Zt).
Here Zt is the set of atoms used in a sparse representation of data point yt. The main challenges in
dictionary selection are that the evaluation of h is NP-hard in general [25]  and the objective function
h is not submodular [17] and therefore the well-known greedy algorithm [27] cannot be applied. The
previous approaches construct a good proxy of dictionary selection that can be easily solved  and
analyze the approximation ratio.

1.1 Our contribution

Our main contribution is a novel and efﬁcient algorithm called the replacement orthogonal matching
pursuit (Replacement OMP) for dictionary selection. This algorithm is based on a previous approach
called Replacement Greedy [30] for two-stage submodular maximization  a similar problem to
dictionary selection. However  the algorithm was not analyzed for dictionary selection. We extend
their approach to dictionary selection in the present work  with an additional improvement that exploits
techniques in orthogonal matching pursuit. We compare our method with the previous methods in
Table 1. Replacement OMP has a smaller running time than SDSOMP [10] and Replacement
Greedy. The only exception is SDSMA [10]  which intuitively ignores any correlation of the atoms.
In our experiment  we demonstrate that Replacement OMP outperforms SDSMA in terms of
test residual variance. We note that the constant approximation ratios of SDSMA  Replacement
Greedy  and Replacement OMP are incomparable in general. In addition  we demonstrate that
Replacement OMP achieves a competitive performance with dictionary learning algorithms in a
smaller running time  in numerical experiments.

in which they add a global constraint(cid:80)T

Generalized sparsity constraint
Incorporating further prior knowledge on the data domain often
improves the quality of dictionaries [28  29  11]. A typical example is a combinatorial constraint
independently imposed on each support Zt. This can be regarded as a natural extension of the
structured sparsity [19] in sparse regression  which requires the support to satisfy some combinatorial
constraint  rather than a cardinality constraint. A global structure of supports is also useful prior
information. Cevher and Krause [6] proposed a global sparsity constraint called the average sparsity 
t=1|Zt| ≤ s(cid:48). Intuitively  the average sparsity constraint
requires that the most data points can be represented by a small number of atoms. If the data points
are patches of a natural image  most patches are a simple background  and therefore the number
of the total size of the supports must be small. The average sparsity has been also intensively
studied in dictionary learning [11]. To deal with these generalized sparsities in a uniﬁed manner 
we propose a novel class of sparsity constraints  namely p-replacement sparsity families. We prove
that Replacement OMP can be applied for the generalized sparsity constraint with a slightly worse
approximation ratio. We emphasize that the OMP approach is essential for efﬁciency; in contrast 
Replacement Greedy cannot be extended to the average sparsity setting because it can only handle
local constraints on Zt  and yields an exponential running time.

Online extension In some practical situations  it is not always feasible to store all data points yt 
but these data points arrive in an online fashion. We show that Replacement OMP can be extended
to the online setting  with a sublinear approximate regret. The details are given in Section 5.

1.2 Related work

Krause and Cevher [22] ﬁrst introduced dictionary selection as a combinatorial analogue of dictionary
learning. They proposed SDSMA and SDSOMP  and analyzed the approximation ratio using the
coherence of the matrix A. Das and Kempe [10] introduced the concept of the submodularity ratio

2

Table 1: Comparison of known methods with Replacement OMP. The constants ms  Ms  and
Ms 2 are the restricted concavity and smoothness constants of u(yt ·) (t ∈ [T ]); see Section 2. The
running time is from the (cid:96)2-utility function u and the individual sparsity constraint.

Method

SDSMA [22]
SDSOMP [22]

Replacement Greedy [30]

Replacement OMP

m1ms
M1Ms

Approximation ratio
(1 − 1/e) [10]
O(1/k) [10]
1 − exp
1 − exp

(cid:16)− Ms 2
(cid:16)− Ms 2

(cid:17)2(cid:16)
(cid:17)2(cid:16)

m2s

O((k + d)nT )

Running time

Generalized
sparsity
No
O((s + k)sdknT ) No
No

O(s2dknT )

O((n + ds)kT )

Yes

(cid:17)(cid:17)
(cid:17)(cid:17)

m2s

(cid:16) m2s
(cid:16) m2s

Ms 2

Ms 2

and reﬁned the analysis via the restricted isometry property [5]. A connection to the restricted
concavity and submodularity ratio has been investigated by Elenberg et al. [13]  Khanna et al. [21]
for sparse regression and matrix completion. Balkanski et al. [4] studied two-stage submodular
maximization as a submodular proxy of dictionary selection  devising various algorithms. Stan et al.
[30] proposed Replacement Greedy for two-stage submodular maximization. It is unclear that
these methods provide an approximation guarantee for the original dictionary selection.
To the best of our knowledge  there is no existing research in the literature that addresses online
dictionary selection. For a related problem in sparse optimization  namely online linear regression 
Kale et al. [20] proposed an algorithm based on supermodular minimization [23] with a sublinear
approximate regret guarantee. Elenberg et al. [12] devised a streaming algorithm for weak submodular
function maximization. Chen et al. [7] dealt with online maximization of weakly DR-submodular
functions.

Organization The rest of this paper is organized as follows. Section 2 provides the basic concepts
and deﬁnitions. Section 3 formally deﬁnes dictionary selection with generalized sparsity constraints.
Section 4 presents our algorithm  Replacement OMP. Section 5 sketches the extension to the online
setting. The experimental results are presented in Section 6.

2 Preliminaries
Notation For a positive integer n  [n] denotes the set {1  2  . . .   n}. The sets of reals and nonneg-
ative reals are denoted by R and R≥0  respectively. We similarly deﬁne Z and Z≥0. Vectors and
matrices are denoted by lower and upper case letters in boldface  respectively: a  x  y for vectors and
A  X  Y for matrices. The ith standard unit vector is denoted by ei; that is  ei is the vector such that
its ith entry is equal to one and all other entries are zero. For a matrix A ∈ Rd×n and X ⊆ [n]  AX
denotes the column submatrix of A with respect to X. The maximum and minimum singular values
of a matrix A are denoted by σmax(A) and σmin(A)  respectively. For a positive integer k  we deﬁne
σmax(A  k) := maxX⊆[n] : |X|≤k σmax(AX ). We deﬁne σmin(A  k) in a similar way. For t ∈ [T ] 
denote the maximizer of ut(w) subject to supp(w) ⊆ Zt.
let ut(w) := u(yt  Aw). Let w(Zt)
Throughout the paper  V denotes the ﬁxed ﬁnite ground set. For X ⊆ V and a ∈ V \ X  we deﬁne
X + a := X ∪ {a}. Similarly  for a ∈ V \ X and b ∈ X  we deﬁne X − b + a := (X \ {b}) ∪ {a}.

t

2.1 Restricted concavity and smoothness

The following concept of restricted strong concavity and smoothness is crucial in our analysis.
Deﬁnition 2.1 (Restricted strong concavity and restricted smoothness [26]). Let Ω be a subset of
Rd × Rd and u : Rd → R be a continuously differentiable function. We say that u is restricted
strongly concave with parameter mΩ and restricted smooth with parameter MΩ if 

(cid:107)y − x(cid:107)2

2 ≥ u(y) − u(x) − (cid:104)∇u(x)  y − x(cid:105) ≥ − MΩ
2

− mΩ
2
for all (x  y) ∈ Ω.
We deﬁne Ωs p := {(x  y) ∈ Rd × Rd : (cid:107)x(cid:107)0 (cid:107)y(cid:107)0 ≤ s (cid:107)x− y(cid:107)0 ≤ p} and Ωs := Ωs s for positive
integers s and p. We often abbreviate MΩs  MΩs p  and mΩs as Ms  Ms p  and ms  respectively.

(cid:107)y − x(cid:107)2

2

3

3 Dictionary selection with generalized sparsity constraints

In this section  we formalize our problem  dictionary selection with generalized sparsity constraints.
In this setting  the supports Zt for each t ∈ [T ] cannot be independently selected  but we impose
(cid:81)T
a global constraint on them. We formally write such constraints as a down-closed 1 family I ⊆
t=1 2V . Therefore  we aim to ﬁnd X ⊆ V with |X| ≤ k maximizing

T(cid:88)

t=1

h(X) =

Z1 ... Zt⊆X : (Z1 ... Zt)∈I

max

ft(Zt)

(2)

Since a general down-closed family is too abstract  we focus on the following class. First  we deﬁne
the set of feasible replacements for the current support Z1 ···   ZT and an atom a as

Fa(Z1 ···   ZT ) = {(Z(cid:48)

1 ···   Z(cid:48)

T ) ∈ I : Z(cid:48)

t ⊆ Zt + a  |Zt \ Z(cid:48)

(3)
That is  the set of members in I obtained by adding a and removing at most one element from each
a∈V Fa(Z1 ···   ZT ). If Z1  . . .   ZT are clear from the context  we
simply write it as Fa.

Zt. Let F(Z1 ···   ZT ) =(cid:83)
Deﬁnition 3.1 (p-replacement sparsity). A sparsity constraint I ⊆ (cid:81)T

t| ≤ 1 (∀t ∈ [T ])} .

T ) ∈ F(Z1  . . .   ZT ) (p(cid:48) ∈ [p]) such that each element in Z∗

p(cid:48)=1 and each element in Zt \ Z∗

t=1 2V is p-replacement
T ) ∈ I  there is a sequence of p feasible replacements
t \ Zt appears at least once
t appears at most once in the sequence

1   . . .   Z∗

sparse if for any (Z1  . . .   ZT )  (Z∗
(Z p(cid:48)
1   . . .   Z p(cid:48)
in the sequence (Z p(cid:48)
(Zt \ Z p(cid:48)
t )p

t \ Zt)p

p(cid:48)=1.

The following sparsity constraints are all p-replacement sparsity families. See Appendix B for proof.
Example 3.2 (individual sparsity). The sparsity constraint for the standard dictionary selection can
be written as I = {(Z1 ···   ZT ) | |Zt| ≤ s (∀t ∈ [T ])}. We call it the individual sparsity constraint.
This constraint is a special case of an individual matroid constraint  described below.
Example 3.3 (individual matroids). This was proposed by [30] as a sparsity constraint for two-stage
submodular maximization. An individual matroid constraint can be written as I = {(Z1 ···   ZT ) |
Zt ∈ It (∀t ∈ [T ])} where (V It) is a matroid2 for each t ∈ [T ]. An individual sparsity constraint is
a special case of an individual matroid constraint where (V It) is the uniform matroid for all t.
Example 3.4 (block sparsity). Block sparsity was proposed by Krause and Cevher [22]. This sparsity
requires that the support must be sparse within each prespeciﬁed block. That is  disjoint blocks
B1 ···   Bb ⊆ [T ] of data points are given in advance  and an only small subset of atoms can be used
t∈Bb(cid:48) Zt| ≤ sb(cid:48) (∀b(cid:48) ∈ [b])} where sb(cid:48) ∈ Z≥0 for
each b(cid:48) ∈ [b] are sparsity parameters.
Example 3.5 (average sparsity [6]). This sparsity imposes a constraint on the average number of
used atoms among all data points. The number of atoms used for each data point is also restricted.
t=1 |Zt| ≤ s(cid:48)} where st ∈ Z≥0 for each t ∈ [T ] and
s(cid:48) ∈ Z≥0 are sparsity parameters.
Proposition 3.6. The replacement sparsity parameters of individual matroids  block sparsity  and
average sparsity are upper-bounded by k  k  and 3k − 1  respectively.

in each block. Formally  I = {(Z1 ···   ZT ) | |(cid:83)
Formally  I = {(Z1 ···   ZT ) | |Zt| ≤ st (cid:80)T

4 Algortihms

In this section  we present Replacement Greedy [30] and Replacement OMP for dictionary
selection with generalized sparsity constraints.

1A set family I is said to be down-closed if X ∈ I and Y ⊆ X then Y ∈ I.
2A matroid is a pair of a ﬁnite ground set V and a non-empty down-closed family I ⊆ 2V that satisfy that

for all Z  Z(cid:48) ∈ I with |Z| < |Z(cid:48)|  there is an element a ∈ Z(cid:48) \ Z such that Z ∪ {a} ∈ I

4

4.1 Replacement Greedy

T(cid:88)

Replacement Greedy was ﬁrst proposed as an algorithm for a different problem  two-stage sub-
modular maximization [4]. In two-stage submodular maximization  the goal is to maximize

t=1

max

ft(Zt) 

h(X) =

Zt⊆X : Zt∈It

(4)
where ft is a nonnegative monotone submodular function (t ∈ [T ]) and It is a matroid. Despite the
similarity of the formulation  in dictionary selection  the functions ft are not necessarily submodular 
but come from the continuous function ut. Furthermore  in two-stage submodular maximization  the
constraints on Zt are individual for each t ∈ [T ]  while we pose a global constraint I. In the following 
we present an adaptation of Replacement Greedy to dictionary selection with generalized sparsity
constraints.
Replacement Greedy stores the current dictionary X and supports Zt ⊆ X such that
(Z1  . . .   ZT ) ∈ I  which are initialized as X = ∅ and Zt = ∅ (t ∈ [T ]). At each step  the
algorithm considers the gain of adding an element a ∈ V to X with respect to each function ft 
t) − f (Zt)}. See Algo-
i.e.  the algorithm selects a that maximizes max(Z(cid:48)
rithm 1 for a pseudocode description. Note that for the individual matroid constraint I  the algorithm
coincides with the original Replacement Greedy [30].

(cid:80)T
t=1{ft(Z(cid:48)

T )∈Fa

1 ... Z(cid:48)

Algorithm 1 Replacement Greedy & Replacement OMP
1: Initialize X ← ∅ and Zt ← ∅ for t = 1  . . .   T .
2: for i = 1  . . .   k do
3:

Pick a∗ ∈ V that maximizes

(cid:40)max(Z(cid:48)

T )∈Fa∗(cid:80)T
(cid:110) 1

1 ···  Z(cid:48)
T )∈Fa∗
1 ···  Z(cid:48)
1 ···   Z(cid:48)

max(Z(cid:48)
and let (Z(cid:48)
Set X ← X + a∗ and Zt ← Z(cid:48)

Ms 2

(cid:80)T
t) − ft(Zt)}
t=1 {ft(Z(cid:48)
t=1(cid:107)∇ut(w(Zt)

t

(Replacement Greedy)
t\Zt(cid:107)2 − Ms 2
)Z(cid:48)

(cid:80)T
t=1(cid:107)(w(Zt)
(Replacement OMP)

(cid:107)2(cid:111)

)Zt\Z(cid:48)

t

t

T ) be a replacement achieving a maximum.

t for each t ∈ [T ].

4:
5: return X.
Stan et al. [30] showed that Replacement Greedy achieves an ((1 − 1/
e)/2)-approximation
when ft are monotone submodular. We extend their analysis to our non-submodular setting. The
proof can be found in Appendix C.
Theorem 4.1. Assume that ut is m2s-strongly concave on Ω2s and Ms 2-smooth on Ωs 2 for t ∈ [T ]
and that the sparsity constraint I is p-replacement sparse. Let (Z∗
T ) ∈ I be optimal supports
of an optimal dictionary X∗. Then the solution (Z1 ···   ZT ) ∈ I of Replacement Greedy after
k(cid:48) steps satisﬁes

1  ···   Z∗

√

T(cid:88)

t=1

ft(Zt) ≥ m2
2s
M 2
s 2

1 − exp

(cid:18)

(cid:18)
− k(cid:48)

p

(cid:19)(cid:19) T(cid:88)

t=1

Ms 2
m2s

ft(Z∗
t )

4.2 Replacement OMP

heavy computation: in each greedy step  we need to evaluate(cid:80)T

Now we propose our algorithm  Replacement OMP. A down-side of Replacement Greedy is its
t) ∈
Fa(Z1  . . .   Zt)  which amounts to solving linear regression problems snT times if u is the (cid:96)2-utility
function. To avoid heavy computation  we propose a proxy of this quantity by borrowing an idea
from orthogonal matching pursuit. Replacement OMP selects an atom a ∈ V that maximizes

t) for each (Z(cid:48)

1  . . .   Z(cid:48)

(Z(cid:48)

1 ···  Z(cid:48)

T )∈Fa(Z1 ···  ZT )

max

(cid:107)∇ut(w(Zt)

t

t\Zt(cid:107)2 − Ms 2
)Z(cid:48)

(cid:107)(w(Zt)

t

(cid:107)2

)Zt\Z(cid:48)

t

.

(5)

This algorithm requires the smoothness parameter Ms 2 before the execution. Computing Ms 2
is generally difﬁcult  but this parameter for the squared (cid:96)2-utility function can be bounded by
max(A  2). This value can be computed in O(n2d) time.
σ2

5

t=1 ft(Z(cid:48)
T(cid:88)

t=1

(cid:40)

T(cid:88)

1

Ms 2

t=1

(cid:41)

Theorem 4.2. Assume that ut is m2s-strongly concave on Ω2s and Ms 2-smooth on Ωs 2 for t ∈ [T ]
and that the sparsity constraint I is p-replacement sparse. Let (Z∗
T ) ∈ I be optimal
supports of an optimal dictionary X∗. Then the solution (Z1 ···   ZT ) ∈ I of Replacement OMP
after k(cid:48) steps satisﬁes

1  ···   Z∗

T(cid:88)

(cid:18)

(cid:18)
− k(cid:48)

p

(cid:19)(cid:19) T(cid:88)

t=1

Ms 2
m2s

ft(Z∗
t ).

ft(Zt) ≥ m2
2s
M 2
s 2

1 − exp

t=1

4.3 Complexity

t

and ∇ut(w(Zt)

t

t=1 ft(Z(cid:48)

Now we analyze the time complexity of Replacement Greedy and Replacement OMP. In general 
Fa has O(nT ) members  and therefore it is difﬁcult to compute Fa. Nevertheless  we show that
Replacement OMP can run much faster for the examples presented in Section 3.
In Replacement Greedy  it is difﬁcult to ﬁnd an atom with the largest gain at each step. This
t). Conversely  in Replacement
) for all t ∈ [T ]  the problem of calculating gain of

is because we need to maximize a nonlinear function(cid:80)T

OMP  if we can calculate w(Zt)
each atom is reduced to maximizing a linear function.
In the following  we consider the (cid:96)2-utility function and average sparsity constraint because it is the
most complex constraint. A similar result holds for the other examples. In fact  we show that this
task reduces to maximum weighted bipartite matching. The Hungarian method returns the maximum
weight bipartite matching in O(T 3) time. We can further improve the running time to O(T log T )
time by utilizing the structure of this problem. Due to the limitation of space  we defer the details to
Appendix C. In summary  we obtain the following:
Theorem 4.3. Assume that the assumption of Theorem 4.2 holds. Further assume that u is the
(cid:96)2-utility function and I is the average sparsity constraint. Then Replacement OMP ﬁnds the
solution (Z1 ···   ZT ) ∈ I

(cid:18) σ2

(cid:19)2(cid:18)

(cid:18)

ft(Zt) ≥

max(A  2s)
σ2
min(A  2)

1 − exp

− 1
3

σ2
min(A  2)
σ2
max(A  2s)

ft(Z∗
t )

(cid:19)(cid:19) T(cid:88)

t=1

T(cid:88)

t=1

in O(T k(n log T + ds)) time.
Remark 4.4. If ﬁnding an atom with the largest gain is computationally intractable  we can add an
atom whose gain is no less than τ times the largest gain. In this case  we can bound the approximation
ratio with replacing k(cid:48) with τ k(cid:48) in Theorem 4.1 and 4.2.

5 Extensions to the online setting

Our algorithms can be extended to the following online setting. The problem is formalized as a
two-player game between a player and an adversary. At each round t = 1  . . .   T   the player must
select (possibly in a randomized manner) a dictionary Xt ⊆ V with |Xt| ≤ k. Then  the adversary
reveals a data point yt ∈ Rd and the player gains ft(Xt) = maxw∈Rk:(cid:107)w(cid:107)0≤s u(yt  AX w). The
performance measure of a player’s strategy is the expected α-regret:

T(cid:88)

(cid:34) T(cid:88)

(cid:35)

regretα(T ) = α max

X∗:|X∗|≤k

ft(X∗) − E

ft(Xt)

 

t=1

t=1

where α > 0 is a constant independent from T corresponding to the ofﬂine approximation ratio  and
the expectation is taken over the randomness in the player.
For this online setting  we present an extension of Replacement Greedy and Replacement OMP
with sublinear α-regret  where α is the corresponding ofﬂine approximation ratio. The details are
provided in Appendix D.

6 Experiments

In this section  we empirically evaluate our proposed algorithms on several dictionary selection
problems with synthetic and real-world datasets. We use the squared (cid:96)2-utility function for all

6

(a) synthetic  T = 100  time

(b) synthetic  T = 100  residual

(c) voc  T = 100  residual

(d) synthetic  T = 1000  time

(e) synthetic  T = 1000  residual

(f) voc  T = 1000  residual

(g) synthetic  T = 1000  time

(h) synthetic  T = 1000  residual

(i) voc  T = 1000  residual

Figure 1: The experimental results for the ofﬂine setting. In all ﬁgures  the horizontal axis indicates
the size of the output dictionary. (a)  (b)  and (c) are the results for T = 100. (d)  (e)  and (f) are
the results for T = 1000. (g)  (h)  and (i) are the results for T = 1000 with an average sparsity
constraint. For each setting  we provide the plot of the running time for the synthetic dataset  test
residual variance for the synthetic dataset  and test residual variance for VOC2006 image dataset.

of the experiments. Since evaluating the value of the objective function is NP-hard  we plot the
approximated residual variance obtained by orthogonal matching pursuit.

Ground set We use the ground set consisting of several orthonormal bases that are standard choices
in signal and image processing  such as 2D discrete cosine transform and several 2D discrete wavelet
transforms (Haar  Daubechies 4  and coiﬂet). In all of the experiments  the dimension is set to d = 64 
which corresponds to images of size 8 × 8 pixels. The size of the ground set is n = 256.

Machine All the algorithms are implemented in Python 3.6. We conduct the experiments in a
machine with Intel Xeon E3-1225 V2 (3.20 GHz and 4 cores) and 16 GB RAM.

Datasets We conduct experiments on two types of datasets. The ﬁrst one is a synthetic dataset. In
each trial  we randomly pick a dictionary with size k out of the ground set  and generate sparse linear
combinations of the columns of this dictionary. The weights of the linear combinations are generated
from the standard normal distribution. The second one is a dataset of real-world images extracted
from PASCAL VOC2006 image datasets [15]. In each trial  we randomly select an image out of 2618
images and divide it into patches of 8 × 8 pixels  then select T patches uniformly at random. All the
patches are normalized to zero mean and unit variance. We make datasets for training and test in the
same way  and use the training dataset for obtaining a dictionary and the test dataset for measuring
the quality of the output dictionary.

7

2.55.07.510.012.515.017.520.0dictionary size103102101100101102running timeSDS_MASDS_OMPRGRepOMP2.55.07.510.012.515.017.520.0dictionary size0.00.20.40.60.8test residual varianceSDS_MASDS_OMPRGRepOMP2.55.07.510.012.515.017.520.0dictionary size0.40.50.60.70.8test residual varianceSDS_MASDS_OMPRGRepOMP20406080100dictionary size102101100101102running timeSDS_MARepOMPRepOMPdMODK-SVD20406080100dictionary size0.00.10.20.30.40.50.60.70.8test residual varianceSDS_MARepOMPRepOMPdMODK-SVD20406080100dictionary size0.300.350.400.45test residual varianceSDS_MARepOMPRepOMPdMODK-SVD20406080100dictionary size102running timeSDS_MA (average)RepOMP (average)RepOMPd (average)20406080100dictionary size0.00.10.20.30.40.50.60.70.8test residual varianceSDS_MA (average)RepOMP (average)RepOMPd (average)20406080100dictionary size0.2250.2500.2750.3000.3250.3500.3750.400test residual varianceSDS_MA (average)RepOMP (average)RepOMPd (average)(a) synthetic

(b) voc

Figure 2: The experimental results for the online setting. In both ﬁgures  the horizontal axis indicates
the number of rounds. (a) is the result with synthetic datasets  and (b) is the result with VOC2006
image datasets.

6.1 Experiments on the ofﬂine setting

We implement our proposed methods  Replacement Greedy (RG) and Replacement OMP
(RepOMP)  as well as the existing methods for dictionary selection  SDSMA and SDSOMP. We also
implement a heuristically modiﬁed version of RepOMP  which we call RepOMPd. In RepOMPd 
√
we replace Ms 2 with some parameter that decreases as the size of the current dictionary grows 
which prevents the gains of all the atoms from being zero. Here we use Ms 2/
i as the decreasing
parameter where i is the number of iterations so far. In addition  we compare these methods with
standard methods for dictionary learning  MOD [14] and KSVD [2]  which is set to stop when the
change of the objective value becomes no more than 10−6 or 200 iterations are ﬁnished. Orthogonal
matching pursuit is used as a subroutine in both methods.
First  we compare the methods for dictionary selection with small datasets of T = 100. The parameter
of sparsity constraints is set to s = 5. The results averaged over 20 trials are shown in Figure 1(a) 
(b)  and (c). The plot of the running time for VOC2006 datasets is omitted as it is much similar to that
for synthetic datasets. In terms of running time  SDSMA is the fastest  but the quality of the output
dictionary is unsatisfactory. RepOMP is several magnitudes faster than SDSOMP and RG  but its
quality is almost the same with SDSOMP and RG. In Figure 1(b)  test residual variance of SDSOMP 
RG  and RepOMP are overlapped  and in Figure 1(c)  test residual variance of RepOMP is slightly
worse than that of SDSOMP and RG. From these results  we can conclude that RepOMP is by far
the most practical method for dictionary selection.
Next we compare the dictionary selection methods with the dictionary learning methods with larger
datasets of T = 1000. SDSOMP and RG are omitted because they are too slow to be applied to
datasets of this size. The results averaged over 20 trials are shown in Figure 1(d)  (e)  and (f). In
terms of running time  RepOMP and RepOMPd are much faster than MOD and KSVD  but their
performances are competitive with MOD and KSVD.
Finally  we conduct experiments with the average sparsity constraints. We compare RepOMP and
RepOMPd with Algorithm 2 in Appendix C with a variant of SDSMA proposed for average sparsity
in Cevher and Krause [6]. The parameters of constraints are set to st = 8 for all t ∈ [T ] and s(cid:48) = 5T .
The results averaged over 20 trials are shown in Figure 1(g)  (h)  and (i). RepOMP and RepOMPd
outperform SDSMA both in running time and quality of the output.
In Appendix E  We provide further experimental results. There we provide examples of image
restoration  in which the average sparsity works better than the standard dictionary selection.

6.2 Experiments on the online setting

Here we give the experimental results on the online setting. We implement the online version of
SDSMA  RG and RepOMP  as well as an online dictionary learning algorithm proposed by Mairal
et al. [24]. For all the online dictionary selection methods  the hedge algorithm is used as the
subroutines. The parameters are set to k = 20 and s = 5. The results averaged over 50 trials are
shown in Figure 2(a)  (b). For both datasets  Online RepOMP shows a better performance than
Online SDSMA  Online RG  and the online dictionary learning algorithm.

8

0255075100125150175200round0.10.20.30.40.50.60.70.8test residual varianceSDS_MARGRepOMPMairal et al.0255075100125150175200round0.40.50.60.70.8test residual varianceSDS_MARGRepOMPMairal et al.Acknowledgement

The authors would thank Taihei Oki and Nobutaka Shimizu for their stimulating discussions. K.F.
was supported by JSPS KAKENHI Grant Number JP 18J12405. T.S. was supported by ACT-I  JST.
This work was supported by JST CREST  Grant Number JPMJCR14D2  Japan.

References
[1] A. Agarwal  A. Anandkumar  P. Jain  and P. Netrapalli. Learning sparsely used overcomplete
dictionaries via alternating minimization. SIAM Journal on Optimization  26(4):2775–2799 
2016.

[2] M. Aharon  M. Elad  and A. Bruckstein. K-SVD: An algorithm for designing overcomplete
dictionaries for sparse representation. IEEE Transactions on Signal Processing  54(11):4311–
4322  2006.

[3] S. Arora  R. Ge  and A. Moitra. New algorithms for learning incoherent and overcomplete
dictionaries. In Proceedings of the Conference on Learning Theory (COLT)  pages 779–806 
2014.

[4] E. Balkanski  B. Mirzasoleiman  A. Krause  and Y. Singer. Learning sparse combinatorial repre-
sentations via two-stage submodular maximization. In Proceedings of The 33rd International
Conference on Machine Learning (ICML)  pages 2207–2216  2016.

[5] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information

Theory  51(12):4203–4215  2005.

[6] V. Cevher and A. Krause. Greedy dictionary selection for sparse representation. IEEE Journal

of Selected Topics in Signal Processing  5(5):979–988  2011.

[7] L. Chen  H. Hassani  and A. Karbasi. Online continuous submodular maximization. In Proceed-
ings of the 21st International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 
volume 84  pages 1896–1905  2018.

[8] Y. Cong  J. Yuan  and J. Luo. Towards scalable summarization of consumer videos via sparse

dictionary selection. IEEE Transactions on Multimedia  14(1):66–75  2012.

[9] Y. Cong  J. Liu  G. Sun  Q. You  Y. Li  and J. Luo. Adaptive greedy dictionary selection for

web media summarization. IEEE Transactions on Image Processing  26(1):185–195  2017.

[10] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection 
sparse approximation and dictionary selection. In Proceedings of the 28th International Confer-
ence on Machine Learning (ICML)  pages 1057–1064  2011.

[11] B. Dumitrescu and P. Irofti. Dictionary Learning Algorithms and Applications. Springer  2018.

[12] E. Elenberg  A. G. Dimakis  M. Feldman  and A. Karbasi. Streaming weak submodularity:
Interpreting neural networks on the ﬂy. In Advances in Neural Information Processing Systems
(NIPS) 30  pages 4047–4057. 2017.

[13] E. R. Elenberg  R. Khanna  and A. G. Dimakis. Restricted strong convexity implies weak
In Proceedings of NIPS Workshop on Learning in High Dimensions with

submodularity.
Structure  2016.

[14] K. Engan  S. O. Aase  and J. Hakon Husoy. Method of optimal directions for frame design.
In Proceedings of the IEEE International Conference on the Acoustics  Speech  and Signal
Processing  volume 05  pages 2443–2446  1999.

[15] M. Everingham  A. Zisserman  C. K. I. Williams  and L. Van Gool.

CAL Visual Object Classes Challenge 2006 (VOC2006) Results.
network.org/challenges/VOC/voc2006/results.pdf.

The PAS-
http://www.pascal-

[16] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Springer 

2013.

9

[17] S. Fujishige. Submodular Functions and Optimization. Elsevier  2nd edition  2005.

[18] G. H. Golub and C. F. Van Loan. Matrix Computations  volume 3. JHU Press  2012.

[19] J. Huang  T. Zhang  and D. Metaxas. Learning with structured sparsity. The Journal of Machine

Learning Research  12:3371–3412  2009.

[20] S. Kale  Z. Karnin  T. Liang  and D. Pál. Adaptive feature selection: Computationally efﬁcient
online sparse linear regression under RIP. In Proceedings of the 34th International Conference
on Machine Learning (ICML)  pages 1–22  2017.

[21] R. Khanna  E. Elenberg  A. Dimakis  J. Ghosh  and S. Neghaban. On approximation guarantees
for greedy low rank optimization. In Proceedings of the 34th International Conference on
Machine Learning (ICML)  pages 1837–1846  2017.

[22] A. Krause and V. Cevher. Submodular dictionary selection for sparse representation.

In
Proceedings of the 27th International Conference on Machine Learning (ICML)  pages 567–
574  2010.

[23] E. Liberty and M. Sviridenko. Greedy minimization of weakly supermodular set functions. In
Approximation  Randomization  and Combinatorial Optimization. Algorithms and Techniques
(APPROX/RANDOM 2017)  volume 81  pages 19:1–19:11  2017.

[24] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online learning for matrix factorization and sparse

coding. Journal of Machine Learning Research  11:19–60  2010.

[25] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing 

24(2):227–234  1995.

[26] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-
dimensional analysis of m-estimators with decomposable regularizers. Statistical Science  27
(4):538–557  2012.

[27] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher. An analysis of approximations for maximizing

submodular set functions - I. Mathematical Programming  14(1):265–294  1978.

[28] R. Rubinstein  M. Zibulevsky  and M. Elad. Double sparsity: Learning sparse dictionaries for
sparse signal approximation. IEEE Transactions on Signal Processing  58(3):1553–1564  2010.

[29] C. Rusu  B. Dumitrescu  and S. A. Tsaftaris. Explicit shift-invariant dictionary learning. IEEE

Signal Processing Letters  21(1):6–9  2014.

[30] S. Stan  M. Zadimoghaddam  A. Krause  and A. Karbasi. Probabilistic submodular maximization
in sub-linear time. Proceedings of the 34th International Conference on Machine Learning
(ICML)  pages 3241–3250  2017.

[31] M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In

Advances in Neural Information Processing Systems (NIPS)  pages 1577–1584  2009.

[32] M. Zhou  H. Chen  L. Ren  G. Sapiro  L. Carin  and J. W. Paisley. Non-parametric bayesian
In Advances in Neural Information

dictionary learning for sparse image representations.
Processing Systems (NIPS) 22  pages 2295–2303. 2009.

10

,Kaito Fujii
Tasuku Soma