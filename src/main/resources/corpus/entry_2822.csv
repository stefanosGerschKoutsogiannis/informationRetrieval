2019,Large-scale optimal transport map estimation using projection pursuit,This paper studies the estimation of large-scale optimal transport maps (OTM)  which is a well known challenging problem owing to the curse of dimensionality.
Existing literature approximates the large-scale OTM by a series of one-dimensional OTM problems through iterative random projection.
Such methods  however  suffer from slow or none convergence in practice due to the nature of randomly selected projection directions. 
Instead  we propose an estimation method of large-scale OTM by combining the idea of projection pursuit regression and sufficient dimension reduction. 
The proposed method  named projection pursuit Monge map (PPMM)  adaptively selects the most ``informative'' projection direction in each iteration. 
We theoretically show the proposed dimension reduction method can consistently estimate the most ``informative'' projection direction in each iteration. 
Furthermore  the PPMM algorithm weakly convergences to the target large-scale OTM in a reasonable number of steps. 
Empirically  PPMM is computationally easy and converges fast. 
We assess its finite sample performance through the applications of Wasserstein distance estimation and generative models.,Large-scale optimal transport map estimation using

projection pursuit

Cheng Meng1 Yuan Ke1 Jingyi Zhang1 Mengrui Zhang1 Wenxuan Zhong1 Ping Ma1

{cheng.meng25  yuan.ke  jingyi.zhang25  mengrui.zhang  wenxuan  pingma }@uga.edu

1Department of Statistics  University of Georgia

Abstract

This paper studies the estimation of large-scale optimal transport maps (OTM) 
which is a well known challenging problem owing to the curse of dimensionality.
Existing literature approximates the large-scale OTM by a series of one-dimensional
OTM problems through iterative random projection. Such methods  however  suffer
from slow or none convergence in practice due to the nature of randomly selected
projection directions. Instead  we propose an estimation method of large-scale OTM
by combining the idea of projection pursuit regression and sufﬁcient dimension
reduction. The proposed method  named projection pursuit Monge map (PPMM) 
adaptively selects the most “informative” projection direction in each iteration.
We theoretically show the proposed dimension reduction method can consistently
estimate the most “informative” projection direction in each iteration. Furthermore 
the PPMM algorithm weakly convergences to the target large-scale OTM in a
reasonable number of steps. Empirically  PPMM is computationally easy and
converges fast. We assess its ﬁnite sample performance through the applications of
Wasserstein distance estimation and generative models.

1

Introduction

Recently  optimal transport map (OTM) draws great attention in machine learning  statistics  and
computer science due to its close relationship to generative models  including generative adversarial
nets [19]  the “decoder” network in variational autoencoders [27]  among others. In a generative
model  the goal is usually to generate a “fake” sample  which is indistinguishable from the genuine
one. This is equivalent to ﬁnd a transport map φ from random noises with distribution pX (e.g. 
Gaussian distribution or uniform distribution) to the underlying population distribution pY of the
genuine sample  e.g.  the MNIST or the ImageNet dataset. Nowadays  generative models have been
widely-used for generating realistic images [12  33]  songs [4  13] and videos [32  53]. Besides
generative models  OTM also plays essential roles in various machine learning applications  say color
transfer [14  41]  shape match [50]  transfer learning [10  38] and natural language processing [38].
Despite its impressive performance  the computation of OTM is challenging for a large-scale sample
with massive sample size and/or high dimensionality. Traditional methods for estimating the OTM
includes ﬁnding a parametric map and using ordinary differential equations [8  2]. To address the
computational concern  recent developments of OTM estimation have been made based on solving
linear programs [44  37]. Let {xi}n
i=1 ∈ Rd be two samples from two continuous
probability distributions functions pX and pY   respectively. Estimating the OTM from pX to pY by
solving a linear program requiring O(n3 log(n)) computational time for ﬁxed d [38  47]. To alleviate
the computational burden  some literature [11  17  1  21] pursued fast computation approaches of the
OTM objective  i.e.  the Wasserstein distance. Another school of methods aims to estimate the OTM
efﬁciently when d is small  including multi-scale approaches [35  18] and dynamic formulations

i=1 ∈ Rd and {yi}n

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(cid:124)
i θ}n

i=1 and {y

(cid:124)
i θ}n

[48  36]. These methods utilize the space discretization  thus are generally not applicable in high-
dimensional cases.
The random projection method (or known as the radon transformation method) is proposed to estimate
OTMs efﬁciently when d is large [39  40]. Such a method tackles the problem of estimating a d-
dimensional OTM iteratively by breaking down the problem into a series of subproblems  each of
which ﬁnds a one-dimensional OTM using projected samples. Denote Sd−1 as the d-dimensional
unit sphere. In each iteration  a random direction θ ∈ Sd−1 is picked  and the one-dimensional OTM
is then calculated between the projected samples {x
i=1. The collection of all the
one-dimensional maps serves as the ﬁnal estimate of the target OTM. The sliced method modiﬁes
the random projection method by considering a large set of random directions from Sd−1 in each
iteration [7  42]. The “mean map” of the one-dimensional OTMs over these random directions is
considered as a component of the ﬁnal estimate of the target OTM. We call the random projection
method  the sliced method  and their variants as the projection-based approach. Such an approach
reduces the computational cost of calculating an OTM from O(n3 log(n)) to O(Kn log(n))  where
K is the number of iterations until convergence. However  there is no theoretical guideline on the
order of K. In addition  the existing projection-based approaches usually require a large number of
iterations to convergence or even fail to converge. We speculate that the slow convergence is because
a randomly selected projection direction may not be “informative”  leading to a one-dimensional
OTM that failed to be a decent representation of the target OTM. We illustrate such a phenomenon
through an illustrative example as follows.
An illustrative example. The
left and right panels in Figure
1 illustrates the importance of
choosing the “informative” pro-
jection direction in OTM estima-
tion. The goal is to obtain the
OTM φ∗ which maps a source
distribution pX (colored in red)
to a target distribution pY (col-
ored in green). For each panel 
we ﬁrst randomly pick a projec-
tion direction (black arrow) and
obtain the marginal distributions of pX and pY (the bell-shaped curves)  respectively. The one-
dimensional OTM then can be calculated based on the marginal distributions. Applying such a map
to the source distribution yields the transformed distribution (colored in blue). One can observe that
the transformed distributions are signiﬁcantly different from the target ones. Such an observation
indicates that the one-dimensional OTM with respect to a random projection direction may fail to
well-represent the target OTM. This observation motivates us to select the “informative” projection
direction (red arrow)  which yields a better one-dimensional OTM.
Our contributions. To address the issues mentioned above  this paper introduces a novel statistical
approach to estimate large-scale OTMs. The proposed method  named projection pursuit Monge map
(PPMM)  improves the existing projection-based approaches from two aspects. First  PPMM uses a
sufﬁcient dimension reduction technique to estimate the most “informative” projection direction in
each iteration. Second  PPMM is based on projection pursuit [16]. The idea is similar to boosting that
search for the next optimal direction based on the residual of previous ones. Theoretically  we show
the proposed method can consistently estimate the most “informative” projection direction in each
iteration  and the algorithm weakly convergences to the target large-scale OTM in a reasonable number
of steps. The ﬁnite sample performance of the proposed algorithm is evaluated by two applications:
Wasserstein distance estimation and generative model. We show the proposed method outperforms
several state-of-the-art large-scale OTM estimation methods through extensive experiments on various
synthetic and real-world datasets.

Figure 1: Illustration for the “informative” projection direction

2 Problem setup and methodology
Optimal transport map and Wasserstein distance. Denote X ∈ Rd and Y ∈ Rd as two continuous
random variables with probability distribution functions pX and pY   respectively. The problem of
ﬁnding a transport map φ : Rd → Rd such that φ(X) and Y have the same distribution  has been

2

widely-studied in mathematics  probability  and economics  see [14  50  43] for examples of some
new developments. Note that the transport map between the two distributions is not unique. Among
all transport maps  it may be of interest to deﬁne the “optimal” one according to some criteria. A
standard approach  named Monge formulation [52]  is to ﬁnd the OTM1 φ∗ that satisﬁes

φ∗ = inf
φ∈Φ

Rd

(cid:107)X − φ(X)(cid:107)pdpX  

where Φ is the set of all transport maps  (cid:107) · (cid:107) is the vector norm and p is a positive integer. Given the
existence of the Monge map  the Wasserstein distance of order p is deﬁned as

Wp(pX   pY ) =

Denote (cid:98)φ as an estimator of φ∗. Suppose one observe X = (x1  . . .   xn)

(cid:124) ∈ Rn×d and Y =
(cid:124) ∈ Rn×d from pX and pY   respectively. The Wasserstein distance Wp(pX   pY ) thus

Rd

.

(cid:107)X − φ∗(X)(cid:107)pdpX

(y1  . . .   yn)
can be estimated by

(cid:19)1/p

(cid:90)

(cid:18)(cid:90)

(cid:99)Wp(X  Y) =

(cid:32)

n(cid:88)

i=1

1
n

(cid:107)xi −(cid:98)φ(xi)(cid:107)p

(cid:33)1/p

.

zi =

s(cid:88)
i=1 ∈ R is the univariate response  {xi}n

(cid:124)
j xi) + i 

i = 1  . . .   n 

fj(β

j=1

Projection pursuit method. Projection pursuit regression [16  24  15  26] is widely-used for high-
dimensional nonparametric regression models which takes the form.

(1)

i=1 are i.i.d. normal errors. The goal is to estimate the unknown link functions {fj}s

where s is a hyper-parameter  {zi}n
and {i}n
R → R and the unknown coefﬁcients {βj}s
The additive model (1) can be ﬁtted in an iterative fashion. In the kth iteration  k = 2  . . .   s 
j=1 obtained from previous k − 1 iterations. Denote
(cid:124)
j xi)  i = 1  . . .   n  the residuals. Then (fk  βk) can be estimated by solving

denote {((cid:98)fj (cid:98)βj)}k−1
i = zi −(cid:80)k−1
j=1 (cid:98)fj((cid:98)β
j=1 the estimate of {(fj  βj)}k−1

i=1 ∈ Rd are covariates 
j=1 :

j=1 ∈ Rd.

R[k]
the following least squares problem

n(cid:88)

(cid:104)

i=1

min
fk βk

(cid:105)2

.

i − fk(β
R[k]

(cid:124)
k xi)

The above iterative process explains the intuition behind the projection pursuit regression. Given
the model ﬁtted in previous iterations  we ﬁt a one dimensional regression model using the current
residuals  rather than the original responses. We then add this new regression model into the ﬁtted
function in order to update the residuals. By adding small regression models to the residuals  we
gradually improve ﬁtted model in areas where it does not perform well.
The intuition of projection pursuit regression motivates us to modify the existing projection-based
OTM estimation approaches from two aspects. First  in the kth iteration  we propose to seek a new
projection direction for the one-dimensional OTM in the subspace spanned by the residuals of the
previously k − 1 directions. On the contrary  following a direction that is in the span of used ones
can lead to an inefﬁcient one dimensional OTM. As a result  this “move” may hardly reduce the
Wasserstein distance between pX and pY . Such inefﬁcient “moves” can be one of the causes of
the convergence issue in existing projection-based OTM estimation algorithms. Second  in each
iteration  we propose to select the most “informative” direction with respect to the current residuals
rather than a random one. Speciﬁcally  we choose the direction that explains the highest proportion
of variations in the subspace spanned by the current residuals. Intuitively  this direction addresses
the maximum marginal “discrepancy” between pX and pY among the ones that are not considered
by previous iterations. We propose to estimate this most “informative” direction with sufﬁcient
dimension reduction techniques introduced as follows.

1Such a map is thus also called the Monge map.

3

Sufﬁcient dimension reduction. Consider a regression problem with univariate response Z and
a d-dimensional predictor X. Sufﬁcient dimension reduction for regression aims to reduce the
dimension of X while preserving its regression relation with Z. In other words  sufﬁcient dimension
X with some B ∈ Rd×q and q ≤ d  such
(cid:124)
reduction seeks a set of linear combinations of X  say B
X  i.e.  Z ⊥⊥ X|B
(cid:124)
(cid:124)
that Z depends on X only through B
X. Then  the column space of B  denoted
as S(B) is called a dimension reduction space (DRS). Furthermore  if the union of all possible DRSs
is also a DRS  we call it the central subspace and denote it as SZ|X. When SZ|X exists  it is the
minimum DRS. We call a sufﬁcient dimension reduction method exclusive if it induces a DRS that
equals to the central subspace. Some popular sufﬁcient dimension reduction techniques include
sliced inverse regression (SIR) [30]  principal Hessian directions (PHD) [31]  sliced average variance
estimator (SAVE) [9]  directional regression (DR) [29]  among others.
Estimation of the most “informative” projec-
tion direction. Consider estimating an OTM be-
tween a source sample and a target sample. We
ﬁrst form a regression problem by adding a bi-
nary response  which equals zero for the source
sample and one for the target sample. We then uti-
lize the sufﬁcient dimension reduction technique
to select the most “informative” projection direc-
tion. To be speciﬁc  we select the projection di-
rection ξ ∈ Rd as the eigenvector corresponds to
the largest eigenvalue of the estimated B. The
direction ξ is most “informative” in the sense that 
the projected samples Xξ and Yξ have the most
substantial “ discrepancy.” The metric of the “dis-
crepancy” depends on the choice of the sufﬁcient
dimension reduction technique. Figure 2 gives a
toy example to illustrate this idea. In this paper 
we opt to use SAVE for calculating B  and hence
the “discrepancy” metric is the difference between Var(Xξ) and Var(Yξ). Empirically  we ﬁnd other
sufﬁcient dimension reduction techniques  like PHD and DR  also yield similar performance. The
SIR method  however  yields inferior performance  since it only considers the ﬁrst moment. The
Algorithm 1 below introduces our estimation method of “informative" projection direction in detail.

Figure 2: The most “informative” projection di-
rection ensures the projected samples (illustrated
by the distributions colored in red and blue  re-
spectively) have the largest “discrepancy”.

Algorithm 1 Select the most “informative” projection direction using SAVE

Input: two standardized matrix X ∈ Rn×d and Y ∈ Rn×d

Step 1: calculate (cid:98)Σ ∈ Rd×d  i.e.  the sample variance-covariance matrix of(cid:0)X
Step 2: calculate the sample variance-covariance matrices of X(cid:98)Σ−1/2 and Y(cid:98)Σ−1/2  denoted as
(cid:98)Σ1 ∈ Rd×d and (cid:98)Σ2 ∈ Rd×d  respectively
matrix (((cid:98)Σ1 − Id)2 + ((cid:98)Σ2 − Id)2)/4
Output: the ﬁnal result is given by (cid:98)Σ−1/2ξ/||(cid:98)Σ−1/2ξ||  where || · || denotes the Euclidean norm

Step 3: calculate the eigenvector ξ ∈ Rd  which corresponding to the largest eigenvalue of the

(cid:1)

Y

Projection pursuit Monge map algorithm. Now  we are ready to present our estimation method
for large-scale OTM. The detailed algorithm  named projection pursuit Monge map  is summarized
in Algorithm 2 below. In each iteration  the PPMM applies a one-dimensional OTM following the
most “informative” projection direction selected by the Algorithm 1.
Computational cost of PPMM. In Algorithm 2  the computational cost mainly resides in the ﬁrst
two steps within each iteration. In step (a)  one calculates ξk using Algorithm 1  whose computational
cost is of order O(nd2). In step (b)  one calculates a one-dimensional OTM using the look-up table 
which is simply a sorting algorithm [40  38].
The computational cost for step (b) is of order O(n log(n)). Suppose that the algorithm converges
Empirically  we ﬁnd K = O(d) works reasonably well. When log(n)1/2 ≤ d (cid:28) n2/3  the order

after K iterations. The overall computational cost of Algorithm 2 is of order O(cid:0)Knd2 + Kn log(n)(cid:1).
of computational cost of PPMM is o(cid:0)n3 log(n)(cid:1) which is smaller than the computational cost of

4

Algorithm 2 Projection pursuit Monge map (PPMM)

Input: two matrix X ∈ Rn×d and Y ∈ Rn×d
k ← 0  X[0] ← X
repeat

(a) calculate the projection direction ξk ∈ Rd between X[k] and Y (using Algorithm 1)
(b) ﬁnd the one-dimensional OTM φ(k) that matches X[k]ξk to Yξk (using look-up table)
(c) X[k+1] ← X[k] + (φ(k)(X[k]ξk) − X[k]ξk)ξ

(cid:124)
k and k ← k + 1

until converge

The ﬁnal estimator is given by(cid:98)φ : X → X[k]

the naive method for calculating OTMs. When d ≤ log(n)1/2  the order of computational cost
reduces to O (Kn log(n)) which is faster than the exiting projection-based methods given PPMM
converges faster. The memory cost for Algorithm 2 mainly resides in the step (a)  which is of the
order O(Knd2).

3 Theoretical results

Exclusiveness of SAVE. For mathematical simplicity  we assume E[X] = E[Y ] = 0d. When
E[X] (cid:54)= E[Y ]  one can use a ﬁrst-order dimension reduction method like SIR to adjust means before
applying SAVE.

−1/2
Denote W = (X + Y )/2  ΣW = Var(W )  and Z = W Σ
W . For a univariate continuous response
variable R  one can approximate the central subspace SR|Z by SSAVE  which is the population version
of the dimension reduction space of SAVE. To be speciﬁc  SSAVE is the column space of matrix

W |R) − Id]2(cid:111)

−1/2

 

(cid:110)

1
4

E[Var(Z|R) − Id]2 =

E[Var(XΣ

−1/2
W |R) − Id]2 + E[Var(Y Σ

where the above equation used the fact that X ⊥⊥ Y .
Assumption 1. Let P be the projection onto the central space SR|Z with respect to the inner project
a · b = a
b. For any nonzero vectors u  v ∈ Rd  such that u is orthogonal to SR|Z and v ∈ SR|Z  we
(cid:124)
assume

(cid:124)

Z|P Z) is a linear function of Z;
(a) E(u
Z|P Z) is a nonrandom number;
(cid:124)
(b) Var(u

(c) Let ((cid:101)Z (cid:101)R) be an independent copy of (Z  R). E

that is  it is not equal almost surely to a constant.

(cid:104)

v

(cid:105)
(Z − (cid:101)Z)2|R (cid:101)R)

(cid:124)

is non degenerate;

ance matrix estimator of Σ1 and Σ2  respectively. Denote

Theorem 1. Let R be a univariate continuous response variable. Under Assumption 1  the dimension
reduction space induced by SAVE is exclusive. In other words  SSAVE = SR|Z.

Consistency of the most “informative” projection direction. Let(cid:98)Σ1 and(cid:98)Σ2 be the sample covari-
(cid:104)
((cid:98)Σ1 − Id)2 + ((cid:98)Σ2 − Id)2(cid:105)
Denote ξ1 and (cid:98)ξ1 the eigenvectors correspond to the largest eigenvalues of ΣSAVE and (cid:98)ΣSAVE 

(cid:2)(Σ1 − Id)2 + (Σ2 − Id)2(cid:3)

and (cid:98)ΣSAVE =

ΣSAVE =

1
4

1
4

.

respectively. Further  denote r = Rank(ΣSAVE)  the rank of ΣSAVE.
Assumption 2. Let {xi  yi}n

i=1 be an i.i.d. sample of (X  Y ). We assume that

(a) Denote xij and yik the jth and kth component of xi and yi  respectively. E(xijyik) = 0

for all 1 ≤ i ≤ n and 1 ≤ j  k ≤ d;

(b) There are r1  r2 > 0 and b1  b2 > 0 such that  for any s > 0  1 ≤ i ≤ n and 1 ≤ j ≤ d 

P (|xij| > s) ≤ exp{−(s/b1)r1}

and P (|yij| > s) ≤ exp{−(s/b2)r2} ;

5

(c) Let λ1 . . .   λd be the eigenvalues of ΣSAVE in descending order. There exist positive

constants cl cu and c3 such that

cl ≤ min
1≤l≤r−1

(λl − λl+1)d−1/2 ≤ cu 

and

0 ≤ λr+1 < c3.

Theorem 2 shows that Algorithm 1 can consistently estimate the most “informative” projection
direction. The Op in Theorem 2 stands for order in probability  which is similar to O but for random
variables.
Theorem 2. Under Assumption 2  the SAVE estimator of most “informative” projection direction
satisﬁes 

(cid:107)(cid:98)ξ1 − ξ1(cid:107)∞ = Op(r4

(cid:114)

√

log d

n

+ r4

d

log d

n

) 

as n  d → ∞.

Weak convergence of PPMM algorithm. Denote φ∗ as the d-dimensional optimal transport map
from pX to pY and φ(K) as the PPMM estimator after K iterations  i.e. φ(K)(X) = X[K]. The
following theorem gives the weak convergence results of the PPMM algorithm.
Theorem 3. Suppose Assumption 1 and Assumption 2 hold. Let K ≥ Cd for some large enough
positive constant C  one has

(cid:16)

(cid:99)Wp

(cid:17) → Wp

(cid:16)

(cid:17)

φ(K)(X)  X

φ∗(X)  X

 

and φ(K)(X) → φ∗(X) as n → ∞.

Works are proving the convergence rates of the empirical optimal transport objectives [5  49  6  54].
The convergence rate of the OTM has rarely been studied except for a recent paper [25]. We believe
Theorem 3 is the ﬁrst step in this direction.

4 Numerical experiments

4.1 Estimation of optimal transport map

(cid:124) from pX = Nd(µX   ΣX ) and Y =
(cid:124) from pY = Nd(µY   ΣY )  respectively. We set n = 10  000  d = {10  20  50} 

Suppose that we observe i.i.d. samples X = (x1  . . .   xn)
(y1  . . .   yn)
µX = −2  µY = 2  ΣX = 0.8|i−j|  and ΣY = 0.5|i−j|  for i  j = 1  . . .   d.
We apply PPMM to estimate the OTM between pX and pY from {xi}n
i=1. In comparison 
we also consider the following two projection-based competitors: (1) the random projection method
(RANDOM) as proposed in [39  40]; (2) the sliced method as proposed in [7  42]. The number of
slices L is set to be 10  20  and 50. We assess the convergence of each method by the estimated
  where φ(k)(·) is the
estimator of OTM after kth iteration. For all three methods  we set the maximum number of iterations
to be 200. Notice that  the Wasserstein distance between pX and pY admits a closed form 

Wasserstein distance of order 2 after each iteration  i.e. (cid:99)W2

i=1 and {yi}n
(cid:17)

φ(k)(X)  X

(cid:16)

X )1/2(cid:17)

X ΣY Σ1/2

 

(2)

2 + trace

2 (pX   pY ) = ||µX − µY ||2
W 2

ΣX + ΣY − 2(Σ1/2
which serves as the ground-truth. The results are presented in Figure 3.
In all three scenarios  PPMM (red
line) converges to the ground truth
within a small number of iterations.
The ﬂuctuations of the convergence
curves observed in Figure 3 are caused
by the non-equal sample means. This
can be adjusted by applying a ﬁrst-
order dimension reduction method
(e.g.  SIR). We do not pursue this ap-
proach as the ﬂuctuations do not cover
the main pattern in Figure 3. When
d = 10  RANDOM and SLICED con-
verge to the ground truth but in a much

Figure 3: The black dashed line is the true value of the
Wasserstein distance as in (2). The colored lines represent
the sample mean of the estimated Wasserstein distances over
100 replications  and the vertical bars represent the standard
deviations.

(cid:16)

6

slower manner. When d = 20 and 50  neither RANDOM nor SLICED manages to converge within
200 iterations. We also ﬁnd a large number of slices L does not necessarily lead to a better estimation
for the SLICED method. As we can see  PPMM is the only one among three that is adaptive to
large-scale OTM estimation problems.
In Table 1 below  we compare the computational cost of three methods by reporting the CPU time per
iteration over 100 replication.2 As we expected  the RANDOM method has the lowest CPU time per
iteration due to it does not select projection direction. We notice that the CPU time per iteration of
the SLICED method is proportional to the number of slices L. Last but not least  the CPU time per
iteration of PPMM is slightly larger than RANDOM but much smaller than SLICED.

Table 1: The mean CPU time (sec) per iteration  with standard deviations presented in parentheses

d = 10
d = 20
d = 50

PPMM

0.019 (0.008)
0.027 (0.011)
0.059 (0.036)

RANDOM
0.011 (0.008)
0.014 (0.008)
0.015 (0.008)

SLICED(10)
0.111 (0.019)
0.125 (0.027)
0.171 (0.037)

SLICED(20)
0.213 (0.024)
0.247 (0.033)
0.338 (0.049)

SLICED(50)
0.529 (0.031)
0.605 (0.058)
0.863 (0.117)

In the Table 2 below  we report the mean convergence time over 100 replications for PPMM 
RANDOM  SLICED  the reﬁned auction algorithm (AUCTIONBF)[3]  the revised simplex algorithm
(REVSIM) [34] and the shortlist method (SHORTSIM) [20].3 Table 2 shows that the PPMM is the
most computationally efﬁcient method thanks to its cheap per iteration cost and fast convergence.

Table 2: The mean convergence time (sec) for estimating the Wasserstein distance  with standard
deviations presented in parentheses. The symbol “-” is inserted when the algorithm fails to converge.

d = 10
d = 20
d = 50

PPMM RANDOM SLICED(10) AUCTIONBF REVSIM SHORTSIM
42.5 (3.2)
0.6 (0.1)
50.2 (6.6)
2.1 (0.3)
5.5 (0.4)
56.5 (7.1)

99.7 (10.4)
109.4 (12.5)
125.5 (13.3)

40.2 (4.0)
42.6 (5.3)
46.5 (5.6)

4.8 (1.7)
24.4 (3.2)

23.0 (2.6)
230.2 (28.4)

-

-

4.2 Application to generative models

Figure 4: Illustration for the generative model using manifold
learning and optimal transport

A critical issue in generative models is
the so-called mode collapse  i.e.  the
generated “fake” sample fails to cap-
ture some modes present in the train-
ing data [22  45]. To address this is-
sue  recent studies [51  22  28] incor-
porated generative models with the op-
timal transportation theory. As illus-
trated in Figure 4  one can decompose
the problem of generating fake sam-
ples into two major steps: (1) manifold learning and (2) probability transformation. The step (1) aims
to discover the manifold structure of the training data by mapping the training data from the original
space X ⊂ Rd to a latent space Z ⊂ Rd∗
with d∗ (cid:28) d. Notice that the probability distribution
of the transformed data in Z may not be convex  leading to the problem of mode collapse. The
step (2) then addresses the mode collapse issue through transporting the distribution in Z to the
uniform distribution U ([0  1]d∗
). Then  the generative model takes a random input from U ([0  1]d∗
)
and sequentially applies the inverse transformations in step (2) and step (1) to generate the output. In
practice  one may implement the step (1) and (2) using variational autoencoders (VAE) and OTM 
respectively. As we can see  the estimation of OTM plays an essential role in this framework.
In this subsection  we apply PPMM as well as RANDOM and SLICED to generative models to study
two datasets: MINST and Google doodle dataset. For the SLICED method  we set the number of
slices to be 10  20  and 50. For all three methods  we set the number of iterations is set to be 10d∗.
We use the squared Euclidean distance as the cost for the VAE model.

2The experiments are implemented by an Intel 2.6 GHz processor.
3AUCTIONBF  REVSIM and SHORTSIM are implemented by the R package “transport” [46].

7

Table 3: The FID for the generated samples (lower the better)  with standard deviations presented in
parentheses

MNIST

Doodle (face)
Doodle (cat)
Doodle (bird)

PPMM

0.17 (0.01)
0.59 (0.09)
0.24 (0.03)
0.36 (0.03)

RANDOM SLICED(10)
2.98 (0.01)
4.62 (0.02)
8.78 (0.04)
5.69 (0.01)
5.99 (0.01)
8.93 (0.03)
7.81 (0.03)
5.44 (0.01)

SLICED(20)
3.04 (0.01)
6.01 (0.01)
5.26 (0.01)
5.50 (0.01)

SLICED(50)
3.12 (0.01)
5.52 (0.01)
5.33 (0.01)
4.98 (0.01)

MNIST. We ﬁrst study the MNIST
dataset  which contains 60 000 train-
ing images and 10 000 testing images
of hand written digits. We pull each
28 × 28 image to a 784-dimensional
vector and rescale the grayscale val-
ues from [0  255] to [0  1]. Following
the method in [51]  we apply VAE to
encode the data into a latent space Z
of dimensionality d∗ = 8. Then  the
OTM from the distribution in Z to
U ([0  1]8) is estimated by PPMM as
well as RANDOM and SLICED.
First  we visually examine the fake sample generated with PPMM. In the left-hand panel of Figure 5 
we display some random images generated by PPMM. The right-hand panel of Figure 5 shows that
PPMM can predict the continuous shift from one digit to another. To be speciﬁc  let a  b ∈ R784 be
the sample of two digits (e.g. 3 and 9) in the testing set. Let T : X → Z be the map induced by

VAE and(cid:98)φ the OTM estimated by PPMM. Then (cid:98)φ(T (·)) maps the sample distribution to U ([0  1]8).
We linearly interpolate between(cid:98)φ(T (a)) and(cid:98)φ(T (b)) with equal-size steps. Then we transform the

Figure 5: Left: random samples generated by PPMM. Right:
linear interpolation between random pairs of images.

interpolated points back to the sample distribution to generate the middle columns in the right panel
of Figure 5.
We use the “Fr´echet Inception Distance” (FID) [23] to quantify the similarity between the generated
fake sample and the training sample. Speciﬁcally  we ﬁrst generate 1 000 random inputs from
U ([0  1]8). We then apply PPMM  RANDOM  and SLICED to this input sample  yields the fake
samples in the latent space Z. Finally  we calculate the FID between the encoded training sample
in the latent space and the generated fake samples  respectively. A small value of FID indicates the
generated fake sample is similar to the training sample and vice versa. The sample mean and sample
standard deviation (in parentheses) of FID over 50 replications are presented in Table 3. Table 3
indicates PPMM signiﬁcantly outperforms the other two methods in terms of estimating the OTM.
Google doodle dataset. The Google Doodle dataset4 contains over 50 million drawings created
by users with a mouse under 20 secs. We analyze a pre-processed version of this dataset from the
quick draw Github account5. In the dataset we use  the drawings are centered and rendered into
28 × 28 grayscale images. We pull each 28 × 28 image to a 784-dimensional vector and rescale
the grayscale values from [0  255] to [0  1]. In this experiment  we study the drawings from three
different categories: smile face  cat  and bird. These three categories contain 161 666  123 202  and
133 572 drawings  respectively. Within each category  we randomly split the data into a training set
and a validation set of equal sample sizes.
We apply VAE to the training set with a stopping criterion selected by the validation set. The
dimension of the latent space is set to be 16. Let a  b ∈ R784 be two vectors in the validation

set  T : X → Z be the map induced by VAE and (cid:98)φ be the OTM estimated by PPMM. Note that
(cid:98)φ(T (·)) maps the sample distribution to U ([0  1]16). We then linearly interpolate between(cid:98)φ(T (a))
and(cid:98)φ(T (b)) with equal-size steps. The results are presented in Figure 6.

4https://quickdraw.withgoogle.com/data
5https://github.com/googlecreativelab/quickdraw-dataset

8

Figure 6: Linear interpolation between random pairs of images from the dataset of smile face (left) 
cat (center)  and bird (right).

Then  we quantify the similarity between the generated fake samples and the truth by calculating the
FID in the latent space. The sample mean and sample standard deviation (in parentheses) of FID over
50 replications are presented in Table 3. Again  the results in Table 3 justify the superior performance
of PPMM over existing projection-based methods.

5 Extensions

First  the PPMM can be extended to address the peniten-
tial heterogeneous in the dataset by assigning non-equal
weights to the points in source and target samples. This
is equivalent to calculate weighted variance-covariance
matrices in Step 2 of Algorithm 1. Second  the PPMM
method can be modiﬁed to allow the sizes of the source
and target samples to be different. In such a scenario 
we can replace the look-up table in the Step (b) of Algo-
rithm 2 with an approximate lookup table. Recall that
the one-dimensional lookup table is just sorting  the one-
dimensional approximate look-up table can be achieved
by combining sorting and linear interpolation. We validate
the above extensions with a simulated experiment similar
to the one in Section 4.1 except that we draw 5  000 and
1  000 points from pX and pY   respectively. We set d = 10 and assign weights to the observations
randomly. The estimation results are presented in Figure 7. In addition  the average convergence time
is: PPMM(0.3s)  RANDOM (1.4s)  SLICED10 (14s) and SHORTSIM (74s).
Theorem 3 suggests that  for the PPMM algorithm  the
number of iterations until converge  i.e.  K  is on the order
of dimensionality d. Here we use a simulated example
to assess whether this order is attainable. We follow a
similar setting as in Section 4.1 except that we increase d
from 10 to 100 with a step size of 10. Besides  we set the
termination criteria to be a hard threshold  i.e.  10−5. In
Figure 8  we report the sample mean (solid line) and stan-
dard deviation (vertical bars) of K over 100 replications
with respect to the increased d. One can observe a clear
linear pattern.

Figure 7: Experiment for heterogeneous
data with non-equal sample sizes. The
black dashed line is the oracle calculated by
SHORTSIM

Figure 8: Number of iterations to converge

Acknowledgment

We would like to thank Xiaoxiao Sun  Rui Xie  Xinlian Zhang  Yiwen Liu  and Xing Xin for many
fruitful discussions. We would also like to thank Dr. Xianfeng David Gu for his insightful blog
about the Optimal transportation theory. Also  we would like to thank the UC Irvine Machine
Learning Repository for dataset assistance. This work was partially supported by National Science
Foundation grants DMS-1440037  DMS-1440038  DMS-1438957  and NIH grants R01GM113242 
R01GM122080.

9

References
[1] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein generative adversarial networks. In

International Conference on Machine Learning  pages 214–223  2017.

[2] J.-D. Benamou  Y. Brenier  and K. Guittet. The monge–kantorovitch mass transfer and its
computational ﬂuid mechanics formulation. International Journal for Numerical methods in
ﬂuids  40(1-2):21–30  2002.

[3] D. P. Bertsekas. Auction algorithms for network ﬂow problems: A tutorial introduction.

Computational optimization and applications  1(1):7–66  1992.

[4] M. Blaauw and J. Bonada. Modeling and transforming speech using variational autoencoders.

In Interspeech  pages 1770–1774  2016.

[5] E. Boissard et al. Simple bounds for the convergence of empirical and occupation measures in

1-wasserstein distance. Electronic Journal of Probability  16:2296–2333  2011.

[6] E. Boissard and T. Le Gouic. On the mean speed of convergence of empirical and occupation
measures in wasserstein distance. In Annales de l’IHP Probabilités et statistiques  volume 50 
pages 539–563  2014.

[7] N. Bonneel  J. Rabin  G. Peyré  and H. Pﬁster. Sliced and radon wasserstein barycenters of

measures. Journal of Mathematical Imaging and Vision  51(1):22–45  2015.

[8] Y. Brenier. A homogenized model for vortex sheets. Archive for Rational Mechanics and

Analysis  138(4):319–353  1997.

[9] R. D. Cook and S. Weisberg. Sliced inverse regression for dimension reduction: Comment.

Journal of the American Statistical Association  86(414):328–332  1991.

[10] N. Courty  R. Flamary  D. Tuia  and A. Rakotomamonjy. Optimal transport for domain
adaptation. IEEE transactions on pattern analysis and machine intelligence  39(9):1853–1865 
2017.

[11] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in

neural information processing systems  pages 2292–2300  2013.

[12] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on
deep networks. In Advances in neural information processing systems  pages 658–666  2016.

[13] J. Engel  C. Resnick  A. Roberts  S. Dieleman  M. Norouzi  D. Eck  and K. Simonyan. Neural
In Proceedings of the 34th
audio synthesis of musical notes with wavenet autoencoders.
International Conference on Machine Learning-Volume 70  pages 1068–1077. JMLR. org 
2017.

[14] S. Ferradans  N. Papadakis  G. Peyré  and J.-F. Aujol. Regularized discrete optimal transport.

SIAM Journal on Imaging Sciences  7(3):1853–1882  2014.

[15] J. H. Friedman. Exploratory projection pursuit. Journal of the American statistical association 

82(397):249–266  1987.

[16] J. H. Friedman and W. Stuetzle. Projection pursuit regression. Journal of the American statistical

Association  76(376):817–823  1981.

[17] A. Genevay  M. Cuturi  G. Peyré  and F. Bach. Stochastic optimization for large-scale optimal

transport. In Advances in neural information processing systems  pages 3440–3448  2016.

[18] S. Gerber and M. Maggioni. Multiscale strategies for computing optimal transport. The Journal

of Machine Learning Research  18(1):2440–2471  2017.

[19] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems 
pages 2672–2680  2014.

10

[20] C. Gottschlich and D. Schuhmacher. The shortlist method for fast computation of the
earth mover’s distance and ﬁnding optimal solutions to transportation problems. PloS one 
9(10):e110214  2014.

[21] I. Gulrajani  F. Ahmed  M. Arjovsky  V. Dumoulin  and A. C. Courville. Improved training of
wasserstein gans. In Advances in Neural Information Processing Systems  pages 5769–5779 
2017.

[22] Y. Guo  D. An  X. Qi  Z. Luo  S.-T. Yau  X. Gu  et al. Mode collapse and regularity of optimal

transportation maps. arXiv preprint arXiv:1902.02934  2019.

[23] M. Heusel  H. Ramsauer  T. Unterthiner  B. Nessler  and S. Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information
Processing Systems  pages 6626–6637  2017.

[24] P. J. Huber. Projection pursuit. The annals of Statistics  pages 435–475  1985.

[25] J.-C. Hütter and P. Rigollet. Minimax rates of estimation for smooth optimal transport maps.

arXiv preprint arXiv:1905.05828  2019.

[26] A. Ifarraguerri and C.-I. Chang. Unsupervised hyperspectral image analysis with projection

pursuit. IEEE Transactions on Geoscience and Remote Sensing  38(6):2529–2538  2000.

[27] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 

2013.

[28] S. Kolouri  P. E. Pope  C. E. Martin  and G. K. Rohde. Sliced-wasserstein autoencoder: An

embarrassingly simple generative model. arXiv preprint arXiv:1804.01947  2018.

[29] B. Li and S. Wang. On directional regression for dimension reduction. Journal of the American

Statistical Association  102(479):997–1008  2007.

[30] K.-C. Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical

Association  86(414):316–327  1991.

[31] K.-C. Li. On principal hessian directions for data visualization and dimension reduction:
Journal of the American Statistical Association 

Another application of stein’s lemma.
87(420):1025–1039  1992.

[32] X. Liang  L. Lee  W. Dai  and E. P. Xing. Dual motion gan for future-ﬂow embedded video
prediction. In Proceedings of the IEEE International Conference on Computer Vision  pages
1744–1752  2017.

[33] Y. Liu  Z. Qin  Z. Luo  and H. Wang. Auto-painter: Cartoon image generation from sketch by

using conditional generative adversarial networks. arXiv preprint arXiv:1705.01908  2017.

[34] D. G. Luenberger  Y. Ye  et al. Linear and nonlinear programming  volume 2. Springer  1984.

[35] Q. Mérigot. A multiscale approach to optimal transport.
volume 30  pages 1583–1592. Wiley Online Library  2011.

In Computer Graphics Forum 

[36] N. Papadakis  G. Peyré  and E. Oudet. Optimal transport with proximal splitting. SIAM Journal

on Imaging Sciences  7(1):212–238  2014.

[37] O. Pele and M. Werman. Fast and robust earth mover’s distances. In 2009 IEEE 12th Interna-

tional Conference on Computer Vision  pages 460–467. IEEE  2009.

[38] G. Peyré  M. Cuturi  et al. Computational optimal transport. Foundations and Trends R(cid:13) in

Machine Learning  11(5-6):355–607  2019.

[39] F. Pitie  A. C. Kokaram  and R. Dahyot. N-dimensional probability density function transfer and
its application to color transfer. In Computer Vision  2005. ICCV 2005. Tenth IEEE International
Conference on  volume 2  pages 1434–1439. IEEE  2005.

11

[40] F. Pitié  A. C. Kokaram  and R. Dahyot. Automated colour grading using colour distribution

transfer. Computer Vision and Image Understanding  107(1-2):123–137  2007.

[41] J. Rabin  S. Ferradans  and N. Papadakis. Adaptive color transfer with relaxed optimal transport.
In 2014 IEEE International Conference on Image Processing (ICIP)  pages 4852–4856. IEEE 
2014.

[42] J. Rabin  G. Peyré  J. Delon  and M. Bernot. Wasserstein barycenter and its application to texture
mixing. In International Conference on Scale Space and Variational Methods in Computer
Vision  pages 435–446. Springer  2011.

[43] S. Reich. A nonparametric ensemble transform method for bayesian inference. SIAM Journal

on Scientiﬁc Computing  35(4):A2013–A2024  2013.

[44] Y. Rubner  L. J. Guibas  and C. Tomasi. The earth mover’s distance  multi-dimensional scaling 
and color-based image retrieval. In Proceedings of the ARPA image understanding workshop 
volume 661  page 668  1997.

[45] T. Salimans  H. Zhang  A. Radford  and D. Metaxas. Improving gans using optimal transport.

arXiv preprint arXiv:1803.05573  2018.

[46] D. Schuhmacher  B. Bähre  C. Gottschlich  V. Hartmann  F. Heinemann  and B. Schmitzer.
Transport: Computation of Optimal Transport Plans and Wasserstein Distances  2019. R
package version 0.12-1.

[47] V. Seguy  B. B. Damodaran  R. Flamary  N. Courty  A. Rolet  and M. Blondel. Large-scale

optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283  2017.

[48] J. Solomon  R. Rustamov  L. Guibas  and A. Butscher. Earth mover’s distances on discrete

surfaces. ACM Transactions on Graphics (TOG)  33(4):67  2014.

[49] B. K. Sriperumbudur  K. Fukumizu  A. Gretton  B. Schölkopf  G. R. Lanckriet  et al. On the
empirical estimation of integral probability metrics. Electronic Journal of Statistics  6:1550–
1599  2012.

[50] Z. Su  Y. Wang  R. Shi  W. Zeng  J. Sun  F. Luo  and X. Gu. Optimal mass transport for shape
matching and comparison. IEEE transactions on pattern analysis and machine intelligence 
37(11):2246–2259  2015.

[51] I. Tolstikhin  O. Bousquet  S. Gelly  and B. Schoelkopf. Wasserstein auto-encoders. arXiv

preprint arXiv:1711.01558  2017.

[52] C. Villani. Optimal transport: old and new. Springer Science & Business Media  2008.

[53] C. Vondrick  H. Pirsiavash  and A. Torralba. Generating videos with scene dynamics. In

Advances In Neural Information Processing Systems  pages 613–621  2016.

[54] J. Weed and F. Bach. Sharp asymptotic and ﬁnite-sample rates of convergence of empirical

measures in wasserstein distance. arXiv preprint arXiv:1707.00087  2017.

12

,Cheng Meng
Yuan Ke
Jingyi Zhang
Mengrui Zhang
Ping Ma