2016,Adaptive optimal training of animal behavior,Neuroscience experiments often require training animals to perform tasks designed to elicit various sensory  cognitive  and motor behaviors. Training typically involves a series of gradual adjustments of stimulus conditions and rewards in order to bring about learning. However  training protocols are usually hand-designed  relying on a combination of intuition  guesswork  and trial-and-error  and often require weeks or months to achieve a desired level of task performance. Here we combine ideas from reinforcement learning and adaptive optimal experimental design to formulate methods for adaptive optimal training of animal behavior. Our work addresses two intriguing problems at once: first  it seeks to infer the learning rules underlying an animal's behavioral changes during training; second  it seeks to exploit these rules to select stimuli that will maximize the rate of learning toward a desired objective.  We develop and test these methods using data collected from rats during training on a two-interval sensory discrimination task.  We show that we can accurately infer the parameters of a policy-gradient-based learning algorithm that describes how the animal's internal model of the task evolves over the course of training.  We then formulate a theory for optimal training  which involves selecting sequences of stimuli that will drive the animal's internal policy toward a desired location in the parameter space. Simulations show that our method can in theory provide a substantial speedup over standard training methods. We feel these results will hold considerable theoretical and practical implications both for researchers in reinforcement learning and for experimentalists seeking to train animals.,Adaptive optimal training of animal behavior

Ji Hyun Bak1 4 Jung Yoon Choi2 3 Athena Akrami3 5 Ilana Witten2 3 Jonathan W. Pillow2 3

1Department of Physics  2Department of Psychology  Princeton University

3Princeton Neuroscience Institute  Princeton University

4School of Computational Sciences  Korea Institute for Advanced Study

jhbak@kias.re.kr  {jungchoi aakrami iwitten pillow}@princeton.edu

5Howard Hughes Medical Institute

Abstract

Neuroscience experiments often require training animals to perform tasks designed
to elicit various sensory  cognitive  and motor behaviors. Training typically involves
a series of gradual adjustments of stimulus conditions and rewards in order to bring
about learning. However  training protocols are usually hand-designed  relying
on a combination of intuition  guesswork  and trial-and-error  and often require
weeks or months to achieve a desired level of task performance. Here we combine
ideas from reinforcement learning and adaptive optimal experimental design to
formulate methods for adaptive optimal training of animal behavior. Our work
addresses two intriguing problems at once: ﬁrst  it seeks to infer the learning rules
underlying an animal’s behavioral changes during training; second  it seeks to
exploit these rules to select stimuli that will maximize the rate of learning toward a
desired objective. We develop and test these methods using data collected from rats
during training on a two-interval sensory discrimination task. We show that we can
accurately infer the parameters of a policy-gradient-based learning algorithm that
describes how the animal’s internal model of the task evolves over the course of
training. We then formulate a theory for optimal training  which involves selecting
sequences of stimuli that will drive the animal’s internal policy toward a desired
location in the parameter space. Simulations show that our method can in theory
provide a substantial speedup over standard training methods. We feel these results
will hold considerable theoretical and practical implications both for researchers in
reinforcement learning and for experimentalists seeking to train animals.

1

Introduction

An important ﬁrst step in many neuroscience experiments is to train animals to perform a particular
sensory  cognitive  or motor task. In many cases this training process is slow (requiring weeks to
months) or difﬁcult (resulting in animals that do not successfully learn the task). This increases the
cost of research and the time taken for experiments to begin  and poorly trained animals—for example 
animals that incorrectly base their decisions on trial history instead of the current stimulus—may
introduce variability in experimental outcomes  reducing interpretability and increasing the risk of
false conclusions.
In this paper  we present a principled theory for the design of normatively optimal adaptive training
methods. The core innovation is a synthesis of ideas from reinforcement learning and adaptive
experimental design: we seek to reverse engineer an animal’s internal learning rule from its observed
behavior in order to select stimuli that will drive learning as quickly as possible toward a desired
objective. Our approach involves estimating a model of the animal’s internal state as it evolves over
training sessions  including both the current policy governing behavior and the learning rule used to

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1:
(A) Stimulus space for a 2AFC discrimination task  with optimal separatrix between
correct “left” and “right” choices shown in red. Filled circles indicate a “reduced” set of stimuli
(consisting of those closest to the decision boundary) which have been used in several prominent
studies [3  6  9]. (B) Schematic of active training paradigm. We infer the animal’s current weights
wt and its learning rule (“RewardMax”)  parametrized by φ  and use them to determine an optimal
stimulus xt for the current trial (“AlignMax”)  where optimality is determined by the expected weight
change towards the target weights wgoal.

modify this policy in response to feedback. We model the animal as using a policy-gradient based
learning rule [15]  and show that parameters of this learning model can be successfully inferred from
a behavioral time series dataset collected during the early stages of training. We then use the inferred
learning rule to compute an optimal sequence of stimuli  selected adaptively on a trial-by-trial basis 
that will drive the animal’s internal model toward a desired state. Intuitively  optimal training involves
selecting stimuli that maximally align the predicted change in model parameters with the trained
behavioral goal  which is deﬁned as a point in the space of model parameters. We expect this research
to provide both practical and theoretical beneﬁts: the adaptive optimal training protocol promises a
signiﬁcantly reduced training time required to achieve a desired level of performance  while providing
new scientiﬁc insights into how and what animals learn over the course of the training period.

2 Modeling animal decision-making behavior

Let us begin by deﬁning the ingredients of a generic decision-making task. In each trial  the animal
is presented with a stimulus x from a bounded stimulus space X  and is required to make a choice
y among a ﬁnite set of available responses Y . There is a ﬁxed reward map r : {X  Y } → R. It
is assumed that this behavior is governed by some internal model  or the psychometric function 
described by a set of parameters or weights w. We introduce the “y-bar” notation ¯y(x) to indicate the
correct choice for the given stimulus x  and let Xy denote the “stimulus group” for a given y  deﬁned
as the set of all stimuli x that map to the same correct choice y = ¯y(x).
For concreteness  we consider a two-alternative forced-choice (2AFC) discrimination task where the
stimulus vector for each trial  x = (x1  x2)  consists of a pair of scalar-valued stimuli that are to be
compared [6  8  9  16]. The animal should report either x1 > x2 or x1 < x2  indicating its choice
with a left (y = L) or right (y = R) movement  respectively. This results in a binary response space 
Y = {L  R}. We deﬁne the reward function r(x  y) to be a Boolean function that indicates whether
a stimulus-response pair corresponds to a correct choice (which should therefore be rewarded) or not:

(cid:26)1

0

r(x  y) =

if {x1 > x2  y = L} or {x1 < x2  y = R};
otherwise.

(1)

Figure 1A shows an example 2-dimensional stimulus space for such a task  with circles representing
a discretized set of possible stimuli X  and the desired separatrix (the boundary separating the two
stimulus groups XL and XR) shown in red. In some settings  the experimenter may wish to focus on
some “reduced” set of stimuli  as indicated here by ﬁlled symbols [3  6  9].
We model the animal’s choice behavior as arising from a Bernoulli generalized linear model (GLM) 
also known as the logistic regression model. The choice probabilities for the two possible stimuli at
trial t are given by

pR(xt  wt) =

1

1 + exp(−g(xt)(cid:62)wt)

 

pL(xt  wt) = 1 − pR(xt  wt)

(2)

2

ABpast observations.........optimal stimulus selectionstimulusstimulus spaceactive training schematicweightsresponsestimulus 1stimulus 2 target weightsanimal’slearningrule xwhere g(x) = (1  x(cid:62))(cid:62) is the input carrier vector  and w = (b  a(cid:62))(cid:62) is the vector of parameters or
weights governing behavior. Here b describes the animal’s internal bias to choosing “right” (y = R) 
and a = (a1  a2) captures the animal’s sensitivity to the stimulus.1
We may also incorporate the trial history as additional dimensions of the input governing the animal’s
behavior; humans and animals alike are known to exhibit history-dependent behavior in trial-based
tasks [1  3  5  7]. Based on some preliminary observations from animal behavior (see Supplementary
Material for details)  we encode the trial history as a compressed stimulus history  using a binary
variable ¯y(x) deﬁned as L = −1 and R = +1. Taking into account the previous d trials  the input
carrier vector and the weight vector become:

g(xt) → (1  x(cid:62)

t   ¯y(xt−1) ···   ¯y(xt−d))(cid:62) 

wt → (b  a(cid:62)  h1 ···   hd).

(3)

The history dependence parameter hd describes the animal’s tendency to stick to the correct answer
from the previous trial (d trials back). Because varying number of history terms d gives a family of
psychometric models  determining the optimal d is a well-deﬁned model selection problem.

3 Estimating time-varying psychometric function

In order to drive the animal’s performance toward a desired objective  we ﬁrst need a framework to
describe  and accurately estimate  the time-varying model parameters of the animal behavior  which
is fundamentally non-stationary while training is in progress.

3.1 Constructing the random walk prior
We assume that the single-step weight change at each trial t follows a random walk  wt − wt−1 = ξt 
where ξt ∼ N (0  σ2
t )  for t = 1 ···   N. Let w0 be some prior mean for the initial weight. We
assume σ2 = ··· = σN = σ  which is to believe that although the behavior is variable  the variability
of the behavior is a constant property of the animal. We can write this more concisely using a state-
space representation [2  11]  in terms of the vector of time-varying weights w = (w1  w2 ···   wN )(cid:62)
and its prior mean w0 = w01:

D(w − w0) = ξ ∼ N (0  Σ) 

(4)
1  σ2 ···   σ2) is the N × N covariance matrix  and D is the sparse banded
where Σ = diag(σ2
matrix with ﬁrst row of an identity matrix and subsequent rows computing ﬁrst order differences.
Rearranging  the full random walk prior on the N-dimensional vector w is
w ∼ N (w0  C)  where C−1 = D(cid:62)Σ−1D.

(5)

In many practical cases there are multiple weights in the model  say K weights. The full set of
parameters should now be arranged into an N × K array of weights {wti}  where the two subscripts
consistently indicate the trial number (t = 1 ···   N) and the type of parameter (i = 1 ···   K) 
respectively. This gives a matrix

W = {wti} = (w∗1 ···   w∗i ···   w∗K) = (w1∗ ···   wt∗ ···   wN∗)(cid:62)

(6)
where we denote the vector of all weights at trial t as wt∗ = (wt1  wt2 ···   wtK)(cid:62)  and the time
series of the i-th weight as w∗i = (w1i  w2i ···   wN i)(cid:62).
∗K)(cid:62) be the vectorization of W   a long vector with the columns
Let w = vec(W ) = (w(cid:62)
of W stacked together. Equation (5) still holds for this extended weight vector w  where the
extended D and Σ are written as block diagonal matrices D = diag(D1  D2 ···   DK) and Σ =
diag(Σ1  Σ2 ···   ΣK)  respectively  where Di is the weight-speciﬁc N × N difference matrix and
Σi is the corresponding covariance matrix. Within a linear model one can freely renormalize the units
of the stimulus space in order to keep the sizes of all weights comparable  and keep all Σi’s equal.
We used a transformed stimulus space in which the center is at 0 and the standard deviation is 1.

∗1 ···   w(cid:62)

1We use a convention in which a single-indexed tensor object is automatically represented as a column vector

(in boldface notation)  and the operation (· · ··· ) concatenates objects horizontally.

3

3.2 Log likelihood

Let us denote the log likelihood of the observed data by L =(cid:80)N

is the trial-speciﬁc log likelihood. Within the binomial model we have

t=1 Lt  where Lt = log p(yt|xt  wt∗)

Lt = (1 − δyt R) log(1 − pR(xt  wt∗)) + δyt R log pR(xt  wt∗).

(7)
Abbreviating pR(xt  wt∗) = pt and pL(xt  wt∗) = 1 − pt  the trial-speciﬁc derivatives are solved to
be ∂Lt/∂wt∗ = (δyt R − pt) g(xt) ≡ ∆t and ∂2Lt/∂wt∗∂wt∗ = −pt(1 − pt)g(xt)g(xt)(cid:62) ≡ Λt.
Extension to the full weight vector is straightforward because distinct trials do not interact. Working
out with the indices  we may write

 M11 M12

M21 M22
...
MK1 MK2



··· M1K
M2K
...

...
··· MKK

(8)

= vec([∆1 ···   ∆N ](cid:62)) 

∂L
∂w

∂2L
∂w2 =

where the (i  j)-th block of the full second derivative matrix is an N × N diagonal matrix deﬁned by
Mij = ∂2L/∂w∗i∂w∗j = diag((Λ1)ij ···   (Λt)ij ···   (ΛN )ij). After this point  we can simplify
our notation such that wt = wt∗. The weight-type-speciﬁc w∗i will no longer appear.

3.3 MAP estimate of w

(cid:18) 1

2

log(cid:12)(cid:12)C−1(cid:12)(cid:12) − 1

2

(cid:19)

The posterior distribution of w is a combination of the prior and the likelihood (Bayes’ rule):

log p(w|D) ∼

(w − w0)(cid:62)C−1(w − w0)

+ L.

(9)

We can perform a numerical maximization of the log posterior using Newton’s method (we used the
Matlab function fminunc)  knowing its gradient j and the hessian H explicitly:

j =

∂(log p)

∂w

= −C−1(w − w0) +

∂L
∂w

 

H =

∂2(log p)

∂w2 = −C−1 +

∂2L
∂w2 .

(10)

The maximum a posteriori (MAP) estimate ˆw is where the gradient vanishes  j( ˆw) = 0. If we work
with a Laplace approximation  the posterior covariance is Cov = −H−1 evaluated at w = ˆw.

3.4 Hyperparameter optimization

The model hyperparameters consist of σ1  governing the variance of w1  the weights on the ﬁrst trial
of a session  and σ  governing the variance of the trial-to-trial diffusive change of the weights. To set
these hyperparameters  we ﬁxed σ1 to a large default value  and used maximum marginal likelihood
or “evidence optimization” over a ﬁxed grid of σ [4  11  13]. The marginal likelihood is given by:

p(y|x  σ) =

dwp(y|x  w)p(w|σ) =

p(y|x  w)p(w|σ)

p(w|x  y  σ)

≈ exp(L) · N (w|w0  C)
N (w| ˆw −H−1)

 

(11)

(cid:90)

where ˆw is the MAP estimate of the entire vector of time-varying weights and H is the Hessian of the
log-posterior over w at its mode. This formula for marginal likelihood results from the well-known
Laplace approximation to the posterior [11  12]. We found the estimate not to be insensitive to σ1 so
long as it is sufﬁciently large.

3.5 Application

We tested our method using a simulation  drawing binary responses from a stimulus-free GLM
yt ∼ logistic(wt)  where wt was diffused as wt+1 ∼ N (wt  σ2) with a ﬁxed hyperparameter
σ. Given the time series of responses {yt}  our method captures the true σ through evidence
maximization  and provides a good estimate of the time-varying w = {wt} (Figure 2A). Whereas the
estimate of the weight wt is robust over independent realizations of the responses  the instantaneous
weight changes ∆w = wt+1 − wt are not reproducible across realizations (Figure 2B). Therefore it is
difﬁcult to analyze the trial-to-trial weight changes directly from real data  where only one realization
of the learning process is accessible.

4

Figure 2: Estimating time-varying model parameters. (A-B) Simulation: (A) Our method captures
the true underlying variability σ by maximizing evidence. (B) Weight estimates are accurate and
robust over independent realizations of the responses  but weight changes across realizations are
not reproducible. (C-E) From the choice behavior of a rat under training  we could (C) estimate the
time-varying weights of its psychometric model  and (D) determine the characteristic variability by
evidence maximization. (E) The number of history terms to be included in the model was determined
by comparing the BIC  using the early/mid/late parts of the rat dataset. Because log-likelihood is
calculated up to a constant normalization  both log-evidence and BIC are shown in relative values.

We also applied our method to an actual experimental dataset from rats during the early training
period for a 2AFC discrimination task  as introduced in Section 2 (using classical training methods
[3]  see Supplementary Material for detailed description). We estimated the time-varying weights
of the GLM (Figure 2C)  and estimated the characteristic variability of the rat behavior σrat = 2−7
by maximizing marginal likelihood (Figure 2D). To determine the length d of the trial history
dependence  we ﬁt models with varying d and used the Bayesian Information Criterion BIC(d) =
−2 log L(d) + K(d) log N (Figure 2E). We found that animal behavior exhibits long-range history
depedence at the beginning of training  but this dependence becomes shorter as training progresses.
Near the end of the dataset  the behavior of the rat is best described drat = 1 (single-trial history
dependence)  and we use this value for the remainder of our analyses.

4

Incorporating learning

The fact that animals show improved performance  as training progresses  suggests that we need a
non-random component in our model that accounts for learning. We will ﬁrst introduce a simple
model of weight change based on the ideas from reinforcement learning  then discuss how we can
incorporate the learning model into our time-varying estimate method.
A good candidate model for animal learning is the policy gradient update from reinforcement learning 
for example as in [15]. There are debates as to whether animals actually learn using policy-based
methods  but it is difﬁcult to deﬁne a reasonable value function that is consistent with our preliminary
observations of rat behavior (e.g. win-stay/lose-switch). A recent experimental study supports the
use of policy-based models in human learning behavior [10].

4.1 RewardMax model of learning (policy gradient update)

Here we consider a simple model of learning  in which the learner attempts to update its policy (here
the weight parameters in the model) to maximize the expected reward. Given some ﬁxed reward
function r(x  y)  the expected reward at the next-upcoming trial t is deﬁned as

(cid:68)(cid:104)r(xt  yt)(cid:105)p(yt|xt wt)

(cid:69)

ρ(wt) =

PX (xt)

(12)

where PX (xt) reﬂects the subject animal’s knowledge as to the probability that a given stimulus x
will be presented at trial t  which may be dynamically updated. One way to construct the empirical

5

010002000trials-0.500.51weight wtrue weightbest fit-8-6-4log2 -12-8-40log evd. (rel.)log evdmax-evdtrue 010002000trials-0.500.51weight wtrue weightrepeated fits-2024w (rep 1) 10-3-2024w (rep 2)10-3-0.500.5bias b00.51sensitivity a1sensitivity a20200040006000trials0.511.5history dependence h-12-10-8-6-4log2 -400-300-200-1000log evd. (rel.)log evdmax-evd012345d (trials back)-200-1000BIC (rel.)earlymidlateABDCEFigure 3: Estimating the learning model. (A-B) Simulated learner with σsim = αsim = 2−7. (A) The
four weight parameters of the simulated model are successfully recovered by our MAP estimate with
the learning effect incorporated  where (B) the learning rate α is accurately determined by evidence
maximization. (C) Evidence maximization analysis on the rat training dataset reveals σrat = 2−6
and αrat = 2−10. Displayed is a color plot of log evidence on the hyperparameter plane (in relative
values). The optimal set of hyperparameters is marked with a star.

(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)t

∂ρ
∂w

PX is to accumulate the stimulus statistics up to some timescale τ ≥ 0; here we restrict to the simplest
limit τ = 0  where only the most recent stimulus is remembered. That is  PX (xt) = δ(xt − xt−1).
In practice ρ can be evaluated at wt = wt−1  the posterior mean from previous observations.
Under the GLM (2)  the choice probability is p(y|x  w) = 1/(1 + exp(−yg(x)(cid:62)w))  where
L = −1 and R = +1  trial index suppressed. Therefore the expected reward can be written out
explicitly  as well as its gradient with respect to w:

where we deﬁne the effective reward function f (x) ≡(cid:80)

x∈X

∂ρ
∂w

=

PX (x) f (x) pR(x  w) pL(x  w) g(x)

y∈Y yr(x  y) for each stimulus. In the
spirit of the policy gradient update  we consider the RewardMax model of learning  which assumes
that the animal will try to climb up the gradient of the expected reward by

(13)

∆wt = α

(14)
where ∆wt = (wt+1 − wt). In this simplest setting  the learning rate α is the only learning
hyperparameter φ = {α}. The model can be extended by incorporating more realistic aspects of
learning  such as the non-isotropic learning rate  the rate of weight decay (forgetting)  or the skewness
between experienced and unexperienced rewards. For more discussion  see Supplementary Material.

≡ v(wt  xt; φ) 

4.2 Random walk prior with drift

Because our observation of a given learning process is stochastic and the estimate of the weight
change is not robust (Figure 2B)  it is difﬁcult to test the learning rule (14) on any individual dataset.
However  we can still assume that the learning rule underlies the observed weight changes as

(cid:104)∆w(cid:105) = v(w  x; φ)

(15)
where the average (cid:104)·(cid:105) is over hypothetical repetitions of the same learning process. This effect of
non-random learning can be incorporated into our random walk prior as a drift term  to make a fully
Bayesian model for an imperfect learner. The new weight update prior is written as D(w − w0) =
v + ξ  where v is the “drift velocity” and ξ ∼ N (0  Σ) is the noise. The modiﬁed prior is

w − D−1v ∼ N (w0  C) 

(16)
Equations (9-10) can be re-written with the additional term D−1v. For the RewardMax model
v = α∂ρ/∂w  in particular  the ﬁrst and second derivatives of the modiﬁed log posterior can be
written out analytically. Details can be found in Supplementary Material.

C−1 = D(cid:62)Σ−1D.

4.3 Application

To test the model with drift  a simulated RewardMax learner was generated  based on the same task
structure as in the rat experiment. The two hyperparameters {σsim  αsim} were chosen such that the

6

010002000trials-101model weightstrueestimated-9-8-7-6log2 -2-10log evidence (rel.)log evidencetrue max-evd-11-10-9-8-7log2 -8-7-6-5log2 -20-15-10-50log evidence (rel.)ABCresulting time series data is qualitatively similar to the rat data. The simulated learning model can be
recovered by maximizing the evidence (11)  now with the learning hyperparameter α as well as the
variability σ. The solution accurately reﬂects the true αsim  shown where σ is ﬁxed at the true σsim
(Figures 3A-3B). Likewise  the learning model of a real rat was obtained by performing a grid search
on the full hyperparameter plane {σ  α}. We get σrat = 2−6 and αrat = 2−10 (Figure 3C). 2
Can we determine whether the rat’s behavior is in a regime where the effect of learning dominates the
effect of noise  or vice versa? The obtained values of σ and α depend on our choice of units which
is arbitrary; more precisely  α ∼ [w2] and σ ∼ [w] where [w] scales as the weight. Dimensional
analysis suggests a (dimensionless) order parameter β = α/σ2  where β (cid:29) 1 would indicate a regime
where the effect of learning is larger than the effect of noise. Our estimate of the hyperparameters
gives βrat = αrat/σ2

rat ≈ 4  which leaves us optimistic.

5 AlignMax: Adaptive optimal training

Whereas the goal of the learner/trainee is (presumably) to maximize the expected reward  the trainer’s
goal is to drive the behavior of the trainee as close as possible to some ﬁxed model that corresponds
to a desirable  yet hypothetically achievable  performance. Here we propose a simple algorithm that
aims to align the expected model parameter change of the trainee (cid:104)∆wt(cid:105) = v(wt  xt; φ) towards a
ﬁxed goal wgoal. We can summarize this in an AlignMax training formula

xt+1 = argmax

x

(wgoal − wt)(cid:62) (cid:104)∆wt(cid:105) .

(17)

x∈X PX (x)(cid:80)

is deﬁned as DKL =(cid:80)

Looking at Equations (13)  (14) and (17)  it is worth noting that g(x) puts a heavier weight on more
distinguishable or “easier” stimuli (exploitation)  while pLpR puts more weight on more difﬁcult
stimuli  with more uncertainty (exploration); an exploitation-exploration tradeoff emerges naturally.
We tested the AlignMax training protocol3 using a simulated learner with ﬁxed hyperparameters
αsim = 0.005 and σsim = 0  using wgoal = (b  a1  a2  h)goal = (0 −10  10  0) in the current
paradigm. We chose a noise-free learner for clear visualization  but the algorithm works as well
in the presence of noise (σ > 0  see Supplementary Material for a simulated noisy learner). As
expected  our AlignMax algorithm achieves a much faster training compared to the usual algorithm
where stimuli are presented randomly (Figure 4). The task performance was measured in terms of the
success rate  the expected reward (12)  and the Kullback-Leibler (KL) divergence. The KL divergence
y∈Y ˆpy(x) log(ˆpy(x)/py(x)) where ˆpy(x) = r(x  y) is the
“correct” psychometric function  and a smaller value of DKL indicates a behavior that is closer to
the ideal. Both the expected reward and the KL divergence were evaluated using a uniform stimulus
distribution PX (x). The low success rate is a distinctive feature of the adaptive training algorithm 
which selects adversarial stimuli such that the “lazy ﬂukes” are actively prevented (e.g. such that a
left-biased learner wouldn’t get thoughtless rewards from the left side). It is notable that the AlignMax
training eliminates the bias b and the history dependence h (the two stimulus-independent parameters)
much more quickly compared to the conventional (random) algorithm  as shown in Figure 4A.
Two general rules were observed from the optimal trainer. First  while the history dependence h is
non-zero  AlignMax alternates between different stimulus groups in order to suppress the win-stay
behavior; once h vanishes  AlignMax tries to neutralize the bias b by presenting more stimuli from the
“non-preferred” stimulus group yet being careful not to re-install the history dependence. For example 
it would give LLRLLR... for an R-biased trainee. This suggests that a pre-deﬁned  non-adaptive
de-biasing algorithm may be problematic as it may reinforce an unwanted history dependence (see
Supp. Figure S1). Second  AlignMax exploits the full stimulus space by starting from some “easier”
stimuli in the early stage of training (farther away from the true separatrix x1 = x2)  and presenting
progressively more difﬁcult stimuli (closer to the separatrix) as the trainee performance improves.
This suggests that using the reduced stimulus space may be suboptimal for training purposes. Indeed 
training was faster on the full stimulus plane  than on the reduced set (Figures 4B-4C).

2Based on a 2000-trial subset of the rat dataset.
3When implementing the algorithm within the current task paradigm  because of the way we model the
history variable as part of the stimulus  it is important to allow the algorithm to choose up to d + 1 future stimuli 
in this case as a pair {xt+1  xt+2}   in order to generate a desired pattern of trial history.

7

Figure 4: AlignMax training (solid lines) compared to a random training (dashed lines)  for a
simulated noise-free learner. (A) Weights evolving as training progresses  shown from a simulated
training on the full stimulus space shown in Figure 1A. (B-C) Performances measured in terms of
the success rate (moving average over 500 trials)  the expected reward and the KL divergence. The
simulated learner was trained either (B) in the full stimulus space  or (C) in the reduced stimulus
space. The low success rate is a natural consequence of the active training algorithm  which tends to
select adversarial stimuli to facilitate learning.

6 Discussion

In this work  we have formulated a theory for designing an optimal training protocol of animal
behavior  which works adaptively to drive the current internal model of the animal toward a desired 
pre-deﬁned objective state. To this end  we have ﬁrst developed a method to accurately estimate the
time-varying parameters of the psychometric model directly from animal’s behavioral time series 
while characterizing the intrinsic variability σ and the learning rate α of the animal by empirical
Bayes. Interestingly  a dimensional analysis based on our estimate of the learning model suggests
that the rat indeed lives in a regime where the effect of learning is stronger than the effect of noise.
Our method to infer the learning model from data is different from many conventional approaches of
inverse reinforcement learning  which also seek to infer the underlying learning rules from externally
observable behavior  but usually rely on the stationarity of the policy or the value function. On the
contrary  our method works directly on the non-stationary behavior. Our technical contribution is
twofold: ﬁrst  building on the existing framework for estimation of state-space vectors [2  11  14] 
we provide a case in which parameters of a non-stationary model are successfully inferred from real
time-series data; second  we develop a natural extension of the existing Bayesian framework where
non-random model change (learning) is incorporated into the prior information.
The AlignMax optimal trainer provides important insights into the general principles of effective
training  including a balanced strategy to neutralize both the bias and the history dependence of the
animal  and a dynamic tradeoff between difﬁcult and easy stimuli that makes efﬁcient use of a broad
range of the stimulus space. There are  however  two potential issues that may be detrimental to the
practical success of the algorithm: First  the animal may suffer a loss of motivation due to the low
success rate  which is a natural consequence of the adaptive training algorithm. Second  as with any
model-based approach  mismatch of either the psychometric model (logistic  or any generalization
model) or the learning model (RewardMax) may result in poor performances of the training algorithm.
These issues are subject to tests on real training experiments. Otherwise  the algorithm is readily
applicable. We expect it to provide both a signiﬁcant reduction in training time and a set of reliable
measures to evaluate the training progress  powered by direct access to the internal learning model of
the animal.

Acknowledgments
JHB was supported by the Samsung Scholarship and the NSF PoLS program. JWP was supported by grants
from the McKnight Foundation  Simons Collaboration on the Global Brain (SCGB AWD1004351) and the NSF
CAREER Award (IIS-1150186). We thank Nicholas Roy for the careful reading of the manuscript.

8

References
[1] A. Abrahamyan  L. L. Silva  S. C. Dakin  M. Carandini  and J. L. Gardner. Adaptable history

biases in human perceptual decisions. Proc. Nat. Acad. Sci.  113(25):E3548–E3557  2016.

[2] Y. Ahmadian  J. W. Pillow  and L. Paninski. Efﬁcient Markov chain Monte Carlo methods for

decoding neural spike trains. Neural Computation  23(1):46–96  2011.

[3] A. Akrami  C. Kopec  and C. Brody. Trial history vs. sensory memory - a causal study of
the contribution of rat posterior parietal cortex (ppc) to history-dependent effects in working
memory. Society for Neuroscience Abstracts  2016.

[4] C. M. Bishop. Pattern Recognition and Machine Learning. Information science and statistics.

Springer  2006.

[5] L. Busse  A. Ayaz  N. T. Dhruv  S. Katzner  A. B. Saleem  M. L. Schölvinck  A. D. Zaharia 
and M. Carandini. The detection of visual contrast in the behaving mouse. J. Neurosci. 
31(31):11351–11361  2011.

[6] A. Fassihi  A. Akrami  V. Esmaeili  and M. E. Diamond. Tactile perception and working

memory in rats and humans. Proc. Nat. Acad. Sci.  111(6):2331–2336  2014.

[7] I. Fründ  F. A. Wichmann  and J. H. Macke. Quantifying the effect of intertrial dependence on

perceptual decisions. J. Vision  14(7):9–9  2014.

[8] D. M. Green and J. A. Swets. Signal Detection Theory and Psychophysics. Wiley  New York 

1966.

[9] A. Hernández  E. Salinas  R. García  and R. Romo. Discrimination in the sense of ﬂutter: new

psychophysical measurements in monkeys. J. Neurosci.  17(16):6391–6400  1997.

[10] J. Li and N. D. Daw. Signals in human striatum are appropriate for policy update rather than

value prediction. J. Neurosci.  31(14):5504–5511  2011.

[11] L. Paninski  Y. Ahmadian  D. G. Ferreira  S. Koyama  K. Rahnama Rad  M. Vidne  J. Vogelstein 
and W. Wu. A new look at state-space models for neural data. J. Comp. Neurosci.  29(1):107–
126  2010.

[12] J. W. Pillow  Y. Ahmadian  and L. Paninski. Model-based decoding  information estimation  and
change-point detection techniques for multineuron spike trains. Neural Comput  23(1):1–45 
Jan 2011.

[13] M. Sahani and J. F. Linden. Evidence optimization techniques for estimating stimulus-response
functions. In S. Becker  S. Thrun  and K. Obermayer  editors  Adv. Neur. Inf. Proc. Sys. 15 
pages 317–324. MIT Press  2003.

[14] A. C. Smith  L. M. Frank  S. Wirth  M. Yanike  D. Hu  Y. Kubota  A. M. Graybiel  W. A.
Suzuki  and E. N. Brown. Dynamic analysis of learning in behavioral experiments. J. Neurosci. 
24(2):447–461  2004.

[15] R. S. Sutton  D. Mcallester  S. Singh  and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In S. A. Solla  T. K. Leen  and K. Muller  editors  Adv.
Neur. Inf. Proc. Sys. 12  pages 1057–1063. MIT Press  2000.

[16] C. W. Tyler and C.-C. Chen. Signal detection theory in the 2afc paradigm: Attention  channel

uncertainty and probability summation. Vision Research  40(22):3121–3144  2000.

9

,Misha Denil
Babak Shakibi
Laurent Dinh
Marc'Aurelio Ranzato
Nando de Freitas
Yunpeng Pan
Evangelos Theodorou
Yossi Arjevani
Ohad Shamir
Ji Hyun Bak
Jonathan Pillow