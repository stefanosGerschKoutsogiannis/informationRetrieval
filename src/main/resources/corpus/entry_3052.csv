2018,Learning convex bounds for linear quadratic control policy synthesis,Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a numbers of fields  from artificial intelligence and robotics  to medicine and finance.
This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function.
We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters  given data.
The algorithm involves sequential convex programing  and enjoys reliable local convergence and robust stability guarantees.
Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach  with strong performance and robustness properties observed in both.,Learning convex bounds for linear quadratic control

policy synthesis

Department of Information Technology

Department of Information Technology

Thomas B. Schön

Uppsala University

Sweden

thomas.schon@it.uu.se

Jack Umenberger

Uppsala University

Sweden

jack.umenberger@it.uu.se

Abstract

Learning to make decisions from observed data in dynamic environments remains
a problem of fundamental importance in a number of ﬁelds  from artiﬁcial intelli-
gence and robotics  to medicine and ﬁnance. This paper concerns the problem of
learning control policies for unknown linear dynamical systems so as to maximize
a quadratic reward function. We present a method to optimize the expected value
of the reward over the posterior distribution of the unknown system parameters 
given data. The algorithm involves sequential convex programing  and enjoys
reliable local convergence and robust stability guarantees. Numerical simulations
and stabilization of a real-world inverted pendulum are used to demonstrate the
approach  with strong performance and robustness properties observed in both.

1 Introduction

Decision making for dynamical systems in the presence of uncertainty is a problem of great prevalence
and importance  as well as considerable difﬁculty  especially when knowledge of the dynamics is
available only via limited observations of system behavior. In machine learning  the data-driven
search for a control policy to maximize the expected reward attained by a stochastic dynamic process
is known as reinforcement learning (RL) [45]. Despite remarkable recent success in games [32  43]  a
major obstacle to the deployment RL-based control on physical systems (e.g. robots and self-driving
cars) is the issue of robustness  i.e.  guaranteed safe and reliable operation. With the necessity of such
guarantees widely acknowledged [2]  so-called ‘safe RL’ remains an active area of research [21].
The problem of robust automatic decision making for uncertain dynamical systems has also been
the subject of intense study in the area of robust control (RC) [57]. In RC  one works with a set
of plausible models and seeks a control policy that is guaranteed to stabilize all models within the
set. In addition  there is also a performance objective to optimize  i.e. a reward to be maximized  or
equivalently  a cost to be minimized. Such cost functions are usually deﬁned with reference to either
a nominal model [20  25] or the worst-case model [36] in the set. RC has been extremely successful
in a number of engineering applications [38]; however  as has been noted  e.g.  [48  35]  robustness
may (understandably) come at the expense of performance  particularly for worst-case design.
The problem we address in this paper lies at the intersection of reinforcement learning and robust
control  and can be summarized as follows: given observations from an unknown dynamical system 
we seek a policy to optimize the expected cost (as in RL)  subject to certain robust stability guarantees
(as in RC). Speciﬁcally  we focus our attention on control of linear time-invariant dynamical systems 
subject to Gaussian disturbances  with the goal of minimizing a quadratic function penalizing state
deviations and control action. When the system is known  this is the classical linear quadratic
regulator (LQR)  a.k.a. H2  optimal control problem [8]. We are interested in the setting in which the
system is unknown  and knowledge of the dynamics must be inferred from observed data.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Contributions and paper structure The principal contribution of this paper is an algorithm
to optimize the expected value of the linear quadratic regulator reward/cost function  where the
expectation is w.r.t.
the posterior distribution of unknown system parameters  given observed
data; cf. Section 3 for a detailed problem formulation. Speciﬁcally  we construct a sequence of
convex approximations (upper bounds) to the expected cost  that can be optimized via semideﬁnite
programing [50]. The algorithm  developed in Section 4  invokes the majorize-minimization (MM)
principle [29]  and consequently enjoys reliable convergence to local optima. An important part of
our contribution lies in guarantees on the robust stability properties of the resulting control policies 
cf. Section 4.3. We demonstrate the proposed method via two experimental case studies: i) the
benchmark problem on simulated systems considered in [17  48]  and ii) stabilization of a real-world
inverted pendulum. Strong performance and robustness properties are observed in both. Moving
forward  from a machine learning perspective this work contributes to the growing body of research
concerned with ensuring robustness in RL  cf. Section 2. From a control perspective  this work
appropriates cost functions more commonly found in RL (namely  expected reward) to a RC setting 
with the objective of reducing conservatism of the resulting robust control policies.

2 Related work

Incorporating various notions of ‘robustness’ into RL has long been an area of active research [21].
In so-called ‘safe RL’  one seeks to respect certain safety constraints during exploration and/or policy
optimization  for example  avoiding undesirable regions of the state-action space [22  1]. A related
problem is addressed in ‘risk-sensitive RL’  in which the search for a policy takes both the expected
value and variance of the reward into account [31  19]. Recently  there has been an increased interest
in notions of robustness more commonly considered in control theory  chieﬂy stability [35  3]. Of
particular relevance is the work of [4]  which employs Lyapunov theory [27] to verify stability of
learned policies. Like the present paper  [4] adopts a Bayesian framework; however  [4] makes use
of Gaussian processes [39] to model the uncertain nonlinear dynamics  which are assumed to be
deterministic. A major difference between [4] and our work is the cost function; in the former the
policy is selected by optimizing for worst-case performance  whereas we optimize the expected cost.
Robustness of data-driven control has also been the focus of a recently developed family of methods
referred to as ‘coarse-ID control’  cf. [47  17  7  44]  in which ﬁnite-data bounds on the accuracy
of the least squares estimator are combined with modern robust control tools  such as system level
synthesis [55]. Coarse-ID builds upon so-called ‘H1 identiﬁcation’ methods for learning models
of dynamical systems  along with error bounds that are compatible with robust synthesis methods
[26  14  13]. H1 identiﬁcation assumes an adversarial (i.e. worst-case) disturbance model  whereas
Coarse-ID is applicable to probabilistic models  such as those considered in the present paper. Of
particular relevance to the present paper is [17]  which provides sample complexity bounds on the
performance of robust control synthesis for the inﬁnite horizon LQR problem  when the true system
is not known. Such bounds necessarily consider the worst-case model  given the observed data  where
as we are concerned with expected cost over the posterior distribution of models.
This approach of controller synthesis w.r.t. distributions over models has much in common with
the ﬁeld of probabilistic robust control [11  46]. Early work in this area applied statistical learning
theory [53] to randomized algorithms for feasibility analysis and policy design  cf. e.g.  [51  52]. Of
particular relevance to the present paper is the so-called ‘scenario approach’ to control: robustness
requirements lead to semi-inﬁnite convex programs  which are approximated by sampling a ﬁnite
number of constraints  cf. e.g.  [9  10]. A key focus of the scenario approach is bounding sample
complexity (i.e.  the number of sampled constraints required to ensure some probability of feasibility) 
without resorting to statistical learning theory  so as to reduce conservatism.
In closing  we brieﬂy mention the so-called ‘Riemann-Stieltjes’ class of optimal control problems 
for uncertain continuous-time dynamical systems  cf. e.g.  [41  40]. Such problems often arise in
aerospace applications (e.g. satellite control) where the objective is to design an open-loop control
signal (e.g. for an orbital maneuver) rather than a feedback policy.

2

3 Problem formulation

In this section we describe in detail the speciﬁc problem that we address in this paper. The following
++) denotes the cone of
notation is used: Sn denotes the set of n ⇥ n symmetric matrices; Sn
+ (Sn
positive semdeﬁnite (positive deﬁnite) matrices. A ⌫ B denotes A  B 2 Sn
+  similarly for  and
++. The trace of A is denoted tr A. The transpose of A is denoted A0. |a|2
Q is shorthand for a0Qa.
Sn
The convex hull of set ⇥ is denoted conv⇥. The set of Schur stable matrices is denoted S.
Dynamics  reward function and policies We are concerned with control of discrete linear time-
invariant dynamical systems of the form

1

T PT

wt ⇠N (0  ⇧) 

xt+1 = Axt + But + wt 

(1)
where xt 2 Rnx  ut 2 Rnu  and wt 2 Rnw denote the state  input  and unobserved exogenous
disturbance at time t  respectively. Let ✓ := {A  B  ⇧}. Our objective is to design a feedback control
policy ut = (xt) that minimizes the cost function limT!1
t=0 E [x0tQxt + u0tRut]  where
xt evolves according to (1)  and Q ⌫ 0 and R  0 are user deﬁned weight matrices. A number of
different parametrizations of the policy  have been considered in the literature  from neural networks
(popular in RL  e.g.  [4]) to causal (typically linear) dynamical systems (common in RC  e.g.  [36]).
In this paper  we will restrict our attention to static-gain policies of the form ut = Kxt  where
K 2 Rnu⇥nx is constant. As noted in [17]  controller synthesis and implementation  is simpler (and
more computationally efﬁcient) for such policies. When the parameters of the true system  denoted
✓tr := {Atr  Btr  ⇧tr}  are known this is the inﬁnite horizon LQR problem  the optimal solution of
which is well-known [5]. We assume that ✓tr is unknown; rather  our knowledge of the dynamics
must be inferred from observed sequences of inputs and states.
Observed data We adopt the data-driven setup used in [17]  and assume that D := {xr
0:T}N
r=1
where xr
t=0 is the observed state sequence attained by evolving the true system for T
t}T
time steps  starting from an arbitrary xr
t=0. Each of
t}T
0:T = {ur
these N independent experiments is referred to as a rollout. We perform parameter inference in the
ofﬂine/batch setting; i.e.  all data D is assumed to be available at the time of controller synthesis.
Optimization objective Given observed data and  possibly  prior knowledge of the system  we
then have the posterior distribution over the model parameters denoted ⇡(✓) := p(A  B  ⇧|D)  in
place of the true parameters ✓tr. The function that we seek to minimize is the expected cost w.r.t. the
posterior distribution  i.e. 

0 and driven by arbitrary input ur

0:T = {xr

0:T   ur

1
T

TXt=0

E [x0tQxt + u0tRut | xt+1 = Axt + But + wt  wt ⇠N (0  ⇧)   {A  B  ⇧}⇠ ⇡(✓)] .
lim
T!1
(2)
In practice  the support of ⇡ almost surely contains {A  B} that are unstabilizable  which implies
that (2) is inﬁnite. Consequently  we shall consider averages over conﬁdence regions w.r.t. ⇡. For
convenience  let us denote the inﬁnite horizon LQR cost  for given system parameters ✓  by

J(K|✓) := lim
t!1

E [x0t(Q + K0RK)xt | xt+1 = (A + BK)xt + wt  w ⇠N (0  ⇧)]
=⇢tr X⇧ with X = (A + BK)0X(A + BK) + Q + K0RK  A + BK 2S

otherwise 

1 

(3a)

(3b)

where the second equality follows from standard Gramian calculations  and S denotes the set of
Schur stable matrices. As an alternative to (2) we may consider a cost function like J c(K) :=
R⇥c J(K|✓)⇡(✓)d✓  where ⇥c denotes a c % conﬁdence region of the parameter space w.r.t. the
posterior ⇡. Though better suited to optimization than (2)  which is almost surely inﬁnite  this integral
cannot be evaluated in closed form  due to the complexity of J(·|✓) w.r.t. ✓. Furthermore  there is
still no guarantee that ⇥c contain only stabilizable models. To circumvent both of these issues  we
propose the following Monte Carlo (MC) approximation of J c(K) 
i ⇠ ⇥c \M  

(4)
where M is the number of samples used  and M denotes the set of stabilizable {A  B}. Note that (4)
is not a true MC approximation of J c(K) as only stabilizable samples {Ai  Bi}2M are used.

MXM

J(K|✓i) ✓

J c
M (K) :=

i = 1  . . .   M 

i=1

1

3

t=1

t|xr

t1  ur

r=1YT
t1 ✓ ) = NAxr

Posterior distribution Given data D  the parameter posterior distribution is given by Bayes’ rule:
(5)

p(xr

⇡(✓) := p(✓|D) =

t1 ✓ ) =: ¯⇡(✓) 

1
p(D)

p(D|✓)p(✓) / p(✓)YN

t|xr

t1  ur

t1 + Bur

t1  ⇧  and
where p(✓) denotes our prior belief on ✓  p(xr
¯⇡ = p(D)⇡ denotes the unnormalized posterior. To sample from ⇡  we can distinguish between two
different cases. First  consider the case when ⇧tr is known or can be reliably estimated independently
of {A  B}. This is the setting in  e.g.  [17]. In this case  the likelihood can be equivalently expressed as
a Gaussian distribution over {A  B}. Then  when the prior p(A  B) is uniform (i.e. non-informative)
or Gaussian (self-conjugate)  the posterior p(A  B|⇧tr D) is also Gaussian  cf. Appendix A.1.1.
Second  consider the general case in which ⇧tr  along with {A  B}  is unknown. In this setting  one
can select from a number of methods adapted for Bayesian inference in dynamical systems  such as
Metropolis-Hastings [33]  Hamiltonian Monte Carlo [15]  and Gibbs sampling [16  56]. When one
places a non-informative prior on ⇧ (e.g.  p(⇧) / det(⇧) nx+1
)  each iteration of a Gibbs sampler
targeting ⇡ requires sampling from either a Gaussian or an inverse Wishart distribution  for which
reliable numerical methods exist; cf. Appendix A.1.2. In both of these cases we can sample from
⇡ and evaluate ¯⇡ point-wise. To draw ✓i ⇠ ⇥c \M   as in (4)  we can ﬁrst draw a large number of
samples from ⇡  discard the (100c)% of samples with the lowest unnormalized posterior values 
and then further discard any samples that happen to be unstabilizable. For convenience  we deﬁne
˜⇥c
i=1 : ✓i ⇠ ⇥c \M   i = 1  . . .   M}  which should be interpreted as a set of M
M := {{✓i}M
realizations of this procedure for sampling ✓i ⇠ ⇥c \M .
Summary We seek the solution of the optimization problem minK J c

2

M (K) for K 2 Rnu⇥nx.

4 Solution via semideﬁnite programing

In this section we present the principal contribution of this paper: a method for solving minK J c
M (K)
via convex (semideﬁnite) programing (SDP). It is convenient to consider an equivalent representation

min
i=12Snx
K  {Xi}M
s.t.

++

1

tr Xi⇧i 

MXM
Xi ⌫ (Ai + BiK)0Xi(Ai + BiK) + Q + K0RK  {Ai  Bi  ⇧i}2 ˜⇥c
M  

i=1

(6a)

(6b)

where the Comparison Lemma [34  Lecture 2] has been used to replace the equality in (3b) with the
✏ := {S 2 Sn : S ⌫ ✏I  S  µI}  where ✏ and µ are
inequality in (6b). We introduce the notation Sn
arbitrarily small and large positive constants  respectively. Sn
✏ serves as a compact approximation of
++  suitable for use with SDP solvers  i.e.  S 2 Sn
Sn
4.1 Common Lyapunov relaxation

✏ =) S 2 Sn

++.

The principal challenge in solving (6) is that the constraint (6b) is not jointly convex in K and X i.
The usual approach to circumventing this nonconvexity is to ﬁrst apply the Schur complement to
(6b)  and then conjugate by the matrix diag(X1
  I  I  I)  which leads to the equivalent constraint

i

2664

X1

i

X1

i

(Ai + BiK)X1

i

Q1/2X1
i
KX1

i

X1

(Ai + BiK)0 X1
0
I
0

i
0
0

i Q1/2 X1

i K0

3775 ⌫ 0.

0
R1

(7)

and Li = KX1

With the change of variables Yi = X1
  (7) becomes an linear matrix inequality
(LMI)  in Yi and Li. This approach is effective when M = 1 (i.e. we have a single nominal system 
as in standard LQR). However  when M > 1 we cannot introduce a new Yi for each X1
  as we lose
uniqueness of the controller K in Li = KX1
for i 6= j. One
strategy (prevalent in robust control  e.g.  [17  §C]) is to employ a ‘common Lyapunov function’ 
i.e.  Y = X1
for all i = 1  . . .   M. This gives the following convex relaxation (upper bound) of

  i.e.  in general LiY 1

6= LjY 1

j

i

i

i

i

i

i

4

problem (6) 

K  Y 2Snx

✏

s.t.

min
  {Zi}M

i=12Snx

G0i

Y  ⌫ 0  2664
 Zi Gi
tr Y 1⇧i  min

Y Zi

min
Y

tr Zi 

(8a)

Y A0i + L0B0i Y Q1/2

Y

AiY + BiL

Q1/2Y

L

Y
0
0

0
I
0

tr Zi s.t. Zi ⌫ G0iY 1Gi () min

Y Zi

(8b)

L0

M  

0
R1

3775 ⌫ 0 ✓ i 2 ˜⇥c
Y  ⌫ 0.
tr Zi s.t.  Zi Gi

G0i

where Gi denotes the Cholesky factorization of ⇧i  i.e.  ⇧i = GiG0i  and {Zi}M
used to encode the cost (6a) with the change of variables  i.e. 

i=1 are slack variables

The approximation in (8) is highly conservative  which motivates the iterative local optimization
method presented in Section 4.2. Nevertheless  (8) provides a principled way (i.e.  a one-shot convex
program) to initialize the iterative search method derived in Section 4.2.

Iterative improvement by sequential semideﬁnite programing

4.2
To develop this iterative search method ﬁrst consider an equivalent representation of J(K|✓i) 
J(K|✓i) = min
Xi2Snx

tr Xi⇧i

✏

(9a)

s.t. 24

Xi  Q (Ai + BiK)0 K0
0
Ai + BiK

X1

K

i
0

R1 35 ⌫ 0 

recall: ✓i = {Ai  Bi  ⇧i}.

(9b)

This representation highlights the nonconvexity of J(K|✓i) due to the X1
term  which was addressed
(in the usual way) by a change of variables in Section 4.1. In this section  we will instead replace X1
i
with a linear approximation and prove that this leads to a tight convex upper bound. Given S 2 Sn
++ 
linear) Taylor series approximation of S1 about some
let T (S  S0) denote the ﬁrst order (i.e.
0 + @S1
(S  S0) S1
nominal S0 2 Sn
0 .
We now deﬁne the function

++  i.e.  T (S  S0) := S1

(S  S0) = S1

0  S1

0

i

@S S=S0

ˆJ(K  ¯K|✓i) := min
Xi2Snx

✏

tr Xi⇧i

s.t. 24

Xi  Q (Ai + BiK)0 K0
Ai + BiK T (Xi  ¯Xi)
0

K

0

R1 35 ⌫ 0 

(10a)

(10b)

where ¯Xi is any Xi 2 Snx
J( ¯K|✓i) = tr ¯Xi⇧i. Analogously to (4)  we deﬁne

✏

that achieves the minimum in (9)  with K = ¯K for some nominal ¯K  i.e. 

ˆJ c
M (K  ¯K) :=

ˆJ(K  ¯K|✓i).

(11)

1

MX✓i2 ˜⇥c

M

M (K  ¯K) is a convex upper bound on J c

We now show that ˆJ c
proof is given in A.2.2 and makes use of the following technical lemma (cf. A.2.1 for proof) 
Lemma 4.1. T (S  S0)  S1 for all S  S0 2 Sn
series expansion of S1 about S0 .
Theorem 4.1. Let ˆJ c
M (K  ¯K) is a convex upper bound on J c
ˆJ c
bound is ‘tight’ at ¯K  i.e.  ˆJ c
M ( ¯K  ¯K) = J c

M (K  ¯K) be deﬁned as in (11)  with ¯K such that J c

M (K)  i.e.  ˆJ c
M ( ¯K).

M ( ¯K) is ﬁnite. Then
M (K) 8K. Furthermore  the

++  where T (S  S0) denotes the ﬁrst-order Taylor

M (K)  which is tight at K = ¯K. The

M (K  ¯K)  J c

Iterative algorithm To improve upon the common Lyapunov solution given by (8)  we can solve
a sequence of convex optimization problems: K(k+1) = arg minK ˆJ c
M (K  K(k))  cf. Algorithm 1
for details. This procedure of optimizing tight surrogate functions in lieu of the actual objective

5

M (K(k+1)  K(k))  J c

function is an example of the ‘majorize-minimization (MM) principle’  a.k.a. optimization transfer
[29]. MM algorithms enjoy good numerical robustness  and (with the exception of some pathological
cases) reliable convergence to local minima [49]. Indeed  it is readily veriﬁed that J c
M (K(k)) =
ˆJ c
M (K(k)  K(k))  ˆJ c
M (K(k+1))  where equality follows from tightness of
the bound  and the second inequality is due to the fact that ˆJ c
M (K  K(k)) is an upper bound. This
implies that {J c
Before proceeding  let us comment brieﬂy on the computational complexity of the approach  which
will be dominated by the convex program minK ˆJ c
M (K  ¯K) in (11). The complexity of each iteration
of an interior point method for solving this problem is O(max{m3  M mn3  M m2n2})  cf. e.g. [30 
§2]  where m = nxnu + M nx(nx + 1)/2 denotes the dimensionality of the decision variable  and
n = 2nx + nu denotes the dimension of the LMI in (10b). It has been observed that the number of
iterations required for convergence grows slowly with problem dimension [50]. For computation
times on numerical examples  refer to Table 2.

M (K(k))}1k=1 is a converging sequence.

from Section 3.

M (K) via semideﬁnite programing

Carlo approximation M  convergence tolerance ✏.

Algorithm 1 Optimization of J c
1: Input: observed data D  conﬁdence c  LQR cost matrices Q and R  number of particles in Monte
2: Generate M samples from ⇥c \M   i.e.  ˜⇥c
M  using the appropriate Bayesian inference method
3: Solve (8). Let Kcl denote the optimal solution of (8). Set K(0) 1  K(1) Kcl and k 1.
4: while |J c
5:
6: end while
7: return K(k) as the control policy.

M (K  K(k)). Set K(k+1) K⇤ and k k + 1.

M (K(k))  J c

M (K(k1))| >✏ do

Solve K⇤ = arg minK ˆJ c

Remark 4.1. This sequential SDP approach can be applied in other robust control settings  e.g. 
mixed H2/H1 [20]  to improve on the common Lyapunov solution  cf. Section5.1 for an illustration.

4.3 Robustness

Hitherto  we have considered the performance component of the robust control problem  namely
minimization of the expected cost; we now address the robust stability requirement. It is desirable for
the learned policy to stabilize every model in the conﬁdence region ⇥c; in fact  this is necessary for
the cost J c(K) to be ﬁnite. Algorithm 1 ensures stability of each of the M sampled systems from
˜⇥c
M  which implies that  stabilizes the entire region as M ! 1. However  we would like to be
able to say something about robustness for ﬁnite M. To this end  we make two remarks. First  if
closed-loop stability of each sampled model is veriﬁed with a common Lyapunov function  then the
policy stabilizes the convex hull of the sampled systems:
Theorem 4.2. Suppose there exists K 2 Rnx⇥nu such that (Ai + BiK)0X(Ai + BiK) X  0 for
X  0 and all ⇥= {Ai  Bi}N
i=1. Then (A + BK)0X(A + BK)  X  0 for all {A  B}2 conv⇥ 
where conv⇥ denotes the convex hull of ⇥.

The proof of Theorem 4.2 is given in A.2.3. The conditions of Theorem 4.2 hold for the common
Lyapunov approach in (8)  and can be made to hold for Algorithm 1 by introducing an additional
Lyapunov stability constraint (with common Lyapunov function) for each sampled system  at the
expense of some conservatism. Second  we observe empirically that Algorithm 1 returns policies
that very nearly stabilize the entire region ⇥c  despite a very modest number of samples M relative
to the dimension of the parameter space  cf. Section5.1  in particular Figure 2. In principle  results
from probabilistic robust control could be used to bound the number of samples required for such
robustness properties  cf. e.g.  [9  Theorem 1]  however  at least for the examples in this paper  such
bounds appear to be quite conservative. Furthermore  a number of recent papers have investigated
sampling (or grid) based approaches to stability veriﬁcation of control policies  e.g.  [54  4  6].
Understanding why policies from Algorithm 1 generalize effectively to the entire region ⇥c  as well
as clarifying connections to probabilistic robust control  are interesting topics for future research.

6

5 Experimental results

5.1 Numerical simulations using synthetic systems

t=1|xr

t  Axr

t1 Bur

r=1PT

In this section  we study the inﬁnite horizon LQR problem speciﬁed by
Atr = toeplitz(a  a0)  a = [1.01  0.01  0  . . .   0] 2 Rnx  Btr = I  ⇧tr = I  Q = 103I  R = I 
where toeplitz(r  c) denotes the Toeplitz matrix with ﬁrst row r and ﬁrst colum c. This is the same
problem studied in [17  §6] (for nx = 3)  where it is noted that such dynamics naturally arise in
consensus and distributed averaging problems. To obtain problem data D  each rollout involves
simulating (1)  with the true parameters  for T = 6 time steps  excited by ut ⇠N (0  I) with x0 = 0.
Note: to facilitate comparison with [17]  we too shall assume that ⇧tr is known. Furthermore  for all
experiments ⇥c will denote a 95% conﬁdence region  as in [17]. We compare the following methods
of control synthesis: existing methods: (i) nominal: standard LQR using the nominal model from
the least squares  i.e.  {Als  Bls} := arg minA BPN
t1|2; (ii) worst-case:
optimize for worst-case model (95% conﬁdence) s.t. robust stability constraints  i.e.  the method
of [17  §5.2]; (iii) H2/H1: enforce stability constraint from [17  §5.2]  but optimize performance
for the nominal model {Als  Bls}; proposed method(s): (iv) CL: the common Lyapunov relaxation
of 8; (v) proposed: the method proposed in this paper  i.e.  Algorithm 1; additional new methods:
(vi) alternate-r: initialize with the H2/H1 solution  and apply the iterative optimization method
proposed in Section 4.2  cf. Remark 4.1; (vii) alternate-s: optimize for the nominal model {Als  Bls} 
enforce stability for the sampled systems in ˜⇥c
M. Before proceeding  we wish to emphasize that
the different control synthesis methods have different objectives; a lower cost does not mean that
the associated method is ‘better’. This is particularly true for worst-case which seeks to optimize
performance for the worst possible model so as to bound the cost on the true system.
To evaluate performance  we compare the cost of applying a learned policy K to the true system
✓tr = {Atr  Btr}  to the optimal cost achieved by the optimal controller Klqr (designed using ✓tr) 
In Figure 1 we plot LQR
i.e.  J(K|✓tr)/J(Klqr|✓tr). We refer to this as ‘LQR suboptimality.’
suboptimality is shown as a function of the number of rollouts N  for nx = 3. We make the following
observations. Foremost  the methods that enforce stability ‘stochastically’ (i.e. point-wise)  namely
proposed and alternate-s  attain signiﬁcantly lower costs than the methods that enforce stability
‘robustly’. Furthermore  in situations with very little data  e.g. N = 5  the robust control methods are
usually unable to ﬁnd a stabilizing controller  yet the proposed method ﬁnds a stabilizing controller
in the majority of trials. Finally  we note that the iterative procedure in proposed (and alternate-s)
signiﬁcantly improves on the common-Lyapunov relaxation CL; similarly  alternate-r consistently
improves upon H2/H1 (as expected).

 

 

%
0
0
1

%
0
0
1

%
0
0
1

%
4
7

 

 

 

%
8
2

%
4
2

%
8
6

%
8
7

%
8
7

%
8
7

 

 

 

 

 

 

%
2
6

 

%
8
5

%
2

 

 

%
8
4

%
4

 

 

%
2
4

%
2

 

%
2

 

Figure 1: LQR suboptimality as a function of the number of rollouts (i.e. amount of training data).
1 suboptimality denotes cases in which the method was unable to ﬁnd a stabilizing controller for
the true system (including infeasibility of the optimization problem for policy synthesis)  and the %
denotes the frequency with which this occurred for the 50 experimental trials conducted.

7

Table 1: Median % of unstable closed-loop models  with open-loop models sampled from a 95%
conﬁdence region of the posterior  for system of varying dimension nx; cf. Section 5.1 for details. 50
experiments were conducted  with N = 50. The policy synthesis optimization problems were always
feasible  except for the worst-case method  which was infeasible in 46% of trials when nx = 12.
H2/H1 and alternate-r have the same robustness guarantees as worst-case  and are omitted.

optimal
61.6
95.37
99.6
100

nominal worst-case CL
28.75
58.41
81.9
94.28

0
0
0
0

0
0
0
0

nx
3
6
9
12

proposed
0.10
0.18
0.24
0.27

alternate-s
1.35
1.76
1.40
1.27

Table 2: Mean computation times in seconds for control synthesis for system of varying dimension
nx; cf. Section 5.1 for details. 50 experiments were conducted  with N = 50.

nx worst-case H2/H1 CL
3
6
9
12

0.159
0.173
0.208
0.329

1.91
2.05
2.51
3.72

0.605
0.962
1.79
3.90

proposed
20.3
28.9
48.1
96.8

alternate-s
4.56
13.4
27.1
62.9

It is natural to ask whether the reduction in cost exhibited by proposed (and alternate-s) come at
the expense of robustness  namely  the ability to stabilize a large region of the parameter space.
Empirical results suggest that this is not the case. To investigate this we sample 5000 fresh (i.e. not
used for learning) models from ⇥c \M and check closed-loop stability of each; this is repeated for
50 independent experiments with varying nx and N = 50. The median percentage of models that
were unstable in closed-loop is recorded in Table 1. We make two observations: (i) the proposed
method exhibits strong robustness. Even for nx = 12 (i.e.  288-dim parameter space)  it stabilizes
more than 99% of samples from the conﬁdence region  with only M = 100 MC samples. (ii) when
the robust methods (worst-case  H2/H1  alternate-r) are feasible  the resulting policies were found
to stabilize 100% of samples; however  for nx = 12  the methods were infeasible almost half the
time  whereas proposed always returned a policy. Further evidence is provided in Figure 2  which
plots robustness and performance as a function of the number of MC samples  M. For nx = 3
and M  800  the entire conﬁdence region is stabilized with very high probability  suggesting that
M ! 1 is not required for robust stability in practice.

101

100

10-1

10-2

0

200

400

600

800

1000

12
10
8
6
4
2
0

0

200

400

600

800

1000

Figure 2: (left) Median % of unstable closed-loop models  with open-loop models sampled from
a 95% conﬁdence region of the posterior  for nx = 3 and N = 15  as a function of the number of
samples M used in the MC approximation (4). (right) LQR suboptimality as a function of M. 50
experiments were conducted  cf. Section5.1 for details. Shaded regions cover the interquartile range.

8

%
%
0
0
0
0
1
1

%
%
0
0
0
0
1
1

%
0
0
1

(a)

(b)

Figure 3: (a) (Median) LQR cost on real-world pendulum experiment  as a function of the number
of rollouts. 1 cost denotes controllers that resulted in instability during testing. n/a denotes cases
in which the synthesis problem was infeasible. Five trials were conducted to evaluate the cost of
each policy. The shaded region spans from the minimum to maximum cost. Note: for this particular
experiment  the nominal models from least squares happened to yield stabilizing controllers that
offered good performance. Such behavior is not to be expected in general  cf. Figure 1. (b) pendulum
angle and control signal recorded after 10 rollouts.

5.2 Real-world experiments on a rotary inverted pendulum

We now apply the proposed algorithm to the classic problem of stabilizing a (rotary) inverted
pendulum  on real (i.e. physical) hardware (Quanser QUBE 2)  cf. A.3 for details. To generate
training data  the superposition of a non-stabilizing control signal and a sinusoid of random frequency
is applied to the rotary arm motor while the pendulum is inverted. The arm and pendulum angles
(along with velocities) are sampled at 100Hz until the pendulum angle exceeds 20  which takes
no more than 5 seconds. This constitutes one rollout. We applied the worst-case  H2/H1  and
proposed methods to optimize the LQ cost with Q = I and R = 1. To generate bounds ✏A 
kAls  Atrk2 and ✏B  kBls  Btrk2 for worst-case and H2/H1  we sample {Ai  Bi}5000
i=1 from a
95% conﬁdence region of the posterior  using Gibbs sampling  and take ✏A = maxi kAls  Aik2
and ✏B = maxi kBls  Bik2. The proposed method used 100 such samples for synthesis. We also
applied the least squares policy iteration method [28]  but none of the policies could stabilize the
pendulum given the amount of training data. Results are presented in Figure 3  from which we make
the following remarks. First  as in Section5.1  the proposed method achieves high performance
(low cost)  especially in the low data regime where the magnitude of system uncertainty renders the
other synthesis methods infeasible. Insight into this performance is offered by Figure 3(b)  which
indicates that policies from the proposed method stabilize the pendulum with control signals of
smaller magnitude. Second  performance of the proposed method converges after very few rollouts.
Data-inefﬁciency is a well-known limitation of RL; understanding and mitigating this inefﬁciency is
the subject of considerable research [17  48  18  42  23  24]. Investigating the role that a Bayesian
approach to uncertainty quantiﬁcation plays in the apparent sample-efﬁciency of the proposed method
is an interesting topic for further inquiry.

Acknowledgments

This research was ﬁnancially supported by the Swedish Foundation for Strategic Research (SSF)
via the project ASSEMBLE (contract number: RIT15-0012) and via the projects Learning ﬂexible
models for nonlinear dynamics (contract number: 2017-03807) and NewLEADS - New Directions
in Learning Dynamical Systems (contract number: 621-2016-06079)  both funded by the Swedish
Research Council.

9

References
[1] P. Abbeel and A. Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In Proceedings

of the 22nd international conference on Machine learning  pages 1–8. ACM  2005.

[2] D. Amodei  C. Olah  J. Steinhardt  P. Christiano  J. Schulman  and D. Mané. Concrete problems in AI

safety. arXiv preprint arXiv:1606.06565  2016.

[3] A. Aswani  H. Gonzalez  S. S. Sastry  and C. Tomlin. Provably safe and robust learning-based model

predictive control. Automatica  49(5):1216–1226  2013.

[4] F. Berkenkamp  M. Turchetta  A. Schoellig  and A. Krause. Safe model-based reinforcement learning with

stability guarantees. In Advances in Neural Information Processing Systems (NIPS). 2017.

[5] D. P. Bertsekas. Dynamic programming and optimal control. Belmont  MA: Athena Scientiﬁc  1995.

[6] R. Bobiti and M. Lazar. A sampling approach to ﬁnding lyapunov functions for nonlinear discrete-time

systems. In Control Conference (ECC)  2016 European  pages 561–566. IEEE  2016.

[7] R. Boczar  N. Matni  and B. Recht. Finite-data performance guarantees for the output-feedback control of

an unknown system. arXiv preprint arXiv:1803.09186  2018.

[8] J. B. Burl. Linear optimal control: H2 and H1 methods. Addison-Wesley Longman Publishing Co.  Inc. 

1998.

[9] G. C. Calaﬁore and M. C. Campi. Uncertain convex programs: randomized solutions and conﬁdence levels.

Mathematical Programming  102(1):25–46  2005.

[10] G. C. Calaﬁore and M. C. Campi. The scenario approach to robust control design. IEEE Transactions on

Automatic Control  51(5):742–753  2006.

[11] G. C. Calaﬁore  F. Dabbene  and R. Tempo. Research on probabilistic methods for control system design.

Automatica  47(7):1279–1293  2011.

[12] C. K. Carter and R. Kohn. On gibbs sampling for state space models. Biometrika  81(3):541–553  1994.
[13] J. Chen and G. Gu. Control-oriented system identiﬁcation: an H1 approach  volume 19. Wiley-

Interscience  2000.

[14] J. Chen and C. N. Nett. The Caratheodory-Fejer problem and H2/H1 identiﬁcation: a time domain
approach. In Proceedings of the 32nd IEEE Conference on Decision and Control (CDC)  pages 68–73 
1993.

[15] S. H. Cheung and J. L. Beck. Bayesian model updating using hybrid Monte Carlo simulation with applica-
tion to structural dynamic models with many uncertain parameters. Journal of engineering mechanics 
135(4):243–255  2009.

[16] J. Ching  M. Muto  and J. Beck. Bayesian linear structural model updating using Gibbs sampler with
modal data. In Proceedings of the 9th International Conference on Structural Safety and Reliability  pages
2609–2616. Millpress  2005.

[17] S. Dean  H. Mania  N. Matni  B. Recht  and S. Tu. On the sample complexity of the linear quadratic

regulator. arXiv preprint arXiv:1710.01688  2017.

[18] M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efﬁcient approach to policy search. In
Proceedings of the 28th International Conference on machine learning (ICML-11)  pages 465–472  2011.

[19] S. Depeweg  J. M. Hernández-Lobato  F. Doshi-Velez  and S. Udluft. Decomposition of Uncertainty in

Bayesian Deep Learning for Efﬁcient and Risk-sensitive Learning. arXiv:1710.07283  2017.

[20] J. Doyle  K. Zhou  K. Glover  and B. Bodenheimer. Mixed H2 and H1 performance objectives. II. Optimal

control. IEEE Transactions on Automatic Control  39(8):1575–1587  1994.

[21] J. Garcıa and F. Fernández. A comprehensive survey on safe reinforcement learning. Journal of Machine

Learning Research  16(1):1437–1480  2015.

[22] P. Geibel and F. Wysotzki. Risk-sensitive reinforcement learning applied to control under constraints.

Journal of Artiﬁcial Intelligence Research  24:81–108  2005.

10

[23] S. Gu  T. Lillicrap  Z. Ghahramani  R. E. Turner  and S. Levine. Q-prop: Sample-efﬁcient policy gradient

with an off-policy critic. In International Conference on Learning Representations  2017.

[24] S. Gu  T. Lillicrap  I. Sutskever  and S. Levine. Continuous deep Q-learning with model-based acceleration.

In International Conference on Machine Learning  pages 2829–2838  2016.

[25] W. M. Haddad  D. S. Bernstein  and D. Mustafa. Mixed-norm H2/H1 regulation and estimation: The

discrete-time case. Systems & Control Letters  16(4):235–247  1991.

[26] A. J. Helmicki  C. A. Jacobson  and C. N. Nett. Control oriented system identiﬁcation: a worst-
case/deterministic approach in H1. IEEE Transactions on Automatic control  36(10):1163–1176  1991.

[27] H. K. Khalil. Noninear systems. Prentice-Hall  New Jersey  2(5):5–1  1996.

[28] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of machine learning research 

4(Dec):1107–1149  2003.

[29] K. Lange  D. R. Hunter  and I. Yang. Optimization transfer using surrogate objective functions. Journal of

computational and graphical statistics  9(1):1–20  2000.

[30] Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approximation with application to

system identiﬁcation. SIAM J. Matrix Analysis and Applications  31(3):1235–1256  2009.

[31] O. Mihatsch and R. Neuneier. Risk-sensitive reinforcement learning. Machine learning  49(2-3):267–290 

2002.

[32] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves  M. Riedmiller 
A. K. Fidjeland  G. Ostrovski  et al. Human-level control through deep reinforcement learning. Nature 
518(7540):529  2015.

[33] B. Ninness and S. Henriksen. Bayesian system identiﬁcation via Markov chain Monte Carlo techniques.

Automatica  46(1):40–51  2010.

[34] Oliveira  Maurício de. MAE 280B: Linear Control Design.  2009.

[35] C. J. Ostafew  A. P. Schoellig  and T. D. Barfoot. Robust Constrained Learning-based NMPC enabling
reliable mobile robot path tracking. The International Journal of Robotics Research  35(13):1547–1563 
2016.

[36] I. R. Petersen  M. R. James  and P. Dupuis. Minimax optimal control of stochastic uncertain systems with

relative entropy constraints. IEEE Transactions on Automatic Control  45(3):398–412  2000.

[37] K. B. Petersen and M. S. Pedersen. The Matrix Cookbook.  2012.

[38] I. Postlethwaite  M. C. Turner  and G. Herrmann. Robust control applications. Annual Reviews in Control 

31(1):27–39  2007.

[39] C. E. Rasmussen. Gaussian processes in machine learning. In Advanced lectures on machine learning 

pages 63–71. Springer  2004.

[40] I. M. Ross  R. J. Proulx  and M. Karpenko. Unscented optimal control for space ﬂight. In 24th International

Symposium on Space Flight Dynamics  2014.

[41] I. M. Ross  R. J. Proulx  M. Karpenko  and Q. Gong. Riemann–stieltjes optimal control problems for

uncertain dynamic systems. Journal of Guidance  Control  and Dynamics  38(7):1251–1263  2015.

[42] J. Schulman  P. Moritz  S. Levine  M. Jordan  and P. Abbeel. High-dimensional continuous control using

generalized advantage estimation. arXiv preprint arXiv:1506.02438  2015.

[43] D. Silver  A. Huang  C. J. Maddison  A. Guez  L. Sifre  G. Van Den Driessche  J. Schrittwieser 
I. Antonoglou  V. Panneershelvam  M. Lanctot  et al. Mastering the game of Go with deep neural
networks and tree search. nature  529(7587):484–489  2016.

[44] M. Simchowitz  H. Mania  S. Tu  M. I. Jordan  and B. Recht. Learning without mixing: Towards a sharp

analysis of linear system identiﬁcation. arXiv preprint arXiv:1802.08334  2018.

[45] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction  volume 1. MIT press Cambridge 

1998.

11

[46] R. Tempo  G. Calaﬁore  and F. Dabbene. Randomized algorithms for analysis and control of uncertain

systems: with applications. Springer Science & Business Media  2012.

[47] S. Tu  R. Boczar  A. Packard  and B. Recht. Non-asymptotic analysis of robust control from coarse-grained

identiﬁcation. arXiv preprint arXiv:1707.04791  2017.

[48] S. Tu and B. Recht. Least-squares temporal difference learning for the linear quadratic regulator. arXiv

preprint arXiv:1712.08642  2017.

[49] F. Vaida. Parameter convergence for EM and MM algorithms. Statistica Sinica  pages 831–840  2005.

[50] L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM review  38(1):49–95  1996.

[51] M. Vidyasagar. Statistical learning theory and randomized algorithms for control. IEEE Control Systems 

18(6):69–85  1998.

[52] M. Vidyasagar. Randomized algorithms for robust controller synthesis using statistical learning theory.

Automatica  37(10):1515–1528  2001.

[53] M. Vidyasagar. A theory of learning and generalization. Springer-Verlag New York  Inc.  2002.

[54] J. Vinogradska  B. Bischoff  D. Nguyen-Tuong  A. Romer  H. Schmidt  and J. Peters. Stability of controllers
for gaussian process forward models. In International Conference on Machine Learning  pages 545–554 
2016.

[55] Y.-S. Wang  N. Matni  and J. C. Doyle. A system level approach to controller synthesis. arXiv preprint

arXiv:1610.04815  2016.

[56] A. Wills  T. B. Schön  F. Lindsten  and B. Ninness. Estimation of linear systems using a Gibbs sampler.

IFAC Proceedings Volumes  45(16):203–208  2012.

[57] K. Zhou and J. C. Doyle. Essentials of robust control  volume 104. Prentice Hall Upper Saddle River  NJ 

1998.

12

,Jack Umenberger
Thomas Schön