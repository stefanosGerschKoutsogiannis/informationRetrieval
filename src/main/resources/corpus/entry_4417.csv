2010,Probabilistic Belief Revision with Structural Constraints,Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated  the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence.,Probabilistic Belief Revision with Structural

Constraints

Peter B. Jones

MIT Lincoln Laboratory
Lexington  MA 02420
jonep@ll.mit.edu

Venkatesh Saligrama

Dept. of ECE

Boston University
Boston  MA 02215

srv@bu.edu

Abstract

Sanjoy K. Mitter
Dept. of EECS

MIT

Cambridge  MA 02139
mitter@mit.edu

Experts (human or computer) are often required to assess the probability of un-
certain events. When a collection of experts independently assess events that are
structurally interrelated  the resulting assessment may violate fundamental laws of
probability. Such an assessment is termed incoherent. In this work we investigate
how the problem of incoherence may be affected by allowing experts to specify
likelihood models and then update their assessments based on the realization of a
globally-observable random sequence.
Keywords: Bayesian Methods  Information Theory  consistency

1 Introduction

Coherence is perhaps the most fundamental property of probability estimation. Coherence will be
formally deﬁned later  but in essence a coherent probability assessment is one that exhibits logical
consistency.
Incoherent assessments are those that cannot be correct  that are at odds with the
underlying structure of the space  and so can’t be extended to a complete probability distribution [1 
2]. From a decision theoretic standpoint  treating assessments as odds  incoherent assessments result
in guaranteed losses to assessors. They are dominated strategies  meaning that for every incoherent
assessment there is a coherent assessment that uniformly improves the outcome for the assessors.
Despite this fact  expert assessments (human and machine) are vulnerable to incoherence [3].
Previous authors have used coherence as a tool for fusing distributed expert assessments [4  5  6].
The focus has been on static coherence in which experts are polled once about some set of events
and the responses are then fused through a geometric projection. Besides relying on arbitrary scor-
ing functions to deﬁne the “right” projection  such analyses don’t address dynamically evolving
assessments or forecasts. This paper is  to our knowledge  the ﬁrst attempt to analyze the problem of
coherence under Bayesian belief dynamics. The importance of dynamic coherence is demonstrated
in the following example.
Consider two uncertain events A1 and A2 where A1 ⊆ A2 (e.g. A2 = {NASDAQ ↑ tomorrow}
and A1 = {NASDAQ ↑ tomorrow ≥ 10 points}). To be coherent  a probability assessment must
obey the relation P (A1) ≤ P (A2). For the purposes of the example  suppose the initial belief is
P (A1) = P (A2) = 0.5 which is coherent. Next  suppose there is some binary random variable
This work was sponsored by the U.S. Government under Air Force Contract FA8721-05-C-0002. Opinions 
interpretations  conclusions  and recommendations are those of the authors and are not necessarily endorsed by
the United States Government

1

Z that is believed to correlate with the underlying event (e.g. Z = 11{Google↑today} where 11 is an
indicator function). The believed dependence between Z and Ai is captured by a likelihood model
P (Z|Ai) that gives the probability of observing Z when event Ai does or doesn’t occur. For the
example  suppose Z = 0 and the believed likelihoods are P (Z = 0|A1) = 1 and P (Z = 0| ¯A1) and
P (Z = 0|A2) = P (Z = 0| ¯A2) = 0.5 where ¯A is the complement of A. There’s nothing inherently
irrational in this belief model  but when Bayes’ Rule is applied  it gives P (A1|Z = 0) = 0.67 >
P (A2|Z = 0) = 0.5. The belief update has introduced incoherence!

1.1 Motivating Example

Concerned with their network security  BigCorps wants to purchase an Intrusion Detection and Pre-
vention System (IDPS). They have two options  IDPS1 and IDPS2. IDPS1 detects both distributed
denial of service (DDoS) attacks and port scan (PS) attacks  while IDPS2 detects only DDoS attacks.
While studying the NIST guide to IDPSs [7]  BigCorps’ CTO notes the recommendation that ”orga-
nizations should consider using multiple types of IDPS technologies to achieve more comprehensive
and accurate detection and prevention of malicious activity.” Following the NIST recommendation 
BigCorps purchases both IDPSs and sets them to work monitoring network trafﬁc.
One morning while reading the output reports of the two detectors  an intrepid security analyst
witnesses an interesting behavior. IDPS2 is registering an attack probability of 0.1 while detector
IDPS1 is reading an attack probability of 0.05. Since the threats detected by IDPS1 are a superset
of those detected by IDPS2  the probability assigned by IDPS1 should always be larger than that
assigned by IDPS2. The dilemma faced by our analyst is how to reconcile the logically incoherent
outputs of the two detectors. Particularly  how to ascribe probabilities in a way that is logically
consistent  but still retains as much as possible the expert assessments of the detectors.

1.2 Contributions of this Work

This work introduces the concept of dynamic coherence  one that has not been previously treated
in the literature. We suggest two possible forms of dynamic coherence and analyze the relationship
between them. They are implemented and compared in a simple network modeling simulation.

1.3 Previous Work

Previous authors have analyzed coherence with respect to contingent (or conditional) probability
assessments [8  9  10]. These developments attempt to determine conditions characterizing coherent
subjective posteriors. While likelihood models are a form of contingent probability assessment  this
paper goes further in analyzing the impact of these assessments on coherent belief dynamics.
In [11  12] a different form of conditional coherence is suggested which derives from coherence of a
joint probability distribution over observations and states of nature. It is shown that for this stronger
form of conditional coherence  certain specially structured event sets and likelihood functions will
produce coherent posterior assessments.
Logical consistency under non-Bayesian belief dynamics has been previously analyzed.
In [13]
conditions for invariance under permutations of the observational sequence under Jeffrey’s rule are
developed. A comparison of Jeffrey’s rule and Pearl’s virtual evidence method is made in [14] which
shows that the virtual evidence method implicitly assumes the conditions of Jeffrey’s update rule.

2 Model
Let Ω = {ω1  ω2  . . .} be an event space and (Ω F) a measurable space. Let θ : Ω → Θ be a
measurable random variable; consider Θ = {θ1  θj  . . .   θJ} to be the set of all possible “states of
the world.” Also  let Zi : Ω → Z be a sequence of measureable random variables; consider Zi to
be the sequence of observations  with Z = {z1  z2  . . .   zK} and K < ∞. Let Ωθ (resp. ΩZi) be

2

the pre-image of θ (resp. Zi). Since the random variables are assumed measureable  Ωθ and ΩZi are
measureable sets (i.e. elements of F)  as are their countable intersections and unions.
i }Ωθ and let A = {Ai}. We
For i = 1  2  . . .   N  let Aθ
call elements of A events under assessment. The characteristic matrix χ for the events under
assessment is deﬁned as

i be a subset of Θ  let Ai = ∪{θ∈Aθ

(cid:189)

χij =

θj ∈ Aθ
o.w.

i

.

1
0

(cid:163)

An individual probability assessment P : A → [0  1] maps each event under assessment to the
unit interval. In an abuse of notation  we will let P (cid:44)
(joint) probability assessment. A coherent assessment (i.e. one that is logically consistent) can be
described geometrically as lying in the convex hull of the columns of χ  meaning ∃λ ∈ [0  1]J s.t.

··· P (AN )

P (A1) P (A2)

(cid:80)

i λi = 1 and P = χλ.

(cid:164)T be a

We now consider a sequence of probability assessments Pn deﬁned as follows: Pn is the result of
a belief revision process based on an initial probability assessment P0  a likelihood model pn(z|A) 
and a sequence of observations Z1  Z2  . . .   Zn. A likelihood model pn(z|A) is a pair of probability
mass functions over the observations: one conditioned on A and the other conditioned on ¯A (where
¯A denotes the complement of A). We will make the simplifying assumption that the likelihood
model is static  i.e. pn(z|A) = p(z|A) and pn(z| ¯A) = p(z| ¯A) for all n.
In this paper we assume belief revision dynamics governed by Bayes’ rule  i.e.
1

p(zn+1|A) ∗ Pn

Pn+1 =

p(zn+1|A) ∗ Pn + p(zn+1| ¯A) ∗ (1 − Pn)

=

1 + p(zn+1| ¯A)
p(zn+1|A)

1−Pn
Pn

To simplify development  denote p(z = zi|Aj) = αij and p(z = zi| ¯Aj) = βij and assume ∀j  ∃i
s.t. αij (cid:54)= βij (i.e. each event has at least one informative observation) and αij ∈ (0  1)  βij ∈ (0  1)
for all i  j (i.e. no observation determines absolutely whether any event obtains). Then by induction
the posterior probability of event A after n observations is:

Pn(Aj) =

1 + 1−P0

P0

(cid:179)

(cid:81)K

1

i=1

βij
αi

(cid:180)ni

(1)

(2)

when ni is the number of observations zi.

3 Probability convergence for single assessors

For a single assessor revising his estimate of the likelihood of event A  let the probability model
be given by p(z = zi|A) = αi and p(z = zi| ¯A) = βi. It is convenient to rewrite (1) in terms of
the ratio ρi = ni
n and for simplicity assuming P0 = 0.5 (although the analysis holds for general
P0 ∈ (0  1)). Substituting yields

(cid:104)(cid:81)K

(cid:179)

1

i=1

βi
αi

(cid:180)ρi(cid:105)n

Pn =

1 +

Note that 1) ρ is the empirical distribution over the observations  and so converges almost surely
(a.s.) to the true generating distribution  and 2) the convergence properties of Pn are determined by
the quantity between the square brackets in (2). Speciﬁcally  let

(cid:182)ρi

(cid:181)

K(cid:89)

i=1

βi
αi

L∞ = lim
n→∞

L∞ is commonly referred to as the likelihood ratio  familiar from classical binary hypothesis testing.
Since ρ converges a.s. and the function is continuous  L∞ exists a.s. If L∞ < 1 then Pn → 1; if
L∞ > 1 then Pn → 0; if L∞ = 1 then Pn → 1
2 .

3

3.1 Matched likelihood functions

Assume that A obtains; then L∞ =

Assume that the likelihood model is both inﬁnitely precise and inﬁnitely accurate  meaning that
when A (resp. ¯A) obtains observations are generated i.i.d. according to α (resp. β).

(cid:179)
(cid:81)K
(cid:182)αi
(cid:181)

i=1

βi
αi

βi
αi

=

K(cid:89)

i=1

(cid:180)αi a.s. Let L∞ = log L∞ which in this case yields
K(cid:88)

= −D(α||β) < 0

αi log βi
αi

i=1

L∞ = log

where all relations hold a.s.  D(·||·) is the relative entropy [15]  and the last inequality follows since
by assumption α (cid:54)= β. Since L∞ < 0 ⇔ L∞ < 1  this implies that when the true generating
distribution is α  Pn → 1 a.s.
Similarly  when ¯A obtains  we have

(cid:181)

K(cid:89)

(cid:182)βi

βi
αi

K(cid:88)

i=1

=

L∞ = log

βi log βi
αi

= D(β||α) > 0

and Pn → 0 a.s.

i=1

3.2 Mismatched likelihood functions

(cid:80)

Now consider the situation when the expert assessed likelihood model is incorrect. Assume the
observation generating distribution is γ = P(Zi = z) where γ (cid:54)= α and γ (cid:54)= β. In this case 
L∞ =

. We deﬁne

γi log βi
αi

T (γ) = −L∞ =

γi log αi
βi

(3)

(cid:88)

i

Then the probability simplex over the observation space Z can be partitioned into two sets:
P0 = {γ|T (γ) < 0} and P1 = {γ|T (γ) > 0}. By the a.s. convergence of the empirical dis-
tribtuion  γ ∈ Pi ⇒ Pn → i. (The boundary set {γ|T (γ) = 0} represents an unstable equilibrium
in which Pn a.s. converges to 1
2 ).
The problem of mismatched likelihood functions is similar to composite hypothesis testing (c.f.
[16] and references therein). Composite hypothesis testing attempts to design tests to determine the
truth or falsity of a hypothesis with some ambiguity in the underlying parameter space. Because of
this ambiguity  each hypothesis Hi corresponds not to a single distribution  but to a set of possible
distributions. In the mismatched likelihood function problem  composite spaces are formed due to
the properties of Bayes’ rule for a speciﬁc likelihood model. A corollary of the above result is that if
Hi ⊆ Pi then Bayes’ rule (under the speciﬁc likelihood model) is an asymptotically perfect detector.

4 Multiple Assessors with Structural Constraints

In Section 3 we analyzed convergence properties of a single event under assessment. Considering
multiple events introduces the challenge of deﬁning a dynamic concept of coherence for the assess-
ment revision process. In this section we suggest two possible deﬁnitions of dynamic coherence and
consider some of the implications of these deﬁnitions.

4.1 Step-wise Coherence

We ﬁrst introduce a step-wise deﬁnition of coherence  and derive equivalency conditions for the
special class of 2-expert likelihood models.

4

Deﬁnition 1 Under the Bayes’ rule revision process  a likelihood model p(z|A) is step-wise coher-
ent (SWC) if Pn ∈ convhull(χ) ⇒ Pn+1 ∈ convhull(χ) for all z ∈ Z.
Essentially this deﬁnition says that if the posterior assessment process is coherent at any time  it
will remain coherent perpetually  independent of observation sequence. We derive necessary and
sufﬁcient conditions for SWC for the characteristic matrix given by

(cid:183)

(cid:184)

χ =

1
0

1
1

0
0

(4)

Generalizations of this development are possible for any χ ∈ {0  1}2×|Θ|.
Note that under the characteristic matrix given by (4) a model is SWC iff Pn(A1) ≥ Pn(A2) for
all n and all coherent P0. Proceeding inductively  assume Pn is marginally SWC  i.e. Pn(A1) =
Pn(A2) = π. Due to the continuity of the update rule  a model will be SWC iff it is coherent at the
margins. For coherence  for any i we must have Pn+1(A1) ≥ Pn+1(A2). By substitution into (1)
αi1π+βi1(1−π) ≥
By monotonicity  αi1π+βi1(1−π)
ately  for χ given by (4)  the model will be SWC iff αi1
αi2

αi2π+βi2(1−π) or  equivalently  αi1
αi2π+βi2(1−π) ∈

  βi1
. Since αi1
αi2
βi2
∀i  or (rearranging)

≥ αi1π+βi1(1−π)
αi2π+βi2(1−π) .

(cid:111)(cid:105)

≥ αi1

αi2

≥ βi1

βi2

αi1
αi2

  βi1
βi2

(cid:111)

αi2

  max

αi1
αi2

αi1π

αi2π

degener-

(cid:110)

(cid:110)

(cid:104)

min

∀i 

αi1
βi1

≥ αi2
βi2

(5)

4.2 Asymptotic coherence

While it is relatively simple to characterize coherent models in the two assessor case  in general
SWC is difﬁcult to check. As such  we introduce a simpler condition:
Deﬁnition 2 A likelihood model p(z|A) is weakly asymptotically coherent (WAC) if for all obser-
vation generating distributions γ s.t. limn→∞ Pn ∈ {0  1}N   ∃i s.t. limn→∞ Pn = χei a.s.  where
ei is the ith unit vector.

Lemma 1 Step-wise coherence implies weakly asymptotic coherence.

Assume that a model is SWC but not WAC. Since it’s not WAC  there exists a γ s.t. Zi drawn
IID from γ a.s. results in Pn → ˆP where ˆP ∈ {0  1}N is not a column of χ and is therefore
not coherent. Since this holds regardless of initial conditions  assume the process is initialized
coherently. Then  by a separating hyperplane argument  there must exist some n (and therefore
some zn) s.t. Pn ∈ convhull(χ) and Pn+1 /∈ convhull(χ). This contradicts the assumption that
the likelihood model is SWC. Therefore any SWC model is also WAC. We demonstrate that the
converse is not true by counterexample in Section 4.2.2.

4.2.1 WAC for static models

Analogous to (3)  we deﬁne

For a given γ  deﬁne the logical vector r(γ) as

(cid:88)
 0

i

Tj(γ) =

γi log αij
βij

.

rj(γ) =

Tj(γ) < 0
Tj(γ) > 0
undet Tj(γ) = 0

1

Lemma 2 A likelihood model is WAC if ∀γ s.t. limn→∞ Pn ∈ {0  1}N   ∃i s.t. r(γ) = χei.

5

(6)

(7)

Deﬁne the sets Pi = {γ|r(γ) = χei}. Lemma 2 states that for a WAC likelihood model  {Pi}
partitions the simplex (excluding unstable edge events) into sets of distributions s.t. γ ∈ Pi ⇒
Pn → χei.
It is simple to show that the sets Pi are convex  and by deﬁnition the boundaries
between sets are linear.

4.2.2 Motivating Example Revisited

Consider again the motivating example of the two IDPSs from Section 1.1. Recall that IDPS1 detects
a superset of the attacks detected by IDPS2  and so this scenario conforms to the characteristic matrix
analyzed in Section 4.1. Therefore (5) gives necessary and sufﬁcient conditions for SWC  while (7)
gives necessary and sufﬁcient conditions for WAC.
Suppose that both the IDPSs use the interval between packet arrivals as their observation and as-
sume the learned likelihood models for the two IDPSs happen to be geometrically distributed with
parameters x1  x2 (when an attack is occurring) and y1  y2 (when no attack is occurring)  with the
index denoting the IDPS. We will analyze SWC and WAC for this class of models.
Plugging the given likelihood model into (5) implies that the model is SWC iff  for z = 0  1  2  . . .

(cid:181)

(cid:182)z x1

(cid:181)

1 − x1
1 − y1
≥ x2

≥

y1
and 1−x1
1−y1

1 − x2
1 − y2
≥ 1−x2
1−y2

(cid:182)z x2

y2

(cid:88)

z

(8)

(10)

Equation (8) will be satisﬁed iff x1
y1
sufﬁcient condition for SWC.
Now  we turn to WAC. Forming T as deﬁned in (6)  we see that

y2

  which is therefore a necessary and

Tj(γ) =

γzz log

1 − xj
1 − yj

+ log xj
yj

= µ log

1 − xj
1 − yj

+ log xj
yj

(9)

where µ = Eγ[z]. By the structure of the characteristic matrix  the model will be WAC iff T2(γ) >
0 ⇒ T1(γ) > 0 for all µ ≥ 0. Assume for convenience that xi > yi. Then {γ|Ti(γ) < 0} =
{γ|µ <

log yi/xi

log(1−xi)/(1−yi)} and therefore the model is WAC iff
≥ log x1
y1
log 1−x1
1−y1

log x2
y2
log 1−x2
1−y2

Comparing the conditions for SWC (8) to those for WAC (10)  we see that any parameters satisfying
(8) also satisfy (10) but not vice versa. For example x1 = 0.3  x2 = 0.5  y1 = 0.2  y2 = 0.25 don’t
satisfy (8)  but do satisfy (10). Thus WAC is truly a weaker sense of convergence than SWC.

5 Coherence with only ﬁnitely many observations
As shown in Sections 3 and 4  a WAC likelihood model generates a partition {Pi} over the obser-
vation probability simplex such that γ ∈ Pi ⇒ Pn → χei. The question we now address is  given
a WAC likelihood model and ﬁnitely many observations (with empirical distribution ˆγn)  how to
revise an incoherent posterior probability assessment Pn so that it is both coherent and consistent
with the observed data.

Principle of Conserving Predictive Uncertainty: Given ˆγn  choose λ such that
λi = Pr[limn→∞ ˆγn ∈ Pi] for each i (where γ ∈ Pi iff Pn → χei).

The principle of conserving predictive uncertainty states that in revising an incoherent assessment
Pn to a coherent one ˜Pn  the weight vectors over the columns of χ should reﬂect the uncertainty in
whether the observations are being generated by a distribution in the corresponding element of the
partition {Pi} (and therefore whether Pn is converging to χei).

6

Given a uniform prior over generating distributions γ and assuming Lebesgue measure µ over the
parameters of the generating distribution  we can write

(cid:90)

γ∈Pi

P (ˆγn|γ)P (γ)

(cid:82)
P P (ˆγn|γ(cid:48))P (γ(cid:48))dµ(cid:48) dµ
(cid:82)
P P (ˆγn|γ(cid:48))dµ(cid:48)

γ∈Pi

(cid:90)

1

=

P (ˆγn|γ)dµ

(cid:90)
(cid:90)

P (γ ∈ Pi|ˆγn) =

=

=

P (γ|ˆγn)dµ
P (ˆγn|γ)

(cid:82)
P P (ˆγn|γ(cid:48))dµ(cid:48) dµ

γ∈Pi

γ∈Pi

In the limit of large n P (ˆγn|γ) .= e−nD(ˆγ||γ) (where .= denotes equality to the ﬁrst degree in the
exponent; c.f. [15]). This implies that as n gets large  Pr[limn→∞ ˆγn ∈ Pi] is dominated by the
i = argminγ∈Pi D(ˆγn||γ) (i.e. the reverse i-projection  or Maximum Likelihood estimate).
point γ∗
This suggests the following approximation method for determining a coherent projection of Pn:

(cid:80)
j≤|{Pi}| P (ˆγ|γ∗
j )
The relationship between the ML estimates (γ∗
i ) and the probability over the columns of the charac-
teristic matrix is represented graphically in Figure 1. As will be shown in Section 6  the principle of
conserving predictive uncertainty can even be effectively applied to non-WAC models.

P (ˆγ|γ∗
i )

λj =

(11)

The observation simplex

2 r

J

r
r

J
P1
J
J
ˆγn=γ∗
B
B
γ∗
γ∗
B
3
BB

r

1

4

B
γ∗

B

P2

P3

J

P4

J

J

J

JJ

χe1
J
B
B

The outcome simplex

pppppppp
ppppppppppp
p
ppppppppppppppppppppppppppppλr
p
p
p
p
p
p
p
Z

BB
χe2

J
B















J

J

B

J

p

p

p

p

p

p

p

p

p

p

p

p

J
Z
J
ZZ χe4
JJ

χe3

Figure 1: The relationship between observation and outcome simplices

5.1 Sparse coherent approximation
In general |Θ| (the length of the vector λ) can be of order 2N (where N is the number of assessors) 
so solving for λ directly using (11) may be computationally infeasible. The following result sug-
gests that to generate the optimal (in the sense of capturing to most possible weight) O(N) sparse
approximation of λ we need only calculate the O(N 2) reverse i-projections.
Let λ be determined according to (11) and let {Pi} be as deﬁned in Section 4. Assume wlog that
λi ≥ λj for all i > j. Deﬁne the neighborhood of Pi as N (Pi) = {Pj : |r(Pi)−r(Pj)| = 1} where
r(Pi) is deﬁned as in (7). The neighborhood of Pi is the set of partition elements such that the limit
of one (and only one) assessor’s probability assessment has changed. The size of the neighborhood
is thus less than or equal to N.
By the assumed ordering of λ and (11)  it is immediately evident that ˆγ = γ∗
1  i.e. the maximally
weighted partition element is the one that contains the empirical distribution. It can be shown that
j<i N (Pj). Therefore the total number of projections
2 ∈ N (P1)  and thus recursively that γ∗
γ∗
(cid:88)
in calculating the i = N largest weights is bounded by

i ∈(cid:83)

(cid:88)

(cid:88)

(cid:91)

N (Pj)

|N (Pj)| ≤

|N (Pj)| ≤

max

j

N = N 2.

j<i

(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)

j<i

(cid:175)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) ≤

j<i

j<i

7

6 Experimental Results

Consider a three-assessor situation with an identity characteristic matrix  i.e. each of three assessors
estimates the probability that his unique outcome has occured knowing exactly one has occurred.
Suppose each event is a priori equally likely  and a sequence of iid observations is generated with
conditional probability p(zi|Ai) = 0.4 and p(z¯i|Ai) = 0.3 (thus observation zi is evidence that
event Ai has occurred). Optimal joint estimation results in the posterior distribution convergence re-
gions shown in Figure 6(a). Marginal estimation introduces incoherent convergence regions (6(b));
but for well-calibrated models  the empirical distribution is exponentially unlikely to lie in an in-
coherent region. However  miscalibrated models (6(c)) may lead to the true distribution lying in
an incoherence region. WAC-approximation can ameliorate such miscalibration. The results of a



Q

q



AA



A

q

QQ



(a)

A
A


q









A


q



AA



A

q


A
A

A




AA
(b)

A


A

q

A

A

A

A








q
a

AA




A









(c)

A


A

a



a



A

A
AA

A

A
A

A



q





A

q

A

A

A

q





q
a

AA


A




A





rˆγ



a

6

JJ^

A

AA

A


A

a





(d)

A

q

A

A

A

Figure 2: (a) Decision boundaries for optimal joint estimator; (b) Decision boundaries for marginal
estimator; (c) Decision boundaries for miscalibrated observation model; (d) WAC approximation

Monte Carlo implementation of this miscalibrated estimation is shown in Figure 3. The top line
(blue) shows the average error for accepting the posterior assessments generated by the miscali-
brated observation models. The next line (green) corresponds to renormalization at each time step 
equivalent to projecting the posterior into the coherent set with a divergence-based objective func-
tion. Next (red) shows the error generated by standard (L2) projection of the miscalibrated posterior
into the coherent set. Finally  in cyan is shown the WAC approximation.

Figure 3: Comparison of mean-square errors as a function of the number of observations under four
different estimation techniques

7 Conclusions

This paper has introduced the problem of dynamic coherence and analyzed it when the dynamics
are induced by Bayes’ rule. First  we demonstrated how under subjective event likelihood models
(potentially unmatched to the true underlying distributions) Bayes’ rule results in a partition over
the observation probability simplex. Then we introduced two concepts of dynamic coherence: step-
wise coherence and weak  asymptotic coherence. Next we suggested a principle of conservation of
predictive uncertainty  by which observation-based incoherence can be mitigated (even in incoherent
models). Finally  we brieﬂy analyzed the computational impact of coherent approximation.

8

20040060080010001200140016001800200000.10.20.30.40.50.60.70.8Number of ObservationsEstimate Mean Squared Error Straight BayesRescalingL2 ProjectionWACReferences

[1] V.S. Borkar  V.R. Konda  and S.K. Mitter. On De Finetti coherence and Kolmogorov probabil-

ity. Statistics and Probability Letters  66(4):417–421  March 2004.

[2] Bruno de Finetti. Theory of Probability  volume 1-2. Wiley New York  1974.

[3] Daniel Kahneman  Paul Slovic  and Amos Tversky  editors. Judgment under uncertainty:

Heuristics and biases. Cambridge University Press  1982.

[4] J.B. Predd et al. Aggregating forecasts of chance from incoherent and abstaining experts.

Decision Analysis  5:177–189  2008.

[5] D.N. Osherson and M.Y. Vardi. Aggregating disparate estimates of chance. Games and Eco-

nomic Behavior  56(1):148–173  July 2006.

[6] P. Jones  S. Mitter  and V. Saligrama. Revision of marginal probability assessments. In the

13th Internationl Conference on Information Fusion  Edinburgh  UK  2010.

[7] K. Scarfone and P. Mell. Guide to intrusion detection and prevention systems (IDPS). Tech-
nical Report 800-94  National Institute of Standards and Technology  Technology Administra-
tion  US Dept. of Commerce.

[8] D.A. Freedman and R.A. Purves. Bayes’ method for bookies. The Annals of Mathematical

Statistics  40(4):1177–1186  August 1969.

[9] D. Heath and W. Sudderth. On ﬁnitely additive priors  coherence  and extended admissibility.

The Annals of Statistics  6(2):333–345  March 1978.

[10] D.A. Lane and W. Sudderth. Coherent and continuous inference. The Annals of Statistics 

11(1):114–120  March 1983.

[11] E. Regazzini. De Finetti’s coherence and statistical inference. The Annals of Statistics 

15(2):845–864  June 1987.

[12] E. Regazzini. Coherent statistical inference and bayes theorem. The Annals of Statistics 

19(1):366–381  March 1991.

[13] P. Diaconis and S.L. Zabell. Updating subjective probability. Journal of the American Statis-

tical Association  77(380):822–830  December 1982.

[14] H. Chan and A. Darwiche. On the revision of probabilistic beliefs using uncertain evidence.

Artiﬁcial Intelligence  40(4):67–90  August 2005.

[15] Joy Thomas and Thomas Cover. Elements of Information Theory. Wiley Interscience  2nd

edition  2006.

[16] M. Feder and N. Merhav. Universal composite hypothesis testing: A competitive minimax

approach. IEEE Transactions on Information Theory  48(6):1504–1517  June 2002.

9

,Ying Liu
Alan Willsky
Antoine Gautier
Quynh Nguyen
Matthias Hein