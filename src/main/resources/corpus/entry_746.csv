2016,Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach,We consider the problem of optimizing an expensive objective function when a finite budget of total evaluations is prescribed. In that context  the optimal solution strategy for Bayesian optimization can be formulated as a dynamic programming instance. This results in a complex problem with uncountable  dimension-increasing state space and an uncountable control space. We show how to approximate the solution of this dynamic programming problem  using rollout  and propose rollout heuristics specifically designed for the Bayesian optimization setting. We present numerical experiments showing that the resulting algorithm for optimization with a finite budget outperforms several popular Bayesian optimization algorithms.,Bayesian Optimization with a Finite Budget:

An Approximate Dynamic Programming Approach

Massachusetts Institute of Technology

Massachusetts Institute of Technology

Remi R. Lam

Cambridge  MA
rlam@mit.edu

Karen E. Willcox

Cambridge  MA

kwillcox@mit.edu

David H. Wolpert
Santa Fe Institute

Santa Fe  NM

dhw@santafe.edu

Abstract

We consider the problem of optimizing an expensive objective function when a
ﬁnite budget of total evaluations is prescribed. In that context  the optimal solution
strategy for Bayesian optimization can be formulated as a dynamic programming in-
stance. This results in a complex problem with uncountable  dimension-increasing
state space and an uncountable control space. We show how to approximate the
solution of this dynamic programming problem using rollout  and propose rollout
heuristics speciﬁcally designed for the Bayesian optimization setting. We present
numerical experiments showing that the resulting algorithm for optimization with
a ﬁnite budget outperforms several popular Bayesian optimization algorithms.

1

Introduction

Optimizing an objective function is a central component of many algorithms in machine learning
and engineering. It is also essential to many scientiﬁc models  concerning everything from human
behavior  to protein folding  to population biology. Often  the objective function to optimize is
non-convex and does not have a known closed-form expression. In addition  the evaluation of this
function can be expensive  involving a time-consuming computation (e.g.  training a neural network 
numerically solving a set of partial differential equations  etc.) or a costly experiment (e.g.  drilling a
borehole  administering a treatment  etc.). Accordingly  there is often a ﬁnite budget specifying the
maximum number of evaluations of the objective function allowed to perform the optimization.
Bayesian optimization (BO) has become a popular optimization technique for solving problems
governed by such expensive objective functions [17  9  2]. BO iteratively updates a statistical model
and uses it as a surrogate for the objective function. At each iteration  this statistical model is used to
select the next design to evaluate. Most BO algorithms are greedy  ignoring how the design selected
at a given iteration will affect the future steps of the optimization. Thus  the decisions made are
typically one-step optimal. Because of this shortsightedness  such algorithms balance  in a greedy
fashion  the BO exploration-exploitation trade-off: evaluating designs to improve the statistical model
or to ﬁnd the optimizer of the objective function.
In contrast to greedy algorithms  a lookahead approach is aware of the remaining evaluations and can
balance the exploration-exploitation trade-off in a principled way. A lookahead approach builds an
optimal strategy that maximizes a long-term reward over several steps. That optimal strategy is the
solution of a challenging dynamic programming (DP) problem whose complexity stems  in part  from

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

the increasing dimensionality of the involved spaces as the budget increases  and from the presence
of nested maximizations and expectations. This is especially challenging when the design space takes
an uncountable set of values.
The ﬁrst contribution of this paper is to use rollout [1]  an approximate dynamic programming
(ADP) algorithm to circumvent the nested maximizations of the DP formulation. This leads to a
problem signiﬁcantly simpler to solve. Rollout uses suboptimal heuristics to guide the simulation
of optimization scenarios over several steps. Those simulations allow us to quantify the long-term
beneﬁts of evaluating a given design. The heuristics used by rollout are typically problem-dependent.
The second contribution of this paper is to build heuristics adapted to BO with a ﬁnite budget that
leverage existing greedy BO strategies. As demonstrated with numerical experiments  this can lead to
improvements in performance.
The following section of this paper provides a brief description of Gaussian processes and their use
in Bayesian optimization (Sec. 2)  followed by a brief overview of dynamic programming (Sec. 3).
Sec. 4 develops the connection between BO and DP and discusses some of the related work. We then
propose to employ the rollout algorithm (with heuristics adapted to BO) to mitigate the complexity of
the DP algorithm (Sec. 5). In Sec. 6  we numerically investigate the proposed algorithm and present
our conclusions in Sec. 7.

2 Bayesian Optimization

We consider the following optimization problem:

(OP) x∗ = argminx∈X f (x) 

(1)
where x is a d-dimensional vector of design variables. The design space  X   is a bounded subset of
Rd  and f : X (cid:55)→ R is an objective function that is expensive to evaluate. We are interested in ﬁnding
a minimizer x∗ of the objective function using a ﬁnite budget of N function evaluations. We refer to
this problem as the original problem (OP).
In the Bayesian optimization (BO) setting  the (deterministic or noisy) objective function f is modeled
as a realization of a stochastic process  typically a Gaussian process (GP) G  on a probability space
(Ω  Σ  P)  which deﬁnes a prior distribution over functions. A GP is fully deﬁned by a mean function
m : X → R (often set to zero without loss of generality) and a covariance kernel κ : X 2 → R (see
[16] for an overview of GP):
(2)

f ∼ G(m  κ).

The BO algorithm starts with an initial design x1 and its associated value y1 = f (x1) provided by
the user. This deﬁnes the ﬁrst training set S1 = {(x1  y1)}. At each iteration k ∈ {1 ···   N}  the
GP prior is updated  using Bayes rule  to obtain posterior distributions conditioned on the current
i=1 containing the past evaluated designs and observations. For any
training set Sk = {(xi  yi)}k
(potentially non-evaluated) design x ∈ X   the posterior mean µk(x) and posterior variance σ2
k(x) of
the GP  conditioned on Sk  are known in closed-form and are considered cheap to evaluate:

σ2

µk(x) = K(Xk  x)(cid:62)[K(Xk  Xk) + λI]−1Yk 

k(x) = κ(x  x) − K(Xk  x)(cid:62)[K(Xk  Xk) + λI]−1K(Xk  x) 

(3)
(4)
where K(Xk  Xk) is the k × k matrix whose ijth entry is κ(xi  xj)  K(Xk  x) (respectively Yk) is
the k × 1 vector whose ith entry is κ(xi  x) (respectively yi)  and λ is the noise variance. A new
design xk+1 is then selected and evaluated with this objective function to provide an observation
yk+1 = f (xk+1). This new pair (xk+1  yk+1) is added to the current training set Sk to deﬁne the
training set for the next iteration Sk+1 = Sk ∪ {(xk+1  yk+1)}.
In BO  the next design to evaluate is selected by solving an auxiliary problem (AP)  typically of the
form:

(5)
where Uk is a utility function to maximize. The rationale is that  because the optimization run-time or
cost is dominated by the evaluation of the expensive function f  time and effort should be dedicated
to choosing a good and informative (in a sense deﬁned by the auxiliary problem) design to evaluate.

(AP) xk+1 = argmaxx∈X Uk(x;Sk) 

2

Solving this auxiliary problem (sometimes called maximization of an acquisition or utility function)
does not involve the evaluation of the expensive objective function f  but only the posterior quantities
of the GP and  thus  is considered cheap.
Examples of utility functions  Uk  used to select the next design to evaluate in Bayesian optimization
include maximizing the probability of improvement (PI) [12]  maximizing the expected improvement
(EI) in the efﬁcient global optimization (EGO) algorithm [10]  minimizing a linear combination µ−ασ
of the posterior mean µ and standard deviation σ in GP upper conﬁdence bound (GP-UCB) [18]  or
maximizing a metric quantifying the information gain [19  6  7]. However  the aforementioned utility
functions are oblivious to the number of objective function evaluations left and  thus  lead to greedy
optimization strategies. Devising methods that account for the remaining budget would allow to better
plan the sequence of designs to evaluate  balance in a principled way the exploration-exploitation
trade-off encountered in BO  and thus potentially lead to performance gains.

3 Dynamic Programming

In this section  we review some of the key features of dynamic programming (DP) which addresses
optimal decision making under uncertainty for dynamical systems. BO with a ﬁnite budget can be
seen as such a problem. It has the following characteristics: (1) a statistical model to represent the
objective function  (2) a system dynamic that describes how this statistical model is updated as new
information is collected  and (3) a goal that can be quantiﬁed with a long-term reward. DP provides
us with a mathematical formulation to address this class of problem. A full overview of DP can be
found in [1  15].
We consider a system governed by a discrete-stage dynamic. At each stage k  the system is fully
characterized by a state zk ∈ Zk. A control uk  from a control space Uk(zk)  that generally depends
on the state  is applied. Given a state zk and a control uk  a random disturbance wk ∈ Wk(zk  uk)
occurs  characterized by a random variable Wk with probability distribution P(·|zk  uk). Then  the
system evolves to a new state zk+1 ∈ Zk+1  according to the system dynamic. This can be written in
the following form:
(6)
where z1 is an initial state  N is the total number of stages  or horizon  and Fk : Zk × Uk × Wk (cid:55)→
Zk+1 is the dynamic of the system at stage k (where the spaces’ dependencies are dropped for ease
of notation).
We seek the construction of an optimal policy (optimal in a sense yet to deﬁne). A policy 
π = {π1 ···   πN}  is sequence of rules  πk : Zk (cid:55)→ Uk  for k = 1 ···   N  mapping a state
zk to a control uk = πk(zk).
At each stage k  a stage-reward function rk : Zk ×Uk ×Wk (cid:55)→ R  quantiﬁes the beneﬁts of applying
a control uk to a state zk  subject to a disturbance wk. A ﬁnal reward function rN +1 : ZN +1 (cid:55)→ R 
similarly quantiﬁes the beneﬁts of ending at a state zN +1. Thus  the expected reward starting from
state z1 and using policy π is:

∀k ∈ {1 ···   N} ∀(zk  uk  wk) ∈ Zk × Uk × Wk 

zk+1 = Fk(zk  uk  wk) 

(cid:35)

(cid:34)

N(cid:88)

k=1

Jπ(z1) = E

rN +1(zN +1) +

rk(zk  πk(zk)  wk)

 

(7)

where the expectation is taken with respect to the disturbances. An optimal policy  π∗  is a policy
that maximizes this (long-term) expected reward over the set of admissible policies Π:

J∗(z1) = Jπ∗ (z1) = max
π∈Π

(8)
where J∗ is the optimal reward function  also called optimal value function. Using Bellman’s
principle of optimality  the optimal reward is given by a nested formulation and can be computed
using the following DP recursive algorithm  working backward from k = N to k = 1:

Jπ(z1) 

JN +1(zN +1) = rN +1(zN +1) 
Jk(zk) = max
uk∈Uk

E[rk(zk  uk  wk) + Jk+1(Fk(zk  uk  wk))].

(9)
(10)

The optimal reward J∗(z1) is then given by J1(z1)  and if u∗
side of Eq. 10 for all k and all zk  then the policy π∗ = {π∗

k = π∗
1 ···   π∗

k(zk) maximizes the right hand
N} is optimal (e.g.  [1]  p.23).

3

4 Bayesian Optimization with a Finite Budget

In this section  we deﬁne the auxiliary problem of BO with a ﬁnite budget (Eq. 5) as a DP instance.
At each iteration k  we seek to evaluate the design that will lead  once the evaluation budget N
has been consumed  to the maximum reduction of the objective function. In general  the value of
the objective function f (x) at a design x is unknown before its evaluation and  thus  estimating
the long-term effect of an evaluation is not possible. However  using the GP representing f  it is
possible to characterize the unknown f (x) by a distribution. This can be used to simulate sequences
of designs and function values (i.e.  optimization scenarios)  compute their rewards and associated
probabilities  without evaluating f. Using this simulation machinery  it is possible to capture the
goal of achieving a long term reward in a utility function Uk. We now formulate the simulation of
optimization scenarios in the DP context and proceed with the deﬁnition of such utility function Uk.
We consider that the process of optimization is a dynamical system. At each iteration k  this system
is fully characterized by a state zk equal to the training set Sk. The system is actioned by a control
uk equal to the design xk+1 selected to be evaluated. For a given state and control  the value of the
objective function is unknown and modeled as a random variable Wk  characterized by:

Wk ∼ N

(11)
where µk(xk+1) and σ2
k(xk+1) are the posterior mean and variance of the GP at xk+1  conditioned
on Sk. We deﬁne a disturbance wk to be equal to a realization fk+1 of Wk. Thus  wk = fk+1
represents a possible (simulated) value of the objective function at xk+1. Note that this simulated
value of the objective function  fk+1  is not the value of the objective function yk+1 = f (xk+1).
Hence  we have the following identities: Zk = (X × R)k  Uk = X and Wk = R.
The new state zk+1 is then deﬁned to be the augmented training set Sk+1 = Sk ∪ {(xk+1  fk+1)} 
and the system dynamic can be written as:
(12)
The disturbances wk+1 at iteration k + 1 are then characterized  using Bayes’ rule  by the posterior
of the GP conditioned on the training set Sk+1.
To optimally control this system (i.e.  to use an optimal strategy to solve OP)  we deﬁne the stage-
reward function at iteration k to be the reduction in the objective function obtained at stage k:

Sk+1 = Fk(Sk  xk+1  fk+1) = Sk ∪ {(xk+1  fk+1)}.

(cid:0)µk(xk+1)  σ2

k(xk+1)(cid:1)  

(cid:110)

(cid:111)

rk(Sk  xk+1  fk+1) = max

0  f

Sk
min − fk+1

 

(13)

Sk
min is the minimum value of the objective function in the training set Sk. We deﬁne the ﬁnal
where f
reward to be zero: rN +1(SN +1) = 0. The utility function  at a given iteration k characterized by Sk 
is deﬁned to be the expected reward:
(14)
where the expectation is taken with respect to the disturbances  and Jk+1 is deﬁned by Eqs. 9-10.
Note that E[rk(Sk  xk+1  fk+1)] is simply the expected improvement given  for all x ∈ X   by:

∀xk+1 ∈ X   Uk(xk+1;Sk) = E[rk(Sk  xk+1  fk+1) + Jk+1(Fk(Sk  xk+1  fk+1))] 
(cid:33)

(cid:32)

(cid:33)

(cid:32)

Sk
f
min − µk (x)

σk (x)

+ σk(x)φ

f

Sk
min − µk (x)

σk (x)

 

(15)

EI(x;Sk) =

Sk
f
min − µk (x)

Φ

(cid:16)

(cid:17)

where Φ is the standard Gaussian CDF and φ is the standard Gaussian PDF.
In other words  the GP is used to simulate possible scenarios  and the next design to evaluate is
chosen to maximize the decrease of the objective function  over the remaining iterations  averaged
over all possible simulated scenarios.
Several related methods have been proposed to go beyond greedy BO strategies. Optimal formula-
tions for BO with a ﬁnite budget have been explored in [14  4]. Both formulations involve nested
maximizations and expectations. Those authors note that their N-steps lookahead methods scale
poorly with the number of steps considered (i.e.  the budget N); they are able to solve the problem
for two-steps lookahead. For some speciﬁc instances of BO (e.g.  ﬁnding the super-level set of a
one-dimensional function)  the optimal multi-step strategy can be computed efﬁciently [3]. Approx-
imation techniques accounting for more steps have been recently proposed. They leverage partial

4

tree exploration [13] or Lipschitz reward function [11] and have been applied to cases where the
control spaces Uk are ﬁnite (e.g.  at each iteration  uk is one of the 4 or 8 directions that a robot
can take to move before it evaluates f). Theoretical performance guarantees are provided for the
algorithm proposed in [11]. Another approximation technique for non-greedy BO has been proposed
in GLASSES [5] and is applicable to uncountable control space Uk. It builds an approximation of
the N-steps lookahead formulation by using a one-step lookahead algorithm with approximation
of the value function Jk+1. The approximate value function is induced by a heuristic oracle based
on a batch Bayesian optimization method. The oracle is used to select up to 15 steps at once to
approximate the value function.
In this paper  we propose to use rollout  an ADP algorithm  to address the intractability of the DP
formulation. The proposed approach is not restricted to countable control spaces  and accounts for
more than two steps. This is achieved by approximating the value function Jk+1 with simulations
over several steps  where the information acquired at each simulated step is explicitly used to simulate
the next step. Note that this is a closed-loop approach  in comparison to GLASSES [5] which is an
open-loop approach. In contrast to the DP formulation  the decision made at each simulated step of
the rollout is not optimal  but guided by problem-dependent heuristics. In this paper we propose the
use of heuristics adapted to BO  leveraging existing greedy BO strategies.

5 Rollout for Bayesian Optimization

Solving the auxiliary problem deﬁned by Eqs. 5 14 is challenging. It requires the solution of nested
maximizations and expectations for which there is no closed-form expression known. In ﬁnite spaces 
the DP algorithm already suffers from the curse of dimensionality. In this particular setting  the state
spaces Zk = (X × R)k are uncountable and their dimension increases by d + 1 at each stage. The
control spaces Uk = X are also uncountable  but of ﬁxed dimension. Thus  solving Eq. 5 with utility
function deﬁned by Eq. 14 is intractable.
To simplify the problem  we use ADP to approximate Uk with the rollout algorithm (see [1  15] for
an overview). It is a one-step lookahead technique where Jk+1 is approximated using simulations
over several future steps. The difference with the DP formulation is that  in those simulated future
steps  rollout relaxes the requirement to optimally select a design (which is the origin of the nested
maximizations). Instead  rollout uses a suboptimal heuristic to decide which control to apply for a
given state. This suboptimal heuristic is problem-dependent and  in the context of BO with a ﬁnite
budget  we propose to use existing greedy BO algorithms as such a heuristic. Our algorithm proceeds
as follows.
For any iteration k  the optimal reward to go  Jk+1 (Eq. 14)  is approximated by Hk+1  the reward to
go induced by a heuristic π = (π1 ···   πN )  also called base policy. Hk+1 is recursively given by:
(16)
(17)
for all n ∈ {k + 1 ···   N − 1}  where γ ∈ [0  1] is a discount factor incentivizing the early collection
of reward. A discount factor γ = 0  leads to a greedy strategy that maximizes the immediate collection
of reward. This corresponds to maximizing the EI. On the other hand  γ = 1  means that there is no
differentiation between collecting reward early or late in the optimization. Note that Hk+1 is deﬁned
by recursion  and involves nested expectations. However  the nested maximizations are replaced by
the use of the base policy π. An important point is that  even if its deﬁnition is recursive  Hk+1 can
be computed in a forward manner  unlike Jk+1 which has to be computed in a backward fashion (see
Eqs. 9 10). The DP and the rollout formulations are illustrated in Fig.1. The approximated reward

HN (SN ) = EI(πN (SN );SN ) 
Hn(Sn) = E [rn(Sn  πn(Sn)  fn+1) + γHn+1(F(Sn  πn(Sn)  fn+1))]  

Hk+1 is then numerically approximated by (cid:101)Hk+1 using several simpliﬁcations. First  we use a rolling

horizon  h  to alleviate the curse of dimensionality. At a given iteration k  a rolling horizon limits the
number of stages considered to compute the approximate reward to go by replacing the horizon N by
˜N = min{k + h  N}. Second  expectations are taken with respect to the (Gaussian) disturbances
and are approximated using Gauss-Hermite quadrature. We obtain the following formulation:
(18)

˜H ˜N (S ˜N ) = EI(π ˜N (S ˜N );S ˜N ) 

(cid:17)(cid:17)(cid:105)

(cid:101)Hn(Sn) =

α(q)(cid:104)

Nq(cid:88)

q=1

(cid:16)

rn

Sn  πn(Sn)  f (q)

n+1

F

Sn  πn(Sn)  f (q)

n+1

 

(19)

(cid:16)

(cid:16)

+ γ(cid:101)Hn+1

(cid:17)

5

(cid:16)

α(q)(cid:104)

Nq(cid:88)

q=1

(cid:17)

+ γ(cid:101)Hk+1

(cid:16)

(cid:16)

Uk(xk+1;Sk) =

Sk  xk+1  f (q)
We note that for the last iteration  k = N  the utility function is known in closed form:

Sk  xk+1  f (q)

F

k+1

k+1

rk

(cid:17)(cid:17)(cid:105)

.

(20)

(21)

Figure 1: Graphs representing the DP (left) and the rollout (right) formulations (in the binary
decisions  binary disturbances case). Each white circle represents a training set  each black circle
represents a training set and a design. Double arrows are decisions that depend on decisions lower in
the graph (leading to nested optimizations in the DP formulation)  single arrows represent decisions
made using a heuristic (independent of the lower part of the graph). Dashed lines are simulated values
of the objective function and lead to the computation of expectations. Note the simpler structure of
the rollout graph compared to the DP one.

for all n ∈ {k + 1 ···   ˜N − 1}  where Nq ∈ N is the number of quadrature weights α(q) ∈ R
k+1 ∈ R  and rk is the stage-reward deﬁned by Eq. 13. Finally  for all iterations
and points f (q)
k ∈ {1 ···   N − 1} and for all xk+1 ∈ X   we deﬁne the utility function to be:

UN (xN +1;SN ) = EI(xN +1;SN ).

The base policy π used as a heuristic in the rollout is problem-dependent. A good heuristic π should
be cheap to compute and induce an expected reward Jπ close to the optimal expected reward Jπ∗
(Eq. 7). In the context of BO with a ﬁnite budget  this heuristic should mimic an optimal strategy that
balances the exploration-exploitation trade-off. We propose to use existing BO strategies  in particular 
maximization of the expected improvement (which has an exploratory behavior) and minimization of
the posterior mean (which has an exploitation behavior) to build the base policy. For every iteration
k ∈ {1 ···   N − 1}  we deﬁne π = {πk+1 ···   π ˜N} such that  at stage n ∈ {k + 1  ˜N − 1}  the
policy component  πn : Zn (cid:55)→ X   maps a state zn = Sn to the design xn+1 that maximizes the
expected improvement (Eq. 15):

xn+1 = argmax

x∈X

EI(x;Sn).

(22)

The last policy component  π ˜N : Z ˜N (cid:55)→ X   is deﬁned to map a state z ˜N = S ˜N to the design x ˜N +1
that minimizes the posterior mean (Eq. 3):

x ˜N +1 = argmin
x∈X

(cid:0)N h

µ ˜N (x).

(cid:1) applications of a heuristic. In our approach 
|Sk|2(cid:1) of work (rank-1 update of the

(23)

(cid:0)

q

Each evaluation of the utility function requires O
the heuristic involves optimizing a quantity that requires O
Cholesky decomposition to update the GP  and back-substitution for the posterior variance).
To summarize  we propose to use rollout  a one-step lookahead algorithm that approximates Jk+1.
This approximation is computed using simulation over several steps (e.g.  more than 3 steps)  where
the information collected after a simulated step is explicitly used to simulate the next step (i.e.  it is a
closed-loop approach). This is achieved using a heuristic instead of the optimal strategy  and thus 
leads to a formulation without nested maximizations.

6

Skxk+1fk+1Sk+1xk+2fk+2Sk+2...SN···Skxk+1fk+1Sk+1πk+1(Sk+1)fk+2Sk+2...SN···6 Experiments and Discussion

In this section  we apply the proposed algorithm to several optimization problems with a ﬁnite budget
and demonstrate its performance on GP generated and classic test functions.
We use a zero-mean GP with square-exponential kernel (hyper-parameters: maximum variance
σ2 = 4  length scale L = 0.1  noise variance λ = 10−3) to generate 24 objective functions deﬁned
on X = [0  1]2. We generate 10 designs from a uniform distribution on X   and use them as 10
different initial guesses for optimization. Thus  for each optimization  the initial training set S1
contains one training point. All algorithms are given a budget of N = 15 evaluations. For each of
the initial guess and each objective function  we run the BO algorithm with the following utility
functions: PI  EI and GP-UCB (with the parameter balancing exploration and exploitation set to
α = 3). We also run the rollout algorithm proposed in Sec. 5 and deﬁned by Eqs. 5 20  for the same
objective functions and with the same initial guesses for different parameters of the rolling horizon
h ∈ {2  3  4  5} and discount factor γ ∈ {0.5  0.7  0.9  1.0}. All algorithms use the same kernel and
hyper-parameters as those used to generate the objective functions.
Given a limited evaluation budget  we evaluate the performance of an algorithm for the original
problem (Eq. 1) in terms of gap G [8]. The gap measures the best decrease in objective function from
the ﬁrst to the last iteration  normalized by the maximum reduction possible:

.

(24)

G =

SN +1
S1
f
min − f
min
S1
min − f (x∗)
f

The mean and the median performances of the rollout algorithm are computed for the 240 experiments
for the 16 conﬁgurations of discount factors and rolling horizons. The results are reported in Table 1.

Table 1: Mean (left) and median (right) performance G over 24 objective functions and 10 initial
guesses for different rolling horizons h and discount factors γ.

γ

0.5
0.7
0.9
1.0

h = 2

h = 3

h = 4

h = 5

0.790
0.787
0.816
0.818

0.811
0.786
0.767
0.793

0.799
0.787
0.827
0.842

0.817
0.836
0.828
0.812

γ

0.5
0.7
0.9
1.0

h = 2

h = 3

h = 4

h = 5

0.849
0.849
0.896
0.870

0.862
0.830
0.839
0.861

0.858
0.806
0.876
0.917

0.856
0.878
0.850
0.858

The mean gap achieved is G = 0.698 for PI  G = 0.762 for EI and G = 0.711 for GP-UCB.
All the conﬁgurations of the rollout algorithm outperform the three greedy BO algorithms. The
best performance is achieved by the conﬁguration γ = 1.0 and h = 4. For this conﬁguration  the
performance increase with respect to EI is about 8%. The worst mean conﬁguration (γ = 0.9 and
h = 3) still outperforms EI by 0.5%.
The median performance achieved is G = 0.738 for PI  G = 0.777 for EI and G = 0.770 for
GP-UCB. All the conﬁgurations of the rollout algorithm outperform the three greedy BO algorithms.
The best performance is achieved by the conﬁguration γ = 1.0 and h = 4 (same as best mean
performance). For this conﬁguration  the performance increase with respect to EI is about 14%.
The worst rollout conﬁguration (γ = 0.7 and h = 4) still outperforms EI by 2.9%. The complete
distribution of gaps achieved by the greedy BO algorithms and the best and worst conﬁgurations of
the rollout is shown in Fig. 2.
We notice that increasing the length of the rolling horizon does not necessarily increase the gap (see
Table 1). This is a classic result from DP (Sec. 6.5.1 of [1]). We also notice that discounting the
future rewards has no clear effect on the gap. For all discount factors tested  we notice that reward is
not only collected at the last stage (See Fig. 2). This is a desirable property. Indeed  in a case where
the optimization has to be stopped before the end of the budget is reached  one would wish to have
collected part of the reward.
We now evaluate the performance on test functions.1 We consider four rollout conﬁgurations R-4-9
(h = 4  γ = 0.9)  R-4-10 (h = 4  γ = 1.0)  R-5-9 (h = 5  γ = 0.9) and R-5-10 (h = 5  γ = 1.0)

1Test functions from http://www.sfu.ca/~ssurjano/optimization.html.

7

Figure 2: Left: Histogram of gap for the rollout (best and worst mean conﬁgurations tested) and
greedy BO algorithms. Right: Median gap of the rollout (for the best and worst mean conﬁgurations
tested) and other algorithms as a function of iteration (budget of N = 15).

and two additional BO algorithms: PES [7] and the non-greedy GLASSES [5]. We use a square-
exponential kernel for each algorithm (hyper-parameters: maximum variance σ2 = 4  noise variance
λ = 10−3  length scale L set to 10% of the design space length scale). We generate 40 designs from
a uniform distribution on X   and use them as 40 different initial guesses for optimization. Each
algorithm is given N = 15 evaluations. The mean and median gap (over the 40 initial guesses) for
each function deﬁne 8 metrics (shown in Table 2). We found that rollout had the best metric 3 times
out of 8  and was never the worst algorithm. PES was found to perform best on 3 metrics out of
8 but was the worst algorithm for 2 metrics out of 8. GLASSES was never the best algorithm and
performed the worst in one metric. Note that the rollout conﬁguration R-4-9 outperforms GLASSES
on 5 metrics out of 6 (excluding the case of the Griewank function). Thus  our rollout algorithm
performs well and shows robustness.

Table 2: Mean and median gap G over 40 initial guesses.
R-4-10

GLASSES

EI

UCB

PES

PI

Function name

Branin-Hoo

Goldstein-Price

Griewank

Six-hump Camel Mean

Mean
Median
Mean
Median
Mean
Median

Median

0.847
0.922
0.873
0.983
0.827
0.904
0.850
0.893

0.818
0.909
0.866
0.981
0.884
0.953
0.887
0.970

0.848
0.910
0.733
0.899
0.913
0.970
0.817
0.915

0.861
0.983
0.819
0.987
0.972
0.987
0.664
0.801

0.846
0.909
0.782
0.919
12
12
0.776
0.941

R-4-9
0.904
0.959
0.895
0.991
0.882
0.967
0.860
0.926

R-5-9

R-5-10

0.898
0.943
0.784
0.985
0.885
0.962
0.825
0.900

0.887
0.921
0.861
0.989
0.930
0.960
0.793
0.941

0.903
0.950
0.743
0.928
0.867
0.954
0.803
0.907

7 Conclusions

We presented a novel algorithm to perform Bayesian optimization with a ﬁnite budget of evaluations.
The next design to evaluate is chosen to maximize a utility function that quantiﬁes long-term rewards.
We propose to employ an approximate dynamic programming algorithm  rollout  to approximate
this utility function. Rollout leverages heuristics to circumvent the need for nested maximizations.
We propose to build such a heuristic using existing suboptimal Bayesian optimization strategies  in
particular maximization of the expected improvement and minimization of the posterior mean. The
proposed approximate dynamic programming algorithm is empirically shown to outperform popular
greedy and non-greedy Bayesian optimization algorithms on multiple test cases.
This work was supported in part by the AFOSR MURI on multi-information sources of multi-physics
systems under Award Number FA9550-15-1-0038  program manager Dr. Jean-Luc Cambier.

2This gap G = 1 results from an arbitrary choice made by one optimizer used by GLASSES to evaluate the
origin. The origin happens to be the minimizer of the Griewank function. We thus exclude those results from the
analysis.

8

0.00.10.20.30.40.50.60.70.80.91.0G020406080100120140RealizationsRollout(Best)Rollout(Worst)EIUCBPI02468101214Iterationk0.00.20.40.60.81.0GRollout(Best)Rollout(Worst)EIUCBPIReferences
[1] D. P. Bertsekas. Dynamic programming and optimal control  volume 1. Athena Scientiﬁc  1995.

[2] E. Brochu  V. M. Cora  and N. De Freitas. A tutorial on Bayesian optimization of expensive cost
functions  with application to active user modeling and hierarchical reinforcement learning. arXiv preprint
arXiv:1012.2599  2010.

[3] J. M. Cashore 

L. Kumarga 

and P.

I. Frazier.

Multi-step Bayesian

for

tion
https://people.orie.cornell.edu/pfrazier/pub/workingpaper-CashoreKumargaFrazier.pdf.

one-dimensional

Working

feasibility

determination.

paper. Retrieved

optimiza-
from

[4] D. Ginsbourger and R. Le Riche. Towards Gaussian process-based optimization with ﬁnite time horizon.

In mODa 9–Advances in Model-Oriented Design and Analysis  pages 89–96. Springer  2010.

[5] J. González  M. Osborne  and N. D. Lawrence. GLASSES: Relieving the myopia of Bayesian optimisation.
In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics  pages 790–799 
2016.

[6] P. Hennig and C. J. Schuler. Entropy search for information-efﬁcient global optimization. The Journal of

Machine Learning Research  13(1):1809–1837  2012.

[7] J. M. Hernández-Lobato  M. W. Hoffman  and Z. Ghahramani. Predictive entropy search for efﬁcient
global optimization of black-box functions. In Advances in Neural Information Processing Systems  pages
918–926  2014.

[8] D. Huang  T. T. Allen  W. I. Notz  and N. Zeng. Global optimization of stochastic black-box systems via

sequential kriging meta-models. Journal of Global Optimization  34(3):441–466  2006.

[9] D. R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global

Optimization  21(4):345–383  2001.

[10] D. R. Jones  M. Schonlau  and W. J. Welch. Efﬁcient global optimization of expensive black-box functions.

Journal of Global Optimization  13(4):455–492  1998.

[11] C. K. Ling  K. H. Low  and P. Jaillet. Gaussian process planning with lipschitz continuous reward functions:
Towards unifying bayesian optimization  active learning  and beyond. In 30th AAAI Conference on Artiﬁcial
Intelligence  2016.

[12] D. J. Lizotte. Practical Bayesian Optimization. PhD thesis  Edmonton  Alta.  Canada  2008. AAINR46365.

[13] R. Marchant  F. Ramos  and S. Sanner. Sequential Bayesian optimisation for spatial-temporal monitoring.

2015.

[14] M. A. Osborne  R. Garnett  and S. J. Roberts. Gaussian processes for global optimization.
International Conference on Learning and Intelligent Optimization (LION3)  pages 1–15  2009.

In 3rd

[15] W. B. Powell. Approximate Dynamic Programming: Solving the Curses of Dimensionality  volume 842.

John Wiley & Sons  2011.

[16] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press  Cambridge 

MA  2006.

[17] J. Snoek  H. Larochelle  and R. P. Adams. Practical Bayesian optimization of machine learning algorithms.

In Advances in Neural Information Processing Systems  pages 2951–2959  2012.

[18] N. Srinivas  A. Krause  S. M. Kakade  and M. Seeger. Gaussian process optimization in the bandit setting:
No regret and experimental design. In Proceedings of the 27th International Conference on Machine
Learning  pages 1015–1022  2010.

[19] J. Villemonteix  E. Vazquez  and E. Walter. An informational approach to the global optimization of

expensive-to-evaluate functions. Journal of Global Optimization  44(4):509–534  2009.

9

,Remi Lam
Karen Willcox
David Wolpert
Hadrien Hendrikx
Francis Bach
Laurent Massoulié