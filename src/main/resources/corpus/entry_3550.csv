2018,Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization,Despite the success of single-agent reinforcement learning  multi-agent reinforcement learning (MARL) remains challenging due to complex interactions between agents. Motivated by decentralized applications such as sensor networks  swarm robotics  and power grids  we study policy evaluation in MARL  where agents with jointly observed state-action pairs and private local rewards collaborate to learn the value of a given policy.  
In this paper  we propose a double averaging scheme  where each agent iteratively performs averaging over both space and time to incorporate neighboring gradient information and local reward information  respectively. We prove that the proposed algorithm converges to the optimal solution at a global geometric rate. In particular  such an algorithm is built upon a primal-dual reformulation of the mean squared Bellman error minimization problem  which gives rise to a decentralized convex-concave saddle-point problem. To the best of our knowledge  the proposed double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems.,Multi-Agent Reinforcement Learning via

Double Averaging Primal-Dual Optimization

Hoi-To Wai

The Chinese University of Hong Kong

Shatin  Hong Kong

htwai@se.cuhk.edu.hk

Zhuoran Yang

Princeton University
Princeton  NJ  USA
zy6@princeton.edu

Zhaoran Wang

Northwestern University

Evanston  IL  USA

zhaoranwang@gmail.com

Mingyi Hong

University of Minnesota
Minneapolis  MN  USA

mhong@umn.edu

Abstract

Despite the success of single-agent reinforcement learning  multi-agent reinforce-
ment learning (MARL) remains challenging due to complex interactions between
agents. Motivated by decentralized applications such as sensor networks  swarm
robotics  and power grids  we study policy evaluation in MARL  where agents with
jointly observed state-action pairs and private local rewards collaborate to learn the
value of a given policy.
In this paper  we propose a double averaging scheme  where each agent iteratively
performs averaging over both space and time to incorporate neighboring gradient
information and local reward information  respectively. We prove that the proposed
algorithm converges to the optimal solution at a global geometric rate. In particular 
such an algorithm is built upon a primal-dual reformulation of the mean squared
projected Bellman error minimization problem  which gives rise to a decentralized
convex-concave saddle-point problem. To the best of our knowledge  the proposed
double averaging primal-dual optimization algorithm is the ﬁrst to achieve fast
ﬁnite-time convergence on decentralized convex-concave saddle-point problems.

1

Introduction

Reinforcement learning combined with deep neural networks recently achieves superhuman perfor-
mance on various challenging tasks such as video games and board games [34  45]. In these tasks 
an agent uses deep neural networks to learn from the environment and adaptively makes optimal
decisions. Despite the success of single-agent reinforcement learning  multi-agent reinforcement
learning (MARL) remains challenging  since each agent interacts with not only the environment but
also other agents. In this paper  we study collaborative MARL with local rewards. In this setting  all
the agents share a joint state whose transition dynamics is determined together by the local actions
of individual agents. However  each agent only observes its own reward  which may differ from
that of other agents. The agents aim to collectively maximize the global sum of local rewards. To
collaboratively make globally optimal decisions  the agents need to exchange local information. Such
a setting of MARL is ubiquitous in large-scale applications such as sensor networks [42  9]  swarm
robotics [23  8]  and power grids [3  13].
A straightforward idea is to set up a central node that collects and broadcasts the reward information 
and assigns the action of each agent. This reduces the multi-agent problem into a single-agent one.
However  the central node is often unscalable  susceptible to malicious attacks  and even infeasible

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

in large-scale applications. Moreover  such a central node is a single point of failure  which is
susceptible to adversarial attacks. In addition  the agents are likely to be reluctant to reveal their local
reward information due to privacy concerns [5  27]  which makes the central node unattainable.
To make MARL more scalable and robust  we propose a decentralized scheme for exchanging local
information  where each agent only communicates with its neighbors over a network. In particular 
we study the policy evaluation problem  which aims to learn a global value function of a given policy.
We focus on minimizing a Fenchel duality-based reformulation of the mean squared Bellman error in
the model-free setting with inﬁnite horizon  batch trajectory  and linear function approximation.
At the core of the proposed algorithm is a “double averaging" update scheme  in which the algorithm
performs one average over space (across agents to ensure consensus) and one over time (across
observations along the trajectory). In detail  each agent locally tracks an estimate of the full gradient
and incrementally updates it using two sources of information: (i) the stochastic gradient evaluated on
a new pair of joint state and action along the trajectory and the corresponding local reward  and (ii) the
local estimates of the full gradient tracked by its neighbors. Based on the updated estimate of the full
gradient  each agent then updates its local copy of the primal parameter. By iteratively propagating
the local information through the network  the agents reach global consensus and collectively attain
the desired primal parameter  which gives an optimal approximation of the global value function.
Related Work The study of MARL in the context of Markov game dates back to [28]. See also
[29  24  21] and recent works on collaborative MARL [51  1]. However  most of these works consider
the tabular setting  which suffers from the curse of dimensionality. To address this issue  under the
collaborative MARL framework  [53] and [25] study actor-critic algorithms and policy evaluation
with on linear function approximation  respectively. However  their analysis is asymptotic in nature
and largely relies on two-time-scale stochastic approximation using ordinary differential equations
[2]  which is tailored towards the continuous-time setting. Meanwhile  most works on collaborative
MARL impose the simplifying assumption that the local rewards are identical across agents  making
it unnecessary to exchange the local information. More recently  [17–19  31  37] study deep MARL
that uses deep neural networks as function approximators. However  most of these works focus on
empirical performance and lack theoretical guarantees. Also  they do not emphasize on the efﬁcient
exchange of information across agents. In addition to MARL  another line of related works study
multi-task reinforcement learning (MTRL)  in which an agent aims to solve multiple reinforcement
learning problems with shared structures [52  39  32  33  48].
The primal-dual formulation of reinforcement learning is studied in [30  32  33  26  10  7  50  12  11 
15] among others. Except for [32  33] discussed above  most of these works study the single-agent
setting. Among them  [26  15] are most related to our work. In speciﬁc  they develop variance
reduction-based algorithms [22  14  43] to achieve the geometric rate of convergence in the setting
with batch trajectory. In comparison  our algorithm is based on the aforementioned double averaging
update scheme  which updates the local estimates of the full gradient using both the estimates of
neighbors and new states  actions  and rewards. In the single-agent setting  our algorithm is closely
related to stochastic average gradient (SAG) [43] and stochastic incremental gradient (SAGA) [14] 
with the difference that our objective function is a ﬁnite sum convex-concave saddle-point problem.
Our work is also related to prior work in the broader contexts of primal-dual and multi-agent
optimization. For example  [38] apply variance reduction techniques to convex-concave saddle-point
problems to achieve the geometric rate of convergence. However  their algorithm is centralized and
it is unclear whether their approach is readily applicable to the multi-agent setting. Another line
of related works study multi-agent optimization  for example  [49  36  6  44  41]. However  these
works mainly focus on the general setting where the objective function is a sum of convex local cost
functions. To the best of our knowledge  our work is the ﬁrst to address decentralized convex-concave
saddle-point problems with sampled observations that arise from MARL.
Contribution Our contribution is threefold: (i) We reformulate the multi-agent policy evaluation
problem using Fenchel duality and propose a decentralized primal-dual optimization algorithm with
a double averaging update scheme. (ii) We establish the global geometric rate of convergence for
the proposed algorithm  making it the ﬁrst algorithm to achieve fast linear convergence for MARL.
(iii) Our proposed algorithm and analysis is of independent interest for solving a broader class of
decentralized convex-concave saddle-point problems with sampled observations.
Organization In §2 we introduce the problem formulation of MARL. In §3 we present the proposed
algorithm and lay out the convergence analysis. In §4 we illustrate the empirical performance of the
proposed algorithm. We defer the detailed proofs to the supplementary material.

2

Notation Unless otherwise speciﬁed  for a vector x  (cid:107)x(cid:107) denotes its Euclidean norm; for a matrix
X  (cid:107)X(cid:107) denotes its spectral norm  i.e.  the largest singular value.
2 Problem Formulation
In this section  we introduce the background of MARL  which is modeled as a multi-agent Markov
decision process (MDP). Under this model  we formulate the policy evaluation problem as a primal-
dual convex-concave optimization problem.
Multi-agent MDP Consider a group of N agents. We are interested in the multi-agent MDP:

(cid:0)S {Ai}N

i=1  γ(cid:1)  

i=1 P a {Ri}N

where S is the state space and Ai is the action space for agent i. We write s ∈ S and a :=
(a1  ...  aN ) ∈ A1 ×···×AN as the joint state and action  respectively. The function Ri(s  a) is the
local reward received by agent i after taking joint action a at state s  and γ ∈ (0  1) is the discount
factor. Both s and a are available to all agents  whereas the reward Ri is private for agent i.
In contrast to a single-agent MDP  the agents are coupled together by the state transition matrix
P a ∈ R|S|×|S|  whose (s  s(cid:48))-th element is the probability of transiting from s to s(cid:48)  after taking a
joint action a. This scenario arises from large-scale applications such as sensor networks [42  9] 
swarm robotics [23  8]  and power grids [3  13]  which strongly motivates the development of a
multi-agent RL strategy. Moreover  under the collaborative setting  the goal is to maximize the
collective return of all agents. Suppose there exists a central controller that collects the rewards of 
and assigns the action to each individual agent  the problem reduces to the classical MDP with action
i=1 Ri(s  a). Thus  without such a central
controller  it is essential for the agents to collaborate with each other so as to solve the multi-agent
problem based solely on local information.
Furthermore  a joint policy  denoted by π  speciﬁes the rule of making sequential decisions for the
agents. Speciﬁcally  π(a|s) is the conditional probability of taking joint action a given the current
state s. We deﬁne the reward function of joint policy π as an average of the local rewards:

space A and global reward function Rc(s  a) = N−1(cid:80)N

Rπ

c (s) := 1
N

(1)
That is  Rπ
c (s) is the expected value of the average of the rewards when the agents follow policy π
at state s. Besides  any ﬁxed policy π induces a Markov chain over S  whose transition matrix is
denoted by P π. The (s  s(cid:48))-th element of P π is given by

where Rπ

i=1 Rπ

i (s) 

i (s) := Ea∼π(·|s)

(cid:2)Ri(s  a)(cid:3) .

(cid:80)N

[P π]s s(cid:48) =(cid:80)

a∈A π(a|s) · [P a]s s(cid:48).

When this Markov chain is aperiodic and irreducible  it induces a stationary distribution µπ over S.
Policy Evaluation A central problem in reinforcement learning is policy evaluation  which refers to
learning the value function of a given policy. This problem appears as a key component in both value-
based methods such as policy iteration  and policy-based methods such as actor-critic algorithms
[46]. Thus  efﬁcient estimation of the value functions in multi-agent MDPs enables us to extend the
successful approaches in single-agent RL to the setting of MARL.
Speciﬁcally  for any given joint policy π  the value function of π  denoted by V π : S → R  is deﬁned
as the expected value of the discounted cumulative reward when the multi-agent MDP is initialized
with a given state and the agents follows policy π afterwards. For any state s ∈ S  we deﬁne

p=1 γpRπ

c (sp)|s1 = s  π

.

(2)
To simplify the notation  we deﬁne the vector V π ∈ R|S| through stacking up V π(s) in (2) for all s.
By deﬁnition  V π satisﬁes the Bellman equation
V π = Rπ

c is obtained by stacking up (1) and [P π]s s(cid:48) := Eπ[P a
where Rπ
Moreover  it can be shown that V π is the unique solution of (3).
When the number of states is large  it is impossible to store V π. Instead  our goal is to learn an
approximate version of the value function via function approximation. In speciﬁc  we approximate
V π(s) using the family of linear functions

(3)
s s(cid:48)] is the expected transition matrix.

c + γP πV π  

V π(s) := E(cid:104)(cid:80)∞

(cid:105)

(cid:8)Vθ(s) := φ(cid:62)(s)θ : θ ∈ Rd} 

3

(cid:17)(cid:13)(cid:13)(cid:13)2

(cid:16)

(cid:13)(cid:13)(cid:13)ΠΦ

1
2

where θ ∈ Rd is the parameter  φ(s) : S → Rd is a known dictionary consisting of d features 
e.g.  a feature mapping induced by a neural network. To simplify the notation  we deﬁne Φ :=
(...; φ(cid:62)(s); ...) ∈ R|S|×d and let Vθ ∈ R|S| be the vector constructed by stacking up {Vθ(s)}s∈S.
With function approximation  our problem is reduced to ﬁnding a θ ∈ Rd such that Vθ ≈ V π.
Speciﬁcally  we seek for θ such that the mean squared projected Bellman error (MSPBE)

(cid:16)

(cid:13)(cid:13)(cid:13)Φ(cid:62)D

1
2

Vθ − γP πVθ − Rπ

+ ρ(cid:107)θ(cid:107)2

MSPBE(cid:63)(θ) :=

(4)
is minimized  where D = diag[{µπ(s)}s∈S ] ∈ R|S|×|S| is a diagonal matrix constructed using
the stationary distribution of π  ΠΦ : R|S| → R|S| is the projection onto subspace {Φθ : θ ∈ Rd} 
deﬁned as ΠΦ = Φ(Φ(cid:62)DΦ)−1Φ(cid:62)D  and ρ ≥ 0 is a free parameter controlling the regularization
on θ. For any positive semideﬁnite matrix A  we deﬁne (cid:107)v(cid:107)A =
v(cid:62)Av for any vector v. By
direct computation  when Φ(cid:62)DΦ is invertible  the MSPBE deﬁned in (4) can be written as

√

D

c

+ ρ(cid:107)θ(cid:107)2 =

Vθ − γP πVθ − Rπ

c

MSPBE(cid:63)(θ) =

(Φ(cid:62)DΦ)−1

where we deﬁne A := E(cid:2)φ(sp)(cid:0)φ(sp) − γφ(sp+1)(cid:1)(cid:62)(cid:3)  C := E(cid:2)φ(sp)φ(cid:62)(sp)(cid:3)  and b :=
E(cid:2)Rπ
c (sp)φ(sp)(cid:3). Here the expectations in A  b  and C are all taken with respect to (w.r.t. )

the stationary distribution µπ. Furthermore  when A is full rank and C is positive deﬁnite  it can be
shown that the MSPBE in (5) has a unique minimizer.
To obtain a practical optimization problem  we replace the expectations above by their sampled
averages from M samples. In speciﬁc  for a given policy π  a ﬁnite state-action sequence {sp  ap}M
p=1
is simulated from the multi-agent MDP using joint policy π. We also observe sM +1  the next state of
sM . Then we construct the sampled versions of A  b  C  denoted respectively by ˆA  ˆC  ˆb  as

C−1

+ ρ(cid:107)θ(cid:107)2 
(5)

(cid:13)(cid:13)(cid:13)Aθ− b

(cid:13)(cid:13)(cid:13)2

1
2

(cid:17)(cid:13)(cid:13)(cid:13)2

(cid:80)M

p=1 Cp  ˆb := 1

M

p=1 bp  with

  Cp := φ(sp)φ(cid:62)(sp)  bp := Rc(sp  ap)φ(sp)  

(6)

(cid:80)M

(cid:80)M
Ap := φ(sp)(cid:0)φ(sp) − γφ(sp+1)(cid:1)(cid:62)

p=1 Ap  ˆC := 1

where Rc(sp  ap) := N−1(cid:80)N

ˆA := 1
M

M

(cid:13)(cid:13)(cid:13) ˆAθ − ˆb

(cid:13)(cid:13)(cid:13)2

1
2

i=1 Ri(sp  ap) is the average of the local rewards received by each
agent when taking action ap at state sp. Here we assume that M is sufﬁciently large such that ˆC is
invertible and ˆA is full rank. Using the terms deﬁned in (6)  we obtain the empirical MSPBE

+ ρ(cid:107)θ(cid:107)2  

ˆC−1

MSPBE(θ) :=

(7)
which converges to MSPBE(cid:63)(θ) as M → ∞. Let ˆθ be a minimizer of the empirical MSPBE  our
estimation of V π is given by Φ ˆθ. Since the rewards {Ri(sp  ap)}N
i=1 are private to each agent 
it is impossible for any agent to compute Rc(sp  ap)  and minimize the empirical MSPBE (7)
independently.
Multi-agent  Primal-dual  Finite-sum Optimization Recall that under the multi-agent MDP  the
agents are able to observe the states and the joint actions  but can only observe their local rewards.
Thus  each agent is able to compute ˆA and ˆC deﬁned in (6)  but is unable to obtain ˆb. To resolve
this issue  for any i ∈ {1  . . .   N} and any p ∈ {1  . . .   M}  we deﬁne bp i := Ri(sp  ap)φ(sp) and
p=1 bp i  which are known to agent i only. By direct computation  it is easy to verify

ˆbi := M−1(cid:80)M

that minimizing MSPBE(θ) in (7) is equivalent to solving

min
θ∈Rd

1
N

MSPBEi(θ) where MSPBEi(θ) :=

1
2

+ ρ(cid:107)θ(cid:107)2 .

(8)

(cid:13)(cid:13)(cid:13) ˆAθ − ˆbi

(cid:13)(cid:13)(cid:13)2

ˆC−1

N(cid:88)

i=1

The equivalence can be seen by comparing the optimality conditions of two optimization problems.
Importantly  (8) is a multi-agent optimization problems [36] whose objective is to minimize a
summation of N local functions coupled together by the common parameter θ. Here MSPBEi(θ)
is private to agent i and the parameter θ is shared by all agents. As inspired by [35  30  15]  using
Fenchel duality  we obtain the conjugate form of MSPBEi(θ)  i.e. 

(cid:0) ˆAθ − ˆbi

(cid:1) − 1

(cid:17)

w(cid:62)

i

ˆCwi

2

+ ρ(cid:107)θ(cid:107)2 .

(9)

(cid:13)(cid:13)(cid:13) ˆAθ − ˆbi

(cid:13)(cid:13)(cid:13)2

1
2

+ ρ(cid:107)θ(cid:107)2 = max
wi∈Rd

ˆC−1

(cid:16)

w(cid:62)

i

4

Observe that each of ˆA  ˆC  ˆbi can be expressed as a ﬁnite sum of matrices/vectors. By (9)  problem
(8) is equivalent to a multi-agent  primal-dual and ﬁnite-sum optimization problem:

min
θ∈Rd

max

wi∈Rd i=1 ... N

1

N M

N(cid:88)

M(cid:88)

i=1

p=1

(cid:0)w(cid:62)
(cid:124)

i Apθ − b(cid:62)

(cid:123)(cid:122)
p iwi − 1
2

w(cid:62)
i Cpwi +

ρ
2

:=Ji p(θ wi)
denoted
by

J(θ {wi}N

(cid:107)θ(cid:107)2(cid:1)
(cid:125)

.

(10)

(1/N M )(cid:80)N

Hereafter 

(cid:80)M

the

is

global

function

objective

i=1

i=1)

i=1.

:=
p=1 Ji p(θ  wi)  which is convex w.r.t. the primal variable θ and is concave

w.r.t. the dual variable {wi}N
It is worth noting that the challenges in solving (10) are three-fold. First  to obtain a saddle-
point solution ({wi}N
i=1  θ)  any algorithm for (10) needs to update the primal and dual variables
simultaneously  which can be difﬁcult as objective function needs not be strongly convex with respect
to θ. In this case  it is nontrivial to compute a solution efﬁciently. Second  the objective function of
(10) consists of a sum of M functions  with M (cid:29) 1 potentially  such that conventional primal-dual
methods [4] can no longer be applied due to the increased complexity. Lastly  since θ is shared by
all the agents  when solving (10)  the N agents need to reach a consensus on θ without sharing the
local functions  e.g.  Ji p(·) has to remain unknown to all agents except for agent i due to privacy
concerns. Although ﬁnite-sum convex optimization problems with shared variables are well-studied 
new algorithms and theory are needed for convex-concave saddle-point problems. Next  we propose a
novel decentralized ﬁrst-order algorithm that tackles these difﬁculties and converges to a saddle-point
solution of (10) with linear rate.

3 Primal-dual Distributed Incremental Aggregated Gradient Method
We are ready to introduce our algorithm for solving the optimization problem in (10). Since θ is shared
by all the N agents  the agents need to exchange information so as to reach a consensual solution.
Let us ﬁrst specify the communication model. We assume that the N agents communicate over a
network speciﬁed by a connected and undirected graph G = (V  E)  with V = [N ] = {1  ...  N}
and E ⊆ V × V being its vertex set and edge set  respectively. Over G  it is possible to deﬁne
a doubly stochastic matrix W such that Wij = 0 if (i  j) /∈ E and W 1 = W (cid:62)1 = 1  note
λ := λmax(W − N−111(cid:62)) < 1 since G is connected. Notice that the edges in G may be formed
independently of the coupling between agents in the MDP induced by the stochastic policy π.
We handle problem (10) by judiciously combining the techniques of dynamic consensus [41  54]
and stochastic (or incremental) average gradient (SAG) [20  43]  which have been developed
independently in the control and machine learning communities  respectively. From a high level
viewpoint  our method utilizes a gradient estimator which tracks the gradient over space (across
N agents) and time (across M samples). To proceed with our development while explaining the
intuitions  we ﬁrst investigate a centralized and batch algorithm for solving (10).
Centralized Primal-dual Optimization Consider the primal-dual gradient updates. For any t ≥ 1 
at the t-th iteration  we update the primal and dual variables by

i}N
i=1) 

θt+1 = θt − γ1∇θJ(θt {wt

i + γ2∇wiJ(θt {wt

i}N
i=1)  i ∈ [N ]  

wt+1

i = wt

(11)
where γ1  γ2 > 0 are step sizes  which is a simple application of a gradient descent/ascent update to
the primal/dual variables. As shown by Du et al. [15]  when ˆA is full rank and ˆC is invertible  the
Jacobian matrix of the primal-dual optimal condition is full rank. Thus  within a certain range of step
size (γ1  γ2)  recursion (11) converges linearly to the optimal solution of (10).
Proposed Method The primal-dual gradient method in (11) serves as a reasonable template for
developing an efﬁcient decentralized algorithm for (10). Let us focus on the update of the primal
variable θ in (11)  which is a more challenging part since θ is shared by all N agents. To evaluate the
gradient w.r.t. θ  we observe that – (a) agent i does not have access to the functions  {Jj p(·)  j (cid:54)= i} 
of the other agents; (b) computing the gradient requires summing up the contributions from M
samples. As M (cid:29) 1  doing so is undesirable since the computation complexity would be O(M d).
We circumvent the above issues by utilizing a double gradient tracking scheme for the primal
θ-update and an incremental update scheme for the local dual wi-update in the following primal-
dual distributed incremental aggregated gradient (PD-DistIAG) method. Here each agent i ∈ [N ]

5

Algorithm 1 PD-DistIAG Method for Multi-agent  Primal-dual  Finite-sum Optimization

i }i∈[N ]  initial gradient estimators s0

i   w1

i = d0

i = 0  ∀ i ∈ [N ]  initial

Input: Initial estimators {θ1
counter τ 0
for t ≥ 1 do

p = 0  ∀ p ∈ [M ]  and stepsizes γ1  γ2 > 0.

The agents pick a common sample indexed by pt ∈ {1  ...  M}.
Update the counter variable as:
  ∀ p (cid:54)= pt .

p = τ t−1

= t  τ t

p

τ t
pt
for each agent i ∈ {1  . . .   N} do

Update the gradient surrogates by
j + 1
M

j=1 Wijst−1

i =(cid:80)N

st
i = dt−1
dt

i + 1
M

i   wt
i ) = 0 and ∇wiJi p(θ0

(cid:104)∇wiJi pt (θt
i =(cid:80)N

j=1 Wijθt

i  dt

θt+1

where ∇θJi p(θ0
Perform primal-dual updates using st

i   w0

(cid:104)∇θJi pt(θt

(cid:105)

)

 

τ t−1
pt
i

(12)

(13)

(14)

i) − ∇θJi pt(θ

i   wt

τ t−1
pt
i
τ t−1
pt
  w
i

  w

(cid:105)

)

 

τ t−1
pt
i

i) − ∇wiJi pt(θ
i   w0

i ) = 0 for all p ∈ [M ] for initialization.
i as surrogates for the gradients w.r.t. θ and wi:
j − γ1st

i + γ2dt
i .

i = wt

i  wt+1

(15)

end for

end for

i

is obtained by ﬁrst combining {θt

i}t≥1. We construct sequences {st

i. The details of our method are presented in Algorithm 1.

i}t≥1 and
i}t≥1 to track the gradients with respect to θ and wi  respectively. Similar to (11)  in the t-th
i. As for the primal variable  to
i}i∈[N ] using the weight matrix W  

maintains a local copy of the primal parameter {θt
{dt
iteration  we update the dual variable via gradient update using dt
achieve consensus  each θt+1
and then update in the direction of st
Let us explain the intuition behind the PD-DistIAG method through studying the update (13). Recall
i}N
that the global gradient desired at iteration t is given by ∇θJ(θt {wt
i=1)  which represents a
double average – one over space (across agents) and one over time (across samples). Now in the
case of (13)  the ﬁrst summand on the right hand side computes a local average among the neighbors
of agent i  and thereby tracking the global gradient over space. This is in fact akin to the gradient
tracking technique in the context of distributed optimization [41]. The remaining terms on the right
hand side of (13) utilize an incremental update rule akin to the SAG method [43]  involving a swap-in
swap-out operation for the gradients. This achieves tracking of the global gradient over time.
To gain insights on why the scheme works  we note that st
i represent some surrogate functions
for the primal and dual gradients. Moreover  for the counter variable  using (12) we can alternatively
represent it as τ t
p is the iteration index where
the p-th sample is last visited by the agents prior to iteration t  and if the p-th sample has never been
visited  we have τ t
i. The following lemma
shows that gθ(t) is a double average of the primal gradient – it averages over the local gradients
across the agents  and for each local gradient; it also averages over the past gradients for all the
i}N
samples evaluated up till iteration t + 1. This shows that the average over network for {st
i=1 can
always track the double average of the local and past gradients  i.e.  the gradient estimate gθ(t) is
‘unbiased’ with respect to the network-wide average.
(cid:80)M
Lemma 1 For all t ≥ 1 and consider Algorithm 1  it holds that
p=1 ∇θJi p(θ
M(cid:88)
N(cid:88)

Proof. We shall prove the statement using induction. For the base case with t = 1  using (13) and the
update rule speciﬁed in the algorithm  we have

p = max{(cid:96) ≥ 0 : (cid:96) ≤ t  p(cid:96) = p}. In other words  τ t

p = 0. For any t ≥ 1  deﬁne gθ(t) := (1/N )(cid:80)N

gθ(t) = 1
NM

(cid:80)N

N(cid:88)

i and dt

i=1 st

τ t
p
i ) .

(16)

τ t
p
i

  w

i=1

1

∇θJi pt(θ

τ 1
p
i

  w

τ 1
p
i )  

(17)

gθ(1) =

1
N

1
M

i=1

∇θJi p1(θ1

i   w1

i ) =

N M

i=1

p=1

6

(cid:104)∇θJi pt+1 (θt+1

i

1
M

j +

(cid:104)∇θJi pt+1(θt+1

i

Wijst

N(cid:88)

i=1

gθ(t + 1) =

1
N

N(cid:88)

(cid:26) N(cid:88)

i=1

j=1

= gθ(t) +

1

N M

N(cid:88)

(cid:20) (cid:88)

i=1

p(cid:54)=pt+1

(cid:105)(cid:27)

(cid:105)

  wt+1

i

) − ∇θJi pt+1 (θ

τ t
pt+1
i

τ t
pt+1
i

)

  w

  wt+1

i

) − ∇θJi pt+1(θ

τ t
pt+1
i

τ t
pt+1
  w
i

)

.

(18)
p for all p (cid:54)= pt+1. The induction assumption in (16)
(cid:21)

∇θJi pt+1(θ

τ t
pt+1
i

τ t
pt+1
  w
i

) .

(19)

N(cid:88)

where we use the fact ∇θJi p(θ
induction step  suppose (16) holds up to iteration t. Since W is doubly stochastic  (13) implies

i ) = 0 for all p (cid:54)= p1 in the above. For the

i   w0

τ 1
p

i ) = ∇θJi p(θ0

τ 1
p
i

  w

Notice that we have τ t+1
pt+1
can be written as

= t + 1 and τ t+1

p = τ t

gθ(t) =

1

N M

∇θJi p(θ

τ t+1
p
i

  w

τ t+1
p
i

)

+

1

N M

i=1

Finally  combining (18) and (19)  we obtain the desired result that (16) holds for the t + 1th iteration.
Q.E.D.
This  together with (17)  establishes Lemma 1.

As for the dual update (14)  we observe the variable wi is local to agent i. Therefore its gradient
i  involves only the tracking step over time [cf. (14)]  i.e.  it only averages the gradient
surrogate  dt
over samples. Combining with Lemma 1 shows that the PD-DistIAG method uses gradient surrogates
that are averages over samples despite the disparities across agents. Since the average over samples
are done in a similar spirit as the SAG method  the proposed method is expected to converge linearly.
Storage and Computation Complexities Let us comment on the computational and storage com-
plexity of PD-DistIAG method. First of all  since the method requires accessing the previously
evaluated gradients  each agent has to store 2M such vectors in the memory to avoid re-evaluating
these gradients. Each agent needs to store a total of 2M d real numbers. On the other hand  the
per-iteration computation complexity for each agent is only O(d) as each iteration only requires to
evaluate the gradient over one sample  as delineated in (14)–(15).
Communication Overhead The PD-DistIAG method described in Algorithm 1 requires an infor-
mation exchange round [of st
i] among the agents at every iteration. From an implementation
stand point  this may incur signiﬁcant communication overhead when d (cid:29) 1  and it is especially
ineffective when the progress made in successive updates of the algorithm is not signiﬁcant. A
natural remedy is to perform multiple local updates at the agent using different samples without
exchanging information with the neighbors. In this way  the communication overhead can be reduced.
Actually  this modiﬁcation to the PD-DistIAG method can be generally described using a time varying
weight matrix W (t)  such that we have W (t) = I for most of the iteration. The convergence of
PD-DistIAG method in this scenario is part of the future work.

i and θt

3.1 Convergence Analysis

The PD-DistIAG method is built using the techniques of (a) primal-dual batch gradient descent 
(b) gradient tracking for distributed optimization and (c) stochastic average gradient  where each
of them has been independently shown to attain linear convergence under certain conditions; see
[41  43  20  15]. Naturally  the PD-DistIAG method is also anticipated to converge at a linear rate.
To see this  let us consider the condition for the sample selection rule of PD-DistIAG:
A1 A sample is selected at least once for every M iterations  |t − τ t
p| ≤ M for all p ∈ [M ]  t ≥ 1.
The assumption requires that every samples are visited inﬁnitely often. For example  this can be
enforced by using a cyclical selection rule  i.e.  pt = (t mod M ) + 1; or a random sampling scheme
without replacement (i.e.  random shufﬂing) from the pool of M samples. Finally  it is possible
to relax the assumption such that a sample can be selected once for every K iterations only  with
K ≥ M. The present assumption is made solely for the purpose of ease of presentation. Moreover 
to ensure that the solution to (10) is unique  we consider:
A2 The sampled correlation matrix ˆA is full rank  and the sampled covariance ˆC is non-singular.
The following theorem conﬁrms the linear convergence of PD-DistIAG:

7

Theorem 1 Under A1 and A2  we denote by (θ(cid:63) {w(cid:63)
to the optimization problem in (10).
λmax( ˆA(cid:62) ˆC−1 ˆA))/λmin( ˆC). Deﬁne θ(t) := 1
primal step size γ1 is sufﬁciently small  then there exists a constant 0 < σ < 1 that

i=1) the primal-dual optimal solution
Set the step sizes as γ2 = βγ1 with β := 8(ρ +
If the

i as the average of parameters.

i }N

N

(cid:13)(cid:13)θ(t) − θ(cid:63)(cid:13)(cid:13)2

+ (1/βN )(cid:80)N

(cid:13)(cid:13)wt

i − w(cid:63)

i

i=1

(cid:13)(cid:13)2

(cid:80)N
= O(σt)  (1/N )(cid:80)N

i=1 θt

(cid:13)(cid:13)θt
i − θ(t)(cid:13)(cid:13) = O(σt) .

i=1

i }N

If N  M (cid:29) 1 and the graph is geometric  a sufﬁcient condition for convergence is to set γ =
O(1/ max{N 2  M 2}) and the resultant rate is σ = 1 − O(1/ max{M N 2  M 3}).
The result above shows the desirable convergence properties for PD-DistIAG method – the primal
i}N
dual solution (θ(t) {wt
i=1) converges to (θ(cid:63) {w(cid:63)
i=1) at a linear rate; also  the consensual error
of the local parameters ¯θt
i converges to zero linearly. A distinguishing feature of our analysis is that
it handles the worst case convergence of the proposed method  rather than the expected convergence
rate popular for stochastic / incremental gradient methods.
Proof Sketch Our proof is divided into three steps. The ﬁrst step studies the progress made by the
algorithm in one iteration  taking into account the non-idealities due to imperfect tracking of the
gradient over space and time. This leads to the characterization of a Lyapunov vector. The second step
analyzes the coupled system of one iteration progress made by the Lyapunov vector. An interesting
feature of it is that it consists of a series of independently delayed terms in the Lyapunov vector. The
latter is resulted from the incremental update schemes employed in the method. Here  we study a
sufﬁcient condition for the coupled and delayed system to converge linearly. The last step is to derive
condition on the step size γ1 where the sufﬁcient convergence condition is satisﬁed.
Speciﬁcally  we study the progress of the Lyapunov functions:

(cid:107)(cid:98)v(t)(cid:107)2 := Θ
i − θ(t)(cid:107)2 
That is (cid:98)v(t) is a vector whose squared norm is equivalent to a weighted distance to the optimal

primal-dual solution  Ec(t) and Eg(t) are respectively the consensus errors of the primal parameter
and of the primal aggregated gradient. These functions form a non-negative vector which evolves as:

  Ec(t) := 1

i=1 (cid:107)θt

τ t
p
j   w

NM

i=1

N

.

i

j=1

(cid:13)(cid:13)2(cid:17)
(cid:13)(cid:13)wt
(cid:80)N
(cid:80)M
i − w(cid:63)
p=1 ∇θJj p(θ
 max(t−2M )+≤q≤t (cid:107)(cid:98)v(q)(cid:107)

max(t−2M )+≤q≤t Ec(q)
max(t−2M )+≤q≤t Eg(q)

N

i=1

i − 1

Eg(t) := 1

(cid:16)(cid:13)(cid:13)θ(t) − θ(cid:63)(cid:13)(cid:13)2
+ (1/βN )(cid:80)N
(cid:113)(cid:80)N
(cid:13)(cid:13)st
 ≤ Q(γ1)
 (cid:107)(cid:98)v(t + 1)(cid:107)
 1 − γ1a0 + γ2

Ec(t + 1)
Eg(t + 1)

Q(γ1) =

0

τ t
p

(cid:113)(cid:80)N
j )(cid:13)(cid:13)2
  
 .

where the matrix Q(γ1) ∈ R3×3 is deﬁned by (exact form given in the supplementary material)

(20)

(21)

1 a1

γ1a2

λ

0
γ1

a4 + γ1a5 λ + γ1a6

γ1a3

In the above  λ := λmax(W − (1/N )11(cid:62)) < 1  and a0  ...  a6 are some non-negative constants
that depends on the problem parameters N  M  the spectral properties of A  C  etc.  with a0 being
positive. If we focus only on the ﬁrst row of the inequality system  we obtain

(cid:107)(cid:98)v(t + 1)(cid:107) ≤(cid:0)1 − γ1a0 + γ2

1 a1)

max

(t−2M )+≤q≤t

(cid:107)(cid:98)v(q)(cid:107) + γ1a2

max

(t−2M )+≤q≤t

Ec(q) .

In fact  when the contribution from Ec(q) can be ignored  then applying [16  Lemma 3] shows that
1 a1 < 0  which is possible as a0 > 0. Therefore  if Ec(t)
also converges linearly  then it is anticipated that Eg(t) would do so as well. In other words  the linear

(cid:107)(cid:98)v(t + 1)(cid:107) converges linearly if −γ1a0 + γ2
convergence of (cid:107)(cid:98)v(t)(cid:107)  Ec(t) and Eg(t) are all coupled in the inequality system (20).
radius of Q(γ1) in (21) is strictly less than one  then each of the Lyapunov functions  (cid:107)(cid:98)v(t)(cid:107)  Ec(t) 

Formalizing the above observations  Lemma 1 in the supplementary material shows a sufﬁcient
condition on γ1 for linear convergence. Speciﬁcally  if there exists γ1 > 0 such that the spectral
Eg(t)  would enjoy linear convergence. Furthermore  Lemma 2 in the supplementary material gives
an existence proof for such an γ1 to exist. This concludes the proof.

8

Remark While delayed inequality system has been studied in [16  20] for optimization algorithms 
the coupled system in (20) is a non-trivial generalization of the above. Importantly  the challenge
here is due to the asymmetry of the system matrix Q and the maximum over the past sequences on
the right hand side are taken independently. To the best of our knowledge  our result is the ﬁrst to
characterize the (linear) convergence of such coupled and delayed system of inequalities.
Extension Our analysis and algorithm may in fact be applied to solve general problems that involves
multi-agent and ﬁnite-sum optimization  e.g. 

(cid:80)N

(cid:80)M

minθ∈Rd J(θ) := 1
NM

i=1

p=1 Ji p(θ) .

(22)

For instance  these problems may arise in multi-agent empirical risk minimization  where data
samples are kept independently by agents. Our analysis  especially with convergence for inequality
systems of the form (20)  can be applied to study a similar double averaging algorithm with just the
primal variable. In particular  we only require the sum function J(θ) to be strongly convex  and the
objective functions Ji p(·) to be smooth in order to achieve linear convergence. We believe that such
extension is of independent interest to the community. At the time of submission  a recent work [40]
applied a related double averaging distributed algorithm to a stochastic version of (22). However 
their convergence rate is sub-linear as they considered a stochastic optimization setting.

4 Numerical Experiments

To verify the performance of our proposed method  we conduct an experiment on the mountaincar
dataset [46] under a setting similar to [15] – to collect the dataset  we ran Sarsa with d = 300 features
to obtain the policy  then we generate the trajectories of actions and states according to the policy with
M samples. For each sample p  we generate the local reward  Ri(sp i  ap i) by assigning a random
portion for the reward to each agent such that the average of the local rewards equals Rc(sp  ap).
We compare our method to several centralized methods – PDBG is the primal-dual gradient descent
method in (11)  GTD2 [47]  and SAGA [15]. Notably  SAGA has linear convergence while only
requiring an incremental update step of low complexity. For PD-DistIAG  we simulate a communica-
tion network with N = 10 agents  connected on an Erdos-Renyi graph generated with connectivity
of 0.2; for the step sizes  we set γ1 = 0.005/λmax( ˆA)  γ2 = 5 × 10−3.

Figure 1: Experiment with mountaincar dataset. For this problem  we have d = 300  M = 5000
samples  and there are N = 10 agents. (Left) Graph Topology. (Middle) ρ = 0.01. (Right) ρ = 0.

i=1 MSPBE(θt

objective  i.e.  it is (1/N )(cid:80)N

Figure 1 compares the optimality gap in terms of MSPBE of different algorithms against the epoch
number  deﬁned as (t/M ). For PD-DistIAG  we compare its optimality gap in MSPBE as the average
i ) − MSPBE(θ(cid:63)). As seen in the left panel  when the
regularization factor is high with ρ > 0  the convergence speed of PD-DistIAG is comparable to
that of SAGA; meanwhile with ρ = 0  the PD-DistIAG converges at a slower speed than SAGA.
Nevertheless  in both cases  the PD-DistIAG method converges faster than the other methods except
for SAGA. Additional experiments are presented in the supplementary materials to compare the
performance at different topology and regularization parameter.
Conclusion In this paper  we have studied the policy evaluation problem in multi-agent rein-
forcement learning. Utilizing Fenchel duality  a double averaging scheme is proposed to tackle the
primal-dual  multi-agent  and ﬁnite-sum optimization arises. The proposed PD-DistIAG method
demonstrates linear convergence under reasonable assumptions.

9

0100200300400500Epoch10-810-610-410-2Optimality Gap of MSPBEPDBGGTD2SAGAPD-DistIAG010002000300040005000Epoch10-610-410-2100Optimality Gap of MSPBEPDBGGTD2SAGAPD-DistIAGAcknowledgement The authors would like to thank for the useful comments from three anonymous
reviewers. HTW’s work was supported by the grant NSF CCF-BSF 1714672. MH’s work has been
supported in part by NSF-CMMI 1727757  and AFOSR 15RT0767.

References
[1] G. Arslan and S. Yüksel. Decentralized Q-learning for stochastic teams and games. IEEE

Transactions on Automatic Control  62(4):1545–1558  2017.

[2] V. S. Borkar. Stochastic approximation: A dynamical systems viewpoint. Cambridge University

Press  2008.

[3] D. S. Callaway and I. A. Hiskens. Achieving controllability of electric loads. Proceedings of

the IEEE  99(1):184–199  2011.

[4] A. Chambolle and T. Pock. On the ergodic convergence rates of a ﬁrst-order primal–dual

algorithm. Mathematical Programming  159(1-2):253–287  2016.

[5] K. Chaudhuri  C. Monteleoni  and A. D. Sarwate. Differentially private empirical risk mini-

mization. Journal of Machine Learning Research  12(Mar):1069–1109  2011.

[6] J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed optimization and

learning over networks. IEEE Transactions on Signal Processing  60(8):4289–4305  2012.

[7] Y. Chen and M. Wang. Stochastic primal-dual methods and sample complexity of reinforcement

learning. arXiv preprint arXiv:1612.02516  2016.

[8] P. Corke  R. Peterson  and D. Rus. Networked robots: Flying robot navigation using a sensor

net. Robotics Research  pages 234–243  2005.

[9] J. Cortes  S. Martinez  T. Karatas  and F. Bullo. Coverage control for mobile sensing networks.

IEEE Transactions on Robotics and Automation  20(2):243–255  2004.

[10] B. Dai  N. He  Y. Pan  B. Boots  and L. Song. Learning from conditional distributions via dual

embeddings. arXiv preprint arXiv:1607.04579  2016.

[11] B. Dai  A. Shaw  N. He  L. Li  and L. Song. Boosting the actor with dual critic. arXiv preprint

arXiv:1712.10282  2017.

[12] B. Dai  A. Shaw  L. Li  L. Xiao  N. He  J. Chen  and L. Song. Smoothed dual embedding

control. arXiv preprint arXiv:1712.10285  2017.

[13] E. Dall’Anese  H. Zhu  and G. B. Giannakis. Distributed optimal power ﬂow for smart

microgrids. IEEE Transactions on Smart Grid  4(3):1464–1475  2013.

[14] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in neural information
processing systems  pages 1646–1654  2014.

[15] S. S. Du  J. Chen  L. Li  L. Xiao  and D. Zhou. Stochastic variance reduction methods for policy

evaluation. arXiv preprint arXiv:1702.07944  2017.

[16] H. R. Feyzmahdavian  A. Aytekin  and M. Johansson. A delayed proximal gradient method
with linear convergence rate. In Machine Learning for Signal Processing (MLSP)  2014 IEEE
International Workshop on  pages 1–6. IEEE  2014.

[17] J. Foerster  Y. M. Assael  N. de Freitas  and S. Whiteson. Learning to communicate with deep
multi-agent reinforcement learning. In Advances in Neural Information Processing Systems 
pages 2137–2145  2016.

[18] J. Foerster  N. Nardelli  G. Farquhar  P. Torr  P. Kohli  S. Whiteson  et al. Stabilising experience

replay for deep multi-agent reinforcement learning. arXiv preprint arXiv:1702.08887  2017.

10

[19] J. K. Gupta  M. Egorov  and M. Kochenderfer. Cooperative multi-agent control using deep
reinforcement learning. In International Conference on Autonomous Agents and Multi-agent
Systems  pages 66–83  2017.

[20] M. Gurbuzbalaban  A. Ozdaglar  and P. A. Parrilo. On the convergence rate of incremental

aggregated gradient algorithms. SIAM Journal on Optimization  27(2):1035–1048  2017.

[21] J. Hu and M. P. Wellman. Nash Q-learning for general-sum stochastic games. Journal of

Machine Learning Research  4(Nov):1039–1069  2003.

[22] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[23] J. Kober and J. Peters. Reinforcement learning in robotics: A survey. In Reinforcement Learning 

pages 579–610. Springer  2012.

[24] M. Lauer and M. Riedmiller. An algorithm for distributed reinforcement learning in cooperative

multi-agent systems. In International Conference on Machine Learning  2000.

[25] D. Lee  H. Yoon  and N. Hovakimyan. Primal-dual algorithm for distributed reinforcement

learning: Distributed gtd2. arXiv preprint arXiv:1803.08031  2018.

[26] X. Lian  M. Wang  and J. Liu. Finite-sum composition optimization via variance reduced

gradient descent. arXiv preprint arXiv:1610.04674  2016.

[27] A. Lin and Q. Ling. Decentralized and privacy-preserving low-rank matrix completion. 2014.

Preprint.

[28] M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In

International Conference on Machine Learning  pages 157–163  1994.

[29] M. L. Littman. Value-function reinforcement learning in Markov games. Cognitive Systems

Research  2(1):55–66  2001.

[30] B. Liu  J. Liu  M. Ghavamzadeh  S. Mahadevan  and M. Petrik. Finite-sample analysis of

proximal gradient td algorithms. In UAI  pages 504–513  2015.

[31] R. Lowe  Y. Wu  A. Tamar  J. Harb  P. Abbeel  and I. Mordatch. Multi-agent actor-critic for

mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275  2017.

[32] S. V. Macua  J. Chen  S. Zazo  and A. H. Sayed. Distributed policy evaluation under multiple

behavior strategies. IEEE Transactions on Automatic Control  60(5):1260–1274  2015.

[33] S. V. Macua  A. Tukiainen  D. G.-O. Hernández  D. Baldazo  E. M. de Cote  and S. Zazo.
Diff-dac: Distributed actor-critic for multitask deep reinforcement learning. arXiv preprint
arXiv:1710.10363  2017.

[34] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves 
M. Riedmiller  A. K. Fidjeland  G. Ostrovski  et al. Human-level control through deep rein-
forcement learning. Nature  518(7540):529  2015.

[35] A. Nedi´c and D. P. Bertsekas. Least squares policy evaluation algorithms with linear function

approximation. Discrete Event Dynamic Systems  13(1-2):79–110  2003.

[36] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE

Transactions on Automatic Control  54(1):48–61  2009.

[37] S. Omidshaﬁei  J. Pazis  C. Amato  J. P. How  and J. Vian. Deep decentralized multi-task
multi-agent reinforcement learning under partial observability. In International Conference on
Machine Learning  pages 2681–2690  2017.

[38] B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems.

In Advances in Neural Information Processing Systems  pages 1416–1424  2016.

11

[39] E. Parisotto  J. L. Ba  and R. Salakhutdinov. Actor-mimic: Deep multi-task and transfer

reinforcement learning. arXiv preprint arXiv:1511.06342  2015.

[40] S. Pu and A. Nedi´c. Distributed stochastic gradient tracking methods. arXiv preprint

arXiv:1805.11454  2018.

[41] G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transac-

tions on Control of Network Systems  2017.

[42] M. Rabbat and R. Nowak. Distributed optimization in sensor networks.

Symposium on Information Processing in Sensor Networks  pages 20–27  2004.

In International

[43] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming  162(1-2):83–112  2017.

[44] W. Shi  Q. Ling  G. Wu  and W. Yin. Extra: An exact ﬁrst-order algorithm for decentralized

consensus optimization. SIAM Journal on Optimization  25(2):944–966  2015.

[45] D. Silver  J. Schrittwieser  K. Simonyan  I. Antonoglou  A. Huang  A. Guez  T. Hubert  L. Baker 
M. Lai  A. Bolton  et al. Mastering the game of Go without human knowledge. Nature  550
(7676):354–359  2017.

[46] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction  volume 1. MIT press

Cambridge  1998.

[47] R. S. Sutton  H. R. Maei  and C. Szepesvári. A convergent o(n) temporal-difference algorithm
for off-policy learning with linear function approximation. In Advances in neural information
processing systems  pages 1609–1616  2009.

[48] Y. W. Teh  V. Bapst  W. M. Czarnecki  J. Quan  J. Kirkpatrick  R. Hadsell  N. Heess  and
R. Pascanu. Distral: Robust multi-task reinforcement learning. arXiv preprint arXiv:1707.04175 
2017.

[49] J. Tsitsiklis  D. Bertsekas  and M. Athans. Distributed asynchronous deterministic and stochastic
gradient optimization algorithms. IEEE Transactions on Automatic Control  31(9):803–812 
1986.

[50] M. Wang. Primal-dual π learning: Sample complexity and sublinear run time for ergodic

markov decision problems. arXiv preprint arXiv:1710.06100  2017.

[51] X. Wang and T. Sandholm. Reinforcement learning to play an optimal Nash equilibrium in
team Markov games. In Advances in Neural Information Processing Systems  pages 1603–1610 
2003.

[52] A. Wilson  A. Fern  S. Ray  and P. Tadepalli. Multi-task reinforcement learning: A hierarchical
Bayesian approach. In International Conference on Machine Learning  pages 1015–1022  2007.

[53] K. Zhang  Z. Yang  H. Liu  T. Zhang  and T. Ba¸sar. Fully decentralized multi-agent reinforcement

learning with networked agents. arXiv preprint arXiv:1802.08757  2018.

[54] M. Zhu and S. Martínez. Discrete-time dynamic average consensus. Automatica  46(2):322–329 

2010.

12

,Hoi-To Wai
Zhuoran Yang
Zhaoran Wang
Mingyi Hong