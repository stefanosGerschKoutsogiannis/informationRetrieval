2019,Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals,We consider the problem of computing the best-fitting ReLU with
  respect to square-loss on a training set when the examples have been
  drawn according to a spherical Gaussian distribution (the labels can
  be arbitrary).  Let $\opt < 1$ be the population loss of the
  best-fitting ReLU.  We prove:
\begin{itemize}
\item Finding a ReLU with square-loss $\opt + \epsilon$ is as
  hard as the problem of learning sparse parities with noise  widely thought
  to be computationally intractable.  This is the first hardness
  result for learning a ReLU with respect to Gaussian marginals  and
  our results imply --{\em unconditionally}-- that gradient descent cannot
  converge to the global minimum in polynomial time.
\item There exists an efficient approximation algorithm for finding the
  best-fitting ReLU that achieves error $O(\opt^{2/3})$.  The
  algorithm uses a novel reduction to noisy halfspace learning with
  respect to $0/1$ loss. 
\end{itemize}
Prior work due to Soltanolkotabi \cite{soltanolkotabi2017learning} showed that gradient descent {\em can} find the best-fitting ReLU with respect to Gaussian marginals  if the training set is {\em exactly} labeled by a ReLU.,Time/Accuracy Tradeoffs for Learning a ReLU with

respect to Gaussian Marginals

Surbhi Goel

Department of Computer Science

University of Texas at Austin
surbhi@cs.utexas.edu

Sushrut Karmalkar

Department of Computer Science

University of Texas at Austin
sushrutk@cs.utexas.edu

Adam R. Klivans

Department of Computer Science

University of Texas at Austin
klivans@cs.utexas.edu

Abstract

We consider the problem of computing the best-ﬁtting ReLU with respect to
square-loss on a training set when the examples have been drawn according to
a spherical Gaussian distribution (the labels can be arbitrary). Let opt < 1 be the
population loss of the best-ﬁtting ReLU. We prove:
• Finding a ReLU with square-loss opt+ϵ is as hard as the problem of learning
sparse parities with noise  widely thought to be computationally intractable.
This is the ﬁrst hardness result for learning a ReLU with respect to Gaus-
sian marginals  and our results imply –unconditionally– that gradient descent
cannot converge to the global minimum in polynomial time.
• There exists an efﬁcient approximation algorithm for ﬁnding the best-ﬁtting
ReLU that achieves error O(opt2/3). The algorithm uses a novel reduction
to noisy halfspace learning with respect to 0/1 loss.

Prior work due to Soltanolkotabi [Sol17] showed that gradient descent can ﬁnd the
best-ﬁtting ReLU with respect to Gaussian marginals  if the training set is exactly
labeled by a ReLU.

Introduction

1
A Rectiﬁed Linear Unit (ReLU) is a function parameterized by a weight vector w ∈ Rd that maps
Rd → R as follows: ReLUw(x) = max(0  w · x). ReLUs are now the nonlinearity of choice in
modern deep networks. The computational complexity of learning simple neural networks that use
the ReLU activation is an intensely studied area  and many positive results rely on assuming that the
marginal distribution on the examples is a spherical Gaussian [ZYWG19  GLM18  ZSJ+17  MR18].
Recent work due to Soltanolkotabi [Sol17] shows that gradient descent will learn a single ReLU in
polynomial time  if the marginal distribution is Gaussian (see also [BG17]). His result  however 
requires that the training set is noiseless; i.e.  there is a ReLU that correctly classiﬁes all elements of
the training set.
Here we consider the more realistic scenario of empirical risk minimization or learning a ReLU
with noise (often referred to as agnostically learning a ReLU). We assume that a learner has ac-
cess to a training set from a joint distribution D on Rd × R where the marginal distribution
on Rd is Gaussian but the distribution on the labels can be arbitrary within [0  1]. We deﬁne

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

opt = minw ∥w∥≤1 Ex y∼D[(ReLUw(x) − y)2]  and the goal is to output a function of the form
max(0  w · x) with square-loss at most opt + ϵ.

1.1 Our Results

Our main results give a trade-off between the accuracy of the output hypothesis and the running time
of the algorithm. We give the ﬁrst evidence that there is no polynomial-time algorithm for ﬁnding a
ReLU with error opt + ϵ  even when the marginal distribution is Gaussian:
Theorem 1 (Informal version of Theorem 3). Assuming hardness of the problem of learning sparse
parities with noise  any algorithm for ﬁnding a ReLU on data drawn from a distribution with Gaus-
sian marginals that has error at most opt + ϵ runs in time dΩ(log(1/ϵ)).

Since gradient descent is known to be a statistical-query algorithm (see Section 4)  a consequence
of Theorem 1 is the following:
Corollary 1. Gradient descent fails to converge to the global minimum for learning the best-ﬁtting
ReLU with respect to square-loss in polynomial time  even when the marginals are Gaussian.

This above corollary is unconditional (i.e. does not rely on any hardness assumptions) and shows
the necessity of the realizable/noiseless setting in the work of Soltanolkotabi [Sol17] and Brutzkus
and Globerson [BG17]. We also give the ﬁrst approximation algorithm for ﬁnding the best-ﬁtting
ReLU with respect to Gaussian marginals:
Theorem 2 (Informal version of Theorem 5). There exists a polynomial-time algorithm for ﬁnding
a ReLU with error O(opt2/3) + ϵ.

The above result uses a novel reduction from learning a ReLU to the problem of learning a halfspace
with respect to 0/1 loss. We note that the problem of ﬁnding a ReLU with error O(opt) + ϵ remains
an outstanding open problem.

1.2 Our Techniques

Hardness Result. For our hardness result  we follow the same approach as Klivans and Kothari
[KK14] who gave a reduction from learning sparse parity with noise to the problem of agnostically
learning halfspaces with respect to Gaussian distributions. The idea is to embed examples drawn
from {−1  1}d into Rd by multiplying each coordinate with a random draw from a half-normal
distribution. The key technical component in their result is a correlation lemma showing that for a
parity function on variables indicated by index set S  the majority function on the same index set is
weakly correlated with a Gaussian lift of the parity function on S.
In our work we must overcome two technical difﬁculties. First  in the Klivans and Kothari result  it
is obvious that for distributions induced by learning sparse parity with noise  the best ﬁtting majority
function will be the one that is deﬁned on inputs speciﬁed by S. In our setting with respect to ReLUs 
however  the constant function 1/2 will have square-loss 1/4  and this may be much lower than the
square-loss of any function of the form max(0  w· x). Thus  we need to prove the existence of a gap
between the correlation of ReLUs with random noise (see Claim 3) versus the correlation of ReLUs
with parity (see Claim 4).
Second  Klivans and Kothari use known formulas on the discrete Fourier coefﬁcients of the majority
function and an application of the central limit theorem to analyze how much the best-ﬁtting majority
correlates with the Gaussian lift of parity. No such bounds are known  however  for the ReLU
function. As such we must perform a (somewhat involved) analysis of the ReLU function’s Hermite
expansion in order to obtain quantitative correlation bounds.

Approximation Algorithm. For our polynomial-time algorithm that outputs a ReLU with error
O(opt2/3) + ϵ  we apply a novel reduction to agnostically learning halfspaces. We give a simple
transformation on the training set to a Boolean learning problem and show that the weight vector w
corresponding to the best ﬁtting halfspace on this transformed data set is not too far from the weight
vector corresponding to the best ﬁtting ReLU. We can then apply recent work for agnostically learn-
ing halfspaces with respect to Gaussians that have constant-factor approximation error guarantees.
The exponent 2/3 appears due to the use of an averaging argument (see Section 5).

2

1.3 Related Work

Several recent works have proved hardness results for ﬁnding the best-ﬁtting ReLU with respect to
square loss (equivalently  agnostically learning a ReLU with respect to square loss). Results show-
ing NP-hardness (e.g.  [MR18  BDL18]) use marginal distributions that encode hard combinatorial
problems. The resulting marginals are far from Gaussian. Work due to Goel et al. [GKKT17] uses
a reduction from sparse parity with noise but only obtains hardness results for learning with respect
to discrete distributions (uniform on {0  1}d).
Using parity functions as a source of hardness for learning deep networks has been explored recently
by Shalev-Shwartz et. al. [SSSS17] and Abbe and Sandon [AS18]. Their results  however  do
not address the complexity of learning a single ReLU or consider the case of Gaussian marginals.
Shamir [Sha18] proved that gradient descent fails to learn certain classes of neural networks with
respect to Gaussian marginals  but these results do not apply to learning a single ReLU [VW19].
In terms of positive results for learning a ReLU  work due to Kalai and Sastry [KS09] (and follow-
up work [KKKS11]) gave the ﬁrst efﬁcient algorithm for learning any generalized linear model
(GLM) that is monotone and Lipschitz  a class that includes ReLUs. Their algorithms work for
any distribution and can tolerate bounded  mean-zero and additive noise. Soltanolkotabi [Sol17]
and Brutzkus and Globerson [BG17] were the ﬁrst to prove that gradient descent converges to the
unknown ReLU in polynomial time with respect to Gaussian marginals as long as the labels have
no noise. Other works for learning one-layer ReLU networks with respect to Gaussian marginals or
marginals with milder distribution assumptions [ZYWG19  GLM18  ZSJ+17  GKLW19  GKM18 
MR18] also assume a noiseless training set or training set with mean-zero i.i.d.
(typically sub-
Gaussian) noise. This is in contrast to the setting here (agnostic learning)  where we assume nothing
about the noise model.
There are several works for the related (but different) problem of agnostically learning halfspaces
with respect to Gaussian marginals [KKMS08  ABL14  Zha18  DKS18]. While agnostically learn-
ing ReLUs may seem like an easier problem than agnostically learning halfspaces (at ﬁrst glance the
learner sees “more information” from the ReLU’s real-valued labels)  the quantitative relationship
between the two problems is still open. In the halfspace setting  we can assume without loss of
generality that an adversary has ﬂipped an opt fraction of the labels. In contrast  in the setting with
ReLUs and square loss  it is possible for the adversary to corrupt every label.

2 Preliminaries
Deﬁne ReLU(a) = max(0  a) and the set of functions CReLU := {ReLUw|w ∈ Rd ∥w∥2 ≤ 1}
where ReLUw(x) = max(0  w· x). Deﬁne sign(a) to be 1 if a ≥ 0 and -1 otherwise. Let errD(h) :=
E(x y)∼D[(h(x) − y)2]  Also deﬁne optD(C) = minc∈C errD(c) to be the error of the best-ﬁtting
c ∈ C for distribution D. We will use x−i to denote the vector x restricted to the indices except
i. The ‘half-normal’ distribution will refer to the standard normal distribution truncated to R≥0.
We will use n and its subscripted versions to denote natural numbers unless otherwise stated. In
this paper  we will suppress the conﬁdence parameter δ  since one can use standard techniques to
amplify the probability of success of our learning algorithms.

Agnostic learning. The model of learning we work with in the paper is the agnostic model of
learning. In this model the labels are allowed to be arbitrary and the task of the learner is to output
a hypothesis within an ϵ error of the optimal. More formally 
Deﬁnition 1. A class C is said to be agnostically learnable in time t over the Gaussian distribution to
error ϵ if there exists an algorithm A such that for any distribution D on X×Y with the marginal on
X being Gaussian  A uses at most t draws from D  runs in time at most t  and outputs a hypothesis
h ∈ C such that errD(h) ≤ optD(C) + ϵ.
We assume that A succeeds with constant probability. Note that the algorithm above outputs the
error of h over samples S.

“best-ﬁtting” c ∈ C with respect to D up to an additive ϵ. We will denotecerrS (h) to be the empirical

3

In this work we will show that agnostically learning CReLU
Learning Sparse Parities with Noise.
over the Gaussian distribution is as hard as the problem of learning sparse parities with noise over
the uniform distribution on the hypercube.
Deﬁnition 2 (k-SLPN). Given access to samples drawn from the uniform distribution over {±1}d
and target function y being the parity function over an unknown set S ⊆ [d] of size k  the problem
of learning sparse parities with noise is the problem of recovering the set S given access to noisy
labels where the label is ﬂipped with probability η.

Learning sparse parities with noise is generally considered to be a computationally hard problem
and has been used to give hardness results for both supervised [GKKT17] and unsupervised learning
problems [BGS14]. The current best known algorithm for solving sparse parities with constant noise
rate is due to Valiant [Val15] and runs in time ≈ d0.8k.
Assumption 1. Any algorithm for solving k-SLPN up to constant error must run in time dΩ(k).

P∞

Gaussian Lift of a Function Our reduction will require the following deﬁnition of a Gaussian lift
of a boolean function from [KK14].
Deﬁnition 3 (Gaussian lift [KK14]). The Gaussian lift of a function f : {±1}d → R is the function
f γ : Rd → R such that for any x ∈ Rd  f γ(x) = f (sign(x1)  . . .   sign(xd)).
Hermite Analysis and Gaussian Density We will assume that the marginal over our samples x
is the standard normal distribution N (0  Id). This implies that w · x for a vector w is distributed
as N (0 ∥w∥2). We recall the basics of Hermite analysis. We say a function f : R → R is
square integrable if EN (0 1)[f 2] < ∞. For any square integrable function f deﬁne its Hermite
expansion as f (x) =
are the normalized Hermite poly-
nomials  and Hi the unnormalized (probabilists) Hermite polynomials. The normalized Hermite
polynomials form an orthonormal basis with respect to the univariate standard normal distribution
(E[ ¯Hi(x) ¯Hj(x)] = δij). The associated inner product for square integrable functions f  g : R → R

bfi ¯Hi(x) where ¯Hi(x) = Hi(x)√

is deﬁned as ⟨f  g⟩ := Ex∼N (0 1)[f (x)g(x)]. Each coefﬁcient bfi in the expansion of f (x) satisﬁes
bfi = Ex∼N (0 1)[f (x) ¯Hi(x)]. We will need the following facts about Hermite polynomials.
Fact 2 ([KKMS08]). dsign0 = 0 and for i ≥ 1 dsigni =

Fact 1. For all m ≥ 0  H2m+1(0) = 0 and H2m = (−1)m (2m)!
m!2m .
πi! Hi−1(0).

q

i=0

i!

2

3 Hardness of Learning ReLU

In this section  we will show that if there is an algorithm that agnostically learns a ReLU in polyno-
mial time  then there is an algorithm for learning sparse parities with noise in time do(k)  violating
Assumption 1. We follow the approach of [KK14]. Let χS be an unknown parity for some S ⊆ [d].
We will show that there is an unbiased ReLU that is correlated with the Gaussian lift of the unknown
sparse parity function. Notice that dropping a coordinate j ∈ S from the input samples makes the
labels of the resulting training set totally independent from the input. In contrast  dropping j /∈ S
results in a training set that is still labeled by a noisy parity. Therefore  we can use an agnostic
learner for ReLUs to detect a correlated ReLU and distinguish between the two cases. This allows
us to identify the variables in S one by one.
We formalize the above approach by ﬁrst proving the following key property 
Lemma 1 (ReLU Correlation Lemma). Let χγ
S denote the Gaussian lift of the parity on variables in
S ⊂ [d]. For every S ⊂ [d] with |S| ≤ k and k = 4l + 2 for some l ≥ 0  there exists ReLUwS such
that ⟨ReLUwS   χγ
⟩ ≥ 2
Proof. Let wS = 1√
show that

P
−O(k) where ReLUwS only depends on variables in S.
!#

i∈S e(i) where e(i) is 1 at coordinate i and 0 everywhere else. We will

"

2πk

α

(cid:18)P

(cid:19) Y

⟨ReLUwS   χγ

S

⟩ =

1√
2π

Ez∼N (0 Id)

ReLU

4

i∈S zi√
k

sign(zi)

i∈S

≥ 2

−O(k).

Letdsignn and [ReLUn denote the degree n Hermite coefﬁcients of the sign function and ReLU func-

tion respectively. It is easy to see that the Hermite expansion of the Gaussian lift of a parity supported
on S is 

Y

 ∞X

Y

dsignn

!

X

Y

n1 ... nk

i∈S

S(z) =
χγ

sign(zi) =

i∈S

i∈S

n=0

¯Hn(zi)

=

¯Hni(zi)

(1)

In order to ﬁnish the proof of Lemma 1 we will need the expansion of ReLU
in terms of
products of univariate Hermite polynomials. Toward this end we establish the following two claims
(see proofs in the supplemental).
√
Claim 1 (Hermite expansion: univariate ReLU). [ReLU0 = 1/
[ReLUi = 1√
Claim 2 (Hermite expansion: multivariate ReLU). For any S ⊆ [d] with |S| = k 

2π  [ReLU1 = 1/2 and for i ≥ 2 

(Hi(0) + iHi−2(0)).

2πi!

(cid:18)

dsignni
(cid:16)∑

(cid:17)

i zi√
k

n!

n1!··· nk!

¯Hnj (zj)

n1+...+nk=n

Combining Equation 1 and Claim 2 now yields 

ReLU

Ez∼N (0 Id)

= Ez∼N (0 Id)

X
#

(cid:19)

∞X

(cid:18)P
"

i∈S zi√
k

=

n=0

ReLU

i∈S zi√
k

[ReLUn
kn/2

(cid:18)P
" ∞X
0@ X
kY
dsignmj
X
(cid:18)
X

n1+...+nk=n

m1 ... mk

n=0

j=1

[ReLUn
kn/2
[ReLUn
kn/2

[ReLUn
kn/2

·

(cid:19)

·

Y
X

i∈S

sign(zi)

(cid:18)

1A35
(cid:19)1/2 kY

n1!··· nk!

n!

n1+...+nk=n

¯Hmj (zj)

X

(cid:18)

m1 ... mk

n!

n1!··· nk!

×

∞X
∞X

n=0

=

=

(cid:19)1/2 kY

j=1

!

¯Hni(zi)

(cid:19)1/2 kY

n!

n1!··· nk!

i=1

dsignmi

E[ ¯Hni(zi) ¯Hmi(zi)]

(cid:19)1/2 kY
dsignni

i=1

i=1

n=0

"

(cid:19)

n1+...+nk=n

(cid:18)P

From Fact 2 and Claim 1 we see that dsign2m = 0 and [ReLU2m+1 = 0 for m ≥ 1. Additionally 
sincedsign0 = 0 we see that each ni ≥ 1. This gives us 
X
X

s
(cid:19)1/2 kY
r
(cid:19)3/2 kY

1√
2πn!kn/2

(Hn(0) + nHn−2(0))

∞X
∞X

n1 ... nk≥1
n1+...+nk=n

n1!··· nk!

Ez∼N (0 Id)

i∈S zi√
k

Hnj−1(0)

Y

sign(zi)

(cid:18)

(cid:18)

ReLU

#

πnj!

i∈S

n=k

i=1

n!

=

2

·

1√
2πkn/2

=

n=k

(Hn(0) + nHn−2(0))

n1 ... nk≥1
n1+...+nk=n

2
π

Hnj−1(0).

i=1

To ﬁnish the proof of Lemma 1  we will look at each term in the outer summation above. Let the
term for any ﬁxed n ≥ k be denoted by Tn. Since ¯Hi(0) = 0 for odd i  observe that Tn is non-zero
if and only if n is even and each ni = 2n

(cid:18)
(−1)

Tn =

1√
2πk n

2

n
2

n!

(n/2)!2 n

2

1

n1!··· nk!
(cid:19)
≥ 0. We have
′
i
(n − 2)!
− 1)!2 n

−1

2

′
i + 1 for n
+ n(−1)

−1

n
2

( n
2

5

(cid:19)3/2 kY

r

′
k + 1)!

j=1

(−1)n

′
j

2
π

′
j)!
(2n
′
′
j!2n
j

n

2

n−k

(cid:19)3/2 (2n
(cid:18)

1)!··· (2n
′
1!··· n
′
′
k!

n

′
k)!

1

1 + 1)!··· (2n
′

′
k + 1)!

(2n

(cid:19)3/2 (2n

1)!··· (2n
′
1!··· n
′
′
k!

n

′
k)!

X

(cid:18)

×

2

≥0
′
n
j = n−k
′
i
j n
−1n!

∑
(−1) n
X
2πk n
×

2

2 (n/2)!2 n

2

(cid:18)

√

=

1

1 + 1)!··· (2n
′
(cid:19)

(2n

(cid:18)

−1 +

n
n − 1
1

2 (−1)
2 k
π k

2

∑

(2n

≥0
′
n
j
j = n−k
′
j n
(−1)1+ k

2

2 n!
2 (n − 1)(n/2)!2

′
k + 1)!

1 + 1)!··· (2n
′
X

n−k
2 π k

2

∑

≥0
′
n
j = n−k
′
i
j n

√

=

2πk n

P∞
Since k = 4l + 2 (by assumption)  Tn > 0 for all even n ≥ k and equal to 0 for all odd n. Thus

2

n=k Tn > Tk. Lower bounding Tk  we have

√

Tk =

(4l + 2)!

2π(4l + 2)2l+1(4l + 1)(2l + 1)!π2l+1

√

≈

=

1

(cid:18)

(cid:19)2l+1

2π(4l + 2)2l+1(4l + 1)π2l+1
1√
π(4l + 1)

2
eπ

= 2

−O(k)

p

2π(4l + 2)

(cid:18)

4l + 2

e

(cid:19)4l+2

1p

2π(2l + 1)

(cid:19)2l+1

(cid:18)

e

2l + 1

Now we present our main algorithm (Algorithm 1) that reduces learning sparse parities with noise
to agnostically learning ReLUs and a proof of its correctness.

Algorithm 1 Learning Sparse Parities with Noise using Agnostic ReLU learner

Input Training set S of M1 samples (xi  yi)M1

i=1  validation set V of M2 samples

(xi  yi)M1+M2

i=M1+1  error parameter ϵ  Agnostic ReLU learner A

Output Set of relevant variables Vrel

1: Set Vrel = ∅
2: Set S1  . . .  Sd := ∅
3: for i = 1 to M1 + M2 do
4:
5:
6:
7: for j ∈ [d] do
8:
9:
10:
11:
12: Return Vrel

ComputecerrVj (hj)
ifcerrVj (hj) ≥ 1

2
Add j to Vrel

4π

Draw n independent univariate half Gaussians g1  . . .   gd
Construct x′ such that for all j ∈ [d]  x
′
j and set y
j := gjxi
For all j ∈ [d]  if i ≤ M1 add (x′
) to Sj else to Vj
′
−j  y
Run A on Sj to obtain hypothesis hj
− ϵ/4 then

− 1

′

= yi+1

2

(cid:16)

(cid:17)

(cid:16)

(cid:17)

Theorem 3. If there is an algorithm to agnostically learn unbiased ReLUs on the Gaussian dis-
tribution in time and samples T (d  1/ϵ)  then there is an algorithm to solve k-SLPN in time
O

where η is the noise rate.

+ O(d)T

2O(k)
(1−2η)2 log(d)

d  2O(k)
1−2η

In particular  if Assumption 1 is true  then any algorithm for agnostically learning (unbiased) ReLUs
on the Gaussian distribution must run in time dΩ(log(1/ϵ)).

6

∏

Proof. Given a set of samples from the k-SPLN problem  we claim that Algorithm 1 can recover all
indices j belonging to the sparse parity when run with appropriate parameters. We will ﬁrst show
that if a variable is relevant then the error is smaller compared to when it is irrelevant. It is easy to
otherwise. Let Dj denote the
see that y
distribution obtained by dropping the jth coordinate from the lifted distribution and let S denote the
set of active indices of the parity. The proof of the theorem follows from the following claims 

with probability 1− η and 1−∏

′
i∈S sign(x
i)+1

i∈S sign(x

′ is

′
i)

2

2

Claim 3. If j ∈ S then for all w  errDj (ReLUw) =

∥w∥2
2

− ∥w∥√

2π

≥ 1

2

− 1
4π .

+ 1
2

Claim 4. If j /∈ S then there exists w∗ with ∥w∗∥ = 1√
−O(k)
2
1−2η .

2π

such that errDj (ReLUw∗ ) < 1

2

− 1

4π

−

−O(k)
1−2η = 2

−ck
Claims 3 and 4 imply that we have a gap of at least 2
1−2η for some c > 0 between the
−ck
relevant and irrelevant variable case. Setting ϵ = 2
1−2η in Algorithm 1 will let us detect this gap.
Since A is an agnostic learner for ReLU  as long as M1 = T (d  2/ϵ) we know that with probability
2/3  for all j ∈ S  A runs on Sj and outputs hj such that errDj (hj) ≤ minw errDj (ReLUw) ≤
− 1
[Ver] we see that using a validation set of M2 = 100/ϵ2 samples  we have for all j  |cerrVj (hj) −
1
2
Using standard concentration inequalities for sub-Gaussian and subexponential random variables
errDj (hj)| ≤ ϵ/4. Therefore  we can differentiate the two cases as in the Algorithm with conﬁdence
> 1/2. It is easy to see that the run time of the algorithm is O(d)T (d  2/ϵ) + O(1/ϵ2)  and that this
can be ampliﬁed to obtain an algorithm with any desired conﬁdence using standard techniques.

− ϵ/2  and for all j /∈ S  errDj (hj) ≥ 1

− 1
4π .

4π

2

4 Lower Bounds for SQ Algorithms

A consequence of Theorem 3 is that any statistical-query algorithm for agnostically learning a ReLU
with respect to Gaussian marginals yields a statistical-query algorithm for learning parity functions
on k unknown input bits. This implies that there is no polynomial time statistical-query (SQ) algo-
rithm that learns a ReLU with respect to Gaussian marginals for a certain restricted class of queries.
We present the formal theorem and defer the proof to the supplemental.
Theorem 4. Any SQ algorithm for agnostically learning a ReLU with respect to any distribution D
satisfying Gaussian marginals over the attributes  requires dΩ(log(1/ϵ)) unit norm correlation queries
or queries independent of the target with tolerance
poly(d 1/ϵ) to an oracle that returns τ-approximate
expectations with respect to D.

1

Remark: Note that this implies there is no do(1/ϵ)-time gradient descent algorithm that can ag-
nostically learn ReLU(w · x)  under the reasonable assumption that for every i the gradients of
E(x y)∼D
can be computed by O(d) queries whose norms are polyno-
mially bounded.

ReLUw(ν(x)−i) − y+1

2

h(cid:0)

i

(cid:1)2

5 Approximation Algorithm

In this section we give a learning algorithm that runs in polynomial time in all input parameters
and outputs a ReLU that has error O(opt2/3) + ϵ where opt is the error of the best-ﬁtting ReLU.
The main reduction is a hard thresholding of the labels to create a training set with Boolean labels.
We then apply a recent result giving a polynomial-time approximation algorithm for agnostically
learning halfspaces over the Gaussian distribution due to Awasthi et. al. [ABL14]. We present our
algorithm and give a proof of its correctness.

7

∗

err0/1(w

) ≤ Pr[x ̸∈ Sgood\{v : w∗ · v ∈ (0  2α)}]
≤ Pr[x ̸∈ Sgood] + Pr[x ∈ {v : w∗ · v ∈ (0  2α)}]
≤ opt

−g2/2dg ≤ opt

Z

2α

e

α2 +

1√
2π

0

α2 + 2α.

(cid:0)

(cid:1)

Algorithm 2

Input Training set S of m samples (xi  yi)m
A from [ABL14] and a parameter α

Output Weight vectorbw
2: Run A to recoverbw close in err0/1.
3: Returnbw

:= {(x  sign(y − α)) | (x  y) ∈ S}.

1: Construct S

′

i=1  the agnostic halfspace learning algorithm

Theorem 5. There is an algorithm (Algorithm 2) that given O(poly(d  1/ϵ)) samples (x  y) such
that x is drawn from N (0  Id) and y ∈ [0  1] recovers a unit vector w such that err(ReLUw) ≤
O(opt2/3) + ϵ where opt := min∥w∥=1 err(ReLUw).

Proof. Let w∗
= arg min∥w∥=1 err(ReLUw) and so  err(ReLUw∗ ) = opt. Deﬁne the Sgood to be the
set of points that are α-close to the optimal ReLU  i.e. Sgood = {x : |y − ReLUw∗ (x)| ≤ α}. By
Markov’s inequality 

Pr[x ̸∈ Sgood] = Pr[|y − ReLUw∗ (x)| ≥ α] ≤ opt
α2 .

This implies that all but an opt
α2 fraction of the points are α-close to their corresponding y’s. In the
ﬁrst step of Algorithm 2  the labels become Boolean. Deﬁne the 0/1 error of the vector w as follows 
err0/1(w) = E[sign(y − α) ̸= sign(w · x)]. Let w† be the argmin of err0/1(w) over all vectors w
with ∥w∥2 ≤ 1. Since for all elements in Sgood\{v : w∗ · v ∈ (0  2α)}  sign(y − α) = sign(w∗ · x) 

We now apply Theorem 8 from [ABL14] which gives an algorithm with polynomial running time
in d and 1/ϵ that outputs a w such that ∥w∥ = 1 and ∥w − w†∥ ≤ O(
) + ϵ. For unit
vectors a  b  θ(a  b) < C Pr[sign(a · x) ̸= sign(b · x)] for some absolute constant C where θ(a  b)
is the angle between the vectors (see Lemma 2 in [ABL14]). The triangle inequality and the fact
that ∥a − b∥ ≤ θ(a  b) implies that if err0/1(a)  err0/1(b) < η then ∥a − b∥ ≤ C Pr[sign(a · x) ̸=
sign(b· x)] ≤ O(η). Applying this to w† and w∗ yields ∥w† − w∗∥ < O( opt
α2 + 2α). Since the ReLU
function is 1-Lipschitz  we have

opt
α2 + 2α

err(ReLUw) = E[(y − ReLU(w · x))2]

≤ 2E[(y − ReLU(w∗ · x))2] + 2E[(ReLU(w∗ · x) − ReLU(w · x))2]
(cid:18)
≤ 2opt + 2E[((w∗ − w) · x)2]
= 2opt + 2∥w∗ − w∥2 ≤ O

(cid:17)2
Setting α = opt1/3 and rescaling ϵ we have err(ReLUw) ≤ O(opt2/3) + ϵ.

opt
α2 + 2α

(cid:19)

opt +

(cid:16)

+ ϵ

6 Conclusions and Open Problems

We have shown hardness for solving the empirical risk minimization problem for just one ReLU
with respect to Gaussian distributions and given the ﬁrst nontrivial approximation algorithm. Can
we achieve approximation O(opt) + ϵ? Note our results holds only for the case of unbiased ReLUs 
as the constant function 1/2 may achieve smaller square-loss than any unbiased ReLU. Interestingly 
all positive results that we are aware of for learning ReLUs (or one-layer ReLU networks) with
respect to Gaussians also assume the ReLU activations are unbiased (e.g.  [BG17  Sol17  GKM18 
GKLW19  GLM18  ZYWG19]). How difﬁcult is the biased case?

8

Acknowledgments

Surbhi Goel and Adam R. Klivans were supported by NSF Award CCF-1717896. Sushrut Karmalkar
was supported by NSF Award CNS-1414023.

References

[ABL14] Pranjal Awasthi  Maria Florina Balcan  and Philip M Long. The power of localization
for efﬁciently learning linear separators with noise. In Proceedings of the forty-sixth
annual ACM symposium on Theory of computing  pages 449–458. ACM  2014.

[AS18] Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. CoRR 

abs/1812.06369  2018.

[BDL18] Digvijay Boob  Santanu S. Dey  and Guanghui Lan. Complexity of training relu neural

network. CoRR  abs/1809.10787  2018.

[BG17] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet
with gaussian inputs. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70  pages 605–614. JMLR. org  2017.

[BGS14] Guy Bresler  David Gamarnik  and Devavrat Shah. Structure learning of antiferromag-
netic ising models.
In Zoubin Ghahramani  Max Welling  Corinna Cortes  Neil D.
Lawrence  and Kilian Q. Weinberger  editors  Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural Information Processing Systems 2014 
December 8-13 2014  Montreal  Quebec  Canada  pages 2852–2860  2014.

[DKS18] Ilias Diakonikolas  Daniel M. Kane  and Alistair Stewart. Learning geometric concepts
with nasty noise. In Ilias Diakonikolas  David Kempe 0001  and Monika Henzinger 
editors  Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Com-
puting  STOC 2018  Los Angeles  CA  USA  June 25-29  2018  pages 1061–1073. ACM 
2018.

[GKKT17] Surbhi Goel  Varun Kanade  Adam R. Klivans  and Justin Thaler. Reliably learning the
relu in polynomial time. In Satyen Kale and Ohad Shamir  editors  Proceedings of the
30th Conference on Learning Theory  COLT 2017  Amsterdam  The Netherlands  7-10
July 2017  volume 65 of Proceedings of Machine Learning Research  pages 1004–1042.
PMLR  2017.

[GKLW19] Rong Ge  Rohith Kuditipudi  Zhize Li  and Xiang Wang. Learning two-layer neural
networks with symmetric inputs. In International Conference on Learning Representa-
tions  2019.

[GKM18] Surbhi Goel  Adam R. Klivans  and Raghu Meka. Learning one convolutional layer
with overlapping patches.
In Jennifer G. Dy and Andreas Krause 0001  editors 
ICML  volume 80 of JMLR Workshop and Conference Proceedings  pages 1778–1786.
JMLR.org  2018.

[GLM18] Rong Ge  Jason D. Lee  and Tengyu Ma. Learning one-hidden-layer neural networks
with landscape design. In International Conference on Learning Representations  2018.

[KK14] Adam Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian
space. In Approximation  Randomization  and Combinatorial Optimization. Algorithms
and Techniques (APPROX/RANDOM 2014). Schloss Dagstuhl-Leibniz-Zentrum fuer
Informatik  2014.

[KKKS11] Sham M. Kakade  Adam Kalai  Varun Kanade  and Ohad Shamir. Efﬁcient learning
of generalized linear and single index models with isotonic regression. In NIPS  pages
927–935  2011.

[KKMS08] Adam Tauman Kalai  Adam R Klivans  Yishay Mansour  and Rocco A Servedio. Ag-

nostically learning halfspaces. SIAM Journal on Computing  37(6):1777–1805  2008.

9

[KS09] Adam Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regres-

sion. In COLT  2009.

[MR18] Pasin Manurangsi and Daniel Reichman. The computational complexity of training

relu(s). CoRR  abs/1810.04207  2018.

[Sha18] Ohad Shamir. Distribution-speciﬁc hardness of learning neural networks. Journal of

Machine Learning Research  19:32:1–32:29  2018.

[Sol17] Mahdi Soltanolkotabi. Learning relus via gradient descent.

Information Processing Systems  pages 2007–2017  2017.

In Advances in Neural

[SSSS17] Shai Shalev-Shwartz  Ohad Shamir  and Shaked Shammah. Failures of gradient-based
deep learning. In International Conference on Machine Learning  pages 3067–3075 
2017.

[Val15] Gregory Valiant. Finding correlations in subquadratic time  with applications to learn-
ing parities and the closest pair problem. Journal of the ACM (JACM)  62(2):13  2015.

[Ver] Roman Vershynin. Four lectures on probabilistic methods for data science.

[VW19] Santosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural net-
works: Polynomial convergence and sq lower bounds. In Alina Beygelzimer and Daniel
Hsu  editors  Proceedings of the Thirty-Second Conference on Learning Theory  vol-
ume 99 of Proceedings of Machine Learning Research  pages 3115–3117  Phoenix 
USA  25–28 Jun 2019. PMLR.

[Zha18] Chicheng Zhang. Efﬁcient active learning of sparse halfspaces. In Sébastien Bubeck 
Vianney Perchet  and Philippe Rigollet  editors  Conference On Learning Theory 
COLT 2018  Stockholm  Sweden  6-9 July 2018  volume 75 of Proceedings of Machine
Learning Research  pages 1856–1880. PMLR  2018.

[ZSJ+17] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery
In Proceedings of the 34th Inter-
guarantees for one-hidden-layer neural networks.
national Conference on Machine Learning-Volume 70  pages 4140–4149. JMLR. org 
2017.

[ZYWG19] Xiao Zhang  Yaodong Yu  Lingxiao Wang  and Quanquan Gu. Learning one-hidden-
In The 22nd International Conference on

layer relu networks via gradient descent.
Artiﬁcial Intelligence and Statistics  pages 1524–1534  2019.

10

,Surbhi Goel
Sushrut Karmalkar
Adam Klivans