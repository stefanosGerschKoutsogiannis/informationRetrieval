2011,Scalable Training of Mixture Models via Coresets,How can we train a statistical mixture model on a massive data set? In this paper  we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data  which guarantees that models fitting the coreset will also provide a good fit for the original data set. We show that  perhaps surprisingly  Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely  we prove that a weighted set of $O(dk^3/\eps^2)$ data points suffices for computing a $(1+\eps)$-approximation for the optimal model on the original $n$ data points. Moreover  such coresets can be efficiently constructed in a map-reduce style computation  as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry  as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets  including a density estimation problem in the context of earthquake detection using accelerometers in  mobile phones.,Scalable Training of Mixture Models via Coresets

Dan Feldman

MIT

Matthew Faulkner

Caltech

Andreas Krause

ETH Zurich

Abstract

How can we train a statistical mixture model on a massive data set? In this paper  we
show how to construct coresets for mixtures of Gaussians and natural generalizations. A
coreset is a weighted subset of the data  which guarantees that models ﬁtting the coreset
will also provide a good ﬁt for the original data set. We show that  perhaps surprisingly 
Gaussian mixtures admit coresets of size independent of the size of the data set. More
precisely  we prove that a weighted set of O(dk3/ε2) data points sufﬁces for computing
a (1 + ε)-approximation for the optimal model on the original n data points. Moreover 
such coresets can be efﬁciently constructed in a map-reduce style computation  as well as
in a streaming setting. Our results rely on a novel reduction of statistical estimation to
problems in computational geometry  as well as new complexity results about mixtures of
Gaussians. We empirically evaluate our algorithms on several real data sets  including a
density estimation problem in the context of earthquake detection using accelerometers in
mobile phones.

Introduction

1
We consider the problem of training statistical mixture models  in particular mixtures of Gaussians
and some natural generalizations  on massive data sets. Such data sets may be distributed across
a cluster  or arrive in a data stream  and have to be processed with limited memory. In contrast to
parameter estimation for models with compact sufﬁcient statistics  mixture models generally require
inference over latent variables  which in turn depends on the full data set. In this paper  we show that
Gaussian mixture models (GMMs)  and some generalizations  admit small coresets: A coreset is a
weighted subset of the data which guarantees that models ﬁtting the coreset will also provide a good
ﬁt for the original data set. Perhaps surprisingly  we show that Gaussian mixtures admit coresets of
size independent of the size of the data set.
We focus on ε-semi-spherical Gaussians  where the covariance matrix Σi of each component i has
eigenvalues bounded in [ε  1/ε]  but some of our results generalize even to the semi-deﬁnite case.
In particular  we show that given a data set D of n points in Rd  ε > 0 and k ∈ N  how one can
efﬁciently construct a weighted set C of O(dk3/ε2) points  such that for any mixture of k ε-semi-
spherical Gaussians θ = [(w1  µ1  Σ1)  . . .   (wk  µk  Σk)] it holds that the log-likelihood ln P (D |
θ) of D under θ is approximated by the (properly weighted) log-likelihood ln P (C | θ) of C under
θ to arbitrary accuracy as ε → 0. Thus solving the estimation problem on the coreset C (e.g.  using
weighted variants of the EM algorithm  see Section 3.3) is almost as good as solving the estimation
problem on large data set D. Our algorithm for constructing C is based on adaptively sampling
points from D and is simple to implement. Moreover  coresets can be efﬁciently constructed in a
map-reduce style computation  as well as in a streaming setting (using space and update time per
point of poly(dkε−1 log n log(1/δ))).
Existence and construction of coresets have been investigated for a number of problems in compu-
tational geometry (such as k-means and k-median) in many recent papers (cf.  surveys in [1  2]). In
this paper  we demonstrate how these techniques from computational geometry can be lifted to the
realm of statistical estimation. As a by-product of our analysis  we also close an open question on
the VC dimension of arbitrary mixtures of Gaussians. We evaluate our algorithms on several syn-
thetic and real data sets. In particular  we use our approach for density estimation for acceleration
data  motivated by an application in earthquake detection using mobile phones.

1

i=1 wiN (x; µi  Σi)  where w1  . . .   wk ≥ 0 are the mixture weights (cid:80)

P (x | θ) =(cid:80)k
|2πΣi| exp(cid:0)− 1
negative log likelihood of the data is L(D | θ) = −(cid:80)

2 Background and Problem Statement
Fitting mixture models by MLE. Suppose we are given a data set D = {x1  . . .   xn} ⊆ Rd. We
consider ﬁtting a mixture of Gaussians θ = [(w1  µ1  Σ1)  . . .   (wk  µk  Σk)]  i.e.  the distribution
i wi = 1  and
µi and Σi are mean and covariance of the i-th mixture component  which is modeled as a multivariate
normal distribution N (x  µi  Σi) =
will discuss extensions to more general mixture models. Assuming the data was generated i.i.d.  the
j ln P (xj | θ)  and we wish to obtain the
maximum likelihood estimate (MLE) of the parameters θ∗ = argminθ∈C L(D | θ)  where C is a set
of constraints ensuring that degenerate solutions are avoided1. Hereby  for a symmetric matrix A 
spec A is the set of all eigenvalues of A. We deﬁne

(x − µi)(cid:1). In Section 4  we

2 (x − µi)T Σ−1

1√

i

C = Cε = {θ = [(w1  µ1  Σ1)  . . .   (wk  µk  Σk)] | ∀i : spec(Σi) ⊆ [ε  1/ε]}

to be the set of all mixtures of k Gaussians θ  such that all the eigenvalues of the covariance matrices
of θ are bounded between ε and 1/ε for some small ε > 0.
Approximating the log-likelihood. Our goal is to approximate the data set D by a weighted set
C = {(γ1  x(cid:48)
m)} ⊆ R × Rd  such that L(D | θ) ≈ L(C | θ) for all θ  where we

1)  . . .   (γm  x(cid:48)

deﬁne L(C | θ) = −(cid:80)

i γi ln P (x(cid:48)

i | θ).

What kind of approximation accuracy may we hope to expect? Notice that there is a nontrivial
issue of scale: Suppose we have a MLE θ∗ for D  and let α > 0. Then straightforward linear
α for a scaled data set αD = {αx : x ∈ D} by simply
algebra shows that we can obtain an MLE θ∗
scaling all means by α  and covariance matrices by α2. For the log-likelihood  however  it holds
α) = d ln α + L(D | θ∗). Therefore  optimal solutions on one scale can be efﬁciently
that L(αD | θ∗
transformed to optimal solutions at a different scale  while maintaining the same additive error.
This means  that any algorithm which achieves absolute error ε at any scale could be used to achieve
parameter estimates (for means  covariances) with arbitrarily small error  simply by applying the
algorithm to a scaled data set and transforming back the obtained solution. An alternative  scale-
invariant approach may be to strive towards approximating L(D | θ) up to multiplicative error
(1 + ε). Unfortunately  this goal is also hard to achieve: Choosing a scaling parameter α such that
d ln α + L(D | θ∗) = 0 would require any algorithm that achieves any bounded multiplicative
error to essentially incur no error at all when evaluating L(αD | θ∗). The above observations hold
even for the case k = 1 and Σ = I  where the mixture θ consists of a single Gaussian  and the
log-likelihood is the sum of squared distances to a point µ and an additive term.
Motivated by the scaling issues discussed above  we use the following error bound that was sug-
gested in [3] (who studied the case where all Gaussians are identical spheres). We decompose the
negative log-likelihood L(D | θ) of a data set D as

(cid:19)

(cid:18)

L(D | θ) = − n(cid:88)
where Z(θ) =(cid:80)

k(cid:88)
φ(D | θ) = − n(cid:88)

wi(cid:112)|2πΣi| exp
k(cid:88)

j=1

i=1

ln

i

− 1
2

(xj − µi)T Σ−1

i

(xj − µi)

= −n ln Z(θ) + φ(D | θ)

wi√
|2πΣi| is a normalizer  and the function φ is deﬁned as
(xj − µi)T Σ−1

Z(θ)(cid:112)|2πΣi| exp

− 1
2

wi

ln

i

(cid:18)

j=1

i=1

(cid:19)

(xj − µi)

.

Hereby  Z(θ) plays the role of a normalizer  which can be computed exactly  independently of the
set D. φ(D | θ) captures all dependencies of L(D | θ) on D  and via Jensen’s inequality  it can be
seen that φ(D | θ) is always nonnegative.
We can now use this term φ(D | θ) as a reference for our error bounds. In particular  we call ˜θ a
(1 + ε)-approximation for θ if (1 − ε)φ(D | θ) ≤ φ(D | ˜θ) ≤ φ(D | θ)(1 + ε).
Coresets. We call a weighted data set C a (k  ε)-coreset for another (possibly weighted) set D ⊆
Rd  if for all mixtures θ ∈ C of k Gaussians it holds that

(1 − ε)φ(D | θ) ≤ φ(C | θ) ≤ φ(D | θ)(1 + ε).

1equivalently  C can be interpreted as prior thresholding.

2

(a) Example data set

(b) Iteration 1

(c) Iteration 3

(d) Final approximation B

(e) Sampling distribution

(f) Coreset

Figure 1: Illustration of the coreset construction for example data set (a). (b c) show two iterations of con-
structing the set B. Solid squares are points sampled uniformly from remaining points  hollow squares are
points selected in previous iterations. Red color indicates half the points furthest away from B  which are
kept for next iteration. (d) ﬁnal approximate clustering B on top of original data set. (e) Induced non-uniform
sampling distribution: radius of circles indicates probability; color indicates weight  ranging from red (high
weight) to yellow (low weight). (f) Coreset sampled from distribution in (e).
Hereby φ(C | θ) is generalized to weighted data sets C in the natural way (weighing the contribu-
j ∈ C by γj). Thus  as ε → 0  for a sequence of (k  ε)-coresets Cε we
tion of each summand x(cid:48)
(cid:81)
have that supθ∈C |L(Cε | θ) − L(D | θ)| → 0  i.e.  L(Cε | θ) uniformly (over θ ∈ C) approximates
L(D | θ). Further  under the additional condition that all variances are sufﬁciently large (formally
λ∈spec(Σi) λ ≥ 1
(2π)d for all components i)  the log-normalizer ln Z(θ) is negative  and conse-
quently the coreset in fact provides a multiplicative (1 + ε) approximation to the log-likelihood  i.e. 

(1 − ε)L(D | θ) ≤ L(C | θ) ≤ L(D | θ)(1 + ε).

ε   d and k.

More details can be found in the supplemental material.
Note that if we had access to a (k  ε)-coreset C  then we could reduce the problem of ﬁtting a mixture
model on D to one of ﬁtting a model on C  since the optimal solution θC is a good approximation
(in terms of log-likelihood) of θ∗. While ﬁnding the optimal θC is a difﬁcult problem  one can use
a (weighted) variant of the EM algorithm to ﬁnd a good solution. Moreover  if |C| (cid:28) |D|  running
EM on C may be orders of magnitude faster than solving it on D. In Section 3.3  we give more
details about solving the density estimation problem on the coreset.
The key question is whether small (k  ε)-coresets exist  and whether they can be efﬁciently con-
structed. In the following  we answer this question afﬁrmatively. We show that  perhaps surprisingly 
one can efﬁciently ﬁnd coresets C of size independent of the size n of D  and with polynomial de-
pendence on 1
3 Efﬁcient Coreset Construction via Adaptive Sampling
Naive approach: uniform sampling. A naive approach towards approximating D would be to
just pick a subset C uniformly at random. In particular  suppose the data set is generated from a
mixture of two spherical Gaussians (Σi = I) with weights w1 = 1√
n. Unless
√
n) points are sampled  with constant probability no data point generated from Gaussian
m = Ω(
2 is selected. By moving the means of the Gaussians arbitrarily far apart  L(D | θC) can be made
arbitrarily worse than L(D | θD)  where θC and θD are MLEs on C and D respectively. Thus  even
for two well-separated Gaussians  uniform sampling can perform arbitrarily poorly. This example
already suggests that  intuitively  in order to achieve small multiplicative error  we must devise a
sampling scheme that adaptively selects representative points from all “clusters” present in the data
set. However  this suggests that obtaining a coreset requires solving a chicken-and-egg problem 
where we need to understand the density of the data to obtain the coreset  but simultaneously would
like to use the coreset for density estimation.

n and w2 = 1 − 1√

3

−2−1.5−1−0.500.511.52−4−3−2−10123−2−1.5−1−0.500.511.52−4−3−2−10123−2−1.5−1−0.500.511.52−4−3−2−10123−2−1.5−1−0.500.511.52−4−3−2−10123Better approximation via adaptive sampling. The key idea behind the coreset construction is
that we can break the chicken-and-egg problem by ﬁrst obtaining a rough approximation B of the
clustering solution (using more than k components  but far fewer than n)  and then to use this so-
lution to bias the random sampling. Surprisingly  a simple procedure which iteratively samples a
small number β of points  and removes half of the data set closest to the sampled points  provides
a sufﬁciently accurate ﬁrst approximation B for this purpose. This initial clustering is then used to
sample the data points comprising coreset C according to probabilities which are roughly propor-
tional to the squared distance to the set B. This non-uniform random sampling can be understood as
an importance-weighted estimate of the log-likelihood L(D | θ)  where the weights are optimized
in order to reduce the variance. The same general idea has been found successful in constructing
coresets for geometric clustering problems such as k-means and k-median [4]. The pseudocode for
obtaining the approximation B  and for using it to obtain coreset C is given in Algorithm 1.

Output: Coreset C =(cid:8)(γ(x1)  x1)  . . .   (γ(x|C|)  x|C|)(cid:9)

Algorithm 1: Coreset construction
Input: Data set D  ε  δ  k
D(cid:48) ← D; B ← ∅;
while |D(cid:48)| > 10dk ln(1/δ) do

Sample set S of β = 10dk ln(1/δ) points uniformly at random from D(cid:48);
Remove (cid:100)|D(cid:48)|/2(cid:101) points x ∈ D(cid:48) closest to S (i.e.  minimizing dist(x  S)) from D(cid:48);
Set B ← B ∪ S;

Set B ← B ∪ D(cid:48);
for each b ∈ B do Db ← the points in D whose closest point in B is b. Ties broken arbitrarily;
for each b ∈ B and x ∈ Db do

m(x) ←(cid:108) 5|Db| +
x ∈ D  we have x(cid:48) = x with probability m(x)/(cid:80)

x(cid:48)∈D dist(x(cid:48) B)2

dist(x B)2

(cid:109)

;

(cid:80)

for each x(cid:48) ∈ C do γ(x(cid:48)) ←

(cid:80)
x∈D m(x)
|C|·m(x(cid:48)) ;

x(cid:48)∈D m(x(cid:48));

Pick a non-uniform random sample C of 10(cid:100)dk|B|2 ln(1/δ)/ε2(cid:101) points from D  where for every x(cid:48) ∈ C and

We have the following result  proved in the supplemental material:
Theorem 3.1. Suppose C is sampled from D using Algorithm 1 for parameters ε  δ and k. Then 
with probability at least 1 − δ it holds that for all θ ∈ Cε 

φ(D | θ)(1 − ε) ≤ φ(C | θ) ≤ φ(D | θ)(1 + ε).

In our experiments  we compare the performance of clustering on coresets constructed via adaptive
sampling  vs. clustering on a uniform sample. The size of C in Algorithm 1 depends on |B|2 =
log2 n. By replacing B in the algorithm with a constant factor approximation B(cid:48)  |B(cid:48)| = l for the
k-means problem  we can get a coreset C of size independent of n. Such a set B(cid:48) can be computed
in O(ndk) time either by applying exhaustive search on the output C of the original Algorithm 1 or
by using one of the existing constant-factor approximation algorithms for k-means (say  [5]).
3.1 Sketch of Analysis: Reduction to Euclidean Spaces
For space limitations  the proof of Theorem 3.1 is included in the supplemental material  we only
provide a sketch of the analysis  carrying the main intuition. The key insight in the proof is that the
contribution log P (x | θ) to the likelihood L(D | θ) can be expressed in the following way:
Lemma 3.2. There exist functions φ  ψ  and f such that  for any point x ∈ Rd and mixture model
θ  ln P (x | θ) = −fφ(x)(ψ(θ)) + Z(θ)  where

(cid:88)

˜wiexp(cid:0)−Widist(˜x − ˜µi  si)2(cid:1) .

f˜x(y) = − ln

i

Hereby  φ is a function that maps a point x ∈ Rd into ˜x = φ(x) ∈ R2d  and ψ is a function that
maps a mixture model θ into a tuple y = (s  w  ˜µ  W ) where w is a k-tuple of nonnegative weights
˜w1  . . .   ˜wk summing to 1  s = s1  . . .   sk ⊆ R2d is a set of k d-dimensional subspaces that are
weighted by weights W1 ···   Wk > 0  and ˜µ = ˜µ1 ···   ˜µk ∈ R2d is a set of k means.
The main idea behind Lemma 3.2 is that level sets of distances between points and subspaces
are quadratic forms  and can thus represent level sets of the Gaussian probability density func-
tion (see Figure 2(a) for an illustration). We recognize the “soft-min” function ∧w(cid:48)(η) ≡

4

(a) Gaussian pdf as Euclidean distances

(b) Tree for coreset construction

Figure 2: (a) Level sets of the distances between points on a plane (green) and (disjoint) k-dimensional sub-
spaces are ellipses  and thus can represent contour lines of the multivariate Gaussian. (b) Tree construction for
generating coresets in parallel or from data streams. Black arrows indicate “merge-and-compress” operations.
The (intermediate) coresets C1  . . .   C7 are enumerated in the order in which they would be generated in the
streaming case. In the parallel case  C1  C2  C4 and C5 would be constructed in parallel  followed by parallel
construction of C3 and C6  ﬁnally resulting in C7.

− ln(cid:80)

i w(cid:48)

(cid:80)

iexp (−ηi) as an approximation upper-bounding the minimum min(η) = mini ηi for
ηi = Widist(˜x − ˜µi  si)2 and η = [η1  . . .   ηk]. The motivation behind this transformation is that it
allows expressing the likelihood P (x | θ) of a data point x given a model θ in a purely geometric
manner as soft-min over distances between points and subspaces in a transformed space. Notice
that if we use the minimum min() instead of the soft-min ∧ ˜w()  we recover the problem of approx-
imating the data set D (transformed via φ) by k-subspaces. For semi-spherical Gaussians  it can be
shown that the subspaces can be chosen as points while incurring a multiplicative error of at most
1/ε  and thus we recover the well-known k-means problem in the transformed space. This insight
suggests using a known coreset construction for k-means  adapted to the transformation employed.
The remaining challenge in the proof is to bound the additional error incurred by using the soft-min
function ∧ ˜w(·) instead of the minimum min(·). We tackle this challenge by proving a general-
ized triangle inequality adapted to the exponential transformation  and employing the framework
described in [4]  which provides a general method for constructing coresets for clustering problems
of the form mins
As proved in [4]  the key quantity that controls the size of a coreset is the pseudo-dimension of the
functions Fd = {f˜x for ˜x ∈ R2d}. This notion of dimension is closely related to the VC dimension
of the (sub-level sets of the) functions Fd and therefore represents the complexity of this set of
functions. The ﬁnal ingredient in the proof of Theorem 3.1 is a new bound on the complexity of
mixtures of k Gaussians in d dimensions proved in the supplemental material.
3.2 Streaming and Parallel Computation
One major advantage of coresets is that they can be constructed in parallel  as well as in a streaming
setting where data points arrive one by one  and it is impossible to remember the entire data set due
to memory constraints. The key insight is that coresets satisfy certain composition properties  which
have previously been used by [6] for streaming and parallel construction of coresets for geometric
clustering problems such as k-median and k-means.
1. Suppose C1 is a (k  ε)-coreset for D1  and C2 is a (k  ε)-coreset for D2. Then C1 ∪ C2 is
2. Suppose C is a (k  ε)-coreset for D  and C(cid:48) is a (k  δ)-coreset for C. Then C(cid:48) is a (k  (1 +

i f˜x(s).

a (k  ε)-coreset for D1 ∪ D2.
ε)(1 + δ) − 1)-coreset for D.

In the following  we review how to exploit these properties for parallel and streaming computation.
Streaming.
In the streaming setting  we assume that points arrive one-by-one  but we do not have
enough memory to remember the entire data set. Thus  we wish to maintain a coreset over time 
while keeping only a small subset of O(log n) coresets in memory. There is a general reduction that
shows that a small coreset scheme to a given problem sufﬁces to solve the corresponding problem
on a streaming input [7  6]. The idea is to construct and save in memory a coreset for every block of
poly(dk/ε) consecutive points arriving in a stream. When we have two coresets in memory  we can
merge them (resulting in a (k  ε)-coreset via property (1))  and compress by computing a single core-
set from the merged coresets (via property (2)) to avoid increase in the coreset size. An important
subtlety arises: While merging two coresets (via property (1)) does not increase the approximation
error  compressing a coreset (via property (2)) does increase the error. A naive approach that merges
and compresses immediately as soon as two coresets have been constructed  can incur an exponen-
tial increase in approximation error. Fortunately  it is possible to organize the merge-and-compress
operations in a binary tree of height O(log n)  where we need to store in memory a single coreset

5

x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 C1	  C2	  C3	  C4	  C5	  C6	  C7	  for each level on the tree (thus requiring only poly(dkε−1 log n) memory). Figure 2(b) illustrates
this tree computation. In order to construct a coreset for the union of two (weighted) coresets  we
use a weighted version of Algorithm 1  where we consider a weighted point as duplicate copies of
a non-weighted point (possibly with fractional weight). A more formal description can be found
in [8]. We summarize our streaming result in the following theorem.
Theorem 3.3. A (k  ε)-coreset for a stream of n points in Rd can be computed for the ε-
semi-spherical GMM problem with probability at least 1 − δ using space and update time
poly(dkε−1 log n log(1/δ)).
Parallel/Distributed computations. Using the same ideas from the streaming model  a (non-
parallel) coreset construction can be transformed into a parallel one. We partition the data into
sets  and compute coresets for each set  independently  on different computers in a cluster. We then
(in parallel) merge (via property (1)) two coresets  and compute a single coreset for every pair of
such coresets (via property (2)). Continuing in this manner yields a process that takes O(log n)
iterations of parallel computation. This computation is also naturally suited for map-reduce [9] style
computations  where the map tasks compute coresets for disjoint parts of D  and the reduce tasks
perform the merge-and-compress operations. Figure 2(b) illustrates this parallel construction.
Theorem 3.4. A (k  ε)-coreset for a set of n points in Rd can be computed for the ε-semi-
spherical GMM problem with probability at least 1 − δ using m machines in time (n/m) ·
poly(dkε−1 log(1/δ) log n).
3.3 Fitting a GMM on the Coreset using Weighted EM
One approach  which we employ in our experiments  is to use a natural generalization of the EM
algorithm  which takes the coreset weights into account. We here describe the algorithm for the case
of GMMs. For other mixture distributions  the E and M steps are modiﬁed appropriately.

Algorithm 2: Weighted EM for Gaussian mixtures
Input: Coreset C  k  TOL
Output: Mixture model θC
Lold = ∞; Initialize means µ1  . . .   µk by sampling k points from C with probability proportional to their
weight. Initialize Σi = I and wi = 1
repeat

k for all i;

Lold = L(C | θ); for j = 1 to n do for i = 1 to k do Compute ηi j = γi
for i = 1 to k do

wi ← wi/(cid:80)

(cid:96) wi; µi ←(cid:80)

j ηi j

j − µi

j/(cid:80)

j ηi j; Σi ←(cid:80)

j ηi jx(cid:48)

until L(C | θ) ≥ Lold − T OL ;

(cid:0)x(cid:48)

(cid:80)
wiN (x(cid:48)
(cid:1)(cid:0)x(cid:48)
(cid:96) w(cid:96)N (x(cid:48)
j − µi

j ;µi Σi)
j ;µ(cid:96) Σ(cid:96)) ;

(cid:1)T /(cid:80)

j ηi j;

Using a similar analysis as for the standard EM algorithm  Algorithm 2 is guaranteed to converge 
but only to a local optimum. However  since it is applied on a much smaller set  it can be initialized
using multiple random restarts.
4 Extensions and Generalizations
We now show how the connection between estimating the parameters for mixture models and
problems in computational geometry can be leveraged further. Our observations are based on the
link between mixture of Gaussians and projective clustering (multiple subspace approximation) as
shown in Lemma 3.2.
Generalizations to non-semi-spherical GMMs. For simplicity  we generalized the coreset con-
struction for the k-means problem  which required assumptions that the Gaussians are ε-semi-
spherical. However  several more complex coresets for projective clustering were suggested recently
(cf.  [4]). Using the tools developed in this article  each such coreset implies a corresponding coreset
for GMMs and generalizations. As an example  the coresets for approximating points by lines [10]
implies that we can construct small coresets for GMMs even if the smallest singular value of one of
the corresponding covariance matrices is zero.
Generalizations to (cid:96)q distances and other norms. Our analysis is based on combinatorics (such
as the complexity of sub-levelsets of GMMs) and probabilistic methods (non-uniform random sam-
pling). Therefore  generalizations to other non-Euclidean distance functions  or error functions such
as (non-squared) distances (mixture of Laplace distributions) is straightforward. The main property

6

(a) MNIST

(b) Tetrode recordings

(c) CSN data

(d) CSN detection

Figure 3: Experimental results for three real data sets. We compare likelihood of the best model obtained on
subsets C constructed by uniform sampling  and by the adaptive coreset sampling procedure.

that we need is a generalization of the triangle inequality  as proved in the supplemental material.
For example  replacing the squared distances by non-squared distances yields a coreset for mixture
of Laplace distributions. The double triangle inequality (cid:107)a − c(cid:107)2 ≤ 2((cid:107)a − b(cid:107) + (cid:107)b − c(cid:107)2) that we
used in this paper is replaced by H¨older’s inequality  (cid:107)a − c(cid:107)2 ≤ 2O(q) (cid:107)a − b(cid:107) + 2(cid:107)b − c(cid:107)2. Such
a result is straight-forward from our analysis  and we summarize it in the following theorem.
Theorem 4.1. Let q ≥ 1 be an integer. Consider Algorithm 1  where dist(· ·)2 is replaced by
dist(· ·)q and ε2 is replaced by εO(q). Suppose C is sampled from D using this updated version of
Algorithm 1 for parameters ε  δ and k. Then  with prob. at least 1 − δ it holds that for all θ ∈ Cε 

φ(D | θ)(1 − ε) ≤ φ(C | θ) ≤ φ(D | θ)(1 + ε) 

where Z(θ) =(cid:80)
using the normalizer g(θi) =(cid:82) exp

g(θi) and φ(D | θ) = −(cid:80)
(cid:13)(cid:13)(cid:13)Σ
(cid:16)− 1

wi

2

i

x∈D ln(cid:80)k

−1/2
i

(x − µi)

wi

Z(θ)g(θi) exp

i=1

(cid:13)(cid:13)(cid:13)q(cid:17)

dx.

(cid:16)− 1

2

(cid:13)(cid:13)(cid:13)Σ

(cid:13)(cid:13)(cid:13)q(cid:17)

−1/2
i

(x − µi)

5 Experiments
We experimentally evaluate the effectiveness of using coresets of different sizes for training mixture
models. We compare against running EM on the full set  as well as on an unweighted  uniform
sample from D. Results are presented for three real datasets.
MNIST handwritten digits. The MNIST dataset contains 60 000 training and 10 000 testing
grayscale images of handwritten digits. As in [11]  we normalize each component of the data to
have zero mean and unit variance  and then reduce each 784-pixel (28x28) image using PCA  retain-
ing only the top d = 100 principal components as a feature vector. From the training set  we produce
coresets and uniformly sampled subsets of sizes between 30 and 5000  using the parameters k = 10
(a cluster for each digit)  β = 20 and δ = 0.1 (see Algorithm 1)  and ﬁt GMMs using EM with 3
random restarts. The log likelihood (LLH) of each model on the testing data is shown in Figure 3(a).
Notice that coresets signiﬁcantly outperform uniform samples of the same size  and even a coreset
of 30 points performs very well. Further note how the test-log likelihood begins to ﬂatten out for
|C| = 1000. Constructing the coreset and running EM on this size takes 7.9 seconds (Intel Xeon 2.6
GHz)  over 100 times faster than running EM on the full set (15 minutes).
Neural tetrode recordings. We also compare coresets and uniform sampling on a large dataset
containing 319 209 records of rat hippocampal action potentials  measured by four co-located elec-
trodes. As done by [11]  we concatenate the 38-sample waveforms produced by each electrode to
obtain a 152-dimensional vector. The vectors are normalized so each component has zero mean
and unit variance. The 319 209 records are divided in half to obtain training and testing sets. From
the training set  we produce coresets and uniformly sampled subsets of sizes between 70 and 1000 
using the parameters k = 33 (as in [11])  β = 66  and δ = 0.1  and ﬁt GMMs. The log likelihood of
each model on the held-out testing data is shown in Figure 3(b). Coreset GMMs obtain consistently
higher LLH than uniform sample GMMs for sets of the same size  and even a coreset of 100 points
performs very well. Overall  training on coresets achieves approximately the same likelihood as
training on the full set about 95 times faster (1.2 minutes vs. 1.9 hours).
CSN cell phone accelerometer data. Smart phones with accelerometers are being used by the
Community Seismic Network (CSN) as inexpensive seismometers for earthquake detection. In [12] 
7 GB of acceleration data were recorded from volunteers while carrying and operating their phone in
normal conditions (walking  talking  on desk  etc.). From this data  17-dimensional feature vectors
were computed (containing frequency information  moments  etc.). The goal is to train  in an online

7

101102103104105444546474849505152Training Set SizeLog Likelihood on Test Data Set  Full SetUniformSampleCoreset102103104105−1800−1600−1400−1200−1000−800−600−400−2000Training Set SizeLog Likelihood on Test Data Set  UniformSampleCoresetFull Set101102103104105−250−200−150−100−500Training Set SizeLog Likelihood on Test Data Set  UniformSampleFull SetCoreset1011021031041050.550.60.650.70.75Training Set SizeArea Under ROC CurveUniformSampleFull SetCoresetfashion  GMMs based on normal data  which then can be used to perform anomaly detection to de-
tect possible seismic activity. Motivated by the limited storage on smart phones  we evaluate coresets
on a data set of 40 000 accelerometer feature vectors  using the parameters k = 6  β = 12  and δ =
0.1. Figure 3(c) presents the results of this experiment. Notice that on this data set  coresets show an
even larger improvement over uniform sampling. We hypothesize that this is due to the fact that the
recorded accelerometer data is imbalanced  and contains clusters of vastly varying size  so uniform
sampling does not represent smaller clusters well. Overall  the coresets obtain a speedup of approx-
imately 35 compared to training on the full set. We also evaluate how GMMs trained on the coreset
compare with the baseline GMMs in terms of anomaly detection performance. For each GMM  we
compute ROC curves measuring the performance of detecting earthquake recordings from the South-
ern California Seismic Network (cf.  [12]). Note that even very small coresets lead to performance
comparable to training on the full set  drastically outperforming uniform sampling (Fig. 3(d)).
6 Related Work
Theoretical results on mixtures of Gaussians. There has been a signiﬁcant amount of work on
learning and applying GMMs (and more general distributions). Perhaps the most commonly used
technique in practice is the EM algorithm [13]  which is however only guaranteed to converge to a
local optimum of the likelihood. Dasgupta [14] is the ﬁrst to show that parameters of an unknown
GMM P can be estimated in polynomial time  with arbitrary accuracy ε  given i.i.d. samples from
P . However  his algorithm assumes a common covariance  bounded excentricity  a (known) bound
on the smallest component weight  as well as a separation (distance of the means)  that scales as
d). Subsequent works relax the assumption on separation to d1/4 [15] and k1/4 [16]. [3] is
Ω(
the ﬁrst to learn general GMMs  with separation d1/4. [17] provides the ﬁrst result that does not
require any separation  but assumes that the Gaussians are axis-aligned. Recently  [18] and [19]
provide algorithms with polynomial running time (except exponential dependence on k) and sample
complexity for arbitrary GMMs. However  in contrast to our results  all the results described above
crucially rely on the fact that the data set D is actually generated by a mixture of Gaussians. The
problem of ﬁtting a mixture model with near-optimal log-likelihood for arbitrary data is studied
by [3]  who provides a PTAS for this problem. However  their result requires that the Gaussians
are identical spheres  in which case the maximum likelihood problem is identical to the k-means
problem.
In contrast  our results make only mild assumptions about the Gaussian components.
Furthermore  none of the algorithms described above applies to the streaming or parallel setting.

√

Coresets. Approximation algorithms in computational geometry often make use of random sam-
pling  feature extraction  and -samples [20]. Coresets can be viewed as a general concept that
includes all of the above  and more. See a comprehensive survey on this topic in [4]. It is not clear
that there is any commonly agreed-upon deﬁnition of a coreset  despite several inconsistent attempts
to do so [6  8]. Coresets have been the subject of many recent papers and several surveys [1  2]. They
have been used to great effect for a host of geometric and graph problems  including k-median [6] 
k-mean [8]  k-center [21]  k-line median [10] subspace approximation [10  22]  etc. Coresets also
imply streaming algorithms for many of these problems [6  1  23  8]. A framework that generalizes
and improves several of these results has recently appeared in [4].
7 Conclusion
We have shown how to construct coresets for estimating parameters of GMMs and natural general-
izations. Our construction hinges on a natural connection between statistical estimation and cluster-
ing problems in computational geometry. To our knowledge  our results provide the ﬁrst rigorous
guarantees for obtaining compressed ε-approximations of the log-likelihood of mixture models for
large data sets. The coreset construction relies on an intuitive adaptive sampling scheme  and can be
easily implemented. By exploiting certain closure properties of coresets  it is possible to construct
them in parallel  or in a single pass through a stream of data  using only poly(dkε−1 log n log(1/δ))
space and update time. Unlike most of the related work  our coresets provide guarantees for any
given (possibly unstructured) data  without assumptions on the distribution or model that generated
it. Lastly  we apply our construction on three real data sets  demonstrating signiﬁcant gains over no
or naive subsampling.

Acknowledgments This research was partially supported by ONR grant N00014-09-1-1044  NSF
grants CNS-0932392  IIS-0953413 and DARPA MSEE grant FA8650-11-1-7156.

8

References
[1] P. K. Agarwal  S. Har-Peled  and K. R. Varadarajan. Geometric approximations via coresets. Combina-

torial and Computational Geometry - MSRI Publications  52:1–30  2005.

[2] A. Czumaj and C. Sohler. Sublinear-time approximation algorithms for clustering via random sampling.

Random Struct. Algorithms (RSA)  30(1-2):226–256  2007.

[3] Sanjeev Arora and Ravi Kannan. Learning mixtures of separated nonspherical gaussians. Annals of

Applied Probability  15(1A):69–92  2005.

[4] D. Feldman and M. Langberg. A uniﬁed framework for approximating and clustering data. In Proc. 41th

Annu. ACM Symp. on Theory of Computing (STOC)  2011.

[5] S. Har-Peled and A. Kushal. Smaller coresets for k-median and k-means clustering. Discrete & Compu-

tational Geometry  37(1):3–19  2007.

[6] S. Har-Peled and S. Mazumdar. On coresets for k-means and k-median clustering. In Proc. 36th Annu.

ACM Symp. on Theory of Computing (STOC)  pages 291–300  2004.

[7] Jon Louis Bentley and James B. Saxe. Decomposable searching problems i: Static-to-dynamic transfor-

mation. J. Algorithms  1(4):301–358  1980.

[8] D. Feldman  M. Monemizadeh  and C. Sohler. A PTAS for k-means clustering based on weak coresets.

In Proc. 23rd ACM Symp. on Computational Geometry (SoCG)  pages 11–18  2007.

[9] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: Simpliﬁed data processing on large clusters.

OSDI’04: Sixth Symposium on Operating System Design and Implementation  2004.

In

[10] D. Feldman  A. Fiat  and M. Sharir. Coresets for weighted facilities and their applications. In Proc. 47th

IEEE Annu. Symp. on Foundations of Computer Science (FOCS)  pages 315–324  2006.

[11] Ryan Gomes  Andreas Krause  and Pietro Perona. Discriminative clustering by regularized information

maximization. In Proc. Neural Information Processing Systems (NIPS)  2010.

[12] Matthew Faulkner  Michael Olson  Rishi Chandy  Jonathan Krause  K. Mani Chandy  and Andreas
Krause. The next big one: Detecting earthquakes and other rare events from community-based sensors.
In In Proc. ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN) 
2011.

[13] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via the em

algorithm. J. Roy. Statist. Soc. Ser. B  39:1–38  1977.

[14] S. Dasgupta. Learning mixtures of gaussians. In Fortieth Annual IEEE Symposium on Foundations of

Computer Science (FOCS)  1999.

[15] S. Dasgupta and L.J. Schulman. A two-round variant of em for gaussian mixtures. In Sixteenth Conference

on Uncertainty in Artiﬁcial Intelligence (UAI)  2000.

[16] S. Vempala and G. Wang. A spectral algorithm for learning mixture models. In In Proceedings of the

43rd Annual IEEE Symposium on Foundations of Computer Science  2002.

[17] J. Feldman  R. A. Servedio  and R. O’Donnell. Pac learning axis-aligned mixtures of gaussians with no

separation assumption. In COLT  2006.

[18] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians.

Foundations of Computer Science (FOCS)  2010.

In In Proc.

[19] M. Belkin and K. Sinha. Polynomial learning of distribution families. In In Proc. Foundations of Com-

puter Science (FOCS)  2010.

[20] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning appli-

cations. Inf. Comput.  100(1):78–150  1992.

[21] S. Har-Peled and K. R. Varadarajan. High-dimensional shape ﬁtting in linear time. Discrete & Computa-

tional Geometry  32(2):269–288  2004.

[22] M.W. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proceedings of

the National Academy of Sciences  106(3):697  2009.

[23] G. Frahling and C. Sohler. Coresets in dynamic geometric data streams. In Proc. 37th Annu. ACM Symp.

on Theory of Computing (STOC)  pages 209–217  2005.

9

,Lucas Theis
Matthias Bethge