2017,Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization,We develop a new accelerated stochastic gradient method for efficiently solving the convex regularized empirical risk minimization problem in mini-batch settings. The use of mini-batches has become a golden standard in the machine learning community  because the mini-batch techniques stabilize the gradient estimate and can easily make good use of parallel computing. The core of our proposed method is the incorporation of our new ``double acceleration'' technique and variance reduction technique. We theoretically analyze our proposed method and show that our method much improves the mini-batch efficiencies of previous accelerated stochastic methods  and essentially only needs size $\sqrt{n}$ mini-batches for achieving the optimal iteration complexities for both non-strongly and strongly convex objectives  where $n$ is the training set size. Further  we show that even in non-mini-batch settings  our method achieves the best known convergence rate for non-strongly convex and strongly convex objectives.,Doubly Accelerated

Stochastic Variance Reduced Dual Averaging Method

for Regularized Empirical Risk Minimization

Tomoya Murata

NTT DATA Mathematical Systems Inc.   Tokyo  Japan

murata@msi.co.jp

Graduate School of Information Science and Technology  The University of Tokyo  Tokyo  Japan

PRESTO  Japan Science and Technology Agency  Japan

Center for Advanced Integrated Intelligence Research  RIKEN  Tokyo  Japan

Taiji Suzuki

Department of Mathematical Informatics

taiji@mist.i.u-tokyo.ac.jp

Abstract

We develop a new accelerated stochastic gradient method for efﬁciently solving
the convex regularized empirical risk minimization problem in mini-batch settings.
The use of mini-batches has become a golden standard in the machine learning
community  because the mini-batch techniques stabilize the gradient estimate and
can easily make good use of parallel computing. The core of our proposed method
is the incorporation of our new “double acceleration” technique and variance re-
duction technique. We theoretically analyze our proposed method and show that
our method much improves the mini-batch efﬁciencies of previous accelerated
stochastic methods  and essentially only needs size
n mini-batches for achieving
the optimal iteration complexities for both non-strongly and strongly convex objec-
tives  where n is the training set size. Further  we show that even in non-mini-batch
settings  our method achieves the best known convergence rate for non-strongly
convex and strongly convex objectives.

√

1

Introduction

We consider a composite convex optimization problem associated with regularized empirical risk
minimization  which often arises in machine learning. In particular  our goal is to minimize the
sum of ﬁnite smooth convex functions and a relatively simple (possibly) non-differentiable convex
function by using ﬁrst order methods in mini-batch settings. The use of mini-batches is now a
golden standard in the machine learning community  because it is generally more efﬁcient to execute
matrix-vector multiplications over a mini-batch than an equivalent amount of vector-vector ones each
over a single instance; and more importantly  mini-batch techniques can easily make good use of
parallel computing.
Traditional and effective methods for solving the abovementioned problem are the “proximal gradient”
(PG) method and “accelerated proximal gradient” (APG) method [10  3  20]. These methods are
well known to achieve linear convergence for strongly convex objectives. Particularly  APG achieves
optimal iteration complexities for both non-strongly and strongly convex objectives. However  these
methods need a per iteration cost of O(nd)  where n denotes the number of components of the
ﬁnite sum  and d is the dimension of the solution space. In typical machine learning tasks  n and d

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

correspond to the number of instances and features respectively  which can be very large. Then  the
per iteration cost of these methods can be considerably high.
A popular alternative is the “stochastic gradient descent” (SGD) method [19  5  17]. As the per
iteration cost of SGD is only O(d) in non-mini-batch settings  SGD is suitable for many machine
learning tasks. However  SGD only achieves sublinear rates and is ultimately slower than PG and
APG.
Recently  a number of stochastic gradient methods have been proposed; they use a variance reduction
technique that utilizes the ﬁnite sum structure of the problem (“stochastic averaged gradient” (SAG)
method [15  16]  “stochastic variance reduced gradient" (SVRG) method [6  22] and SAGA [4]).
Even though the per iteration costs of these methods are same as that of SGD  they achieve a linear
convergence for strongly convex objectives. Consequently  these methods dramatically improve the
total computational cost of PG. However  in size b mini-batch settings  the rate is essentially b times
worse than in non-mini-batch settings (the extreme situation is b = n which corresponds to PG). This
means that there is little beneﬁt in applying mini-batch scheme to these methods.
More recently  several authors have proposed accelerated stochastic methods for the composite ﬁnite
sum problem (“accelerated stochastic dual coordinate ascent” (ASDCA) method [18]  Universal
Catalyst (UC) [8]  “accelerated proximal coordinate gradient” (APCG) method [9]  “stochastic
primal-dual coordinate” (SPDC) method [23]  and Katyusha [1]). ASDCA (UC)  APCG  SPDC and
Katyusha essentially achieve the optimal total computational cost1 for strongly convex objectives2
in non-mini-batch settings. However  in size b mini-batch settings  the rate is essentially
b times
worse than that in non-mini-batch settings  and these methods need size O(n) mini-batches for
achieving the optimal iteration complexity3  which is essentially the same as APG. In addition 
[12  13] has proposed the “accelerated mini-batch proximal stochastic variance reduced gradient”
(AccProxSVRG) method and its variant  the “accelerated efﬁcient mini-batch stochastic variance
reduced gradient” (AMSVRG) method. In non-mini-batch settings  AccProxSVRG only achieves the
same rate as SVRG. However  in mini-batch settings  AccProxSVRG signiﬁcantly improves the mini-
batch efﬁciency of non-accelerated variance reduction methods  and surprisingly  AccProxSVRG
essentially only needs size O(
κ) mini-batches for achieving the optimal iteration complexity for
strongly convex objectives  where κ is the condition number of the problem. However  the necessary
size of mini-batches depends on the condition number and gradually increases when the condition
number increases and ultimately matches with O(n) for a large condition number.

√

√

Main contribution

We propose a new accelerated stochastic variance reduction method that achieves better convergence
than existing methods do  and it particularly takes advantages of mini-batch settings well; it is called
the “doubly accelerated stochastic variance reduced dual averaging” (DASVRDA) method. We
describe the main feature of our proposed method below and list the comparisons of our method with
several preceding methods in Table 1.

Our method signiﬁcantly improves the mini-batch efﬁciencies of state-of-the-art methods. As a
n) mini-batches4for achieving the optimal
result  our method essentially only needs size O(
iteration complexities for both non-strongly and strongly convex objectives.

√

1More precisely  the rate of ASDCA (UC) is with extra log-factors  and near but worse than the one of APCG 

SPDC and Katyusha. This means that ASDCA (UC) cannot be optimal.

tion method [11].

2Katyusha also achieves a near optimal total computational cost for non-strongly convex objectives.
3We refer to “optimal iteration complexity” as the iteration complexity of deterministic Nesterov’s accelera-

4Actually  when L/ε ≤ n and L/µ ≤ n  our method needs size O(n(cid:112)ε/L) and O(n(cid:112)µ/L) mini-batches 

√
respectively  which are larger than O(
n)  but smaller than O(n). Achieving optimal iteration complexity for
solving high accuracy and bad conditioned problems is much more important than doing ones with low accuracy
and well-conditioned ones  because the former needs more overall computational cost than the latter.

2

We regard one computation of a full gradient as n/b iterations in size b mini-batch settings  for
a fair comparison. “Unattainable” implies that the algorithm cannot achieve the optimal iteration

Table 1: Comparisons of our method with SVRG (SVRG++ [2])  ASDCA (UC)  APCG  SPDC 
Katyusha and AccProxSVRG. n is the number of components of the ﬁnite sum  d is the dimension of
the solution space  b is the mini-batch size  L is the smoothness parameter of the ﬁnite sum  µ is the
strong convexity parameter of objectives  and ε is accuracy. “Necessary mini-batch size” indicates
the order of the necessary size of mini-batches for achieving the optimal iteration complexities

O((cid:112)L/µlog(1/ε)) and O((cid:112)L/ε) for strongly and non-strongly convex objectives  respectively.
complexity even if it uses size n mini-batches. (cid:101)O hides extra log-factors.
(cid:1) + bL
(cid:1)(cid:1)
(cid:17)(cid:17)
(cid:19)(cid:19)
(cid:113) nbL
(cid:113) L

(cid:17)
(cid:1)(cid:17)
log(cid:0) 1
(cid:17)
(cid:1)(cid:17)
(cid:113) nbL
log(cid:0) 1
(cid:17)
(cid:1)(cid:17)
(cid:113) nbL
log(cid:0) 1
(cid:1)(cid:17)
(cid:17)
(cid:113) nbL
log(cid:0) 1
(cid:1)(cid:17)
(cid:17)
(cid:113) nbL
log(cid:0) 1
(cid:1)(cid:17)
(cid:17) L
(cid:17)
(cid:113) L
log(cid:0) 1
(cid:1)(cid:17)
(cid:17)
(cid:113) L
log(cid:0) 1

O(cid:0)d(cid:0)nlog(cid:0) 1
(cid:16)
(cid:16) n+
(cid:101)O
(cid:18)
(cid:18)
nlog(cid:0) 1
(cid:1) +
(cid:16)
(cid:1) + (b +
nlog(cid:0) 1

Necessary mini-batch size
L/µ ≥ n
otherwise
Unattainable Unattainable

(cid:16)
(cid:16)
(cid:16)
(cid:16)
(cid:16)
(cid:16) n−b

in size b mini-batch settings
O

No direct analysis
√

O(n)

O(cid:0)n(cid:112) µ
O(cid:0)n(cid:112) µ

Unattainable Unattainable

in size b mini-batch settings

L/ε ≥ nlog2(1/ε)

Necessary mini-batch size

Total computational cost

Total computational cost

O(n)

O(n)

(cid:16)(cid:113) L

(cid:16)
(cid:16)
(cid:16)
(cid:16)
(cid:16)

Non-strongly convex

AccProxSVRG O

µ-strongly convex

No direct analysis

No direct analysis

µ + b
√
n)

ASDCA (UC)

Unattainable

Unattainable

Unattainable

O

(cid:16)
(cid:16)

d

Unattainable

Unattainable

Unattainable

Unattainable

Unattainable

Unattainable

SVRG (++)

d

n + bL
µ

(cid:101)O

O

O

√
ε
nbL√

ε

n +

n +

n +

n +

O(n)

O(n)

DASVRDA

APCG

SPDC

(cid:16)

O

otherwise

Katyusha

(cid:17)(cid:17)

O

µ

√

O (

n)

d

n +

n−1

n + (b +

O(n)

Unattainable

(cid:101)O(cid:0)n(cid:112) ε

(cid:1)

L

(cid:1)
(cid:1)

L

L

√

O (

n)

d

d

d

d

O

d

(cid:16)

O

d

ε

n)

ε

ε

ε

ε

ε

ε

µ

ε

µ

ε

µ

µ

µ

µ

ε

ε

(cid:17)

ε

d

(cid:16)

O(n)

O(n)

2 Preliminary

In this section  we formally describe the problem to be considered in this paper and the assumptions
for our theory.

We use (cid:107) · (cid:107) to denote the Euclidean L2 norm (cid:107) · (cid:107)2: (cid:107)x(cid:107) = (cid:107)x(cid:107)2 =(cid:112)(cid:80)

i . For natural number n 

[n] denotes set {1  . . .   n}.
In this paper  we consider the following composite convex minimization problem:

i x2

(cid:80)n

{P (x) def= F (x) + R(x)} 

min
x∈Rd

i=1 fi(x). Here each fi

(1)
: Rd → R is a Li-smooth convex function and
where F (x) = 1
R : Rd → R is a relatively simple and (possibly) non-differentiable convex function. Problems
n
of this form often arise in machine learning and fall under regularized empirical risk minimization
(ERM). In ERM problems  we are given n training examples {(ai  bi)}n
i=1  where each ai ∈ Rd
is the feature vector of example i  and each bi ∈ R is the label of example i. Important examples
of ERM in our setting include linear regression and logistic regression with Elastic Net regularizer
R(x) = λ1(cid:107) · (cid:107)1 + (λ2/2)(cid:107) · (cid:107)2
We make the following assumptions for our analysis:
Assumption 1. There exists a minimizer x∗ of (1).
Assumption 2. Each fi is convex  and is Li-smooth  i.e. 

2 (λ1  λ2 ≥ 0).

(cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ Li(cid:107)x − y(cid:107) (∀x  y ∈ Rd).

2(cid:107)x − y(cid:107)2 + R(x)(cid:9)  takes
(cid:8) 1

Assumption 3. Regularization function R is convex  and is relatively simple  which means that
computing the proximal mapping of R at y  proxR(y) = argmin x∈Rd
O(d) computational cost  for any y ∈ Rd.
We always consider Assumptions 1  2 and 3 in this paper.
Assumption 4. There exists µ > 0 such that objective function P is µ-optimally strongly convex 
i.e.  P has a minimizer and satisﬁes

(cid:107)x − x∗(cid:107)2 ≤ P (x) − P (x∗)

(∀x ∈ Rd ∀x∗ ∈ argminx∈Rd f (x)).

µ
2

Note that the requirement of optimally strong convexity is weaker than the one of ordinary strong
convexity (for the deﬁnition of ordinary strong convexity  see [11]).
We further consider Assumption 4 when we deal with strongly convex objectives.

3

3 Our Approach: Double Acceleration

In this section  we provide high-level ideas of our main contribution called “double acceleration.”
First  we consider deterministic PG (Algorithm 1) and (non-mini-batch) SVRG (Algorithm 2). PG
is an extension of the steepest descent to proximal settings. SVRG is a stochastic gradient method
using the variance reduction technique  which utilizes the ﬁnite sum structure of the problem  and
it achieves a faster convergence rate than PG does. As SVRG (Algorithm 2) matches with PG
(Algorithm 1) when the number of inner iterations m equals 1  SVRG can be seen as a generalization
of PG. The key element of SVRG is employing a simple but powerful technique called the variance
setting gk = ∇fik (xk−1) − ∇fik ((cid:101)x) + ∇F ((cid:101)x) rather than vanilla stochastic gradient ∇fik (xk−1).
reduction technique for gradient estimate. The variance reduction of the gradient is realized by
Generally  stochastic gradient ∇fik (xk−1) is an unbiased estimator of ∇F (xk−1)  but it may have
is  the variance converges to zero as xk−1 and(cid:101)x to x∗.
high variance. In contrast  gk is also unbiased  and one can show that its variance is “reduced”; that
Algorithm 1: PG ((cid:101)x0  η  S)

Algorithm 3: One Stage PG ((cid:101)x  η)
(cid:101)x+ = proxηR((cid:101)x − η∇F ((cid:101)x)).
return (cid:101)x+.
Algorithm 4: One Stage SVRG ((cid:101)x  η  m)
x0 =(cid:101)x.
gk = ∇fik (xk−1) − ∇fik ((cid:101)x) + ∇F ((cid:101)x).
Pick ik ∈ [1  n] randomly.
xk = proxηR(xk−1 − ηgk).

for k = 1 to m do

(cid:80)n

end for
return 1
n

k=1 xk.

for s = 1 to S do

(cid:101)xs = One Stage PG((cid:101)xs−1  η).
Algorithm 2: SVRG ((cid:101)x0  η  m  S)

(cid:80)S
s=1(cid:101)xs.

end for
return 1
S

for s = 1 to S do

(cid:101)xs = One Stage SVRG((cid:101)xs−1  η  m).

end for
return 1
S

(cid:80)S
s=1(cid:101)xs.
Algorithm 5: APG ((cid:101)x0  η  S)
(cid:101)x−1 =(cid:101)x0 (cid:101)θ0 = 0.
(cid:101)θs = s+1
(cid:101)xs = One Stage PG((cid:101)ys  η).

for s = 1 to S do

2   (cid:101)ys =(cid:101)xs−1 + (cid:101)θs−1−1(cid:101)θs

end for
return xS.

((cid:101)xs−1 −(cid:101)xs−2).

Next  we explain the method of accelerating SVRG and obtaining an even faster convergence rate
based on our new but quite natural idea “outer acceleration.” First  we would like to remind you
that the procedure of deterministic APG is given as described in Algorithm 5. APG uses the famous
“momentum” scheme and achieves the optimal iteration complexity. Our natural idea is replacing
One Stage PG in Algorithm 5 with One Stage SVRG. With slight modiﬁcations  we can show
that this algorithm improves the rates of PG  SVRG and APG  and is optimal. We call this new
algorithm outerly accelerated SVRG. However  this algorithm has poor mini-batch efﬁciency  because
in size b mini-batch settings  the rate of this algorithm is essentially
b times worse than that of
non-mini-batch settings. State-of-the-art methods APCG  SPDC  and Katyusha also suffer from the
same problem in the mini-batch setting.
Now  we illustrate that for improving the mini-batch efﬁciency  using the “inner acceleration” tech-
nique is beneﬁcial. The author of [12] has proposed AccProxSVRG in mini-batch settings. Ac-
cProxSVRG applies the momentum scheme to One Stage SVRG  and we call this technique “inner”
acceleration. He showed that the inner acceleration could signiﬁcantly improve the mini-batch
efﬁciency of vanilla SVRG. This fact indicates that inner acceleration is essential to fully utilize
the mini-batch settings. However  AccProxSVRG is not a truly accelerated method  because in
non-mini-batch settings  the rate of AccProxSVRG is same as that of vanilla SVRG.
In this way  we arrive at our main high-level idea called “double” acceleration  which involves
applying momentum scheme to both outer and inner algorithms. This enables us not only to lead to

√

4

the optimal total computational cost in non-mini-batch settings  but also to improving the mini-batch
efﬁciency of vanilla acceleration methods.
We have considered SVRG and its accelerations so far; however  we actually adopt stochastic variance
reduced dual averaging (SVRDA) rather than SVRG itself  because we can construct lazy update
rules of (innerly) accelerated SVRDA for sparse data. In Section G of supplementary material  we
brieﬂy discuss a SVRG version of our proposed method and provide its convergence analysis.

4 Algorithm Description

In this section  we describe the concrete procedure of the proposed algorithm in detail.

n

i=1  m  b  S)

for s = 1 to S do

4.1 DASVRDA for non-strongly convex objectives

Algorithm 6: DASVRDAns((cid:101)x0 (cid:101)z0  γ {Li}n
i=1 Li  Q = {qi} =(cid:8) Li
(cid:9)  η =
(cid:80)n
(cid:101)x−1 =(cid:101)z0 (cid:101)θ0 = 0  ¯L = 1
(cid:17) s+2
(cid:16)
(cid:101)θs =
2   (cid:101)ys =(cid:101)xs−1 + (cid:101)θs−1−1(cid:101)θs
((cid:101)xs (cid:101)zs) = One Stage AccSVRDA((cid:101)ys (cid:101)xs−1  η  m  b  Q).
return (cid:101)xS.
Algorithm 7: One Stage AccSVRDA ((cid:101)y (cid:101)x  η  m  b  Q)
x0 = z0 =(cid:101)y  ¯g0 = 0  θ0 = 1

end for

1 − 1

2.

n ¯L

γ

1

(1+ γ(m+1)

.

((cid:101)xs−1 −(cid:101)xs−2) + (cid:101)θs−1(cid:101)θs

b

) ¯L

((cid:101)zs−1 −(cid:101)xs−1).

for k = 1 to m do

Pick independently i1
θk = k+1

2   yk =

(cid:80)

k  . . .   ib
1 − 1

k ∈ [1  n] according to Q  set Ik = {i(cid:96)
xk−1 + 1
θk

(cid:16)
(cid:17)
(∇fi(yk) − ∇fi((cid:101)x)) + ∇F ((cid:101)x)  ¯gk =
(cid:110)(cid:104)¯gk  z(cid:105) + R(z) +
(cid:107)z − z0(cid:107)2(cid:111)
(cid:17)

1 − 1

2ηθkθk−1

zk−1.

1
nqi

θk

1

(cid:96)=1.

(cid:16)
= proxηθkθk−1R (z0 − ηθkθk−1¯gk) .

¯gk−1 + 1
θk

gk.

θk

k}b
(cid:17)

gk = 1
b

i∈Ik
zk = argmin
z∈Rd
1 − 1

(cid:16)

xk =
end for
return (xm  zm).

θk

xk−1 + 1
θk

zk.

Algorithm 6 can be seen as a direct generalization of APG  because if we set m = 1  One Stage

We provide details of the doubly accelerated SVRDA (DASVRDA) method for non-strongly convex
objectives in Algorithm 6. Our momentum step is slightly different from that of vanilla deterministic

accelerated methods: we not only add momentum term (((cid:101)θs−1 − 1)/(cid:101)θs)((cid:101)xs−1 −(cid:101)xs−2) to the
current solution(cid:101)xs−1 but also add term ((cid:101)θs−1/(cid:101)θs)((cid:101)zs−1 −(cid:101)xs−1)  where(cid:101)zs−1 is the current more
“aggressively” updated solution rather than(cid:101)xs−1; thus  this term also can be interpreted as momentum5.
Then  we feed(cid:101)ys to One Stage Accelerated SVRDA (Algorithm 7) as an initial point. Note that
Accelerated SVRDA is essentially the same as one iteration PG with initial point(cid:101)ys; then  we can
see that(cid:101)zs = (cid:101)xs  and Algorithm 6 essentially matches with deterministic APG. Next  we move
reduced gradient gk  we use the full gradient of F at (cid:101)xs−1 rather than the initial point (cid:101)ys. The
5This form also arises in Monotone APG [7]. In Algorithm 7 (cid:101)x = xm can be rewritten as (2/(m(m +
1))(cid:80)m
k=1 kzk  which is a weighted average of zk; thus  we can say that(cid:101)z is updated more “aggressively” than
(cid:101)x. For the outerly accelerated SVRG (that is a combination of Algorithm 6 with vanilla SVRG  see section 3) (cid:101)z
and(cid:101)x correspond to xm and (1/m)(cid:80)m
k=1 xk in [22]  respectively. Thus  we can also see that(cid:101)z is updated more
“aggressively” than(cid:101)x.

to One Stage Accelerated SVRDA (Algorithm 7). Algorithm 7 is essentially a combination of the
“accelerated regularized dual averaging” (AccSDA) method [21] with the variance reduction technique
of SVRG. It updates zk by using the weighted average of all past variance reduced gradients ¯gk
instead of only using a single variance reduced gradient gk. Note that for constructing variance

5

Adoption of (Innerly) Accelerated SVRDA rather than (Innerly) Accelerated SVRG enables us to
construct lazy updates for sparse data (for more details  see Section E of supplementary material).

4.2 DASVRDA for strongly convex objectives
Algorithm 8: DASVRDAsc(ˇx0  γ {Li}n

i=1  m  b  S  T )

for t = 1 to T do

ˇxt = DASVRDAns(ˇxt−1  ˇxt−1  γ {Li}n

i=1  m  b  S).

end for
return ˇxT .

Algorithm 8 is our proposed method for strongly convex objectives. Instead of directly accelerating
the algorithm using a constant momentum rate  we restart Algorithm 6. Restarting scheme has several
advantages both theoretically and practically. First  the restarting scheme only requires the optimal
strong convexity of the objective instead of the ordinary strong convexity. Whereas  non-restarting
accelerated algorithms essentially require the ordinary strong convexity of the objective. Second 
for restarting algorithms  we can utilize adaptive restart schemes [14]. The adaptive restart schemes
have been originally proposed for deterministic cases. The schemes are heuristic but quite effective
empirically. The most fascinating property of these schemes is that we need not prespecify the
strong convexity parameter µ  and the algorithms adaptively determine the restart timings. [14]
have proposed two heuristic adaptive restart schemes: the function scheme and gradient scheme.
We can easily apply these ideas to our method  because our method is a direct generalization of

the deterministic APG. For the function scheme  we restart Algorithm 6 if P ((cid:101)xs) > P ((cid:101)xs−1). For
the gradient scheme  we restart the algorithm if ((cid:101)ys −(cid:101)xs)(cid:62)((cid:101)ys+1 −(cid:101)xs) > 0. Here(cid:101)ys −(cid:101)xs can be
interpreted as a “one stage” gradient mapping of P at(cid:101)ys. As(cid:101)ys+1 −(cid:101)xs is the momentum  this scheme

can be interpreted such that we restart whenever the momentum and negative one Stage gradient
mapping form an obtuse angle (this means that the momentum direction seems to be “bad”). We
numerically demonstrate the effectiveness of these schemes in Section 6.

Parameter tunings

For DASVRDAns  only learning rate η needs to be tuned  because we can theoretically obtain the
optimal choice of γ  and we can naturally use m = n/b as a default epoch length (see Section 5). For
DASVRDAsc  both learning rate η and ﬁxed restart interval S need to be tuned.

5 Convergence Analysis of DASVRDA Method

(cid:19)2

(cid:16)

(cid:17)2

4

1 − 1

γ

(S + 2)2

1 − 1
γ

i=1  m  b  S) satisﬁes

(P ((cid:101)x0) − P (x∗)) +

In this section  we provide the convergence analysis of our algorithms. Unless otherwise speciﬁed 
serial computation is assumed. First  we consider the DASVRDAns algorithm.

Theorem 5.1. Suppose that Assumptions 1  2 and 3 hold. Let(cid:101)x0 (cid:101)z0 ∈ Rd  γ ≥ 3  m ∈ N  b ∈ [n]
and S ∈ N. Then DASVRDAns((cid:101)x0 (cid:101)z0  γ {Li}n
(cid:33)
(cid:32)(cid:18)
E [P ((cid:101)xS) − P (x∗)] ≤
(cid:107)(cid:101)z0 − x∗(cid:107)2
the optimal choice of γ is (3+(cid:112)9 + 8b/(m + 1))/2 = O(1+b/m) (see Section B of supplementary
Corollary 5.2. Suppose that Assumptions 1  2  and 3 hold. Let(cid:101)x0 ∈ Rd  γ = γ∗   m ∝ n/b and b ∈
mb)(cid:112) ¯L(cid:107)(cid:101)x0 − x∗(cid:107)2/ε) 
[n]. If we appropriately choose S = O((cid:112)(P ((cid:101)x0) − P (x∗))/ε+(1/m+1/
then a total computational cost of DASVRDAns ((cid:101)x0  γ∗ {Li}n
i=1  m  b  S) for E [P ((cid:101)xS) − P (x∗)] ≤
(cid:33)(cid:33)
n(cid:1)(cid:114) ¯L(cid:107)(cid:101)x0 − x∗(cid:107)2

The proof of Theorem 5.1 is found in the supplementary material (Section A). We can easily see that

material). We denote this value as γ∗. From Theorem 5.1  we obtain the following corollary:

P ((cid:101)x0) − P (x∗)

+(cid:0)b +

(cid:32)

(cid:32)

(cid:114)

2

η(m + 1)m

O

d

n

ε is

√

.

ε

ε

.

√

6

Remark. If we adopt a warm start scheme for DASVRDAns  we can further improve the rate to

(cid:32)

(cid:32)

O

d

nlog

(cid:18) P ((cid:101)x0) − P (x∗)

(cid:19)

ε

(cid:114)

L(cid:107)(cid:101)x0 − x∗(cid:107)2

ε

(cid:33)(cid:33)

√

+ (b +

n)

(see Section C and D of supplementary material).

Next  we analyze the DASVRDAsc algorithm for optimally strongly convex objectives. Combining
Theorem 5.1 with the optimal strong convexity of the objective function immediately yields the
following theorem  which implies that the DASVRDAsc algorithm achieves a linear convergence.
Theorem 5.3. Suppose that Assumptions 1  2  3 and 4 hold. Let ˇx0 ∈ Rd  γ = γ∗  m ∈ N  b ∈ [n]
and T ∈ N. Deﬁne ρ def= 4{(1 − 1/γ∗)2 + 4/(η(m + 1)mµ)}/{(1 − 1/γ∗)2(S + 2)2}. If S is
sufﬁciently large such that ρ ∈ (0  1)  then DASVRDAsc(ˇx0  γ∗ {Li}n
E[P (ˇxT ) − P (x∗)] ≤ ρT [P (ˇx0) − P (x∗)].

i=1  m  b  S  T ) satisﬁes

O

√

From Theorem 5.3  we have the following corollary.
Corollary 5.4. Suppose that Assumptions 1  2  3 and 4 hold. Let ˇx0 ∈ Rd  γ = γ∗  m ∝ n/b 
√
b ∈ [n]. There exists S = O(1 + (b/n + 1/
if we appropriately choose T = O(log(P (ˇx0) − P (x∗)/ε)  then a total computational cost of
DASVRDAsc(ˇx0  γ∗ {Li}n

n)(cid:112) ¯L/µ)  such that 1/log(1/ρ) = O(1). Moreover 
 log
n(cid:1)(cid:115)

i=1  m  b  S  T ) for E [P (ˇxT ) − P (x∗)] ≤ ε is
(cid:18) P (ˇx0) − P (x∗)

n +(cid:0)b +

(cid:19) .

d

√
Remark. Corollary 5.4 also implies that DASVRDAsc only needs size O(

√
Remark. Corollary 5.4 implies that if the mini-batch size b is O(
{Li}n

i=1  n/b  b  S  T ) still achieves the total computational cost of O(d(n +(cid:112)n ¯L/µ)log(1/ε)) 
which is much better than O(d(n +(cid:112)nb ¯L/µ)log(1/ε)) of APCG  SPDC  and Katyusha.
achieving the optimal iteration complexity O((cid:112)L/µlog(1/ε))  when L/µ ≥ n. In contrast  APCG 
SPDC and Katyusha need size O(n) mini-batches and AccProxSVRG does O((cid:112)L/µ) ones for
size O(n(cid:112)µ/L) mini-batches 6. This size is smaller than O(n) of APCG  SPDC  and Katyusha  and

achieving the optimal iteration complexity. Note that even when L/µ ≤ n  our method only needs

n)  DASVRDAsc(ˇx0  γ∗ 

n) mini-batches for

¯L
µ

ε

the same as that of AccProxSVRG.

6 Numerical Experiments

In this section  we provide numerical experiments to demonstrate the performance of DASVRDA.
We numerically compare our method with several well-known stochastic gradient methods in mini-
batch settings: SVRG [22] (and SVRG++ [2])  AccProxSVRG [12]  Universal Catalyst [8]   APCG
[9]  and Katyusha [1]. The details of the implemented algorithms and their parameter tunings
are found in the supplementary material. In the experiments  we focus on the regularized logistic
regression problem for binary classiﬁcation  with regularizer λ1(cid:107) · (cid:107)1 + (λ2/2)(cid:107) · (cid:107)2
2.
We used three publicly available data sets in the experiments. Their sizes n and dimensions d  and
common min-batch sizes b for all implemented algorithms are listed in Table 2.

Table 2: Summary of the data sets and mini-batch size used in our numerical experiments

Data sets

a9a
rcv1
sido0

n

32  561
20  242
12  678

d
123

47  236
4  932

b

180
140
100

For regularization parameters  we used three settings (λ1  λ2) = (10−4  0)  (10−4  10−6)  and
(0  10−6). For the former case  the objective is non-strongly convex  and for the latter two cases 

6Note that the required size is O(n(cid:112)µ/L)(≤ O(n))  which is not O(n(cid:112)L/µ) ≥ O(n).

7

(a) a9a  (λ1  λ2) = (10−4  0)

(b) a9a  (λ1  λ2) = (10−4  10−6)

(c) a9a  (λ1  λ2) = (0  10−6)

(d) rcv1  (λ1  λ2) = (10−4  0)

(e) rcv1  (λ1  λ2) = (10−4  10−6)

(f) rcv1  (λ1  λ2) = (0  10−6)

(g) sido0  (λ1  λ2) = (10−4  0)

(h) sido0  (λ1  λ2) = (10−4  10−6)

(i) sido0  (λ1  λ2) = (0  10−6)

Figure 1: Comparisons on a9a (top)  rcv1 (middle) and sido0 (bottom) data sets  for regularization
parameters (λ1  λ2) = (10−4  0) (left)  (λ1  λ2) = (10−4  10−6) (middle) and (λ1  λ2) = (0  10−6)
(right).

the objectives are strongly convex. Note that for the latter two cases  the strong convexity of the
objectives is µ = 10−6 and is relatively small; thus  it makes acceleration methods beneﬁcial.
Figure 1 shows the comparisons of our method with the different methods described above on several
settings. “Objective Gap” means P (x) − P (x∗) for the output solution x. “Elapsed Time [sec]”
means the elapsed CPU time (sec). “Restart_DASVRDA” means DASVRDA with heuristic adaptive
restarting (Section 4). We can observe the following from these results:

• Our proposed DASVRDA and Restart DASVRDA signiﬁcantly outperformed all the other

methods overall.

• DASVRDA with the heuristic adaptive restart scheme efﬁciently made use of the local
strong convexities of non-strongly convex objectives and signiﬁcantly outperformed vanilla
DASVRDA. For the other settings  the algorithm was still comparable to vanilla DASVRDA.
• UC+AccProxSVRG7 outperformed vanilla AccProxSVRG but was outperformed by our

methods overall.

7Although there has been no theoretical guarantee for UC + AccProxSVRG  we thought that it was fair
to include experimental results about that because UC + AccProxSVRG gives better performances than the
vanilla AccProxSVRG. Through some theoretical analysis  we can prove that UC + AccProxSVRG also has the
similar rate and mini-batch efﬁciency to our proposed method  although these results are not obtained in any
literature. However  our proposed method is superior to this algorithm both theoretically and practically  because
the algorithm has several drawbacks due to the use of UC as follows. First  the algorithm has an additional
logarithmic factor in its convergence rate. This factor is generally not negligible and slows down its practical
performances. Second  the algorithm has more tuning parameters than our method. Third  the stopping criterion
of each sub-problem of UC is hard to be tuned.

8

• APCG sometimes performed unstably and was outperformed by vanilla SVRG. On sido0
• Katyusha always outperformed vanilla SVRG  but was signiﬁcantly outperformed by our

data set  for Ridge Setting  APCG signiﬁcantly outperformed all the other methods.

methods.

7 Conclusion

√

√

n)(cid:112)L/ε)) and
n)(cid:112)L/µ)log(1/ε)) in size b mini-batch settings for non-strongly and optimally

In this paper  we developed a new accelerated stochastic variance reduced gradient method for
regularized empirical risk minimization problems in mini-batch settings: DASVRDA. We have shown
that DASVRDA achieves the total computational costs of O(d(nlog(1/ε) + (b +
O(d(n + (b +
√
strongly convex objectives  respectively. In addition  DASVRDA essentially achieves the optimal
iteration complexities only with size O(
In the numerical
experiments  our method signiﬁcantly outperformed state-of-the-art methods  including Katyusha
and AccProxSVRG.

n) mini-batches for both settings.

Acknowledgment

This work was partially supported by MEXT kakenhi (25730013  25120012  26280009 and
15H05707)  JST-PRESTO and JST-CREST.

References
[1] Z. Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In 48th

Annual ACM Symposium on the Theory of Computing  pages 19–23  2017.

[2] Z. Allen-Zhu and Y. Yuan. Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex
Objectives. In Proceedings of the 33rd International Conference on Machine Learning  pages
1080–1089  2016.

[3] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[4] A. Defazio  F. Bach  and S. Lacoste-Julien. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems 27  pages 1646–1654  2014.

[5] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimiza-

tion. Machine Learning  69(2-3):169–192  2007.

[6] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems 26  pages 315–323  2013.

[7] H. Li and Z. Lin. Accelerated proximal gradient methods for nonconvex programming. In

Advances in Neural Information Processing Systems 28  pages 379–387  2015.

[8] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization.

Advances in Neural Information Processing Systems 28  pages 3384–3392  2015.

In

[9] Q. Lin  Z. Lu  and L. Xiao. An accelerated proximal coordinate gradient method. In Advances

in Neural Information Processing Systems 27  pages 3059–3067  2014.

[10] Y. Nesterov. Gradient methods for minimizing composite objective function. Mathematical

Programming  140(1):125–161  2013.

[11] Y. Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer

Science & Business Media  2013.

[12] A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. In Advances in

Neural Information Processing Systems 27  pages 1574–1582  2014.

9

[13] A. Nitanda. Accelerated stochastic gradient descent for minimizing ﬁnite sums. In Proceedings
of the 19th International Conference on Artiﬁcial Intelligence and Statistics  pages 195–203 
2016.

[14] B. O’Donoghue and E. Candes. Adaptive restart for accelerated gradient schemes. Foundations

of computational mathematics  15(3):715–732  2015.

[15] N. L. Roux  M. Schmidt  and F. R. Bach. A stochastic gradient method with an exponential
convergence _rate for ﬁnite training sets. In Advances in Neural Information Processing Systems
25  pages 2663–2671  2012.

[16] M. Schmidt  N. L. Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming  162(1):83–112  2017.

[17] S. Shalev-Shwartz and Y. Singer. Logarithmic regret algorithms for strongly convex repeated

games. Technical report  The Hebrew University  2007.

[18] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss. The Journal of Machine Learning Research  14(1):567–599  2013.

[19] Y. Singer and J. C. Duchi. Efﬁcient learning using forward-backward splitting. In Advances in

Neural Information Processing Systems 22  pages 495–503  2009.

[20] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Technical

report  University of Washington  Seattle  2008.

[21] L. Xiao. Dual averaging method for regularized stochastic learning and online optimization. In

Advances in Neural Information Processing Systems 22  pages 2116–2124  2009.

[22] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[23] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical
risk minimization. In Proceedings of the 32nd International Conference on Machine Learning 
pages 353–361  2015.

10

,Daniel Hernández-Lobato
José Miguel Hernández-Lobato
Tomoya Murata
Taiji Suzuki
Shuyang Sun
Jiangmiao Pang
Jianping Shi
Shuai Yi
Wanli Ouyang