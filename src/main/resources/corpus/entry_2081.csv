2012,Feature Clustering for Accelerating Parallel Coordinate Descent,Large scale $\ell_1$-regularized loss minimization problems arise in numerous applications such as compressed sensing and high dimensional supervised learning  including classification and regression problems.  High performance algorithms and implementations are critical to efficiently solving these problems.  Building upon previous work on coordinate descent algorithms for $\ell_1$ regularized problems  we introduce a novel family of algorithms called block-greedy coordinate descent that includes  as special cases  several existing algorithms such as SCD  Greedy CD  Shotgun  and Thread-greedy.  We give a unified convergence analysis for the family of block-greedy algorithms.  The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.  Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.  We hope that algorithmic approaches and convergence analysis we provide will not only advance the field  but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\ell_1$-regularization problems.,Feature Clustering for Accelerating

Parallel Coordinate Descent

Chad Scherrer

Independent Consultant

Yakima  WA

chad.scherrer@gmail.com

Ambuj Tewari

Department of Statistics
University of Michigan

Ann Arbor  MI

tewaria@umich.edu

Mahantesh Halappanavar

Paciﬁc Northwest National Laboratory

Richland  WA

Paciﬁc Northwest National Laboratory

David J Haglin

Richland  WA

mahantesh.halappanavar@pnnl.gov

david.haglin@pnnl.gov

Abstract

Large-scale `1-regularized loss minimization problems arise in high-dimensional
applications such as compressed sensing and high-dimensional supervised learn-
ing  including classiﬁcation and regression problems. High-performance algo-
rithms and implementations are critical to efﬁciently solving these problems.
Building upon previous work on coordinate descent algorithms for `1-regularized
problems  we introduce a novel family of algorithms called block-greedy coor-
dinate descent that includes  as special cases  several existing algorithms such as
SCD  Greedy CD  Shotgun  and Thread-Greedy. We give a uniﬁed convergence
analysis for the family of block-greedy algorithms. The analysis suggests that
block-greedy coordinate descent can better exploit parallelism if features are clus-
tered so that the maximum inner product between features in different blocks is
small. Our theoretical convergence analysis is supported with experimental re-
sults using data from diverse real-world applications. We hope that algorithmic
approaches and convergence analysis we provide will not only advance the ﬁeld 
but will also encourage researchers to systematically explore the design space of
algorithms for solving large-scale `1-regularization problems.

1

Introduction

Consider the `1-regularized loss minimization problem

min
w

1
n

nXi=1

`(yi  (Xw)i) + kwk1  

(1)

where X 2 IRn⇥p is the design matrix  w 2 IRp is a weight vector to be estimated  and the
loss function ` is such that `(y ·) is a convex differentiable function for each y. This formulation
includes `1-regularized least squares (Lasso) (when `(y  t) = 1
2 (y  t)2) and `1-regularized logistic
regression (when `(y  t) = log(1+exp(yt))). In recent years  coordinate descent (CD) algorithms
have been shown to be efﬁcient for this class of problems [Friedman et al.  2007; Wu and Lange 
2008; Shalev-Shwartz and Tewari  2011; Bradley et al.  2011].
Motivated by the need to solve large scale `1 regularized problems  researchers have begun to ex-
plore parallel algorithms. For instance  Bradley et al. [2011] developed the Shotgun algorithm.
More recently  Scherrer et al. [2012] have developed “GenCD”  a generic framework for expressing

1

parallel coordinate descent algorithms. Special cases of GenCD include Greedy CD [Li and Osher 
2009; Dhillon et al.  2011]  the Shotgun algorithm of [Bradley et al.  2011]  and Thread-Greedy CD
[Scherrer et al.  2012].
In fact  the connection between these three special cases of GenCD is much deeper  and more fun-
damental  than is obvious under the GenCD abstraction. As our ﬁrst contribution  we describe a
general randomized block-greedy that includes all three as special cases. The block-greedy algo-
rithm has two parameter: B  the total number of feature blocks and P   the size of the random subset
of the B blocks that is chosen at every time step. For each of these P blocks  we greedily choose  in
parallel  a single feature weight to be updated.
Second  we present a non-asymptotic convergence rate analysis for the randomized block-greedy
coordinate descent algorithms for general values of B 2{ 1  . . .   p} (as the number of blocks cannot
exceed the number of features) and P 2{ 1  . . .   B}. This result therefore applies to stochastic CD 
greedy CD  Shotgun  and thread-greedy. Indeed  we build on the analysis and insights in all of these
previous works. Our general convergence result  and in particular its instantiation to thread-greedy
CD  is novel.
Third  based on the convergence rate analysis for block-greedy  we optimize a certain “block spectral
radius” associated with the design matrix. This parameter is a direct generalization of a similar
spectral parameter that appears in the analysis of Shotgun. We show that the block spectral radius
can be upper bounded by the maximum inner product (or correlation if features are mean zero)
between features in distinct blocks. This motivates the use of correlation-based feature clustering to
accelerate the convergence of the thread-greedy algorithm.
Finally  we conduct an experimental study using a simple clustering heuristic. We observe dramatic
acceleration due to clustering for smaller values of the regularization parameter  and show charac-
teristics that must be paid particularly close attention for heavily regularized problems  and that can
be improved upon in future work.

2 Block-Greedy Coordinate Descent

Scherrer et al. [2012] describe “GenCD”  a generic framework for parallel coordinate descent algo-
rithms  in which a parallel coordinate descent algorithm can be determined by specifying a select
step and an accept step. At each iteration  features chosen by select are evaluated  and a proposed
increment is generated for each corresponding feature weight. Using this  the accept step then de-
termines which proposals are to be updated.
In these terms  we consider the block-greedy algorithm that takes as
part of the input a partition of the features into B blocks. Given this 
each iteration selects features corresponding to a set of P randomly
selected blocks  and accepts a single feature from each block  based
on an estimate of the resulting reduction in the objective function.
The pseudocode for the randomized block-greedy coordinate descent
is given by Algorithm 1. The algorithm can be applied to any function
of the form F + R where F is smooth and convex  and R is convex
and separable across coordinates. Our objective function (1) satisﬁes
these conditions. The greedy step chooses a feature within a block for
which the guaranteed descent in the objective function (if that feature
alone were updated) is maximized. This descent is quantiﬁed by |⌘j| 
which is deﬁned precisely in the next section. To arrive at an heuristic
understanding  it is best to think of |⌘j| as being proportional to the
absolute value |rjF (w)| of the jth entry in the gradient of the smooth part F . In fact  if R is zero
(no regularization) then this heuristic is exact.
The two parameters  B and P   of the block-greedy CD algorithm have the ranges B 2{ 1  . . .   p}
and P 2{ 1  . . .   B}. Setting these to speciﬁc values gives many existing algorithms. For instance
when B = p  each feature is a block on its own. Then  setting P = 1 amounts to randomly choosing
a single coordinate and updating it which gives us the stochastic CD algorithm of Shalev-Shwartz
and Tewari [2011]. Shotgun [Bradley et al.  2011] is obtained when B is still p but P  1. Another

Figure 1: The design space
of block-greedy coordinate
descent.

2

Algorithm 1 Block-Greedy Coordinate Descent

Parameters: B (no. of blocks) and P  B (degree of parallelism)
while not converged do

Select a random subset of size P from the B available blocks
Set J to be the features in the selected blocks
Propose increment ⌘j  j 2 J
Accept J0 = {j : ⌘j has maximal absolute value in its block}
Update weight wj wj  ⌘j for all j 2 J0

// parallel

// parallel

extreme is the case when all the features constitute a single block. That is  B = 1. Then block-
greedy CD is a deterministic algorithm and becomes the greedy CD algorithm of Li and Osher
[2009]; Dhillon et al. [2011]. Finally  we can choose non-trivial values of B that lie strictly between
1 and p. When this is the case  and we choose to update all blocks in parallel each time (P = B) 
we arrive at the thread-greedy algorithm of Scherrer et al. [2012]. Figure 1 shows a schematic
representation of the parameterization of these special cases.

3 Convergence Analysis

Of course  there is no reason to expect block-greedy CD to converge for all values of B and P . In
this section  we give a sufﬁcient condition for convergence and derive a convergence rate assuming
this condition.
Bradley et al. express the convergence criteria for Shotgun algorithm in terms of the spectral ra-
dius (maximal eigenvalue) ⇢(XT X). For block-greedy  the corresponding quantity is a bit more
complicated. We deﬁne

⇢block = max
M2M

⇢(M )

where M is the set of all B ⇥ B submatrices that we can obtain from XT X by selecting exactly one
index from each of the B blocks. The intuition is that if features from different blocks are almost
orthogonal then the matrices M in M will be close to identity and will therefore have small ⇢(M ).
Highly correlated features within a block do not increase ⇢block.
As we said above  we will assume that we are minimizing a “smooth plus separable” convex function
F + R where the convex differentiable function F : Rp ! R satisﬁes a second order upper bound

F (w +)  F (w) + rF (w)T +


2

T XT X

In our case  this inequality will hold as soon as `00(y  t)   for any y  t (where differentiation is
w.r.t. t). The function R is separable across coordinates: R(w) = Pp
j=1 r(wj). The function
kwk1 is clearly separable.
The quantity ⌘j appearing in Algorithm 1 serves to quantify the guaranteed descent (based on second
order upper bound) if feature j alone is updated and is obtained as a solution of the one-dimensional
minimization problem

⌘j = argmin

⌘

rjF (w)⌘ +


2

⌘2 + r(wj + ⌘)  r(wj) .

Note that if there is no regularization  then ⌘j is simply rjF (w)/ = gj/ (if we denote
rjF (w) by gj for brevity). In the general case  by ﬁrst order optimality conditions for the above
one-dimensional convex optimization problem  we have gj +⌘j +⌫j = 0 where ⌫j is a subgradient
of r at wj + ⌘j. That is  ⌫j 2 @r(wj + ⌘j). This implies that r(wj + ⌘j) r(w0)  ⌫j(wj + ⌘j  w0)
for any w0.
Theorem 1. Let P be chosen so that

✏ =

(P  1)(⇢block  1)

(B  1)
3

is less than 1. Suppose the randomized block-greedy coordinate algorithm is run on a smooth plus
separable convex function f = F +R to produce the iterates {wk}k1. Then the expected accuracy
after k steps is bounded as

E [f (wk)  f (w?)]  C

B R2
1

(1  ✏)P ·

1
k

.

Here the constant C only depends on (Lipschitz and smoothness constants of) the function F   R1 is
an upper bound on the norms {kwk  w?k1}k1  and w? is any minimizer of f.
Proof. We ﬁrst calculate the expected change in objective function following the Shotgun analysis.
We will use wb to denote wjb (similarly for ⌫b  gb etc.)

E [f (w0)  f (w)] = PEb⌘bgb +


2

(⌘b)2 + r(wb + ⌘b)  r(wb)

+


2

P (P  1)Eb6=b0⇥⌘b · ⌘b0 · AT

jbAjb0⇤

P



Deﬁne the B ⇥ B matrix M (that depends on the current iterate w) with entries Mb b0 = AT
Then  using r(wb + ⌘b)  r(wb)  ⌫b⌘b  we continue
⌘T ⌘ + ⌫T ⌘ +

2B(B  1)⇥⌘>M⌘  ⌘T ⌘⇤

B⌘T g +

P (P  1)

Above (with some abuse of notation)  ⌘  ⌫ and g are B length vectors with components ⌘b  ⌫b and
gb respectively. By deﬁnition of ⇢block  we have ⌘>M⌘  ⇢block⌘T ⌘. So  we continue
(⇢block  1)⌘T ⌘

⌘T ⌘  gT ⌘  ⌘T ⌘ +

B⌘T g +

P (P  1)
2B(B  1)

jbAjb.




2


2

P

where we used ⌫ = g  ⌘. Simplifying we get
E [f (w0)  f (w)] 

P
2B

[1 + ✏]k⌘k2

2

where

✏ =

(P  1)(⇢block  1)

(B  1)

should be less than 1.
Now note that k⌘k2
1 2 where the “inﬁnity-2” norm k·k 1 2 of a p-vector is  by
deﬁnition  as follows: take the `1 norm within a block and take the `2 of the resulting values. Note
that in the second step above  we moved from a B-length ⌘ to a p length ⌘.
This gives us

2 =Pb |⌘jb|2 = k⌘k2

E [f (w0)  f (w)]  

(1  ✏)P

2B

k⌘k2

1 2 .

For the rest of the proof  assume  = 0. In that case ⌘ = g/. Thus  convexity and the fact that
the dual norm of the “inﬁnity-2” norm is the “1-2” norm  give

f (w)  f (w?)  rf (w)T (w  w?)  krf (w)k1 2 ·k w  w?k1 2

Putting the last two inequalities together gives (for any upper bound R1 on kw  w?k1  kw 
w?k1 2)

E [f (w0)  f (w)]  

(1  ✏)P
2BR2
1

(f (w)  f (w?))2 .

Deﬁning the accuracy ↵k = f (wk)  f (w?)  we translate the above into the recurrence

E [↵k+1  ↵k]  

(1  ✏)P
2BR2
1

4

E⇥↵2
k⇤

and by Jensen’s we have (E [↵k])2  E⇥↵2

E [↵k+1]  E [↵k]  
which solves to (up to a universal constant factor)

k⇤ and therefore

(1  ✏)P
2BR2
1

(E [↵k])2

Even when > 0  we can still relate k⌘k1 2 to f (w)  f (w?) but the argument is a little more
involved. We refer the reader to the supplementary for more details.

E [↵k] 

2BR2
1

(1  ✏)P ·

1
k

In particular  consider the case where all blocks are updated in parallel as in the thread-greedy
coordinate descent algorithm of Scherrer et al. [2012]. Then P = B and there is no randomness in
the algorithm  yielding the following corollary.
Corollary 2. Suppose the block-greedy coordinate algorithm with B = P (thready-greedy) is run
on a smooth plus separable convex function f = F + R to produce the iterates {wk}k1.
If
⇢block < 2  then

f (wk)  f (w?) = O✓

1

(2  ⇢block)k◆ .

4 Feature Clustering

The convergence analysis of section 3 shows that we need to minimize the block spectral radius.
Directly ﬁnding a clustering that minimizes ⇢block is a computationally daunting task. Even with

BB. In the absence of an efﬁcient
equal-sized blocks  the number of possible partitions is p!/ p
search strategy for this enormous space  we ﬁnd it convenient to work instead in terms of the inner
product of features from distinct blocks. The following proposition makes the connection between
these approaches precise.
Proposition 3. Let S 2 RB⇥B be positive semideﬁnite  with Sii = 1  and |Sij| <" for i 6= j. Then
the spectral radius of S has the upper bound

⇢(S)  1 + (B  1) ".

Proof. Let x be the eigenvector corresponding to the largest eigenvalue of S  scaled so that kxk1 =
1. Then

⇢ (S) = kSxk1 =Xi

xi + SijXj6=i



Xi

0@|xi| + "Xj6=i

|xj|1A = 1 + (B  1) "

xj

Proposition 3 tells us that we can partition the features into clusters using a heuristic approach that
strives to minimize the maximum absolute inner product between the features (columns of the design
matrix) Xi and Xj where i and j are features in different blocks.

4.1 Clustering Heuristic

Given p features and B blocks  we wish to distribute the features evenly among the blocks  attempt-
ing to minimize the absolute inner product between features across blocks. Moreover  we require an
approach that is efﬁcient  since any time spent clustering could instead have been used for iterations
of the main algorithm. We describe a simple heuristic that builds uniform-sized clusters of features.
To construct a given block  we select a feature as a “seed”  and assign the nearest features to the seed
(in terms of absolute inner product) to be in the same block. Because inner products with very sparse
features result in a large number of zeros  we choose at each step the most dense unassigned feature
as the seed. Algorithm 2 provides a detailed description. This heuristic requires computation of
O(Bp) inner products. In practice it is very fast—less than three seconds for even the large KDDA
dataset.

5

Algorithm 2 A heuristic for clustering p features into B blocks  based on correlation

U { 1 ···   p}
for b = 1 to B  1 do

s arg maxj2U NNZ(Xj)
for j 2 U do
Jb { j yielding the dp/Be largest values of cj}
U U\Jb

cj |h Xs  Xji|

JB U
return {Jb|b = 1 ···   B}

// parallel

Name
NEWS20
REUTERS
REALSIM
KDDA

# Features
1  355  191
47  237
20  958
20  216  830

# Samples
19  996
23  865
72  309
8  407  752

Source

# Nonzeros
9  097  916 Keerthi and DeCoste [2005]
1  757  800
3  709  083
305  613  510

Lewis et al. [2004]
RealSim
Lo et al. [2011]

Table 1: Summary of input characteristics.

5 Experimental Setup

Platform All our experiments are conducted on a 48-core system comprising of 4 sockets and 8
banks of memory. Each socket is an AMD Opteron processor codenamed Magny-Cours  which is a
multichip processor with two 6-core chips on a single die. Each 6-core processor is equipped with a
three-level memory hierarchy as follows: (i) 64 KB of L1 cache for data and 512 KB of L2 cache
that are private to each core  and (ii) 12 MB of L3 cache that is shared among the 6 cores. Each
6-core processor is linked to a 32 GB memory bank with independent memory controllers leading
to a total system memory of 256 GB (32 ⇥ 8) that can be globally addressed from each core. The
four sockets are interconnected using HyperTransport-3 technology1.
Datasets A variety of datasets were chosen2 for experimentation; these are summarized in Ta-
ble 1. We consider four datasets: (i) NEWS20 contains about 20  000 UseNet postings from 20
newsgroups. The data was gathered by Ken Lang at Carnegie Mellon University circa 1995. (ii)
REUTERS is the RCV1-v2/LYRL2004 Reuters text data described by Lewis et al. [2004]. In this
term-document matrix  each example is a training document  and each feature is a term. Nonzero
values of the matrix correspond to term frequencies that are transformed using a standard tf-idf nor-
malization. (iii) REALSIM consists of about 73  000 UseNet articles from four discussion groups:
simulated auto racing  simulated aviation  real auto racing  and real aviation. The data was gathered
by Andrew McCallum while at Just Research circa 1997. We consider classifying real vs simulated
data  irrespective of auto/aviation. (iv) KDDA represents data from the KDD Cup 2010 challenge
on educational data mining. The data represents a processed version of the training set of the ﬁrst
problem  algebra 2008 2009  provided by the winner from the National Taiwan University. These
four inputs cover a broad spectrum of sizes and structural properties.
Implementation For the current work  our empirical results focus on thread-greedy coordinate de-
scent with 32 blocks. At each iteration  a given thread must step through the nonzeros of each of its
features to compute the proposed increment (the ⌘j of Section 3) and the estimated beneﬁt of choos-
ing that feature. Once this is complete  the thread (without waiting) enters the line search phase 
where it remains until all threads are being updated by less than the speciﬁed tolerance. Finally  all
updates are performed concurrently. We use OpenMP’s atomic directive to maintain consistency.
Testing framework
We compare the effect of clustering to randomization (i.e. features are randomly assigned to blocks) 
for a variety of values for the regularization parameter . To test the effect of clustering for very

1Further details on AMD Opteron can be found at http://www.amd.com/us/products/

embedded/processors/opteron/Pages/opteron-6100-series.aspx.

2from http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/

6

L
E
R
×
0
1

7

6

5

4

3

2

1

7

6

L
E
R
×
0
1

5

4

3

2

0
3
9
6

.

5
2
9
6

.

0
2
9
6

.

L
E
R
×
0
1

0

5

10

15

0

5

10

15

0

5

10

15

0

50

100

150

L
E
R
×
0
1

7

6

5

4

3

2

1

0

4
0
1

Z
N
N

3
0
1

2
0
1

4
0
1

4
0
1

Z
N
N

3
0
1

2
0
1

Z
N
N

3
0
1

2
0
1

0
1

0

5

10
Time (min)

15

0
1

0

5

10
Time (min)

15

0
1

0

4
0
1

3
0
1

Z
N
N

2
0
1

0
1

1

5

10
Time (min)

15

0

50

100
Time (min)

150

(a) NEWS20 
0 = 104

(b) REUTERS 
0 = 104

(c) REALSIM 
0 = 104

(d) KDDA 
0 = 106

Figure 2: Convergence results. For each dataset  we show the regularized expected loss (top) and
number of nonzeros (bottom)  using powers of ten as regularization parameters. Results for random-
ized features are shown in black  and those for clustered features are shown in red. Note that the
allowed running time for KDDA was ten times that of other datasets.

Active blocks

Iterations per second

NNZ @ 1K sec

Objective @ 1K sec
NNZ @ 10K iter

Objective @ 10K iter

 = 104

Randomized Clustered

 = 105

Randomized Clustered

 = 106

Randomized Clustered

32
153
184
0.472
74
0.570

6
12.9
215
0.591
203
0.593

32
152
797
0.264
82
0.515

32
12.3
8592
0.321
8812
0.328

32
136
1248
0.206
110
0.472

32
12.3
19473
0.136
19919
0.141

Table 2: The effect of feature clustering  for REUTERS.

sparse weights  we ﬁrst let 0 be the largest power of ten that leads to any nonzero weight esti-
mates. This is followed by the next three consecutive powers of ten. For each run  we measure the
regularized expected loss and the number of nonzeros at one-second intervals. Times required for
clustering and randomization are negligible  and we do not report them here.

6 Results

Figure 2 shows the regularized expected loss (top) and number of nonzeros (bottom)  for several
values of the regularization parameter . Black and red curves indicate randomly-permuted features
and clustered features  respectively. The starting value of  was 104 for all data except KDDA 
which required  = 106 in order to yield any nonzero weights.
In the upper plots  within a color  the order of the 4 curves  top to bottom  corresponds to successively
decreasing values of . Note that a larger value of  results in a sparser solution with greater
regularized expected loss and a smaller number of nonzeros. Thus  for each subﬁgure of Figure 2 
the order of the curves in the lower plot is reversed from that of the upper plot.
Overall  results across datasets are very consistent. For large values of   the simple clustering
heuristic results in slower convergence  while for smaller values of  we see considerable beneﬁt.
Due to space limitations  we choose a single dataset for which to explore results in greater detail.
Of the datasets we tested  REUTERS might reasonably lead to the greatest concern. Like the other
datasets  clustered features lead to slow convergence for large  and fast convergence for small .
However  REUTERS is particularly interesting because for  = 105  clustered features seem to
provide an initial beneﬁt that does not last; after about 250 seconds it is overtaken by the run with
randomized features.

7

6
0
1

●

●

Z ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
N
N

●

●

●●

5
0
1

4
0
1

●

●

●

●
●●●

●●

●●●●●

●●●●●●●●●●●●

Block

L
E
R
×
0
1

7

6

5

4

3

2

1

4
0
1

3
0
1

2
0
1

Z
N
N

1

10

102
103
Iterations

104

105

1

10

102
103
Iterations

104

105

0
1

(a) Block density

(b) Regularized expected loss

(c) Number of nonzeros

Figure 3: A closer look at performance characteristics for REUTERS.

Table 2 gives a more detailed summary of the results for REUTERS  for the three largest values of .
The ﬁrst row of this table gives the number of active blocks  by which we mean the number of blocks
containing any nonzeros. For an inactive block  the corresponding thread repeatedly conﬁrms that
all weights remain zero without contributing to convergence.
In the most regularized case  = 104  clustered data results in only six active blocks  while for
other cases every block is active. Thus in this case features corresponding to nonzero weights are
colocated within these few blocks  severely limiting the advantage of parallel updates.
In the second row  we see that for randomized features  the algorithm is able to get through over ten
times as many iterations per second. To see why  note that the amount of work for a given thread is
a linear function of the number of nonzeros over all of the features in its block. Thus  the block with
the greatest number of nonzeros serves as a bottleneck.
The middle two rows of Figure 2 summarize the state of each run after 1000 seconds. Note that for
this test  randomized features result in faster convergence for the two largest values of .
For comparison  the ﬁnal two rows of Figure 2 provide a similar summary based instead on the
number of iterations. In these terms  clustering is advantageous for all but the largest value of .
Figure 3 shows the source of this problem. First  Figure 3a shows the number of nonzeros in all
features for each of the 32 blocks. Clearly the simple heuristic results in poor load-balancing. For
comparison  Figures 3b and 3c show convergence rates as a function of the number of iterations.

7 Conclusion

We have presented convergence results for a family of randomized coordinate descent algorithms
that we call block-greedy coordinate descent. This family includes Greedy CD  Thread-Greedy
CD  Shotgun  and Stochastic CD. We have shown that convergence depends on ⇢block  the maximal
spectral radius over submatrices of XT X resulting from the choice of one feature for each block.
Even though a simple clustering heuristic helps for smaller values of the regularization parameter 
our results also show the importance of considering issues of load-balancing and the distribution of
weights for heavily-regularized problems.
A clear next goal in this work is the development of a clustering heuristic that is relatively well
load-balanced and distributes weights for heavily-regularized problems evenly across blocks  while
maintaining good computational efﬁciency.

Acknowledgments
The authors are grateful for the helpful suggestions of Ken Jarman  Joseph Manzano  and our anony-
mous reviewers.
Funding for this work was provided by the Center for Adaptive Super Computing Software - Mul-
tiThreaded Architectures (CASS-MT) at the U.S. Department of Energy’s Paciﬁc Northwest Na-
tional Laboratory. PNNL is operated by Battelle Memorial Institute under Contract DE-ACO6-
76RL01830.

8

References
J Friedman  T Hastie  H H¨oﬂing  and R Tibshirani. Pathwise coordinate optimization. Annals of

Applied Statistics  1(2):302–332  2007.

T Wu and K Lange. Coordinate descent algorithms for lasso penalized regression. Annals of Applied

Statistics  2:224–244  2008.

S Shalev-Shwartz and A Tewari. Stochastic methods for `1-regularized loss minimization. Journal

of Machine Learning Research  12:1865–1892  2011.

J K Bradley  A Kyrola  D Bickson  and C Guestrin. Parallel Coordinate Descent for L1-Regularized
Loss Minimization. In Proceedings of the 28th International Conference on Machine Learning 
pages 321–328  2011.

C Scherrer  A Tewari  M Halappanavar  and D Haglin. Scaling up Parallel Coordinate Descent

Algorithms. In International Conference on Machine Learning  2012.

Y Li and S Osher. Coordinate Descent Optimization for `1 Minimization with Application to Com-
pressed Sensing ; a Greedy Algorithm Solving the Unconstrained Problem. Inverse Problems and
Imaging  3:487–503  2009.

I S Dhillon  P Ravikumar  and A Tewari. Nearest neighbor based greedy coordinate descent. In

Advances in Neural Information Processing Systems 24  pages 2160–2168  2011.

D Lewis  Y Yang  T Rose  and F Li. RCV1 : A New Benchmark Collection for Text Categorization

Research. Journal of Machine Learning Research  5:361–397  2004.

S S Keerthi and D DeCoste. A modiﬁed ﬁnite Newton method for fast solution of large scale linear

SVMs. Journal of Machine Learning Research  6:341–361  2005.

RealSim. Document classiﬁcation data gathered by Andrew McCallum.  circa 1997. URL:http:

//people.cs.umass.edu/˜mccallum/data.html.

Hung-Yi Lo  Kai-Wei Chang  Shang-Tse Chen  Tsung-Hsien Chiang  Chun-Sung Ferng  Cho-Jui
Hsieh  Yi-Kuang Ko  Tsung-Ting Kuo  Hung-Che Lai  Ken-Yi Lin  Chia-Hsuan Wang  Hsiang-Fu
Yu  Chih-Jen Lin  Hsuan-Tien Lin  and Shou de Lin. Feature engineering and classiﬁer ensemble
for KDD Cup 2010  2011. To appear in JMLR Workshop and Conference Proceedings.

9

,Halid Yerebakan
Bartek Rajwa
Murat Dundar
Constantinos Daskalakis
Nishanth Dikkala
Gautam Kamath