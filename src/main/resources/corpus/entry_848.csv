2016,Bootstrap Model Aggregation for Distributed Statistical Learning,In distributed  or privacy-preserving learning  we are often given a set of probabilistic models estimated from different local repositories  and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models  which  however  tends to be degenerate or not applicable on non-convex models  or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models  and then learn a joint model based on the combined bootstrap set. Unfortunately  the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work  we propose two variance reduction methods to correct the bootstrap noise  including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.,Bootstrap Model Aggregation for Distributed

Statistical Learning

Jun Han

Department of Computer Science

Dartmouth College

jun.han.gr@dartmouth.edu

Qiang Liu

Department of Computer Science

Dartmouth College

qiang.liu@dartmouth.edu

Abstract

In distributed  or privacy-preserving learning  we are often given a set of probabilis-
tic models estimated from different local repositories  and asked to combine them
into a single model that gives efﬁcient statistical estimation. A simple method is to
linearly average the parameters of the local models  which  however  tends to be
degenerate or not applicable on non-convex models  or models with different param-
eter dimensions. One more practical strategy is to generate bootstrap samples from
the local models  and then learn a joint model based on the combined bootstrap
set. Unfortunately  the bootstrap procedure introduces additional noise and can
signiﬁcantly deteriorate the performance. In this work  we propose two variance
reduction methods to correct the bootstrap noise  including a weighted M-estimator
that is both statistically efﬁcient and practically powerful. Both theoretical and
empirical analysis is provided to demonstrate our methods.

1

Introduction

Modern data science applications increasingly involve learning complex probabilistic models over
massive datasets. In many cases  the datasets are distributed into multiple machines at different
locations  between which communication is expensive or restricted; this can be either because the
data volume is too large to store or process in a single machine  or due to privacy constraints as
these in healthcare or ﬁnancial systems. There has been a recent growing interest in developing
communication-efﬁcient algorithms for probabilistic learning with distributed datasets; see e.g.  Boyd
et al. (2011); Zhang et al. (2012); Dekel et al. (2012); Liu and Ihler (2014); Rosenblatt and Nadler
(2014) and reference therein.
This work focuses on a one-shot approach for distributed learning  in which we ﬁrst learn a set
of local models from local machines  and then combine them in a fusion center to form a single
model that integrates all the information in the local models. This approach is highly efﬁcient in
both computation and communication costs  but casts a challenge in designing statistically efﬁcient
combination strategies. Many studies have been focused on a simple linear averaging method that
linearly averages the parameters of the local models (e.g.  Zhang et al.  2012  2013; Rosenblatt
and Nadler  2014); although nearly optimal asymptotic error rates can be achieved  this simple
method tends to degenerate in practical scenarios for models with non-convex log-likelihood or
non-identiﬁable parameters (such as latent variable models  and neural models)  and is not applicable
at all for models with non-additive parameters (e.g.  when the parameters have discrete or categorical
values  or the parameter dimensions of the local models are different).
A better strategy that overcomes all these practical limitations of linear averaging is the KL-averaging
method (Liu and Ihler  2014; Merugu and Ghosh  2003)  which ﬁnds a model that minimizes the
sum of Kullback-Leibler (KL) divergence to all the local models. In this way  we directly combine
the models  instead of the parameters. The exact KL-averaging is not computationally tractable

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

because of the intractability of calculating KL divergence; a practical approach is to draw (bootstrap)
samples from the given local models  and then learn a combined model based on all the bootstrap
data. Unfortunately  the bootstrap noise can easily dominate in this approach and we need a very large
bootstrap sample size to obtain accurate results. In Section 3  we show that the MSE of the estimator
obtained from the naive way is O(N−1 + (dn)−1)  where N is the total size of the observed data 
and n is bootstrap sample size of each local model and d is the number of machines. This means that
to ensure a MSE of O(N−1)  which is guaranteed by the centralized method and the simple linear
averaging  we need dn (cid:38) N; this is unsatisfying since N is usually very large by assumption.
In this work  we use variance reduction techniques to cancel out the bootstrap noise and get better
KL-averaging estimates. The difﬁculty of this task is ﬁrst illustrated using a relatively straightforward
control variates method  which unfortunately suffers some of the practical drawback of the linear
averaging method due to the use of a linear correction term. We then propose a better method based
on a weighted M-estimator  which inherits all the practical advantages of KL-averaging. On the
theoretical part  we show that our methods give a MSE of O(N−1 + (dn2)−1)  which signiﬁcantly
improves over the original bootstrap estimator. Empirical studies are provided to verify our theoretical
results and demonstrate the practical advantages of our methods.
This paper is organized as follows. Section 2 introduces the background  and Section 3 introduces
our methods and analyze their theoretical properties. We present numerical results in Section 4 and
conclude the paper in Section 5. Detailed proofs can be found in the appendix.

d(cid:88)

N/d(cid:88)

k=1

j=1

N/d(cid:88)

j=1

∗

2 Background and Problem Setting
Suppose we have a dataset X = {xj  j = 1  2  ...  N} of size N  i.i.d. drawn from a probabilistic
model p(x|θ
∗ is the unknown true
parameter that we want to estimate based on X. In the distributed setting  the dataset X is partitioned
k=1 X k  where X k denotes the k-th subset which we assume is stored

into d disjoint subsets  X =(cid:83)d

) within a parametric family P = {p(x|θ) : θ ∈ Θ}; here θ

in a local machine. For simplicity  we assume all the subsets have the same data size (N/d).
The traditional maximum likelihood estimator (MLE) provides a natural way for estimating the true
parameter θ

∗ based on the whole dataset X 

Global MLE: ˆθmle = arg max

θ∈Θ

log p(xk

j | θ)  where X k = {xk
j}.

(1)

However  directly calculating the global MLE is challenging due to the distributed partition of the
dataset. Although distributed optimization algorithms exist (e.g.  Boyd et al.  2011; Shamir et al. 
2014)  they require iterative communication between the local machines and a fusion center  which
can be very time consuming in distributed settings  for which the number of communication rounds
forms the main bottleneck (regardless of the amount of information communicated at each round).
We instead consider a simpler one-shot approach that ﬁrst learns a set of local models based on each
subset  and then send them to a fusion center in which they are combined into a global model that
captures all the information. We assume each of the local models is estimated using a MLE based on
subset X k from the k-th machine:

Local MLE: ˆθk = arg max

θ∈Θ

log p(xk

j | θ)  where k ∈ [d] = {1  2 ···   d}.

(2)

The major problem is how to combine these local models into a global model. The simplest way is to
linearly average all local MLE parameters:

Linear Average: ˆθlinear =

1
d

ˆθk.

d(cid:88)

k=1

Comprehensive theoretical analysis has been done for ˆθlinear (e.g.  Zhang et al.  2012; Rosenblatt and
Nadler  2014)  which show that it has an asymptotic MSE of E|| ˆθlinear − θ
∗||2 = O(N−1). In fact 
it is equivalent to the global MLE ˆθmle up to the ﬁrst order O(N−1)  and several improvements have
been developed to improve the second order term (e.g.  Zhang et al.  2012; Huang and Huo  2015).

2

Unfortunately  the linear averaging method can easily break down in practice  or is even not applicable
when the underlying model is complex. For example  it may work poorly when the likelihood has
multiple modes  or when there exist non-identiﬁable parameters for which different parameter values
correspond to a same model (also known as the label-switching problem); models of this kind include
latent variable models and neural networks  and appear widely in machine learning. In addition  the
linear averaging method is obviously not applicable when the local models have different numbers of
parameters (e.g.  Gaussian mixtures with unknown numbers of components)  or when the parameters
are simply not additive (such as parameters with discrete or categorical values). Further discussions
on the practical limitations of the linear averaging method can be found in Liu and Ihler (2014).
All these problems of linear averaging can be well addressed by a KL-averaging method which
(cid:80)d
averages the model (instead of the parameters) by ﬁnding a geometric center of the local models
in terms of KL divergence (Merugu and Ghosh  2003; Liu and Ihler  2014). Speciﬁcally  it ﬁnds a
k=1 KL(p(x| ˆθk) || p(x|θ))  which
model p(x | θ
∗
KL) where θ
(cid:90)
is equivalent to 

∗
KL is obtained by θ

∗
KL = arg minθ

(cid:27)

(cid:26)

p(x | ˆθk) log p(x | θ)dx

.

(3)

Exact KL Estimator:

∗
KL = arg max

θ

θ∈Θ

η(θ) ≡ d(cid:88)

k=1

Liu and Ihler (2014) studied the theoretical properties of the KL-averaging method  and showed
∗
KL = ˆθmle  when the distribution family is a full
that it exactly recovers the global MLE  that is  θ
exponential family  and achieves an optimal asymptotic error rate (up to the second order) among all
the possible combination methods of { ˆθk}.
Despite the attractive properties  the exact KL-averaging is not computationally tractable except for
very simple models. Liu and Ihler (2014) suggested a naive bootstrap method for approximation: it
j=1 from each local model p(x| ˆθk)  k ∈ [d] and use it to

draws parametric bootstrap sample {(cid:101)xk
j}n

approximate each integral in (3). The optimization in (3) then reduces to a tractable one 

(cid:26)

ˆη(θ) ≡ 1
n

(cid:27)
j | θ)

log p((cid:101)xk

d(cid:88)

n(cid:88)

k=1

j=1

.

(4)

KL-Naive Estimator: ˆθKL = arg max

Intuitively  we can treat each (cid:101)Xk = {(cid:101)xk

j}n

θ∈Θ

j=1 as an approximation of the original subset X k =

j}N/d

j=1   and hence can be used to approximate the global MLE in (1).

{xk
Unfortunately  as we show in the sequel  the accuracy of ˆθKL critically depends on the bootstrap
sample size n  and one would need n to be nearly as large as the original data size N/d to make ˆθKL
achieve the baseline asymptotic rate O(N−1) that the simple linear averaging achieves; this is highly
undesirably since N is often assumed to be large in distributed learning settings.

3 Main Results

We propose two variance reduction techniques for improving the KL-averaging estimates and discuss
their theoretical and practical properties. We start with a concrete analysis on the KL-naive estimator
ˆθKL  which was missing in Liu and Ihler (2014).

log p(x | θ)  ∂ log p(x|θ)

Assumption 1. 1.
∀θ ∈ Θ; 2. ∂2 log p(x|θ)
∀x ∈ X   and C1  C2 are some positive constans.
Theorem 2. Under Assumption 1  ˆθKL is a consistent estimator of θ

is positive deﬁnite and C1 ≤ (cid:107) ∂2 log p(x|θ)

  and ∂2 log p(x|θ)
∂θ∂θ(cid:62)

∂θ∂θ(cid:62)

∂θ∂θ(cid:62)

∂θ

are continuous for ∀x ∈ X and
∗ for

(cid:107) ≤ C2 in a neighbor of θ

KL as n → ∞  and
∗

E( ˆθKL − θ

∗
KL) = o(

)  E(cid:107) ˆθKL − θ

KL(cid:107)2 = O(
∗

1
dn

1
dn

) 

where d is the number of machines and n is the bootstrap sample size for each local model p(x | ˆθk).
∗
The proof is in Appendix A. Because the MSE between the exact KL estimator θ
KL and the true
∗ is O(N−1) as shown in Liu and Ihler (2014)  the MSE between ˆθKL and the true
parameter θ

3

parameter θ

∗ is
E(cid:107) ˆθKL − θ

KL − θ
∗

∗(cid:107)2 = O(N−1 + (dn)−1).

KL(cid:107)2 + E(cid:107)θ
∗(cid:107)2 ≈ E(cid:107) ˆθKL − θ
∗
(5)
∗ equal O(N−1)  as what is achieved by the simple linear
To make the MSE between ˆθKL and θ
averaging  we need draw dn (cid:38) N bootstrap data points in total  which is undesirable since N is often
assumed to be very large by the assumption of distributed learning setting (one exception is when the
data is distributed due to privacy constraint  in which case N may be relatively small).
Therefore  it is a critical task to develop more accurate methods that can reduce the noise introduced
by the bootstrap process. In the sequel  we introduce two variance reduction techniques to achieve
this goal. One is based a (linear) control variates method that improves ˆθKL using a linear correction
term  and another is a multiplicative control variates method that modiﬁes the M-estimator in (4) by
assigning each bootstrap data point with a positive weight to cancel the noise. We show that both
method achieves a higher O(N−1 + (dn2)−1) rate under mild assumptions  while the second method
has more attractive practical advantages.

3.1 Control Variates Estimator

The control variates method is a technique for variance reduction on Monte Carlo estimation (e.g. 
Wilson  1984). It introduces a set of correlated auxiliary random variables with known expectations
or asymptotics (referred as the control variates)  to balance the variation of the original estimator. In
j=1 is know to be drawn from the local

our case  since each bootstrapped subsample (cid:101)X k = {(cid:101)xk
model p(x | ˆθk)  we can construct a control variate by re-estimating the local model based on (cid:101)X k:
j}n
n(cid:88)
(cid:101)θk = arg max
log p((cid:101)xk

where(cid:101)θk is known to converge to ˆθk asymptotically. This allows us to deﬁne the following control

Bootstrapped Local MLE:

for k ∈ [d] 

j | θ) 

θ∈Θ

(6)

j=1

variates estimator:

d(cid:88)

k=1

Bk((cid:101)θk − ˆθk) 

(7)

KL-Control Estimator: ˆθKL−C = ˆθKL +

d(cid:88)

where Bk is a matrix chosen to minimize the asymptotic variance of ˆθKL−C; our derivation shows
that the asymptotically optimal Bk has a form of

Bk = −(

I( ˆθk))−1I( ˆθk) 

k ∈ [d] 

(8)

k=1

where I( ˆθk) is the empirical Fisher information matrix of the local model p(x | ˆθk). Note that this
differentiates our method from the typical control variates methods where Bk is instead estimated
using empirical covariance between the control variates and the original estimator (in our case  we

can not directly estimate the covariance because ˆθKL and(cid:101)θk are not averages of i.i.d. samples).The

procedure of our method is summarized in Algorithm 1. Note that the form of (7) shares some
similarity with the one-step estimator in Huang and Huo (2015)  but Huang and Huo (2015) focuses
on improving the linear averaging estimator  and is different from our setting.
We analyze the asymptotic property of the estimator ˆθKL−C  and summarize it as follows.

Theorem 3. Under Assumption (1)  ˆθKL−C is a consistent estimator of θ
asymptotic MSE is guaranteed to be smaller than the KL-naive estimator ˆθKL  that is 

KL as n → ∞  and its
∗

nE(cid:107) ˆθKL−C − θ

as n → ∞.
In addition  when N > n×d  the ˆθKL−C has “zero-variance” in that E(cid:107) ˆθKL−θ
KL(cid:107)2 = O((dn2)−1).
∗
Further  in terms of estimating the true parameter  we have

KL(cid:107)2 < nE(cid:107) ˆθKL − θ
∗

KL(cid:107)2 
∗

E(cid:107) ˆθKL−C − θ

∗(cid:107)2 = O(N−1 + (dn2)−1).

(9)

4

k=1.

Algorithm 1 KL-Control Variates Method for Combining Local Models
1: Input: Local model parameters { ˆθk}d

2: Generate bootstrap data {(cid:101)xk
(cid:80)n
j=1 log p((cid:101)xk
j}n
j=1 from each p(x| ˆθk)  for k ∈ [d].
4: Re-estimate the local parameters(cid:101)θk via (6) based on the bootstrapped data subset {(cid:101)xk
j|θ).
j}n
j=1 
(cid:80)n
∂log p((cid:101)xk
∂log p((cid:101)xk

3: Calculate the KL-Naive estimator  ˆθKL = arg maxθ∈Θ

5: Estimate the empirical Fisher information matrix I( ˆθk) = 1
n

(cid:80)d

k ∈ [d].

j | ˆθk)

k=1

j=1

1
n

∂θ

∂θ

for

(cid:62)

 

j | ˆθk)

for k ∈ [d].

6: Ouput: The parameter ˆθKL−C of the combined model is given by (7) and (8).

∗ reduces to
The proof is in Appendix B. From (9)  we can see that the MSE between ˆθKL−C and θ
O(N−1) as long as n (cid:38) (N/d)1/2  which is a signiﬁcant improvement over the KL-naive method
which requires n (cid:38) N/d. When the goal is to achieve an O() MSE  we would just need to take
n (cid:38) 1/(d)1/2 when N > 1/  that is  n does not need to increase with N when N is very large.

Meanwhile  because ˆθKL−C requires a linear combination of ˆθk (cid:101)θk and ˆθKL  it carries the practical

drawbacks of the linear averaging estimator as we discuss in Section 2. This motivates us to develop
another KL-weighted method shown in the next section  which achieves the same asymptotical
efﬁciency as ˆθKL−C  while still inherits all the practical advantages of KL-averaging.

3.2 KL-Weighted Estimator

assigning each bootstrap data point(cid:101)xk
ˆθk)/p((cid:101)xk

the probability ratio acts like a multiplicative control variate (Nelson  1987)  which has the advantage
of being positive and applicable to non-identiﬁable  non-additive parameters. Our estimator is deﬁned
as

j a positive weight according to the probability ratio p((cid:101)xk
Our KL-weighted estimator is based on directly modifying the M-estimator for ˆθKL in (4)  by
j | (cid:101)θk) of the actual local model p(x| ˆθk) and the re-estimated model p(x|(cid:101)θk) in (6). Here
j |
(cid:27)
We ﬁrst show that this weighted estimator(cid:101)η(θ) gives a more accurate estimation of η(θ) in (3) than
Lemma 4. As n → ∞ (cid:101)η(θ) is a more accurate estimator of η(θ) than ˆη(θ)  in that

the straightforward estimator ˆη(θ) deﬁned in (4) for any θ ∈ Θ.

(cid:26)(cid:101)η(θ) ≡ d(cid:88)

KL-Weighted Estimator: ˆθKL−W = arg max

p((cid:101)xk
j|(cid:101)θk)
j| ˆθk)
p((cid:101)xk

log p((cid:101)xk
j|θ)

n(cid:88)

(10)

θ∈Θ

1
n

k=1

j=1

.

as n → ∞ 

for any θ ∈ Θ.

(11)

nVar((cid:101)η(θ)) ≤ nVar(ˆη(θ)) 

This estimator is motivated by Henmi et al. (2007) in which the same idea is applied to reduce the
asymptotic variance in importance sampling. Similar result is also found in Hirano et al. (2003)  in
which it is shown that a similar weighted estimator with estimated propensity score is more efﬁcient
than the estimator using true propensity score in estimating the average treatment effects. Although
being a very powerful tool  results of this type seem to be not widely known in machine learning 
except several applications in semi-supervised learning (Sokolovska et al.  2008; Kawakita and
Kanamori  2013)  and off-policy learning (Li et al.  2015).
We go a step further to analyze the asymptotic property of our weighted M-estimator ˆθKL−W that

maximizes(cid:101)η(θ). It is natural to expect that the asymptotic variance of ˆθKL−W is smaller than that of
KL as n → ∞  and has a
∗

ˆθKL based on maximizing ˆη(θ); this is shown in the following theorem.

Theorem 5. Under Assumption 1  ˆθKL−W is a consistent estimator of θ
better asymptotic variance than ˆθKL  that is 

nE(cid:107) ˆθKL−W − θ

KL(cid:107)2 ≤ nE(cid:107) ˆθKL − θ
∗

KL(cid:107)2 
∗

when n → ∞.

5

Algorithm 2 KL-Weighted Method for Combining Local Models
1: Input: Local MLEs { ˆθk}d

2: Generate bootstrap sample {(cid:101)xk
3: Re-estimate the local model parameter (cid:101)θk in (6) based on bootstrap subsample {(cid:101)xk
j}n

j=1 from each p(x| ˆθk)  for k ∈ [d].

k=1.

j}n

j=1  for

each k ∈ [d].

4: Output: The parameter ˆθKL−W of the combined model is given by (10).

When N > n × d  we have E(cid:107) ˆθKL−W − θ
estimating the true parameter θ

∗ is

KL(cid:107)2 = O((dn2)−1) as n → ∞. Further  its MSE for
∗

E(cid:107) ˆθKL−W − θ

∗(cid:107)2 = O(N−1 + (dn2)−1).

(12)

The proof is in Appendix C. This result is parallel to Theorem 3 for the linear control variates
estimator ˆθKL−C. Similarly  it reduces to an O(N−1) rate once we take n (cid:38) (N/d)1/2.
Meanwhile  unlike the linear control variates estimator  ˆθKL−W inherits all the practical advantages of
KL-averaging: it can be applied whenever the KL-naive estimator can be applied  including for models
with non-identiﬁable parameters  or with different numbers of parameters. The implementation of
ˆθKL−W is also much more convenient (see Algorithm 2)  since it does not need to calculate the
Fisher information matrix as required by Algorithm 1.

4 Empirical Experiments

We study the empirical performance of our methods on both simulated and real world datasets. We
ﬁrst numerically verify the convergence rates predicted by our theoretical results using simulated
data  and then demonstrate the effectiveness of our methods in a challenging setting when the number
of parameters of the local models are different as decided by Bayesian information criterion (BIC).
Finally  we conclude our experiments by testing our methods on a set of real world datasets.
The models we tested include probabilistic principal components analysis (PPCA)  mixture of
s=1 αsN (µs  Σs)
where θ = (αs  µs  Σs). PPCA model is deﬁned with the help of a hidden variable t  p(x | θ) =
s=1 αsps(x | θs)  where θ = {αs  θs}m

PPCA and Gaussian Mixtures Models (GMM). GMM is given by p(x | θ) =(cid:80)m
(cid:82) p(x | t; θ)p(t | θ)dt  where p(x | t; θ) = N (x; µ + W t  σ2)  and p(t | θ) = N (t; 0  I) and
θ = {µ  W  σ2}. The mixture of PPCA is p(x | θ) =(cid:80)m

and each ps(x | θs) is a PPCA model.
Because all these models are latent variable models with unidentiﬁable parameters  the direct linear
averaging method are not applicable. For GMM  it is still possible to use a matched linear averaging
which matches the mixture components of the different local models by minimizing a symmetric KL
divergence; the same idea can be used on our linear control variates method to make it applicable to
GMM. On the other hand  because the parameters of PPCA-based models are unidentiﬁable up to
arbitrary orthonormal transforms  linear averaging and linear control variates can no longer be applied
easily. We use expectation maximization (EM) to learn the parameters in all these three models.

s=1

4.1 Numerical Veriﬁcation of the Convergence Rates
We start with verifying the convergence rates in (5)  (9) and (12) of MSE E|| ˆθ − θ
∗||2 of the different
estimators for estimating the true parameters. Because there is also an non-identiﬁability problem in
calculating the MSE  we again use the symmetric KL divergence to match the mixture components 
and evaluate the MSE on W W (cid:62) to avoid the non-identiﬁability w.r.t. orthonormal transforms. To
verify the convergence rates w.r.t. n  we ﬁx d and let the total dataset N be very large so that N−1 is
negligible. Figure 1 shows the results when we vary n  where we can see that the MSE of KL-naive
ˆθKL is O(n−1) while that of KL-control ˆθKL−C and KL-weighted ˆθKL−W are O(n−2); both are
consistent with our results in (5)  (9) and (12).
In Figure 2(a)  we increase the number d of local machines  while using a ﬁx n and a very large
N  and ﬁnd that both ˆθKL and ˆθKL−W scales as O(d−1) as expected. Note that since the total

6

observation data size N is ﬁxed  the number of data in each local machine is (N/d) and it decreases
as we increase d. It is interesting to see that the performance of the KL-based methods actually
increases with more partitions; this is  of course  with a cost of increasing the total bootstrap sample
size dn as d increases. Figure 2(b) considers a different setting  in which we increase d when ﬁxing
the total observation data size N  and the total bootstrap sample size ntot = n × d. According to (5)
and (12)  the MSEs of ˆθKL and ˆθKL−W should be about O(n−1
tot) respectively when
N is very large  and this is consistent with the results in Figure 2(b). It is interesting to note that
the MSE of ˆθKL is independent with d while that of ˆθKL−W increases linearly with d. This is not
conﬂict with the fact that ˆθKL−W is better than ˆθKL  since we always have d ≤ ntot.
Figure 2(c) shows the result when we set n = (N/d)α and vary α  where we ﬁnd that ˆθKL−W
quickly converges to the global MLE as α increases  while the KL-naive estimator ˆθKL converges
signiﬁcantly slower. Figure 2(d) demonstrates the case when we increase N while ﬁx d and n  where
we see our KL-weighted estimator ˆθKL−W matches closely with N  except when N is very large in
which case the O((dn2)−1) term starts to dominate  while KL-naive is much worse. We also ﬁnd the
linear averaging estimator performs poorly  and does not scale with O(N−1) as the theoretical rate
claims; this is due to unidentiﬁable orthonormal transform in the PPCA model that we test on.

tot) and O(dn−2

(a) PPCA

(b) Mixture of PPCA

(c) GMM

Figure 1: Results on different models with simulated data when we change the bootstrap sample size
n  with ﬁxed d = 10 and N = 6 × 107. The dimensions of the PPCA models in (a)-(b) are 5  and
that of GMM in (c) is 3. The numbers of mixture components in (b)-(c) are 3. Linear averaging and
KL-Control are not applicable for the PPCA-based models  and are not shown in (a) and (b).

(a) Fix N and n

(b) Fix N and ntot

(d) Fix n and d

Figure 2: Further experiments on PPCA with simulated data. (a) varying n with ﬁxed N = 5 × 107.
(b) varying d with N = 5 × 107  ntot = n × d = 3 × 105. (c) varying α with n = ( N
d )α  N = 107
and d. (d) varying N with n = 103 and d = 20. The dimension of data x is 5 and the dimension of
latent variables t is 4.

(c) Fix N  n = ( N

d )α and d

4.2 Gaussian Mixture with Unknown Number of Components

We further apply our methods to a more challenging setting for distributed learning of GMM when
the number of mixture components is unknown. In this case  we ﬁrst learn each local model with EM
and decide its number of components using BIC selection. Both linear averaging and KL-control
ˆθKL−C are not applicable in this setting  and and we only test KL-naive ˆθKL and KL-weighted
ˆθKL−W . Since the MSE is also not computable due to the different dimensions  we evaluate ˆθKL
and ˆθKL−W using the log-likelihood on a hold-out testing dataset as shown in Figure 3. We can
see that ˆθKL−W generally outperforms ˆθKL as we expect  and the relative improvement increases

7

100 1000Bootstrap Size (n)-4-3-2-10Log MSE100 1000Bootstrap Size (n)-3-2-101Log MSE100 1000Bootstrap Size (n)-5-4-3-2-1Log MSEKL-NaiveKL-ControlKL-Weighted10 100 1000d-4-3-2-10Log MSE2004006008001000d-4-3-2-1Log MSE0.50.60.70.80.9 -3.5-3-2.5-2-1.5-1-0.5Log MSEGlobal MLELinearKL-NaiveKL-Weighted1000001e+06 1e+07 N-3-2-10Log MSEsigniﬁcantly as the dimension of the observation data x increases. This suggests that our variance
reduction technique works very efﬁciently in high dimension problems.

(a) Dimension of x = 3

(b) Dimension of x = 80

(c) varying the dimension of x

Figure 3: GMM with the number of mixture components estimated by BIC. We set n = 600 and
the true number of mixtures to be 10 in all the cases. (a)-(b) vary the total data size N when the
dimension of x is 3 and 80  respectively. (c) varies the dimension of the data with ﬁxed N = 105.
The y-axis is the testing log likelihood compared with that of global MLE.

4.3 Results on Real World Datasets

Finally  we apply our methods to several real world datasets  including the SensIT Vehicle dataset on
which mixture of PPCA is tested  and the Covertype and Epsilon datasets on which GMM is tested.
From Figure 4  we can see that our KL-Weight and KL-Control (when it is applicable) again perform
the best. The (matched) linear averaging performs poorly on GMM (Figure 4(b)-(c))  while is not
applicable on mixture of PPCA.

(a) Mixture of PPCA  SensIT Vehicle

(b) GMM  Covertype

(c) GMM  Epsilon

Figure 4: Testing log likelihood (compared with that of global MLE) on real world datasets. (a)
Learning Mixture of PPCA on SensIT Vehicle. (b)-(c) Learning GMM on Covertype and Epsilon.
The number of local machines is 10 in all the cases  and the number of mixture components are
taken to be the number of labels in the datasets. The dimension of latent variables in (a) is 90. For
Epsilon  a PCA is ﬁrst applied and the top 100 principal components are chosen. Linear-matched and
KL-Control are not applicable on Mixture of PPCA and are not shown on (a).

5 Conclusion and Discussion

We propose two variance reduction techniques for distributed learning of complex probabilistic
models  including a KL-weighted estimator that is both statistically efﬁcient and widely applicable
for even challenging practical scenarios. Both theoretical and empirical analysis is provided to
demonstrate our methods. Future directions include extending our methods to discriminant learning
tasks  as well as the more challenging deep generative networks on which the exact MLE is not
computable tractable  and surrogate likelihood methods with stochastic gradient descent are need.
We note that the same KL-averaging problem also appears in the “knowledge distillation" problem
in Bayesian deep neural networks (Korattikara et al.  2015)  and it seems that our technique can be
applied straightforwardly.
Acknowledgement This work is supported in part by NSF CRII 1565796.

8

2468N#104-0.14-0.12-0.1-0.08-0.06-0.04-0.02Average LLKL-NaiveKL-Weighted246810N#104-7-6-5-4-3-2Average LL20406080100Dimension of Data-2-1.5-1-0.5Average LL12345N#104-7-6-5-4-3-2-1Average LL0510N#104-4-3-2-10Average LL0510N#104-1-0.8-0.6-0.4-0.20Average LLLinear-MatchedKL-NaiveKL-ControlKL-WeightedReferences
S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine
Learning  3(1)  2011.

Y. Zhang  M. J. Wainwright  and J. C. Duchi. Communication-efﬁcient algorithms for statistical

optimization. In NIPS  2012.

O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao. Optimal distributed online prediction using

mini-batches. In JMLR  2012.

Q. Liu and A. T. Ihler. Distributed estimation  information loss and exponential families. In NIPS 

2014.

J. Rosenblatt and B. Nadler. On the optimality of averaging in distributed statistical learning. arXiv

preprint arXiv:1407.2724  2014.

Y. Zhang  J. Duchi  M. I. Jordan  and M. J. Wainwright. Information-theoretic lower bounds for

distributed statistical estimation with communication constraints. In NIPS  2013.

S. Merugu and J. Ghosh. Privacy-preserving distributed clustering using generative models. In Data
Mining  2003. ICDM 2003. Third IEEE International Conference on  pages 211–218. IEEE  2003.

O. Shamir  N. Srebro  and T. Zhang. Communication efﬁcient distributed optimization using an

approximate Newton-type method. In ICML  2014.

C. Huang and X. Huo. A distributed one-step estimator. arXiv preprint arXiv:1511.01443  2015.

J. R. Wilson. Variance reduction techniques for digital simulation. American Journal of Mathematical

and Management Sciences  4  1984.

B. L. Nelson. On control variate estimators. Computers & Operations Research  14  1987.

M. Henmi  R. Yoshida  and S. Eguchi. Importance sampling via the estimated sampler. Biometrika 

94(4)  2007.

K. Hirano  G. W. Imbens  and G. Ridder. Efﬁcient estimation of average treatment effects using the

estimated propensity score. Econometrica  71  2003.

N. Sokolovska  O. Cappé  and F. Yvon. The asymptotics of semi-supervised learning in discriminative

probabilistic models. In ICML. ACM  2008.

M. Kawakita and T. Kanamori. Semi-supervised learning with density-ratio estimation. Machine

learning  91  2013.

L. Li  R. Munos  and C. Szepesvári. Toward minimax off-policy value estimation. In AISTATS  2015.

A. Korattikara  V. Rathod  K. Murphy  and M. Welling. Bayesian dark knowledge. arXiv preprint

arXiv:1506.04416  2015.

9

,JUN HAN
Qiang Liu
Omer Ben-Porat
Moshe Tennenholtz