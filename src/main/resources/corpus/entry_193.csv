2014,A Safe Screening Rule for Sparse Logistic Regression,The l1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation  its application to high dimensional data still poses significant challenges. In this paper  we present a fast and effective sparse logistic regression screening rule (Slores) to identify the zero components in the solution vector  which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover  Slores is independent of solvers for sparse logistic regression  thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Extensive experimental results demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression is improved by one magnitude in general.,A Safe Screening Rule for Sparse Logistic Regression

Jie Wang

Arizona State University

Tempe  AZ 85287

jie.wang.ustc@asu.edu

Jiayu Zhou

Arizona State University

Tempe  AZ 85287

jiayu.zhou@asu.edu

Jun Liu

SAS Institute Inc.
Cary  NC 27513

Peter Wonka

Arizona State University

Tempe  AZ 85287

jun.liu@sas.com

peter.wonka@asu.edu

Jieping Ye

Arizona State University

Tempe  AZ 85287

jieping.ye@asu.edu

Abstract

The (cid:96)1-regularized logistic regression (or sparse logistic regression) is a widely
used method for simultaneous classiﬁcation and feature selection. Although many
recent efforts have been devoted to its efﬁcient implementation  its application to
high dimensional data still poses signiﬁcant challenges. In this paper  we present a
fast and effective sparse logistic regression screening rule (Slores) to identify the
“0” components in the solution vector  which may lead to a substantial reduction
in the number of features to be entered to the optimization. An appealing feature
of Slores is that the data set needs to be scanned only once to run the screening and
its computational cost is negligible compared to that of solving the sparse logistic
regression problem. Moreover  Slores is independent of solvers for sparse logis-
tic regression  thus Slores can be integrated with any existing solver to improve
the efﬁciency. We have evaluated Slores using high-dimensional data sets from
different applications. Experiments demonstrate that Slores outperforms the ex-
isting state-of-the-art screening rules and the efﬁciency of solving sparse logistic
regression can be improved by one magnitude.

1

Introduction

Logistic regression (LR) is a popular and well established classiﬁcation method that has been widely
used in many domains such as machine learning [4  7]  text mining [3  8]  image processing [9  15] 
bioinformatics [1  13  19  27  28]  medical and social sciences [2  17] etc. When the number of
feature variables is large compared to the number of training samples  logistic regression is prone
to over-ﬁtting. To reduce over-ﬁtting  regularization has been shown to be a promising approach.
Typical examples include (cid:96)2 and (cid:96)1 regularization. Although (cid:96)1 regularized LR is more challenging
to solve compared to (cid:96)2 regularized LR  it has received much attention in the last few years and
the interest in it is growing [20  25  28] due to the increasing prevalence of high-dimensional data.
The most appealing property of (cid:96)1 regularized LR is the sparsity of the resulting models  which is
equivalent to feature selection.
In the past few years  many algorithms have been proposed to efﬁciently solve the (cid:96)1 regularized
LR [5  12  11  18]. However  for large-scale problems  solving the (cid:96)1 regularized LR with higher
accuracy remains challenging. One promising solution is by “screening”  that is  we ﬁrst identify
the “inactive” features  which have 0 coefﬁcients in the solution and then discard them from the
optimization. This would result in a reduced feature matrix and substantial savings in computational
cost and memory size. In [6]  El Ghaoui et al. proposed novel screening rules  called “SAFE” 
to accelerate the optimization for a class of (cid:96)1 regularized problems  including LASSO [21]  (cid:96)1

1

regularized LR and (cid:96)1 regularized support vector machines. Inspired by SAFE  Tibshirani et al.
[22] proposed “strong rules” for a large class of (cid:96)1 regularized problems  including LASSO  elastic
net  (cid:96)1 regularized LR and more general convex problems. In [26]  Xiang et al. proposed “DOME”
rules to further improve SAFE rules for LASSO based on the observation that SAFE rules can be
understood as a special case of the general “sphere test”. Although both strong rules and the sphere
tests are more effective in discarding features than SAFE for solving LASSO  it is worthwhile to
mention that strong rules may mistakenly discard features that have non-zero coefﬁcients in the
solution and the sphere tests are not easy to be generalized to handle the (cid:96)1 regularized LR. To the
best of our knowledge  the SAFE rule is the only screening test for the (cid:96)1 regularized LR that is
“safe”  that is  it only discards features that are guaranteed to be absent from the resulting models.
In this paper  we develop novel screening rules  called
“Slores”  for the (cid:96)1 regularized LR. The proposed screen-
ing tests detect inactive features by estimating an upper
bound of the inner product between each feature vector
and the “dual optimal solution” of the (cid:96)1 regularized L-
R  which is unknown. The more accurate the estimation
is  the more inactive features can be detected. An accu-
rate estimation of such an upper bound turns out to be
quite challenging. Indeed most of the key ideas/insights
behind existing “safe” screening rules for LASSO heavi-
ly rely on the least square loss  which are not applicable
for the (cid:96)1 regularized LR case due to the presence of the
logistic loss. To this end  we propose a novel framework
to accurately estimate an upper bound. Our key techni-
cal contribution is to formulate the estimation of an upper
bound of the inner product as a constrained convex optimization problem and show that it admits
a closed form solution. Therefore  the estimation of the inner product can be computed efﬁciently.
Our extensive experiments have shown that Slores discards far more features than SAFE yet requires
much less computational efforts. In contrast with strong rules  Slores is “safe”  i.e.  it never discards
features which have non-zero coefﬁcients in the solution.
To illustrate the effectiveness of Slores  we compare Slores  strong rule and SAFE on a data set of
prostate cancer along a sequence of 86 parameters equally spaced on the λ/λmax scale from 0.1 to
0.95  where λ is the parameter for the (cid:96)1 penalty and λmax is the smallest tuning parameter [10] such
that the solution of the (cid:96)1 regularized LR is 0 [please refer to Eq. (1)]. The data matrix contains 132
patients with 15154 features. To measure the performance of different screening rules  we compute
the rejection ratio which is the ratio between the number of features discarded by screening rules
and the number of features with 0 coefﬁcients in the solution. Therefore  the larger the rejection
ratio is  the more effective the screening rule is. The results are shown in Fig. 1. We can see that
Slores discards far more features than SAFE especially when λ/λmax is large while the strong rule
is not applicable when λ/λmax ≤ 0.5. We present more results and discussions to demonstrate the
effectiveness of Slores in Section 6. For proofs of the lemmas  corollaries  and theorems  please
refer to the long version of this paper [24].

Figure 1: Comparison of Slores  strong
rule and SAFE on the prostate cancer
data set.

2 Basics and Motivations

In this section  we brieﬂy review the basics of the (cid:96)1 regularized LR and then motivate the general
screening rules via the KKT conditions. Suppose we are given a set of training samples {xi}m
and the associate labels b ∈ (cid:60)m  where xi ∈ (cid:60)p and bi ∈ {1 −1} for all i ∈ {1  . . .   m}. The (cid:96)1
i=1
regularized logistic regression is:

m(cid:88)

i=1

min
β c

1
m

log(1 + exp(−(cid:104)β  ¯xi(cid:105) − bic)) + λ(cid:107)β(cid:107)1 

(LRPλ)

where β ∈ (cid:60)p and c ∈ (cid:60) are the model parameters to be estimated  ¯xi = bixi  and λ > 0 is the
tuning parameter. We denote by X ∈ (cid:60)m×p the data matrix with the ith row being ¯xi and the jth
column being ¯xj.

2

Let C = {θ ∈ (cid:60)m : θi ∈ (0  1)  i = 1  . . .   m} and f (y) = y log(y) + (1 − y) log(1 − y) for
y ∈ (0  1). The dual problem of (LRPλ) [24] is given by

(cid:41)

f (θi) : (cid:107) ¯XT θ(cid:107)∞ ≤ mλ (cid:104)θ  b(cid:105) = 0  θ ∈ C

.

(cid:40)

min

θ

g(θ) =

m(cid:88)

i=1

1
m

(LRDλ)
λ  c∗

To simplify notations  we denote the feasible set of problem (LRDλ) as Fλ  and let (β∗
λ) and
θ∗
λ be the optimal solutions of problems (LRPλ) and (LRDλ) respectively. In [10]  the authors have
shown that for some special choice of the tuning parameter λ  both of (LRPλ) and (LRDλ) have
closed form solutions. In fact  let P = {i : bi = 1}  N = {i : bi = −1}  and m+ and m− be the
cardinalities of P and N respectively. We deﬁne
λmax = 1

(cid:107)∞ 

(1)

λmax

where

[θ∗

λmax

]i =

m(cid:107) ¯XT θ∗

(cid:40) m−
m   if i ∈ P 
m   if i ∈ N  

m+

i = 1  . . .   m.

(2)

([·]i denotes the ith component of a vector.) Then  it is known [10] that β∗
whenever λ ≥ λmax. When λ ∈ (0  λmax]  it is known that (LRDλ) has a unique optimal solution
[24]. We can now write the KKT conditions of problems (LRPλ) and (LRDλ) as

λ = 0 and θ∗

λ = θ∗

λmax

mλ 

−mλ 
[−mλ  mλ] 

(cid:104)θ∗
λ  ¯xj(cid:105) ∈

if [β∗
if [β∗
if [β∗

λ]j > 0 
λ]j < 0 
λ]j = 0.

j = 1  . . .   p.

(3)

In view of Eq. (3)  we can see that

|(cid:104)θ∗

λ  ¯xj(cid:105)| < mλ ⇒ [β∗

λ]j = 0.

λ. Although it is unknown  we can still estimate a region Aλ which contains θ∗

(R1)
In other words  if |(cid:104)θ∗
λ  ¯xj(cid:105) < mλ  then the KKT conditions imply that the coefﬁcient of ¯xj in the
solution β∗
λ is 0 and thus the jth feature can be safely removed from the optimization of (LRPλ).
However  for the general case in which λ < λmax  (R1) is not applicable since it assumes the
knowledge of θ∗
λ. As
a result  if maxθ∈Aλ |(cid:104)θ  ¯xj(cid:105)| < mλ  we can also conclude that [β∗
λ]j = 0 by (R1). In other words 
(R1) can be relaxed as
|(cid:104)θ  ¯xj(cid:105)| < mλ ⇒ [β∗
(R1(cid:48))
In this paper  (R1(cid:48)) serves as the foundation for constructing our screening rules  Slores. From
(R1(cid:48))  it is easy to see that screening rules with smaller T (θ∗
λ  ¯xj) are more aggressive in discarding
λ  ¯xj)  we need to restrict the region Aλ which includes
features. To give a tight estimation of T (θ∗
θ∗
λ as small as possible. In Section 3  we show that the estimation of the upper bound T (θ∗
λ  ¯xj)
can be obtained via solving a convex optimization problem. We show in Section 4 that the convex
optimization problem admits a closed form solution and derive Slores in Section 5 based on (R1(cid:48)).

λ  ¯xj) := max
θ∈Aλ

λ]j = 0.

T (θ∗

3 Estimating the Upper Bound via Solving a Convex Optimization Problem
λ  ¯xj(cid:105)|. In
In this section  we present a novel framework to estimate an upper bound T (θ∗
the subsequent development  we assume a parameter λ0 and the corresponding dual optimal θ∗
are
given. In our Slores rule to be presented in Section 5  we set λ0 and θ∗
λ0
given
in Eqs. (1) and (2). We formulate the estimation of T (θ∗
λ  ¯xj) as a constrained convex optimization
problem in this section  which will be shown to admit a closed form solution in Section 4.
θi(1−θi) ≥ 4
)  [∇2g(θ)]i i = 1
For the dual function g(θ)  it follows that [∇g(θ)]i = 1
m log( θi
m .
1−θi
Since ∇2g(θ) is a diagonal matrix  it follows that ∇2g(θ) (cid:23) 4
m I  where I is the identity matrix.
Thus  g(θ) is strongly convex with modulus µ = 4
m [16]. Rigorously  we have the following lemma.
Lemma 1. Let λ > 0 and θ1  θ2 ∈ Fλ  then

λ  ¯xj) of |(cid:104)θ∗
to be λmax and θ∗

λmax

λ0

m

1

a).
(4)
b). If θ1 (cid:54)= θ2  the inequality in (4) becomes a strict inequality  i.e.  “≥” becomes “>”.

g(θ2) − g(θ1) ≥ (cid:104)∇g(θ1)  θ2 − θ1(cid:105) + 2

m(cid:107)θ2 − θ1(cid:107)2
2.

3

belong to Fλ0. Therefore  Lemma 1 can

λ0
. In fact  we have the following theorem.

Given λ ∈ (0  λ0]  it is easy to see that both of θ∗
be a useful tool to bound θ∗
Theorem 2. Let λmax ≥ λ0 > λ > 0  then the following holds:

λ with the knowledge of θ∗

λ and θ∗

λ0

(cid:104)

(cid:16) λ

(cid:17) − g(θ∗

(cid:16)

a).
b). If θ∗

(cid:107)θ∗
λ (cid:54)= θ∗

λ0

g

λ0

θ∗

2 ≤ m
(cid:107)2
2

λ − θ∗
  the inequality in (5) becomes a strict inequality  i.e.  “≤” becomes “<”.
λ is inside a ball centred at θ∗

with radius

1 − λ

)  θ∗

) +

λ0

λ0

λ0

λ0

λ0

λ0

λ0

(5)

Theorem 2 implies that θ∗

(cid:17)(cid:104)∇g(θ∗

(cid:105)(cid:105)

(cid:114)

(cid:104)

(cid:16) λ

λ0

θ∗

λ0

(cid:17) − g(θ∗

r =

m
2

g

) + (1 − λ

λ0

)(cid:104)∇g(θ∗

λ0

)  θ∗

λ0

λ0

.

(6)

(cid:105)(cid:105)

Recall that to make our screening rules more aggressive in discarding features  we need to get a tight
λ  ¯xj(cid:105)| [please see (R1(cid:48))]. Thus  it is desirable to further restrict the
upper bound T (θ∗
possible region Aλ of θ∗

λ  ¯xj) of |(cid:104)θ∗

λ. Clearly  we can see that

(cid:104)θ∗
λ  b(cid:105) = 0

(7)
  ¯xj(cid:105) =

(8)

(9)

λ is feasible for problem (LRDλ). On the other hand  we call the set Iλ0 = {j : (cid:104)θ∗
λ0
. We have the following lemma for the active set.
λ of problem (LRDλ)  the active set Iλ = {j : |(cid:104)θ∗

since θ∗
|mλ0|  j = 1  . . .   p} the “active set” of θ∗
Lemma 3. Given the optimal solution θ∗
mλ  j = 1  . . .   p} is not empty if λ ∈ (0  λmax].
Since λ0 ∈ (0  λmax]  we can see that Iλ0 is not empty by Lemma 3. We pick j0 ∈ Iλ0 and set

λ0

λ  ¯xj(cid:105)| =

It follows that (cid:104)¯x∗  θ∗

λ0

λ for problem (LRDλ)  θ∗

λ satisﬁes

¯x∗ = sign((cid:104)θ∗

  ¯xj0(cid:105))¯xj0 .

λ0

(cid:105) = mλ0. Due to the feasibility of θ∗
(cid:104)θ∗
λ  ¯x∗(cid:105) ≤ mλ.

As a result  Theorem 2  Eq. (7) and (9) imply that θ∗

λ is contained in the following set:

Since θ∗

λ ∈ Aλ

λ0

Aλ

λ0

:= {θ : (cid:107)θ − θ∗

λ0

2 ≤ r2 (cid:104)θ  b(cid:105) = 0 (cid:104)θ  ¯x∗(cid:105) ≤ mλ}.
(cid:107)2

  we can see that |(cid:104)θ∗
T (θ∗

λ  ¯xj(cid:105)| ≤ maxθ∈Aλ
λ  ¯xj; θ∗

λ0

λ0

) := max
θ∈Aλ
λ0

|(cid:104)θ  ¯xj(cid:105)|. Therefore  (R1(cid:48)) implies that if
|(cid:104)θ  ¯xj(cid:105)|

(UBP)

is smaller than mλ  we can conclude that [β∗
of (LRPλ). Notice that  we replace the notations Aλ and T (θ∗
λ  ¯xj; θ∗
to emphasize their dependence on θ∗
would be an applicable screening rule to discard features which have 0 coefﬁcients in β∗
closed form solution of problem (UBP) in the next section.

λ]j = 0 and ¯xj can be discarded from the optimization
) and Aλ
λ0
)  (R1(cid:48))
λ. We give a

. Clearly  as long as we can solve for T (θ∗

λ  ¯xj) with T (θ∗

λ  ¯xj; θ∗

λ0

λ0

λ0

4

Solving the Convex Optimization Problem (UBP)

In this section  we show how to solve the convex optimization problem (UBP) based on the standard
Lagrangian multiplier method. We ﬁrst transform problem (UBP) into a pair of convex minimization
problem (UBP(cid:48)) via Eq. (11) and then show that the strong duality holds for (UBP(cid:48)) in Lemma 6. The
strong duality guarantees the applicability of the Lagrangian multiplier method. We then give the
closed form solution of (UBP(cid:48)) in Theorem 8. After we solve problem (UBP(cid:48))  it is straightforward
to compute the solution of problem (UBP) via Eq. (11).
Before we solve (UBP) for the general case  it is worthwhile to mention a special case in which
P¯xj = ¯xj − (cid:104)¯xj  b(cid:105)
b = 0. Clearly  P is the projection operator which projects a vector onto the
(cid:107)b(cid:107)2
orthogonal complement of the space spanned by b. In fact  we have the following theorem.
Theorem 4. Let λmax ≥ λ0 > λ > 0  and assume θ∗
then T (θ∗

is known. For j ∈ {1  . . .   p}  if P¯xj = 0 

) = 0.

λ0

2

λ  ¯xj; θ∗

λ0

4

Because of (R1(cid:48))  we immediately have the following corollary.
Corollary 5. Let λ ∈ (0  λmax) and j ∈ {1  . . .   p}. If P¯xj = 0  then [β∗
For the general case in which P¯xj (cid:54)= 0  let

λ]j = 0.

T+(θ∗

λ  ¯xj; θ∗

λ0

) := max
θ∈Aλ
λ0

Clearly  we have

(cid:104)θ  +¯xj(cid:105)  T−(θ∗

λ  ¯xj; θ∗

λ0

(cid:104)θ −¯xj(cid:105).

) := max
θ∈Aλ
λ0

T (θ∗

λ  ¯xj; θ∗

λ0

) = max{T+(θ∗

λ  ¯xj; θ∗

λ0

)  T−(θ∗

λ  ¯xj; θ∗

λ0

)}.

Therefore  we can solve problem (UBP) by solving the two sub-problems in (10).
Let ξ ∈ {+1 −1}. Then problems in (10) can be written uniformly as

Tξ(θ∗

λ  ¯xj; θ∗

λ0

(cid:104)θ  ξ¯xj(cid:105).

) = max
θ∈Aλ
λ0

(10)

(11)

(UBPs)

To make use of the standard Lagrangian multiplier method  we transform problem (UBPs) to the
following minimization problem:

(UBP(cid:48))

−Tξ(θ∗

λ  ¯xj; θ∗

(cid:104)θ −ξ¯xj(cid:105)

λ0

λ0

λ0

) = min
θ∈Aλ
λ0
(cid:104)θ −ξ¯xj(cid:105).
is known. The strong duality holds for problem

(cid:104)θ  ξ¯xj(cid:105) = − minθ∈Aλ
by noting that maxθ∈Aλ
Lemma 6. Let λmax ≥ λ0 > λ > 0 and assume θ∗
(UBP(cid:48)). Moreover  problem (UBP(cid:48)) admits an optimal solution in Aλ
Because the strong duality holds for problem (UBP(cid:48)) by Lemma 6  the Lagrangian multiplier method
is applicable for (UBP(cid:48)). In general  we need to ﬁrst solve the dual problem and then recover the
optimal solution of the primal problem via KKT conditions. Recall that r and ¯x∗ are deﬁned by
Eq. (6) and (8) respectively. Lemma 7 derives the dual problems of (UBP(cid:48)) for different cases.
Lemma 7. Let λmax ≥ λ0 > λ > 0 and assume θ∗
let ¯x = −ξ¯xj. Denote

is known. For j ∈ {1  . . .   p} and P¯xj (cid:54)= 0 

λ0

λ0

λ0

.

(cid:110)

(cid:111)

.

U1 = {(u1  u2) : u1 > 0  u2 ≥ 0} and U2 =

(u1  u2) : u1 = 0  u2 = −(cid:104)P¯x P¯x∗(cid:105)
(cid:107)P¯x∗(cid:107)2

2

a). If

(cid:104)P¯x P¯x∗(cid:105)

(cid:107)P¯x(cid:107)2(cid:107)P¯x∗(cid:107)2

∈ (−1  1]  the dual problem of (UBP(cid:48)) is equivalent to:
2 + u2m(λ0 − λ) + (cid:104)θ∗

(cid:107)P¯x + u2P¯x∗(cid:107)2

¯g(u1  u2) = − 1

λ0

2u1

max

(u1 u2)∈U1

  ¯x(cid:105) − 1

2 u1r2.

(UBD(cid:48))

Moreover  ¯g(u1  u2) attains its maximum in U1.
b). If

(cid:104)P¯x P¯x∗(cid:105)

= −1  the dual problem of (UBP(cid:48)) is equivalent to:

(cid:107)P¯x(cid:107)2(cid:107)P¯x∗(cid:107)2

(cid:40)

max

(u1 u2)∈U1∪U2

¯¯g(u1  u2) =

¯g(u1  u2) 
− (cid:107)P¯x(cid:107)2
(cid:107)P¯x∗(cid:107)2

mλ 

if (u1  u2) ∈ U1 
if (u1  u2) ∈ U2.

(UBD(cid:48)(cid:48))

We can now solve problem (UBP(cid:48)) in the following theorem.
Theorem 8. Let λmax ≥ λ0 > λ > 0  d = m(λ0−λ)
r(cid:107)P¯x∗(cid:107)2
and P¯xj (cid:54)= 0  let ¯x = −ξ¯xj.

and assume θ∗

λ0

is known. For j ∈ {1  . . .   p}

a). If

(cid:104)P¯x P¯x∗(cid:105)

(cid:107)P¯x(cid:107)2(cid:107)P¯x∗(cid:107)2

≥ d  then

Tξ(θ∗

λ  ¯xj; θ∗

λ0

) = r(cid:107)P¯x(cid:107)2 − (cid:104)θ∗

λ0

  ¯x(cid:105);

(12)

5

b). If

(cid:104)P¯x P¯x∗(cid:105)

(cid:107)P¯x(cid:107)2(cid:107)P¯x∗(cid:107)2

< d  then
λ  ¯xj; θ∗

λ0

Tξ(θ∗

) = r(cid:107)P¯x + u∗

2P¯x∗(cid:107)2 − u∗

2m(λ0 − λ) − (cid:104)θ∗

λ0

where

∆

2a2

√
2 = −a1+
u∗
 
a2 = (cid:107)P¯x∗(cid:107)4
2(1 − d2) 
a1 = 2(cid:104)P¯x  P¯x∗(cid:105)(cid:107)P¯x∗(cid:107)2
a0 = (cid:104)P¯x  P¯x∗(cid:105)2 − d2(cid:107)P¯x(cid:107)2
∆ = a2

2(cid:107)P¯x∗(cid:107)2
2 
1 − 4a2a0 = 4d2(1 − d2)(cid:107)P¯x∗(cid:107)4

2(1 − d2) 

  ¯x(cid:105) 

(13)

(14)

2((cid:107)P¯x(cid:107)2

2(cid:107)P¯x∗(cid:107)2

2 − (cid:104)P¯x  P¯x∗(cid:105)2).

Notice that  although the dual problems of (UBP(cid:48)) in Lemma 7 are different  the resulting upper
bound Tξ(θ∗
) can be given by Theorem 8 in a uniform way. The tricky part is how to deal
with the extremal cases in which

∈ {−1  +1}.

λ  ¯xj; θ∗

(cid:104)P¯x P¯x∗(cid:105)

λ0

(cid:107)P¯x(cid:107)2(cid:107)P¯x∗(cid:107)2

5 The proposed Slores Rule for (cid:96)1 Regularized Logistic Regression
Using (R1(cid:48))  we are now ready to construct the screening rules for the (cid:96)1 Regularized Logistic
Regression. By Corollary 5  we can see that the orthogonality between the jth feature and the
response vector b implies the absence of ¯xj from the resulting model. For the general case in which
P¯xj (cid:54)= 0  (R1(cid:48)) implies that if T (θ∗
)} < mλ 
then the jth feature can be discarded from the optimization of (LRPλ). Notice that  letting ξ = ±1 
λ  ¯xj; θ∗
T+(θ∗
) have been solved by Theorem 8. Rigorously  we have the
following theorem.
Theorem 9 (Slores). Let λ0 > λ > 0 and assume θ∗

) = max{T+(θ∗

) and T−(θ∗

λ  ¯xj; θ∗

λ  ¯xj; θ∗

λ  ¯xj; θ∗

λ  ¯xj; θ∗

)  T−(θ∗

is known.

λ0

λ0

λ0

λ0

λ0

λ0

1. If λ ≥ λmax  then β∗
2. If λmax ≥ λ0 > λ > 0 and either of the following holds:

λ = 0;

(a) P¯xj = 0 
(b) max{Tξ(θ∗
then [β∗
λ]j = 0.

λ  ¯xj; θ∗

λ0

) : ξ = ±1} < mλ 

the Slores rule as summarized below in Algorithm 1.

Algorithm 1 R = Slores(X  b  λ  λ0  θ∗

)

λ0

λ]R and c∗
λ.

Based on Theorem 9  we construct
Notice that  the output R of Slores is the indices
of the features that need to be entered to the
optimization. As a result  suppose the output
of Algorithm 1 is R = {j1  . . .   jk}  we can
substitute the full matrix X in problem (LRPλ)
with the sub-matrix XR = (¯xj1   . . .   ¯xjk ) and
just solve for [β∗
On the other hand  Algorithm 1 implies that
Slores needs ﬁve inputs. Since X and b come
with the data and λ is chosen by the user  we on-
ly need to specify θ∗
and λ0. In other words 
we need to provide Slores with a dual opti-
mal solution of problem (LRDλ) for an arbi-
trary parameter. A natural choice is by setting
λ0 = λmax and θ∗
= θ∗
given by Eq. (1)
and Eq. (2) in closed form.

λmax

λ0

λ0

Initialize R := {1  . . .   p};
if λ ≥ λmax then
else

set R = ∅;

for j = 1 to p do

remove j from R;

if P¯xj = 0 then
else if max{Tξ(θ∗
then

remove j from R;

end if
end for
end if
Return: R

λ  ¯xj; θ∗

λ0 ) : ξ = ±1} < mλ

6 Experiments

We evaluate our screening rules using the newgroup data set [10] and Yahoo web pages data sets
[23]. The newgroup data set is cultured from the data by Koh et al. [10]. The Yahoo data set-
s include 11 top-level categories  each of which is further divided into a set of subcategories. In

6

our experiment we construct ﬁve balanced binary classiﬁcation datasets from the topics of Com-
puters  Education  Health  Recreation  and Science. For each topic  we choose samples from
one subcategory as the positive class and randomly sample an equal number of samples from the
rest of subcategories as the negative class. The statistics of the data sets are given in Table 1.

Health

Recreation

Science

m

Table 1: Statistics of the test data sets.
Data set
newsgroup
Computers
Education

p

no. nonzeros

61188
25259
20782
18430
25095
24002

11269
216
254
228
370
222

1467345
23181
28287
40145
49986
37227

Table 2: Running time (in seconds) of Slores 
strong rule  SAFE and the solver.
SAFE
1128.65

We compare the performance of Slores and the
strong rule which achieves state-of-the-art per-
formance for (cid:96)1 regularized LR. We do not in-
clude SAFE because it is less effective in dis-
carding features than strong rules and requires
much higher computational time [22]. Fig. 1
has shown the performance of Slores  strong
rule and SAFE. We compare the efﬁciency of
the three screening rules on the same prostate
cancer data set in Table 2. All of the screen-
ing rules are tested along a sequence of 86 pa-
rameter values equally spaced on the λ/λmax
scale from 0.1 to 0.95. We repeat the procedure
100 times and during each time we undersam-
ple 80% of the data. We report the total running time of the three screening rules over the 86 values
of λ/λmax in Table 2. For reference  we also report the total running time of the solver1. We observe
that the running time of Slores and strong rule is negligible compared to that of the solver. However 
SAFE takes much longer time even than the solver.
In Section 6.1  we evaluate the performance of Slores and strong rule. Recall that we use the re-
jection ratio  i.e.  the ratio between the number of features discarded by the screening rules and the
number of features with 0 coefﬁcients in the solution  to measure the performance of screening rules.
Note that  because no features with non-zero coefﬁcients in the solution would be mistakenly dis-
carded by Slores  its rejection ratio is no larger than one. We then compare the efﬁciency of Slores
and strong rule in Section 6.2.
The experiment settings are as follows. For each data set  we undersample 80% of the date and
run Slores and strong rules along a sequence of 86 parameter values equally spaced on the λ/λmax
scale from 0.1 to 0.95. We repeat the procedure 100 times and report the average performance and
running time at each of the 86 values of λ/λmax. Slores  strong rules and SAFE are all implemented
in Matlab. All of the experiments are carried out on a Intel(R) (i7-2600) 3.4Ghz processor.

Solver
10.56

Slores
0.37

Strong Rule

0.33

6.1 Comparison of Performance

In this experiment  we evaluate the performance of the Slores and the strong rule via the rejection
ratio. Fig. 2 shows the rejection ratio of Slores and strong rule on six real data sets. When λ/λmax >
0.5  we can see that both Slores and strong rule are able to identify almost 100% of the inactive
features  i.e.  features with 0 coefﬁcients in the solution vector. However  when λ/λmax ≤ 0.5 
strong rule can not detect the inactive features. In contrast  we observe that Slores exhibits much
stronger capability in discarding inactive features for small λ  even when λ/λmax is close to 0.1.
Taking the data point at which λ/λmax = 0.1 for example  Slores discards about 99% inactive
features for the newsgroup data set. For the other data sets  more than 80% inactive features are
identiﬁed by Slores. Thus  in terms of rejection ratio  Slores signiﬁcantly outperforms the strong
rule. Moreover  the discarded features by Slores are guaranteed to have 0 coefﬁcients in the solution.
But strong rule may mistakenly discard features which have non-zero coefﬁcients in the solution.

6.2 Comparison of Efﬁciency

We compare efﬁciency of Slores and the strong rule in this experiment. The data sets for evaluating
the rules are the same as Section 6.1. The running time of the screening rules reported in Fig. 3
includes the computational cost of the rules themselves and that of the solver after screening. We
plot the running time of the screening rules against that of the solver without screening. As indicated
by Fig. 2  when λ/λmax > 0.5  Slores and strong rule discards almost 100% of the inactive features.

1In this paper  the ground truth is computed by SLEP [14].

7

(a) newsgroup

(b) Computers

(c) Education

(d) Health

(e) Recreation

(f) Science

Figure 2: Comparison of the performance of Slores and strong rules on six real data sets.

(a) newsgroup

(b) Computers

(c) Education

(d) Health

(e) Recreation

(f) Science

Figure 3: Comparison of the efﬁciency of Slores and strong rule on six real data sets.

As a result  the size of the feature matrix involved in the optimization of problem (LRPλ) is greatly
reduced. From Fig. 3  we can observe that the efﬁciency is improved by about one magnitude on
average compared to that of the solver without screening. However  when λ/λmax < 0.5  strong
rule can not identify any inactive features and thus the running time is almost the same as that of the
solver without screening. In contrast  Slores is still able to identify more than 80% of the inactive
features for the data sets cultured from the Yahoo web pages data sets and thus the efﬁciency is
improved by roughly 5 times. For the newgroup data set  about 99% inactive features are identiﬁed
by Slores which leads to about 10 times savings in running time. These results demonstrate the
power of the proposed Slores rule in improving the efﬁciency of solving the (cid:96)1 regularized LR.

7 Conclusions
In this paper  we propose novel screening rules to effectively discard features for (cid:96)1 regularized
LR. Extensive numerical experiments on real data demonstrate that Slores outperforms the existing
state-of-the-art screening rules. We plan to extend the framework of Slores to more general sparse
formulations  including convex ones  like group Lasso  fused Lasso  (cid:96)1 regularized SVM  and non-
convex ones  like (cid:96)p regularized problems where 0 < p < 1.

8

References

[1] M. Asgary  S. Jahandideh  P. Abdolmaleki  and A. Kazemnejad. Analysis and identiﬁcation of β-turn
types using multinomial logistic regression and artiﬁcial neural network. Bioinformatics  23(23):3125–
3130  2007.

[2] C. Boyd  M. Tolson  and W. Copes. Evaluating trauma care: The TRISS method  trauma score and the

injury severity score. Journal of Trauma  27:370–378  1987.

[3] J. R. Brzezinski and G. J. Knaﬂ. Logistic regression modeling for context-based classiﬁcation. In DEXA

Workshop  pages 755–759  1999.

[4] K. Chaudhuri and C. Monteleoni. Privacy-preserving logistic regression. In NIPS  2008.
[5] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Ann. Statist.  32:407–499 

2004.

[6] L. El Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination for the lasso and sparse supervised

learning problems. arXiv:1009.4219v2.

[7] J. Friedman  T. Hastie  and R. Tibshirani. Additive logistic regression: a statistical view of boosting. The

Annals of Statistics  38(2)  2000.

[8] A. Genkin  D. Lewis  and D. Madigan. Large-scale bayesian logistic regression for text categorization.

Technometrics  49:291–304(14)  2007.

[9] S. Gould  J. Rodgers  D. Cohen  G. Elidan  and D. Koller. Multi-class segmentation with relative location

prior. International Journal of Computer Vision  80(3):300–316  2008.

[10] K. Koh  S. J. Kim  and S. Boyd. An interior-point method for large scale (cid:96)1-regularized logistic regression.

J. Mach. Learn. Res.  8:1519–1555  2007.

[11] B. Krishnapuram  L. Carin  M. Figueiredo  and A. Hartemink. Sparse multinomial logistic regression:
Fast algorithms and generalization bounds. IEEE Trans. Pattern Anal. Mach. Intell.  27:957–968  2005.
[12] S. Lee  H. Lee  P. Abbeel  and A. Ng. Efﬁcient l1 regularized logistic regression. In In AAAI-06  2006.
[13] J. Liao and K. Chin. Logistic regression for disease classiﬁcation using microarray data: model selection

in a large p and small n case. Bioinformatics  23(15):1945–1951  2007.

[14] J. Liu  S. Ji  and J. Ye. SLEP: Sparse Learning with Efﬁcient Projections. Arizona State University  2009.
[15] S. Martins  L. Sousa  and J. Martins. Additive logistic regression applied to retina modelling. In ICIP (3) 

pages 309–312. IEEE  2007.

[16] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer  2004.
[17] S. Palei and S. Das. Logistic regression model for prediction of roof fall risks in bord and pillar workings

in coal mines: An approach. Safety Science  47:88–96  2009.

[18] M. Park and T. Hastie. (cid:96)1 regularized path algorithm for generalized linear models. J. R. Statist. Soc. B 

69:659–677  2007.

[19] M. Sartor  G. Leikauf  and M. Medvedovic. LRpath: a logistic regression approach for identifying en-

riched biological groups in gene expression data. Bioinformatics  25(2):211–217  2009.

[20] D. Sun  T. Erp  P. Thompson  C. Bearden  M. Daley  L. Kushan  M. Hardt  K. Nuechterlein  A. Toga  and
T. Cannon. Elucidating a magnetic resonance imaging-based neuroanatomic biomarker for psychosis:
classiﬁcation analysis using probabilistic brain atlas and machine learning algorithms. Biological Psychi-
atry  66:1055–1–60  2009.

[21] R. Tibshirani. Regression shringkage and selection via the lasso. J. R. Statist. Soc. B  58:267–288  1996.
[22] R. Tibshirani  J. Bien  J. Friedman  T. Hastie  N. Simon  J. Taylor  and R. Tibshirani. Strong rules for

discarding predictors in lasso-type problems. J. R. Statist. Soc. B  74:245–266  2012.

[23] N. Ueda and K. Saito. Parametric mixture models for multi-labeled text. Advances in neural information

processing systems  15:721–728  2002.

[24] J. Wang  J. Zhou  J. Liu  P. Wonka  and J. Ye. A safe screening rule for sparse logistic regression.

arXiv:1307.4145v2  2013.

[25] T. T. Wu  Y. F. Chen  T. Hastie  E. Sobel  and K. Lange. Genome-wide association analysis by lasso

penalized logistic regression. Bioinformatics  25:714–721  2009.

[26] Z. J. Xiang and P. J. Ramadge. Fast lasso screening tests based on correlations. In IEEE ICASSP  2012.
[27] J. Zhu and T. Hastie. Kernel logistic regression and the import vector machine.
In T. G. Dietterich 
S. Becker  and Z. Ghahramani  editors  NIPS  pages 1081–1088. MIT Press  2001.

[28] J. Zhu and T. Hastie. Classiﬁcation of gene microarrays by penalized logistic regression. Biostatistics 

5:427–443  2004.

9

,Jie Wang
Jiayu Zhou
Jun Liu
Peter Wonka
Jieping Ye
Dangna Li
Kun Yang
Wing Hung Wong
Miguel Carreira-Perpinan
Pooya Tavallali