2014,Ranking via Robust Binary Classification,We propose RoBiRank  a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification. The algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature. Further  in large scale problems where explicit feature vectors and scores are not given  our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386 133 x 49 824 519 pairwise interactions between items to be ranked  our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm  given the same amount of wall-clock time for computation.,Ranking via Robust Binary Classiﬁcation

Hyokun Yun

Amazon

Seattle  WA 98109

yunhyoku@amazon.com

Parameswaran Raman  S. V. N. Vishwanathan

Department of Computer Science

University of California
Santa Cruz  CA 95064

{params vishy}@ucsc.edu

Abstract

We propose RoBiRank  a ranking algorithm that is motivated by observing a close
connection between evaluation metrics for learning to rank and loss functions
for robust classiﬁcation. It shows competitive performance on standard bench-
mark datasets against a number of other representative algorithms in the literature.
We also discuss extensions of RoBiRank to large scale problems where explicit
feature vectors and scores are not given. We show that RoBiRank can be efﬁ-
ciently parallelized across a large number of machines; for a task that requires
386  133 × 49  824  519 pairwise interactions between items to be ranked  RoBi-
Rank ﬁnds solutions that are of dramatically higher quality than that can be found
by a state-of-the-art competitor algorithm  given the same amount of wall-clock
time for computation.

1

Introduction

Learning to rank is a problem of ordering a set of items according to their relevances to a given
context [8]. While a number of approaches have been proposed in the literature  in this paper we
provide a new perspective by showing a close connection between ranking and a seemingly unrelated
topic in machine learning  namely  robust binary classiﬁcation.
In robust classiﬁcation [13]  we are asked to learn a classiﬁer in the presence of outliers. Standard
models for classiﬁcation such as Support Vector Machines (SVMs) and logistic regression do not
perform well in this setting  since the convexity of their loss functions does not let them give up
their performance on any of the data points [16]; for a classiﬁcation model to be robust to outliers 
it has to be capable of sacriﬁcing its performance on some of the data points. We observe that
this requirement is very similar to what standard metrics for ranking try to evaluate. Discounted
Cumulative Gain (DCG) [17] and its normalized version NDCG  popular metrics for learning to
rank  strongly emphasize the performance of a ranking algorithm at the top of the list; therefore  a
good ranking algorithm in terms of these metrics has to be able to give up its performance at the
bottom of the list if that can improve its performance at the top.
In fact  we will show that DCG and NDCG can indeed be written as a natural generalization of robust
loss functions for binary classiﬁcation. Based on this observation we formulate RoBiRank  a novel
model for ranking  which maximizes the lower bound of (N)DCG. Although the non-convexity
seems unavoidable for the bound to be tight [9]  our bound is based on the class of robust loss
functions that are found to be empirically easier to optimize [10]. Indeed  our experimental results
suggest that RoBiRank reliably converges to a solution that is competitive as compared to other
representative algorithms even though its objective function is non-convex.
While standard deterministic optimization algorithms such as L-BFGS [19] can be used to estimate
parameters of RoBiRank  to apply the model to large-scale datasets a more efﬁcient parameter es-
timation algorithm is necessary. This is of particular interest in the context of latent collaborative

1

retrieval [24]; unlike standard ranking task  here the number of items to rank is very large and ex-
plicit feature vectors and scores are not given.
Therefore  we develop an efﬁcient parallel stochastic optimization algorithm for this problem. It has
two very attractive characteristics: First  the time complexity of each stochastic update is indepen-
dent of the size of the dataset. Also  when the algorithm is distributed across multiple number of
machines  no interaction between machines is required during most part of the execution; therefore 
the algorithm enjoys near linear scaling. This is a signiﬁcant advantage over serial algorithms  since
it is very easy to deploy a large number of machines nowadays thanks to the popularity of cloud
computing services  e.g. Amazon Web Services.
We apply our algorithm to latent collaborative retrieval task on Million Song Dataset [3] which con-
sists of 1 129 318 users  386 133 songs  and 49 824 519 records; for this task  a ranking algorithm
has to optimize an objective function that consists of 386  133 × 49  824  519 number of pairwise
interactions. With the same amount of wall-clock time given to each algorithm  RoBiRank leverages
parallel computing to outperform the state-of-the-art with a 100% lift on the evaluation metric.

2 Robust Binary Classiﬁcation

Suppose we are given training data which consists of n data points (x1  y1)  (x2  y2)  . . .   (xn  yn) 
where each xi ∈ Rd is a d-dimensional feature vector and yi ∈ {−1  +1} is a label associated
with it. A linear model attempts to learn a d-dimensional parameter ω  and for a given feature
L(ω) :=(cid:80)n
vector x it predicts label +1 if (cid:104)x  ω(cid:105) ≥ 0 and −1 otherwise. Here (cid:104)· ·(cid:105) denotes the Euclidean dot
product between two vectors. The quality of ω can be measured by the number of mistakes it makes:
i=1 I(yi · (cid:104)xi  ω(cid:105) < 0). The indicator function I(· < 0) is called the 0-1 loss function 
because it has a value of 1 if the decision rule makes a mistake  and 0 otherwise. Unfortunately 
since the 0-1 loss is a discrete function its minimization is difﬁcult [11]. The most popular solution
to this problem in machine learning is to upper bound the 0-1 loss by an easy to optimize function
[2]. For example  logistic regression uses the logistic loss function σ0(t) := log2(1 + 2−t)  to come
up with a continuous and convex objective function

n(cid:88)

i=1

L(ω) :=

σ0(yi · (cid:104)xi  ω(cid:105)) 

(1)

which upper bounds L(ω). It is clear that for each i  σ0(yi · (cid:104)xi  ω(cid:105)) is a convex function in ω;
therefore  L(ω)  a sum of convex functions  is also a convex function which is relatively easier to
optimize [6]. Support Vector Machines (SVMs) on the other hand can be recovered by using the
hinge loss to upper bound the 0-1 loss.
However  convex upper bounds such as L(ω) are known to be sensitive to outliers [16]. The basic
intuition here is that when yi · (cid:104)xi  ω(cid:105) is a very large negative number for some data point i  σ(yi ·
(cid:104)xi  ω(cid:105)) is also very large  and therefore the optimal solution of (1) will try to decrease the loss on
such outliers at the expense of its performance on “normal” data points.
In order to construct robust loss functions  consider the following two transformation functions:

ρ1(t) := log2(t + 1)  ρ2(t) := 1 −

1

log2(t + 2)

 

(2)

which  in turn  can be used to deﬁne the following loss functions:

σ1(t) := ρ1(σ0(t))  σ2(t) := ρ2(σ0(t)).

(3)
One can see that σ1(t) → ∞ as t → −∞  but at a much slower rate than σ0(t) does; its derivative
σ(cid:48)1(t) → 0 as t → −∞. Therefore  σ1(·) does not grow as rapidly as σ0(t) on hard-to-classify data
points. Such loss functions are called Type-I robust loss functions by Ding [10]  who also showed
that they enjoy statistical robustness properties. σ2(t) behaves even better: σ2(t) converges to a
constant as t → −∞  and therefore “gives up” on hard to classify data points. Such loss functions
are called Type-II loss functions  and they also enjoy statistical robustness properties [10].
In terms of computation  of course  σ1(·) and σ2(·) are not convex  and therefore the objective
function based on such loss functions is more difﬁcult to optimize. However  it has been observed

2

in Ding [10] that models based on optimization of Type-I functions are often empirically much
more successful than those which optimize Type-II functions. Furthermore  the solutions of Type-I
optimization are more stable to the choice of parameter initialization. Intuitively  this is because
Type-II functions asymptote to a constant  reducing the gradient to almost zero in a large fraction
of the parameter space; therefore  it is difﬁcult for a gradient-based algorithm to determine which
direction to pursue. See Ding [10] for more details.

3 Ranking Model via Robust Binary Classiﬁcation

where Wxy is a real-valued score given to item y in context x.

Let X = {x1  x2  . . .   xn} be a set of contexts  and Y = {y1  y2  . . .   ym} be a set of items to
be ranked. For example  in movie recommender systems X is the set of users and Y is the set of
movies. In some problem settings  only a subset of Y is relevant to a given context x ∈ X ; e.g.
in document retrieval systems  only a subset of documents is relevant to a query. Therefore  we
deﬁne Yx ⊂ Y to be a set of items relevant to context x. Observed data can be described by a set
W := {Wxy}x∈X  y∈Yx
We adopt a standard problem setting used in the literature of learning to rank. For each context x
and an item y ∈ Yx  we aim to learn a scoring function f (x  y) : X × Yx → R that induces a
ranking on the item set Yx; the higher the score  the more important the associated item is in the
given context. To learn such a function  we ﬁrst extract joint features of x and y  which will be
denoted by φ(x  y). Then  we parametrize f (· ·) using a parameter ω  which yields the linear model
fω(x  y) := (cid:104)φ(x  y)  ω(cid:105)  where  as before  (cid:104)· ·(cid:105) denotes the Euclidean dot product between two
vectors. ω induces a ranking on the set of items Yx; we deﬁne rankω(x  y) to be the rank of item y
in a given context x induced by ω. Observe that rankω(x  y) can also be written as a sum of 0-1 loss
functions (see e.g. Usunier et al. [23]):
rankω(x  y) =

(cid:88)

(4)

I (fω(x  y) − fω(x  y(cid:48)) < 0) .

y(cid:48)∈Yx y(cid:48)(cid:54)=y

3.1 Basic Model

(cid:88)

(cid:88)

If an item y is very relevant in context x  a good parameter ω should position y at the top of the list;
in other words  rankω(x  y) has to be small  which motivates the following objective function [7]:

cx

L(ω) :=

(5)
where cx is an weighting factor for each context x  and v(·) : R+ → R+ quantiﬁes the relevance
level of y on x. Note that {cx} and v(Wxy) can be chosen to reﬂect the metric the model is going to
be evaluated on (this will be discussed in Section 3.2). Note that (5) can be rewritten using (4) as a
sum of indicator functions. Following the strategy in Section 2  one can form an upper bound of (5)
by bounding each 0-1 loss function by a logistic loss function:

v(Wxy) · rankω(x  y) 

y∈Yx

x∈X

(cid:88)

(cid:88)

x∈X

y∈Yx

L(ω) :=

cx

(cid:88)

v (Wxy) ·

y(cid:48)∈Yx y(cid:48)(cid:54)=y

σ0 (fω(x  y) − fω(x  y(cid:48))) .

(6)

Just like (1)  (6) is convex in ω and hence easy to minimize.

3.2 DCG

Although (6) enjoys convexity  it may not be a good objective function for ranking. This is because
in most applications of learning to rank  it is more important to do well at the top of the list than at
the bottom  as users typically pay attention only to the top few items. Therefore  it is desirable to
give up performance on the lower part of the list in order to gain quality at the top. This intuition is
similar to that of robust classiﬁcation in Section 2; a stronger connection will be shown below.
Discounted Cumulative Gain (DCG) [17] is one of the most popular metrics for ranking. For each
context x ∈ X   it is deﬁned as:

DCG(ω) := cx

 

(7)

(cid:88)

v (Wxy)

log2 (rankω(x  y) + 2)

y∈Yx

3

1 −

(cid:26)
 (cid:88)
 (cid:88)

y(cid:48)∈Yx y(cid:48)(cid:54)=y

y(cid:48)∈Yx y(cid:48)(cid:54)=y

(cid:88)

(cid:88)

x∈X

y∈Yx

(cid:88)

(cid:88)

x∈X

y∈Yx

 .
 .

 .

where v(t) = 2t − 1 and cx = 1. Since 1/ log(t + 2) decreases quickly and then asymptotes to
a constant as t increases  this metric emphasizes the quality of the ranking at the top of the list.
Normalized DCG (NDCG) simply normalizes the metric to bound it between 0 and 1 by calculating
the maximum achievable DCG value mx and dividing by it [17].

3.3 RoBiRank

Now we formulate RoBiRank  which optimizes the lower bound of metrics for ranking in form (7).
Observe that maxω DCG(ω) can be rewritten as

(cid:27)

1

log2 (rankω(x  y) + 2)

.

(8)

(cid:88)

(cid:88)

min

ω

cx

x∈X

y∈Yx

v (Wxy) ·

Using (4) and the deﬁnition of the transformation function ρ2(·) in (2)  we can rewrite the objective
function in (8) as:

L2(ω) :=

cx

v (Wxy) · ρ2

I (fω(x  y) − fω(x  y(cid:48)) < 0)

(9)

Since ρ2(·) is a monotonically increasing function  we can bound (9) with a continuous function by
bounding each indicator function using the logistic loss:

L2(ω) :=

cx

v (Wxy) · ρ2

σ0 (fω(x  y) − fω(x  y(cid:48)))

(10)

This is reminiscent of the basic model in (6); as we applied the transformation ρ2(·) on the logistic
loss σ0(·) to construct the robust loss σ2(·) in (3)  we are again applying the same transformation
on (6) to construct a loss function that respects the DCG metric used in ranking. In fact  (10) can be
seen as a generalization of robust binary classiﬁcation by applying the transformation on a group of
logistic losses instead of a single loss. In both robust classiﬁcation and ranking  the transformation
ρ2(·) enables models to give up on part of the problem to achieve better overall performance.
As we discussed in Section 2  however  transformation of logistic loss using ρ2(·) results in Type-II
loss function  which is very difﬁcult to optimize. Hence  instead of ρ2(·) we use an alternative trans-
formation ρ1(·)  which generates Type-I loss function  to deﬁne the objective function of RoBiRank:

(cid:88)

(cid:88)

x∈X

y∈Yx

 (cid:88)

y(cid:48)∈Yx y(cid:48)(cid:54)=y

L1(ω) :=

cx

v (Wxy) · ρ1

σ0 (fω(x  y) − fω(x  y(cid:48)))

(11)

Since ρ1(t) ≥ ρ2(t) for every t > 0  we have L1(ω) ≥ L2(ω) ≥ L2(ω) for every ω. Note
that L1(ω) is continuous and twice differentiable. Therefore  standard gradient-based optimization
techniques can be applied to minimize it. As is standard  a regularizer on ω can be added to avoid
overﬁtting; for simplicity  we use the (cid:96)2-norm in our experiments.

3.4 Standard Learning to Rank Experiments

We conducted experiments to check the performance of RoBiRank (11) in a standard learning to
rank setting  with a small number of labels to rank. We pitch RoBiRank against the following
algorithms: RankSVM [15]  the ranking algorithm of Le and Smola [14] (called LSRank in the se-
quel)  InfNormPush [22]  IRPush [1]  and 8 standard ranking algorithms implemented in RankLib1
namely MART  RankNet  RankBoost  AdaRank  CoordAscent  LambdaMART  ListNet and Ran-
domForests. We use three sources of datasets: LETOR 3.0 [8]   LETOR 4.02 and YAHOO LTRC
[20]  which are standard benchmarks for ranking (see Table 2 for summary statistics). Each dataset
consists of ﬁve folds; we consider the ﬁrst fold  and use the training  validation  and test splits pro-
vided. We train with different values of regularization parameter  and select one with the best NDCG

1http://sourceforge.net/p/lemur/wiki/RankLib
2http://research.microsoft.com/en-us/um/beijing/projects/letor/letor4dataset.aspx

4

on the validation dataset. The performance of the model with this parameter on the test dataset is
reported. We used implementation of the L-BFGS algorithm provided by the Toolkit for Advanced
Optimization (TAO)3 for estimating the parameter of RoBiRank. For the other algorithms  we either
implemented them using our framework or used the implementations provided by the authors.

Figure 1: Comparison of RoBiRank with a number of competing algorithms.

We use values of NDCG at different levels of truncation as our evaluation metric [17]; see Figure 1.
RoBiRank outperforms its competitors on most of the datasets; due to space constraints  we only
present plots for the TD 2004 dataset here and other plots can be found in Appendix B. The per-
formance of RankSVM seems insensitive to the level of truncation for NDCG. On the other hand 
RoBiRank  which uses non-convex loss function to concentrate its performance at the top of the
ranked list  performs much better especially at low truncation levels. It is also interesting to note
that the NDCG@k curve of LSRank is similar to that of RoBiRank  but RoBiRank consistently out-
performs at each level. RoBiRank dominates Inf-Push and IR-Push at all levels. When compared to
standard algorithms  Figure 1 (right)  again RoBiRank outperforms especially at the top of the list.
Overall  RoBiRank outperforms IRPush and InfNormPush on all datasets except TD 2003 and
OHSUMED where IRPush seems to fare better at the top of the list. Compared to the 8 standard
algorithms  again RobiRank either outperforms or performs comparably to the best algorithm except
on two datasets (TD 2003 and HP 2003)  where MART and Random Forests overtake RobiRank at
few values of NDCG. We present a summary of the NDCG values obtained by each algorithm in
Table 2 in the appendix. On the MSLR30K dataset  some of the additional algorithms like InfNorm-
Push and IRPush did not complete within the time period available; indicated by dashes in the table.

4 Latent Collaborative Retrieval

For each context x and an item y ∈ Y  the standard problem setting of learning to rank requires
training data to contain feature vector φ(x  y) and score Wxy assigned on the x  y pair. When the
number of contexts |X| or the number of items |Y| is large  it might be difﬁcult to deﬁne φ(x  y)
and measure Wxy for all x  y pairs. Therefore  in most learning to rank problems we deﬁne the set
of relevant items Yx ⊂ Y to be much smaller than Y for each context x  and then collect data only
for Yx. Nonetheless  this may not be realistic in all situations; in a movie recommender system  for
example  for each user every movie is somewhat relevant.
On the other hand  implicit user feedback data is much more abundant. For example  a lot of users
on Netﬂix would simply watch movie streams on the system but do not leave an explicit rating. By
the action of watching a movie  however  they implicitly express their preference. Such data consist
only of positive feedback  unlike traditional learning to rank datasets which have score Wxy between
each context-item pair x  y. Again  we may not be able to extract feature vectors for each x  y pair.
In such a situation  we can attempt to learn the score function f (x  y) without a feature vector φ(x  y)
by embedding each context and item in an Euclidean latent space; speciﬁcally  we redeﬁne the score
function to be: f (x  y) := (cid:104)Ux  Vy(cid:105)  where Ux ∈ Rd is the embedding of the context x and Vy ∈ Rd

3http://www.mcs.anl.gov/research/projects/tao/index.html

5

51015200.40.60.81kNDCG@kTD2004RoBiRankRankSVMLSRankInfNormPushIRPush51015200.40.60.81kNDCG@kTD2004RoBiRankMARTRankNetRankBoostAdaRankCoordAscentLambdaMARTListNetRandomForests(cid:88)
(cid:88)

y(cid:48)(cid:54)=y

y(cid:48)(cid:54)=y

 .


(cid:88)

(x y)∈Ω

(cid:88)

y(cid:48)(cid:54)=y

 .

is that of the item y. Then  we can learn these embeddings by a ranking model. This approach was
introduced in Weston et al. [24]  and was called latent collaborative retrieval.
Now we specialize RoBiRank model for this task. Let us deﬁne Ω to be the set of context-item pairs
(x  y) which was observed in the dataset. Let v(Wxy) = 1 if (x  y) ∈ Ω  and 0 otherwise; this is a
natural choice since the score information is not available. For simplicity  we set cx = 1 for every
x. Now RoBiRank (11) specializes to:

L1(U  V ) =

ρ1

σ0(f (Ux  Vy) − f (Ux  Vy(cid:48)))

(12)

Note that now the summation inside the parenthesis of (12) is over all items Y instead of a smaller
set Yx  therefore we omit specifying the range of y(cid:48) from now on. To avoid overﬁtting  a regularizer
is added to (12); for simplicity we use the Frobenius norm of U and V in our experiments.

4.1 Stochastic Optimization
When the size of the data |Ω| or the number of items |Y| is large  however  methods that require
exact evaluation of the function value and its gradient will become very slow since the evaluation
takes O (|Ω| · |Y|) computation.
In this case  stochastic optimization methods are desirable [4];
in this subsection  we will develop a stochastic gradient descent algorithm whose complexity is
independent of |Ω| and |Y|.
For simplicity  let θ be a concatenation of all parameters {Ux}x∈X
∇θL1(U  V ) of (12) is (cid:88)

  {Vy}y∈Y

. The gradient

∇θρ1

σ0(f (Ux  Vy) − f (Ux  Vy(cid:48)))

(x y)∈Ω

Finding an unbiased estimator of the gradient whose computation is independent of |Ω| is not difﬁ-
cult; if we sample a pair (x  y) uniformly from Ω  then it is easy to see that the following estimator

|Ω| · ∇θρ1

σ0(f (Ux  Vy) − f (Ux  Vy(cid:48)))

(13)

is unbiased. This still involves a summation over Y  however  so it requires O(|Y|) calculation.
Since ρ1(·) is a nonlinear function it seems unlikely that an unbiased stochastic gradient which
randomizes over Y can be found; nonetheless  to achieve convergence guarantees of the stochastic
gradient descent algorithm  unbiasedness of the estimator is necessary [18].
We attack this problem by linearizing the objective function by parameter expansion. Note the
following property of ρ1(·) [5]:

ρ1(t) = log2(t + 1) ≤ − log2 ξ +

ξ · (t + 1) − 1

.

log 2
t+1. Now introducing an auxiliary

(14)

This holds for any ξ > 0  and the bound is tight when ξ = 1
parameter ξxy for each (x  y) ∈ Ω and applying this bound  we obtain an upper bound of (12) as

L(U  V  ξ) :=

ξxy

− log2 ξxy +

y(cid:48)(cid:54)=y σ0(f (Ux  Vy) − f (Ux  Vy(cid:48))) + 1

− 1

.

(15)

log 2

(cid:16)(cid:80)

(cid:88)

(x y)∈Ω

Now we propose an iterative algorithm in which  each iteration consists of (U  V )-step and ξ-step;
in the (U  V )-step we minimize (15) in (U  V ) and in the ξ-step we minimize in ξ. Pseudo-code can
be found in Algorithm 1 in Appendix C.

(cid:80)

(cid:16)(cid:80)

(U  V )-step The partial derivative of (15) in terms of U and V can be calculated as:
. Now it is easy
∇U V L(U  V  ξ) := 1
to see that the following stochastic procedure unbiasedly estimates the above gradient:

y(cid:48)(cid:54)=y ∇U V σ0(f (Ux  Vy) − f (Ux  Vy(cid:48)))

(x y)∈Ω ξxy

log 2

(cid:17)

(cid:17)

6

Figure 2: Left: Scaling of RoBiRank on Million Song Dataset. Center  Right: Comparison of
RoBiRank and Weston et al. [24] when the same amount of wall-clock computation time is given.

• Sample (x  y) uniformly from Ω
• Sample y(cid:48) uniformly from Y \ {y}
• Estimate the gradient by

|Ω| · (|Y| − 1) · ξxy

log 2

· ∇U V σ0(f (Ux  Vy) − f (Ux  Vy(cid:48))).

(16)

Therefore a stochastic gradient descent algorithm based on (16) will converge to a local minimum
of the objective function (15) with probability one [21]. Note that the time complexity of calculating
(16) is independent of |Ω| and |Y|. Also  it is a function of only Ux and Vy; the gradient is zero in
terms of other variables.

ξ-step When U and V are ﬁxed  minimization of ξxy variable is independent of each other and
y(cid:48)(cid:54)=y σ0(f (Ux Vy)−f (Ux Vy(cid:48) ))+1. This of course requires
a simple analytic solution exists: ξxy =
O(|Y|) work. In principle  we can avoid summation over Y by taking stochastic gradient in terms of
ξxy as we did for U and V . However  since the exact solution is simple to compute and also because
most of the computation time is spent on (U  V )-step  we found this update rule to be efﬁcient.

(cid:80)

1

Parallelization The linearization trick in (15) not only enables us to construct an efﬁcient stochas-
tic gradient algorithm  but also makes possible to efﬁciently parallelize the algorithm across multiple
number of machines. Due to lack of space  details are relegated to Appendix D.

4.2 Experiments

In this subsection we use the Million Song Dataset (MSD) [3]  which consists of 1 129 318 users
(|X|)  386 133 songs (|Y|)  and 49 824 519 records (|Ω|) of a user x playing a song y in the training
dataset. The objective is to predict the songs from the test dataset that a user is going to listen to4.
Since explicit ratings are not given  NDCG is not applicable for this task; we use precision at 1 and
10 [17] as our evaluation metric. In our ﬁrst experiment we study the scaling behavior of RoBiRank
as a function of number of machines. RoBiRank p denotes the parallel version of RoBiRank which
is distributed across p machines. In Figure 2 (left) we plot mean Precision@1 as a function of the
number of machines × the number of seconds elapsed; this is a proxy for CPU time. If an algorithm
linearly scales across multiple processors  then all lines in the ﬁgure should overlap with each other.
As can be seen RoBiRank exhibits near ideal speed up when going from 4 to 32 machines5.
In our next experiment we compare RoBiRank with a state of the art algorithm from Weston et al.
[24]  which optimizes a similar objective function (17). We compare how fast the quality of the
solution improves as a function of wall clock time. Since the authors of Weston et al. [24] do not
make available their code  we implemented their algorithm within our framework using the same
data structures and libraries used by our method. Furthermore  for a fair comparison  we used the
same initialization for U and V and performed an identical grid-search over the step size parameter.

4the original data also provides the number of times a song was played by a user  but we ignored this in our

experiment.

5The graph for RoBiRank 1 is hard to see because it was run for only 100 000 CPU-seconds.

7

00.511.522.53·10600.10.20.3numberofmachines×secondselapsedMeanPrecision@1RoBiRank4RoBiRank16RoBiRank32RoBiRank100.20.40.60.81·10500.10.20.3secondselapsedMeanPrecision@1Westonetal.(2012)RoBiRank1RoBiRank4RoBiRank16RoBiRank3200.20.40.60.81·10505·10−20.10.150.2secondselapsedMeanPrecision@10Westonetal.(2012)RoBiRank1RoBiRank4RoBiRank16RoBiRank32(cid:88)

1 +

Φ

(cid:88)

y(cid:48)(cid:54)=y

  

It can be seen from Figure 2 (center  right) that on a single machine the algorithm of Weston et al.
[24] is very competitive and outperforms RoBiRank. The reason for this might be the introduction
of the additional ξ variables in RoBiRank  which slows down convergence. However  RoBiRank
training can be distributed across processors  while it is not clear how to parallelize the algorithm
of Weston et al. [24]. Consequently  RoBiRank 32 which uses 32 machines for its computation can
produce a signiﬁcantly better model within the same wall clock time window.

5 Related Work

In terms of modeling  viewing ranking problems as generalization of binary classiﬁcation problems
is not a new idea; for example  RankSVM deﬁnes the objective function as a sum of hinge losses 
similarly to our basic model (6) in Section 3.1. However  it does not directly optimize the ranking
metric such as NDCG; the objective function and the metric are not immediately related to each
other. In this respect  our approach is closer to that of Le and Smola [14] which constructs a convex
upper bound on the ranking metric and Chapelle et al. [9] which improves the bound by introducing
non-convexity. The objective function of Chapelle et al. [9] is also motivated by ramp loss  which
is used for robust classiﬁcation; nonetheless  to our knowledge the direct connection between the
ranking metrics in form (7) (DCG  NDCG) and the robust loss (3) is our novel contribution. Also 
our objective function is designed to speciﬁcally bound the ranking metric  while Chapelle et al. [9]
proposes a general recipe to improve existing convex bounds.
Stochastic optimization of the objective function for latent collaborative retrieval has been also ex-
plored in Weston et al. [24]. They attempt to minimize

I(f (Ux  Vy) − f (Ux  Vy(cid:48)) < 0)

where Φ(t) =(cid:80)t

1

k=1

(x y)∈Ω
k . This is similar to our objective function (15); Φ(·) and ρ2(·) are asymptoti-
cally equivalent. However  we argue that our formulation (15) has two major advantages. First  it is
a continuous and differentiable function  therefore gradient-based algorithms such as L-BFGS and
stochastic gradient descent have convergence guarantees. On the other hand  the objective function
of Weston et al. [24] is not even continuous  since their formulation is based on a function Φ(·)
that is deﬁned for only natural numbers. Also  through the linearization trick in (15) we are able
to obtain an unbiased stochastic gradient  which is necessary for the convergence guarantee  and to
parallelize the algorithm across multiple machines as discussed in Appendix D. It is unclear how
these techniques can be adapted for the objective function of Weston et al. [24].

(17)

6 Conclusion

In this paper  we developed RoBiRank  a novel model on ranking  based on insights and techniques
from robust binary classiﬁcation. Then  we proposed a scalable and parallelizable stochastic opti-
mization algorithm that can be applied to latent collaborative retrieval task which large-scale data
without feature vectors and explicit scores have to take care of. Experimental results on both learning
to rank datasets and latent collaborative retrieval dataset suggest the advantage of our approach.
As a ﬁnal note  the experiments in Section 4.2 are arguably unfair towards WSABIE. For instance 
one could envisage using clever engineering tricks to derive a parallel variant of WSABIE (e.g. 
by averaging gradients from various machines)  which might outperform RoBiRank on the MSD
dataset. While performance on a speciﬁc dataset might be better  we would lose global convergence
guarantees. Therefore  rather than obsess over the performance of a speciﬁc algorithm on a speciﬁc
dataset  via this paper we hope to draw the attention of the community to the need for developing
principled parallel algorithms for this important problem.

Acknowledgments We thank anonymous reviewers for their constructive comments  and Texas
Advanced Computing Center for infrastructure and support for experiments. This material is par-
tially based upon work supported by the National Science Foundation under grant No. IIS-1117705.

8

References
[1] S. Agarwal. The inﬁnite push: A new support vector ranking algorithm that directly optimizes

accuracy at the absolute top of the list. In SDM  pages 839–850. SIAM  2011.

[2] P. L. Bartlett  M. I. Jordan  and J. D. McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[3] T. Bertin-Mahieux  D. P. Ellis  B. Whitman  and P. Lamere. The million song dataset. In Pro-
ceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011) 
2011.

[4] L. Bottou and O. Bousquet. The tradeoffs of large-scale learning. Optimization for Machine

Learning  page 351  2011.

[5] G. Bouchard. Efﬁcient bounds for the softmax function  applications to inference in hybrid

models. 2007.

[6] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  Cambridge 

England  2004.

[7] D. Buffoni  P. Gallinari  N. Usunier  and C. Calauz`enes. Learning scoring functions with
order-preserving losses and standardized supervision. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11)  pages 825–832  2011.

[8] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. Journal of Machine

Learning Research-Proceedings Track  14:1–24  2011.

[9] O. Chapelle  C. B. Do  C. H. Teo  Q. V. Le  and A. J. Smola. Tighter bounds for structured

estimation. In Advances in neural information processing systems  pages 281–288  2008.

[10] N. Ding. Statistical Machine Learning in T-Exponential Family of Distributions. PhD thesis 

PhD thesis  Purdue University  West Lafayette  Indiana  USA  2013.

[11] V. Feldman  V. Guruswami  P. Raghavendra  and Y. Wu. Agnostic learning of monomials by

halfspaces is hard. SIAM Journal on Computing  41(6):1558–1590  2012.

[12] R. Gemulla  E. Nijkamp  P. J. Haas  and Y. Sismanis. Large-scale matrix factorization with
In Conference on Knowledge Discovery and Data

distributed stochastic gradient descent.
Mining  pages 69–77  2011.

[13] P. J. Huber. Robust Statistics. John Wiley and Sons  New York  1981.
[14] Q. V. Le and A. J. Smola. Direct optimization of ranking measures. Technical Report

0704.3359  arXiv  April 2007. http://arxiv.org/abs/0704.3359.

[15] C.-P. Lee and C.-J. Lin. Large-scale linear ranksvm. Neural Computation  2013. To Appear.
[16] P. Long and R. Servedio. Random classiﬁcation noise defeats all convex potential boosters.

Machine Learning Journal  78(3):287–304  2010.

[17] C. D. Manning  P. Raghavan  and H. Sch¨utze. Introduction to Information Retrieval. Cam-

bridge University Press  2008. URL http://nlp.stanford.edu/IR-book/.

[18] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[19] J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research.

Springer  2nd edition  2006.

[20] T. Qin  T.-Y. Liu  J. Xu  and H. Li. Letor: A benchmark collection for research on learning to

rank for information retrieval. Information Retrieval  13(4):346–374  2010.

[21] H. E. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical

Statistics  22:400–407  1951.

[22] C. Rudin. The p-norm push: A simple convex ranking algorithm that concentrates at the top

of the list. The Journal of Machine Learning Research  10:2233–2271  2009.

[23] N. Usunier  D. Buffoni  and P. Gallinari. Ranking with ordered weighted pairwise classiﬁca-

tion. In Proceedings of the International Conference on Machine Learning  2009.

[24] J. Weston  C. Wang  R. Weiss  and A. Berenzweig. Latent collaborative retrieval. arXiv

preprint arXiv:1206.4603  2012.

9

,Hyokun Yun
Parameswaran Raman
S. Vishwanathan
Raef Bassily
Kobbi Nissim
Uri Stemmer
Abhradeep Guha Thakurta
Quanfu Fan
Chun-Fu (Richard) Chen
Hilde Kuehne
Marco Pistoia
David Cox