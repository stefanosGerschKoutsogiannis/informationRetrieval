2017,Learning Multiple Tasks with Multilinear Relationship Networks,Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks  a fundamental problem of multi-task learning is how to exploit the task relatedness underlying  parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features  MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.,Learning Multiple Tasks with Multilinear

Relationship Networks

Mingsheng Long  Zhangjie Cao  Jianmin Wang  Philip S. Yu
School of Software  Tsinghua University  Beijing 100084  China

psyu@uic.edu

{mingsheng jimwang}@tsinghua.edu.cn

caozhangjie14@gmail.com

Abstract

Deep networks trained on large-scale data can learn transferable features to promote
learning multiple tasks. Since deep features eventually transition from general to
speciﬁc along deep networks  a fundamental problem of multi-task learning is how
to exploit the task relatedness underlying parameter tensors and improve feature
transferability in the multiple task-speciﬁc layers. This paper presents Multilinear
Relationship Networks (MRN) that discover the task relationships based on novel
tensor normal priors over parameter tensors of multiple task-speciﬁc layers in deep
convolutional networks. By jointly learning transferable features and multilinear
relationships of tasks and features  MRN is able to alleviate the dilemma of negative-
transfer in the feature layers and under-transfer in the classiﬁer layer. Experiments
show that MRN yields state-of-the-art results on three multi-task learning datasets.

1

Introduction

Supervised learning machines trained with limited labeled samples are prone to overﬁtting  while
manual labeling of sufﬁcient training data for new domains is often prohibitive. Thus it is imperative
to design versatile algorithms for reducing the labeling consumption  typically by leveraging off-the-
shelf labeled data from relevant tasks. Multi-task learning is based on the idea that the performance
of one task can be improved using related tasks as inductive bias [4]. Knowing the task relationship
should enable the transfer of shared knowledge from relevant tasks such that only task-speciﬁc features
need to be learned. This fundamental idea of task relatedness has motivated a variety of methods 
including multi-task feature learning that learns a shared feature representation [1  2  6  5  23]  and
multi-task relationship learning that models inherent task relationship [10  14  29  31  15  17  8].
Learning inherent task relatedness is a hard problem  since the training data of different tasks may be
sampled from different distributions and ﬁtted by different models. Without prior knowledge on the
task relatedness  the distribution shift may pose a major difﬁculty in transferring knowledge across
different tasks. Unfortunately  if cross-task knowledge transfer is impossible  then we will overﬁt
each task due to limited amount of labeled data. One way to circumvent this dilemma is to use an
external data source  e.g. ImageNet  to learn transferable features through which the shift in the
inductive biases can be reduced such that different tasks can be correlated more effectively. This idea
has motivated some latest deep learning methods for learning multiple tasks [25  22  7  27]  which
learn a shared representation in feature layers and multiple independent classiﬁers in classiﬁer layer.
However  these deep multi-task learning methods do not explicitly model the task relationships.
This may result in under-transfer in the classiﬁer layer as knowledge can not be transferred across
different classiﬁers. Recent research also reveals that deep features eventually transition from general
to speciﬁc along the network  and feature transferability drops signiﬁcantly in higher layers with
increasing task dissimilarity [28]  hence the sharing of all feature layers may be risky to negative-
transfer. Therefore  it remains an open problem how to exploit the task relationship across different
deep networks while improving the feature transferability in task-speciﬁc layers of the deep networks.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

This paper presents Multilinear Relationship Network (MRN) for multi-task learning  which discovers
the task relationships based on multiple task-speciﬁc layers of deep convolutional neural networks.
Since the parameters of deep networks are natively tensors  the tensor normal distribution [21] is
explored for multi-task learning  which is imposed as the prior distribution over network parameters
of all task-speciﬁc layers to learn ﬁnd-grained multilinear relationships of tasks  classes and features.
By jointly learning transferable features and multilinear relationships  MRN is able to circumvent the
dilemma of negative-transfer in feature layers and under-transfer in classiﬁer layer. Experiments show
that MRN learns ﬁne-grained relationships and yields state-of-the-art results on standard benchmarks.

2 Related Work

Multi-task learning is a learning paradigm that learns multiple tasks jointly by exploiting the shared
structures to improve generalization performance [4  19] and mitigate manual labeling consumption.
There are generally two categories of approaches: (1) multi-task feature learning  which learns a
shared feature representation such that the distribution shift across different tasks can be reduced
[1  2  6  5  23]; (2) multi-task relationship learning  which explicitly models the task relationship
in the forms of task grouping [14  15  17] or task covariance [10  29  31  8]. While these methods
have achieved improved performance  they may be restricted by their shallow learning paradigm that
cannot embody task relationships by suppressing the task-speciﬁc variations in transferable features.
Deep networks learn abstract representations that disentangle and hide explanatory factors of variation
behind data [3  16]. Deep representations manifest invariant factors underlying different populations
and are transferable across similar tasks [28]. Thus deep networks have been successfully explored
for domain adaptation [11  18] and multi-task learning [25  22  32  7  20  27]  where signiﬁcant
performance gains have been witnessed. Most multi-task deep learning methods [22  32  7] learn a
shared representation in the feature layers and multiple independent classiﬁers in the classiﬁer layer
without inferring the task relationships. However  this may result in under-transfer in the classiﬁer
layer as knowledge cannot be adaptively propagated across different classiﬁers  while the sharing of
all feature layers may still be vulnerable to negative-transfer in the feature layers  as the higher layers
of deep networks are tailored to ﬁt task-speciﬁc structures and may not be safely transferable [28].
This paper presents a multilinear relationship network based on novel tensor normal priors to learn
transferable features and task relationships that mitigate both under-transfer and negative-transfer. Our
work contrasts from prior relationship learning [29  31] and multi-task deep learning [22  32  7  27]
methods in two key aspects. (1) Tensor normal prior: our work is the ﬁrst to explore tensor normal
distribution as priors of network parameters in different layers to learn multilinear task relationships in
deep networks. Since the network parameters of multiple tasks natively stack into high-order tensors 
previous matrix normal distribution [13] cannot be used as priors of network parameters to learn task
relationships. (2) Deep task relationship: we deﬁne the tensor normal prior on multiple task-speciﬁc
layers  while previous deep learning methods do not learn the task relationships. To our knowledge 
multi-task deep learning by tensor factorization [27] is the ﬁrst work that tackles multi-task deep
learning by tensor factorization  which learns shared feature subspace from multilayer parameter
tensors; in contrast  our work learns multilinear task relationships from multiplayer parameter tensors.

3 Tensor Normal Distribution

3.1 Probability Density Function

Tensor normal distribution is a natural extension of multivariate normal distribution and matrix-variate
normal distribution [13] to tensor-variate distributions. The multivariate normal distribution is order-1
tensor normal distribution  and matrix-variate normal distribution is order-2 tensor normal distribution.
Before deﬁning tensor normal distribution  we ﬁrst introduce the notations and operations of order-K
tensor. An order-K tensor is an element of the tensor product of K vector spaces  each of which
has its own coordinate system. A vector x ∈ Rd1 is an order-1 tensor with dimension d1. A matrix
X ∈ Rd1×d2 is an order-2 tensor with dimensions (d1  d2). A order-K tensor X ∈ Rd1×...×dK
with dimensions (d1  . . .   dK) has elements {xi1...iK : ik = 1  . . .   dk}. The vectorization of X is
unfolding the tensor into a vector  denoted by vec(X ). The matricization of X is a generalization of
vectorization  reordering the elements of X into a matrix. In this paper  to simply the notations and

2

k=1 dk) × ((cid:81)K

describe the tensor relationships  we use the mode-k matricization and denote by X(k) the mode-k
matrix of tensor X   where row i of X(k) contains all elements of X having the k-th index equal to i.
k=1 dk) × 1 vector 
the normal distribution on a tensor X can be considered as a multivariate normal distribution on vector
k=1 dk. However  such an ordinary multivariate normal distribution ignores
the special structure of X as a d1 × . . . × dK tensor  and as a result  the covariance characterizing the
k=1 dk)  which is often prohibitively
large for modeling and estimation. To exploit the structure of X   tensor normal distributions assume
k=1 dk) covariance matrix Σ1:K can be decomposed into the Kronecker
product Σ1:K = Σ1 ⊗ . . .⊗ ΣK  and elements of X (in vectorization) follow the normal distribution 

Consider an order-K tensor X ∈ Rd1×...×dK . Since we can vectorize X to a ((cid:81)K
vec(X ) of dimension(cid:81)K
correlations across elements of X is of size ((cid:81)K
k=1 dk) × ((cid:81)K
that the ((cid:81)K
between the dk rows of the mode-k matricization X(k) of dimension dk × ((cid:81)

(1)
where ⊗ is the Kronecker product  Σk ∈ Rdk×dk is a positive deﬁnite matrix indicating the covariance
k(cid:48)(cid:54)=k dk(cid:48))  and M is a
mean tensor containing the expectation of each element of X . Due to the decomposition of covariance
as the Kronecker product  the tensor normal distribution of an order-K tensor X   parameterized by
mean tensor M and covariance matrices Σ1  . . .   ΣK  can deﬁne probability density function as [21]

vec (X ) ∼ N (vec (M)   Σ1 ⊗ . . . ⊗ ΣK)  

(cid:32) K(cid:89)

(cid:33)

(cid:18)

−d/2

|Σk|−d/(2dk)

× exp

k=1

p (x) = (2π)

ΣK  d =(cid:81)K

(2)
where |·| is the determinant of a square matrix  and x = vec (X )   µ = vec (M)   Σ1:K = Σ1⊗. . .⊗
k=1 dk. The tensor normal distribution corresponds to the multivariate normal distribution
with Kronecker decomposable covariance structure. X following tensor normal distribution  i.e.
vec (X ) following the normal distribution with Kronecker decomposable covariance  is denoted by
(3)

X ∼ T Nd1×...×dK (M  Σ1  . . .   ΣK) .

 

(x − µ)TΣ−1

− 1
2

3.2 Maximum Likelihood Estimation
Consider a set of n samples {Xi}n
i=1 where each Xi is an order-3 tensor generated by a tensor normal
distribution as in Equation (2). The maximum likelihood estimation (MLE) of the mean tensor M is

(cid:19)
1:K (x − µ)

The MLE of covariance matrices (cid:98)Σ1  . . .  (cid:98)Σ3 are computed by iteratively updating these equations:

i=1

(4)

(cid:99)M =

1
n

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

(cid:98)Σ1 =
(cid:98)Σ2 =
(cid:98)Σ3 =

1

nd2d3

1

nd1d3

1

nd1d2

(Xi − M)(1)

(Xi − M)(2)

(Xi − M)(3)

Xi.

n(cid:88)
(cid:16)(cid:98)Σ3 ⊗(cid:98)Σ2
(cid:16)(cid:98)Σ3 ⊗(cid:98)Σ1
(cid:16)(cid:98)Σ2 ⊗(cid:98)Σ1

(cid:17)−1
(cid:17)−1
(cid:17)−1

(Xi − M)T

(1) 

(Xi − M)T

(2) 

(5)

(Xi − M)T

(3).

guaranteed. Covariance matrices (cid:98)Σ1  . . .  (cid:98)Σ3 are not identiﬁable and the solutions to maximizing

This ﬂip-ﬂop algorithm [21] is efﬁcient to solve by simple matrix manipulations and convergence is
density function (2) are not unique  while only the Kronecker product Σ1⊗. . .⊗ΣK (1) is identiﬁable.

i=1

4 Multilinear Relationship Networks

This work models multiple tasks by jointly learning transferable representations and task relationships.
Given T tasks with training data {Xt Yt}T
}
1  . . .   yt
Nt
are the Nt training examples and associated labels of the t-th task  respectively drawn from D-
n ∈ RD and
dimensional feature space and C-cardinality label space  i.e. each training example xt
n ∈ {1  . . .   C}. Our goal is to build a deep network for multiple tasks yt
n) which learns
yt
transferable features and adaptive task relationships to bridge different tasks effectively and robustly.

t=1  where Xt = {xt

} and Yt = {yt

1  . . .   xt
Nt

n = ft(xt

3

Figure 1: Multilinear relationship network (MRN) for multi-task learning: (1) convolutional layers
conv1–conv5 and fully-connected layer f c6 learn transferable features  so their parameters are shared
across tasks; (2) fully-connected layers f c7–f c8 ﬁt task-speciﬁc structures  so their parameters are
modeled by tensor normal priors for learning multilinear relationships of features  classes and tasks.

4.1 Model

We start with deep convolutional neural networks (CNNs) [16]  a family of models to learn transferable
features that are well adaptive to multiple tasks [32  28  18  27]. The main challenge is that in multi-
task learning  each task is provided with a limited amount of labeled data  which is insufﬁcient to
build reliable classiﬁers without overﬁtting. In this sense  it is vital to model the task relationships
through which each pair of tasks can help with each other to enable knowledge transfer if they are
related  and can remain independent to mitigate negative transfer if they are unrelated. With this idea 
we design a Multilinear Relationship Network (MRN) that exploits both feature transferability and
task relationship to establish effective and robust multi-task learning. Figure 1 shows the architecture
of the proposed MRN model based on AlexNet [16]  while other deep networks are also applicable.
We build the proposed MRN model upon AlexNet [16]  which is comprised of convolutional layers
(conv1–conv5) and fully-connected layers (f c6–f c8). The (cid:96)-th f c layer learns a nonlinear mapping
n is the hidden representation of each point xt
n 
ht (cid:96)
Wt (cid:96) and bt (cid:96) are the weight and bias parameters  and a(cid:96) is the activation function  taken as ReLU
j=1 exj for the output layer.
Denote by y = ft(x) the CNN classiﬁer of t-th task  and the empirical error of CNN on {Xt Yt} is

n = a(cid:96)(cid:0)Wt (cid:96)ht (cid:96)−1
a(cid:96)(x) = max(0  x) for hidden layers or softmax units a(cid:96) (x) = ex/(cid:80)|x|

+ bt (cid:96)(cid:1) for task t  where ht (cid:96)

n

(6)

(cid:0)xt

n

(cid:1)   yt

n

(cid:1) 

min
ft

J(cid:0)ft

Nt(cid:88)

n=1

n) is the conditional probability that CNN assigns
where J is the cross-entropy loss function  and ft (xt
n. We will not describe how to compute the convolutional layers since these layers can
n to label yt
xt
learn transferable features in general [28  18]  and we will simply share the network parameters of
these layers across different tasks  without explicitly modeling the relationships of features and tasks
in these layers. To beneﬁt from pre-training and ﬁne-tuning as most deep learning work  we copy
these layers from a model pre-trained on ImageNet 2012 [28]  and ﬁne-tune all conv1–conv5 layers.
As revealed by the recent literature ﬁndings [28]  the deep features in standard CNNs must eventually
transition from general to speciﬁc along the network  and the feature transferability decreases while
the task discrepancy increases  making the features in higher layers f c7–f c8 unsafely transferable
across different tasks. In other words  the f c layers are tailored to their original task at the expense
of degraded performance on the target task  which may deteriorate multi-task learning based on
deep neural networks. Most previous methods generally assume that the multiple tasks can be well
correlated given the shared representation learned by the feature layers conv1–f c7 of deep networks
[25  22  32  27]. However  it may be vulnerable if different tasks are not well correlated under deep
features  which is common as higher layers are not safely transferable and tasks may be dissimilar.
Moreover  existing multi-task learning methods are natively designed for binary classiﬁcation tasks 
which are not good choices as deep networks mainly adopt multi-class softmax regression. It remains
an open problem to explore the task relationships of multi-class classiﬁcation for multi-task learning.
In this work  we jointly learn transferable features and multilinear relationships of features and tasks
for multiple task-speciﬁc layers L in a Bayesian framework. Based on the transferability of deep

4

conv1inputconv2conv3conv4conv5fc6fc7TN-Priorfc8TN-PriorTask 1Task Toutputt=1 the complete training data of T tasks  and by Wt (cid:96) ∈ RD(cid:96)

networks discussed above  the task-speciﬁc layers L are set to {f c7  f c8}. Denote by X = {Xt}T
t=1 
Y = {Yt}T
2 the network parameters
of the t-th task in the (cid:96)-th layer  where D(cid:96)
2 are the rows and columns of matrix Wt (cid:96). In order
to capture the task relationship in the network parameters of all T tasks  we construct the (cid:96)-th layer
set of parameter tensors of all the task-speciﬁc layers L = {f c7  f c8}. The Maximum a Posteriori
(MAP) estimation of network parameters W given training data {X  Y} for learning multiple tasks is

parameter tensor as W (cid:96) =(cid:2)W1 (cid:96); . . . ; WT (cid:96)(cid:3) ∈ RD(cid:96)

2×T . Denote by W =(cid:8)W (cid:96) : (cid:96) ∈ L(cid:9) the

1 and D(cid:96)

1×D(cid:96)

1×D(cid:96)

p (W|X  Y) ∝ p (W) · p (Y |X  W )

p(cid:0)W (cid:96)(cid:1) · T(cid:89)

Nt(cid:89)

p(cid:0)yt

n

(cid:12)(cid:12)xt
n W (cid:96)(cid:1) 

(7)

(cid:89)

(cid:96)∈L

=

t=1

n=1

where we assume that for prior p (W)  the parameter tensor of each layer W (cid:96) is independent on the
parameter tensors of the other layers W (cid:96)(cid:48)(cid:54)=(cid:96)  which is a common assumption made by most feed-
forward neural network methods [3]. Finally  we assume when the network parameter is sampled
from the prior  all tasks are independent. These independence assumptions lead to the factorization of
the posteriori in Equation (7)  which make the ﬁnal MAP estimation in deep networks easy to solve.
The maximum likelihood estimation (MLE) part p (Y |X  W ) in Equation (7) is modeled by deep
CNN in Equation (6)  which can learn transferable features in lower layers for multi-task learning.
We opt to share the network parameters of all these layers (conv1–f c6). This parameter sharing
strategy is a relaxation of existing deep multi-task learning methods [22  32  7]  which share all the
feature layers except for the classiﬁer layer. We do not share task-speciﬁc layers (the last feature
layer f c7 and classiﬁer layer f c8)  with the expectation to potentially mitigate negative-transfer [28].
The prior part p (W) in Equation (7) is the key to enabling multi-task deep learning since this prior
part should be able to model the multilinear relationship across parameter tensors. This paper  for the
ﬁrst time  deﬁnes the prior for the (cid:96)-th layer parameter tensor by tensor normal distribution [21] as
(8)
2×T
3 ∈ RT×T are the mode-1  mode-2  and mode-3
where Σ(cid:96)
covariance matrices  respectively. Speciﬁcally  in the tensor normal prior  the row covariance matrix
1 models the relationships between features (feature covariance)  the column covariance matrix Σ(cid:96)
Σ(cid:96)
2
models the relationships between classes (class covariance)  and the mode-3 covariance matrix Σ(cid:96)
models the relationships between tasks in the (cid:96)-th layer network parameters {W1 (cid:96)  . . .   WT (cid:96)}. A
3
common strategy used by previous methods is to use identity covariance for feature covariance [31  8]
and class covariance [2]  which implicitly assumes independent features and classes and cannot
capture the dependencies between them. This work learns all feature covariance  class covariance 
task covariance and all network parameters from data to build robust multilinear task relationships.
We integrate the CNN error functional (6) and tensor normal prior (8) into MAP estimation (7)
and taking negative logarithm  which leads to the MAP estimation of the network parameters W  a
regularized optimization problem for Multilinear Relationship Network (MRN) formally writing as

p(cid:0)W (cid:96)(cid:1) = T ND(cid:96)

(cid:0)O  Σ(cid:96)

1×D(cid:96)
2  and Σ(cid:96)

2 ∈ RD(cid:96)

1 ∈ RD(cid:96)

(cid:1)  

2  Σ(cid:96)
3

1  Σ(cid:96)

2×D(cid:96)

1×D(cid:96)

1  Σ(cid:96)

min
t=1 Σ(cid:96)

ft|T

k|K

k=1

Nt(cid:88)
(cid:32)

n=1

T(cid:88)
(cid:88)

t=1

(cid:96)∈L

J(cid:0)ft

(cid:0)xt

n

(cid:1)   yt

n

(cid:1)
−1vec(W (cid:96)) − K(cid:88)

k=1

+

1
2

vec(W (cid:96))

T

(Σ(cid:96)

1:K )

k|(cid:17)(cid:33)
(cid:16)|Σ(cid:96)

 

D(cid:96)
D(cid:96)
k

ln

(9)

where D(cid:96) =(cid:81)K

k and K = 3 is the number of modes in parameter tensor W  which could be
k=1 D(cid:96)
K = 4 for the convolutional layers (width  height  number of feature maps  and number of tasks);
2 ⊗ Σ(cid:96)
3 is the Kronecker product of the feature covariance Σ(cid:96)
1  class covariance Σ(cid:96)
2 
Σ(cid:96)
and task covariance Σ(cid:96)
3. Moreover  we can assume shared task relationship across different layers as
3 = Σ3  which enhances connection between task relationships on features f c7 and classiﬁers f c8.
Σ(cid:96)

1 ⊗ Σ(cid:96)

1:3 = Σ(cid:96)

4.2 Algorithm
The optimization problem (9) is jointly non-convex with respect to the parameter tensors W as well as
feature covariance Σ(cid:96)
3. Thus  we alternatively optimize

2  and task covariance Σ(cid:96)

1  class covariance Σ(cid:96)

5

one set of variables with the others ﬁxed. We ﬁrst update Wt (cid:96)  the parameter of task-t in layer-(cid:96).
When training deep CNN by back-propagation  we only require the gradient of the objective function
(denoted by O) in Equation (10) w.r.t. Wt (cid:96) on each data point (xt
n)  which can be computed as

=

∂O (xt

n)   yt
n)

∂J (ft (xt

n  yt
n)
∂Wt (cid:96)

1:3)−1vec(cid:0)W (cid:96)(cid:1)]··t is the (:  :  t) slice of a tensor folded from elements (Σ(cid:96)

(10)
1:3)−1vec(W (cid:96))
where [(Σ(cid:96)
that are corresponding to parameter matrix Wt (cid:96). Since training a deep CNN requires a large amount
of labeled data  which is prohibitive for many multi-task learning problems  we ﬁne-tune from an
AlexNet model pre-trained on ImageNet as in [28]. In each epoch  after updating W  we can update
3 by the ﬂip-ﬂop algorithm as
the feature covariance Σ(cid:96)

2  and task covariance Σ(cid:96)

∂Wt (cid:96)

··t 

+(cid:2)(Σ(cid:96)

n  yt

1:3)−1vec(cid:0)W (cid:96)(cid:1)(cid:3)

Σ(cid:96)

1 =

(cid:16)
1  class covariance Σ(cid:96)
(W (cid:96))(1)
(cid:16)
(W (cid:96))(2)
(cid:16)
(W (cid:96))(3)

1
D(cid:96)
2T
1
D(cid:96)
1T
1
1D(cid:96)
D(cid:96)
2

2 =

3 =

Σ(cid:96)

Σ(cid:96)

(cid:17)−1
(cid:17)−1
(cid:17)−1

2

3 ⊗ Σ(cid:96)
Σ(cid:96)
3 ⊗ Σ(cid:96)
Σ(cid:96)
2 ⊗ Σ(cid:96)
Σ(cid:96)

1

1

(W (cid:96))T

(1) + ID(cid:96)

1

(W(cid:96))T

(2) + ID(cid:96)

2

(W (cid:96))T

(3) + IT .

 

 

(11)

(cid:0)BT ⊗ A(cid:1) vec (X) = vec (AXB). Taking the computation of Σ(cid:96)

where the last term of each update equation is a small penalty traded off by  for numerical stability.
However  the above updating equations (11) are computationally prohibitive  due to the dimension
2 × D(cid:96)
explosion of the Kronecker product  e.g. Σ(cid:96)
2. To speed up
1D(cid:96)
−1 = A−1 ⊗ B−1 and
computation  we will use the following rules of Kronecker product: (A ⊗ B)
3 ∈ RT×T as an example  we have
−1(cid:17)

(W (cid:96))(3) i·(cid:0)Σ(cid:96)
2 ⊗ Σ(cid:96)
(cid:16)

(cid:1)−1
(W (cid:96))T
−1W (cid:96)··j(Σ(cid:96)
(Σ(cid:96)
1)
2)

1 is of dimension D(cid:96)

(W (cid:96))(3) i·vec

(3) j· + Iij

2 ⊗ Σ(cid:96)

3)ij =

+ Iij 

1D(cid:96)

(12)

(Σ(cid:96)

=

1

1
D(cid:96)
1D(cid:96)
2
1
D(cid:96)
1D(cid:96)
2

1D(cid:96)
2

O(cid:0)T 2D(cid:96)

where (W (cid:96))(3) i· denotes the i-th row of the mode-3 matricization of tensor W (cid:96)  and W (cid:96)··j denotes
the (:  :  j) slice of tensor W (cid:96). We can derive that updating Σ(cid:96)
3 has a computational complexity of
2. The total computational complexity of updating
1D(cid:96)

2T(cid:1)(cid:1)  which is still expensive.

(cid:1)(cid:1)  similarly for Σ(cid:96)
k=1 will be O(cid:0)D(cid:96)

1 + D(cid:96)
2
k|3
covariance matrices Σ(cid:96)

2T(cid:0)D(cid:96)

1 and Σ(cid:96)
1D(cid:96)

(cid:0)D(cid:96)

1T + D(cid:96)

k|3
k=1 should be low-rank  since the
A key to computation speedup is that the covariance matrices Σ(cid:96)
k|3
features and tasks are enforced to be correlated for multi-task learning. Thus  the inverses of Σ(cid:96)
k=1
do not exist in general and we have to compute the generalized inverses using eigendecomposition.
k and maintain all eigenvectors with eigenvalues greater
We perform eigendecomposition for each Σ(cid:96)
than zero. The rank r of the eigen-reconstructed covariance matrices should be r ≤ min(D(cid:96)
2  T ).
k|3
Thus  the total computational complexity for Σ(cid:96)
1 + D(cid:96)
is straight-forward to see the computational complexity of updating the parameter tensor W is the cost
of back-propagation in standard CNNs plus the cost for computing the gradient of regularization term
k=1.

k=1 is reduced to O(cid:0)rD(cid:96)
2 + T(cid:1)(cid:1) given generalized inverses (Σ(cid:96)

by Equation (10)  which is O(cid:0)rD(cid:96)

2 + T(cid:1)(cid:1). It

2T(cid:0)D(cid:96)

2T(cid:0)D(cid:96)

k)−1|3

1 + D(cid:96)

2 + D(cid:96)

1  D(cid:96)

1D(cid:96)

1D(cid:96)

4.3 Discussion

The proposed Multilinear Relationship Network (MRN) is very ﬂexible and can be easily conﬁgured
to deal with different network architectures and multi-task learning scenarios. For example  replacing
the network backbone from AlexNet to VGGnet [24] boils down to conﬁguring task-speciﬁc layers
L = {f c7  f c8}  where f c7 is the last feature layer while f c8 is the classiﬁer layer in the VGGnet.
The architecture of MRN in Figure 1 can readily cope with homogeneous multi-task learning where
all tasks share the same output space. It can cope with heterogeneous multi-task learning where
different tasks have different output spaces by setting L = {f c7}  by only considering feature layers.
The multilinear relationship learning in Equation (9) is a general framework that readily subsumes
many classical multi-task learning methods as special cases. Many regularized multi-task algorithms
can be classiﬁed into two main categories: learning with feature covariances [1  2  6  5] and learning

6

with task relations [10  14  29  31  15  17  8]. Learning with feature covariances can be viewed
as a representative formulation in feature-based methods while learning with task relations is for
parameter-based methods [30]. More speciﬁcally  previous multi-task feature learning methods [1  2]
can be viewed as a special case of Equation (9) by setting all covariance matrices but the feature
covariance to identity matrix  i.e. Σk = I|K
k=2; and previous multi-task relationship learning methods
[31  8] can be viewed as a special case of Equation (9) by setting all covariance matrices but the
task covariance to identity matrix  i.e. Σk = I|K−1
k=1 . The proposed MRN is more general in the
architecture perspective in dealing with parameter tensors in multiple layers of deep neural networks.
It is noteworthy to highlight a concurrent work on multi-task deep learning using tensor decomposition
[27]  which is feature-based method that explicitly learns the low-rank shared parameter subspace.
The proposed multilinear relationship across parameter tensors can be viewed as a strong alternative
to the tensor decomposition  with the advantage to explicitly model the positive and negative relations
across features and tasks. As a defense of [27]  the tensor decomposition can extract ﬁner-grained
feature relations (what to share and how much to share) than the proposed multilinear relationships.

5 Experiments

We compare MRN with state-of-the-art multi-task and deep learning methods to verify the efﬁcacy of
learning transferable features and multilinear task relationships. Codes and datasets will be released.

5.1 Setup

Figure 2: Examples of the Ofﬁce-Home dataset.

Ofﬁce-Caltech [12] This dataset is the standard benchmark for multi-task learning and transfer
learning. The Ofﬁce part consists of 4 652 images in 31 categories collected from three distinct
domains (tasks): Amazon (A)  which contains images downloaded from amazon.com  Webcam (W)
and DSLR (D)  which are images taken by Web camera and digital SLR camera under different
environmental variations. This dataset is organized by selecting the 10 common categories shared by
the Ofﬁce dataset and the Caltech-256 (C) dataset [12]  hence it yields four multi-class learning tasks.
Ofﬁce-Home1 [26] This dataset is to evaluate
transfer learning algorithms using deep learning.
It consists of images from 4 different domains:
Artistic images (A)  Clip Art (C)  Product im-
ages (P) and Real-World images (R). For each
domain  the dataset contains images of 65 object
categories collected in ofﬁce and home settings.
ImageCLEF-DA2 This dataset is the benchmark for ImageCLEF domain adaptation challenge 
organized by selecting the 12 common categories shared by the following four public datasets (tasks):
Caltech-256 (C)  ImageNet ILSVRC 2012 (I)  Pascal VOC 2012 (P)  and Bing (B). All three datasets
are evaluated using DeCAF7 [9] features for shallow methods and original images for deep methods.
We compare MRN with standard and state-of-the-art methods: Single-Task Learning (STL)  Multi-
Task Feature Learning (MTFL) [2]  Multi-Task Relationship Learning (MTRL) [31]  Robust Multi-
Task Learning (RMTL) [5]  and Deep Multi-Task Learning with Tensor Factorization (DMTL-TF)
[27]. STL performs per-task classiﬁcation in separate deep networks without knowledge transfer.
MTFL extracts the low-rank shared feature representations by learning feature covariance. RMTL
extends MTFL to further capture the task relationships using a low-rank structure and identify outlier
tasks using a group-sparse structure. MTRL captures the task relationships using task covariance of a
matrix normal distribution. DMTL-TF tackles multi-task deep learning by tensor factorization  which
learns shared feature subspace instead of multilinear task relationship in multilayer parameter tensors.
To go deep into the efﬁcacy of jointly learning transferable features and multilinear task relationships 
we evaluate two MRN variants: (1) MRN8  MRN using only one network layer f c8 for multilinear
relationship learning; (2) MRNt  MRN using only task covariance Σ3 for single-relationship learning.
The proposed MRN model can natively deal with multi-class problems using the parameter tensors.
However  most shallow multi-task learning methods such as MTFL  RMTL and MTRL are formulated

1http://hemanthdv.org/OfficeHome-Dataset
2http://imageclef.org/2014/adaptation

7

SpoonSinkMugPenKnifeBedBikeKettleTVKeyboardClassesAlarm-ClockDesk-LampHammerChairFanReal WorldProductClipartArtTable 1: Classiﬁcation accuracy on Ofﬁce-Caltech with standard evaluation protocol (AlexNet).

Method

STL (AlexNet)

MTFL [2]
RMTL [6]
MTRL [31]

DMTL-TF [27]

MRN8
MRNt

MRN (full)

A
88.9
90.0
91.3
86.4
91.2
91.7
91.1
92.5

W
73.0
78.9
82.3
83.0
88.3
96.4
96.3
97.5

5%
D
80.4
90.2
88.8
95.1
92.5
96.9
97.4
97.9

C
88.7
86.9
89.1
89.1
85.6
86.5
86.1
87.5

Avg
82.8
86.5
87.9
88.4
89.4
92.9
92.7
93.8

A
92.2
92.4
92.6
91.1
92.2
92.7
92.5
93.6

W
80.9
85.3
85.2
87.1
91.9
97.1
97.7
98.6

10%
D
88.2
89.5
93.3
97.0
97.4
97.3
96.6
98.6

C
88.9
89.2
87.2
87.6
86.8
86.6
86.7
87.3

Avg
87.6
89.1
89.6
90.7
92.0
93.4
93.4
94.5

A
91.3
93.5
94.3
90.0
92.6
93.2
91.9
94.4

W
83.3
89.0
87.0
88.8
97.6
96.9
96.6
98.3

20%
D
93.7
95.2
96.7
99.2
94.5
99.4
95.9
99.9

C
94.9
92.6
93.4
94.3
88.4
82.8
90.0
89.1

Avg
90.8
92.6
92.4
93.1
93.3
94.4
93.6
95.5

Table 2: Classiﬁcation accuracy on Ofﬁce-Home with standard evaluation protocol (VGGnet).

Method

STL (VGGnet)

MTFL [2]
RMTL [6]
MTRL [31]

DMTL-TF [27]

MRN8
MRNt

MRN (full)

A
35.8
40.1
42.3
42.7
49.2
52.7
52.0
53.3

C
31.2
30.4
32.8
33.3
34.5
34.7
34.0
36.4

5%
P
67.8
61.5
62.3
62.9
67.1
70.1
69.9
70.5

R
62.5
59.5
60.6
61.3
62.9
67.6
66.8
67.7

Avg
49.3
47.9
49.5
50.1
53.4
56.3
55.7
57.0

A
51.0
50.3
49.7
51.6
57.2
59.1
58.6
59.9

C
40.7
35.0
34.6
36.3
42.3
42.7
42.6
42.7

10%

P
75.0
66.3
65.9
67.7
73.6
75.1
74.9
76.3

R
68.8
65.0
64.6
66.3
69.9
72.8
72.4
73.0

Avg
58.9
54.2
53.7
55.5
60.8
62.4
62.1
63.0

A
56.1
55.2
55.2
55.8
58.3
58.4
57.7
58.5

C
54.6
38.8
39.2
39.9
56.1
55.6
54.8
55.6

20%

P
80.4
69.1
69.6
70.2
79.3
80.4
80.2
80.7

R
71.8
70.0
70.5
71.2
72.1
72.4
71.6
72.8

Avg
65.7
58.3
58.6
59.3
66.5
66.7
66.1
66.9

only for binary-class problems  due to the difﬁculty in dealing with order-3 parameter tensors for
multi-class problems. We adopt one-vs-rest strategy to enable them working on multi-class datasets.
We follow the standard evaluation protocol [31  5] for multi-task learning and randomly select 5% 
10%  and 20% samples from each task as training set and use the rest of the samples as test set. We
compare the average classiﬁcation accuracy for all tasks based on ﬁve random experiments  where
standard errors are generally less than ±0.5%  which are not signiﬁcant and thus are not reported for
space limitation. We conduct model selection for all methods using ﬁve-fold cross-validation on the
training set. For deep learning methods  we adopt AlexNet [16] and VGGnet [24]  ﬁx convolutional
layers conv1–conv5  ﬁne-tune fully-connected layers f c6–f c7  and train classiﬁer layer f c8 via
back-propagation. As the classiﬁer layer is trained from scratch  we set its learning rate to be 10 times
that of the other layers. We use mini-batch stochastic gradient descent (SGD) with 0.9 momentum
and learning rate decaying strategy  and select learning rate between 10−5 and 10−2 by stepsize 10 1
2 .

5.2 Results

The multi-task classiﬁcation results on the Ofﬁce-Caltech  Ofﬁce-Home and ImageCLEF-DA datasets
based on 5%  10%  and 20% sampled training data are shown in Tables 1  2 and 3  respectively. We
observe that the proposed MRN model signiﬁcantly outperforms the comparison methods on most
multi-task problems. The substantial accuracy improvement validates that our multilinear relationship
networks through multilayer and multilinear relationship learning is able to learn both transferable
features and adaptive task relationships  which enables effective and robust multi-task deep learning.
We can make the following observations from the results. (1) Shallow multi-task learning methods
MTFL  RMTL  and MTRL outperform single-task deep learning method STL in most cases  which
conﬁrms the efﬁcacy of learning multiple tasks by exploiting shared structures. Among the shallow
multi-task methods  MTRL gives the best accuracies  showing that exploiting task relationship may
be more effective than extracting shared feature subspace for multi-task learning. It is worth noting
that  although STL cannot learn from knowledge transfer  it can be ﬁne-tuned on each task to improve
performance  and thus when the number of training samples are large enough and when different
tasks are dissimilar enough (e.g. Ofﬁce-Home dataset)  STL may outperform shallow multi-task
learning methods  as evidenced by the results in Table 2. (2) Deep multi-task learning method
DMTL-TF outperforms shallow multi-task learning methods with deep features as input  which
conﬁrms the importance of learning deep transferable features to enable knowledge transfer across
tasks. However  DMTL-TF only learns the shared feature subspace based on tensor factorization of
the network parameters  while the task relationships in multiple network layers are not captured. This
may result in negative-transfer in the feature layers [28] and under-transfer in the classiﬁer layers.
Negative-transfer can be witnessed by comparing multi-task methods with single-task methods: if
multi-task learning methods yield lower accuracy in some of the tasks  then negative-transfer arises.

8

Table 3: Classiﬁcation accuracy on ImageCLEF-DA with standard evaluation protocol (AlexNet).

Method

STL (AlexNet)

MTFL [2]
RMTL [6]
MTRL [31]

DMTL-TF [27]

MRN8
MRNt

MRN (full)

C
77.4
79.9
81.1
80.8
87.9
87.0
88.5
89.6

I

60.3
68.6
71.3
68.4
70.0
74.4
73.5
76.9

5%
P
48.0
43.4
52.4
51.9
58.1
61.8
63.3
65.4

B
45.0
41.5
40.9
42.9
34.1
47.6
51.1
49.4

Avg
57.7
58.3
61.4
61.0
62.5
67.7
69.1
70.3

C
78.9
82.9
81.5
83.1
89.1
89.1
88.0
88.1

I

70.5
71.4
71.7
72.7
82.1
82.2
83.1
84.6

10%

P
48.1
56.7
55.6
54.5
58.7
64.4
67.4
68.7

B
41.8
41.7
45.3
45.5
48.0
49.3
54.8
55.6

Avg
59.8
63.2
63.5
63.9
69.5
71.2
73.3
74.3

C
83.3
83.1
83.3
83.7
91.7
91.1
91.1
92.8

I

74.9
72.2
73.3
75.5
80.0
84.1
83.5
83.3

20%

P
49.2
54.5
53.7
57.5
63.2
65.7
65.7
67.4

B
47.1
52.5
49.2
49.4
54.1
54.1
55.7
57.8

Avg
63.6
65.6
64.9
66.5
72.2
73.7
74.0
75.3

We go deeper into MRN by reporting the results of the two MRN variants: MRN8 and MRNt  all
signiﬁcantly outperform the comparison methods but generally underperform MRN (full)  which
verify our motivation that jointly learning transferable features and multilinear task relationships can
bridge multiple tasks more effectively. (1) The disadvantage of MRN8 is that it does not learn the
task relationship in the lower layers f c7  which are not safely transferable and may result in negative
transfer [28]. (2) The shortcoming of MRNt is that it does not learn the multilinear relationship of
features  classes and tasks  hence the learned relationships may only capture the task covariance
without capturing the feature covariance and class covariance  which may lose some intrinsic relations.

(a) MTRL Relationship

(b) MRN Relationship

(c) DMTL-TF Features

(d) MRN Features

Figure 3: Hinton diagram of task relationships (a)(b) and t-SNE embedding of deep features (c)(d).

5.3 Visualization Analysis

3

We show that MRN can learn more reasonable task relationships with deep features than MTRL with
shallow features  by visualizing the Hinton diagrams of task covariances learned by MTRL and MRN
(Σf c8
) in Figures 3(a) and 3(b)  respectively. Prior knowledge on task similarity in the Ofﬁce-Caltech
dataset [12] describes that tasks A  W and D are more similar with each other while they are relatively
dissimilar to task C. MRN successfully captures this prior task relationship and enhances the task
correlation across dissimilar tasks  which enables stronger transferability for multi-task learning.
Furthermore  all tasks are positively correlated (green color) in MRN  implying that all tasks can
better reinforce each other. However  some of the tasks (D and C) are still negatively correlated (red
color) in MTRL  implying these tasks should be drawn far apart and cannot improve with each other.
We illustrate the feature transferability by visualizing in Figures 3(c) and 3(d) the t-SNE embeddings
[18] of the images in the Ofﬁce-Caltech dataset with DMTL-TF features and MRN features  respec-
tively. Compared with DMTL-TF features  the data points with MRN features are discriminated better
across different categories  i.e.  each category has small intra-class variance and large inter-class
margin; the data points are also aligned better across different tasks  i.e. the embeddings of different
tasks overlap well  implying that different tasks reinforce each other effectively. This veriﬁes that with
multilinear relationship learning  MRN can learn more transferable features for multi-task learning.

6 Conclusion

This paper presented multilinear relationship networks (MRN) that integrate deep neural networks
with tensor normal priors over the network parameters of all task-speciﬁc layers  which model the task
relatedness through the covariance structures over tasks  classes and features to enable transfer across
related tasks. An effective learning algorithm was devised to jointly learn transferable features and
multilinear relationships. Experiments testify that MRN yields superior results on standard datasets.

9

AWDCAWDCAWDCAWDCCAWDCAWDAcknowledgments

This work was supported by the National Key R&D Program of China (2016YFB1000701)  National
Natural Science Foundation of China (61772299  61325008  61502265  61672313) and TNList Fund.

References
[1] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled

data. Journal of Machine Learning Research  6:1817–1853  2005.

[2] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learning 

73(3):243–272  2008.

[3] Y. Bengio  A. Courville  and P. Vincent. Representation learning: A review and new perspectives. IEEE

Transactions on Pattern Analysis and Machine Intelligence  35(8):1798–1828  2013.

[4] R. Caruana. Multitask learning. Machine learning  28(1):41–75  1997.
[5] J. Chen  L. Tang  J. Liu  and J. Ye. A convex formulation for learning a shared predictive structure from
multiple tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence  35(5):1025–1038  2013.
[6] J. Chen  J. Zhou  and J. Ye. Integrating low-rank and group-sparse structures for robust multi-task learning.

[7] X. Chu  W. Ouyang  W. Yang  and X. Wang. Multi-task recurrent neural network for immediacy prediction.

[8] C. Ciliberto  Y. Mroueh  T. Poggio  and L. Rosasco. Convex learning of multiple tasks and their structure.

In KDD  2011.

In ICCV  2015.

In ICML  2015.

[9] J. Donahue  Y. Jia  O. Vinyals  J. Hoffman  N. Zhang  E. Tzeng  and T. Darrell. Decaf: A deep convolutional

activation feature for generic visual recognition. In ICML  2014.

[10] T. Evgeniou and M. Pontil. Regularized multi-task learning. In KDD  2004.
[11] X. Glorot  A. Bordes  and Y. Bengio. Domain adaptation for large-scale sentiment classiﬁcation: A deep

learning approach. In ICML  2011.

CVPR  2012.

[12] B. Gong  Y. Shi  F. Sha  and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation. In

[13] A. K. Gupta and D. K. Nagar. Matrix variate distributions. Chapman & Hall  2000.
[14] L. Jacob  J.-P. Vert  and F. R. Bach. Clustered multi-task learning: A convex formulation. In NIPS  2009.
[15] Z. Kang  K. Grauman  and F. Sha. Learning with whom to share in multi-task feature learning. In ICML 

[16] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

[17] A. Kumar and H. Daume III. Learning task grouping and overlap in multi-task learning. ICML  2012.
[18] M. Long  Y. Cao  J. Wang  and M. I. Jordan. Learning transferable features with deep adaptation networks.

[19] A. Maurer  M. Pontil  and B. Romera-Paredes. The beneﬁt of multitask representation learning. The

Journal of Machine Learning Research  17(1):2853–2884  2016.

[20] I. Misra  A. Shrivastava  A. Gupta  and M. Hebert. Cross-stitch networks for multi-task learning. In CVPR 

2011.

networks. In NIPS  2012.

In ICML  2015.

2016.

[21] M. Ohlson  M. R. Ahmad  and D. Von Rosen. The multilinear normal distribution: Introduction and some

basic properties. Journal of Multivariate Analysis  113:37–47  2013.

[22] W. Ouyang  X. Chu  and X. Wang. Multisource deep learning for human pose estimation. In CVPR  2014.
[23] B. Romera-Paredes  H. Aung  N. Bianchi-Berthouze  and M. Pontil. Multilinear multitask learning. In

[24] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In

[25] N. Srivastava and R. Salakhutdinov. Discriminative transfer learning with tree-based priors. In NIPS  2013.
[26] H. Venkateswara  J. Eusebio  S. Chakraborty  and S. Panchanathan. Deep hashing network for unsupervised

domain adaptation. In CVPR  2017.

[27] Y. Yang and T. Hospedales. Deep multi-task representation learning: A tensor factorisation approach.

[28] J. Yosinski  J. Clune  Y. Bengio  and H. Lipson. How transferable are features in deep neural networks? In

[29] Y. Zhang and J. Schneider. Learning multiple tasks with a sparse matrix-normal penalty. In NIPS  2010.
[30] Y. Zhang and Q. Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114  2017.
[31] Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In

[32] Z. Zhang  P. Luo  C. C. Loy  and X. Tang. Facial landmark detection by deep multi-task learning. In ECCV 

ICML  2013.

ICLR  2015.

ICLR  2017.

NIPS  2014.

UAI  2010.

2014.

10

,Raffaello Camoriano
Lorenzo Rosasco
Mingsheng Long
ZHANGJIE CAO
Jianmin Wang
Philip Yu