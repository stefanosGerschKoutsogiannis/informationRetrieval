2013,Adaptivity to Local Smoothness and Dimension in Kernel Regression,We present the first result for kernel regression where the procedure adapts locally at a point $x$ to both the unknown local dimension of the metric and the unknown H\{o}lder-continuity of the regression function at $x$. The result holds with high probability simultaneously at all points $x$ in a metric space of unknown structure.",Adaptivity to Local Smoothness and Dimension in

Kernel Regression

Samory Kpotufe

Toyota Technological Institute-Chicago(cid:3)

samory@ttic.edu

Vikas K Garg

Toyota Technological Institute-Chicago

vkg@ttic.edu

Abstract

We present the ﬁrst result for kernel regression where the procedure adapts locally
at a point x to both the unknown local dimension of the metric space X and the
unknown H¨older-continuity of the regression function at x. The result holds with
high probability simultaneously at all points x in a general metric space X of
unknown structure.

1 Introduction

Contemporary statistical procedures are making inroads into a diverse range of applications in the
natural sciences and engineering. However it is difﬁcult to use those procedures ”off-the-shelf”
because they have to be properly tuned to the particular application. Without proper tuning their
prediction performance can suffer greatly. This is true in nonparametric regression (e.g. tree-based 
k-NN and kernel regression) where regression performance is particularly sensitive to how well the
method is tuned to the unknown problem parameters.
In this work  we present an adaptive kernel regression procedure  i.e. a procedure which self-tunes 
optimally  to the unknown parameters of the problem at hand.
We consider regression on a general metric space X of unknown metric dimension  where the output
Y is given as f (x) + noise. We are interested in adaptivity at any input point x 2 X : the algorithm
must self-tune to the unknown local parameters of the problem at x. The most important such
parameters (see e.g. [1  2])  are (1) the unknown smoothness of f  and (2) the unknown intrinsic
dimension  both deﬁned over a neighborhood of x. Existing results on adaptivity have typically
treated these two problem parameters separately  resulting in methods that solve only part of the
self-tuning problem.
In kernel regression  the main algorithmic parameter to tune is the bandwidth h of the kernel. The
problem of (local) bandwidth selection at a point x 2 X has received considerable attention in both
the theoretical and applied literature (see e.g. [3  4  5]). In this paper we present the ﬁrst method
which provably adapts to both the unknown local intrinsic dimension and the unknown H¨older-
continuity of the regression function f at any point x in a metric space of unknown structure. The
intrinsic dimension and H¨older-continuity are allowed to vary with x in the space  and the algorithm
must thus choose the bandwidth h as a function of the query x  for all possible x 2 X.
It is unclear how to extend global bandwidth selection methods such as cross-validation to the local
bandwidth selection problem at x. The main difﬁculty is that of evaluating the regression error at x
since the ouput Y at x is unobserved. We do have the labeled training sample to guide us in selecting
h(x)  and we will show an approach that guarantees a regression rate optimal in terms of the local
problem complexity at x.

(cid:3)

Other afﬁliation: Max Planck Institute for Intelligent Systems  Germany

1

The result combines various insights from previous work on regression. In particular  to adapt to
H¨older-continuity  we build on acclaimed results of Lepski et al. [6  7  8]. In particular some such
Lepski’s adaptive methods consist of monitoring the change in regression estimates fn;h(x) as the
bandwidth h is varied. The selected estimate has to meet some stability criteria. The stability criteria
is designed to ensure that the selected fn;h(x) is sufﬁciently close to a target estimate fn;~h(x) for
a bandwidth ~h known to yield an optimal regression rate. These methods however are generally
instantiated for regression in R  but extend to high-dimensional regression if the dimension of the
input space X is known. In this work however the dimension of X is unknown  and in fact X is
allowed to be a general metric space with signiﬁcantly less regularity than usual Euclidean spaces.
To adapt to local dimension we build on recent insights of [9] where a k-NN procedure is shown to
adapt locally to intrinsic dimension. The general idea for selecting k = k(x) is to balance surrogates
of the unknown bias and variance of the estimate. As a surrogate for the bias  nearest neighbor
distances are used  assuming f is globally Lipschitz. Since Lipschitz-continuity is a special case of
H¨older-continuity  the work of [9] corresponds in the present context to knowing the smoothness of
f everywhere. In this work we do not assume knowledge of the smoothness of f  but simply that f
is locally H¨older-continuous with unknown H¨older parameters.
Suppose we knew the smoothness of f at x  then we can derive an approach for selecting h(x) 
similar to that of [9]  by balancing the proper surrogates for the bias and variance of a kernel estimate.
Let (cid:22)h be the hypothetical bandwidth so-obtained. Since we don’t actually know the local smoothness
of f  our approach  similar to Lepski’s  is to monitor the change in estimates fn;h(x) as h varies  and
pick the estimate fn;^h(x) which is deemed close to the hypothetical estimate fn;(cid:22)h(x) under some
stability condition.
We prove nearly optimal local rates ~O
in terms of the local dimension d
at any point x and H¨older parameters (cid:21); (cid:11) depending also on x. Furthermore  the result holds with
high probability  simultaneously at all x 2 X   for n sufﬁciently large. Note that we cannot union-
bound over all x 2 X   so the uniform result relies on proper conditioning on particular events in our
variance bounds on estimates fn;h((cid:1)).
We start with deﬁnitions and theoretical setup in Section 2. The procedure is given in Section 3 
followed by a technical overview of the result in Section 4. The analysis follows in Section 5.

(cid:0)2(cid:11)=(2(cid:11)+d)

(cid:21)2d=(2(cid:11)+d)n

(cid:0)

(cid:1)

2 Setup and Notation

2.1 Distribution and sample
We assume the input X belongs to a metric space (X ; (cid:26)) of bounded diameter (cid:1)X (cid:21) 1. The output
Y belongs to a space Y of bounded diameter (cid:1)Y. We let (cid:22) denote the marginal measure on X and
(cid:22)n denote the corresponding empirical distribution on an i.i.d. sample of size n. We assume for
simplicity that (cid:1)X and (cid:1)Y are known.
The algorithm runs on an i.i.d training sample f(Xi; Yi)gn
fXign

i=1 of size n. We use the notation X

1 and Y = fYign
1 .

:
=

Regression function

= E [Y jx] satisﬁes local H¨older assumptions: for every
:
We assume the regression function f (x)
x 2 X and r > 0  there exists (cid:21); (cid:11) > 0 depending on x and r  such that f is ((cid:21); (cid:11))-H¨older at x on
B(x; r):

8x

0 2 B(x; r)

jf (x) (cid:0) f (x

0

)j (cid:20) (cid:21)(cid:26)(x; x
0

)(cid:11):

We note that the (cid:11) parameter is usually assumed to be in the interval (0; 1] for global deﬁnitions
of H¨older continuity  since a global (cid:11) > 1 implies that f is constant (for differentiable f). Here
however  the deﬁnition being given relative to x  we can simply assume (cid:11) > 0. For instance the
function f (x) = x(cid:11) is clearly locally (cid:11)-H¨older at x = 0 with constant (cid:21) = 1 for any (cid:11) > 0. With
higher (cid:11) = (cid:11)(x)  f gets ﬂatter locally at x  and regression gets easier.

2

Notion of dimension

We use the following notion of metric-dimension  also employed in [9]. This notion extends some
global notions of metric dimension to local regions of space . Thus it allows for the intrinsic dimen-
sion of the data to vary over space. As argued in [9] (see also [10] for a more general theory) it often
coincides with other natural measures of dimension such as manifold dimension.
Deﬁnition 1. Fix x 2 X   and r > 0. Let C (cid:21) 1 and d (cid:21) 1. The marginal (cid:22) is (C; d)-homogeneous
on B(x; r) if we have (cid:22)(B(x; r

0 (cid:20) r and 0 < (cid:15) < 1.

(cid:0)d(cid:22)(B(x; (cid:15)r

)) (cid:20) C(cid:15)

)) for all r

0

0

In the above deﬁnition  d will be viewed as the local dimension at x. We will require a general
upper-bound d0 on the local dimension d(x) over any x in the space. This is deﬁned below and can
be viewed as the worst-case intrinsic dimension over regions of space.
Assumption 1. The marginal (cid:22) is (C0; d0)-maximally-homogeneous for some C0 (cid:21) 1 and d0 (cid:21) 1 
i.e. the following holds for all x 2 X and r > 0: suppose there exists C (cid:21) 1 and d (cid:21) 1 such that (cid:22)
is (C; d)-homogeneous on B(x; r)  then (cid:22) is (C0; d0)-homogeneous on B(x; r).

Notice that if (cid:22) is (C; d)-homogeneous on some B(x; r)  then it is (C0; d0)-homogeneous on
B(x; r) for any C0 > C and d0 > d. Thus  C0; d0 can be viewed as global upper-bounds on
the local homogeneity constants. By the deﬁnition  it can be the case that (cid:22) is (C0; d0)-maximally-
homogeneous without being (C0; d0)-homogeneous on the entire space X .
The algorithm is assumed to know the upper-bound d0. This is a minor assumption: in many situa-
tions where X is a subset of a Euclidean space RD  D can be used in place of d0; more generally  the
global metric entropy (log of covering numbers) of X can be used in the place of d0 (using known
relations between the present notion of dimension and metric entropies [9  10]). The metric entropy
is relatively easy to estimate since it is a global quantity independent of any particular query x.
Finally we require that the local dimension is tight in small regions. This is captured by the following
assumption.
Assumption 2. There exists r(cid:22) > 0; C
where r < r(cid:22)  then for any r

> 0 such that if (cid:22) is (C; d)-homogeneous on some B(x; r)

0 (cid:20) r  (cid:22)(B(x; r

)) (cid:20) C

0d.

r

0

0

0

This last assumption extends (to local regions of space) the common assumption that (cid:22) has an upper-
bounded density (relative to Lebesgue). This is however more general in that (cid:22) is not required to
have a density.

2.2 Kernel Regression

We consider a positive kernel K on [0; 1] highest at 0  decreasing on [0; 1]  and 0 outside [0; 1]. The
kernel estimate is deﬁned as follows: if B(x; h) \ X 6= ; 
wi(x)Yi; where wi(x) =

fn;h(x) =

P

:

X

K((cid:26)(x; Xi)=h)
j K((cid:26)(x; Xj)=h)

We set wi(x) = 1=n; 8i 2 [n] if B(x; h) \ X = ;.

i

3 Procedure for Bandwidth Selection at x
Deﬁnition 2. (Global cover size) Let (cid:15) > 0. Let N(cid:26)((cid:15)) denote an upper-bound on the size of the
smallest (cid:15)-cover of (X ; (cid:26)).
We assume the global quantity N(cid:26)((cid:15)) is known or pre-estimated. Recall that  as discussed in Section
2  d0 can be picked to satisfy ln(N(cid:26)((cid:15))) = O(d0 log((cid:1)X =(cid:15)))  in other words the procedure requires
only knowledge of upper-bounds N(cid:26)((cid:15)) on global cover sizes.
The procedure is given as follows:

(cid:26)
n . For any x 2 X   the set of admissible bandwidths is given as
h (cid:21) 16(cid:15) : (cid:22)n(B(x; h=32)) (cid:21) 32 ln(N(cid:26)((cid:15)=2)=(cid:14))

(cid:27)\(cid:26)

Fix (cid:15) = (cid:1)X
^Hx =

(cid:27)dlog((cid:1)X =(cid:15))e

:

(cid:1)X
2i

i=0

n

3

(cid:0)
4 ln (N(cid:26)((cid:15)=2)=(cid:14)) + 9C04d0

(cid:1)

Let Cn;(cid:14) (cid:21) 2K(0)

K(1)

(cid:1)2Y Cn;(cid:14)

n (cid:1) (cid:22)n(B(x; h=2))

and Dh =
^(cid:27)h = 2
At every x 2 X select the bandwidth:

8<:h 2 ^Hx :

^h = max

p
p
: For any h 2 ^Hx  deﬁne

i

2^(cid:27)h; fn;h(x) +

2^(cid:27)h

:

h

fn;h(x) (cid:0)
\

h02 ^Hx:h0<h

9=; :

Dh0 6= ;

The main difference with Lepski’s-type methods is in the parameter ^(cid:27)h. In Lepski’s method  since
d is assumed known  a better surrogate depending on d will be used.

4 Discussion of Results

We have the following main theorem.
Theorem 1. Let 0 < (cid:14) < 1=e. Fix (cid:15) = (cid:1)X =n. Let Cn;(cid:14) (cid:21) 2K(0)
Deﬁne C2 =
least 1 (cid:0) 2(cid:14) over the choice of (X; Y)  simultaneously for all x 2 X and all r satisfying

(cid:0)
9C04d0 + 4 ln (N(cid:26)((cid:15)=2)=(cid:14))
. There exists N such that  for n > N  the following holds with probability at
(cid:19)1=(2(cid:11)+d0)

!1=(2(cid:11)+d0)(cid:18)

(cid:0)d0
4
6C0

 

(cid:1)

K(1)

:

r(cid:22) > r > rn   2

2d0C 2

0 (cid:1)d0X

(cid:1)2Y Cn;(cid:14)

C2(cid:21)2

n

Let x 2 X   and suppose f is ((cid:21); (cid:11))-H¨older at x on B(x; r). Suppose (cid:22) is (C; d)-homogeneous on
B(x; r). Let Cr

rd0(cid:0)d. We have

:
=

1

CC0(cid:1)d0X

(cid:12)(cid:12)f^h(x) (cid:0) f (x)

(cid:12)(cid:12)2 (cid:20) 96C02d0 (cid:1) (cid:21)2d=(2(cid:11)+d)

 

2d(cid:1)2Y Cn;(cid:14)
C2Cr(cid:21)2n

:

:

!2(cid:11)=(2(cid:11)+d)

n!1(cid:0)(cid:0)(cid:0)(cid:0)! 0.
The result holds with high probability for all x 2 X   and for all r(cid:22) > r > rn  where rn
Thus  as n grows  the procedure is eventually adaptive to the H¨older parameters in any neighborhood
of x. Note that the dimension d is the same for all r < r(cid:22) by deﬁnition of r(cid:22). As previously
discussed  the deﬁnition of r(cid:22) corresponds to a requirement that the intrinsic dimension is tight in
small enough regions. We believe this is a technical requirement due to our proof technique. We
hope this requirement might be removed in a longer version of the paper.
Notice that r is a factor of n in the upper-bound. Since the result holds simultaneously for all
r(cid:22) > r > rn  the best tradeoff in terms of smoothness and size of r is achieved. A similar tradeoff
is observed in the result of [9].
As previously mentioned  the main idea behind the proof is to introduce hypothetical bandwidths (cid:22)h
and and ~h which balance respectively  ^(cid:27)h and (cid:21)2h2(cid:11)  and O((cid:1)2Y =(nhd)) and (cid:21)2h2(cid:11) (see Figure 1).
In the ﬁgure  d and (cid:11) are the unknown parameters in some neighborhood of point x.
The ﬁrst part of the proof consists in showing that the variance of the estimate using a bandwidth
h is at most ^(cid:27)h. With high probability ^(cid:27)h is bounded above by O((cid:1)2Y =(nhd). Thus by balancing
(cid:0)2(cid:11)=(2(cid:11)+d). We then have to show
O((cid:1)2Y =(nhd) and (cid:21)2h2(cid:11)  using ~h we would achieve a rate of n
that the error of fn;(cid:22)h cannot be too far from that fn;~h.
Finally the error of fn;^h  ^h being selected by the procedure  will be related to that of fn;(cid:22)h.
The argument is a bit more nuanced that just described above and in Figure 1: the respective curves
O((cid:1)2Y =(nhd) and (cid:21)2h2(cid:11) are changing with h since dimension and smoothness at x depend on the
size of the region considered. Special care has to be taken in the analysis to handle this technicality.

4

(Left) The proof argues over (cid:22)h  ~h which balance respectively  ^(cid:27)h and (cid:21)2h2(cid:11)  and
Figure 1:
O((cid:1)2Y =(nhd)) and (cid:21)2h2(cid:11). The estimates under ^h selected by the procedure is shown to be close to
that of (cid:22)h  which in turn is shown to be close to that of ~h which is of the right adaptive form.

(Right) Simulation results comparing the error of the proposed method to that of a global h
selected by cross-validation. The test size is 1000 for all experiments. X (cid:26) R70 has diameter
1  and is a collection of 3 disjoint ﬂats (clusters) of dimension d1 = 2; d2 = 5; d3 = 10  and
equal mass 1=3. For each x from cluster i we have the output Y = (sinkxk)ki + N (0; 1)
where k1 = 0:8; k2 = 0:6; k3 = 0:4. For the implementation of the proposed method  we set
^(cid:27)h(x) = ^varY =n(cid:22)n(B(x; h))  where ^varY is the variance of Y on the training sample. For both our
method and cross-validation  we use a box-kernel  and we vary h on an equidistant 100-knots grid
on the interval from the smallest to largest interpoint distance on the training sample.

5 Analysis

X
We will make use of the the following bias-variance decomposition throughout the analysis. For any
x 2 X and bandwidth h  deﬁne the expected regression estimate
(cid:12)(cid:12)(cid:12)fn;h(x) (cid:0) efn;h(x)
(cid:12)(cid:12)(cid:12)2

(cid:12)(cid:12)(cid:12)efn;h(x) (cid:0) f (x)

jfn;h(x) (cid:0) f (x)j2 (cid:20) 2

efn;h(x)

:
= EYjXfn;h(x) =

wif (Xi):

We have

(cid:12)(cid:12)(cid:12)2

:

i

+ 2

(1)

The bias term above is easily bounded in a standard way. This is stated in the Lemma below.
Lemma 1 (Bias). Let x 2 X   and suppose f is ((cid:21); (cid:11))-H¨older at x on B(x; h). For any h > 0  we
have

(cid:12)(cid:12)(cid:12)efn;h(x) (cid:0) f (x)

(cid:12)(cid:12)(cid:12)2 (cid:20) (cid:21)2h2(cid:11):
(cid:12)(cid:12)(cid:12)efn;h(x) (cid:0) f (x)
(cid:12)(cid:12)(cid:12) (cid:20)P

Proof. We have

i wi(x)jf (Xi) (cid:0) f (x)j (cid:20) (cid:21)h(cid:11).

The rest of this section is dedicated to the analysis of the variance term of (1). We will need various
supporting Lemmas relating the empirical mass of balls to their true mass. This is done in the next
subsection. The variance results follow in the subsequent subsection.

5.1 Supporting Lemmas
We often argue over the following distributional counterpart to ^Hx((cid:15)).
Deﬁnition 3. Let x 2 X and (cid:15) > 0. Deﬁne

(cid:26)
h (cid:21) 8(cid:15) : (cid:22)(B(x; h=8)) (cid:21) 12 ln(N(cid:26)((cid:15)=2)=(cid:14))

(cid:27)\(cid:26)

Hx((cid:15)) =

n

(cid:27)dlog((cid:1)X =(cid:15))e

:

i=0

(cid:1)X
2i

5

3000400050006000700080009000100001234567Training SizeNMSE Cross ValidationAdaptiveLemma 2. Fix (cid:15) > 0 and let Z denote an (cid:15)=2-cover of X   and let S(cid:15) =

4 ln(N(cid:26)((cid:15)=2)=(cid:14))

:
=

(cid:13)n

n

. With probability at least 1 (cid:0) (cid:14)  for all z 2 Z and h 2 S(cid:15) we have

p
p
(cid:13)n (cid:1) (cid:22)(B(z; h)) + (cid:13)n=3;
(cid:22)n(B(z; h)) (cid:20) (cid:22)(B(z; h)) +
(cid:13)n (cid:1) (cid:22)n(B(z; h)) + (cid:13)n=3:
(cid:22)(B(z; h)) (cid:20) (cid:22)n(B(z; h)) +

i=0

(cid:27)dlog((cid:1)X =(cid:15))e

(cid:26)

(cid:1)X
2i

. Deﬁne

(2)
(3)

Idea. Apply Bernstein’s inequality followed by a union bound on Z and S(cid:15).

The following two lemmas result from the above Lemma 2.
Lemma 3. Fix (cid:15) > 0 and 0 < (cid:14) < 1. With probability at least 1 (cid:0) (cid:14)  for all x 2 X and h 2 Hx((cid:15)) 
we have for C1 = 3C04d0 and C2 =

 

(cid:0)d0
4
6C0

C2(cid:22)(B(x; h=2)) (cid:20) (cid:22)n(B(x; h=2)) (cid:20) C1(cid:22)(B(x; h=2)):

Lemma 4. Let 0 < (cid:14) < 1  and (cid:15) > 0. With probability at least 1(cid:0)(cid:14)  for all x 2 X   ^Hx((cid:15)) (cid:26) Hx((cid:15)).

Proof. Again  let Z be an (cid:15)=2 cover and deﬁne S(cid:15) and (cid:13)n as in Lemma 2. Assume (2) in the
statement of Lemma 2. Let h > 16(cid:15)  we have for any z 2 Z and x within (cid:15)=2 of z 

(cid:22)n(B(x; h=32)) (cid:20) (cid:22)n(B(z; h=16)) (cid:20) 2(cid:22)(B(z; h=16)) + 2(cid:13)n (cid:20) 2(cid:22)(B(x; h=8)) + 2(cid:13)n;
2 (cid:22)n(B(x; h=32)) (cid:0) (cid:13)n. Pick h 2 ^Hx and conclude.

and we therefore have (cid:22)(B(x; h=8)) (cid:21) 1

5.2 Bound on the variance

The following two results of Lemma 5 to 6 serve to bound the variance of the kernel estimate.
These results are standard and included here for completion. The main result of this section is the
variance bound of Lemma 7. This last lemma bounds the variance term of (1) with high probability
simultaneously for all x 2 X and for values of h relevant to the algorithm.
Lemma 5. For any x 2 X and h > 0:

(cid:12)(cid:12)(cid:12)fn;h(x) (cid:0) efn;h(x)

(cid:12)(cid:12)(cid:12)2 (cid:20)

X

EYjX

w2

i (x)(cid:1)2Y :

P
Lemma 6. Suppose that for some x 2 X and h > 0  (cid:22)n(B(x; h)) 6= 0. We then have:

i

K(0)

i w2

K(1) (cid:1) n(cid:22)n(B(x; h))

i (x) (cid:20) maxi wi(x) (cid:20)
(cid:0)

(cid:1)
(cid:12)(cid:12)(cid:12)fn;h(x) (cid:0) efn;h(x)
(cid:12)(cid:12)(cid:12)2 (cid:20)
9C04d0 + 4 ln (N(cid:26)((cid:15)=2)=(cid:14))
for all x 2 X and all h 2 ^Hx((cid:15)) 

:

Lemma 7 (Variance bound). Let 0 < (cid:14) < 1=2 and (cid:15) > 0.
2K(0)
K(1)

:
=
  With probability at least 1 (cid:0) 3(cid:14) over the choice of (X; Y) 

Deﬁne Cn;(cid:14)

(cid:1)2Y Cn;(cid:14)

n(cid:22)n(B(x; h=2))

.

Proof. We prove the lemma statement for h 2 Hx((cid:15)). The result then follows for h 2 ^Hx((cid:15)) with
the same probability since  by Lemma 4  ^Hx((cid:15)) (cid:26) Hx((cid:15)) under the same event of Lemma 2.
Consider any (cid:15)=2-cover Z of X . Deﬁne (cid:13)n as in Lemma 2 and assume statement (3). Let x 2 X
and z 2 Z within distance (cid:15)=2 of x. Let h 2 Hx((cid:15)). We have

(cid:22)(B(x; h=8)) (cid:20) (cid:22)(B(z; h=4)) (cid:20) 2(cid:22)n(B(z; h=4)) + 2(cid:13)n (cid:20) 2(cid:22)n(B(x; h=2)) + 2(cid:13)n;

and we therefore have (cid:22)n(B(x; h=2)) (cid:21) 1
2 (cid:13)n. Thus deﬁne Hz denote the
union of Hx((cid:15)) for x 2 B(z; (cid:15)=2). With probability at least 1(cid:0) (cid:14)  for all z 2 Z  and x 2 B(z; (cid:15)=2) 

2 (cid:22)(B(x; h=8)) (cid:0) (cid:13)n (cid:21) 1

6

and h 2 Hz the sets B(z; h)\X  B(x; h)\X are all non empty since they all contain B(x
; h=2)\X
0
0 such that h 2 Hx0 ((cid:15)) . The corresponding kernel estimates are therefore well deﬁned.
for some x
Assume w.l.o.g. that Z is a minimal cover  i.e. all B(z; (cid:15)=2) contain some x 2 X .
We ﬁrst condition on X ﬁxed and argue over the randomness in Y. For any x 2 X and h > 0 
let Yx;h denote the subset of Y corresponding to points from X falling in B(x; h). We deﬁne
(cid:30)(Yx;h)

(cid:12)(cid:12)(cid:12)fn;h(x) (cid:0) efn;h(x)
(cid:12)(cid:12)(cid:12).

:
=

We note that changing any Yi value changes (cid:30)(Yz;h) by at most (cid:1)Y wi(z). Applying McDiarmid’s
inequality and taking a union bound over z 2 Z and h 2 Hz  we get

P(9z 2 Z;9h 2 S(cid:15); (cid:30)(Yz;h) > E(cid:30)(Yz;h) + t) (cid:20) N 2

(cid:26) ((cid:15)=2) exp

(cid:12)(cid:12)(cid:12)fn;h(z) (cid:0) efn;h(z)

We then have with probability at least 1 (cid:0) 2(cid:14)  for all z 2 Z and h 2 Hz 

(cid:12)(cid:12)(cid:12)2 (cid:20) 2 EY jX
(cid:18)

(cid:16)(cid:12)(cid:12)(cid:12)fn;h(z) (cid:0) efn;h(z)
(cid:18)N(cid:26)((cid:15)=2)

(cid:19)(cid:19)

(cid:12)(cid:12)(cid:12)(cid:17)2

(cid:20)

4 ln

(cid:14)

+ 2 ln

(cid:1)

K(0)(cid:1)2Y

K(1) (cid:1) n(cid:22)n(B(z; h))

(cid:14)

;

0BB@(cid:0)
(cid:18)N(cid:26)((cid:15)=2)

(cid:1)2Y

1CCA :
X

w2

i (z)

2t2

X
(cid:19)

i

(cid:1)2Y (cid:1)

w2

i (z)

i

(4)

where we apply Lemma 5 and 6 for the last inequality.
Now ﬁx any z 2 Z  h 2 Hz and x 2 B(z; (cid:15)=2). We have j(cid:30)(Yx;h) (cid:0) (cid:30)(Yz;h)j (cid:20)
maxf(cid:30)(Yx;h); (cid:30)(Yz;h)g since both quantities are positive. Thus j(cid:30)(Yx;h) (cid:0) (cid:30)(Yz;h)j changes by
at most maxi;j fwi(z); wj(x)g (cid:1) (cid:1)Y if we change any Yi value out of the contributing Y values. By
Lemma 6  maxi;j fwi(z); wj(x)g (cid:20) (cid:12)n;h(x; z)
: Thus

K(0)

:
=

nK(1) min((cid:22)n(B(x; h)); (cid:22)n(B(z; h)))

1

:
=

deﬁne h(x; z)
 h(x; z). By what we
just argued  changing any Yi makes h(z) vary by at most (cid:1)Y. We can therefore apply McDiarmid’s
inequality to have that  with probability at least 1 (cid:0) 3(cid:14)  for all z 2 Z and h 2 Hz 

x:(cid:26)(x;z)(cid:20)(cid:15)=2

(cid:12)n;h(x; z)

sup

j(cid:30)(Yx;h) (cid:0) (cid:30)(Yz;h))j and h(z)

:
=

 h(z) (cid:20) EYjX h(z) + (cid:1)Y

(5)
To bound the above expectation for any z and h 2 Hz  consider a sequence fxig1
1 ; xi 2 B(z; (cid:15)=2)
such that h(xi; z) i!1(cid:0)(cid:0)(cid:0)! h(z). Fix any such xi. Using Holder’s inequality and invoking Lemma
5 and Lemma 6  we have

2n

:

2 ln(N(cid:26)((cid:15)=2)=(cid:14))

r

p
EYjX((cid:30)(Yxi;h) (cid:0) (cid:30)(Yz;h)2)
q

(cid:12)n;h(xi; z)

(cid:20)

4(cid:1)2Y (cid:12)n;h(xi; z)

(cid:12)n;h(xi; z)

EYjX h(xi; z) =

(cid:20)

=

p
p

1

EYjX j(cid:30)(Yxi;h) (cid:0) (cid:30)(Yz;h)j (cid:20)

(cid:12)n;h(xi; z)

2EYjX(cid:30)(Yxi;h)2 + 2EYjX(cid:30)(Yz;h)2

s

(cid:12)n;h(xi; z)
(cid:20) 2(cid:1)Y

2(cid:1)Y

(cid:12)n;h(xi; z)

nK(1)(cid:22)n(B(z; h))

:

K(0)

s

Since h(xi; z) is bounded for all xi 2 B(z; (cid:15))  the Dominated Convergence Theorem yields

EYjX h(z) = lim
i!1

E YjX h(xi; z) (cid:20) 2(cid:1)Y

nK(1)(cid:22)n(B(z; h))

K(0)

:

Therefore  using (5)  we have for any z 2 Z  any h 2 Hz  and any x 2 B(z; (cid:15)=2) that  with
probability at least 1 (cid:0) 3(cid:14)

 

s

!

r

j(cid:30)(Yx;h) (cid:0) (cid:30)(Yz;h))j (cid:20) (cid:1)Y (cid:12)n;h(x; z)

2 ln(N(cid:26)((cid:15)=2)=(cid:14))

2n

:

(6)

2

nK(1)(cid:22)n(B(z; h))

K(0)

+

7

Figure 2: Illustration of the selection procedure. The intervals Dh are shown containing f (x). We
will argue that fn;^h(x) cannot be too far from fn;(cid:22)h(x).

Now notice that (cid:12)n;h(x; z) (cid:20)

K(0)

nK(1)(cid:22)n(B(x; h=2))

  so by Lemma 3 

(cid:22)n(B(z; h)) (cid:20) (cid:22)n(B(x; 2h)) (cid:20) C1(cid:22)(B(x; 2h)) (cid:20) C1C04d0 (cid:22)(B(x; h=2))

(cid:20) C2C1C04d0 (cid:22)n(B(x; h=2)) (cid:20) C04d0 (cid:22)n(B(x; h=2)):

Hence  (6) becomes j(cid:30)(Yx;h) (cid:0) (cid:30)(Yz;h))j (cid:20) 3(cid:1)Y
Combine with (4)  using again the fact that (cid:22)n(B(z; h)) (cid:21) (cid:22)n(B(x; h=2)) to obtain
(cid:1)
+ 2j(cid:30)(Yx;h) (cid:0) (cid:30)(Yz;h))j2
9C04d0 + 4 ln (N(cid:26)((cid:15)=2)=(cid:14))

(cid:12)(cid:12)(cid:12)fn;h(x) (cid:0) efn;h(x)

(cid:12)(cid:12)(cid:12)2 (cid:20) 2

(cid:20)

C04d0 K(0)

nK(1)(cid:22)n(B(x;h=2)) :

:

q
(cid:12)(cid:12)(cid:12)fn;h(z) (cid:0) efn;h(z)
(cid:12)(cid:12)(cid:12)2
(cid:1)(cid:0)

2(cid:1)2Y

n(cid:22)n(B(x; h=2))

5.3 Adaptivity

The proof of Theorem 1 is given in the appendix. As previously discussed  the main part of the
argument consists of relating the error of fn;(cid:22)h(x) to that of fn;~h(x) which is of the right form for
B(x; r) appropriately deﬁned as in the theorem statement.
To relate the error of fn;^h(x) to that fn;(cid:22)h(x)  we employ a simple argument inspired by Lepski’s
adaptivity work. Notice that  by deﬁnition of ^h (see Figure 1 (Left))  for any h (cid:20) (cid:22)h ^(cid:27)h (cid:21) (cid:21)2h2(cid:11).
Therefore by Lemma 1 and 7 that  for any h < (cid:22)h  kfn;h (cid:0) fk2 (cid:20) 2^(cid:27)h so the intervals Dh must
all contain f (x) and therefore must intersect. By the same argument ^h (cid:21) (cid:22)h and D^h and D(cid:22)h must
intersect. Now since ^(cid:27)h is decreasing  we can infer that fn;^h(x) cannot be too far from fn;(cid:22)h(x)  so
their errors must be similar. This is illustrated in Figure 2.

References
[1] C. J. Stone. Optimal rates of convergence for non-parametric estimators. Ann. Statist.  8:1348–

1360  1980.

[2] C. J. Stone. Optimal global rates of convergence for non-parametric estimators. Ann. Statist. 

10:1340–1353  1982.

[3] W. S. Cleveland and C. Loader. Smoothing by local regression: Principles and methods. Sta-

tistical theory and computational aspects of smoothing  1049  1996.

[4] L. Gyorﬁ  M. Kohler  A. Krzyzak  and H. Walk. A Distribution Free Theory of Nonparametric

Regression. Springer  New York  NY  2002.

8

[5] J. Lafferty and L. Wasserman. Rodeo: Sparse nonparametric regression in high dimensions.

Arxiv preprint math/0506342  2005.

[6] O. V. Lepski  E. Mammen  and V. G. Spokoiny. Optimal spatial adaptation to inhomogeneous
smoothness: an approach based on kernel estimates with variable bandwidth selectors. The
Annals of Statistics  pages 929–947  1997.

[7] O. V. Lepski and V. G. Spokoiny. Optimal pointwise adaptive methods in nonparametric esti-

mation. The Annals of Statistics  25(6):2512–2546  1997.

[8] O. V. Lepski and B. Y. Levit. Adaptive minimax estimation of inﬁnitely differentiable func-

tions. Mathematical Methods of Statistics  7(2):123–156  1998.

[9] S. Kpotufe. k-NN Regression Adapts to Local Intrinsic Dimension. NIPS  2011.
[10] K. Clarkson. Nearest-neighbor searching and metric space dimensions. Nearest-Neighbor

Methods for Learning and Vision: Theory and Practice  2005.

9

,Samory Kpotufe
Vikas Garg
Jing Qian
Venkatesh Saligrama