2012,Optimal kernel choice for large-scale two-sample tests,Abstract Given samples from distributions $p$ and $q$  a two-sample test determines whether to reject the null hypothesis that $p=q$  based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD)  which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is thus critical in ensuring the test has high power  and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error)  the kernel is chosen so as to maximize the test power  and minimize the probability of making a Type II error. The test statistic  test threshold  and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams  where the observations cannot all be stored in memory. In experiments  the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.,Optimal kernel choice for large-scale two-sample tests

Arthur Gretton 1 3 Bharath Sriperumbudur 1 Dino Sejdinovic 1 Heiko Strathmann2

1Gatsby Unit and 2CSD  CSML  UCL  UK; 3MPI for Intelligent Systems  Germany

{arthur.gretton bharat.sv dino.sejdinovic heiko.strathmann}@gmail
Sivaraman Balakrishnan

Kenji Fukumizu

Massimiliano Pontil
CSD  CSML  UCL  UK

ISM  Japan

LTI  CMU  USA

sbalakri@cs.cmu.edu

m.pontil@cs.ucl.ac.uk

fukumizu@ism.ac.jp

Abstract

Given samples from distributions p and q  a two-sample test determines whether
to reject the null hypothesis that p = q  based on the value of a test statistic
measuring the distance between the samples. One choice of test statistic is the
maximum mean discrepancy (MMD)  which is a distance between embeddings
of the probability distributions in a reproducing kernel Hilbert space. The kernel
used in obtaining these embeddings is critical in ensuring the test has high power 
and correctly distinguishes unlike distributions with high probability. A means of
parameter selection for the two-sample test based on the MMD is proposed. For a
given test level (an upper bound on the probability of making a Type I error)  the
kernel is chosen so as to maximize the test power  and minimize the probability
of making a Type II error. The test statistic  test threshold  and optimization over
the kernel parameters are obtained with cost linear in the sample size. These
properties make the kernel selection and test procedures suited to data streams 
where the observations cannot all be stored in memory. In experiments  the new
kernel selection approach yields a more powerful test than earlier kernel selection
heuristics.

1 Introduction

The two sample problem addresses the question of whether two independent samples are drawn from
the same distribution. In the setting of statistical hypothesis testing  this corresponds to choosing
whether to reject the null hypothesis H0 that the generating distributions p and q are the same  vs.
the alternative hypothesis HA that distributions p and q are different  given a set of independent
observations drawn from each.
A number of recent approaches to two-sample testing have made use of mappings of the distribu-
tions to a reproducing kernel Hilbert space (RKHS); or have sought out RKHS functions with large
amplitude where the probability mass of p and q differs most [8  10  15  17  7]. The most straight-
forward test statistic is the norm of the difference between distribution embeddings  and is called
the maximum mean discrepancy (MMD). One difﬁculty in using this statistic in a hypothesis test 
however  is that the MMD depends on the choice of the kernel. If we are given a family of kernels 
we obtain a different value of the MMD for each member of the family  and indeed for any positive
deﬁnite linear combination of the kernels. When a radial basis function kernel (such as the Gaus-
sian kernel) is used  one simple choice is to set the kernel width to the median distance between
points in the aggregate sample [8  7]. While this is certainly straightforward  it has no guarantees of
optimality. An alternative heuristic is to choose the kernel that maximizes the test statistic [15]: in
experiments  this was found to reliably outperform the median approach. Since the MMD returns
a smooth RKHS function that minimizes classiﬁcation error under linear loss  then maximizing the

1

MMD corresponds to minimizing this classiﬁcation error under a smoothness constraint.
If the
statistic is to be applied in hypothesis testing  however  then this choice of kernel does not explicitly
address the question of test performance.
We propose a new approach to kernel choice for hypothesis testing  which explicitly optimizes the
performance of the hypothesis test. Our kernel choice minimizes Type II error (the probability of
wrongly accepting H0 when p ￿= q)  given an upper bound on Type I error (the probability of
wrongly rejecting H0 when p = q). This corresponds to optimizing the asymptotic relative efﬁ-
ciency in the sense of Hodges and Lehmann [13  Ch. 10]. We address the case of the linear time
statistic in [7  Section 6]  for which both the test statistic and the parameters of the null distribu-
tion can be computed in O(n)  for sample size n. This has a higher variance at a given n than
the U-statistic estimate costing O(n2) used in [8  7]  since the latter is the minimum variance un-
biased estimator. Thus  we would use the quadratic time statistic in the “limited data  unlimited
time” scenario  as it extracts the most possible information from the data available. The linear time
statistic is used in the “unlimited data  limited time” scenario  since it is the cheapest statistic that
still incorporates each datapoint: it does not require the data to be stored  and is thus appropriate
for analyzing data streams. As a further consequence of the streaming data setting  we learn the
kernel parameter on a separate sample to the sample used in testing; i.e.  unlike the classical testing
scenario  we use a training set to learn the kernel parameters. An advantage of this setting is that our
null distribution remains straightforward  and the test threshold can be computed without a costly
bootstrap procedure.
We begin our presentation in Section 2 with a review of the maximum mean discrepancy  its linear
time estimate  and the associated asymptotic distribution and test. In Section 3 we describe a cri-
terion for kernel choice to maximize the Hodges and Lehmann asymptotic relative efﬁciency. We
demonstrate the convergence of the empirical estimate of this criterion when the family of kernels is
a linear combination of base kernels (with non-negative coefﬁcients)  and of the kernel coefﬁcients
themselves. In Section 4  we provide an optimization procedure to learn the kernel weights. Finally 
in Section 5  we present experiments  in which we compare our kernel selection strategy with the
approach of simply maximizing the test statistic subject to various constraints on the coefﬁcients of
the linear combination; and with a cross-validation approach  which follows from the interpretation
of the MMD as a classiﬁer. We observe that a principled kernel choice for testing outperforms com-
peting heuristics  including the previous best-performing heuristic in [15]. A Matlab implementation
is available at: www.gatsby.ucl.ac.uk/ ∼ gretton/adaptMMD/adaptMMD.htm
2 Maximum mean discrepancy  and a linear time estimate

We begin with a brief review of kernel methods  and of the maximum mean discrepancy [8  7  14].
We then describe the family of kernels over which we optimize  and the linear time estimate of the
MMD.

2.1 MMD for a family of kernels
Let Fk be a reproducing kernel Hilbert space (RKHS) deﬁned on a topological space X with repro-
ducing kernel k  and p a Borel probability measure on X . The mean embedding of p in Fk is a unique
element µk(p) ∈F k such that Ex∼pf (x) = ￿f  µk(p)￿Fk for all f ∈F k [4]. By the Riesz rep-
resentation theorem  a sufﬁcient condition for the existence of µk(p) is that k be Borel-measurable
and Ex∼pk1/2(x  x) < ∞. We assume k is a bounded continuous function  hence this condition
holds for all Borel probability measures. The maximum mean discrepancy (MMD) between Borel
probability measures p and q is deﬁned as the RKHS-distance between the mean embeddings of p
and q. An expression for the squared MMD is thus

ηk(p  q) = ￿µk(p) − µk(q)￿2
Fk

= Exx￿k(x  x￿) + Eyy￿k(y  y￿) − 2Exyk(x  y) 

(1)

(2)

where x  x￿ i.i.d.∼ p and y  y￿ i.i.d.∼ q. By introducing

hk(x  x￿  y  y￿) = k(x  x￿) + k(y  y￿) − k(x  y￿) − k(x￿  y) 

we can write

ηk(p  q) = Exx￿yy￿hk(x  x￿  y  y￿) =: Evhk(v) 

2

where we have deﬁned the random vector v := [x  x￿  y  y￿]. If µk is an injective map  then k is said
to be a characteristic kernel  and the MMD is a metric on the space of Borel probability measures 
i.e.  ηk (p  q) = 0 iff p = q [16]. The Gaussian kernels used in the present work are characteristic.
Our goal is to select a kernel for hypothesis testing from a particular family K of kernels  which we
now deﬁne. Let {ku}d

u=1 be a set of positive deﬁnite functions ku : X×X→ R. Let
βu = D  βu ≥ 0  ∀u ∈{ 1  . . .   d}￿

K :=￿k : k =

βuku 

(3)

for some D > 0  where the constraint on the sum of coefﬁcients is needed for the consistency proof
(see Section 3). Each k ∈K is associated uniquely with an RKHS Fk  and we assume the kernels
are bounded  |ku|≤ K  ∀u ∈{ 1  . . .   d}. The squared MMD becomes

d￿u=1

d￿u=1

ηk(p  q) = ￿µk(p) − µk(q)￿2
Fk

=

βuηu(p  q) 

d￿u=1

where ηu(p  q) := Evhu(v). It is clear that if every kernel ku  u ∈{ 1  . . .   d}  is characteristic and at
least one βu > 0  then k is characteristic. Where there is no ambiguity  we will write ηu := ηu(p  q)
and Ehu := Evhu(v). We denote h = (h1  h2  . . .   hd)￿ ∈ Rd×1  β = (β1 β 2  . . .  β d)￿ ∈ Rd×1 
and η = (η1 η 2  . . .  η d)￿ ∈ Rd×1. With this notation  we may write

ηk(p  q) = E(β￿h) = β￿η.

2.2 Empirical estimate of the MMD  asymptotic distribution  and test

We now describe an empirical estimate of the maximum mean discrepancy  given i.i.d. samples
X := {x1  . . .   xn} and Y := {y1  . . .   yn} from p and q  respectively. We use the linear time
estimate of [7  Section 6]  for which both the test statistic and the parameters of the null distribution
can be computed in time O(n). This has a higher variance at a given n than a U-statistic estimate
costing O(n2)  since the latter is the minimum variance unbiased estimator [13  Ch. 5]. That
said  it was observed experimentally in [7  Section 8.3] that the linear time statistic yields better
performance at a given computational cost than the quadratic time statistic  when sufﬁcient data
are available (bearing in mind that consistent estimates of the null distribution in the latter case are
computationally demanding [9]). Moreover  the linear time statistic does not require the sample
to be stored in memory  and is thus suited to data streaming contexts  where a large number of
observations arrive in sequence.
The linear time estimate of ηk(p  q) is deﬁned in [7  Lemma 14]: assuming for ease of notation that
n is even 

ˇηk =

2
n

n/2￿i=1

hk(vi) 

(4)

where vi := [x2i−1  x2i  y2i−1  y2i] and hk(vi) := k(x2i−1  x2i) + k(y2i−1  y2i)− k(x2i−1  y2i)−
k(x2i  y2i−1); this arrangement of the samples ensures we get an expectation over independent
variables as in (2) with cost O(n). We use ˇηk to denote the empirical statistic computed over the
samples being tested  to distinguish it from the training sample estimate ˆηk used in selecting the
kernel. Given the family of kernels K in (3)  this can be written ˇηk = β￿ ˇη  where we again use
the convention ˇη = (ˇη1  ˇη2  . . .   ˇηd)￿ ∈ Rd×1. The statistic ˇηk has expectation zero under the null
hypothesis H0 that p = q  and has positive expectation under the alternative hypothesis HA that
p ￿= q.
Since ˇηk is a straightforward average of independent random variables  its asymptotic distribution
is given by the central limit theorem (e.g. [13  Section 1.9]). From [7  corollary 16]  under the
assumption 0 < E(h2

k) < ∞ (which is true for bounded continuous k) 

where the factor of two arises since the average is over n/2 terms  and

n1/2 (ˇηk − ηk(p  q)) D→N (0  2σ2
k) 

σ2
k = Evh2

k(v) − [Ev(hk(v))]2 .

3

(5)

(6)

Unlike the case of a quadratic time statistic  the null and alternative distributions differ only in
mean; by contrast  the quadratic time statistic has as its null distribution an inﬁnite weighted sum of
χ2 variables [7  Section 5]  and a Gaussian alternative distribution.
To obtain an estimate of the variance based on the samples X  Y   we will use an expression derived
from the U-statistic of [13  p. 173] (although as earlier  we will express this as a simple average so
as to compute it in linear time). The population variance can be written

k(v) − Ev v￿(hk(v)hk(v￿)) =
Expanding in terms of the kernel coefﬁcients β  we get

σ2
k = Evh2

1
2

Ev v￿(hk(v) − hk(v￿))2.

where Qk := cov(h) is the covariance matrix of h. A linear time estimate for the variance is

k := β￿Qkβ 
σ2

(7)

k = β￿ ˇQkβ  where
ˇσ2

n/4￿i=1
4
n
and wi := [v2i−1  v2i] 1 h∆ k(wi) := hk(v2i−1) − hk(v2i).
We now address the construction of a hypothesis test. We denote by Φ the CDF of a standard Normal
random variable N (0  1)  and by Φ−1 the inverse CDF. From (5)  a test of asymptotic level α using
the statistic ˇηk will have the threshold

￿ ˇQk￿uu￿ =

h∆ u(wi)h∆ u￿(wi) 

tk α = n−1/2σk√2Φ−1(1 − α) 

(8)
bearing in mind the asymptotic distribution of the test statistic  and that ηk(p  p) = 0. This threshold
is computed empirically by replacing σk with its estimate ˇσk (computed using the data being tested) 
which yields a test of the desired asymptotic level.
The asymptotic distribution (5) holds only when the kernel is ﬁxed  and does not depend on the
sample X  Y . If the kernel were a function of the data  then a test would require large deviation
probabilities over the supremum of the Gaussian process indexed by the kernel parameters (e.g.
[1]). In practice  the threshold would be computed via a bootstrap procedure  which has a high
computational cost. Instead  we set aside a portion of the data to learn the kernel (the “training
data”)  and use the remainder to construct a test using the learned kernel parameters.

3 Choice of kernel

The choice of kernel will affect both the test statistic itself  (4)  and its asymptotic variance  (6).
Thus  we need to consider how these statistics determine the power of a test with a given level α (the
upper bound on the Type I error). We consider the case where p ￿= q. A Type II error occurs when
the random variable ˇηk falls below the threshold tk α deﬁned in (8). The asymptotic probability of a
Type II error is therefore

P (ˇηk < tk α) =Φ ￿Φ−1(1 − α) −

ηk(p  q)√n

σk√2 ￿ .

As Φ is monotonic  the Type II error probability will decrease as the ratio ηk(p  q)σ−1
k
Therefore  the kernel minimizing this error probability is

increases.

(9)

k∗ = arg sup
k∈K

ηk(p  q)σ−1
k  

with the associated test threshold tk∗ α. In practice  we do not have access to the population estimates
ηk(p  q) and σk  but only their empirical estimates ˆηk  ˆσk from m pairs of training points (xi  yi)
(this training sample must be independent of the sample used to compute the test parameters ˇηk  ˇσk).
We therefore estimate tk∗ α by a regularized empirical estimate tˆk∗ α  where

1This vector is the concatenation of two four-dimensional vectors  and has eight dimensions.

ˆk∗ = arg sup
k∈K

ˆηk (ˆσk λ)−1  

4

k λ − sup
k∈K

ˆηk ˆσ−1

k∈K
≤ sup

￿￿￿￿sup
k∈K￿￿￿ˆηk ˆσ−1
k∈K￿ˆσ2
C1√d
D√λm

≤ sup

≤

sup

k + ￿β￿2

and

k∈K

ˆηk ˆσ−1

ηkσ−1

ηkσ−1

k λ − sup
k∈K

k λ − ηkσ−1

￿￿￿￿sup

k ￿￿￿￿ = OP￿m−1/3￿
k ￿￿￿￿ ≤ sup
k ￿￿￿
k∈K￿￿￿ˆηk ˆσ−1
k ￿￿￿
k∈K￿￿￿ηkσ−1
k λ￿￿￿ + sup
ˆσk λσk λ ￿￿￿￿ + sup
ηk￿￿￿￿
2λm￿−1/2 | ˆηk − ηk| + sup
ηk￿￿￿￿￿
k∈K| ˆηk − ηk| + sup
k∈K
σk￿ ￿β￿2

k λ − ηkσ−1
k λ − ηkσ−1

k￿
k∈K|ˆσk λ − σk λ|￿ + C3D2λm 

+ sup
2λm + σ2
k∈K
k∈K|ˆηk − ηk| + C2 sup

ˆσk λ − σk λ
k + ˆσ2

ˆσk λ − σk λ

k + ￿β￿2

(σ2
2λm

2λm (σ2

￿β￿2

k ˆσ2

k∈K

ηk

k∈K

√d
D√λm￿C1 sup

≤

ηk

σk￿￿￿￿￿

k

σ2
k λ − σ2

σk λ (σk λ + σk)￿￿￿￿￿
m)1/2￿￿￿￿￿

k) + ￿β￿2

2λ2

and we deﬁne the regularized standard deviation ˆσk λ =￿β￿￿ ˆQ + λmI￿ β =￿ˆσ2
2.
k + λm￿β￿2
The next theorem shows the convergence of supk∈K ˆηk (ˆσk λ)−1 to supk∈K ηk(p  q)σ−1
k   and of ˆk∗
to k∗  for an appropriate schedule of decrease for λm with increasing m.
Theorem 1. Let K be deﬁned as in (3). Assume supk∈K x y∈X |k(x  y)| < K and σk is bounded
away from zero. Then if λm =Θ ￿m−1/3￿ 

ˆk∗

P→ k∗.

Proof. Recall from the deﬁnition of K that ￿β￿1 = D  and that ￿β￿2 ≤ ￿β￿1 and ￿β￿1 ≤ √d￿β￿2
[11  Problem 3 p. 278]  hence ￿β￿2 ≥ Dd−1/2. We begin with the bound

where constants C1  C2  and C3 follow from the boundedness of σk and ηk. The the ﬁrst result in the
theorem follows from supk∈K |ˆηk − ηk| = OP (m−1/2) and supk∈K |ˆσk λ − σk λ| = OP (m−1/2) 
which are proved using McDiarmid’s Theorem [12] and results from [3]: see Appendix A of the
supplementary material.
Convergence of ˆk∗ to k∗: For k ∈K deﬁned in (3)  we show in Section 4 that ˆk∗ and k∗ are unique
optimizers of ˆηk ˆσ−1
  the result follows
from [18  Corollary 3.2.3(i)].

k   respectively. Since supk∈K

P→ supk∈K

k λ and ηkσ−1

ˆηk
ˆσk λ

ηk
σk

We remark that other families of kernels may be worth considering  besides K. For instance  we
could use a family of RBF kernels with continuous bandwidth parameter θ ≥ 0. We return to this
point in the conclusions (Section 6).

4 Optimization procedure

u=1

ˆβ∗uku ∈K that maximizes the ratio ˆηk/ˆσk λ. We perform
this optimization over training data  then use the resulting parameters ˆβ∗ to construct a hypothesis
test on the data to be tested (which must be independent of the training data  and drawn from the
same p  q). As discussed in Section 2.2  this gives us the test threshold without requiring a bootstrap

We wish to select kernel k = ￿d
procedure. Recall from Sections 2.2 and 3 that ˆηk = β￿ ˆη  and ˆσk λ = ￿β￿￿ ˆQ + λmI￿ β 
α(β; ˆη  ˆQ) := ￿β￿ ˆη￿￿β￿￿ ˆQ + λmI￿ β￿−1/2

where ˆQ is a linear-time empirical estimate of the covariance matrix cov(h). Since the objective
is a homogenous function of order zero in β  we

can omit the constraint ￿β￿1 = D  and set

(10)

ˆβ∗ = arg max
β￿0

α(β; ˆη  ˆQ).

5

Feature selection

 

max ratio
opt
l2
max mmd

5

10

15

20

25

30

Dimension

35

30

25

2
x

20

15

10

5

35

30

25

2
y

20

15

10

5

Grid of Gaussians  p

10

20
x1

30

Grid of Gaussians  q

10

20
y1

30

0.5

r
o
r
r
e

I
I

r
e
p
y
T

0.4

0.3

0.2

0.1

0

 

0

1

0.8

0.6

0.4

0.2

r
o
r
r
e

I
I

e
p
y
T

Grid of Gaussians

 

max ratio
opt
l2
max mmd
xval
median

 

0

0

5

10

15

Ratio ε

Figure 1: Left: Feature selection results  Type II error vs number of dimensions  average over 5000
trials  m = n = 104. Centre: 3 × 3 Gaussian grid  samples from p and q. Right: Gaussian grid
results  Type II error vs ε  the eigenvalue ratio for the covariance of the Gaussians in q; average over
1500 trials  m = n = 104. The asymptotic test level was α = 0.05 in both experiments. Error bars
give the 95% Wald conﬁdence interval.

If ˆη has at least one positive entry  there exists β ￿ 0 such that α(β; ˆη  ˆQ) > 0. Then clearly 
α( ˆβ∗; ˆη  ˆQ) > 0  so we can write ˆβ∗ = arg maxβ￿0 α2(β; ˆη  ˆQ). In this case  the problem (10)
becomes equivalent to a (convex) quadratic program with a unique solution  given by

min{β￿￿ ˆQ + λmI￿ β : β￿ ˆη = 1 β ￿ 0}.

(11)

Under the alternative hypothesis  we have ηu > 0  ∀u ∈{ 1  . . .   d}  so the same rea-
to β∗ =
soning can be applied to the population version of the optimization problem 
arg maxβ￿0 α(β; η  cov(h))  which implies the optimizer β∗ is unique. In the case where no entries
in ˆη are positive  we obtain maximization of a quadratic form subject to a linear constraint 

i.e. 

max{β￿￿ ˆQ + λmI￿ β : β￿ ˆη = −1 β ￿ 0}.

While this problem is somewhat more difﬁcult to solve  in practice its exact solution is irrelevant to
the Type II error performance of the proposed two-sample test. Indeed  since all of the squared MMD
estimates calculated on the training data using each of the base kernels are negative  it is unlikely the
statistic computed on the data used for the test will exceed the (always positive) threshold. Therefore 
when no entries in ˆη are positive  we (arbitrarily) select a single base kernel ku with largest ˆηu/ˆσu λ.
The key component of the optimization procedure is the quadratic program in (11). This problem can
be solved by interior point methods  or  if the number of kernels d is large  we could use proximal-
gradient methods. In this case  an ￿-minimizer can be found in O(d2/√￿) time. Therefore  the
overall computational cost of the proposed test is linear in the number of samples  and quadratic in
the number of kernels.

5 Experiments

We compared our kernel selection strategy to alternative approaches  with a focus on challenging
problems that beneﬁt from careful kernel choice. In our ﬁrst experiment  we investigated a synthetic
data set for which the best kernel in the family K of linear combinations in (3) outperforms the best
individual kernel from the set {ku}d
u=1 . Here p was a zero mean Gaussian with unit covariance 
and q was a mixture of two Gaussians with equal weight  one with mean 0.5 in the ﬁrst coordinate
and zero elsewhere  and the other with mean 0.5 in the second coordinate and zero elsewhere.
Our base kernel set {ku}d
u=1 contained only d univariate kernels with ﬁxed bandwidth (one for each
dimension): in other words  this was a feature selection problem. We used two kernel selection
strategies arising from our criterion in (9): opt - the kernel from the set K that maximizes the ratio
ˆηk/ˆσk λ  as described in Section 4  and max-ratio - the single base kernel ku with largest ˆηu/ˆσu λ.

6

AM signals  p

AM signals  q

1

0.8

0.6

0.4

0.2

r
o
r
r
e

I
I

e
p
y
T

0

 
0

Amplitude modulated signals

 

max ratio
opt
median
l2
max mmd

0.2

0.4

0.6

0.8

1

1.2

Added noise σε

Figure 2: Left: amplitude modulated signals  four samples from each of p and q prior to noise being
added. Right: AM results  Type II error vs added noise  average over 5000 trials  m = n = 104.
The asymptotic test level was α = 0.05. Error bars give the 95% Wald conﬁdence interval.

We used λn = 10−4 in both cases. An alternative kernel selection procedure is simply to maximize
the MMD on the training data  which is equivalent to minimizing the error in classifying p vs. q
under linear loss [15]. In this case  it is necessary to bound the norm of β  since the test statistic
can otherwise be increased without limit by rescaling the β entries. We employed two such kernel
selection strategies: max-mmd - a single base kernel ku that maximizes ˆηu (as proposed in [15]) 
and l2 - a kernel from the set K that maximizes ˆηk subject to the constraint ￿β￿2 ≤ 1 on the vector
of weights.
Our results are shown in Figure 1. We see that opt and l2 perform much better than max-ratio and
max-mmd  with the former each having large ˆβ∗ weights in both the relevant dimensions  whereas the
latter are permitted to choose only a single kernel. The performance advantage decreases as more
irrelevant dimensions are added. Also note that on these data  there is no statistically signiﬁcant
difference between opt and l2  or between max-ratio and max-mmd.
Difﬁcult problems in two-sample testing arise when the main data variation does not reﬂect the
difference between p and q; rather  this is encoded as perturbations at much smaller lengthscales. In
these cases  a good choice of kernel becomes crucial. Both remaining experiments are of this type.
In the second experiment  p and q were both grids of Gaussians in two dimensions  where p had
unit covariance matrices in each mixture component  and q was a grid of correlated Gaussians with
a ratio ε of largest to smallest covariance eigenvalues. A sample dataset is provided in Figure 1. The
testing problem becomes more difﬁcult when the number of Gaussian centers in the grid increases 
and when ε → 1. In experiments  we used a ﬁve-by-ﬁve grid.
We compared opt  max-ratio  max-mmd  and l2  as well as an additional approach  xval  for which
we chose the best kernel from {ku}d
u=1 by ﬁve-fold cross-validation  following [17]. In this case 
we learned a witness function on four ﬁfths of the training data  and used it to evaluate the linear
loss on p vs q for the rest of the training data (see [7  Section 2.3] for the witness function deﬁnition 
and [15] for the classiﬁcation interpretation of the MMD). We made repeated splits to obtain the
average validation error  and chose the kernel with the highest average MMD on the validation sets
(equivalently  the lowest average linear loss). This procedure has cost O(m2)  and is much more
computationally demanding than the remaining approaches.
Our base kernels {ku}d
u=1 in (3) were multivariate isotropic Gaussians with bandwidth varying
between 2−10 and 215  with a multiplicative step-size of 20.5  and we set λn = 10−5. Results
are plotted in Figure 1: opt and max-ratio are statistically indistinguishable  followed in order of
decreasing performance by xval  max-mmd  and l2. The median heuristic fails entirely  yielding
the 95% error expected under the null hypothesis. It is notable that the cross-validation approach
performs less well than our criterion  which suggests that a direct approach addressing the Type II
error is preferable to optimizing the classiﬁer performance.
In our ﬁnal experiment  the distributions p  q were short samples of amplitude modulated (AM)
signals  which were carrier sinusoids with amplitudes scaled by different audio signals for p and q.

7

These signals took the form

y(t) = cos(ωct) (As(t) + oc) + n(t) 

where y(t) is the AM signal at time t  s(t) is an audio signal  ωc is the frequency of the carrier
signal  A is an amplitude scaling parameter  oc is a constant offset  and n(t) is i.i.d. Gaussian noise
with standard deviation σε. The source audio signals were [5  Vol. 1  Track 2; Vol. 2 Track 17] 
and had the same singer but different accompanying instruments. Both songs were normalized to
have unit standard deviation  to avoid a trivial distinction on the basis of sound volume. The audio
was sampled at 8kHz  the carrier was at 24kHz  and the resulting AM signals were sampled at
120kHz. Further settings were A = 0.5 and oc = 2. We extracted signal fragments of length 1000 
corresponding to a time duration of 8.3 × 10−3 seconds in the original audio. Our base kernels
{ku}d
u=1 in (3) were multivariate isotropic Gaussians with bandwidth varying between 2−15 and
215  with a multiplicative step-size of 2  and we set λn = 10−5. Sample extracts from each source
and Type II error vs noise level σε are shown in Figure 2. Here max-ratio does best  with successively
decreasing performance by opt  max-mmd  l2  and median. We remark that in the second and third
experiments  simply choosing the kernel ku with largest ratio ˆηu/ˆσu λ does as well or better than
solving for ˆβ∗ in (11). The max-ratio strategy is thus recommended when a single best kernel exists
in the set {ku}d
u=1  although it clearly fails when a linear combination of several kernels is needed
(as in the ﬁrst experiment).
Further experiments are provided in the supplementary material. These include an empirical veri-
ﬁcation that the Type I error is close to the design parameter α  and that kernels are not chosen at
extreme values when the null hypothesis holds  additional AM experiments  and further synthetic
benchmarks.

6 Conclusions

We have proposed a criterion to explicitly optimize the Hodges and Lehmann asymptotic relative
efﬁciency for the kernel two-sample test: the kernel parameters are chosen to minimize the asymp-
totic Type II error at a given Type I error. In experiments using linear combinations of kernels  this
approach often performs signiﬁcantly better than the simple strategy of choosing the kernel with
largest MMD (the previous best approach)  or maximizing the MMD subject to an ￿2 constraint on
the kernel weights  and yields good performance even when the median heuristic fails completely.
A promising next step would be to optimize over the parameters of a single kernel (e.g.  over the
bandwidth of an RBF kernel). This presents two challenges: ﬁrst  in proving that a ﬁnite sample
estimate of the kernel selection criterion converges  which might be possible following [15]; and
second  in efﬁciently optimizing the criterion over the kernel parameter  where we could employ a
DC programming [2] or semi-inﬁnite programming [6] approach.
Acknowledgements: Part of this work was accomplished when S. B. was visiting the MPI for
Intelligent Systems. We thank Samory Kpotufe and Bernhard Sch¨olkopf for helpful discussions.

References
[1] R. Adler and J. Taylor. Random Fields and Geometry. Springer  2007.
[2] Andreas Argyriou  Raphael Hauser  Charles A. Micchelli  and Massimiliano Pontil. A dc-

programming algorithm for kernel selection. In ICML  pages 41–48  2006.

[3] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3:463–482  2002.

[4] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and

Statistics. Kluwer  2004.

[5] Magnetic Fields. 69 love songs. Merge  MRG169  1999.
[6] P. Gehler and S. Nowozin. Inﬁnite kernel learning. Technical Report TR-178  Max Planck

Institute for Biological Cybernetics  2008.

[7] A. Gretton  K. Borgwardt  M. Rasch  B. Schoelkopf  and A. Smola. A kernel two-sample test.

JMLR  13:723–773  2012.

8

[8] A. Gretton  K. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. J. Smola. A kernel method for
the two-sample problem. In Advances in Neural Information Processing Systems 15  pages
513–520  Cambridge  MA  2007. MIT Press.

[9] A. Gretton  K. Fukumizu  Z. Harchaoui  and B. Sriperumbudur. A fast  consistent kernel two-
sample test. In Advances in Neural Information Processing Systems 22  Red Hook  NY  2009.
Curran Associates Inc.

[10] Z. Harchaoui  F. Bach  and E. Moulines. Testing for homogeneity with kernel Fisher discrimi-
nant analysis. In Advances in Neural Information Processing Systems 20  pages 609–616. MIT
Press  Cambridge  MA  2008.

[11] R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge Univ Press  1990.
[12] C. McDiarmid. On the method of bounded differences. In Survey in Combinatorics  pages

148–188. Cambridge University Press  1989.

[13] R. Serﬂing. Approximation Theorems of Mathematical Statistics. Wiley  New York  1980.
[14] A. J. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A Hilbert space embedding for dis-
tributions. In Proceedings of the International Conference on Algorithmic Learning Theory 
volume 4754  pages 13–31. Springer  2007.

[15] B. Sriperumbudur  K. Fukumizu  A. Gretton  G. Lanckriet  and B. Schoelkopf. Kernel choice
and classiﬁability for RKHS embeddings of probability distributions. In Advances in Neural
Information Processing Systems 22  Red Hook  NY  2009. Curran Associates Inc.

[16] B. Sriperumbudur  A. Gretton  K. Fukumizu  G. Lanckriet  and B. Sch¨olkopf. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research 
11:1517–1561  2010.

[17] M. Sugiyama  T. Suzuki  Y. Itoh  T. Kanamori  and M. Kimura. Least-squares two-sample test.

Neural Networks  24(7):735–751  2011.

[18] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes. Springer 

1996.

9

,Kenneth Latimer
E.J. Chichilnisky
Fred Rieke
Jonathan Pillow
Reshad Hosseini
Suvrit Sra