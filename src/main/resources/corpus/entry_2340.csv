2019,Gradient Dynamics of Shallow Univariate ReLU Networks,We present a theoretical and empirical study of the gradient dynamics of overparameterized shallow  ReLU networks with one-dimensional input  solving least-squares interpolation. We show that the gradient dynamics of such networks are determined by the gradient flow in a non-redundant parameterization of the network function. We examine the principal qualitative features of this gradient flow. In particular  we determine conditions for two learning regimes: \emph{kernel} and \emph{adaptive}  which depend both on the relative magnitude of initialization of weights in different layers and the asymptotic behavior of initialization coefficients in the limit of large network widths. We show that learning in the kernel regime yields smooth interpolants  minimizing curvature  and reduces to \emph{cubic splines} for uniform initializations. Learning in the adaptive regime favors instead \emph{linear splines}  where knots cluster adaptively at the sample points.,Gradient Dynamics of Shallow Univariate

ReLU Networks

Francis Williams∗

Matthew Trager∗

Claudio Silva

Daniele Panozzo

Denis Zorin

Joan Bruna

New York University

Abstract

We present a theoretical and empirical study of the gradient dynamics of overparam-
eterized shallow ReLU networks with one-dimensional input  solving least-squares
interpolation. We show that the gradient dynamics of such networks are determined
by the gradient ﬂow in a non-redundant parameterization of the network function.
We examine the principal qualitative features of this gradient ﬂow. In particular  we
determine conditions for two learning regimes: kernel and adaptive  which depend
both on the relative magnitude of initialization of weights in different layers and
the asymptotic behavior of initialization coefﬁcients in the limit of large network
widths. We show that learning in the kernel regime yields smooth interpolants 
minimizing curvature  and reduces to cubic splines for uniform initializations.
Learning in the adaptive regime favors instead linear splines  where knots cluster
adaptively at the sample points.

Introduction

1
An important open problem in the theoretical study of neural networks is to describe the dynamical
behavior of the parameters during training and  in particular  the inﬂuence of the dynamics on the
generalization error. To make progress on these issues  a number of studies have focused on a
tractable class of architectures  namely single hidden-layer neural networks. For a ﬁxed number of
neurons  negative results establish that  even with random initialization  gradient descent may be
trapped in arbitrarily bad local minima [27  31]  which motivates an asymptotic analysis that studies
the optimization and generalization properties of these models as the number of neurons m grows.
Recently  several works [13  2  6  12  23] explained the success of gradient descent at optimizing
the loss in the over-parameterized regime  i.e.  when the number of neurons in signiﬁcantly higher
than the number of training samples. In parallel  another line of work established global convergence
of gradient descent using tools from optimal transport and mean-ﬁeld theory [8  26  22  29]. The
essential difference between these two approaches was pointed out in [7]  and is related to the use of a
different scaling parameter as the number of neurons tends to inﬁnity: in one case  the neural network
behaves asymptotically as a kernel machine [19]  which in turn implies that as over-parameterization
increases  the parameters stay close to their initial value; in contrast  in the mean ﬁeld setting 
parameters asymptotitically evolve following a PDE based on a continuity equation.
Although both scaling regimes explain the success of gradient descent optimization on over-
parametrized networks  they paint a different picture when it comes to generalization. The generaliza-
tion properties in the kernel regime borrow from the well established theory of kernel regression in
Reproducing Kernel Hilbert Spaces (RKHS)  which has been applied to kernels arising from neural
networks in [17  14  20  24  10]  and provide a somehow underwhelming answer to the beneﬁt of neu-
ral networks compared to kernel methods. However  in practice  large neural networks do not exhibit

∗Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the traits of kernel/lazy learnings  since ﬁlter weights signiﬁcantly deviate from their initialization
despite the over-parameterization. Also  empirically  active learning provides better generalization
than kernel learning [7]  although the theoretical reasons for this are still poorly understood.
In this paper  we consider a simpliﬁed setting  and study wide  single-hidden layer ReLU networks
deﬁned with one-dimensional inputs. We show how the kernel and active dynamics deﬁne fundamen-
tally different function estimation models. For a ﬁxed number of neurons  the network may follow
either of these dynamics  depending on the initialization. Speciﬁcally  we show that kernel dynamics
correspond to interpolation with cubic splines  whereas active dynamics yields adaptive linear splines 
where neurons accumulate at the discontinuities and yield piecewise linear approximations.
Further related work. Our work lies at the intersection between two lines of research: the works 
described above  that study the optimization and generalization for shallow neural networks  and the
works that attempt to shed light on these properties on low-dimensional inputs. In the latter category 
we mention [4] for their study of the abilities of ReLU networks to approximate low-dimensional
manifolds  and [32] for their empirical study of 3D surface reconstruction using precisely the intrinsic
bias of SGD in overparametrised ReLU networks. Another remarkable recent work is [11]  where
the approximation power of deep ReLU networks is studied in the context of univariate functions.
Our analysis in the active regime (Sec. 3.1) is closely related to [21]  in which the authors establish
convergence of gradient descent to piece-wise linear functions under initializations sufﬁciently close
to zero. We provide an Eulerian perspective using Wasserstein gradient ﬂows that simpliﬁes the
analysis  and is consistent with their conclusions. The implicit bias of SGD dynamics appears in
several works  such as [30  15]  and  closest to our setup  in [28]  where the authors observe a link
between gradient dynamics and linear splines. They do not however observe the connection with
cubic splines  although they observe experimentally that the function returned by a network is often
smooth and not piecewise linear. Finally  we mention related work that studies the tesselation of
ReLU networks on the input space [16].
Main contributions. The goal of this paper is to describe the qualitative behavior of the dynamics
or 1D shallow ReLU networks. Our main contributions can be summarized as follows.
• We investigate the gradient dynamics of shallow 1D ReLU networks using a “canonical” parame-
terization (Sec. 3.1). We show that the dynamics in this case are are completely determined by the
evolution of the residuals. Furthermore  neurons will always accumulate at certain sample points
where the residual is large and of opposite sign compared to neighboring samples. This means that
the dynamics in the reduced parameterization biases towards functions that are piecewise-linear.
• We observe that the dynamics in full parameters are related to the dynamics in canonical parameters
by a change of metric that depends only on the network at initialization. This change of metric is
expressible in terms of invariants δi associated with each neuron. When δi (cid:29) 0 the dynamics in full
parameters (locally) agree with the dynamics in reduced parameters; when δi (cid:28) 0  the dynamics in
full parameters (locally) follow kernel dynamics  in which only the outer layer weights change.
• We consider the idealized kernel dynamics in the limit of inﬁnite neurons  and we show that the
RKHS norm of a function f corresponds to a weighted L2 norm of the second derivative f(cid:48)(cid:48)  i.e.  a
form of linearized curvature. For appropriate initial distributions of neurons  the solution to kernel
learning is a smooth cubic spline (Theorem 5). This illustrates the qualitative difference between
the “reduced” and “kernel” regimes  which depend on parameter lift at initialization. Arbitrary
initializations will locally interpolate between these two regimes.
• We also discuss the effect of applying a scaling parameter α(m) the network function (where m is
the number of neurons)  which becomes important as the number of neurons tends to inﬁnity. As
argued in [7]  when α(m) = o(m)  the variation of each neuron will asymptotically go to zero (lazy
regime)  so our local analysis translates into a global one.

2 Preliminaries
We consider the problem of training of a two-layer ReLU neural network with m scalar inputs and a
single scalar output using the least-squares loss:

ci[aix − bi]+ 

z = (a ∈ Rm  b ∈ Rm  c ∈ Rm).

(1)

2

min

z

L(z) =

1
2

|fz(xj) − yj|2

where fz(x) =

s(cid:88)

j=1

m(cid:88)

1

α(m)

i=1

Figure 1: Left: A network function fz(x) interpolating input samples (blue x’s). The knots of fz(x)
as a piecewise linear function are plotted as green circles. Right: The canonical parameters of the
network visualized as in (6). Each particle represents a neuron and the color indicates the sign of i.
The samples xj correspond to lines uxj + v = 0. The colored regions which correspond to different
activation patterns of neurons on the training data.
Here (xi  yi) ∈ R2  i = 1  . . .   s is a given set of samples  z is a vector of parameters  and α(m) is a
normalization factor that will be important as we consider the limit m → ∞. We are interested in
the minimization of (1) performed by gradient descent over the parameters z. This scheme may be
analyzed through its continuous-time counterpart  the gradient ﬂow
z(cid:48)(t) = −∇L(z(t)).

(2)
While (2) describes the dynamics of z(t) in parameter space  we are ultimately interested in the
(cid:80)
trajectories of the function fz(t). Let F := {f : R → R} denote the space of square-integrable
scalar functions  and let ϕ be the function-valued mapping ϕ(z) := fz. Since L(z) = R(ϕ(z)) with
j≤s |f (xj) − yj|2  we have by the chain rule that the dynamics of g(t) := ϕ(z(t)) =
R(f ) = 1
2
fz(t) are described by

z(0) = z0 

g(0) = fz0 

g(cid:48)(t) = −∇ϕ(z(t))(cid:62)∇ϕ(z(t))∇R(g(t)) .

(3)
The dynamics in function space are thus controlled by a time-varying tangent kernel Kt
:=
∇ϕ(z(t))(cid:62)∇ϕ(z(t)). It was shown in [19] that under certain assumptions the kernel Kt remains
nearly constant throughout training.
It is immediate to see that the parameters z can be continuously rescaled without affecting the
function fz  according to (ai  bi  ci) (cid:55)→ (aiki  biki  ci/ki) with ki > 0. In order to eliminate this
ambiguity  we introduce the following canonical parameterization of the network’s functional space:

ri(cid:104)˜x  d(θi)(cid:105)+ 

w = (r ∈ Rm  θ ∈ [0  2π)m) 

˜x = (x  1).

(4)

m(cid:88)

i=1

˜fw(x) =

1
m

(cid:18) m

α(m)

(cid:113)

(cid:19)
i   arctan(−bi/ai)

where d(θi) = (cos θi  sin θi) ∈ S1. The natural mapping into canonical parameters is given by

π(ai  bi  ci) =

ci

a2
i + b2

= (ri  θi).

(5)

This mapping clearly satisﬁes ˜fπ(z) = fz. We can also deﬁne the loss with respect to this parameter-
ization as ˜L(w) = L(z) where w = π(z). We will compare the dynamics of L(z) with those of
˜L(w) to study the impact on training of different choices of parameterization and initialization  as
well as the asymptotic behavior of (3) as m increases.
Visualizing a network function. We can visualize a network function fz(x) in two ways. First  we
can plot fz(x) as a scalar function (Figure 1  left). Note that fz(x) is a continuous piecewise linear
functions in x whose knots are the points where the operand inside a ReLU activation changes sign 
namely ei = bi/ai  ai (cid:54)= 0  i = 1  . . .   m. Alternatively  we can visualize the canonical parameters
w = π(z) in R2  by plotting a neuron (ri  θi) as a particle with coordinates

(ui  vi) = (|ri| cos(θi) |ri| sin(θi)) 

(6)
and coloring each particle according to i = sign(ri) (Figure 1  right). In this visualization  each
training sample point xj can be represented as the line uxj + v = 0  which identiﬁes the half-space
of neurons that are active at xj. The collection of such lines for all samples partitions the plane into
activation regions  where neurons have a ﬁxed activation pattern on the training data.

3

(ei f(ei))(xj yj)xi>0<03 Training Dynamics
Our goal is to solve (1) using the gradient ﬂow (2) of the loss L(z)1. We begin in Section 3.1 by
investigating the gradient dynamics in the canonical parameterization:
w(cid:48)(t) = −∇ ˜L(w(t)).

(7)
While the relationship between ﬂows (2) and (7) is nonlinear  we argue in Section 3.2 that these are
related by a simple change of metric.

w(0) = w0 

3.1 Dynamics in the Canonical Parameters
We assume that the canonical parameters (ri  θi) are initialized i.i.d. from some base distribution
µ(r  θ). The function ˜fw is well-deﬁned pointwise as m → ∞  by the law of large numbers.
Following the mean-ﬁeld formulation of single-hidden layer networks [22  8  26]  we express the
function as an expectation with respect to the probability measure over the cylinder D = R × S1:

(cid:90)

D

˜fw(x) =

ϕ(w; x)µ(m)(dw)  

(cid:80)m

where ϕ(w; x) := ri(cid:104)˜x  d(θi)(cid:105)+ and µ(m)(w) = 1
mined by the m particles wi  i = 1 . . . m. The least squares loss in this case becomes

i=1 δwi(w) is the empirical measure deter-

m

˜L(w) =

1
2

(cid:107) ˜fw − y(cid:107)2X
m(cid:88)
(cid:104)ϕwi  y(cid:105)X +

= Cy − 1
m

i=1

m(cid:88)

i i(cid:48)=1

(cid:104)ϕwi  ϕwi(cid:48)(cid:105)X  

1

2m2

where (cid:104)f  g(cid:105)X :=(cid:80)s

(cid:90)

(cid:90)(cid:90)

j=1 f (xj)g(xj) is the empirical dot-product. This loss may be interpreted as
the Hamiltonian of a system of m-interacting particles  under external ﬁeld F and interaction kernel
K deﬁned respectively by F (w) := (cid:104)ϕw  y(cid:105)X  K(w  w(cid:48)) := (cid:104)ϕw  ϕw(cid:48)(cid:105)X . We may also express this
Hamiltonian in terms of the empirical measure  by abusing notation

˜L(µ(m)) = Cy −

F (w)µ(m)(dw) +
A direct calculation shows that the gradient ∇wi

D

1
2
˜L(w) can be written as

D2

K(w  w(cid:48))µ(m)(dw)µ(m)(dw(cid:48)) .

m
2

∇wi

where V is the potential function V (w; µ) := −F (w) +(cid:82)

˜L(w) = ∇wV (wi; µ(m))  

D K(w  w(cid:48))µ(dw(cid:48)). The gradient ﬂow in
the space of parameters w can now be interpreted in Eulerian terms as a gradient ﬂow in the space
of measures over D  by using the notion of Wasserstein gradient ﬂows [22  8  26]. Indeed  particles
evolve in D by “feeling” a velocity ﬁeld ∇V deﬁned in D. This formalism allows us now to describe
the dynamics independently of the number of neurons m  by replacing the empirical measure µ(m)
by any generic probability measure µ in D. The evolution of a measure under a generic time-varying
vector ﬁeld is given by the so-called continuity equation:2

∂tµt = div(∇V µt) .

√

(8)
The global convergence of this PDE for interaction kernels arising from single-hidden layer neural
networks has been established under mild assumptions in [22  8  25]. Although the conditions for
global convergence hold in the mean ﬁeld limit m → ∞  a propagation-of-chaos argument from
statistical mechanics gives Central Limit Theorems for the behavior of ﬁnite-particle systems as
ﬂuctuations of order 1/
The dynamics in D are thus described by the velocity ﬁeld ∇V (w; µt)  which depends on the current
state of the system through the measure µt(w)  describing the probability of encountering a particle
1To be precise  we should replace the gradient ∇L(z) with the Clarke subdifferential ∂L(z) [9]  since
L(z) is only piecewise smooth. At generic smooth points z  the subdifferential coincides with the gradient
∂L(z(t)) = {∇L(z)}.
2Understood in the weak sense  i.e.  ∂t
c (D) continuously differentiable and with compact support.
C 1

(cid:0)(cid:82)
D φ(w)µt(dw)(cid:1) = −(cid:82) (cid:104)∇φ(w) ∇V (w; µt)(cid:105)µt(dw)  ∀φ ∈

m around the mean-ﬁeld solution; see [26  25] for further details.

4

at position w at time t. We emphasize that equation (8) is valid for any measure  including the
empirical measure µ(m)  and is therefore an exact model for the dynamics in both the ﬁnite-particle
and inﬁnite-particle regime. Let us now describe its speciﬁc form in the case of the empirical loss
given above.
Assume without loss of generality that the data points xj ∈ R  j ≤ s satisfy xj ≤ xj(cid:48) whenever
j < j(cid:48). Denote

Cs+j := {j(cid:48); j(cid:48) > j}  for j = 1 . . . s − 1 .

Cj := {j(cid:48); j(cid:48) ≤ j} for j = 1 . . . s 
j = arctan(xj) ± π/2 partition the circle S1 into 2s − 1
We observe that for each j  two angles α±
regions Ak (visualized as the colored regions in Figure 1)  which are in one-to-one correspondence
with the sets Ck  in the sense that θ ∈ Ak if and only if {j;(cid:104)˜xj  d(θ)(cid:105) ≥ 0} = Ck . We also denote by
Bj the half-circle where (cid:104)˜xj  θ(cid:105) ≥ 0. Let t(θ) be the tangent vector of S1 at θ (so t(θ) = d(θ)⊥) and
w = (r  θ)  where we suppose θ ∈ Ak. A straightforward calculation (see Appendix B) shows that
the angular and radial components of ∇V (w; µt) are given by

 

j∈Ck

R×Bj

∇rV (w; µt) =

∇θV (w; µt) = r

ρj(t)˜xj  t(θ)

ρj(t)˜xj  d(θ)

where ρj(t) =(cid:82)

(9)
r(cid:104)˜xj  θ(cid:105)µt(dr  dθ) − yj is the residual at point xj at time t. These expressions
show that the dynamics are entirely controlled by the s-dimensional vector of residuals ρ(t) =
(ρ1(t)  . . . ρs(t))  and that the velocity ﬁeld is piece-wise linear on each cylindrical region R × Ak
(e.g. Figure 9 in Appendix D). Under the assumptions that ensure global convergence of (8)  we
have limt→∞ ˜L(µt) = 0  and therefore (cid:107)ρ(t)(cid:107) → 0. The oscillations of ρ(t) as it converges to zero
determine the relative orientation of the ﬂow within each region. The exact dynamics for the vector
of residuals are given by the following proposition  proved in Appendix B:
Proposition 1. For each j 

j∈Ck

 

(cid:42)(cid:88)

(cid:42)(cid:88)

(cid:43)

(cid:43)

where Σk(t) =(cid:82)

R×Ak

˙ρj(t) = −˜x(cid:62)

(cid:0)r2t(θ) t(θ)(cid:62) + d(θ) d(θ)(cid:62)(cid:1) µt(dr  dθ) tracks the covariance of the measure

k;Ak⊂Bj

ρj(cid:48)(t)˜xj(cid:48)

Σk(t)

j(cid:48)∈Ck

(10)

j

along each cylindrical region.
Equation (10) deﬁnes a system of ODEs for the residuals ρ(t)  but its coefﬁcients are time-varying 
and behave roughly as quadratic terms in ρ(t) (since they are second-order moments of the measure
whereas the residuals are ﬁrst-order moments). It may be possible to obtain asymptotic control of the
oscillations ρ(t) by applying Duhamel’s principle  but this is left for future work.
Now let w = (r  θ) with θ at a boundary of two regions Ak  Ak+1. The velocity ﬁeld is modiﬁed at
the transition by

(cid:88)

(cid:88)

  

k−1(cid:88)

where j∗ is such that (cid:104)˜xj∗  d(θ)(cid:105) = 0  since θ is at the boundary of Ak. It follows that the only
discontinuity is in the angular direction  of magnitude |rρj∗(t)|(cid:107)˜xj∗(cid:107). In particular  an interesting
phenomenon arises when the angular components of ∇V (w)|Ak and ∇V (w)|Ak+1 have opposite
signs  corresponding to an “attractor” or “repulsor” that attracts/repels particles along the direction
given by ˜xj∗ (See Figure 9 in Appendix D). Writing sk =
  we deduce
from (9) that this occurs when |sk| < |ρj∗(t)|(cid:107)˜xj∗(cid:107) and sign(sk) (cid:54)= sign(ρj∗(t)). We expand this
condition in the following Lemma.
Lemma 2. A data point xk is an attractor/repulsor if and only if

(cid:68)(cid:80)

ρj(t)˜xj  t(θ)

(cid:69)

j∈Ck

ρiρk(cid:104)˜xi  ˜xk(cid:105) > −ρ2

k(cid:107)˜xk(cid:107)2  or

ρiρk(cid:104)˜xi  ˜xk(cid:105) > −ρ2

k(cid:107)˜xk(cid:107)2.

i=1

i=k+1

In words  mass will concentrate towards input points where the residual is currently large and of
opposite sign from a weighted average of neighboring residuals. This is in stark contrast with the
kernel dynamics (Section 3.3)  where there is no adaptation to the input data points. We point out that
this qualitative behavior has been established in [21] under appropriate initial conditions  sufﬁciently
close to zero  in line with our mean-ﬁeld analysis. We also refer to Section B.2 of the Appendix 
where we describe the adaptive regime when the objective is augmented with TV regularization.

5

s(cid:88)

∇V (w)|Ak−∇V (w)|Ak+1= ρj∗(t)

(cid:18) r(cid:104)˜xj∗  t(θ)(cid:105)

(cid:104)˜xj∗  θ(cid:105)

(cid:19)

 

δ = −100

δ = 0

δ = 100

Figure 2: The value of δ interpolates between different kinds of local trajectories of neurons. The
plots are in the coordinate frame (∇ ˜L ∇ ˜L⊥). Left: the neurons move radially towards and away
from the origin. Middle: the trajectories containing both radial and tangential components. Right: the
trajectories are parallel to the gradient ∇ ˜L.

3.2 Dynamics in the Full Parameters
The dynamics of gradient ﬂow (2) are different from the dynamics of the gradient ﬂow (7). For
the gradient ﬂow in canonical parameters we have observed adaptive learning behavior under the
assumption of iid distribution of parameter initialization. The full set of parameters z = (a  b  c) 
may exhibit both kernel and adaptive behavior depending on the initialization. To characterize this
behavior we rely on the following lemma.
Lemma 3. If z(t) = (a(t)  b(t)  c(t)) is a solution of the gradient ﬂow (2)  then the quantities

δ = (ci(t)2 − ai(t)2 − bi(t)2)m

i=1

(11)

remain constant for all t. In particular  given a reduced neuron (ri  θi)  we can uniquely recover the
original neuron parameters (ai  bi  ci) from δi computed from the initialization.
Lemma 3 allows us to analyze how canonical parameters evolve under full gradient ﬂow in (a  b  c).
Overall  the behavior is qualitatively the same  except it is in addition dependent on the relative scale
of redundant parameters.
Proposition 4. Let z(t) be a solution of the gradient ﬂow (2) of L(z)  and let δ = (δi) ∈ Rm be
the vector of invariants (11)  which depend only on the initialization z(0). If w(t) = (r(t)  θ(t)) is
curve of canonical parameters corresponding to z(t)  then we have that

˙wi(t) = Pi · ∇wi

˜L(w) 

i = 1  . . .   m 

where

Pi =

(cid:34) m2

α(m)2 (a2

i + c2
i )

i + b2
0

0
1
a2
i +b2
i

(12)

(13)

(cid:35)

.

(cid:21)

(cid:35)

(cid:20)∇ri

(cid:34) m2

(cid:20)dri

(cid:21)

dτi

With respect to rescaled differentials dτ = rdθ  corresponding to representing the ﬂow locally in a
Cartesian system aligned with the radial direction (pointing away from z = 0) and its perpendicular 
the ﬂow can be written as

˜L(w)dt
˜L(w)dt

 

α(m)2 (a2

i + c2
i )

i + b2
0

0
c2
i
i (cid:28) a2

.

=

i = 1  . . .   m 

∇τi
i for all i (i.e.  δi (cid:28) 0)  then radial motion
From these equations  one can see that if c2
will dominate. In other words  initializing the ﬁrst layer with signiﬁcantly larger values than the
i (cid:29) a2
second leads to kernel learning. On the other hand  if c2
i   then a solution of the gradient
ﬂow (2) will follow the same trajectory as for the reduced gradient ﬂow (7). Also  if α(m) = o(m) 
the radial component will dominate as m increases. Figure 2 shows the trajectories corresponding to
different values of δi for each neuron  with α(m) = m. The extreme cases of δ = −∞ and δ = +∞
correspond to the “kernel” and “adaptive” regimes  respectively. Note that as δ increases  the neurons
cluster at sample points  as explained in our analysis in Section 3.1  and in accordance to [21].

i + b2

i + b2

(14)

6

1.000.750.500.250.000.250.500.751.001.000.750.500.250.000.250.500.751.00LL1.000.750.500.250.000.250.500.751.001.000.750.500.250.000.250.500.751.00LL1.000.750.500.250.000.250.500.751.001.000.750.500.250.000.250.500.751.00LL3.3 Kernel Dynamics
We now consider the dynamics in the special case where δ (cid:28) 0  and we consider m → ∞. To obtain
the kernel regime in this case  it is sufﬁcient to consider a normalization α(m) = o(m). In particular 
when α(m) = 1  as shown in the previous section  the parameters a and b remain mostly ﬁxed and
the parameters c change throughout training  corresponding to the so-called random-features (RF)
kernel of Rahimi and Recht [24].
In the limit case where a and b are completely ﬁxed to their initial values  if we choose c close to
the zero vector  then the least squares problem (1) solved using gradient ﬂow  is equivalent to the
minimal-norm constraint problem solution:
minimize (cid:107)c(cid:107)2
subject to fz(xi) = yi 

(15)
Given an initial distribution µ0 over the domain Da × Db of parameters a and b  the random-feature
(RF) kernel associated with (15) is given by

i = 1  . . .   s.

KRF(x  x(cid:48)) =

[xa − b]+ · [x(cid:48)a − b]+µ0(da  db) .

Da×Db

rem ˜fz(x) = (cid:80)s
bounded for each a. Deﬁne ν(u) =(cid:82) |a|µa(ab)dq(a). Then the solution (15) solves

The solution of (15) can now be written in terms of this RF kernel using the representer theo-
j=1 αjKRF(xj  x)  where α is a vector of minimal RKHS norm that fulﬁlls the
interpolation constraints. Under appropriate assumptions  the solution to (15) is a cubic spline.
:= E(a b)∼µ0 (a2 + b2) < ∞. Let
Theorem 5. Assume the measure µ0 has ﬁnite second moment σ2
µ0
µ0(a  b) = q(a)µa(b) be the decomposition in terms of marginal and conditional  and assume µa is

(16)

(cid:90)

(cid:90)

(cid:107)f(cid:107)2

|f(cid:48)(cid:48)(u)|2
ν(u)

du

Ω

min

f

s.t.

RF :=

f (xi) = yi   i = 1 . . . s  

(17)
where Ω := supp(ν). Moreover  if µ0 is such that µ0(a  b) = q(a)1(b ∈ Ia)  where Ia ⊂ R is an
arbitrary interval  then (15) will be a cubic spline.
Notice that the assumptions on µ0 to obtain an exact cubic spline kernel impose that if A  B is a
random vector distributed according to µ0  then B|A is uniform over an arbitrary interval IA that
can depend upon A. The proof illustrates that one may generalize the interval IA by any countable
union of intervals. In particular  independent uniform initialization yields cubic splines  but radial
distributions  such as A  B being jointly Gaussian  do not (see Section A.3 in the Appendix). We
remark that machine learning packages such as PyTorch use a uniform distribution for linear layer
parameter initialization by default. We verify that indeed  solutions to (1) converge to cubic splines
as m grows in Figure 3. We also point out that in Kernel Learning  early termination of gradient ﬂow
acts as a regularizer favoring smooth  non-interpolatory solutions (see [19]).
The analysis and comparison of these kernels has recently been addressed in [5  14] in the general
high-dimensional setting  by describing its spectral decay in terms of spherical harmonics. Our results
complement them in the particular one-dimensional setting thanks to the explicit functional form
of the resulting RKHS norms. Additionally  Savarese et al. [28] study the functional form of the

minimization in the variation norm  leading to a penality of the form(cid:82) |f(cid:48)(cid:48)(u)|du. We have instead

√

L2 norms (RKHS) in the kernel regime. The L2 norms do not provide any adaptivity as opposed to
the L1 norm [3]. An interesting question is to precisely describe the transition between these two
regimes as a function of the initialization.
Numerical Experiments. For our numerical experiments  we use gradient descent with the pa-
rameterization (1) and α(m) =
m  appropriately scaling the weights a  b  c to achieve different
dynamical behaviors. We also refer to Section D in the Appendix for additional experiments.
Cubic Splines. We show in Figure 3 that when −δ (cid:28) r2 (i.e.  in the kernel regime)  and as the
number of neurons grows  the network function fz converges to a cubic spline. For this experiment 
we used 10 points sampled from a square wave  and trained only the parameters c (i.e.  δi = ∞).
Network Dynamics as a Function of δ. We show in Figure 4 that as we vary δ  the network function
goes from being smooth and non-adaptive in the kernel regime (δ = −∞  i.e.training only the
parameter c) to very adaptive (δ = ∞  i.e.training only the parameters a  b). Note that as δ increases 
clusters of knots emerge at the sample positions (collinear points in the uv diagrams).

7

m = 102

m = 103

m = 104

Figure 3: A cubic spline with vanishing second derivative at its endpoints (blue line) is approximated
by a neural network (δ = −100) while varying the number m of neurons.

δ = −∞

δ = −1

δ = 0

δ = 1

δ = ∞

Figure 4: Comparison of ﬁtting the network function to a sinusoid as δ varies (10000 epochs).

4 Concluding Remarks
We have studied the implicit bias of gradient descent in the approximation of univariate functions with
single-hidden layer ReLU networks. Despite being an extremely simpliﬁed learning setup  it provides
a clear illustration that such implicit bias can be drastically different depending on how the neural
architecture is parameterized  normalized  or even initialized. Building on recent theoretical work
that studies neural networks in the overparameterized regime  we show how the model can behave
either as a ‘classic’ cubic spline interpolation kernel  or as an adaptive interpolation method  where
neurons concentrate on sample points where the approximation most needs them. Moreover  in the
one-dimensional case  we complement existing works [29] to reveal a transition between these two
extreme training regimes  which roughly correspond to W 1 2 and W 2 2 Sobolev spaces respectively.
Although in our univariate setup there is no clear advantage of one functional space over the other 
our full description of the dynamics may prove useful in the high-dimensional regime  where the
curse of dimensionality affects Hilbert spaces deﬁned by kernels [3]. We believe that the analysis
of the PDE resulting from the mean-ﬁeld regime (where adaptation occurs) in the low-dimensional
setting will be useful to embark in the analysis of the high-dimensional counterpart. We note however
that naively extending our analysis to high-dimensions would result in an exponential increase in the
number of regions that deﬁne our piecewise linear ﬂow  thus we anticipate that new tools might be
needed. Moreover  the interpretation of ReLU features in terms of Green’s functions (as ﬁrst pointed
out in [29]) does not directly carry over to higher dimensions. Lastly  another important limitation of
the mean-ﬁeld analysis is that it cannot be easily adapted to deep neural network architectures  since
neurons are no longer exchangeable as in the many-particle system described above.
Acknowledgements: This work was partially supported by the Alfred P. Sloan Foundation  NSF
RI-1816753  NSF CAREER CIF 1845360  Samsung Electronics  the NSF CAREER award 1652515 
the NSF grant IIS-1320635  the NSF grant DMS-1436591  the NSF grant DMS-1835712  the SNSF
grant P2TIP2_175859  the Moore-Sloan Data Science Environment  the DARPA D3M program 
NVIDIA  Labex DigiCosme  DOA W911NF-17-1-0438  a gift from Adobe Research  and a gift from
nTopology. Any opinions  ﬁndings  and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the views of DARPA.

8

fz(x)(xj yj)Cubic Splinefz(x)(xj yj)Cubic Splinefz(x)(xj yj)Cubic Splinefz(ei)yjfz(ei)yjfz(ei)yjfz(ei)yjfz(ei)yjxi>0<0xi>0<0xi>0<0xi>0<0xi>0<0References
[1] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathemati-

cal society  68(3):337–404  1950.

[2] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. arXiv
preprint arXiv:1901.08584  2019.

[3] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal

of Machine Learning Research  18(1):629–681  2017.

[4] Ronen Basri and David Jacobs. Efﬁcient representation of low-dimensional manifolds using

deep networks. arXiv preprint arXiv:1602.04723  2016.

[5] Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint

arXiv:1905.12173  2019.

[6] Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-

parameterized deep relu networks. arXiv preprint arXiv:1902.01384  2019.

[7] Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable program-

ming. arXiv preprint arXiv:1812.07956  2018.

[8] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems  pages 3036–3046  2018.

[9] Frank H Clarke. Generalized gradients and applications. Transactions of the American Mathe-

matical Society  205:247–262  1975.

[10] Amit Daniely. Sgd learns the conjugate kernel class of the network. In Advances in Neural

Information Processing Systems  pages 2422–2430  2017.

[11] I Daubechies  R DeVore  S Foucart  B Hanin  and G Petrova. Nonlinear approximation and

(deep) relu networks. arXiv preprint arXiv:1905.02199  2019.

[12] Simon S Du  Jason D Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent ﬁnds

global minima of deep neural networks. arXiv preprint arXiv:1811.03804  2018.

[13] Simon S Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably optimizes

over-parameterized neural networks. arXiv preprint arXiv:1810.02054  2018.

[14] Behrooz Ghorbani  Song Mei  Theodor Misiakiewicz  and Andrea Montanari. Linearized

two-layers neural networks in high dimension. arXiv preprint arXiv:1904.12191  2019.

[15] Suriya Gunasekar  Jason Lee  Daniel Soudry  and Nathan Srebro. Characterizing implicit bias

in terms of optimization geometry. arXiv preprint arXiv:1802.08246  2018.

[16] Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. arXiv preprint

arXiv:1901.09021  2019.

[17] Trevor Hastie  Andrea Montanari  Saharon Rosset  and Ryan J Tibshirani. Surprises in high-

dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560  2019.

[18] Thomas Hotz and Fabian JE Telschow. Representation by integrating reproducing kernels.

arXiv preprint arXiv:1202.4443  2012.

[19] Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems  pages
8571–8580  2018.

[20] Chao Ma  Lei Wu  et al. A comparative analysis of the optimization and generalization property
of two-layer neural network and random feature models under gradient descent dynamics. arXiv
preprint arXiv:1904.04326  2019.

9

[21] Hartmut Maennel  Olivier Bousquet  and Sylvain Gelly. Gradient descent quantizes relu network

features. arXiv preprint arXiv:1803.08367  2018.

[22] Song Mei  Andrea Montanari  and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences  115(33):E7665–
E7671  August 2018.

[23] Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient

descent takes the shortest path? arXiv preprint arXiv:1812.10004  2018.

[24] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems  pages 1177–1184  2008.

[25] Grant Rotskoff  Samy Jelassi  Joan Bruna  and Eric Vanden-Eijnden. Global convergence of

neuron birth-death dynamics. arXiv preprint arXiv:1902.01843  2019.

[26] Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.
arXiv preprint arXiv:1805.00915  2018.

[27] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural

networks. arXiv preprint arXiv:1712.08968  2017.

[28] Pedro Savarese  Itay Evron  Daniel Soudry  and Nathan Srebro. How do inﬁnite width bounded

norm networks look in function space? arXiv preprint arXiv:1902.05040  2019.

[29] Justin Sirignano and Konstantinos Spiliopoulos. Mean ﬁeld analysis of neural networks: A

central limit theorem. arXiv preprint arXiv:1808.09372  2018.

[30] Daniel Soudry  Elad Hoffer  Mor Shpigel Nacson  Suriya Gunasekar  and Nathan Srebro. The
implicit bias of gradient descent on separable data. The Journal of Machine Learning Research 
19(1):2822–2878  2018.

[31] Luca Venturi  Afonso S Bandeira  and Joan Bruna. Spurious valleys in two-layer neural network

optimization landscapes. arXiv preprint arXiv:1802.06384  2018.

[32] Francis Williams  Teseo Schneider  Claudio Silva  Denis Zorin  Joan Bruna  and Daniele
Panozzo. Deep geometric prior for surface reconstruction. arXiv preprint arXiv:1811.10943 
2018.

10

,Francis Williams
Matthew Trager
Daniele Panozzo
Claudio Silva
Denis Zorin
Joan Bruna