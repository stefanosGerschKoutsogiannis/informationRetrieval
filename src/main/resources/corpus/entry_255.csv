2011,A concave regularization technique for sparse mixture models,Latent variable mixture models are a powerful tool for exploring the structure in large datasets. A common challenge for interpreting such models is a desire to impose sparsity  the natural assumption that each data point only contains few latent features. Since mixture distributions are constrained in their L1 norm  typical sparsity techniques based on L1 regularization become toothless  and concave regularization becomes necessary. Unfortunately concave regularization typically results in EM algorithms that must perform problematic non-concave M-step maximizations. In this work  we introduce a technique for circumventing this difficulty  using the so-called Mountain Pass Theorem to provide easily verifiable conditions under which the M-step is well-behaved despite the lacking concavity. We also develop a correspondence between logarithmic regularization and what we term the pseudo-Dirichlet distribution  a generalization of the ordinary Dirichlet distribution well-suited for inducing sparsity. We demonstrate our approach on a text corpus  inferring a sparse topic mixture model for 2 406 weblogs.,A concave regularization technique

for sparse mixture models

School of Operations Research and Information Engineering

Center for Applied Mathematics

Johan Ugander

Cornell University

jhu5@cornell.edu

Martin Larsson

Cornell University

mol23@cornell.edu

Abstract

Latent variable mixture models are a powerful tool for exploring the structure in
large datasets. A common challenge for interpreting such models is a desire to
impose sparsity  the natural assumption that each data point only contains few la-
tent features. Since mixture distributions are constrained in their L1 norm  typical
sparsity techniques based on L1 regularization become toothless  and concave reg-
ularization becomes necessary. Unfortunately concave regularization typically re-
sults in EM algorithms that must perform problematic non-concave M-step maxi-
mizations. In this work  we introduce a technique for circumventing this difﬁculty 
using the so-called Mountain Pass Theorem to provide easily veriﬁable conditions
under which the M-step is well-behaved despite the lacking concavity. We also
develop a correspondence between logarithmic regularization and what we term
the pseudo-Dirichlet distribution  a generalization of the ordinary Dirichlet distri-
bution well-suited for inducing sparsity. We demonstrate our approach on a text
corpus  inferring a sparse topic mixture model for 2 406 weblogs.

1

Introduction

The current trend towards ‘big data’ has created a strong demand for techniques to efﬁciently extract
structure from ever-accumulating unstructured datasets. Speciﬁc contexts for this demand include
latent semantic models for organizing text corpora  image feature extraction models for navigating
large photo datasets  and community detection in social networks for optimizing content delivery.
Mixture models identify such latent structure  helping to categorize unstructured data.
Mixture models approach datasets as a set D of element d ∈ D  for example images or text doc-
uments. Each element consists of a collection of words w ∈ W drawn with replacement from a
vocabulary W. Each element-word pair observation is further assumed to be associated with an
unobserved class z ∈ Z  where Z is the set of classes. Ordinarily it is assumed that |Z| (cid:28) |D| 
namely that the number of classes is much less than the number of elements. In this work we ex-
plore an additional sparsity assumption  namely that individual elements only incorporate a small
subset of the |Z| classes  so that each element arises as a mixture of only (cid:96) (cid:28) |Z| classes. We de-
velop a framework to overcome mathematical difﬁculties in how this assumption can be harnessed
to improve the performance of mixture models.
Our primary context for mixture modeling in this work will be latent semantic models of text data 
where elements d are documents  words w are literal words  and classes z are vocabulary topics.
We apply our framework to models based on Probabilistic Latent Semantic Analysis (PLSA) [1].
While PLSA is often outperformed within text applications by techniques such as Latent Dirichlet
Allocation (LDA) [2]  it forms the foundation of many mixture model techniques  from computer
vision [3] to network community detection [4]  and we emphasize that our contribution is an opti-
mization technique intended for broad application outside merely topic models for text corpora. The

1

near-equivalence between PLSA and Nonnegative Matrix Factorization (NMF) [5  6] implies that
our technique is equally applicable to NMF problems as well. Sparse inference as a rule targets point
estimation  which makes PLSA-style models appropriate since they are inherently frequentist  de-
riving point-estimated models via likelihood maximization. In contrast  fully Bayesian frameworks
such as Latent Dirichlet Allocation (LDA) output a posterior distribution across the model space.
Sparse inference is commonly achieved through two largely equivalent techniques: regularization
or MAP inference. Regularization modiﬁes ordinary likelihood maximization with a penalty on
the magnitudes of the parameters. Maximum a posteriori (MAP) inference employs priors con-
centrated towards small parameter values. MAP PLSA is an established technique [7]  but earlier
work has been limited to log-concave prior distributions (corresponding to convex regularization
functions) that make a concave contribution to the posterior log-likelihood. While such priors allow
for tractable EM algorithms  they have the effect of promoting smoothing rather than sparsity. In
contrast  sparsity-inducing priors are invariably convex in their contribution. In this work we resolve
this difﬁculty by showing how  even though concavity fails in general  we are able to derive simple
checkable conditions that guarantee a unique stationary point to the M-step objective function that
serves as the unique global maximum. This rather surprising result  using the so-called Mountain
Pass Theorem  is a noteworthy contribution to the theory of learning algorithms which we expect
has many applications outside merely PLSA.
Section 2 brieﬂy outlines the structure of MAP inference for PLSA. Section 3 discusses priors ap-
propriate for inducing sparsity  and introduces a generalization of the Dirichlet distribution which
we term the pseudo-Dirichlet distribution. Section 4 contains our main result  a tractable EM algo-
rithm for PLSA under sparse pseudo-Dirichlet priors using the Mountain Pass Theorem. Section 5
presents empirical results for a corpus of 2 406 weblogs  and section 6 concludes with a discussion.

2 Background and preliminaries

2.1 Standard PLSA

(cid:88)

w d

(cid:104)(cid:88)

Within the PLSA framework  word-document-topic triplets (w  d  z) are assumed to be i.i.d. draws
from a joint distribution on W × D × Z of the form

(1)
where θ consists of the model parameters P (w | z)  P (z | d) and P (d) for (w  d  z) ranging over
W × D × Z. Following [1]  the corresponding data log-likelihood can be written

P (w  d  z | θ) = P (w | z)P (z | d)P (d) 

(cid:96)0(θ) =

n(w  d) log

P (w | z)P (z | d)

where n(w  d) is the number of occurrences of word w in document d  and n(d) =(cid:80)

w n(w  d) is
the total number of words in d. The goal is to maximize the likelihood over the set of admissible θ.
This is accomplished using the EM algorithm  iterating between the following two steps:
E-step: Find P (z | w  d  θ(cid:48))  the posterior distribution of the latent variable z  given (w  d) and a
current parameter estimate θ(cid:48).
M-step: Maximize Q0(θ | θ(cid:48)) over θ  where

n(d) log P (d) 

(cid:88)

+

d

(cid:105)

(2)

Q0(θ | θ(cid:48)) =

n(d) log P (d) +

d

w d z

n(w  d)P (z | w  d  θ(cid:48)) log

P (w | z)P (z | d)

.

z

(cid:88)

(cid:88)

(cid:104)

(cid:105)

We refer to [1] for details on the derivations  as well as extensions using so-called tempered EM.
The resulting updates corresponding to the E-step and M-step are  respectively 

n(d)

2

and

P (z | w  d  θ) =

P (w | z)P (z | d)
z(cid:48) P (w | z(cid:48))P (z(cid:48) | d)

(cid:80)
(cid:80)
(cid:80)
d P (z | w  d  θ(cid:48))n(w  d)
w(cid:48) d P (z | w(cid:48)  d  θ(cid:48))n(w(cid:48)  d)

 

 

P (d) =

P (w | z) =

(cid:80)
w P (z | w  d  θ(cid:48))n(w  d)

P (z | d) =

(3)

(4)

n(d)(cid:80)

d(cid:48) n(d(cid:48))

.

Note that PLSA has an alternative parameterization  where (1) is replaced by P (w  d  z | θ) =
P (w|z)P (d|z)P (z). This formulation is less interesting in our context  since our sparsity assump-
tion is intended as a statement about the vectors (P (z | d) : z ∈ Z)  d ∈ D.

2.2 MAP PLSA

The standard MAP extension of PLSA is to introduce a prior density P (θ) on the parameter vector 
and then maximize the posterior data log-likelihood (cid:96)(θ) = (cid:96)0(θ) + log P (θ) via the EM algorithm.
In order to simplify the optimization problem  we impose the reasonable restriction that the vectors
(P (w | z) : w ∈ W) for z ∈ Z  (P (z | d) : z ∈ Z) for d ∈ D  and (P (d) : d ∈ D) be mutually
independent under the prior P (θ). That is 

(cid:89)

z∈Z

P (θ) =

fz(P (w | z) : w ∈ W) × (cid:89)

d∈D

gd(P (z | d) : z ∈ Z) × h(P (d) : d ∈ D) 

for densities fz  gd and h on the simplexes in R|W|  R|Z| and R|D|  respectively. With this structure
on P (θ) one readily veriﬁes that the M-step objective function for the MAP likelihood problem 
Q(θ | θ(cid:48)) = Q0(θ | θ(cid:48)) + log P (θ)  is given by

(cid:88)

(cid:88)

Q(θ | θ(cid:48)) =

Fz(θ | θ(cid:48)) +

Gd(θ | θ(cid:48)) + H(θ | θ(cid:48)) 

where

Fz(θ | θ(cid:48)) =

Gd(θ | θ(cid:48)) =

H(θ | θ(cid:48)) =

(cid:88)
(cid:88)
(cid:88)

w d

w z

z

d

P (z | w  d  θ(cid:48))n(w  d) log P (w | z) + log fz(P (w | z) : w ∈ W) 

P (z | w  d  θ(cid:48))n(w  d) log P (z | d) + log gd(P (z | d) : z ∈ Z) 

n(d) log P (d) + log h(P (d) : d ∈ D).

d

As a comment  notice that if the densities fz  gd  or h are log-concave then Fz  Gd  and H are
concave in θ. Furthermore  the functions Fz  Gd  and H can be maximized independently  since
the corresponding non-negativity and normalization constraints are decoupled. In particular  the
|Z| + |D| + 1 optimization problems can be solved in parallel.

3 The pseudo-Dirichlet prior
The parameters for PLSA models consist of |Z| + |D| + 1 probability distributions taking their
values on |Z| + |D| + 1 simplexes. The most well-known family of distributions on the simplex
is the Dirichlet family  which has many properties that make it useful in Bayesian statistics [8].
Unfortunately the Dirichlet distribution is not a suitable prior for modeling sparsity for PLSA  as we
shall see  and to address this we introduce a generalization of the Dirichlet distribution which we
call the pseudo-Dirichlet distribution.
To illustrate why the Dirichlet distribution is unsuitable in the present context  consider placing a
symmetric Dirichlet prior on (P (z | d) : z ∈ Z) for each document d. That is  for each d ∈ D 

gd(P (z | d) : z ∈ Z) ∝ (cid:89)

P (z | d)α−1 

z∈Z

where α > 0 is the concentration parameter. Let fz and h be constant. The relevant case for sparsity
is when α < 1  which concentrates the density toward the (relative) boundary of the simplex. It is
easy to see that the distribution is in this case log-convex  which means that the contribution to the
log-likelihood and M-step objective function Gd(θ | θ(cid:48)) will be convex. We address this problem
in Section 4. A bigger problem  however  is that for α < 1 the density of the symmetric Dirichlet
distribution is unbounded and the MAP likelihood problem does not have a well-deﬁned solution 
as the following result shows.

3

Proposition 1 Under the above assumptions on fz  gd and h there are inﬁnitely many sequences
(θm)m≥1  converging to distinct limits  such that limm→∞ Q(θm | θm) = ∞. As a consequence 
(cid:96)(θm) tends to inﬁnity as well.
Proof. Choose θm as follows: P (d) = |D|−1 and P (w | z) = |W|−1 for all w  d and z. Fix d0 ∈ D
and z0 ∈ Z  and set P (z0 | d0) = m−1  P (z | d0) = 1−m−1
|Z|−1 for z (cid:54)= z0  and P (z | d) = |Z|−1 for
all z and d (cid:54)= d0. It is then straightforward to verify that Q(θm | θm) tends to inﬁnity. The choice of
d0 and z0 was arbitrary  so by choosing two other points we get a different sequence with a different
limit. Taking convex combinations yields the claimed inﬁnity of sequences. The second statement
follows from the well-known fact that Q(θ | θ(cid:48)) ≤ (cid:96)(θ) for all θ and θ(cid:48). (cid:3)
This proposition is a formal statement of the observation that when the Dirichlet prior is unbounded 
any single zero element in P (z|d) leads to an inﬁnite posterior likelihood  and so the optimization
problem is not well-posed. To overcome these unbounded Dirichlet priors while retaining their
sparsity-inducing properties  we introduce the following class of distributions on the simplex.
Deﬁnition 1 A random vector conﬁned to the simplex in Rp is said to follow a pseudo-Dirichlet
distribution with concentration parameter α = (α1  . . .   αp) ∈ Rp and perturbation parameter
 = (1  . . .   p) ∈ Rp

+ if it has a density on the simplex given by

P (x1  . . .   xp | α  ) = C

(i + xi)αi−1

(5)

i=1

for a normalizing constant C depending on α and . If αi = α and i =  for all i and some ﬁxed
α ∈ R   ≥ 0  we call the resulting distribution symmetric pseudo-Dirichlet.
Notice that if i > 0 for all i  the pseudo-Dirichlet density is indeed bounded for all α. If i = 0
and αi > 0 for all i  we recover the standard Dirichlet distribution. If i = 0 and αi ≤ 0 for some
i then the density is not integrable  but can still be used as an improper prior. Like the Dirichlet
distribution  when α < 1 the pseudo-Dirichlet distribution is log-convex  and it will make a convex
contribution to the M-step objective function of any EM algorithm.
The psuedo-Dirichlet distribution can be viewed as a bounded perturbation of the Dirichlet distri-
bution  and for small values of the perturbation parameter   many of the properties of the original
Dirichlet distribution hold approximately. In our discussion section we offer a justiﬁcation for al-
lowing α ≤ 0  framed within a regularization approach.

4 EM under pseudo-Dirichlet priors

p(cid:89)

We now derive an EM algorithm for PLSA under sparse pseudo-Dirichlet priors. The E-step is the
same as for standard PLSA  and is given by (3). The M-step consists in optimizing each Fz  Gd and
H individually. While our M-step will not offer a closed-form maximization  we are able to derive
simple checkable conditions under which the M-step has a stationary point that is also the global
maximum. Once the conditions are satisﬁed  the M-step optimum can be found via a practitioner’s
favorite root-ﬁnding algorithm. For consideration  we propose an iteration scheme that in practice
we ﬁnd converges rapidly and well. Because our sparsity assumption focuses on the parameters
P (z|d)  we perform our main analysis on Gd  but for completeness we state the corresponding
result for Fz. The less applicable treatment of H is omitted.
z P (z |
d) = 1 and P (z | d) ≥ 0 for all z. We use symmetric pseudo-Dirichlet priors with parameters
αd = (αd  . . .   αd) and d = (d  . . .   d) for αd ∈ R and d > 0. Since each Gd is treated
separately  let us ﬁx d and write

Consider the problem of maximizing Gd(θ | θ(cid:48)) over (P (z | d) : z ∈ Z) subject to(cid:80)

xz = P (z | d) 

cz =

P (z | w  d  θ(cid:48))n(w  d) 

where the dependence on d is suppressed in the notation. For x = (xz : z ∈ Z) and a ﬁxed θ(cid:48)  we
write Gd(x) = Gd(θ | θ(cid:48))  which yields  up to an additive constant 

Gd(x) =

(αd − 1) log(d + xz) + cz log xz

.

(cid:88)

w

(cid:88)

(cid:104)

z

4

(cid:105)

The task is to maximize Gd  subject to(cid:80)

z xz = 1 and xz ≥ 0 for all z. Assuming that every word
w is observed in at least one document d and that all components of θ(cid:48) are strictly positive  Lemma 1
below implies that any M-step optimizer must have strictly positive components. The non-negativity
constraint is therefore never binding  so the appropriate Lagrangian for this problem is

Ld(x; λ) = Gd(x) + λ

(cid:104)
1 −(cid:88)

(cid:105)

xz

 

z

and it sufﬁces to consider its stationary points.
Lemma 1 Assume that every word w has been observed in at least one document d  and that P (z |
w  d; θ(cid:48)) > 0 for all (w  d  z). If xz → 0 for some z  and the nonnegativity and normalization
constraints are maintained  then Gd(x) → −∞.
Proof. The assumption implies that cz > 0  ∀z. Therefore  since log(d + xz) and log xz are
bounded from above  ∀z  when θ stays in the feasible region  xz → 0 leads to Gd(x) → −∞. (cid:3)
The next lemma establishes a property of the stationary points of the Lagrangian Ld which will be
the key to proving our main result.
Lemma 2 Let (x  λ) be any stationary point of Ld such that xz > 0 for all z. Then λ ≥ n(d) −
(1 − αd)|Z|. If in addition to the assumptions of Lemma 1 we have n(d) ≥ (1 − αd)|Z|  then

(x  λ) < 0

for all z ∈ Z.

∂2Gd
∂x2
z
− 1−αd

d+xz

Proof. We have ∂Ld
λxz = cz−(1−αd) xz
yields

∂xz

d+xz

= cz
xz

Furthermore (cid:80)

z cz =(cid:80)

∂xz

− λ. Since ∂Ld

≥ cz−(1−αd)  which  after summing over z and using that(cid:80)
w n(w  d)(cid:80)

z P (z | w  d  θ(cid:48)) = n(d)  so λ ≥ n(d) − (1 − αd)|Z|.

(x  λ) = 0 at the stationary point  we get
z xz = 1 

λ ≥(cid:88)

cz − (1 − αd)|Z|.

z

For the second assertion  using once again that ∂Ld
shows that

∂xz

(x  λ) = 0 at the stationary point  a calculation

(cid:104)

(cid:105)

∂2Gd
∂x2
z

(x  λ) = −

1

x2
z(d + xz)

x2
zλ + czd

.

The assumptions imply that cz > 0  so it sufﬁces to prove that λ ≥ 0. This follows from our
hypothesis and the ﬁrst part of the lemma. (cid:3)
This allows us to obtain our main result result concerning the structure of the optimization problem
associated with the M-step.
Theorem 1 Assume that (i) every word w has been observed in at least one document d  (ii) P (z |
w  d  θ(cid:48)) > 0 for all (w  d  z)  and (iii) n(d) > (1 − αd)|Z| for each d. Then each Lagrangian
Ld has a unique stationary point  which is the global maximum of the corresponding optimization
problem  and whose components are strictly positive.

The proof relies on the following version of the so-called Mountain Pass Theorem.
Lemma 3 (Mountain Pass Theorem) Let O ⊂ Rn be open  and consider a continuously differen-
tiable function φ : O → R s.t. φ(x) → −∞ whenever x tends to the boundary of O. If φ has two
distinct strict local maxima  it must have a third stationary point that is not a strict local maximum.
Proof. See p. 223 in [9]  or Theorem 5.2 in [10]. (cid:3)
Proof of Theorem 1. Consider a ﬁxed d. We ﬁrst prove that the corresponding Lagrangian Ld
can have at most one stationary point. To simplify notation  assume without loss of generality that
Z = {1  . . .   K}  and deﬁne

(cid:101)Gd(x1  . . .   xK−1) = Gd

(cid:16)

x1  . . .   xK−1  1 − K−1(cid:88)

(cid:17)

xk

.

k=1

5

.

.

d+xK

∂2Gd
∂x2
k

k=1 y2
k

− 1−αd

k=1 xk and λ = cK
xK

{(x1  . . .   xK−1) ∈ RK−1

k xk < 1}. The following facts are readily veriﬁed:

The constrained maximization of Gd is then equivalent to maximizing (cid:101)Gd over the open set O =
++ :(cid:80)
(i) If (x  λ) is a stationary point of Ld  then (x1  . . .   xK−1) is a stationary point of (cid:101)Gd.
(ii) If (x1  . . .   xK−1) is a stationary point of (cid:101)Gd  then (x  λ) is a stationary point of Ld  where
xK = 1 −(cid:80)K−1
(iii) For any y = (y1  . . .   yK−1 (cid:80)K−1
with Lemma 2 imply that (x1  . . .   xK−1) is a stationary point of (cid:101)Gd and that ∇2(cid:101)Gd is negative
Next  suppose for contradiction that there are two distinct such points. By Lemma 1  (cid:101)Gd tends to
third point (˜x1  . . .   ˜xK−1)  stationary for (cid:101)Gd  that is not a strict local maximum. But by (ii)  this
(˜x1  . . .   ˜xK−1) has to be a strict local max for (cid:101)Gd  which is a contradiction. We deduce that Ld has

−∞ near the boundary of O  so we may apply the mountain pass theorem to get the existence of a
yields a corresponding stationary point (˜x  ˜λ) for Ld. The same reasoning as above then shows that

Now  suppose that (x  λ) is a stationary point of Ld. Property (i) and property (iii) in conjunction

k=1 yk)  we have yT∇2(cid:101)Gdy =(cid:80)K

deﬁnite there. Hence it is a strict local maximum.

at most one stationary point.
Finally  the continuity of Gd together with its boundary behavior (Lemma 1) implies that a maxi-
mizer exists and has strictly positive components. But the maximizer must be a stationary point of
Ld  so together with the previously established uniqueness  the result follows. (cid:3)
Condition (i) in Theorem 1 is not a real restriction  since a word that does not appear in any doc-
ument typically will be removed from the vocabulary. Moreover  if the EM algorithm is initialized
such that P (z | w  d; θ(cid:48)) > 0 for all (w  d  z)  Theorem 1 ensures that this will be the case for all fu-
ture iterates as well. The critical assumption is Condition (iii). It can be thought of as ensuring that
the prior does not drown the data. Indeed  sufﬁciently large negative values of αd  corresponding to
strong prior beliefs  will cause the condition to fail.
While there are various methods available for ﬁnding the stationary point of Ld  we have found that
the following ﬁxed-point type iterative scheme produces satisfactory results.

xz ←

n(d) + (1 − αd)

(cid:104)

cz
1

d+xz

−(cid:80)

.

(6)

xz ← xz(cid:80)

z xz

(cid:105)  
= 1 −(cid:88)

xz(cid:48)

d+xz(cid:48)

z(cid:48)

∂Ld
∂λ

xz  so by summing over z and using that(cid:80)

xz.

z

To motivate this particular update rule  recall that
− λ 

− 1 − αd
d + xz
At the stationary point  λxz = cz − 1−αd

z cz = n(d)  we get λ = n(d) − (1 − αd)(cid:80)

and(cid:80)

∂Ld
∂xz

cz
xz

d+xz

=

z

xz

d+xz

. Substituting this for λ in ∂Ld

z xz = 1
= 0
and rearranging terms yields the ﬁrst part of (6). Notice that Lemma 2 ensures that the denominator
stays strictly positive. Further  the normalization is a classic technique to restrict x to the simplex.
Note that (6) reduces to the standard PLSA update (4) if αd = 1.
For completeness we also consider the topic-vocabulary distribution (P (w|z) : w ∈ W). We
impose a symmetric pseudo-Dirichlet prior on the vector (P (w | z) : w ∈ W) for each z ∈ Z. The
corresponding parameters are denoted by αz and z. Each Fz is optimized individually  so we ﬁx
z ∈ Z and write yw = P (w | z). The objective function Fz(y) = Fz(θ | θ(cid:48)) is then given by
P (z | w  d  θ(cid:48))n(w  d).

(αz − 1) log(z + yw) + bw log yz

(cid:88)

(cid:88)

Fz(y) =

bw =

(cid:104)

(cid:105)

(7)

∂xz

 

w

d

The following is an analog of Theorem 1  whose proof is essentially the same and therefore omitted.
(cid:80)
Theorem 2 Assume condition (i) and (ii) of Theorem 1 are satisﬁed  and that for each z ∈ Z 
w bw ≥ (1 − αz)|W|. Then each Fz has a unique local optimum on the simplex  which is also a

global maximum and whose components are strictly positive.

6

Unfortunately there is no simple expression for(cid:80)

w bw in terms of the inputs to the problem. On
the other hand  the sum can be evaluated at the beginning of each M-step  which makes it possible
to verify that αz is not too negative.

5 Empirical evaluation

To evaluate our framework for sparse mixture model inference  we develop a MAP PLSA topic
model for a corpus of 2 406 blogger.com blogs  a dataset originally analyzed by Schler et al. [11]
for the role of gender in language. Unigram frequencies for the blogs were built using the python
NLTK toolkit [12]. Inference was run on the document-word distribution of 2 406 blogs and 2 000
most common words  as determined by the aggregate frequencies across the entire corpus. The
implications of Section 4 is that in order to adapt PLSA for sparse MAP inference  we simply need
to replace equation (4) from PLSA’s ordinary M-step with an iteration of (6).
The corpus also contains a user-provided ‘category’ for each blog  indicating one of 28 categories.
We focused our analysis on 8 varied but representative topics  while the complete corpus contained
over 19 000 blogs. The user-provided topic labels are quite noisy  and so in order to have cleaner
ground truth data for evaluating our model we chose to also construct a synthetic  sparse dataset.
This synthetic dataset is employed to evaluate parameter choices within the model.
To generate our synthetic data  we ran PLSA on our text corpus and extracted the inferred P (w|z)
and P (d) distributions  while creating 2 406 synthetic P (z|d) distributions where each synthetic
blog was a uniform mixture of between 1 and 4 topics. These distributions were then used to
construct a ground-truth word-document distribution Q(w  d)  which we then sampled N times 
where N is the total number of words in our true corpus. In this way we were able to generate a
realistic synthetic dataset with a sparse and known document-topic distribution.
We evaluate the quality of each model by calculating the model perplexity of the reconstructed word-
document distribution as compared to the underlying ground truth distribution used to generate the
synthetic data. Here model perplexity is given by

P(P (w  d)) = 2−(cid:80)

w d Q(w d) log2 P (w d) 

where Q(w  d) is the true document-word distribution used to generate the synthetic dataset and
P (w  d) is the reconstructed matrix inferred by the model. Using this synthetic dataset we are able
to evaluate the roles of α and  in our algorithm  as seen in Figure 1.
From Figure 1 we can conclude that α should in practice be chosen close the algorithm’s feasible
lower bound  and  can be almost arbitrarily small. Choosing α = (cid:100)1−maxd n(d)/k(cid:101) and  = 10−6 
we return to our blog data with its user-provided labels. In Figure 2 we see that sparse inference
indeed results in P (z|d) distributions with signiﬁcantly sparser support. Furthermore  we can more
easily see how certain categories of blogs cluster in their usage of certain topics. For example 
a majority of the blogs self-categorized as pertaining to ‘religion’ employ almost exclusively the
second topic vocabulary of the model. The ﬁve most exceptional unigrams for this topic are ‘prayer’ 
‘christ’  ‘jesus’  ‘god’  and ‘church’.

6 Discussion

We have shown how certain latent variable mixture models can be tractably extended with sparsity-
inducing priors using what we call the pseudo-Dirichlet distribution. Our main theoretical result
shows that the resulting M-step maximization problem is well-behaved despite the lack of concavity 
and empirical ﬁndings indicate that the approach is indeed effective. Our use of the Mountain Pass
Theorem to prove that all local optima coincide is to the best of our knowledge new in the literature 
and we ﬁnd it intriguing and surprising that the global properties of maximizers  which are very
rarely susceptible to analysis in the absence of concavity  can be studied using this tool.
The use of log-convex priors (equivalently  concave regularization functions) to encourage sparsity is
particularly relevant when the parameters of the model correspond to probability distributions. Since
each distribution has a ﬁxed L1 norm equal to one  the use of L1-regularization  which otherwise
would be the natural choice for inducing sparsity  becomes toothless. The pseudo-Dirichlet prior
i log(xi + ). We mention in

we introduce corresponds to a concave regularization of the form(cid:80)

7

Figure 1: Model perplexity for inferred models with k = 8 topics as a function of the concentration
parameter α of the pseudo-Dirichlet prior  shown from the algorithm’s lower bound α = 1− n(d)/k
to the uniform prior case of α = 1. Three different choices of  are shown  as well as the base-
line PLSA perplexity corresponding to a uniform prior. The dashed line indicates the perplexity
P(Q(w  d))  which should be interpreted as a lower bound.

Figure 2: Document-topic distributions P (z|d) for the 8 different categories of blogs studied. All
distributions share the same color scale.

passing that the same sum-log regularization has also been used for sparse signal recovery in [13].
It should be emphasized that the notion of sparsity we discuss in this work is not in the formal sense
of a small L0 norm. Indeed  Theorem 1 shows that  no different from ordinary PLSA  the estimated
parameters for MAP PLSA will all be strictly positive. Instead  we seek sparsity in the sense that
most parameters should be almost zero.
Next  let us comment on the possibility to allow the concentration parameter αd to be negative 
assuming for simplicity that fz and h are constant. Consider the normalized likelihood  where
clearly (cid:96)(θ) may be replaced by (cid:96)(θ)/N 

log(d + P (z | d)) 

−(cid:88)

d

(cid:96)(θ)
N

=

(cid:96)0(θ)

N

(cid:88)

1 − αd

N

z

which by (2) we deduce only depends on the data through the normalized quantities n(w  d)/N.
This indicates that the quantity (1 − αd)/N  which plays the role of a regularization ‘gain’ in the
normalized problem  must be non-negligible in order for the regularization to have an effect. For
realistic sizes of N  allowing αd < 0 therefore becomes crucial.
Finally  while we have chosen to present our methodology as applied to topic models  we expect the
same techniques to be useful in a notably broader context. In particular  our methodology is directly
applicable to problems solved through Nonnegative Matrix Factorization (NMF)  a close relative of
PLSA where matrix columns or rows are often similarly constrained in their L1 norm.
Acknowledgments: This work is supported in part by NSF grant IIS-0910664.

8

−50−40−30−20−1002.852.92.95x 105model perplexityα  ε=10−6ε=0.1ε=1PLSAα  P(z|d)  PLSA  Religion    P(z|d)  Ps-Dir MAP  Religion    P(z|d)  PLSA  Internet    P(z|d)  Ps-Dir MAP  Internet    P(z|d)  PLSA  Engineering    P(z|d)  Ps-Dir MAP  Engineering   P(z|d)  PLSA  Technology    P(z|d)  Ps-Dir MAP  Technology    P(z|d)  PLSA  Fashion    P(z|d)  Ps-Dir MAP  Fashion    P(z|d)  PLSA  Media    P(z|d)  Ps-Dir MAP  Media    P(z|d)  PLSA  Tourism    P(z|d)  Ps-Dir MAP  Tourism    P(z|d)  PLSA  Law    P(z|d)  Ps-Dir MAP  Law  References
[1] T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Machine Learn-

ing  42:177–196  2001.

[2] D.M. Blei  A.Y. Ng  and M.I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[3] A. Bosch  A. Zisserman  and X. Munoz. Scene Classiﬁcation via pLSA. In European Confer-

ence on Computer Vision  2006.

[4] I. Psorakis and B. Sheldon. Soft Partitioning in Networks via Baysian Non-negative Matrix

Factorization. In NIPS  2010.

[5] C. Ding  T. Li  and W. Peng. Nonnegative matrix factorization and probabilistic latent semantic
indexing: Equivalence chi-square statistic  and a hybrid method. In Proceedings of AAAI ’06 
volume 21  page 342  2006.

[6] E. Gaussier and C. Goutte. Relation between PLSA and NMF and implications. In Proceedings

of ACM SIGIR  pages 601–602. ACM  2005.

[7] A. Asuncion  M. Welling  P. Smyth  and Y.W. Teh. On smoothing and inference for topic
models. In Proc. of the 25th Conference on Uncertainty in Artiﬁcial Intelligence  pages 27–34 
2009.

[8] A. Gelman. Bayesian data analysis. CRC Press  2004.
[9] R. Courant. Dirichlet’s principle  conformal mapping  and minimal surfaces. Interscience 

New York  1950.

[10] Y. Jabri. The Mountain Pass Theorem: Variants  Generalizations and Some Applications.

Cambridge University Press  2003.

[11] J. Schler  M. Koppel  S. Argamon  and J. Pennebaker. Effects of age and gender on blogging.
In Proc. of the AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs 
pages 191–197  2006.

[12] S. Bird  E. Klein  and Loper E. Natural language processing with Python. O’Reilly Media 

2009.

[13] E.J. Cand`es  M.B. Wakin  and S.P. Boyd. Enhancing sparsity by reweighted (cid:96)1 minimization.

Journal of Fourier Analysis and Applications  14:877–905  2008.

9

,Michaël Perrot
Amaury Habrard
Jeremy Maitin-Shepard
Viren Jain
Michal Januszewski
Peter Li
Pieter Abbeel