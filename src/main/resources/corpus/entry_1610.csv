2018,Does mitigating ML's impact disparity require treatment disparity?,Following precedent in employment discrimination law  two notions of disparity are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently;
algorithms exhibit impact disparity when outcomes differ across subgroups (even unintentionally). Naturally  we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning processes (DLPs). Here  the sensitive feature is used during training but a group-blind classifier is produced. In this paper  we show that: (i) when sensitive and (nominally) nonsensitive features are correlated  DLPs will indirectly implement treatment disparity  undermining the policy desiderata they are designed to address; (ii) when group membership is partly revealed by other features  DLPs induce within-class discrimination; and (iii) in general  DLPs provide suboptimal trade-offs between accuracy and impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs.,Does mitigating ML’s impact disparity

require treatment disparity?

Zachary C. Lipton1  Alexandra Chouldechova1  Julian McAuley2

zlipton@cmu.edu  achould@cmu.edu  jmcauley@cs.ucsd.edu

1Carnegie Mellon University

2University of California  San Diego

Abstract

Following precedent in employment discrimination law  two notions of disparity
are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment
disparity if they formally treat members of protected subgroups differently; al-
gorithms exhibit impact disparity when outcomes differ across subgroups (even
unintentionally). Naturally  we can achieve impact parity through purposeful treat-
ment disparity. One line of papers aims to reconcile the two parities proposing
disparate learning processes (DLPs). Here  the sensitive feature is used during
training but a group-blind classiﬁer is produced. In this paper  we show that: (i)
when sensitive and (nominally) nonsensitive features are correlated  DLPs will
indirectly implement treatment disparity  undermining the policy desiderata they
are designed to address; (ii) when group membership is partly revealed by other fea-
tures  DLPs induce within-class discrimination; and (iii) in general  DLPs provide
suboptimal trade-offs between accuracy and impact parity. Experimental results on
several real-world datasets highlight the practical consequences of applying DLPs.

1

Introduction

Effective decision-making requires choosing among options given the available information. That
much is unavoidable  unless we wish to make trivial decisions. In selection processes  such as hiring 
university admissions  and loan approval  the options are people; the available features include (but
are rarely limited to) direct evidence of qualiﬁcations; and decisions impact lives.
Laws in many countries restrict the ways in which certain decisions can be made. For example  Title
VII of the US Civil Rights Act [1]  forbids employment decisions that discriminate on the basis of
certain protected characteristics. Interpretation of this law has led to two notions of discrimination:
disparate treatment and disparate impact. Disparate treatment addresses intentional discrimination 
including (i) decisions explicitly based on protected characteristics; and (ii) intentional discrimination
via proxy variables (e.g literacy tests for voting eligibility). Disparate impact addresses facially
neutral practices that might nevertheless have an “unjustiﬁed adverse impact on members of a
protected class” [1]. One might hope that detecting unjustiﬁed impact were as simple as detecting
unequal outcomes. However  absent intentional discrimination  unequal outcomes can emerge due
to correlations between protected and unprotected characteristics. Complicating matters  unequal
outcomes may not always signal unlawful discrimination [2].
Recently  owing to the increased use of machine learning (ML) to assist in consequential decisions 
the topic of quantifying and mitigating ML-based discrimination has attracted interest in both policy
and ML. However  while the existing legal doctrine offers qualitative ideas  intervention in an ML-
based system requires more concrete formalism. Inspired by the relevant legal concepts  technical
papers have proposed several criteria to quantify discrimination. One criterion requires that the
fraction given a positive decision be equal across different groups. Another criterion states that a

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Demonstration of a DLP’s undesirable side effects on a simple example of hiring data
(see §4.1). An unconstrained classiﬁer (vertical line) hires candidates based on work experience 
yielding higher hiring rates for men than for women. A DLP (dashed diagonal) achieves near-parity
by differentiating based on an irrelevant attribute (hair length). The DLP hurts some short-haired
women  ﬂipping their decisions to reject  and helps some long-haired men.

classiﬁer should be blind to the protected characteristic. Within the technical literature  these criteria
are commonly referred to as disparate impact and disparate treatment  respectively.
In this paper  we call these technical criteria impact parity and treatment parity to distinguish them
from their legal antecedents. The distinction between technical and legal terminology is important to
maintain. While impact and treatment parity are inspired by legal concepts  technical approaches that
achieve these criteria may fail to satisfy the underlying legal and ethical desiderata.
We demonstrate one such disconnect through DLPs  a class of algorithms designed to simultaneously
satisfy treatment- and impact-parity criteria [3–5]. DLPs operate according to the following principle:
The protected characteristic may be used during training  but is not available to the model at
prediction time. In the earliest such approach the protected characteristic is used to winnow the set of
acceptable rules from an expert system [3]. Others incorporate the protected characteristic as either a
regularizer  a constraint  or to preprocess the training data [5–7].
These approaches are grounded in the premise that DLPs are acceptable in cases where using a
protected characteristic as a direct input to the model would constitute disparate treatment and thus
be impermissible. Indeed  DLPs in some sense operationalize a form of prospective fair “test design”
that is well aligned with the ruling in Ricci v. DeStefano [8]. In this paper we investigate the utility of
DLPs as a technical solution and present the following cautionary insights:
1. When protected characteristics are redundantly encoded in the other features  sufﬁciently powerful

DLPs can (indirectly) implement any form of treatment disparity.

2. When protected characteristics are partially encoded DLPs induce within-class discrimination

based on irrelevant features  and can harm some members of the protected group.

3. DLPs provide a suboptimal trade-off between accuracy and impact parity.
4. While disparate treatment is by deﬁnition illegal  the status of treatment disparity is debated [9].

2 Disparate Learning Processes

To begin our formal description of the prior work  we’ll introduce some notation. A dataset consists
of n examples  or data points {xi ∈ X   yi ∈ Y}  each consisting of a feature vector xi and a label yi.
A supervised learning algorithm f : X n × Y n → (X → [0  1]) is a mapping from datasets to models.
The learning algorithm produces a model ˆy : X → Y  which given a feature vector xi  predicts the
corresponding output yi. In this discussion we focus on binary classiﬁcation (Y = {0  1}).
We consider probabilistic classiﬁers which produce estimates ˆp(x) of the conditional probability
P(y = 1 | x) of the label given a feature vector x. To make a prediction ˆy(x) ∈ Y given an estimated
probability ˆp(x) a threshold rule is used: ˆyi = 1 iff ˆpi > t. The optimal choice of the threshold
t depends on the performance metric being optimized. In our theoretical analysis  we consider

2

0.02.55.07.510.012.515.017.520.0Work experience (years)051015202530Hair length (cm)Acc=0.96; p% rule=26% - UnconstrainedAcc=0.74; p% rule=105% - DLPWoman advantaged by DLPWoman disadvantaged by DLPMan advantaged by DLPMan disadvantaged by DLP(cid:80)n

optimizing the immediate utility [10]  of which classiﬁcation accuracy (expected 0 − 1 loss) is a
special case. We will deﬁne this metric more precisely in the next section.
In formal descriptions of discrimination-aware ML  a dataset possesses a protected feature zi ∈ Z 
making each example a three-tuple (xi  yi  zi). The protected characteristic may be real-valued  like
age  or categorical  like race or gender. The goal of many methods in discrimination-aware ML is not
only to maximize accuracy  but also to ensure some form of impact parity. Following related work 
we consider binary protected features that divide the set of examples into two groups a and b. Our
analysis extends directly to settings with more than two groups.
Of the various measures of impact disparity  the two that are the most relevant here are the Calders-
1(ˆpi > t)  where nz =
Verwer gap and the p-% rule. At a given threshold t  let qz = 1
nz
1(zi = z). The Calders-Verwer (CV) gap  qa − qb  is the difference between the proportions
assigned to the positive class in the advantaged group a and the disadvantaged group b [4]. The p-%
rule is a related metric[5]. Classiﬁers satisfy the p-% rule if qb/qa ≥ p/100.
Many papers in discrimination-aware ML propose to optimize accuracy (or some other risk) subject
to constraints on the resulting level of impact parity as assessed by some metric [3  7  11–14]. Use of
DLPs presupposes that using the protected feature z as a model input is impermissible in this effort.
Discarding protected features  however  does not guarantee impact parity [15]. DLPs incorporate z in
the learning algorithm  but without making it an input to the classiﬁer. Formally  a DLP is a mapping:
X n × Y n × Z n → (X → Y). By deﬁnition  DLPs achieve treatment parity. However  satisfying
treatment parity in this fashion may still violate disparate treatment.

i

(cid:80)

i:zi=z

Alternative approaches. Researchers have proposed a number of other techniques for reconciling
accuracy and impact parity. One approach consists of preprocessing the training data to reduce the
dependence between the resulting model predictions and the sensitive attribute [6  16–19]. These
methods differ in terms of which variables they affect and the degree of independence achieved. [6]
proposed ﬂipping negative labels of training examples form the protected group. [20] proposed learn-
ing representations (cluster assignments) so that group membership cannot be inferred from cluster
membership. [17] and [19] also construct representations designed to be marginally independent
from Z.

3 Theoretical Analysis

We present a set of simple theoretical results that demonstrate the optimality of treatment disparity 
and highlight some properties of DLPs. We summarize our results as follows:
1. Direct treatment disparity on the basis of z is the optimal strategy for maximizing classiﬁcation

accuracy1 subject to CV and p-% constraints.

2. When X fully encodes Z  a sufﬁciently powerful DLP is equivalent to treatment disparity.
In Section 4  we empirically demonstrate a related point:
3. When X only partially encodes Z  a DLP may be suboptimal and can induce intra-group disparity

on the basis of otherwise irrelevant features correlated with Z.

Treatment disparity is optimal Absent impact parity constraints  the Bayes-optimal decision
rule for minimizing expected 0 − 1 loss (i.e.  maximizing accuracy) is given by d∗
uncon(x  z) =
δ(pY |X Z(x  z) ≥ 0.5)  where δ() is an indicator function.
We now show that the optimal decision rules in the CV and p-% constrained problems have a similar
form. The optimal decision rule will again be based on thresholding pY |X Z(x  z)  but at group-
speciﬁc thresholds. These rules can be thought of as operationalizing the following mechanism:
Suppose that we start with the classiﬁcations of the unconstrained rule d∗
uncon(x  z)  and this results
in a CV gap of qa − qb > γ. To reduce the CV gap to γ we have two mechanisms: We can (i) ﬂip
predictions from 0 to 1 in group b  and (ii) we can ﬂip predictions from 1 to 0 in group a. The optimal
strategy is to perform these ﬂips on group b cases that have the highest value of pY |X Z(x  z) and
group a cases that have the lowest value of pY |X Z(x  z).

1Our results are all presented in terms of a more general performance metric  of which classiﬁcation accuracy

is a special case.

3

The results in this section adapt the work of [10]  who establish optimal decision rules d under exact
parity. In that work  the authors characterize the optimal decision rule d = d(x  z) that maximizes the
immediate utility u(d  c) = E[Y d(X  Z) − cd(X  Z)] for (0 < c < 1)  under different exact parity
criteria. We begin with a lemma showing that expected classiﬁcation accuracy has the functional
form of an immediate utility function.
Lemma 1. Optimizing classiﬁcation accuracy is equivalent to optimizing immediate utility with
c = 0.5.
Proof. The expected accuracy of a binary decision rule d(X) can be written as E[Y d(X) + (1 −
Y )(1 − d(X))]. Expanding and rearranging this expression gives
E[Y d(X) + (1 − Y )(1 − d(X))] = E(2Y d(X) − d(X)) + E(Y ) + 1 = 2u(d  0.5) + E(Y ) + 1.
The only term in this expression that depends on d is the immediate utility u. Thus the decision rule
that maximizes u also maximizes accuracy.

We note that the results in this section are related to the recent independent work of [21]  who derive
Bayes-optimal decision rules under the same parity constraints we consider here  working instead
with the cost-sensitive risk  CS(d; c) = π(1 − c)FNR(d) + (1 − π)cFPR(d)  where π = P(Y = 1).
One can show that u(d  c) = −CS(d; c) + π(1− c)  and hence the problem of maximizing immediate
utility considered here is equivalent to minimizing cost-sensitive risk as in [21]. In our case  it will be
more convenient to work with the immediate utility.
For the next set of results  we follow [10] and assume that pY |X Z(X  Z)  viewed as a random variable 
has positive density on [0  1]. This ensures that the optimal rules are unique and deterministic by
disallowing point-masses of probability that would necessitate tie-breaking among observations with
equal probability. The ﬁrst result that we state is a direct corollary of two results in [10]. It considers
the case where we desire exact parity  i.e.  that qa = qb.
Corollary 2. The optimal decision rules d∗ under various parity constraints have the following form
and are unique up to a set of probability zero:
the optimum is d∗(x  z) =
1. Among rules satisfying statistical parity (the 100% rule) 
δ(pY |X Z(x  z) ≥ tz)  where tz ∈ [0  1] are constants that depend only on group membership z.
2. Among rules that have equal false positive rates across groups  the optimum is d∗(x  z) =
δ(pY |X Z(x  z) ≥ sz)  where sz are constants that depend only on group membership z (but are
different from tz).
3. (1) and (2) continue to hold even in the resource-constrained setting where the overall proportion

of cases classiﬁed as positive is constrained.

Proof. (1) and (2) are direct corollaries of Lemma 1 combined with Thm 3.2 and Prop 3.3 of [10].

The next set of results establishes optimality under general p-% and CV rules.
Proposition 3. Under the same assumptions as above  the optimum among rules that satisfy the CV
constraint 0 ≤ qa − qb < γ or the p-% rule also has the form d∗(x  z) = δ(pY |X Z(x  z) ≥ tz) 
where tz ∈ [0  1] are constants that depend on the group membership z  and on the choice of
constraint parameter γ or p. The thresholds tz are different for the CV constraint and p-% rule.

Proof. Suppose that the optimal solution under the CV or p-% rule constraint classiﬁes proportions
qa and qb of the advantaged and disadvantaged groups  respectively  to the positive class. As shown
in Corbett-Davies et al. [10]  we can rewrite the immediate utility as
u(d  0.5) = E[d(X  Z)(pY |X Z − 0.5)].

Thus the utility will be maximized when d∗(X  Z) = 1 for the qz proportion of individuals in each
group that have the highest values of pY |X Z. Since the optimal values of qz may differ between the
CV-constrained solution and the p-% solution  the optimal thresholds may differ as well.

Our ﬁnal result shows that a decision rule that does not directly use z as an input variable or for
determining thresholds will have lower accuracy than the optimal rule that uses this information. That
is  we show that DLPs are suboptimal for trading off accuracy and impact parity.

4

Theorem 4. Let d∗(x  z) be the optimal decision rule under a the CV-γ or p-% constraint. Let
dDLP (x) be the optimal solution to a DLP. If d(x  z) and dDLP (x) satisfy CV or p-% constraints
with the same qa and qb  the DLP solution results in lower or equal accuracy (equal only if the
solutions are the same.)

Proof. From Proposition 3  we know that the unique accuracy-optimizing solution is given by
d∗(x  z) = δ(pY |X Z(x  z) ≥ tz)  where tz is the 1 - qz quantile of pY |X Z. The difference in
immediate utility between the two decision rules can be expressed as follows:
∗
E[d
∗
= E[(d
= E[pY |X Z−.5|d

(X  Z)(pY |X Z − .5)] − E[dDLP (X)(pY |X Z − .5)]

(X  Z) − dDLP (X))(pY |X Z − 0.5)]

= 1  dDLP = 0)−E[pY |X Z−.5|d
∗

= 1  dDLP = 0]P(d

∗
= 0  dDLP = 1)

=(cid:0)E[pY |X Z − .5 | d

∗

= 0  dDLP = 1](cid:1)P(d

∗
= 0  dDLP = 1]P(d

∗

= 1  dDLP = 0)

∗

= 1  dDLP = 0] − E[pY |X Z − .5 | d
∗

≥ 0
The ﬁnal inequality follows since d∗(X  Z) = 1 for the highest values of pY |X Z  so pY |X Z is
stochastically greater on the event {d∗ = 1  dDLP = 0} than on {d∗ = 0  dDLP = 1}. Note that equality
holds only if P(d∗ = 1  dDLP = 0) = 0  i.e.  if the two rules are (almost surely) equivalent.

Our results continue to hold under “do no harm” constraints  where we require that any individual in
the disadvantaged group who was classiﬁed as positive under the unconstrained rule duncons(x  z)
remains positively classiﬁed. This corresponds to the setting where the proportion of cases in the
disadvantaged group classiﬁed as positive is constrained to be no lower than the proportion under
the unconstrained rule (or no lower than some ﬁxed value qmin
). Such constraints impose an upper
bound on the optimal thresholds tb  but do not change the structure of the optimal rules.

a

Functional equivalence when protected characteristic is redundantly encoded. Consider the
case where the protected feature z is redundantly encoded in the other features x. More precisely 
suppose that there exists a known subcomputation g such that z = g(x). This allows for any function
of the data f (x  z) to be represented as a function of x alone via ˜f (x) = f (x  g(x)). While it remains
the case that ˜f (x) does not directly use z as an input variable—and thus satisﬁes treatment parity— ˜f
should be no less legally suspect from a disparate treatment perspective than the original function f
that uses z directly. The main difference for the purpose of our discussion is that ˜f  resulting from a
DLP  may technically satisfy treatment parity  while f does not.
While this form of “strict” redundancy is unlikely  characterizing this edge case is important for
considering whether DLPs should have different legal standing vis-a-vis disparate treatment than
methods that use z directly. This is particularly relevant if one thinks of the ‘practitioner’ in question
as having discriminatory intent. Furthermore  the partial encoding of the protected attribute is
commonplace in settings where discrimination is a concern (as with gender in our experiment in
§4). Indeed  the very premise of DLPs requires that x is signiﬁcantly correlated with z. Moreover 
DLPs provide an incentive for practitioners to game the system by adding features that are predictive
of the protected attribute but not necessarily of the outcome  as these would improve the DLP’s
performance.

Within-class discrimination when protected characteristic is partially redundantly encoded.
When the protected characteristic is partially encoded in the other features  disparate treatment may
induce within-class discrimination by applying the beneﬁt of the afﬁrmative action unevenly  and can
even harm some members of the protected class. Next we demonstrate this phenomenon empirically
using (synthetically biased) university admissions data and several public datasets. The ease of
producing such examples might convince the reader that the varied effects of intervention with a DLP
on members of the disadvantaged group raises practice and policy concerns about DLPs.

4 Empirical Analysis

This preceding analysis demonstrates several theoretical advantages to increasing impact parity via
treatment disparity:

5

olds maximizes accuracy subject to an impact parity constraint.

• Optimality: As demonstrated for CV score and for p-% rule  intervention via per-group thresh-
• Rational ordering: Within each group  individuals with higher probability of belonging to the
• Does no harm to the protected group: The treatment disparity intervention can be constrained

positive class are always assigned to the positive class ahead of those with lower probabilities.

to only beneﬁt members of the disadvantaged class.

DLPs attempt to produce a classiﬁer that satisﬁes the parity constraints  by relying upon the proxy
features to satisfy the parity metric. Typically  this is accomplished either by introducing constraints
to a convex optimization problem  or by adding a regularization term and tuning the corresponding
hyper-parameter. Because the CV score and p-% rule are non-convex in model parameters (scores
only change when a point crosses the decision boundary)  [4  5] introduce convex surrogates aimed at
reducing the correlation between the sensitive feature and the prediction.
These approaches presume that the proxy variables contain information about the sensitive attribute.
Otherwise  the parity could only be satisﬁed via a trivial solution (e.g. assign either everyone or
nobody to the positive class). So we must consider two scenarios: (i) the proxy variables x fully
encode z  in which case  a sufﬁciently powerful DLP will implicitly reconstruct z  because this gives
the optimal solution to the impact-constrained objective; and (ii) x doesn’t fully capture z  or the
DLP is unable to recover z from x  in which case the DLP may be sub-optimal  may violate rational
ordering within groups  and may harm members of the disadvantaged group.

4.1 Synthetic data example: work experience and hair length in hiring

To begin  we illustrate our arguments empirically with a simple synthetic data experiment. To
construct the data  we sample nall = 2000 total observations from the data-generating process
described below. 70% of the observations are used for training  and the remaining 30% are reserved
for model testing.

zi ∼ Bernoulli(0.5)
hair_lengthi | zi = 1 ∼ 35 · Beta(2  2)
hair_lengthi | zi = 0 ∼ 35 · Beta(2  7)

work_expi | zi ∼ Poisson(25 + 6zi) − Normal(20  σ = 0.2)
yi | work_exp ∼ 2 · Bernoulli(pi) − 1 

where pi = 1/ (1 + exp[−(−25.5 + 2.5work_exp)])

This data-generating process has the following key properties: (i) the historical hiring process was
based solely on the number of years of work experience; (ii) because women on average have fewer
years of work experience than men (5 years vs. 11)  men have been hired at a much higher rate than
women; and (iii) women have longer hair than men  a fact that was irrelevant to historical hiring
practice.
Figure 1 shows the test set results of applying a DLP to the available historical data to equalize
hiring rates between men and women. We apply the DLP proposed by Zafar et al. [5]  using code
available from the authors.2 While the DLP nearly equalizes hiring rates (satisfying a 105-% rule)  it
does so through a problematic within-class discrimination mechanism. The DLP rule advantages
individuals with longer hair over those with shorter hair and considerably longer work experience.
We ﬁnd that several women who would have been hired under historical practices  owing to their
12+ years of work experience  would not be hired under the DLP due to their short hair (i.e.  their
male-like characteristics captured in x). Similarly  several men  who would not have been hired
based on work experience alone  are advantaged by the DLP due to their longer hair (i.e.  their
‘female-like’ characteristics in x). The DLP violates rational ordering  and harms some of the most
qualiﬁed individuals in the protected group. Group parity is achieved at the cost of individual
unfairness.
Granted  we might not expect factors such as hair length to knowingly be used as inputs to a typical
hiring algorithm. We construct this toy example to illustrate a more general point: since DLPs do
not have direct access to the protected feature  they must infer from the other features which people
are most likely to belong to each subgroup. Using the protected feature directly can yield more

2https://github.com/mbilalzafar/fair-classification/

6

Figure 2: (left) probability of the sensitive variable versus (unconstrained) admission probability 
on unseen test data. Downward triangles indicate individuals rejected only after applying the DLP
(“treatment”)  while upward triangles indicate individuals accepted only by the DLP. The remaining
∼4 000 blue/yellow dots indicate people whose decisions are not altered. Many students beneﬁting
from the DLP are males who ‘look like’ females based on other features  whereas females who ‘look
like’ males are hurt by the DLP. Detail view (center) and summary statistics (right) of the same plot.

reasonable policies: For example  by applying per-group thresholds  we could hire the highest rated
individuals in each group  rather than distorting rankings within groups based on how female/male
individuals appear to be from their other features.

4.2 Case study: Gender bias in CS graduate admissions

For our next example  we demonstrate a similar result but this time by analyzing real data with
synthetic discrimination  to empirically demonstrate our arguments. We consider a sample of ∼9 000
students considered for admission to the MS program of a large US university over an 11-year period.
Half of the examples are withheld for testing. Available attributes include basic information  such as
country of origin  interest areas  and gender  as well as quantitative ﬁelds such as GRE scores. Our
data also includes a label in the form of an ‘above-the-bar’ decision provided by faculty reviewers.
Admission rates for male and female applicants were observed to be within 1% of each other. So  to
demonstrate the effects of DLPs  we corrupt the data with synthetic discrimination. Of all women who
were admitted  i.e.  zi = b  yi = 1  we ﬂip 25% of those labels to 0: giving noisy labels ¯yi = yi · η 
for η ∼ Bernoulli(.25). This simulates historical bias in the training data.
We then train three logistic regressors: (1) To predict the (synthetically corrupted) labels ¯yi from the
non-sensitive features xi; (2) The same model  applying the DLP of [5]; and (3) A model to predict
the sensitive feature zi from the non-sensitive features xi. The data contains limited information
that predicts gender  though such predictions can be made better than random (AUC=0.59) due to
different rates of gender imbalance across (e.g.) countries and interests.
Figure 2 (left) shapes our basic intuition for what is happening: Considering the probability of
admission for the unconstrained classiﬁer (y-axis)  students whose decisions are ‘ﬂipped’ (after
applying the fairness constraint) tend to be those close to the decision boundary. Furthermore 
students predicted to be male (x-axis) tend to be ﬂipped to the negative class (left half of plot) while
students predicted to be female tend to be ﬂipped to the positive class (right half of plot). This
is shown in detail in Figure 2 (center and right). Of the 43 students whose decisions are ﬂipped
to ‘non-admit ’ 5 are female  each of whom has ‘male-like’ characteristics according to their other
features as demonstrated in our synthetic hair-length example. Demonstrated here with real-world
data  the DLP both disrupts the within-group ordering and violates the do no harm principle by
disadvantaging some women who  but for the DLP  would have been admitted.

Comparison with Treatment Disparity. To demonstrate the better performance of per-group
thresholding  we implement a simple decision scheme and compare its performance to the
DLP.
Our thresholding rule for maximizing accuracy subject to a p-% rule works as follows: Recall that
100 qa − qb < 0. We denote
the p-% rule requires that qb/qa > p/100  which can be written as
100 qa − qb as the p-gap. To maximize accuracy subject to satisfying the p-% rule  we
the quantity p
construct a score that quantiﬁes reduction in p-gap per reduction in accuracy. Starting from the

p

7

0.00.20.40.60.81.0p(female)0.00.20.40.60.81.0p(admit) (unconstrained)Graduate admissions w/ 25% synthetic rejection of femalesFemale  admitted because of treatmentFemale  rejected because of treatmentMale  admitted because of treatmentMale  rejected because of treatment0.00.10.20.30.40.50.6p(female)0.420.440.460.480.500.520.540.560.580.60p(admit) (unconstrained)Graduate admissions w/ 25% synthetic rejection of femalesFemale  admitted because of treatmentFemale  rejected because of treatmentMale  admitted because of treatmentMale  rejected because of treatment0.00.10.20.30.40.50.6p(female)0.47550.50830.48180.5221p(admit) (unconstrained)Graduate admissions w/ 25% synthetic rejection of femalesFemale  admitted because of treatmentFemale  rejected because of treatmentMale  admitted because of treatmentMale  rejected because of treatmentTable 1: Statistics of public datasets.

dataset
Income
Marketing
Credit
Employee Attr.
Customer Attr.

source
UCI [22]
UCI [23]
UCI [24]
IBM [25]
IBM [25]

protected feature
Gender (female)
Status (married)
Gender (female)
Status (married)
Status (married)

prediction target
income > $50k
customer subscribes
credit card default
employee attrition
customer attrition

n
32 561
45 211
30 000
1 470
7 043

accuracy-maximizing classiﬁcations ˆy (thresholding at .5)  we then ﬂip those predictions which close
the gap fastest:
1. Assign each example with {˜yi = 0  zi = b} or {˜yi = 1  zi = a}  a score ci equal to the reduction

in the p-gap divided by the reduction in accuracy:
(a) For each example in group a with initial ˆyi = 1  ci =
(b) For each example in group b with initial ˆyi = 0  ci =

p

100na(2 ˆpi−1).
nb(1−2 ˆpi).

1

2. Flip examples in descending order according to this score until the desired CV-score is reached.

These scores do not change after each iteration  so the greedy policy leads to optimal ﬂips (equivalently 
optimal classiﬁcation thresholds).

The unconstrained classiﬁer achieves a p-% rule of 71.4%. By applying this thresholding strategy 
we were able to obtain the same accuracy as the method of [5]  but with a higher p-% rule of 78.3%
compared to 77.6%. Note that on this data  the method of [5] maxes out at a p-% rule of 77.6%. That
is  the method is limited in what p-% rules may be achieved. By contrast  the thresholding rule can
achieve any desired parity level. Subject to a < 1% drop in accuracy relative to the DLP we can
achieve a p-% rule of ∼ 100%.

4.3 Examples on public datasets

Finally  for reproducibility  we repeat our experiments from Section 4.2 on a variety of public
datasets (code and data will be released at publication time). Again we compare applying our simple
thresholding scheme against the fairness constraint of [5]  considering a binary outcome and a single
protected feature. Basic info about these datasets (including the prediction target and protected
feature) is shown in Table 1.

The protocol we follow is the same as in Section 4.2. Each of these datasets exhibits a certain degree
of bias w.r.t. the protected characteristic (Table 2)  so no synthetic discrimination is applied. In
Table 2  we compare (1) The p-% rule obtained using the classiﬁer of [5] compared to that of a naïve
classiﬁer (column k vs. column h); and (2) The p-% rule obtained when applying our thresholding
strategy from Section 4.2. As before  half of the data are withheld for testing.

First  we note that in most cases  the method of [5] increases the p-% rule (column k vs. h)  while
maintaining an accuracy similar to that of unconstrained classiﬁcation (column i vs. f). One exception
is the UCI-Credit dataset  in which both the accuracy and the p-% rule simultaneously decrease;
although this is against our expectations  note that the optimization technique of [5] is an approxima-
tion scheme and does not offer accuracy guarantees in practice (nor can it in general achieve a p-%
rule of 100%). However these details are implementation-speciﬁc and not the focus of this paper.
Second  as in Section 4.2  we note that the optimal thresholding strategy is able to offer a strictly
larger p-% rule (column l vs. k) at a given accuracy (in this case  the accuracy from column i). In
most cases  we can obtain a p-% rule of (close to) 100% at the given accuracy.

We emphasize that the goal of our experiments is not to ‘beat’ the method of [5]  or even to comment
on any speciﬁc discrimination-aware classiﬁcation scheme. Rather  we emphasize that any DLP is
fundamentally upper-bounded (in terms of the p-% rule/accuracy trade-off) by simple schemes that
explicitly consider the protected feature. Our experiments validate this claim  and reveal that the
two schemes make strikingly different decisions. While concealing the protected feature from the
classiﬁer may be conceptually desirable  practitioners should be aware of the consequences.

8

Table 2: Comparison between unconstrained classiﬁcation  DLPs  and thresholding schemes. Note
that the p-% rules from [5] were the strongest that could be obtained with their method; on complex
datasets p-% rules of 100% are rarely obtained in practice  due to their speciﬁc approximation scheme.
Employee and Customer datasets are from IBM  the others are UCI datasets.

naïve (unconstrained)

classiﬁcation
label p-% acc. prot./non-prot.

in positive

fair (constrained)
classiﬁcation [5]
p-% acc. prot./non-prot.
h

in positive

j

i

optimal
threshold
p-% at
const. acc.

l

p-%
k

g

basic statistics

dataset %prot. %prot.
in +’ve

a

b

c

%non-prot.

in +’ve

d

Income
Marketing
Credit
Employee
Customer

66.9% 30.6%
60.2% 14.1%
60.4% 24.1%
45.8% 19.2%
48.3% 33.0%

10.9%
10.1%
20.8%
12.5%
19.7%

e

35.8%
71.9%
86.0%
65.0%
59.7%

f

0.85
0.89
0.82
0.87
0.80

8% / 25%
3% / 4%
10% / 12%
8% / 12%
15% / 30%

31% 0.85
82% 0.89
88% 0.74
65% 0.86
49% 0.79

7% / 24%
3% / 3%
21% / 25%
8% / 11%
16% / 19%

29%
102%
85%
69%
84%

52.9%
100.3%
100.0%
100.4%
100.2%

5 Discussion
Coming to terms with treatment disparity. Legal considerations aside  treatment disparity ap-
proaches have three advantages over DLPs: they optimally trade accuracy for representativeness 
preserve rankings among members of each group  and do no harm to members of the disadvantaged
group. In addition  treatment disparity has another advantage: by setting class-dependent thresh-
olds  it’s easier to understand how treatment disparity impacts individuals. It seems plausible that
policy-makers could reason about thresholds to decide on the right trade-off between group equality
and individual fairness. By contrast the tuning parameters of DLPs may be harder to reason about
from a policy standpoint. Several key challenges remain. Our theoretical arguments demonstrate
that thresholding approaches are optimal in the setting where we assume complete knowledge of the
data-generating distribution. It is not always clear how best to realize these gains in practice  where
imbalanced or unrepresentative datasets can pose a signiﬁcant obstacle to accurate estimation.

Separating estimation from decision-making.
In the context of algorithmic  or algorithm-
supported decision-making  it’s often useful to obtain not just a classiﬁcation  but also an accurate
probability estimate. These estimates could then be incorporated into the decision-theoretic part of
the pipeline where appropriate measures could be taken to align decisions with social values. By
intervening at the modeling phase  DLPs distort the predicted probabilities themselves. It’s not clear
what the outputs of the resulting classiﬁers actually signify. In unconstrained learning approaches 
even if the label itself may reﬂect historical prejudice  one at least knows what is being estimated. This
leaves open the possibility of intervening at decision time to promote more equal outcomes.

Fairness beyond disparate impact How best to quantify discrimination and unfairness remains
an important open question. The CV scores and p − % rules offer one set of deﬁnitions  but there are
many other parity criteria to which our results do not directly apply  e.g.  equality of opportunity [13].
Other notions of fairness and the trade-offs between them have been studied [14  26–29]. In a recent
paper  Zafar et al. [30] depart from parity-based deﬁnitions and propose instead a preference-based
notion of fairness. Dwork et al. [11] address the problem of how best to incorporate information
about protected characteristics for several of these other fairness criteria.

Problematically  research into fairness in ML is often motivated by the case in which our ground-truth
data is tainted  capturing existing discriminatory patterns. Characterizing different forms of data bias 
how to detect them  and how to draw valid inference from such data remain important outstanding
challenges.

Even in settings where treatment disparity in favor of disadvantaged groups is an acceptable solution 
questions remain of “how”  “how much?” and “when?”. While in some cases treatment disparity
may arguably be correcting for omitted variable bias historical discrimination  in other settings it
may be viewed as itself a form of discrimination. For example  in the United States  Asian students
are simultaneously over-represented and discriminated against in higher education [2]. Such policy
judgments require a keen understanding and awareness of the social and historical context in which
the algorithms are developed and meant to operate. Recent work on identifying proxy discrimination
[31] and causal formulations of fairness [32–34] offer some promising approaches to translating such
understanding into technological solutions.

9

References
[1] Civil rights act of 1964  1964. Accessed on September 11th  2017.

[2] Anemona Hartocollis and Stepanie Saul.

Afﬁrmative action battle has a new fo-
2017. URL https://www.nytimes.com/2017/08/02/us/
cus: Asian americans.
affirmative-action-battle-has-a-new-focus-asian-americans.html?mcubz=1.

[3] Dino Pedreshi  Salvatore Ruggieri  and Franco Turini. Discrimination-aware data mining. In

KDD  2008.

[4] Toshihiro Kamishima  Shotaro Akaho  and Jun Sakuma. Fairness-aware learning through

regularization approach. In ICDM Workshops  2011.

[5] Muhammad Bilal Zafar  Isabel Valera  Manuel Gomez Rodriguez  and Krishna P Gummadi.

Fairness constraints: Mechanisms for fair classiﬁcation. In AISTATS  2017.

[6] Faisal Kamiran and Toon Calders. Classifying without discriminating. In Computer  Control

and Communication  2009.

[7] Faisal Kamiran  Toon Calders  and Mykola Pechenizkiy. Discrimination aware decision tree

learning. In ICDM  2010.

[8] Pauline Kim. Auditing algorithms for discrimination. 2017.

[9] Pauline T Kim. Data-driven discrimination at work. William & Mary Law Review  58(3)  2017.

[10] Sam Corbett-Davies  Emma Pierson  Avi Feller  Sharad Goel  and Aziz Huq. Algorithmic

decision making and the cost of fairness. arXiv preprint arXiv:1701.08230  2017.

[11] Cynthia Dwork  Nicole Immorlica  Adam Tauman Kalai  and Max Leiserson. Decoupled

classiﬁers for fair and efﬁcient machine learning. arXiv preprint arXiv:1707.06613  2017.

[12] Yahav Bechavod and Katrina Ligett. Learning fair classiﬁers: A regularization-inspired ap-

proach. arXiv preprint arXiv:1707.00044  2017.

[13] Moritz Hardt  Eric Price  Nati Srebro  et al. Equality of opportunity in supervised learning. In

NIPS  2016.

[14] Ya’acov Ritov  Yuekai Sun  and Ruofei Zhao. On conditional parity as a notion of non-

discrimination in machine learning. arXiv preprint arXiv:1706.08519  2017.

[15] Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard Zemel. Fairness

through awareness. In Innovations in Theoretical Computer Science Conference  2012.

[16] Faisal Kamiran and Toon Calders. Data preprocessing techniques for classiﬁcation without

discrimination. Knowledge and Information Systems  33(1):1–33  2012.

[17] Michael Feldman  Sorelle A Friedler  John Moeller  Carlos Scheidegger  and Suresh Venkata-

subramanian. Certifying and removing disparate impact. In KDD  2015.

[18] Philip Adler  Casey Falk  Sorelle A Friedler  Gabriel Rybeck  Carlos Scheidegger  Brandon
Smith  and Suresh Venkatasubramanian. Auditing black-box models by obscuring features.
arXiv preprint arXiv:1602.07043  2016.

[19] James E Johndrow and Kristian Lum. An algorithm for removing sensitive information:
application to race-independent recidivism prediction. arXiv preprint arXiv:1703.04957  2017.

[20] Rich Zemel  Yu Wu  Kevin Swersky  Toni Pitassi  and Cynthia Dwork. Learning fair representa-

tions. In ICML  2013.

[21] Aditya Menon and Robert Williamson. The cost of fairness in binary classiﬁcation. In Fairness 

Accountability and Transparency  2018.

[22] Ron Kohavi. Scaling up the accuracy of naive-bayes classiﬁers: a decision-tree hybrid. In KDD 

1996.

10

[23] S. Moro  P. Cortez  and P. Rita. A data-driven approach to predict the success of bank telemar-

keting. Decision Support Systems  2014.

[24] I. C. Yeh and C. H. Lien. The comparisons of data mining techniques for the predictive accuracy

of probability of default of credit card clients. Expert Systems with Applications  2009.

[25] IBM Watson analytics blog.

watson-analytics-blog/.

https://www.ibm.com/communities/analytics/

[26] Matthew Joseph  Michael Kearns  Jamie Morgenstern  Seth Neel  and Aaron Roth. Rawlsian

fairness for machine learning. arXiv preprint arXiv:1610.09559  2016.

[27] Jon Kleinberg  Sendhil Mullainathan  and Manish Raghavan. Inherent trade-offs in the fair

determination of risk scores. arXiv preprint arXiv:1609.05807  2016.

[28] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. Big Data  2017.

[29] Richard Berk  Hoda Heidari  Shahin Jabbari  Michael Kearns  and Aaron Roth. Fairness in
criminal justice risk assessments: The state of the art. arXiv preprint arXiv:1703.09207  2017.

[30] Muhammad Bilal Zafar  Isabel Valera  Manuel Gomez Rodriguez  Krishna P Gummadi  and
Adrian Weller. From parity to preference-based notions of fairness in classiﬁcation. arXiv
preprint arXiv:1707.00010  2017.

[31] Anupam Datta  Matt Fredrikson  Gihyuk Ko  Piotr Mardziel  and Shayak Sen. Proxy non-

discrimination in data-driven systems. arXiv preprint arXiv:1707.08120  2017.

[32] Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. arXiv preprint arXiv:1705.10378 

2017.

[33] Niki Kilbertus  Mateo Rojas-Carulla  Giambattista Parascandolo  Moritz Hardt  Dominik
Janzing  and Bernhard Schölkopf. Avoiding discrimination through causal reasoning. arXiv
preprint arXiv:1706.02744  2017.

[34] Matt J Kusner  Joshua R Loftus  Chris Russell  and Ricardo Silva. Counterfactual fairness.

arXiv preprint arXiv:1703.06856  2017.

11

,Zachary Lipton
Julian McAuley
Alexandra Chouldechova