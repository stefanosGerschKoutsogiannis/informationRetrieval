2018,Deep State Space Models for Unconditional Word Generation,Autoregressive feedback is considered a necessity for successful unconditional text generation using stochastic sequence models. However  such feedback is known to introduce systematic biases into the training process and it obscures a principle of generation: committing to global information and forgetting local nuances. We show that a non-autoregressive deep state space model with a clear separation of global and local uncertainty can be built from only two ingredients: An independent noise source and a deterministic transition function. Recent advances on flow-based variational inference can be used to train an evidence lower-bound without resorting to annealing  auxiliary losses or similar measures. The result is a highly interpretable generative model on par with comparable auto-regressive models on the task of word generation.,Deep State Space Models for

Unconditional Word Generation

Florian Schmidt

ETH Zürich

Thomas Hofmann

ETH Zürich

Department of Computer Science

Department of Computer Science

florian.schmidt@inf.ethz.ch

thomas.hofmann@inf.ethz.ch

Abstract

Autoregressive feedback is considered a necessity for successful unconditional text
generation using stochastic sequence models. However  such feedback is known
to introduce systematic biases into the training process and it obscures a principle
of generation: committing to global information and forgetting local nuances. We
show that a non-autoregressive deep state space model with a clear separation of
global and local uncertainty can be built from only two ingredients: An independent
noise source and a deterministic transition function. Recent advances on ﬂow-
based variational inference can be used to train an evidence lower-bound without
resorting to annealing  auxiliary losses or similar measures. The result is a highly
interpretable generative model on par with comparable auto-regressive models on
the task of word generation.

1

Introduction

Deep generative models for sequential data are an active ﬁeld of research. Generation of text  in
particular  remains a challenging and relevant area [HYX+17]. Recurrent neural networks (RNNs) are
a common model class  and are typically trained via maximum likelihood [BVV+15] or adversarially
[YZWY16  FGD18]. For conditional text generation  the sequence-to-sequence architecture of
[SVL14] has proven to be an excellent starting point  leading to signiﬁcant improvements across
a range of tasks  including machine translation [BCB14  VSP+17]  text summarization [RCW15] 
sentence compression [FAC+15] and dialogue systems [SSB+16]. Similarly  RNN language models
have been used with success in speech recognition [MKB+10  GJ14]. In all these tasks  generation is
conditioned on information that severely narrows down the set of likely sequences. The role of the
model is then largely to distribute probability mass within relatively constrained sets of candidates.
Our interest is  by contrast  in unconditional or free generation of text via RNNs. We take as point of
departure the shortcomings of existing model architectures and training methodologies developed
for conditional tasks. These arise from the increased challenges on both  accuracy and coverage.
Generating grammatical and coherent text is considerably more difﬁcult without reliance on an
acoustic signal or a source sentence  which may constrain  if not determine much of the sentence
structure. Moreover  failure to sufﬁciently capture the variety and variability of data may not surface
in conditional tasks  yet is a key desideratum in unconditional text generation.
The de facto standard model for text generation is based on the RNN architecture originally proposed
by [Gra13] and incorporated as a decoder network in [SVL14]. It evolves a continuous state vector 
emitting one symbol at a time  which is then fed back into the state evolution – a property that
characterizes the broader class of autoregressive models. However  even in a conditional setting 
these RNNs are difﬁcult to train without substitution of previously generated words by ground
truth observations during training  a technique generally referred to as teacher forcing [WZ89].
This approach is known to cause biases [RCAZ15a  GLZ+16] that can be detrimental to test time

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

performance  where such nudging is not available and where state trajectories can go astray  requiring
ad hoc ﬁxes like beam search [WR16] or scheduled sampling [BVJS15]. Nevertheless  teacher
forcing has been carried over to unconditional generation [BVV+15].
Another drawback of autoregressive feedback [Gra13] is in the dual use of a single source of
stochasticity. The probabilistic output selection has to account for the local variability in the next
token distribution. In addition  it also has to inject a sufﬁcient amount of entropy into the evolution of
the state space sequence  which is otherwise deterministic. Such noise injection is known to compete
with the explanatory power of autoregressive feedback mechanisms and may result in degenerate  near
deterministic models [BVV+15]. As a consequence  there have been a variety of papers that propose
deep stochastic state sequence models  which combine stochastic and deterministic dependencies 
e.g. [CKD+15  FSPW16]  or which make use of auxiliary latent variables [GSC+17]  auxiliary losses
[SATB17]  and annealing schedules [BVV+15]. No canoncial architecture has emerged so far and it
remains unclear how the stochasticity in these models can be interpreted and measured.
In this paper  we propose a stochastic sequence model that preserves the Markov structure of standard
state space models by cleanly separating the stochasticity in the state evolution  injected via a
white noise process  from the randomness in the local token generation. We train our model using
variational inference (VI) and build upon recent advances in normalizing ﬂows [RM15  KSW16]
to deﬁne rich enough stochastic state transition functions for both  generation and inference. Our
main goal is to investigate the fundamental question of how far one can push such an approach in
text generation  and to more deeply understand the role of stochasticity. For that reason  we have
used the most basic problem of text generation as our testbed: word morphology  i.e. the mechanisms
underlying the formation of words from characters. This enables us to empirically compare our
model to autoregressive RNNs on several metrics that are intractable in more complex tasks such as
word sequence modeling.

2 Model

We argue that text generation is subject to two sorts of uncertainty: Uncertainty about plausible
long-term continuations and uncertainty about the emission of the current token. The ﬁrst reﬂects
the entropy of all things considered “natural language"  the second reﬂects symbolic entropy at a
ﬁxed position that arises from ambiguity  (near-)analogies  or a lack of contextual constraints. As a
consequence  we cast the emission of a token as a fundamental trade-off between committing and
forgetting about information.

2.1 State space model

Let us deﬁne a state space model with transition function

F : Rd × Rd → Rd 

(ht  ξt) (cid:55)→ ht+1 = F (ht  ξt) 

(1)
F is deterministic  yet driven by a white noise process ξ  and  starting from some h0  deﬁnes a
homogeneous stochastic process. A local observation model P (wt|ht) generates symbols wt ∈ Σ
and is typically realized by a softmax layer with symbol embeddings.
The marginal probability of a symbol sequence w = w1:T is obtained by integrating out h = h1:T  

ξt

iid∼ N (0  I) .

(cid:90) T(cid:89)

P (w) =

p(ht|ht−1)P (wt|ht) dh .

(2)

t=1

Here p(ht|ht−1) is deﬁned implicitly by driving F with noise as we will explain in more detail
below.1In contrast to common RNN architectures  we have deﬁned F to not include an auto-regressive
input  such as wt−1  making potential biases as in teacher-forcing a non-issue. Furthermore  this
implements our assumption about the role of entropy and information for generation. The information
about the local outcome under P (wt|ht) is not considered in the transition to the next state as there is
no feedback. Thus in this model  all entropy about possible sequence continuations must arise from
the noise process ξ  which cannot be ignored in a successfully trained model.

1For ease of exposition  we assume ﬁxed length sequences  although in practice one works with end-of-

sequence tokens and variable length sequences.

2

h = h1...T from h0 and ξ via F and (iii) sample from the observation model(cid:81)T

The implied generative procedure follows directly from the chain rule. To sample a sequence
of observations we (i) sample a white noise sequence ξ = ξ1...T (ii) deterministically compute
t=1 P (wt|ht). The
remainder of this section focuses on how we can deﬁne a sufﬁciently powerful familiy of state
evolution functions F and how variational inference can be used for training.

2.2 Variational inference

Model-based variational inference (VI) allows us to approximate the marginalization in Eq. (2) by
posterior expectations with regard to an inference model q(h|w). It is easy to verify that the true
posterior obeys the conditional independences ht ⊥⊥ rest| ht−1  wt:T   which informs our design of
the inference model  cf. [FSPW16]:

q(h|w) =

q(ht|ht−1  wt:T ) .

(3)

This is to say  the previous state is a sufﬁcient summary of the past. Jensen’s inequality then directly
implies the evidence lower bound (ELBO)

T(cid:89)

t=1

(cid:20)

log P (w) ≥ Eq

log P (w|h) + log

=: L =

Lt := Eq [log P (wt|ht)] + Eq

log

p(ht|ht−1)

q(ht|ht−1  wt:T )

(cid:21)

p(h)
q(h|w)

(cid:20)

T(cid:88)

t=1

Lt

(cid:21)

(4)

(5)

(cid:20)

This is a well-known form  which highlights the per-step balance between prediction quality and
the discrepancy between the transition probabilities of the unconditioned generative and the data-
conditioned inference models [FSnPW16  CKD+15]. Intuitively  the inference model breaks down
the long range dependencies and provides a local training signal to the generative model for a single
step transition and a single output generation.
Using VI successfully for generating symbol sequences requires parametrizing powerful yet tractable
next state transitions. As a minimum requirement  forward sampling and log-likelihood computation
need to be available. Extensions of VAEs [RM15  KSW16] have shown that for non-sequential
models under certain conditions an invertible function h = f (ξ) can shape moderately complex
distributions over ξ into highly complex ones over h  while still providing the operations necessary
for efﬁcient VI. The authors show that a bound similar to Eq. (5) can be obtained by using the law of
the unconscious statistician [RM15] and a density transformation to express the discrepancy between
generative and inference model in terms of ξ instead of h

L = Eq(ξ|w)

log P (w|f (ξ)) + log

p(f (ξ))
q(ξ|w)

+ log |det Jf (ξ)|

(6)

This allows the inference model to work with an implicit latent distribution at the price of computing
the Jacobian determinant of f. Luckily  there are many choices such that this can be done in O(d)
[RM15  DSB16].

2.3 Training through coupled transition functions

We propose to use two separate transition functions Fq and Fg for the inference and the generative
model  respectively. Using results from ﬂow-based VAEs we derive an ELBO that reveals the intrinsic
coupling of both and expresses the relation of the two as a part of the objective that is determined
solely by the data. A shared transition model Fq = Fg constitutes a special case.

Two-Flow ELBO For a transition function F as in Eq. (1) ﬁx h = h∗ and deﬁne the restriction
f (ξ) = F (h  ξ)|h=h∗. We require that for any h∗  f is a diffeomorphism and thus has a differentiable
inverse. In fact  as we work with (possibly) different Fg and Fq for generation and inference  we have
restrictions fg and fq  respectively. For better readability we will omit the conditioning variable h∗
in the sequel.

3

(cid:21)

By combining the per-step decomposition in (5) with the ﬂow-based ELBO from (6)  we get (implic-
itly setting h∗ = ht−1):
Lt = Eq(ξ|w)

+ log(cid:12)(cid:12)det Jfq (ξt)(cid:12)(cid:12)(cid:21)

log P (wt|fq(ξt)) + log

(cid:20)

(7)

.

p(fq(ξt)|ht−1)
q(ξt|ht−1; wt:T )

As our generative model also uses a ﬂow to transform ξt into a distribition on ht  it is more natural to
use the (simple) density in ξ-space. Performing another change of variable  this time on the density
of the generative model  we get

p(ht|ht−1) = p(ζt|ht−1) · |det Jf

−1
g

(fq(ξt))| =

r(ζt)

|det Jfg (ζt)|   ζt := (f−1

g

◦ fq)(ξt)

(8)

where r now is simply the (multivariate) standard normal density as ξt does not depend ht−1 
whereas ht does. We have introduced new noise variable ζt = s(ξt) to highlight the importance of
◦ fq  which is a combined ﬂow of the forward inference ﬂow and the
the transformation s = f−1
inverse generative ﬂow. Essentially  it follows the suggested ξ-distribution of the inference model
into the latent state space and back into the noise space of the generative model with its uninformative
distribution. Putting this back into Eq. (7) and exploiting the fact that the Jacobians can be combined
via det Js = det Jfq /det Jfg we ﬁnally get

g

(cid:20)

(cid:21)

Lt = Eq(ξ|w)

log P (wt|fq(ξt)) + log

r(s(ξt))

q(ξt|ht−1; wt:T )

+ log |det Js(ξt)|

.

(9)

Interpretation Naïvely employing the model-based ELBO approach  one has to learn two inde-
pendently parametrized transition models p(ht|ht−1) and q(ht|ht−1  wt...T )  one informed about
the future and one not. Matching the two then becomes and integral part of the objective. However 
since the transition model encapsulates most of the model complexity  this introduces redundancy
where the learning problem is most challenging. Nevertheless  generative and inference model do
address the transition problem from very different angles. Therefore  forcing both to use the exact
same transition model might limit ﬂexibility during training and result in an inferior generative model.
Thus our model casts Fg and Fq as independently parametrized functions that are coupled through
the objective by treating them as proper transformations of an underlying white noise process. 2

Special cases Additive Gaussian noise ht+1 = ht + ξt can be seen as the simplest form of Fg
or  alternatively  as a generative model without ﬂow (as Jfg = I). Of course  repeated addition of
noise does not provide a meaningful latent trajectory. Finally  note that for Fg = Fq  s = id and the
nominator in the second term becomes a simple prior probability r(ξt)  whereas the determinant
reduces to a constant. We now explore possible candidates for the ﬂows in Fg and Fq.

2.4 Families of transition functions

F (ht−1  ξt) = g(ht−1) + G(ht−1)ξt

Since the Jacobian of a composed function factorizes  a ﬂow F is often composed of a chain of
individual invertible functions F = Fk ◦ ··· ◦ F1 [RM15]. We experiment with individual functions
(10)
where g is a multilayer MLP Rd → Rd and G is a neural network Rd → Rd × Rd mapping ht−1
to a lower-triangular d × d matrix with non-zero diagonal entries. Again  we use MLPs for this
mapping and clip the diagonal away from [−δ  δ] for some hyper parameter 0 < δ < 0.5. The
lower-triangular structure allows computing the determinant in O(d) and stable inversion of the
mapping by substitution in O(d2). As a special case we also consider the case when G is restricted to
diagonal matrices. Finally  we experiment with a conditional variant of the Real NVP ﬂow [DSB16].
Computing F −1
is central to our objective and we found that depending on the ﬂow actually
parametrizing the inverse directly results in more stable and efﬁcient training.

g

2Note that identifying s as an invertible function allows us to perform a backwards density transformation
which cancels the regularizing terms. This is akin to any ﬂow objective (e.g. see equation (15) in[RM15]) where
applying the transformation additionally to the prior cancels out the Jacobian term. We can think of s as a
stochastic bottleneck with the observation model P (wt|ht) attached to the middle layer. Removing the middle
layer collapses the bottleneck and prohibits learning compression.

4

2.5

Inference network

(cid:81) q(ht|ht−1  wt:T ) but treated it as a black-box otherwise. Remember that sampling from the

the inference network q(h|w) =
So far we have only motivated the factorization of
inference network amounts to sampling ξt ∼ q(·|ht−1  wt...T ) and then performing the deterministic
transition Fq(ht−1  ξt). We observe much better training stability when conditioning q on the data
wt...T only and modeling interaction with ht−1 exclusively through Fq. This coincides with our
intuition that the two inputs to a transition function provide semantically orthogonal contributions.
We follow existing work [DSB16] and choose q as the density of a normal distribution with diagonal
covariance matrix. We follow the idea of [FSPW16] and incorporate the variable-length sequence
wt:T by conditioning on the state of an RNN running backwards in time across w1...T . We embed
the symbols w1...T in a vector space RdE and use use a GRU cell to produce a sequence of hidden
states aT   . . .   a1 where at has digested tokens wt:T . Together ht−1 and at parametrize the mean
and co-variance matrix of q.

2.6 Optimization

Except in very speciﬁc and simple cases  for instance  a Kalman ﬁlter  it will not be possible to
efﬁciently compute the q-expectations in Eq. (5) exactly. Instead  we sample q in every time-step
as is common practice for sequential ELBOs [FSnPW16  GSC+17]. The re-parametrization trick
allows pushing all necessary gradients through these expectations to optimize the bound via stochastic
gradient-based optimization techniques such as Adam [KB14].

2.7 Extension: Importance-weighted ELBO for tracking the generative model

Conceptionally  there are two ways we can imagine an inference network to propose ξ1:T sequences
for a given sentence w1:T . Either  as described above  by digesting w1...T right-to-left and proposing
ξ1:T left-to-right. Or  by iteratively proposing a ξt taking into account the last state ht−1 proposed
and the generative deterministic mechanism Fg. The latter allows the inference network to peek at
states ht that Fg could generate from ht−1 before proposing an actual target ht. This allows the
inference model to track a multi-modal Fg without need for Fq to match its expressiveness. As a
consequence  this might offer the possibility to learn multi-modal generative models  without the
need to employ complex multi-modal distributions in the inference model.
Our extension is built on importance weighted auto-encoders (IWAE) [BGS15]. The IWAE ELBO is
derived by writing the log marginal as a Monte Carlo estimate before using Jensen’s inequality. The
result is an ELBO and corresponding gradients of the form3

(cid:34)

L = E

h(k)

log

1
K

(cid:35)

K(cid:88)

k=1

(cid:124)

p(w  h(k))
q(h(k)|w)

(cid:123)(cid:122)

(cid:125)

=: ω(k)

(cid:34) K(cid:88)

k=1

ω(k)(cid:80)

k(cid:48) ω(k(cid:48))

(cid:35)

  ∇L = E

h(k)

∇ log ω(k)

  h(k)∼ q(·|w) (11)

The authors motivate (11) as a weighting mechanism relieving the inference model from explaining
the data well with every sample. We will use the symmetry of this argument to let the inference
model condition on potential next states hg t = Fg(ht−1  ξt)  ξt∼N (0  I) from the generative model
without requiring every hg t to allow q to make a good proposal. In other words  the K sampled
outputs of Fg become a vectorized representation of Fg to condition on. In our sequential model 
computing ω(k) exactly is intractable as it would require rolling out the network until time T . Instead 
we limit the horizon to only one time-step. Although this biases the estimate of the weights and
consequently the ELBO  longer horizons did empirically not show beneﬁts. When proceeding to
time-step t + 1 we choose the new hidden state by sampling h(k) with probability proportionally to
ω(k). Algorithm 1 summarizes the steps carried out at time t for a given ht−1 (to not overload the
notation  we drop t in hg t) and a more detailed derivation of the bound is given in Appendix A.

3Here we have tacitly assumed that h can be rewritten using the reprametrization trick so that the expectation
can be expressed with respect to some parameter-free base-distribution. See [BGS15] for a detailed derivation of
the gradients in (11).

5

Algorithm 1 Detailed forward pass with importance weighting

Simulate Fg:
Instantiate the inference family:
Sample inference:
Compute gradients as in (11) where ω(k) = P (wt|h(k))p(h(k)|ht−1)/qk(h(k))
Sample h(k) according to ω(1) . . . ω(K) for the next step.

g = Fg(ht−1  ξ(k))  where ξ(k) ∼ N (0  I)  k = 1  . . .   K
h(k)
qk(h) = q(h|h(k)
h(k) ∼ qk

g   ht−1  wt:T )

3 Related Work

Our work intersects with work directly addressing teacher-forcing  mostly on language modelling
and translation (which are mostly not state space models) and stochastic state space models (which
are typically autoregressive and do not address teacher forcing).
Early work on addressing teacher-forcing has focused on mitigating its biases by adapting the RNN
training procedure to partly rely on the model’s prediction during training [BVJS15  RCAZ15b].
Recently  the problem has been addressed for conditional generation within an adversarial framework
[GLZ+16] and in various learning to search frameworks [WR16  LAOL17]. However  by design
these models do not perform stochastic state transitions.
There have been proposals for hybrid architectures that augment the deterministic RNN state se-
quences by chains of random variables [CKD+15  FSPW16]. However  these approaches are largely
patching-up the output feedback mechanism to allow for better modeling of local correlations  leaving
the deterministic skeleton of the RNN state sequence untouched. A recent evolution of deep stochas-
tic sequence models has developed models of ever increasing complexity including intertwined
stochastic and deterministic state sequences [CKD+15  FSPW16] additional auxiliary latent variables
[GSC+17] auxiliary losses [SATB17] and annealing schedules [BVV+15]. At the same time  it
remains often unclear how the stochasticity in these models can be interpreted and measured.
Closest in spirit to our transition functions is work by Maximilian et al.[KSBvdS17] on generation
with external control inputs. In contrast to us they use a simple mixture of linear transition functions
and work around using density transformations akin to [BO14]. In our unconditional regime we
found that relating the stochasticity in ξ explicitly to the stochasticity in h is key to successful training.
Finally  variational conditioning mechanisms similar in spirit to ours have seen great success in image
generation[GDGW15].
Among generative unconditional sequential models GANs are as of today the most prominent
architecture [YZWY16  JKMHL16  FGD18  CLZ+17]. To the best of our knowledge  our model is
the ﬁrst non-autoregressive model for sequence generation in a maximum likelihood framework.

4 Evaluation

Naturally  the quality of a generative model must be measured in terms of the quality of its outputs.
However  we also put special emphasis on investigating whether the stochasticity inherent in our
model operates as advertised.

4.1 Data Inspection

Evaluating generative models of text is a ﬁeld of ongoing research and currently used methods
range from simple data-space statistics to expensive human evaluation [FGD18]. We argue that
for morphology  and in particular non-autoregressive models  there is an interesting middle ground:
Compared to the space of all sentences  the space of all words has still moderate cardinality which
allows us to estimate the data distribution by unigram word-frequencies. As a consequence  we can
reliably approximate the cross-entropy which naturally generalizes data-space metrics to probabilistic
models and addresses both  over-generalization (assigning non-zero probability to non-existing words)
and over-conﬁdence (distributing high probability mass only among a few words).
This metric can be addressed by all models which operate by ﬁrst stochastically generating a sequence
of hidden states and then deﬁning a distribution over the data-space given the state sequence. For our

6

(cid:90)

K(cid:88)

k=1

model we approximate the marginal by a Monte Carlo estimate of (2)

P (w) =

P (w|h)p(h)dh =

1
K

P (w|h(k))  h(k) ∼ p(h)

(12)

Note that sampling from p(h) boils down to sampling ξ1...T from independent standard normals and
then applying Fg. In particular  the non-autoregressive property of our model allows us to estimate
all words in some set S using K samples each by using only K independent trajectories h overall.
Finally  we include two data-space metrics as an intuitive  yet less accurate measure. From a collection
of generated words  we estimate (i) the fraction of words that are in the training vocabulary (w ∈ V )
and (ii) the fraction of unique words that are in the training vocabulary (w ∈ V unique).4

4.2 Entropy Inspection

We want to go beyond the usual evaluation of existing work on stochastic sequence models and
also assess the quality of our noise model. In particular  we are interested in how much information
contained in a state ht about the output P (wt|ht) is due to the corresponding noise vector ξt. This is
quantiﬁed by the mutual information between the noise ξt and the observation wt given the noise
ξ1:t−1 that deﬁned the preﬁx up to time t. Since ht−1 is a deterministic function of ξ1:t−1  we write

I(t) = I(wt; ξt|ht−1) = Eht−1

H[wt|ht−1] − H[wt|ξt  ht−1]

≥ 0

(13)

(cid:20)

(cid:21)

to quantify the dependence between noise and observation at one time-step. For a model ignoring the
noise variables  knowledge of ξt does not reduce the uncertainty about wt  so that I(t) = 0. We can
use Monte Carlo estimates for all expectations in (13).

5 Experiments

5.1 Dataset and baseline

For our experiments  we use the BooksCorpus [KZS+15  ZKZ+15]  a freely available collection of
novels comprising of almost 1B tokens out of which 1.3M are unique. To ﬁlter out artefacts and some
very uncommon words found in ﬁction  we restrict the vocabulary to words of length 2 ≤ l ≤ 12 with
at least 10 occurrences that only contain letters resulting in a 143K vocabulary. Besides the standard
10% test-train split at the word level  we also perform a second  alternative split at the vocabulary
level. That means  10 percent of the words  chosen regardless of their frequency  will be unique to
the test set. This is motivated by the fact that even a small test-set under the former regime will result
in only very few  very unlikely words unique to the test-set. However  generalization to unseen words
is the essence of morphology. As an additional metric to measuring generalization in this scenario 
we evaluate the generated output under Witten-Bell discounted character n-gram models trained on
either the whole corpus or the test data only.
Our baseline is a GRU cell and the standard RNN training procedure with teacher-forcing5. Hidden
state size and embedding size are identical to our model’s.

5.2 Model parametrization

We stick to a standard softmax observation model and instead focus the model design on different
combinations of ﬂows for Fg and Fq. We investigate the ﬂow in Equation (10)  denoted as TRIL  its
diagonal version DIAG and a simple identity ID. We denote repeated application of (independently
parametrized) ﬂows as in 2 × TRIL. For the weighted version we use K ∈ {2  5  10} samples.
In addition  for Fg we experiment with a sequence of Real NVPs with masking dimensions d =
2 . . . 7 (two internal hidden layers of size 8 each). Furthermore  we investigate deviating from the
factorization (3) by using a bidirectional RNN conditioning on all w1...T in every timestep. Finally 
for the best performing conﬁguration  we also investigate state-sizes d = {16  32}.

4Note that for both data-space metrics there is a trivial generation system that achieves a ‘perfect’ score.

Hence  both must be taken into account at the same time to judge performance.

5It should be noted that despite the greatly reduced vocabulary in character-level generation  RNN training

without teacher-forcing for our data still fails miserably.

7

5.3 Results
Table 1 shows the result for the standard split. By ± we indicate mean and standard deviation across
5 or 10 (for IWAE) identical runs6. The data-space metrics require manually trading off precision
and coverage. We observe that two layers of the TRIL ﬂow improve performance. Furthermore 
importance weighting signiﬁcantly improves the results across all metrics with diminishing returns at
K = 10. Its effectiveness is also conﬁrmed by an increase in variance across the weights ω1 . . . ωT
during training which can be attributed to the signiﬁcance of the noise model (see 5.4 for more
details). We found training with REAL-NVP to be very unstable. We attribute the relatively poor
performance of NVP to the sequential VI setting which deviates heavily from what it was designed
for and keep adaptions for future work.

Model
TRIL
TRIL  K=2
TRIL  K=5
TRIL  K=10
2×TRIL
2×TRIL  K=2
2×TRIL  K=5
2×TRIL  K=10
2×TRIL  K=10  BIDI
d = 16 2×TRIL  K=10
d = 32 2×TRIL  K=10
REAL-NVP-[2 3 4 5 6 7]
BASELINE-8D
BASELINE-16D
ORACLE-TRAIN

H[Ptrain  ˆP ] H[Ptest  ˆP ] w ∈ V unique w ∈ V
12.13±.11
11.76±.12
11.46±.05
11.43±.05
11.91±.08
11.55±.09
11.42±.07
11.33±.05
11.33±.09
11.21
11.27
11.77
12.92
12.55
7.0

11.99±.11
11.82±.12
11.51±.05
11.47±.05
11.86±.13
11.61±.09
11.46±.06
11.38±.06
11.39±.10
11.43
11.13
11.81
12.97
12.60
7.027

0.18±.00
0.16±.01
0.16±.01
0.16±.01
0.17±.01
0.16±.00
0.16±.00
0.16±.00
0.16±.01
0.15
0.15
0.12
0.13
0.14
0.27

0.43±.03
0.46±.02
0.48±.02
0.49±.02
0.45±.02
0.47±.01
0.49±.01
0.49±.01
0.48±.00
0.48
0.50
0.53
0.53
0.62
1.0

¯I
0.95±.04
1.06±.16
1.08±.13
1.12±.12
0.89±.07
1.00±.13
1.20±.12
1.28±.13
1.25±.16
1.43
1.31
0.94
–
–
–

Table 1: Results on generation. The cross entropy is computed wrt. both training and test set.
ORACLE-TRAIN is a model sampling from the training data.

Interestingly  our standard inference model is on par with the equivalently parametrized bidirectional
inference model suggesting that historic information can be sufﬁciently stored in the states and
conﬁrming d-separation as the right principle for inference design.
The poor cross-entropy achieved by the baseline can partly be explained by the fact that auto-
regressive RNNs are trained on conditional next-word-predictions. Estimating the real data-space
distribution would require aggregating over all possible sequences w ∈ V T . However  the data-space
metrics clearly show that the performance cannot solely be attributed to this.
Table 2 shows that generalization for the alternative split is indeed harder but cross entropy results
carry over from the standard setting. Here we sample trajectories and extract the argmax from the
observation model which resembles more closely the procedure of the baseline. Under n-gram
perplexity both models are on par with a slight advantage of the baseline on longer n-grams and
slightly better generalization of our proposed model.

Model
2×TRIL  K=10
BASELINE-8D
ORACLE-TRAIN
ORACLE-TEST

H[Ptrain  ˆP ] H[Ptest  ˆP ]
11.56
12.90
–
–

12.27
13.67
–
–

n-gram from train+test
P5
30.7
24.8
4.1
3.9

P4
20.9
17.5
4.8
4.5

P3
12.8
12.1
6.7
6.0

P2
10.4
11.4
10.1
9.5

n-gram from test

P2
13.1
14.5
13.2
7.9

P3
21.9
22.7
15.7
4.1

P4
49.6
48.3
21.4
2.9

P5
81.1
80.5
26.4
2.6

Table 2: Results for the alternative data split: Cross entropy and perplexity under n = 2  3  4  5-gram
language models estimated on either the full corpus or the test set only.

To give more insight into how the transition functions inﬂuence the results  Table 1a presents an
exhaustive overview for all combinations of our simple ﬂows. We observe that a powerful generative
6Single best model with d = 8: 2 × TRIL  K = 10 achieved H[Ptrain  ˆP ] = 11.26 and H[Ptest  ˆP ] = 11.28.
7Note that the training-set oracle is not optimal for the test set. The entropy of the test set is 6.80.

8

ﬂow is essential for successful models while the inference ﬂow can remain relatively simple – yet
simplistic choices  such as ID degrade performance. Choosing Fg slightly more powerful than Fq
emerges as a successful pattern.

Flow Fq

DIAG

TRIL

2×TRIL

g
F
w
o
l
F

ID

ID 14.23±.00 14.23±.00 14.23±.00 –
DIAG 12.82±.37 12.35±.37 12.20±.25 –
TRIL 13.55±.01 11.99±.11 –
–
11.86±.13 –
–
(a) Test cross entropy H[Ptest  ˆP ]

2×TRIL –

Flow Fq

ID

TRIL

0±.00

DIAG

0±.00

2×TRIL
0±.00
–
0.93±.15 0.85±.16 0.92±.13 –
0.65±.01 0.95±.04 –
–
0.89±.07 –
–
–
(b) Average mutual information ¯I

Table 3: Results for different combinations of ﬂows driving generative and inference transitions.
A bar indicates combinations that did not allow for stable training. We also report ID for Fg for
completeness but note that it is by design unsiuted for this contextual setting.

5.4 Noise Model Analysis

We use K = 20 samples to approximate the entropy terms in (13). In addition we denote by ¯I the
average mutual information across all time-steps. Figure 3 shows how ¯I along with the symbolic
entropy H[wt|ht] changes during training. Remember that in a non-autoregressive model  the latter
corresponds to information that cannot be recovered in later timesteps. Over the course of the training 
more and more information is driven by ξt and absorbed into states ht where it can be stored.
Figures 1 and 1b show ¯I for all trained models. In addition  Figure 3 shows a box-plot of I(t) for each
t = 1 . . . T for the conﬁguration 2×TRIL  K=10. As initial tokens are more important to remember 
it should not come as a surprise that I(t) is largest ﬁrst and decreases over time  yet with increased
variance.

2

s
t
i
b
1

0

1

2

4

8
3
word position t = 1 . . . T

5

6

7

4

s
t
2
i
b

0

¯I
H[wt|ht]
baseline

9

10

training time

Figure 2: Noise mutual information I(t) over
sequence position t = 1 . . . T .

Figure 3: Entropy analysis over training time.
For reference the dashed line indicates the
overall word entropy of the trained baseline.

6 Conclusion

In this paper we have shown how a deep state space model can be deﬁned and trained with the help
of variational ﬂows. The recurrent mechanism is driven purely by a simple white noise process
and does not require an autoregressive conditioning on previously generated symbols. In addition 
we have shown how an importance-weighted conditioning mechanism integrated into the objective
allows shifting stochastic complexity from the inference to the generative model. The result is a
highly ﬂexible framework for sequence generation with an extremely simple overall architecture  a
measurable notion of latent information and no need for pre-training  annealing or auxiliary losses. We
believe that pushing the boundaries of non-autoregressive modeling is key to understanding stochastic
text generation and can open the door to related ﬁelds such as particle ﬁltering [NLRB17  MLT+17].

9

References
[BCB14]

[BGS15]

[BO14]

[BVJS15]

[BVV+15]

[CKD+15]

[CLZ+17]

[DSB16]

[FAC+15]

[FGD18]

Dzmitry Bahdanau  Kyunghyun Cho  and Yoshua Bengio. Neural machine translation
by jointly learning to align and translate. CoRR  abs/1409.0473  2014. 1
Yuri Burda  Roger B. Grosse  and Ruslan Salakhutdinov. Importance weighted autoen-
coders. CoRR  abs/1509.00519  2015. 2.7  3  A
Justin Bayer and Christian Osendorfer. Learning stochastic recurrent networks. arXiv
preprint arXiv:1411.7610  2014. arXiv. 3
Samy Bengio  Oriol Vinyals  Navdeep Jaitly  and Noam Shazeer. Scheduled sampling
for sequence prediction with recurrent neural networks. CoRR  abs/1506.03099  2015.
1  3
Samuel R. Bowman  Luke Vilnis  Oriol Vinyals  Andrew M. Dai  Rafal Józefowicz  and
Samy Bengio. Generating sentences from a continuous space. CoRR  abs/1511.06349 
2015. 1  1  1  1  3
Junyoung Chung  Kyle Kastner  Laurent Dinh  Kratarth Goel  Aaron C Courville  and
Yoshua Bengio. A recurrent latent variable model for sequential data. In NIPS  pages
2980–2988  2015. 1  2.2  3  3
Tong Che  Yanran Li  Ruixiang Zhang  R. Devon Hjelm  Wenjie Li  Yangqiu Song 
and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial
networks. CoRR  abs/1702.07983  2017. 3
Laurent Dinh  Jascha Sohl-Dickstein  and Samy Bengio. Density estimation using real
NVP. CoRR  abs/1605.08803  2016. 2.2  2.4  2.5
Katja Filippova  Enrique Alfonseca  Carlos A Colmenares  Lukasz Kaiser  and Oriol
In EMNLP 2015  pages
Vinyals. Sentence compression by deletion with lstms.
360–368  2015. 1
William Fedus  Ian J. Goodfellow  and Andrew M. Dai. Maskgan: Better text generation
via ﬁlling in the ______. CoRR  abs/1801.07736  2018. 1  3  4.1

[FSnPW16] Marco Fraccaro  Søren Kaae Sø nderby  Ulrich Paquet  and Ole Winther. Sequential
neural models with stochastic layers. pages 2199–2207. Curran Associates  Inc.  2016.
2.2  2.6

[FSPW16] Marco Fraccaro  Søren Kaae Sønderby  Ulrich Paquet  and Ole Winther. Sequential
neural models with stochastic layers. pages 2199–2207  2016. NIPS. 1  2.2  2.5  3  3
[GDGW15] Karol Gregor  Ivo Danihelka  Alex Graves  and Daan Wierstra. DRAW: A recurrent

neural network for image generation. CoRR  abs/1502.04623  2015. 3
Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent
neural networks. In ICML 2014  pages 1764–1772  2014. 1

[GJ14]

[GLZ+16] Anirudh Goyal  Alex Lamb  Ying Zhang  Saizheng Zhang  Aaron C. Courville  and
Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks.
In NIPS 2016  pages 4601–4609  2016. 1  3
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850  2013. 1  1

[Gra13]

[GSC+17] Anirudh Goyal  Alessandro Sordoni  Marc-Alexandre Côté  Nan Rosemary Ke  and
Yoshua Bengio. Z-forcing: Training stochastic recurrent networks. In NIPS 2017 
pages 6716–6726  2017. 1  2.6  3

[HYX+17] Z. Hu  Z. Yang  Liang X.  R. Salakhutdinov  and E. R. Xing. Toward controlled
generation of text. In International Conference on Machine Learning (ICML)  2017. 1
[JKMHL16] Matt J. Kusner and José Miguel Hernández-Lobato. Gans for sequences of discrete

elements with the gumbel-softmax distribution. 11 2016. 3
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
CoRR  abs/1412.6980  2014. 2.6

[KB14]

[KSBvdS17] Maximilian Karl  Maximilian Soelch  Justin Bayer  and Patrick van der Smagt. Deep
variational bayes ﬁlters: Unsupervised learning of state space models from raw data.
arXiv preprint arXiv:1605.06432  2017. ICLR. 3

10

[KSW16]

[KZS+15]

[LAOL17]

Diederik P. Kingma  Tim Salimans  and Max Welling. Improving variational inference
with inverse autoregressive ﬂow. CoRR  abs/1606.04934  2016. 1  2.2
Ryan Kiros  Yukun Zhu  Ruslan Salakhutdinov  Richard S Zemel  Antonio Tor-
ralba  Raquel Urtasun  and Sanja Fidler. Skip-thought vectors. arXiv preprint
arXiv:1506.06726  2015. 5.1
Rémi Leblond  Jean-Baptiste Alayrac  Anton Osokin  and Simon Lacoste-Julien.
SEARNN: training rnns with global-local losses. CoRR  abs/1706.04499  2017. 3

[MKB+10] Tomáš Mikolov  Martin Karaﬁát  Lukáš Burget  Jan ˇCernock`y  and Sanjeev Khudanpur.

Recurrent neural network based language model. In INTERSPEECH 2010  2010. 1

[MLT+17] Chris J. Maddison  Dieterich Lawson  George Tucker  Nicolas Heess  Mohammad
Norouzi  Andriy Mnih  Arnaud Doucet  and Yee Whye Teh. Filtering variational
objectives. CoRR  abs/1705.09279  2017. 6

[NLRB17] Christian A Naesseth  Scott W Linderman  Rajesh Ranganath  and David M Blei.

Variational sequential monte carlo. arXiv preprint arXiv:1705.11140  2017. 6

[RCAZ15a] Marc’Aurelio Ranzato  Sumit Chopra  Michael Auli  and Wojciech Zaremba. Sequence
level training with recurrent neural networks. arXiv preprint arXiv:1511.06732  2015.
1

[RCAZ15b] Marc’Aurelio Ranzato  Sumit Chopra  Michael Auli  and Wojciech Zaremba. Sequence

[RCW15]

[RM15]

[SATB17]

[SSB+16]

[SVL14]

[VSP+17]

[WR16]

[WZ89]

level training with recurrent neural networks. CoRR  abs/1511.06732  2015. 3
Alexander M. Rush  Sumit Chopra  and Jason Weston. A neural attention model for
abstractive sentence summarization. CoRR  abs/1509.00685  2015. 1
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing
ﬂows. In ICML 2015  pages 1530–1538  2015. 1  2.2  2.2  2.2  2.4  2
Samira Shabanian  Devansh Arpit  Adam Trischler  and Y Bengio. Variational bi-lstms.
11 2017. 1  3
Iulian Vlad Serban  Alessandro Sordoni  Yoshua Bengio  Aaron C Courville  and Joelle
Pineau. Building end-to-end dialogue systems using generative hierarchical neural
network models. In AAAI  volume 16  pages 3776–3784  2016. 1
Ilya Sutskever  Oriol Vinyals  and Quoc V. Le. Sequence to sequence learning with
neural networks. In NIPS 2017  NIPS’14  pages 3104–3112  Cambridge  MA  USA 
2014. MIT Press. 1  1
Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N.
Gomez  Lukasz Kaiser  and Illia Polosukhin. Attention is all you need. CoRR 
abs/1706.03762  2017. 1
Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search
optimization. CoRR  abs/1606.02960  2016. 1  3
Ronald J Williams and David Zipser. A learning algorithm for continually running
fully recurrent neural networks. Neural computation  1(2):270–280  1989. 1

[YZWY16] Lantao Yu  Weinan Zhang  Jun Wang  and Yong Yu. Seqgan: Sequence generative

adversarial nets with policy gradient. CoRR  abs/1609.05473  2016. 1  3

[ZKZ+15] Yukun Zhu  Ryan Kiros  Richard Zemel  Ruslan Salakhutdinov  Raquel Urtasun  Anto-
nio Torralba  and Sanja Fidler. Aligning books and movies: Towards story-like visual
explanations by watching movies and reading books. arXiv preprint arXiv:1506.06724 
2015. 5.1

11

,Florian Schmidt
Thomas Hofmann