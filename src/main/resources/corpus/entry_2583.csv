2018,Improved Network Robustness with Adversary Critic,Ideally  what confuses neural network should be confusing to humans. However  recent experiments have shown that small  imperceptible perturbations can change the network prediction. To address this gap in perception  we propose a novel approach for learning robust classifier. Our main idea is: adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target. We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN)  where the adversarial attack on classifier acts as a generator  and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping  we introduce adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. In the experiments  we show the effectiveness of our defense. Our method surpasses in terms of robustness networks trained with adversarial training. Additionally  we verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing.,Improved Network Robustness

with Adversary Critic

Alexander Matyasko  Lap-Pui Chau

School of Electrical and Electronic Engineering
Nanyang Technological University  Singapore

aliaksan001@ntu.edu.sg  elpchau@ntu.edu.sg

Abstract

Ideally  what confuses neural network should be confusing to humans. However 
recent experiments have shown that small  imperceptible perturbations can change
the network prediction. To address this gap in perception  we propose a novel
approach for learning robust classiﬁer. Our main idea is: adversarial examples
for the robust classiﬁer should be indistinguishable from the regular data of the
adversarial target. We formulate a problem of learning robust classiﬁer in the
framework of Generative Adversarial Networks (GAN)  where the adversarial
attack on classiﬁer acts as a generator  and the critic network learns to distinguish
between regular and adversarial images. The classiﬁer cost is augmented with
the objective that its adversarial examples should confuse the adversary critic. To
improve the stability of the adversarial mapping  we introduce adversarial cycle-
consistency constraint which ensures that the adversarial mapping of the adversarial
examples is close to the original. In the experiments  we show the effectiveness
of our defense. Our method surpasses in terms of robustness networks trained
with adversarial training. Additionally  we verify in the experiments with human
annotators on MTurk that adversarial examples are indeed visually confusing.

1

Introduction

Deep neural networks are powerful representation learning models which achieve near-human
performance in image [1] and speech [2] recognition tasks. Yet  state-of-the-art networks are sensitive
to small input perturbations. [3] showed that adding adversarial noise to inputs produces images
which are visually similar to the original inputs but which the network misclassiﬁes with high
conﬁdence. In speech recognition  [4] introduced an adversarial attack  which can change any audio
waveform  such that the corrupted signal is over 99.9% similar to the original but transcribes to any
targeted phrase. The existence of adversarial examples puts into question generalization ability of
deep neural networks  reduces model interpretability  and limits applications of deep learning in
safety and security-critical environments [5  6].
Adversarial training [7  8  9] is the most popular approach to improve network robustness. Adversarial
examples are generated online using the latest snapshot of the network parameters. The generated
adversarial examples are used to augment training dataset. Then  the classiﬁer is trained on the
mixture of the original and the adversarial images. In this way  adversarial training smoothens a
decision boundary in the vicinity of the training examples. Adversarial training (AT) is an intuitive
and effective defense  but it has some limitations. AT is based on the assumption that adversarial
noise is label non-changing. If the perturbation is too large  the adversarial noise may change the true
underlying label of the input. Secondly  adversarial training discards the dependency between the
model parameters and the adversarial noise. As a result  the neural network may fail to anticipate
changes in the adversary and overﬁt the adversary used during training.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

F1

x1

ˆx1

x2

ˆx2

F2

Figure 1: Adversarial examples should be indistinguishable from the regular data of the adversarial
target. The images in the ﬁgure above are generated using Carlini and Wagner [10] l2-attack on the
network trained with our defense  such that the conﬁdence of the prediction on the adversarial images
is 95%. The conﬁdence on the original images x1 and x2 is 99%.

Ideally  what confuses neural network should be confusing to humans. So the changes introduced by
the adversarial noise should be associated with removing identifying characteristics of the original
label and adding identifying characteristics of the adversarial label. For example  images that are
adversarial to the classiﬁer should be visually confusing to a human observer. Current techniques [7 
8  9] improve robustness to input perturbations from a selected uncertainty set. Yet  the model’s
adversarial examples remain semantically meaningless. To address this gap in perception  we propose
a novel approach for learning robust classiﬁer. Our core idea is that adversarial examples for the robust
classiﬁer should be indistinguishable from the regular data of the attack’s target class (see ﬁg. 1).
We formulate the problem of learning robust classiﬁer in the framework of Generative Adversarial
Networks (GAN) [11]. The adversarial attack on the classiﬁer acts as a generator  and the critic
network learns to distinguish between natural and adversarial images. We also introduce a novel
targeted adversarial attack which we use as the generator. The classiﬁer cost is augmented with the
objective that its adversarial images generated by the attack should confuse the adversary critic. The
attack is fully-differentiable and implicitly depends on the classiﬁer parameters. We train the classiﬁer
and the adversary critic jointly with backpropagation. To improve the stability of the adversarial
mapping  we introduce adversarial cycle-consistency constraint which ensures that the adversarial
mapping of the adversarial examples is close to the original. Unlike adversarial training  our method
does not require adversarial noise to be label non-changing. To the contrary  we require that the
changes introduced by adversarial noise should change the “true” label of the input to confuse the
critic. In the experiments  we demonstrate the effectiveness of the proposed approach. Our method
surpasses in terms of robustness networks trained with adversarial training. Additionally  we verify
in the experiments with human annotators that adversarial examples are indeed visually confusing.

2 Related work

Adversarial attacks
Szegedy et al. [3] have originally introduced a targeted adversarial attack
which generates adversarial noise by optimizing the likelihood of input for some adversarial target
using a box-constrained L-BFGS method. Fast Gradient Sign method (FGSM) [7] is a one-step
attack which uses a ﬁrst-order approximation of the likelihood loss. Basic Iterative Method (BIM) 
which is also known as Projected Gradient Descent (PGD)  [12] iteratively applies the ﬁrst-order
approximation and projects the perturbation after each step. [6] propose an iterative method which at
each iteration selects a single most salient pixel and perturbs it. DeepFool [13] iteratively generates
adversarial perturbation by taking a step in the direction of the closest decision boundary. The decision
boundary is approximated with ﬁrst-order Taylor series to avoid complex non-convex optimization.
Then  the geometric margin can be computed in the closed-form. Carlini and Wagner [10] propose
an optimization-based attack on a modiﬁed loss function with implicit box-constraints. [14] intro-
duce a black-box adversarial attack based on transferability of adversarial examples. Adversarial
Transformation Networks (ATN) [15] trains a neural network to attack.

2

Defenses against adversarial attacks Adversarial training (AT) [7] augments training batch with
adversarial examples which are generated online using Fast Gradient Sign method. Virtual Adversarial
training (VAT) [16] minimizes Kullback-Leibler divergence between the predictive distribution of
clean inputs and adversarial inputs. Notably  adversarial examples can be generated without using
label information and VAT was successfully applied in semi-supervised settings. [17] applies iterative
Projected Gradient Descent (PGD) attack to adversarial training. Stability training [18] minimizes a
task-speciﬁc distance between the output on clean and the output on corrupted inputs. However  only
a random noise was used to distort the input. [19  20] propose to maximize a geometric margin to
improve classiﬁer robustness. Parseval networks [21] are trained with the regularization constraint 
so the weight matrices have a small spectral radius. Most of the existing defenses are based on robust
optimization and improve the robustness to perturbations from a selected uncertainty set.
Detecting adversarial examples is an alternative way to mitigate the problem of adversarial examples at
test time. [22] propose to train a detector network on the hidden layer’s representation of the guarded
model. If the detector ﬁnds an adversarial input  an autonomous operation can be stopped and human
intervention can be requested. [23] adopt a Bayesian interpretation of Dropout to extract conﬁdence
intervals during testing. Then  the optimal threshold was selected to distinguish natural images from
adversarial. Nonetheless  Carlini and Wagner [24] have extensively studied and demonstrated the
limitations of the detection-based methods. Using modiﬁed adversarial attacks  such defenses can
be broken in both white-box and black-box setups. In our work  the adversary critic is somewhat
similar to the adversary detector. But  unlike adversary-detection methods  we use information from
the adversary critic to improve the robustness of the guarded model during training and do not use
the adversary critic during testing.
Generative Adversarial Networks [11] introduce a generative model where the learning problem
is formulated as an adversarial game between discriminator and generator. The discriminator is
trained to distinguish between real images and generated images. The generator is trained to produce
naturally looking images which confuse the discriminator. A two-player minimax game is solved by
alternatively optimizing two models. Recently several defenses have been proposed which use GAN
framework to improve robustness of neural networks. Defense-GAN [25] use the generator at test
time to project the corrupted input on the manifold of the natural examples. Lee et al. [26] introduce
Generative Adversarial Trainer (GAT) in which the generator is trained to attack the classiﬁer. Like
Adversarial Training [7]  GAT requires that adversarial noise does not change the label. Compare
with defenses based on robust optimization  we do not put any prior constraint on the adversarial
attack. To the contrary  we require that adversarial noise for robust classiﬁer should change the “true”
label of the input to confuse the critic. Our formulation has three components (the classiﬁer  the critic 
and the attack) and is also related to Triple-GAN [27]. But  in our work: 1) the generator also fools
the classiﬁer; 2) we use the implicit dependency between the model and the attack to improve the
robustness of the classiﬁer. Also  we use a ﬁxed algorithm to attack the classiﬁer.

3 Robust Optimization

We ﬁrst recall a mathematical formulation for the robust multiclass classiﬁcation. Let f (x; W) be a
k-class classiﬁer  e.g. neural network  where x ∈ RN is in the input space and W are the classiﬁer
parameters. The prediction rule is ˆk(x) = arg max f (x). Robust optimization seeks a solution
robust to the worst-case input perturbations:

L(f (xi + ri)  yi)

min
W

max
ri∈Ui

(1)
where L is a training loss  ri is an arbitrary (even adversarial) perturbation for the input xi  and Ui is
an uncertainty set  e.g. lp-norm -ball Ui = {ri : (cid:107)ri(cid:107)p ≤ }. Prior information about the task can
be used to select a problem-speciﬁc uncertainty set U.
Several regularization methods can be shown to be equivalent to the robust optimization  e.g. l1
lasso regression [28] and l2 support vector machine [29]. Adversarial training [7] is a popular
regularization method to improve neural network robustness. AT assumes that adversarial noise is
label non-changing and trains neural network on the mixture of original and adversarial images:

N(cid:88)

i=1

N(cid:88)

min
W

i=1

3

L(f (xi)  yi) + λL(f (xi + ri)  yi)

(2)

where ri is the adversarial perturbation generated using Fast Gradient Sign method (FGSM). Shaham
et al. [30] show that adversarial training is a form of robust optimization with l∞-norm constraint.
Madry et al. [17] experimentally argue that Projected Gradient Descent (PGD) adversary is inner
maximizer of eq. (1) and  thus  PGD is the optimal ﬁrst-order attack. Adversarial training with PGD
attack increases the robustness of the regularized models compare to the original defense. Margin
maximization [19] is another regularization method which generalizes SVM objective to deep neural
networks  and  like SVM  it is equivalent to the robust optimization with the margin loss.
Selecting a good uncertainty set U for robust opti-
mization is crucial. Poorly chosen uncertainty set
may result in an overly conservative robust model.
Most importantly  each perturbation r ∈ U should
leave the “true” class of the original input x un-
changed. To ensure that the changes of the network
prediction are indeed fooling examples  Goodfellow
et al. [7] argue in favor of a max-norm perturbation
constraint for image classiﬁcation problems. How-
ever  simple disturbance models (e.g. l2- and l∞-
norm -ball used in adversarial training) are inade-
quate in practice because the distance to the decision
boundary for different examples may signiﬁcantly
vary. To adapt uncertainty set to the problem at hand 
several methods have been developed for construct-
ing data-dependent uncertainty sets using statistical
hypothesis tests [31]. In this work  we propose a
novel approach for learning a robust classiﬁer which
is orthogonal to prior robust optimization methods.
Ideally  inputs that are adversarial to the classiﬁer
should be confusing to a human observer. So the
changes introduced by the adversarial noise should
be associated with the removing of identifying char-
acteristics of the original label and adding the identifying characteristics of the adversarial target. For
example  adversarial images in Figure 2 are visually confusing. The digit ‘1’ (second row  eighth col-
umn) after adding the top stroke was classiﬁed by the neural network as digit ‘7’. Likewise  the digit
‘7’ (eighth row  second column) after removing the top stroke was classiﬁed by the network as digit
‘1’. Similarly for other images in Figure 2  the model’s “mistakes” can be predicted visually. Such
behavior of the classiﬁer is expected and desired for the problems in computer vision. Additionally  it
improves the interpretability of the model. In this work  we study image classiﬁcation problems  but
our formulation can be extended to the classiﬁcation tasks in other domains  e.g. audio or text.
Based on the above intuition  we develop a novel formulation for learning a robust classiﬁer. Classiﬁer
is robust if its adversarial examples are indistinguishable from the regular data of the adversarial
target (see ﬁg. 1). So  we formulate the following mathematical problem:

Figure 2:
Images off-diagonal are cor-
rupted with the adversarial noise generated
by CW [10] l2-norm attack  so the predic-
tion conﬁdence on the adversarial images is
at least 95%. The prediction conﬁdence on
the original images is 99%.

N(cid:88)

min

L(f (xi)  yi) + λD [pdata (x  y)   padv (x  y)]

(3)

i=1

where pdata (x  y) and padv (x  y) is the distribution of the natural and the adversarial for f examples
and the parameter λ controls the trade-off between accuracy and robustness. Note that the distribution
padv (x  y) is constructed by transforming natural samples (x  y) ∼ pdata (x  y) with y (cid:54)= yadv  so that
adversarial example xadv = Af (x; yadv) is classiﬁed by f as the attack’s target yadv.
The ﬁrst loss in eq. (3)  e.g. NLL  ﬁts the model predictive distribution to the data distribution. The
second term measures the probabilistic distance between the distribution of the regular and adversarial
images and constrains the classiﬁer  so its adversarial examples are indistinguishable from the regular
inputs. It is important to note that we minimize a probabilistic distance between joint distributions
because the distance between marginal distributions pdata(x) and padv(x) is trivially minimized when
r ∼ 0. Compare with adversarial training  the proposed formulation does not impose the assumption
that adversarial noise is label non-changing. To the contrary  we require that adversarial noise for the
robust classiﬁer should be visually confusing and  thus  it should change the underlying label of the
input. Next  we will describe the implementation details of the proposed defense.

4

4 Robust Learning with Adversary Critic

Dk

As we have argued in the previous section  adversarial examples for the robust classiﬁer should be
indistinguishable from the regular data of the adversarial target. Minimizing the statistical distance
between pdata (x  y) and padv (x  y) in eq. (3) requires probability density estimation which in itself
is a difﬁcult problem. Instead  we adopt the framework of Generative Adversarial Networks [11].
We rely on a discriminator  or an adversary critic  to estimate a measure of difference between
two distributions. The discriminator given an input-label pair (x  y) classiﬁes it as either natural
or adversarial. For the k-class classiﬁer f  we implement the adversary critic as a k-output neural
network (see ﬁg. 3). The objective for the k-th output of the discriminator D is to correctly distinguish
between natural and adversarial examples of the class yk:
L(f∗  Dk) = min
Ex∼pdata(x|y) [log (1 − Dk (Af∗ (x; yk)))]
Ex∼pdata(x|yk) [log Dk (x)] + Ey:y(cid:54)=yk
(4)
where Af (x  yk) is the targeted adversarial attack on the classiﬁer f which transforms the input x
to the adversarial target yk. An example of such attack is Projected Gradient Descent [12] which
iteratively takes a step in the direction of the target yk. Note that the second term in eq. (4) is
computed by transforming the regular inputs (x  y) ∼ pdata (x  y) with the original label y different
from the adversarial target yk.
Our architecture for the discriminator in Figure 3 is slightly different from the previous work on
joint distribution matching [27] where the label information was added as the input to each layer
of the discriminator. We use class label only in the ﬁnal classiﬁcation layer of the discriminator.
In the experiments  we observe that with the proposed architecture: 1) the discriminator is more
stable during training; 2) the classiﬁer f converges faster and is more robust. We also regularize
the adversary critic with a gradient norm penalty [32]. For the gradient norm penalty  we do not
interpolate between clean and adversarial images but simply compute the penalty at the real and
adversarial data separately. Interestingly  regularizing the gradient of the binary classiﬁer has the
interpretation of maximizing the geometric margin [19].
The objective for the classiﬁer f is to minimize the number of mistakes subject to that its adversarial
examples generated by the attack Af fool the adversary critic D:
L(f  D∗) = min

k (Af (x; yk))] (5)
where L is a standard supervised loss such as negative log-likelihood (NLL) and the parameter λ
controls the trade-off between test accuracy and classiﬁer robustness. To improve stability of the
adversarial mapping during training  we introduce adversarial cycle-consistency constraint which
ensures that adversarial mapping Af of the adversarial examples should be close to the original:

Ex y∼pdata(x y)L(f (x)  y) + λ

Ey:y(cid:54)=yk

Ex∼pdata(x|y) [log D∗

(cid:88)

yk

f

Lcycle(ys  yt) = Ex∼pdata(x|ys)

(cid:2)(cid:107)Af (Af (x  yt)  ys) − x(cid:107)2

(cid:3) ∀ys (cid:54)= yt

(6)

real1

adv1

. . .

realk

advk

Xadv

Af

D

Xreal

Algorithm 1 High-Conﬁdence Attack Af
1: Input: Image x  target y  network f  conﬁdence C.
2: Output: Adversarial image ˆx.
3: ˆx ← x
4: while py(ˆx) < C do
f ← log C − log py( ˆx)
5:
6: w ← ∇ log py( ˆx)
r ← f(cid:107)w(cid:107)2
7:
w
ˆx ← ˆx + r
8:
9: end while

2

Figure 3: Multiclass Adversary Critic.

5

rk =

log C − log pk(x)
(cid:107)∇x log pk(x)(cid:107)2

(7)

where ys is the original label of the input and yt is the adversarial target. Adversarial cycle-consistency
constraint is similar to cycle-consistency constraint which was introduced for image-to-image transla-
tion [33]. But  we introduce it to constraint the adversarial mapping Af and it improves the robustness
of the classiﬁer f. Next  we discuss implementation of our targeted adversarial attack Af .
Our defense requires that the adversarial attack Af is differentiable. Additionally  adversarial
examples generated by the attack Af should be misclassiﬁed by the network f with high conﬁdence.
Adversarial examples which are close to the decision boundary are likely to retain some identifying
characteristics of the original class. An attack which optimizes for the mistakes  e.g. DeepFool [13] 
guarantees the conﬁdence of 1
k for k-way classiﬁer. To generate high-conﬁdence adversarial examples 
we propose a novel adversarial attack which iteratively maximizes the conﬁdence of the adversarial
target. The conﬁdence of the target k after adding perturbation r is pk(x + r). The goal of the attack
is to ﬁnd the perturbation  so the adversarial input is misclassiﬁed as k with the conﬁdence at least C:

min (cid:107)r(cid:107)
s. t. pk(x + r) ≥ C

We apply a ﬁrst-order approximation to the constraint inequality:

min (cid:107)r(cid:107)
s. t. pk(x) + r∇xpk(x) ≥ C

Softmax in the ﬁnal classiﬁcation layer saturates quickly and shatters the gradient. To avoid small
gradients  we use log-likelihood instead. Finally  the l2-norm minimal perturbation can be computed
using a method of Lagrange multipliers as follows:

target k with the conﬁdence C. Our attack can be equivalently written as xadv = x +(cid:81)Nmax
Because we use the approximation of the non-convex decision boundary  we iteratively update
(cid:80)i
perturbation r for Nmax steps using eq. (7) until the adversarial input xadv is misclassiﬁed as the
i=1 I(p(x +
j=1 rj) ≤ C)ri where I is an indicator function. The discrete stopping condition introduces a
non-differentiable path in the computational graph. We replace the gradient of the indicator function
I with sigmoid-adjusted straight-through estimator during backpropagation [34]. This is a biased
estimator but it has low variance and performs well in the experiments.
The proposed attack is similar to Basic Iterative Method (BIM) [12]. BIM takes a ﬁxed -norm step
|log C−log py( ˆx)|
in the direction of the attack target while our method uses an adaptive step γ =
(cid:107)∇x log py( ˆx)(cid:107) . The
difference is important for our defense:

1. BIM introduces an additional parameter . If  is too large  then the attack will not be

accurate. If  is too small  then the attack will require many iterations to converge.

2. Both attacks are differentiable. However  for BIM attack during backpropagation  all
the gradients ∂ri
∂w have an equal weight . For our attack  the gradients will be weighted
adaptively depending on the distance γ to the attack’s target. The step γ for our attack is
also fully-differentiable.

Full listing of our attack is shown in algorithm 1. Next  we discuss how we select the adversarial
target yt and the attack’s target conﬁdence C during training.
The classiﬁer f approximately characterizes a conditional distribution p (y|x). If the classiﬁer f∗ is
optimal and robust  its adversarial examples generated by the attack Af should fool the adversary
critic D. Therefore  the attack Af to fool the critic D should generate adversarial examples with the
conﬁdence C equal to the conﬁdence of the classiﬁer f on the regular examples. During training 
we maintain a running mean of the conﬁdence score for each class on the regular data. The attack
target yt for the input x with the label ys can be sampled from the masked uniform distribution.
Alternatively  the class with the closest decision boundary [13] can be selected. The latter formulation
resulted in more robust classiﬁer f and we used it in all our experiments. This is similar to support
vector machine formulation which maximizes the minimum margin.
Finally  we train the classiﬁer f and the adversary critic D jointly using stochastic gradient descent
by alternating minimization of Equations (4) and (5). Our formulation has three components (the
classiﬁer f  the critic D  and the attack Af ) and it is similar to Triple-GAN [27] but the generator in
our formulation also fools the classiﬁer.

6

5 Experiments

Adversarial training [7] discards the dependency between the model parameters and the adversarial
noise. In this work  it is necessary to retain the implicit dependency between the classiﬁer f and the
adversarial noise  so we can backpropagate through the adversarial attack Af . For these reasons 
all experiments were conducted using Tensorﬂow [35] which supports symbolic differentiation and
computation on GPU. Backpropagation through our attack requires second-order gradients ∂2f (x;w)
which increases computational complexity of our defense. At the same time  this allows the model to
anticipate the changes in the adversary and  as we show  signiﬁcantly improves the model robustness
both numerically and perceptually.
We perform experiments on MNIST dataset. While MNIST is a simple classiﬁcation task  it remains
unsolved in the context of robust learning. We evaluate robustness of the models against l2 attacks.
Minimal adversarial perturbation r is estimated using DeepFool [13]  Carlini and Wagner [10]  and
the proposed attack. To improve the accuracy of DeepFool and our attack during testing  we clip the
l2-norm of perturbation at each iteration to 0.1. Note that our attack with the ﬁxed step is equivalent
to Basic Iterative Method [12]. We set the maximum number of iterations for DeepFool and our
attack to 500. The target conﬁdence C for our attack is set to the prediction conﬁdence on the original
input x. DeepFool and our attack do not handle domain constraints explicitly  so we project the
perturbation after each update. For Carlini and Wagner [10]  we use implementation provided by the
authors with default settings for the attack but we reduce the number of optimization iterations from
10000 to 1000. As suggested in [13]  we measure the robustness of the model as follows:

∂x∂w

1
|D|

ρadv(Af ) =

(cid:107)r(x)(cid:107)2
(cid:107)x(cid:107)2
where Af is the attack on the classiﬁer f and D is the test set.
We compare our defense with reference (no defense)  Adversarial Training [7  8] ( = 0.1)  Virtual
Adversarial Training (VAT) [16] ( = 2.0)  and l2-norm Margin Maximization [19] (λ = 0.1) defense.
We study the robustness of two networks with rectiﬁed activation: 1) a fully-connected neural network
with three hidden layers of size 1200 units each; 2) Lenet-5 convolutional neural network. We train
both networks using Adam optimizer [36] with batch size 100 for 100 epochs. Next  we will describe
the training details for our defense.
Our critic has two layers with 1200 units each and leaky rectiﬁed activation. We also add Gaussian
noise to the input of each layer. We train both the classiﬁer and the critic using Adam [36] with the
momentum β1 = 0.5. The starting learning rate is set to 5 · 10−4 and 10−3 for the classiﬁer and the
discriminator respectively. We train our defense for 100 epochs and the learning rate is halved every
40 epochs. We set λ = 0.5 for fully-connected network and λ = 0.1 for Lenet-5 network which we
selected using validation dataset. Both networks are trained with λrec = 10−2 for the adversarial
cycle-consistency loss and λgrad = 10.0 for the gradient norm penalty. The number of iterations for
our attack Af is set to 5. The attack conﬁdence C is set to the running mean class conﬁdence of the
classiﬁer on natural images. We pretrain the classiﬁer f for 1 epoch without any regularization to get
an initial estimate of the class conﬁdence scores.
Our results for 10 independent runs are summarized in Table 1  where the second column shows the
test error on the clean images  and the subsequent columns compare the robustness ρ to DeepFool [13] 
Carlini and Wagner [10]  and our attacks. Our defense signiﬁcantly increases the robustness of the

(cid:88)

x∈D

(8)

Defense
Reference
[7]
[16]
[19]
Our

%
1.46
0.90
0.84
0.84
1.18

[13]
0.131
0.228
0.244
0.262
0.290

(a)

[10]
0.124
0.210
0.215
0.230
0.272

Our
0.173
0.299
0.355
0.453
0.575

Defense
Reference
[7]
[16]
[19]
Our

%
0.64
0.55
0.60
0.54
0.93

[10]
0.148
0.191
0.195
0.225
0.278

Our
0.207
0.286
0.330
0.470
0.590

[13]
0.157
0.215
0.225
0.248
0.288
(b)

Table 1: Results on MNIST dataset for fully-connected network in table 1a and for Lenet-5 convolu-
tional network in table 1b. Column 1: test error on original images. Column 3-5: robustness ρ under
DeepFool [13]  Carlini and Wagner [10]  and the proposed attack.

7

(a)

(b)

(c)

Figure 4: Figure 4a shows a random subset of test images (average conﬁdence 97%). Figure 4b
shows adversarial examples at the class decision boundary (average conﬁdence 34%). Figure 4c
shows high-conﬁdence adversarial images (average conﬁdence 98%).

Defense
Reference
[7]
[16]
[19]
Our

% Change % No change
0.57
19.02
35.08
60.47
87.99

98.74
77.21
59.68
34.52
9.86

Defense
Reference
[7]
[16]
[19]
Our

% Change % No change
2.54
19.1
26.8
81.77
92.29

96.53
75.94
67.73
13.15
6.51

(a)

(b)

Table 2: Results of Amazon Mechanical Turk experiment for fully-connected network in table 2a and
for Lenet-5 convolutional network in ﬁg. 4c. Column 2: shows percent of adversarial images which
human annotator label with its adversarial target  so adversarial noise changed the “true” label of
the input. Column 3: shows percent of the adversarial images which human annotator label with its
original label  so adversarial noise did not change the underlying label of the input.

model to adversarial examples. Some adversarial images for the neural network trained with our
defense are shown in Figure 4. Adversarial examples are generated using Carlini and Wagner
[10] attack with default parameters. As we can observe  adversarial examples at the decision
boundary in Figure 4b are visually confusing. At the same time  high-conﬁdence adversarial examples
in Figure 4c closely resemble natural images of the adversarial target. We propose to investigate and
compare various defenses based on how many of its adversarial “mistakes” are actual mistakes.
We conduct an experiment with human annotators on MTurk. We asked the workers to label
adversarial examples. Adversarial examples were generated from the test set using the proposed
attack. The attack’s target was set to class closest to the decision boundary and the target conﬁdence
was set to the model’s conﬁdence on the original examples. We split 10000 test images into 400
assignments. Each assignment was completed by one unique annotator. We report the results for four
defenses in Table 2. For the model trained without any defense  adversarial noise does not change the
label of the input. When the model is trained with our defense  the high-conﬁdence adversarial noise
actually changes the label of the input.

6 Conclusion

In this paper  we introduce a novel approach for learning a robust classiﬁer. Our defense is based
on the intuition that adversarial examples for the robust classiﬁer should be indistinguishable from
the regular data of the adversarial target. We formulate a problem of learning robust classiﬁer in the
framework of Generative Adversarial Networks. Unlike prior work based on robust optimization  our
method does not put any prior constraints on adversarial noise. Our method surpasses in terms of
robustness networks trained with adversarial training. In experiments with human annotators  we also
show that adversarial examples for our defense are indeed visually confusing. In the future work  we
plan to scale our defense to more complex datasets and apply it to the classiﬁcation tasks in other
domains  such as audio or text.

8

Acknowledgments

This work was carried out at the Rapid-Rich Object Search (ROSE) Lab at Nanyang Technological
University (NTU)  Singapore. The ROSE Lab is supported by the National Research Foundation 
Singapore  and the Infocomm Media Development Authority  Singapore. We thank NVIDIA Corpo-
ration for the donation of the GeForce Titan X and GeForce Titan X (Pascal) used in this research.
We also thank all the anonymous reviewers for their valuable comments and suggestions.

References
[1] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR 

2016.

[2] G. Hinton  L. Deng  D. Yu  G. E. Dahl  A. r. Mohamed  N. Jaitly  A. Senior  V. Vanhoucke 
P. Nguyen  T. N. Sainath  and B. Kingsbury. Deep neural networks for acoustic modeling in
speech recognition: The shared views of four research groups. In IEEE Signal Processing
Magazine  2012.

[3] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus.

Intriguing properties of neural networks. In ICLR  2013.

[4] N. Carlini and D. Wagner. Audio Adversarial Examples: Targeted Attacks on Speech-to-Text.

arXiv preprint arXiv:1801.01944  2018.

[5] Mahmood Sharif  Sruti Bhagavatula  Lujo Bauer  and Michael K. Reiter. Accessorize to a crime:
Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security  2016.

[6] Nicolas Papernot  Patrick D. McDaniel  Somesh Jha  Matt Fredrikson  Z. Berkay Celik  and
Ananthram Swami. The limitations of deep learning in adversarial settings. In IEEE European
Symposium on Security and Privacy  2016.

[7] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples. In ICLR  2015.

[8] A. Kurakin  I. Goodfellow  and S. Bengio. Adversarial Machine Learning at Scale. In ICLR 

2017.

[9] Florian Tramèr  Alexey Kurakin  Nicolas Papernot  Ian Goodfellow  Dan Boneh  and Patrick

McDaniel. Ensemble adversarial training: Attacks and defenses. In ICLR  2018.

[10] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks.

In IEEE Symposium on Security and Privacy  2017.

[11] Ian J. Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil

Ozair  Aaron C. Courville  and Yoshua Bengio. Generative adversarial nets. In NIPS  2014.

[12] A. Kurakin  I. Goodfellow  and S. Bengio. Adversarial examples in the physical world. arXiv

preprint arXiv:1607.02533  2016.

[13] S. M. Moosavi-Dezfooli  A. Fawzi  and P. Frossard. Deepfool: A simple and accurate method

to fool deep neural networks. In CVPR  2016.

[14] Nicolas Papernot  Patrick D. McDaniel  Ian J. Goodfellow  Somesh Jha  Z. Berkay Celik  and
Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the
2017 ACM on Asia Conference on Computer and Communications Security  2017.

[15] Shumeet Baluja and Ian Fischer. Learning to attack: Adversarial transformation networks. In

AAAI  2018.

[16] T. Miyato  S.-i. Maeda  M. Koyama  K. Nakae  and S. Ishii. Distributional Smoothing with

Virtual Adversarial Training. In ICLR  2015.

9

[17] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards Deep Learning Models

Resistant to Adversarial Attacks. In ICLR  2018.

[18] S. Zheng  Y. Song  T. Leung  and I. Goodfellow. Improving the robustness of deep neural

networks via stability training. In CVPR  2016.

[19] Alexander Matyasko and Lap-Pui Chau. Margin maximization for robust classiﬁcation using

deep learning. In IJCNN  2017.

[20] Gamaleldin Fathy Elsayed  Dilip Krishnan  Hossein Mobahi  Kevin Regan  and Samy Bengio.

Large margin deep networks for classiﬁcation. In NIPS  2018.

[21] Moustapha Cissé  Piotr Bojanowski  Edouard Grave  Yann Dauphin  and Nicolas Usunier.

Parseval networks: Improving robustness to adversarial examples. In ICML  2017.

[22] J. Hendrik Metzen  T. Genewein  V. Fischer  and B. Bischoff. On Detecting Adversarial

Perturbations. In ICLR  2017.

[23] R. Feinman  R. R. Curtin  S. Shintre  and A. B. Gardner. Detecting Adversarial Samples from

Artifacts. arXiv preprint arXiv:1703.00410  2017.

[24] Nicholas Carlini and David A. Wagner. Adversarial examples are not easily detected: Bypassing
ten detection methods. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security  2017.

[25] Pouya Samangouei  Maya Kabkab  and Rama Chellappa. Defense-GAN: Protecting classiﬁers

against adversarial attacks using generative models. In ICLR  2018.

[26] H. Lee  S. Han  and J. Lee. Generative Adversarial Trainer: Defense to Adversarial Perturbations

with GAN. arXiv preprint arXiv:1705.03387  2017.

[27] Chongxuan LI  Tauﬁk Xu  Jun Zhu  and Bo Zhang. Triple generative adversarial nets. In NIPS 

2017.

[28] Huan Xu  Constantine Caramanis  and Shie Mannor. Robust regression and lasso. In NIPS 

2009.

[29] Huan Xu  Constantine Caramanis  and Shie Mannor. Robustness and regularization of support

vector machines. In Journal of Machine Learning Research  2009.

[30] U. Shaham  Y. Yamada  and S. Negahban. Understanding Adversarial Training: Increasing
Local Stability of Neural Nets through Robust Optimization. arXiv preprint arXiv:1511.05432 
2015.

[31] Dimitris Bertsimas  Vishal Gupta  and Nathan Kallus. Data-driven robust optimization. In

Mathematical Programming  2018.

[32] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville.

Improved training of wasserstein gans. In NIPS  2017.

[33] J. Zhu  T. Park  P. Isola  and A. A. Efros. Unpaired image-to-image translation using cycle-

consistent adversarial networks. In ICCV  2017.

[34] Yoshua Bengio  Nicholas Léonard  and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432  2015.

[35] The Tensorﬂow Development Team. TensorFlow: Large-Scale Machine Learning on Heteroge-

neous Distributed Systems. arXiv preprint arXiv:1603.04467  2015.

[36] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. arXiv preprint

arXiv:1412.6980  2015.

10

,Alexander Matyasko
Lap-Pui Chau