2011,Optimal Reinforcement Learning for Gaussian Systems,The exploration-exploitation trade-off is among the central challenges of reinforcement learning. The optimal Bayesian solution is intractable in general. This paper studies to what extent analytic statements about optimal learning are possible if all beliefs are Gaussian processes. A first order approximation of learning of both loss and dynamics  for nonlinear  time-varying systems in continuous time and space  subject to a relatively weak restriction on the dynamics  is described by an infinite-dimensional partial differential equation. An approximate finite-dimensional projection gives an impression for how this result may be helpful.,Optimal Reinforcement Learning

for Gaussian Systems

Philipp Hennig

Max Planck Institute for Intelligent Systems

Department of Empirical Inference

Spemannstraße 38  72070 T¨ubingen  Germany

phennig@tuebingen.mpg.de

Abstract

The exploration-exploitation trade-off is among the central challenges of rein-
forcement learning. The optimal Bayesian solution is intractable in general. This
paper studies to what extent analytic statements about optimal learning are possible
if all beliefs are Gaussian processes. A ﬁrst order approximation of learning of
both loss and dynamics  for nonlinear  time-varying systems in continuous time
and space  subject to a relatively weak restriction on the dynamics  is described
by an inﬁnite-dimensional partial differential equation. An approximate ﬁnite-
dimensional projection gives an impression for how this result may be helpful.

1

Introduction – Optimal Reinforcement Learning

Reinforcement learning is about doing two things at once: Optimizing a function while learning
about it. These two objectives must be balanced: Ignorance precludes efﬁcient optimization; time
spent hunting after irrelevant knowledge incurs unnecessary loss. This dilemma is famously known
as the exploration exploitation trade-off. Classic reinforcement learning often considers time cheap;
the trade-off then plays a subordinate role to the desire for learning a “correct” model or policy. Many
classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration  such
as “-greedy” [1]  or “Thompson sampling” [2]. However  at least since a thesis by Duff [3] it has
been known that Bayesian inference allows optimal balance between exploration and exploitation. It
requires integration over every possible future trajectory under the current belief about the system’s
dynamics  all possible new data acquired along those trajectories  and their effect on decisions taken
along the way. This amounts to optimization and integration over a tree  of exponential cost in the
size of the state space [4]. The situation is particularly dire for continuous space-times  where both
depth and branching factor of the “tree” are uncountably inﬁnite. Several authors have proposed
approximating this lookahead through samples [5  6  7  8]  or ad-hoc estimators that can be shown to
be in some sense close to the Bayes-optimal policy [9].
In a parallel development  recent work by Todorov [10]  Kappen [11] and others introduced an idea to
reinforcement learning long commonplace in other areas of machine learning: Structural assumptions 
while restrictive  can greatly simplify inference problems. In particular  a recent paper by Simpkins
et al. [12] showed that it is actually possible to solve the exploration exploitation trade-off locally 
by constructing a linear approximation using a Kalman ﬁlter. Simpkins and colleagues further
assumed to know the loss function  and the dynamics up to Brownian drift. Here  I use their work as
inspiration for a study of general optimal reinforcement learning of dynamics and loss functions of
an unknown  nonlinear  time-varying system (note that most reinforcement learning algorithms are
restricted to time-invariant systems). The core assumption is that all uncertain variables are known up
to Gaussian process uncertainty. The main result is a ﬁrst-order description of optimal reinforcement
learning in form of inﬁnite-dimensional differential statements. This kind of description opens up
new approaches to reinforcement learning. As an only initial example of such treatments  Section 4

1

presents an approximate Ansatz that affords an explicit reinforcement learning algorithm; tested in
some simple but instructive experiments (Section 5).
An intuitive description of the paper’s results is this: From prior and corresponding choice of learning
machinery (Section 2)  we construct statements about the dynamics of the learning process (Section
3). The learning machine itself provides a probabilistic description of the dynamics of the physical
system. Combining both dynamics yields a joint system  which we aim to control optimally. Doing so
amounts to simultaneously controlling exploration (controlling the learning system) and exploitation
(controlling the physical system).
Because large parts of the analysis rely on concepts from optimal control theory  this paper will use
notation from that ﬁeld. Readers more familiar with the reinforcement learning literature may wish to
mentally replace coordinates x with states s  controls u with actions a  dynamics with transitions
p(s(cid:48) | s  a) and utilities q with losses (negative rewards) −r. The latter is potentially confusing  so
note that optimal control in this paper will attempt to minimize values  rather than to maximize them 
as usual in reinforcement learning (these two descriptions are  of course  equivalent).

2 A Class of Learning Problems
We consider the task of optimally controlling an uncertain system whose states s ≡ (x  t) ∈ K ≡
RD×R lie in a D +1 dimensional Euclidean phase space-time: A cost Q (cumulated loss) is acquired
at (x  t) with rate dQ/dt = q(x  t)  and the ﬁrst inference problem is to learn this analytic function
q. A second  independent learning problem concerns the dynamics of the system. We assume the
dynamics separate into free and controlled terms afﬁne to the control:

dx(t) = [f (x  t) + g(x  t)u(x  t)] dt

(1)

where u(x  t) is the control function we seek to optimize  and f  g are analytic functions. To simplify
our analysis  we will assume that either f or g are known  while the other may be uncertain (or 
alternatively  that it is possible to obtain independent samples from both functions). See Section
3 for a note on how this assumption may be relaxed. W.l.o.g.  let f be uncertain and g known.
Information about both q(x  t) and f (x  t) = [f1  . . .   fD] is acquired stochastically: A Poisson
process of constant rate λ produces mutually independent samples
yq(x  t) = q(x  t)+q and yf d(x  t) = fd(x  t)+f d where q ∼ N (0  σ2
f d). (2)
The noise levels σq and σf are presumed known. Let our initial beliefs about q and f be given by
d GP kf d (fd; µf d  Σf d) 
respectively  with kernels kr  kf 1  . . .   kf D over K  and mean / covariance functions µ / Σ. In other
words  samples over the belief can be drawn using an inﬁnite vector Ω of i.i.d. Gaussian variables  as

Gaussian processes GP kq (q; µq  Σq); and independent Gaussian processes(cid:81)D

q ); f d ∼ N (0  σ2

˜fd([x  t]) = µf d([x  t])+

f d ([x  t]  [x(cid:48)  t(cid:48)])Ω(x(cid:48)  t(cid:48))dx(cid:48) dt = µf d([x  t])+(Σ1/2
Σ1/2

f d Ω)([x  t]) (3)

the second equation demonstrates a compact notation for inner products that will be used throughout.
It is important to note that f  q are unknown  but deterministic. At any point during learning  we can
use the same samples Ω to describe uncertainty  while µ  Σ change during the learning process.
To ensure continuous trajectories  we also need to regularize the control. Following control custom 
R−1u with control cost scaling matrix R. Its units
we introduce a quadratic control cost ρ(u) = 1
[R] = [x/t]/[Q/x] relate the cost of changing location to the utility gained by doing so.
The overall task is to ﬁnd the optimal discounted horizon value

(cid:124)
2 u

v(x  t) = min

u

e−(τ−t)/γ

q[χ[τ  u(χ  τ )]  τ ] +

(cid:124)
u(χ  τ )

R−1u(χ  τ )

1
2

dτ

(4)

where χ(τ  u) is the trajectory generated by the dynamics deﬁned in Equation (1)  using the control
law (policy) u(x  t). The exponential deﬁnition of the discount γ > 0 gives the unit of time to γ.
Before beginning the analysis  consider the relative generality of this deﬁnition: We allow for a
continuous phase space. Both loss and dynamics may be uncertain  of rather general nonlinear form 
and may change over time. The speciﬁc choice of a Poisson process for the generation of samples is

2

(cid:90)

(cid:90) ∞

t

(cid:20)

(cid:21)

somewhat ad-hoc  but some measure is required to quantify the ﬂow of information through time.
The Poisson process is in some sense the simplest such measure  assigning uniform probability
density. An alternative is to assume that datapoints are acquired at regular intervals of width λ. This
results in a quite similar model but  since the system’s dynamics still proceed in continuous time  can
complicate notation. A downside is that we had to restrict the form of the dynamics. However  Eq.
(1) still covers numerous physical systems studied in control  for example many mechanical systems 
from classics like cart-and-pole to realistic models for helicopters [13].

3 Optimal Control for the Learning Process

The optimal solution to the exploration exploitation trade-off is formed by the dual control [14] of a
joint representation of the physical system and the beliefs over it. In reinforcement learning  this idea
is known as a belief-augmented POMDP [3  4]  but is not usually construed as a control problem.
This section constructs the Hamilton-Jacobi-Bellman (HJB) equation of the joint control problem
for the system described in Sec. 2  and analytically solves the equation for the optimal control. This
necessitates a description of the learning algorithm’s dynamics:
At time t = τ  let the system be at phase space-time sτ = (x(τ )  τ ) and have the Gaussian process
belief GP(q; µτ (s)  Στ (s  s(cid:48))) over the function q (all derivations in this section will focus on q 
and we will drop the sub-script q from many quantities for readability. The forms for f  or g  are
entirely analogous  with independent Gaussian processes for each dimension d = 1  . . .   D). This
(cid:124) ∈ RN collected at space-times
belief stems from a ﬁnite number N of samples y0 = [y1  . . .   yN ]
(cid:124) ∈ KN (note that t1 to tN need not be equally
S0 = [(x1  t1)  . . .   (xN   tN )]
spaced  ordered  or < τ). For arbitrary points s∗ = (x∗  t∗) ∈ K  the belief over q(s∗) is a Gaussian
with mean function µτ   and co-variance function Στ [15]
i   S0)[K(S0  S0) + σ2
i   s∗

(5)
where K(S0  S0) is the Gram matrix with elements Kab = k(sa  sb). We will abbreviate K0 ≡
yI] from here on. The co-vector k(s∗  S0) has elements ki = k(s∗  si) and will
[K(S0  S0) + σ2
chance of acquiring a datapoint yτ in this time is λ dt. Marginalising over this Poisson stochasticity 
we expect one sample with probability λ dt  two samples with (λ dt)2 and so on. So the mean after
dt is expected to be

be shortened to k0. How does this belief change as time moves from τ to τ + dt? If dt(cid:95) 0  the

i ) = k(s∗
j ) = k(s∗

(cid:124) ≡ [s1  . . .   sN ]

i   S0)[K(S0  S0) + σ2

q I]−1k(S0  s∗
j )

µτ (s∗
i   s∗

j ) − k(s∗

q I]−1y0

Στ (s∗

0 y0 +O(λ dt)2 (6)
µτ + dt = λ dt (k0  kτ )
where we have deﬁned the map kτ = k(s∗  sτ )  the vector ξτ with elements ξτ i = k(si  sτ )  and
the scalar κτ = k(sτ   sτ ) + σ2

+ (1− λ dt−O(λ dt)2)· k0K−1

ξτ
κτ

yτ

(cid:124)
τ

ξ

µτ + dt = k0K−1

q. Algebraic re-formulation yields
(cid:124)
t K−1

0 ξt)(κt − ξ
K−1

0 y0 + λ(kt − k0

(cid:124)

0 ξt)−1(yt − ξ
(cid:124)
τ K−1

(cid:124)
t K−1
0 ξτ ) = σ2

0 y0) dt.

(7)
q + Σ(sτ   sτ ) 

0 y0 = µ(sτ )  the mean prediction at sτ and (κτ − ξ
Note that ξ
the marginal variance there. Hence  we can deﬁne scalars ¯Σ  ¯σ and write

(cid:124)
τ K−1

(cid:18)K0

(cid:19)−1(cid:18)y0

(cid:19)

(cid:124)
(yτ − ξ
τ K−1
0 y0)
(cid:124)
(κτ − ξ
τ K−1
0 ξτ )1/2

=

[Σ1/2Ω](sτ ) + σω
[Σ(sτ   sτ ) + σ2]1/2

≡ ¯Σ1/2

τ Ω + ¯στ ω with ω ∼ N (0  1).

(8)

So the change to the mean consists of a deterministic but uncertain change whose effects accumulate
linearly in time  and a stochastic change  caused by the independent noise process  whose variance
accumulates linearly in time (in truth  these two points are considerably subtler  a detailed proof is
left out for lack of space). We use the Wiener [16] measure dω to write
dµsτ (s∗) = λ

dt ≡ λLsτ (s∗)[ ¯Σ1/2

τ Ω dt + ¯στ dω]

(cid:124)
kτ − k0
K−1
0 ξτ
(cid:124)
τ K−1
0 ξτ )−1/2

(κτ − ξ

[Σ1/2Ω](sτ ) + σω
[Σ(sτ   sτ ) + σ2]1/2

(9)
where we have implicitly deﬁned the innovation function L. Note that L is a function of both s∗ and
sτ . A similar argument ﬁnds the change of the covariance function to be the deterministic rate

dΣsτ (s∗

i   s∗

j ) = −λLsτ (s∗

(cid:124)
i )L
sτ

(s∗

j ) dt.

(10)

3

(cid:26)(cid:90)(cid:90) (cid:20)(cid:18)

v(zτ ) = min

u

(cid:26)(cid:90)

= min

u

(cid:19)

R−1u

(cid:124)

u

1
2

(cid:21)

(cid:27)

(cid:27)

(14)

dt

µq(sτ ) + Σ1/2

qτ Ωq + σqωq +

dt + v(zτ + dt)

dω dΩ

µτ
q +Σ1/2

qτ Ωq+

(cid:124)
u

R−1u+

1
2

v(zτ )

dt

+

∂v
∂t

+[A+Bu+CΩ]

(cid:124)∇v+

1
2

(cid:124)
tr[D

(∇2v)D]dΩ

Integration over ω can be performed with ease  and removes the stochasticity from the problem; The
uncertainty over Ω is a lot more challenging. Because the distribution over future losses is correlated
through space and time  ∇v  ∇2v are functions of Ω  and the integral is nontrivial. But there are some
obvious approximate approaches. For example  if we (inexactly) swap integration and minimisation 
draw samples Ωi and solve for the value for each sample  we get an “average optimal controller”.
This over-estimates the actual sum of future rewards by assuming the controller has access to the true
system. It has the potential advantage of considering the actual optimal controller for every possible
system  the disadvantage that the average of optima need not be optimal for any actual solution. On
the other hand  if we ignore the correlation between Ω and ∇v  we can integrate (17) locally  all terms
in Ω drop out and we are left with an “optimal average controller”  which assumes that the system
locally follows its average (mean) dynamics. This cheaper strategy was adopted in the following.
Note that it is myopic  but not greedy in a simplistic sense – it does take the effect of learning into
account. It amounts to a “global one-step look-ahead”. One could imagine extensions that consider
the inﬂuence of Ω on ∇v to a higher order  but these will be left for future work. Under this ﬁrst-order
approximation  analytic minimisation over u can be performed in closed form  and bears

u(z) = −RB(z)

(cid:124)∇v(z) = −Rg(x  t)

(cid:124)∇xv(z).

So the dynamics of learning consist of a deterministic change to the covariance  and both deterministic
and stochastic changes to the mean  both of which are samples a Gaussian processes with covariance
(cid:124). This separation is a fundamental characteristic of GPs (it is the
function proportional to LL
nonparametric version of a more straightforward notion for ﬁnite-dimensional Gaussian beliefs  for
data with known noise magnitude).
We introduce the belief-augmented space H containing states z(τ ) ≡ [x(τ )  τ  µτ
q (s  s(cid:48))  Στ
Στ
Under our beliefs  z(τ ) obeys a stochastic differential equation of the form

f D 
f D]. Since the means and covariances are functions  H is inﬁnite-dimensional.

f 1  . . .   Στ

f 1  . . .   µτ

q (s)  µτ

(cid:104)

dz = [A(z) + B(z)u + C(z)Ω] dt + D(z) dω

(11)

with free dynamics A  controlled dynamics Bu  uncertainty operator C  and noise operator D

A =
B = [g(s∗)  0  0  0  . . . ]; C = diag(Σ1/2

(cid:124)
(cid:124)
(cid:124)
f (zx  zt)   1   0   0   . . .   0   −λLqL
q   −λLf 1L
f 1   . . .   −λLf DL
µτ
f D
f 1   . . .   λLf D ¯Σ1/2

f τ   0  λLq ¯Σ1/2

  λLf 1 ¯Σ1/2

q

f d   0  . . .   0);

;

(12)

(13)
The value – the expected cost to go – of any state s∗ is given by the Hamilton-Jacobi-Bellman
equation  which follows from Bellman’s principle and a ﬁrst-order expansion  using Eq. (4):

D = diag(0  0  λLq ¯σq  λLf 1 ¯σf 1  . . .   λLf D ¯σf D  0  . . .   0)

(cid:105)

tr(cid:2)D

(cid:124)

(∇2v)D(cid:3) .

The optimal Hamilton-Jacobi-Bellman equation is then

A more explicit form emerges upon re-inserting the deﬁnitions of Eq. (12) into Eq. (16):

γ−1v(z) = [µτ

f (zx  zt)∇x + ∇t

(cid:124)
[∇xv(z)]

(cid:124)

g

(zx  zt)Rg(zx  zt)∇xv(z)

γ−1v(z) = µτ

q + A

(cid:124)∇v − 1
2

[∇v]

(cid:124)

(cid:124)∇v +

BRB

1
2

q +(cid:2)µτ
(cid:124)

(cid:123)(cid:122)
(cid:88)

free drift cost

+

(cid:3)v(z)
(cid:125)
− λ(cid:2)LcL
(cid:124)

− 1
2

(cid:124)
(cid:123)(cid:122)
(cid:124)
c∇Σc

(cid:3)v(z)
(cid:125)

c=q f1 ... fD

exploration bonus

(cid:123)(cid:122)
(cid:2)L

+

1
2

(cid:124)

control beneﬁt
(cid:123)(cid:122)
(cid:124)
f d(∇2

λ2 ¯σ2
c

diffusion cost

v(z))Lf d

µf d

(15)

(16)

(17)

(cid:3)
(cid:125)

(cid:125)

Equation (17) is the central result: Given Gaussian priors on nonlinear control-afﬁne dynamic
systems  up to a ﬁrst order approximation  optimal reinforcement learning is described by an inﬁnite-
dimensional second-order partial differential equation. It can be interpreted as follows (labels in the

4

equation  note the negative signs of “beneﬁcial” terms): The value of a state comprises the immediate
utility rate; the effect of the free drift through space-time and the beneﬁt of optimal control; an
exploration bonus of learning  and a diffusion cost engendered by the measurement noise. The ﬁrst
two lines of the right hand side describe effects from the phase space-time subspace of the augmented
space  while the last line describes effects from the belief part of the augmented space. The former
will be called exploitation terms  the latter exploration terms  for the following reason: If the ﬁrst
two lines line dominate the right hand side of Equation (17) in absolute size  then future losses are
governed by the physical sub-space – caused by exploiting knowledge to control the physical system.
On the other hand  if the last line dominates the value function  exploration is more important than
exploitation – the algorithm controls the physical space to increase knowledge. To my knowledge 
this is the ﬁrst differential statement about reinforcement learning’s two objectives. Finally  note the
role of the sampling rate λ: If λ is very low  exploration is useless over the discount horizon.
Even after these approximations  solving Equation (17) for v remains nontrivial for two reasons:
First  although the vector product notation is pleasingly compact  the mean and covariance functions
are of course inﬁnite-dimensional  and what looks like straightforward inner vector products are in
fact integrals. For example  the average exploration bonus for the loss  writ large  reads

(cid:124)
q∇Σq v(z) = −
−λLqL

λL(q)
sτ

(s∗

i )L(q)
sτ

(s∗
j )

∂v(z)
i   s∗
j )

∂Σ(s∗

ds∗

i ds∗
j .

K

(18)

(cid:90)(cid:90)

(note that this object remains a function of the state sτ ). For general kernels k  these integrals may
only be solved numerically. However  for at least one speciﬁc choice of kernel (square-exponentials)
and parametric Ansatz  the required integrals can be solved in closed form. This analytic structure
is so interesting  and the square-exponential kernel so widely used that the “numerical” part of the
paper (Section 4) will restrict the choice of kernel to this class.
The other problem  of course  is that Equation (17) is a nontrivial differential Equation. Section 4
presents one  initial attempt at a numerical solution that should not be mistaken for a deﬁnitive answer.
Despite all this  Eq. (17) arguably constitutes a useful gain for Bayesian reinforcement learning:
It replaces the intractable deﬁnition of the value in terms of future trajectories with a differential
equation. This raises hope for new approaches to reinforcement learning  based on numerical analysis
rather than sampling.

Digression: Relaxing Some Assumptions

This paper only applies to the speciﬁc problem class of Section 2. Any generalisations and extensions
are future work  and I do not claim to solve them. But it is instructive to consider some easier
extensions  and some harder ones: For example  it is intractable to simultaneously learn both g and
f nonparametrically  if only the actual transitions are observed  because the beliefs over the two
functions become inﬁnitely dependent when conditioned on data. But if the belief on either g or f
is parametric (e.g. a general linear model)  a joint belief on g and f is tractable [see 15  §2.7]  in
fact straightforward. Both the quadratic control cost ∝ u
Ru and the control-afﬁne form (g(x  t)u)
are relaxable assumptions – other parametric forms are possible  as long as they allow for analytic
optimization of Eq. (14). On the question of learning the kernels for Gaussian process regression
on q and f or g  it is clear that standard ways of inferring kernels [15  18] can be used without
complication  but that they are not covered by the notion of optimal learning as addressed here.

(cid:124)

4 Numerically Solving the Hamilton-Jacobi-Bellman Equation

Solving Equation (16) is principally a problem of numerical analysis  and a battery of numeri-
cal methods may be considered. This section reports on one speciﬁc Ansatz  a Galerkin-type
projection analogous to the one used in [12]. For this we break with the generality of previous
sections and assume that the kernels k are given by square exponentials k(a  b) = kSE(a  b; θ  S) =
θ2 exp(− 1
S−1(a − b)) with parameters θ  S. As discussed above  we approximate by
setting Ω = 0. We ﬁnd an approximate solution through a factorizing parametric Ansatz: Let the
value of any point z ∈ H in the belief space be given through a set of parameters w and some
nonlinear functionals φ  such that their contributions separate over phase space  mean  and covariance
functions:

2 (a − b)

(cid:124)

(cid:88)

v(z) =

(cid:124)
φe(ze)

we

with φe  we ∈ RNe

(19)

e=x Σq µq Σf  µf

5

This projection is obviously restrictive  but it should be compared to the use of radial basis functions
for function approximation  a similarly restrictive framework widely used in reinforcement learning.
The functionals φ have to be chosen conducive to the form of Eq. (17). For square exponential
kernels  one convenient choice is

φa

φb

Σ(zΣ) =

s (zs) = k(sz  sa; θa  Sa)
i   s∗
i )µz(s∗

[Σz(s∗

µz(s∗

µ(zµ) =

K

φc

j ) − k(s∗
j )k(s∗

j )]k(s∗

i   s∗
i   sc  θc  Sc)k(s∗

i   sb; θb  Sb)k(s∗

j   sb; θb  Sb) ds∗

i ds∗

j

j   sc  θc  Sc) ds∗

i ds∗

j

and

(20)

(21)

(22)

(cid:90)(cid:90)
(cid:90)(cid:90)

K

(the subtracted term in the ﬁrst integral serves only numerical purposes). With this choice  the
integrals of Equation (17) can be solved analytically (solutions left out due to space constraints). The
approximate Ansatz turns Eq. (17) into an algebraic equation quadratic in wx  linear in all other we:

(cid:124)
xΨ(zx)wx − q(zx) +

w

1
2

e=x µq Σq µf  Σf

Ξe(ze)we = 0

(23)

(cid:88)

using co-vectors Ξ and a matrix Ψ with elements
(cid:124)∇xφa
Lsτ (s∗

a(zs) = γ−1φa
Ξx
a (zΣ) = γ−1φa
ΞΣ

(cid:90)(cid:90)
s (zs) − f (zx)

Σ(zΣ) + λ

s (zs) − ∇tφa
i )Lsτ (s∗
j )

(cid:90)(cid:90)

K
µ(zµ) − λ2 ¯σ2

sτ

2

K
(cid:124)
g(zx)Rg(zx)

(cid:124)
s (z)]

a (zµ) = γ−1φa
Ξµ
Ψ(z)k(cid:96) = [∇xφk

i )Lsτ (s∗
j )

Lsτ (s∗
[∇xφ(cid:96)

s(z)]

s (zs)
∂φΣ(zΣ)
∂Σz(s∗
i   s∗
j )
∂2φa
∂µz(s∗

ds∗

i ds∗

j

µ(zµ)
i )∂µz(s∗
j )

(24)

ds∗

i ds∗

j

Note that Ξµ and ΞΣ are both functions of the physical state  through sτ . It is through this functional
dependency that the value of information is associated with the physical phase space-time. To solve
for w  we simply choose a number of evaluation points zeval sufﬁcient to constrain the resulting
system of quadratic equations  and then ﬁnd the least-squares solution wopt by function minimisation 
using standard methods  such as Levenberg-Marquardt [19]. A disadvantage of this approach is that is
has a number of degrees of freedom Θ  such as the kernel parameters  and the number and locations
xa of the feature functionals. Our experiments (Section 5) suggest that it is nevertheless possible to
get interesting results simply by choosing these parameters heuristically.

5 Experiments

5.1

Illustrative Experiment on an Artiﬁcial Environment

As a simple example system with a one-dimensional state space  f  q were sampled from the model
described in Section 2  and g set to the unit function. The state space was tiled regularly  in a bounded
region  with 231 square exponential (“radial”) basis functions (Equation 20)  initially all with weight
x = 0. For the information terms  only a single basis function was used for each term (i.e. one
wi
single φΣq  one single φµq  and equally for f  all with very large length scales S  covering the entire
region of interest). As pointed out above  this does not imply a trivial structure for these terms 
because of the functional dependency on Lsτ . Five times the number of parameters  i.e. Neval = 1175
evaluation points zeval were sampled  at each time step  uniformly over the same region. It is not
intuitively clear whether each ze should have its own belief (i.e. whether the points must cover the
belief space as well as the phase space)  but anecdotal evidence from the experiments suggests that it
sufﬁces to use the current beliefs for all evaluation points. A more comprehensive evaluation of such
aspects will be the subject of a future paper. The discount factor was set to γ = 50s  the sampling
rate at λ = 2/s  the control cost at 10m2/($s). Value and optimal control were evaluated at time
steps of δt = 1/λ = 0.5s.
Figure 1 shows the situation 50s after initialisation. The most noteworthy aspect is the nontrivial
structure of exploration and exploitation terms. Despite the simplistic parameterisation of the
corresponding functionals  their functional dependence on sτ induces a complex shape. The system

6

x

x

40

20

0
−20
−40

40

20

0
−20
−40

0

20

40

60

80

100

0.5

0
−0.5
−1

0.5

0

40

20

0
−20
−40

40

20

0
−20
−40

0

20

40

60

80

100

0

20

40

60

80

100

0

20

40

60

80

100

t

t

0
−2
−4
−6
−8

0

−0.5

−1

Figure 1: State after 50 time steps  plotted over phase space-time. top left: µq (blue is good).
The belief over f is not shown  but has similar structure. top right: value estimate v at current
belief: compare to next two panels to note that the approximation is relatively coarse. bottom left:
exploration terms. bottom right: exploitation terms. At its current state (black diamond)  the system
is in the process of switching from exploitation to exploration (blue region in bottom right panel is
roughly cancelled by red  forward cone in bottom left one).

constantly balances exploration and exploitation  and the optimal balance depends nontrivially on
location  time  and the actual value (as opposed to only uncertainty) of accumulated knowledge. This
is an important insight that casts doubt on the usefulness of simple  local exploration boni  used in
many reinforcement learning algorithms.
Secondly  note that the system’s trajectory does not necessarily follow what would be the optimal
path under full information. The value estimate reﬂects this  by assigning low (good) value to regions
behind the system’s trajectory. This amounts to a sense of “remorse”: If the learner would have
known about these regions earlier  it would have strived to reach them. But this is not a sign of
sub-optimality: Remember that the value is deﬁned on the augmented space. The plots in Figure 1
are merely a slice through that space at some level set in the belief space.

5.2 Comparative Experiment – The Furuta Pendulum

The cart-and-pole system is an under-actuated problem widely studied in reinforcement learning. For
variation  this experiment uses a cylindrical version  the pendulum on the rotating arm [20]. The
task is to swing up the pendulum from the lower resting point. The table in Figure 2 compares the
average loss of a controller with access to the true f  g  q  but otherwise using Algorithm 1  to that
of an -greedy TD(λ) learner with linear function approximation  Simpkins’ et al.’s [12] Kalman
method and the Gaussian process learning controller (Fig. 2). The linear function approximation of
TD(λ) used the same radial basis functions as the three other methods. None of these methods is free
of assumptions: Note that the sampling frequency inﬂuences TD in nontrivial ways rarely studied
(for example through the coarseness of the -greedy policy). The parameters were set to γ = 5s 
λ = 50/s. Note that reinforcement learning experiments often quote total accumulated loss  which
differs from the discounted task posed to the learner. Figure 2 reports actual discounted losses. The
GP method clearly outperforms the other two learners  which barely explore. Interestingly  none of
the tested methods  not even the informed controller  achieve a stable controlled balance  although

7

u

θ1

(cid:96)1

(cid:96)2

θ2

Method
Full Information (baseline)
TD(λ)
Kalman ﬁlter Optimal Learner
Gaussian process optimal learner

cumulative loss
4.4 ±0.3
6.401±0.001
6.408±0.001
4.6 ±1.4

Figure 2: The Furuta pendulum system: A pendulum of length (cid:96)2 is attached to a rotatable arm of
length (cid:96)1. The control input is the torque applied to the arm. Right: cost to go achieved by different
methods. Lower is better. Error measures are one standard deviation over ﬁve experiments.

the GP learner does swing up the pendulum. This is due to the random  non-optimal location of basis
functions  which means resolution is not necessarily available where it is needed (in regions of high
curvature of the value function)  and demonstrates a need for better solution methods for Eq. (17).
There is of course a large number of other algorithms methods to potentially compare to  and these
results are anything but exhaustive. They should not be misunderstood as a critique of any other
method. But they highlight the need for units of measure on every quantity  and show how hard
optimal exploration and exploitation truly is. Note that  for time-varying or discounted problems 
there is no “conservative” option that cold be adopted in place of the Bayesian answer.

6 Conclusion

Gaussian process priors provide a nontrivial class of reinforcement learning problems for which
optimal reinforcement learning reduces to solving differential equations. Of course  this fact alone
does not make the problem easier  as solving nonlinear differential equations is in general intractable.
However  the ubiquity of differential descriptions in other ﬁelds raises hope that this insight opens
new approaches to reinforcement learning. For intuition on how such solutions might work  one
speciﬁc approximation was presented  using functionals to reduce the problem to ﬁnite least-squares
parameter estimation.
The critical reader will have noted how central the prior is for the arguments in Section 3: The
dynamics of the learning process are predictions of future data  thus inherently determined exclusively
by prior assumptions. One may ﬁnd this unappealing  but there is no escape from it. Minimizing
future loss requires predicting future loss  and predictions are always in danger of falling victim to
incorrect assumptions. A ﬁnite initial identiﬁcation phase may mitigate this problem by replacing
prior with posterior uncertainty – but even then  predictions and decisions will depend on the model.
The results of this paper raise new questions  theoretical and applied. The most pressing questions
concern better solution methods for Eq. (14)  in particular better means for taking the expectation
over the uncertain dynamics to more than ﬁrst order. There are also obvious probabilistic issues: Are
there other classes of priors that allow similar treatments? (Note some conceptual similarities between
this work and the BEETLE algorithm [4]). To what extent can approximate inference methods –
widely studied in combination with Gaussian process regression – be used to broaden the utility of
these results?

Acknowledgments

The author wishes to express his gratitude to Carl Rasmussen  Jan Peters  Zoubin Ghahramani 
Peter Dayan  and an anonymous reviewer  whose thoughtful comments uncovered several errors and
crucially improved this paper.

8

References
[1] R.S. Sutton and A.G. Barto. Reinforcement Learning. MIT Press  1998.
[2] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of two samples.

Biometrika  25:275–294  1933.

[3] M.O.G. Duff. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes.

PhD thesis  U of Massachusetts  Amherst  2002.

[4] P. Poupart  N. Vlassis  J. Hoey  and K. Regan. An analytic solution to discrete Bayesian reinforcement
learning. In Proceedings of the 23rd International Conference on Machine Learning  pages 697–704  2006.
[5] Richard Dearden  Nir Friedman  and David Andre. Model based Bayesian exploration. In Uncertainty in

Artiﬁcial Intelligence  pages 150–159  1999.

[6] Malcolm Strens. A Bayesian framework for reinforcement learning. In International Conference on

Machine Learning  pages 943–950  2000.

[7] T. Wang  D. Lizotte  M. Bowling  and D. Schuurmans. Bayesian sparse sampling for on-line reward

optimization. In International Conference on Machine Learning  pages 956–963  2005.

[8] J. Asmuth  L. Li  M.L. Littman  A. Nouri  and D. Wingate. A Bayesian sampling approach to exploration

in reinforcement learning. In Uncertainty in Artiﬁcial Intelligence  2009.

[9] J.Z. Kolter and A.Y. Ng. Near-Bayesian exploration in polynomial time. In Proceedings of the 26th

International Conference on Machine Learning. Morgan Kaufmann  2009.

[10] E. Todorov. Linearly-solvable Markov decision problems. Advances in Neural Information Processing

Systems  19  2007.

[11] H. J. Kappen. An introduction to stochastic control theory  path integrals and reinforcement learning. In 9th
Granada seminar on Computational Physics: Computational and Mathematical Modeling of Cooperative
Behavior in Neural Systems.  pages 149–181  2007.

[12] A. Simpkins  R. De Callafon  and E. Todorov. Optimal trade-off between exploration and exploitation. In

American Control Conference  2008  pages 33–38  2008.

[13] I. Fantoni and L. Rogelio. Non-linear Control for Underactuated Mechanical Systems. Springer  1973.
[14] A.A. Feldbaum. Dual control theory. Automation and Remote Control  21(9):874–880  April 1961.
[15] C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. MIT Press  2006.
[16] N. Wiener. Differential space. Journal of Mathematical Physics  2:131–174  1923.
[17] T. Kailath. An innovations approach to least-squares estimation — part I: Linear ﬁltering in additive white

noise. IEEE Transactions on Automatic Control  13(6):646–655  1968.

[18] I. Murray and R.P. Adams. Slice sampling covariance hyperparameters of latent Gaussian models.

arXiv:1006.0868  2010.

[19] D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society

for Industrial and Applied Mathematics  11(2):431–441  1963.

[20] K. Furuta  M. Yamakita  and S. Kobayashi. Swing-up control of inverted pendulum using pseudo-state

feedback. Journal of Systems and Control Engineering  206(6):263–269  1992.

9

,Yash Deshpande
Andrea Montanari