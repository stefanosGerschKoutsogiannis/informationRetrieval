2015,Subset Selection by Pareto Optimization,Selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection  sparse regression  dictionary learning  etc. In this paper  we propose the POSS approach which employs evolutionary Pareto optimization to find a small-sized subset with good performance. We prove that for sparse regression  POSS is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently. Particularly  for the \emph{Exponential Decay} subclass  POSS is proven to achieve an optimal solution. Empirical study verifies the theoretical results  and exhibits the superior performance of POSS to greedy and convex relaxation methods.,Subset Selection by Pareto Optimization

Chao Qian

Yang Yu

Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology  Nanjing University

Collaborative Innovation Center of Novel Software Technology and Industrialization

{qianc yuy zhouzh}@lamda.nju.edu.cn

Nanjing 210023  China

Abstract

Selecting the optimal subset from a large set of variables is a fundamental problem
in various learning tasks such as feature selection  sparse regression  dictionary
learning  etc. In this paper  we propose the POSS approach which employs evo-
lutionary Pareto optimization to ﬁnd a small-sized subset with good performance.
We prove that for sparse regression  POSS is able to achieve the best-so-far the-
oretically guaranteed approximation performance efﬁciently. Particularly  for the
Exponential Decay subclass  POSS is proven to achieve an optimal solution. Em-
pirical study veriﬁes the theoretical results  and exhibits the superior performance
of POSS to greedy and convex relaxation methods.

1

Introduction

Subset selection is to select a subset of size k from a total set of n variables for optimizing some
criterion. This problem arises in many applications  e.g.  feature selection  sparse learning and
compressed sensing. The subset selection problem is  however  generally NP-hard [13  4]. Previous
employed techniques can be mainly categorized into two branches  greedy algorithms and convex
relaxation methods. Greedy algorithms iteratively select or abandon one variable that makes the
criterion currently optimized [9  19]  which are however limited due to its greedy behavior. Convex
relaxation methods usually replace the set size constraint (i.e.  the (cid:96)0-norm) with convex constraints 
e.g.  the (cid:96)1-norm constraint [18] and the elastic net penalty [29]; then ﬁnd the optimal solutions to
the relaxed problem  which however could be distant to the true optimum.
Pareto optimization solves a problem by reformulating it as a bi-objective optimization problem
and employing a bi-objective evolutionary algorithm  which has signiﬁcantly developed recently in
theoretical foundation [22  15] and applications [16]. This paper proposes the POSS (Pareto Opti-
mization for Subset Selection) method  which treats subset selection as a bi-objective optimization
problem that optimizes some given criterion and the subset size simultaneously. To investigate the
performance of POSS  we study a representative example of subset selection  the sparse regression.
The subset selection problem in sparse regression is to best estimate a predictor variable by linear
regression [12]  where the quality of estimation is usually measured by the mean squared error  or
equivalently  the squared multiple correlation R2 [6  11]. Gilbert et al. [9] studied the two-phased
approach with orthogonal matching pursuit (OMP)  and proved the multiplicative approximation
guarantee 1 + Θ(µk2) for the mean squared error  when the coherence µ (i.e.  the maximum cor-
relation between any pair of observation variables) is O(1/k). This approximation bound was later
improved by [20  19]. Under the same small coherence condition  Das and Kempe [2] analyzed the
forward regression (FR) algorithm [12] and obtained an approximation guarantee 1− Θ(µk) for R2.
These results however will break down when µ ∈ w(1/k). By introducing the submodularity ratio
γ  Das and Kempe [3] proved the approximation guarantee 1 − e−γ on R2 by the FR algorithm;
this guarantee is considered to be the strongest since it can be applied with any coherence. Note
that sparse regression is similar to the problem of sparse recovery [7  25  21  17]  but they are for

1

different purposes. Assuming that the predictor variable has a sparse representation  sparse recovery
is to recover the exact coefﬁcients of the truly sparse solution.
We theoretically prove that  for sparse regression  POSS using polynomial time achieves a mul-
tiplicative approximation guarantee 1 − e−γ for squared multiple correlation R2  the best-so-far
guarantee obtained by the FR algorithm [3]. For the Exponential Decay subclass  which has clear
applications in sensor networks [2]  POSS can provably ﬁnd an optimal solution  while FR cannot.
The experimental results verify the theoretical results and exhibit the superior performance of POSS.
We start the rest of the paper by introducing the subset selection problem. We then present in
three subsequent sections the POSS method  its theoretical analysis for sparse regression  and the
empirical studies. The ﬁnal section concludes this paper.

2 Subset Selection

The subset selection problem originally aims at selecting a few columns from a matrix  so that the
matrix is most represented by the selected columns [1]. In this paper  we present the generalized
subset selection problem that can be applied to arbitrary criterion evaluating the selection.

2.1 The General Problem
Given a set of observation variables V = {X1  . . .   Xn}  a criterion f and a positive integer k  the
subset selection problem is to select a subset S ⊆ V such that f is optimized with the constraint
|S| ≤ k  where | · | denotes the size of a set. For notational convenience  we will not distinguish
between S and its index set IS = {i | Xi ∈ S}. Subset selection is formally stated as follows.
Deﬁnition 1 (Subset Selection). Given all variables V = {X1  . . .   Xn}  a criterion f and a posi-
tive integer k  the subset selection problem is to ﬁnd the solution of the optimization problem:

arg minS⊆V f (S)

s.t.

|S| ≤ k.

(1)

The subset selection problem is NP-hard in general [13  4]  except for some extremely simple crite-
ria. In this paper  we take sparse regression as the representative case.

2.2 Sparse Regression

Sparse regression [12] ﬁnds a sparse approximation solution to the regression problem  where the
solution vector can only have a few non-zero elements.
Deﬁnition 2 (Sparse Regression). Given all observation variables V = {X1  . . .   Xn}  a predictor
variable Z and a positive integer k  deﬁne the mean squared error of a subset S ⊆ V as

M SEZ S = minα∈R|S| E(cid:104)

(Z −(cid:88)

αiXi)2(cid:105)

.

Sparse regression is to ﬁnd a set of at most k variables minimizing the mean squared error  i.e. 

arg minS⊆V M SEZ S

s.t.

i∈S

|S| ≤ k.

For the ease of theoretical treatment  the squared multiple correlation
Z S = (V ar(Z) − M SEZ S)/V ar(Z)
R2

is used to replace M SEZ S [6  11] so that the sparse regression is equivalently

arg maxS⊆V R2

Z S

s.t.

|S| ≤ k.

(2)

Sparse regression is a representative example of subset selection [12]. Note that we will study Eq. (2)
in this paper. Without loss of generality  we assume that all random variables are normalized to have
Z S is simpliﬁed to be 1 − M SEZ S.
expectation 0 and variance 1. Thus  R2
For sparse regression  Das and Kempe [3] proved that the forward regression (FR) algorithm  pre-
sented in Algorithm 1  can produce a solution SF R with |SF R| = k and R2
−γSF R  k ) ·
OP T (where OP T denotes the optimal function value of Eq. (2))  which is the best currently known
approximation guarantee. The FR algorithm is a greedy approach  which iteratively selects a vari-
able with the largest R2 improvement.

Z SF R ≥ (1−e

2

Algorithm 1 Forward Regression
Input: all variables V = {X1  . . .   Xn}  a predictor variable Z and an integer parameter k ∈ [1  n]
Output: a subset of V with k variables
Process:
1: Let t = 0 and St = ∅.
2: repeat
3:
4:
5: until t = k
6: return Sk

Let X∗ be a variable maximizing R2
Let St+1 = St ∪ {X∗}  and t = t + 1.

Z St∪{X}  i.e.  X∗ = arg maxX∈V \St R2

Z St∪{X}.

3 The POSS Method

The subset selection in Eq. (1) can be separated into two objectives  one optimizes the criterion  i.e. 
minS⊆V f (S)  meanwhile the other keeps the size small  i.e.  minS⊆V max{|S| − k  0}. Usually
the two objectives are conﬂicting  that is  a subset with a better criterion value could have a larger
size. The POSS method solves the two objectives simultaneously  which is described as follows.
Let us use the binary vector representation for subsets membership indication  i.e.  s ∈ {0  1}n
represents a subset S of V by assigning si = 1 if the i-th element of V is in S and si = 0 otherwise.
We assign two properties for a solution s: o1 is the criterion value and o2 is the sparsity 

(cid:26)+∞ 

s.o1 =

f (s)  otherwise

s = {0}n  or |s| ≥ 2k

s.o2 = |s|.

 

where the set of o1 to +∞ is to exclude trivial or overly bad solutions. We further introduce the
isolation function I : {0  1}n → R as in [22]  which determines if two solutions are allowed to be
compared: they are comparable only if they have the same isolation function value. The implemen-
tation of I is left as a parameter of the method  while its effect will be clear in the analysis.
As will be introduced later  we need to compare solutions. For solutions s and s(cid:48)  we ﬁrst judge if
they have the same isolation function value. If not  we say that they are incomparable. If they have
the same isolation function value  s is worse than s(cid:48) if s(cid:48) has a smaller or equal value on both the
properties; s is strictly worse if s(cid:48) has a strictly smaller value in one property  and meanwhile has a
smaller or equal value in the other property. But if both s is not worse than s(cid:48) and s(cid:48) is not worse
than s  we still say that they are incomparable.
POSS is described in Algorithm 2. Starting from the solution representing an empty set and the
archive P containing only the empty set (line 1)  POSS generates new solutions by randomly ﬂip-
ping bits of an archived solution (in the binary vector representation)  as lines 4 and 5. Newly
generated solutions are compared with the previously archived solutions (line 6). If the newly gen-
erated solution is not strictly worse than any previously archived solution  it will be archived. Before
archiving the newly generated solution in line 8  the archive set P is cleaned by removing solutions
in Q  which are previously archived solutions but are worse than the newly generated solution.
The iteration of POSS repeats for T times. Note that T is a parameter  which could depend on the
available resource of the user. We will analyze the relationship between the solution quality and T in
later sections  and will use the theoretically derived T value in the experiments. After the iterations 
we select the ﬁnal solution from the archived solutions according to Eq. (1)  i.e.  select the solution
with the smallest f value while the constraint on the set size is kept (line 12).

4 POSS for Sparse Regression

In this section  we examine the theoretical performance of the POSS method for sparse regression.
For sparse regression  the criterion f is implemented as f (s) = −R2
Z s. Note that minimizing
−R2
We need some notations for the analysis. Let Cov(· ·) be the covariance between two random
variables  C be the covariance matrix between all observation variables  i.e.  Ci j = Cov(Xi  Xj) 

Z s is equivalent to the original objective that maximizes R2

Z s in Eq. (2).

3

Algorithm 2 POSS
Input: all variables V = {X1  . . .   Xn}  a given criterion f and an integer parameter k ∈ [1  n]
Parameter: the number of iterations T and an isolation function I : {0  1}n → R
Output: a subset of V with at most k variables
Process:
1: Let s = {0}n and P = {s}.
2: Let t = 0.
3: while t < T do
4:
5:
6:

if (cid:64)z ∈ P such that I(z) = I(s(cid:48)) and(cid:0)(z.o1 < s(cid:48).o1 ∧ z.o2 ≤ s(cid:48).o2) or (z.o1 ≤ s(cid:48).o1 ∧
z.o2 < s(cid:48).o2)(cid:1) then

Select s from P uniformly at random.
Generate s(cid:48) from s by ﬂipping each bit of s with probability 1/n.

Q = {z ∈ P | I(z) = I(s(cid:48)) ∧ s(cid:48).o1 ≤ z.o1 ∧ s(cid:48).o2 ≤ z.o2}.
P = (P \ Q) ∪ {s(cid:48)}.

7:
8:
9:
10:
11: end while
12: return arg mins∈P |s|≤k f (s)

end if
t = t + 1.

elements bi with i ∈ S. Let Res(Z  S) = Z −(cid:80)

and b be the covariance vector between Z and observation variables  i.e.  bi = Cov(Z  Xi). Let CS
denote the submatrix of C with row and column set S  and bS denote the subvector of b  containing
i∈S αiXi denote the residual of Z with respect to
S  where α ∈ R|S| is the least square solution to M SEZ S [6]. The submodularity ratio presented
in Deﬁnition 3 is a measure characterizing how close a set function f is to submodularity. It is easy
to see that f is submodular iff γU k(f ) ≥ 1 for any U and k. For f being the objective function R2 
we will use γU k shortly in the paper.
Deﬁnition 3 (Submodularity Ratio [3]). Let f be a non-negative set function. The submodularity
ratio of f with respect to a set U and a parameter k ≥ 1 is

(cid:80)

x∈S(f (L ∪ {x}) − f (L))

f (L ∪ S) − f (L)

.

γU k(f ) =

min

L⊆U S:|S|≤k S∩L=∅

4.1 On General Sparse Regression

Our ﬁrst result is the theoretical approximation bound of POSS for sparse regression in Theorem 1.
Let OP T denote the optimal function value of Eq. (2). The expected running time of POSS is the
average number of objective function (i.e.  R2) evaluations  the most time-consuming step  which
is also the average number of iterations T (denoted by E[T ]) since it only needs to perform one
objective evaluation for the newly generated solution s(cid:48) in each iteration.
Theorem 1. For sparse regression  POSS with E[T ] ≤ 2ek2n and I(·) = 0 (i.e.  a constant func-
tion) ﬁnds a set S of variables with |S| ≤ k and R2
The proof relies on the property of R2 in Lemma 1  that for any subset of variables  there always
exists another variable  the inclusion of which can bring an improvement on R2 proportional to the
current distance to the optimum. Lemma 1 is extracted from the proof of Theorem 3.2 in [3].
Lemma 1. For any S ⊆ V   there exists one variable ˆX ∈ V − S such that

Z S ≥ (1 − e−γ∅ k ) · OP T .

Z S∪{ ˆX} − R2
R2

Z S ≥ γ∅ k
k

(OP T − R2

Z S).

Z S + R2

Z S(cid:48). Because R2

we get (cid:80)

k be the optimal set of variables of Eq. (2)  i.e.  R2
Z S∗
k ⊆ S ∪ ¯S  we have R2
Z S increases with S and S∗
Z S. By Deﬁnition 3  |S(cid:48)| = | ¯S| ≤ k and R2

k − S
Proof. Let S∗
and S(cid:48) = {Res(X  S) | X ∈ ¯S}. Using Lemmas 2.3 and 2.4 in [2]  we can easily derive that
Z S∪ ¯S ≥
Z S∪ ¯S = R2
R2
Z ∅ = 0 
R2
Z S∗
Z S). Let ˆX(cid:48) = arg maxX(cid:48)∈S(cid:48) R2
Z X(cid:48).
Z S). Let ˆX ∈ ¯S correspond to ˆX(cid:48)  i.e. 
k (OP T −R2

Z S(cid:48) ≥ γ∅ k(OP T − R2
k (OP T − R2
Z S) ≥ γ∅ k
Z  ˆX(cid:48) ≥ γ∅ k

Z X(cid:48) ≥ γ∅ kR2
X(cid:48)∈S(cid:48) R2
Z  ˆX(cid:48) ≥ γ∅ k|S(cid:48)| (OP T − R2

Then  R2
Res( ˆX  S) = ˆX(cid:48). Thus  R2

= OP T . Let ¯S = S∗

Z S(cid:48) ≥ OP T − R2

Z S). The lemma holds.

= OP T . Thus  R2

Z S∪{ ˆX}−R2

Z S = R2

k

k

4

Z s(cid:48) ≥ (1 − γ∅ k
R2
k

)R2

Z s +

γ∅ k
k

· OP T ≥ (1 − (1 − γ∅ k
k

Z s ≥ (1 − (1 − γ∅ k

Z s ≥ (1 − (1 − γ∅ k

Z s ≥ (1 − (1 − γ∅ k

Proof of Theorem 1. Since the isolation function is a constant function  all solutions are allowed
to be compared and we can ignore it. Let Jmax denote the maximum value of j ∈ [0  k] such that in
the archive set P   there exists a solution s with |s| ≤ j and R2
k )j) · OP T . That
is  Jmax = max{j ∈ [0  k] | ∃s ∈ P |s| ≤ j ∧ R2
k )j) · OP T}. We then analyze
the expected iterations until Jmax = k  which implies that there exists one solution s in P satisfying
that |s| ≤ k and R2
The initial value of Jmax is 0  since POSS starts from {0}n. Assume that currently Jmax = i < k.
Let s be a corresponding solution with the value i  i.e.  |s| ≤ i and R2
k )i)· OP T .
It is easy to see that Jmax cannot decrease because cleaning s from P (lines 7 and 8 of Algorithm 2)
implies that s is “worse” than a newly generated solution s(cid:48)  which must have a smaller size and a
larger R2 value. By Lemma 1  we know that ﬂipping one speciﬁc 0 bit of s (i.e.  adding a speciﬁc
k (OP T −
variable into S) can generate a new solution s(cid:48)  which satisﬁes that R2
Z s). Then  we have
R2

k )k) · OP T ≥ (1 − e−γ∅ k ) · OP T .

Z s ≥ (1− (1− γ∅ k

Z s ≥ γ∅ k

Z s(cid:48) − R2
)i+1) · OP T.

1

1

Pmax

Pmax

enPmax

n (1 − 1

· 1
n (1− 1

n )n−1 ≥ 1

Since |s(cid:48)| = |s| + 1 ≤ i + 1  s(cid:48) will be included into P ; otherwise  from line 6 of Algorithm 2  s(cid:48)
must be “strictly worse” than one solution in P   and this implies that Jmax has already been larger
than i  which contradicts with the assumption Jmax = i. After including s(cid:48)  Jmax ≥ i + 1. Let Pmax
denote the largest size of P . Thus  Jmax can increase by at least 1 in one iteration with probability at
is a lower bound on the probability of selecting s in
  where
least
n )n−1 is the probability of ﬂipping a speciﬁc bit of s and keeping
line 4 of Algorithm 2 and 1
other bits unchanged in line 5. Then  it needs at most enPmax expected iterations to increase Jmax.
Thus  after k · enPmax expected iterations  Jmax must have reached k.
By the procedure of POSS  we know that the solutions maintained in P must be incomparable.
Thus  each value of one property can correspond to at most one solution in P . Because the solutions
with |s| ≥ 2k have +∞ value on the ﬁrst property  they must be excluded from P . Thus  |s| ∈
{0  1  . . .   2k − 1}  which implies that Pmax ≤ 2k. Hence  the expected number of iterations E[T ]
(cid:3)
for ﬁnding the desired solution is at most 2ek2n.
−γSF R  k ) · OP T [3]  it is easy to see
Comparing with the approximation guarantee of FR  (1 − e
that γ∅ k ≥ γSF R k from Deﬁnition 3. Thus  POSS with the simplest conﬁguration of the isolation
function can do at least as well as FR on any sparse regression problem  and achieves the best
previous approximation guarantee. We next investigate if POSS can be strictly better than FR.

4.2 On The Exponential Decay Subclass

Our second result is on a subclass of sparse regression  called Exponential Decay as in Deﬁnition 4.
In this subclass  the observation variables can be ordered in a line such that their covariances are
decreasing exponentially with the distance.
Deﬁnition 4 (Exponential Decay [2]). The variables Xi are associated with points y1 ≤ y2 ≤ . . . ≤
yn  and Ci j = a|yi−yj| for some constant a ∈ (0  1).
Since we have shown that POSS with a constant isolation function is generally good  we prove
below that POSS with a proper isolation function can be even better: it is strictly better than FR
on the Exponential Decay subclass  as POSS ﬁnds an optimal solution (i.e.  Theorem 2) while FR
cannot (i.e.  Proposition 1). The isolation function I(s ∈ {0  1}n) = min{i | si = 1} implies that
two solutions are comparable only if they have the same minimum index for bit 1.
Theorem 2. For the Exponential Decay subclass of sparse regression  POSS with E[T ] ∈ O(k2(n−
k)n log n) and I(s ∈ {0  1}n) = min{i | si = 1} ﬁnds an optimal solution.
The proof of Theorem 2 utilizes the dynamic programming property of the problem  as in Lemma 2.
Lemma 2. [2] Let R2(v  j) denote the maximum R2
Z S value by choosing v variables  including
Z S | S ⊆ {Xj  . . .   Xn}  Xj ∈
necessarily Xj  from Xj  . . .   Xn. That is  R2(v  j) = max{R2
(cid:17)
S |S| = v}. Then  the following recursive relation holds:
where the term in(cid:0)(cid:1) is the R2 value by adding Xj into the variable subset corresponding to R2(v  i).
R2(v + 1  j) = maxj+1≤i≤n

j + (bj − bi)2 a2|yi−yj|

1 − a2|yi−yj| − 2bjbi

1 + a|yi−yj|

R2(v  i) + b2

a|yi−yj|

(cid:16)

 

5

1

1

sired solutions to ﬁnd in the i-th phase. Then  E[ξi] ≤(cid:80)n−i

Proof of Theorem 2. We divide the optimization process into k + 1 phases  where the i-th (1 ≤
i ≤ k) phase starts after the (i−1)-th phase has ﬁnished. We deﬁne that the i-th phase ﬁnishes when
for each solution corresponding to R2(i  j) (1 ≤ j ≤ n − i + 1)  there exists one solution in the
archive P which is “better” than it. Here  a solution s is “better” than s(cid:48) is equivalent to that s(cid:48) is
“worse” than s. Let ξi denote the iterations since phase i− 1 has ﬁnished  until phase i is completed.
Starting from the solution {0}n  the 0-th phase has ﬁnished. Then  we consider ξi (i ≥ 1). In this
phase  from Lemma 2  we know that a solution “better” than a corresponding solution of R2(i  j) can
be generated by selecting a speciﬁc one from the solutions “better” than R2(i−1  j +1)  . . .   R2(i−
1  n) and ﬂipping its j-th bit  which happens with probability at least
.
enPmax
Thus  if we have found L desired solutions in the i-th phase  the probability of ﬁnding a new desired
solution in the next iteration is at least (n−i+1−L)·
  where n−i+1 is the total number of de-
n−i+1−L ∈ O(n log nPmax). There-
fore  the expected number of iterations E[T ] is O(kn log nPmax) until the k-th phase ﬁnishes  which
implies that an optimal solution corresponding to max1≤j≤n R2(k  j) has been found. Note that
Pmax ≤ 2k(n−k)  because the incomparable property of the maintained solutions by POSS ensures
that there exists at most one solution in P for each possible combination of |s| ∈ {0  1  . . .   2k − 1}
and I(s) ∈ {0  1  . . .   n}. Thus  E[T ] for ﬁnding an optimal solution is O(k2(n − k)n log n). (cid:3)
Then  we analyze FR (i.e.  Algorithm 1) for this special class. We show below that FR can be
blocked from ﬁnding an optimal solution by giving a simple example.
Example 1. X1 = Y1  Xi = riXi−1 + Yi  where ri ∈ (0  1)  and Yi are independent random
variables with expectation 0 such that each Xi has variance 1.

n )n−1 ≥ 1

· 1
n (1− 1

enPmax

enPmax

Pmax

L=0

For i < j  Cov(Xi  Xj) = (cid:81)j
Exponential Decay class by letting y1 = 0 and yi =(cid:80)i

k=i+1 rk. Then  it is easy to verify that Example 1 belongs to the

k=2 loga rk for i ≥ 2.

Proposition 1. For Example 1 with n = 3  r2 = 0.03  r3 = 0.5  Cov(Y1  Z) = Cov(Y2  Z) = δ
and Cov(Y3  Z) = 0.505δ  FR cannot ﬁnd the optimal solution for k = 2.
Proof. The covariances between Xi and Z are b1 = δ  b2 = 0.03b1 + δ = 1.03δ and b3 = 0.5b2 +
Z S can be simply represented
0.505δ = 1.02δ. Since Xi and Z have expectation 0 and variance 1  R2
= 1.0609δ2 
as bT
Z {X2 X3} = 1.4009δ2.
Z {X1 X3} = 2.0103δ2  R2
R2
The optimal solution for k = 2 is {X1  X3}. FR ﬁrst selects X2 since R2
is the largest  then
Z X2
selects X1 since R2

S bS [11]. We then calculate the R2 value as follows: R2
= 1.0404δ2; R2

Z {X2 X3}; thus produces a local optimal solution {X1  X2}.

Z {X1 X2} = 2.0009δ2  R2

Z {X2 X1} > R2

S C−1

= δ2  R2

Z X3

Z X2

Z X1

It is also easy to verify that other two previous methods OMP [19] and FoBa [26] cannot ﬁnd the
optimal solution for this example  due to their greedy nature.

5 Empirical Study

We conducted experiments on 12 data sets1 in Table 1 to compare POSS with the following methods:
• FR [12] iteratively adds one variable with the largest improvement on R2.
• OMP [19] iteratively adds one variable that mostly correlates with the predictor variable residual.
• FoBa [26] is based on OMP but deletes one variable adaptively when beneﬁcial. Set parameter
ν = 0.5  the solution path length is ﬁve times as long as the maximum sparsity level (i.e.  5 × k) 
and the last active set containing k variables is used as the ﬁnal selection [26].
• RFE [10] iteratively deletes one variable with the smallest weight by linear regression.
• Lasso [18]  SCAD [8] and MCP [24] replaces the (cid:96)0 norm constraint with the (cid:96)1 norm penalty 
the smoothly clipped absolute deviation penalty and the mimimax concave penalty  respectively. For
implementing these methods  we use the SparseReg toolbox developed in [28  27].
For POSS  we use I(·) = 0 since it is generally good  and the number of iterations T is set to be
(cid:98)2ek2n(cid:99) as suggested by Theorem 1. To evaluate how far these methods are from the optimum  we
also compute the optimal subset by exhaustive enumeration  denoted as OPT.

1The data sets are from http://archive.ics.uci.edu/ml/ and http://www.csie.ntu.
edu.tw/˜cjlin/libsvmtools/datasets/. Some binary classiﬁcation data are used for regression.
All variables are normalized to have mean 0 and variance 1.

6

data set
housing
eunite2001
svmguide3
ionosphere

#inst
506
367
1284
351

#feat
13
16
21
34

Table 1: The data sets.
data set
sonar
triazines
coil2000
mushrooms

#inst
208
186
9000
8124

#feat
60
60
86
112

data set
clean1
w5a
gisette
farm-ads

#inst
476
9888
7000
4143

#feat
166
300
5000
54877

Table 2: The training R2 value (mean±std.) of the compared methods on 12 data sets for k = 8. In
each data set  ‘•/◦’ denote respectively that POSS is signiﬁcantly better/worse than the correspond-
ing method by the t-test [5] with conﬁdence level 0.05. ‘-’ means that no results were obtained after
running several days.
Data set
housing
eunite2001
svmguide3
ionosphere
sonar
triazines
coil2000
mushrooms
clean1
w5a
gisette
farm-ads

.7415±.0300•
.8349±.0150•
.2557±.0270•
.5921±.0353•
.5112±.0425•
.4073±.0591•
.0619±.0075•
.9909±.0022•
.4132±.0315•
.3313±.0246•
.6731±.0134•
.4170±.0113•

.7423±.0301•
.8442±.0144•
.2601±.0279•
.5929±.0346•
.5138±.0432•
.4107±.0600•
.0619±.0075•
.9909±.0022•
.4145±.0309•
.3341±.0258•
.6747±.0145•
.4170±.0113•

.7354±.0297•
.8320±.0150•
.2397±.0237•
.5740±.0348•
.4496±.0482•
.3793±.0584•
.0570±.0075•
.8652±.0474•
.3563±.0364•
.2694±.0385•
.5709±.0123•
.3771±.0110•

.7429±.0300•
.8348±.0143•
.2615±.0260•
.5920±.0352•
.5171±.0440•
.4150±.0592•
.0624±.0076•
.9909±.0021•
.4169±.0299•
.3319±.0247•
.7001±.0116•
.4196±.0101•

.7388±.0304•
.8424±.0153•
.2136±.0325•
.5832±.0415•
.4321±.0636•
.3615±.0712•
.0363±.0141•
.6813±.1294•
.1596±.0562•
.3342±.0276•
.5360±.0318•

POSS

.7437±.0297
.8482±.0132
.2701±.0257
.5990±.0329
.5365±.0410
.4301±.0603
.0627±.0076
.9912±.0020
.4368±.0300
.3376±.0267
.7265±.0098
.4217±.0100

OPT

.7437±.0297
.8484±.0132
.2705±.0255
.5995±.0326

–
–
–
–
–
–
–
–
POSS: win/tie/loss

–

11/0/0

12/0/0

12/0/0

12/0/0

12/0/0

FR

FoBa

OMP

RFE

MCP

–

To assess each method on each data set  we repeat the following process 100 times. The data set is
randomly and evenly split into a training set and a test set. Sparse regression is built on the training
set  and evaluated on the test set. We report the average training and test R2 values.

5.1 On Optimization Performance

Table 2 lists the training R2 for k = 8  which reveals the optimization quality of the methods. Note
that the results of Lasso  SCAD and MCP are very close  and we only report that of MCP due to the
page limit. By the t-test [5] with signiﬁcance level 0.05  POSS is shown signiﬁcantly better than all
the compared methods on all data sets.
We plot the performance curves on two data sets for k ≤ 8 in Figure 1. For sonar  OPT is calculated
only for k ≤ 5. We can observe that POSS tightly follows OPT  and has a clear advantage over
the rest methods. FR  FoBa and OMP have close performances  while are much better than MCP 
SCAD and Lasso. The bad performance of Lasso is consistent with the previous results in [3  26].
We notice that  although the (cid:96)1 norm constraint is a tight convex relaxation of the (cid:96)0 norm constraint
and can have good results in sparse recovery tasks  the performance of Lasso is not as good as POSS
and greedy methods on most data sets. This is due to that  unlike assumed in sparse recovery tasks 
there may not exist a sparse structure in the data sets. In this case  (cid:96)1 norm constraint can be a bad
approximation of (cid:96)0 norm constraint. Meanwhile  (cid:96)1 norm constraint also shifts the optimization
problem  making it hard to well optimize the original R2 criterion.
Considering the running time (in the number of objective function evaluations)  OPT does exhaustive
kk time  which could be unacceptable for a slightly large data set. FR 
FoBa and OMP are greedy-like approaches  thus are efﬁcient and their running time are all in the
order of kn. POSS ﬁnds the solutions closest to those of OPT  taking 2ek2n time. Although POSS
is slower by a factor of k  the difference would be small when k is a small constant.
Since the 2ek2n time is a theoretical upper bound for POSS being as good as FR  we empirically
examine how tight this bound is. By selecting FR as the baseline  we plot the curve of the R2
value over the running time for POSS on the two largest data sets gisette and farm-ads  as shown
in Figure 2. We do not split the training and test set  and the curve for POSS is the average of 30
independent runs. The x-axis is in kn  the running time of FR. We can observe that POSS takes
about only 14% and 23% of the theoretical time to achieve a better performance  respectively on the
two data sets. This implies that POSS can be more efﬁcient in practice than in theoretical analysis.

search  thus needs(cid:0)n

(cid:1) ≥ nk

k

7

(a) on svmguide3

(b) on sonar

(a) on gisette

(b) on farm-ads

Figure 1: Training R2 (the larger the better).

Figure 2: Performance v.s. running time of POSS.

(a) on svmguide3

(b) on sonar

Figure 3: Test R2 (the larger the better).

(a) on training set (RSS)

(b) on test set

Figure 4: Sparse regression with (cid:96)2 regularization
on sonar. RSS: the smaller the better.

5.2 On Generalization Performance

When testing sparse regression on the test data  it has been known that the sparsity alone may be
not a good complexity measure [26]  since it only restricts the number of variables  but the range of
the variables is unrestricted. Thus better optimization does not always lead to better generalization
performance. We also observe this in Figure 3. On svmguide3  test R2 is consistent with training R2
in Figure 1(a)  however on sonar  better training R2 (as in Figure 1(b)) leads to worse test R2 (as in
Figure 3(b))  which may be due to the small number of instances making it prone to overﬁtting.
As suggested in [26]  other regularization terms may be necessary. We add the (cid:96)2 norm regulariza-
tion into the objective function  i.e. 

RSSZ S = minα∈R|S| E(cid:2)(Z −(cid:80)

i∈S αiXi)2(cid:3) + λ|α|2

2.

|S| ≤ k . We then test all the compared
The optimization is now arg minS⊆V RSSZ S s.t.
methods to solve this optimization problem with λ = 0.9615 on sonar. As plotted in Figure 4  we
can observe that POSS still does the best optimization on the training RSS  and by introducing the
(cid:96)2 norm  it leads to the best generalization performance in R2.

6 Conclusion

In this paper  we study the problem of subset selection  which has many applications ranging from
machine learning to signal processing. The general goal is to select a subset of size k from a large
set of variables such that a given criterion is optimized. We propose the POSS approach that solves
the two objectives of the subset selection problem simultaneously  i.e.  optimizing the criterion and
reducing the subset size.
On sparse regression  a representative of subset selection  we theoretically prove that a simple POSS
(i.e.  using a constant isolation function) can generally achieve the best previous approximation
guarantee  using time 2ek2n. Moreover  we prove that  with a proper isolation function  it ﬁnds an
optimal solution for an important subclass Exponential Decay using time O(k2(n−k)n log n)  while
other greedy-like methods may not ﬁnd an optimal solution. We verify the superior performance of
POSS by experiments  which also show that POSS can be more efﬁcient than its theoretical time.
We will further study Pareto optimization from the aspects of using potential heuristic operators [14]
and utilizing infeasible solutions [23]; and try to apply it to more machine learning tasks.

Acknowledgements We want to thank Lijun Zhang and Jianxin Wu for their helpful comments.
This research was supported by 973 Program (2014CB340501) and NSFC (61333014  61375061).

8

34567kOPTPOSSFRFoBaOMPRFEMCPSCADLasso3456780.180.20.220.240.26kR23456780.250.30.350.40.450.50.55kR2102030400.640.660.680.70.72Running time in knR2POSSFR6kn2ek2n= 43kn102030400.390.40.410.42Running time in knR2POSSFR10kn2ek2n= 43kn3456780.160.180.20.22kR23456780.050.10.150.2kR256780.70.710.720.730.74kRSS56780.230.2350.240.2450.250.2550.26kR2References
[1] C. Boutsidis  M. W. Mahoney  and P. Drineas. An improved approximation algorithm for the column

subset selection problem. In SODA  pages 968–977  New York  NY  2009.

[2] A. Das and D. Kempe. Algorithms for subset selection in linear regression.

Victoria  Canada  2008.

In STOC  pages 45–54 

[3] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection  sparse ap-

proximation and dictionary selection. In ICML  pages 1057–1064  Bellevue  WA  2011.

[4] G. Davis  S. Mallat  and M. Avellaneda. Adaptive greedy approximations. Constructive Approximation 

13(1):57–98  1997.

[5] J. Demˇsar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine Learning

Research  7:1–30  2006.

[6] G. Diekhoff. Statistics for the Social and Behavioral Sciences: Univariate  Bivariate  Multivariate.

William C Brown Pub  1992.

[7] D. L. Donoho  M. Elad  and V. N. Temlyakov. Stable recovery of sparse overcomplete representations in

the presence of noise. IEEE Transactions on Information Theory  52(1):6–18  2006.

[8] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal

of the American Statistical Association  96(456):1348–1360  2001.

[9] A. C. Gilbert  S. Muthukrishnan  and M. J. Strauss. Approximation of functions over redundant dictio-

naries using coherence. In SODA  pages 243–252  Baltimore  MD  2003.

[10] I. Guyon  J. Weston  S. Barnhill  and V. Vapnik. Gene selection for cancer classiﬁcation using support

vector machines. Machine Learning  46(1-3):389–422  2002.

[11] R. A. Johnson and D. W. Wichern. Applied Multivariate Statistical Analysis. Pearson  6th edition  2007.
[12] A. Miller. Subset Selection in Regression. Chapman and Hall/CRC  2nd edition  2002.
[13] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing  24(2):227–

234  1995.

[14] C. Qian  Y. Yu  and Z.-H. Zhou. An analysis on recombination in multi-objective evolutionary optimiza-

tion. Artiﬁcial Intelligence  204:99–119  2013.

[15] C. Qian  Y. Yu  and Z.-H. Zhou. On constrained Boolean Pareto optimization. In IJCAI  pages 389–395 

Buenos Aires  Argentina  2015.

[16] C. Qian  Y. Yu  and Z.-H. Zhou. Pareto ensemble pruning. In AAAI  pages 2935–2941  Austin  TX  2015.
[17] M. Tan  I. Tsang  and L. Wang. Matching pursuit LASSO Part I: Sparse recovery over big dictionary.

IEEE Transactions on Signal Processing  63(3):727–741  2015.

[18] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:

Series B (Methodological)  58(1):267–288  1996.

[19] J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Infor-

mation Theory  50(10):2231–2242  2004.

[20] J. A. Tropp  A. C. Gilbert  S. Muthukrishnan  and M. J. Strauss. Improved sparse approximation over

quasiincoherent dictionaries. In ICIP  pages 37–40  Barcelona  Spain  2003.

[21] L. Xiao and T. Zhang. A proximal-gradient homotopy method for the sparse least-squares problem. SIAM

Journal on Optimization  23(2):1062–1091  2013.

[22] Y. Yu  X. Yao  and Z.-H. Zhou. On the approximation ability of evolutionary optimization with application

to minimum set cover. Artiﬁcial Intelligence  180-181:20–33  2012.

[23] Y. Yu and Z.-H. Zhou. On the usefulness of infeasible solutions in evolutionary search: A theoretical

study. In IEEE CEC  pages 835–840  Hong Kong  China  2008.

[24] C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics 

38(2):894–942  2010.

[25] T. Zhang. On the consistency of feature selection using greedy least squares regression. Journal of

Machine Learning Research  10:555–568  2009.

[26] T. Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations. IEEE Trans-

actions on Information Theory  57(7):4689–4708  2011.

[27] H. Zhou. Matlab SparseReg Toolbox Version 0.0.1. Available Online  2013.
[28] H. Zhou  A. Armagan  and D. Dunson. Path following and empirical Bayes model selection for sparse

regression. arXiv:1201.3528  2012.

[29] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal

Statistical Society: Series B (Statistical Methodology)  67(2):301–320  2005.

9

,Behzad Golshan
John Byers
Evimaria Terzi
Chao Qian
Yang Yu
Zhi-Hua Zhou
Minje Jang
Sunghyun Kim
Changho Suh
Sewoong Oh
Simon Lyddon
Stephen Walker
Chris Holmes
Scott Gigante
Adam Charles
Smita Krishnaswamy
Gal Mishne