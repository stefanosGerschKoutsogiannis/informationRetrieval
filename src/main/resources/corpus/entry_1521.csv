2009,Directed Regression,When used to guide decisions  linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships  it is beneficial to account for the decision objective when computing regression coefficients. Empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. We propose directed regression  an efficient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. We also develop a theory that motivates the algorithm.,Directed Regression

Yi-hao Kao

Stanford University
Stanford  CA 94305

Benjamin Van Roy
Stanford University
Stanford  CA 94305

Xiang Yan

Stanford University
Stanford  CA 94305

yihaokao@stanford.edu

bvr@stanford.edu

xyan@stanford.edu

Abstract

When used to guide decisions  linear regression analysis typically involves esti-
mation of regression coefﬁcients via ordinary least squares and their subsequent
use to make decisions. When there are multiple response variables and features
do not perfectly capture their relationships  it is beneﬁcial to account for the de-
cision objective when computing regression coefﬁcients. Empirical optimization
does so but sacriﬁces performance when features are well-chosen or training data
are insufﬁcient. We propose directed regression  an efﬁcient algorithm that com-
bines merits of ordinary least squares and empirical optimization. We demonstrate
through a computational study that directed regression can generate signiﬁcant
performance gains over either alternative. We also develop a theory that motivates
the algorithm.

1 Introduction

When used to guide decision-making  linear regression analysis typically treats estimation of re-
gression coefﬁcients separately from their use to make decisions. In particular  estimation is carried
out via ordinary least squares (OLS) without consideration of the decision objective. The regression
coefﬁcients are then used to optimize decisions.
When there are multiple response variables and features do not perfectly capture their relationships 
it is beneﬁcial to account for the decision objective when computing regression coefﬁcients. Im-
perfections in feature selection are common since it is difﬁcult to identify the right features and the
number of features is typically restricted in order to avoid over-ﬁtting.
Empirical optimization (EO) is an alternative to OLS which selects coefﬁcients that minimize em-
pirical loss in the training data. Though it accounts for the decision objective when computing
regression coefﬁcients  EO sacriﬁces performance when features are well-chosen or training data is
insufﬁcient.
In this paper  we propose a new algorithm – directed regression (DR) – which is a hybrid between
OLS and EO. DR selects coefﬁcients that are a convex combination of those that would be se-
lected by OLS and those by EO. The weights of OLS and EO coefﬁcients are optimized via cross-
validation.
We study DR for the case of decision problems with quadratic objective functions. The algorithm
takes as input a training set of data pairs  each consisting of feature vectors and response variables 
together with a quadratic loss function that depends on decision variables and response variables.
Regression coefﬁcients are computed for subsequent use in decision-making. Each future decision
depends on newly sampled feature vectors and is made prior to observing response variables with
the goal of minimizing expected loss.
We present computational results demonstrating that DR can substantially outperform both OLS and
EO. These results are for synthetic problems with regression models that include subsets of relevant

1

features. In some cases  OLS and EO deliver comparable performance while DR reduces expected
loss by about 20%. In none of the cases considered does either OLS or EO outperform DR.
We also develop a theory that motivates DR. This theory is based on a model in which selected
features do not perfectly capture relationships among response variables. We prove that  for this
model  the optimal vector of coefﬁcients is a convex combination of those that would be generated
by OLS and EO.

2 Linear Regression for Decision-Making

(cid:80)

1   . . .   x(n)

Suppose we are given a set of training data pairs O = {(x(1)  y(1)) ···   (x(N )  y(N ))}. Each nth
K ∈ (cid:60)M and a vector y(n) ∈ (cid:60)M of response
data pair is comprised of feature vectors x(n)
variables. We would like to compute regression coefﬁcients r ∈ (cid:60)K so that given a data pair (x  y) 
k rkxk of feature vectors estimates the expectation of y conditioned on x.
the linear combination
We restrict attention to cases where M > 1  with special interest in problems where M is large 
because it is in such situations that DR offers the largest performance gains.
We consider a setting where the regression model is used to guide future decisions. In particular 
after computing regression coefﬁcients  each time we observe feature vectors x1  . . .   xK we will
have to select a decision u ∈ (cid:60)L before observing the response vector y. The choice incurs a loss

(cid:96)(u  y) = u(cid:62)G1u + u(cid:62)G2y 

(cid:80)K

where the matrices G1 ∈ (cid:60)L×L and G2 ∈ (cid:60)L×M are known  and the former is positive deﬁnite and
symmetric. We aim to minimize expected loss  assuming that the conditional expectation of y given
x is

k=1 rkxk. As such  given x and r  we select a decision

(cid:33)

(cid:195)

K(cid:88)

k=1

K(cid:88)

k=1

ur(x) = argmin

u

(cid:96)

u 

rkxk

= −1

2 G−1

1 G2

rkxk.

The question is how best to compute the regression coefﬁcients r for this purpose.
To motivate the setting we have described  we offer a hypothetical application.
Example 1. Consider an Internet banner ad campaign that targets M classes of customers. An
average revenue of ym is received per customer of class m that the campaign reaches. This quantity
is random and inﬂuenced by K observable factors x1m  . . .   xKm. These factors may be correlated
across customers classes; for example  they could capture customer preferences as they relate to
ad content or how current economic conditions affect customers. For each mth class  the cost of
reaching the umth customer increases with um because ads are ﬁrst targeted at customers that can
be reached at lower cost. This cost is quadratic  so that we pay γmu2
m to reach um customers  where
γm is a known constant.
The application we have described ﬁts our general problem context. It is natural to predict the
response vector y using a linear combination
k rkxk of factors with the regression coefﬁcients
rk computed based on past observations O = {(x(1)  y(1)) ···   (x(N )  y(N ))}. The goal is to
maximize expected revenue less advertising costs. This gives rise to a loss function that is quadratic
in u and y:

(cid:80)

M(cid:88)

(cid:96)(u  y) =

(γmu2

m − umym).

m=1

One might ask why not construct M separate linear regression models  one for each response vari-
able  each with a separate set of K coefﬁcients. The reason is that this gives rise to M K coefﬁcients;
when M is large and data is limited  this could lead to over-ﬁtting. Models of the sort we consider 
where regression coefﬁcients are shared across multiple response variables  are sometimes referred
to as general linear models and have seen a wide range of applications [7  8]. It is well-known
that the quality of results is highly sensitive to the choice of features  even more so than for models
involving a single response variable [7].

2

3 Algorithms

Ordinary least squares (OLS) is a conventional approach to computing regression coefﬁcients. This
would produce a coefﬁcient vector

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)2

N(cid:88)

n=1

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)y(n) − K(cid:88)
N(cid:88)

k=1

rOLS = argmin
r∈(cid:60)K

rkx(n)

k

.

(1)

Note that OLS does not take the decision objective into account when computing regression coefﬁ-
cients. Empirical optimization (EO)  as studied for example in [2  6]  offers an alternative that does
so. This approach minimizes empirical loss on the training data:

rEO = argmin
r∈(cid:60)K

n=1

(cid:96)(ur(x(n))  y(n)).

(2)

Note that EO does not explicitly aim to estimate the conditional expectation of the response vector.
Instead it focusses on decision loss that would be incurred with the training data. Both rOLS and
rEO can be computed efﬁciently by minimizing convex quadratic functions.
As we will see in our computational and theoretical analyses  OLS and EO can be viewed as two
extremes  each offering room for improvement. In this paper  we propose an alternative algorithm
– directed regression (DR) – which produces a convex combination rDR = (1 − λ)rOLS + λrEO
of coefﬁcients computed by OLS and EO. The term directed is chosen to indicate that DR is inﬂu-
enced by the decision objective though  unlike EO  it does not simply minimize empirical loss. The
parameter λ ∈ [0  1] is computed via cross-validation  with an objective of minimizing average loss
on validation data. Average loss is a convex quadratic function of λ  and therefore can be easily
minimized over λ ∈ [0  1].
DR is designed to generate decisions that are more robust to imperfections in feature selection than
OLS. As such  DR addresses issues similar to those that have motivated work in data-driven robust
optimization  as surveyed in [3]. Our focus on making good decisions despite modeling inaccuracies
also complements recent work that studies how models deployed in practice can generate effective
decisions despite their failure to pass basic statistical tests [4].

4 Computational Results

In this section  we present results from applying OLS  EO  and DR to synthetic data. To generate a
data set  we ﬁrst sample parameters of a generative model as follows:

pendently from N (0  1).

1. Sample P matrices C1  . . .   CP ∈ (cid:60)M×Q  with each entry from each matrix drawn inde-
2. Sample a vector ˜r ∈ (cid:60)P from N (0  I).
3. Sample Ga ∈ (cid:60)L×L and Gb ∈ (cid:60)L×M   with each entry of each matrix drawn from N (0  1).

Let G1 = G(cid:62)

a Ga and G2 = G(cid:62)

a Gb.

Given generative model parameters C1  . . .   CP and ˜r  we sample each training data pair (x(n)  y(n))
as follows:

(cid:80)P

1. Sample a vector φ(n) ∈ (cid:60)Q from N (0  I) and a vector w(n) ∈ (cid:60)M from N (0  σ2
2. Let y(n) =
i=1 ˜riCiφ(n) + w(n).
3. For each k = 1  2 ···   K  let x(n)

k = Ckφ(n).

wI).

The vector φ(n) can be viewed as a sample from an underlying information space. The matrices
C1  . . .   CP extract feature vectors from φ(n). Note that  though response variables depend on P
feature vectors  only K ≤ P are used in the regression model.
Given generative model parameters and a coefﬁcient vector r ∈ (cid:60)K  it is easy to evaluate the
expected loss (cid:96)(r) = Ex y[(cid:96)(ur(x)  y)]. It is also easy to evaluate the minimal expected loss (cid:96)∗ =

3

(a)

(b)

Figure 1: (a) Excess losses delivered by OLS  EO  and DR  for different numbers N of training
samples. (b) Excess losses delivered by OLS  EO  and DR  using different numbers K of the 60
features.

minr Ex y[(cid:96)(ur(x)  y)]. We will assess each algorithm in terms of the excess loss (cid:96)(r)− (cid:96)∗ delivered
by the coefﬁcient vector r that the algorithm computes. Excess loss is nonnegative  and this allows
us to make comparisons in percentage terms.
We carried out two sets of experiments to compare the performance of OLS  EO  and DR. In the
ﬁrst set  we let M = 15  L = 15  P = 60  Q = 20  σw = 5  and K = 50. For each N ∈
{10  15  20  30  50}  we ran 100 trials  each with an independently sampled generative model and
training data set. In each trial  each algorithm computes a coefﬁcient vector given the training data
and loss function. With DR  λ is selected via leave-one-out cross-validation when N ≤ 20  and via
5-fold cross-validation when N > 20. Figure 1(a) plots excess losses averaged over trials. Note that
the excess loss incurred by DR is never larger than that of OLS or EO. Further  when N = 20  the
excess loss of OLS and EO are both around 20% larger than that of DR. For small N  OLS is as
effective as DR  while  EO becomes as effective as DR as N grows large.
In the second set of experiments  we use the same parameter values as in the ﬁrst set  except we ﬁx
N = 20 and consider use of K ∈ {45  50  55  58  60} feature vectors. Again  we ran 100 trials for
each K  applying the three algorithms as in the ﬁrst set of experiments. Figure 1(b) plots excess
losses averaged over trials. Note that when K = 55  DR delivers excess loss around 20% less than
EO and OLS. When K = P = 60  there are no missing features and OLS matches the performance
of DR.
Figure 2 plots the values of λ selected by cross-validation  each averaged over the 100 trials  as
a function of N and K. As the number of training samples N grows  so does λ  indicating that
DR is weighted more heavily toward EO. As the number of feature vectors K grows  λ diminishes 
indicating that DR is weighted more heavily toward OLS.

5 Theoretical Analysis

In this section  we formulate a generative model for the training data and future observations. For
this model  optimal coefﬁcients are convex combinations of rOLS and rEO. As such  our model and
analysis motivate the use of DR.

5.1 Model

In this section  we describe a generative model that samples the training data set  as well as “missing
features ” and a representative future observation. We then formulate an optimization problem where
the objective is to minimize expected loss on the future observation conditioned on the training data
and missing features. It may seem strange to condition on missing features since in practice they are
unavailable when computing regression coefﬁcients. However  we will later establish that optimal

4

10203040500200040006000800010000NExcess Loss  OLSEODR455055600100020003000400050006000KExcess Loss  OLSEODR(a)

(b)

Figure 2: (a) The average values of selected λ  for different numbers N of training samples. (b) The
average values of selected λ  using different numbers K of the 60 features.

coefﬁcients are convex combinations of rOLS and rEO  each of which can be computed without
observing missing features. Since directed regression searches over these convex combinations  it
should approximate what would be generated by a hypothetical algorithm that observes missing
features.
We will assume that each feature  whether observed or missing  is a linear function of an “infor-
mation vector” drawn from (cid:60)Q. Speciﬁcally  the N training data samples depend on information
vectors φ(1)  . . .   φ(N ) ∈ (cid:60)Q. A linear function mapping an information vector to a feature vector
can be represented by a matrix in (cid:60)M×Q  and to describe our generative model  it is useful to deﬁne
an inner product for such matrices. In particular  we deﬁne the inner product between matrices A
and B by

N(cid:88)

n=1

(cid:104)A  B(cid:105) =

1
N

(Aφ(n))(cid:62)(Bφ(n)).

Our generative model takes several parameters as input. First  there are the number of samples
N  the number of response variables M  and the number of feature vectors K. Second  a parameter
µQ speciﬁes the expected dimension of the information vector. Finally  there are standard deviations
σr  σ  and σw  of observed feature coefﬁcients  missing feature coefﬁcients  and noise  respectively.
Given parameters N  M  K  µQ  σr  σ  and σw  the generative model produces data as follows:

1. Sample Q from the geometric distribution with mean µQ.
2. Sample φ(1)  . . .   φ(N ) ∈ (cid:60)Q from N (0  IQ).
3. Sample C1  . . .   CK and D1 ···   DJ ∈ (cid:60)M×Q with each entry i.i.d. from N (0  1)  where

K + J = M Q.

4. Apply the Gram-Schmidt algorithm with respect

to the inner product deﬁned
above to generate an orthonormal basis ˜C1  . . .   ˜CK  ˜D1  . . .   ˜DJ from the sequence
C1  . . .   CK  D1  . . .   DJ.

5. Sample r∗ ∈ (cid:60)K from N (0  σ2
6. For n = 1 ···   N  sample w(n) ∈ (cid:60)M from N (0  σ2

 IJ).

r IK) and r⊥∗ ∈ (cid:60)J from N (0  σ2
(cid:105)
(cid:104)
wIM )  and let
(cid:104)
(cid:105)
K(cid:88)

··· CKφ(n)
···
˜DJ φ(n)

C1φ(n)
˜D1φ(n)

J(cid:88)

 

 

r∗
kx(n)

k +

r⊥∗
j z(n)

j + w(n).

x(n) =

z(n) =

y(n) =

(3)

(4)

(5)

k=1

j=1

5

102030405000.20.40.60.8Nl4550556000.20.40.60.8Kl7. Sample ˜φ uniformly from {φ(1) ···   φ(N )} and ˜w ∈ (cid:60)M from N (0  σ2

wIM ). Generate ˜x 

˜z  and ˜y by the same functions in (3)  (4)  and (5).

The samples z(1)  . . .   z(N )  ˜z represent missing features. The Gram-Schmidt procedure ensures two
properties. First  since (cid:104)Ck  ˜Dj(cid:105) = 0  missing features are uncorrelated with observed features. If
this were not the case  observed features would provide information about missing features. Second 
since ˜D1  . . .   ˜DJ are orthonormal  the distribution of missing features is invariant to rotations in the
J-dimensional subspace from which they are drawn. In other words  all directions in that space are
equally likely.
We deﬁne an augmented training set O = {(x(1)  z(1)  y(1)) ···   (x(N )  z(N )  y(N ))} and consider
selecting regression coefﬁcients ˆr ∈ (cid:60)K that solve

E[(cid:96)(ur(˜x)  ˜y)|O].

min
r∈(cid:60)K

Note that the probability distribution here is implicitly deﬁned by our generative model  and as such 
ˆr may depend on N  M  K   µQ  σr  σ  σw  and O.

5.2 Optimal Solutions
Our primary interest is in cases where prior knowledge about the coefﬁcients r∗ is weak and does
not signiﬁcantly inﬂuence ˆr. As such  we will from here on restrict attention to the case where σr is
asymptotically large. Hence  ˆr will no longer depend on σr.
It is helpful to consider two special cases. One is where σ = 0 and the other is where σ is
asymptotically large. We will refer to ˆr in these extreme cases as ˆr0 and ˆr∞. The following theorem
establishes that these extremes are delivered by OLS and EO.
Theorem 1. For all N  M  K  µQ  σw  and O 

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)2

rkx(n)

k

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)y(n) − K(cid:88)
N(cid:88)
N(cid:88)

k=1

n=1

ˆr0 = argmin
r∈(cid:60)K

and

ˆr∞ = argmin
r∈(cid:60)K

n=1

(cid:96)(ur(x(n))  y(n)).

Note that σ represents the degree of bias in a regression model that assumes there are no missing
features. Hence  the above theorem indicates that OLS is optimal when there is no bias while EO
is optimal as the bias becomes asymptotically large.
It is also worth noting that the coefﬁcient
vectors ˆr0 and ˆr∞ can be computed without observing the missing features  though ˆr is deﬁned by
an expectation that is conditioned on their realizations. Further  computation of ˆr0 and ˆr∞ does not
require knowledge of Q or σw.
Our next theorem establishes that the coefﬁcient vector ˆr is always a convex combination of ˆr0 and
ˆr∞.
Theorem 2. For all N  M  K  µQ  σw  σ  and O 

ˆr = (1 − λ)ˆr0 + λˆr∞ 

where λ = 1
σ2
w
1+
N σ2


.

Our two theorems together imply that  with an appropriately selected λ ∈ [0  1]  (1 − λ)rOLS +
λrEO = ˆr. This suggests that directed regression  which optimizes λ via cross-validation to generate
a coefﬁcient vector rDR = (1 − λ)rOLS + λrEO  should approximate ˆr well without observing the
missing features or requiring knowledge of Q  σ  or σw.

6

Interpretation

5.3
To develop intuition for our results  we consider an idealized situation where the coefﬁcients r∗ and
r⊥∗ are provided to us by an oracle. Then the optimal coefﬁcient vector would be

rO = argmin
r∈(cid:60)K

E[(cid:96)(ur(˜x)  ˜y)|O  r∗  r⊥∗].

It can be shown that rOLS is a biased estimator of rO  while rEO is an unbiased one. However 
the variance of rOLS is smaller than that of rEO. The optimal tradeoff is indeed captured by the
value of λ provided in Theorem 2. In particular  as the number of training samples N increases 
variance diminishes and λ approaches 1  placing increasing weight on EO. On the other hand  as
the number of observed features K increases  model bias decreases and λ approaches 0  placing
increasing weight on OLS. Our experimental results demonstrate that the value of λ selected by
cross-validation exhibits the same behavior.

6 Extensions

Though we only treated linear models and quadratic objective functions  our work suggests that
there can be signiﬁcant gains in broader problem settings from a tighter coupling between machine
learning and decision-making. In particular  machine learning algorithms should factor decision
objectives into the learning process.
It will be interesting to explore how to do this with other
classes of models and objectives.
One might argue that feature mis-speciﬁcation is not a critical issue in light of effective methods
for subset selection. In particular  rather than selecting a few features and facing the consequences
of model bias  one might select an enormous set of features and apply a method like the lasso [10]
to identify a small subset. Our view is that even this enormous set will result in model biases that
might be ameliorated by generalizations of DR. There is also the concern that data requirements
grow with the size of the large feature set  albeit slowly. Understanding how to synthesize DR with
subset selection methods is an interesting direction for future research.
Another issue that should be explored is the effectiveness of cross-validation in optimizing λ. In
particular  it would be helpful to understand how the estimate relates to the ideal value of λ identiﬁed
by Theorem 2. More general work on the selection of convex combinations of models (e.g.  [1  5])
may lend insights to our setting.
Let us close by mentioning that the ideas behind DR ought to play a role in reinforcement learning
(RL) as presented in [9]. RL algorithms learn from experience to predict a sum of future rewards
as a function of a state  typically by ﬁtting a linear combination of features of the state. This so-
called approximate value function is then used to guide sequential decision-making. The problem
we addressed in this paper can be viewed as a single-period version of RL  in the sense that each
decision incurs an immediate cost but bears no further consequences. It would be interesting to
extend our idea to the multi-period case.

Acknowledgments

We thank James Robins for helpful comments and suggestions. The ﬁrst author is supported by a
Stanford Graduate Fellowship. This research was supported in part by the National Science Foun-
dation through grant CMMI-0653876.

x(N )(cid:62) (cid:164)(cid:62)

x(n)
1
  Z

···
=

(cid:163)

x(n)
K

  z(n) =
z(1)(cid:62) ···

(cid:105)

(cid:104)
z(N )(cid:62) (cid:164)(cid:62)

z(n)
1

···
 

(cid:105)

.

z(n)
J

Let X

=
y(1)(cid:62) ···

=
  ¯r = E[r∗|O]  ¯r⊥ = E[r⊥∗|O]. For any matrix V   let V † denote
(V (cid:62)V )−1V (cid:62). Recall that (cid:104)Ck  ˜Dj(cid:105) = 0 ∀ k  j implies that each column of X is orthogonal to

Y

Proof of Theorem 1. For each n  let x(n) =

Appendix

(cid:163)

(cid:163)
y(N )(cid:62) (cid:164)(cid:62)

x(1)(cid:62) ···

(cid:104)

7

each column of Z. Because r∗  r⊥∗  O are jointly Gaussian  as σr → ∞  we have

= argmin
(r r⊥)

(cid:163)

Let a(n) = G

b(1)(cid:62) ···

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)y(n) − K(cid:88)
(cid:184)(cid:183)

k=1

N(cid:88)
(cid:183) 1

n=1

1
2σ2
w

(cid:184)

−

rkx(n)

Z
IJ

X 1
σw
σw
0
1
σ
− 1
1 G2z(n)  A =
2

r
r⊥

− 1
1 G2x(n)  b(n) = G
2

. We have
E[(cid:96)(ur(˜x)  ˜y)|O] = argmin

1
N

r

n=1

j=1

k − J(cid:88)
(cid:184)(cid:176)(cid:176)(cid:176)(cid:176)2
(cid:163)
N(cid:88)

=

(cid:183)

¯r
¯r⊥

1
Y
σw
0

= argmin
(r r⊥)

(cid:184)
(cid:176)(cid:176)(cid:176)(cid:176)(cid:183)
b(N )(cid:62) (cid:164)(cid:62)
N(cid:88)
N(cid:88)

n=1

r

r

= argmin

= argmin

r

n=1

ˆr = argmin

ur(x(n))(cid:62)G1ur(x(n)) + ur(x(n))(cid:62)G2 E[˜y|˜x = x(n)  O]

1

4 r(cid:62)a(n)(cid:62)a(n)r − 1

2 r(cid:62)a(n)(cid:62)(a(n)¯r + b(n)¯r⊥)

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)2

J(cid:88)

j=1

r⊥2

j

r⊥
j z(n)

j

(cid:34)

+

1
2σ2


(X(cid:62)X)−1X(cid:62)Y

(Z(cid:62)Z + σ2
w
σ2

a(1)(cid:62) ···

I)−1Z(cid:62)Y

a(N )(cid:62) (cid:164)(cid:62)

(cid:35)

.

E˜y[(cid:96)(ur(˜x)  ˜y)|˜x = x(n)  O]

= ¯r + A†B¯r⊥ = X†Y + A†B(Z(cid:62)Z + σ2
w
σ2


I)−1Z(cid:62)Y.

Taking σ → 0 and σ → ∞ yields

ˆr0 = X†Y  
ˆr∞ = X†Y + A†BZ†Y .

The ﬁrst part of the theorem then follows because

ˆr0 = X†Y = argmin

r

(cid:107)Y − Xr(cid:107)2 = argmin

r

n=1

N(cid:88)

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)y(n) − K(cid:88)

(cid:176)(cid:176)(cid:176)(cid:176)(cid:176)2

.

rkx(n)

k

k=1

  B =

(6)

(7)
(8)

(cid:96)(ur(x(n))  y(n)) = argmin

ur(x(n))(cid:62)G1ur(x(n)) + ur(x(n))(cid:62)G2y(n)

We now prove the second part. Note that

N(cid:88)
r(cid:62)A(cid:62)Ar − 2r(cid:62) N(cid:88)

n=1

argmin

r

= argmin

r

N(cid:88)

n=1

r

(cid:163)
 G(cid:62)

n=1

hk =

h(n)(cid:62)y(n) = (A(cid:62)A)−1H(cid:62)Y 

h(N )(cid:62) (cid:164)(cid:62)


2 G−1

1 G2Ckφ(1)

...

2 G−1
G(cid:62)

1 G2Ckφ(N )

where h(n) = G(cid:62)

2 G−1

1 G2x(n) and H =

h(1)(cid:62) ···

. Each kth column of H

is in span{col X  col Z} because G(cid:62)
1 G2Ck ∈ (cid:60)M×Q = span{C1 ···   CK  ˜D1 ···   ˜DJ}.
2 G−1
Since the residual Y (cid:48) = Y − XX†Y − ZZ†Y upon projecting Y onto span {col X  col Z} is
k Y (cid:48) = 0 ∀ k and hence H(cid:62)Y (cid:48) = 0. This implies H(cid:62)Y =
orthogonal to the subspace  we have h(cid:62)
H(cid:62)XX†Y + H(cid:62)ZZ†Y . Further  since a(n)(cid:62)a(n) = h(n)(cid:62)x(n)  a(n)(cid:62)b(n) = h(n)(cid:62)z(n) ∀ n  we
have

ˆr∞ = X†Y + A†BZ†Y = (A(cid:62)A)−1(cid:161)
= (A(cid:62)A)−1(cid:161)

H(cid:62)XX†Y + H(cid:62)ZZ†Y

(cid:162)

A(cid:62)AX†Y + A(cid:62)BZ†Y
= (A(cid:62)A)−1H(cid:62)Y.

(cid:162)

Proof of Theorem 2. Because (cid:104) ˜Di  ˜Dj(cid:105) = 1{i = j}  we have Z(cid:62)Z = N I. Plugging this into (6)
and comparing the resultant expression with (7) and (8) yield the desired result.

8

References
[1] J.-Y. Audibert. Aggregated estimators and empirical complexity for least square regression.

Annales de l’Institut Henri Poincare Probability and Statistics  40(6):685–736  2004.

[2] P. L. Bartlett and S. Mendelson. Empirical minimization. Probability Theory and Related

Fields  135(3):311–334  2006.

[3] D. Bertsimas and A. Thiele. Robust and data-driven optimization: Modern decision-making

under uncertainty. In Tutorials on Operations Research. INFORMS  2006.

[4] O. Besbes  R. Philips  and A. Zeevi. Testing the validity of a demand model: An operations

perspective. 2007.

[5] F. Bunea  A. B. Tsybakov  and M. H. Wegkamp. Aggregation for Gaussian regression. The

Annals of Statistics  35(4):1674–1697  2007.

[6] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other

learning applications. Information and Computation  100:78–150  1992.

[7] K. Kim and N. Timm. Univariate and Multivariate General Linear Models: Theory and

Applications with SAS. Chapman & Hall/CRC  2006.

[8] K. E. Muller and P. W. Stewart. Linear Model Theory: Univariate  Multivariate  and Mixed

Models. Wiley  2006.

[9] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press  Cam-

bridge  MA  1998.

[10] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of Royal Statistical

Society  1996.

9

,Rahul Krishnan
Simon Lacoste-Julien
David Sontag
Yu-Chuan Su
Kristen Grauman
Abhimanyu Dubey
Otkrist Gupta
Ramesh Raskar
Nikhil Naik