2017,Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning,Under/overestimation of state/action values are harmful for reinforcement learning agents. In this paper  we show that a state/action value estimated using the Bellman equation can be decomposed to a weighted sum of path-wise values that follow log-normal distributions. Since log-normal distributions are skewed  the distribution of estimated state/action values can also be skewed  leading to an imbalanced likelihood of under/overestimation. The degree of such imbalance can vary greatly among actions and policies within a single problem instance  making the agent prone to select actions/policies that have inferior expected return and higher likelihood of overestimation. We present a comprehensive analysis to such skewness  examine its factors and impacts through both theoretical and empirical results  and discuss the possible ways to reduce its undesirable effects.,Log-normality and Skewness of Estimated

State/Action Values in Reinforcement Learning

Liangpeng Zhang1 2  Ke Tang3 1  and Xin Yao3 2

1School of Computer Science and Technology 
University of Science and Technology of China

2University of Birmingham  U.K.

3Shenzhen Key Lab of Computational Intelligence 
Department of Computer Science and Engineering 

Southern University of Science and Technology  China

lxz472@cs.bham.ac.uk  tangk3@sustc.edu.cn  xiny@sustc.edu.cn

Abstract

Under/overestimation of state/action values are harmful for reinforcement learn-
ing agents. In this paper  we show that a state/action value estimated using the
Bellman equation can be decomposed to a weighted sum of path-wise values that
follow log-normal distributions. Since log-normal distributions are skewed  the
distribution of estimated state/action values can also be skewed  leading to an
imbalanced likelihood of under/overestimation. The degree of such imbalance can
vary greatly among actions and policies within a single problem instance  making
the agent prone to select actions/policies that have inferior expected return and
higher likelihood of overestimation. We present a comprehensive analysis to such
skewness  examine its factors and impacts through both theoretical and empirical
results  and discuss the possible ways to reduce its undesirable effects.

1

Introduction

In reinforcement learning (RL) [1  2]  actions executed by the agent are decided by comparing relevant
state values V or action values Q. In most cases  the ground truth V and Q are not available to the
agent  and the agent has to rely on estimated values ˆV and ˆQ instead. Therefore  whether or not an
RL algorithm yields sufﬁciently accurate ˆV and ˆQ is a key factor to its performance. Many researches
have proved that  for many popular RL algorithms such as Q-learning [3] and value iteration [4] 
estimated values are guaranteed to converge in the limit to their ground truth values [5  6  7  8].
Still  under/overestimation of state/action values occur frequently in practice. Such phenomena are
often considered as the result of insufﬁcient sample size or the utilisation of function approximation
[9]. However  recent researches have pointed out that the basic estimators of V and Q derived
from the Bellman equation  which were considered unbiased and have been widely applied in RL
algorithms  are actually biased [10] and inconsistent [11]. For example  van Hasselt [10] showed that
the max operator in the Bellman equation and its transforms introduces bias to the estimated action
values  resulting in overestimation. New operators and algorithms have been proposed to correct such
biases [12  13  14]  inconsistency [11] and other issues of value-based RL [15  16  17  18].
This paper shows that  despite having great improvements in recent years  the value estimator
of RL can still suffer from under/overestimation. Speciﬁcally  we show that the distributions of
estimated state/action values are very likely to be skewed  resulting in imbalanced likelihood of
under/overestimation. Such skewness and likelihood can vary dramatically among actions/policies
within a single problem instance. As a result  the agent may frequently select undesirable ac-
tions/policies  regardless of its value estimator being unbiased.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Illustration of positive skewness (red distribution) and negative skewness (blue distribution).
Thick and thin vertical lines represent the corresponding expected values and medians  respectively.

Such phenomenon is illustrated in Figure 1. An estimated state/action value following the red
distribution has a mean 0.21 and a median −0.61  thus tends to be underestimated. Another following
the blue distribution  on the other hand  has a mean −0.92 and a median 0.61  thus likely to be
overestimated. Despite that the red expected return is noticeably greater than the blue  the probability
of an unbiased agent arriving at the opposite conclusion (blue is better) and thus selecting the inferior
action/policy is around 0.59  which is even worse than random guessing.
This paper also indicates that such skewness comes from the Bellman equation passing the dispersion
of transition dynamics to the state/action values. Therefore  as long as a value is estimated by
applying the Bellman equation to the observations of transition  it can suffer from the skewness
problem  regardless of the algorithm being used. Instead of proposing new algorithms  this paper
suggests two general ways to reduce the skewness. The ﬁrst is to balance the impacts of positive and
negative immediate rewards to the estimated values. We show that positive rewards lead to positive
skewness and vice versa  and thus  a balance between the two may help neutralise the harmful effect
of skewness. The second way is to simply collect more observations of transitions. However  our
results in this paper indicate that the effectiveness of this approach diminishes quickly as the sample
size grows  and thus is recommended only when observations are cheap to obtain.
In the rest of this paper  we will elaborate our analysis to the distributions of state/action values
estimated by the Bellman equation. Speciﬁcally  we will show that an estimated value in a general
MDP can be decomposed to path-wise values in normalised single-reward Markov chains. The
path-wise values are shown to obey log-normal distributions  and thus the distribution of an estimated
value is the convolution of such log-normal distributions. To understand which factors have the most
impact to the skewness  we derive the expressions of the parameters of these log-normal distributions.
We then discuss whether the skewness of estimated values can be reduced in order to improve
learning performance. Finally  we provide our empirical results to complement our theoretical ones 
illustrating how substantial the undesirable effect of skewness can be  as well as to what degree such
effect can be reduced by obtaining more observations.

2 Preliminaries

The standard RL setup of [1] is followed in this paper. An environment is formulated as a ﬁnite
discounted Markov Decision Process (MDP) M = (S  A  P  R  γ)  where S and A are ﬁnite sets
of states and actions  P (s(cid:48)|s  a) is a transition probability function  R(s  a  s(cid:48)) is an immediate
reward function  and γ ∈ (0  1) is a discount factor. A trajectory (s1  a1  s2  r1)  (s2  a2  s3  r2)  ... 
(st  at  st+1  rt) represents the interaction history between the agent and the MDP. The number of
occurrences of state-action pair (s  a) and transition (s  a  s(cid:48)) in such trajectory are denoted Ns a and
Ns a s(cid:48)  respectively.
A policy is denoted π  and V π(s) is the state value of π starting from s. An action value Qπ(s  a) is
essentially a state value following a non-stationary policy that selects a at the ﬁrst step but follows π
thereafter. It can be analysed in the same way as V π  so it sufﬁces to focus on V π in the following
sections. For convenience  superscript π in V π will be dropped if it is clear from the context.
s(cid:48)∈S P (s(cid:48)|s  π(s))(R(s  π(s)  s(cid:48)) + γV π(s(cid:48))) 
which is called the Bellman equation. Most model-based and model-free RL algorithms utilise this
equation  its equivalents  or its transforms to estimate state values. Since P and R are unknown to
the agent  estimated values ˆV (s) are computed from estimated transitions ˆP and rewards ˆR instead 

For any s ∈ S and policy π  it holds that V π(s) =(cid:80)

2

-5-4-3-2-1012345estimated value00.10.20.30.40.5densityvalue 1value 2where ˆP (s(cid:48)|s  a) = Ns a s(cid:48)/Ns a and ˆR(s  a  s(cid:48)) = rt with (st  at  st+1)=(s  a  s(cid:48)). This is done
explicitly in model-based learning  and implicitly with frequencies of updates in model-free learning.
We will show in later section that the skewness of estimated values is decided by the dynamic effects
of the environment rather than the learning algorithm being used  and therefore  it sufﬁces to focus
on the model-based case in order to evaluate such skewness.

The skewness in this paper refers to the Pearson 2 coefﬁcient (E[X] − median[X])/(cid:112)Var[X]

[19  20]. Following this deﬁnition  a distribution has a positive skewness if and only if its mean is
greater than its median  and vice versa. Assuming that the bias of ˆV is corrected or absent  we have
E[ ˆV ] = V . Thus  a positive skewness of ˆV means Pr( ˆV <V ) > 0.5  indicating a higher likelihood
of underestimation  while a negative skewness indicates a higher likelihood of overestimation.
An informative indicator of skewness is CDF ˆV (V )−0.5 where CDF ˆV is the cumulative distribution
function of ˆV . The sign of this indicator is consistent with the Pearson 2 coefﬁcient  while its absolute
value gives the extra probability of under/overestimation of ˆV compared to a zero-skew distribution.
A log-normal distribution with location parameter µ and scale parameter σ is denoted lnN (µ  σ2). A
random variable X follows lnN (µ  σ2) if and only if ln(X) follows normal distribution N (µ  σ2).
The parameters µ and σ of log-normal distribution can be calculated from its mean and variance

(cid:1)  where E[X] and Var[X] are the mean and

(cid:1)  and σ2 = ln(cid:0)1 + Var[X]

by µ = ln(cid:0)

E[X]2

√E[X]2+Var[X]

variance of X ∼ lnN (µ  σ2)  respectively.

E[X]2

3 Log-normality of Estimated State Values

In this section  we elaborate our analysis to the distributions of estimated values ˆV . The analysis
is formed of three steps. First  state values in general MDPs are decomposed to the state values
in relevant normalised single-reward Markov chains. Second  they are further decomposed into
path-wise state values. Third  the path-wise state values are shown to obey log-normal distributions.

3.1 Decomposing into Normalised Single-reward Markov chains

V π = B(P π◦RπJ )  or V π(si) =(cid:80)

Given an MDP M and a policy π  the interaction between π and M forms a Markov chain M π  with
transition probability pi j = P (sj|si  π(si)) and reward ri j = R(si  π(si)  sj) from arbitrary state
si to state sj. Let P π be the transition matrix of M π  V π be the (column) vector of state values 
Rπ be the reward matrix  and J be a vector of 1 with the same size of V π. Then Bellman equation
is equivalent to V π = P π◦RπJ + γP πV π = (I − γP π)−1(P π◦RπJ )  where I is an identity
matrix  and ◦ is Hadamard product.
This equation indicates that a state value is a weighted sum of dynamic effects  with rewards serving
as the weights of summation. Precisely  let B = (I − γP π)−1  then the equation above becomes
j k rj k(bi j pj k). Here  term (bi j pj k) describes the joint
dynamic effect starting from si ending with transition sjsk  which will be elaborated in Section 3.2.
j k denote a normalised single-reward Markov chain (NSR-MC) of M π  which has exactly the
Let M π
same S  A  γ and P π as M π  but all rewards are trivially 0 except rj k = 1. For an NSR-MC M π
j k 
the equation above becomes V π
(si) = bi j pj k. Thus  a state value V of a general MDP M can
M π
be rewritten as the weighted sum of state values of all |S|2 NSR-MCs {M π

j k} of M  i.e.

j k

M (si) =(cid:80)

V π

j k rj kVM π

j k

(si).

(1)

Therefore  the next step of analysis is to examine the state values in NSR-MCs.

3.2 Decomposing into Path-wise State Values
Seeing Markov chain M π as a directed graph  a walk w of length |w| in such graph is a sequence
of |w| successive transitions through states s1  s2  s3  ...  s|w|+1.1 A path is a walk without repeated
states  with exception to the last state s|w|+1  which can be either a visited or an unvisited one.

1Superscripts here refer to the timestamps on w rather than the indices of speciﬁc states in S.

3

p1 1

1

p1 2

p2 1

p2 2

2

p3 1

p2 3

p3 2

p3 3

3

p3 4

4

Figure 2: Illustration of walks and a representative path. "Forward" and "backward" transitions are
drawn in thick and thin arrows  respectively  and pi j denotes the transition probability from si to sj.

w∈Wi j k

In an NSR-MC with unique non-zero reward rj k = 1  a state value V π(si) = bi j pj k can be
expanded as a sum of the discounted occurrence probabilities of walks that start from si and end
with transition (sj  π(sj)  sk). Let Wi j k denotes the set of all possible walks w satisfying s1=si 
(st st+1) on w pst st+1). Since

s|w|=sj and s|w|+1=sk. Then we have V (si) =(cid:80)

(γ|w|−1(cid:81)

Wi j k is inﬁnite  the walks in Wi j k need to be put into ﬁnite groups for further analysis.
Concretely  a step in a walk is considered "forward" if it arrives to a previously unvisited state  and
"backward" if the destination has already been visited before that step. The latter also includes the
cases where st+1 = st  that is  the agent stays at the same state after transition. The only exception
to this classiﬁcation is the last transition of a walk  which is always considered a "forward" one 
regardless of if its destination having been visited or not. The start state s1 and all such "forward"
transitions of a walk w form a representative path of w  denoted ˜w.
This is illustrated by Figure 2. In this example  all walks from s1 passing s2 ending with s3s4  such
as (s1s1s2s3s3s4)  (s1s2s3s1s2s3s4) and (s1s2s3s2s3s2s3s4)  are grouped with the representative
path (s1s2s3s4). Note that transition s1s3 will not happen within this group; rather  it belongs to the
groups that have s1s3 in their representative paths.
As can be seen from Figure 2  all possible walks sharing one representative path ˜w compose a chain
which has the same transition probability values with the original Markov chain M π  but with only
two type of transitions: (forward) si to si+1 (i ≤ | ˜w|); (backward) si to sj (j ≤ i ≤ | ˜w|). We call
this chain the derived chain of ˜w  denoted M π( ˜w)  or simply M ( ˜w). Then the inﬁnite sum becomes

V (s) =(cid:80)

˜w∈ ˜W VM ( ˜w)(s) 

(2)

where ˜W is the set of all representative paths that start from s and end with the unique 1-reward
transition of the relevant NSR-MC. Such VM ( ˜w)(s) are called path-wise state values of M π.
Since the main concern of this paper is the skewness of ˆV   we do not provide a constructive method of
obtaining all M π( ˜w). Rather  we point out that the size of ˜W is at most (|S|!)  and thus an estimated
value ˆV in NSR-MCs can be decomposed to ﬁnitely many estimated path-wise state values.

3.3 Log-normality of Estimated Path-wise State Values

possibility of(cid:80)i+1

Strictly speaking  derived chain M ( ˜w) of a representative path ˜w is not necessarily a Markov chain 
because only part of the transitions in the original Markov chain M π is included  allowing the
j=1 psi sj < 1. However  this does not make the path-wise state values violate

Bellman equation  and thus they can be treated as regular state values.
Since a representative path ˜w has no repeated states (except for s| ˜w|+1 which can either be a new state
or the same as some sk)  the superscripts here can be treated as the indices of states for convenience.
Therefore  path-wise state value VM ( ˜w)(si) is denoted Vi  and pi j refers to psi sj in this section.
Given ˜w  the most important path-wise value is V1  which belongs to the start point of ˜w.
Deﬁnition 3.1. Given a derived chain M ( ˜w) and discount factor γ  let pi j be the transition probability
from si to sj on M ( ˜w). The joint dynamic effect of M ( ˜w) for i ≤ | ˜w| is recursively deﬁned as

1 − γ(pi i +(cid:80)i−1

γpi i+1

(cid:81)i−1

Di =

j=1 pi j

k=j Dk)

.

Lemma 3.2. For all i < | ˜w|  path-wise state values satisfy Vi = Di Vi+1.

4

j=1 pi j(ri j+γVj). By deﬁnition of M ( ˜w)
j=1 pi jVj
for i < | ˜w|. When i = 1  this becomes V1 = γ(p1 1V1+p1 2V2) = γp1 2
V2 = D1V2. Sup-
1−γp1 1
j=i Dj)Vk+1 for i ≤ k 
l=j Dl)Vk+1 + pk+1 k+2Vk+2] =
Vk+2 = Dk+1Vk+2. Thus  by the principle of induction  Vi =

Proof. By Bellman equation  it holds that Vi = (cid:80)| ˜w|+1
we have pi j = 0 for j > i+1 and ri j = 0 for (i  j) (cid:54)= (| ˜w| | ˜w|+1). Thus Vi = γ(cid:80)i+1
pose Vi = Di Vi+1 holds for all i ≤ k < | ˜w|−1. Then Vi = ((cid:81)k
and therefore  Vk+1 = γ(cid:80)k+2
j=1 pk+1 jVj = γ[(cid:80)k+1
(cid:81)k
1−γ(pk+1 k+1+(cid:80)k
Di Vi+1 holds for all i < | ˜w|.
Lemma 3.3. For all i ≤ | ˜w|  Vi = 1

j=1 pk+1 j((cid:81)k

j=i Dj. Particularly  V1 = 1

(cid:81)| ˜w|

(cid:81)| ˜w|

j=1 Dj.

j=1 pk+1 j

γpk+1 k+2

l=j Dl)

γ

γ

Proof. By deﬁnition of ˜w  there are two possible cases of the last step from s| ˜w| to s| ˜w|+1:
(I) s| ˜w|+1 /∈ {s1  ...  s| ˜w|}; (II) there exists k ≤ | ˜w| such that s| ˜w|+1 = sk.
V| ˜w| = p| ˜w| | ˜w|+1(r| ˜w| | ˜w|+1 + γV| ˜w|+1) + γ(cid:80)| ˜w|
(Case I) There is no transition starting from s| ˜w|+1 in this case  thus V| ˜w|+1 = 0. Therefore 
j=1 p| ˜w| jVj =
1−γ(p| ˜w| | ˜w|+(cid:80)| ˜w|−1
j=i Dj.
p| ˜w| | ˜w|+1(r| ˜w| | ˜w|+1 + γVk) + γ(cid:80)| ˜w|
(Case II with s| ˜w|+1 = sk) In this case V| ˜w|+1 = Vk and p| ˜w| | ˜w|+1 = p| ˜w| k  thus V| ˜w| =
j=1 p| ˜w| jVj which is the

γ D| ˜w|. Thus Vi = ((cid:81)| ˜w|−1
j=1 j(cid:54)=k p| ˜w| jVj = p| ˜w| | ˜w|+1 + γ(cid:80)| ˜w|

j=1 p| ˜w| jVj = p| ˜w| | ˜w|+1 + γ(cid:80)| ˜w|
(cid:81)| ˜w|

j=i Dj)V| ˜w| = 1
γ

(cid:81)| ˜w|−1

k=j Dk)

p| ˜w| | ˜w|+1

= 1

p| ˜w| j

j=1

(cid:81)| ˜w|

same expression as the ﬁrst case  and therefore Vi = 1
γ

j=i Dj also holds for this case.

(cid:81)i−1

γ . Thus we have ln(V1) = − ln(γ) +(cid:80)| ˜w|

the equation above becomes ln( ˆV1) = − ln(γ) +(cid:80)| ˜w|

In both of the two cases above  V1 is the product of D1  D2  ...  D| ˜w| given by Deﬁnition 3.1  and
an additional factor 1
j=1 ln(Dj). By replacing all pi j in
Deﬁnition 3.1 with estimated transition ˆpi j  we get the “estimated” 2 joint dynamic effects ˆD. Then
j=1 ln( ˆDj). Assuming ˆDi’s as independent
random variables  it can be shown by the central limit theorem that as | ˜w| grows  ln( ˆV1) will tend to
a normal distribution  and therefore  ˆV1 approximates a log-normal distribution.
The “estimated” joint dynamic effects ˆD are actually mutually dependent in most cases  thus the
rigorous analysis of log-normality is more complicated. The main idea here is to ﬁrst prove all
ˆDi ≤ γ  and then show that the summation involving terms pi j
ˆDk in Deﬁnition 3.1 diminish
quickly with the size of ˜w  which indicates that ˆDi is mostly decided by ˆpi i and ˆpi i+1 and thus the
dependency between any two ˆD is relatively weak. As the focus here is to see the skewness of ˆV1 
such analysis is skipped  and we proceed to the study of parameters of log-normal distribution of ˆV1.
Since ˆpi i and ˆpi i+1 are the main factors that decide ˆDi  we provide the result on the most repre-
sentative case where pi i + pi i+1 = 1 and all other pi j are 0 for i < | ˜w|. Such M ( ˜w) is denoted
M0( ˜w) in the following text. It is easy to see that all ˆDi are mutually independent in such chains.
The delta method [21  22] below is used to obtain the expressions of parameters.
Lemma 3.4 (Delta method[21  22]). Suppose X is a random variable with ﬁnite moments  E[X]
being its mean and Var[X] being its variance. Suppose f is a sufﬁciently differentiable function.
Then it holds that E[f (X)] ≈ f (E[X])  and Var[f (X)] ≈ f(cid:48)(E[X])2 Var[X].
Lemma 3.5. Let ˆDj be Dj replacing all p with ˆp. Let Ni denotes the number of visits to the chain
state si in a learning trajectory. In M0( ˜w) derived chains it holds that E[ ˆDj] ≈ γpj j+1
  and
1−γpj j
Var[ ˆDj] ≈ γ2(1−γ)2

(1−γpj j )4 · pj j pj j+1

k=j

Nj

.

Proof. It holds that Var[ˆpj j+1] = ( 1
Nj
Deﬁnition 3.1.

)2Njpj jpj j+1 = pj j pj j+1

Nj

  then by applying Lemma 3.4 to

2Such “estimation” is not done explicitly in actual algorithms  but implicitly when using Bellman equation.

5

Lemma 3.6. In M0( ˜w) derived chains it holds that

| ˜w|(cid:89)

j=1

1
γ

E[ ˆV1] =

E[ ˆDj] 

(cid:18) | ˜w|(cid:89)

j=1

Var[ ˆV1] ≈ 1
γ2

(Var[ ˆDj] + E[ ˆDj]2) −

| ˜w|(cid:89)

(cid:19)

.

E[ ˆDj]2

j=1

j=1(Var[Xj] + E[Xj]2) −
E[Xj]2. Since all ˆD are independent in M0( ˜w)  by applying this and Lemma 3.4 to Lemma

Proof. For independent X1  X2  ...  Xn it holds that Var[X1...Xn] =(cid:81)n
(cid:81)n
ln(cid:0)

3.3  the above results can be obtained.
Theorem 3.7. In M0( ˜w) with sufﬁciently large | ˜w|  it holds that ˆV1

(cid:1) and σ2 = ln(cid:0)1+ Var[ ˆV1]

(cid:1)  where E[ ˆV1] and Var[ ˆV1] are given by Lemma 3.6.

˙∼ lnN (µ  σ2) with µ =

j=1

√

E[ ˆV1]2

E[ ˆV1]2+Var[ ˆV1]

E[ ˆV1]2

Proof. By applying the equations on the parameters of log-normal (see Section 2) to ˆV1.

4 Skewness of Estimated State Values  and Countermeasures

This section interprets the results presented in Section 3 in terms of skewness  and discuss how to
reduce the undesirable effects of skewness. The skewness is mainly decided by two factors: (a)
parameter σ of log-normal distributions; (b) non-zero immediate rewards.

2σ

Impact of Parameter σ of Log-normal Distributions

4.1
A regular log-normal distribution lnN (µ  σ2) has a positive skewness  which means a sampled value
from such distribution has more than 0.5 probability to be less than its expected value  resulting in a
higher likelihood of underestimation. Precisely  if X ∼ lnN (µ  σ2)  then E[X] = exp(µ + σ2/2)
and median[X] = exp(µ)  thus the Pearson 2 coefﬁcient of X is greater than 0. Additionally  since
lnN (µ  σ2) has a CDF(x) = 0.5(1 + erf( ln(x)−µ√
)) where erf(x) is the Gauss error function  our
√
indicator CDF(E[X])−0.5 equals to 0.5 erf(σ/
8). This indicates that σ has a stronger impact than
µ to the scale of the skewness in log-normal distributions.
Combining Lemma 3.6 and Theorem 3.7 shows that σ is decided by a complicated interaction
between all observed dynamic effect ˆDj’s. By Lemma 3.5  transition probabilities pj ∗ completely
decide E[ ˆDj]  and have substantial impacts to Var[ ˆDj].
This indicates that the main cause of skewness is the transition dynamics of MDPs rather than learning
algorithms. As an extreme case  if the forward transition of a state-action pair is deterministic (i.e.
pj j+1 = 1)  then its Var[ ˆDj] = 0  resulting no contribution to the skewness. If an estimated
value consists of a large portion of such transitions  then the likelihoods of overestimation and
underestimation are both very low. On the other hand  if backward transition probability pj j (or any
pj k with k ≤ j) is close to 1  then Var[ ˆDj] increases dramatically  resulting a noticeable skewness.
Real-world problems can be a mix of these two extremes  which leads to a great variety of skewness
among different actions/policies  making learning signiﬁcantly more difﬁcult.
By Lemma 3.5  σ is also dependent to the number of observations Nj. As Nj grows inﬁnitely 
Var[ ˆDj] slowly decreases to 0  which reduces Var[ ˆV1] in Lemma 3.6 and eventually leads σ to
0. This indicates that running algorithms more steps does help reduce the skewness of estimated
values and improve the overall performance. However  the expression of Var[ ˆDj] in Lemma 3.5 also
indicates that the degree of improvement diminishes quickly as Nj grows. Therefore  collecting more
observations is not always an efﬁcient way to reduce the skewness.

6

(a)

(b)

(c)

Figure 3: (a) Log-normals weighted by positive reward (red) and negative reward (blue). Thick/thin
vertical lines are means & medians. (b  c) Convolution of two log-normals  given by the purple curve.

1 − p

1  rD

1

1 − p

2

p

1

p

1

1 − p

3

p

1

...

p

1

n

1  rG

Figure 4: A chain MDP with n states  forward probability p  goal reward rG and distraction reward
rD. Transitions under taking action a+ is drawn in solid arrows  and a− in dotted arrows.

4.2

Impact of Non-zero Immediate Rewards

Non-zero immediate rewards decide not only the scale of skewness  but also the direction of skewness.
By Equation 1 and 2 in Sections 3.1 and 3.2  path-wise values are weighted by their corresponding
immediate rewards before being summed into state values. If a path-wise state value is weighted by a
positive reward  then the resulting distribution is still a regular log-normal  which has a positive skew-
ness and thus a higher likelihood of underestimation. However  if it is weighted by a negative reward 
then the result is a ﬂipped log-normal  which has a negative skewness and thus a higher likelihood of
overestimation. This is illustrated in Figure 3 (a)  where the red and blue distributions correspond to
the estimated path-wise values weighted by a positive and a negative reward  respectively.
In general cases  the sum of positively skewed random variables is not necessarily a positively skewed
random variable. However  the sum of regular log-normal random variables can be approximated by
another log-normal [23]  thus is still positively skewed. Since path-wise state values are approximately
log-normal  it is clear that if an MDP only has positive immediate rewards  then all estimated values
are likely to be positively skewed and thus have higher likelihoods to be underestimated.
On the other hand  if an estimated value is composed of both positive and negative rewards  then the
skewness of regular and ﬂipped log-normal distributions may partly be neutralised in their convolution.
The purple distribution in Figure 3 (b) shows the result of convolution of two skewed distributions
that lie symmetrically to x = 0. The skewness is perfectly neutralised in this case  resulting in a
symmetric distribution with a balanced likelihood of under/overestimation. In the case of Figure 3
(c)  the convolution is still a skewed one  but the scale of this skewness is less than the original ones.
To make learning easier  one may hope to design the reward function such that the more desirable
actions/policies have both higher expected returns and higher likelihood of overestimation than the
less desirable ones. However  the former requires more positive rewards  while the latter calls for
more negative rewards  causing an unsolvable dilemma. Therefore  it is more realistic just to balance
the likelihood of under/overestimation  so that all actions/policies can compete fairly with each other.
Reward shaping [24  25] can be a promising choice to achieve this goal  as it preserves the optimality
of policies. Since a better balance of positive and negative rewards directly reduces the impact of the
skewness of all relevant log-normal distributions  this approach might be more effective than simply
collecting more observations.

5 Experiments

In this section  we present our empirical results on the skewness of estimated values. There are two
purposes in these experiments: (a) to demonstrate how substantial the harm of the skewness can be;
(b) to see the improvement provided by collecting more observations  as mentioned in Section 4.1.
We conducted experiments in chain MDPs shown in Figure 4. There are n > 0 states s1  s2  ...  sn in
a chain MDP. At each state  the agent has two possible actions a+ and a−. By taking a+ at si with

7

-4-2024estimated path-wise value00.20.40.60.8densitypositivenegative-4-2024estimated path-wise value00.20.40.60.8densitypositivenegativeconvolution-4-2024estimated path-wise value00.511.5densitypositivenegativeconvolution(a)

(b)

Figure 5: (a) Distribution of ˆV π+

(s1) at m = 200. (b) Underestimation probability curve.

are all 1  we have Var[ ˆV π−

i < n  the agent has probability p > 0 to be sent to si+1  and 1 − p to remain at si. Taking a+ at sn
yields a goal reward rG > 0  and the agent remains at sn. Taking a−  on the other hand  sends the
agent from si to si−1 (i > 1) or s1 (i = 1) with probability 1  and if a− is taken at s1  then the agent
will be provided a distraction reward rD > 0.
The objective of the learning agent is to discover a policy that leads it to the goal sn and collects rG
as often as possible  rather than being distracted by rD. There are two policy of interest: π+ that
always take a+  and π− that always take a−. Other policies can be proved to be always worse than
π+ and π− in terms of V π(s1) regardless of rG  rD  p  and discount factor γ.
Since using max operator may introduce bias [10]  we modiﬁed the default value iteration algorithm
[4] to let it output the unbiased estimated state values by following predetermined policies rather than
using max operator. In each run of experiment  m observations were collected for each state-action
pair  resulting in a data set of size 2mn. Then  the observations were passed to the modiﬁed value
iteration algorithm to estimate the state values of π+ and π− under discount factor γ = 0.9.
The Markov chain M π+ and M π−
here are both single-path ones  and thus the corresponding
theoretical distributions of ˆV can be computed directly by applying Theorem 3.7. Further  since
transition probabilities in M π−
] = 0  and thus its estimated values
always equal trivially to the ground truth one (i.e. it will never be under/overestimated).
The empirical and theoretical distributions of estimated state value ˆV π+
(s1) with m = 200  n = 20 
p = 0.1  rG = 1e6 in 1000 runs is shown in Figure 5 (a). One-sample Kolmogorov-Smirnov test was
conducted against the null hypotheses that the empirical data came from the theoretical log-normal
distributions. The resulting p-value was 0.1190  which failed to reject the null hypothesis at 5%
signiﬁcance level  indicating no signiﬁcant difference between the theoretical and sample distribution.
More importantly  Figure 5 (a) shows a clear positive skewness  indicating a higher likelihood of
underestimation. The empirical value of indicator CDF(E[ ˆV ])−0.5 was +0.103  meaning that in
60.3% of runs  the state value was underestimated. This further indicates that  if the distraction reward
rD is set to a value such that V π−
(s1)  then the agent will wrongly
select π− with probability close to 0.603  which is worse than random guess.
To see whether collecting more observations helps reduce skewness  the same experiments as above
were conducted with the number of observations per state-action m ranged from 20 to 400. Figure 5
(b) shows the theoretical and empirical probability of underestimation Pr( ˆV π+
(s1)).
At m = 20  200 and 400  the empirical underestimate probability was 0.741  0.603 and 0.563 
respectively. While from m = 20 to 200 there was an signiﬁcant improvement of 0.138  or a 18.6%
relative improvement  from 200 to 400 it was only 0.040  or 6.6% relative. This result supports
the analysis in Section 4.1  demonstrating that the merit of collecting more observations is most
noticeable when the sample size is low  and diminishes quickly as the sample size grows.
We also conducted experiments in the complex maze domain [26] in the same manner as above. In
this domain  the task of the agent is to ﬁnd a policy that can collect all ﬂags and bring them to the
goal as often as possible  without falling into any traps. The maze used is given in Figure 6 (a).
The states in this domain is represented by the current position of the agent and the status of the three
ﬂags. The agent starts at the start point indicated by S with no ﬂag. At each time step  the agent can

(s1) is slightly less than V π+

(s1) < E ˆV π+

8

05101520estimated value00.050.10.150.2densitysample distributiontheoretical distribution0100200300400#observations per state-action0.50.550.60.650.70.750.8probability of underestimationempiricaltheoretical1

T

S

X

T

T

2 T

X

T
T

X

X 3

X X X
X
G

X

(a)

(b)

(c)

Figure 6: (a) A complex maze. S  G  numbers  and circles stand for start  goal  ﬂags  and traps 
respectively. (b) Distribution of ˆV π∗

(sstart) at m = 10. (c) Underestimation probability curve.

select one of the four directions to move to. The agent is then sent to the adjacent grid at the chosen
direction with probability 0.7  and at each of the other three directions with probability 0.1  unless the
destination is blocked  in which case the agent remains at the current grid. Additionally  at the ﬂag
grids (numbers in Figure 6 (a))  taking actions also provides the corresponding ﬂag to the agent if
that ﬂag has not been obtained yet. At the goal point (G)  taking arbitrary action yields an immediate
reward equals to 1  100  1002 or 1003 if the agent holds 0  1  2 or 3 ﬂags  respectively. Then the agent
is sent back to the start point  and all three ﬂag are reset to their initial position. Finally  at any trap
grid (circles)  taking actions sends the agent to S and resets all ﬂags without yielding a goal reward.
The complex maze in Figure 6(a) has 440 states  4 actions  32 non-zero immediate rewards  and
complicated transition patterns  and thus is difﬁcult to analyse manually. However  it is noticeable
that all non-zero immediate rewards are positive  and thus according to Section 4.2  estimated state
values are likely to have positive skew  resulting in greater likelihood of underestimation.
Figure 6 (b) shows the empirical distribution of estimated value ˆV π∗
(sstart  no ﬂag) under γ = 0.9 and
m = 10 in 1000 runs. Although it is not a path-wise state value  the distribution is approximately
log-normal with parameter µ ≈ 8.21  σ ≈ 0.480. In 67.6% of these 1000 runs  the optimal state
value at the start state was underestimated.
The effect of collecting a larger sample is show in Figure 6 (c). The probability of underestimation
decreased from 0.676 at m = 10 to 0.597 at m = 50  0.563 at m = 100  and 0.556 at m = 200.
The data points approximated an exponential function y = 0.1725 exp(−0.04015x) + 0.5546  which
suggests that it can be very difﬁcult to achieve underestimation probability lower than 0.55 by
collecting more data in this domain.

6 Conclusion and Future Work

This paper has shown that estimated state values computed using the Bellman equation can be
decomposed to the relevant path-wise state values  and the latter obey log-normal distributions.
Since log-normal distributions are skewed  the estimated state values also have skewed distributions 
resulting in imbalanced likelihood of under/overestimation  which can be harmful for learning.
We have also pointed out that the direction of such imbalance is decided by the immediate reward
associated to the log-normal distributions  and thus  by carefully balancing the impact of positive and
negative rewards when designing the MDPs  such undesirable imbalance can possibly be neutralised.
Collecting more observations  on the other hand  helps reduce the skewness to a degree  but such
effect becomes less signiﬁcant when the sample size is already large.
It would be interesting to see how the skewness studied in this paper interacts with function approxi-
mation (e.g. neural networks [27  28])  policy gradient [29  30]  or Monte-Carlo tree search [31  32].
A reasonable guess is that these techniques introduce their own skewness  and the two different
skewness amplify each other  making learning even more difﬁcult. On the other hand  reducing the
skewness discussed in this paper may improve learning performance even when such techniques
are used. Therefore  developing a concrete method of balancing positive and negative rewards (as
discussed in Section 4.2) can be very helpful  and will be investigated in the future.

9

20004000600080001000012000estimated state value0123density10-4m=10lognormal fit04080120160200#observations per state-action0.50.550.60.650.7probability of underestimationdatacurve fitAcknowledgements

This paper was supported by Ministry of Science and Technology of China (Grant No.
2017YFB1003102)  the National Natural Science Foundation of China (Grant Nos. 61672478
and 61329302)  the Science and Technology Innovation Committee Foundation of Shenzhen (Grant
No. ZDSYS201703031748284)  EPSRC (Grant No. J017515/1)  and in part by the Royal Society
Newton Advanced Fellowship (Reference No. NA150123).

References
[1] Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press 

Cambridge  MA  USA  1st edition  1998.

[2] Csaba Szepesvári. Algorithms for reinforcement learning. Synthesis lectures on artiﬁcial

intelligence and machine learning  4(1):1–103  2010.

[3] Christopher Watkins and Peter Dayan. Q-learning. Machine learning  8(3-4):279–292  1992.

[4] Martin Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

Wiley-Interscience  1994.

[5] Peter Dayan. The convergence of TD (λ) for general λ. Machine learning  8(3-4):341–362 

1992.

[6] John N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Machine Learning 

16(3):185–202  1994.

[7] Michael L. Littman  Thomas L. Dean  and Leslie P. Kaelbling. On the complexity of solving
In Proceedings of the Eleventh Conference on Uncertainty in

markov decision problems.
Artiﬁcial Intelligence  pages 394–402. Morgan Kaufmann Publishers Inc.  1995.

[8] Csaba Szepesvári. The asymptotic convergence-rate of Q-learning. In Proceedings of the 10th
International Conference on Neural Information Processing Systems  pages 1064–1070. MIT
Press  1997.

[9] Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement
learning. In Proceedings of the 1993 Connectionist Models Summer School Hillsdale  NJ.
Lawrence Erlbaum. Citeseer  1993.

[10] Hado Van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems 

pages 2613–2621  2010.

[11] Marc G. Bellemare  Georg Ostrovski  Arthur Guez  Philip S. Thomas  and Rémi Munos.
Increasing the action gap: New operators for reinforcement learning. In Proceedings of the 30th
AAAI Conference on Artiﬁcial Intelligence  pages 1476–1483  2016.

[12] Donghun Lee  Boris Defourny  and Warren B. Powell. Bias-corrected Q-learning to control
In Adaptive Dynamic Programming And Reinforcement

max-operator bias in Q-learning.
Learning (ADPRL)  2013 IEEE Symposium on  pages 93–99. IEEE  2013.

[13] Hado Van Hasselt  Arthur Guez  and David Silver. Deep reinforcement learning with double
In Proceedings of the 30th AAAI Conference on Artiﬁcial Intelligence  pages

Q-learning.
2094–2100  2016.

[14] Carlo D’Eramo  Alessandro Nuara  Matteo Pirotta  and Marcello Restelli. Estimating the
maximum expected value in continuous reinforcement learning problems. In Proceedings of the
31th AAAI Conference on Artiﬁcial Intelligence  pages 1840–1846  2017.

[15] Dimitri P. Bertsekas and Huizhen Yu. Q-learning and enhanced policy iteration in discounted

dynamic programming. Mathematics of Operations Research  37(1):66–94  2012.

[16] Paul Wagner. Policy oscillation is overshooting. Neural Networks  52:43–61  2014.

10

[17] Nan Jiang  Alex Kulesza  Satinder Singh  and Richard Lewis. The dependence of effective
planning horizon on model accuracy. In Proceedings of the 2015 International Conference on
Autonomous Agents and Multiagent Systems  pages 1181–1189. International Foundation for
Autonomous Agents and Multiagent Systems  2015.

[18] Harm Van Seijen  A. Rupam Mahmood  Patrick M. Pilarski  Marlos C. Machado  and Richard S.
Sutton. True online temporal-difference learning. Journal of Machine Learning Research 
17(145):1–40  2016.

[19] David P. Doane and Lori E. Seward. Measuring skewness: a forgotten statistic. Journal of

Statistics Education  19(2):1–18  2011.

[20] Harold Hotelling and Leonard M. Solomons. The limits of a measure of skewness. The Annals

of Mathematical Statistics  3(2):141–142  05 1932.

[21] Gary W. Oehlert. A note on the delta method. The American Statistician  46(1):27–29  1992.

[22] George Casella and Roger L. Berger. Statistical inference. 2nd edition  2002.

[23] Norman C. Beaulieu and Qiong Xie. An optimal lognormal approximation to lognormal sum

distributions. IEEE Transactions on Vehicular Technology  53(2):479–489  2004.

[24] Andrew Y. Ng  Daishi Harada  and Stuart Russell. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In Proceedings of the Sixteenth International
Conference on Machine Learning  volume 99  pages 278–287  1999.

[25] John Asmuth  Michael L. Littman  and Robert Zinkov. Potential-based shaping in model-based
reinforcement learning. In Proceedings of the 23th AAAI Conference on Artiﬁcial Intelligence 
pages 604–609  2008.

[26] Liangpeng Zhang  Ke Tang  and Xin Yao. Increasingly cautious optimism for practical PAC-
MDP exploration. In Proceedings of the 24th International Joint Conference on Artiﬁcial
Intelligence  pages 4033–4040  2015.

[27] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G.
Bellemare  Alex Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig Pe-
tersen  Charles Beattie  Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan
Wierstra  Shane Legg  and Demis Hassabis. Human-level control through deep reinforcement
learning. Nature  518(7540):529–533  2015.

[28] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap 
Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
ment learning. In Proceedings of the 33rd International Conference on Machine Learning 
pages 1928–1937  2016.

[29] Sham Kakade. A natural policy gradient. Advances in neural information processing systems 

2:1531–1538  2002.

[30] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust
region policy optimization. In Proceedings of The 32nd International Conference on Machine
Learning  pages 1889–1897  2015.

[31] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning.

Conference on Machine Learning  2006.

In European

[32] Cameron B. Browne  Edward Powley  Daniel Whitehouse  Simon M. Lucas  Peter I. Cowling 
Philipp Rohlfshagen  Stephen Tavener  Diego Perez  Spyridon Samothrakis  and Simon Colton.
A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence
and AI in games  4(1):1–43  2012.

11

,Franz Kiraly
Louis Theran
Xi Chen
Yu Cheng
Bo Tang
Liangpeng Zhang
Ke Tang
Xin Yao