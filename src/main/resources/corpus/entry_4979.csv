2009,Sensitivity analysis in HMMs with application to likelihood maximization,This paper considers a sensitivity analysis in Hidden Markov Models with continuous state and observation spaces. We propose an Infinitesimal Perturbation Analysis (IPA) on the filtering distribution with respect to some parameters of the model. We describe a methodology for using any algorithm that estimates the filtering density  such as Sequential Monte Carlo methods  to design an algorithm that estimates its gradient. The resulting IPA estimator is proven to be asymptotically unbiased  consistent and has computational complexity linear in the number of particles. We consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations. We derive an IPA estimator for the gradient of the log-likelihood  which may be used in a gradient method for the purpose of likelihood maximization. We illustrate the method with several numerical experiments.,Sensitivity analysis in HMMs

with application to likelihood maximization

Pierre-Arnaud Coquelin 

Vekia  Lille  France

pacoquelin@vekia.fr

Romain Deguest⁄

Columbia University  New York City  NY 10027

rd2304@columbia.edu

INRIA Lille - Nord Europe  Sequel Project  France

Rémi Munos

remi.munos@inria.fr

Abstract

This paper considers a sensitivity analysis in Hidden Markov Models with con-
tinuous state and observation spaces. We propose an Inﬁnitesimal Perturbation
Analysis (IPA) on the ﬁltering distribution with respect to some parameters of the
model. We describe a methodology for using any algorithm that estimates the ﬁl-
tering density  such as Sequential Monte Carlo methods  to design an algorithm
that estimates its gradient. The resulting IPA estimator is proven to be asymptoti-
cally unbiased  consistent and has computational complexity linear in the number
of particles.
We consider an application of this analysis to the problem of identifying unknown
parameters of the model given a sequence of observations. We derive an IPA
estimator for the gradient of the log-likelihood  which may be used in a gradient
method for the purpose of likelihood maximization. We illustrate the method with
several numerical experiments.

1 Introduction

We consider a parameterized hidden Markov model (HMM) deﬁned on continuous state and ob-
servation spaces. The HMM is deﬁned by a state process (Xt)t‚0 ∈ X and an observation process
(Yt)t‚1 ∈ Y that are parameterized by a continuous parameter θ = (θ1  . . .   θd) ∈ Θ  where Θ is a
compact subset of Rd.
The state process is a Markov chain taking its values in a (measurable) state space X  with initial
probability measure µ ∈ M(X) (i.e. X0 ∼ µ) and Markov transition kernel K(θ  xt  dxt+1). We
assume that we can sample this Markov chain using a transition function F and independent random
numbers  i.e. for all t ≥ 0 

i.i.d.∼ ν 

Xt+1 = F (θ  Xt  Ut)  with Ut

(1)
where F : Θ × X × U → X and (U  σ(U)  ν) is a probability space. In many practical situations
U = [0  1]p  ν is uniform  thus Ut is a p-uple of uniform random numbers. For simplicity  we
adopt the notations F (θ  x¡1  u)   Fµ(θ  u)  where Fµ is the ﬁrst transition function (i.e. X0 =
Fµ(θ  U¡1) with U¡1 ∼ ν).
The observation process (Yt)t‚1 lies in a (measurable) space Y and is linked with the state process
by the conditional probability measure P(Yt ∈ dyt|Xt = xt) = g(θ  xt  yt) dyt  where g : Θ ×

∗

also afﬁliated with CMAP  Ecole Polytechnique  France

1

X × Y → [0  1] is the marginal density function of Yt given Xt. We assume that observations are
conditionally independent given the state.
Since the transition and observation processes are parameterized by the parameter θ  the state Xt
and the observation Yt processes depend explicitly on θ. For notation simplicity we will omit to
write the dependence of θ (in K  F   g  Xt  Yt  ...) when there is no possible ambiguity.
One of the main interest in HMMs is to recover the state at time n given a sequence of past obser-
vations (y1  . . .   yn) (written y1:n). The ﬁltering distribution (or belief state)

πn(dxn)   P(Xn ∈ dxn|Y1:n = y1:n)

is the distribution of Xn conditioned on the information y1:n. We deﬁne analogously the predictive
distribution

πn+1jn(dxn+1)   P(Xn+1 ∈ dxn+1|Y1:n = y1:n).

∫

Our contribution is an Inﬁnitesimal Perturbation Analysis (IPA) that estimates the gradient ∇πn
(where ∇ refers to the derivative with respect to the parameter θ) of the ﬁltering distribution πn.
More precisely  we estimate ∇πn(f) (where π(f)  
X f(x)π(dx)) for any integrable function f
under the ﬁltering distribution πn.
We also consider as application  the problem of parameter identiﬁcation in HMMs which consists
⁄ of the model that has served to generate the sequence
in estimating the (unknown) parameter θ
of observations. In a Maximum Likelihood (ML) approach  one searches for the parameter θ that
maximizes the likelihood (or its logarithm) given the sequence of observations. The log-likelihood
of parameter θ is deﬁned by ln(θ)   log pθ(y1:n) where pθ(y1:n) dy1:n   P(Y1:n(θ) ∈ dy1:n).
The Maximum Likelihood (ML) estimator ˆθn   arg maxθ2Θ ln(θ) is asymptotically consistent
⁄ when n → ∞ under some
(in the sense that ˆθn converges almost surely to the true parameter θ
identiﬁably conditions and mild assumptions on the model  see Theorem 2 of [DM01]). Thus  using
the ML approach  the parameter identiﬁcation problem reduces to an optimization problem.
Our second contribution is a sensitivity analysis of the predictive distribution ∇πt+1jt  for t <
n  which enables to estimate the gradient ∇ln(θ) of the log-likelihood function  which may be
used in a (stochastic) gradient method for the purpose of optimizing the likelihood. The approach
is numerically illustrated on two parameter identiﬁcation problems (autoregressive model and a
stochastic volatility model) and compared to other approaches (EM algorithm  the Kalman ﬁlter 
and the Likelihood ratio approach) when these latter apply.

2 Links with other works

First  let us mention that we are interested in the continuous state case since numerous applications
in signal processing  ﬁnance  robotics  or telecommunications naturally ﬁt in this framework. In the
general setting there exists no closed-form expression of the ﬁltering distribution (unlike in ﬁnite
spaces where the Viterbi algorithm may apply or in linear-Gaussian models where the Kalman ﬁlter
can be used). Thus  in this paper  we will make use of the so-called Sequential Monte Carlo
methods (SMC) (also known as Particle Filters) which are numerical tools that can be applied to
a large class of models  see e.g. [DFG01]. For illustration  a challenging example in ﬁnance is
the problem of parameter estimation in the stochastic volatility model  which is a non-linear non-
Gaussian continuous space HMM parameterized by three continuous parameters (see e.g. [ME07])
which will be described in the experimental section.
A usual approach for parameter estimation consists in performing a maximum likelihood estimation
(MLE)  i.e. search for the most likely value of the parameter  given the observed data. For ﬁnite state
space problems  the Expectation Maximization (EM) algorithm is a popular method for solving the
MLE problem. However  in continuous space problems  see [CM05]  the EM algorithm is difﬁcult to
use mainly because the Expectation part relies on the estimation of the posterior path measure which
is intractable in many situations. The Maximization part may also be very complicated and time-
consuming when the model does not belong to a linear or exponential family. An alternative method
consists in using brute force optimization methods based on the evaluation of the likelihood such
as grid-based or simulated annealing methods. These approaches  which can be seen as black-box
optimization are not very efﬁcient in high dimensional parameter spaces.

2

Another approach is to treat the parameter as part of the state variable and then compute the optimal
ﬁlter (see [DFG01] and [Sto02]). In this case  the Bayesian posterior distribution of the parameter
is a marginal of the optimal ﬁlter. It is well known that those methods are stable only under certain
conditions  see [Pap07]  and do not perform well in practice for a large number of time steps.
A last solution consists in using an optimization procedure based on the evaluation of the gradient
of the log-likelihood function with respect to the parameter. These approaches have been studied in
the ﬁeld of continuous space HMMs e.g. in [DT03  FLM03  PDS05  Poy06]. The idea was to use a
likelihood ratio approach (also called score method) to evaluate the gradient of the likelihood. This
approach suffers from high variance of the estimator  in particular for problems with small noise in
the dynamic. To tackle this issue  [PDS05] proposed to use a marginal particle ﬁlter instead of a
simple path-based particle ﬁlter as Monte Carlo approximation method. This approach is efﬁcient
in terms of variance reduction but its computational complexity becomes quadratic in the number of
particles instead of being linear  like in path-based particle methods.
The IPA approach proposed in this paper is an alternative gradient-based maximum likelihood ap-
proach. Compared with works on gradient approaches previously cited  the IPA provides usually a
lower variance estimators than the likelihood ratio methods  and its numerical complexity is linear
in the number of particles.
Other works related to ours are the so-called tangent ﬁlter approach described in [CGN01] for dy-
namics coming from a discretization of a diffusion process  and the Finite-Difference (FD) approach
described in a different setting (i.e. policy gradient in Partially Observable Markov Decision Pro-
cesses) in [CDM08]. A similar FD estimator could be designed in our setting too but the resulting
FD estimator would be biased (like usual FD schemes) whereas the IPA estimator is not.

3 Sequential Monte Carlo methods (SMC)
Given a measurable test function f : X → R  we have:
πn(f)   E[f(Xn)|Y1:n = y1:n] =

∫∏

f(xn)
n

∏

∫

n

t=0 K(xt¡1  dxt)Gt(xt)

t=0 K(xt¡1  dxt)Gt(xt)

=

∏

∏

E[f(Xn)

E[

where we used the simpliﬁed notation: Gt(xt)   g(xt  yt) and G0(x0)   1.
In general  it is impossible to write πn(f) analytically except for speciﬁc cases (such as lin-
ear/Gaussian with Kalman ﬁltering). In this paper  we consider a numerical approximation of πn(f)
based on a SMC method. But it should be mentioned that other methods (such as Extended Kalman
ﬁlter  quantization methods  Markov Chain Monte Carlo methods) may be used as well to build the
IPA estimator that we propose in the next section.
The basic SMC method  called Bootstrap Filter  see [DFG01] for details  approximates πn(f) by an
empirical distribution πN

n) made of N particles x1:N
n .

n (f)   1

∑

i=1 f(xi

N

N

n

t=0 Gt(Xt)]

.

n

t=0 Gt(Xt)]

(2)

Algorithm 1 Generic Sequential Monte Carlo

for t = 1 to n do

t¡1

Sampling: Sample ui
importance sampling weights wi
Resampling: Set xi
w1:N
end for
RETURN: πN

t =exki
∑

n (f) = 1

N

.

t

i=1 f(xi
n)

iid∼ ν and setexi
t = F (xi
t¡1  ui
t = Gt(exi
PN
j=1 Gt(exj
 
t  ∀i ∈ {1  . . .   N}  where k1:N are indices selected from the weights

t¡1) ∀i ∈ {1  . . .   N}. Then deﬁne the

t )

t)

N

The sampling (or transition) step generates a successor particle population ex1:N
the setex1:N

according to the
are eval-
from
. Resampling is used to avoid the problem of degeneracy
of the algorithm  i.e. that most of the weights decreases to zero. It consists in selecting new parti-

state dynamics from the previous population x1:N
uated  and the resampling (or selection) step resamples (with replacement) N particles x1:N

t¡1. The importance sampling weights w1:N

according to the weights w1:N

t

t

t

t

t

3

∑

tφ(exi

∑

t) = E[ 1

N
i=1 wi

t = j) = wj

t)]).
i=1 φ(xi
cle positions such as to preserve a consistency property (i.e.
by an independent
The simplest version introduced in [GSS93] chooses the selection indices k1:N
sampling from the set {1  . . .   N} according to a multinomial distribution with parameters w1:N
 
t   for all 1 ≤ i ≤ N. The idea is to replicate the particles in proportion to their
i.e. P(ki
weights. Many variants have been proposed in the literature  among which the stratiﬁed resampling
method [Kit96] which is optimal in terms of variance minimization.
n (f) to πn(f) (e.g. Law of Large Numbers or Central Limit Theorems) are
Convergence issues of πN
n (f) is
discussed in [Del04] or [DM08]. For our purpose we note that under mild conditions on f  πN
an asymptotically unbiased (see [DMDP07] for the asymptotic expression of the bias) and consistent
estimator of πn(f).

N

N

t

t

4 Inﬁnitesimal Perturbation Analysis in HMMs

∏
∏
∇E[
E[

4.1 Sensitivity analysis of the ﬁltering distribution

]

∏

∏
∇E[f(Xn)

[

∏
∏
∏

n

n

n

n

n

=

E[

E[

E[f(Xn)

−πn(f)

t=0 Gt(Xt)]

t=0 Gt(Xt)]

t=0 Gt(Xt)]

t=0 Gt(Xt)]

The following decomposition of the gradient of the ﬁltering distribution πn applied to a function f:
∇[πn(f)] = ∇
t=0 Gt(Xt)]
t=0 Gt(Xt)]
(3)
shows that the problem of ﬁnding an estimator of ∇πn(f) is reduced to the problem of ﬁnding an
estimator of ∇E[f(Xn)
t=0 Gt(Xt)]. There are two dominant inﬁnitesimal methods for estimat-
ing the gradient of an expectation in a Markov chain: the Inﬁnitesimal Perturbation Analysis (IPA)
method and the Score Function (SF) method (also called likelihood ratio method)  see for instance
[Gla91] and [Pﬂ96] for a detailed presentation of both methods. SF has been used in [DT03  FLM03]
to estimate ∇πn. Although IPA is known for having a lower variance than SF in general  as far as
we know  it has never been used in this context. This is therefore the object of this Section.
Under appropriate smoothness assumptions (see Proposition 1 below)  the gradient of an expectation
over a random variable X is equal to an expectation involving the pair of random variables (X ∇X)

n

n

(where 0 refers to the derivative with respect to the state variable). Applying this property to estimate
∇E [f(Xn)

n

∏

t=0 Gt(Xt)]  we deduce
∇E

f(Xn)

[

]

0(X)∇X] 
∇E[f(X)] = E[∇[f(X)]] = E[f
]]
n∏
n∏
)
n∏

[
∇
n∑
f(Xn)
∇[Gt(Xt)]
n∑
Gt(Xt)

Gt(Xt)

Gt(Xt)

= E

t=0

t=0

t=0

∇[f(Xn)] + f(Xn)

[
[(
[(

= E

= E

f

0(Xn)∇Xn + f(Xn)

]

{
Now we deﬁne an augmented Markov chain (Xt  Zt  Rt)t‚0 by the following recursive relations
(where Zt   ∇Xt)

X0 = Fµ(U¡1)  U¡1 ∼ ν
Z0 = ∇Fµ(U¡1) 
R0 = 0 

(4)

.

t=0

t=0

)

Gt(Xt)

Gt(Xt)

Gt(Xt)

n∏

t(Xt)∇Xt + ∇Gt(Xt)
0
G

]
 Xt+1 = F (Xt  Ut)  where Ut ∼ ν
∏

Zt+1 = ∇F (Xt  Ut) + F
Rt+1 = Rt + G
∏
∏

t=0 Gt(Xt)]

Gt+1(Xt+1)

t=0 Gt(Xt)]

n

n

t=0 Gt(Xt)]

0(Xt  Ut)Zt 

t+1(Xt+1)Zt+1+rGt+1(Xt+1)
0

 

.

(5)

t=0

∀t ≥ 0 

∏

By introducing this augmented Markov Chain in Equation (4) and using Equation (3) we can rewrite
∇πn(f) as:

∇πn(f) =

E[(f

E[(f

=

0(Xn)Zn + f(Xn)Rn)
t=0 Gt(Xt)]
0(Xn)Zn + Rn(f(Xn) − πn(f)))

E[

n

n

∏

∏

E[Rn
E[

− πn(f)
t=0 Gt(Xt)]

n

E[

n

t=0 Gt(Xt)]

We now state some sufﬁcient conditions under which the previous derivations are sound.

4

Proposition 1. Equation (5) is valid on Θ whenever the following conditions are satisﬁed:

• for all θ ∈ Θ  the path θ 7→ (X0  X1 ···   Xn)(θ) is almost surely (a.s.) differentiable 
• for all θ ∈ Θ  f is a.s. continuously differentiable at Xn(θ)  and for all 1 ≤ t ≤ n  Gt is
a.s. continuously differentiable at (θ  Xt(θ)) 
• θ 7→ f(Xn(θ)) and for all 1 ≤ t ≤ n  θ 7→ Gt(θ  Xt(θ)) are a.s. continuous and piecewise
differentiable throughout Θ 
• Let D be the random subset of Θ at which f(Xn(θ)) or one Gt(θ  Xt(θ)) fails to be differ-
t=0 Gt(Xt)] < ∞ 
entiable. We require that E[sup
θ /2D

0(Xn) Zn + Rn (f(Xn) − πn(f))|∏

|f

n

The proof of this Proposition is a direct application of Theorem 1.2 from [Gla91]. We notice that
requiring the a.s. differentiability of the path θ 7→ (X0  X1 ···   Xn)(θ) is equivalent to requiring
that for all θ ∈ Θ  the transition function F is a.s. continuously differentiable with respect to θ.
From Equation (5)  we can derive the IPA estimator of ∇πn(f) by using a SMC algorithm:

N∑

[

f

i=1

(

)]

rj
n

 

N∑

j=1

I N
n

  1
N

0(xi

n)zi

n + f(xi
n)

ri
n

− 1
N

(6)

n) are particles derived by using a SMC algorithm on the augmented Markov chain

n  ri

n  zi

where (xi
(Xt  Zt  Rt) described in Algorithm 2.
Algorithm 2 IPA estimation of ∇πn
for t = 1 to n do

t¡1

For all i ∈ {1  . . .   N} do
Sample ui
Set ˜zi
Set ˜ri
Set (xi

t = ∇F (xi
t¡1 + G
t = ri
t  zi
t  ri

iid∼ ν and set ˜xi
0(xi
t¡1) + F
t¡1  ui
t+rGt(˜xi
0
t)˜zi
t(˜xi
t)
Gt(˜xi
t)
t   ˜zki

t) = (˜xki

end for
RETURN: I N

n = 1

N

N
i=1

t = F (xi

t¡1  ui

t¡1) 
t¡1)zi

t¡1 

t¡1  ui
Pj Gt(˜xj
  and compute the weights wi t = Gt(˜xi
t)
t )
t   ˜rki
t )  where k1:N are the indices selected from w1:N
0(xi

)]

∑

− 1

(

n + f(xi
n)

N
j=1 rj
n

n)zi

[

ri
n

f

t

N

 

∑

Proposition 2. Under the assumptions of Proposition 1  the estimator I N
n ] = ∇πn(f) + O(N
O(N
∇πn(f) almost surely. In addition  its (asymptotic) variance is O(N
¡1).

¡1) and is consistent with ∇πn(f)  i.e. E[I N

n deﬁned by (6) has a bias
¡1)  and limN!1 I N
n =

E[H(X0:n)Qn

Proof. We use the general SMC convergence properties for Feynman-Kac (FK) models (see [Del04]
or [DM08]) which  applied to a FK ﬂow with Markov chain X0:n  (random) potential func-
tions G(X0:n)  and test function H(X0:n)  states that the SMC estimate:
0:n) is
. Moreover  an asymptotic expression of the bias  given
consistent with
¡1). Applying those results to the test function
in [DMDP07]  shows that it is of order O(N
0(Xn)Zn + Rn(f(Xn)− πn(f))  using the representation (5) of the gradient  we deduce that
H   f
the SMC estimator (6) is asymptotically unbiased and consistent with ∇πn(f). Now the asymptotic
¡1) since the Central Limit Theorem (see e.g. [Del04  DM08]) applies to the IPA
variance is O(N
estimator (6) of (5).

i=1 H(xi

E[Qn

t=0 G(Xt)]

t=0 G(Xt)]

1
N

N

∑

Remark 1. Notice that the computation of the gradient estimator requires O(nN md) (where m is
the dimension of X) elementary operations  which is linear in the number of particles N and linear
in the number of parameters d  and has memory requirement O(N md).

5

∇ln(θ) =

∇πt+1jt(Gt+1)
πt+1jt(Gt+1)

n¡1∑

t=0

∏
∏

4.2 Gradient of the log-likelihood

In the Maximum Likelihood approach for the problem of parameter identiﬁcation  one may follow
a stochastic gradient method for maximizing the log-likelihood ln(θ) where the gradient

is obtained by estimating each term ∇πt+1jt(Gt+1) of the sum using a similar decomposition as in
(5) and (4) for the predictive distribution applied to Gt+1:

]

∇πt+1jt(Gt+1) = ∇

t

k=0 Gk(Xk)]

k=0 Gk(Xk)]

=

t

k=0 Gk(Xk)]

− πt+1jt(Gt+1)

k=0 Gk(Xk)]

∏
∏
∇E[
E[

t

k=0 Gk(Xk)]
k=0 Gk(Xk)]

t

[

t

E[Gt+1(Xt+1)

E[

∏
∇E[Gt+1(Xt+1)
[(

∏

E[

t

Gk(Xk)] = E

with
∇E[Gt+1(Xt+1)

t∏

k=0

) t∏

]

Gk(Xk)

.

+Gt+1(Xt+1)

t∑

∇Gt+1(Xt+1) + G
t+1(Xt+1)∇Xt+1
0
k(Xk)∇Xk + ∇Gk(Xk)
0
G
)

Gk(Xk)

k=0

∑

− 1

N

j rj

t¡1)

t)(ri
t + Gt(˜xi
t)˜zi
t)
i=1 Gt(˜xi

N

t¡1

k=0

 

We deduce the IPA estimator of ∇ln(θ)
∇Gt(˜xi

N
i=1

  n∑

∑

(

J N
n

0
t(˜xi
t) + G

∑

t=1

n  ri

n  zi

n  ˜ri

n  ˜zi

n) (and (˜xi

where (xi
n)) are particles derived by using a SMC algorithm on the aug-
mented Markov chain (Xt  Zt  Rt) described in the previous subsection. Using similar arguments
as those detailed in proofs of Propositions 1 and 2  we have that this estimator is asymptotically
unbiased and consistent with ∇ln(θ).
∑
The resulting gradient algorithm is described in Algorithm 3. The steps γk are chosen appropriately
k < ∞)  see e.g. [KY97]
k‚1 γk = ∞ and
so that local convergence occurs (e.g. such that
for a detailed analysis of Stochastic Approximation algorithms.
Algorithm 3 Likelihood Maximization by gradient ascent using the IPA estimator of ∇ln(θ)
for k = 1  2  . . .   Number of gradient steps do

∑

k‚1 γ2

0 = 0
Initialize J N
for t = 1 to n do

t¡1

For all i ∈ {1  . . .   N} do
iid∼ ν and set ˜xi
Sample ui
t¡1 + PN
Set ˜zi
t¡1) + F
t¡1  ui
i=1(rGt(˜xi
Set J N
t+rGt(˜xi
t)˜zi
t)
Gt(˜xi
t)
t   ˜zki

t = ∇F (xi
t = J N
t = ri
t  zi

t¡1 + G
t  ri

Set ˜ri
Set (xi

t) = (˜xki

t   ˜rki

0
t(˜xi

t = F (xi

0(xi

t¡1) 
t¡1  ui
t¡1)zi
t¡1 
t¡1  ui
PN
0
t+Gt(˜xi
t)˜zi
t)(ri
t(˜xi
i=1 Gt(˜xi
t)

t)+G

N Pj rj

t¡1¡ 1

t¡1))

 

and compute the weights wi

Pj Gt(˜xj
t = Gt(˜xi
t)
t )
t )  where k1:N are indices selected from w1:N
.

t

end for
Perform a gradient ascent step: θk = θk¡1 + γk J N

n (θk¡1)

end for

6

Figure 1: Box-and-whiskers plots of the three parameters (φ  σ  β) estimates for the AR1 model
with θ? = (0.8  1.0  1.0). We compare three methods: (1) Kalman  (2) EM and (3) IPA. Here we
used n = 500 observations and N = 102 particles.

5 Numerical experiments

We consider two typical problems and report our results focussing on the variance of the estimator:
Autoregressive model AR1 is a simple linear-Gaussian HMMs thus may be solved by other meth-
ods (such as Kalman ﬁltering and EM algorithms) which enables to compare the performances of
several algorithms for parameter identiﬁcation. The dynamics are

X0 ∼ N (0  σ2) 

and for t ≥ 1  Xt = φXt¡1 + σUt 

i.i.d.∼ N (0  1) and Vt

(7)
i.i.d.∼ N (0  1) are independent sequences of random variables  and

Yt = Xt + βVt 

where Ut
θ = (φ  σ  β) is a three-dimensional parameter in (R+)3.
Stochastic volatility model is very popular in the ﬁeld of quantitative ﬁnance [ME07] to evaluate
derivative securities  such as options. This is a non-linear non-Gaussian model  so the Kalman
method cannot be used anymore. The dynamics are

X0 ∼ N (0  σ2) 

and for t ≥ 1  Xt = φXt¡1 + σUt 

Yt = β exp (Xt/2) Vt 

(8)

i.i.d.∼ N (0  1) and Vt

i.i.d.∼ N (0  1) and the parameter θ = (φ  σ  β) ∈ (R+)3.

where again Ut
5.1 Parameter identiﬁcation
Figure 1 shows the results of our IPA gradient estimator for the AR1 parameter identiﬁcation prob-
lem and compares those with two other methods: Kalman ﬁlter (K) and EM (which apply since the
⁄ = (0.8  1.0  1.0). Notice the apparent
model is linear-Gaussian). The unknown parameter used is θ
⁄ (even for Kalman which provides here the exact
bias of the three methods in the estimation of θ
ﬁltering distribution) since the number of observations n = 500 is ﬁnite. For IPA  we used N = 102
particles and 150 gradient iterations. Algorithm 3 was run 50 times with random starting points uni-
formly drawn between [θ  ¯θ]  where θ = (0.5  0.5  0.5) and ¯θ = (1.0  1.5  1.5) in order to illustrate
that the method is not sensitive to the starting point.
We observe that in terms of estimation accuracy  IPA is very competitive to the other methods 
Kalman and EM  which are designed for speciﬁc models (here linear-Gaussian). The IPA method
applies to general models  for example  to the stochastic volatility model. Figure 2 shows the sets of
estimates of θ? = (0.8  1.0  1.0) using IPA with n = 103 observations and N = 102 particles (no
comparison is made here since Kalman does not apply and EM becomes more complicated).
5.2 Variance study for Score and IPA algorithms
IPA and Score methods provide gradient estimators for general models. We compare the variance
of the corresponding estimators of the gradient ∇ln for the AR1 since for this model we know its
exact value (using Kalman).

7

1230.810.820.830.840.850.860.870.880.89φMethod1230.750.80.850.90.951σMethod1231.061.081.11.121.141.161.181.21.221.24βMethodFigure 2: Box-and-whiskers plots of the three parameters (φ  σ  β) estimates for the IPA method
applied to the stochastic volatility model with θ? = (0.8  1.0  1.0). We used n = 103 observations
and N = 102 particles.

Figure 3 shows the variance of the IPA and Score estimators of the partial derivative ∂σln (we
focused our study on σ since the problem of volatility estimation is challenging  and also because
the value of σ inﬂuences the respective performances of the two algorithms  which is not the case
for the other parameters φ  β). We used n = N = 103. The IPA estimator performs better than the
Score estimator for small values of σ. On the other hand  in case of huge variance in the state model 
it is better to use the Score estimator.

Figure 3: Variance of the log-likelihood derivative ∂σln computed with both the IPA and Score meth-
⁄ = (φ?  σ?  β?) = (0.8  1.0  1.0) and the estimations are computed at
ods. The true parameter is θ
θ = (0.7  σ  0.9).

Let us mention that the variance of the IPA (as well as Score) estimator increases when the number
of observations n increases. However  under weak conditions on the HMM [LM00]  the ﬁltering
distribution and its gradient forget exponentially fast the initial distribution. This property has al-
ready been used for EM estimators in [CM05] to show that ﬁxed-lag smoothing drastically reduces
the variance without signiﬁcantly raising the bias. Similar smoothing (either ﬁxed-lag or discounted)
would provide efﬁcient variance reduction techniques for the IPA estimator as well.

6 Conclusions
We proposed a sensitivity analysis in HMMs based on an Inﬁnitesimal Perturbation Analysis and
provided a computationally efﬁcient gradient estimator that provides an interesting alternative to
the usual Score method. We showed how this analysis may be used for estimating the gradient of
the log-likelihood in a gradient-based likelihood maximization approach for the purpose of param-
eter identiﬁcation. Finally let us mention that estimators of higher-order derivatives (e.g. Hessian)
could be derived as well along this IPA approach  which would enable to use more sophisticated
optimization techniques (e.g. Newton method).

8

1230.80.850.90.9511.051.11.151.21.25ValuesParameter number0.20.40.60.811.21.4102030405060708090100110σvnN  Score methodIPA methodReferences
[CDM08] P.A. Coquelin  R. Deguest  and R. Munos. Particle ﬁlter-based policy gradient in

POMDPs. In Neural Information Processing Systems  2008.
F. Cérou  F. Le Gland  and N. J. Newton. Stochastic particle methods for linear tangent
ﬁltering equations. In J.-L. Menaldi  E. Rofman  and A. Sulem  editors  Optimal Con-
trol and PDE’s - Innovations and Applications  in honor of Alain Bensoussan’s 60th
anniversary  pages 231–240. IOS Press  2001.
O. Cappé and E. Moulines. On the use of particle ﬁltering for maximum likelihood
parameter estimation. European Signal Processing Conference  2005.
P. Del Moral. Feynman-Kac Formulae  Genealogical and Interacting Particle Systems
with Applications. Springer  2004.

[CGN01]

[CM05]

[Del04]

[DM01]

[DM08]

[DFG01] A. Doucet  N. De Freitas  and N. Gordon. Sequential Monte Carlo Methods in Practice.

Springer  2001.
R. Douc and C. Matias. Asymptotics of the maximum likelihood estimator for general
hidden markov models. Bernouilli  7:381–420  2001.
R. Douc and E. Moulines. Limit theorems for weighted samples with applications to
sequential monte carlo methods. Annals of Statistics  36:5:2344–2376  2008.

[DT03]

[FLM03]

[Gla91]
[GSS93]

[DMDP07] P. Del Moral  A. Doucet  and GW Peters. Sharp Propagation of Chaos Estimates for
Feynman–Kac Particle Models. SIAM Theory of Probability and its Applications  51
(3):459–485  2007.
A. Doucet and V.B. Tadic. Parameter estimation in general state-space models using
particle methods. Ann. Inst. Stat. Math  2003.
J. Fichoud  F. LeGland  and L. Mevel. Particle-based methods for parameter estimation
and tracking : numerical experiments. Technical Report 1604  IRISA  2003.
P. Glasserman. Gradient estimation via perturbation analysis. Kluwer  1991.
N. Gordon  D. Salmond  and A. F. M. Smith. Novel approach to nonlinear and non-
gaussian bayesian state estimation. In Proceedings IEE-F  volume 140  pages 107–113 
1993.
G. Kitagawa. Monte-Carlo ﬁlter and smoother for non-Gaussian nonlinear state space
models. J. Comput. Graph. Stat.  5:1–25  1996.
H. J. Kushner and G. Yin. Stochastic Approximation Algorithms and Applications.
Springer-Verlag  Berlin and New York  1997.
F. LeGland and L. Mevel. Exponential forgetting and geometric ergodicity in hidden
markov models. mathematic and control sugnal and systems  13:63–93  2000.
R. Mamon and R.J. Elliott. Hidden markov models in ﬁnance. International Series in
Operations Research and Management Science  104  2007.
A. Papavasiliou. A uniformly convergent adaptive particle ﬁlter. Journal of Applied
Probability  42 (4):1053–1068  2007.
G. Poyadjis  A. Doucet  and S.S. Singh. Particle methods for optimal ﬁlter derivative:
Application to parameter estimation. In IEEE ICASSP  2005.
G. Pﬂug. Optimization of Stochastic Models: The Interface Between Simulation and
Optimization. Kluwer Academic Publishers  1996.
G. Poyiadjis. Particle Method for Parameter Estimation in General State Space Models.
PhD thesis  University of Cambridge  2006.
G. Storvik. Particle ﬁlters for state-space models with the presence of unknown static
parameters. IEEE Transactions on Signal Processing  50:281–289  2002.

[PDS05]

[Poy06]

[Sto02]

[Kit96]

[KY97]

[LM00]

[ME07]

[Pap07]

[Pﬂ96]

9

,Wei-Sheng Lai
Jia-Bin Huang
Ming-Hsuan Yang