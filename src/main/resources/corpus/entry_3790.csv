2010,Non-Stochastic Bandit Slate Problems,We consider bandit problems  motivated by applications in online advertising and news story selection  in which the learner must repeatedly select a slate  that is  a subset of size s from K possible actions  and then receives rewards for just the selected actions. The goal is to minimize the regret with respect to total reward of the best slate computed in hindsight. We consider unordered and ordered versions of the problem  and give efficient algorithms which have regret O(sqrt(T))  where the constant depends on the specific nature of the problem. We also consider versions of the problem where we have access to a number of policies which make recommendations for slates in every round  and give algorithms with O(sqrt(T)) regret for competing with the best such policy as well. We make use of the technique of relative entropy projections combined with the usual multiplicative weight update algorithm to obtain our algorithms.,Non-Stochastic Bandit Slate Problems

Satyen Kale

Yahoo! Research
Santa Clara  CA

Georgia Inst. of Technology

Lev Reyzin∗

Atlanta  GA

Robert E. Schapire†
Princeton University

Princeton  NJ

skale@yahoo-inc.com

lreyzin@cc.gatech.edu

schapire@cs.princeton.edu

Abstract

We consider bandit problems  motivated by applications in online advertising and
news story selection  in which the learner must repeatedly select a slate  that is 
a subset of size s from K possible actions  and then receives rewards for just the
selected actions. The goal is to minimize the regret with respect to total reward of
the best slate computed in hindsight. We consider unordered and ordered versions
of the problem  and give efﬁcient algorithms which have regret O(
T )  where the
constant depends on the speciﬁc nature of the problem. We also consider versions
√
of the problem where we have access to a number of policies which make recom-
mendations for slates in every round  and give algorithms with O(
T ) regret for
competing with the best such policy as well. We make use of the technique of
relative entropy projections combined with the usual multiplicative weight update
algorithm to obtain our algorithms.

√

1

Introduction

In traditional bandit models  the learner is presented with a set of K actions. On each of T rounds 
an adversary (or the world) ﬁrst chooses rewards for each action  and afterwards the learner decides
which action it wants to take. The learner then receives the reward of its chosen action  but does
In the standard bandit setting  the learner’s goal is to
not see the rewards of the other actions.
compete with the best ﬁxed arm in hindsight.
In the more general “experts setting ” each of N
experts recommends an arm on each round  and the goal of the learner is to perform as well as the
best expert in hindsight.
The bandit setting tackles many problems where a learner’s decisions reﬂect not only how well
it performs but also the data it learns from — a good algorithm will balance exploiting actions
it already knows to be good and exploring actions for which its estimates are less certain. One
such real-world problem appears in computational advertising  where publishers try to present their
customers with relevant advertisements. In this setting  the actions correspond to advertisements 
and choosing an action means displaying the corresponding ad. The rewards correspond to the
payments from the advertiser to the publisher  and these rewards depend on the probability of users
clicking on the ads.
Unfortunately  many real-world problems  including the computational advertising problem  do not
ﬁt so nicely into the traditional bandit framework. Most of the time  advertisers have the ability to
display more than one ad to users  and users can click on more than one of the ads displayed to
them. To capture this reality  in this paper we deﬁne the slate problem. This setting is similar to the
traditional bandit setting  except that here the advertiser selects a slate  or subset  of S actions.
In this paper we ﬁrst consider the unordered slate problem  where the reward to the learning algo-
rithm is the sum of the rewards of the chosen actions in the slate. This setting is applicable when all
∗This work was done while Lev Reyzin was at Yahoo! Research  New York. This material is based upon
work supported by the National Science Foundation under Grant #0937060 to the Computing Research Asso-
ciation for the Computing Innovation Fellowship program.

†This work was done while R. Schapire was visiting Yahoo! Research  New York.

1

actions in a slate are treated equally. While this is a realistic assumption in certain settings  we also
deal with the case when different positions in a slate have different importance. Going back to our
computational advertising example  we can see not all ads are given the same treatment (i.e. an ad
displayed higher in a list is more likely to be clicked on). One may plausibly assume that for every
ad and every position that it can be shown in  there is a click-through-rate associated with the (ad 
position) pair  which speciﬁes the probability that a user will click on the ad if it is displayed in that
position. This is a very general user model used widely in practice in web search engines. To ab-
stract this  we turn to the ordered slate problem  where for each action and position in the ordering 
the adversary speciﬁes a reward for using the action in that position. The reward to the learner then
is the sum of the rewards of the (actions  position) pairs in the chosen ordered slate.1 This setting
is similar to that of Gy¨orgy  Linder  Lugosi and Ottucs´ak [10] in that the cost of all actions in the
chosen slate are revealed  rather than just the total cost of the slate.
Finally  we show how to tackle these problems in the experts setting  where instead of competing
with the best slate in hindsight  the algorithm competes with the best expert  recommending different
slates on different rounds.
One key idea appearing in our algorithms is to use a variant of the multiplicative weights expert
algorithm for a restricted convex set of distributions. In our case  the restricted set of distributions
over actions corresponds to the one deﬁned by the stipulation that the learner choose a slate instead
of individual actions. Our variant ﬁrst ﬁnds the distribution generated by multiplicative weights and
then chooses the closest distribution in the restricted subset using relative entropy as the distance
metric — this is a type of Bregman projection  which has certain nice properties for our analysis.
Previous Work. The multi-armed bandit problem  ﬁrst studied by Lai and Robbins [15]  is a
classic problem which has had wide application. In the stochastic setting  where the rewards of the
arms are i.i.d.  Lai and Robbins [15] and Auer  Cesa-Bianchi and Fischer [2] gave regret bounds of

O(K ln(T )). In the non-stochastic setting  Auer et al. [3] gave regret bounds of O((cid:112)K ln(K)T ).2

This non-stochastic setting of the multi-armed bandit problem is exactly the speciﬁc case of our
problem when the slate size is 1  and hence our results generalize those of Auer et al. which can be
recovered by setting s = 1.
Our problem is a special case of the more general online linear optimization with bandit feedback
problem [1  4  5  11]. Specializing the best result in this series to our setting  we get worse regret

bounds of O((cid:112)T log(T )). The constant in the O(·) notation is also worse than our bounds. For a

more speciﬁc comparison of regret bounds  see Section 2. Our algorithms  being specialized for the
slates problem  are simpler to implement as well  avoiding the sophisticated self-concordant barrier
techniques of [1].
This work also builds upon the algorithm in [18] to learn subsets of experts and the algorithm in [12]
for learning permutations  both in the full information setting. Our work is also a special case of
the Combinatorial Bandits setting of Cesa-Bianchi and Lugosi [9]; however  our algorithms obtain
better regret bounds and are computationally more efﬁcient.
Our multiplicative weights algorithm also appears under the name Component Hedge in the inde-
pendent work of Koolen  Warmuth and Kivinen [14]. Furthermore  the expertless  unordered slate
problem is studied by Uchiya  Nakamura and Kudo [17] who obtain the same asymptotic bounds as
appear in this paper  though using different techniques.

2 Statement of the problem and main results

For vectors x  y ∈ RK  x · y denotes their inner product  viz. (cid:80)

Notation.
i xiyi. For ma-
trices X  Y ∈ Rs×K  X • Y denotes their inner product considering them vectors in RsK  viz.

1The unordered slate problem is a special case of the ordered slate problem for which all positional fac-
tors are equal. However  the bound on the regret that we get when we consider the unordered slate problem
separately is a factor of ˜O(

s) better than when we treat it as a special case of the ordered slate problem.

2The difference in the regret bounds can be attributed to the deﬁnition of regret in the stochastic and non-
stochastic settings. In the stochastic setting  we compare the algorithm’s expected reward to that of the arm
with the largest expected reward  with the expectation taken over the reward distribution.

√

2

(cid:80)
p and q  let RE(p (cid:107) q) denote their relative entropy  i.e. RE(p (cid:107) q) =(cid:80)

ij XijYij. For a set S of actions  let 1S be the indicator vector for that set. For two distributions

).

i pi ln( pi
qi

Problem Statement. In a sequence of rounds  for t = 1  2  . . .   T   we are required to choose a slate
from a base set A of K actions. An unordered slate is a subset S ⊆ A of s out of the K actions. An
ordered slate is a slate together with an ordering over its s actions; thus  it is a one-to-one mapping
π : {1  2  . . .   s} → A. Prior to the selection of the slate  the adversary chooses losses3 for the
actions in the slates. Once the slate is chosen  the cost of only the actions in the chosen slate is
revealed. This cost is deﬁned in the following manner:

• Unordered slate. The adversary chooses a loss vector (cid:96)(t) ∈ RK which speciﬁes a loss
(cid:96)j(t) ∈ [−1  1] for every action j ∈ A. For a chosen slate S  only the coordinates (cid:96)j(t) for

• Ordered slate. The adversary chooses a loss matrix L(t) ∈ Rs×K which speciﬁes a loss
Lij(t) ∈ [−1  1] for every action j ∈ A and every position i  1 ≤ i ≤ s  in the ordering on
the slate. For a chosen slate π  the entries Li π(i)(t) for every position i are revealed  and

j ∈ S are revealed  and the cost incurred for choosing S is(cid:80)
the cost incurred for choosing π is(cid:80)s
(cid:88)

i=1 Li π(i)(t).

T(cid:88)

T(cid:88)

(cid:88)

(cid:96)j(t) − min

RegretT =

t=1

j∈S(t)

S

t=1

j∈S

(cid:96)j(t).

j∈S (cid:96)j(t).

In the unordered slate problem  if slate S(t) is chosen in round t  for t = 1  2  . . .   T   then the regret
of the algorithm is deﬁned to be

Here  the subscript S is used as a shorthand for ranging over all slates S. The regret for the ordered
slate problem is deﬁned analogously.
Our goal is to design a randomized algorithm for online slate selection such that E[RegretT ] = o(T ) 
where the expectation is taken over the internal randomization of the algorithm.
Competing with policies. Frequently in applications we have access to N policies which are algo-
rithms that recommend slates to use in every round. These policies might leverage extra information
that we have about the losses in the next round. It is therefore beneﬁcial to devise algorithms that
have low regret with respect to the best policy in the pool in hindsight  where regret is deﬁned as:

T(cid:88)

(cid:88)

t=1

j∈S(t)

RegretT =

(cid:96)j(t) − min

ρ

T(cid:88)

(cid:88)

t=1

j∈Sρ(t)

(cid:96)j(t).

Here  ρ ranges over all policies  Sρ(t) is the recommendation of policy ρ at time t  and S(t) is the
algorithm’s chosen slate. The regret is deﬁned analogously for ordered slates. More generally  we
may allow policies to recommend distributions over slates  and our goal is to minimize the expected
regret with respect to the best policy in hindsight  where the expectation is taken over the distribution
recommended by the policy as well as the internal randomization of the algorithm.
Our results. We are now able to formally state our main results:
Theorem 2.1. There are efﬁcient (running in poly(s  K) time in the no-policies case  and in
poly(s  K  N) time with N policies) randomized algorithms achieving the following regret bounds:

Unordered slates

4(cid:112)sK ln(K/s)T
4(cid:112)sK ln(N)T

(Sec. 3.2)
(Sec. 4.1)

Ordered slates

4s(cid:112)K ln(K)T
4s(cid:112)K ln(N)T

(Sec. 3.3)
(Sec. 4.2)

and [9] are O((cid:112)s3K ln(K/s)T ) in the unordered slates problem  and O(s2(cid:112)K ln(K)T ) in the

To compare  the best bounds obtained for the no-policies case using the more general algorithms [1]

√
ordered slates problem. It is also possible  in the no-policies setting  to devise algorithms that have
regret bounded by O(
T ) with high probability  using the upper conﬁdence bounds technique of [3].
We omit these algorithms in this paper for the sake of brevity.

No policies
N policies

3Note that we switch to losses rather than rewards to be consistent with most recent literature on online

learning. Since we allow negative losses  we can easily deal with rewards as well.

3

Algorithm MW(P)
Initialization: An arbitrary probability distribution p(1) ∈ P on the experts  and some η > 0.
For t = 1  2  . . .   T :

1. Choose distribution p(t) over experts  and observe the cost vector (cid:96)(t).
2. Compute the probability vector ˆp(t + 1) using the following multiplicative update rule:

for every expert i 

where Z(t) =(cid:80)

ˆpi(t + 1) = pi(t) exp(−η(cid:96)i(t))/Z(t)
i pi(t) exp(−η(cid:96)i(t)) is the normalization factor.

3. Set p(t + 1) to be the projection of ˆp(t + 1) on the set P using the RE as a distance

function  i.e. p(t + 1) = arg minp∈P RE(p (cid:107) ˆp(t + 1)).
Figure 1: The Multiplicative Weights Algorithm with Restricted Distributions

(1)

3 Algorithms for the slate problems with no policies

3.1 Main algorithmic ideas

Our starting point is the Hedge algorithm for learning online with expert advice. In this setting  on
each round t  the learner chooses a probability distribution p(t) over experts  each of which then
suffers a (fully observable) loss represented by the vector (cid:96)(t). The learner’s loss is then p(t) · (cid:96)(t).
The main idea of our approach is to apply Hedge (and ideas from bandit variants of it  especially
Exp3 [3]) by associating the probability distributions that it selects with mixtures of (ordered or
unordered) slates  and thus with the randomized choice of a slate. However  this requires that the
selected probability distributions have a particular form  which we describe shortly. We therefore
need a special variant of Hedge which uses only distributions p(t) from some ﬁxed convex subset
P of the simplex of all distributions. The goal then is to minimize regret relative to an arbitrary
distribution p ∈ P. Such a version of Hedge is given in Figure 1  and a statement of its performance
below. This algorithm is implicit in the work of [13  18].
Theorem 3.1. Assume that η > 0 is chosen so that for all t and i  η(cid:96)i(t) ≥ −1. Then algorithm
MW(P) generates distributions p(1)  . . .   p(T ) ∈ P  such that for any p ∈ P 

T(cid:88)

T(cid:88)

(cid:96)(t) · p(t) − (cid:96)(t) · p ≤ η

t=1

t=1

((cid:96)(t))2 · p(t) +

RE(p (cid:107) p(1))

η

.

Here  ((cid:96)(t))2 is the vector that is the coordinate-wise square of (cid:96)(t).

3.2 Unordered slates with no policies

convex polytope deﬁned by the linear constraints(cid:80)K

To apply the approach described above  we need a way to compactly represent the set of distributions
over slates. We do this by embedding slates as points in some high-dimensional Euclidean space 
and then giving a compact representation of the convex hull of the embedded points. Speciﬁcally  we
represent an unordered slate S by its indicator vector 1S ∈ RK  which is 1 for all coordinates j ∈ S 
and 0 for all others. The convex hull X of all such 1S vectors can be succinctly described [18] as the
j=1 xj = s and xj ≥ 0 for j = 1  . . .   K. An
algorithm is given in [18] (Algorithm 2) to decompose any vector x ∈ X into a convex combination
of at most K indicator vectors 1S. We embed the convex hull X of all the 1S vectors in the simplex
of distributions over the K actions simply by scaling down all coordinates by s so that they sum to
1. Let P be this scaled down version of X . Our algorithm is given in Figure 2.
Step 3 of MW(P) requires us to compute the arg minp∈P RE(p (cid:107) ˆp(t + 1))  which can be solved by
convex programming. A linear time algorithm is given in [13]  and a simple algorithm (from [18])
is the following: ﬁnd the least index k such that clipping the largest k coordinates of p to 1
s and
rescaling the rest of the coordinates to sum up to 1 − k
s ensures that all coordinates are at most 1
s  
and output the probability vector thus obtained. This can be implemented by sorting the coordinates 
and so it takes O(K log(K)) time.

4

(cid:113) (1−γ)s ln(K/s)

Bandit Algorithm for Unordered Slates
Initialization: Start an instance of MW(P) with the uniform initial distribution p(1) = 1
η =
For t = 1  2  . . .   T :

(cid:113) (K/s) ln(K/s)

  and γ =

KT

T

.

K 1. Set

K 1A.

corresponding to slates S as sp(cid:48)(t) =(cid:80)

1. Obtain the distribution p(t) from MW(P).
2. Set p(cid:48)(t) = (1 − γ)p(t) + γ
3. Note that p(cid:48)(t) ∈ P. Decompose sp(cid:48)(t) as a convex combination of slate vectors 1S
4. Choose a slate S to display with probability qS  and obtain the loss (cid:96)j(t) for all j ∈ S.
5. Set ˆ(cid:96)j(t) = (cid:96)j(t)/(sp(cid:48)
6. Send ˆ(cid:96)(t) as the loss vector to MW(P).

S qS1S  where qS > 0 and(cid:80)

j(t)) if j ∈ S  and 0 otherwise.

S qS = 1.

Figure 2: The Bandit Algorithm with Unordered Slates

We now prove the regret bound of Theorem 2.1. We use the notation Et[X] to denote the expectation
of a random variable X conditioned on all the randomness chosen by the algorithm up to round
t  assuming that X is measurable with respect to this randomness. We note the following facts:
s . This immediately implies that

Et[ˆ(cid:96)j(t)] =(cid:80)

S(cid:51)j qS · (cid:96)j (t)
j (t) = (cid:96)j(t)  since p(cid:48)
sp(cid:48)

Et[ˆ(cid:96)(t) · p(t)] = (cid:96)(t) · p(t) and E[ˆ(cid:96)(t) · p] = (cid:96)(t) · p  for any ﬁxed distribution p.
Note that if we decompose a distribution p ∈ P as a convex combination of 1
s 1S vectors and ran-
domly choose a slate S according to its weight in the combination  then the expected loss  averaged
over the s actions chosen  is (cid:96)(t) · p. We can bound the difference between the expected loss (aver-
aged over the s actions) in round t suffered by the algorithm  (cid:96)(t) · p(cid:48)(t)  and (cid:96)(t) · p(t) as follows:

j(t) =(cid:80)

S(cid:51)j qS · 1

KT

and γ =

by setting η =
It remains to verify that ηˆ(cid:96)j(t) ≥ −1 for all i and t. We know that ˆ(cid:96)j(t) ≥ − K
so all we need to check is that

(cid:113) (1−γ)s ln(K/s)

sγ   because p(cid:48)
K   which is true for our choice of γ.

≤ sγ

KT

T

.

j(t) ≥ γ
K  

5

We note that the leading factor of 1
We now bound the terms on the RHS. First  we have

s on the expected regret is due to the averaging over the s positions.

(cid:96)(t) · p(cid:48)(t) − (cid:96)(t) · p(t) = (cid:88)
=(cid:88)

(cid:96)(t) · p(cid:48)(t) − (cid:96)(t) · 1
s

j

t

Using this bound and Theorem 3.1  if S(cid:63) = arg minS
E[RegretT ]

1S(cid:63) ≤ η

(cid:96)j(t)(p(cid:48)

s

E
t

[(ˆ(cid:96)(t))2 · p(t)] = (cid:88)
= (cid:88)

S

qS ·

(cid:34)

(cid:88)

j∈S

((cid:96)j(t))2pj(t)
(sp(cid:48)

j(t))2

(cid:35)

·(cid:88)

S(cid:51)j

because pj (t)

j (t) ≤ 1
p(cid:48)

((cid:96)j(t))2pj(t)
(sp(cid:48)

j

j(t))2
1−γ   and all |(cid:96)j(t)| ≤ 1.
(cid:113) (1−γ)s ln(K/s)

E[RegretT ] ≤ η

KT
1 − γ

+ s ln(K/s)

(cid:113) (K/s) ln(K/s)

η

s 1S  we have
RE( 1

j

E[(ˆ(cid:96)(t))2 · p(t)] +

j(t) − pj(t)) ≤ (cid:88)
(cid:80)
t (cid:96)(t) · 1
(cid:88)

qS =(cid:88)

(cid:34)

t

((cid:96)j(t))2pj(t)
(sp(cid:48)

j(t))2

j

+ sγT ≤ 4(cid:112)sK ln(K/s)T  

(cid:96)j(t) · γ
K

≤ γ.

s 1S(cid:63) (cid:107) p(1))

η

+ γT.

(cid:35)

· sp(cid:48)

j(t) ≤

K

s(1 − γ)  

KT

T

and γ =

(cid:113) K ln(K)

Bandit Algorithm for Ordered Slates
Initialization: Start an instance of MW(P) with the uniform initial distribution p(1) = 1
η =

(cid:113) (1−γ) ln(K)
of Mπ matrices corresponding to ordered slates π as sp(cid:48)(t) =(cid:80)
and(cid:80)

1. Obtain the distribution p(t) from MW(P).
2. Set p(cid:48)(t) = (1 − γ)p(t) + γ
3. Note that p(cid:48)(t) ∈ P  and so sp(cid:48)(t) ∈ M. Decompose sp(cid:48)(t) as a convex combination
π qπMπ  where qπ > 0

. For t = 1  2  . . .   T :

sK 1. Set

sK 1A.

4. Choose a slate π to display w.p. qπ  and obtain the loss Li π(i)(t) for all 1 ≤ i ≤ s.
5. Construct the loss matrix ˆL(t) as follows: for 1 ≤ i ≤ s  set ˆLi π(i)(t) = Li π(i)(t)

π qπ = 1.

i π(i)(t)  and
sp(cid:48)

all other entries are 0.

6. Send ˆL(t) as the loss vector to MW(P).

Figure 3: Bandit Algorithm for Ordered Slates

3.3 Ordered slates with no policies

A similar approach can be used for ordered slates. Here  we represent an ordered slate π by the
subpermutation matrix Mπ ∈ Rs×K which is deﬁned as follows: for i = 1  2  . . .   s  we have
matrices is the convex polytope deﬁned by the linear constraints:(cid:80)K
i π(i) = 1  and all other entries are 0. In [7  16]  it is shown that the convex hull M of all the Mπ
(cid:80)s
M π
j=1 Mij = 1 for i = 1  . . .   s;
i=1 Mij ≤ 1 for j = 1  . . .   K; and Mij ≥ 0 for i = 1  . . .   s and j = 1  . . .   K. Clearly  all
subpermutation matrices Mπ ∈ M. To complete the characterization of the convex hull  we can
show (details omitted) that given any matrix M ∈ M  we can efﬁciently decompose it into a convex
combination of at most K 2 subpermutation matrices.
We identify matrices in Rs×K with vectors in RsK in the obvious way. We embed M in the simplex
of distributions in RsK simply by scaling all the entries down by s so that their sum equals one. Let
P be this scaled down version of M. Our algorithm is given in Figure 3.
The projection in step 3 of MW(P) can be computed simply by solving the convex program.
In
practice  however  noticing that the relative entropy projection is a Bregman projection  the cyclic
projections method of Bregman [6  8] is likely to work faster. Adapted to the speciﬁc problem at
hand  this method works as follows (see [8] for details): ﬁrst  for every column j  initialize a dual
variable λj = 1. Then  alternate between row phases and column phases. In a row phase  iterate over
s . The column phase is a little more complicated:
all rows  and rescale them to make them sum to 1
s . Set α(cid:48) = min{λj  α} 
ﬁrst  for every column j  compute the scaling factor α to make it sum to 1
and scale the column by α(cid:48)  and update λj ← λj/α(cid:48). Repeat these alternating row and column
phases until convergence to within the desired tolerance.

The regret bound analysis is similar to that of Section 3.2. We have Et[ˆLij(t)] = (cid:80)

π:π(i)=j qπ ·
= Lij(t)  and hence Et[ˆL(t) • p(t)] = L(t) • p(t) and E[ˆL(t) • p] = L(t) • p. We can show

ij

Lij (t)
sp(cid:48)
also that L(t) • p(cid:48)(t) − L(t) • p(t) ≤ γ.
Using this bound and Theorem 3.1  if π(cid:63) = arg minπ

(cid:80)
t L(t) • 1
(cid:88)

s Mπ  we have

=(cid:88)

t

E[RegretT ]

s

L(t)•p(cid:48)(t)−L(t)• 1
s

Mπ(cid:63) ≤ η

t

E[(ˆL(t))2•p(t)]+

RE( 1

s Mπ(cid:63)(cid:107)p(1))

η

+γT.

We now bound the terms on the RHS. First  we have

[(ˆL(t))2 • p(t)] =(cid:88)

qπ ·

E
t

(cid:34) s(cid:88)

π

i=1

(Li π(i)(t))2pi π(i)(t)

(sp(cid:48)

i π(i)(t))2

6

(cid:35)

(cid:34)

s(cid:88)

K(cid:88)

i=1

j=1

=

(Lij(t))2pij(t)

(sp(cid:48)

ij(t))2

(cid:35)

· (cid:88)

qπ
π:π(i)=j

Bandit Algorithm for Unordered Slates With Policies
Initialization: Start an instance of MW with no restrictions over the set of distributions over the N
policies  with the initial distribution r(1) = 1
.
For t = 1  2  . . .   T :

(cid:113) (1−γ)s ln(N )

(cid:113) (K/s) ln(N )

N 1. Set η =

  and γ =

KT

T

ρ=1 rρ(t)φρ(t).

1. Obtain the distribution over policies r(t) from MW  and the recommended distribution

over slates φρ(t) ∈ P for each policy ρ.

2. Compute the distribution p(t) =(cid:80)N
corresponding to slates S as sp(cid:48)(t) =(cid:80)

K 1.

S qS1S  where qS > 0 and(cid:80)

3. Set p(cid:48)(t) = (1 − γ)p(t) + γ
4. Note that p(cid:48)(t) ∈ P. Decompose sp(cid:48)(t) as a convex combination of slate vectors 1S
5. Choose a slate S to display with probability qS  and obtain the loss (cid:96)j(t) for all j ∈ S.
6. Set ˆ(cid:96)j(t) = (cid:96)j(t)/sp(cid:48)
7. Set the loss of policy ρ to be λρ(t) = ˆ(cid:96)(t) · φρ(t) in the MW algorithm.
Figure 4: Bandit Algorithm for Unordered Slates With Policies

j(t) if j ∈ S  and 0 otherwise.

S qS = 1.

(cid:34)

s(cid:88)

K(cid:88)
1−γ   all |Lij(t)| ≤ 1.

j=1

=

i=1

because pij (t)

ij (t) ≤ 1
p(cid:48)

(cid:35)

(Lij(t))2pij(t)

(sp(cid:48)

ij(t))2

· sp(cid:48)

ij(t) ≤ K
1 − γ

 

Finally  we have RE( 1
rem 3.1  we get the stated regret bound from Theorem 2.1:

s Mπ(cid:63) (cid:107) p(1)) = ln(K). Plugging these bounds into the bound of Theo-

+ sγT ≤ 4s(cid:112)K ln(K)T  

E[RegretT ] ≤ η

(cid:113) (1−γ) ln(K)

+ s ln(K)

sKT
1 − γ

(cid:113) K ln(K)

η

and γ =

KT

T

by setting η =

  which satisfy the necessary technical conditions.

4 Competing with a set of policies
4.1 Unordered Slates with N Policies
In each round  every policy ρ recommends a distribution over slates φρ(t) ∈ P  where P is the X
scaled down by s as in Section 3.2. Our algorithm is given in Figure 4.
(cid:80)
Again the regret bound analysis is along the lines of Section 3.2. We have for any j  Et[ˆ(cid:96)j(t)] =
ρ((cid:96)(t) ·
S(cid:51)j qS · (cid:96)j (t)
sp(cid:48)
φρ(t))rρ(t) = (cid:96)(t) · p(t). We can also show as before that (cid:96)(t) · p(cid:48)(t) − (cid:96)(t) · p(t) ≤ γ.
Using this bound and Theorem 3.1  if ρ(cid:63) = arg minρ
E[RegretT ]
(cid:96)(t) · p(cid:48)(t) − (cid:96)(t) · φρ(cid:63)(t) ≤ η

j (t) = (cid:96)j(t). Thus  Et[λρ(t)] = (cid:96)(t) · φρ(t)  and hence Et[λ(t) · r(t)] =(cid:80)
=(cid:88)

t (cid:96)(t) · φρ(t)  we have
E[(λ(t))2 · r(t)] +

RE(eρ(cid:63)(cid:107)r(1))

(cid:80)
(cid:88)

+ γT 

s

t

t

η

where eρ(cid:63) is the distribution (vector) that is concentrated entirely on policy ρ(cid:63).
We now bound the terms on the RHS. First  we have

[(λ(t))2 · r(t)] = E
E
t

t

≤ E

t

(cid:34)(cid:88)
(cid:34)(cid:88)

ρ

(cid:35)

(cid:34)(cid:88)
(cid:35)
((ˆ(cid:96)(t))2 · φρ(t))rρ(t)

λρ(t)2rρ(t)

= E

ρ

t

ρ

7

(cid:35)
( ˆ(cid:96)(t) · φρ(t))2rρ(t)

= E

[(ˆ(cid:96)(t))2 · p(t)] ≤

t

K

s(1 − γ) .

Bandit Algorithm for Ordered Slates with Policies
Initialization: Start an instance of MW with no restrictions  over the set of distributions over the N
policies  starting with r(1) = 1
For t = 1  2  . . .   T :

(cid:113) (1−γ) ln(N )

(cid:113) K ln(N )

N 1. Set η =

and γ =

KT

T

.

over ordered slates φρ(t) ∈ P for each policy ρ.

1. Obtain the distribution over policies r(t) from MW  and the recommended distribution

2. Compute the distribution p(t) =(cid:80)N
Mπ matrices corresponding to ordered slates π as sp(cid:48)(t) = (cid:80)
and(cid:80)

3. Set p(cid:48)(t) = (1 − γ)p(t) + γ
4. Note that p(cid:48)(t) ∈ P  and so sp(cid:48)(t) ∈ X . Decompose sp(cid:48)(t) as a convex combination of
π qπMπ  where qπ > 0

ρ=1 rρ(t)φρ(t).

5. Choose a slate π to display w.p. qπ  and obtain the loss Li π(i)(t) for all 1 ≤ i ≤ s.
6. Construct the loss matrix ˆL(t) as follows: for 1 ≤ i ≤ s  set ˆLi π(i)(t) = Li π(i)(t)

π qπ = 1.

sK 1A.

i π(i)(t)  and
sp(cid:48)

all other entries are 0.

7. Set the loss of policy ρ to be λρ(t) = ˆL(t) • φρ(t) in the MW algorithm.

Figure 5: Bandit Algorithm for Ordered Slates with Policies

The ﬁrst inequality above follows from Jensen’s inequality  and the second one is proved exactly as
in Section 3.2. Finally  we have RE(eρ(cid:63) (cid:107) p(1)) = ln(N). Plugging these bounds into the bound
above  we get the stated regret bound from Theorem 2.1:

+ sγT ≤ 4(cid:112)sK ln(N)T  

E[RegretT ] ≤ η

(cid:113) (1−γ)s ln(N )

KT
1 − γ
and γ =

+ s ln(N)

(cid:113) (K/s) ln(N )

η

KT

T

by setting η =
tions.

  which satisfy the necessary technical condi-

4.2 Ordered Slates with N Policies
In each round  every policy ρ recommends a distribution over ordered slates φρ(t) ∈ P  where P is
M scaled down by s as in Section 3.3. Our algorithm is given in Figure 5.
The regret bound analysis is exactly along the lines of that in Section 4.1  with L(t) and ˆL(t) playing
the roles of (cid:96)(t) and ˆ(cid:96)(t) respectively  with the inequalities from Section 3.3. We omit the details
for brevity. We get the stated regret bound from Theorem 2.1:

E[RegretT ] ≤ 4s(cid:112)K ln(N)T .

5 Conclusions and Future Work

√
In this paper  we presented efﬁcient algorithms for the unordered and ordered slate problems with
regret bounds of O(
T )  in the presence and and absence of policies  employing the technique of
Bregman projections on a convex set representing the convex hull of slate vectors.
Possible future work on this problem is in two directions. The ﬁrst direction is to handle other user
models for the loss matrices  such as models incorporating the following sort of interaction between
the chosen actions: if two very similar ads are shown  and the user clicks on one  then the user is
less likely to click on the other. Our current model essentially assumes no interaction.
√
T ) regret bounds for the slate problems in
The second direction is to derive high probability O(
the presence of policies. The techniques of [3] only give such algorithms in the no-policies setting.

References
[1] ABERNETHY  J.  HAZAN  E.  AND RAKHLIN  A. Competing in the dark: An efﬁcient algo-

rithm for bandit linear optimization. In COLT (2008)  pp. 263–274.

8

[2] AUER  P.  CESA-BIANCHI  N.  AND FISCHER  P. Finite-time analysis of the multiarmed

bandit problem. Machine Learning 47  2-3 (2002)  235–256.

[3] AUER  P.  CESA-BIANCHI  N.  FREUND  Y.  AND SCHAPIRE  R. E. The nonstochastic

multiarmed bandit problem. SIAM J. Comput. 32  1 (2002)  48–77.

[4] AWERBUCH  B.  AND KLEINBERG  R. Online linear optimization and adaptive routing. J.

Comput. Syst. Sci. 74  1 (2008)  97–114.

[5] BARTLETT  P. L.  DANI  V.  HAYES  T. P.  KAKADE  S.  RAKHLIN  A.  AND TEWARI 
In COLT (2008) 

A. High-probability regret bounds for bandit online linear optimization.
pp. 335–342.

[6] BREGMAN  L. The relaxation method of ﬁnding the common point of convex sets and its
application to the solution of problems in convex programming. USSR Comp. Mathematics
and Mathematical Physics 7 (1967)  200–217.

[7] BRUALDI  R. A.  AND LEE  G. M. On the truncated assignment polytope. Linear Algebra

and its Applications 19 (1978)  33–62.

[8] CENSOR  Y.  AND ZENIOS  S. Parallel optimization. Oxford University Press  1997.
[9] CESA-BIANCHI  N.  AND LUGOSI  G. Combinatorial bandits. In COLT (2009).
[10] GY ¨ORGY  A.  LINDER  T.  LUGOSI  G.  AND OTTUCS ´AK  G. The on-line shortest path
problem under partial monitoring. Journal of Machine Learning Research 8 (2007)  2369–
2403.

[11] HAZAN  E.  AND KALE  S. Better algorithms for benign bandits. In SODA (2009)  pp. 38–47.
[12] HELMBOLD  D. P.  AND WARMUTH  M. K. Learning permutations with exponential weights.

In COLT (2007)  pp. 469–483.

[13] HERBSTER  M.  AND WARMUTH  M. K. Tracking the best linear predictor. Journal of

Machine Learning Research 1 (2001)  281–309.

[14] KOOLEN  W. M.  WARMUTH  M. K.  AND KIVINEN  J. Hedging structured concepts. In

COLT (2010).

[15] LAI  T.  AND ROBBINS  H. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics 6 (1985)  4–22.

[16] MENDELSOHN  N. S.  AND DULMAGE  A. L. The convex hull of sub-permutation matrices.

Proceedings of the American Mathematical Society 9  2 (Apr 1958)  253–254.

[17] UCHIYA  T.  NAKAMURA  A.  AND KUDO  M. Algorithms for adversarial bandit problems

with multiple plays. In ALT (2010)  pp. 375–389.

[18] WARMUTH  M. K.  AND KUZMIN  D. Randomized PCA algorithms with regret bounds that

are logarithmic in the dimension. In In Proc. of NIPS (2006).

9

,Been Kim
Cynthia Rudin
Julie Shah
James Atwood
Don Towsley
Amit Zohar
Lior Wolf