2013,Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent,We present and study a distributed optimization algorithm by employing  a stochastic dual coordinate ascent method. Stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems. It still lacks of efforts in studying them in a distributed framework. We make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network  with an analysis of the tradeoff between  computation and  communication. We verify  our analysis by experiments on real data sets. Moreover  we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing SVMs in the same distributed framework  and observe competitive performances.,Trading Computation for Communication:

Distributed Stochastic Dual Coordinate Ascent

Tianbao Yang

NEC Labs America  Cupertino  CA 95014

tyang@nec-labs.com

Abstract

We present and study a distributed optimization algorithm by employing a stochas-
tic dual coordinate ascent method. Stochastic dual coordinate ascent methods en-
joy strong theoretical guarantees and often have better performances than stochas-
tic gradient descent methods in optimizing regularized loss minimization prob-
lems. It still lacks of efforts in studying them in a distributed framework. We
make a progress along the line by presenting a distributed stochastic dual coor-
dinate ascent algorithm in a star network  with an analysis of the tradeoff be-
tween computation and communication. We verify our analysis by experiments
on real data sets. Moreover  we compare the proposed algorithm with distributed
stochastic gradient descent methods and distributed alternating direction methods
of multipliers for optimizing SVMs in the same distributed framework  and ob-
serve competitive performances.

n(cid:88)

i=1

1
n

Introduction

1
In recent years of machine learning applications  the size of data has been observed with an unprece-
dented growth. In order to efﬁciently solve large scale machine learning problems with millions of
and even billions of data points  it has become popular to take advantage of the computational power
of multi-cores in a single machine or multi-machines on a cluster to optimize the problems in a par-
allel fashion or a distributed fashion [2].
In this paper  we consider the following generic optimization problem arising ubiquitously in super-
vised machine learning applications:

φ(w(cid:62)xi; yi) + λg(w) 

min
w∈Rd

P (w)  where P (w) =

(1)
where w ∈ Rd denotes the linear predictor to be optimized  (xi  yi)  xi ∈ Rd  i = 1  . . .   n denote
the instance-label pairs of a set of data points  φ(z; y) denotes a loss function and g(w) denotes a
regularization on the linear predictor. Throughout the paper  we assume the loss function φ(z; y) is
convex w.r.t the ﬁrst argument and we refer to the problem in (1) as Regularized Loss Minimization
(RLM) problem.
The RLM problem has been studied extensively in machine learning  and many efﬁcient sequential
algorithms have been developed in the past decades [8  16  10].
In this work  we aim to solve
the problem in a distributed framework by leveraging the capabilities of tens of hundreds of CPU
cores. In contrast to previous works of distributed optimization that are based on either (stochastic)
gradient descent (GD and SGD) methods [21  11] or alternating direction methods of multipliers
(ADMM) [2  23]  we motivate our research from the recent advances on (stochastic) dual coordinate
ascent (DCA and SDCA) algorithms [8  16]. It has been observed that DCA and SDCA algorithms
can have comparable and sometimes even better convergence speed than GD and SGD methods.
However  it lacks efforts in studying them in a distributed fashion and comparing to those SGD-
based and ADMM-based distributed algorithms.

1

In this work  we bridge the gap by developing a Distributed Stochastic Dual Coordinate Ascent
(DisDCA) algorithm for solving the RLM problem. We summarize the proposed algorithm and our
contributions as follows:

• The presented DisDCA algorithm possesses two key characteristics: (i) parallel computa-
tion over K machines (or cores); (ii) sequential updating of m dual variables per iteration
on individual machines followed by a “reduce” step for communication among processes.
It enjoys a strong guarantee of convergence rates for smooth or no-smooth loss functions.
• We analyze the tradeoff between computation and communication of DisDCA invoked by
m and K.
Intuitively  increasing the number m of dual variables per iteration aims at
reducing the number of iterations for convergence and therefore mitigating the pressure
caused by communication. Theoretically  our analysis reveals the effective region of m  K
versus the regularization path of λ.
• We present a practical variant of DisDCA and make a comparison with distributed ADMM.
We verify our analysis by experiments and demonstrate the effectiveness of DisDCA by
comparing with SGD-based and ADMM-based distributed optimization algorithms run-
ning in the same distributed framework.

2 Related Work
Recent years have seen the great emergence of distributed algorithms for solving machine learning
related problems [2  9]. In this section  we focus our review on distributed optimization techniques.
Many of them are based on stochastic gradient descent methods or alternating direction methods of
multipliers.
Distributed SGD methods utilize the computing resources of multiple machines to handle a large
number of examples simultaneously  which to some extent alleviates the high computational load
per iteration of GD methods and also improve the performances of sequential SGD methods. The
simplest implementation of a distributed SGD method is to calculate the stochastic gradients on
multiple machines  and to collect these stochastic gradients for updating the solution on a master
machine. This idea has been implemented in a MapReduce framework [13  4] and a MPI frame-
work [21  11]. Many variants of GD methods have be deployed in a similar style [1]. ADMM
has been employed for solving machine learning problems in a distributed fashion [2  23]  due to
its superior convergences and performances [5  23]. The original ADMM [7] is proposed for solv-
ing equality constrained minimization problems. The algorithms that adopt ADMM for solving
the RLM problems in a distributed framework are based on the idea of global variable consensus.
Recently  several works [19  14] have made efforts to extend ADMM to its online or stochastic
versions. However  they suffer relatively low convergence rates.
The advances on DCA and SDCA algorithms [12  8  16] motivate the present work. These studies
have shown that in some regimes (e.g.  when a relatively high accurate solution is needed)  SDCA
can outperform SGD methods. In particular  S. Shalev-Shwartz and T. Zhang [16] have derived
new bounds on the duality gap  which have been shown to be superior to earlier results. However 
there still lacks of efforts in extending these types of methods to a distributed fashion and comparing
them with SGD-based algorithms and ADMM-based distributed algorithms. We bridge this gap by
presenting and studying a distributed stochastic dual ascent algorithm. It has been brought to our
attention that M. Tak´ac et al. [20] have recently published a paper to study the parallel speedup of
mini-batch primal and dual methods for SVM with hinge loss and establish the convergence bounds
of mini-batch Pegasos and SDCA depending on the size of the mini-batch. This work differenti-
ates from their work in that (i) we explicitly take into account the tradeoff between computation
and communication; (ii) we present a more practical variant and make a comparison between the
proposed algorithm and ADMM in view of solving the subproblems  and (iii) we conduct empirical
studies for comparison with these algorithms. Other related but different work include [3]  which
presents Shotgun  a parallel coordinate descent algorithm for solving (cid:96)1 regularized minimization
problems.
There are other unique issues arising in distributed optimization  e.g.  synchronization vs asynchro-
nization  star network vs arbitrary network. All these issues are related to the tradeoff between
communication and computation [22  24]. Research in these aspects are beyond the scope of this
work and can be considered as future work.

2

3 Distributed Stochastic Dual Coordinate Ascent
In this section  we present a distributed stochastic dual coordinate ascent (DisDCA) algorithm and
its convergence bound  and analyze the tradeoff between computation and communication. We also
present a practical variant of DisDCA and make a comparison with ADMM. We ﬁrst present some
notations and preliminaries.
For simplicity of presentation  we let φi(w(cid:62)xi) = φ(w(cid:62)xi; yi). Let φ∗
i (α) and g∗(v) be the convex
conjugate of φi(z) and g(w)  respectively. We assume g∗(v) is continuous differentiable. It is easy
to show that the problem in (1) has a dual problem given below:

n(cid:88)
(cid:80)n
Let w∗ be the optimal solution to the primal problem in (1) and α∗ be the optimal solution to the
i=1 αixi  and w(α) = ∇g∗(v)  it can be veriﬁed
dual problem in (2). If we deﬁne v(α) = 1
λn
that w(α∗) = w∗  P (w(α∗)) = D(α∗). In this paper  we aim to optimize the dual problem (2)
in a distributed environment where the data are distributed evenly across over K machines. Let
(xk i  yk i)  i = 1  . . .   nk denote the training examples on machine k. For ease of analysis  we
assume nk = n/K. We denote by αk i the associated dual variable of xk i  and by φk i(·)  φ∗
k i(·)
the corresponding loss function and its convex conjugate. To simplify the analysis of our algorithm
and without loss of generality  we make the following assumptions about the problem:

i (−αi) − λg∗

D(α)  where D(α) =

n(cid:88)

αixi

.

max
α∈Rn

−φ∗

1
n

i=1

(cid:33)

(cid:32)

1
λn

i=1

(2)

• φi(z) is either a (1/γ)-smooth function or a L-Lipschitz continuous function (c.f.

the
deﬁnitions given below). Exemplar smooth loss functions include e.g.  L2 hinge loss
φi(z) = max(0  1 − yiz)2  logistic loss φi(z) = log(1 + exp(−yiz)). Commonly used
Lipschitz continuous functions are L1 hinge loss φi(z) = max(0  1 − yiz) and absolute
loss φi(z) = |yi − z|.
• g(w) is a 1-strongly convex function w.r.t to (cid:107) · (cid:107)2. Examples include (cid:96)2 norm square
2 + µ(cid:107)w(cid:107)1.
1/2(cid:107)w(cid:107)2
• For all i  (cid:107)xi(cid:107)2 ≤ 1  φi(z) ≥ 0 and φi(0) ≤ 1.

2 and elastic net 1/2(cid:107)w(cid:107)2

Deﬁnition 1. A function φ(z) : R → R is a L-Lipschitz continuous function  if for all a  b ∈ R
|φ(a) − φ(b)| ≤ L|a − b|. A function φ(z) : R → R is (1/γ)-smooth  if it is differentiable and its
gradient ∇φ(z) is (1/γ)-Lipschitz continuous  or for all a  b ∈ R  we have φ(a) ≤ φ(b) + (a −
b)(cid:62)∇φ(b) + 1
2γ (a − b)2. A convex function g(w) : Rd → R is β-strongly convex w.r.t a norm (cid:107) · (cid:107) 
if for any s ∈ [0  1] and w1  w2 ∈ Rd  g(sw1 + (1 − s)w2) ≤ sg(w1) + (1 − s)g(w2) − 1
2 s(1 −
s)β(cid:107)w1 − w2(cid:107)2.

3.1 DisDCA Algorithm: The Basic Variant

The detailed steps of the basic variant of the DisDCA algorithm are described by a pseudo code in
Figure 1. The algorithm deploys K processes running simultaneously on K machines (or cores)1 
each of which only accesses its associated training examples. Each machine calls the same proce-
dure SDCA-mR  where mR manifests two unique characteristics of SDCA-mR compared to SDCA.
(i) At each iteration of the outer loop  m examples instead of one are randomly sampled for updating
their dual variables. This is implemented by an inner loop that costs the most computation at each
outer iteration. (ii) After updating the m randomly selected dual variables  it invokes a function
Reduce to collect the updated information from all K machines that accommodates naturally to the
distributed environment. The Reduce function acts exactly like MPI::AllReduce if one wants to
j=1 ∆αk ij xij to a
implement the algorithm in a MPI framework. It essentially sends ∆vk = 1
λn
process  adds all of them to vt−1  and then broadcasts the updated vt to all K processes. It is this step
that involves the communication among the K machines. Intuitively  smaller m yields less computa-
tion and slower convergence and therefore more communication and vice versa. In next subsection 
we would give a rigorous analysis about the convergence  computation and communication.
Remark: The goal of the updates is to increase the dual objective. The particular options presented
in routine IncDual is to maximize the lower bounds of the dual objective. More options are provided

(cid:80)m

1We use process and machine interchangeably.

3

DisDCA Algorithm (The Basic Variant)

Start K processes by calling the following procedure SDCA-mR with input m and T

Procedure SDCA-mR

k = 0  v0 = 0  w0 = ∇g∗(0)

Input: number of iterations T   number of samples m at each iteration
Let: α0
Read Data: (xk i  yk i)  i = 1 ···   nk
Iterate: for t = 1  . . .   T

Iterate: for j = 1  . . .   m
Randomly pick i ∈ {1 ···   nk} and let ij = i
Find ∆αk i by calling routine IncDual(w = wt−1  scl = mK)
Set αt

(cid:80)m
k i = αt−1
j=1 ∆αk ij xk ij → vt−1
Reduce: vt : 1
λn
Update: wt = ∇g∗(vt)

k i + ∆αk i

Routine IncDual(w  scl)
k iw − scl
2λn

k i + ∆α)) − ∆αx(cid:62)

Option I:

−φ∗
Let ∆αk i = max
∆α
Option II:
k i = −∂φk i(x(cid:62)
Let zt−1
Let ∆αk i = sk izt−1
s(φ∗

k i(−αt−1

k i ) + φk i(x(cid:62)

k i(−(αt−1
k iw) − αt−1

k i

k i where sk i ∈ [0  1] maximize
k iw) +

k iwt−1) + zt−1

k i x(cid:62)

(∆α)2(cid:107)xk i(cid:107)2

2

γs(1 − s)

2

k i )2 − scl
(zt−1
2λn

s2(zt−1

k i )2(cid:107)xk i(cid:107)2

2

Figure 1: The Basic Variant of the DisDCA Algorithm

in supplementary materials. The solutions to option I have closed forms for several loss functions
(e.g.  L1  L2 hinge losses  square loss and absolute loss) [16]. Note that different from the options
presented in [16]  the ones in Incdual use a slightly different scalar factor mK in the quadratic term
to adapt for the number of updated dual variables.

3.2 Convergence Analysis: Tradeoff between Computation and Communication

In this subsection  we present the convergence bound of the DisDCA algorithm and analyze the
tradeoff between computation  convergence or communication. The theorem below states the con-
vergence rate of DisDCA algorithm for smooth loss functions (The omitted proofs and other deriva-
tions can be found in supplementary materials) .
Theorem 1. For a (1/γ)-smooth loss function φi and a 1-strongly convex function g(w)  to obtain
an p duality gap of E[P (wT ) − D(αT )] ≤ P   it sufﬁces to have
1
λγ

(cid:18)(cid:18) n

(cid:18) n

(cid:19) 1

T ≥

(cid:19)

(cid:19)

1
λγ

mK

log

+

mK

+

.

P

Remark: In [20]  the authors established a convergence bound of mini-batch SDCA for L1-SVM
that depends on the spectral norm of the data. Applying their trick to our algorithmic framework is
equivalent to replacing the scalar mK in DisDCA algorithm with βmK that characterizes the spectral
norm of sampled data across all machines XmK = (x11  . . . x1m  . . .   xKm). The resulting conver-
λγ with βmK
λγ .
gence bound for (1/γ)-smooth loss functions is given by substituting the term 1
√
The value of βmK is usually smaller than mK and the authors in [20] have provided an expression
for computing βmK based on the spectral norm σ of the data matrix X/
n.
n = (x1  . . . xn)/
However  in practice the value of σ cannot be computed exactly. A safe upper bound of σ = 1
assuming (cid:107)xi(cid:107)2 ≤ 1 gives the value mK to βmK  which reduces to the scalar as presented in Fig-
ure 1. The authors in [20] also presented an aggressive variant to adjust β adaptively and observed
improvements. In Section 3.3 we develop a practical variant that enjoys more speed-up compared to
the basic variant and their aggressive variant.

√

mK

1

Tradeoff between Computation and Communication We are now ready to discuss the tradeoff
between computation and communication based on the worst case analysis as indicated by Theo-

4

rem 1. For the analysis of tradeoff between computation and communication invoked by the number
of samples m and the number of machines K  we ﬁx the number of examples n and the number of
dimensions d. When we analyze the tradeoff involving m  we ﬁx K and vice versa. In the follow-
ing analysis  we assume the size of model to be communicated is ﬁxed d and is independent of m 
though in some cases (e.g.  high dimensional sparse data) one may communicate a smaller size of
data that depends on m.
It is notable that in the bound of the number of iterations  there is a term 1/(λγ). To take this term
into account  we ﬁrst consider an interesting region of λ for achieving a good generalization error.
Several pieces of works [17  18  6] have suggested that in order to obtain an optimal generalization
error  the optimal λ scales like Θ(n−1/(1+τ ))  where τ ∈ (0  1]. For example  the analysis in [18]
suggests λ = Θ

(cid:16) 1√

for SVM.

(cid:17)

n

(cid:16) n

(cid:17)

(cid:16) n

First  we consider the tradeoff involving the number of samples m by ﬁxing the number pro-
cesses K. We note that the communication cost is proportional to the number of iterations
  while the computation cost per node is proportional to mT =
T = Ω

mK + n1/(1+τ )

γ

K + mn1/(1+τ )

due to that each iteration involves m examples. When m ≤ Θ

 
Ω
the communication cost decreases as m increases  and the computation costs increases as m in-
 
creases  though it is dominated by Ω(n/K). When the value of m is greater than Θ

γ

(cid:16) nτ /(1+τ )
(cid:16) nτ /(1+τ )

K

(cid:17)
(cid:17)

(cid:17)

K

the communication cost is dominated by Ω
  then increasing the value of m will become
less inﬂuential on reducing the communication cost; while the computation cost would blow up
substantially.
Similarly  we can also understand how the number of nodes K affects the tradeoff between the com-
munication cost  proportional to ˜Ω(KT ) = ˜Ω

γ

m + Kn1/(1+τ )

γ

(cid:17) 2  and the computation cost  pro-
(cid:17)
(cid:16) nτ /(1+τ )

  as K increases the computation cost

would decrease and the communication cost would increase. When it is greater than Θ

. When K ≤ Θ

portional to Ω

K + mn1/(1+τ )

γ

(cid:16) n

(cid:17)

 

m

(cid:17)

(cid:16) n1/(1+τ )
(cid:16) n
(cid:16) nτ /(1+τ )
(cid:17)
(cid:16) mn1/(1+τ )

m

(cid:17)

γ

and the effect of increasing K on

the computation cost would be dominated by Ω
reducing the computation cost would diminish.
According to the above analysis  we can conclude that when mK ≤ Θ (nλγ)  to which we refer as
the effective region of m and K  the communication cost can be reduced by increasing the number
of samples m and the computation cost can be reduced by increasing the number of nodes K.
Meanwhile  increasing the number of samples m would increase the computation cost and similarly
increasing the number of nodes K would increase the communication cost. It is notable that the
larger the value of λ the wider the effective region of m and K  and vice versa. To verify the tradeoff
of communication and computation  we present empirical studies in Section 4. Although the smooth
loss functions are the most interesting  we present in the theorem below about the convergence of
DisDCA for Lipschitz continuous loss functions.
Theorem 2. For a L-Lipschitz continuous loss function φi and a 1-strongly convex function g(w) 
to obtain an P duality gap E[P ( ¯wT ) − D(¯αT )] ≤ P   it sufﬁces to have

T ≥ 4L2
λP

where ¯wT =(cid:80)T−1

t=T0

n

≥ 20L2
λP

+ T0 +

wt/(T − T0)  ¯αT =(cid:80)T−1

mK

t=T0

+ max

0 

n

mK

log

αt/(T − T0).

(cid:18)

(cid:18) λn

(cid:19)(cid:19)

2mKL2

+

n

mK

 

Remark: In this case  the effective region of m and K is mK ≤ Θ(nλP ) which is narrower than
that for smooth loss functions  especially when P (cid:28) γ. Similarly  if one can obtain an accurate
estimate of the spectral norm of all data and use βmK in place of mK in Figure 1  the convergence
bound can be improved with 4L2
. Again  the practical variant presented in next
λP
section yields more speed-up.

mK in place of 4L2

βmK

λP

2We simply ignore the communication delay in our analysis.

5

the practical updates at the t-th iteration

t = wt−1

Initialize: u0
Iterate: for j = 1  . . .   m
Randomly pick i ∈ {1 ···   nk} and let ij = i
Find ∆αk i by calling routine IncDual(w = uj−1
Update αt

k i + ∆αk i and update uj

k i = αt−1

t

t = uj−1

  scl = k)
t + 1
λnk

∆αk ixk i

Figure 2: the updates at the t-th iteration of the practical variant of DisDCA

3.3 A Practical Variant of DisDCA and A Comparison with ADMM

2/2 and v = w.

In this section  we ﬁrst present a practical variant of DisDCA motivated by intuition and then we
make a comparison between DisDCA and ADMM  which provides us more insight about the prac-
tical variant of DisDCA and differences between the two algorithms. In what follows  we are par-
ticularly interested in (cid:96)2 norm regularization where g(w) = (cid:107)w(cid:107)2
A Practical Variant We note that in Algorithm 1  when updating the values of the following sam-
pled dual variables  the algorithm does not use the updated information but instead wt−1 from last
iteration. Therefore a potential improvement would be leveraging the up-to-date information for
updating the dual variables. To this end  we maintain a local copy of wk in each machine. At
k  k = 1 ···   K are synchronized with the global wt−1.
the beginning of the iteration t  all w0
Then in individual machines  the j-th sampled dual variable is updated by IncDual(wj−1
  k) and
k = wj−1
the local copy wj
∆αk ij xk ij for updating the next dual
variable. At the end of the iteration  the local solutions are synchronized to the global variable
wt = wt−1 + 1
xk ij . It is important to note that the scalar factor in IncDual
is now k because the dual variables are updated incrementally and there are k processes running
parallell. The detailed steps are presented in Figure 2  where we abuse the same notation uj
t for the
local variable at all processes. The experiments in Section 4 verify the improvements of the practical
variant vs the basic variant. It still remains an open problem to us what is the convergence bound
of this practical variant. However  next we establish a connection between DisDCA and ADMM
that sheds light on the motivation behind the practical variant and the differences between the two
algorithms.
A Comparison with ADMM First we note that the goal of the updates at each iteration in DisDCA
is to increase the dual objective by maximizing the following objective:

k is also updated by wj

(cid:80)m

k + 1
λnk

(cid:80)K

j=1 ∆αt

k=1

k ij

λn

k

m(cid:88)
where ˆwt−1 = wt−1 − 1/(λnk)(cid:80)m

−φ∗

1
nk

max

i=1

α

i (−αi) − λ
2
i=1 αt−1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ˆwt−1 + 1/(λnk)

m(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

i xi and we suppress the subscript k associated with each
machine. The updates presented in Algorithm 1 are solutions to maximizing the lower bounds of
the above objective function by decoupling the m dual variables. It is not difﬁcult to derive that the
dual problem in (3) has the following primal problem (a detailed derivation and others can be found
in supplementary materials):

αixi

 

(3)

DisDCA: min
w

1
nk

φi(x(cid:62)

i w) +

λ
2

wt−1 − 1/(λnk)

αt−1
i xi

.

(4)

We refer to ˆwt as the penalty solution. Second let us recall the updating scheme in ADMM. The
(deterministic) ADMM algorithm at iteration t solves the following problems in each machine:

ADMM: wt

k = arg min
w

1
nk

φi(x(cid:62)

i w) +

ρK
2

(cid:107)w − (wt−1 − ut−1

k

(cid:107)2
2 

(5)

(cid:124)

(cid:125)

)

nk(cid:88)

i=1

where ρ is a penalty parameter and wt−1 is the global primal variable updated by

m(cid:88)

i=1

(cid:32)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)w −

(cid:33)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

m(cid:88)

i=1

ˆwt−1

(cid:123)(cid:122)
K(cid:88)

k=1

wt =

ρK( ¯wt + ¯ut−1)

ρK + λ

  with ¯wt =

1
K

6

K(cid:88)

k=1

k  ¯ut−1 =
wt

1
K

ut−1

k

 

k

k = ut−1

k + wt

is the local “dual” variable updated by ut

k − wt. Comparing the subprob-
and ut−1
lem (4) in DisDCA and the subproblem (5) in ADMM leads to the following observations. (1) Both
aim at solving the same type of problem to increase the dual objective or decrease the primal ob-
jective. DisDCA uses only m randomly selected examples while ADMM uses all examples. (2)
However  the penalty solution ˆwt−1 and the penalty parameter are different. In DisDCA  ˆwt−1 is
constructed by subtracting from the global solution the local solution deﬁned by the dual variables
α  while in ADMM it is constructed by subtracting from the global solution the local Lagrangian
variables u. The penalty parameter in DisDCA is given by the regularization parameter λ while that
in ADMM is a parameter that is needed to be speciﬁed by the user.
Now  let us explain the practical variant of DisDCA from the viewpoint of inexactly solving the
subproblem (4). Note that if the optimal solution to (3) is denoted by α∗
i   i = 1  . . .   m  then
the optimal solution u∗ to (4) is given by u∗ = ˆwt−1 + 1
i xi.
In fact  the updates
at the t-th iteration of the practical variant of DisDCA is to optimize the subproblem (4) by the
(cid:80)m
SDCA algorithm with only one pass of the sampled data points and an initialization of α0
i =
αt−1
It means that the initial primal solution for solving the subproblem (3) is
i=1 αt−1
u0 = ˆwt−1 + 1
In a recent work [23] of applying ADMM to solving the L2-SVM problem in the same distributed
fashion  the authors exploited different strategies for solving the subproblem (5) associated with
L2-SVM  among which the DCA algorithm with only one pass of all data points gives the best
performance in terms of running time (e.g.  it is better than DCA with several passes of all data
points and is also better than a trusted region Newton method). This from another point of view
validates the practical variant of DisDCA.
Finally  it is worth to mention that unlike ADMM whose performance is signiﬁcantly affected by
the value of the penalty parameter ρ  DisDCA is a parameter free algorithm.

i xi = wt−1. That explains the initialization step in Figure 2.

(cid:80)m
i=1 α∗

  i = 1 . . .   m.

i

λnk

λnk

4 Experiments
In this section  we present some experimental results to verify the theoretical results and the empir-
ical performances of the proposed algorithms. We implement the algorithms by C++ and openMPI
and run them in cluster. On each machine  we only launch one process. The experiments are per-
formed on two large data sets with different number of features  covtype and kdd. Covtype data
has a total of 581  012 examples and 54 features. Kdd data is a large data used in kdd cup 2010 
which contains 19  264  097 training examples and 29  890  095 features. For covtype data  we use
522  911 examples for training. We apply the algorithms to solving two SVM formulations  namely
L2-SVM with hinge loss square and L1-SVM with hinge loss  to demonstrate the capabilities of
DisDCA in solving smooth loss functions and Lipschitz continuous loss functions. In the legend of
ﬁgures  we use DisDCA-b to denote the basic variant  DisDCA-p to denote the practical variant  and
DisDCA-a to denote the aggressive variant of DisDCA [20].
Tradeoff between Communication and Computation To verify the convergence analysis  we
show in Figures 3(a)∼3(b)  3(d)∼3(e) the duality gap of the basic variant and the practical variant
of the DisDCA algorithm versus the number of iterations by varying the number of samples m per
iteration  the number of machines K and the values of λ. The results verify the convergence bound
in Theorem 1. At the beginning of increasing the values of m or K  the performances are improved.
However  when their values exceed certain number  the impact of increasing m or K diminishes.
Additionally  the larger the value of λ the wider the effective region of m and K. It is notable that the
effective region of m and K of the practical variant is much larger than that of the basic variant. We
also brieﬂy report a running time result: to obtain an  = 10−3 duality gap for optimizing L2-SVM
on covtype data with λ = 10−3  the running time of DisDCA-p with m = 1  10  102  103 by ﬁxing
K = 10 are 30  4  0  5 seconds 3  respectively  and the running time with K = 1  5  10  20 by ﬁxing
m = 100 are 3  0  0  1 seconds  respectively. The speed-up gain on kdd data by increasing m is even
larger because the communication cost is much higher. In supplement  we present more results on
visualizing the communication and computation tradeoff.
The Practical Variant vs The Basic Variant To further demonstrate the usefulness of the practical
variant  we present a comparison between the practical variant and the basic variant for optimizing

30 second means less than 1 second. We exclude the time for computing the duality gap at each iteration.

7

(a) varying m

(b) varying m

(c) Different Algorithms

(d) varying K

(e) varing K

(f) Different Algorithms

Figure 3: (a b): duality gap with varying m; (d e): duality gap with varying K; (c  f) comparison of
different algorithms for optimizing SVMs. More results can be found in supplementary materials.

the two SVM formulations in supplementary material. We also include the performances of the ag-
gressive variant proposed in [20]  by applying the aggressive updates on the m sampled examples in
each machine without incurring additional communication cost. The results show that the practical
variant converges much faster than the basic variant and the aggressive variant.
Comparison with other baselines Lastly  we compare DisDCA with SGD-based and ADMM-
based distributed algorithms running in the same distributed framework. For optimizing L2-SVM 
we implement the stochastic average gradient (SAG) algorithm [15]  which also enjoys a linear con-
vergence for smooth and strongly convex problems. We use the constant step size (1/Ls) suggested
by the authors for obtaining a good practical performance  where the Ls denotes the smoothness
parameter of the problem  set to 2R + λ given (cid:107)xi(cid:107)2
2 ≤ R ∀i. For optimizing L1-SVM  we compare
to the stochastic Pegasos. For ADMM-based algorithms  we implement a stochastic ADMM in [14]
√
(ADMM-s) and a deterministic ADMM in [23] (ADMM-dca) that employes the DCA algorithm for
solving the subproblems. In the stochastic ADMM  there is a step size parameter ηt ∝ 1/
t. We
choose the best initial step size among [10−3  103]. We run all algorithms on K = 10 machines and
set m = 104  λ = 10−6 for all stochastic algorithms. In terms of the parameter ρ in ADMM  we ﬁnd
that ρ = 10−6 yields good performances by searching over a range of values. We compare DisDCA
with SAG  Pegasos and ADMM-s in Figures 3(c)  3(f) 4  which clearly demonstrate that DisDCA is
a strong competitor in optimizing SVMs. In supplement we compare DisDCA by setting m = nk
against ADMM-dca with four different values of ρ = 10−6  10−4  10−2  1 on kdd. The results show
that the performances deteriorate signiﬁcantly if the ρ is not appropriately set  while DisDCA can
produce comparable performance without additional efforts in tuning the parameter.

5 Conclusions
We have presented a distributed stochastic dual coordinate descent algorithm and its convergence
rates  and analyzed the tradeoff between computation and communication. The practical variant has
substantial improvements over the basic variant and other variants. We also make a comparison with
other distributed algorithms and observe competitive performances.

4The primal objective of Pegasos on covtype is above the display range.

8

02040608010000.51duality gapDisDCA−b (covtype  L2SVM  K=10  λ=10−3)  m=1m=10m=102m=103m=1040204060801000.70.750.80.850.9number of iteration (*100)duality gapDisDCA−b (covtype  L2SVM  K=10  λ=10−6)  02040608010000.51duality gapDisDCA−p (L2SVM  K=10  λ=10−3)  m=1m=10m=102m=103m=10402040608010000.511.5number of iteration (*100)duality gapDisDCA−p (L2SVM  K=10  λ=10−6)  204060801000.50.60.70.80.9number of iterations (*100)primal objcovtype  L1SVM  K=10  m=104  λ=10−6  204060801000.70.720.740.76primal objcovtype  L2SVM  K=10  m=104  λ=10−6  DisDCAADMM−s (η=10)SAGDisDCAADMM−s (η=100)Pegasos02040608010000.51dualtiy gapDisDCA−b (covtype  L2SVM  m=102  λ=10−3)  K=1K=5K=100204060801000.70.80.91DisDCA−b (covtype  L2SVM  m=102  λ=10−6)number of iterations (*100)duality gap0102030405000.51duality gapDisDCA−p (covtype  L2SVM  m=103  λ=10−3)  K=1K=5K=100204060801000123DisDCA−p (covtype  L2SVM  m=103  λ=10−6)number of iterations (*100)duality gap0204060801000.20.40.60.81kdd  L2SVM  K=10  m=104  λ=10−6primal obj  DisDCA−pADMM−s (η=10)SAG0204060801000.20.40.60.8number of iterations (*100)primal objkdd  L1SVM  K=10  m=104  λ=10−6  DisDCA−pADMM−s (η=100)PegasosReferences
[1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization.

5451–5452  2012.

In CDC  pages

[2] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Found. Trends Mach. Learn.  3:1–
122  2011.

[3] J. K. Bradley  A. Kyrola  D. Bickson  and C. Guestrin. Parallel Coordinate Descent for L1-

Regularized Loss Minimization. In ICML  2011.

[4] C. T. Chu  S. K. Kim  Y. A. Lin  Y. Yu  G. R. Bradski  A. Y. Ng  and K. Olukotun. Map-Reduce

for machine learning on multicore. In NIPS  pages 281–288  2006.

[5] W. Deng and W. Yin. On the global and linear convergence of the generalized alternating

direction method of multipliers. Technical report  2012.

[6] M. Eberts and I. Steinwart. Optimal learning rates for least squares svms using gaussian ker-

nels. In NIPS  pages 1539–1547  2011.

[7] D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems

via ﬁnite element approximation. Comput. Math. Appl.  2:17–40  1976.

[8] C.-J. Hsieh  K.-W. Chang  C.-J. Lin  S. S. Keerthi  and S. Sundararajan. A dual coordinate

descent method for large-scale linear svm. In ICML  pages 408–415  2008.

[9] H. D. III  J. M. Phillips  A. Saha  and S. Venkatasubramanian. Protocols for learning classiﬁers

on distributed data. JMLR- Proceedings Track  22:282–290  2012.

[10] S. Lacoste-Julien  M. Jaggi  M. W. Schmidt  and P. Pletscher. Stochastic block-coordinate

frank-wolfe optimization for structural svms. CoRR  abs/1207.4747  2012.

[11] J. Langford  A. Smola  and M. Zinkevich. Slow learners are fast. In NIPS  pages 2331–2339.

2009.

[12] Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex
differentiable minimization. Journal of Optimization Theory and Applications  pages 7–35 
1992.

[13] G. Mann  R. McDonald  M. Mohri  N. Silberman  and D. Walker. Efﬁcient Large-Scale dis-
tributed training of conditional maximum entropy models. In NIPS  pages 1231–1239. 2009.
[14] H. Ouyang  N. He  L. Tran  and A. G. Gray. Stochastic alternating direction method of multi-

pliers. In ICML  pages 80–88  2013.

[15] N. L. Roux  M. W. Schmidt  and F. Bach. A stochastic gradient method with an exponential

convergence rate for ﬁnite training sets. In NIPS  pages 2672–2680  2012.

[16] S. Shalev-Shwartz and T. Zhang. Stochastic Dual Coordinate Ascent Methods for Regularized

Loss Minimization. JMLR  2013.

[17] S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Anal. Appl.

(Singap.)  1(1):17–41  2003.

[18] K. Sridharan  S. Shalev-Shwartz  and N. Srebro. Fast rates for regularized objectives. In NIPS 

pages 1545–1552  2008.

[19] T. Suzuki. Dual averaging and proximal gradient descent for online alternating direction mul-

tiplier method. In ICML  pages 392–400  2013.

[20] M. Tak´ac  A. S. Bijral  P. Richt´arik  and N. Srebro. Mini-batch primal and dual methods for

svms. In ICML  2013.

[21] C. H. Teo  S. Vishwanthan  A. J. Smola  and Q. V. Le. Bundle methods for regularized risk

minimization. JMLR  pages 311–365  2010.

[22] K. I. Tsianos  S. Lawlor  and M. G. Rabbat. Communication/computation tradeoffs in

consensus-based distributed optimization. In NIPS  pages 1952–1960  2012.

[23] C. Zhang  H. Lee  and K. G. Shin. Efﬁcient distributed linear classiﬁcation algorithms via the

alternating direction method of multipliers. In AISTAT  pages 1398–1406  2012.

[24] M. Zinkevich  M. Weimer  A. Smola  and L. Li. Parallelized stochastic gradient descent. In

NIPS  pages 2595–2603  2010.

9

,Tianbao Yang