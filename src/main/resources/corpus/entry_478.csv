2011,Algorithms for Hyper-Parameter Optimization,Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally  hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently  computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets  but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [Larochelle et al.  2007] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.,Algorithms for Hyper-Parameter Optimization

James Bergstra

The Rowland Institute

Harvard University

bergstra@rowland.harvard.edu

R´emi Bardenet

Laboratoire de Recherche en Informatique

Universit´e Paris-Sud
bardenet@lri.fr

Yoshua Bengio

D´ept. d’Informatique et Recherche Op´erationelle

Universit´e de Montr´eal

yoshua.bengio@umontreal.ca

Bal´azs K´egl

Linear Accelerator Laboratory
Universit´e Paris-Sud  CNRS

balazs.kegl@gmail.com

Abstract

Several recent advances to the state of the art in image classiﬁcation benchmarks
have come from better conﬁgurations of existing techniques rather than novel ap-
proaches to feature learning. Traditionally  hyper-parameter optimization has been
the job of humans because they can be very efﬁcient in regimes where only a few
trials are possible. Presently  computer clusters and GPU processors make it pos-
sible to run more trials and we show that algorithmic approaches can ﬁnd better
results. We present hyper-parameter optimization results on tasks of training neu-
ral networks and deep belief networks (DBNs). We optimize hyper-parameters
using random search and two new greedy sequential methods based on the ex-
pected improvement criterion. Random search has been shown to be sufﬁciently
efﬁcient for learning neural networks for several datasets  but we show it is unreli-
able for training DBNs. The sequential algorithms are applied to the most difﬁcult
DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best
previously reported. This work contributes novel techniques for making response
surface models P (y|x) in which many elements of hyper-parameter assignment
(x) are known to be irrelevant given particular values of other elements.

1

Introduction

Models such as Deep Belief Networks (DBNs) [2]  stacked denoising autoencoders [3]  convo-
lutional networks [4]  as well as classiﬁers based on sophisticated feature extraction techniques
have from ten to perhaps ﬁfty hyper-parameters  depending on how the experimenter chooses to
parametrize the model  and how many hyper-parameters the experimenter chooses to ﬁx at a rea-
sonable default. The difﬁculty of tuning these models makes published results difﬁcult to reproduce
and extend  and makes even the original investigation of such methods more of an art than a science.
Recent results such as [5]  [6]  and [7] demonstrate that the challenge of hyper-parameter opti-
mization in large and multilayer models is a direct impediment to scientiﬁc progress. These works
have advanced state of the art performance on image classiﬁcation problems by more concerted
hyper-parameter optimization in simple algorithms  rather than by innovative modeling or machine
learning strategies. It would be wrong to conclude from a result such as [5] that feature learning
is useless. Instead  hyper-parameter optimization should be regarded as a formal outer loop in the
learning process. A learning algorithm  as a functional from data to classiﬁer (taking classiﬁcation
problems as an example)  includes a budgeting choice of how many CPU cycles are to be spent
on hyper-parameter exploration  and how many CPU cycles are to be spent evaluating each hyper-
parameter choice (i.e. by tuning the regular parameters). The results of [5] and [7] suggest that
with current generation hardware such as large computer clusters and GPUs  the optimal alloca-

1

tion of CPU cycles includes more hyper-parameter exploration than has been typical in the machine
learning literature.
Hyper-parameter optimization is the problem of optimizing a loss function over a graph-structured
conﬁguration space. In this work we restrict ourselves to tree-structured conﬁguration spaces. Con-
ﬁguration spaces are tree-structured in the sense that some leaf variables (e.g. the number of hidden
units in the 2nd layer of a DBN) are only well-deﬁned when node variables (e.g. a discrete choice of
how many layers to use) take particular values. Not only must a hyper-parameter optimization algo-
rithm optimize over variables which are discrete  ordinal  and continuous  but it must simultaneously
choose which variables to optimize.
In this work we deﬁne a conﬁguration space by a generative process for drawing valid samples.
Random search is the algorithm of drawing hyper-parameter assignments from that process and
evaluating them. Optimization algorithms work by identifying hyper-parameter assignments that
could have been drawn  and that appear promising on the basis of the loss function’s value at other
points. This paper makes two contributions: 1) Random search is competitive with the manual
optimization of DBNs in [1]  and 2) Automatic sequential optimization outperforms both manual
and random search.
Section 2 covers sequential model-based optimization  and the expected improvement criterion. Sec-
tion 3 introduces a Gaussian Process based hyper-parameter optimization algorithm. Section 4 in-
troduces a second approach based on adaptive Parzen windows. Section 5 describes the problem of
DBN hyper-parameter optimization  and shows the efﬁciency of random search. Section 6 shows
the efﬁciency of sequential optimization on the two hardest datasets according to random search.
The paper concludes with discussion of results and concluding remarks in Section 7 and Section 8.

2 Sequential Model-based Global Optimization

Sequential Model-Based Global Optimization (SMBO) algorithms have been used in many applica-
tions where evaluation of the ﬁtness function is expensive [8  9]. In an application where the true
ﬁtness function f : X → R is costly to evaluate  model-based algorithms approximate f with a sur-
rogate that is cheaper to evaluate. Typically the inner loop in an SMBO algorithm is the numerical
optimization of this surrogate  or some transformation of the surrogate. The point x∗ that maximizes
the surrogate (or its transformation) becomes the proposal for where the true function f should be
evaluated. This active-learning-like algorithm template is summarized in Figure 1. SMBO algo-
rithms differ in what criterion they optimize to obtain x∗ given a model (or surrogate) of f  and in
they model f via observation history H.

SMBO(cid:0)f  M0  T  S(cid:1)

H ← ∅ 
For t ← 1 to T  

1
2
3
4
5
6
7
Figure 1: The pseudo-code of generic Sequential Model-Based Optimization.

x∗ ← argminx S(x  Mt−1) 
Evaluate f (x∗) 
H ← H ∪ (x∗  f (x∗)) 
Fit a new model Mt to H.

(cid:46) Expensive step

return H

The algorithms in this work optimize the criterion of Expected Improvement (EI) [10]. Other cri-
teria have been suggested  such as Probability of Improvement and Expected Improvement [10] 
minimizing the Conditional Entropy of the Minimizer [11]  and the bandit-based criterion described
in [12]. We chose to use the EI criterion in our work because it is intuitive  and has been shown to
work well in a variety of settings. We leave the systematic exploration of improvement criteria for
future work. Expected improvement is the expectation under some model M of f : X → RN that
f (x) will exceed (negatively) some threshold y∗:

(cid:90) ∞

−∞

EIy∗ (x) :=

max(y∗ − y  0)pM (y|x)dy.

(1)

2

The contribution of this work is two novel strategies for approximating f by modeling H: a hier-
archical Gaussian Process and a tree-structured Parzen estimator. These are described in Section 3
and Section 4 respectively.

3 The Gaussian Process Approach (GP)

Gaussian Processes have long been recognized as a good method for modeling loss functions in
model-based optimization literature [13]. Gaussian Processes (GPs  [14]) are priors over functions
that are closed under sampling  which means that if the prior distribution of f is believed to be a GP
with mean 0 and kernel k  the conditional distribution of f knowing a sample H = (xi  f (xi))n
i=1
of its values is also a GP  whose mean and covariance function are analytically derivable. GPs with
generic mean functions can in principle be used  but it is simpler and sufﬁcient for our purposes
to only consider zero mean processes. We do this by centering the function values in the consid-
ered data sets. Modelling e.g. linear trends in the GP mean leads to undesirable extrapolation in
unexplored regions during SMBO [15].

The above mentioned closedness property  along with the fact that GPs provide an assessment of
prediction uncertainty incorporating the effect of data scarcity  make the GP an elegant candidate
for both ﬁnding candidate x∗ (Figure 1  step 3) and ﬁtting a model Mt (Figure 1  step 6). The runtime
of each iteration of the GP approach scales cubically in |H| and linearly in the number of variables
being optimized  however the expense of the function evaluations f (x∗) typically dominate even
this cubic cost.

3.1 Optimizing EI in the GP
We model f with a GP and set y∗ to the best value found after observing H: y∗ = min{f (xi)  1 ≤
i ≤ n}. The model pM in (1) is then the posterior GP knowing H. The EI function in (1) encap-
sulates a compromise between regions where the mean function is close to or better than y∗ and
under-explored regions where the uncertainty is high.

EI functions are usually optimized with an exhaustive grid search over the input space  or a Latin
Hypercube search in higher dimensions. However  some information on the landscape of the EI cri-
terion can be derived from simple computations [16]: 1) it is always non-negative and zero at training
points from D  2) it inherits the smoothness of the kernel k  which is in practice often at least once
differentiable  and noticeably  3) the EI criterion is likely to be highly multi-modal  especially as
the number of training points increases. The authors of [16] used the preceding remarks on the
landscape of EI to design an evolutionary algorithm with mixture search  speciﬁcally aimed at opti-
mizing EI  that is shown to outperform exhaustive search for a given budget in EI evaluations. We
borrow here their approach and go one step further. We keep the Estimation of Distribution (EDA 
[17]) approach on the discrete part of our input space (categorical and discrete hyper-parameters) 
where we sample candidate points according to binomial distributions  while we use the Covariance
Matrix Adaptation - Evolution Strategy (CMA-ES  [18]) for the remaining part of our input space
(continuous hyper-parameters). CMA-ES is a state-of-the-art gradient-free evolutionary algorithm
for optimization on continuous domains  which has been shown to outperform the Gaussian search
EDA. Notice that such a gradient-free approach allows non-differentiable kernels for the GP regres-
sion. We do not take on the use of mixtures in [16]  but rather restart the local searches several times 
starting from promising places. The use of tesselations suggested by [16] is prohibitive here  as our
task often means working in more than 10 dimensions  thus we start each local search at the center
of mass of a simplex with vertices randomly picked among the training points.

Finally  we remark that all hyper-parameters are not relevant for each point. For example  a DBN
with only one hidden layer does not have parameters associated to a second or third layer. Thus it
is not enough to place one GP over the entire space of hyper-parameters. We chose to group the
hyper-parameters by common use in a tree-like fashion and place different independent GPs over
each group. As an example  for DBNs  this means placing one GP over common hyper-parameters 
including categorical parameters that indicate what are the conditional groups to consider  three
GPs on the parameters corresponding to each of the three layers  and a few 1-dimensional GPs over
individual conditional hyper-parameters  like ZCA energy (see Table 1 for DBN parameters).

3

4 Tree-structured Parzen Estimator Approach (TPE)
Anticipating that our hyper-parameter optimization tasks will mean high dimensions and small ﬁt-
ness evaluation budgets  we now turn to another modeling strategy and EI optimization scheme for
the SMBO algorithm. Whereas the Gaussian-process based approach modeled p(y|x) directly  this
strategy models p(x|y) and p(y).
Recall from the introduction that the conﬁguration space X is described by a graph-structured gen-
erative process (e.g. ﬁrst choose a number of DBN layers  then choose the parameters for each).
The tree-structured Parzen estimator (TPE) models p(x|y) by transforming that generative process 
replacing the distributions of the conﬁguration prior with non-parametric densities. In the exper-
imental section  we will see that the conﬁguation space is described using uniform  log-uniform 
quantized log-uniform  and categorical variables.
In these cases  the TPE algorithm makes the
following replacements: uniform → truncated Gaussian mixture  log-uniform → exponentiated
truncated Gaussian mixture  categorical → re-weighted categorical. Using different observations
{x(1)  ...  x(k)} in the non-parametric densities  these substitutions represent a learning algorithm
that can produce a variety of densities over the conﬁguration space X . The TPE deﬁnes p(x|y)
using two such densities:

g(x)

(2)
where (cid:96)(x) is the density formed by using the observations {x(i)} such that corresponding loss
f (x(i)) was less than y∗ and g(x) is the density formed by using the remaining observations.
Whereas the GP-based approach favoured quite an aggressive y∗ (typically less than the best ob-
served loss)  the TPE algorithm depends on a y∗ that is larger than the best observed f (x) so that
some points can be used to form (cid:96)(x). The TPE algorithm chooses y∗ to be some quantile γ of the
observed y values  so that p(y < y∗) = γ  but no speciﬁc model for p(y) is necessary. By maintain-
ing sorted lists of observed variables in H  the runtime of each iteration of the TPE algorithm can
scale linearly in |H| and linearly in the number of variables (dimensions) being optimized.

(cid:26)(cid:96)(x)

p(x|y) =

if y < y∗
if y ≥ y∗ 

(cid:90) y∗

−∞

(cid:90) y∗

−∞

4.1 Optimizing EI in the TPE algorithm
The parametrization of p(x  y) as p(y)p(x|y) in the TPE algorithm was chosen to facilitate the
optimization of EI.

−∞

−∞

p(x|y)p(y)

p(x)

dy

p(y)dy 

EIy∗ (x) =

(cid:17)−1

(y∗ − y)

(cid:90) y∗

−∞

γ(cid:96)(x)+(1−γ)g(x)

−∞ p(y)dy

γ + g(x)

(cid:96)(x) (1 − γ)

∝ (cid:16)

(y∗ − y)p(y|x)dy =

By construction  γ = p(y < y∗) and p(x) =(cid:82)
(3)
(cid:90) y∗
(cid:90) y∗
R p(x|y)p(y)dy = γ(cid:96)(x) + (1 − γ)g(x). Therefore
(y∗ − y)p(x|y)p(y)dy = (cid:96)(x)
(y∗ − y)p(y)dy = γy∗(cid:96)(x) − (cid:96)(x)
γy∗(cid:96)(x)−(cid:96)(x)(cid:82) y∗

. This last expression
so that ﬁnally EIy∗ (x) =
shows that to maximize improvement we would like points x with high probability under (cid:96)(x)
and low probability under g(x). The tree-structured form of (cid:96) and g makes it easy to draw many
candidates according to (cid:96) and evaluate them according to g(x)/(cid:96)(x). On each iteration  the algorithm
returns the candidate x∗ with the greatest EI.
4.2 Details of the Parzen Estimator
The models (cid:96)(x) and g(x) are hierarchical processes involving discrete-valued and continuous-
valued variables. The Adaptive Parzen Estimator yields a model over X by placing density in
the vicinity of K observations B = {x(1)  ...  x(K)} ⊂ H. Each continuous hyper-parameter was
speciﬁed by a uniform prior over some interval (a  b)  or a Gaussian  or a log-uniform distribution.
The TPE substitutes an equally-weighted mixture of that prior with Gaussians centered at each of
the x(i) ∈ B. The standard deviation of each Gaussian was set to the greater of the distances to the
left and right neighbor  but clipped to remain in a reasonable range. In the case of the uniform  the
points a and b were considered to be potential neighbors. For discrete variables  supposing the prior
was a vector of N probabilities pi  the posterior vector elements were proportional to N pi + Ci
where Ci counts the occurrences of choice i in B. The log-uniform hyper-parameters were treated
as uniforms in the log domain.

4

Table 1: Distribution over DBN hyper-parameters for random sampling. Options separated by “or”
such as pre-processing (and including the random seed) are weighted equally. Symbol U means
uniform  N means Gaussian-distributed  and log U means uniformly distributed in the log-domain.
CD (also known as CD-1) stands for contrastive divergence  the algorithm used to initialize the layer
parameters of the DBN.

Whole model

Parameter
pre-processing
ZCA energy
random seed
classiﬁer learn rate
classiﬁer anneal start
classiﬁer (cid:96)2-penalty
n. layers
batch size

Prior
raw or ZCA
U (.5  1)
5 choices
log U (0.001  10)
log U (100  104)
0 or log U (10−7  10−4)
1 to 3
20 or 100

Parameter
n. hidden units
W init
a
algo A coef
CD epochs
CD learn rate
CD anneal start
CD sample data

Per-layer
Prior
log U (128  4096)
U (−a  a) or N (0  a2)
algo A or B (see text)
U (.2  2)
log U (1  104)
log U (10−4  1)
log U (10  104)
yes or no

5 Random Search for Hyper-Parameter Optimization in DBNs

One simple  but recent step toward formalizing hyper-parameter optimization is the use of random
search [5]. [19] showed that random search was much more efﬁcient than grid search for optimizing
the parameters of one-layer neural network classiﬁers. In this section  we evaluate random search
for DBN optimization  compared with the sequential grid-assisted manual search carried out in [1].
We chose the prior listed in Table 1 to deﬁne the search space over DBN conﬁgurations. The details
of the datasets  the DBN model  and the greedy layer-wise training procedure based on CD are
provided in [1]. This prior corresponds to the search space of [1] except for the following differences:
(a) we allowed for ZCA pre-processing [20]  (b) we allowed for each layer to have a different size 
(c) we allowed for each layer to have its own training parameters for CD  (d) we allowed for the
possibility of treating the continuous-valued data as either as Bernoulli means (more theoretically
correct) or Bernoulli samples (more typical) in the CD algorithm  and (e) we did not discretize the
possible values of real-valued hyper-parameters. These changes expand the hyper-parameter search
problem  while maintaining the original hyper-parameter search space as a subset of the expanded
search space.
The results of this preliminary random search are in Figure 2. Perhaps surprisingly  the result of
manual search can be reliably matched with 32 random trials for several datasets. The efﬁciency
of random search in this setting is explored further in [21]. Where random search results match
human performance  it is not clear from Figure 2 whether the reason is that it searched the original
space as efﬁciently  or that it searched a larger space where good performance is easier to ﬁnd. But
the objection that random search is somehow cheating by searching a larger space is backward –
the search space outlined in Table 1 is a natural description of the hyper-parameter optimization
problem  and the restrictions to that space by [1] were presumably made to simplify the search
problem and make it tractable for grid-search assisted manual search. Critically  both methods train
DBNs on the same datasets.
The results in Figure 2 indicate that hyper-parameter optimization is harder for some datasets. For
example  in the case of the “MNIST rotated background images” dataset (MRBI)  random sampling
appears to converge to a maximum relatively quickly (best models among experiments of 32 trials
show little variance in performance)  but this plateau is lower than what was found by manual search.
In another dataset (convex)  the random sampling procedure exceeds the performance of manual
search  but is slow to converge to any sort of plateau. There is considerable variance in generalization
when the best of 32 models is selected. This slow convergence indicates that better performance is
probably available  but we need to search the conﬁguration space more efﬁciently to ﬁnd it. The
remainder of this paper explores sequential optimization strategies for hyper-parameter optimization
for these two datasets: convex and MRBI.

6 Sequential Search for Hyper-Parameter Optimization in DBNs

We validated our GP approach of Section 3.1 by comparing with random sampling on the Boston
Housing dataset  a regression task with 506 points made of 13 scaled input variables and a scalar

5

Figure 2: Deep Belief Network (DBN) performance according to random search. Random
search is used to explore up to 32 hyper-parameters (see Table 1). Results found using a
grid-search-assisted manual search over a similar domain with an average 41 trials are
given in green (1-layer DBN) and red (3-layer DBN). Each box-plot (for N = 1  2  4  ...)
shows the distribution of test set performance when the best model among N random trials
is selected. The datasets “convex” and “mnist rotated background images” are used for
more thorough hyper-parameter optimization.

regressed output. We trained a Multi-Layer Perceptron (MLP) with 10 hyper-parameters  including
learning rate  (cid:96)1 and (cid:96)2 penalties  size of hidden layer  number of iterations  whether a PCA pre-
processing was to be applied  whose energy was the only conditional hyper-parameter [22]. Our
results are depicted in Figure 3. The ﬁrst 30 iterations were made using random sampling  while
from the 30th on  we differentiated the random samples from the GP approach trained on the updated
history. The experiment was repeated 20 times. Although the number of points is particularly small
compared to the dimensionality  the surrogate modelling approach ﬁnds noticeably better points than
random  which supports the application of SMBO approaches to more ambitious tasks and datasets.
Applying the GP to the problem of optimizing DBN performance  we allowed 3 random restarts to
the CMA+ES algorithm per proposal x∗  and up to 500 iterations of conjugate gradient method in
ﬁtting the length scales of the GP. The squared exponential kernel [14] was used for every node.
The CMA-ES part of GPs dealt with boundaries using a penalty method  the binomial sampling part
dealt with it by nature. The GP algorithm was initialized with 30 randomly sampled points in H.
After 200 trials  the prediction of a point x∗ using this GP took around 150 seconds.
For the TPE-based algorithm  we chose γ = 0.15 and picked the best among 100 candidates drawn
from (cid:96)(x) on each iteration as the proposal x∗. After 200 trials  the prediction of a point x∗ using
this TPE algorithm took around 10 seconds. TPE was allowed to grow past the initial bounds used
with for random sampling in the course of optimization  whereas the GP and random search were
restricted to stay within the initial bounds throughout the course of optimization. The TPE algorithm
was also initialized with the same 30 randomly sampled points as were used to seed the GP.

6.1 Parallelizing Sequential Search
Both the GP and TPE approaches were actually run asynchronously in order to make use of multiple
compute nodes and to avoid wasting time waiting for trial evaluations to complete. For the GP ap-
proach  the so-called constant liar approach was used: each time a candidate point x∗ was proposed 
a fake ﬁtness evaluation equal to the mean of the y’s within the training set D was assigned tem-
porarily  until the evaluation completed and reported the actual loss f (x∗). For the TPE approach 
we simply ignored recently proposed points and relied on the stochasticity of draws from (cid:96)(x) to
provide different candidates from one iteration to the next. The consequence of parallelization is
that each proposal x∗ is based on less feedback. This makes search less efﬁcient  though faster in
terms of wall time.

6

1248163264128experimentsize(#trials)0.00.20.40.60.81.0accuracymnistbasic1248163264128experimentsize(#trials)0.00.10.20.30.40.50.60.70.80.9accuracymnistbackgroundimages1248163264128experimentsize(#trials)0.00.10.20.30.40.50.6accuracymnistrotatedbackgroundimages1248163264128experimentsize(#trials)0.450.500.550.600.650.700.750.800.85accuracyconvex1248163264128experimentsize(#trials)0.40.50.60.70.80.91.0accuracyrectangles1248163264128experimentsize(#trials)0.450.500.550.600.650.700.750.80accuracyrectanglesimagesMRBI

convex
14.13 ±0.30 % 44.55 ±0.44%
TPE
16.70 ± 0.32% 47.08 ± 0.44%
GP
18.63 ± 0.34% 47.39 ± 0.44%
Manual
Random 18.97 ± 0.34 % 50.52 ± 0.44%

Table 2: The test set classiﬁcation error of
the best model found by each search algo-
rithm on each problem. Each search algo-
rithm was allowed up to 200 trials. The man-
ual searches used 82 trials for convex and 27
trials MRBI.

Figure 3: After time 30  GP optimizing
the MLP hyper-parameters on the Boston
Housing regression task. Best minimum
found so far every 5 iterations  against
time. Red = GP  Blue = Random. Shaded
areas = one-sigma error bars.

Runtime per trial was limited to 1 hour of GPU computation regardless of whether execution was on
a GTX 285  470  480  or 580. The difference in speed between the slowest and fastest machine was
roughly two-fold in theory  but the actual efﬁciency of computation depended also on the load of the
machine and the conﬁguration of the problem (the relative speed of the different cards is different in
different hyper-parameter conﬁgurations). With the parallel evaluation of up to ﬁve proposals from
the GP and TPE algorithms  each experiment took about 24 hours of wall time using ﬁve GPUs.

7 Discussion
The trajectories (H) constructed by each algorithm up to 200 steps are illustrated in Figure 4  and
compared with random search and the manual search carried out in [1]. The generalization scores
of the best models found using these algorithms and others are listed in Table 2. On the convex
dataset (2-way classiﬁcation)  both algorithms converged to a validation score of 13% error.
In
generalization  TPE’s best model had 14.1% error and GP’s best had 16.7%. TPE’s best was sig-
niﬁcantly better than both manual search (19%) and random search with 200 trials (17%). On the
MRBI dataset (10-way classiﬁcation)  random search was the worst performer (50% error)  the GP
approach and manual search approximately tied (47% error)  while the TPE algorithm found a new
best result (44% error). The models found by the TPE algorithm in particular are better than pre-
viously found ones on both datasets. The GP and TPE algorithms were slightly less efﬁcient than
manual search: GP and EI identiﬁed performance on par with manual search within 80 trials  the
manual search of [1] used 82 trials for convex and 27 trials for MRBI.
There are several possible reasons for why the TPE approach outperformed the GP approach in
these two datasets. Perhaps the inverse factorization of p(x|y) is more accurate than the p(y|x) in
the Gaussian process. Perhaps  conversely  the exploration induced by the TPE’s lack of accuracy
turned out to be a good heuristic for search. Perhaps the hyper-parameters of the GP approach itself
were not set to correctly trade off exploitation and exploration in the DBN conﬁguration space. More
empirical work is required to test these hypotheses. Critically though  all four SMBO runs matched
or exceeded both random search and a careful human-guided search  which are currently the state
of the art methods for hyper-parameter optimization.
The GP and TPE algorithms work well in both of these settings  but there are certainly settings
in which these algorithms  and in fact SMBO algorithm in general  would not be expected to do
well. Sequential optimization algorithms work by leveraging structure in observed (x  y) pairs. It is
possible for SMBO to be arbitrarily bad with a bad choice of p(y|x). It is also possible to be slower
than random sampling at ﬁnding a global optimum with a apparently good p(y|x)  if it extracts
structure in H that leads only to a local optimum.
8 Conclusion

This paper has introduced two sequential hyper-parameter optimization algorithms  and shown them
to meet or exceed human performance and the performance of a brute-force random search in two
difﬁcult hyper-parameter optimization tasks involving DBNs. We have relaxed standard constraints
(e.g. equal layer sizes at all layers) on the search space  and fall back on a more natural hyper-
parameter space of 32 variables (including both discrete and continuous variables) in which many

7

0102030405014161820222426TimeBestvaluesofarFigure 4: Efﬁciency of Gaussian Process-based (GP) and graphical model-based (TPE) se-
quential optimization algorithms on the task of optimizing the validation set performance
of a DBN of up to three layers on the convex task (left) and the MRBI task (right). The
dots are the elements of the trajectory H produced by each SMBO algorithm. The solid
coloured lines are the validation set accuracy of the best trial found before each point in
time. Both the TPE and GP algorithms make signiﬁcant advances from their random ini-
tial conditions  and substantially outperform the manual and random search methods. A
95% conﬁdence interval about the best validation means on the convex task extends 0.018
above and below each point  and on the MRBI task extends 0.021 above and below each
point. The solid black line is the test set accuracy obtained by domain experts using a
combination of grid search and manual search [1]. The dashed line is the 99.5% quan-
tile of validation performance found among trials sampled from our prior distribution (see
Table 1)  estimated from 457 and 361 random trials on the two datasets respectively.

variables are sometimes irrelevant  depending on the value of other parameters (e.g. the number of
layers). In this 32-dimensional search problem  the TPE algorithm presented here has uncovered new
best results on both of these datasets that are signiﬁcantly better than what DBNs were previously
believed to achieve. Moreover  the GP and TPE algorithms are practical: the optimization for each
dataset was done in just 24 hours using ﬁve GPU processors. Although our results are only for
DBNs  our methods are quite general  and extend naturally to any hyper-parameter optimization
problem in which the hyper-parameters are drawn from a measurable set.
We hope that our work may spur researchers in the machine learning community to treat the hyper-
parameter optimization strategy as an interesting and important component of all learning algo-
rithms. The question of “How well does a DBN do on the convex task?” is not a fully speciﬁed 
empirically answerable question – different approaches to hyper-parameter optimization will give
different answers. Algorithmic approaches to hyper-parameter optimization make machine learning
results easier to disseminate  reproduce  and transfer to other domains. The speciﬁc algorithms we
have presented here are also capable  at least in some cases  of ﬁnding better results than were pre-
viously known. Finally  powerful hyper-parameter optimization algorithms broaden the horizon of
models that can realistically be studied; researchers need not restrict themselves to systems of a few
variables that can readily be tuned by hand.
The TPE algorithm presented in this work  as well as parallel evaluation infrastructure  is available
as BSD-licensed free open-source software  which has been designed not only to reproduce the
results in this work  but also to facilitate the application of these and similar algorithms to other
hyper-parameter optimization problems.1

Acknowledgements

This work was supported by the National Science and Engineering Research Council of Canada 
Compute Canada  and by the ANR-2010-COSI-002 grant of the French National Research Agency.
GPU implementations of the DBN model were provided by Theano [23].

1“Hyperopt” software package: https://github.com/jaberg/hyperopt

8

050100150200time(trials)0.150.200.250.300.350.400.450.50error(fractionincorrect)Dataset:convexmanual99.5’thq.GPTPE050100150200time(trials)0.50.60.70.80.9error(fractionincorrect)Dataset:mnistrotatedbackgroundimagesmanual99.5’thq.GPTPEReferences
[1] H. Larochelle  D. Erhan  A. Courville  J. Bergstra  and Y. Bengio. An empirical evaluation of deep

architectures on problems with many factors of variation. In ICML 2007  pages 473–480  2007.

[2] G. E. Hinton  S. Osindero  and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation 

18:1527–1554  2006.

[3] P. Vincent  H. Larochelle  I. Lajoie  Y. Bengio  and P. A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. Machine Learning
Research  11:3371–3408  2010.

[4] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  November 1998.

[5] Nicolas Pinto  David Doukhan  James J. DiCarlo  and David D. Cox. A high-throughput screening ap-
proach to discovering good forms of biologically inspired visual representation. PLoS Comput Biol 
5(11):e1000579  11 2009.

[6] A. Coates  H. Lee  and A. Ng. An analysis of single-layer networks in unsupervised feature learning.

NIPS Deep Learning and Unsupervised Feature Learning Workshop  2010.

[7] A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector
quantization. In Proceedings of the Twenty-eighth International Conference on Machine Learning (ICML-
11)  2010.

[8] F. Hutter. Automated Conﬁguration of Algorithms for Solving Hard Computational Problems. PhD thesis 

University of British Columbia  2009.

[9] F. Hutter  H. Hoos  and K. Leyton-Brown. Sequential model-based optimization for general algorithm

conﬁguration. In LION-5  2011. Extended version as UBC Tech report TR-2010-10.

[10] D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global

Optimization  21:345–383  2001.

[11] J. Villemonteix  E. Vazquez  and E. Walter. An informational approach to the global optimization of

expensive-to-evaluate functions. Journal of Global Optimization  2006.

[12] N. Srinivas  A. Krause  S. Kakade  and M. Seeger. Gaussian process optimization in the bandit setting:

No regret and experimental design. In ICML  2010.

[13] J. Mockus  V. Tiesis  and A. Zilinskas. The application of Bayesian methods for seeking the extremum.
In L.C.W. Dixon and G.P. Szego  editors  Towards Global Optimization  volume 2  pages 117–129. North
Holland  New York  1978.

[14] C.E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning.
[15] D. Ginsbourger  D. Dupuy  A. Badea  L. Carraro  and O. Roustant. A note on the choice and the estimation

of kriging models for the analysis of deterministic computer experiments. 25:115–131  2009.

[16] R. Bardenet and B. K´egl. Surrogating the surrogate: accelerating Gaussian Process optimization with

mixtures. In ICML  2010.

[17] P. Larra˜naga and J. Lozano  editors. Estimation of Distribution Algorithms: A New Tool for Evolutionary

Computation. Springer  2001.

[18] N. Hansen. The CMA evolution strategy: a comparing review. In J.A. Lozano  P. Larranaga  I. Inza  and
E. Bengoetxea  editors  Towards a new evolutionary computation. Advances on estimation of distribution
algorithms  pages 75–102. Springer  2006.

[19] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Learning Workshop

(Snowbird)  2011.

[20] A. Hyv¨arinen and E. Oja. Independent component analysis: Algorithms and applications. Neural Net-

works  13(4–5):411–430  2000.

[21] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR  2012. Accepted.
[22] C. Bishop. Neural networks for pattern recognition. 1995.
[23] J. Bergstra  O. Breuleux  F. Bastien  P. Lamblin  R. Pascanu  G. Desjardins  J. Turian  and Y. Bengio.
Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Comput-
ing Conference (SciPy)  June 2010.

9

,Mahdi Milani Fard
Yuri Grinberg
Amir-massoud Farahmand
Joelle Pineau
Doina Precup
Srinath Sridhar
Davis Rempe
Julien Valentin
Leonidas Guibas