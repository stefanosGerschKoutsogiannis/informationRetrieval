2009,Efficient Recovery of Jointly Sparse Vectors,We consider the reconstruction of sparse signals in the multiple measurement vector (MMV) model in which the signal  represented as a matrix  consists of a set of jointly sparse vectors. MMV is an extension of the single measurement vector (SMV) model employed in standard compressive sensing (CS). Recent theoretical studies focus on the convex relaxation of the MMV problem based on the $(2 1)$-norm minimization  which is an extension of the well-known $1$-norm minimization employed in SMV. However  the resulting convex optimization problem in MMV is significantly much more difficult to solve than the one in SMV. Existing algorithms reformulate it as a second-order cone programming (SOCP) or semidefinite programming (SDP)  which is computationally expensive to solve for problems of moderate size. In this paper  we propose a new (dual) reformulation of the convex optimization problem in MMV and develop an efficient algorithm based on the prox-method. Interestingly  our theoretical analysis reveals the close connection between the proposed reformulation and multiple kernel learning. Our simulation studies demonstrate the scalability of the proposed algorithm.,Efﬁcient Recovery of Jointly Sparse Vectors

Liang Sun  Jun Liu  Jianhui Chen  Jieping Ye

School of Computing  Informatics  and Decision Systems Engineering

{sun.liang j.liu jianhui.chen jieping.ye}asu.edu

Arizona State University

Tempe  AZ 85287

Abstract

We consider the reconstruction of sparse signals in the multiple measurement vec-
tor (MMV) model  in which the signal  represented as a matrix  consists of a set
of jointly sparse vectors. MMV is an extension of the single measurement vector
(SMV) model employed in standard compressive sensing (CS). Recent theoret-
ical studies focus on the convex relaxation of the MMV problem based on the
(2  1)-norm minimization  which is an extension of the well-known 1-norm mini-
mization employed in SMV. However  the resulting convex optimization problem
in MMV is signiﬁcantly much more difﬁcult to solve than the one in SMV. Ex-
isting algorithms reformulate it as a second-order cone programming (SOCP) or
semideﬁnite programming (SDP) problem  which is computationally expensive
to solve for problems of moderate size. In this paper  we propose a new (dual)
reformulation of the convex optimization problem in MMV and develop an efﬁ-
cient algorithm based on the prox-method. Interestingly  our theoretical analysis
reveals the close connection between the proposed reformulation and multiple ker-
nel learning. Our simulation studies demonstrate the scalability of the proposed
algorithm.

1 Introduction

Compressive sensing (CS)  also known as compressive sampling  has recently received increasing
attention in many areas of science and engineering [3]. In CS  an unknown sparse signal is recon-
structed from a single measurement vector. Recent theoretical studies show that one can recover
certain sparse signals from far fewer samples or measurements than traditional methods [4  8]. In
this paper  we consider the problem of reconstructing sparse signals in the multiple measurement
vector (MMV) model  in which the signal  represented as a matrix  consists of a set of jointly sparse
vectors. MMV is an extension of the single measurement vector (SMV) model employed in standard
compressive sensing.
The MMV model was motivated by the need to solve the neuromagnetic inverse problem that arises
in Magnetoencephalography (MEG)  which is a modality for imaging the brain [7]. It arises from
a variety of applications  such as DNA microarrays [11]  equalization of sparse communication
channels [6]  echo cancellation [9]  magenetoencephalography [12]  computing sparse solutions to
linear inverse problems [7]  and source localization in sensor networks [17]. Unlike SMV  the signal
in the MMV model is represented as a set of jointly sparse vectors sharing their common nonzeros
occurring in a set of locations [5  7]. It has been shown that the additional block-sparse structure can
lead to improved performance in signal recovery [5  10  16  21].
Several recovery algorithms have been proposed for the MMV model in the past [5  7  18  24  25].
Since the sparse representation problem is a combinatorial optimization problem and is in general
NP-hard [5]  the algorithms in [18  25] employ the greedy strategy to recover the signal using an
iterative scheme. One alternative is to relax it into a convex optimization problem  from which the

1

global optimal solution can be obtained. The most widely studied approach is the one based on the
(2  1)-norm minimization [5  7  10]. A similar relaxation technique (via the 1-norm minimization)
is employed in the SMV model. Recent studies have shown that most of theoretical results on the
convex relaxation of the SMV model can be extended to the MMV model [5]  although further the-
oretical investigation is needed [26]. Unlike the SMV model where the 1-norm minimization can
be solved efﬁciently  the resulting convex optimization problem in MMV is much more difﬁcult to
solve. Existing algorithms formulate it as a second-order cone programming (SOCP) or semdeﬁnite
programming (SDP) [16] problem  which can be solved using standard software packages such as
SeDuMi [23]. However  for problems of moderate size  solving either SOCP or SDP is computa-
tionally expensive  which limits their use in practice.
In this paper  we derive a dual reformulation of the (2  1)-norm minimization problem in MMV.
More especially  we show that the (2  1)-norm minimization problem can be reformulated as a
min-max problem  which can be solved efﬁciently via the prox-method with a nearly dimension-
independent convergence rate [19]. Compared with existing algorithms  our algorithm can scale to
larger problems while achieving high accuracy. Interestingly  our theoretical analysis reveals the
close relationship between the resulting min-max problem and multiple kernel learning [14]. We
have performed simulation studies and our results demonstrate the scalability of the proposed algo-
rithm in comparison with existing algorithms.
Notations: All matrices are boldface uppercase. Vectors are boldface lowercase. Sets and spaces
are denoted with calligraphic letters. The p-norm of the vector v = (v1 ···   vd)T ∈ IRd is deﬁned
as (cid:107)v(cid:107)p :=
p . The inner product on IRm×d is deﬁned as (cid:104)X  Y(cid:105) = tr(XT Y). For
matrix A ∈ IRm×d  we denote by ai and ai the ith row and the ith column of A  respectively. The
(r  p)-norm of A is deﬁned as:

(cid:179)(cid:80)d

i=1 |vi|p

(cid:180) 1

(cid:195)
m(cid:88)

(cid:33) 1

p

(cid:107)A(cid:107)r p :=

(cid:107)ai(cid:107)p

r

.

(1)

2 The Multiple Measurement Vector Model

i=1

In the SMV model  one aims to recover the sparse signal w from a measurement vector b = Aw
for a given matrix A [3]. The SMV model can be extended to the multiple measurement vector
(MMV) model  in which the signal is represented as a set of jointly sparse vectors sharing a common
set of nonzeros [5  7]. The MMV model aims to recover the sparse representations for SMVs
simultaneously. It has been shown that the MMV model provably improves the standard CS recovery
by exploiting the block-sparse structure [10  21].
Speciﬁcally  in the MMV model we consider the reconstruction of the signal represented by a matrix
W ∈ IRd×n  which is given by a dictionary (or measurement matrix) A ∈ IRm×d and multiple
measurement vector B ∈ IRm×n such that

B = AW.

(2)
Each column of A is associated with an atom  and a set of atom is called a dictionary. A sparse
representation means that the matrix W has a small number of rows containing nonzero entries.
Usually  we have m (cid:191) d and d > n.
Similar to SMV  we can use (cid:107)W(cid:107)p 0 to measure the number of rows in W that contain nonzero en-
tries. Thus  the problem of ﬁnding the sparsest representation of the signal W in MMV is equivalent
to solving the following problem  a.k.a. the sparse representation problem:

(cid:107)W(cid:107)p 0 

s.t. AW = B.

(P0) : min
W

(3)
Some typical choices of p include p = ∞ and p = 2 [25]. However  solving (P0) requires enumer-
ating all subsets of the set {1  2 ···   d}  which is essentially a combinatorial optimization problem
and is in general NP-hard [5]. Similar to the use of the 1-norm minimization in the SMV model  one
natural alternative is to use (cid:107)W(cid:107)p 1 instead of (cid:107)W(cid:107)p 0  resulting in the following convex optimiza-
tion problem (P1):
(4)

s.t. AW = B.

(cid:107)W(cid:107)p 1 

(P1) : min
W

2

The relationship between (P0) and (P1) for the MMV model has been studied in [5].
For p = 2  the optimal W is given by solving the following convex optimization problem:

min
W

1
2

(cid:107)W(cid:107)2

2 1

s.t. AW = B.

(5)

Existing algorithms formulate Eq. (5) as a second-order cone programming (SOCP) problem or a
semideﬁnite programming (SDP) problem [16]. Recall that the optimizaiton problem in Eq. (5) is
equivalent to the following problem by removing the square in the objective:

min
W

1
2

(cid:107)W(cid:107)2 1

s.t. AW = B.

d(cid:88)

By introducing auxiliary variable ti(i = 1 ···   d)  this problem can be reformulated in the standard
second-order cone programming (SOCP) formulation:

min

W t1 ···  td

s.t.

ti

1
2
(cid:107)Wi(cid:107)2 ≤ ti  ti ≥ 0  i = 1 ···   d  AW = B.

i=1

(6)

Based on this SOCP formulation  it can also be transformed into the standard semideﬁnite program-
ming (SDP) formulation:

d(cid:88)

i=1

ti

1
2

(cid:183)

tiI WiT
Wi
ti

min

W t1 ···  td

s.t.

(cid:184)

≥ 0  ti ≥ 0  i = 1 ···   d  AW = B.

(7)

The interior point method [20] and the bundle method [13] can be applied to solve SOCP and SDP.
However  they do not scale to problems of moderate size  which limits their use in practice.

3 The Proposed Dual Formulation

In this section we present a dual reformulation of the optimization problem in Eq. (5). First  some
preliminary results are summarized in Lemmas 1 and 2:
Lemma 1. Let A and X be m-by-d matrices. Then the following holds:

(cid:104)A  X(cid:105) ≤ 1
2

2 1 + (cid:107)A(cid:107)2

2 ∞

.

(8)

(cid:80)m
When the equality holds  we have (cid:107)X(cid:107)2 1 = (cid:107)A(cid:107)2 ∞.
Proof. It follows from the deﬁnition of the (r  p)-norm in Eq. (1) that (cid:107)X(cid:107)2 1 =
i=1 (cid:107)xi(cid:107)2 
and (cid:107)A(cid:107)2 ∞ = max1≤i≤m (cid:107)ai(cid:107)2. Without loss of generality  we assume that (cid:107)ak(cid:107)2 =
max1≤i≤m (cid:107)ai(cid:107)2 for 1 ≤ k ≤ m . Thus  (cid:107)A(cid:107)2 ∞ = (cid:107)ak(cid:107)2  and we have

(cid:161)(cid:107)X(cid:107)2

(cid:162)

(cid:104)A  X(cid:105) =

≤ 1
2

i=1

i=1

i=1

(cid:33)2

aixiT ≤ m(cid:88)
m(cid:88)
(cid:107)ai(cid:107)2(cid:107)xi(cid:107)2 ≤ m(cid:88)
(cid:107)ak(cid:107)2
 =
(cid:195)
m(cid:88)
(cid:190)

(cid:107)xi(cid:107)2

2 +

(cid:189)
(cid:104)A  X(cid:105) − 1
2

(cid:107)X(cid:107)2

max
X

1
2

i=1

2 1

Clearly  the last inequality becomes equality when (cid:107)X(cid:107)2 1 = (cid:107)A(cid:107)2 ∞.
Lemma 2. Let A and X be deﬁned as in Lemma 1. Then the following holds:

m(cid:88)

(cid:162)

i=1

.

(cid:107)ak(cid:107)2(cid:107)xi(cid:107)2 = (cid:107)ak(cid:107)2

(cid:107)xi(cid:107)2

(cid:161)(cid:107)A(cid:107)2

2 ∞ + (cid:107)X(cid:107)2

2 1

=

(cid:107)A(cid:107)2

2 ∞.

1
2

3

Proof. Denote the set Q = {k : 1 ≤ k ≤ m (cid:107)ak(cid:107)2 = max1≤i≤m (cid:107)ai(cid:107)2}. Let {αk}m
that αk = 0 for k /∈ Q  αk ≥ 0 for k ∈ Q  and
of Lemma 1 become equalities if and only if we construct the matrix X as follows:

k=1 be such
k=1 αk = 1. Clearly  all inequalities in the proof

Thus  the maximum of (cid:104)A  X(cid:105) − 1
as in Eq. (9).

2 ∞  which is achieved when X is constructed

(9)

Based on the results established in Lemmas 1 and 2  we can derive the dual formulation of the
optimization problem in Eq. (5) as follows. First we construct the Lagrangian L:

L(W  U) =

1
2

(cid:107)W(cid:107)2

2 1 − (cid:104)U  AW − B(cid:105) =

1
2

(cid:107)W(cid:107)2

2 1 − (cid:104)U  AW(cid:105) + (cid:104)U  B(cid:105).

The dual problem can be formulated as follows:

(cid:189)

max
U
It follows from Lemma 2 that

min
W

(cid:190)
2 1 − (cid:104)U  AW(cid:105)

min
W

(cid:107)W(cid:107)2

1
2

(cid:107)W(cid:107)2

1
2

2 1 − (cid:104)U  AW(cid:105) + (cid:104)U  B(cid:105).
(cid:189)

(cid:190)
2 1 − (cid:104)AT U  W(cid:105)

(cid:107)W(cid:107)2

= min
W

1
2

(10)

= −1
2

(cid:107)AT U(cid:107)2

2 ∞.

Note that from Lemma 2  the equality holds if and only if the optimal W∗ can be represented as

(11)
where α = [α1 ···   αd]T ∈ IRd  αi ≥ 0 if (cid:107)(AT U)i(cid:107)2 = (cid:107)AT U(cid:107)2 ∞  αi = 0 if (cid:107)(AT U)i(cid:107)2 <
(cid:107)AT U(cid:107)2 ∞  and
i=1 αi = 1. Thus  the dual problem can be simpliﬁed into the following form:

(cid:80)d

W∗ = diag(α)AT U 

max
U

(12)
Following the deﬁnition of the (2 ∞)-norm  we can reformulate the dual problem in Eq. (12) as a
min-max problem  as summarized in the following theorem:
Theorem 1. The optimization problem in Eq. (5) can be formulated equivalently as:

(cid:107)AT U(cid:107)2

2 ∞ + (cid:104)U  B(cid:105).

−1
2

(cid:80)d

min

i=1 θi=1 θi≥0

max
u1 ···  un

j bj − 1
uT
2

θiuT

j Giuj

 

(13)

(cid:40)

n(cid:88)

j=1

d(cid:88)

i=1

(cid:41)

where the matrix Gi is deﬁned as Gi = aiaT
Proof. Note that (cid:107)AT U(cid:107)2

2 ∞ can be reformulated as follows:

i (1 ≤ i ≤ d)  and ai is the ith column of A.

(cid:80)m

(cid:189)

if k ∈ Q
otherwise.

xk =
2(cid:107)X(cid:107)2

αkak 
0 
2(cid:107)A(cid:107)2

2 1 is 1

(cid:107)AT U(cid:107)2

2 ∞ = max
1≤i≤d

(cid:170)
i U(cid:107)2
d(cid:88)

2

(cid:169)(cid:107)aT
(cid:80)d

max

i=1 θi=1

i=1

=

θi≥0 

= max
1≤i≤d

{tr(UT aiaT

i U)} = max
1≤i≤d

{tr(UT GiU)}

θitr(UT GiU).

(14)

Substituting Eq. (14) into Eq. (12)  we obtain the following problem:
(cid:104)U  B(cid:105) − 1
2

2 ∞ + (cid:104)U  B(cid:105) ⇔ max

(cid:107)AT U(cid:107)2

−1
2

(cid:80)d

max
U

min

U

i=1 θi=1 θi≥0

d(cid:88)

i=1

θitr(UT GiU).

(15)

Since the Slater’s condition [2] is satisﬁed  the minimization and maximization in Eq. (15) can be
exchanged  resulting in the min-max problem in Eq. (13).
Corollary 1. Let (θ∗  U∗) be the optimal solution to Eq. (13) where θ∗ = (θ∗
then (cid:107)(AT U∗)i(cid:107)2 = (cid:107)AT U∗(cid:107)2 ∞.

1 ···   θ∗

d)T . If θ∗

i > 0 

4

(cid:107)(cid:161)

Based on the solution to the dual problem in Eq. (13)  we can construct the optimal solution to the
primal problem in Eq. (5) as follows. Let W∗ be the optimal solution of Eq. (5). It follows from
Lemma 2 that we can construct W∗ based on AT U∗ as in Eq. (11). Recall that W∗ must satisfy
the equality constraint AW∗ = B. The main result is summarized in the following theorem:
Theorem 2. Given W∗ = diag(α)AT U∗  where α = [α1 ···   αd] ∈ IRd  αi ≥ 0  αi > 0 only if
i=1 αi = 1. Then  AW∗ = B if and only if (α  U∗) is the

AT U∗(cid:162)i (cid:107)2 = (cid:107)AT U∗(cid:107)2 ∞  and

optimal solution to the problem in Eq. (13).
Proof. First we assume that (α  U∗) is the optimal solution to the problem in Eq. (13). It follows
that the partial derivative of the objective function with respect to U∗ in Eq. (13) is 0  that is 

(cid:80)d

B − Adiag(α)AT U∗ = 0 ⇔ AW∗ = B.

Next we prove the reverse direction by assuming AW∗ = B. Since W∗ = diag(α)AT U∗  we have
(16)

0 = B − AW∗ = B − Adiag(α1 ···   αd)AT U∗.

Deﬁne the function φ(θ1 ···   θd  U) as

φ(θ1 ···   θd  U) = (cid:104)U  B(cid:105) − 1
2

θitr(UT GiU) =

j bj − 1
uT
2

θiuT

j Giuj

.

We consider the function φ(α1 ···   αd  U) with ﬁxed θi = αi(1 ≤ i ≤ d). Note that this function
is concave with respect to U  thus its maximum is achieved when its partial derivative with respect
∂U is zero when U = U∗. Thus  we have
to U is zero. It follows from Eq. (16) that ∂φ
With a ﬁxed U = U∗  φ(θ1 ···   θd  U∗) is a linear combination of θi(1 ≤ i ≤ d) as:

∀U  φ(α1 ···   αd  U) ≤ φ(α1 ···   αd  U∗).

(cid:40)

n(cid:88)

j=1

d(cid:88)

i=1

d(cid:88)

i=1

(cid:41)

d(cid:88)

φ(θ1 ···   θd  U∗) = (cid:104)U∗  B(cid:105) − 1
2

θi(cid:107)(AT U∗)i(cid:107)2
2.

i=1

By the assumption  we have (cid:107)(AT U∗)i(cid:107) = (cid:107)AT U∗(cid:107)2 ∞  if αi > 0. Thus  we have

d(cid:88)
(cid:80)d
φ(α1 ···   αd  U∗) ≤ φ(θ1 ···   θd  U∗) ∀θ1 ···   θd satisfying
i=1 θi = 1  θi ≥ 0  we have

Therefore  for any U  θ1 ···   θd such that
(17)
which implies that (α1 ···   αd  U∗) is a saddle point of the min-max problem in Eq. (13). Thus 
(α  U∗) is the optimal solution to the problem in Eq. (13).

φ(α1 ···   αd  U) ≤ φ(α1 ···   αd  U∗) ≤ φ(θ1 ···   θd  U∗) 

θi = 1  θi ≥ 0.

i=1

Theorem 2 shows that we can reconstruct the solution to the primal problem based on the solution to
the dual problem in Eq. (13). It paves the way for the efﬁcient implementation based on the min-max
formulation in Eq.(13). In this paper  the prox-method [19]  which is discussed in detail in the next
section  is employed to solve the dual problem in Eq. (13).
An interesting observation is that the resulting min-max problem in Eq. (13) is closely related to the
optimization problem in multiple kernel learning (MKL) [14]. The min-max problem in Eq. (13)
can be reformulated as

(cid:190)

min

i=1 θi=1 θi≥0

max
u1 ···  un

j bj − 1
uT
2

uT
j Guj

 

(18)

where the positive semideﬁnite (kernel) matrix G is constrained as a linear combination of a set of
base kernels

as G =

Gi = aiaiT

i=1 θiGi.

(cid:189)

n(cid:88)

j=1

(cid:80)d

(cid:80)d
(cid:111)d

i=1

(cid:110)

The formulation in Eq. (18) connects the MMV problem to MKL. Many efﬁcient algorithms [14 
22  27] have been developed in the past for MKL  which can be applied to solve (13). In [27] 
an extended level set method was proposed to solve MKL  which was shown to outperform the
one based on the semi-inﬁnite linear programming formulation [22]. However  the extended level
√
set method involves a linear programming in each iteration and its theoretical convergence rate of
O(1/
N) (N denotes the number of iterations) is slower than the proposed algorithm presented in
the next section.

5

4 The Main Algorithm

We propose to employ the prox-method [19] to solve the min-max formulation in Eq. (13)  which has
a differentiable and convex-concave objective function. The algorithm is called “MMVprox”. The
prox-method is a ﬁrst-order method [1  19] which is specialized for solving the saddle point problem
and has a nearly dimension-independent convergence rate of O(1/N) (N denotes the number of
iterations). We show that each iteration of MMVprox has a low computational cost  thus it scales to
large-size problems.
The key idea is to convert the min-max problem to the associated variational inequality (v.i.) prob-
lem  which is then iteratively solved by a series of v.i. problems. Let z = (θ  U). The problem in
Eq. (13) is equivalent to the following associated v.i. problem [19]:

Find z∗ = (θ∗  U∗) ∈ S : (cid:104)F (z∗)  z − z∗(cid:105) ≥ 0 ∀z ∈ S S = X × Y 

(19)

where

(cid:181)

(cid:182)
∂U φ(θ  U)

F (z) =

(20)
is an operator constituted by the gradient of φ(· ·)  X = {θ ∈ IRd : (cid:107)θ(cid:107)1 = 1  θi ≥ 0}  and
Y = IRm×n.
In solving the v.i. problem in Eq. (19)  one key building block is the following projection problem:

φ(θ  U) − ∂

∂
∂θ

(cid:183)

(cid:184)
2 + (cid:104)˜z  ¯z − z(cid:105)

 

Pz(¯z) = arg min
˜z∈S

(cid:107)˜z(cid:107)2

1
2

where ¯z = ( ¯θ  ¯U) and ˜z = ( ˜θ  ˜U). Denote (θ∗  U∗) = Pz(¯z). It is easy to verify that

and

(cid:107) ˜θ − (θ − ¯θ)(cid:107)2
2 

1
2

θ∗ = arg min
˜θ∈X
U∗ = U − ¯U.

(21)

(22)

(23)

Following [19]  we present the pseudocode of the proposed MMVprox algorithm in Algorithm 1. In
each iteration  we compute the projection (21) so that wt s is sufﬁciently close to wt s−1 (controlled
by the parameter δ).
[L denotes the Lipschitz
continuous constant of the operator F (·)]  the inner iteration converges within two iterations  i.e. 
wt 2 = wt 1 always holds. Moreover  Algorithm 1 has a global dimension-independent convergence
rate of O(1/N).

It has been shown in [19] that  when γ ≤ 1√

2L

Algorithm 1 The MMVprox Algorithm
Input: A  B  γ  z0 = (θ0  U0)  and δ
Output: θ  U and W.
Step t (t ≥ 1): Set wt 0 = zt−1 and ﬁnd the smallest s = 1  2  . . . such that
wt s = Pzt−1(γF (wt s−1)) (cid:107)wt s − wt s−1(cid:107)2 ≤ δ.
(cid:80)t

(cid:80)t

Set zt = wt s
Final Step: Set θ =

i=1 θi

t

  U =

i=1 Ui

t

  W = diag(θ)AT U.

Time Complexity It costs O(dmn) to evaluate the operator F (·) at a given point. θ∗ in Eq. (22)
involves the Euclidean projection onto the simplex [1]  which can be solved in linear time  i.e.  in
O(d); and U∗ in Eq. (23) can be analytically computed in O(mn) time. Recall that at each iteration
t  the inner iteration is at most 2. Thus  the time complexity for any given outer iteration is O(dmn).
Our analysis shows that MMVprox scales to large-size problems.
In comparison  the second-order methods such as SOCP have a much higher complexity per iter-
ation. According to [15]  the SOCP in Eq. (6) costs O(d3(n + 1)3) per iteration. In MMV  d is
typically larger than m. In this case  the proposed MMVprox algorithm has a much smaller cost
per iteration than SOCP. This explains why MMVprox scales better than SOCP  as shown in our
experiments in the next section.

6

Table 1: The averaged recovery results over 10 experiments (d = 100  m = 50  and n = 80).

(cid:112)(cid:107)W − Wp(cid:107)2

(cid:112)(cid:107)AWp − B(cid:107)2

F /(dn)

F /(mn)

Data set
1
2
3
4
5
6
7
8
9
10
Mean
Std

5 Experiments

3.2723e-6
3.4576e-6
2.6971e-6
2.4099e-6
2.9611e-6
2.5701e-6
2.0884e-6
2.3454e-6
2.6807e-6
2.7172e-6
2.7200e-6
4.1728e-7

1.4467e-5
1.8234e-5
1.4464e-5
1.4460e-5
1.4463e-5
1.4459e-5
1.4469e-5
1.4475e-5
1.4461e-5
1.4481e-5
1.4843e-5
1.1914e-6

In this section  we conduct simulations to evaluate the proposed MMVprox algorithm in terms of the
recovery quality and scalability.
Experiment Setup We generated a set of synthetic data sets (by varying the values of m  n  and
d) for our experiments: the entries in A ∈ IRm×d were independently generated from the standard
normal distribution N (0  1); W ∈ IRd×n (the ground truth of the recovery problems) was generated
in two steps: (1) randomly select k rows with nonzero entries; (2) randomly generate the entries of
those k rows from N (0  1). We denote by Wp the solution obtained from the proposed MMVprox
algorithm. Ideally  Wp should be close to W. Our experiments were performed on a PC with Intel
Core 2 Duo T9500 2.6G CPU and 4G RAM. We employed the optimization package SeDuMi [23]
for solving the SOCP formulation. All codes were implemented in Matlab. In all experiments  we
terminate MMVprox when the change of the consecutive approximate solutions is less than 1e-6.
Recovery Quality In this experiment  we evaluate the recovery quality of the proposed MMVprox
algorithm. We applied MMVprox on the data sets of size d = 100  m = 50  n = 80  and reported
the averaged experimental results over 10 random repetitions. We measured the recovery quality in
F /(mn) 
terms of the mean squared error:
which measures the violation of the constraint in Eq. (5). The experimental results are presented in
Table 1. We can observe from the table that MMVprox recovers the sparse signal successfully in all
cases.
Next  we study how the recovery error changes as the sparsity of W varies. Speciﬁcally  we applied
MMVprox on the data sets of size d = 100  m = 400  and n = 10 with k (the number of nonzero
rows of W) varying from 0.05d to 0.7d  and used
F /(dn) as the recovery quality
measure. The averaged experimental results over 20 random repetitions are presented in Figure 1.
We can observe from the ﬁgure that MMVprox works well in all cases  and a larger k (less sparse
W) tends to result in a larger recovery error.

(cid:112)(cid:107)AWp − B(cid:107)2

(cid:112)(cid:107)W − Wp(cid:107)2

(cid:112)(cid:107)W − Wp(cid:107)2

F /(dn). We also reported

Figure 1: The increase of the recovery error as the sparsity level decreases

7

0.050.20.350.50.700.511.52x 10−6k/dpkW−Wpk2F/(dn)Scalability In this experiment  we study the scalability of the proposed MMVprox algorithm. We
generated a collection of data sets by varying m from 10 to 200 with a step size of 10  and setting
n = 2m and d = 4m accordingly. We applied SOCP and MMVprox on the data sets and recorded
their computation time. The experimental results are presented in Figure 2 (a)  where the x-axis
corresponds to the value of m  and the y-axis corresponds to log(t)  where t denotes the computa-
tion time (in seconds). We can observe from the ﬁgure that the computation time of both algorithms
increases as m increases and SOCP is faster than MMVprox on small problems (m ≤ 40); when
m > 40  MMVprox outperforms SOCP; when the value of m is large (m > 80)  the SOCP for-
mulation cannot be solved by SeDuMi  while MMVprox can still be applied. This experimental
result demonstrates the good scalability of the proposed MMVprox algorithm in comparison with
the SOCP formulation.

(a)

(b)

Figure 2: Scalability comparison of MMVprox and SOCP: (a) the computation time for both algorithms as the
problem size varies; and (b) the average computation time of each iteration for both algorithms as the problem
size varies. The x-axis denotes the value of m  and the y-axis denotes the computation time in seconds (in log
scale).

To further examine the scalability of both algorithms  we compare the execution time of each itera-
tion for both SOCP and the proposed algorithm. We use the same setting as in the last experiment 
i.e.  n = 2m  d = 4m  and m ranges from 10 to 200 with a step size of 10. The time comparison
of SOCP and MMVprox is presented in Figure 2 (b). We observe that MMVprox has a signiﬁcantly
lower cost than SOCP in each iteration (note that SOCP is not applicable for m > 80). This is
consistent with our complexity analysis in Section 4.
We can observe from Figure 2 that when m is small  the computation time of SOCP and MMVprox
is comparable  although MMVprox is much faster in each iteration. This is because MMVprox is a
ﬁrst-order method  which has a slower convergence rate than the second-order method SOCP. Thus 
there is a tradeoff between scalability and convergence rate. Our experiments show the advantage of
MMVprox for large-size problems.

6 Conclusions
In this paper  we consider the (2  1)-norm minimization for the reconstruction of sparse signals in
the multiple measurement vector (MMV) model  in which the signal consists of a set of jointly
sparse vectors. Existing algorithms formulate it as second-order cone programming or semdeﬁnite
programming  which is computationally expensive to solve for problems of moderate size. In this
paper  we propose an equivalent dual formulation for the (2  1)-norm minimization in the MMV
model  and develop the MMVprox algorithm for solving the dual formulation based on the prox-
method. In addition  our theoretical analysis reveals the close connection between the proposed
dual formulation and multiple kernel learning. Our simulation studies demonstrate the effectiveness
of the proposed algorithm in terms of recovery quality and scalability. In the future  we plan to
compare existing solvers for multiple kernel learning [14  22  27] with the proposed MMVprox
algorithm. In addition  we plan to examine the efﬁciency of the prox-method for solving various
MKL formulations.

Acknowledgements
This work was supported by NSF IIS-0612069  IIS-0812551  CCF-0811790  NIH R01-HG002516 
NGA HM1582-08-1-0016  and NSFC 60905035.

8

50100150200−20246810mlog(t)  SOCPMMVprox50100150200−10−8−6−4−2024mlog(t)  SOCPMMVproxReferences
[1] A. Ben-Tal and A. Nemirovski. Non-Euclidean restricted memory level method for large-scale convex

optimization. Mathematical Programming  102(3):407–56  2005.

[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  Cambridge  UK  2004.
[3] E. Cand`es. Compressive sampling. In International Congress of Mathematics  number 3  pages 1433–

1452  Madrid  Spain  2006.

[4] E. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: exact signal reconstruction from highly

incomplete frequency information. IEEE Transactions on Information Theory  52(2):489–509  2006.

[5] J. Chen and X. Huo. Theoretical results on sparse representations of multiple-measurement vectors. IEEE

Transactions on Signal Processing  54(12):4634–4643  2006.

[6] S.F. Cotter and B.D. Rao. Sparse channel estimation via matching pursuit with application to equalization.

IEEE Transactions on Communications  50(3):374–377  2002.

[7] S.F. Cotter  B.D. Rao  Kjersti Engan  and K. Kreutz-Delgado. Sparse solutions to linear inverse problems

with multiple measurement vectors. IEEE Transactions on Signal Processing  53(7):2477–2488  2005.

[8] D.L. Donoho. Compressed sensing. IEEE Transactions on Information Theory  52(4):1289–1306  2006.
[9] D.L. Duttweiler. Proportionate normalized least-mean-squares adaptation in echo cancelers. IEEE Trans-

actions on Speech and Audio Processing  8(5):508–518  2000.

[10] Y.C. Eldar and M. Mishali. Robust recovery of signals from a structured union of subspaces. To Appear

in IEEE Transactions on Information Theory  2009.

[11] S. Erickson and C. Sabatti. Empirical bayes estimation of a sparse vector of gene expression changes.

Statistical Applications in Genetics and Molecular Biology  4(1):22  2008.

[12] I.F. Gorodnitsky  J.S. George  and B.D. Rao. Neuromagnetic source imaging with focuss: a recursive
weighted minimum norm algorithm. Electroencephalography and Clinical Neurophysiology  95(4):231–
251  1995.

[13] H. Jean-Baptiste and C. Lemarechal. Convex Analysis and Minimization Algorithms I: Fundamentals

(Grundlehren Der Mathematischen Wissenschaften). Springer  Berlin  1993.

[14] G.R.G. Lanckriet  N. Cristianini  P. Bartlett  L. El Ghaoui  and M.I. Jordan. Learning the kernel matrix

with semideﬁnite programming. Jouranl of Machine Learning Research  5:27–72  2004.

[15] M. Lobo  L. Vandenberghe  S. Boyd  and H. Lebret. Applications of second-order cone programming.

Linear Algebra and its Applications  284(1-3):193–228  1998.

[16] F. Parvaresh M. Stojnic and B. Hassibi. On the reconstruction of block-sparse signals with an optimal

number of measurements. CoRR  2008.

[17] D. Malioutov  M. Cetin  and A. Willsky. Source localization by enforcing sparsity through a laplacian. In

IEEE Workshop on Statistical Signal Processing  pages 553–556  2003.

[18] M. Mishali and Y.C. Eldar. Reduce and boost: Recovering arbitrary sets of jointly sparse vectors. IEEE

Transactions on Signal Processing  56(10):4692–4702  2008.

[19] A. Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with Lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on
Optimization  15(1):229–251  2005.

[20] Y.E. Nesterov and A.S. Nemirovskii.

Interior-point Polynomial Algorithms in Convex Programming.

SIAM Publications  Philadelphia  PA  1994.

[21] M. Duarte R.G. Baraniuk  V. Cevher and C. Hegde. Model-based compressive sensing. Submitted to

IEEE Transactions on Information Theory  2008.

[22] S. Sonnenburg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf. Large scale multiple kernel learning. Journal of

Machine Learning Research  7:1531–1565  2006.

[23] J.F. Sturm. Using sedumi 1.02  a MATLAB toolbox for optimization over symmetric cones. Optimization

Methods and Software  11(12):625–653  1999.

[24] J.A. Tropp. Algorithms for simultaneous sparse approximation. Part II: Convex relaxation. Signal Pro-

cessing  86(3):589–602  2006.

[25] J.A. Tropp  A.C. Gilbert  and M.J. Strauss. Algorithms for simultaneous sparse approximation. Part I:

Greedy pursuit. Signal Processing  86(3):572–588  2006.

[26] E. van den Berg and M. P. Friedlander. Joint-sparse recovery from multiple measurements. Technical

Report  Department of Computer Science  University of British Columbia  2009.

[27] Z. Xu  R. Jin  I. King  and M.R. Lyu. An extended level method for efﬁcient multiple kernel learning. In

Advances in Neural Information Processing Systems  pages 1825–1832  2008.

9

,Jianshu Chen
Ji He
Yelong Shen
Lin Xiao
Xiaodong He
Jianfeng Gao
Xinying Song
Li Deng