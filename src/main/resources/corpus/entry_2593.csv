2018,Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents,Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems  but are much faster (e.g. hours vs. days) because they parallelize better. However  many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima)  and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents  specifically novelty search (NS) and quality diversity (QD) algorithms  can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks  while retaining scalability. Our experiments confirm that the resultant new algorithms  NS-ES and two QD algorithms  NSR-ES and NSRA-ES  avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast  scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.,Improving Exploration in Evolution Strategies for
Deep Reinforcement Learning via a Population of

Novelty-Seeking Agents

Edoardo Conti⇤

Joel Lehman

Vashisht Madhavan⇤

Kenneth O. Stanley

Uber AI Labs
Abstract

Felipe Petroski Such

Jeff Clune

Evolution strategies (ES) are a family of black-box optimization algorithms able
to train deep neural networks roughly as well as Q-learning and policy gradient
methods on challenging deep reinforcement learning (RL) problems  but are much
faster (e.g. hours vs. days) because they parallelize better. However  many RL
problems require directed exploration because they have reward functions that are
sparse or deceptive (i.e. contain local optima)  and it is unknown how to encourage
such exploration with ES. Here we show that algorithms that have been invented
to promote directed exploration in small-scale evolved neural networks via popu-
lations of exploring agents  speciﬁcally novelty search (NS) and quality diversity
(QD) algorithms  can be hybridized with ES to improve its performance on sparse
or deceptive deep RL tasks  while retaining scalability. Our experiments conﬁrm
that the resultant new algorithms  NS-ES and two QD algorithms  NSR-ES and
NSRA-ES  avoid local optima encountered by ES to achieve higher performance
on Atari and simulated robots learning to walk around a deceptive trap. This paper
thus introduces a family of fast  scalable algorithms for reinforcement learning that
are capable of directed exploration. It also adds this new family of exploration
algorithms to the RL toolbox and raises the interesting possibility that analogous
algorithms with multiple simultaneous paths of exploration might also combine
well with existing RL algorithms outside ES.

Introduction

1
In RL  an agent tries to learn to perform a sequence of actions in an environment that maximizes
some notion of cumulative reward [1]. However  reward functions are often deceptive  and solely
optimizing for reward without some mechanism to encourage intelligent exploration can lead to
getting stuck in local optima and the agent failing to properly learn [1–3]. Unlike in supervised
learning with deep neural networks (DNNs)  wherein local optima are not thought to be a problem
[4  5]  the training data in RL is determined by the actions an agent takes. If the agent greedily
takes actions that maximize reward  the training data for the algorithm will be limited and it may not
discover alternate strategies with larger payoffs (i.e. it can get stuck in local optima) [1–3]. Sparse
reward signals can also be a problem for algorithms that only maximize reward  because at times there
may be no reward gradient to follow. The possibility of deceptiveness and/or sparsity in the reward
signal motivates the need for efﬁcient and directed exploration  in which an agent is motivated to visit
unexplored states in order to learn to accumulate higher rewards. Although deep RL algorithms have
performed amazing feats in recent years [6–8]  they have mostly done so despite relying on simple 
undirected (aka dithering) exploration strategies  in which an agent hopes to explore new areas of its
environment by taking random actions (e.g. epsilon-greedy exploration) [1].
A number of methods have been proposed to promote directed exploration in RL [9  10]  including
recent methods that handle high-dimensional state spaces with DNNs. A common idea is to encourage

⇤Equal contribution  corresponding authors: vashisht@uber.com  edoardo.conti@gmail.com.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

an agent to visit states it has rarely or never visited (or take novel actions in those states). Methods
proposed to track state (or state-action pair) visitation frequency include (1) approximating state
visitation counts based on either auto-encoded latent codes of states [11] or pseudo-counts from state-
space density models [12  13]  (2) learning a dynamics model that predicts future states (assuming
predictions will be worse for rarely visited states/state-action pairs) [14–16]  and (3) methods based
on compression (novel states should be harder to compress) [9].
Those methods all count each state separately. A different approach to is to hand-design (or learn) an
abstract  holistic description of an agent’s lifetime behavior  and then encourage the agent to exhibit
different behaviors from those previously performed. That is the approach of novelty search (NS) [3]
and quality diversity (QD) algorithms [17–19]  which are described in detail below. Such algorithms
are also interestingly different  and have different capabilities  because they perform exploration with
a population of agents rather than a single agent (discussed in SI Sec. 6.2). NS and QD have shown
promise with smaller neural networks on problems with low-dimensional input and output spaces
[17–22]. Evolution strategies (ES) [23] was recently shown to perform well on high-dimensional
deep RL tasks in a short amount of wall clock time by scaling well to many distributed computers. In
this paper  for the ﬁrst time  we study how these two types of algorithms can be hybridized with ES
to scale them to deep neural networks and thus tackle hard  high-dimensional deep reinforcement
learning problems  without sacriﬁcing the speed/scalability beneﬁts of ES. We ﬁrst study NS  which
performs exploration only (ignoring the reward function) to ﬁnd a set of novel solutions [3]. We then
investigate algorithms that balance exploration and exploitation  speciﬁcally novel instances of QD
algorithms  which seek to produce a set of solutions that are both novel and high-performing [17–20].
Both NS and QD are explained in detail in Sec. 3.
ES directly searches in the parameter space of a neural network to ﬁnd an effective policy. A team
from OpenAI recently showed that ES can achieve competitive performance on many reinforcement
learning (RL) tasks while offering some unique beneﬁts over traditional gradient-based RL methods
[24]. Most notably  ES is highly parallelizable  which enables near linear speedups in runtime as a
function of CPU/GPU workers. For example  with hundreds of parallel CPUs  ES was able to achieve
roughly the same performance on Atari games with the same DNN architecture in 1 hour as A3C did
in 24 hours [24]. In this paper  we investigate adding NS and QD to ES only; in future work  we will
investigate how they might be hybridized with Q-learning and policy gradient methods. We start with
ES because (1) its fast wall-clock time allows rapid experimental iteration  and (2) NS and QD were
originally developed as neuroevolution methods  making it natural to try them ﬁrst with ES  which is
also an evolutionary algorithm.
Here we test whether encouraging novelty via NS and QD improves the performance of ES on
sparse and/or deceptive control tasks. Our experiments conﬁrm that NS-ES and two simple versions
of QD-ES (NSR-ES and NSRA-ES) avoid local optima encountered by ES and achieve higher
performance on tasks ranging from simulated robots learning to walk around a deceptive trap to the
high-dimensional pixel-to-action task of playing Atari games. Our results add these new families
of exploration algorithms to the RL toolbox  opening up avenues for studying how they can best be
combined with RL algorithms  whether ES or others.
2 Background
2.1 Evolution Strategies
Evolution strategies (ES) are a class of black box optimization algorithms inspired by natural
evolution [23]: At every iteration (generation)  a population of parameter vectors (genomes) is
perturbed (mutated) and  optionally  recombined (merged) via crossover. The ﬁtness of each resultant
offspring is then evaluated according to some objective function (reward) and some form of selection
then ensures that individuals with higher reward tend to produce offspring for the next generation.
Many algorithms in the ES class differ in their representation of the population and methods of
recombination; the algorithms subsequently referred to in this work belong to the class of Natural
Evolution Strategies (NES) [25  26]. NES represents the population as a distribution of parameter
vectors ✓ characterized by parameters : p(✓). Under a ﬁtness function  f (✓)  NES seeks to
maximize the average ﬁtness of the population  E✓⇠p[f (✓)]  by optimizing  with stochastic
gradient ascent.
Recent work from OpenAI outlines a version of NES applied to standard RL benchmark problems
[24]. We will refer to this variant simply as ES going forward. In their work  a ﬁtness function
f (✓) represents the stochastic reward experienced over a full episode of agent interaction  where ✓

2

nPn
rE✓⇠[f (✓)] ⇡ 1

parameterizes the policy ⇡✓. From the population distribution pt   parameters ✓i
sampled and evaluated to obtain f (✓i
an estimate of approximate gradient of expected reward:
i=1 f (✓i

t ⇠N (✓t  2I) are
t). In a manner similar to REINFORCE [27]  ✓t is updated using

t)r log p(✓i
t)

where n is the number of samples evaluated per generation. Intuitively  NES samples parameters in
the neighborhood of ✓t and determines the direction in which ✓t should move to improve expected
reward. Since this gradient estimate has high variance  NES relies on a large n for variance reduction.
Generally  NES also evolves the covariance of the population distribution  but for the sake of fair
comparison with Salimans et al. [24] we consider only static covariance distributions  meaning  is
ﬁxed throughout training.
To sample from the population distribution  Salimans et al. [24] apply additive Gaussian noise to
the current parameter vector : ✓i
t = ✓t + ✏i where ✏i ⇠N (0  I). Although ✓ is high-dimensional 
previous work has shown Gaussian parameter noise to have beneﬁcial exploration properties when
applied to deep networks [26  28  29]. The gradient is then estimated by taking a sum of sampled
parameter perturbations weighted by their reward:

r✓tE✏⇠N (0  I)[f (✓t + ✏)] ⇡ 1

i=1 f (✓i

t)✏i

nPn

To ensure that the scale of reward between domains does not bias the optimization process  we
follow the approach of Salimans et al. [24] and rank-normalize f (✓i
t) before taking the weighted sum.
Overall  this NES variant exhibits performance on par with contemporary  gradient-based algorithms
on difﬁcult RL domains  including simulated robot locomotion and Atari environments [30].
2.2 Novelty Search (NS)
Inspired by nature’s drive towards diversity  NS encourages policies to engage in notably different
behaviors than those previously seen. The algorithm encourages different behaviors by computing
the novelty of the current policy with respect to previously generated policies and then encourages the
population distribution to move towards areas of parameter space with high novelty. NS outperforms
reward-based methods in maze and biped walking domains  which possess deceptive reward signals
that attract agents to local optima [3]. In this work  we investigate the efﬁcacy of NS at the scale
of DNNs by combining it with ES. In NS  a policy ⇡ is assigned a domain-dependent behavior
characterization b(⇡) that describes its behavior. For example  in the case of a humanoid locomotion
problem  b(⇡) may be as simple as a two-dimensional vector containing the humanoid’s ﬁnal {x  y}
location. Throughout training  every ⇡✓ evaluated adds b(⇡✓) to an archive set A with some probability.
A particular policy’s novelty N (b(⇡✓)  A) is then computed by selecting the k-nearest neighbors of
b(⇡✓) from A and computing the average distance between them:

N (✓  A) = N (b(⇡✓)  A) = 1

|S|Pj2S ||b(⇡✓)  b(⇡j)||2

S = kN N (b(⇡✓)  A)

= {b(⇡1)  b(⇡2)  ...  b(⇡k)}

Above  the distance between behavior characterizations is calculated with an L2-norm  but any
distance function can be substituted. Previously  NS has been implemented with a genetic algorithm
[3]. We next explain how NS can now be combined with ES  to leverage the advantages of both.
3 Methods
3.1 NS-ES
We use the ES optimization framework  described in Sec. 2.1  to compute and follow the gradient of
expected novelty with respect to ✓t. Given an archive A and sampled parameters ✓i
t = ✓t + ✏i  the
gradient estimate can be computed:

r✓tE✏⇠N (0  I)[N (✓t + ✏  A)|A] ⇡ 1

i=1 N (✓i

t  A)✏i

nPn

The gradient estimate obtained tells us how to change the current policy’s parameters ✓t to increase
the average novelty of our parameter distribution. We condition the gradient estimate on A  as the
archive is ﬁxed at the beginning of a given iteration and updated only at the end. We add only the
behavior characterization corresponding to each ✓t  as adding those for each sample ✓i
t would inﬂate
the archive and slow the nearest-neighbors computation. As more behavior characterizations are
added to A  the novelty landscape changes  resulting in commonly occurring behaviors becoming
“boring." Optimizing for expected novelty leads to policies that move towards unexplored areas of
behavior space.

3

NS-ES could operate with a single agent that is rewarded for acting differently than its ancestors.
However  to encourage additional diversity and get the beneﬁts of population-based exploration
described in SI Sec. 6.2  we can instead create a population of M agents  which we will refer to as
the meta-population. Each agent  characterized by a unique ✓m  is rewarded for being different from
all prior agents in the archive (ancestors  other agents  and the ancestors of other agents)  an idea
related to that of Liu et al. [31]  which optimizes for a distribution of M diverse  high-performing
policies. We hypothesize that the selection of M is domain dependent and that identifying which
domains favor which regime is a fruitful area for future research.
We initialize M random parameter vectors and at every iteration select one to update. For our
experiments  we probabilistically select which ✓m to advance from a discrete probability distribution
as a function of ✓m’s novelty. Speciﬁcally  at every iteration  for a set of agent parameter vectors
⇧= {✓1 ✓ 2  ... ✓ M}  we calculate each ✓m’s probability of being selected P (✓m) as its novelty
normalized by the sum of novelty across all policies:
(1)
PM

Having multiple  separate agents represented as independent Gaussians is a simple choice for the
meta-population distribution. In future work  more complex sampling distributions that represent the
multi-modal nature of meta-population parameter vectors could be tried.
After selecting an individual m from the meta-population  we compute the gradient of expected
novelty with respect to m’s current parameter vector  ✓m
t   and perform an update step accordingly:

P (✓m) = N (✓m A)

j=1 N (✓j  A)

✓m
t+1 ✓m

t + ↵ 1

nPn

t

  A)✏i

i=1 N (✓i m
t   ↵ is the stepsize  and ✓i m

i = ✓m

t + ✏i  where
Where n is the number of sampled perturbations to ✓m
t+1) is computed and added to the
✏i ⇠N (0  I). Once the current parameter vector is updated  b(⇡✓m
shared archive A. The whole process is repeated for a pre-speciﬁed number of iterations  as there is
no true convergence point of NS. During training  the algorithm preserves the policy with the highest
average episodic reward and returns this policy once training is complete. Although Salimans et al.
[24] return only the ﬁnal policy after training with ES  the ES experiments in this work return the
best-performing policy to facilitate fair comparison with NS-ES. Algorithm 1 in SI Sec. 6.5 outlines
a simple  parallel implementation of NS-ES. It is important to note that the addition of the archive
and the replacement of the ﬁtness function with novelty does not damage the scalability of the ES
optimization procedure (SI Sec. 6.4).
3.2 QD-ES Algorithms: NSR-ES and NSRA-ES
NS-ES alone can enable agents to avoid deceptive local optima in the reward function. Reward
signals  however  are still very informative and discarding them completely may cause performance
to suffer. Consequently  we train a variant of NS-ES  which we call NSR-ES  that combines the
reward (“ﬁtness") and novelty calculated for a given set of policy parameters ✓. Similar to NS-ES and
ES  NSR-ES operates on entire episodes and can thus evaluate reward and novelty simultaneously for
any sampled parameter vector: ✓i m
  A) 
average the two values  and set the average as the weight for the corresponding ✏i. The averaging
process is integrated into the parameter update rule as:

t + ✏i. Speciﬁcally  we compute f (✓i m

) and N (✓i m

t = ✓m

t

t

✓m
t+1 ✓m

t + ↵ 1

f (✓i m

t

)+N (✓i m

t

2

 A)

✏i

i=1

nPn

Intuitively  the algorithm follows the approximated gradient in parameter-space towards policies
that both exhibit novel behaviors and achieve high rewards. Often  however  the scales of f (✓) and
N (✓  A) differ. To combine the two signals effectively  we rank-normalize f (✓i m
  A)
independently before computing the average. Optimizing a linear combination of novelty and reward
was previously explored in Cuccu and Gomez [32] and Cuccu et al. [33]  but not with large neural
networks on high-dimensional problems. The result of NSR-ES is a set of M agents being optimized
to be both high-performing  yet different from each other.
NSR-ES has an equal weighting of the performance and novelty gradients that is static across training.
We explore a further extension of NSR-ES called NSRAdapt-ES (NSRA-ES)  which takes advantage
of the opportunity to dynamically weight the priority given to the performance gradient f (✓i m
) vs.
the novelty gradient N (✓i m
  A) by intelligently adapting a weighting parameter w during training. By
doing so  the algorithm can follow the performance gradient when it is making progress  increasingly

) and N (✓i m

t

t

t

t

4

✓m
t+1 ✓m

t + ↵ 1

i=1 wf (✓i m

t

)✏i + (1  w)N (✓i m

t

  A)✏i

nPn

try different things if stuck in a local optimum  and switch back to following the performance gradient
once unstuck. For a speciﬁc w at a given generation  the parameter update rule for NSRA-ES is
expressed as follows:

We set w = 1.0 initially and decrease it if performance stagnates across a ﬁxed number of generations.
We continue decreasing w until performance increases  at which point we increase w. While many
previous works have adapted exploration pressure online by learning the amount of noise to add to
the parameters [25  26  28  34]  such approaches rest on the assumption that an increased amount
of parameter noise leads to increased behavioral diversity  which is often not the case (e.g. too
much noise may lead to degenerate policies) [20]. Here we directly adapt the weighting between
behavioral diversity and performance  which more directly controls the trade-off of interest. SI
Sec. 6.5 provides a more detailed description of how we adapt w as well as pseudocode for NSR-ES
and NSRA-ES. Source code and hyperparameter settings for our experiments can be found here:
https://github.com/uber-research/deep-neuroevolution
4 Experiments
4.1 Simulated Humanoid Locomotion problem
We ﬁrst tested our implementation of NS-ES  NSR-ES  and NSRA-ES on the problem of having a
simulated humanoid learn to walk. We chose this problem because it is a challenging continuous
control benchmark where most would presume a reward function is necessary to solve the problem.
With NS-ES  we test whether searching through novelty alone can ﬁnd solutions to the problem.
A similar result has been shown for much smaller neural networks (⇠50-100 parameters) on a
more simple simulated biped [20]  but here we test whether NS-ES can enable locomotion at the
scale of deep neural networks on a much more sophisticated environment. NSR-ES and NSRA-ES
experiments then test the effectiveness of combining exploration and reward pressures on this difﬁcult
continuous control problem. SI Sec. 6.7 outlines complete experimental details.
The ﬁrst environment is in a slightly modiﬁed version of OpenAI Gym’s Humanoid-v1 environment.
Because the heart of this challenge is to learn to walk efﬁciently  not to walk in a particular direction 
we modiﬁed the environment reward to be isotropic (i.e. indifferent to the direction the humanoid
traveled) by setting the velocity component of reward to distance traveled from the origin as opposed
to distance traveled in the positive x direction.
As described in section 2.2  novelty search requires a domain-speciﬁc behavior characterization (BC)
for each policy  which we denote as b(⇡✓i). For the Humanoid Locomotion problem the BC is the
agent’s ﬁnal {x  y} location  as it was in Lehman and Stanley [20]. NS also requires a distance
function between two BCs. Following Lehman and Stanley [20]  the distance function is the square
of the Euclidean distance:

dist(b(⇡✓i)  b(⇡✓j )) = ||b(⇡✓i)  b(⇡✓j )||2

2

The ﬁrst result is that ES obtains a higher ﬁnal reward than NS-ES (p < 0.05) and NSR-ES (p < 0.05);
these and all future p values are calculated via a Mann-Whitney U test. The performance gap is
more pronounced for smaller amounts of computation (Fig. 1 (c)). However  many will be surprised
that NS-ES is still able to consistently solve the problem despite ignoring the environment’s reward
function. While the BC is aligned [35] with the problem in that reaching new {x  y} positions tends
to also encourage walking  there are many parts of the reward function that the BC ignores (e.g.
energy-efﬁciency  impact costs).
We hypothesize that with a sophisticated BC that encourages diversity in all of the behaviors the
multi-part reward function cares about  there would be no performance gap. However  such a BC
may be difﬁcult to construct and would likely further exaggerate the amount of computation required
for NS to match ES. NSR-ES demonstrates faster learning than NS-ES due to the addition of reward
pressure  but ultimately results in similar ﬁnal performance after 600 generations (p > 0.05  Fig. 1
(c)). Promisingly  on this non-deceptive problem  NSRA-ES does not pay a cost for its latent
exploration capabilities and performs similarly to ES (p > 0.05).
The Humanoid Locomotion problem does not appear to be a deceptive problem  at least for ES. To
test whether NS-ES  NSR-ES  and NSR-ES speciﬁcally help with deception  we also compare ES
to these algorithms on a variant of this environment we created that adds a deceptive trap (a local
optimum) that must be avoided for maximum performance (Fig. 1 (b)). In this new environment 

5

a small three-sided enclosure is placed at a short distance in front of the starting position of the
humanoid and the reward function is simply the distance traveled in the positive x direction.
Fig. 1 (d) and SI Sec. 6.8 show the reward received by each algorithm and Fig. 2 shows how the
algorithms differ qualitatively during search on this problem. In every run  ES gets stuck in the local
optimum due to following reward into the deceptive trap. NS-ES is able to avoid the local optimum
as it ignores reward completely and instead seeks to thoroughly explore the environment  but doing
so also means it makes slow progress according to the reward function. NSR-ES demonstrates
superior performance to NS-ES (p < 0.01) and ES (p < 0.01) as it beneﬁts from both optimizing for
reward and escaping the trap via the pressure for novelty. Like ES  NSRA-ES learns to walk into the
deceptive trap initially  as it initially is optimizing for reward only. Once stuck in the local optimum 
the algorithm continually increases its pressure for novelty  allowing it to escape the deceptive trap
and ultimately achieve much higher rewards than NS-ES (p < 0.01) and NSR-ES (p < 0.01). Based
just on these two domains  NSRA-ES seems to be the best algorithm across the board because it
can exploit well when there is no deception  add exploration dynamically when there is  and return
to exploiting once unstuck. The latter is likely why NSRA-ES outperforms even NSR-ES on the
deceptive humanoid locomotion problem.

Figure 1: Humanoid Locomotion Experiment. The humanoid locomotion task is shown without
a deceptive trap (a) and with one (b)  and results on them in (c) and (d)  respectively. Here and in
similar ﬁgures below  the median reward (of the best seen policy so far) per generation across 10
runs is plotted as the bold line with 95% bootstrapped conﬁdence intervals of the median (shaded).
Following Salimans et al. [24]  policy performance is measured as the average performance over ⇠30
stochastic evaluations.

Fig. 2 also shows the beneﬁt of maintaining a meta-population (M = 5) in the NS-ES  NSR-ES  and
NSRA-ES algorithms. Some lineages get stuck in the deceptive trap  incentivizing other policies
to explore around the trap. At that point  all three algorithms begin to allocate more computational
resources to this newly discovered  more promising strategy via the probabilistic selection method
outlined in Sec. 3.1. Both the novelty pressure and having a meta-population thus appear to be useful 
but in future work we look to disambiguate the relative contribution made by each.

4.2 Atari
We also tested NS-ES  NSR-ES  and NSRA-ES on numerous games from the Atari 2600 environment
in OpenAI Gym [36]. Atari games serve as an informative benchmark due to their high-dimensional
pixel input and complex control dynamics; each game also requires different levels of exploration

6

Figure 2: ES gets stuck in the deceptive local optimum while NS-ES  NSR-ES & NSRA-ES
explore to ﬁnd better solutions. An overhead view of a representative run is shown for each
algorithm on the Humanoid Locomotion with Deceptive Trap problem. The black star represents
the humanoid’s starting point. Each diamond represents the ﬁnal location of a generation’s policy 
i.e. ⇡(✓t)  with darker shading for later generations. For NS-ES  NSR-ES  & NSRA-ES plots  each
of the M = 5 agents in the meta-population and its descendants are represented by different colors.
Similar plots for all 10 runs of each algorithm are provided in SI Sec. 6.10.

to solve. To demonstrate the effectiveness of NS-ES  NSR-ES  and NSRA-ES for local optima
avoidance and directed exploration  we tested on 12 different games with varying levels of complexity 
as deﬁned by the taxonomy in Bellemare et al. [12]. Primarily  we focused on games in which  during
preliminary experiments  we observed ES prematurely converging to local optima (Seaquest  Q*Bert 
Freeway  Frostbite  and Beam Rider). However  we also included a few other games where ES did not
converge to local optima to understand the performance of our algorithm in less-deceptive domains
(Alien  Amidar  Bank Heist  Gravitar  Zaxxon  and Montezuma’s Revenge). SI Sec. 6.6 describes
additional experimental details. We report the median reward across 5 independent runs of the best
policy found in each run (see Table 1).
For the behavior characterization  we follow an idea from Naddaf [37] and concatenate Atari game
RAM states for each timestep in an episode. RAM states in Atari 2600 games are integer-valued vec-
tors of length 128 in the range [0  255] that describe all the state variables in a game (e.g. the location
of the agent and enemies). Ultimately  we want to automatically learn behavior characterizations
directly from pixels. A plethora of recent research suggests that this is a viable approach [12  38  39].
For example  low-dimensional  latent representations of the state space could be extracted from
auto-encoders [11  40] or networks trained to predict future states [14  16]. In this work  however 
we focus on learning with a pre-deﬁned  informative behavior characterization and leave the task
of jointly learning a policy and latent representation of states for future work. In effect  basing
novelty on RAM states provides a conﬁrmation of what is possible in principle with a sufﬁciently
informed behavior characterization. We also emphasize that  while during training NS-ES  NSR-ES 
and NSRA-ES use RAM states to guide novelty search  the policy itself  ⇡✓t  operates only on image
input and can be evaluated without any RAM state information. The distance between behavior
characterizations is the sum of L2-distances at each timestep t:

dist(b(⇡✓i)  b(⇡✓j )) =PT

t=1 ||(bt(⇡✓i))  bt(⇡✓j ))||2

For trajectories of different lengths  the last state of the shorter trajectory is repeated until the lengths
of both match. Because the BC distance is not normalized by trajectory length  novelty is biased
to be higher for longer trajectories. In some Atari games  this bias can lead to higher performing
policies  but in other games longer trajectories tend to have a neutral or even negative relationship
with performance. In this work we found it beneﬁcial to keep novelty unnormalized  but further
investigation into different BC designs could yield additional improvements.

7

Table 1 compares the performance of each algorithm discussed above to each other and with those
from two popular methods for exploration in RL  namely Noisy DQN [29] and A3C+ [12]. Noisy
DQN and A3C+ only outperform all the ES variants considered in this paper on 3/12 games and 2/12
games respectively. NSRA-ES  however  outperforms the other algorithms on 5/12 games  suggesting
that NS and QD are viable alternatives to contemporary exploration methods.
While the novelty pressure in NS-ES does help it avoid local optima in some cases (discussed below) 
optimizing for novelty alone does not result in higher reward in most games (although it does in
some). However  it is surprising how well NS-ES does in many tasks given that it is not explicitly
attempting to increase reward. Because NSR-ES combines exploration with reward maximization  it
is able to avoid local optima encountered by ES while also learning to play the game well. In each of
the 5 games in which we observed ES converging to premature local optima (i.e. Seaquest  Q*Bert 
Freeway  Beam Rider  Frostbite)  NSR-ES achieves a higher median reward. In the other games  ES
does not beneﬁt from adding an exploration pressure and NSR-ES performs worse. It is expected that
if there are no local optima and reward maximization is sufﬁcient to perform well  the extra cost of
encouraging exploration will hurt performance. Mitigating such costs  NSRA-ES optimizes solely
for reward until a performance plateau is reached. After that  the algorithm will assign more weight
to novelty and thus encourage exploration. We found this to be beneﬁcial  as NSRA-ES achieves
higher median rewards than ES on 8/12 games and NSR-ES on 9/12 games. It’s superior performance
validates NSRA-ES as the best among the evolutionary algorithms considered and suggests that using
an adaptive weighting between novelty and reward is a promising direction for future research.
In the game Seaquest  the avoidance of local optima is particularly important (Fig. 3). ES performance
ﬂatlines early at a median reward of 960  which corresponds to a behavior of the agent descending to
the bottom  shooting ﬁsh  and never coming up for air. This strategy represents a classic local optima 
as coming up for air requires temporarily foregoing reward  but enables far higher rewards to be
earned in the long run (Salimans et al. [24] did not encounter this particular local optimum with their
hyperparameters  but the point is that ES without exploration can get stuck indeﬁnitely on whichever
major local optima it encounters). NS-ES learns to come up for air in all 5 runs and achieves a
slightly higher median reward of 1044.5 (p < 0.05). NSR-ES also avoids this local optimum  but its
additional reward signal helps it play the game better (e.g. it is better at shooting enemies)  resulting
in a signiﬁcantly higher median reward of 2329.7 (p < 0.01). Because NSRA-ES takes reward
steps initially  it falls into the same local optimum as ES. Because we chose (without performing a
hyperparameter search) to change the weighting w between performance and novelty infrequently
(only every 50 generations)  and to change it by a small amount (only 0.05)  200 generations was not
long enough to emphasize novelty enough to escape this local optimum. We found that by changing
w every 10 generations  this problem is remedied and the performance of NSRA-ES equals that of
NSR-ES (p > 0.05  Fig. 3). These results motivate future research into better hyperparameters for
changing w  and into more complex  intelligent methods of dynamically adjusting w  including with
a population of agents with different dynamic w strategies.
The Atari results illustrate that NS is an effective mechanism for encouraging directed exploration 
given an appropriate behavior characterization  for complex  high-dimensional control tasks. A
novelty pressure alone produces impressive performance on many games  sometimes even beating
ES. Combining novelty and reward performs far better  and improves ES performance on tasks where
it appears to get stuck on local optima.
5 Discussion and Conclusion
NS and QD are classes of evolutionary algorithms designed to avoid local optima and promote
exploration in RL environments  but have only been previously shown to work with small neural
networks (on the order of hundreds of connections). ES was recently shown to be capable of training
deep neural networks that can solve challenging  high-dimensional RL tasks [24]. It also is much
faster when many parallel computers are available. Here we demonstrate that  when hybridized with
ES  NS and QD not only preserve the attractive scalability properties of ES  but also help ES explore
and avoid local optima in domains with deceptive reward functions. To the best of our knowledge  this
paper reports the ﬁrst attempt at augmenting ES to perform directed exploration in high-dimensional
environments. We thus provide an option for those interested in taking advantage of the scalability of
ES  but who also want higher performance on domains that have reward functions that are sparse or
have local optima. The latter scenario will likely hold for most challenging  real-world domains that
machine learning practitioners will wish to tackle in the future.

8

GAME
ALIEN
AMIDAR
BANK HEIST
BEAM RIDER†
FREEWAY†
FROSTBITE†
GRAVITAR
MONTEZUMA
MS. PACMAN
Q*BERT†
SEAQUEST†
ZAXXON

ES NS-ES NSR-ES NSRA-ES
4846.4
305.0
152.9
906.4
32.9
3785.4
1140.9
0.0
5171.0
1350.0
960.0
7303.3

2186.2
255.8
130.0
876.9
32.3
2978.6
732.9
0.0
3495.2
1400.0
2329.7
6723.3

3283.8
322.2
140.0
871.7
31.1
367.4
1129.4
0.0
4498.0
1075.0
960.0
9885.0

1124.5
134.7
50.0
805.5
22.8
250.0
527.5
0.0
2252.2
1234.1
1044.5
1761.9

DQN NOISYDQN
2404
2403
1610
924
1068
455
20793
10564
32
31
753
1000
366
447
3
2
2674
2722
15545
11241
4163
2282
4806
6920

A3C+
1848.3
964.7
991.9
5992.1
27.3
506.6
246.02
142.5
2380.6
15804.7
2274.1
7956.1

Table 1: Atari Results. The scores are the median  across 5 runs  of the mean reward (over ⇠30
stochastic evaluations) of each run’s best policy. SI Sec. 6.9 plots performance over time  along
with bootstrapped conﬁdence intervals of the median  for each ES algorithm for each game. In some
cases rewards reported here for ES are lower than those in Salimans et al. [24]  which could be due
to differing hyperparameters (SI Sec. 6.6). Games with a † are those in which we observed ES to
converge prematurely  presumably due to it encountering local optima. The DQN and A3C results
are reported after 200M frames of  and one to many days of  training. All evolutionary algorithm
results are reported after ⇠ 2B frames of  and ⇠2 hours of  training

Figure 3: Seaquest Case Study. By switching the weighting between novelty and reward  w  every
10 generations instead of every 50  NSRA-ES is able to overcome the local optimum ES ﬁnds and
achieve high scores on Seaquest.

Additionally  this work highlights alternate options for exploration in RL domains. The ﬁrst is to
holistically describe the behavior of an agent instead of deﬁning a per-state exploration bonus. The
second is to encourage a population of agents to simultaneously explore different aspects of an
environment. These new options thereby open new research areas into (1) comparing holistic vs.
state-based exploration  and population-based vs. single-agent exploration  more systematically and
on more domains  (2) investigating the best way to combine the merits of all of these options  and
(3) hybridizing holistic and/or population-based exploration with other algorithms that work well
on deep RL problems  such as policy gradients and DQN. It should be relatively straightforward to
combine NS with policy gradients (NS-PG). It is less obvious how to combine it with Q-learning
(NS-Q)  but may be possible.
As with any exploration method  encouraging novelty can come at a cost if such an exploration
pressure is not necessary. In Atari games such as Alien and Gravitar  and in the Humanoid Locomotion
problem without a deceptive trap  both NS-ES and NSR-ES perform worse than ES. To avoid this
cost  we introduce the NSRA-ES algorithm  which attempts to invest in exploration only when
necessary. NSRA-ES tends to produce better results than ES  NS-ES  and NSR-ES across many
different domains  making it an attractive new algorithm for deep RL tasks. Similar strategies for
adapting the amount of exploration online may also be advantageous for other deep RL algorithms.
How best to dynamically balance between exploitation and exploration in deep RL remains an open 
critical research challenge  and our work underscores the importance of  and motivates further  such
work. Overall  our work shows that ES is a rich and unexploited parallel path for deep RL research.
It is worthy of exploring not only because it is an alternative algorithm for RL problems  but also
because innovations created in the ES family of algorithms could be ported to improve other deep RL
algorithm families like policy gradients and Q learning  or through hybrids thereof.

9

Acknowledgments
We thank all of the members of Uber AI Labs  in particular Thomas Miconi  Rui Wang  Peter Dayan 
John Sears  Joost Huizinga  and Theofanis Karaletsos  for helpful discussions. We also thank Justin
Pinkul  Mike Deats  Cody Yancey  Joel Snow  Leon Rosenshein and the entire OpusStack Team
inside Uber for providing our computing platform and for technical support.

References
[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction  volume 1. 1998.

[2] Gunar E. Liepins and Michael D. Vose. Deceptiveness and genetic algorithm dynamics. Technical Report
CONF-9007175-1  Oak Ridge National Lab.  TN (USA); Tennessee Univ.  Knoxville  TN (USA)  1990.

[3] Joel Lehman and Kenneth O. Stanley. Novelty search and the problem with objectives.

Programming Theory and Practice IX (GPTP 2011)  2011.

In Genetic

[4] Kenji Kawaguchi. Deep learning without poor local minima. In NIPS  pages 586–594  2016.

[5] Yann Dauphin  Razvan Pascanu  Çaglar Gülçehre  Kyunghyun Cho  Surya Ganguli  and Yoshua Bengio.
Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. ArXiv
e-prints  abs/1406.2572  2014.

[6] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G Bellemare 
Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al. Human-level control through
deep reinforcement learning. Nature  518(7540):529–533  2015.

[7] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap  Tim Harley 
David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML 
pages 1928–1937  2016.

[8] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region policy

optimization. In ICML  pages 1889–1897  2015.

[9] Jürgen Schmidhuber. Formal theory of creativity  fun  and intrinsic motivation (1990–2010).

Transactions on Autonomous Mental Development  2(3):230–247  2010.

IEEE

[10] Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational

approaches. Frontiers in Neurorobotics  1:6  2009.

[11] Haoran Tang  Rein Houthooft  Davis Foote  Adam Stooke  OpenAI Xi Chen  Yan Duan  John Schulman 
Filip DeTurck  and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement
learning. In NIPS  pages 2750–2759  2017.

[12] Marc Bellemare  Sriram Srinivasan  Georg Ostrovski  Tom Schaul  David Saxton  and Remi Munos.

Unifying count-based exploration and intrinsic motivation. In NIPS  pages 1471–1479  2016.

[13] Georg Ostrovski  Marc G Bellemare  Aaron van den Oord  and Rémi Munos. Count-based exploration

with neural density models. arXiv preprint arXiv:1703.01310  2017.

[14] Bradly C Stadie  Sergey Levine  and Pieter Abbeel. Incentivizing exploration in reinforcement learning

with deep predictive models. arXiv preprint arXiv:1507.00814  2015.

[15] Rein Houthooft  Xi Chen  Yan Duan  John Schulman  Filip De Turck  and Pieter Abbeel. Vime: Variational

information maximizing exploration. In NIPS  pages 1109–1117  2016.

[16] Deepak Pathak  Pulkit Agrawal  Alexei A Efros  and Trevor Darrell. Curiosity-driven exploration by

self-supervised prediction. arXiv preprint arXiv:1705.05363  2017.

[17] A. Cully  J. Clune  D. Tarapore  and J.-B. Mouret. Robots that can adapt like animals. Nature  521:503–507 

2015. doi: 10.1038/nature14422.

[18] Jean-Baptiste Mouret and Jeff Clune.

arXiv:1504.04909  2015.

Illuminating search spaces by mapping elites. arXiv preprint

[19] Justin K Pugh  Lisa B. Soros  and Kenneth O. Stanley. Quality diversity: A new frontier for evolutionary

computation. 3(40)  2016. ISSN 2296-9144.

10

[20] Joel Lehman and Kenneth O. Stanley. Evolving a diversity of virtual creatures through novelty search and
local competition. In GECCO ’11: Proceedings of the 13th annual conference on Genetic and evolutionary
computation  pages 211–218  2011.

[21] Roby Velez and Jeff Clune. Novelty search creates robots with general skills for exploration. In Proceedings
of the 2014 Conference on Genetic and Evolutionary Computation  GECCO ’14  pages 737–744  2014.

[22] Joost Huizinga  Jean-Baptiste Mouret  and Jeff Clune. Does aligning phenotypic and genotypic modularity
improve the evolution of neural networks? In Proceedings of the 2016 on Genetic and Evolutionary
Computation Conference (GECCO)  pages 125–132  2016.

[23] Ingo Rechenberg. Evolutionsstrategien. In Simulationsmethoden in der Medizin und Biologie  pages

83–114. 1978.

[24] Tim Salimans  Jonathan Ho  Xi Chen  and Ilya Sutskever. Evolution strategies as a scalable alternative to

reinforcement learning. arXiv preprint arXiv:1703.03864  2017.

[25] Daan Wierstra  Tom Schaul  Jan Peters  and Juergen Schmidhuber. Natural evolution strategies.

Evolutionary Computation  2008.  pages 3381–3387  2008.

In

[26] Frank Sehnke  Christian Osendorfer  Thomas Rückstieß  Alex Graves  Jan Peters  and Jürgen Schmidhuber.

Parameter-exploring policy gradients. Neural Networks  23(4):551–559  2010.

[27] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning  8(3-4):229–256  1992.

[28] Matthias Plappert  Rein Houthooft  Prafulla Dhariwal  Szymon Sidor  Richard Y Chen  Xi Chen  Tamim
Asfour  Pieter Abbeel  and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint
arXiv:1706.01905  2017.

[29] Meire Fortunato  Mohammad Gheshlaghi Azar  Bilal Piot  Jacob Menick  Ian Osband  Alex Graves  Vlad
Mnih  Remi Munos  Demis Hassabis  Olivier Pietquin  et al. Noisy networks for exploration. arXiv
preprint arXiv:1706.10295  2017.

[30] Marc G Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The arcade learning environment:

An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR)  47:253–279  2013.

[31] Yang Liu  Prajit Ramachandran  Qiang Liu  and Jian Peng. Stein variational policy gradient. arXiv preprint

arXiv:1704.02399  2017.

[32] Giuseppe Cuccu and Faustino Gomez. When novelty is not enough. In European Conference on the

Applications of Evolutionary Computation  pages 234–243. Springer  2011.

[33] Giuseppe Cuccu  Faustino Gomez  and Tobias Glasmachers. Novelty-based restarts for evolution strategies.

In Evolutionary Computation (CEC)  2011 IEEE Congress on  pages 158–163. IEEE  2011.

[34] Nikolaus Hansen  Sibylle D Müller  and Petros Koumoutsakos. Reducing the time complexity of the
derandomized evolution strategy with covariance matrix adaptation (cma-es). Evolutionary computation 
11(1):1–18  2003.

[35] Justin K Pugh  Lisa B Soros  Paul A Szerlip  and Kenneth O Stanley. Confronting the challenge of quality
diversity. In Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation
(GECCO)  pages 967–974  2015.

[36] Greg Brockman  Vicki Cheung  Ludwig Pettersson  Jonas Schneider  John Schulman  Jie Tang  and

Wojciech Zaremba. Openai gym  2016.

[37] Yavar Naddaf. Game-independent ai agents for playing atari 2600 console games. 2010.

[38] Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In

The International Joint Conference on Neural Networks  pages 1–8. IEEE  2010.

[39] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 

2013.

[40] Aaron van den Oord  Nal Kalchbrenner  Lasse Espeholt  Oriol Vinyals  Alex Graves  et al. Conditional

image generation with pixelcnn decoders. In NIPS  pages 4790–4798  2016.

[41] Christopher Stanton and Jeff Clune. Curiosity search: producing generalists by encouraging individuals to

continually explore and acquire skills throughout their lifetime. PloS one  2016.

11

[42] Phillip Paquette. Super mario bros. in openai gym  2016.

[43] James Kirkpatrick  Razvan Pascanu  Neil Rabinowitz  Joel Veness  Guillaume Desjardins  Andrei A Rusu 
Kieran Milan  John Quan  Tiago Ramalho  Agnieszka Grabska-Barwinska  et al. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences  page 201611835  2017.

[44] Roby Velez and Jeff Clune. Diffusion-based neuromodulation can eliminate catastrophic forgetting in

simple neural networks. arXiv preprint arXiv:1705.07241  2017.

[45] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences  3(4):

128–135  1999.

[46] Antoine Cully and Jean-Baptiste Mouret. Behavioral repertoire learning in robotics. In Proceedings of the

15th annual conference on Genetic and evolutionary computation  pages 175–182. ACM  2013.

[47] Andrei A Rusu  Sergio Gomez Colmenarejo  Caglar Gulcehre  Guillaume Desjardins  James Kirkpatrick 
Razvan Pascanu  Volodymyr Mnih  Koray Kavukcuoglu  and Raia Hadsell. Policy distillation. arXiv
preprint arXiv:1511.06295  2015.

[48] Max Jaderberg  Valentin Dalibard  Simon Osindero  Wojciech M Czarnecki  Jeff Donahue  Ali Razavi 
Oriol Vinyals  Tim Green  Iain Dunning  Karen Simonyan  et al. Population based training of neural
networks. arXiv preprint arXiv:1711.09846  2017.

[49] Risto Miikkulainen  Jason Liang  Elliot Meyerson  Aditya Rawal  Dan Fink  Olivier Francon  Bala Raju 
Arshak Navruzyan  Nigel Duffy  and Babak Hodjat. Evolving deep neural networks. arXiv preprint
arXiv:1703.00548  2017.

[50] Jorge Gomes  Pedro Mariano  and Anders Lyhne Christensen. Systematic derivation of behaviour charac-

terisations in evolutionary robotics. arXiv preprint arXiv:1407.0577  2014.

[51] Elliot Meyerson  Joel Lehman  and Risto Miikkulainen. Learning behavior characterizations for novelty
search. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2016)  Denver 
Colorado  2016. ACM.

[52] Joel Lehman and Kenneth O. Stanley. Abandoning objectives: Evolution through the search for novelty
alone. Evolutionary Computation  19(2):189–223  2011. URL http://www.mitpressjournals.org/
doi/pdf/10.1162/EVCO_a_00025.

[53] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen. Improved

techniques for training gans. In NIPS  pages 2234–2242  2016.

[54] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. In ICML  pages 448–456  2015.

[55] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

12

,Aaron van den Oord
Nal Kalchbrenner
Lasse Espeholt
koray kavukcuoglu
Oriol Vinyals
Alex Graves
Edoardo Conti
Vashisht Madhavan
Felipe Petroski Such
Joel Lehman
Kenneth Stanley
Jeff Clune